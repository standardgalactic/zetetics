A Wiley-Science Wise Co-Publication
David L. Andrews
Biomedical Photonics, 
Spectroscopy,  
and Microscopy
Photonics Volume 4


PHOTONICS


PHOTONICS
Scientific Foundations, Technology
and Applications
Biomedical Photonics, Spectroscopy,
and Microscopy
Volume IV
Edited by
DAVID L. ANDREWS
School of Chemical Sciences
University of East Anglia
Norwich, UK

Copyright © 2015 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or
by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400,
fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission
should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken,
NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in
preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and specifically disclaim any implied warranties of
merchantability or fitness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be suitable
for your situation. You should consult with a professional where appropriate. Neither the publisher nor
author shall be liable for any loss of profit or any other commercial damages, including but not limited to
special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our
Customer Care Department within the United States at (800) 762-2974, outside the United States at
(317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may
not be available in electronic formats. For more information about Wiley products, visit our web site at
www.wiley.com.
Library of Congress Cataloging-in-Publication Data applied for.
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

CONTENTS
List of Contributors
ix
Preface
xiii
1
Fluorescence
1
David J. S. Birch, Yu Chen, and Olaf J. Rolinski
1.1
Introduction, 1
1.2
Spectra, 2
1.3
Quantum Yield, 6
1.4
Lifetime, 12
1.5
Quenching, 23
1.6
Anisotropy, 30
1.7
Microscopy, 38
1.8
Conclusions, 48
Acknowledgments, 48
References, 49
2
Single-Molecule Detection and Spectroscopy
59
Michel Orrit
2.1
Introduction, 59
2.2
Experimental Setups, 62
2.3
Fluorescence Spectroscopy, 66
2.4
Fluorescence Correlation Spectroscopy, 72
2.5
Fluorescence Excitation Spectroscopy, 78
v

vi
CONTENTS
2.6
Other Detection Methods, 86
2.7
Conclusion, 93
Acknowledgments, 94
References, 94
3
Resonance Energy Transfer
101
David L. Andrews, David S. Bradshaw, Rayomond Dinshaw, and Gregory D. Scholes
3.1
Introduction, 101
3.2
History of RET, 102
3.3
The Photophysics of RET, 103
3.4
Investigative Applications of RET in Molecular Biology, 113
3.5
The Role of RET in Light-Harvesting Complexes, 118
Acknowledgments, 122
References, 122
4
Biophotonics of Photosynthesis
129
Valter Zazubovich and Ryszard Jankowiak
4.1
Introduction, 129
4.2
Structure of Pigment–Protein Complexes and Structure–Function
Relationships, 130
4.3
Key Concepts in Physics of Pigment–Protein Complexes, 133
4.4
Experimental Techniques, 141
4.5
Examples, 145
4.6
Conclusions, 156
Acknowledgments, 157
References, 157
5
Optical Sectioning Microscopy and Biological Imaging
165
John Girkin
5.1
Introduction and Background, 165
5.2
Confocal Imaging, 168
5.3
Nonlinear Microscopy, 172
5.4
Practical Implementation of Nonlinear Microscopy, 181
5.5
Recent Advances in Nonlinear Microscopy, 184
5.6
Widefield Optical Sectioning by Specialized Illumination Methods, 186
5.7
Summary, 190
References, 191
6
Cell Handling, Sorting, and Viability
197
Darwin Palima, Thomas Aabo, Andrew Ba˜nas, and Jesper Gl¨uckstad
6.1
Handling Cells with Light, 198
6.2
Optical Sorting, 215
6.3
Cell Viability, 220
References, 230

CONTENTS
vii
7
Tissue Polarimetry
239
Alex Vitkin, Nirmalya Ghosh, and Antonello de Martino
7.1
Introduction, 239
7.2
Polarized Light Fundamentals, 240
7.3
Instrumentation, 266
7.4
Forward Modeling and Testing in Phantoms, 282
7.5
Applications, 297
7.6
Conclusions and Outlook, 313
References, 314
8
Optical Waveguide Biosensors
323
Daphn´e Duval and Laura M. Lechuga
8.1
Introduction, 323
8.2
Fundamentals of Label-Free Optical Waveguide Biosensing, 324
8.3
Surface Biofunctionalization, 328
8.4
Evaluation of Optical Biosensors, 331
8.5
Integrated Optical Waveguide-Based Biosensors, 334
8.6
Optical Fiber-Based Biosensors, 349
8.7
Lab-On-A-Chip Integration, 354
8.8
Summary, 357
References, 358
9
Light Propagation in Highly Scattering Turbid Media: Concepts,
Techniques, and Biomedical Applications
367
R. R. Alfano, W. B. Wang, L. Wang, and S. K. Gayen
9.1
Introduction, 367
9.2
Physics Behind Optical Imaging Through a Highly Scattering
Turbid Medium, 369
9.3
Study of Ballistic and Diffused Light Components, 378
9.4
Photon-Sorting Gates, 384
9.5
Transition From Ballistic to Diffuse Imaging in Turbid Media, 402
9.6
Conclusion, 404
Acknowledgments, 404
References, 404
10
Photodynamic Therapy
413
Rakkiyappan Chandran, Tyler G. St. Denis, Daniela Vecchio, Pinar Avci,
Magesh Sadasivam, and Michael R. Hamblin
10.1
Historical Overview of PDT, 413
10.2
Introduction to PDT, 415
10.3
Photosensitizer Structure and Photophysical Properties, 418
10.4
Light Dosimetry and Photodynamic Therapy Light Sources, 422
10.5
Light-Based Strategies to Enhance PDT, 423
10.6
PDT Targeting and Nanotechnology, 425

viii
CONTENTS
10.7
PDT for Dermatology, 428
10.8
PDT for Oncology, 433
10.9
PDT for Infectious Disease, 435
10.10 PDT in Ophthalmology, 445
10.11 PDT and The Immune System, 446
10.12 Conclusion, 449
Acknowledgment, 449
References, 449
11
Imaging and Probing Cells Beyond the Optical Diffraction Limit
469
Mark Sch¨uttpelz and Thomas Huser
11.1
The Quest for Achieving Optical Resolution Beyond
ABBE’S Limit, 469
11.2
Stimulated Emission Depletion Microscopy, 474
11.3
Photoactivated Localization Microscopy and Stochastic Optical
Reconstruction Microscopy, 477
11.4
Structured Illumination Microscopy, 483
11.5
Super-Resolution Optical Fluctuation Imaging and Other
Approaches, 491
11.6
Outlook, 495
Acknowledgments, 496
References, 497
12
Technology
503
Ann E. Elsner and Christopher A. Clark
12.1
Basic Ocular Anatomy and Physiology, 503
12.2
Measurement Techniques, 514
12.3
Anterior Segment Diagnostics, Refractive Measurements,
and Treatment, 522
12.4
Diagnostic Applications and Treatment of Posterior Segment, 529
References, 534
Index
543

LIST OF CONTRIBUTORS
Thomas Aabo DTU Fotonik, Department of Photonics Engineering, Technical Uni-
versity of Denmark, Kgs. Lyngby, Denmark
R. R. Alfano Institute for Ultrafast Spectroscopy and Lasers (IUSL), City College
of New York, New York, NY, USA; and Physics Department, City College of New
York, New York, NY, USA
David L. Andrews School of Chemistry, University of East Anglia, Norwich
Research Park, Norwich, Norfolk, UK
Pinar Avci Wellman Center for Photomedicine, Massachusetts General Hospital,
Boston, MA, USA; and Department of Dermatology, Harvard Medical School,
Boston, MA, USA
Andrew Ba˜nas DTU Fotonik, Department of Photonics Engineering, Technical
University of Denmark, Kgs. Lyngby, Denmark
David J. S. Birch Photophysics Group, Department of Physics, Strathclyde Univer-
sity, Glasgow, UK
David S. Bradshaw School of Chemistry, University of East Anglia, Norwich
Research Park, Norwich, Norfolk, UK
Yu Chen Photophysics Group, Department of Physics, Strathclyde University,
Glasgow, UK
Christopher A. Clark School of Optometry, Indiana University, Bloomington, IN,
USA
Antonello De Martino Ecole Polytechnique, Palaiseau, France
ix

x
LIST OF CONTRIBUTORS
Rayomond Dinshaw Department of Chemistry, Institute for Optical Sciences and
Centre for Quantum Information and Quantum Control, University of Toronto,
Toronto, ON, Canada
Daphn´e Duval NanoBiosensors and Bioanalytical Applications Group, Centro
Investigacion Biomedica en Red – Bioingenieria, Biomateriales y Nanomedicina,
Consejo Superior de Investigaciones Cientificas, Institut Catala de Nanociencia i
Nanotecnologia, Bellaterra, Barcelona, Spain
Ann E. Elsner School of Optometry, Indiana University, Bloomington, IN, USA
S. K. Gayen Physics Department, City College of New York, New York, NY, USA
Rakkiyappan Chandran Wellman Center for Photomedicine, Massachusetts
General Hospital, Boston, MA, USA; and Department of Dermatology, Harvard
Medical School, Boston, MA, USA; and Amity Institute of Nanotechnology,
Amity University, Noida, India
Nirmalya Ghosh Department of Physical Sciences, Indian Institute of Science
Education and Research, Kolkata, India
John Girkin Biophysical Sciences Institute, Department of Physics, Durham Uni-
versity, Durham, UK
Jesper Gl¨uckstad DTU Fotonik, Department of Photonics Engineering, Technical
University of Denmark, Kgs. Lyngby, Denmark
Michael R. Hamblin Wellman Center for Photomedicine, Massachusetts General
Hospital, Boston, MA, USA; and Department of Dermatology, Harvard Medical
School, Boston, MA, USA; and Harvard-MIT Division of Health Sciences and
Technology, Cambridge, MA, USA
Thomas Huser Biomolecular Photonics, Department of Physics, University of
Bielefeld, Bielefeld, Germany
Ryszard Jankowiak Department of Chemistry, Kansas State University, Manhattan,
KS, USA; and Department of Physics, Kansas State University, Manhattan, KS,
USA
Laura M. Lechuga NanoBiosensors and Bioanalytical Applications Group, Centro
Investigacion Biomedica en Red – Bioingenieria, Biomateriales y Nanomedicina,
Consejo Superior de Investigaciones Cientificas, Institut Catala de Nanociencia i
Nanotecnologia, Bellaterra, Barcelona, Spain
Michel Orrit Institute of Physics, Leiden University, Leiden, The Netherlands
Darwin Palima DTU Fotonik, Department of Photonics Engineering, Technical
University of Denmark, Kgs. Lyngby, Denmark
Olaf J. Rolinski Photophysics Group, Department of Physics, Strathclyde Univer-
sity, Glasgow, UK

LIST OF CONTRIBUTORS
xi
Magesh Sadasivam Wellman Center for Photomedicine, Massachusetts General
Hospital, Boston, MA, USA; and Department of Dermatology, Harvard Medi-
cal School, Boston, MA, USA; and Amity Institute of Nanotechnology, Amity
University, Noida, India
Gregory D. Scholes Department of Chemistry, Institute for Optical Sciences and
Centre for Quantum Information and Quantum Control, University of Toronto,
Toronto, ON, Canada
Mark Sch¨uttpelz Biomolecular Photonics, Department of Physics, University of
Bielefeld, Bielefeld, Germany
Tyler G. St. Denis Wellman Center for Photomedicine, Massachusetts General
Hospital, Boston, MA, USA; and Department of Chemistry, Columbia University,
New York, NY, USA
Daniela Vecchio Wellman Center for Photomedicine, Massachusetts General
Hospital, Boston, MA, USA; and Department of Dermatology, Harvard Medi-
cal School, Boston, MA, USA
Alex Vitkin Medical Biophysics and Radiation Oncology, University of Toronto,
Toronto, ON, Canada
L. Wang Institute for Ultrafast Spectroscopy and Lasers (IUSL), City College of
New York, New York, NY, USA
W. B. Wang Institute for Ultrafast Spectroscopy and Lasers (IUSL), City College of
New York, New York, NY, USA; and Physics Department, City College of New
York, New York, NY, USA
Valter Zazubovich Department of Physics, Concordia University, Montreal, QC,
Canada


PREFACE
Since its inception, the term “photonics” has been applied to increasingly wide realms
of application, with connotations that distinguish it from the broader-brush terms
“optics” or “the science of light.” The briefest glance at the topics covered in these
volumes shows that such applications now extend well beyond an obvious usage of
the term to signify phenomena or mechanistic descriptions involving photons. Those
who first coined the word partly intended it to convey an aspiration that new areas of
science and technology, based on microscale optical elements, would one day develop
into a comprehensive range of commercial applications as familiar and distinctive
as electronics. The fulfilment of that hope is amply showcased in the four present
volumes, whose purpose is to capture the range and extent of photonics science and
technology.
It is interesting to reflect that in the early 1960s, the very first lasers were usually
bench-top devices whose only function was to emit light. In the period of growth
that followed, most technical effort was initially devoted to increasing laser stability
and output levels, often with scant regard for possibilities that might be presented by
truly photon-based processes at lower intensities. The first nonlinear optical processes
were observed within a couple of years of the first laser development, while quantum
optics at first grew slowly in the background, then began to flourish more spectacularly
several years later. A case can be made that the term “photonics” itself first came
into real prominence in 1982, when the trade publication that had previously been
entitled Optical Spectra changed its name to Photonics Spectra. At that time the
term still had an exotic and somewhat contrived ring to it, but it acquired a new
respectability and wider acceptance with the publication of Bahaa Saleh and Malvin
Teich’s definitive treatise, Fundamentals of Photonics, in 1991. With the passage
of time, the increasing pace of development has been characterized by the striking
xiii

xiv
PREFACE
progress in miniaturization and integration of optical components, paving the way for
fulfilment of the early promise. As the laser industry has evolved, parallel growth in
the optical fiber industry has helped spur the continued push toward the long-sought
goal of total integration in optical devices.
Throughout the commissioning, compiling, and editing that have led to the publi-
cation of these new volumes, it has been my delight and privilege to work with many
of the world’s top scientists. The quality of the product attests to their commitment
and willingness to devote precious time to writing chapters that glow with authori-
tative expertise. I also owe personal thanks to the ever-professional and dependable
staff of Wiley, without whose support this project would never have come to fruition.
It seems fitting that the culmination of all this work is a sequence of books published
at the very dawning of the UNESCO International Year of Light. Photonics is shap-
ing the world in which we live, more day by day, and is now ready to take its place
alongside electronics, reshaping modern society as never before.
David L. Andrews
Norwich, U.K., July 2014

1
FLUORESCENCE
David J. S. Birch, Yu Chen, and Olaf J. Rolinski
Photophysics Group, Department of Physics, Strathclyde University, Glasgow, UK
1.1
INTRODUCTION
Within the wide range of spectroscopic techniques facilitated by photonics, fluores-
cence sits alongside the likes of spectrophotometry, Raman, FTIR, circular dichro-
ism, and ultrafast in providing complementary and unique information. Although
fluorescence can hardly be called a new phenomenon, there can be little doubt that
it continues to facilitate many important new observations and techniques across a
whole range of disciplines. Just as photonics has become an enabling technology so
too fluorescence has become an enabling phenomenon. Fluorescence has made, and
continues to make, particular impact in the biosciences and in healthcare. This has
been dramatically demonstrated in recent years by the key role played by fluores-
cence in the complete sequencing of the human genome and in the displacement of
radioactive markers by fluorescence probes in disease diagnostics. Underpinning the
impact of fluorescence is a research base founded upon the fact that the nanosecond
timescales and nanometer distances, in which the properties of fluorescence can be
influenced, are ideally matched to many physiological processes and structures.
Originating from a spin-allowed singlet−singlet transition, fluorescence has a
much higher quantum yield and is usually easier to study than its photophysical
counterpart, phosphorescence, which involves a spin-forbidden triplet–singlet transi-
tion. Reflecting its more generic usage, and greater range of materials and conditions
that facilitate fluorescence, here we concentrate on fluorescence rather than phospho-
rescence. Fluorescence is traditionally associated with aromatic molecules, of which
there is a vast number, but recently there has been the emergence of a whole new
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
1

2
FLUORESCENCE
range of complementary luminescent nanoparticle emitters fabricated from semicon-
ductors, gold and diamond.
When the readily accessible properties of fluorescence are combined with the
high sensitivity afforded by photon counting photonics, fluorescence has enabled
the ultimate limit of single molecule detection to be realized and this in turn is
helping to open up new frontiers, such as molecular pathology, whereby metabolism,
disease, and pharmacology can be studied at the most fundamental level. Taking
the perspective of fluorescence as an enabling phenomenon, we have chosen in this
chapter to survey the main techniques and measurements it “enables.” We cover
spectra, quantum yield, lifetime, quenching, anisotropy, and microscopy, in each
case citing topical review articles, many of the original references, underlying theory
and modern day applications. The applications are also supported by descriptions
of the context, theory, instrumentation, and techniques. Throughout we focus on the
methods which are in most widespread use, while highlighting many of the most
recent developments. There are already plenty of excellent general texts that survey
fluorescence in the wider context of photophysics, and its related techniques, and in
more depth than we do here [1–4]. Nevertheless, we hope our approach will provide
a useful introduction for readers seeking to learn the basics through to the current
state of the field by means of examples of what fluorescence might be able to do
for them.
1.2
SPECTRA
Measuring absorption and fluorescence spectra is usually the first place to start in
any fluorescence study. The origins of many of the fundamentals of fluorescence lie
within spectra and although at times they might lack specificity, the importance of
spectra in providing supporting information should not be overlooked when more
advanced implementations of fluorescence are being undertaken.
1.2.1
Background and Theory
Fluorescence can be viewed as a multidimensional contour of intensity, wavelength,
quantum yield, decay time, polarization, and position that together characterize the
emitting species. Fluorescence spectra are today quite simple to measure and reveal
information on the energy levels of a fluorophore, in terms of electronic (∼2–3 eV)
and vibrational (∼0.01 eV) properties, that are superimposed on what is effectively
a rotational continuum. All of these are capable of being influenced by the local
environment, and hence, fluorescence spectra are not only a fingerprint of a molecule,
but also can be used as a probe of local interactions.
The fluorescence spectroscopy of aromatic molecules is predominantly in the near
ultraviolet (UV) to near-infrared (IR) (∼250–900 nm) as it is due to the excitation
of weakly bound π-electrons rather than the more strongly bound σ-electrons. In
general, where π-electron delocalization increases with the size of the molecule, the
absorption and fluorescence spectra shift to longer wavelengths in the manner of a

SPECTRA
3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
220
240
260
280
300
320
340
360
380
400
420
440
460
Fluorescence spectroscopy
I0(  )
λ
I0(  )
λ
If(  )
λ
I(  )
λ
Absorption spectroscopy
Excited
state
Ground
state
Abs.
Fl.
Nonradiative
relaxation
(Heat)
Wavelength (nm)
Relative intensity
PPO
NATA
HSA
Fl.
Abs.
Vibrational levels
Electronic levels
(Heat)
(a)
(b)
(c)
(d)
S1
S0
FIGURE 1.1
Geometries for the measurement of (a) absorption (I0 – I) and (b) fluores-
cence (If) spectra. (c) Simplified Jablonski energy level scheme for singlet states involved in
fluorescence and (d) corresponding spectra measured in condensed media. PPO refers to the
scintillator 2,5-diphenyloxazole, NATA is N-acetyl-l-tryptophanamide, a derivative of the flu-
orescent amino acid tryptophan, and HSA is the protein human serum albumin, which contains
a single tryptophan.
particle in a box [2]. This can often lead to a useful intuitive expectation of where
spectra occur for different molecules in respect to each other before any measurement
is performed.
Figure 1.1 illustrates some of the basics of fluorescence spectroscopy. Unlike
the measurement of absorption spectra, which by necessity requires the incident
light and transmitted light to be detected in-line, fluorescence is usually detected
off-axis in order to minimize the detection of the excitation light as this would
otherwise swamp the much smaller fluorescence signal. In general, fluorescence is
isotropic and is usually detected at 90◦to the excitation as illustrated. The energy level
scheme shown in Figure 1.1 just relates to the singlet manifold and is a simplified
version of the Jablonski scheme [2]. The spectra shown illustrate the fact that in
condensed media fluorescence occurs from the lowest vibrational level of the lowest
electronic excited state S1 to vibrational levels of the S0 electronic ground state
(Kasha’s rule [2]). Therefore, whereas absorption spectra contain information on
the vibrational spacing of the excited state, fluorescence spectra contain information
on the vibrational spacing of ground state. The fact that vibrational levels, although
quantized, are not well resolved is due to the spectral smearing generated by rotational
modes of lower energy. Taken together these properties result in fluorescence being

4
FLUORESCENCE
Automated
polarizer
Xenon lamp
Computer with instrument
control, steady-state spectra
acquisition and analysis
Wavelength (nm)
550
0
5
10
15
20
25
600
650
Intensity (a.u.)
Focus lens
Excitation monochromator
Emission monochromator
Automated
polarizer
Longpass
filter
Cuvette-
containing
sample
Photomultiplier
FIGURE 1.2
Typical fluorimeter schematic for recording excitation and fluorescence
spectra.
shifted to longer wavelengths as compared to absorption, the so-called Stokes shift,
with both spectra often displaying mirror image symmetry across their overlap [1].
1.2.2
Experimental
Figure 1.2 shows the common L-format configuration of a fluorimeter for recording
fluorescence spectra. It comprises a xenon source, excitation monochromator (e.g.,
Seya-Namioka geometry as shown here or Czerny-Turner), sample compartment,
emission monochromator, and photomultiplier detector. Further optical components,
either lenses or mirrors, for focusing are required in order to match the cone angles
of the excitation and fluorescence to the monochromator f number. Polarizers, either
dichroic for the visible or quartz prisms to extend down to the UV, are sometimes
added (e.g., for use in anisotropy studies—see Section 1.6). Fluorescence spectra
are usually corrected for the spectral response of the emission monochromator and
detecting photomultiplier, by division of this combined instrumental function.

SPECTRA
5
This configuration can also be used to record excitation spectra by keeping the
monochromator tuned to an emission wavelength while scanning the excitation
monochromator wavelength 𝜆. For an optically dilute sample, the excitation spectrum
is equivalent to the sample’s absorption spectrum. From the Beer–Lambert law,
I (𝜆) = I0 (𝜆) 10−𝜀(𝜆)cd,
(1.1)
where I is the transmitted intensity after sample absorption, I0 the incident intensity, c
the molar concentration in mol L−1 (M), d the sample path length in cm, and 𝜀(𝜆) the
decadic molar extinction coefficient in mol−1 L cm−1, the ordinate in an absorption
spectrum and the molecular fingerprint which is described by the spectral shape.
𝜀(𝜆)cd is defined as the optical density or absorbance of the sample.
In the limit of dilute solution, defined as 𝜀(𝜆)cd ≪1, and defining the fluorescence
quantum yield Φf as the ratio of the rate of total fluorescence emission If to the rate
of absorption:
Φf = If∕(I0 −I),
(1.2)
we obtain [5]
If = 2.303Φf𝜀(𝜆) cd.
(1.3)
Because Φf is usually independent of excitation wavelength for aromatic molecules
in condensed media, Eq. (1.3) shows how detecting fluorescence (even at only one
emission wavelength), while scanning the excitation wavelength, leads to an exci-
tation spectrum that is equivalent to the absorption spectrum for dilute solutions
because If ∝𝜀(𝜆). In the case of absorption, the spectrophotometer used to measure
spectra automatically corrects for the spectral distribution of the light source, the
spectral response of the monochromator and detecting photomultiplier, by dividing
the two signals generated in a dual beam arrangement of sample and reference chan-
nels. By similar means fluorescence excitation spectra need to be corrected for the
light source spectral output and the excitation monochromator spectral response. In
this regard, rhodamine B in ethylene glycol has long been used as a quantum counter
[6] as its fluorescence yield is independent of excitation wavelength in the range
220–600 nm and this overcomes the wavelength response of the photomultiplier. The
limit of detection for fluorescence is orders of magnitude lower than that of absorp-
tion (e.g., ∼10−12 M compared to 10−8 M) because the latter ultimately relies upon
measuring a small difference between two large signals in the sample and reference
channel. Fluorescence on the other hand compares a signal with zero signal or low
background, which is a much easier measurement.
1.2.3
Application Example—Melanin Spectra
In order to illustrate how absorption and fluorescence spectra often interplay
in tandem, we consider the example of the auto-oxidation of 3,4-dihydroxy-l-
phenylalanine (l-DOPA) to produce melanin (Fig. 1.3). l-DOPA is a small molecule
as aromatics go, but during its auto-oxidation a complex series of intermediaries are

6
FLUORESCENCE
l-DOPA
NH2
OH
HO
HO
0
50
100
150
200
250
300
Polymerization time (min)
0
50
100
150
200
250
300
500
450
400
0
100
200
300
400
Polymerization time (min)
700
600
500
400
Wavelength (nm)
Wavelength (nm)
300
0.0
0.2
0.4
0.6
Absorption
Emission intensity (a.u.)
excitation at 275 nm
0.8
1.0
O
(a)
(b)
FIGURE 1.3
(a) Absorption and (b) fluorescence spectra as eumelanin is polymerized from
the auto-oxidation of an aqueous solution of l-DOPA at pH 12. The fluorescence excitation
wavelength is 275 nm. (For a color version of this figure, see the color plate section.)
formed before an extended structure, believed to bear a close similarity to natural
eumelanin, is formed [7].
Initially the phenyl ring of l-DOPA is seen to dominate the absorption spectrum
with a peak occurring below 300 nm, similar to that for benzene, toluene, and the three
fluorescent amino acids phenylalanine, tyrosine, and tryptophan. At these times, the
fluorescence is quite strong. However, as time progresses, more extended structures
are formed to give the characteristically broad band attenuation spectrum of melanin
describing the photoprotective action of melanin and the 275-nm excitation generates
less fluorescence as the excitation energy is dissipated nonradiatively [8]. Ideally it
is preferable to use intrinsic fluorescence to report on structure. This is because it
does not distort native structure as extrinsic fluorescence probes can do. However,
complex heterostructures such as melanin usually contain multiple fluorophores and
give a complex intrinsic fluorescence signature that is difficult to interpret. The same
is true of the many types of protein that contain multiple fluorescent amino acids.
1.3
QUANTUM YIELD
Measuring absolute fluorescence quantum yield is not so common an objective these
days since most of the fundamental processes of aromatic are well-established and the
focus of fluorescence has shifted more toward biological molecules, where their nat-
ural environment is usually saturated with the quencher oxygen. However, recording
relative changes is still very important in many studies.
1.3.1
Theory
Because the time-averaged rate of absorption must equal the total decay rate
(otherwise energy would be piling up in a molecule), we can also express

QUANTUM YIELD
7
Eq. (1.2) describing the fluorescence quantum yield Φf in terms of intramolecular
processes as
Φf = kr∕(kr + knr),
(1.4)
where kr is the radiative decay rate (units s−1), knr the nonradiative decay rate (com-
posed of internal conversion to the ground state or intersystem crossing to the lowest
triplet state), and their sum is the total decay rate. For this reason floppy molecules
generally have a lower Φf than more rigid molecules. External interactions, for exam-
ple, collisional quenching by oxygen, bring additional terms to the denominator and
further reduce Φf. For this reason, oxygen needs to be removed, usually by freeze-
pump-thaw cycles or by nitrogen bubbling, for the unquenched Φf to be determined
correctly. It will be seen that Φf can have a maximum value of 1. Unfortunately the
simplicity of the definition of Φf belies the difficulty of determining it.
1.3.2
Experimental
It can be appreciated from Eq. (1.2) that, in principle, measuring the fluorescence
quantum yield requires the determination of the number of photons absorbed and
emitted due to fluorescence in a given time. This is a difficult requirement for which
integrating spheres can be used. The sample is placed inside a reflective sphere with
excitation and emission ports in an attempt to collect all the isotropic fluorescence
over a 4π solid angle. Multiple reflections relay the fluorescence to the exit port. Both
solids and solutions can be measured. Figure 1.4 shows one such configuration for
an integrating sphere. Excitation and fluorescence rays are depicted and re-excitation
by one of the fluorescence pathways shown illustrates a common source of error.
Fluorescence from sample
Sample located
directly in
excitation path 
(a)
(b)
Baffle
Excitation
I0(  )
λ
If(  )
λ
FIGURE 1.4
(a) Typical integrating sphere light paths and (b) construction with sample
loading mechanism displayed for cuvettes (top) and solid samples and powders (bottom).
Photos courtesy of Horiba Scientific.

8
FLUORESCENCE
A baffle is usually incorporated to minimize reflections from the entrance to exit
ports. Excitation light still needs to be spectrally filtered out after the exit port.
Integrating spheres have a long-established [9] and ongoing role to play in the
quantification of standards, though they tend to be somewhat impractical and hence
not in widespread use. Other absolute methods include calorimetry, photoacoustic
spectroscopy actinometry, and thermal lensing. However, the usual recourse is to ratio
the spectrally corrected fluorescence spectra of the unknown sample and a suitable ref-
erence standard of known quantum yield, that is, to determine a relative rather than an
absolute quantum yield. A literature survey soon reveals that fluorescence quantum
yields have often courted controversy over the years due to inconsistent measure-
ments between different laboratories. Procedures for absolute and relative quantum
yield measurements in solution are discussed in a recent article along with a com-
prehensive list of references on previous work and recommendations for standards
(Table 1.1) [10].
The process by which relative quantum yields are determined involves first correct-
ing the fluorescence spectrum for the spectral responses of the emission monochroma-
tor and detecting photomultiplier in order to obtain a true and undistorted fluorescence
spectrum. For the sample S and reference standard R, of absorbance AS and AR, and
respective solvent refractive indices nS and nR, we obtain
ΦfS =
ΦfRIfS(𝜆)ARn2
S
IfR(𝜆)ASn2
R
,
(1.5)
where IfS(𝜆) and IfR(𝜆) refer to the integrated and spectrally corrected fluorescence
spectra over wavelength for the sample and reference, respectively. Ideally the same
solvent is used for both the sample and reference, but the n2 ratio corrects for
the effect of any solvent difference on the optical geometry over which fluorescence
is collected [11]. Solvent refractive index also influences the fluorescence lifetime
and the higher precision of lifetime measurement has proved useful in resolving
seemingly conflicting results for quantum yield standards [12].
The accuracy in Φf is typically no better than 5–10% and such a spread in values
is clearly evident in the literature [10]. This seems incongruous with modern day
instrumentation, but in fact relative changes of either peak or integrated fluorescence,
without measurement of the actual Φf, or even spectral correction, are often ade-
quate. The accurate determination of fluorescence quantum yield, like removing the
quencher oxygen from solutions before measurement, is no longer of importance in
many applications. This is illustrated by the following example.
1.3.3
Application Example—ThT Detection of Sheet Structure
Sheet structures occur in nature, for example, as stacks of graphite and β-sheets of
protein or peptide that sometimes form fibrils [13]. Currently sheet structures are of
considerable interest when synthesized as isolated monosheets in carbon nanotubes
and graphene [14]. Thioflavin T (ThT) is a fluorescent dye (see Fig. 1.5) that has been

TABLE 1.1
Some fluorescence quantum yields (Φf) with errors (ΔΦf) at 25◦C that can be useful as standards for comparison and
determining relative quantum yields
Dye
Quinine sulfate
Coumarin 153
Fluorescein
Rhodamine 6G
Rhodamine 101
Oxazine 1
HITCI
IR125
Solvent
0.105 M HClO4
Ethanol
0.1 M NaOH
Ethanol
Ethanol
Ethanol
Ethanol
DMSO
Absorbance (nm)
270–400
350–500
400–550
425–575
475–620
500–710
535–825
550–875
Emission (nm)
385–700
465–750
490–690
505–750
540–750
615–950
700–950
750–1000
Φf
0.59
0.53
0.89
0.91
0.915
0.15
0.30
0.23
ΔΦf
0.04
0.04
0.04
0.04
0.028
0.01
0.01
0.01
Source: Reprinted from Reference 10 with permission of Macmillan Publishers Ltd.
9

10
FLUORESCENCE
750
t / h:
450
600
341
314
247
219
150
300
219
80
55
29
0.8
ThT
650
600
550
500
0
Fluorescence intensity / (a.u.)
(nm)
λ
H3C
CH3
CH3
CH3
Cl–
S
N
N
+
FIGURE 1.5
Increase in ThT fluorescence as beta sheets of Aβ are formed. The excitation
wavelength is 450 nm and the fluorescence peak is at 480 nm. Reprinted from Reference 17
with permission from Elsevier.
widely used for revealing the formation of sheet structures [15], most notably when
studying the aggregation of the peptide beta-amyloid (Aβ); the early oligomers of
which are thought to be the precursor to Alzheimer’s disease. As already mentioned,
floppymoleculesgenerallyhavealowfluorescenceyieldduetohavingmorefavorable
dissipative pathways other than fluorescence. Conversely rigidity imparts a higher
fluorescence yield. ThT behaves as a molecular rotor and upon excitation undergoes
intramolecular rotation to a low-fluorescent twisted intramolecular charge transfer
(TICT) state [16]. This rotation is restricted between sheet structures, increasing the
fluorescence quantum yield, which can thus be used as a probe of sheet formation.
Figure 1.5 shows the peak wavelength and shape of the fluorescence spectrum of ThT
in Aβ are constant as β-sheets are formed [17]. Hence it is perfectly workable to
use the relative change in fluorescence quantum yield to monitor structural changes
without actually having to put a number to Φf.
One of the limitations of using ThT to detect Aβ aggregation is that the early-
stage oligomers, that are thought to disrupt neuronal cellular membranes and initiate
Alzheimer’s disease, are not detected since only the β-sheets formed later lead to an
increase in fluorescence. Changes in fluorescence lifetimes (to be discussed in the
next section) have recently been shown to address this short-coming by using the
intrinsic tyrosine fluorescence lifetime in the case of Aβ [17] and ThT fluorescence
lifetime in the case of insulin fibrils [18].
In cases like eumelanin, where the intrinsic fluorescence is complex (see Fig. 1.3)
due to the presence of multiple fluorophores, extrinsic probes with bespoke struc-
tural sensing characteristics like ThT come to the fore. Surprisingly, although the
composition of melanin is well known to be largely dihydroxyindoles, its secondary
structure, the minimum functional unit, and the very existence of a protomolecule

QUANTUM YIELD
11
FIGURE 1.6
Probable structural steps in eumelanin synthesis.
that can be replicated to form a secondary structure remains unclear despite decades
of research with a whole gamut of techniques [7,8]. The significance of not knowing
the structure of eumelanin has bearing on a whole range of topics from bioelectronics
(melanin is an efficient conductor and readily binds metal ions) to melanoma, the
most virulent form of skin cancer. The likely structural steps in melanin synthesis are
depicted in Figure 1.6.
Planar oligomeric structures of dihydroxyindoles are thought to form sheets
that are bound together by π–π interactions. The oxidation of primary structures,
either spontaneously or catalyzed, is generally thought to give rise to protomolecules
(oligomers) composed of four or five monomers. These combine into planar sheets
that stack due to π bonding, albeit less strongly bound than σ covalent bonds, thus
accounting for eumelanin’s softness in the manner of graphite. The close packing,
including π–π∗interactions (∼0.3–0.4 nm apart [8]), being consistent with eume-
lanin’s photo-protection via ultrafast nonradiative relaxation. The larger sheet struc-
tures have been proposed to form onion-like layers [19]. Recent fluorescence studies,
based on the increase in ThT fluorescence quantum yield as eumelanin is formed by
auto-oxidation of l-DOPA, support the notion of assembly via a protomolecule that
subsequently forms sheets rather than assembly by monomer addition [20]. This is
illustrated in Figure 1.7.

12
FLUORESCENCE
(a)
482 nm
(ThT)
530 nm
Fluorescence intensity (a.u.)
Normalized
fluorescence intensity
12
100
80
60
40
20
20
40
60
Time (min)
80
100
120
0
8
4
0
120
0
595
475
(nm)
(b)
Time lag due to
protomolecule
formation
Planar sheets
formed
λ
t (nm)
FIGURE 1.7
(a) Fluorescence spectrum of ThT, excited at 450 nm and peaking at 482 nm,
superimposed on the intrinsic fluorescence background, as eumelanin is synthesized at pH 10
by the auto-oxidation of l-DOPA. (b) ThT fluorescence at 482 nm as the eumelanin synthesis
progresses at pH 10 (◦), pH 10.8 (▫) and pH 11.4 (Δ). Reproduced from Reference 20 with
permission from AIP Publishing LLC.
The melanin growth, as reflected by the increase in ThT fluorescence can be
described using a classic sigmoidal function analogous to that encountered in amyloid
fibrillation such as that of Aβ [21]:
If(t) ∼If0 + Imax∕{1 + exp[−km(t −t1∕2)]},
(1.6)
where If(t) is the fluorescence intensity and If0 is the initial or background fluorescence
intensity. The parameter Imax is the fluorescence maximum above the background, km
is the rate of melanin formation, and t1/2 is the time when the fluorescence intensity
has reached the half maximum value.
The very existence of the time-lag suggests that eumelanin is not formed by
monomer addition, but rather it requires a protomolecule (or protomolecules) in a
“binding ready” conformation to be preformed first. Moreover, the increase in ThT
fluorescence in itself further supports the presence of sheet-like structures in eume-
lanin with the fluorescence no longer increasing when the eumelanin-forming reaction
is complete.
1.4
LIFETIME
The fluorescence lifetime of a molecule is the average time it spends in the excited
state before emitting a fluorescence photon. Typically in the nanosecond region, the
fact that the fluorescence lifetime has a finite value is due to local perturbations and
therein rests its usefulness as a probe on the angstrom scale.

LIFETIME
13
Lifetime measurement has emerged in recent years as the most powerful and
versatile technique in fluorescence spectroscopy. It can be performed in either the
time or frequency domain. In comparison to spectra, quantum yield, and intensity,
lifetime provides more kinetic information, a visual image that is easier to interpret, is
analogous to a movie compared to a photo, resolves dynamics more closely, provides
a more stringent test of kinetics models, and is easier to calibrate. Fluorescence
lifetime is also making a mark beyond spectroscopy. For example, in sensing and
fluorescence lifetime imaging microscopy (FLIM), to be discussed in Section 1.7.
1.4.1
Theory
The fluorescence lifetime 𝜏f for aromatic fluorophores can be defined in terms of
rate parameters, consistent with the definition for the fluorescence quantum yield Φf
(Eq. 1.4), as [2]
𝜏f =
1
kr + knr
.
(1.7)
In the time domain, 𝜏f can be conveniently visualized as it describes the fluores-
cence impulse response to δ-function excitation at t = 0 given by
i(t) = i(0)exp −(t∕𝜏f),
(1.8)
and i(t) is the solution to the rate equation defining the decay of the concentration
(population) of the S1 singlet excited state M*, due to fluorescence emission:
d [M∗]
dt
= −(kr + knr)[M∗] = −[M∗]
𝜏f
(1.9)
if multiple and noninteractive fluorescent emitters are present, the same treatment can
be extended linearly by simply adding further exponential terms describing different
lifetimes. The application of fluorescence lifetimes has benefitted greatly from the
fact that most aromatic fluorophores possess a single exponential decay.
Combining Eqs. (1.4) and (1.7) we obtain
𝜏f = Φf
kr
.
(1.10)
Equation (1.10) illustrates why the fluorescence lifetime can be used to monitor
changes that influence fluorescence, as providing the radiative rate kr remains con-
stant, 𝜏f offers equivalent information to Φf, is easier to measure, more precise and
less prone to systematic error. Conversely if Φf and 𝜏f are measured, kr and knr can
be determined.

14
FLUORESCENCE
kr can be described by the Einstein coefficient A and the transition dipole moment
operator M as [2]
kr =
∑
m
Au0 →lm ∝∕< 𝜓∗
l ∕M∕𝜓u > ∕2
(1.11)
where, unlike for spectral lines in the simpler atomic case, A has to be summed over
the whole fluorescence spectrum attributed to the lowest vibrational level u of the
excited S1 singlet state to the different vibrational levels m of the singlet S0 ground
state.
The Strickler–Berg expression [22] and its variants [2] enable kr to be deter-
mined more easily by introducing the Einstein B coefficient relating to the absorption
spectrum:
kr = 2.88 × 10−9n2 < 𝜐−3
F
> A−1
v ∫
𝜀(𝜐)d𝜐
𝜐
,
(1.12)
where < 𝜐−3
F
> A−1
v
is the reciprocal of the mean value of 𝜐−3
F over the fluorescence
spectrum (𝜐= 1/𝜆) and ∫𝜀(𝜐)d𝜐
𝜐
relates to the area under the absorption frequency
spectrum.
The fluorescence lifetime dependence on n2 via the Strickler–Berg equation has
been shown to probe refractive index in cells using protein labeled with the green
fluorescent protein [23].
Equation (1.12) can be approximated to
kr ∼10−4∕𝜀max.
(1.13)
Given 𝜀max ∼105 mol−1 cm−1 for a strong absorber, Eq. (1.13) predicts that, in
the absence of intramolecular nonradiative processes or intermolecular quenching, 𝜏f
is ∼1 ns. It should be cautioned that agreement between Eqs. (1.10) and (1.12) can be
quite variable [24] due to potential sources of experimental error (see Section 1.3) and
the presence of more complex photophysics, for example, quinine sulfate having a
bi- not mono-exponential fluorescence lifetime decay [25]. Significant discrepancies
between kr determined from Eq. (1.10) (which monitors the emitting state) and
Eq. (1.12) (which monitors the absorbing state) have often been used to determine
the presence of “hidden states,” such as those present in polyenes that are facilitated
by intra-molecular twisting [26,27].
In the frequency domain, the equivalent expressions to Eq. (1.8) are
tan 𝜙= −𝜔𝜏f and
(1.14)
m = 1∕(1 + 𝜔2𝜏2
f
)1∕2 ,
(1.15)
where 𝜔is the modulation frequency (in rad s−1) and m a modulation factor that
describes the relative amplitude of the fluorescence and excitation wave forms. The

LIFETIME
15
negative sign in Eq. (1.14) simply denotes the fluorescence lags the excitation by a
phase angle 𝜙.
Equations (1.14) and (1.15) lead to an alternative method for measuring fluores-
cence lifetime as discussed below.
1.4.2
Experimental
There are two main approaches to measuring fluorescence lifetime. Pulse fluorometry
in the time domain and phase-modulation fluorometry in the frequency domain [2–4].
The two different approaches are in theory equivalent and complementary, as they
are linked by Fourier transforms. For example, δ-function excitation in the time
domain is equivalent to exciting with a white frequency spectrum. However, in
experimental implementation and data analysis, there are some significant differences
that have bearing on the relative merits of the two methods. With phase modulation,
the excitation light is modulated or chopped and the fluorescence lifetime determined
from either the phase shift or modulation depth of fluorescence with respect to
excitation. With pulse fluorometry, a temporally sharp excitation pulse is used and
the fluorescence decay recorded with as close a representation of Eq. (1.8) as possible.
Equation (1.14) in particular can be very useful, in so far as it underpins a simple
and quick method of determining a single exponential lifetime by measuring 𝜙and
knowing 𝜔. However, if we think in terms of the equivalence of time and frequency
domain measurements the difference is that Eq. (1.14) is a single point (frequency)
measurement, whereas Eq. (1.8) offers the prospect of measuring over a wide time
range and thus providing a more rigorous test of any kinetic model. This limitation
has been ameliorated to some extent by the introduction of various multifrequency
methods [3, 28, 29], but at the expense of the simplicity and speed afforded by Eq.
(1.14). Despite the advantage of a quick measurement using Eq. (1.14), it would seem
pulse fluorometry has found more widespread favor for fluorescence lifetime mea-
surement. The most popular implementation of pulse fluorometry is time-correlated
single-photon counting (TCSPC) [30,31]. Based on delayed coincidence techniques
developed for nuclear physics, and then adapted for recording scintillation decay
[32], TCSPC and its variants have some critical advantages over other methods,
particularly when researching unknown kinetics. The main advantages are
1. Single-photon counting sensitivity down to the single molecule limit such that
data precision can be simply enhanced by increasing the measurement time.
2. Temporal display of the kinetics aiding interpretation of data.
3. Known statistical basis (Poisson) and therefore meaningful and objective
assessment of the kinetics.
4. As a digital technique it minimizes the effect of signal amplitude variations
that can distort analog methods.
Figure 1.8 shows a schematic and photograph of a typical TCSPC fluorometer.
The arrangement is very similar to the fluorimeter shown in Figure 1.2, illustrating

16
FLUORESCENCE
Sample chamber
Photomultiplier
Pulsed laser diode
Monochromator
Monochromator
(a)
(b)
MHz
Pulsed ps/ns
LD/LED
Automated
polarizer
Focus lens
counts
Time (ns)
weighted
residuals
(std. dev.)
Cuvette
containing
sample
Longpass
filter
Automated
polarizer
Photomultiplier
Emission monochromator
Excitation monochromator
Computer with instrument
control, decay acquisition,
and reconvolution analysis
FIGURE 1.8
(a) Schematic of a typical pulse fluorometer and (b) a modular implementation
incorporating ps diode laser excitation and microchannel photomultiplier detection. Photo
courtesy of Horiba Scientific.
how hybrid instruments can be easily engineered and measurements interchanged
simply by swapping components or routing light paths. Polarizers are used primarily
for anisotropy decay studies and are not generally needed for lifetime measurements
with low viscosity solvents such as water, ethanol, and cyclohexane. However, in
viscous solvents, where the fluorophore rotational correlation time is comparable
to 𝜏f, polarizers are used in order to correct for any intrinsic polarization of the
fluorometer, such as that caused by diffraction gratings, laser excitation, and so on
(see Section 1.6).
These days a semiconductor laser diode (LD) or light-emitting diode (LED) is
typically used for excitation. Perhaps surprisingly, a conventional photon counting
and timing photomultiplier based on a vacuum tube is still the most widely used
detector [33, 34]. Photomultipliers incorporate electron amplification using either
discrete or continuous dynodes, the latter incorporated in the microchannel plate
photomultiplier, which offers the fastest response. Today semiconductor sources
can provide the wavelengths needed to excite most fluorophores. Previously, most
pulse fluorometers used the broad spectral continuum of pulsed spark sources [35].
Even though these nanosecond sources were of low repetition rate (∼30 kHz) and
low energy (∼10 pJ overall wavelengths), it was still possible to achieve a full
spectroscopic capability by using a monochromator rather than a filter on the emission
as well as the excitation channel [36]. Semiconductor excitation sources first appeared
in TCSPC as red/near-IR AlGaAs diode lasers [37]. The subsequent development of
GaN technology [38] provided visible LDs and brought LEDs into the measurement
frame [39]. The switch from flashlamps to semiconductor sources was made complete
when the critically important area of protein fluorescence was demonstrated using
AlGaN LED excitation [40–42]. LED pulse energies are only ∼1 pJ and comparable

LIFETIME
17
to spark sources, but their MHz repetition rates and temporal stability give them clear
advantages. TCSPC at repetition rates as high as 100 MHz have been achieved with
laser diodes [43]. However, to avoid re-excitation and a “saw tooth” fluorescence
decay that is difficult to analyze, this is only practicable for fast lifetimes that fully
decay within the 10 ns measurement time window.
Pulse fluorometry measures the photon-time distribution describing the fluores-
cence decay by using a timing device to measure a “start”–“stop” time interval
between excitation of fluorescence and its detection. There are a number of ways to
achieve this. These include fast oscilloscopes, sampling oscilloscopes (boxcar inte-
grators), multichannel scaling, stroboscopic detector gating, and a time-to-amplitude
converter (TAC), as traditionally used in TCSPC. The TAC converts the time differ-
ence t to a voltage v, which is then digitized in an analog to digital convertor and
stored as a detected photon count event in a multichannel analyzer (MCA) such that
v ∝t. Repeating the cycle in accordance with the excitation pulse frequency allows
a histogram of counts to be accumulated that is representative of the fluorescence
decay, such that Dt is the number of decay counts registered at a time t. Some further
refinements and precautions are necessary though.
The pulse height of photomultiplier output for single photon detection is variable
due to photoelectrons having different paths and their statistical fluctuation in number.
For this reason, and to discriminate against the lower pulse height distribution derived
from noise, the timing of photomultiplier signals is derived at a constant fraction
of the pulse height (constant fraction discrimination, CFD) [31]. Temporal delays
due to optical paths, the photomultiplier, cables, and electronics are inherent, but
have no bearing on the quality of measurement, provided they remain constant. A
time delay (cable or electronic) that can be varied between measurements to ensure
the decay profile is observable and can be moved within the TAC range is essential
though. Traditionally nuclear instrument modules (NIMs) were used for the timing
electronics, but these have by and large been superseded by customized PC cards or
stand-alone modules.
One limitation of the original (simplex) form of TCSPC as described so far is its
inefficiency. For the histogram of photon arrival times to reproduce the fluorescence
decay curve, the fluorescence detection rate has to be reduced to ∼1% of the source
repetition rate in order to avoid data pileup [30, 31]. Otherwise photons arriving
early at the peak of the distribution are preferentially recorded and the decay time
appears anomalously shortened. This needs a little thought when encountered for
the first time, although the 1% ratio also has a statistical basis in that it ensures
the probability of detecting two photons per excitation pulse is negligible. Ways of
correcting for this inefficiency have long been the focus of attention. Early approaches
included statistical analysis [44] and discrimination between one- and multiphoton
pulse signals using either photon pulse height or timing [45]. These methods were
generally unsatisfactory. In reality, as the repetition rate of sources increased into the
MHz range with the advent of mode-locked lasers and semiconductor sources, the 1%
count rate was no longer a limitation for cuvette studies and this is still the case today.
However, as the repetition rate of sources increases to the MHz region a sec-
ond inefficiency of TCSPC emerges; the dead time of the timing electronics when

18
FLUORESCENCE
processing a “stop” signal. This was typically ∼10 μs with traditional TCSPC using
a TAC and MCA, thus limiting count rate to ∼100 kHz. This is adequate for many
purposes, but much more restrictive than the 1% pile-up limitation when the source
repetition rate exceeds 10 MHz. The first step to minimizing the effect of dead time
is to simply reverse the “start–stop” roles of the source rate and fluorescence rate,
thus reducing the duty cycle of the timing electronics. For the study of transient
samples, such as in single-molecule microscopy studies, this is still not good enough.
Consequently, faster timing electronics have been developed with such applications
in mind that have also become of more general benefit to cuvette-based spectroscopy.
At least two orders of magnitude reduction in TCSPC dead time is now common
[46], with <10 ns recently reported [43]. Hence measurement times in the millisec-
ond region, with adequate statistics for recording single exponential decay times, are
now available.
Another more general limitation of the original simplex form of TCSPC is its
inadequacy for accessing much of the multidimensional contour of fluorescence (i.e.,
intensity, time, polarization, spectra, position) other than through sequential measure-
ments, which can be onerous. For example, time-resolved fluorescence spectra are
traditionally measured by sequentially recording the fluorescence decay at different
emission wavelengths and then assembling intensity slices at different time delays or
recording the whole emission spectrum sequentially at different time delays during
the fluorescence decay [3]. The introduction of multiplexed TCSPC [47–49], whereby
multiple decay curves are acquired simultaneously, started to address this and con-
tinues to be developed using multiple detection channels and timing. Multiplexing
techniques are of particular benefit to single-molecule fluorescence microscopy and
lifetime imaging [50, 51]. The trend toward miniaturization through lab-on-a-chip
developments [52, 53] has facilitated an increase in the number of measurement
channels and out-of-the-lab applications [54].
The requirements for data analysis in TCSPC are complicated by the finite
duration of the temporal response of the fluorometer. Even if the excitation pulse
is effectively a 𝛿-function (i.e., ≪𝜏f), the measured fluorescence response will not
be identical to the sample’s impulse response (e.g., that given by Eq. 1.8). This
is largely because of the intrinsic jitter in photoelectron mean transit time in the
photomultiplier, to a lesser extent contributed to by the differences in wavelength and
optical paths between measuring the excitation and fluorescence. Jitter in the timing
electronics, such as the CFD, also contributes, with all these time spreads contributing
to the overall instrumental response half-width, approximately as the sum of their
squares [31].
The solution is to use convolution analysis, whereby the excitation pulse is con-
sidered to be composed of a series of 𝛿-functions and the fluorescence decay a linear
superposition of the corresponding fluorescence impulse responses [31] i.e.
F(t) =
t
∫
0
P(t′)i(t −t′)dt′ = P(t) ⊗i (t) ,
(1.16)

LIFETIME
19
where F(t) is the expected fluorescence decay, which is broader than the fluorescence
impulse response i(t −t′) because of the effect of the finite instrumental response
or prompt P(t′). The variable t′ is a moving time delay that defines the instant at
which each 𝛿-function component of the instrumental pulse generates the start of
a fluorescence response, the whole procedure often denoted as ⊗to denote the
convolution. P(t′) is usually measured by tuning the fluorescence monochromator
to the excitation wavelength and replacing the fluorescent sample with a colloid
(LUDOX silica [55] is often used) that simulates the isotropic nature of fluorescence
by diffuse scattering of the excitation pulse.
Using Eq. (1.16) to analyze fluorescence decay requires numerical analysis using
reconvolution rather than deconvolution. That is to say i(t −t′) in Eq. (1.16) is iterated
to give 𝜏f from Eq. (1.8) when F(t) best describes the measured decay data Dt, as
recorded at channels corresponding to t. Usually nonlinear least squares (NLLS) is
the fitting procedure used with a 𝜒2 goodness-of-fit criterion conveniently providing
a single figure of merit for ease of comparison [56]:
𝜒2 =
∑
Data
channels
[
Dt −F(t)
√
Dt
]2
,
(1.17)
where Dt −F(t) is the actual deviation (residual) at each datum (squared to allow
for the deviations being both positive and negative),
√
Dt the expected standard
deviation, since the statistics are Poisson and for a good fit 𝜒2∕(n −𝜈) ≃1 for n data
channels, and 𝜈the number of parameters to be fitted (decay constants, amplitudes,
etc.). In order to interpolate between the temporal widths of the data channels and to
provide a first-order correction to the wavelength dependence of the photomultiplier,
an additional parameter that shifts F(t) with respect to Dt is usually included.
Other statistical methods are available to further assess the goodness of fit [56].
The weighted residuals, Wt (Eq. 1.18), are also often presented as their distribution
underpins 𝜒2; random rather than systematic deviations being taken to indicate a
good fit to the data:
Wt = Dt −F(t)
√
Dt
.
(1.18)
Figure 1.9 illustrates the foregoing discussion by showing a typical fluorescence
decay that is of comparable duration to the excitation pulse and successfully analyzed
using reconvolution. Depending on their relative decay times and amplitudes up to
three exponential decay components is usually quite straight forward to extract using
reconvolution.
While NLLS reconvolution is still the workhorse for decay analysis in TCSPC,
other approaches are useful in special cases, for example, the maximum entropy
method [57], describing lifetime distributions, and maximum likelihood for handling
poor data statistics [58].

20
FLUORESCENCE
Decay data Dt = 
 fn responses
Exc. pulse  P(t) = 
 fns
Fitted fn.F(t)
Weighted
residuals Wt
1 exp
2 = 0.93
FIGURE 1.9
Reconvolution analysis of a fluorescence decay.
Standards for fluorescence lifetimes have been suggested from a Round-Robin
study by nine laboratories that compared results using both pulsed and phase flu-
orometry for dilute and degassed solutions [59]. Although the idea of chemical
standards of time is perhaps in some ways peculiar, they can be quite useful when ini-
tially testing fluorometers or in providing confidence in new measurements. Table 1.2
shows some of the average values of up to eight measurements using both TCSPC
and phase-modulation measurements taken from this Round-Robin study. Agreement
with the TCSPC and phase-modulation results was within a few percent.
1.4.3
Application Example—In Vivo Glucose Sensing
One of the ongoing challenges in biosensing is metabolic monitoring through, and
in, the complex barrier of skin and tissue. Monitoring blood glucose for diabetes
control is the archetypical measurement problem [60], with the concentration range
of physiological interest ∼5–30 mM. At present diabetics rely largely on finger-prick
blood sampling applied to a reagent strip for glucose analysis, but this has poor patient
compliance and is intermittent. Continuous glucose monitors are available that are
TABLE 1.2
Some suggested fluorescence lifetime (𝜏f) standards
Dye and solvent
Excitation (nm)
Fluorescence (nm)
𝜏f (ns)
Rubrene in methanol
300–514
550–610
9.90 ± 0.30
9,10-Diphenyl anthracene in methanol
295–360
400–475
8.70 ± 0.50
Anthacene in methanol
295–360
375–442
5.10 ± 0.30
2,5-Diphenyloxazole in methanol
295–330
340–400
1.65 ± 0.05
Erythrosin B in methanol
488–568
550–580
0.47 ± 0.02
Source: Adapted from Reference 59 with permission from the American Chemical Society.
Measurements were performed on deoxygenated solutions at 20◦C and represent the average of up to eight
measurements across different laboratories using both pulsed and phase fluorometry.

LIFETIME
21
based on subcutaneously implanted amperometric enzyme electrodes or microdialysis
sensing platforms. Although undoubtedly of benefit and improving, they still have
drawbacks that include suboptimal accuracy, drift, and frequent calibration [61]. So
far the search for a minimally invasive sensor that can reliably operate continuously
has defied advances in modern technology. Many alternative technologies are being
investigated, including near-IR, Raman, impedance and photoacoustic spectroscopy,
as well as optical coherence tomography, and polarimetry, but none has yet reached
routine clinical application [62].
Fluorescence also shows significant promise in this area. It would be ideal if
the intrinsic fluorescence of tissue could be used, for example, glucose-induced
cellular metabolism of NAD(P)H [63], but the photo-protective action of endogenous
absorbers such as melanin makes this impractical. Extrinsic fluorescence platforms
include those based on boronic acid [64], concanavalin A [65], carbon nanotubes
[66] and enzymes, such as hexokinase [67], and glucose/galactose binding protein
[68]. Each of these assays has strength and weakness, but ultimately a workable
assay will need to operate in the red/near-IR therapeutic spectral window in order
to minimize absorption and scattering by endogenous species. However, some such
sensing schemes have been reported [66,69,70].
The advantages of fluorescence lifetime over intensity measurements come to the
fore in applications like glucose sensing. Fluorescence lifetimes are essentially inde-
pendent of fluorophore concentration, photobleaching, and ground-state quenching,
they can discriminate against scattered excitation light and auto-fluorescence, and
are easy to calibrate absolutely. Moreover, recent developments in semiconductor
photonics and on-chip integration, some are already mentioned, combined with fiber
optics, mean that ergonomic and miniaturized instrument systems, using either pulse
or phase fluorometry, are well within reach for glucose sensing.
By way of an example, here we describe a reversible fluorescence lifetime plat-
form based on glucose/galactose-binding protein (GBP) from Escherichia coli. GBP
has a single polypeptide chain that folds into two lobular domains, separated by a
hinge region where glucose binds (Fig. 1.10). Binding is accompanied by a marked
conformational change in the protein, with the two lobules closing round the glu-
cose molecule to form a “closed-form” of GBP. When appropriately labeled, the
glucose-induced conformational change in GBP can produce a significant change
in fluorescence [71]. In Figure 1.10 the environmentally sensitive probe badan (6-
bromoacetyl-2-dimethylaminonaphthalene) is attached to position H152C (i.e., a
mutant where histidine is substituted by cysteine at position 152 of the polypep-
tide chain near the glucose-binding site). The addition of glucose results in a 300%
increase in fluorescence intensity [71]. In the presence of glucose the fluorescence
decay can be described well by an impulse response comprising two time constants
of 𝜏f1 ∼1 ns and 𝜏f2 ∼3 ns. These components are associated with an equilib-
rium that depends on the glucose concentration between the open (and quenched)
glucose-free and closed (and enhanced) glucose-bound forms of GBP, respectively.
The fluorescence impulse response, with reference to Eq. (1.8), is
i (t) = i1exp −(t∕𝜏f1) + i2exp −(t∕𝜏f2).
(1.19)

22
FLUORESCENCE
Badan
Attached at
Cysteine 152
Glucose
FIGURE 1.10
Glucose/galactose-binding protein (GBP), genetically modified and labeled
with badan [71]. When glucose enters the cleft, the hinged lobes fold together, increasing the
fluorescence of the badan label.
Figure 1.11a shows the relative change in fluorescence fractions F1 and F2 as the
glucose concentration is varied in neutral buffer to include the 5–30 mM physiological
range. This is in terms of the weighted average <𝜏f> for the two decay components:
< 𝜏f >=
i1𝜏f1
i1𝜏f1 + i2𝜏f2
𝜏f1 +
i2𝜏f2
i1𝜏f1 + i2𝜏f2
𝜏f2 = F1𝜏f1 + F2𝜏f2.
(1.20)
(a)
(b)
F2
F1
Closed form
Open form
Glucose (μM)
Glucose (mM)
τ f2 ~ 3.1 ns
τf1 ~ 0.8 ns
Kd = 2.35 μM
Kd = 13 mM
1.0
0.5
Fraction
0.0
0
100
200
300
1
0
20
40
10
100
(%)
<  f> –  f1
  f1
τ
τ
τ
FIGURE 1.11
(a) Fractions of fluorescence due to open, F1, and closed, F2, forms of GBP-
badan with Kd = 2.35 μM in PBS buffer at pH 7.4. (b) Percentage change in average lifetime
<𝜏f>−𝜏f1
𝜏f1
(cf. Eq. 1.20) as a function of glucose concentration for a GBP mutant, immobilized on
agarose beads coupled to an optical fiber in PBS buffer at pH 7.4, with Kd of 13 mM designed
for the physiological range [72].

QUENCHING
23
Figure 1.11b was obtained for badan-GBP immobilized on agarose beads coupled
to an optical fiber in neutral buffer, an approach which could also be suitable for
implanting in subcutaneous tissue [72]. The dissociation constant Kd is 13 mM and
ideal for monitoring over the physiological range of glucose. The measurand is the
fractional increase in <𝜏f> on addition of glucose as compared to the fluorescence
lifetime at zero glucose 𝜏f1.
GBP labeled with acrylodan (a similar dye to badan) has recently been success-
fully demonstrated in preclinical trials on swine using a fiber optic insert in the
subcutaneous tissue and found to be free from significant effects of common interfer-
ents [73]. Badan-GBP has also been encapsulated on a micron scale in layer-by-layer
assemblies of poly-lysine and poly-glutamic acid and demonstrated in plasma, paving
the way to a minimally invasive implementation as a smart tattoo [74].
The underlying mechanism behind the large increase in badan-GBP fluorescence
when glucose binds, and GBP folds, is likely to be complex. Badan is well known
to be environmentally sensitive with a fluorescence spectrum that shifts with solvent
polarity; however, in this case the fluorescence spectrum shows little change. Some of
the more commonly encountered fluorescence quenching mechanisms are discussed
in the next section.
1.5
QUENCHING
A fluorescence quencher can be simply defined as any species, which, by inter-
acting with the fluorophore, reduces its quantum yield. There are many molecular
mechanisms of quenching, each lowering the proportion of the number of radiative
transitions in respect to the number of photons absorbed (Eq. 1.4).
Quenching studies are useful in gaining information on accessibility to the
quencher, determining location of fluorophores, diffusion parameters, and perme-
ability and structure of media and macromolecules. They have also led to a whole
range of sensors for detecting different analytes. The primary experimental tech-
niques for studying quenching have already been described, namely spectra (both
absorption and fluorescence), quantum yield, and lifetime. Again the latter is better
placed to reveal most about the quenching kinetics and bring to bear advantages in
sensing already mentioned for the case of glucose monitoring. In all fluorescence
quenching studies it needs to be kept in mind that a single quenching mechanism is
not always present and often more complicated hybrid kinetics are observed. Here
we summarize the most commonly encountered forms of fluorescence quenching.
1.5.1
Theory
1.5.1.1
Static Quenching
Perhaps the most trivial form of quenching is so-called
static quenching where the quencher molecules Q form ground-state complexes with
the fluorophore M (Fig. 1.12a). The MQ complexes can still be excited, but either do
not fluoresce or have a reduced fluorescence quantum yield.

24
FLUORESCENCE
(a)
δ(0)
δ(0)
δ(0)
[M*]
[M*]
[M*Q]
[M]
[M]
kq[Q]
[MQ]
+Q
–Q
(b)
k’nr
knr
knr
kr
kr
Q
Q*
FIGURE 1.12
(a) Schematics of the mechanism of static quenching. The MQ complex can
be excited, but is not fluorescent. As Q does not affect the M excited state, there is no change
in its fluorescence lifetime. (b) Dynamic quenching.
The association constant, Ks, controls the equilibrium between free and
fluorophore–quencher complexes:
Ks =
[MQ]
[M]q[Q] =
[M]0 −[M]q
[M]q[Q]
,
(1.21)
where [M]q, [Q], and [MQ] are the concentrations of the ground state molecular fluo-
rophore, quencher, and fluorophore–quencher complex, respectively, in the presence
of the quencher. [M]0 is the initial concentration of the fluorophore in the absence
of the quencher and [M]q the concentration of the fluorophore in the presence of the
quencher. Equation (1.21) can be easily converted into the well-known Stern–Volmer
equation [3]:
If
Ifq
= [M]0
[M]q
= 1 + Ks[Q],
(1.22)
where If and Ifq are the fluorescence intensities measured in the absence and presence
of the quencher, respectively. The absorption spectrum can reveal evidence for the
presence of static quenching as MQ will be likely to have a different spectrum to M.
However, because static quenching does not interfere with the excited-state kinetics,
but only reduces the concentration of the fluorescent dye, no change in fluorescence
lifetime is observed. In addition to well-known static quenchers, for example, mercury
ions, some newly developed materials, such as graphene [75] and carbon nanotubes
[76], have also been recently found to act as static quenchers for certain dyes.
1.5.1.2
Dynamic Quenching
Dynamic quenching occurs during the collision of
the electronically excited molecule M∗and the quenching molecule Q (see Fig. 1.12b).
During the collision, the excitation energy is transferred to the quencher and dissipated
as heat. Because the excited state kinetics is affected, the fluorescence lifetime in
addition to the yield is reduced.

QUENCHING
25
The kinetic equations for the concentration of the excited dye [M∗], following
𝛿-function excitation at t = 0, in the absence and in the presence of a quencher Q, are
given by Eq. (1.9) and Eq. (1.23), respectively.
[Q] > 0 ⇒d[M∗]
dt
= −(𝜏−1
f
+ kq[Q])[M∗],
(1.23)
where 𝜏f is the fluorescence lifetime in the absence of quencher and kq is the diffusion-
controlled quenching rate constant. In the steady state d[M∗]/dt = 0. These equations
yield the Stern–Volmer equation for dynamic quenching and fluorescence intensities
analogous to static quenching, but this time including the lifetime:
If
Ifq
=
𝜏−1
f
+ kq [Q]
𝜏−1
f
= 1 + 𝜏fkq [Q] .
(1.24)
The solution of Eq. (1.23) (equivalent to Eq. (1.8), but including quenching) is
[M∗] = [M∗]0 exp(−t/𝜏fq), where
1
𝜏fq
= 1
𝜏f
+ kq [Q] .
(1.25)
This indicates that the fluorescence decay of the dynamically quenched dye
remains exponential, but with a reduced lifetime 𝜏fq < 𝜏f. After simple rearrangement
of Eq. (1.25), we obtain the Stern–Volmer dependence for lifetimes:
𝜏f
𝜏fq
= 1 + 𝜏fkq [Q] .
(1.26)
The right-hand side of this equation is the same as in Eq. (1.24) with both steady-
state and time-resolved fluorescence yielding the Stern–Volmer constant KSV = 𝜏fkq
of dimension M−1.
Several compounds, often containing heavy atoms to facilitate intersystem cross-
ing to the triplet state, are well known as dynamic quenching agents when present
at sufficiently high concentration. This has led to a wide range of structural probes
and analyte sensors based on the Stern–Volmer equation. For example, oxygen, with
a triplet ground state and occurring at ∼10−3 M in aliphatic solvents, is a highly effi-
cient dynamic quencher of fluorescence (diffusion constant in water ∼10−5 cm2 s−1).
Oxygen sensors based on the long-lived excited states of transition metal complexes
have been extensively developed [77,78]. Other quenchers include chloride, bromide,
and iodide ions [79,80]; transition, noble, and heavy metals [81,82]; amines; carbon
tetrachloride; and acrylamide [83,84].
In some cases, collisional interactions can result in the formation of an excimer or
exciplex [2] which have their own fluorescence characteristics and add complexity to
the kinetics. Indeed carbon tetrahalides have been shown to exhibit hybrid quenching
composed of static, dynamic, and exciplex components [2].

26
FLUORESCENCE
In addition, kq is not always a constant, but has a
√
t time dependence that in
the simplest case can be derived from Smoluchowski’s solution [85] of the diffusion
equation
kq = 4𝜋RD
[
1 +
R
√
𝜋Dt
]
,
(1.27)
where D is the sum of the diffusion coefficients of the fluorophore and quencher and
R is the sum of their interaction radii.
This leads to a nonexponential impulse response given by [3]
i(t) = i0 exp
[
−t∕𝜏fq −2b (t∕𝜏fq
)1∕2]
,
(1.28)
where b = 4R2NA(𝜋D)
1
2 [Q]/1000 and NA = Avogadro’s number.
Such transient effects are exaggerated in higher viscosity solvents such as glycerol,
which reduces collisional quenching, but can be observed even in low viscosity
solvents such as ethanol at higher time resolution. It can be observed as a departure
from an exponential decay at short times in both TCSPC and phase fluorometry, with
and without excimer/exciplex formation [86–88].
1.5.1.3
F¨orster Resonance Energy Transfer
Direct collisional contact between
the dye and the quencher is not required in the third common mechanism of quenching
that we will consider. F¨orster resonance energy transfer (FRET) has become popular
in recent years as a “spectroscopic ruler” for measuring distances and studying the
structure and dynamics of macromolecular and microheterogeneous molecular sys-
tems [89]. Named after Theodor F¨orster, who discovered and described the quantum-
mechanical nature of this phenomenon in the late 1940s, the mathematical description
of FRET is obtained by applying perturbation methods to an excited dye–quencher
pair, which are denoted donor D and acceptor A (see Fig. 1.13). The interaction is
usually dipole–dipole in origin, and the spectral overlap (the “resonance”) between
the donor fluorescence and acceptor absorption is a prerequisite for FRET and this
gives it higher intrinsic specificity than collisional quenching.
F¨orster’s treatment gives the rate of D–A energy transfer kDA to be
kDA = 1
𝜏D
(R0
r
)6
,
(1.29)
where r is the D–A distance (both D and A are regarded as point dipoles and thus
assumed to have no spatial dimension) and 𝜏D is the fluorescence lifetime of the donor
in the absence of acceptor (𝜏f as defined previously). Since the kDA is proportional
to the inverse sixth power of the distance r, FRET can be used as an extremely
sensitive mechanism for determining D–A separations and thus measuring molecular
distances. The actual scale of this “molecular ruler” is determined by the parameter

QUENCHING
27
(a)
(b)
θA
θA
θD
μD
μA
θr
R
κ2=4
[A]
[D]
knr kr
kDA
[D*]
[A*]
κ2=1
κ2=0
δ(0)
FIGURE 1.13
(a) The mechanism of F¨orster resonance energy transfer. (b) The orientation
factor in FRET. 𝜇D and 𝜇A are the transition dipole moments of donor and acceptor molecules
and R is the separation vector.
R0, the F¨orster or critical transfer distance. According to F¨orster’s theory, R0 can be
calculated using the relevant spectroscopic properties of the donor and acceptor:
R6
0 = 9000 ln(10)𝜅2Φf
128𝜋5NAn4
∞
∫
0
ID (𝜆)𝜀A (𝜆) 𝜆4d𝜆,
(1.30)
where 𝜅2 is the orientational factor, Φf the quantum yield of donor fluorescence
(without acceptor), NA the Avogadro’s number, and n the refractive index of the
intervening medium. The integral in Eq. (1.30) represents the degree of spectral
overlap between the donor fluorescence spectrum ID(𝜆) and the acceptor absorption
spectrum 𝜀A(𝜆). Significant spectral overlap ensures a high critical transfer distance
R0 (typical values vary from ∼10 to 100 ˚A) and thus a high rate of FRET. Equa-
tion (1.29) allows the determination of D–A separations; this simple equation finds
numerous applications in the materials and life sciences. The other critical factor
affecting R0 is the orientational factor 𝜅2, defined as
𝜅2 = [cos 𝜃T −3 cos 𝜃D cos 𝜃A]2.
(1.31)
The angles are defined in Figure 1.13b, where some limiting situations have also
been depicted for 0 < 𝜅2 < 4.
For fast and freely rotating donor and acceptor molecules, 𝜅2 can be averaged over
time and equals 0.67 and this assumption often has to be made even when not strictly
correct. If the mutual orientation of D and A is not fully random, the orientational
effects may significantly affect FRET, which can be explored to extract extra structural
information. The orientational dependence of FRET has been discussed in numerous
studies [90] and recently conclusively demonstrated using labeled nucleic acids [91].

28
FLUORESCENCE
The presence of FRET is manifest in the fluorescence spectrum of the sample
by a decrease in the intensity of the donor spectrum and increase in the acceptor
spectrum (if the acceptor is fluorescent). FRET not only reduces the intensity of
donor fluorescence but also affects its fluorescence decay. In the simplest case, if the
original donor decay is a single exponential with the lifetime 𝜏D and D and A are
separated at the fixed distance rDA, then the fluorescence impulse response is
i(t) = i0 exp
[
−
(
1 +
( R0
rDA
)6)
t
𝜏D
]
.
(1.32)
Equation (1.32) shows that the decay remains a single exponential, but with a
reduced lifetime that allows determination of the D–A distance.
In the presence of acceptors distributed around the donors according to the D–A
distribution function 𝜌(r), the fluorescence impulse response is modified to
i(t) = i0 exp[−t∕𝜏D −
∞
∫
0
(1 −tkDA (r))𝜌(r) dr] .
(1.33)
Equation (1.33) shows the potential of FRET for not only determining distances but
also studying the morphology of a medium on the sub-nanometer scale. Unfortunately,
although initially promising, without making a priori assumptions, interpretation of
FRET in amorphous materials such as silica was found to be limited to fractals and
simple model geometries [92,93] and not give the detailed morphology sought. The
underlying problem is the ill-posed nature of Eq. (1.33), such that a unique solution for
the distribution function 𝜌(r) does not usually exist. The usual approach is to assume
a hypothetical formula for 𝜌(r) and then try to determine the “best-fit” parameters.
Fortunately Eq. (1.33) can be solved for the most commonly encountered special
case, namely that of a random distribution of acceptors 𝜌(r) ∼r2, leading to the
well-known F¨orster decay function
i(t) = i0 exp[−t∕𝜏D −𝛾(t∕𝜏D
)1∕2] ,
(1.34)
where 𝛾= [A]∕CA, and
CA =
3
4𝜋3∕2R3
0NA
,
(1.35)
where [A] is the actual acceptor concentration and CA the critical molar acceptor
concentration.
Equation (1.34) can be implemented in different ways. For example R0 can be
determined from Eq. (1.30) to give CA using Eq. (1.35). Then by reconvoluting
Eq. (1.34) in order to fit to the experimental fluorescence decay, the local concentration
of the acceptor in the donor environment can be found. In microheterogeneous

QUENCHING
29
systems, this may well not be the same as the bulk concentration, thus indicating
structure, accessibility, or other information from the donor–acceptor proximity. The
measurement is effectively the same when FRET is used as a sensor platform for
determining the presence and concentration of analytes of interest and we illustrate
this in the next section.
1.5.2
Application—Metal Ion Quenching
Metal ion detection has widespread relevance to many important areas that include
human metabolism, neurology, and the environment. By way of an example
of fluorescence-based sensing, we consider here the quenching by copper ions
in different media of the red/near-IR fluorescent dye carbocyanine dye, 3,3-
diethylthiadicarbocyanine iodide (DTDCI). Hydrated copper ions (Cu2+⋅5H2O) can
quench both collisionally and by FRET via an overlapping broad band absorption
that peaks around 800 nm.
Figure 1.14 presents the result of fitting Eq. (1.34) to the fluorescence decay for
DTDCI quenched by hydrated copper ions in propylene glycol (a and b), and in water
pools (c and d) of a hydrated perfluorosulfonate ion exchange membrane (Nafion,
Dupont Corp.) [94]. Plots a and b show a “pure” FRET example that is described by
Eq. (1.34), that is, a constant value of the donor lifetime 𝜏D and 𝛾increasing linearly
with copper ion concentration. Plots c and d show an example of a not infrequent
case of hybrid kinetics where FRET is accompanied by collisional quenching, which
leads to a decrease of the donor lifetime with increasing quencher concentration.
0.5
1
1.5
[Cu2+](mM)
[Cu2+](mM)
[Cu2+](mM)
0
50
40
30
20
10
0
0.4
0.6
0.8
0.5
1
1.5
2
[Cu2+](mM)
0
50
40
30
20
10
0
γ
γ
0
0.2
2
3
4
D/ns
0
1
50
40
30
20
10
0
60
40
20
0
(a)
(b)
(c)
(d)
τ
D/ns
τ
FIGURE 1.14
Donor lifetimes 𝜏D and 𝛾values for the DTDCI→Cu2+ quenching in propy-
lene glycol (a and b), and in Nafion (c and d) [94].

30
FLUORESCENCE
Notwithstanding the homogeneous and heterogeneous differences in the two
media, the dominant difference between the two cases is the much lower diffusion
coefficient (∼10−7 cm2 s−1) in propylene glycol. Such behavior has been observed in
numerous other combinations, for example, Cu2+ sensing down to 10 ppb in Nafion
using rhodamine 800 as a donor [95]. Models of increasing complexity have been pro-
posed for handling such hybrid kinetics, starting with essentially linear combinations
of collisional and FRET terms [96].
Two of the advantages of using hydrated transition metal ions as acceptors are
their good approximation to the point dipole demanded by F¨orster’s theory and
minimal disturbance of native structure. These have been recently put to good effect
when measuring distances in model 𝛼-helical peptides [97,98]. However, transition
metal ion acceptors have the disadvantage of a relatively short R0 value (∼1–2 nm)
caused by the weak d–d transitions as compared to the 𝜋–𝜋∗transitions of aromatic
acceptors. This problem can be overcome in sensing by using a chelating molecule,
for example, bathocuproine used for Cu(I) [99]. For FRET sensing of the colorless
metal ion zinc, a fluorescently labeled enzyme, carbonic anhydrase II, has been
successfully used [100, 101]. Quantum dots are emerging as a more robust donor
than aromatic dyes for sensing metal ions, although their decay time kinetics can also
be complex [102]. Although comparatively large in size (∼2–6 nm) [103], they are
still a useful alternative to conventional dyes when bound to protein for FRET studies
[104]. Similarly green fluorescent protein [105] and its subsequent other visible forms
are finding increasing use with FRET [106].
1.6
ANISOTROPY
Fluorescence anisotropy describes the polarization of fluorescence and along with
quenching, is one of the most important methods in fluorescence spectroscopy, partic-
ularly when time-resolved using similar techniques to those already described for the
measurement of fluorescence lifetimes. Just as fluorescence quenching is influenced
by translational diffusion or proximity for FRET to occur, fluorescence anisotropy
is influenced by rotational diffusion and energy migration. The importance of FRET
and anisotropy can be judged from their sub-nanometer spatial resolution, which is
over an order of magnitude better than any techniques using an optical microscope.
This has led to their application as a tool for molecular nanometrology.
1.6.1
Theory
The basis of fluorescence anisotropy techniques is to use polarized excitation to create
a spatially selected, non-random, distribution of excited fluorescent molecules which
then randomize, most commonly by Brownian rotation, but also at times by energy
migration depending on the system. The extent by which the fluorescence emission is
depolarized depends on the rotation of the fluorophore (or extent of energy migration
or both), which in turn brings with it the measurement of parameters that influence
molecular rotation. Figure 1.15 shows a typical geometrical arrangement with respect

ANISOTROPY
31
Measurement geometry
z
Random molecule orientation
IVV(t)
x
y
Polarizer
Exc.
IVH(t)
Fl.
Anisotropy introduced via
excited absorption dipole
Anisotropy destruction by
Brownian rotation of emission
dipole tracked by fluorescence
Abs. probability ~ cos2α
FIGURE 1.15
Measurement geometry for anisotropy studies, selective excitation using
polarized light and depolarized fluorescence emission due to Brownian rotation. The probability
of excitation goes as cos2𝛼, where 𝛼is the angle between the plane of polarization and the
absorption dipole.
to the spectrometers shown in Figures 1.2 and 1.8 and the photoselection in a random
distribution of molecules that is designed to be produced. Both steady-state and time-
resolved anisotropy can be performed using the configuration shown in Figure 1.15.
The theory of fluorescence anisotropy has developed over many years from the
work of Perrin, Jablonski, Weber, Wahl, and others [3] and has been frequently
reviewed [107,108].
Both steady-state and time-resolved anisotropy can be measured using the con-
figuration of Figure 1.15. By recording vertically (V) and horizontally (H) polarized
fluorescence decay curves, IVV(t) and IVH(t), orthogonal to vertically (V) polarized
excitation, a time-resolved anisotropy function r(t) can be generated that is much more
revealing (particularly if heterogeneous depolarization mechanisms are present) than
the steady-state anisotropy, that provides just a time average, as it neglects the time
dependence, that is,
r(t) = IVV(t) −GIVH(t)
IVV(t) + 2GIVH(t),
(1.36)
where G is a factor (G = IHV(t)/IHH(t)) determined for horizontal orientation of the
excitation polarizer in order to correct for differences in the polarization transmissions
of the fluorescence detection channel at V and H, which are largely due to the emission
monochromator. The factor of 2 in the denominator arises because there is a plane
in the direction of the excitation in which depolarization of fluorescence is not
detected, but which is identical to that detected in the emission channel. Because

32
FLUORESCENCE
the denominator in Eq. (1.36) describes all three planes of polarization, it actually
also describes the fluorescence decay undistorted by any polarization bias. This can
be seen from inserting Eqs. (1.37) and (1.38) below into Eq. (1.36) for the simplest
depolarization case where a fluorophore can be treated as a spherical rigid rotor
undergoing Brownian isotropic rotation in a medium such as a solvent for
IVV(t) = exp(−t∕𝜏f)[1 + 2r0 exp(−t∕𝜏c)],
(1.37)
IVH(t) = exp(−t∕𝜏f)[1 −r0 exp(−t∕𝜏c)],
(1.38)
whereby we obtain
r(t) = r0 exp
(
−t
𝜏c
)
,
(1.39)
where r0 is the initial anisotropy at t = 0, which has a maximum value of 0.4 for one
photon excitation, and 𝜏c the rotational correlation time, which describes the rate of
depolarization due to isotropic rotation.
In this case, 𝜏c can be described by the Stokes–Einstein equation:
𝜏c = 𝜂V
kT = 1
6D,
(1.40)
where 𝜂is the microviscosity, V the hydrodynamic volume = 4𝜋R3/3 prescribed by
the rotor of hydrodynamic radius R, T the temperature, k the Boltzmann constant,
and D the rotational diffusion coefficient.
The denominator in Eq. (1.36) can be useful in determining 𝜏f free from errors
due to polarization effects when 𝜏f is comparable to the rotational correlation time
𝜏c, such as might occur for aromatic fluorophores in viscous solvents. An alternative
adaptation of Figure 1.15 achieves the same by having vertical excitation polarization
and the emission polarizer at the so-called magic angle of 54.8◦to the vertical. The
same aim can also be achieved at other specific polarizer orientations [30].
In the case where a fluorophore is partitioned between two different environ-
ments characterized by 𝜏c1 and 𝜏c2 or has a bimodal anisotropic rotation [109], then
providing 𝜏f is the same for each rotational component:
r(t) = (1 −f)r0 exp
(
−t
𝜏c1
)
+ fr0 exp
(
−t
𝜏c2
)
,
(1.41)
where f defines the fraction of fluorescence associated with each decay component.
Equation (1.41) finds wide-ranging application, including that of nanoparticle
metrology in the example given in Section 1.6.3 [110].

ANISOTROPY
33
If the fluorophore undergoes restricted motion such as in a lipid bilayer [111], or
wobbles when tethered to a slowly rotating macromolecule such as a protein [107],
then Eq. (1.41) is modified for these special cases, since 𝜏c2 ≫𝜏c1, to give
r(t) = (1 −f)r0 exp
(
−t
𝜏c1
)
+ fr0 ,
(1.42)
fr0 is sometimes referred to as the residual anisotropy r∞and in the case of lipid bilayer
membranes leads to the determination of an order parameter S for the membrane from
[111]
S =
√r∞
r0
.
(1.43)
Although often an approximation sometimes Eqs. (1.41) and (1.42) can be com-
bined to useful effect to give
r(t) = (1 −f −g)r0 exp
(
−t
𝜏c1
)
+ fr0 exp
(
−t
𝜏c2
)
+ gr0
(1.44)
For example, Eq. (1.44) is useful where unbound dye coexists alongside dye
wobbling when bound to a slowly rotating nanoparticle.
1.6.2
Experimental
The experimental methodologies for steady-state and time-resolved anisotropy mea-
surements are achieved by combining the techniques described in Sections 1.2 and
1.4, respectively, with the polarization orientations shown in Figure 1.15. One extra
point of care is to ensure that in determining fluorescence anisotropy from Eq. (1.36)
any fluctuations in the source intensity are corrected. This is usually achieved by
averaging out any drift by alternating between the measurement of IVV(t) and IVH(t),
and sometimes by the use of a T-format, which offers continuous correction [48].
Although there are different approaches to the analysis of anisotropy decay data,
the usual method is to use NLLS in an analogous manner to that described in Section
1.4.2 for fluorescence lifetime analysis. Rearranging the anisotropy function given
by Eq. (1.36), we obtain
dt = r(t)[IVV(t) + 2GIVH(t)] = IVV(t) −GIVH(t).
(1.45)
The function within square brackets in Eq. (1.45) is the fluorescence decay, which
we can fit to separately using as many decay parameters as required to give a good
fit in order to obtain the fluorescence impulse response i(t) and then keep it fixed,
that is, with reference to Eq. (1.19) i1, 𝜏f1, i2, 𝜏f2, and so on. The function on the
right-hand side of Eq. (1.45) contains the difference data dt that carries the rotational
information and which we can iteratively fit to using NLLS reconvolution of P(t) with

34
FLUORESCENCE
anisotropy impulse response functions r(t) that are intuitively thought to be applicable
(e.g., Eq. 1.39, 1.41, or 1.44). Then, we obtain r0, 𝜏c1, and 𝜏c2, again using 𝜒2 as the
goodness-of-fit criterion, that is,
𝜒2 =
∑
Data
channels
[
dt −P(t) ⊗r(t)i(t)
√
dt
]2
(1.46)
Since the rotation is tracked by the fluorescence decay, ideally 𝜏c should be ∼𝜏f.
In comparison to fluorescence quantum yield and lifetime, there are not really
any established standards for rotational correlation time. The rotation of rhodamine
6G has perhaps been studied the most and is probably the nearest to a standard.
Some rotational measurements on rhodamine 6G are listed in Table 1.3. The effect of
solvent in sticking to rotating fluorophores needs to be kept in mind when considering
fluorescence anisotropy [108].
The advantages of using rhodamine 6G as a standard are that it is an isotropic
rotor, has a monoexponential fluorescence decay (𝜏f ∼4 ns [115, 116]) and a high
fluorescence quantum yield (0.91 in ethanol [10]), is commercially available in high
purity, and is photo- and chemically stable over a wide range of pH (<2 to >12). Other
xanthene dyes, for example, rhodamine 101, have also been considered as rotational
standards, and a useful IUPAC Technical Report [117] recently listed rotational data
on a range of fluorophores, including rhodamine 6G and rhodamine 101, and made
recommendations for checking the accuracy of measurements. It should be noted
however that r0 for rhodamine 6G decreases quite rapidly when excited below its 0-0
transition, which is ∼515 nm [117].
TABLE 1.3
Some reported rotational parameters for rhodamine 6G using one-photon
excitation and a range of techniques
Hydrodynamic
radius R ( ˚A)
Temperature
(◦C)
Excitation
(nm)
𝜏c (ps) in
ethanol
𝜏c (ps) in
methanol
𝜏c (ns) in
propylene
glycol
6.0 ± 0.03 [112]
5.4 [113]
Not
specified.
Probably
room
temperature
530
175 ± 13
91 ± 8
5.4 [114]
21
580
195 ± 5
95 ± 3
5.6 ± 0.1 [115]
5.3 ± 0.3 [116]
27
400
6.16 ± 1.10
[117]

ANISOTROPY
35
R ~ 6 nm
R ~ 6 Å
c1 ~ 100 ps
τ
c2 ~ 100 ns
τ
FIGURE 1.16
Depiction of the coexistence of dual anisotropy kinetics for the case of a
fluorophore bound to a much larger nanoparticle and also unbound, both entities rotating freely
under Brownian motion in solution. Since from Eq. (1.40) 𝜏c ∝R3, three orders of magnitude
difference in rotational correlation time is observed for only one order of magnitude difference
in radius. This helps reduce error propagation when measuring the radius. For the same reason,
the dimensions of the fluorophore usually have negligible effect.
1.6.3
Application Example—Nanoparticle Metrology
Fluorescence anisotropy using fluorescence probes has undoubtedly been most widely
used in biomolecular research for the study of structure and dynamics in protein [107]
and membranes [118]. Here, we choose a different and in principle simpler example
that concerns the measurement of nanoparticle size, and which uses the theory of
multiple anisotropy decay components already described.
There has been a recent upsurge in interest in nanoparticles, concerning research
not only into their new forms such as semiconductors and gold, but also into their
older forms such as carbon and silica. Part of the interest lies in concern over the
toxicology and environmental aspects of all types of nanoparticles, particularly those
in the 1–10 nm range, which can easily traverse cellular membranes. Measuring such
small sizes in in situ presents a significant challenge to metrology, though one in
which fluorescence anisotropy decay is ideally suited.
The situation depicted by Figure 1.16 of a fluorophore bound to a much larger
nanoparticle, and also unbound, can be realized using either electrostatic or cova-
lent binding. Figure 1.17 illustrates a real example [119]. In Figure 1.17a, the non-
binding of anionic fluorescein to the anionic nanoparticles (rotational correlation time
∼200 ns) of LUDOX [55] silica AM30 is reflected with little change in the anisotropy
decay when the dye is added to the colloid. Figure 1.17b shows how the cationic rho-
damine 6G binds to the nanoparticles, but has too fast a fluorescence lifetime (∼4 ns)
to effectively track the much slower depolarization as the nanoparticles rotate and is
therefore still unsuitable for metrology purposes. Figure 1.17c shows that the bind-
ing and longer fluorescence lifetime (∼25 ns) of the cationic 6-methoxyquinolinium

36
FLUORESCENCE
0.4
0.3
0.2
0.1
0.0
0
5
10
Time (ns)
Time (ns)
Anisotropy
Anisotropy
Time (ns)
15
20
0.4
(a)
(c)
(b)
0.3
0.2
0.1
0.0
0.4
0.3
0.2
0.1
0.0
0
0
50
100
150
5
10
15
20
FIGURE 1.17
Effect on the fluorescence anisotropy decay (black data points) of adding dif-
ferent fluorophores to LUDOX [55] AM30 at pH 8.9: (a) Fluorescein, (b) rhodamine 6G,
(c) 6-methoxyquinolinium (structures also shown). For comparison, the gray data points
describe the dye rotation in methanol without LUDOX [119].
fluorophore are good enough to track the rotation, revealing the particle rotation and
enabling the measurement of its radius.
The simplest case is when all the dye binds to the nanoparticle and then Eqs. (1.39)
and (1.40) can be used. However, in this case, the microviscosity has to be already
known or found from a separate anisotropy measurement or other means. If the
anisotropy can be described by the presence of fluorophores that are both free and also
bound to a nanoparticle, then a single anisotropy decay, analyzed using Eqs. (1.40) and
(1.41), enables both the microviscosity of the medium and the nanoparticle radius to
be conveniently determined in a single measurement [110,116]. However, things are
not always so straight forward. Other effects, which may be present, and can influence
the measured r(t), include nanaoparticles aggregating and fluorophores wobbling on
the nanoparticle, diffusing on the surface of the nanoparticle, aggregating on or off the
nanoparticle, undergoing homo-FRET, and having a different fluorescence lifetime
on and off the nanoparticle [119, 120]. Fortunately, some of these, though not all
of these together, can usually be handled in the data analysis and, moreover, the
sample can be designed at the outset to minimize such effects. Note also that having a
complex multiexponential fluorescence decay tracking a nanoparticle rotation is not
in itself any handicap whatsoever as long it is taken into account in the data analysis
in Eq. (1.45).
One important difference between measuring anisotropy decay and fluorescence
decay is the higher statistical precision (number of counts) needed in IVV(t) and

ANISOTROPY
37
TABLE 1.4
Rotational parameters for LUDOX silica colloids labeled with a
6-methoxyquinolinium dye [120] and obtained by fitting the anisotropy decay
to Eq. (1.44)
LUDOX
𝜏c1 (ns)
𝜏c2 (ns)
gr0
r0
𝜒2
Peak in dt
Rm (nm)
Rfa (nm)
SM30
24
65 ± 15
0.12
0.24
1.26
1 × 105
3.5
4.0 ± 0.4
AM30
17
273 ± 60
0.06
0.28
1.23
5 × 105
6.0
6.4 ± 0.5
AS40
10
1400 ± 590
0.07
0.28
1.27
1 × 106
12
11.0 ± 1.6
dt is the peak count in the difference curve in Eq. (1.45). The data channel width was 28 ps. Rm is the
manufacturers quoted value for the nanoparticle radius, most probably obtained using electron microscopy
on the dry colloid. Rfa is the radius obtained from the 𝜏c2 component of the fluorescence anisotropy decay.
gr0 is attributed to colloidal nanoparticle aggregates. 𝜏c1 is too long to be free dye rotation, and may reflect
dye wobbling and/or diffusing on the surface of the nanoparticle.
IVH(t) before a meaningful comparison of rotational models can be made. This is
because it is not the absolute value, but the difference of IVV(t) and IVH(t) that is
used in the anisotropy function (Eq. 1.36). The anisotropy measurements shown in
Figure 1.17 were obtained using TCSPC with ∼10,000 counts in the peak channel
of the difference curve [119] and this is adequate for many purposes. However, if a
more complex model, such as that of Eq. (1.44) is to be deployed then much higher
statistical precision is required. This is illustrated in Table 1.4 for a range of LUDOX
colloids [120].
The nanoparticle radii obtained using fluorescence anisotropy decay and those
from the manufacturer’s data sheet [55] are in good agreement. It is also worth noting
that in the case of AS40 𝜏c2/𝜏f ∼56, which is way outside usual practice and only
possible because of the large number of counts obtained in dt.
This need for increasingly high statistical precision in the face of correlations
between the fitted parameters, when more than one rotational correlation time is
present, is the reason why the determination of rotational correlation time distribu-
tions (which would lead to useful nanoparticle size distributions in this example) is
extremely difficult using fluorescence anisotropy decay. The problem is analogous
to the difficulty of determining distance distributions in FRET in so far as unique
solutions do not in general exist.
One way of improving the measurement capability is to increase the dynamic
range of r0 above its maximum of 0.4 by the use of multiphoton excitation, though
this is at the expense of the simplicity of measurement as it requires femtosecond
laser excitation [116]. Multiphoton excitation enforces a higher angular alignment of
the absorption dipole with the plane of polarization of excitation with a probability
cos2j𝛼(Fig. 1.15), where j is the number of photons absorbed. For two- and three-
photon excitation, r0 increases from 0.4 for one-photon excitation to a maximum of
0.57 and 0.67, respectively, where 𝛽, the angle between the absorption and emission
dipoles, is taken to be zero [116] as given by
r0n =
2j
2j + 3
[3
2cos2𝛽−1
2
]
.
(1.47)

38
FLUORESCENCE
Given the paucity of standards for rotational time measurements, Table 1.4 sug-
gests that well-defined silica colloids also have something to offer in this capacity.
Equally there is a recognized lack of international standards for nanoparticle metrol-
ogy in general. When compared with traditional methods for nanoparticle metrology
such as small-angle neutron, X-ray, or light scattering and electron microscopy,
fluorescence anisotropy decay offers an inexpensive alternative that enables mea-
surements in situ, with sub-nanometer resolution, in the important 1–10 nm range.
Fluorescence anisotropy decay can be used to measure the size of other types
of nanoparticles apart from silica, provided they can be labeled extrinsically with
a fluorophore. Nanoparticle intrinsic fluorescence is not necessarily helpful because
internal depolarization mechanisms, as in the case of carbon soot nanoparticles [121],
can distort the measurement.
So far we have considered only the situation where the fluorescence lifetime
associated with each rotational correlation time is the same. This is not always
the case and such a change can lead to some peculiar anisotropy decay curves. For
example, where a fluorescence lifetime associated with a longer rotational correlation
time is longer than that of a faster fluorescence decay time associated with a faster
rotational correlation time, that is, 𝜏f1 < 𝜏f2 and 𝜏c1 < 𝜏c2. In this case, r(t) initially
falls with time, goes through a dip, and then rises again as the longer lived fluorescence
and rotational components start to dominate. Such effects have been observed and
analyzed in protein and lipid bilayer membranes [122], often reflecting structural
heterogeneity, but also with silica nanoparticles for free and bound dye [123]. This
behavior can be successfully modeled by reworking Eqs. (1.37) and (1.38) to take
account of the different fluorescence lifetime and then reformulating r(t).
Finally, in addition to the examples already mentioned, both steady-state and
time-resolved fluorescence anisotropy continue to find multifarious applications.
These include metal ion sensing [124], drug compliance monitoring [125], beta-
amyloid aggregation [126], surface modification [127], and pH meter measurement
of nanoparticle growth during silica hydrogel production [128].
1.7
MICROSCOPY
Fluorescence microscopy is a noninvasive, nondestructive technique, capable of imag-
ing at levels from a single molecule, cell, tissue, to human. No other method can
interrogate molecules in living cells with anything remotely approaching fluorescence
microscopy’s combination of spatial resolution, sensitivity, selectivity, and dynami-
cal insight. The capability to visualize biomolecules, by virtue of either intrinsic or
extrinsic fluorescence, enables the study of elementary biochemical reactions in cells
and, not surprisingly, this has become a prominent technique in biological research.
Moreover, it is fair to say that fluorescence microscopy has enjoyed somewhat of
a renaissance in recent years with a growth paralleled across fluorescence only by
lifetime measurements. Much of this growth has been fueled by the translation of
spectroscopy, molecular photophysics, and laser photonics as we will illustrate here.

MICROSCOPY
39
Near-field source
of illumination
(a)
(b)
(c)
Objective
Sample
Objective
Sample
Objective
Sample
Filter
Confocal
apertures
Dichroic
beamsplitter
Filter
Dichroic
beamsplitter
Filter
Detector
Detector
Detector
FIGURE 1.18
Schematics of some common fluorescence microscope configurations:
(a) wide-field; (b) confocal; and (c) near-field.
1.7.1
Systems and Techniques
Figure 1.18 illustrates a schematic of some common fluorescence microscope config-
urations. Figure1.18a is representative of the most common configuration, a conven-
tional epifluorescence (wide-field) microscope, which is traditionally operated with
only incoherent optical excitation, such as from a high pressure mercury lamp fitted
with a wavelength filter. The subsequent development of the confocal microscope and
laser-based techniques greatly improved the spatial resolution and imaging capability
of fluorescence microscopes. In a confocal microscope, as shown in Figure 1.18b,
optical sections are illuminated through a pinhole aperture, and the fluorescence sig-
nal is channeled via another pinhole near the detector. The image is then compiled
from laser scans of the surface. By means of the pinholes, the confocal microscope is
able to discriminate against out-of-focus fluorescence and display-enhanced contrast
in comparison with wide-field optical microscopes, allowing the reconstruction of
three-dimensional (3D) structures from the images obtained. The development of 3D
microscopy gained extra impetus with the introduction of Ti:sapphire ultrafast laser
technology. This made multiphoton excitation much more routine than hitherto had
been the case [129].
Multiphoton excitation microscopy generally uses long wavelength (near-IR) light
and excites fluorescent dyes through an additive process, whereby the energy of
two photons give the equivalent energy of one photon (cf. Fig. 1.1). While the
advantages of multiphoton excitation in spectroscopy, for example, in increasing
the dynamic range of anisotropy decay measurements, have already been mentioned
(Eq. 1.47), further advantages accrue in microscopy. Using near-IR, multiphoton
excitation minimizes the scattering in the sample, the background fluorescence, and
photo-bleaching by virtue of the greater confinement of the excitation volume. The
latter also facilitates the 3D imaging of sections over a greater optical penetration
depth than is possible with one-photon confocal excitation [130].

40
FLUORESCENCE
The equivalent expression to one-photon absorption (Eq. 1.3) for j photons
absorbed is nonlinear:
If = 𝜂(Φf∕j)Nd𝜎j𝜌j,
(1.48)
where If is the rate of fluorescence, 𝜂the fluorescence detection efficiency, Φf/j the
fluorescence quantum yield corrected for j-photon absorption, N the absorber number
density (cm−3), d the optical path length (cm), 𝜎j the absorption cross-section for
j photons (cm2j s j-1) and 𝜌the excitation photon flux density (photons cm−2 s−1,
typically GW cm−2 over a 10-μm spot size).
From Eq. (1.48) it can be seen that the slope of logeIf versus loge 𝜌should give a
straight line of slope j. This is a useful method of determining the number of photon
absorbed (see Section 1.7.2). Multiphoton excitation can also be used to selectively
excite fluorophores and quantum states according to their different selection rules.
Several reviews are available on the application of multiphoton excitation to biological
molecules in microscopy [131] and in spectroscopy [132].
If fluorescence spectroscopy has progressed through the development of temporal
resolution, then much of the push in fluorescence microscopy in recent years has been
dominated by improving spatial resolution. This has been both in terms of sectioning
in depth and lateral resolution such that sample volumes ∼10 fL can now be excited
and this is sufficiently small for single molecules to be studied in isolation.
Conventional optical microscopes operate in the far field and have a diffraction-
limited resolving power described by the Abbe criterion:
Δ =
𝜆
2NA,
(1.49)
where Δ is the smallest possible resolvable distance between two point sources, given
an emission wavelength, 𝜆, and an imaging system with a numerical aperture (NA).
NA is defined as nsin 𝜃, where n is the refractive index of the imaging medium and 𝜃is
the half-angle of the maximum cone of light collected by the objective. This diffraction
limit constrains the spatial resolution for visible wavelength to approximately 200–
300 nm.
Progress in terms of depth resolution using multiphoton confocal microscopy has
been complemented by other approaches. For example, total internal reflection fluo-
rescence (TIRF) microscopy employs an evanescent wave generated when incident
light is totally internally reflected at a glass–water/air interface to achieve selective
illumination. Penetration depths of ≤100 nm in the low-refractive index medium have
been achieved [133].
Improvements in lateral resolution have been obtained using a variety of tech-
niques in recent years. This has created the exciting new area of single-molecule
microscopy, which overcomes the ensemble smearing of information. This promises
to provide heterogeneity and other fundamental information that might answer some
of the big questions that underpin disease and therapeutics. For example, in protein
does all protein of a given type fold by the same path? Can we monitor, and thereby

MICROSCOPY
41
+ MeOH
– MeOH
200
Counts
100
0
500
600
Wavelength (nm)
700
800
Monomer
Trimer
FIGURE 1.19
(a) Scanning confocal microscope images of single allophycocyanin (APC)
molecules entrapped in hydrated pores of a thin silica film fabricated around the protein using
the sol-gel process. The excitation wavelength is 532 nm. (b) The fluorescence spectra for the
trimer (red) and monomer (blue) single molecule forms are shown. Dissociation of the trimer
is induced by alcohol. Each APC unit contains two phycocyanobilin fluorophores, which
are indicated by red dots. Reproduced from Reference 134 with permission from Springer
Science + Business Media. (For a color version of this figure, see the color plate section.)
better understand, metabolism, disease, and therapeutics in cells at the level of binary
molecular interactions? Figure 1.19 shows confocal microscope images and fluores-
cence spectra of a single protein aggregate and disaggregated monomer encapsulated
for ease of observation in a hydrated silica nanopore [134].
Single-molecule microscopes come in different forms. Whatever their form, one
of the underlying objectives is to enhance the fluorescence signal-to-noise ratio by
reducing the lateral resolution and this can be adequately achieved using a con-
focal microscope. Such an arrangement is commonly used in fluorescence corre-
lation spectroscopy (FCS), whereby a fluorescent entity (dye, protein, nanoparti-
cle, etc.) is observed by a photon detector as it diffuses in solution in and out
of the focal spot [135]. The autocorrelation function of the resultant fluorescence
signal can be interpreted in terms of simple diffusion dynamics to provide infor-
mation on size, aggregation, microviscosity, and diffusion constants that comple-
ment fluorescence quenching and anisotropy data. Two-photon excitation can also
be used for excitation with confocal FCS in order to further confine the excitation
volume [136].
Near-field scanning optical microscopy (NSOM) also enables single-molecule
studies and breaks through the far-field resolution limit (Eq. 1.49), like TIRF, by
exploiting the properties of evanescent waves (Fig. 1.18c). Here excitation is via
a scanning single-mode optical fiber, which has a tapered tip with an internally
reflective (aluminum-coated) sub-wavelength aperture that is held ∼10 nm from the
sample. The resolution depends on the aperture diameter rather than the wavelength
of the light in this case and can reach ∼50 nm [137], though the technique does not
lend itself to depth profiling. Fluorescence lifetime measurements have been achieved

42
FLUORESCENCE
using NSOM [138], but early promise was not realized due to a lack of reproducibility
in lifetime measurements caused by the plasmonic effect of the aluminized tip.
The ability to probe metabolism, disease, and therapeutics inside cells is a chal-
lenging task, but one in which single-molecule fluorescence, combined with spec-
troscopic techniques, has considerable potential to advance our knowledge. Several
excellent reviews of fluorescence techniques applied to the study of single molecule
are available [139–141].
One difficulty encountered in single-molecule microscopy is bleaching of the
fluorophore during a measurement, since multiple excitations (often using MHz
lasers) mean the likelihood of even low-probability nonemissive transitions being
populated is high. For example, a single fluorophore with Φf as high as 0.9 might
be seen to have a negligible probability for photobleaching p (say ∼10−6), but it
only has to be excited 1/p times to be likely to bleach. Blinking also occurs where a
fluorophore goes from the singlet excited state into a long lived triplet (“dark”) state,
which may or may not then repopulate the singlet state and allow the continuation of
fluorescence. Figure 1.20 illustrates both these phenomena.
As already mentioned, oxygen is a ubiquitous quencher of fluorescence, and it
also facilitates the bleaching of dye molecules. Although purging with nitrogen or
freeze-pump-thaw cycles might be adequate for enhancing the fluorescence yield and
lifetime in ensemble spectroscopy, oxygen scavenging chemicals are frequently used
Bleaching
Blinking
2 μm
FIGURE 1.20
Laser scanning confocal microscope image of single rhodamine B molecules
on a glass slide, illustrating the bleaching and blinking of dye molecules, these being a
characteristic behavior of single molecules. The image acquisition time was 5.5 min using
40 nm × 40 nm pixels with an integration time of 5 ms. Reproduced from Reference 142.

MICROSCOPY
43
in single-molecule microscopy in order to mitigate bleaching. Triplet quenchers can
also be used to minimize blinking [143].
Perhaps somewhat ironically the quenching of fluorescence has been developed
as means to enhance resolution in fluorescence microscopy. In the last two decades a
whole new range of super-resolution light microscopy (nanoscopy) techniques have
emerged for imaging in the far field and yet which break through the diffraction limit
by means of active control of the fluorescence [144]. Stimulated emission depletion
microscopy (STED) [145] was an early example overcoming the Abbe limitation
(Eq. 1.49). Here the emission of fluorescence excited by an ultrafast laser pulse is
confined to a region that is much smaller than that covered by the diffraction limit
by following up the initial excitation with a further ultrafast excitation pulse that
defines the resolution by depleting the fluorescence of dye within a doughnut around
the central spot. Various other methods have been developed that are also based
on the fundamental principle of active control of fluorescence, but using different
mechanisms to achieve <20 nm resolution (i.e., superior to NSOM). These include
photoactivated localization microscopy (PALM) [146], stochastic optical reconstruc-
tion microscopy (STORM) [147], and fluorescence PALM (FPALM) [148]. All these
require a low concentration of well-separated fluorophores and then controlling their
emission in order to avoid spatial overlap of emission that would otherwise blur the
reconstruction of the image. Three-dimensional imaging is also possible and recently
multicolor 3D super-resolution imaging has been demonstrated with a precision
<10 nm by means of stochastically switching fluorophores [149]. Structured illumina-
tion microscopy [150] is an alternative approach and gives rapid 3D super-resolution
imaging of cells by analyzing Moir´e interference fringes.
Some quite spectacular and well-resolved images have been produced using these
techniques and they are opening-up new possibilities for intracellular research in
particular. Figure 1.21 illustrates this for images [151] obtained using direct photoac-
tivation dSTORM [152], which uses direct excitation of a single fluorophore rather
than a coupled pair of fluorophores. The super-resolution image typically generated
is a computational reconstruction of the localized fluorophore density compiled from
detecting fluorophores blinking.
The advantages of bringing more of the multidimensional nature of fluorescence
to bear on microscopy became evident from quite early on. Traditionally, contrast
in fluorescence microscopy is provided by fluorescence intensity. This is dependent
on fluorophore extinction coefficient, quantum yield, the number of fluorophores
present, and excitation intensity. The early fluorescence lifetime measurements with
microscopes were only point-by-point single measurements [153] and the use of
fluorescence lifetime as a means of contrast came later through the introduction of
fluorescence lifetime imaging microscopy (FLIM). The advantages of fluorescence
lifetimes mentioned in Section 1.4 also translate to microscopy. For example, the
lifetime does not change with intensity; therefore, lifetime measurements are not
dependent on the local concentration of fluorophore, bleaching, optical path of the
microscope, excitation light intensity, or detection efficiency. Furthermore, the fluo-
rescence lifetime usually depends on the intrinsic characteristics of the fluorophore
and the local environment (e.g., microviscosity, pH, refractive index), as well as

44
FLUORESCENCE
0
−100 −50
0
50
x (nm)
Density
82 nm
100
40
80
120
Fluorescence
(a)
(b)
Localization microscopy
FWHM (nm)
Frequency
Localization density
160
1 μm
2 μm
2 μm
1.0 μm
1.0 μm
FIGURE 1.21
Super-resolution dSTORM images of (a) actin filaments stained with
phalloidin-Alexa Fluor 647 on glass [151]. The spread in full width half maximum (FWHM) of
the actin filament is shown in the upper boxed area. The localization density of a cross-section
through two crossing filaments is shown in the lower boxed area. (b) Epidermal growth factor
(EGF) is conjugated to Alexa Fluor 647 on HeLa cell surfaces, and bound to EGF recep-
tors. Activation of EGF receptors results in dimerization of receptors and clustering into pits
and vesicles with diameters ranging from 50 to 150 nm. Note how the localized microscopy
image obtained by using a Gaussian intensity distribution at each point to find the center
improves the resolution dramatically over the diffraction-limited fluorescence image that is
actually recorded. Compare also with the improvement over the single-molecule confocal
image without localization shown in Figure 1.20 on the same scale.
interactions with other molecules, such as through collisional or FRET quench-
ing. Thus, as well as helping to distinguish spectrally overlapping fluorophores,
imaging of the fluorescence lifetime can be used to probe the surroundings and
dynamical processes of a fluorophore. FLIM can be integrated with wide-field, con-
focal, two-photon, and other microscopy systems and has been extensively reviewed
[154, 155,156]. All the optical sources used for fluorescence lifetime spectroscopy
can be used in FLIM. Similarly, for scanning FLIM systems, the same detectors as for
spectroscopy can be used. Single-photon avalanche diodes (SPADs) are more widely
used for detecting fluorescence in microscopy than in spectroscopy. SPADs have high
detection efficiency and their small active area is more compatible with microscope
than monochromator images and can also serve as a confocal aperture [157].
Both TCSPC and phase-modulation methods, as described in Section 1.4, can be
used for FLIM. The relative merits of each in spectroscopy also translate to a large
extent to microscopy, for example, speed of FLIM with wide-field phase-modulation
and precision of TCSPC with confocal scanning. FRET [158] and anisotropy [159]
(Sections 1.5 and 1.6, respectively) can be combined with FLIM, both as a further
means of contrast and also for the molecular information they can provide, for
example, intracellular microviscosity and refractive index using the Strickler-Berg
expression (Eq. 1.12).

MICROSCOPY
45
1.7.2
Application Example—Gold Nanorods in Cells
Nanoparticles have emerged in recent years as providing new approaches to imaging,
sensing, drug delivery, and therapeutics [160]. Noble metals and quantum dots in
particular have been shown to complement aromatic fluorophores, not only by gen-
erating their own distinctive emission but also by modifying aromatic fluorescence
[161]. Just as Raman has benefitted from surface plasmon resonance (SPR) enhance-
ment, so too can fluorescence be enhanced and plasmonics tuned to further good
effect [162,163].
In its bulk form, gold has an extremely low luminescence quantum yield (∼10−10)
but in nanoparticle form it not only has a workable quantum yield, but has advantages
over conventional aromatic probes in so far as it is less cytotoxic and does not photo-
bleach. In the context of fluorescence the interest in nanoparticles, already mentioned
in Section 1.6.3, is primarily twofold. First, as more needs to be learned about the
intra- and intercellular transport properties of nanoparticles with respect to toxicity
[164], gold provides a useful paradigm. Second, the intracellular imaging properties
of gold nanoparticles offer a new approach to this fundamental area of biology.
As compared to in the bulk, gold spheres of dimensions <5 nm have been found
to have a much enhanced quantum yield of ∼10−5–10−4, which is thought to be
due to electron-hole recombination following absorption of a photon to promote
an electron from the narrow d band to the sp band above the Fermi level [165].
Gold nanorods were subsequently found to have an even higher quantum yield of
(∼10−4 to 10−3) when exciting the transverse surface plasmon mode [166]. One-
photon excitation is convenient, but a higher quantum yield would be desirable for
cellular imaging purposes. Multiphoton excitation of surface plasmon modes to obtain
resonant coupling in gold nanorods has also been demonstrated and shown to offer
comparable emission intensity to aromatic flurophores [167, 168]. This significant
development brought the additional advantages of multiphoton excitation to bear on
cellular microscopy studies with gold, as we illustrate below. More recently, quantum-
confined fluorescent gold clusters (a few tens of atoms with a radius ≤1 nm) have
been found to have “molecular-like” properties and relatively intense emission upon
one-photon excitation. The clusters are synthesized and stabilized in protein and were
recently reviewed [169].
The seeded growth method is typically used for synthesizing gold nanorods [170].
Typically, the steps are 2.5 mL HAuCl4 × 3H2O (0.001 M) and 0.6 mL ice-cold
NaBH4 (0.01 M) are added into 7.5 mL cetyltrimethylammonium bromide (CTAB)
(0.1 M) to prepare the seed solution. The growth solution is then synthesized by
adding 0.15 M BDAC, 50 mL HAuCl4 × 3H2O (0.001 M), 2 mL AgNO3 (0.004 M),
and 700 μL ascorbic acid (0.778 M) to 50 mL CTAB solution (0.1 M). The 80 μl
seed solution is then injected into the growth solution to grow gold nanorods.
The absorption spectrum of gold nanorods is characterized by two SPR absorption
bands that depend on the aspect ratio. Figure 1.22 shows a scanning electron micro-
scope (SEM) image of gold nanorods, the emission power dependence confirming
the two-photon nature of the fs Ti:sapphire laser excitation (Eq. 1.48) and typical
absorption spectra featuring longitudinal and weaker transverse plasmon modes over
a range of aspect ratios.

46
FLUORESCENCE
(c)
(a)
(b)
400
500
600
700
800
900
1000
1100
0.0
0.5
1.0
1.5
2.0
Absorption (a.u.)
Wavelength (nm)
L,2
L,3
L,4
L,5
T
0.80
0.85
0.90
0.95
1.00
1.05
3.1
3.2
3.3
3.4
3.5
3.6
Log Intenstiy (a.u.)
Log (power) (mW)
FIGURE 1.22
Gold nanorods: (a) SEM measurement. (b) Emission intensity dependence
on laser excitation power giving a slope of 1.97, confirming two-photon excitation at 750 nm.
(c) Plasmon absorption bands showing transverse (T) and longitudinal (L, AR) modes for
aspect ratios (AR) of 2, 3, 4, and 5. (For a color version of this figure, see the color plate
section.)
Gold nanorods are of great interest for biological imaging due to their remarkable
absorption and emission in the visible and near-IR regions when enhanced by SPR.
Especially, the absorption band between 700 and 900 nm corresponding to the longi-
tudinal SPR when excited by two photons in order to generate luminescence of high
intensity that is compatible with the spectral window in cells and tissue. Two-photon
emission from gold nanorods has been found to be sensitive to the polarization of the
incident excitation and has enabled single particle imaging. All these properties make
gold nanorods attractive probes for in vitro and in vivo imaging and their properties,
applications, and synthesis have been reviewed [171,172].
To prepare cell culture samples, gold nanorods dispersions are centrifuged and
redispersed in deionized water. In our example here Madin-Darby canine kidney
(MDCK) cells are then treated with 100 μL of gold nanorod solution and incubated for
3 h under standard cell culture conditions at 37◦C and 5% CO2. The cells are washed
and can then be stained with a fluorophore, in our case 4′,6-diamidino-2-phenylindole
(DAPI). The sample is then dispersed on a glass slide with a cover slip for imaging.
Figure 1.23 compares the intensity image and lifetime image of gold nanorods
in MDCK cells. The elliptical shapes in (a) and (b) correspond to nuclei stained

MICROSCOPY
47
25 ps
3.5 ns
(a)
(b)
(c)
10 μm
FIGURE 1.23
Two-photon excited MDCK cells stained with DAPI: Fluorescence intensity
image (a) without gold nanorods and (b) with gold nanorods. (c) FLIM image. Images are
obtained using two-photon 750 nm excitation in the longitudinal plasmon band with emission
collected over 535–590 nm. The scanning areas are (a) 133 μm × 133 μm, for (b) and (c)
67 μm × 67 μm 173. (For a color version of this figure, see the color plate section.)
by DAPI, and the bright spots in (b) are due to two-photon excited emission from
gold nanorods which are not present in (a). The fluorescence lifetime of DAPI in
cells is found to be typically a few nanoseconds, while the two-photon excited decay
time of gold nanorods is measured here to be <100 ps (limited by the instrumental
resolution), as confirmed by a FLIM study of pure gold nanorods on a glass slide
and time-resolved luminescence measurement of gold nanorods in a cuvette. The
FLIM image, (c), taken from the same sample area as in (b), with different coded
colors representing different lifetimes, clearly distinguishes the emissions from DAPI
(blue) and gold nanorods (orange). Previous work has shown that in fact the decay
constant associated with the gold nanorod emission is as low as a few ps [174,
175]. Figure 1.23 illustrates how fluorescence lifetime can provide a highly selective
contrast parameter for gold nanorods with respect to dye stains and endogenous
fluorophores.
Opportunities with energy transfer to fluorophores are also available with gold
nanorods. This is illustrated in Figure 1.24a, again using a two-photon FLIM image
of gold nanorods incubated in MDCK cells [176]. Concentrating on the detailed
structures, lifetime decays from rectangular regions labeled A and B are displayed
in Figure 1.24b. A decay curve of only DAPI in the cell culture is plotted as a
reference, and is described well by a single-exponential decay with a lifetime of 2.5
ns. Both decay curves in the regions A and B can be described by a biexponential
model, with a shorter component less than 100 ps ascribed to gold nanorods and a
longer ns component to DAPI. For decay curve in region A, the longer component is
2.5 ns. A reduced lifetime component of 0.9 ns for DAPI is found for the decay curve
derived from region B, indicating energy transfer between DAPI and gold nanorods in
this region.
Clearly, as well as using the ultrafast decay time of gold nanorods for contrast, some
of the other properties of gold nanorods such as dual plasmon modes, polarization
dependence of excitation, FRET and ability to conjugate can all be brought to bear on
intracellular studies. Finally, although previously mentioned here only briefly, gold

48
FLUORESCENCE
10 μm
A
B
0
2
4
6
8
10
10
–2
10
–1
10
0
DAPI
Spot A
Spot B
Intensity (a.u.)
Lifetime (ns)
25 ps
3.5 ns
(a)
(b)
FIGURE 1.24
(a) Two-photon excited FLIM image of GNRs incubated in MDCK cells.
Experimental conditions are as for Figure 1.23, but with a scanning area of 67 μm × 67 μm.
(b) Normalized fluorescence decay curves derived from regions A and B, with the fluorescence
decay curve of DAPI shown for reference. Reproduced from Reference 176 with permission
from AIP Publishing LLC. (For a color version of this figure, see the color plate section.)
nanoclusters of a few atoms synthesized in protein [169] would also seem to have
much to offer, for example in intracellular sensing of metal ions [177].
1.8
CONCLUSIONS
In this chapter, we have surveyed some of the main capabilities, techniques, and
measurements that are enabled by fluorescence. This has been supported by references
to both original articles and reviews of each topic. Space constraints dictate that
we are unable to provide the rigor of complete and dedicated texts [1–4], but we
hope that we have been able to go some way toward bridging the gap between the
fundamental principles and the perspectives and opportunities generated by modern
day applications of fluorescence [178].
Although there are important theoretical aspects still to be solved, the field of
fluorescence is largely driven by its experimental applications. One important exper-
imental aspect, that has so far received comparatively scant attention, is the method-
ology and tricks-of-the-trade that are so essential for good practice in fluorescence
measurements. A recent text seeks to address such a requirement, including all the
main areas covered here, and provides a useful complement to the present body of
literature [179].
ACKNOWLEDGMENTS
The authors wish to acknowledge the research support of EPSRC, BBSRC, and SFC.
They are also grateful to Dr. Ashok Ganesan for granting permission to include
Figure 1.3.

REFERENCES
49
REFERENCES
[1] I. B. Berlman, Handbook of Fluorescence Spectra of Aromatic Molecules, 2nd ed.
(Academic Press, New York, 1971).
[2] J. B. Birks, Photophysics of Aromatic Molecules (Wiley-Interscience, London, 1970).
[3] J. R. Lakowicz, Principles of Fluorescence Spectroscopy, 3rd ed. (Springer, New York,
2006).
[4] B. Valeur, Molecular Fluorescence Principles and Applications, 2nd ed. (Wiley-VCH,
Weinheim, 2012).
[5] C. A. Parker and W. T. Rees, “Correction of fluorescence spectra and measurement of
fluorescence quantum efficiency,” Analyst 85, 587–600 (1960).
[6] W. H. Melhuish, “Calibration of spectrofluorimeters for measuring corrected emission
spectra,” J. Opt. Sci. Am. 52(11), 1256–1258 (1962).
[7] T. B. Fitzpatrick, G. Szabo, M. M. Wick, and J. A. Parrish, “Biochemistry and physiology
of melanin pigmentation,” in Biochemistry and Physiology of the Skin, edited by L. A.
Goldsmith (Oxford University Press, New York, 1983), pp. 687–712.
[8] P. Meredith and T. Sarna, “The physical and chemical properties of eumelanin,”Pigm.
Cell Res. 19, 572–594 (2006).
[9] W. R. Ware and W. Rothman, “Relative fluorescence quantum yields using an integrating
sphere. The quantum yield of 9,10-diphenylanthracene in cyclohexane,” Chem. Phys.
Lett. 39(3), 449–453 (1976).
[10] C. W¨urth, M. Grabolle, J. Pauli, M. Spieles, and U. Resch-Genger, “Relative and
absolute determination of fluorescence quantum yields of transparent samples,” Nat.
Prot. 8, 1535–1550 (2013).
[11] W. H. Melhuish, “Distribution of fluorescence from a disk-shaped cuvette,” J. Opt. Sci.
Am. 51, 278–279 (1961).
[12] D. J. S. Birch and R. E. Imhof, “Fluorescence lifetimes and relative quantum yields of
9,10-diphenylanthracene in dilute solutions of cyclohexane and benzene,” Chem. Phys.
Lett. 32, 56–58 (1975).
[13] I. W. Hamley, “Peptide fibrillization,” Angew. Chemie. Int. Ed. 46, 8128–8147 (2007).
[14] A. K. Geim, and K. S. Novoselov, “The rise of graphene,” Nat. Mater. 6, 183–191
(2007).
[15] H. LeVine III, “Thioflavine T interaction with synthetic Alzheimer’s disease β-amyloid
peptides: detection of amyloid aggregation in solution,” Protein Sci. 2, 404–410 (1993).
[16] V. I. Stsiapura, A. A. Maskevich, V. A. Kuzmitsky, V. N. Uversky, I. M. Kuznetsova,
and K. K. Turoverov, “Thioflavin T as a molecular rotor: fluorescent properties of
thioflavin T in solvents with different viscosity,” J. Phys. Chem. B 112(49), 15893–15902
(2008).
[17] O. J. Rolinski, M. Amaro, and D. J. S. Birch, “Early detection of amyloid aggregation
using intrinsic fluorescence,” Biosens. Bioelectron. 25(10), 2249–2252 (2010).
[18] J. Mohanty, S. D. Choudhury, H. Pal, and A. C. Bhasikuttan, “Early detection of
insulin fibrillation: a fluorescence lifetime assay to probe the pre-fibrillar regime,”
Chem. Commun. 48, 2403–2405 (2012).
[19] A. A. R. Watt, J. P. Bothma, and P. Meredith, “The supramolecular structure of melanin,”
Soft Matter. 5, 3754–3760 (2009).

50
FLUORESCENCE
[20] J. Sutter, T. Bidl´akov´a, J. Karolin, and D. J. S. Birch, “Eumelanin kinetics and sheet
structure,” Appl. Phys. Lett. 100, 13701 (2012).
[21] N. Benseny-Cases, M. C´ocera, and J. Cladera, “Conversion of non-fibrillar 𝛽-sheet
oligomers into amyloid fibrils in Alzheimer’s disease peptide aggregation,” Biochem.
Biophys. Commun. 361, 916–921 (2007).
[22] S. J. Strickler and R. A. Berg, “Relationship between absorption intensity and fluores-
cence lifetime of molecules,” J. Chem. Phys. 37, 814–822 (1962).
[23] K. Suhling, J. Siegel, D. Phillips, P. M. W. French, S. Leveque-Fort, S. E. D. Webb, and
D. M. Davis, “Imaging the environment of green fluorescent protein,” Biophys. J. 83(6),
3589–3595 (2002).
[24] W. R. Ware and B. A. Baldwin, “Absorption intensity and fluorescence lifetimes of
molecules,” J. Chem. Phys. 40(6), 1703–1705 (1964).
[25] D. V. O’Connor, S. R. Meech, and D. Phillips, “Complex fluorescence decay of quinine
bisulphate in aqueous sulphuric acid solution,” Chem. Phys. Lett. 88, 22–26 (1982).
[26] J. B. Birks, “Radiative lifetime anomalies,” Z. Phys. Chem. 101, 91–104 (1976).
[27] D. J. S. Birch and R. E. Imhof, “The origin of fluorescence from trans-trans diphenylbu-
tadiene,” Chem. Phys. Lett. 88, 243–247 (1982).
[28] J. R. Lakowicz, G. Laczko, H. Cherek, E. Gratton, and M. Linkeman, “Analysis of
fluorescence decay kinetics from variable-frequency phase shift and modulation data,”
Biophys. J. 46, 463–477 (1984).
[29] D. W. Piston, G. Marriott, T. Radivoyevich, R. M. Clegg, T. M. Jovin, and E. Gratton,
“Wide-band acousto-optic light modulator frequency domain fluorometry and phospho-
rimetry,” Rev. Sci. Instrum. 60(8), 2596–2600 (1989).
[30] D. V. O’Connor and D. Phillips. Time-Correlated Single Photon Counting (Academic
Press, London, 1984).
[31] D. J. S. Birch and R. E. Imhof, “Time-domain fluorescence spectroscopy using time-
correlated single-photon counting,” in Topics in Fluorescence Spectroscopy, Vol. 1:
Techniques, edited by J. R. Lakowicz (Plenum Press, New York, 1991), pp. 1–95.
[32] L. M. Bollinger and G. E. Thomas, “Measurement of the time-dependence of scintillation
intensity by a delayed coincidence method,” Rev. Sci. Instrum. 32, 1044–1052 (1961).
[33] G. Hungerford and D. J. S. Birch, “Single-photon timing detectors for fluorescence
lifetime spectroscopy,” Meas. Sci. Technol. 7, 121–135 (1996).
[34] M. D. Eisaman, J. Fan, A. Migdall, and S. V. Polyakov, “Single-photon sources and
detectors,” Rev. Sci. Instrum. 82, 071101 (2011).
[35] D. J. S. Birch and R. E. Imhof, “Coaxial nanosecond flashlamp,” Rev. Sci. Instrum. 52,
1206–1212 (1981).
[36] D. J. S. Birch and R. E. Imhof, “A single-photon counting fluorescence decay-time
spectrometer,” J. Phys. E. Sci. Instrum. 10, 1044–1049 (1977).
[37] T. Imasaka, A. Yoshitake, K. Hirata, Y. Kawabata, and N. Ishibashi, “Pulsed semicon-
ductor laser fluorometry for lifetime measurements,” Anal. Chem. 57, 947–949 (1985).
[38] S. Nakamura, “InGaN-based violet laser diodes,” Semicon. Sci. Technol. 14, R27–R40
(1999).
[39] W. J. O’Hagan, M. McKenna, D. C. Sherrington, O. J. Rolinski, and D. J. S. Birch,
“MHz LED source for nanosecond fluorescence sensing,” Meas. Sci. Technol. 13, 84–
91 (2002).

REFERENCES
51
[40] C. D. McGuinness, K. Sagoo, D. McLoskey, and D. J. S. Birch, “A new sub-nanosecond
LED at 280 nm: application to protein fluorescence,” Meas. Sci. Technol. 15, L19–L22
(2004).
[41] C. D. McGuinness, K. Sagoo, D. McLoskey, and D. J. S. Birch, “Selective excitation of
tryptophan fluorescence decay in proteins using a subnanosecond 295 nm light-emitting
diode and time-correlated single-photon counting,” Appl. Phys. Lett. 86, 261911–261913
(2005).
[42] C. D. McGuinness, A. M. Macmillan, K. Sagoo, D. McLoskey, and D. J. S. Birch,
“Excitation of fluorescence decay using a 265 nm pulsed light-emitting diode: evidence
for aqueous phenylalanine rotamers,” Appl. Phys. Lett. 89, 063901–063903 (2006).
[43] D. McLoskey, D. Campbell, A. Allison, and G. Hungerford, “Fast-time-correlated
single-photon counting fluorescence lifetime acquisition using a 100 MHz semicon-
ductor excitation source,” Meas. Sci. Technol. 22, 067001 (2011).
[44] P. B. Coates, “The correction for photon “pile-up” in the measurement of radiative
lifetimes,” J. Phys. E. Sci. Instrum. 1, 878–879 (1968).
[45] C. C. Davis and T. A. King, “Single photon counting pileup corrections for time-varying
light sources,” Rev. Sci. Instrum. 41(3), 407–408 (1970).
[46] W. Becker, “Advanced Time-Correlated Single Photon Counting Techniques (Springer,
Berlin, 2005).
[47] D. J. S. Birch, R. E. Imhof, and A. Dutch, “Pulse fluorometry using simultaneous
acquisition of fluorescence and excitation,” Rev. Sci. Instrum. 55, 1255–1264 (1984).
[48] D. J. S. Birch, A. S. Holmes, J. R. Gilchrist, R. E. Imhof, S. M. Al-Alawi, and B.
Nadolski, “A Multiplexed single photon instrument for routine measurement of time-
resolved fluorescence anisotropy,” J. Phys. E. Sci. Instrum. 20, 471–473 (1987).
[49] D. J. S. Birch, A. S. Holmes, R. E. Imhof, B. Z. Nadolski, and K. Suhling, “Multiplexed
array fluorometry,” J. Phys. E. Sci. Instrum. 21, 415–417 (1988).
[50] W. Becker, A. Bergmann, E. Haustein, Z. Petrasek, P. Schwille, C. Biskup, L. Kel-
bauskas, K. Benndorf, N. Kl¨ocker, T. Anhut, et al., “Fluorescence lifetime images and
correlation spectra obtained by multidimensional time-correlated single photon count-
ing,” Micr. Res. Tech. 69, 186–195 (2006).
[51] W. Becker, A. Bergmann, and C. Biskup, “Multispectral fluorescence lifetime imaging
by TCSPC,” Micr. Res. Tech. 70, 403–409 (2007).
[52] D. McLoskey, D. J. S. Birch, A. Sanderson, K. Suhling, E. Welch, and P. J. Hicks, “Mul-
tiplexed single-photon counting, I. A time-correlated fluorescence lifetime camera,” Rev.
Sci. Instrum. 67, 2228–2237 (1996).
[53] B. R. Rae, C. Griffin, J. McKendry, J. M. Girkin, H. X. Zhang, E. Gu, D. Renshaw,
E. Charlton, M. D. Dawson, and R. K. Henderson, “CMOS driven micro-pixel LEDs
integrated with single photon avalanche diodes for time resolved fluorescence measure-
ments,” J. Phys. D. Appl. Phys. 41, 094011 (2008).
[54] Y. Wang, B. R. Rae, R. K. Henderson, Z. Gong, J. McKendry, E. Gu, M. D. Dawson, G.
A. Turnbull, and I. D. W. Samuel, “Ultra-portable explosives sensor based on a CMOS
fluorescence lifetime analysis micro-system,” AIP Adv. 1, 032115 (2011).
[55] DuPont. LUDOX Colloidal Silica: Properties, Uses, Storage and Handling: Data Sheets
(DuPont, 1987).
[56] P. R. Bevington. Data Reduction and Error Analysis for the Physical Sciences (McGraw-
Hill, New York, 1969).

52
FLUORESCENCE
[57] A. K. Livesey and J. Skilling, “Maximum entropy method,” Acta Crystallogr. A41,
113–122 (1985).
[58] C. Zander, M. Sauer, K. H. Drexhage, D.-S. Ko, A. Schulz, J. Wolfrum, L. Brand, C.
Eggeling, and C. A. M. Seidel, “Detection and characterization of single molecules in
aqueous solution,” Appl. Phys. B 63, 517–523 (1996).
[59] N. Boens, W. Qin, N. Basaric, J. Hofkens, M. Ameloot, J. Pouget, J.-P. Lefevre, B.
Valeur, E. Gratton, M. VandeVen, et al., “Fluorescence lifetime standards for time and
frequency domain fluorescence spectroscopy,” Anal. Chem. 79, 2137–2149 (2007).
[60] J. C. Pickup, “Diabetic control and its measurement,” in Textbook of Diabetes, 3rd ed.
edited by J. C. Pickup and G. Williams (Blackwell, Oxford, 2003), pp. 34.1–34.17.
[61] J. C. Pickup, F. Hussain, N. D. Evans, and N. Sachedina, “In vivo glucose monitoring:
the clinical reality and the promise,” Biosens. Bioelectron. 20, 1897–1902 (2005).
[62] A. Ciudin, C. Hernandez, and R. Simo, “Non-invasive methods of glucose measurement:
current status and future perspectives,” Curr. Diab. Rev. 8, 48–54 (2012).
[63] N. D. Evans, L. Gnudi, O. J. Rolinski, D. J. S. Birch, and J. C. Pickup, “Non-invasive
glucose monitoring by NAD(P)H autofluorescence spectroscopy in fibroblasts and
adipocytes: a model for skin glucose sensing,” Diabet. Technol. Ther. 5, 807–816 (2003).
[64] T. D. James, K. R. R. A. S. Sandanayake, and S. Shinkai, “A glucose-selective molecular
fluorescence sensor,” Angew. Chem. Int. Ed. 33, 2207–2209 (1994).
[65] S. Mansouri and J. S. Schultz, “A miniature optical glucose sensor based on affinity
binding,” Biotechnology 2, 885–890 (1984).
[66] P. W. Barons, R. S. Parker, and M. S. Strano, “In-vivo fluorescence detection of glucose
using a single-walled carbon nanotube optical sensor: design, fluorophore properties,
advantages, and disadvantages,” Anal. Chem. 77, 7556–7562 (2005).
[67] F. Hussain, D. J. S. Birch, and J. C. Pickup, “Glucose sensing based on the intrinsic
fluorescence of sol-gel immobilized yeast hexokinase,” Anal. Biochem. 339, 137–143
(2005).
[68] T. Tolosa, I. Gryczynski, L. R. Eichhorn, J. D. Dattelbaum, F. N. Castellano, G. Rao, and
J. R. Lakowicz, “Glucose sensor for low-cost lifetime-based sensing using a genetically
engineered protein,” Anal. Biochem. 267, 114–120 (1999).
[69] O. J. Rolinski, D. J. S Birch, L. J. McCartney, and J. C. Pickup, “A time-resolved
near-infra-red fluorescence assay for glucose: opportunities for trans-dermal sensing,”
J. Photochem. Photobiol. B 54, 26–34 (2000).
[70] K. J. Thomas, D. B. Sherman, T. J. Amiss, S. A. Andaluz, and J. B. Pitner, “A long
wavelength fluorescence glucose biosensor based on bioconjugates of galactose/glucose
binding protein and Nile Red derivatives,” Diabet. Technol. Ther. 8, 261–268
(2006).
[71] F. Khan, L. Gnudi, and J. C. Pickup, “Fluorescence-based sensing of glucose using
engineered glucose/galactose-binding protein: a comparison of fluorescence resonance
energy transfer and environmentally sensitive dye labelling strategies,” Biochem. Bio-
phys. Res. Commun. 365, 102–106 (2008).
[72] T. Saxl, F. Khan, M. Ferla, D. Birch, and J. Pickup. “A fluorescence lifetime-based fibre-
optic glucose sensor using glucose/galactose-binding protein,” Analyst 136, 968–972
(2011).
[73] K. Weidemaier, A. Lastovich, S. Keith, J. B. Pitner, M. Sistare, R. Jacobson, and D.
Kurisko, “Multi-day pre-clinical demonstration of glucose/galactose binding protein-
based fiber optic sensor,” Biosens. Bioelectron. 26(10), 4117–4123 (2011).

REFERENCES
53
[74] J. C. Pickup, Z.-L. Zhi, F. Khan, T. Saxl, and D. J. S. Birch, “Nanomedicine and its
potential in diabetes research and practice. Diabetes/Metabolism,” Res. Rev. 24, 604–610
(2008).
[75] M. De Miguel, M. Alvaro, and H. Garcia, “Graphene as a quencher of electronic excited
states of photochemical probes,” Langmuir 28, 2849–2857.
[76] D. K. Singh, P. K. Iyer, and P. K. Giri, “Role of molecular interactions and structural
defects in the efficient fluorescence quenching by carbon nanotubes,” Carbon 50, 4495–
4505 (2012).
[77] C. Preininger, I. Klimant, and O. S. Wolfbeis, “Optical fiber sensor for biological oxygen
demand,” Anal. Chem. 66, 1841–1846 (1994).
[78] J. N. Demas and B. A. DeGraff, “Design and applications of highly luminescent tran-
sition metal complexes,” in Topics in Fluorescence Spectroscopy, Vol. 4: Probe Design
and Chemical Sensing, edited by J. R. Lakowicz (Plenum Press, New York, 1994), pp.
71–107.
[79] C. D. Geddes, K. Apperson, J. Karolin, and D. J. S. Birch, “Chloride sensitive fluorescent
indicators,” Anal. Biochem. 293, 60–66 (2001).
[80] F. Kielar, C. P. Montgomery, E. J. New, D. Parker, R. A. Poole, S. L. Richardson, and
P. A. Stenson, “A mechanistic study of the dynamic quenching of the excited state of
europium(III) and terbium(III) microcyclic complexes by charge- or electron transfer,”
Org. Biomol. Chem. 5, 2975–2982 (2007).
[81] O. S. Wolfbeis and W. Trettnak, “Fluorescence quenching of acridinium and 6-
methoxyquinolinium ions by Pb2+, Hg2+, Cu2+, Ag2+ and hydrogen sulphide,” Spec-
trochim. Acta A 43, 405–408 (1987).
[82] B. Juskowiak, “Efficient quenching of the fluorescence of binapthyl-based amphiphiles
by mercury(II) complexes,” Anal. Chim. Acta 320, 115–124 (1996).
[83] S. P. Van and G. S. Hammond, “Amine quenching of aromatic fluorescence and fluo-
rescent exciplexes,” J. Am. Chem. Soc. 100(12), 3895–3902 (1978).
[84] D. M. Davis, D. McLoskey, D. J. S. Birch, P. R. Gellert, R. S. Kittlety, and R. M. Swart,
“The fluorescence and circular dichroism of proteins in reverse micelles: application to
the photophysics of human serum albumin and N-acetyl-l-tryptophanamide,” Biophys.
Chem. 60, 63–77 (1996).
[85] M. Smoluchowski, “Search for a mathematical theory of kinetic coagulation of colloid
solutions,” Z. Phys. Chem. 92, 129–168 (1917).
[86] J. R. Lakowicz, M. L. Johnson, N. Joshi, I. Gryczynski, and G. Laczko, “Transient
effects in quenching detected by harmonic-content frequency domain fluorometry,”
Chem. Phys. Lett. 131, 343–348 (1986).
[87] D. J. S. Birch, A. Dutch, R. E. Imhof, B. Nadolski, and I. Soutar, “The effect of transient
quenching on the excimer kinetics of 2,5-diphenyloxazole,” J. Photochem. 38, 239–254
(1987).
[88] N. Periasamy, S. Doraiswany, B. Venkataraman, and G. R. Fleming, “Diffusion
controlled reactions: experimental verification of the time-dependent rate equation,”
J. Chem. Phys. 89(8), 4799–4806 (1988).
[89] L. Stryer and R. P. Haugland, “Energy transfer: a spectroscopic ruler,” Proc. Natl. Acad.
Sci. USA 58, 719–726 (1967).
[90] S. Saini, G. Srinivas, and B. Bagchi, “Distance and orientation dependence of excitation
energy transfer: from molecular systems to metal nanoparticles,” J. Phys. Chem. B
113(7), 1817–1832 (2009).

54
FLUORESCENCE
[91] A. Iqbal, S. Arslan, B. Okumus, T. J. Wilson, G. Giraud, D. G. Norman, T. Ha, and D.
M. J. Lilley, “Orientation dependence in fluorescent energy transfer between Cy3 and
Cy5 terminally-attached to double-stranded nucleic acids,” Proc. Natl. Acad. Sci. USA
105, 11176–11181 (2008).
[92] J. Klafter and A. Blumen, “Direct energy transfer in restricted geometries,” J. Lumin.
34, 77–82 (1985).
[93] O. J. Rolinski and D. J. S. Birch, “Structural sensing using fluorescence nanotomogra-
phy,” J. Chem. Phys. 116(23), 10411–10418 (2002).
[94] O. J. Rolinski, D. J. S. Birch, and A. S. Holmes, “Metal ion quenching kinetics of DTDCI
in viscous solution and Nafion membranes: model system for near infrared fluorescence
sensing,” J. Biomed. Opt. 3(3), 346–356 (1998).
[95] D. J. S. Birch, O. J. Rolinski, and D. Hatrick, “Fluorescence lifetime sensor of copper
ions in water,” Rev. Sci. Instrum. 67(8), 2732–2737 (1996).
[96] U. G¨osele, M. Hauser, U. K. A. Klein, and R. Frey, “Diffusion at long range energy
transfer,” Chem. Phys. Lett. 34, 519–522 (1975).
[97] J. W. Taraska, M. C. Puljung, and W. N. Zagotta, “Short-distance probes for protein
backbone structure based on energy transfer between bimane and transition metal ions,”
Proc. Natl. Acad. Sci. USA 106(38), 16227–16232 (2009).
[98] J. W. Taraska, “Mapping membrane protein structure with fluorescence,” Curr. Opin.
Struct. Biol. 22, 507–513 (2012).
[99] O. J. Rolinski and D. J. S. Birch, “A fluorescence lifetime sensor for Cu(I) ions,” Meas.
Sci. Technol. 10, 127–136 (1999).
[100] R. B. Thompson and M. W. Patchan, “Lifetime-based fluorescence energy transfer
biosensing of zinc,” Anal. Biochem. 227, 123–128 (1995).
[101] T. K. Hurst, D. Wang, R. B. Thompson, and C. A. Fierke, “Carbonic anhydrase II-based
metal ion sensing: advances and new perspectives,” Biochim. Biophys. Acta (Proteins
and Proteomics) 1804, 393–403 (2010).
[102] J. U. Sutter, D. J. S. Birch, and O. J. Rolinski, “CdSe/ZnS core/shell quantum dots as
luminescence lifetime sensors for Cu2+,” Meas. Sci. Technol. 23, 055103 (2012).
[103] M. Bruchez Jr, M. Moronne, P. Gin, S. Weiss, and A. P. Alivisatos, “Semiconductor
nanocrystals as fluorescent biological labels,” Science 281, 2013–2015 (1998).
[104] A. R. Clapp, I. L. Medintz, J. M. Mauro, R. F. Brent, M. G. Bawendi, and H. Mattoussi,
“Fluorescence resonance energy transfer between quantum dot donors and dye-labelled
protein acceptors,” J. Am. Chem. Soc. 126, 301–310 (2004).
[105] R. Y. Tsien, “The green fluorescent protein,” Annu. Rev. Biochem. 67, 67509–67544
(1998).
[106] A. J. W. G Visser, S. P. Laptenok, N. V. Visser, A. van Hoek, D. J. S. Birch, J.-C.
Brochon, and J. W. Borst, “Time-resolved FRET fluorescence spectroscopy of visible
fluorescent protein pairs,” Eur. Biophys. J. 39, 241–253 (2010).
[107] R. F. Steiner, “Fluorescence anisotropy: theory and applications,” in Topics in Fluores-
cence Spectroscopy, edited by J. R. Lakowicz (Plenum Press, New York, 1991), Vol. 2,
pp. 1–52.
[108] A. Kawski, “Fluorescence anisotropy: theory and applications of rotational depolariza-
tion,” Crit. Rev. Anal. Chem. 23(6), 459–529 (1993).
[109] M. D. Barkley, A. A. Kowalczyk, and L. Brand, “Fluorescence decay studies of
anisotropic rotations of small molecules,” J. Chem. Phys. 75(7), 3581–3593 (1981).

REFERENCES
55
[110] D. J. S. Birch and C. D. Geddes, “Sol-gel particle growth studied using fluores-
cence anisotropy: an alternative to scattering techniques,” Phys. Rev. E 62, 2977–2980
(2000).
[111] M. P. Heyn, “Determination of lipid order parameters and rotational correlation times
from fluorescence depolarization experiments,” FEBS Lett. 108(2), 359–364 (1979).
[112] G. Porter, P. J. Sadkowski, and C. J. Tredwell, “Picosecond rotational diffusion in
kinetic and steady state fluorescence spectroscopy,” Chem. Phys. Lett. 49(3), 416–420
(1977).
[113] H. J. Eichler, U. Klein, and D. Langhans. Measurement of orientational relaxation times
of rhodamine 6G with a streak camera,” Chem. Phys. Lett. 67(1), 21–23 (1979).
[114] R. Wijnaendts Van Resandt and L. De Maeyer, “Picosecond rotational diffusion by
differential single-photon fluorescence spectroscopy,” Chem. Phys. Lett. 78(2), 219-223
(1981).
[115] U. Narang, R. Wang, P. N. Prasad, and F. V. Bright, “Effect of aging on the dynamics of
rhodamine 6G in tetramethyl orthosilicate-derived sol-gels,” J. Phys. Chem. 98, 17–22
(1994).
[116] C. D. Geddes, J. Karolin, and D. J. S. Birch, “1 and 2-photon fluorescence anisotropy
decay in silicon alkoxide sol-gels: interpretation in terms of self-assembled nanoparti-
cles,” J. Phys. Chem. 106, 3835–3841 (2002).
[117] M. Ameloot, M. vandeVen, A. U. Acu˜na, and B. Valeur, “Fluorescence anisotropy
measurements in solution: methods and reference materials,” Pure Appl. Chem. 85,
589–608 (2013).
[118] B. R. Lentz, “Membrane ‘fluidity’ as detected by diphenylhexatriene probes,” Chem.
Phys. Lipids 50, 171–190 (1989).
[119] P. Yip, J. Karolin, and D. J. S. Birch, “Fluorescence anisotropy metrology of electro-
statically and covalently labelled silica nanoparticles,” Meas. Sci. Technol. 23, 084003
(2012).
[120] K. Apperson, J. Karolin, R. W. Martin, and D. J. S. Birch, “Nanoparticle metrology
standards based on the time resolved fluorescence anisotropy of silica colloids,” Meas.
Sci. Technol. 20 025310 (2009).
[121] A. Bruno, C. de Lisio, P. Minutolo, and A. D’Alessio, “Characterization of ultrafast
fluorescence from nanometric carbon particles,” J. Opt. A. Pure Appl. Opt. 10, S578–
S584 (2008).
[122] L. Brand, J. R. Knutson, L. Davenport, J. M. Beecham, R. E. Dale, D. G. Walbridge,
and A. A. Kowalczyk, “Time-resolved fluorescence spectroscopy: some applications of
associative behavior to studies of proteins and membranes,” in Spectroscopy and the
Dynamics of Biological Systems, edited by P. Bayler and R. E. Dale (Academic Press,
London, 1985), pp. 259–305.
[123] T. A. Smith, M. Irwanto, D. J. Haines, K. P. Ghiggino, and D. P. Millar, “Time-resolved
fluorescence anisotropy measurements of the adsorption of rhodamine B and a labeled
polyelectrolyte onto colloidal silica,” Colloid Polym. Sci. 276, 1032–1037 (1998).
[124] R. B. Thompson, B. P. Maliwal, and V. L. Feliccia, “Determination of picomolar con-
centrations of metal ions using fluorescence anisotropy: biosensing with a ‘reagentless’
enzyme transducer,” Anal. Chem. 70(22), 4717–4723 (1998).
[125] Z. Gryczynski, O. O. Abugo, and J. R. Lakowicz, “Polarization sensing of fluorophores
in tissues for drug compliance monitoring,” Anal. Biochem. 273, 204–211 (1999).

56
FLUORESCENCE
[126] D. Allsop, L. Swanson, S. Moore, Y. Davies, A. York, O. M. A. El-Agnaf, and I. Soutar,
“Fluorescence anisotropy: a method for early detection of Alzheimer beta-peptide (A
beta) aggregation,” Biochem. Biophy. Res. Commun. 285, 58–62 (2001).
[127] D. Tleugabulova, A. M. Duft, M. A. Brook, and J. D. Brennan, “Monitoring solute
interactions with poly(ethylene oxide)-modified colloidal silica nanoparticles via fluo-
rescence anisotropy decay,” Langmuir 20, 101–108 (2004).
[128] A. Cleary, J. Karolin, and D. J. S. Birch, “pH tracking of silica hydrogel nanoparticle
growth,” Appl. Phys. Lett. 89, 113125–113127 (2006).
[129] W. Denk, J. H. Strickler, and W. W. Webb, “Two-photon laser scanning fluorescence
microscopy,” Science 248, 73–76 (1990).
[130] V. E. Centonze and J. G. White, “Multiphoton excitation provides optical sections from
deeper within scattering specimens than confocal imaging,” Biophys. J. 75, 2015–2024
(1998).
[131] A. Diaspro, G. Chirico, and M. Collini, “Two-photon fluorescence excitation and related
techniques in biological microscopy,” Quart. Rev. Biophys. 38, 97–166 (2005).
[132] D. J. S. Birch, “Multiphoton excited fluorescence spectroscopy of biomolecular sys-
tems,” Spectrochim. Acta. 57, 2313–2336 (2001).
[133] N. L. Thompson and J. K. Pero, “Total internal reflection microscopy: applications in
biophysics,” in Fluorescence Spectroscopy in Biology, edited by O.S. Wolfbeis, M. Hof,
R. Hutterer and V. Fidler (Springer, Berlin, 2005), pp. 79–103.
[134] A. M. Macmillan, J. Karolin, D. Panek, C. D. McGuinness, J. C. Pickup, D. Graham,
W. E. Smith, and D. J. S. Birch, “Improved biocompatibility of protein encapsulation in
sol-gel materials,” J. Sol-Gel Sci. Technol. 49, 380–384 (2009).
[135] N. Thompson, “Fluorescence correlation spectroscopy,” in Topics in Fluorescence Spec-
troscopy, edited by J. R. Lakowicz (Plenum Press, New York, 1991), Vol. 2, pp. 337–378.
[136] J. Mertz, C. Xu, and W. W. Webb, “Single-molecule detection by two-photon excited
fluorescence,” Opt. Lett. 20(24), 2532–2534 (1995).
[137] J. K. Trautman, J. J. Macklin, L. E. Brus, and E. Betzig, “Near-field spectroscopy of
single molecules at room temperature,” Nature 369, 40–42 (1994).
[138] X. Sunney Xie and R. C. Dunn, “Probing single molecule dynamics,” Science 265,
361–364 (1994).
[139] W. E. Moerner, and D. P. Fromm, “Methods of single-molecule fluorescence spec-
troscopy and microscopy,” Rev. Sci. Instrum. 74, 3597–3619 (2003).
[140] M. Kumbhhakar, S. Nath, T. Mukherjee, J. P. Mittal, and H. Pal, “Single-molecule
detection in exploring nanoenvironments: an overview,” J. Photochem. Photobiol C.
Photochem. Rev. 5, 113–137 (2004).
[141] J. Hohlbein, K. Gryte, M. Heilemann, and A. N. Kapanidis, “Surfing on a new wave of
single molecule fluorescence methods,” Phys. Biol. 7, 031001-22 (2010).
[142] D. P´anek. “Developments for Single Molecule Studies,” PhD thesis, University of
Strathclyde, Glasgow, UK, 2011.
[143] T. Ha and P. Tinnefeld, “Photophysics of fluorescent probes for single-molecule bio-
physics and super-resolution imaging,” Annu. Rev. Phys. Chem. 63, 595–617 (2012).
[144] M. A. Thompson, M. D. Lew, and W. E. Moerner, “Extending microscopic resolution
with single-molecule imaging and active control,” Annu. Rev. Biophys. 41, 321–342
(2012).

REFERENCES
57
[145] S. W. Hell and J. Wichmann, “Breaking the diffraction resolution limit by stimulated
emission: stimulated-emission-depletion fluorescence microscopy,” Opt. Lett. 19(11),
780–782 (1994).
[146] E. Betzig, G. H. Patterson, R. Sougrat, O. W. Lindwasser, S. Olenych, J. S. Bonifa-
cino, M. W. Davidson, J. Lippincott-Schwartz, and H. F. Hess, “Imaging intracellular
fluorescent proteins at nanometer resolution,” Science 313, 1642–1645 (2006).
[147] M. J. Rust, M. Bates, and X. Zhuang, “Stochastic optical reconstruction microscopy
(STORM) provides sub-diffraction-limit image resolution,” Nat. Meth. 3, 793–795
(2006).
[148] S. T. Hess, T. P. K. Girirajan, and M. D. Mason, “Ultra-high resolution imaging
by fluorescence photoactivation localization microscopy,” Biophys. J. 91, 4258–4272
(2006).
[149] D. Aquino, A. Sch¨onle, C. Geisler. C. Middendorff, C. A. Wurm, Y. Okamura, T. Lang,
S. W. Hell, and A. Egner, “Two-color nanoscopy of three-dimensional volumes by 4Pi
detection of stochastically switched fluorophores,” Nat. Meth. 8(4), 353–359 (2011).
[150] M. G. Gustafsson, “Surpassing the lateral resolution limit by a factor of two using
structured illumination microscopy,” J. Microsc. 198(Pt 2), 82–87 (2000).
[151] E. J. Rees, M. Erdelyi, D. Pinotsi, A. Knight, D. Metcalf, and C. F. Kaminski, “Blind
assessment of localization microscope image resolution,” Opt. Nanosc. 1, 1–12 (2012).
[152] M. Heilemann, S. van de Linde, Sch¨uttpelz, R. Kasper, B. Seefeldt, A. Mukherjee, P.
Tinnefeld, and M. Sauer, “Subdiffraction-resolution fluorescence imaging with conven-
tional fluorescent probes,” Angew Chem. Int. Ed. 47, 6172–6176 (2008).
[153] M. A. J. Rodgers, and P. A. Firey, “Instrumentation for fluorescence microscopy with
picosecond time resolution,” Photochem. Photobiol. 42(5), 613–616 (1985).
[154] X. F. Wang, A. Periasamy, and B. Herman, “Fluorescence lifetime imaging microscopy
(FLIM): instrumentation and applications,” Crit. Rev. Anal. Chem. 23(5), 369–395
(1992).
[155] T. French, P. T. C. So, C. Y. Dong, K. M. Berland, and E. Gratton, “Fluorescence lifetime
imaging techniques for microscopy,” Meth. Cell. Biol. 56, 277–304 (1998).
[156] J. W. Borst, and A. J. W. G. Visser, “Fluorescence lifetime imaging microscopy in life
sciences,” Meas. Sci. Technol. 21, 102002 (2010).
[157] G. S. Buller, and R. J. Collins, “Single-photon generation and detection,” Meas. Sci.
Technol. 21, 012002 (2010).
[158] A. Periasamy, “Fluorescence energy transfer microscopy: a mini review,” J. Biomed.
Opt. 6(3), 287–291 (2001).
[159] K. Suhling, P. M. W. French, and D. Phillips, “Time-resolved fluorescence microscopy,”
Photochem. Photobiol. 4, 13–22 (2005).
[160] T. L. Doane, and C. Burda, “The unique role of nanoparticles in nanomedicine: imaging,
drug delivery and therapy,” Chem. Soc. Rev. 41, 2885–2911 (2012).
[161] V. Biju, T. Itoh, A. Anas, A. Sujith, and M. Ishikawa, “Semiconductor quantum dots and
metal nanoparticles: syntheses, optical properties, and biological applications,” Anal.
Bioanal. Chem. 391, 2469–2495 (2008).
[162] J. R. Lakowicz, Y. B. Shen, S. D’Auria, J Malicka, J. Y. Fang, and Z. Gryczynski,
“Effects of silver island films on fluorescence intensity, lifetimes, and resonance energy
transfer,” Anal. Biochem. 301(2), 261–277 (2002).

58
FLUORESCENCE
[163] J. R. Lakowicz, “Radiative decay engineering: biophysical and biomedical applications,”
Anal. Biochem. 298(1), 1–24 (2001).
[164] R. Tantra and A. Knight, “Cellular uptake and intracellular fate of engineered nanoparti-
cles: a review on the application of imaging techniques,” Nanotoxicology 5(3), 381–392
(2011).
[165] J. P. Wilcoxon, J. E. Martin, F. Parsapour, B. Wiedenman, and D. F. Kelley, “Photolu-
minescence from nanosize gold clusters,” J. Chem. Phys. 108(21), 9137–9143 (1998).
[166] M. B. Mohamed, V. Volkov, S. Link, and M. A. El-Sayed. “The ‘lightening’ gold
nanorods: fluorescence enhancement of over a million compared to gold metal,” Chem.
Phys. Lett. 317, 517–523 (2000).
[167] R. A. Farrer, F. L. Butterfield, V. W. Chen, and J. T. Fourkas, “Highly efficient
multiphoton-absorption-induced luminescence from gold nanoparticles,” Nano Lett.
5(6), 1139–1142 (2005).
[168] K. Imura, T. Nagahara, and H. Okamoto, “Near-field two-photon-induced photolumi-
nescence from single gold nanorods and imaging of plasmon modes,” J. Phys. Chem. B
109(27), 13214–13220 (2005).
[169] D. M. Chevrier, A. Chatt, and P. Zhang, “Properties of protein-stabilized fluorescent
gold nanoclusters: short review,” J. Nanophoton. 6, 064504-01–064504-16 (2012).
[170] J. Murphy, T. K. Sau, A. M. Gole, C. J. Orendorff, J. Gao, L. Gou, S. E. Hunyadi, and
T. Li, “Anisotropic metal nanoparticles: synthesis, assembly, and optical applications,”
J. Phys. Chem. B 109, 13857–13870 (2005).
[171] X. Huang, S. Neretina, and M. A. El-Sayed, “Gold nanorods: from synthesis and
properties to biological and biomedical applications,” Adv. Mater. 21, 4880–4910 (2009).
[172] R. Sardar, A. M. Funston, P. Mulvaney, and R. W. Murray, “Gold nanoparticles: past,
present, and future,” Langmuir 25(24), 13840–13851 (2009).
[173] Y. Zhang, J. Yu, D. J. S. Birch, and Y. Chen, “Gold nanorods for fluorescence lifetime
imaging in biology,” J. Biomed. Opt. 15(2), 020504-3 (2010).
[174] O. P. Varnavski, M. B. Mohamed, M. A. El-Sayed, and T. Goodson III, “Relative
enhancement of ultrafast emission in gold nanorods,” J. Phys. Chem. B. 107, 3101–
3104 (2003).
[175] P. Biagioni, M. Celebrano, M. Savoini, G. Grancini, D. Brida, S. M´at´efi-Tempfli, M.
M´at´efi-Tempfli, L. Du`o, B. Hecht, G. Cerullo, et al., “Dependence of the two-photon
photoluminescence yield of gold nanostructures on the laser pulse duration,” Phys. Rev.
B 80, 045411-5 (2009).
[176] Y. Zhang, D. J. S. Birch, and Y. Chen, “Two-photon excited surface plasmon enhanced
energy transfer between DAPI and gold nanoparticles: opportunities in intra-cellular
imaging and sensing,” App. Phys. Lett. 99, 103701-3 (2011).
[177] C. V. Durgadas, C. P. Sharma, and K. Sreenivasan, “Fluorescent gold clusters as nanosen-
sors for copper ions in live cells,” Analyst 136, 933–940 (2011).
[178] D. J. S. Birch, “Fluorescence detections and directions,” Meas. Sci. Technol. 22,052002
(2011).
[179] Y. Engelborough and A. J. W. G. Visser (eds.), Fluorescence Spectroscopy and
Microscopy Methods and Protocols (Springer, New York, 2013).

2
SINGLE-MOLECULE DETECTION
AND SPECTROSCOPY
Michel Orrit
Institute of Physics, Leiden University, Leiden, The Netherlands
2.1
INTRODUCTION
At the end of the ninteenth century, evidence had accumulated for the existence of
atoms and molecules. Final hard proofs appeared at the beginning of the twentieth
century, with the quantization of charge, Brownian motion, and most importantly
the diffraction of X-rays by crystals. Already at that time, Jean Perrin [1] proposed
and attempted to observe single fluorescent molecules directly by eye in an optical
microscope. To decrease background, he used black soap films, which scatter only
minute amounts of light, mostly through Rayleigh scattering, with low concentrations
of fluorescent molecules. Unfortunately, because of the limited responsivity of the
human eye, of the low brightness of light sources and the poor quality of optical filters
and of the fluorescent dyes of those days, Perrin could not detect single molecules
by eye. Instead, he performed a very interesting study of fluorescent black films,
where he found another evidence for molecular structures, the quantized thickness of
soap multilayers. Thanks to spectacular progress in all above-mentioned components,
including optical microscopes, light sources and detectors, filters, and dyes, Perrin’s
thought experiment can nowadays be done in a straightforward manner, and single
molecules can even be seen in an optical microscope with the bare eye.
Even long after Perrin’s first heroic attempt, it was thought that molecules were
much too weak emitters to be detected optically. The progress of electron microscopy
led to observe single rows of atoms in crystals already in the 1950s, but the resolution
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
59

60
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
was too low, and the irradiation damage too high, to observe individual molecules
under the electron microscope. More recently, however, higher selectivity and better
collection of secondary electrons have made it possible to observe individual isolated
heavy atoms in a sample made of light atoms with an electron microscope. The
detection of single atoms and single molecules on surfaces first became a reality
in the early 1980s with the invention of the scanning tunneling microscope (STM)
[2], and later of its variant for insulating substrates, the atomic force microscope
(AFM) [3]. These two techniques have opened the way to microscopic investigations
of matter at the single-atom and single-molecule levels and are widely used today in
many laboratories.
Meanwhile, thanks to lasers and general progress in optical techniques, it had
become possible in the 1970s to observe single atoms in the gas phase, in dilute
atomic beams. The atoms were detected via the short bursts of fluorescence light they
emitted when crossing the laser focus. Then, single trapped ions were also detected
by their fluorescence. Yet, observing a single fluorescent atom in gas phase is a very
different problem from observing a single molecule in condensed phase, for the main
two reasons:
r First, condensed matter produces strong optical background via Rayleigh and
Raman scattering. Moreover, other fluorescent impurities in the sample may
easily dominate the fluorescence from a single molecule; therefore, the purity
of the sample is crucial. This problem obviously does not arise in vacuum.
r Second, a single atom usually can fluoresce millions of photons per second,
without any degradation. An atom in vacuum is a stable system, even in its
excited state. It can thus perform indefinitely many excitation–emission cycles,
which facilitates detection considerably. This convenient property of atoms is
unfortunately not shared by most fluorescing systems in condensed matter. The
immense majority of them are subject to irreversible transformations putting an
end to their luminescence. Those processes are called photobleaching and will
be discussed briefly later on.
In the late 1980s and early 1990s, progress in sources, optics, and detectors has
opened the way to detect single molecules in condensed matter. These experiments
have been extended to other single luminescent objects, including semiconductor
nanocrystals or quantum dots, metal particles, color centers, and so on. They are
nowadays performed in condensed matter environments as diverse and complex as
facets of catalyst crystals or live biological cells. Part of the driving force behind this
research domain is the broad current interest for nanoscience, that is, the structure and
dynamics of matter at nanometer scales. By coupling lasers to optical microscopes,
the powerful spectroscopic techniques developed in the 1970s and 1980s have been
transported not only down to sub-micron scales, but even down to nanometer scales if
a single object can be selected. Moreover, as we will see in this chapter, the selection
of single objects opens new motivations, methods, and results. This relatively young
field of investigation is called single-molecule optics or nano-optics.

INTRODUCTION
61
In this chapter, we briefly review the principles of optical microscopy and of the
fluorescence method, as these two techniques are the workhorses of single-molecule
studies. Let us stress again how small a molecule really is. If a dew drop was magnified
to the size of the Earth, a single water molecule would be about the size of a person.
It may appear hopeless to look for optical signals from such a tiny source in the
background of all other molecules in the drop. This is possible, however, due to the
special properties of the fluorescence signal, and to two tricks:
r First, the size of the illuminated sample must be reduced as much as possible.
For a given light intensity, that is, for a given signal from the molecule, the back-
ground will be proportional to the number of illuminated sample molecules, that
is, proportional to the volume of the illuminated sample. Reducing background
will require reducing the illuminated spot, usually by focusing the laser beam
very tightly into the sample. This focusing step thus performs a spatial selection.
r Second, the intrinsic background of the detection method used must in principle
be low enough for the single molecule’s signal to dominate the background
from all sample molecules in the illuminated volume. An illuminated volume
of 1 μm [3] contains about 109 sample molecules. The detection method must
therefore have a selectivity ratio, that is, the ratio of signals from the molecule
of interest to that of a sample molecule, higher than 109. Only very few optical
methods present such a high selectivity. Fluorescence does. The high selectivity
of fluorescence arises from optical resonance. A fluorescent molecule must
first absorb a laser photon, which a sample molecule cannot do. Therefore, the
laser frequency performs a spectral selection on the molecules contained in the
sample. Only molecules resonantly excited by the laser can emit fluorescence. It
is interesting to compare fluorescence to infrared absorption, for example, which
might open up studies of single chemical bonds. Unfortunately, the selectivity
of IR absorption is 1000 at best, because of the width and shape of vibrational
resonances. Therefore, such a method could only be applied to a single bond
in combination with spatial selection of less than 1000 molecules. Selection of
such a small excited sample cannot be achieved by diffraction-limited optics
alone.
We then proceed to discuss particular implementations of the fluorescence tech-
nique and give examples of the results that can be obtained with them. Fluorescence
of immobilized molecules (Section 2.3) is the easiest to discuss, as the signal arises
from a single system with a fixed emission dipole. Such recent developments as
superresolution microscopy are based on the fluorescence of immobilized single
molecules. When molecules can move and tumble, fluorescence signals are subject
to even more dynamical processes. Their evaluation requires statistical methods,
in particular autocorrelation of fluorescence time traces. The associated technique,
known as fluorescence correlation spectroscopy (FCS), is discussed in Section 2.4.
In the special case of cryogenic experiments at low temperatures, much information
can be obtained from fluorescence excitation spectroscopy, which will be discussed

62
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
in Section 2.5. These conditions give rise to narrow resonance lines, which make
it possible to perform highly accurate quantum optics experiments. Finally, single
molecules and single nano-objects can be detected by other far-field optical tech-
niques such as scattering or photothermal detection. Several experiments have been
demonstrated in the last years. These new methods will be discussed in Section 2.6
with their applications to single metal nanoparticles and other small objects.
2.2
EXPERIMENTAL SETUPS
2.2.1
Principles
The basis of single-molecule optical detection is an optical microscope [4], which
in principle is nothing else than a simple lens system for magnifying small objects.
The first lens, called the objective, has a short focal length (a few millimeters) and
creates an image of the object in the intermediate image plane. This image in turn
can be looked at with another lens, the eye piece. Typical magnification ratios are
in the range of 40–100. The resolution of the image provided by the microscope
is limited by diffraction. The Abbe-Rayleigh criterion states that, for a wavelength
𝜆, the smallest distance d resolvable between two point sources as deduced from
diffraction theory is d = 0.61 𝜆
NA, where NA = n sin 𝛼, called numerical aperture of
the objective lens, is proportional to the index of refraction n in the object space, and
𝛼is half the maximal angle under which the objective lens collects light from the
object (see Fig. 2.1b). In single-molecule experiments, the numerical aperture should
be as large as possible for two different reasons:
(i) The spatial resolution improves for larger NA, both laterally and axially (see
below).
(ii) The collection efficiency, that is, the brightness of the image, increases very
quickly with NA, quadratically for small apertures. In addition to angle consid-
erations, the index of refraction of the object medium is a very important factor
in the numerical aperture. Indeed, because of total reflection at interfaces, a
high index enables collection over a wider range of angles. This is the reason
why most microscopes use immersion to enhance resolution and collection
efficiency.
The main difficulty in manufacturing microscope objectives is to achieve a good
correction of all aberrations (spherical, chromatic) also for off-axis rays with inci-
dence angles which can be larger than 60◦. This is achieved by assembling several
lenses (sometimes more than 10) made out of different glasses, and which have to be
anti-reflection-coated for a good luminosity. Good microscope objectives are there-
fore quite advanced and expensive pieces of technology. Immersion oil objectives
reach an NA of 1.4, corresponding to collection angles close to 90◦.
An ideal microscope lens will image a point source as an Airy pattern if a circular
iris or diaphragm limits the aperture. This point-spread function (PSF) has therefore

EXPERIMENTAL SETUPS
63
(a)
(b)
(c)
λ/2
50/50
GT
Filter
Confocal
pinhole
Spectrometer
or APD
Iris
x
y
z
λ
α
λ
α
2n sin
α air
α oil
α
(n sin
)2
2n
FIGURE 2.1
(a) Scheme of a confocal setup for single nano-object spectroscopy. The
confocal spot can be scanned with mirrors and a telecentric lens system (not shown) or simply
by scanning the sample with a translation stage (after Reference 90). (b) Definition of the
numerical aperture NA = n sin 𝛼and its relation to the lateral and axial resolutions. (c) For
sources placed in a refringent material, a wetting layer of a high-index liquid (immersion oil)
considerably increases the numerical aperture. This has the double advantage of enhancing
both the resolution and the collection efficiency.
the classical form of the diffraction spot from a round hole. If the light collected is
a Gaussian beam instead, provided it is fully included in the acceptance pupil of the
objective, the PSF will be a Gaussian spot. For many experiments in 3D samples, it
is important to consider the depth of focus, that is, the length of the PSF in the axial
direction. This size L (sometimes called Rayleigh length) is approximately
L = 2n 𝜆
NA2 .
The 3D appearance of the PSF is thus an elongated (prolate or cigar-shaped)
ellipsoid (Fig. 2.1b). Decreasing this length is a third reason to make the numerical
aperture as high as possible, as the volume of the PSF scales as NA−4.
The numerical aperture is proportional to the refraction index. Therefore, it is of
advantage to collect light through glass, or through high-index oil, when possible
(Fig. 2.1c). Special water-immersion objectives are used for biological samples. To
fully benefit from high index, the index must of course be matched between the
sample and objective glass. This is achieved thanks to the immersion oil. A further
advantage of immersion is the higher efficiency of fluorescence collection. An air gap

64
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
leads to light losses by total internal reflection at the interface from high- to low-index
media (see Fig. 2.1c). For emitters placed at an air–glass interface, the situation is
even worse, as a large fraction of the emitted light (up to 90%) is radiated into the
high-index material. Collecting emission on the air side therefore entails big losses
of intensity, in addition to losses in resolution.
2.2.2
Correction of Aberrations
The ideal microscope objective would image a planar object onto a plane field,
without distortion, and without change in image with wavelength (achromatism). To
achieve this, the following aberrations must be corrected:
(i) Chromatic aberrations from the dispersion of glasses (their refractive index
is larger for blue than for red light). Even with well-corrected objectives, the
focus often moves by some microns when the wavelength varies over the
visible spectrum.
(ii) Geometrical aberrations: spherical (change of focal point with distance from
axis), coma, due to changes of the image point with ray direction, field curvature
(image focus is not obtained on a plane but on a curved surface), field distortion
(pincushion or barrel images), and so on.
Simple spherical lenses made out of ordinary dispersive glass suffer from all these
aberrations and cannot fulfill the requirements of the ideal lens for large numeri-
cal apertures. For example, a planoconvex lens has spherical aberrations which are
minimized by placing the convex surface on the side of the parallel beam. Aspheric
singlets used in CD readers correct in principle perfectly for their focus, but are
designed to work at one wavelength only and have strong chromatic aberrations. To
approximate the ideal lens’ requirements, one uses combinations of spherical lenses
possessing various radii of curvature, thicknesses, and materials, and one varies their
positions. To design objectives and other multilens systems, special codes calculate
imaging with rays far from paraxial for arbitrary systems of lenses. A good objective
may contain more than 10 lenses, which have to be positioned with specifications as
narrow as microns for some of them. The air–glass interfaces have to be antireflec-
tion coated to reduce reflection losses, and small gaps between the lenses are often
bridged with a high-index medium, usually UV-polymerizable glue, after the respec-
tive position of the lenses has been adjusted by hand. Therefore, objective lenses are
expensive and sensitive optical components.
Nowadays, most objectives are infinity-corrected. This means that they are not
calculated to form their image directly in the intermediate image plane, but to form
an image at infinity. Another lens called tube lens then images the plane waves
into the intermediate image. Some manufacturers use the tube lens to correct some
aberrations. In that case, tube lens and objective must be used in combination for
optimal correction. The advantages of infinity-corrected objectives are essential in
confocal microscopy, polarization studies, and spectroscopy, because plane waves

EXPERIMENTAL SETUPS
65
are easier to filter and manipulate than spherical waves (e.g., they are not distorted
by flat windows).
2.2.3
Polarization Structure at the Focus
The electric field of a laser wave is a vector quantity, which complicates the polar-
ization structure in the case of high numerical apertures. We will briefly discuss the
polarization of the field at the focus of a linearly polarized laser wave [4]. For low
NA, the polarization of the spot is the same as that of the incident beam. At the
focal point itself, by symmetry, the polarization of the incident beam is conserved. At
high NA, however, and for parts of the PSF away from the center, interference of the
incoming rays leads to deviations from this polarization. For large incidence angle
and a linearly polarized incident beam, simple consideration of the interference of
different rays shows that the axial (i.e., polarized along the axis) component of the
field has a node at the center and presents two (weak) lobes in the focal plane. The
third transverse component (perpendicular to the axis and to the incoming polariza-
tion) is even weaker and presents four lobes. By using annular illumination, and/or
by introducing phase masks in the incoming beam, the polarization of exciting laser
light at the focus can be manipulated, and can even present a PSF with a single
lobe for the longitudinal polarization. This is of great interest to determine the full
3D orientation of single absorbers, since their transverse polarization can be easily
probed with linearly polarized light and normal illumination.
A similar, but distinct problem is to find the polarization structure of a wave
radiated by a linear dipole at the focus, and collimated into a plane wave by an
objective with large numerical aperture. In the case of a dipole lying in the focal
plane, it can be shown [5] that the polarization is linear throughout the field, parallel
to the dipole along the horizontal and vertical directions (N, S, E, W), and significantly
tilted in the NW, NE, SW, SE positions. In the case of a dipole perpendicular to the
focal plane, the polarization is radial.
2.2.4
Various Microscopy Methods
There are several ways to record images with a microscope. We briefly mention the
most important ones for single-molecule studies.
2.2.4.1
Confocal Microscopy
In this method, only one point of the sample is
imaged onto a photodetector. If the sample is scanned in three dimensions, a 3D image
is obtained by recording the optical signal as a function of sample position. To reduce
background, that is, to ensure that the signal arises only from the focus, a diaphragm
or pinhole is inserted in one of the image planes (see scheme in Fig. 2.1a). To scan the
area to be imaged, one can move either the sample itself with piezoelectric transducers
(sample scanning), or the focus by means of tilting mirrors (beam scanning). In the
latter case, a telecentric system images the spot of the illumination beam on the
scanning mirror onto the input lens of the objective. In this way, the two points remain
conjugated when the mirror is tilted during scanning. Fluorescence is collected in the

66
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
backward scattered geometry and is folded back exactly onto the incoming path by
the mirror-telecentric system assembly.
2.2.4.2
Wide-Field Imaging
In this method, a large part of the field is illuminated
by an unfocused beam (epi-illumination), and the image is formed on a multichannel
detector such as a CCD camera or an image intensifier. For a 3D sample, this design
leads to background arising from emission by planes below and above the imaged
plane of the sample.
2.2.4.3
Total Internal Reflection Illumination
To reduce the background, the
excitation light can be sent at a large incidence angle on the surface, achieving total
internal reflection (TIR). Fluorescence and other emissions can be collected either
across the interface, or on the same side as the illumination. In that case, collection
has to be performed in a solid angle where no illumination light is present, which
requires a large NA.
The confocal configuration schematically presented in Figure 2.1 is particularly
useful to detect single molecules because of its simplicity and of its low background.
This design has the advantage that only the focus is excited with high efficiency, thus
limiting photobleaching in other sample planes. Moreover, fluorescence arising from
other points does not reach the detector. The spatial selection of single molecules or
single nano-objects is therefore performed in two steps, with equivalent selectivity:
r Excitation selection, by focusing the laser beam on a small spot.
r Detection selection, by detecting light arising from the same area only.
Several other optical designs have been developed in the last 20 years with various
elements for excitation and collection: single-mode optical fiber, parabolic mirror,
aspheric lenses, gradient index lenses, and so on. An overview can be found in
Reference 6.
2.3
FLUORESCENCE SPECTROSCOPY
2.3.1
Introduction, Signal-to-Noise Ratio
If single molecules are immobilized in or on a solid or in a highly viscous sample, it
becomes possible to study the same molecule for long periods of time. This situation
is very different from experiments in liquid solutions, where statistics over a large
number of molecules usually always give a signal, even in difficult conditions with
strong background and noise. For molecules immobilized in solids or on surfaces, it
is of crucial importance to be able to “recognize” the molecule, that is, to distinguish
its signal from background and noise.
The following discussion of the signal/noise ratio applies not only to microscopic
images, but also to other signals from immobilized single molecules, for example,
their spectra. Let us consider the background B and the signal S of the molecule as
functions of a scanning parameter (typically one position coordinate of the sample).

FLUORESCENCE SPECTROSCOPY
67
If t is the acquisition time per channel or pixel, the number of acquired background
counts is Bt. Assuming shot noise to be dominant (which is usually the case for
weak signals), the noise from the background is
√
Bt, which has to be compared
to the signal St. The most favorable case to detect the molecule is when its signal
occupies about one pixel or one channel width (a broad structure is more difficult to
distinguish from noise than a sharp one). The molecule will be detectable if and only
if the signal-to-noise ratio is significantly larger than unity, let us say, equal to 3:
St > 3
√
Bt, or t > 10 B
S2 .
For a typical background of 100 counts per second arising from the dark counts
of the avalanche detector, and for an acquisition time of 1 s, it follows that the signal
has to be larger than 30 counts per second to be clearly distinguished from noise.
This condition becomes only a lower bound if the molecule’s image is spread over
more than one pixel.
2.3.2
Sample Preparation
In fluorescence experiments, the main source of background is fluorescence from
residual impurities or from optical elements. Raman scattering is often negligible. To
observe single molecules, it is therefore crucial to reduce contamination by fluorescent
impurities, and to work with very clean optical parts. The excited volume must be
as small as possible, and the substrate should be nonfluorescent (fused silica, or very
pure glass). Hereafter, we cite a few usual methods to prepare samples.
2.3.2.1
Spin Coating
Quick spinning of a flat substrate with a thin layer of solution
leads to a uniform film, draining slower and slower as time goes on. At the same
time, evaporation decreases the thickness too, and increases the concentration and
the viscosity. In a first phase, viscous draining is dominant in reducing the thickness,
then evaporation takes over. The combination of viscous draining and evaporation
leads to uniform films of tunable thickness, for example, to prepare photoresists in the
semiconductor industry. Typical spin-coating parameters (1000 rpm, 1% weight of
polymer in a solvent like water) lead to thicknesses between 10 nm and a few microns.
Spin coating is very interesting to reduce the illuminated volume, and therefore the
background, by reducing the thickness.
2.3.2.2
Langmuir–Blodgett Films
Even thinner, monomolecular films are
obtained by the Langmuir–Blodgett technique, that is, deposition onto a sub-
strate from a monomolecular layer at the air–water interface. The fluorescent dye
molecules are usually introduced at low concentration in the spreading solution of
the amphiphilic molecules, but may also be adsorbed from an aqueous solution.
2.3.2.3
Bilayers, Membranes, and Black Lipid Films
These thin layers are par-
ticular cases of a pair of monomolecular layers, which can be either deposited on

68
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
a substrate or suspended to a hole. Cell and vesicle membranes are particularly
important matrixes for single biomolecules.
2.3.2.4
Other Preparation Methods
Molecules can be deposited directly from
solution in a volatile solvent substrate (in that case they are more sensitive to oxygen
and water), included in nanocrystals, etc., or simply selected or imaged within a
3D sample, such as a cell, for example. In the latter case, a thin sample should be
preferred to limit background.
Much information can sometimes be obtained from images with diffraction-limited
single-molecule spots. Counting them can provide concentrations, the stoichiometry
of complexes [7], and so on. Another important application is colocalization [8],
which makes it possible to test interactions between biomolecules labeled with two
different fluorophores. However, much progress has been done recently in superres-
olution imaging, as will be discussed below.
2.3.3
Orientation
The orientation of the in-plane component of the molecular transition moment can
be obtained easily by polarization measurements. Two different methods are used
dominantly:
2.3.3.1
Polarization Analysis in Detection
The fluorescence beam is sent to a
polarizing beam splitter and the two beams are sent to different detectors. An exam-
ple of polarization-dependent images is given in Figure 2.2, where the polarization
FIGURE 2.2
Example of rotational diffusion of single dye molecules in glycerol at low
temperature (200 K), monitored via intensity fluctuations in two orthogonal fluorescence
polarizations (left panels). On the right panel, the polarization is color-coded, red for vertical,
green for horizontal. The diffusion times of molecules are comparable to the scan time, about
1 s per line. (For a color version of this figure, see the color plate section.)

FLUORESCENCE SPECTROSCOPY
69
direction of the fluorescence of single dye molecules is color-coded. The intensity
ratio gives two possible solutions for the in-plane orientation. Discriminating between
those requires a third measurement. In combination with polarized excitation, polar-
ized analysis gives the fluorescence anisotropy, related to the angular mobility and
rotational diffusion of the fluorophore during fluorescence. This method is very use-
ful to monitor rotational relaxation of slowly moving objects, such as dyes in highly
viscous glass formers [9,10]. These experiments show that different single molecules
diffuse as Brownian particles on short timescales, but that different molecules diffuse
at different rates, and that this diffusion can change in time. This inhomogeneity
gives rise to the complex (nonexponential) behavior of the glass just above the glass
transition. Single-molecule experiments directly confirm the spatial heterogeneity of
glass dynamics.
2.3.3.2
Polarization Modulation in Excitation
The polarization of the exciting
beam can be modulated in time, for example, with a rotating polarizer. The phase
and amplitude of the sinusoidal variation of the detected intensity gives information
about the orientation and diffusion of the fluorophore. For a fixed molecule, the
modulation is sinusoidal with maximum contrast. For a fast diffusing molecule,
however, the intensity modulation depths decrease, indicating diffusion or hampered
diffusion [11].
As discussed in the section on optical microscopy, the full 3D orientation of
the dipole moment can be obtained with a widely opened excitation beam, or by
measuring the distribution of fluorescence photons as a function of the angles of
emission and comparison to the emission pattern of a dipole. This latter method
requires a matrix of detectors for fast determination.
2.3.4
Blinking
When the fluorescence intensity of a nano-object is recorded as a function of time (in
a so-called fluorescence intensity trace), random variations of the average emission
intensity are often observed [12]. The variations can sometimes be progressive,
but often present sudden jumps between bright and dark states. We discuss only
the latter behavior here, which is commonly known as blinking or intermittency.
Blinking is a characteristic feature of the emission of single nano-objects, which has
been observed for organic molecules, semiconductor nanocrystals, and color centers.
Figure 2.3 presents typical blinking fluctuations in the luminescence signal of a single
semiconductor nanocrystal. In large populations, blinking is almost always hidden
because the fluctuations of individual objects are not synchronized. In some cases, an
external parameter can synchronize the blinking of all objects (an obvious example
is switching the exciting laser on). In those cases, blinking will appear as a transient
variation of the average signal. It is one of the most powerful features of single-
molecule methods that no synchronization is needed to observe such fluctuations
directly, as the example of blinking shows.
Several possible mechanisms can lead to blinking. Most of them involve a “dark”
state of the emitter, that is, a state which does not fluoresce, either because it does not

70
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
Intensity (a.u.)
1000
800
600
400
200
0
1000
800
600
400
200
0
0
500
1000
1500
2000
2500
Intensity (a.u.)
1320
1340
1360
Time (s)
1380
1400
FIGURE 2.3
Example of blinking luminescence traces of a semiconductor nanocrystal
(CdSe core with a ZnS shell), showing bright and dark periods with a wide distribution of
durations. After Reference 12.
absorb efficiently, or because its fluorescence yield is too low. Hereafter, we examine
a few possible sources of blinking.
2.3.4.1
Triplet State
A molecule in its triplet state can in theory absorb and emit
photons, but usually with very low fluorescence yield only. Moreover, the laser
wavelength is normally not suited to excite triplet–triplet transitions. Therefore, the
triplet state is dark. The molecule’s fluorescence trace presents dark periods lasting for
the triplet lifetime on average. Depending on the molecule, this time can be as short
as microseconds and as long as tens of milliseconds for usual dyes. Atmospheric
oxygen has a strong influence on triplet lifetime. Since O2 has a triplet ground
state, it can exchange electrons with a molecular triplet, yielding two singlet states,
one the ground state of the dye, the other one singlet oxygen, a very reactive and
phosphorescent species.
2.3.4.2
Electron Transfer
The excited molecule may accept an electron from its
environment, or give one electron to it [12]. The radical ion left often has very low
fluorescence yield. The charge-transfer state lives as long as the electron is away,
giving rise to dark states with lifetimes longer than seconds if the electron goes far
away, as the example of rhodamine 6G in polyvinyl alcohol shows [13]. This process
appears to be very general, as was recently demonstrated in the groups of Sauer and
Tinnefeld. Charge transfer rates can be controlled by adding redox species to the
solution. This scheme (known as ROXS) [14] profoundly modifies the lifetimes of
the triplet state and of the charged dark states, which can have the added benefit that
bleaching (see next paragraph) is also significantly reduced.
2.3.4.3
Other Reversible Photochemical Reactions
Excited molecules may
change conformation. If the change is reversible, and if the two forms have different
optical properties, blinking will follow. An example of intramolecular reactions of

FLUORESCENCE SPECTROSCOPY
71
this kind is cis-trans isomerization, which is common in cyanine dyes. The excited
molecule may also react with its environment, for example, by abstracting a proton
from a nearby molecule or from an acid matrix. Several dyes present protonated
“leuco” forms, in which an additional proton shortens the conjugation path and
shifts the absorption spectrum to the blue. The main feature of these photochemical
reactions involving large rearrangements of atoms is that they often require large
activation energies and are therefore frozen at low temperatures.
2.3.4.4
Other Sources of Blinking
They may exist in principle. For example, the
orientation of the molecule could switch between parallel and perpendicular to the
excitation polarization. The absorption spectrum of the molecule could also shift
between different spectral positions. This spectral diffusion is a prominent effect
in low-temperature experiments (see below), and has also been observed at room
temperature in polymers [15]. However, such “physical” processes do not seem to
lead to significant blinking, entailing large intensity fluctuations.
2.3.5
Bleaching
Many photochemical reactions are irreversible. In that case, the molecule stays in the
dark state the first time it goes there. Fluorescence is lost for good, and a new single
molecule must be found. At room temperature, all fluorescent organic molecules
photobleach sooner or later. Much effort has been spent to screen, adapt, and design
dyes able to resist photobleaching as far as possible. This has been crucial for laser
dyes, but also for fluorescent labeling in biology. In spite of several decades of
efforts, little is known about photobleaching and how to control it. Atmospheric
oxygen and small reactive molecules such as water obviously open efficient channels
for photobleaching. Production of highly reactive singlet oxygen from the triplet state
of the dye is a well-characterized degradation channel. That is the reason why red
and infrared dyes with their triplet state below the energy of singlet oxygen are highly
photoresistant, even in air. Because of the important role of oxygen, many single-
molecule fluorescence studies are done in the presence of reducing agents (oxygen
scavengers, mercaptoethanol, Trolox®, etc.). The ROXS scheme (see above), by
shortening the lifetimes of metastable and reactive states, also improves the resistance
of dyes to bleaching. However, even in inert and dry atmosphere, or under high
vacuum, photobleaching still occurs, albeit at a lower rate. Because photobleaching
involves barrier crossing in chemical reactions, it is likely that low temperatures
considerably decrease the process. Temperature studies of rhodamine in PVA have
shown that indeed, bleaching is reduced by several orders of magnitude at low
temperatures for some molecules, but not for all of them [16]. Recent articles report
dyes of the perylene or terrylene-diimide family, which present excellent resistance
to bleaching, and still can be made water soluble by convenient substitution [17].
2.3.6
Superresolution
The size of the PSF, for example, the image spot of a single molecule, is limited
by diffraction. However, as was realized from the very beginning of single-molecule

72
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
observations, the center of the image of a single molecule can be found with accuracy
much higher than the width of the PSF. If N photons are detected, we can exploit our
knowledge that all of them arise from the same single molecule, a point-like region of
space. Therefore, the position of their center of gravity is determined down to roughly
Δx∕
√
N, where Δx is the spatial width of the PSF. Indeed, the probability distribution
of the average of N random variables, each of them distributed along the same normal
Gaussian probability law, is itself a Gaussian with width reduced by
√
N.
In low-temperature experiments (see Section 2.5), single molecules can be dis-
criminated by their resonance frequency. The selection of many different molecules
at many different laser frequencies gives rise to a superresolved image [18], where
each molecule provides one point with Δx∕
√
N accuracy. Because a large number of
photons can be accumulated for each single molecule, the accuracy can reach a few
nanometers only. This has been done recently on a sample of millions of molecules in
a crystal at low temperature [19], providing a superresolved image of the crystal. In
2006, three important papers appeared almost simultaneously, demonstrating super-
resolution based on photochemical switching of single molecules [20–22]. These
three papers proposed the same idea of using photochemical reactions to switch
molecules on and off, and localizing the on-molecules with superresolution accuracy.
The proposed photochemical reactions differed, however, ranging from transitions to
dark states in autofluorescent proteins to blinking or bleaching reactions in dyes. In all
cases, the central idea of superresolution imaging with photoswitches is to randomly
select a small subpopulation of all fluorescent molecules present in the sample by
switching them to a bright state, either with an auxiliary laser or by spontaneous
blinking in the right conditions [23]. The concentration of these activated switches
should be low enough that each one of them gives rise to a spot isolated from the
others. Because these spots are well separated from each other, one can find their
centers very accurately. For the brightest molecules, providing many photons, the spot
center can be located to an accuracy of a few tens of nanometers, sometimes even bet-
ter. Another related superresolution method uses the transient binding of fluorescent
molecules to the surface or object to be imaged [24]. Only the immobile molecules
give rise to a measurable spot, whose center determines with superresolution the
source’s location.
2.4
FLUORESCENCE CORRELATION SPECTROSCOPY
We now discuss the case of fluorescent molecules diffusing in a fluid environment,
for example, a drop of solution, a thin liquid film or a Langmuir film, a membrane, a
capillary channel, a cell, and so on. The most important feature of such experiments
is that molecules come into the focal volume and leave it again, and that statistics
over large numbers of molecules are both possible and necessary to extract useful
information. Usually, there are not enough photons emitted during each crossing
of the focal volume for a detailed study of each single molecule; therefore, one
has to accumulate data collected over large numbers of molecules. Microscopy and

FLUORESCENCE CORRELATION SPECTROSCOPY
73
spectroscopy methods in fluid solutions are therefore a special type of single-molecule
methods, in which usually only one molecule is detected at a given time (although
fluorescence correlation also works for small numbers of molecules simultaneously
present in the detection volume), but in which it is in general impossible to focus
on individuals or to compare different individuals. In this sense, these correlation
methods are related to ensemble measurements, and give statistical histograms. In
some applications, however, for example, in multiparameter analyses, statistical cor-
relations between different quantities are obtained. This analysis method is really a
single-molecule method.
2.4.1
Photon Counting Histograms, Burst Analysis
A first way to statistically analyze the fluorescence signal is to plot a histogram of
the signal intensity, that is, the number of counts during a given time interval, for
example, a few tens of microseconds for intense signals [25]. For a single emitter
with constant emission rate, the distribution is Poissonian. In the case of diffusing
molecules, the distribution of the number of molecules is also Poissonian, and the
observed histogram is a convolution of these two distributions. Diffusion during the
emission broadens the distributions even more. Note that this method does not give
the time dependence of fluctuations, unless the time window is varied.
A variant of this method is the analysis of burst sizes. This is particularly useful in
the case of a solution flowing in a capillary. Each fluorescent molecule crossing the
laser focus gives a burst of light with the same maximal intensity, and with a duration
determined mainly by the flow velocity (if the flow is faster than diffusion over the
focus). Burst size analysis is used in fluorescence biomedical assays.
2.4.2
Fluorescence Correlation Spectroscopy
The principle of this method is to keep track of intensity fluctuations of a fluorescence
time trace I(t). The second-order correlation function g(2)(𝜏) used to characterize the
fluctuations is given by
g(2)(𝜏) = ⟨I(t)I(t + 𝜏)⟩
⟨I(t)⟩2
,
where the angular brackets indicate averaging over a very long integration time under
assumption of a stationary fluorescence time trace. In practice, the integration time
is finite and must be taken into account in the evaluation of the correlation function
from real data.
The first application of this method was proposed by Hanbury Brown and Twiss to
analyze the coherence from astronomical light sources. The correlation method was
then applied in the 1970s to analyze thermodynamic fluctuations of fluids via light
scattering. Quasielastic light scattering can be measured in the spectral domain (first-
order correlation of the field), or in the intensity domain (second-order correlation). It
was then realized that one of the fluctuating quantities could be the number of emitting

74
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
molecules in the focal volume, giving rise to FCS [26]. FCS really took off in the early
1990s with the large increase in detection efficiency thanks to avalanche photodiodes
(APDs), microscope optics, and better filters. The method is very powerful and now
widely used for the following reasons:
r It gives access to a wide range of times, often more than eight orders of magni-
tude. This range is limited by the photon count rate and the detector dead time
(but this limitation can be circumvented by cross-correlating the signals of two
detectors) on the short-time side, and only by the experiment’s duration on the
long-time side.
r Because the correlation data are averaged over a long integration time which
can reach minutes or hours, noise is considerably reduced, enabling weak and
subtle effects to be detected.
r FCS is much less sensitive to concentration requirements than single-molecule
measurements. It also works for small numbers of molecules in the focus, at the
expense of a lower contrast, only partly offset by the higher signal. Because the
number of molecules used is high, FCS is also less sensitive to photobleaching.
The illumination doses received by each molecule are much lower than in other
single-molecule observations.
All sources of fluctuations of the fluorescence will give rise to a signature in the
correlation function. Hereafter, we examine the main sources of fluctuations [27].
2.4.2.1
Translational Diffusion
A molecule crossing the laser focus can be seen
as a concentration fluctuation, which will relax according to Brownian diffusion.
From elementary diffusion theory, we know that the mean squared displacement of
a diffusing molecule increases linearly with time, and therefore that the volume in
which the molecule has the highest probability of being found increases as a power
1.5 of time. The correlation function thus decays as 𝜏−1.5 for long times. Because the
initial position cannot be known more accurately than the PSF, the fluctuation decay
is cutoff for short times by the diffusion time within the focus, 𝜏D = Δx2∕4D, where
Δx is the size of the focus and D is the diffusion coefficient. For a fluorescent dye in
water, D ≈1 μm2 ms−1 = 10−9 m2 s−1. One can give a more rigorous treatment based
on a Fourier transform of the diffusion equation, assuming the fluorescence efficiency
to vary spatially as a Gaussian, which is valid in the transverse plane for a Gaussian
laser beam, but only approximately valid in the axial direction. This calculation gives
for the correlation function
g(2)(𝜏) = 1 + 1
N
(
1 + 4D𝜏
Δx2
)−1 (
1 + 4D𝜏
Δz2
)−1∕2
,
where Δx and Δz are the transverse and axial beam waists, respectively, and N is the
average number of molecules in the excitation volume. Einstein related the diffusion

FLUORESCENCE CORRELATION SPECTROSCOPY
75
coefficient to the local viscosity 𝜂around the molecule and its diameter 2R (if the
molecule is a sphere) by
D = kBT
6𝜋𝜂R.
The diffusion coefficient therefore is inversely proportional to the viscosity of the
medium and to particle radius. Big molecules such as proteins will diffuse slower
than small dyes, but the times will scale only linearly with radius.
2.4.2.2
Rotational Diffusion
A molecule absorbs and fluoresces as a dipole,
although not necessarily with the same dipole direction and magnitude. Therefore,
rotation of the molecule has an influence on the absorption of laser light, even when
the exciting laser light is unpolarized, and on the detected fluorescence if there is an
analyzer in the detection path. Rotational diffusion in three dimensions is a complex
process, which obeys a second-order diffusion equation. Its solution involves spher-
ical harmonics and Legendre polynomials, and in general displays multiexponential
relaxation. In the case of a spherical rotor, however, the general solution is much
simplified and the rotational relaxation is single-exponential [27]. It is interesting to
note the difference from the translational case: because the space of available angles
is finite, the equilibrium is reached more rapidly. A reasoning similar to the one for
translational diffusion gives the rotational diffusion constant as a function of viscos-
ity and of the hydrodynamic volume VH of the diffusion object (close to the actual
volume), by the Debye–Stokes–Einstein relation:
𝜏rot = 𝜂VH
kBT .
We see that this time now scales as the volume of the particle, instead of its radius
for translational diffusion. For small dye molecules, rotational diffusion times in
water are on the order of nanoseconds, comparable to fluorescence lifetimes. This
means that the orientation of a molecule may vary considerably during fluorescence.
This can be detected in steady state or in pulsed experiments by measuring fluores-
cence polarization, yielding an observable called fluorescence anisotropy. Rotational
diffusion of small dyes is usually too fast to detect it by FCS in water. Rotational
diffusion can be followed in highly viscous environments, or if the dyes are rigidly
fixed to larger diffusing objects, such as big proteins.
2.4.2.3
Dark State (Triplet)
We now consider variations of the fluorescence inten-
sity due to molecular transitions to a different state, usually a dark one in which fluo-
rescence is suppressed. Figure 2.4 shows as an example the correlation of fluctuations
induced by transitions of a molecule in its triplet state. Chemical or physical changes
in the molecule or its environment can lead to changes in the absorption, or more often
to quenching of the fluorescence. The resulting fluorescence fluctuations are due to

76
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
40
40
20
Contrast
00
1
2
30
20
10
0
1×10−3
0.01
0.1
Time (ms)
1
10
g(2) (  ) −1
τ
Laser power (   W)
μ
FIGURE 2.4
Autocorrelation functions of the fluorescence intensity of a terrylene molecule
in an anthracene crystal for two excitation intensities. The intensity fluctuations are due to
intersystem crossing (ISC) into and from the triplet state. The high contrast of the correlation
indicates that the on-times are much shorter than the off-times. In the present case, intersystem
crossing is induced by coupling to the anthracene matrix.
random passages in the dark state, so that the signal of each single molecule resem-
bles a random telegraph. The correlation function of a random telegraph switching
between two states with intensities I1, I2 at rates k1(k2) to leave state 1 (2) for state 2
(1) is given by [25]
g(2)(𝜏) −1 = k1k2(I1 −I2)2
(k2I1 + k1I2)2 exp[−(k1 + k2)𝜏].
In the special case I2 = 0, the contrast of the correlation becomes k1∕k2. This
expression shows that short excursions to high intensities generate a higher contrast,
and therefore will be more visible, than short excursions to low intensities. We also
note that the decay rate of the correlation is always the sum of the two relaxation rates
of the two states. The two individual rates can be recovered by combining knowledge
of time and contrast.
In general, molecules in fluid solution are subject to the three effects mentioned
above, and the correlation function can be rather complex. In many cases, the flu-
orescence intensity can be written approximately as a product of independently
fluctuating quantities (e.g., translational diffusion, rotational diffusion, and chemical
fluctuations). The correlation function then can be expressed as a product of averages,
that is, as a product of the functions pertaining to each type of fluctuations alone,
I(t) = f(t) × g(t) entails that ⟨I(t)I(t′)⟩= ⟨f(t)f(t′)⟩⟨g(t)g(t′)⟩,

FLUORESCENCE CORRELATION SPECTROSCOPY
77
which means that, if the timescales are well separated, each type of fluctuations can be
recognized on a logarithmic timescale, nearly independently of the other processes.
2.4.2.4
Variants of FCS
The correlation function is a general method, which pro-
vides time-resolved information from a fluctuating signal. It can obviously be applied
to other signals than fluorescence intensity, for example, fluorescence lifetimes, but
also nonoptical signals such as currents in ion channels, for instance. Hereafter, we
mention a few extensions of FCS.
Two different signals can be correlated with one another, for example, two inten-
sities. This is cross-correlation [28]. For example, dual-color FCS is very important
in molecular biology. The fluorescence signals of two different dyes are separated
by a dichroic beam splitter and measured by two detectors. If the two dyes diffuse
together (e.g., because they are attached to two interacting proteins), the translational
diffusion of the complex will appear in the cross-correlation. If the molecules do not
interact, the fluctuations will be independent of each other, and no cross-correlation
will appear.
Fluorescence can be generated by two-photon or three-photon pulsed excitation.
The advantages of this scheme are that there is less scattering of the longer excitation
wavelengths, and that the focal volume is limited by the nonlinearity; therefore,
no pinhole is needed in the detection to obtain a slice of the sample (this effect is
called optical sectioning). Photobleaching is suppressed altogether for out-of-focus
molecules, although it is often enhanced for the molecules in the focus.
Correlation can also be applied to any optical signal, linear or nonlinear. A good
example is Coherent Anti-Stokes Raman Scattering (CARS). Being a coherent pro-
cess, CARS would be very difficult to observe with a single molecule. However, it
can be observed with small particles down to 100 nm in diameter, such as organelles
in cells. Correlation techniques can also be applied to the strongly fluctuating fluo-
rescence signals obtained upon enhancement by metal nanostructures.
2.4.3
Multiparameter Analysis
Fluorescence gives rise to several different observables such as intensity, lifetime,
spectrum, polarization, and FRET. In order to fully exploit the fluorescence signal of
a single molecule, one ideally would like to measure all of them with a time resolution
as high as possible [29]. Because the number of fluorescence photons per unit time
is limited, one has to choose the more relevant parameters and measure these with
suitable time resolution. Determination of a fluorescence lifetime with an accuracy of
a few percentages requires at least a thousand photons. A polarization measurement
(or a crude measurement of the shift of a fluorescence spectrum) can be done with
two detectors and a polarizing beam-splitter (or a dichroic beam splitter) and requires
at least a few tens of photons. Once two (or more) quantities are measured for a
population of molecules which have crossed the excitation volume, the quantities can
be cross-correlated. The correlation may reveal different conformations of proteins,
protein–protein or protein–DNA complexes.

78
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
2.5
FLUORESCENCE EXCITATION SPECTROSCOPY
2.5.1
Zero-Phonon Line and Phonon Wing
Fluorescence excitation is usually equivalent to absorption. It is mostly useful in
low-temperature studies of single molecules, where certain lines become extremely
narrow and intense. In that case, high resolution is more easily reached by excitation
with a narrow single-frequency laser than by other spectroscopic devices such as
spectrographs. Before discussing low-temperature spectroscopy, we need some basic
knowledge of the optical spectroscopy of impurities in solids. We start with the
simple case of only one molecule in a solid at zero temperature. If there was no
coupling to vibrations, the absorption spectrum would consist of a single line, with
a width given by the excited state lifetime. However, the electronic system of a
molecule is in principle coupled to all possible vibration modes in the molecule and
in the solid matrix around it. In the Born–Oppenheimer picture, this coupling arises
from changes in vibrational potential between ground and excited electronic states:
Because the nuclear wavefunctions in the excited state differ from those in the ground
state, exciting the electron modifies the nuclear movement, that is, it can bring about
vibrations in the molecule. Fortunately, one does not have to consider all possible
vibration modes to understand the absorption and fluorescence spectra. Usually only a
few of these modes are coupled strongly to the optical transition, which simplifies the
analysis considerably. We first consider coupling of the electron to a single harmonic
vibration mode. Often, a totally symmetric C–C stretching mode (breathing mode)
is responsible for most of the coupling. We further assume that only the equilibrium
position of the nuclei along the associated vibration coordinate is modified. This case
is called linear vibronic coupling.
We first consider only one deformation mode R of a molecule, with a harmonic
potential Eg(R) in the ground state |g⟩. The minimum of this potential is displaced
by ΔR in the excited state |e⟩with potential Ee(R), without a change in the curvature.
In the crude Born–Oppenheimer approximation, we can write the wavevectors as
products:
|g, m⟩≃|g⟩|m⟩, and |e, n⟩≃|e⟩|̃n⟩,
where ̃n means n quanta in the excited well potential, and m means m quanta in
the ground state potential. One usually calls Franck–Condon amplitude the overlap
amplitude:
f n
m = ⟨̃n | m⟩,
and Franck–Condon factor the square modulus of this overlap:
Fn
m = |⟨̃n | m⟩|2 .

FLUORESCENCE EXCITATION SPECTROSCOPY
79
The Franck–Condon factors represent the intensities of the m-n transitions from the
ground to excited states. The first ones are the 0–0 and 0–1 transitions. These factors
can be expressed analytically from the overlap of harmonic oscillator wavefunctions.
The special case where one of the wavefunctions is the ground vibrational state is
most important. It can be shown easily that the Franck–Condon factors have the
following form:
Fn
0 = 𝜉2n
n! e−𝜉2,
where 𝜉= ΔR
√
MΩ
2ℏis the displacement of the harmonic oscillator upon excitation
in dimensionless units of the spatial spread of the ground state. M is the mass and Ω
the angular frequency of the oscillator.
The absorption spectra of a molecule at low enough temperature are recorded
starting from the ground vibrational state of the ground electronic state, and going to
all vibrational states of the excited electronic state (these states are called vibronic
[vibr-onic] states). The intensities of these absorptions are proportional to the Franck–
Condon factors. A similar argument applies to the emission or fluorescence spectrum,
starting from the ground vibrational state of the excited electronic state, and going
to all vibrational states of the ground electronic state. For linear vibronic coupling,
the intensity distributions of the absorption and emission spectra are symmetric with
respect to the zero-phonon line (ZPL). Absorption and fluorescence spectra are mirror
images of each other with respect to the 0–0 transition.
We now consider the coupling of a molecule to a large number of modes, for
example, to a branch of phonons in a host crystal. Each mode is now weakly coupled
(weak 𝜉) and contributes a weak 0–1 absorption sideband shifted to the blue of the
main absorption line (which corresponds to the 0–0 transition). Neglecting the 0–2
transitions (although they are weak, their number is large, of order N2, with N the
number of modes), we find a broad band on the blue side of the 0–0 line. The 0–0
line, which is common to all modes, is called ZPL, and the blue-shifted broad band
is the phonon wing (PW). The intensity of the ZPL is called the Debye–Waller factor
and is analogous to the intensity of spots in X-ray diffraction, or to the intensity of
the recoil-free structure in M¨ossbauer spectroscopy. This intensity decreases very
rapidly with temperature. One often uses the following approximated formula for the
oscillator strength of the ZPL:
IZPL ≈exp
[
−𝜉2∕tanh
(
ℏΩ
2kBT
)]
.
The intensity of the ZPL therefore decreases exponentially with temperature. In
most organic materials, the intensity of the ZPL becomes negligible at temperatures
higher than 30 K. In diamond, which is a much harder material with a high Debye
temperature (about 1000 K), the ZPL of certain impurities can still be observed at
room temperature.

80
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
In the preceding model, the ZPL is a narrow line, with a linewidth given by the
excited state lifetime. However, several processes contribute to its broadening:
r Higher-order coupling to phonons, notably via the change in curvature of the
potential. The associated process is called quadratic coupling.
r Anharmonicity of the vibrations.
r Slower processes, called spectral diffusion, which arise from drift and jumps of
the resonance frequency due to dynamics in the molecule’s environment.
If spectral diffusion is neglected, the homogeneous linewidth of a molecule is
composed of two contributions, one from the lifetime T1 of the excited state, the other
one from “pure dephasing.” This process, often called “decoherence” in quantum
optics, arises from interactions with phonons and other dynamical modes activated
at finite temperatures. One therefore writes the full width at half maximum of the
optical line, measured in units of angular frequencies 𝜔as
𝛾hom = 1
T1
+ 2
T∗
2
,
where T∗
2 represents the “pure dephasing” time, that is, the lifetime of the coherence
between the two electronic levels, determined by bath fluctuations at relatively low
frequencies (phonons, etc.). At very low temperatures, the pure dephasing rate tends
to zero, since no degrees of freedom are activated any more. In many molecular
crystals, this limit is reached already at a few Kelvins, which means that the optical
width is limited by the lifetime of the excited state only. For an allowed transition the
lifetime is of the order of a few nanoseconds, corresponding to a width of 30 MHz,
or one-thousandth of a cm−1.
When coupling to both phonons and intramolecular vibrations is considered, the
absorption spectrum of a single molecule consists of a single very sharp line at
the lowest frequency, the ZPL of the 0–0 transition. At higher energies, we find the
phonon wing (PW) of the 0–0 transition, the ZPL of 0–1, 0–2, … transitions of various
intramolecular vibrations, followed by their PWs. Note that the vibronic ZPLs are
much broader than the pure electronic ZPL because of the width of the vibrational
levels, determined by the lifetime of the molecular vibration, rarely longer than a
few picoseconds. Each intramolecular vibration mode gives rise to its own vibronic
progression, ZPLs and PWs, but only the pure electronic ZPL is common to all
modes.
2.5.2
Inhomogeneous Broadening
In usual samples, billions of molecules are observed at the same time. Because of
defects and disorder, the electronic transition of each individual molecule is shifted
with respect to the average value. This phenomenon is called inhomogeneous broad-
ening. It is nearly independent of temperature, because it is determined by structure,

FLUORESCENCE EXCITATION SPECTROSCOPY
81
and structure is nearly frozen at low temperatures (at least as long as the matrix is
solid). The size of this inhomogeneous broadening depends strongly on the quality
of the sample and on the way the impurity molecules are embedded in the matrix.
For optical transitions of molecules, the inhomogeneous width varies from about
300 cm−1 (10 THz) in a polymer or in a frozen solution, to less than 3 × 10−2 cm−1
(1 GHz) in unstressed sublimation-grown crystals, which are the best molecular crys-
tals one can grow. A typical value of inhomogeneous broadening for substitutional
impurities in a low-quality molecular crystal is about 10 cm−1. Therefore, in all cases,
the inhomogeneous width is several orders of magnitude broader than the lifetime-
limited linewidth of a single molecule at low temperature. The broad absorption
profiles observed in ensemble experiments result from the superposition of the many
randomly shifted narrow lines of individual molecules.
Comparatively little is known about the inhomogeneous distributions and the asso-
ciated lineshapes. They are determined by the local structure around the impurity.
In many cases, the local structure is relatively well defined and can be considered
as perturbed by many independent defects, such as vacancies, grain boundaries, or
dislocations in crystals. Assuming the contributions of such defects to be approx-
imately equal, and applying the central limit theorem, we then expect a Gaussian
inhomogeneous lineshape. However, other models give different shapes. For exam-
ple, a uniform distribution of defects interacting with an inverse cubic dependence on
distance (dipole–dipole interaction) gives rise to a Lorentzian lineshape. In general,
however, the inhomogeneous distribution is neither Gaussian nor Lorentzian, and
often is asymmetrical.
In crystals, a guest molecule often may occupy several imbedding positions,
called insertion sites. They give rise to multiplets of ZPLs in absorption spectra or
in fluorescence spectra recorded with broad excitation, or excited at high energies
(several thousands of wavenumbers) above the 0–0 transition. Each one of these sites
corresponds to molecules with slightly different atom configurations, giving rise to
slightly different vibrational spectra, lifetimes, and so on. A particularly important
case is that of Shpol’skii matrices, which are crystals of linear n-alkanes. Their crystal
structure is layered with the long axis of the molecules nearly perpendicular to the
layer. Narrow Shpol’skii lines are often obtained when the guest molecule matches
the holes left in the crystal when removing one or a few host molecules. Shpol’skii
systems are very current and useful in molecular spectroscopy.
2.5.3
Hole-Burning [30]
Spectral hole-burning is a nonlinear optical technique applied to large ensembles
of molecules, in which a spectral feature (the hole) results from a modification by
light of the optical properties of a material. Let us consider an ideal hole-burning
experiment at zero temperature. An ensemble of molecules absorbing with very
narrow homogeneous lines (ZPLs) but a broad inhomogeneous distribution of optical
resonance frequencies is irradiated with a monochromatic (or very narrow) laser. In
first approximation, if the laser intensity is not too high, only the resonant molecules
will absorb light. The other ones do not see the laser. Now, an excited molecule

82
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
may undergo a number of possible photophysical and photochemical processes. For
example, the molecule can be temporarily stored in the triplet state, with a different
(usually lower or zero) absorption. During the lifetime of the triplet, the sample will
absorb less at the frequency of the illuminating laser. If an absorption spectrum of the
sample is measured, it will show a hole—a transient spectral hole—at the frequency
of the laser. But a molecule can also enter a much longer-lived dark state. Some of
these product states may have very long (for all practical purposes infinite) lifetimes,
in particular if the molecule undergoes a (photo-)chemical reaction. If the spectral
hole is permanent, one calls the process persistent spectral hole-burning (PSHB).
Molecules in solid matrices at low temperatures can undergo a number of different
processes leading to photochemical and photophysical hole-burning. Photochemistry
may involve electron, proton transfers, or large rearrangements of atoms. Photo-
chemical hole-burning generally leads to very large shifts of the photoproducts,
which means that products of the reaction do not absorb in the same spectral regions
as the educts (in other words, no antiholes can be found in the spectrum). In pho-
tophysical processes, the conformation of neighboring atoms or groups of atoms is
modified by illumination, leading to a shift of the absorption line, much larger than
the homogeneous width, but usually smaller than the inhomogeneous bandwidth. The
resulting antihole is much broader than the hole, and can only be measured after very
deep and broad holes have been burned. Photophysical hole-burning is also called
“light-induced spectral diffusion” in the context of single-molecule spectroscopy.
The experimental method to burn persistent holes is very simple, since the holes
are long-lived. A single tunable narrow-band laser suffices. It is first used to irradiate
the sample somewhere in the inhomogeneous bandwidth. Then, the laser is scanned
to record an absorption spectrum, revealing the hole. Contrast enhancement meth-
ods can be used to detect narrow holes, for example by holography, polarization, or
lock-in detection methods. The hole-burning method is very useful in molecular spec-
troscopy (determination of lifetimes, vibrational spectroscopy), in the spectroscopy
of molecules or ions with spin-multiplet states (via the analysis of satellite holes),
for the study of dynamical degrees of freedom in solids at low temperatures, notably
glasses, for the effect of external perturbations such as electric and magnetic fields,
pressure, and stress. A hole being very narrow, it enhances the sensitivity of these
experiments by several orders of magnitude as compared to bulk experiments. In the
past 20 years, hole-burning materials have also been proposed as optical memories. A
hole-burning sample can be seen as a photographic plate sensitive to several millions
of different colors. However, practical difficulties, such as the low temperature and
the problem of burning while reading, have limited their applications so far. Current
research aims at using such media as analyzers and processors of optical informa-
tion, which could treat many different wavelengths in a massively parallel way, or as
nonlinear media for photonics applications [31].
2.5.4
Single-Molecule Spectroscopy
We now consider a host–guest system in which molecules are very stable, that is, in
which hole-burning is inefficient or unlikely. If we focus our laser on a very small

FLUORESCENCE EXCITATION SPECTROSCOPY
83
30
20
0
10
(a) N = 10, ÷ 1
(b) N = 102, ÷ 5
(c) N = 103, ÷ 50
(d) N = 104, ÷ 500
−4
−2
0
Frequency (units of ΓI)
2
4
Absorption (a.u.)
FIGURE 2.5
For a large number of molecules (d: 10,000 molecules), the absorption spectrum
appears smooth and reproduces the inhomogeneous band profile. At lower and lower numbers
of molecules, spectral fluctuations start to appear, until single-molecule zero-phonon lines are
completely resolved (a, here for 10 molecules). Figure after Reference 36. The natural width
of the zero-phonon lines has been greatly exaggerated for illustration purposes.
volume of sample and move it spatially, we will start to see statistical fluctuations
of the number of molecules in resonance with the laser. If we scan the frequency
of our laser at a fixed spot in the sample, we will see characteristic fluctuations of
the optical absorption, or of the total fluorescence of the sample. These fluctuations
have been first detected by Moerner in 1987 and are called statistical fine structure
[32]. Analysis of this structure provides the homogeneous width of the molecules.
In the same way that FCS reveals the average diffusion time in an ensemble of
molecules, the autocorrelation of the statistical fine structure provides a peak whose
width is related to the homogeneous linewidth. As illustrated in Figure 2.5, if we
further reduce the focal volume and/or the concentration, the relative amplitude of
the statistical fine structure increases, as it scales with the inverse square root of the
average number of molecules in the focus. In the regime where the average number
of molecules is less than unity, the spectrum ideally consists of a set of resolved sharp
lines on a low background. Each single peak corresponds to the absorption line of a
single molecule.
The first optical signal of a single molecule has been detected in 1989 by Moerner
and Kador with a complex method, involving a double frequency modulation (laser
frequency and molecular resonance frequency were modulated at different frequen-
cies) of the absorption of a thin sample of pentacene in a para-terphenyl crystal.
Although this method is very sensitive, its application to a single molecule is difficult
because of photon noise and of optical saturation. The absorption signal is measured
as a weak variation of the intensity of the transmitted beam. In order to reduce the rel-
ative photon noise on that beam, the intensity has to be large, and as a consequence,

84
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
the molecular signal saturates (in other words, the absorption cross-section of the
molecule decreases). Orrit and Bernard showed in 1990 that a fluorescence excitation
method provides a much better signal/noise ratio. Since then, the fluorescence excita-
tion method has been improved and generalized. It was first used for low-temperature
spectroscopy, then for microscopy at room temperature.
2.5.5
Scope of Low-Temperature Single-Molecule Spectroscopy
We will not review the wealth of new results brought by single-molecule spectroscopy
at low temperatures during the past 20 years, as such reviews have been published
already [33–36]. We only give a brief summary of the different types of experiments
that have been done with fluorescence excitation of single molecules.
2.5.5.1
External Field Effects
Just as with spectral hole-burning, the narrow lines
of single molecules provide a vastly enhanced sensitivity of optical signals to external
perturbations such as electric or magnetic fields and mechanical pressure and stresses.
The unique advantage of single molecules for such probing of external actions is that
a molecule reports local conditions in its surroundings, free of ensemble averaging.
This can be useful in many cases in nanoscience, for example, to probe trapped
charges in organic conductors [37,38].
2.5.5.2
Optically Detected Magnetic Resonance and Quantum Optics
Electron
paramagnetic resonance (EPR) is a powerful technique to explore the electronic
structure of materials. However, this technique requires billions of spins to pro-
vide measurable magnetic resonance signals. Optically detected magnetic resonance
(ODMR) transfers the sensitivity of optical measurements to the magnetic domain
and enhances to sensitivity of EPR by orders of magnitude. The optical detection of a
single molecule has made it possible to detect a single electron spin with high signal-
to-noise ratio [39, 40]. This experiment has been recently extended to single color
centers in diamond, which present paramagnetic ground and excited states. Using the
electron spin as a probe for neighboring nuclear spins, several groups have proposed
a broad range of quantum optical manipulations on nuclear spins, ultimately detected
through changes of optical signals [41–43].
A single molecule is also a well-defined electronic system, in which the quantum
state can be manipulated. Single quantum systems can be applied to deliver single
photons on command [44] for a variety of experiments, from near-field imaging [45]
to photon number interference [46].
2.5.5.3
Spectral Diffusion
Spectral diffusion of a single-molecule line results
from changes of the molecule’s optical resonance frequency. These changes are
themselves caused by slow motion or transitions in the molecule’s neighborhood, with
ranges varying from nanoseconds (comparable to the fluorescence lifetime) to minutes
or even longer. Some examples of spectrally diffusing molecules with widely different
dynamics in the same samples are presented in Figure 2.6. In such a wide-spread time
window, many processes can contribute to spectral diffusion. Single molecules enable

FLUORESCENCE EXCITATION SPECTROSCOPY
85
−10
−5
−5
−5
0
5
10
(a)
(b)
(c)
0
Frequency (GHz)
5
10
10
5
0
300
150
Time (s)
10
FIGURE 2.6
Example of spectral diffusion traces measured for single terrylene molecules
in a 2,3-dimethylanthracene crystal [47]. The dynamics is attributed to methyl group con-
formational changes. Figure reproduced from Reference 47 with permission from © 2012
Wiley-VCH Verlag.
a study of these different processes on a truly local scale, without ensemble averaging.
Among the most studied degrees of freedom have been the tunneling systems of
glasses and polymers at cryogenic temperatures. Single molecules signals show
random telegraph blinking assignable to switching of individual two-level systems
[48], as well as more complex behavior [49,50]. Another well-studied example is the
flipping of the central phenyl group of para-terphenyl in a crystal at low temperature,
studied through the spectral diffusion of probe molecules such as pentacene [51]
or terrylene [52]. These studies have proved that host molecules at crystal defects
such as domain walls are particularly prone to conformational changes. Flipping
phenyl groups lead to nearly independent two-level systems and to a specific pattern
of spectral diffusion for the probe molecules [53, 54]. Many other slow degrees of
freedom in low-temperature matrices can lead to spectral diffusion, such as subtle
reorientations of methyl groups [55], or correlated molecular motions characteristic
for incommensurate phases [56].
2.5.5.4
Interacting Single Molecules
In regular single-molecule experiments,
excited molecules are very far from each other, so as to achieve spatial and spec-
tral selection. Sometimes, however, molecules can be close enough to significantly
interact and give rise to new coupled states. One such example is given by the chro-
mophores of individual antenna complexes studied by Van Oijen et al. [57]. These
authors found in the same individual complex weakly coupled states from the B800
ring as well as strongly coupled states demonstrating delocalized exciton states in the
B850 ring. These systems have been recently studied at room temperature by ultrafast
single-molecule spectroscopy and found to exhibit quantum coherence even at room
temperature [58]. Applying variable electric fields to single molecules gives access to
a wider range of frequencies, and may help reaching the resonance condition for two
neighboring molecules. This effect was used by Hettich et al. [59] to study the non-
linear optical response of two terrylene molecules in a crystal and demonstrate their
excitonic coupling. Yet another possibility to enhance the probability to find coupled

86
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
chromophores is to link them covalently. This technique has allowed Basch´e’s group
to study energy transfer between single donor and acceptor moieties [60].
This short overview shows the wide scope of single-molecule experiments at low
temperature. Much remains to be done, in particular concerning the interaction of
single molecules with plasmonic structures. For these experiments, the main advan-
tage of low-temperature experiments is that a large number of distinguishable single
molecules can be coupled to the same plasmonic structure.
2.6
OTHER DETECTION METHODS
In this section, we review the main techniques used to detect individual nano-objects,
in particular metal nanoparticles, through their effect on a probing laser beam. The
object or particle can be observed in dark field, that is, with a high contrast in a
direction where no intensity is scattered in the absence of the particle. It can also
be detected in bright field, by comparing scattered intensities with and without the
particle in the beam. This latter scheme amounts to an interferometric detection of the
particle. Finally, pump-probe methods exploit a change of the scattered probe wave
which is conditional to pump excitation. Therefore, these methods have the power
to selectively detect absorbing (e.g., metal) nanoparticles against any background of
nonabsorbing (dielectric, nonmetallic) scatterers. These different optical techniques
have become widespread in the past few years and given rise to a large body of
work. Here, we only mention the main advances, stressing the underlying detection
principles.
2.6.1
General Considerations on Signal, Background, and Noise
To detect a small object, usually less than 100 nm in size, in the optical far field, one
should optimize its interaction with the light beam. This is achieved by placing the
object at the focus of a spherical wave or of a Gaussian beam, or very close to this
focus. Indeed, the sensitivity optimum is not always exactly at the wave focus. To a
good approximation, a small object in an optical field behaves as an oscillating dipole,
radiating a scattered wave. Superposition of this scattered wave with the incoming
wave leads to measurable intensity changes in different spatial regions. Hereafter, we
briefly discuss the three main ways to measure these changes and to detect and study
individual particles.
2.6.1.1
Dark Field
The relative change of intensity is the strongest in directions
where the incident wave, in the absence of a particle, would not produce any intensity.
The wave scattered by the particle will thus contribute a nonzero intensity appearing
on a dark background. That principle is called dark-field scattering. For ideal condi-
tions, it can in principle provide an arbitrarily large signal-to-background ratio. This
property is very similar to that of fluorescence, because no fluorescence background
exists where no fluorescent object is present. In practice, however, these ideal low-
background conditions are very difficult to achieve for Rayleigh scattering, because

OTHER DETECTION METHODS
87
small defects in the substrate, impurities, or small structures also scatter light in
all directions. This problem is particularly serious in biological cells, because of
scattering by organelles, vesicles, and other small cell structures.
2.6.1.2
Bright Field
The wave scattered by a small object also presents an ampli-
tude in the direction of the transmitted wave, and thereby interferes with it. In the
forward direction, the interference always reduces the intensity of the incident wave,
leading to extinction of the incident wave. The relation between extinction and the
scattering amplitude is given by the optical theorem in scattering theory [61]. The
optical theorem expresses the conservation of energy: the energy removed from the
incident wave is either locally absorbed, or scattered away. There, it appears as new
scattered waves and as a simultaneous weakening of the transmitted wave. Local
absorption can give rise to heat release or to re-emitted radiation at different (usually
lower) frequencies, that is, as fluorescence or photoluminescence. A similar interfer-
ence effect occurs in other propagation directions of the incident wave, particularly
when this wave is reflected by an interface, or scattered by a bigger object. The
following reasoning thus applies to those cases as well.
Let us discuss the signal/noise ratio in such an extinction experiment. The detected
intensity ΔI change results from a sum of a reference (incoming) field r and of a
small scattered field s:
ΔI = |r + sei𝜙|2 −|r|2 ≈2Re(r∗sei𝜙),
where 𝜙is the phase difference between the incident field and the scattered field at
a large distance, taking all phases into account (including propagation, Gouy shifts,
and phase shift due to the polarizability of the particle). For example, a pure absorber
placed at the focus of a plane wave has a dipole in quadrature with the incident
field. The spherical wave it scatters introduces an additional Gouy phase of 𝜋∕2. The
combination of these shifts gives rise to destructive interference between transmitted
and scattered waves, attenuating the incident wave.
In dark field scattering, there is no reference field, no background, and the signal-
to-noise ratio can in principle be arbitrarily large. Note, however, that the intensity
scattered by a small object is very weak and is easily drowned by background from
experimental imperfections.
In a bright-field experiment, the scattered intensity |s|2 is negligible for a small
object. The signal arises mostly from the interference term 2rs cos 𝜙(for real reference
and scattered fields), which has to be detected against the background |r|2 of the
intensity transmitted or reflected by the substrate. This bright-field difference signal
therefore scales as the amplitude of the scattered field, that is, as the volume of the
scatterer. For measurements limited by photon noise, the noise on the background
scales as the square root of the background, therefore as the amplitude of the reference
field, just as the signal does. Therefore, the signal-to-noise ratio is independent of the
intensity of reference field. This useful property allows one to adapt the experimental
configuration to available sources and detectors. Choosing a weak reference field (e.g.,
a weak reflection from the interface between two nearly index-matched media [62])

88
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
is a convenient means to suppress laser noise (which scales as |r|2) without losing
signal/noise ratio. A similar reduction can be achieved by means of an interferometer
[63], but at the cost of a more complex setup and of more difficult adjustments.
2.6.1.3
Absorption Detection
The dissipation of light energy by the single object,
due to absorption processes, may give rise to a number of secondary effects, including
local heat production, luminescence, and other emissions. These effects can also be
used to detect true absorption, as opposed to extinction, and provide a means to
distinguish metal nanoparticles from dielectric scatterers. As we have seen above,
fluorescence or photoluminescence is a highly selective and sensitive way to detect
absorbers, but heat-induced, photothermal and pump-probe effects are also useful
to detect metal particles with a high selectivity. The signal-to-noise ratio in such
experiments will depend on the sources of background, which can be very low in
luminescence or photothermal measurements.
In the next sections, we successively discuss these three big classes of methods
with experiments from the past few years on small metal nanoparticles.
2.6.2
Dark-Field Scattering, Total Internal Reflection
Dark-field microscopy was first used by Zsigmondy around 1900 to study the scatter-
ing of suspensions of gold nanoparticles with his “ultramicroscope.” His observations
helped clarify the structure of colloidal suspensions and convince the last skeptics
of the reality of atoms and molecules [64]. In 1998, Feldmann’s group published
first experiments in near-field [65], while Yguerabide et al. [66] studied individual
nanoparticles in far-field in view of labeling in biological applications. Studying the
dark-field scattering of silver nanoparticles, Schultz pointed out the high interest of
nanoparticles of different sizes and shapes for multicolor labeling [67].
Light scattered by a small object can be detected against a dark background in a
variety of ways, often also used in fluorescence microscopy. Microscope companies
offer dark-field condensers with associated objectives to specifically observe scat-
tering objects. Alternatively, high-numerical-aperture objectives can accommodate
annular illumination with detection in the center of the pupil, or vice-versa. Total
internal reflection offers a convenient way to illuminate particles when they are situ-
ated close to an interface on the low-index side. Scattered light can be observed on
that same side with very low background. The main advantage of dark-field scattering
is its ease of operation, because it only requires a standard microscope with a regu-
lar objective. Moreover, it also allows for wide-field imaging and makes it possible
to follow hundreds of particles in parallel at the same time. Scattering spectra are
observed directly, without any need for background subtraction or post-processing.
Hereafter, we mention some recent results obtained by dark-field scattering of metal
nanoparticles. Soennichsen et al. [68] used dark-field scattering images to obtain the
spectra of nanorods and nanospheres of various shapes and sizes. These spectra are
free from the large broadening resulting from inhomogeneity in ensemble experi-
ments. They demonstrated a spectacular reduction in plasmon width for rods and

OTHER DETECTION METHODS
89
attributed the main channel of plasmon damping to relaxation toward electron–hole
pairs for small particles. They recognized the importance of radiative damping for
large particles. The radiative channel broadens the plasmon resonance for large sizes,
whereas surface-induced broadening becomes noticeable for small sizes, 10 nm and
lower, so that a minimum in broadening occurs at about 30 nm.
Dark-field scattering in combination with scanning electron microscopy (SEM)
reveals the plasmon bands of more complex assemblies of nanoparticles such as the
pairs of nanorods studied by Funston et al. [69]. Various relative positions of the rods
give rise to hybridized or coupled plasmon bands, in agreement with calculations. The
polarization of scattered light can be obtained by a rotating polarizer. With this tech-
nique, a large number of silver and gold nanoparticles can be monitored in parallel
during their growth. Dark-field scattering is a very versatile method allowing many
correlated measurements in parallel thanks to its ease of operation. Its limitation,
however, is that the scattered intensity scales as the squared volume of the nanoparti-
cles. Therefore, for small particles, dark-field scattering is easily overcome by signals
from dielectric impurities, substrate roughness, or even thermodynamic fluctuations
of the medium. Its application to small objects requires very clean substrates, and
often reaches its limits around 30 nm for gold nanospheres under standard conditions
of optical quality and cleanliness.
2.6.3
Absorption, Extinction, Interference-Based Methods
The reduction in transmitted intensity due to a small object is called extinction.
As compared to the dark-field scattering signal, the extinction signal scales as the
amplitude of the field scattered by the object, instead of its intensity. However, this
signal has to be detected against a strong background of transmitted light. Similarly, a
change in reflection intensity scales with the field amplitude and therefore the volume
of the particle, but has to be detected against the background of light reflected by the
interface.
The first discussion of this interference effect for a single molecule in reflection
was given by Plakhotnik and Palm [70] for a low-temperature spectroscopy exper-
iment. The spectral shape of the interference line can change from absorptive to
dispersive, depending on the phase difference between the two fields. This differ-
ence was varied by choosing molecules at different distances from the interface. The
authors proposed to extend this technique to nonfluorescent objects. Lindfors et al.
[62] studied the plasmon resonance of a single gold nanoparticle immobilized on
a surface and surrounded by immersion oil with a similar reflection technique. For
large particles, the scattered intensity dominated the weak reflection from the nearly
index-matched glass–oil interface, but for small particles the reflection was stronger.
Destructive interference of the scattered field led to reduced reflection, enabling
detection of particles down to 10 nm in diameter against the weak reflected back-
ground. The reflection of a water–glass interface was used as the reference signal by
Jacobsen et al. [71] to demonstrate fast and wide-field detection of gold nanoparti-
cles immobilized on a glass surface but in contact with an aqueous phase, which is
interesting for the detection of biomolecules. Metal nanoparticles were distinguished

90
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
from dielectric scatterers such as particles or substrate defects by their plasmon res-
onance probed at two different wavelengths. This scattering method in transmission
or reflection has recently been improved so as to reach the threshold of a single
molecule’s absorption [72].
The main limitation of the scattering method, however, is not its sensitivity. Dif-
ferent principles of subtraction have been used to extract the small extinction signal
from the strong transmitted or reflected background. A more serious obstacle to this
method of detection of small particles is the lack of specificity of their scattering
signals against those from other scatterers. These can be dielectric particles in sus-
pension, impurities, corrugations or refractive index inhomogeneities of the substrate,
or organelles in biological cells. To distinguish those from metal nanoparticles, one
needs specific signals arising from the absorption itself instead of the extinction. As
explained in the next section, pump-probe methods do provide such specificity.
2.6.4
Pump-Probe and Photothermal Detections
Pump-probe detection techniques achieve selectivity by measuring only that part of
the scattered probe signal which is conditional to the presence of the pump. Whereas
this usually entails a significant loss in signal amplitude, it has the considerable benefit
that only resonantly absorbing objects, such as metal nanoparticles, semiconductor
nanocrystals, or dye molecules, have sufficient nonlinear susceptibilities to produce
a sizable signal. Cross talk between pump and probe beams induced by the nonlinear
𝜒(3) susceptibility of transparent dielectrics is negligible at the intensities used in
microscopy of absorbing objects. In pump-probe techniques, one measures the change
of intensity of the transmitted or reflected probe beam caused by a pump beam. The
wave measured is a superposition of the scattered probe field with the transmitted
or reflected probe beam, as explained above (Section 2.6.1.2). Now, however, the
subtracted intensity is not that of the probe beam without the particle, but rather
the probe intensity without pump excitation. This subtraction is usually done by
modulating the pump power at high frequency (often around 1 MHz) and by detecting
the change in probe intensity with a lock-in amplifier. Any nonlinearity of the 𝜒(3)
type will give rise to a change of scattered probe field due to the presence of the pump,
by a signal proportional to the probe amplitude and to the pump intensity (or squared
amplitude). The physical origin of the nonlinearity can be the heat dissipation due
to pump absorption (photothermal effect), an electronic response (electronic pump-
probe, ground-state depletion), or a vibrational response (vibrational pump-probe,
stimulated Raman scattering).
2.6.4.1
Pump-Probe
For excitations at the plasmon frequency of a metal parti-
cle, the pump mainly excites conduction electrons. The energy dumped into these
electronic degrees of freedom launches transient responses of, in turn, the collective
plasmon oscillations, then incoherent motions of conduction electrons, then phonons
and vibrations of the nanoparticle as a whole. Finally, the energy is dissipated in the
environment and diffuses away from the particle. The activation of these degrees of
freedom takes place on different timescales, determined by their coupling strengths.

OTHER DETECTION METHODS
91
Plasmonic excitations are damped in less than 10 fs, the electron bath equilibrates
with the lattice within a few picoseconds, the nanoparticle vibrations are damped in
some hundreds of picoseconds, and heat dissipation into the environment proceeds
between nanoseconds and microseconds, depending on the length scale considered.
According to which optical technique is used and to the timescale, these different
degrees of freedom can contribute to a pump-probe signal.
We first discuss pulsed experiments with 200 fs pulses. This pulse duration is too
long to resolve the nonlinearity of plasmon oscillations, but it clearly shows a prompt
optical response of hot electrons, which decays within a few ps. This response arises
from the transient distribution of hot electrons and holes, which causes changes in
real and imaginary parts of the optical permittivity. It is strong enough to allow for
detection of gold nanospheres down to 10 nm in diameter. With a laser repetition rate
of 80 MHz, however, the signal-to-noise ratio is much lower than that of photothermal
detection. Indeed, because of the low duty cycle, much less photons are absorbed and
used as probes in the pulsed experiments. The advantage of these pulsed experiments,
however, is that they give access to the time-resolved response up to 12 ns, and thereby
probe vibration frequencies and damping rates. Single-nanoparticle studies have the
unique advantage of revealing full distributions of shape and size as well as rare cases
of composed particles such as dumbbells, and of providing access to intrinsic damping
mechanisms, which are often hidden by inhomogeneity in ensemble measurements.
Optical probing of vibrations of single metal nanoparticles has been recently reviewed
[74]. In metal nanospheres, because the isotropic heating process does not break
the spherical symmetry, only radial vibration modes are efficiently excited. The
fundamental normal mode and higher-order ones (overtones) can be observed [75].
A nonradial extensional mode appears in the pump-probe trace when the spherical
symmetry is broken, either by the shape in slightly elongated particles, or by the
contact with another spherical particle in pairs of touching nanospheres (dumbbells)
[76]. In the dumbbell, optical excitation also leads to low-frequency oscillations
of the centers of the two component particles around their equilibrium positions,
resembling the stretching mode of a diatomic molecule. Transient vibrational traces
of gold nanorods [77] mainly show the breathing mode and the extension mode
at a much lower frequency, as the example of Figure 2.7 shows. The ratio of their
frequencies depends on the rod’s shape and gives information about the elastic moduli
of the single-crystal rods. Silver nanocubes were recently studied by Staleva et al.
[78]. Again, the fundamental breathing mode is the dominant one in the traces, with
sometimes nonradial modes at lower frequency, possibly arising from a deviation
from cubic symmetry. The origin of the damping of mechanical vibrations of gold
nanoparticles was explored by experiments in optical traps [79], and confirmed the
importance of intrinsic damping [80] pointed out in ensemble experiments on high
quality samples [81].
2.6.4.2
Photothermal Contrast
In photothermal microscopy, the pump beam is
again absorbed by the particle to be detected, whereas the probe beam monitors the
change of index of refraction in the environment of the absorber. As the probe
wavelength is usually chosen out of the absorption range of the particle, it is

92
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
15
(a)
10
5
0
0.0
0.1
0.2
0.3
Time delay (ns)
0.4
0.5
0.6
80
40
1)
2)
Frequency (GHz)
Spectral density (a.u.)
0
(10−4)
δ
FIGURE 2.7
A pump-probe time trace of a single gold nanorod (25 nm × 60 nm) showing
the fast electronic response in the first few picoseconds, followed by coherent oscillations of
the diameter (high frequency) and of the length of the rod (low frequency). Reproduced from
Reference 73 with permission from © 2012 American Chemical Society.
advantageous to increase the probe intensity as much as possible to reduce pho-
ton noise. The heating beam is modulated at a high frequency in the MHz range, and
the ensuing variations of scattered intensity of the continuous-wave probe beam are
detected with a lock-in amplifier. The photothermal signal thus makes use of a weak
effect, the change of refractive index of the surrounding medium with temperature,
but as it accumulates the contributions of many photons, both for pumping and for
probing, it can still achieve excellent signal-to-noise ratios. The method has been
implemented in microscopy in different ways, first on silver nanoparticles in free
solution by Matawari et al. [82], then in a polarization-sensitive configuration by
Boyer et al. [83] in a confocal microscope. Berciaud et al. improved on this method
and pushed the sensitivity to gold nanoparticles 1.4 nm in diameter [84]. The pho-
tothermal method has been adapted to wide-field microscopy by Absil et al. [85] by
exploiting image subtraction at video rate and recently pushed to single-molecule
sensitivity by Gaiduk et al. [86] (see Fig. 2.8).
Photothermal detection of small gold nanoparticles requires the absorption of many
photons and, by its very principle, an associated heating. The ensuing temperature
rise at the particle may be a limitation for probing application in biology or soft
matter studies. Moreover, even temperatures as low as 100◦C can cause reshaping
of gold nanorods and of complex nanoparticles. Therefore, a compromise must be
found between particle size, integration time, and maximum allowed temperature
rise. In glycerol, an absorbed power of 3 nW can be detected with a signal-to-noise
ratio of 8 in an integration time of 10 ms, corresponding to a temperature increase
of less than 0.1 K for a 20-nm gold particle [87]. Such high sensitivities enable the
photothermal detection of single nonfluorescent molecules, as shown in Figure 2.8.
In water, this signal is reduced by a factor of about 5.
2.6.4.3
Electronic Response and Ground-State Depletion
This detection tech-
nique closely resembles photothermal contrast, but relies on an electronic nonlinear-
ity instead of a thermal one. A pump beam is modulated at high frequency (MHz or

CONCLUSION
93
2    m
μ
15
5
0
20
Time (s)
40
(v)
(iv)
(iii)
(ii)
(i)
0.05 nm2
SNR
(a)
(b)
FIGURE 2.8
(a) Photothermal image showing signals originating from single absorbing
molecules. The right panel (b) shows time traces recorded on some of the spots, showing one-
step photobleaching after several seconds of illumination at high intensity [86]. Reproduced
from Reference 86 with permission from © 2010 AAAS.
higher) and the modulation of a confocal probe beam is detected with a lock-in ampli-
fier. However, the probe frequency is now chosen in an absorption band of the object
to be detected, which may be the same as the one addressed by the pump. The pump
and probe frequencies should be far enough apart to be efficiently separated before
detection of the probe. For a molecule, the depletion of ground state population by
the pump obviously leads to a decrease of probe absorption. For a metal particle, the
hot electrons and holes created by the pump change the optical response to the probe
in a manner similar to the time-resolved experiments described above. Chong et al.
[88] used this method to image immobilized gold nanoparticles (diameter 20 nm).
Moreover, the sensitivity of the technique was enough to detect single Atto647 dye
molecules. This technique will be a useful alternative to photothermal microscopy in
cases where heating is undesirable, or in media where temperature-induced refractive
index changes are small.
2.7
CONCLUSION
In this review chapter, we have described the main optical methods used for detection
and studies of single small objects, from molecules to metal nanoparticles. Fluores-
cence is still the workhorse method of single-molecule detection and has been recently
augmented by fascinating applications in superresolution imaging. Fluorescence-
related methods have been immediately extended to a broad range of emitting objects,
including semiconductor nanocrystals [89] and metal nanoparticles [90]. Although the
photoluminescence yield of gold nanoparticles is very low, of the order of 10−6, their
coupling to light is dramatically improved by their plasmonic properties, generating
very large absorption cross sections. Therefore, the fluorescence (or photolumines-
cence) signals of plasmonic particles can be comparable or even stronger than that

94
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
of fluorescent molecules. However, gold particles are much more stable and durable
than molecules. Their main disadvantage is their rather large size.
In addition to fluorescence, a number of new optical signals have been proposed
during the last few years, of which we reviewed some here. The most direct ones
are based on scattering in dark or bright field in linear optics. They are very gen-
eral and experimentally simple, but often lack sensitivity and selectivity for the
objects of interest. By relying on nonlinear spectroscopy, more refined methods can
enhance selectivity through one or several resonant steps. One example is photother-
mal detection, which provides contrast for absorbing objects only, or the closely
related methods of ground state depletion and stimulated Raman scattering. Pump-
probe spectroscopy gives time-resolved information, which can be exploited to obtain
phase-sensitive information [91].
Among the new developments that were hardly mentioned in this review is the
coupling of optically active nano-objects to plasmonic nanostructures [92]. Large
field enhancement in the vicinity of metallic nanostructures can lead to spectacular
enhancement of the coupling to light of molecules and other nano-objects, in absorp-
tion as well as in emission. These lead to surface-enhanced Raman scattering of single
molecules [93], tip-enhanced Raman scattering [94, 95], enhanced fluorescence by
bowtie antennas [96], nanorods [97], or antennas in boxes [98]. Enhanced electric
fields in metallic nanostructures will certainly raise many fundamental questions and
find a broadening range of applications in the coming years.
ACKNOWLEDGMENTS
We acknowledge financial support by ERC (Advanced Grant SiMoSoMa). The work
presented in this chapter was done within the research program of the “Stichting
voor Fundamenteel Onderzoek der Materie,” which is financially supported by the
Netherlands Organization for Scientific Research (NWO).
REFERENCES
[1] J. Perrin, “La Fluorescence,” Ann. Phys 9, 133 (1918).
[2] G. Binnig, H. Rohrer, and C. Gerber, “Surface studies by scanning tunneling microscopy,"
Phys. Rev. Lett. 49, 57–61 (1982).
[3] G. Binnig and H. Rohrer, “Scanning tunneling microscopy—from birth to adolescence,”
Rev. Mod. Phys. 59, 615–625 (1987).
[4] L. Novotny and B. Hecht, Principles of Nano-Optics (Cambridge University Press,
2006).
[5] J. T. Fourkas, “Rapid determination of the three-dimensional orientation of single
molecules,” Opt. Lett. 26, 211–213 (2001).
[6] D. P. Fromm and W. E. Moerner, “Methods of single-molecule fluorescence spectroscopy
and microscopy,” Rev. Sci. Instrum. 74, 3597–3619 (2003).

REFERENCES
95
[7] T. Schmidt, G. J. Sch¨utz, and H. J. Gruber, “Local stoichiometries determined by counting
individual molecules,” Anal. Chem. 68, 4397–4401 (1996).
[8] G. J. Sch¨utz, W. Trabesinger, and T. Schmidt, “Direct observation of ligand colocalization
on individual receptor molecules,” Biophys. J. 74, 2223–2226 (1998).
[9] L. A. Deschenes and D. A. Vanden Bout, “Heterogeneous dynamics and domains in
supercooled o-terphenyl: a single molecule study,” J. Phys. Chem. B 106, 11438–11445
(2002).
[10] R. Zondervan, F. Kulzer, G. C. G. Berkhout, and M. Orrit, “Local viscosity of supercooled
glycerol near Tg probed by rotational diffusion of ensembles and single dye molecules,”
Proc. Natl. Acad. Sci. USA 194, 12628–12633 (2007).
[11] T. Ha, T. Enderle, D. S. Chemla, P. R. Selvin, and S. Weiss, “Single molecule dynamics
studied by polarization modulation,” Phys. Rev. Lett. 77, 3979–3982 (1996).
[12] F. Cichos, C. von Borczyskowski, and M. Orrit, “Power-law intermittency of single
emitters,” Curr. Opin. Coll. Interf. Sci. 12, 272–284 (2007).
[13] R. Zondervan, F. Kulzer, S. B. Orlinskii, and M. Orrit, “Photoblinking of rhodamine 6G
in poly(vinyl alcohol): radical dark state formed through the triplet,” J. Phys. Chem. A
107, 6770–6776 (2003).
[14] J. Vogelsang, R. Kasper, C. Steinhauer, B. Person, M. Heileman, M. Sauer, and P.
Tinnefeld, “A reducing and oxidizing system minimizes photobleaching and blinking of
fluorescent dyes,” Angew. Chem. Int. Ed. 47, 5465–5469 (2008).
[15] H. P. Lu and X. S. Xie, “Single-molecule spectral fluctuations at room temperature,”
Nature 385, 143–146 (1997).
[16] R. Zondervan, F. Kulzer, M. A. Kol’chenko, and M. Orrit, “Photobleaching of rhodamine
6G in poly(vinyl alcohol) at the ensemble and single-molecule levels,” J. Phys. Chem. A
108, 1657–1665 (2004).
[17] M. Davies, C. Jung, P. Wallis, T. Schnitzler, C. Li, K. M¨ullen, and C. Br¨auchle, “Pho-
tophysics of new photostable rylene derivatives: applications in single-molecule studies
and membrane labelling,” Chem. Phys. Chem. 12, 1588–1595 (2011).
[18] A. M. van Oijen, J. K¨ohler, J. Schmidt, M. Muller, and G. J. Brakenhoff, “3-Dimensional
super-resolution by spectrally selective imaging,” Chem. Phys. Lett. 292, 183–187.
[19] A. V. Naumov, A. A. Gorshelev, Y. G. Vainer, L. Kador, and J. K¨ohler, “Far-field
nanodiagnostics of solids with visible light by spectrally selective imaging.” Angew.
Chem. Int. Ed. 48, 9747–9750 (2009)
[20] E. Betzig, G. H. Patterson, R. Sougrat, O. W. Lindwasser, S. Olenych, J. S. Bonifacino, M.
W. Davidson, J. Lippincott-Schwartz, and H. F. Hess, “Imaging intracellular fluorescent
proteins at nanometer resolution,” Science 313, 1642–1645 (2006)
[21] M. J. Rust, M. Bates, and X. W. Zhuang, “Sub-diffraction-limit imaging by stochastic
optical reconstruction microscopy (STORM),” Nat. Methods 3, 793–795 (2006).
[22] S. T. Hess, T. P. K. Girirajan, and M. D. Mason, “Ultra-high resolution imaging
by fluorescence photoactivation localization microscopy,” Biophys. J. 91, 4258–4272
(2006).
[23] M. Heilemann, S. van de Linde, A. Mukherjee, and M. Sauer, “Superresolution imaging
with small organic fluorophores,” Angew. Chem. Int. Ed. 48, 6903–6908 (2009).
[24] A. Sharonov and R. M. Hochstrasser, “Wide-field subdiffraction imaging by accumulated
binding of diffusing probes,” Proc. Natl. Acad. Sci. USA 103, 18911–18916 (2006).

96
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
[25] M. Lippitz, F. Kulzer, and M. Orrit, “Statistical evaluation of single nano-object fluores-
cence,” Chem. Phys. Chem. 6, 770–789 (2005).
[26] D. Magde, E. L. Elson, and W. W. Webb, “Fluorescence correlation spectroscopy. 2. An
experimental realization,” Biopolymers 13, 29–61 (1974).
[27] J. Widengren and U. Mets, “Conceptual basis of fluorescence correlation spectroscopy
and related techniques and tools in bioscience,” in Single Molecule Detection in Solution,
edited by C. Zander, J. Enderlein, and R. A. Keller (Wiley-VCH, Berlin, 2002), Chap. 3,
pp. 69–120.
[28] J. Ries and P. Schwille, “Fluorescence correlation spectroscopy,” Bioessays 34, 361–368
(2012).
[29] P. J. Rothwell, S. Berger, O. Kensch, S. Felekyan, M. Antonik, B. M. Wohrl, T. Restle, R. S.
Goody, and C. A. M. Seidel, “ Multiparameter single-molecule fluorescence spectroscopy
reveals heterogeneity of HIV-1 reverse transcriptase: primer/template complexes,” Proc.
Natl. Acad. Sci. USA 100, 1655–1660 (2003).
[30] W. E. Moerner, Persistent Spectral Hole-Burning: Science and Applications Topics in
Current Physics (Springer, Berlin, 1988), Vol. 44.
[31] G. Heinze, C. Hubrich, and T. Halfmann, “Stopped light and image storage by electro-
magnetically induced transparency up to the regime of one minute,” Phys. Rev. Lett. 111,
033601 (2013).
[32] W. E. Moerner and T. P. Carter, “Statistical fine structure of inhomogeneously broadened
absorption lines,” Phys. Rev. Lett. 50, 2705–2708 (1987).
[33] W. E. Moerner and M. Orrit, “Illuminating single molecules in condensed matter,” Science
283, 1670–1676 (1999).
[34] Ph. Tamarat, A. Maali, B. Lounis, and M. Orrit, “Ten years of single molecule spec-
troscopy,” J. Phys. Chem. A 104, 1–16 (2000).
[35] T. Plakhotnik, E. A. Donley, and U. P. Wild, “Single-molecule spectroscopy,” Ann. Rev.
Phys. Chem. 48, 181–212 (1997).
[36] M. Orrit and W. E. Moerner, “High-resolution single-molecule spectroscopy in condensed
matter,” in Physics and Chemistry at Low Temperatures, edited by L. Khriachtchev (Pan
Stanford, Singapore, 2011), pp. 381–417.
[37] M. A. Kol’chenko, A. A. L. Nicolet, M. D. Galouzis, C. Hofmann, B. Kozankiewicz, and
M. Orrit, “Single molecules detect ultra-slow oscillators in a molecular crystal excited
by ac-voltages,” New J. Phys. 11, 023037 (2009).
[38] A. A. L. Nicolet, M. A. Kol’chenko, C. Hofmann, B. Kozankiewicz, and M. Orrit,
“Nanoscale probing of charge transport in an organic field-effect transistor at cryogenic
temperatures,” Phys. Chem. Chem. Phys. 15, 4415–4421 (2013).
[39] J. K¨ohler, J. A. J. M. Disselhorst, M. C. J. M. Donckers, E. J. J. Groenen, J. Schmidt, and
W. E. Moerner, “Magnetic resonance of a single molecular spin,” Nature 363, 242–244
(1993).
[40] J. Wrachtrup, C. von Borczyskowski, J. Bernard, M. Orrit, and R. Brown, “Opti-
cal detection of magnetic resonance in a single molecule,” Nature 363, 244–245
(1993).
[41] L. Childress, M. V. G. Dutt, J. M. Taylor, A. S. Zibrov, F. Jelezko, J. Wrachtrup, P. R.
Hemmer, and M. D. Lukin, “Coherent dynamics of coupled electron and nuclear spin
qubits in diamond,” Science 314, 281–285 (2006).

REFERENCES
97
[42] R. Hanson, V. V. Dobrovitski, A. E. Feiguin, O. Gywat, and D. D. Awschalom, “Coherent
dynamics of a single spin interacting with an adjustable spin bath,” Science 320, 352–355
(2008).
[43] T. van der Sar, Z. H. Wang, M. S. Blok, H. Bernien, T. H. Taminiau, D. M. Toyli, D.
A. Lidar, D. D. Awschalom, R. Hanson, and V. V. Dobrovitski, “Decoherence-protected
quantum gates for a hybrid solid-state spin register,” Nature 484, 82–86 (2012).
[44] C. Brunel, B. Lounis, P. Tamarat, and M. Orrit, “Triggered source of single photons based
on controlled single molecule fluorescence,” Phys. Rev. Lett. 83 2722–2725 (1999).
[45] J. Michaelis, C. Hettich, J. Mlynek, and V. Sandoghdar, “Optical microscopy using a
single-molecule light source,” Nature 405, 325–328 (2000).
[46] R. Lettow, Y. L. A. Rezus, A. Renn, G. Zumofen, E. Ikonen, S. G¨otzinger, and V.
Sandoghdar, “Quantum interference of tunably indistinguishable photons from remote
organic molecules,” Phys. Rev. Lett. 104, 123605 (2010).
[47] Y. Tian, P. Navarro, B. Kozankiewicz, and M. Orrit, Spectral diffusion of single diben-
zoterrylene molecules in 2,3-dimethylanthracene,” Chem. Phys. Chem. 13, 3510–3515
(2012).
[48] A. Zumbusch, L. Fleury, R. Brown, J. Bernard, and M. Orrit, “Probing individual 2-level
systems in a polymer by correlation of single molecular fluorescence,” Phys. Rev. Lett.
70, 3584–3587 (1993).
[49] A. M. Boiron, P. Tamarat, B. Lounis, R. Brown, and M. Orrit, “Are the spectral trails of
single molecules consistent with the standard two-level system model of glasses at low
temperatures? Chem. Phys. 247, 119–132 (1999).
[50] A. V. Naumov, Y. G. Vainer, and L. Kador, Does the standard model of low-temperature
glass dynamics describe a real glass? Phys. Rev. Lett. 98, 145501 (2007).
[51] W. P. Ambrose, T. Basch´e, and W. E. Moerner, “Detection and spectroscopy of single
pentacene molecules in a para-terphenyl crystal by means of fluorescence excitation,” J.
Chem. Phys. 95, 7150–7163 (1991).
[52] F. Kulzer, S. Kummer, R. Matzke, C. Br¨auchle, and T. Basch´e, “Single-molecule optical
switching of terrylene in p-terphenyl,” Nature 387, 688–691 (1997).
[53] P. D. Reilly and J. L. Skinner, “Spectral diffusion of individual pentacene molecles in
p-terphenyl crystal—stochastic theoretical model and analysis of experimental data,” J.
Chem. Phys. 102, 1540–1552 (1995).
[54] E. Geva, P. D. Reilly, and J. L. Skinner, “Spectral dynamics of individual molecules in
glasses and crystals,” Acc. Chem. Res. 29, 579–584 (1996).
[55] Y. X. Tian, P. Navarro, B. Kozankiewicz, and M. Orrit, “Spectral diffusion of single
dibenzoterrylene molecules in 2,3-dimethylanthracene,” Chem. Phys. Chem. 13, 3510–
3515 (2012).
[56] M. Paers, V. Palm, and J. Kikas, “Single-molecule probing of incommensurate biphenyl,”
Low Temp. Phys. 36, 448–450 (2010).
[57] A. M. van Oijen, M. Ketelaars, J. K¨ohler, T. J. Aartsma, and J. Schmidt, “Unraveling the
electronic structure of individual photosynthetic pigment-protein complexes,” Science
285, 400–402 (1999).
[58] R. Hildner, D. Brinks, J. B. Nieder, R. J. Cogdell, and N. F. van Hulst, “Quantum coherent
energy transfer over varying pathways in single light-harvesting complexes,” Science 340,
1448–1451 (2013).

98
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
[59] C. Hettich, C. Schmitt, J. Zitzmann, S. Kuhn, I. Gerhardt, and V. Sandoghdar, “Nanometer
resolution and coherent optical dipole coupling of two individual molecules,” Science
298, 385–389 (2002).
[60] B. F¨uckel, G. Hinze, F. Nolde, K. M¨ullen, and T. Basch´e, “Control of the electronic
energy transfer pathway between two single fluorophores by dual pulse excitation,” Phys.
Rev. Lett. 103, 103003 (2009).
[61] H. van de Hulst, Light Scattering by Small Particles (Dover, New York, 1981).
[62] K. Lindfors, T. Kalkbrenner, P. Stoller, and V. Sandoghdar, “Detection and spectroscopy
of gold nanoparticles using supercontinuum white light confocal microscopy,” Phys. Rev.
Lett. 93, 037401 (2004).
[63] F. V. Ignatovich and L. Novotny, “Real-time and background-free detection of nanoscale
particles,” Phys. Rev. Lett. 96, 013901 (2006)
[64] R. Zsigmondy, Colloids and the Ultramicroscope: A Manual of Colloid Chemistry and
Ultramicroscopy, translated by J. Alexander (John Wiley & Sons, New York, 1909).
[65] T. Klar, M. Perner, S. Grosse, G. von Plessen, W. Spirkl, and J. Feldmann, “Surface-
plasmon resonances in single metallic nanoparticles,” Phys. Rev. Lett. 80, 4249–4252
(1998).
[66] J. Yguerabide and E. E. Yguerabide, “Light scattering submicroscopic particles as highly
fluorescent analogs and their use as tracer labels in clinical and biological applications,”
Anal. Biochem. 262, 137–156 (1998).
[67] S. Schultz, D. R. Smith, J. J. Mock, and D. A. Schultz, “Single-target molecule detection
with nonbleaching multicolor optical immunolabels,” Proc. Natl. Acad. Sci. USA 97,
996–1001 (2000).
[68] C. S¨onnichsen, T. Franzl, T. Wilk, G. von Plessen, J. Feldmann, O. Wilson, and P.
Mulvaney, “Drastic reduction of plasmon damping in gold nanorods,” Phys. Rev. Lett.
88, 077402 (2002).
[69] A. M. Funston, C. Novo, T. J. Davis, and P. Mulvaney, “Plasmon coupling of gold
nanorods at short distances and in different geometries,” Nano Lett. 9, 1651–1658 (2009).
[70] T. Plakhotnik and V. Palm, “Interferometric signatures of single molecules,” Phys. Rev.
Lett. 87, 183602 (2001).
[71] V. Jacobsen, P. Stoller, C. Brunner, V. Vogel, and V. Sandoghdar, “Interferometric optical
detection and tracking of very small gold nanoparticles at a water-glass interface,” Opt.
Express 14, 405–414 (2006).
[72] P. Kukura, M. Celebrano, A. Renn, and V. Sandoghdar, “Single-molecule sensitivity in
optical absorption at room temperature,” J. Phys. Chem. Lett. 1, 3323–3327 (2010).
[73] P. V. Ruijgrok, P. Zijlstra, A. Tchebotareva, and M. Orrit, “Damping of acoustic vibrations
of single gold nanoparticles,” Nano Lett. 12, 1063–1069 (2012).
[74] A. L. Tchebotareva, P. V. Ruijgrok, P. Zijlstra, and M. Orrit, “Probing the acoustic
vibrations of single metal nanoparticles by ultrashort laser pulses,” Laser Photon. Rev. 4,
581–597 (2010).
[75] M. A. van Dijk, M. Lippitz, and M. Orrit, “Detection of acoustic oscillations of single
gold nanoparticles by time-resolved interferometry,” Phys. Rev. Lett. 95, 267406 (2005).
[76] A. L. Tchebotareva, M. A. van Dijk, P. V. Ruijgrok, V. Fokkema, M. H. S. Hesselberth,
M. Lippitz, and M. Orrit, “Acoustic and optical modes of single dumbbells of gold
nanoparticles,” Chem. Phys. Chem. 9, 111–114 (2009).

REFERENCES
99
[77] P. Zijlstra, A. L. Tchebotareva, J. M. W. Chon, M. Gu, and M. Orrit, “Acoustic oscillations
and elastic moduli of single gold nanorods,” Nano Lett. 8, 3493–3497 (2008).
[78] H. Staleva and G. Hartland, “Transient absorption studies of single silver nanocubes,”
J. Phys. Chem. C 112, 7535–7539 (2008).
[79] P. V. Ruijgrok, N. R. Verhart, P. Zijlstra, A. L. Tchebotareva, and M. Orrit, “Brownian
fluctuations and heating of an optically aligned gold nanorod,” Phys. Rev. Lett. 107,
037401 (2011).
[80] P. V. Ruijgrok, P. Zijlstra, A. L. Tchebotareva, and M. Orrit, “Damping of acoustic
vibrations of single gold nanoparticles optically trapped in water,” Nano Lett. 12, 1063–
1069 (2012).
[81] M. Pelton, J. E. Sader, J. Burgin, M. Liu, P. Guyot-Sionnest, and D. Gosztola, “Damping
of acoustic vibrations in gold nanoparticles,” Nat. Nanotechnol. 4, 492–495 (2009)
[82] K. Matawari, T. Kitamori, and T. Sawada, “Individual detection of single nanometer-
sized particles in liquid by photothermal microscopy,” Anal. Chem. 70, 5037–5041
(1998).
[83] D. Boyer, P. Tamarat, A. Maali, B. Lounis, and M. Orrit, “Photothermal imaging of
nanometer-sized metal particles among scatterers,” Science 297 1160–1163 (2002).
[84] S. Berciaud, L. Cognet, G. A. Blab, and B. Lounis, “Photothermal heterodyne imaging
of individual nonfluorescent nanoclusters and nanocrystals,” Phys. Rev. Lett. 93, 257402
(2004).
[85] E. Absil, G. Teissier, M. Gross, M. Atlan, N. Warnasooriya, S. Suck, M. Coppey-Moisan,
and D. Fournier, “Photothermal heterodyne holography of gold nanoparticles,” Opt.
Express 18, 780–786 (2010).
[86] A. Gaiduk, M. Yorulmaz, P. V. Ruijgrok, and M. Orrit, “Room-temperature detection
of a single molecule’s absorption by photothermal contrast,” Science 330, 353–356
(2010).
[87] P. Gaiduk, P. V. Ruijgrok, M. Yorulmaz, and M. Orrit, “Detection limits in photothermal
microscopy,” Chem. Sci. 1, 343–350 (2010).
[88] S. Chong, W. Min, and X. S. Xie, “Ground-state depletion microscopy: detection sensi-
tivity of single-molecule optical absorption at room temperature,” J. Phys. Chem. Lett. 1,
3316–3322 (2010).
[89] O. Schwartz and D. Oron, “A present understanding of colloidal quantum dot blinking,”
J. Chem. 52, 992–1001 (2012).
[90] P. Zijlstra and M. Orrit, “Single metal nanoparticles: optical detection, spectroscopy and
applications,” Rep. Prog. Phys. 74, 106401, 55pp. (2011)
[91] D. Brinks, F. D. Stefani, F. Kulzer, R. Hildner, T. H. Taminiau, Y. Avlasevich, K. M¨ullen,
and N. F. van Hulst, “Visualizing and controlling vibrational wave packets of single
molecules,” Nature 465, 905-U5 (2010).
[92] E. Ringe, B. Sharma, A. I. Henry, L. D. Marks, and R. P. Van Duyne, “Single nanoparticle
plasmonics,” Phys. Chem. Chem. Phys. 15, 4110–4129 (2013).
[93] P. G. Etchegoin and E. C. Le Ru, “A perspective on single molecule SERS: current status
and future challenges,” Phys. Chem. Chem. Phys. 10, 6079–6089 (2008).
[94] E. M. V. Lantman, T. Deckert-Gaudig, A. J. G. Mank, V. Deckert, and B. M. Weckhuysen,
“Catalytic processes monitored at the nanoscale with tip-enhanced Raman spectroscopy,”
Nat. Nanotechnol. 7, 583–586 (2012).

100
SINGLE-MOLECULE DETECTION AND SPECTROSCOPY
[95] R. Zhang, Y. Zhang, Z. C. Dong, S. Jiang, C. Zhang, L. G. Chen, L. Zhang, Y. Liao, J.
Aizpurua, Y. Luo, et al., “Chemical mapping of a single molecule by plasmon-enhanced
Raman scattering,” Nature 498, 82–86 (2013).
[96] A. Kinkhabwala, Z. F. Yu, S. H. Fan, Y. Avlasevich, K. Mullen, and W. E. Moerner,
“Large single-molecule fluorescence enhancements produced by a bowtie nanoantenna,”
Nat. Photon. 3, 654–657 (2009).
[97] H. F. Yuan, S. Khatua, P. Zijlstra, M. Yorulmaz, and M. Orrit, “Thousand-fold enhance-
ment of single-molecule fluorescence near a single gold nanorod,” Angew. Chem. Int. Ed.
52, 1217–1221 (2013).
[98] D. Punj, M. Mivelle, S. B. Moparthi, T. S. van Zanten, H. Rigneault, N. F. van Hulst, M.
F. Garcia-Parajo, and J. Wenger, “A plasmonic ‘antenna-in-box’ platform for enhanced
single-molecule analysis at micromolar concentrations,” Nat. Nanotechnol. 8, 512–516
(2013).

3
RESONANCE ENERGY TRANSFER
David L. Andrews,1 David S. Bradshaw,1
Rayomond Dinshaw,2 and Gregory D. Scholes2
1School of Chemistry, University of East Anglia, Norwich Research Park, Norwich,
Norfolk, UK
2Department of Chemistry, Institute for Optical Sciences and Centre for Quantum
Information and Quantum Control, University of Toronto, Toronto, ON, Canada
3.1
INTRODUCTION
The absorption of ultraviolet or visible light by an atom or molecule, resulting
in its electronic excitation, is a familiar and staple photophysical process. Owing
to the quantization of electronic states, the associated ultraviolet/visible absorption
spectrum contains bands reflecting fixed energy gaps. It is of significant interest to
ascertain the destination of the discrete energy held within an electronic excitation,
and the mechanism by which the excitation arrives there. One such photodynamical
process involves energy transfer from one molecule (or atom) to another. Remarkably,
this “hopping” occurs over spatial dimensions that are typically much larger than the
cross-section of a molecule and, in some cases, the effect is completed on a timescale
in the sub-picosecond range. This chapter gives an introduction to the key aspects of
this energy transfer, from its photophysics to its applications to molecular biology.
Formally, resonance energy transfer (RET) or electronic energy transfer (EET) is
a photophysical process wherein excitation is transferred from an initially populated
donor molecule (or chromophore) to an acceptor molecule through intermolecular
interactions. It is a ubiquitous process found in numerous systems including
conjugated polymers, light-harvesting proteins, and various other multichromophore
systems. A common application of energy transfer—and, indeed, one deployed
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
101

102
RESONANCE ENERGY TRANSFER
in nature for solar energy conversion by plants—is the migration of incident light
energy, captured at the periphery of a chromophore array, to a central chromophore,
effectively concentrating the excitation. Traditionally, energy transfer within
light-harvesting complexes is understood according to F¨orster RET theory, which
will be introduced in the following sections. In recent years, researchers have also
become interested in special circumstances where F¨orster theory fails to adequately
describe energy transfer, since these cases are likely to inspire new applications and
will certainly indicate new physical concepts.
3.2
HISTORY OF RET
3.2.1
The First Experiments
RET, the process in which the energy of an excited atom or molecule (usually called
the donor, but known historically as the “sensitizer”) is transferred nonradiatively to an
acceptor molecule (activator), occurs through intermolecular dipole–dipole coupling.
The origins of its discovery can be traced back to 1922, when the phenomenon of RET
(sensitized fluorescence) was first experimentally observed by Cario and Franck [1–3]
in the gas phase. Their spectroscopic experiment involved illuminating a mixture of
mercury and thallium vapors at a wavelength absorbed solely by the mercury; the
resulting fluorescence spectra proved to include frequencies that could only be emitted
from thallium. Such energy transfer in vapors was at first assumed to be uniquely
associated with interatomic collisions, but a discovery that transfer could occur at
larger separations than the collision radius showed that this was not necessarily the
case. Soon RET was also being observed in solutions [4], and over the following
years in a rapidly increasing number of other physical systems.
3.2.2
Early Developments of Theory
The first theoretical explanation of the phenomenon was proposed by Jean Per-
rin [5], later a Nobel laureate. Recognizing that energy could be transferred from
an excited molecule to its neighbors amongst closely spaced molecules through
dipole interactions, he named this process transfert d’activation, and his paper on
the subject became the earliest attempt to describe nonradiative (near-field) energy
transfer. Despite its initial success, however, Perrin’s model incorrectly predicted
that nonradiative energy transfer should be possible between dye molecules up to an
intermolecular distance of 1000 ˚A, an error deriving from an inaccurate assumption
that the molecules would act as Hertzian oscillators with exactly defined resonance
frequencies. Five years later [6], Perrin’s son Francis developed a corresponding
quantum mechanical theory of RET, based on Kallman and London’s results [7].
In this work he recognized a “spreading of absorption and emission frequency” due
to the interactions of the dye with the solvent, reducing the probability of energy
transfer. As a result, efficient transfer was calculated to occur up to 150–250 ˚A,
still approximately a factor of three more than had been experimentally observed. A
detailed and highly readable survey of these early contributions of J. Perrin and F.
Perrin can be found in a review by Berberan-Santos [8].

THE PHOTOPHYSICS OF RET
103
3.2.3
F¨orster Theory
Extending the ideas of J. Perrin and F. Perrin, F¨orster developed the first essentially
correct theoretical treatment of RET [9–11]. F¨orster determined that energy transfer,
through dipolar coupling between molecules, mostly depends on two important quan-
tities: spectrum overlap and intermolecular distance. Following the observation that
“the absorption and fluorescence spectra of similar molecules are far from completely
overlapping,” he found a means to quantify the spectral overlap integral. The static
dipole–dipole interaction was known to have an inverse proportionality on the cube
of the molecular separation. Since the rate of energy transfer is proportional to the
square of such a coupling, it thus depends on the sixth power of the separation—that
is, the now-famous R−6 distance-dependence law. Moreover, the acceptor distance at
which this rate equates to that of spontaneous emission by the donor, later termed
the F¨orster radius R0, was calculated to lie between 10 and 100 ˚A, agreeing with
experimental observations.
Much later, the distance dependence predicted by F¨orster was fully verified by
fluorescence studies of donor-acceptor pairs at known separations [12,13], leading to
the suggested employment of RET as a “spectroscopic ruler” by Stryer and Haugland
[13]; hence, a technique to measure the proximity relationships and conformational
change in macromolecules was realized (see Section 3.4). With the introduction of
the first lasers in the 1960s, the modern understanding of RET led to a raft of modern
applications. An excellent in-depth review on the modern history of RET is given by
Clegg [14].
3.3
THE PHOTOPHYSICS OF RET
RET is a process that is known to operate across a diverse and extensive range of
physical systems, encompassing not only gases and dye solutions, but also protein
complexes, doped crystals, and polymers, to name but a few. Nonetheless, at a fun-
damental level it is possible to identify numerous common features in the underlying
photophysics.
3.3.1
Primary Excitation Processes
To approach the subject in detail, let us commence with the photoexcitation pro-
cess that creates the conditions for RET to occur. When resonant ultraviolet or
visible radiation impinges on any nonhomogeneous dielectric material, the primary
result of photon absorption is the population of electronic excited states in individ-
ual atomic, molecular, or other nanoscale centers—henceforth, the latter are to be
grouped together under the generic term “chromophore.” Typically, such absorption
is immediately followed by a rapid but partial degradation of the acquired energy,
the associated losses (largely due to vibrational dissipation) ultimately to be manifest
in the form of heat. This effect is reflected in the principle that the release of elec-
tronic energy by fluorescence generally occurs from the lowest vibrational level of

104
RESONANCE ENERGY TRANSFER
FIGURE 3.1
Typical spectral discrimination between the fluorescence from donor and
acceptor species (here notionally based on a cyan fluorescent protein donor and a yellow
fluorescent protein acceptor): (a) the transmission characteristics of a short-wavelength filter
ensure initial excitation of only the donor; a dichroic beam splitter and another narrow emission
filter ensuring that only the (Stokes-shifted) fluorescence from the donor reaches a detector;
(b) in the same system a longer-wavelength emission filter ensures capture of only the acceptor
fluorescence, following RET.
the excited state. However, if any nearby chromophore has a suitably disposed elec-
tronic state, of a similar or slightly lower energy, that neighbor may acquire the major
part of the electronic excitation through RET—a process that takes place well before
any further thermal degradation of the excited state energy occurs. The mechanism is
most commonly studied through spectrometric differentiation of fluorescence emerg-
ing from the initially excited energy donor and from the energy acceptor species, as
illustrated in Figure 3.1. As will be shown in the following, the propensity for energy
to be transferred between any two chromophores is severely restricted by distance,
and if no suitable acceptor is within reach, the donor will generally shed its energy
by fluorescence or local dissipation.
3.3.2
Coupling of Electronic Transitions
In systems where RET occurs, the donors and acceptors are usually also fluorophores,
that is, chromophores that have the capacity to decay by fluorescent emission. More-
over, in RET, the transitions of donor decay and acceptor excitation are generally
electric dipole-allowed—although other possibilities do occasionally arise. Accord-
ingly the theory of energy transfer, for donor–acceptor displacements beyond the
region of significant wavefunction overlap, is traditionally conceived in terms of an
electrodynamical coupling between transition dipoles.
Consider the pairwise transfer of excitation between two chromophores D and A.
In the context of this elementary mechanism, D is designated the donor and A the

THE PHOTOPHYSICS OF RET
105
acceptor. Specifically, let it be assumed that prior excitation of the donor generates an
electronically excited species D∗. Forward progress of the energy is then accompanied
by donor decay to the ground electronic state. Acquiring the energy, A undergoes a
transition from its ground to its excited state. The RET process may be expressed by
the following photophysical equation:
D∗+ A →D + A∗.
(3.1)
The excited acceptor, A∗, subsequently decays either in a further transfer event,
or by another means such as fluorescence. As the D∗and A∗excited states are real,
with measurable lifetimes, the core process of energy transfer itself is fundamentally
separable from the initial electronic excitation of D and the eventual decay of A; the
latter processes do not, therefore, enter into the theory of the pair transfer.
Since F¨orster theory explicitly relies on the electric dipole approximation, that is,
energy transfer occurring through electric dipole couplings, the model breaks down in
certain circumstances and becomes invalid. Consequently, problems of interpretation
can arise when an electronic transition is electric dipole-forbidden, but allowed via
magnetic dipole or higher-order multipoles; weak interactions may develop, to facil-
itate the energy transfer, that are unaccounted for by the F¨orster theory. This scenario
often occurs for interactions between highly symmetric chromophores. In the Tay-
lor series expansion of the Coulombic interaction between transition moments, the
leading nonzero term involves electric dipole moments while the higher orders relate
to magnetic dipoles, and to electric and magnetic multipoles. All of these additional
terms are more strongly dependent on distance and, therefore, multipolar interactions
are significant at very small donor–acceptor separations; an alternative mechanism
described by Dexter theory [15] should be considered when there is donor–acceptor
wavefunction overlap (vide infra).
3.3.3
Dissipation and Line Broadening
To delve more deeply into the nature of the RET process, it needs to be recognized that
Eq. (3.1) tells only part of the story, dealing as it does with only electronic excitations.
In general, other dissipative processes are also engaged. In a solid, the linewidth of
optical transitions manifests the influence of local electronic environments that, in
the case of strong coupling, may lead to the production of phonon side-bands. Similar
effects in solutions or disordered solids represent inhomogeneous interactions with
a solvent or host, while the broad bands exhibited by chromophores in complex
molecular systems signify extensively overlapped vibrational levels, including those
associated with skeletal modes of the superstructure. In each case, the designation
of RET as a “resonance” process has not only the connotation of overall energy
conservation, but also conveys the sense that energy can transfer from any level within
the continuum of the donor-excited state to a corresponding level in the excited state
continuum of the acceptor. The overall transfer efficiency thus involves a factor known
as the spectral overlap [16], which denotes a frequency-weighted integral product of
the donor emission and acceptor absorption profiles, as illustrated in Figure 3.2.

106
RESONANCE ENERGY TRANSFER
S
S
D
A
FD
2  1
A
σ
ω
1
ω
2
ω
δ
FA
2  2
D
σ
3
ω
ω
4
ω
δ
FIGURE 3.2
Energetics and spectral overlap features (top) for energy transfer from D to
A (and below, potentially backward transfer from A to D). For each chromophore, F denotes
the fluorescence spectrum and 𝜎the absorption. Wavy downward lines denote vibrational
dissipation.
F¨orster theory relies on a crucial assumption relating to the microscopic description
of the dynamics, namely that the electronic coupling between donor and acceptor is
very weak in comparison to line broadening. The electronic coupling acts as a small
perturbation to promote energy transfer, but it is not sufficiently large to support
delocalized excited states. In this weak coupling regime, vibrational equilibrium is
achieved much faster than the rate at which RET occurs, and thus the electronic and
vibrational states are essentially decoupled when RET takes place. When the excita-
tion is localized on one molecule at any time and the transfer of excitation from the
donor to the acceptor occurs incoherently, in an irreversible “hopping” fashion, the
dynamics can be described according to conventional kinetic rate equations. Accord-
ingly, a time-independent transfer rate can be determined from Fermi’s golden rule—a
quantum mechanical prescription that underpins most of molecular spectroscopy [17].
To formulate other energy transfer theories that relax this assumption is a great
challenge, strongly depending on how the magnitude of the interchromophore interac-
tion compares to the interaction of this system with the “bath”—the latter term being
commonly used in this context to signify the surroundings of the donor–acceptor
pair. The interchromophore interaction is characterized by direct electronic cou-
pling, while the system-bath interaction can be characterized by a quantity called the

THE PHOTOPHYSICS OF RET
107
“reorganization energy,” which quantifies the extent to which the host or solvent
and other environmental degrees of freedom accommodate the change of electron
distribution on promotion of a molecule from its electronic ground to excited state.
A fundamental breakdown of F¨orster theory occurs in the intermediate and strong
coupling regimes. Here, the interchromophore coupling between donor and acceptor
molecules can no longer be regarded as sufficiently small to promote incoherent
energy transfer. In the strong coupling limit, when the coupling between the donor
and acceptor molecules is large compared to the system-bath coupling, delocalized
excitonic states are created; the excitation is coherently shared, being no longer local-
ized on either the donor or acceptor [18]. The applicable description of the strong
coupling regime is known as Redfield theory [19]. In Redfield theory, it is the bath
that promotes energy transfer and relaxation through stochastic fluctuations.
In the intermediate coupling regime, widely applicable to multichromophore light-
harvesting proteins, the electronic coupling is similar in magnitude to the system-bath
coupling. Such cases, in which the understanding of energy transfer becomes far more
difficult, are currently the subject of extensive studies [20].
3.3.4
F¨orster Equation
The F¨orster theory delivers an expression for the rate of pairwise energy transfer,
wF, for any donor–acceptor separation, R, that is substantially smaller than the wave-
lengths of visible radiation. For systems where the common host material for the
donor and acceptor has refractive index n, at an optical frequency corresponding to
the mean transferred energy, the F¨orster result is expressible as follows [21–23]:
wF = 1
𝜏0
D
[R0
R
]6
,
(3.2)
where 𝜏0
D is the radiative decay lifetime of the donor molecule in the absence of
transfer. The F¨orster radius R0 (in ˚Angstroms)—that is, the distance at which the rates
of donor deactivation by RET and by spontaneous fluorescence become equal—is
defined by
R0 = 0.2108 [𝜅2Φ0
Dn−4J] 1
6 .
(3.3)
In this expression, Φ0
D is the fluorescence quantum yield of the donor with transfer
absent, 𝜅is the orientation factor (vide infra), and J represents an integral of the
product of the donor emission spectrum with the acceptor absorption spectrum (in
units of M−1 cm−1 nm4); the latter denotes a spectral overlap which, as stated earlier,
is a key determinant of energy transfer efficiency. It is evident from Figure 3.2, that the
propensity for forward transfer is usually significantly greater than that for backward
transfer, due to a sizeable difference in the spectral overlaps for the two processes.
Similar to the rate, the transfer efficiency ΦT is expressible as
ΦT =
1
1 + (R∕R0)6 = 1 −𝜏fl
𝜏D∗= 1 −Ifl
ID∗,
(3.4)

108
RESONANCE ENERGY TRANSFER
0.0
Donor–acceptor distance R
R0
0.5
Transfer efficiency
1.0
FIGURE 3.3
Distance dependence of the transfer efficiency between a pair of chromophores,
calculated according to Eq. (3.4).
where Ifl and ID∗are the intensities of the donor fluorescence with the acceptor present
and excluded, respectively, and 𝜏fl specifically denotes the fluorescence lifetime of the
donor measured within its RET environment. As graphically depicted in Figure 3.3, a
donor–acceptor displacement equal to R0 corresponds to a transfer efficiency of 50%.
The third equality on the right of Eq. (3.4), which holds provided decay processes
follow single-exponential decay kinetics, provides a formula that is cast in terms
of easily measurable quantities. This is particularly useful since it allows energy
transfer efficiencies to be calculated simply on the basis of intensity measurements
(e.g., using a fluorimeter), obviating the separate time-resolved measurements that
would otherwise be generally necessary for evaluation of the characteristic decay
lifetimes 𝜏fl and 𝜏D∗. When a given electronically excited chromophore is situated
within a distance R0 of a suitable acceptor, RET will generally be the dominant
decay mechanism; conversely, for distances beyond R0, spontaneous decay (usually
fluorescence) will be the primary means of donor deactivation.
3.3.5
Orientation Dependence
The 𝜅factor depends on the orientations of the donor and acceptor, both with respect
to each other, and with respect to their mutual displacement unit vector ̂R, as follows:
𝜅= ( ̂𝝁D ⋅̂𝝁A) −3( ̂R ⋅̂𝝁D)( ̂R ⋅̂𝝁A).
(3.5)
For each chromophore, ̂𝝁designates a unit vector in the direction of the appropriate
transition dipole moment—a measure of charge displacement during the associated
transition, which can be calculated using quantum theoretical methods [24]. The

THE PHOTOPHYSICS OF RET
109
R
θD
θA
μA
μD
θ T
FIGURE 3.4
Relative orientations and positions of the donor and acceptor and their transition
moments: angles 𝜃D and 𝜃A are subtended by donor and acceptor transition moments (𝝁D and
𝝁A, respectively) against the interchromophore displacement vector, R; the symbol 𝜃T is the
angle between the transition moments.
possible values of 𝜅2, as featured in Eq. (3.3), lie in the range (0, 4). It is evident that
in the case of fixed chromophore positions and orientations the result delivered by
(3.5) is a function of three independent angles, as shown and defined in Figure 3.4:
𝜅= cos 𝜃T −3 cos 𝜃D cos 𝜃A.
(3.6)
Unfavorable orientations can thus reduce the rate of energy transfer to zero;
other configurations, including many of those found in photobiological systems,
optimize the transfer rate. The angular disposition of chromophores is therefore a
very important facet of energy transfer. It is important to note that transfer is not
necessarily precluded when the transition moments lie in perpendicular directions—
provided that neither is also disposed orthogonally to R(= R ̂R).
In any at least partially fluid or disordered system, the relative orientation of all
donor–acceptor pairs may not be identical, and it is then the distributional average of
𝜅2 that determines the overall measured response. In the isotropic case (completely
uncorrelated orientations) the 𝜅2 factor averages to 2∕3; departures from this value
provide a quantitative signature of the degree of orientational correlation. In molecules
of sufficiently high symmetry it can also happen that either the donor or the acceptor
transition moment is not unambiguously identifiable with a particular direction in the
corresponding chromophore reference frame. Specifically, the electronic transition
may then relate to a transition involving a degenerate state—as can occur with
square planar complexes, for example [25]. Alternatively, the same observational
features might indicate rapid but orientationally confined motions. The considerable
complication which each of these effects brings into the trigonometric analysis of
RET has been extensively researched and reported by van der Meer [26,27].
3.3.6
Polarization Features
When linearly polarized laser light is used to excite any specific species within a
complex disordered solid or liquid system, the probability for initial excitation of
any particular molecule is proportional to cos2𝜃, where 𝜃is the angle between the

110
RESONANCE ENERGY TRANSFER
appropriate excitation transition moment and the electric polarization vector of the
input radiation. Consequently the population of excited molecules has a markedly
anisotropic distribution, a phenomenon associated with the term photoselection. If
radiative decay were to ensue instantaneously, that is, from precisely the initially
populated excited level, then the fluorescence would carry the full imprint of that
anisotropy and itself exhibit a degree of polarization—the highest value possible.
Accounting for the necessary three-dimensional rotational average [28], it is readily
shown that the fluorescence intensity components polarized parallel to and perpen-
dicular to the polarization of the excitation beam, I|| and I⊥, respectively, would
then lie in the ratio 3:1. Commonly observed departures from this result thus signify
the extent to which the orientation of the emission dipole differs from that of the
prior, initial excitation—which may be due to intervening decay, molecular motion,
or intermolecular energy transfer.
The two most widely used quantitative expressions of polarization retention are
the fluorescence anisotropy, r, or the degree of polarization, P. Both convey the same
information; they are defined and related as follows:
r =
I|| −I⊥
I|| + 2I⊥
,
P =
I|| −I⊥
I|| + I⊥
⇒
r =
2P
3 −P.
(3.7)
The denominator of the expression for r designates the net fluorescence intensity.
In a specific situation where the donor and acceptor have transition dipole moments
oriented in parallel, then r = 0.4 and P = 0.5.
A key molecular factor determining any loss in polarization is the angle 𝜃between
the directions of the absorption and emission transition dipole moments. In terms of
this parameter and its influence on the measured fluorescence anisotropy, the case
where internal decay intervenes between excitation and fluorescence decay within
a single molecule is no different from that of a donor–acceptor pair in which the
absorption and emission processes are spatially separated—provided the donor and
acceptor in the latter case have a fixed mutual orientation (the orientation of the pair
being random). The following result, derived by Levshin [29] and Perrin [30], can be
applied in both situations:
P = 3 cos2 𝜃−1
3 + cos2 𝜃.
(3.8)
In the case of a donor–acceptor pair, 𝜃is to be interpreted as the angle 𝜃T shown in
Figure 3.4. Equation (3.8) thus allows direct calculation of this microscopic parameter,
through measurement of the macroscopic quantity P. Moreover when P proves to
exhibit a time-dependent decay, a study of the kinetics provides information on the
extent of rotational motion intervening between the absorption and emission events.
Very different behavior is observed for RET systems in which the donor and
acceptor are orientationally uncorrelated, that is, where they are both, independently,
randomly oriented. In such cases there is a very rapid loss of polarization “memory,”
and it transpires that the associated degree of anisotropy is precisely 1/25, that is,
r = 0.04 [31]; two or more energy transfer jumps will therefore usually, to all intents

THE PHOTOPHYSICS OF RET
111
and purposes, destroy any polarization in any ensuing fluorescence. However, it
should be noted that there is a surprising recovery in the anisotropy at distances
approaching the transfer wavelength. The effect is sufficiently strong to warrant
attention in dilute solution studies [32].
3.3.7
Diffusion Effects
So far only RET between a donor–acceptor pair has been considered. The discussion
is now extended to an ensemble of donors D and acceptors A, all units of which
are distributed randomly within an m-dimensional volume. For systems in which
translational diffusion is extremely slow compared to the rate of energy transfer, the
time dependence of the donor intensity decay at time t, ID∗(t), as obtained by F¨orster
[11], is given by the following expression:
ID∗(t) = ID∗(0) exp
[
−t
𝜏D∗−2𝛾
(
t
𝜏D∗
) m
6
]
.
(3.9)
The most commonly applied form of this expression is when the m equals 3, that
is, RET in three dimensions. In Eq. (3.9), the parameter 𝛾is explicitly written as
𝛾= 2
3𝜋
3
2 CAR3
0,
(3.10)
in which CA is the concentration of acceptors (number per unit volume) and
(4∕3)𝜋R3
0CA represents the average number of acceptor chromophores in a sphere of
radius R0; the orientational factor is again set as 2∕3.
Cases where diffusion is comparable to the transfer rate become very complicated,
and calculations by Butler and Pilling [33] have shown that large errors arise on using
F¨orster theory for systems with diffusion coefficients in excess of 10−5 cm2 s−1. To
address such systems, a successful approximation was developed by G¨osele et al.
[34]. This approach involves the insertion of a multiplier G within the second term
in the exponential of Eq. (3.9). With m = 3, the parameter G is given by
G =
(
1 + 5.47x + 4.00x2
1 + 3.34x
) 3
4
,
(3.11)
in which x = D(R6
0∕𝜏D∗)−1∕3t2∕3, where D is the mutual diffusion coefficient. In con-
trast to the F¨orster theory, the above method provides an excellent approximation—as
was verified by the authors of Reference 33.
F¨orster theory also fails at donor–acceptor displacements similar in magnitude
to chromophore dimensions for another reason. This is because transition dipole
moments no longer appear point-sized, and the centers of the transition moments, from
which charge displacement is measured, become difficult to define. Circumstances
where this is especially apparent are extended chromophores such as carotenoid,
chlorophyll, and other photosynthetic substances. An expedient solution to the prob-
lem is the transition density cube (TDC) method, which involves explicit calculations

112
RESONANCE ENERGY TRANSFER
of the interactions between relevant transition densities on the donor and acceptor
[35]. The key advance of transition densities over their approximation via the Taylor
expansion is that exact Coulombic coupling is determined, that is, all the terms in the
expansion are retained, and the wavefunction shapes of the interacting chromophores
are captured. In fact the shapes of the transition densities (which are related to the
wavefunctions) are vital in understanding the molecular interactions. In its original
form, the TDC method uses quantum mechanical computations to obtain the transition
densities between ground and excited states. These are discretized into infinitesimal
volume elements, followed by a summation over all Coulombic interactions. Higher
order energy transfer processes can occur via electric quadrupole–electric quadrupole
interactions or second-order electric dipole–electric dipole couplings [36].
3.3.8
Long-Range Transfer
It was originally assumed that a “radiative” mechanism would correctly describe
energy transfer for donor–acceptor separations over and beyond 100 ˚A, and some
recent literature on the subject still perpetuates this over-statement. Certain sources
wrongly treat F¨orster “radiationless” energy transfer as exact, distinct, and separable
from “radiative” energy transfer—the latter signifying successive but independent
processes of fluorescence emission by a donor, and capture of the ensuing photon by
an acceptor.
AlthoughthatcertainlyistheobservedcharacterofREToververylongdistances—
as for example between donor and acceptor components in a dilute solution—it
is now known that both “radiative” and F¨orster transfer are simply the long- and
short-range limits of one powerful, all-pervasive mechanism. The latter, determined
from quantum electrodynamical calculations, is the outcome of the unified theory of
RET [37]. This not only embraces F¨orster and “radiative” energy transfer, but also
addresses the intermediate range in which neither of these mechanisms is fully valid.
An expression for the total pairwise energy transfer rate, ranging from molecular
dimensions up to interstellar distances, is written as
w = wF + wI + wrad,
(3.12)
where wF represents the F¨orster rate of Eq. (3.2), equally expressible in an alternative
form as follows [38,39]:
wF =
9𝜅2c4
8𝜋𝜏0
Dn4R6 ∫FD(𝜔)𝜎A(𝜔)d𝜔
𝜔4 ,
(3.13)
where FD(𝜔) denotes the normalized fluorescence spectrum of the donor, 𝜎A(𝜔)
represents the linear absorption cross-section of the acceptor, and 𝜔is an optical fre-
quency in radians per unit time. Returning to Eq. (3.12), wrad is the rate of “radiative”
energy transfer—explicitly given by
wrad =
9𝜅′2
8𝜋𝜏0
DR2 ∫FD(𝜔)𝜎A(𝜔) d𝜔,
(3.14)

INVESTIGATIVE APPLICATIONS OF RET IN MOLECULAR BIOLOGY
113
and wI is the intermediate term that is cast as
wI =
9c2
8𝜋𝜏0
Dn2R4 (𝜅2 −2𝜅𝜅′) ∫FD(𝜔)𝜎A(𝜔)d𝜔
𝜔2 .
(3.15)
In both Eqs. (3.14) and (3.15), the symbol 𝜅′ denotes an orientation factor identical
to (3.5) but with the factor of “3” omitted from the second term. In summary, the
unified theory of RET contains not only the R−6 term of F¨orster theory and the R−2
term denoting the inverse-square law of “radiative” transfer, but also a previously
unidentified R−4 intermediate term.
3.3.9
Dexter Transfer
Before concluding this section, it is worth observing that other forms of donor–
acceptor coupling are also possible, although considerably less relevant to the systems
of interest in the following account of applications. For example, the transfer of
energy between atomic components with significantly overlapped wavefunctions
is usually described in terms of Dexter theory [40]—where the coupling involves
electron exchange and is associated with an exponential decay with distance, directly
reflecting the radial form of overlapping wavefunctions and electron distributions.
Unlike F¨orster transfer, singlet–triplet energy exchange (3D∗+ 1A →1D + 3A∗) may
also be allowed by the Dexter mechanism. This is because Dexter transfer, which is
not simply expressible in terms of transition moments, is not precluded by the dipole-
forbidden character of the transitions T1 →S0 and S0 →T1 within chromophores D
and A, respectively. The Dexter mechanism requires only the conservation of total
spin. Compared to materials in which the donor and acceptor orbitals do not spatially
overlap, such systems are of less use for either device or analytical applications. The
Dexter mechanism is generally operational only at very short distances (<10 ˚A), and
other terms dominate the electronic coupling in cases of orbital overlap. The reader
is referred to References 15 and 41 for relevant discussions.
3.4
INVESTIGATIVE APPLICATIONS OF RET
IN MOLECULAR BIOLOGY
Without doubt, the field in which measurements of RET have had the greatest impact
is molecular biology. The importance of RET to this subject, especially in application
to biological macromolecules, was first realized following the construction of spec-
troscopic equipment for routine fluorescence measurements [42–45]. Toward the turn
of the twenty-first century, RET underwent a period of significant redevelopment as a
spectroscopic technique [46]. This resurgence arose mainly due to the advent of new
experimentation methods, for example, single-pair RET [47], and further advances
in instrumentation. The key advantage of RET techniques over others is that fluo-
rescence measurements are highly sensitive, being made against a zero background;
moreover, the UV/visible signals are relatively easy to detect and are specific and the
required instrumentation is noninvasive.

114
RESONANCE ENERGY TRANSFER
3.4.1
Spectroscopic Ruler
A major use of RET, based on its strong distance dependence, exploits its capacity to
supply accurate spatial information about molecular structures. This derives from a
quantitative assessment of the interchromophore separations, based on comparisons
between the corresponding RET efficiencies [48–51]. Such a technique is popularly
known as a “spectroscopic ruler.” The elucidations of molecular structure by such
means usually lack information on the relative orientations of the groups involved,
and as an expedient the calculations usually ignore the kappa parameter (3.5). The
apparent crudeness of this approach becomes more defensible on realizing that,
even if it were to introduce a factor of two inaccuracy, the deduced group spacing
would still be in error by only 12% (since 21∕6 = 1.12). Refinements to the theory
to accommodate the effect of fluctuations in position or orientation of the participant
groups introduce considerable complexity, although progress is being made in several
areas [52–56].
3.4.2
Conformational Change
Through identification of motions in macromolecules, that is, the variation in prox-
imity of one chromophore with respect to another, a number of valuable RET appli-
cations arise, including the detection of conformational changes and folding in pro-
teins [49,57–59], and the inspection of intracellular protein–protein [24,60–63] and
protein–DNA [64,65] interactions (see, e.g., Figure 3.5). These and other such pro-
cesses can be registered by selectively exciting one chromophore using laser light
and monitoring either the decrease in fluorescence from that chromophore, or the
rise in the generally longer-wavelength fluorescence from the other chromophore as
it adopts the role of acceptor. The judicious use of optical dichroic filters can make
this RET technique perfectly straightforward—see Figure 3.1. In cases where the two
material components of interest lack suitably overlapped absorption and fluorescence
FIGURE 3.5
RET method for the detection of protein–protein interactions. Biochemical
interactions between the proteins result in the attached chromophores becoming closer in
proximity—thus allowing energy transfer to occur between them. As a consequence, the
emission frequency will differ to those cases where energy transfer is absent.

INVESTIGATIVE APPLICATIONS OF RET IN MOLECULAR BIOLOGY
115
features in an optically accessible wavelength range, molecular tagging with site-
specific “extrinsic” (i.e., artificially attached) chromophores can solve the problem.
Located at a molecular site of interest, and being selected on the basis of a signifi-
cant spectral overlap with the counterpart component, such tags can act either in the
capacity of donor or acceptor. Lanthanide ions, with their characteristically prominent
and line-like absorption features, prove particularly valuable in this connection [66].
Also useful in this respect are the semiconductor nanocrystals known as quantum
dots. These crystalline nanoparticles offer several unique traits, including size- and
composition-tunable emission from visible to infrared wavelengths, the possibility
of a single light source simultaneously exciting different-sized dots, large absorption
coefficients across a wide spectral range, and very high level of photostability [67,68].
3.4.3
Intensity-Based Imaging
In the last two decades, there has been burgeoning interest in microscopy based on
RET [69–75], typical instrumentation for which is illustrated in Figure 3.6. There
are three specific types of RET method routinely used in the production of biologi-
cal images. The principles of sensitized-emission RET have already been described
(Fig. 3.1). For microscopy purposes this method is fairly inaccurate; no RET donor–
acceptor pair is ideal, that is, there will almost always be some overlap between
the donor and acceptor absorption bands, and also the donor and acceptor emission
Tungsten
Halogen
Lamphouse
Cooled
Widefield
CCD Camera
Ar-Kr Laser
System
with AOTF
Arc-Discharge
Lamphouse
Inverted
Tissue Culture
Microscope
Nipkow Confocal
Scan Head
Cooled Gen III
Intensified
CCD Camera
Systems
FIGURE 3.6
Typical commercial RET microscope (Olympus Corporation).

116
RESONANCE ENERGY TRANSFER
spectra. Therefore, filters that completely separate these kinds of spectrum are difficult
to design. Various calculational algorithms [76–78] have been proposed to compen-
sate for this problem, although the methods are complex and no single procedure has
received universal acceptance.
A widely used alternative, experimental approach [79, 80] involves deliberately
photobleaching the acceptor, the result of which is complete exclusion of RET. In
this method, the donor emission is analyzed before and after the acceptor is bleached
by the input of an intense laser beam (at a suitable wavelength). The difference
between the donor intensities, with and without the laser input, enables a determi-
nation of the transfer efficiency by employing Eq. (3.4). Here, account is taken of
spectral bleed-through between the two absorption bands, and equally between the
two emission bands. Signal contamination is still not entirely eliminated, due to a
small amount of back-transfer through donor excitation by acceptor emission. Often
the main disadvantage in prolonged illumination of the acceptor is the possibility of
damage to the sample. Therefore, in practice, photobleaching is seldom appropriate
for in vivo studies.
3.4.4
Lifetime-Based Imaging
Fluorescence need not be characterized from excitation and emission spectra alone;
highly significant information can also be secured from lifetime measurements. Thus,
when suitable time-resolved instrumentation is available, the determination of decay
kinetics (usually on the nanosecond timescale) enables analysis through RET-based
fluorescence lifetime imaging microscopy (FRET-FLIM) [81–84]. In this method,
spectral bleed-through is no longer an issue since measurements are made only for
the determination of donor lifetimes; back-transfer is usually extremely low and
within the noise level. The presence of the acceptor within the local environment of
the donor influences the fluorescence lifetime of the donor. By measuring the donor
lifetime in the presence and absence of the acceptor one can accurately calculate the
transfer efficiency by the use of Eq. (3.4). Drawbacks to FRET-FLIM are the technical
challenges the technique presents, and the expense of the equipment. Nonetheless,
in optical systems that are equipped to provide both intensity and lifetime measure-
ments, a comparison of the two types of image affords a particularly rich source of
information, as illustrated by the cancerous cell images of Figure 3.7.
3.4.5
Other Applications
Beyond the realm of molecular biology, RET has value in a number of more specif-
ically chemical applications. In connection with quantum dots of variable size, the
viable operation of a biomimetic scheme known as a “nanofountain” (Fig. 3.8) has
been proven experimentally by Ohtsu’s group [85]. Other prominent examples are
found in the fields of synthetic macromolecules and chemical sensors. In polymer
science, building on the pioneering principles of Morawetz [86], RET is now used to
determine morphological information on polymer interfaces. Such studies have, for
instance, enabled the quantitative characterization of interfacial thickness in polymers

INVESTIGATIVE APPLICATIONS OF RET IN MOLECULAR BIOLOGY
117
1.9
2.0
2.1
2.2
2.3
(b)
(a)
FIGURE 3.7
MDA-MB-231 cancerous cell images recorded with argon laser two-photon
excitation, RET microscope based on (a) intensity and (b) fluorescence lifetime (ns). In the
latter, areas of locally reduced lifetime signify clustered intracellular vesicles. Adapted from
Reference 84.
of various structures [87]. Moreover, RET has been utilized in the study of polymer
conformational dynamics. One especially interesting application is the effective dif-
ferentiation between various collapsed and/or ordered homopolymer chain confor-
mations through the associated distribution of transfer efficiencies [88,89].
The fabrication of RET-based, analyte-specific sensors has enabled detection of a
variety of species, including dimers of functionalized calixarenes in organic solutions
[90], copper(II) in aqueous solution [91], hydrogen peroxide [92], phosgene [93],
and many others. These chemical sensors usually work on the principle of a donor–
acceptor system designed such that the presence of the analyte causes the acceptor
chromophore to move within closer proximity to a donor, enabling an RET process
that is not observed in the absence of the analyte. Therefore, on irradiation of the
system with the relevant chemical present, a strong emission from the acceptor signals
the presence of the analyte.
FIGURE 3.8
Scheme for an optical nanofountain composed of 2–10 nm CuCl quantum dots,
distributed in an NaCl matrix. (For a color version of this figure, see the color plate section.)

118
RESONANCE ENERGY TRANSFER
3.5
THE ROLE OF RET IN LIGHT-HARVESTING COMPLEXES
3.5.1
Introduction
Light-harvesting proteins play the functional role of capturing solar radiation and
transferring the resulting excitation to the reaction centers where they are used to
carry out photosynthetic chemical reactions [94]. In high-order plants and many
algae, the major light-harvesting protein is Light-Harvesting Complex II (LHC-II)
which contains the green pigment chlorophyll. In addition to chlorophyll-containing
complexes, some plants, algae, and bacteria utilize other light-harvesting anten-
nae including phycobilisomes and phycobiliproteins to supplement the absorption
of sunlight. These peripheral light-harvesting proteins transfer photoexcitation to
chlorophyll-a molecules located in the membrane, which subsequently pass the exci-
tation on to photosystem I (PSI) and photosystem II (PSII). At the center of the
PSII complex resides the reaction center, the site of water oxidation catalyzed by
a Mn4OxCa complex. The oxidation of water by PSII, along with processes car-
ried out by other protein complexes including PSI, mobile electron carriers, and the
cytochrome bf complex, complete the photosynthetic cycle. These other processes
include the release of molecular oxygen, the reduction of nicotinamide–adenine din-
ucleotide phosphate (NADP+) and adenosine diphosphate (ADP), as well as the
generation of a proton gradient across the membrane.
Photosynthesis starts with the absorption of light by a chromophore, such as
chlorophyll, in a light-harvesting protein, resulting in an electronic transition from the
ground state to an excited state. The excited state is short-lived, relaxing to the ground
state after a mere nanosecond, so that the first events of photosynthesis involving the
transport of this electronic excitation to reaction centers must be ultrafast [95]. The
requirement of ultrafast energy migration beyond a nearest neighbor means that these
systems are often highly optimized multi-chromophore complexes. The optimization
lies in the structure of the light-harvesting complexes that bind the chromophores.
Figure 3.9 shows absorption spectra recorded for several such complexes, illustrating
their broad spectral cross-sections, often specialized in a particular wavelength range.
Figure 3.10 shows structural models elucidated from X-ray crystallography for two
light-harvesting complexes. The extraordinary density of chromophores is notable,
particularly in LHC-II, the major light-harvesting complex of higher plants and
green algae.
Photosynthetic organisms achieve numerous advantages by employing differ-
ent complexes to harvest sunlight and to drive chemical reactions [95]: (a) Light-
harvesting proteins are able to increase the spatial and spectral cross-section for
the absorption of sunlight without being costly to the organism, in many photosyn-
thetic species the reaction center may be serviced by tens of light-harvesting proteins;
(b) the wide variety of these complexes allows individual species the ability to survive
in varying light conditions; (c) light-harvesting antennae have evolved photoprotec-
tion responses, such as down-regulation, to avoid damage in the case of excessive
exposure to sunlight.

THE ROLE OF RET IN LIGHT-HARVESTING COMPLEXES
119
700
600
500
Wavelength (nm)
LHC-II
PC612
400
PE545
PE555
PC645
FIGURE 3.9
Normalized absorption spectrum of LHC-II (green); the main light-harvesting
protein in high order plants along with the normalized absorption spectra of four cryptophyte
light-harvesting proteins: PE545 (blue), PE555 (pink), PC612 (purple), and PC645 (red). The
different region in which these complexes absorb is clearly evident. (For a color version of
this figure, see the color plate section.)
3.5.2
Photosynthetic Excitons
Within a light-harvesting complex, strong electronic interactions between chro-
mophores can result in new delocalized excited states. Although not always present,
these delocalized excited states known as excitons can extend over multiple chro-
mophores and can have a profound impact on the electronic structure and energy
FIGURE 3.10
Structural models of two photosynthetic light harvesting complexes: (a) a
cryptophyte PC645 antenna complex, and, (b) the major chlorophyll-a/b complex from higher
plants and green algae, LHCII. (For a color version of this figure, see the color plate section.)

120
RESONANCE ENERGY TRANSFER
en
em
g
g
eβ
Eβ
eα
E
Absorption
Energy
α
FIGURE 3.11
Illustration of the electronic energy spacing of a homodimer, which consists
of two identical chromophores, each modeled as a two-level system. Molecular excited states
en and em are coupled by an electronic potential, Vnm, resulting in delocalized excited states e𝛼
and e𝛽. Absorption frequencies of the dimer occur at the corresponding energies E𝛼and E𝛽.
transfer dynamics within a light-harvesting protein. The Frenkel or molecular exci-
ton model describes delocalized excited states in biological systems and molecular
aggregates arising from the superposition of localized molecular excited states [18].
In other words, several molecules can cooperatively (in phase) absorb light and share
the excitation quantum mechanically.
Delocalization of the excited state results in an increase in the spatial extent of
the excited state, changing the nature of the chromophore (Fig. 3.11). The LH2 com-
plex, isolated from purple bacteria, one of the most widely studied light-harvesting
complexes, provides a clear example of how excitons can substantially change the
electronic structure of a protein as well as the energy transfer dynamics [96-98].
The LH2 complex only contains one type of chromophore, bacteriochlorophyll-a
molecules; 27 of these molecules are arranged into two rings. In one ring, there are
18 closely packed B850 bacteriochlorophyll-a molecules; due to their close prox-
imity to each other along with their preferential orientations the coupling between
these chromophores is strong, with nearest neighbor coupling being approximately
300 cm−1. In the B800 ring, there are nine loosely packed bacteriochlorophyll-
a molecules where adjacent molecules have a 30 cm−1 electronic coupling. Photon
absorption in the B800 ring occurs at 800 nm, corresponding to electronically isolated
chlorophyll-a molecules, while the B850 ring is red-shifted to 850 nm (Fig. 3.12).
This shift is in part a response to interactions between the chromophores and the
protein; however, a significant contribution is due to the strong excitonic coupling of
molecules. In addition to changing the electronic landscape of the system, excitons in
LH2 also change the energy transfer dynamics by setting up a gradient which allows
excitation to flow “downhill” from the high energy B800 ring to the lower energy
B850 ring.
F¨orster theory for molecules cannot capture the consequences of delocalized exci-
tation, and therefore, the unexpectedly rapid timescale for energy transfer in several
light harvesting complexes were a mystery for many years. Eventually it was realized

THE ROLE OF RET IN LIGHT-HARVESTING COMPLEXES
121
B850
B850
B800
B800
700
ea
g
g
g
ea
eb
2Vab
eβ
eα
800
Wavelength (nm)
chromophore a
chromophore a
chromophore b
900
FIGURE 3.12
(a) Absorption spectrum of LH2 extracted from a strain of the purple bac-
terium Rhodopseudomonas acidophila. The B800 ring absorbs at 800 nm and is due to the
nine weakly coupled BChl chromophores, while the B850 ring absorbs at 850 nm and is due to
the 18 tightly packed BChl molecules. (b) X-ray crystallography structure of LH2 from Rps.
acidophila (strain 10050) illustrating 27 bacteriochlorophyll-a chromophores. Blue-colored
chromophores indicate the B800 ring, while red-colored chromophores indicate the B850 ring.
Energy-level diagrams for chromophores in the (c) B800 ring and (d) B850 ring. Adapted from
References 101 and 102. (For a color version of this figure, see the color plate section.)
that donors and acceptors needed to be redefined in order to formulate an appropriate
theory, called Generalized F¨orster Theory (GFT) [99,100] that could quantitatively
predict energy transfer in excitonic systems. The idea is to partition the system into
both strongly coupled chromophores and weakly coupled chromophores, then choose
effective donor and acceptor electronic states. GFT is applicable when multiple chro-
mophores, which are strongly coupled, act together as a donor and are weakly coupled
to another group of strongly coupled acceptors.

122
RESONANCE ENERGY TRANSFER
ACKNOWLEDGMENTS
Research at the University of East Anglia is supported by the Leverhulme Trust
and EPSRC. The work at the University of Toronto is supported by the Natural
Sciences and Engineering Research Council of Canada and the US Air Force Office
of Scientific Research (FA9550-10-1-0260).
REFERENCES
[1] J. Franck, “Einige aus der Theorie von Klein und Rosseland zu ziehende Folgerun-
gen ¨uber Fluorscence, photochemische Prozesse und die Electronenemission gl¨uhender
K¨orper,” Z. Phys. 9, 259–266 (1922).
[2] G. Cario, “ ¨Uber Entstehung wahrer Lichtabsorption un scheinbare Koppelung von
Quantenspr¨ungen,” Z. Phys. 10, 185–199 (1922).
[3] G. Cario and J. Franck, “ ¨Uber Zerlegugen von Wasserstoffmolek¨ulen durch angeregte
Quecksilberatome,” Z. Phys. 11, 161–166 (1922).
[4] E. Gaviola and P. Pringsheim, “ ¨Uber den einfluß der konzentration auf die polarisation
der fluoreszenz von farbstoffl¨osungen,” Z. Phys. 24, 24–36 (1924).
[5] J. Perrin, “Fluorescence et induction mol´eculaire par r´esonance,” C. R. Acad. Sci. 184,
1097–1100 (1927).
[6] F. Perrin, “Th´eorie quantique des transferts d’activation entre molecules de mˆeme
esp´ece. Cas des solutions fluorescents,” Ann. Phys. 17, 283–314 (1932).
[7] H. Kallman and F. London, “ ¨Uber quantenmechanische Energie¨ubertragung zwischen
atomaren Systemen” Z. Phys. Chem. B 2, 207–243 (1928).
[8] M. N. Berberan-Santos, “Pioneering contributions of Jean and Francis Perrin to molecu-
lar luminescence,” in New Trends in Fluorescence Spectroscopy. Applications to Chem-
ical and Life Sciences, edited by B. Valeur and J.-C. Brochon (Springer, Berlin, 2001),
Chap. 2.
[9] T. F¨orster, “Energiewanderung und fluoreszenz,” Naturwissenschaften 33, 166–175
(1946).
[10] T. F¨orster, “Zwischenmolekulare Energiewanderung und Fluoreszenz,” Ann. Phys. 2,
55–75 (1948).
[11] T. F¨orster, “10th Spiers Memorial Lecture. Transfer mechanisms of electronic excita-
tion,” Discuss. Faraday Soc. 27, 7–17 (1959).
[12] S. A. Latt, H. T. Cheung, and E. R. Blout, “Energy transfer: a system with relatively
fixed donor-acceptor separation,” J. Am. Chem. Soc. 87, 995–1003 (1965).
[13] L. Stryer and R. P. Haugland, “Energy transfer: a spectroscopic ruler,” Proc. Natl. Acad.
Sci. 58, 719–726 (1967).
[14] R. M. Clegg, “The history of FRET: from conception through the labors of birth,” in
Reviews in Fluorescence, edited by C. D. Geddes and J. R. Lakowicz (Springer, New
York, 2006), Vol. 3, Chap. 1.
[15] G. D. Scholes, “Long-range resonance energy transfer in molecular systems,” Annu.
Rev. Phys. Chem. 54, 57–87 (2003).
[16] D. L. Andrews and J. Rodr´ıguez, “Resonance energy transfer: spectral overlap, efficiency
and direction,” J. Chem. Phys. 127, 084509 (2007).

REFERENCES
123
[17] L. Mandel and E. Wolf, Optical Coherence and Quantum Optics (University Press,
Cambridge, 1995), p. 871.
[18] G. D. Scholes and G. Rumbles, “Excitons in nanoscale systems,” Nat. Mater. 5, 683–696
(2006).
[19] V. May and O. K¨uhn, Charge and Energy Transfer Dynamics in Molecular Systems
(Wiley-VCH, Weinheim, 2004).
[20] A. Ishizaki, T. R. Calhoun, G. S. Schlau-Cohen, and G. R. Fleming, “Quantum coherence
and its interplay with protein environments in photosynthetic electronic energy transfer,”
Phys. Chem. Chem. Phys. 12, 7319–7337 (2010).
[21] J. R. Lakowicz, Principles of Fluorescence Spectroscopy, 2nd ed. (Kluwer Academic,
New York, 1999), Chap. 10.
[22] B. Valeur, Molecular Fluorescence: Principles and Applications (Wiley-VCH,
Weinheim, 2002), Chap. 9.
[23] S. E. Braslavsky, E. Fron, H. B. Rodr´ıguez, E. San Rom´an, G. D. Scholes, G. Schweitzer,
B. Valeur, and J. Wirz, “Pitfalls and limitations in the practical use of F¨orster’s theory
of resonance energy transfer,” Photochem. Photobiol. Sci. 7, 1444–1448 (2008).
[24] T. Ansbacher, H. K. Srivastava, T. Stein, R. Baer, M. Merkx, and A. Shurki, “Calculation
of transition dipole moment in fluorescent proteins—towards efficient energy transfer,”
Phys. Chem. Chem. Phys. 14, 4109–4117 (2012).
[25] C. Galli, K. Wynne, S. M. Lecours, M. J. Therien, and R. M. Hochstrasser, “Direct
measurement of electronic dephasing using anisotropy,” Chem. Phys. Lett. 206, 493–
499 (1993).
[26] B. W. van der Meer, G. Coker, and S.-Y. Chen, Resonance Energy Transfer Theory and
Data (VCH, New York, 1994).
[27] B. W. van der Meer, Resonance Energy Transfer, edited by D. L. Andrews and A. A.
Demidov (Wiley, New York 1999), pp. 151–172.
[28] D. L. Andrews and T. Thirunamachandran, “On three-dimensional rotational averages,”
J. Chem. Phys. 67, 5026–5033 (1977).
[29] W. L. Levshin, “Polarisierte Fluoreszenz und Phosphoreszenz der Farbstofflosungen.
IV,” Z. Phys. 32, 307–326 (1925).
[30] F. Perrin, “La fluorescence des solutions,” Ann. Phys. 12, 169–275 (1929).
[31] V. M. Agranovich and M. D. Galanin, Electronic Excitation Energy Transfer in Con-
densed Matter (Elsevier/North-Holland, Amsterdam, The Netherlands, 1982).
[32] D. L. Andrews and G. Juzeliunas, “The range dependence of fluorescence anisotropy in
molecular energy transfer,” J. Chem. Phys. 95, 5513–5518 (1991).
[33] P. R. Butler and M. J. Pilling, “The breakdown of F¨orster kinetics in low viscosity
liquids. An approximate analytical form for the time-dependent rate constant,” Chem.
Phys. 41, 239–243 (1979).
[34] U. G¨osele, M. Hauser, U. K. A. Klein, and R. Frey, “Diffusion and long-range energy
transfer,” Chem. Phys. Lett. 34, 519–522 (1975).
[35] B. P. Krueger, G. D. Scholes, and G. R. Fleming, “Calculation of couplings and energy
transfer pathways between the pigments of LH2 by the ab initio transition density cube
method,” J. Phys. Chem. B 102, 5378–5386 (1998).
[36] D. L. Andrews and J. M. Leeder, “Resonance energy transfer: when a dipole fails,”
J. Chem. Phys. 130, 184504 (2009).

124
RESONANCE ENERGY TRANSFER
[37] D. L. Andrews, “A unified theory of radiative and radiationless molecular-energy trans-
fer,” Chem. Phys. 135, 195–201 (1989).
[38] A. A. Demidov and D. L. Andrews, in Encyclopedia of Chemical Physics and Physical
Chemistry, edited by J. H. Moore and N. D. Spencer (Institute of Physics, Bristol, 2001),
Vol. 3, pp. 2701—2715.
[39] D. L. Andrews, “Mechanistic principles and applications of resonance energy transfer,”
Canad. J. Chem. 86, 855–870 (2008).
[40] D. L. Dexter, “A theory of sensitized luminescence in solids,” J. Chem. Phys. 21,
836–850 (1953).
[41] D. L. Andrews, C. Curutchet, and G. D. Scholes, “Resonance energy transfer: beyond
the limits,” Laser Photon. Rev. 5, 114–123 (2011).
[42] I. Z. Steinberg, “Long-range nonradiative transfer of electronic excitation energy in
proteins and polypeptides,” Annu. Rev. Biochem. 40, 83–114 (1971).
[43] L. Stryer, “Fluorescence energy transfer as a spectroscopic ruler,” Ann. Rev. Biochem.
47, 819–846 (1978).
[44] C. G. dos Remedios, M. Miki, and J. A. Barden, “Fluorescence resonance energy transfer
measurements of distances in actin and myosin. A critical evaluation,” J. Muscle Res.
Cell Motil. 8, 97–117 (1987).
[45] P. Wu and L. Brand, “Resonance energy transfer: methods and applications,” Anal.
Biochem. 218, 1–13 (1994).
[46] P. R. Selvin, “The renaissance of fluorescence resonance energy transfer,” Nat. Struct.
Biol. 7, 730–734 (2000).
[47] T. Ha, T. Enderle, D. F. Ogletree, D. S. Chemla, P. R. Selvin, and S. Weiss, “Probing
the interaction between two single molecules: fluorescence resonance energy transfer
between a single donor and a single acceptor,” Proc. Natl. Acad. Sci. USA 93, 6264–6268
(1996).
[48] S. Hohng, C. Joo, and T. Ha, “Single-molecule three-color FRET,” Biophys. J. 87,
1328–1337 (2004).
[49] B. Schuler, “Single-molecule fluorescence spectroscopy of protein folding,” Chem.
Phys. Chem. 6, 1206–1220 (2005).
[50] S. V. Koushik, H, Chen, C. Thaler, H. L. Puhl III, and S. S. Vogel, “Cerulean, venus and
venusY67C FRET reference standards,” Biophys. J. 91, L99–L101 (2006).
[51] H. Sahoo, “F¨orster resonance energy transfer–a spectroscopic nanoruler: principle and
applications,” J. Photochem. Photobiol. C 12, 20–30 (2011).
[52] C. G. dos Remedios and P. D. J. Moens, “Fluorescence resonance energy transfer
spectroscopy is a reliable ‘ruler’ for measuring structural changes in proteins—dispelling
the problem of the unknown orientation factor,” J. Struct. Biol. 115, 175–185 (1995).
[53] F. Tanaka, “Theory of time-resolved fluorescence under the interaction of energy transfer
in a bichromophoric system: effect of internal rotations of energy donor and acceptor,”
J. Chem. Phys. 109, 1084–1092 (1998).
[54] Z. G. Yu, “Fluorescent resonant energy transfer: correlated fluctuations of donor and
acceptor,” J. Chem. Phys. 127, 221101 (2007).
[55] M. Isaksson, N. Norlin, P.-O. Westlund, and L. B.- ˚A. Johansson, “On the quantitative
molecular analysis of electronic energy transfer within donor-acceptor pairs,” Phys.
Chem. Chem. Phys. 9, 1941–1951 (2007).

REFERENCES
125
[56] S. Jang, “Generalization of the F¨orster resonance energy transfer theory for quantum
mechanical modulation of the donor-acceptor coupling,” J. Chem. Phys. 127, 174710
(2007).
[57] D. S. Talaga, W. L. Lau, H. Roder, J. Tang, Y. W. Jia, W. F. DeGrado, and R. M.
Hochstrasser, “Dynamics and folding of single two-stranded coiled-coil peptides studied
by fluorescent energy transfer confocal microscopy,” Proc. Natl. Acad. Sci. USA 97,
13021–13026 (2000).
[58] T. Heyduk, “Measuring protein conformational changes by FRET/LRET,” Curr. Opin.
Biotechnol. 13, 292–296 (2002).
[59] B. Schuler and W. A. Eaton, “Protein folding studied by single-molecule FRET,” Curr.
Opin. Struct. Biol. 18, 16–26 (2008).
[60] T. Zal and N. R. J. Gascoigne, “Using live FRET imaging to reveal early protein–protein
interactions during T cell activation,” Curr. Opin. Immunol. 16, 418–427 (2004).
[61] M. Parsons, B. Vojnovic, and S. Ameer-Beg, “Imaging protein–protein interactions
in cell motility using fluorescence resonance energy transfer (FRET),” Biochem. Soc.
Trans. 32, 431–433 (2004).
[62] W. B. Frommer, M. W. Davidson, and R. E. Campbell, “Genetically encoded biosensors
based on engineered fluorescent proteins,” Chem. Soc. Rev. 38, 2833–2841 (2009).
[63] A. W. Nguyen, X. You, A. M. Jabaiah, and P. S. Daugherty, “Fluorescent protein FRET
applications,” in Reviews in Fluorescence, edited by C. D. Geddes (Springer, New York,
2010), Chap. 13.
[64] A. Hillisch, M. Lorenz, and S. Diekmann, “Recent advances in FRET: distance
determination in protein–DNA complexes,” Curr. Opin. Struct. Biol. 11, 201–207
(2001).
[65] F. G. E. Cremazy, E. M. M. Manders, P. I. H. Bastiaens, G. Kramer, G. L. Hager, E.
B. van Munster, P. J. Verschure, T. W. J. Gadella, and R. van Driel, “Imaging in situ
protein–DNA interactions in the cell nucleus using FRET–FLIM,” Exp. Cell Res. 309,
390–396 (2005).
[66] P. R. Selvin, “Principles and biophysical applications of lanthanide-based probes,” Annu.
Rev. Biophys. Biomol. Struct. 31, 275–302 (2002).
[67] W. C. W. Chan, D. J. Maxwell, X. Gao, R. E. Bailey, M. Han, and S. Nie, “Luminescent
quantum dots for multiplexed biological detection and imaging,” Curr. Opin. Biotechnol.
13, 40–46 (2002).
[68] A. R. Clapp, I. L. Medintz, and H. Mattoussi, “F¨orster resonance energy transfer inves-
tigations using quantum-dot fluorophores,” Chem. Phys. Chem. 7, 47–57 (2006).
[69] R. M. Clegg, “Fluorescence resonance energy transfer,” in Fluorescence Imaging Spec-
troscopy and Microscopy, edited by X. F. Wang and B. Herman (John Wiley & Sons,
New York, 1996), Chap. 7.
[70] A. Periasamy, Methods in Cellular Imaging (Oxford University Press, New York, 2001).
[71] F. S. Wouters, P. J. Verveer, and P. I. H. Bastiaens, “Imaging biochemistry inside cells,”
Trends Cell Biol. 11, 203–211 (2001).
[72] A. Hoppe, K. Christensen, and J. A. Swanson, “Fluorescence resonance energy transfer-
based stoichiometry in living cells,” Biophys. J. 83, 3652–3664 (2002).
[73] E. A. Jares-Erijman and T. M. Jovin, “FRET imaging,” Nat. Biotechnol. 21, 1387–1395
(2003).

126
RESONANCE ENERGY TRANSFER
[74] A. Periasamy and R. N. Day, Molecular Imaging: FRET Microscopy and Spectroscopy
(Oxford University Press, New York, 2005).
[75] R. N. Day and M. W. Davidson, “Fluorescent proteins for FRET microscopy: monitoring
protein interactions in living cells,” Bioessays 34, 341–350 (2012).
[76] G. W. Gordon, G. Berry, X. H. Liang, B. Levine, and B. Herman, “Quantitative flu-
orescence resonance energy transfer measurements using fluorescence microscopy,”
Biophys. J. 74, 2702–2713 (1998).
[77] Z. Xia and Y. Liu “Reliable and global measurement of fluorescence resonance energy
transfer using fluorescence microscopes,” Biophys. J. 81, 2395–2402 (2001).
[78] J. van Rheenen, M. Langeslag, and K. Jalink, “Correcting confocal acquisition to
optimize imaging of fluorescence resonance energy transfer by sensitized emission,”
Biophys. J. 86, 2517–2529 (2004).
[79] T. S. Karpova, C. T. Baumann, L. He, X. Wu, A. Grammer, P. Lipsky, G. L. Hager, and
J. G. McNally, “Fluorescence resonance energy transfer from cyan to yellow fluorescent
protein detected by acceptor photobleaching using confocal microscopy and a single
laser,” J. Microsc. 209, 56–70 (2003).
[80] E. B. van Munster, G. J. Kremers, M. J. W. Adjobo-Hermans, and T. W. J. Gadella,
“Fluorescence resonance energy transfer (FRET) measurement by gradual acceptor
photobleaching,” J. Microsc. 218, 253–262 (2005).
[81] P. I. H. Bastiaens and A. Squire, “Fluorescence lifetime imaging microscopy: spatial
resolution of biochemical processes in the cell,” Trends Cell Biol. 9, 48–52 (1999).
[82] R. R. Duncan, A. Bergmann, M. A. Cousin, D. K. Apps, and M. J. Shipston, “Multi-
dimensional time-correlated single photon counting (TCSPC) fluorescence lifetime
imaging microscopy (FLIM) to detect FRET in cells,” J. Microsc. 215, 1–12 (2004).
[83] H. Wallrabe and A. Periasamy, “Imaging protein molecules using FRET and FLIM
microscopy,” Curr. Opin. Biotechnol. 16, 19–27 (2005).
[84] M. Peter, S. M. Ameer-Beg, M. K. Y. Hughes, M. D. Keppler, S. Prag, M. Marsh, B.
Vojnovic, and T. Ng, “Multiphoton-FLIM quantification of the EGFP-mRFP1 FRET pair
for localization of membrane receptor-kinase interactions,” Biophys. J. 88, 1224–1237
(2005).
[85] T. Kawazoe, K. Kobayashi, and M. Ohtsu, “Optical nanofountain: a biomimetic device
that concentrates optical energy in a nanometric area,” Appl. Phys. Lett. 86, 103102
(2005).
[86] H. Morawetz, “Studies of synthetic polymers by nonradiative energy transfer,” Science
240, 172–176 (1988).
[87] J. P. S. Farinha and J. M. G. Martinho, “Resonance energy transfer in polymer interfaces,”
Springer Ser. Fluoresc. 4, 215–255 (2008).
[88] G. Srinivas and B. Bagchi, “Detection of collapsed and ordered polymer structures by
fluorescence resonance energy transfer in stiff homopolymers: bimodality in the reaction
efficiency distribution,” J. Chem. Phys. 116, 837–844 (2002).
[89] S. Saini, H. Singh, and B. Bagchi, “Fluorescence resonance energy transfer (FRET) in
chemistry and biology: non-F¨orster distance dependence of the FRET rate,” J. Chem.
Sci. 118, 23–35 (2006).
[90] R. K. Castellano, S. L. Craig, C. Nuckolls, and J. Rebek, “Detection and mechanistic
studies of multicomponent assembly by fluorescence resonance energy transfer,” J. Am.
Chem. Soc. 122, 7876–7882 (2000).

REFERENCES
127
[91] C. Cano-Raya, M. D. Fern´andez-Ramos, and L. F. Capit´an-Vallvey, “Fluorescence
resonance energy transfer disposable sensor for copper(II),” Anal. Chim. Acta 555,
299–307 (2006).
[92] A. E. Albers, V. S. Okreglak, and C. J. Chang, “A FRET-based approach to ratiomet-
ric fluorescence detection of hydrogen peroxide,” J. Am. Chem. Soc. 128, 9640–9641
(2006).
[93] H. Zhang and D. M. Rudkevich, “A FRET approach to phosgene detection,” Chem.
Commun. 1238–1239 (2007).
[94] R. E. Blankenship, Molecular Mechanisms of Photosynthesis (Blackwell Science,
Oxford, 2002).
[95] G. D. Scholes, G. R. Fleming, A. Olaya-Castro, and R. van Grondelle, “Lessons from
nature about solar light harvesting,” Nat. Chem. 3, 763–774 (2011).
[96] G. D. Scholes and G. R. Fleming, “On the mechanism of light-harvesting in photosyn-
thetic purple bacteria: B800 to B850 energy transfer,” J. Phys. Chem. B 104, 1854–1868
(2000).
[97] R. J. Cogdell, A. Gall, and J. K¨ohler, “The architecture and function of the light-
harvesting apparatus of purple bacteria: from single molecules to in vivo membranes,”
Q. Rev. Biophys. 39, 227–324 (2006).
[98] J. Str¨umpfer, M. Sener, and K, Schulten, “How quantum coherence assists photosyn-
thetic light harvesting,” J. Phys. Chem. Lett. 4, 536–542 (2012).
[99] G. D. Scholes, X. J. Jordanides, and G. R. Fleming, “Adapting the F¨orster theory of
energy transfer for modeling dynamics in aggregated molecular assemblies,” J. Phys.
Chem. B 105, 1640–1651 (2001).
[100] S. Jang and Y.-C. Cheng, “Resonance energy flow dynamics of coherently delocalized
excitons in biological and macromolecular systems: recent theoretical advances and
open issues,” WIREs Comput. Mol. Sci. 3, 84–104 (2013).
[101] J. L. Herek, W. Wohlleben, R. J. Cogdell, D. Zeidler, and M. Motzkus, “Quantum control
of energy flow in light harvesting,” Nature 417, 533–535 (2002).
[102] F. Kulzer and M. Orrit, “Single molecule optics,” Annu. Rev. Phys. Chem. 55, 585–611
(2004).


4
BIOPHOTONICS OF
PHOTOSYNTHESIS
Valter Zazubovich1 and Ryszard Jankowiak2,3
1Department of Physics, Concordia University, Montreal, QC, Canada
2Department of Chemistry, Kansas State University, Manhattan, KS, USA
3Department of Physics, Kansas State University, Manhattan, KS, USA
4.1
INTRODUCTION
Biophotonics (an extension of photonics), in a broad sense, refers to techniques and
phenomena related to interaction between biological matter and light (i.e., photons).
For example, Nature uses biophotonics to harvest photons to achieve photosynthesis
and to create vision. Light-absorbing pigments in various biological systems start
a highly complex series of photoinitiated processes. Photosynthesis is likely the
best-known and most widespread process involving interaction between light and
biological entities. In photosynthesis, light is harvested and its energy is used to drive
chemical reactions that support nearly all life on Earth. Photosynthetic organisms
possess a large variety of pigment–protein complexes (which perform as either light-
harvesting antennae or as reaction centers (RCs) where primary photochemistry
takes place), containing dense networks of chlorophylls (Chls) and other pigments.
In plants, algae, and cyanobacteria, these proteins are grouped into Photosystem I
(PS I) and Photosystem II (PS II) that work together as parts of the electron transfer
chain [1, 2]. Other organisms, incapable of oxygenic photosynthesis, have various
dissimilar antenna arrangements, but, interestingly, the RC design is similar for all
known photosynthesizing species (see below). Detailed information on structures of
various photosynthetic proteins was obtained by X-ray crystallography [3–10].
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
129

130
BIOPHOTONICS OF PHOTOSYNTHESIS
Photosynthesis research is a dynamic field, with most efforts currently directed
toward detailed understanding of PS II functioning, hydrogen and biofuel produc-
tion, as well as utilizing solutions tested by Nature in light-harvesting complexes to
improve solar cells. Another field related to biophotonics of photosynthesis involves
studies of pigment–protein interactions and protein dynamics, at both physiological
and low temperatures. In this case, one is interested not only in protein influence on
excitation energy transfer (EET) and/or charge separation (CS) rates, but also in the
effect of the protein scaffolding on pigment-site energies.
Photosynthetic proteins serve as model systems for protein research in general.
Natural photosynthetic complexes (PCs) are excellent working models for the devel-
opment of better artificial PCs that will serve human needs. Optical spectroscopies
(both in time and frequency domains) can directly probe interactions of pigment–
protein complexes with light and are widely used in studies of the primary processes
of photosynthesis.
This chapter is to some extent based on our recent review of hole burning (HB) and
related spectroscopies [11], but also includes the most recent developments in the field
of photosynthesis biophotonics. Due to space limitations, regrettably many interesting
and important issues will not be addressed in any detail; in particular, not much
attention will be devoted to time-domain methods used in the area of photosynthesis
research. Readers interested in these methods are referred to an excellent book [12].
The recently developed two-dimensional electronic spectroscopy (2D-ES), which
probes excitonic couplings and EET on the femtosecond time scale, and serves as a
unified framework for most time- and frequency-domain techniques, is discussed in
References 13–15. Brief description of the 2D-ES, following [13] and some recent
photosynthesis-related results were reviewed also in Reference 11.
In this chapter, we will discuss relevant experimental techniques, key physical
concepts, and some information gained from frequency-domain optical spectroscopic
studies of PCs. Particular attention will be devoted to results of single photosynthetic
complex spectroscopy (SPCS), as well as to those of complementary subensemble
frequency-domain techniques such as (non-photochemical) spectral hole burning
(NPHB). The chapter is concluded with comments on current efforts and future
directions in frequency-domain spectroscopies in the area of photosynthesis.
4.2
STRUCTURE OF PIGMENT–PROTEIN COMPLEXES
AND STRUCTURE–FUNCTION RELATIONSHIPS
4.2.1
Photosystem I (PS I) and Photosystem II (PS II)
These PCs, involved in oxygenic photosynthesis, use chlorophyll (Chl) molecules
or their magnesium-free analogs, pheophytins (Pheo), as both donors and acceptors
of energy and electrons; various other pigments (e.g., carotenoids) are employed
to increase the range of useful absorbed wavelengths and to assist in photoprotec-
tion. The electrons are produced by PS II via the light-induced splitting of water;
other members of the electron transport chain, for example, Cytochrome b6f, further

STRUCTURE OF PIGMENT–PROTEIN COMPLEXES
131
FIGURE 4.1
(a) Photosystem II dimeric core, 3ARC.pdb [3]; peripheral light-harvesting
complexes not depicted. (b) Plant Photosystem I, 3LW5.pdb [4]. Gray/green: Chl a; dark blue:
Pheo a; yellow: carotenoids; red: quinones; orange: non-heme iron (PS II); orange/yellow:
FeS clusters (PS I). Both complexes are depicted with stromal side facing the viewer. This and
subsequent structure figures were produced with RasMol software. (For a color version of this
figure, see the color plate section.)
contribute to the transmembrane proton gradient [16]. The electrons are shuttled
through a variety of electron carriers to the PS I complex where a second light-induced
CS reaction takes place, and electrons are used to reduce NADP+ to NADPH. The
proton gradient is used by ATP-ase, and ATP and NADPH are then employed to
power the Calvin cycle and create the carbohydrates.
Figure 4.1 shows structures of two PCs from higher plants, PS II (frame A [3])
and PS I (frame B [4]). The PS II core is dimeric, with each monomer consisting of
the core antenna complexes CP43 and CP47, and the RC. Additionally, the PS II core
is surrounded by peripheral antenna complexes LHC II [9], CP24, CP26, and CP29,
not depicted in Figure 4.1a [16]. The plant PS I supercomplex pictured in Figure 4.1b
consists of the core complex (most notably the PsaA and PsaB Chl-binding proteins)
and the LHC I peripheral light-harvesting complexes (Lhca1-4) [4]. In cyanobacteria,
the PS I core occurs in a trimeric form, and Lhca-type antenna complexes are not
present [7]. In the trimer, the monomers are oriented with the PsaL subunit (see
Fig. 4.1) toward the C3 symmetry axis. Environmental conditions can cause significant

132
BIOPHOTONICS OF PHOTOSYNTHESIS
FIGURE 4.2
(a) Arrangement of pigments in active (D1) and inactive (D2) branches of the
PS II RC; 3ARC.pdb [3]. Green: Chl a; dark blue: Pheo a; yellow: carotenoids; red: quinones;
orange: non-heme iron; brown/purple: oxygen-evolving cluster, (b) Pigment arrangement in
BRC, 1M3X.pdb [5]. Green: BChl a; dark blue: BPheo a; yellow: carotenoids; red: quinones;
orange: non-heme iron. Gray wireframe represents protein and some lipids. (For a color version
of this figure, see the color plate section.)
changes in supercomplex structures. For instance, cyanobacteria grown in iron-deficit
conditions exhibit one or more rings of IsiA (CP43′) protein, homologous to CP43,
surrounding the trimeric PS I core; a fraction of LHC II complexes is known to migrate
between PS II and PS I, depending on the illumination levels. Structures shown here
are meant only to give a flavor of the complex organization of photosynthetic proteins.
Figure 4.2a shows a schematic arrangement of the six chlorophylls (Chls) and
two pheophytins (Pheos) in PS II RC based on the structure of T. vulcanus [3]. The
PD1 and PD2 Chls are analogous to the PA and PB bacteriochlorophylls (BChls) of
the bacterial reaction center (BRC) special pair (Fig. 4.2b), respectively, while the
ChlD1,D2 and PheoD1,D2 molecules correspond to the monomeric BChlA,B and the
bacteriopheophytin (BPheoA,B) molecules of the BRCs. By analogy with the BRCs,
it is believed that the PD1/PD2, ChlD1, and PheoD1 molecules participate in primary
CS in the PS II RC, although the nature of the primary electron donor in isolated PS
II RCs is still a matter of debate (see Section 4.5.5). An obvious difference between
PS II RC and BRC is that the former contains two additional peripheral Chls (i.e.,
ChlzD1 and ChlzD2). However, both are weakly coupled to the remaining pigments
and do not affect CS dynamics significantly.
4.2.2
PCs of Purple Bacteria
Unlike plants, algae, or cyanobacteria, purple bacteria are incapable of oxygenic
photosynthesis. However, design features of bacterial reaction centers (BRCs) closely
resemble those of higher plants (Fig. 4.2). BRC contains two main subunits, L and M,
approximately related to each other by C2 symmetry, as well as the H-subunit and the
cytochrome [5]. L and M proteins provide the scaffold for two nearly symmetrical
arrangements of BChl and BPheo molecules, see Figure 4.2b. Upon excitation of
any RC pigments, the energy is quickly transferred to the pair of strongly coupled
BChl a molecules (special pair), which serves as a primary electron donor. Despite
the nearly symmetric two-branch arrangement, the electron transfer occurs along the
L- (A in PS I, D1 in PS II) branch in wild-type organisms. The electrons are quickly

KEY CONCEPTS IN PHYSICS OF PIGMENT–PROTEIN COMPLEXES
133
FIGURE 4.3
(a) The BRC-LH1 complex structure, 1PYH.pdb [10]. PufX subunit is not
depicted, but its location is indicated. The complex is depicted with H subunit (see Fig. 4.2)
toward the viewer. (b) LH2 structure, 2FKW.pdb [6], depicted with the cytoplasm side up. In
both frames, gray/green: Bchl a; yellow: carotenoids. (For a color version of this figure, see
the color plate section.)
transported to QA immobile quinone and then to QB mobile quinone acceptors. The
lowest energy absorption of the BRC belongs to the special pair, and is located at a
wavelength ranging from ∼870 nm to ∼960 nm, depending on the particular species.
The BRC is surrounded by the LH1 antenna protein (Fig. 4.3a). Although initially
it was believed that LH1 exhibits nearly perfect Cn symmetry, recent AFM and
cryo-microscopy studies indicated the perfect ring is broken by the PufX protein
subunit [10]. Nevertheless, the LH1 complex is an assembly of n identical protein
subunits holding in place an array of strongly coupled BChl a molecules. Each subunit
contains two BChl a molecules in a slightly nonidentical local environment. The
LH1-BRC assembly is surrounded by multiple copies of the LH2 antenna complex
[6]. The latter is approximately of Cn symmetry (with n = 8 or 9, depending on
the species), Figure 4.3b. Unlike LH1, LH2 contains two distinct rings of BChl a
molecules. While in the B850 ring (labeling refers to the wavelengths of the strongest
absorption band associated with respective ring, see also Section 4.5.1), center-to-
center distances between neighboring BChl a molecules are of the order of 9 ˚A,
electrostatic couplings are strong and respective excitations are highly delocalized,
and molecules of the B800 ring are relatively weakly interacting. Overall, energy
harvested by the B800 ring of the LH2 is transferred to the B850 ring in about 1.6 ps
at low temperatures and in less than one picosecond at physiological temperatures.
In several picoseconds the excitation energy is transferred to the LH1 complexes and
then to the BRCs. Net effectiveness of this chain of processes approaches 100% in
terms of the number of primary CS events per number of photons absorbed.
4.3
KEY CONCEPTS IN PHYSICS OF
PIGMENT–PROTEIN COMPLEXES
The physical concepts behind light-harvesting and primary CS, and their optical
spectroscopic manifestations, have been addressed in numerous books and reviews
[2, 11, 12, 16–19]. Thus, here we will briefly discuss the most relevant concepts and
phenomena, which are essential for understanding the examples in Section 4.5.

134
BIOPHOTONICS OF PHOTOSYNTHESIS
4.3.1
Excitons
As evident from the previous section, light-harvesting complexes are networks of
interacting pigment molecules. The concept of an exciton, the excited electronic state
delocalized over many molecules, proved extremely useful in describing the pro-
cesses occurring in light-harvesting complexes and their spectroscopic properties. It
originates from condensed-matter physics; namely, due to the electrostatic interac-
tions between identical molecules in the crystal, the excitation becomes delocalized,
and excitation energy can travel from molecule to molecule in a wave-like fashion
[20]. In other words, the resulting excited state becomes a superposition of excited
states of multiple molecules (in case of a perfect crystal—of all molecules in the
crystal). Such excitons are called Frenkel excitons.
In order to offer a simple illustration, we consider the case of a pigment dimer. The
logic of this example can be easily extended to PCs containing more than two pigment
molecules. Consider an isolated molecule with eigenfunctions 𝜑i and eigenenergies
Ei defined by the Hamiltonian
H𝜑i = Ei𝜑i,
(4.1)
where i = g or e for ground and excited electronic states, respectively. If one now
introduces one more identical molecule into the system, interacting with the first one
with the (electrostatic) potential energy V, total wavefunctions become combinations
of products of wavefunctions of individual molecules. The wavefunction for the dimer
ground state becomes 𝜑g
1𝜑g
2 and the single-excited states localized on molecules
1 and 2 become 𝜓e
1 = 𝜑e
1𝜑g
2 and 𝜓e
2 = 𝜑g
1𝜑e
2, respectively. Wavefunctions of the
excitonically coupled dimer may be written as 𝜓± =
1
√
2(𝜓e
1 ± 𝜓e
2), with
⟨𝜓g|H1 + H2 + V|𝜓g⟩= 2Eg + Dg and
⟨𝜓±|H1 + H2 + V|𝜓±⟩= Eg + Ee + De ± V1,2
,
(4.2)
where Dg = ⟨𝜓g|V|𝜓g⟩is van der Waals interaction or dispersion energy for the
ground state, De = ⟨𝜓e
1|V|𝜓e
1⟩= ⟨𝜓e
2|V|𝜓e
2⟩is the van der Waals interaction for the
excited dimer, and V1,2 = ⟨𝜓e
1|V|𝜓e
2⟩= ⟨𝜓e
2|V|𝜓e
1⟩is the resonance transfer integral
determining the EET rate and dimer excitonic splitting. The sign and magnitude of
V1,2 are determined by the orientation of the transition dipole-moment vectors of the
two molecules. For long enough interpigment distances (>1 nm), the interpigment
coupling V1,2 can be sufficiently reliably calculated in dipole–dipole approxima-
tion [12]:
V1,2 = 𝜅𝜇1𝜇2
4𝜋𝜀R3 ,
(4.3)
where 𝜇1 and 𝜇2 are transition dipole moments of molecules 1 and 2, 𝜅is orientation
factor (−2…+2), and R is the distance between the molecules. Smaller distances
require more elaborate approximations.

KEY CONCEPTS IN PHYSICS OF PIGMENT–PROTEIN COMPLEXES
135
(a)
Chl 1
Chl 2
(b)
1
μ
2
μ
1
μ
2
μ
Dimer
ν
h L
ν
h L
ν
h U
ν
h U
~2 V1,2
E
Ee
De
Dg
E
Eg
+
=
FIGURE 4.4
Effects of exciton interaction on the energy levels in a degenerate dimer of
Chls. The splitting is equal to 2V1,2. Schematic spectra on the right are shown for two cases:
(a) “head-to-tail” dimer (top spectra) and (b) a dimer with two perpendicular orientations of
the dipole moments (lower spectra). h𝜈L and h𝜈U correspond to the energies of transitions to
the lower and upper excitonic states, respectively. The same coupling was assumed for both
configurations of the dipole moments. Adapted with permission from Reference 11. Copyright
(2011) American Chemical Society.
Skipping the detailed treatment of Dg and De, one can depict excitonic splitting
into upper (h𝜈U) and lower (h𝜈L) components in the case of the dimer shown in
Figure 4.4. Distribution of the oscillator strength and the magnitude of splitting
depend on the orientation of the transition dipole moments of the two pigments as
shown in Figure 4.4b. In the upper part of Frame B, the higher-energy state possesses
most of the oscillator strength. However, for different structural arrangements the
higher-energy state can be weak or even forbidden. In the lower case in Frame B, the
two bands have equal intensity. We stress that excitonic states are still delocalized
in this case. The extent of delocalization is reduced if the pigments involved have
different transition energies in the absence of interaction (“site energies,” determined
by the local environment) to begin with.
In general, in multipigment cases, new eigenstates of the system can be
described as
𝜙k =
N
∑
n=1
cnk𝜓n,
(4.4)
where 𝜓n are the locally excited states. Energies and oscillator strengths of the exci-
tonic states are found by determining eigenvalues and eigenvectors of a matrix which
has site energies as diagonal elements and interpigment couplings as off-diagonal
ones. The cnk in Eq. (4.4) can be found from the eigenvectors of that matrix. Deter-
mination of site energies is the major problem, despite several different approaches
developed for this purpose (see [21,22] for PS I, for example). In particular, interac-
tions such as hydrogen bonding, coordination state of Chls, and environment-induced

136
BIOPHOTONICS OF PHOTOSYNTHESIS
deviations of pigments from planarity are difficult to account for. Thus, site energies
are usually left as free parameters while attempting a simultaneous fit to as many
different optical spectra as possible; the search for realistic site energies is guided by
experimental constraints and aided by fitting algorithms [23,24]. If the couplings Vn,m
within certain groups of pigments are large, and the couplings to other pigments in
the complex are small (smaller than the differences in site energies and smaller than
the reorganization energy), such groups could be treated as independent domains,
and the optical spectra of the complex can be calculated as a sum of the excitonic
spectra of such groups.
4.3.2
Excitation Energy Transfer
In PCs, due to electrostatic coupling between the pigments, (radiationless) EET can
take place from one pigment to another. In general, EET occurs from a manifold of
vibrational states, associated with the excited electronic state of the donor molecule,
into a manifold of vibrational states associated with the ground state of the acceptor
molecule. Thus, the nature of pigment–pigment and pigment–protein interactions
is critical in understanding EET. If coupling between chromophores is weak (i.e.,
excitations are localized on single pigments), the energies of the chromophores are
considered to be independent even if there is a finite probability of EET. Such EET
can be described by the F¨orster theory [25]. When electronic coupling is weak enough
that inter- and intramolecular relaxation processes occur on a time scale faster than
EET, the rate of EET, k 1→2, can be described by Fermi’s golden rule rate expression:
k1→2 = 4𝜋2
h |V1,2|2𝜌(E),
(4.5)
where 𝜌(E) is related to the energy “matching” of the energy levels of the donor
and acceptor molecules (i.e., Franck–Condon (FC) weighted density of states), h is
Planck’s constant, and V1,2 is the coupling between pigments 1 and 2 as indicated in
Figure 4.4. Note all parameters related to the vibrational states are combined in
𝜌(E) =
∞
∫
E=0
dEGD(E)GA(E),
(4.6)
where GD(E) and GA(E) are Franck–Condon weighted and thermally averaged com-
bined densities of states. F¨orster showed [25] that GD(E) and GA(E) can be related
to absorption (molar extinction coefficient 𝜀A(𝜔)) and fluorescence (fD(𝜔)) spectra
of donor and acceptor pigments, which will be discussed in Section 4.3.3 and which
can be experimentally determined using spectral hole-burning (SHB), fluorescence
line narrowing (FLN), or ΔFLN (combination of SHB and FLN) [26–33].
For very short interpigment distances, both Coulomb and exchange interac-
tions need to be taken into account. The exchange mechanism was described by

KEY CONCEPTS IN PHYSICS OF PIGMENT–PROTEIN COMPLEXES
137
Dexter [34]. The mechanisms involving electrostatic and exchange interactions have
different distance dependence and different spin selection rules. Since the exchange
mechanism depends strongly on the overlap of the electronic wavefunctions, the
mechanism is restricted to rather short distances. In contrast, F¨orster mechanism
permits EET over several nanometers. On the other hand, the exchange mechanism
does not have the spin restriction and permits EET from a triplet state to a singlet
state. For instance, quenching chlorophyll triplets by carotenoids require an exchange
mechanism [2, 35, 36]. The more recently developed Redfield EET framework has
been discussed in References 17 and 18.
If couplings within certain group of pigments are large enough, this group should
be treated as one supermolecule, and optical spectra and transition dipoles of relevant
states of such system must be calculated as described in the previous section. Thus,
one can consider EET with one or both donor and acceptor being strongly coupled
groups of pigments. Excitation transfer between higher and lower states of the same
excitonically coupled system has to be also taken into account. Relaxation to the
lower state can occur by exchanging energy with the bath; the coupling with the bath
is represented by the spectral density of states, 𝜌(𝜔). Detailed description of EET pro-
cesses in strongly coupled systems is beyond the scope of this review. Qualitatively,
in the case when 𝜏EET ≪𝜏vib, where 𝜏EET = k−1
1→2(see Eq. 4.5) is the transfer time
and 𝜏vib corresponds to the vibrational relaxation time, the exciton can move freely
from molecule to molecule. Since this type of motion requires fixed-phase relations
between exciton wavefunctions of different molecules, it is called coherent transfer
[37, 38]. If 𝜏vib ≪𝜏EET, it is impossible to construct a meaningful wavefunction
which involves large contributions from different molecules. The excitation can hop
from pigment to pigment (i.e., there is no phase correlation between the excitation
before and after the hop; incoherent regime). Different regimes of excitation motion
can be described also by the density matrix approach. In summary, partially coherent
energy relaxation is expected within the strongly coupled domains, while incoherent
EET can occur between the low energy states of each domain.
4.3.3
Homogeneous and Inhomogeneous Broadening, Zero-Phonon Lines
(ZPLs), and Phonon Sidebands (PSBs)
The information on primary processes within PC can be obtained using various
time-domain methods including pump-probe, time-resolved fluorescence, and 2D-
ES. However, information about the dynamics of the system, for instance about the
EET or CS rates, is contained also in the widths of the so-called homogeneously
broadened lines, Γhom. The Γhom is related to the T2 relaxation time by
1
T2
= 𝜋cΓhom =
1
2T1
+ 1
T∗
2
,
(4.7)
where T1 is the excited-state lifetime and T∗
2 is the pure dephasing time that strongly
depends on temperature since dephasing is caused by phonon scattering. Thus, in
order to obtain information on the EET or CS processes affecting T1, one has to

138
BIOPHOTONICS OF PHOTOSYNTHESIS
1
2
Frequency - Wavelength
Γinh
Absorption
2
FIGURE 4.5
Pigments dispersed in an amorphous solid matrix and the SDF (of width Γinh)
describing the probability to find a molecule with given zero-phonon transition frequency;
see text. Adapted with permission from Reference 11. Copyright (2011) American Chemical
Society. (For a color version of this figure, see the color plate section.)
perform frequency-domain experiments at low temperatures. At liquid helium tem-
peratures, and in the presence of picosecond timescale or faster EET (typical for
PCs), the homogeneous linewidths are strongly dominated by T1, with T∗
2 being just
a minor contribution.
Proteins at low temperatures behave, to a significant extent, as amorphous solids.
It is well known that electronic transitions in amorphous phases are subject to inho-
mogeneous broadening due to variations of the local environment. This broadening
obscures the information on the dynamics of the individual complexes contained
in the homogeneously broadened lines. The origin of inhomogeneous broadening
is illustrated in Figure 4.5. Consider an ensemble of pigments in a rigid disordered
matrix, as shown schematically in the left frame; the immediate environments of the
identical pigment molecules (labeled as sites 1, 2, and 3) in the disordered matrix are
different, and as a result, the molecules absorb at somewhat different energies, 𝜔1,
𝜔2, and 𝜔3, respectively. This is illustrated by the three narrow spectra (colored solid
lines) shown in the right frame of Figure 4.5 (In a perfect crystal, all three transitions
would have the same frequency, that is, 𝜔1 = 𝜔2 = 𝜔3, and the resulting narrow
absorption band would be only homogeneously broadened.)
The narrow lines, as depicted in Figure 4.5, are known as zero-phonon lines
(ZPLs). A comprehensive theory of impurity spectra in solids, describing the ZPLs,
can be found, for example, in an excellent book by Rebane [39]. For zero-phonon
transition, there is no net change in the number of phonons. Building on each ZPL
(of width Γhom) is a broad phonon (delocalized lattice vibration) wing, so-called
phonon sideband (PSB). It is located at higher energies from ZPL in absorption
(see Figure 4.5), and at lower energies from ZPL in emission spectra. Since the
PSB contributes to the ensemble absorption and fluorescence origin bands, the width
of the respective ensemble spectra is approximately given by Γinh + S𝜔m, where
𝜔m is the mean phonon frequency (for PC typically about 20 cm−1) and S is a

KEY CONCEPTS IN PHYSICS OF PIGMENT–PROTEIN COMPLEXES
139
Absorption
Fluorescence
Fluorescence
E1
ZPL
PSB
Wavelength
Γhom
m
E0
qi
ω
FIGURE 4.6
Left: Potential curves in the case of linear electron–phonon coupling, ¯h𝜔g =
¯h𝜔e and single molecule fluorescence spectrum for a transition between two vibronic states of
a guest molecule embedded in an amorphous host. Optical transitions are shown as vertical
arrows in accordance with the Franck–Condon principle. The most likely transitions are those
with the largest wavefunction overlap; the zero-phonon transition is relatively unlikely in the
depicted strong electron–phonon coupling case. Right: Single molecule fluorescence spectrum.
The PSB is displaced to lower energies by 𝜔m, which corresponds to the mean phonon
frequency. Γhom is the homogeneous linewidth. Adapted with permission from Reference 11.
Copyright (2011) American Chemical Society.
dimensionless parameter called the Huang-Rhys factor (S). S in the low temperature
limit is defined as
S(T = 0) =
∑
i
Mi𝜔i
2ℏ(Δqi)2,
(4.8)
where Mi and 𝜔i are the reduced mass and frequency of the phonon mode i, respec-
tively. One can think of S as the average number of phonons excited in the course of
electronic transition.
Weak interaction between the electronic transition and the delocalized phonon
modes correspond to small equilibrium position changes Δqi and a correspondingly
small value of S. In the absorption/fluorescence spectrum, weak coupling (S ≪
1) is reflected in an intense ZPL and a weak PSB. Conversely, a large value of
S (S > 1) corresponds to strong coupling, producing a weak ZPL and an intense PSB.
For very strong coupling (S >∼10), the ZPL is so weak that it is masked by realistic
experimental noise. In general, the (linear) electron–phonon (el–ph) coupling is strong
and S is large for molecules which experience large geometry changes and/or large
changes of the electronic charge distribution upon electronic excitation. Figure 4.6
illustrates the above points. Here the configurational coordinate diagram of the ground
and excited states of a hypothetical molecule is shown for a single phonon mode.
The two parabolas represent the potential energy; the electronic ground state and the
first excited state are labeled as E0 and E1, respectively. Energy levels of a pigment
molecule are depicted in the interaction with the local phonon with ℏ𝜔g = ℏ𝜔e.

140
BIOPHOTONICS OF PHOTOSYNTHESIS
E
Excited state
Hopping
Tunneling
Ground state
q
d
Δ
V
FIGURE 4.7
Schematic representation of the two tiers of the protein energy landscape, as
well as the processes responsible for crossing the barriers. Both ground and excited states
of the pigment molecule embedded in the protein are shown. Adapted with permission from
Reference 43. Copyright (2012) American Chemical Society.
Within the linear el–ph coupling approximation, only the equilibrium position, but
not the frequency 𝜔i of the phonon mode i (with i = g, e), is changed between the two
electronic states. In this case, qi represents the lattice normal coordinate belonging to
phonon mode i, and Δqi corresponds to the change of the equilibrium position.
In the low temperature limit, an intuitive formula describing the whole shape of the
single-molecule spectrum, including both ZPL and PSB, but not intramolecular vibra-
tions, was first given in Reference 40 (see [11] for more sophisticated treatments):
LA∕F(𝜔−Ω0) = e−Sl0(𝜔−Ω0)
⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟
ZPL
+
∞
∑
R=1
SR e−S
R! lR(𝜔−Ω0 ∓R𝜔m)
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
PSB
,
(4.9)
where −R𝜔m and +R𝜔m correspond to absorption LA(𝜔) and fluorescence LF(𝜔),
respectively. The first term represents the Lorentzian ZPL l0(𝜔−Ω0), which peaks
at Ω0 and possesses a homogeneous width Γhom. The second term is the PSB,
consisting of a sum over all R-phonon transitions; the function l1(𝜔) is the one-phonon
profile (i.e., the normalized spectral density function discussed above). Guided by
experimental data, the one-phonon profile l1 is often approximated by a Gaussian at
its lower frequency wing and by a Lorentzian at its higher frequency wing. However,
one can also directly employ the PSB profiles measured by ΔFLN spectroscopy
[28–30]. Obviously, the profile described by Eq. (4.9) is directly related to GD(E) and
GA(E) discussed above (see Eq. 4.6). However, Eq. (4.9) is valid only for a relatively
narrow one-phonon profile when the mean-phonon approximation is reasonable (M.
Reppert and R. Jankowiak, unpublished data).
4.3.4
Proteins at Low Temperatures
As follows from the discussion above, in order to use homogeneous linewidths for
the determination of excited-state lifetimes (i.e., EET and CS times), one needs to

EXPERIMENTAL TECHNIQUES
141
minimize T∗
2 and work at as low temperatures as possible. Low temperature experi-
ments trap biomolecules in a physical state in which large-scale motions necessary
for crossing the high barriers are frozen out, while the motions within the lower hier-
archical tiers of the protein energy landscape potentially remain active. The idea of
hierarchically structured protein energy landscapes was introduced by Frauenfelder et
al. [41,42]. The whole protein energy landscape may be represented by the so-called
“folding funnel.”
From a somewhat different perspective, application of the energy landscape con-
cept to pigment–protein complexes could also be considered as an extension of
theories developed in condensed-matter physics to explain the anomalous low tem-
perature properties of glasses. According to those theories, glasses owe their unusual
low temperature properties, including heat capacity and heat conductivity, as well as
optical/spectroscopic properties [44–46], to the presence of the so-called two-level
systems (TLS). The exact atomic origins of the TLS are not known in most cases, but
in general they are groups of atoms which can assume two different configurations,
with (phonon-assisted) tunneling being possible between these configurations. The
so-called standard TLS model [44–46] implied lack of interaction between individual
TLS. However, the standard model failed to explain some experimental observations,
and TLS–TLS interactions or multilevel systems (MLS) had to be introduced, even for
glasses. While glasses exhibit just the broad barrier parameter distributions (the par-
ticular functional shape of which is a matter of debate, see Section 4.5.4), responsible
for broad distributions of conformational change rates, in proteins these distributions
additionally have hierarchal character [41, 42]. Figure 4.7 presents the adaptation
of the TLS model to proteins, and includes two hierarchal tiers of the energy land-
scape. The model includes both ground- and excited-electronic states of the pigment
molecule surrounded by the amorphous host and being in interaction with TLS (or
multi-well energy landscape), as the excited state processes are important in treatment
of optical spectroscopy experiments (see Section 4.4). Barriers on the higher tier of
the landscape, with pigment in the ground state, are high enough, and corresponding
relaxation times may vary from hours to days and beyond at liquid helium tempera-
tures. In terms of optical experiments, this corresponds to the (relatively infrequent)
shifts of the spectral lines (slow spectral diffusion). Barriers on a lower tier can be
crossed by thermally induced hopping or by tunneling at a much higher rate. If tiny
conformational changes on that lower barrier tier are occurring much faster than
the time scale of the experiment, this fast spectral diffusion would contribute to the
observed width of the optical spectral line (or hole, see below) [47,48].
4.4
EXPERIMENTAL TECHNIQUES
4.4.1
Spectral Hole-Burning, Fluorescence Line-Narrowing, and 𝚫FLN
Several low temperature, laser-based techniques have been developed that can over-
come the inhomogeneous broadening and access the homogeneous (natural) ZPL
width, which can provide dynamical information. These techniques are SHB [46–52],
fluorescence line-narrowing (FLN) [53], and, ultimately, single-molecule (or, in the
context of this work—single PC) spectroscopy (SPCS) [54–56].

142
BIOPHOTONICS OF PHOTOSYNTHESIS
SHB involves selective excitation of the pigment molecules by narrow-band laser
and phototransforming either those molecules themselves or their immediate envi-
ronment. As a result of either process, the molecule ceases to absorb at its original
wavelength. A particular variety of HB, known as nonphotochemical hole-burning
(NPHB) [33, 46, 50, 57, 58] is dominant in amorphous solids, including PCs. In
the case of NPHB, the pigment molecules in PC do not experience photochemical
transformations and remain parts of the coupled pigment network. The shift of the
pigment’s transition frequency occurs as a result of small conformational changes
in the protein surrounding the pigment molecule, a phenomenon that can be treated
as tunneling through the barrier on the multi-well protein energy landscape (see
above), at least at temperatures below 10 K (see Section 4.5.4). The “strength” of
the barrier can be described with the tunneling parameter 𝜆= d
√
2mV∕ℏ, where
d is the displacement along a generalized coordinate, m is the effective mass of the
entity rearranging during conformational change, and V is the barrier height. It appears
(otherwise persistent HB would be impossible) that in most amorphous solids, includ-
ing proteins, barriers in the excited state are lower than in the ground state and can be
crossed with sufficient probability within the lifetime of the excited state. The absorp-
tion spectrum, after burning at 𝜔B with photon flux P for time t, can be described
with SHB master equation [33, 58]:
D(Ω, t) = 1.5 ∫d𝜔L(Ω −𝜔)G(𝜔) ∫d𝜆f(𝜆)
× ∫d𝛼sin 𝛼cos2 𝛼e−P𝜎𝜑(𝜆,T1)L(𝜔B−𝜔)t cos2 𝛼.
(4.10)
Here 𝜎and 𝜙are the integral absorption cross-section of the pigment and the HB
quantum yield, respectively. The latter depends on the tunneling parameter 𝜆:
𝜑(𝜆, T1) =
Ω0 exp(−2𝜆)
Ω0 exp(−2𝜆) + T−1
1
.
(4.11)
f(𝜆) is the distribution of 𝜆and T1 may include EET or CS times, as well as radiative
lifetime, but not T∗
2. L(𝜔B −𝜔) is the single site-absorption profile (Eq. 4.9). Ω0 ∼1012
Hz can be interpreted as an attempt frequency. Alpha is the angle between the laser
polarization and the transition dipole moment. G(𝜔) is the site-distribution function
(SDF), which is Gaussian with the width Γinh before burning, and it describes the
probability of finding a pigment with a ZPL at a given frequency. Since convolution of
a Lorentzian with itself is a Lorentzian with twice the width, the width of the (shallow)
resonant spectral hole is twice the Γhom. Since in NPHB pigment molecules do
not experience photochemical changes, they continue contributing to the absorption
spectrum, just at different wavelengths. These pigments with altered environments
are redistributed around the spectral hole, giving rise to the broad anti-hole (or
“photoproduct”). The true NPHB spectrum is conservative.
FLN spectroscopy [53] involves measurement of fluorescence spectra selectively
excited with a narrow-band laser. The advantage of FLN over SHB is that no

EXPERIMENTAL TECHNIQUES
143
phototransformations are required, either in the pigment molecule or in its imme-
diate environment. On the downside, in FLN experiments it is difficult to disentangle
different contributions to the PSB part of the spectrum [26,27], and the intensity of
the ZPL part is difficult to determine due to scattering of excitation light. Recently,
a combination of FLN with SHB, the ΔFLN spectroscopy, has been applied to a
number of PCs [28–30], providing new insights into the el-ph coupling parameters.
ΔFLN directly yields the correct PSB shape due to double selection [30]. Whether
the ZPL contribution can also be measured with sufficient precision in the ΔFLN
experiments is not completely clear, and likely depends on the design of a particular
experimental setup. FLN and ΔFLN allow for the measurement of intramolecular
vibrational frequencies, providing a selective tool for the identification of Chls with
unique interactions with the surrounding protein.
Parameters provided by SHB yield information on (a) lifetimes (T1) of the zero-
point level of S1(Qy)-states due to EET or ET, as determined by the widths of
zero-phonon holes (ZPH); (b) Γinh values, typically ∼50−200 cm−1 derived from
the SHB action spectrum (see, e.g., [59]), the ZPH depth dependence on wavelength
for constant irradiation dose; (c) the extent of correlation and excitonic interaction
between the absorption bands of different Qy-states, using satellite hole-burning;
and (d) Franck–Condon factors and frequencies of Chl molecular vibration modes
active in electronic transitions as determined by vibronic satellite HB spectroscopy.
Electron–(protein) phonon coupling parameters (Huang-Rhys factors, S, and PSB
shape) can be also obtained. These vibration data can also be obtained by ΔFLN.
Parameters (a)–(d) are essential for understanding EET since they determine spectral
density 𝜌(see above). Other quantities are important for determining particular origins
of relevant excitonic states, or the parameters of local protein environments. In terms
of protein dynamics, SHB may be used to determine pure dephasing of Qy-optical
transitions due to coupling with MLS of proteins (smallest barrier tier of the protein
energy landscape)—by measuring the temperature dependence of the Γhom, as well as
the parameters of the higher tiers of the protein energy landscapes—from hole evolu-
tion upon burning and recovery, at fixed (burn) temperature and upon thermocycling
[51, 60, 61]. Additionally, the linear pressure shift rates for Qy-absorption bands and
ZPH, as well as their broadening rates, provide information on the nature of excitonic
coupling between neighboring Chl molecules; specifically, contributions from elec-
trostatic and electron-exchange interactions. The latter lead to charge-transfer (CT)
character for Qy-states. In the absence of excitonic interactions, the pressure-shift
rates reflect local compressibility of the protein. The large permanent dipole-moment
changes (Δ𝜇) of the S0→S1 transitions can be used to identify states with significant
CT character [31–33]. Aside from detecting CT states, the ability to resolve closely
spaced states is also enhanced by high pressure and Stark HB spectroscopy [31–33].
4.4.2
Single Photosynthetic Complex Spectroscopy
The key advantage of SPCS is that this approach, by definition, eliminates the prob-
lem of automatic subensemble averaging inherent in other techniques for overcoming
inhomogeneous broadening, such as SHB. The vast majority of SPCS experimen-
tal setups are based on confocal fluorescence microscopes, with the microscope

144
BIOPHOTONICS OF PHOTOSYNTHESIS
objective located with the sample inside the cryostat in the case of low temperature
experiments. The small amount of fairly diluted sample is spin-coated onto a proper
substrate, ensuring that only one complex at a time is present in the diffraction-limited
volume selected by the microscope objective. Then one can obtain its fluorescence
or fluorescence excitation spectra. Large amounts of SPCS data are available for a
number of PC (the following references represent just a fraction of available works),
including LH2 antenna complexes of purple bacteria at low [56, 62–72] and room
temperatures [73,74], and cyanobacterial PS I [75–83]. Some measurements on LHC
II antenna complex of PS II had been performed at cryogenic temperatures a while
ago [84]; recently a number of works have been published on room-temperature
conformational switching in this complex [85,86]. SPCS studies of conformational
switching were extended to peripheral antenna proteins of plant PS I [86]. Some
SPCS results were reported also for the LH1 complex of purple bacteria [87, 88].
SPCS experiments on minor antenna complexes of PS II—CP24, CP26, and CP29
have been mentioned in Reference 85 but not many details were presented, as this
work focused mainly on the LHC II and Lhca complexes. SPCS on a number of
other photosynthetic systems were reported in Reference 89. Figure 4.8 contains
a schematic of a typical single molecule/SPCS experiment, as well as raster-scan
results obtained in Reference 77 for Synechocystis PCC6803 PS I.
It is important to recognize with respect to comparisons between NPHB and SPCS,
that (a) light intensities by necessity employed in SPCS experiments are orders of
Cryostat
MO
MM
LP
DM
APD
FM
IM
SPEC
35 μm
35 μm
50
100
Fluorescence counts
per 0.2 s
150
CCD
EP
(b)
(a)
FIGURE 4.8
(a) Scheme of the confocal microscope used for individual complex spec-
troscopy. EP is excitation pinhole, DM is dichroic mirror, MM is motorized mirror, MO
is microscope objective, LP is long-pass filter, and FM is flipping mirror. APD itself and
the entrance slit of the imaging spectrograph (IM SPEC) effectively comprise the detection
pinholes. (b) Raster-scan image of the thin film containing single PS I complexes from Syne-
chocystis PCC 6803 (red peaks) [77]. Complexes were excited with 250 nW μm−2 (25 W cm−2)
at 680 nm and fluorescence was collected at > 700 nm. T = 10 K. Adapted with permission
from Reference 77. Copyright (2007) American Chemical Society. (For a color version of this
figure, see the color plate section.)

EXAMPLES
145
magnitude higher than in NPHB experiments; (b) if the system is capable of NPHB,
as is the case for various PCs, a majority of line shifts observed in SPCS in that
system will be light-induced (line shifts as NPHB on a single-molecule level) [90];
and (c) the average of the multiple SPCS experiments should be compatible with sub-
ensemble averages delivered by NPHB. More extensive discussion on these issues is
presented in Section 4.5.
4.5
EXAMPLES
4.5.1
LH2 Antenna Complex of Purple Bacteria
LH2 has been widely studied by SHB, SPCS, and time-domain methods. As men-
tioned in the introduction, LH2 is a cyclic structure possessing approximate Cn
symmetry, see Figure 4.3b. It contains two rings of Bchl a molecules, B800 and
B850. The absorption spectrum of the LH2 complex is shown in Figure 4.9. The
circular symmetry of the LH2 complex and small interpigment distances in the B850
ring result in strong excitonic effects. According to excitonic calculations and in
B800
B850
750
800
850
900
785
795
Wavelength (nm)
AE1
A
E3
E2
E1
x
*
805
40
30
Time (min)
20
10
0
FIGURE 4.9
Left: Absorption spectrum of LH2 complex of Rhodopseudomonas acidophila
and the scheme of excitonic levels from Reference 91. Asterisk indicates overlapping E4 level
of the lower manifold and E2 of the upper manifold; x indicates overlapping E3 of the lower
manifold and E4 of the upper manifold. The frame also contains a 1.5 K single-complex
emission (blue) and fluorescence excitation spectra of LH2 from Rb. acidophila. Adapted with
permission from Reference 71. Copyright (2012) American Chemical Society. Right: The
B800 region of the single-complex spectra, exhibiting both small and large spectral shifts due
to conformational changes involving different tiers of the protein energy landscape. Adapted
with permission from Reference 66. Copyright (2004) IOP Publishing. (For a color version of
this figure, see the color plate section.)

146
BIOPHOTONICS OF PHOTOSYNTHESIS
agreement with the experiment (see [91] and references therein), most of the oscilla-
tor strength of the B850 ring is concentrated in a doubly degenerate, second-lowest
(exciton) energy level. The scheme of B850 excitonic levels from Reference 91 is
presented below the absorption spectrum. In the ideal ring, the lowest energy level of
the B850 (referred to as B870) is forbidden by symmetry considerations; however,
due to diagonal and off-diagonal energy disorder, this level gains some oscillator
strength and can be observed in spectroscopic experiments. The shape of the SDF of
this state was uncovered using ZPH action spectroscopy [59, 92]. The B870 state is
also visible in the satellite hole structure resulting from the excitation into the B800
band and subsequent downhill EET. Narrow line in the blue single complex emission
spectrum shown in Figure 4.9 likely belongs to B870.
Another issue extensively explored for LH2 is energy transfer within the B800
ring and from B800 to B850 Chls. HB into the red edge of the B800 band, where
B800→B800 EET is not present, allows one to estimate the B800→B850 EET time.
The ∼2 ps B800→B850 EET time has been determined by SHB [91] and early SPCS
experiments [62]. Several times faster EET has been observed within the B800 band.
However, the SHB-based 2 ps 5 K EET time is somewhat longer than those observed
by time-domain techniques [93, 94]. One must remember that SHB is expected to
preferentially probe the longer lifetime side of the EET time distribution if such
a distribution is present (EET time as T1 in Eq. 4.11 for the HB yield). Due to
dispersion of donor–acceptor energy gaps in PCs in general, and in LH2 in particular,
there should be a distribution of the EET times. It has been suggested that the whole
B850 density of states, including the upper excitonic components (see Fig. 4.9),
defines the B800-B850 EET rates [95, 96]. Very recently we have confirmed [90],
using extensive HB modeling, that the evolution of the spectral holes burned into
the B800 band is indeed in agreement with B800→B850 EET time distributions
reported in References 95 and 96 (and not with the conventional F¨orster models).
However, it is not clear how these results could be reconciled with results of pressure-
tuning of spectral holes (and bands) [91], which suggest that B800→B850 EET time
does not depend on the energy gap between the B800 and B850 bands (which is
pressure-dependent), that is, it is not dependent on the positions of upper excitonic
components of B850 manifold. Thus, the exact mechanism of B800→B850 EET
remains elusive.
The LH2 is the pigment–protein complex which is by far the most thoroughly
studied by means of SPCS (References 56, 62–74, are just a fraction of the papers
devoted to SPCS in LH2). For instance, the direct observation of excitonic bands
due to the strongly coupled BChls of the B850 manifold has been reported [63] (see
black single-complex excitation spectrum in Figure 4.9; note the splitting of E1 level
and appreciable strength of the E2 level due to energy disorder). This data, along
with the analysis presented in Reference 64, indicates that the C8 (or C9) symmetry
of the LH2 complex may be disturbed, that is, that these complexes could exhibit
elliptical distortion. Whether this is the case in vivo, or it is an effect of a particular
sample preparation procedure, is still a matter of debate, as no elliptical distortion has
been observed for LH1 [88]. This issue has been revisited very recently [72], with
the conclusion that SPCS, based on fluorescence detection, favors LH2 complexes

EXAMPLES
147
with certain kinds of symmetry distortion. Polarization-sensitive measurements on
the B800 band, as well as correlated shifts of several lines in the B800 spectrum,
proved that even for relatively weak (V1,2 ∼25 cm−1) interactions between the B800
molecules, the respective electronic states can be significantly delocalized [66]. The
fluorescence spectra of the single LH2 (as well as LH1) [87] complexes have been
used to support invoking exciton self-trapping in B850 (B875 in LH1) [26, 27].
Single B800 molecule excitation spectra containing both ZPL and PSB were recently
reported [67].
Spectral diffusion and underlying low temperature protein dynamics are the key
subjects of the majority of recent LH2 SPCS publications [56, 65–70]. Spectral fluc-
tuations resulting from protein dynamics at room temperature have been considered
in References 73 and 74. It has been demonstrated that spectral diffusion behavior
of the B800 spectral lines is in agreement with the models [41, 42], predicting the
presence of several hierarchical tiers in the protein energy landscape [65, 66] (see
right part of Figure 4.9). The likely distances between pigment molecules and entities
experiencing conformational changes responsible for spectral shifts of various mag-
nitudes have also been suggested [56, 69]. It remains to be seen if the reported SPCS
data is statistically sufficient to serve as representative of the typical or average low
temperature behavior of the LH2 protein, or if some fractions of these observations
reflect the dynamics of the surrounding amorphous host or host–protein interface
rather than that of the “pure/intact” LH2 proteins. Our comparison of SPCS and
NPHB results (including evolution of ZPH width and NPHB anti-hole), indicates
that while NPHB data is in qualitative agreement with SPCS results and the under-
lying multi-tier energy landscape models, quantitatively SPCS and SHB data are in
disagreement [90]. The ∼4.5 cm−1 B800 hole (Γhom < 2.25 cm−1) is not broaden-
ing noticeably in several hours and definitely is not approaching the shape/width
of the first cumulant distributions based on SPCS data [69] (see Figure 4.10), as
would be expected if both SPCS and SHB observations reflected the same thermally
induced protein dynamics. In other words, the fastest, smallest-barrier tier dynamics
attributed in Reference 68 to the surface TLS [97] does not manifest in SHB data.
This indicates that this smallest-barrier tier dynamics in SPCS experiments might
either be sample-preparation- or amorphous host-dependent, or are characteristic for
complexes favored by SPCS [72]. The important principal questions are (1) Would the
line shifts observed in SPCS experiments occur anyway, whether they are observed
or not; or are they, in fact, light-induced (i.e., are the NPHB on a single-molecule
scale?) and (2) What is the mechanism responsible for the line shifts on a molecular
level? Interestingly, temperature dependence of the shift rates observed in Reference
70 suggested both tunneling and barrier hopping were present in comparable propor-
tions. On the other hand, our analysis of SHB results [90] indicated that for photon
budgets typically used in SPCS experiments, most shifts should be light-induced,
with the NPHB process obeying the model based on excited-state tunneling.
The above discussion illustrates that both SHB and SPCS provide valuable com-
plementary information on EET and protein dynamics in LH2. In particular, SHB
offers the average values serving as benchmarks in SPCS experiments, helping to
determine if observed phenomena are statistically relevant features of the protein

148
BIOPHOTONICS OF PHOTOSYNTHESIS
15
0.01
5 K
t = 45 min
t = 0
–0.01
–0.02
–0.03
–0.04
12350
12370
809
808
Wavelength (nm)
807
Frequency (cm–1)
Δ Absorbance
12390
0
20
15
10
5
0
–5
–10
–15
20
10
0
–10
–20
10
5
0
–5
–10
–15
10
5
0
–5
–10
–15
Occurrence
Occurrence
Occurrence
Occurrence
First cumulant (cm–1)
First cumulant (cm–1)
First cumulant (cm–1)
First cumulant (cm–1)
300
(a)
(b)
(c)
(d)
250
200
150
100
50
0
250
200
150
100
50
0
250
200
150
100
50
0
250
200
150
100
50
0
FIGURE 4.10
Left: The first cumulant distributions from [69]. Two tiers of the LH2 protein
energy landscape correspond to two-tiered shift distribution. Each spectral line experiences
frequent shifts about its equilibrium location, as well as infrequent shifts between different
equilibrium locations. Adapted with permission from Reference 69. Copyright (2008) Amer-
ican Physical Society. Right: Spectral hole burnt at the red edge of the B800 band (dotted
curve) does not exhibit broadening and its width (4.5 cm−1) does not become comparable to
the width of the first cumulant distribution (dashed line). Hole recovery is due to the white
light of the FTIR spectrometer; recovery in the dark is negligible. Solid curve represents the
same hole 45 minutes after burning. Adapted with permission from Reference 90. Copyright
(2010) American Chemical Society.
dynamics of intact protein per se. More work is needed to resolve the questions
raised above.
4.5.2
Cyanobacterial Photosystem I
As shown in Section 4.2, PS I is a quite complicated entity, with the core including
almost 100 Chl a molecules per monomer (i.e., per P700 primary donor, see Fig-
ure 4.1B). The Qy absorption of all these molecules is concentrated in a relatively
narrow spectral region, resulting in a high degree of spectral congestion (see left frame
of Fig. 4.11). Thus, most of the SHB and SPCS studies of PS I so far focused on the
so-called “red antenna states,” or states absorbing lower in energy than the P700. At
physiological temperatures, these low energy pigments are capable of the uphill EET
to the P700, and the organism benefits from extending photosystem absorption to a
broader wavelength range. While in cyanobacteria the respective pigments are part of
the core antenna, in plant PS I they belong to the Lhca peripheral antenna complexes.
(For recent review on structure and intersubunit EET in plant and cyanobacterial
PS I, see Reference 98.) Understanding which particular chlorophylls known from
structural data are responsible for the red states promises to offer insight into the func-
tioning of the PS I antenna, to provide benchmarks for excitonic calculations, and
to help determine which particular methods of theoretically calculating pigment-site
energies are the most correct. The 5 K absorption spectra of PS I from Thermosyne-
chococcus elongatus and Synechocystis PCC 6803 are presented in Figure 4.11a,

EXAMPLES
149
0.75
2000
1500
1000
500
0.5
0.25
0
640
660
680
700
720
700
710
720
730
740
Wavelength (nm)
Wavelength (nm)
Absorbance
Intensity (counts)
FIGURE 4.11
Left: 5 K absorption spectra of PS I from T. elongatus (solid line) and
Synechocystis PCC 6803 (dotted line), along with respective satellite hole structures obtained
with illumination at 660 nm (double arrow). The hole spectra were rescaled for clarity. Short
arrows indicate satellite holes belonging to red states. Right: Single-complex spectra of PS I of
T. elongatus [77]. Note shifts of the spectral lines likely belonging to the C710 state between
sequentially measured spectra (arrows). The structure on the red edge of the spectrum is due
to CCD etaloning. Adapted with permission from Reference 77. Copyright (2007) American
Chemical Society.
along with the satellite hole structures resulting from high energy excitation (e.g., at
650 nm) [31–33, 99]. It was the careful analysis of such satellite hole structures, and
their irradiation dose dependence, which first allowed the identification of two sepa-
rate red states for Synechocystis [31, 99] and three for T. elongatus [32]. Existence of
two (three) states has been confirmed by pressure- and Stark-SHB experiments [32,
99] as differences in pressure-induced shifts; dipole-moment change and electron-
phonon coupling (S) can be utilized to distinguish between partially overlapping
bands. It has been demonstrated that the lowest energy states of both cyanobacteria
(C714 and C719, respectively) exhibit large values of all three of these parameters
and therefore they most likely possess significant charge-transfer character.
The first PS I SPCS results were reported in Reference 75 for trimeric PS I from
T. elongatus. The single-complex fluorescence spectra were dominated by broad
structureless bands peaked at ∼730 nm and resembling the bulk-emission spectra.
At that time it was believed the latter emission originates from the lowest energy
C719 state. This observation of broad bands in SPCS is in agreement with SHB data,
demonstrating the C719 state possesses significant CT character and is characterized
by very large el-ph coupling. In addition to this broad band, some narrow lines have
been observed at around 710 nm, both in fluorescence and in fluorescence excitation
spectra. Figure 4.11b depicts several fluorescence spectra of the single trimeric PS
I from T. elongatus from Reference 77. For Synechocystis PCC 6803, the broad
band peaked at ∼722 nm has been observed in Reference 77 in the fluorescence
spectra of single PS I complexes, in agreement with the similarity between the lowest

150
BIOPHOTONICS OF PHOTOSYNTHESIS
energy states of T. elongatus (C719) and Synechocystis (C714) [30–32, 99]. Narrow
lines were observed in Synechocystis PS I in later works [78, 81]. These narrow
lines experience spectral shifts (see Figure 4.11) which are a manifestation of (likely
light-induced, see previous section) spectral diffusion/protein dynamics. At about the
same time, the data on PS I from T. elongatus with prereduced P700 were reported,
including an additional emission band at 745 nm [76]. It was later demonstrated that
the location of the lowest energy emission band is strongly dependent on the oxidation
state of the P700 both in single-complex and bulk samples, and in several species,
with oxidized P700 serving as a strong quencher of the lowest of the red states (see
[83] and references therein). Thus, it currently appears likely that in T. elongatus, the
745 nm emission originates from the C719 state, while ∼730 nm emission originates
from the C715 state, with the latter state also exhibiting strong el-ph coupling, and
likely being the main contributor to (shallow) ZPH observed so far by us [32] at
∼720 nm. It was recognized early on that red states most likely belong to groups of
strongly coupled Chls [21,22]. However, in PS I, such groups are numerous and one
cannot base such assignments on interpigment couplings alone. Assignment of the
red states to various chlorophyll dimers and trimers known from the structure data
is a topic of continuous lively discussions [32, 76, 79, 81, 83, 99], with suggestions
based on energy transfer, protein dynamics, electron–phonon coupling, effects of
monomerization, and other arguments. Currently there appears to be an agreement
that the origin of the lowest energy red band is the B7-A32 dimer [32, 83]. If B31-
B32-B33 trimer is the origin of either C715 or C710 state remains a matter of debate.
In view of the above discussion, the narrow lines belonging to the C715 state may
not be observed in SPCS experiments due to strong electron–phonon coupling rather
than fast spectral diffusion, just as for the C719 state.
Very recently, low temperature SPCS studies have revealed strong anticorrelation
between the fluorescence of different PSI emitters from Synechocystis PCC 6803 and
T. elongatus, suggesting there may be no unique EET pathways within a given com-
plex [82]. Rather, fluctuations of the protein environment may dynamically modify
the site energies, producing multiple alternating EET pathways even at low tem-
peratures. These results further support the idea that protein dynamics may play a
critical role in the very efficient EET in light-harvesting complexes, in agreement
with observations based on recent 2D-ES experiments [100].
In summary, more studies elucidating various properties of the red bands of PS I,
as well as their similarities and differences between different species, are necessary.
These may include, for example, studies of EET times and their distributions, spec-
tral hole-growth kinetics, NPHB antihole shape, and hole-recovery experiments (see
Section 4.5.4 for an example of application of these techniques to a simpler system,
CP43), the latter accessing the distribution of the barriers in the protein energy land-
scape in the ground, rather than an excited, electronic state. In view of the discussion
on LH2 above, one may also suggest the spectral diffusion behavior of different
lowest energy states of PS I is affected not only by the supposed looseness of the
respective protein pockets, but also by pigments’ closeness to the protein/amorphous
host interface and by the nature of that host. The final assignment should

EXAMPLES
151
simultaneously explain the low-temperature, light-induced spectral dynamics [78–83]
as well as other effects accessible by SPCS, SHB, and lower resolution techniques.
4.5.3
Plant LHC II—Single Complex Spectroscopy and
Nonphotochemical Quenching
LHC II, the peripheral antenna complex of PS II, is the most abundant PC, containing
about half of all Chl pigments in chloroplasts. LHC II naturally occurs in trimeric
form [9]. Each monomer binds eight Chl a and six Chl b molecules, as well as four
carotenoids. LHC II is responsible for both effective capturing of photons and EET
toward the reaction center, and for protection against excess light and reactive oxygen
species. In LHC II, these two tasks are accomplished by conformational switching
between light-harvesting and dissipating states [101, 102], while in minor light-
harvesting complexes of PS II (e.g., CP29), the carotenoid radical is likely formed
[35, 36]. Chls in each monomeric subunit form several clusters of strongly coupled
molecules, and intramonomer EET rates are fast. The rate of the intermonomeric EET
is still a matter of debate. In early low temperature SPCS experiments [84], trimeric
LHC II complexes usually exhibited three separate emission lines, while monomeric
LHC II exhibited single emission line. This indicates that intermonomer coupling is
weak. The same result was recently obtained by SHB [103]. On the other hand, in
Reference 85, it has been suggested that intermonomer energy transfer is present.
Recent LHC II SPCS works were devoted to conformational switching [85, 86].
While the majority of observed single LHC II complexes exhibited a single-emission
band at about 682 nm (at room temperature one is not expected to resolve three
separate bands for the trimer even in the absence of intermonomer EET), a ∼5%
fraction of the complexes demonstrated additional longer wavelength band. The
appearance, disappearance, or shifts of that additional band did not significantly
affect the 682 nm band, except for intensities of the two bands being anticorrelated.
Blue spectral shifts have also been observed occasionally. Figure 4.12 presents the
structure of the LHC II trimer, as well as examples of SPCS results from Reference 85.
The analysis based on the Redfield theory and the disordered exciton model showed
that shifts of fluorescence bands within ∼670–705 nm range could be explained by
changes in the realization of static disorder of the pigment site energies [17]. These
changes, if light-induced, would represent higher temperature analogs of NPHB
on a single-complex level. Spectral shifts beyond the above range represent the
emergence of special protein conformations, with resulting excited states possessing
strong CT character. Such a Chl–Chl CT state has been proposed as the key player in
one of the models describing non-photochemical quenching, the high light protection
mechanism (Other models are reviewed in Reference 104). However, in Reference 104
it has been proposed, based on Stark spectroscopy results, that two different emitting
states are present in the dissipative state of LHC II. One of these was attributed
to the CT state involving Chls 610-611-612, while the other supposedly is a Chl-
carotenoid mixed-excitonic/CT state (similar to that in minor PS II antenna complexes
[35,36]).

152
BIOPHOTONICS OF PHOTOSYNTHESIS
120
0
40
80
120
0
30
60
100
I
I
I
I
I/III
II
II
II
80
60
40
20
10
20
30
40
50
60
0
30
60
10
20
30
40
50
60
650
700
750
Wavelength (nm)
Illumination time (s)
FL (cps)
650
700
750
Wavelength (nm)
650
700
750
650
700
750
650
700
750
650
700
750
Wavelength (nm)
Wavelength (nm)
(a)
(b)
(c)
Wavelength (nm)
Wavelength (nm)
FIGURE 4.12
Left: The structure of the LHC II trimer based on Reference 9. Highlighted
(groups of) pigments were considered as possible origin of the lowest energy band of LHC II.
Adapted with permission from Reference 103. Copyright (2011) American Chemical Society.
Right: Examples of single RT LHC II fluorescence spectra and their evolution. Adapted with
permission from Reference 85. Copyright (2010) Elsevier. (For a color version of this figure,
see the color plate section.)
4.5.4
CP43—The Protein Energy Landscape Parameters
Analysis of the results of both SHB and SPCS experiments often requires disentan-
gling the effects due to spectral diffusion/protein dynamics from others, for example,
those related to EET or CS rates (and distributions thereof, see Sections 4.5.1 and
4.5.5). Although LH2 and PS I proved to be ideal systems for low temperature SPCS
research, due to the large spectral separation between absorption and emission, in
some sense these complexes are too rich in various interesting physics. It is beneficial
to explore protein dynamics/spectral diffusion in a simpler system, in order to gain
better understanding of the protein energy landscapes alone. CP43 core antenna com-
plex of PS II offers such an opportunity. CP43 contains just 13 Chl molecules [8]. It
was extensively explored by means of optical spectroscopy and, for instance, the SDF
of the lowest energy states and the electron–phonon coupling parameters are well
known [103, 105–108]. It was also agreed that the lowest energy (“A”) state of CP43
is likely localized on a single Chl molecule. The absorption spectrum, hole-growth
kinetics (HGK), and hole-recovery (HR) data for holes burnt at 686 nm (no downhill
EET), into A-band, are presented in Figure 4.13. The left frame contains the HGK
data obtained for 5 K. The insert represents the absorption spectrum with the SDF of
the two lowest energy states of CP43 [103, 105–108].
SHB yield 𝜙(see Eq. 4.11) in CP43 does not depend on temperature up to ∼12 K
[43]. The HGK curves such as depicted in Figure 4.13, but for temperatures up to
12 K, result from the increase of Γhom (T) [105] alone, without change of 𝜙, which
proves that tunneling in the excited state of the pigment–protein system is indeed the
origin of NPHB (or single-molecule line shifts, if SPCS experiments were performed

EXAMPLES
153
0.0
0.5
1.0
0.25
0.5
0.75
1
0.00001
660
670 680
690
0
10
–2.104
–3.104
–4.104
20 30 40
Wavelength (nm)
Frequency (GHz)
Irradiation dose (J cm–2)
Fractional hole depth
Relative hole depth
Absorbance
Δ Fluorescence (cps)
0.001
0.1
1
100
10000
Recovery time (s)
FIGURE 4.13
Left: The 5 K HGK curve and its fit, yielding the parameters of the excited-
state barrier distribution; 𝜆B = 686 nm. The insert contains the absorption spectrum of CP43
with the SDF of the two lowest energy states, A (red) and B (blue) [105]. Right: Hole-
recovery data for 20%-deep hole. Smooth solid curve (good fit to experimental data) results
from modeling in Gaussian 𝜆-distribution framework (see left frame of Fig. 4.14), while dashed
curve results from modeling in uniform 𝜆-distribution framework (see right frame of Fig. 4.14).
The insert represents a sample hole spectrum. Adapted with permission from Reference 43,
103. Copyright (2011, 2012) American Chemical Society. (For a color version of this figure,
see the color plate section.)
on CP43). The phonon-assisted tunneling rate is expected to be weakly temperature-
dependent for realistic TLS asymmetry Δ (see Figure 4.7) values [109]. The latter can
be estimated based on the shape of the NPHB anti-hole. From this, as well as from
the hole thermocycling data, the upper limit of the md2 parameter, characterizing the
tunneling entities, was determined (md2 = 1⋅10−46 kg⋅m2). This indicates that the
conformational changes triggered by excitation of the pigment most likely involve
the lighter ones of the protein side-groups moving less than 1 ˚A. Proton tunneling
also cannot be excluded at the moment.
Unfortunately, HGK data alone does not allow distinguishing between different
𝜆or barrier distribution shapes, and this problem is addressed in HR experiments.
The issue of the barrier distribution shape is an important one—widespread theories
of low temperature amorphous solids predict and hole-recovery experiments indeed
seem to indicate a ∼1∕
√
V ground-state barrier distribution in glasses [47,48, 51, 60,
61] (this would correspond to uniform 𝜆-distribution), while Gaussian 𝜆-distribution
has routinely been used to model the hole-burning process [33, 57, 58, 109]. A
superposition of ∼1∕
√
V and Gaussian terms has been deduced from hole recovery
for a protein (phycobilisomes) in Reference [60]. Recently we noticed, however, that
holes which are far from saturation are contributed to by sub-distributions of 𝜆(or V),
that are quantitatively, and sometimes qualitatively, different from true full 𝜆(or V)
distributions. Figure 4.14 depicts the partial 𝜆distributions actually contributing to
the holes of different depths burnt at 686 nm in CP43. Modeling shows that the shape
of the sub-distributions encoded in the (non-saturated) holes is drastically different in

154
BIOPHOTONICS OF PHOTOSYNTHESIS
a
a
b
b
5
7.5
10
12.5
8
Excited state λ
(a)
(b)
Probability (a.u.)
9
10
11
FIGURE 4.14
(a) Calculated excited-state partial 𝜆-distributions for Gaussian true full 𝜆-
distributions [43] (black: 20%-deep hole, a; blue: 55%-deep hole, b). Areas under curves
are normalized to same value. (b) Same for uniform 𝜆-distribution. Dashed lines represent
respective full true 𝜆-distributions. Adapted with permission from Reference 43. Copyright
(2012) American Chemical Society. (For a color version of this figure, see the color plate
section.)
two cases, and Gaussian distributions lead to HR predictions more consistent with the
experiment. In Figure 4.13, right frame, solid line providing better fit to experimental
data originates from the Gaussian distribution model, while dashed line providing
poorer fit originates from the uniform 𝜆-distribution (∼1∕
√
V) model. An important
feature of this analysis is that it is actually not protein-specific, and would apply to
any amorphous system, including glasses. The only condition we imposed on our
modeling was that the ground-state partial barrier (or 𝜆-) distributions have the same
qualitative shape as the excited state ones, depicted in Figure 4.14. Thus, it might well
be that fitting the HR data with ∼1∕
√
V barrier distributions is not sufficiently justified
in glasses as well, and the past success of respective fittings might be coincidental.
4.5.5
Probing Electron Transfer (ET) Times and Their Distributions
in Photosynthetic RCs by SHB
In the final example of this chapter, we demonstrate how SHB can be employed to
study the ET processes in photosynthetic RCs. A particular variety of SHB, called
transient or triplet-bottleneck HB, is used to access fast, picosecond-timescale ET
rates. Generation of transient HB spectra requires the presence of a third, relatively
long-lived state. That is, the singlet excited state evolves into a triplet state or another
long-lived (μs to ms range) product (e.g., a CS-state). In photosynthetic RCs, the
formation of a triplet state is the result of a recombination of the original charge-
separated state formed after the first couple of steps of the primary CS process. The
pigment’s ground state is depopulated for the lifetime of the long-lived state and
the respective spectral hole can be observed, but only for the duration of this lifetime.
The transient holes discussed below are acquired as the difference between the
absorption spectra measured with the excitation on and off (after saturation of a
persistent hole component, if any), that is, they are a result of a dynamic equilibrium.

EXAMPLES
155
A quite clear picture of CS processes and factors affecting rates of the primary CS
steps has been achieved for BRC [110]. The primary electron donor in BRC is the
strongly coupled Bchl a dimer (see Fig. 4.2b), and the primary CS times are of the
order of ps. No significant distribution of CS times was observed; fast CS combined
with extremely strong electron–phonon coupling [110] lead to negligible persistent
HB in BRCs. However, the primary CS processes in the RC of PS II are not understood
sufficiently. The coupling between PD1 and PD2 Chls in PS II RC (Fig. 4.2a) is weaker
than the special pair coupling in BRC; in fact, couplings between most pigments in
PS II RC are comparable. That led to multimer models for PS II RC [111], and to
the proposal that the exact nature of the primary electron donor may vary from RC to
RC due to particular realizations of disorder [112–115] in a given RC. Additionally,
it is not clear if the structure and pigment–pigment and pigment–protein interactions
in RC are preserved during the isolation, that is, if the isolated RC samples studied
by different groups correctly represent the RCs inside the intact PS II core (Fig. 4.2a)
[114–116].
An example of resonant transient HB spectra (0.5 cm−1 resolution) obtained for
PS II RC of Chlamydomonas reinhardtii (from [114]) is shown in Figure 4.15a. These
transient holes are obtained with 𝜆B = 682.0 (green curve a), 684.0 (red curve b),
686.0 (brown curve c), and 688.0 nm (blue spectrum d). Curves (a) and (b) possess
prominent ZPH. Curve (c) reveals an extremely weak ZPH, as indicated by the brown
arrow. The ZPH widths, which varied from 2.4 to 7.6 cm−1 at 682 nm, depending
on illumination dose, are believed to reflect a distribution of CS time (𝜏CS). The
660
670
680
684
a
682 nm
684 nm
688 nm
686 nm
b
c
d
(a)
(b)
2.8
ps
690
0
0
0.5
1
1.5
0.1
0.2
0.3
0.4
Wavelength (nm)
Fractional depth
Δ Absorbance
Hole width (cm–1)
FIGURE 4.15
(a) Resonant transient HB spectra for Chlamydomonas reinhardtii; spectra
a, b, c, and d were obtained with 𝜆B of 682.0, 684.0, 686.0, and 688.0 nm, respectively. Adapted
with permission from Reference 114. Copyright (2012) American Chemical Society. The inset
corresponds to the Lorentzian fit (black curve) of ZPH of curve b. (b) Persistent hole width
versus depth data for 𝜆B = 686 nm (circles) and 680 nm (triangles) obtained for isolated PS II
RC from spinach, and modeling results with the CS time distribution (dashed curve), without
any distribution (dotted curve), and with best distribution (solid curve, see text for details).
Adapted with permission from Reference 108. Copyright (2011) American Chemical Society.
(For a color version of this figure, see the color plate section.)

156
BIOPHOTONICS OF PHOTOSYNTHESIS
ZPH widths shown in Figure 4.15a correspond to 𝜏CS in the range of 1.4–4.4 ps, in
agreement with previous data obtained for primary CS in spinach RCs ([116, 117]
and references therein). The presence of a distribution of 𝜏CS is consistent with data
obtained by 2D ES for spinach RCs at 77 K [118], and with earlier photon echo and
modeling data [117]. It has been suggested, based on time-domain data, that two CS
pathways are active in PSII RCs, namely PD1 and/or ChlD1 CS path, according to
the nature of the primary electron donor [112,113]. It was suggested the PD1 path is
dominant for longer wavelengths and is characterized by sub-picosecond CS times,
while the ChlD1 CS path corresponds to longer CS times. The depicted HB results are
in agreement with this picture. Note the ZPHs are nearly absent at 𝜆B = 686 nm and
entirely absent at 688.0 nm and longer wavelengths, suggesting that excited states
of cofactors are strongly coupled with the CT state(s) lying in the long-wavelength
region [113]. This assignment is in agreement with [119] where it was shown that
excitation at wavelengths as long as 695.0 nm (T = 1.7 K) can induce QA
−formation
in PS II cores.
It was suggested long ago that the distribution of CS rates in PS II RC might be
very broad, with the longest CS times reaching nanosecond range [117]. The 2D-ES
[118] did not explore the latter range of CS times, as their analysis included only a
single 2-ns exponent in this range, which could as well be assigned to fluorescence
lifetime. In Reference 112 it was argued that for a fraction of RCs, the low temperature
CS times are slow for both PD1 and ChlD1 CS paths. The slow CS is most effectively
probed by persistent HB. In Reference 116, we suggested that persistent and transient
HB probe slow and fast ends, respectively, of the CS time distribution. The transient
holes in Reference 116 were in reasonable agreement with the fast end of the CS
time distribution of Reference 117. Recently we explored the slow end of the CS time
distribution for PS II RC from spinach [108]. Figure 4.15b depicts the dependences
of persistent hole width on (fractional) hole depth for 𝜆B = 686 nm (circles) and 𝜆B =
680 nm (triangles), and results of modeling for no CS time distribution (dotted line),
CS time distribution from Reference 117 (dashed line); and a distribution containing
∼40% of slow (hundreds of ps) CS times and 60% of fast (sub-distribution centered
on 1.5 ps) CS times, solid line, with the latter distribution obviously providing bet-
ter fit to the experimental data. Interestingly, the SHB results are incompatible with
significant contribution of the intermediate, tens of ps, times to the distribution [118].
4.6
CONCLUSIONS
Due to space limitation, this chapter did not touch many interesting issues/phenomena
related to biophotonics of photosynthesis. However, our main objective was to make
the reader aware and appreciative of the potential of SHB and SPCS, and the intri-
cacies of protein structure(s) and complexity of EET and CS processes ultimately
responsible for sustaining life on Earth. We anticipate application of information-rich
techniques briefly discussed in the chapter, that is, SPCS, SHB, FLN, and ΔFLN, will
continue to unravel the rich physics underlying the excitonic structure and various
dynamic processes in these important biological systems. Resonant (persistent) holes

REFERENCES
157
burned into the lowest energy state of various RCs and antenna pigment complexes
(and their modeling), as well as nonresonant hole-burning, should continue to provide
information on spectral diffusion and interactions of pigments within the protein envi-
ronment. That is, SHB and SPCS can offer complementary information on protein
motion in conformational phase space and on the distributions of the barriers on pro-
tein energy landscapes. We have also illustrated how transient HB spectra can provide
CS rates in photosynthetic RCs. Advanced descriptions of various optical spectra,
more accurately reflecting the influences of el-ph coupling, lifetime broadening, and
coupling to vibrational modes, as well as experimentally determined spectral density
profiles, are desired to further advance application of frequency-domain techniques
in the area of photosynthesis research.
ACKNOWLEDGMENTS
Authors would like to thank our long-time collaborators, as well as graduate students
and postdoctoral fellows. V.Z. acknowledges support from NSERC and CFI. RJ
acknowledges support from the Division of Chemical Sciences, Geosciences, and
Biosciences, Office of Basic Energy Sciences of the U.S. Department of Energy;
grant DE-FG02-11ER16281.
REFERENCES
[1] H. T. Witt, A. M¨uller, and B. Rumberg, “Electron-transport system in photosynthesis of
green plants analysed by sensitive flash photometry,” Nature 197, 987–991 (1963).
[2] R. E. Blankenship, Molecular Mechanisms of Photosynthesis (Blackwell Science Ltd,
London, 2008).
[3] Y. Umena, K. Kawakami, J.-R. Shen, and N. Kamiya, “Crystal structure of oxygen-
evolving photosystem II at a resolution of 1.9 ˚A,” Nature 473 55–60 (2011).
[4] A. Amunts, H. Toporik, A. Borovikova, and N. Nelson, “Structure determination and
improved model of plant photosystem I,” J. Biol. Chem. 285, 3478–3486 (2010).
[5] A. Camara-Artigas, D. Brune, and J. P. Allen, “Interactions between lipids and bacterial
reaction centers determined by protein crystallography,” Proc. Natl. Acad. Sci. USA 99,
11055–11060 (2002).
[6] V. Cherezov, J. Clogston, M. Z. Papiz, and M. Caffrey, “Room to move: crystallizing
membrane proteins in swollen lipidic mesophases,” J. Mol. Biol. 357, 1605–1618 (2006).
[7] P. Jordan, P. Fromme, H. T. Witt, O. Klukas, W. Saenger, and N. Krauβ, “Three-
dimensional structure of cyanobacterial photosystem I at 2.5 ˚A resolution,” Nature 411,
909–917 (2001).
[8] B. Loll, J. Kern, W. Saenger, A. Zouni, and J. Biesiadka, “Towards complete cofactor
arrangement in the 3.0 ˚A resolution structure of photosystem II,” Nature 438, 1040–1044
(2005).
[9] J. Standfuss, A. C. Terwisscha van Scheltinga, M. Lamborghini, and W. Kuehlbrandt,
“Mechanisms of photoprotection and nonphotochemical quenching in pea light-
harvesting complex at 2.5 ˚A resolution,” EMBO J. 24, 919–928 (2005).

158
BIOPHOTONICS OF PHOTOSYNTHESIS
[10] A. W. Roszak, T. D. Howard, J. Southall, A. T. Gardiner, C. J. Law, N. W. Isaacs, and
R. J Cogdell, “Crystal structure of the RC-LH1 core complex from Rhodopseudomonas
palustris,” Science 302, 1969–1972 (2003).
[11] R. Jankowiak, M. Reppert, V. Zazubovich, J. Pieper, and T. Reinot, “Site selective and
single complex laser-based spectroscopies: a window on excited state electronic struc-
ture, excitation energy transfer, and electron-phonon coupling of selected photosynthetic
complexes,” Chem. Rev. 111, 4546–4598 (2011).
[12] H. van Amerongen, L. Valkunas, and R. van Grondelle, Photosynthetic Excitons (World
Scientific, Singapore, 2000).
[13] D. Abramavicius and S. Mukamel, “Energy-transfer and charge-separation pathways
in the reaction center of photosystem II revealed by coherent two-dimensional optical
spectroscopy,” J. Chem. Phys. 113, 184501 (2010).
[14] M. Cho, “Coherent two-dimensional optical spectroscopy,” Chem. Rev. 108, 1331–1418
(2008).
[15] T. Brixner, J. Stenger, H. M. Vaswani, M. Cho, R. E. Blankenship, and G. R. Fleming,
“Two-dimensional spectroscopy of electronic couplings in photosynthesis,” Nature 434,
625–628 (2005).
[16] J. Barber, “Photosystem II: the engine of life,” Q. Rev. Biophys. 36, 71–89 (2003).
[17] V. I. Novoderezhkin and R. van Grondelle, “Physical origins and models of energy
transfer in photosynthetic light-harvesting,” Phys. Chem. Chem. Phys. 12, 7352–7365
(2010).
[18] R. van Grondelle and V. I. Novoderezhkin, “Energy transfer in photosynthesis: exper-
imental insights and quantitative models,” Phys. Chem. Chem. Phys. 8, 793–807
(2006).
[19] T. Renger and E. Schlodder, “Primary photophysical processes in photosystem II: bridg-
ing the gap between crystal structure and optical spectra,” Chem. Phys. Chem. 11,
1141–1153 (2010).
[20] A. S. Davydov, Theory of Molecular Excitons (Plenum, New York, 1971).
[21] A. Damjanovic, H. M. Vaswani, P. Fromme and G. R. Fleming, “Chlorophyll excitations
in photosystem I of Synechococcus elongatus,” J. Phys. Chem. B 106, 10251–10262
(2002).
[22] M. Byrdin, P. Jordan, N. Krauβ, P. Fromme, D. Stehlik and E. Schlodder, “Light
harvesting in photosystem I: modeling based on the 2.5- ˚A structure of photosystem I
from synechococcus elongatus,” Biophys. J. 83, 433–457 (2002).
[23] M. Reppert, K. Acharya, B. Neupane, and R. Jankowiak, “On the unusual temperature-
dependent emission of the CP47 antenna protein complex of photosystem II,” J. Phys.
Chem. Lett. 1, 2310–2315 (2010).
[24] G. Raszewski and T. Renger, “Light harvesting in photosystem II core complexes is
limited by the transfer to the trap: can the core complex turn into a photoprotective
mode?”, J. Am. Chem. Soc. 130, 4431 (2008).
[25] T F¨orster, “Intermolecular energy migration and fluorescence,” Ann. Phys. 2, 55–75
(1984).
[26] A. Freiberg, M. R¨atsep, K. Timpmann, G. Trinkunas and N. W. Woodbury, “Self-trapped
excitons in LH2 antenna complexes between 5 K and ambient temperature,” J. Phys.
Chem. B 107, 11510–11519 (2003).

REFERENCES
159
[27] K. Timpmann, M. R¨atsep, C. N. Hunter and A. Freiberg, “Emitting excitonic polaron
states in core LH1 and peripheral LH2 bacterial light-harvesting complexes,” J. Phys.
Chem. B 108, 10581–10588 (2004).
[28] J. Pieper, M. R¨atsep, K.-D. Irrgang and A. Freiberg, “Chromophore-chromophore and
chromophore-protein interactions in monomeric light-harvesting complex II of green
plants studied by spectral hole burning and fluorescence line narrowing,” J. Phys. Chem.
B 113, 10870–10880 (2009).
[29] M. R¨atsep, J. Pieper, K.-D. Irrgang and A. Freiberg, “Excitation wavelength-dependent
electron-phonon and electron-vibrational coupling in the CP29 antenna complex of
green plants,” J. Phys. Chem. B 112, 110–118 (2008).
[30] M. Reppert, V. Naibo and R. Jankowiak, “Accurate modeling of fluorescence line nar-
rowing difference spectra: direct measurement of the single-site fluorescence spectrum,”
J. Chem. Phys. 133, 014506 (2010).
[31] J. M. Hayes, S. Matsuzaki, M. R¨atsep and G. J. Small, “Red chlorophyll a antenna states
of photosystem I of the cyanobacterium Synechocystis sp. PCC 6803,” J. Phys. Chem.
B 104, 5625–5633 (2000).
[32] V. Zazubovich, S. Matsuzaki, T. W. Johnson, J. M. Hayes, P. R. Chitnis and G. J. Small,
“Red antenna states of photosystem I from cyanobacterium Synechococcus elongatus:
a spectral hole burning study,” Chem. Phys. 275, 47–59 (2002).
[33] T. Reinot, V. Zazubovich, J. M. Hayes, and G. J Small, “New insights on persis-
tent nonphotochemical hole burning and its application to photosynthetic complexes,”
J. Phys. Chem. B 105, 5083–5098 (2001).
[34] D. A. Dexter, “Theory of sensitized luminescence in solids,” J. Chem. Phys. 21, 836–851
(1953).
[35] N. E. Holt, D. Zigmantas, L. Valkunas, X. P. Li, K. K. Niyogi and G. R. Fleming,
“Carotenoid cation formation and the regulation of photosynthetic light harvesting,”
Science 307, 433–436 (2005).
[36] T. K. Ahn, T. J. Avenson, M. Ballottari, Y. C. Cheng, K. K. Niyogi, R. Bassi, and G. R.
Fleming, “Architecture of a charge-transfer state regulating light harvesting in a plant
antenna protein,” Science 320, 794–797 (2008).
[37] G. S. Engel, T. R. Calhoun, E. L. Read, T.-K. Ahn, T. Manˇcal, Y.-C. Cheng, R. E.
Blankenship and G. R. Fleming, “Evidence for wavelike energy transfer through quan-
tum coherence in photosynthetic systems,” Nature 446, 782–786 (2007).
[38] G. D. Scholes, “Quantum-coherent electronic energy transfer: did nature think of it first?
J. Phys. Chem. Lett. 1, 2–8 (2010).
[39] K. K. Rebane, Impurity Spectra of Solids (Plenum, New York, 1970).
[40] J. M. Hayes, J. K. Gillie, D. Tang, and G. J. Small, “Theory for spectral hole burning of
the primary electron donor state of photosynthetic reaction centers,” Biochim. Biophys.
Acta 932, 287–305 (1988).
[41] H. Frauenfelder, S. G. Sligar, and P. G. Wolynes, “The energy landscapes and motions
of proteins,” Science 254, 1598–1603 (1991).
[42] P. W. Fenimore, H. Frauenfelder, B. H. McMahon, and R. D. Young, “Proteins are
paradigms of stochastic complexity,” Physica A 351, 1–13 (2005).
[43] M. Najafi, N. Herascu, M. Seibert, R. Picorel, R. Jankowiak, and V. Zazubovich,
“Spectral hole burning, recovery, and thermocycling in chlorophyll-protein complexes:

160
BIOPHOTONICS OF PHOTOSYNTHESIS
distributions of barriers on the protein energy landscape,” J. Phys. Chem. B 116, 11780–
11790 (2012).
[44] P. W. Anderson, B. I. Halperin, and C. M. Varma, “Anomalous low-temperature thermal
properties of glasses and spin glasses,” Phil. Mag. 25, 1–9 (1972).
[45] W. A. Phillips, “Tunneling states in amorphous solids,” J. Low. Temp. Phys. 7, 351–360
(1972).
[46] R. Jankowiak and G. J Small, “Hole-burning spectroscopy and relaxation dynamics of
amorphous solids at low temperatures,” Science 237, 618–625 (1987).
[47] J. Zollfrank, J. Friedrich, J. M. Vanderkooi, and J. Fidy, “Conformational relaxation
of a low-temperature protein as probed by photochemical hole burning: horseradish
peroxidase,” Biophys. J. 59, 305–312 (1991).
[48] K. Fritsch, J. Friedrich, F. Parak, and J. L. Skinner, “Spectral diffusion and the energy
landscape of a protein,” Proc. Natl. Acad. Sci. USA 93, 15141–15145 (1996).
[49] W. E. Moerner, Topics in Current Physics, Persistent Spectral Hole Burning: Science
and Applications (Springer, New York, 1987), Vol. 44.
[50] R. Jankowiak, J. M. Hayes, and G. J Small, “Spectral hole-burning spectroscopy in
amorphous molecular solids and proteins,” Chem. Rev. 93, 1471–1502 (1993).
[51] Y. Berlin, A. Burin, J. Friedrich, and J. K¨ohler, “Spectroscopy of proteins at low tem-
perature. Part I: experiments with molecular ensembles,” Phys. Life Rev. 3, 262–292
(2006).
[52] R. Purchase and S. V¨olker, “Spectral hole burning: examples from photosynthesis,”
Photosynth. Res. 101, 245–266 (2009).
[53] R. I. Personov, E. I. Al’shitz, and L. A. Bykovskaya, “The effect of fine structure appear-
ance in laser-excited fluorescence spectra of organic compounds in solid solutions,” Opt.
Commun. 6, 169–173, (1972).
[54] W. E. Moerner and D. P. Fromm, “Methods of single-molecule fluorescence spec-
troscopy and microscopy,” Rev. Sci. Inst. 74, 3597–3619 (2003).
[55] F. Kulzer and M. Orrit, “Single-molecule optics,” Ann. Rev. Phys. Chem. 55, 585–611
(2004).
[56] Y. Berlin, A. Burin, J. Friedrich, and J. K¨ohler, “Low temperature spectroscopy of
proteins. Part II: experiments with single protein complexes,” Phys. Life Rev. 4, 64–89
(2007).
[57] L. Shu and G. J. Small, “Mechanism of nonphotochemical hole burning: cresyl violet
in polyvinyl alcohol films,” J. Opt. Soc. Am. B 9, 724–731 (1992).
[58] T. Reinot, N. C. Dang, and G. J. Small, “A model for persistent hole burned spectra
and hole growth kinetics that includes photoproduct absorption: application to free base
phthalocyanine in hyperquenched glassy ortho-dichlorobenzene at 5 K,” J. Chem. Phys.
119, 10404–10414 (2003).
[59] H.-M. Wu, M. R¨atsep, I.-J. Lee, R. J. Cogdell, and G. J. Small, “Exciton level structure
and energy disorder of the B850 ring of the LH2 antenna complex,” J. Phys. Chem. B
101, 7654–7663 (1997).
[60] W. K¨ohler, W. Friedrich, and H. Scheer, “Conformational barriers in low-temperature
proteins and glasses,” Phys. Rev. A 37, 660–662 (1988).
[61] W. K¨ohler, J. Zollfrank, and J. Friedrich, “Thermal Irreversibility of optically labeled
low-temperature glasses,” Phys. Rev. B. 39, 5414–5424 (1989).

REFERENCES
161
[62] A. M. van Oijen, M. Ketelaars, J. K¨ohler, T. J. Aartsma, and J. Schmidt, “Spectroscopy
of individual light-harvesting 2 complexes of Rhodopseudomonas acidophila: diagonal
disorder, intercomplex heterogeneity, spectral diffusion, and energy transfer in the B800
band,” Biophys. J. 78, 1570–1577 (2000).
[63] M. Ketelaars, A. M. van Oijen, M. Matsushita, J. K¨ohler, J. Schmidt, and T. J.
Aartsma, “Spectroscopy on the B850 band of individual light-harvesting 2 complexes of
Rhodopseudomonas acidophila. I. experiments and Monte Carlo simulations,” Biophys.
J. 80, 1591–1603 (2001).
[64] M. Matsushita, M. Ketelaars, A. M. van Oijen, J. K¨ohler, T. J. Aartsma, and J.
Schmidt, “Spectroscopy on the B850 band of individual light-harvesting 2 complexes
of Rhodopseudomonas acidophila. II. Exciton states of an elliptically deformed ring
aggregate.,” Biophys. J. 80, 1604–1614 (2001).
[65] C. Hofmann, T. J. Aartsma, H. Michel, and J. K¨ohler, “Direct observation of tiers in the
energy landscape of a chromoprotein: A single-molecule study,” Proc. Natl. Acad. Sci.
USA 100, 15534–15538 (2003).
[66] C. Hofmann, T. J. Aartsma, H. Michel, and J. K¨ohler, “Spectral dynamics in the B800
band of LH2 from Rhodospirillum molischianum: a single-molecule study,” New J.
Phys. 6, 8–15 (2004).
[67] C. Hofmann, H. Michel, M. van Heel, and J. K¨ohler, “Multivariate analysis of
single-molecule spectra: surpassing spectral diffusion,” Phys. Rev. Lett. 94, 195501
(2005).
[68] J. Baier, M. F. Richter, R. J. Cogdell, S. Oellerich, and J. K¨ohler, “Do proteins at
low temperature behave as glasses? A single-molecule study,” J. Phys. Chem. B 111,
1135–1138 (2007).
[69] J. Baier, M. F. Richter, R. J. Cogdell, S. Oellerich, and J. K¨ohler, “Determination of the
spectral diffusion kernel of a protein by single-molecule spectroscopy,” Phys. Rev. Lett.
100, 018108 (2008).
[70] H. Oikawa, S. Fujiyoshi, T. Dewa, M. Nango, and M. Matsushita, “How deep is the
potential well confining a protein in a specific conformation? A single-molecule study
on temperature dependence of conformational change between 5 and 18 K,” J. Am.
Chem. Soc. 130, 4580–4581 (2008).
[71] R. Kunz, K. Timpmann, J. Southall, R. J. Cogdell, A. Freiberg, and J. K¨ohler, “Exciton
self trapping in photosynthetic pigment−protein complexes studied by single-molecule
spectroscopy, J. Phys. Chem. B 116, 11017−11023 (2012).
[72] S. Jang, R. J. Silbey, R. Kunz, C. Hofmann, and J. K¨ohler, “Is there elliptic distortion in
the light harvesting complex 2 of purple bacteria?” J. Phys. Chem. B 115, 12947–12953
(2011).
[73] V. I. Novoderezhkin, D. Rutkauskas, and R. van Grondelle, “Dynamics of the emission
spectrum of a single LH2 complex: interplay of slow and fast nuclear motions,” Biophys.
J. 90, 2890–2902 (2006).
[74] J. Janusonis, L. Valkunas, D. Rutkauskas, and R. van Grondelle, “Spectral dynamics of
individual bacterial light-harvesting complexes: alternative disorder model,” Biophys.
J. 94, 1348–1358 (2008).
[75] F. Jelezko, C. Tietz, U. Gerken, J. Wrachtrup, and R. Bittl, “Single-molecule spec-
troscopy on photosystem I pigment−protein complexes, J. Phys. Chem. B 104, 8093–
8096 (2000).

162
BIOPHOTONICS OF PHOTOSYNTHESIS
[76] A. F. Elli, F. Jelezko, C. Tietz, H. Studier, M. Brecht, R. Bittl, and J. Wrachtrup,
“Red pool chlorophylls of photosystem I of the cyanobacterium Thermosynechococcus
elongatus: a single-molecule study,” Biochemistry 45, 1454–1458 (2006).
[77] K. J. Riley, T. Reinot, R. Jankowiak, P. Fromme, and V. Zazubovich, “Red antenna
states of photosystem I from cyanobacteria Synechocystis PCC 6803 and Thermosyne-
chococcus elongatus: single-complex spectroscopy and spectral hole burning study,”
J. Phys. Chem. B 111, 286–292 (2007).
[78] M. Brecht, H. Studier, A. F. Elli, F. Jelezko, and R. Bittl, “Assignment of red antenna
states in photosystem I from Thermosynechoccocus elongatus by single-molecule spec-
troscopy,” Biochemistry 46, 799–806 (2007).
[79] M. Brecht, J. B. Nieder, H. Studier, E. Schlodder, and R. Bittl, “Red antenna states
of photosystem I from Synechococcus sp. PCC 7002,” Photosynth. Res. 95, 155–162
(2008).
[80] M. Brecht, H. Studier, V. Radics, J. B. Nieder, and R. Bittl, “Spectral diffusion induced
by proton dynamics in pigment-protein complexes,” J. Am. Chem. Soc. 130, 17487–
17493 (2008).
[81] M. Brecht, V. Radics, J. B. Nieder, H. Studier, and R. Bittl, “Red antenna states of
photosystem I from Synechocystis PCC 6803,” Biochemistry 47, 5536–5543 (2008).
[82] M. Brecht, V. Radics, J. B. Nieder, and R. Bittl, “Protein dynamics-induced variation
of excitation energy transfer pathways,” Proc. Natl. Acad. Sci. USA 106, 11857–11861
(2009).
[83] E. Schlodder, M. Hussels, M. Cetin, N. V. Karapetyan, and M. Brecht, “Fluorescence of
the various red antenna states in photosystem I complexes from cyanobacteria is affected
differently by the redox state of P700,” Biochim. Biophys. Acta 1807, 1423–1431 (2011).
[84] C. Tietz, F. Jelezko, U. Gerken, S. Schuler, A. Schubert, H. Rogl, and J. Wrachtrup,
“Single molecule spectroscopy of the light-harvesting complex II of higher plants,”
Biophys. J. 81, 556–562 (2001).
[85] T. P. J. Kr¨uger, V. I. Novoderezhkin, C. Ilioaia, and R. van Grondelle, “Fluorescence
spectral dynamics of single LHCII trimers,” Biophys. J. 98, 3093–3101 (2010).
[86] T. P. J. Kr¨uger, E. Wientjes, R. Croce, and R. van Grondelle, “Conformational switching
explains the intrinsic multifunctionality of plant light-harvesting complexes,” Proc. Natl.
Acad. Sci. USA 108, 13516–13521 (2011).
[87] M. Ketelaars, C. Hofmann, J. K¨ohler, T. D. Howard, R. J. Cogdell, J. Schmidt, and T.
J. Aartsma, “Spectroscopy on individual light-harvesting 1 complexes of Rhodopseu-
domonas acidophila,” Biophys. J. 83, 1701–1715 (2002).
[88] U. Gerken, F. Jelezko, B. Go1tze, M. Bransch¨adel, C. Tietz, R. Ghosh, and J. Wrachtrup,
“Membrane environment reduces the accessible conformational space available to an
integral membrane protein,” J. Phys. Chem. B 107, 338–343 (2003).
[89] Y. Saga and H. Tamiaki, “Fluorescence spectroscopy of single photosynthetic light-
harvesting supramolecular systems,” Cell Biochem. Biophys. 40, 149–165 (2004).
[90] D. Grozdanov, N. Herascu, T. Reinot, R. Jankowiak, and V. Zazubovich, “Low-
temperature protein dynamics of the B800 molecules in the LH2 light-harvesting com-
plex: spectral hole burning study and comparison with single photosynthetic complex
spectroscopy,” J. Phys. Chem. B 114, 3426–3438 (2010).
[91] V. Zazubovich, R. Jankowiak, and G. J. Small, “A high-pressure spectral hole burn-
ing study of correlation between energy disorder and excitonic couplings in the LH2

REFERENCES
163
Complex from Rhodopseudomonas acidophila” J. Phys. Chem. B 106, 6802–6814
(2002).
[92] H.-M. Wu, N. R. S. Reddy, and G. J. Small, “Direct observation and hole burning of
the lowest exciton level (B870) of the LH2 antenna complex of Rhodopseudomonas
acidophila (Strain 10050),” J. Phys. Chem. B 101, 651–656 (1997).
[93] H.-M. Wu, S. Savikhin, N. R. S. Reddy, R. Jankowiak, R. J. Cogdell, W. S. Struve,
and G. J. Small, “Femtosecond and hole-burning studies of B800’s excitation energy
relaxation dynamics in the LH2 antenna complex of Rhodopseudomonas acidophila
(Strain 10050),” J. Phys. Chem. 100, 12022–12033 (1996).
[94] T. Pullerits, S. Hess, J. L. Herek, and V. Sundstr¨om, “Temperature dependence of
excitation transfer in LH2 of Rhodobacter sphaeroides,” J. Phys. Chem. B 101, 10560–
10567 (1997).
[95] G. D. Scholes and G. R. Fleming, “On the mechanism of light harvesting in photosyn-
thetic purple bacteria: B800 to B850 energy transfer,” J. Phys. Chem. B 104, 1854–1868
(2000).
[96] S. Jang, M. D. Newton, and R. J. Silbey, “Multichromophoric f¨orster resonance energy
transfer,” Phys. Rev. Lett. 92, 218301 (2004).
[97] A. Heuer and P. Neu, “Tunneling dynamics of side chains and defects in proteins,
polymer glasses, and OH-doped network glasses,” J. Chem. Phys. 107, 8686–8696
(1997).
[98] M. K. Sener, C. Jolley, A. Ben-Shem, P. Fromme, N. Nelson, R. Croce, and K. Schulten,
“Comparison of the light-harvesting networks of plant and cyanobacterial I photosys-
tem,” Biophys. J. 89, 1630–1642 (2005).
[99] M. R¨atsep, T. W. Johnson, P. R. Chitnis, and G. J. Small, “The red-absorbing chlorophyll
a antenna states of photosystem I: a hole-burning study of Synechocystis sp. PCC 6803
and its mutants,” J. Phys. Chem. B 104, 836–847 (2000).
[100] H. Lee, Y.-C. Cheng, and G. R. Fleming, “Coherence dynamics in photosynthesis:
protein protection of excitonic coherence,” Science 316, 1462–1465 (2007).
[101] A. A. Pascal, Z. Liu, K. Broess, B. van Oort, H. van Amerongen, C. Wang, P. Horton,
B. Robert, W. Chang, and A. V. Ruban, “Molecular basis of photoprotection and control
of photosynthetic light-harvesting,” Nature 436, 134–137 (2005).
[102] A. V. Ruban, R. Berera, C. Ilioaia, I. H. M. van Stokkum, J. T. M. Kennis, A. A.
Pascal, H. van Amerongen, B. Robert, P. Horton, and R. van Grondelle, “Identification
of a mechanism of photoprotective energy dissipation in higher plants,” Nature 450,
575–578 (2007).
[103] N. Herascu, M. Najafi, A. Amunts, J. Pieper, K.-D. Irrgang, R. Picorel, M. Seibert, and
V. Zazubovich, “Parameters of the protein energy landscapes of several light-harvesting
complexes probed via spectral hole growth kinetics measurements,” J. Phys. Chem. B
115, 2737–2747 (2011).
[104] M. Wahadoszamen, R. Berera, A. M. Ara, E. Romero, and R. van Grondelle, “Identifi-
cation of two emitting sites in the dissipative state of the major light harvesting antenna,”
Phys. Chem. Chem. Phys. 14, 759–766 (2012).
[105] R. Jankowiak, V. Zazubovich, M. R¨atsep, S. Matsuzaki, M. Alfonso, R. Picorel, M.
Seibert, and G. J. Small, “The CP43 core antenna complex of photosystem II possesses
two quasi-degenerate and weakly coupled Qy-trap states,” J. Phys. Chem. B 104, 11805–
11815 (2000).

164
BIOPHOTONICS OF PHOTOSYNTHESIS
[106] N. C. Dang, V. Zazubovich, M. Reppert, B. Neupane, R. Picorel, M. Seibert, and
R. Jankowiak, “The CP43 proximal antenna complex of higher plant photosystem II
revisited: modeling and hole burning study. I,” J. Phys. Chem. B 112, 9921–9933 (2008).
[107] J.L. Hughes, R. Picorel, M. Seibert, and E. Krausz, “Photophysical behavior and assign-
ment of the low-energy chlorophyll states in the CP43 proximal antenna protein of
higher plant photosystem II,” Biochemistry 45, 12345–12357 (2006).
[108] N. Herascu, S. Ahmouda, R. Picorel, M. Seibert, R. Jankowiak, and V. Zazubovich,
“Effects of the distributions of energy or charge transfer rates on spectral hole burning
in pigment-protein complexes at low temperatures,” J. Phys. Chem. B 115, 15098–15109
(2011).
[109] N. C. Dang, T. Reinot, M. Reppert, and R. Jankowiak, “Temperature dependence of hole
growth kinetics in aluminum−phthalocyanine−tetrasulfonate in hyperquenched glassy
water,” J. Phys. Chem. B 111, 1582–1589 (2007).
[110] H. Wang, S. Lin, J. P. Allen, J. C. Williams, S. Blankert, C. Laser, and N. W. Woodbury,
“Protein dynamics control the kinetics of initial electron transfer in photosynthesis,”
Science 316, 747–750 (2008).
[111] J. R. Durrant, D. R. Klug, S. L. S. Kwa, R. van Grondelle, G. Porter, and J. P. Dekker,
“A multimer model for P680, the primary electron donor of photosystem II,” Proc. Natl.
Acad. Sci. USA 92, 4798–4802 (1995).
[112] V. I. Novoderezhkin, E. Romero, J. P. Dekker, and R. van Grondelle, “Multiple charge-
separation pathways in photosystem II: modeling of transient absorption kinetics,”
Chem. Phys. Chem. 12, 681–688 (2011).
[113] V. I. Novoderezhkin, J. P. Dekker, and R. van Grondelle, “Mixing of exciton and
charge-transfer states in photosystem II reaction centers: modeling of stark spectra with
modified redfield theory,” Biophys. J. 93, 1293–1311 (2007).
[114] R. Jankowiak, “Probing electron-transfer times in photosynthetic reaction centers by
hole-burning spectroscopy,” J. Phys. Chem. Lett. 3, 1684–1694 (2012).
[115] K. Acharya, V. Zazubovich, M. Reppert, and R. Jankowiak, “Primary electron donor(s)
in isolated reaction center of photosystem II from Chlamydomonas reinhardtii,” J. Phys.
Chem. B 116, 4860−4870 (2012).
[116] K. Riley, R. Jankowiak, M. R¨atsep, G. J. Small, and V. Zazubovich, “Evidence for highly
dispersive primary charge separation kinetics and gross heterogeneity in the isolated PS
II reaction center of green plants,” J. Phys. Chem. B 108, 10346–10356 (2004).
[117] V. I. Prokhorenko and A. R. Holzwarth, “Primary processes and structure of the photo-
system II reaction center: a photon echo study,” J. Phys. Chem. B 104, 11563–11578
(2000).
[118] J. A. Myers, K. L. M. Lewis, F. D. Fuller, P. F. Tekavec, C. E. Yocum, and J. P.
Ogilvie, “Two-dimensional electronic spectroscopy of the D1-D2-yt b559 photosystem
II reaction center complex,” J. Phys. Chem. Lett. 1, 2774–2780 (2010).
[119] E. Krausz, J. L. Hughes, P. Smith, R. Pace, and S. Peterson ˚Arsk¨old, “Oxygen-evolving
photosystem II core complexes: a new paradigm based on the spectral identification of
the charge-separating state, the primary acceptor and assignment of low-temperature
fluorescence,” Photochem. Photobiol. Sci. 4, 744–753 (2005).

5
OPTICAL SECTIONING MICROSCOPY
AND BIOLOGICAL IMAGING
John Girkin
Biophysical Sciences Institute, Department of Physics, Durham University, Durham, UK
5.1
INTRODUCTION AND BACKGROUND
Optical imaging has played a key role in the development of biology and indeed until
the discovery of DNA, and its role in life, was probably the core scientific technology
forming the basis of all discoveries. Initially this imaging was just undertaken with
the scientist using their eyes to observe and subsequently record what was going on.
However, the invention of the lens over 2000 years ago meant that biological samples
could be studied in finer detail, indeed Pliny the Elder described their use in surgery
both for observing fine tissue damage and to use the sun to cauterize wounds [1].
The subsequent invention of the microscope around 400 years ago brought about a
revolution in biological thinking as cells were visualized and appreciated for the first
time and these results were first recorded by Robert Hooke [2]. The development of
the optical microscope, to visualize ever finer structure, has also prompted advances
in the understanding of optics such as the work on diffraction by Abbe and around
70 years ago by Zernike [3,4], thus the growth of optical microscopy and advanced
physics has been synergistic. These significant improvements in optical imaging led
to a real desire in biology to visualize, with as finer detail as possible, in real time,
the motion of cells, and their subcellular structure, and in the past 30 years this has
become possible, initially with the advent of practical confocal microscopy, and more
recently with a range of nonlinear-based methods.
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
165

166
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
This chapter will explore the physical processes that lie behind the modern methods
of obtaining optically sectioned images with an emphasis on the practical details that
should be taken into consideration when selecting a specific method. Clearly in that
decision it is important always to put the biological context first. Understanding the
basic physics behind the different methods is important in helping to choose the best
tool for a specific task. Frequent compromises have to be made trading off image
quality, with speed, resolution, and minimal damage to the sample and all aspects
should be considered before following a certain route. Understanding how an imaging
method works is clearly crucial to obtaining and interpreting the most accurate data
to reflect what is taking place within the sample. This chapter does not go into a
rigorous mathematical derivation of optically sectioned imaging but core references
are provided, where applicable, for the interested reader.
5.1.1
The Biological Context and Limitations
Before considering the physics and optics behind optical sectioning methods, it is
important to place these into the correct biological context, as this frequently places
limitations on the instrumentation. Ideally, the life scientist would like to observe all
processes taking place inside as intact a system as possible, and clearly this would be
in an intact, living organism. Although this may be possible in single-cell creatures
(within the limits imposed for diffraction, etc.), for more complex organisms this
clearly become much harder. Prior to the invention of optical sectioning microscopes,
studying dynamic processes within biology was undertaken by fixing a range of
physically sectioned samples taken at different points from different samples and
trying to develop a model for the processes taking place. An analogy is that of trying
to work out the rules of a sport, say football, or soccer, by going to one match and
digging a hole in the pitch after one minute (one is perturbing the system radically),
pointing a camera in a specific direction and taking a single image. One then goes to
a second match, digs another hole, in what you believe is the same place on the pitch,
after 2 mins and recording a second image and so on for a full 90 separate minutes at
90 different matches. One then takes all the single images and attempts to determine
the rules. Clearly a much better route would be to watch a single match, from above
the pitch, and monitor what is taking place. There is a risk that the ball, or perhaps
players, might hit the camera but that is clearly less invasive than watching a number
of matches in which you have dug holes.
With modern optically sectioning systems, one is thus trying to have a method that
enables one to observe the biology with minimal perturbation, in real time. Clearly
even using an optical sectioning technique one is perturbing the system, as the internal
cells of biological systems are not used to being subject to light, but this is clearly
less significant than physically sectioning the sample!
Crucial to any imaging method is the contrast mechanism that is being utilized
to obtain the microscopic image. In most biological imaging at present this is based
around fluorescence and the details of this process are covered in chapters elsewhere
in this book though from an imaging perspective there are two important points.
First, the labeled sample will be excited at one wavelength and then will emit light,

INTRODUCTION AND BACKGROUND
167
a short time later, at a different wavelength. In conventional single-photon imaging,
the light emitted is at a longer wavelength than the excitation light. The delay in
the fluorescence is known as the fluorescence lifetime and can be used to enhance
the detection and also to provide information on the exact conditions present near the
fluorophore. A full detailed explanation of all aspects of fluorescence is well beyond
the scope of this chapter, and indeed this volume, and the interested reader may try
the currently accepted gold standard book by Lakowicz [5].
Biologically the development of fluorescent labels that go to a specific feature of
a cell have been crucial in helping to understand all of the processes of life. This
technique was revolutionized in the mid-1990s with the development of fluorescent
proteins (initially green fluorescent protein or GFP) in which the cell produces its
own fluorophores when specific genes are active [6,7]. Again it must be remembered
that this is not “noninvasive” as the cell does not normally produce this compound
and these large molecules (238 amino acids) are consuming chemicals within the cell
thus slightly altering its natural biochemistry. However, the fluorescence (protein) is
produced only when a targeted gene is active and can thus be very specific. GFP,
in line with many other fluorophores, is also not perfectly stable when subjected to
repeated excitation and emission cycles, and when the compound eventually breaks
down, the resulting chemicals can be toxic to the cells in which they are present in a
process known as phototoxicity.
Other contrast mechanisms presently used do not, generally, require the presence
of a fluorescent compound, either indigenous or introduced. These include harmonic
generation (second and third are now routinely used) along with Raman-based imag-
ing methods, most specifically Coherent Anti-Stokes Raman (CARS) and full details
of these methods are presented later in this chapter. The crucial feature with all of
the approaches to develop optically sectioned data is that within an image the most
important parameter that sets the imaging limitation is the method of contrast. With-
out contrast no features can be seen and the remaining parts of this chapter can be
viewed as methods of just improving the contrast in a specific volume or optical slice
of tissue.
5.1.2
Definitions and Terms
In order to be self-consistent the next section explains the terms used throughout the
rest of the chapter.
Numerical Aperture (NA). This is the light-collecting ability of any lens. It is
defined as sine of the half angle of the light cone entering the lens. The higher
the numerical aperture of a lens the greater its light-collecting ability and the finer
detail that can be seen in the image. In all optical microscopy systems, this sets the
ultimate resolution. By definition NA = nSin(𝜃), where 𝜃is the half angle of light
cone produced by the lens and n is the refractive index [8].
Refractive Index (n). The ratio of the speed of light in a material to its speed
in a vacuum is expressed by the refractive index, such that the higher the refractive
index, the slower the light travels in the material. As light passes from one refractive
index to another, unless it impacts the new surface at 90◦, its path will be changed.

168
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
The size of this change is determined by the relative values of the refractive indices
through which the light is traveling [8]. By manipulating the refractive index (e.g.,
using oil and water objectives) it is possible to develop microscope objective lenses
with a numerical aperture greater than 1 leading to higher resolution as noted below.
For reference n ∼1 in air, ∼1.33 in water, and ∼1.5 in oil and glass.
Lateral Resolution. In a perfect imaging system, the resolution in the imaging
plane (XY plane in this chapter) is set by the longest wavelength of light involved
in the imaging process and the numerical aperture of the lens. A definition of the
resolution of a system is the minimum separation between two points at which these
two points can each be seen or resolved. In effect what takes place is that each point
produces an Airy disc and as the points become closer together the finite peaks at the
center of the discs start to overlap until you can no longer distinguish between the
two points. Using the Rayleigh criteria [8], the lateral resolution is given as
d = (0.61 × 𝜆)
NA
,
(5.1)
where 𝜆is the wavelength of the light used for the imaging and NA the numerical
aperture of the lens.
Axial Resolution. In a system producing optical sectioning, the ultimate axial
resolution (along the optical axis of Z direction in this chapter) is also given by
the wavelength and numerical aperture though the resolution is now nonlinearly
dependant on the numerical aperture
daxial = (1.78 × 𝜆)n
(NA)2
.
(5.2)
5.2
CONFOCAL IMAGING
After a brief introduction to confocal imaging, the basic physical principles will be
explained and the most practical implementation of the confocal microscope will be
described. The more advanced optical sectioning methods all build upon this core
concept and optical configuration.
5.2.1
Introduction to Confocal Imaging
The most basic form of optical sectioned imaging is that of a confocal microscope. The
concept was first developed by Marvin Minsky in 1957 and subsequently accepted
as a granted patent in 1961 [9]. The basic principles of a confocal imaging system
have changed very little since this original design, and in 1957, the concept was
very much an instrument ahead of its time. High spectral brightness light sources
were not available (the laser being the obvious source invented in 1960), and solid-
state detectors and electronics were also in their infancy. The main concept that was
introduced, and the basis of all confocal imaging systems, is that of a pinhole that is
“confocal” with the focal plane of the objective as shown in Figure 5.1.

CONFOCAL IMAGING
169
FIGURE 5.1
Basic optical configuration of a confocal imaging system. (For a color version
of this figure, see the color plate section.)
In this simplified diagram, light enters from the left-hand side (in blue) and passes
through the beam splitter (in a typical biological confocal system this is a dichroic
beam splitter that transmits one wavelength and reflects another) and is subsequently
focused by the objective lens. Fluorescent light (or backscattered light in many
configurations) from the focal plane (shown as purple) then passes back through
the objective and is subsequently reflected by the beam splitter and focused down
through a pinhole onto the detector. Light from outside the objective’s focal plane
(shown in red dotted line) passes back through the objective and is also reflected
by the beam splitter but, as this light is not from the focus, on passing through
the lens it is rejected by the pinhole and the majority of this light thus fails to
reach the detector. The system thus only permits light from the focal plane to reach
the detector. Different depths can then be imaged by moving the focus through
the sample.
Minsky’s original system used long-life oscilloscope screens to display the image,
but this was not practical and the first more biological friendly system was built
by Egger and Petr˘an [10] who used it to image ganglion cells and brain tissue in
reflection mode. Several systems were subsequently built on this model and the
theoretical aspects were then fully explained by Sheppard and Wilson [11]. All of
these early systems suffered from one major drawback, namely the sample had to be
scanned through the focused spot in both the X and Y axes to produce a single image
plane. The paper by Sheppard and Wilson explained ways around this limitation
and the first practical beam scanned system was built in 1987 [12,13] using a laser
illumination source and photomultipliers as detectors. The optical configuration of
most confocal microscopes has not changed significantly from this date.
5.2.2
Point Scanned Microscopy
The most commonly used method of scanned confocal microscopy is one in which the
excitation spot is raster scanned across the sample using a pair of scanning mirrors.

170
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
FIGURE 5.2
Beam scanning confocal microscope.
Each point in the plane is thus illuminated in sequence. Figure 5.2 illustrates the basic
principles of the technique.
Light from the excitation source (almost universally a laser as a high brightness,
good quality light beam is required) is directed onto the sample using a dichroic beam
splitter (though a partially reflecting optic can be used for reflection imaging). The
light then passes onto a pair of scanning galvanometer mirrors before being directed
onto the back aperture of the microscope objective. Two configurations of scanners
are possible, close-coupled (where the orthogonal scanners are mounted as close
together as possible) and reimaged (where the scanners are reimaged onto each other
using lenses or concave mirrors). The returned light is then reimaged by the objective
onto the scanning mirrors in which the beam is descanned to pass through the dichroic
mirror and the pinhole to reach the detector. Without the descanning on the returned
path the beam would only pass through the pinhole in one position of the scan and the
descanning thus means that only a single pinhole is required rather than one for each
position of the scan. In many systems, a multiple series of dichroic mirrors is used
with several detectors enabling the fluorescence for a number of fluorophores to be
collected simultaneously (in figure 5.2 one would collect blue light, the other green).
In the above figure, the optics will work both for fluorescence and reflection imaging
and the latter is an underused method in complete biological samples as the reflection
image can help place the fluorescence image in context. This is even more valid when
an infrared light source is used with its greater penetration. The returned signal for
each point on the scan is stored in a computer and the image thus built up pixel by
pixel. After a scan has been obtained for one plane the objective can be moved toward
or away from the sample to build a full three-dimensional image of the specimen. Typ-
ically, a single image plane can be recorded in around 1 second (for a 512 × 512 pixel
image).

CONFOCAL IMAGING
171
System variations are also possible in which the beam scanning is undertaken
using acousto-optic modulators. These are optical devices in which a high frequency
sound wave (typically around 40 MHz) is sent across a crystal to produce a Bragg
diffraction grating. If the sound wave is a traveling wave the output beam is scanned as
the diffraction angle changes at very high frequency permitting high speed scanning.
The major complication, ignoring expense and drive challenges, is that the device is
spectrally sensitive and different wavelengths are diffracted at different angles. This
method of imaging will be returned to in Section 5.3 when the method is considered
for nonlinear excitation.
With the use of the confocal pinhole in the imaging system, there is an addi-
tional improvement in the lateral resolution beyond that of wide-field imaging and
full details of confocal imaging can be found in the excellent text edited by Paw-
ley [14]. By adjusting the size of the pinhole the user can reject more or less of
the light from outside the focal plane of the objective. Normally, the pinhole will
be closed down to obtain the best possible resolution (the exact size being set by
the optical configuration of instrument and beyond the scope of this chapter) but
with feint samples, or ones in which the excitation level has to be kept low to
reduce photobleaching, it may be opened. As mentioned above, typically a laser is
used for excitation and until a few years ago this was almost universally an air-
cooled argon ion laser with a lifetime of only a few thousand hours and a source
of considerable heat in the laboratory. With the advent of laser diodes (in particular
those which operate in the blue) in the late 1990s, compact and reliable sources
are now available with high reliability [15]. Detection of the low light levels emit-
ted is normally via a photomultiplier or avalanche photodiode and the selection of
the correct detector is an important decision in the purchase and use of a scanning
confocal system.
A variation on the beam scanned confocal microscope is through the use of a
Nipkow spinning disc.
As illustrated in Figure 5.3, an expanded laser beam is incident on a rapidly
rotating array of microlenses. These focus part of the beam down onto a matching
array of holes and onto the sample. The returned light then passes back up through
the same hole and is reflected back onto a CCD detector (or in fact, the eye can be
used). By careful arrangement of the lenses in a spiral pattern it is possible to scan
the entire sample at high speed. Typically, over 7000 lenses and holes will be used
rotating at speeds of 1000 rpm.
These systems provide some optical sectioning capability, but are not as efficient
as rejecting the out-of-focus light as the more conventional point scanned system
[16]. In addition, they are wasteful of the laser light as only a small portion of the
beam is used at any one time. The main advantage that such a system has is that
very rapid imaging is possible with frame rates well in excess of 75fps, the limitation
mainly being due to the sensitivity of the camera and fluorescence efficiency of the
sample. They are well suited to imaging calcium transients, microtubule dynamics,
and fluorescence from highly punctuate samples at high speed and are commercially
available from several sources. Later in this chapter this method of high speed optical
sectioning microscopy will be compared with single plane illumination.

172
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
Microlens
array
Nipkow
disks
Pinhole
array
Direction of
rotation
Lens
Specimen
Objective
Dichoric
mirror
Objective
Camera
Spread laser
beam
Specimen
FIGURE 5.3
Nipkow spinning disk confocal.
5.2.3
Summary on Basic Confocal Microscopy
Confocal microscopy, using a pinhole to reject the out-of-focus light is thus clearly a
powerful instrument to help develop three-dimensional images of complete biological
structures. High resolution (diffraction limited) images can be captured in three
dimensions and the method works very well on samples up to a depth of around
50 microns. Beyond this depth, the image contrast is lost for several reasons. These
include light from outside the focal plane being scattered through the pinhole, and
light from the focal plane being scattered away from the pinhole. The short wavelength
excitation normally used in confocal microscopy, to match the excitation peak of most
common fluorophores, is also scattered before reaching the focal plane thus reducing
the number of excitation events at the focus of the objective. Thus for relatively
thin (perhaps up to around 50 microns) samples, confocal microscopy provides an
excellent method of capturing three-dimensional, optically sectioned images, but a
more sophisticated method is required for imaging at greater depths.
5.3
NONLINEAR MICROSCOPY
In confocal microscopy, described earlier, the crucial feature that enables optical
sectioning is the use of a pinhole to reject emission light returned from areas outside
the focal plane; in all forms of nonlinear microscopy, the optical sectioning is achieved
by localized excitation. In the last few years, the terms multiphoton and non-linear
microscopy have been used interchangeably which is scientifically incorrect. In the
following section, a convention has been adopted based upon the processes involved
in generating the detected signals. All methods produce a nonlinear response in
the material due to the interaction with light. In the case of fluorescence it is the

NONLINEAR MICROSCOPY
173
One-photon excitation
λ
Excitation   1
Ground state
λ
Fluorescence   2
λ
λ
  1 <   2
Two-photon excitation
λ1
λ1
Ground state
IR photon 2
IR photon 1
λ
Fluorescence   2
(a)
(b)
λ
λ
  1 >   2
FIGURE 5.4
(a) Single-photon excitation and (b) two-photon excitation.
absorption of two or more photons with a resulting emission of a single longer
wavelength photon; this will be termed multiphoton fluorescence microscopy or more
normally multiphoton microscopy. The other methods are harmonic and Coherent
Anti-Stokes Raman Scattering (CARS) and the physical basis behind these methods
will be explained later which will also show how they are nonlinear, but should not
technically be thought of as multiphoton.
The most important common feature in all of these methods is that the material
being excited behaves in a nonlinear manner with the excitation intensity, leading to
localized excitation due to the focusing of the excitation beam. Figure 5.4 illustrates
the difference in the excitation through linear (5.4a) and nonlinear (5.4b) excitation,
with the localized effect being clearly shown at the focus.
The resulting signal can then only have come from the focus of the light beam
and by scanning this point of excitation around in the sample an image can be
reconstructed, which is in 3D due to the inherent optical sectioning, as a consequence
of the local excitation. The different methods will now be examined in detail.
5.3.1
Multiphoton Fluorescence Microscopy
The basic concept of multiphoton (two or three photon) fluorescence imaging is
straightforward and the basic physical theory was first put forward by Goeppert-
Mayer in 1931 [17], for which she was awarded the Nobel Prize in Physics in 1963.
Using the then recently developed theory of quantum mechanics she postulated, and
demonstrated mathematically, it was possible for electrons to move between different
quantum mechanical levels through the absorption, or in fact emission, of two or more

174
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
quanta of light (photons), the total energy of the photons being the equivalent of a
single photon at a shorter wavelength. As the probability of such a multiple photon
event happening is very low (compared to single-photon transitions), this effect
was very much viewed as a mathematical, or quantum mechanical, curiosity. The
theoretical details of these effects are covered elsewhere in this book. However, in
practical terms with the invention of the laser in 1960 with its very high photon density,
when focused, the first reported observation of this effect was made in 1961 using a
ruby laser as the light source [18]. The first use of the effect was then in high resolution
laser spectroscopy and the measurements of fundamental constants [19], and the first
mention as an application for microscopy followed in 1978 [20,21], but it was a further
13 years before the method was first reported in the now seminal paper by Denk,
Strickler, and Webb [22]. In this paper, the team undertook a heroic experiment using
an optically complex colliding pulse dye laser, but they demonstrated that optically
sectioned microscopy was practical. Indeed this paper then opened the way for the
array of nonlinear-based microscopy methods that have emerged in the following
20 years. Although the concept of nonlinear microscopy had been discussed before,
and indeed in nonbiological systems shown, Denk’s paper showed that nonlinear
imaging, with minimal damage to the sample, was possible and this paper stimulated
thought in a number of innovative researchers around the world. As described below
the number of different excitation routes rose rapidly with each method having its
strengths and weaknesses but the core practical concept was now out and available.
A major reason for the previous slow adoption of nonlinear methods, into fields
outside atomic laser spectroscopy, was due to the lack of suitable laser sources. In
order to achieve suitable excitation rates, laser powers of around 1 kW are required,
though continuous–wave, two-photon excitation was subsequently reported, but has
never been used routinely [23]. Thus one requires a short, intense pulse of light, but
not too intense, to cause either thermal (generally a slow continuous wave effect)
or explosive (Q-switched or nanosecond pulse) damage to the sample. For imaging
purposes even with the correct form of laser pulse sufficiently high intensities can
only be found at the focus of high NA microscope objectives when the required flux
is only present in a very small volume localized in three dimensions.
Typically, the excitation source will be at twice the single-photon excitation wave-
length (half the photon energy). However, to be of practical use the excitation source
should not be linearly absorbed within the sample, otherwise one will have heating
and in the case of live biological samples cell death. In order to minimize such linear
events, most multiphoton imaging is undertaken using ultra-short pulse laser sources
typically with around 100 fs (100 × 10−15) pulse durations. One thus has a high peak
power (leading to high excitation rates) but a low average power minimizing heating
effects. A number of factors therefore affect the size of the fluorescent signal but most
notably the number of excitation events. For a pulsed laser, this is given by [24]
N = I2
peak𝛿
{
𝜋∗NA2
h ∗c ∗𝜆
}2
,
(5.3)
where Ipeak =
Pavg
Δv ∗𝜏p .

NONLINEAR MICROSCOPY
175
Here Ipeak is the peak intensity, Pavg the average power on the sample, 𝜏the
pulse width, 𝜈the repetition rate of the laser, NA the numerical aperture of the
objective lens, 𝜆the laser wavelength and h, c, and 𝜋have their normal physical
meanings; 𝛿is the two-photon absorption cross-section for the transition at the
laser wavelength and is typically around 10−50 (cm4s) per photon per molecule. For
practical implementation of this equation in microscopy, the repetition rate is limited
by considering the fluorescence lifetime of the fluorophore and also the speed with
which one wishes to image. (A laser at 1 Hz may have a low repetition rate giving
a high N, but scanning a 512 × 512 pixel image would not be practical.) These
considerations will be revisited when recent advances in laser sources are discussed.
The main advantages to the researcher of the multiphoton fluorescence technique
are that it enables imaging at significantly greater depths within samples and maintains
the viability of live samples, provided the conditions are correct. The main reason for
the ability to image more deeply arises from two physical principles. The excitation
wavelength is typically in the near-infrared (∼720–950 nm from a Ti:Sapphire laser)
and this wavelength is not linearly absorbed by most of the biological tissues and
scattering is reduced compared to visible wavelengths. Hence, as one moves deeper
into the sample, he/she is therefore able to increase the laser power and maintain a
high peak power despite the loss of a few photons due to scattering or aberration
caused by the optical properties of the tissue being imaged.
The second, more important, principle is that as a result of the localization of
the excitation one can collect all of the fluorescent light. In the other main optical
sectioning microscope, the confocal microscope, the fluorescent light has to pass back
through the instrument and then a pinhole in order to obtain the required sectioning
ability [25]. In the case of the multiphoton system all the light can be utilized to build
up the image, without the need to be descanned and pass through optical apertures,
as all the signal photons can only have come from the focal volume. It is possible to
image live samples with minimal damage as the number of phototoxic events within
the sample is significantly reduced under the correct conditions [26]. The entire depth
of the sample is not subjected to potentially damaging blue or ultraviolet light and
any damage that is caused by multiphoton processes can be limited to the focal plan.
By careful control of the imaging conditions, it is therefore possible to obtain images
from deep within the sample.
5.3.2
Harmonic Microscopy (SHG and THG)
Harmonic microscopy is based upon another nonlinear effect that takes place within
certain molecules, though this time more classical in nature rather than quantum
mechanical. The effect was known of for many years but was again only seen with
the invention of the laser. As an aside, the first paper in 1961 [27] contains a classic
editorial error where the spectral line showing the light at a specific wavelength was
removed by the editor as being dust on the photograph. However, before considering
the effect within a microscopic, or imaging, context, the basis of the process should
be considered.

176
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
Harmonic generation is best not considered as the direct addition of two photons
to produce one-half of the wavelength (or twice the frequency) or further additions
for high order harmonics (third, fourth, fifth, etc.). If one considers light as a wave
hitting a molecule, the electric field will cause changes in the charge distribution
within the molecule, inducing a dipole moment within the molecule. Under nor-
mal circumstances, this effect is very small and the size of the dipole is linear with
the electric field. The dipole moment per unit volume of the material (intrinsic and
induced by the electromagnetic wave) is known as the polarization. The total polar-
ization (Ptotal) can then be considered as the intrinsic value with a subsequent series
of additional changes that depend on the incoming electric field (E). Mathematically,
this is expressed in a simplified form as
Ptotal = P0
1 + x(1)E1 + x(2)E2 + x(3)E3,
(5.4)
where the term P0
1 is the permanent polarization, p1 = x(1)E1 has the change in
polarization being linear with the applied electric field, p2 = x(2)E2 (the second-order
polarization) has the change varying with the square of the electric field and so on.
Full details can be found in any textbook on nonlinear optics [28]. The light wave
(electric field) is just propagated by the P1 term but the second term causes both a
linear continuation of the light field and crucially a wave in which the electric field
has exactly twice the incoming frequency. At normal intensities of light this effect
is very small but at high electric field (such as those seen in high power lasers) this
harmonic wave can become large enough to be detected and seen. The simple effect
is illustrated in Figure 5.5 with the x(2) coupling with the incoming wave of frequency
𝜔to produce an output containing light at two frequencies 𝜔and 2𝜔. The diagram
also illustrates that the wave produced travels in the same direction as the incoming
light, thus the emission is in the forward direction in an imaging system and this can
be important in the detection method selected as discussed in Section 5.4.
This is the basis of, for example, green laser pointers, where the fundamental laser
is based on Nd and lases around 1064 nm but with a crystal inside the laser cavity
the light is doubled (wavelength halved) to 532 nm.
However, this process is coherent with the incoming waves of light and thus all
light produced by this process is coherent. In most normal situations, this means
that the waves produced by one molecule will destructively interfere with the waves
from the molecule next door. In crystals with a noncentrosymmetric structure the
light fields produced are such that they do not cancel out and the second harmonic
is viewed. This effect continues for the third harmonic and further up the frequency
chain but with decreasing amplitude.
FIGURE 5.5
Illustration of second harmonic generation.

NONLINEAR MICROSCOPY
177
At a base level, one can think of the molecule as vibrating in response to a wave.
If the amplitude of the wave becomes very large, then harmonics can be produced at
twice, three times, and so on, up the series of the frequency of the fundamental wave.
In many cases, the individual molecules will be arranged in such a way that these
harmonics all cancel out, but with the correct pattern they will all be in phase and the
harmonic will be seen.
In the case of microscopy, most materials are aligned such that they do not have
any second harmonic terms. The potential of nonlinear imaging was first described by
Sheppard et al. [29] in 1977, which was subsequently followed by a further paper by
the same group [30] in which they described a scanning microscope with an optical
enhancement cavity around the sample to increase the signal that would be suitable
for biological samples. Roth and Freund [31] first reported biological samples in an
international journal in 1981. Having demonstrated that collagen could be imaged by
second harmonic generation (SHG), they then applied the method to help solve a real
biological question, that of the structure and order of collagen within a rat’s tail [32].
The method now has gained widespread acceptance and is of particular value in
the imaging of in vivo tissue as the collagen structure can be visualized without the
need for an exogenous fluorophore and thus its structure can be used to place other
fluorescent markers in a physical context. As the light is emitted at exactly half the
incoming wavelength, it is also easy to separate the SHG signal from other fluorescent
markers using narrowband dichroic filters.
Over the next few years, little was done in biological imaging, and as with mul-
tiphoton fluorescence microscopy it was only really with the advent of biologist-
friendly femtosecond laser sources that the method really established a role within
biological microscopy. Various tissue samples were imaged over the next few years,
but the most significant advance was made when a dye was placed inside the cell
membrane; rather than exciting the molecule by tuning to a two-photon absorption
band and detecting the subsequent fluorescence, the authors used the dye as a source
of harmonic generation [33]. This was achieved as the dye was in the cell membrane,
and thus in a very thin layer, such that the molecules were generally aligned in one
direction. This means that there was no destructive interference of the light. Indeed
this paper took the work one stage further as the dye was voltage sensitive; thus as
the potential within the cell changed, fewer or an increasing number of the molecules
aligned themselves within the membrane, and thus, the SHG signal provided a read-
out of the membrane potential. Moreaux et al. [34] then took this further by tuning
the laser close to the florescence wavelength and thus had an enhancement in the
signal due to the inherent amplification of matching the wavelength to the dipole of
the molecule.
Third harmonic imaging (THG), although used less commonly primarily due to
the lower excitation rate, has also been shown to have real advantages for label-free
imaging of lipids under the correct conditions [35]. This method has since been
advanced through improved detection and was first applied to significant biological
challenges in 2006 [36].
The method had also at that point been applied to in vivo imaging of the cornea but,
potentially more interestingly, as a method of tumor detection [37]. This demonstrated

178
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
FIGURE 5.6
A combined SHG and multiphoton image of yellow fluorescent protein illus-
trating the advantage of a combination imaging method. The SHG provides the biological
context for the structure and position of the YFP expressing pericytes. Image courtesy of
Dr. Paul Thomas, Henry Wellcome Laboratory for Cell Imaging, University of East Anglia.
(For a color version of this figure, see the color plate section.)
that SHG was a viable tool for in vivo use. The advance in the method can be seen
in the work published in 2013 where human adipocytes have been imaged directly
using third harmonic microscopy [38]. Both of these papers illustrate the major poten-
tial in harmonic imaging for in vivo human use as no exogenous compounds were
introduced. The method can also be linked with more conventional two-photon fluo-
rescence, where the SHG provides the information about the structure of the sample
and the fluorophore (yellow fluorescent protein, YFP, as an example in Figure 5.6),
a specific biological tissue type. In this case with the laser tuned to 950 nm (close to
the two-photon peak for YFP), the collagen and muscle fibers emitted at the SHG of
475 nm (well away from the YFP emission peak around 560 nm).
5.3.3
Coherent Anti-Stokes Raman Scattering Microscopy
CARS (coherent anti-Stokes Raman scattering) is the third, and most recently devel-
oped, method of nonlinear microscopy. CARS is a four-wave mixing process in
which a pump beam at frequency 𝜔p (with a wavelength 𝜆p) and a Stokes beam at
frequency 𝜔s interact with a sample to generate an anti-Stokes signal at frequency
𝜔as = 2𝜔p −𝜔s. The CARS process was originally discovered in the Ford Motor
Company [39], but the acronym does not relate to its place of invention. CARS can
either be considered as a classical wave process, in a similar manner to that which
harmonic imaging was explained, or as a quantum mechanical photon process.
If we consider the classical model, in Eq. (5.4), we need to examine the E3 term. In
the case of CARS, we have three laser beams and therefore the electric field is not a

NONLINEAR MICROSCOPY
179
FIGURE 5.7
Energy diagram for basic CARS microscopy.
single cubic term. The three beams interact with each other through the x(3) term with
the high peak powers of the electric fields driving the polarization of the molecules.
By considering the molecular vibrations as a classical oscillator, with a frequency
𝜔v, this interaction effect can be enhanced by driving the system at a resonance (i.e.,
the difference in the energy levels within the molecule). In CARS, this oscillator is
not driven by a single optical wave, but by the beat frequency (𝜔p −𝜔s) between the
pump and the Stokes beams. When this beat frequency is close to 𝜔v (where ΔE is
the energy difference corresponding to 𝜔v in Figure 5.7), the molecular oscillator is
driven very efficiently. It is the movement of the electrons within the molecule that
give rise to the x(3) polarization term, and in effect this periodic motion is probed
by a third beam (actually 𝜔p) whose frequency is altered by the addition of the beat
frequency to produce the 𝜔CARS emission at the so-called anti-Stokes wavelength.
In the quantum mechanical situation, a photon of wavelength 𝜆p (frequency 𝜔p) is
absorbed (where the higher energy level may be a virtual) and then a second photon
at wavelength 𝜆s drives the excited electron down to a real vibrational state. A third
photon can now be absorbed sending the electron to another virtual state from which
it is again instantaneously driven down, in this case to the ground state emitting light
at the energy difference, which is then detected (𝜆detect).
In both processes, the emitted light and driving of the various energy levels is
undertaken coherently and thus all the individual molecules within the volume emit
light at the same time and in phase giving an enhancement in the signal providing the
phase matching condition is achieved. In order for constructive rather than destructive
interference to take place, the three wavelengths involved in the process need to have
the correct relative phase relationship. This is given by
n(𝜆det)
𝜆det
= n(𝜆P)
𝜆P
+ n(𝜆s)
𝜆s
,
(5.5)
where n is the refractive index of the material (at the given suffix wavelength) and
𝜆is the wavelength. In many optical techniques where CARS or other nonlinear
parametric process are used, this places a significant constraint on the conditions (in
frequency doubling for example the size and cut of crystals). However, in nonlinear
microscopy, due to the high numerical aperture of the lenses involved, the excitation
volume has a radius of around a micron, not much longer than the wavelength of the
light, and thus this constraint can be largely ignored.

180
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
As noted above, the process is driven by the x(3) term and the light is strongly
emitted in the forward direction. As the process is parametric and the electrons end
up back in their original state, no energy is lost in the process and no heating of
the sample takes place, thus there is no risk of the molecule being damaged. The
downside of this is that in fact no resonant molecule needs to be present at all in the
focal volume leading to a nonresonant background. The removal of this background
is a considerable practical complication and the details can be found in Section 5.4.
We now need to consider how this method has been used in a microscopic imaging
context. The first practical system was demonstrated in 1999 by Sunney Xie’s team,
based at the Pacific Northwest National Laboratory [40], where the optical perfor-
mance of the instrument was assessed as well as its use in live cells was demonstrated.
In the following 5 years, a number of papers were published, mainly by the same
group who had subsequently moved to Harvard, all demonstrating variations on the
basic method. Initially, improved epi detection [41], followed polarization sensitivity
[42]. An excellent review of all the advances made in this period can be found in the
work by Cheng and Xie [43]. Beyond the technical advances, the method has been
increasingly applied to real biological challenges.
5.3.4
Stimulated Raman Scattering Microscopy
Although CARS is a very powerful non-inear imaging method, it has a few drawbacks.
First, the signal includes a nonresonant background term that can be hard to avoid
and affects the signal-to-noise ratio, and second, the spectra that are determined (by
tuning the lasers) are not always those expected in conventional Raman spectroscopy
and thus the results can be confusing. A variation of CARS in that it is a nonlinear
process using Raman transitions is stimulated Raman scattering [44]. Here, the two
laser wavelengths are tuned such that their beat frequency is close to a Raman
vibration. When this beat frequency exactly matches the Raman frequency of a
vibration, molecular mode amplification of the Raman signal is achieved by what
can be viewed of as “stimulated emission.” The pump beam can be thought of as
exciting a mode up to a virtual state and it is then driven back down to a state close
its original state by the Stokes beam. This state will have a slightly higher energy
than the original ground state with the energy difference being the vibrational mode
or Raman frequency.
As the virtual state is driven down by the Stokes beam one now has in effect
two Stokes photons (the stimulating photon and the energy from the excited state
decaying down), the loss of a pump photon and a molecule now in a slightly higher
vibration level. The imaging signal can thus be detected by looking for either a loss
in intensity of the pump beam (SRL) or an increase in intensity of the Stokes beam
(SRG). If the frequency difference between the two incoming beams does not match
a vibrational mode, the excited molecule is not driven down to the lower energy state
and thus there is no background signal. This process relies on the intensity of two
light sources and is thus again nonlinear. Although the nonresonant background seen
in CARS is not present, one is now looking to detect a small change in intensity on a
signal that comes directly from the laser leading to practical challenges.

PRACTICAL IMPLEMENTATION OF NONLINEAR MICROSCOPY
181
5.4
PRACTICAL IMPLEMENTATION OF NONLINEAR MICROSCOPY
The core of all nonlinear microscope optical designs are common, but before consid-
ering each aspect of a multiphoton microscope in detail and how recent advances in
the technology and physics has lead to improved imaging, we will consider what is
needed for a multiphoton microscope. In a practical multiphoton system, the required
components are shown in Figure 5.8.
Light from a mode-locked laser source is directed into an optical microscope using
an XY scanning system in a very similar method to that used in the confocal system,
but with one important practical difference, that is, it lacks optical fiber which is
frequently used for beam coupling in confocal systems. When ultrashort laser pulses
are sent down an optical fiber, two effects start to take place. First, due to the high
peak power, nonlinear effects start to take place within the fiber. This can alter
the spectrum of the light emerging, generally broadening the optical spectrum and
crucially removing light from the wavelengths required for the imaging. Although
this can be useful in certain methods of producing ultrashort pulses in general, this
causes problems. Further details, and a summary of the laser types and complications
for use of lasers in nonlinear microscopy, can be found in Reference 45. The second
effect for femtosecond laser sources, those preferred for multiphoton fluorescence
and harmonic imaging, is that the optical pulses lengthen in time due to group velocity
dispersion (gvd). Femtosecond laser pulses have a significant optical bandwidth due
to their ultrashort nature. As a typical example, a 100-fs laser pulse at 800 nm (typical
Photomultiplier IV
(transmission)
Laser
Sample condenser lens
Dichroic
Dichroic objective lens
Scan mirror
Scan mirror
Photomultiplier I
(no descanning)
Photomultiplier Il
Photomultiplier Ill
FIGURE 5.8
Basic optical configuration for nonlinear microscopy.

182
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
for multiphoton imaging) will have a spectral width of around 15 nm. As this light
enters an optical fiber, the longer wavelengths travel more rapidly through the fiber
due to the refractive index change with wavelength; thus at the end of the fiber the blue
light will trail the red light that emerges at the front of the pulse. For the typical laser
pulse quoted above, in around 1 m of optical fiber, the pulse may be lengthened to
2 ps, lowering the peak power and signal according to Eq. (5.3) with the concomitant
loss of signal. Thus, the preferred route to beam delivery is using conventional optics
rather than mirrors.
The light is then passed through a high numerical aperture microscope objective
(in order to minimize the focal volume and to increase the excitation rate and reso-
lution). It is important to note here that not all microscope objectives are equivalent
when it comes to delivering infrared ultrashort pulses. As can be appreciated from
the comments above relating to optical fibers, one effect is that the pulses can be
lengthened [46]. A high numerical aperture objective may contain 16 lenses made
of a range of glasses. These have the effect of lengthening the pulse due to their
dispersion properties. Lengthening our typical pulse from its initial 100 fs to around
300–400 fs produces a loss of signal. Generally, the higher the magnification and
NA of the lens the higher the dispersion due to the complexity of design. Thus, it
is always worth arranging with the microscope manufacturer to try several lenses
before settling on the preferred choice for the imaging you wish to undertake.
There are practical methods for removing this lengthening effect by making the
light at the leading temporal edge of the pulse to travel a further distance than the
trailing edge and hence the different wavelengths “bunch up.” This can be achieved
using a spectrally dispersive element to separate the wavelengths spatially and then
letting the required wavelengths to travel further before the pulse is spectrally recom-
bined. This dispersion compensation can be achieved in a number of configurations
using gratings, prisms, or complex optical coatings. Weiner [47] provided an excel-
lent review of the methods and this provides far more detail than is required in this
chapter. Commercial femtosecond lasers now incorporate the option of dispersion
compensation within the sample and imaging optics.
As well as the dispersion in the glass, there is also the complication of chromatic
aberration. Many high performance lenses designed for fluorescence imaging are
very well optimized for use across the visible spectrum (plan-fluor lenses being
corrected from 405–647 nm), but with nonlinear imaging the input wavelength may
be around 1200 nm or longer with the signal being emitted at less than 600 nm. This
means that chromatic aberration can be a significant issue, with the lens concentrating
the excitation light at one plane, but the light collection being higher at another. In
addition, the optical coatings in typical lenses are optimized for visible use and may
even become reflectors at the longer wavelengths. Again, the only solution to these
practical issues is through experimentation, though in the last few years the major
microscope companies have become aware of these problems and designed lenses
suitable for nonlinear microscopy, and as the field grows, the desire for suitable optics
is only likely to increase.
In the case of the CARS systems, one requires two laser wavelengths that are tuned
to the vibration difference one wishes to probe. The most conventional method of

PRACTICAL IMPLEMENTATION OF NONLINEAR MICROSCOPY
183
undertaking this now is through the use of one fixed wavelength and a tunable source
that is synchronized to the fixed wavelength. This synchronization, and tuning, can be
achieved simultaneously by using one laser to pump an optical parametric oscillator
and using the output from that source, along with the original pump source, as the
light to excite the CARS signal. This is now the general configuration that is used,
and as mentioned above, the pulse lengths are typically around a few picoseconds in
order to have a high peak power but good spectral overlap with the specific vibrational
modes being examined [48].
Having delivered the light sufficiently well to the target, the resulting emission
light can then either be detected using a nondescanned detector (placed as close as
possible to the back of the objective after a suitable dichroic mirror) or the signal
can go back through the scanning optics and be collected on the detectors normally
used for confocal imaging. The non-descanned detector uses the inherent optical
sectioning of nonlinear microscopy to collect as much signal light as possible. Every
signal photon can only have come from the focus of the beam and thus collecting the
light as soon as possible makes sense. Even with the best mirrors in the configuration
shown above, the light will pass off an additional 11 surfaces if the descanned detectors
are used, leading to a loss of signal. In certain circumstances, some of the incoming
light can be scattered to local focal maxima and this can lead to signal being generated
outside the focal plane, one such tissue sample being the brain. There are methods to
overcome this complication by altering the incoming beam and undertaking image
subtraction, but the details [49] are beyond the scope of this chapter.
In all nonlinear imaging systems, however, it is also possible to collect the light
in the forward direction. In the case of fluorescence, the light is emitted at all solid
angles (4𝜋steradians) and thus as much light goes forward as backwards (though
frequently through more samples and thus a potential loss of signal). In the case
of second and third harmonic imaging, the emitting dipole has a preference for the
forward direction and thus collecting after the sample, as shown above, is a very
sensible option [50].
For the CARS microscopy, the situation is more complex. As described above,
the CARS signal is generated by a parametric process and this means that no energy
is transferred to the sample. Thus, CARS can occur when no resonant molecules are
present in the focal volume as the energy difference between the pump and Stokes
wavelength is simply removed as a nonresonant background, though the CARS
signal is much larger when there is a resonant molecule present. This nonresonant
background signal can limit the sensitivity and the method of detection can be critical
in achieving a high signal-to–noise ratio. In the Forward CARS configuration [51],
the signal is detected in the forward direction using a high NA objective on the far
side of the sample (transmission detector in the above diagram). This F-CARS signal
is large (due to the process of scattering being forward-biased in this situation) and
thus the nonresonant background (frequently caused due to the water in the sample)
can be removed by simple image subtraction. However, in the epi configuration,
the back-scattered light is detected, when the nonresonant light is not present [41].
This method is particularly suitable when objects being imaged are smaller than the
excitation wavelength as more light is directed back through the system. It is also a

184
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
preferred method in highly scattering samples (e.g., brain) when the normally forward
propagating CARS signal is back-scattered.
There is one other consideration that is frequently overlooked in signal detection
in nonlinear microscopy. All practical systems now use a pulse laser source run-
ning at a high repetition frequency and thus lock-in, or phase-sensitive detection is
possible [52]. Using a radio-frequency lock-in detector (such as the chips present
in every mobile phone) background light caused by room lights (typically around
100–120 Hz) is rejected very easily as the laser sources are typically running around
80 MHz. Indeed by adjusting the phase of the detector it is possible to make a mea-
surement of fluorescent lifetimes, or separate the SHG and CARS signals (which
are virtually instantaneous with the laser pulse) from fluorescent signals, which are
typically delayed by several nanoseconds due to the inherent fluorescent lifetime in
all fluorescent processes.
5.5
RECENT ADVANCES IN NONLINEAR MICROSCOPY
5.5.1
Miniature Instrumentation and Clinical applications
It is now clear that the basic methodologies in nonlinear microscopy are now well
established, but it is worth considering the current directions that research projects
are taking. One obvious line of activity is to reduce the size and complexity of the
instrumentation. In terms of laser sources, as described above, this has already taken
place and, from that perspective, it is now the cost rather than the complexity that is the
limiting factor. For beam-scanned systems, again the technology currently available
in commercial systems performs well in the standard biological imaging laboratory.
However, the next moves are clearly toward miniaturization as the imaging methods
move toward less-invasive and more in vivo applications, where the ultimate goal
may well be for routine clinical use.
The first published work in the direction of miniaturization was undertaken in
around 2001 in Bell Labs [53]. Here, the system delivered the ultrashort pulses to
a miniature scanning device within a rat brain. The end of the fiber was driven in a
figure-of-eight pattern so that a specific area of the brain was imaged. The animal was
free to move though tethered through the optical fiber. The pulse lengthening effect
of sending ultrashort pulses through a length of the optical fiber was compensated for
using a prechirping technique. Here, for the standard optical fiber in which the longer
wavelengths travel more quickly using a grating or prism, the shorter wavelengths
of light can be sent into the optical fiber first such that by the time the light reaches
the end of the fiber the different wavelengths are again back together temporally, and
one has ultra-short pulses [54]. The emitted light was collected by the same scanning
fiber and detected after the tethering system. Two years later the method was used to
understand a biological questions that had not previously been possible to even ask
as the imaging methodology was not available [55].
The resolution of this method was around 3–5 μm and an alternative method
has been the development of a more rigid probe that is placed in an anesthetized

RECENT ADVANCES IN NONLINEAR MICROSCOPY
185
animal. In order to make the probes as small as possible, a technology originally
developed for the telecoms markets, that of graded index (GRIN) optics, has been
used [56]. Again the light is delivered via an optical fiber with pulse compression,
but the emerging light is reimaged from the end of the fiber into the sample using two
GRIN lenses. This means that one has a relatively high numerical aperture imaging
systems with a diameter of around 2 mm. Subsequently, though not used in a nonlinear
manner, a single longer GRIN optic has been used [57], along with a combination
of GRIN and conventional optics in a nonlinear system [58]. This technology has
also been applied to CARS imaging [59] though the detection via the excitation fiber
is not ideal as explained above due to the directionality of the CARS emission. An
alternative route to optically sectioned imaging is via a coherent optical fiber bundle,
this clearly increased the diameter of the probe but has the advantage of potentially
higher resolution images [60].
Thus, the route to miniaturized probes has clearly made very rapid progress and
some of these methods are now making their way into the clinic primarily as a
potential cancer diagnostic instrument. Generally, here the full range of nonlinear
imaging methods are thrown at the challenge with the different modalities highlight-
ing different aspects of the tissue, most frequently skin [61]. For obvious reasons,
the application of fluorescently labeled fluorophores is ethically much harder in the
clinic, and thus endogenous compounds have to be excited by one of the nonlinear
methods described above. In the reference quoted, multiphoton fluorescence, har-
monic excitation, and CARS are all used to examine the skin in one of the first
full-scale clinical trials of nonlinear imaging as a method of optical bioscopy.
5.5.2
Adaptive Optics for In-Depth Imaging
It should also be noted that as well as the development of miniature probes for
minimally invasive in-depth imaging, conventional non-linear microscopes can have
their depth and resolution increased through the use of active optical elements, or
adaptive optics [62, 63]. Typically in microscopy the optical element is a mirror
whose shape can be changed via the application of a control voltage. As one images
deeply into the sample, the tissue through which the light has to pass has an optical
effect on the excitation light increasing the focused spot size. This leads to a loss
of resolution and also a loss of signal as the excitation light is spread over a larger
volume. Figure 5.9 illustrates the way that the method works and has mostly been
applied in beam scanned systems. Here the incoming laser beam is directed off an
adaptive optical element before entering the back aperture of the lens. By placing
the opposite wavefront distortion on the incoming light to that induced by the tissue
optics the result at the focus of the imaging system is a plane wavefront, and the
resolution and signal levels can be restored.
There are several methods to determine the actual shape of the mirror, but the
vast majority currently use an image metric based method. Various mirror shapes
are applied to the adaptive optic element and the effect on the quality of the image
(using a factor such as contrast, brightness, edge definition) is used to determine if
that mirror shape has improved the image. Using different optimization algorithms

186
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
Mirror control
Light source
Adaptive optic
Sample
Image analysis
and determine shape
Image detection
Objective
FIGURE 5.9
Basic implementation of adaptive optics to compensate for optical aberrations.
(as described in the two papers quoted) the best mirror shape can be approached very
rapidly. In one practical implementation when the technique was applied to CARS
microscopy for the first time [64] the signal at a depth of 750 microns (when the
objective hit the sample) was larger than the original signal without adaptive optics at
a depth of 20 microns. In addition to removing sample-induced aberrations, adaptive
optics can also remove the residual aberrations present in an optical system even
when it has been aligned.
5.6
WIDEFIELD OPTICAL SECTIONING BY SPECIALIZED
ILLUMINATION METHODS
In all of the systems described above a point source of illumination has been used
that has subsequently been scanned across the sample to build up an image pixel
by pixel. While this without doubt provides the highest resolution images (without
considering sub-diffraction imaging methods) the beam scanning is inherently slow
compared to using a camera to record an entire image in one shot. However, in most
situations this does not provide any level of optical sectioning but through the use of
controlled light patterns optical sectioning can take place.
5.6.1
Single Plane Illumination Microscopy (SPIM)
In the first method, Single (or selective) Plane Illumination Microscopy, a sheet of
light is used to illuminate the sample [65]. The basic configuration is illustrated in
Figure 5.10. Here light is introduced to the sample (which needs to be transparent such

WIDEFIELD OPTICAL SECTIONING BY SPECIALIZED ILLUMINATION METHODS
187
Imaging objective
Fluorescence
image
Illumination beam
launch objective
Biological sample
SPIM sheet
y
x
z
Red/IR image
(transmission)
FIGURE 5.10
Basic single-plane illumination microscope configuration.
as an embryo) in a sheet produced using a cylindrical lens and microscope objective
combination. This creates a sheet of light within the sample only illuminating a
single plane. With a sample that is fluorescently labeled only the areas illuminated
will produce fluorescence, which is then viewed perpendicular to the illumination
source through a second microscope objective.
One has thus taken a single optical slice through the sample and captured an entire
optical plane in a single camera exposure. By then moving the sample through this
illumination system an entire three-dimensional image can be built up rapidly. In
the illustration of Figure 5.10 an additional feature is present, that of an infrared
transmission image, which can be used to synchronize any repeated movement in the
sample, for example, from a beating heart, to effectively freeze the motion without
needing to stop the heart beating [66,67]. There are a large number of variations in
the method, for example, using nonlinear excitation within the beam [68] and through
the use of multiple angles of excitation [69,70] but the basic principle of illuminating
and observing perpendicular, or nearly so, applies.
5.6.2
Structured Illumination Microscopy
An alternative approach to obtaining optical sectioning is provided by the method of
structured light [71] and a brief description is included here as a commercial “add-on”
to a standard microscope is now available. The basic system is shown in Figure 5.11.
Light from a noncoherent source, and even a candle has been used as the method
is highly efficient, is collimated and directed onto a grid with a regular pattern of
lines, ideally high contrast sine waves of a single frequency. This grid pattern is then
projected onto the sample through the objective and the returned light is detected on

188
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
Grid
Beam
splitter
Objective
Sample
Light
Piezo
drive
Camera
Computer
FIGURE 5.11
Basic structured illumination configuration.
a CCD camera in either fluorescence or a reflection mode and an image is captured.
The grid is then translated laterally by a third of its spacing and second image taken
before it is advance a further third and the final picture stored.
Each of the images has the grid pattern present and if all the three images are
directly added together one just has the conventional wide field image. However, if
the images are manipulated in another way,
Isection = [(I1 −I2)2 + (I1 −I3)2 + (I2 −I3)2]1∕2,
(5.6)
where I1, I2, and I3 are the three individual images, then in the focal plane the
pattern adds in such a way as to remove the structured illumination to give a uniform
illumination. However, the light from outside the focal plane combines to leave a
uniformly dark image. This means that after combining the images one only has the
image from the focal plane left, and hence an optical section has been obtained.
This method does not provide the high optical sectioning of the pinhole, in practical
instruments, but is significantly simpler than any scanned system and hence has a
role as a low cost optical sectioning system. The method has also been combined
with both rigid and flexible endoscopes to produce miniature high speed optically
sectioned images [72]. The advantage over other systems described above is the speed
of imaging as the beam does not need to be scanned to every pixel. The method of

WIDEFIELD OPTICAL SECTIONING BY SPECIALIZED ILLUMINATION METHODS
189
placing a pattern onto the image has also been combined with an SPIM system to
improve the resolution and contrast in the images [73].
5.6.3
Sub-diffraction-Limited Imaging
It is worth a brief final comment on methods that have been developed to beat Abbe’s
diffraction limit for imaging resolution [3]. If these methods truly beat Abbe’s theory
is a subject for conjecture far beyond the scope of this chapter, but several methods
without doubt now produce images in which the resolution is around a tenth of that
predicted by Abbe and frequently even higher resolution images are possible under
ideal circumstances. The brief synopsis below does not attempt to cover all of the
methods, but outlines three.
The first method to be mentioned is that pioneered by Stefan Hell and lateral
resolutions of 28 nm have been reported through the use of stimulated emission
to deplete the fluorescently excited volume with a second laser pulse [74, 75]. In
Stimulated Emission Depletion (STED) microscopy two laser beams are aligned
co-linearly through the objective, but with different beam profiles Figure 5.12.
A pulsed excitation beam is used to excite the fluorescence within the sample with
pulses up to 100 ps. A second pulse between 100 fs and 300 ps, tuned to the emission
wavelength, is timed to arrive at the end of the excitation pulse and with a spatial
profile that overlaps the excitation pulse, but generally has a dark “hole” overlapping
the central point of the original excitation beam. This second beam causes stimulated
emission from the sample and hence a gated detector (activated after the end of the
second pulse) only receives fluorescent light from the undepleted, central portion
of the initial excitation volume. Increasing the intensity of the stimulating beam
effectively reduces the “size” of the central dark regions meaning that the remaining
fluorescence comes from a smaller area, which can be sub-wavelength. To build up
images, either the sample or the beam needs to be scanned in a point-by-point manner,
and clearly, this means that the imaging method is not rapid.
FIGURE 5.12
STED microscopy using the dark region at the center of an annular beam to
get sub-wavelength resolution.

190
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
Structured illumination can also be used to build up sub-diffraction-limited imag-
ing and this method has been pursued by many groups in a number of different
formats and reviews of the early work can be found in two major articles written
by Gustafsson [76] and Heintzmann [77]. In simple terms, the detail in any image
is carried by the high spatial frequencies and the limiting element in transmitting
these spatial frequencies is typically set by the numerical aperture of the objective
lens. To overcome this limitation, a moir´e fringe method is adopted. A spatial light
pattern is placed onto the sample of known frequency and this pattern is rotated to
several positions, typically nine or fifteen, with images being taken at every angle.
The pattern of light interacts with the high frequency detail in the sample, produc-
ing a low “beat frequency” determined by the spatial frequency difference between
the pattern and detail within the sample. This low frequency information is able to
pass through the objective lens and all of the images are subsequently processed
(via Fourier transforms in a computer) adding back the original pattern frequency to
produce a high resolution image. Typical images are now possible at around 10 nm,
and as the images are captured using a wide field camera, the method is faster than
point scanning.
The most recent method of beating the diffraction limit involves the use of spe-
cial fluorophores that can be switched from an active to inactive state (i.e., their
fluorescence properties are controllable). These methods have developed two names
for basically the same process, namely STORM [78] or PALM [79, 80]. Here a
photoswitchable fluorophore is introduced into the substance. Initially all of the flu-
orophores are in the “off-state.” A very low level of light (typically UV) is then
directed onto the sample, which has the effect of altering some of the fluorescent
molecules into the “on-state” through a photo-induced molecular shape change. A
conventional wide field fluorescent image is then taken from the small number of
fluorescent molecules. A third burst of light then “switches-off” all the fluorophores.
This process is then repeated many times stochastically switching on the fluorophores
in the sample. The images are subsequently analyzed as each fluorescent molecule
will have illuminated a small array of pixels on the camera. The center of mass of
each array of pixels is found (typically this can be done with an accuracy of around
2–10 nm) and this is then assumed to be the point of light emission. Thus by building
up a large number of images one assumes that every potential fluorophore has been
excited and where there are several then they will have been excited many times to
build up the contrast in the image. In a variation of this, it has been found that many
fluorescent molecules have a natural tendency to move from the “off” to “on” state
when imaged rapidly enough and when the concentration of molecules is low. Thus
by applying the correct mathematical formulae one can build up the same form as
those with the photoswitchable fluorophores [81].
5.7
SUMMARY
This chapter has only touched upon the methods available for optical microscopy
based upon fluorescence and can only provide an outline of the work undertaken in

REFERENCES
191
the field. There are many high quality textbooks on the subject, some of which have
been referenced, and the reader is encouraged to study those for the specific details of
methods mentioned here. The practical implementation of standard instruments has
been covered with an outline of the physical processes behind the different excitation
methods. With increasing complexity the chapter has ended with three-dimensional
imaging techniques with optical resolutions down to 10 nm, approaching that of
electron microscopes. Optical microscopes have developed a long way in 400 years
and it is to be anticipated that with advances in new technologies new tools will soon
be provided to users of optical microscopy.
REFERENCES
[1] Pliny the Elder. The Natural History (H.G Bohm, London, 1893; translated by John
Bostock) Book XXXVII, Chap 10.
[2] R. Hooke, Micrographia: or, Some Physiological Descriptions of Minute Bodies Made
by Magnifying Glasses (J. Martyn and J. Allestry, London, 1665).
[3] E. Abbe, “On the estimation of the aperture in the microscope,” J. R. Microsc. Soc. 2,
388–423 (1881).
[4] F. Zernike, “The concept of degree of coherence and its application to optical problems,”
Physica 5, 785–795 (1938).
[5] J. R. Lakowicz, Principles of Fluorescence Spectroscopy (Springer, New York, 2010).
[6] R. Heim, A. Cubitt, and R. Tsein, “Improved green fluorescence,” Nature 373, 663–664
(1995).
[7] N. Shaner, P. Steinbach, and R. Tsien, “A guide to choosing fluorescent proteins,” Nat.
Methods 2, 905–909 (2005).
[8] E. Hecht, Optics, 4th ed. (Pearson, 2003).
[9] M. Minsky, US Patent No. 3,013,467, 1961.
[10] M. D. Egger and M. Petr˘an, “New reflected-light microscope for viewing unstained brain
and ganglion cells,” Science 157, 305–307 (1967).
[11] C. J. Sheppard and T. Wilson, “The theory of the direct-view confocal microscope,”
J. Microsc. 124, 107–117 (1981).
[12] J. G. White, W. B. Amos, and M. Fordham, “An evaluation of confocal versus conven-
tional imaging of biological structures by fluorescence light microscopy,” J. Cell Biol.
105, 41–48 (1987).
[13] G. Van Meer, E. H. K. Stelzer, R. W. Wijnaendts-van Resandt, and K. Simons, “Sorting
of sphingolipids in epithelial (Madin–Darby canine kidney) cells,” J. Cell Biol. 105,
1623–1635 (1987).
[14] J. Pawley, Handbook of Biological Confocal Microscopy, 3rd ed. (Kluwer Academic
Publishers, 2006).
[15] J. M. Girkin, A. I. Ferguson, D. L. Wokosin, and A. M. Gurney, “Confocal microscopy
using an InGaN violet laser diode at 406nm,” Opt. Exp. 7, 336–341 (2000).
[16] A. Egner, V. Andresen, and S. W. Hell, “Comparison of the axial resolution of practi-
cal Nipkow-disk confocal fluorescence microscopy with that of multifocal multiphoton
microscopy: theory and experiment,” J. Microsc. 206, 24–32 (2002).

192
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
[17] M. Goeppert-Mayer, “Ueber Elementarake mit zwei Quantenspruengen,” Annu. Phys. 9,
273–283 (1931).
[18] W. Kaiser and C. G. B. Garrett, “Two photon excitation of CaF2:Eu2+,” Phys. Rev. Lett.
7, 229–231 (1961).
[19] S. A. Lee, R. Wallenstein, and T. W. Hansch, “Doppler free two photon spectroscopy,”
Phys. Rev. Lett. 35, 1262–1265 (1975).
[20] C. J. R. Shepherd and R. Kompfiner, “Resonant scanning optical microscope,” Appl. Opt.
17, 2879–2891 (1978).
[21] T. Wilson and C. J. R. Shepherd, “Non-linear microscopy,” in Theory and Practice of
Scanning Optical Microscopy (Academic Press, Boston, MA, 1984), Chap. 10.
[22] W. Denk, J. H. Strickler, and W. W. Webb, “Two-photon excitation in laser-scanning
fluorescence microscopy,” Science 248, 73–76 (1990).
[23] P. E. Hannienen, E. Soni, and S. W. Hell, “Continuous wave excitation two-photon
fluorescence microscopy,” J. Microsc. 176, 222–225 (1994).
[24] W. Denk, D. W. Piston, and W. W. Webb, “Two-photon excitation in laser scanning
microscopy,” in Handbook of biological confocal microscopy, 2nd ed., edited by J. B.
Pawley (Plenum Press, New York, 1995), pp. 445–458.
[25] J. M. Girkin and D. L. Wokosin, “Practical multiphoton microscopy,” in Confocal and
Multiphoton Microscopy, edited by A. Diaspro (John Wiley & Sons, New York, 2000),
Chap. 31.
[26] K. Koenig, Y. Tadir, P. Patrizio, M. W. Berns, and B. J. Tromberg, “Effects of ultraviolet
exposure and near infrared laser tweezers on human spermatozoa,” Hum. Reprod. 11,
2162–2164 (1996).
[27] P. Franken, A. Hill, C. Peters, and G. Weinreich, “Generation of optical harmonics,” Phys.
Rev. Lett. 7, 118–120 (1961).
[28] R. Boyd, Nonlinear Optics, 3rd ed. (Elsevier, 2008).
[29] C. J. R. Sheppard, J. N. Gannaway, R. Kompfner, and D. Walsh, “The scanning harmonic
optical microscope,” IEEE Trans. Quantum Electron. 13, 100D (1977).
[30] C. J. R. Sheppard and R. Kompfner, “Resonant scanning optical microscope,” Appl. Opt.
17, 2879–2882 (1978).
[31] S. Roth and I. Freund, “Optical second harmonic scattering in rat-tail tendon,” Biopoly-
mers 20, 1271–1290 (1981).
[32] S. Roth and I. Freund, “Second harmonic generation and orientational order in connec-
tive tissue: a mosaic model for fibril orientational ordering in rat-tail tendon,” J. Appl.
Crystallogr. 15, 72–78 (1982).
[33] P. J. Campagnola, M. D. Wei, A. Lewis, and L. M. Loew, “High-resolution nonlinear
optical imaging of live cells by second harmonic generation,” Biophys. J. 77, 3341–3349
(1999).
[34] L. Moreaux, O. Sandre, M. Blanchard-Desce, and J. Mertz, “Membrane imaging by
simultaneous second-harmonic generation and two-photon microscopy,” Opt. Lett. 25,
320–322 (2000).
[35] Y. Barad, H. Eisenberg, M. Horowitz, and Y. Silberg, “Nonlinear scanning laser
microscopy by third harmonic generation,” Appl. Phys. Lett. 70, 922–924 (1997).
[36] D. Debarre, W. Supatto, A. Pena, A. Fabre, T. Tordjmann, L. Combettes, M.-C. Schanne-
Klein, and E. Beaurepaire, “Imaging lipid bodies in cells and tissue using third-harmonic
generation microscopy,” Nat. Methods 3, 47–53 (2006).

REFERENCES
193
[37] Y. Guo, H. E. Savage, F. Liu, S. P. Schantz, P. P. Ho, and R. R. Alfano, “Subsurface
tumor progression investigated by noninvasive optical second harmonic tomography,”
Proc. Natl. Acad. Sci. USA 96, 10854–10856 (1999).
[38] C.-K. Tsai, T.-D. Wang, J.-W. Lin, R.-B. Hsu, L.-Z. Guo, S.-T. Chen, and T.-M. Liu,
“Virtual optical biopsy of human adipocytes with third harmonic generation microscopy,”
Biomed. Opt. Exp. 4, 178–186 (2013).
[39] P. D. Maker and R. W. Terhune, “Study of optical effects due to an induced polarization
third order in the electric field,” Phys. Rev. 137, 7–10 (1965).
[40] A. Zumbusch, G. R. Holtom, and X. S. Xie, “Three-dimensional vibrational imaging by
coherent anti-stokes Raman scattering,” Phys. Rev. Lett. 82, 4142–4145 (1999).
[41] A. Volkmer, J. Cheng, and X. S. Xie, “Vibrational imaging with high sensitivity via
epidetected coherent anti-stokes Raman scattering microscopy,” Phys. Rev. Lett. 87,
023901 (2001).
[42] J. Cheng, L. D. Book, and X. S. Xie, “Polarization coherent anti-Stokes Raman scattering
microscopy,” Opt. Lett. 26, 1341–1343 (2001).
[43] J. Cheng and X. S. Xie, “Coherent anti-Stokes Raman scattering microscopy: instrumen-
tation, theory, and applications,” J. Phys. Chem. B 108, 827–840 (2004).
[44] C. W. Freudiger, W. Min, B. G. Saar, S. Lui, G. R. Holtom, C. He, J. C. Tsai, J. X. Kang,
and X. S. Xie, “Label-free biomedical imaging with a high sensitivity by stimulated
Raman scattering microscopy,” Science 322, 1857–1861 (2008).
[45] J. M. Girkin, “Laser sources for non-linear microscopy,” in Handbook of Biomedical
Nonlinear Optical Microscopy, edited by B. Masters and P. So (Oxford University Press,
2008), Chap. 8.
[46] M. M¨uller, J. Squier, R. Wolleschensky, U. Simon, and G. Brakenhoff, “Dispersion pre-
compensation of 15 femtosecond optical pulses for high-numerical-aperture objectives,”
J. Microsc. 191, 141–150 (1998).
[47] A. M. Weiner, “Femtosecond pulse shaping using spatial light modulators,” Rev. Sci.
Instrum. 71, 1929–1951 (2000).
[48] F. Ganikhanov, S. Carrasco, X. S. Xie, M. Katz, W. Seitz, and D. Kopf, “Broadly tunable
dual-wavelength light source for coherent anti-Stokes Raman scattering microscopy,”
Opt. Lett. 31, 1292–1294 (2006).
[49] A. Leray and J. Mertz, “Rejection of two-photon fluorescence background in thick tissue
by differential aberration imaging,” Opt. Exp. 14, 10565–10573 (2006).
[50] N. Moreno, J. A. Feijo, and G. Cox, “Implementation and evaluation of a detector for
forward propagated second harmonic signals,” Micron 35, 721–724 (2004).
[51] A. Zumbusch, G. R. Holtom, and X. S. Xie, “Three-dimensional vibrational imaging by
coherent anti-Stokes Raman scattering,” Phys. Rev. Lett. 82, 4142–4145 (1999).
[52] W. G. Fisher, D. W. Piston, and E. A. Wachter, “Phase sensitive demodulation in multi-
photon microscopy,” Microsc. Microanal. 8, 191–202 (2002).
[53] F. Helmchen, M. S. Fee, D. W. Tank, and W. Denk, “A miniature head-mounted two-
photon microscope. High-resolution brain imaging in freely moving animals,” Neuron
31, 903–912 (2001).
[54] R. L. Fork, O. E. Martinez, and J. P. Gordon, “Negative dispersion using pairs of prisms,”
Opt. Lett. 9, 150–152 (1984).
[55] F. Helmchen, “Dynamic confocal imaging of living brain miniaturization of fluorescence
microscopes using fibre optics,” Exp. Physiol. 87, 737–745 (2002).

194
OPTICAL SECTIONING MICROSCOPY AND BIOLOGICAL IMAGING
[56] D. Bird and M. Gu, “Two-photon fluorescence endoscopy with a micro-optic scanning
head,” Opt. Lett. 17, 1552–1554 (2003).
[57] C. D. Saunter, S. Semprini, C. Buckley, J. Mullins, and J. M. Girkin, “High spatial
resolution fluorescent imaging,” Biomed. Opt. Exp. 3, 1274–1278 (2012).
[58] R. P. J. Barretto, B. Messerschmidt, and M. J. Schnitzer, “In vivo fluorescence imaging
with high-resolution microlenses,” Nat. Methods 6, 511–512 (2009).
[59] B. G. Saar, R. S. Johnston, C. W. Freudiger, X. S. Xie, and E. J. Seibel, “Coherent Raman
scanning fiber endoscopy,” Opt. Lett. 36, 2396–2398 (2011).
[60] S. Santos, K. K. Chu, D. Lim, N. Bozinovic, T. N. Ford, C. Hourtoule, A. C. Bartoo, S.
K. Singh, and J. Mertz, “Optically sectioned fluorescence endomicroscopy with hybrid-
illumination imaging through a flexible fiber bundle,” J. Biomed. Opt. 14, 030502 (2011).
[61] K. K¨onig, H. G. Breunig, R. B¨uckle, M. Kellner-H¨ofer, M. Weinigel, E. B¨uttner, W.
Sterry, and J. Lademann, “Optical skin biopsies by clinical CARS and multiphoton
fluorescence/SHG tomography,” Laser Phys. Lett. 8, 465–468 (2011).
[62] M. J. Booth, “Adaptive optics in microscopy,” Phil. Trans. A Math. Phys. Eng. Sci. 365,
2829–2843 (2007).
[63] J. M. Girkin, S. Poland, and A. J. Wright, “Adaptive optics for deeper imaging of
biological samples,” Curr. Opin. Biotechnol. 20, 106–110 (2009).
[64] A. J. Wright, S. P. Poland, J. M. Girkin, C. W. Freudiger, C. L. Evans, and X. S. Xie,
“Adaptive optics for enhanced signal in CARS microscopy,” Opt. Express 15, 18209–
18219 (2007).
[65] J. Huisken, J. Swoger, F. Del Bene, J. Wittbrodt, and E. H. K. Stelzer, “Optical sectioning
deep inside live embryos by selective plane illumination microscopy,” Science 305,
1007–1009 (2004).
[66] J. M. Taylor, C. D. Saunter, G. D. Love, J. M. Girkin, D. J. Henderson, and B. Chaudhry,
“Real-time optical gating for three-dimensional beating heart imaging,” J. Biomed. Opt.
16, 116021 (2011).
[67] J. M. Taylor, J. M. Girkin, and G. D. Love, “High-resolution 3D optical microscopy
inside the beating zebrafish heart using prospective optical gating,” Biomed. Opt. Express
3, 314–321 (2012).
[68] J. Palero, S. I. C. O. Santos, D. Artigas, and P. Loza-Alvarez, “A simple scanless two-
photon fluorescence microscope using selective plane illumination,” Opt. Express 18,
8491–8498 (2010).
[69] J. Swoger, J. Huisken, and E. H. K. Stelzer, “Multiple imaging axis microscopy improves
resolution for thick-sample applications,” Opt. Lett. 28, 1654–1656 (2003).
[70] J. Huisken and D. Y. R. Stainier, “Even fluorescence excitation by multidirec-
tional selective plane illumination microscopy (mSPIM),” Opt. Lett. 32, 2608–2610
(2007).
[71] M. A. A. Neil, R. Juskaitis, and T. Wilson, “Method of obtaining optical sectioning by
using structured light in a conventional microscope,” Opt. Lett. 22, 1905–1907 (1997).
[72] N. Bozinovic, C. Ventalon, T. Ford, and J. Mertz, “Fluorescence endomicroscopy with
structured illumination,” Opt. Express 16, 8016–8023 (2008).
[73] P. J. Keller, A. D. Schmidt, A. Santella, K. Khairy, Z. Bao, J. Wittbrodt, and E. H. K.
Stelzer, “Fast, high-contrast imaging of animal development with scanned light sheet-
based structured-illumination microscopy,” Nat. Methods 7, 637–642 (2010).

REFERENCES
195
[74] S. W. Hell and J. Wichmann, “Breaking the diffraction resolution limit by stimulated
emission’: stimulated-emission-depletion fluorescence microscopy,” Opt. Lett. 19, 780–
782 (1994).
[75] T. A. Klar, E. Engel, and S. W. Hell, “Breaking Abbe’s diffraction resolution limit in
fluorescence microscopy with stimulated emission depletion beams of various shapes,”
Phys. Rev. E 64, 066613 (2001).
[76] M. G. L. Gustafsson, “Nonlinear structured-illumination microscopy’: wide-field fluo-
rescence imaging with theoretically unlimited resolution,” Proc. Natl. Acad. Sci. 102,
13081–13086 (2005).
[77] R. Heintzmann and M. G. L. Gustafsson, “Subdiffraction resolution in continuous sam-
ples,” Nat. Photon. 3, 362–364 (2009).
[78] M. J. Rust, M. Bates, and X. Zhuang, “Sub-diffraction-limit imaging by stochastic optical
reconstruction microscopy (STORM),” Nat. Methods 3, 793–796 (2006).
[79] E. Betzig, G. H. Patterson, R. Sougrat, O. W. Lindwasser, S. Olenych, J. S. Bonifacino, M.
W. Davidson, J. Lippincott-Schwartz, and H. F. Hess, “Imaging intracellular fluorescent
proteins at nanometer resolution,” Science 313, 1642–1645 (2006).
[80] S. T. Hess, T. P. K. Girirajan, and M. D. Mason, “Ultra-high resolution imaging by
fluorescence photoactivation localization microscopy,” Biophys. J. 91, 4258–4272 (2006).
[81] S. Cox, E. Rosten, J. Monypenny, T. Jovanovic-Talisman, D. T. Burnette, J. Lippincott-
schwartz, G. E. Jones, and R. Heintzmann, “Bayesian localization microscopy reveals
nanoscale podosome dynamics,” Nat. Methods 9, 195–200 (2012).


6
CELL HANDLING, SORTING,
AND VIABILITY
Darwin Palima, Thomas Aabo, Andrew Ba˜nas,
and Jesper Gl¨uckstad
DTU Fotonik, Department of Photonics Engineering, Technical University of Denmark,
Kgs. Lyngby, Denmark
The significance of optics in the study of biological cells is highlighted by the fact that
the invention of the microscope in the 1700s enabled the discovery of the cell itself.
Another highlight occurred two centuries later when improvements in microscope
aberration-correction supported the development of cell theory, which established the
cell as a fundamental unit of life [1]. The enabling role of optics in biology continues
in contemporary biophotonics that no longer just uses light for basic imaging, but can
exploit optical energy to enable new microscopic modalities, trigger biomolecular
processes, or perform cellular microdissection and microsurgery. Moreover, the opti-
cal momentum has been effectively utilized for precision mechanical manipulation
using noncontact optical forces that arise when light exchanges momentum with the
material, for example, during light scattering, reflection, or refraction. These devel-
opments can help realize an all-optical “lab-on-a-microscope” concept [2], where
spatially sculpted light is used to assemble structures, perform optical fractiona-
tion, and control the functionality of light-driven pumps, valves, and mixers inside
microfluidic channels.
This chapter focuses on the applications of optical forces for creating optical
traps and micromanipulation systems for biological cells. The optical force can be
used to handle cells, for example, to measure their mechanical properties, keep them
held away from substrates and other surfaces to avoid perturbations, or to move and
deflect cellular populations having distinct characteristics for sorting in microfluidic
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
197

198
CELL HANDLING, SORTING, AND VIABILITY
environments. As we will see, light energy can be a concern in this case and care
must be exercised to avoid triggering unwanted parasitic effects that can potentially
interfere with cellular functions and affect cell viability.
6.1
HANDLING CELLS WITH LIGHT
Ashkin’s pioneering 1970 demonstration [3] on using light to exert mechanical forces
on microscopic objects was motivated by atom trapping and, for over a decade, optical
trapping studies used nonbiological objects. The report on the first optically trapped
atoms [4] even came out shortly before the first demonstration of optical handling
of cells where Ashkin et al. used visible light (Argon ion laser, 𝜆= 514.5 nm) to
trap and manipulate viruses and bacteria (tobacco mosaic virus and Escherichia coli)
[5]. Although they claimed trapping with no apparent damage, nevertheless, they
quickly followed up with a report using near-infrared (Nd:YAG laser, 𝜆= 1.06 μm)
for optical micromanipulation of yeast cells (Saccharomyces cerevisiae) and bacteria
(Escherichia coli), motivated by having improved optical traps with less optical
damage [6]. These reports used “optical tweezers”—at the time a newly developed
tool, originally intended for atom trapping [7], which can trap cells using a single beam
tightly focused by high numerical aperture microscope objectives. Optical tweezers
has since become the de facto tool for optical manipulation of cells. However, traps
using weakly focused counter-propagating beams can be preferable when the intensity
hotspots from sharply focused optical tweezers are problematic for cell viability. We
will return to the important issue of cell viability in more detail in Section 6.3 but,
first, we start by taking a quick review of the basic principles that enable light to
mechanically handle cells and proceed to survey various examples of optical cell
handling and optical sorting applications.
6.1.1
Light–Cell Interaction: Momentum Exchange
Light can exert force and induce mechanical effects on cells and other objects by
exchanging momentum with it. This momentum exchange depends both on the spatial
distribution of the illuminating light as well as the spatial distribution of the cell’s
refractive index and absorption. Light–matter interaction can lead to the exchange of
both linear and angular momentum, the latter leading to optical torque and rotational
dynamics [8].
Dielectric microspheres have been used since Ashkin’s pioneering experiments
[3], and they remain as preferred structures for optical trapping research, partly
owing to their practical accessibility when performing experiments as well as the
tractability of a dielectric microsphere model for theoretical analyses. This model is
relevant for cell handling, either as a simplified model of the cell itself or for the
actual modeling of trapped dielectric microspheres that are often used as handles for
indirectly manipulating cells with light.
A simplified ray optics model is shown in Figure 6.1, which illustrates the optical
force that arises when light interacts with a dielectric sphere, here manifesting as

HANDLING CELLS WITH LIGHT
199
Displacement
Optical force
P
PR
(a)
(b)
Laser beam
Objective
Bead
Fb
Fa
Fgrad
Fscat
Fscat
Fgrad
(c)
(d)
F
P
a
b
PT
2
PT
2
FIGURE 6.1
Ray optics model of optical force on a dielectric microsphere. (a) Illustra-
tion of a single incident ray—geometry, optical force, and force components Fgrad and Fscat.
(b) Illustration of a focused beam showing total Fgrad and Fscat. (c) Typical plot of optical force
versus displacement with a linear region for small displacements. (d) Elastic spring model for
small displacements with stiffness kx, ky, kz.
refraction at the interface. In this picture, the optical force balances the rate of
optical momentum change due to light deflections in accordance with the momentum
conservation principle. In the ray model (valid for larger objects, size ≫𝜆), the optical
force may be decomposed into components parallel and orthogonal to the incident ray,
which may be denoted as scattering force and gradient force, respectively [9]. Upon
summing the respective force components from all incident rays, we can get some
correspondence with the electromagnetic wave model that similarly decomposes the
optical force into scattering and gradient force components [10,11] (valid for Rayleigh
scattering regime [10] but can be extended in some cases [11] up to size ≤𝜆). In
this decomposition, the scattering force describes the force from the scattering of
wavevectors associated with the plane-wave components. The gradient force accounts
for interference between these plane waves that can create spatially nonuniform fields
that, in turn, exert nonzero force on induced dipoles in the trapped material directed
along the intensity gradient.

200
CELL HANDLING, SORTING, AND VIABILITY
When light having optical power, P, which is incident within a medium with
refractive index, nm, illuminates a cell, the optical force exerted by light on the cell
due to the interaction may be written as [9]
F = QnmP
c .
(6.1)
The quality factor, Q, is a measure of how well the system generates force, which
depends on how much it alters the incident light momentum flux, nmP∕c (where c∕nm
is the speed of light in the surrounding material)1. For example, perfect plane-wave
reflection at normal incidence corresponds to Q = 2, while total absorption leads to
Q = 1.
Achieving high quality factors is highly desirable, especially when handling cells,
to get the most force, while keeping modest input powers, and to avoid parasitic
effects from high intensities. For a beam incident in water (nm = 1.33), a quality
factor of Q = 1 can exert 4.44 pN for every 1 mW of incident power. For the trapped
microsphere illustrated in Figure 6.1, the optical force and quality factor depend on
the bead position (typically similar to the simplified version shown in Figure 6.1c).
For small displacements, the optical force can be modeled as an elastic spring obeying
Hooke’s law, F = −ks. The optical force decays with further displacements beyond
a characteristic displacement comparable to the bead radius2. For comparison, we
may take, as a point of reference, Ashkin’s estimates, based on ray optics model, that
found −0.276 < Qaxial < 0.490 for a dielectric sphere that is axially translated along
its equilibrium point (relative refractive index = nsphere∕nm = 1.2, e.g., polystyrene
in water, NA = 1.25, fully utilized, e.g., by overfilled objective illumination). The
quality factor falls down to −0.019 < Q < 0.147 when the numerical aperture is
lowered to NA ∼0.64 (now based on Gaussian apodization, e.g., with underfilled
objectives).
In contrast, Ganic et al. [12] used a more elaborate modeling based on a fully
vectorial diffraction approach to model the focused beam and its interaction with the
particle, with the possibility to account for the expected spherical aberration due to
the index mismatch between the immersion oil and the aqueous trapping environ-
ment. The volume field information obtained allows the calculation of the Maxwell
stress tensor, which forms the basis of the force calculation. A comparison of their
simulation and experimental results for the maximum trapping efficiency is shown in
Figure 6.2 with the size-independent ray-optics calculation indicated for comparison.
It is interesting to note that, although the ray-optics result is strictly valid only for
radius r ≫𝜆, the more sophisticated model gives a comparable result and exhibits a
1It may seem counterintuitive that light traveling nm times slower in a medium increases its momentum by
nm times, compared to vacuum, instead of decreasing by the slowdown factor. Actually, this momentum
dependence on the refractive index is the subject of the so-called Abraham–Minkowski controversy, with
Minkowski for the proportional relation and Abraham for the inverse relation. Interested readers can refer
to reviews on the subject (e.g., [126,127]).
2Hence, the peak force may be determined, using an escape force method, by applying a monotonically
increasing external force, for example, using precalibrated fluid drag, and then noting the escape point.

HANDLING CELLS WITH LIGHT
201
1
0.18
Experiment
Theory
0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0
10
20
30
40
Distance from cover glass (   m)
50
60
70
80
0.1
0.01
1
1E-3
0.1
0.01
1E-3
Max. ATE
Max. ATE
1E-4
1E-5
1E-6
0.01
0.1
Experimental data
Theory (with SA)
Theory (without SA)
Particle diameter (   m)
(b)
(a)
1
FIGURE 6.2
Peak axial trapping efficiency, ATE, of glass particles suspended in water,
illuminated by a laser beam (𝜆0 = 1.064 μm), focused by an oil immersion microscope
objective (NA = 1.3): (a) as a function of particle size; the effect of SA is considered at a
depth of 9 μm from the cover glass; the arrow indicates the ray-optics result; (b) as a function
of the distance from the cover glass for particle diameter D = 2.7 μm; the arrow indicates
the transverse trapping efficiency obtained for mammalian cells [16]. Plots adapted from
Reference 12.
very weak size dependence already starting at 2r ∼𝜆(with the observed r3 depen-
dence for r < 𝜆is expected from Rayleigh scattering model). These experimentally
determined values of trapping efficiencies are similar to those reported by others, who
also typically get lower Q than the theoretical calculations (e.g., Q = 0.026 ∼0.18 for
microspheres with radius r = 5 μm illuminated at NA = 1.3) [13]. Trapping through
a thick sample chamber can also degrade trapping efficiencies by an order of mag-
nitude, due to aberrations, as illustrated in Figure 6.2b. A similar degradation based
on spherical aberration is expected based on two-component analysis (scattering and
gradient force) when trapping subwavelength particles [14]. However, the low power
levels sufficient for trapping when using aberration correction [15] suggests that the
higher Q values can be revived even when trapping through turbid medium.
A generic setup for these experimental force measurements is shown in Figure 6.3.
Force calibrations require a sensitive position detector due to the position dependence
of the optical force. Also, it is crucial to properly measure the power at the trapping
region, for example, a dual-objective method [17] to account for the optics losses
when direct in situ power measurement is not practical.
Overfilling the objective can utilize the full numerical aperture and give higher
Q values. In practical terms, however, given the fixed available power in a laser
beam, overfilling reduces the incident power on the sample and, hence, underfilling
microscope objectives by an amount that depends on the size of the trapped particle
optimizes the optical force [18]. This also avoids the laser heating of overfilled
microscope objectives.
Single-beam optical tweezers use diffraction-limited trapping beams in the order
of the wavelength. Consequently, the trapping stiffness and trapping efficiency start to
exhibit only a weak dependence on particle sizes larger than the wavelength from the

202
CELL HANDLING, SORTING, AND VIABILITY
PSD
LL1
SL1
BL1-2
B1
L1
L2
L3
L5
L4
GMM1
DM1
DM2
OBJ1
T
PBSH
GMM2
L6
LA2
PZ1
LA1
C
FIGURE 6.3
Schematic illustration of typical components in an optical force measure-
ment system (adapted from Reference 19): LA1—Ar+-pumped Ti:Sapphire trapping laser;
LA2—HeNe position probe laser; B1—beam expander; L1–L6—assorted lenses; GMM1,
GMM2—computer-controlled gimbal mounted mirrors; DM1, DM2—dichroic mirrors;
PBSH—polarizing beam splitter; BL, BL2—block filters; OBJ1—high NA microscope objec-
tive; SL1—tailor-made sample slide; PZ1—piezo-controlled sample stage; C—oil immersion
condenser to collect the probe laser light; PSD—position-sensitive detector; LL1—laser line
filter. The system also includes a low noise preamplifier and a computer equipped for data
acquisition (not shown).
initial r3 dependence for subwavelength particles, as shown in Figure 6.2a. However,
it is actually the trapping beam size, rather than the wavelength, that determines
particle-size dependence. This is illustrated, for example, in Figure 6.4 which plots
the trapping stiffness measurements based on results reported by Wei et al. [20] using
dual-beam traps. In contrast to single-beam optical tweezers that require diffraction-
limited tightly focused beams for stable axial trapping, dual-beam traps can use
two counter-propagating trapping beams that are broader than the wavelength. Dual-
beam traps may be generated, for example, using diverging beams exiting from
two well-aligned fibers or as counter-propagating beams delivered using a pair of
low NA microscope objectives [21]. Figure 6.4 shows that for these broad beams, the
trapping stiffness still varies as ∼r2.6 for particles already several times larger than the
wavelength (but smaller than the trapping beams). The figure also plots the trapping
stiffness for Chinese Hamster ovary (CHO) cells. Although it achieves trapping
stiffness comparable to the polymer beads, the CHO cell result falls below the r2.6 fit
due to a lower trapping efficiency, which is the case for many biological cells.
6.1.2
Dielectric Tagging: Using Microspheres as Optical Handles
Dried cells can have refractive index comparable to polystyrene microspheres [22],
but living biological cells have high water content and so typically have lower

HANDLING CELLS WITH LIGHT
203
log kx = 2.58log(r) – 1.8535
log kz = 2.58log(r) – 2.4109
–1.5
–1
–0.5
0
0.5
1
1.5
0.2
0.4
0.6
0.8
1
1.2
1.4
Trap stiffness, log(k)
Particle radius, log(r)
FIGURE 6.4
Optical force constants for different sizes of silica particles. The slope of this
log(k) versus log(r) plot indicates that k varies as r2.58. Stiffness measurements for CHO cells
fall below the interpolated line. Plots are based on data from Reference 20. Optics express.
refractive indices closer to that of water (see some values in Table 6.1). Compared to
polystyrene, which has 1.572 refractive index at 1064 nm, it is clear that these cells
will generate less light deflections when trapped in aqueous media and, consequently,
lowers the achievable Q-values.
Although it can be useful to describe the mean optical parameters of cells, such
as refractive index, these cells are actually not homogeneous spheres; subcellular
structures such as the cell membrane, nucleus, and organelles, together with other
inhomogeneities, can add cell-dependent scattering effects [24] that can potentially
enhance the optical force. However, recalling the r3 dependence of Q in Figure 6.2, the
smallest structures cannot be expected to substantially alter the optical force. Detailed
analysis of the light scattering profile in light scattering spectroscopy, however, can
reveal details on the subcellular, subwavelength structures [25], which can have some
value for distinguishing between different cells subpopulations.
Experimental measurements on some mammalian cells yield lateral Q-values
∼0.03, as shown by the leftmost bars in Figure 6.5a. This is comparable to the exper-
imental values when trapping microspheres in the presence of aberrations (Fig. 6.2b).
TABLE 6.1
Refractive index of
different cell types
Cell type
Refractive index
Escherichia coli
1.387
Hemoglobin cytoplasm
1.387
Blood plasma
1.351
Mammalian cells
1.38–1.41
Fibroblasts
1.358–1.374
Source: Adapted from Reference 23.

204
CELL HANDLING, SORTING, AND VIABILITY
(c)
(a)
(b)
0.20
0.18
0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0
1
2
Number of spheres
Qlateral
Qlateral
5
6
7
0.20
CHO
RPE
HL60
C2GM
CHO
RPE
HL60
C2GM
0.18
0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0
1
2
Number of spheres
Number of spheres
5
6
0
0
5
10
15
20
25
30
35
1
2
3
4
5
7
Guiding velocity (   m s–1)
FIGURE 6.5
Effect of dielectric tagging. Lateral Q-values of different cell types containing
different amounts of (a) 2-μm and (b) 3-μm polymer microspheres. (c) Axial cell guiding data,
displaying maximum guiding velocities for cells containing different numbers of microspheres
per cell. Cells with no spheres displayed no axial guiding. CHO, Chinese Hamster ovary
cells; RPE, retinal pigment epithelial cells; human, promyelocytic leukemia cells; C2GM,
hematopoietic stem-cell-like cells. Adapted from Reference 16. (For a color version of this
figure, see the color plate section.)
The low Q-values achieved when optically handling cells can present an obstacle for
generating sufficient force using only minimal, biologically safe power levels. There
are many dyes available for staining cells to produce clearer microscope images of
otherwise mostly transparent cells, but increasing cellular absorption defeats the pur-
pose of using lower intensities in the first place (i.e., to minimize energy absorption
within the cell). On the other hand, the Kramers–Kronig relations imply that the
enhanced absorption in stained cells is accompanied by changes in the refractive
index [22]. It would be interesting to find out whether illuminating at off-resonance
wavelengths can achieve desired refractive index change at acceptable absorption
levels that maintain cell viability (assuming we have available dyes that are suitable
for live-cell staining).
Without staining, we can improve the handle we have on the cell by using suitable
structures that can be strongly trapped to serve as handles for the cell. Inspired

HANDLING CELLS WITH LIGHT
205
by intracellular tagging techniques, a so-called intracellular dielectric tagging was
demonstrated to improve the optical handle on mammalian cells [16]. Using the
cells’ ability for phagocytosis—a cellular process for ingesting foreign solid matter
(optimal uptake after 24 hours incubation [16])—it was shown that the ingested
polymer microspheres can serve as intracellular handles, which leads to improved
optical manipulation of the cell. The resulting improvement in the lateral Q-values,
using different number of ingested microspheres, for different mammalian cells is
shown in Figure 6.5a and b. The longitudinal effect, measured in terms of the velocity
improvement when axially pushing the cell, is shown in Figure 6.5c.
Instead of trapping beads inside the cell, an alternative that does not require
incubation is to trap dielectric microbeads outside the cell and use them as “opti-
cal grippers” to grab the cell, in analogy to robotic grippers in mechatronics sys-
tems. An example of this indirect manipulation is presented in Figure 6.6. Besides
enhancing the optical force, the use of extracellular optical handles also shifts the
high intensity of the sharply focused trapping beams away from the cell, which
helps maintain its viability during manipulation (in contrast, the directly trapped cell
died after 12 s of optical manipulation). Aside from cells, the indirect manipulation
using dielectric microspheres has also been extensively applied for manipulating
A
A
A
A
(a)
(b)
(c)
(d)
B
B
B
B
FIGURE 6.6
Indirect cell handling using trapped microspheres increases the optical force
and minimizes harmful radiation on directly trapped cell. (a) Solution of Dictyostelium dis-
coideum cell and inert silica microspheres; (b) t = 0 s: one cell, A, is trapped directly, while
another cell, B, is trapped indirectly; (c) t = 12 s: the cells are being transported; (d) t = 15 s:
the cells are released—the directly trapped cell, A, is dead; the indirectly manipulated cell, B,
is still alive. Adapted from Reference 30. IEEE Conference.

206
CELL HANDLING, SORTING, AND VIABILITY
tethered biomolecules like nucleic acids and proteins. This indirect approach is sur-
veyed in a recent work by Banerjee et al. [26]. Using properly calibrated precision
trapping systems—with Angstrom displacement resolutions and 0.1–100 pN force
range—tethering biomolecules to trapped microspheres has been highly successful
for investigating single-molecule biophysics to reveal the mechanical properties of
individual biomolecules, as reviewed by Svoboda and Block [27] and recently by
Perkins [28]. The force dynamic range has recently been extended to the nanonewton
regime using antireflection-coated, high refractive index microspheres [29], which
should open more possibilities previously available only using AFM-based manipu-
lation. For the remainder of this chapter, we focus on cell-level applications of optical
forces; readers interested on single-molecule biophysics applications are referred to
the abovementioned reviews.
6.1.3
Cell Positioning
One of the more direct applications of optical forces is to hold cells fixed at a des-
ignated position. Cells can be moved using beam modulation techniques to shift
the location of the trapping beam, provided the optical force is sufficient to move
the cell against the drag force and follow the beam. Multiple cells can be simulta-
neously handled, for example, by scanning to create time-share a single beam, or
creating multiple dynamic traps by holography or generalized phase contrast [31].
The noncontact nature of optical forces enables reaching into sterile microbiologi-
cal environments, including sealed ones, and doing mechanical adjustments without
risking contamination and without the space/overcrowding problems of contact-based
approaches such as micropipettes and AFM. Indeed, optical traps can even manip-
ulate intracellular components without puncturing the cell. Optical traps, created,
repositioned, or removed on demand, offer a higher degree of specificity in terms of
particle targeting and positioning and more versatile 3D control compared to other
field-type approaches based on hydrodynamic, electric, and/or magnetic fields. It is
also a practical advantage that the optical trapping systems are typically implemented
in the same microscope that is already being used to study the cells. However, optical
traps do have some weaknesses, and so hybrid approaches combining the strengths
of the different techniques sounds appealing, subject to overcoming some technical
implementation issues. We look at some applications of optical cell positioning below.
6.1.3.1
Single-Cell Analysis: Microscopy and Microspectroscopy
One of the
motivations for handling single cells is to perform fundamental tests that are capable
of resolving variability between individual cell behaviors, as opposed to bulk testing
whose population-averaged results is affected by the level of cellular heterogeneity
and of little value when the interest is in rare subpopulations (e.g., circulating tumor
cells). Performing these single-cell studies require tools for monitoring cells while
they are held by optical traps for sufficient time intervals to complete testing. Various
microscopic imaging modalities can reveal morphological information, which can
indicate ongoing cellular processes or help determine the cell’s mechanical proper-
ties using observed deformations. Optical trapping systems have been combined with

HANDLING CELLS WITH LIGHT
207
different imaging modalities such as brightfield imaging, phase contrast, and digital
holographic microscopy [32], which can also retrieve quantitative phase informa-
tion [33,34]. Optical trapping systems have also been combined with spectroscopic
imaging, some of which will be discussed below.
The need for sensitive monitoring of trapped cells may be addressed by integrat-
ing various instrumentation for realizing environmental controls and sensors, where
the specific instrumentation depends on the cells under study. One such Biophotonics
workstation is schematically depicted in Figure 6.7. This system integrates the capac-
ity for simultaneous optical manipulation, heat stress, fluorescence, and intracellular
CCD
Tube lens
Lens wheel
Filter wheel
Excitation
Dichroic/filter
Fan/heatsink
Peltier
element
Heating stage
Bottom
obj
White light
Spatial light
modulator
Top
obj
FIGURE 6.7
BioPhotonics workstation equipped with fluorescence system and temperature
control [35]. RSI. The heating stage lies between the opposing objectives that deliver counter-
propagating beam traps. Fluorescence is imaged by the top objective through the dichroic mirror
and filters to the CCD camera. Brightfield imaging uses white light illumination through the
bottom objective.

208
CELL HANDLING, SORTING, AND VIABILITY
pH measurements [35]. Its multifunctionality can be utilized, for example, in yeast
cell studies where the measured fluorescence enables the calculation of intracellular
pH distribution, which turns out to be a good indicator of yeast cell health [36].
As mentioned previously, subcellular structures influence the scattering properties
of the cell. Hence, analysis of the scattered trapping beam itself can yield infor-
mation on the internal dynamics of trapped cell, for example, to monitor cellular
processes [37]. Combining spectroscopy with optical trapping can achieve chemical
imaging, for example, by tagging the cells with fluorescent labels, or using label-free
approaches like Raman [38] or coherent Raman spectroscopy [21]. The Raman spectra
can be excited using the same laser used for trapping [39]. Early biological experi-
ments using this technique, dubbed as Laser tweezers Raman spectroscopy (LTRS),
showed it had sufficient sensitivity to reproduce the characteristic bands of red blood
cells and distinguish physiological states of yeast cells (alive vs. dead) that other-
wise appear visually similar without staining. Photodamage can be minimized over
sustained operation by switching between low power trapping-only mode and higher
power trapping-with-excitation mode (e.g., 2 mW and 20 mW, respectively [39]).
6.1.3.2
Cellular Microenvironment: Nonbiological Stimuli
Together with cell
monitoring systems, having auxiliary systems for controlling the cellular micro-
environment that work simultaneously with optical traps broadens the arena for
studying single-cell behavior. In well-calibrated systems, the trap itself can work
as a force monitor, for example in characterizing cell motility. Auxiliary controls
can be used either to maintain temperature and avoid temperature-dependent effects
or to vary temperature and explicitly study its effects. Peltier elements can be used
with trapping systems utilizing long working distance objectives due to the available
working space. In the tight working conditions of high NA tweezers, options for tem-
perature control include introducing another beam with different wavelength for laser
heating or pumping/circulation of temperature-controlled fluid into the system [40].
Aside from temperature control, microfluidics also provides the possibility for intro-
ducing new cells and varying the chemical microenvironment of the trapped cell [41].
One application of environmental control is in real-time control of cellular growth
conditions. One area of interest is in finding optimal growth condition for producing
lipid in algae since such lipids can be readily converted to biodiesel. In an effort to
establish correlations among lipid trigger conditions, growth rate, and lipid production
in algae, Wu et al. used LTRS, illustrated in Figure 6.8, to quantitatively investigate the
lipid content of trapped single cells from different algal species and different growth
conditions including light exposure, nitrogen, silicon, CO2, pH, and temperature [42].
Differences in temporal response of individual cells, such as lags in the onset of
cellular response, can cause averaged bulk measurements that paint a different picture
of the cell population. Using optical traps to hold cells can keep them in the observa-
tion zone for monitoring temporal effects after their chemical microenvironment is
modified, for example, by pumping fluids containing different reagents. Mortitz et al.
used LTRS to trap and monitor the metabolic states of E. coli as different antibiotics
are pumped into the sample chamber [43]. Singh et al. used Raman microspectroscopy
to detect real-time stress response of yeast cells to hyperosmotic environments (i.e.,

HANDLING CELLS WITH LIGHT
209
Long-pass
5 μm
1
2
785 nm
(a)
(b)
3
Pinhole
Grating
CCD
FIGURE 6.8
Setup for single-cell laser tweezers Raman spectroscopy (LTRS). Adapted
from Reference 42. PNAS. (a) Brightfield images of individual trapped microalgal cells: 1,
Neochloris oleoabundans; 2, Botryococcus braunii; and 3, Chlamydomonas reinhardtii. (b)
Instrument layout. The 785-nm laser beam is used as both trap beam and Raman excitation
beam. (For a color version of this figure, see the color plate section.)
fluids with higher solute content) [44]. In this case, the Raman signatures enable them
to monitor the glycerol and ethanol production in the yeast cells as they switch to
fermentation mode as a stress response for balancing the osmotic pressure when in
glucose-rich environment.
Besides biochemical stimuli, studies show that cells also react to mechanical and
geometric cues so that their mechanical environment also plays a role in shaping cellu-
lar development [45]. Thus, just as control over the chemical environment is important
for understanding cell biology, so too is having reconfigurable mechanical microenvi-
ronments. Combining optical manipulation with three-dimensional microfabrication,
for example, as multiphoton polymerization, provides a way to create such recon-
figurable mechanical microenvironments, where optical forces are used to rearrange
prefabricated microstructures [46]. With chemical functionalization becoming avail-
able for these microstructures [47,48] including selective 3D functionalization [49],
creating reconfigurable synthetic microenvironments having chemical, mechanical,
and even optical cues [50], could be on the horizon.
6.1.3.3
Cellular Microenvironment: Intercellular Interaction
Synthetic micro-
environments with chemical and mechanical cues, reconfigured using optical traps,

210
CELL HANDLING, SORTING, AND VIABILITY
(a)
(b)
(c)
(d)
FIGURE 6.9
Snapshots during the preparation of Hanseniaspora uvarum and Saccha-
romyces cerevisiae cells for microscale, mixed-culture fermentation using an interactive opti-
cal trapping system. Adapted from Reference 51. FEMS Microbiology Letters. Dynamic traps
simultaneously bring S. cerevisiae cells along the indicated paths to surround a single H.
uvarum cell. Images were recorded at: (a) 0 s; (b) 30 s; (c) 60 s; and (d) 90 s. Scale bar
represents 10 μm.
would make interesting tools for single-cell microbiology. However, the trapped
cells themselves could already serve as building blocks, naturally equipped with
mechanical and biochemical cues, for constructing reconfigurable microenviron-
ments. This setup for optically assembling cells provides a way of investigating
intercellular communication. Using an interactive optical trapping system, Arneborg
et al. demonstrated that, in a mixed yeast culture, confining a yeast cell from one
species (Hanseniaspora uvarum) in a cage composed of yeast cells belonging another
species (S. cerevisiae) arrests cell division in the confined cell [51] (see Fig. 6.9).
The confined cell remains viable and resumes its cell division processes after being
released from confinement.
Intercellular signaling in bacterial populations can lead to a so-called quorum sens-
ing behavior where bacterial processes get synchronized. This synchronization and
coordinated population activity in bacteria can lead to public health concerns, such as

HANDLING CELLS WITH LIGHT
211
in biofilm production and antibiotic resistance [52]. One proposed use of optical traps
in quorum-sensing studies is in helping create biofilm models using microfluidics to
deliver bacteria to a construction region where optical traps work to precisely position
cells and subsequent photopolymerized hydrogel encapsulation is used to mimic the
extracellular matrix (ECM) [53]. A similar procedure combining microfluidic cell
delivery, optical assembly, and hydrogel encapsulation has also been proposed as
a possible tool for creating cellular micropatterns resembling synthetic tissue [54].
These demonstrations illustrate the promising prospects of applying optical microma-
nipulation, working in synergy with microfluidics and optical materials processing,
for fundamental cellular studies. However, creating artificial microenvironments that
sufficiently mimic the chemical and physical properties of the cellular environment,
especially its time-varying features, can be challenging. On the other hand, new capa-
bilities are constantly being developed, such as optically reconfigurable microenvi-
ronments, optically addressable functionalization, optically manipulated sources of
chemical [55], mechanical [56], and optical stimulation [50], that, together with 3D
microfabrication of ECM and other developments in microtechnology [57], serve to
widen the cellular biology toolbox [2].
6.1.4
Cellular Biomechanics: Mechanotransduction and Cell Deformations
Cells interact mechanically with other surrounding cells and extracellular material
and, hence, it can be expected that the mechanical properties of cells play an important
role in their function and development. The cell can work as a force transducer in a
process called mechanotransduction, where mechanical inputs on the cell membrane
are converted to biochemical signals within the cell. Cells in vivo constantly react to
the microenvironment and act in an integrated manner depending upon the surround-
ing mechanical and spatial context and their intrinsic mechanosensory mechanisms
[58]. Mechanotransduction responds to piconewton forces on the cell membrane,
which is within the usual range of optical forces.
One interesting area in mechanotransduction is the study of cellular adhesion to the
ECM, which governs processes like cell migration, for example, during cellular dif-
ferentiation, immune response, or cancer cell spreading. Optical traps have been used
in mechanotransduction studies to hold beads coated with ECM proteins and anti-
bodies, which are then delivered onto cell surface adhesion receptors to provide both
biochemical cues and calibrated mechanical stimulation [59, 60]. The cells exhibit
different responses to the biochemicals depending on the accompanying mechanical
context, highlighting that cells are able to distinguish free chemicals to those that are
attached to a rigid ECM [59]. Understanding mechanotransduction can benefit from
fluorescent tags to image the signaling pathway from the cell surface receptors to the
interior. However, designing fluorescent tags requires a priori knowledge on what it
is that should be tagged. Felsenfeld et al. [60] used optically trapped coated beads
in mechanotransduction studies that identified the role of the tyrosine kinase Src in
regulating cellular response to mechanical stimuli. Based on this, Wang et al. [61]
were able to design a highly selective, genetically encode Src reporter, which they
then used to image and quantify the spatiotemporal activation of Src upon mechanical

212
CELL HANDLING, SORTING, AND VIABILITY
stimulation by optically pulling fibronectin-coated beads that are attached to cell sur-
face receptors. Among others, they saw the presence of both an immediate distal
Src activation and a slower wave propagating at ∼18 nm s−1 when local mechanical
stimulation is applied to human umbilical vein endothelial cells.
In another experiment, Resnick showed that a trapped microsphere exerting force
not exceeding 200 pN directly on the primary cilium of kidney epithelial cells elicits
the release of Ca2+ ions inside the cell [62]. This reproduces previous results that
stimulated intracellular Ca2+ release by flow-induced stimulation of the cilium [63].
As opposed to fluids that can apply shear forces over the entire exposed cell mem-
brane, Resnick’s experiment produced the Ca2+ signal using localized mechanical
excitation. However, since smaller objects are harder to manipulate (recall that the
Q-value decreases as r3 in the Rayleigh regime), it is envisioned that combining
microfabrication and optical manipulation be used in a structure-mediated approach
for highly localized stimulation [64]. In this approach, nanotips—intended for space-
and/or time-programmed stimulation—can form part of relatively bigger structures
that are not only easy to optically manipulate, but are also capable of exerting stronger
forces due to their larger Q-values compared to direct nanotip manipulation. A similar
structure-mediated approach has already been used to apply controlled forces on the
cell membrane for topographic surface mapping [65]. Extending this approach for
the stimulation of highly localized receptors on the cell membrane can help probe
mechanotransduction and other cellular mechanisms.
Cell mechanics is not only concerned with how mechanical stimuli affect cellular
processes, but also on what the cell’s mechanical properties can reveal about the
cellular physiology. The physical properties of biomaterials play a role in biological
processes at different scales extending from biomolecules to organs. At the cell level,
the view that deformation and other mechanical properties of cells can give insights on
possible connections with cellular process such as cell differentiation, cell health, and
disease progression is driving multidisciplinary research initiatives at the interface
between physical science and biology3. Different mechanical models of the cell have
been developed [66] based on the different experimental techniques that were used
for the mechanical measurements.
Two examples of using optical traps for investigating the viscoelastic properties of
cells are illustrated in Figure 6.10. The procedure by Mills et al. [67] is illustrated in
Figures 6.10a and 6.10b, where a red blood cell is stretched by attaching beads to the
cell, pulling on the bead, and then analyzing the cell deformations. The out-of-focus
bead on the right is held in place by a calibrated optical trap as the in-focus bead on
the left, fixed to the bottom of the chamber is displaced using a translation stage.
Figure 6.10c illustrates the so-called optical stretcher by Guck et al., which uses
counter-propagating beams to directly trap and stretch the cell. The optical stretcher
mechanism can be intuitively understood in terms of the refractive index dependence
of the optical momentum flux, nmP∕c, where nm is the refractive index of the medium
3For example, the U.S. National Cancer Institute runs a program “Physical Sciences in Oncology” aimed
at building cross-disciplinary teams working at the convergence between cancer biology and physical
science/engineering.

HANDLING CELLS WITH LIGHT
213
(a)
(c)
(b)
Transverse diameter
Projected axial diameter
Microbeads fixed to slide and
moves with microscope stage
Silica microbeads
Microbead held in optical trap
Stretched cell
Trapped cell
Laser light
(785 nm, up to 800 mW)
Optical fiber
Optical fiber
0.1 – 0.3 mm
FIGURE 6.10
Optical cell stretching. (a, b) Cell stretching using attached microbeads.
Adapted from Reference 67. Two 4.12-μm diameter silica microbeads are nonspecifically
attached to the red cell at diametrically opposite points. (a) The left bead is anchored to the
surface of the glass slide. (b) The right bead is trapped using the optical tweezers while the
slide and attached left bead are moved to stretch the cell. The image shows deformation at
193 pN of force. (c) Optical stretcher: counter-propagating beams trap and stretch the cell.
Adapted from Reference 68. Mechanics and chemistry biosystems. Not drawn to scale; optical
fiber diameter is 125 ± 5 μm.
where light propagates. Light incident in a medium having lower refractive index
than the cell increases its optical momentum flux upon entering the cell and lose
momentum upon exit. The forces resulting from these momenta changes act on
the cell wall and can lead to cellular deformation. In a simplified calculation for
a light ray of power P at normal incidence, and neglecting any losses inside the
cell, the momentum conservation on the front cell boundary predicts a force pulling
opposite to the light propagation. Including the effects from partial reflection, R,
yields a Q-value, Qfront = [n + Rn −(1 −R)](1 −R). The reflection R depends on the
refractive index. A similar analysis on the exit side predicts a force pushing along
the light propagation with Qback = [n + Rn −(1 −R)](1 −R). These forces combine
to stretch the cell. The unequal forces (Qback > Qfront) yield a net force along the
light propagation, and so an opposing beam is needed to produce a stable trap, which

214
CELL HANDLING, SORTING, AND VIABILITY
also doubles the stretching forces. Cell stretching forces reaching up to hundreds of
piconewtons can be generated using 800 mW from each beam.
The optical stretcher can form part of a cell measurement system for possible
clinical applications. Combining the optical stretcher with microfluidics for conveying
multiple cells that are measured individually, promising results have been obtained in
differentiating between the mechanical properties of healthy cells and cancer cells:
for small deformations, cancer cells, on the average, are softer than healthy ones [69].
With cellular heterogeneity, the analysis cannot be based on a single-cell measurement
alone but on the statistical properties of individually measured cells.
6.1.5
Cytometry
The optical stretcher approach for studying disease finds support in the many clin-
ical applications of flow cytometry, for example, see review by Jaye et al. [70],
which highlights the fact that the statistical properties of cellular populations taken
from patients can form the basis for routine diagnosis, prognosis, and monitoring.
Cytometry deals with the measurement of cell properties, and in flow cytometry, this
measurement is done by streaming a single file of cells past an optical interroga-
tion region where cells are individually illuminated, typically by lasers, as shown in
Figure 6.11. Hydrodynamic focusing of a core stream (green) by sheath fluid (gray)
creates a single file of cells in an interrogation region (yellow sphere) where cells are
individually illuminated by a laser beam. Each cell creates its characteristic forward
scatter (FSC), orthogonal scatter (SSC) intensity, and possibly fluorescence (FL1)
signals. The flow cytometer illustrated in Figure 6.11 is not only capable of measur-
ing the cell properties using the FSC, SSC, and FL1 signals but can also use these
measurements as a basis for sorting cells (e.g., fluorescence-activated cell sorting,
FACS [71]). For cell sorting, droplets created around single cells are charged posi-
tive or negative depending on the measured parameters. Droplets are then deflected
by electric field and sorted, according to their charge, into collection tubes. Read-
ers interested on flow cytometry are referred to Shapiro’s book [72], considered a
standard reference on the subject.
Conventional flow cytometry equipment tends to be large, bulky, and expensive,
especially when incorporating multiple excitation lasers to enable the measurement
of many different cell parameters4. Thus, there are efforts to develop low cost and
compact alternatives, for example, the μFACS system illustrated in Figure 6.11b uses
a microfabricated disposable flow chamber in a fluorescence microscope [73]. The
microfluidic chamber constricts as it approaches a T-junction enabling to flow one
cell at a time (see Fig. 6.11b inset). The control system diverts the flow based on the
detected fluorescence to sort cells between two collection channels.
The microfabricated fluorescence-activated cell sorter described above [73] is one
of the early attempts at cost-effective cell cytometry and sorting systems. Its high
citation indicates a growing interest in alternative approaches and many microfluidics-
based systems are being pursued. In the next section, we discuss a number of optical
4Desktop cytometers are now starting to become commercially available.

OPTICAL SORTING
215
105
1.12
104
104
105
103
103
0
0
<APC-CY7-A>: CD4
<PE-A>: FOXP3
Charged
metal
plates
Singlet
cells
Sorted cell
droplets
High voltage
amplifiers
Computer
Preamplifier
Pt electrodes
Photomultiplier tube
Beam splitter
VIDEO
Dichroic
CCD camera
Focusing optics
Coherent innova 
Ar laser 488 nm
60X, 1.4 N.A.
oil immersion lens
Microfabricated 
cell-sorting
device
(a)
(b)
SSC
FSC
FL1
Excitation
beam
FIGURE 6.11
(a) Schematic of a conventional flow cytometer (including FACS) that moni-
tors fluorescence, forward and side scattering (inset shows sample data where FOXP3+ CD4+
regulatory T-cells are identified from a mixture of lymphocytes). Adapted from Reference 70.
(b) Cytometer using a disposable microfabricated flow chamber (inset) in a fluorescence micro-
scope with a photomultiplier tube for FACS (μFACS). Adapted from Reference 73. Journal of
immunology. (For a color version of this figure, see the color plate section.)
methods for sorting that are being proposed, which nicely complements the many
optical tools developed for measuring cell properties (optical cytometry) that are not
necessarily limited to optical scattering and fluorescence.
6.2
OPTICAL SORTING
Measuring single-cell properties by cytometry can generate useful statistics of a
heterogeneous cell population. The capacity for cell sorting is necessary when it is

216
CELL HANDLING, SORTING, AND VIABILITY
not enough to just know the population statistics, and therefore, subsequent processing
and biological studies need to be performed on a selected subpopulation. The ability
to sort cells is of interest to commercial markets such as stem cells, production cells,
and circulating tumor cells (CTCs). This market that has attracted a lot of attention
from different technologies and implementations of cells sorters has been proven
with varying success using methods such as magnetics, electrophoresis, acoustics,
and surface affinity, to name a few. Bhagat et al. [74] did an extensive review of the
various technologies comparing the methods, mechanisms, separation markers, and
flow rate/throughput.
Optical techniques already form part of conventional cell sorters in the form of
scattering and fluorescence detection, which is used as basis for separating cells using
different mechanisms. In this section, we focus on techniques that use optical forces
as the cell separation mechanism itself. Optical cell sorting systems can be divided
into two main categories: active sorting (open-loop systems) and passive sorting
(closed-loop systems). Active optical sorting uses a beam control system for dynamic
illumination to selectively exert optical force depending on the measured properties
of the cells. On the other hand, passive optical sorting uses static illumination and
relies on the dependence of the force on the cells’ intrinsic optical properties as basis
for automatically separating cells.
6.2.1
Active Sorting
One potential application of optical forces in cell sorting is to provide an alternative
optical flow switching mechanism in microfluidic implementations of fluorescence-
activated cell sorting (so-called μFACS systems similar to the illustration in
Fig. 6.11b). Wang et al. [75] successfully demonstrated this possibility. They
employed hydrodynamic focusing to individually stream mammalian cells into a
fluorescence analysis region. Unwanted HeLa cells do not fluoresce and simply flow
directly to the waste channel; on the other hand, fluorescence from GFP-expressing
target HeLa cells initiates optical switching downstream using a laser beam that is
dynamically switched in and out of a specified point by an acoustooptic modulator
(AOM). Static illumination for 2–4 ms exerts transverse force to laterally divert the
cell toward the collection channel. As for the recovery rate, more than 85% of flu-
orescent cells were collected with 97% purity for flow throughputs up to 77 cells
per second. A similar successful demonstration by Perroud et al. [76] used infected
macrophage cells to highlight the lower risks associated with confined microflows as
opposed to the potential biohazard of free atomized droplets in conventional FACS. A
recent work on laser- and microfludics-based sorting used pulsed lasers for creating
microbubbles on demand to switch the flow and collect desired cells based on their
fluorescence signature [77]. This related method, based on laser-induced cavitation
instead of optical forces, is able to sort 1500 mammalian cells per second with 90%
purity; trading-off purity for higher throughput, it can sort up to 20,000 cells per
second with 37% purity.
In their critical review of the role of holographic optical tweezers in lab-on-chip
devices [78], Padgett and Di Leonardo assert that incorporating optical manipulation

OPTICAL SORTING
217
to lab-on-chip devices would only add modest additional complexity if the microen-
vironment would already require imaging. Although chemical contrast with fluores-
cence detection has long replaced cellular morphology as the cell sorting criterion,
conventional FACS integrates fluorescence from the whole cell and, hence, lacks
morphological information. To address this gap, imaging cytometers offering mor-
phological details are now commercially available. However, these machines are built
upon conventional cytometers and still tend to be large, bulky, and expensive. Hence,
there is sustained search for breakthroughs in so-called disruptive technologies that
can offer competitive performance using simple, low cost innovative solutions. One
focus is in imaging-based cell-sorting alternatives, which can benefit from parallel
developments in high-speed imaging and high-speed image analysis.
Recent work by Wang et al. [79] and Landenberger et al. [80] using optical
forces for sorting cells in microfluidics systems incorporate possibilities for both
fluorescence-based as well as image-based sorting criteria. In contrast to previous
optical switch based on static illumination, these works use a dynamic trap that
can be arbitrarily positioned within a given field of view. Landenberger et al. used
the brightfield images to position traps on the edge of target cells. This minimizes
illumination within the cell bulk and optimizes the transverse force as the beam is
displaced by galvanometric mirrors to drag selected cells from the hydrodynamically
focused flow to the section of laminar flow that eventually proceeds to the collection
channel. Wang et al., on the other hand, simplified the microfluidic requirements
by doing away with hydrodynamic focusing altogether (but retaining a buffer flow
to the collection channel). They used holographic optical traps that can arbitrarily
position multiple traps for parallel addressing within a field of view where cells are
distributed. Target cells are trapped and displaced into the buffer flow for collection
(See Fig. 6.12). To avoid using tightly focused beams, one can instead use relatively
broad multiple dynamic beams to axially propel chosen cells from an initial flow
headed toward a waste channel to a buffer flow for collection (see Fig. 6.12b). In
proof-of-principle experiments, Perch-Nielsen et al. used axial optical propulsion to
(a)
(b)
Inlet
CCD
Cell mixture
Detected
Ignored
Matched filter
Objective
Outlet
(waste)
Outlet
(collection)
Inlet
: Non-target cell
: Target cell
: Optical trap
Optical tweezers
Sample flow
Butter flow
ROI
Trap
direction
FIGURE 6.12
Image-based parallel cell identification and optical sorting. (a) Transverse
force used for lateral displacements. Adapted from Reference 79). (b) Axial force used for
optical propulsion. Adapted from Reference 82). (For a color version of this figure, see the
color plate section.)

218
CELL HANDLING, SORTING, AND VIABILITY
move slower particles at the bottom of the chamber to the faster moving flow in the
parabolic flow profile within the chamber [81].
Conventional FACS can perform cell sorting at cell throughputs in tens of thou-
sands per second and optical sorting throughputs are lagging way behind. Hence,
one possible direction for optical sorting is to find niche applications that set lower
premiums on throughputs, for example in samples where only limited volumes are
available. Nonetheless, it is hoped that parallel approaches can help raise achievable
cellular throughputs. Parallel cell sorting is also possible using passive optical sorting
methods, as will be described in the following section.
6.2.2
Passive Sorting
Although active sorting relies on an allied system for measuring cell properties to
serve as the sorting criteria to guide the targeting behavior of a switching beam,
passive sorting does not specifically target cells and can even use static illumination
to automatically sort cells. The cells separate when intrinsic cellular properties result
in differences in the optical force experienced by the cell. The benefits from the
relative simplicity of automatic passive sorting is hampered, on the other hand, by
its limited sorting applications since not all desired sorting criteria would directly
translate to differences in optical trapping response.
One example of passive optical sorting is optical chromatography [83], which can
simply use a weakly focused beam to illuminate opposite the flow in a microchannel.
With the axial optical force depending on particle size, the competition between the
opposing optical and drag forces leads to axial particle separation. A sample microflu-
idic design for cell sorting by optical chromatography is illustrated in Figure 6.13a
[84]. In essence, optical chromatography separates particles by exploiting the size-
dependent optical force in a geometry characterized by coaxial (counter-propagating)
flow and laser illumination. We can expect this size-dependent effect to similarly man-
ifest when using an orthogonal geometry, as illustrated in Figure 6.13b [85]. Here, a
flow containing particles with different sizes is hydrodynamically focused to create
a single line of particles. Focusing light orthogonally in this region leads to size-
dependent separation as larger particles tend to get pushed farther. In Figure 6.13b,
this effect works to enhance microfluidic pinched flow, which would otherwise also
lead to similar size-based particle separation even without orthogonal illumination.
Since the optical force on a particle depends on its geometry, refractive index,
and other light scattering properties, a given static structured illumination can be
experienced as different optical landscapes by different particles. Thus, particles
in a flow that encounter this structured illumination can be deflected in different
ways and can lead to particle separation downstream in a process coined as optical
fractionation. This fractionation has been demonstrated using optical lattices (periodic
light patterns), but can also work using some other patterns that are specifically
tailored and optimized by experimental calibration to achieve the desired separation
of the expected particles in the sample. Examples of optical fractionation using
a periodic optical lattice [86] and a nonlattice-like illumination pattern [87] for
separating polystyrene beads according to size are illustrated in Figure 6.14.

OPTICAL SORTING
219
Laser
Laser
(a)
(b)
Flow
Flow
Laser
(c)
FIGURE 6.13
Coaxial and orthogonal geometries of size-dependent scattering force for
automatic particle separation: (a, b) Coaxial (counterpropagating) geometry showing microflu-
idic device and the weakly focused laser beam in optical chromatography for particle separation
near the focus of the beam. Adapted from Reference 84. (b) Orthogonal geometry between
illumination and hydrodynamically focused flow leads to size-dependent particle separation
downstream (in this case enhanced by the expanding chamber). Adapted from Reference 85.
Many optical sorting techniques are successfully demonstrated using dielectric
beads. However, it can be challenging to realize similar sorting performance when
using biological cells since their low-Q can require much higher power exposures
that can affect cellular viability. Achieving optical sorting with good cellular viability
is important in finding niche applications, given that the high throughputs in conven-
tional FACS is achieved at pressures and flow rates that subject cells to mechanical
stress levels that may affect viability of fragile cells. We examine important issue of
cell viability in performing optical cell handling and sorting in the following section.

220
CELL HANDLING, SORTING, AND VIABILITY
(a)
(b)
Optical lattice
Flow
Flow
5 μm
6.84 μm
5.17 μm
3.0 μm
2.3 μm
FIGURE 6.14
Optical fractionation in an optical landscape. (a) Periodic landscape: a larger
particle flows with minimal deflection through the optical lattice while smaller particles are
angularly deflected. Adapted from Reference 2. (b) Experimentally calibrated pattern generated
using an acousto-optic modulator: Particles flowing from the right are guided to the bottom
of the figure by the line trap and eventually separate based on size as the calibrated gaps and
intensities of the subsequent pattern are only able to drag the largest particles to the top of the
figure. Adapted from Reference 87. Nature Materials, Optics letters. (For a color version of
this figure, see the color plate section.)
6.3
CELL VIABILITY
Studying the viability of microorganisms can be very complex and goes beyond the
simple dead-or-alive determination. This is reflected in the rich vocabulary describ-
ing the possible states of the organism that expands as new characterization methods
are found. These present terms include dead, moribund, starved, dormant, resting,
quiescent, viable but nonculturable, injured, sublethally damaged, inhibited, resus-
citable, living, active, and vital [88]. The multitude of terms stresses the fact that care
should be taken to avoid confusion, when describing the state of the cell. A profound
description and discussion of the terms was performed by Kell et al. [89].
The use of optical trapping of microorganisms has often been regarded as nonin-
vasive and relatively benign compared to mechanical methods such as pipette suction
and AFM. However, the benign nature of the handling of microorganisms is subject
to carefully chosen parameters. Otherwise, a spectrum of possible damage can arise,
the most severe of which Arthur Ashkin had coined the term “opticution” (death by
light) [90].
Itisthereforeessentialwhendoingsingle-cellstudiestohavecompletecontrolover
the amount of damage inflicted to the particular system being studied and preferably
to be able to minimize or completely remove the cause of the damage. Therefore,
numerous studies over the last 20 years have been performed to characterize, quantify,
and find the source of the damage (see summary in Table 6.2). These studies vary in
parameters such as microorganisms used, trapping wavelength, power levels, viability
detection method, and trapping method.
In many of these studies, viability appears as precursors to some other main
experiments to discount possible damage the optical trapping system may have had on
the microorganism. While this does not impact the validity of the studies themselves, it
does somewhat hinder direct comparisons across multiple studies, as many variations

TABLE 6.2
Survey of cell viability studies
Wavelength (nm)
Organism/system
Detection method
Viability parameter
Reference
1070
S. cerevisiae
Image analysis
Growth rate, division time
91
1064
DNA
DNA tether damage
ROS
92
870, 930
Multiple bacteria
APO LOGIX JC-1 assay kit
Membrane potential, ROS
93
780
T cells
CFSE, PI
Membrane rupture
94
1064
E. coli, Listeria moncytogenes
CFDA-SE, GFP
pHi
95
840–930
E. coli
GFP construct
Gene expression
96
1064
S. cerevisiae
Visual inspection
Division time
97
1064
E. coli
Visual inspection
Division time, length increase
98
830, 880
S. cerevisiae
Visual inspection
Division time
99
700–850
Caenorhabditis elegans
β-galactosidase assay
Heat shock gene expression
100
308–1064
NC37 lymphoblasts
Comet assay
DNA damage
101
836
E. coli
SYTO 9, PI
Doubling time
102
809
CHO, T-lymphocytes
Acridine orange, SYBR 14, PI
Membrane rupture, DNA damage
103
790–1064
E. coli
Rotation rate
Image analysis
104
1064
CHO
Laurdan, acridine orange, PI, SNARF
Heating, membrane rupture, pHi
105
700–1064
CHO
Cell counting
Cloning efficiency
106
760–800
Spermatozoa
SYBRTM 14, PI
Membrane rupture
107
700–840
Chromosomes
Visual inspection
Chromosome bridging
108
1064
S. cerevisiae
Visual inspection
Division
6
221

222
CELL HANDLING, SORTING, AND VIABILITY
in the results will likely be due to variations in the aforementioned parameters. The
similarities and the discrepancies between these studies will be discussed below. It is
assumed in the following section that this single-beam optical tweezers are used for
trapping, unless explicitly stated otherwise.
6.3.1
Choosing the Optimal Wavelength
Lasers are routinely used for blasting materials in industrial settings and so their
safety in handling cells is a natural cause for concern among biologists. The trick
lies in using wavelengths where cells are practically transparent. Some of the earliest
optical tweezers used visible wavelengths, which is convenient for alignment and for
visualizing the trap, but is largely absorbed by biological systems [3]. Optical trapping
systems used today with live cell systems almost exclusively use NIR lasers and one
of the most preferred wavelengths is 1064 nm, due to the low absorption in water and
other biological chromophores, as shown by the absorption spectrum in Figure 6.15.
This was first confirmed by K¨onig et al. [107] by trapping human spermatozoa at
selected wavelengths ranging from 365 to 1064 nm for detecting paralysis, autoflu-
orescence and PI staining. Significant rapid loss of cell vitality and viability was
observed at both 365 nm and 760 nm, while trapping at 800 nm was less harmful to
sperm cells.
An early study by Vorobjev et al. on the wavelength dependence of photodamage
in the NIR region utilized chromosome bridge formation in rat and kangaroo cells to
determine the extent of the photodamage. It focused on the range 700–840 nm and
found minimal damage at 700 nm and 800–820 nm [108].
250
10−3
10−2
10−1
100
101
102
103
104
500
750
1000
1250
Wavelength (nm)
Diode lasers
Nd:YAG
Hb
Region of
relative transparency
HbO2
H2O
H2O
Absorption coefficient (cm−1)
1500 4000
8000
12,000
FIGURE 6.15
Absorption spectrum of water, deoxyhemoglobin (Hb), and oxyhemoglobin
(HbO2). Note the placement of the Nd:YAG laser at the minimum of the water absorption.
Adapted from Reference 27. Annual review of biophysics and biomolecular structure.

CELL VIABILITY
223
1400
1200
1000
800
600
400
200
LD50 (s)
Cloning efficiency (%)
Wavelength (nm)
800
900
1000
20
40
60
80
100
120
FIGURE 6.16
The wavelength dependence of rotation rate (left vertical axis) of E. coli and
cloning efficiency of CHO (right vertical axis). LD50(s) is the time in seconds for the rotation
rate of E. coli to decrease to half of the start rotation rate (solid circles, solid line). The cloning
efficiency (%) is measured from CHO cells exposed to 88 mW for 5 min of trapping (open
circle, dashed line) data from Liang et al. [106]. Adapted from Reference 104. Biophysics
Journal.
Figure 6.16 shows the combined data from two independent studies on the wave-
length dependence of viability [104,106]. Liang et al. studied the cloning efficiency
of CHO cells using a Nd:YAG laser (1064 nm) and a tunable Ti:sapphire laser (700–
990 nm). They found wavelength-dependent photodamage with a minimum located
between 950 and 990 nm and another at around 830 nm. The highest sensitivity to
trapping was found at 740–760 nm and 900 nm. Furthermore, sensitivity at 1064 nm
was also notably higher than in the two minima regions [106].
Using the change in rotation rate of the E. coli flagellum as an indicator of cell
health, Neuman et al. studied wavelength dependence between 790 and 1064 nm
[104]. They found sensitivity minima at the same wavelengths obtained by Liang
et al. The sensitivity at 1064 nm was also increased compared to the minima at
950–990 nm but not to the same extent as Liang et al. observed. The difference in
sensitivity could be attributed to both the viability detection method and the different
microorganisms studied.
From Figure 6.16, the region at 870–940 nm has the most negative effect on the
viability of E.coli. The CHO cells show a similar trend but only has a single data
point in that region. It has been observed in CHO air-dried monolayers that there is
a unique spectral absorption band in the oxidized cytochrome-c oxidase complex at
870 nm. This absorption band coincides well with the minimum viability found by
Neuman et al., as shown in Figure 6.16. Furthermore, the second minimum at 930 nm
correlates well with a strong absorption peak at exactly 930 nm in porcine fat (long
chain hydrocarbons) [109,110]. It is postulated that this unique absorption band is, in
all probability, caused by the carbon–hydrogen (C–H) covalent atomic bonds in the
oil structure of the fat [110]. Despite the many fundamental differences between cells
of eukaryotic and prokaryotic species, one component shared by almost every type of
living organism is the structural features of lipid carbon–hydrogen (C–H) bonds in the

224
CELL HANDLING, SORTING, AND VIABILITY
form of a phospholipid bilayer membrane. Consequently, the cause of photodamage
around 930 nm is likely shared among both eukaryotic and prokaryotic cells.
Leitz et al. (2002) investigated the expression of a heat-shock gene in transgenic
C. elegans at wavelengths between 700 and 850 nm and found the least damage at
810 nm and the most damage at 700–760 nm [100]. A recent study on wavelength
dependence by Mirsaidov et al. [96] trapped 5 × 5 2D array of transformed E. coli
bacteria positioned in a gel matrix. The cells were exposed to the laser light from 840 to
930 nm and the expression of a GFP plasmid was measured. They found wavelength-
dependent photodamage, but surprisingly with minimal damage observed at 900 nm,
in conflict with Neuman et al. who found the most damage to E. coli in that part of
the spectrum. Differences in the detection method and a possible strain dependence
could again explain the discrepancies.
From the available data, selecting the ideal wavelength for live cell trapping and
measuring is not an easy task. All studies show that cell viability is maintained for
longer periods when selecting a wavelength above 800 nm and that trapping below
800 nm is far more harmful. Mohanty et al. even suggested a shift in damaging
mechanism between 780 and 800 nm being linked to a change from direct to indirect
DNA damage [101].
Above 800 nm the picture is not so clear with data pointing in different directions.
However, two windows do appear more benign than other parts of the spectrum.
The region around 830 nm and the regions around 970 nm show minimal sensitivity
in both E. coli and CHO cells [104, 106]. The most popular wavelength for optical
tweezers, 1064 nm, is situated between the most sensitive and the least sensitive
wavelengths, making it a questionable choice if cell viability is of primary concern.
In practice, other considerations such as beam quality, power, and of course price
will factor into purchasing decisions.
6.3.2
Thermal Effects
Even though the absorbance of NIR lasers in both water and biological material
is relatively small, the high intensity at the focus could potentially cause excessive
heating of both the water and the trapped cell. Optical trapping of liposomes, human
sperm cells, and hamster ovary cells increases the temperature of 14.5◦C/W, 10◦C/W,
11.5◦C/W, respectively, at 1064 nm [105,111]. Using a 1064-nm laser to manipulate
a Langmuir lipid monolayer at an air/water interface, Wurlitzer et al. [112] measured
an increase of 5◦C/W. Trapping polystyrene beads ranging from 500 to 2200 nm with
a 1064-nm laser and measuring the increase in thermal motion, Peterman et al. [113]
found an increase of 8.1◦C/W in the focus. Leitz et al. [100] calculated temperature
increases for C. elegans at different wavelengths ranging from 700 to 1064 nm and
found increases of 1.19◦C/W at 700 nm up to 38.4◦C/W at 1064 nm. However,
they did not find a clear correlation between the theoretically calculated temperature
increases and measured cell damage, indicating the bulk photodamage was not caused
by thermal damage.
These findings also show that by using moderate power (<20 mW) when trapping
only a small increase in temperature is observed. All these findings were done on

CELL VIABILITY
225
single beam traps using high NA objectives, delivering high power at the focal point.
It has been shown that the majority of the heat increase will be created in the focal
point [113]. Instead of using tightly focused beams, an alternative is to use nonfocused
counter-propagating dual-beams for trapping. The maximum power density is orders
of magnitude lower, further reducing any possible heating effects [91]. Another option
to reduce photodamage is to use Laguerre-Gaussian hollow beams to only illuminate
the cell periphery [114], which also optimizes the trapping efficiency [13].
In a recent study, Wetzel et al. in 2011 utilized the optical stretcher, a two-beam
laser trap, to simulate heat shocks that cells typically experience during measurements
in optical traps. They measured temperature increase of 23 ± 2 using 1 W power from
each fiber [115]. The results show that about 60% of the cells survived heat shocks
without vital damage at temperatures of up to 58◦C for 0.5 s. By varying the duration of
the heat shocks, it was shown that 60% of the cells stayed viable when exposed to 48◦C
for 5 s. Proteins will usually start to denature at 43◦C. In cells the process of denatura-
tion is reversible as long as DNA is not damaged and certain enzymes are still working
[116]. However, these enzymes will eventually also be denatured and it is therefore
extremely important to keep the duration to a minimum, if viability is a concern.
6.3.3
Power and Energy Dose
Using the lowest possible power for trapping would be prudent even when using
wavelengths expected to have low absorption. Due to the high NA objectives used
in single-beam optical trapping, the intensities at the focal point can become high
enough to induce nonlinear effects such as two-photon absorption. The two-photon
process was demonstrated by Zhang et al. [103] who used a 809-nm laser (190 mW)
to trap both CHO cells and T-lymphocytes, while simultaneously exciting the probes
PI and SYBR 14 with the same laser. Another demonstration of the two-photon pro-
cess was performed by Goks¨or et al. [117] who excited the probe PI, in spermatozoa
by using a CW laser (1064 nm) as both a trapping and excitation source. However,
very high trapping power (2 W) and a long exposure time (1 min) was needed to
visualize the fluorescence. K¨onig et al. [107] suggested a two-photon mechanism
to account for some of the photodamage inflicted on sperm cells at wavelengths
below 800 nm.
Using a pulsed laser to trap cells will reach peak powers that are high enough for
two-photon effects to be much more probable. The use of mode-locked lasers and
Q-switched lasers would possibly create this effect but they are normally not used for
optical trapping. CW lasers operate at a much lower peak power level and therefore
the probability is much lower. The clearest indication of two-photon processes is
a quadratic dependence on the laser intensity. However, if a one-photon process
dominates, then the photodamage parameter will have a linear dependence on laser
intensity. This was shown by Neuman et al. [104] who observed the sensitivity of the
rotation rate of E. coli as being linearly dependent on the trapping power. This was
further supported by Mirsaidov et al. [96] who, trapping E. coli in CW and timeshared
traps, found that viability was linearly dependent on the peak power. Furthermore,
they found viability depending only on the total dose delivered. A similar study by

226
CELL HANDLING, SORTING, AND VIABILITY
Ayano et al. [98] also found the inhibition of both growth rate and division time to
be proportional to the total dose.
In contrast to this, Aabo et al. [91] observed that the total dose delivered did not
fully characterize the amount of photodamage. Aabo et al. trapped S. cerevisiae at
1070 nm and observed the growth rate and division time at various laser power and
exposure times. By keeping the dose constant but varying the exposure time and peak
power, a dominant effect of the peak power over the exposure time was observed. This
supports findings by Mohanty et al. [101] who, using a comet assay to detect DNA
damage, found a similar dependence of peak power over exposure time at different
wavelengths (750–1064 nm).
The findings of Aabo et al. [91] and Mohanty et al. [101] do not exclude the
possibility of the single-photon process as the cause of photodamage; however, they
underline the complicated nature of the repair mechanisms present in cells, which pos-
sibly reacts nonlinearly to increased photodamage. The discrepancies between these
studies could be caused by variations in both microorganisms and detection method.
6.3.4
Growth and Division Time
In one of the earliest studies, Ashkin et al. [6] showed the division of a trapped yeast
cell to indicate the nonintrusive and gentle method of optical trapping. However, as
any microbiologist will tell you, you need a lot of data to say anything with any
degree of certainty.
One of the primary measurements of a cell’s health state is the cell’s ability to
grow and multiply. Many different factors have to be in place for a cell to function
and grow at the optimum rate. Measuring growth dependence on photodamage is
therefore a very “global” measurement of the cell’s sensitivity to an external stress
factor. Because of the complexity involved in growth and division, it is therefore
difficult to determine the cause of the growth or division arrest, as almost any stress
factor will result in changes in growth or division rate. The need to measure over
prolonged periods (2–4 h) in order to gather data also complicates matters further. As a
result very few extensive studies have been published regarding this health parameter.
Sacconi et al. [99] observed a delay in the start of division of S. cerevisiae after
trapping at 830 nm at 200 mW for 5 min. The delay was between 3 and 7 h; however,
the sample size was very small (n = 4). In contrast, a study by Luo et al. [97], trapping
at 1064 nm but also on S. cerevisiae cells found no discernible effect in division time
by trapping at 300 mW for 10 min. The small number of cells used (n = 3) in the
study does question the accuracy of their findings.
In a study on E. coli, Ericsson et al. [102] positioned cells in arrays using an
836-nm laser. They did not see any change in the division rate at trapping powers
(output power) up to 2 W; however, trapping times are not listed in the results, which
hinder comparison with similar studies.
Ayano et al. [98] did find both the division time and growth rate of E. coli at
exposure to 1064 nm significantly inhibited at 0.36 and 0.54 J, respectively. This
indicates an uncoupling of the division time from the growth as a response to laser
inhibition. This is consistent with results by Aabo et al. [91] who found S. cerevisiae

CELL VIABILITY
227
cells’ division times and growth rates to be affected differently at exposure to 1064 nm
laser light. They also found the growth inhibition to be affected after the trapping
laser was turned off. Their data cannot determine whether the permanent damage
was only located in the mother cells or also transferred to daughter cells. However, a
possible transference of photodamage from mother to daughter cell was suggested by
Ericsson et al. [102], who did find the division of daughter cells not directly exposed
to be inhibited in division ability.
A recent study by Eriksson et al. [41] used the ability of S. cerevisiae cells to bud
after being trapped for 10 sec at various intensities (0–360 mW). They saw a small
decrease in the number of cells that would bud at 360 mW, and almost all cells (98%)
would bud at exposure to 240 mW. This indicates that the exposure was not lethal,
but as they did not measure the division time it is not possible to determine if the
photodamage caused a lag in division time.
The discrepancies in the literature when looking at the cell division time response
to laser light are quite large, with some reports of no change [102] in division time
and others showing significant changes even at very low intensities [91,98]. However,
the most recent and comprehensive studies do point toward a gradual response by
the cell to laser inhibition. Therefore, any study performed where the division rate is
used as a cell health indicator, where optical trapping is involved, needs to be careful
in separating the laser inhibition response from any other cell stress response.
The inhibition of the growth rate of cells has to our knowledge only been described
in two studies [91, 98]. From those studies a clear growth inhibition is evident and
there does not appear to be a lower threshold at which inhibition is not seen. For cells
exposed to a defined pulse Ayano et al. [98] showed that trapping E. coli at 3 mW
completely stopped the growth. After the cells were released they started growing
again, but with a delay of 140 min, Aabo et al. [91] observed similar behavior where
exponential growth was regained in cells after trapping was stopped. Nevertheless, the
use of optical trapping to measure growth should be approached with great caution,
as significant changes in growth rate have been observed.
6.3.5
Propidium Iodide
The use of propidium iodide (PI) as a viability measurement is very popular as it is
easy to handle and gives a simple binary signal (stained/not stained). PI intercalates
into double-stranded DNA and will become fluorescent. It is membrane impermeant
and will therefore not stain cells, which have a normal membrane. Cells that are
stained will therefore have a ruptured membrane, which is generally assumed to be
a nonrecoverable injury to a cell [118]. However, a cell that is not stained does not
necessarily mean that the cell is not damaged. It only means that the membrane
is still intact and even though the nonstaining of a cell indicates viability it does
therefore not guarantee it. Several of the studies in Table 6.2 included the probe
[94,102,103,105,107]. These are discussed and compared below.
Trapping CHO cells and T–lymphocytes, Zhang et al. [103] did not notice any
membrane rupture trapping at 809 nm for 15 min at 180 mW. Similar results were
found by K¨onig et al. [107] on sperm cells using 800 nm.

228
CELL HANDLING, SORTING, AND VIABILITY
Liu et al. [105] observed PI staining of some sperm cells trapped at 300 mW for
longer than 2 min using a 1064-nm laser. A recent study by Harris and McConnell [94]
using a trapping laser wavelength of 780 nm on T-lymphocytes observed membrane
ruptures after 15 min, at exposures of 9 mW. The variance between these studies could
to some extend be caused by wavelength dependence. As shown in Figure 6.16,
780 nm is more damaging than 800 nm when measuring cloning efficiency and
rotation rate of E. coli flagella. It is therefore also possible that cell membrane damage
will have the same wavelength dependence. But the differences between these studies
also illustrate one of the problems using PI as a viability detector, which is the absence
of detailed information on the cells’ health. Because the signal from PI is on/off it
necessitates a lot of data to say anything quantitative about the photodamage. Harris
and McConnell [94] claimed “By using PI staining, it was clear that the cells remained
viable for periods of up to 25 minutes.” However, as shown by Aabo et al. [36], there
exists a lag between decrease in cell health and membrane rupture. The extent of
this lag and what causes the cell membrane to rupture is at present not properly
characterized and needs to be further elucidated. Furthermore, studies have shown
the discrepancy between intact membranes and loss of the ability to reproduce [102].
Therefore, using PI viability measurements alone to determine the state of the cell
health can be dangerous and could lead to misinterpretation of the data.
6.3.6
Internal pH (pHi)
The cells control over the pHi is paramount for many aspects of cell metabolism
and any changes in the internal pH will therefore affect the cells general health state.
Furthermore, it can reveal information at a much early time point than growth rates
or live/dead staining with PI or a similar probe [36]. Another potential benefit of the
method is that it allows for spatial measurements of the pHi while trapping, which can
reveal information regarding the pH in the different organelles within the cell [35].
However, only one study has to our knowledge been published which characterizes
the relationship between photodamage and pHi [95]. Another study by Liu et al. [105]
did measure the pHi of CHO cells using the pH-sensitive probe SNARF by using
two-photon excitation with the trapping laser (1064 nm). However, they equilibrated
internal pH of the cells with the external pH using the ionophore nigericin, which
trades H+ for K+ [119, 120], thereby removing a possible cellular pHi response to
photodamage.
Rasmussen et al. [95] trapped E. coli and three Listeria bacteria species at 1064 nm
with 6 and 18 mW power in the sample plane. They observed a drop in pHi dependent
on trapping strength for all species; however, they saw a different pHi drop in different
species, indicating that different species will have a different cellular response to laser
trapping. If the pHi response is so varied between bacteria species, it is plausible that
similar species dependence is present in other viability parameters, for example,
growth rate, division time, and membrane rupture. This species dependence response
to trapping could therefore explain some of the discrepancies seen between other
studies.

CELL VIABILITY
229
6.3.7
Reactive Oxygen Species
The presence of oxygen is essential for aerobe yeast metabolic activity. The reactive
nature of molecular oxygen creates various reactive species from the yeast metabolism
or reaction of sensitive metabolites with oxygen. These species are classified as
reactive oxygen species (ROS). The molecules are either oxidants such as hydrogen
peroxide (H2O2) or anti-oxidants such as the super oxide anion (O2−⋅). On top of
the primary ROS molecules, the primary ROS can react with other compounds to
produce new toxic reactive species. These species create oxidative stress upon the
cell and cause damage to various cell components. Several extensive review have
been published on the subject [121,122].
The cause of photodamage was initially proposed to be ROS production using
laser–tissue interaction by K¨onig et al. [107]. They proposed a two-photon effect
whereby the creation of oxidative stress was induced. However, they did not believe
that this effect would be possible at wavelengths higher than 800 nm. This was also
suggested by Liu et al. [105]. The creation of ROS at longer wavelength was postulated
by Neuman et al. [104], who trapped E. coli at 1064 nm under both anaerobic and
aerobic conditions. The rotation rate of the flagella was used as a photodamage
parameter. They found that the photodamage was significantly reduced when the
cells were trapped under anaerobic conditions compared to aerobic conditions. This
suggested that the presence of oxygen was necessary for the photodamage to occur.
They suggested that the presence of an unknown photosensitizer to mediate the
production of singlet oxygen.
Neuman et al. [104] also proposed using known oxygen scavengers, antioxidant
such as β-carotene or α-tocopherol to remove the effect of ROS. However, the use
of such compound would be difficult to administer into cells, as it would have to
cross the cell membrane. Furthermore, for such an approach to work the antioxidant
must be at the right place at the right time to react and neutralize the ROS before it
reacts with its substrate (lipids, DNA, proteins). This means it will be in competition
with the substrate, which will most likely be in abundance within the cell. Therefore,
it is likely that considerable amounts of antioxidants are necessary for it to have a
noticeable effect on the damage caused by the ROS. To further impede the approach
of using antioxidants, the most reactive ROS species (⋅OH) will only move a very
short distance before it reacts with its substrate. Therefore, the antioxidant will also
have to spatially be located very close to the ROS for it to work.
The possibility of ROS as a significant factor in photodamage was further sup-
ported by the findings of Rasmussen et al. [95]. They found E. coli to be less affected
by laser trapping when they had been grown under aerobic conditions than anaerobic
conditions. When cells are grown in conditions with a surplus of oxygen the prob-
ability of ROS generation is higher than if cell are grown with little to no oxygen.
The cells will therefore adapt to the increase in oxidative stress by increasing the
cells’ ability to handle the ROS, producing more antioxidants. This would explain
why the aerobic grown cells would be less sensitive to a possible oxidative stress than
anaerobic grown cells.

230
CELL HANDLING, SORTING, AND VIABILITY
A recent paper by Landry et al. [92] has linked optical trapping of polystyrene
microspheres with the generation of singlet oxygen. Using a 1064-nm laser, they
trapped a DNA tether between two polystyrene beads. They then measured the
tether lifetime using a standard force measurement method of optical tweezers. They
observed that by adding a quencher specific to singlet oxygen the damage caused to
the DNA tethers was significantly reduced. Singlet oxygen has been documented to
cause damage on both single- and double-stranded DNA [123,124]. The generation
of singlet oxygen was confirmed additionally by the use of an anthracene derivative,
which exhibits specific reactivity toward singlet oxygen. This was even further con-
firmed by using the probes singlet-oxygen sensor green (SOSG), which as its name
suggests is specifically reactive toward singlet oxygen.
To excite molecular oxygen from the ground level to its excited triplet state
takes 0.98 eV. Even though the energy from the laser is consistent with the triplet
excitation energy (1.17 eV, 1064 nm) the transition is strictly forbidden by spin,
symmetry, and Laport selection rules [125]. To create the excited state single oxygen,
a photosensitizer is necessary. The photosensitizer must absorb the energy, store it
as vibrational energy, and transfer it to the ground state oxygen. Landry et al. [92]
identified the photosensitizer to the polystyrene bead itself. Switching the polystyrene
bead to a silica bead and observing a significant drop in damage to the DNA tether
with no signal from the SOSG probe confirmed this.
The creation of singlet oxygen by photodamage and specifically the damage
to DNA would be consistent with the findings of Mohanty et al. [101] who also
observed DNA damage in the near-IR. However, it is possible that other ROS would
cause similar DNA damage but only two ROS are thought to be energetic enough: the
hydroxyl radical and singlet oxygen. This points toward either the hydroxyl radical or
singlet oxygen as probable candidates for cause of photodamage at around 1064 nm.
Based on the findings by Landry et al. [92], it would be pertinent to use a singlet
oxygen detecting probe such as the SOSG on cells to establish if singlet oxygen is
also created in a biological system. If this were the case then the next step would be
to find the sensitizer or sensitizers responsible for the generation of singlet oxygen.
REFERENCES
[1] P. Mazzarello, “A unifying concept: the history of cell theory,” Nat. Cell Biol. 1, E13–
E15 (1999).
[2] J. Gl¨uckstad, “Microfluidics: sorting particles with light,” Nat. Mater. 3, 9–10 (2004).
[3] A. Ashkin, “Acceleration and trapping of particles by radiation pressure,” Phys. Rev.
Lett. 24, 156–159 (1970).
[4] S. Chu, J. Bjorkholm, A. Ashkin, and A. Cable, “Experimental observation of optically
trapped atoms,” Phys. Rev. Lett. 57, 314–317 (1986).
[5] A. Ashkin and J. Dziedzic, “Optical trapping and manipulation of viruses and bacteria,”
Science 235, 1517–1520 (1987).
[6] A. Ashkin, J. Dziedzic, and T. Yamane, “Optical trapping and manipulation of single
living cells using infrared laser beams,” Nature 330, 769–771 (1987).

REFERENCES
231
[7] A. Ashkin, “History of optical trapping and manipulation of small-neutral particle,
atoms, and molecules,” IEEE J. Sel. Top. Quantum Electron. 6, 841–856 (2000).
[8] S. Parkin, et al., “Optical torque on microscopic objects,” Methods Cell Biol. 82, 525–
561 (2007).
[9] A. Ashkin, “Forces of a single-beam gradient laser trap on a dielectric sphere in the ray
optics regime,” Biophys. J. 55, 569–582 (1992).
[10] Y. Harada, “Radiation forces on a dielectric sphere in the Rayleigh scattering regime,”
Opt. Commun. 124, 529–541 (1996).
[11] A. Rohrbach and E. H. K. Stelzer, “Optical trapping of dielectric particles in arbitrary
fields,” J. Opt. Soc. Amer. A 18, 839 (2001).
[12] D. Ganic, X. Gan, and M. Gu, “Exact radiation trapping force calculation based on
vectorial diffraction theory,” Opt. Express 12, 2670–2675 (2004).
[13] N. B. Simpson, D. McGloin, K. Dholakia, L. Allen, and M. J. Padgett, “Optical tweezers
with increased axial trapping efficiency,” J. Modern Opt. 45, 1943–1949 (1998).
[14] A. Rohrbach and E. H. K. Stelzer, “Trapping forces, force constants, and potential
depths for dielectric spheres in the presence of spherical aberrations,” Appl. Opt. 41,
2494–2507 (2002).
[15] T. ˇCiˇzm´ar, M. Mazilu, and K. Dholakia, “In situ wavefront correction and its application
to micromanipulation,” Nat. Photon. 4, 388–394 (2010).
[16] P. Mthunzi, A. C. Riches, C. T. A. Brown, F. J. Gunn-Moore, and K. Dholakia, “Intracel-
lular dielectric tagging for improved optical manipulation of mammalian cells,” IEEE
J. Sel. Top. Quantum Electron. 16, 608–618 (2010).
[17] N. B. Viana, M. S. Rocha, O. N. Mesquita, A. Mazolli, and P. A. M. Neto, “Char-
acterization of objective transmittance for optical tweezers,” Appl. Opt. 45, 4263
(2006).
[18] M. Mahamdeh, C. P´erez Campos, and E. Sch¨affer, “Under-filling trapping objectives
optimizes the use of the available laser power in optical tweezers,” Opt. Express 19,
11759 (2011).
[19] E. F¨allman et al. “Optical tweezers based force measurement system for quantitating
binding interactions: system design and application for the study of bacterial adhesion,”
Biosens. Bioelectron. 19, 1429–1437 (2004).
[20] M.-T. Wei, K.-T. Yang, A. Karmenyan, and A. Chiou, “Three-dimensional optical force
field on a Chinese hamster ovary cell in a fiber-optical dual-beam trap,” Opt. Express
14, 3056–3064 (2006).
[21] H.-U. Ulriksen et al., “Independent trapping, manipulation and characterization by an
all-optical biophotonics workstation,” J. Eur. Opt. Soc. Rapid Publ. 3, 080341–080345
(2008).
[22] L. Cherkezyan et al., “Targeted alteration of real and imaginary refractive index of
biological cells by histological staining,” Opt. Lett. 37, 1601 (2012).
[23] M. Fr´en´ea and H. Naoufel, “On-chip cell positioning and sorting using contactless
methods: a comparison between different force-fields,” Biomed. Eng. (2009).
[24] C. Liu and C. E. Capjack, “Effects of cellular fine structure on scattered light pattern,”
IEEE Trans. Nanobiosci. 5, 76–82 (2006).
[25] V. Backman et al., “Measuring cellular structure at submicrometer scale with light
scattering spectroscopy,” IEEE J. Sel. Top. Quantum Electron. 7, 887–893 (2001).

232
CELL HANDLING, SORTING, AND VIABILITY
[26] A. G. Banerjee, S. Chowdhury, W. Losert, and S. K. Gupta, “Survey on indirect optical
manipulation of cells, nucleic acids, and motor proteins,” J. Biomed. Opt. 16, 051302
(2011).
[27] K. Svoboda and S. M. Block, “Biological applications of optical forces,” Annu. Rev.
Biophys. Biomol. Struct. 23, 247–285 (1994).
[28] T. T. Perkins, “Optical traps for single molecule biophysics: a primer,” Laser Photon.
Rev. 3, 203–220 (2009).
[29] A. Jannasch, A. F. Demir¨ors, P. D. J. Van Oostrum, A. Van Blaaderen, and E. Sch¨affer,
“Nanonewton optical force trap employing anti-reflection coated, high-refractive-index
titania microspheres,” Nat. Photon. 6, 469–473 (2012).
[30] S. Chowdhury, P. Svec, W. Losert, and S. K. Gupta, “Gripper synthesis for indirect
manipulation of cells using holographic optical tweezers,” IEEE Int. Conf. Robotics
Automation 2749–2754 (2012). doi:10.1109/ICRA.2012.6225153
[31] J. Gl¨uckstad and D. Palima, Generalized Phase Contrast: Applications in Optics and
Photonics (Springer, New York, 2009).
[32] B. Kemper, et al. “Monitoring of laser micromanipulated optically trapped cells by
digital holographic microscopy,” J. Biophoton. 3, 425–431 (2010).
[33] B. Kemper, A. Vollmer, C. E. Rommel, J. Schnekenburger, and G. Von Bally, “Simplified
approach for quantitative digital holographic phase contrast imaging of living cells,”
J. Biomed. Opt. 16, 026014 (2011).
[34] P. J. Rodrigo, D. Palima, and J. Gl¨uckstad, “Accurate quantitative phase imaging using
generalized phase contrast,” Opt. Express 16, 2740–2751 (2008).
[35] T. Aabo, A. R. Ba˜nas, J. Gl¨uckstad, H. Siegumfeldt, and N. Arneborg, “BioPhotonics
workstation: a versatile setup for simultaneous optical manipulation, heat stress, and
intracellular pH measurements of a live yeast cell,” Rev. Sci. Instrum. 82, 083707
(2011).
[36] T. Aabo, J. Gl¨uckstad, H. Siegumfeldt, and N. Arneborg, “Intracellular pH distribution
as a cell health indicator in Saccharomyces cerevisiae,” J. R. Soc. Interface 8, 1635–1643
(2011).
[37] G. Volpe, G. P. Singh, and D. Petrov, “Dynamics of a growing cell in an optical trap,”
Appl. Phys. Lett. 88, 231106 (2006).
[38] C. Xie, M. A. Dinno, and Y.-Q. Li, “Near-infrared Raman spectroscopy of single
optically trapped biological cells,” Opt. Lett. 27, 249–251 (2002).
[39] M. Lankers, J. Popp, and W. Kiefer, “Raman and fluorescence spectra of single optically
trapped microdroplets in emulsions,” Appl. Spectrosc. 48, 1166–1168 (1994).
[40] H. Mao, J. R. Arias-Gonzalez, S. B. Smith, I. Tinoco, and C. Bustamante, “Temperature
control methods in a laser tweezers system,” Biophys. J. 89, 1308–1316 (2005).
[41] E. Eriksson et al., “A microfluidic device for reversible environmental changes around
single cells using optical tweezers for cell selection and positioning,” Lab on a Chip 10,
617–625 (2010).
[42] H. Wu et al., “In vivo lipidomics using single-cell Raman spectroscopy,” Proc. Natl.
Acad. Sci. USA 108, 3809–3814 (2011).
[43] T. J. Moritz et al. “Evaluation of Escherichia coli cell response to antibiotic treatment
by use of Raman spectroscopy with laser tweezers,” J. Clin. Microbiol. 48, 4287–4290
(2010).

REFERENCES
233
[44] G. P. Singh, C. M. Creely, G. Volpe, H. Gr¨otsch, and D. Petrov, “Real-time detection
of hyperosmotic stress response in optically trapped single yeast cells using Raman
microspectroscopy,” Anal. Chem. 77, 2564–2568 (2005).
[45] D. E. Ingber, “Mechanical control of tissue growth: function follows form,” Proc. Natl.
Acad. Sci. USA 102, 11571–11572 (2005).
[46] P. J. Rodrigo et al. “Optical microassembly platform for constructing reconfigurable
microenvironments for biomedical studies,” Opt. Express 17, 6578–6583 (2009).
[47] T. Matsuoka et al. “Functionalized 2PP structures for the biophotonics workstation,”
SPIE OPTO 79500Q–79500Q–6 (2011). doi:10.1117/12.877189
[48] B. L. Aekbote et al., “Aminosilane-based functionalization of two-photon polymerized
3D SU-8 microstructures,” Eur. Polym. J. 48, 1745–1754 (2012).
[49] A. Ovsianikov, Z. Li, J. Torgersen, J. Stampfl, and R. Liska, “3D photografting: selec-
tive functionalization of 3D matrices via multiphoton grafting and subsequent click
chemistry (Adv. Funct. Mater. 16/2012),” Adv. Funct. Mater. 22, 3527 (2012).
[50] D. Palima et al., “Wave-guided optical waveguides,” Opt. Express 20, 2004–2014 (2012).
[51] N. Arneborg et al., “Interactive optical trapping shows that confinement is a determinant
of growth in a mixed yeast culture,” FEMS Microbiol. Lett. 245, 155–159 (2005).
[52] Z. Li and S. K. Nair, “Quorum sensing: how bacteria can coordinate activity and
synchronize their response to external signals?,” Protein Sci. 21, 1403–1417 (2012).
[53] W. Timp, U. Mirsaidov, P. Matsudaira, and G. Timp, “Jamming prokaryotic cell-to-cell
communications in a model biofilm,” Lab on a chip 9, 925–934 (2009).
[54] U. Mirsaidov et al., “Live cell lithography: using optical tweezers to create synthetic
tissue,” Lab on a chip 8, 2174–2181 (2008).
[55] H. Kress et al., “Cell stimulation with optically manipulated microsources,” Nat. Meth-
ods 6, 905–909 (2009).
[56] T. Wu et al., “A photon-driven micromotor can direct nerve fibre growth,” Nat. Photonics
6, 62–67 (2011).
[57] J. H. Sung and M. L. Shuler, “Microtechnology for mimicking in vivo tissue environ-
ment,” Ann. Biomed. Eng. 40, 1289–1300 (2012).
[58] C. C. DuFort, M. J. Paszek, and V. M. Weaver, “Balancing forces: architectural control
of mechanotransduction,” Nat. Rev. Mol. Cell Biol. 12, 308–319 (2011).
[59] D. Choquet, D. P. Felsenfeld, and M. P. Sheetz, “Extracellular matrix rigidity causes
strengthening of integrin–cytoskeleton linkages,” Cell 88, 39–48 (1997).
[60] D. P. Felsenfeld, P. L. Schwartzberg, A. Venegas, R. Tse, and M. P. Sheetz, “Selective
regulation of integrin–cytoskeleton interactions by the tyrosine kinase Src,” Nat. Cell
Biol. 1, 200–206 (1999).
[61] Y. Wang et al., “Visualizing the mechanical activation of Src,” Nature 434, 1040–1045
(2005).
[62] A. Resnick, “Use of optical tweezers to probe epithelial mechanosensation,” J. Biomed.
Opt. 15, 015005 (2010).
[63] S. M. Nauli et al. “Polycystins 1 and 2 mediate mechanosensation in the primary cilium
of kidney cells,” Nat. Genet. 33, 129–137 (2003).
[64] J. Gl¨uckstad, A. R. Ba˜nas, T. Aabo, and D. Palima, “Structure-mediated micro-to-nano
coupling using sculpted light and matter,” SPIE Photonics Europe 84241L–84241L–6
(2012). doi:10.1117/12.924097

234
CELL HANDLING, SORTING, AND VIABILITY
[65] D. B. Phillips et al., “Surface imaging using holographic optical tweezers,” Nanotechnol.
22, 285503 (2011).
[66] C. T. Lim, E. H. Zhou, and S. T. Quek, “Mechanical models for living cells–a review,”
J. Biomech. 39, 195–216 (2006).
[67] J. P. Mills, L. Qie, M. Dao, C. T. Lim, and S. Suresh, “Nonlinear elastic and viscoelastic
deformation of the human red blood cell with optical tweezers,” Mech. Chem. Biosystems
1, 169–180 (2004).
[68] J. Guck et al., “The optical stretcher: a novel laser tool to micromanipulate cells,”
Biophys. J. 81, 767–784 (2001).
[69] A. Fritsch et al. “Are biomechanical changes necessary for tumour progression?,” Nat.
Phy. 6, 730–732 (2010).
[70] D. L. Jaye, R. A. Bray, H. M. Gebel, W. A. C. Harris, and E. K. Waller, “Transla-
tional applications of flow cytometry in clinical practice,” J. Immunol. 188, 4715–4719
(2012).
[71] W. A. Bonner, “Fluorescence activated cell sorting,” Rev. Instrum. 43, 404 (1972).
[72] H. M. Shapiro, Practical Flow Cytometry. 736pp. (John Wiley & Sons, 2003).
[73] A. Y. Fu, C. Spence, A. Scherer, F. H. Arnold, and S. R. Quake, “A microfabricated
fluorescence-activated cell sorter,” Nat. Biotechnol. 17, 1109–1111 (1999).
[74] A. A. S. Bhagat et al., “Microfluidics for cell separation,” Med. Biol. Eng. Comput. 48,
999–1014 (2010).
[75] M. M. Wang et al., “Microfluidic sorting of mammalian cells by optical force switching,”
Nat. Biotechnol. 23, 83–87 (2005).
[76] T. D. Perroud et al., “Microfluidic-based cell sorting of Francisella tularensis infected
macrophages using optical forces,” Anal. Chem. 80, 6365–6372 (2008).
[77] T.-H. Wu et al., “Pulsed laser triggered high speed microfluidic fluorescence activated
cell sorter,” Lab on a Chip 12, 1378–1383 (2012).
[78] M. Padgett, R. Di Leonardo, and R. Leonardo, “Di holographic optical tweezers and
their relevance to lab on chip devices,” Lab on a Chip 11, 1196–1205 (2011).
[79] X. Wang et al., “Enhanced cell sorting and manipulation with combined optical tweezer
and microfluidic chip technologies,” Lab on a Chip 11, 3656–3662 (2011).
[80] B. Landenberger, H. H¨ofemann, S. Wadle, and A. Rohrbach, “Microfluidic sorting of
arbitrary cells with dynamic optical tweezers,” Lab on a Chip 12, 3177–3183 (2012).
[81] I. Perch-Nielsen, D. Palima, J. S. Dam, and J. Gl¨uckstad, “Parallel particle identifica-
tion and separation for active optical sorting,” J. Opt. A: Pure Appl. Opt. 11, 034013
(2009).
[82] A. R. Ba˜nas, D. Palima, F. Pedersen, and J. Gl¨uckstad, “Development of a compact bio-
optofluidic cell sorter,” SPIE OPTO 82740N–82740N–6 (2012). doi:10.1117/12.909908
[83] T. Imasaka, Y. Kawabata, T. Kaneta, and Y. Ishidzu, “Optical chromatography,” Anal.
Chem. 67, 1763–1765 (1995).
[84] S. J. Hart, A. Terray, J. Arnold, and T. A. Leski, “Sample concentration using optical
chromatography,” Opt. Express 15, 2724 (2007).
[85] K. H. Lee, S. B. Kim, K. S. Lee, and H. J. Sung, “Enhancement by optical force of
separation in pinched flow fractionation,” Lab on a Chip 11, 354–357 (2011).
[86] M. MacDonald, G. Spalding, and K. Dholakia, “Microfluidic sorting in an optical
lattice,” Nature 426, 421–424 (2003).

REFERENCES
235
[87] G. Milne, D. Rhodes, M. MacDonald, and K. Dholakia, “Fractionation of polydisperse
colloid with acousto-optically generated potential energy landscapes,” Opt. Lett. 32,
1144–1146 (2007).
[88] P. Breeuwer and T. Abee, “Assessment of viability of microorganisms employing fluo-
rescence techniques,” Int. J. Food Microbiol. 55, 193–200 (2000).
[89] D. B. Kell, A. S. Kaprelyants, D. H. Weichart, C. R. Harwood, and M. R. Barer,
“Viability and activity in readily culturable bacteria: a review and discussion of the
practical issues,” Antonie van Leeuwenhoek 73, 169–187 (1998).
[90] A. Ashkin and J. Dziedzic, “Optical trapping and manipulation of single liv-
ing cells using infrared-laser beams,” Ber. Bunsen-Ges. Phys. Chem. 93, 254–260
(1989).
[91] T. Aabo et al., “Effect of long-and short-term exposure to laser light at 1070 nm on
growth of Saccharomyces cerevisiae,” J. Biomed. Opt. 15, 041505 (2010).
[92] M. P. Landry, P. M. McCall, Z. Qi, and Y. R. Chemla, “Characterization of photoactivated
singlet oxygen damage in single-molecule optical trap experiments,” Biophys. J. 97,
2128–2136 (2009).
[93] E. Bornstein, W. Hermans, S. Gridley, and J. Manni, “Near-infrared photoinactivation of
bacteria and fungi at physiologic temperatures,” Photochem. Photobiol. 85, 1364–1374
(2009).
[94] J. Harris and G. McConnell, “Optical trapping and manipulation of live T cells with a
low numerical aperture lens,” Opt. Express 16, 14036–14043 (2008).
[95] M. B. Rasmussen, L. B. Oddershede, and H. Siegumfeldt, “Optical tweezers cause phys-
iological damage to Escherichia coli and Listeria bacteria,” Appl. Environ. Microbiol.
74, 2441–2446 (2008).
[96] U. Mirsaidov et al., “Optimal optical trap for bacterial viability,” Phys. Rev. E 78, 1–7
(2008).
[97] C. Luo et al., “The combination of optical tweezers and microwell array for cells
physical manipulation and localization in microfluidic device,” Biomed. Microdevices
9, 573–578 (2007).
[98] S. Ayano, Y. Wakamoto, S. Yamashita, and K. Yasuda, “Quantitative measurement of
damage caused by 1064-nm wavelength optical trapping of Escherichia coli cells using
on-chip single cell cultivation system,” Biochem. Biophys. Res. Commun. 350, 678–684
(2006).
[99] L. Sacconi, I. M. Toli´c-Nørrelykke, C. Stringari, R. Antolini, and F. S. Pavone, “Optical
micromanipulations inside yeast cells,” Appl. Opt. 44, 2001–2007 (2005).
[100] G. Leitz, E. F¨allman, S. Tuck, and O. Axner, “Stress response in Caenorhabditis elegans
caused by optical tweezers: wavelength, power, and time dependence,” Biophys. J. 82,
2224–2231 (2002).
[101] S. Mohanty, A. Rapp, and S. Monajembashi, “Comet assay measurements of DNA
damage in cells by laser microbeams and trapping beams with wavelengths spanning a
range of 308 nm to 1064 nm,” Radiat. Res. Soc. 157, 378–385 (2002).
[102] M. Ericsson, D. Hanstorp, P. Hagberg, J. Enger, and T. Nystr¨om, “Sorting out bacterial
viability with optical tweezers,” J. Bacteriol. 182, 5551–5555 (2000).
[103] Z. X. Zhang et al., “Cell viability and DNA denaturation measurements by two-photon
fluorescence excitation in CW Al:GaAs diode laser optical traps,” J. Biomed. Opt. 4,
256 (1999).

236
CELL HANDLING, SORTING, AND VIABILITY
[104] K. C. Neuman, E. Chadd, G. Liou, K. Bergman, and S. M. Block, “Characterization of
photodamage to Escherichia coli in optical traps,” Biophys. J. 77, 2856–2863 (1999).
[105] Y. Liu, G. J. Sonek, M. W. Berns, and B. J. Tromberg, “Physiological monitoring of
optically trapped cells: assessing the effects of confinement by 1064-nm laser tweezers
using microfluorometry,” Biophys. J. 71, 2158–2167 (1996).
[106] H. Liang et al. “Wavelength dependence of cell cloning efficiency after optical trapping,”
Biophys. J. 70, 1529–1533 (1996).
[107] K. K¨onig, Y. Tadir, P. Patrizio, M. W. Berns, and B. J. Tromberg, “Effects of ultraviolet
exposure and near infrared laser tweezers on human spermatozoa,” Hum. Reprod. 11,
2162–2164 (1996).
[108] I. Vorobjev, H. Liang, W. H. Wright, and M. W. Berns, “Optical trapping for chromosome
manipulation: a wavelength dependence of induced chromosome bridges,” Biophys. J.
64, 533–538 (1993).
[109] J. M. Conway, K. H. Norris, and C. E. Bodwell, “A new approach for the estimation of
body composition: infrared interactance,” Am. J. Clin. Nutr. 40, 1123–1130 (1984).
[110] M. Irie, “Evaluation of porcine fat with fiber-optic spectroscopy,” J. Anim. Sci. 77,
2680–2684 (1999).
[111] Y. Liu et al., “Evidence for localized cell heating induced by infrared optical tweezers,”
Biophys. J. 68, 2137–2144 (1995).
[112] S. Wurlitzer, C. Lautz, M. Liley, C. Duschl, and T.M. Fischer, “Micromanipulation of
Langmuir-monolayers with optical tweezers,” J. Phys. Chem. 105, 182–187 (2001).
[113] E. J. G. Peterman, F. Gittes, and C. F. Schmidt, “Laser-induced heating in optical traps,”
Biophys. J. 84, 1308–1316 (2003).
[114] R. Dasgupta, S. Ahlawat, R. S. Verma, S. Shukla, and P. K. Gupta “Optical trapping of
spermatozoa using Laguerre-Gaussian laser modes,” J. Biomed. Opt. 15, 065010 (2010).
[115] F. Wetzel et al., “Single cell viability and impact of heating by laser absorption,” Eur.
Biophys. J. 40, 1109–1114 (2011).
[116] H. Lodish et al., Molecular Cell Biology (Freeman, New York, 2000).
[117] M. Goks¨or, J. Enger, and D. Hanstorp, “Optical manipulation in combination with
multiphoton microscopy for single-cell studies,” Appl. Opt. 43, 4831–4837 (2004).
[118] K. H. Jones and J. Senft, “An improved method to determine cell viability by simulta-
neous staining with fluorescein diacetate-propidium iodide,” J. Histochem. Cytochem.
33, 77 (1985).
[119] C. Riondet, R. Cachon, Y. Wache, G. Alcaraz, and C. Divi`es, “Measurement of the
intracellular pH in Escherichia coli with the internally conjugated fluorescent probe 5-
(and 6-) carboxyfluorescein succinimidyl ester,” Biotechnol. Tech. 11, 735–738 (1997).
[120] L. G. Chitarra, P. Breeuwer, R. W. Van Den Bulk, and T. Abee, “Rapid fluorescence
assessment of intracellular pH as a viability indicator of Clavibacter michiganensis
subsp. michiganensis,” J. Appl. Microbiol. 88, 809–816 (2000).
[121] M. J. Davies, “The oxidative environment and protein damage,” Biochim. Biophys. Acta
1703, 93–109 (2005).
[122] D. J. Jamieson, “Oxidative stress responses of the yeast Saccharomyces cerevisiae,”
Yeast (Chichester, England) 14, 1511–1527 (1998).
[123] M. S. Cooke, M. D. Evans, M. Dizdaroglu, and J. Lunec, “Oxidative DNA damage:
mechanisms, mutation, and disease,” FASEB J. 17, 1195–1214 (2003).

REFERENCES
237
[124] D. Ribeiro et al., “Singlet oxygen induced DNA damage and mutagenicity in a single-
stranded SV40-based shuttle vector,” Photochem. Photobiol. 55, 39–45 (1992).
[125] A. A. Trabanco, G. Montalban, G. Rumbles, A. G. M. Barrett, and B. M. Hoffman, “A
seco-porphyrazine: superb sensitizer for singlet oxygen generation and endoperoxide
synthesis,” Synlett 5, 1010–1012 (2000).
[126] R. Pfeifer, T. Nieminen, N. Heckenberg, and H. Rubinsztein-Dunlop, “Colloquium:
momentum of an electromagnetic wave in dielectric media,” Rev. Mod. Phys. 79, 1197–
1216 (2007).
[127] B. A. Kemp, “Resolution of the Abraham-Minkowski debate: implications for the elec-
tromagnetic wave theory of light in matter,” J. Appl. Phys. 109, 111101 (2011).


7
TISSUE POLARIMETRY
Alex Vitkin,1 Nirmalya Ghosh,2 and
Antonello de Martino3
1Medical Biophysics and Radiation Oncology, University of Toronto, Toronto,
ON, Canada
2Department of Physical Sciences, Indian Institute of Science Education and Research,
Kolkata, India
3Ecole Polytechnique, Palaiseau, France
7.1
INTRODUCTION
Although optical characterization of tissues is increasingly being studied and used
for biomedical diagnostics, in the vast majority of cases this characterization is based
on intensity measurements. However, the interaction with the sample of interest
also induces significant changes in light polarization, which also conveys useful
information about the morphology and functional state of the investigated tissue
[1–4]. Polarimetry typically provides contrasts quite different from those observed
in ordinary intensity imaging. Taken alone or in conjunction with other techniques
these contrasts may prove very useful for biomedical diagnostics.
In spite of its great potential, polarimetry is far from having reached the same
degree of maturity in the biomedical field as intensity-based techniques. One possi-
ble reason for this “lag” is the strongly depolarizing nature of almost all biological
tissues, which require sophisticated experimental setups and data treatment proce-
dures to be properly characterized. In fact, in complex random media like tissues,
numerous complexities due to multiple scattering and spatially inhomogeneous bire-
fringence, as well as simultaneous occurrences of several polarization events, present
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
239

240
TISSUE POLARIMETRY
formidable challenges for biomedical tissue polarimetry [3]. However, polarimetric
instrumentation as well as theoretical understanding and numerical simulation of
the interaction of polarized light and biological tissues are progressing impressively,
paving the way for attractive applications. Polarization can be used to discriminate
singly versus multiply scattered light. Tissue optical anisotropy and depolarization
power may be characterized quantitatively, even when these effects occur simultane-
ously. Via proper analysis and modeling, these data may ultimately provide reliable
techniques for “optical biopsy” which, in turn, may dramatically improve the man-
agement of various diseases.
In this chapter, we review polarized light fundamentals and mathematical formu-
lations of polarized light and its propagation in biological tissues, discuss advances in
various emerging polarimetric measurement systems, describe forward and inverse
problems in polarimetry of turbid media (both theoretical modeling and experimental
validation), and focus on applications related to tissue diagnosis and assessment.
7.2
POLARIZED LIGHT FUNDAMENTALS
7.2.1
Polarization States
7.2.1.1
Totally Polarized States
For a purely monochromatic optical wave of
frequency 𝜔, propagating along the z axis, its electric field E vibrates in the xy plane
according to
E(z, t) =
[
E0x cos(𝜔t −𝛽z + 𝜙x)
E0y cos(𝜔t −𝛽z + 𝜙y)
]
= Re[exp(i𝜔−i𝛽z)J],
(7.1)
with
𝛽= |β| = (n −ik)𝜔
c
(7.2)
being the modulus of the propagation vector β, c is the speed of light in vacuum,
and n and k are the real and imaginary parts of the refractive index, which determine,
respectively, the speed of light and the absorption in the medium. The amplitudes E0i
and phases 𝜙i are constants and define the Jones vector J as [5–7]
J =
[
E0x exp(i𝜙x)
E0y exp(i𝜙y)
]
.
(7.3)
The polarization of the wave—the shape of the trajectory described by E in the
xy plane—depends only on the ratio of the amplitudes tan 𝛼and the phase difference
𝜑defined, respectively, as
tan 𝜐=
E0y
E0x
and
𝜙= 𝜙y −𝜙x.
(7.4)

POLARIZED LIGHT FUNDAMENTALS
241
Ey0
Ex0
y
x
α
υ
ε
FIGURE 7.1
The polarization ellipse of a wave propagating in the z direction (toward the
observer). E0x and E0y are the amplitudes of the field oscillations along the x and y directions;
their ratio is equal to tan 𝜐. 𝛼is the azimuth of the major axis of the ellipse and 𝜀is its ellipticity.
Ellipticity is positive or negative for left- or right-handed states, respectively.
This trajectory is in general elliptical and is represented in Figure 7.1. Besides the
parameters defined in Eq. (7.4), the ellipse can also be described by the orientation
(azimuth) 𝛼of its major axis and its ellipticity 𝜀, which is positive for left handedness
and negative for right handedness. The ellipticity 𝜀varies between the two limits
of zero (linearly polarized light) and ±45◦(circularly polarized light), which thus
represent the two limits of generally elliptical polarization. Table 7.1 lists the Jones
vectors of usual polarization states (with H, V, P, and M for linear polarizations along
the horizontal, vertical, +45◦, and −45◦directions, and L and R for left and right
circular polarizations, respectively).
The intensity of a fully polarized wave characterized by the Jones vector J is given
by
I = Ix + Iy = 1
2
(E2
ox + E2
oy
) = 1
2(J ⊗J∗).
(7.5)
TABLE 7.1
Usual polarization states: Jones vectors, azimuths, ellipticities, and shapes
of the ellipses
State
H
V
P
M
L
R
Elliptical
J
[ 1
0
] [ 0
1
]
1
√
2
[ 1
1
]
1
√
2
[ 1
−1
]
1
√
2
[ 1
−i
]
1
√
2
[ 1
i
] [ cos 𝛼cos 𝜀−i sin 𝛼sin 𝜀
sin 𝛼cos 𝜀+ i cos 𝛼sin 𝜀
]
𝛼
0
90◦
45◦
−45◦
Undefined Undefined
𝛼
𝜀
0
0
0
0
45◦
−45◦
𝜀
Shape
of the
ellipse

242
TISSUE POLARIMETRY
S
L
F
A
y
x
α
ε
ε
FIGURE 7.2
Illustration of the extinction method of analysis of arbitrary elliptical polar-
izations. The initial elliptical polarization is transformed into a linear one (L) by inserting a
quarter-wave plate with its slow axis S at azimuth 𝛼. Extinction is then obtained by setting a
linear analyzer A perpendicular to L. The ellipticity 𝜀is thus measured as the angle between
the analyzer set for extinction and the fast axis F of the quarter-wave plate.
Experimentally, the usual “recipe” to determine that a light beam propagating
along z is linearly polarized along an azimuth 𝛼is to observe its extinction through
a linear analyzer set perpendicular to 𝛼. This characterization may be extended to
elliptically polarized beams as illustrated in Figure 7.2.
To determine the ellipticity 𝜀(as the beam may not be fully linearly polarized),
a quarter-wave plate (QWP) is inserted in the beam path with its slow axis at the
azimuth 𝛼. Due to the 90◦induced phase shift thus introduced, the initial incident
elliptical polarization is transformed into a linear one, oriented at 𝛼+ 𝜀from the x
reference axis. Then, a linear analyzer with its passing axis set at 𝜀from the fast axis
of the QWP will totally extinguish the beam. In practice, the extinction is achieved
by trial-and-error procedure, and the azimuth 𝛼and the ellipticity 𝜀are eventually
determined from the angular settings of the QWP and the analyzer when maximum
extinction is obtained.
7.2.1.2
Partially Polarized States
If one tries the extinction method to characterize
“natural” light directly coming from a source such as the sun or a light bulb, the
detected intensity is seen to be independent of the settings of the QWP and the
analyzer. One can thus conclude that the light coming from the sun or the light bulb
is totally depolarized.
In other cases—for example the light coming from a bulb but reflected on a plastic
floor en route to observer—the intensity detected through the QWP and the analyzer
varies between Imin and Imax. This provides an experimental definition of the degree
of polarization (DOP) of the light beam
DOP = Imax −Imin
Imax + Imin
.
(7.6)

POLARIZED LIGHT FUNDAMENTALS
243
For totally polarized states, Imin vanishes and DOP = 1. At the other extreme, for
totally unpolarized light Imin = Imax and DOP = 0. For partially polarized states, the
DOP may take any intermediate values between zero and one.
For partially polarized states, the motion of the electric field in the xy plane is no
longer a perfect ellipse, but rather a somewhat disordered one. In case of a totally
random motion of the electric vector E, one would surmise that in the extinction
procedure the analyzer would detect the same constant intensity. What is implicitly
assumed in this description is that the light polarization may be defined at any instant,
but may vary significantly over time scales much shorter than the integration time of
the detector. As a result, this detector takes the temporal averages of the intensities,
sequentially generated by different totally polarized states. While this idea is basically
correct, it is important to emphasize that this averaging of intensities (i.e., incoherent
sum) of polarized contributions is not necessarily temporal.
To this end, consider the scattering experiments schematized in Figure 7.3. In one
case (top panel), the object is optically thin and the laser undergoes single scattering
by the rough surface. Conversely, in the other case the object is optically thick and
multiple scattering is dominant. In both cases, the incident laser beam is spatially
coherent, and the scattering objects are static (we ignore for the moment any possible
thermal/Brownian motions). It is well known that in these conditions what is observed
on the screen is a speckle pattern due to the interferences, at each point of the screen,
of many scattered waves which reach this point with random (but static) amplitudes
and relative phases [8].
FIGURE 7.3
Scattering experiment of a linearly polarized coherent beam by static samples.
Top: single scattering by a thin sample. The polarization state of the speckle spots is the same
as that of the incident beam throughout the speckle pattern. Bottom: multiple scattering by
an optically thick sample. The polarization state varies from speckle to speckle. (For a color
version of this figure, see the color plate section.)

244
TISSUE POLARIMETRY
The major difference between single and multiple scattering regimes is that for the
former, the polarization of all scattered waves is the same as that of the incident laser,
while in case of multiple scattering these polarizations are random. Consequently,
as outlined in Figure 7.3, for single scattering all the speckles feature the same
polarization as the incident laser, while in the other case, each speckle is still fully
polarized, but this polarization varies randomly from one speckle to the next.
Can one thus conclude that multiple scattering always depolarizes an incident
polarized beam? The correct answer depends on how the sample is illuminated and
how the emerging light is detected.
r If the beam is coherent, the sample is static and the detection zone is smaller than
the size of the speckles, then there is no depolarization. The initial polarization
state is converted into another fully polarized one at each point of the speckle
pattern.
r If the detector is much larger than the speckle size, then if one applies the
extinction method to analyze the polarization, some speckles will exhibit varying
intensity, but randomly so (different speckles displaying different polarization
states). Thus, the overall detected signal will be constant, and according to the
criterion defined in Eq. (7.6), the detected light is totally depolarized. This is
because the large detector performs a sum of intensities of the contributions of
different polarized states (the speckles), this sum being performed spatially in
this case. On the other hand, for single scattering the same large detector would
see an extinction for suitably aligned QWP and analyzer, and the emerging light
would be considered fully polarized.
r If the illumination beam is not spatially coherent (which is generally the case
for a beam from a classical source), then the relative phases at different points
of this beam change very rapidly, and so does the resultant speckle pattern. As
a result, even if the detector is small, the polarization will vary rapidly in time,
and the measured DOP would be small.
r The same conclusion may be reached (strong depolarization at each point of
the screen) if the beam is coherent but the sample is not static, as for example a
liquid suspension of small scatterers which undergo Brownian motion. However,
the typical time constants for Brownian-motion-induced speckle variations are
much longer (ms to seconds) than those due to the lack of spatial coherence for
a light beam coming from a classical source (∼fs), and they may be temporally
resolved by many detectors.
r Finally, if the incident light is polychromatic, then each wavelength creates its
own speckle pattern, with basically no correlation between the patterns created
at different wavelengths. As no interferences are possible between waves with
different wavelengths, the incoherent intensity summation takes places naturally,
leading to depolarization in each point of the observation screen.
To summarize, “true” depolarization requires that the detected signal is the sum of
intensities due to various polarized contributions with different polarizations. This

POLARIZED LIGHT FUNDAMENTALS
245
summation may be performed temporally, spatially, or spectrally, and it depends not
only on the sample itself but also on the characteristics of the illumination beam and
of the detection system.
7.2.1.3
Stokes Vectors
Following the above presentation, polarized states are not
characterized in terms of well-determined field amplitudes, but rather from intensities
measured through various analyzers, which are in turn averages of quadratic functions
of the field amplitudes. These quantities may be arranged in various ways. The most
commonly used is the Stokes vector, which is a real four-row single-column array
defined as
S =
⎡
⎢
⎢
⎢
⎢⎣
I
Q
U
V
⎤
⎥
⎥
⎥
⎥⎦
=
⎡
⎢
⎢
⎢
⎢⎣
Ix + Iy
Ix −Iy
IP −IM
IL −IR
⎤
⎥
⎥
⎥
⎥⎦
=
⎡
⎢
⎢
⎢
⎢⎣
⟨ExE∗
x + EyE∗
y
⟩
⟨ExE∗
x −EyE∗
y
⟩
⟨ExE∗
y + EyE∗
x
⟩
⟨i(ExE∗
y −EyE∗
x)⟩
⎤
⎥
⎥
⎥
⎥⎦
,
(7.7a)
where Ix, Iy, IP, and IM are the intensities measured through ideal linear polarizers
oriented along x or y axes, or at +45◦or −45◦azimuths. IL and IR are measured
through left and right circular polarizers, respectively. It can be shown [9–12] that
Stokes vectors can define any polarization state and any DOP. The first element I is
the polarization-independent light intensity, and as such is equal to any other sum
of orthogonal intensities (e.g., IP + IM or IL + IR). Also note that S is not a vector
in the geometric space, rather this array of intensity values represent a directional
vector in the polarization state space (Poincar´e sphere, described subsequently). For
totally polarized states defined by Jones vectors of the form given by Eq. (7.3), the
corresponding Stokes vectors are
S =
⎛
⎜
⎜
⎜
⎜
⎜
⎜⎝
1
2
(E2
0x + E2
0y
)
1
2
(E2
0x −E2
0y
)
(E0xE0y cos 𝜑)
(E0xE0y sin 𝜑)
⎞
⎟
⎟
⎟
⎟
⎟
⎟⎠
.
(7.7b)
Note that the Stokes vector by its definition (Eq. 7.7a) incorporates both the pure
(totally polarized) and mixed (partially polarized) states and this definition evolves
from the so-called 2 × 2 coherency matrix N [12]. The coherency matrix is defined
through the ensemble averaged Jones vector as N = ⟨J ⊗J∗⟩and accordingly its
four elements are ⟨ExE∗
x⟩, ⟨ExEy∗⟩, ⟨EyE∗
x⟩, and ⟨EyE∗
y⟩[12]. Conversely, it follows
that the Stokes vector elements (I, Q, U, and V) are the expansion coefficients of the
coherency matrix in terms of the identity matrix and the three Pauli spin matrices
[12].

246
TISSUE POLARIMETRY
TABLE 7.2
Normalized Stokes vectors for usual totally polarized states (cf Table 7.1)
State
H
V
P
M
L
R
Elliptical
S
⎡
⎢
⎢
⎢⎣
1
1
0
0
⎤
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢⎣
1
−1
0
0
⎤
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢⎣
1
0
1
0
⎤
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢⎣
1
0
−1
0
⎤
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢⎣
1
0
0
1
⎤
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢⎣
1
0
0
−1
⎤
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢⎣
1
cos 2𝛼cos 2𝜀
sin 2𝛼cos 2𝜀
sin 2𝜀
⎤
⎥
⎥
⎥⎦
Stokes vectors can be written in normalized form as
ST = I
(
1, Q
I , U
I , V
I
)
= I(1, q, u, v) = I(1, sT),
(7.7c)
where the superscript T means the transpose. The three intensity-normalized coor-
dinates q, u, and v are the quantities which actually define the polarization state
independently of the total intensity I. The normalized Stokes vectors for usual fully
polarized states are listed in Table 7.2.
At the other extreme, for totally unpolarized states, Q = U = V = 0, which
corresponds to the fact that no matter how the analyzer is oriented, for such states the
transmitted intensity is always the same, equal to one half of the total intensity.
The DOP defined experimentally in Eq. (7.6) can be expressed in terms of the
normalized Stokes vector components as
DOP =
√
q2 + u2 + v2.
(7.8a)
It is straightforward to check that for Stokes vectors of the form (7.7b) we get
DOP = 1. In addition to the overall DOP, we can define the linear and circular DOPs,
respectively, as
DOPL =
√
q2 + u2
and
DOPC = v.
(7.8b)
Experimentally, the DOPL can be measured by the extinction method using only
a rotatable linear polarizer, without the QWP. Equation (7.6) then applies with Imax
and Imin, representing the maximum and minimum intensities measured when the
polarizer is rotated. On the other hand, DOPC can be measured by using left and
right circular polarizers. Calling again IL and IR the intensities measured through
these circular polarizers, DOPC is given by the same formula (7.6) but with IL and IR
replacing Imin and Imax.
Physical realizability. In contrast with Jones vectors, which could be any 2D com-
plex vectors, any four-dimensional element real array is not necessarily an acceptable
Stokes vector, due to the condition
DOP ≤1.
(7.9)

POLARIZED LIGHT FUNDAMENTALS
247
L
Constant
ellipticity
Constant
azimuth
M
H
R
q
P
A
V
2
2
B
v
u
α
ε
FIGURE 7.4
Geometrical representation of Stokes vectors within the Poincar´e sphere. Any
given polarization state is represented by a point whose Cartesian coordinates are the intensity-
normalized coordinates (q, u, v). The radial coordinate is the DOP and the “longitude” and
“latitude” are, respectively, 2𝛼and 2𝜀. Totally polarized states are found at the surface of the
unit radius sphere, while partially polarized states are inside (e.g., points A and B, respectively).
Linearly polarized states, among which H, V, P, and M states, are on the “equator” while the
L and R circular states are found at the “poles.”
The Poincar´e sphere. A very convenient geometrical representation of all possible
polarization states involves the intensity-normalized coordinates q, u, and v defined
above, as illustrated in Figure 7.4 [12]. In this space, the DOP is nothing else but
the distance of the representative point from origin. Thus, the physical realizability
condition given by Eq. (7.9) implies that all acceptable Stokes vectors are represented
by points located within the unit radius sphere, also called the Poincar´e sphere.
Totally polarized states are found at the surface of the sphere (point A) while partially
polarized states are inside (point B). The other spherical coordinates, the points
“latitude” and “longitude” are nothing else but twice the azimuth 𝛼and ellipticity
𝜀, as shown by the last column of Table 7.2 for totally polarized states. As shown
throughout this chapter, this geometrical representation provides simple and intuitive
descriptions of many aspects of the interaction between polarized light and samples
and/or instruments.
Before we move on to define sample polarization interactions through this “con-
ventional” Stokes–Mueller algebra, it may be worthwhile to briefly mention the
validity regime of such algebra. Note that both the Jones vector (in the field repre-
sentation, Eq. 7.3) and the Stokes vector (intensity-based representation, Eq. 7.7a)
deal with two-dimensional electromagnetic field and are applicable when the light
wave is completely transverse (plane electromagnetic waves or more generally to
uniformly polarized elementary beams). However, it has been shown that even for
paraxial beam-like fields, the spatial mode (field distribution) and polarization are

248
TISSUE POLARIMETRY
not always separable (unlike plane waves or elementary beams), and accordingly
one needs different algebra to describe such inhomogeneous polarization [13]. This
so-called classical entanglement between polarization and spatial mode is handled
by defining beam coherency polarization matrix (a variant of the 2 × 2 coherency
matrix incorporating simultaneously both the field polarization and its spatial distri-
bution) [13]. In other general cases involving three-dimensional field (as encountered
in tight focusing scenarios and in the near field), the two-dimensional polarimetry
formalisms have been extended via the definition of 3 × 3 coherency matrix and
generalized nine-element Stokes vector [14]. Moreover, there are other emerging
“unconventional” polarization algebra formulations involving vector beams, geo-
metric phases (Pancharatnam–Berry phase) arising from spin orbit interactions of
light, radial, and azimuthal polarization of light beams, and so forth [15–17]. Even
though such generalized polarization algebra may find useful biomedical applica-
tions; these have yet not been explored in tissue polarimetry research (possibly due to
the unknown magnitude/relative importance of these effects in tissues, and possibly
due to the numerous complexities in tissue signal detection and analysis). We thus
restrict our discussion to the “conventional” polarization algebra and its applications
in tissue polarimetry.
7.2.2
Interaction with a Sample
7.2.2.1
Mueller Matrices
For any optical system operating in the linear regime
(which is always the case except when the light source is a high power pulsed laser),
the output intensities are linear functions of the inputs. As a result, the transformation
of the Stokes vectors must also be linear, thus described by a 4 × 4 real matrix M
called Mueller matrix [18]:
Sout = M ∙Sin.
(7.10)
In analogy with S, which represents the polarization state of the beam, M thus
represents the corresponding polarization properties of the sample; the link between
the two is accomplished via the Mueller–Stokes calculus as described by Eq. (7.10).
Physical realizability. Analogous to Stokes vectors, any 4 × 4 real matrix is
not necessarily a physically acceptable Mueller matrix. An obviously necessary
condition is that in never transforms a physically acceptable Stokes vector Sin into
an “overpolarized” one Sout, with DOP > 1. However, this condition is necessary but
not sufficient. A general procedure for determining the acceptability of M, based on
the calculation of the so-called coherency matrix N [12, 19], is given in Reference
20 and is outside the scope of this review. This procedure may also be used to
“correct” a slightly unphysical matrix (possibly due to measurement errors) to make
it acceptable.
Depolarizing or nondepolarizing character. A physically acceptable Mueller
matrix is said to be nondepolarizing if any input totally polarized Stokes vector
Sin is transformed into a totally polarized Sout. (in other words, the DOP does not
decrease). Necessary and sufficient conditions for a matrix to be nondepolarizing

POLARIZED LIGHT FUNDAMENTALS
249
have been examined [20,21]. Conversely, for depolarizing Mueller matrices the DOP
of Sout is smaller than that of Sin. It is important to note that, for any resulting Mueller
matrix, the reduction of the DOP depends on the input state Sin; for example, a given
tissue sample in a particular examination geometry can be more depolarizing for
input circular states than for input linear ones, or vice versa. It is therefore impossible
to uniquely define the depolarization power of a generic Mueller matrix M.
Among the various definitions of this depolarization power of a sample described
by M, one very widely used is the “depolarization index” Pd [22]:
Pd =
√
√
√
√
√
√
∑
i,j
M2
i,j −M2
1,1
3M2
1,1
=
√
√
√
√Tr(MTM) −M2
1,1
3M2
1,1
,
(7.11)
which varies from 0 for a total depolarizer to 1 for nondepolarizing matrices.
7.2.2.2
Diattenuation and Polarizance
These two properties can be determined
unambiguously for any Mueller matrix M from its very definition in terms of inten-
sities. Quite generally, M can be rewritten as
M = m11
( 1
DT
P
m
)
,
(7.12)
where D (first row) and P (first column) are the diattenuation and polarizance vectors,
respectively, while m is a 3 × 3 real matrix. These formulations are easily justified
as follows.
Diattenuation. For a diattenuating system, the output intensity depends on the
polarization of the incident wave. If we consider an intensity-normalized input Stokes
vector S
ST
in = (1, sT)
with
‖s‖ = DOP ≤1
(7.13)
corresponding to arbitrary polarizations at constant intensity (normalized to 1), then
the output intensity (i.e., the first component of Sout) is simply given by
Iout = m11(1 + D ⋅s).
(7.14)
This output intensity reaches its maximum (resp. minimum) value Imax (resp. Imin)
when the scalar product D ⋅s is maximum (resp. minimum) under the constraint
‖s‖ = DOP ≤1, that is, when s = ± D
‖D‖. We thus obtain
ST
max =
(
1, DT
‖D‖
)
and
Imax = m11(1 + ‖D‖)
(7.15a)
ST
min =
(
1, −DT
‖D‖
)
,
and
Imin = m11(1 −‖D‖)
(7.15b)

250
TISSUE POLARIMETRY
from which we immediately get for the scalar diattenuation D
D = Imax −Imin
Imax + Imin
= ‖D‖.
(7.16)
In summary, the diattenuator vector D defines both the scalar diattenuation D
and the polarization states transmitted with the largest (or the lowest) intensity. Note
that the two polarization states providing these extreme intensity transmission values
are totally polarized, and located at diametrally opposite positions on the Poincar´e
sphere.
Polarizance. We now consider this property which corresponds to the capacity
of the sample to polarize incoming beams, specifically to impart a finite DOP to an
unpolarized input beam. In this case the Stokes vector of the output beam is simply
Sout = m11
( 1
DT
P
m
) ⎛
⎜
⎜
⎜⎝
I
0
0
0
⎞
⎟
⎟
⎟⎠
= m11I
(
1
P
)
.
(7.17)
In a way analogous to diattenuation, from the polarizance vector P we can define
a scalar polarizance P = ‖P‖,
Independence of polarizance and diattenuation. In practice, diattenuation
and/or polarizance may occur
r at interfaces, where the transmission and/or reflections coefficients may be dif-
ferent for the polarization in the plane of incidence (p polarization) or orthogonal
to this plane (s polarization), due to the well-known Fresnel laws [23]: trans-
mission is larger (and reflection is smaller) for p than for s polarization; and
r during the propagation in bulk anisotropic materials. In this case, diattenuation
and/or polarizance is due to the dependence of the imaginary part of the optical
refractive index on the light polarization.
For the simple cases cited above, the polarizance is directly related to the diatten-
uation: P = D. For more complex systems, and in particular depolarizing ones, P and
D become totally independent parameters, as shown by the following examples:
1. Consider a polarizer followed by a perfect depolarizer. Such a system clearly
exhibits diattenuation, as the intensity transmitted by the polarizer depends
on the orientation of the incoming linear polarization. However, the beam
emerging from the depolarizer is, by definition, totally depolarized, implying
that an initially depolarized beam would remain totally depolarized. As a result,
such a system exhibits a strong diattenuation but no polarizance.
2. Let us now consider the same elementary components, but in reverse order
(depolarizer first, polarizer afterwards). In this case, any incoming polarized
beam is transformed by the depolarizer into a totally depolarized beam whose

POLARIZED LIGHT FUNDAMENTALS
251
Grazing
view
–0.2
0.1
0.4
FIGURE 7.5
Normalized Mueller matrix image of a resected human cervix in backscattering
geometry. The field of view is 3 × 3 cm2. The sample is about 5 mm thick and has conical
shape. All elements are normalized by m11 and these normalized values are given in the color
scale shown at top right, ranging from −0.2 to 0.4. This sample features no diattenuation, as
shown by the vanishing elements in the first row, and significant linear polarizance (elements
m21 and m31) at some points where the light emerges at grazing angles from the sample surface,
and is polarized by a local “Brewster angle” effect (A. de Martino, unpublished results). (For
a color version of this figure, see the color plate section.)
intensity does not depend on the incoming polarization. Then, the polarizer
transforms this beam into a polarized one, with always the same polarization.
In contrast with the previous case, now the system exhibits zero diattenuation
but a strong polarizance (P = 1 for a perfect output polarizer).
While such effects may seem purely “academic,” they can actually occur in real
systems, and more particularly in biological samples. Figure 7.5 shows a Mueller
image of an ex vivo surgical sample of uterine cervix, normalized by m11. On this
image, some narrow regions, close to the edge of the sample, clearly exhibit nonzero
linear polarizance (elements m21 and m31), without any detectable counterpart in the
diattenuation (elements m12 and m13). As indicated in the figure, these regions are
seen under grazing angles. Then, as outlined above, the light component polarized
parallel to the local surface normal (the local p polarized component) is much better
transmitted at the air–tissue interface than the component polarized parallel to the
surface itself (the local s polarized component). As a result, the light emerging from
these regions is strongly polarized even if the sample is illuminated with unpolarized
light. On the other hand, the main contribution to this emerging light is due to rays
impinging on the samples in other places, at incidences closer to normal and then
multiply scattered. As a result, there is little or no diattenuation effect for these
impinging rays and the situation is quite similar to the case 2 outlined above.

252
TISSUE POLARIMETRY
7.2.2.3
Homogeneous Diattenuators and Retarders
Polarimetric elements are
called homogeneous if they exhibit two fully polarized orthogonal eigenstates, that
is, two polarization states which are transmitted without alteration and which do not
interfere with each other. In practice, such light states are linearly polarized along
two perpendicular directions, or circularly polarized and rotating in opposite senses.
The normalized Stokes vectors S1 and S2 of such orthogonal states are of the form
ST
1 = (1, sT), ST
2 = (1, −sT)
(7.18)
with ‖s‖ = 1, as these states are fully polarized. Orthogonal states are thus found
on the surface of the Poincar´e sphere at diametrally opposed positions. For any
homogeneous polarimetric element, there are thus two (and only two) such states
which are left invariant on the Poincar´e sphere.
Homogeneous diattenuators: These elements are uniquely determined by their
diattenuation vector D. Their (totally polarized) eigenpolarization states correspond-
ing, respectively, to maximum and minimum transmissions are given by Eqs. (7.15)
with
smax = D
D,
smin = −D
D.
(7.19)
The corresponding Mueller matrix is then given in synthetic form by [24]
MD = 𝜏
( 1
DT
D
md
)
where mD =
√
1 −D2I3 + (1 −
√
1 −D2)D
DT,
(7.20)
where 𝜏is the intensity transmission for incident unpolarized light. As already men-
tioned, diattenuation may occur due to reflection and/or refraction at an interface, or
to propagation in anisotropic or chiral materials. Anisotropy may introduce linear
dichroism. If so, for any propagation direction (except very particular ones deter-
mined by the symmetry of the material) the imaginary part of the refractive index
k may take two different values, kL and kH; The former, corresponding the lowest
absorption, is valid for a wave linearly polarized at azimuth 𝜃and the latter for the
orthogonal polarization, at 𝜃+ 90◦. The linear (scalar) dichroism is then defined as
Δk = kH −kL > 0.
(7.21)
For a parallel slab of thickness L, the intensity transmissions for the two eigenpo-
larizations are respectively
Tmax = exp(−2kLL),
Tmin = exp(−2kHL)
(7.22)
resulting in a scalar diattenuation
D = Tmax −Tmin
Tmax + Tmin
= sinh(d)
(7.23)
where d = ΔkL is the dichroism integrated over the slab thickness L.

POLARIZED LIGHT FUNDAMENTALS
253
Chiral media (e.g., a biological fluid with glucose) can also feature dichroism, but
then it is circular dichroism. The above formulas are still valid, but in this case the
eigenpolarizations for which the absorption coefficients are well defined are left and
right circular ones.
The matrix of a homogeneous diattenuator is symmetric, explicitly written for a
linear diattenuator as [12]:
MLD(𝜏, D, 𝜃)
= 𝜏
2
⎛
⎜
⎜
⎜
⎜
⎜⎝
1
D cos 2𝜃
D sin 2𝜃
0
D cos 2𝜃
cos2 2𝜃+
√
1 −D2 sin2 2𝜃
(1 −
√
1 −D2) cos 2𝜃sin 2𝜃
0
D sin 2𝜃
(1 −
√
1 −D2) cos 2𝜃sin 2𝜃
sin2(2𝜃) +
√
1 −D2 cos2 2𝜃
0
0
0
0
√
1 −D2
⎞
⎟
⎟
⎟
⎟
⎟⎠
(7.24)
implying that the maximum and minimum intensity transmittances are obtained for
linearly polarized states with azimuths 𝜃and 𝜃+ 90◦. A straightforward calculation
indeed shows that
MLD(𝜏, D, 𝜃)
⎛
⎜
⎜
⎜
⎜⎝
1
cos 2𝜃
sin 2𝜃
0
⎞
⎟
⎟
⎟
⎟⎠
= 𝜏
2 (1 + D)
⎛
⎜
⎜
⎜
⎜⎝
1
cos 2𝜃
sin 2𝜃
0
⎞
⎟
⎟
⎟
⎟⎠
and
MLD(𝜏, D, 𝜃)
⎛
⎜
⎜
⎜
⎜⎝
1
−cos 2𝜃
−sin 2𝜃
0
⎞
⎟
⎟
⎟
⎟⎠
= 𝜏
2 (1 −D)
⎛
⎜
⎜
⎜
⎜⎝
1
−cos 2𝜃
−sin 2𝜃
0
⎞
⎟
⎟
⎟
⎟⎠
,
(7.25)
which is a direct check that these states are unchanged by MLD(𝜏,D,𝜃), but are trans-
mitted with intensity factors 𝜏
2 (1 ± D). Similarly, for circular diattenuators the corre-
sponding matrix is
MCD(𝜏, D) = 𝜏
2
⎛
⎜
⎜
⎜
⎜
⎜⎝
1
0
0
D
0
√
1 −D2
0
0
0
0
√
1 −D2
0
D
0
0
1
⎞
⎟
⎟
⎟
⎟
⎟⎠
(7.26)
and of course in this case there is no need to define any particular azimuth 𝜃.
Homogeneous retarders: For these elements too there are two orthogonal eigen-
polarization states, each of which is transmitted without modification of its shape. In
contrast with diattenuators, retarders transmit both eigenstates with the same intensity
coefficients, but different phases. This phase difference is the scalar retardation 𝛿.

254
TISSUE POLARIMETRY
In practice, like diattenuation, retardation may be caused by reflection and/or
refraction at an interface, or by propagation in anisotropic materials. This latter
case is particularly relevant for tissue polarimetry. Anisotropic tissues are typically
connective tissues, with fibrillar proteins (collagen is the prime example) which may
be spatially organized with a preferential orientation. Then if we consider a light
wave propagating along z, and call 𝜃the azimuth of the fiber orientation direction in
the xy plane, the refractive index is larger for light linearly polarized at azimuth 𝜃
than at 𝜃+ 90◦. These azimuths then define the slow (𝜃) and fast (𝜃= 90◦) axes in the
xy plane, with refractive indices nS and nF. The linear birefringence of the material
is then defined as
Δn = nS −nF.
(7.27)
Propagation over a distance L in such a material introduces a scalar retardation 𝛿
(in radians)
𝛿= 2𝜋Δn L
𝜆
.
(7.28)
For homogeneous retarders, the orthogonal Stokes eigenvectors are again of the
form given by Eq. (7.18). A pure retarder can be described geometrically as rotation
in the space of Stokes vectors. Mathematically the Mueller matrix MR of the retarder
can be written in compact notation as [24]
MR =
( 1
0T
0
mR
)
,
(7.29)
where 0 represents the null vector and the 3 × 3 submatrix, mR, is a rotation matrix
in the (q, u, v) space. As a result, the action of a homogeneous diattenuator on an
arbitrary incident Stokes vector S is a mere rotation of its representative point on the
Poincar´e sphere, described by mR. Moreover, the axis of this rotation is defined by
the two diametrally opposed points representing the two eigenpolarizations, and the
rotation angle is nothing else but the retardation 𝛿[12].
For linear retarders with eigenstates linearly polarized along 𝜃and 𝜃+90◦
azimuths, the Mueller matrices are [12]
MLR(𝜏, 𝛿, 𝜃)
= 𝜏
⎛
⎜
⎜
⎜
⎜⎝
1
0
0
0
0
cos2 2𝜃+ sin2 2𝜃cos 𝛿
cos 2𝜃sin 2𝜃(1 −cos 𝛿)
−sin 2𝜃sin 𝛿
0
cos 2𝜃sin 2𝜃(1 −cos 𝛿)
sin2 2𝜃+ cos2 2𝜃cos 𝛿
cos 2𝜃sin 𝛿
0
sin 2𝜃sin 𝛿
−cos 2𝜃sin 𝛿
cos 𝛿
⎞
⎟
⎟
⎟
⎟⎠
(7.30)
And Eq. (7.25) is still valid, provided both intensity transmittance factors 𝜏
2(1 ± D)
are replaced by 𝜏.

POLARIZED LIGHT FUNDAMENTALS
255
We now consider circular retarders, that is, elements for which the eigenpolariza-
tions are counter-rotating circular ones. The Mueller matrices of such elements are
of the form:
MCR(𝜓) = 𝜏
⎛
⎜
⎜
⎜
⎜⎝
1
0
0
0
0
cos 2𝜓
−sin 2𝜓
0
0
sin 2𝜓
cos 2𝜓
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟⎠
.
(7.31)
When a linearly polarized wave interacts with a circular retarder, its polarization
remains linear, but it is rotated by an angle 𝜓which is nothing else but the circular
retardance, as it can be checked by a straightforward calculation analogous to that
shown in Eq. (7.25). Circular birefringence is observed in media lacking any mirror
symmetry, like solutions of chiral molecules where only one enantiomer is present.
An example of major practical clinical importance is that of glucose, as discussed in
greater detail later. Chiral media are usually characterized by their optical activity 𝜒
rather than their circular birefringence Δnc:
𝜒= 2𝜋
𝜆Δn
(7.32a)
so that the optical rotation 𝜓due to propagation over a distance L is simply
𝜓= 𝜒L.
(7.32b)
We point out, however, that in tissues (like in essentially any usual materials,
except for example some particular liquid crystals) optical activity is an extremely
weak effect, much weaker than linear birefringence whenever this latter effect is
present.
Finally, we point out that the scalar retardation of any homogeneous retarder
(linear or circular) is easily determined from its Mueller matrix MR as
𝛿, 𝜓= cos−1
(Tr(MR)
2
−1
)
.
(7.33)
7.2.2.4
Depolarizers
By definition, a depolarizer is an object that reduces the
DOP of the incoming light. The conceptually simplest (and most relevant in practice)
depolarizers are those for which the Mueller matrix MΔ is diagonal
MΔ = 𝜏
⎛
⎜
⎜
⎜
⎜⎝
1
0
0
0
0
a
0
0
0
0
b
0
0
0
0
c
⎞
⎟
⎟
⎟
⎟⎠
(7.34)

256
TISSUE POLARIMETRY
with absolute values of a, b, and c smaller than 1. If so, any incident Stokes vector Si
of the form
ST
i = I(1, q, u, v)
(7.35a)
is transformed into
ST
out = 𝜏I(1, aq, bu, cv)
(7.35b)
which implies that
DOPout =
√
a2q2 + b2u2 + c2v2 ≤
√
q2 + u2 + v2 = DOPin.
(7.35c)
In the geometrical representation, the action of a depolarizer defined in (7.34) is
to “pull” the representative point of the incoming Stokes vector toward the origin. As
a result, the Poincar´e sphere is transformed into an ellipsoid limited by the segments
[−a,a], [−b,b], and [−c,c] along the q, u, and v axes.
As discussed in Section 7.2.1.2, depolarization occurs due to incoherent addition
of intensities of polarized states with different polarizations. In tissues, depolariza-
tion is due to multiple scattering in the first place, together with spatially varying
linear birefringence in connective tissues [4]. However, these effects alone are not
sufficient to cause “real” depolarization, but would give rise to a speckle pattern with
DOP = 1 everywhere but with different polarizations from one point to another.
“True” depolarization occurs if this speckle pattern is “blurred” by motion of the
scattering sample, lack of spatial coherence of the illumination beam, sample motion
(e.g., blood flow during in vivo measurements), and the like.
Depolarization is a major polarimetric effect in virtually all tissues (with the
noticeable exceptions of eye cornea and aqueous and vitreous humors). In the absence
of strong optical anisotropy, the tissue can be viewed as a suspension of isotropic
scatterers. Then
r If the incident light is linearly polarized along the azimuth 𝛼, the reduction of
DOP (also called the depolarization power) is independent of 𝛼. A necessary
and sufficient condition for this is a = b in Eq. (7.34). Moreover, as there is no
reason why the azimuth of the “blurred” ellipse should change, a and b must be
positive, and thus finally comprised between 0 and 1.
r If the incident light is circularly polarized, then the depolarization effect is
described by the c diagonal term. This term is always positive in forward
scattering, and also in the vast majority of cases of backward scattering, where
the handedness is reversed with respect to the incident one, as it occurs for a
reflection on a mirror [25]. However, c may sometimes take on negative values
in other geometries, and more particularly in backward scattering [26].This
unusual behavior may be observed when each individual scattering process takes
place at small angles from the forward direction, and the overall backscattering
process is dominated by “U turns” with large radii of curvature and circular
polarizations which follow these turns “adiabatically” [27].

POLARIZED LIGHT FUNDAMENTALS
257
r There is no general relationship between a = b and c. As discussed in more
detail in Section 7.4.1.2 when the size of the scatterers is much smaller than
the optical wavelength (Rayleigh scattering regime) then a = b > |c|, resulting
in stronger depolarization for circularly than linearly polarized incident light.
Conversely, when the scatterers are larger than the wavelength, the opposite
holds (a = b < c). As a result, comparison of linear and circular depolarizing
powers provides useful information about the average size of the scatterers.
In practice, the vast majority of tissues behave as Rayleigh or Rayleigh-Gans
scatterers [28–31], at least with respect to their polarization response.
In the most general case, the Mueller matrix of a depolarizer, MΔ, is given in
compact notation as [24]
MΔ =
( 1
0T
0
mΔ
)
,
(7.36)
where mΔ is a 3 × 3 real symmetric matrix. This matrix can be diagonalized to recover
the form given by Eq. (7.34) where the eigenvalues a, b, and c are real numbers varying
between −1 and 1. Thus, the Mueller matrix of the most general depolarizer depends
on six parameters (as it can be seen from the very definition of the mΔ matrix as
a 3 × 3 symmetric matrix, or by the fact that the diagonalization process involves
not only the three eigenvalues, but also the basis formed by the eigenvectors of mΔ.
General depolarizers are thus rather complex mathematical objects, this complexity
being related to situations like multiple scattering in anisotropic media. Here we will
not discuss the properties of general depolarizers any further, and in the following
we will only consider depolarizers of the form given by Eq. (7.34). We thus define
the depolarizing power of such samples as
Δ = 1 −1
3(|a| + |b| + |c|)
(7.37a)
which can be further specialized in depolarizing powers for linear and circular polar-
izations as
ΔL = 1 −1
2(|a| + |b|)
and
ΔC = 1 −|c|.
(7.37b)
As a final remark, the depolarization powers Δ defined in Eqs. (7.35) and (7.36)
vary from 0, for nondepolarizing samples, to 1 for totally depolarizing ones, while
the opposite holds for the general depolarization index Pd defined in Eq. (7.11).
7.2.3
Decompositions into “Elementary” Component Matrices
Ideally, any polarimetric measurement, once obtained, should be interpreted by fitting
to the measured Mueller matrices the results of numerical simulations based on a
relevant model. Unfortunately this is not always possible, especially with complex

258
TISSUE POLARIMETRY
and/or disordered samples such as biological tissues. These exhibit depolarization
in virtually all experimental conditions and have several of the above-described
polarization effects occurring simultaneously, making accurate measurements and
interpretation of the measured data difficult. The use of Mueller polarimetry followed
by additional methods of results extraction and interpretation is thus essential in tissue
studies.
It is thus desirable to develop and use novel approaches to “classify” and inter-
pret the measured tissue Mueller polarimetry data, for example by determining its
retardance and depolarization properties (in addition to the immediately and unequiv-
ocally accessible diattenuation and polarizance). Unfortunately, retardance and depo-
larization cannot be uniquely determined from a given Mueller matrix, except in very
special cases of “pure” retarders or depolarizers.
The most usual way to process complex Mueller matrices then consists of decom-
posing them into products of elementary matrices with well-defined polarimetric
properties and gleans quantitative parameters, meaning biophysical metrics from
these. In the following, we first briefly list the form of these elementary matrices, and
then outline the most commonly used procedures to decompose any Mueller matrix
into various combinations (generally products) of elementary matrices. Note that
these mathematical analysis methods do not imply that the sequence of polarization
alterations, as described by a particular decomposition product, actually happens in
tissue; rather, we are simply stating that the end result of the proposed decomposition
sequence is “equivalent” to the experimentally measured one and lends itself to easier
extraction of meaningful biophysical quantities.
Product decompositions thus represent an arbitrary Mueller matrix as a product
of elementary Mueller matrices—diattenuators, retarders, and depolarizers. These
decompositions are characterized by the number of elementary components and their
respective positions in the multiplication. The order of the components is important
since generally depolarizer matrices commute neither with diattenuator nor with
retarder matrices. In principle, product decompositions are better suited to describe
physical situations in which the beam interacts sequentially with different parts of the
sample, each of which is characterized by a well-defined fundamental polarization
property. Although this generally does not happen in biological tissues (where the
effects more likely occur simultaneously), product decompositions have nevertheless
been shown to yield accurate and reliable measures of tissue polarization properties.
7.2.3.1
Forward and Reverse Product Decompositions into Three Factors
All
these decompositions describe the input matrix M as a product of a diattenuator, a
retarder, and a depolarizer. Actually with three elementary component types, there
are six different possible orders that yield slightly different results, as in general,
matrices do not commute [32]. Among these, the most widely used choice is that of
Lu and Chipman [24]:
M = MΔPMR.MD,
(7.38)
where the “special” symbol MΔP has been used for the depolarizer. Actually, for
this decomposition to be quite general if MD and MR represent homogeneous

POLARIZED LIGHT FUNDAMENTALS
259
diattenuators and retarders, then the depolarizer cannot be a “pure” depolarizer of
the form defined in Eq. (7.34), as the product matrix M would exhibit no polarizance
and three parameters would be missing. As a result, in general the “depolarizer” has
nonzero polarizance and its elementary matrix is of the form
MΔ P =
( 1
0T
P
mΔ
)
.
(7.39)
With these assumptions, the approach is numerically stable and always provides
physically realizable elementary matrices MΔP, MR, and MD. This procedure is
thus very convenient and is widely used for the phenomenological interpretation of
experimental (or even simulated) Mueller matrices. Moreover, its accuracy has been
checked for well-characterized controlled systems (phantoms) as described in detail
in Section 7.4.
What happens if the order of the elementary components is changed [32]? Simple
matrix algebra shows that the above results are easily generalized to two out of
the other five possible product sequences, namely those for which the diattenuator
precedes the depolarizer:
M = M′
RM′
ΔPM′
D or M = M′′
ΔP M′′
DM′′
R.
(7.40)
More precisely, the depolarizer matrices keep the form defined in Eq. (7.39) and the
M′ and M
′′ matrices are deduced from those provided by the standard decomposition
(7.38) by unitary transformations.
This kind of simple generalization is no longer valid for the three remaining cases
in which the depolarizer precedes the diattenuator. Morio and Goudail [33] introduced
a “reverse” decomposition procedure for these three cases with the same definition of
the depolarizer, but this procedure could lead to unstable or even unphysical results
in case of very strong depolarizations. This issue has been solved by Ossikovski
et al. [34] assuming that when the depolarizer precedes the diattenuator, the former
features zero polarizance. The “standard” reverse decomposition takes then the form
M = MDMRMD′Δ,
(7.41)
with a depolarizer matrix of the form
MD′Δ =
(
1
D′T
0
mΔ
)
.
(7.42)
As in the case of “direct” decompositions defined by Eqs. (7.38) and (7.40), the
matrices of the three possible “reverse” cases (with the depolarizer preceding the
diattenuator) are deduced from one another by simple orthogonal transformations.

260
TISSUE POLARIMETRY
Rev.
LC
0
0.07
0.7
Δ
1
D
FIGURE 7.6
Polarimetric images of a pig skin sample in backscattering geometry taken at
600 nm. Field of view 5 × 5 cm2. Top: diattenuation and depolarization images obtained with
the reverse decomposition via Eq. (7.41). Bottom: diattenuation and depolarization images
obtained with the Lu-Chipman decomposition. Color bars give the scales for scalar diattenu-
ation D (left) and depolarization power Δ (right). Adapted from Reference 35. (For a color
version of this figure, see the color plate section.)
The reverse decomposition procedure is also stable and always provides physically
realizable Mueller matrices for the elementary components.
Figure 7.6 shows an example of the results provided by direct and reverse decom-
positions on a pig skin sample. The depolarization images are almost identical, which
support the intuitive assumption that depolarization is due to multiple scatterings
occurring inside the sample. Conversely, the diattenuation images are quite different
for the two decompositions, the reverse one giving sharper details and larger absolute
values. As the diattenuation is likely to be due to interface effects (diattenuation inside
biological tissues is typically very small in the visible, as absorption is essentially
due to hemoglobin, which is microscopically isotropic), the reverse decomposition
apparently yield a better image of such effects, with a clearer correlation to the details
of the depolarization image. These details are absent in the diattenuation image pro-
vided by the forward Lu-Chipman analysis (Eq. 7.38). The reverse decomposition
seems a priori better suited to characterize the surface of such samples.
In summary, three-factor product decompositions may be very useful when study-
ing turbid media, as they yield numerically stable and accurate information about the
samples, provided these samples can be reasonably described as stacks comprising
a depolarizer (typically the turbid medium), a retarder and a diattenuator in a given
order. For isotropic tissues, the latter two effects are expected to occur essentially at
the sample surface, while for samples like striated muscles birefringence is expected
to arise from the bulk of the sample; consequently, three-factor decompositions may

POLARIZED LIGHT FUNDAMENTALS
261
not be necessarily well suited to all cases, a situation that justified the development
of the alternative decompositions we outline in the following sections. In general
though, it must be borne in mind that whatever decomposition approach is chosen,
the claim is not that its physical embodiment is “what actually happens in tissue”;
rather, this simply provides a mathematically “equivalent” framework that enables
easier extraction of meaningful (and hopefully accurate) biophysical metrics that are
contained but otherwise hidden in the complex Mueller matrix of biological tissues.
7.2.3.2
Symmetric Decomposition
In symmetric decompositions, the input
Mueller matrix M is decomposed into a product of five factors [36,37]:
M = MD2.MR2.MdΔ ⋅MR1.MD1,
(7.43)
where MD1 and MD2 represent homogeneous diattenuators, MR1 and MR2 homo-
geneous retarders, and MdΔ a diagonal depolarizer as per Eq. (7.34). The central
position of the depolarizer in the symmetric decomposition can be very useful for
samples which can be viewed as purely depolarizing media bounded by tilted input
and output interfaces: in this case the diattenuation and retardation effects are likely
to occur at the output interfaces and the depolarization in between. Moreover, in many
cases of practical interest the Mueller matrix of the depolarizer is indeed diagonal.
However, the relevance of this decomposition for tissue polarimetry may be limited
by two issues which may frequently in practice:
r As discussed in Section 7.2.2.4, the Mueller matrices of depolarizers consisting
of spherical scatterers are diagonal (Eq. 7.34) with a = b. In presence of such
“degeneracy” of the depolarizer eigenvalues, the Mueller matrices MR1 and
MR2 of the retarders commute with MdΔ (i.e., MRiMdΔ = MdΔ MR1) and thus
cannot be unambiguously determined (only their product is unambiguous), and
some a priori knowledge of these “retarders” is required. This issue has been
raised in a work devoted to experimental validation of this decomposition [38].
r In contrast with the decompositions into three factors, the symmetric decompo-
sitions are not “universal,” as there is a class of Mueller matrices, the so-called
“non-Stokes diagonalizable” Mueller matrices, for which these decompositions
cannot be applied. For such matrices there is one, and only one totally polar-
ized input Stokes vector which retains its full polarization (DOP = 1) after
transformation by the Mueller matrix [38, 39]. Analogous to the degenerate
depolarizers, such matrices are not some artificial theoretical “curiosities,” but
do occur in nature (e.g., in scattering cholesteric structures such as shiny beetle
cuticles, observed in backscattering [40]).
7.2.3.3
Logarithmic Decomposition
The logarithmic decomposition was pro-
posed recently as a complementary alternative to the standard product decompositions
for spatially homogeneous systems where the polarimetric effects occur simultane-
ously and “continuously” during the propagation of light and not sequentially as

262
TISSUE POLARIMETRY
implied by the product approaches above. This decomposition is a natural gener-
alization [41, 42] of the classic differential matrix formalism [43] applied to the
depolarizing case. The approach, based on the physical picture of a continuously
distributed depolarization, parallels and complements the product decomposition
approach whereby depolarization is modeled as a spatially localized “lumped” phe-
nomenon. In particular, the differential matrix methodology appears particularly well
adapted to the phenomenological description of the scattering in turbid media such
as biological tissues.
For a beam propagating along z, the Mueller matrix M(z + dz) is deduced from
M(z) by
M(z + dz) = U(dz)M(z) = [I + mdz],
(7.44)
where I is the identity matrix, U(dz) is the differential propagation matrix, and the
differential matrix m can be defined in the most general case as
m =
⎛
⎜
⎜
⎜
⎜⎝
𝛼
𝛽′′
𝛾′′
𝛿′′
𝛽′
𝛼1
𝜇′′
𝜈′′
𝛾′
−𝜇′
𝛼2
𝜂′′
𝛿′
−𝜈′
−𝜂′
𝛼3
⎞
⎟
⎟
⎟
⎟⎠
.
(7.45)
Equation (7.44) can also be rewritten as
dM
dz = mM.
(7.46)
Let us first assume that the medium is nondepolarizing and postpone the discussion
of the effects of depolarization. Then it can be shown that m features additional
transposition symmetries, namely 𝛽′= 𝛽′′, 𝛾′= 𝛾′′, 𝛿′= 𝛿′′, 𝜇′= 𝜇′′, and so on [37].
Then, the differential propagation matrix U(dz) takes the form
U(dz) = [I + mdz] ≈(1 + 𝛼dz)
⎛
⎜
⎜
⎜
⎜⎝
1
𝛽dz
𝛾dz
𝛿dz
𝛽dz
1
𝜇dz
𝜈dz
𝛾dz
−𝜇dz
1
𝜂dz
𝛿dz
−𝜈dz
−𝜂dz
1
⎞
⎟
⎟
⎟
⎟⎠
,
(7.47)
which shows that 𝛼dz is simply the differential attenuation of unpolarized light over
the distance dz. Moreover, due to the small size of the nondiagonal elements of
mdz (due to the magnitude of dz itself), these elements describe the differential
polarimetric effects due to the propagation over the distance dz as follows [43]:
r 𝛽dz is the differential linear diattenuation along the x–y laboratory axes.
r 𝛾dz is the differential linear diattenuation along the ±45◦axes.
r 𝛿dz is the differential circular diattenuation.
r 𝜂dz is the differential linear retardation along the x–y axes.

POLARIZED LIGHT FUNDAMENTALS
263
r 𝜈dz is the differential linear retardation along the ±45◦axes.
r 𝜇dz is the differential circular retardation.
Another essential consequence of the smallness of the off-diagonal elements of the
U(dz) matrices is that they always commute (provided dz is small enough). Indeed, a
straightforward matrix calculation shows that to first order
U1(dz)U2(dz) = [I + m1dz][I + m2dz] = [I + (m1 + m2)dz] = U2(dz)U1(dz) (7.48)
In other words, the differential approach is well adapted to the propagation of
light media featuring simultaneously various polarimetric properties, by eliminating
the ambiguities related to the noncommutation of Mueller matrices describing finite
instead of differential effects.
Note that in the general case of longitudinally inhomogeneous medium, m is
a function of the propagation coordinate z (the direction of light propagation). If
the medium can be considered homogeneous in the z direction over a distance
L, expression (7.46) can be easily integrated to give
M = exp(m L) = exp(L)
(7.48)
which can be also written as
ln(M) = m L = L.
(7.49)
This expression indicates that the fundamental properties of the medium under
consideration, contained in m, can be deduced from the logarithm of the related
Mueller matrix if the total thickness L is known.
In the presence of depolarization, the matrix m takes its general form (7.45) and
can be decomposed into a sum of two matrices
m = mm + mu,
(7.50)
where mm has the same shape and symmetry properties as the m matrix defined in
Eq. (7.45) (and is thus nondepolarizing). More specifically
mm = 1
2
⎛
⎜
⎜
⎜
⎜⎝
2𝛼
𝛽′ + 𝛽′′
𝛾′ + 𝛾′′
𝛿′ + 𝛿′′
𝛽′ + 𝛽′′
2𝛼1
𝜇−𝜇′
𝜈−𝜈′
𝛾′ + 𝛾′′
𝜇′ −𝜇′′
2𝛼2
𝜂−𝜂′
𝛿′ + 𝛿′′
𝜈′ −𝜈′′
𝜂′ −𝜂′′
2𝛼3
⎞
⎟
⎟
⎟
⎟⎠
and
mu = 1
2
⎛
⎜
⎜
⎜
⎜⎝
0
𝛽′ −𝛽′′
𝛾′ −𝛾′′
𝛿′ −𝛿′′
𝛽−𝛽′
0
𝜇′ + 𝜇′′
𝜈′ + 𝜈′′
𝛾′′ −𝛾′
𝜇′′ + 𝜇′
0
𝜂′ + 𝜂′′
𝛿′′ −𝛿′
𝜈′′ + 𝜈′
𝜂′′ + 𝜂′
0
⎞
⎟
⎟
⎟
⎟⎠
(7.51)

264
TISSUE POLARIMETRY
Then mm thus represents the mean values of the six elementary polarization
properties, while mu represents their respective uncertainties due to depolarization
[41,42].
The elementary polarization properties of the medium can thus be directly deter-
mined from the logarithm L of the Mueller matrix M, by constructing the Lorentz
antisymmetric, Lm, and symmetric, Lu, components of L (L = Lm + Lu), respectively,
as follows:
Lm = 1
2(L −GLTG)
and
Lu = 1
2(L + GLTG),
(7.52)
where G = diag(1, −1, −1, −1) is the Minkowski metric tensor.
Apparently, for a depolarizing medium, the off-diagonal elements of Lm represent
the mean values of the six elementary properties accumulated over path lengthL, while
the off-diagonal elements of Lu express their respective uncertainties. Further, the
main diagonal elements of the matrix Lu(𝛼1, 𝛼2, and 𝛼3) represent the depolarization
coefficients (after the subtraction of the isotropic absorption 𝛼from the diagonal)
along the xy, ±45◦, and circular axes [42]. The accumulated polarization parameters
(intrinsic properties integrated over the path length L) can be determined from the
elements of the Lm and Lu matrices as
r Integrated dichroism
d =
√
L2
m,12 + L2
m,13 + L2
m,14.
(7.53a)
r Integrated depolarization
Δ = 1
3|𝛼1 + 𝛼2 + 𝛼3|.
(7.53b)
r Linear retardation
𝛿=
√
L2
m,24 + L2
m,34.
(7.53c)
r Optical rotation
𝜓= 1
2LM,23.
(7.53d)
r Total retardation
R =
√
𝛿2 + 4𝜓2.
(7.53e)
The corresponding uncertainties (standard deviations) of the parameters, diattenu-
ation, linear retardance, and optical rotation (and total retardance) can also be derived
employing the same set of equations (7.53) on the off-diagonal elements of the
matrix Lu.

POLARIZED LIGHT FUNDAMENTALS
265
The conversion relationships between the medium polarimetry characteristics
defined in the polar decomposition formalism (forward decomposition and its vari-
ants, reverse and symmetric decomposition) and those in the logarithmic decom-
position (differential matrix formalism) also worth a brief mention here. In fact,
the Mueller matrix corresponding to each accumulated polarization parameter (ele-
ments of the differential matrix m scaled by path length L) can be obtained from
the solution of Eq. (7.46) and the eigenvalues of m. These Mueller matrices for
each of the individual polarization effects can be related to those defined in the
polar decomposition formalism, to yield conversion from one set of parameters to
the other:
Dlog = tanh(d) = tanh
[√
L2
m,12 + L2
m,13 + L2
m,14
]
↔Dpol,
(7.54a)
Δlog = 1 −1
3(e−𝛼1 + e−𝛼2 + e−𝛼3) ↔Δpol.
(7.54b)
Note that the above two conversion relations, transforms dichroism into diatten-
uation (7.54a) and converts the three logarithmic depolarization coefficients into a
single “conventional” net depolarization (7.54b). However, no such conversion is
required for retardance.
The accuracy of this decomposition has been successfully tested with experimental
Mueller matrices of phantoms consisting of dispersion of polystyrene microspheres
in sucrose containing polyacrylamide and Monte-Carlo simulation, nematic liquid
crystal plates [44] and adhesive tapes [42] and samples with diattenuators embedded
in scattering and optically active media [45], to vary the commutation properties of the
elementary polarization components. Some of these validation studies are discussed
in Section 7.4.2.2.
7.2.4
Summary
In this section, we introduced the Stokes Mueller formalism, which provides a math-
ematically sound and physically intuitive description of all light polarization states,
including partially polarized ones, which are particularly relevant for tissue polarime-
try. The essential polarimetric properties, namely the diattenuation, the polarizance,
the retardation, and the depolarization have been defined for “simple” polarimetric
objects.
While Mueller matrices provide the most complete polarimetric characterization
of any samples of interest, in vast majority of cases the most interesting information
about these samples—such as those relevant to diagnostics for biological tissues for
example—remain “hidden” in the 16 elements of the matrices. The decomposition
procedures outlined in this part allow one to “extract” this information, provided the
various decomposition procedures are used wisely.
On the one hand, each procedure is based on a specific description or representation
of the sample, for example as a “stack” of simple elements following each other in a
well-defined order (product decompositions). Of course, the parameters provided by

266
TISSUE POLARIMETRY
such procedures may be accurate only if the sample is actually of the form assumed
by the decomposition, which is seldom the case. So the question then becomes
whether the assumed physical representation implied by a specific mathematical
decomposition can be considered as is “equivalent” to the original tissue? A “toolbox”
of several procedures is then needed to decide how critical this issue really is, and
how to cope with it.
Another important feature is the stability of the results. As a rule of thumb, the
more factors are involved, the more unstable these factors may be for a given product
(the initial Mueller matrix M). Commutation (multiplication order) issues may also
arise, which may compromise the possibility to actually determine the values of
some factors. For example, the stability and uniqueness of the solution provided by
the three-factor decompositions (the Lu Chipman in its forward and reverse forms) is
an advantage of these “simple” decompositions over the more complex symmetrical
ones. This may explain why the former are the most widely used, even though they
may provide only “effective” values of diattenuation, retardation, and depolarization
in many cases, and all the more so in biological tissues.
In contrast, the logarithmic decomposition seems very appropriate to the charac-
terization of tissues by its very definition, as these are turbid media with volumetri-
cally “distributed” polarimetric properties (i.e., effects occur simultaneously and not
sequentially as matrix multiplication/product decompositions imply). Moreover, this
decomposition is intrinsically stable and has been successfully validated in several
complex situations. One limitation, which may be overcome soon, is that if has been
developed and tested only in the forward scattering direction, while for diagnostic
purposes the backward geometry is typically more interesting and practical.
7.3
INSTRUMENTATION
A huge variety of polarimetric instruments have been developed, to meet the many
different requirements related to the studied objects. In this part, we outline the
general principles behind the design of polarimetric instruments, mention the various
optical polarization components, and provide illustrative examples of actually used
instruments, without attempting to be exhaustive.
7.3.1
General Principles
7.3.1.1
Overall Scheme of Polarimetric Instruments
A generic polarimetric
instrument is schematized in Figure 7.7. The light emitted by one (or possibly sev-
eral) source(s) traverses a Polarization State Generator (PSG) and an illumination
optical system (IO) which define the polarization state and other characteristics of
the beam which impinges on the sample (Sp). The light emerging from the sample
in a particular direction of interest (shown in transmission for illustration purposes
in Figure 7.7) is then captured by another optical system (DO), traverses a Polar-
ization State Analyzer (PSA) whose transmission depends on the light polarization,
and is eventually detected by one (or possibly several) detectors (D). Though in

INSTRUMENTATION
267
S
PSG
IO
Sp
DO
PSA
D
FIGURE 7.7
Block diagram of a generic polarimetric instrument. S, source(s); PSG, polar-
ization state generator; IO, illumination optics; Sp, sample; DO, detection optics; PSA, polar-
ization state analyzer; D, detector(s). Transmission geometry is illustrated above, but polariza-
tion analysis in many other detection directions is often possible and/or desirable.
practice the PSG, the PSA, and the other optics may overlap each other, either
physically or conceptually, these elements are represented as separate as they play
different roles.
The source(s) and detector(s) may be point-like or extended, and the other optics
may implement full field imaging or pointwise measurements, the latter possibly with
spatial scanning. Moreover, the instrument may be designed for single or multiple
wavelength operation, with various requirements on acquisition speed or acceptable
signal-to-noise ratio (SNR). Of course, the polarimetric elements composing the PSG
and the PSA must also be chosen to satisfy the same requirements as the other optics.
For example, their geometrical ´etendue and/or spectral bandwidth must be sufficient
if full field imaging and/or multispectral operation are required. These issues will be
briefly discussed later for various optical elements used for polarization studies.
7.3.1.2
Polarization Modulation and Analysis
Sequential polarization mod-
ulation and analysis. Among the various schemes actually used for polarization
modulation by the PSG and analysis by the PSA, the conceptually simplest is the
modulation and analysis using discrete polarization basis states. In this scheme,
the PSG sequentially generates m different polarizations, represented by m physical
P
R
D
y
x
z
θ
FIGURE 7.8
PSA based on a rotatable retarder (R) preceding a linear polarizer (P) and the
detector (D). The pass axis of P is aligned along x, while the fast axis of the retarder makes a
variable angle 𝜃= 𝜔t with the x-axis.

268
TISSUE POLARIMETRY
Stokes vectors Sj incident on the sample, which can be considered as the columns of
a real 4 × m matrix, the modulation matrix W. The PSG is said to be
r Incomplete if m < 4,
r Complete if m = 4 and the four Stokes vectors Sj are linearly independent,
meaning that the device correctly “samples” the whole polarization space. The
4 × 4 matrix W is then invertible.
r Redundant, if m > 4 and the Sj vectors form a complete basis of the polarization
space. In this case, the instrument “oversamples” this space, which is a possible
way to reduce the noise or the systematic errors. In this case, the 4 x m W matrix
possesses a pseudo-inverse.
Quite similarly to PSGs, a PSA can be characterized by its analysis matrix A.
When this PSA is illuminated by a light beam emerging from the sample whose
Stokes vector is S, it generates a set of n signals Ii (the eventually detected intensities),
each of which is obtained by passing the light through a “polarization filter” described
by a Stokes vector S′
i. Mathematically this filtering corresponds to taking the scalar
product of the incoming and the filter Stokes vectors:
Ii = S′
i ⋅S.
(7.55)
The Ii form a signal vector I of dimension n, related to the Stokes vector S emerging
from the sample by
I = A ⋅S.
(7.56)
In a way analogous to PSGs, PSAs may be incomplete, complete, or redundant
depending on the dimension of the polarization space spanned by the line vectors S′
i
of A.
In a generic polarimeter as schematized in Figure 7.7, each Stokes vector Sj
generated by the PSG is transformed into MSj, (where M is the Mueller matrix of
the sample) and generates a corresponding signal vector Ij. When the index j spans
the range from 1 to m, these signal vectors form the columns of matrix B, as for each
of the incident PSG states the PSA produces n signals. The raw signal matrix can be
written as
B = A ⋅M ⋅W.
(7.57)
Obviously, in order to retrieve useful information about M from the raw data B,
the matrices A and W must be known—the system must be calibrated. Moreover,
provided both the PSG and the PSA are complete or redundant (i.e., the instrument is
a Mueller polarimeter), the sample Mueller matrix M can be extracted from the raw
data B by inverting Eq. (7.57):
M = A−1 ⋅B ⋅W−1.
(7.58)

INSTRUMENTATION
269
Periodic time modulation. If the PSGs and/or the PSA polarimetric properties
vary periodically in time at angular frequency 𝜔, for example due to the presence of
rotating elements (see Figure 7.8), the detected signal(s) also vary periodically at the
same frequency, allowing decomposition by Fourier analysis into harmonics; exper-
imentally, this enables improved noise rejection and higher SNR via synchronous
phase-sensitive detections (e.g., through lock-in amplification). The basis states of
the PSG and/or the PSA are simultaneously encoded in different Fourier components
if the detected signal(s). The modulation and analysis matrices of the PSG and the
PSA are then defined in frequency (Fourier) space. We will not explicitly cover the
(quite cumbersome!) equations giving a rigorous description of polarization encoding
and/or detection in Fourier space, but a few simple examples of very widely used
setups based on this scheme will be described in Section 7.3.2. Generally, temporal
polarization modulation is easier to implement with point-like measurement systems,
as detection schemes with imaging cameras are more challenging to modulate in
synchrony.
Spectral or spatial modulation. The idea behind these schemes is quite similar
to above. If the PSG and/or the PSA exhibit very fast (and well known!) variation
of their characteristics with wavelengths while the Mueller matrix of the sample is
constant or varies much more slowly, then the Fourier analysis of the signal can be
realized in wavelength space. This scheme is very appealing in realizing “snapshot”
spectroscopic polarimeters. The same idea may be applied for imaging applications,
by imposing very fast variations of the polarization response of the PSA in the image
plane, with subsequent Fourier analysis to extract the much more slowly varying
polarization properties of the object under study.
7.3.1.3
Design and Optimization Considerations
Here we address the following
question, which is of importance when designing a complete polarimeter and has been
widely discussed in the literature [46–52]; how can we use the degrees of freedom
we have on the actual matrices W (through the judicious selection of PSG input
states) and A (through the judicious selection of PSA filter states) for a given type of
instrument to minimize the noise on the final matrix M, in the presence of additive
noise on the raw data B?
We first discuss this issue for complete PSAs. Equation (7.56) can be rewritten as
I = A ⋅S
⇔
S = A−1 ⋅I.
(7.59)
If the raw measured intensity vector I is affected by an additive noise 𝛿I, this noise
will introduce an additive noise 𝛿S in the value of S, given by
𝛿S = A−1 ⋅𝛿I.
(7.60)
The rms noise on the raw data is nothing else but the usual Euclidean norm ‖𝛿I‖
of 𝛿I. With proper definition of the norm of matrices (the largest of their singular
values), we get an upper bound of the noise on the S vector:
‖𝛿S‖ ≤‖A−1‖‖𝛿I‖.
(7.61)

270
TISSUE POLARIMETRY
If the noise ‖𝛿I‖ on the raw measurements does not depend on the exact configu-
ration of the PSA, as is usually the case, a good criterion to optimize the design of a
complete PSA consists in minimizing the norm ‖A−1‖ [47].
Actually this criterion tells us that A is “as far as possible from singular” and thus
is optimally conditioned to be inverted as required to extract S (which contains the
information about the sample Mueller matrix M) from I. To achieve this optimization,
the PSA basis states must be “as different as possible” from each other. For complete
PSAs, this condition is achieved when the four basis states are at the tips of a regular
tetrahedron on the Poincar´e sphere [47]. This simple “geometrical” criterion can be
generalized to the case of redundant PSAs, from which noise propagation is optimized
when the basis states form simple symmetric polyhedra (such as an octahedron for
n = 6, a cube for n = 8,and so on…) at the surface of the Poincar´e sphere [52].
Moreover, when A is optimized, the noise on S is not only minimized, but it is also
most equally distributed among the various components of S [48].
On the other hand, this general criterion may not be valid if one is interested
in specific components of S, for which the noise can be minimized further at the
expense of increasing it for other components (presumably less relevant in a particular
application).
These results are easily generalized from PSAs to complete Mueller polarime-
ters: not surprisingly, the noise is minimized and most equally distributed on all M
components if both ‖W−1‖ and ‖A−1‖ are minimized [49–51].
7.3.1.4
Complete versus Incomplete Instruments
At first sight, one would
assume that complete or redundant polarimeters are always superior to incom-
plete ones, as they are the only polarimeters able to provide full Mueller matrices
upon mathematical analysis. However, for practical applications, complete Mueller
polarimeters are not always the best choice, for two reasons:
1. Complete polarimeters are necessarily more complex than incomplete ones,
and it may be difficult to make them compliant with demanding experimental
requirements in terms of speed, spectral or angular bandwidth, and the like
which may be imposed by the envisioned application.
2. If the polarimetric response of the sample under study is known to be of a
specific form, then incomplete polarimeters may be sufficient, and even better
than complete ones, as they can be designed to optimize the SNR and accuracy
of the measurements of the Mueller matrix elements of interest.
The best practice, whenever possible, is to explore the polarimetric response of
the samples of interest with complete instruments, and then, once the necessary
polarimetric measurements are defined to make the device to be used in practice
(e.g., for diagnostic purposes) as simple and rugged as possible.
As discussed below, for bulk biological tissues the “hierarchy” of the polarimetric
effects is strong depolarization, modest linear birefringence, very weak optical activity
(circular birefringence), and essentially nonexistent diattenuation/dichroism (either

INSTRUMENTATION
271
linear or circular). In the absence of linear birefringence, tissues are essentially pure
depolarizers, with diagonal Mueller matrices featuring typically m22 = m33 ≠m44
(i.e., often (but not always!) linear depolarization is independent of orientation and
is different from circular depolarization). Accurate determination of these elements
can be done by incomplete polarimeters. On the other hand, if two of the three
mentioned characteristics are simultaneously present (e.g., linear birefringence and
depolarization, or depolarization and optical activity) and are of interest, then the
determination of the full Mueller matrix and subsequent decomposition of this by the
procedures outlined in Section 7.2 may be unavoidable.
7.3.2
Commonly Used PSAs
The many different designs for PSAs can be classified according to several pos-
sible criteria, with significant overlaps. Note that in principle, any PSA may be
converted into a PSG (and the other way around) by interchanging the source(s) and
the detector(s). In the following, after a short description of the usual polarization
handling components, we present commonly used polarimetric PSAs, by increasing
“complexity” starting from those based on simple linear polarizers.
7.3.2.1
PSAs Based on Linear Polarizers
Linear polarizers are the most basic
polarization handling components, and they are ubiquitous in polarimetric setups. In
practice, these elements are of two main types:
r Dichroic polarizers, consisting of thin sheets of (typically plastic) materials
which exhibit full absorption for one polarization and typically 70–85% trans-
mission for the other. These polarizers typically feature quite broad angular
acceptances, well adapted for imaging applications, but somewhat limited spec-
tral bandwidths (the visible or the NIR for example).
r Polarizing beam splitters, which transmit two linear orthogonal polarization
in different directions. These elements are made of complementary prisms
of crystalline materials (Glan or Wollaston prisms) or glass with multilayer
dielectric coatings. Crystalline beam splitters feature very wide spectral but
rather narrow angular bandwidths while the opposite holds for glass beam
splitters.
Sequential PSAs. As immediately obvious from the very definition of Stokes
vectors (Eq. 7.7a), a single linear polarizer coupled with a single detector allows
the determination of the I, Q, and U (but not V) components of the Stokes vector
S of incoming light propagating along the z axis, provided the polarizer can be
oriented sequentially along x, y, +45◦, and −45◦in the transverse plane to provide
the intensities Ix, Iy, IP, and IM, respectively.
PSAs withcontinuouslyrotatingpolarizers.Alternatively, if thepolarizer rotates
continuously at angular frequency 𝜔, it transforms the Stokes vector S that traverses
it into
S′(t) = MLD(1, 1, 𝜔t) S,
(7.62)

272
TISSUE POLARIMETRY
where MLD (1, 0, 𝜔t) is the matrix of a linear diattenuator defined in Eq. (7.24) with
𝜏= 1 and D = 1 (perfect polarizer) oriented at 𝜃= 𝜔t. Equation (7.62) above can
then be rewritten as
S′(t) = 1
2
⎛
⎜
⎜
⎜
⎜⎝
1
cos 2𝜔t
sin 2𝜔t
0
cos 2𝜔t
cos2 2𝜔t
cos 2𝜔t sin 2𝜔t
0
sin 2𝜔t
cos 2𝜔t sin 2𝜔t
sin2 2𝜔t
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟⎠
⎛
⎜
⎜
⎜
⎜⎝
I
Q
U
V
⎞
⎟
⎟
⎟
⎟⎠
.
(7.63)
The detected signal I1 is the first component of S′(t)
I1(t) = I + Q cos(2𝜔t) + U sin(2𝜔t)
(7.64)
from which I, Q, and U are immediately extracted by a straightforward Fourier
analysis, as the amplitudes of the constant term and the two in-phase and quadrature
components at frequency 2𝜔.
7.3.2.2
Combinations of Linear Polarizers and Constant Retarders
Constant
retarders are also very commonly found in polarimetric setups, to generate and
analyze elliptically (circularly) polarized states. For these components, there are also
two basic technologies:
r Retardation plates. These plates are slabs of birefringent materials (typically
quartz or mica) which introduce a phase retardation Δ𝜙= 2𝜋⋅Dn ⋅d/𝜆between
their two orthogonal eigenpolarizations (Δn is the refractive index difference,
d the plate thickness, and 𝜆the wavelength). The natural dispersion of Δ𝜙due
to the 1/𝜆dependence can be reduced, around well-specified wavelengths, by
assemblies of two plates with their slow axes at 90◦to each other. Generally,
the angular acceptance is rather small, except for very thin plates (called “true
zero order” plates).
r Fresnel rhombs. These devices are based on the very achromatic phase retar-
dation introduced by a total internal reflection between in-plane and out-of-
plane polarizations. The spectral bandwidth may be huge (essentially equal
to the transparency window of the material) but the angular acceptance is
again very small. These components are thus very well suited to spectro-
scopic point-sensing polarimeters but present formidable challenges for imaging
applications.
Sequential PSAs with a linear polarizer and a removable QWP. As noted
above, linear polarizers alone cannot measure the V component of the incoming
Stokes vector S. The conceptually simplest way to address this limitation is to add
to the measurements performed with the polarizer alone another two measurements
performed with a QWP (a retarder with 90◦retardation) with its axes set at ±45◦

INSTRUMENTATION
273
from those of the polarizer, to convert the linear polarizer into right and left circular
polarizers. As a result, such a system may be operated
r in redundant mode [53], by taking into account the full set of six measurements
(Ix, Iy, IP, IM, IL, IR) and solving the overdetermined system of six equations
for the four unknowns I, Q, U, V (a particularly simple example of rectangular
matrix A):
Ix = I + Q, Iy = I −Q, IP = I + U, IM = I −U, IL = I + V, IR = I −V, (7.65)
r in complete, nonredundant mode, by skipping the measurements of I−45◦and
IR and relying on the fact that the total intensity can be written as
I = Ix + Iy = IP + IM = IL + IR.
(7.66)
As mentioned earlier, the nonredundant scheme is obviously faster but the accu-
racy obtained with the redundant mode may be significantly better due to possible
systematic errors which may be “averaged out” by increasing the number of mea-
surements.
Sequential PSAs with a linear polarizer and a rotatable retarder set at discrete
azimuths. In this kind of PSAs, the light to be analyzed passes through a rotatable
retarder (R) and then a polarizer (P), which can be assumed to be aligned along
the x axis without loss of generality. The light polarization is analyzed by recording
the detected signal while the angle 𝜃= 𝜔t between the fast axis of the retarder and
the passing axis of the polarizer varies.
Upon reaching the detector, the Stokes vector S emanating from the sample of
interest is transformed into
S′(t) = MLD(1, 1, 0) MLR(𝛿, 𝜃) S,
(7.67)
where 𝛿is the retardation of R. For a fixed value of 𝛿, the detected intensity ID varies
as a function of 𝜃according to
ID(𝜃) = I + Q
[
cos2 𝛿
2 + sin2 𝛿
2 cos 4𝜃
]
+ U sin2 𝛿
2 sin 4𝜃−V sin 𝛿sin 2𝜃
.
(7.68)
When the retarder azimuth sequentially takes n discrete values 𝜃i, the correspond-
ing intensities are given by the formula above, which can be rewritten in matrix form
to explicitly show the analysis matrix A:
⎡
⎢
⎢
⎢
⎢⎣
I1
I2
....
In
⎤
⎥
⎥
⎥
⎥⎦
=
⎡
⎢
⎢
⎢
⎢
⎢
⎢⎣
1
cos2 𝛿
2 + sin2 𝛿
2 cos 𝜃1
sin2 𝛿
2 sin 4𝜃1
sin 𝛿sin 2𝜃1
1
cos2 𝛿
2 + sin2 𝛿
2 cos 𝜃2
sin2 𝛿
2 sin 4𝜃2
sin 𝛿sin 2𝜃2
...
...
...
...
1
cos2 𝛿
2 + sin2 𝛿
2 cos 𝜃n
sin2 𝛿
2 sin 4𝜃n
sin 𝛿sin 2𝜃n
⎤
⎥
⎥
⎥
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢
⎢⎣
I
Q
U
V
⎤
⎥
⎥
⎥
⎥⎦
.
(7.69)

274
TISSUE POLARIMETRY
If n = 4, the PSA is complete provided 𝛿is different from p𝜋. Moreover, ‖‖A−1‖‖
is minimized for 𝛿= 132◦and 𝜃1,2 = ±51.7 ◦and 𝜃3,4 = ±15.1◦[47]. The a priori
surprising value of 132◦stems from the criterion cited above: the basis states of the
PSA must form a regular tetrahedron on the surface of the Poincar´e sphere. It turns out
that to do so, for example, a quarter wave is not sufficient: such a component would
transform the initial linearly polarized state, into other states by rotations of 90◦about
various axes. As a result, these states would be confined to the same hemisphere as
the initial linear state and would never form a regular tetrahedron. This does not mean
that QWPs cannot be used in actual implementations of PSAs (and it is often the case
in practice) but the noise in the final result may be slightly larger than with 𝛿= 132◦.
PSAs with a linear polarizer and a continuously rotating retarder. As the
retarder is continuously rotated at angular frequency 𝜔, Eq. (68) above immediately
shows that the only Fourier components with nonzero amplitudes are DC, sin(2𝜔t),
cos(4𝜔t), and sin(4𝜔t). Calling these amplitudes IDC, IS2𝜔, IC4𝜔, and IS4𝜔, we obtain
their expression in function of S given in Eq. (7.70), which defines the analysis
matrix A of the PSA in Fourier space. Again this matrix becomes singular for
𝛿= p𝜋, as expected: in this case, the retarder is either half-wave and provides no
sensitivity to the circularity component V; or “non-existent” (if p is an even integer)
and the PSA reduces to a fixed linear polarizer. Analogous to above, A is optimal
for 𝛿= 129◦:
⎡
⎢
⎢
⎢
⎢⎣
IDC
IS2𝜔
IC4𝜔
IS4𝜔
⎤
⎥
⎥
⎥
⎥⎦
=
⎡
⎢
⎢
⎢
⎢
⎢
⎢⎣
1
cos2 (
𝛿
2
)
0
0
0
0
0
−sin 𝛿
0
sin2 (
𝛿
2
)
0
0
0
0
sin2 (
𝛿
2
)
0
⎤
⎥
⎥
⎥
⎥
⎥
⎥⎦
⎡
⎢
⎢
⎢
⎢⎣
I
Q
U
V
⎤
⎥
⎥
⎥
⎥⎦
.
(7.70)
Practical considerations. PSAs and PSGs comprising a linear polarizer and a
rotatable retarder (often improperly called “compensator”) achieve complete polar-
ization generation and analysis with only two passive polarization elements. Due to
this simplicity, they are very widely used, with the two operation modes outlined
above, with the retarders rotating either stepwise (discretely) or continuously:
r Continuous rotation is usually preferred when the detector is fast enough, which
is usually the case for pointwise measurements requiring a single photodetector
or spectroscopic measurements with one-dimensional CCD arrays at the focal
plane of a spectrometer. In this latter case, the retarders must be reasonably
achromatic, like Fresnel rhombs or, to a lesser extent, “achromatic” retardation
plates made of stacks of elementary birefringent plates. Compared to step-
wise rotation, continuous retarder rotation is easier to implement and provides
shorter measurement times. However, in this configuration the PSG and the
PSA cannot be optimized independently of each other, due to possible “mix-
ing” of the Fourier components encoding the various polarization basis states.

INSTRUMENTATION
275
A typical choice is to rotate the PSG and PSA retarders at frequencies 3𝜔and
5𝜔[50, 54].
r Conversely, stepwise rotation may be better suited for polarimetric imaging
applications, typically with two-dimensional CCDs. In this scheme, prior to
each image acquisition the retarders are set at the prescribed couple of azimuths
and left still during the camera integration time, until the full set of n × m raw
images are taken. For this polarimetric imaging, it is essential to use retarders
with wide enough angular acceptances. Image “wander” is frequently observed
in relation with retarder rotation, and must be corrected for by suitable imaging
treatment/analysis procedures.
r Continuous retarder rotation and Fourier analysis of the signal would be possi-
ble with two-dimensional CCDs, only for extremely fast CCDs (both in acqui-
sition and data transmission) and would require large computational power.
A practically easier alternative would involve stroboscopic illumination, syn-
chronized with the polarization modulation. This approach has already been
implemented for partial polarimetric measurements [55] but not for complete
Mueller polarimetry.
7.3.2.3
Combinations of a Linear Polarizer and Variable Retarder(s)
Histori-
cally, the first variable retarders were the Babinet Soleil compensators, made of two
crystalline wedges which could slide with respect to each other, and which were
equivalent to a single birefringent crystal with variable thickness. Though still in use,
these devices have been largely replaced by others, of the two following types:
r Pockels cells. In these elements, a driving voltage is applied to two or four suit-
ably oriented crystals, to modify their birefringence, and the resulting retarda-
tion between two linear orthogonal eigenpolarizations. These devices typically
exhibit very fast time responses (in the nanosecond range, if a suitable voltage
driver is used), but they require high driving voltages (∼200 V for Δ𝜙= 180◦
in the visible) and exhibit very small spatial and angular apertures. In practice
these devices can be used only with laser beams, and thus are included in PSGs
(but generally not in PSAs) when studying highly scattering samples like thick
tissues.
r Photoelastic modulators. These devices consist of a slab of glass (typically
fused silica) about 10 cm long. Piezoelectric transducers generate a sound wave
at frequency 𝜈, which, in turn, creates a time dependent birefringence in the glass
slab at the same frequency 𝜈, which can be used to modulate the polarization
of a passing light beam at 𝜈and its harmonics 2𝜈, 3𝜈. . . This effect is usable
if v is a sonic resonant frequency of the glass bar. Such resonances, extremely
narrow, typically lie around 50 kHz. As a result, polarimeters with photoelastic
modulators are typically operated with synchronous detection locked on the
modulator excitation and/or its harmonics. Such high frequency modulation
schemes reduce the 1/f and other noise sources thereby increasing the SNR,
an advantage which may be crucial when the available signal is low. This

276
TISSUE POLARIMETRY
scheme is typically realized electronically with single point-sensing detectors,
but it can also be implemented optically, with a stroboscopic illumination of the
polarimeter to “freeze” the polarization at specific phases with respect to the
modulator excitation.
r Liquid crystals cells. In these components, a very thin (∼10 μm thick) layer of
nematic liquid crystal is “sandwiched” between two glass plates with transpar-
ent electrodes. AC driving voltages up to 20 V are sufficient to modulate the
retardation over 360◦or more. The angular acceptance of these devices is very
large (tens of degrees), making them very well suited for imaging applications.
On the other hand, the time response is slow (several tens of milliseconds) for
a significant shift in retardation. The spectral range is also limited to the visible
and the near infrared.
Polarizer and one variable nonrotating retarder. As these elements are typically
set at azimuths 45◦apart from each other, we will assume without loss of generality
that the retarder is aligned along the x axis, and the polarizer at 45◦without loss of
generality, as shown in Figure 7.9. The incoming Stokes vector S is transformed by
the variable retarder VR and polarizer into
S′(t) = MLD(1, 1, 45◦) MLR(1, 𝛿(t), 0) S,
(7.71)
where 𝛿(t) is the time-dependent retardation of the VR. The detected intensity is now
ID(t) = 1
2(I + U cos 𝛿(t) −V sin 𝛿(t))
(7.72)
and does not depend on Q at all. This result is general: a PSA for which the only
variable parameter is the retardation of a VR set at constant azimuth is never complete.
Thus, the measurements provided by this setup have to be completed by other
measurements for Mueller matrix determination, for example, via a removable QWP
QWP
VR
45°
45°
P
D
y
x
z
FIGURE 7.9
Scheme of a PSA composed of a variable retarder (VR), a polarizer (P), and
a detector (D). The VR has its fast and slow axes aligned with x- and y-axes, respectively,
and its retardation is varied to analyze the incoming beam polarization. The measurements
are performed with and without the removable QWP to allow complete determination of the
incoming beam polarization.

INSTRUMENTATION
277
in added at the entrance of the device. With this additional component, Eq. (7.71)
becomes
S′(t) = MLD(1, 1, 45◦) MLR(1, 𝛿(t), 0) MLR(1, 90◦, 45◦)S
(7.73)
which transforms the detected intensity into
IQW
D
(t) = 1
2 (I −Q sin 𝛿(t) + U cos 𝛿(t))
(7.74)
and thus completes the determination of S when the retardation 𝛿(t) is varied to
define the analysis matrix A. As mentioned above, the temporal evolution of 𝛿may
be essentially arbitrary for nonresonant devices like Pockels cells or liquid crystals.
For resonant devices like photoelastic modulators, the retardation varies sinusoidally
in time:
𝛿(t) = 𝛿0 sin 𝜔t.
(7.75)
The Fourier expansions of the functions cos(𝛿0sin𝜔t) and sin(𝛿0sin𝜔t) are well
known. Keeping the first three terms (DC, 𝜔, and 2𝜔frequencies), we obtain
ID(t) = 1
2(I + U[J0(𝛿0) + 2J2(𝛿0)cos2𝜔t] −V[2J1(𝛿0)sin𝜔t]),
(7.76)
IQW
D
(t) = 1
2(I −Q[2J1(𝛿0)sin𝜔t] + U[J0(𝛿0) + 2J2(𝛿0)cos2𝜔t]),
(7.77)
where Jn are Bessel functions. The Fourier components at zero, 𝜔, and 2𝜔frequencies
are the relevant quantities for the synchronous detection scheme generally used to
optimize the SNR. Analogous to the rotating retarder scheme, the expressions above
can be recast in terms of the system analysis matrix A, which is then redundant, with
six measured quantities (IDC, IS𝜔, IC2𝜔, and the like with the QWP inserted) for the
four components of S.
Polarizer and two nonrotating variable retarders. This type of PSA is schema-
tized in Figure 7.10. The incoming light traverses two variable retarders, whose fast
axes are set at constant azimuths 𝜃1 and 𝜃2 from the x axis, then again a linear polar-
izer set along x before being detected. The Stokes vector S′ of the light reaching the
detector is now
S′(t) = MLR(1, 𝛿2, 𝜃2) MLR(1, 𝛿1, 𝜃1) S.
(7.78)
The calculation is quite similar to the previous ones and will not be reproduced
here. The main result is that by varying the retardations 𝛿1 and 𝛿2, it is easy to make
this device a complete PSA, provided the orientations 𝜃1 and 𝜃2 are properly chosen.
Actually, the best choice is 𝜃1 = 0◦and 𝜃2 = 45◦, as any arbitrarily chosen elliptical
“polarization filter” can be obtained by adjusting the retardations. This property is

278
TISSUE POLARIMETRY
P
D
VR2
2
VR1
y
x
z
θ
1
θ
FIGURE 7.10
PSA composed of two variable retarders (VRi), a polarizer (P), and a detector
(D). The VR fast and slow axes are set at azimuths 𝜃1 and 𝜃2 with respect to the x axis and
their retardations 𝛿1 and 𝛿2 are varied in time to analyze the incoming beam polarization.
also valid when the device is used as a PSG, with a source replacing the detector
and the light travelling in the –z direction: such a PSG can generate any predefined
Stokes vector S.
Such versatility is not really needed to make complete Mueller polarimeters: as
long as the PSG and the PSA are each able to generate four linearly independent
basis states, complete measurements are possible (W and A are invertible). However,
potential SNR improvements inherent in no-moving-mechanical-part polarimeters
(often with synchronous detection schemes) offer significant advantages in low-signal
situations often encountered in biomedicine. Further, the possibility to generate any
state by the PSG and to project the emerging state on any polarization state at the
PSA opens interesting possibilities for polarization contrast optimization of different
FLC
P
C
WP
L
S
PBS
C1
C2
45°
FIGURE 7.11
Three PSA setups implemented for OSC imaging. Left: a sequential PSA.
The incident beam traverses a ferroelectric liquid crystal (FLC) equivalent to a half wave
plate which can be rapidly switched between two azimuths separated by 45◦, and a linear
polarizer (P) before reaching the camera (C). This system allows sequential acquisition of the
orthogonally polarized images with switching times less than 1 ms [58]. Center: the incoming
beam is separated into two beams with orthogonal polarizations by the Wollaston prism (WP),
to form two polarized images on the sensor (S) via the lens (L) [59]. Right: the two orthogonal
polarizations are separated by the polarizing beam splitter (PBS) and form two images on two
separate cameras C1 and C2 [60].

INSTRUMENTATION
279
tissue states to be distinguished from each other (e.g., cancerous and noncancerous
tissues) without measuring the full Mueller matrices [56,57].
7.3.3
Examples of Tissue Polarimetry Instruments
In this part, we illustrate the above considerations by giving some examples of
polarimetric instruments, without attempting to exhaustively describe the very wide
variety of such devices which have been successfully implemented.
7.3.3.1
Orthogonal State Contrast Imagers
Orthogonal state contrast (OSC)
imaging (which has also been called, somewhat improperly, DOP imaging) is quite
a simple, and thus experimentally very appealing polarimetric characterization tech-
nique, whenever applicable. In this modality, the sample is illuminated with linearly
polarized light and imaged with two polarization filters, one parallel and one perpen-
dicular to the illumination polarization. The two images, III and I⊥, are then combined
to form the OSC image:
IOSC = III −I⊥
III + I⊥
.
(7.79)
The OSC contrast is independent of the overall reflectivity of the sample (within
the limits imposed by the instrumental dynamical range!) and thus a purely polari-
metric characterization technique (the tissue reflectance (III + I⊥) can be studied
independently). Moreover, for isotropic pure depolarizers, as may be true for many
biological tissues, OSC directly provides the elements m22 = m33 of the sample-
normalized Mueller matrix, which define the tissue depolarizing power for linearly
polarized light. Of course, OSC can also be implemented with co- or counter-oriented
circular polarizations, and then it provides m44.
An OSC imager thus comprises a linearly polarized source, and a PSA able to
provide images with two linear orthogonal polarizations as efficiently as possible.
Three schemes of such PSAs have been described in the literature and are outlined
in Figure 7.11. In the first scheme (left panel), the acquisition of the two polarized
images is realized sequentially, by “optically rotating” the linear polarizer by means
of a liquid crystal cell with constant half wave retardation whose orientation can be
switched between two azimuths 45◦apart from each other in less than 1 ms. With
so fast a polarization modulation, the time needed to take both polarized images is
determined solely by the CCD speed and the integration time necessary to reach a
good SNR. Such a device has been used on a colposcope for in vivo examination
of uterine cervix at 10 fps for the OSC images [58]. In the second scheme (center
panel), the two polarizations are separated by means of a Wollaston prism and two
images are formed simultaneously on the CCD sensor (S) in the focal plane of the
lens (L) [59]. The third scheme (right panel) makes use of a polarizing beam splitter
(PBS) and two separate cameras to take the polarized images. In the last scheme,
the polarized images are taken simultaneously at video rates, but there may be issues

280
TISSUE POLARIMETRY
Sample
Laser
Lock-in
amplifier
PEM
P2
L2
L1
γ
QWP2
fp
fc
QWP1
P1
C
FIGURE 7.12
Highly sensitive complete Mueller point-sensing polarimeter. The PSG com-
prises a linear polarizer and a removable quarter-wave plate (QWP) while the PEM-based PSA
is of the type described above. fc and fp are the chopper and PEM modulation frequencies,
respectively. 𝛾is the detection angle. Adapted from References 61 and 62.
with misregistration, that is, unavoidable shifts in the pixel-to-pixel correspondence
of identical points in the optical images.
7.3.3.2
A nonimaging Mueller Polarimeter
A complete point-sensing polarime-
ter combining a PSG with a polarizer and a removable QWP and a PSA based with
a photoelastic modulator as described above is shown in Figure 7.12.
To boost the SNR as much as possible in this arrangement, the illuminating laser
is chopped (at the sub kHz range, fc) to allow synchronous detection of the “DC”
component, in addition to that of the signals modulated by the PEM at 𝜔(50 kHz, fp)
and 2𝜔(100 kHz).
7.3.3.3
Complete Mueller Imagers
In both examples shown below, the polari-
metric images are taken with a CCD. Of course imaging can also be achieved by
using a single detector and a spatially scanning laser beam.
Dual rotating retarders. In these instruments, the PSG and the PSA both com-
prise a linear polarizer and a rotating retarder. An example of such an instrument is
shown in Figure 7.13. As mentioned above, in such setups the rotatable retarders are
sequentially set at discrete azimuths and are kept fixed for each raw image acquisition
by the CCD. As acquisition of full Mueller images can take tens of seconds these
instruments are well suited to the characterization of static samples.
Pairs of variable retarders. The instrument shown in Figure 7.14 is the Mueller
imager developed at Ecole Polytechnique, for examination of ex-vivo tissue samples.
Both the PSG and the PSA are of the type outlined in Figure 7.10 with nematic liquid
crystals as non-rotating variable retarders.

Rotating quarter-wave retarder
Rotating quarter-wave retarder
Fixed linear polarizer
Restraint
Patient
Fixed linear polarizer
16-bit CCD camera
Spinning ground-glass disk
(coherence scrambler)
HeNe laser
FIGURE 7.13
Mueller imager with PSG and PSA, each comprising of a polarizer and a
rotating retarder set sequentially at discrete azimuths for image acquisition by the CCD. From
Reference 63.
Halogen source
Aperture
stop
CCD
Zoom
Close-up lens
Spectral filter
P
P
VR
VR
VR
VR
PSA
PSG
Metallic plate
Sample
f1
f1
f2
f2
FIGURE 7.14
Mueller imager for examination of ex-vivo samples. The PSG and PSA are
both made of a linear polarizer and two VR making use nematic liquid crystals at fixed
orientations. The retardations of each of the four VRs are switched discretely between pairs
of different values to generate the 16 polarization basis states. The illumination system is
designed to ensure that the polarizations defined by the PSG are uniform over the entire
5 × 5 cm2 field of view. Adapted from Reference 64.

282
TISSUE POLARIMETRY
Nematic liquid crystals feature very wide angular acceptance, no image motion
when retardance is changed, and a time response typically less than 100 ms. Full
Mueller images are then acquired in a few seconds by sequentially switching the
retardations of each of the four VRs between pairs of suitably chosen values and
leaving these retardations constant during the CCD integration time. By using a white
light source (halogen lamp) and interference filters, the wavelength can be selected
at will throughout the visible spectrum to enable spectral polarimetry studies.
7.3.4
Summary
In this section, we briefly reviewed the basic principles of design and optimization of
polarimetric instrumentation, and outlined the most widely used practical implemen-
tations. These may take a variety of forms, depending on the selected optical systems
(imaging or non-imaging) and the way the polarization is encoded and detected (in
discrete states, or in Fourier components of periodically varying systems such as
photoelastic modulators). The availability of so many possibilities is very valuable
in practice, as it allows one to “tailor” the polarimetric system for the specific needs
of the envisioned application. In particular, polarization modulation and detection
may be “added” to virtually any optical system, at the microscopic as well as macro-
scopic levels; note that this is not the case for many other techniques widely used in
biomedicine, such as confocal imaging or OCT, which operate only at microscopic
scales. Last but not least, often polarimetry may be implemented at very reasonable
cost, an essential point for many possible practical applications.
7.4
FORWARD MODELING AND TESTING IN PHANTOMS
7.4.1
Forward Modeling of Polarized Light Propagation in Tissue
7.4.1.1
Overview
As outlined in the introduction, tissue polarimetry research has
two major application directions:
(a) Polarized light tissue imaging, wherein polarization can be used as a gating
mechanism to filter out multiply scattered (image blurring) photons in order
to enhance contrast and to improve imaging resolution; and
(b) Tissue characterization/diagnosis, wherein the rich abundance of important
biophysical information contained in the intrinsic tissue polarimetry charac-
teristics (retardance, diattenuation, and depolarization encoded in the tissue
Mueller matrix) are extracted and quantified.
For either, accurate modeling of interaction of polarized light with biological
turbid media or tissue is extremely useful. The insight gained from such forward
models helps in designing and optimizing experiments, and analyzing/interpreting
measured data for specific applications.

FORWARD MODELING AND TESTING IN PHANTOMS
283
Driven by the two different classes of applications, forward modeling of polar-
ized light transport in tissue also has takes different routes. For the tissue imaging
applications, the major emphasis has been on modeling depolarization of multiply
scattered light in turbid media. In these analytical/heuristic approaches, the tissue is
typically modeled as a turbid medium having bulk-average scattering and absorption
properties, where propagation leads to depolarization as a result of strong multi-
ple scattering events (birefringence and other simultaneously occurring polarization
effects are often ignored) [1, 65–69]. The main aim of these models has been to under-
stand the overall depolarization trends, its dependence on the scattering properties
of the media (density, size, shape, and refractive index of the scatterers), and on the
incident state of polarization, and to design/optimize general polarization schemes
to discriminate against multiply scattered photons for tissue imaging in “simple”
geometries.
Applications involving extraction/quantification of the intrinsic tissue polarime-
try characteristics for tissue characterization and assessment, on the other hand,
require more accurate and complete forward models incorporating the simultane-
ously exhibited complex tissue polarimetry and scattering events [3]. Nevertheless,
for either of the aforementioned approaches, rigorous electromagnetic (EM) theory
based models for analyzing polarized light propagation in tissue has not been fea-
sible. This is because the Maxwell’s equations-based EM theoretical approach will
need to identify and incorporate the spatial/temporal distribution of complex tissue
dielectric structures (different cells and subcellular structures, connective stromal
tissues/extracellular matrix, blood and lymphatic networks, interstitial fluids, etc.),
which is clearly a formidable (if not impossible) task.
Instead, light propagation through such media is often modeled using the radiative
transport theory [70]. Although the scalar radiative transport theory and its simplified
approximation, the diffusion equation, have been successfully used in tissue optics
(specifically to yield light intensity distribution in tissue volume and measurable quan-
tities such as diffuse reflectance, transmittance, spatially resolved diffuse reflectance
and so on), both are intensity-based techniques, and hence typically neglect polar-
ization [1, 70]. Alternatively, the vector radiative transfer equation (VRTE), which
includes polarization information by describing transport of the Stokes vectors of light
(photon packet) through a random medium, has been explored for tissue polarimetry
modeling [1]. However, solving the VRTE in real systems is rather complex and
the solutions are often too slow and insufficiently flexible to handle the necessary
boundary conditions for arbitrary geometries, heterogeneities, and optical properties
as desirable in case of tissue.
The polarization sensitive Monte Carlo (PSMC) technique is a more general
and robust approach that overcomes these limitations [70, 71]. The PSMC tech-
nique has thus been widely explored for tissue forward modeling, specifically for
applications involving tissue characterization/diagnosis as described in greater detail
subsequently. For polarized light imaging applications, relatively simpler analyti-
cal/heuristic approaches based on photon diffusion formalisms [68], random walk
models [69], and maximum entropy principles [65] have proven moderately suc-
cessful. In the following, we provide a very brief account of some of these simpler

284
TISSUE POLARIMETRY
analytical approximations developed to deal with depolarization of multiply scat-
tered light in turbid medium. Interested readers are referred to References 1, 65–69
for further details.
7.4.1.2
Modeling Depolarization of Multiply Scattered Light: Photon Diffusion
Formalism
As noted, these simpler analytical theories are aimed at deriving rela-
tionships between various quantities of practical interest such as the DOP (either
linear or circular) of forward-scattered or back-scattered light from a turbid medium,
average path lengths, the optical transport parameters of the medium, and so forth.
As in the case of radiative transport theory, in these models also, the turbid medium
is considered to have bulk-average scattering and absorption properties, representa-
tive of isotropic tissue volumes. The turbid medium is usually modeled through the
optical transport parameters, namely, the absorption coefficient (𝜇a), single scatter-
ing coefficient (𝜇s), and single scattering anisotropy (g) [70]. As is known from the
transport theory, the linear isotropic optical coefficients are defined so that
la = 𝜇−1
a
and
ls = 𝜇−1
s
(7.80)
give the absorption and scattering mean free paths, respectively.
The anisotropy parameter g is defined as the average cosine of scattering angle.
The value of g ranges from –1 to +1, where g = –1 corresponds to fully backward
scattering, g = 0 corresponds to forward-backward symmetric scattering (isotropic
scattering being a special case) and g = +1 corresponds to fully forward scattering.
In general, the value of g depends on the average size of the scatterers in the medium
relative to the wavelength of the irradiation. For a medium composed of scatterers
whose size is much smaller than the wavelength (radius a ≪𝜆), the anisotropy
parameter g is ∼0, its value approaching unity (g ∼1) for media composed of larger
sized scatterers (a ≥𝜆). The latter regime applies to most biological tissues in the
visible/near-infrared spectral range (g ∼0.7–0.95) [70].
Another couple of parameters frequently used in tissue optics are
𝜇∕
s = 𝜇s(1 −g) and l∗= (𝜇∕
s ) −1
(7.81)
namely, the reduced scattering coefficient and the transport mean free path. Because
of typical tissue g-values, 1 −g is a small number, and thus 𝜇s > 𝜇s
/ and l∗> ls.
The use of 𝜇′
s assumes that the reflection and transmission for a slab of tissue with
optical parameters 𝜇a, 𝜇s, and g are the same as those for the same slab with optical
parameters 𝜇a, 𝜇s
/, and g = 0 [70]. This so-called similarity principle is not exact and
holds if the light distribution is studied far enough away from the light source and
boundaries, typically at a distance greater than l∗, which in turn is referred to as the
typical length scale over which the propagation direction of photons gets randomized
in a multiply scattering medium.
In the photon diffusion formalisms, the depolarization of multiply scattered light
is usually modeled by assuming an exponential decay of the single path (photon
undergoing successive scattering events) DOP with increasing number of scattering

FORWARD MODELING AND TESTING IN PHANTOMS
285
events (n). Analytical expressions for depolarization via the characteristic depolar-
ization length scales 𝜉L and 𝜉C (for linear and circular polarizations, respectively)
has also been derived based on the so-called maximum entropy principle [1, 65]. The
degree of residual polarization of multiply scattered light at a chosen detection point
can be obtained by averaging (or weighing) the single path DOP over the path length
distribution function.
Approximate analytical expressions for the path length distribution function are
conveniently obtained from the solution of photon diffusion equation for a given
detection geometry [1, 70]. The resulting DOP DOPL,C of diffusely transmitted
or reflected light from a turbid medium for two practical geometries, the forward
scattering and the backscattering geometries, has accordingly been derived for linear
and circular polarizations:
r Forward scattering geometry, slab of thickness L
DOPL,C ≅2L
ls
sinh
[ ls
𝜍L,C
]
exp
[
−L
𝜍L,C
]
(7.82a)
where ls is the scattering mean free path defined in Eq. (7.80), and
𝜁L,C =
[1
3ls𝜉L,C
] 1
2 ,
(7.82b)
where 𝜉L,C is the characteristic depolarization length scales for linear and circu-
lar polarizations. These lengths are defined as the average distances a (linearly
or circularly) polarized photon has to go through in the medium to see its DOP
decrease by a factor of e. The ratios 𝜉L∕ls and 𝜉C∕ls thus define the average
number of collisions required to reduce the DOP by a factor of e.
r Back-scattering geometry, semi-infinite medium, L →∞
DOPL,C ≅3
2 exp
[
−𝛾
√
3l∗
𝜉L,C
]
,
(7.83)
where l∗(= ls/(1 −g)) is the transport mean free path of Eq. (7.81) and 𝛾is the
correlation decay parameter with value ranging between 1.5 and 3 [1].
We emphasize that the expressions for residual DOP in Eqs. (7.82) and (7.83) are
valid in the photon diffusion limit, that is, after many scattering events. Moreover,
these expressions typically neglect absorption, which can also be incorporated. In
backscattering geometry, Eq. (7.83) is then modified to [1]:
DOPL,C ≅3
2 exp
[
−𝛾
{√
3l∗(1 + 𝜇a𝜉L,C)
𝜉L,C
−
√
3l∗𝜇a
}]
(7.84)

286
TISSUE POLARIMETRY
Equations (7.82) to (7.84) essentially capture all the qualitative features of depo-
larization of light by multiple scattering (implicit is the assumption that the other
polarimetry effects such as birefringence and diattenuation are absent). This is illus-
trated in Figure 7.13, where the computed variations of characteristic length scales
of depolarization (normalized by the scattering mean free path ls) 𝜉L∕ls and 𝜉C∕ls
are shown as a function of the anisotropy parameter g of the medium. The inset
shows the variations of 𝜉L∕ls and 𝜉C∕ls as a function of the size parameter of the
scatterers
X = 2𝜋a nm
𝜆
,
(7.85)
where a is the scatterer radius, nm the surrounding medium refractive index, and 𝜆
the wavelength. The observed qualitative trends can be summarized as follows:
(a) For isotropic scattering media composed of Rayleigh scatterers (g ∼0,
a ≪𝜆), depolarization of circularly polarized light is stronger than linearly
polarized light (𝜉L>𝜉c).
(b) The reverse is true if (𝜉L<𝜉c), for anisotropic scattering media consisting of
larger scatterers (g ≥0.7, a ≥𝜆, the so-called Mie regime).
(c) The values of 𝜉L∕ls and 𝜉C∕ls increase monotonously (depolarization
decreases) with increasing anisotropy g. When the same quantities are plotted
as functions of the size parameter X, their variation is no longer monotonous,
as they decrease for X ≥10 (inset of Figure 7.15). This behavior is actually
due to a decrease in g with increasing X in this range, an effect attributed to
Mie resonances.
The underlying mechanism for depolarization dependence of linearly and circu-
larly polarized light on the size of scatterers is worth a brief mention here. The
decrease of depolarization with increasing g is easily understood as a decrease of
the polarization randomization when scattering becomes more and more peaked in
the forward direction.
The difference in relative rates of depolarization of linearly and circularly polar-
ized light can be attributed to the different mechanisms of depolarization of the two.
The depolarization of incident linearly polarized light occurs primarily due to the
randomization of the direction of the incident field vector as a result of multiple scat-
tering [65,66]. On the other hand, depolarization of circularly polarized light occurs
both due to the randomization of the field vector’s direction and randomization of
helicity [66]. Note that scattering at large angles flips the helicity of the circular
polarization state resulting in its larger depolarization. In a turbid medium, light
travels along many possible zig-zag paths, having contributions from scattering at
various angles. For Rayleigh scatterers (where forward and backward scattering are
approximately equally likely), the contribution of the large angle scattered photons
is greater as compared to the larger sized Mie scatterers (where forward scatter-
ing predominates), ensuing stronger randomization of helicity and thus resulting in

FORWARD MODELING AND TESTING IN PHANTOMS
287
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
30
Size parameter (X)
Value of g
  I/l and   c/l 
  l /l and   c/l
ξ
ξ
ξ
ξ
  c/l
ξ
  I/l
ξ
ξ  c/l
ξ  l/l
FIGURE 7.15
The theoretically computed variations of characteristic length scales of depo-
larization (normalized by scattering mean free path ls) of incident linearly (𝜉L∕ls) and circularly
(𝜉C∕ls) polarized light as a function of anisotropy parameter g. The inset shows the correspond-
ing variation as a function of size parameter of scatterer X. The calculations were made for
surrounding medium and scatterer refractive indices, respectively, equal to 1.33 and 1.59; and
𝜆= 0.6328 μm). Adapted from Reference 65.
stronger depolarization of circularly polarized light in Rayleigh media. As the scat-
terer size increases, the additional cause of depolarization of circularly polarized
light, that is, the flipping of helicity due to back scattering also gets reduced, result-
ing in weaker depolarization of circularly polarized light for anisotropic scattering
samples [66,67].
Note that the depolarization of light in a turbid medium is additionally influenced
by the refractive index of the scatterers present in the medium. In fact, it has been
shown that despite having large value of anisotropy parameter g, the depolarization
characteristics of weakly fluctuating random medium (low value of the relative refrac-
tive index contrast m = ns /nm ∼≤1.05) can be similar to that of media composed
of Rayleigh scatterers [61, 66]. This intriguing behavior originates from the fact that
the anisotropic scatterers (g ≥0.7, X ≫2) having a lower value of relative refractive
index m (m ≤1.05) seem to belong in the weak scattering or Rayleigh-Gans regime,
where each volume element within the scatterer gives electrical dipole scattering in
an independent manner [72], yielding Rayleigh-like scattering matrix elements. The
retention of this dipolar nature of scattering thus leads to depolarization characteris-
tics of such low refractive index, anisotropic scattering medium (such as majority of
biological tissues) much similar to that of Rayleigh scatterers.
These depolarization trends have been verified experimentally in turbid media
composed of spherical scatterers having varying sizes [72–75]. One should,

288
TISSUE POLARIMETRY
however, exercise caution in generalizing these depolarization trends to arbitrary
detection geometries, as depolarization metrics may exhibit more complex behav-
iors. Moreover, these analytical approaches, while useful, are approximate by their
very nature and are accordingly restricted to the “tissue imaging” applications
(employing polarization gating methods and various other co/cross polarization
detection schemes). In contrast, for applications involving extraction/quantification
of the intrinsic tissue polarimetry characteristics for tissue characterization, more
encompassing approach such as the polarization-sensitive Monte-Carlo modeling are
required.
7.4.1.3
Robust Polarization-Sensitive Monte-Carlo (PSMC) Approach for Mod-
eling Complex Tissue Polarimetry Characteristics
In this statistical approach to
radiative transfer, the multiple scattering trajectories of individual photons are deter-
mined using a random number generator to predict the probability of each scattering
event. It is also assumed that scattering events occur independently and exhibit no
coherence effects. The position and propagation direction of each photon are ini-
tialized and modified as the photon propagates through the scattering medium. The
photon propagates in the sample between scattering events a distance 𝓁sampled from
the probability distribution [70,71]:
P(𝓁) d𝓁= 1
𝜇t
exp[−𝜇t 𝓁] d𝓁,
(7.86)
where the extinction coefficient 𝜇t describes the cumulative effect of scattering and
absorption in the decay of the propagating light beam:
𝜇t = 𝜇s + 𝜇a.
(7.87)
In conventional intensity-based MC models, when the photon encounters a scatter-
ing event, a scattering plane and angle are statistically sampled based on the so-called
scattering phase function. Various types of scattering phase function has been used
in MC tissue models, namely, the Mie theory-computed phase function or analytical
approximations like the Henyey–Greenstein function [1].
In the polarization-sensitive Monte Carlo (PSMC) model, the photon’s polariza-
tion, with respect to a set of arbitrary orthonormal axes defining its reference frame,
is represented as a Stokes vector S and polarization effects are applied using medium
Mueller matrices M, as illustrated in Figure 7.16. Upon encountering a scattering
event, a scattering plane and angle are statistically sampled based on the polarization
state of the photon (Stokes vector) and the full scattering Mueller matrix M (note
that the angular variation of the m11 element of the scattering Mueller matrix is
actually the scattering phase function for unpolarized light). The photon’s reference
frame is first expressed in the scattering plane and then transformed to the labora-
tory (experimentally observable) frame through multiplication of appropriate rotation
matrices (reference frame manipulation) and by the Mueller matrix corresponding to

FORWARD MODELING AND TESTING IN PHANTOMS
289
S0
Si = R (  ).S0
Ss = M (  ).Si = M (  ).R (  ).S0
φ
ϕ
ϕ
θ
θ
θ
ey
0
ex
i
ex
s
ey
s
ez
s
ez
i
ez
0
ex
0
ey
i
FIGURE 7.16
Transformation of the polarization state by an individual scattering event.
The Stokes vector S0 describing the incident polarization is expressed in the reference frame
(e0
x, e0
y, e0
z)defined from the previous scattering event. Then the azimuth 𝜙and the polar angle
𝜃of the scattered photon are sampled statistically from the phase function. The Stokes vector
i describing the incident polarization is expressed as Si in the (ei
x, ei
y, ei
z)frame obtained by
rotating the initial frame by an angle 𝜙in the (x,y) plane. Then the output Stokes vector
Se obtained from the previous one by applying the scattering Mueller matrix M(𝜃), on Si is
expressed in the (es
x, es
y, es
z)frame obtained by rotating the intermediate frame in the scattering
plane by an angle 𝜃around ei
y = es
y.
the scattering event. Usually, the scattering Mueller matrix M is computed using Mie
theory, which assumes the scattering medium to be composed of discrete spherical
scatterers [76,77]. In principle, incorporation of scattering Mueller matrices for other
non-spherical scatterers or the scattering matrices for other continuously fluctuating
medium (such as tissue) is also possible [78].
The evolution of polarization state of each photon packet is tracked (via the
Stokes vectors) following successive scattering events. The absorption effects are
incorporated after successive scattering events by multiplying the photon by a factor
that decreases its weight
Ω =
𝜇s
𝜇a + 𝜇s
(7.88)
also called albedo, as conventionally done in intensity-based MC models [70].
Upon encountering an interface (either an internal one, representing tissue layers
of different optical properties, or an external one, representing external boundary), the
probability of either reflection or transmission is calculated using Fresnel coefficients.
Assuming no interference effects, the final Stokes vectors for light exiting the sample
in a particular direction (at any user selected detection point) are computed as the
sum of all the appropriate sub-populations of photons. The sample Mueller matrix
(for a given detection geometry) can then be calculated by sequentially changing
the input polarization between four (or more) states, by recording the corresponding
output Stokes vectors for each respective input states, and performing algebraic
manipulations [76].

290
TISSUE POLARIMETRY
While the effects of scattering on the polarization evolution with successive scat-
tering events can be modeled by the scattering Mueller matrix M, the effects of
other medium polarimetry effects such as linear birefringence and optical activity
can also be incorporated by including their corresponding Mueller matrices. How-
ever, difficulty arises in modeling many simultaneous polarization events in the
presence of multiple scattering. Matrix multiplication is generally not commutative
(MAMB ≠MBMA); thus, different orders in which these are applied will yield dif-
ferent effects (cf decomposition discussion of Section 7.2.3). Yet in actual tissues,
these effects (such as optical activity due to chiral molecules and linear birefringence
due to anisotropic tissue structures) are exhibited simultaneously and not one after
the other as sequential multiplication implies. Thus, a realistic tissue model must
include these simultaneous polarization effects in the presence of scattering.
This problem is tackled through the use of the so-called N-matrix formalism
[7, 79], which combines several polarization effects into a single matrix describing
them simultaneously. Briefly, inthis formalism, thematrixof thesampleis represented
as exponential sum of differential matrices (N-matrices), wherein each matrix includes
a single polarization effect [7, 79]. The issue of ordering of noncommutative matrices
is overcome as matrix addition is always commutative. The differential matrices
corresponding to each optical property exhibited by the sample are then summed
to express the combined effect, and are subsequently applied to the photons as
they propagate between scattering events. Note that an analogous differential matrix
formalism for combining simultaneous polarimetry effects has been discussed in
Section 7.2.3.3, in context to inverse polarimetry analysis. Finally, the scattering
histories of a large number of photon packets (typically 107–109 photons are required
to generate statistically acceptable results) are tracked as they propagate through the
medium and are summed to yield the macroscopic parameters of practical interest
(Stokes vectors, Mueller matrices, path length distributions, polarization statistics
from different scattering histories, etc.).
In the simulation, circular and linear birefringence (these are the two relevant
tissue polarimetry effects in addition to depolarization) are modeled through the
optical activity 𝜒in degrees per centimeter, and through the anisotropy in refractive
indices (Δn), respectively [76]. Here, Δn = (ne −no) is the difference in refractive
index along the extraordinary axis (ne) and the ordinary axis (no). For simplicity, it
is generally assumed that the direction of the extraordinary axis and the value for
Δn is constant throughout the scattering medium (although recent research efforts
are exploring the effects of multiple uniaxial domains of varying magnitude and
orientation of birefringence [4; 80]. In each simulation, ne and no are taken as input
parameters and a specific direction of the extraordinary axis is chosen. As each photon
propagates between scattering events, the difference in refractive indices seen by the
photon depends on the propagation direction with respect to the extraordinary axis.
The effect is usually modeled using standard formulae describing the angular varia-
tion of refractive index in uniaxial medium. Similar to conventional MC model, the
scattering and absorption properties are modeled using the optical transport param-
eters (scattering coefficient 𝜇s and absorption coefficient 𝜇a and single scattering
anisotropy g).

FORWARD MODELING AND TESTING IN PHANTOMS
291
7.4.2
Experimental Testing and Validation in Tissue Phantoms
7.4.2.1
Forward Model
The polarization sensitive Monte Carlo forward model,
describedaboveandtheMueller matrixinverseanalysis methods, describedinSection
7.2.4, requires comprehensive validation prior to their implementation for polarized
light assessment of complex systems like tissues. The validity of these approaches
has thus been tested on various tissue simulating phantoms exhibiting scattering and
polarization properties, which are known and user-controlled a priori [61, 62, 81].
The validation studies have been performed on optical phantoms exhibiting either
sequential or simultaneous polarization effects in presence of multiple scattering
[45]. Note that in complex systems like tissues, no unique order (or sequence) can
be assigned to the polarimetry effects; rather, these are exhibited simultaneously
(except a few specific type of layered tissues, wherein sequential occurrence of some
polarimetric effects may indeed happen). We shall thus present selected validation
results on tissue phantoms exhibiting simultaneous scattering and polarization effects
[3, 61, 62].
These solid phantoms were developed using polyacrylamide as a base medium,
with sucrose-induced optical activity, polystyrene microspheres-induced scattering,
and mechanical stretching to cause linear birefringence (or linear retardance). To
apply controllable strain to produce linear birefringence, one end of the polyacry-
lamide phantoms was clamped to a mount and the other end to a linear transla-
tional stage, inducing varying linear birefringence with its axis along the direction of
strain. These phantom systems mimic the complexity of biological tissues, exhibit-
ing simultaneous effects of linear birefringence, optical activity, and depolarization
due to multiple scattering. The experimentally recorded Mueller matrix from a bire-
fringent, chiral, turbid phantom measured in the transmission geometry is shown in
Figure 7.17a. The corresponding matrix generated through the PSMC model, using
the same input optical parameters is shown in Figure 7.17b.
–1
0
+1
1
–0.031
0.003
–0.006
–0.021
0.768
–0.037
0.020
–0.005
0.023
0.104
–0.774
0.001
0.039
0.797
0.192
1
–0.004
0.002
–0.001
0.001
0.774
0.031
–0.039
–0.006
–0.043
0.123
–0.795
–0.001
–0.025
0.814
0.1
+1
0
–1
(a)
(b)
FIGURE 7.17
(a) The experimentally recorded Mueller matrix in the forward (transmission)
detection geometry from a birefringent (extension = 4 mm, corresponding to a value of linear
retardance 𝛿= 1.345 rad for 1 cm path length), chiral (concentration of sucrose = 1 M, 𝜒= 1.96◦
cm−1) turbid (𝜇s = 30 cm−1, g = 0.95) phantom of thickness of 1 cm. (b) The corresponding
Mueller matrix generated through the PSMC model with input parameters: linear birefringence
Δn = 1.36 ×10−5 (corresponding to 𝛿= 1.345 rad), 𝜒= 1.96◦cm−1, 𝜇s = 30 cm−1, g = 0.95.
Adapted from Reference 3. (For a color version of this figure, see the color plate section.)

292
TISSUE POLARIMETRY
The simultaneous occurrence of the constituent medium polarization properties
(depolarization, linear birefringence, and optical activity) are observed to contribute
in a complex interrelated way to the Mueller matrix elements (both the experimental
and the MC-simulated one), resulting in essentially all sixteen nonzero elements.
Even though a direct quantification of the individual polarimetric contributions from
such “lumped” system Mueller matrix is not possible, some qualitative features of
the constituent polarimetry effects may readily be identified from the Mueller matrix
elements:
1. In both the experimental and the MC-generated Mueller matrices, the sig-
nature of linear birefringence is reflected as considerable magnitudes of the
m34 and m43 elements (representing horizontal/vertical linear birefringence),
as one would expect for a linear retarder with orientation angle 𝜃= 90◦. This
is however, accompanied with nonzero magnitudes of m24 and m42 elements,
which is a manifestation of the random orientation effects of linear birefrin-
gence (as experienced by different subpopulation of multiply scattered photons
undergoing zig-zag paths in the medium).
2. Chirality (optical rotation) is manifested as a difference of m23 and m32 ele-
ments, even though the effect is considerably weaker. The inequality of the
magnitudes of m23 and m32 elements is a manifestation of interrelated “cross-
talk” contributions from other simultaneous effects.
3. The effect of depolarization is prominently reflected in the diagonal elements
of the Mueller matrix. While the m22 element closely resembles polarization
loss (horizontal/vertical linear depolarization) due to multiple scattering alone
(additionally weakly influenced by optical rotation effects), the m33 (±45◦
linear depolarization) and m44 (circular depolarization) elements are strongly
influenced by simultaneous linear birefringence effects and are accordingly
considerably lower in magnitude.
4. In absence of any intrinsic diattenuation effects (either in the experimental
phantom or in the simulation), the characteristic elements m12, m13, and m14
do not exhibit appreciable magnitudes.
The excellent agreement between the experimental and the simulated Mueller
matrices emphasizes the capability of the PSMC model in simulating complex tissue
polarimetry effects (simultaneous optical activity and birefringence in the presence
of multiple scattering). Yet the complicated nature of the resultant Mueller matrix
underscores the problems associated with polarimetric data interpretation in such
complex systems. But as illustrated below, despite these complexities, the constituent
polarimetry characteristics can be successfully isolated and quantified using the
Mueller matrix inverse analysis (various decomposition) methods. The theory behind
these was described in Section 7.2.3; we now turn to experimental validation studies
of these approaches.
7.4.2.2
Inverse Polarimetric Analysis
Figure 7.18 displays the results of inverse
polarimetry analysis (Mueller matrix decomposition) on the experimental Mueller

FORWARD MODELING AND TESTING IN PHANTOMS
293
0
–0.03
–0.00162 0.000613
–0.03
0
–00596
–0.0153
0.00162
0.0596
0
–1.39
0.000613 0.0153
1.39
0
0
–0.00554
0.00554
0.0053
–0.0053
0.00491
–0.263
0.00012
0.00353
0.00012
–0.301
0.0214
0.00491 0.00353
0.0214
–0.148
–1
–0.5
0
0.5
1
–0.1
–0.15
–0.2
–0.25
–0.3
–0.5
0
1
–0.0312
–0.0214
0.0029
–0.0055
–0.0066
0.768
–0.037
0.0204
0.023
0.104
–0.773
0.0014
0.039
0.797
0.192
0.4
–0.4
–0.6
+1
0.5
0
+1
0
–1
+1
0
–1
MD
MR
MΔ
Lm
Lu
0.2
0
–0.2
1.8
1.6
1
0.755
0.769
0.845
0.998
1
1
0.184
–0.051
0.033
–0.982
0.981 0.186
–0.031
0.031
1
1
0.999
0.999
FIGURE 7.18
The experimentally recorded Mueller matrix in the forward detection geom-
etry (top panel, reproduced from Fig. 7.17a). The basis matrices (MΔ, MR, MD) obtained
following the forward polar decomposition process (middle panel). The matrix logarithms Lm
and Lu, derived using the logarithmic decomposition (bottom panel). The estimated polariza-
tion parameters using both the polar decomposition and the differential matrix approaches are
listed in Table 7.3. Adapted from Reference 44. (For a color version of this figure, see the
color plate section.)
matrix shown in Figure 7.17a [44]. Since these tissue phantoms exhibit simultaneous
(rather than sequential) polarization effects, use of the forward, reverse, or symmet-
ric product decomposition within the polar decomposition family (which employs
sequential factorization of the elementary polarization effects) is expected to be
somewhat inaccurate. For these three sequential product approaches, representative
results are shown for the forward decomposition only (Figure 7.18, middle); these are
compared with the logarithmic (differential matrix) decomposition which assumes
simultaneous occurrence of the constituent polarimetry effects) (Fig. 7.18, bottom)

294
TISSUE POLARIMETRY
TABLE 7.3
The values for the polarization parameters, Δ, 𝛿, 𝜓, and D, extracted
using the logarithmic decomposition from the Lm matrix.
Control
Logarithmic
Forward polar
Parameters
input
decomposition
decomposition
Δ
∼0.188
0.211
0.210
𝛿(radian)
∼1.574
1.386 ± 0.022
1.386
Ψ (radian)
∼0.030
0.030 ± 0.001
0.030
D
∼0
0.030 ± 0.009
0.032
The uncertainties in the𝛿, 𝜓, and D parameters are derived from the Lu matrix. The values for the parameters
extracted via forward polar decomposition (fourth column). The second column shows the control inputs.
The control input for the net depolarization coefficient Δ was determined from the experimental Mueller
matrix of the pure depolarizing phantom having no birefringence and optical activity. The expected value
for linear retardance (𝛿) and optical rotation (𝜓) are estimated by using the corresponding values from
the nonscattering phantom (𝛿0 = 1.345 rad and 𝜓0 = 0.026 rad) and accounting for increased path length
⟨Z⟩(𝛿= 𝛿0 ⋅⟨Z⟩, 𝜓= 𝜓0 ⋅⟨Z⟩, ⟨Z⟩= 1.17 cm is the MC-derived average photon path length).
Source: Adapted from Reference 44.
Evidently, the forward polar decomposition-derived basis matrices exhibit simpler
characteristic features of standard sequential and homogeneous depolarization, retar-
dance, and diattenuation matrices with many of the off-diagonal null elements (see
Section 7.2.2), as expected. Similarly, the Lorentz antisymmetric Lm and symmetric
Lu components of the matrix logarithm derived via the differential matrix formalism
exhibit characteristic features of simultaneous depolarization, linear birefringence,
and optical activity effects (their mean values and uncertainties discussed in Section
7.2.3.3).
The estimated polarization parameters using both the polar decomposition and
the differential matrix formalisms are listed in the accompanying Table 7.3. In the
absence of any intrinsic diattenuation property, the values for D estimated from
both the decomposition formalisms are quite low and comparable (as expected). The
estimated values for the depolarization coefficient Δ from the two formalisms are
self-consistent and are also in excellent agreement with the controlled input.
Turning to the other two biologically important polarization parameters (linear
retardance 𝛿and optical rotation 𝜓), several interesting trends are seen. The derived
optical rotation 𝜓in the presence of turbidity shows small increase as compared to
that from analogous nonscattering phantom (𝜓0 ∼0.026 rad, not shown here), which
arises from the increase in optical path length due to multiple scattering (resulting
in accumulations of 𝜓values). In fact, the accumulated value of 𝜓matches well the
expected one𝜓= 𝜓0 ⋅⟨Z⟩, where ⟨Z⟩is the Monte Carlo-derived average photon
path length for this specific geometry.
In contrast, although the composition-derived value for linear retardance 𝛿of
the turbid phantom is larger (𝛿∼1.386 rad) than that for the clear phantom (𝛿0 ∼
1.345 rad), it is significantly lower than that expected for accumulated photon path
length (1.574). This has been shown to arise due the curved zig-zag propagation
paths for a group of multiply scattered photon population. Since components of such

FORWARD MODELING AND TESTING IN PHANTOMS
295
curved photon paths are directed along the linear birefringence axis, this leads to a
reduction in the apparent 𝛿[44, 81].
The uncertainties (standard deviations) in the differential matrix-derived polariza-
tion parameters, Δ𝛿log-M, Δ𝜓log-M (derived from the elements of the Lu matrix), also
warrant a brief comment. The uncertainty in linear retardance (Δ𝛿log-M) arises both
due to strong depolarization present in this turbid phantom and due to the random
orientation effects of linear birefringence (as observed by different sub-population
of photons undergoing zig-zag paths in the medium). Since the latter effect does
not influence the value for optical rotation, the resulting uncertainty Δ𝜓log-M is also
considerably lower as compared to Δ𝛿log-M. In general, the standard deviations in
the polarization parameters contain useful information on the randomness of the
medium and are also additionally influenced by the choice of the detection geometry,
as described below.
The representative experimental results from complex tissue-like turbid media pre-
sented above (and continuing validation studies on several other phantoms exhibiting
varyingbirefringence, optical activity, anddepolarizationeffects) demonstratethat the
intrinsic polarization parameters of such medium can be self-consistently extracted
using either the logarithmic decomposition or the polar decomposition approach.
One should note, however, that the illustrative results discussed above are based
on Mueller matrices recorded in the forward detection geometry but the extension
of this approach to backward detection geometry (which is well-suited for in-situ
measurements) is warranted and has also been validated [82].
While the latter geometry is important for conceptual and practical reasons, it
offers additional technical hurdles; notably, the polarization parameters associated
with the intrinsic polarimetric properties of the sample in question can be difficult to
infer since they are more strongly coupled with the scattering-induced artifacts. In
fact, it has been shown that the scattering-induced artifacts gets considerably reduced
for detection positions located at distances larger than a transport length away from
the point of illumination (r > l∗). Thus, one possible method to measure polarization
parameters from a turbid medium using backward detection geometry is to perform
measurements at a distance larger than a transport length l∗away from the point of
illumination [82].
It must, however, be noted that in the backscattering geometry, the reduction
in apparent linear retardance 𝛿due to the effect of the curved-propagation path is
more prominent as compared to forward scattering geometry [44, 82]. Moreover, the
apparent optical rotation Ψ in this geometry also gets considerably reduced due to the
contribution of optical rotation of different handedness from two distinct subgroups of
photons; while photons undergoing a series of forward scattering events to eventually
emerge in the backward direction result in accumulated Ψ value, subgroups of pho-
tons that suffer scattering in the backward hemisphere only (backscattered photons)
changes the handedness of rotation, leading to cancellation of net optical rotation
[82]. For analogous reasons, the uncertainties in the differential matrix-derived linear
retardance and optical rotation parameters (Δ𝛿log-M, Δ𝜓log-M) are also considerably
larger in the backward detection geometry as compared to forward detection geometry
[44].

296
TISSUE POLARIMETRY
Note that the phantoms described above do not exhibit any intrinsic diattenuation
effect. Even though the magnitude of intrinsic diattenuation (dichroism) is typically
very weak in most tissues, the effect of nonzero diattenuation effect is also worth
a mention. As discussed in Section 7.2.3, in context to the forward and reverse
polar decompositions, presence of layered tissue structures (interfaces) may yield
significant diattenuation value, due to the different polarization response of the Fresnel
reflection/transmission coefficients. For such tissues (e.g., skin), the diattenuation
effect may not exhibit in a distributed manner (unlike depolarization or birefringence,
which are typically exhibited from the bulk of tissue), rather may be exhibited in a
sequential fashion. Since in the framework of the logarithmic decomposition (or in
the differential matrix formalism), all the elementary polarization and depolarization
properties of the medium are represented simultaneously, use of the logarithmic
decomposition in such situation may lead to deviations in the extracted polarization
parameters from those using polar decomposition. Moreover, in such case, either of
the forward or the reverse decomposition may turn out to be advantageous depending
upon the actual sequence of occurrence of the constituent polarization properties (as
demonstrated with illustrative example in Fig. 7.6).
For most of the tissues exhibiting volumetrically distributed scattering and polar-
ization effects, on the other hand, the logarithmic decomposition is expected to be
more suitable. Thus, one must exercise caution in applying the different decompo-
sition processes for tissue polarimetry characterization, a judicial choice of which
(based on the experimental detection geometry and a priori estimate of the tissue
morphology) may lead to realistic estimation of the intrinsic tissue polarization char-
acteristics.
7.4.3
Summary
This part was devoted to the theoretical modeling of the propagation of polarized light
in tissues. We first summarized the well-established methods for forward modeling,
that is, the determination of the polarimetric response of well-known samples. Once
adequate methods are available for forward modeling, the main issue to be addressed
in practice is the inverse problem, that is, the interpretation of measured polarimetric
responses of unknown samples.
As the main characteristic of biological tissues is predominantly their depolar-
izing power, which is related to multiple scattering, in first approximations most
tissues can be modeled as suspensions of spherical scatterers in optically isotropic
media. In such systems light propagation in multiple scattering regime may be conve-
niently described by the photon diffusion formalism, which provides simple analytical
approximations for the DOP of the emerging light as function of the absorption and
scattering coefficients 𝜇s and 𝜇a and the scattering anisotropy g, which, in turn,
depend on the size and the index of the scatterers. Virtually all tissues exhibit larger
depolarization for circular than linear incident polarization, as expected for scatterers
much smaller than the light wavelength (Rayleigh regime) or with low index contrast
(Rayleigh Gans regime). Beyond simple analytical approximations, the polarimet-
ric response of suspensions of scatterers can be evaluated by Monte Carlo (MC)

APPLICATIONS
297
simulations, provided the scatterers are not too closely packed, so that the light
propagation in such suspensions can be considered as a sequence of “isolated” scat-
tering events. MC method requires extensive computations, but it can fully take into
account the actual geometry of the experiment, overall yielding accurate results. This
accuracy has been experimentally tested on tissue phantoms exhibiting very com-
plex polarimetric responses, including linear and circular birefringences as well as
depolarization due to scattering.
These phantoms were also used to tackle the problem of inverse polarimetric
analysis, by using the decomposition procedures outlined in Section 7.2.3. The loga-
rithmic decomposition is expected to be the most accurate, as in the studied phantoms
all polarimetric effects occur simultaneously and not sequentially, as assumed in the
“three factor” forward and reverse polar decompositions. However, polar (forward)
decomposition provides surprisingly good results, a finding which is yet to be fully
understood. The logarithmic decomposition also allows to quantify the effect of
increased photon path length in the evaluation of the phantom birefringence. More-
over, these phantom studies provide a good grasp of the dependence of the sample
polarimetric response on the measurement geometry, which can act as a potentially
misleading “artifact” if not properly taken into account.
In summary, the theoretical “toolbox” for the interpretation of polarimetric mea-
surements, while not absolutely complete, seems sufficient for most practical inves-
tigations as those illustrated in the next part.
7.5
APPLICATIONS
In this section, we present some illustrative examples of selected polarimetry applica-
tions in biomedicine, both for optical imaging and tissue characterization applications.
7.5.1
Polarization-Gated Surface and Subsurface Imaging
Light polarization can be used as a gating mechanism able to separate the pho-
tons which have been deeply scattered in the bulk of the sample from those which
remained close to the surface: the former are generally depolarized while the latter
typically retain a significant part of their initial polarization. As a result, if an image
(or a spectrum) I⊥is taken on with polarization orthogonal to that of the incident
photons, the contribution of the deeply scattered photons is largely dominant. The
contribution of the depolarized photons is also present in the image III taken with the
same polarization as the incident light. As a result, the images I⊥and Ipol = III −I⊥
are dominated by the “deep” or “superficial” contributions, respectively [83]. The
penetration depth corresponding to the “superficial” contribution may be as small
as a few mean free paths [84]. Spectral filtering may be added to polarization to
further enhance the discrimination between “surface” and “bulk” contributions [85].
It is thus possible to get valuable information about the surface and bulk properties
the tissue under study by taking only two orthogonally polarized image (or spec-
tra). The use of linear polarizations discussed above can of course be generalized to

298
TISSUE POLARIMETRY
orthogonal circular polarizations as well. Moreover, in order to enhance the polari-
metric response, the polarized image Ipol can be normalized by the total intensity, to
provide the OSC
IOSC = III −I⊥
III + I⊥
.
(7.89)
Polarized imaging has been used, among other applications, in dermatology, to
enhance the typically subtle visual contrasts differentiating various types of tissues.
Early skin fibrosis of a thymic nude mice induced by X-ray irradiation was revealed
by polarized imaging and suitable image analysis, while remaining undetectable
by ordinary intensity imaging [85]. A variety of pigmented and nonpigmented skin
abnormalities were studied by means of this technique by Jacques and coworkers [87],
as illustrated in Figure 7.19. Polarized imaging revealed the disruption of the normal
skin structure (collagen organization) due to cancerous lesions such as malignant
basal cell carcinomas, suggesting a promising route to improve the definition of the
excision margins in surgery [88].
The polarization gating schemes described above have also been exploited for
depth selective spectroscopic measurements in tissue, which can improve the diag-
nostic efficacy of the spectroscopic approaches (elastic scattering [89, 90], fluores-
cence [91], and Raman spectroscopy [92]). As noted above, the underlying principle
is similar: the photons which are scattered (or re-emitted) from deeper tissue layers
undergo multiple scattering events and are depolarized to a larger extent. Polariza-
tion gating thus effectively selects photons which have not traveled beyond a few
scattering mean free paths (mpf = l−1∼100 μm in typical tissues).
Polarization-resolved spectroscopic approaches are thus expected to be particu-
larly suitable for early detection of epithelial cancers (where the majority of human
malignancies originate). Several illustrative examples demonstrate this approach.
For instance, the polarization preserving component (from superficial tissue layers)
extracted using polarization gating exhibited a fine structure component that was peri-
odic in wavelength [89]. This was identified as being due to light that is Mie scattered
by surface epithelial cell nuclei. By analyzing the amplitude and frequency of this
signal using Mie theory, the size distribution and the refractive index of the nuclei
was calculated. Since the size (and distribution) and refractive index of epithelial cell
nuclei are valuable parameters for detecting precancerous changes, this technique
holds promise for in situ and early diagnosis of epithelial cancer [93]. In addition
to the fine structure component, the polarization-gated elastic scattering signal also
showed inverse power law spectral dependence [90]. This was related to the self-
similar (fractal) nature of microscale fluctuations of local refractive index in tissue.
This was modeled using Fractal-Born approximation of light scattering to yield the
fractal micro-optical parameters, namely, the fractal dimension and the fractal upper
scale, which were also related to the pathological status of the examined tissue [90].
Polarization-gated fluorescence spectroscopic measurements have also been
explored for improving diagnostic efficacy of fluorescence-based methods [91, 94].
Specifically, polarization-gated fluorescence measurements have been successfully

APPLICATIONS
299
FIGURE 7.19
Normal intensity images and OSC images of a pigmented skin lesion (a
freckle, top) and nonpigmented one (malignant basal cell carcinoma, bottom). The normaliza-
tion by the total intensity in the OSC procedure is clearly very efficient in removing the effects
of pigmentation in the polarimetric contrast. Adapted from Reference 87.
exploited to decouple, isolate, and quantify fluorescence contributions from tissue
layers. Note that fluorescence signal from layered epithelial tissues, detected with
conventional measurement technique (and similar to diffuse reflectance studies), is
due to contributions from different endogenous fluorophores (having different quan-
tum yields, lifetimes, and overlapping spectral line shapes) present in the superficial
epithelial layer and the underlying connective tissue (stroma). Depth-resolved fluo-
rescence measurement facilitated by the polarization gating should thus help improve
contrast in the autofluorescence from malignant and normal sites. Similar strategy
on polarization-resolved measurements has also been exploited for depth-resolved
Raman spectroscopic measurements in layered epithelial tissues [92].

300
TISSUE POLARIMETRY
7.5.2
Tissue Assessment with Mueller Polarimetry
The OSC technique outlined above can provide quantitative and useful information
primarily if the sample is a pure (linear and/or circular) depolarizer. In more complex
cases of practical interest, the tissue may exhibit, in addition to depolarization, linear
and possibly circular birefringence. If so, complete polarimetric characterization
likely requires full Mueller matrix polarimetry, typically followed by an analysis
based on the decompositions outlined in Section 7.2.3 and an interpretation of the
polarimetric properties obtained by this procedure.
7.5.2.1
Noninvasive Glucometry
Glucose is an optically active (chiral) molecule,
and its structural asymmetry imposes a unique “fingerprint” on polarized light that
interacts with it. For example, the plane of polarization of linearly polarized light
is rotated upon passage through a glucose solution, by an amount proportional to
the light interaction length, its concentration, and its wavelength-dependent rotatory
power (a known quantity). As such, the glucose concentration in clear media can be
easily determined by polarimetric measurements of optical rotation (this is routinely
done, for example in biochemical laboratories and in the food industry). Can a
similar polarized light approach be used in biomedicine, for example to noninvasively
determine blood glucose levels in diabetic patients? If yes, this would represent a
tremendous advance, as noninvasive glucose sensing remains arguably one of the
most pressing unsolved problems in clinical medicine. Numerous approaches have
been and are being actively explored to address this difficult challenge [61, 95].
Unfortunately, the polarimetric approach for biomedical glucometry is also fraught
with difficulties. Unlike its “easy” transparent media counterpart, light propagating in
tissue does not have a unique interaction length which is needed to convert measured
optical rotation to glucose concentration; other dominant and complex tissue effects
such as multiple scattering and inhomogeneous birefringence depolarize the light
and alter its polarization properties (including causing chirality-unrelated apparent
rotation of its linear polarized fraction); the glucose chirality effects are rather weak
(owing to the low physiologic levels of blood glucose, in the range of 3–30 mM,
or ∼0.5–5 g L−1 of blood, yielding optical rotations in the millidegree range); other
chiral molecules present in tissue mask the already-weak glucose signal, and so on.
Nevertheless, we and others have performed careful fundamental feasibility studies
toward polarimetric tissue glucometry, as briefly summarized below.
Figure 7.20 shows millidegree-level optical rotations 𝜓induced by physiological
(millimolar) glucose concentrations in a 1-cm-thick polystyrene microsphere scat-
tering phantom (𝜇s ∼30 cm−1) [61]. This study was performed with a sensitive
polarimetric system that utilized polarization modulation and synchronous detection
similar to Figure 7.12 but with a dual balanced photodetector approach, optimized
to measure weak polarization signals in largely depolarized background. The results
shown are for the forward-direction transmission geometry, with the rotations cal-
culated from the determined Stokes vectors via 𝜓= 0.5tan−1(u/q). The forward
transmission direction was selected to enable direct results interpretation (path length
≥cuvette thickness, rotation directly proportional to glucose concentration) and to

APPLICATIONS
301
100
10–1
10–2
10–3
10–4
10–4
10–3
10–2
Glucose concentration (M)
Optical rotation    (degree)
10–1
100
ψ
FIGURE 7.20
Logarithmic plot of experimentally determined optical rotation as a function
of glucose concentration in scattering media (1.4 μm diameter polystyrene microspheres in
water, 𝜇s ∼28 cm−1) down to physiological glucose levels. Measurements were performed
using forward detection geometry (𝛾= 0◦in Fig. 7.12) through 1 cm of turbid media in a
quartz cuvette. Adapted from Reference 61.
minimize rotation artifacts unrelated to chirality (e.g., scattering-induced apparent
rotation, see Figure 7.21). The drawback of the transmission geometry is severe
depolarization, limiting polarimetric measurements to 2–3 mm thicknesses of tissue
[61]; this is why the scattering coefficient for this study in 1-cm-thick cuvette was
lowered to ∼30% of typical tissue levels (∼100 cm−1). Nevertheless, this preliminary
study shows the polarimetric potential for measuring physiological glucose levels in
tissue-like scattering media.
Backward detection geometry in reflection mode is more convenient for practical
tissue applications, yet the chirality-unrelated optical rotation artifacts can be severe
[82]. A method to suppress these artifacts is thus essential, and the Mueller matrix
polar decomposition methodology can be used to advantage here. Figure 7.21 shows
the variations in the scattering-induced apparent rotation and in the chirality-induced
true optical rotation as a function of offset distance from the point of illumination,
in the backscattering direction [3, 82]. As seen, the scattering rotation artifact can be
nearly an order of magnitude larger than the small chirality effect, completely dom-
inating and masking the presence of glucose. The polar decomposition reveals that
the scattering rotation does not arise due to intrinsic chirality of the medium, rather
is due to the scattering-induced linear diattenuation effect (also shown in the figure).
Importantly, polar decomposition can effectively separate out these two differently
sized contributions, revealing the glucose signal in this important yet artifact-prone

302
TISSUE POLARIMETRY
40
30
20
10
0.15
0.10
0.05
0.00
2
3
Detection distance X (mm)
Chirality-induced rotation
Scattering-induced rotation
x
Diattenuation (d)
4
5
Rotation (°), diattenuation d
FIGURE 7.21
Optical rotation (𝜓), derived from the decomposition of Monte Carlo-
generated Mueller matrices (open triangles), of backscattered light as a function of distance
from the center of the incident beam impinging on a chiral (𝜒= 0.082◦cm−1, corresponding
to 100 mM concentration of glucose) isotropic turbid medium (𝜇s = 30 cm−1, g = 0.95, thick-
ness = 1 cm). The solid triangles represent the corresponding scattering-induced rotation of
the polarization vector, derived from the Stokes parameters of the scattered light (for incident
polarization state S = (1,0,1,0)T). Also shown (open stars) is the apparent diattenuation which
seems to be responsible for the scattering-induced rotation artifact. The inset shows the back-
ward detection geometry. The chirality-induced rotation approaches zero as the detection angle
approaches the exact backscattering detection (𝜒= 0
◦cm−1, data not shown). The symbols
represent Monte Carlo and decomposition-derived data points and the lines serve to guide the
eye. Adapted from References 3 and 82.
detection geometry. Despite the highly preliminary nature of this study—the com-
plicating effects of other chiral confounders, experimental validation of these Monte
Carlo simulations results, signal changes due to varying optical absorption, and other
complexities yet to be evaluated—this does suggest a method to tease out small glu-
cose specific signals in highly scattering tissues. In combination with Monte Carlo-
determined path length distributions, we are currently investigating spectroscopic
polarimetry coupled with chemometric regression analysis to isolate glucose rota-
tions from contributions of other chiral confounders [96]. Further, given the extremely
challenging nature of the noninvasive glucometry problem, it is worthwhile to con-
sider hybrid approaches; that is, combining spectral turbid polarimetry with another
diagnostic modality (e.g., spectral diffuse reflectance, Raman, photoacoustics) by
carefully drawing on each technique’s complementary strengths in isolating small
tissue glucose signals.
7.5.2.2
Cancer Diagnosis
Smith et al. [63] demonstrated the potential of
full Mueller polarimetric imaging in dermatology by studying various tissue

APPLICATIONS
303
abnormalities. Melanomas exhibited reduced depolarization power with respect to
surrounding healthy tissue; as shown in several other studies, this trend seems to be
quite general when comparing healthy and (pre)cancerous tissues. No such effect was
observed on benign lesions, or in lupus lesions, which, however, exhibited slightly
reduced retardance, whose slow and fast axes showed rapid spatial variation across
the lesions.
Polar decomposition of Mueller matrices has also been evaluated for the diagno-
sis of oral precancer [97]. Precancers induced in cheek pouches of hamsters were
imaged in vivo and then analyzed histologically. Again, the precancerous tissues were
characterized by reduced depolarization and retardance with respect to surrounding
healthy tissue.
Ex vivo colon samples have been studied by Pierangelo et al. [98,99] with multi-
spectral Mueller polarimetry. In contrast with the previous examples, these samples
did not exhibit any significant retardance nor diattenuation. The polarimetric infor-
mation was thus provided mostly by the depolarization power throughout the visible
spectrum (from 500 to 700 nm). Depolarization was found to increase with increas-
ing wavelength from the green to the red part of the spectrum. This general trend is
due to decreasing absorption, which thus increases the average number of scattering
events photons suffer before being detected and results in higher polarization scram-
bling/loss. Moreover, depolarization varied significantly with the degree of tumor
penetration within the colon structure. At the beginning (stage T1), the tumor is
confined to the most internal superficial layers (the mucosa and submucosa). Then at
stage T2, it gets ulcerated (its thickness decreases by eliminating the “top” layers of
cancerous tissue while the tumor attacks the underlying layer (the muscularis)). At
stage T3, the tumor reaches the outermost layer (the serosa or pericolic tissue) and
eventually perforates it (stage T4).
Figure 7.22 shows an example of depolarization images of a colon with a can-
cerous lesion, together with the corresponding histologic pathology analysis. Using
the healthy region (H) at the left of the figure as a reference, the burgeoning part
B (graded histologically as T1) is less depolarizing, in agreement with the previous
examples. However, at inner regions of the lesion (point 2, stage T2), depolarization
increases again, and even more so in the innermost region (stage T3). In the lowest
part of the figure, the sample is “folded,” showing directly pericolic tissue, which
appears to be extremely depolarizing (probably due to its very weak absorption, as
this tissue is essentially fat, with very low hemoglobin concentration). Thus the non-
monotonic behavior of depolarization with the disease progression may be attributed
to the influence of the pericolic tissue, which increases when ulceration progresses,
making the tissue more and more depolarizing after the initial depolarization decrease
observed at the initial burgeoning stage.
In spite of its complicated non-monotonic behavior, the depolarization contrast
might be used as a diagnostic tool to provide a quick tumor staging, a possibility
which would be particularly useful if polarimetric imaging could be implemented
endoscopically.
In addition to tumor detection and staging, polarimetric imaging might also be
useful for the follow-up of radiochemotherapy (RC) of locally advanced cancers [99].

FIGURE 7.22
(a) Photo of the colon sample, with healthy (H) tissue, burgeoning (B) and
ulcerated (U) cancerous regions. (b) Depolarization image (values given by the color bar from
0 (blue) to 1 (red)) of the same sample. The solid line indicates the depth cut which has
been studied histologically. In this cut, shown in panel, (c), the regions marked 1 (healthy),
2 (burgeoning), 3 and 4 (ulcerated) are seen, together with the staging of the lesions, from T1 to
T3. The different layers are identified by M (mucosa), SM (submucosa), MP (muscularis), and P
(pericolic tissue, or serosa). (S) and (C) designate stromal and cancerous tissues, characterized
by low and high cell densities, respectively. Adapted from Reference 99. (For a color version
of this figure, see the color plate section.)

APPLICATIONS
305
We studied three mesorectum samples excised after neoadjuvant radiochemotherapy.
For T3 to T4 lesions, with or without regional metastatic lymph nodes, such neoad-
juvant procedures are known to make surgery easier and to reduce the probability
of relapse [100]. The patients were irradiated with standard doses (∼50 Gy) and
operated 6 weeks later. For the three samples, the “footprints” of the initial tumors
exhibited polarimetric contrasts (compared to healthy regions) which could be cor-
related with the degree of cancer regression after RC. Further, tumor depolarization
appeared lower than that of surrounding healthy tissue, even though the residual
tumor volume fraction/cellular tumor compartment was estimated from pathology to
be only a few percent. Such high polarimetric sensitivity suggests that it is not the
tumoral (cellular) tissue itself which is detected, but more probably the difference
in the fibrous collagen structure induced by the presence of cancer cells (as further
discussed below).
Another topic of potentially addressable by tissue polarimetry is the diagnosis of
uterine cervical cancer. This disease, due to infection by Human Papilloma Virus
(HPV), begins with precancerous lesions confined within the epithelium and classi-
fied as CIN1, CIN2 or CIN3 (acronym for Cervical Intraepithelial Neoplasia) when
anomalous cells invade the lowest third, two-thirds, or the entire epithelium thick-
ness. If not treated, CIN3 lesions eventually disrupt the epithelium basal membrane
and evolve into potentially lethal invasive cancer. This evolution is very slow, with
5–10 years typically elapsing from the initial infection to the onset of invasive can-
cer. As with many cancers, the disease can be cured very effectively by surgically
removing the anomalous regions on the cervix. Thus it is of paramount importance
to screen female populations to enable early disease detection, and to localize the
pathological regions as accurately as possible.
In developed countries, the first screening step is the Pap smear, which consists
of a cytologic examination of the cells collected from the cervix. If these cells seem
anomalous (typically exhibit large nuclei), the patient then undergoes a detailed exam-
ination of the cervix by means of a colposope (long working distance microscope)
to localize the CIN lesions, take biopsies and eventually decide a treatment course.
However, colposcopy is a notoriously difficult and operator-dependent examination,
with relatively poor performance [101]. There are thus many surgeries performed
without real necessity, or with incorrect surgical margins relative to the true extend
on the tumor. Conversely, essentially no screening is proposed in developing countries
(systematic Pap smears would require too many pathologists to examine the samples).
Thus this disease kills ∼275,000 women worldwide every year, the vast majority of
which could have been saved by a simple and accurate surgery. Clearly then, useful
alternatives to the Pap smear, possibly by simple optical means, are very much needed
to solve this outstanding public health problem [102]. In this respect, a large amount
of work has been performed on fluorescence / reflectance cervical imaging by several
groups, including those led by Richards Kortum, Follen, and MacAuley [102,103]. A
meta-study, carried out by these researcher and others and based on 26 clinical stud-
ies [103], concluded that these methods offered similar performance as colposcopy,
and may thus be used as adjuncts. For resource limited countries, particularly simple
and low-cost techniques, such as VIA (Visual Inspection with Acetic acid, which is

306
TISSUE POLARIMETRY
basically a colposcopic examination with the naked eye or a standard color camera)
have also been evaluated and showed good potential, in spite of rather low sensitivity,
of the order of 30% [104]. To summarize, significant progress is still needed in the
management of cervical cancer, especially in low resource countries.
Polarimetric imaging is also a promising approach in addressing this topic [64].
Several samples have been imaged ex vivo by using the polarimeter shown in Figure
7.13 of Section 7.3.3.3. Typical results, obtained from two samples, are shown in
Figure 7.23. The first sample was healthy, while the second one contained a CIN3
zone and a benign lesion (an ectropion), where a layer of glandular tissue was seen
(this tissue is normally present only in the endocervix). The pathology diagnosis
was established according to the standard procedure: fixing the sample in forma-
lin, cutting it into 4 mm thick blocks, embedding it in paraffin, shaving from the
CIN 3
Gland.
Intensity images
0°
25°
50°
0.4
0.7
1
δ
Δ
Healthy
Healthy
FIGURE 7.23
Images of two ex vivo cervix samples, one healthy (top) and the other one
with a CIN3 zone and a benign lesion (visible glandular tissue) (bottom). Left column: raw
intensity images. Middle column: images of scalar retardance 𝛿, in color scale from 0◦to 50◦.
Right column: images of depolarization Δ, from 0.4 to 1. Retardance and depolarization were
obtained from raw Mueller images by standard Lu-Chipman decomposition procedure of the
experimentally determined Mueller matrix images. On each sample, the solid black line shows
the limit of the intact epithelium. The straight lines indicate the locations of the histological
cuts where the pathology diagnostic was made (white for healthy, purple for CIN3, and orange
for glandular tissue). The dashed black lines designate regions where at least one among the
16 raw intensity images was saturated due to tissue “glare.” The top left image shows an
example of such saturated region, shown in red. Obviously, polarimetric data in these regions
are suspect. Adapted from Reference 64. (For a color version of this figure, see the color plate
section.)

APPLICATIONS
307
resulting “wax blocks” 5-μm-thick slices, deparaffinizing and staining them for
microscopic examination. As a result, pathology diagnosis was established only
along the “lines” shown in the intensity images (left column) of Figure 7.23; they are
repeated in the corresponding polarimetrc images for landmarking and for correlation
with microscopy-derived pathology grades. Viewing the ordinary intensity images
in the left column, basically no difference was observable between the healthy and
CIN3 tissues, while the glandular tissue was more reddish (and thus more absorbing
at the operating wavelength, 550 nm).
The Mueller images were decomposed by the standard Lu-Chipman procedure,
and the resulting scalar retardance and depolarization images are shown. The healthy
sample is characterized by strong retardance almost everywhere, with predominantly
tangential slow axis orientation (not shown). On the pathologic sample, there is a
healthy region in the lower left part of the image, again characterized by a strong
retardance, while both the CIN3 and the ectropion show essentially ∼zero retardance.
The depolarization is again very strong everywhere on the healthy sample, while on
the other one it decreases from healthy to CIN3 to glandular regions.
Although these results (described more fully in [64]) must be substantiated with
much more extensive studies, preliminary trends strongly suggest that uterine cervix
tissue exhibits strong birefringence in its healthy regions, with birefringence disap-
pearing at the very earliest stages of the precancerous evolution.
These observations are very promising in the context of an “automated” optical
diagnosis of uterine cervical precancer (thus not requiring the often unavailable exper-
tise of a medical specialist) and are substantiated by the results of other groups. For
example, Shukla and Pradhan [105] have also studied histology of cervical samples
and showed that the connective tissue beneath healthy epithelium exhibits signifi-
cantly larger retardance values than that beneath precancerous epithelium. It is well
known that precancerous evolution of epithelia modifies the structure of neighboring
connective tissues via a decrease in size and concentration of collagen fibers [106].
Such modifications are probably the main reason of the observed disappearance of
the tissue birefringence in abnormal regions of cervical tissues, and might in fact
occur in other sites/clinical scenarios of interest for medical diagnostics.
7.5.2.3
Characterization of Structural Tissue Anisotropy and Applications
Sev-
eral studies have addressed tissue structural anisotropy monitoring using polarimetric
techniques [3, 62, 107,108]; the above discussions of colonic and cervical patholo-
gies are particular examples of this approach in oncology. As mentioned, tissue struc-
tural anisotropy can stem from aligned orientations of tissue fibrillar components,
such as collagen and elastin, actin-myosin fibers, and mineralized hydroxyapatite
crystals [1]. Since such structural alignments often manifest as linear birefringence
effects, quantification of linear birefringence may represent a sensitive metric for
changes in tissue structure. A number of investigations have therefore addressed such
polarization birefringence measurement for the detection of tissue abnormalities like
osteoarthritis, thermal injury and cancer (e.g., basal and squamous cell carcinomas)
[3, 62, 107, 108]. Measurement of complete Mueller matrix and its inverse analy-
sis via the various decomposition approaches (outlined in Section 2.3) are clearly

308
TISSUE POLARIMETRY
advantageous for this purpose because (i) the small birefringence alterations can
be efficiently decoupled and quantified in presence of the other tissue polarimetry
effects; (ii) the decomposition process yields additional tissue polarimetry metrics
(e.g., diattenuation, depolarization, and potentially others not discussed above, such
as retardance ellipticity), which provide useful complementary information thus gen-
erating a more complete picture of the complex biophysical alterations taking place
in tissue. In the following, we offer two other cancer-unrelated illustrative exam-
ples of tissue structural anisotropy characterization with polarized light: bladder wall
abnormalities and regenerative heart treatments.
Bladder is an example of internal organ whose structure and function engenders
anisotropic tissue structure. Its purpose is to store and then expel urine, and its wall
layers extend and then contract to allow this to happen. Microstructural remodeling
in its epithelial layers is known to occur under mechanical distension and during
various disease processes (e.g., bladder outlet obstruction). As a step toward devel-
oping a turbid polarimetry platform for human bladder pathology studies, we have
recently obtained birefringence maps in normal ex-vivo distended rat bladders that
demonstrate the differential response of different bladder regions (dome, ventral, and
dorsal sides) to changes in filling pressure [109]. The results of Figure 7.24 were
obtained under pressures that represent typical physiological ranges in normal rodent
(and human) bladders; as we progress to examine pathological cases, the upper end
of the pressure range will likely have to increase. As seen, the dome region of the
bladder shows maximum birefringence when the bladder is distended to high pres-
sures, whereas the ventral tissues remains roughly isotropic during distension. In
addition, the average anisotropy direction is longitudinal, along the urethra to dome.
Using the analysis in Section 7.2.3.1, we converted the retardance values (derived
from polar decomposition) to birefringence by measuring the bladder wall thickness
with an optical coherence tomography (OCT) system, and by estimating the average
photon path length in this reflection geometry via our polarization-sensitive Monte
Carlo model (Section 7.4.1.3). The derived wall anisotropy trends thus represent an
intrinsic tissue property of its anisotropy/organization independent of thickness, to
better aid in understanding the structure-functions relation in healthy bladders. These
new insights into the wall microstructure anisotropy of ex vivo distending bladders
may also help improve the functionality of the artificially engineered bladder tissues
[110].
Myocardial muscle tissues exhibit high level of linear birefringence in its healthy
state due to the aligned arrangement of cardiomyocytes and collagen fibers [111].
Following a heart attack, the structural anisotropy is expected to reduce due to struc-
tural remodeling, with cardiomyocyte atrophy and an increase in fibrotic collagen
content (scar tissue). Various postinfarct therapies (e.g., stem cell tissue regenera-
tion) aim to restore heart muscle towards its normal structure (by further structural
remodeling), and more importantly restore some of its functional status. Mueller
matrix measurement and its inverse analysis may serve as a sensitive probe the state
of the myocardium after infarction and report on the success of regenerative treat-
ments [111,112]. In order to explore this possibility, Mueller matrices were recorded
in the transmission geometry, from 1-mm-thick ex vivo myocardial samples from

APPLICATIONS
309
Δn
Bladder3, distended
at 3.3 kPa
Bladder2, distended
at 2.2 kPa
Bladder1, distended
at 1 kPa
Dome
6
5
4
3
2
1
× 10–5
Dorsal
ventral
FIGURE 7.24
Variation of the local birefringence of ex vivo rat bladder tissue with pressure.
The left, middle, and right columns show the retardance images of three different regions
of the organ (dome, dorsal, and ventral surfaces, as shown at the extreme left of the figure)
for distending pressures equal to 1.0, 2.2, and 3.3 kPa, respectively. The field of view is
2 mm in diameter. The images were taken in backscattering geometry, at 25◦from the exact
backscattering direction, and processed by the polar decomposition (see Section 7.2.3.1) to
extract the retardance parameters, namely the scalar retardance and the anisotropy orientation,
the latter being shown by arrows. The tissue birefringence, shown in the color bar scale of
Δn at the extreme right, was calculated from the scalar retardance and the tissue effective
thickness. This thickness was derived from OCT measurements and evaluation of the average
path length of the photon trajectories due to the multiple scattering events by Monte Carlo
simulations. Note the maximum birefringence in the dome region upon maximum distension,
and the overall longitudinal anisotropy orientation (along the urethra-dome axis). Adapted
from Reference 108. (For a color version of this figure, see the color plate section.)
Lewis rats after myocardial infarction, both with and without stem cell treatments.
Measurements were made using both the point measurement and imaging polarime-
try systems. The point measurement system employed polarization modulation and
synchronous lock-in detection (described in Section 7.3.3.2, Fig. 7.12). Imaging
polarimetry employed dc measurements with the PSG-PSA–based approach (also
discussed in Section 7.3) to construct the Mueller matrix. The Mueller matrices mea-
sured by either of these systems were analyzed via polar decomposition to obtain
linear retardance (𝛿) values. The results are summarized in Figure 7.24.

310
TISSUE POLARIMETRY
The observed main features can be summarized as (i) the infarcted region of
the untreated myocardium exhibits a large decrease in the magnitude of 𝛿(Fig.
7.25b). In contrast, in the infarcted region after stem cell treatment an increase in 𝛿
toward the native levels is observed (Fig. 7.25b), indicating regrowth and reorganiza-
tion/remodeling of the myocardium. (ii) The polarimetry images (Fig. 7.25c) from the
same tissue also show similar retardance trends, although with some variations (due
to difference in measurement geometry and spatial heterogeneity in tissue optics).
The spatial variation of the retardance images (Fig. 7.25c) not only shows difference
from infarct to normal, but within each region as well (the 𝛿values are higher in the
middle of the myocardial wall with gradual lower values toward the edges).
This variation through the myocardial wall is attributed to the change in orien-
tation of the myocyte fibers through the wall [112]. Nevertheless, the increase in
𝛿in the infarcted regions of the stem-cell treated hearts indicates reorganization
and regrowth of the myocardium microstructure caused by therapeutic stem cell
injection, as was subsequently confirmed by histology [112]. Note that other tissue
polarimetry metrics that emerged from the Mueller matrix decomposition analy-
sis, namely, diattenuation and depolarization, also yielded complementary and use-
ful microstructural information (on the orientation/alignment of the myocyte fibers
through the wall and their changes with infarction as well as with stem cell treatment)
[44, 111, 112]. Moreover, a study with nonlinear microscopy (second harmonic gen-
eration, two-photon excited fluorescence) has also validated and complemented the
polarimetry results, and yielded useful information on the underlying causes of the
measured retardance signals, in the context of collagen versus cardiomyocytes com-
ponents and their spatial organization. The details of these results have been published
[44, 111, 112].
Although quantification of tissue structural anisotropy via polarimetry have shown
considerable promise for a variety of applications involving tissue diagnosis and ther-
apy monitoring, several technical and conceptual challenges in quantifying intrinsic
tissue anisotropy still remain to be resolved [113,114]. These include understanding
the influence of complex three dimensionally oriented birefringent tissue structures,
orientation-varying spatial domains or potentially non-uniaxial (biaxial) birefringent
domains, on the measured Stokes vector or Mueller matrix elements; and devel-
opment of methods to extract geometry-independent metrics of tissue anisotropy
(intrinsic birefringence and its actual orientation). Recent studies have attempted
to address these issues [109, 113–115]. For example, a sphere-cylinder scattering
model has been adopted in polarization sensitive Monte Carlo simulations to forward
model the effect of such complex microstructural architecture (skeletal muscle as
representative birefringent tissue) on the Mueller matrix elemental images [114]. A
dual projection polarimetry method (whereby the sample is imaged twice at different
incident angles of the probing beam) in combination with Mueller matrix decompo-
sition has also been developed to quantify true intrinsic magnitude and orientation
angle of retardance from three dimensionally oriented birefringent structures [113].
After successful validation of this approach on birefringent spherical phantoms, the
method has been explored for the measurement (imaging) of the anisotropy axis and
its true magnitude in ex-vivo porcine myocardium tissue [113].

Normal
Infarcted
Normal
(b)
(a)
180°
180°
0°
0°
90°
90°
270°
270°
(c)
0
0
1.5
1.0
0.5
0.0
0
45
90
ϕ
π
π
Angular position (  ) (°)
ϕ
ϕ
δ (ra)
135
180
225
270
315
Untreated/control
- - - - - - - - - - - 
360
FIGURE 7.25
Linear retardance 𝛿extracted from experimentally determined Mueller matri-
ces (using the Mueller matrix polar decomposition analysis (Section 7.2.3) corresponding to
1-mm-thick tissue sections from Lewis rat hearts following myocardial infarction. (a) White-
light photographs of untreated (left panel) and stem cell treated (right panel) tissue. (b) Mueller
matrix-derived linear retardance (𝛿) values at different angular positions from the untreated
(solid triangles) and treated (open triangles) samples. (c) The corresponding linear retardance
(𝛿) images for the same tissue derived from imaging polarimetry measurements. Adapted from
References 3 and 111.

312
TISSUE POLARIMETRY
7.5.3
Summary
In this section, we reviewed some examples of “real-world” applications of polarimet-
ric imaging for biomedical purposes. In a first instance, polarization can be used as a
gating mechanism, allowing to isolate nondepolarized contribution in scattered light
to enhance single scattering signal coming from the most superficial tissue layers,
which can be used for example to evaluate the size of the cell nuclei and differentiate
healthy and cancerous tissues.
The multiply scattered contribution, which is usually dominant, typically exhibits
strong depolarization. In the absence of other major polarimetric effects, this param-
eter can be easily evaluated by partial polarimetric techniques like OSC, which are
relatively simple to implement in imaging systems and may prove very useful, for
example in dermatology for assessment of the surgical margins for the removal of
melanomas. Exploratory studies on colon samples suggest that simple OSC may
also be sufficient for early detection and characterization cancerous polyps at various
stages. At the earliest precancerous stages, depolarization decreases (this seems to be
a quite general trend) while at more advanced stages, at least in colon, the depolar-
ization variation is more complex due to increased contribution of the serosa, which
is weakly absorbing and strongly depolarizing.
Conversely, many other samples of interest for medical applications, such as
uterine cervix and oral cavity, may exhibit not only depolarization, but also linear
and circular birefringence. If so, full Mueller polarimetry, with subsequent matrix
decomposition, is needed to “disentangle” these effects to eventually provide relevant
parameters for diagnostics. As a rule of thumb, precancerous evolution seems to be
associated with a decrease of tissue birefringence and depolarizing power, but here
too the detailed behavior may be more complex, at least for cervical tissue, due
to the presence of two very different epithelia (malpighian and columnar). Further
studies, involving tens or even hundreds of samples, are thus needed to fully assess
the performance of polarimetric imaging for cancer detectionand staging.
Polarimetry is also promising in fields other than oncology. Noninvasive glu-
cometry has been viewed as “holy grail” in diabetology for decades. The optical
activity induced by glucose may prove relevant for this purpose, provided extremely
weak rotations can be measured and isolated from artifacts. Highly sensitive Mueller
polarimetry coupled with matrix decomposition has demonstrated the capability to
extract optical rotations in the millidegree range induced by glucose at physiological
levels in the presence of multiple scattering and strong geometry induced artifacts.
However, the elimination of the contribution of optically active compounds other
than glucose remains a formidable challenge.
Last but not least, polarimetry may also prove very useful for noninvasive char-
acterization of many other anisotropic tissues, such as myocardial muscle, where
birefringence seems to provide a very sensitive indicator of the tissue status (healthy
or infarcted or regenerating) and for the follow-up of treatments like tissue recon-
stitution from stem cells. Preliminary studies on bladder wall tissue also show the
relevance of polarimetric imaging for the study of biomechanical properties of tissues,
in part aimed at improving tissue engineering for grafts.

CONCLUSIONS AND OUTLOOK
313
All these examples clearly show the strong potential of polarimetry, both in imag-
ing and nonimaging modes, for medical applications. However, large-scale studies
are still needed for each application to fully assess the performance of the tech-
nique. Moreover, each application may require specialized developments, both in
instrumentation and in data analysis.
7.6
CONCLUSIONS AND OUTLOOK
Biological tissues typically exhibit quite complex polarimetric responses, which
may require sophisticated experimental and data treatment techniques to eventually
provide relevant parameters for medical diagnostics and tissue assessment. This
complexity is certainly the main reason why polarimetry has not yet been developed
to its full potential. However, this field has been progressing very quickly in the past
decade, holding the promise of great improvements in optical diagnostics for many
diseases in various fields.
In almost all tissues, the incident beam is strongly scattered, with scattering path
lengths on the order of ∼100 μm, leading to significant depolarization of the emerg-
ing light. The other polarimetric effects often present are linear retardation, due to
possible tissue linear birefringence, and (generally weaker) optical activity (or cir-
cular retardation/birefringence). Adequate description of these effects requires the
Stokes Mueller formalism, which is not so widely known in biophotonics. It is thus
described in some detail in the first part of this chapter, including the currently avail-
able procedures of matrix decompositions, which are essential for polarimetric data
analysis and constitute an active research field in themselves.
Polarimetric instrumentation is also rapidly evolving, based on the general prin-
ciples and typical setups outlined in the second part. In this respect, it is worth
emphasizing that polarimetry is intrinsically a low cost technique, which can be
added to virtually any optical system, allowing operation at any chosen spatial scale,
from microscopic to mesoscopic to macroscopic.
The interpretation of polarimetric data is also a challenging and active field.
Forward modeling of the polarimetric responses of known systems can be realized by
various approaches, from simple analytical models to numerically demanding Monte
Carlo simulations. Forward modeling has provided considerable physical insight in
the mechanisms of depolarization in suspensions of spherical scatterers in optically
isotropic media. Other practically important properties, such as the effect of tissue
linear and circular birefringence, are currently being actively explored.
While forward modeling is extremely useful to understand the basic mechanisms
defining the polarimetric response of a given type of sample in a given measurement
geometry, for optical diagnostics in principle one has to solve the inverse problem, that
is to determine the tissue “nature” from the measured polarimetric data. Ideally, this
should be achieved by fitting simulation based on a relevant multiparameter physical
model of the tissue to the experimental data. The values of the physical parameters,
such as scatterer densities, optical indexes, layer thicknesses, and the like, could then
be “parametrically” mapped onto “conventional” (optical, histologic, etc.) images

314
TISSUE POLARIMETRY
whose meaning is clear. Unfortunately this is seldom feasible, due to the difficulty of
realistic and accurate tissue models (without relying on too many loosely determined
“parameters”), and to the computational burden related to such fitting procedure. As a
result, polarimetric data are usually interpreted in terms of the elementary polarimetric
properties provided by matrix decompositions. For a number of samples and various
pathologies, these properties exhibit interesting trends which may eventually provide
highly relevant tissue assessment metrics related to underlying tissue biophysical
properties. To this end, the current developments in Mueller matrix decomposition
theory and polarimetric instrumentation advances must be pursued in concert with
extensive ex vivo and in vivo cross-checking and validation studies.
So far, polarimetry has been limited to tissues accessible to direct imaging, and
many possibilities, such as polarimetry-guided surgery, have still to be explored within
this parameter space. But of course the scope of the polarimetric methodologies
would be significantly broadened if it could be extended to endoscopic systems.
Several research groups are at the forefront of this exciting polarimetric development
[116–118].
To conclude, given the expanding range of medical applications and the current
improvements in both experimental setups and data analysis procedures, the medium-
term and long-term future of tissue polarimetry appear very promising.
REFERENCES
[1] V. V. Tuchin, L. Wang, and D. `A. Zimnyakov, Optical Polarization in Biomedical
Applications (Springer, New York, 2006).
[2] L. V. Wang, G. L Cot´e, and S. L. Jacques, “Special section guest editorial: tissue
polarimetry,” J. Biomed. Opt. 7, 278 (2002).
[3] N. Ghosh and I. A. Vitkin, “Tissue polarimetry: concepts, challenges, applications and
outlook,” J. Biomed. Opt. 16, 110801 (2011).
[4] S. L. Jacques, “Polarized light imaging of biological tissues” in Handbook of Biomedical
Optics, edited by D. Boas and N. Ramanujam (CRC Press, 2011).
[5] R. C. Jones, “A new calculus for the treatment of optical systems,” J. Opt. Soc. Am. 31,
488–493 (and following papers in the same issue) (1941).
[6] E. Collett, Polarized Light: Fundamentals and Applications (Marcel Dekker Inc., New
York, 1990).
[7] D. S. Kliger, J. W. Lewis, and C. E. Randall, Polarized Light in Optics and Spectroscopy
(Academic Press–Harcourt Brace Jovanovich, New York, 1990).
[8] J. W. Goodman, “Statistical Properties of Laser Speckle Patterns”, in Laser Speckle and
Related Phenomena, Vol. 9, Topics in Applied Physics, edited by J. C. Dainty (Springer,
Berlin, Heidelberg, New York, Tokyo, 1984).
[9] G. G. Stokes, “On the composition and resolution of streams of polarized light from
different sources,” Trans. Cambridge Phil. Soc. 9, 339–416 (1852).
[10] S. Huard, The Polarization of Light (John Wiley & Sons, New York, 1997).
[11] R. A. Chipman, “Polarimetry,” in Handbook of Optics, 2nd ed., edited by M. Bass
(McGraw-Hill, New York, 1994), Vol. 2, Chap. 22, pp. 22.1–22.37.

REFERENCES
315
[12] D. Goldstein, Polarized Light (Marcel Dekker, New York, 2003).
[13] B. N. Simon, S. Simon, F. Gori, M. Santarsiero, R. Borghi, N. Mukunda, and R. Simon,
“Nonquantum entanglement resolves a basic issue in polarization optics,” Phys. Rev.
Lett. 104, 023901 (2010).
[14] E. Wolf, “Unified theory of coherence and polarization of random electromagnetic
beams,” Phys. Lett. A 312, 263–267 (2003).
[15] G. Milione, H. I. Sztul, D. A. Nolan, and R. R. Alfano, “Higher-order Poincar´e sphere,
stokes parameters, and the angular momentum of light,” Phys. Rev. Lett. 107, 053601
(2011).
[16] G. Milione, S. Evans, D. A. Nolan, and R. R. Alfano, “Higher order Pancharatnam-Berry
phase and the angular momentum of light,” Phys. Rev. Lett. 108, 190401 (2012).
[17] L. Marrucci, E. Karimi, S. Slussarenko, B. Piccirillo, E. Santamato, E. Nagali, and
Fabio Sciarrino, “Spin-to-orbital conversion of the angular momentum of light and its
classical and quantum applications,” J. Opt. 13, 064001 (2011).
[18] H. Mueller, “The foundation of optics,” J. Opt. Soc. Am. 38, 551 (1948)
[19] J. J. Gil, “Characteristic properties of Mueller matrices,” J. Opt. Soc. Am. A 17, 328–334
(2000).
[20] S. R. Cloude, “Group theory and polarisation algebra,” Optik 75, 26 (1986).
[21] D. G. M. Anderson and R. Barakat, “Necessary and sufficient conditions for a Mueller
matrix to be derivable from a Jones matrix,” J. Opt. Soc. Am. A 11, 2305 (1994).
[22] J. J. Gil and E. Bernabeu, “A depolarization criterion in Mueller matrices,” Opt. Act.
32, 259–261 (1985).
[23] M. Born and E. Wolf, Principles of Optics (Cambridge University Press, New York,
2002).
[24] S. Lu and R. A. Chipman, “Interpretation of Mueller matrices based on polar decompo-
sition,” J. Opt. Soc. Am. A 13, 1106–1113 (1996).
[25] M. Xu and R. R. Alfano, “Random walk of polarized light in turbid media,” Phys. Rev.
Lett. 95, 213901 (2005).
[26] R. Ossikovski, M. Anastasiadou, and A. de Martino, “Product decompositions of
depolarizing Mueller matrices with negative determinants,” Opt. Commun. 281, 2406
(2008).
[27] S. P. Morgan and M. E. Ridgway, “Polarization properties of light backscattered from a
two layer scattering medium,” Opt. Express 7, 395–402 (2000).
[28] A. H. Hielscher, A. A. Eick, J. R. Mourant, D. Shen, J. P. Freyer, and I. J. Bigio, “Diffuse
backscattering Mueller matrices of highly scattering media,” Opt. Express 1, 441–453
(1997).
[29] V. Sankaran, J. T. Walsh Jr., and D. J. Maitland, “Comparative study of polarized light
propagation in biologic tissues,” J. Biomed. Opt. 7, 300–306 (2002).
[30] M.-R Antonelli, A. Pierangelo, T. Novikova, P. Validire, A. Benali, B. Gayet, and A.
De Martino, “Mueller matrix imaging of human colon tissuefor cancer diagnostics: how
Monte Carlo modeling can help in the interpretation of experimental data,” Opt. Express
18, 10200–10208 (2010).
[31] M. Ahmad, S. Alali, A. Kim, M. F. G. Wood, M. Ikram, and I.A. Vitkin, “Do different
turbid media with matched bulk optical properties also exhibit similar polarization
properties ?” Biomed. Opt. Express 2, 3248–3258 (2011).

316
TISSUE POLARIMETRY
[32] N. Ghosh, M. F. G. Wood, and I. A. Vitkin, “Influence of the order of the constituent
basis matrices on the Mueller matrix decomposition derived polarization parameters
in complex turbid media such as biological tissues,” Opt. Commun. 283, 1200–1208
(2010).
[33] J. Morio and F. Goudail, “Influence of the order of diattenuator, retarder, and polarizer
in polar decomposition of Mueller matrices,” Opt. Lett. 29, 2234–2236 (2004).
[34] R. Ossikovski, A. De Martino, and S. Guyot, “Forward and reverse product decompo-
sitions of depolarizing Mueller matrices,” Opt. Lett. 32, 689 (2007).
[35] M. Anastasiadou, S. Ben Hatit, R. Ossikovski, S. Guyot, and A. de Martino, “Experi-
mental validation of the reverse polar decomposition of depolarizing Mueller matrices,”
J. Eur. Opt. Soc (JEOS) – Rapid Publ. 2, 07018 (2007).
[36] R. Ossikovski, M. Anastasiadou, S. Ben Hatit, E. Garcia-Caurel, and A. de Martino,
“Depolarizing Mueller matrices: how to decompose them?” Phys. Stat. Sol (a) 205,
720–727 (2008).
[37] R. Ossikovski, “Analysis of depolarizing Mueller matrices through a symmetric decom-
position,” J. Opt. Soc. Am. A 26, 1109–1118 (2009)
[38] C. Fallet, A. Pierangelo, R. Ossikovski, and A. de Martino, “Experimental validation of
the symmetric decomposition of Mueller matrices,” Opt. Express 18, 831–842, (2009).
[39] R. Ossikovski, C. Fallet, A. Pierangelo, and A. de Martino, “Experimental implemen-
tation and properties of Stokes nondiagonalizable depolarizing Mueller matrices,” Opt.
Lett. 34, 974 (2009).
[40] R. Ossikovski, M. Foldyna, C. Fallet, and A. de Martino, “Experimental evidence for
naturally occurring nondiagonal depolarizers,” Opt. Lett. 34, 2426–2428 (2009).
[41] R. Ossikovski, “Differential matrix formalism for depolarizing anisotropic media,” Opt.
Lett. 36, 2330–2332 (2011).
[42] N. Ortega-Quijano and J. L. Arce-Diego, “Depolarizing differential Mueller matrices,”
Opt. Lett. 36, 2429–2431 (2011).
[43] R. M. A. Azzam, “Propagation of partially polarized light through anisotropic media
with or without depolarization: a differential 4 × 4 matrix calculus,” J. Opt. Soc. Am.
68, 1756–1767 (1978).
[44] S. Kumar, H. Purwar, R. Ossikovski, I. Alex Vitkin, and N. Ghosh, “Comparative
study of differential matrix and extended polar decomposition formalisms for polari-
metric characterization of complex tissue-like turbid media,” J. Biomed. Opt. 17, 105006
(2012).
[45] N. Ortega-Quijano, B. Haj-Ibrahim, E. Garc´ıa-Caurel, J.-L. Arce-Diego, and R.
Ossikovski, “Experimental validation of Mueller matrix differential decomposition,”
Opt. Express 20, 1151–1163 (2012).
[46] D. H. Goldstein, “Mueller matrix dual-rotating retarder polarimeter,” Appl. Opt. 31,
6676–6683 (1990).
[47] D. S. Sabatke, M. R. Descour, E. L. Dereniak, W. C. Sweatt, S. A. Kemme, and G. S.
Phipps, “Optimization of retardance for a complete Stokes polarimeter,” Opt. Lett. 25,
802–804 (2000).
[48] J. S. Tyo, “Noise equalization in Stokes parameter images obtained by use of variable-
retardance polarimeters,” Opt. Lett. 25, 1198–1200 (2000).
[49] E. Compain and B. Dr´evillon, “High-frequency modulation of the four states of polar-
ization of light with a single phase modulator,” Rev. Sci. Instrum. 69, 1574–1580 (1998).

REFERENCES
317
[50] M. H. Smith, “Optimization of a dual-rotating-retarder Mueller matrix polarimeter,”
Appl. Opt. 41, 2488–2493 (2002).
[51] J. Zallat, S. A¨ınouz S, and M. Ph. Stoll, “Optimal configurations for imaging polarime-
ters: impact of image noise and systematic errors,” J. Opt. A. Pure Appl. Opt. 8, 807
(2006).
[52] D. Layden, M. F. G. Wood, and A. Vtikin, “Optimum selection of input polariza-
tion states in determining the sample Mueller matrix: a dual photoelastic polarimeter
approach,” Opt. Express 20, 20466–20481 (2012).
[53] E. Collett, “Measurement of the four Stokes polarization parameters with a single
circular polarizer,” Opt. Commun. 52, 77–80 (1984).
[54] R. W. Collins and J. Koh, “Dual rotating-compensator multi-channel ellipsometer:
instrument design for real-time Mueller matrix spectroscopy of surfaces and films,”
J. Opt. Soc. Am. A 16, 1997–2006 (1999).
[55] C.-Y. Han and Y.-F. Chao, “Photoelastic modulated imaging ellipsometry by strobo-
scopic illumination technique,” Rev. Sci. Instrum. 77, 023107–5 (2006).
[56] M. Richert, X. Orlik, and A. De Martino, “Adapted polarized state contrast image,” Opt.
Express 17, 14199–14210 (2009).
[57] G. Anna, F. Goudail, and D. Dolfi “General state contrast imaging: an optimized polari-
metric imaging modality insensitive to spatial intensity fluctuations,” J. Opt. Soc. Am.
A 29, 892–900 (2012).
[58] M. Anastasiadou, A. de Martino, D. Cl´ement, F. Li`ege, B. Laude-Boulesteix, N. Quang,
J. Dreyfuss, B. Huynh, A. Nazac, L. Schwartz, et al. “Polarimetric imaging for the
diagnosis of cervical cancer,” Phys. Stat. Sol. (c) 5, 1423 (2008).
[59] A, B´eni`ere, M. Alouini, F. Goudail, and D. Dolfi, “Design and experimental val-
idation of a snapshot polarization contrast imager,” Appl. Opt. 48, 5764–5773
(2009).
[60] J. C. Ramella-Roman, K. Lee, S. A. Prahl, and S. L. Jacques, “Designing, testing and
clinical studies of a handheld polarized light camera,” J. Biomed. Opt. 9, 1305–1310
(2004).
[61] M. F. G. Wood, N. Ghosh, X. Guo, and I. A. Vitkin, “Towards noninvasive glucose
sensing using polarization analysis of multiply scattered light,” in Handbook of Optical
Sensing of Glucose in Biological Fluids and Tissues, Series in Medical Physics and
Biomedical Engineering, edited by V. V. Tuchin (Taylor and Francis Publishing, London,
2008), Vol. 12, Chap. 17.
[62] N. Ghosh, M. Wood, and A. Vitkin, “Polarized light assessment of complex turbid
media such as biological tissues using Mueller matrix decomposition,” in Handbook of
Photonics for Biomedical Science, edited by V. V. Tuchin (Taylor and Francis Publishing,
London, 2010), Chap. 9.
[63] M. H. Smith, P. Burke, A. Lompado, E. Tanner, and L. W. Hillman “Mueller matrix
imaging polarimetry in dermatology,” Proc. SPIE 3911, 210–216 (2000)
[64] A. Pierangelo, A. Nazac, A. Benali, P. Validire, H. Cohen, T. Novikova, B. Haj Ibrahim,
S. Manhas, C. Fallet, M.-R. Antonelli, et al. “Polarimetric imaging of uterine cervix: a
case study,” Opt. Express 21, 14120–14130 (2013).
[65] D. Bicout, C. Brosseau, A. S. Martinez, and J. M. Schmitt, “Depolarization of multiply
scattered waves by spherical diffusers: influence of size parameter,” Phys. Rev. E 49,
1767–1770 (1994).

318
TISSUE POLARIMETRY
[66] E. E. Gorodnichev, A. I. Kuzolov, and D. B. Rozozkin, “Diffusion of circularly polarized
light in a disordered medium with large scale inhomogeneities,” JETP Lett. 68, 22–28
(1998).
[67] A. D. Kim and M. Moscoso, “Influence of the refractive index on the depolarization of
multiply scattered waves,” Phys. Rev. E 64, 026612 (2001).
[68] L. F. Rojas-Ochoa, D. Lacoste, R. Lenke, P. Schurtenberger, and F. Scheffold, “Depo-
larization of backscattered linearly polarized light,” J. Opt. Soc. Am. A 21, 1799–1804
(2004).
[69] M. Xu and R. R. Alfano, “Random walk of polarized light in turbid media,” Phys. Rev.
Lett. 95, 213901 (2005).
[70] A. J. Welch, M. J. C. Van Germert, W. M. Star, and B. C. Wilson, “Overview of tissue
optics,” in Optical Thermal Response of Laser Irradiated Tissue, edited by A. J. Welch
and M. J. C. Van Germert (Plenum, New York, 1995).
[71] L. Wang, S. L. Jacques, and L. Zheng, “MCML—Monte Carlo modeling of light
transport in multi-layered tissues,” Comput. Methods Programs Biomed. 47, 131–146
(1995).
[72] N. Ghosh, P. K. Gupta, A. Pradhan, and S. K. Majumder, “Anomalous behavior of
depolarization of light in a turbid medium,” Phys. Lett. A 354, 236–242 (2006).
[73] V. Sankaran, K. Schonenberger, J. T. Walsh Jr., and D. J. Maitland, “Polarization
discrimination of coherently propagating light in turbid media,” Appl. Opt. 38, 4252–
4261 (1999).
[74] V. Sankaran, J. T. Walsh Jr., and D. J. Maitland, “Comparative study of polarized light
propagation in biological tissues,” J. Biomed. Opt. 7, 300–306(2002).
[75] N. Ghosh, A. Pradhan, P. K. Gupta, S. Gupta, V. Jaiswal, and R. P. Singh, “Depolarization
of light in a multiply scattering medium: effect of refractive index of scatterer,” Phys.
Rev. E 70, 066607 (2004).
[76] M. F. G. Wood, X. Guo, and I. A. Vitkin, “Polarized light propagation in multiply
scattering media exhibiting both linear birefringence and optical activity: Monte Carlo
model and experimental methodology,” J. Biomed. Opt. 12, 014029 (2007).
[77] J. C. Ramella-Roman, S. A. Prahl, and S. L. Jacques, “Three Monte Carlo programs
of polarized light transport into scattering media: part I,” Opt. Express 13, 4420–4438
(2005).
[78] M. Moscoso, J. B. Keller, and G. Papanicolaou, “Depolarization and blurring of optical
images by biological tissue,” J. Opt. Soc. Am. A 18, 948–960 (2001).
[79] R. Clark Jones, “New calculus for the treatment of optical systems. VII. Properties of
the N-matrices,” J. Opt. Soc. Am. 38, 671–685 (1948).
[80] S. Alali, Y. Wang, and I. Alex Vitkin “Detecting axial heterogeneity of birefringence in
layered turbid media using polarizd light imaging,” Biomed. Opt. Express 3, 3250–3263
(2012)
[81] N. Ghosh, M. F. G. Wood, and I. A. Vitkin, “Mueller matrix decomposition for extraction
of individual polarization parameters from complex turbid media exhibiting multiple
scattering, optical activity and linear birefringence,” J. Biomed. Opt. 13, 044036 (2008).
[82] N. Ghosh, M. F. G. Wood, and I. A. Vitkin, “Polarimetry in turbid, birefringent, optically
active media: a Monte Carlo study of Mueller matrix decomposition in the backscattering
geometry,” J. Appl. Phys. 105, 102023 (2009).

REFERENCES
319
[83] S. G. Demos, W. B. Wang, J. Ali, and R. R. Alfano, “New optical difference approaches
for subsurface imaging of tissues,” OSA TOPS 21, in Advances in Optical Imaging and
Photon Migration, edited by J. G. Fujimoto and M. S. Patterson, pp. 405–410 (1998).
[84] Y. Liu, Y. L. Kim, X. Li, and V. Backman, “Investigation of depth selectivity of polar-
ization gating for tissue characterization,” Opt. Express 13, 601–611 (2005).
[85] S. G. Demos, H. B. Radousky, and R. R. Alfano, “Deep subsurface imaging in tissues
using spectral and polarization filtering,” Opt. Express 7, 23–28 (2000).
[86] A. P. Sviridov, V. C. M. Hassan, A. C. Boccara, A. Russo, P. Smith, and A. Gandjbackche,
“Enhancement of hidden structures of early skin fibrosis using polarization degree
patterns and Pearson correlation analysis,” J. Biomed. Opt. 10, 051706 (2005).
[87] S. L. Jacques, J. C. Ramella-Roman, and K. Lee, “Imaging skin pathology with polarized
light,” J. Biomed. Opt. 7, 329–340 (2002).
[88] S. L. Jacques, R. Samathama, S. Isenhath, and K. Lee, “Polarized light camera to guide
surgical excision of skin cancers,” Proc. SPIE 6842 68420I (2008).
[89] V. Backman, M. B. Wallace, L. T. Perelman, J. T. Arendt, R. Gurjar, M. G. Muller, Q.
Zhang, G. Zonios, E. Kline, T. McGillican, et al., “Detection of preinvasive cancer cells
in situ,” Nature (London) 406, 35–36 (2000).
[90] M. Hunter et al., “Tissue self-affinity and polarized light scattering in the born approxi-
mation: a new model for precancer detection,” Phys. Rev. Lett. 97, 138102 (2006).
[91] N. Ghosh, S. K. Majumder, H. S. Patel, and P. K. Gupta, “Depth resolved fluorescence
measurement in layered turbid medium by polarized fluorescence spectroscopy,” Opt.
Lett. 30, 162–164 (2005).
[92] Z. J. Smith and A. J. Berger, “Surface-sensitive polarized Raman spectroscopy of
biological tissue,” Opt. Lett. 30, 1363–1365 (2005).
[93] R. S. Gurjar, V. Backman, L. T. Perelman, I. Georgakoudi, K. Badizadegan, I. Itzkan,
R. R. Dasari, and M. S. Feld, “Imaging human epithelial properties with polarized light
scattering spectroscopy,” Nat. Med. 7, 1245–1249 (2001).
[94] N. C. Biswal, S. Gupta, N. Ghosh, and A. Pradhan, “Recovery of intrinsic fluorescence
from the measured fluorescence in a turbid medium: an experimental approach,” Opt.
Express 11, 3320–3331 (2003).
[95] R. J. Mc Nichols and G. L. Cote, “Optical glucose sensing in biological fluids: an
overview,” J. Biomed. Opt. 5, 5–16 (2000).
[96] M. F. G. Wood, D. Cˆot´e, and I. A. Vitkin, “Combined optical intensity and polarization
methodology for analyte concentration determination in simulated optically clear and
turbid biological media,” J. Biomed. Opt. 13, 044037 (2008).
[97] J. Chung, W. Jung, M. J. Hammer-Wilson, P. Wilder-Smith, and Z. Chen, “Use of
polar decomposition for the diagnosis of oral precancer” Appl. Opt. 46, 3038–3045
(2007).
[98] A. Pierangelo, A. Benali, M. R. Antonelli, T. Novikova, P. Validire, B. Gayet, and A.
de Martino, “Ex-vivo characterization of human colon cancer by Mueller polarimetric
imaging,” Opt. Express 19, 1582–1593 (2011).
[99] A. Pierangelo, S. Manhas, A. Benali, C. Fallet, J.-L. Totobenazara, M. R. Antonelli, T.
Novikova, B. Gayet, A. de Martino, and P. Validire “Multispectral Mueller polarimetric
imaging detecting residual cancer and cancer regression after neoadjuvant treatment for
colorectal carcinomas,” J. Biomed. Opt. 18, 046014 (2013).

320
TISSUE POLARIMETRY
[100] O. Visser, R. Bakx, F. A. Zoetmulder, C. C. Levering, S. Meijer, J. F. Slors, and J. J. van
Lanschot, “The influence of total mesorectal excision on local recurrence and survival in
rectal cancer patients: a population-based study in Greater Amsterdam,” J. Surg. Oncol.
95(6), 447–454 (2007).
[101] M. Underwood, M. Arbyn, W. Parry-Smith, S. De Bellis-Ayres, R. Todd, C. Redman,
and E. Moss, “Accuracy of colposcopy-directed punch biopsies: a systematic review
and meta-analysis,” BJOG 119, 1293–1301 (2012).
[102] N. Thekkek and R. Richards-Kortum, “Optical imaging for cervical cancer detection:
solutions for a continuing global problem,” Nat. Rev. Cancer 8, 725–731 (2008).
[103] M. Cardenas-Turanzas, J. A. Freeberg, J. L. Benedet, E. N. Atkinson, D. D. Cox, R.
Richards-Kortum, C. MacAuley, M. Follen, and S. B. Cantor, “The clinical effectiveness
of optical spectroscopy for the in vivo diagnosis of cervical intraepithelial neoplasia:
where are we?,” Gynecol. Oncol. 107 S138–S146 (2007).
[104] P. E. Gravitt, P. Paul, H. A. Katki, H. Vendantham , G. Ramakrishna, M. Sudula, B.
Kalpana, B. M. Ronnett, K. Vijayaraghavan, K. V. Shah, and CATCH Study Team.
“Effectiveness of VIA, Pap, and HPV DNA testing in a cervical cancer screening
program in a peri-urban community in Andhra Pradesh, India,” PLoS ONE 5(10),
e13711 (2010). doi:10.1371/journal.pone.0013711
[105] P. Shukla and A. Pradhan, “Mueller decomposition images for cervical tissue: potential
for discriminating normal and dysplastic states,” Opt. Express 17, 1600–1609 (2009).
[106] D. Arifler, I. Pavlova, A. Gillenwater, and R. R. Kortum, “Light scattering from collagen
fiber networks: micro-optical properties of normal and neoplastic stroma,” J. Biophys.
92, 3260–3274 (2007).
[107] D. J. Maitland and J. T. Walsh Jr., “Quantitative measurements of linear birefringence
during heating of native collagen,” Lasers Surg. Med. 20, 310–318 (1997).
[108] M. F. G. Wood, N. Ghosh, E. H. Moriyama, B. C. Wilson, and I. A. Vitkin, “Proof-of-
principle demonstration of a Mueller matrix decomposition method for polarized light
tissue characterization in vivo,” J. Biomed. Opt. 14, 014029(2009).
[109] S. Alali, K. Aitken, D. Bagli, and I. A. Vitkin, “Optical assessment of anisotropy in
ex-vivo distended rat bladders,’’ J. Biomed. Opt. 17, 109801 (2012).
[110] G. S. Jack, R. Zhang, M. Lee, Y. Xu, B. M. Wu, and L. V. Rodrigues, “Urinary bladder
smooth muscle engineered from adipose stem cells and a three dimensional synthetic
composite,’’ Biomaterials 30, 3259–3270 (2009)
[111] N. Ghosh, M. F. G. Wood, S. Li, R. D. Weisel, B. C. Wilson, R. Li, and I. A. Vitkin,
“Mueller matrix decomposition for polarized light assessment of biological tissues,” J.
Biophoton. 2, 145 (2009).
[112] M. F. G. Wood, N. Ghosh, M. A. Wallenburg, S.-H. Li, R. D. Weisel, B. C. Wilson,
R.-Ki Li, and A. Vitkin, “Polarization birefringence measurements for characterizing
the myocardium, including healthy, infracted, and stem cell treated regenerating cardiac
tissues,” J. Biomed. Opt. 15, 047009 (2010).
[113] M. A. Wallenburg, M. F. G. Wood, N. Ghosh, and I. A. Vitkin, “Polarimetry-based
method to extract geometry-independent metrics of tissue anisotropy,” Opt. Lett. 35,
2570–2572 (2010).
[114] H. He, Z. Nan, L. Ran, Y. Tianliang, L. Wei, H. Yonghong, and M. Hui, “Application
of sphere-cylinder scattering model to skeletal muscle,” Opt. Express 18, 15104–15112
(2010).

REFERENCES
321
[115] S. Alali, Y. Wang, and I. A. Vitkin, “Detecting axial heterogeneity of birefringence in
layered turbid medial using polarized light imaging,” Biomed. Opt. Express 3, 3250–
3263 (2012).
[116] A. Myakov, L. Nieman, L. Wicky, U. Utzinger, R. Richards-Kortum, and K. Sokolov,
“Fiber optic probe for polarized reflectance spectroscopy in vivo: design and perfor-
mance,” J. Biomed. Opt. 7(3), 388–397 (2002).
[117] V. M. Turzhitsky, A. J. Gomes, Y. L. Kim, Y. Liu, A. Kromine, J. D. Rogers, M.
Jameel, H. K. Royand, and V. Backman,” Measuring mucosal blood supply in vivo with
a polarization gating probe,” Appl. Opt. 47(32), 6046–6057 (2008).
[118] T. C. Wood and D. S. Elson, “Polarization response measurement and simulation of
rigid endoscopes,” Biomed. Opt. Express 1, 463–471 (2010).


8
OPTICAL WAVEGUIDE BIOSENSORS
Daphn´e Duval and Laura M. Lechuga
NanoBiosensors and Bioanalytical Applications Group, Centro Investigacion Biomedica
en Red – Bioingenieria, Biomateriales y Nanomedicina, Consejo Superior de
Investigaciones Cientificas, Institut Catala de Nanociencia i Nanotecnologia, Bellaterra,
Barcelona, Spain
8.1
INTRODUCTION
A biosensor is a self-contained device composed of a bioreceptor and a transducer.
The substance to be detected, the analyte, is captured by the appropriate bioreceptor
(DNA, antibodies, etc.) taking advantage of the high specificity of the biomolecular
recognition process. This biomolecular detection is then converted by the transducer
into a physical signal that can be further processed and related to the concentration
of the substance under analysis. As compared to standard analytical techniques that
are usually time-consuming and expensive and require trained personnel, biosensors
offer clear advantages as they can allow simple, rapid, and continuous in situ detection
for a broad range of applications such as clinical diagnostic, biomedical research,
drug discovery or pathogen, and toxin detection in food and water.
A wide range of biosensors have been explored based on optical, electrical, mag-
netic, and mechanical principles. Among them, optical biosensors have undisputed
advantages when compared to their counterparts: noninvasive and nondestructive
nature, absence of risks of electrical shocks or explosion, immunity to electromagnetic
interferences, higher sensitivity, miniaturization capabilities, and possibility of mul-
tiplexing. Optical biosensors can operate according to two protocols: fluorescence-
based detection and label-free detection. In fluorescence-based detection, fluorescent
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
323

324
OPTICAL WAVEGUIDE BIOSENSORS
labels are used to indicate the presence of the target molecule resulting in an extremely
sensitive detection. However, the preparatory labeling procedure is costly and labo-
rious and is subjected to possible contaminations, human errors, mechanical dam-
age of the sensing probe, or even alterations of the natural function of the tagged
biomolecule. In addition, fluorescence signal makes quantitative analysis difficult due
to the inability to precisely manipulate the amount of label for each target analyte. In
contrast, in a label-free scheme the molecules are not labeled and can be detected in
their natural forms. This type of detection is relatively simple and cost-effective and
enables the quantitative and kinetic estimation of biomolecular interactions in com-
plex real samples such as blood, serum, and urine. Main label-free optical detection
methods are based on refractive index (RI), optical absorption, and Raman spectro-
scopic detection [1].
Fluorescence-based biosensors have been extensively studied and described in a
large number of reviews and books [2–4]. In this chapter, we will mainly discuss
label-free detection due to its inherent advantages. The focus will be on RI-based
label-free optical biosensors which employ the evanescent field detection principle.
Surface plasmon resonance sensors falls into this category but we will focus here
only in optical waveguide biosensors including devices using optical fibers (OFs)
and integrated optics (IO)-based waveguides. IO-based sensors include interferome-
ters, microcavities, grating-, and photonic crystal (PC)-based structures. More recent
trends such as biosensors based on porous silicon or silicon photonic wires will also
be described. In the case of OF-based biosensors, a few configurations employ label-
free detection and we will discuss grating-, interferometric-, and absorbance-based
OF biosensors.
First, we detail the fundamentals related to label-free optical waveguide biosens-
ing. In the following, we discuss the surface functionalization strategies required to
immobilize the bioreceptors on the waveguide surface and the criteria and procedures
to evaluate optical biosensors. Then, we review the state-of-the-art in RI-based label-
free biosensors, dividing them into IO-based biosensors and OF-based biosensors,
with emphasis on the most relevant ones in terms of sensing performance. Finally,
we discuss the integration of optical biosensors into lab-on-a-chip (LOC) platforms
for point-of-care diagnosis.
8.2
FUNDAMENTALS OF LABEL-FREE OPTICAL
WAVEGUIDE BIOSENSING
8.2.1
Principle of Operation
An optical waveguide is composed of a material with a certain refractive index in
which the light is confined (the core), surrounding by materials with lower refractive
indices (the claddings). IO waveguides can confine the light in one dimension (planar
waveguides) or in two dimensions (channel and rib waveguides). In the case of OFs,
light is confined in two dimensions within cylindrical layers. The guided modes of a
waveguide are described by Maxwell’s equations and can be characterized by their
effective refractive indices [5].

FUNDAMENTALS OF LABEL-FREE OPTICAL WAVEGUIDE BIOSENSING
325
Cladding/Substrate
Core
neff
neff’
Sensing area
Cladding
dp
FIGURE 8.1
Scheme of the evanescent field detection principle.
The modes propagate in the waveguide by total internal reflections (TIRs) at the
core-cladding interface. At the point of reflection, a standing wave of the electro-
magnetic field, the evanescent field, is created and penetrates into the cladding with
an exponential decay. As shown in Figure 8.1, the evanescent field acts as a probe.
When the analyte and its corresponding bioreceptor previously immobilized on the
waveguide surface react together, it results in a variation of the refractive index of
the cladding. This variation of refractive index affects the effective refractive index
of the guided mode through the modification of its evanescent field. The concept of
evanescent field sensing was first reported by Lukosz and Tiefenthaler in 1983 [6,7]
and has been the subject of an increasing number of publications since then.
The depth dp at which the intensity of the electric or magnetic field of the evanes-
cent field drops to 1/e of its original value is called the depth of penetration and is
expressed as
dp = 𝜆
2𝜋⋅
1
√
n2
1 sin2 𝜃−n2
2
,
(8.1)
where 𝜆is the wavelength of light, 𝜃the incident angle, and n1 and n2 the refractive
indices of the core and the cladding, respectively. Typical penetration depths range
between 50 and 700 nm, depending mainly on the wavelength and on the contrast
between the refractive indices of the core and the cladding. With a penetration depth
in the range of tens of nanometers, only the analytes captured by the immobilized
bioreceptors will be detected since the ones not captured will be too far away from
the sensor surface to be sensed by the evanescent field. Therefore, it is not necessary
to label the target molecules neither to carry out a prior separation of nonspecific
components. Evanescent field detection principle enables a label-free, highly sensi-
tive, and specific detection if appropriate bioreceptors and immobilization protocols
are employed.
Both transverse electric (TE) and transverse magnetic (TM) polarized guided
light can be used in the evanescent wave detection mechanism. However, TM modes
usually have larger evanescent field which means greater sensitivity than TE modes.

326
OPTICAL WAVEGUIDE BIOSENSORS
Regarding the wavelength, working in the infrared range would provide a longer
penetration depth than in the visible range as can be deduced from Eq. (8.1). But this
is not always translated in an increase of the sensitivity since the water absorption is
much higher in the infrared than in the visible: at 625 nm, the losses in water are of 2 ×
10−3 cm−1 while at 3 μm they are as high as 11,400 cm−1 [8]. Such increase in optical
loss will significantly affect the signal, mainly the noise, and consequently the limit of
detection (LOD) of the sensor. A balance should be found between a large penetration
depth and the losses in order to obtain an optimum detection efficiency, noise signal,
and sensitivity [9]. Up to now, most of the biosensors have been developed for
operation in the visible or near-infrared range, mainly at 633 and 1550 nm.
8.2.2
Optical Waveguide Technology
8.2.2.1
Integrated Optical Waveguide
Although a wide range of material and
fabrication processes are available for optical waveguide technology [5], only a
few of them have the required properties to afford highly sensitive biosensors. The
sensitivity of an optical waveguide biosensor is strongly dependent on the waveguide
materials and its design. In particular, the electric field strength at the core-cladding
interface as well as the overlap between the evanescent field and the sensing area have
to be optimized to maximize the impact on the effective refractive index of the guided
wave. This generally implies the use of large index contrast materials for the core
and the claddings, a combination that provides an evanescent field much larger than
for low contrast waveguides (Eq. 8.1). In addition, the materials must be transparent
and have low propagation losses at the wavelength of interest.
With the above considerations, the materials commonly employed as waveguide
materials include silicon nitride (Si3N4, n = 1.9 to 2.1), silicon oxynitride (SiOxNY,
n = 1.45 to 2.1), tantalum pentoxide (Ta2O5, n = 2.1 to 2.3), and titanium dioxide
(TiO2, n = 2.5 to 2.8) on silicon or glass substrates. These materials lead to an
index difference between core and cladding in the order of 10−1 refractive index unit
(RIU). More recently, silicon-on-insulator (SOI)-based structures have emerged with
excellent results. Lithium niobate (LiNbO3) or III–V semiconductor materials are
widely employed for optical telecommunications but they have shown less suitability
for biosensing as they are more complex and expensive while their electro-optical or
piezo-electrical properties are generally pointless for biosensing.
Polymer materials have also been employed due to their interesting properties as
high transparency, versatility, easy and low-cost fabrication, and possibility of mass
production, although they do not comply with the requirement of large index contrast.
Usual index contrast in polymers is in the order of 10−3 RIU, that is, two orders lower
than what can be achieved with silicon photonics. According to Eq. (8.1), a weaker
evanescent field will be generated, leading to lower sensitivity. Even though this
loss of sensitivity can be balanced by larger waveguide dimensions and consequent
better in-coupling of light, the reported LODs are worse than those of silicon-based
devices. In the last years, new polymers have been synthesized with index contrast in
the order of 10−2 RIU, opening the route to future polymeric biosensors with better
performances [10–12].

FUNDAMENTALS OF LABEL-FREE OPTICAL WAVEGUIDE BIOSENSING
327
Common fabrication processes for optical waveguide biosensors include ion diffu-
sion in glass, chemical vapor deposition, spin-coating, nanoimprinting, photolithog-
raphy, and electron-beam lithography.
8.2.2.2
Optical Fibers
In conventional optical fibers, the core and the cladding are
both made of silica. The core is generally doped with germanium to slightly increase
its refractive index for light propagation by TIR. For a single-mode OF with step-
index profile, the core diameter is between 8 and 12 μm and the cladding diameter is
of 125 μm. For multimodal fiber, the diameter of the core is usually between 20 and
200 μm, for a cladding diameter around 125–400 μm.
To fabricate OFs, a large diameter preform (about 10 cm in diameter) is first
manufactured, by vapor-phase oxidation or direct-melt process, with a careful control
of the index profile to differentiate the core from the cladding. The preform is then
placed in a drawing tower where the preform tip is heated and the OF is pulled
out as a string until reaching the final diameter. Coatings are then deposited to
physically protect the OF. For biosensing applications, the cladding and the coatings
are removed to define the sensing area. This can be achieved by mechanical polishing
or by chemical etching.
Fibers appear as a convenient material for optical sensors because they are inex-
pensive and provide an easy and efficient signal delivery as well as multiplexed
capabilities and long interaction lengths. However, the index contrast of an OF is
very low, about 0.5–1 × 10−2 RIU, resulting in a small penetration depth of the
evanescent field and, consequently, in a low sensitivity. To circumvent this drawback,
two approaches are normally employed: amplification of the signal response by flu-
orescence or/and the use of special geometries to increase the evanescent field [13].
When tapering an OF, the fractional power guided in the core decreases and part
of the energy passes into the cladding, resulting in an increase of the evanescent field
magnitude and penetration depth. To do so, the OF is fixed at two ends on a movable
translation stage of a pulling machine. The middle of the fiber is heated with a flame
or a laser beam and, at the same time, the translation stages move in the opposite
direction. The glass melts and the fiber is elongated so that its diameter decreases. As
illustrated in Figure 8.2b, a continuous tapered OF consists of a convergent region (the
taper transition) and of a region of almost fixed diameter (the taper waist). Waists of
Cladding
Core
Untapered
fiber
Taper
transition
Taper
waist
Waist diameter
Core
Cladding
(a)
(b)
FIGURE 8.2
Schemes of (a) a single-mode fiber and (b) a tapered fiber.

328
OPTICAL WAVEGUIDE BIOSENSORS
1–10 mm in length and diameters down to 100 nm can be obtained. Another approach
to increase sensitivity is to bend the OF. Bending produces not only an increase of the
losses but also an increase of the evanescent field. It is usually obtained by forming
a U-shape probe with the core of a uniformly decladded fiber.
The development of microstructured optical fibers (MOFs) opened new oppor-
tunities for OF-based biosensing. MOFs are OFs that incorporate air holes within
their cross-section to control the interaction between the guided light and the ana-
lytes located within the holes, while simultaneously acting as tiny fluidic channels. As
MOFs can be fabricated from a single material, it is possible to use materials that were
unavailable for conventional OFs like silica, lead silica, tellurite, and bismuth glass.
First a structured preform is produced, with extruded items in the range of 10–15 mm.
This structured preform is then drawn out until reaching the intended diameter. With
the technological advances in the extrusion and drawing of OFs, it is now possible
to reach cores as small as 400 nm [14]. Two classes of MOFs have been developed:
OFs with solid “suspended” cores [14] and air-core photonic band gap OFs [15].
8.3
SURFACE BIOFUNCTIONALIZATION
To achieve a specific detection, a biosensor must include a recognition element, or
bioreceptor, specific to the analyte to be detected. The choice of the bioreceptor
and the way to immobilize it on the transducer surface depends on the envisioned
applications, the transducer type, and the surface material.
8.3.1
Bioreceptors
In addition to the specificity between the analyte and its bioreceptor, there are some
important features to take into account in the selection of the bioreceptor such as
the affinity between the analyte and the bioreceptor, the stability of the complex
formed by the two molecules, and the fact that the formation of this complex must
be detectable by the transducer.
Peptides and antibodies are typical bioreceptors to study protein–protein interac-
tion. In particular, immunosensors are based on the specific and high affinity between
an antigen and its antibody. They can be applied to the continuous monitoring of
critical-care analytes in serious illness of infectious diseases or to any immunological
alteration. Many studies are also focusing on specific clinical biomarkers for the early
detection of a myriad of human diseases such as cancer, neurodegenerative, diabetes,
or cardiovascular disorders. In addition, as specific antibodies can be obtained for low
molecular-weight organic molecules such as toxic pollutants, the immunodetection
has been extended to other areas such as environmental monitoring or food control,
allowing the detection of toxic or explosive species as well as the control of environ-
mental pollutants. The pairs BSA/anti-BSA, HSA/anti-HSA, and IgG/anti-IgG are
model systems widely used for biosensing proofs-of-concept.
Nucleic acid sensors are based on the complementation of base pairs between a
DNA or RNA target strand and its complementary DNA or RNA bioreceptor. These

SURFACE BIOFUNCTIONALIZATION
329
sensors are of considerable interest as they provide sequence-specific information in
a faster, simpler, and cheaper way than traditional hybridization assays.
Cellular components or even whole cells can also be employed as bioreceptors
to detect any type of microorganism (bacteria, virus) or to study extracellular or
intracellular membrane transport proteins.
More recently, synthetic receptors based on engineered molecules have emerged
as an attractive alternative to biological receptors. Aptamers are synthetic DNAs,
RNAs, or Peptide Nucleic Acids (PNAs) which are created with the appropriate
sequences to detect a specific target with high specificity and affinity [16]. Another
class of synthetic biomimetic receptors are the molecular imprinted polymers (MIPs)
prepared by a template process that usually involves the polymerization of the plastic
monomers in the presence of the target analytes [17]. However, until now, biomimetic
receptors have not achieved the same specificity as the natural receptors, and they are
only suitable for certain special applications.
Enzymes can also be used as bioreceptors to catalyze reactions, forming the so-
called catalytic biosensors. These are kinetic devices that monitor the rate of forma-
tion of a product or the disappearance of a reactant related to the biocatalyzed reaction.
These assays, particularly adapted for electrochemical transducers, are barely applied
to label-free optical transducers.
8.3.2
Immobilization Techniques
Once the application (i.e., the bioreceptor) and the transducer (i.e., the surface mate-
rial) are chosen, the most appropriate immobilization technique must be employed.
The goal of the immobilization technique is to create a recognition layer with an
efficient coverage of the transducer surface that is capable of interacting with the
target with high specificity and speed but without modifying the bioreceptor prop-
erties or the transducer sensitivity. It should also minimize nonspecific adsorptions
when analyzing complex samples such as blood or serum without previous process-
ing. In addition, the recognition layer should be stable in time and in some cases
sufficiently robust to be regenerated without severely affecting the binding properties
of the bioreceptor. The choice of the most effective strategy of immobilization is
the key factor that turns a sensing device into a valid and applicable analytical tool
as the bioreceptor layer directly affects the reproducibility, the selectivity, and the
resolution of any sensor device. Several strategies can be employed to immobilize
the bioreceptor on the sensor surface [18–20]:
– Physical adsorption by direct deposition of the bioreceptor using van der Waals
and electrostatic interactions between the biomolecules and the surface. It is a
simple and rapid method that can result in the immobilization of large quantities
of bioreceptors with high packing density. However, it can lead to desorption of
the active bioreceptors under flow conditions and during surface regeneration as
it usually implies high or low pH solutions, salt concentration, organic solvent,
and so on. In addition, problems in reproducibility and in the orientation of the
bioreceptors are common drawbacks of this strategy.

330
OPTICAL WAVEGUIDE BIOSENSORS
– Covalent binding of the bioreceptor to the surface, using a crosslinker previously
immobilized on the surface or following more complex strategies [18]. The
binding is made through one of the functional groups of the bioreceptor. It is
recommended to use a chemical group that will not compromise the functionality
of the molecule. Amino, carboxylic, or thiol groups are the preferred option
to immobilize proteins. For antibodies, the added difficulty is to immobilize
them in an oriented way to let the affinity-binding sites free. The oriented
immobilization of antibodies can be achieved through the carbohydrates groups
of the constant region or by using affinity proteins such as the A or G protein
[19]. Another solution is to immobilize antigens on the transducer surface and
to detect the target antigen through a competitive assay with its antibody. To
immobilize nucleic acid, it is possible to take advantage of the versatility of
the DNA synthesis that allows the incorporation of reactive groups at the end
of the sequence. Covalent binding is one of the most widely used techniques
due to its flexibility and to the wide range of linkers that can be used to attach
any type of bioreceptor to the surface. However, because of the number of
steps required, it can lead to nonspecific binding. To minimize such nonspecific
binding, blocking agents can help blocking surface functional groups that could
bind to other molecules than the target.
– Noncovalent interactions to a previously deposited active layer, either by non-
specific electrostatic interactions or by noncovalent affinity binding (biotin-
avidin systems, His-Tag systems, protein A/G for antibodies, etc.) [18–20].
For instance, the biotin-avidin system is obtained through the formation of a
sandwich-like layer: biotin on the surface/avidin/biotinylated biomolecule.
– Physical entrapment of bioreceptors in a polymer layer. This technique enables
the incorporation of guest molecules within the polymer matrix in a single step.
The most common polymer matrices are polyaniline, polythiophene, polypyr-
role, dextran, and their derivatives. However, this technique is less employed
than the others due to problems associated with the diffusion and mass transport
of the analyte toward the surface.
Whatever the immobilization technique employed, a previous chemical activation
of the sensor surface is mandatory. For a metal oxide surface such as TiO2, the
first step is to create surface hydroxyl groups which can be then attached to the
functional groups of the bioreceptors via linker agents. Silicon-, silicon oxide-, and
silicon nitride-based surfaces can be functionalized using the well-known silane
chemistry [21]. Silicon-based surfaces require a prior activation step to oxidize the
surface and to expose the silanol groups for crosslinking with silane. In the case of
silicon nitride, methods are based on an initial etching step to remove the native
SiO2 layer to be further oxidized to form a new oxide layer [22]. Other strategies
involve the derivatization of the Si–H and Si–N groups of the silicon nitride using
more drastic conditions [21, 23]. The activated surface is subsequently modified
with organosilanes forming a silane layer with exposed reactive groups susceptible to
react with the bioreceptors. Hundreds of different organosilanes with a wide variety of

EVALUATION OF OPTICAL BIOSENSORS
331
structures, length, functionality, and chemical and physical properties are nowadays
commercially available, although the most commonly employed are those with short
alkyl chain (as propyl or butyl), ending in amino, thiol, epoxy, or carboxylic groups.
When studying complex samples that contain high concentrations of nonspecific
molecules, the bioreceptor layer must be prepared in a way that avoids matrix effects
and large background signals. In this case, less conventional reagents like dextran
polymer or poly(ethylene) glycol (PEG) derivatives can be employed to provide a
more hydrophilic, biocompatible, and antifouling surface.
8.3.3
Biofunctionalization Strategies for Multiplexing
Usually the immobilization of the bioreceptors is done ex situ at the sensor chip
level. However, for multiplexing purposes, the sensors included in an array must
be functionalized with different bioreceptors in order to detect a different target
each. To do so, two main approaches can be employed: microfluidic patterning
and microcontact printing. The microfluidic patterning consists of incorporating a
microfluidic network including an independent microchannel on top of each sensor.
Each sensor can be individually functionalized using laminar flow streams [24]. The
microcontact printing is a versatile method for a region-specific functionalization
based on the selective deposition of small volumes of the reagents under static
conditions. Common microdeposition platforms are based on Surface Patterning
Tool (SPT) cantilever [25] or on the noncontact piezoelectric method [26]. Hybrid
methods combining microfluidic patterning and microcontact printing can also be
implemented to take advantage of each method to create highly resistant multiplexed
detection interfaces in a microfluidic network [27].
8.4
EVALUATION OF OPTICAL BIOSENSORS
The following specifications are frequently used to characterize a biosensor device:
– The sensitivity is the magnitude of the transducer response to a change in the
analyte concentration. It depends on the nature of the analyte and on the type of
transducer.
– The selectivity quantifies the capability of the biosensor to exclusively detect
the analyte of interest. It depends on the inherent binding capabilities of the
bioreceptor.
– The dynamic range is the interval of concentrations of the analyte (minimum
and maximum) that can be detected by the biosensor (see Fig. 8.3). A wide
dynamic range is desired to detect a broad interval of the analyte concentration.
– The response time is the time required by the transducer to convert a change at the
transducer surface such as analyte binding into a variation of the output signal.
– The reproducibility is the ability of the biosensor to reproduce output response
when the same analyte concentration is applied in the same conditions.

332
OPTICAL WAVEGUIDE BIOSENSORS
FIGURE 8.3
Standard calibration curve showing the relationship between the analyte con-
centration [analyte] and the sensor response change ΔSr.
Additional characteristics of a biosensor are reliability, versatility, ease of use, and
potential for miniaturization, multiplexing and low-cost fabrication.
Two types of sensing can be differentiated:
– The homogeneous or bulk sensing: the variation of the refractive index occurs
homogeneously in all the volume above the sensor surface. This sensing is
nonselective as it does not enable to differentiate between analytes.
– The surface sensing: a thin layer of bioreceptors is immobilized on the sensor
surface and interact selectively with the corresponding analyte.
The relationship between the sensor response Sr and the measurand m (i.e., the
refractive index of the solution in bulk or the analyte concentration) is given by
Sr = a + bm,
(8.2)
where a is the output signal at zero input signal and b is the slope or sensitivity of
the transducer, also called Ssensor. This relationship assumes that the response of the
transducer is linear, which is usually not the case. However, this approximation is
considered as correct in the dynamic range [28]. In label-free optical detection, the
response of the sensor can be the phase of the wave (𝜑), the resonance wavelength
(𝜆), or the incident angle (𝜃).
To evaluate the bulk sensitivity of a device, the sensor response change ΔSr
induced by a refractive index variation Δn is evaluated for a set of solutions of
different refractive indices successively injected in the sensing area. The results are

EVALUATION OF OPTICAL BIOSENSORS
333
represented in a calibration curve as ΔSr versus Δn. According to Eq. (8.2), the slope
corresponds to the bulk sensitivity:
Ssensor = ΔSr
Δn .
(8.3)
The LOD in terms of refractive index unit (RIU) is given by
ΔnMIN =
ΔSr,MIN
Ssensor
,
(8.4)
where ΔSr,MIN is the resolution of the transducer, or minimum signal detectable.
When the minimum signal detectable depends on the whole intensity pattern (like in
the case of interferometers) it is a function of the baseline noise. For transducers such
as microcavities or grating couplers, the minimum signal detectable is the minimum
shift in wavelength or in angle that can be detected which depends on the resonance
linewidth.
The procedure to evaluate the surface sensitivity of a biosensor is similar to that
of the bulk sensitivity except that the transducer surface needs to be previously
functionalized with the appropriate bioreceptor. Solutions of different concentrations
of the target analyte ([analyte]) are successively injected in the sensing area and the
sensor response change ΔSr is evaluated for each concentration, with the surface
of the biosensor regenerated between each injection. The results are plotted in a
calibration curve as ΔSr versus [analyte] like the one presented in Figure 8.3. The
surface sensitivity is given by
Ssensor =
ΔSr
[analyte],
(8.5)
and the LOD of the biosensor in term of analyte concentration by
[analyte]MIN =
ΔSr,MIN
Ssensor
.
(8.6)
When possible, it is normally expressed in surface mass density (for example as
pg mm−2) which is an evaluation of the real biosensing capabilities of evanescent wave
sensors as they are sensitive to any accumulation of mass on their surface. However,
in many cases, the surface LOD is expressed in terms of analyte concentration as it
is much easier to estimate than surface mass density. However, analyte concentration
cannot be used to directly compare between different sensor sensitivities as it will
depend on the target molecule and its affinity constant. Table 8.1 shows a comparison
of the LOD in RIU and in pg mm−2 for the optical waveguide biosensors included in
this chapter.
As a first step, evaluation of the biosensor capabilities is usually done using
purified or spiked samples in buffer. But in order to assess the real performances of
any device, real samples such as blood, urine, serum, saliva, tears, cerebrospinal or
medullar fluids, or environmental samples have to be tested and validated.

334
OPTICAL WAVEGUIDE BIOSENSORS
TABLE 8.1
Comparison of the limit of detection in bulk and in surface for main
optical waveguide biosensors
RI detection limit
Mass detection limit
Device configuration
(RIU)
(pg mm−2)
IO-based
Interferometers
10−7–10−8
0.02–1
Ring resonators
Gratings
Photonic crystal
Si wire
Slot waveguide
10−5–10−7
10−6
10−4–10−5
10−6
10−6
1–15
0.3–5
2–10
0.3–3
1–15
OF-based
FBG
10−6
13
LPG
10−5
–
IO, integrated optics; OF, optical fiber; FBG, fiber Bragg grating; LPG, long period grating.
8.5
INTEGRATED OPTICAL WAVEGUIDE-BASED BIOSENSORS
8.5.1
Interferometric Biosensors
The working principle of interferometric biosensors relies on the superposition of
two or more light waves which results in the creation of a new wave pattern I(t) ∝
cos(Δ𝜑(t)) called the interference pattern. The incoming light beam usually comes
from the same source and is split into two beams of equal intensity that travel through
different optical paths and are recombined before arriving at a detector. For biosensing
applications, the two arms of the interferometer are usually referred to as the sensing
arm and the reference arm, this last one compensates for refractive index fluctuations
and unspecific adsorption (see Fig. 8.4). The variation of the effective refractive
FIGURE 8.4
Illustration of the working principle of interferometric waveguide biosensors
applied to the Mach–Zehnder interferometer (MZI).

INTEGRATED OPTICAL WAVEGUIDE-BASED BIOSENSORS
335
index induced by binding events in the sensing arm produces a variation of the phase
difference Δ𝜑between the two arms which can be evaluated through the interference
pattern: as depicted in Figure 8.4, the difference between the initial value Δ𝜑1 and
the final value Δ𝜑2 is given by the number of fringes, where a complete fringe
represents a phase difference of 2𝜋[29].
The contrast of the interference pattern (difference between the maximum and
the minimum intensity) depends on the coupling factor of the divisor and on the
propagation losses of the guided mode in the interferometer arms. To obtain the
maximum contrast, it is important to design a divisor or Y-junction with a coupling
factor of 3 dB to equally divide the input beam in each arm of the interferometer.
Moreover, propagation losses in the sensor and reference arms should be identical.
In most of the interferometric configurations the waveguide should be single
mode and the light linearly polarized to avoid multimodal and cross-polarization
interference at the output.
8.5.1.1
Mach–Zehnder Interferometer
In the integrated MZI, the split of the
input-guided beam into two arms and the recombination of these two beams into a
unique output beam are achieved via two integrated Y-junctions (Fig. 8.4). The phase
difference Δ𝜑between the two arms is given by
Δ𝜑(t) = 2𝜋L
𝜆(neff,s(t) −neff,R),
(8.7)
where neff,S and neff,R are the effective refractive indices of the guided modes in the
sensing and reference arms, respectively, L the length of the sensing area, and 𝜆the
wavelength of the light. Typical values of L range from a few millimeters to several
centimeters.
One of the first fabrications and applications of an integrated MZI as biosensor
was reported in 1997 [30]. The MZI was based on Si3N4 rib waveguides on a SiO2/Si
substrate and optimized for operation at 633 nm. The reported LOD in bulk was of
1 × 10−4 RIU and the feasibility of this device as biosensor was demonstrated by an
immunoassay: anti-hCG was adsorbed onto the surface of the sensing area and the
analyte, hCG, was specifically detected at a concentration of 3 × 10−8 M. After this
pioneering demonstration, silicon technology-based MZIs were further optimized
to improve sensitivity. For a device with a 15-mm-long sensing window, a bulk
sensitivity of 1 × 10−7 RIU was later demonstrated as well as the specific detection
of DNA hybridization (Fig. 8.5a). Figure 8.5b shows the corresponding calibration
curve with the lowest concentration detected (1 × 10−11 M) of 58 mer-DNA in buffer
[31]. Glass-based [32] and polymer-based [10,33] MZIs have also been successfully
developed for biosensing purposes although the reported LODs generally remain
worse than for silicon nitride-based MZIs.
However due to the periodic nature of the output signal, the MZI response can
give false-positive reading because of problems of ambiguity, sensitivity fading, and
intensity fluctuations as illustrated in Figure 8.6a [34]. A modulation system can help
overcome these limitations by tuning the phase difference between the two arms.

336
OPTICAL WAVEGUIDE BIOSENSORS
−1
3.5
5
2.5
2
1.5
1
0.5
0
1E – 13
1E – 11
1E – 9
1E – 7
DNA (M)
1E – 5
1E – 3
−2
−3
Signal (a.u.)
Signal (a.u.)
−4
−5
−6
70
(a)
(b)
75
80
85
90
95
Time (min)
1 μM non complementary DNA
100 nM complementary DNA
100
105
110
115
1 μM complementary DNA
Complementary
Non-complementary
FIGURE 8.5
(a) Real-time DNA hybridization signal corresponding to 1 μM and 100 nM
complementary DNA, and 1 μM noncomplementary DNA; (b) calibration curve for the
hybridization of complementary (red line) and noncomplementary (blue line) DNA (58 mers).
Reprinted from Reference 31 under the Creative Commons Attribution License. (For a color
version of this figure, see the color plate section.)
Electro-optic [34], acousto-optic [35], or thermo-optic [36] modulation approaches
can be employed but they rely on complex fabrication processes and important elec-
tronic equipment. To avoid additional processes or equipment, alternative strategies
have been suggested as an all-optical wavelength modulation system [37]. The phase
difference between the two arms is modulated by periodically varying the laser input
wavelength, providing a linear response and an unambiguous direct read-out of the
phase variation produced by a binding event. The required wavelength shift being infe-
rior at 2 nm, it can be achieved by modulating the driving current of low cost commer-
cial laser diodes. The viability of the all-optical modulated MZI for biosensing appli-
cations was demonstrated through the immunoreaction of the pair hGH/anti-hGH
(Fig. 8.6b) [37]. Another approach consists in adjusting the input wavelength before
1.2
0
−10
−20
−30
−40
−50
−60
0
5
10
Time (min)
15
20
Fringe
order 
ambiguity
Sensitivity
fading
Directional
ambiguity
Correct
Quadrature
points
1
0.8
0.6
0.4
0.2
0
−4
4
−2
2
Δ   b
Δ   s(rad)
0
−0.2
(a)
(b)
π
π
φ
φ
φ
Δ  φ
φ
Pout/Pin (a.u.)
π
π
π
π
mAb-hGH 1 μg mL−1
Δ    = 2.94.2    rad
mAb-hGH 5 μg mL−1
Δ   = 8.52.2    rad
FIGURE 8.6
(a) MZI output showing the fringe pattern and the associated intrinsic prob-
lems of interferometric system. Reprinted from Reference 34 with permission from Elsevier.
(b) Real-time output of the wavelength-modulated MZI highlighting the linear and direct phase
read-out. Reprinted from Reference 37 with permission from OSA.

INTEGRATED OPTICAL WAVEGUIDE-BASED BIOSENSORS
337
Analyte
Cylindrical lens
CCD Camera
Antibody
Input
beam
1 2
3
Output beams
4
Integrated waveguide structure
Input
Channel
Y-junction
FIGURE 8.7
Four-channel Young interferometer. Reprinted with permission from Refer-
ence 39, copyright 2013 American Chemical Society.
each measurement in order to start the evaluation with the same phase difference
between the two arms of the MZI [10, 33]. However, this last approach requires
the use of expensive tunable lasers [10] or additional processes and equipment [33].
Working with frequency-resolved MZI instead of the conventional single-wavelength
MZI also permits to circumvent the problems due to the periodic output even if tunable
lasers are again unavoidable [32,38].
8.5.1.2
Young Interferometer
In the Young interferometer, a Y-junction splits the
input waveguide into a sensing arm and a reference arm but, contrary to the MZI,
there is no second Y-junction at the output. As depicted in Figure 8.7, the interference
pattern is produced off-chip by projecting the output light from both arms onto a
screen or CCD camera. The phase difference between the two interfering beams is
given by
Δ𝜑(t) = 2𝜋
𝜆
[
d ⋅x
f
−(neff,s(t) −neff,R) ⋅L
]
,
(8.8)
where d is the distance between the two arms, f the distance between the sensor output
and the camera, and x the position of the interference pattern on the screen. When a
biomolecular interaction takes place in the sensing area, the Δ𝜑variation is deduced
from the shift of the interference pattern on the screen.
The use of an integrated Young interferometer as sensor was first demonstrated by
Brandenburg and Henninger in 1994 [40]. The waveguides were produced by thermal
K+-Na+ exchange in a BK7 substrate and the position of the fringes on the CCD
array was evaluated by Fourier analysis of the intensity distribution. The problem of
ambiguity was solved by using a partially coherent source (superluminescent diode
or laser diode operating below laser threshold) and analyzing the resulting function

338
OPTICAL WAVEGUIDE BIOSENSORS
envelop. They first achieved an LOD of 1 × 10−5 RIU which was further decreased
by two orders of magnitude [41].
A few years later, a four-channel Young interferometer based on Si3N4/SiO2
waveguides was developed and integrated with microfluidics to have independent
measurements in each sensing arm (Fig. 8.7). This system is highly sensitive as
demonstrated by the reported LODs: 0.02 pg mm−2 for surface sensing and 8.5 ×
10−8 RIU in bulk, which is one of the lowest LODs reported until now. An example
of a bioanalytical application of this device is the specific detection of herpes simplex
virus 1 (HSV-1), showing that 8.5 × 102 particles per mL in buffer and 105 particles
per mL in serum can be detected [39].
More recently, a polymer-based Young interferometer has been developed [11].
This device is less sensitive than silicon-based devices (LOD of 1.2 × 10−5 RIU) but
presents advantages in terms of easy fabrication since roll-to-roll techniques can be
foreseen to produce low cost disposable sensors. The biosensing capabilities of the
device were demonstrated through the detection of the C-reactive protein (CRP) with
human CRP-specific antibodies previously adsorbed on the waveguide surface.
Young interferometers have also been developed in free space: planar waveguides
are integrated with diffraction gratings for in- and out-coupling of light but both
the split and the recombination of the sensing and reference beams are produced
off-chip, making its integration into an LOC platform more complicated. However,
this configuration is extremely sensitive with reported LODs of 4 × 10−8 RIU and
0.013 pg mm−2 [42].
8.5.1.3
Other
Interferometric
Configurations
The
birefringent
waveguide
biosensor is based on the interference between two orthogonally polarized beams
(TE and TM) propagating in a straight birefringent waveguide. A birefringent waveg-
uide biosensor with an integrated microfluidic channel has been implemented for the
detection of pathogenic bacteria such as Listeria monocytogenes in food [43]. Speci-
ficity was demonstrated as well as the capabilities of the device to perform detection
in milk. However, this device is not extremely sensitive as the lowest detected con-
centration was of 106 CFU mL–1 in buffer (CFU stands for colony-forming unit).
In the bimodal waveguide (BiMW) interferometer, two guided modes of the same
polarization, the fundamental TE00 and the first order TE10 modes, interfere in
a straight Si3N4 waveguide (Fig. 8.8) [44]. The sensitivity level of the BiMW is
comparable to other integrated interferometers as LODs of 2.5 × 10−7 RIU in bulk
[44] and of 8 pg mL−1 for the label-free detection of hGH [45] have been reported.
Other interferometric-based biosensors such as the Hartman interferometer [46]
or the Dual Polarization Interferometer [47] (DPI) have not been included in this
discussion since they are scarcely used nowadays.
8.5.2
Integrated Optical Microcavity-Based Biosensors
Microcavities for highly sensitive biosensing were theoretically described for the
first time in 2001 [48]. Since then, it has been the subject of significant research and
technological development, as can be deduced from the rising number of articles and
high impact reviews published on the subject during the last years [49–52]. Even if

INTEGRATED OPTICAL WAVEGUIDE-BASED BIOSENSORS
339
FIGURE 8.8
Working principle of the BiMW interferometer, highlighting the TE00 and
TE10 mode profiles as well as the output detection scheme via a two-sectional photodiode.
they are not nominally as sensitive as interferometric biosensors (difference about one
order of magnitude between LODs [53]), they present undeniable advantages such as
a small foot print and the possibility to produce dense arrays for multiplexed analysis,
sensitivities independent of the length of the sensing area, and a strong theoretical and
technological background emanated from the telecommunication industry. Integrated
optical microcavity biosensors can be implemented using various geometries such as
microdisks, microrings, microtoroids, or microspheres [50] and in different materials
(silicon, silicon nitride, polymer). The following explanation will be focused on
integrated microring biosensors but the same operation principle can be applied to
other configurations.
An integrated microring resonator consists of an input bus waveguide (cross-
sectional area below a square micrometer) and of a ring (radius r between ten and
hundreds of micrometers) separated by a gap of about 1 μm (Fig. 8.9). An output
bus waveguide can also be added on the other side of the ring. Light is evanescently
coupled from the input waveguide to the microring and propagates into the ring in the
form of whispering gallery modes (WGMs), generating constructive interferences
for wavelengths factors of the ring circumference. The resonance wavelength, 𝜆, is
given by
𝜆= 2𝜋rneff
m ,
(8.9)
FIGURE 8.9
Working principle of the ring resonator biosensor. (For a color version of this
figure, see the color plate section.)

340
OPTICAL WAVEGUIDE BIOSENSORS
where m is an integer number. As depicted in Figure 8.9, the resonance wavelength is
identified as a dip in the transmission spectra when excited with a tunable laser. When
a biointeraction takes place on the ring surface, it induces a change in the effective
refractive index of the WGM that results in a shift of the resonance wavelength.
Historically, microring resonators have been fabricated for telecommunication wave-
lengths, mainly 1550 nm. However, novel waveguide technologies are investigated
for ring resonators operating at lower wavelengths [54].
The resonant nature of the WGM creates an extremely long effective interaction
length Leff which is related to the resonator quality factor (Q-factor) of the
microring by
Leff = Q 𝜆
2𝜋n,
(8.10)
where n is the resonator refractive index. The Q-factor is associated with the optical
losses of the cavity and describes the photon lifetime within the cavity or, in other
words, the number of revolutions of light supported by the resonator. Depending
on the configuration and on the materials, usual Q-factors range from 104 to 108.
For instance, for a ring with a refractive index of 1.45 operating at 1550 nm and a
Q-factor of 106, the effective interaction length will be as long as 17 cm. Therefore,
despite their small physical length, the ring resonators can be highly sensitive using
a small surface area and low sample volumes. The Q-factor can also be related to the
resonance wavelength 𝜆and the resonance linewidth 𝛿𝜆by Q = 𝜆∕𝛿𝜆. The LOD of
a microcavity-based biosensor is therefore related to its Q-factor since the minimum
detectable shift in 𝜆depends on 𝛿𝜆.
SiO2/SixNy- [54], glass- [55], and polymer- [12, 56] based microring resonators
have been reported and successfully applied to DNA, proteins, bacteria, or virus
detection. However, much of the research focuses on SOI-based microresonators.
For instance, SOI microring resonators (5 μm radius, Q-factor of 2 × 104) featuring
a bulk sensitivity of 70 nm per RIU and a LOD of 10−5 RIU have been fabricated in a
multiplexed configuration for antibody detection in complex samples [57]. SOI-based
ring resonators have also given rise to the commercial Maverick platform from the
company Genalyte, Inc. (USA), where the microrings are integrated in a multiplexed
configuration (up to 128 rings) and individually addressed by a bus waveguide.
This platform has been employed by several research groups and applied to a wide
range of applications, like for instance the detection of Bean Pod Mottle Virus
(BPMV) in buffer (LOD of 10 ng mL−1) and in complex leaf extracts (Fig. 8.10) [58].
The detection and identification of transfer-messenger RNA (tmRNA) has also been
demonstrated with an LOD down to 53 fmol of Streptococcus pneumoniae tmRNA,
equivalent to approximately 3.16 × 107 CFU of the corresponding bacteria [59].
Special effort is being put into the development of high Q-factor microring
resonators with polymer materials. Using a set of silicate-based inorganic–organic
hybrid polymers, microring resonators have been fabricated with a difference of
refractive indices between core and cladding of 7 × 10−2 RIU [12]. As a result,
Q-factors of 5 × 104 were achieved, leading to a bulk sensitivity of 50 nm per RIU and
a corresponding LOD of 10−5 RIU, values comparable to the ones reported for SOI

INTEGRATED OPTICAL WAVEGUIDE-BASED BIOSENSORS
341
FIGURE 8.10
(a) Microring sensor responses to increasing concentrations of BPMV and
(b) corresponding calibration curve. The blue “X” indicates the response from the 1:200
dilution of the extract from a BPMV infected sample. (c) Microring sensor response to diluted
extracts from soybean leaves. Red: 1:200 dilution of the leaf extract from a BPMV infected
sample. Blue: 1:200 dilution of the leaf extract from a healthy sample. Arrow indicates “on”
and * indicates “off” (entrance and exit of the sample solution, respectively). Adapted from
Reference 58 with permission from Elsevier. (For a color version of this figure, see the color
plate section.)
microrings. The biosensing capabilities of this polymer-based microring resonator
has been demonstrated by the immobilization of human-IgG, with protein A as
receptor [12].
Instead of being laterally coupled as previously described, the resonators can also
be coupled vertically to a bus waveguide buried below the resonator surface [55,56].
This configuration offers several advantages as compared to the lateral coupling. First,
the vertical gap dimensions are easier to control than for the lateral gap, allowing the
replacement of high-resolution fabrication techniques such as e-beam lithography
by optical lithography. The vertical coupling also relaxes the alignment between
the resonator and the bus waveguide. And, which is of importance for biosensing,
the influence of the variation of the refractive index on the resonance linewidth is
drastically reduced since the coupling region is totally isolated from the sensing area.
WGM can also be excited along the perimeter of a glass capillary, leading to
a biosensor with an already integrated microfluidic channel in which the binding
occurs at the interior of the capillary. These devices, called optofluidic ring res-
onators (OFRRs), are easily fabricated by pulling the capillary while softening the
glass with a CO2 laser. To excite the WGMs, both optical fibers and waveguides can
be used. The second option is usually preferred as it leads to sensing platforms that
incorporate the microfluidics and the photonic sensor in a single chip with multiplexed
capabilities while using small sample volumes. OFRR sensitivity is comparable to
other IO biosensors showing protein detection capabilities in the pg mm−2 range
(Q-factors in the order of 105/106) [50]. OFRRs have been used in various sensing
applications including the detection of DNA, viruses, proteins, and cancer biomark-
ers. For instance, it has been recently employed for the detection of CD4+ and CD8+
T-Lymphocyte which are biological indicators of the HIV progression. CD4 and CD8
antibodies were immobilized on the inner surface of the OFRR and medically signif-
icant concentrations of CD4+ and CD8+ (160 to 1000 cells μL−1) were successfully
detected (Fig. 8.11) [60].

342
OPTICAL WAVEGUIDE BIOSENSORS
200 CD4 + cells per μL
Negative control
Photodetector
Tunable laser
Sample
−5
−2
0
2
4
6
8
10
12
WGM shift (pm)
0
(b)
(a)
5
10
15
Time (min)
20
25
30
35
40
FIGURE 8.11
(a) Scheme of the OFRR biosensor. (b) Sensor response to the injection of
200 CD4+ cells μL−1. The negative control is diluted serum and red blood cells. Adapted from
Reference 60 under the Creative Commons Attribution License.
The resolution of the configurations previously described is mostly restricted by
their Q-factors, limiting their mass detection limit to the range of pg mm−2. However,
microtoroids and microspheres afford much higher Q-factors, in the order of 108/109,
giving rise to the first all-optical label-free single-molecule technique [50,61]. More
details about the enhancement mechanisms responsible for single-molecule detection
can be found elsewhere [61]. Microtoroids are doughnut-shaped resonators fabricated
from layers of silicon dioxide grown on silicon substrates. The silicon dioxide is first
etched to form microdisks which are then undercut to form silicon dioxide disks on
silicon post. The toroids are finally obtained by heating the silicon dioxide with a CO2
laser. This step produces an almost atomically smooth surface, resulting in extremely
high Q-factors. Microspheres are easier to fabricate as they are generally obtained
from a standard single-mode optical fiber. However, the major disadvantage of these
structures lies in the difficulty of integration with the fluidics in the case of the micro-
spheres and with input waveguides or optical fibers in the case of the microtoroids.
8.5.3
Grating-Based Biosensors
A diffraction grating is a periodic structure usually employed to couple light into or
out of an optical waveguide. The resonant coupling between the guided mode and a
diffraction order of the grating is governed by the phase-matching condition as
neff = sin 𝜃+ m 𝜆
Λ,
(8.11)
where 𝜃is the incident angle of light, Λ the grating period, and m the diffraction
order. As can be deduced from this equation, a variation of the effective refractive
index will affect the resonance angle or the resonance wavelength. Therefore, grating-
based biosensors can be achieved with two optical configurations: the angular and
the wavelength interrogation (Fig. 8.12). In the angular approach, a scan of the
resonance angle using a laser source of fixed wavelength is employed (Fig. 8.12a),

INTEGRATED OPTICAL WAVEGUIDE-BASED BIOSENSORS
343
FIGURE 8.12
Schemes of grating-based biosensor configurations: (a) angular interrogation,
(b) wavelength interrogation with grating as coupler, and (c) wavelength interrogation with
grating as reflector. (For a color version of this figure, see the color plate section.)
while in the wavelength interrogation configuration it requires a tunable laser to scan
the resonance wavelength at a fixed angle of incidence (Fig. 8.12b).
TiO2 and Ta2O5 core layers on glass substrate are commonly used as waveguide
structure in grating-based biosensors. Depending on the configuration, the grating
can be defined at the interface core/substrate, at the interface core/cladding or at both
interfaces. To fulfill Eq. (8.11), submicronic grating periods are requested, making
conventional UV lithography generally unsuitable for the fabrication. Direct grating
writing methods such as electron-beam lithography or focused ion beam milling
can be employed although alternative methods like optical holography or embossing
methods (as nanoimprint lithography) are usually preferred because they are simpler
and have lower costs and the possibility of mass production.
The first grating-based biosensor was developed at the end of the 1980s by
Lukosz’s group using angular interrogation: light was coupled into the waveguide via
an input grating and then propagated until the output of the waveguide where it was
detected by a standard photodetector [62]. The chip was placed on a rotation stage
in order to perform angular scans and to determine the shift of the resonance angle
induced by the bioreactions occurring in the sensing area. An LOD around 10−6 RIU
was demonstrated as well as some initial biosensing evaluations using the label-free
immunoreaction of the pair IgG/anti-IgG. Despite the important drawback of having
to achieve a precise mechanical movement of the rotation stage, this configuration is
employed in the Optical Waveguide Lightmode Spectroscopy (OWLS) configuration.
OWLS is commercialized by MicroVacuum Ltd. (Hungary) and has been used in a
wide range of applications. Among them, the detection of Legionella pneumophila
contamination in water samples with an LOD of 1.3 × 104 CFU mL−1 [63] or, more
recently, the detection of vitellogenin, a biomarker for assessing exposure to environ-
mental endocrine disruptors, with a dynamic range between 3 and 100 ng mL−1 [64].
In the wavelength-interrogated approach, an input grating coupler is used to couple
the light into the waveguide at fixed angle and a detector is placed at the waveguide
output to monitor the intensity of light as a function of the input laser wavelength.
Specific molecular binding on the grating surface is detected as a shift in the resonance
wavelength (Eq. 8.11). A sensitivity of 142 nm per RIU and a corresponding LOD of
1 × 10−5 RIU were reported while the surface sensing capabilities were demonstrated
by the specific detection of IgG at 200 ng mL−1 (approximate surface mass density
of 20 pg mm−2) [65]. The Wavelength Interrogation Optical Sensor (WIOS) is based

344
OPTICAL WAVEGUIDE BIOSENSORS
45
Bovin serum albumin
Reference
Signal
PBS buffer
PBS buffer
Neutravidin
About
20pg mm−2
Biotin-5H-FITC
Biotion-5H-FITC
Perk pos. (a.u.)
248
249
250
251
252
253
254
255
256
257
258
259
50
Time (min)
55
60
FIGURE 8.13
Reaction of biotin-5H-FITC (concentration of 1 μg mL−1) on adsorbed
neutravidin (signal) and BSA (reference). Adapted from Reference 66 with permission from
Elsevier.
on the same working principle except that a second grating was added to out-couple
the light [66]. The laser source is a compact and low-cost VCSEL emitting at 763 nm.
The limited tuning range (2 nm) of the laser source results in a small linear detec-
tion range but it is overcome by adjusting the angle of incidence with a motorized
mirror. A compact device including four channels and the laser source, the mirror,
the flow cell, and other components was developed and tested for biosensing of low
molecular weight analytes (label-free detection of molecules as small as 200 Da),
such as biotin, and large biomolecules, such as antibodies, showing mass detection
limits of 0.3 pg mm−2 (Fig. 8.13). The WIOS technology has been used for the
detection of sulfonamides antibiotics in milk, achieving an LOD of 0.5 ng mL−1 but
including an additional amplification step with a secondary antibody [67]. In the field
of genomics, it has been employed to study the kinetics of covalent immobilization
of double-stranded oligonucleotides to evaluate the optimal hybridization efficiency
after sequential denaturation [68]. A similar technology has been developed by Corn-
ing, Inc. (USA). Their commercial device, called EpicTM System, has a detection
limit around 5 pg mm−2 and a broad dynamic range.
Grating-based biosensors have also been developed using the grating as a reflector
instead of a coupler (Fig. 8.12c). In this case, light from a tunable laser is coupled into
the waveguide by end-fire and a resonance wavelength is reflected back after inter-
action with the grating. A bioreaction on the grating surface results in a wavelength
shift in the spectrum. An experimental sensitivity of 140 nm per RIU, corresponding
to an LOD of 6 × 10−6 RIU, has been demonstrated using Si/SiO2/Si3N4 waveg-
uides [69]. Direct and label-free detection of the PepN enzyme was achieved with
an experimental resolution of about 4 pm adlayer growth which is equivalent to the

INTEGRATED OPTICAL WAVEGUIDE-BASED BIOSENSORS
345
capture of 3 × 109 mol cm−2. A similar device, the SpectroSens, is commercialized
by Stratophase Ltd. (UK) since 2008. The gratings and waveguides are simultane-
ously fabricated in a one-step UV writing method into three-layer silica-on-silicon
substrates. The SpectroSens surface mass sensitivity is estimated around 5 pg mm−2,
which is high when compared to the capabilities of interferometric or microring
biosensors. However, this low sensitivity is compensated by the multiplexing capac-
ity of the platform. Each chip contains eight individual sensing areas that can be
functionalized with different capture probes and two chips can be packaged into
a single disposable cartridge, for the simultaneous data acquisition of 16 channels.
Multiplexed, label-free detection of eight different biological agents, including bacte-
rial spores, vegetative cells, viruses, and proteinaceous toxins has been demonstrated
in real time [70].
8.5.4
Photonic Crystal-Based Biosensors
The concept of PC, introduced in the 1980s, is based on a spatial modulation of the
refractive index with a period comparable to the working wavelength. PCs exist in
one, two, or three dimensions according to the number of directions of the index
variation periodicity. The periodicity generates a range of wavelengths disallowed
to propagate in the PC waveguide called the photonic bandgap. By introducing a
defect, that is, a violation of the periodicity, a defect mode will appear in the photonic
bandgap which will be seen as a relatively sharp peak in the transmission or reflection
spectrum of the PC waveguide. The main PC property exploited for sensing is the
dependency of the width and position of the defect peak on the periodicity and on the
refractive index of the surrounding materials. In addition, light can be localized in a
very small volume, leading to strong light–analyte interactions.
A significant contribution in this area was made by Fauchet’s Group using SOI-
based 2D PC nanocavity biosensors. Biotin–streptavidin interaction and covalent
immobilization of BSA were first evaluated and a minimum mass coverage of 2.5 fg
was detected [71]. More recently, they developed an array of three nanocavities
coupled to a PC waveguide for multiplexed purpose (Fig. 8.14a) [72]. As each cavity
has a different set of lattice constant, air hole radius, and defect diameter, they
are all characterized by their own resonance wavelength, allowing the simultaneous
monitoring of the three wavelength shifts (Fig. 8.14b). However, as the cavities have
low Q-factors (around 400), the sensitivity of the device is very low with an LOD
in bulk of about 10−2 RIU and an LOD of 67 nM for the detection of IgG (surface
sensitivity of 1 ng mm−2) [72].
The nanoscale optofluidic sensor array (NOSA) is based on 1D PC microcavities
fabricated by electron-beam lithography on a SOI wafer [73]. The PC resonators are
evanescently coupled by an adjacent single-mode waveguide. With this configura-
tion, an LOD in bulk of 7 × 10−5 RIU was demonstrated. Resonators with differ-
ent cavity spacing and therefore different resonance wavelengths were patterned in
array for immunochemical multiplexed detection [74]. The multiplexing capabilities
were successfully demonstrated with the individual and concurrent detection of three
cancer biomarkers, the interleukins 4, 6, and 8, using a sandwich assay with a dynamic
range from 1 to 100 μg mL−1 [74].

346
OPTICAL WAVEGUIDE BIOSENSORS
2.67 K X
SUDESHNA
(a)
10.00 kV
Pixel Size = 41.8 nm
20 Jan 2010
Photo No. = 1927
SE2
6.2 mm
2 μm
1480
(b)
Transmission (a.u.)
1500
1520
Wavelength (nm)
1540
After lgG
Before lgG
1560
FIGURE 8.14
(a) SEM image of multiple nanocavities coupled to PC waveguides (a = 372,
380, and 388 nm, defect radius = 73, 75, and 77 nm) in series. (b) Experimental transmission
spectra of the structure before and after target (IgG) binding. Reprinted from Reference 72
with permission from Elsevier. (For a color version of this figure, see the color plate section.)
More recently, a multiplexed SOI-based PC biosensor for the detection of a
biomarker from lung cancer cell lysates has been described [75]. Four PC micro-
cavities with high Q-factor (∼104) are arrayed on the four arms of a multimode
interference power splitter for simultaneous detection. A multiplexed specific detec-
tion of a relevant cancer-associated protein with a sensitivity down to 2 cells μL−1
was demonstrated. This result is particularly interesting, as the detection was done
in complex mixtures containing more than 20,000 to 50,000 proteins derived from a
whole cell lysate of a lung cancer cell line.
8.5.5
Recent Trends in Label-Free Integrated Optics-Based
Waveguide Biosensors
During the last years different types of optical waveguide transducers have been
presented in the literature looking for biosensors with enhanced sensitivities and/or

INTEGRATED OPTICAL WAVEGUIDE-BASED BIOSENSORS
347
(a)
Buried oxide
Electrical field
E
Si
Aqueous solution
0.26 μm
50 μm
(b)
FIGURE 8.15
(a) Cross-section of a 0.26 μm × 0.45 μm SOI photonic wire waveguide and
the corresponding electric field distribution for TM polarization. (b) Top view optical image
of an SOI wire-based ring resonator. Adapted from Reference 76 with permission from OSA.
(For a color version of this figure, see the color plate section.)
capabilities toward miniaturization. These improvements are usually obtained by an
increase of the light–analyte interaction via a strong confinement of the light (silicon
wire- and slot waveguide-based biosensors) or by an enhancement of the sensitivity
due to larger surface area (porous silicon-based biosensors and BICELLs sensors).
Silicon photonic wires are sub-wavelength waveguides fabricated by e-beam
lithography and reactive ion etching on SOI wafers. The high index contrast between
the silicon core (n = 3.5) and the silica cladding (n = 1.5) together with their sub-
wavelength size yields a strong field confinement as illustrated in Figure 8.15a [76].
By using the silicon wires in conventional sensing configurations such as MZIs or
resonators (Fig. 8.15b), a high sensitivity can be achieved. A biosensor array of five
Si wire-based ring resonators, including one as reference, has been fabricated with
ring radius between 20 and 28 μm. A bulk sensitivity of 135 nm per RIU (LOD < 2 ×
10−6 RIU) and the simultaneous monitoring of multiplexed molecular binding with
an LOD of 0.3 pg mm−2 have been achieved [77]. A dense array of six spiral MZI
integrated with an SU-8 fluidic channel has also been developed [78]. Multiplexed
and specific binding between complementary and mismatched IgG/anti-IgG pairs
were demonstrated with an LOD around 0.3 pg mm−2.
Slot waveguides consist of two slabs of high refractive index separated by a
nanometer-scale low refractive index slot region where light is strongly confined
(Figs. 8.16a and 8.16b) [79]. Strong light–analyte interaction can be achieved in
this region, leading to enhanced sensitivity when compared to conventional waveg-
uides. So far, slot waveguides have been mainly used in resonator-based biosensors
(Fig. 8.16c). For instance, an Si3N4–SiO2 slot-waveguide-based ring resonators have
been fabricated with a radius of 70 μm and slots of 200 nm for both the waveguide
and the resonator [80]. A bulk sensitivity of 212 nm per RIU was reported, which is
significantly better than conventional resonators. However, the biosensing evaluation
done by covalently immobilizing the antibody anti-BSA and detecting their specific
target BSA showed a moderate LOD of 16 pg mm−2. More recently, an array of
eight slot-waveguide ring resonators integrated with a PDMS microfluidic network
to individually address each resonator has been reported [81]. LODs of 5 × 10−6 RIU

348
OPTICAL WAVEGUIDE BIOSENSORS
(a)
(b)
(c)
nc
ns
nH
nH
Wslot
Wslot
200 nm
600 nm
y
Ex mode profile 
Sensing
window
Light
in
Light
out
Ring
Bus
SiO2 top cladding
Si3N4
Si3N4
Si3N4
Si3N4
z
x
−0.4
−0.4
−0.3−0.2
−0.2
−0.1 0.0
SiO2
Si
Si
0.0
X (μm)
70 μm
Y (μm)
0.1 0.2
0.2
0.3 0.40.0144905
1.0
0.4
FIGURE 8.16
(a) Scheme of a slot waveguide. (b) Calculated Ex profile in an Si/SiO2 slot
at 1.55 μm highlighting the strong confinement of light in the slot region. (c) Left: top view
photograph of a 70-μm radius Si3N4 slot waveguide microring resonator. Right: SEM picture
of the coupling region. Adapted from Reference 79 under the Creative Commons Attribution
License. (For a color version of this figure, see the color plate section.)
in bulk and of 0.9 pg mm−2 in surface were demonstrated. The enhancement of the
surface mass detection limit when compared to the structure previously described is
due to the presence of two reference channels for control and drift compensation, and
to the use of a laser with smaller wavelength steps.
Porous silicon (pSi) is a crystalline material that consists of nanometer to micron-
sized columnar pores in a silicon matrix (Fig. 8.17a). Due to its large surface area
(up to 800 m2 g−1), its fast preparation and its diverse and tunable optical properties,
pSi is an appealing material for sensing applications [82–84]. Porous Si is generally
produced by electrochemical etching technique in the presence of hydrofluoric acid
(HF) [84]. Pore size, porosity, and pore depth are adjustable through judicious choice
of silicon wafer, dopant type, dopant density, current density, HF:surfactant ratio, and
so on. Porous Si has been extensively exploited as a material for optical biosensors,
mainly in fluorescent and cavity-based biosensors, including single layer interfer-
ometers and resonant microcavities. These configurations have been employed for
detection of glucose, DNA, bacteria, viruses, proteins, and other biomedical ana-
lytes for treatment and diagnostic [83,84]. pSi waveguide-based biosensors can also

OPTICAL FIBER-BASED BIOSENSORS
349
37.50
(b)
(a)
20
30
40
50
60
37.55 37.60 37.65
Resonance angle (°)
Reflectance (%)
37.70 37.75 37.80
GA+Probe+EA
Complementary
FIGURE 8.17
(a) SEM image of pSi with distribution of pore sizes between approximately
30 and 100 nm. Reprinted from Reference 82 with permission from Elsevier. (b) pSi waveguide
resonances corresponding to the DNA probe immobilization (GA + probe + EA) and to the
binding with its complementary DNA. Adapted from Reference 85 with permission from
Elsevier.
be implemented using pSi waveguide based on two pSi layers: a low porosity (high
index) layer, and a high porosity (low index) layer [85]. As in a standard SPR, a prism
is used to evanescently couple a laser beam into the waveguide at a specific angle and a
detector is placed at the output face of the prism. Since the coupling angle depends on
the refractive index, the angle shifts when biomolecules are infiltrated into the pores
of the waveguide (Fig. 8.17b). With this device, the selective detection of DNA has
been demonstrated, with a theoretical LOD of 5 × 10−8 M, equivalent to 5 pg mm−2
[85]. However, pSi waveguide-based biosensors have not been extensively studied,
certainly because they do not take as much advantage of the pSi large sensing area as
the cavities do. Another problem related to pSi waveguides is the lack of uniformity
and reproducibility of the porous layer due to the multiple etching parameters.
More recently, a new IO-based biosensor called the Biophotonic Sensing Cells
(BICELLs) has been introduced [86]. It consists of a periodic network of nanopillars
(in Si, SiO2, or polymer) which are interrogated by three independent reflectivity mea-
surements (spectrometry, reflectometry, and ellispometry). Even it is not a waveguide-
baseddeviceit is worthytomentionit, inparticular for thehighthroughput capabilities
offered by the micrometer and sub-micrometer spot size of the read-out interrogation.
In addition, the large surface area provided by the nanopillar results in an increased
sensitivity. BICELLs biosensing capabilities were demonstrating by detecting anti-
gestrinone antibodies in unpurified rabbit serum, with an LOD of 64 pg ml−1 [87].
8.6
OPTICAL FIBER-BASED BIOSENSORS
Due to their sensitivity to strain, temperature, pressure, vibration, curvature, and
refractive index of the surrounding medium, OFs can be used in many healthcare
applications such as thermal and pressure mapping, low temperature monitoring, or

350
OPTICAL WAVEGUIDE BIOSENSORS
immunosensing [88]. In the following, we will focus on biosensing applications where
an analyte react with its specific bioreceptor previously immobilized on the fiber
surface. However, most of the OF-based biosensors use fluorescent labels [13, 14].
Indeed, OF biosensors are particularly suited for fluorescence detection since the
emitted light can easily be collected by the OF and transported to the analyzer. The
use of fluorescence tags is also a way to make up for their low sensibility. Actually,
the main advantage of OF biosensors when compared to IO biosensors is that their
small and flexible shape enable them to be placed in small vessels and in tissues for
minimally invasive in situ sensing.
8.6.1
Fiber Bragg Grating-Based Biosensors
A fiber Bragg grating (FBG) is a photosensitive fiber in which a Bragg grating with
periodicity of hundreds of nanometers has been written through UV illumination.
As light propagates inside the core, the grating selectively reflects one wavelength,
given by
𝜆= 2neffΛ.
(8.12)
Changes in the vicinity of the grating will result in a shift of the resonance wave-
length. As illustrated in Figure 8.18, FBG can be mainly implemented into two
configurations: in reflection, detecting only the reflected wavelength or in transmis-
sion, detecting the whole spectra with a minimum for the resonance wavelength.
FBGs are particularly suitable for multiplexed detection as multiple gratings with
different periods can be written on a single fiber. Each grating will be characterized
by its proper resonance wavelength and the shifts of the resonance wavelengths of all
the gratings can easily be processed with one analyzer.
The sensitivity of conventional FBG is limited around 200 nm per RIU by the low
refractive index contrast between core and cladding [89] but several approaches have
been developed to circumvent this limitation. One method consists in etching the
fiber core to enhance its evanescent field: a sensitivity of 29 nm per RIU was reported
for a fiber with a core diameter of 5 μm while this value drastically increases up to
Transmitted light
Reflected light
Incident light
Wavelength
Wavelength
Λ
Transmision
Reflection
Intensity
Grating
Wavelength
FIGURE 8.18
Principle of the FBG biosensor.

OPTICAL FIBER-BASED BIOSENSORS
351
1394 nm per RIU when its core diameter was reduced to 3.4 μm (corresponding LOD
of 7.2 × 10−6 RIU) [90]. DNA probes of 20 bases were immobilized on the surface
of the fiber grating and the hybridization of 0.7 μg mL−1 of the complimentary target
DNA was successfully detected [90]. Light–analyte interaction was also increased by
inscribing FBGs into a photosensible three-hole MOF [91]. By injecting the sample
solution through the holes, where the light intensity is higher, an LOD in bulk of
6 × 10−6 RIU was achieved. However, biosensing capabilities of this configuration
were not demonstrated.
Another strategy to enhance the sensitivity of FBG consists in writing tilted
gratings. In a tilted FBG, the grating plane forms an angle with the perpendicular of
the fiber axis which results in an enhanced coupling between the core mode and the
cladding modes. BSA was immobilized on the surface of the tilted FBG for anti-BSA
detection [92]. The calculated LOD in term of surface density concentration and anti-
BSA concentration were of 13 pg mm−2 and 86 ng mL−1, respectively. More recently,
a double-tilted FBG was employed for a modified Fabry–Perot core-cladding closed-
loop cavity [93]. This biosensor was functionalized with PNA probes and a specific
DNA detection at a 1 × 10–8 M concentration was demonstrated.
8.6.2
Long-Period Grating-Based Biosensors
Long-period grating (LPG) biosensors have larger grating periods than FBG, typically
between 100 μm and 1 mm, resulting in the coupling of the core modes into the
cladding modes (Fig. 8.19a). The resonance condition is expressed as
𝜆= (neff,core −neff,cladding)Λ,
(8.13)
where neff,core and neff,cladding are the effective refractive indices of the core modes and
cladding modes, respectively. As the coupling to the cladding modes depends on the
difference between the two effective indices, LPG biosensors are more sensitive than
FBG biosensors, typically between 102 and 103 times more sensitive. And, due to the
large grating period, they are also easier to fabricate than FBGs. In addition, there
is no need for etching the cladding to define the sensing area because the cladding
modes are directly sensitive to changes in their surroundings. The first demonstration
of LPG as a biosensor was reported in 2000 by DeLisa et al. [94]. A specific detection
of IgG over a dynamic range of 2–100 μg mL−1 in buffer was carried out with anti-
IgG immobilized on the LPG surface. Antigen detection was also confirmed in a
protein mixture and in crude cell lysate from Escherichia coli (E. coli).
To enhance sensitivity, LPG with shorter grating periods (around 160 μm) have
been developed in order to be in the vicinity of the dispersion turning points, resulting
in conjugate dual-peak cladding modes that are extremely sensitive to external per-
turbations [97]. With these dual-peak LPG biosensors, a hybridization of 1 × 10−6 M
DNA was demonstrated. To further enhance the sensitivity, dual-peak LPG biosensors
have been implemented on an etched fiber [98]. Following these works, dual-peak
LPGs with a bulk sensitivity of 570 nm per RIU have been applied to the label-free and
specific detection of E. coli K12 cells using physically adsorbed bacteriophages as

λ
Δ  2
λ
Δ  1
Long period grating
(a)
(b)
(c)
700
PBS
PBS
PLL
PLL
DNA
DNA
−80
−75
−76
−74
−72
−70
−68
−66
−64
−62
−60
−58
−70
−65
−60
−55
−50
800
Wavelength   (nm)
λ
dB
dB
Wavelength   (nm)
λ
900
830
840
850
860
1000
Core mode
Broadband
source
Cladding
mode
Cladding
mode
Optical spectrum
analyzer
Re mode
FIGURE 8.19
(a) Scheme of the LPG biosensor. Reprinted from Reference 95 with permission from Elsevier. (b) Experimental
transmission spectra for PBS (phosphate buffered saline solution), PLL (poly-l-lysine), and double-stranded DNA. (c) Close up of
dotted rectangle in (b) highlighting the wavelength shifts. Adapted from Reference 96 with permission from OSA.
352

OPTICAL FIBER-BASED BIOSENSORS
353
bioreceptors [99]. Optimizing the device design and the configuration, an ultra-high
sensitivity of 2500 nm per RIU has been recently demonstrated [95].
LPG biosensors can also be implemented into PC fibers. In the first reported
work on PC-based LPG biosensor, a spectral shift of 1.4 nm per nanometer of DNA
monolayer deposited was demonstrated (Figs. 8.19b and 8.19c) although the LOD
in bulk was only of 10–4 RIU [96]. An improved sensitivity in bulk of 1500 nm per
RIU, corresponding to an LOD of 2 × 10−5 RIU, was later obtained [100]. However,
no surface sensing experiments were reported.
8.6.3
Optical Fiber-Based Interferometric Biosensors
In OF-based interferometry, the core mode, which acts as the reference beam, inter-
feres with the cladding modes that are sensitive to the external medium. It generally
implies the use of tunable lasers emitting in the near IR. As for the previous trans-
duction schemes, sensitivity is significantly enhanced by using tapered fibers.
A single-mode/multimode/single-mode PC fiber interferometer has been recently
developed, using a fusion arc splicing technique to splice a 33-mm-long PC fiber
between to single-mode fibers [101]. The microbubbles created in the splice region
act as a modal splitter to excite the cladding modes. This device has a bulk sensitivity
of 320 nm per RIU for an RI range between 1.33 and 1.34 and its biosensing capa-
bilities were demonstrated by the binding between streptavidin and biotin previously
immobilized on the PC fiber surface.
Interferences between core mode and cladding modes were also used in OF-based
MZI and Michelson interferometers (MIs) [102]. As illustrated in Figures 8.20a and
8.20b, mode splitting is achieved by incorporating waist-enlarged fiber taper in a
single-mode fiber. Immunosensing tests were done with anti-IgG as target and IgG
Enlarged bitaper
(a)
(b)
(c)
(d)
Enlarged bitaper
Enlarged bitaper
Cladding mode
Core mode
Cladding mode
Cladding mode
Silver
reflector
Core mode
Cladding mode
L
L/2
SMF
SMF
Input
Output
SMF
1.4
0.3
0.25
0.2
0.15
0.1
0.05
0
−0.05
−0.1
−0.15
1.2
1
0.8
0.6
0.4
0.2
0
0
500
1000
1500
Concentration (nM)
Experimental data
Absolute maximum wavelength shift (nm)
Absolute maximum wavelength shift (nm)
R2 = 0.9676
max = 1.478 nm
Langmuir isotherm
2000
2500
0
500
1000
1500
Concentration (nM)
2000
2500
Input
Output
Δλ
R2 = 0.985
max = 0.3109 nm
Δλ
Experimental data
Langmuir isotherm
FIGURE 8.20
Schemes of the waist-enlarged bitaper-based immunosensors: (a) Mach–
Zehnder configuration and (b) Michelson configuration. Change in wavelength as a function
of anti-IgG concentration for: (c) MZI sensor and (d) MI sensor. Adapted from Reference 102
with permission from Elsevier.

354
OPTICAL WAVEGUIDE BIOSENSORS
immobilized on the fiber surface. Figures 8.20c and 8.20d show the calibration curve
for both MZI and MI devices. The MZI biosensor is clearly the most sensitive device,
with an LOD of 1.8 × 10−10 M.
Fabry–Perot interferometric biosensors can also be fabricated with OF by incor-
porating a short piece of hollow fiber between two pieces of single-mode fiber [103]
or by adding a hollow core fiber at one extremity of a single-mode fiber [104]. The
spectral reflectance of these devices is sensitive to the optical path length in the cavity:
when the target analytes bind to the receptor, the thickness of the cavity increases
and, consequently, a shift in the interferometric spectrum is observed. Various bioan-
alytical applications have been reported, such as the specific hybridization of 26-mer
target DNA with complementary capture DNA [103], the detection of anti-pig IgG
[105] or, more recently, the detection of BSA with an estimated LOD of 0.48 ng [104].
8.6.4
Evanescence Wave Absorbance-Based Optical Fiber
The working principle of RI-based evanescent wave absorbance biosensors relies
on the fact that the transmission through the fiber depends on the refractive index
difference between the core and the cladding: index variation induced by bioreactions
will produce a change in the transmitted intensity. As for the previous OF-based
configuration, sensitivity is noticeably improved by using tapered OF. Taking into
account that the working wavelength must be carefully chosen to avoid changes in the
transmission due to molecule absorption instead of index variation, evanescent wave
absorbance based biosensors have been mainly implemented at 1310 or 1550 nm.
Specific detection of BSA at a concentration of 10 fg mL−1 were achieved with
tapered OF with waist diameters of 5–10 μm and total lengths of 1000–1200 μm
[106]. A similar biosensor was used for the detection of DNA hybridization: 15-mer
DNA was immobilized in a gold-coated taper and its complementary 10-mer DNA
was successfully detected at a concentration of 750 fM [107]. In addition, specificity
was demonstrated as the sensor was able to distinguish between complementary DNA
and DNA with a single nucleotide mismatch. Based on the same working principle, a
tapered OF biosensor (waist diameter of 10 μm) has been applied to the diagnosis of
celiac disease, with specific detection of antibodies in the range of 1–15 ppm [108]. A
tapered OF biosensor with a waist diameter of 6–7 μm has also been used for in situ
real-time monitoring of E. coli growth (Fig. 8.21) [109].
The transmission spectrum of an OF coupler is also strongly affected by the refrac-
tive index of the surrounding medium. For instance, an OF coupler has been fabricated
with a fused single mode fiber of 9 μm in diameter in the coupler region and applied to
biosensing: binding of streptavidin was demonstrated over a range of 0.5–2 μg mL−1
by immobilizing biotin on the coupler surface via aminosilane chemistry [110].
8.7
LAB-ON-A-CHIP INTEGRATION
LOC platforms are miniaturized analytical devices which integrate all functionalities
on a single chip, from fluid handling, sample preparation (filtration, homogenization,

LAB-ON-A-CHIP INTEGRATION
355
0.2
−0.30
−0.25
−0.20
−0.15
In (lsig/lref)
−0.10
−0.05
0.00
Core
Cladding
Tapered region
np
E.coli
nw
nsol
(b)
(c)
(a)
0.4 0.6 0.8 1.0
Time (h)
1.2 1.4 1.6 1.8
6.14
22.50
7.72
7.72
Epoxy
Fiber optic
Sample addition/removal port
Plexiglass
Scales in mm
FIGURE 8.21
(a) Scheme of the microreactor for bacterial growth; (b) representation of
E. coli bacteria immobilized on the taper region; and (c) sensor response versus time during
E. coli growth on the tapered fiber. Adapted from Reference 109 with permission from Elsevier.
and dilution), target detection, transducer readout, and signal processing. Taking
advantage of the miniaturization, LOC platforms can perform high throughput screen-
ing while consuming only tiny amounts of samples, reagents, space, and energy. They
can also contain enough hard-wired intelligence and robustness to be used by non-
skilled personnel (for instance the patient itself) and should deliver in real time a mul-
titude of data directly to a central database. The best example so far of a stand-alone
LOC is the glucose sensor daily used by diabetic people to test their concentration of
glucose in blood: a small drop of blood is placed on a disposable test strip containing
an electrochemical biosensor and the result appears on a digital display within a few
seconds. The spread of such tests for the detection of very low concentrations of
analytes or toxins in real-time would be of great benefit for early medical diagnosis,
food safety, or environment monitoring.
Label-free optical biosensors appear as one of the most promising candidates for
LOC platforms as they combine highly sensitive and on-chip detection with the pos-
sibility of miniaturization and photonic integration of components on one single chip
(Fig. 8.22). This is particularly true for IO biosensors based on silicon and silicon-
related materials as they can benefit of the strong know-how of the semiconductor
microfabrication industry and its potential for mass production with consequent
reduction of costs.
Some commercial platforms based on optical waveguide biosensors have been
described along this chapter. However, they cannot be considered as real LOC device
because they are bulky and expensive. Moreover, they present some of the drawbacks
of traditional methods (such as ELISA or RIA tests) as they must be used by trained
personnel in a laboratory environment. To achieve a real standalone LOC platform,
the optical transducer in a multiplexed configuration must be integrated with all the
required units (as light sources, photodetectors, microfluidics, processing electronics,
etc.) as well as robust biofunctionalization protocols for the biological receptors.
Even though the individual components are well known, there are still major barriers

356
OPTICAL WAVEGUIDE BIOSENSORS
FIGURE 8.22
Representation of an IO-based LOC platform.
to overcome for their assembly, among others because the subsystem interfaces
between them are difficult to optimize. Integration could be monolithic (all function-
alities are incorporated in one single chip) or hybrid (functionalities are separated on
several chips).
IO-based LOC is still in its early stages and most of the work done so far mainly
includes the integration of the optical transducers with the fluidics, the so-called
optofluidics [111,112]. A challenge specific to IO-based LOC platforms is the way
to reliably couple light into the submicronic cross-section of the waveguides. High
alignment tolerance between the input beam and the photonic chip is also mandatory
as the photonic chips would have to be frequently replaced and the LOC platform
should be portable. Grating couplers arise as the most suitable solution since they
allow for efficient light coupling together with high alignment tolerances.
Few examples can be found in the literature of complete IO-based LOC platforms.
In most reported prototypes, the transducers are microring resonators or interferom-
eters as it has been pointed out throughout this chapter that they are the most viable
alternatives to conventional diagnosis techniques. For instance, vertically coupled
polymeric microresonators showing high performance for biosensing have been inte-
grated with a digital electrowetting-on-dielectric microfluidic system [56] and with
a thin film InGaAs-based metal-semiconductor-metal photodetector [113], demon-
strating that the use of a polymer waveguide does not impede the integration with
complex elements. However, the biosensing capabilities of this intricate system have
not been reported yet. Arrays of 16 and 128 spiral ring resonators based on silicon
photonic wires have also been integrated with microfluidic channels and input and
output grating couplers on a 6 × 9 mm2 chip [114]. An important advantage of this
platform is the integration of unconventional grating couplers formed by rows of
holes that are completely etched, allowing the simultaneous fabrication of both the

SUMMARY
357
gratings and the waveguide structures in only one step by deep-UV lithography. This
platform has been applied to the multiplexed detection of E. coli bacteria with a view
to perform serotyping but complete multiplexed analytical studies have not been
reported yet. In addition, the read-out instrument occupies an area of 60 × 45 cm2
which is still far from a truly portable and stand-alone LOC platform.
Regarding interferometric biosensors, a promising monolithically integrated LOC
platform has been developed based on arrays of MZI fabricated by standard silicon
technology and integrated with lasers, photodetectors, and microfluidics [115]. The
read-out is based on frequency-resolved MZI to solve the problems of interferometric
sensors [38]. They demonstrated a moderate LOD in bulk of 4 × 10−6 RIU and proof-
of-concepts of biosensing were reported [115]. Nevertheless, complete bioanalytical
studies are still pending to validate this LOC platform as a reliable diagnostic tool.
Silicon nitride BiMW interferometers have been integrated with submicronic grating
couplers and with a 3D network of SU-8 polymer microfluidics assembled at the
wafer level to ensure perfect sealing and compact packaging [116]. An all-optical
wavelength modulation system has been applied to the BiMWs to provide a linear
response and a direct read-out of the phase variation system without additional fabri-
cation processes or instrumentation. The sensitivity and specificity of the modulated
BiMW have been demonstrated through the label-free detection of the anti-hTSH
with an LOD of 17 ng mL−1. However multiplexed detection, which is an important
requirement for LOC platform, has not been reported yet.
As illustrated through the previous examples, the application to real-world bioan-
alytical studies is one of the bottlenecks in the development of LOC platforms based
on optical waveguide biosensors. The main reason is that the integration of such plat-
forms requires a strong multidisciplinary expertise in photonics, micro-fabrication,
material science, electronics, and biochemistry but the last one is often left aside:
the groups working in this field are generally mainly composed of physicists and
engineers to develop efficient platforms with high degree of integration, deferring
the bioanalytical part. However, the development of reliable biochemistry processes
(choice of the surface activation process and the immobilization technique as well as
the type and conditions of the assay) is as relevant and crucial step as the development
of the optical transducer as it will turn a sensing device into a valid analytical tool
with the required quality standards.
8.8
SUMMARY
In this chapter, we have overviewed optical waveguide biosensors for label-free
detection, demonstrating their excellent performances: high sensitivity, selectivity,
potential for multiplexing and miniaturization, and so on. Some of them, such as
interferometers, ring resonators, grating-, and OF-based biosensors are relatively
mature and some have even been commercialized. Others, like PC-, silicon wire-
based biosensors, or microstructured optical fibers are still in their early stages but
they have great potential for the next generation of label-free biosensors.

358
OPTICAL WAVEGUIDE BIOSENSORS
Optical waveguide biosensors, and in particular IO-based biosensors, have become
the most suitable technology for the implementation of stand-alone LOC platforms
that could be employed for ultrasensitive real-time diagnosis at the point-of-care.
Progress has been made in the last years in this area but there are still limitations in
the integration of all the components into one single platform and, above all, in the
connections with real-world bioanalytical applications. However, due to the intensive
research effort that is being done, there is no doubt that LOC platforms based on
label-free IO biosensors will be soon available in the market.
REFERENCES
[1] X. D. Fan, I. M. White, S. I. Shopova, H. Y. Zhu, J. D. Suter, and Y. Z. Sun, “Sensitive
optical biosensors for unlabeled targets: a review,” Anal. Chim. Acta 620(1–2), 8–26
(2008).
[2] M. Sch¨aferling, “Fluorescence-based biosensors,” in Encyclopedia of Analytical
Chemistry (John Wiley & Sons, 2006).
[3] M. Strianese, M. Staiano, G. Ruggiero, T. Labella, C. Pellecchia, and S. D’Auria,
“Fluorescence-based biosensors,” in Spectroscopic Methods of Analysis, edited by
W. M. Bujalowski (Humana Press, Springer, 2012), pp. 193–216.
[4] M. C. Morris, “Fluorescence-based biosensors: from concepts and applications,” in
Progress in Molecular Biology and Translational Science (Elsevier Press, 2013).
[5] G. Lifante, Integrated Photonics: Fundamentals (John Wiley & Sons, 2003).
[6] W. Lukosz and K. Tiefenthaler, “Directional switching in planar waveguides effected
by adsorption-desorption processes,” IEE 2nd Eur. Conf. Integ. Opt. 227, 152–155
(1983).
[7] W. Lukosz, “Principles and sensitivities of integrated optical and surface plasmon sen-
sors for direct affinity sensing and immunosensing,” Biosens. Bioelectron. 6(3), 215–225
(1991).
[8] G. M. Hale and M. R. Querry, “Optical constants of water in the 200-nm to 200-μm
wavelength region,” Appl. Opt. 12(3), 555–563 (1973).
[9] H. K. Hunt and A. M. Armani, “Label-free biological and chemical sensors,” Nanoscale
2(9), 1544–1559 (2010).
[10] R. Bruck, E. Melnik, P. Muellner, R. Hainberger, and M. L¨ammerhofer, “Integrated
polymer-based Mach-Zehnder interferometer label-free streptavidin biosensor compat-
ible with injection molding,” Biosens. Bioelectron. 26(9), 3832–3837 (2011).
[11] M. Wang, J. Hiltunen, C. Liedert, L. Hakalahti, and R. Myllyl¨a, “An integrated Young
interferometer based on UV-imprinted polymer waveguides for label-free biosensing
applications,” J. Eur. Opt. Soc. Rapid Publ. 7, 12019–1/7 (2012).
[12] L. Wang, J. Ren, X. Han, T. Claes, X. Jian, P. Bienstman, R. Baets, M. Zhao, and G.
Morthier, “A label-free optical biosensor built on a low-cost polymer platform,” IEEE
Photon. J. 4(3), 920–930 (2012).
[13] A. Leung, P. M. Shankar, and R. Mutharasan, “A review of fiber-optic biosensors,” Sens.
Actuators B: Chem. 125(2), 688–703 (2007).

REFERENCES
359
[14] T. M. Monro, S. Warren-Smith, E. P. Schartner, A. Franc¸ois, S. Heng, H. Ebendorff-
Heidepriem, and S. V. Afshar, “Sensing with suspended-core optical fibers,” Opt. Fiber
Technol. 16(6), 343–356 (2010).
[15] Y. Skibina, V. V. Tuchin, V. I. Beloglazov, G. Shteinmaeer, I. L. Betge, R. Wedell, and
N. Langhoff, “Photonic crystal fibres in biomedical investigations,” Quantum Electron.
41(4), 284 (2011).
[16] A. Sassolas, L. J. Blum, and B. D. Leca-Bouvier, “Optical detection systems using
immobilized aptamers,” Biosens. Bioelectron. 26(9), 3725–3736 (2011).
[17] L. Ye and K. Mosbach, “Molecular imprinting: synthetic materials as substitutes for
biological antibodies and receptors,” Chem. Mater. 20(3), 859–868 (2008).
[18] L. S. Wong, F. Khan, and J. Micklefield, “Selective covalent protein immobilization:
strategies and applications,” Chem. Rev. 109(9), 4025–4053 (2009).
[19] B. Prieto-Simon, M. Campas, and J. L. Marty, “Biomolecule immobilization in biosensor
development: tailored strategies based on affinity interactions,” Protein Peptide Lett.
15(8), 757–763 (2008).
[20] F. Rusmini, Z. Zhong, and J. Feijen, “Protein immobilization strategies for protein
biochips,” Biomacromolecules 8(6), 1775–1789 (2007).
[21] M.-J. Ba˜nuls, R. Puchades, and ´A. Maquieira, “Chemical surface modifications for the
development of silicon-based label-free integrated optical (IO) biosensors: a review,”
Anal. Chim. Acta 777, 1–16 (2013).
[22] J. Diao, D. Ren, J. R. Engstrom, and K. H. Lee, “A surface modification strategy on
silicon nitride for developing biosensors,” Anal. Biochem. 343(2), 322–328 (2005).
[23] A. Arafat, M. Giesbers, M. Rosso, E. J. R. Sudh¨olter, K. Schro¨en, R. G. White,
L. Yang, M. R. Linford, and H. Zuilhof, “Covalent biofunctionalization of silicon nitride
surfaces,” Langmuir 23(11), 6233–6244 (2007).
[24] S. Takayama, J. C. McDonald, E. Ostuni, M. N. Liang, P. J. A. Kenis, R. F. Ismagilov,
and G. M. Whitesides, “Patterning cells and their environments using multiple laminar
fluid flows in capillary networks,” Proc. Natl. Acad. Sci. USA 96(10), 5545–5548 (1999).
[25] A. B. Gonzalez-Guerrero, M. Alvarez, A. G. Casta˜no, C. Dominguez, and L. M.
Lechuga, “A comparative study of in-flow and micro-patterning biofunctionalization
protocols for nanophotonic silicon-based biosensors,” J. Colloid Interface Sci. 393,
402–410 (2013).
[26] J. T. Kirk, G. E. Fridley, J. W. Chamberlain, E. D. Christensen, M. Hochberg, and
D. M. Ratner, “Multiplexed inkjet functionalization of silicon photonic biosensors,”
Lab on a Chip 11(7), 1372–1377 (2011).
[27] T. F. Didar, A. M. Foudeh, and M. Tabrizian, “Patterning multiplex protein microarrays
in a single microfluidic channel,” Anal. Chem. 84(2), 1012–1018 (2011).
[28] J. Fraden, Handbook of Modern Sensor: Physics, Designs, and Applications, 3rd ed.
(Springer, 2003).
[29] D. Duval, A. B. Gonzalez-Guerrero, S. Dante, C. Dominguez, and L. M. Lechuga, “Inter-
ferometric waveguide biosensors based on Si-technology for point-of-care diagnostic,”
Proc. SPIE 8431, 84310P–1/11 (2012).
[30] E. F. Schipper, A. M. Brugman, C. Dominguez, L. M. Lechuga, R. P. H. Kooyman, and
J. Greve, “The realization of an integrated Mach-Zehnder waveguide immunosensor in
silicon technology,” Sens. Actuators B. Chem. 40(2–3), 147–153 (1997).

360
OPTICAL WAVEGUIDE BIOSENSORS
[31] K. E. Zinoviev, L. G. Carrascosa, J. S´anchez del R´ıo, B. Sep´ulveda, C. Dom´ınguez, and
L. M. Lechuga, “Silicon photonic biosensors for lab-on-a-chip applications,” Adv. Opt.
Technol. 2008, 383927–1/6 (2008).
[32] A. Crespi, Y. Gu, B. Ngamsom, H. J. W. M. Hoekstra, C. Dongre, M. Pollnau,
R. Ramponi, H. H. van den Vlekkert, P. Watts, G. Cerullo, et al., “Three-dimensional
Mach-Zehnder interferometer in a microfluidic chip for spatially-resolved label-free
detection,” Lab on a Chip 10(9), 1167–1173 (2010).
[33] A. Mathesz, L. F´abi´an, S. Valkai, D. Alexandre, P. V. S. Marques, P. Ormos, E. K. Wolff,
and A. D´er, “High-speed integrated optical logic based on the protein bacteri-
orhodopsin,” Biosen. Bioelectron. 46(0), 48–52 (2013).
[34] R. G. Heideman and P. V. Lambeck, “Remote opto-chemical sensing with extreme
sensitivity: design, fabrication and performance of a pigtailed integrated optical phase-
modulated Mach–Zehnder interferometer system,” Sens. Actuators B. Chem. 61(1–3),
100–127 (1999).
[35] M. B. Duhring and O. Sigmund, “Improving the acousto-optical interaction in a
Mach-Zehnder interferometer,” J. Appl. Phys. 105(8), 083529 (2009).
[36] P. Dumais, C. L. Callender, J. P. Noad, and C. J. Ledderhof, “Integrated optical sensor
using a liquid-core waveguide in a Mach-Zehnder interferometer,” Opt. Express 16(22),
18164–18172 (2008).
[37] S. Dante, D. Duval, B. Sepulveda, A. B. Gonzalez-Guerrero, J. R. Sendra, and L. M.
Lechuga, “All-optical phase modulation for integrated interferometric biosensors,” Opt.
Express 20(7), 7195–7205 (2012).
[38] M. Kitsara, K. Misiakos, I. Raptis, and E. Makarona, “Integrated optical frequency-
resolved Mach-Zehnder interferometers for label-free affinity sensing,” Opt. Express
18(8), 8193–8206 (2010).
[39] A. Ymeti, J. Greve, P. V. Lambeck, T. Wink, H. van, Beumer, R. R. Wijn, R. G. Heideman,
V. Subramaniam, and J. S. Kanger, “Fast, ultrasensitive virus detection using a young
interferometer sensor,” Nano Lett. 7(2), 394–397 (2007).
[40] A. Brandenburg and R. Henninger, “Integrated optical Young interferometer,” Appl.
Opt. 33(25), 5941–5947 (1994).
[41] A. Brandenburg, “Differential refractometry by an integrated-optical Young interferom-
eter,” Sens. Actuators B. Chem. 39(1–3), 266–271 (1997).
[42] K. Schmitt, B. Schirmer, C. Hoffmann, A. Brandenburg, and P. Meyrueis, “Inter-
ferometric biosensor based on planar optical waveguide sensor chips for label-free
detection of surface bound bioreactions,” Biosens. Bioelectron. 22(11), 2591–2597
(2007).
[43] J. H. Sim, Y. H. Kwak, C. H. Choi, S.-H. Paek, S. S. Park, and S. Seo, “A birefringent
waveguide biosensor platform for label-free live cell detection of Listeria monocyto-
genes,” Sens. Actuators B. Chem. 173, 752–759 (2012).
[44] K. E. Zinoviev, A. B. Gonzalez-Guerrero, C. Dominguez, and L. M. Lechuga, “Inte-
grated bimodal waveguide interferometric biosensor for label-free analysis,” J. Light-
wave Technol. 29(13), 1926–1930 (2011).
[45] A. B. Gonzalez-Guerrero, J. M. Rodriguez-Frade, M. Mellado, C. Dominguez, and
L. M. Lechuga, “High sensitivity detection of human growth hormone using a bimodal
waveguide interferometer,” in XI Conference on Optical Chemical Sensors and Biosen-
sors (EUROPT(R)ODE XI), 2012.

REFERENCES
361
[46] B. H. Schneider, J. G. Edwards, and N. F. Hartmann, “Hartman interferometer: versatile
integrated optic sensor for label-free, real-time quantification of nucleic acids, proteins,
and pathogens,” Clin. Chem. 43(9), 1757–1763 (1997).
[47] G. H. Cross, A. A. Reeves, S. Brand, J. F. Popplewell, L. L. Peel, M. J. Swann, and N. J.
Freeman, “A new quantitative optical biosensor for protein characterisation,” Biosen.
Bioelectron. 19(4), 383–390 (2003).
[48] R. W. Boyd and J. E. Heebner, “Sensitive disk resonator photonic biosensor,” Appl. Opt.
40(31), 5742–5747 (2001).
[49] Y. Sun and X. Fan, “Optical ring resonators for biochemical and chemical sensing,”
Anal. Bioanal. Chem. 399(1), 205–211 (2011).
[50] F. Vollmer and L. Yang, “Label-free detection with high-Q microcavities: a review
of biosensing mechanisms for integrated devices,” Nanophotonics 1(3–4), 267–291
(2012).
[51] C. Barrios, “Integrated microring resonator sensor arrays for labs-on-chips,” Anal.
Bioanal. Chem. 403(6), 1467–1475 (2012).
[52] M. S. Luchansky and R. C. Bailey, “High-Q optical sensors for chemical and biological
analysis,” Anal. Chem. 84(2), 793–821 (2012).
[53] M. C. Estevez, M. Alvarez, and L. M. Lechuga, “Integrated optical devices for lab-on-
a-chip biosensing applications,” Laser Photon. Rev. 6(4), 463–487 (2012).
[54] R. Heideman, M. Hoekman, and E. Schreuder, “TriPleX-based integrated optical ring
resonators for lab-on-a-chip and environmental detection,” IEEE J. Select. Top. Quantum
Electron. 18(5), 1583–1596 (2012).
[55] A. Ramachandran, S. Wang, J. Clarke, S. J. Ja, D. Goad, L. Wald, E. M. Flood,
E. Knobbe, J. V. Hryniewicz, S. T. Chu, et al., “A universal biosensing platform based
on optical micro-ring resonators,” Biosens. Bioelectron. 23(7), 939–944 (2008).
[56] M. W. Royal, N. M. Jokerst, and R. B. Fair, “Integrated sample preparation and sens-
ing: polymer microresonator sensors embedded in digital electrowetting microfluidic
systems,” IEEE Photon. J. 4(6), 2126–2135 (2012).
[57] K. De Vos, J. Girones, T. Claes, Y. De Koninck, S. Popelka, E. Schacht, R. Baets,
and P. Bienstman, “Multiplexed antibody detection with an array of silicon-on-insulator
microring resonators,” IEEE Photon. J. 1(4), 225–235 (2009).
[58] M. S. McClellan, L. L. Domier, and R. C. Bailey, “Label-free virus detection using
silicon photonic microring resonators,” Biosens. Bioelectron. 31(1), 388–392 (2012).
[59] O. Scheler, J. T. Kindt, A. J. Qavi, L. Kaplinski, B. Glynn, T. Barry, A. Kurg, and R. C.
Bailey, “Label-free, multiplexed detection of bacterial tmRNA using silicon photonic
microring resonators,” Biosen. Bioelectron. 36(1), 56–61 (2012).
[60] J. T. Gohring and X. Fan, “Label free detection of CD4+ and CD8+ T cells using the
optofluidic ring resonator,” Sensors 10(6), 5798–5808 (2010).
[61] F. Vollmer and S. Arnold, “Whispering-gallery-mode biosensing: label-free detection
down to single molecules,” Nat. Meth. 5(7), 591–596 (2008).
[62] P. M. Nellen, K. Tiefenthaler, and W. Lukosz, “Integrated optical input grating couplers
as biochemical sensors,” Sens. Actuators 15(3), 285–295 (1988).
[63] I. R. Cooper, S. T. Meikle, G. Standen, G. W. Hanlon, and M. Santin, “The rapid and
specific real-time detection of Legionella pneumophila in water samples using optical
waveguide lightmode spectroscopy,” J. Microbiol. Methods 78(1), 40–44 (2009).

362
OPTICAL WAVEGUIDE BIOSENSORS
[64] N. Ad´anyi, K. Majer-Baranyi, A. Nagy, G. N´emeth, I. Szendr˝o, and A. Sz´ek´acs, “Optical
waveguide lightmode spectroscopy immunosensor for detection of carp vitellogenin,”
Sens. Actuators B. Chem. 176, 932–939 (2013).
[65] S. Grego, K. H. Gilchrist, J. B. Carlson, and B. R. Stoner, “A compact and multichannel
optical biosensor based on a wavelength interrogated input grating coupler,” Sens.
Actuators B. Chem. 161(1), 721–727 (2012).
[66] K. Cottier, M. Wiki, G. Voirin, H. Gao, and R. E. Kunz, “Label-free highly sensitive
detection of (small) molecules by wavelength interrogation of integrated optical chips,”
Sens. Actuators B. Chem. 91(1–3), 241–251 (2003).
[67] J. Adrian, S. Pasche, J.-M. Diserens, F. S´anchez-Baeza, H. Gao, M. P. Marco, and
G. Voirin, “Waveguide interrogated optical immunosensor (WIOS) for detection of
sulfonamide antibiotics in milk,” Biosens. Bioelectron. 24(11), 3340–3346 (2009).
[68] J. Razumovitch, K. de Franca, F. Kehl, M. Wiki, W. Meier, and C. Vebert, “Opti-
mal hybridization efficiency upon immobilization of oligonucleotide double helices,”
J. Phys. Chem. B 113(24), 8383–8390 (2009).
[69] S. V. Pham, M. Dijkstra, A. J. F. Hollink, L. J. Kauppinen, R. M. de Ridder, M. Pollnau,
P. V. Lambeck, and H. J. W. M. Hoekstra, “On-chip bulk-index concentration and direct,
label-free protein sensing utilizing an optical grated-waveguide cavity,” Sens. Actuators
B. Chem. 174, 602–608 (2012).
[70] D. Bhatta, A. A. Michel, M. Marti Villalba, G. D. Emmerson, I. J. G. Sparrow, E. A.
Perkins, M. B. McDonnell, R. W. Ely, and G. A. Cartwright, “Optical microchip array
biosensor for multiplexed detection of bio-hazardous agents,” Biosens. Bioelectron.
30(1), 78–86 (2011).
[71] M. R. Lee and P. M. Fauchet, “Two-dimensional silicon photonic crystal based biosens-
ing platform for protein detection,” Opt. Express 15(8), 4530–4535 (2007).
[72] S. Pal, E. Guillermain, R. Sriram, B. L. Miller, and P. M. Fauchet, “Silicon photonic
crystal nanocavity-coupled waveguides for error-corrected optical biosensing,” Biosens.
Bioelectron. 26(10), 4024–4031 (2011).
[73] S. Mandal and D. Erickson, “Nanoscale optofluidic sensor arrays,” Opt. Express 16(3),
1623–1631 (2008).
[74] S. Mandal, J. M. Goddard, and D. Erickson, “A multiplexed optofluidic biomolecular
sensor for low mass detection,” Lab on a Chip 9(20), 2924–2932 (2009).
[75] S. Chakravarty, W.-C. Lai, Y. Zou, H. A. Drabkin, R. M. Gemmill, G. R. Simon, S. H.
Chin, and R. T. Chen, “Multiplexed specific label-free detection of NCI-H358 lung
cancer cell line lysates with silicon based photonic crystal microcavity biosensors,”
Biosens. Bioelectron. 43, 50–55 (2013).
[76] D. X. Xu, A. Densmore, A. Delˆage, P. Waldron, R. McKinnon, S. Janz, J. Lapointe,
G. Lopinski, T. Mischki, E. Post, et al., “Folded cavity SOI microring sensors for high
sensitivity and real time measurement of biomolecular binding,” Opt. Express 16(19),
15137–15148 (2008).
[77] D. X. Xu, M. Vachon, A. Densmore, R. Ma, A. Delˆage, S. Janz, J. Lapointe, Y. Li,
G. Lopinski, D. Zhang, et al., “Label-free biosensor array based on silicon-on-insulator
ring resonators addressed using a WDM approach,” Opt. Lett. 35(16), 2771–2773 (2010).
[78] A. Densmore, M. Vachon, D.-X. Xu, S. Janz, R. Ma, Y.-H. Li, G. Lopinski, A. Delˆage,
J. Lapointe, C. C. Luebbert, et al., “Silicon photonic wire biosensor array for multiplexed
real-time and label-free molecular detection,” Opt. Lett. 34(23), 3598–3600 (2009).

REFERENCES
363
[79] C. A. Barrios, “Optical slot-waveguide based biochemical sensors,” Sensors 9(6), 4751–
4765 (2009).
[80] C. A. Barrios, M. J. Ba˜nuls, V. Gonzalez-Pedro, K. B. Gylfason, B. S´anchez, A. Griol,
A. Maquieira, H. Sohlstr¨om, M. Holgado, and R. Casquel, “Label-free optical biosensing
with slot-waveguides,” Opt. Lett. 33(7), 708–710 (2008).
[81] C. F. Carlborg, K. B. Gylfason, A. Kazmierczak, F. Dortu, M. J. Banuls Polo,
A. Maquieira Catala, G. M. Kresbach, H. Sohlstrom, T. Moh, L. Vivien, et al., “A
packaged optical slot-waveguide ring resonator sensor array for multiplex label-free
assays in labs-on-chips,” Lab on a Chip 10(3), 281–290 (2010).
[82] S. M. Weiss, G. Rong, and J. L. Lawrie, “Current status and outlook for silicon-
based optical biosensors,” Phys. E. Low-Dimens. Syst. Nanostruct. 41(6), 1071–1075
(2009).
[83] A. Jane, R. Dronov, A. Hodges, and N. H. Voelcker, “Porous silicon biosensors on the
advance,” Trends Biotechnol. 27(4), 230–239 (2009).
[84] S. Dhanekar and S. Jain, “Porous silicon biosensor: current status,” Biosens. Bioelectron.
41, 54–64 (2013).
[85] G. Rong, A. Najmaie, J. E. Sipe, and S. M. Weiss, “Nanoscale porous silicon waveguide
for label-free DNA sensing,” Biosens. Bioelectron. 23(10), 1572–1576 (2008).
[86] M. Holgado, R. Casquel, B. S´anchez, C. Molpeceres, M. Morales, and J. L. Oca˜na,
“Optical characterization of extremely small volumes of liquid in sub-micro-holes by
simultaneous reflectivity, ellipsometry and spectrometry,” Opt. Exp. 15(20), 13318–
13329 (2007).
[87] F. J. Sanza, M. Holgado, F. J. Ortega, R. Casquel, D. L´opez-Romero, M. J. Ba˜nuls, M. F.
Laguna, C. A. Barrios, R. Puchades, and A. Maquieira, “Bio-photonic sensing cells over
transparent substrates for anti-gestrinone antibodies biosensing,” Biosens. Bioelectron.
26(12), 4842–4847 (2011).
[88] V. Mishra, N. Singh, U. Tiwari, and P. Kapur, “Fiber grating sensors in medicine: current
and emerging applications,” Sens. Actuators A. Phys. 167(2), 279–290 (2011).
[89] R. Kashyap, “Principles of optical fiber grating sensors,” in Fiber Bragg Gratings, 2nd
ed. (Academic Press, Boston, 2010), pp. 441–502.
[90] A. N. Chryssis, S. S. Saini, S. M. Lee, Y. Hyunmin, W. E. Bentley, and M. Dagenais,
“Detecting hybridization of DNA by highly sensitive evanescent field etched core
fiber Bragg grating sensors,” IEEE J. Select. Top. Quantum Electron. 11(4), 864–872
(2005).
[91] M. C. Phan Huy, G. Laffont, V. Dewynter, P. Ferdinand, P. Roy, J.-L. Auguste,
D. Pagnoux, W. Blanc, and B. Dussardier, “Three-hole microstructured optical fiber
for efficient fiber Bragg grating refractometer,” Opt. Lett. 32(16), 2390–2392 (2007).
[92] S. Maguis, G. Laffont, P. Ferdinand, B. Carbonnier, K. Kham, T. Mekhalif, and M.-C.
Millot, “Biofunctionalized tilted Fiber Bragg Gratings for label-free immunosensing,”
Opt. Express 16(23), 19049–19062 (2008).
[93] A. Candiani, M. Sozzi, A. Cucinotta, S. Selleri, R. Veneziano, R. Corradini, R. Marchelli,
P. Childs, and S. Pissadakis, “Optical fiber ring cavity sensor for label-free DNA detec-
tion,” IEEE J. Select. Top. Quantum Electron. 18(3), 1176–1183 (2012).
[94] M. P. DeLisa, Z. Zhang, M. Shiloach, S. Pilevar, C. C. Davis, J. S. Sirkis, and W. E.
Bentley, “Evanescent wave long-period fiber bragg grating as an immobilized antibody
biosensor,” Anal. Chem. 72(13), 2895–2900 (2000).

364
OPTICAL WAVEGUIDE BIOSENSORS
[95] R. Garg, S. M. Tripathi, K. Thyagarajan, and W. J. Bock, “Long period fiber grating
based temperature-compensated high performance sensor for bio-chemical sensing
applications,” Sens. Actuators B. Chem. 176, 1121–1127 (2013).
[96] L. Rindorf, J. B. Jensen, M. Dufva, L. H. Pedersen, P. E. Høiby, and O. Bang, “Photonic
crystal fiber long-period gratings for biochemical sensing,” Opt. Express 14(18), 8224–
8231 (2006).
[97] X. Chen, L. Zhang, K. Zhou, E. Davies, K. Sugden, I. Bennion, M. Hughes, and
A. Hine, “Real-time detection of DNA interactions with long-period fiber-grating-based
biosensor,” Opt. Lett. 32(17), 2541–2543 (2007).
[98] X. Chen, K. Zhou, L. Zhang, and I. Bennion, “Dual-peak long-period fiber gratings with
enhanced refractive index sensitivity by finely tailored mode dispersion that uses the
light cladding etching technique,” Appl. Opt. 46(4), 451–455 (2007).
[99] M. Smietana, W. J. Bock, P. Mikulic, A. Ng, R. Chinnappan, and M. Zourob, “Detection
of bacteria using bacteriophages as recognition elements immobilized on long-period
fiber gratings,” Opt. Express 19(9), 7971–7978 (2011).
[100] L. Rindorf and O. Bang, “Highly sensitive refractometer with a photonic-crystal-fiber
long-period grating,” Opt. Lett. 33(6), 563–565 (2008).
[101] D. J. J. Hu, J. L. Lim, M. K. Park, L. T. H. Kao, Y. Wang, H. Wei, and W. Tong, “Photonic
crystal fiber-based interferometric biosensor for streptavidin and biotin detection,” IEEE
J. Select. Top. Quantum Electron. 18(4), 1293–1297 (2012).
[102] L. H. Chen, C. C. Chan, K. Ni, P. B. Hu, T. Li, W. C. Wong, P. Balamurali, R. Menon,
M. Shaillender, B. Neu, et al., “Label-free fiber-optic interferometric immunosen-
sors based on waist-enlarged fusion taper,” Sens. Actuators B. Chem. 178, 176–184
(2013).
[103] X. Wang, K. L. Cooper, A. Wang, J. Xu, Z. Wang, Y. Zhang, and Z. Tu, “Label-free DNA
sequence detection using oligonucleotide functionalized optical fiber,” Appl. Phys. Lett.
89(16), 163901–163903 (2006).
[104] L. H. Chen, X. M. Ang, C. C. Chan, M. Shaillender, B. Neu, W. C. Wong, P. Zu, and K. C.
Leong, “Layer-by-layer (chitosan/polystyrene sulfonate) membrane-based fabry-perot
interferometric fiber optic biosensor,” IEEE J. Select. Top. Quantum Electron. 18(4),
1457–1464 (2012).
[105] Y. Zhang, X. Chen, Y. Wang, K. L. Cooper, and A. Wang, “Microgap multicavity
fabry-p´erot biosensor,” J. Lightwave Technol. 25(7), 1797–1804 (2007).
[106] A. Leung, P. M. Shankar, and R. Mutharasan, “Model protein detection using antibody-
immobilized tapered fiber optic biosensors (TFOBS) in a flow cell at 1310 nm and
1550 nm,” Sens. Actuators B. Chem. 129(2), 716–725 (2008).
[107] A. Leung, P. M. Shankar, and R. Mutharasan, “Label-free detection of DNA hybridiza-
tion using gold-coated tapered fiber optic biosensors (TFOBS) in a flow cell at 1310 nm
and 1550 nm,” Sens. Actuators B. Chem. 131(2), 640–645 (2008).
[108] J. M. Corres, I. R. Matias, J. Bravo, and F. J. Arregui, “Tapered optical fiber biosensor
for the detection of anti-gliadin antibodies,” Sens. Actuators B. Chem. 135(1), 166–171
(2008).
[109] M. I. Zibaii, A. Kazemi, H. Latifi, M. K. Azar, S. M. Hosseini, and M. H. Ghezelaiagh,
“Measuring bacterial growth by refractive index tapered fiber optic biosensor,”
J. Photochem. Photobiol. B. Biol. 101(3), 313–320 (2010).

REFERENCES
365
[110] H. Tazawa, T. Kanie, and M. Katayama, “Fiber-optic coupler based refractive index
sensor and its application to biosensing,” Appl. Phys. Lett. 91(11), 113901–113903
(2007).
[111] X. FanandI. M. White, “Optofluidicmicrosystems for chemical andbiological analysis,”
Nat. Photon. 5(10), 591–597 (2011).
[112] D. Psaltis, S. R. Quake, and C. Yang, “Developing optofluidic technology through the
fusion of microfluidics and optics,” Nature 442(7101), 381–386 (2006).
[113] L. Luan, M. W. Royal, R. Evans, R. B. Fair, and N. M. Jokerst, “Chip scale optical
microresonator sensors integrated with embedded thin film photodetectors on elec-
trowetting digital microfluidics platforms,” IEEE Sens. J. 12(6), 1794–1800 (2012).
[114] S. Janz, D. X. Xu, M. Vachon, N. Sabourin, P. Cheben, H. McIntosh, H. Ding, S. Wang,
J. H. Schmid, A. Delˆage, et al., “Photonic wire biosensor microarray chip and instru-
mentation with application to serotyping of Escherichia coli isolates,” Opt. Express
21(4), 4623–4637 (2013).
[115] E. Makarona, P. S. Petrou, A. Bourkoula, A. Botsialas, M. Kitsara, S. E. Kakabakos,
R. Stoffer, G. Jobst, G. Nounesis, I. Raptis, et al., “Monolithically integrated Mach-
Zehnder biosensors for real-time label-free monitoring of biomolecular reactions,” in
33rd Annual International Conference of the IEEE EMBC, pp. 7654–7657 (2011).
[116] D. Duval, A. B. Gonzalez-Guerrero, S. Dante, J. Osmond, R. Monge, L. J. Fernandez,
K. E. Zinoviev, C. Dominguez, and L. M. Lechuga, “Nanophotonic lab-on-a-chip plat-
forms including novel bimodal interferometers, microfluidics and grating couplers,”
Lab on a Chip 12(11), 1987–1994 (2012).


9
LIGHT PROPAGATION IN HIGHLY
SCATTERING TURBID MEDIA:
CONCEPTS, TECHNIQUES, AND
BIOMEDICAL APPLICATIONS
R. R. Alfano,1,2 W. B. Wang,1,2 L. Wang,1 and
S. K. Gayen2
1Institute for Ultrafast Spectroscopy and Lasers (IUSL), City College of New York,
New York, NY, USA
2Physics Department, City College of New York, New York, NY, USA
9.1
INTRODUCTION
The field of biomedical optics has emerged as one of major fields in science and
engineering since the late 1980s [1–6]. Optical imaging and spectroscopic techniques
are making headway into medical diagnosis and treatment practices. Proliferation of
optical and spectroscopic methods in the medical arena is being made possible by
advances in versatile compact light sources, such as lasers and light emitting diodes;
optical fibers; miniaturized spectrometers, charge-coupled device (CCD) and near-
infrared area cameras, high-sensitivity detectors; microelectronic components and
systems; and compact computers with robust memory and computation prowess.
Photodynamic Therapy (PDT) [7], Optical Coherence Tomography (OCT) [8,9], and
Optical Biopsy [10] are three examples of optical methods making transition from
laboratory to clinic and having significant impact on medical treatment and diagnosis,
respectively. Another advance in the horizon is a “photonic pill”/“pill CAM” equipped
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
367

368
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
with millimeter-size spectroscopic equipment and wireless communication devices
for visualization of tissue surface inside the gastrointestinal (GI) tract [11]. Photonic
wound healing is another development around the corner. Optical stitches can be
made using near-infrared (NIR) lasers operating in the 1000–1800 nm wavelength
range to excite water and collagen molecules inside the tissue [12]. Other promising
optical and multimodality techniques for detecting objects hidden inside turbid media,
in particular, cancerous or other diseased tissues in the host normal tissues, include
(1) photoacoustic tomography (PAT) which forms images by detecting the light-
induced pressure waves [13, 14], (2) combination of the ultrasound (US) and NIR
imaging for the purpose of cancer detection [15, 16], and (3) using cancer receptor
targeting contrast agents and their emission to enhance the imaging contrast of
cancerous and normal tissues [17–19].
Optics will provide new armamentaria to complement the current medical tools,
such as X-rays, CAT (computer-aided tomography), magnetic resonance imaging
(MRI), ultrasound, radioactive isotopes, and PET (positron emission tomography)
scanner, for imaging body parts; as well as for chemical testing, which requires blood,
urine, and other body liquids. The interest in light-based modalities arises from a need
to have tools, which are safe, compact, affordable, and enable bloodless monitoring of
tissue chemicals preferably without removing tissue. Light offers the opportunity to
overcome the limitations of many current methods. Most non-optical imaging meth-
ods cannot reveal a diagnosis, giving only an image for doctors to interpret. Optical
spectroscopy can, in principle, monitor molecular changes in key body chemicals
that may accompany diseases and offers diagnostic potential [20–25]. Over the past
two decades, significant progress has been made in optical spectroscopy and imaging
for cancer detection and diagnosis. Combined with different mathematical tools, the
fluorescence spectroscopy can be used to extract changes of intrinsic tissue emission
spectra during the induction of morphological and molecular changes in human tissue
[26]. Other spectral approaches, which can be used to examine changes associated
with biomolecules in tissue during cancer development [27], include Raman spec-
troscopy [28–31], scattering spectroscopy [32], Stokes shift spectroscopy [33–36],
excitation-emission matrix (EEM) spectroscopy [37], and time-resolved fluorescence
spectroscopy [38,39].
This chapter provides an overview of the physics of light propagation in highly
scattering turbid media, such as biological tissues, the techniques that enable using
light emergent from those media for gaining information about the media interior, and
their potential application in biomedical imaging. One of the goals of these researches
is to develop optical tomography systems. The interaction of light with a scattering
medium, characteristics of the transmitted light, key parameters that characterize the
light emergent from the medium, and light gating methods that reduce the deleterious
effect of multiple scattered light and help clear up images will be presented. The major
application of this endeavor will be to develop an Optical Tomography Diagnostic
System for diagnosing diseases of breast, brain, prostate, and other body organs.
The material presented here is mainly derived from some of the pioneering articles
published by the researchers at the Institute for Ultrafast Spectroscopy and Lasers

PHYSICS BEHIND OPTICAL IMAGING
369
(IUSL) in the City College of New York. By no means is this a review of the entire
field and of contributions made by all researchers active in the field.
9.2
PHYSICS BEHIND OPTICAL IMAGING THROUGH
A HIGHLY SCATTERING TURBID MEDIUM
Biomedical optical imaging techniques are used to identify, locate, and diagnose an
“object,” such as a tumor inside a biological tissue. Biological tissues are scattering
media made up of molecules, cells, vessel structures, and layers of different sizes
on nanometer to millimeter scales. These scattering processes blur the image. The
successful use of optical methods to solve biomedical imaging problem has eluded
researchers since the futile attempts to detect breast tumors using light from lamp in
1929 by Max Cutler [40]. The interest in this technique, known as diaphanography,
has increased throughout this century as brighter light or transillumination sources and
sophisticated image recording methods were invented and new inverse mathematics
were developed.
The basis of transillumination is the difference in the transmission of light through a
turbid medium (such as normal tissue) and an “object” (such as a tumor) inside it. This
differential transmission, in principle, should lead to the formation of a “shadow” of
the object (tumor) in transmitted light. Observation of that shadow image is hindered
since light scattered by the medium (tissue) severely degrades and completely buries
the image in the background light. The key to successful development of optical
imaging techniques is to deal with the problem of light scattering because light
scattering blurs images. It is important to understand how light propagates through
a scattering medium. Depending on wavelength of light, it is both scattered and
absorbed in varying degree by biological tissues resulting in diffusive glow [41]. It
is important to know how the characteristics of intensity, coherence, and polarization
of the incident light changes as it is absorbed and scattered inside the tissue. The
depth of light penetration into the tissue depends on its wavelength. There are several
optical transmission windows in tissue; in particular, the first is from 700 to 1100 nm
called therapeutic window. The key chromophore molecules in biological tissues are
blood, water, proteins, and fat. The light that emerges from the medium has different
characteristics than the incident light.
Absorption results from electronic and vibrational transitions within the energy
levels of the atoms and molecules in the tissue. Scattering arises from variations
of index of refraction in the microscopic and macroscopic structures of the tissue;
it increases strongly as the wavelength of light becomes comparable to the size of
the scattering elements. In addition to absorption and scattering of light, fluorescence
emittedbytheobjectsuponabsorptionofincidentlightmayalsoprovidethenecessary
contrast for imaging and has led to fluorescence imaging approaches [42–45]. This
chapter will focus mainly on direct imaging approaches based on scattering and
absorption contrasts. A scattered light pulse comprises three components: ballistic,
snake, and diffuse. The transmitted and backscattered light is composed of unscattered

370
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
Incident light
Turbid medium
Scattered light
Object
Transmitted light
Ballistic
Snake
Diffusive
I(t)
t
I(t)
t
I(t)
t
FIGURE 9.1
Schematic diagram of light pulse propagating in a turbid medium showing the
ballistic, snake, and diffuse photons. Adapted from Reference 52.
(or coherently scattered), weakly scattered (snake), and multiple-scattered (diffuse)
photons, as displayed schematically in Figure 9.1.
9.2.1
Components of Transmitted Light
Laser light characterized by high degree of directionality, spectral brightness,
monochromaticity and coherence is a natural choice for imaging through highly scat-
tering media. Strong absorption and scattering by the intervening medium attenuate
the intensity of incident light and modify its salient features, such as directionality,
coherence, polarization and pulse duration in the case of ultrashort pulses of light.
The transmitted photons of an ultrashort pulse of light are composed of three com-
ponents: ballistic, snake, and diffusive [46], as shown schematically in Figure 9.1.
The ballistic or coherently forward scattered photons propagate in the direction of
the incoming beam, traverse the shortest path, retain most of the characteristics of the
incident light, and carry the maximum information for formation of a direct image.
The multiple-scattered photons travel long distances within the medium, lose many
of their initial characteristics, and emerge later in all directions. They constitute what
is known as the diffusive component of the transmitted light. The photons that scatter
slightly in the forward direction retain significant initial properties and information on
structures hidden in the scattering medium. These photons are called snake photons
because their trajectories resemble a wriggling snake.
Since the photons of an incident short pulse of light spend different times in transit
through the intervening medium, the transmitted pulse becomes broadened with the
ballistic photons arriving first, followed by the snake and diffusive photons, also dis-
played in Figure 9.1. Ballistic photons are most effective in forming a shadow image.
Snake photons also generate transillumination images whose resolution depends on
the position and width of the temporal slide used in imaging. The snake photons
that form a well-resolved shadow image arrive much earlier than most of the diffuse

PHYSICS BEHIND OPTICAL IMAGING
371
photons. The intensity of the forward-transmitting light is attenuated by absorption
and scattering. The relative intensities of these three components in the transmitted
beam depend on the color of light and characteristics of the sample. In a highly
scattering optically thick medium, the ballistic component is extremely weak, while
the diffuse component is the most intense. To form images of the structures inside
the turbid medium using ballistic and snake photons, the most numerous diffusive
photons need be strongly discriminated against.
Two alternative approaches are being pursued to alleviate this problem. The first
involves filtering out the image-bearing photons from the background of multiple-
scattered image-blurring photons using clever experimental techniques [8,9,47–72].
The second approach, sometimes referred to as the inverse problem [4–6, 73, 74],
is based on detection of all of the photons including multiple-scattered photons
at various positions around the object. The aim is to construct the image from the
measured intensities, known experimental parameters, and tissue characteristics using
model of light propagation and sophisticated computer algorithms. The impetus for
solving the inverse problem stems from the realization that for tissues that are more
than a few centimeters thick, the only transmitted light is the scattered light.
Both frequency-domain and time-domain data may be used in the second approach.
The thrust is to develop appropriate algorithms to generate a map of the interior
structure of a turbid medium in a reasonable time. The Monte Carlo method focuses
on the particle nature of light and promises higher accuracy at the cost of higher
computational complexity and time [73]. Alternate methods based on the diffusion
approximation of the radiative transfer equation are being pursued by various groups
[4–6,74–76].
Near-infrared light spanning the 700–1100 nm spectral range in the first optical
transmission window appears to be a judicious choice. Light in this wavelength
range is not as strongly absorbed by tissue as the visible light, and so will have
higher transmission and less likelihood of causing tissue burns. The availability
of broadly wavelength-tunable ultrafast solid-state lasers, such as Ti:sapphire [77]
and Cr:forsterite [78], to cover this spectral range is an advantage. The wavelength
tunability offers the possibility of monitoring different chemical species by tuning
to different wavelengths. Near-infrared light is also less scattered than visible light
and is not known to cause tissue damage even with prolonged exposure at intensity
levels that may be needed for routine screening. The balance of small absorption and
scattering can improve images.
9.2.2
Key Optical Parameters for Describing Light Propagation
in Highly Scattering Media
Knowledge of the key optical properties of scattering medium (Intralipid-10% sus-
pension in water, milk, polystyrene beads of different sizes suspended in water, and
breast, brain, and prostate tissues) is needed for developing photonic techniques for
optical imaging and tomography approaches. The knowledge of four optical param-
eters is needed for imaging. These are absorption length la, scattering mean free path
ls, transport mean free path lt, and anisotropy factor g.

372
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
The ballistic light is considered to be the fraction of the incident light that is
not affected by scattering events within the medium and retains the characteristics
(coherence, directionality, polarization) of the incident beam. The attenuation of the
ballistic photons in traversing a length L of a turbid medium may be estimated by
Ib = Ii exp[−(L∕la + L∕ls)],
(9.1)
where Ii is the intensity of the incident beam; la is the absorption length, defined as
the mean distance a photon travels before being absorbed by the turbid medium; and
ls is the scattering mean free path, defined as a mean distance between consecutive
scattering events for a photon. The parameters la and ls are related to the absorption
coefficient 𝜇a and scattering coefficient 𝜇s of the medium, respectively, as
la = 𝜇−1
a
and ls = 𝜇−1
s .
(9.2)
The total attenuation coefficient is 𝜇T = 1
la + 1
ls = 1
lT , where lT is the total attenu-
ation length.
The spatial anisotropy of scattering is given by the anisotropy factor g, defined as
the mean cosine of the scattering angles, that is, g = ⟨cos 𝜃⟩. The value of g is 1 for
propagation in the incident direction and 0 for isotropic scattering.
Another important parameter is the transport mean free path lt defined as the
distance in which a photon is fully randomized (forgets its original direction of
motion) after numerous scattering events, as illustrated schematically, in Figure 9.2.
Turbid media
ls
ls
ls
ls
It
ls
ls cos∞   ≈ 0
θ
ls cosn  θ
ls cos3  θ
ls cos2  θ
ls cos θ
FIGURE 9.2
Mean free scattering distance ls, mean free transport distance lt, and anisotropy
factor, g.

PHYSICS BEHIND OPTICAL IMAGING
373
The relationship between lt and ls is given by
⟨lt⟩=
∑
lŝn,
(9.3)
where ̂n represents the vector displacement of a photon in turbid media. Written
explicitly,
⟨lt⟩= ⟨ls + ls cos 𝜃+ ls cos2 𝜃+ ls cos3 𝜃+ ⋯+ ls cosn 𝜃+ ⋯⟩
⟨lt⟩= ls∕(1 −⟨cos 𝜃⟩) = ls∕(1 −g).
(9.4)
One also defines the reduced scattering coefficient,
𝜇′
s = 𝜇s(1 −g) = (lt)−1.
(9.5)
The parameters la, ls, 𝜇a, and 𝜇s are intrinsic properties of the material medium
and are given by
ls = 𝜇−1
s
= (N𝜎s)−1, and la = 𝜇−1
a ,
(9.6)
where N is the volume concentration of particles, and 𝜎s and 𝜎a are the scattering
cross-section and absorption cross-section, respectively.
The intensity of snake light is found, from experiments, to follow the equation
[79,80],
Is(Δt) = A exp[−bz∕lt],
(9.7)
in time interval Δt, where b is a parameter that depends on Δt, and has an average value
of 0.8. The snake light is a portion of the photons that arrive before multiple-scattered
diffusive photons and after the ballistic component.
The values of g, ls, and lt depend on particle size and are calculated using Mie
scattering theory [81] and shown in Figures 9.3 and 9.4, respectively. In Figure 9.3
it can be seen that the g factor greatly depends on wavelength, especially when
the particle size is less than 1 μm, which is close to the wavelengths of 0.527 and
1.0
0.8
0.6
0.4
0.2
0.0
0.01
0.1
Particle dimeter (μm)
1
1054 nm
527 nm
g-factor
10
FIGURE 9.3
Calculated values of g-factor as a function of particle diameter [81].

374
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
100
10
It, Is (mm)
1
0.1
0.01
0.01
0.1
Particle dimeter (μm)
Is.527 nm
It.1054 nm
It.527 nm
Is.1054 nm
1
10
FIGURE 9.4
Values of lt and ls calculated as a function of particle diameter in 10% diluted
polystyrene bead suspension in water [81].
1.054 μm. For the Intralipid-10% suspension with an average particle diameter of
∼0.5 μm, the values of g will be ∼0.9 and ∼0.6 for 0.527 and 1.054 μm, respectively.
At larger particle diameters, g oscillates around 0.85 with a deviation of ∼5% for
the wavelength of 1.054 μm. In Figure 9.4, the calculation was performed for 1%
(volume density) bead solutions (10% in stock). Due to the wavelength dependence,
there is smaller difference between lt and ls for particles with smaller diameter. When
the diameter increases, the difference increases. When diameter is more than 3 μm,
the difference in values of both lt and ls increases for the two wavelengths.
9.2.3
Values of Key Optical Parameters for Human Tissues
and Some Model Media
Tissue scatters light strongly in the forward direction, the g value is ∼0.9. A value of
g = 0.9 makes lt = 10ls. Intralipid-10% suspension with g ∼0.8 and 0.6 for 530 nm
and 1054 nm, respectively, and lower absorption coefficient has been widely used as
a model medium to simulate homogenized breast tissue [82, 83]. The particle size
distribution in Intralipid-10% solution is shown in Figure 9.5.
Associated optical parameters of Intralipid-2% suspension and human breast tis-
sues at visible and IR wavelengths are shown in Table 9.1. Intralipid-10% suspension
contained 50-g soya bean oil, 6-g phospholipid, 11.25-g glycerol, and 2% concentra-
tion solution was made by diluting Intralipid-10% with distilled water [81].
It can be seen from the data in Table 9.1 that the transport mean free path of 2%
diluted Intralipid-10% suspension is ∼3.4–5 mm, 5 mm, and 6.6 mm at 630 nm,
800 nm, and 1064 nm wavelengths, respectively. In the early time detection lKF.s =
2.3 mm and lt = 4.6 mm, if g ∼0.5 at 1054 nm.
For the 2% diluted Intralipid-10% suspension, the la = ∼500 nm from 620 to
1064 nm. For breast tissues, lt = ∼0.5–2.6 mm at 620 nm and la = ∼100 mm. For
the mean cosine of scattering angle g, there is a difference between real breast

TABLE 9.1
Optical properties of human breast tissues and 2% intralipid solution
Tissue type
and 2%
intralipid
Transport
mean free
path lt (mm)
Absorption
length la
Anisotropy
factor, g
Scattering
length ls (mm)
Wavelength
(nm)
Glandular
1.25
0.8
17
0.91
0.067
1064
700
Adipose (fatty
breast)
0.7
1.2
0.77
1.0
1.3
155
10
100
100
130
0.95
0.0040 ± 0.0006
620
1064
700
633
800
Carcinoma
1.21
0.5
0.6
0.54
0.96
230
155
24
62.5
0.88
1064
700
620
633
800
Benign
2.6
30
620
Fibroadenoma
2.08
1.55
13
100
1064
620
2% diluted
of stock
intralipid-
10%
3.0
10
3.4–5.0
6.6
5.0
500
500
0.5
0.71–0.8
0.5
0.64
1.0
3.3
1.83
2.3
1.23
620
1064
633
1064
800
1054
527
Source: Adapted from Reference 81.
0
100
200
300
400
500
600
700
1
10
100
1000
10,000
Particle number
Size (nm)
FIGURE 9.5
The particle size distribution in Intralipid-10% suspension. Adapted from
Reference 81.

376
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
TABLE 9.2
Transport mean free path and absorption length with standard deviations
for various random media at 620 and 1064 nm measured using time-resolved
transmission and diffusion approximation
Transport mean free path, lt (mm)
Absorption length (nm)
Random media
1064 nm
620 nm
1064 nm
620 nm
Benign human breast
∼5.1∗
2.6 ± 0.2
30 ± 5
Fatty human breast
∼1.4∗
0.7 ± 0.1
200
155 ± 50
Chicken breast
4.9 ± 0.3
2.5 ± 0.5
70 ± 10
60 ± 30
Chicken fat
0.74 ± 0.1
0.6
230 ± 70
155
Pork breast
3.5 ± 0.5
1.2 ± 0.2
70 ± 10
70 ± 8
Pork fat
0.58 ± 0.05
0.35 ± 0.05
400 ± 60
300 ± 50
Whole milk (25%)
1.0 ± 0.1
253 ± 50
2% solution of
Intralipid-10%
10 ± 1, 4.4∗∗
3.0 ± 0.1
500 ± 50
500 ± 50
∗Estimated by 𝜇t-IR ≈0.5𝜇t-Visible; ∗∗Estimated by 𝜇t-1054 nm from data.
Source: Adapted from Reference 81.
tissue and Intralipid suspension. The gtissue is from 0.88 to 0.95, while gIntralipid
is from 0.5 to 0.83, which was calculated from Mie’s theory. Table 9.2 and
Table 9.3 show the calculated results from the time-resolved and other measurements
[57,79,81,84].
For 2% diluted Intralipid solution, the scattering mean free path at 1054 nm,
ls ≈2.2 mm, and lt ≈ls∕(1–0.5) = 4.4 mm has been measured using a K-F imaging
system [83].
Breast tissue consists of fat, fiber, glandular, and blood. The mean free transport
path and the absorption coefficients of four different breast tissues at 1060, 800, and
625 nm are displayed in Table 9.3 [81].
The lt values for adipose (fat) tissue listed in Table 9.3 are different from values
in Table 9.2. The main reason of the discrepancy is that samples used for Table 9.3
were compressed homogenized tissues, which no longer possessed any natural
structure [81]. The values in Table 9.2 were obtained from in vitro samples. It can
be seen from Table 9.3 that the transport mean free path of the normal glandular
TABLE 9.3
Transport mean free path and absorption length with standard deviations
for various random media at 620 and 1064 nm [81]
1060 nm
800 nm
625 nm
Tissue types
lt (mm)
la (mm)
lt (mm)
la (mm)
lt (mm)
la (mm)
Normal glandular
1.25
17
0.84
25
0.60
20
Normal adipose
1.2
10
1.4
14
1.28
13
Fibroadenoma
2.08
13
1.55
100
1.04
17
Ductal carcinoma
1.21
7
1.14
33
0.79
8

PHYSICS BEHIND OPTICAL IMAGING
377
FIGURE 9.6
Absorption spectra of tissue constituents. Adapted from Reference 85.
tissue strongly depends on wavelength and decreases with the decrease of the
wavelength.
9.2.4
Optical Absorption Spectra of Key Chromophores in Tissues
The absorption in NIR of different key chromophores in tissue, such as water (H2O),
hemoglobin (Hb), and oxy-hemoglobin (HbO2), as a function of wavelength is shown
in Figure 9.6 [85]. It suggests that the wavelength range of 700–1100 nm is most
suitable for optical mammography since there is a weaker absorption by H2O, the
main component of tissue. Attenuation spectra of distilled water and the 2% diluted
intralipid-10% stock solution in 1-mm-thick cell are shown in Figure 9.7. This curve
shows that scattering dominates from 200 to 800 nm, while absorption by water
dominates the attenuation for wavelength greater than 1300 nm. The absorption of
water appears at ∼900 nm, ∼1200 nm, ∼1400 nm, and longer. There is another optical
window in 1600–1800 nm region where scattering is low.
The absorption in 700–1100 nm range is shown in Figure 9.8 for water (H2O),
oxyhemoglobin (HbO2), and deoxyhemoglobin (Hb). In this wavelength range, Hb
and HbO2 can be used to measure hypoxia and may play an important role to separate
cancer from normal breast tissues. Hypoxia can be an important marker of breast

378
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
3.0
2.5
2.0
1.5
1.0
0.5
0.0
200
400
600
800
Water
2% Intralipid
Attenuation (O.D.)
1 mm cell
1000
Wavelength (nm)
1200
1400
1600
1800
FIGURE 9.7
Attenuation of light by water and 2% diluted intralipid in an 1-mm cell.
cancer. Further water absorption appears in the 1000–1400 nm region and offers
another biomarker for cancer detection.
9.3
STUDY OF BALLISTIC AND DIFFUSE LIGHT COMPONENTS
It was believed prior to 1989 that when a light pulse enters a highly scattering medium
it broadens and expands. However, Yoo and Alfano [46] observed a startling effect
in the ultrashort time domain. A unscattered weak pulse of light (ballistic) traveling
collinearly out of 10-mm-thick scattering medium consisting of 0.33 μm latex beads
Wavelength (nm)
O.D.
FIGURE 9.8
Absorption spectra of Hb, HbO2, and H2O. (For a color version of this figure,
see the color plate section.)

STUDY OF BALLISTIC AND DIFFUSE LIGHT COMPONENTS
379
d = 15.8 μm
3.134
0.46
0.33
0
0
0.5
IC / It
1.0
10
20
n σt z
30
FIGURE 9.9
Plots of ratio of coherent ballistic (Ic) to total (It) intensity of scattered light
in the forward direction versus (n𝜎sz) for four different scatterer diameters. The solid curve is
plotted from Eq. (1) in Reference 46.
suspended in water appeared when using 80-fs, 620-nm laser pulses. A Hamamatsu
streak camera of 5 ps time resolution was used in the experiment. The temporal and
angular information was obtained simultaneously indicating that the light component
was coherent. This coherent light pulse was designated as the “ballistic pulse” which
obeys Eq. (9.1) and is the weak remnant of the input laser pulse.
The transition from coherent to incoherent scattering is presented in Figure 9.9
for a 10-mm-thick slab of bead random medium with bead diameters from 0.33 to
15.8 μm. At a low concentration of scatterers, up to n = 7.0 × 1016 m−3 (n𝜎sz = 17.9),
only the coherent component is visible. The incoherent component becomes visible
at a slightly higher concentration, where n = 8.0 × 1016 m−3 (n𝜎sz = 20.4). The
incoherent component rapidly gains in intensity relative to the coherent counterpart
as the concentration increases. The corresponding n𝜎sz values are 22.8, 24.3, and
26.8. When n𝜎sz is above 26, only the incoherent component is visible.
The coherent and incoherent components of ultrafast laser pulses passing through
a slab of random media may be separated from each other using time-resolved
measurement techniques. The ability to time-resolve the coherent from incoherent
components is important in medical imaging and diagnostics. The diffusion approxi-
mations of RTE that is commonly used to describe light propagation through optically
thick media (sample thickness z ≫lt) fails [86] to explain the experimental data as
the value of z∕lt decreases below 10.
There is a regime of scattering where the diffusion approximation fails, in particu-
lar, when the source-detector distance is small, say z∕lt ≤10 for inverse imaging. Dif-
fusion theory is used to explain occurrences where the transport of particles and waves
in random media occurs. The analytical solution for the diffusion equation are gen-
erally available and are found to be valid [87,88] when z∕lt ≫1. For many physical

380
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
media, the wave transport occurs in the region where z∕lt is small. The diffusion
approximation has been extensively used in many of the pioneering light-scattering
studies [88–94], in applications of light scattering to study physical and biological
media [95–97] and inverse imaging [4–6].
It is important to know at what value of z∕lt the diffusion approximation begins to
fail. The transport of photons through a slab of random medium was directly studied
using ultrafast laser pulses and time-resolved detection. Yoo et al. [86] provided
the first direct insight on when and how the diffusion approximation deviates from
the actual transport of photons through a slab of random medium [87]. The results
resolved the controversy on whether or not photon transport is diffusive after passing
through a slab of random medium of only a few transport mean free path thick. The
time-resolved studies using 100-fs, 620-nm laser pulses show that the prediction of the
diffusion approximation deviates monotonically from the measured scattered pulse as
the value of z∕lt decreases below 10. The scattered photons are found to arrive much
earlier than predicted by diffusion approximation. Ballistic photon transmission has
been shown to exist in a small-diameter random-bead medium [46].
A direct method to probe the transport of photon pulses in a random medium is to
measure the distribution of photon arrivals in the time domain. When a plane-front
ultrafast pulse is incident normally onto a slab of random medium, the temporal
distribution of photon arrivals at a point on the opposite side of the slab is predicted
by the diffusion theory as [46]
Iz(t) = D
𝜋z2
∑∞
m=1 m
(𝜋z
d
)2
sin
(m𝜋z
d
)
× exp
[
−Dt
(m𝜋
d
)2]
,
(9.8)
where D = vlt∕3 is the diffusion coefficient, d = z + 2z0, z0 = 0.71lt, 𝜈is the speed
of photons, and z and lt were defined earlier.
The experimental arrangement shown schematically in Figure 9.10 closely
imitates the geometry governed by Eq. (9.8). Ultrafast laser pulses of 100 fs duration
were generated at the repetition rate of 82 MHz by a colliding pulse mode-locked
dye-laser system. The laser power was 10 mW at a wavelength centered on 620 nm.
The laser beam of 4 mm diameter was split into two beams: a reference beam to mark
the zero time of the signal beam, which was expanded to 35 mm diameter. The central
portion of the expanded beam was selected by a diaphragm and passed through
the random medium consisting of latex beads suspended in water contained in a
cylindrical 50 mm diameter and 10 mm thickness glass cell. Photons scattered out
of the cell will be lost, that is, both sides of the cell satisfied an absorbing boundary
condition. A black pinhole of 2 mm diameter was placed at the center on the opposite
side of the cell. The temporal distribution of photons scattered out from this pinhole in
the forward direction at an angle of 10 mrad was measured by a synchro-scan streak
camera. The diameter of the incident beam was set to 20 mm; any further increase in
the diameter will not change temporal profile of the scattered photons. The measured
scattered pulse profile can be described by Eq. (9.8) if the diffusion approximation
is valid.

STUDY OF BALLISTIC AND DIFFUSE LIGHT COMPONENTS
381
Glass
slides
100 fs
Photodiode
Reference
pulse
Mirror
Mirror
Mirror
Sample
Aperture
Trigger
Streak
camera
CCD
fs
Laser
Beam splitter
Beam
splitter
Lens
Beam
expander
Mirror
FIGURE 9.10
A schematic diagram of the experimental setup.
Experimental studies were carried out for different bead diameters, each at a series
of different concentrations. For highly concentrated random medium, where z∕lt ≫1,
the diffusion equation holds, which was not surprising. In this case, the transport mean
free path was obtained by fitting the scattered pulse profile using Eq. (9.8). Figure 9.11
illustrates an example where the diffusion theory fits the experimental data. The pho-
ton transport mean free path lt was found to be 0.309 mm which is close to the value of
0.25 mm computed from Mie theory using lm
t = 1∕n𝜎m, where n is the number density
of scattering particles and 𝜎m is the momentum-transfer scattering cross-section.
Figure 9.11 and Figure 9.12 display a series of transmitted pulse profiles for
media with decreasing bead concentration for bead diameters of 0.296 and 3.134 μm,
respectively. Figures 9.11a and 9.12a show that the experimental (solid) curves can
be fitted by diffusion theory (dashed curves), where z∕lt is large as indicated in
the figures. Figures 9.11b and 9.12b, with z∕lt ≈10, clearly show deviations. The
measured photons arrive earlier than predicted by the diffusion approximation. The
peaks of the theoretical (dashed) curves are normalized to the peak of the experimental
data, whereas the dash-dotted curves are normalized to the total intensity of the
experimental results. None of the theoretical plots from the diffusion approximation
agrees with the experimental results at early time when z∕lt > 10.
A quantitative indication where the diffusion approximation breaks down is the
average time of photon arrival from the pulse. The average arrival time is computed by
̄t = ∫
∞
0
dt′ t′I(t′)
/
∫
∞
0
dt′I(t′),
(9.9)

382
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
0
600
Time (ps)
(c) Z / lt = 5.4
(b) Z / lt = 10.6
(a) Z / lt = 32.4
Transmitted intensity (a.u.)
1200
FIGURE 9.11
Transmitted pulse profiles of 10-mm-thick random media consisting of
latex beads of diameter 0.296 μm. The bead concentration n, lt, and z∕lt are, respectively,
(a) 6.65 × 108 mm−3, 0.309 mm, and 32.4; (b) 2.18 × 108 mm−3, 0.941 mm, and 10.6; and
(c) 1.10 × 108 mm−3, 1.853 mm, and 5.4. Solid curves are the experimental results and dashed
curves are computed from the diffusion theory where their peak intensity is normalized.
Adapted from Reference 86.
0
600
Time (ps)
(d) Z / lt = 2.8
(c) Z / lt = 5.5
(b) Z / lt = 10.1
(a) Z / lt = 23.1
1200
Transmitted intensity (a.u.)
FIGURE 9.12
Transmitted pulse profiles of 10-mm-thick random media consisting of
latex beads of diameter 3.134 μm. The bead concentration n, lt, and z∕lt are, respectively,
(a) 9.62 × 105 mm−3, 0.433 mm, and 23.1; (b) 4.20 × 105 mm−3, 0.989 mm, and 10.1; and
(c) 2.28 × 105 mm−3, 1.823 mm, and 5.5. Solid curves trace the experimental results. Dashed
and dash-dotted curves are computed from the diffusion theory where their peak and total
intensity are normalized to the experimental curves, respectively [86].

STUDY OF BALLISTIC AND DIFFUSE LIGHT COMPONENTS
383
where I(t′) is the temporal distribution of the scattered pulse measured experimentally.
The theoretical prediction of the average arrival time can be obtained exactly from
Eq. (9.9) as
̄t = 1
2
z(z + 4z0)
vlt
∼z2
2vlt
.
(9.10a)
The average length ⟨z⟩traveled by diffusive light is
⟨z⟩= z2
2lt
.
(9.10b)
The average time of photon arrival ̄t as a function of z∕lt predicted by Eq. (9.10) is
plotted by a solid curve in Figure 9.13. The average time of arrival of the experimen-
tally measured photons are computed from Eq. (9.9). The results from random media
of beads with diameters d = 0.296, 0.46, 1.09, 3.134, and 11.9 μm in Figure 9.13 are
represented by plots of stars, triangles, circles, squares, and pluses, respectively. The
dotted curve is obtained by multiplying Eq. (9.10) by 0.9 to indicate where the exper-
imental data deviate by more than 10%. The data displayed in Figure 9.13 clearly
demonstrate that the average time of arrival deviates by 10% when z∕lt ≈10. The
deviation increases as z∕lt is reduced further below 10 and many more photons arrive
earlier than predicted by diffusion theory. Beads of larger diameter will increase
the amount of light scattered in the forward direction. This anisotropic scattering
2
20
100
1000
Average time (ps)
10
.296
Diameter (μm)
.46
1.09
3.134
11.9
50
Z / lt
FIGURE 9.13
Plot of average time of arrival ̄t versus z∕lt. The solid curve is the ̄t predicted
by diffusion theory, and the dotted curve is 0.9̄t. Experimental results (corresponding bead
diameter) are plotted by stars (0.296 μm), triangles (0.46 μm), circles (1.09 μm), squares
(3.134 μm), and plusses (11.9 μm) [86].

384
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
will result in reducing the average arrival time of the photons, which is apparent in
Figure 9.13, when z∕lt is less than 5. For a given value of z∕lt (<5), the largest bead
diameter (11.9 μm) medium (plusses) shows the smallest average arrival time, while
the smallest bead diameter (1.09 μm) medium (circles) shows the largest average
arrival time, and the intermediate bead diameter (3.134 μm) medium (squares) shows
that its average arrival time falls between that for the 11.9 and 1.09 μm beads. So, dif-
fusion approximation breaks down for source-to-detection distances z when z∕lt > 5.
A better theory is needed for describing light transport in scattering media in the
regime where the diffusion approximation breaks down. Cai et al. have developed
cumulant solutions of the radiative transport equation, which are significant improve-
ments [98,99], and Ntziachristos et al. have demonstrated an approach namely early
photon tomography to use the ballistic photons for imaging objects embedded inside
turbid media such as tissue [100].
9.4
PHOTON-SORTING GATES
Over the years, several schemes have evolved to sort out the direct image-bearing
ballistic and snake photons from the multiple-scattered diffuse photons [3]. These
schemes exploit one or more of the changes that scattering induces on the characteris-
tics such as directionality, polarization, absorption, coherence, and temporal duration
of the incident light. Since the image-bearing photons change the least, the idea is
to devise a gate that let the photon with a specific initial property through, but will
block others.
A time gate capitalizes on the fact that image-bearing photons emerge from the
scattering medium sample sooner than the diffusive photons. To realize a time gate
in practice, one needs a shutter that will open for a short duration, typically few
picoseconds, to let the early photons through and then close in time to leave out the
delayed scattered photons.
A space gate exploits the fact that the ballistic and snake photons come out from
the tissue in the incident direction, while the multiple-scattered light emerges in all
directions. So, a small aperture centered on the direction of incidence and placed
after the sample will collect the ballistic and snake light and effectively discriminate
against a significant fraction of multiple scattered light.
Similar gating schemes based on polarization and coherence of light have also
been devised. Often a single gate may not be discriminating enough. A space gate
cannot filter out the photons that first been scattered out of the incident direction and
then back into it. A time gate employed after the space gate can cut out such “online”
scattered photons, since scattering makes them travel longer distances and emerge
out of the sample later than the ballistic photons that propagate straight through.
The implementation of an optical imaging modality based on any of the gating
methods mentioned above requires a laser and beam delivery optics to illuminate
the sample, a detection scheme to monitor the transmitted light, and a signal pro-
cessing unit to construct the image from the collected data and known experimental
parameters.

PHOTON-SORTING GATES
385
The use of absorption also helps cut out the diffusive light in favor of ballistic and
snake components due to longer distance ⟨z⟩= z2
2lt traveled by diffusive photon over
ballistic distance z. We present below the experimental arrangements for realization
of some of these gates, and images of targets in turbid media obtained using these
gating techniques.
9.4.1
Time Gate
Time-resolved techniques were among the first methods used to separate out the
ballistic and snake components from the diffusive component [52–58,62,101–103].
Two of the earlier developed and successfully used methods to select temporally the
ballistic and snake components over the diffusion part are optical Kerr Gate (OKG)
[52–54] and Streak Camera [58].
Ballistic 2D images were first demonstrated in 1991 using a two-dimensional (2D)
picosecond optical Kerr gate imaging system [52]. Spatially resolved phantom light
point source (200 μm in diameter) and phantom test bar chart with bars of 100 μm
wide were imaged in highly scattering medium including a 3.5-mm-thick sample of
human breast tissue, a 3-mm-thick chicken breast tissue, and a 5-cm-thick water cell
with suspended polystyrene balls. The experimental setup of the picosecond Kerr gate
imaging system consisted of a mode-locked Nd:glass laser, a CS2 Kerr shutter, and a
2D readout system. The laser pulse had a duration of 8 ps, pulse energy of ∼4 mJ, a
peak power of 5 × 108 W, and a wavelength of 1054 nm. The 1054-nm laser beam was
sent through a potassium dihydrogen phosphate (KDP) crystal to produce the second
harmonic component with a wavelength of 527 nm and a peak power of about 107 W.
The 527-nm beam was used to probe the hidden object. The typical transmission
efficiency of CS2 Kerr shutter was ∼10%. The image was recorded by an intensified
2D charge-coupled device (CCD) camera with 640 × 480 pixels, a gain of 6000, and
a dynamic range of 100:1. Its minimum detection level is ∼1 lux cm−2. A personal
computer (PC), a frame grabber, a software package, and a video printer were used
to store, process, and display the images. All photographs shown in Figure 9.14 were
obtained from a single laser shot, which consisted of a laser train with ∼100 pulses
each separated by ∼10 ns.
Objects hidden behind various scattering walls, including human breast tissue,
chicken breast tissue, and a model system (polystyrene spheres suspended in a
water cell), were detected with this system. The picosecond Kerr gate was capa-
ble of separating the ballistic image of a point source in the scattering medium from
the diffusive noise. In one of the experiments, the scattering medium consisted of
polystyrene spheres of diameter ∼0.46 μm suspended in water in an optical cell with
the dimensions 5 × 5 × 5 cm3. The volume density of the spheres was 0.88% for
all measurements except in the double-quasi-point source measurement for which
the density was 0.3%. The calculated scattering coefficient was N𝜎L ∼21.7 for a
single point source and bar chart and N𝜎L ∼7.4 for the double quasi-point source
fluorescence imaging test, where N is the number density of the scattering particles,
𝜎is the scattering cross-section, and L is the thickness of the sample. When a breast

386
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
(a)
(b)
1
2
3
4
1
2
3
4
FIGURE 9.14
Two-dimensional images of the test bar chart behind (a) 3.5-mm-thick human
breast tissue and (b) 3-mm-thick chicken breast tissue illuminated by 8-ps, 530-nm laser pulses.
The test object is a five-line pairs per millimeter target. Dark bars are the object, and the white
area is the transparent background. The width of each bar is ∼100 μm. (1) Reference image
(tissue removed). (2) No time gate (standard transillumination). (3) tD = 0, time gate at zero
delay time. (4) tD = 22 ps, time gate at 22-ps delay time. A 200-μm spatial resolution has been
obtained in both photographs of (3) from the time gated ballistic imaging. The image contrast
is poor at a later delayed gate time as shown in (4). Without the time gate in (2), the image
was totally blurred. Because of the high fiber content of chicken breast tissue, the resolution
obtained from the time-gated ballistic imaging in (B3) is poorer than that from (A3). Part of
the nonuniformity of the image at a time segment can be accounted for by the laser beam
nonuniformity, the sample refractive index variation, and internal structures. Adapted from
Reference 52.
tissue was used as a scattering medium, its total attenuation coefficient, 𝜇a + 𝜇s could
vary from ∼2.5 to 600 cm−1, where 𝜇a is the absorption coefficient and 𝜇s is the
scattering coefficient [101,104–106].
To produce a single quasi-point source, a 527-nm laser beam was focused onto
a ground glass plate placed inside the scattering medium. The diameter of the focal
beam spot was ∼200 μm. To produce a double quasi-point source, the 527-nm beam
was split into two beams and focused onto a 200-μm-thick polydiacetylene (PDA)
film placed inside the scattering medium to form a fluorescence double point source.
PDA fluoresces from 550 to 850 nm with fluorescence lifetime of ∼12 ps [107].
The fluorescence light from the double-point source separated by ∼400 μm was
resolved by the picosecond Kerr gate, but could not be resolved by the steady state
transillumination imaging. For the steady-state image, the double-point image merged
into strong noise wings. The diameter of each fluorescence point source was ∼200 μm.
PDA film situated inside turbid water (N𝜎L = 7.4) was illuminated by two 527-nm

PHOTON-SORTING GATES
387
laser pulses. The resolved image of the two-point sources separated by 400 μm was
formed by the ballistic part of the signal light.
A sequence of measured time-dependent 2D Kerr-gated images of a bar chart
behind 3.5-mm-thick human breast tissue and 3-mm-thick chicken breast tissue are
displayed in Figures 9.14a and b, respectively. The bars of the test chart (five line pairs
per millimeter, corresponding to ∼100 μm spatial resolution) were illuminated by
8 ps, 530-nm laser pulses. In Figure 9.14, a1 and b1 represent reference photographs,
which display the image of the test chart without the scattering wall. Figure 9.14, a2
and b2 represent images obtained from standard transillumination (no time gate) of
the chart behind these two tissue samples; no clear image can be observed in either of
these cases. Figure 9.14, a3 and b3 show the Kerr gate bar images of the ballistic and
snake signals at a gating time of t = 0 (time resolution 𝛿t ∼10 ps). Clear bar images
with dimension of ∼0.1 mm (separated by 0.2 mm) can be resolved in human breast
tissue as shown in Figure 9.14, a3. As the gating time was delayed by 22 ps, the
collected images were gradually broadened and blurred as shown in Figure 9.14, a4
and b4. These results demonstrate that the 2D Kerr imaging system as opposed to 1D
time-gated imaging system can be used to image in vitro human organs and tissues.
It is estimated that 2D low-level signal images with an attenuation factor of e−30
through a scattering wall can be obtained. It is expected that it will soon be possible
to detect highly resolved images of ultra small (less than a few millimeters) objects
embedded inside thick biological and medical samples with the use of a high repetition
femtosecond multiple Kerr gate system.
Since breasts do not have bars inside, as a more realistic approximation, a droplet
of water or suspension of Intralipid-10% of different concentrations were used as
targets. The scattering medium was water suspension of Intralipid-10% that had a
different concentration than that of the target. To demonstrate the capability of Kerr
gate, time-resolved images of translucent droplets in high scattering turbid media
were obtained. A Fourier spatial filtering technique combined with Kerr time gating
system improved the dynamic range and signal-to-noise ratio [108–110].
A schematic of the sample phantom and host cell arrangement is shown in Fig-
ure 9.15. Diluted Intralipid-10% suspension of various concentrations and pure water
were used for the phantom droplets in a 2% diluted Intralipid host turbid cell. The
inside dimension of the host cell was 50 × 50 × 50 mm3. The phantom droplets were
generated in a 50-mL burette with a straight bore stopcock with a polytetrafluo-
roethylene plug. The subdivision or the limit of error of this burette is 0.1 mL. For a
2% diluted Intralipid stock solution of 10% (the final solution was 0.2%), the mea-
sured attenuation coefficient [108] was 0.38 mm−1. The absolute signal collected
was ∼10−10 of the input probe beam. The absorption length of the modeled Intralipid
suspension is on the order of 500 mm and can be neglected in the experimental
arrangements. Two measured time-resolved KF two-dimensional images of water
droplets in the middle of a 50-mm-thick host cell are shown in Figures 9.16a and b.
Because the scattering loss from the water droplet is less than that from the surround-
ing host, the intensity of the projected image of the water droplet was higher than
that of the host. The shape of these water droplets depends on the amount of water
released. The delay time between the stopper opening and the photographic time was

388
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
TS
SC
TM
(a)
(b)
Laser
pulse
Laser
pulse
Sample holder
Sample cell
Cat slice
t = 0
t = 20 ps
t = 26 ps
t = 40 ps
Turbid host medium
(2% intralipid solution)
5 cm thick
FIGURE 9.15
Experimental arrangement showing turbid sample cell and translucent
intralipid phantom droplets (left side). The cut from the side section of the cell displays
pictorially the droplet releasing inside the host medium. SC, sample cell with inside dimen-
sions of 50 mm × 50 mm × 50 mm; TM, 2% dilution of a 10% stock Intralipid suspension for
the host turbid medium; TS, translucent phantom sample, x% diluted intralipid suspension in
a burette (50 × 50 × 50 mm3 tank). The right side shows the Kerr gate time-resolved images of
an object (a CAT slice) embedded inside a turbid medium (5-cm-thick 2% intralipid solution).
The images taken at early times of 0 and 20 ps clearly show the existence of the object even
inside a thick scattering medium, which can not been seen visually [54].
varied between ∼2 and 10 s. The inhomogeneity of the brighter circle in Figure 9.16
that corresponds to the collection aperture or signal beam diameter of ∼12.7 mm was
accounted for by the nonuniformity of the laser intensity distribution.
To determine the contrast of the transmitted early snake light images as shown in
Figure 9.17, four different phantom droplets with 1, 2, 3, and 5% diluted Intralipid
suspensions were dropped into a 25–mm-thick host medium. The projected image
from the 5% Intralipid droplet shown in Figure 9.17d is the darkest because of the
increased scattering from the droplet, while the projected image from the 2% Intralipid
droplet shown in Figure 9.17b is hardly distinguishable from the surrounding host
with the identical scattering property. The image obtained from the 1% Intralipid

(a)
(b)
FIGURE 9.16
Early time two-dimensional shadow images of water phantom droplets in a
host 2% 50-mm-thick diluted Intralipid stock solution. The spatial distribution of the water
drop depends on the amount of water and the delay of release time. The dark shadow at the
top middle part of the white laser beam circle is the end tip of burette: (a) delay of ∼2 s from
release of the stopper and (b) delay of ∼10 s from release of the stopper [54].
(a)
(b)
(c)
(d)
FIGURE 9.17
Early time images of translucent Intralipid phantom drops of various concen-
trations in a host 2% 50-mm-thick diluted Intralipid stock solution. The images show diluted
Intralipid phantom drops at (a) 1%, (b) 2%, (c) 3%, and (d) 5% concentration [54].

390
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
M
M
M
BS
M
Breast
L
L
L
P
P
M
Breast holder
Pump beam
530 nm
Probe beam
1060 nm
Kerr cell
CCD
camera
Computer
Motor controller
SHG
1060 nm
1060 nm
530 nm
Delay
FIGURE 9.18
Schematic diagram of a picosecond optical Kerr gate.
droplet (Fig. 9.17a) is brighter and the image from the 3% Intralipid droplet shown
in Fig. 9.17c is darker than that from the 2% surrounding host. A 1% concentration
dilution difference between the phantom droplet and the host medium can be distin-
guished visually. The changes of the shape of the drop can be measured. A much
smaller concentration difference of 0.1% between the phantom droplets and the host
medium could be identified with digital video signal processing [108]. This work
demonstrates that small differences in scattering properties and the shapes of small
dimension translucent droplets can be spatially determined inside a large host turbid
medium that may be useful in moving toward optical mammography.
The optical Kerr gate (OKG) [52–54] acts as a camera with an ultrafast shutter that
is triggered by an intense gating pulse to take a picture at the instant when the image-
bearing light emerges and closes soon enough to block the diffusive light. A typical
OKG experimental arrangement, shown schematically in Figure 9.18, consists of a
Kerr active material such as carbon disulfide (CS2) placed between two crossed polar-
izers. An intense ultrashort pulse of light is split into two parts. One part illuminates
the sample, while the other part—the reference or gating pulse—induces a transient
birefringence in the Kerr medium. The light emerging from the sample is made to
overlap spatially and temporally with the gating pulse inside the Kerr medium. If the
reference pulse is blocked, a very small fraction of the incident light, typically less
than one part in a million, may pass through the crossed polarizers. In this case, the
gate is “closed.” In the presence of the reference pulse, the fraction of the light from
the sample that is linearly polarized by the first polarizer becomes elliptically polar-
ized as it passes through the Kerr medium while coincident with the reference pulse.
A fraction of this elliptically polarized light can now pass through the analyzer. This

PHOTON-SORTING GATES
391
may be looked on as the gate being “open.” Thus, the light can only pass through the
gate when it overlaps both temporally and spatially with the intense reference pulse.
The duration of the gating pulse or the recovery time of the Kerr medium,
whichever is longer, determines how long the gate remains open. Picosecond and
subpicosecond time resolution may be attained using Kerr NL Bi glass plate. The gate
position in time may be varied by adjusting the arrival of the reference pulse by use of
an optical delay line. The image bearing early light may be extracted in a single shot
by adjusting the gate position.
As detailed above, a picosecond OKG that uses a picosecond Nd3+:glass laser, a
CS2 Kerr cell, and a two-dimensional charge-coupled device (CCD) camera has been
successfully used to image test objects hidden inside human breast tissue, chicken
breast tissue, and a suspension of polystyrene spheres in water. Combined time and
space gating (to be discussed in Section 9.4.3) was realized by placing Kerr cell in
the arrangement discussed above at the back focal plane of a lens, and an aperture at
the center of the front focal plane [110]. Another lens placed at a distance equal to
its focal length behind the aperture was used to collimate, select, and direct light to
the CCD camera.
Compared to simple OKG, this Kerr-Fourier gate (KFG) provides higher dynamic
range, signal-to-noise ratio, and contrast at a signal level of ∼10−10 of illumination
intensity. The sensitivity of the technique to detect small changes in optical properties
was underscored when it successfully imaged a pure water droplet inside 5 cm long 2%
intralipid suspension as shown in Figure 9.19. The photo image of CAT inside the 5 cm
tank of Intralipid suspension is shown in Figure 9.19f. In related development, the
feasibility of constructing a three dimensional image was demonstrated by combining
two dimensional shadow grams formed with a KFG using a back-projection algorithm
(inverse image reconstruction) on a personal computer [72].
In addition to using Kerr gate and streak camera, the time gating in the picosecond
and nanosecond domain have been accomplished using gated intensified CCD cam-
eras and has been used for research on detection of targets in turbid media including
biological tissues [75,111–120].
9.4.2
Absorption Gate
To improve ability to see objects in a scattering medium a simple novel approach
of using absorption present in scattering media was proposed [121]. One thinks
absorption hurts imaging; however, for scattering medium it helps due to larger
distance traveled by diffusive light. The absorption may reduce the intensity of
multiple scattered light (noise) below the ballistic signal (image) intensity. This
reduction in noise-to-signal ratio allows to see through random scattering media that
would otherwise be opaque in the absence of absorption.
The average distance ̄z that multiple scattered light travels in the medium is z2∕2lt
(see Eq. 9.10b), which is longer than z, the distance that the ballistic light traverses
through a slab of thickness z. Light traveling over a longer path length has a higher
probability of being absorbed. Thus, the introduction of absorbing dye into a random
medium will cause it to absorb the multiple scattered light more than the ballistic

392
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 9.19
Images of water droplet diffusing into a 2% intralipid suspension (upper
frame), and a CAT drawn in a transparent plastic sheet hidden inside (lower frames) a 5 cm
thick intralipid solution using the optical Kerr-Fourier gate. In a-c, gate delay is 0 ps, and the
times after the valve that releases the water droplet is open are 0 s (a), 3 s (b), and 5 s (c),
respectively. In (d) gate delay is 40 ps, (e) – 20 ps, and (f) – 0 ps [54].
light. The reduction of noise over signal with the use of absorption was investigated
by using time- and angle-resolved ultrafast laser pulse measurements. When a plane
ultrafast pulse is normally incident upon a slab of random medium with absorbing
boundaries, the temporal profile of the scattered pulse exiting from a point on the
opposite side of the slab predicted by the diffusion theory is
Iz(t) = D
𝜋z2
∑∞
m=1 m
(𝜋z
d
)2
sin
(m𝜋z
d
)
× exp
[
−Dt
(m𝜋
d
)2]
exp
(
−vt
la
)
,
(9.11)
where D = vlt∕3 is the diffusion coefficient, d = z + 2z0, z0 = 0.71lt, 𝜈is the speed
of the photon, z is the thickness of the slab, lt is the transport mean free path, and la
is the absorption length.
The intensities of the diffuse and ballistic light are computed and plotted in
Figure 9.20 by solid and dashed lines, respectively. The squares and the triangles show
experimental data. This figure illustrates a case for scattering medium consisting of
0.3% concentration latex beads of 0.296 μm diameter suspended in water. lt = 1.0 mm

PHOTON-SORTING GATES
393
102
Absorption length (mm)
101
10–8
10–7
10–6
10–5
Transmitted intensity
10–4
Diffusive
Ballistic
10–3
10–2
103
FIGURE 9.20
Theoretical prediction and experimental results of the total diffuse light and
the signal intensity (in arbitrary units) for light transmitted through a random medium with
different absorption lengths, using z = 10 mm, ls = 0.3 mm, and lt = 1.0 mm. The solid and
dashed curves correspond to the ballistic and diffuse light intensity, respectively. The squares
and triangles correspond to the measured ballistic and diffuse light intensity, respectively [121].
is found experimentally and ls = 1∕n𝜎s = 0.3 mm, where N = 2.2 × 1017 m−3 is the
scatterer density and 𝜎s = 1.54 × 10−14 m2 is the scattering cross-section computed
numerically from the Mie theory. The thickness of the slab of random medium is
z = 10 mm, and the absorption length is varied from 1000 mm to 1 mm. The intensity
of diffuse light is much higher than the intensity of ballistic light when the medium
is less absorbing, for example, when la = 1000 mm, the diffuse intensity is 130 times
stronger than that of the ballistic signal. The solid angle of collection Ω for the diffuse
light is taken to be 1 × 10−7. As the absorption in the medium is increased, the diffuse
intensity decreases faster than the ballistic intensity. The diffuse intensity is decreased
below the ballistic intensity when la is less than 2 mm. This reduction of diffuse light
with respect to ballistic signal will substantially improve the quality of the image of
an object hidden in a highly scattering random medium [121].
The intensities of the diffuse and ballistic signal presented in Figure 9.20 were
measured by using an angle- and time-resolved technique that was described in Ref-
erence 86. In this method a beam of ultrafast laser pulses of 100 fs duration (repetition
rate 82 MHz, average power 15 mW, wavelength centered at 620 nm, beam diameter
10 mm) from a colliding pulse mode-locked dye laser system was incident upon a slab
of random medium. This random medium consisted of a 0.3% concentration of latex
beads of 0.296 μm diameter suspended in water in a 10-mm-thick cylindrical glass
cell with a diameter of 50 mm. A black pinhole of 2 mm diameter was placed at the
center of the opposite side of the cell. The temporal distribution of the photons from
this pinhole within 3 mrad in the forward direction was measured by a synchro-scan

394
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
(a) Ia = 550 mm × 1
(b) Ia = 13.5 mm × 5.4
(c) Ia = 3.4 mm × 95.2
(d) Ia = 1.83 mm × 990
0
300
600
Time (ps)
Normalized intensity
900
FIGURE 9.21
Transmitted pulse profiles through a slab of random medium 10 mm thick
with a 0.3% concentration of latex beads of 0.296 μm diameter at different absorbing dye
concentrations. The absorption lengths of these media are indicated on the curves [121].
streak camera. The temporal profiles of ballistic signals and diffuse light pulses in
the forward direction for different amounts of absorbing dye added in the random
medium are displayed in Figure 9.21. The narrow peak at time 44 ps corresponds to
the ballistic signal that travels in the incident direction and arrives first. In contrast,
the diffuse light pulse that consists of photons that had undergone multiple scattering
traversed many different trajectories and arrived at different times which resulted in
a broad temporal profile. Figure 9.21a presents the temporal profile of transmitted
light when no dye was added. The tiny ballistic peak at 44 ps is much smaller than
the large and broad temporal distribution of the diffuse pulse. The total intensity of
the diffuse light is found to be 130 times greater than the intensity of the ballistic
signal. It is impossible to see through such a cloudy medium. To study how the diffuse
light intensity may decrease with respect to the ballistic signal, increasing amount of
absorbing dye (Malachite Green) was added to the random medium. As the temporal
profiles in Figures 9.21b–9.21d show both the ballistic and diffuse signal intensities
decreased when the absorptive dye concentration increased, but the intensity of the
diffuse light decreased more, enhancing the relative intensity of the ballistic signal to
the diffuse signal.
A demonstration that an object hidden behind a scattering wall can in fact be
imaged with the aid of absorption [121] is shown in Figure 9.22. A cross of 3 mm
and 2 mm wide bars serves as an object and the scattering medium is placed between
the object and the imaging video camera. The image of the object with the interfer-
ence pattern which arises from light diffraction at the edge of the bar is shown in
Figure 9.22a when there were no scatterers in the sample cell. This image disappears
when the medium behind the cross is made scattering (Fig. 9.22b), reappears when

PHOTON-SORTING GATES
395
Laser beam
Object
Random medium
Image
Streak camera
Video
Computer
Imaging setup
Images
(a)
(b)
(c)
(d)
FIGURE 9.22
Imaging setup (top) and photographic images of a cross obtained with (a) no
scatterer in the cell; (b) scatterers are added into the cell where the thickness is 20 scattering
mean free path, the laser wavelength is 620 nm; (c) malachite green dye is added to the scattering
medium in the cell with a resulting absorption length of 4.6 mm; (d) the laser is tuned to 515 nm
where the absorption length of the scattering medium increased to 75 mm [121].
the scattering medium is made absorbing (Fig. 9.22c), and disappears again when
the laser wavelength is tuned away from the absorbing spectral band of the medium
(Fig. 9.22d). So, absorption can help reduce the effects of scattering. It can be used
for coding information as light passes through fog or clouds by selecting the light
wavelength in the water absorption range from 800 to 1100 nm (Fig. 9.8) and improve
images of tumor growing in breast. Water absorption in breast tissue can clear up
images [122].
9.4.3
Fourier Space Gate and Microscopic Imaging
Another simple way to improve imaging of hidden microscale structures and features
is to use changes in spatial frequencies of light wave passing through scattering media.
Ordinary microscopes cannot obtain a clear image of objects hidden inside or behind
a highly scattering random media. Scanning confocal and two photon microscopes
are standard methods for locating objects inside a medium at different depths [123]
and have been used to acquire image information of objects in highly scattering media
with limited success. Scanning confocal microscopes combined with dye staining and
multiple (2 and 3) photon nonlinear optical excitation are other methods to image
objects inside scattering media with additional improvement [124,125].

396
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
According to modern optical theory, the spatial Fourier Transform of the image
is located on the back focal plane of a lens [126]. Image information of a reasonable
level of resolution is carried by the low spatial frequency light that is located at
the center of the focal plane. The higher spatial frequency light, which carries the
detailed diffraction pattern of the object, is located away from the central region of
the focal plane. One can obtain an image of an object by using the low frequency
light alone, but one obtains somewhat less diffraction detail. When objects are hidden
inside or behind a highly scattering host medium, a substantial fraction of the direct
image-bearing light is multiply scattered in all directions, thereby emerging from
the host scattering medium with a wide range of spatial frequencies. Thus images
are degraded by the high spatial frequencies that arise from multiply scattered light.
Indeed, an image of the object cannot be obtained at all with the conventional method
when there is strong enough scattering.
It has been demonstrated that by incorporation of a spatial filter into a microscope,
images of microscopic objects hidden behind a slab of highly scattering media can
be obtained [127].
The space-gate experimental setup is shown in Figure 9.23 [127]. A 4 mm
diameter beam of ultrashort laser pulses of 100 fs duration at a wavelength of 625 nm
was generated by a colliding pulse mode-locked (CPM) laser. The pulses were split
by a glass slide, yielding a probe and a reference pulse. The reference pulse did not
travel through the microscope but rejoined the probe beam slightly ahead of it in
time and was used to indicate zero time in the time-resolved detection mode. The
probe beam was incident upon the preparation consisting of a test bar chart (pattern
4, corresponding to a spacing of 176.7 μm per opaque or transparent bar) in front of
a slab of a scattering medium.
A conventional Reichert microscope with a 4X objective was used in the trans-
mission mode. As the transmitted light traversed the microscope body tube, it was
spatially filtered by a small circular aperture located at the back focal plane of the
objective lens and directed through the microscope trinocular. The image was detected
Modified microscope
Glass slides
Lens
Focal
plane
Objective
lens
Scattering
media
Reference pulse
Computer/Controller
Streak
camera
CCD
CPM
Laser
Glass slides
Aperture
f
Bar chart
100 fs
FIGURE 9.23
Schematic diagram of an experimental arrangement for space-gated micro-
scopic imaging.

PHOTON-SORTING GATES
397
(a)
(b)
(c)
(d)
353.4 μm
FIGURE 9.24
Images of the bar chart hidden behind a 2-mm-thick 1.5% intralipid scattering
medium obtained in the steady-state mode with (a) no spatial filter and (b) 3 mm, (c) 1 mm,
and (d) 0.5 mm diameter spatial filters [127].
by a streak camera with a CCD detector operated in either the steady-state focus mode
or the time-resolved streak mode.
Figure 9.24 shows the effect of spatial filtering on image quality for the test bar
chart with a z = 2.0-mm-thick scattering medium placed on top of the chart. The
images were obtained in the steady-state mode. The random scattering medium was
an Intralipid suspension of 1.5% volume concentration contained in a glass cell. The
scattering mean free path of the suspension was determined by another experiment to
be 0.15 mm. Therefore the optical thickness z∕ls was ∼13 in this case. Figure 9.24a
shows the image obtained without spatial filtering. No image of the bars is seen.
A uniform bright background is seen, showing that the intensity of noisy multiply
scattered light is stronger than that of the signal light. Figure 9.24b was recorded with
a 3.0-mm spatial filter at the Fourier plane of the micro-objective lens. A distinct
image of the bars is readily discerned although there is a significant amount of
scattering-induced noise. For Figure 9.24c, a 1-mm aperture was used. The image
now is even clearer, with less noise than that of the image shown in Figure 9.24b.
Figure 9.24d was taken with a 0.5-mm aperture at the Fourier plane. The noise has
almost disappeared. Note that without filtering, the bars are not visible at all; however

398
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
even with a 3.0 mm aperture, they are readily apparent. The image quality improves
with the smaller apertures since the background multiply scattered noise light is
diminished, thus improving the signal-to-noise ratio considerably.
It was demonstrated that an improvement in the microscope image quality of
objects hidden behind a highly scattering medium can be achieved by using both
spatial gating and time-gated detection. This technique can be used with different
kinds of light sources and different kinds of time-resolved methods for a variety of
biological and medical applications.
The Fourier space gate really acts as a time gate for light from scattering media
[128]. The ballistic and snake light correspond to zero and lower-order spatial frequen-
cies, respectively; while the higher spatial frequency components represent diffuse
light. The 4f Fourier space gate acts as an intrinsic equivalent time gate which is
accomplished by adjusting the size of the aperture to select the ballistic and earlier
fraction of snake light, and rejecting the high spatial frequency diffusive light. An
aperture at focal plane of a lens in 4F passes spatial frequencies that are smaller than
the cutoff frequency: ̄𝛼= D∕2f𝜆, where D is the aperture size. Consider an illumina-
tion with 𝜆= 625 nm light and objective lens of focal length f = 10 mm, an aperture
with diameter of 0.5 mm will filter out all spatial frequencies higher than 40 lines
per mm, thus cleaning up the noise from scattering. The smallest size distinguishable
will be 25 μm features of the object in a scattering medium [128].
In the Fourier space gate arrangement, the ballistic and snake components appear
near the central dc signal frequency region, whereas the diffusive component appears
off the central region of the Fourier spectral plane. A spatial filter placed at the Fourier
spectral plane can reduce the diffusive component and allow most of the ballistic
and snake components to pass through. By adjusting the aperture size at the Fourier
plane, a spatial Fourier filter may be made to act as variable temporal gate for light
emanating from a scattering medium.
Figure 9.25 displays the normalized temporal profiles of the scattered 530 nm
wavelength light pulses passed through spatial Fourier filters with different aperture
sizes for thin and thick media. Figure 9.25 shows the results for a 50-mm cell with
0.4% Intralipid suspension. With a 10-mm diameter spatial filter, the measured pulse
profile exhibited a fast rising edge and a slow decaying tail. The FWHM of the
temporal profile was approximately 65 ps. The width was reduced to 37 ps for a
3-mm diameter spatial filter and to 21 ps for a 1-mm diameter filter. The pulse
narrowing by the spatial 4f Fourier filter for all three samples is summarized in
Figure 9.26. The decrease of the pulse duration with the decrease in aperture diameter
demonstrates that Fourier spatial filters act as temporal gates for the early light in the
scattered light pulse.
9.4.4
Polarization Gate
Thepolarizationgatemakesuseofthefactthatscatteringeventsdepolarizeanincident
beam of linearly polarized light so that ballistic and snake photons retain much of
the initial polarization while the multiple scattered photons are depolarized [48–50].
In practice, a polarization gate is implemented by shining the object through a linear

PHOTON-SORTING GATES
399
Intensity
1.2
1
0.8
0.6
0.4
0.2
0–50
0
50
1 mm
3 mm
ϕ = 10 mm
(a)
100
150
200
250
300
1.2
1
0.8
0.6
0.4
0.2
0
1 mm
3 mm
ϕ = 10 mm
(b)
200
250
300
350
Time (ps)
400
450
500
550
FIGURE 9.25
Temporal profiles of the spatially filtered laser pulses scattered by 0.4%
intralipid suspension in a 10- and 50-mm cell. The solid, dashed, and dotted-dashed curves
show the temporal profiles of scattered pulses when the diameter of the spatial Fourier filter is
1, 3, and 10 mm, respectively [128].
polarizer and collecting the emerging light through a second linear polarizer. The
degree of polarization [defined as (Ip −Is)∕(Ip + Is), where Ip and Is are transmitted
intensities of light with the axis of the second polarizer parallel and perpendicular,
respectively, to that of the first polarizer used to obtain linearly polarized interrogating
light] of the transmitted light is used to select the image-bearing component, since it
is expected ideally to be unity for ballistic light and zero for completely depolarized
diffuse light. When a pulse of initially polarized light is used to illuminate a turbid
medium, such as human tissue, ice or snow, the ballistic and snake-like components of
the light, which are either backscattered directly from the surface of the turbid media
or take direct paths through the turbid media, substantially maintain the polarization
of the initially polarized light, while the diffuse component of the light, which
tends to travel longer, less direct paths through the turbid media before either being

400
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
0
0
10
20
30
40
50
60
70
Pulse duration (FWHM) (ps)
Aperture diameter (mm)
2
4
6
8
10
12
FIGURE 9.26
Changes of the pulse duration of the scattered pulse as a function of aperture
diameter for the same concentration-length product: 2-mm cell with 10% intralipid suspension
(circles), 10-mm cell with 2% Intralipid suspension (triangles), and 50-mm cell with 0.4%
Intralipid suspension (dots) [128].
backscattered from or emerging from the opposite end of the turbid medium, becomes
comparatively more depolarized than the ballistic and snake-like components.
Polarization-enabled high-resolution and high-speed optical imaging of objects
located in, at the surface of, or behind highly scattering media using polarized illumi-
nating light and polarization-difference imaging. The surface image information is
predominately carried by the parallel polarization image component while the perpen-
dicular image component contains beneath-the-surface image information. Images
of structures at different depths within a turbid medium can be obtained using the
perpendicular polarization component resulting from the use of different illuminating
wavelengths.
Figure 9.27 shows the polarization components of the backscattered light from
bovine gray matter brain tissue when linearly polarized 1064-nm, 6.5-ps pulses were
used. The diameter of the illuminating beam was 0.5 mm. The parallel polarization
component Ip(t) is more intense than the perpendicular polarization component Is(t).
In addition, when the temporal profile of the perpendicular component was normal-
ized to the peak intensity of the parallel component Is,normal(t), the temporal profiles
were different, having different peak time positions and full-width-at-half-maximum
(FWHM). The scattering parameters of gray matter allow for clear observation of
the above-mentioned differences in the two backscattering polarization components
within the temporal resolution of our system (about 15 ps).
The differences in the two polarization components arise from the fact that the
backscattered photons that are perpendicularly polarized must first undergo sufficient
scattering events to lose their polarization information. By contrast, the photons that
are directly backscattered from the surface of the tissue have less of an opportunity

PHOTON-SORTING GATES
401
Retrorefractive geometry
Gray matter
Bovine brain tissue
   = 1064 nm
−50
0
50
Time (ps)
I(t)//
I(t)⊥
INorm(t)⊥
Intensity (a.u.)
100
150
200
λ
FIGURE 9.27
Temporal profiles of the parallel polarization component, perpendicular polar-
ization component, and normalized perpendicular polarization component of backscattered
light obtained from bovine gray matter brain tissue illuminated with 1064-nm, 6.5-ps laser
pulses [49].
to depolarize; therefore, the backscattering photons from the surface of the tissue and
initial layers beneath the surface belong mostly to the parallel polarization image com-
ponent. The perpendicular polarization component contains predominately photons
that penetrated the tissue to certain depths before they emerged in the backscattering
direction. This salient concept explains the differences in the temporal profiles of the
two polarization components shown in Figure 9.27. The parallel component contains
all photons backscattered from the surface of the tissue before they depolarize. As a
result, the parallel component is more intense and the time of peak intensity is when
the backscattered light from the surface arrives. The perpendicular component time
of peak intensity arises from the depolarized photons that propagated in the tissue
to a certain depth and then backscattered. In the backscattering geometry discussed
above, the image information of the surface and near-surface of tissues is contained
predominately in the parallel polarization image component while the perpendicular
polarization component contains information of that portion of the tissue well beneath
the surface.
Optical polarization imaging has been implemented in the transmission geometry
as well. Since the ballistic and snake photons encounter fewer scattering events
than the diffusive photons, in transmission, the parallel polarized component (Ip)
carries more direct image information than the perpendicular polarized component
(Is). Hence, images recorded with Ip provide higher contrast than that recorded
without polarization selection, or with Is [50, 129]. Polarization difference imaging
that utilizes Ip −Is provides even higher contrast by removing the contribution from
partially depolarized light that results from Malus’ Law.
Polarization-gated imaging has been extended using circularly polarized light,
developing theoretical framework based on cumulant theory, and applied to different
types of turbid media [130–133].

402
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
Since cancerous and normal tissues show different scattering and depolarization
properties caused by their different morphological and molecular structures due to
the cancer development, the polarization imaging techniques have been widely used
for the detection of cancers in breast, prostate, and other organs with the use of either
native biomarkers or the cancer receptor targeting contrast agents [134–139].
9.5
TRANSITION FROM BALLISTIC TO DIFFUSE IMAGING
IN TURBID MEDIA
As we discussed above, there are three components called ballistic, snake, and dif-
fusive in optical pulse propagation in a highly scattering turbid medium. Three key
length parameters—scattering length, ls; transport mean free path, lt; and absorption
length, la (alternatively, the corresponding coefficients, 𝜇s, 𝜇′
s, and 𝜇a, respectively)—
are used to describe light propagation characteristics through the medium. Another
key parameter is the anisotropy factor, g defined as the ⟨cos 𝜃⟩, where 𝜃is the scat-
tering angle. The relationship between these parameters and the relations governing
the attenuation of the intensity of the ballistic and snake light as they propagate
through the medium have been presented in Section 9.2.2. Ballistic and snake light
may provide direct images of targets in turbid media, provided the deleterious effect
of diffusive light (noise for direct imaging) could be sufficiently reduced. Section 9.4
reviewed some of the techniques for reducing the diffusive light noise. However, for
sufficiently thick samples such as human breast or prostate the ballistic and snake
light intensity are reduced so much and the diffusive noise becomes so dominant that
direct imaging is not feasible. Diffusive light, that is multiple scattered within the
medium, carries a wealth of information about the interior structures of the medium.
Different approaches, referred to as inverse image reconstructions, that use knowl-
edge of incident light properties, measurement of light emergent from the medium at
the boundaries, average optical properties of the medium, an appropriate model for
light propagation through the medium, and numerical algorithm to estimate optical
properties of every voxel of the sample, are needed to extract the relevant information
about the embedded objects, such as tumors in a breast [4–6,44,76].
The diffuse light intensity ID described in diffusion approximation is
ID = I0 exp(−𝜇effz),
(9.12)
where I0 is the incident intensity, z is the sample thickness, and 𝜇eff is the effective
scattering coefficient:
𝜇eff =
√
3𝜇a(𝜇′
s + 𝜇a),
(9.13)
that is related to the diffusion coefficient D = c∕3𝜇a(𝜇′
s + 𝜇a).
Another area of interest is the transition regime between ballistic and diffuse light
transport because it carries valuable information about characteristics of scattering
[140,141]. Assuming quasi-one-dimensional propagation and a small detection solid

TRANSITION FROM BALLISTIC TO DIFFUSE IMAGING IN TURBID MEDIA
403
angle dΩ the detected light intensity was shown to be a sum of ballistic light intensity
and a fraction of diffuse light intensity [140]:
I∕I0 = exp(−(𝜇sz) + (𝛿Ω∕4𝜋) exp(−𝜇effz).
(9.14)
Equation (9.14) does not include a term for snake (quasi-ballistic) photons and
makes the assumption that absorption is negligible, 𝜇a ≪𝜇s, that is the medium is
diffusive (not absorptive). A transition depth, zc, is defined as
zc = (𝜇eff −𝜇s)−1 ln(dΩ∕4𝜋),
(9.15)
such that for z ≪zc the photons detected are mainly ballistic, and for z ≫zc most
photons are diffusive in nature.
Figure 9.28 shows the confocal signal as a function of the thickness (L) of the
scattering medium (0.48-μm latex spheres suspensions in water with ls = 74 μm)
obtained by Kempe et al. [141]. The ballistic regime characterized by an exponential
decay extends to Lmax (∼1 mm) at which ballistic and diffuse light intensities are
equal, followed by the regime with much slower decay of intensity where diffuse
light dominates.
In thin media the light intensity is governed by ballistic transport while for thick
media by diffusive transport. The dynamic range of detection system must be >105.
The use of diffusion equations for inverse problem requires the distance between
source and detector to be < 5lt.
0.0
10−10
10−8
10−6
10−4
10−2
100
0.5
1.0
1.5
NA = 0.10
NA = 0.015
L (mm)
Normalized signal
2.0
2.5
FIGURE 9.28
Semilogarithmic plot of the signal as a function of sample thickness for a
suspension with ls = 74 μm. Shown is the signal obtained with an NA of 0.10 for a pinhole
with vp = 1.24 and for an NA of 0.015 with vp = 1.16. The solid line is the fit to the part of the
curves that exhibits an exponential falloff [141].

404
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
9.6
CONCLUSION
The use of light and optical imaging approaches for biomedical applications is still at
the initial stages. Optical mammography is on the horizon using inverse reconstruction
on time domain with optical molecular fingerprints such as hypoxia as a marker for
cancer. With the advent of compact wavelength-agile lasers, sensitive detectors and
spectrometers, optical fiber beam delivery and signal acquisition systems, powerful
personal computers, and numerical algorithms, photonic approaches will play an
important role in combating diseases through early detection and diagnosis.
ACKNOWLEDGMENTS
We acknowledge IUSL researchers, G. E. Andersen, Feng Liu, K. M. Yoo, X. Ni,
W. Cai, J. Dolne, X. Liang, P. P. Ho, S. Demos, B. B. Das, Q. Z. Wang, and G. Zhang,
for their contribution to the work reviewed here, and acknowledge Y. Budansky for
his help on figure drawing. The reviewed research was supported in part by US Army
Medical Research and Materiel Command, NASA, and NYSTAR.
REFERENCES
[1] G. J. Muller, R. R. Alfano, S. R. Arridge, J. Beuthan, E. Gratton, M. Kaschke, B. R.
Masters, S. Svanberg, and P. van der Zeect, Medical Optical Tomography: Functional
Imaging and Monitoring, Vol. IS 11, SPIE Institute Series (SPIE, Bellingham, WA,
1993).
[2] A. Yodh and B. Chance, “Spectroscopy and imaging with diffusing light,” Phys. Today
48, 34–40 (1995).
[3] S. K. Gayen and R. R. Alfano, “Emerging optical biomedical imaging techniques,” Opt.
Photon. News 7(3), 17–22 (1996), and references therein.
[4] J. C. Hebden, S. R. Arridge, and D. T. Deply, “Optical imaging in medicine: I. Experi-
mental techniques,” Phys. Med. Biol. 42, 825–840 (1997).
[5] S. R. Arridge and J. C. Hebden, “Optical imaging in medicine: II. Modeling and recon-
struction,” Phys. Med. Biol. 42, 841–853 (1997).
[6] S. R. Arridge, “Optical tomography in medical imaging,” Inverse Probl. 15, R41–R93
(1999).
[7] M. R. Hamblin, and T. Hasan, “Photodynamic therapy: a new antimicrobial approach
to infectious disease,” Photochem. Photobiol. Sci. 3, 436–450 (2004).
[8] D. Huang, E. A. Swanson, C. P. Lin, J. S. Shuman, W. G. Stinson, W. Chang, M. R.
Hee, T. Flotte, K. Gregory, C. A. Puliafito, and J. G. Fujimoto, “Optical coherence
tomography,” Science 254, 1178–1181 (1991).
[9] W. Drexler and J. G. Fujimoto, Optical Coherence Tomography: Technology and Appli-
cations (Springer, 2008).
[10] R. R. Alfano, “Advances in optical imaging and photon migration,” OSA Topical Meeting
Proceedings, edited and chaired by R. R. Alfano, OSA Volume 21, March (1994).

REFERENCES
405
[11] L. Wang, G. Zhang, J. C. Luo, F. Zeng, Q. Z. Wang, S. A. Alfano, A. Katz, M. Zevallos,
and R. R. Alfano, “Wireless spectroscopic compact photonic explorer for diagnostic
optical imaging,” Biomed. Microdev. 7, 111–115 (2005).
[12] T. K. Gayen, A. Katz, H. E. Savage, S. A. McCormick, M. Alrubaiee, Y. Budansky,
J. Lee, and R. R. Alfano, “Aorta and skin tissues welded by near infrared Cr4+:YAG
laser,” J. Clin. Laser Med. Surg. 21, 259–269 (2003).
[13] L. V. Wang and S. Hu, “Photoacoustic tomography: in vivo imaging from organelles to
organs,” Science 335 (6075), 1458–1462 (2012).
[14] L. V. Wang, “Multiscale photoacoustic microscopy and computed tomography,” Nat.
Photon. 3, 503–509 (2009).
[15] Q. Zhu, T. Durduran, M. Holboke, V. Ntziachristos, and A. Yodh, “Imager that combines
near infrared diffusive light and ultrasound,” Opt. Lett. 24(15), 1050–1053 (1999).
[16] Z. Jiang, D. Piao, G. R. Holyoak, J. W. Ritchey, K. E. Bartels, G. Slobodov, C. F. Bunting,
and J. S. Krasinski, “Trans-rectal ultrasound–coupled spectral optical tomography of
total hemoglobin concentration enhances assessment of the laterality and progression
of a transmissible venereal tumor in canine prostate,” Urology 77(1), 237–242 (2011).
[17] S. Achilefu, R. B. Dorshow, J. E. Bugaj, and R. Rajagopalan, “Novel receptor-targeted
fluorescence contrast agent for in vivo tumor imaging,” Invest. Radiol. 35, 479–485
(2000).
[18] Y. Pu, W. B. Wang, S. Achilefu, and R. R. Alfano, “Study of rotational dynamics of
receptor-targeted contrast agents in cancerous and normal prostate tissues using time-
resolved picosecond emission spectroscopy,” Appl. Opt. 50(7), 1312–1322 (2011).
[19] V. Ntziachristos, A. G. Yodh, M. Schnall, and B. Chance, “Concurrent MRI and diffuse
optical tomography of breast after indocyanine green enhancement,” Proc. Natl. Acad.
Sci. USA 97(6), 2767–2772 (2000).
[20] A. Villringer and B. Chance, “Non-invasive optical spectroscopy and imaging of human
brain function,” Trends Neurosci. 20, 435–442 (1997).
[21] E. B. Hanlon, R. Manoharan, T.-W. Koo, K. E. Shafer, J. T. Motz, M. Fitzmaurice,
J. R. Kramer, I. Itzkan, R. R. Dasari, and M. S. Feld, “Prospects for in vivo Raman
spectroscopy,” Phys. Med. Biol. 45, R1–R59 (2000).
[22] R. R. Alfano, A. Pradhan, G. C. Tang, and S. J. Wahl, “Optical spectroscopic diagnosis
of cancer and normal breast tissues,” J. Opt. Soc. Am. B 6, 1015–1023 (1989).
[23] S. K. Gayen and R. R. Alfano, “Sensing lesions in tissues with light,” Opt. Express 4,
475–480 (1999).
[24] R. Richards-Kortum and E. Sevick-Muraca, “Quantitative optical spectroscopy for tissue
diagnosis,” Annu. Rev. Phys. Chem. 47, 555–606 (1996).
[25] R. R. Alfano, D. B. Tata, J. Cordero, P. Tomashefsky, F. W. Longo, and M. A. Alfano,
“Laser induced fluorescence spectroscopy from native cancerous and normal tissue,”
IEEE J. Quantum Electron. 20, 1507 (1984).
[26] Y. Pu, W. B. Wang, G. C. Tang, and R. R. Alfano,”Changes of collagen and NADH in
human cancerous and normal prostate tissues studied using fluorescence spectroscopy
with selective excitation wavelength,” J. Biomed. Opt. 15, 047008-1-5 (2010).
[27] I. Georgakoudi, B. C. Jacobson, M. G. Muller, E. E. Sheets, K. Badizadegan, D. L.
Carr-Locke, C. P. Crum, C. W. Boone, R. R. Dasari, J. Vand Dam, et al., “NAD(P)H
and collagen as in vivo quantitative fluorescent biomarkers of epithelial precancerous
changes,” Cancer Res. 62, 682–687 (2002).

406
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
[28] A. S. Haka, Z. Volynskaya, J. A. Gardecki, J. Nazemi, J. Lyons, D. Hicks,
M. Fitzmaurice, R. R. Dasari, J. P. Crowe, and M. S. Feld, “In vivo margin assess-
ment during partial mastectomy breast surgery using Raman spectroscopy,” Cancer
Res. 66, 3317–3322 (2006).
[29] R. R. Alfano, C. H. Liu, W. L. Sha, H. R. Zhu, D. L. Akins, J. Cleary, R. Prudente, and E.
Cellmer, “Human breast tissues studied by IR Fourier transform Raman spectroscopy,”
Lasers Life Sci. 4, 23 (1991).
[30] Y. Zhou, C.-H. Liu, Y. Sun, Y. Pu, S. Boydston-White, Y. Liu, and R. R. Alfano, “Human
brain cancer studied by resonance Raman spectroscopy,” J. Biomed. Opt. 17, 116021-1-7
(2012).
[31] C.-H. Liu, Y. Zhou, Y. Sun, J. Y. Li, L. X. Zhou, S. Boydston-White, V. Masilamani,
K. Zhu, Yang Pu, and R. R. Alfano, “Resonance Raman and Raman spectroscopy
for breast cancer detection,” Technol. Cancer Res. Treat. (TCRT) 12(4), 371–382
(2013).
[32] I. J. Bigio, S. G. Bown, G. Briggs, C. Kelley, S. Lakhani, D. Pickard, P. M. Ripley,
I. G. Rose, and C. Saunders, “Diagnosis of breast cancer using elastic-scattering spec-
troscopy: preliminary clinical results,” J. Biomed. Opt. 5(2), 221–228 (2000).
[33] R. R. Alfano and Y. Yang, “Stokes shift emission spectroscopy of human tissue and key
biomolecules,” IEEE J. Quantum Electron. 9, 148–153 (2003).
[34] Y. Pu, W. Wang, Y. Yang, and R. R. Alfano, “Stokes shift spectroscopy highlights
differences of cancerous and normal human tissues,” Opt. Lett. 37(16), 3360–3362
(2012).
[35] Y. Pu, W. Wang,Y. Yang, and R. R. Alfano, “Stokes shift spectroscopic analysis of multi-
fluorophores for human cancer detection in breast and prostate tissues,” J. Biomed. Opt.
18 (1), 017005-1-8 (2013).
[36] J. Ebenezar, Yang Pu, C. H. Liu, W. B. Wang, and R. R. Alfano, “Diagnostic potential
of Stokes shift spectroscopy of breast and prostate tissues—a preliminary pilot study,”
Technol. Cancer Res. Treat. 10, 153–161 (2011).
[37] R. Drezek, K. Sokolov, U. Utzinger, I. Boiko, A. Malpica, M. Follen, and R. Richards-
Kortum, “Understanding the contributions of NADH and collagen to cervical tissue
fluorescence spectra: modeling, measurements, and implications,” J. Biomed. Opt. 6(4),
385–396 (2001).
[38] Y. Pu, W. B. Wang, B. B. Das, and R. R. Alfano, “Time-resolved near infrared spectral
wing emission and imaging of human cancerous and normal prostate tissues,” Opt.
Commun. 282, 4308–4314 (2009).
[39] M. J. Niedre, R. H. de Kleine, E. Aikawa, D. G. Kirsch, R. Weissleder, and V. Ntziachris-
tos, “Early photon tomography allows fluorescence detection of lung carcinomas and
disease progression in mice in vivo,” Proc. Natl. Acad. Sci. USA 105 (49), 19126–19131
(2008).
[40] M. Cutler, “Transillumination as an aid in the diagnosis of breast lesion,” Surg. Gynecol.
Obstet. 48, 721–730(1929).
[41] B. C. Wilson and S. L. Jacques, “Optical reflectance and transmittance of tissues:
principles and applications,” IEEE J. Quantum Electron. 26, 2186–2199 (1990).
[42] I. Gannot, A. Garashi, V. Chernomordik, and A. Gandjbachkhe, “Quantitative optical
imaging of the pharmacokinetics of fluorescent-specific antibodies to tumor markers
through tissue-like turbid media,” Opt. Lett. 29, 742–744 (2004).

REFERENCES
407
[43] A. Godavarty, M. J. Eppstein, C. Zhang, S. Theru, A. B. Thompson, M. Gurfinkel,
and E. M. Sevick-Muraca, “Fluorescence-enhanced optical imaging in large tissue
volumes using a gain-modulated ICCD camera,” Phys. Med. Biol. 48, 1701–1720
(2003).
[44] A. Corlu, R. Choe, T. Durduran, M. A. Rosen, M. Schweiger, S. R. Arridge, M. D.
Schnall, and A. G. Yodh, “Three-dimensional in vivo fluorescence diffuse optical tomog-
raphy of breast cancer in humans,” Opt. Express 15, 6696–6716 (2007).
[45] M. Alrubaiee, M. Xu, S. K. Gayen, and R. R. Alfano, “Localization and cross sec-
tion reconstruction of fluorescent targets in ex vivo breast tissue using independent
component analysis,” Appl. Phys. Lett. 89, 133902 (2006).
[46] K. M. Yoo and R. R. Alfano, “Time-resolved coherent and incoherent components of
forward light scattering in random media,” Opt. Lett. 15, 320–322 (1990).
[47] D. S. Dilworth, E. N. Leith, and J. L. Lopez, “Three-dimensional confocal imaging of
objects embedded within thick diffusing media,” Appl. Opt. 30, 1796–1803 (1991).
[48] H. Horinaka, K. Hashimoto, K. Wada, and Y. Cho, “Extraction of quasi-straight-forward-
propagating-photons from diffused light transmitting through a scattering medium by
polarization modulation,” Opt. Lett. 20, 1501–1503 (1995).
[49] S. G. Demos and R. R. Alfano, “Temporal gating in highly scattering media by the
degree of optical polarization,” Opt. Lett. 21, 161–163 (1996).
[50] S. G. Demos and R. R. Alfano, “Optical polarization imaging,” Appl. Opt. 36, 150–163
(1997).
[51] S. G. Demos, H. Savage, A. S. Heerdt, S. Schantz, and R. R. Alfano, “Time-resolved
degree of polarization for human breast tissue,” Opt. Commun. 124, 439–442 (1996).
[52] L. Wang, P. P. Ho, C. Liu, G. Zhang, and R. R. Alfano, “Ballistic 2-D imaging through
scattering wall using an ultrafast Kerr gate,” Science 253, 769–771 (1991).
[53] L. Wang, P. P. Ho, X. Liang, H. Dai, and R. R. Alfano, “Fourier-Kerr imaging in thick
turbid media,” Opt. Lett. 18, 241–243 (1993).
[54] R. R. Alfano, X. Liang, L. Wang, and P. P. Ho, “Time-resolved imaging of translucent
droplets in highly scattering turbid media,” Science 264, 1913–1915 (1994).
[55] G. W. Faris and M. Banks, “Imaging through highly scattering media with a novel
upconverting time gate,” in Proceeding on Advances in Optical Imaging and Photon
Migration 21, edited by R. R. Alfano (Optical Society of America, Washington, DC,
1994), pp. 139–142.
[56] J. Reintjes, M. Bashkansky, M. D. Duncan, R. Mahon, L. L. Tankersley, J. A. Moon,
C. L. Adler, and J. M. S. Prewitt, “Time-gated imaging with nonlinear optical Raman
interactions,” Opt. Phot. News 4(10), 28–32 (1993).
[57] O. Jarlman, R. Berg, and S. Svanberg, “Time-resolved transillumination of the breast,”
Acta Radiol. 33, 277–279 (1992).
[58] K. M. Yoo, B. B. Das, and R. R. Alfano, “Imaging of a translucent object hidden in a
highly scattering medium from early portion of the diffuse component of a transmitted
ultrafast laser pulse,” Opt. Lett. 17, 958–960 (1992).
[59] K. G. Spears J. Serafin, N. H. Abramson, X. M. Zhu, and H. Bjelkhagen, “Chronoco-
herent imaging for medicine,” IEEE Trans. Biomed. Eng. 36, 1210–1221 (1989).
[60] E. Leith, E. Arons, H. Chen, Y. Chen, D. Dilworth, J. Lopez, M. Shih, P. C. Sun, and
G. Vossler, “Electronic holography for imaging through tissue,” Opt. Phot. News 4(10),
19–23 (1993).

408
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
[61] E. Arons, H. Chen, K. Clay, D. Dilworth, R. Draper, J. Lopez, E. Leith, M. Shih, and
P. C. Sun, “New holographic methods for improved imagery through scattering media,”
in Proceedings on Advances in Optical imaging and Photon Migration 21, edited by
R. R. Alfano (Optical Society of America, Washington, DC, 1994), pp. 239–243.
[62] A. D. Sappey, “Optical imaging through turbid media with a degenerate four wave
mixing correlation time gate.,” Appl. Opt. 33, 8346–8353 (1994).
[63] M. A. O’Leary, D. A. Boas, B. Chance, and A. G. Yodh, “Experimental images of
heterogeneous turbid media by frequency-domain diffusing-photon tomography,” Opt.
Lett. 20, 426–428 (1995).
[64] E. M. Sevick, C. L. Hutchinson, J. K. Frisoli, M. L. Johnson, K. Nowaczyk, H. Szmacin-
ski, and J. R. Lakowicz, “The physical basis of biomedical optical imaging using time-
dependent measurements of photon migration in the frequency domain,” in Medical
Optical Tomography: Functional Imaging and Monitoring, edited by G. J. Muller, R. R.
Alfano, S. R. Arridge, J. Beuthan, E. Gratton, M. Kaschke, B. R. Masters, S. Svanberg,
and P. van der Zeect, SPIE Institute Series (SPIE, Bellingham, WA, 1993), Vol. IS 11,
pp. 485–512.
[65] E. Gratton, W. W. Mantulin, M. J. vandeVen, J. B. Fishkin, M. B. Maris, and B. Chance,
“A novel approach to laser tomography,” Bioimaging 1, 40–46 (1993).
[66] Y. Yamashita and M. Kaneko, “Visible and infrared diaphanoscopy for medical diagno-
sis,” in Medical Optical Tomography: Functional Imaging and Monitoring, edited by
G. J. Muller, R. R. Alfano, S. R. Arridge, J. Beuthan, E. Gratton, M. Kaschke, B. R.
Masters, S. Svanberg, and P. van der Zeect, SPIE Institute Series (SPIE, Bellingham,
WA, 1993), Vol. IS 11, pp. 283–316.
[67] J. Beuthan, “IR-diaphanoscopy in medicine,” in Medical Optical Tomography: Func-
tional Imaging and Monitoring, edited by G. J. Muller, R. R. Alfano, S. R. Arridge,
J. Beuthan, E. Gratton, M. Kaschke, B. R. Masters, S. Svanberg, and P. van der Zeect,
SPIE Institute Series (SPIE, Bellingham, WA, 1993), Vol. IS 11, pp. 263–282.
[68] K. P. Chan, M. Yamada, B. Devaraj, and H. Inaba, “Optical imaging through highly
scattering media by use of heterodyne detection in the 1.3 μm wavelength region,”
Opt. Lett. 20, 492–494 (1995).
[69] H. Inaba, “Coherent detection imaging for medical laser tomography,” in Medical
Optical Tomography: Functional Imaging and Monitoring, edited by G. J. Muller, R. R.
Alfano, S. R. Arridge, J. Beuthan, E. Gratton, M. Kaschke, B. R. Masters, S. Svanberg,
and P. van der Zeect, SPIE Institute Series (SPIE, Bellingham, WA, 1993), Vol. IS 11,
pp. 317–347.
[70] A. Schmidt, R. Corey, and P. Saulnier, “Imaging through random media by use of
low-coherence optical heterodyning,” Opt. Lett. 20, 404–406 (1995).
[71] A. F. Frecher, W. Drexler, C. K. Hitzenberger, and T. Lasser, “In vivo optical coherence
tomography in ophthalmology,” in Medical Optical Tomography: Functional Imaging
and Monitoring, edited by G. J. Muller, R. R. Alfano, S. R. Arridge, J. Beuthan, E.
Gratton, M. Kaschke, B. R. Masters, S. Svanberg, and P. van der Zeect, SPIE Institute
Series (SPIE, Bellingham, WA, 1993), Vol. IS 11, pp. 355–370.
[72] L. Kalpaxis, L. M. Wang, P. Galland, X. Liang, P. P. Ho, and R. R. Alfano, “Three-
dimensional temporal image reconstruction of an object hidden in highly scattering
media by time-gated optical tomography,” Opt. Lett. 18, 1691–1693 (1993).
[73] M. Hiraoka, M. Firbank, M. E. Essenpreis, M. Cope, S. R. Arridge, P. Zee, and D.
Delpy, “Monte Carlo simulation of light transport through inhomogeneous tissue,”
Proc. SPIE—Int. Soc. Opt. Eng. 1888, 149–159 (1993).

REFERENCES
409
[74] S. R. Arrige, “The forward and inverse problems in time-resolved infrared imaging,”
in Medical Optical Tomography: Functional Imaging and Monitoring, edited by G. J.
Muller, R. R. Alfano, S. R. Arridge, J. Beuthan, E. Gratton, M. Kaschke, B. R. Masters,
S. Svanberg, and P. van der Zeect, SPIE Institute Series (SPIE, Bellingham, WA, 1993),
Vol. IS 11, pp. 35–64.
[75] W. Cai, S. K. Gayen, M. Xu, M. Zevallos, M. Alrubaiee, M. Lax, and R. R. Alfano,
“Optical three-dimensional inverse image reconstruction of objects in turbid media
from ultrafast time-sliced optical transmission measurements,” Appl. Opt. 38, 4237–
4246 (1999).
[76] M. Xu, M. Alrubaiee, S. K. Gayen, and R. R. Alfano, “Optical diffuse imaging of an
ex vivo model cancerous human breast using independent component analysis,” IEEE
J. Sel. Top. Quantum Electron. 14, 43 (2008).
[77] P. F. Moulton, “Spectroscopic and laser properties of Ti3+Al2O3,” J. Opt. Soc. Am. B 3,
125–133 (1986).
[78] V. Petricevic, S. K. Gayen, and R. R. Alfano, “Laser action in chromium-activated
forsterite for near-infrared excitation: is Cr4+ the lasing ion?” Appl. Phys. Lett. 53,
2590–2593 (1988).
[79] F. Liu, K. M. Yoo, and R. R. Alfano, “Transmitted photon intensity through biological
tissues within various time windows,” Opt. Lett. 10, 740–742 (1994).
[80] A. Ishimaru, Wave Propagation in Random Media (Academic Press, New York, 1978),
Vols. 1 and 2.
[81] L. Wang, “Picosecond Kerr Gated Imaging of Phantoms in Turbid Media,” PhD thesis,
The City University of New York (1995).
[82] C. J. M. Moes, M. C. van Gemert, W. M. Star, J. P. A. Marijnissen, and S. A. Prahl,
“Measurements and calculations of energy fluence rate in a scattering and absorbing
phantom,” Appl. Opt. 28, 2292–2296 (1989).
[83] H. J. van Staveren, C. J. M. Moes, J. van Marle, S. A. Prahl, and M. J. C. van Gemert,
“Light scattering in Intralipid-10% in the wavelength range of 400-1100 nm,” Appl.
Opt. 31, 4507–4514 (1991).
[84] V. G. Peters, D. R. Wyman, M. S. Patterson, and G. L. Frank, “Optical properties of
normal and diseased human breast tissues in the visible and near infrared,” Phys. Med.
Biol. 9, 1317–1334 (1990).
[85] J-L Boulnois, “Photophysical processes in recent medical laser developments: A
review,” Lasers in Med. Sci. 1, 47 (1986).
[86] K. M. Yoo, F. Liu, and R. R. Alfano, “When does the diffusion approximation fail to
describe photon transport in random media?” Phys. Rev. Lett. 64, 2647–2650 (1990);
Phys. Rev. Lett. 65, 2210–2211 (1990).
[87] M. Lax, V. Narayanamurti, and R. C. Fulton, “Classical diffusive photon transport
in a slab,” in Proceedings of the Symposium on Laser Optics of Condensed Matter,
Leningrad, June 1987, edited by J. L. Birman, H. Z. Cummins, and A. A. Kaplyanskij]
(Plenum, New York, 1987).
[88] G. H. Watson, P. A. Fleury, and S. L. McCall, “Searching for photon localization in the
time domain,” Phys. Rev. Lett. 58, 945–948 (1987).
[89] E. Akkermans, P. E. Wolf, and R. Maynard, “Coherent backscattering of light by disor-
dered media: analysis of the peak line shape,” Phys Rev. Lett. 56, 1471–1474 (1986).
[90] M. J. Stephen and G. Cwilich, “Rayleigh scattering and weak localization: effects of
polarization,” Phys Rev. B 34, 7564–7572 (1986).

410
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
[91] M. B. van der Mark, M. P. van Albada, and A. Lagendijk, “Light scattering in strongly
scattering media: multiple scattering and weak localization,” Phys. Rev. B 37, 3575–3592
(1988).
[92] M. Rosenbluh, I. Edrei, M. Kaveh, and I. Freund, “Precision determination of the line
shape for coherently backscattered light from disordered solids: comparison of vector
and scalar theories,” Phys Rev. A 35, 4458–4460 (1987).
[93] S. Feng, C. Kane, P. A. Lee, and A. Douglas Stone, “Correlation and fluctuations of
coherent wave transmission through disordered media,” Phys Rev. Lett. 61, 834–837
(1988).
[94] M. J. Stephen, “Temporal fluctuations in wave propagation in random media,” Phys.
Rev. B 37, 1–5 (1988).
[95] D. J. Pine, D. A. Weitz, P. M. Chaikin, and E. Herbolzheimer, “Diffusing wave spec-
troscopy,” Phys. Rev. Lett. 60, 1134–1137 (1988).
[96] D. A. Weitz, D. J. Pine, P. N. Pusey, and R. J. A. Tough, “Nondiffusive Brown-
ian motion studied by diffusing-wave spectroscopy,” Phys. Rev. Lett. 63, 1747–1750
(1989).
[97] I. Freund, M. Kaveh, and M. Rosenbluh, “Dynamic multiple scattering: ballistic photons
and the breakdown of the photon-diffusion approximation,” Phys. Rev. Lett. 60, 1130–
1133 (1988).
[98] W. Cai, M. Lax, and R. R. Alfano, “Analytical solution of the elastic Boltzmann transport
equation in an infinite uniform medium using cumulant expansion,” J. Phys. Chem. B
104, 3996–4000 (2000).
[99] W. Cai, M. Lax, and R. R. Alfano, “Cumulant solution of the elastic Boltzmann transport
equation in an infinite uniform medium,” Phys. Rev. E. 61, 3871–3876 (2000).
[100] M. J. Niedre, R. H. de Kleine, E. Aikawa, D. G. Kirsch, R. Weissleder, and V. Ntziachris-
tos, “Early photon tomography allows fluorescence detection of lung carcinomas and
disease progression in mice in vivo,” Proc. Natl. Acad. Sci. USA 105(49), 19126–19131
(2008).
[101] S. L. Jacques, “Time-resolved propagation of ultrashort laser pulses within turbid tis-
sues,” Appl. Opt. 28, 2223–2229 (1989).
[102] M. S. Patterson, B. Chance, and B. C. Wilson, “Time-resolved reflectance and trans-
mittance for the non-invasive measurement of tissue optical properties,” Appl. Opt. 28,
2331–2336 (1989).
[103] F. Liu, K. M. Yoo, and R. R. Alfano, “Should the photon flux or the photon density
be used to describe the temporal profiles of scattered ultrashort laser pulses in random
media?” Opt. Lett. 18, 432–434 (1993).
[104] G. Navarro and A. Profio, “Contrast in diaphanography of the breast,” Med. Phys. 15,
181–187 (1988).
[105] R. Marchesini, A. Bertoni, S. Andreola, E. Melloni, and A. E. Sichirollo, “Extinction
and absorption coefficients and scattering phase functions of human tissues in vitro,”
Appl. Opt. 28, 2318–2324 (1989).
[106] S. T. Flock, B. C. Wilson, and M. S. Patterson, “Total attenuation of tissues and phantom
materials at 633 nm,” Med. Phys. 14, 835–841 (1987).
[107] P. P. Ho, R. Dorsinville, N. L. Yang, G. Odian, G. Eichmann, T. Jimbo, Q. Z. Wang,
G. C. Tang, N. D. Chen, W. K. Zou, et al., “Ultrafast nonlinear optical processes in
4BCMU-polydiacetylene,” Proc. SPIE (Int. Soc. Opt. Eng.) 682, 36–41 (1986).

REFERENCES
411
[108] X. Liang, L. Wang, P. P. Ho, and R. R. Alfano, “Two-dimensional Kerr-Fourier imaging
of translucent phantoms in thick turbid media,” Appl. Opt. 34, 3463–3467 (1995).
[109] L. Wang, P. Ho, and R. R. Alfano, “Double-stage picosecond Kerr gate for ballistic
time-gated optical imaging in turbid media,” Appl. Opt. 32, 241–243 (1993).
[110] L. Wang, P. P. Ho, X. Liang, H. Dai, and R. R. Alfano, “Kerr-Fourier imaging of hidden
objects in thick media,” Opt. Lett. 18, 522–524 (1993).
[111] M. E. Zevallos, S. K. Gayen, M. Alrubaiee, and R. R. Alfano, “Time-gated backscat-
tered ballistic light imaging of objects in turbid water,” Appl. Phys. Lett. 86, 011115
(2005).
[112] S. K. Gayen, M. Alrubaiee, H. E. Savage, S. P. Schantz, and R. R. Alfano, “Parotid gland
tissues investigated by picosecond time-gated and spectroscopic imaging techniques,”
IEEE J. Sel. Top. Quantum Electron. 7, 906 (2001).
[113] M. Alrubaiee, S. K. Gayen, R. R. Alfano, and J. A. Koutcher, “Spectral and temporal
near-infrared imaging of ex vivo cancerous and normal human breast tissues,” Technol.
Cancer Res. Treat. 4, 455–482 (2005).
[114] J. Selb, D. K. Joseph, and D. A. Boas, “Time-gated optical system for depth-resolved
functional brain imaging,” J. Biomed. Opt. 11, 044008 (2006).
[115] C. D’Andrea, D. Comelli, A. Pifferi, A. Torricelli, G. Valentini, and R. Cubeddu, “Time-
resolved optical imaging through turbid media using a fast data acquisition system based
on a gated CCD camera,” J. Phys. D. Appl. Phys. 36, 1675 (2003).
[116] M. J. Niedrea, R. H. de Kleine, E. Eikawa, D. G. Kirsch, R. Weissleder, and V.
Ntziachristos, “Early photon tomography allows fluorescence detection of lung carci-
nomas and disease progression in mice in vivo,” Proc. Nat. Acad. Sci. USA 105, 19126–
19131 (2008).
[117] Y. Wang, S. K. Gayen, M. Alrubaiee, and R. R. Alfano, “Near-infrared center-of-
intensity time gated imaging for detection of a target in a highly scattering turbid
medium,” Technol. Cancer Res. Treat. 11, 309–315 (2012).
[118] L. Gu, D. J. Hall, Z. Qin, E. Anglin, J. Joo, D. J. Mooney, S. B. Howell, and M. J. Sailor,
“In vivo time-gated fluorescence imaging with biodegradable luminescent porous silicon
nanoparticles,” Nat. Commun. 4, 2326 (2013).
[119] V. Y. Soloviev, K. B. Tahir, J. McGinty, D. S. Elson, M. A. A. Neil, P. M. W. French, and
S. R. Arridge, “Fluorescence lifetime imaging by using time-gated data acquisition,”
Appl. Opt. 46, 7384–7391 (2007).
[120] R. Cubeddu, D. Comelli, C. D’Andrea, P. Taroni, and G. Valentini, “Time-resolved
fluorescence imaging in biology and medicine,” J. Phys. D. Appl. Phys. 35, R61 (2002).
[121] K. M. Yoo, Feng Liu, and R. R. Alfano, “Imaging through a scattering wall using
absorption,” Opt. Lett. 16, 1068–1070 (1991).
[122] S. K. Gayen, M. E. Zevallos, M. Alrubaiee, J. M. Evans, and R. R. Alfano, “Two-
dimensional near-infrared transillumination imaging of biomedical media with a
chromium-doped forsterite laser,” Appl. Opt. 37, 5327 (1998).
[123] J. B. Pawley, Handbook of Biological Confocal Microscopy (Plenum, New York, 1989).
[124] R. Arora, G. I. Petrov, and V. V. Yakovlev, “Hyperspectral coherent anti-Stokes Raman
scattering microscopy imaging through turbid medium,” J. Biomed. Opt. 16, 021116
(2011).
[125] I. M. Vellekoop and C. M. Aegerter, “Scattered light fluorescence microscopy: imaging
through turbid layers,” Opt. Lett. 35, 1245 (2010).

412
LIGHT PROPAGATION IN HIGHLY SCATTERING TURBID MEDIA
[126] A. Vander Lugt, Optical Signal Processing (Wiley Interscience, New York, 1992),
pp. 95–101.
[127] G. E. Anderson, Feng Liu, and R. R. Alfano, “Microscope imaging through highly
scattering media,” Opt. Lett. 19, 981–983 (1994).
[128] Q. Z. Wang, X. Liang, L. Wang, P. P. Ho, and R. R. Alfano, “Fourier spatial filter acts as a
temporal gate for light propagating through a turbid medium,” Opt. Lett. 20, 1498–1500
(1995).
[129] S. K. Gayen, M. E. Zevallos, and R. R. Alfano, “Near-infrared laser spectroscopic
imaging: a step towards diagnostic optical imaging of human tissues,” Lasers Life Sci.
8, 187–198 (1999).
[130] K. C. Hadley and I. A. Vitkin, “Optical rotation and linear and circular depolariza-
tion rates in diffusely scattered light from chiral, racemic, and achiral turbid media,”
J. Biomed. Opt. 7, 291–299 (2002).
[131] K. G. Phillips, M. Xu, S. K. Gayen, and R. R. Alfano, “Time-resolved ring structure of
circularly polarized beams backscattered from forward scattering media,” Opt. Express
13, 7954–7969 (2005).
[132] W. Cai, Xiaohui Ni, S. K. Gayen, and R. R. Alfano, “Analytical cumulant solution of
the vector radiative transfer equation investigates backscattering of circularly polarized
light from turbid media,” Phys. Rev. E 74, 056605 (2006).
[133] N. Ghose and I. A. Vitkin, “Tissue polarimetry: concepts, challenges, applications and
outlook,” J. Biomed. Opt. 16, 110801 (20011).
[134] R. R. Alfano, S. G. Demos, and W. B. Wang, “Imaging of objects in turbid media based
upon the preservation of polarized luminescence emitted from contrast agents,” U.S.
Patent, No. 6,091,983, July 18, 2000.
[135] Y. Pu, W. Wang, R. B. Dorshow, B. Idyud, B. Das, and R. R. Alfano, “Mini review of
ultrafast fluorescence polarization spectroscopy (Invited),” Appl. Opt. 52(5), 917–929
(2013).
[136] W. B. Wang, J. H. Ali, J. H. Vitenson, J. M. Lombardo, and R. R. Alfano, “Spectral
polarization imaging of human rectum-membrane-prostate tissues,” IEEE J. Sel. Top.
Quantum Electron. 9(2), 288–293 (2003).
[137] Y. Pu, W. Wang, M. Xu, J. A. Eastham, G. Tang, and R. R. Alfano, “Characterization
and three-dimensional localization of cancerous prostate tissue using backscattering
scanning polarization imaging and independent component analysis,” J. Biomed. Opt.
17 (8), 081419-1-8 (2012).
[138] W. B. Wang, S. G. Demos, J. Ali, and R. R. Alfano, “Imaging fluorescent objects
embedded inside animal tissues using polarization difference technique,” Opt. Commun.
142, 161–166 (1997).
[139] D. M. Jameson and J. A. Ross, “Fluorescence polarization/anisotropy in diagnostics and
imaging,” Chem. Rev. 110, 2685–2708 (2010).
[140] A. Yaroshevsky, Z. Glasser, E. Granot, and S. Sternklar, “Transition from the ballistic
to diffusive regime in a turbid medium,” Opt. Lett. 36, 1395–1397 (2011).
[141] M. Kempe, A. Z. Genack, W. Rudolph, and P. Dorn, “Ballistic and diffuse light detection
in confocal and heterodyne imaging systems,” J. Opt. Soc. Am. A 14, 216–223 (1997).

10
PHOTODYNAMIC THERAPY
Rakkiyappan Chandran,1,2,3 Tyler G. St. Denis,1,4
Daniela Vecchio,1,2 Pinar Avci,1,2 Magesh Sadasivam,1,2,3
and Michael R. Hamblin1,2,5
1Wellman Center for Photomedicine, Massachusetts General Hospital, Boston, MA, USA
2Department of Dermatology, Harvard Medical School, Boston, MA, USA
3Amity Institute of Nanotechnology, Amity University, Noida, India
4Department of Chemistry, Columbia University, New York, NY, USA
5Harvard-MIT Division of Health Sciences and Technology, Cambridge, MA, USA
10.1
HISTORICAL OVERVIEW OF PDT
Around 1900, the then medical student Oscar Raab was working in the laboratory
of Professor Herman von Tappeiner at the Pharmacological Institute of Munich,
Germany. Professor von Tappeiner was interested in the antimalarial application
of acridine dyes such as acridine orange. Accordingly, Raab was working on a
Paramecium spp. protozoan model, exposing samples to varying concentrations of
acridine orange. While acridine orange did exhibit antiprotozoan activity, there was
one major inconsistency: in one experiment the lowest dilution of acridine orange
killed all paramecia in 60–100 min, while in an earlier experiment the same dilution
killed the same amount of protozoans in 800–1000 min. This greater than 10-fold
difference in time-dependent killing prompted Raab and von Tappeiner to thoroughly
analyze the environmental variables of the two experiments. It was discovered that the
latter experiment, which demonstrated enhanced antiprotozoan activity of acridine
orange, was performed during an intense thunderstorm with lightning that illuminated
the laboratory. These two investigators verified that light enhanced the lethality of
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
413

414
PHOTODYNAMIC THERAPY
acridine orange, and published their work, and as early as 1900 realized the potential
medical, and specifically dermatological applications, of this technology.
By 1904, the importance of oxygen (O2) in this reaction was discovered, and
the light-sensitive dye photosensitization of cells in the presence of O2 was termed
(in German) a photodynamischer effekt, translating to a photodynamic effect. While
minimal human experiments had started as early as 1905, the two World Wars inter-
vened, redirecting researchers’ scientific efforts to war-related subjects, thus delaying
advancements in the field. Nevertheless, small steps forward were made, the most
important of which was the switch from experimenting with photosensitizers (PS)
derived from coal-tar and textile research (e.g., dyes of the phenothiazinium [1]
and acridine [2] class) toward using natural product-derived PS, such as the cyclic
tetrapyrroles known as porphyrins. Of the porphyrin PS, hematoporphyrin, and the
so-called hematoporphyrin derivative (HpD; a mixture containing ester- and ether-
linked hematoporphyrin dimers, trimers, and larger polymers) paved the way for PDT
clinical applications: Lipson et al. conducted several trials at the Mayo Clinic and in
1993, the sodium salt of HpD, known as Photofrin®, was approved by the FDA for
the treatment and management of various neoplastic conditions [3].
After exogenous application of PS was proven to be effective in the management
of disease, researchers in the 1980s were inspired to evaluate the possibility of
creating endogenous PS for PDT. The idea was simple and was inspired by a class
of diseases known as the porphyrias, which are a consequence of genetic mutations
that result in defects of the heme biosynthesis pathway. Almost all host cells have
machinery for manufacturing heme and it is known that the immediate precursor of
heme, protoporphyrin IX, can act as a PS with an absorption in the red portion of
the electromagnetic spectrum. By overloading cells with the initial building block
of heme, 5-aminolevulinic acid (5-ALA), the biological feedback loop that prevents
autophotosensitization is bypassed, resulting in localized photosensitivity—a sort of
controlled porphyria. Malik and colleagues used put 5-ALA and red light to the
test in 1987, and its overwhelming positive effect resulted in several clinical trials
and the acceptance of 5-ALA-mediated PDT as a targeted medical technology for
dermatological diseases [4]. In 1999, Levulan® Kerastick® (a 5-aminolevulinic acid
hydrochloride salve) and blue light was approved by the FDA for the treatment of
cancerous and precancerous skin lesions.
Even though PDT was born out of Raab’s accidental photodestruction of microor-
ganisms, from 1900 to 1990, the amount of papers exploring PDT for the control
of pathogen microorganisms were few and far between. As a consequence of the
emergence of antibiotic resistance in the latter half of twentieth century, interest in
antimicrobial PDT resurfaced. In the 1990s, susceptibility differences between Gram-
positive and Gram-negative bacteria were realized and overcome with the application
of cationic PS including the already clinically approved methylene blue and other
coal tar-derived dyes. As of the twenty-first century, antimicrobial PDT has been the
subject of a great deal of research interest as a consequence of the urgency for novel
modes of infection control especially in cases where antibiotics are ineffective. One
of the most attractive features of PDT is the apparent inability of microorganisms to

INTRODUCTION TO PDT
415
develop resistance to PDT, such that antibiotic-resistant and susceptible isolates are
equally susceptible to the phototoxic PDT effect.
10.2
INTRODUCTION TO PDT
10.2.1
Photophysics
PDT consists of the concomitant application of three important agents: light, PS,
and O2. Due to the aromatic and/or conjugated nature of PS, electrons of the highest
occupied molecular orbital (HOMO) are extensively delocalized, resulting in photonic
absorption in the visible and sometimes near-infrared portion of the electromagnetic
spectrum. In the ground state, the PS is in a singlet state, in which all electrons are
spin paired. Briefly, all paired electrons possess opposite spin values of either +1∕2
or −1∕2 and the sum of these spins, S, is zero. The spin state, also known as spin
multiplicity (Ms), is given by 2S + 1. When a photon (hv) of the appropriate energy
is absorbed in a very rapid (10−15 s) event, electronic absorption may occur before
PS vibrational events are observed. Because there is no change in the Ms of the PS,
electronic excitation is a process permitted by quantum mechanics.
Electron excitation by hv absorption results in the promotion of a single electron
to the lowest unoccupied molecular orbital (LUMO) without reversing the electronic
spin. The excited singlet state violates the Aufbau principle (from the German Aufbau
meaning building up) stating that lowest energy levels of electrons are most stable.
Due to the large energy gap between the ground singlet and excited singlet states, the
unstable excited singlet state exists for only a short period of time. It is possible for the
electron in the new HOMO of the excited singlet PS to be promoted to the new LUMO,
forming a higher excited singlet state, but it is much more likely that other processes
occur so as to lower the PS energy. For example, the energy may be transferred into
vibrational bond energy or heat (internal conversion). Alternatively fluorescence may
occur in which a photon is emitted by the excited state PS, bringing the PS back
to the ground singlet state. The selective accumulation of PS in neoplastic tissue
may be verified by selective tissue fluorescence and the appreciable fluorescence of
organic PS has aided photodynamic diagnosis of several diseases and has allowed for
pharmacokinetic and dosimetric PDT monitoring.
Fluorescence, however, is not responsible for the cytotoxic effects of PDT. In
the excited singlet state, the promoted electron may reverse its spin, becoming spin
parallel to its formal orbital partner. This process is known as intersystem crossing
and results in a triplet Ms, and though it is unfavorable in terms of multiplicity, it is
energetically favorable due to the smaller energy gap between the triplet state and the
ground singlet state relative to that of the excited singlet state and the ground singlet
state. The triplet state is a much longer-lived species namely because the promoted
electron cannot simply relax to the ground state, as it would then possess identical
quantum numbers with that of its partner electron. This represents a clear violation
of the Pauli exclusion principle such that for PS relaxation to the ground state, the PS
promoted electron must reverse its spin, re-entering the excited singlet state, and then

416
PHOTODYNAMIC THERAPY
FIGURE 10.1
Schematic illustration of photodynamic action (Jablonski diagram). The PS
initially absorbs a photon that excites it to the short-lived excited singlet state. This can lose
energy by fluorescence, internal conversion to heat, or by intersystem crossing to the long-lived
triplet state. This triplet PS can interact with molecular oxygen in two pathways, type 1 and
type 2, leading to the formation of reactive oxygen species (ROS) and singlet oxygen (1O2),
respectively.
emit hv to relax to the ground singlet state. This delayed emission process is called
phosphorescence. The relative energies of the ground singlet, excited singlet, and
excited triplet states, as well as the processes of fluorescence and phosphorescence
are illustrated in the classic Jablonski diagram shown in Figure 10.1.
Phosphorescence would be the most favorable energy decay process for PS, pro-
vided an excited triplet state PS existed in isolation. However, PS are prepared in
aqueous solution and are incubated in/on cells, both of which are environments with
an abundance of oxygen (ignoring hypoxic tumor environments). Dioxygen (O2)
contains two outer electrons in different orbitals that are spin parallel. O2 has a
triplet Ms in its ground state and this explains the paramagnetic and diradical natural
of O2. Because both the excited triplet PS and ground state O2 are in the triplet
state, and they have comparable energies, such that orbital interactions between the
two molecules is a spin-allowed process and results in either physical or chemical
quenching of the excited triplet state PS by O2. In the physical quenching process,
excited triplet state PS energy is transferred to ground state O2, flipping the spin of a
singlet electron and pairing both peripheral electrons in a single orbital. Due to the
violation of Aufbau principle, this species, known as singlet oxygen (O2(1Δg), 1O2)
is highly reactive. Nevertheless, this is a favored process, which does not violate any

INTRODUCTION TO PDT
417
quantum mechanical rules, as two triplet state molecules generate two singlet state
molecules. Moreover, according to basic molecular orbital theory, electron reorien-
tation results in movement into an orthogonal pi antibonding orbital, which is still
stable. The generation of 1O2 by excited triplet state PS quenching is known as type II
photochemistry.
1O2 is a metastable species and its lifetime is dependent on its interaction with sol-
vent molecules. 1O2 is approximately 22 kcal mol−1 more energetic than the ground
triplet state O2. As 1O2 relaxes to the ground state, it transfers energy to molecules of
its immediate surrounding. In protium water (H2O), 1O2 has a lifetime on the order of
4 μs; however, in deuterated water (D2O), 1O2 has a lifetime approximately 10-fold
longer. It should be noted here that 1O2 might persist in the gaseous state for hours.
This secondary kinetic isotope effect has given investigators the ability to determine
if the PDT lethality is dependent on 1O2 or radical oxygen species (discussed in
Section 10.2.2) generated during PDT by switching experimental solvents from H2O
to D2O.
10.2.2
Photochemistry
1O2 may interact with organic molecules in several interesting ways, which do not
occur with oxygen in the triplet ground state. Because 1O2 has the same quantum state
(singlet) as most commonly encountered molecules, it may react with them readily.
1O2 is an electrophilic agent with a low-lying, asymmetrical LUMO. Moreover, 1O2
has significantly more double bond character than 3O2 allowing it to participate in
Diels-Alder [5] as well as other pericyclic reactions.
Excited triplet state PS may be chemically quenched in an entirely different
process, affording an entirely different chemical reactivity. Type I photochemical
reactions encompass electron transfer, hydrogen abstraction, and photoadditions of
the triplet state PS, though it is traditionally considered a radical mechanism. This
is because the excited triplet state PS may interact with various electron donor or
acceptor molecules. In the case of electron donation, an excited triplet state PS may
directly donate its excited electron to oxygen, forming the PS radical cation and
a superoxide radical anion (O∙−
2 ). O∙−
2
may then undergo spontaneous or enzyme-
catalyzed dismutation forming hydrogen peroxide (H2O2). H2O2 is not a radical
species and is not intrinsically toxic to cells at low concentrations. However, due to
the presence of lone pairs on both oxygen atoms of H2O2 there is electron repulsion
between the two atoms, therefore lengthening the H2O2 sigma bond. This means that
the H2O2 bond is particularly prone to heterolysis, provided a suitable reducing agent
is present—H2O2 is thus a radical progenitor. This is the case when H2O2 is exposed
to ferrous iron (Fe2+) or other transition metals (e.g., Cu+) with a facile Xn∕Xn+1
oxidation potential, in what is known as the Fenton reaction, Eq. (10.1):
Fe2+ + H2O2 →Fe3+ + HO−+ OH.
(10.1)
This results in the formation of a hydroxide ion (HO−), ferric iron (Fe3+), and the
hydroxyl radical [6]. ∙OH is an extremely reactive species whose diffusion distance is

418
PHOTODYNAMIC THERAPY
very short and because cells are so prone to severe ∙OH damage, almost all cells lock
Fe2+ in heme complexes or iron–sulfur centers and additionally possess the enzyme
catalase to convert H2O2 to H2O and O2. Moreover, while O∙−
2 is not as oxidizing
as ∙OH and may not get inside of cells as easily, cells possess superoxide dismutase
enzymes to convert it to H2O2 (only to be subsequently quenched by catalase). ∙OH
is not enzymatically converted into any species, though antioxidants such as reducing
sugars (ascorbic acid) and peptides (glutathione) may convert ∙OH to HO−. ∙OH may
react with lipids, proteins, and nucleic acids destroying normal biological function
of these molecules. Note that radicals may alternatively emerge from PDT by 1O2
action as well. This occurs when 1O2 induces peroxidation of olefin-containing
substrates. If biomolecular hydroperoxide adducts are exposed to a suitable reducing
agent, they too may form dangerous oxygen radicals (biomolecule-O∙adducts) that
quickly abstract electrons or hydrogen atoms from surrounding molecules, inducing
free radical reactions that are capable of rapidly altering biomolecule structure
and function.
10.3
PHOTOSENSITIZER STRUCTURE AND
PHOTOPHYSICAL PROPERTIES
Red and near infrared light is ideal for biomedical optics as wavelengths in the
ranges of 600–800 nm allow for deeper tissue penetration whilst still permitting
electronic excitation. Wavelengths shorter than 600 nm scatter light and absorption of
wavelengths greater than 800 nm give increased molecular vibrations. The importance
of this 600–800 nm optical window is elaborated in Section 10.4.1.
The PS molecule always contains a chromophore, which, by definition is a cyclic
aromatic system with delocalized π-electrons, so they are deeply colored. This is
verified by a brief survey of PS classes that includes azine dyes (e.g., acridines,
phenothiaziniums), porphyrins (and reduced derivates such as chlorins and bacteri-
ochlorins), phthalocyanines and naphthalocyanines, triarylmethanes, xanthenes, and
cyanines. Chromophores typically possess intense absorption in the visible portion of
the electromagnetic spectrum when an electron-donating auxiliary group, appropri-
ately known as an auxochrome is present. Moreover, if an auxiliary group is capable
of reducing π symmetry in aromatic systems—as is the case with certain chlorins and
bacteriochlorins via their isocyclic ring which contains a chlorin-conjugated olefin—
further electron delocalization occurs, resulting in a bathochromic shift in photonic
absorption as well as increased absorption intensity.
By and large, PS design and synthesis have been driven by at least three con-
siderations. The three main considerations are (1) intense absorption in the optical
window, (2) little to no dark toxicity (i.e., PS toxicity only occurs where light of the
appropriate wavelength and intensity is applied, (3) high PDT activity and retention
of PS in diseased tissue without PS accumulation in healthy tissue.
Furthermore, an ideal photosensitizing agent should be a single pure compound
to allow quality control analysis with low manufacturing costs and good stability in

PHOTOSENSITIZER STRUCTURE AND PHOTOPHYSICAL PROPERTIES
419
storage. It should have a high absorption peak between 650 and 800 nm (red to far
red) as absorption of photons with wavelengths longer than 800 nm does not provide
enough energy to excite oxygen to its singlet state, and the capacity for forming a
substantial yield of reactive oxygen species upon irradiation. It should have no dark
toxicity and relatively rapid clearance from normal tissues, thereby minimizing pho-
totoxic side-effects. Other pertinent desirable properties of photosensitizing agents
have been summarized [7]. While the interval between drug administration and irra-
diation is usually long, so that the sensitizer is given sufficient time to diffuse from
normal tissues, reports now suggest that the tumor response may be sometimes better
when light is delivered at a shorter drug-light interval when PS is still present in the
blood vessels, thus producing marked vascular damage [8]. Some reports suggest
that a pronounced inflammatory response and necrotic cell death after illumination
are important in the immune-stimulating function of PDT, while others suggest that
PS that produce more apoptosis and less inflammation are suitable for applications
such as brain tumors where swelling is undesirable. Recent findings show that certain
PDT-induced apoptotic cell death mechanisms are highly immunogenic and capable
of driving antitumor immunity as well [9]. Finally the light-mediated destruction of
the PS known as photobleaching was thought to be undesirable, but some reports
suggest that this property may make light dosimetry less critical as over-treatment is
avoided when the PS is destroyed during the illumination [10].
The first PS to be clinically employed for cancer therapy was a water-soluble
mixture of porphyrins called hematoporphyrin derivative (HPD), a purified form
which later became known as Photofrin. Although Photofrin is still the most widely
employed PS, the product has some disadvantages including a long-lasting skin pho-
tosensitivity and a relatively low absorbance at 630 nm. There has been a major effort
among medicinal chemists to discover second-generation PS and several hundred
compounds have been proposed as potentially useful for anticancer PDT. Table 10.1
displays the most promising PS that have been used clinically for cancer PDT (whether
approved or in trials) and their structural formulae are shown in Figure 10.2.
The discovery that ALA was a biosynthetic precursor of the PS protoporphyrin
IX [11] has led to many applications in which ALA or ALA-esters can be topically
applied, or administered orally. These are considered to be “pro-drugs,” needing to be
converted to protoporphyrin IX to be active photosensitizers. Successes in treatment
of many tumor types (and other lesions mainly in dermatologic sites) have been
observed.
The most effective PS tend to be relatively hydrophobic compounds that rapidly
diffuse into tumor cells and localize in intracellular membrane structures such as
mitochondria and endoplasmic reticulum (ER). More polar compounds tend to be
taken up by adsorptive or fluid phase endocytosis and this process is slower necessi-
tating a longer drug-light interval. Many hypotheses have been proposed to account
for the tumor-localizing properties in PDT [12]. These include the preponderance
of leaky and tortuous tumor blood vessels due to neovascularization and absence of
lymphatic drainage known as the enhanced permeability and retention effect [13].
Some of the most effective compounds bind preferentially to low density lipoprotein

TABLE 10.1
Photosensitizers approved or in clinical trials for cancer
Photosensitizer
Structure
Wavelength (nm)
Approved
Trials
Cancer types
Photofrin (HPD)
Porphyrin
630
World wide
Lung, oesophagus, bile duct,
bladder, brain, ovarian
ALA
Porphyrin precursor
635
World wide
Skin, bladder, brain, oesophagus
ALA esters
Porphyrin precursor
635
Europe
Skin, bladder
Foscan (mTHPC)
Chlorin
652
Europe
USA
Head and neck, lung, brain, skin,
bile duct
Verteporfin
Chlorin
690
World wide
(AMD)
UK
Ophthalmic, pancreatic, skin
HPPH
Chlorin
665
USA
Head and neck, oesophagus, lung
Purlytin (SnEt2)
Chlorin
660
USA
Skin, breast
Taloporfin, LS11, MACE, Npe6
Chlorin
660
USA
Liver, colon, brain
Fotolon (PVP-Ce6),
Radachlorin, Photodithazine
Chlorin
660
Belarus,
Russia
Nasopharyngeal, sarcoma, brain
Silicon phthalocyanine (PC4)
Chthalocyanine
675
USA
Cutaneous T cell lymphoma
Padoporfin (TOOKAD)
Bacteriochlorin
762
USA
Prostate
Motexafin lutetium (LuTex)
Texaphyrin
732
USA
Breast
420

PHOTOSENSITIZER STRUCTURE AND PHOTOPHYSICAL PROPERTIES
421
FIGURE 10.2
Chemical structures of the clinically tested PS listed in Table 10.1. Photofrin
(HPD), ALA, ALA methyl ester, Foscan (mTHPC), Verteporfin (BPD-MA), HPPH (Pho-
tochlor), Purlytin (SnEt2), Taloporfin (MACE, Npe6, LS11), Fotolon (PVP-ce6), silicon
phthalocyanine (PC4), TOOKAD (Padoporfin), LuTex (motexafin lutetium).
(LDL), suggesting that upregulated LDL receptors found on tumor cells could be
important [14].
There have been targeting studies in which PS are covalently attached to various
molecules that have some affinity for neoplasia or to receptors expressed on specific
tumors [15]. The intention is to rely on the ability of the targeting vehicle to control

422
PHOTODYNAMIC THERAPY
localization factors so that the PS can be chosen based on its photochemical properties.
These vehicles include monoclonal antibodies, antibody fragments, peptides, proteins
such as transferrin, epidermal growth factor and insulin, LDL, various carbohydrates,
somatostatin, folic acid, and many others.
10.4
LIGHT DOSIMETRY AND PHOTODYNAMIC THERAPY
LIGHT SOURCES
The aforementioned photochemical reactions that lead to select tissue death are due to
quantum effects (number of photons absorbed) rather than power density effects (rate
at which photons are delivered). Moreover, tissue penetration by light is dependent
on the incident power density that is necessary to reach a sufficient photon threshold
at a specified depth in tissue.
10.4.1
Tissue Optics
Like all media that light interacts with, tissue is capable of both light absorption and
light scattering. However, the heterogeneity of tissue complicates the design of light
dosimetry regimens for PDT. Even though tissue is rich with chromophores such
as oxygenated hemoglobin and melanin, light is more likely to be scattered before
being propagated through cellular media and exciting an endogenous or exogenous
chromophore.
Scattering occurs at the microscopic biointerface (e.g., between two tissues,
organelles) with different refractive indexes. Fat globules and dense protein compart-
ments have higher refractive indexes than water (1.55 versus 1.33). Hydroxyapatite,
the main mineral constituent of bone, has a refractive index of 1.65. The source
of scattering in cells is due to intracellular organelles, which generate microscopic
boundaries with the cytoplasm. The absorptive properties of tissue are not necessar-
ily the same as the excitation of molecules. For example, water has an absorption
spectrum most intense in the 900–1060 nm range, which corresponds to molecular
vibrations. The lipid absorption profile is most intense in the 900–950 and 1000–
1060 nm ranges, and oxygenated hemoglobin and deoxygenated hemoglobin have
peak absorptions in the 500–600 nm range with intense UV absorption.
Melanin is produced by melanocytes that are found in the basal layer of the
epidermis with dendritic protrusions extending toward epithelial tissue. This is why
metastasis of melanoma is very hard to treat: all cavities of the body are lined with
epithelial tissue such that malignant melanocytes can easily invade the epithelium.
Melanin has a strong UV absorption with decreased absorption in the red and near-
infrared portions of the electromagnetic spectrum. However, the absorption in the
600–800 nm range by melanin is not strong enough to hinder effective phototherapy.
The total tissue attenuation coefficient of any given wavelength of light is equal
to the sum of the scattering and absorption at that wavelength. Tissue acts as a
short pass filter and the smaller attenuation coefficient for longer-wavelength light
has driven the synthesis of PS that absorb in the far-red and near-infrared portions

LIGHT-BASED STRATEGIES TO ENHANCE PDT
423
of the electromagnetic spectrum. However, this is seldom an issue in designing
PDT regimens, due to the dual spatial selectivity (a consequence of selective PS
localization and spatially confined light excitation) of PDT, which limits applications
to those diseases that are easily accessible and anatomically shallow.
10.4.2
Fundamental Dosimetry Concepts
PDT dosimetry has been adopted from conventions developed for anticancer radiation
therapies. In radiology, absorbed dose is reported in sieverts [16] for biological tissue,
where 1 Sv is equal to 1 J kg−1. As a consequence, radiation treatment takes into
account the mass and volume of tumor, as determined by imaging modalities, organ
motion, and uncertainties of tumor location. However, Sv is an inappropriate unit for
PDT, as it fails to take into account the photochemical action of the PS. Moreover,
due to the nature of PDT, diseased tissue is accessed from a surface (e.g., a tumor) and
taking surface area of the malady into account is of utmost importance. Accordingly,
PDT dose is defined as
(μgPS∕gtarget tissue) J cm−2.
(10.2)
In cases where the PS concentration is homogenous, dosimetry is defined as light
dose only (J cm−2). This is evident in cases where PS is known to have high binding
affinity to cellular targets, such as in antimicrobial PDT.
10.5
LIGHT-BASED STRATEGIES TO ENHANCE PDT
The significant advances and developments in light-based strategies for achieving
selective-enhanced PS concentration on target tissues play a major role in PDT.
On the basis of the PDT phenomena, the ooptical excitation of the PS produces
reactive oxygen species that cause localized, apoptotic, and/or necrotic cell death.
Two-photon absorption has been applied using various chromophores with highly
efficient two-photon absorption (TPA) cross-sections that are needed for several
innovative applications, such as fluorescence three-dimensional (3D) microscopy,
deeper-penetrating PDT, 3D micro- and nanofabrication, high-density optical data
storage, and many others. The potential noninvasive application of new modalities—
two-photon excitation-PDT (TPE-PDT) and metronomic PDT will be discussed here.
10.5.1
Two-Photon Excitation-PDT
Two-photon excitation (TPE)-PDT which offers the potential for better selectivity and
less damage to normal tissue in treating wet age-related macular degeneration is also
used in a wide range of applications, ranging from engineering to physiology. Recent
advances in TPA and in the design of two-photon chromophores are rapidly increasing
the scope of this field. PDT agents are targeted to over-expressed receptor sites in
tumors and are also capable of being activated by TPA rather than the one-photon
activation mechanism employed in all previous PDT agents. Two-photon PDT has

424
PHOTODYNAMIC THERAPY
the distinct advantage of exciting these new PS far in the near-infrared (780–900 nm)
within the tissue transparency window using pulsed laser light that allows the direct
treatment (no surgical intervention) of subcutaneous tumors to depths of several
centimeters [17].
The Prasad group has demonstrated the use of infrared excitation in conjunction
with an efficient TPA PS in PDT [18]. Certain newly developed organic molecules
were studied and it was found that chromophores can act as “photon harvesters” that
absorb two photons of near infrared light and transfer the energy to the PS, which
can generate 1O2 in the presence of atmospheric oxygen [19]. Shen et al. performed
enhanced two-photon 1O2 generation by PS-doped conjugated polymer nanoparticles.
They used conjugated polymer, poly[9,9-dibromohexylfluorene-2,7-ylenethylene-
alt-1,4-(2,5-dimethoxy)phenylene] (PFEMO), and tetraphenylporphyrin (TPP) show-
ing that these PS-doped conjugated polymer nanoparticles can act as novel PS for
two-photon PDT and related applications [20]. Spangler and Starkey et al. have treated
human tumor xenografts of human breast cancer (MDA-MB-231) and both small-cell
lung cancer (SCLC) (NCI-H69) and non-small-cell lung cancer (NSCLC) (A-459)
successfully using targeted two-photon PDT-PS for the treatment of subcutaneous
tumors, suggesting that tumors can be treated noninvasively using two-photon PDT
[17,21,22]. Khurana et al. used endothelial cells (YPEN-1) to assess the TPE-PDT
efficacy of the PS Photofrin® and verteporfin (which are known to be effective by
one-photon excitation). They incubated a confluent monolayer of endothelial cells
with each PS and performed TPE-PDT by irradiating the cell layer with the 865-nm
laser line of a femtosecond Ti:sapphire laser at various output powers. They achieved
TPE-PDT induced cell death with both PS, where verteporfin was around seven times
more effective, consistent with its higher two-photon cross-section. Using verteporfin
and by varying the laser dose, they were also able to demonstrate the quadratic inten-
sity dependence of cellular two-photon PDT [23]. Mir et al. compared TP-PDT using
Photofrin and phthalocyanines dyes on neovascular age-related macular degeneration
(AMD) excited by a femtosecond Ti-sapphire laser [24].
Karotki et al. used Photofrin to demonstrate proof-of-principle of 2-γ killing (dam-
age to tissues by two-photon excitation) of vascular endothelial cells in vitro. They
established the technique for measuring the 2-γ absorption spectra and cross-sections
of PDT sensitizers so as to ensure that there were negligible contributions from 1-γ
interactions. Adherent YPEN-1 endothelial cells were incubated with Photofrin for
24 h and then treated by PDT at 850 nm where the 1-γ contribution was negligi-
ble which shows that 2-γ excitation dominates over 1-γ excitation above 800 nm
[25]. Starkey et al. demonstrated the application of a new 2-γ activated PDT agent
which induced xenograft tumor regressions after near-IR laser treatment through
the entire body of the host mouse. In their experiment human SCLC (NCI-H69),
NSCLC (A549), and breast cancer (MDA-MB-231) xenografts were induced in SCID
mice and microarray expression profiles were conducted to assess the similarity of
responses to single- and two-photon activated PDT [17]. Thus, application of 2-γ
PDT allows spatially selective treatment of cancers and TPA-PDT has an advantage
for higher selectivity than one-photon absorption PDT, and femtosecond pulsed laser
is further more suitable for TPA-PDT than pico- and nanosecond pulsed lasers.

PDT TARGETING AND NANOTECHNOLOGY
425
10.5.2
Metronomic PDT
Metronomic photodynamic therapy (mPDT) is a new methodology, in which both
the PS and light are delivered continuously at low rates for extended periods of time
to increase selective tumor cell kill through apoptosis. The focus of mPDT treatment
is for treating various malignant tumors often in the brain, in which selectivity tumor
cell killing versus damage to normal brain is critical. Studies have shown that high-
dose ALA can induce necrosis in normal brain (cortex) as well as tumor, whereas an
acute low-dose regimen provides tumor-selective apoptosis without necrosis [26] but
mPDT with ALA could provide an even greater induction of apoptosis in brain tumor
cells without compromising selectivity. Thus, in vitro studies confirmed the enhanced
induction of apoptosis in 9L gliosarcoma cells after ALA-mPDT as compared with a
similar aPDT regimen [27].
Bogaards et al. performed fluorescence-guided resection (FGR) and PDT to eval-
uate fluorescence imaging and PDT treatment techniques in a specific intracranial
tumor model. The model was the VX2 carcinoma grown by injection of tumor cells
into the normal rabbit brain. mPDT uses ALA-induced protoporphyrin IX (PpIX)
for both FGR and PDT [28]. Studies of “fractionated” PDT, particularly for treating
skin tumors using ALA-PpIX, with improved responses have also been reported [29].
Though mPDT has several promising advantages, there are still challenges for clinical
implementation like administering the PS continuously during an extended period of
time to ensure that a high equilibrium level of drug within the tumor is achieved. In
mPDT there is also a need to maintain tumor to normal tissue selectivity and avoid
systemic toxicity. Moreover, administering the delivery of light continuously to the
entire tumor for a long time in a minimally invasive manner is still being extensively
studied. For these reasons researchers have implanted LED sources, such as the linear
LED arrays developed for interstitial and endoscopic treatments, into the brain [30].
This procedure is less invasive than repeatedly opening the skull to deal with recurrent
brain cancer. Furthermore, the implantation of optical fibers, balloon applicators, or
use of organic-LED “patches” connected through the skull to an external laser source
have been investigated [31]. All these studies show a promise for metronomic PDT
to be used to effectively deal with brain tumors.
10.6
PDT TARGETING AND NANOTECHNOLOGY
The targeted delivery of PS to diseased cells is one of the major challenges in PDT.
Nanotechnology has opened a new realm for improving therapeutic strategies, created
to overcome growing problems of contemporary medicine, such as drug toxicity and
drug resistance. This emerging modality can lead to targeted PDT (TPDT) aiming for
improving the delivery of PS to cancer tissue, enhancing specificity and efficiency of
PDT simultaneously. The application of nanotechnology has advanced in the field of
PDT and has provided hope for the development of nanoscale delivery carriers for
PS, which might improve the efficiency of PDT and can overcome many side-effects
associated with classical PDT. Nanoparticles (NPs) have high surface-to-volume

426
PHOTODYNAMIC THERAPY
ratios enabling modification with various surface functional groups; and multifunc-
tional NP capable of targeting molecular motifs, and allowing both imaging and
delivery of therapeutics is an exciting area of research that holds great promise for
cancer therapy in future. The various recent advances in NP technology may provide
a range of novel approaches to PDT [32–34].
NPs exhibit unique pharmacokinetics, including minimal renal filtration that
extends circulation in the blood pool depending on the surface functionalization
characteristics of the particles [35]. However, these approaches are still at the proof-
of-principle stage. Mainly, NPs can improve the delivery of PS, as they can with
many other imaging contrast agents or therapeutic compounds form functionalized
“nanoplatforms” with two or more functionalities, leading to theranostic (diagnose
and treat) NP [36–40]. The main goal of this emerging theranostic field is to gain
the ability to image and monitor the diseased tissue, delivery kinetics, and drug
efficacy with the long-term hope of tuning the therapy and adjusting the dose with
heretofore-unattainable control. Vatansever et al. [36] have illustrated the diversity of
the theranostic approach showing some of the components that have been attached
to nanostructures to provide multifunctionality in PDT (shown in Fig. 10.3). These
DNA and other nucleic acids
Polyethylene
Glycol
Protein
MRI active agent
Photosensitizer
Polymeric coating
Functional group to tune
surface charge
Dyes for optical imaging
Adsorbed drug
Sensitive coverage
FIGURE 10.3
Nanoplatforms with theranostic functionalities: Fe2O3, magnetic NPs, photo-
sensitizer, appropriate polymeric coating, surface-charge tuning groups, optical imaging dyes,
targeting agents, adsorbed/bonded drugs, sensitive coverage, DNA and other nucleic acids,
polyethylene glycol, proteins, MRI active agents and so on. Reprinted with permission from
Reference 36. (For a color version of this figure, see the color plate section.)

PDT TARGETING AND NANOTECHNOLOGY
427
Silica
NP
CH3
H3C
H3C
NH HN
N
N
CH3
COOH
HOOC
HOOC
QD
FIGURE 10.4
Nanotechnology taking PDT from bench to bedside. A wide range of lipid,
polymer, ceramic, magnetic, and other functionalized nanoparticles have been used in PDT that
can improve photosensitizer delivery, together with fullerenes, carbon nanotubes and quantum
dots, and raises the question of whether nanotechnology will truly improve the translation of
PDT from mouse to man. (For a color version of this figure, see the color plate section.)
nanoconstructs have evolved to such an extent that some of them may be able to be
injected into the blood stream, circulate harmlessly, and eventually seek out and bind
their targets, to deliver a therapeutic payload (as shown in Fig. 10.4).
Nonviral vehicles that are based on synthetic polymers, dendrimers, liposomes,
cell-penetrating peptides, and inorganic nanoparticles have been developed and stud-
ied in detail for their potential application in PDT [41–44]. These types of vehicle
also play a major role and possess the beneficial capability to be easily modified for
theranostic purposes. Recently, both the delivery vehicle, as well as DNA, labeled
with organic dyes have made the use of more sophisticated imaging techniques, such
as time-lapse microscopy, fluorescence resonance energy transfer (FRET), and flu-
orescence correlation spectroscopy (FCS) [45–47]. These techniques provide more
precise information regarding polyplex trafficking, detachment, and leading toward
development of theranostic systems. Koo et al. used novel nanoparticles like glycol
chitosan nanoparticles (CNPs) for both in vivo diagnosis and therapy. They evaluated
the in vivo biodistribution and tumor site accumulation of CNPs with or without
drug by using radioisotopes [48]. Kim et al. also carried out in vivo studies on tumor
targeting ability in tumor-bearing mice by using the near-infrared fluorescent (NIRF)
dye, CNPs, Cy5.5, and noninvasive fluorescence imaging [49].

428
PHOTODYNAMIC THERAPY
Due to some limitation in fluorescent dyes, an alternative optical reporter agent
such as quantum dots (QDs) may play a role in PDT, by acting as either PS themselves
or as PS-carriers [50, 51]. QDs have been used because of their high extinction
coefficients, strong control over optical properties, and reduced susceptibility to
photobleaching [52]. QDs can easily form FRET pairs with fluorescent organic
dyes and are useful for monitoring polyplex trafficking in vitro [38]. Samia et al.
studied the interaction between CdSe QDs with a silicon phthalocyanine PDT-PS,
Pc4. They found that the QDs could be used to sensitize the PDT agent through FRET
mechanism, or interact directly with molecular oxygen via a triplet energy-transfer
process [53], thus generating reactive 1O2 that can be used for PDT [54]. Rakovich
et al. studied PDT properties of novel CdTe quantum dots-methylene blue hybrid PS.
In vitro studies on HepG2 and HeLa cancer cells showed effective cell kill by the
methylene blue-semiconductor nanocrystal hybrid system [55]. However, it is still
necessary to understand the metabolism of QDs in the body and related toxicity issues.
Furthermore, iron oxide nanoparticles (IONPs) have been investigated as nanoplat-
forms on which to construct probes containing multiple imaging motifs that with the
ambitious goal to load drugs into these nanosystems to achieve all-in-one theranostic
agents for PDT [56, 57]. Recently, Feridex® particles have been approved by the
FDA for the detection of liver and spleen lesions, and also the analog Combidex® has
entered into phase III clinical trials for lymph node imaging [40]. Also due to their
high temperature decomposition, IONPs can play a dual role for imaging/therapy,
because of their potential in hyperthermia after excitation by alternating magnetic
fields, which has emerged as a useful strategy in nanoparticle preparation for thera-
nostics application.
Recently, fullerenes (closed carbon cages) have shown potential in several different
cancer therapeutic approaches such as PDT, photothermal treatment, radiotherapy,
and chemotherapy. The Hamblin group has extensively studied fullerenes for their
potential application in PDT [58]. Fullerenes can also acts as novel contrast agents
in magnetic resonance [59] imaging [60]. However, their non-biodegradability and
possible toxicity remain a concern in the area of biomedical applications. The rapid
growth in nanotechnology will continue to grow in coming years and will be further
studied to potentiate PDT by increasing selectivity and increasing multifunctionality.
10.7
PDT FOR DERMATOLOGY
In 1905, Jesionek and von Tappenier described the first applications of PDT in humans
by using eosin and white light to treat a number of dermatologic conditions such as
condylomata lata, lupus vulgaris, and nonmelanoma skin cancer (NMSC) [61]. Since
then, the list of PDT applications in dermatology has been consistently growing.
While treatment of precancerous and malignant skin tumors (mainly NMSC) are the
most commonly used applications of PDT; during recent years, we have witnessed a
broadening spectrum of so-called off-label uses especially in non-oncological derma-
tologic conditions [62]. Among them, acne, psoriasis, cutaneous T-cell lymphoma,
leishmaniasis, rosacea, photorejuvenation, and warts are just a few examples that

PDT FOR DERMATOLOGY
429
have been studied with promising results [62]. While the use of PDT in infectious
dermatologic conditions will be covered within a separate section, in this section
we will focus on its applications for precancerous and malignant skin tumors, acne
vulgaris and photorejuvenation.
10.7.1
Actinic Keratosis
Actinic keratosis (AK) is the most common precancerous lesion of the skin and is
frequently seen in the elderly. The lesions develop on areas of chronic ultraviolet
exposure such as face and scalp and often occur in multiples at these sites, leading
to what is termed field cancerization. In a longitudinal study conducted by Marks
et al., it was reported that 60% of squamous cell carcinoma (SCC) occurred from
progression of AKs [63]. Cryosurgery, 5-fluorouracil, imiquimod, and curettage are
the currently used treatment options each with advantages and limitations [64, 65].
In several studies, it has been reported that thin and moderate thickness AK on the
face and scalp respond well to ALA-PDT, with typical clearance rates of 89–92%
3 months after therapy, and response was reported to be equivalent or superior to
cryotherapy [66–68]. One-year sustained lesion clearance rates following ALA-PDT
(up to two treatments) and patch ALA-PDT (single treatment) were 78% and 63–
69%, respectively [69,70]. A large randomized intraindividual study with 119 patients
compared efficacy of ALA-methyl ester (MAL)-PDT with cryotherapy in treatment
of face/scalp AK [68]. MAL-PDT was given as a single treatment and repeated
after 3 months if required [68]. The results showed that, after the initial treatment,
PDT cleared more lesions (87% vs. 76%) than cryotherapy, and had equivalent
outcome when the initial nonresponders were retreated (89% vs. 86%) [68]. In a
right/left comparison study with imiquimod, ALA-PDT cleared significantly more
moderate AK lesions (58% vs. 37%), and equal numbers of thin AK lesions on
the hands/forearms (72% lesions) [71]. However, according to Kaufmann’s report,
when compared, MAL-PDT was less effective than cryotherapy for acral AK (lesion
clearance 78% vs. 88% at 6 months) [72]. This reduced efficacy of PDT for AK
on acral sites compared to that for facescalp lesions is probably in part due to a
higher proportion of less-responsive thicker lesions on these sites. A few studies also
have been done using PDT as a therapy for actinic cheilitis [73–75]. A recent study
by Sotiriou et al. demonstrated complete clinical cure of 80% and histological cure
of 73% when MAL-PDT and imiquimod 5% cream were used in sequence. Lastly,
PDT was suggested to be used for the prevention of nonmelanoma skin cancer in
organ transplant recipients and other immunocompromised patients [76]. Apalla and
colleagues suggested a preventive potential of field PDT in immunocompromised
individuals [76]. They have conducted a study among photodamaged patients with
facial AK, where ALA-PDT demonstrated a significant delay over control sites
of about 6 months until new AK developed [76]. In an intrapatient randomized
study of 27 renal organ transplant patients with AK and other skin lesions, a single
treatment with MAL-PDT significantly delayed (9.6 vs. 6.8 months for control site)
the development of new lesions; and by 12 months, when 62% of treated areas were
free from new lesions, it was only 35% for control areas [77]. PDT was shown to

430
PHOTODYNAMIC THERAPY
decrease the expression of p53, a marker of early skin cancer, which supports its
preventive indication in carcinogenesis [78,79].
10.7.2
Bowen’s Disease/Squamous Cell Carcinoma In Situ
Bowen’s disease is a form of squamous cell carcinoma (SCC) in situ and mainly
seen in elderly people. It most often appears as a scaly, crusted, erythematous,
well-demarcated plaque on sun-exposed surfaces such as the face, scalp, hands, and
lower legs [80]. Once SCC invades into the dermis, which happens in 3–20% of the
cases, metastasis develops in more than one-third of the patients [80, 81]. Surgical
excision, electrodessication, curettage, and 5-fluorouracil are the common treatment
options [82]. Rapid healing time and capacity to treat more than one lesion at a time
makes PDT to be an attractive alternative therapy. Unlike surgical excision that can be
complicatedbywounddehiscenceornecrosis,especiallyinthevascularcompromised
areas such as the lower legs; PDT is a noninvasive option that spares the tissue and
results in less morbidity [83]. A randomized clinical trial by Morton et al. was the first
to demonstrate that ALA-PDT was equally effective as cryotherapy [84]. Moreover,
while ALA-PDT cleared 100% of lesions after two treatment sessions without disease
recurrence, cryotherapy required three sessions for 100% lesion clearance and disease
recurrence has been observed [84]. Complications such as ulceration and infection
were reported in the cryotherapy group but not in the PDT group [84]. ALA-PDT
was also shown to be significantly more effective than 5-fluorouracil in terms of
both immediate and long-term efficacy [85]. A large multicenter trial involving 40
European medical centers investigated the efficacy, tolerability, and cosmetic outcome
of MAL-PDT for Bowen disease [86]. In this study, Morton et al. observed a 3-month
complete response rate of 86%, which was similar to the efficacy of cryotherapy and
5-fluorouracil [86]. However, at 12 months, the sustained lesion complete response
rate with MAL-PDT as well as the cosmetic result were superior to the other two
modalities [86].
10.7.3
Basal Cell Carcinoma
Basal cell carcinoma (BCC) commonly occurs on the head, neck, and nose. Although
it rarely metastasizes, there is always a risk of aggressive growth, causing extensive
tissue destruction and significant morbidity [87]. Although the most commonly used
therapy for BCC has been surgery or other forms of ablation, in cosmetically sensitive
locations such as the face, a noninvasive form of treatment like PDT is an attractive
choice [88]. In a review by Peng et al., it was reported that among 826 superficial
and 208 nodular BCCs treated with PDT, complete clearance rates of 87% and
53% were achieved at follow-up periods of 3–36 months [89]. However, ALA-PDT
when used for treatment of nodular BCCs alone showed a less favorable response
[90, 91]. Interestingly, MAL-PDT achieved much better results than ALA-PDT in
the treatment of BCCs, especially for nodular BCCs and this may be attributed to
its lipophilicity, faster skin penetration, and higher selectivity [62,92,93]. According
to Rhodes et al.’s [94] report complete response rates between groups treated with

PDT FOR DERMATOLOGY
431
surgery versus MAL-PDT did not differ significantly (98% vs. 91%) but at 24-month
follow-up, higher recurrence after MAL-PDT (9%) than surgery (2%) was observed.
Moreover, in a 5-year follow-up study, the higher recurrence rate with PDT (14%)
versus surgery (4%) continued [95]. PDT might be considered a first-line therapy for
patients who are not appropriate for surgery such as patients with bleeding disorders
or those at high risk of scarring.
10.7.4
Melanoma
Melanoma, a cancer that arises from melanocytes can be cured by surgical resection
with almost 80% effectiveness for thin lesions. However, once metastases occur, it is
largely refractory to existing therapies [96]. Several in vitro and in vivo studies as well
as a few clinical reports showed encouraging results of the efficacy of PDT, suggesting
a possible role as an adjuvant therapy in the management of advanced melanoma.
Using different human and mice melanoma cells in vitro and in vivo, significant
apoptosis, necrosis, tumor growth arrest, and prolonged the survival of the animals
has been observed; however, complete remission was rarely achieved and/or was
followed by recurrence and side effects [97–106]. Clinical reports showed regression
of choroidal melanoma and skin melanoma metastasis following PDT [107–109].
PDT-induced acute inflammation attracts leukocytes, mainly neutrophils into the
treated tumors. The pro-inflammatory effects of PDT may increase dendritic-cell
migration, antigen uptake, and maturation. PDT can create a long-lasting tumor-
specific immunity (memory), which was shown in certain mouse and rat models by
the rejection of tumors upon rechallenge [59]. When PDT was applied in combina-
tion with intratumoral injection of naive dendritic cells, eradication of both CT26
colorectal carcinoma cells in BALB/c mice and B16 melanoma in C57BL/6 mice
have been achieved in a significant proportion of animals and among the mice which
the tumors were not cured, prolonged survival was observed. However, neither PDT
nor intratumoral injection of naive dendritic cells alone were effective. It is also
important to note that PDT in combination with intratumoral injection of naive
dendritic cells administered to one tumor site led to tumor regression at distant sites,
including multiple lung metastases [110]. However, optical interference by the highly
pigmented melanin, the antioxidant effect of melanin, the sequestration of PS inside
melanosomes, and the efflux of PS by ATP-binding cassette (ABC) transporters were
all shown to create resistance against PDT, and specific strategies are required to
overcome these resistance mechanisms [111].
10.7.5
Mycosis Fungoides
Mycosis fungoides (MF) is the most common and indolent form of cutaneous T-
cell lymphoma (CTCL) with no curative treatment [112]. Response to the current
treatment options has been disappointing, and patients with advanced CTCL continue
to have poor prognosis [113]. The idea for use of PDT in cutaneous T-cell lymphoma
emerged from earlier reports which demonstrated that PpIX produced from ALA
preferentially accumulated in malignant T lymphocytes [114]. Even though not many,

432
PHOTODYNAMIC THERAPY
a few case reports of successful treatment of CTCL lesions with PDT resulting in
both clinical and histologic clearance further suggest that this modality might be a
good alternative as an additional therapeutic option for patients with lesions resistant
to traditional therapy [115–117]. In order to test the effect of PDT in treatment
of MF, Edstr¨om et al. conducted a study with 10 plaque MF lesions and 2 tumor
MF lesions from 10 patients [118]. Upon topical application of 20% ALA to the
lesion and the adjacent skin for 5–6 h, the lesion was exposed to red light at around
630 nm [118]. Authors reported complete clinical clearance and no recurrence during
the 4–19 months of follow-up for seven of nine plaque lesions, however neither of the
tumor lesions responded to PDT. Moreover, histological examination confirmed the
regression of the infiltrate following treatment.
10.7.6
Acne Vulgaris
Acne vulgaris is a chronic inflammatory skin disorder and is most commonly seen
in adolescents, although it can affect people of any age. Even though pathogenesis
of acne vulgaris has not yet been clarified, the current consensus is that it involves
four main events: follicular hyperconification, increased sebum secretion effected by
the androgenic hormone secretions, colonization of Propionibacterium acnes, and
inflammation [119]. Current treatment options consist of topical retinoids, benzoyl
peroxide, topical or oral antibiotics, and oral isotretinoin for severe cases [120].
Despite the efficacy of oral retinoids and antibiotics in the treatment of acne vulgaris,
there is still a necessity for additional treatment options for patients who are resistant,
intolerant, or unable to take conventional therapies [121]. The bacterium P. acnes
which has been implicated in the pathogenesis of acne is known to naturally produce
endogenous porphyrins, predominantly coproporphyrin III that can act as natural PS
[120]. Given this fact, it was suggested that phototherapy through the absorption of
light (specifically blue light) by porphyrins can cause a photochemical reaction and
form reactive free radicals and 1O2 that in turn cause destruction of the P. acnes and
the sebaceous glands [119,122]. Red light is able to penetrate deeper in tissues when
compared to blue light and it has been demonstrated that red light can affect the
sebum secretion of sebaceous glands and change keratinocyte behavior [123, 124].
Furthermore, red light might also exert its effects by modulating cytokines from
macrophages and other cells, that in turn may reduce inflammation [124,125].
Blue, green [126], and yellow [53] light have all proved to be effective in the
treatment of mild-to-moderate acne vulgaris lesions. Moreover, in the presence of
ALA or MAL, the amount of accumulating porphyrins were shown to increase, espe-
cially if the temperature was raised [54]. ALA and MAL-PDT were both shown to
be effective and safe way of treating acne [55, 56]. Hongcharu et al. reported the
first clinical trial for the treatment of acne vulgaris where he used ALA-PDT with a
550–570 nm broad band light source [57]. Since then there have been a number of
clinical trials which have demonstrated successful treatment of acne with ALA-PDT
with various light sources and regimens [58–61]. In order to increase specificity of
treatment and shorten the incubation time, PDT was suggested to be used with intrale-
sional injection of ALA and results were found to be more satisfactory compared

PDT FOR ONCOLOGY
433
to conventional ALA-PDT [62]. Sun avoidance for at least 48 h after treatment is
necessary due to the risk of post-treatment photosensitivity [63]. In a study conducted
in Korea, a lower concentration of liposome-encapsulated 5-ALA was introduced to
minimize this risk [63]. Improved inflammatory acne with minimal side effects was
achieved by the end of the study [63]. Use of other PS such as indocyanine green
dye (in combination with NIR diode laser—803 or 809 nm) [69] and chlorophyll
[70] for the treatment of acne lesions have also been demonstrated. However, side
effects in PDT treatment of acne vulgaris were commonly reported which included
pain, edema, hyperpigmentation, and blistering rash [127–129]. The clinical trials and
case reports of PDT for acne have been critically analyzed in a review by Sakamoto
and Anderson [71].
10.7.7
Photorejuvenation
Multiple studies have reported improvement in fine wrinkles, mottled hyperpigmen-
tation, roughness, sallowness, and upregulation of collagen production and increased
epidermal proliferation following PDT [130]. In a small randomized split-face study
among 10 patients with moderate photodamage, MAL-PDT (1 h vs. 3 h incubation)
was shown to improve tactile roughness, fine lines, and skin tightness in most of the
patients on the side treated after 3-h incubation [131]. The same group also evaluated
the effect of a combination treatment where MAL-PDT was applied to one half of the
perioral area following fractional photothermolysis to both sides of the face and the
same procedure was repeated at 3 weeks [132]. Improvement in superficial rhytides
and overall patient satisfaction was reported to be greater in the combined treatment
side [132]. Another split-face study but this time with PDL applied to both sides of
the face, 1 h after ALA was applied to one side, improvement in softness and texture
and disappearance of solar lentigines was observed on the PDT-PDL side [133].
10.8
PDT FOR ONCOLOGY
PDT is a modality that has been applied for many types of malignant cancers occurring
in the head and neck, breast, brain, lung, pancreas, peritoneal cavity, prostate, and
skin. Though PDT was discovered in the beginning of the twentieth century, it
took 70 more years to apply this technology to cancerous tumors. Diamond et al.
hypothesized that the tumor localizing and tumor-phototoxic properties of porphyrins
might be exploited for killing cancer cells. In vivo studies on rats showed encouraging
results by suppressing the growth of gliomas for 10–20 days, but eventually other
viable cancer cells from deeper regions started to proliferate [134]. It was not until
1975, that PDT was taken as a serious research topic in oncology, when scientists
achieved complete eradication of mammary tumors in mice with hematoporphyrin
derivative (HPD). In the same year, there was another significant development when
the same compound was used to completely eliminate bladder carcinoma in mice,
with red light activation of HPD [135]. Human trials were further carried out in
the following year when five patients were diagnosed with bladder cancer using

434
PHOTODYNAMIC THERAPY
HPD. PDT with HPD was tried in another human trial where the bladder carcinoma
was recurrent in nature. PDT slowed down the tumor growth in this case where the
transurethral resections, radiotherapy, and chemotherapy had failed. In a follow-up
study with 25 patients diagnosed with 113 tumors, it was found there was complete
response in 98 of the tumors, 13 of them showed partial response, and 2 of them
showed none [136]. Following this many research groups started experimenting with
PDT of tumors in various other parts of the human body.
The next tumors to be treated with PDT were lung tumors [135]. This was followed
by esophageal cancer [137], gastric carcinoma [138], brain tumors [139], intraocular
tumors [140], breast cancer [141], head and neck tumors [142], colorectal cancer
[142], cutaneous malignancies [143], intraperitoneal cancer [144], mesothelioma
[145,146], and pancreatic cancer [147]. The whole spectrum of the known malignant
tumors has been studied by PDT experimental approaches by altering various dose-
time-concentrations of different PS and light. However, PDT has had its limitations in
its potency and specificity of the PS. Another issue at hand was the diagnosed patients
were often already at an advanced stage and the tumors had already become refractory
to other treatment modalities. Though testing the PS on highly advanced malignant
tumors was not an ideal way of testing the potential of PS against tumors, often it
was the only avenue available for this experimental therapy. PDT is more a local
therapy than a systemic therapy like chemotherapy. So, PDT is not effective in
targeting the metastatic cancer cells that had not yet developed into a tumor. Moreover,
penetration of light has always been an issue when it comes to treating tumors in
various organs in the body.
Since the early days a lot of changes have been made to PS to increase their speci-
ficity and potency for tumors. More powerful and selective PS have been developed
since then and many are now available in the market. The tumor photodamage and
tissue photocytotoxicity (expected downside of PDT) can be controlled by light expo-
sure dose, fluence, wavelength of light used, oxygen availability, and time between
administration of drug and the light exposure.
It is well known that the killing of cancer cells through PDT occurs by one or more
of three pathways, that is, necrosis, autophagy, or apoptosis. Here, the ROS generated
by PDT when the PS are located inside tumor cells the kills them directly. PDT also
affects the tumor vasculature leading to tumor infarction. Finally PDT can induce an
immune response that leads the host immune system to kill the tumor cells. These
three mechanisms are not separate from each other and all occur at the same time.
Contributions from all three mechanisms and particularly from the immune system
component are needed over a long period of time for complete eradication of the
tumor growth and to prevent tumor recurrence.
The direct tumor killing is thought unlikely to be responsible for complete tumor
eradication as there is a nonhomogenous distribution of the PS in and around the
tumor. The PS in this case is either delivered intravenously and accumulates via
the EPR effect. Another possibility is that the PDT effect in and around the tumor
reduces the ambient oxygen supply thus affecting the tumor-cell destruction. The
photochemical process of the oxygen consumption leaves a gradient of oxygen con-
centration contributing to the incomplete killing. This rapid depletion in oxygen in

PDT FOR INFECTIOUS DISEASE
435
the PDT-treated tumor may be both a blessing and a curse. On the one hand, the acute
hypoxia after vascular shut-down adds to tumor destruction and necrosis, while on the
other oxygen consumption limits the effectiveness of ongoing PDT and may induce
hypoxia sensitive tumor survival pathways [148]. This is an area of PDT mechanism
that needs considerably more study.
10.9
PDT FOR INFECTIOUS DISEASE
It has been known since the first days of PDT, more than one hundred years ago,
that different microorganisms can be killed by the combination of dyes and light
in vitro [149–151]. Since those days the combination of light and nontoxic dye in the
presence of ambient oxygen which underlies PDT has been primarily developed as
a treatment for cancer, ophthalmologic disorder, and in dermatology. However, the
inexorable increase of antibiotic resistance of pathogenic bacteria due to the excessive
and sometimes inappropriate use of antibiotic has led to a focus of attention on PDT
as an alternative treatment for infectious disease [152].
Antimicrobial PDT may be a new approach to killing or eliminating pathogens
such as bacteria, fungi, protozoa, yeasts, and multicellular parasites [153]. Some
advantages of PDT compared to other alternative antimicrobial methods are it is a
noninvasive method and there have been no reports of induction of resistance to PDT
in any microbes where it has been tested [154]. Moreover, all the studies that have
examined the killing of antibiotic resistant bacteria have demonstrated them to be
equally susceptible to PDT destruction as their na¨ıve counterparts [155]. Furthermore,
PDT kills microbes almost instantly while antibiotics can take hours or days, and PDT
is effective in infections in tissues with compromised blood supply where systemic
antibiotics do not penetrate.
10.9.1
In Vitro Studies of PDT
In the 1990s, it was observed that there was a fundamental difference in susceptibility
to PDT between Gram-positive and Gram-negative bacteria; neutral or anionic PS
molecules were effective to inactivate Gram-positive bacteria, whereas they are less
effective or fail against Gram-negative bacterial cells [156]. The high susceptibility of
Gram-positive species compared to Gram-negative is explained by their structural and
physiological differences, in particular differences in the structure of the bacterial cell
wall. The cell wall of Gram-negative bacteria consists of an inner cytoplasmic mem-
brane and an outer membrane that are separated by the peptidoglycan-containing
periplasm. The outer membrane forms a physical and functional barrier between
the cell and its environment. This complex structure means Gram-negative bacteria
are less permeable to many PS. Several studies have been performed to test new
approaches to permit a better PS penetration and improve killing of Gram-negative
bacteria using PDT. Nitzan et al. used the polycationic peptide polymyxin B non-
apeptide (PMBN), to increase the permeability of the Gram-negative outer membrane

436
PHOTODYNAMIC THERAPY
and allowed PS to penetrate in the cells where the ROS generated during illumination
can cause fatal damage [157].
Malik et al. also studied a mixture of hemin and DP as PS [158]. The effect of
the combined mixture was stronger than separate constituents and was as strong in
the dark as under illumination [156]. Bertoloni et al. found that the use of ethylene-
diaminetetraacetic acid (EDTA) to release LPS or the induction of competence with
calcium chloride sensitized E. coli and Klebsiella pneumoniae to PDT by hematopor-
phyrin or zinc phthalocyanine [159]. A different approach adopted by other groups
was to use a PS molecule with an intrinsic positive charge. Methylene blue [160],
toluidine blue O (TBO) [161], phenothiazinium dyes [162,163], or cationic phthalo-
cyanines [164] or cationic porphyrins [165, 166], combined with appropriate light
have a bactericidal effect on both Gram-positive and Gram-negative bacteria. It has
been demonstrated that for the PS to be effective, it can just come into close contact
with the cells. In this case, if 1O2 can be generated in sufficient quantities near to the
bacterial outer membrane, it will be able to diffuse into the cell to inflict damage on
vital structures [167].
Many improvements have been reported in several in vitro studies, to make PDT
more effective against Gram-negative bacteria. Choi et al. compared the effect of ALA
performing illumination with blue and red light using a LED device against P. acnes.
Propionibacterium acnes was killed with both PDT and by light alone; in particular,
blue light was more effective due to the high natural content of porphyrin in the bac-
teria. The pretreatment with ALA increased significantly the killing of P. acnes [168].
Huang et al. reported that polyethylenimine [169] covalently conjugated to chlo-
rin(e6) improved the permeability of the bacteria to the PS and increased the ability to
kill Gram-negative bacteria. In a successive study [169], different sized polyethylen-
imine polymers to form PEI-ce6 conjugates were tested and they showed an improve-
ment of ce6 PDT activity and also PDT effect of all three conjugates investigated
depended on pH values [170].
Another group investigated the effect of PDT using a tetracationic mesoarylsubsti-
tuted porphyrin (RM24) on Pseudomonas aeruginosa, one of the less susceptible bac-
teria to PDTt. The results showed that photoeradication is dependent on PS concentra-
tions, cellular density, and light dose. RM24 was able to induce oxidative stress mainly
by 1O2 formation. A standardized experimental condition of a spot test was used to
study PDT sensitivity differences among three strains of P. aeruginosa [171,172].
10.9.2
Photoinactivation of Viruses, Fungi, and Parasites
Many in vitro PDT studies on viruses have been performed to allow sterilization of
blood or blood products [173,174]. The reports concluded that lipid-enveloped viruses
weremoresusceptibletoPDT thannonenvelopedstrains. Manyof theclinical applica-
tions of PDT for localized infections have been performed on lesions of viral etiology.
We will discuss more detailed about in vivo study and clinical trials in next section.
Several works have focused on the use of PDT to kill yeasts and fungi in vitro.
Compared with bacteria, studies have shown that various species of yeast and fungi are
much less susceptible [175]. Opportunistic fungal pathogens represent an increasing

PDT FOR INFECTIOUS DISEASE
437
problem for human health especially in immunocompromised patients, due to difficult
diagnosis and the fact that not many antifungal drugs are available. The interest in PDT
as an alternative antifungal treatment is increasing. Many studies have tried to identify
the photochemical and photophysical mechanisms involved in photoinactivation; not
only to develop potent and clinically compatible PS but also to understand how
photoinactivation is affected by key microbial phenotypic elements (such as multidrug
resistance and efflux, virulence factors, and pathogenesis determinants) [175].
Human pathogenic parasites have also been killed by combinations of PS and
light. Plasmodium falciparum, responsible for malaria, has been killed using N-
(4-butanol)pheophorbide derivative [176] and also by silicon phthalocyanines [177].
Trypanosoma cruzi has been killed by the combination of light and PS such as hemato-
porphyrin and aluminum-sulfonated phthalocyanine [178]. Human helminth eggs in
wastewater were inactivated by a cationic meso-substituted porphyrin and light [179].
10.9.3
Animal Models of Wound Infections
Over the years, numerous studies have been designed to investigate the effects of PDT
on prevention and treatment of infectious diseases utilizing animal models of infec-
tion, to mimic closely the real disease observed in humans. In in vivo studies of PDT,
one of the major difficulties is monitoring the development of the infection in animal
models and its response to treatment. To facilitate the noninvasive monitoring of
animal models of infection, Hamblin and his group developed a procedure that uses
bioluminescent genetically engineered bacteria and a light-sensitive imaging system
to allow real-time visualization of infections also developing several mouse models
of localized infections that can be followed by bioluminescence imaging (Figs. 10.5
and 10.6). It is common for wounds to become infected with different, clinical rele-
vant bacterial strains. Different animal wounds have been used to mimic the wounds
observed in humans such as surgical-site wounds, burns, dermal excision, incision,
abrasions, lacerations, and open fractures. Hamblin et al. [172,180] reported the first
time the use of mouse wound infection models to investigate the effect of PDT on treat-
ing excisional wounds infected with Escherichia coli and Pseudomonas aeruginosa.
Many wound models such as abrasion and surgical-site wounds infected with differ-
ent Gram-positive and Gram-negative bacterial strains have been used to investigate
the efficacy of PDT using different PS to investigate also their ability to kill bacteria
[53,181,182]. Wong et al. [162] and Zolifagehari et al. [183] studied in animal models
of wounds, the effect of methylene blue and TBO mediated PDT on Vibrio vulnificus
and methicillin-resistant Staphylococcus aureus wound infections, respectively.
Burns very commonly undergo infection. In past years, the majority of patients
with serious burns died from related infections. Pathogens like Pseudomonas aerug-
inosa, S. aureus, and filamentous Candida are frequently responsible for the burn
infections.
Using a guinea pig model, Orenstein et al. [184] studied the effect of porphyrins
on the eradication of S. aureus in burns. The wounds, third-degree burns, induced on
the back of guinea pigs were infected with 108 CFU of S. aureus. A reduction of 99%
of the viable bacteria was achieved treating with PDT.

438
PHOTODYNAMIC THERAPY
FIGURE 10.5
Bioluminescent MRSA infection in mouse skin abrasion. Successive bio-
luminescence images of a representative mouse skin scratch model infected with 108 CFU
MRSA. Reprinted with permission from Reference 53. (For a color version of this figure, see
the color plate section.)
Lambrechts et al. [185] and Dai et al. [186] employed mouse models to evaluate
PDT for S. aureus and Acinetobacter baumannii burn infections, respectively. Burns
were created on the backs of mice by applying two preheated (95◦C) brass blocks on
the backs of mice for 10 seconds. PDT inactivated significantly both S. aureus and
A. baumannii.
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
FIGURE 10.6
PDT treatment of MRSA infection in mouse skin abrasion. (a–e). Successive
bioluminescence images of a representative mouse skin scratch model infected with 108 CFU
MRSA treated with (a) light alone, (b) PDT using RLP068/Cl, or (c) PDT using TBO at
30 min after bacterial inoculation + 15 min from PS application. PDT was carried out with a
combination of 75 μL PS and 84 J cm−2 red light, (d) RLP068/Cl or (e) TBO alone after 30 min
from bacterial inoculation + 15 min after application of PS. Animals were imaged at 2, 6, and
14 min, without to be exposed to light, (f) Dose response of mean bacterial bioluminescent of
mouse scratch abrasion infected with MRSA after treatment with light alone, RLP068/Cl and
TBO dark controls and PDT using RLP068/Cl or TBO assuming as TIME 0 the end of 10 min.
Reprinted with permission from Reference 53. (For a color version of this figure, see the color
plate section.)

PDT FOR INFECTIOUS DISEASE
439
(a)
(b)
(c)
(d)
(e)
(f)
30′ after
bacteria
inoculation
30′ after
bacteria
inoculation
Time 0
10′ after PS
application
Time 0
10′ after PS
application
2 min
6 min
14 min
12 J cm–2
36 J cm–2
84 J cm–2
1
0.1
0.01
0.001
Light control
DC/RLP068
PDT/RLP068
DC/TBO
PDT/TBO
0.0001
0
12
24
36
Remaining fraction of RLU
Light exposure (J/cm2)
48
60
72
84

440
PHOTODYNAMIC THERAPY
Other studies investigated the effect of PDT in soft tissue infections. This kind of
infections can induce a high risk of mortality. A commonly used animal model of a
localized soft-tissue infection is the intramuscular injection of a bacterial suspension
into the mouse thigh muscle. This has been carried out with many bacterial species
including E. coli, Pseudomonas aeruginosa, and S. aureus. Berthiaume et al. [187]
investigated the effect of PDT against Pseudomonas aeruginosa using bacteria mixed
in vitro with the tin(IV) chlorin e6-monoclonal antibody conjugate and shining the
light after injection. The significant decrease of bacteria was demonstrated. Gad et al.
[188] used luminescent bacteria and studied the PDT for S. aureus infection in soft
tissue. Staphylococcus aureus cells were used to infect the wound and found that
there was a light dose-dependent reduction of bioluminescence.
10.9.3.1
Clinical Applications of PDT in Infected Wounds
Not many clinical tri-
als of PDT have been reported in the treatment of localized bacterial infections. In one
trial, five patients with brain abscesses after craniotomy and surgical drainage were
treated with hematoporphyrin into the abscess followed by illumination. The study
produced a positive clinical response [189]. Preliminary results have been obtained
from a UK-based study on the effect of PDT mediated by a new phenothiazinium
derivative (PP904) when applied on no healing leg ulcers infected with bacteria.
Brown et al. (2009) found a reduction of bacteria and an improvement in wound heal-
ing. These are only preliminary results that need to be confirmed [190]. A study case
has been reported by Clayton and Harrison [191], a significant improvement in the
healing of an infected leg ulcer was observed in a 72-year-old patient who was treated
with topical ALA-PDT twice weekly over 4 weeks. More studies are necessary to
clarify how much of this improvement was due to the antimicrobial effect.
10.9.4
Oral and Dental Infections
Many in vitro, in vivo, and ex vivo studies have been carried out in oral and dental
infections. Peri-implantitis, periodontitis, periodontal diseases, endodontic infection,
and oral candidiasis are the most common oral infections and several studies have
been carried out to verify the efficacy of PDT against the main bacteria responsible
of this kind of infections.
Significant reduction of Actinobacillus actinomycetemcomitans, Porphyromonas
gingivalis, and Prevotella intermedia was reported in PDT treatment of peri-
implantitis, using TBO [192]. Moreover, in peri-implantitis dog models have been
demonstrated a reduction of Prevotella sp., Fusobacterium sp., and Streptococcus
betahaemolyticus. Bacteria were not 100% destroyed in all samples [193] but a
second study showed a 99.8% reduction of Prevotella [194].
In in vivo models [169,195–197] of periodontitis, different PS have been tested.
TBO significantly reduced P. gingivalis [169], a similar effect was observed in a
beagle dog model using chlorin(e6) as PS, whereas BLC1010 (a new chlorin(e6)
photosensitizer) did not kill bacteria during the PDT test [196].
Recently, Bottura et al. demonstrated the effect of PDT on reducing bone loss
caused by experimental periodontitis in immuosuppressed rats [198]. In another

PDT FOR INFECTIOUS DISEASE
441
study [16], the effect of PDT on inflammatory cells was evaluated. The treatment of
chronic periodontitis by PDT leads to a specific decrease of antigen-presenting cells
populations according to the drug delivery system. The incomplete eradication of
biofilm microorganisms, observed in periodontal diseases, could be due to restricted
penetration of PS in oral biofilm [199] or the ability of bacterial cells to expel PS via
multidrug resistance pumps [200]. To improve the penetration of the PS, poly(lactic-
co-glycolic) (PLGA) nanoparticles were used as carriers of MB. The results suggested
that PLGA nanoparticles had the potential to be used as nanocarriers of MB, to diffuse
in biofilms and release the encapsulated drug in active form [201].
Ex vivo extracted teeth have been used to test PDT for endodontic infections. Ente-
rococcus faecalis is the pathogen most commonly associated with current endodontic
infections [202]. Several studies have been carried out to evaluate the effect of tolui-
dine blue [203,204], methylene blue [204–206], and a conjugate between polyethylen-
imine and chlorin(e6) [207] with PDT in endodontic treatment. The effect of PDT
was also evaluated on the inactivation of C. albicans in oral candidiasis. Mima et al.
evaluated the efficacy of using hematoporphyrin with PDT in an animal model of
oral candidiasis [208]. Candidiasis was verified by the presence of white patches or
pseudomembranes on the dorsal tongue associated with a significant number of CFU
per mL of C. albicans. PDT significantly reduced the viability of C. albicans. In a
previous study, a dose-dependent photoeradication of C. albicans in an immunode-
ficient murine model was described [209]. In this study, Teichert et al. evaluated the
efficacy of using methylene blue (MB)–mediated PDT to treat oral candidiasis in an
immunosuppressed mouse model, mimicking the disease found in human patients.
The results indicated a MB-dependent effect of PDT, and completed eradication of
fungi from the oral cavity.
10.9.4.1
Clinical Applications of PDT in Oral and Dental Infections
PDT for
dental infectious represents the largest growth area of clinical antimicrobial PDT. This
is because three companies are actively involved in clinical trials and are marketing
therapies that are still relatively unknown among the general medical and dental
profession. In PDT of periodontitis, the PS is usually injected into the dental pocket
followed soon after by light delivery into the dental pocket using a narrow fiber optic
tip. de Oliveira et al. [195] compared PDT with TBO with scaling and rooting planing
(SRP) using manual instruments in 10 patients with aggressive periodontitis. Both
treatments had similar clinical effects. Additional application of a single application
of PDT to SRP resulted in significantly higher reduction of bleeding scores than
using SRP alone [210,211]. Braun et al. [212] reported PDT administered after SRP
treatment in patients with Fusobacterium nucleatum infected periodontitis reduced
periodontal inflammatory symptom and successfully treated F. nucleatum infection
[213]. Ruhling et al. [214] compared PDT using TBO with ultrasonic debridement in
persistent pockets of maintenance patients and found that both therapies performed
similarly. Another application of antimicrobial PDT in dentistry is in the sterilization
of endodontic root canals in patients who are being treated for necrotic pulp and
periapical lesions. PDT can be combined with the usual mechanical debridement
and chemical antimicrobials. Pinheiro et al. [215] reported a significant reduction in

442
PHOTODYNAMIC THERAPY
viable bacteria using TBO + urea peroxide preparation and red light in addition to
mechanical instrumentation to sterilize root canals in children with deciduous teeth
with necrotic pulps.
10.9.5
Leishmania
Leishmania parasites can lead to mortality and morbidity. Mucocutaneous leish-
maniasis can lead to infections that may be restricted to cutaneous sites involving
the multiplication of microorganisms in the mucous membranes. Also, in visceral
leishmaniasis, they can spread throughout the reticuloendothelial system.
Akilov et al. [216–218] reported the use of a mouse model to study the efficacy of
PDT for cutaneous leishmaniasis. Parasites were inoculated intradermally into each
ear of mice. PDT was performed 3 weeks after infection. Two PS were used, (3,7-
bis(N,N-dibutylamino)-phenothiazinium bromide (PPA904) and ALA-PPIX. Mice
were sacrificed 5 days after PDT and the load of parasites was quantified. Using both
PS reduction of the parasite was observed, but in case of ALA there was also tissue
destruction.
10.9.5.1
Clinical Applications of PDT in Leishmaniasis
Leishmaniasis is a dis-
ease caused by protozoan parasite transmitted by bite of certain species of sand fly.
Cutaneous Leishmaniasis is the most common form of Leishmaniasis. A weekly
treatment of 10% ALA PDT with red light (630 nm, 100 J cm−2) was more effec-
tive than topical paromomycin [219]. PDT with 75 J cm−2 red light performed for
12 weeks also showed good results [220]. Leishmania tropica infection which proved
to be resistant to various therapeutic regimes was effectively treated by PDT [153].
10.9.6
Mycobacterium tuberculosis
Mycobacterium tuberculosis manifests as latent infection or progressive contagious
disease. The predominance of these infected cases is in the latent form; the remainder
is active and often contagious.
O’Riordan et al. [221, 222] developed a mouse model of localized mycobacte-
rial infection and used PDT to treat this infection in vivo. A subcutaneous pocket
made in a line along the dorsal surface of each mouse and collagen implants were
placed at either side of the dorsal midline and the incisions then closed with nylon
sutures. Three PS were tested in the studies: verteporfin, benzo[a]phenothiazinium
chloride, and benzo[a]phenoselenazinium chloride. Monitoring with real-time fluo-
rescence imaging was used to track the delivery of the PS to the infected sites. The
concentration of 105 CFU of M. bovis induced granulomas, a significant reduction in
viable mycobacterial cells was demonstrated in PDT-treated granulomas.
10.9.7
Otitis Media with Effusion (OME)
Bacterial infections contribute to the OME a diffuse infection among children. By
using a gerbil model, Jung et al. [223] evaluated the antibacterial effects of PDT

PDT FOR INFECTIOUS DISEASE
443
in vivo on Haemophilus influenzae and Streptococcus pneumoniae, responsible for
OME. Bacteria were injected through the bullae and PDT using Photogem solution
was performed 2 days after the infection by laser illumination. The results showed
a significant reduction of both bacterial strains; in particular, PDT was effective in
killing Streptococcus pneumoniae in 87%, whereas it was effective in eradicating
H. influenzae in 50% of the infected bullae with OME.
10.9.8
Osteomyelitis
Other studies have focused on the effect of PDT in the treatment of osteomyelitis. In a
study, Bisland et al. [224] used a bioluminescent strain of biofilm-producing S. aureus
grown onto kirschner wires (K-wire). Staphylococcus aureus-coated K-wires were
exposed to methylene blue (MB) or ALA-mediated PDT either in vitro or following
implant into the tibial medullary cavity of Sprague-Dawley rats. The infection was
monitored using bioluminescence. Staphylococcus aureus infections were subject to
PDT 10 days post inoculation. Treatment comprised administration of ALA resulted
in significant delay in bacterial growth and inhibited biofilm formation on implants
in bone.
10.9.9
Viral Infections
Another relevant field that has been investigated is PDT for viral infections. Smetana
et al. [225] inoculated herpes simplex virus (HSV) on the backs of guinea pigs
resulting in a local infection after 24 hours. The treatment with ALA-PDT either
immediately or up to 6 hours after infection produced a beneficial effect. The treatment
at this time determined no HSV could be isolated whereas ALA-PDT, 2 days after
infection, had no effect on the HSV titer.
10.9.9.1
Clinical Applications of PDT in Viral Infections
The first clinical tri-
als for PDT in infectious disease were directed toward viral lesions. Topical PDT
was commonly tested to treat herpes simplex lesions. Proflavine PDT was able to
inactivate herpes keratitis [226]. However, other studies showed PDT to be ineffec-
tive [227–229]. Several clinical trials have been conducted using PDT against viral
infections, especially human papilloma virus (HPV). Systemic and topical PDT has
been used to treat papillomatosis caused by HPV. Systemic treatment of 48 patients
significantly decreased papilloma growth rate compared to control patients. A follow-
up for 3 years of subset of patients confirmed that the improvement was maintained
[230]. Abramson et al. [231] and Bujia et al. [232] reported similar results. ALA-PDT
has been largely used to treat cutaneous warts caused by HPV. In a study, 42 of the
48 plantar warts in 31 patients, who received ALA, showed complete response and
no significant side effects postoperation. In another study, white light was shown
to be more effective compared to red or blue light and also standard cryotherapy in
30 patients recalcitrant warts [233]. ALA-PDT achieved excellent results also on three
patients with recalcitrant facial warts [234]. It has been suggested that the beneficial
effect of treatment of HPV infections by PDT is because of its anti-inflammatory and

444
PHOTODYNAMIC THERAPY
antiproliferative effects in the lesions [235]. The efficacy of the PDT treatment has
been tested also in condyloma acuminata, the genital warts caused by HPV. In women,
HPV can also infect the uterine cervix and may lead to the development of cervical
intraepithelial neoplasia (CIN) and cervical cancer depending on the virus subtype.
PDT with polyhematoporphyrin improved cytological measures when treating CIN
and also eradicated HPV [236]. Similar results were obtained using photolon [237]
and 5-ALA [238]. Topical application of 5-ALA PDT also successfully treated 13 of
the 14 cases of anogenital condyloma accuminata [239]. Topical ALA or MAL-PDT
has also been used to treat condyloma in the vulva, vagina, and penis wherein selec-
tive accumulation of PPIX was demonstrated in the lesions [238,240]. The response
observed after PDT treatment in genital lesions could dependent on the immune
response as discussed in the study of Abdel-Hady et al. [241].
A study [242] on PDT mediated by benzoporphyrin derivative was used in a
photopheresis technique in 10 HIV-1 infected patients. In photopheresis, isolated
leukocyte-rich fraction (buffy coat) from the blood was incubated with BPD for
30 minutes and exposed it to UVA light in a specialized photopheresis apparatus
and reinfused back to the patient. In the first protocol of this study, the pheresis was
repeated for a total of nine treatments. Subsequently, the protocol was changed for a
total of 24 pheresis sessions. Three patients who had rapidly rising viral loads prior
to initiating therapy stabilized. Two had a sustained greater than 0.5 log decrement
and five had stable plasma viral loads (less than a 0.5 log increment or decrement)
with varied effects on absolute CD4- and CD8-positive lymphocyte counts. One
patient achieved a greater than 1.0 log decrement in HIV-1 plasma viral load and
undetectable in vivo cell-free and cell-associated HIV-1 infectivity with an increased
in vitro lymphocyte mitogen stimulation index.
10.9.10
Clinical Applications for Peptic Ulcer Disease (PUD)
Another field of application of PDT for clinical trials has been PUD, a human
disease of gastrointestinal tract with the presence of ulceration in mucosa layer,
usually associated with Helicobacter pylori infection. Wilder-Smith et al. performed
the first clinical trial [243] irradiating with blue and white light 13 HP-positive
volunteers in a zone of gastric tantrum HP. The number was greatly reduced in
biopsies treated with ALA and blue or white light compared to control. Hamblin
et al. [244] showed that H. pylori naturally accumulates the photoactive porphyrins,
coproporphyrin, and protoporphyrin, thus the bacterial cells should be intrinsically
sensitive to photoinactivation without any added PS, especially when blue light is
applied. On this basis, Ganz et al. [245] used endoscopically delivered blue light
(405 nm, 40 J cm−2) to eradicate H. pylori in regions of gastric antrum in 10 patients
who were positive for the bacterium. In a pilot study on 18 patients with H. pylori
infection, the whole stomach was illuminated with blue light. They obtained largest
reduction in bacterial load in the antrum of the stomach (>97%) followed by body
(>95%) and fundus (>86%) [246]. However, lasting eradication of bacteria was not
achieved with a repopulation of bacteria in days following illumination.
PDT to treat infections is in its infancy and there are some limitations that need
to be overcome. PDT for infection is limited to the areas of body where light can

PDT IN OPHTHALMOLOGY
445
be delivered relatively easily such as skin and body cavities. More study will be
necessary to find new methods to administer PS and to delivery the light. Moreover,
for clinical applications, it is necessary to optimize PS that need to be approved.
The clinical applications of antimicrobial PDT have been slow, however although a
few clinical trials have been carried out for different infectious diseases, the positive
results obtained in periodontitis gives hope that PDT could be used to clinically treat
a higher number of different kind of infectious diseases.
10.10
PDT IN OPHTHALMOLOGY
PDT was first approved 10 years ago for age-related macular degeneration (AMD).
Thereafter, it was approved by choroidal neovascularization (CNV) in pathological
myopia. It is a minimally invasive treatment and has an excellent side effect profile.
The treatments of idiopathic choroidal neovascularization, secondary CNV in inflam-
matory diseases of the retina and choroid, choroidal haemangioma, vasoproliferative
tumors, malignant melanoma of the choroid, and central serous chorioretinopathy
with PDT was performed with some remarkable results; however, in this section we
will mainly focus on CNV associated with AMD [247].
In neovascular or wet AMD, new vessels grow under the retina distorting vision and
also leading to scarring [248]. This is exacerbated if the blood vessels leak [248]. Laser
photocoagulation and recently introduced antibody therapies are the only treatments
for AMD with proven long-term benefits; however, there are certain limitations for
laser photocoagulation treatment [249]. First, only 10–15% of all neovascular lesions
are small enough and sufficiently delineated by fluorescein angiography to be eligible
for laser treatment. Second, even if laser treatment is initially successful, there is at
least 50% chance that leakage will reoccur within the next 2 years. Fortunately, with
careful monitoring following laser treatment, when detected early recurrences are
amenable to additional laser treatment. Third, at least half the patients with sufficiently
well-circumscribed neovascular lesions have some initial leakage beneath the center
of the fovea.
PDT has been investigated for treatment of neovascular membranes without affect-
ing the retina. The advantage of PDT is that the process of destruction can be confined
to the area of choroidal neovascularization by the intravenous administration of a PS
and irradiation of solely this region. PDT with verteporfin (Visudyne®) was the first
approved therapy for treating the subfoveal lesions and is usually administrated in
liposomal formulation [250,251]. There are multiple aspects of the selectivity in the
angio-occlusion by PDT [249] concerning vascular damage and the subsequent blood
flow stasis. This selectivity is inherent in the short time interval between the drug
injection and the light application (i.e., 15 minutes after the start of the intravenous
injection of the Visudyne, when the 83-second irradiation takes place, most of the
drug that is in the retina is still within the blood vessels, and more specifically, on or
in the endothelial cells lining the CNV that are to undergo the angio-occlusion. This
ensures that the PDT effect takes place only in the endothelium being irradiated. A
second possible reason for the observed selectivity is that retinal vessels will empty
of Visudyne fluorescence significantly faster than the choroidal vessels. Thus at the

446
PHOTODYNAMIC THERAPY
time of irradiation, the retinal capillaries appear to have much less drug in them
than the CNV and choroidal vessels in general. This may well imply a significant
level of protection of the retinal capillaries. Third, retinal capillaries are protected
by the blood retinal barrier, which signifies among others tighter junctions between
endothelial cells and hence probably more resistance to PDT.
The effect of AMD with PDT has been investigated by treating subfoveal CNV
which had some fraction of classic CNV [252, 253]. Other inclusion criteria in the
study was greatest linear diameter of less than or equal to 5.4 mm, and a visual
acuity at baseline between 20/40 and 20/200. The best results were achieved for
the predominantly classic lesions. Thirty-one percentage of the placebo group lost
less than three lines (of visual acuity), whereas 59% of the PDT-treated eyes lost
less than three lines (of visual acuity). Large visual acuity losses (i.e., more than
6 lines or 30 letters) were relatively rare in the PDT-treated group as compared to
more than one-third of the patients (36%) in the placebo group. Improvement in
visual acuity by more than three lines was found in 9% of the PDT-treated patients
at the end of this 2-year study and was rarely observed in the placebo group. In the
Verteporfin-In-PDT (VIP) study, patients were enrolled with occult subfoveal lesions
[254–256]. Again, losses in visual acuity were statistically significantly less in the
PDT-treated group as compared to the placebo group, with the best results obtained
for the smaller lesions. Outcome, in terms of letters/lines gained or lost, was also
better for those patients with the worse visual acuity at baseline.
Data were obtained from a large fraction of the previous patient groups (both the
placebo and PDT-treated groups), during a 3-year time period and results demon-
strated that the mean change in vascular acuity from baseline was quite similar at
24 months (−1.5 lines) and at 60 months (−1.6 lines) and no additional safety issues
were noted [257]. The treatment rate (number of treatments/year), which was decided
based on fluorescein angiography (i.e., leakage), was significantly lower in the third,
fourth, and fifth years as compared to the first 2 years. These data suggest that PDT
for treating CNV associated with AMD remains a standard therapy for patients with
recent progression of wet age-related macular degeneration with subfoveal CNV.
The effects of PDT on subfoveal or juxtafoveal (CNV) secondary to pathologic
myopia with verteporfin in Japanese patients were compared with the visual outcomes
of PDT-treated patients with that of age-matched and visual acuity-matched untreated
controls. In this study, 43 eyes of 42 patients with pathologic myopia were treated
with PDT for their myopic CNV. The patients were monitored continually for a period
of little over than a year taking the age and initial acuity, by matching them with the
untreated controls. The results indicated that PDT was beneficial for maintaining
vision in Japanese patients with myopic CNV. The visual outcome after PDT was
better than the natural course of the disease as compared with the controls [258].
10.11
PDT AND THE IMMUNE SYSTEM
The unique properties of PDT may offer the hope of combination approaches such as
PDT plus activation of the immune system during antitumor PDT. The mechanisms

PDT AND THE IMMUNE SYSTEM
447
that operate to allow PDT to destroy tumors are multifactorial. The photophysical
mechanism mentioned in Section 10.2.1 plays a major role in triggering the immune
system because many effects of PDT on cancer cells that are grown in tissue culture
have been reported that, if replicated in vivo, would make activation of the immune
system probable after PDT treatment in patients as the combination of PS with
their activating light causes an unusual mixture of apoptotic and necrotic cell death
[259–261]. There is also an effect against the tumor vasculature whereby illumination
produces the shutdown of vessels that subsequently deprives the tumor of oxygen and
nutrients [259]. PDT has effects on cancer cells that make immune activation more
likely in an in vivo tumor treated with PDT. PDT can induce strong expression of
heat shock proteins (especially HSP70) [262,263] that has been shown to potentiate
immune recognition of tumors. PDT can cause activation of the transcription factors
such as nuclear factor kappa B (NF𝜅B) [264] and activator protein (AP)-1 [265],
leading to the production of a large variety of inflammatory mediators including
eicosanoids and interleukins (ILs) 1, 6, 8, and 10. Neutrophils are an important cell
type for the PDT response [266] and if mice are depleted of neutrophils before
PDT, the curative effect is lost [267]. PDT has been shown to induce both a systemic
neutrophilia and a strong and prolonged tumor infiltration by neutrophils. In addition,
tumor infiltration by dendritic cells (DCs), macrophages, and mast cells has been
observed [268]. Complement activation is also observed both in the tumor and serum
after PDT [269]. Nevertheless, it is clear that although PDT has the potential to
stimulate a systemic antitumor immune response in animal models of cancer, this
favorable result is not always observed. The explanation for this observation may be
due to variations in the immunogenicity and antigen expression of different syngeneic
mouse tumors, the presence of immune suppression caused by regulatory T-cells
(Tregs), or the existence of DC dysfunction caused by the tumor.
There are extensive studies that examine the pathways of apoptosis that are induced
after PDT in both normal and tumor cells in tissue culture, such as signaling pathways,
mitochondrial events, and mediators of apoptosis. Many studies have examined the
relationship between the mode of tumor cell death (by methods other than PDT) and
the efficiency of induction of the immune response both in vitro and in vivo [270,271].
Sluiter et al. showed the importance of neutrophils in producing an effective response
to PDT in some (but not all) tumor models. They demonstrated that neutrophils
adhere to the microvascular wall after PDT in vivo, but PDT did not stimulate the
expression of P-selectin (one of the principal adhesion molecules that bind leukocytes)
by endothelial cells (ECs). The ECs retracted after PDT, which enabled neutrophils to
adhere to the subendothelial matrix by their β2-integrin adhesion receptors, and this
could be blocked by anti-β2-integrin antibodies86. This finding was supported by a
report, which showed that expression levels of the adhesion molecules intercellular
adhesion molecule 1 (ICAM1) and vascular cell adhesion molecule 1 (VCAM1),
were downregulated in ECs after PDT [272]. The administration of antirat neutrophil
serum with PDT in rhabdomyosarcoma-bearing rats completely abrogated the normal
PDT-induced retardation of tumor growth [267], which showed that an influx of
neutrophils is required for an effective anti-tumor response in this model. An increase
in the number of peripheral-blood neutrophils was found 4 hours after PDT treatment,

448
PHOTODYNAMIC THERAPY
and lasted for 24 hours. The increase in neutrophils was preceded by an increase in
serum levels of IL1β. Anti-GCSF (granulocyte colony-stimulating factor) antibodies
decreased neutrophil numbers and decreased the efficacy of PDT [273].
Another major study by Korbelik et al. showed that Photofrin-based PDT cured
100% of EMT6 mammary sarcomas in syngeneic BALB/c mice, but no long-term
cures were observed in non-obese diabetic (NOD), severe combined immunodeficient
(SCID), or nude mice. The adoptive transfer of splenic T-lymphocytes from naive
BALB/c mice into SCID mice before PDT postponed the recurrence of treated tumors,
whereas adoptive transfer carried out immediately or 7 days after PDT had no benefit.
Adoptive transfer of nonadherent splenocytes (a mixture of CD4+ and CD8+ T cells
with some B cells, NK cells, and monocytes) from normal mice cured of EMT6
tumors by PDT 5 weeks previously, fully restored the curative effect of PDT on
EMT6 tumors that were growing in SCID mice. Splenocytes obtained from donors
that were cured by X-rays were much less effective. The depletion of specific T-cell
populations from donor splenocytes indicated that CD8+ cytotoxic T-lymphocytes
had the most curative effect, whereas CD4+ helper T cells played a supportive
role [274]. Hamblin and coworkers have illustrated the various mechanism of PDT
inducing immune system as shown in Figure 10.7 [275].
Finally, PDT has a significant effect on the immune system, which can be either
immunostimulatory, or in some cases immunosuppressive [59].
Excited singlet
state
On-thermal red light
Triplet
state
Ground
state
PS*
PS*
Tumour cell necrosis
and apoptosis
Shutdown of
microvessels
Dendritic
cell
Neutrophil
PS
102
302
Inflammation
FIGURE 10.7
The mechanism of action of PDT on tumors. The photosensitizer (PS) absorbs
light and an electron moves to the first short-lived excited singlet state. This is followed by
intersystem crossing, in which the excited electron changes its spin and produces a longer-
lived triplet state. The PS triplet transfers energy to ground-state triplet oxygen, which produces
reactive singlet oxygen (1O2). 1O2 can directly kill tumor cells by the induction of necrosis
and/or apoptosis, can cause destruction of tumor vasculature, and produces an acute inflamma-
tory response that attracts leukocytes such as dendritic cells and neutrophils. Reprinted with
permission from Reference 59. (For a color version of this figure, see the color plate section.)

REFERENCES
449
10.12
CONCLUSION
Since PDT was discovered 112 years ago, the field has come a long way. PDT
is arguably the most interdisciplinary therapeutic approach in modern biomedical
research. There are thriving research efforts by chemists to design and synthesize
novel PS, and by nanotechnologists to formulate PS in smart, targeted and ther-
anostic nanoconstructs. Physicists are studying tissue optics, light dosimetry, and
photophysical mechanisms, while engineers are designing more versatile and less-
expensive light sources based on LED and OLED arrays rather than cumbersome
and expensive lasers. Cell biologists have a fertile arena to probe deeper into cell
signaling, transcription factors, and epigenetics that govern the cellular response to
PDT. Microbiologists are heavily involved in dissecting the molecular pathogenesis
pathways and virulence factors that govern how microbes both cause diseases and
respond to PDT. Translational researchers are devising an ever-more comprehensive
and realistic panel of animal models to test PDT for a diverse range of diseases.
Finally, clinicians are increasingly initiating clinical trials of PDT and utilizing PDT
in their own practices. Dermatology may be the most mature clinical specialty for
PDT usage, but oncology, dentistry, and ophthalmology are close runners up. Infec-
tious disease is perhaps the area that is thought to have the most potential for future
growth, while immunology is considered to be a largely unexplored area in which
PDT may possibly lead to a cure for advanced metastatic cancer if the factors that
govern the activation of the immune response can be fully understood.
ACKNOWLEDGMENT
This work was supported by the US NIH (R01AI050875).
REFERENCES
[1] A. F. Barbosa, B. B. Sangiorgi, S. L. Galdino, M. Barral-Netto, I. R. Pitta, and A.
L. Pinheiro, “Photodynamic antimicrobial chemotherapy (PACT) using phenothiazine
derivatives as photosensitizers against Leishmania braziliensis,” Lasers Surg. Med. 44,
850–855 (2012).
[2] K. Kusuzaki, S. Hosogi, E. Ashihara, T. Matsubara, H. Satonaka, T. Nakamura, A.
Matsumine, A. Sudo, A. Uchida, H. Murata, et al., “Translational research of photody-
namic therapy with acridine orange which targets cancer acidity,” Curr. Pharm. Des.
18, 1414–1420 (2012).
[3] R. L. Lipson and E. J. Baldes, “The photodynamic properties of a particular hematopor-
phyrin derivative,” Arch. Dermatol. 82, 508–516 (1960).
[4] Z. Malik and H. Lugaci, “Destruction of erythroleukaemic cells by photoactivation of
endogenous porphyrins,” Br. J. Cancer 56, 589–595 (1987).
[5] T. Moller, M. B. Sarosi, and E. Hey-Hawkins, “Asymmetric phospha-diels-alder reac-
tion: a stereoselective approach towards p-chiral phosphanes through diastereotopic face
differentiation,” Chemistry 18, 16604–16607 (2012).

450
PHOTODYNAMIC THERAPY
[6] G. R. Shusterman, J. Fluke, Y.-Y. T. Yuan, National Child Abuse and Neglect Data
System, United States Department of Health and Human Services, Office of the Assis-
tant Secretary for Planning and Evaluation, and Walter R. McDonald & Associates,
Male Perpetrators of Child Maltreatment: Findings from NCANDS (U.S. Department
of Health and Human Services, Office of the Assistant Secretary for Planning and
Evaluation, Washington, DC, 2005).
[7] R. R. Allison and C. H. Sibata, “Oncologic photodynamic therapy photosensitizers: a
clinical review,” Photodiagnosis Photodyn. Ther. 7, 61–75 (2010).
[8] B. Chen, T. Roskams, and P. A. de Witte, “Antivascular tumor eradication by hypericin-
mediated photodynamic therapy,” Photochem. Photobiol. 76, 509–513 (2002).
[9] A. D. Garg, D. Nowis, J. Golab, P. Vandenabeele, D. V. Krysko, and P. Agostinis,
“Immunogenic cell death, DAMPs and anticancer therapeutics: an emerging amalga-
mation,” Biochim. Biophys. Acta. 1805, 53–71 (2010).
[10] M. Ascencio, P. Collinet, M. O. Farine, and S. Mordon, “Protoporphyrin IX fluorescence
photobleaching is a useful tool to predict the response of rat ovarian cancer following
hexaminolevulinate photodynamic therapy,” Lasers Surg. Med. 40, 332–341 (2008).
[11] F. S. De Rosa and M. V. Bentley, “Photodynamic therapy of skin cancers: sensitizers,
clinical studies and future directives,” Pharm. Res. 17, 1447–1455 (2000).
[12] M. R. Hamblin and E. L. Newman, “On the mechanism of the tumour-localising effect
in photodynamic therapy,” J. Photochem. Photobiol. B 23, 3–8 (1994).
[13] A. K. Iyer, K. Greish, T. Seki, S. Okazaki, J. Fang, K. Takeshita, and H. Maeda,
“Polymeric micelles of zinc protoporphyrin for tumor targeted delivery based on EPR
effect and singlet oxygen generation,” J. Drug. Target 15, 496–506 (2007).
[14] D. Kessel, “The role of low-density lipoprotein in the biodistribution of photosensitizing
agents,” J. Photochem. Photobiol. B 14, 261–262 (1992).
[15] S. A. Sibani, P. A. McCarron, A. D. Woolfson, and R. F. Donnelly, “Photosensitiser
delivery for photodynamic therapy. Part 2: systemic carrier platforms,” Expert Opin.
Drug. Deliv. 5, 1241–1254 (2008).
[16] S. Seguier, S. L. Souza, A. C. Sverzut, A. R. Simioni, F. L. Primo, A. Bodineau, V.
M. Correa, B. Coulomb, and A. C. Tedesco, “Impact of photodynamic therapy on
inflammatory cells during human chronic periodontitis,” J. Photochem. Photobiol. B
101, 348–354 (2010).
[17] J. R. Starkey, A. K. Rebane, M. A. Drobizhev, F. Meng, A. Gong, A. Elliott, K.
McInnerney, and C. W. Spangler, “New two-photon activated photodynamic therapy
sensitizers induce xenograft tumor regressions after near-IR laser treatment through the
body of the host mouse,” Clin. Cancer Res. 14, 6564–6573 (2008).
[18] M. Pawlicki, H. A. Collins, R. G. Denning, and H. L. Anderson, “Two-photon absorption
and the design of two-photon dyes,” Angew. Chem. Int. Ed. 48, 3244–3266 (2009).
[19] J. D. Bhawalkar, N. D. Kumar, C. F. Zhao, and P. N. Prasad, “Two-photon photodynamic
therapy,” J. Clin. Laser. Med. Surg. 15, 201–204 (1997).
[20] X. Shen, L. Li, H. Wu, S. Q. Yao, and Q. H. Xu, “Photosensitizer-doped conjugated poly-
mer nanoparticles for simultaneous two-photon imaging and two-photon photodynamic
therapy in living cells,” Nanoscale 3, 5140–5146 (2011).
[21] C. W. Spangler, J. R. Starkey, A. Rebane, F. Meng, A. Gong, and M. Drobizhev,
“Synthesis, characterization and preclinical studies of two-photon-activated targeted
PDT therapeutic triads,” Proc. SPIE, 6139, 61390X–61390X (2006).

REFERENCES
451
[22] C. W. Spangler, A. Rebane, J. Starkey, and M. Drobizhev, “Targeted two-photon PDT
photo-sensitizers for the treatment of subcutaneous tumors,” Proc. SPIE, 7380, 73803Z–
73803Z (2009).
[23] M. Khurana, H. A. Collins, A. Karotki, H. L. Anderson, D. T. Cramb, and B. C.
Wilson, “Quantitative in vitro demonstration of two-photon photodynamic therapy using
photofrin and visudyne,” Photochem. Photobiol. 83, 1441–1448 (2007).
[24] Y. Mir, J. E. van Lier, J. F. Allard, D. Morris, and D. Houde, “Two-photon absorption
cross section of excited phthalocyanines by a femtosecond Ti-sapphire laser,” Pho-
tochem. Photobiol. Sci. 8, 391–395 (2009).
[25] A. Karotki, M. Khurana, J. R. Lepock, and B. C. Wilson, “Simultaneous two-photon
excitation of photofrin in relation to photodynamic therapy,” Photochem. Photobiol. 82,
443–452 (2006).
[26] L. Lilge, M. Portnoy, and B. C. Wilson, “Apoptosis induced in vivo by photodynamic
therapy in normal brain and intracranial tumour tissue,” Br. J. Cancer 83, 1110–1117
(2000).
[27] S. K. Bisland, L. Lilge, A. Lin, R. Rusnov, and B. C. Wilson, “Metronomic photody-
namic therapy as a new paradigm for photodynamic therapy: rationale and preclinical
evaluation of technical feasibility for treating malignant brain tumors,” Photochem.
Photobiol. 80, 22–30 (2004).
[28] A. Bogaards, A. Varma, K. Zhang, D. Zach, S. K. Bisland, E. H. Moriyama, L. Lilge,
P. J. Muller, and B. C. Wilson, “Fluorescence image-guided brain tumour resection
with adjuvant metronomic photodynamic therapy: pre-clinical model and technology
development,” Photochem. Photobiol. Sci. 4, 438–442 (2005).
[29] E. R. de Haas, H. C. de Vijlder, H. J. Sterenborg, H. A. Neumann, and D. J. Robinson,
“Fractionated aminolevulinic acid-photodynamic therapy provides additional evidence
for the use of PDT for non-melanoma skin cancer,” J. Eur. Acad. Dermatol. Venereol.
22, 426–430 (2008).
[30] J. Chen, L. Keltner, J. Christophersen, F. Zheng, M. Krouse, A. Singhal, and S. S. Wang,
“New technology for deep light distribution in tissue for phototherapy,” Cancer J. 8,
154–163 (2002).
[31] H. Hirschberg, S. Madsen, K. Lote, T. Pham, and B. Tromberg, “An indwelling
brachytherapy balloon catheter: potential use as an intracranial light applicator for
photodynamic therapy,” J. Neurooncol. 44, 15–21 (1999).
[32] M. Ferrari, “Cancer nanotechnology: opportunities and challenges,” Nat. Rev. Cancer
5, 161–171 (2005).
[33] S. K. S. Ying-Ying Huang, H. C. Tianhong Dai, A. Yaroslavsky, M. Garcia-Diaz, J.
Chang, L. Y. Chiang, and M. R. Hamblin, “Can nanotechnology potentiate photody-
namic therapy?,” Nanotechnol. Rev. 1, 111–146 (2012).
[34] A. Gupta, P. Avci, M. Sadasivam, R. Chandran, N. Parizotto, D. Vecchio, W. C. de Melo,
T. Dai, L. Y. Chiang, and M. R. Hamblin, “Shining light on nanotechnology to help
repair and regeneration,” Biotechnol. Adv. 31, 607–631 (2012).
[35] F. Alexis, E. Pridgen, L. K. Molnar, and O. C. Farokhzad, “Factors affecting the clearance
and biodistribution of polymeric nanoparticles,” Mol. Pharm. 5, 505–515 (2008).
[36] F. Vatansever, R. Chandran, M. Sadasivam, L. Y. Chiang, and M. R. Hamblin, “Multi-
functionality in theranostic nanoparticles: is more always better?,” J. Nanomed. Nan-
otech. 3 (2012), http://dx.doi.org/10.4172/2157-7439.1000e4120.

452
PHOTODYNAMIC THERAPY
[37] S. Bhaskar, F. Tian, T. Stoeger, W. Kreyling, J. M. de la Fuente, V. Grazu, P. Borm,
G. Estrada, V. Ntziachristos, and D. Razansky, “Multifunctional Nanocarriers for diag-
nostics, drug delivery and targeted treatment across blood-brain barrier: perspectives on
tracking and neuroimaging,” Part Fibre Toxicol. 7, 3 (2010).
[38] S. S. Kelkar and T. M. Reineke, “Theranostics: combining imaging and therapy,” Bio-
conjug. Chem. 22, 1879–1903 (2011).
[39] X. Ma, Y. Zhao, and X. J. Liang, “Theranostic nanoparticles engineered for clinic and
pharmaceutics,” Acc. Chem. Res. 44, 1114–1122 (2011).
[40] J. Xie, S. Lee, and X. Chen, “Nanoparticle-based theranostic agents,” Adv. Drug Deliv.
Rev. 62, 1064–1079 (2010).
[41] T. Niidome and L. Huang, “Gene therapy progress and prospects: nonviral vectors,”
Gene. Ther. 9, 1647–1652 (2002).
[42] S. Akhtar, “Non-viral cancer gene therapy: beyond delivery,” Gene Ther. 13, 739–740
(2006).
[43] C. Ruggiero, L. Pastorino, and O. L. Herrera, “Nanotechnology based targeted drug
delivery,” Conf. Proc. IEEE Eng. Med. Biol. Soc. 2010, 3731–3732 (2010).
[44] S. Perni, P. Prokopovich, J. Pratten, I. P. Parkin, and M. Wilson, “Nanoparticles: their
potential use in antibacterial photodynamic therapy,” Photochem. Photobiol. Sci. 10,
712–720 (2011).
[45] Y. Matsumoto, K. Itaka, T. Yamasoba, and K. Kataoka, “Intranuclear fluorescence
resonance energy transfer analysis of plasmid DNA decondensation from nonviral gene
carriers,” J. Gene. Med. 11, 615–623 (2009).
[46] P. Schwille, “Fluorescence correlation spectroscopy and its potential for intracellular
applications,” Cell Biochem. Biophys. 34, 383–408 (2001).
[47] H. Sahoo and P. Schwille, “FRET and FCS–friends or foes?,” Chemphyschem 12, 532–
541 (2011).
[48] H. Koo, M. S. Huh, I. C. Sun, S. H. Yuk, K. Choi, K. Kim, and I. C. Kwon, “In vivo
targeted delivery of nanoparticles for theranosis,” Acc. Chem. Res. 44, 1018–1028
(2011).
[49] K. Kim, J. H. Kim, H. Park, Y. S. Kim, K. Park, H. Nam, S. Lee, J. H. Park, R. W. Park,
I. S. Kim, et al., “Tumor-homing multifunctional nanoparticles for cancer theragnosis:
simultaneous diagnosis, drug delivery, and therapeutic monitoring,” J. Control Release
146, 219–227 (2010).
[50] R. Bakalova, H. Ohba, Z. Zhelev, M. Ishikawa, and Y. Baba, “Quantum dots as photo-
sensitizers?,” Nat. Biotechnol. 22, 1360–1361 (2004).
[51] E. Yaghini, A. M. Seifalian, and A. J. MacRobert, “Quantum dots and their potential
biomedical applications in photosensitization for photodynamic therapy,” Nanomedicine
(London) 4, 353–363 (2009).
[52] U. Resch-Genger, M. Grabolle, S. Cavaliere-Jaricot, R. Nitschke, and T. Nann, “Quan-
tum dots versus organic dyes as fluorescent labels,” Nat. Methods 5, 763–775 (2008).
[53] D. Vecchio, T. Dai, L. Huang, L. Fantetti, G. Roncucci, and M. R. Hamblin, “Antimi-
crobial photodynamic therapy with RLP068 kills methicillin-resistant Staphylococcus
aureus and improves wound healing in a mouse model of infected skin abrasion PDT
with RLP068/Cl in infected mouse skin abrasion,” J. Biophotonics 6, 733–742 (2012).
[54] A. C. Samia, X. Chen, and C. Burda, “Semiconductor quantum dots for photodynamic
therapy,” J. Am. Chem. Soc. 125, 15736–15737 (2003).

REFERENCES
453
[55] A. Rakovich, D. Savateeva, T. Rakovich, J. F. Donegan, Y. P. Rakovich, V. Kelly, V.
Lesnyak, and A. Eychmuller, “CdTe quantum Dot/Dye hybrid system as photosensitizer
for photodynamic therapy,” Nanoscale Res. Lett. 5, 753–760 (2010).
[56] S. Lee and X. Chen, “Dual-modality probes for in vivo molecular imaging,” Mol.
Imaging 8, 87–100 (2009).
[57] J. Xie, J. Huang, X. Li, S. Sun, and X. Chen, “Iron oxide nanoparticle platform for
biomedical applications,” Curr. Med. Chem. 16, 1278–1294 (2009).
[58] S. K. Sharma, L. Y. Chiang, and M. R. Hamblin, “Photodynamic therapy with fullerenes
in vivo: reality or a dream?,” Nanomedicine (London) 6, 1813–1825 (2011).
[59] A. P. Castano, P. Mroz, and M. R. Hamblin, “Photodynamic therapy and anti-tumour
immunity,” Nat. Rev. Cancer 6, 535–545 (2006).
[60] Z. Chen, L. Ma, Y. Liu, and C. Chen, “Applications of functionalized fullerenes in tumor
theranostics,” Theranostics 2, 238–250 (2012).
[61] A. Jesionek and V. H. Tappeiner, “Behandlung der hautcarcinome nut fluorescierenden
stoffen,” Dtsch. Arch. Klin. Med. 85, 223–227 (1905).
[62] Y. Lee and E. D. Baron, “Photodynamic therapy: current evidence and applications in
dermatology,” Semin. Cutan. Med. Surg. 30, 199–209 (2011).
[63] R. Marks, G. Rennie, and T. S. Selwood, “Malignant transformation of solar keratoses
to squamous cell carcinoma,” Lancet 1, 795–797 (1988).
[64] S. M. Dinehart, “The treatment of actinic keratoses,” J. Am. Acad. Dermatol. 42, 25–28
(2000).
[65] M. H. Gold and M. S. Nestor, “Current treatments of actinic keratosis,” J. Drugs
Dermatol. 5, 17–25 (2006).
[66] D. J. Piacquadio, D. M. Chen, H. F. Farber, J. F. Fowler, Jr., S. D. Glazer, J. J. Goodman,
L. L. Hruza, E. W. Jeffes, M. R. Ling, T. J. Phillips, et al., “Photodynamic therapy with
aminolevulinic acid topical solution and visible blue light in the treatment of multiple
actinic keratoses of the face and scalp: investigator-blinded, phase 3, multicenter trials,”
Arch. Dermatol. 140, 41–46 (2004).
[67] M. Tarstedt, I. Rosdahl, B. Berne, K. Svanberg, and A. M. Wennberg, “A randomized
multicenter study to compare two treatment regimens of topical methyl aminolevulinate
(Metvix)-PDT in actinic keratosis of the face and scalp,” Acta. Derm. Venereol. 85,
424–428 (2005).
[68] C. Morton, S. Campbell, G. Gupta, S. Keohane, J. Lear, I. Zaki, S. Walton, N. Kerrouche,
G. Thomas, P. Soto, et al., “Intraindividual, right-left comparison of topical methyl
aminolaevulinate-photodynamic therapy and cryotherapy in subjects with actinic ker-
atoses: a multicentre, randomized controlled study,” Br. J. Dermatol. 155, 1029–1036
(2006).
[69] R. M. Szeimies, E. Stockfleth, G. Popp, F. Borrosch, H. Bruning, R. Dominicus, H.
Mensing, U. Reinhold, K. Reich, A. C. Moor, et al., “Long-term follow-up of photody-
namic therapy with a self-adhesive 5-aminolaevulinic acid patch: 12 months data,” Br.
J. Dermatol. 162, 410–414 (2010).
[70] E. H. Tschen, D. S. Wong, D. M. Pariser, F. E. Dunlap, A. Houlihan, M. B. Ferdon;Phase
IV ALA-PDT Actinic Keratosis Study Group. “Photodynamic therapy using aminolae-
vulinic acid for patients with nonhyperkeratotic actinic keratoses of the face and scalp:
phase IV multicentre clinical trial with 12-month follow up,” Br. J. Dermatol. 155,
1262–1269 (2006).

454
PHOTODYNAMIC THERAPY
[71] E. Sotiriou, Z. Apalla, F. Maliamani, N. Zaparas, D. Panagiotidou, and D. Ioannides,
“Intraindividual, right-left comparison of topical 5-aminolevulinic acid photodynamic
therapy vs. 5% imiquimod cream for actinic keratoses on the upper extremities,” J. Eur.
Acad. Dermatol. Venereol. 23, 1061–1065 (2009).
[72] R. Kaufmann, L. Spelman, W. Weightman, J. Reifenberger, R. M. Szeimies, E.
Verhaeghe, N. Kerrouche, V. Sorba, H. Villemagne, and L. E. Rhodes, “Multicentre
intraindividual randomized trial of topical methyl aminolaevulinate-photodynamic ther-
apy vs. cryotherapy for multiple actinic keratoses on the extremities,” Br. J. Dermatol.
158, 994–999 (2008).
[73] E. Sotiriou, Z. Apalla, E. Chovarda, D. Panagiotidou, and D. Ioannides, “Photody-
namic therapy with 5-aminolevulinic acid in actinic cheilitis: an 18-month clinical and
histological follow-up,” J. Eur. Acad. Dermatol. Venereol. 24, 916–920 (2010).
[74] C. Berking, T. Herzinger, M. J. Flaig, M. Brenner, C. Borelli, and K. Degitz, “The
efficacy of photodynamic therapy in actinic cheilitis of the lower lip: a prospective
study of 15 patients,” Dermatol. Surg. 33, 825–830 (2007).
[75] E. Sotiriou, A. Lallas, C. Goussi, Z. Apalla, A. Trigoni, E. Chovarda, and D. Ioannides,
“Sequential use of photodynamic therapy and imiquimod 5% cream for the treat-
ment of actinic cheilitis: a 12-month follow-up study,” Br. J. Dermatol. 165, 888–892
(2011).
[76] Z. Apalla, E. Sotiriou, E. Chovarda, I. Lefaki, D. Devliotou-Panagiotidou, and D. Ioan-
nides, “Skin cancer: preventive photodynamic therapy in patients with face and scalp
cancerization. A randomized placebo-controlled study,” Br. J. Dermatol. 162, 171–175
(2010).
[77] H. C. Wulf, S. Pavel, I. Stender, and C. A. Bakker-Wensveen, “Topical photodynamic
therapy for prevention of new skin lesions in renal transplant recipients,” Acta Derm.
Venereol. 86, 25–28 (2006).
[78] L. Bagazgoitia, J. Cuevas Santos, A. Juarranz, and P. Jaen, “Photodynamic therapy
reduces the histological features of actinic damage and the expression of early oncogenic
markers,” Br. J. Dermatol. 165, 144–151 (2011).
[79] R. M. Szeimies, L. Torezan, A. Niwa, N. Valente, P. Unger, E. Kohl, S. Schreml,
P. Babilas, S. Karrer, and C. Festa-Neto, “Clinical, histopathological and immunohisto-
chemical assessment of human skin field cancerization before and after photodynamic
therapy,” Br. J. Dermatol. 167, 150–159 (2012).
[80] M. M. Lee and M. M. Wick, “Bowen’s disease,” CA Cancer J. Clin. 40, 237–242
(1990).
[81] M. J. Rasmussen, Precancerous Lesions (Lea & Febiger, Philadelphia, PA, 1979).
[82] G. Moreno, A. L. Chia, A. Lim, and S. Shumack, “Therapeutic options for Bowen’s
disease,” Australas J. Dermatol. 48, 1–8; quiz 9–10 (2007).
[83] S. B. Ball and R. P. Dawber, “Treatment of cutaneous Bowen’s disease with particular
emphasis on the problem of lower leg lesions,” Australas J. Dermatol. 39, 63–68; quiz
69–70 (1998).
[84] C. A. Morton, C. Whitehurst, H. Moseley, J. H. McColl, J. V. Moore, and R. M. Mackie,
“Comparison of photodynamic therapy with cryotherapy in the treatment of Bowen’s
disease,” Br. J. Dermatol. 135, 766–771 (1996).
[85] A. Salim, J. A. Leman, J. H. McColl, R. Chapman, and C. A. Morton, “Randomized
comparison of photodynamic therapy with topical 5-fluorouracil in Bowen’s disease,”
Br. J. Dermatol. 148, 539–543 (2003).

REFERENCES
455
[86] C. Morton, M. Horn, J. Leman, B. Tack, C. Bedane, M. Tjioe, S. Ibbotson, A. Khemis,
and P. Wolf, “Comparison of topical methyl aminolevulinate photodynamic therapy with
cryotherapy or fluorouracil for treatment of squamous cell carcinoma in situ: results of
a multicenter randomized trial,” Arch. Dermatol. 142, 729–735 (2006).
[87] R. K. Roenigk, J. L. Ratz, P. L. Bailin, and R. G. Wheeland, “Trends in the presentation
and treatment of basal cell carcinomas,” J. Dermatol. Surg. Oncol. 12, 860–865 (1986).
[88] E. Stockfleth and W. Sterry, “New treatment modalities for basal cell carcinoma,” Recent
Results Cancer Res. 160, 259–268 (2002).
[89] Q. Peng, T. Warloe, K. Berg, J. Moan, M. Kongshaug, K. E. Giercksky, and J. M.
Nesland, “5-Aminolevulinic acid-based photodynamic therapy. Clinical research and
future challenges,” Cancer 79, 2282–2308 (1997).
[90] M. R. Thissen, C. A. Schroeter, and H. A. Neumann, “Photodynamic therapy with
delta-aminolaevulinic acid for nodular basal cell carcinomas using a prior debulking
technique,” Br. J. Dermatol. 142, 338–339 (2000).
[91] L. Berroeta, C. Clark, R. S. Dawe, S. H. Ibbotson, and C. J. Fleming, “A randomized
study of minimal curettage followed by topical photodynamic therapy compared with
surgical excision for low-risk nodular basal cell carcinoma,” Br. J. Dermatol. 157,
401–403 (2007).
[92] A. M. Soler, T. Warloe, A. Berner, and K. E. Giercksky, “A follow-up study of recurrence
and cosmesis in completely responding superficial and nodular basal cell carcinomas
treated with methyl 5-aminolaevulinate-based photodynamic therapy alone and with
prior curettage,” Br. J. Dermatol. 145, 467–471 (2001).
[93] R. M. Szeimies, “Methyl aminolevulinate-photodynamic therapy for basal cell carci-
noma,” Dermatol. Clin. 25, 89–94 (2007).
[94] L. E. Rhodes, M. de Rie, Y. Enstrom, R. Groves, T. Morken, V. Goulden, G. A. Wong, J.
J. Grob, S. Varma, and P. Wolf, “Photodynamic therapy using topical methyl aminole-
vulinate vs surgery for nodular basal cell carcinoma: results of a multicenter randomized
prospective trial,” Arch. Dermatol. 140, 17–23 (2004).
[95] L. E. Rhodes, M. A. de Rie, R. Leifsdottir, R. C. Yu, I. Bachmann, V. Goulden, G. A.
Wong, M. A. Richard, A. Anstey, and P. Wolf, “Five-year follow-up of a randomized,
prospective trial of topical methyl aminolevulinate photodynamic therapy vs surgery for
nodular basal cell carcinoma,” Arch. Dermatol. 143, 1131–1136 (2007).
[96] C. M. Balch, J. E. Gershenwald, S. J. Soong, J. F. Thompson, M. B. Atkins, D. R. Byrd,
A. C. Buzaid, A. J. Cochran, D. G. Coit, S. Ding, et al., “Final version of 2009 AJCC
melanoma staging and classification,” J. Clin. Oncol. 27, 6199–6206 (2009).
[97] S. A. Ozler, J. S. Nelson, P. E. Liggett, J. M. de Queiroz, Jr., and M. W. Berns,
“Photodynamic therapy of experimental subchoroidal melanoma using chloroaluminum
sulfonated phthalocyanine,” Arch. Ophthalmol. 110, 555–561 (1992).
[98] M. Dellian, C. Richert, F. Gamarra, and A. E. Goetz, “Photodynamic eradication of
amelanotic melanoma of the hamster with fast acting photosensitizers,” Int. J. Cancer
65, 246–248 (1996).
[99] L. M. Davids, B. Kleemann, D. Kacerovska, K. Pizinger, and S. H. Kidson, “Hypericin
phototoxicity induces different modes of cell death in melanoma and human skin cells,”
J. Photochem. Photobiol. B 91, 67–76 (2008).
[100] L. H. Young, M. A. Howard, L. K. Hu, R. Y. Kim, and E. S. Gragoudas, “Photody-
namic therapy of pigmented choroidal melanomas using a liposomal preparation of
benzoporphyrin derivative,” Arch. Ophthalmol. 114, 186–192 (1996).

456
PHOTODYNAMIC THERAPY
[101] K. W. Woodburn, Q. Fan, D. Kessel, Y. Luo, and S. W. Young, “Photodynamic therapy of
B16F10 murine melanoma with lutetium texaphyrin,” J. Invest. Dermatol. 110, 746–751
(1998).
[102] R. Haddad, O. Kaplan, R. Greenberg, A. Siegal, Y. Skornick, and H. Kashtan, “Photo-
dynamic therapy of murine colon cancer and melanoma using systemic aminolevulinic
acid as a photosensitizer,” Int. J. Surg. Investig. 2, 171–178 (2000).
[103] L. Hu, X. Wu, Y. Song, L. H. Young, and E. S. Gragoudas, “Photodynamic therapy
of pigmented choroidal melanomas in rabbits,” Zhonghua Yan Ke Za Zhi 38, 491–494
(2002).
[104] J. Barge, R. Decreau, M. Julliard, J. C. Hubaud, A. S. Sabatier, J. J. Grob, and P. Verrando,
“Killing efficacy of a new silicon phthalocyanine in human melanoma cells treated with
photodynamic therapy by early activation of mitochondrion-mediated apoptosis,” Exp.
Dermatol. 13, 33–44 (2004).
[105] J. M. Dabrowski, K. Urbanska, L. G. Arnaut, M. M. Pereira, A. R. Abreu, S. Simoes,
and G. Stochel, “Biodistribution and photodynamic efficacy of a water-soluble, stable,
halogenated bacteriochlorin against melanoma,” Chem. Med. Chem. 6, 465–475 (2011).
[106] J. M. Dabrowski, M. Krzykawska, L. G. Arnaut, M. M. Pereira, C. J. Monteiro, S.
Simoes, K. Urbanska, and G. Stochel, “Tissue uptake study and photodynamic therapy
of melanoma-bearing mice with a nontoxic, effective chlorin,” Chem. Med. Chem. 6,
1715–1726 (2011).
[107] I. A. Barbazetto, T. C. Lee, I. S. Rollins, S. Chang, and D. H. Abramson, “Treatment of
choroidal melanoma using photodynamic therapy,” Am. J. Ophthalmol. 135, 898–899
(2003).
[108] M. J. Donaldson, L. Lim, C. A. Harper, J. Mackenzie, and G. Campbell W, “Primary
treatment of choroidal amelanotic melanoma with photodynamic therapy,” Clin. Exp.
Ophthalmol. 33, 548–549 (2005).
[109] S. V. Sheleg, E. A. Zhavrid, T. V. Khodina, G. A. Kochubeev, Y. P. Istomin, V. N. Chalov,
and I. N. Zhuravkin, “Photodynamic therapy with chlorin e(6) for skin metastases of
melanoma,” Photodermatol. Photoimmunol. Photomed. 20, 21–26 (2004).
[110] H. Saji, W. Song, K. Furumoto, H. Kato, and E. G. Engleman, “Systemic antitumor effect
of intratumoral injection of dendritic cells in combination with local photodynamic
therapy,” Clin. Cancer Res. 12, 2568–2574 (2006).
[111] Y. Y. Huang, D. Vecchio, P. Avci, R. Yin, M. Garcia-Diaz, and M. R. Hamblin,
“Melanoma resistance to photodynamic therapy: new insights,” Biol. Chem. 394, 239–
250 (2012).
[112] R. Willemze, E. S. Jaffe, G. Burg, L. Cerroni, E. Berti, S. H. Swerdlow, E. Ralfkiaer,
S. Chimenti, J. L. Diaz-Perez, L. M. Duncan, et al., “WHO-EORTC classification for
cutaneous lymphomas,” Blood 105, 3768–3785 (2005).
[113] M. Duvic and R. Edelson, “Cutaneous T-cell lymphoma,” J. Am. Acad. Dermatol. 51,
S43–S45 (2004).
[114] K. Rittenhouse-Diakun, H. Van Leengoed, J. Morgan, E. Hryhorenko, G. Paszkiewicz,
J. E. Whitaker, and A. R. Oseroff, “The role of transferrin receptor (CD71) in pho-
todynamic therapy of activated and malignant lymphocytes using the heme precursor
delta-aminolevulinic acid (ALA),” Photochem. Photobiol. 61, 523–528 (1995).
[115] J. A. Leman, D. C. Dick, and C. A. Morton, “Topical 5-ALA photodynamic therapy for
the treatment of cutaneous T-cell lymphoma,” Clin. Exp. Dermatol. 27, 516–518 (2002).

REFERENCES
457
[116] A. Orenstein, J. Haik, J. Tamir, E. Winkler, H. Trau, Z. Malik, and G. Kostenich,
“Photodynamic therapy of cutaneous lymphoma using 5-aminolevulinic acid topical
application,” Dermatol. Surg. 26, 765–769; discussion 769–770 (2000).
[117] E. A. Coors and P. von den Driesch, “Topical photodynamic therapy for patients with
therapy-resistant lesions of cutaneous T-cell lymphoma,” J. Am. Acad. Dermatol. 50,
363–367 (2004).
[118] D. W. Edstrom, A. Porwit, and A. M. Ros, “Photodynamic therapy with topical 5-
aminolevulinic acid for mycosis fungoides: clinical and histological response,” Acta.
Derm. Venereol. 81, 184–188 (2001).
[119] S. Y. Lee, C. E. You, and M. Y. Park, “Blue and red light combination LED phototherapy
for acne vulgaris in patients with skin phototype IV,” Lasers Surg. Med. 39, 180–188
(2007).
[120] J. S. Strauss, D. P. Krowchuk, J. J. Leyden, A. W. Lucky, A. R. Shalita, E. C. Siegfried,
D. M. Thiboutot, A. S. Van Voorhees, K. A. Beutner, C. K. Sieck, et al., and American
Academy of Dermatology/American Academy of Dermatology Association, “Guide-
lines of care for acne vulgaris management,” J. Am. Acad. Dermatol. 56, 651–663
(2007).
[121] S. Ibbotson, “What is the role of photodynamic therapy in the treatment of acne vul-
garis?,” Photodiagnosis Photodyn. Ther. 9, 2–4 (2012).
[122] E. V. Ross, “Optical treatments for acne,” Dermatol. Ther. 18, 253–266 (2005).
[123] M. H. Aziz-Jalali, S. M. Tabaie, and G. E. Djavid, “Comparison of red, and infrared
low-level laser therapy in the treatment of acne vulgaris,” Ind. J. Dermatol. 57, 128–130
(2012).
[124] N. S. Sadick, “Handheld LED array device in the treatment of acne vulgaris,” J. Drugs
Dermatol. 7, 347–350 (2008).
[125] A. M. Rotunda, A. R. Bhupathy, and T. E. Rohrer, “The new age of acne therapy: light,
lasers, and radiofrequency,” J. Cosmet. Laser. Ther. 6, 191–200 (2004).
[126] L. Bowes, D. Manstein, and R. Anderson, “Effect of 532nm KTP laser exposure on acne
and sebaceous glands,” Lasers Med. Sci. 18, S6–S7 (2003).
[127] W. Hongcharu, C. R. Taylor, Y. Chang, D. Aghassi, K. Suthamjariya, andR. R. Anderson,
“Topical ALA-photodynamic therapy for the treatment of acne vulgaris,” J. Invest.
Dermatol. 115, 183–192 (2000).
[128] B. Pollock, D. Turner, M. R. Stringer, R. A. Bojar, V. Goulden, G. I. Stables, and W.
J. Cunliffe, “Topical aminolaevulinic acid-photodynamic therapy for the treatment of
acne vulgaris: a study of clinical efficacy and mechanism of action,” Br. J. Dermatol.
151, 616–622 (2004).
[129] Y. Itoh, Y. Ninomiya, S. Tajima, and A. Ishibashi, “Photodynamic therapy of acne vul-
garis with topical delta-aminolaevulinic acid and incoherent light in Japanese patients,”
Br. J. Dermatol. 144, 575–579 (2001).
[130] E. Kohl, L. A. Torezan, M. Landthaler, and R. M. Szeimies, “Aesthetic effects of topical
photodynamic therapy,” J. Eur. Acad. Dermatol. Venereol. 24, 1261–1269 (2010).
[131] R. Ruiz-Rodriguez, L. Lopez, D. Candelas, and J. Pedraz, “Photorejuvenation using
topical 5-methyl aminolevulinate and red light,” J. Drugs Dermatol. 7, 633–637 (2008).
[132] R. Ruiz-Rodriguez, L. Lopez, D. Candelas, and B. Zelickson, “Enhanced efficacy of
photodynamic therapy after fractional resurfacing: fractional photodynamic rejuvena-
tion,” J. Drugs Dermatol. 6, 818–820 (2007).

458
PHOTODYNAMIC THERAPY
[133] D. Key, “Aminolaevulinic acid-pulsed dye laser photodynamic therapy for the treatment
of photoaging,” Cosmet. Dermatol. 18, 31–36 (2005).
[134] I. Diamond, A. F. McDonagh, C. B. Wilson, S. G. Granelli, S. Nielsen, and R. Jaenicke,
“Photherapy of maliagnant tumors,” Lancet 300, 1175–1177 (1972).
[135] T. J. Dougherty, G. B. Grindey, R. Fiel, K. R. Weishaupt, and D. G. Boyle, “Photoradia-
tion therapy. II. Cure of animal tumors with hematoporphyrin and light,” J. Natl. Cancer
Inst. 55, 115–121 (1975).
[136] T. J. Dougherty, J. E. Kaufman, A. Goldfarb, K. R. Weishaupt, D. Boyle, and A.
Mittleman, “Photoradiation therapy for the treatment of malignant tumors,” Cancer
Res. 38, 2628–2635 (1978).
[137] K. Moghissi, K. Dixon, M. Stringer, and J. A. Thorpe, “Photofrin PDT for early stage
oesophageal cancer: long term results in 40 patients and literature review,” Photodiag-
nosis Photodyn. Ther. 6, 159–166 (2009).
[138] S. Mimura, H. Narahara, H. Uehara, T. Otani, and S. Okuda, “Photodynamic therapy
for gastric cancer,” Gan To Kagaku Ryoho 23, 41–46 (1996).
[139] E. A. Popovic, A. H. Kaye, and J. S. Hill, “Photodynamic therapy of brain tumors,”
J. Clin. Laser Med. Surg. 14, 251–261 (1996).
[140] S. Piermarocchi, M. Sartore, G. Lo Giudice, V. Maritan, E. Midena, and T. Segato,
“Combination of photodynamic therapy and intraocular triamcinolone for exudative
age-related macular degeneration and long-term chorioretinal macular atrophy,” Arch.
Ophthalmol. 126, 1367–1374 (2008).
[141] A. Dimofte, T. C. Zhu, S. M. Hahn, and R. A. Lustig, “In vivo light dosimetry for
motexafin lutetium-mediated PDT of recurrent breast cancer,” Lasers Surg. Med. 31,
305–312 (2002).
[142] M. A. Biel, “Photodynamic therapy of head and neck cancers,” Methods Mol. Biol. 635,
281–293 (2010).
[143] R. R. Allison, T. S. Mang, and B. D. Wilson, “Photodynamic therapy for the treatment
of nonmelanomatous cutaneous malignancies,” Semin. Cutan. Med. Surg. 17, 153–163
(1998).
[144] R. B. Veenhuizen, J. P. Marijnissen, P. Kenemans, M. C. Ruevekamp-Helmers, L. W. T.
Mannetje, T. J. Helmerhorst, and F. A. Stewart, “Intraperitoneal photodynamic therapy
of the rat CC531 adenocarcinoma,” Br. J. Cancer 73, 1387–1392 (1996).
[145] S. M. Hahn, R. P. Smith, and J. Friedberg, “Photodynamic therapy for mesothelioma,”
Curr. Treat. Options Oncol. 2, 375–383 (2001).
[146] R. R. Allison, E. Zervos, and C. H. Sibata, “Cholangiocarcinoma: an emerging indication
for photodynamic therapy,” Photodiagnosis Photodyn. Ther. 6, 84–92 (2009).
[147] B. G. Fan and A. Andren-Sandberg, “Photodynamic therapy for pancreatic cancer,”
Pancreas 34, 385–389 (2007).
[148] L. Milla Sanabria, M. E. Rodriguez, I. S. Cogno, N. B. Rumie Vittar, M. F. Pansa, M. J.
Lamberti, and V. A. Rivarola, “Direct and indirect photodynamic therapy effects on the
cellular and molecular components of the tumor microenvironment,” Biochim. Biophys.
Acta 1835, 36–45 (2013).
[149] C. Raab, “Ber die wirkung fluoreszierender stoffe auf infusoria,” Z. Biol. 39, 524–546
(1900).
[150] A. Jesionek and H. von Tappenier, “Zur behandlung der hautcarcinomit mit fluore-
scierenden stoffen,” Muench. Med. Wochenschr. 47, 2042–2044 (1903).

REFERENCES
459
[151] W. Hausmann, “Die sensibilisierende wirkung tierscher farbstoffe und ihne physiolo-
gische bedeutung,” Wien. Klin. Wochenschr. 21, 1527–1529 (1908).
[152] G. H. Cassell and J. Mekalanos, “Development of antimicrobial agents in the era of new
and reemerging infectious diseases and increasing antibiotic resistance,” J. Am. Med.
Assoc. 285, 601–605 (2001).
[153] M. R. Hamblin and T. Hasan, “Photodynamic therapy: a new antimicrobial approach to
infectious disease?,” Photochem. Photobiol. Sci. 3, 436–450 (2004).
[154] F. M. Lauro, P. Pretto, L. Covolo, G. Jori, and G. Bertoloni, “Photoinactivation of
bacterial strains involved in periodontal diseases sensitized by porphycene-polylysine
conjugates,” Photochem. Photobiol. Sci. 1, 468–470 (2002).
[155] T. Maisch, “A new strategy to destroy antibiotic resistant microorganisms: antimicrobial
photodynamic treatment,” Mini. Rev. Med. Chem. 9, 974–983 (2009).
[156] Z. Malik, H. Ladan, and Y. Nitzan, “Photodynamic inactivation of gram-negative bacte-
ria: problems and possible solutions,” J. Photochem. Photobiol. B 14, 262–266 (1992).
[157] Y. Nitzan, M. Gutterman, Z. Malik, and B. Ehrenberg, “Inactivation of gram-negative
bacteria by photosensitized porphyrins,” Photochem. Photobiol. 55, 89–96 (1992).
[158] Z. Malik, H. Ladan, Y. Nitzan, and B. Ehrenberg, “The bactericidal activity of a
deuteroporphyrin-hemin mixture on gram-positive bacteria. A microbiological and spec-
troscopic study,” J. Photochem. Photobiol. B 6, 419–430 (1990).
[159] G. Bertoloni, F. Rossi, G. Valduga, G. Jori, and J. van Lier, “Photosensitizing activity of
water- and lipid-soluble phthalocyanines on Escherichia coli,” FEMS Microbiol. Lett.
59, 149–155 (1990).
[160] M. Wainwright and K. B. Crossley, “Methylene blue—a therapeutic dye for all sea-
sons?,” J. Chemother. 14, 431–443 (2002).
[161] M. N. Usacheva, M. C. Teichert, and M. A. Biel, “Comparison of the methylene blue
and toluidine blue photobactericidal efficacy against gram-positive and gram-negative
microorganisms,” Lasers Surg. Med. 29, 165–173 (2001).
[162] T. W. Wong, Y. Y. Wang, H. M. Sheu, and Y. C. Chuang, “Bactericidal effects of
toluidine blue-mediated photodynamic action on Vibrio vulnificus,” Antimicrob. Agents
Chemother. 49, 895–902 (2005).
[163] T. G. St Denis, L. Huang, T. Dai, and M. R. Hamblin, “Analysis of the bacterial
heat shock response to photodynamic therapy-mediated oxidative stress,” Photochem.
Photobiol. 87, 707–713 (2011).
[164] A. Minnock, D. I. Vernon, J. Schofield, J. Griffiths, J. H. Parish, and S. T. Brown,
“Photoinactivation of bacteria. Use of a cationic water-soluble zinc phthalocyanine to
photoinactivate both gram-negative and gram-positive bacteria,” J. Photochem. Photo-
biol. B 32, 159–164 (1996).
[165] X. Ragas, D. Sanchez-Garcia, R. Ruiz-Gonzalez, T. Dai, M. Agut, M. R. Hamblin,
and S. Nonell, “Cationic porphycenes as potential photosensitizers for antimicrobial
photodynamic therapy,” J. Med. Chem. 53, 7796–7803 (2010).
[166] L. Bourre, F. Giuntini, I. M. Eggleston, C. A. Mosse, A. J. Macrobert, and M. Wilson,
“Effective photoinactivation of gram-positive and gram-negative bacterial strains using
an HIV-1 Tat peptide-porphyrin conjugate,” Photochem. Photobiol. Sci. 9, 1613–1620
(2010).
[167] T. A. Dahl, W. R. Midden, and P. E. Hartman, “Pure singlet oxygen cytotoxicity for
bacteria,” Photochem. Photobiol. 46, 345–352 (1987).

460
PHOTODYNAMIC THERAPY
[168] M. S. Choi, S. J. Yun, H. J. Beom, H. R. Park, and J. B. Lee, “Comparative study of
the bactericidal effects of 5-aminolevulinic acid with blue and red light on Propionibac-
terium acnes,” J. Dermatol. 38, 661–666 (2011).
[169] N. Komerik, H. Nakanishi, A. J. MacRobert, B. Henderson, P. Speight, and M. Wilson,
“In vivo killing of Porphyromonas gingivalis by toluidine blue-mediated photosensiti-
zation in an animal model,” Antimicrob. Agents Chemother. 47, 932–940 (2003).
[170] L. Huang, T. Zhiyentayev, Y. Xuan, D. Azhibek, G. B. Kharkwal, and M. R. Hamblin,
“Photodynamic inactivation of bacteria using polyethylenimine-chlorin(e6) conjugates:
effect of polymer molecular weight, substitution ratio of chlorin(e6) and pH,” Lasers
Surg. Med. 43, 313–323 (2011).
[171] V. T. Orlandi, E. Caruso, S. Banfi, and P. Barbieri, “Effect of organic matter on the
in vitro photoeradication of Pseudomonas aeruginosa by means of a cationic tetraaryl-
porphyrin,” Photochem. Photobiol. 88, 557–564 (2012).
[172] M. R. Hamblin, T. Zahra, C. H. Contag, A. T. McManus, and T. Hasan, “Optical
monitoring and treatment of potentially lethal wound infections in vivo,” J. Infect. Dis.
187, 1717–1725 (2003).
[173] C. M. Allen, J. M. Weber, and J. E. van Lier, “Sulfophthalocyanines for photodynamic
inactivation of viruses in blood products: effect of structural modifications,” Photochem.
Photobiol. 62, 184–189 (1995).
[174] H. Mohr, B. Lambrecht, and A. Selz, “Photodynamic virus inactivation of blood com-
ponents,” Immunol. Invest. 24, 73–85 (1995).
[175] T. Dai, B. B. Fuchs, J. J. Coleman, R. A. Prates, C. Astrakas, T. G. St Denis, M. S.
Ribeiro, E. Mylonakis, M. R. Hamblin, and G. P. Tegos, “Concepts and principles of
photodynamic therapy as an alternative antifungal discovery platform,” Front Microbiol.
3, 120 (2012).
[176] P. Grellier, R. Santus, E. Mouray, V. Agmon, J. C. Maziere, D. Rigomier, A. Dagan,
S. Gatt, and J. Schrevel, “Photosensitized inactivation of Plasmodium falciparum- and
Babesia divergens-infected erythrocytes in whole blood by lipophilic pheophorbide
derivatives,” Vox Sang. 72, 211–220 (1997).
[177] X. J. Zhao, S. Lustigman, Y. S. Li, M. E. Kenney, and E. Ben-Hur, “Structure-activity
and mechanism studies on silicon phthalocyanines with Plasmodium falciparum in the
dark and under red light,” Photochem. Photobiol. 66, 282–287 (1997).
[178] R. Kliukiene, A. Maroziene, N. Cenas, K. Becker, and J. S. Blanchard, “Photoinactiva-
tion of trypanothione reductase and glutathione reductase by Al-phthalocyanine tetrasu-
lfonate and hematoporphyrin,” Biochem. Biophys. Res. Commun. 218, 629–632 (1996).
[179] Z. Alouini and M. Jemli, “Destruction of helminth eggs by photosensitized porphyrin,”
J. Environ. Monit. 3, 548–551 (2001).
[180] M. R. Hamblin, D. A. O’Donnell, N. Murthy, C. H. Contag, and T. Hasan, “Rapid
control of wound infections by targeted photodynamic therapy monitored by in vivo
bioluminescence imaging,” Photochem. Photobiol. 75, 51–57 (2002).
[181] T. Dai, G. P. Tegos, T. Zhiyentayev, E. Mylonakis, and M. R. Hamblin, “Photody-
namic therapy for methicillin-resistant Staphylococcus aureus infection in a mouse skin
abrasion model,” Lasers Surg. Med. 42, 38–44 (2010).
[182] X. Ragas, T. Dai, G. P. Tegos, M. Agut, S. Nonell, and M. R. Hamblin, “Photodynamic
inactivation of Acinetobacter baumannii using phenothiazinium dyes: in vitro and in vivo
studies,” Lasers Surg. Med. 42, 384–390 (2010).

REFERENCES
461
[183] P. S. Zolfaghari, S. Packer, M. Singer, S. P. Nair, J. Bennett, C. Street, and M. Wilson,
“In vivo killing of Staphylococcus aureus using a light-activated antimicrobial agent,”
BMC Microbiol. 9, 27 (2009).
[184] A. Orenstein, D. Klein, J. Kopolovic, E. Winkler, Z. Malik, N. Keller, and Y. Nitzan, “The
use of porphyrins for eradication of Staphylococcus aureus in burn wound infections,”
FEMS Immunol. Med. Microbiol. 19, 307–314 (1997).
[185] S. A. Lambrechts, T. N. Demidova, M. C. Aalders, T. Hasan, and M. R. Hamblin,
“Photodynamic therapy for Staphylococcus aureus infected burn wounds in mice,”
Photochem. Photobiol. Sci. 4, 503–509 (2005).
[186] T. Dai, G. P. Tegos, Z. Lu, L. Huang, T. Zhiyentayev, M. J. Franklin, D. G. Baer, and
M. R. Hamblin, “Photodynamic therapy for Acinetobacter baumannii burn infections
in mice,” Antimicrob. Agents Chemother. 53, 3929–3934 (2009).
[187] F. Berthiaume, S. R. Reiken, M. Toner, R. G. Tompkins, and M. L. Yarmush, “Antibody-
targeted photolysis of bacteria in vivo,” Biotechnology (N Y) 12, 703–706 (1994).
[188] F. Gad, T. Zahra, K. P. Francis, T. Hasan, and M. R. Hamblin, “Targeted photodynamic
therapy of established soft-tissue infections in mice,” Photochem. Photobiol. Sci. 3,
451–458 (2004).
[189] G. Lombard and M. M. Lanotte, “The treatment of neurosurgical infections by lasers
and porphyrins,” Edizione Libreria Progetto 363–366 (1985).
[190] S. B. Brown, “Photodiagnosis and photodynamic therapy,” in 12th International Pho-
todynamic Association World Congress 6, 150–151 (Seattle, 2009).
[191] T. H. Clayton and P. V. Harrison, “Photodynamic therapy for infected leg ulcers,” Br. J.
Dermatol. 156, 384–385 (2007).
[192] O. Dortbudak, R. Haas, T. Bernhart, and G. Mailath-Pokorny, “Lethal photosensitization
for decontamination of implant surfaces in the treatment of peri-implantitis,” Clin. Oral.
Implants Res. 12, 104–108 (2001).
[193] J. A. Shibli, M. C. Martins, L. H. Theodoro, R. F. Lotufo, V. G. Garcia, and E. J. Mar-
cantonio, “Lethal photosensitization in microbiological treatment of ligature-induced
peri-implantitis: a preliminary study in dogs,” J. Oral. Sci. 45, 17–23 (2003).
[194] R. R. Hayek, N. S. Araujo, M. A. Gioso, J. Ferreira, C. A. Baptista-Sobrinho, A.
M. Yamada, and M. S. Ribeiro, “Comparative study between the effects of photody-
namic therapy and conventional therapy on microbial reduction in ligature-induced
peri-implantitis in dogs,” J. Periodontol. 76, 1275–1281 (2005).
[195] R. R. de Oliveira, H. O. Schwartz-Filho, A. B. Novaes, Jr., and M. Taba, Jr., “Antimicro-
bial photodynamic therapy in the non-surgical treatment of aggressive periodontitis: a
preliminary randomized controlled clinical study,” J. Periodontol. 78, 965–973 (2007).
[196] B. W. Sigusch, A. Pfitzner, V. Albrecht, and E. Glockmann, “Efficacy of photodynamic
therapy on inflammatory signs and two selected periodontopathogenic species in a
beagle dog model,” J. Periodontol. 76, 1100–1105 (2005).
[197] Z. Malik, J. Hanania, and Y. Nitzan, “Bactericidal effects of photoactivated porphyrins—
an alternative approach to antimicrobial drugs,” J. Photochem. Photobiol. B 5, 281–293
(1990).
[198] P. E. Bottura, J. Milanezi, L. A. Fernandes, H. C. Caldas, M. Abbud-Filho, V. G.
Garcia, and M. A. Baptista, “Nonsurgical periodontal therapy combined with laser and
photodynamic therapies for periodontal disease in immunosuppressed rats,” Transplant
Proc. 43, 2009–2016 (2011).

462
PHOTODYNAMIC THERAPY
[199] Y. Ogura, T. Ooka, Asadulghani, J. Terajima, J. P. Nougayrede, K. Kurokawa, K. Tashiro,
T. Tobe, K. Nakayama, S. Kuhara, E. Oswald, et al., “Extensive genomic diversity and
selective conservation of virulence-determinants in enterohemorrhagic Escherichia coli
strains of O157 and non-O157 serotypes,” Genome. Biol. 8, R138 (2007).
[200] G. P. Tegos, K. Masago, F. Aziz, A. Higginbotham, F. R. Stermitz, and M. R. Hamblin,
“Inhibitors of bacterial multidrug efflux pumps potentiate antimicrobial photoinactiva-
tion,” Antimicrob. Agents Chemother. 52, 3202–3209 (2008).
[201] V. Klepac-Ceraj, N. Patel, X. Song, C. Holewa, C. Patel, R. Kent, M. M. Amiji, and N.
S. Soukos, “Photodynamic effects of methylene blue-loaded polymeric nanoparticles
on dental plaque bacteria,” Lasers Surg. Med. 43, 600–606 (2011).
[202] I. N. Rocas, J. F. Siqueira, Jr., and K. R. Santos, “Association of Enterococcus
faecalis with different forms of periradicular diseases,” J. Endod. 30, 315–320
(2004).
[203] M. B. Fonseca, P. O. Junior, R. C. Pallota, H. F. Filho, O. V. Denardin, A. Rapoport,
R. A. Dedivitis, J. F. Veronezi, W. J. Genovese, and A. L. Ricardo, “Photodynamic
therapy for root canals infected with Enterococcus faecalis,” Photomed. Laser Surg. 26,
209–213 (2008).
[204] L. C. Souza, P. R. Brito, J. C. de Oliveira, F. R. Alves, E. J. Moreira, H. R. Sampaio-
Filho, I. N. Rocas, and J. F. Siqueira, Jr., “Photodynamic therapy with two different
photosensitizers as a supplement to instrumentation/irrigation procedures in promoting
intracanal reduction of Enterococcus faecalis,” J. Endod. 36, 292–296 (2010).
[205] R. Ng, F. Singh, D. A. Papamanou, X. Song, C. Patel, C. Holewa, N. Patel, V. Klepac-
Ceraj, C. R. Fontana, R. Kent, et al., “Endodontic photodynamic therapy ex vivo,”
J. Endod. 37, 217–222 (2011).
[206] N. S. Soukos, P. S. Chen, J. T. Morris, K. Ruggiero, A. D. Abernethy, S. Som, F.
Foschi, S. Doucette, L. L. Bammann, C. R. Fontana, et al., “Photodynamic therapy for
endodontic disinfection,” J. Endod. 32, 979–984 (2006).
[207] A. S. Garcez, M. S. Ribeiro, G. P. Tegos, S. C. Nunez, A. O. Jorge, and M. R. Hamblin,
“Antimicrobial photodynamic therapy combined with conventional endodontic treat-
ment to eliminate root canal biofilm infection,” Lasers Surg. Med. 39, 59–66 (2007).
[208] E. G. Mima, A. C. Pavarina, L. N. Dovigo, C. E. Vergani, C. A. Costa, C. Kurachi,
and V. S. Bagnato, “Susceptibility of Candida albicans to photodynamic therapy in a
murine model of oral candidosis,” Oral. Surg. Oral. Med. Oral. Pathol. Oral. Radiol.
Endod. 109, 392–401 (2010).
[209] M. C. Teichert, J. W. Jones, M. N. Usacheva, and M. A. Biel, “Treatment of oral
candidiasis with methylene blue-mediated photodynamic therapy in an immunodeficient
murine model,” Oral. Surg. Oral. Med. Oral. Pathol. Oral. Radiol. Endod. 93, 155–160
(2002).
[210] P. Chondros, D. Nikolidakis, N. Christodoulides, R. Rossler, N. Gutknecht, and A.
Sculean, “Photodynamic therapy as adjunct to non-surgical periodontal treatment in
patients on periodontal maintenance: a randomized controlled clinical trial,” Lasers
Med. Sci. 24, 681–688 (2009).
[211] N. Christodoulides, D. Nikolidakis, P. Chondros, J. Becker, F. Schwarz, R. Rossler,
and A. Sculean, “Photodynamic therapy as an adjunct to non-surgical periodon-
tal treatment: a randomized, controlled clinical trial,” J. Periodontol. 79, 1638–1644
(2008).

REFERENCES
463
[212] A. Braun, C. Dehn, F. Krause, and S. Jepsen, “Short-term clinical effects of adjunctive
antimicrobial photodynamic therapy in periodontal treatment: a randomized clinical
trial,” J. Clin. Periodontol. 35, 877–884 (2008).
[213] B. W. Sigusch, M. Engelbrecht, A. Volpel, A. Holletschke, W. Pfister, and J. Schutze,
“Full-mouth antimicrobial photodynamic therapy in Fusobacterium nucleatum-infected
periodontitis patients,” J. Periodontol. 81, 975–981 (2010).
[214] A. Ruhling, J. Fanghanel, M. Houshmand, A. Kuhr, P. Meisel, C. Schwahn, and
T. Kocher, “Photodynamic therapy of persistent pockets in maintenance patients—a
clinical study,” Clin. Oral. Investig. 14, 637–644 (2010).
[215] S. L. Pinheiro, A. A. Schenka, A. A. Neto, C. P. de Souza, H. M. Rodriguez, and
M. C. Ribeiro, “Photodynamic therapy in endodontic treatment of deciduous teeth,”
Lasers Med. Sci. 24, 521–526 (2009).
[216] O. E. Akilov, S. Kosaka, K. O’Riordan, and T. Hasan, “Photodynamic therapy for cuta-
neous leishmaniasis: the effectiveness of topical phenothiaziniums in parasite eradica-
tion and Th1 immune response stimulation,” Photochem. Photobiol. Sci. 6, 1067–1075
(2007).
[217] O. E. Akilov, S. Kosaka, K. O’Riordan, and T. Hasan, “Parasiticidal effect of delta-
aminolevulinic acid-based photodynamic therapy for cutaneous leishmaniasis is indirect
and mediated through the killing of the host cells,” Exp. Dermatol. 16, 651–660 (2007).
[218] O. E. Akilov, W. Yousaf, S. X. Lukjan, S. Verma, and T. Hasan, “Optimization of topical
photodynamic therapy with 3,7-bis(di-n-butylamino)phenothiazin-5-ium bromide for
cutaneous leishmaniasis,” Lasers Surg. Med. 41, 358–365 (2009).
[219] A. Asilian and M. Davami, “Comparison between the efficacy of photodynamic therapy
and topical paromomycin in the treatment of Old World cutaneous leishmaniasis: a
placebo-controlled, randomized clinical trial,” Clin. Exp. Dermatol. 31, 634–637 (2006).
[220] K. Gardlo, Z. Horska, C. D. Enk, L. Rauch, M. Megahed, T. Ruzicka, and C. Fritsch,
“Treatment of cutaneous leishmaniasis by photodynamic therapy,” J. Am. Acad. Der-
matol. 48, 893–896 (2003).
[221] K. O’Riordan, D. S. Sharlin, J. Gross, S. Chang, D. Errabelli, O. E. Akilov, S. Kosaka,
G. J. Nau, and T. Hasan, “Photoinactivation of mycobacteria in vitro and in a new
murine model of localized Mycobacterium bovis BCG-induced granulomatous infec-
tion,” Antimicrob. Agents Chemother. 50, 1828–1834 (2006).
[222] K. O’Riordan, O. E. Akilov, S. K. Chang, J. W. Foley, and T. Hasan, “Real-time fluo-
rescence monitoring of phenothiazinium photosensitizers and their anti-mycobacterial
photodynamic activity against Mycobacterium bovis BCG in in vitro and in vivo models
of localized infection,” Photochem. Photobiol. Sci. 6, 1117–1123 (2007).
[223] J. Y. Jung, P. S. Kwon, J. C. Ahn, R. Ge, M. W. Suh, and C. K. Rhee, “In vitro and
in vivo photodynamic therapy of otitis media in gerbils,” Laryngoscope 119, 1781–1787
(2009).
[224] S. K. Bisland, C. Chien, B. C. Wilson, and S. Burch, “Pre-clinical in vitro and in vivo
studies to examine the potential use of photodynamic therapy in the treatment of
osteomyelitis,” Photochem. Photobiol. Sci. 5, 31–38 (2006).
[225] Z. Smetana, Z. Malik, A. Orenstein, E. Mendelson, and E. Ben-Hur, “Treatment of viral
infections with 5-aminolevulinic acid and light,” Lasers Surg. Med. 21, 351–358 (1997).
[226] C. Moore, C. Wallis, J. L. Melnick, and M. D. Kuns, “Photodynamic treatment of herpes
keratitis,” Infect Immun. 5, 169–171 (1972).

464
PHOTODYNAMIC THERAPY
[227] M. G. Myers, M. N. Oxman, J. E. Clark, and K. A. Arndt, “Failure of neutral-red
photodynamic inactivation in recurrent herpes simplex virus infections,” N. Engl. J.
Med. 293, 945–949 (1975).
[228] T. W. Chang, N. Fiumara, and L. Weinstein, “Genital herpes: treatment with methylene
blue and light exposure,” Int. J. Dermatol. 14, 69–71 (1975).
[229] A. P. Roome, A. E. Tinkler, A. L. Hilton, D. G. Montefiore, and D. Waller, “Neutral
red with photoinactivation in the treatment of herpes genitalis,” Br. J. Vener. Dis. 51,
130–133 (1975).
[230] M. J. Shikowitz, A. L. Abramson, K. Freeman, B. M. Steinberg, and M. Nouri, “Efficacy
of DHE photodynamic therapy for respiratory papillomatosis: immediate and long-term
results,” Laryngoscope 108, 962–967 (1998).
[231] A. L. Abramson, M. J. Shikowitz, V. M. Mullooly, B. M. Steinberg, C. A. Amella,
and H. R. Rothstein, “Clinical effects of photodynamic therapy on recurrent laryngeal
papillomas,” Arch. Otolaryngol. Head Neck Surg. 118, 25–29 (1992).
[232] J. Bujia, J. Feyh, and E. Kastenbauer, “Photodynamic therapy with derivatives from
hemotoporphyrines for recurrent laryngeal papillomatosis of the children. Early results,”
An. Otorrinolaringol. Ibero. Am. 20, 251–259 (1993).
[233] I. M. Stender, J. Lock-Andersen, and H. C. Wulf, “Recalcitrant hand and foot warts
successfully treated with photodynamic therapy with topical 5-aminolaevulinic acid: a
pilot study,” Clin. Exp. Dermatol. 24, 154–159 (1999).
[234] M. Y. Lin and L. H. Xiang, “Topical 5-aminolevulinic acid photodynamic therapy for
recalcitrant facial flat wart in Chinese subjects,” J. Dermatol. 35, 658–661 (2008).
[235] R. Rossi, N. Bruscino, F. Ricceri, M. Grazzini, M. Dindelli, and T. Lotti, “Photodynamic
treatment for viral infections of the skin,” G. Ital. Dermatol. Venereol. 144, 79–83 (2009).
[236] H. Ichimura, S. Yamaguchi, A. Kojima, T. Tanaka, K. Niiya, M. Takemori, K. Hasegawa,
and R. Nishimura, “Eradication and reinfection of human papillomavirus after photo-
dynamic therapy for cervical intraepithelial neoplasia,” Int. J. Clin. Oncol. 8, 322–325
(2003).
[237] Y. P. Istomin, T. P. Lapzevich, V. N. Chalau, S. V. Shliakhtsin, and T. V. Trukhachova,
“Photodynamic therapy of cervical intraepithelial neoplasia grades II and III with Pho-
tolon,” Photodiagnosis Photodyn. Ther. 7, 144–151 (2010).
[238] E. V. Ross, R. Romero, N. Kollias, C. Crum, and R. R. Anderson, “Selectivity
of protoporphyrin IX fluorescence for condylomata after topical application of 5-
aminolaevulinic acid: implications for photodynamic treatment,” Br. J. Dermatol. 137,
736–742 (1997).
[239] V. Nucci, D. Torchia, and P. Cappugi, “Treatment of anogenital condylomata acuminata
with topical photodynamic therapy: report of 14 cases and review,” Int. J. Infect. Dis.
14(Suppl 3), e280–e282 (2010).
[240] M. K. Fehr, C. F. Chapman, T. Krasieva, B. J. Tromberg, J. L. McCullough, M. W. Berns,
and Y. Tadir, “Selective photosensitizer distribution in vulvar condyloma acuminatum
after topical application of 5-aminolevulinic acid,” Am. J. Obstet. Gynecol. 174, 951–957
(1996).
[241] E. S. Abdel-Hady, P. Martin-Hirsch, M. Duggan-Keen, P. L. Stern, J. V. Moore, G. Cor-
bitt, H. C. Kitchener, and I. N. Hampson, “Immunological and viral factors associated
with the response of vulval intraepithelial neoplasia to photodynamic therapy,” Cancer
Res. 61, 192–196 (2001).

REFERENCES
465
[242] Z. P. Bernstein, T. Dougherty, S. Gollnick, S. A. Schwartz, S. D. Mahajan, J. Kepner,
A. Sumlin, C. Stewart, P. Wallace, A. Adal, et al., “Photopheresis in HIV-1 infected
patients utilizing benzoporphyrin derivative (BPD) verteporfin and light,” Curr. HIV
Res. 6, 152–163 (2008).
[243] C. H. Wilder-Smith, P. Wilder-Smith, P. Grosjean, H. van den Bergh, A. Woodtli,
P. Monnier, G. Dorta, F. Meister, and G. Wagnieres, “Photoeradication of Helicobacter
pylori using 5-aminolevulinic acid: preliminary human studies,” Lasers Surg. Med. 31,
18–22 (2002).
[244] M. R. Hamblin, J. Viveiros, C. Yang, A. Ahmadi, R. A. Ganz, and M. J. Tolkoff,
“Helicobacter pylori accumulates photoactive porphyrins and is killed by visible light,”
Antimicrob. Agents Chemother. 49, 2822–2827 (2005).
[245] R. A. Ganz, J. Viveiros, A. Ahmad, A. Ahmadi, A. Khalil, M. J. Tolkoff, N. S. Nishioka,
and M. R. Hamblin, “Helicobacter pylori in patients can be killed by visible light,”
Lasers Surg. Med. 36, 260–265 (2005).
[246] A. J. Lembo, R. A. Ganz, S. Sheth, D. Cave, C. Kelly, P. Levin, P. T. Kazlas, P. C.
Baldwin, 3rd, W. R. Lindmark, J. R. McGrath, et al., “Treatment of Helicobacter pylori
infection with intra-gastric violet light phototherapy: a pilot clinical trial,” Lasers Surg.
Med. 41, 337–344 (2009).
[247] G. E. Lang, S. Mennel, G. Spital, J. Wachtlin, B. Jurklies, H. Heimann, B. Damato, and
C. H. Meyer, “Different indications of photodynamic therapy in ophthalmology,” Klin
Monbl Augenheilkd 226, 725–739 (2009).
[248] R. Wormald, J. Evans, L. Smeeth, and K. Henshaw, “Photodynamic therapy for neovas-
cular age-related macular degeneration,” Cochrane Database Syst. Rev. 19, CD002030
(2005).
[249] L. F. Stuart, W. B. Jeffrey, G. M. Maureen, and C. H. Allen, “Age-related macular
degeneration,” New. Eng. J. Med. 342, 483–492 (2000).
[250] R. K. Chowdhary, I. Shariff, and D. Dolphin, “Drug release characteristics of lipid based
benzoporphyrin derivative,” J. Pharm. Pharm. Sci. 6, 13–19 (2003).
[251] D. Husain, J. W. Miller, N. Michaud, E. Connolly, T. J. Flotte, and E. S. Gragoudas,
“Intravenous infusion of liposomal benzoporphyrin derivative for photodynamic ther-
apy of experimental choroidal neovascularization,” Arch. Ophthalmol. 114, 978–985
(1996).
[252] TAP-Study-Group, “Vertoporfin therapy for subfoveal choroidal neovascularization
in age-related macular degeneration. Three-year results of an open-labeled exten-
sion of 2 randomized clinical trials-TAP report 5,” Arch. Opthalmol. 120, 1307–1314
(2002).
[253] TAP-Study-Group, “Effects of vertoporfin therapy on contrast on sensitivity: Results
from the treatment of age-related macular degeneration with photodynamic therapy
(TAP) investigation-TAP report No.4,” Retina 22, 536–544 (2002).
[254] VIP-Study-Group, “Photodynamic therapy of subfoveal choroidal neovscularization in
pathologic myopia with verteporfin. 1-year results of a randomized clinical trial-VIP
report no.1,” Opthalmology 108, 841–852 (2001).
[255] VIP-Study-Group, “Verteporfin therapy of subfoveal choroidal neovascularization in
age-related macular degeneration: two-year results of a randomized clinical trial includ-
ing lesions with occult with no classic choroidal neovascularization verteporfin in pho-
todynamic therapy report 2,” Am. J. Ophthalmol. 131, 541–560 (2001).

466
PHOTODYNAMIC THERAPY
[256] VIP-Study-Group, “Verteporfin therapy of subfoveal choroidal neovascularization in
pathologic myopia: 2-year results of a randomized clinical trial VIP report no. 3,”
Ophthalmology 110, 667–673 (2003).
[257] P. Kaiser, “Verteporfin therapy of subfoveal choroidal neovascularization in age-related
macular degeneration: 5-year results of two randomized clinical trials with an open-label
extension: TAP report no. 8,” Graefes Arch. Clin. Exp. Ophthalmol. 244, 1132–1142
(2006).
[258] K. Hayashi, K. Ohno-Matsui, S. Teramukai, N. Shimada, M. Moriyama, W. Hara,
T. Yoshida, T. Tokoro, and M. Mochizuki, “Photodynamic therapy with verteporfin for
choroidal neovascularization of pathologic myopia in Japanese patients: comparison
with nontreated controls,” Am. J. Ophthalmol. 145, 518–526 (2008).
[259] A. P. Castano, T. N. Demidova, and M. R. Hamblin, “Mechanisms in photodynamic ther-
apy: Part three ¨AˆıPhotosensitizer pharmacokinetics, biodistribution, tumor localization
and modes of tumor destruction,” Photodiagnosis Photodyn. Ther. 2, 91–106 (2005).
[260] X. Y. Shen, N. Zacal, G. Singh, and A. J. Rainbow, “Alterations in mitochondrial
and apoptosis-regulating gene expression in photodynamic therapy-resistant variants of
HT29 colon carcinoma cells,” Photochem. Photobiol. 81, 306–313 (2005).
[261] N. L. Oleinick, R. L. Morris, and I. Belichenko, “The role of apoptosis in response
to photodynamic therapy: what, where, why, and how,” Photochem. Photobiol. Sci. 1,
1–21 (2002).
[262] C. J. Gomer, S. W. Ryter, A. Ferrario, N. Rucker, S. Wong, and A. M. Fisher, “Photody-
namic therapy-mediated oxidative stress can induce expression of heat shock proteins,”
Cancer Res. 56, 2355–2360 (1996).
[263] M. Korbelik, J. Sun, and I. Cecic, “Photodynamic therapy-induced cell surface expres-
sion and release of heat shock proteins: relevance for tumor response,” Cancer Res. 65,
1018–1026 (2005).
[264] D. J. Granville, C. M. Carthy, H. Jiang, J. G. Levy, B. M. McManus, J. Y. Matroule,
J. Piette, and D. W. Hunt, “Nuclear factor-kappaB activation by the photochemothera-
peutic agent verteporfin,” Blood 95, 256–262 (2000).
[265] G. Kick, G. Messer, G. Plewig, P. Kind, and A. E. Goetz, “Strong and prolonged
induction of c-jun and c-fos proto-oncogenes by photodynamic therapy,” Br. J. Cancer
74, 30–36 (1996).
[266] J. Sun, I. Cecic, C. S. Parkins, and M. Korbelik, “Neutrophils as inflammatory and
immune effectors in photodynamic therapy-treated mouse SCCVII tumours,” Pho-
tochem. Photobiol. Sci. 1, 690–695 (2002).
[267] W. J. de Vree, M. C. Essers, H. S. de Bruijn, W. M. Star, J. F. Koster, and W. Sluiter,
“Evidence for an important role of neutrophils in the efficacy of photodynamic therapy
in vivo,” Cancer Res. 56, 2908–2911 (1996).
[268] G. Krosl, M. Korbelik, and G. J. Dougherty, “Induction of immune cell infiltration into
murine SCCVII tumour by photofrin-based photodynamic therapy,” Br. J. Cancer 71,
549–555 (1995).
[269] I. Cecic, K. Serrano, M. Gyongyossy-Issa, and M. Korbelik, “Characteristics of com-
plement activation in mice bearing Lewis lung carcinomas treated by photodynamic
therapy,” Cancer Lett. 225, 215–223 (2005).
[270] W. J. Magner and T. B. Tomasi, “Apoptotic and necrotic cells induced by different
agents vary in their expression of MHC and costimulatory genes,” Mol. Immunol. 42,
1033–1042 (2005).

REFERENCES
467
[271] W. C. Bartholomae, F. H. Rininsland, J. C. Eisenberg, B. O. Boehm, P. V. Lehmann,
and M. Tary-Lehmann, “T cell immunity induced by live, necrotic, and apoptotic tumor
cells,” J. Immunol. 173, 1012–1022 (2004).
[272] C. Volanti, G. Gloire, A. Vanderplasschen, N. Jacobs, Y. Habraken, and J. Piette, “Down-
regulation of ICAM-1, and VCAM-1 expression in endothelial cells treated by photo-
dynamic therapy,” Oncogene 23, 8649–8658 (2004).
[273] W. Sluiter, W. J. de Vree, A. Pietersma, and J. F. Koster, “Prevention of late lumen
loss after coronary angioplasty by photodynamic therapy: role of activated neutrophils,”
Mol. Cell. Biochem. 157, 233–238 (1996).
[274] M. Korbelik, G. Krosl, J. Krosl, and G. J. Dougherty, “The role of host lymphoid
populations in the response of mouse EMT6 tumor to photodynamic therapy,” Cancer
Res. 56, 5647–5652 (1996).
[275] A. P. Castano, P. Mroz, and M. R. Hamblin, “Photodynamic therapy and anti-tumour
immunity,” Nat. Rev. Cancer 6, 535–545 (2006).


11
IMAGING AND PROBING CELLS
BEYOND THE OPTICAL
DIFFRACTION LIMIT
Mark Sch¨uttpelz and Thomas Huser
Biomolecular Photonics, Department of Physics, University of Bielefeld,
Bielefeld, Germany
11.1
THE QUEST FOR ACHIEVING OPTICAL RESOLUTION
BEYOND ABBE’S LIMIT
The nondestructive chemical analysis of biological structures and dynamic processes
in the crowded intracellular environment, at cellular membranes, and between cells
with a spatial resolution well beyond the diffraction limit is a major problem in
biomedical research. Many fundamental biological interactions and biochemical pro-
cesses that occur at the subcellular length scale can ultimately lead to the onset of
disease. These underlying causes, such as genetic mutations, the overexpression of
proteins, misfolding of proteins, absence of proteins, or interactions between macro-
molecules that should not be interacting, can often be identified using biochemical
assays after large quantities of cells have been digested and dissolved into their
individual macromolecular constituents, but typically not within the cellular con-
text. Here, specialized approaches, such as fluorescence in situ hybridization (FISH)
or specific live cell fluorophores have to be used, although many of these are not
applicable in vivo [1]. Imagine, for example, the structure and size of mitochondria.
Mitochondria are the powerhouses of cells, yet they have dimensions that can barely
be imaged by standard optical microscopy, so slight structural defects in mitochondria
are difficult to very difficult to detect. Mitochondria are, however, the first cellular
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
469

470
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
compartments that signal the tendency of a cell to die, that is, to undergo apoptosis.
It has, for example, been reported, that one way by which stem cells enable diseased
cells to recover is by the transfer of mitochondria [2]. Another important example
for subcellular structures affecting the health of an entire organism are slight differ-
ences (mutations) in structural proteins that, for example, make up the cytoskeleton
of cells, which leads to abnormal shapes of organs, body parts, or the fatal cessation
of function, for example, of heart muscle cells. Most viruses are also well below
the spatial resolution of optical microscopes, making it difficult to follow their life
cycle within a cell. Chromosomes that store the genetic code of a cell can only be
visualized just before and during mitosis, but not in chromatin during the often very
long G phase. Despite these shortcomings in its spatial resolution, optical microscopy
is, however, still the most important imaging technique for biomedical research due
to its affordability, long history, and the large number of contrast mechanisms avail-
able. It also remains the method of choice for live cell imaging [1]. These facts have
continued to drive research into more advanced optical microscopy techniques, and,
lately, especially in the area of super-resolution optical microscopy.
Arguably, theorigins of super-resolutionoptical microscopyarecloselytiedinwith
the first experimental realization of near-field optics, which enabled optical imaging
beyond the diffraction limit. The primary idea behind nanoscale optical imaging and
analysis is that in order to overcome the hurdles imposed by Abbe’s diffraction limit
[1], a nanoscale light source has to be brought in close proximity with the sample to
probe a subset of the sample. By raster scanning the source across the sample, a map
of the sample on the sub-wavelength scale can then be created, which bridges the gap
between conventional optics and electron-beam or X-ray based analyses. Imaging,
local probing, and manipulation of biological targets on the nanoscale or the use
of optically active nanoscale objects is then exploited to either obtain higher spatial
resolution or to achieve higher sensitivity, and, for example, to obtain information
about individual events even in the crowded molecular environment inside a cell.
Even though the initially most successful realization of near-field optics, which
was also successfully commercialized, has seen a number of interesting applications
in materials science, its use with biological materials is rather limited. This is mostly
due to the fact, that early near-field optical probes were made by pulling optical glass
fibers to very fine tips that are subsequently coated with a thin film of aluminum
to prevent leakage of light except at the very end of the probe tip. This results in
optical apertures acting as light sources with a diameter of 50–100 nm on average.
The concept of and an example for such near-field optical probe tips is shown
in Figure 11.1a. These tips are rather fragile and tedious to handle, which makes
their use in aqueous environments particularly difficult. Some of the more interesting
biological applications involving such tips, such as the sub-wavelength-scale imaging
of the co-localization of fluorescently labeled membrane-associated proteins in cells
[3], or intracellular probing of analytes, have thus already been demonstrated many
years ago. Nonetheless, these applications have demonstrated the difficulties and
limitations of near-field optical techniques for live cell applications and have sparked
a large number of follow-on developments, such as tip-enhanced spectroscopies, the

THE QUEST FOR ACHIEVING OPTICAL RESOLUTION BEYOND ABBE’S LIMIT
471
FIGURE 11.1
Overview of different optical super-resolution microscopy techniques.
(a) Near-field optical microscopy makes use of ultrasharp glass fiber tips that are metal-coated
to prevent leakage from regions other than the very end of the tip. The inset shows a microscope
image of such a fiber tip with light launched through the tip. (b) Single molecule localization
microscopy fits the position of single fluorescent molecules with a 2D Gaussian fit function to
determine the center of mass of fluorescent spots. (c) Stimulated emission depletion (STED)
microscopy makes use of the principle of stimulated emission to remove fluorescence from
parts of a confocal laser beam. (d) Structured illumination microscopy utilizes 2D/3D frequency
mixing between a known illumination pattern and the sample to reconstruct the original sample
with double the resolution. (For a color version of this figure, see the color plate section.)
development of novel probes, the use of single molecule fluorescence in biology, and
a number of novel concepts for far-field optical super-resolution microscopies that
do not require the use of probe tips. Tip-enhanced spectroscopies have, nevertheless,
continued to be of interest to a number of research groups. In this modality, the fine,
nanometer-sized tips of scanning probe microscopes are typically coated with gold
or silver to facilitate the excitation of surface plasmons in these tips. The resulting
enhanced electric fields can then serve as a means, for example, to locally excite
fluorophores by two-photon excitation, or to enhance the inelastic scattering of light,
that is, Raman scattering, underneath the tips [4].
Even though the dynamic registration of single fluorescent molecules in aqueous
media was achieved a number of years earlier, near-field optics also marked the
beginnings of single molecule fluorescence microscopy [5]. The extension of single
molecule fluorescence microscopy to biological applications in the crowded envi-
ronment of cells, however, has proven difficult to achieve, because single molecule
detection with conventional optics results in diffraction-limited laser spots on the
order of a few hundred nanometers and is thus limited to nanomolar molecular

472
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
concentrations to ensure that only a single molecule is probed within the probe vol-
ume at any given point in time. Typical association constants of biological molecules,
in particular proteins, however, are in the micromolar concentration range. This
requires that one focuses either on rare events, for example, low copy number gene
expression [6], or sparse labeling of the molecules of interest, where only approx-
imately every thousandth molecule is fluorescently labeled [7], or the more recent
introduction of photoswitchable fluorophores. If, however, biological targets, for
example, motor proteins, are labeled with single fluorescent molecules, and if they
can operate under single molecule conditions, then their position can be located with
very high precision of close to 1 nm [8]. The point spread function (PSF) of a micro-
scope objective with high numerical aperture can be determined very accurately and,
in fact, the emission pattern of a single fluorescent molecule typically represents
exactly the PSF. Typical PSFs in the lateral dimensions of a microscope can be fitted
with high accuracy by two-dimensional (2D) Gaussian fit functions. Localization of
the position of a single molecule to within 1.5 nm has been demonstrated [8] (see
the schematics in Fig. 11.1b, where the hourglass-shaped cross section of a confo-
cal microscope is used to excite a molecule, resulting in the 2D intensity spot (the
PSF) shown underneath the upper figure, which can be fit with a 2D Gaussian to
determine the spot’s center of mass). This is sufficient to determine even small steps
during enzymatic turnovers or the translational movement of molecular motors. If
labeling with brighter probes is possible, for example, through attachment of multiple
fluorophores, or fluorescent nanoparticles, this can even be extended to the cellular
analysis of vesicular trafficking by motor proteins [9] or virus–cell interactions [10].
11.1.1
The Advent of Far-Field Optical Super-Resolution
Imaging Techniques
Super-resolution microscopy techniques have seen a dramatic increase in activity in
recent years. In essence, all these techniques rely on schemes that make use of our
ability to control molecular emission properties in the far-field. They can be roughly
separated into approaches that either use our ability to locate individual fluorescent
molecules with high precision (see Fig. 11.1b), or approaches that reduce the probe
volume by exploiting specific molecular processes.
The first successful demonstration (also in a commercial sense) of far-field super-
resolution optical microscopy was achieved using the concept of stimulated emission
depletion (STED) [11]. It is a direct extension of the popular confocal fluorescence
microscopy [1], providing instant optical sectioning capability and single molecule
sensitivity. STED microscopy is not only a direct super-resolution imaging technique,
in many cases requiring no further image processing or mathematical deconvolution,
but it is also directly compatible with most other confocal optical analysis techniques
[12]. STED makes use of the fact that fluorescence can be depleted by stimulating the
emission of photons from excited states—the same mechanism that is, for example,
used to achieve tunable emission in dye lasers. In STED, a shaped laser beam with
a wavelength within the emission spectrum of the fluorophores in the sample is

THE QUEST FOR ACHIEVING OPTICAL RESOLUTION BEYOND ABBE’S LIMIT
473
overlapped with the excitation beam (see the cross-section schematics in Fig. 11.1c).
The shape of this STED beam is typically a donut mode, which depletes fluorescence
in those parts of the sample where both beams overlap, while the central hole continues
to spontaneously emit fluorescent photons. The resolution of this technique is, in
principle, only limited by the specific depletion pattern used, by the amount of
overlap that can be achieved between the two beams, and the intensity of the depletion
beam [13] (among other, somewhat less stringent parameters, such as the fluorophore
density, or the signal-to-noise ratio with which the fluorescence is detected).
In a technically somewhat simpler implementation compared to STED that also
achieves optical super-resolution, structured illumination microscopy has gained
considerable interest, mostly due to the fact that it can readily be applied to most
samples prepared for conventional fluorescence microscopy. Structured illumination
microscopy acquires high-resolution images of fluorescent samples by exciting them
with a sinusoidal illumination pattern rather than uniform illumination [14]. No spe-
cial fluorescent probes are required for this type of super-resolution microscopy, but
they might be beneficial in enhancing its ultimately achievable resolution even further.
In structured illumination microscopy, a periodic illumination pattern is created in the
focal plane and then moved across the sample laterally and at different angles. A small
number (∼9–15) images have to be acquired in order to reconstruct a high-resolution
image per vertical plane. This scheme works because sample features with higher
spatial frequency than the illumination pattern are modulated by the pattern and result
in beat frequencies that fall within the transfer function of the microscope (see the
schematics in Fig. 11.1d). Since the periodicity of the illumination pattern is known,
its effects on the image can be calculated and high-resolution images can be recon-
structed. By moving the sample through the illumination pattern in the vertical direc-
tion, high-resolution images of the entire sample in all three dimensions are obtained.
Alternatively, in the mid-2000s, wide-field microscopy approaches utilizing the
localization of single fluorescent molecules have emerged. The most prominent imple-
mentations of this concept, such as photoactivated localization microscopy (PALM)
[15] or stochastic optical reconstruction microscopy (STORM) [16] use photoswitch-
ing of molecular fluorescence emission to identify individual molecules, the coordi-
nates of which are then located on the nanometer scale. Typically, PALM and STORM
require the acquisition of large amounts of image data, each of which have to be fitted
to the known PSF of the microscope system, and the final images have to be recon-
structed from the large set of individual snapshots. Special fluorescent probes with
photoswitchable emission are often required for this purpose. These techniques work
well in the lateral dimensions, but extension to the third dimension at high resolution
remains somewhat challenging, and it seems unlikely that the extensive number of
images that have to be acquired together with the high rate of sample exposure, and
the computational effort needed for reconstructing images will permit rapid live cell
imaging anytime soon.
In the following we will discuss the most prominent far-field super-resolution opti-
cal microscopy approaches in greater detail, discuss some of their recent successes,
and speculate on their potential and further developments for the near future.

474
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
11.2
STIMULATED EMISSION DEPLETION MICROSCOPY
Stimulated emission depletion (STED) microscopy is based on the ingenious concept
first postulated by Einstein that the emission of fluorophores can be stimulated when
a strong field at a wavelength within their emission spectrum is present. The use of
this concept to overcome the diffraction limit was first suggested by Hell in 1994
[11]. The idea behind it is actually quite simple and related to some other approaches
that play with the fluorescent states of molecules. If, after the excitation of a large
number of fluorescent molecules, for example, in the confocal spot of a laser scanning
microscope, one can selectively remove some of these molecules from contributing
to the signal, then the detection volume can be considerably decreased. The selective
removal of such molecules is called “depletion.” To truly remove most or, possibly,
all fluorescent molecules from the excited state in this region, the depletion has
to be saturated, requiring fairly high laser intensities. In the case of STED, the
excited state fluorescence of some of the molecules in the excitation volume is
depleted by stimulating their emission, which occurs on much faster timescales and
in a spectrally much narrower region than spontaneous fluorescence. The trick then
becomes to find ways to distinguish the spontaneous emission of fluorescence from
the stimulated emission. Multiple such ways of distinguishing stimulated emission
from spontaneous emission, such as time-gated fluorescence detection, depletion at
the far end of the emission spectrum, and detection of the spontaneous emission
by bandpass filters, or the use of narrow blocking filters that prevent the spectrally
narrow stimulated emission from reaching the detector have been demonstrated [17].
The nonlinear nature of saturated emission depletion is what ultimately improves the
spatial resolution of STED microscopy, and the classical Abbe lateral resolution limit
transforms into [17]
Δr ≈
𝜆
2NA
√
1 + I∕Isat
,
(11.1)
where I/Isat is the ratio between the excitation intensity I and the intensity of the
saturated depletion beam Isat. This implies that for a ratio of I/Isat = 100, the theoretical
improvement in the lateral resolution of a STED microscope is about a factor of
10 improved over Abbe’s limit.
The major advantage of STED microscopy is that in most cases no special sample
preparation is required (some fluorophores are not as conducive to STED as most
others), and that, again in most cases, no further image processing or mathematical
deconvolution is required. The detection sensitivity of STED microscopy is at the level
of single fluorescent molecules, and in some of its most impressive demonstrations,
the increase in spatial resolution has actually been shown by imaging fluorescent
molecules that were immobilized on a surface. In STED, a shaped laser beam with
a wavelength within the emission spectrum of the fluorescent probes in the sample
is overlapped with the excitation beam (see Fig. 11.2). In the original experimental
realization the shaped depletion beam was sent through a phase mask, resulting in
two foci in the axial direction that were overlapped with the confocal excitation

STIMULATED EMISSION DEPLETION MICROSCOPY
475
FIGURE 11.2
The concept of stimulated emission depletion (STED). (a) Fluorophores
image with the diffraction-limited spot of a confocal microscope result in an optical resolution
of approximately 250 nm. (b) If a donut-shaped saturated depletion beam is superposed onto
the excitation beam, fluorescence within the depletion pattern is removed by STED, resulting
in a reduced spot size of the remaining spontaneous fluorescence emission.
volume [18]. This resulted in an ∼5× improvement in the axial resolution of a
confocal microscope. In more recent implementations, the shape of the STED beam
is typically chosen to be a donut mode, which depletes fluorescence evenly in all
lateral directions where both beams overlap, while the central “hole” of the donut
continues to spontaneously emit fluorescent photons. The resolution of this technique
is, in principle, only limited by the specific depletion pattern used, and the intensity
of the depletion beam [13]. As a super-resolution microscopy technique, STED has
been demonstrated to produce routine images with ∼20 nm resolution in the lateral
directions and, when combined with counter-propagating beams (4pi illumination,
isoSTED), even in the vertical direction. When applied to more robust fluorescent
probes, that is, nitrogen vacancies in diamond, an even higher spatial resolution
has been accomplished. In fluorescence correlation spectroscopy mode, where the
excitation and STED beams are focused into a solution of probe molecules, a reduction
of the excitation volume by a factor of five has been demonstrated [19]. This same
experiment also demonstrated the single molecule fluorescence sensitivity of STED.
More recently, fluorescence lifetime imaging was achieved with STED by combining
a broadband super-continuum-generating pulsed laser as excitation source together
with single photon counting [20]. Other, important aspects that have recently been
demonstrated using STED include the implementation of multicolor detection [21],
and video-rate imaging of synaptic vesicle movement [22]. The limitations of STED
microscopy, as well as those of the other super-resolution methods presented here
will be discussed at the end of this article.

476
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
11.2.1
Applications of STED Microscopy
A large number of technical improvements since its initial realization, such as
the use of coherent super-continuum white-light sources, or inexpensive polymer-
based phase masks have made STED microscopy ever more attractive for biological
research. Most recently, even deep-tissue imaging in the brain of a living mouse
has been demonstrated [23], breaking another barrier for super-resolution optical
microscopy. Another recent impressive use of STED microscopy is its application to
imaging the clustering of a surface glycoprotein on the surface of individual virions
of the human immunodeficiency virus (HIV) indicating that STED and other super-
resolution microscopy approaches can be used to distinguish between different (e.g.,
infectious and immature) forms of virus [24].
One of the most impressive demonstrations of STED microscopy was its recent
use in the search for lipid rafts [25]. Lipid rafts are believed to be intermediate struc-
tures on the cellular plasma membrane, rich in sphingolipids and cholesterol, which
represent a different lipid phase. These structures, which have been the topic of active
investigation for the last 20 years, are thought to have dimensions on the order of a few
tens of nanometers and are believed to be metastable on the millisecond timescale.
One of the great virtues of the mechanism behind STED is that, as indicated by the
formula shown at the beginning of this section, its spatial resolution is a function of
the intensity of the depleting beam. For a low ratio of I/Isat the gain in resolution
is minimal (e.g., a factor of 2). As the intensity of the saturated depletion beam is
increased, the gain in spatial resolution increases rapidly, ultimately leading to an
increase in resolution of a factor of 10 and better. The exact relationship between
gain in resolution and the depletion beam power typically needs to be calibrated
beforehand. Because of its confocal nature, however, STED can still detect fluores-
cence from single fluorescent molecules and the increase in resolution can be directly
determined from determining the full-width-at-half-maximum (FWHM) of the PSF
of single molecules depending on the STED beam intensity. Similarly, by focusing
a STED beam into a solution containing fluorescent molecules, the resulting bursts
of fluorescence whenever a fluorescent molecule diffuses through the beam can be
detected and autocorrelation of these bursts results in an average diffusion time of the
molecules through the confocal volume. As the size of the detection volume shrinks
with increasing intensity of the saturated depletion beam, the measured diffusion
time of the molecules will similarly decrease. When corrected for the change in
detection volume, however, the diffusion time of noninteracting molecules will be
unaltered no matter what size excitation/detection volume was used. Eggeling and
coworkers performed such an experiment on the plasma membrane of living PtK2
cells [25]. They labeled phospholipids, such as phosphoethanolamine (PE), as well
as sphingolipids, such as sphingomyelin, by chemical synthesis with the fluorescent
dye Atto647N, and similarly, the glycosylphosphatidylinositol (GPI)-anchor of GPI-
anchored proteins by acyl carrier protein tagging. They then used STED correlation
spectroscopy to measure the transit times of these different membrane constituents
through the STED volume focused to the cell’s plasma membrane. A seemingly
linear dependence of the decrease in the transit time of PE through the STED volume

PHOTOACTIVATED LOCALIZATION MICROSCOPY
477
was observed, while for STED focal areas below 80 nm in diameter, a plateau in the
transit time of sphingomyelin was observed. This lower limit in the diffusion time
of sphingomyelin through the STED focal area is consistent with transient trapping
of the sphingolipid and provides strong evidence for the existence of transient lipid
structures, such as the elusive lipid rafts.
11.3
PHOTOACTIVATED LOCALIZATION MICROSCOPY AND
STOCHASTIC OPTICAL RECONSTRUCTION MICROSCOPY
The potential of utilizing single molecule fluorescence for achieving optical super-
resolution was initially postulated in a short and somewhat cryptic paper by Betzig,
published in 1995 [26]. It took until the year 2006, before three research groups
independently demonstrated similar optical reconstruction methods with a lateral
sub-diffraction resolution of better than 20 nm. While Eric Betzig and collabora-
tors, now with the Howard Hughes Medical Institute at the Janelia Farm Research
Campus, termed this method “Photoactivated Localization Microscopy” (PALM)
[15], S. T. Hess and colleagues from the University of Maine called it “Fluores-
cence Photoactivation Localization Microscopy” (FPALM) [27]. Using a somewhat
different approach using organic fluorophores, Xiaowei Zhuang and collaborators
from Harvard University developed STORM “Stochastic Optical Reconstruction
Microscopy” [16a].
These photoswitching techniques all have in common that they exploit the temporal
separation of single fluorescent emitters, even if the molecules cannot be isolated in
space due to the diffraction limit: Multiple localizations of single molecules obtained
in a series of images are used to reconstruct a super-resolved image based upon the
positions obtained from the localization algorithm.
All of these methods overcome the problem of so far optically unresolvable struc-
tures consisting of fluorescent molecules by utilizing special properties of photoacti-
vatable proteins or switchable fluorophores that make it possible to tell if the fluores-
cence emission was due to a single molecule or not. FPALM uses fluorescent proteins
such as the photoactivatable green fluorescent protein (PA-GFP), while PALM also
makes use of photoswitchable proteins such as the green fluorescent protein Dronpa
[28], or yellow fluorescent proteins like Kaede, Kikume Green–Red (KikGR), and
Eos Fluorescent Protein (EosFP). In turn, the first fluorophores that were used for
STORM were carbocyanine fluorophores [16a], which exhibit photoswitching prop-
erties in the presence [29] or absence of an activator molecule [30]. The latter method,
where the bare fluorophore in a special buffer system is used, is therefore called
directSTORM (dSTORM). Fluorophores are either switched between a fluorescent
bright (“on”) and a nonfluorescent dark (“off”) state upon illumination with light of
different wavelengths (PALM/STORM) or they are photoactivated and subsequently
photobleached (FPALM).
Due to their technically relatively simple implementation, wide-field single-
molecule based localization approaches such as PALM, STORM, and dSTORM
are currently widely used for super-resolution imaging. Further applications of

478
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
photoswitching microscopy comprise dynamic studies in living cells and quantitative
high-resolution fluorescence imaging, for example, studies that count the number
of biomolecules and their structural organization in small subcellular structures or
membranes [31].
11.3.1
Direct Stochastic Optical Reconstruction Microscopy
dSTORM utilizes the intrinsic photoswitching properties that can be observed from a
large set of commercially available fluorophores. This simplification over the original
implementation of STORM has opened up the potential of localization microscopy
to a much larger set of fluorophores that can be used individually or simultaneously
for multicolor imaging. The photochemical properties of these fluorophores can be
accessed by irradiating the sample with either one [32] or two [33] wavelengths if
the appropriate buffer conditions are found. As mentioned earlier, carbocyanine dyes
were the first fluorophores used for photoswitching microscopy. Cy5 is irradiated
with red laser light at a wavelength of ∼647 nm to excite the fluorophore and to
read out the fluorescence signal for localization. The switching to the nonfluorescent
(“off”) state occurs simultaneously while exciting the fluorophore for localization.
The fluorescent (“on”) state can be reversibly recovered by the use of green laser
light with a wavelength of ∼514 nm. For live cell imaging, an excitation power
ranging from 0.5 to 5 kW cm−2 is used. For fixed cells there is no need to prevent
cell damage and higher excitation intensities of up to 50 kW cm−2 can be applied
[34]. For efficient switching properties of cyanine dyes and in order to ensure their
prolonged use, oxygen in the buffer medium has to be removed with an enzymatic
oxygen-scavenging system [35]. This leads to a dramatically improved reversibility
of the photoswitching process that is needed to achieve sufficiently high localization
statistics. A system consisting of 0.5 mg ml−1 glucose oxidase, 40 μg ml−1 glucose
catalase, and 10% (wt/vol) glucose is recommended. If this is used, additional sealing
of the sample chamber with a cover slip or a thin silicon sheet is needed to prevent
ingress of oxygen. Before imaging monoethanolamine (MEA) is added to achieve
a final thiol concentration of 10–200 mM to prevent photobleaching. Rhodamine
and oxazine fluorophores, however, follow a different photoswitching mechanism
compared to carbocyanine dyes. These fluorophores require only a single wavelength
for fluorescence excitation and switching to the nonfluorescent state. Fluorescence
recovery is caused by oxidation of remaining oxygen in the buffer medium.
For localization microscopy a relatively small subset of all fluorophores attached
to the biological structure to be resolved is activated stochastically at any time,
effectively confining the fluorescence emission of the activated fluorophores. A fast
but low-noise charge-coupled device (CCD) or scientific CMOS (sCMOS) camera
with high quantum efficiency for converting photons into photoelectrons (typically a
deep-cooled, back-thinned CCD camera) capable of exposure times in the millisecond
range is used for image acquisition. Then, high-precision molecular localization with
nanometer accuracy is performed for each fluorescent spot detected in the image [36].
A sequence of several thousands to tens of thousands of fluorescence images (“image
stack”) is recorded using either wide-field or total internal reflection fluorescence

PHOTOACTIVATED LOCALIZATION MICROSCOPY
479
FIGURE 11.3
60 μm × 60 μm image of the cytoskeleton of a COS7 cell labeled with
Cy5. (a) Total internal reflection fluorescence (TIRF) image of the cell. (b) dSTORM image
reconstructed from approximately 1 million localizations. Total image frame acquisition time:
100 s.
(TIRF) microscopy (see Fig. 11.3a). All localized fluorophores are combined into
one super-resolved image that is able to resolve structures down to 20 nm (see
Fig. 11.3b).
11.3.2
Localization of Single Fluorophores
The localization accuracy is dominated by photon statistics, that is, the Poisson
distribution of emitted photons. It depends on the number of photons emitted by a
fluorophore if background noise is negligible compared to the fluorescence signal.
The standard error SE in the fitted fluorophore position, that is, the uncertainty for
the localization, can then be written as
SE = 𝜎∕
√
N,
(11.2)
where 𝜎is the standard deviation of the PSF of the experimental setup that is approxi-
mated by a 2D Gaussian distribution function, and N is the number of detected photons
emitted by the fluorophore [36]. If background noise and the error increase due to a
finite pixel size are taken into account, the standard error has to be expanded to
SE =
√
𝜎2 + a2∕12
N
+ 8𝜋𝜎4b2
a2N2 ,
(11.3)
where a is the pixel size and b the background noise [36].
To obtain a localization accuracy of 10 nm or better, more than 1000 photons
have to be detected from one fluorophore within a data acquisition time in the
millisecond range. For accuracy down to a few nanometers ∼10,000 photons have
to be collected. Due to switching and bleaching of fluorophores usually only several
hundred photons reach the detector such that an accuracy of around 20 nm is expected
for most experiments.

480
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
FIGURE 11.4
Principle of single molecule localization microscopy. (a) Single frame of
a sequence of dSTORM images indicating the presence of single molecules based on the
discrete spots visible in the image. (b) Close-up of one of the fluorescent spots. Each spot is
first recognized, then fit with a 2D Gaussian fit function to (c) result in an image of the fitted
spots. From here, the (d) center of mass of each of the spots is determined.
Besides finding the appropriate chemical environment optimal for photoswitching
depending on the fluorophore and the sample, data processing and analysis is a
challenging task. The experimentally obtained data, that is, the acquired image stack,
has to be processed rapidly but also accurately. The major drawback of the localization
method is that the computational effort of localizing the fluorophores with nanometer
accuracy is tremendous. Long data processing times can, hence, limit fast and real-
time biological applications. However, data processing can be separated into three
parts (see Fig. 11.4): (1) Find the locations of spot candidates indicating fluorophores
(e.g., using a simple thresholding condition); (2) fit a model of the PSF to each of
these spot candidates; and (3) find the true location of the fluorophores by examining
the results of the fit parameters. A fast and robust image-processing algorithm for
the localization of fluorophores with nanometer accuracy was found using only a
few assumptions and simplifications [37]. This scheme mainly consists of a fast and
accurate PSF reversal using fixed-width, fixed-orientation Gaussian functions for
images with low signal-to-noise ratio capable of real-time image reconstruction on a
present-day personal computer.
The entire image stack is analyzed image-by-image by processing each acquired
raw image (Fig. 11.4a) in mainly five steps: (1) noise reduction, (2) finding likely
candidates (3) fitting a 2D Gaussian function to pixels close to the candidate position
(Fig. 11.4c): (4) judging the fit; and, finally, (5) localization and visualization of the
fluorophore position.

PHOTOACTIVATED LOCALIZATION MICROSCOPY
481
Noise reduction is an important task because raw data images contain a very weak
fluorescence signal emitted from a single fluorophore only. Detection of up to 1000
photons per exposure is required and a nevertheless significant background and noise
level on the camera chip lead to a marginal signal-to-noise ratio. Noise reduction
algorithms and subsequent finding of likely spot candidates have to be reliable:
finding many spot candidates caused by fluorescence emission (true positives) while
yielding the least number of false-spot candidates that are caused by noise (false
positives). The false positive rate must be kept as low as possible because it can lead
to false localizations, and therefore waste a lot of computation time. Furthermore,
false positives lead to artifacts in the reconstructed image that can, in the worst
case, exceed the number of true localizations. Noise reduction can be performed
with different algorithms, for example, averaging, median filtering, or by a Gaussian
kernel mask. While different morphological transformations might also be possible,
an averaging algorithm performs quite similarly but much faster (∼0.05 ms per image)
than all the others.
To find spot candidates, a non-maximum suppression (NMS) algorithm is used.
The NMS extracts the positions of pixels that are local maximums by setting all pixels
in the defined neighborhood that are lower than the maximum value in that region
to black. The located maximum positions are then considered spot candidates and
sorted in descending order by their intensity. A 2D Gaussian function G is fitted to the
raw image data at the assumed maximum positions starting with the spot candidate
with the highest intensity:
G(⃗x) =
A
2𝜋
√
|V|
exp
(
−1
2(⃗x −⃗x0)TV(⃗x −⃗x0)
)
+ B,
(11.4)
where A is the amplitude of the emitted signal, B the local background signal, ⃗x0
(superscript T indicates the transpose of the vector) the fluorophore position with
“subpixel”-accuracy and V the fixed covariance matrix given by
V =
(
𝜎2
x
𝜌𝜎x𝜎y
𝜌𝜎x𝜎y
𝜎2
y
)
,
(11.5)
where 𝜎x and 𝜎y are the standard deviations in x and y directions, respectively. 𝜌
indicates the correlation between the x- and y-axes. The user must supply an initial
rough estimate for this covariance matrix followed by internal iterative improvement
of these parameters while processing the raw data images for the purpose of speed
and accuracy. Fitting is performed using the Levenberg–Marquardt least-squares
algorithm. A spot candidate is considered a fluorescence signal if defined localiza-
tion criteria, such as a reasonable position and amplitude threshold, are met. After
processing the image stack, a super-resolution image is then reconstructed based on
the positive localizations. With an optimized fit process, separation of the average
mask and utilization of advanced processor instruction sets, real-time computation of
sub-diffraction images is possible even for the shortest possible dSTORM acquisition
rates taken with a frame rate of ∼1 kHz.

482
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
PALM/STORM measurements are prone to high background arising from auto-
fluorescence of the sample and residual fluorophores in the supernatant buffer. There-
fore, and in order to collect many photons for precise localization, dyes with high
quantum yield are preferred. Additionally, the fluorophores have to exhibit short
“on”-times compared to “off”-times such that only a small fraction of fluorophores
is fluorescent while the others remain in the dark state. Otherwise it is possible that
two adjacent fluorophores are both in the “on”-state at the same time, leading to
false localization. On the other hand, according to the Nyquist–Shannon sampling
theorem the density of the fluorophores has to be twice as high as the desired (super)
resolution. Taking the size of a fluorophore-labeled antibody into account, a biolog-
ical structure has to be very densely labeled with antibodies. As shown above, in
dSTORM a Gaussian function is fit to a separated fluorescence signal. With increas-
ing labeling density the probability for non-separated signals rises. In order to handle
these images fitting of multiple connected PSFs to a fluorescence signal emitted from
connected fluorophores [38] is required.
11.3.3
Beyond dSTORM
A natural, immediate extension of STORM is super-resolution imaging in three
dimensions, termed 3D-STORM. In order to determine the axial position and, hence,
to obtain the complete 3D information of a fluorophore with sub-diffraction reso-
lution, optical astigmatism was used in a first implementation. In this approach, a
cylindrical lens is incorporated into the optical detection path that yields two different
focal planes in the x and y directions. This leads to an ellipsoidal deformation of the
PSF for fluorophores that are above or below the focal plane, and to a symmetri-
cally round PSF for fluorophores that are in the average focal plane. This results
in an additional axial resolution of ∼50 nm, even in micrometer thick samples,
while maintaining a lateral resolution of 20–30 nm [39]. Other recent approaches
to extend STORM to the third dimension include the use of temporal focusing
in wide-field two-photon excitation [40], or the combination with selective plane
excitation [41].
Besides photoswitching between “on”- and “off”-states many similar methods
have emerged that take advantage of related phenomena. This includes reversible pho-
tobleaching microscopy (RPM) and ground-state depletion with individual molecule
return (GSDIM) that use standard dyes such as fluorescein, Alexa Fluor, ATTO dyes,
and carbocyanine dyes either under simple buffer conditions or in an oxygen scaveng-
ing system with additional thiols. RPM utilizes reversibly induced long-lived (>10 s)
dark states of the fluorophore. After high-power illumination of the sample, single
fluorophores stochastically return to the ground state. Reversible photobleaching was
reported for different dyes, in dry samples in air, and this approach does not require
any special (oxygen scavenging) buffer [42]. The GSDIM technique, on the other
hand, lowers the number of fluorescent molecules by populating a metastable dark
(triplet) state. The images acquired during such an experiment collect fluorescent
photons only from individual fluorophores that stochastically return to the ground
(singlet) state [43].

STRUCTURED ILLUMINATION MICROSCOPY
483
11.3.4
Applications of Localization Microscopy
Recent applications of dSTORM and PALM have shown that super-resolution meth-
ods are powerful tools for studying biological samples since the size of many bio-
logical structures, which could not be optically resolved in the past, is in the range
between 10 and 100 nm. It is expected that imaging of single molecules in living
cells has the potential to reveal new concepts in cell biology. For instance, PALM
was used to study the organization of single proteins in bacteria [44] and near the
surface in eukaryotic cells [45] to determine how proteins assemble and organize
themselves into clusters that are responsible, for example, for chemotaxis in bacte-
rial cells. Recently, using dSTORM the structure of nuclear pore complexes in the
nuclear envelopes of isolated Xenopus laevis oocytes was investigated. The lateral
resolution obtained with two-color dSTORM was ∼15 nm. To further improve the
spatial resolution, super-resolved images accumulated from hundreds of nuclear pore
complexes were taken into consideration to determine the diameter of the central
channel to be 41 ± 7 nm. This level of detail was sufficient to determine that the
nuclear pore glycoprotein-210 (gp210) that anchors the pore complex to the nuclear
membrane is distributed in an eightfold radial symmetry [46].
Optical super-resolution methods permit not only the reconstruction of highly
spatially resolved images, but also the quantitative counting of individual molecules
in cells. For example, another recent application reported that this method can be
used to count the centromere-specific histone H3 variant in the centromere protein
A (CENP-A) with single-molecule sensitivity in fission yeast (Schizosaccharomyces
pombe). Using this approach, it was found that CENP-A is deposited solely during
the G2 phase of the cell cycle [47].
In summary, localization-based microscopy approaches result in highly resolved
images and also provide quantitative information about biological samples. The
implementation of dSTORM circumvents the use of only a select number of flu-
orophores and extends this method to a much wider range of fluorophores. The
application to living biological samples is, however, somewhat restricted to problems
exhibiting fairly slow dynamics, due to the large number of images that need to be
acquired in order to reconstruct highly resolved fluorescence images.
11.4
STRUCTURED ILLUMINATION MICROSCOPY
Structured illumination microscopy (SIM) as a way of improving the spatial resolution
of optical microscopy was first demonstrated in slightly different implementations
by Heintzmann and Cremer in 1999 [48], as well as Frohn and coworkers [49], and
separately by Gustafsson and coworkers [50], in 2000. SIM is in essence an imple-
mentation of wide-field fluorescence microscopy, but using an illumination pattern
rather than homogeneous excitation. The illumination pattern can be produced by a
number of different ways. Early papers used interference patterns in two dimensions
(e.g., created by the counter-propagation of two evanescent waves at the surface of
a glass prism [49], or by using interference between different diffraction orders of

484
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
a diffraction grating [48–50]); more recently, electro-optical devices (e.g., a spatial
light modulator (SLM) [51]) are employed. Fluorescence images of the sample are
then typically acquired with a highly sensitive CCD camera. In essence, this type
of microscopy is an extension of patterned optical microscopy, which was initially
invented as a means of optical sectioning without the need for confocal optics [52].
SIM is based on essentially the same idea as frequency mixing in radio wave
transmission. A carrier frequency is mixed with a sample frequency in order to
transmit the signal across the passband of the optical microscope. In first order, this
simple trick enables optical sectioning without the need for a confocal pinhole to help
reject out-of-focus light. By projecting an illumination pattern into the sample, in the
focal plane the modulation produced by the pattern provides maximum contrast.
Imagine, for example, a sinusoidal illumination pattern that is leading to “striped”
excitation of the sample (this description is based on exciting fluorescence contrast
but could certainly be applied to other contrast mechanisms). If this pattern is shifted
across the sample, its projection will produce a strong contrast in the focal plane
according to the projected excitation intensity. When an image of this pattern is
acquired on a CCD camera held in a conjugate plane to the focal plane, the response
of the sample will be imaged with maximum intensity obtained for regions within
the vertical focal plane of the microscope plane that are exposed to the peaks of
the illumination pattern. Regions exposed to the valleys of the pattern, on the other
hand, will exhibit little to no contribution to the image and appear dark. At the
same time, since we are transmitting light vertically through the sample, parts of the
sample that are out-of-focus will also be excited, but with less pronounced contrast.
If the illumination pattern is now shifted across the sample in the lateral direction,
other parts of the sample in the focal plane will now be illuminated and lead to
high intensity at the CCD camera, while the background will still only contribute
with a much weaker contrast. A simple algorithm that calculates the difference
between the highest intensity and the lowest intensity for every pixel in a series of
images where the illumination pattern was shifted across the sample will then result
in an image consisting primarily of contributions from the focal plane. Out-of-focus
contributions will have very similar intensities for all illumination conditions and will
thus be removed by this simple “max–min” difference approach. More sophisticated
algorithms will furthermore lead to denoising of the images and improve contrast
even more. Such approaches to SIM are available in commercial implementations,
for example, in the form of the Zeiss Apotome, a low-cost addition to an optical
microscope providing confocal-like axial sectioning [52,53].
Now imagine the frequency of the illumination pattern being increased to the
point where it approaches the maximum spatial frequency that can still be transmitted
through the microscope lens. If this pattern of known frequency is used to illuminate
another (not necessarily regular) pattern of unknown frequency it will lead to the
well-known Moire effect in two dimensions, that is, spatial frequency mixing leading
to additional contrast in an image. This is demonstrated in Figure 11.1d.
Let us first investigate this concept in the easily understandable case of pure
frequency mixing. The addition of two cosine functions with different frequencies

STRUCTURED ILLUMINATION MICROSCOPY
485
𝜔1 and 𝜔2, leads to a beat mode carrying with two new frequencies, one at the sum
of the two original frequencies, the other at the difference frequency:
cos(𝜔1t) + cos(𝜔2t) = 2 cos
(𝜔1t + 𝜔2t
2
)
cos
(𝜔1t −𝜔2t
2
)
.
(11.6)
The same holds true for spatial frequencies k1 and k2:
cos(k1x) + cos(k2x) = 2 cos
(k1x + k2x
2
)
cos
(k1x −k2x
2
)
.
(11.7)
This is the well-known mechanism of heterodyning. If one of the two frequencies is
known, then the other one can be determined by measuring, for example, the reduced
frequency of the envelope of the beat pattern. For a limited passband this does,
however, only work up to twice the frequency of the passband. For any frequencies
higher than that, even the difference frequency will be lying beyond the passband.
Translated to the 2D space of images, this is best understood by studying images in
Fourier space. An arbitrary image when transformed to Fourier space results in a 2D
Fourier image showing spatial frequencies within a circular pattern. Low frequencies
appear near the center of the Fourier image, while high spatial frequencies appear near
the borders of the Fourier image. If an image is filtered, for example, by the optical
transfer function (OTF) of a microscope lens such that its smallest features still occupy
more than just a single pixel on a CCD camera, then the Fourier-transform of the image
results in a circle, where the highest spatial frequencies occupy the circumference
of the circle and determine its radius, while low spatial frequencies appear near the
center of the circle. A sinusoidal pattern with a low spatial frequency results in two
delta peaks, while the axis through these two delta peaks indicates the direction
along which the periodically varying pattern is oriented. If the periodicity of this
pattern is increased, the two delta peaks will begin to move toward the circumference
of the circle of “allowed” frequencies being transmitted through the microscope
optics. Now, imagine such a pattern with a frequency close to the maximum allowed
frequency mixing with an unknown pattern of interest containing spatial frequencies
beyond the passband of the OTF of the microscope. In this case, a Moire pattern
similar to our example in the 1D case will form that downshifts any frequencies
less than twice the maximum allowed frequency into the range of allowed spatial
frequencies. Based on the knowledge of the modulation frequency, the unknown
frequency can then be calculated. In the following, we will briefly cover the theory
for 2D/3D SIM. Generally speaking, the image D observed of a fluorescent sample in
a fluorescence microscope is the convolution of the fluorescence emission E and the
PSF H: D(⃗r) = (E ⊗H)(⃗r). In the case of a structured illumination microscope, the
fluorescence emission is also the product of a spatially varying illumination pattern
I and the spatial distribution of the fluorescent dyes S: E(⃗r) = S(⃗r)I(⃗r). Now, let us

486
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
assume the illumination pattern is a sinusoidal stripe pattern along a specific angle,
described by a wave vector kill, and a specific phase 𝜑:
I(⃗r) = I0
2
[
1 −cos
(
⃗kill ⋅⃗r + 𝜑
)]
.
(11.8)
The resulting image is then described by: D(⃗r) = (S(⃗r) ⋅I(⃗r)) ⊗H(⃗r). If we now
move our discussion to the reciprocal Fourier space, this expression is simplified
because the deconvolution between the PSF and the fluorescence emission turns into
a simple product:
̃D(⃗k) = ̃H(⃗k)[̃I(⃗k) ⊗̃S(⃗k)].
(11.9)
The Fourier transform of the illumination pattern turns into a series of Delta
functions, which is the virtue of using a sinusoidally varying stripe pattern:
̃I(⃗k) = I0
2
[
𝛿(⃗k) −1
2𝛿(⃗k −⃗kill)e−i𝜑−1
2𝛿(⃗k + ⃗kill)ei𝜑]
.
(11.10)
This simplifies the calculation of the convolution of the pattern with the fluorophore
distribution, and thus the observed image in reciprocal space:
̃D(⃗k) = I0
2
̃H(⃗k)
[
̃S(⃗k) −1
2
̃S(⃗k −⃗kill)e−i𝜑−1
2
̃S(⃗k + ⃗kill)ei𝜑]
.
(11.11)
Thus, if the illumination pattern and the PSF of the microscope are known, then
from the experimentally observed image D, the original (super-resolved) fluorophore
distribution S can be determined. This process is, however, nontrivial and a number
of schemes for determining the original fluorophore distribution in the case of linear
structured illumination have been derived. Here, we briefly reiterate the basic scheme
introduced in the seminal paper by Gustafsson et al. [14]:
First, the experimental PSF needs to be determined. This is typically done using a
sub-diffraction limited structure, ideally a single fluorescent molecule or fluorescent
quantum dot. In practice a fluorescent bead with ≤100 nm diameter suffices and sis
ufficiently stable and long-lasting to obtain the PSF in a series of images taken at
different z-steps. From this experimental PSF the background (e.g., due to camera
offsets, etc.) is subtracted and the PSF is then rotationally averaged. The OTF of the
microscope is then determined from this PSF by calculating the 2D Fourier transform
of the PSF.
Next, for a given angle, an image at each of N different phase positions is taken
by translating the interference pattern across the sample. The phase shifts should be
evenly spaced (Δ𝜗= 2𝜋/N). N is typically 3 if 2D images are to be reconstructed and
5 for reconstructing 3D images.
This procedure is repeated and similar images are acquired for each of M different
angles by rotating the interference pattern. The number of angles is M ≤N, typically

STRUCTURED ILLUMINATION MICROSCOPY
487
3, to evenly cover the focal plane by the phase-shifted images. The angles are also
evenly spaced across the image space (Δ𝜃= 2𝜋/M).
The M × N images that have been acquired in this fashion are subsequently
background subtracted and intensity corrected to compensate for source fluctuations
and photobleaching. The M × N images then undergo a 2D Fourier transform to
reciprocal space.
For each angle, the N individual information components are separated by pairwise
cross-correlation to find an initial estimate of the frequency shift vector, phase shift,
and modulation depth. These initial estimates are improved using linear regression.
For each angle the information components are then combined using Wiener filtering
and apodization to even out overlapping components and to ensure an evenly extended
image in Fourier space. The image is then transformed back to real space to result in
an image with approximately 2× improved spatial resolution. The steps from image
acquisition, excitation with a structured illumination pattern, and the reconstruction
of a super-resolved image is demonstrated in Figure 11.5 based on a sample consisting
of 100 nm fluorescent beads.
FIGURE 11.5
The principle behind structured illumination microscopy. (a) A sample of
100 nm diameter fluorescent beads image by wide-field fluorescence microscopy, and (b) its
corresponding 2D image in Fourier space. (c) A sequence of images obtained by exciting the
sample with an interference pattern that is phase-shifted and rotated is required to reconstruct
a high resolution image of the sample. Here, five phases along one angle of the illumination
pattern are shown. (d) Recombined components of the sample after imaging by structured
illumination and subsequent 2D Fourier transformation. (e) Reconstructed image at double the
resolution obtained by 2D Fourier-transformation of the image shown in (d).

488
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
For reconstructing 3D images, this procedure needs to also be extended to the third
dimension. In this case, a 3D interference pattern is required, resulting in approxi-
mately a factor of 2 improvement in resolution in all dimensions [14]. Perfect recon-
struction is usually complicated by the somewhat unknown (or not very precisely
known) phase of the interference pattern, the need for perfect modulation, noise,
sample movement, etc. [54]. Alternative ways of reconstructing structured illumina-
tion images even in the absence of detailed information about the illumination pattern
are currently also under active investigation and include Bayesian reconstruction [55]
and speckle patterns [56], for example.
The illumination pattern required for this type of imaging is typically created by
sending laser light through a transmission diffraction grating, then collecting the +1
and −1 diffraction orders and collimating them. To create a 2D pattern, these beams
are then focused to opposing ends at the back focal plane of an oil-immersion objective
lens, where they undergo total internal reflection at the glass-sample interface. The
resulting evanescent waves are counter-propagating and lead to a standing evanescent
wave just above the cover slip surface, which can excite fluorescence within the first
100–200 nm of the sample near the glass surface. If a 3D interference pattern is
desired, then the +1 and −1 diffraction orders are focused to opposing spots in
the back focal plane that just avoid the criterion for total internal reflection. In this
case, the 0 diffraction order is also sent into the microscope objective, leading to a
more complex, 3D diffraction pattern. If a diffraction grating is used to create the
interference pattern, the grating needs to physically be rotated and translated (typically
using piezoelectric transducers) in order to create the different phase shifts and angles
at which the sample is illuminated. Alternatively, other, faster optical elements, such
as a SLM, or galvanometric mirrors can be used to create the interference pattern,
which enables rapid phase and angle changes. To obtain fluorescence images of
the sample, highly sensitive CCD cameras, typically of the electron-multiplying,
back-illuminated type are used, or, to match the faster pattern creating devices, more
recently, scientific CMOS cameras are used. This enables even live cell SIM in two
[51, 57] and three [58] dimensions. To illustrate the enhanced resolution obtained by
imaging a biological sample, Fig. 11.6 shows a two-color 3D SIM image of human T
cells (stained with a red fluorescent antibody against actin) and a fluorescent clone of
the human immunodeficiency virus HIV-1. Figure 11.6a shows the best image of this
sample obtained by fluorescence deconvolution microscopy, while Fig. 11.6b shows
the 3D SIM image of the same sample. Note that due to the 2× improvement in spatial
resolution along all three axes, the volume resolution is improved by a factor of 8,
which is best reflected by the maximum intensity projection images shown here.
The advantage of SIM, compared to techniques, such as confocal microscopy,
or even single molecule localization microscopy and STED approaches is that it
collects entire images of the sample by a CCD camera held in the conjugate plane of
the microscope. Thus, it utilizes the full detection efficiency of the optical microscopy
setup and no photons are lost at a confocal pinhole. In order to truly obtain a super-
resolved image, however, computational processing is required to accomplish the
resolution improvement and optical sectioning. At this point, photons from out-of-
plane regions will be rejected by software. Confocal microscopy can, however, also be

STRUCTURED ILLUMINATION MICROSCOPY
489
FIGURE 11.6
(a) Deconvolution fluorescence microscopy image, and (b) 3D structured
illumination microscopy image of human T cells infected with a fluorescent clone of HIV-1.
The cytoskeleton is labeled with a fluorescent antibody against actin [34], while the clone of
HIV-1 carries the green fluorescent protein fused to gag, a structural protein of HIV. (For a
color version of this figure, see the color plate section.)
considered a form of structured illumination as was initially postulated by Sheppard
[59], and recently demonstrated by the group of Enderlein [60]. In order to take
advantage of this approach, however, an image of the confocal spot for every image
pixel of the final super-resolved image needs to be taken while the confocal spot
is swept across the sample. In order to overcome the time lag introduced by this
serial imaging, alternatively a large number of confocal spots can also be imaged
simultaneously onto the CCD camera, for example, the spots created by a spinning
disk confocal microscope, or the pattern created by a multifocal optical microscope
[61]. This method also results in a 2× improvement of the lateral resolution; however,
implementing this approach to also improve the axial resolution by a factor of 2 is
more cumbersome and has not yet been accomplished.
SIM is only limited to a 2× improvement in optical resolution in the linear case,
where, for example a sinusoidal intensity pattern is used as the illumination pattern. If
a nonlinearity can be introduced into the system, then the resolution improvement is,
in theory, unlimited. This has been postulated early on by Heintzmann [62], but only
been demonstrated in 2005 by Mats Gustafsson [63], who used saturated structured
illumination to achieve the required nonlinearity. In saturated SIM, the intensity
modulation of the interference pattern is increased to the point where the fluorescence
excitation of the sample is saturated, that is, illumination with even higher intensities
does not produce stronger fluorescence. This is due to the relatively long excited state
lifetimes of most organic fluorophores on the order of a few nanoseconds, which
limit the maximum number of photons that can be emitted by a fluorescent molecule.
Thus, the corresponding fluorescence image excited by a saturating illumination
pattern leads to a “flattening” of the image in areas that are exposed with an intensity
above the saturation threshold. In essence, the sinusoidal illumination pattern turns
the resulting fluorescence images into a sinusoidal pattern with flat tops. This strong
deviation from the sinusoidal pattern, along with a much greater number of phase
shifts and an increased number of illumination angles, leads to the reconstruction
of fluorescence images with a demonstrated 50 nm optical spatial resolution [63].
The disadvantage of this type of nonlinear structured illumination is, however, that
fluorescent samples rapidly bleach out and lose their ability to fluoresce due to the

490
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
strong excitation. Alternatively, other forms of optical nonlinearities can be utilized,
for example, the combination of structured illumination with photoswitching similar
to PALM microscopy [64]. These alternative modes of further improving the spatial
resolution of SIM are currently under active and rapid development.
SIM is, however, not limited to wide-field illumination. Other forms of optical
microscopy can also benefit from the SIM mechanism to further improve their spatial
resolution. Recently, this has been demonstrated by the group of Betzig, who utilized
SIM in combination with selective plane microscopy in an effort to combine the
best of these two approaches [65]. In this case, a Bessel beam, sent through the
sample from the side is used in combination with two-photon excitation to excite
fluorescence, which is collected by another microscope lens with high numerical
aperture at an angle of 90◦to the Bessel beam propagation direction and imaged
onto a CCD camera. The Bessel beam is swept across a 2D field of view using a
galvanometric mirror. In this case, if the beam is also rapidly switched on and off such
that only every other beam position leads to fluorescence excitation then the resulting
image will also contain a striped pattern. Similar to the wide-field case, this pattern
can also be shifted across the sample, and a super-resolved image reconstructed. This
method also works in the simpler case of one-photon excitation as shown by Keller
and coworkers [66].
Current efforts in further improving SIM are directed at further increasing the
speed with which patterns can be projected into samples and images can be acquired
by cameras, as well as by extending the number of color channels in which images
of living cells can be acquired [67]. Also, current difficulties limiting SIM, for
example, imaging deep in specimen, which is currently not possible beyond a depth
of ∼30 μm due to optical aberrations that result when interference patterns are
projected deep into a specimen, in addition to light scattering, are being addressed
[68]. Variations of the two-photon Bessel beam excitation and similar multiphoton
excitation schemes look quite promising in this regard. Also, extensions of SIM
to other contrast mechanisms, such as nonlinear optical microscopies beyond the
two-photon Bessel beam illumination scheme, should soon emerge.
11.4.1
Applications of SIM
3D SIM was recently used to study the structure of chromatin and major nuclear
proteins in the mammalian nucleus. By imaging chromatin, nuclear lamina, and the
nuclear pore complex, Schermelleh and coworkers were able to resolve single nuclear
pore complexes that co-localized with channels in the lamin network and peripheral
chromatin. This enabled them to localize distinct components of the nuclear pore
complex and to detect invaginations of the nuclear envelope in prophase [69].
In a different application, Cogger and coworkers used SIM to image fenestrations
in liver sinusoidal endothelial cells [70]. Fenestrations are invaginations in the plasma
membrane that span across the entire cytoplasm and permit, for example, lipopro-
teins, small molecules, and possibly, even viruses to breach the blood–liver barrier
and reach the underlying hepatocytes in the liver. These structures were previously
only seen by electron microscopy and it was debated whether or not these might

SUPER-RESOLUTIONOPTICALFLUCTUATIONIMAGINGANDOTHER APPROACHES
491
be potential artifacts due to sample preparation for electron microscopy. Using a
plasma membrane dye, Cogger and coworkers were able to not only image fenestra-
tions, but also co-localize them with other proteins believed to help stabilize these
structures. This work bears significant potential for future live cell studies of simi-
lar structures and reveal so far unknown dynamic information about the function of
fenestrae [70].
11.5
SUPER-RESOLUTION OPTICAL FLUCTUATION IMAGING
AND OTHER APPROACHES
After the first wave in the rapidly evolving area of super-resolution microscopy
focusing on STED, localization microscopy, and SIM, more recent developments
tend to focus on experimentally simple or at least simpler, as well as faster but yet
more reliable and stable methods. High laser intensities as well as non-physiological
buffer conditions such as the ones used in STORM are expected to damage the
sample: STED, for example, requires an advanced microscope configuration along
with a complex alignment procedure. It is demanding in terms of choosing the right
dyes and labeling biological samples. STED is limited to ∼60 nm resolution in video
rate live cells imaging applications (28 frames/s) [22]. On the other hand, SIM yields
a resolution limit of ∼100 nm and is, technically speaking, simpler compared to
STED. Live cell imaging was demonstrated with frame rates up to 11 Hz only due to
the need for several images to be acquired in order to obtain one super-resolved image
[71]. STORM requires that most of the fluorophores are switched to the dark “off”-
state with non-overlapping fluorescence emission of the remaining fluorophores. This
limits the number of localizations per image and, therefore, requires many images
and long data acquisition times especially in order to resolve dense and complex
structures.
Super-resolution optical fluctuation imaging (SOFI) utilizes the temporal fluctu-
ations of the fluorescence signal caused by, for example, the inherent blinking of
many fluorophores to compute almost background free and, additionally, contrast-
enhanced super-resolution images. If the fluorescence fluctuation is analyzed with
SOFI, it purely relies on stochastic fluctuations and does not necessarily require
controlled reversible photoswitching. It solely requires that the fluorophore is repeat-
edly (stochastically) switched between two different emission states, a fluorescent
“on”- and a nonfluorescent “off”-state. First results were achieved with quantum
dots, showing a fivefold enhancement in resolution [72], later also followed by a
comparable resolution enhancement with organic dyes [73].
Basically, SOFI filters the fluorescence signal in an image such that only correlated
fluctuations, emitted from the same fluorophore, are taken into account. Uncorrelated
fluorescence from neighboring fluorophores is nonlinearly suppressed, which leads
to a sharpened PSF and, hence, an increased optical resolution. Due to pixel-wise
correlation analysis SOFI is somewhat limited to samples that undergo no additional
temporal changes during the few images (as few as 100) that are needed to reconstruct
a SOFI image.

492
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
The fluorescence signal F(⃗r, t) on a detector pixel ⃗r of a sample consisting of N
fluctuating fluorophores can be written as
F(⃗r, t) =
N
∑
k=1
U(⃗r −⃗rk) ⋅𝜀k ⋅sk(t),
(11.12)
where U(⃗r) is the PSF, 𝜀k the mean fluorescence and sk(t) the temporal fluctuation
of the fluorophore. The zero-mean fluctuations can be written as a difference to the
time average given by F(⃗r, t):
𝛿F(⃗r, t) = F(⃗r, t) −⟨F(⃗r, t)⟩=
N
∑
k=1
U(⃗r −⃗rk) ⋅𝜀k ⋅𝛿sk(t)
(11.13)
from which the second-order auto-correlation function can be calculated:
G2(⃗r, 𝜏) = 𝛿F(⃗r, t + 𝜏) ⋅𝛿F(⃗r, t) =
N
∑
k=1
U2(⃗r −⃗rk) ⋅𝜀2
k ⋅𝛿sk(t + 𝜏) ⋅𝛿sk(t)
(11.14)
The pixel value of a SOFI image is obtained from the G2-function for a defined
lag time 𝜏. The auto-correlation function does not directly report on the fluorescence
signal of the fluorophore but on the correlation and the brightness of that signal.
The auto-correlation function Gn can be expanded to higher orders by multiplica-
tion with additional 𝛿F(⃗r, t + 𝜏n). As can be seen in the last equation for G2, Gn then
depends on the nth power of the PSF U(⃗r). If, as a good approximation, a Gaussian
distribution of the PSF is assumed, the standard deviation of the PSF is reduced
by a factor of
√
n. This yields a resolution enhancement by a factor of
√
n for the
calculated order of correlation n.
Indeed, higher orders than the G2-function have to be considered to achieve higher
spatial resolution. To ensure that only photons from the same fluorophore are counted,
the cumulant functions (a quantity related to the correlation function) are used. Hence,
only highly correlated fluctuations are considered such that the PSF is again reduced
by a factor of
√
n for the nth order of the cumulant function. The sequence of events
leading to SOFI images is demonstrated in Figure 11.7. A wide-field fluorescence
image of a rat hippocampal neuron labeled with fluorescent quantum dots, and the
resulting two-color SOFI image are shown in Figure 11.8.
In contrast to localization microscopy methods like PALM and dSTORM, even
weakly fluorescing dyes with a wide range of blinking characteristics or more densely
labeled samples can be analyzed by SOFI. The major drawback of SOFI is its
nonlinear response to brightness and blinking heterogeneities in the sample, which
limits the analysis of higher orders of the cumulant function. However, balanced
super-resolution optical fluctuation imaging (bSOFI), a variant of SOFI, makes
use of estimated blinking statistics in order to balance the image contrast. This

SUPER-RESOLUTIONOPTICALFLUCTUATIONIMAGINGANDOTHER APPROACHES
493
Image stack
(b)
(a)
(c)
(d)
CCD pixel grid
Cut-out
Fluorophore positions
c
y
x
Integration
Auto-
Correlation
Convolved emitter signals
Final image
Time lag
New
values
i–1
i
i+1
i–1 i i+1
2
3
n
Time
Image 1
FIGURE 11.7
Mechanism of SOFI auto-cumulant calculation: (a) CCD camera pixel grid
used to acquire fluorescence images with temporally fluctuating emitters. (b) Expanded view
of the two emitters highlighted by the dashed line in (a) and convolved with the system’s
point spread function recorded on an image stack with hundreds to thousands of images. (c)
Analysis of the images by pixel-wise correlation and auto-cumulant calculation. (d) Final
image reconstructed based on the calculated cumulant values. (For a color version of this
figure, see the color plate section.)
FIGURE 11.8
(a) Wide-field and (b) second order SOFI images of a rat hippocampal neuron,
calculated from 3000 frames, with a frame acquisition time of 50 ms. GABA-B R1 was labeled
with QDot525, represented in green; GABA-B R2 was labeled with QDot625, represented in
red. Figure courtesy of Anja Huss and J¨org Enderlein, Department of Physics, Georg-August-
University G¨ottingen, Germany. (For a color version of this figure, see the color plate section.)

494
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
leads to a more linear brightness and blinking response of the cumulant function
Gn that is needed to obtain a resolution improvement linear with higher cumulant
orders [74].
11.5.1
Bayesian Analysis of Blinking and Bleaching
Another approach for the analysis of wide-field image consisting of fluorophores
and their time-varying properties is the Bayesian analysis of blinking and bleaching
(3B) [75]. The 3B method allows for the analysis of an image stack consisting of
images containing several overlapping fluorophores, for example, densely labeled
biological structures. Each of these fluorophores can emit fluorescent photons but
does not necessarily fluoresce all the time, recorded in all of the acquired images.
Due to the applied model all photons emitted from one fluorophore, even those that do
not necessarily have to be acquired in subsequent images, are collected and analyzed,
which yields an improved determination of the fluorophore position.
In the 3B analysis, the fluorescence emission of each fluorophore is modeled with
a Factorial Hidden Markov Model that is utilized for super-resolved image analysis.
The Hidden Markov Model contains observed and unobserved (“hidden”) states. In
total, three different states can be assigned to a fluorophore: (1) the “on”-state; (2) the
“off”-state; and (3) the “bleached”-state. The Factorial Hidden Markov Model that
is used in 3B analysis allows for an observed state that is conditioned on the hidden
states. Accordingly, it is assumed that each fluorophore can stay or transfer between
the “on”- and the “off”-state with certain probabilities, but remains in the “bleached”-
state that can be reached from the “off”-state only. This model is used to calculate the
probability that a fluorophore is present at a certain position compared to the (null)
hypothesis that the signal arises from noise: the integration over the fluorophore states
can be performed with a forward algorithm. The integral over the four continuous
parameters (the x- and y-position, the radius, and the brightness of the fluorophore) is
calculated using Laplace’s approximation. An exact integration over state sequences
that becomes demanding with increasing number of fluorophores is replaced
with a Markov Chain Monte Carlo sampling to speed up the analysis for larger
images. This rather complex probability calculation generates a density map of the
fluorophore’s position.
The 3B analysis allows performing sub-diffraction optical (localization)
microscopy with a spatial resolution of ∼50 nm and a temporal resolution of 4 s.
It can be performed on a standard wide-field microscope (even using incoherent
sources, such as a Xenon arc lamp or LEDs) in order to obtain images of living
cells that are densely labeled with a standard fluorescent protein. Nevertheless, the
3B analysis takes up to several hours, which is much longer compared to dSTORM
and SOFI. It is reported that an analysis of an area of 1.5 × 1.5 μm2 takes 6 hours
processing time on a standard personal computer [75].
11.5.2
Applications of Fluctuation-Based Superresolution Microscopy
To demonstrate the improved imaging achieved by balanced SOFI, the microtubuli
network in fixed HeLa cells was recently analyzed with bSOFI. The microtubules

OUTLOOK
495
were fluorescently labeled with Alexa647 and imaged on a TIRF microscopy setup.
A 4.6-fold resolution improvement compared to wide-field microscopy was reported
using the bSOFI algorithm for analysis. The SOFI algorithm yields an improvement
on the order of
√
5 for the fifth order cumulant analysis. In combination with TIRF
microscopy, both, the molecular brightness and the blinking on-time ratio of the
fluorophore could be interpreted as a z-position encoding because they are related to
the illumination intensity [34].
The SOFI algorithm described above can easily be extended to two-color super-
resolution optical fluctuations imaging (2cSOFI). This method was introduced using
a standard wide-field experimental setup and quantum dots (QDs) that emit and blink
at different colors. The labeling was performed with primary antibodies directly
conjugated to QDs in order to obtain a higher labeling density, which is important
for super-resolution imaging. The spatial relationship between the processing body
hDcp1a protein and the tubulin cytoskeleton in fixed HeLa cells was visualized.
New perspectives on the role of the cytoskeleton in processing body formation and
assembly along with further insights into the processing bodies’ internal organization
were reported [76].
First applications of the 3B analysis allowed revealing the dynamics of podosome
formation and dissociation. Living cells that stably express the talin protein fused
to the fluorescent protein mCherry were visualized throughout an entire cell with a
resolution of 50 nm on a 4 s-timescale. The authors report that podosomes are highly
dynamic structures and that smaller ring type structures down to 230 nm play an
important part in podosome dynamics [75].
11.6
OUTLOOK
More than a decade after the first practical realizations that enabled use to overcome
the diffraction limit of light, super-resolution microscopy has become a versatile
and robust technique in light microscopy. It has already had a significant impact on
biology, especially on molecular cell biology, and still has an ever-growing influence
on the development of modern microscopy techniques.
All methods mentioned above feature several advantages over traditional optical
microscopy, along with certain limitations, such as the need for specific fluorophores,
or limitations to the choice of sample – but no ideal and comparably perfect method
has been developed yet. STED microscopy requires the use of a rather complex
optical microscope configuration that needs regular alignment and highest quality
optical components. High laser powers are needed for the saturated depletion beam
to achieve a resolution on the nanometer scale, which can, in turn, potentially damage
biological samples. STORM utilizes photophysical and photochemical properties
such as photoswitchable fluorophores under special chemical conditions. A high
labeling density required for very high resolution gives rise to non-separable multiple
fluorescent emitters that cannot be properly localized. The large number of images that
are required to complete a full image reconstruction from thousands of localizations
limits its use in live cell microscopy. The same applies to all algorithms that need
several images in order to perform statistical analysis, for example, SOFI and 3B.

496
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
Although the technical effort is low the computational is much higher and time-
consuming. SIM in its linear implementation yields merely a resolution doubling,
and it still requires the acquisition of small number of images for each 3D plane,
limiting its application to live cell imaging.
Due to the use of fluorescence-based techniques, the choice of proper fluorophores
and appropriate buffer conditions is demanding. Recent improvements on the pho-
tophysical properties of the dyes such as the stability and photoswitching capabili-
ties have led to more sophisticated results. Fast and low-noise detectors additionally
pushed single-molecule-based localization methods. Finally, the technological devel-
opment results in a number of commercially available super-resolution microscopes.
The method of choice would feature both, a lateral and an axial resolution in
the single-digit nanometer range that could easily be expanded to three dimensions
and multicolor applications. Besides an optical resolution enhancement the temporal
resolution should be capable of at least video rate imaging. This could provide an
even deeper insight into the structure of cells and would allow following cellular
dynamics in living cells. It is probably by a combination of techniques, such as SIM
combined with single molecule localization techniques that we will approach such
an ideal scenario in the near future. It seems quite logical that a mid-range super-
resolution technique, such as SIM, could be used to initially survey a sample, such
as cells in culture, to locate potential sites of interest that are then investigated with
higher resolution techniques, such as PALM/STORM.
Another interesting expansion of super-resolution techniques in the near future
will likely include alternative contrast mechanisms that do not require the use of
fluorescent probes. A natural extension appears to be the application of Raman
scattering, a contrast mechanism that enables the chemical analysis of samples based
on bond vibrations in molecules. Although much weaker in nature than fluorescence,
it provides similar information, and certain implementations, such as spontaneous
Raman scattering microscopy or surface-enhanced Raman scattering could certainly
utilize similar concepts as SIM and PALM/STORM. Even STED could possibly
be expanded to other nonlinear optical microcopies, such as coherent anti-Stokes
Raman scattering (CARS) microscopy. The future does indeed look bright for super-
resolution optical microscopy.
ACKNOWLEDGMENTS
We would like to thank all of our colleagues, in the Biomolecular Photonics Group
at the University of Bielefeld, as well as the research groups of Prof. Markus Sauer
at the Julius-Maximilians-University in W¨urzburg, Germany, and Prof. Mike Heile-
mann at the Johann-Wolfgang-Goethe-University in Frankfurt, Germany, and at the
Department of Internal Medicine and the NSF Center for Biophotonics Science and
Technology at the University of California in Davis, CA, USA, for their input and
support throughout the past years. We also thank Anja Huss and Prof. J¨org Enderlein
from the Georg-August-University G¨ottingen, Germany, for providing the two-color
SOFI image of a rat hippocampal neuron, and Dr. Benjamin Dale and Prof. Benjamin

REFERENCES
497
Chen from the Mount Sinai School of Medicine in New York, USA, for providing
the actin and Gag-iGfP samples of human T cells infected with HIV-1.
REFERENCES
[1] J. Pawley, Handbook of Biological Confocal Microscopy, 3rd ed. (Springer, New York,
NY, 2006).
[2] J. L. Spees, S. D. Olson, M. J. Whitney, and D. J. Prockop, “Mitochondrial transfer
between cells can rescue aerobic respiration,” Proc. Natl. Acad. Sci. USA 103(5), 1283–
1288 (2006).
[3] T. Enderle, T. Ha, D. F. Ogletree, D. S. Chemla, C. Magowan, and S. Weiss, “Mem-
brane specific mapping and colocalization of malarial and host skeletal proteins in the
Plasmodium falciparum infected erythrocyte by dual-color near-field scanning optical
microscopy,” Proc. Natl. Acad. Sci. USA 94, 520–525 (1997).
[4] R. M. Stoeckle, Y. D. Suh, V. Deckert, and R. Zenobi, “Nanoscale chemical analysis by
tip-enhanced Raman spectroscopy,” Chem. Phys. Lett. 318, 131–136 (2000).
[5] E. Betzig and R. J. Chichester, “Single molecules observed by near-field scanning optical
microscopy,” Science 262(5138), 1422–1428 (1993).
[6] (a) J. Elf, G. W. Li, and X. S. Xie, “Probing transcription factor dynamics at the single-
molecule level in a living cell,” Science 316 (5828), 1191–1194 (2007); (b) J. Yu, J. Xiao,
X. J. Ren, K. Q. Lao, X. S. Xie, “Probing gene expression in live cells, one protein
molecule at a time,” Science 311(5767), 1600–1603 (2006).
[7] A. E. Miller, A. J. Fischer, T. A. Laurence, C. W. Hollars, R. J. Saykally, J. C. Lagarias,
and T. Huser, “Single-molecule dynamics of phytochrome-bound fluorophores probed
by fluorescence correlation spectroscopy,” Proc. Natl. Acad. Sci. USA 103(30), 11136–
11141 (2006).
[8] A. Yildiz, J. N. Forkey, S. A. McKinney, T. Ha, Y. E. Goldman, P. R. Selvin, and V. Myosin,
“Walks hand-over-hand: single fluorophore imaging with 1.5-nm localization,” Science
300, 2061–2065 (2003).
[9] C. Kural, A. S. Serpinskaya, Y.-H. Chou, R. D. Goldman, V. I. Gelfand, and P. R. Selvin,
“Tracking melanosomes inside a cell to study molecular motors and their interaction,”
Proc. Natl. Acad. Sci. USA 104(13), 5378–5382 (2007).
[10] (a) B. Brandenburg and X. W. Zhuang, “Virus trafficking—learning from single-virus
tracking,” Nat. Rev. Microbiol. 5(3), 197–208 (2007); (b) B. Brandenburg, L. Y. Lee,
M. Lakadamyali, M. J. Rust, X. W. Zhuang, and J. M. Hogle, Imaging poliovirus entry
in live cells. PLoS Biol 5(7), 1543–1555 (2007).
[11] S. W. Hell and J. Wichmann, “Breaking the diffraction resolution limit by stimulated-
emission—stimulated-emission-depletion fluorescence microscopy,” Opt Lett 19(11),
780–782 (1994).
[12] (a) G. Donnert, J. Keller, R. Medda, M. A. Andrei, S. O. Rizzoli, R. Lurmann, R. Jahn, C.
Eggeling, and S. W. Hell, “Macromolecular-scale resolution in biological fluorescence
microscopy,” Proc. Natl. Acad. Sci. USA 103(31), 11440–11445 (2006); (b) K. I. Willig,
S. O. Rizzoli, V. Westphal, R. Jahn, and S. W. Hell, “STED microscopy reveals that
synaptotagmin remains clustered after synaptic vesicle exocytosis,” Nature 440(7086),
935–939 (2006).

498
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
[13] B. Harke, J. Keller, C. K. Ullal, V. Westphal, A. Schoenle, and S. W. Hell, “Resolution
scaling in STED microscopy,” Opt Express 16(6), 4154–4162 (2008).
[14] M. G. L. Gustafsson, L. Shao, P. M. Carlton, C. J. R. Wang, I. N. Golubovskaya, W. Z.
Cande, D. A. Agard, and J. W. Sedat, “Three-dimensional resolution doubling in wide-
field fluorescence microscopy by structured illumination,” Biophys J. 94(12), 4957–4970
(2008).
[15] (a) E. Betzig, G. H. Patterson, R. Sougrat, O. W. Lindwasser, S. Olenych, J. S. Boni-
facino, M. W. Davidson, J. Lippincott-Schwartz, and H. F. Hess, “Imaging intracellular
fluorescent proteins at nanometer resolution,” Science 313(5793), 1642–1645 (2006);
(b) S. Manley, J. M. Gillette, G. H. Patterson, H. Shroff, H. F. Hess, E. Betzig, and J.
Lippincott-Schwartz, “High-density mapping of single-molecule trajectories with pho-
toactivated localization microscopy,” Nat. Methods 5, 155–157 (2008).
[16] (a) M. J. Rust, M. Bates, and X. W. Zhuang, “Sub-diffraction-limit imaging by stochastic
optical reconstruction microscopy (STORM),” Nat. Methods 3(10), 793–795 (2006);
(b) B. Huang, W. Q. Wang, M. Bates, and X. W. Zhuang, “Three-dimensional super-
resolution imaging by stochastic optical reconstruction microscopy,” Science 319, 810–
813 (2008).
[17] S. W. Hell, “Far-field optical nanoscopy,” Science 316(5828), 1153–1158 (2007).
[18] T. A. Klar, S. Jakobs, M. Dyba, A. Egner, and S. W. Hell, “Fluorescence microscopy
with diffraction resolution barrier broken by stimulated emission,” Proc. Natl. Acad. Sci.
USA 97(15), 8206–8210 (2000).
[19] L. Kastrup, H. Blom, C. Eggeling, and S. W. Hell, “Fluorescence fluctuation spectroscopy
in subdiffraction focal volumes,” Phys. Rev. Lett. 94, 178104 (2005).
[20] E. Auksorius, B. R. Boruah, C. Dunsby, P. M. P. Lanigan, G. Kennedy, M. A. A. Neil,
and P. M. W. French, “Stimulated emission depletion microscopy with a supercontinuum
source and fluorescence lifetime imaging,” Opt. Lett. 33(2), 113–115 (2008).
[21] G. Donnert, J. Keller, C. A. Wurm, S. O. Rizzoli, V. Westphal, A. Schonle, R. Jahn,
S. Jakobs, C. Eggeling, and S. W. Hell, “Two-color far-field fluorescence nanoscopy,”
Biophys. J. 92(8), L67–L69 (2007).
[22] V. Westphal, S. O. Rizzoli, M. A. Lauterbach, D. Kamin, R. Jahn, and S. W. Hell, “Video-
rate far-field optical nanoscopy dissects synaptic vesicle movement,” Science 320(5873),
246–249 (2008).
[23] S. Berning, K. I. Willig, H. Steffens, P. Dibaj, and S. W. Hell, “Nanoscopy in a living
mouse brain,” Science 335(6068), 551–551 (2012).
[24] J. Chojnacki, T. Staudt, B. Glass, P. Bingen, J. Engelhardt, M. Anders, J. Schneider,
B. Muller, S. W. Hell, and H. G. Krausslich, “Maturation-dependent HIV-1 surface
protein redistribution revealed by fluorescence nanoscopy,” Science 338(6106), 524–528
(2012).
[25] C. Eggeling, C. Ringemann, R. Medda, G. Schwarzmann, K. Sandhoff, S. Polyakova,
V. N. Belov, B. Hein, C. von Middendorff, et al., “Direct observation of the nanoscale
dynamics of membrane lipids in a living cell,” Nature 457(7233), 1159–1162 (2009).
[26] E. Betzig, “Proposed method for molecular optical imaging,” Opt. Lett. 20(3), 237–239
(1995).
[27] S. T. Hess, T. P. Girirajan, and M. D. Mason, “Ultra-high resolution imaging by
fluorescence photoactivation localization microscopy,” Biophys. J. 91(11), 4258–4272
(2006).

REFERENCES
499
[28] R. Ando, H. Mizuno, and A. Miyawaki, “Regulated fast nucleocytoplasmic shuttling
observed by reversible protein highlighting,” Science 306(5700), 1370–1373 (2004).
[29] M. Bates, T. R. Blosser, and X. Zhuang, “Short-range spectroscopic ruler based on a
single-molecule optical switch,” Phys. Rev. Lett. 94(10), 108101 (2005).
[30] M. Heilemann, E. Margeat, R. Kasper, M. Sauer, and P. Tinnefeld, “Carbocyanine dyes
as efficient reversible single-molecule optical switch,” J. Am. Chem. Soc. 127(11), 3801–
3806 (2005).
[31] (a) H. Shroff, C. G. Galbraith, J. A. Galbraith, and E. Betzig, “Live-cell photoactivated
localization microscopy of nanoscale adhesion dynamics,” Nat. Methods 5(5), 417–423
(2008); (b) S. van de Linde, M. Sauer, and M. Heilemann, “Subdiffraction-resolution fluo-
rescence imaging of proteins in the mitochondrial inner membrane with photoswitchable
fluorophores,” J. Struct. Biol. 164(3), 250–254 (2008).
[32] M. Heilemann, S. van de Linde, A. Mukherjee, and M. Sauer, “Super-resolution imaging
with small organic fluorophores,” Angew Chem. Int. Ed. Engl. 48(37), 6903–6908 (2009).
[33] M. Heilemann, S. van de Linde, M. Schuttpelz, R. Kasper, B. Seefeldt, A. Mukherjee,
P. Tinnefeld, and M. Sauer, “Subdiffraction-resolution fluorescence imaging with con-
ventional fluorescent probes,” Angew Chem. Int. Ed. Engl. 47(33), 6172–6176 (2008).
[34] S. van de Linde, A. Loschberger, T. Klein, M. Heidbreder, S. Wolter, M. Heilemann, and
M. Sauer, “Direct stochastic optical reconstruction microscopy with standard fluorescent
probes,” Nat. Protoc. 6(7), 991–1009 (2011).
[35] I. Rasnik, S. A. McKinney, and T. Ha, “Nonblinking and long-lasting single-molecule
fluorescence imaging,” Nat. Methods 3(11), 891–893 (2006).
[36] R. E. Thompson, D. R. Larson, and W. W. Webb, “Precise nanometer localization analysis
for individual fluorescent probes,” Biophys. J. 82(5), 2775–2783 (2002).
[37] S. Wolter, M. Schuttpelz, M. Tscherepanow, S. Van de Linde., M. Heilemann, and
M. Sauer, “Real-time computation of subdiffraction-resolution fluorescence images,”
J. Microsc-Oxford 237(1), 12–22 (2010).
[38] S. J. Holden, S. Uphoff, and A. N. Kapanidis, “DAOSTORM: an algorithm for high-
density super-resolution microscopy,” Nat. Methods 8(4), 279–280 (2011).
[39] (a) B. Huang, S. A. Jones, B. Brandenburg, and X. Zhuang, “Whole-cell 3D STORM
reveals interactions between cellular structures with nanometer-scale resolution,” Nat
Methods 5(12), 1047–1052 (2008); (b) B. Huang, W. Wang, M. Bates, X. Zhuang, “Three-
dimensional super-resolution imaging by stochastic optical reconstruction microscopy,”
Science 319(5864), 810–813 (2008).
[40] A. Vaziri, J. Y. Tang, H. Shroff, and C. V. Shank, “Multilayer three-dimensional super
resolution imaging of thick biological samples,” Proc. Natl. Acad. Sci. USA 105(51),
20221–20226 (2008).
[41] F. C. Zanacchi, Z. Lavagnino, M. P. Donnorso, A. Del Bue, L. Furia, M. Faretta, and
A. Diaspro, “Live-cell 3D super-resolution imaging in thick biological samples,” Nat.
Methods 8(12), 1047–1049 (2011).
[42] D. Baddeley, I. D. Jayasinghe, C. Cremer, M. B. Cannell, C. Soeller, “Light-induced
dark states of organic fluochromes enable 30 nm resolution imaging in standard media,”
Biophys. J. 96(2), L22–L24 (2009).
[43] (a) J. Bierwagen, I. Testa, J. Folling, D. Wenzel, S. Jakobs, C. Eggeling, and S. W.
Hell, “Far-field autofluorescence nanoscopy,” Nano Lett 10(10), 4249–4252 (2010); (b)
J. Folling, M. Bossi, H. Bock, R. Medda, C. A. Wurm, B. Hein, S. Jakobs, C. Eggeling,

500
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
and S. W. Hell, “Fluorescence nanoscopy by ground-state depletion and single-molecule
return,” Nat Methods 5(11), 943–945 (2008); (c) I. Testa, C. A. Wurm, R. Medda,
E. Rothermel, C. von Middendorf, J. Folling, S. Jakobs, A. Schonle, S. W. Hell, and
C. Eggeling, “Multicolor fluorescence nanoscopy in fixed and living cells by excit-
ing conventional fluorophores with a single wavelength,” Biophys J. 99(8), 2686–2694
(2010).
[44] D. Greenfield, A. L. McEvoy, H. Shroff, G. E. Crooks, N. S. Wingreen, E. Betzig, and
J. Liphardt, “Self-organization of the Escherichia coli chemotaxis network imaged with
super-resolution light microscopy,” PLoS Biol. 7(6), e1000137 (2009).
[45] J. Yao, R. D. Fetter, P. Hu, E. Betzig, and R. Tjian, “Subnuclear segregation of genes and
core promoter factors in myogenesis,” Genes Dev. 25(6), 569–580 (2011).
[46] A. Loschberger, S. van de Linde, M. C. Dabauvalle, B. Rieger; M. Heilemann, G.
Krohne, and M. Sauer, “Super-resolution imaging visualizes the eightfold symmetry of
gp210 proteins around the nuclear pore complex and resolves the central channel with
nanometer resolution,” J. Cell. Sci. 125(Pt 3), 570–575 (2012).
[47] D. Lando, U. Endesfelder, H. Berger, L. Subramanian, P. D. Dunne, J. McColl, D.
Klenerman, A. M. Carr, M. Sauer, R. C. Allshire, et al., “Quantitative single-molecule
microscopy reveals that CENP-A(Cnp1) deposition occurs during G2 in fission yeast,”
Open Biol. 2(7), 120078 (2012).
[48] R. Heintzmann and C. Cremer, “Laterally modulated excitation microscopy: improve-
ment of resolution by using a diffraction grating,” Opt. Biopsies Microsc. Tech. III, Proc.
3568, 185–196 (1999).
[49] J. T. Frohn, H. F. Knapp, and A. Stemmer, “True optical resolution beyond the Rayleigh
limit achieved by standing wave illumination,” Proc. Natl. Acad. Sci. USA 97(13), 7232–
7236 (2000).
[50] M. G. L. Gustafsson, D. A. Agard, and J. W. Sedat, “Doubling the lateral resolution of
wide-field fluorescence microscopy using structured illumination,” Proc. Soc. Photo-Opt.
Ins. 1(13), 141–150 (2000).
[51] (a) L. M. Hirvonen, K. Wicker, O. Mandula, and R. Heintzmann, “Structured illumination
microscopy of a living cell,” Eur Biophys. J. Biophy. 38(6), 807–812 (2009); (b) R.
Fiolka, M. Beck, and A. Stemmer, “Structured illumination in total internal reflection
fluorescence microscopy using a spatial light modulator,”. Opt Lett 33(14), 1629–1631
(2008).
[52] M. A. Neil, R. Juskaitis, and T. Wilson, “Method of obtaining optical sectioning by using
structured light in a conventional microscope,” Opt. Lett. 22(24), 1905–1907 (1997).
[53] M. A. Neil, A. Squire, R. Juskaitis, P. I. Bastiaens, and T. Wilson, “Wide-field optically
sectioning fluorescence microscopy with laser illumination,” J. Microsc. 197(Pt 1), 1–4
(2000).
[54] K. Wicker, O. Mandula, G. Best, R. Fiolka, and R. Heintzmann, “Phase optimisation for
structured illumination microscopy,”. Opt Express 21(2), 2032–2049 (2013).
[55] D. Laksameethanasan, S. S. Brandt, P. Engelhardt, O. Renaud, and S. L. Shorte,
“A Bayesian reconstruction method for micro-rotation imaging in light microscopy,”
Microsc. Res. Tech. 71(2), 158–167 (2008).
[56] E. Mudry, K. Belkebir, J. Girard, J. Savatier, E. Le Moal, C. Nicoletti, M. Allain, and
A. Sentenac, “Structured illumination microscopy using unknown speckle patterns,” Nat.
Photonics 6, 312–315 (2012).

REFERENCES
501
[57] P. Kner, B. B. Chhun, E. R. Griffis, L. Winoto, and M. G. L. Gustafsson, “Super-resolution
video microscopy of live cells by structured illumination,” Nat Methods 6(5), 339–U36
(2009).
[58] L. Shao, P. Kner, E. H. Rego, and M. G. L. Gustafsson, “Super-resolution 3D microscopy
of live whole cells using structured illumination,” Nat. Methods 8(12), 1044–+ (2011).
[59] C. J. R. Sheppard, “Super-resolution in confocal imaging,” Optik 80, 53–54 (1998).
[60] C. B. Muller and J. Enderlein, “Image Scanning Microscopy,” Phys. Rev. Lett. 104(19),
198101 (2010).
[61] A. G. York, S. H. Parekh, D. D. Nogare, R. S. Fischer, K. Temprine, M. Mione, A. B.
Chitnis, C. A. Combs, and H. Shroff, “Resolution doubling in live, multicellular organ-
isms via multifocal structured illumination microscopy,” Nat. Methods 9(7), 749–754
(2012).
[62] (a) R. Heintzmann, T. M. Jovin, and C. Cremer, “Saturated patterned excitation
microscopy—a concept for optical resolution improvement,” J. Opt. Soc. Am. A 19(8),
1599–1609 (2002); (b) R. Heintzmann, “Saturated patterned excitation microscopy with
two-dimensional excitation patterns,” Micron 34(6–7), 283–291 (2003).
[63] M. G. Gustafsson, “Nonlinear structured-illumination microscopy: wide-field fluores-
cence imaging with theoretically unlimited resolution,” Proc. Natl. Acad. Sci. USA
102(37), 13081–13086 (2005).
[64] E. H. Rego, L. Shao, J. J. Macklin, L. Winoto, G. A. Johansson, N. Kamps-Hughes, M. W.
Davidson, and M. G. Gustafsson, “Nonlinear structured-illumination microscopy with
a photoswitchable protein reveals cellular structures at 50-nm resolution,” Proc. Natl.
Acad. Sci. USA 109(3), E135–E143 (2012).
[65] (a) T. A. Planchon, L. Gao, D. E. Milkie, M. W. Davidson, J. A. Galbraith, C. G. Galbraith,
and E. Betzig, “Rapid three-dimensional isotropic imaging of living cells using Bessel
beam plane illumination,” Nat. Methods 8(5), 417–423 (2011); (b) L. Gao, L. Shao, C. D.
Higgins, J. S. Poulton, M. Peifer, M. W. Davidson, X. Wu, B. Goldstein, and E. Betzig,
“Noninvasive imaging beyond the diffraction limit of 3D dynamics in thickly fluorescent
specimens,” Cell 151(6), 1370–1385 (2012).
[66] P. J. Keller, A. D. Schmidt, A. Santella, K. Khairy, Z. Bao, J. Wittbrodt, and E. H.
Stelzer, “Fast, high-contrast imaging of animal development with scanned light sheet-
based structured-illumination microscopy,” Nat. Methods 7(8), 637–642 (2010).
[67] R. Fiolka, L. Shao, E. H. Rego, M. W. Davidson, and M. G. Gustafsson, “Time-lapse
two-color 3D imaging of live cells with doubled resolution using structured illumination,”
Proc. Natl. Acad. Sci. USA 109(14), 5311–5315 (2012).
[68] V. Andresen, K. Pollok, J. L. Rinnenthal, L. Oehme, R. Gunther, H. Spiecker, H.
Radbruch, J. Gerhard, A. Sporbert, Z. Cseresnyes,et al., “High-resolution intravital
microscopy,” PLoS One 7(12) (2012).
[69] L. Schermelleh, P. M. Carlton, S. Haase, L. Shao, L. Winoto, P. Kner, B. Burke, M. C.
Cardoso, D. A. Agard, M. G. L. Gustafsson et al., “Subdiffraction multicolor imaging of
the nuclear periphery with 3D structured illumination microscopy,” Science 320(5881),
1332–1336 (2008).
[70] V. C. Cogger, G. P. McNerney, T. Nyunt, L. D. DeLeve, P. McCourt, B. Smedsrod, D. G.
Le Couteur, and T. R. Huser, “Three-dimensional structured illumination microscopy
of liver sinusoidal endothelial cell fenestrations,” J. Struct. Biol. 171(3), 382–388
(2010).

502
IMAGING AND PROBING CELLS BEYOND THE OPTICAL DIFFRACTION LIMIT
[71] P. Kner, B. B. Chhun, E. R. Griffis, L. Winoto, and M. G. Gustafsson, “Super-resolution
video microscopy of live cells by structured illumination,” Nat. Methods 6(5), 339–342
(2009).
[72] T. Dertinger, R. Colyer, G. Iyer, S. Weiss, and J. Enderlein, “Fast, background-free, 3D
super-resolution optical fluctuation imaging (SOFI),” Proc. Natl. Acad. Sci. USA 106(52),
22287–22292 (2009).
[73] T. Dertinger, M. Heilemann, R. Vogel, M. Sauer, and S. Weiss, “Superresolution optical
fluctuation imaging with organic dyes,” Angew. Chem. Int. Ed. Engl. 49(49), 9441–9443
(2010).
[74] S. Geissbuehler, N. Bocchio, C. Dellagiacoma, C. Berclaz, M. Leutenegger, and T. Lasser,
“Mapping molecular statistics with balanced super-resolution optical fluctuation imaging
(bSOFI),” Opt. Nano. 1(1), 1–7 (2012).
[75] S. Cox, E. Rosten, J. Monypenny, T. Jovanovic-Talisman, D. T. Burnette, J. Lippincott-
Schwartz, G. E. Jones, and R. Heintzmann, “Bayesian localization microscopy reveals
nanoscale podosome dynamics,” Nat. Methods 9(2), 195–200 (2012).
[76] M. Gallina, J. Xu, T. Dertinger. A. Aizer, Y. Shav-Tal, and S. Weiss, “Resolving the
spatial relationship between intracellular components by dual color super resolution
optical fluctuations imaging (SOFI),” Opt. Nano. 2(1), 1–9 (2013).

12
TECHNOLOGY
Ann E. Elsner and Christopher A. Clark
School of Optometry, Indiana University, Bloomington, IN, USA
12.1
BASIC OCULAR ANATOMY AND PHYSIOLOGY
The eye in general and the human eye, in particular, offer both unique opportunities
and challenges when it comes to the uses of light [1–5]. Photonics can be used to
measure a number of different aspects of the eye that have direct effects on detection
and treatment of diseases. As with any tissue being examined or treated with the
use of photonics, safety issues become a limiting factor. The eye’s innate biological
function limits the ability to examine it. That is, ocular structures are designed to
be clear and to capture light rather than reflect it back out of the eye. To understand
the uses of photonics in the eye, one must first have a basic understanding of the
structure and function of a normal eye (Fig. 12.1). The eye changes throughout life,
as part of a normal developmental process that at first improves many aspects of vision
throughout early childhood. Later, as the ocular structures age, there are consequences
of systems failing and resulting in decreases in visual function. More extreme are
overzealous responses of wound healing by the retinal vessels in the eye, such as in
diabetes, since in central nervous system tissues must self-repair to last a lifetime.
The eye, which has the shape of a ball except for the anterior portion that has a
clear, protruding portion that serves as a lens and is directly visible, plus a posterior
portion with the neural connections, which are inside the head and cannot be seen
without instrumentation. The methods and requirements for diagnosis and treatment
differ greatly due both to accessibility and vastly different tissue compositions. The
importance of the entire eye remaining relatively transparent throughout life cannot
be overstated, particularly since the distance traveled is relatively large compared with
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
503

504
TECHNOLOGY
Anterior chamber
Lens
Cornea
Vitreous
chamber
Fovea
Retina
Optic nerve
Cilliary body
FIGURE 12.1
Anatomy of the eye. The front surface, made up of the cornea and the tear
film, serves as the main refractive element and is clear. The lens provides variable focus in
younger individuals. The iris serves as a diaphragm to limit the amount and angle of light
reaching the retina. The retina covers the inside of much of the eye, and contains the neural
elements needed to capture light, transduce light energy to a neural signal, and code the signal
for transmission to the brain. The fovea is the location of the densest array of photoreceptors.
The optic nerve carries the visual signals to the brain, and exits the eye via the optic nerve head.
the collection angle permitted by the entrance through the pupil. From the surface
of the cornea, through the refractive and structural elements, and to retina where
light is collected is roughly 22 mm. The typical refractive error of a human eye is
approximately 60 diopters, where a diopter is defined as 1/m. The anterior elements
of the eye, which have a much wider numerical aperture than the interior portions,
are far more reflective than the retina, which returns at most only a few percent of the
illumination.
12.1.1
Anterior Segment Anatomy
12.1.1.1
Cornea and Tear Film
The most anterior portion of the eye is composed
of the tear film and the cornea (Fig. 12.2). The tear film sits on top of and is anchored
to the cornea, positioned for a number of purposes. The tear film serves as the first
optical surface of the eye and as such, it needs to maintain a smooth, regular surface
to allow for both the eye to see and for optical systems to examine the eye. A high
index of refraction, which differs significantly from that of air, is a feature of this first
surface. The tear film also provides part of the system to protect the anterior eye from
infection from bacteria and viruses through the production of mucus and presence

BASIC OCULAR ANATOMY AND PHYSIOLOGY
505
Anterior chamber
Lens
Iris
Cornea
FIGURE 12.2
Anterior segment optical coherence tomography (OCT) showing the cornea,
anterior chamber, and iris. Only the anterior surface of the lens is hinted at due to the clarity
of the lens. The anterior chamber is fluid filled also and does not reflect back any light. Image
courtesy of Dr. Peter Kolbaum and Dr. Meredith Jansen (Indiana University, Bloomington, IN).
of antibodies. The tear film is composed of three components: a lipid/oil to prevent
evaporation, an aqueous component to keep the surface moist, and a mucin component
to provide structure. Many of these components can be imaged with various photonics
techniques. The cornea provides the structure that accounts for approximately 66% of
a human eye’s focusing power while having a thickness of only 550 μm. The cornea
and the lens are the only two parts of the human body that are biological tissues, not
fluids that are replaced, with a primary requirement of remaining optically clear. This
requires a unique structure to allow for this clarity in the cornea.
For the purpose of this chapter, we will consider that the cornea is composed
of three main layers. The first main layer is the corneal epithelium, which provides
structure for the tear film and the final layer of protection from microbial infection.
The second main layer is the stroma, which provides the bulk of the thickness
of the cornea. This stroma is composed of long protein strands known as fibrils,
collagen filaments 1.5–2.5 μm thick positioned in a lattice structure, and anchored by
additional molecules. The spacing between fibrils must be maintained at <200 nm.
The fibril thickness, the distance between fibrils, and the lattice structure allow for
any light scatter within the cornea to be removed by destructive interference. The
final main layer of the cornea is the endothelium, which provides aqueous pumps
to maintain the exact corneal hydration to maintain the proper fibril spacing in the
stroma for clarity. Between the corneal epithelium and the stroma lies Bowman’s
layer, a thin layer composed of collagen and other proteins and sugars and designed
to protect the corneal stroma. Between the stroma and the corneal endothelium lies
Descemet’s membrane, the basement membrane for the thin and fragile corneal
endothelium.
12.1.1.2
Anterior Chamber and Lens
The next portion of the eye is the anterior
chamber, iris, ciliary body, and lens. The anterior chamber of the eye sits between
the lens and the cornea. It is primarily a fluid-filled chamber that delivers nutrients
such as glucose to the interior of the eye. Structural changes with aging or disease
that occur in this area, specifically those decreasing the depth of the anterior chamber

506
TECHNOLOGY
such that fluid flow is reduced by the physical barrier of the lens in contact with the
outflow pathway, can lead to an unwanted increase in intraocular pressure. Similarly,
decreases in the size or increases in the resistance of the pores through which the fluid
exits the eye, can result in decrease of outflow to the extent that the intraocular pressure
is raised. This increased pressure is one common risk factor for glaucoma, which is
a common progressive neuropathy of the optic nerve. Considerable diagnostic and
treatment efforts have been made to reduce the impact of glaucoma.
The iris functions as a light-controlling mechanism. Through muscle action, the
iris can change size, thereby altering the size of the pupil and ultimately the amount
and angular subtense of light passing through the pupil. The inner diameter of the
iris differs greatly with light level, age, and level of arousal [6, 7]. With changes in
diameter of the iris varying over a range of more than 8 mm in a young eye in the
dark to 1 mm in an older eye or in intense light, the amount of light available for
sight or imaging the structures within the eye varies by more than a factor of 64. The
spatial resolution of both vision and imaging of the structures within the eye are then
limited by the numerical aperture resulting from the pupil diameter of 1–8 mm and
the nodal point of about 17 mm. Further, the angular scatter of structures within the
eye, which may influence vision or imaging when large range scatter occurs, are not
directly characterized by in vivo measurements.
Immediately behind the pupil is the lens, which provides the final 33% of focusing
power for the eye. Anterior changes in the curvature of this lens, due to the muscle
action through spring-like zonules, provides for the eye to focus for various distances.
The muscle action is provided by the ciliary body, which is adjacent and posterior to
the iris. With aging comes stiffening of the lens, resulting in the failure to provide
fine focus, along with gradual opacification of the lens, known as cataract. The ciliary
body also provides the nutrient fluid that fills the anterior chamber and pressurizes
the eye. A mismatch of the rates of production and outflow in the eye results in higher
intraocular pressure. There is a significant increase in the optical aberrations with
age, along with more prevalence of some of the higher order aberrations that are less
common in younger individuals [8–12].
Behind the lens and ciliary body is the posterior chamber, which is largely filled
with a collagenous gel, known as the vitreous body. The vitreous and the pressure
resulting from fluid creation allow for the optical components to maintain alignment.
Any changes in the distances between the optical elements and the retina results in
myopia (nearsightedness) or hyperopia (farsightedness). While the vitreous is clear
in younger eyes, the collagen degrades with aging, causing it to shrink, distort, and
lose clarity. Eventually, the vitreous detaches from the layers beneath it, and not
always cleanly, or has strands that slide into the visual pathway during eye motion.
Considerable optical artifacts result from these changes.
12.1.2
Retina
The retina and its support system, composed of the retinal pigment epithelium (RPE)
and the choroid, line the inside of the eye (Fig. 12.3). The retina is composed of
three types of cells, neural cells responsible for light capture and transmission, glial

BASIC OCULAR ANATOMY AND PHYSIOLOGY
507
FIGURE 12.3
An en face image of the retina of a 54-year-old male, acquired with the Laser
Scanning Digital Camera at 850 nm with a 36◦, foveal-centered field of view. The fovea is
localized by the slightly darker circular region in the center of the image, containing a small,
bright spot which is the reflection from the foveal pit. The darker region on the right is optic
nerve head, where retinal vessels enter and exit the eye, and the neurons exit, carrying the visual
signal for further processing in the brain. The retinal blood vessels form networks, shaped like
trees branches that are contained within the retina in a healthy eye. Image courtesy of Ann
Elsner and Matthew Muller (Indiana University and Aeon Imaging, LLC).
cells that support the neural cells, and vascular cells and structural components to
provide nutrients to the eye. The retina is composed of numerous layers, each with
a specialized function (Fig. 12.4). These layers also vary with the distance from the
fovea and optic nerve head, where the most acute vision is supported and the optic
nerve exits the eye, respectively.
12.1.2.1
Photoreceptors
The photoreceptors are the irreplaceable first element
of vision, and as such, are a key target in modern diagnostics for eye care. The
photoreceptors are in the outermost layer of the neural portion of the retina, and
directly in contact with the RPE, which contains the support cells for the outer
portions of the photoreceptors. Although designed to capture light, photoreceptors
are actually positioned beneath the neural apparatus used to transduce and transmit
the visual signal to the brain, as well as underneath (or exterior to) the retinal vascular
system (Fig. 12.4). This organization has the advantage that the RPE, which is in
contact with a solid vascular plexus just beneath (or outside) the RPE, can provide
oxygen and nutrients, remove wastes, and perform photoreceptor renewal without
these cellular and vascular structures lying in the direct path of vision. There are no
photoreceptors on the optic nerve head. The photoreceptors have two functions, the
first of which is to guide light so that it is captured efficiently [13–15]. The cones
collect light over a small angle, so that precise spatial localization of the optical path
of incoming photons occurs, which helps preserve spatial acuity. The ability to guide

508
TECHNOLOGY
light depends upon the retinal location and status. The width of cone photoreceptors
is about 1–5 μm, and this refers to the outer segment that contains the pigment,
not the cell body or axon. Very narrow cone photoreceptors do not have as good
antenna properties as the slightly wider cones, so that the most efficient guidance is
not actually in the central fovea. While it may seem obvious that retinal degenerations
that lead to photoreceptor structural changes and cell death cause poor guiding of
light by cones, there are crucial, but subtle, changes in guidance by photoreceptors.
Guidance changes degrade vision and also provide early detection of retinal disease
Nerve fiber layer
(reflective and strong form birefringence)
200 μm
Cell body layers
(relatively transparent with current in vivo imaging
techniques in humans)
Photoreceptor layers
(inner segment and outer segment)
RPE
Choroid

BASIC OCULAR ANATOMY AND PHYSIOLOGY
509
such as macular edema in diabetes (Fig. 12.5), the most common cause of reduced
acuity in working individuals in the USA [16].
The second function is to turn light into biological signal, and this is accomplished
by means of transduction into chemical energy and the generation of a graded response
[1–3]. The photoreceptors normally include three types of cones that differ in spectral
sensitivity, with their chief roles for color vision, high resolution spatial and temporal
vision, and bright light conditions. The absorption spectra of the cones are broadband,
with considerable overlap, so that difference signals can provide discrimination for
a wide range of colors (Figs. 12.6 and 12.7) [17–19]. The different types of cones
are positioned with respect to each other so that spatial and spectral information can
be derived from the comparisons, and this distribution can be mapped with highly
magnified views in living subjects [20,21].
Rods are the most numerous photoreceptors, and are responsible for vision in dim
conditions, detection of large objects in the periphery, and working with cones to
maintain an adaptation level. The rods collect light over a wide angle, since their
primary role is to capture and sum together photons to provide the most sensitive
detection possible. Rod outer segments are narrower in diameter than all except the
most central cones [1–3].
Cone photoreceptor densities are highest in our central vision, which is necessary
to produce high acuity and fine discriminations of color within a small area. There
are about 4–5 million cones in the human retina, but about 100 million rods. The
cones are densely packed in the central fovea, but decrease in density with increas-
ing distance from the foveal center (Figs. 12.8 and 12.9) [22–26]. The rods and
short-wavelength cones have the opposite distribution, being scarce in the central
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
FIGURE 12.4
Cross-sectional SD-OCT image (b-scan) of the retina of a normal right eye
of a 46-year-old female Caucasian. The image is the average of multiple scans, adjustable by
user selection, and with automatic eye tracking enabled (Spectralis, Heidelberg Engineering,
Heidelberg, Germany). Top: Average image, presented as a gray scale with stronger signals
being white and weaker signals dark, showing that several strong index of refraction changes
demonstrate many of the layers of the retina and RPE. Middle: Manual segmentation at borders,
so that thickness measurements of the retina can be measured for a number of the layers of the
retina. From top to bottom, the lines are as follows: The border between the vitreous and the
retina where the inner limiting membrane defines the innermost surface of the retina; next is,
the border between the nerve fiber layer and the ganglion cell layer; next is, the border between
the inner plexiform layer and the inner nuclear layer; next is, the border between the inner
nuclear layer and the outer plexiform layer; next is, the border between the outer plexiform
layer and the outer nuclear layer; next is the external limiting membrane that could also be the
myoid structures of the photoreceptors; border between the inner and outer segments of the
photoreceptors; next is the border between the outer segment and the RPE; last is the border
between the RPE and the choroid. Bottom diagram of the cross section of a normal eye outside
of the foveal pit, showing key layers and optical signatures. There is strong birefringence from
the regular spacing of the superficial retinal nerve fiber layers, which also are highly reflective
and obscure deeper structures. Image courtesy of Joel Papay (Indiana University, Bloomington,
IN).

510
TECHNOLOGY
FIGURE 12.5
Laser Scanning Digital Camera and Canon nonmydriatic fundus camera
images of a 63-year-old African American male who has diabetes. Left: The near-infrared
illumination of the Laser Scanning Digital Camera, but not the visible wavelength light of the
fundus camera, penetrates through moderate media opacities. Right: Further, the LSDC and
Canon both show numerous dot hemorrhages and microaneurysms, consistent with diabetic
retinopathy. (For a color version of this figure, see the color plate section.)
fovea but increasing in numbers with increasing distance from the fovea. The optical
density of cone photopigment is also greatest in the foveal center of young, healthy
retinas, but decreases in density with aging along with several retinal diseases, thus
is considered an early warning sign of retinal disease [27–32]. A dense packing in
one location and sparse distribution elsewhere provides a challenge for the neural
coding of the output from the rod and cone photoreceptors and, consequently, the
FIGURE 12.6
Examples of optic nerve head-centered retinal images (10-frame averages)
taken in undilated subjects with dark eyes, and showing the branching of the retinal vessels
entering and exiting the eye. Left, A 40-year-old Asian male; center, a 50-year-old Black
female; right, a 43-year-old Hispanic male. Unlike many visible flood illumination imaging
systems, the high vessel contrast is maintained in the LSDC regardless of increased retinal
pigmentation according to racial or ethnic background.

BASIC OCULAR ANATOMY AND PHYSIOLOGY
511
Spectral characteristics of
innate absorbers in the eye
MP macular pigment
Short wavelength sensitive
pigment (cones)
Melanin
Me
S
Hb (de-oxygenated hemoglobin)
Hb
LP
MP
HbO (oxygenated hemoglobin)
HbO
H2O (water)
H2O
400
.01
0.1
600
800
1000
Wavelength (nm)
Absorption of ocular pigments
Fundus reflectance
Medium wavelength sensitive
pigment (cones)
M
Long wavelength sensitive
pigment (cones)
L
Lens (increase with age)
Le
DP
Rod photopigment
R
FIGURE 12.7
Spectral characteristics of the innate absorbers in the human eye and the over-
all reflectance. The absorbers are demonstrated in two manners, either as the peak absorption
spectra for photopigments and macular pigment, which are well known to have broad spectra
with considerable overlap, or as the spectra expected from absorption due to the normal optical
path length and concentration in the healthy eye. The photopigments bleach on exposure to
light, and therefore change dramatically in optical density and consequently the spectra. Mac-
ular pigment differs by more than 1 log unit in healthy individuals, also altering the relative
absorption spectrum. Modified from Reference 48. (For a color version of this figure, see the
color plate section.)
retina has considerable lateral connections for signal conditioning, the inner and outer
plexiform layers.
The visual system is designed for two competing roles, to detect change that is
meaningful, but at the same time to maintain color and brightness constancy despite
the operation over a more than 11 log unit range. Thus, while a specific wavelength
might appear blue under one set of circumstances, the neural contrast with an
adjacent region of color or the appearance in differing lighting conditions or the
visual periphery can greatly alter the appearance of an object. For this reason, graphic
display must take into consideration the light conditions, viewing angle, size of the
display, and other factors, in addition to the color vision of the observer [17–19].
Information presented using color differences on a fine scale, that is, very few pixels

512
TECHNOLOGY
FIGURE 12.8
Adaptive optics montage of the cone photoreceptors across 10◦of the retina.
Top: montaged image of the retina acquired using AOSLO. White star area represents the fovea.
Three white boxes show areas corresponding to different sampling points. Bottom: 1–3, Cone
photoreceptor densities from the three measured areas, 1.5◦, 6◦, and 9◦from the fovea. Note that
the cone densities decrease with increasing distance from the fovea. Scale bar equals 100 μm.
Image courtesy of Stephen Burns and Toco P Chui (Indiana University, Bloomington, IN).
for each color, often lead to an average response rather than a clear illustration, with
critical applications including medical image representation. For this reason, many
vendors of ophthalmic imaging devices have moved away from the use of colorful
outputs, particularly arbitrarily defined false-coloring of retinal layers or other
structures, which may be eye-catching but are unsuitable for high resolution images.
FIGURE 12.9
In vivo image of the retinal cone photoreceptors with an AOSLO system,
demonstrating the optical resolution. Image courtesy of Stephen Burns (Indiana University,
Bloomington, IN).

BASIC OCULAR ANATOMY AND PHYSIOLOGY
513
The density of cones generally decreases with increasing distance from the fovea,
but this is not monotonic. Cone density across the retina depends upon refractive
error and eye length, aging, and the meridian of the measurement [22–24]. In the
healthy eye, longer eyes tend to have higher refractive errors, and the cones are less
dense in the macular region. It is not known if this is cause or effect: that is, whether
the eye stretched and this led to the cones being farther apart, or whether the spacing
of the cones drove the eye to a length that exceeds the refractive power of the cornea
plus lens. The density of the cone photoreceptors also depends upon the axis of
measurement, with horizontal and vertical meridians different in cone density, and
therefore the potential for color or spatial resolution. This is consistent with the visual
streak shown in classic histology studies of photoreceptor density [25,26].
The depth in the eye of the photoreceptors, while preventing the potential for
reducing scattered light from the overlying retina elements, provides the advan-
tage of metabolic support without requiring yet another layer of vascular networks
(Fig. 12.4). This need for support is largely due to the high metabolic activity of the
photoreceptors, requiring direct access to nutrients and waste material through the
RPE, directly beneath them, and the choroid that envelopes and nourishes the retina.
The inner ring of capillaries in the retina is usually displaced away from the highest
density of cone photoreceptors, and this foveal avascular zone can be mapped by a
variety of optical techniques to study the health of the eye or determine the location
of fixation [33,34].
12.1.2.2
Superficial Retinal Layers
The more anterior layers of the retina process
image information. The straight-through path from the photoreceptor to the brain is
cone or rod to bipolar cell to retinal ganglion cell. The lateral connections are provided
by the horizontal cells and the amacrine cells. The cell bodies are collected together
into densely packed nuclear layers, the outer nuclear layer for cones and rods, the
inner nuclear layer for bipolar cells and horizontal cells, and the ganglion cell layer
for retinal ganglion cells and amacrine cells, with some displacement. The individual
cells have a wide variety of interconnections, typically with the axons of one cell type
terminating on the dendrites of another, in highly elaborate spatial patterns at the outer
and inner plexiform layers. The multi-directional connections along with the highly
irregular shapes of many of the neural processes increase scatter, so that these layers
return through the pupil somewhat more light than do the layers with cell bodies
(Fig. 12.2). A sparsely distributed, specialized type of ganglion cell also contains
the photopigment melanopsin, thought to help with regulation of pupil diameter and
circadian rhythm via temporal mechanisms of adaptation with long time courses
compared to rod and cone pathways.
All the layers of the retina except the crucial outer segments of the photorecep-
tors are displaced laterally from the foveal center, resulting in less scattered light to
degrade visual acuity. The cone axons, called Henle fibers, are displaced approxi-
mately radially out of the center of the fovea. This pattern of similarly sized, tube-
shaped structures is densely and regularly packed in the healthy eye, interspersed
with Mueller cell processes, leading to form birefringence similar to that found in
crystals such as calcite. Form birefringence due to the Henle fiber layer has a specific

514
TECHNOLOGY
pattern, which is an approximately sinusoidal varying amplitude change when mea-
sured at a constant distance radially around fovea, and depends on aging and disease
status [35–38]. There is no measureable birefringence in the centralmost portion of
the fovea, since the birefringence is due to lateral axon displacement with respect to
a measurement beam entering the pupil of the eye.
12.2
MEASUREMENT TECHNIQUES
12.2.1
Scanning and Confocal Apertures
The introduction of novel laser sources has allowed the development of a variety of
measurements and diagnostic techniques [5]. This has paralleled photonic develop-
ments of method of beam scanning and detector configurations. The scanning laser
ophthalmoscope (SLO) is a general class of well-known devices that use illumina-
tion that is scanned across the target. Laser scanning provides a method of reducing
unwanted scattered in highly scattering, layered structures such as the human eye.
However, scanning of the illumination is only effective if the detection method used
removes unwanted scattered light [5, 29, 39–41]. That is, contrast is significantly
decreased if light is scattered from a feature and across the retina, often multiple
times, then eventually reaches the detector at a location corresponding to the region
of the detector meant for another feature. Several configurations accomplish this.
When the light returning from the target is relayed into a conjugate optical plane, and
an aperture is used to limit the light returning to a small focal volume, this aperture
is called a confocal aperture. This method minimizes unwanted scattered light from
adjacent target locations, as well as from other depth planes. Relative depth informa-
tion is provided, and while useful in visualizing pathology as well as normal ocular
structures, the resolution in the axial dimension is limited by the narrow entrance
of the pupil compared with the relatively long length of the eye, that is, the small
numerical aperture [42–46]. The numerical aperture (NA) of an optical system is a
unitless number that quantifies the maximum angle over which the system can collect
or emit light, for instance through a lens. NA = N sin 𝜃, where N is the refractive
index of the medium where the lens is and 𝜃is one-half of the greatest angle of the
cone of light entering or leaving through the lens. Further, the aberrated optics of the
human eye also limit the depth resolution.
A commonly used method is point scanning, that is, scan a single point of light in
a pattern, such as a raster, and allow the light returning from the target to follow the
scan path backward to the detector. This method is actually analog, not digital, since
an image must be built up over time, so that the location of each pixel is coded by
when it is acquired. The point scanning method is extremely light efficient, since the
power needed for the source is only that for one region of the image at a time. Also,
nearly the same optical path can be followed by all points in the image, making even
image illumination possible by careful optical design. The detector is most often
a single point or small array, such as an avalanche photodiode or photomultiplier
tube. Thus, modern two-dimensional (2D) detectors such as charged coupled devices
(CCD) or complementary metal oxide (CMOS) arrays have not been routinely used

MEASUREMENT TECHNIQUES
515
FIGURE 12.10
Principle of line scanning across the retina, demonstrated with a near-
infrared, confocal imaging system, the Laser Scanning Digital Camera. Left, a laser line has
begun to scan across the retina; middle, the line has scanned further along, and more of the
image has built up on the detector array; right, image near completion. The advantage of a
scanning laser as opposed to a flood system is that light scatter from other layers of the retina
can be removed using a confocal aperture that is synchronized with the timing of the scanning.
with point scanning. Instead, an image capture card, often with a computer, is needed
to build up an image. While this single-point scanning method works well when slow
speeds are needed, using a variety of motors and deflection methods, it has proven
difficult to scan in two dimensions and reach high speeds over large areas. Typically,
there is a fast scanner used in combination with a slow scanner, and the fast scanners
are typically resonant galvanometers or rotating polygon mirrors. The fast scanners
in many systems have failed to be robust over time.
Due to the limitations of scanning speed and mechanical failure, as well as the
high cost of detectors, line scanning has gained in popularity over point scanning
(Fig. 12.10) [41,47]. Line scanning is typically used in combination with a line array.
Again, this is analog imaging, not completely digital. The image must be built up,
often using an image capture card or ancillary electronics. The line scanning method
offers several of the advantages of point scanning, such as the potential to greatly
reduce unwanted scattered light, and a similar although not identical optical path for
adjacent points in the image.
Recently, hybrid imaging was introduced, in which the illumination is line scan-
ning, but the detector is a 2D array [41]. While a 2D CCD array can be used, the
sample-and-hold method of detection removes the advantage of scanning for biolog-
ical tissues that scatter light, such as the eye. Use of a confocal aperture removes this
disadvantage, but does require that a conjugate plane be added for the aperture. A
simpler method is to use the rolling shutter feature of a CMOS camera, which reads
only specific lines at a time, in synchrony with the illumination. This method can
be used to create a flexible electronic aperture, which serves the same function as a
confocal aperture, but is rapidly adjustable and requires no moving parts. This is true
digital imaging, requiring no computer to build an image.
12.2.2
Methods to Increase Contrast of Transparent and Scattering Tissues
There are several well-known methods to increase the relative contrast of tissues or
features within an image. One is the use of wavelength, and the eye has many innate
absorbers (Figs. 12.6 and 12.7) [48–50]. The use of near-infrared light has gained in

516
TECHNOLOGY
popularity because of the improved penetration into structures, the relative comfort
for patients who are photophobic, and the ready availability of sources developed
for a variety of non-biological techniques, such as the solid-state lasers used for
communication. However, visible wavelengths produce more contrast for inherent
absorbers such as blood or macular pigment. Similarly, many of the intrinsic fluo-
rophores require visible wavelength stimulation. Also, over 100 years of experience
in ophthalmic practice in being able to directly view the structures of interest, in
sharp distinction to most internal organs such as the heart or stomach, has resulted
in the resistance to imaging of ocular structures by means of unusual wavelength
combinations. The clinical recognition of features requires retraining, such as the
enhanced visibility of blood vessels in the choroid, and has set back the acceptance of
methods that might provide enhanced, albeit different, optical signatures for disease
states (Fig. 12.5). Clearly, though, since most of the naturally occurring absorbers
in the eye have broad band absorption, a careful selection of a limited number of
wavelengths provides sufficient spectral information. Two points are particularly
important in ophthalmic devices. The first point is the naturally occurring chromatic
aberration, both longitudinal (axial) and lateral (transverse), since the eye has its own
refractive elements [51]. When using short and long wavelength visible light, such
as 440 nm and 650 nm, the defocus can be more than 2.5 D. When extending into
the near infrared, the defocus is even worse. Second is the sensitivity to light of the
eye that other parts of the body do not experience, including the extreme discomfort
and potential for damage if using lights that would not cause measureable surface
warming if used on the skin.
A variety of confocal apertures and the relative positioning of the illumination and
detection optical axes have been used to increase contrast in images [29,40–42,48,52].
Apertures formed by using a mechanical stop are the most common. Similarly, a
confocal stop can be used on axis, with the position of the illumination source varying.
While this has long been known in dark field microscopy, the use of laser arrays lends
itself to ease of selection of light sources electronically. Further, as described above,
the flexible electronic aperture provides the potential for a wider range of positions
and widths of apertures. By varying the distance or timing between the illumination
light and the aperture parameters, the relative amounts of directly backscattered and
multiply scattered light is varied. The criterion for a “good” image could be to obtain
the best possible axial resolution, the brightest image, the most contrast for a specific
feature, the most extinction for unwanted superficial reflections, etc.
Analysis of polarization content provides similar advantages to apertures [53–57].
By selecting a specific set of polarization conditions, the specular reflections from
a target can be collected and the scattered light rejected, enhancing the contrast
of retinal surface features. Similarly, the specular reflections can be rejected and
the scattered light collected, enhancing the contrast of deeper retinal features or
those that scatter light. Both the cornea and retina have structures that behave like
crystals. Thus, the contrast of structures, even fairly transparent ones, can be enhanced
by processing polarization information. Many lasers are linearly polarized, or their
outputs can be made into a linearly polarized beam by the addition of the shelf
polarizers. Several sources, such as visual displays with liquid crystal technology,
use polarization elements for intensity control.

MEASUREMENT TECHNIQUES
517
Coherence amplitude and phase information can be processed to greatly enhance
contrast in a scanning laser device. Known by several names such as optical coherence
tomography (OCT) or low coherence imagine (LCI), the axial resolution is greatly
enhanced by two general classes of methods [58–62]. The older method, with advan-
tages in some applications, is the time domain method. Light scanned across the target
is compared, point by point, to a reference arm. The strength of the interference signal
depends on the index of refraction change within the focal volume in each sample.
Thus, if there is a strong interface change between two types of tissues within close
proximity, a large signal is generated. However, if the index of refraction change is
gradual, then the signal will be weaker. If the feature being measured is a pocket
of serous fluid, typical for leakage in diabetic retinopathy or age-related macular
degeneration, there is the potential for a very weak signal due to multiply scattered
light. That is, the return for this region is of low signal strength and does not report
on the specifics of the structure. The interpretation of OCT must include the idea that
the measurement is optical path length with respect to a reference, not actual length.
The OCT measurement depends upon the group index of refraction and the stability of
the target position and reference. In the moving eye, which also can have a pulse, more
rapid measurement techniques or the simultaneous measurement of two depth planes
have aided the quantitative results. When querying a range of depths with time domain
OCT, often by scanning in depth before moving laterally to the next position, the
requirement to move the reference mirror or beam limits the potential sampling rate.
Spectral domain OCT (SD-OCT) and a variety of related methods have gained
popularity because of their sensitivity and more increased speeds, reducing the arti-
facts from eye motion and heartbeat [63,64]. The beam is scanned across the target,
such as the retina or the cornea. Instead of using a reference arm to create interference
fringes, the output from the SD-OCT is spread by a dispersive element across a solid-
state line detector, so that a large range of depth is queried simultaneously. A Fourier
transform of the pixel read-out of the line detector provides, for that location, the
strength of the interference signal at each depth. Then the next location is scanned,
again with all depths queried simultaneously, rapidly building an image.
These techniques receive added benefit from further consideration of polarization
and timing. In polarization-sensitive OCT (PS-OCT), the OCT acts as depth ranging
to permit the quantification of various layers for polarization content, further reporting
on the structure within the layer for both anterior segment and retina [64–69].
12.2.3
Wavefront Aberration Measurement and Compensation
A variety of techniques use the measurement and/or correction of wavefront aberra-
tion, since the optics of the human eye limits the resolution of both vision and imaging
of ocular structures (Fig. 12.11). The principle behind wavefront measurement is that
a plane wave is aberrated by the optics of the eye. By measuring the tilt of each ray
of a set of parallel rays impinging on the cornea, the aberrations of the eye can be
computed. Taking the inverse, then a function can be computed that will undo the
aberrations, leading to more optimal vision and clearer images of the ocular structures.
Even with the contrast-enhancing techniques of the SLO, correction of wavefront
aberrations increases the contrast of features, such as retinal vessels [70]. Several

518
TECHNOLOGY
Retinal image
of point source
Retinal image
of point source
Retinal image
of point source
FIGURE 12.11
Principle of wavefront sensing. Top, parallel light rays enter the eye and are
aberrated by the optics of the eye, resulting in a point spread function that is distorted; middle,
parallel light rays are aberrated by the optics of the eye—additionally, scatter is occurring from
multiple locations in the eye including the tear film, lens, and retina, resulting in not only the
distortion and broadening of the point spread function but a degradation of contrast as well;
bottom, light returning from a point on the retina is measured, then the light impinging on the
eye has a wavefront altered to compensate for the aberrations of the eye. Measuring at controlled
locations within the pupil allows for measurement of the aberrations based upon the deflection
of each individual ray. Scatter, particularly found in older eyes, cannot be as readily corrected.
imaging devices use adaptive optics (AO) such as the AOSLO and AOOCT, benefit-
ting from wavefront correction [21,71,72]. The most common technique is to perform
a Hartman–Shack measurement of the aberrations, then use a modifiable element such
as a deformable mirror to compensate for the optics of the eye (Fig. 12.12). Clearer
images result, but the success depends upon a sufficient pupil diameter so that a strong

MEASUREMENT TECHNIQUES
519
AO imager
AO control system
Imaging
database
SH wavefront
sensor
AO control
loop
Deformable
mirror
SLO optics
resonant
scanners
SLO
steering
mirrors
SLO raster
generation
aperture controls
gain controls
Frame
grabber/
AVI file
generation
Detector
FIGURE 12.12
Schematic of an AO imaging system that uses a Hartman–Shack wave-
front measurement to guide the compensation for the optical aberrations of the eye. Top left,
schematic of the imaging subsystem, which incorporates scanning of the illumination across
the target, collection of light by a detector, storing of data using a computer and frame grabber,
and image processing to convert the incoming datastream into an image; top right, schematic
of the AO control system, which includes a closed loop measurement of the wavefront, in a
continuous manner. A grid pattern of light sources, generated with a lenslet array, is focused
onto the retina. The light returning from the retina is collected in the pupil plane, using a
detector that is separate from the retinal imaging channel. A centroiding algorithm is used
to guide the positioning of a deformable mirror to correct the optical aberrations. With the
aberrations corrected continuously, the AOSLO imaging channel collects the high resolution
image. Bottom left, Hartmann–Shack grid representing a well-focused system; bottom right,
Hartmann–Shack grid of a poorly focused system of an individual eye, prior to compensation
with a deformable mirror.

520
TECHNOLOGY
(a) AO off
(b) AO on
(d) NFL
(c) Cones
FIGURE 12.13
AOSLOimagingof humancones, showingtheseparateeffects of wavelength
correction and depth of focus. (a) Retinal image with adaptive optics, the deformable mirror,
turned off. (b) Retinal image with adaptive optics turned off. Note that the resolution of cones
is significantly improved as compared to (a). (c) Image of the cone photoreceptors adjacent to
a blood vessel, seen as a dark shadow. (d) Image of the nerve fiber layer and blood vessel from
the same location as (c). The retinal nerve fiber layer is the streaks running through the image.
Image (d) is taken with the focus more anterior than (c).
signal can be obtained (Fig. 12.13). For practical operation of the AOSLO, many lab-
oratories have turned to a larger stroke “woofer” mirror and a fine control “tweeter”
mirror, so that the limited range of the deformable mirror is not spent correcting the
optical power needed to correct for defocus and astigmatism. For usability, the sys-
tem does not necessarily require the largest possible pupil for routine measurements,
allowing measurements in the older eyes that have small pupils despite dilatation. A
range of AOSLO configurations includes a system with no measurement and using
only image intensity or contrast/frequency content to adjust the mirror, a wavefront
AO control system based on the imaging beam, or a totally separate wavelength and

MEASUREMENT TECHNIQUES
521
AO control imaging system. It is important also to have the potential to focus down
to deeper layers, while holding the mirror correction. The guidance properties of the
cones result in a strong signal in the photoreceptors layers, and this must be overcome
to image other tissues.
12.2.4
Enhancing Contrast by Wavelength, Dye, and Temporal Modulation,
and Image Processing
Ophthalmic imaging devices including film cameras, digital cameras, and SLOs,
have used contrast enhancing dyes, primarily via fluorescence imaging with sodium
fluorescein and indocyanine green [73–77]. These techniques help to document the
structure of blood vessels, fluid leakage, or damaged tissues that stain with dye.
Sodium fluorescein is commonly used for both anterior segment, applied topically,
and posterior segment, injected or ingested. The optimal excitation wavelength is
roughly Argon blue, or about 488 nm, with emission broadband but within visi-
ble wavelengths. Thus, it emphasizes more superficial structures, even if there is
potential damage deeper. The dye diffuses readily, further masking deeper pathology.
Indocyanine green leaks out of vessels less readily, and is excited by near-infrared
wavelengths. This makes for an improved visualization of the deeper pathological
structures. Either can be used to document small vessels, such as capillaries, and
improved imaging is obtained by the contrast-enhancing features of SLO imaging.
Autofluorescence techniques, including the detection of intrinsic fluorescence such
as the waste product liposfuscin, are also used to diagnose retinal degenerations.
Several types of solid-state lasers have become commonplace for both imaging
and treatment, varying in wavelength, temporal properties such as pulse rate and
duration, and beam configuration, but price is always a consideration [5,78–81]. The
ready modulation of beam power and position makes possible structured illumination.
Of particular interest in recent years have been lasers with long coherence lengths
such as superluminescent, supercontinuum, or vertical cavity surface emitting lasers,
which help to reduce speckle noise. Also, swept sources are being developed at more
affordable costs, which provide a wide range of frequency information without the
expense of additional modulation or detection schemes that analyze frequency [5].
Illumination sources are not limited to lasers, with light emitting diodes (LED), liquid
crystal, digital light projectors, and other solid-state sources serving as illumination
sources for imaging, visual stimulus generation, visual aid or assistive devices, and
treatment devices. The light safety limits for the human eye are highly regulated, with
the ANSI standards the most commonly used in the US [82]. Short-wavelength light
is considered particularly damaging, having both thermal and photochemical damage
mechanisms.
A wide variety of image processing techniques have enhanced optical measure-
ments. A key example is the improvement of the quantification of the parameters of
the clear, curved cornea (Fig. 12.14) [83]. By processing changes over time, often
called 4D imaging, elements such as blood flow are obtained from both AOSLO and
SDOCT, mapping velocities and locations of the smallest vessels in the living eye.

522
TECHNOLOGY
Processing/quanfification algorithms for anterior segment OCT images
Input data: OCT images
Segmentation
Surface
denoising
Full 3D distortion
correction
Output data: ocular geometry
and biometry
3D Merging
0
0
5
10
15
0.2
0.4
0.6
0.8
1
Clustering
Denoising
Statistical thresholding
FIGURE 12.14
Image processing steps to obtain quantitative full 3D geometry of the anterior
segment of the eye from OCT images. Image courtesty of Sergio Ortiz and Susana Marcos,
Instituto de Optica (CSIC), Madrid, Spain. (For a color version of this figure, see the color
plate section.)
12.3
ANTERIOR SEGMENT DIAGNOSTICS, REFRACTIVE
MEASUREMENTS, AND TREATMENT
12.3.1
Cornea and Tear Film
The importance of photonics in the anterior portion of the eye can largely be divided
into two categories which are imaging to detect disease and the use of lasers for treat-
ment of diseases/disorders [84, 85]. As previously stated, the tear film is composed
of three components, which are essentially oil, water, and mucin of which a failure in
any component will result in problems for the entire tear film stability. This results in
a variety of manifestations of dry eye that are characterized by decreased vision and
eye irritation which in some forms such as Sjogren’s syndrome, an auto-inflammatory
disease, can be crippling. These three components are produced in different areas of
the eye. The oil layer is largely produced in glands on the back side of the eye lid.
Infrared light, usually in the form a scanning laser, can be used to image these glands
to determine whether the glands are functioning. Two examples (Fig. 12.15) of normal
Meibomian glands on the left and truncated glands on the right. Infrared is required
for greater penetration as the glands themselves are imbedded in tissue that is highly
scattering [86]. The oil layer itself, as it generally sits on top of the other layers, offers

ANTERIOR SEGMENT DIAGNOSTICS, REFRACTIVE MEASUREMENTS
523
FIGURE 12.15
Infrared meibomography using a laser scanning camera comparing two eyes.
(a) Lower lid of a normal subject showing normal meibomian gland that produce oil/lipid for
the tear film. Notice the white glands extend the entire length of the lower eye lid. (b) Lower lid
of a dry eye subject showing truncated and poorly functioning glands. Notice that the glands
do not extend the full length of the lid. Image courtesy of Donald R Korb (Tear Sciences,
Boston, MA).
a unique opportunity to use interference to measure the oil layer thickness. Either
using a broad beam source to examine chromatic interference patterns, or by using
multiple different wavelengths, an estimate of the oil layer can be made.
Fluorescent dye and excitatory light can also be used to examine the tear film
[84]. The most common type of dye used for this is fluorescein which is easily
dissolved in water (or tears) and excites between about 490 nm and 520 nm. After
instillation, a cobalt filter is used to excite the dye, and then images are taken. As the
tear film destabilizes, large black areas begin to form that show areas of decreased
tear film thickness. This can be seen in Figure 12.16 with the green areas marking
normal tear film and the black spots marking thinned or absent tear film. Long-term
tear film instability and other things like bacterial infections can damage the corneal
epithelium. By using these same dyes, damage can also be tracked to the cornea
which can include a complete break in the epithelium resulting in dye being visible
within the cornea stroma.
The primary clinic method of examining the cornea, as well as virtually any other
structure that is visible in the eye, is through the use of slit lamp microscopes. It can be
thought of primarily as a two channel system, with the first channel being a standard
binocular microscope that allows for exchange of lenses to change magnification and
allow for viewing of other structures such as the retina. The second channel is the
light channel which allows the beam to be adjusted from a slit to a flood source and
allows for insertion of filters such as cobalt and red free filters. With this flexible
system, most parts of the eye can be imaged. The normal cornea (Fig. 12.17a) is
clear and reflects light in a uniform pattern, while a cornea with inflammation and
unwanted new blood vessels scatters light diffusely (Fig. 12.17b).
The cornea curvature is also important to know as it is the primary source of
focusing power for the eye, with a curvature of approximately 8.4 mm (40 diopters)
[87]. This curvature, often referred to as corneal topography, is important for detect-
ing astigmatism, monitoring for diseases of the cornea that cause irregular optics
and for guiding treatment strategies such as refractive surgery. The most common
method for determining corneal curvature is through the use of placido systems. In a

FIGURE 12.16
Fluorescein dye used to measure tear film stability and breakup time. Dark
patches within the brighter slit area represent areas of no or little tear film. The green fluorescent
dye is being excited by a cobalt filter. Image courtesy of Todd Peabody (Indiana University,
Bloomington, IN). (For a color version of this figure, see the color plate section.)
FIGURE 12.17
Slit lamp biomicroscope imaging of the anterior eye. (a) Normal eye showing
illumination of the cornea, iris, and lens. The cornea is the slightly hazy slit to the left. Within
the pupil, one can see part of the lens as it scatters light back at the microscope. (b) New
unwanted blood vessel growth onto the cornea due to excessive wearing of a contact lens.
The slit beam is illuminating the blood vessels. Image courtesy of Todd Peabody (Indiana
University, Bloomington, IN). (For a color version of this figure, see the color plate section.)

ANTERIOR SEGMENT DIAGNOSTICS, REFRACTIVE MEASUREMENTS
525
FIGURE 12.18
Corneal topography of normal and astigmatic eyes. (a) Infrared image
capture of the cornea with illuminated placido ring reflecting on the cornea. (b) Topographic
map created from data collected in (a). This is a normal cornea exhibiting a spherical shape.
Red colors represent more corneal curvature and blue colors represent less curvature. Normal
corneas begin steeper centrally and flatten farther from the center. (c) Astigmatic corneal
topography showing a traditional bow tie pattern. Note that the redder colors representing more
curvature are oriented vertically while the flatter areas are horizontal (Medmont Topographer,
Medmont, Rousehill, Australia). (For a color version of this figure, see the color plate section.)
placido-based system, a known pattern of light, typically in the form of rings, is
projected onto the cornea as seen in Figure 12.18a. An image is then captured and
the distance between points between the rings gives a measure of localized curvature.
These points are then combined into a topographic map of the cornea curvatures.
Normal spherical corneas display a symmetric curvature that is steeper centrally
and flattens peripherally (Fig. 12.18b). Conversely, corneas with astigmatism display
steepening in one meridian as opposed to the other (Fig. 12.18c). Inherently, better
systems place greater numbers of rings on the cornea which then increases the
resolution. One drawback to this is that if two rings intersect or cross one another, a
failure to detect the curvature will occur. Also, as in virtually every use of photonics
in the eye, dry eye, caused by poor tear film, will decrease the accuracy of the system.
The Scheimpflug principle can also be used to measure the corneal topography
[85]. The principle is a geometric optics rule that measures the image plane while the
optical system is off axis. In this system, a light is shown off axis at a known angle
and swept across the cornea. Using the known angle and the cornea’s position in the
light beam, the curvature of the anterior cornea can be calculated. In addition, this
method can also provide the thickness of the cornea, the posterior curvature of the
cornea, and the anterior chamber depth. While Scheimpflug-based systems provide
more information than placido-based systems, they tend to be less accurate than
placido systems as eye movement can badly distort the data in addition to also having
problems with dry eye. OCT is becoming more frequently used to image the anterior
segment including the cornea (Figs. 12.2, 12.14, and 12.19). This is largely due to the

526
TECHNOLOGY
FIGURE 12.19
Anterior segment optical coherence tomography (OCT) showing the cornea,
anterior chamber, iris, and ciliary body. (a) Close up of the cornea in a normal eye for measuring
corneal thickness. (b) Anterior segment OCT for measuring anterior chamber depth. (c) Ciliary
body imaging. Image courtesy of Peter Kolbaum and Meredith Jansen (Indiana University,
Bloomington, IN), using a Visante (Zeiss, Jena, Germany).
fact that OCT has higher resolution than other topography systems. Typical systems
use near-infrared light to allow for greater penetration into the sample. By using
interference fringes reflecting off the layers of the cornea, curvature and thickness
can be measured. One drawback with OCT at the present time is the lack of a wide
field of view in comparison to reflective light systems like placido-based systems.
12.3.2
Anterior Chamber, Ciliary Body, and Lens
Photonic methods of measuring the anterior chamber, ciliary body, and lens are largely
dependent upon the slit lamp microsope, OCT [88–90] (Fig. 12.19), and Scheimpflug
cameras. This is largely due to the fact that objects being examined are either
clear/fluid filled which means they neither reflect nor scatter light or they are obscured
by the iris which prevents penetration by many methods. These structures have a high
level of interest as conditions such as cataracts and glaucoma are influenced by them.
The anterior chamber depth or size influences the pressure of the eye. As the
anterior chamber gets smaller, it increases aqueous outflow resistance which causes

ANTERIOR SEGMENT DIAGNOSTICS, REFRACTIVE MEASUREMENTS
527
the eye’s pressure to go up and increase the risk for glaucoma. This is largely due to the
fact that the aqueous humor that is produced by the ciliary body leaves the eye at the
intersection of the cornea and iris known as the angle. OCT and Scheimplug, using
the same principles previously talked about, are excellent at measuring this depth.
It is especially important as the anterior chamber decreases in depth throughout a
person’s life, largely due to the fact that the lens continues to grow and take the space.
12.3.3
Refraction
As previously stated, refractive error, which can be thought of a defocus, happens
when the axial length of the eye mismatches with the cornea and lens focal point.
To measure refractive error, an enormous number of techniques have been developed
that all require light in some form. The most commonly used systems today are
auto-refractors and Hartmann–Shack aberrometers, which both employ infrared light
to not induce pupil changes and accommodation.
Auto-refractors function essentially by taking a paraxial light beam approximately
1 mm in size and projecting it into the center of the pupil. A focusing system then
is used to minimize the returning point spread function. Due to the narrowness of
the beam, an auto-refractor only measures the refractive error within a small area of
the pupil. Aberrometers (Figs. 12.11 and 12.20) focus a small point of light onto the
retina and then measure the returning light across the pupil to determine the refractive
error. Both systems can be confounded by the eye’s innate focusing system, that is,
accommodation. Auto-refraction is largely useful for most measures of refraction in
the eye, as defocus and astigmatism account for nearly 90% of the total aberrations
in the eye. Aberrometry becomes important largely to drive some surgical refractive
lasers, control adaptive optics systems, and perform research.
Axial length measures have become important largely in the cataract field as the
measurement is critical to inserting the correct lens into the eye following cataract
lens extraction. Recent optical methods such as partial coherence interferometry
FIGURE 12.20
Wavefront across the pupil of two human subjects. (a) Normal wavefront
showing little variability across the pupil represented by a uniform color across the image.
(b) Subject with keratoconus, showing high levels of aberrations across the pupil represented
by large color swings across the pupil. (For a color version of this figure, see the color plate
section.)

528
TECHNOLOGY
(PCI) have allowed for accurate measures of the axial length of the eye which has
a high level of variability (average length = 23.5 mm). An error of only 0.30 mm
would mean the difference between being legal as opposed to not legal to drive
without glasses in most states. PCI, similar to OCT, uses interference reflections to
determine lengths or depths.
12.3.4
Light and Treatment of Anterior Segment Diseases
The uses of light in the anterior eye are generally limited to the use of lasers with one
exception. In all cases, they are used to remove tissue in one form or another. One of
the more commonly used lasers is the frequency-doubling Nd:YAG laser, wavelength
532 nm. The frequency-doubling aspect offers a high degree of precision in the
axial direction that is very useful for surgery; specifically the axial resolution limits
unwanted damage to tissue behind or in front of the spot of surgery. It is primarily
used for two purposes in the anterior eye, the first being the removal of unwanted
growth tissue following cataract surgery. Post cataract surgery, cell ingrowth along
the back side of the new lens can obscure vision resulting in what is commonly
called posterior capsular opacification [85]. The second use of Nd:YAG lasers in the
anterior eye is to create holes in the iris to allow for fluid flow. In some situations,
highly correlated with shallow anterior chamber depth, the iris and lens together can
prevent aqueous humor from flowing into the anterior chamber and out of the eye. As
the production of this fluid does not decrease in response to the blockage, pressure
can build to danger levels that would blind a person in a few days. This is generally
accompanied with a great deal of pain and is called angle closure glaucoma. Laser
can become useful in this situation by creating new holes in the iris to relieve the
pressure and allow for normal fluid flow in the eye.
Angle closure glaucoma is relatively rare compared to primary open angle glau-
coma which has a much higher prevalence. Primary open angle glaucoma employs
two lasers, often in coordination with drug treatment. The angle itself, as people ages,
increases outflow resistance and thereby increases pressure within the eye. These two
lasers target this as a treatment mechanism. The first of these lasers is the argon laser
trabeculoplasty (ALT). It uses heat damage aimed at the angle to create scar tissue.
This scar tissue contracts, as all scars do, and thereby decreases outflow resistance by
pulling tissue wider. The second laser treatment method employs a laser that relies
less on thermal damage, and is called selective laser trabeculoplasty (SLT). The laser
wavelength is typically longer than that found in an Argon laser, and the parameters
of delivery such as the energy density and pulse or other timing parameters, have
been selected to make pigment cells within the angle die. This leads to a cascade
employing normal biological housekeeping cells to remove the dead cells and any
unwanted material within the angle, again reducing decreasing outflow resistance.
Due to the necessity of targeting pigment cells, individuals with more pigment tend
to have better treatment than those with low pigment.
The most common use of lasers in the eye, regardless of anterior or posterior, is
to correct refractive error [91]. There are a number of different methods to start the
procedure such as Laser-Assisted in situ Keratomileusis (LASIK), Laser Epithelial

DIAGNOSTIC APPLICATIONS AND TREATMENT OF POSTERIOR SEGMENT
529
Keratomileusis (LASEK), and Photorefractive Keratectomy (PRK), but they all gen-
erally use an excimer laser to remove tissue from the corneal stroma. The underlying
principle of this is to reshape the curvature of the cornea to optimally focus the image
on the retina. As the laser can only remove tissue, that is, it cannot put on tissue, it
is optimally suited for correcting refractive error resulting from too much power in
the eye. In simple terms, it is better at treating refractive error where the image is
focused in front of the retina like in myopia/nearsightedness. All these procedures
require close attention to curvature and thickness of the cornea.
Recently, ultra short pulse lasers have become popular in eye care as they offer
unique advantages of other lasers and surgical techniques. Historically, refractive
surgery procedures such as LASIK used a surgical blade to cut a flap in the cornea to
retain the epithelium. This allowed for quicker and less painful recovery. However, the
speed of the blade, the movement of the patient’s eye, and potential mechanical errors
can cause surgical complications. The use of femtosecond lasers has reduced these
complications, though visual outcomes remain similar. A new procedure, in which
a gap is made rather than a mechanical or laser-induced cut, provides improved
retention or regrowth of corneal nerves, as seen with corneal microscopy [92].
Similar to LASIK, these lasers are beginning to be employed in cataract lens
extraction. An incision is made to gain access to the lens to cut or break up the
material for removal. The clear corneal incision made by a femtosecond laser, rather
than by a microtome, may provide less gaping of the wound or misalignment of
layers [93] For most of the recent past, ultrasound has been used in eyes with clouded
lenses to break up the lens and then remove it safely. However, the length of time
the ultrasound was used increased the complications. Femtosecond lasers are now
being used to break apart the lens more efficiently, thereby decreasing complications.
The procedure is relatively new, and therefore whether the improvement is made
on a variety of outcome measures is not yet determined. For instance, when the
retina is examined using SD-OCT after femtosecond cataract surgery, there was no
improvement in the amount of edema induced by the surgery [94].
The only non-laser treatment currently used in the eye is used to strengthen the
cornea (cross-linking) [95]. A number of diseases such as keratoconus, which is a
weakened collagen fibrils resulting in a distorted cornea, need treatments to strengthen
the cornea, thereby providing stable vision. To do this, a riboflavin solution is placed
on the cornea after having the epithelium removed. Following that, UV-A light
illuminates the cornea and solution for approximately 30 minutes. The result causes
the collagen fibrils to cross-link with one another, and strengthening the cornea.
12.4
DIAGNOSTIC APPLICATIONS AND TREATMENT
OF POSTERIOR SEGMENT
12.4.1
Imaging Applications
As described above, the field of diagnostic imaging is benefitting from novel light
source and novel detection schemes that enhance the detection of the weak signals
that return from the interior of the eye. The status of the retinal nerve fiber layer,

530
TECHNOLOGY
FIGURE 12.21
AOSLO-montaged nerve fiber bundles and blood vessels. Nerve fiber layer
is observable as white streaks across the image. Image courtesy of Stephen Burns (Indiana
University, Bloomington, IN).
the key target of glaucoma detection but also damaged by diabetic retinopathy, is
accessible with AOSLO (Fig. 12.21), SDOCT (Fig. 12.22), and AOOCT.
The status of the smallest blood vessels in the human retina, the capillaries,
are readily seen with AOSLO (Fig. 12.23), including the foveal avascular zone, and
AOOCT. The damage within layers by diabetes is readily seen (Fig. 12.22, right), even
if it is too subtle to cause distortion of the layers that result in measurably increased
retinal thickening. The capillaries that form outpouchings, the microaneuryms that
are one of the hallmarks of diabetic retinopathy, are visualized both by reflectance
imaging and by the variance mapping that uses the change in reflectivity over time
FIGURE 12.22
Near-infrared SLO and en face SD-OCT images of a diabetic eye. Left: En
face image of the retina. Although there is a central lens reflection, the other reflective, whitish
spots represent focal lesions, due to diabetic retinopathy. Right: Localized area of edema
corresponding to the reflectivity changes in the left image, distorting the layers of the retina
and with some lesions appearing as holes. As these areas are fluid filled, the scatter reduces
the change of coherence, and the decreased signal amplitude at these locations makes them
appear black, although they are not actually holes. The normal blood vessels in the choroid
beneath the retina also appear dark, due to the local scatter.

DIAGNOSTIC APPLICATIONS AND TREATMENT OF POSTERIOR SEGMENT
531
FIGURE 12.23
A 3×3◦map of the standard error of the gray scale over time at each pixel,
showing the complete foveal microvascular network in a 54-year-old male (right eye). Asterisk
indicates the foveola. Scale bar = 100 μm. Image courtesy of Stephen Burns and Toco P Chui
(Indiana University, Bloomington, IN).
to map perfusion (Fig. 12.24) [96–101]. Structures within retinal vessels, such as the
lumen, are also visualized in the living human eye (Fig. 12.25).
The structure of the retinal capillaries can now be mapped over using in vivo
techniques over a wide enough retinal area to pinpoint ischemia, with both AOSLO
and OCT techniques. The technical advancement that underlies the AOSLO capillary
method is using slightly offset confocal apertures to enhance the signal from scattered
light rather than directly backscattered light [101]. When retinal images are collected
serially, and the change over time is analyzed, clear-cut capillary images are obtained.
For OCT techniques, one of the important advancements is the increase in speed of
image acquisition, to allow for scans to be collected in a dense enough pattern to allow
comparison of neighboring retinal locations. The use of SD-OCT is one method, but
swept source OCT (SS-OCT) has achieved even higher acquisition speeds by use of
a high speed analog to digital converter [102].
The high image acquisition rates with SD-OCT, and even ones with SS-OCT,
allow trade-offs of the digital resolution, that is, more pixels, and the bit depth,
that is, more dynamic range at each pixel. This has increased diagnostic efficiency,
along with the use of novel sources like swept source lasers based on Vertical Cavity
Surface Emitting Lasers (VCSEL) and optics to image deeper vascular pathologies,
as well as the anterior segment and posterior segments in one device [103–107].
Another aspect of increased speed is the potential for improved computations, such

532
TECHNOLOGY
FIGURE 12.24
AOSLO image of diabetic retinopathy. (a) AOSLO reflectance image in
a 49-year-old diabetic patient with moderate diabetic retinopathy, showing microaneurysms
(black arrows) of varying sizes. Black arrows indicate microaneurysms. (b) The corresponding
perfusion map for the location in (a), derived from measuring the standard error of the gray
scale over time, rather than averaging the data as in (a). Scale bars = 50. Image courtesy of
Stephen Burns and Toco P Chui (Indiana University, Bloomington, IN).
as using the phase information to obtain blood flow or improving the computations
of polarization-sensitive OCT (PS-OCT).
The interpretation of the results from advanced imaging, often showing struc-
tures never before visualized in vivo, continues to evolve [99, 101, 103]. Clinico-
pathological comparisons, along with animal models and multi-modality imaging
studies, provide methods for interpreting the new imaging modality results [108–110].
The advanced imaging methods do have cost considerations. Efforts are underway
to reduce both the cost of the SLO, making a hybrid imaging device, the Laser
Scanning Digital Camera [41], and the cost of AO-OCT (Fig. 12.26). One key goal
FIGURE 12.25
Adaptive optics image of the retinal vasculature. Artery imaged using
AOSLO, resolving the blood vessel walls and interior of the vessel. Image courtesy of Stephen
Burns and Toco P Chui (Indiana University, Bloomington, IN).

DIAGNOSTIC APPLICATIONS AND TREATMENT OF POSTERIOR SEGMENT
533
FIGURE 12.26
Adaptive optics assisted optical coherence tomography (AOa-OCT). 9◦
long OCT B-scans acquired horizontally, comparing two conditions for imaging the central
fovea of the same human subject. The images contain single data sets and no averaging was
applied. (a) Standard OCT image obtained with a 1.2 mm beam diameter (1∕e∧2). For the
standard OCT image, defocus was corrected with a Badal optometer. The subject’s refrac-
tive error with an autorefractor was −2.5D sphere, −1.25D cylinder at 1◦axis. (b) Adaptive
optics assisted optical coherence tomography (AOa-OCT) image obtained with a 3.4 mm beam
diameter (1∕e∧2). The AOa-OCT system used adaptive optics to achieve diffraction-limited
performance. Each B-scan measures 1000 A-scans, and the data acquisition rate was 48,000
A-scans per second. The white bars indicate lengths of 100 μm in tissue. Data were gray
scale-encoded over 38 dB above the noise floor. (c) The central part of (a) and (b) magni-
fied, showing a 2.75 times smaller speckle size and higher reflectivity for the AOa-OCT data.
(d) Data of a second subject (autorefractor: −0.50 sphere; −0.50 cylinder at 180◦) demonstrat-
ing similar differences between the two modalities. Image courtesy of Barry Cense and Kenta
Sudo (Utsunomiya University) and Kazuhiro Kurokawa and Yoshiaki Yasuno (Computational
Optics Group, University of Tsukuba), Japan.

534
TECHNOLOGY
is to bring the cost of the entire process of diagnostics to a sufficiently low level that
all individuals receive screening for key eye diseases, such as diabetic retinopathy
[111]. At present, one of the barriers to eye care that would prevent sight-threatening
disorders is cost.
The measurement of visual function continues to be a part of ophthalmic tech-
nologies, not only for diagnosis of disease, but also for assessing function such as
in visual rehabilitation. Using imaging with an SLO or other fundus imaging device
provides the potential for more accurate mapping of retinal sensitivity, either by
providing simultaneous imaging of the retina or improvement of the generation and
delivery of the stimulus so that all the light enters the pupil [112–120]. Although
lasers per se were used previously in these devices, a range of stimulation devices,
such as liquid crystal monitors and digital light projectors, can be combined with the
fundus imaging device. By uncoupling the spatial resolution of the fundus imager and
the display device, a much wide range of colors and temporal patterns is obtained,
which are needed to probe the visual system. There has been increased interest in the
optical measurement of neural responses, and the interpretation of these may provide
an objective means of assessing retinal function.
Phototherapy has been used almost since the invention of the laser [121]. Treatment
of the retina for the uncontrolled growth of new blood vessels is a chief reason to
use photocoagulation. While originally the photocoagulation was performed without
enhancers, now a range of absorbing and molecules with biological strategies are
used to enhance the treatment. In addition, surgical procedures to secure detached
tissue are performed with lasers. The performance now is directed toward targeting
specific tissues, either by more accurate localization across the retina or by altering
the structure of a limited number of layers.
REFERENCES
[1] A. Tasman and E. A. Jaeger, Duane’s Foundations of Clinical Ophthalmology
(J.B. Lippincott, 1991).
[2] C. W. Oyster, The Human Eye: Structure and Function (Sinaurer Associates, Sunder-
land, MA, 1999).
[3] J. E. Dowling, The Retina: An Approachable Part of the Brain (Harvard University
Press, Cambridge, 1987).
[4] AE Elsner, in Handbook of Adaptive Optics, edited by J. Porter, H. Queener, J. Lin,
K. Thorn, and A. Awwal (John Wiley & Sons, New York, 2006), pp. 205–234.
[5] A. E. Elsner and M. S. Muller, “Laser applications and system considerations in ocular
imaging,”Laser Photon Rev. 2(5),350–376 (2008).
[6] B. Winn, D. Whitaker, D. B. Elliot, and N. J. Phillips, “Factors affecting light-adapted
pupil size in normal human subjects,” Invest. Ophthalmol. Vis. Sci. 35(3),1132–1137
(1994).
[7] Y. Yang, K. Thompson and S. A. Burns, “Pupil location under mesopic, photopic, and
pharmacologically dilated conditions,” Invest. Ophthalmol Vis. Sci. 43(7), 2508–2512
(2002).

REFERENCES
535
[8] J. C. He, S. Marcos, R. H. Webb and S. A. Burns, “Measurement of the wave-front
aberration of the eye by a fast psychophysical procedure,” J. Opt. Soc. Am. A. Opt.
Image Sci. Vis. 15(9), 2449–2456 (1998).
[9] J. C. He, S. A. Burns, and S. Marcos, “Monochromatic aberrations in the accommodated
human eye,” Vision. Res. 40(1), 41–48 (2000).
[10] E. J. Sarver, J. Schwiegerling, and R. A. Applegate, “Extracting wavefront error from
Shack-Hartmann images using spatial demodulation,” J. Refract. Surg. 22(9), 949–953
(2006).
[11] L. N. Thibos, R. A. Applegate, J. T. Schwiegerling, and R. Webb, “Standards
for reporting the optical aberrations of eyes,” J. Refract. Surg. 18(5), S652–S660
(2002).
[12] J. S. McLellan, S. Marcos, and S. A. Burns, “Age-related changes in monochromatic
wave aberrations of the humage eye,” Invest. Ophthalmol. Vis. Sci. 42(6), 1390–1395
(2001).
[13] S. A. Burns, S. Wu, F. Delori, and A. E. Elsner, “Direct measurement of human-cone-
photoreceptor alignment,” J. Opt. Soc. Am. A. Opt. Image Sci. Vis. 12(10), 2329–2338
(1995).
[14] A. E. Elsner, S. A. Burns, L. A. Lobes Jr., and B. H. Doft, “Cone photopigment bleaching
abnormalities in diabetes,” Invest. Ophthalmol. Vis. Sci. 28(4), 718–724 (1987).
[15] S. A. Burns, S. Wu, J. He, and A. E. Elsner, “Variations in photoreceptor directionally
across the central retina,” J. Opt. Soc. Am. A. Opt. Image Sci. Vis. 14(9), 2033–2040
(1997).
[16] Centers for Disease Control and Prevention. National Diabetes Fact Sheet: National
Estimates and General Information on Diabetes and Prediabetes in the United States,
2011 (U.S. Department of Health and Human Services, Centers for Disease Control and
Prevention, Atlanta, GA, 2011).
[17] A. E. Elsner, J. Pokorny, and S. A. Burns, “Chromaticity discrimination: effects of
luminance contrast and spatial frequency,” J. Opt. Soc. Am. A. 3(7), 916–920 (1986).
[18] A. E. Elsner, S. A. Burns, and J. Pokorny, “Changes in constant hue loci with spatial
frequency,” Color. Res. Appl. 12(1), 42–50 (1987).
[19] A. E. Elsner, M. C. Cheney, A. Weber, and M. Miura, “Visualization of two image
variables simultaneously using cardinal directions of color vision,” Stud. Health Technol.
Inform. 98, 89–91 (2004).
[20] A. Roorda and D. R. Williams, “The arrangement of the three cone classes in the living
human eye,” Nature 397(6719), 520–522 (1999).
[21] D. R. Williams, “Imaging single cells in the living retina,” Vision. Res. 51(13), 1379–
1396 (2011).
[22] T. Y. Chui, H. Song, and S. A. Burns, “Individual variations in human cone photoreceptor
packing density: variations with refractive error,” Invest. Ophthalmol. Vis. Sci. 49(10),
4679–4687 (2008).
[23] H. Song, T. Y. Chui, Z. Zhong, A. E. Elsner, and S. A. Burns, “Variation of cone
photoreceptor packing density with retinal eccentricity and age,” Invest. Ophthalmol.
Vis. Sci. 52(10):7376–7384 (2011).
[24] T. Y. Chui, H. Song, C. A. Clark, J. A. Papay, S. A. Burns, and A. E. Elsner, “Cone
photoreceptor packing density and the outer nuclear layer thickness in healthy subjects,”
Invest. Ophthalmol. Vis. Sci. 53(7), 3545–3553 (2012).

536
TECHNOLOGY
[25] C. A. Curcio, K. R. Sloan Jr., O. Packer, A. E. Hendrickson, and R. E. Kalina, “Distribu-
tion of cones in human and monkey retina: individual variability and radial asymmetry,”
Science 236(4801), 579–582 (1987).
[26] C. A. Curcio, K. R. Sloan, R. E. Kalina, and A. E. Hendrickson, “Human photoreceptor
topography,” J. Comp. Neurol. 292(4), 497–523 (1990).
[27] S. A. Burns, A. E. Elsner, L. A. Lobes Jr., and B. H. Doft, “A psychophysical technique
for measuring cone photopigment bleaching,” Invest. Ophthalmol. Vis. Sci. 28(4), 711–
717 (1987).
[28] A. E. Elsner, L. Berk, S. A. Burns, and P. R. Rosenberg, “Aging and human cone
photopigments,” J. Opt. Soc. Am. A. 5(12), 2106–2112 (1988).
[29] A. E. Elsner, S. A. Burns, G. W. Hughes, and R. H. Webb, “Reflectometry with a
scanning laser ophthalmoscope,” Appl. Opt. 31(19), 3697–3710 (1992).
[30] A. E. Elsner, S. A. Burns, and R. H. Webb, “Mapping cone photopigment optical
density,” J. Opt. Soc. Am. A. 10(1), 52–58 (1993).
[31] A. E. Elsner, S. A. Burns, E. Beausencourt, and J. J. Weiter, “Foveal cone photopigment
distribution: small alterations associated with macular pigment distribution,” Invest.
Ophthalmol. Vis. Sci. 39(12), 2394–2404 (1998).
[32] A. E. Elsner, S. A. Burns, and J. J. Weiter, “Cone photopigment in older subjects:
decreased optical density in early age-related macular degeneration,” J. Opt. Soc. Am.
A. Opt. Image Sci. Vis. 19(1), 215–222 (2002).
[33] A. Bradley, H. Zhang, R. A. Applegate, L. N. Thibos, and A. E. Elsner, “Entoptic image
quality of the retinal vasculature,” Vision. Res. 38(17), 2685–2696 (1998).
[34] T. Y. Chui, Z. Zhong, H. Song, and S. A. Burns, “Foveal avascular zone and its relation-
ship to foveal pit shape,” Optom. Vis. Sci. 89(5), 602–610 (2012).
[35] A. E. Elsner, A. Weber, M. C. Cheney, and D. A. VanNasdale, “Spatial distribution of
macular birefringence associated with the Henle fibers,” Vision. Res. 48(26), 2578–2585
(2008).
[36] D. A. VanNasdale, A. E. Elsner, T. Hobbs, and S. A. Burns, “Foveal phase retar-
dation changes associated with normal aging,” Vision. Res. 51(21–22), 2263–2272
(2011).
[37] A. Weber, A. E. Elsner, M. Miura, S. Kompa, and M. C. Cheney, “Relationship between
foveal birefringence and visual acuity in neovascular age-related macular degeneration,”
Eye (London) 21(3), 353–361 (2007).
[38] D. A. VanNasdale, A. E. Elsner, A. Weber, M. Miura, and B. P. Haggerty, “Determi-
nation of foveal location using scanning laser polarimetry,” J. Vis. 9(3), 21.1–21.17
(2009).
[39] A. Elsner, L. Moraes, E. Beausencourt, A. Remky, J. Weiter, J. Walker, G. Wing,
S. Burns, P. Raskauskas, and L. Kelley, “Scanning laser reflectometry of retinal and
subretinal tissues,” Opt. Express 6(13), 243–250 (2000).
[40] A. Elsner, M. Miura, S. Burns, E. Beausencourt, C. Kunze, L. Kelley, J. Walker, G.
Wing, P. Raskauskas, D. Fletcher, et al., “Multiply scattered light tomography and
confocal imaging: detecting neovascularization in age-related macular degeneration,”
Opt. Express 7(2), 95–106 (2000).
[41] A. E. Elsner, M. S. Muller, B. L. Petrig, J. A. Papay, C. A. Clark, J. Alavanja, and B. P.
Haggerty, Toward Low Cost Imaging: A Laser Scanning Digital Camera. Bio-Optics:
Design and Application (BODA) 2011 paper, BWA1, OSA Technical Digest (CD).

REFERENCES
537
[42] M. E. Hartnett, J. J. Weiter, G. Staurenghi, and A. E. Elsner, “Deep retinal vascular
anomalous complexes in advanced age-related macular degeneration,” Ophthalmology
103(12), 2042–2053 (1996).
[43] E. Beausencourt, A. E. Elsner, M. E. Hartnett, and C. L. Trempe, “Quantitative analysis
of macular holes with scanning laser tomography,” Ophthalmology 104(12), 2018–2029
(1997).
[44] C. Kunze, A. E. Elsner, E. Beausencourt, L. Moraes, M. E. Hartnett, and C. L. Trempe,
“Spatial extent of pigment epithelial detachments in age-related macular degeneration,”
Ophthalmology 106(9), 1830–1840 (1999).
[45] E. Beausencourt, A. Remky, A. E. Elsner, M. E. Hartnett, and C. L. Trempe,
“Infrared scanning laser tomography of macular cysts,” Ophthalmology 107(2), 375–385
(2000).
[46] M. Miura and A. Elsner, “Three dimensional imaging in age-related macular degenera-
tion,” Opt. Express 9(9), 436–443 (2001).
[47] R. Ferguson, D. Hammer, A. Elsner, R. Webb, S. Burns, and J. Weiter, “Wide-field
retinal hemodynamic imaging with the tracking scanning laser ophthalmoscope,” Opt.
Express 12(21), 5198–5208 (2004).
[48] A. E. Elsner, S. A. Burns, J. J. Weiter, and F. C. Delori, “Infra-red imaging of subretinal
structures in the human ocular fundus,” Vision. Res. 36(1), 191–205 (1996).
[49] M. Miura, A. E. Elsner, E. Beausencourt, C. Kunze, M. E. Hartnett, K. Lashkari, and
C. L. Trempe, “Grading of infrared confocal scanning laser tomography and video
displays of digitized color slides in exudative age-related macular degeneration,” Retina
22(3):300–308 (2002).
[50] M. Miura, A. E. Elsner, M. Osako, K. Yamada, T. Agawa, M. Usui, and T. Iwasaki,
“Spectral imaging of the area of internal limiting membrane peeling,” Retina 25(4),
468–472 (2005).
[51] R. J. Zawadzki, B. Cense, Y. Zhang, S. S. Choi, D. T. Miller, and J. S. Werner, “Ultrahigh-
resolution optical coherence tomography with monochromatic and chromatic aberration
correction,” Opt. Express 16(11),8126–8143 (2008).
[52] A. E. Elsner, Q. Zhou, F. Beck, P. E. Tornambe, S. A. Burns, J. J. Weiter, and A. W.
Dreher, “Detecting AMD with multiply scattered light tomography,” Int. Ophthalmol.
23(4–6), 245–250 (2001).
[53] S. A. Burns, A. E. Elsner, M. B. Mellem-Kairala, and R. B. Simmons, “Improved
contrast of subretinal structures using polarization analysis,” Invest. Ophthalmol. Vis.
Sci. 44(9), 4061–4068 (2003).
[54] A. Weber, M. Cheney, Q. Smithwick, and A. Elsner, “Polarimetric imaging and blood
vessel quantification,” Opt. Express 12(21), 5178–5190 (2004).
[55] M. B. Mellem-Kairala, A. E. Elsner, A. Weber, R. B. Simmons, and S. A. Burns,
“Improved contrast of peripapillary hyperpigmentation using polarization analysis,”
Invest. Ophthalmol. Vis. Sci. 46(3), 1099–1106 (2005).
[56] A. E. Elsner, A. Weber, M. C. Cheney, D. A. VanNasdale, and M. Miura, “Imaging
polarimetry in patients with neovascular age-related macular degeneration,” J. Opt. Soc.
Am. A. Opt. Image. Sci. Vis. 24(5), 1468–1480 (2007).
[57] M. Miura, A. E. Elsner, M. C. Cheney, M. Usui, and T. Iwasaki, “Imaging polarimetry
and retinal blood vessel quantification at the epiretinal membrane,” J. Opt. Soc. Am. A.
Opt. Image. Sci. Vis. 24(5), 1431–1437 (2007).

538
TECHNOLOGY
[58] D. Huang, E. A. Swanson, C. P. Lin, J. S. Schuman, W. G. Stinson, W. Chang, M.
R. Hee, T. Flotte, K. Gregory, C. A. Puliafito, et al., “Optical coherence tomography,”
Science 254(5035), 1178–1181 (1991).
[59] A. F. Fercher, C. K. Hitzenberger, W. Drexler, G. Kamp, and H. Sattmann, “In vivo
optical coherence tomography,” Am. J. Ophthalmol. 116(1), 113–114 (1993).
[60] A. G. Podoleanu, R. K. Harding and D. A. Jackson, “Practical implementation of a high-
speed multichannel correlator with fiber-optic delays,” Appl. Opt. 36(30), 7523–7530
(1997).
[61] A. F. Fercher, W. Drexler, C. K. Hitzenberger, and T. Lasser, “Optical coherence
tomography—principles and applications,” Rep. Prog. Phys. 66, 239–303 (2003).
[62] W. Drexler and J. G. Fujimoto. Optical Coherence Tomography: Technology and Appli-
cations (Springer, 2008).
[63] J. F. de Boer, B. Cense, B. H. Park, M. C. Pierce, G. J. Tearney, and B. E. Bouma,
“Improved signal-to- noise ratio in spectral-domain compared with time-domain optical
coherence tomography,” Opt. Lett. 28(21), 2067–2069 (2003).
[64] N. Nassif, B. Cense, B. H. Park, S. H. Yun, T. C. Chen, B. E. Bouma, G. J. Tearney,
and J. F. de Boer, “In vivo human retinal imaging by ultrahigh-speed spectral domain
optical coherence tomography,” Opt. Lett. 29(5), 480–482 (2004).
[65] J. F. de Boer, T. E. Milner, M. J. van Gemert, and J. S. Nelson, “Two-dimensional
birefringence imaging in biological tissue by polarization-sensitive optical coherence
tomography,” Opt. Lett. 22(12), 934–936 (1997).
[66] E. G¨otzinger, M. Pircher, M. Sticker, A. F. Fercher, and C. K. Hitzenberger, “Measure-
ment and imaging of birefringent properties of the human cornea with phase-resolved,
polarization-sensitive optical coherence tomography,” J. Biomed. Opt. 9(1), 94–102
(2004).
[67] M. Yamanari, S. Makita, and Y. Yasuno, “Polarization-sensitive swept-source optical
coherence tomography with continuous source polarization modulation,” Opt. Express
16(8):5892—5906 (2008).
[68] M. Miura, M. Yamanari, T. Iwasaki, A. E. Elsner, S. Makita, T. Yatagai, and Y. Yasuno,
“Imaging polarimetry in age-related macular degeneration,” Invest. Ophthalmol. Vis.
Sci. 49(6), 2661–2667 (2008).
[69] B. Cense, W. Gao, J. M. Brown, S. M. Jones, R. S. Jonnal, M. Mujat, B. H. Park, J. F. de
Boer, and D. T. Miller, “Retinal imaging with polarization-sensitive optical coherence
tomography and adaptive optics,” Opt. Express 17(24), 21634–21651 (2009).
[70] S. A. Burns, S. Marcos, A. E. Elsner, and S. Bara, “Contrast improvement of confocal
retinal imaging by use of phase-correcting plates,” Opt. Lett. 27(6), 400–402 (2002).
[71] S. A. Burns, R. Tumbar, A. E. Elsner, D. Ferguson, and D. X. Hammer, “Large-field-
of-view, modular, stabilized, adaptive-optics-based scanning laser ophthalmoscope,”
J. Opt. Soc. Am. A. Opt. Image Sci. Vis. 24(5), 1313–1326 (2007).
[72] D. T. Miller, O. P. Kocaoglu, Q. Wang, and S. Lee, “Adaptive optics and the eye (super
resolution OCT),” Eye (London) 25(3), 321–330 (2011).
[73] S. Wolf, K. J. Wald, A. E. Elsner, and G. Staurenghi, “Indocyanine green choroidal
videoangiography: a comparison of imaging analysis with the scanning laser ophthal-
moscope and the fundus camera,” Retina 13(3), 266–269 (1993).
[74] S. Wolf, A. Remky, A. E. Elsner, O. Arend, and M. Reim, “Indocyanine green video
angiography in patients with age-related maculopathy-related retinal pigment epithelial
detachments,” Ger. J. Ophthalmol. 3(4–5), 224–227 (1994).

REFERENCES
539
[75] K. J. Wald, A. E. Elsner, S. Wolf, G. Staurenghi, and J. J. Weiter, “Indocyanine green
videoangiography for the imaging of choroidal neovascularization associated with mac-
ular degeneration,” Int. Ophthalmol. Clin. 34(3), 311–325 (1994).
[76] O. Arend, A. Remky, A. E. Elsner, B. Bertram, M. Reim, and S. Wolf, “Quantification of
cystoid changes in diabetic maculopathy,” Invest. Ophthalmol Vis. Sci. 36(3), 608–613
(1995).
[77] O. Arend, A. Remky, A. E. Elsner, S. Wolf, and M. Rein, “Indocyanine green angiogra-
phy in traumatic choroidal rupture: clinicoangiographic case reports,” Ger. J. Ophthal-
mol. 4(5), 257–263 (1995).
[78] B. Cense, E. Koperda, J. M. Brown, O. P. Kocaoglu, W. Gao, R. S. Jonnal, and D. T.
Miller, “Volumetric retinal imaging with ultrahigh-resolution spectral-domain optical
coherence tomography and adaptive optics using two broadband light sources,” Opt.
Express 17(5), 4095–4111 (2009).
[79] Y. Yasuno, Y. Hong, S. Makita, M. Yamanari, M. Akiba, M. Miura, and T. Yatagai, “In
vivo high- contrast imaging of deep posterior eye by 1-microm swept source optical
coherence tomography and scattering optical coherence angiography,” Opt. Express
15(10), 6121–6139 (2007).
[80] S. R. Chinn, E. A. Swanson, and J. G. Fujimoto, “Optical coherence tomography using
a frequency-tunable optical source,” Opt. Lett. 22(5), 340–342 (1997).
[81] M. Gora, K. Karnowski, M. Szkulmowski, B. J. Kaluzny, R. Huber, A. Kowalczyk, and
M. Wojtkowski, “Ultra high-speed swept source OCT imaging of the anterior segment
of human eye at 200 kHz with adjustable imaging range,” Opt. Express 17(17), 14880–
14894 (2009).
[82] F. C. Delori, R. H. Webb, and D. H. Sliney, “Maximum permissible exposures for ocular
safety (ANSI 2000), with emphasis on ophthalmic devices,” J. Opt. Soc. Am. A. Opt.
Image Sci. Vis. 24(5), 1250–1265 (2007).
[83] I. Grulkowski, M. Gora, M. Szkulmowski, I. Gorczynska, D. Szlag, S. Marcos, A.
Kowalczyk, and M. Wojtkowski, “Anterior segment imaging with spectral OCT system
using a high-speed CMOS camera,” Opt. Express 17(6), 4842–4858 (2009).
[84] H. Carlson and D. Kurtz. Clinical Procedures for Ocular Examination (McGraw-Hill,
Indianapolis, IN, 2003).
[85] T. Kohnen and D. D. Koch. Cataract and Refractive Surgery (Springer, Berlin,
2009).
[86] H. Pult and J. J. Nichols, “A review of meibography,” Optom. Vis. Sci. 89(5), E760–E769
(2012).
[87] A. Ararwal and S. Jacob Dr., Agarwal’s Textbook on Corneal Topography (Jaypee
Brothers Medical Publishers, New Delhi, 2010).
[88] S. Ortiz, D. Siedlecki, P. Perez-Merino, N. Chia, A. de Castro, Szkulmowski, M.
Wojtkowski, and S. Marcos, “Corneal topography from spectral optical coherence
tomography (sOCT),” Biomed. Opt. Express 2(12):3232–3247 (2011).
[89] J. A. Izatt, M. R. Hee, E. A. Swanson, C. P. Lin, D. Huang, J. S. Schuman, C. A.
Puliafito, and J. G. Fujimoto, “Micrometer-scale resolution imaging of the anterior eye
in vivo with optical coherence tomography,” Arch. Ophthalmol. 112(12), 1584–1589
(1994).
[90] Y. Li, R. Shekhar, and D. Huang, “Corneal pachymetry mapping with high-speed optical
coherence tomography,” Ophthalmology 113(5), 792–9.e2 (2006).
[91] M. Lawless and C. Hodge. LASIK Int. Ophthalmol. Clin. 53(1), 111–128 (2013).

540
TECHNOLOGY
[92] M. Li, L. Niu, B. Qin, Z. Zhou, K. Ni, Q. Le, J. Xiang, A. Wei, W. Ma, and X. Zhou,
“Confocal comparison of corneal reinnervation after small incision lenticule extraction
(SMILE) and femtosecond laser in situ keratomileusis (FS-LASIK),” PLoS. One. 8(12),
e81435 (2013).
[93] D. S. Grewal and S. Basti, “Comparison of morphologic features of clear corneal
incisions created with a femtosecond laser or a keratome,” J. Cataract. Refract. Surg.
40(4), 521–530 (2014). doi:10.1016/j.jcrs.2013.11.028
[94] I. Conrad-Hengerer, F. H. Hengerer, M. A. Juburi, T. Schultz, and H. B. Dick, “Fem-
tosecond laser-induced macular changes and anterior segment inflammation in cataract
surgery,” J. Refract. Surg. 30(4), 222–226 (2014). doi:10.3928/1081597X-20140321-01
[95] R. N. Gaster, A. L. Caiado Canedo, and Y. S. Rabinowitz, “Corneal collagen cross-
linking for keratoconus and post-lasik ectasia,” Int. Ophthalmol. Clin. 53(1), 79–90.
[96] Z. Zhong, H. Song, T. Y. Chui, B. L. Petrig, and S. A. Burns, “Noninvasive measurements
and analysis of blood velocity profiles in human retinal vessels,” Invest. Ophthalmol.
Vis. Sci. 52(7), 4151–4157 (2011).
[97] Z. Zhong, B. L. Petrig, X. Qi, and S. A. Burns, “In vivo measurement of erythrocyte
velocity and retinal blood flow using adaptive optics scanning laser ophthalmoscopy,”
Opt. Express 16(17), 12746–12756 (2008).
[98] Q. Wang, O. P. Kocaoglu, B. Cense, J. Bruestle, R. S. Jonnal, W. Gao, and D. T. Miller,
“Imaging retinal capillaries using ultrahigh-resolution optical coherence tomography
and adaptive optics,” Invest. Ophthalmol. Vis. Sci. 52(9), 6292–6299 (2011).
[99] J. Tam, P. Tiruveedhula, and A. Roorda, “Characterization of single-file flow through
human retinal parafoveal capillaries using an adaptive optics scanning laser ophthalmo-
scope,” Biomed. Opt. Express 2(4), 781–793 (2011).
[100] V. J. Srinivasan, B. K. Monson, M. Wojtkowski, R. A. Bilonick, I. Gorczynska, R. Chen,
J. S. Duker, J. S. Schuman, and J. G. Fujimoto, “Characterization of outer retinal mor-
phology with high-speed, ultrahigh-resolution optical coherence tomography,” Invest.
Ophthalmol. Vis. Sci. 49(4), 1571–1579 (2008).
[101] S. A. Burns, A. E. Elsner, T. Y. Chui, D. A. Vannasdale Jr., C. A. Clark, T. J. Gast,
V. E. Malinovsky, and A. D. Phan, “In vivo adaptive optics microvascular imaging in
diabetic patients without clinically severe diabetic retinopathy,” Biomed Opt. Express
5(3), 961–974 (2014).
[102] B. Potsaid, B. Baumann, D. Huang, S. Barry, A. E. Cable, J. S. Schuman, J. S. Duker, and
J. G. Fujimoto, “Ultrahigh speed 1050nm swept source/Fourier domain OCT retinal and
anterior segment imaging at 100,000 to 400,000 axial scans per second,” Opt. Express
18(19), 20029–20048 (2010).
[103] Y. J. Hong, M. Miura, S. Makita, M. J. Ju, B. H. Lee, T. Iwasaki, and Y. Yasuno,
“Noninvasive investigation of deep vascular pathologies of exudative macular diseases
by high-penetration optical coherence angiography,” Invest. Ophthalmol. Vis. Sci. 54(5),
3621–3631 (2013). doi:10.1167/iovs.12-11184.
[104] I. Grulkowski, J. J. Liu, B. Potsaid, V. Jayaraman, C. D. Lu, J. Jiang, A. E. Cable, J.
S. Duker, and J. G. Fujimoto, “Retinal, anterior segment and full eye imaging using
ultrahigh speed swept source OCT with vertical-cavity surface emitting lasers,” Biomed.
Opt. Express, 3(11), 2733–2751 (2012).
[105] O. O. Ahsen, Y. K. Tao, B. M. Potsaid, Y. Sheikine, J. Jiang, I. Grulkowski, T. H. Tsai,
V. Jayaraman, M. F. Kraus, J. L. Connolly, et al., “Swept source optical coherence

REFERENCES
541
microscopy using a 1310 nm VCSEL light source,” Opt. Express 21(15), 18021–18033
(2013).
[106] B. Cense, Q. Wang, S. Lee, L. Zhao, A. E. Elsner, C. K. Hitzenberger, and D. T.
Miller, “Henle fiber layer phase retardation measured with polarization-sensitive optical
coherence tomography,” Biomed. Opt. Express 4(11), 2296–2306 (2013).
[107] Y. Jia, S. T. Bailey, D. J. Wilson, O. Tan, M. L. Klein, C. J. Flaxel, B. Potsaid, J. J. Liu,
C. D. Lu, M. F. Kraus, J. G. Fujimoto, and D. Huang, “Quantitative optical coherence
tomography angiography of choroidal neovascularization in age-related macular degen-
eration,” Ophthalmology S0161-6420(14)00104-3. doi:10.1016/j.ophtha.2014.01.034
(2014).
[108] R. F. Spaide, and C. A. Curcio, “Anatomical correlates to the bands seen in the outer
retina by optical coherence tomography: literature review and model,” Retina 31(8),
1609–1619 (2011).
[109] C. A. Curcio, J. D. Messinger, K. R. Sloan, A. Mitra, G. McGwin, and R. F. Spaide,
“Human chorioretinal layer thicknesses measured in macula-wide, high-resolution his-
tologic sections,” Invest. Ophthalmol. Vis. Sci. 52(7), 3943–3954 (2011).
[110] R. W. Lu, C. A. Curcio, Y. Zhang, Q. X. Zhang, S. J. Pittler, D. Deretic, and X. C. Yao,
“Investigation of the hyper-reflective inner/outer segment band in optical coherence
tomography of living frog retina,” J. Biomed. Opt. 17(6), 060504 (2012).
[111] J. Cuadros and G. Bresnick, “EyePACS: an adaptable telemedicine system for diabetic
retinopathy screening,” J. Diabetes. Sci. Technol. 3(3), 509–516 (2009).
[112] S. A. Burns, M. R. Kreitz, and A. E. Elsner. Computer-controlled two-color laser-based
optical stimulator for vision research,” Appl. Opt. 30(16), 2063–2065 (1991).
[113] J. F. Chen, A. E. Elsner, S. A. Burns, R. M. Hansen, P. L. Lou, K. K. Kwong, and A. B.
Fulton, “The effect of eye shape on retinal responses,” Clin. Vis. Sci. 7, 521–530 (1992).
[114] A. Remky, E. Beausencourt, and A. E. Elsner, “Angioscotometry with the scanning
laser ophthalmoscope. Comparison of the effect of different wavelengths,” Invest. Oph-
thalmol. Vis. Sci. 37(11), 2350–2355 (1996).
[115] A. Remky, K. Lichtenberg, A. E. Elsner, and O. Arend, “Short wavelength automated
perimetry in age related maculopathy,” Br. J. Ophthalmol. 85(12), 1432–1436 (2001).
[116] A. Remky, A. E. Elsner, A. J. Morandi, E. Beausencourt, and C. L. Trempe, “Blue-on-
yellow perimetry with a scanning laser ophthalmoscope: small alterations in the central
macula with aging,” J. Opt. Soc. Am. A. Opt. Image Sci. Vis. 18(7), 1425–1436 (2001).
[117] A. Remky and A. E. Elsner, “Blue on yellow perimetry with scanning laser ophthal-
moscopy in patients with age related macular disease,” Br. J. Ophthalmol. 89(4), 464–469
(2005).
[118] R. F. Kaplan, R. A. Cohen, A. Rosengart, A. E. Elsner, T. R. Hedges 3rd, and L. R.
Caplan, “Extinction during time controlled direct retinal stimulation after recovery from
right hemispheric stroke,” J. Neurol. Neurosurg. Psychiatry 59(5), 534–536 (1995).
[119] A. E. Elsner, B. L. Petrig, J. A. Papay, E. J. Kollbaum, C. A. Clark, and M. S. Muller,
“Fixation stability and scotoma mapping for patients with low vision,” Optom. Vis. Sci.
90(20), 164–173 (2013).
[120] Q. X. Zhang, R. W. Lu, C. A. Curcio, and X. C. Yao, “In vivo confocal intrinsic
optical signal identification of localized retinal dysfunction,” Invest. Ophthalmol. Vis.
Sci. 53(13), 8139–8145 (2012).
[121] A. D. Singh, “Ocular phototherapy,” Eye (London) (2012). doi:10.1038/eye.2012.258


INDEX
Note: Page numbers followed by f and t indicate figures and tables, respectively.
Abbe criterion, 40
Abbe-Rayleigh criterion, 62
Abbe’s diffraction, 470
lateral resolution limit, 474
ABC, see ATP-binding cassette (ABC)
Absorption, 2, 3, 3f
spectroscopy, 3f
spectrum, 3, 4, 5
Absorption, extinction, interference-based
methods, 89–90
Absorption detection, 88
Absorption gate, 391–395
Acinetobacter baumannii, 438
Acoustooptic modulator (AOM), 216
Acridine orange, 413
Actinic keratosis (AK), 429–430
Actinobacillus actinomycetemcomitans, 440
Activator, 102
Active sorting (optical), 216–218, 217f
Adaptive optics, 512f
Age-related macular degeneration (AMD),
424, 445
ALA-methyl ester (MAL)-PDT, 429–430
Albedo, 289
AlGaN LED excitation, 16
Allophycocyanin (APC), 41f
ALT, see Argon laser trabeculoplasty
(ALT)
Alzheimer’s disease, 10
5-Aminolevulinic acid (5-ALA), 414
Analyte, 323
Angle closure glaucoma, 528
Anharmonicity, 80
Animal models of wound infections,
437–440
Anisotropic tissues, 254
Anisotropy, fluorescence, 30–38
experimental, 33–35
nanoparticle metrology, 35–38
theory, 30–33
Anisotropy factor, 372, 373f
Anterior chamber, ciliary body, and lens,
526–527
Anterior chamber and lens, 505–506
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.
543

544
INDEX
Anterior segment anatomy
anterior chamber and lens, 505–506
cornea and tear film, 504–505
Anterior segment diagnostics
anterior chamber, ciliary body, and lens,
526–527
cornea and tear film, 522–526
light and treatment, 528–529
refraction, 527–528
Antibodies, 328
Anticancer PDT, 419
Anticancer radiation therapies, 423
Antimicrobial PDT, 435
AO imaging system, 519f
AOM, see Acoustooptic modulator (AOM)
AOSLO imaging of human cones, 520f
Aptamers, 329
Argon laser trabeculoplasty (ALT), 528
Aromatic fluorophores, 13
Atomic force microscope (AFM), 60
ATP-binding cassette (ABC), 431
Aufbau principle, 415
Auto-correlation function, 492
Auto-oxidation, 5–6
Auto-refractors function, 527
Auxochrome, 418
Avogadro’s number, 27
Axial resolution, 168
B800 ring, 120, 121f
B850 ring, 85, 120, 121f
Babinet Soleil compensators, 275
Back-scattering geometry, 285
Bacterial reaction center (BRC), 132, 132f
Balanced super-resolution optical
fluctuation imaging (bSOFI), 492
Ballistic photons, 370, 370f
Basal cell carcinoma (BCC), 430–431
Bayesian analysis of blinking and
bleaching, 494
BCC, see Basal cell carcinoma (BCC)
Beam scanning, 65
Bean Pod Mottle Virus (BPMV), 340, 341f
Beer–Lambert law, 5
BICELLs, see Biophotonic sensing cells
(BICELLs)
Bimodal waveguide (BiMW)
interferometer, 338, 339f
Biomedical optical imaging, 369
Biophotonics, 129
Biophotonic sensing cells (BICELLs), 347,
349
Biophotonics workstation, 207, 207f
Bioreceptors, 323, 328–329
Biosensors, 323
Birefringence, 513
Black lipid films, 67–68
Bleaching, 71, 494
Blinking, 69–70, 494
reversible photochemical reactions,
70–71
triplet state, 70
Blinking and bleaching, 494
Born–Oppenheimer approximation, 78
Botryococcus braunii, 209f
Bowen’s disease/squamous cell carcinoma
in situ, 430
BPMV, see Bean Pod Mottle Virus (BPMV)
Brain tumors, 419
BRC, see Bacterial reaction center (BRC)
Breast cancer
hypoxia for, 377–378
Bright field, 87–88
Brownian diffusion, 74
Brownian isotropic rotation, 32
Brownian particles, 69
Burns, 437
Calorimetry, 8
Cancer, 434
BCC, 430–431
diagnosis, 302–307
human tumor xenografts, 424
nonmelanoma skin cancer (NMSC), 428
photosensitizers approved/in clinical
trials for, 420t, 421f
Catalytic biosensors, 329
Cataract, 506
CCD, see Charge-coupled device (CCD)
CCD camera, 66
Cell deformations, 211–214
Cells handling
cell positioning
intercellular interaction,
microenvironment, 209–211, 210f
nonbiological stimuli,
microenvironment, 208–209, 209f
single-cell analysis, 206–208, 207f

INDEX
545
cellular biomechanics, 211–214, 213f
cytometry, 214–215, 215f
dielectric tagging, 201f, 202–206, 203t,
204f, 205f
light–cell interaction, 198–202, 199f
refractive index, 203t
The cellular plasma membrane, 476
Cell viability, 220–230, 221t
division time, 226–227
energy dose, 225–226
growth, 226–227
internal pH (pHi), 228
optimal wavelength, 222–224,
222f–223f
power, 225–226
propidium iodide (PI), 227–228
reactive oxygen species, 229–230
thermal effects, 224–225
Cervical intraepithelial neoplasia (CIN),
305, 444
Cetyltrimethylammonium bromide
(CTAB), 45
CFD, see Constant fraction discrimination
(CFD)
Charge-coupled device (CCD), 385, 478,
484
Charge separation (CS) rates, 130
Charge-transfer state, 70
Chinese Hamster ovary (CHO) cells, 202
Chitosan nanoparticles (CNPs), 427
Chlamydomonas reinhardtii, 155, 155f,
209f
Chlorophyll (Chl), 130–132, 132f
CHO cells, see Chinese Hamster ovary
(CHO) cells
Choroidal melanoma, 431
Choroidal neovascularization (CNV), 445
Chromatic aberrations, 64
Chromophores, 103, 104, 418
Chromosomes, 470
CIN, see Cervical intraepithelial neoplasia
(CIN)
Circular dichroism, 253
Circular retarders, 255
Cladding, 324, 325, 326
CNPs, see Chitosan nanoparticles (CNPs)
Coherent anti-Stokes Raman scattering
(CARS) microscopy, 77,
178–180, 179f, 182–184
Colliding pulse mode-locked (CPM) laser,
396
Collisional quenching by oxygen, 7
Compensator, 274
Complementary metal oxide (CMOS)
arrays, 514
Complete Mueller imagers, 280–282, 281f
Complete versus incomplete instruments
(tissue polarimetry), 270–271
Cone density, 513
Confocal aperture, 514
Confocal imaging
introduction, 168–169, 169f
point scanned microscopy, 169–172
Confocal microscopy, 65–66
Constant fraction discrimination (CFD), 17
Constant retarders, 272–275
Convolution analysis, 18
Core, 324, 325, 326
Cornea curvature, 523
Corneal epithelium, 505
Corneal stroma, 505
Cornea and tear film, 504–505, 522–526
Coulombic coupling, 112
CP43, 152–154, 153f–154f
C-reactive protein (CRP), 338
CRP, see C-reactive protein (CRP)
Crystalline beam splitters, 271
CTAB, see Cetyltrimethylammonium
bromide (CTAB)
Cutaneous T cell lymphoma (CTCL), 431
Cutler, Max, 369
Cyanobacteria, 131–132
Cyanobacterial photosystem I, 148–151,
149f
Cysteine, 21
Cytometry, 214–215, 215f
Dark field, 86–87
Dark-field scattering, 86, 88–89
Dark state (triplet), 75–77
Debye–Stokes–Einstein relation, 75
Debye–Waller factor, 79
Decoherence, 80
Deconvolution, 19
Deconvolution fluorescence microscopy,
489f
Degree of polarization (DOP), 110,
242–243, 246, 256, 284–285

546
INDEX
Dendritic cells (DCs), 447
Dental Infections, 441–442
Depletion, 474
Depolarization, 244–245
Depolarization index, 249
Depolarization powers, 257
Depolarizer, 255–257
Depth of penetration, 325
Dermatology
acne vulgaris, 432–433
Dermatology, PDT for, 428–429
acne vulgaris, 432–433
actinic keratosis, 429–430
BCC, 430–431
Bowen’s disease/squamous cell
carcinoma in situ, 430
melanoma, 431
mycosis fungoides, 431–432
photorejuvenation, 433
2D-ES, see Two-dimensional electronic
spectroscopy (2D-ES)
Dexter theory, 113
Dexter transfer, 113
Dextran polymer, 331
Diaphanography, 369
Diattenuation, 249–250
Dichroic polarizers, 271
Diethylthiadicarbocyanine iodide (DTDCI),
29
Diffuse photons, 370, 370f
Diffusion coefficients, 26
Diffusion effects, RET, 111–112
Diffusive light, 402–403
Dihydroxyindoles, 10, 11
3,4-Dihydroxy-L-phenylalanine (L-DOPA),
5–6, 6f
auto-oxidation of, 11
Dipole–dipole interaction, 81
Direct electronic coupling, 106
Direct photoactivation dSTORM, 43
Direct stochastic optical reconstruction
microscopy, 478–479
directSTORM (dSTORM), 477
Dissipation, RET, 105–107
Donor, see Sensitizer
DOP, see Degree of polarization (DOP)
Dosimetry, PDT, 423
DSTORM, 482
Dual rotating retarders, 280, 281f
Dynamic quenching, 24–26
Dynamic range, biosensor, 331
ECM, see Extracellular matrix (ECM)
EET, see Excitation energy transfer (EET)
Einstein coefficient, 14
Electronic response and ground-state
depletion, 92–93
Electronic transitions coupling, RET,
104–105
Electron microscopy, 490–491
Electron paramagnetic resonance (EPR), 84
Emission spectrum, 474
Endoplasmic reticulum (ER), 419
Enhanced permeability, 419
Enhancing contrast, 521–522
Eos Fluorescent Protein (EosFP), 477
EpicTM System, 344
Epifluorescence, 39
Escherichia coli, 198, 351, 436, 437
Ethylenediaminetetraacetic acid (EDTA),
436
Eumelanin, 10
synthesis, 11f
Evanescence wave absorbance-based
optical fiber, 354
Evanescent field, 325, 325f
Excitation energy, 24
Excitation energy transfer (EET), 130,
136–137
Excitation monochromator, 4
Excitons, 134–136, 135f
External field effects, 84
Extracellular matrix (ECM), 211
Extrinsic fluorescence, 21
Eye
anatomy, 504f
corneal topography of normal and
astigmatic, 525f
Fabry–Perot interferometric biosensors, 354
FACS, see Fluorescence-activated cell
sorting (FACS)
𝜇FACS systems, 215f, 216
Factorial hidden Markov model, 494
Far-field optical super-resolution imaging
techniques, 472–473

INDEX
547
FBG, see Fiber Bragg grating (FBG)
FCS, see Fluorescence correlation
spectroscopy (FCS)
Fenton reaction, 417
Fermi’s rule, 106
FGR, see Fluorescence-guided resection
(FGR)
Fiber Bragg grating (FBG), 350
Fiber Bragg grating (FBG) biosensors,
350–351, 350f
Fibrils, 505
Field cancerization, 429
FISH, see Fluorescence in situ
hybridization (FISH)
Fission yeast (Schizosaccharomyces
pombe), 483
FLIM, see Fluorescence lifetime imaging
microscopy (FLIM)
FLN, see Fluorescence line-narrowing
(FLN)
ΔFLN technique, 141–143
Floppy molecules, 7
Fluctuation-based superresolution
microscopy, 494–495
Fluid-filled chamber, 505
Fluorescein dye, 524f
Fluorescence, 1–2, 65–66
anisotropy, 30–38
lifetime, 12–23
microscopy, 38–48
multidimensional contour, 18
quantum yield, 6–12, 9t
quenching, 23–30
spectra, 2–6, 3f, 4f
Fluorescence-activated cell sorting (FACS),
214, 215f
Fluorescence anisotropy, 75, 110
Fluorescence-based biosensors, 324
Fluorescence correlation spectroscopy
(FCS), 41f, 73–74, 427
dark state (triplet), 75–77
multiparameter analysis, 77
photon counting histograms, burst
analysis, 73
rotational diffusion, 75
translational diffusion, 74–75
variants, 77
Fluorescence emission, 13
Fluorescence excitation spectroscopy
hole-burning, 81–82
inhomogeneous broadening, 80–81
single-molecule spectroscopy, 82–84
external field effects, 84
interacting single molecules, 85–86
optically detected magnetic
resonance, 84
quantum optics, 84
spectral diffusion, 84–85
zero-phonon line and phonon wing,
78–80
Fluorescence (FL1) signals, 214
Fluorescence fractions, 22
Fluorescence-guided resection (FGR), 425
Fluorescence impulse response, 13
Fluorescence in situ hybridization (FISH),
469
Fluorescence intensity trace, 69
Fluorescence lifetime, 167
Fluorescence lifetime imaging microscopy
(FLIM), 13, 43
Fluorescence line-narrowing (FLN),
141–143
Fluorescence photoactivation localization
microscopy (FPALM), 477
Fluorescence resonance energy transfer
(FRET), 427
Fluorescence spectroscopy, 2, 40
bleaching, 71
blinking, 69–70
reversible photochemical reactions,
70–71
triplet state, 70
orientation, 68
polarization analysis in detection,
68–69
polarization modulation in excitation,
69
sample preparation
bilayers, membranes, and black lipid
films, 67–68
Langmuir–Blodgett films, 67
spin coating, 67
signal-to-noise ratio, 66–67
superresolution, 71–72
Fluorescent dye, 8–10, 523
Fluorimeter, 4

548
INDEX
Fluorophore, 23
Fluorophore distribution, 486
Fluorophore–quencher complexes, 23
Folding funnel, 141
F¨orster equation, 108f
F¨orster radius, 107
F¨orster resonance energy transfer (FRET),
26–29
F¨orster theory, 103, 105, 106, 107, 111
Forward model, tissue polarimetry,
291–292, 292f
Forward modeling of polarized light
propagation in tissue
overview, 282–284
photon diffusion formalism, 284–288
polarization-sensitive Monte-Carlo
(PSMC) approach, 288–290
Forward product decompositions, 258–261
Forward scatter (FSC), 214
Forward scattering geometry, 285
Fourier space gate, 395–398, 396f, 397f,
399f
Fourier transform, 486
Franck–Condon amplitude, 78–79
Franck–Condon factors, 79
Franck–Condon (FC) weighted density of
states, 136
Frenkel excitons, 134
Frenkel or molecular exciton model, 120
Frequency-doubling Nd:YAG laser, 528
Fresnel rhombs, 272
FRET, see F¨orster resonance energy transfer
(FRET)
FRET-FLIM, see Fluorescence resonance
energy transfer lifetime imaging
microscopy (FRET-FLIM)
FSC, see Forward scatter (FSC)
Full-width-at-half-maximum (FWHM), 400
FWHM, see Full-width-at-half-maximum
(FWHM)
Gating mechanism, 297
Gaussian beam, 86
Gaussian distribution function, 479
Gaussian function, 482
Gaussian spot, 63
Generalized F¨orster Theory (GFT),
120–121
Geometrical aberrations, 64
GFP, see Green fluorescent protein (GFP)
GFT, see Generalized F¨orster Theory (GFT)
Glaucoma, 506
Glucose/galactose-binding protein (GBP),
21–23, 22f
Goeppert-Mayer, M., 173
Gold nanorods in cells, 45–48
Graded index (GRIN) optics, 185
Gram-negative bacteria, 435
Gram-positive bacteria, 435
Grating-based biosensors, 342–345, 343f
Green fluorescent protein (GFP), 167
Ground-state depletion with individual
molecule return (GSDIM), 482
Hanseniaspora uvarum, 210, 210f
Harmonic microscopy, 175–178, 176f
Hartmann–Shack aberrometers, 527
Hartmann–Shack grid, 519
Helicobacter pylori infection, 444
Hematoporphyrin derivative (HPD), 414,
416, 419, 433
Henle fiber layer, 513
Henle fibers, 513
Herpes simplex virus (HSV), 443
Herpes simplex virus 1 (HSV-1), 338
Hidden Markov model, 494
Highest occupied molecular orbital
(HOMO), 415
HIV, see Human immunodeficiency virus
(HIV)
Hole-burning, 81–82
Homo-FRET, 36
Homogeneous broadening, pigment-protein
complexes, 137–140
Homogeneous or bulk sensing, 332
Homogeneous diattenuators, 252–253
Homogeneous retarders, 253–255
circular, 255
linear, 254
HPD, see Hematoporphyrin derivative
(HPD)
HPV, see Human papilloma virus (HPV)
HSV-1, see Herpes simplex virus 1 (HSV-1)
Huang-Rhys factor, 139
Human immunodeficiency virus (HIV),
476

INDEX
549
Human papilloma virus (HPV), 305
Human tumor xenografts of human breast
cancer, 424
Hydrogen peroxide (H2O2), 417
Hydroxyapatite, 422
Hypoxia, 378–379
Illumination optical system (IO), 266
Image intensifier, 66
4D imaging, 521
Imaging chromatin, 490
Immobilization techniques, bioreceptors,
329–331
covalent binding, 330
noncovalent interactions, 330
physical adsorption, 329–330
physical entrapment, 330
Immunosensors, 328
Infectious disease, PDT for, 435
animal models of wound infections,
437–440
in vitro studies, 435–436
oral and dental infections, 440–442
osteomyelitis, 443
otitis media with effusion (OME),
442–443
peptic ulcer disease (PUD), 444–445
photoinactivation of viruses, fungi, and
parasites, 436–437
viral infections, 443–444
Inhomogeneous broadening, 80–81,
137–140
Institute for Ultrafast Spectroscopy and
Lasers (IUSL), 368–369
Instrumentation, tissue polarimetry
examples
complete Mueller imagers, 280–282,
281f
nonimaging Mueller polarimeter,
280, 280f
orthogonal state contrast (OSC)
imagers, 279–280
polarization state analyzers (PSAs)
based on linear polarizers, 271–272
linear polarizers and constant
retarders combinations, 272–275
linear polarizers and variable
retarders combinations, 275–279
principles
complete versus incomplete
instruments, 270–271
design and optimization
considerations, 269–270
periodic time modulation, 269
scheme of polarimetric instruments,
266–267, 267f
sequential polarization modulation
and analysis, 267–268
spectral or spatial modulation, 269
Insulin fibrils, 10
Integrated depolarization, logarithmic
decomposition, 264
Integrated dichroism, logarithmic
decomposition, 264
Integrated optical microcavity-based
biosensors, 338–342
Integrated optical waveguide, 326–327
Integrated optical waveguide-based
biosensors
grating-based biosensors, 342–345
integrated optical microcavity-based
biosensors, 338–342
interferometric biosensors, 334–338
photonic crystal-based biosensors,
345–346
recent trends, 346–349
Intensity-based imaging, RET, 115–116,
115f
Interacting single molecules, 85–86
Interference pattern, 334
Interferometric biosensors, 334–338,
334f
Mach–Zehnder interferometer (MZI),
335–337, 336f–337f
other interferometric configurations,
338
Young interferometer, 337–338, 337f
Internal pH (pHi), cell viability, 228
Intersystem crossing (ISC), 75f, 415
Intracellular dielectric tagging, 205
Inverse image reconstructions, 402
Inverse polarimetric analysis, 292–296,
293f, 294t
In vivo glucose sensing, 20–23
IO-based LOC platform, 355–356, 356f
Iris functions, 506

550
INDEX
Iron oxide nanoparticles (IONPs), 428
IUSL, see Institute for Ultrafast
Spectroscopy and Lasers (IUSL)
Jablonski diagram, 416f
Jablonski energy level scheme, 3, 3f
Jones vectors, 240, 241, 245
KDP, see Potassium dihydrogen phosphate
(KDP)
Kerastick®, 414
Kerr-Fourier gate (KFG), 391, 392f
KFG, see Kerr-Fourier gate (KFG)
Kirschner wires (K-wires), 443
Kramers–Kronig relations, 204
Label-free optical waveguide biosensors
operation principle, 324–326
optical waveguide technology
integrated optical waveguide,
326–327
optical fibers (OFs), 327–328, 327f
Lab-on-a-chip integration, 354–357, 355f,
356f
Langmuir–Blodgett films, 67
Langmuir film, 72
Laser-Assisted in situ Keratomileusis
(LASIK), 528
Laser Epithelial Keratomileusis (LASEK),
528–529
Laser scanning digital camera, 510
Laser tweezers Raman spectroscopy
(LTRS), 208, 209f
Lateral resolution, 168
LCI, see Low coherence imagine (LCI)
LDL, see Low density lipoprotein (LDL)
Legionella pneumophila, 343
Leishmania, 442
Levulan®, 414
LHC-II, see Light-Harvesting Complex II
(LHC-II)
Lifetime, fluorescence
experimental, 15–20
in vivo glucose sensing, 20–23
standards, 20t
theory, 13–15
Light-emitting diode (LED), 16
Light-harvesting complex II (LHC-II),
118
Light-harvesting complexes
introduction, 118
photosynthetic excitons, 119–121
Light-harvesting proteins, 118
Light-induced spectral diffusion, 82
Light propagation in highly scattering
turbid media
ballistic to diffuse imaging, transition
from, 402–403, 403f
ballistic light components, 378–384,
381f, 382f, 383f
diffused light components, 378–384
introduction, 367–369
optical imaging, 369–378, 370f
optical absorption spectra of
chromophores in tissues,
377–378, 377f, 378f
optical parameters for human tissues
and model media, 374–377, 375f,
375t, 376t
optical parameters for light
propagation, 371–374
transmitted light components,
370–371
photon-sorting gates, 384–402
absorption gate, 391–395
Fourier space gate, 395–398, 396f,
397f, 399f
microscopic imaging, 395–398
polarization gate, 398–402, 401f
time gate, 385–391, 386f
Light-sensitive dye photosensitization, 414
Light and treatment, 528–529
Limit of detection (LOD), 326
optical waveguide biosensors, 334t
Linear birefringence, 254
Linear polarizers, 271–272
Linear retardation, logarithmic
decomposition, 264
Linear retarders, 254
Linear (scalar) dichroism, 252
Linear vibronic coupling, 78
Line broadening, RET, 105–107
Lipid bilayer, 33
Liquid crystals cells, 276
Listeria monocytogenes, 338
Lithium niobate (LiNbO3), 326
Live cell imaging, 491
Localization microscopy, 483

INDEX
551
Localization of single fluorophores,
479–482
LOD, see Limit of detection (LOD)
Logarithmic decomposition, 261–265, 294t
Long-period grating (LPG) biosensors,
351–353, 352f
Long-range transfer, RET, 112–113
Lorentzian lineshape, 81
Lorentzian ZPL, 140
Low coherence imagine (LCI), 517
Low density lipoprotein (LDL), 419, 421
Lowest unoccupied molecular orbital
(LUMO), 415
LTRS, see Laser tweezers Raman
spectroscopy (LTRS)
Lu-Chipman decomposition, 260, 260f
LUDOX, 35, 36f, 37t
LUDOX silica, 19
LUMO, see Lowest unoccupied molecular
orbital (LUMO)
Mach–Zehnder interferometer (MZI), 334f,
335–337, 336f–337f
Madin-Darby canine kidney (MDCK) cells,
46
Malignant tumors, 434
Markov Chain Monte Carlo sampling, 494
Maximum entropy principle, 285
MEA, see Monoethanolamine (MEA)
Measurement techniques
enhancing contrast, 521–522
increase contrast of transparent and
scattering tissues, 515–517
scanning and confocal apertures,
514–515
wavefront aberration measurement and
compensation, 517–521
Mechanotransduction, cells, 211–214
Meibomian glands, 522
Melanin, 422
Melanin spectra, 5–6
Melanoma, 431
Membranes, 67–68
Metal ion quenching, 29–30
Metronomic PDT, 425
Microbial infection, 505
Microcontact printing, 331
Microfluidic patterning, 331
Microscopic imaging, 395–398
Microscopy, fluorescence, 38–39
gold nanorods in cells, 45–48
systems and techniques, 39–44
Microspheres, 342
Microstructured optical fibers (MOFs), 328
Microtoroids, 342
Microviscosity, 32
Mie regime, 286–287
Minkowski metric tensor, 264
Minsky, Marvin, 168
MIPs, see Molecular imprinted polymers
(MIPs)
Mitochondria, 419
MLS, see Multilevel systems (MLS)
Modulation frequency, 14
MOFs, see Microstructured optical fibers
(MOFs)
Molecular fingerprint, 5
Molecular imprinted polymers (MIPs), 329
Molecular pathology, 2
Molecular ruler, 26
Monoethanolamine (MEA), 478
Mueller matrix, 258
of depolarizer, 255, 257
depolarizing or nondepolarizing
character, 248–249
elements, 292
non-Stokes diagonalizable, 261–265
physical realizability, 248
of retarder, 254
Multiexponential fluorescence decay
tracking, 36
Multilevel systems (MLS), 141
Multiparameter analysis, 77
Multiphoton excitation, 37, 40
Multiphoton excitation microscopy, 39
Multiphoton fluorescence microscopy,
173–175
advantages, 175
Multiplexing, biofunctionalization
strategies, 331
Mycobacterium tuberculosis, 442
Mycosis fungoides (MF), 431–432
MZI, see Mach–Zehnder interferometer
(MZI)
NA, see Numerical aperture (NA)
Nanocrystals, 60
Nanofountain, 116, 117f

552
INDEX
Nano-optics, 60
Nanoparticle metrology, 35–38
Nanoplatforms with theranostic
functionalities, 426f
Nanoscale optofluidic sensor array
(NOSA), 345
Nanotechnology, 425–428, 427f
Near-field scanning optical microscopy
(NSOM), 41–42, 41f
Near-infrared fluorescent (NIRF), 427
Near-infrared light, 371
Near-infrared (NIR) lasers, 368
Neochloris oleoabundans, 209f
Nipkow spinning disk confocal, 172, 173f
Nitrogen bubbling, 7
Nitrogen vacancies, 475
N-matrix formalism, 290
Noise reduction, 481
Nondestructive chemical analysis, 469
Nonimaging Mueller polarimeter, 280, 280f
Noninvasive glucometry, 300–302,
301f–302f
Nonlinear microscopy, 172–180
adaptive optics for in-depth imaging,
185–186, 186f
clinical applications, 184–185
coherent anti-Stokes Raman scattering
(CARS) microscopy, 178–180,
179f
harmonic microscopy, 175–178
miniature instrumentation, 184–185
multiphoton fluorescence microscopy,
173–175
optical configuration for, 181f
practical implementation of, 181–184
stimulated Raman scattering
microscopy, 180
Non-maximum suppression (NMS), 481
Nonmelanoma skin cancer (NMSC), 428
Non-obese diabetic (NOD), 448
Non-photochemical hole-burning (NPHB),
130, 142
Non-small-cell lung cancer (NSCLC), 424
Non-Stokes diagonalizable Mueller
matrices, 261
NOSA, see Nanoscale optofluidic sensor
array (NOSA)
NPHB, see Non-photochemical
hole-burning (NPHB)
Nuclear instrument modules (NIMs), 17
Nuclear lamina, 490
Nuclear pore complex, 490
Nucleic acid sensors, 328–329
Numerical aperture (NA), 62, 167
OCT, see Optical coherence tomography
(OCT)
Ocular anatomy and physiology, 503–504
anterior segment anatomy
anterior chamber and lens, 505–506
cornea and tear film, 504–505
retina, 506–507
photoreceptors, 507–513
superficial retinal layers, 513–514
OF, see Optical fibers (OFs)
OFRR, see Optofluidic ring resonators
(OFRRs)
OKG, see Optical Kerr Gate (OKG)
Oncology, PDT for, 433–435
Ophthalmic imaging devices, 521
Ophthalmology, 445–446
Optical biopsy, 240
Optical biosensors
evaluation, 331–334, 332f
fluorescence-based detection, 323–324
label-free detection, 323–324
Optical coherence tomography (OCT), 308,
505f, 517
Optical fiber-based biosensors, 349–354
evanescence wave absorbance-based
optical fiber, 354
fiber Bragg grating (FBG) biosensors,
350–351, 350f
long-period grating (LPG) biosensors,
351–353, 352f
OF-based interferometry biosensors,
353–354
Optical fiber-based interferometric
biosensors, 353–354
Optical fibers (OFs), 324, 327–328, 327f
Optical forces, 197–198
components of measurement system,
202f
ray optics model of, 199–200, 199f
for silica particles, 203f
Optical Kerr Gate (OKG), 385–391, 390f
Optically detected magnetic resonance
(ODMR), 84

INDEX
553
Optical microscope, 62
Optical resolution beyond Abbe’s limit,
469–472
advent of far-field optical
super-resolution imaging
techniques, 472–473
Optical rotation, logarithmic
decomposition, 264
Optical sectioning microscopy
background, 165–168
confocal imaging
introduction, 168–169, 169f
point scanned microscopy, 169–172
introduction, 165–168
biological context and limitations,
166–167
definitions and terms, 167–168
nonlinear microscopy, 172–180
adaptive optics for in-depth imaging,
185–186
clinical applications, 184–185
coherent anti-Stokes Raman
scattering (CARS) microscopy,
178–180
harmonic microscopy, 175–178
miniature instrumentation, 184–185
multiphoton fluorescence
microscopy, 173–175
practical implementation of, 181–184
stimulated Raman scattering
microscopy, 180
widefield optical sectioning
single plane illumination microscopy
(SPIM), 186–187, 187f
structured illumination microscopy,
187–188f, 187–189
sub-diffraction-limited imaging,
189–190
Optical sorting, 215–220
active, 216–218, 217f
passive, 218–219, 219f, 220f
Optical stretcher, 212–214, 213f
Optical transfer function (OTF), 484
Optical tweezers, 198
Optical waveguide biosensors
evaluation of optical biosensors,
331–334
integrated optical waveguide-based
biosensors
grating-based biosensors, 342–345,
343f
integrated optical microcavity-based
biosensors, 338–342
interferometric biosensors, 334–338
photonic crystal-based biosensors,
345–346
recent trends, 346–349
introduction, 323–324
label-free optical waveguide biosensors
operation principle, 324–326
optical waveguide technology,
326–328
lab-on-a-chip integration, 354–357,
355f, 356f
optical fiber-based biosensors, 349–354
evanescence wave absorbance-based
optical fiber, 354
fiber Bragg grating (FBG)
biosensors, 350–351, 350f
long-period grating (LPG)
biosensors, 351–353, 352f
OF-based interferometry biosensors,
353–354
surface biofunctionalization
bioreceptors, 328–329
immobilization techniques, 329–331
for multiplexing, 331
Optical waveguide lightmode spectroscopy
(OWLS), 343
Optical waveguide technology
integrated optical waveguide, 326–327
optical fibers (OFs), 327–328, 327f
Optimal wavelength, cell viability,
222–224
cell viability, 222f–223f
Optofluidic ring resonators (OFRRs), 341,
342f
Optofluidics, 356
Oral and dental infections, 440–442
Orientational factor, 27
Orientation dependence, FET, 108–109,
109f
Orthogonal scatter (SSC) intensity, 214
Orthogonal state contrast (OSC) imagers,
279–280
OSC imagers, see Orthogonal state contrast
(OSC) imagers
Osteomyelitis, 443

554
INDEX
Otitis media with effusion (OME), 442–443
OWLS, see Optical waveguide lightmode
spectroscopy (OWLS)
Pap smear, 305
Paramecium spp., 413
Para-terphenyl crystal, 83
Partial coherence interferometry (PCI),
528
Partially polarized states, 242–245,
243f
Passive sorting (optical), 218–219, 219f,
220f
PAT, see Photoacoustic tomography (PAT)
Pathogenesis of acne, 432
PBS, see Polarizing beam splitters (PBS)
PDT, see Photodynamic Therapy (PDT)
PEG, see Poly(ethylene) glycol (PEG)
Peptic ulcer disease (PUD), 444–445
Peptide beta-amyloid (A𝛽), 10
Peptide Nucleic Acids (PNAs), 329
Peptides, 328
Periodic time modulation, tissue
polarimetry, 269
Perrin, Francis, 102
Perrin, Jean, 102
Phagocytosis, 205
Pheophytins (Pheos), 130–132, 132f
Phonon
higher-order coupling, 80
Phonon sidebands (PSBs), 137–140
Phonon wing (PW), 79
Phosphorescence, 416
Photoacoustic spectroscopy actinometry, 8
Photoacoustic tomography (PAT), 368
Photoactivated localization microscopy and
stochastic optical reconstruction
microscopy, 477–478
beyond dSTORM, 482
direct stochastic optical reconstruction
microscopy, 478–479
localization microscopy, 483
localization of single fluorophores,
479–482
Photoactivated localization microscopy
(PALM), 43, 473, 490
Photobleaching, 419
Photochemistry, 417–418
Photodetector, 65
Photodynamic Therapy (PDT), 367
for dermatology, 428–429
acne vulgaris, 432–433
actinic keratosis, 429–430
basal cell carcinoma (BCC),
430–431
Bowen’s disease/squamous cell
carcinoma in situ, 430
melanoma, 431
mycosis fungoides, 431–432
photorejuvenation, 433
history, 413–415
and immune system, 446–448
for infectious disease, 435
animal models of wound infections,
437–440
in vitro studies, 435–436
oral and dental infections, 440–442
osteomyelitis, 443
otitis media with effusion (OME),
442–443
peptic ulcer disease (PUD), 444–445
photoinactivation of viruses, fungi,
and parasites, 436–437
viral infections, 443–444
light-based strategies, 423
light dosimetry and photodynamic
therapy light sources, 422
fundamental, 423
tissue optics, 422–423
metronomic PDT, 425
for oncology, 433–435
in ophthalmology, 445–446
photochemistry, 417–418
photophysics, 415–417
photosensitizer structure and
photophysical properties,
418–422
targeting and nanotechnology, 425–428
two-photon excitation-PDT, 423–424
Photodynamischer effekt, 414
Photoelastic modulators, 275–276
Photoexcitation process, RET, 103–104,
104f
Photofrin®, 414, 419
Photoinactivation of viruses, fungi, and
parasites, 436–437
Photoluminescence, 87–88
Photomultipliers, 16

INDEX
555
Photon counting histograms, burst analysis,
73
Photon diffusion formalism, 284–288
Photon harvesters, 424
Photonic bandgap, 345
Photonic crystal-based biosensors,
345–346
Photon-sorting gates, 384–402
absorption gate, 391–395
Fourier space gate, 395–398, 396f, 397f,
399f
microscopic imaging, 395–398
polarization gate, 398–402, 401f
time gate, 385–391, 386f
Photophysics, 415–417
Photopigment melanopsin, 513
Photoreceptors, 507–513
Photorefractive Keratectomy (PRK),
529
Photorejuvenation, 433
Photoselection, 110
Photosensitizers approved or in clinical
trials for cancer, 420t, 421f
Photosensitizing agent, 418
Photoswitchable fluorophores, 472
Photosynthesis, biophotonics of
concepts of pigment–protein complexes,
133–141
excitation energy transfer (EET),
136–137
excitons, 134–136, 135f
homogeneous broadening, 137–140
inhomogeneous broadening,
137–140
phonon sidebands (PSBs), 137–140
proteins at low temperatures,
140–141
zero-phonon lines (ZPLs), 137–140,
138f
examples
CP43, 152–154, 153f–154f
cyanobacterial photosystem I,
148–151, 149f
LH2 antenna complex of purple
bacteria, 145–148, 145f, 148f
plant LHC II, 151, 152f
probing electron transfer (ET) times
and their distributions by SHB,
154–156, 155f
experimental techniques
ΔFLN, 141–143
fluorescence line-narrowing (FLN),
141–143
single photosynthetic complex
spectroscopy (SPCS), 143–145,
144f
spectral hole-burning (SHB),
141–143
introduction, 129–130
pigment–protein complexes structure
photosystem I (PS I), 130–132,
131f–132f
photosystem II (PS II), 130–132,
131f–132f
purple bacteria, PCs of, 132–133,
133f
structure–function relationships,
130–133
Photosynthetic excitons, 119–121, 119f
Photosystem II (PS II), 118, 129, 130–132,
131f–132f
Photosystem I (PS I), 118, 129, 130–132,
131f–132f
Phototherapy, 534
Photothermal contrast, 91–92
Photothermal detection, 92
PI, see Propidium iodide (PI)
Piezoelectric transducers, 275
Pigment-protein complexes
concepts, 133–141
excitation energy transfer (EET),
136–137
excitons, 134–136, 135f
homogeneous broadening, 137–140
inhomogeneous broadening,
137–140
phonon sidebands (PSBs), 137–140
proteins at low temperatures,
140–141
zero-phonon lines (ZPLs), 137–140
structure
photosystem I (PS I), 130–132,
131f–132f
photosystem II (PS II), 130–132,
131f–132f
purple bacteria, PCs of, 132–133,
133f
Plant LHC II, 151, 152f

556
INDEX
Plasmodium falciparum, 437
Plasmonic excitations, 91
Plasmon oscillations, 91
PNAs, see Peptide Nucleic Acids (PNAs)
Pockels cells, 275
Poincar´e sphere, 247–248, 247f
Point scanned microscopy, 169–172, 170f
Point-spread function (PSF), 62, 472
Poissonian, 73
Polarimetric instruments, scheme of,
266–267, 267f
Polarimetry, 239. See also Tissue
polarimetry
Polarizance, 250–251, 251f
Polarization, 32, 65
exciting beam, 69
Polarization analysis in detection, 68–69
Polarization filter, 268, 277
Polarization gate, 398–402, 401f
Polarization-gated surface, 297–299, 299f
Polarization modulation in excitation, 69
Polarization-sensitive Monte-Carlo (PSMC)
technique, 283–284, 288–290,
291f
Polarization-sensitive OCT (PS-OCT), 517
Polarization State Analyzer (PSA),
266–267, 267f, 268–269
with continuously rotating polarizers,
271–272
Mueller imager with, 281f
sequential, 271
of two variable retarder, 278f
of variable retarder (VR), 276f
Polarization state generator (PSG),
266–267, 268–269
Mueller imager with, 281f
for OSC imaging, 278f
Polarization structure, 65
Polarized light fundamentals
decompositions into elementary
component matrices, 257–266
forward and reverse product
decompositions, 258–261, 260f
logarithmic decomposition, 261–265
symmetric decomposition, 261
interaction with sample
depolarizer, 255–257
diattenuation, 249–250
homogeneous diattenuators, 252–253
homogeneous retarders, 253–255
Mueller matrix, 248–249
polarizance, 250–251
polarization states
partially polarized states, 242–245,
243f
Stokes vectors, 245–248, 246t
totally polarized states, 240–242,
241f, 241t, 242f
Polarized light tissue imaging, 282
Polarizer
and one variable nonrotating retarder,
276–277, 276f
and two variable nonrotating retarder,
277–279, 278f
Polarizing beam splitters (PBS), 271, 279
Polydiacetylene (PDA) film, 386–387
Poly(ethylene) glycol (PEG), 331
Poly(lactic-co-glycolic) (PLGA)
nanoparticles, 441
Polymyxin B nonapeptide (PMBN), 435
Poly[9,9-dibromohexylfluorene-2,7-
ylenethylenealt-1,4-(2,5-
dimethoxy)phenylene] (PFEMO),
424
Porous silicon (pSi), 348–349, 349f
Porphyrias, 414
Porphyrins, 414
Porphyromonas gingivalis, 440
Posterior capsular opacification, 528
Posterior segment, diagnostic applications
imaging applications, 529–534
Potassium dihydrogen phosphate (KDP),
385
Prevotella intermedia, 440
Process persistent spectral hole-burning
(PSHB), 82
Propidium iodide (PI), 227–228
Propionibacterium acnes, 432, 436
Protoporphyrin IX (PpIX), 425
PSA, see Polarization State Analyzer (PSA)
PSB, see Phonon sidebands (PSBs)
Pseudomonas aeruginosa, 436, 437
PSG, see Polarization State Generator
(PSG)
PSI, see Photosystem I (PSI)
PSII, see Photosystem II (PSII)
PSMC technique, Polarization sensitive
Monte Carlo (PSMC) technique

INDEX
557
PUD, see Peptic ulcer disease (PUD)
Pulse fluorometry, 15, 16f, 17
Pump-probe methods, 86
Pump-probe and photothermal detections,
90–91
electronic response and ground-state
depletion, 92–93
photothermal contrast, 91–92
Pure dephasing, 80
Purple bacteria, PCs of, 132–133, 133f
LH2 antenna complex of, 132–133,
145–148, 145f, 148f
Quadratic coupling, 80
Quantum dots (QDs), 60, 115, 428, 495
Quantum optics, 84
Quantum yield, fluorescence, 6
experimental, 7–8
theory, 6–7
ThT detection of sheet structure, 8–12
Quarter-wave plate (QWP), 242
Quenching, fluorescence, 23
metal ion quenching, 29–30
theory, 23–29
QWP, see Quarter-wave plate (QWP)
Radiative transport theory, 283, 284
Raman scattering, 67, 86, 471
Raman scattering microscopy, 180
Rayleigh length, 63
Rayleigh scatterers, 286–287
Ray optics model, 199–200, 199f
RCs, see Reaction centers (RCs)
Reaction centers (RCs), 129
Reactive oxygen species (ROS), 229–230
Reconvolution analysis of fluorescence
decay, 20f
Red antenna states, 148
Redfield theory, 107
Reduced scattering coefficient, 373
Reference arm, 334
Refraction, 527–528
Refractive index, 167–168
Refractive index unit (RIU), 326
Regarding interferometric biosensors,
357
Reorganization energy, 107
Reproducibility, biosensor, 331
Residual anisotropy, 33
Resonance energy transfer (RET)
applications in molecular biology,
113–117
conformational change, 114–115,
114f
intensity-based imaging, 115–116
lifetime-based imaging, 116
spectroscopic ruler, 114
chemical applications of, 116–117
history
developments of theory, 102
first experiments, 102
F¨orster theory, 103
introduction, 101–102
nanofountain, 116, 117f
photophysics of
Dexter transfer, 113
diffusion effects, 111–112
dissipation, 105–107
electronic transitions coupling,
104–105
F¨orster equation, 107–108, 108f
line broadening, 105–107
long-range transfer, 112–113
orientation dependence, 108–109,
109f
polarization features, 109–111
primary excitation processes,
103–104
role in light-harvesting complexes
introduction, 118
photosynthetic excitons, 119–121,
119f
Response time, biosensor, 331
RET, see Resonance energy transfer (RET)
Retardation plates, 272
Retention effect, 419
Retina, 506–507
photoreceptors, 507–513
superficial retinal layers, 513–514
Retinal pigment epithelium (RPE), 506
RET microscope, 115f
Reverse product decompositions, 258–261,
260f
Reversible photobleaching microscopy
(RPM), 482
Rhodamine 6G, 34, 34t
Rhodopseudomonas acidophila, 121, 145f
Ring resonator biosensor, 339f

558
INDEX
RIU, see Refractive index unit (RIU)
Rods and cone, 509
ROS, see Reactive oxygen species (ROS)
Rotational diffusion, 68f, 75
ROXS, 70, 71
RPE, see Retinal pigment epithelium
(RPE)
Saccharomyces cerevisiae, 198, 210, 210f
Scalar radiative transport theory, 283
Scaling and rooting planing (SRP), 441
Scanning and confocal apertures, 514–515
Scanning confocal microscopes, 41f, 395
Scanning laser ophthalmoscope (SLO), 514
Scanning tunneling microscope (STM), 60
Scattering, 422
Scattering mean free path, 372, 372f
Scattering phase function, 288
Scientific CMOS, 478
SDF, see Site-distribution function (SDF)
Second harmonic generation (SHG), 177,
178f
Selective laser trabeculoplasty (SLT), 528
Selectivity, biosensor, 331
Semiconductor excitation, 16
Semiconductor laser diode (LD), 16
Sensing arm, 334
Sensitivity, biosensor, 331
Sensitizer, 102
Sequential polarization modulation, tissue
polarimetry, 267–268
Sequential PSAs, 271
with continuously rotating retarder,
274–275
with linear polarizer, 272–275
with removable QWP, 272–273
with rotatable retarder, 273–274
Severe combined immunodeficient (SCID),
448
Seya-Namioka geometry, 4
SHB, see Spectral hole-burning (SHB)
SHG, see Second harmonic generation
(SHG)
Signal-to-background ratio, 86
Signal-to-noise ratio (SNR), 66–67, 87,
267, 473
Silicon-based biosensors, 347, 347f
Silicon nitride BiMW interferometers, 357
Silicon-on-insulator (SOI)-based structures,
326
SIM, see Structured illumination
microscopy (SIM)
Similarity principle, 284
Single-mode fiber, 327, 327f
Single molecule fluorescence microscopy,
471
Single-molecule optical detection
correction of aberrations, 64–65
microscopy methods
confocal microscopy, 65–66
total internal reflection illumination,
66
wide-field imaging, 66
polarization structure, 65
principles, 62–64
Single-molecule optics, 60
Single-molecule spectroscopy, 82–84
external field effects, 84
interacting single molecules, 85–86
optically detected magnetic resonance,
84
quantum optics, 84
spectral diffusion, 84–85
Single nano-object spectroscopy, 63f
Single-photon excitation, 173f
Single photosynthetic complex
spectroscopy (SPCS), 130,
143–145, 144f
Single plane illumination microscopy
(SPIM), 186–187, 187f
Singlet oxygen, 416
Singlet-oxygen sensor green (SOSG), 230
Site-distribution function (SDF), 142
Skin melanoma metastasis, 431
Slot waveguide-based biosensors, 347–348,
347f–348f
Small-cell lung cancer (SCLC), 424
Smoluchowski’s solution, 26
Snake photons, 370–371, 370f, 373
SNR, see Signal-to-noise ratio (SNR)
SOFI, see Super-resolution optical
fluctuation imaging (SOFI)
Solvent refractive index, 8
SOSG, see Singlet-oxygen sensor green
(SOSG)
Space-gated microscopic imaging, 396f

INDEX
559
Spatial light modulator (SLM), 484
SPCS, see Single photosynthetic complex
spectroscopy (SPCS)
Spectra, fluorescence, 2
experimental, 4–5
fluorimeter schematic for, 4f
melanin spectra, 5–6
theory, 2–4
Spectral diffusion, 80, 84–85
Spectral domain OCT (SD-OCT), 517
Spectral hole-burning (SHB), 141–143
Spectral overlap, RET, 105, 106f
Spectral or spatial modulation, tissue
polarimetry, 269
Spectrophotometer, 5
Spectroscopic ruler, 114
SpectroSens, 345
SPIM, see Single plane illumination
microscopy (SPIM)
Spin coating, 67
Spin multiplicity (Ms), 415
Spring-like zonules, 506
SPT, see Surface patterning tool (SPT)
Squamous cell carcinoma (SCC)
field cancerization, 429
SSC intensity, see Orthogonal scatter (SSC)
intensity
Staphylococcus aureus, 440
infections, 443
Static quenching, 23–24, 24f
Stern–Volmer constant, 25
Stern–Volmer equation, 23, 25
Stimulated emission depletion microscopy
(STED), 43, 189, 189f, 474–475,
475f
applications of, 476–477
Stochastic optical reconstruction
microscopy (STORM), 473,
477
dSTORM, 478
Stokes–Einstein equation, 32
Stokes shift, 4
Stokes vectors, 245–248, 246t, 268
geometrical representation, 247f
physical realizability, 246
Poincar´e sphere, 247–248, 247f
Streak camera, 385
Strickler–Berg expression, 14
Structural tissue anisotropy, 307–313, 309f,
311f
Structured illumination microscopy (SIM),
187–188f, 187–189, 483–490,
487f
applications, 490–491
Sub-diffraction-limited imaging, 189–190
Subsurface imaging, 297–299
Superficial retinal layers, 513–514
Superresolution, 71–72
Super-resolution optical fluctuation imaging
(SOFI), 491–494
Bayesian analysis of blinking and
bleaching, 494
fluctuation-based superresolution
microscopy, 494–495
Super-resolution optical microscopy, 470,
471f, 473
Surface biofunctionalization
bioreceptors, 328–329
immobilization techniques, 329–331
for multiplexing, 331
Surface patterning tool (SPT), 331
Surface plasmon resonance (SPR), 45
Surface sensing, 332
Symmetric decomposition, 261
Synechocystis, 148–150
Synthetic receptors, 329
Tapered fiber, 327, 327f
Targeted PDT (TPDT), 425
TDC, see Transition density cube (TDC)
Tear film, 504
Tetracationic mesoarylsubstituted porphyrin
(RM24), 436
Tetraphenylporphyrin (TPP), 424
Thermal lensing, 8
Thermosynechococcus elongatus, 148–150,
149f
Thioflavin T (ThT)
fluorescence, 10f, 12f
Third harmonic imaging (THG), 177–178
Three-factor product decompositions,
258–261, 260f
ThT detection of sheet structure, 8–12
Time-correlated single-photon counting
(TCSPC), 15–18
Time gate, 385–391, 386f

560
INDEX
Time-to-amplitude converter (TAC), 17
Tip-enhanced spectroscopies, 470
TIRs, see Total internal reflections (TIRs)
Tissue assessment with Mueller polarimetry
cancer diagnosis, 302–307, 304f, 306f
characterization of structural tissue
anisotropy and applications,
307–313, 309f, 311f
noninvasive glucometry, 300–302,
301f–302f
Tissue characterization/diagnosis, 282
Tissue optics, 422–423
Tissue polarimetry
applications
polarization-gated surface, 297–299,
299f
subsurface imaging, 297–299
tissue assessment with Mueller
polarimetry, 300–313
experimental testing and validation,
291–297
forward model, 291–292, 292f
inverse polarimetric analysis,
292–296, 293f, 294t
forward modeling of polarized light
propagation in tissue, 282–290
overview, 282–284
photon diffusion formalism, 284–288
polarization-sensitive Monte-Carlo
(PSMC) approach, 288–290
instrumentation
examples, 279–281
principles, 266–271
PSAs, 271–279
introduction, 239–240
polarized light fundamentals
decompositions into elementary
component matrices, 257–266
interaction with sample, 248–257
polarization states, 240–248
TLS, see Two-level systems (TLS)
tmRNA, see Transfer-messenger RNA
(tmRNA)
Toluidine blue O (TBO), 436
Total internal reflection, 88–89
Total internal reflection fluorescence (TIRF)
microscopy, 40, 478–479
Total internal reflection illumination, 66
Total internal reflections (TIRs), 325
Totally polarized states, 240–242, 241f,
241t, 242f
Total retardation, logarithmic
decomposition, 264
Transducers, 323
Transfer-messenger RNA (tmRNA), 340
Transfert d’activation, 102
Transition density cube (TDC), 111–112
Transition dipole moment, 108
Translational diffusion, 74–75
Transport mean free path, 372–373, 372f
Transverse electric (TE) polarized guided
light, 325–326
Transverse magnetic (TM) polarized guided
light, 325–326
Triplet–triplet transitions, 70
Tube lens, 64
Tumors, PDT on, 448
Twisted intramolecular charge transfer
(TICT), 10
Two-dimensional electronic spectroscopy
(2D-ES), 130
Two-level systems (TLS), 141
Two-photon absorption (TPA), 423
Two-photon Bessel beam excitation, 490
Two-photon excitation, 173f
Two-photon excitation-PDT (TPE-PDT),
423
Type II photochemistry, 417
Uterine cervical cancer diagnosis, tissue
polarimetry, 305
Variable retarders, 280, 281f
Vector radiative transfer equation (VRTE),
283
Verteporfin, 424
Vertical Cavity Surface Emitting Lasers
(VCSEL), 531
VIA, see Visual Inspection with Acetic acid
(VIA)
Vibronic states, 79
Viral infections, 443–444
Visual inspection with acetic acid (VIA),
305–306
Visual system, 511
Vitreous body, 506
VRTE, see Vector radiative transfer
equation (VRTE)

INDEX
561
Wavefront aberration measurement and
compensation, 517–521
Wavefront sensing, 518f
Waveguide materials, 326
Wavelength Interrogation Optical Sensor
(WIOS), 343–344
WGMs, see Whispering gallery modes
(WGMs)
Whispering gallery modes (WGMs),
339–341
Wide-field fluorescence microscopy, 483
Wide-field imaging, 66
WIOS, see Wavelength interrogation optical
sensor (WIOS)
Xenon source, 4
Xenopus laevis oocytes, 483
Young interferometer, 337–338, 337f
Zero-phonon lines (ZPLs), 79–81, 137–140,
138f
and phonon wing, 78–80
ZPLs, see Zero-phonon lines (ZPLs)

l-DOPA
NH2
OH
HO
HO
0
50
100
150
200
250
300
Polymerization time (min)
0
50
100
150
200
250
300
500
450
400
0
100
200
300
400
Polymerization time (min)
700
600
500
400
Wavelength (nm)
Wavelength (nm)
300
0.0
0.2
0.4
0.6
Absorption
Emission intensity (a.u.)
excitation at 275 nm
0.8
1.0
O
(a)
(b)
FIGURE 1.3
(a) Absorption and (b) fluorescence spectra as eumelanin is polymerized from the auto-oxidation of
an aqueous solution of l-DOPA at pH 12. The fluorescence excitation wavelength is 275 nm.
+ MeOH
– MeOH
200
Counts
100
0
500
600
Wavelength (nm)
700
800
Monomer
Trimer
FIGURE 1.19
(a) Scanning confocal microscope images of single allophycocyanin (APC) molecules entrapped
in hydrated pores of a thin silica film fabricated around the protein using the sol-gel process. The excitation wavelength
is 532 nm. (b) The fluorescence spectra for the trimer (red) and monomer (blue) single molecule forms are shown.
Dissociation of the trimer is induced by alcohol. Each APC unit contains two phycocyanobilin fluorophores, which are
indicated by red dots. Reproduced from Reference 134 with permission from Springer Science + Business Media.
FIGURE 10.5
Bioluminescent MRSA infection in mouse skin abrasion. Successive bioluminescence images of a
representative mouse skin scratch model infected with 108 CFU MRSA. Reprinted with permission from Reference 53.
Photonics: Scientific Foundations, Technology and Applications, Volume IV, First Edition.
Edited by David L. Andrews.
© 2015 John Wiley & Sons, Inc. Published 2015 by John Wiley & Sons, Inc.

(c)
(a)
(b)
400
500
600
700
800
900
1000
1100
0.0
0.5
1.0
1.5
2.0
Absorption (a.u.)
Wavelength (nm)
L,2
L,3
L,4
L,5
T
0.80
0.85
0.90
0.95
1.00
1.05
3.1
3.2
3.3
3.4
3.5
3.6
Log Intenstiy (a.u.)
Log (power) (mW)
FIGURE 1.22
Gold nanorods: (a) SEM measurement. (b) Emission intensity dependence on laser excitation power
giving a slope of 1.97, confirming two-photon excitation at 750 nm. (c) Plasmon absorption bands showing transverse
(T) and longitudinal (L, AR) modes for aspect ratios (AR) of 2, 3, 4, and 5.
25 ps
3.5 ns
(a)
(b)
(c)
10 μm
FIGURE 1.23
Two-photon excited MDCK cells stained with DAPI: Fluorescence intensity image (a) without gold
nanorods and (b) with gold nanorods. (c) FLIM image. Images are obtained using two-photon 750 nm excitation in the
longitudinal plasmon band with emission collected over 535–590 nm. The scanning areas are (a) 133 μm × 133 μm, for
(b) and (c) 67 μm × 67 μm 173.
10 μm
A
B
0
2
4
6
8
10
10
–2
10
–1
10
0
DAPI
Spot A
Spot B
Intensity (a.u.)
Lifetime (ns)
25 ps
3.5 ns
(a)
(b)
FIGURE 1.24
(a) Two-photon excited FLIM image of GNRs incubated in MDCK cells. Experimental conditions
are as for Figure 1.23, but with a scanning area of 67 μm × 67 μm. (b) Normalized fluorescence decay curves derived
from regions A and B, with the fluorescence decay curve of DAPI shown for reference. Reproduced from Reference 176
with permission from AIP Publishing LLC.

FIGURE 2.2
Example of rotational diffusion of single dye molecules in glycerol at low temperature (200 K),
monitored via intensity fluctuations in two orthogonal fluorescence polarizations (left panels). On the right panel, the
polarization is color-coded, red for vertical, green for horizontal. The diffusion times of molecules are comparable to the
scan time, about 1 s per line.
FIGURE 3.8
Scheme for an optical nanofountain composed of 2–10 nm CuCl quantum dots, distributed in an
NaCl matrix.
700
600
500
Wavelength (nm)
LHC-II
PC612
400
PE545
PE555
PC645
FIGURE 3.9
Normalized absorption spectrum of LHC-II (green); the main light-harvesting protein in high order
plants along with the normalized absorption spectra of four cryptophyte light-harvesting proteins: PE545 (blue), PE555
(pink), PC612 (purple), and PC645 (red). The different region in which these complexes absorb is clearly evident.

FIGURE 3.10
Structural models of two photosynthetic light harvesting complexes: (a) a cryptophyte PC645
antenna complex, and, (b) the major chlorophyll-a/b complex from higher plants and green algae, LHCII.
B850
B850
B800
B800
700
ea
g
g
g
ea
eb
2Vab
eβ
eα
800
Wavelength (nm)
chromophore a
chromophore a
chromophore b
900
FIGURE 3.12
(a) Absorption spectrum of LH2 extracted from a strain of the purple bacterium Rhodopseudomonas
acidophila. The B800 ring absorbs at 800 nm and is due to the nine weakly coupled BChl chromophores, while the B850
ring absorbs at 850 nm and is due to the 18 tightly packed BChl molecules. (b) X-ray crystallography structure of LH2
from Rps. acidophila (strain 10050) illustrating 27 bacteriochlorophyll-a chromophores. Blue-colored chromophores
indicate the B800 ring, while red-colored chromophores indicate the B850 ring. Energy-level diagrams for chromophores
in the (c) B800 ring and (d) B850 ring. Adapted from References 101 and 102.

FIGURE 4.1
(a) Photosystem II dimeric core, 3ARC.pdb [3]; peripheral light-harvesting complexes not depicted.
(b) Plant Photosystem I, 3LW5.pdb [4]. Gray/green: Chl a; dark blue: Pheo a; yellow: carotenoids; red: quinones; orange:
non-heme iron (PS II); orange/yellow: FeS clusters (PS I). Both complexes are depicted with stromal side facing the
viewer. This and subsequent structure figures were produced with RasMol software.
FIGURE 4.2
(a) Arrangement of pigments in active (D1) and inactive (D2) branches of the PS II RC; 3ARC.pdb
[3]. Green: Chl a; dark blue: Pheo a; yellow: carotenoids; red: quinones; orange: non-heme iron; brown/purple: oxygen-
evolving cluster, (b) Pigment arrangement in BRC, 1M3X.pdb [5]. Green: BChl a; dark blue: BPheo a; yellow: carotenoids;
red: quinones; orange: non-heme iron. Gray wireframe represents protein and some lipids.
FIGURE 4.3
(a) The BRC-LH1 complex structure, 1PYH.pdb [10]. PufX subunit is not depicted, but its location
is indicated. The complex is depicted with H subunit (see Fig. 4.2) toward the viewer. (b) LH2 structure, 2FKW.pdb [6],
depicted with the cytoplasm side up. In both frames, gray/green: Bchl a; yellow: carotenoids.

1
2
Frequency - Wavelength
Γinh
Absorption
2
FIGURE 4.5
Pigments dispersed in an amorphous solid matrix and the SDF (of width Γinh) describing the
probability to find a molecule with given zero-phonon transition frequency; see text. Adapted with permission from
Reference 11. Copyright (2011) American Chemical Society.
Cryostat
MO
MM
LP
DM
APD
FM
IM
SPEC
35 μm
35 μm
50
100
Fluorescence counts
per 0.2 s
150
CCD
EP
(b)
(a)
FIGURE 4.8
(a) Scheme of the confocal microscope used for individual complex spectroscopy. EP is excitation
pinhole, DM is dichroic mirror, MM is motorized mirror, MO is microscope objective, LP is long-pass filter, and FM is
flipping mirror. APD itself and the entrance slit of the imaging spectrograph (IM SPEC) effectively comprise the detection
pinholes. (b) Raster-scan image of the thin film containing single PS I complexes from Synechocystis PCC 6803 (red
peaks) [77]. Complexes were excited with 250 nW μm−2 (25 W cm−2) at 680 nm and fluorescence was collected at
> 700 nm. T = 10 K. Adapted with permission from Reference 77. Copyright (2007) American Chemical Society.
B800
B850
750
800
850
900
785
795
Wavelength (nm)
AE1
A
E3
E2
E1
x
*
805
40
30
Time (min)
20
10
0
FIGURE 4.9
Left: Absorption spectrum of LH2 complex of Rhodopseudomonas acidophila and the scheme of
excitonic levels from Reference 91. Asterisk indicates overlapping E4 level of the lower manifold and E2 of the upper
manifold; x indicates overlapping E3 of the lower manifold and E4 of the upper manifold. The frame also contains a 1.5 K
single-complex emission (blue) and fluorescence excitation spectra of LH2 from Rb. acidophila. Adapted with permission
from Reference 71. Copyright (2012) American Chemical Society. Right: The B800 region of the single-complex spectra,
exhibiting both small and large spectral shifts due to conformational changes involving different tiers of the protein energy
landscape. Adapted with permission from Reference 66. Copyright (2004) IOP Publishing.

120
0
40
80
120
0
30
60
100
I
I
I
I
I/III
II
II
II
80
60
40
20
10
20
30
40
50
60
0
30
60
10
20
30
40
50
60
650
700
750
Wavelength (nm)
Illumination time (s)
FL (cps)
650
700
750
Wavelength (nm)
650
700
750
650
700
750
650
700
750
650
700
750
Wavelength (nm)
Wavelength (nm)
(a)
(b)
(c)
Wavelength (nm)
Wavelength (nm)
FIGURE 4.12
Left: The structure of the LHC II trimer based on Reference 9. Highlighted (groups of) pigments
were considered as possible origin of the lowest energy band of LHC II. Adapted with permission from Reference 103.
Copyright (2011) American Chemical Society. Right: Examples of single RT LHC II fluorescence spectra and their
evolution. Adapted with permission from Reference 85. Copyright (2010) Elsevier.
0.0
0.5
1.0
0.25
0.5
0.75
1
0.00001
660
670 680
690
0
10
–2.104
–3.104
–4.104
20 30 40
Wavelength (nm)
Frequency (GHz)
Irradiation dose (J cm–2)
Fractional hole depth
Relative hole depth
Absorbance
Δ Fluorescence (cps)
0.001
0.1
1
100
10000
Recovery time (s)
FIGURE 4.13
Left: The 5 K HGK curve and its fit, yielding the parameters of the excited-state barrier distribution;
𝜆B = 686 nm. The insert contains the absorption spectrum of CP43 with the SDF of the two lowest energy states, A (red)
and B (blue) [105]. Right: Hole-recovery data for 20%-deep hole. Smooth solid curve (good fit to experimental data)
results from modeling in Gaussian 𝜆-distribution framework (see left frame of Fig. 4.14), while dashed curve results
from modeling in uniform 𝜆-distribution framework (see right frame of Fig. 4.14). The insert represents a sample hole
spectrum. Adapted with permission from Reference 43, 103. Copyright (2011, 2012) American Chemical Society.
a
a
b
b
5
7.5
10
12.5
8
Excited state λ
(a)
(b)
Probability (a.u.)
9
10
11
FIGURE 4.14
(a) Calculated excited-state partial 𝜆-distributions for Gaussian true full 𝜆-distributions [43] (black:
20%-deep hole, a; blue: 55%-deep hole, b). Areas under curves are normalized to same value. (b) Same for uniform
𝜆-distribution. Dashed lines represent respective full true 𝜆-distributions. Adapted with permission from Reference 43.
Copyright (2012) American Chemical Society.

660
670
680
684
a
682 nm
684 nm
688 nm
686 nm
b
c
d
(a)
(b)
2.8
ps
690
0
0
0.5
1
1.5
0.1
0.2
0.3
0.4
Wavelength (nm)
Fractional depth
Δ Absorbance
Hole width (cm–1)
FIGURE 4.15
(a) Resonant transient HB spectra for Chlamydomonas reinhardtii; spectra a, b, c, and d were
obtained with 𝜆B of 682.0, 684.0, 686.0, and 688.0 nm, respectively. Adapted with permission from Reference 114.
Copyright (2012) American Chemical Society. The inset corresponds to the Lorentzian fit (black curve) of ZPH of curve
b. (b) Persistent hole width versus depth data for 𝜆B = 686 nm (circles) and 680 nm (triangles) obtained for isolated
PS II RC from spinach, and modeling results with the CS time distribution (dashed curve), without any distribution
(dotted curve), and with best distribution (solid curve, see text for details). Adapted with permission from Reference 108.
Copyright (2011) American Chemical Society.
FIGURE 5.1
Basic optical configuration of a confocal imaging system.
FIGURE 5.6
A combined SHG and multiphoton image of yellow fluorescent protein illustrating the advantage
of a combination imaging method. The SHG provides the biological context for the structure and position of the YFP
expressing pericytes. Image courtesy of Dr. Paul Thomas, Henry Wellcome Laboratory for Cell Imaging, University of
East Anglia.

(c)
(a)
(b)
0.20
0.18
0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0
1
2
Number of spheres
Qlateral
Qlateral
5
6
7
0.20
CHO
RPE
HL60
C2GM
CHO
RPE
HL60
C2GM
0.18
0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0
1
2
Number of spheres
Number of spheres
5
6
0
0
5
10
15
20
25
30
35
1
2
3
4
5
7
Guiding velocity (   m s–1)
FIGURE 6.5
Effect of dielectric tagging. Lateral Q-values of different cell types containing different amounts of (a)
2-μm and (b) 3-μm polymer microspheres. (c) Axial cell guiding data, displaying maximum guiding velocities for cells
containing different numbers of microspheres per cell. Cells with no spheres displayed no axial guiding. CHO, Chinese
Hamster ovary cells; RPE, retinal pigment epithelial cells; human, promyelocytic leukemia cells; C2GM, hematopoietic
stem-cell-like cells. Adapted from Reference 16.
Long-pass
5 μm
1
2
785 nm
(a)
(b)
3
Pinhole
Grating
CCD
FIGURE 6.8
Setup for single-cell laser tweezers Raman spectroscopy (LTRS). Adapted from Reference 42. (a)
Brightfield images of individual trapped microalgal cells: 1, Neochloris oleoabundans; 2, Botryococcus braunii; and
3, Chlamydomonas reinhardtii. (b) Instrument layout. The 785-nm laser beam is used as both trap beam and Raman
excitation beam. PNAS.

105
1.12
104
104
105
103
103
0
0
<APC-CY7-A>: CD4
<PE-A>: FOXP3
Charged
metal
plates
Singlet
cells
Sorted cell
droplets
High voltage
amplifiers
Computer
Preamplifier
Pt electrodes
Photomultiplier tube
Beam splitter
VIDEO
Dichroic
CCD camera
Focusing optics
Coherent innova 
Ar laser 488 nm
60X, 1.4 N.A.
oil immersion lens
Microfabricated 
cell-sorting
device
(a)
(b)
SSC
FSC
FL1
Excitation
beam
FIGURE 6.11
(a) Schematic of a conventional flow cytometer (including FACS) that monitors fluorescence,
forward and side scattering (inset shows sample data where FOXP3+ CD4+ regulatory T-cells are identified from a
mixture of lymphocytes). Adapted from Reference 70. (b) Cytometer using a disposable microfabricated flow chamber
(inset) in a fluorescence microscope with a photomultiplier tube for FACS (μFACS). Adapted from Reference 73. Journal
of immunology.
(a)
(b)
Inlet
CCD
Cell mixture
Detected
Ignored
Matched filter
Objective
Outlet
(waste)
Outlet
(collection)
Inlet
: Non-target cell
: Target cell
: Optical trap
Optical tweezers
Sample flow
Butter flow
ROI
Trap
direction
FIGURE 6.12
Image-based parallel cell identification and optical sorting. (a) Transverse force used for lateral
displacements. Adapted from Reference 79. (b) Axial force used for optical propulsion. Adapted from Reference 82.

(a)
(b)
Optical lattice
Flow
Flow
5 μm
6.84 μm
5.17 μm
3.0 μm
2.3 μm
FIGURE 6.14
Optical fractionation in an optical landscape. (a) Periodic landscape: a larger particle flows with
minimal deflection through the optical lattice while smaller particles are angularly deflected. Adapted from Reference 2.
(b) Experimentally calibrated pattern generated using an acousto-optic modulator: Particles flowing from the right are
guided to the bottom of the figure by the line trap and eventually separate based on size as the calibrated gaps and
intensities of the subsequent pattern are only able to drag the largest particles to the top of the figure. Adapted from
Reference 87. Nature Materials, Optics letters.
FIGURE 7.3
Scattering experiment of a linearly polarized coherent beam by static samples. Top: single scattering
by a thin sample. The polarization state of the speckle spots is the same as that of the incident beam throughout the
speckle pattern. Bottom: multiple scattering by an optically thick sample. The polarization state varies from speckle to
speckle.
Wavelength (nm)
O.D.
FIGURE 9.8
Absorption spectra of Hb, HbO2, and H2O.

Grazing
view
–0.2
0.1
0.4
FIGURE 7.5
Normalized Mueller matrix image of a resected human cervix in backscattering geometry. The field
of view is 3 × 3 cm2. The sample is about 5 mm thick and has conical shape. All elements are normalized by m11 and
these normalized values are given in the color scale shown at top right, ranging from −0.2 to 0.4. This sample features
no diattenuation, as shown by the vanishing elements in the first row, and significant linear polarizance (elements m21
and m31) at some points where the light emerges at grazing angles from the sample surface, and is polarized by a local
“Brewster angle” effect (A. de Martino, unpublished results).
Rev.
LC
0
0.07
0.7
Δ
1
D
FIGURE 7.6
Polarimetric images of a pig skin sample in backscattering geometry taken at 600 nm. Field of view
5 × 5 cm2. Top: diattenuation and depolarization images obtained with the reverse decomposition via Eq. (7.41). Bottom:
diattenuation and depolarization images obtained with the Lu-Chipman decomposition. Color bars give the scales for
scalar diattenuation D (left) and depolarization power Δ (right). Adapted from Reference 35.

–1
0
+1
1
–0.031
0.003
–0.006
–0.021
0.768
–0.037
0.020
–0.005
0.023
0.104
–0.774
0.001
0.039
0.797
0.192
1
–0.004
0.002
–0.001
0.001
0.774
0.031
–0.039
–0.006
–0.043
0.123
–0.795
–0.001
–0.025
0.814
0.1
+1
0
–1
(a)
(b)
FIGURE 7.17
(a) The experimentally recorded Mueller matrix in the forward (transmission) detection geometry
from a birefringent (extension = 4 mm, corresponding to a value of linear retardance 𝛿= 1.345 rad for 1 cm path length),
chiral (concentration of sucrose = 1 M, 𝜒= 1.96◦cm−1) turbid (𝜇s = 30 cm−1, g = 0.95) phantom of thickness of 1 cm.
(b) The corresponding Mueller matrix generated through the PSMC model with input parameters: linear birefringence
Δn = 1.36 ×10−5 (corresponding to 𝛿= 1.345 rad), 𝜒= 1.96◦cm−1, 𝜇s = 30 cm−1, g = 0.95. Adapted from Reference 3.
0
–0.03
–0.00162 0.000613
–0.03
0
–00596
–0.0153
0.00162
0.0596
0
–1.39
0.000613 0.0153
1.39
0
0
–0.00554
0.00554
0.0053
–0.0053
0.00491
–0.263
0.00012
0.00353
0.00012
–0.301
0.0214
0.00491 0.00353
0.0214
–0.148
–1
–0.5
0
0.5
1
–0.1
–0.15
–0.2
–0.25
–0.3
–0.5
0
1
–0.0312
–0.0214
0.0029
–0.0055
–0.0066
0.768
–0.037
0.0204
0.023
0.104
–0.773
0.0014
0.039
0.797
0.192
0.4
–0.4
–0.6
+1
0.5
0
+1
0
–1
+1
0
–1
MD
MR
MΔ
Lm
Lu
0.2
0
–0.2
1.8
1.6
1
0.755
0.769
0.845
0.998
1
1
0.184
–0.051
0.033
–0.982
0.981 0.186
–0.031
0.031
1
1
0.999
0.999
FIGURE 7.18
The experimentally recorded Mueller matrix in the forward detection geometry (top panel, repro-
duced from Fig. 7.17a). The basis matrices (MΔ, MR, MD) obtained following the forward polar decomposition process
(middle panel). The matrix logarithms Lm and Lu, derived using the logarithmic decomposition (bottom panel). The
estimated polarization parameters using both the polar decomposition and the differential matrix approaches are listed in
Table 7.3. Adapted from Reference 44.

FIGURE 7.22
(a) Photo of the colon sample, with healthy (H) tissue, burgeoning (B) and ulcerated (U) cancerous
regions. (b) Depolarization image (values given by the color bar from 0 (blue) to 1 (red)) of the same sample. The solid
line indicates the depth cut which has been studied histologically. In this cut, shown in panel, (c), the regions marked
1 (healthy), 2 (burgeoning), 3 and 4 (ulcerated) are seen, together with the staging of the lesions, from T1 to T3. The
different layers are identified by M (mucosa), SM (submucosa), MP (muscularis), and P (pericolic tissue, or serosa). (S)
and (C) designate stromal and cancerous tissues, characterized by low and high cell densities, respectively. Adapted from
Reference 99.
FIGURE 8.9
Working principle of the ring resonator biosensor.

CIN 3
Gland.
Intensity images
0°
25°
50°
0.4
0.7
1
δ
Δ
Healthy
Healthy
FIGURE 7.23
Images of two ex vivo cervix samples, one healthy (top) and the other one with a CIN3 zone
and a benign lesion (visible glandular tissue) (bottom). Left column: raw intensity images. Middle column: images of
scalar retardance 𝛿, in color scale from 0◦to 50◦. Right column: images of depolarization Δ, from 0.4 to 1. Retardance
and depolarization were obtained from raw Mueller images by standard Lu-Chipman decomposition procedure of the
experimentally determined Mueller matrix images. On each sample, the solid black line shows the limit of the intact
epithelium. The straight lines indicate the locations of the histological cuts where the pathology diagnostic was made
(white for healthy, purple for CIN3, and orange for glandular tissue). The dashed black lines designate regions where at
least one among the 16 raw intensity images was saturated due to tissue “glare.” The top left image shows an example of
such saturated region, shown in red. Obviously, polarimetric data in these regions are suspect. Adapted from Reference 64.
Δn
Bladder3, distended
at 3.3 kPa
Bladder2, distended
at 2.2 kPa
Bladder1, distended
at 1 kPa
Dome
6
5
4
3
2
1
× 10–5
Dorsal
ventral
FIGURE 7.24
Variation of the local birefringence of ex vivo rat bladder tissue with pressure. The left, middle,
and right columns show the retardance images of three different regions of the organ (dome, dorsal, and ventral surfaces,
as shown at the extreme left of the figure) for distending pressures equal to 1.0, 2.2, and 3.3 kPa, respectively. The field
of view is 2 mm in diameter. The images were taken in backscattering geometry, at 25◦from the exact backscattering
direction, and processed by the polar decomposition (see Section 7.2.3.1) to extract the retardance parameters, namely
the scalar retardance and the anisotropy orientation, the latter being shown by arrows. The tissue birefringence, shown in
the color bar scale of Δn at the extreme right, was calculated from the scalar retardance and the tissue effective thickness.
This thickness was derived from OCT measurements and evaluation of the average path length of the photon trajectories
due to the multiple scattering events by Monte Carlo simulations. Note the maximum birefringence in the dome region
upon maximum distension, and the overall longitudinal anisotropy orientation (along the urethra-dome axis). Adapted
from Reference 108.

−1
3.5
5
2.5
2
1.5
1
0.5
0
1E – 13
1E – 11
1E – 9
1E – 7
DNA (M)
1E – 5
1E – 3
−2
−3
Signal (a.u.)
Signal (a.u.)
−4
−5
−6
70
(a)
(b)
75
80
85
90
95
Time (min)
1 μM non complementary DNA
100 nM complementary DNA
100
105
110
115
1 μM complementary DNA
Complementary
Non-complementary
FIGURE 8.5
(a) Real-time DNA hybridization signal corresponding to 1 μM and 100 nM complementary DNA,
and 1 μM noncomplementary DNA; (b) calibration curve for the hybridization of complementary (red line) and noncom-
plementary (blue line) DNA (58 mers). Reprinted from Reference 31 under the Creative Commons Attribution License.
FIGURE 8.10
(a) Microring sensor responses to increasing concentrations of BPMV and (b) corresponding
calibration curve. The blue “X” indicates the response from the 1:200 dilution of the extract from a BPMV infected
sample. (c) Microring sensor response to diluted extracts from soybean leaves. Red: 1:200 dilution of the leaf extract
from a BPMV infected sample. Blue: 1:200 dilution of the leaf extract from a healthy sample. Arrow indicates “on” and
* indicates “off” (entrance and exit of the sample solution, respectively). Adapted from Reference 58 with permission
from Elsevier.
FIGURE 8.12
Schemes of grating-based biosensor configurations: (a) angular interrogation, (b) wavelength
interrogation with grating as coupler, and (c) wavelength interrogation with grating as reflector.
(a)
Buried oxide
Electrical field
E
Si
Aqueous solution
0.26 μm
50 μm
(b)
FIGURE 8.15
(a) Cross-section of a 0.26 μm × 0.45 μm SOI photonic wire waveguide and the corresponding
electric field distribution for TM polarization. (b) Top view optical image of an SOI wire-based ring resonator. Adapted
from Reference 76 with permission from OSA.

2.67 K X
SUDESHNA
(a)
10.00 kV
Pixel Size = 41.8 nm
20 Jan 2010
Photo No. = 1927
SE2
6.2 mm
2 μm
1480
(b)
Transmission (a.u.)
1500
1520
Wavelength (nm)
1540
After lgG
Before lgG
1560
FIGURE 8.14
(a) SEM image of multiple nanocavities coupled to PC waveguides (a = 372, 380, and 388 nm,
defect radius = 73, 75, and 77 nm) in series. (b) Experimental transmission spectra of the structure before and after target
(IgG) binding. Reprinted from Reference 72 with permission from Elsevier.
(a)
(b)
(c)
nc
ns
nH
nH
Wslot
Wslot
200 nm
600 nm
y
Ex mode profile 
Sensing
window
Light
in
Light
out
Ring
Bus
SiO2 top cladding
Si3N4
Si3N4
Si3N4
Si3N4
z
x
−0.4
−0.4
−0.3−0.2
−0.2
−0.1 0.0
SiO2
Si
Si
0.0
X (μm)
70 μm
Y (μm)
0.1 0.2
0.2
0.3 0.40.0144905
1.0
0.4
FIGURE 8.16
(a) Scheme of a slot waveguide. (b) Calculated Ex profile in an Si/SiO2 slot at 1.55 μm highlighting
the strong confinement of light in the slot region. (c) Left: top view photograph of a 70-μm radius Si3N4 slot waveguide
microring resonator. Right: SEM picture of the coupling region. Adapted from Reference 79 under the Creative Commons
Attribution License.

DNA and other nucleic acids
Polyethylene
Glycol
Protein
MRI active agent
Photosensitizer
Polymeric coating
Functional group to tune
surface charge
Dyes for optical imaging
Adsorbed drug
Sensitive coverage
FIGURE 10.3
Nanoplatforms with theranostic functionalities: Fe2O3, magnetic NPs, photosensitizer, appropriate
polymeric coating, surface-charge tuning groups, optical imaging dyes, targeting agents, adsorbed/bonded drugs, sensitive
coverage, DNA and other nucleic acids, polyethylene glycol, proteins, MRI active agents and so on. Reprinted with
permission from Reference 36.
Silica
NP
CH3
H3C
H3C
NH HN
N
N
CH3
COOH
HOOC
HOOC
QD
FIGURE 10.4
Nanotechnology taking PDT from bench to bedside. A wide range of lipid, polymer, ceramic,
magnetic, and other functionalized nanoparticles have been used in PDT that can improve photosensitizer delivery,
together with fullerenes, carbon nanotubes and quantum dots, and raises the question of whether nanotechnology will
truly improve the translation of PDT from mouse to man.

(a)
(b)
(c)
(d)
(e)
(f)
30′ after
bacteria
inoculation
30′ after
bacteria
inoculation
Time 0
10′ after PS
application
Time 0
10′ after PS
application
2 min
6 min
14 min
12 J cm–2
36 J cm–2
84 J cm–2
1
0.1
0.01
0.001
Light control
DC/RLP068
PDT/RLP068
DC/TBO
PDT/TBO
0.0001
0
12
24
36
Remaining fraction of RLU
Light exposure (J/cm2)
48
60
72
84
FIGURE 10.6
PDT treatment of MRSA infection in mouse skin abrasion. (a–e). Successive bioluminescence
images of a representative mouse skin scratch model infected with 108 CFU MRSA treated with (a) light alone, (b) PDT
using RLP068/Cl, or (c) PDT using TBO at 30 min after bacterial inoculation + 15 min from PS application. PDT was
carried out with a combination of 75 μL PS and 84 J cm−2 red light, (d) RLP068/Cl or (e) TBO alone after 30 min from
bacterial inoculation + 15 min after application of PS. Animals were imaged at 2, 6, and 14 min, without to be exposed to
light, (f) Dose response of mean bacterial bioluminescent of mouse scratch abrasion infected with MRSA after treatment
with light alone, RLP068/Cl and TBO dark controls and PDT using RLP068/Cl or TBO assuming as TIME 0 the end of
10 min. Reprinted with permission from Reference 53.

Excited singlet
state
On-thermal red light
Triplet
state
Ground
state
PS*
PS*
Tumour cell necrosis
and apoptosis
Shutdown of
microvessels
Dendritic
cell
Neutrophil
PS
102
302
Inflammation
FIGURE 10.7
The mechanism of action of PDT on tumors. The photosensitizer (PS) absorbs light and an
electron moves to the first short-lived excited singlet state. This is followed by intersystem crossing, in which the excited
electron changes its spin and produces a longer-lived triplet state. The PS triplet transfers energy to ground-state triplet
oxygen, which produces reactive singlet oxygen (1O2). 1O2 can directly kill tumor cells by the induction of necrosis
and/or apoptosis, can cause destruction of tumor vasculature, and produces an acute inflammatory response that attracts
leukocytes such as dendritic cells and neutrophils. Reprinted with permission from Reference 59.
FIGURE 11.1
Overview of different optical super-resolution microscopy techniques. (a) Near-field optical mi-
croscopy makes use of ultrasharp glass fiber tips that are metal-coated to prevent leakage from regions other than the
very end of the tip. The inset shows a microscope image of such a fiber tip with light launched through the tip. (b) Single
molecule localization microscopy fits the position of single fluorescent molecules with a 2D Gaussian fit function to
determine the center of mass of fluorescent spots. (c) Stimulated emission depletion (STED) microscopy makes use of the
principle of stimulated emission to remove fluorescence from parts of a confocal laser beam. (d) Structured illumination
microscopy utilizes 2D/3D frequency mixing between a known illumination pattern and the sample to reconstruct the
original sample with double the resolution.
FIGURE 11.6
(a) Deconvolution fluorescence microscopy image, and (b) 3D structured illumination microscopy
image of human T cells infected with a fluorescent clone of HIV-1. The cytoskeleton is labeled with a fluorescent antibody
against actin [34], while the clone of HIV-1 carries the green fluorescent protein fused to gag, a structural protein of HIV.

Image stack
(b)
(a)
(c)
(d)
CCD pixel grid
Cut-out
Fluorophore positions
c
y
x
Integration
Auto-
Correlation
Convolved emitter signals
Final image
Time lag
New
values
i–1
i
i+1
i–1 i i+1
2
3
n
Time
Image 1
FIGURE 11.7
Mechanism of SOFI auto-cumulant calculation: (a) CCD camera pixel grid used to acquire fluores-
cence images with temporally fluctuating emitters. (b) Expanded view of the two emitters highlighted by the dashed line
in (a) and convolved with the system’s point spread function recorded on an image stack with hundreds to thousands of
images. (c) Analysis of the images by pixel-wise correlation and auto-cumulant calculation. (d) Final image reconstructed
based on the calculated cumulant values.
FIGURE 11.8
(a) Wide-field and (b) second order SOFI images of a rat hippocampal neuron, calculated from 3000
frames, with a frame acquisition time of 50 ms. GABA-B R1 was labeled with QDot525, represented in green; GABA-B
R2 was labeled with QDot625, represented in red. Figure courtesy of Anja Huss and J¨org Enderlein, Department of
Physics, Georg-August-University G¨ottingen, Germany.

FIGURE 12.5
Laser Scanning Digital Camera and Canon nonmydriatic fundus camera images of a 63-year-old
African American male who has diabetes. Left: The near-infrared illumination of the Laser Scanning Digital Camera, but
not the visible wavelength light of the fundus camera, penetrates through moderate media opacities. Right: Further, the
LSDC and Canon both show numerous dot hemorrhages and microaneurysms, consistent with diabetic retinopathy.
Spectral characteristics of
innate absorbers in the eye
MP macular pigment
Short wavelength sensitive
pigment (cones)
Melanin
Me
S
Hb (de-oxygenated hemoglobin)
Hb
LP
MP
HbO (oxygenated hemoglobin)
HbO
H2O (water)
H2O
400
.01
0.1
600
800
1000
Wavelength (nm)
Absorption of ocular pigments
Fundus reflectance
Medium wavelength sensitive
pigment (cones)
M
Long wavelength sensitive
pigment (cones)
L
Lens (increase with age)
Le
DP
Rod photopigment
R
FIGURE 12.7
Spectral characteristics of the innate absorbers in the human eye and the overall reflectance. The
absorbers are demonstrated in two manners, either as the peak absorption spectra for photopigments and macular pigment,
which are well known to have broad spectra with considerable overlap, or as the spectra expected from absorption due to
the normal optical path length and concentration in the healthy eye. The photopigments bleach on exposure to light, and
therefore change dramatically in optical density and consequently the spectra. Macular pigment differs by more than 1
log unit in healthy individuals, also altering the relative absorption spectrum. Modified from Reference 48.

Processing/quanfification algorithms for anterior segment OCT images
Input data: OCT images
Segmentation
Surface
denoising
Full 3D distortion
correction
Output data: ocular geometry
and biometry
3D Merging
0
0
5
10
15
0.2
0.4
0.6
0.8
1
Clustering
Denoising
Statistical thresholding
FIGURE 12.14
Image processing steps to obtain quantitative full 3D geometry of the anterior segment of the eye
from OCT images. Image courtesty of Sergio Ortiz and Susana Marcos, Instituto de Optica (CSIC), Madrid, Spain.
FIGURE 12.16
Fluorescein dye used to measure tear film stability and breakup time. Dark patches within the
brighter slit area represent areas of no or little tear film. The green fluorescent dye is being excited by a cobalt filter.
Image courtesy of Todd Peabody (Indiana University, Bloomington, IN).

FIGURE 12.17
Slit lamp biomicroscope imaging of the anterior eye. (a) Normal eye showing illumination of the
cornea, iris, and lens. The cornea is the slightly hazy slit to the left. Within the pupil, one can see part of the lens as it
scatters light back at the microscope. (b) New unwanted blood vessel growth onto the cornea due to excessive wearing
of a contact lens. The slit beam is illuminating the blood vessels. Image courtesy of Todd Peabody (Indiana University,
Bloomington, IN).
FIGURE 12.18
Corneal topography of normal and astigmatic eyes. (a) Infrared image capture of the cornea with
illuminated placido ring reflecting on the cornea. (b) Topographic map created from data collected in (a). This is a normal
cornea exhibiting a spherical shape. Red colors represent more corneal curvature and blue colors represent less curvature.
Normal corneas begin steeper centrally and flatten farther from the center. (c) Astigmatic corneal topography showing a
traditional bow tie pattern. Note that the redder colors representing more curvature are oriented vertically while the flatter
areas are horizontal (Medmont Topographer, Medmont, Rousehill, Australia).
FIGURE 12.20
Wavefront across the pupil of two human subjects. (a) Normal wavefront showing little variability
across the pupil represented by a uniform color across the image. (b) Subject with keratoconus, showing high levels of
aberrations across the pupil represented by large color swings across the pupil.

WILEY END USER LICENSE
AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook
EULA.

