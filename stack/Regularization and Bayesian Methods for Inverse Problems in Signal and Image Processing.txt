Regularization and
Bayesian Methods
for Inverse Problems in
Signal and Image Processing
Edited by 
Jean-François Giovannelli 
Jérôme Idier
DIGITAL SIGNAL AND IMAGE PROCESSING SERIES


Regularization and Bayesian Methods for Inverse Problems  
in Signal and Image Processing 
 
 

 
 
 

 
Series Editor 
Henri Maître  
Regularization and Bayesian 
Methods for Inverse 
Problems in Signal and 
Image Processing 
 
 
 
 
 
Edited by 
 
Jean-François Giovannelli  
Jérôme Idier  
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
First published 2015 in Great Britain and the United States by ISTE Ltd and John Wiley & Sons, Inc. 
Apart from any fair dealing for the purposes of research or private study, or criticism or review, as 
permitted under the Copyright, Designs and Patents Act 1988, this publication may only be reproduced, 
stored or transmitted, in any form or by any means, with the prior permission in writing of the publishers, 
or in the case of reprographic reproduction in accordance with the terms and licenses issued by the  
CLA. Enquiries concerning reproduction outside these terms should be sent to the publishers at the 
undermentioned address: 
ISTE Ltd  
John Wiley & Sons, Inc.  
27-37 St George’s Road  
111 River Street 
London SW19 4EU 
Hoboken, NJ 07030 
UK  
USA  
www.iste.co.uk  
www.wiley.com 
 
© ISTE Ltd 2015 
The rights of Jean-François Giovannelli and Jérôme Idier to be identified as the authors of this work have 
been asserted by them in accordance with the Copyright, Designs and Patents Act 1988. 
Library of Congress Control Number:  2014956810 
 
British Library Cataloguing-in-Publication Data 
A CIP record for this book is available from the British Library  
ISBN 978-1-84821-637-2 

Contents
INTRODUCTION
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xi
Jean-François GIOVANNELLI and Jérôme IDIER
CHAPTER 1. 3D RECONSTRUCTION IN X-RAY TOMOGRAPHY:
APPROACH EXAMPLE FOR CLINICAL DATA PROCESSING . . . . . . . . .
1
Yves GOUSSARD
1.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2. Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2.1. Data formation models
. . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2.2. Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2.3. Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3. Method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.1. Data formation models
. . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.2. Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3.3. Minimization method . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.3.4. Implementation of the reconstruction procedure . . . . . . . . . . .
14
1.4. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.4.1. Comparison of minimization algorithms . . . . . . . . . . . . . . . .
15
1.4.2. Using a region of interest in reconstruction . . . . . . . . . . . . . .
18
1.4.3. Consideration of the polyenergetic character of the source
. . . . .
21
1.5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.6. Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
CHAPTER 2. ANALYSIS
OF FORCE-VOLUME IMAGES
IN ATOMIC
FORCE MICROSCOPY USING SPARSE APPROXIMATION
. . . . . . . . . .
31
Charles SOUSSEN, David BRIE, Grégory FRANCIUS, Jérôme IDIER
2.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31

vi
Regularization and Bayesian Methods for Inverse Problems
2.2. Atomic force microscopy . . . . . . . . . . . . . . . . . . . . . . . . . .
32
2.2.1. Biological cell characterization . . . . . . . . . . . . . . . . . . . . .
32
2.2.2. AFM modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.2.3. Physical piecewise models
. . . . . . . . . . . . . . . . . . . . . . .
37
2.3. Data processing in AFM spectroscopy . . . . . . . . . . . . . . . . . . .
40
2.3.1. Objectives and methodology in signal processing
. . . . . . . . . .
40
2.3.2. Segmentation of a force curve by sparse approximation . . . . . . .
41
2.4. Sparse approximation algorithms . . . . . . . . . . . . . . . . . . . . . .
43
2.4.1. Minimization of a mixed ℓ2-ℓ0 criterion . . . . . . . . . . . . . . . .
44
2.4.2. Dedicated algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.4.3. Joint detection of discontinuities . . . . . . . . . . . . . . . . . . . .
46
2.5. Real data processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.5.1. Segmentation of a retraction curve:
comparison of strategies . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.5.2. Retraction curve processing . . . . . . . . . . . . . . . . . . . . . . .
50
2.5.3. Force-volume image processing in the approach phase
. . . . . . .
52
2.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
2.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
CHAPTER 3. POLARIMETRIC IMAGE RESTORATION BY NON-LOCAL
MEANS
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
Sylvain FAISAN, François ROUSSEAU, Christian HEINRICH, Jihad ZALLAT
3.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.2. Light polarization and the Stokes–Mueller formalism . . . . . . . . . .
58
3.3. Estimation of the Stokes vectors . . . . . . . . . . . . . . . . . . . . . .
61
3.3.1. Estimation of the Stokes vector in a pixel . . . . . . . . . . . . . . .
61
3.3.2. Non-local means ﬁltering . . . . . . . . . . . . . . . . . . . . . . . .
64
3.3.3. Adaptive non-local means ﬁltering . . . . . . . . . . . . . . . . . . .
66
3.3.4. Application to the estimation of Stokes vectors . . . . . . . . . . . .
69
3.4. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.4.1. Results with synthetic data . . . . . . . . . . . . . . . . . . . . . . .
72
3.4.2. Results with real data . . . . . . . . . . . . . . . . . . . . . . . . . .
75
3.5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
CHAPTER 4. VIDEO PROCESSING
AND REGULARIZED INVERSION
METHODS
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
Guy LE BESNERAIS, Frédéric CHAMPAGNAT
4.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
4.2. Three applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
4.2.1. PIV and estimation of optical ﬂow . . . . . . . . . . . . . . . . . . .
82
4.2.2. Multiview stereovision . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.2.3. Superresolution and non-translational motion . . . . . . . . . . . . .
86

Contents
vii
4.3. Dense image registration
. . . . . . . . . . . . . . . . . . . . . . . . . .
88
4.3.1. Direct formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
4.3.2. Variational formulation . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.3.3. Extension of direct formulation for multiview processing . . . . . .
92
4.4. A few achievements based on direct formulation . . . . . . . . . . . . .
92
4.4.1. Dense optical ﬂow by correlation of local window . . . . . . . . . .
92
4.4.2. Occlusion management in multiview stereovision . . . . . . . . . .
97
4.4.3. Direct models for SR
. . . . . . . . . . . . . . . . . . . . . . . . . .
99
4.5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
4.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
CHAPTER 5. BAYESIAN APPROACH IN PERFORMANCE MODELING:
APPLICATION TO SUPERRESOLUTION
. . . . . . . . . . . . . . . . . . . . .
109
Frédéric CHAMPAGNAT, Guy LE BESNERAIS, Caroline KULCSÁR
5.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
5.1.1. The hiatus between performance modeling and Bayesian inversion
109
5.1.2. Chapter organization . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
5.2. Performance modeling and Bayesian paradigm . . . . . . . . . . . . . .
111
5.2.1. An empirical performance evaluation tool . . . . . . . . . . . . . . .
111
5.2.2. Usefulness and limits of a performance evaluation tool . . . . . . .
111
5.2.3. Bayesian formalism . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
5.3. Superresolution techniques behavior . . . . . . . . . . . . . . . . . . . .
113
5.3.1. Superresolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
5.3.2. SR methods performance: known facts . . . . . . . . . . . . . . . .
115
5.3.3. An SR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
5.3.4. Performance model and properties . . . . . . . . . . . . . . . . . . .
122
5.4. Application examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
5.4.1. Behavior of the optimal ﬁlter with regard to the number of images .
127
5.4.2. Characterization of an approximation: shifts rounding . . . . . . . .
129
5.5. Real data processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
5.5.1. A concrete measure to improve the resolution: the RER . . . . . . .
132
5.5.2. Empirical validation and application ﬁeld . . . . . . . . . . . . . . .
134
5.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
5.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
CHAPTER 6. LINE SPECTRA ESTIMATION FOR IRREGULARLY
SAMPLED SIGNALS IN ASTROPHYSICS . . . . . . . . . . . . . . . . . . . . .
141
Sébastien BOURGUIGNON, Hervé CARFANTAN
6.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
6.2. Periodogram, irregular sampling, maximum likelihood
. . . . . . . . .
144
6.3. Line spectra models: spectral sparsity . . . . . . . . . . . . . . . . . . .
146
6.3.1. An inverse problem with sparsity prior information . . . . . . . . .
147
6.3.2. Difﬁculties in terms of sparse approximation . . . . . . . . . . . . .
149

viii
Regularization and Bayesian Methods for Inverse Problems
6.4. Prewhitening, CLEAN and greedy approaches . . . . . . . . . . . . . .
151
6.4.1. Standard greedy algorithms . . . . . . . . . . . . . . . . . . . . . . .
151
6.4.2. A more complete iterative method: single best replacement . . . . .
153
6.4.3. CLEAN-based methods . . . . . . . . . . . . . . . . . . . . . . . . .
154
6.5. Global approach and convex penalization . . . . . . . . . . . . . . . . .
155
6.5.1. Signiﬁcance of ℓ1 penalization in
. . . . . . . . . . . . . . . . . .
156
6.5.2. Existence and uniqueness . . . . . . . . . . . . . . . . . . . . . . . .
156
6.5.3. Minimizer and regularization parameter characterization . . . . . .
157
6.5.4. Amplitude bias and a posteriori corrections . . . . . . . . . . . . . .
157
6.5.5. Hermitian symmetry and speciﬁcity of the zero frequency
. . . . .
158
6.5.6. Optimization algorithms . . . . . . . . . . . . . . . . . . . . . . . . .
158
6.5.7. Results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
6.6. Probabilistic approach for sparsity . . . . . . . . . . . . . . . . . . . . .
159
6.6.1. Bernoulli–Gaussian model for spectral analysis
. . . . . . . . . . .
160
6.6.2. A structure adapted to the use of MCMC methods . . . . . . . . . .
161
6.6.3. An extended BG model for improved accuracy . . . . . . . . . . . .
162
6.6.4. Stochastic simulation and estimation . . . . . . . . . . . . . . . . . .
162
6.6.5. Results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
6.7. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
6.8. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
CHAPTER 7. JOINT DETECTION-ESTIMATION IN
FUNCTIONAL MRI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
Philippe CIUCIU, Florence FORBES, Thomas VINCENT, LotﬁCHAARI
7.1. Introduction to functional neuroimaging . . . . . . . . . . . . . . . . . .
169
7.2. Joint detection-estimation of brain activity
. . . . . . . . . . . . . . . .
171
7.2.1. Detection and estimation: two interdependent issues . . . . . . . . .
171
7.2.2. Hemodynamics physiological hypotheses . . . . . . . . . . . . . . .
173
7.2.3. Spatially variable convolutive model . . . . . . . . . . . . . . . . . .
175
7.2.4. Regional generative model . . . . . . . . . . . . . . . . . . . . . . .
176
7.3. Bayesian approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
7.3.1. Likelihood
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
7.3.2. A priori distributions . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
7.3.3. A posteriori distribution . . . . . . . . . . . . . . . . . . . . . . . . .
182
7.4. Scheme for stochastic MCMC inference . . . . . . . . . . . . . . . . . .
183
7.4.1. HRF and NRLs simulation . . . . . . . . . . . . . . . . . . . . . . .
183
7.4.2. Unsupervised spatial and spatially adaptive regularization
. . . . .
184
7.5. Alternative variational inference scheme . . . . . . . . . . . . . . . . . .
184
7.5.1. Motivations and foundations . . . . . . . . . . . . . . . . . . . . . .
184
7.5.2. Variational EM algorithm . . . . . . . . . . . . . . . . . . . . . . . .
186
7.6. Comparison of both types of solutions . . . . . . . . . . . . . . . . . . .
190
7.6.1. Experiments on simulated data . . . . . . . . . . . . . . . . . . . . .
190
7.6.2. Experiments on real data
. . . . . . . . . . . . . . . . . . . . . . . .
193

Contents
ix
7.7. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
7.8. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
CHAPTER 8. MCMC AND VARIATIONAL APPROACHES FOR BAYESIAN
INVERSION IN DIFFRACTION IMAGING
. . . . . . . . . . . . . . . . . . . .
201
Hacheme AYASSO, Bernard DUCHÊNE, Ali MOHAMMAD-DJAFARI
8.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
8.2. Measurement conﬁguration . . . . . . . . . . . . . . . . . . . . . . . . .
204
8.2.1. The microwave device . . . . . . . . . . . . . . . . . . . . . . . . . .
204
8.2.2. The optical device . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
8.3. The forward model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
8.3.1. The microwave case . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
8.3.2. The optical case
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
8.3.3. The discrete model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
8.3.4. Validation of the forward model . . . . . . . . . . . . . . . . . . . .
210
8.4. Bayesian inversion approach . . . . . . . . . . . . . . . . . . . . . . . .
211
8.4.1. The MCMC sampling method . . . . . . . . . . . . . . . . . . . . .
213
8.4.2. The VBA method
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
8.4.3. Initialization, progress and convergence of the algorithms . . . . . .
217
8.5. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
8.6. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
8.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
CHAPTER 9. VARIATIONAL BAYESIAN APPROACH AND BI-MODEL
FOR THE RECONSTRUCTION-SEPARATION OF ASTROPHYSICS
COMPONENTS
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
Thomas RODET, Aurélia FRAYSSE, Hacheme AYASSO
9.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
9.2. Variational Bayesian methodology . . . . . . . . . . . . . . . . . . . . .
228
9.3. Exponentiated gradient for variational Bayesian . . . . . . . . . . . . .
229
9.4. Application: reconstruction-separation of astrophysical
components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
9.4.1. Direct model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
9.4.2. A priori distributions . . . . . . . . . . . . . . . . . . . . . . . . . . .
234
9.4.3. A posteriori distribution . . . . . . . . . . . . . . . . . . . . . . . . .
235
9.5. Implementation of the variational Bayesian approach . . . . . . . . . .
236
9.5.1. Separability study . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
9.5.2. Update of the approximation distributions . . . . . . . . . . . . . . .
236
9.6. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
9.6.1. Simulated data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
9.6.2. Real data
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244
9.7. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
246
9.8. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
246

x
Regularization and Bayesian Methods for Inverse Problems
CHAPTER 10. KERNEL VARIATIONAL APPROACH FOR TARGET
TRACKING IN A WIRELESS SENSOR NETWORK
. . . . . . . . . . . . . . .
251
Hichem SNOUSSI, Paul HONEINE, Cédric RICHARD
10.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
10.2. State of the art: limitations of existing methods . . . . . . . . . . . . .
252
10.3. Model-less target tracking . . . . . . . . . . . . . . . . . . . . . . . . .
254
10.3.1. Construction of the likelihood by matrix regression . . . . . . . . .
255
10.3.2. Variational ﬁltering for the tracking of mobile objects . . . . . . .
258
10.4. Simulation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
10.5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
10.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
CHAPTER 11. ENTROPIES AND ENTROPIC CRITERIA
. . . . . . . . . . .
267
Jean-François BERCHER
11.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
11.2. Some entropies in information theory . . . . . . . . . . . . . . . . . . .
268
11.2.1. Main properties and deﬁnitions . . . . . . . . . . . . . . . . . . . .
268
11.2.2. Entropies and divergences in the continuous case . . . . . . . . . .
270
11.2.3. Maximum entropy . . . . . . . . . . . . . . . . . . . . . . . . . . .
272
11.2.4. Escort distributions . . . . . . . . . . . . . . . . . . . . . . . . . . .
272
11.3. Source coding with escort distributions
and Rényi bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
11.3.1. Source coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
11.3.2. Source coding with Campbell measure . . . . . . . . . . . . . . . .
274
11.3.3. Source coding with escort mean
. . . . . . . . . . . . . . . . . . .
275
11.4. A simple transition model . . . . . . . . . . . . . . . . . . . . . . . . .
277
11.4.1. The model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
11.4.2. The Rényi divergence as a consequence . . . . . . . . . . . . . . .
279
11.4.3. Fisher information for the parameter q . . . . . . . . . . . . . . . .
279
11.4.4. Distribution inference with
generalized moment constraint . . . . . . . . . . . . . . . . . . . . .
281
11.5. Minimization of the Rényi divergence and associated entropies
. . .
281
11.5.1. Minimization under generalized moment constraint
. . . . . . . .
282
11.5.2. A few properties of the partition functions . . . . . . . . . . . . . .
283
11.5.3. Entropic functionals derived from the Rényi divergence . . . . . .
285
11.5.4. Entropic criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
11.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
LIST OF AUTHORS
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
INDEX
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
297

Introduction
This book was written in tribute to our colleague Guy Demoment, who was a
researcher at the CNRS from 1977 to 1988, then Professor at the University of Paris-
Sud until 2008, member of the Laboratoire des Signaux et Systèmes (L2S, UMR 8506,
Gif-sur-Yvette) and its director from 1997 to 2001, and the founder of a research group
on inverse problems in signal and image processing at the beginning of the 1980s.
Guy Demoment’s research activities began in 1970, at the interface between
biological and medical engineering, automatic control and the still ﬂedgling ﬁeld of
signal processing. Guy was particularly interested in cardiac function and in the
cardiovascular system [DEM 77]. He derived a mathematical model of the
functioning of the cardiovascular hemodynamic loop which was subsequently used to
develop the control law of cardiac replacement prostheses. He also focused on
aspects closer to theoretical biology such as left ventricle modeling and questions
closer to physics such as the determination of vascular impedance [DEM 81].
This latter aspect naturally leads us to confront models with reality by means of
measurements. The idea makes sense whereas in practice these measurements
provide only indirect and degraded information on the quantities of interest. These
degradations are generally considered in two forms: structure (resolution limitations,
dynamics,
sampling,
etc.)
and
uncertainty
(measurement
noise,
model
approximation, etc.). The restitution of the quantity of interest then raises a real
ill-posed inversion or inference problem. By creating the Groupe Problèmes Inverses
(GPI – Inverse Problems Group), Guy promoted this scientiﬁc approach within the
L2S then to the whole of the signal-image community within the engineering
sciences. Having shared this approach with him within the GPI is a fortunate
Introduction written by Jean-François GIOVANNELLI and Jérôme IDIER.

xii
Regularization and Bayesian Methods for Inverse Problems
opportunity that most of the co-authors of this book have beneﬁted from, as doctoral
students or beginner colleagues.
Undoubtedly, Guy Demoment was an essential contributor to the ﬁeld of inverse
problems in signal and image processing, and its main instigator in the French
community. He has also been passionate about related issues such as that of the
effective implementation of a number of algorithms. In particular, in the context of
linear deconvolution and adaptive spectral analysis, Kalman ﬁltering and smoothing
algorithms
were
given
particular
attention.
Guy
made
several
signiﬁcant
contributions concerning fast versions of these algorithms [DEM 85, DEM 91].
The exploitation of probabilistic models for detection-estimation has also been a
subject of choice for Guy and his collaborators since the end of the 1980s, resulting
in recursive [GOU 89], and then iterative [GOU 90] computational structures. It is
interesting to note that the latter are very competitive precursors to the well-known
greedy algorithms in parsimonious approximation, as is clearly stated in the book.
Regarding more fundamental subjects, he has been interested in the issues of
information modeling and Bayesian inference, inspired by E.T. Jaynes’ works. He
contributed to the use of a maximum entropy principle for the synthesis of a priori
models and to their application to tomography [MOH 87, MOH 88a], and then he
studied the principle of maximum entropy on the mean in the context of inverse
problems [LEB 99]. He then further explored these issues and, during his last period
of scientiﬁc activity, became interested in variational approaches for Bayesian
inference.
Guy’s scientiﬁc sensitivity has also been visible in a signiﬁcant way in his teaching
activities. He created several courses ranging from undergraduate to PhD levels, as
well as in continuing education, and always dedicated to them a lot of energy and
creativity. Some particular examples include a course on Bayesian inference and the
basics of probabilities, and among the most in-depth themes, Kalman algorithms and
their fast versions, as well as the deconvolution of signals.
Beyond his scientiﬁc activities, researches and teachings, Guy has also been
involved in a remarkable way in community life. On a national level, he has been a
member of the Conseil National des Universités (www.cpcnu.fr), a particularly active
member of scholar and research networks, e.g. club EEA (www.clubeea.org) and the
GdR ISIS (gdr-isis.fr). Within the University of Paris-Sud, he has chaired the
pedagogy commission, he has been vice president of the Department of Physics in
Orsay, responsible for bachelor-level diploma, and co-creator of a masters-level
diploma.
With regard to the present book, it concerns “ill-posed inverse problems”. The
readers can refer to the widely cited article [DEM 89] or to a previous collective

Introduction
xiii
book [IDI 08] on this subject, of which Guy Demoment is one of the main
contributors. It is concerned with problems that cannot be resolved on the basis of the
observed data only and the construction of solutions requires other information,
referred to as a priori. These solutions are then speciﬁc to the information taken into
account. The recognition and the explanation of this information are necessary to
appreciate the range of validity and the scope of application of the constructed
solutions. Over the 1980s, the scientiﬁc community has greatly acknowledged the
signiﬁcance of this problematic, and contributions have become very abundant not
only in the signal-image community but also in that of mathematics, computer
science and physics.
As a direct response to this thematic abundance concerning inverse problems, we
have chosen to address a broad spectrum of data processing problems and application
domains, with a particular focus on the diversity of mathematical tools.
From the point of view of application ﬁelds, an important part of the book is
dedicated to different scientiﬁc ﬁelds, which present a large number of inversion
problems: biological and medical imaging, and more speciﬁcally X-ray tomography
(Chapters 1, 2 and 7), astronomy (Chapters 6 and 9) as well as non-destructive
evaluation (Chapter 8). At least one other has been added: video sequence processing
(Chapters 4 and 5). Two other applications that are more rarely met in the ﬁeld of
inversion:
target tracking and sensor networks (Chapter 10) as well as digital
communications (Chapter 11).
The diversity of chapters is also evident when the considered acquisition
modalities come under scrutiny: from the more traditional ones such as tomography
(Chapters 1 and 8) and MRI (Chapter 7), optical imaging in the visible (Chapters 4
and 5) or in the infrared spectrum (Chapters 5 and 9) to more recent modalities such
as atomic force imaging (Chapter 2) and polarized optical imaging (Chapter 3).
Throughout the chapters,
the duality between the approaches known as
“energetic” and “probabilistic” emerges. The ﬁrst type of approach is based on
deterministic construction leading to criteria and to numerical optimization issues as
typically in Chapters 1, 2, 3, 4 and 6. The second type of approach is based on a
Bayesian construction, often hierarchical, which probabilizes unknown objects in
addition to data. It thus leads to a joint distribution; therefore, optimal strategies are
available and performance characterization right from the start becomes possible as
in Chapter 5. The remaining chapters make use of a posteriori distributions to
generate an estimation:
they are explored by using stochastic sampling as in
Chapters 6, 7 and 8 or by an approximated maximization as in Chapters 7, 8, 9
and 10. The latter also introduces a notion of learning and relies on informational
principles discussed in Chapter 11, which presents more theoretical aspects related to
entropy criteria.

xiv
Regularization and Bayesian Methods for Inverse Problems
I.1. Bibliography
[DEM 72] DEMOMENT
G.,
Modèle
de
la
boucle
cardiovasculaire:
évaluation
de
l’autorégulation mécanique et de la fonction ventriculaire gauche,
PhD thesis, no.169,
Orsay center, University of Paris-sud, 29 June 1972.
[DEM 77] DEMOMENT G., Contribution à l’étude du fonctionnement ventriculaire gauche
par des méthodes d’identiﬁcation paramétriques. obtention d’un observateur de l’état du
ventricule, PhD thesis, no.1810, Orsay center, University of Paris-sud, 15 March 1977.
[DEM 82] DEMOMENT G., Introduction à la statistique,
Lecture notes, École supérieure
d’électrité, no. 2906, 1982.
[DEM 83] DEMOMENT G., Déconvolution des signaux,
Lecture notes, École supérieure
d’électrité no. 2964, 1983.
[DEM 85] DEMOMENT G., REYNAUD R., “Fast minimum-variance deconvolution”, IEEE
Transactions on Acoustics, Speech and Signal Processing, vol. ASSP-33, pp. 1324–1326,
1985.
[DEM 87] DEMOMENT G., Algorithme rapides, Lecture notes, École supérieure d’électrité,
no. 3152, 1987.
[DEM 89a] DEMOMENT G., “Equations de Chandrasekhar et algorithmes rapides pour le
traitement du signal et des images”, Traitement du Signal, vol. 6, pp. 103–115, 1989.
[DEM 89b] DEMOMENT G., “Image reconstruction and restoration: Overview of common
estimation structures and problems”, IEEE Transactions on Acoustics, Speech and Signal
Processing, vol. ASSP-37, no. 12, pp. 2024–2036, December 1989.
[DEM 91] DEMOMENT G., REYNAUD R., “Fast RLS algorithms and Chandrasekhar
equations”, HAYKIN S., (ed.), SPIE Conference on Adaptive Signal Processing, San Diego,
CA, pp. 357–367, July 1991.
[DEM 05a] DEMOMENT G., Probabilités: modélisation des incertitudes, inférence logique,
et traitement des données expérimentales. Deuxième partie: application au traitement du
signal, Lecture notes, University of Paris-sud, Orsay center, 2005.
[DEM 05b] DEMOMENT G., Probabilités: modélisation des incertitudes, inférence logique, et
traitement des données expérimentales. Première partie: bases de la théorie, Lecture notes,
University of Paris-sud, Orsay center, 2005.
[GOU 89] GOUSSARD Y., DEMOMENT G., “Recursive deconvolution of Bernoulli-Gaussian
processes using a MA representation”,
IEEE Transactions on Geoscience and Remote
Sensing, vol. GE-27, pp. 384–394, 1989.
[GOU 90] GOUSSARD Y., DEMOMENT G., IDIER J., “A new algorithm for iterative
deconvolution of sparse spike trains”, IEEE International Conference on Acoustic, Speech
and Signal Processing, Albuquerque, NM, pp. 1547–1550, April 1990.
[IDI 08] IDIER J., (ed.), Bayesian Approach to Inverse Problems, ISTE, London and John
Wiley & Sons, New York, April 2008.
[LEB 99] LE BESNERAIS G., BERCHER J.-F., DEMOMENT G., “A new look at entropy for
solving linear inverse problems”, IEEE Transactions on Information Theory, vol. 45, no. 5,
pp. 1565–1578, July 1999.

Introduction
xv
[MOH 87] MOHAMMAD-DJAFARI
A.,
DEMOMENT
G.,
“Maximum entropy Fourier
synthesis with application to diffraction tomography”,
Applied Optics, vol. 26, no. 10,
pp. 1745–1754, 1987.
[MOH 88a] MOHAMMAD-DJAFARI A., DEMOMENT G., “Maximum entropy reconstruction
in X ray and diffraction tomography”, IEEE Transactions on Medical Imaging, vol. MI-7,
no. 4, pp. 345–354, 1988.
[MOH 88b] MOHAMMAD-DJAFARI A., DEMOMENT G., “Utilisation de l’entropie dans les
problèmes de restauration et de reconstruction d’images”, Traitement du Signal, vol. 5,
no. 4, pp. 235–248, 1988.


1
3D Reconstruction in X-ray
Tomography: Approach Example
for Clinical Data Processing
1.1. Introduction
Works presented in this chapter stem from three-dimensional (3D) reconstruction
problems, in X-ray computed tomography (XRCT), within a clinical framework.
More speciﬁcally, the practical objective was to use XRCT to detect and quantify
possible restenosis occurring in some patients after the insertion of a stent. The
quality of reconstructions achieved by clinical tomographs being insufﬁcient for this
purpose, the concern was thus to develop a method capable of rebuilding small
structures in the presence of metal objects, in a 3D context, with a precision higher
than that of tomographs available in hospitals; in addition, this method was supposed
to work with computers commonly available in most research laboratories (such as
personal computers (PCs),
without any particular architecture or processor
hardware).
The development of a solution clearly falls within the framework of conventional
inverse problem solving. However,
it is essential to take into account the
characteristics of 3D XRCT, and notably the very large volume of data to be
processed, the geometric complexity of the collection process of raw data, and
practical barriers to access these data. In order to achieve the objective stated above,
difﬁculties are twofold: (1) at the methodological level, the development of an
inversion method adapted to the intrinsic characteristics of the problem to be
addressed and, (2) at the implementation level, accounting for the practical obstacles
mentioned above as well as for constraints on the processing time inherent in any
Chapter written by Yves GOUSSARD.

2
Regularization and Bayesian Methods for Inverse Problems
clinical application. In the following, we present the retained approach based on the
analysis of the main factors likely to improve the quality of reconstructions while
satisfying the practical constraints which we must face; this brings us to putting the
methodological aspects into perspective, with regard to practical questions, in light of
the applied objective of these works.
1.2. Problem statement
Although image reconstruction methods used in the ﬁrst tomographs were of the
analytical type [AMB 73, HOU 73], the advantages of approaches based on
estimation [HER 71, HER 73, HER 76b], then the ill-posed nature of tomographic
reconstruction problems [HER 76a, HER 79] were recognized very early on. Over
the past 35 years, many academic studies focusing on tomographic reconstruction
have been carried out in the context of solving inverse problems. Generally, the
emphasis is on the main three elements of this type of approach, that is to say,
modeling of the data formation process, choice of the estimator, and development of
techniques that enable the practical computation of the estimate [DEM 89]. These
works have been partly customized according to various imaging modalities (for
example,
transmission [HER 76a],
emission [LEV 87],
diffraction [BER 97]
tomography, and more recently optical and/or multiphysics tomography (see
[BOA 04] for a partial synthesis)) which present largely variable degrees of
difﬁculty:
if estimation conditions are often very unfavorable in diffraction
tomography (eddy current tomography [TRI 10], seismic imaging [VAU 11]) due to
the strong non-linearity of underlying physical phenomena, to the importance of
attenuation phenomena and to the small number of observations with respect to the
number of unknowns, the inversion conditions are generally better in emission
tomography (SPECT for example) and can be qualiﬁed as relatively favorable in
XRCT. This explains why, in this area, reconstruction methods known as “naive”
provide results that have been used in clinical practice for several decades.
Thereafter, we present the elements likely to have a signiﬁcant impact on the
performance of an XRCT inversion method.
1.2.1. Data formation models
In XRCT, all data formation models are based on the Beer–Lambert law, which
describes the attenuation of an X-ray beam through a medium whose spatial
distribution of the attenuation coefﬁcients is referred to by μ. It takes the form:
N : P

n0 exp

−

D
μ(s) ds

[1.1]

3D Reconstruction in X-ray Tomography
3
where N is the random variable 1 that represents the number of photons arriving at the
detector, n0 the number of photons emitted by the source and D the path (straight line)
of photons between the source and the detector. P {ℓ} refers to the Poisson distribution
with parameter ℓ. By discretizing the distribution of the attenuation coefﬁcients of the
medium, the integral involved in [1.1] becomes:

D
μ(s) ds = at
Dμ
[1.2]
where μ is the vector in which all samples of the attenuation coefﬁcients of the
medium are concatenated, and where aD is a vector representing the contribution of
each sample of μ to the integral. By concatenating the variables N corresponding to
each source-detector position into vector N, and by concatenating in an analogous
manner row vectors at
D in matrix A, one obtains:
N : P

n0e−Aμ
[1.3]
where
the
Poisson
distribution
should
be
understood
component-wise,
all
components being independent from each other. A represents the projection matrix
of the tomographic system which is sparse, usually very large and structured in a way
that reﬂects the geometry of the data collection process.
In most methods, reconstruction is carried out not from the photon counts, but
from the auxiliary quantity Y
≜
−ln N/n0. By performing a Taylor-series
expansion of the neg-log-likelihood which derives from [1.3], Sauer and Bouman
[SAU 93, Appendix A] have shown that it assumes the following approximate form:
JV ∝n0
2 ∥y −Aμ∥2
Σ ,
Σ ≜Diag

e−y
[1.4]
In practice, this amounts to assuming that:
Y ≈N {Aμ , 1/n0 Diag {ey}}
[1.5]
where N {m,Q} denotes the normal distribution with mean m and covariance matrix
Q. This approximation allows the estimation of μ to be carried out in a linear Gaussian
framework, the problem remaining generally very large.
The models presented above are valid when X-ray sources and detectors are
punctual and when the radiation emitted by the source is monoenergetic. However,
1. In this chapter, any random quantity is designated by an uppercase character and its realization by
the corresponding lowercase character.

4
Regularization and Bayesian Methods for Inverse Problems
these two assumptions are relatively rough approximations of reality, and their
impact on the quality of reconstructed images can be signiﬁcant in practice.
On the one hand, considering sources and detectors are punchual, while they have
a non-zero surface, oscillations, which are all the larger as the discretization of the
medium is ﬁne, are produced in the reconstructed images . In addition, these artifacts
are highly structured as they reﬂect the paths of radiation in the medium. In an attempt
to limit the magnitude of these phenomena, several techniques aimed at changing the
structure of matrix A in order to account for the thickness or the volume of the ray
beams have been proposed [DEM 04, ZHU 94]. However, it can be observed that these
techniques have a strong empirical character, insofar that they all overlook the fact that
the calculation of the parameter of the Poisson distribution of photon counts requires
the summation of exponential functions (see equation [1.1] or [1.3]), and cannot be
carried out directly on the linear Gaussian model [1.5]. Nevertheless, in practice, these
approaches limit the appearance of artifacts related to the thin ray model.
On the other hand, the hypothesis of monoenergetic X-ray source is never
fulﬁlled in practice, since most of the sources used in XRCT have a spectrum that
ranges from 0 to approximately 120 keV. However, the effect of this hypothesis only
appears when variations of μ with respect to energy are signiﬁcant. In moderately
attenuating media such as water and soft tissue, these variations are small, and the
hypothesis of a monoenergetic radiation source only causes minimal degradations.
On the other hand, in more strongly attenuating media (cortical bone, metal, vascular
calciﬁcations), variations are more signiﬁcant, and the monoenergetic assumption
results in various artifacts (star-shaped motifs, artiﬁcial thickening of strongly
attenuating structures) that can make it difﬁcult, if not impossible, to interpret the
results. Unfortunately, there is a rather scarce number of works aimed at accounting
for the polyenergetic nature of radiation in reconstruction methods in an explicit and
general manner. However, we should emphasize the contribution of [DEM 01], based
on the Alvarez–Macovski decomposition of attenuation coefﬁcients [ALV 76] and on
an empirical parameterization of the variations of these coefﬁcients with respect to
energy. We will refer in more detail to this model in the next section. It should be
noted that, as for accounting for the non-zero surface of detectors, determining the
parameter of the Poisson distribution of polyenergetic radiation requires the
summation of exponential functions and involves strong nonlinearities.
Regardless of the type of data formation model used, the product by A (projection
operation) dominates the computation of the prediction of the observations for a given
medium. Most inversion techniques (direct or iterative) also require the product by At.
Due to the large size of the problem, the choice of the representation of A (implicit or
explicit, accessible by rows or by columns), and the effectiveness of the computation
of the left- and right-products by A have a critical impact on the effectiveness of
reconstruction methods. It should be noted that these issues are rarely addressed in the
open literature.

3D Reconstruction in X-ray Tomography
5
1.2.2. Estimators
As in most of the other modalities, the vast majority of XRCT reconstruction
methods based on an inversion approach rely on penalized maximum likelihood
estimators. The likelihood portion derives from the distribution of photon
counts [1.3] or from the Gaussian approximation [1.5] with respect to variable Y . In
either case, the resulting neg-log-likelihood is convex under the monoenergetic
hypothesis and is well-suited for minimization by descent methods. Under the
polyenergetic hypothesis, the neg-log-likelihood takes a more complex form, and
loses the convexity property due to the strong nonlinearity of the parameterizations
used. These elements are a likely cause of the scarcity of works based on
polyenergetic models.
With regard to the penalty portion, or a priori modeling of the medium to
reconstruct,
a broad consensus can be
observed in employing Markovian
representations with convex potential on ﬁrst-differences [BOU 93, DEL 98,
FES 94]. This is reﬂected in the neg-log-likelihood by the addition of a convex
additive penalty term, quadratic or not. Surprisingly, there is to our knowledge no
real study comparing, in XRCT and under realistic conditions, the effect of quadratic
(Gaussian Markov random ﬁelds) and non-quadratic (edge-preserving) penalty terms.
In practice, the latter are usually used because they better reﬂect the global properties
of media to be reconstructed at a very low marginal cost.
1.2.3. Algorithms
Reconstruction algorithms refer here to numerical optimization procedures that
allow us to minimize the objective functions (usually penalized likelihood) referred
to in the preceding paragraph. As previously mentioned, these objective functions
have been developed under monoenergetic assumptions, and the same assumptions
are in force for algorithms used in XRCT. However, it should be noted that a number
of optimization procedures developed for other modalities (for example, emission
computed tomography) can easily be transposed to XRCT provided that the data
formation model is linear.
Since the introduction of the ﬁrst “algebraic” reconstruction methods [HER 71],
development
of
optimization
techniques
aimed
speciﬁcally
at
tomographic
reconstruction has been the subject of numerous works. The strength of this activity,
and the fact that generic nonlinear optimization methods have been widely ignored by
the community, can be attributed to the practical difﬁculties in solving reconstruction
problems (large size, need to account for bound constraints) and the general perception
that generic optimization methods were unable to cope with these problems in
a satisfactory manner. Most algorithms speciﬁc to emission or to transmission
tomography fall into the framework of iterative descent direction methods, and require

6
Regularization and Bayesian Methods for Inverse Problems
the computation of the gradient of the objective function (or of one of its partial
derivatives) at each iteration. It is not possible to detail here the various approaches
that lead to algebraic structures suited for such and such speciﬁc representation of
matrix A. Among the most signiﬁcant works, ICD algorithms [SAU 93], as well
as approaches based on ordered subsets (OS) that help to signiﬁcantly reduce the
computation time, can be mentioned. However, these works appear to present two
major ﬂaws:
– studies that compare the different approaches are rare and extremely narrow
in scope. If carrying out such studies in a rigorous manner presents signiﬁcant
difﬁculties related to the possible differences of implementation, to the possibility of
parallelization, etc., the adoption of a common framework would nevertheless make it
easier to identify major trends. Today, generic optimization methods no longer present
the same limitations as in the past and lend themselves relatively well to solving
reconstruction problems, which makes the lack of such comparative studies even more
unfortunate;
– the issues of convergence control of reconstruction methods are scarcely or
not addressed, which gives the results a pronounced empirical character and makes
comparison attempts between methods even more difﬁcult. In particular, OS-based
methods are not convergent, which leaves unanswered questions about the choice of a
stopping criterion and its link with the quality of the reconstructed images.
For these reasons, the choice of an XRCT optimization algorithm still seems to be
lacking of rigor and to present a signiﬁcant empirical character.
Despite the abundance and quality of academic works, their penetration in XRCT
clinical applications appears to be minimal. As an example, the recent works
dedicated to applications focus mostly on 3D computerized axial tomography
(systems such as C-Arm and O-Arm) [KAC 11], as opposed to helical tomographs
available in most hospitals. This situation is partly explained by the fact that, for
nearly 30 years, the four major manufacturers of clinical tomographs have focused
almost all of their efforts on hardware improvement and on the timeliness of
obtaining results rather than on the reconstruction methodology. The result is that,
today, measurements are of satisfactory quality and the conditioning of the
reconstruction
problem
is
acceptable,
which
limits
the
need
to
resort
to
reconstruction techniques taking full advantage of the measurements. Moreover, the
large volume of data to process, the geometric complexity of data collection and
processing time constraints make academic works difﬁcult to apply, and their real
contribution sometimes questionable. This is the reason why iterative reconstruction
methods are just beginning to appear in clinical tomographs, in a very elementary
form.
The achievement of the objectives formulated in the introduction requires a range
of choices related to the methodology as well as to the practical aspects of

3D Reconstruction in X-ray Tomography
7
reconstruction in XRCT; the coherence of these choices is essential for obtaining a
satisfactory performance. The situation that we have brieﬂy outlined reveals that the
tools to guide these choices are relatively scarce, and limited in scope. In what
follows, we present how we have made these choices as well as the resulting method.
1.3. Method
Recall that the objective is to develop a 3D reconstruction XRCT method, capable
of processing data generated by clinical tomographs and of creating reconstructed
images of better quality than these latter. In addition, the method should work with
current equipment such as PCs, and consistency in the choice of its various
components appears as essential to achieve these goals. In this section, we detail and
discuss these choices.
1.3.1. Data formation models
One of the critical elements in the choice of a data formation model is accounting,
or not, for the polyenergetic nature of the X-ray source and the dependence of μ with
regard to energy. However, as mentioned earlier, studies dedicated to polyenergetic
models are rare, and even more those that allow the assessment of the respective
contribution of monoenergetic and polyenergetic models within an inversion
framework. Since the studied medium involved strongly attenuating objects (the stent
metal parts), it appeared necessary to develop methods based on both models, and to
compare their effectiveness for the targeted application. Another important element is
accounting for the non-zero surface of the source and of the detectors: artifacts linked
to a thin ray assumption appear during the reconstruction of high resolution images,
and that is precisely the situation in which we ﬁnd ourselves when trying to
accurately image small-sized vascular structures.
Even before developing the models in question, it is necessary to select a
representation of the projection matrix A, the important points being the explicit
(prior computation) or implicit (on-the-ﬂy computation) character of the operator, the
possibility of easily accessing rows or columns and, ﬁnally, the development of an
internal representation adapted to the retained formulation. We have opted for an
explicit representation of the projection operator on a single rotation, since such a
formulation leads to a lower amount of computation than implicit representations, at
the price however of signiﬁcant increase in memory space. Limiting the operator to a
single rotation introduces an additional constraint (integer number of slices of the
object per rotation) that is scarcely penalizing in practice. In order to limit the
memory space required for the storage of the operator, we have developed a coding
system for A that takes advantage of its sparse and structured character, that allows
quick access to the rows of the matrix and that lends itself to the efﬁcient

8
Regularization and Bayesian Methods for Inverse Problems
computation of projection and backprojection operations [GEN 08, GEN 11].
However, for the usual conﬁgurations of clinical tomographs, the storage of A
requires several GB. An approach to circumventing this difﬁculty is detailed in
section 1.3.3.
We now develop a formulation that allows both the thickness of rays and the
polyenergetic character of the source to be considered. To this end, it can be observed
that accounting for any of these phenomena amounts to performing a summation on
the parameter of the Poisson distribution of photon counts given in [1.3]. More
speciﬁcally, by using the independence of attenuation phenomena of different
photons, and the basic properties of the Poisson distribution, [1.3] becomes after
discretization:
N : P

n0
K

k=1
αke−(Aμ)k

[1.6]
In order to account for the thickness of rays, and under the hypothesis that only
detectors have a nonzero surface (the geometry of the source is rarely available), it is
considered that inﬁnitely thin rays reach each detector in K distinct positions, which
may be determined by regular sampling or random selection. In the above equation, it
then gives:
αk = 1/K
and
(Aμ)k = Akμ
[1.7]
and Ak represents the projection matrix under the hypothesis of thin rays obtained for
the k-th conﬁguration of the position of rays on the detectors.
Developing a polyenergetic model requires taking into account the dependence of
attenuation coefﬁcients with respect to energy. Discretizing the spectrum of the source
on K levels, and denoting by μk, the attenuation coefﬁcients of the object at the k-th
energy level yields:
αk : discretized spectrum of the source
(Aμ)k = Aμk
The fact that the object to reconstruct is no longer represented by μ, but by the K
attenuation coefﬁcients maps {μk 1 ≤k ≤K} leads both to a very large
underdetermination of the reconstruction problem,
and to difﬁculties in the
interpretation of the results due to redundancy of the information present in the
different maps. The Alvarez–Macovski decomposition enables μk to be expressed as

3D Reconstruction in X-ray Tomography
9
a function of the attenuation due to the photoelectric effect on the one hand, and of
that due to the Compton effect on the other hand. More speciﬁcally, it gives:
μk = Φ(k)φ + Θ(k)θ
where Φ(k) and Θ(k) are known deterministic functions, and where φ and θ
respectively denote the photoelectric and Compton coefﬁcients [DEM 01]. This
decomposition makes it possible to represent the object by the two maps φ and θ
rather than by the K maps μk. In order to end up with a single map, an empirical
parameterization of θ and φ is introduced on the basis of a single quantity, in general
μ70 (attenuation at 70 keV), this parameterization being determined from the
properties of a set of materials likely to be present in the object to be reconstructed
[DEM 01]. Finally, we obtain:
αk : discretized spectrum of the source
[1.8a]
(Aμ)k = A (Φ(k)φ(μ70) + Θ(k)θ(μ70))
[1.8b]
and it can be observed that this formulation is equivalent to performing a partial
decoupling of the dependence of the attenuation coefﬁcients with respect to space, on
the one hand (variable μ70), and with respect to energy, on the other hand (functions
Φ and Θ).
The previous developments helped to establish a common formulation to take
into account the thickness of rays and the polyenergetic character of the X-ray
source. The next step consists of using this formulation to develop an estimator of the
attenuation coefﬁcients. The most immediate approach is to use the Poisson
neg-log-likelihood of the vector of observed photons counts n, which is directly
derived from [1.6], [1.7] and [1.8]. Unfortunately,
the expressions of this
neg-log-likelihood and of its gradient with respect to the quantity of interest are
complex, and involve transcendental functions whose practical computation is
incompatible with a fast and effective implementation. To overcome this difﬁculty,
the Poisson neg-log-likelihood is approximated by its second Taylor expansion
around y ≜−ln n/n0, in the same spirit as [SAU 93, Appendix A]. The JV criterion
that derives therefrom takes the form:
JV = n0
2
y −
K

k=1
αk(Aμ)k

2
Σ
[1.9]
where Σ has been deﬁned in [1.4].

10
Regularization and Bayesian Methods for Inverse Problems
The above expression deserves interpretation. When the objective is to account for
the thickness of rays, [1.9] and [1.7] lead to:
JV = n0
2
y −
	 1
K
K

k=1
Ak

μ

2
Σ
[1.10]
Comparing [1.10] to [1.5], it can be observed that accounting for the thickness of
rays amounts to using as a projection matrix the mean of matrices Ak. This type of
procedure is similar to some of the empirical approaches mentioned in section 1.2.1
(methods known as ray-driven) and therefore constitute a form of justiﬁcation. The
link with other empirical approaches (including the methods known as pixel-driven
and distance-driven) seems more far-fetched. Our works will be directly based
on [1.10].
In order to account for the polyenergetic character of the X-ray source, [1.9]
and [1.8] give:
JV = n0
2
y −
	 K

k=1
αkΦk

Aφ(μ70) −
	 K

k=1
αkΘk

Aθ(μ70)

2
Σ
[1.11]
It can be observed that JV has a much simpler structure than the corresponding
Poisson neg-log-likelihood, and that the practical evaluation of its value and its
gradient is relatively easy to implement: these operations require two projections and
two backprojections, instead of one projection and one backprojection in the
monoenergetic case. In general, φ(μ70) and θ(μ70) have a nonlinear behavior, which
may complicate the computation of the gradient and degrade the numerical
conditioning of the problem. However, we have observed that, in practice, these
difﬁculties are not such as to call in question the advantages of the approach.
It should ﬁnally be mentioned that the approximation [1.9] makes it possible to
easily and simultaneously account for the polyenergetic character of the X-ray source
and the thickness of rays. As mentioned above, we will develop and compare methods
based on monoenergetic and polyenergetic models ; in either case, ray thickness will
be accounted for in accordance with [1.10].
1.3.2. Estimator
Irrespective of the type of data formation model retained, a maximum a posteriori
(MAP) estimator is used due to its interesting trade-off between simplicity and
adaptability to problems of variable difﬁculty. For the likelihood portion (or data ﬁt),
the quadratic approximation of the Poisson neg-log-likelihood is used, whose general

3D Reconstruction in X-ray Tomography
11
form is given in [1.9]. For the penalty portion, we use an additive, convex
“edge-preserving” term of the form:
JP =
M

m=1
ϕ [Dmμ]
[1.12]
where ϕ is a scalar L2L1 potential, where Dm refers to a – possibly weighted – matrix
of ﬁrst differences in a given direction m (horizontal, vertical, diagonal transverse,
etc.) and where, for any vector u = {un 1 ≤n ≤N}, the notation ϕ [u] denotes
the sum N
n=1 ϕ(un). Such a penalty term may be interpreted as an a priori Markov
distribution of the attenuation coefﬁcients ﬁeld. With regard to the precise choice of
the function ϕ, the following hyperbolic form is adopted for reasons of computational
simplicity:
ϕ(u) =

δ2 + u2
[1.13]
where δ denotes a scaling hyperparameter. Under these hypotheses, performing the
reconstruction amounts to minimizing the criterion J ≜JV + λJP with respect to
the variable of interest, where λ denotes the regularization parameter. In the case of
a monoenergetic model, the variable of interest is μ and J is convex, which lends
itself to iterative minimization with a descent directions method. In the case of a
polyenergetic model, the variable of interest is μ70, and the convexity of J cannot be
guaranteed. However, we will make the assumption that nonlinearities in the model
are sufﬁciently moderate to allow the use of the same minimization techniques as in
the monoenergetic case.
1.3.3. Minimization method
1.3.3.1. Algorithm selection
The minimization of J is an optimization problem of very large size, possibly
nonlinear, convex or nearly convex, and subject to boundary constraints to ensure the
positivity of the quantity of interest to reconstruct. Among the techniques capable of
addressing such a problem, we restrict our choice to gradient-based, descent
directions methods, due to their interesting trade-off between complexity and
numerical effectiveness for the size of the problem under consideration. Within this
type of techniques, the possibilities are multiple, both among generic methods
(derived from numerical analysis and from optimization) and among algorithms
speciﬁcally developed for tomography. The representation that we have adopted for
the projection operator compels us to eliminate the methods requiring an easy access
to the columns of A, such as the ICD method [SAU 93]. Even with these restrictions,
candidate algorithms are still numerous, and the literature offers few objective

12
Regularization and Bayesian Methods for Inverse Problems
elements capable of providing guidance in the choice of a particular method. That is
why we have conducted a comparative study of limited scope about the effectiveness
of several algorithms suited to our problem.
Developing a rigorous comparison methodology of iterative optimization
algorithms for a given application is a non-trivial task. As a ﬁrst step, it is necessary
to specify an implementation framework adapted to the targeted application. Here,
the focus is on sequentially programmed methods (or at least without any particular
concern for parallelization) running on standard hardware such as PCs. This allows
for the deﬁnition of performance metrics such as the total run-time, the number of
projections and backprojections, etc. In this context, the objective is to achieve a
given “image quality”, relatively to the subsequent use of the results. This requires us
to deﬁne image quality metrics such as resolution, ability to distinguish low
contrasts, etc., and then to ﬁnd a stopping rule, usable with all algorithms involved in
the comparison, which ensures that the desired image quality is reached. It is
essential that this stopping rule does not reﬂect the behavior of the algorithm, but the
intrinsic properties of the solution. In other words, the stopping rule should depend
on criterion J regardless of the algorithm used, which excludes some criteria
commonly used such as thresholds on the norm of the solution updates or on the
variation of the criterion between successive iterations. Clearly, a stopping rule
related to optimality conditions satisﬁes the requirements of independence with
respect to the algorithm, and a threshold based on the norm of the projected gradient
is a candidate of choice because of its availability and computational simplicity. It
should be noted that the use of this type of stopping rule eliminates non-convergent
algorithms, and in particular ordered subsets type methods yet very widely used in
tomographic reconstruction. Furthermore, comparison results should be independent
of speciﬁc instances of the observations, which requires us to repeat the same
experiment several times in order to perform a statistical comparison of the
performance of the different methods.
In order to satisfy these requirements, we proposed a comparison methodology
[HAM 11] based on simulated data derived from phantoms containing random
elements that can be used to compute quality metrics of reconstructed images. The
simulated data set allows for a statistical comparison of performance measures for
each algorithm with a common stopping rule. In addition, a statistical comparison of
quality metrics of images reconstructed by each algorithm enables verifying that they
are of statistically indistinguishable quality. We therefore make sure that the chosen
stopping rule guarantees that the achieved image quality is independent of the
minimization algorithm. Such tests can obviously be carried out for several values of
the stopping rule (for example, low, medium and high reconstruction qualities) to
evaluate the performance of different algorithms in various situations.
The methodology presented above was used to compare four minimization
algorithms. Here, we only report the key elements of this study, and the reader is

3D Reconstruction in X-ray Tomography
13
referred to [HAM 11] for further details on the methodology and the results. The four
algorithms were selected because of their good anticipated performance with a
large-size nonlinear problem such as reconstruction in XRCT. Two of them were
“generic”:
the ﬁrst, L-BFGS-B [ZHU 97], is a
limited-memory quasi-Newton
method modiﬁed so as to integrate bound constraints; the second, IPOPT [WAC 06],
is an interior point method based on sequential quadratic programming that can use a
quasi-Newton type of approximation of the Hessian or of the Lagrangian. The other
two methods were speciﬁcally developed for tomography: the ﬁrst, SPS [ERD 99],
operates on quadratic functions surrogate to convex criterion J; the second, TRIOT
[AHN 06], is based on the same principle of quadratic surrogate functions, but uses
an ordered subsets-based approach modiﬁed to preserve the convergence of the
procedure. Comparisons were performed in a 2D framework with the monoenergetic
model, for simplicity reasons and to limit the computation time. We advance the
hypothesis that results obtained in this framework remain valid in more complex
situations (3D geometry, polyenergetic model). Our results have shown that, when
the quality of the desired reconstruction is low to moderate, speciﬁc algorithms, and
notably TRIOT, have a slight advantage. When high quality reconstructions are
desired,
generic algorithms,
and particularly L-BFGS-B, have a signiﬁcant
advantage. This behavior is illustrated in section 1.4.1. The practical objective of the
works being to visualize small structures with accuracy, and therefore with high
image quality, we selected the L-BFGS-B algorithm.
1.3.3.2. Minimization procedure
With 3D clinical data, directly using the L-BFGS-B algorithm to minimize
criterion J is not an option, both due to the huge size of array A and to the
corresponding computation time. However, for our vascular imaging application, the
objective is to accurately visualize very small structures of interest. This leads
naturally to decompose the object into a region of interest, that contains the structures
to reconstruct with accuracy, and a background containing the rest of the object that
does not need to be accurately imaged. From an algebraic point of view, such a
decomposition is readily obtained by partitioning μ into μRI (region of interest) and
μAP (background), and by partitioning A correspondingly. The extension of this
decomposition to the cases of thick rays and polyenergetic model does not present
any difﬁculty.
At the methodological level, it is possible, under certain technical conditions, to
estimate μRI when μAP is unknown [COU 08]. However, practical experience shows
that the reconstruction of μRI is both faster and of better quality when a reasonable
estimation μAP is available. Therefore, we consider that quantities μRI and μAP must
be both reconstructed. The central issue is the impact on the estimation of μRI of
errors in μAP such as a coarse resolution and/or a high estimation error. To accurately
estimate μRI while dedicating minimum effort in the determination of μAP, several
procedures can be envisioned; among these, we may single out (1) the prior

14
Regularization and Bayesian Methods for Inverse Problems
determination of μAP either from images provided by the tomograph or by iterative
reconstruction at a coarse resolution or with low quality; (2) the joint or sequential
estimation of μRI and μAP.
These issues are discussed in detail in [HAM 10], and it appears that the quality
of μAP has little impact on the quality of the reconstruction of μRI. This point is
illustrated in section 1.4.2. Therefore, the preferred option is to determine μAP once
and for all, prior to the reconstruction of μRI. The easiest way to obtain this prior
estimation is to use reconstructions produced by the tomograph, if these are available.
By default, reconstruction by analytical methods, or by iterative methods with a coarse
resolution (typically one quarter of that used for μRI) and a low quality, can be used.
In all cases, a signiﬁcant reduction in the volume of computation and in memory
requirements is obtained : the prior estimate of μAP is either available, or computed
analytically, or rebuilt iteratively in a small number of iterations, the projection matrix
being of small size. The projection matrix used to reconstruct μRI is also of small
size, and this set of features makes it possible to implement the procedure on non-
specialized, if not modest, computer equipment.
1.3.4. Implementation of the reconstruction procedure
Finally, the method we have developed consists of minimizing the criterion
J
= JV + λJP where JV and JP are respectively deﬁned by [1.9]–[1.11] and
[1.12]–[1.13]. The complexity of the data collection geometry (multiple detectors
arrays, helical acquisition mode, ﬂying focal spot, etc.) makes the construction of
matrix A difﬁcult and tedious, without being all the more overwhelming. For
questions of computation speed, we have opted for prior construction over one
rotation and storage of the projection operator. The formulation we have developed
brings forward the possibility of systematically accounting for the thickness of rays
in order to avoid artifacts produced in high-resolution reconstructions by the
hypothesis of point detectors, and if necessary to use polyenergetic modeling of
attenuation phenomena. With regard to the precise shape of penalty term JP, we have
selected
a
second-order
neighborhood
within
each
slice,
and
a
ﬁrst-order
neighborhood across slices (ten-neighbor model). This choice is justiﬁed by the
anisotropy of the representation of μ, the thickness of the slices being generally
much larger than the pixel size in each slice. Finally, the decomposition of the object
into background and region of interest is systematically used, because it corresponds
to the characteristics of the reconstruction problem
while being necessary to
implement the method with the available computational resources. In most cases,
μAP is determined from the images provided by the tomograph, and the expression of
criterion J must be modiﬁed to involve μRI only. To perform the minimization, the
L-BFGS-B algorithm is used, in accordance with the results of the comparison of the

3D Reconstruction in X-ray Tomography
15
optimization algorithms and the targeted reconstruction quality. In addition, the use
of this algorithm ensures in a simple and rigorous manner the positivity of the
estimate of μRI.
As for most inversion methods, the proposed reconstruction procedure requires
the value of several parameters to be set, notably the number of rays per detector, the
discretization stepsize of the spectrum of the X-ray source for the polyenergetic
model, the value of the stopping rule, the value of hyperparameters λ and δ. The
unsupervised determination of such parameters is an important and difﬁcult problem
in inversion; conceptually, such an approach could be envisaged here for a number of
quantities (notably λ). However, from a practical point of view, this approach is to be
ruled out due to the size of the reconstruction problem and to the amount of
computation it would require. In addition,
empirical determination of these
parameters, according to the targeted quality objectives, does not raise major
problems because of the availability of numerical and physical phantoms, and of the
possibility of assessing the quality of reconstructions by experts. Thus, we have
determined that ﬁve to ten rays per detector are largely sufﬁcient to avoid artifacts
during the reconstructions of μRI, and that a decomposition of the spectrum of the
X-ray source into pulse components and smooth-variation curve make it possible to
limit to ten the number of elements in the sums appearing in [1.11]. The other
quantities may be set up by a calibration-based procedure to be carried out for each
acquisition protocol.
1.4. Results
The objective of this section is not to provide experimental justiﬁcation of the
choices on which the method we have developed is based, but, more modestly, to
support and illustrate by typical results some of the assertions made in the preceding
paragraphs.
1.4.1. Comparison of minimization algorithms
In order to compare the four selected minimization algorithms, we have
implemented the procedure described in section 1.3.3.1. It requires, as a ﬁrst step, the
deﬁnition of quality metrics of the reconstructed images and the design of random
phantoms
enabling
the
calculation
of
these
metrics.
Among
the
desired
characteristics in the reconstructed images, we have retained the resolution, and the
ability to distinguish low contrasts. It should be mentioned that a third commonly
used characteristic, the residual noise affecting the reconstructed image, is strongly
linked to the previous characteristics and therefore has not been retained. To evaluate
these characteristics, we have used a 2D numerical phantom (220 mm sidelength,
discretization on a 512 × 512 grid) composed of a homogeneous medium with the

16
Regularization and Bayesian Methods for Inverse Problems
the attenuation of soft tissues, in which eight contrast gauges (disks of 10 mm in
diameter whose attenuation value is close to that of the background) and eight
resolution gauges (strongly attenuating disks of diameters ranging between 0.5 and
3 mm) are inserted. The random character of the phantom is achieved by partitioning
it into 24 ﬁxed cells, and by randomly placing 16 gauges in these 24 cells. An
instance of such a phantom is represented in Figure 1.1. The quality metrics of the
reconstructed images are deﬁned as follows:
for the ability to distinguish low
contrasts, detection of contrast gauges is performed by matched ﬁltering in the eight
cells containing a contrast gauge and in the eight empty cells; the metric is equal to
the proportion of accurate results. For resolution, the “modulation transfer function”
(MTF) is used [DRO 82, HAM 10], which can be interpreted as a 1D frequency
representation of the 2D impulse response of the reconstruction procedure, under the
assumptions of spatial invariance and rotational symmetry. The impulse response is
obtained by simple least-squares estimation from the original phantom and the
reconstructed image, and the resolution metric is deﬁned as twice the value of the
normalized frequency for which the amplitude of the MTF has decreased by 40%
relative to its maximum value.
−100
−50
0
50
100
−100
−50
0
50
100
Figure 1.1. Instance of the 2D random numerical phantom used for the comparison of
optimization algorithms (based on [HAM 11]). Axis scales in mm
The data were obtained by simulating a current generation tomograph, in 2D axial
mode. The geometrical parameters of a Siemens SOMATOM 16 scanner were used,
the projection process being simulated by the SNARK09 software [DAV 09] taking
into account the polyenergetic character of the source. To avoid the inverse crime
[KAI 07] as much as possible, an analytical parameterization of the phantoms was
used, and, in order to better model the propagation of X-rays in the medium, the

3D Reconstruction in X-ray Tomography
17
detectors were separated into several cells whose photon counts were then combined.
To carry out the comparison, 50 independent instances of the random phantom were
drawn, and the random phenomena involved in the data formation process were also
independent of each other.
The criterion used for the reconstructions was based on a monoenergetic data
formation model. The difference between the model used to generate data and that
used to perform the reconstructions corresponds to the most frequent practical
situation. In a preliminary step, the hyperparameter values were calibrated from a
data set different from those used for the comparisons, and three values of the
stopping rule, corresponding visually to a moderate, good and very good quality of
the reconstructed image, were determined. The criterion J was then minimized with
the four methods mentioned in section 1.3.3.1, for each of the 50 datasets. In order to
make the comparison as fair as possible, all methods were initialized in an identical
fashion (image obtained after 20 iterations of the non-convergent OS-SPS method,
itself initialized by an analytical reconstruction), and the elements having a
signiﬁcant impact on the numerical performance of algorithms were implemented in
the same way. The used performance measure was the number of projections and
backprojections required to satisfy the stopping rule, this measure being directly
related to the total volume of computation required to perform the reconstruction. For
each of the three values of the stopping rule, a comparison of the quality measures of
the reconstructed image was made, to ensure lack of any statistically signiﬁcant
difference from one algorithm to another.
The results are synthesized in the diagram of Figure 1.2, which presents the
performance measure and its standard deviation (in positive values for better
readability) for each algorithm as a function of the value of the stopping rule. These
results show that, for a high value of the stopping rule (low reconstruction quality),
the TRIOT method is the most effective, closely followed by L-BFGS-B and SPS,
the IPOPT algorithm being much slower. However, for lower values of the stopping
rule (better image quality), algorithms speciﬁcally developed for tomography are
outperformed by generic methods IPOPT and L-BFGS-B, the latter presenting the
best overall performance. This behavior can be explained by the fact that SPS and
TRIOT are essentially gradient algorithms, and that their asymptotic convergence is
therefore slower than that of quasi-Newton methods such as IPOPT and L-BFGS-B.
A low variance of performance measurements can be observed, except for IPOPT.
This behavior could be attributed to the particular nature of this algorithm (interior
point method). In addition, it should be mentioned that for each of the three selected
stopping rule values, the quality of the reconstructions provided by the four
algorithms showed no statistically signiﬁcant difference. These results are in
agreement with the behavior deﬁned in section 1.3.3.2 and justify our choice of the
L-BFGS-B method to carry out the minimization of criterion J in our reconstruction
methods.

18
Regularization and Bayesian Methods for Inverse Problems
1.4.2. Using a region of interest in reconstruction
Our purpose is to illustrate the performance of the region of interest-based
minimization procedure described in section 1.3.3.2. Speciﬁcally, we are interested
in the total reconstruction time as well as in the quality of the reconstructed region of
interest for various acquisition modes of the background. In the previous paragraph,
we presented a way to evaluate and to compare the quality of images reconstructed
by several methods. Here, this technique cannot be directly applied, because the
images to compare do not result from the same estimator, since the background
varies from one reconstruction to another. This is why the quality of regions of
interest is evaluated in a more limited way, on the one hand by visual inspection for
the same setting of hyperparameters, and, on the other hand, by calculating the
resolution of regions of interest reconstructed for a same estimation variance, these
being obtained by an adequate adjustment of the hyperparameters.
Number of projections + backprojections
10
−2
10
−1
10
0
10
1
10
2
10
3
 
 
L−BFGS−B
IPOPT
SPS
TRIOT
Stopping criterion (descending order)
Figure 1.2. Comparison of the average performance of the four retained algorithms, depending
on the value of the stopping rule (from [HAM 11]). Algorithms speciﬁcally developed for
tomography are more effective for high stopping criteria (low quality of reconstruction), but
are outperformed by generic methods L-BFGS-B and TRIOT for stricter stopping rules
The tests were carried out on data simulated in a 2D framework. The phantom
(sidelength of 200 mm, discretization on a 1240 × 1240 grid) being composed of a
homogeneous medium with the attenuation of soft tissues, in which several objects
were placed: ﬁrst, in the peripheral zone (background), 10 strongly attenuating disks
were included; their function was to allow us to assess the impact of an inaccurate
representation of such objects on the reconstruction quality of the region of interest.
Two versions of the phantom, in which the attenuation of the disks was either that of
iron or that of aluminum, were used. Second, the central part (region of interest, with
a 30 mm sidelength and 186 × 186 grid size) comprised two types of objects,

3D Reconstruction in X-ray Tomography
19
respectively moderately and strongly attenuating, arranged in such manner that
resolution and estimation variance could be simply assessed. The phantom is
represented in Figure 1.3, and the corresponding data were generated using the same
procedure as that described in section 1.4.1.
 
 
200
400
600
800
1000
1200
200
400
600
800
1000
1200
−1000
−500
0
500
1000
1500
2000
2500
3000
a) Full phantom, of size 1240 × 1240
 
 
20
40
60
80
100
120
140
160
180
20
40
60
80
100
120
140
160
180
−1000
−500
0
500
1000
1500
2000
2500
3000
b) Central part, corresponding to the region of interest, of size 186 × 186
Figure 1.3. Phantom used to test the approach to reconstruction of the medium by
decomposition in background and region of interest (based on [HAM 10]). The gray levels
correspond to an attenuation in Hounsﬁeld units, on a [−1000,3000] scale

20
Regularization and Bayesian Methods for Inverse Problems
From these data, the background was reconstructed ﬁrst, with an analytical method
(ﬁltered backprojections) at full resolution, and second, by iteratively minimizing the
criterion J under a monoenergetic assumption, using the L-BFGS-B algorithm, with
resolutions of 1,240, 480, 240, 160, 120, 80 and 40. The quality of reconstruction
not being critical, the stopping rule was set to a high value. For each resolution of
the background, the region of interest was then reconstructed, still with the proposed
iterative method under the monoenergetic assumption, under the following conditions:
for visual assessment, the regularization parameter was set to a low value, so as to
bring forward the artifacts related to the imperfect representation of the background,
and the value of the stopping rule was set to an intermediate value (good quality of
reconstruction). For the evaluation of the resolution, the regularization parameter was
adjusted for each reconstruction so as to obtain an estimation variance similar to that
of the analytical reconstruction. There again, the value of the stopping rule was set to
an intermediate value common to all reconstructions. As in the previous paragraph,
the resolution was evaluated from the MTF of the reconstruction [DRO 82, HAM 10].
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
a) Background iterative
rec., res. 1240
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
b) Background iterative
rec., res. 480
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
c) Background iterative
rec., res. 240
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
d) Background iterative
rec., res. 160
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
e) Background iterative
rec., res. 120
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
f) Background iterative
rec., res. 80
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
g) Background iterative
rec., res. 40
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
h) Background
analytical rec., res. 1240
Figure 1.4. Analysis of regions of interest obtained with a low common value of the
regularization parameter (based on [HAM 10]). Each thumbnail represents the difference
between the exact region of interest and the region of interest reconstructed with a particular
background, on a
[−250, 250] scale in Hounsﬁeld units. The background disks presented
the attenuation characteristics of iron. There is an almost complete absence of artifacts for
background resolutions higher than 120
Figure 1.4 presents the results of the reconstruction of the region of interest for a
low value of the regularization parameter when background discs have the

3D Reconstruction in X-ray Tomography
21
attenuation characteristics of iron. In order to better appreciate the presence of
artifacts, each thumbnail represents the difference between the exact region of
interest and the reconstructed region of interest with a particular background. When
the background is reconstructed iteratively, lack of artifacts can be observed for
resolutions higher than 120, which corresponds to a factor greater than 10 between
the sidelengths of pixels of the background and of the region of interest. Signiﬁcant
artifacts can also be observed when the analytically reconstructed background is used
at full resolution. When the background disks have the attenuation characteristics of
aluminum (less attenuating than iron), the results of the region of interest
reconstruction are similar to those of Figure 1.4, except in the case of the analytically
reconstructed background where no artifacts are observed [HAM 10].
MTFs corresponding to the reconstructions of the region of interest with
backgrounds obtained iteratively with different resolutions are presented in
Figure 1.5. The background disks had the attenuation characteristics of iron. It can be
observed that there is no signiﬁcant loss of resolution in the region of interest for
background resolutions greater than 120. Similar results were obtained when
background disks had the attenuation characteristics of aluminum [HAM 10]. These
results are thus consistent with the visual inspection of reconstruction artifacts.
Finally, our experiments showed that the total volume of computation required to
reconstruct the region of interest is lower when the background is obtained
analytically, then grows according to the resolution chosen to iteratively reconstruct
the background.
These results indicate that the quality of the reconstruction of the region of
interest is relatively insensitive to that of the background. In most cases, an analytical
reconstruction of the background with full resolution of the background is sufﬁcient,
except in the presence of very strongly attenuating objects (e.g. prostheses with
stainless steel components). In this case, iterative reconstruction with low resolution
(factor up to 10 on the pixel sidelength) should be used, which results in a substantial
increase in the total amount of computation. These simulation results have been
conﬁrmed by tests on real data carried out with a physical resolution phantom (CTP
528 section of the Catphan600© phantom, see [HAM 10] for full results). This
validates, at least in a 2D context, the reconstruction approach by decomposition of
the object into background and region of interest, and conﬁrms that it is unnecessary
to carry out the joint, simultaneous or sequential estimation of μAP and μRI.
1.4.3. Consideration of the polyenergetic character of the X-ray source
The results reported in Figure 1.5 support the choices made in section 1.3.3
regarding the algorithm and the minimization procedure, at least in the framework
adopted to perform those tests (2D geometry, monoenergetic model). We hypothesize
that these results remain valid in more complex situations (3D geometry,

22
Regularization and Bayesian Methods for Inverse Problems
polyenergetic model). The subject of this section is to brieﬂy illustrate the effect of
accounting for the polyenergetic nature of the source, ﬁrst with simulated data in 2D,
then with real data in 3D.
Amplitude
0
0.1
0.2
0.3
0.4
0.5
0
0.2
0.4
0.6
0.8
1
 
 
40
80
120
160
240
480
1240
Reduced frequency
Figure 1.5. MTF of regions of interest reconstructed with backgrounds obtained iteratively
at different resolutions (according to [HAM 10]). Background disks with the attenuation
characteristics of iron. It can be observed that there is no signiﬁcant loss of resolution for
background resolutions greater than 120. Similar results were obtained when the background
disks had the attenuation characteristics of aluminium
1.4.3.1. Simulated data in 2D
Data were generated using the same phantom (disks located in the background:
attenuation characteristics of iron) and the same simulation process as for the
evaluation of the approach by region of interest. This choice is justiﬁed by the
presence of strongly attenuating objects within the phantom, and by the fact that the
polyenergetic character of the
X-ray source is accounted for in the projection
simulation process. First, the background was reconstructed analytically, on a 512 ×
512 grid. Second, the region of interest was estimated by the proposed method, under
the monoenergetic and polyenergetic hypotheses, with the same resolution as that of
the background (size 80 × 80). In both cases, the hyperparameters were empirically
adjusted, and the stopping rule was set to a low value (very good quality of
reconstructed image) such as to bring forward the differences between methods.

3D Reconstruction in X-ray Tomography
23
 
 
20
40
60
80
10
20
30
40
50
60
70
80
−1000
−500
0
500
1000
1500
2000
2500
3000
a) Reconstruction of the region of interest under the
monoenergetic hypothesis
 
 
20
40
60
80
10
20
30
40
50
60
70
80
−1000
−500
0
500
1000
1500
2000
2500
3000
b) Reconstruction of the region of interest under the
polyenergetic hypothesis
Figure 1.6. Results of the reconstruction of the region of interest under monoenergetic and
polyenergetic hypotheses. The gray levels correspond to the attenuation at 70 keV expressed
in Hounsﬁeld units, on a [−1000,3000] scale. Under a monoenergetic hypothesis, it can be
observed that there are many artifacts, and several structures present in the region of interest
are not reconstructed. Under a polyenergetic hypothesis, these defects have disappeared almost
completely
An example of reconstructions of regions of interest is presented in Figure 1.6. It
can be observed that under a monoenergetic hypothesis, the reconstructed image
presents visible defects: shadow areas due to the disks of the background, signiﬁcant

24
Regularization and Bayesian Methods for Inverse Problems
underestimation of the attenuation at the center of the image, absence of certain
structures. These defects have almost completely disappeared from the reconstructed
object when accounting for the polyenergetic character of the source during the
reconstruction of the region of interest. This improvement is achieved at the cost of
an increase in the volume of computation by a factor ranging between 2 and 3.
These results, and the many other tests we have carried out, indicate that
accounting for the polyenergetic character of the X-ray source in the reconstruction
method leads to a signiﬁcant improvement in the quality of reconstructions, in the
presence of strongly attenuating objects, both in the background and in the region of
interest. It should be underlined that this improvement depends on the accuracy of
the polyenergetic attenuation model, which in practice, can be difﬁcult to assess, or
even questionable.
1.4.3.2. Real data in 3D
The results presented here were obtained from the multimodality vascular
phantom schematically represented in Figure 1.7. It consists of a tube, simulating a
blood vessel, with two calibrated stenoses, one of which is located inside a stent.
This setup is included in a block of agar simulating soft tissue, and a contrast agent
can be injected into the tube. The phantom was imaged with a clinical tomograph
(Siemens SOMATOM 16) using the protocol used for peripheral vascular
examinations (16 arrays of 672 detectors each, helical mode, 1,160 projections per
rotation, ﬂying focal spot). The reconstructions were performed by the tomograph in
accordance with this protocol (512× 512 images, 0.7 mm spaced slices) on a ﬁeld of
view of 15 cm in diameter.
Figure 1.7. Multimodality vascular phantoms used to generate the data.
The stent is placed around one of the two stenoses
Reconstructions were carried out with the proposed method, under monoenergetic
and polyenergetic hypotheses, using the raw data collected by the tomograph. It
should be emphasized that these “raw” data have undergone extensive preprocessing,
part of which aiming to limit the artifacts related to the polyenergetic character of the

3D Reconstruction in X-ray Tomography
25
X-ray source. Therefore, this casts a doubt on the accuracy of the polyenergetic
attenuation model that we use. The chosen region of interest had a sidelength of
3.25 cm and was discretized on a 128 ×128 × 94 grid to obtain the same voxel size
as that of the reconstruction generated by the tomograph. The background was
reconstructed analytically by the tomograph, on a full ﬁeld of view 50 cm in diameter
(512 × 512 grid). The hyperparameters were empirically adjusted so as to achieve
the same estimation variance as that of images provided by the tomograph, and the
stopping rule was set to an average value to obtain a satisfactory image quality
without unduly increasing the volume of computation.
mm.
mm.
 
 
−15
−10
−5
0
5
10
−45
−40
−35
−30
−25
−20
−200
0
200
400
600
800
1000
a) Tomograph
mm.
mm.
 
 
−15
−10
−5
0
5
10
−45
−40
−35
−30
−25
−20
−200
0
200
400
600
800
1000
b) Monoenergetic model
mm.
mm.
 
 
−15
−10
−5
0
5
10
−45
−40
−35
−30
−25
−20
−200
0
200
400
600
800
1000
c) Polyenergetic model
Figure 1.8. Results of the reconstruction of the region of interest. Gray levels correspond to
the attenuation at 70 keV expressed in Hounsﬁeld units, on a [−200,1100] scale. The proposed
reconstructions present a signiﬁcant improvement
An example of reconstruction is presented in Figure 1.8. The selected slice
corresponds to the extremity of a stent ﬁtted with distal markers. It can be observed
that the method we have developed produces a signiﬁcant improvement over the
image provided by the tomograph, notably with regard to the presence of shadow

26
Regularization and Bayesian Methods for Inverse Problems
areas and to the size of the strongly attenuating markers. However, reconstructions
under monoenergetic and polyenergetic hypotheses appear almost identical.
A more complete evaluation of these methods was made from the reconstructions
of eight vascular phantoms. After result anonymization and random ordering, several
clinically signiﬁcant parameters were evaluated in two ways: by semi-automatic
measures resulting from a clinical image analysis software, and by manual
measurements independently carried out by two physicians. Statistical analysis
conﬁrmed that iterative reconstructions provide more accurate indications about the
shape and size of the stent. In addition, this analysis showed that, under a
polyenergetic hypothesis, intra-stent stenosis measurements are more accurate (in the
statistical sense) than in the other two methods. These results thus conﬁrm the
conclusions drawn from the 2D simulation results.
1.5. Conclusion
Our goal was to develop a 3D reconstruction method in XRCT capable of
processing clinical data for vascular imaging. The difﬁculties related to the modeling
of the data formation process and to the size of the corresponding estimation problem
have been overcome, at least partially, by developing a parsimonious representation
of the projection operator, by decomposing the medium to reconstruct into
background and region of interest, by developing an optimization procedure tailored
to this decomposition and by carefully selecting the minimization algorithm. The last
difﬁculty mentioned in the introduction, that is to say, problems to access the raw
data, has been overcome by combining a certain degree of collaboration with the
manufacturer and a signiﬁcant dose of reverse engineering in order to determine the
precise structure of the dataﬁles. The method we have developed provides room for
many improvements aiming to accelerate computations particularly with regard to
the representation and the coding of the projection / backprojection operator, and to
the implementation of the optimization procedure (massive parallelization for
example). Nevertheless, the results indicate that, for some speciﬁc applications,
methods such as those presented here are of real practical interest, because they
provide reconstructed images of a signiﬁcantly higher quality than the images
produced by clinical apparatus.
More generally, image reconstruction is a typical example of an ill-posed inverse
problem, and it is interesting to analyze the nature of the difﬁculties and the manner
to overcome them with regard to the elements presented in Demoment’s synthesis
article [DEM 89], which reﬂects the state of our knowledge a little more than 20
years ago. Demoment’s article focuses on the methodological aspects of solving
image reconstruction and restoration problems, essentially under the assumption of
linearity of the data formation model. The article insists on the notion of ill-posed
inverse problem, on various regularization techniques and on the question of the

3D Reconstruction in X-ray Tomography
27
determination of the value of hyperparameters. The article also deals, in a less
detailed manner, with the case of nonlinear data formation models such as those
encountered in diffraction tomography, and with the difﬁculties of the practical
computation of the solution.
With regard to 3D reconstruction in XRCT, the method presented here, as well as
recent literature, clearly indicate that the ill-posed character of the problem, and the
regularization techniques necessary for its solution, are now recognized and taken
into account by the research community. The question of the speciﬁcation of the
value of hyperparameters does not pose any major practical difﬁculty, due to the
existence of speciﬁc acquisition protocols which allow us to calibrate reconstruction
methods. The interest of the community seems to have shifted toward the application
of these methods to real large-size datasets, along with the problems of computation
time and implementation arising therefrom. The development of methods usable in
practice involves the harmonious choice of a data formation model,
of a
minimization procedure for the resulting criterion, and of speciﬁc optimization
algorithms. The method that we have presented is a – quite imperfect – attempt
pointing in this direction. A signiﬁcant proportion of recent works in 3D XRCT also
follows this path [KAC 11].
This situation does not seem limited to XRCT. The same trend can be perceived
in emission tomography and diffraction tomography, but with certain nuances such as
the attachment of certain communities to the automatic determination of the value of
hyperparameters [BER 97]. This can be interpreted as the result of a widespread
recognition of the methodological issues synthesized in [DEM 89]. This recognition
has made it possible to address problems hampered by the presence of nonlinearities
in the data formation process or by their ill-conditioning, with the frequent additional
difﬁculty of a large volume of data. This explains the important place taken by
implementation issues. This brief analysis thus brings forward the impact that
methodological studies may have, some 20 years later, on the approach to various
problems and the development of practical solutions.
1.6. Acknowledgments
The works presented here began in 2006 and have beneﬁted from the contribution
of many collaborators. In particular, we should mention Jean-Pierre Dussault
(University of Sherbrooke, Canada), Gilles Soulez, Guy Cloutier, Gilles Beaudoin
and Soﬁane Hadjadj (Centre de recherche du Centre hospitalier de l’Université de
Montréal (CRCHUM) – Research Center of the University of Montreal Health
Centre, Canada). Students and postdoctoral fellows involved in the project have had a
decisive impact on its evolution and on the results obtained. We should emphasize the
contributions of Malik Dussaud, David Gendron, Benoît Hamelin and Saul Pérez.

28
Regularization and Bayesian Methods for Inverse Problems
Research partially funded by the Natural Sciences and Engineering Research
Council of Canada (Discovery grant no. 138417-06 and Collaborative health research
project no. 323572-06), and by the Canadian Institutes of Health Research
(Collaborative health research project no. 323572-06).
1.7. Bibliography
[AHN 06] AHN S., FESSLER J.A., BLATT D., et al., “Convergent incremental optimization
transfer algorithms: Application to tomography”, IEEE Transactions on Medical Imaging,
vol. 25, no. 3, pp. 283–296, March 2006.
[ALV 76] ALVAREZ R.E., MACOVSKI A., “Energy-selective reconstructions in X-ray
computerized tomography”, Physics in Medicine and Biology, vol. 21, no. 5, pp. 733–744,
September 1976.
[AMB 73] AMBROSE J., HOUNSFIELD G.N., “Computerized transverse axial tomography”,
British Journal of Radiology, vol. 46, no. 542, pp. 148–149, February 1973.
[BER 97] VAN DEN BERG P.M., KLEINMAN R.E., “A contrast source inversion method”,
Inverse Problems, vol. 13, no. 6, pp. 1607–1620, December 1997.
[BOA 04] BOAS D.A., DALE A.M., FRANCESCHINI M.A., “Diffuse optical imaging of
brain activation: approaches to optimizing image sensitivity, resolution, and accuracy”,
NeuroImage, vol. 23, pp. S275–S288, 2004.
[BOU 93] BOUMAN C.A., SAUER K.D., “A generalized Gaussian image model for
edge-preserving MAP estimation”, IEEE Transactions on Image Processing, vol. 2, no. 3,
pp. 296–310, July 1993.
[COU 08] COURDURIER M., NOO F., DEFRISE M., et al., “Solving the interior problem
of computed tomography using a priori knowledge”, Inverse Problems, vol. 24, no. 6,
December 2008.
[DAV 09] DAVIDI R., HERMAN G.T., KLUKOWSKA J., “SNARK09:
A programming
system for the reconstruction of 2D images from 1D projections”, 2009. Available at:
www.snark09.com.
[DEL 98] DELANEY A.H., BRESLER Y., “Globally convergent edge-preserving regularized
reconstruction: an application to limited-angle tomography”, IEEE Transactions on Image
Processing, vol. 7, no. 2, pp. 204–221, February 1998.
[DEM 89] DEMOMENT G., “Image reconstruction and restoration: Overview of common
estimation structures and problems”, IEEE Transactions on Acoustics, Speech and Signal
Processing, vol. ASSP-37, no. 12, pp. 2024–2036, December 1989.
[DEM 01] DE MAN B., NUYTS J., DUPONT P., et al., “An iterative maximum-likelihood
polychromatic algorithm for CT”, IEEE Transactions on Medical Imaging, vol. 20, no. 10,
pp. 999–1008, October 2001.
[DEM 04] DE MAN B., BASU S., “Distance-driven projection and backprojection in three
dimension”, Physics in Medicine and Biology, vol. 49, no. 11, pp. 2463–2475, June 2004.

3D Reconstruction in X-ray Tomography
29
[DRO 82] DROEGE R.T., MORIN R.L., “A practical method to measure the MTF of CT
scanners”, Medical Physics, vol. 9, no. 5, pp. 758–760, September 1982.
[ERD 99] ERDO ˇGAN H., FESSLER J.A., “Ordered subsets algorithms for transmission
tomography”, Physics in Medicine and Biology, vol. 44, no. 11, pp. 2835–2851, November
1999.
[FES 94] FESSLER J.A., “Penalized weighted least-squares image reconstruction for positron
emission tomography”, IEEE Transactions on Medical Imaging, vol. 13, no. 2, pp. 290–
300, 1994.
[GEN 08] GENDRON D., GOUSSARD Y., BEAUDOIN G., et al., “Reconstruction of real
tomographic data using algebraic methods”, IEEE International Conference of the
Engineering in Medicine and Biology Society, Vancouver, BC, Canada, pp. 2717–2720,
August 2008.
[GEN 11] GENDRON D., GOUSSARD Y., HAMELIN B., et al., “Reconstruction of real
tomographic data using algebraic methods”, Proceedings of the 11th International Meeting
on Fully 3D Image Reconstruction, Potsdam, Germany, pp. 343–346, July 2011.
[HAM 10] HAMELIN B., GOUSSARD Y., DUSSAULT J.-P., et al., “Design of iterative ROI
transmission tomography reconstruction procedures and image quality analysis”, Medical
Physics, vol. 37, no. 9, pp. 4577–4589, September 2010.
[HAM 11] HAMELIN B., GOUSSARD Y., DUSSAULT J.-P., et al., “Performance comparison
on nonlinear solvers for statistical reconstruction in transmission tomography”, IEEE
Transactions on Medical Imaging, October 2011.
[HER 71] HERMAN G.T.,
ROWLAND S.W.,
“Resolution in ART: An experimental
investigation of the resolving power of an algebraic picture reconstruction technique”,
Journal of Theoretical Biology, vol. 33, pp. 213–223, 1971.
[HER 73] HERMAN G.T., ROWLAND S.W., “Three methods for reconstructing objects
from X-rays: a comparative study”, Computer Graphics and Image Processing, vol. 2,
pp. 151–178, 1973.
[HER 76a] HERMAN G.T., LENT A., “A computer implementation of a Bayesian analysis of
image reconstruction”, Information and Control, vol. 31, no. 4, pp. 364–384, August 1976.
[HER 76b] HERMAN G.T., LENT A., “Quadratic optimization for image reconstruction. I”,
Computer Graphics and Image Processing, vol. 5, pp. 319–332, 1976.
[HER 79] HERMAN G.T., HURWITZ H., LENT A., et al., “On the Bayesian approach to image
reconstruction”, Information and Control, vol. 42, pp. 60–71, 1979.
[HOU 73] HOUNSFIELD G.N., “Computerized transverse axial scanning (tomography). 1.
description of system”, British Journal of Radiology, vol. 46, no. 552, pp. 1016–1022,
December 1973.
[KAC 11] KACHELRIESS M., RAFECAS M., (eds.), Proceedings of the 11th International
Meeting on Fully 3D Image Reconstruction, Potsdam, Germany, July 2011.

30
Regularization and Bayesian Methods for Inverse Problems
[KAI 07] KAIPIO J., SOMERSALO E., “Statistical inverse problems: discretization, model
reduction and inverse crimes”, Journal of Computational and Applied Mathematics,
vol. 198, no. 2, pp. 493–504, 2007.
[LEV 87] LEVITAN E., HERMAN G.T., “A maximum a posteriori probability expectation
maximization algorithm for image reconstruction in emission tomography”,
IEEE
Transactions on Medical Imaging, vol. MI-6, pp. 185–192, September 1987.
[SAU 93] SAUER K.D., BOUMAN C.A., “A local update strategy for iterative reconstruction
from projections”, IEEE Transactions on Signal Processing, vol. 41, no. 2, pp. 534–548,
February 1993.
[TRI 10] TRILLON A., Amélioration de méthodes de reconstruction de défauts à partir de
données issues de capteurs à courants de Foucault, thesis, École centrale de Nantes,
France/École polytechnique de Montréal, Québec, Canada, October 2010.
[VAU 11] VAUTRIN D., VOORONS M., IDIER J., et al., “Seismic imaging of transmission
overhead line structure foundations”, Computational Imaging IX (IS&T/SPIE Symposium
on Electronic Imaging), San Francisco Airport, CA, USA, January 2011.
[WAC 06] WÄCHTER A., BIEGLER L.T., “On the implementation of a primal-dual interior
point ﬁlter line search algorithm for large-scale nonlinear programming”, Mathematical
Programming, vol. 106, no. 1, pp. 25–57, May 2006.
[ZHU 94] ZHUANG W., GOPAL S.S., HEBERT T.J., “Numerical evaluation of methods for
computing tomographic projections”, IEEE Transactions on Nuclear Sciences, vol. 41,
no. 4, pp. 1660–1665, August 1994.
[ZHU 97] ZHU C., BYRD R.H., LU P., et al., “Algorithm 778. L-BFGS-B: Fortran subroutines
for Large-Scale bound constrained optimization”, ACM Transactions on Mathematical
Software, vol. 23, no. 4, pp. 550–560, 1997.

2
Analysis of Force-Volume Images
in Atomic Force Microscopy
Using Sparse Approximation
2.1. Introduction
Atomic force microscopy (AFM) is a recent technology that enables us to
characterize the physicochemical properties of objects and, in particular, biological
samples at the nano-scale. We are focusing on the modality known as force-volume
that consists of measuring force curves on a spatial grid, each force curve
representing the force of interaction between an AFM tip and the surface of the
nano-object as a function of their distance. A force curve is composed of two signals
relative to the approach and the retraction of the tip. The available methods to
interpret these three-dimensional (3D) data are essentially empirical. We propose an
automatic approach to reconstruct a set of two-dimensional (2D) images, each
representing a physicochemical property of the nano-object (topography, elasticity,
etc.). This process is based on the physical models available in the approach and
retraction phases. For the type of samples studied (bacteria), we use electrostatic
interaction and mechanical models in the approach phase and the freely jointed chain
(FJC) model that describes the stretching of biopolymers in the retraction phase.
The algorithm consists of piecewise smoothing each force curve in order to
determine the regions of interest in which the physical models apply. Piecewise
smoothing is reformulated as a sparse approximation problem using a polynomial
dictionary [SOU 11]. The parameters of interest are ﬁnally estimated by adjusting the
physical models in the least squares sense in each of the regions of interest. A
mapping (2D image) of each of the physical parameters is obtained by repeating the
Chapter written by Charles SOUSSEN, David BRIE, Grégory FRANCIUS and Jérôme IDIER.

32
Regularization and Bayesian Methods for Inverse Problems
force curve analysis in every pixel. The method is applied, on the one hand, to the
analysis of electrostatic and mechanical properties of a mutant of the bacterium
Escherichia coli (E. coli) and, on the other hand, to the analysis of exopolymers
produced by the bacterium Pseudomonas ﬂuorescens (P. ﬂuorescens) [POL 11].
We begin by presenting the type of data that can be acquired in force-volume
imaging and the physical parameters that are sought to be extracted from these data.
These parameters are closely related to physical models describing the interaction
of two nano-objects before and after contact. In section 2.3, we propose data analysis
methods. They are based on the concept of sparsity, in the sense where the force curves
contain a few characteristic points (jumps, or changes of slope or curvature) that must
be very precisely estimated. The chosen sparse approximation algorithms are research
methods of characteristic points on a discrete grid. These are generic algorithms for
sparse approximation whose scope is wider than the framework of piecewise signal
smoothing.
Section 2.4 presents these algorithms in a generic manner. Finally, in section 2.5,
a few typical results are shown with real data.
2.2. Atomic force microscopy
2.2.1. Biological cell characterization
The development of near-ﬁeld microscopy techniques and in particular of AFM
microscopy [BIN 88] makes it possible to determine in situ the local physicochemical
properties (electrical, magnetic, vibrational and mechanical) [WIE 94]. These
techniques provide spectroscopic measurements at the nano-scale independently of
the nature of the samples (biological, organic or mineral), and tri-dimensional
images, known as force-volume images in AFM microscopy [BUT 05].
The characterization of the physicochemical properties of biological objects,
particularly microorganisms (bacteria, yeasts, etc.) and animal cells, is a major issue
in areas as diverse as biology, microbiology, the pharmaceutical industry and clinical
medicine. For example, the determination of the electrostatic charge and the elasticity
of bacteria is a fundamental question for understanding the adhesion mechanisms of
bacteria and the infection processes. AFM microscopy is a powerful tool because
measurements can be performed in an aqueous medium or in physiological
conditions with a sub-nanometric spatial resolution. AFM force spectroscopy enables
the observation and the quantitative characterization of the mechanical properties of
deformable biological samples and the measurement of intramolecular interactions
between biomolecules. Given that some cancerous tumors and stem cells are
regulated by mechanical properties, a number of works in nano-medicine have shown
that certain diseases can be diagnosed by AFM microscopy [CRO 09, KAS 08].

Analysis of Force-Volume Images in AFM Using Sparse Approximation
33
2.2.2. AFM modalities
An AFM microscope is able to detect interatomic forces (e.g. capillary,
electrostatic, Van der Waals and frictions) exercising between a tip associated with a
cantilever with ﬁxed spring constant and the surface of a sample [BUT 05] (see
Figure 2.1). There are generally two data acquisition modes.
2.2.2.1. Isoforce and isodistance images
The data result from the scanning by the tip of the whole surface of the sample
according to two operating modes:
– the contact mode: a control system enables us to maintain constant the force
exerted on the cantilever supporting the tip during the scan. This force is proportional
to the deﬂection of the cantilever which is the quantity measured. This mode enables
us to obtain the topography directly, that is to say, the contour of the surfaces;
– the intermittent (or vibrating) mode: a variation of interaction forces is reﬂected
by a variation of the cantilever vibration frequency which is the measured quantity. By
inverting the relationship between the force and the frequency, an image of the force is
obtained. In this case, the control system makes it possible to maintain constant either
the distance between the tip and sample, or the force acting on the cantilever. Thus,
this mode enables us to obtain isodistance or isoforce images.
2.2.2.2. Force spectroscopy
Conversely to the previous mode of acquisition, force spectroscopy is a pointwise
analysis of the sample, obtained by recording the deﬂection of the cantilever as a
function of the separation distance z between the AFM tip and the surface of the
sample (see Figure 2.1). A force curve f(z), therefore, shows the evolution of the
force with respect to the distance z at a location of the sample surface.
The general shape of a force curve is presented in Figure 2.2. The intensity of the
force is calculated from the measurement of the deﬂection of the cantilever, as a
function of the displacement of the tip z, where the largest values of the distance z
correspond to the most distant positions of the tip from the sample. In a force curve,
two signals can be distinguished corresponding to the approach and retraction of the
tip (the solid and dotted lines, respectively). In the following, we describe the
successive areas on these curves during the approach (areas A–C) and retraction
(areas D–E) phases:
– area A: no interaction is observed when the tip is placed at a sufﬁciently large
distance from the sample;
– area B: it corresponds to the surface forces (electrostatic and Van der Waals).
These interactions are negative (attraction between the tip and the surface) or positive
(repulsion). In this area, the tip is not yet in contact with the sample;

34
Regularization and Bayesian Methods for Inverse Problems
AFM cantilever
AFM tip
sample
Figure 2.1. Force spectroscopy. At a location of the sample surface, the nanometric tip moves
towards the sample until the deformable (the sample is mechanically compressed by the
movement of the tip) and then non-deformable contact (at a certain stage, the tip cannot further
distort the sample) with the sample. Then the tip gradually withdraws and loses ﬁnally contact.
In each phase, a signal f(z) is measured. These signals are called approach and retraction
curves
Mechanical properties
Surface forces
Viscoelasticity
Adhesion forces
Relative displacement z (nm)
Force f(z) (nN)
Non-deformable surfaces
Deformable surfaces
Figure 2.2. General shape of a force curve: approach (solid line) and retraction curves (dotted
line). Negative values (positive) represent the attraction forces (repulsion). The indentation δ(z)
is the deformation depth of the sample compressed by the tip. In the case of a non-deformable
sample, δ = 0. In the case of a deformable sample, δ gradually increases until attaining a
limit value corresponding to the end of the deformable regime and to the beginning of the non-
deformable regime (adapted from [GAB 07])

Analysis of Force-Volume Images in AFM Using Sparse Approximation
35
– the point of contact between the tip and the sample is located between areas B
and C;
– area C: mechanical interactions of the cantilever and/or the sample. For a non-
deformable sample, the observed behavior is mainly due to the linear deformation of
the cantilever. However, for a deformable sample, the process of compression and/or
indentation of the sample results in linear or nonlinear behaviors;
– area D: the presence of hysteresis between the approach and retraction curves
reﬂects the viscoelastic properties of the sample. In the case of non-deformable
surfaces, this hysteresis is zero;
– area E: during retraction, the curves can present a signiﬁcant adhesion force
depending on the surface area and the contact time, and especially on the surface
energy between the sample and the tip. Regarding microorganism surfaces, this area
presents many breaks that should be analyzed.
Figures 2.3(c) and 2.3(d) describe force curves measured for E. coli (approach)
and P. ﬂuorescens (retraction) bacteria in an aqueous medium. E. coli is present in the
human body, some of its pathogenic forms being responsible for severe intestinal
infections. P. ﬂuorescens is a bacterium that is present in the water distribution
networks. It is responsible for the formation of bioﬁlms by producing and excreting
exopolymers. These bioﬁlms may induce certain nosocomial diseases. On the
approach curve, the non-deformable contact predominates (increasing region on the
right) while on the retraction curve, numerous jumps in the signal can be observed
related to the progressive loss of contact with the exopolymers located on the surface
of the bacterium. Each decreasing area is a contact region between the tip and the
polymer in which there are physical models describing the stretching of the polymer.
2.2.2.3. Force-volume imaging
By reproducing the previous analysis and by scanning the surface of the sample, a
force-volume image f(x,y,z) is obtained. This image results from the collection of
force curves f(z) on a grid (x,y) representing the surface of the sample (see
Figure 2.4). The resolution in z is less than the nanometer, and the lateral resolution
(x,y) is of the order of the nanometer. A typical process (often carried out empirically
by physicists) consists of estimating the contact point between the tip and the sample
for a given force curve. A 2D image representing the topography of the sample is
obtained by repeating this process for each pixel (x,y), but this analysis still remains
partial since the data have a richer content than the only topography information. Our
objective is, therefore, to devise methods of robust processing in order to characterize
the topography, and also other physical properties of the sample.

36
Regularization and Bayesian Methods for Inverse Problems
a)
b)
−2500
−2400
−2300
−2200
−2100
−2000
−0.5
0
0.5
1
1.5
2
2.5
z (nm)
F (nN)
−3200
−3000
−2800
−2600
−2400
−1
0
1
2
3
4
z (nm)
F (nN)
c)
d)
Figure 2.3. Images of a mutant of the bacterium Escherichia coli a) and of a bacterium
Pseudomonas ﬂuorescens b). c) Approach curve relative to the bacterium Escherichia coli. The
z-axis is inverted in relation to ﬁgures 2.1 and 2.2. d) Retraction curve corresponding to the
course of a exopolymer produced and excreted by the bacterium Pseudomonas ﬂuorescens
x
y
z
Figure 2.4. Force-volume imaging: acquisition of a set of force
curves on a grid (x,y) representing the surface of the sample

Analysis of Force-Volume Images in AFM Using Sparse Approximation
37
2.2.3. Physical piecewise models
The remaining of this chapter will focus on force spectroscopy and force-volume
imaging. The schematic overview of Figure 2.2 is the piecewise modeling of the
approach and retraction curves, where a piece corresponds to a region in which a type
of interaction occurs. We now introduce the selected physical models for each of the
pieces. The purpose of this section is not to draw an exhaustive panorama of the
existing models, but to deﬁne a few physical models adapted to the biological
samples studied in section 2.5. For a more complete panorama, we refer the readers
to [BUT 05, JAN 00, LIN 07]. The retained models in this chapter are graphically
illustrated in Figure 2.5.
a)
b)
c)
Figure 2.5. Piecewise models for the approach and retraction curves (adapted from [POL 11]).
Regarding the approach, three regions of interest are deﬁned in the two domains z →f(z) a)
and δ →f(δ) b), where δ(z) is the indentation deﬁned by [2.2]. The transition points z = Z0
and z = Z1 indicate the beginning and the end of the electrostatic interaction. Z1 also indicates
the beginning of the deformable contact (Hertz’s law). In the z-domain, it is often difﬁcult
to see the transition Z2 between the deformable and non-deformable contacts, that is why it
is not represented here. In the δ-domain, the indentation Δ2 corresponding to Z2 marks the
transition between the deformable and non-deformable contacts (Hooke’s law). c) In the case
of a retraction curve relative to the successive detachments of the monomers, the regions of
interest are the intervals [Zbegin
j
,Zend
j
] where the signal is decreasing, and the detachment areas
are the increasing parts of the curve. An FJC model is being used in every region of interest

38
Regularization and Bayesian Methods for Inverse Problems
2.2.3.1. Approach phase models
In the approach phase, physical models take into account the mechanical
properties of deformable samples, namely the viscoelastic properties of living cells
such as the elasticity of human cells and bacteria, as well as other physicochemical
parameters such as the charge density and the turgor pressure of bacteria covered by
speciﬁc proteins. In the case of the bacterium E. coli, we have successively observed
an electrostatic interaction before contact and then mechanical interactions after
contact. An exponential model is used for the electrostatic interaction [VEL 02]:
fElec(z; κ−1, A, Z1) = A exp (κ(z −Z1))
[2.1]
for z ⩽Z1, where Z1 is the “virtual contact point” and κ−1 is the Debye length, the
distance beyond which a signiﬁcant separation of charged objects can take place. It can
be noted that when the tip and the sample have a charge of the same sign, the contact
point is not well deﬁned because the interaction is repulsive. An alternative deﬁnition
of the contact point is the ﬁrst sample of the approach signal where the electrostatic
interaction is detectable in the measurements (point Z0 in Figure 2.5(a)).
Conversely to the electrostatic model that directly depends on the tip-to-sample
distance z, the mechanical interactions after the contact depend on the indentation of
the sample, that is to say, on the deformation depth of its external surface, deﬁned by:
δ(z) = z −Z0 −f(z) −f(Z0)
kc
[2.2]
where kc is the spring constant of the cantilever on which the tip lies. The indentation
depends not only on z but also on the measured force f(z). In the interval [Δ1,Δ2]
deﬁning the deformable contact (where Δ1 = δ(Z1); see Figures 2.5(a) and(b)),
Hertz’s model is written as:
fHertz(δ; E, Δ1, Δ2) ∝Eδ2
[2.3]
where E is the Young modulus, which is characteristic of the elasticity of the object,
and ∝indicates the proportionality. It should be noted that fHertz(δ; E,Δ1,Δ2)
depends on Z0 because δ depends implicitly on Z0. The parameter kc is a constant
provided by the constructor of the AFM tip. The model [2.3] is speciﬁc to a tip with a
conical or pyramidal geometry. There are more complex models for spherical or ﬂat
tips [LIN 07]. Regarding non-deformable contact, we use Hooke’s model. This is an
afﬁne interaction [SEN 05] of the type:
fHooke(δ; a, kcell, Δ2) = a + kcell δ
[2.4]
where δ > Δ2. The slope kcell is the spring constant of the bacterium. It is closely
linked to its turgor pressure [POL 11, YAO 02].

Analysis of Force-Volume Images in AFM Using Sparse Approximation
39
2.2.3.2. Retraction phase models
During the retraction of the AFM tip, the biomacromolecules located on the
surface of the biological sample are progressively stretched and detached from the tip
until the deﬁnitive loss of contact, in a similar way to detaching a self-gripping
adhesive velcro strip. The successive stretches correspond in Figure 2.5(c) to the
intervals [Zbegin
j
,Zend
j
] in which the curve is decreasing (see also the experimental
curve in Figure 2.3). These intervals are separated by increasing parts related to the
successive detachments of monomers from the tip. The models mostly used in the
literature are called FJC and Worm-Like Chain (WLC) [JAN 00]. Regarding the FJC
model, the monomers are modeled by rigid segments connected two-by-two by
ﬂexible joints. With regard to the WLC model, each element of the chain is no longer
a rigid but a ﬂexible segment (Figure 2.6). This last model is well adapted to the
modeling of protein constituents. There are also inelastic extensions named FJC+ and
WLC+, which bring forward the supplementary physical parameters [JAN 00].
FJC Model
WLC Model
Figure 2.6. Freely Jointed Chain and Worm-Like Chain models for
polymers, seen respectively as a chain of rigid or ﬂexible segments
In this chapter, we will be restricted to the FJC model because the studied
exopolymers are polysaccharides for which the FJC model is well suited. Unlike the
WLC model, the FJC model is not deﬁned by a function z →f(z) but by the
reciprocal function for the samples of the jth interval z
∈
[Zbegin
j
,Zend
j
]
(see Figure 2.5(c)):
zFJC(f; LC, ℓk) = −LC

coth
	 fℓk
kBT

−
	 fℓk
kBT

−1
[2.5]
where kB, T and ℓk are Boltzmann’s constant, the temperature and Kuhn’s length
characterizing the average length of a segment of the chain, and LC represents the
total length of the contour. The number of monomers is deﬁned by the ratio LC/ℓk.

40
Regularization and Bayesian Methods for Inverse Problems
2.3. Data processing in AFM spectroscopy
2.3.1. Objectives and methodology in signal processing
We retain that a force curve is described by a set of parametric piecewise models.
In order to extract physical information (topography, elasticity measurement of the
bacterium, numbers of monomers, etc.), it is essential to detect the regions of interest
(the pieces) where the parametric models are applied. This is the main difﬁculty of
the data analysis. Once the regions of interest are detected, the physical parameters
are estimated with the least-squares method in each region. The processing of a force
curve is ﬁnally summarized in three steps:
– search for discontinuity points in the curve (segmentation) and piecewise
smoothing of the curve;
– determination from the smoothing results, of the regions of interest in which the
parametric models apply. At this stage, the topography (Z1) of the sample is obtained
in the approach phase and the intervals [Zbegin
j
,Zend
j
] describing the successive course
of the polymers are obtained in the retraction phase;
– adjustment of each parametric model in the region of interest where it applies.
This leads during the approach phase to estimate the Debye length for the electrostatic
interaction, Young’s modulus that is characteristic of the elasticity, the turgor pressure
and the elasticity constant of the bacteria in the non-deformable contact area. With
regard to the retraction, the analysis of adhesive proteins (polymers) covering the body
of the bacteria leads to estimate the number of monomers, the total length of the
polymer and the average length of the monomers.
This methodology is common to the approach and retraction curves. The
segmentation of the pieces, which is the most critical point, will be discussed in
detail in section 2.3.2. For the last two points, and notably for the detection of the
regions of interest, we propose solutions as automatic as possible, but sometimes it is
necessary to introduce some empirical adjustment parameters. We now summarize
these two last points.
2.3.1.1. Detection of the regions of interest
Figure 2.5 illustrates that in the case of both approach and retraction curves, the
points that are located at the extremity of every region of interest are discontinuity
points where the force curve shows a jump and/or a slope and/or curvature change.
Regarding the approach curves, the tip-sample contact point is deﬁned as the
discontinuity point (z
=
Z1) of the derivative f ′(z) and the contact area is
materialized by the interval z ⩾Z1 (see Figure 2.5(a)). In the case of retraction
curves, the regions of interest are the intervals in which the signal is decreasing
(Figure 2.5(c)).

Analysis of Force-Volume Images in AFM Using Sparse Approximation
41
2.3.1.2. Parametric model ﬁtting
Once the regions of interest are detected, the last step consists of ﬁtting each
parametric model to the corresponding region of interest. The physical parameters are
estimated using the least-squares method while minimizing a criterion of the type:

z∈Rj

f(z) −fj(z; θj)
2
[2.6]
where Rj designates the jth region of interest and fj(z; θj) is the parametric model
deﬁned for the jth region of interest. θj gathers the shape parameters of the force
curve (for example, θj equals {A,κ−1} for the electrostatic model [2.1] and
{Δ2,E,a,kcell} for the Hertz [2.3] and Hooke [2.4] models all together). Regarding
the retraction phase, the FJC model takes the form f →zj(f; θj). The least-square
criterion relates this time to the quadratic errors (z −zj(f; θj))2 in the z-domain
instead of the f-domain. Although the number of parameters to estimate rarely
exceeds ﬁve, the optimization of the criterion [2.6] must be carefully carried out
because the criterion is likely to present local minima or ﬂat valleys.
2.3.2. Segmentation of a force curve by sparse approximation
The objective of this process is to restore a piecewise smooth signal g(z) from
an experimental force curve f(z) where g is a denoised version of the signal f. The
notion of sparsity is linked to the small number of pieces or in other words, to the small
number of discontinuity points corresponding to jumps in the signal and/or to changes
of slope and/or of curvature. The search for these points will drive us to resolve a
sparse approximation problem. Our analysis is similar to polynomial spline smoothing
[DIE 95] with the difference that the control points are not arbitrarily chosen but in
an adaptive manner [SMI 96]. It is equally similar to the estimation of a piecewise
polynomial signal from sampled data [VET 02]. Unlike the approach presented in
[VET 02], our approach relies on the minimization of a compound criterion made of
a quadratic data ﬁtting term and a regularization term enabling the joint detection of
discontinuities of order 0 and/or 1, . . . , P −1, where a discontinuity of order p is
deﬁned as a jump in the pth derivative of the signal.
2.3.2.1. Detecting jumps in a signal
First, order 0 is considered. To restore a piecewise constant signal g, it is proposed
to minimize the criterion:
J (g; λ) =

z

f(z) −g(z)
2 + λ

z
φ

g′(z)

[2.7]

42
Regularization and Bayesian Methods for Inverse Problems
φ :
→
+ is an even function and increasing on
+, and the penalization term

z φ

g′(z)

has the role to constrain the signal g(z) to be piecewise constant with
a small number of pieces 1. The parameter λ regulates the compromise between the
degree of sparsity (the number of pieces) and the quality of the approximation of f
by g. For a large value of λ, the approximation error is large but the number of pieces
is small. Approximations with better quality are obtained for small values of λ, at the
cost of a larger number of pieces.
To constrain the ﬁrst derivative of the signal g to be sparse, that is non-zero for
a limited number of values of z only, it is necessary that φ is non-differentiable at 0
[FAN 01]. It is, for example, the case of the ℓ1 penalization φ(t) = |t|. This choice
nevertheless presents the inconvenience of further peralizing the high values of |g′(z)|
and complicates the restoration of large amplitude jumps in signal f. The choice of a
non-convex function φ enables us to compensate this problem. We choose to utilize
the ℓ0 cost (or pseudo-norm ℓ0). It is the binary function deﬁned by:
φ(t) = |t|0 =
 0 if t = 0
1 if t ̸= 0
[2.8]
The cost function J is a mixed “ℓ2 – ℓ0” criterion because it is composed of a
quadratic term and a term linked to the ℓ0 pseudo-norm that counts the number of
pieces:
J (g; λ) =

z

f(z) −g(z)
2 + λ

z
|g′(z)|0
[2.9]
It should be noted that as φ(t) = | t |0 is highly non-convex, the criterion J is not
convex and generally presents local minima. There exists an optimal effective
algorithm speciﬁc to the criterion [2.9] [BLA 89], but to minimize the criteria
introduced below as extensions of [2.9], it will be necessary to resort to suboptimal
algorithms.
2.3.2.2. Joint detection of discontinuities at different orders
To simultaneously detect discontinuity points with several orders in a signal (a
discontinuity point is either a jump in the signal, or a slope or a change of curvature),
the criterion [2.9] is replaced by:
J (g0, . . . , gP −1; λ) =

z
	
f(z) −
P −1

p=0
gp(z)

2
+ λ
P −1

p=0

z
|g(p+1)
p
(z)|0
[2.10]
1. We adopt a continuous notation but g′(z) designates in fact the discrete derivative of g in the sense
of ﬁnite differences.

Analysis of Force-Volume Images in AFM Using Sparse Approximation
43
The piecewise smoothed signal is now expressed in the form g = 
p gp, where
each signal gp is a piecewise polynomial with degree p (g0 is piecewise constant, g1
is piecewise afﬁne, etc.). The discontinuity points of order p are the points for which
the derivative of order p + 1 of gp is non-zero. For example, for P = 3, the piecewise
smooth signal is expressed by g = g0 + g1 + g2, where g′
0, g(2)
1
and g(3)
2
are sparse. g
is nothing else than a piecewise quadratic function.
2.3.2.3. Scalar and vector variable selection
We are studying two alternative strategies for the research of discontinuity points,
called scalar and vector variable selection:
– for the selection of scalar variables, the discontinuities detected at different
orders are not generally in a same position z. This occurs when [2.10] is minimized.
A “scalar variable” is deﬁned as a pair (z,p) that describes the position z of a
discontinuity and its order p;
– in order to restore a piecewise polynomial signal with a minimal number of
pieces, it is advantageous to impose the detection of discontinuities for all the
orders p = 0, . . . , P −1 in the same position z, counting 1 (or a discontinuity
position) instead of P (P variables). Thus, the discontinuity in a position z is
associated with a “vector variable” that groups together the amplitudes {g(p+1)
p
(z),
p = 0, . . . , P −1} of P discontinuities relative to the position z, for a unitary cost.
Therefore, it is proposed to replace the regularization term of the criterion [2.10] by
λ||g||∞,0, where:
||g||∞,0 ≜

z
 max
p=0:P −1 |g(p+1)
p
(z)|

0
[2.11]
This term is a mixed ℓ∞,0 pseudo-norm, deﬁned as a composition of the ℓ∞norm
in the dimension p (for ﬁxed z, the maximum of the values g(p+1)
p
(z), p = 0, . . . , P −
1 is calculated) and the ℓ0 pseudo-norm in dimension z, that counts the number of non-
zero values. The activation of the discontinuities at all the orders in a same position z
is favored since the penalization is equal to λ (and no more) as soon as g(p+1)
p
(z) ̸= 0
for at least a value of p.
2.4. Sparse approximation algorithms
Rather than developing an algorithm speciﬁc to the minimization of [2.10] or the
penalized extension [2.11], we are choosing a more general framework:
the
approximation of a signal from a limited number of elements of a dictionary
including the researched structures [ELA 10]. We will see in section 2.4.3 that the
minimization of the criterion [2.10] and its “vectorized” extension refers to this
framework, where the dictionary is a Toeplitz or block-Toeplitz matrix that represents
one or several numerical integrating operators.

44
Regularization and Bayesian Methods for Inverse Problems
2.4.1. Minimization of a mixed ℓ2-ℓ0 criterion
The problem of (scalars) variable selection in a dictionary is formulated as follows,
with f ∈
m the vector gathering the data (f(z) for all z) and A a matrix of size
m × n, called dictionary. The purpose is to ﬁnd a representation of f using a small
number of columns of A, that is to ﬁnd an approximation f ≈Ax such that x ∈
n
is a sparse vector (i.e. containing a large number of zero elements). This problem is
traditionally formulated by:
min
∥x∥0⩽k{E(x) = ∥f −Ax∥2}
[2.12]
or by:
min
x∈
n {J (x; λ) = E(x) + λ∥x∥0}
[2.13]
where ∥· ∥denotes the Euclidean norm and the ℓ0 cost ∥x∥0 ≜n
i=1 |xi|0 counts
the number of non-zero elements of x, and the integer k or the real λ controls the
degree of sparsity. The formulation [2.12] explicitly imposes that k columns, at most,
are selected, while [2.13] is similar to the formulation [2.9]–[2.10] introduced for
piecewise smoothing.
2.4.2. Dedicated algorithms
It should be noted that [2.12] and [2.13] are combinatorial optimization problems.
As a matter of fact, from the moment the support of x, denoted Q, is accessible, the
estimation of the amplitudes xi is reduced to the least-square problem:
min
t
∥f −AQt∥2
where AQ is the extracted matrix of A by selecting all the columns indexed by Q and
t = x|Q designates the restriction of x to the elements indexed by Q.
Problems [2.12] and [2.13] are known to be NP-hard [NAT 95]: there is generally
no optimal algorithm except the one that consists of performing an exhaustive search
for the support Q. However, a number of suboptimal algorithms have been developed
[ELA 10]. Two large families of algorithms can be distinguished. The ﬁrst category
consists of replacing [2.12]–[2.13] with continuous optimization problems, which are
simpler to solve. In particular, the use of the ℓ1 −norm as a replacement for the ℓ0
cost has been widely studied [ZIB 10]. The algorithms of the second category are
directly dedicated to problems [2.12] or [2.13]:
iterative threshold algorithms
[BLU 08, NEE 09] and “greedy” algorithms that select a new column of the

Analysis of Force-Volume Images in AFM Using Sparse Approximation
45
dictionary at each iteration, in such a way that at the kth iteration, k columns are
selected. Among these latter, let us quote the classical orthogonal matching pursuit
(OMP) [PAT 93] and orthogonal least squares (OLS) algorithms [CHE 89] whose
structure is similar: at each iteration, the data f are projected orthogonally onto the
subspace generated by the selected columns. The difference between the two is that
the numerical cost of the selection of the new column is noticeably more important in
the case of OLS [BLU 07] but in compensation, it can be empirically observed that
OLS gives much better results for dictionaries with some highly correlated columns
[SOU 11].
In the following, we use OLS for the optimization of [2.12] and an extension of
OLS, named single best replacement (SBR) [SOU 11], for the optimization of [2.13].
SBR, which performs with a ﬁxed λ, is a deterministic descent algorithm that
consists, from an initial support, of decreasing as much as possible the criterion
J (x; λ) by adding or withdrawing a single variable in the support. SBR is inspired
by the single most likely replacement (SMLR) algorithm, initially proposed to
address deconvolution problems of pulse trains modeled by a Bernoulli–Gaussian
(BG) process [CHA 96, CHA 10, GOU 90, KOR 82]. SBR is an extension of SMLR
to a limit BG model with inﬁnite variance, for problems in which only a sparse prior
is imposed, but no prior on the amplitudes. SBR is one of the forward–backward
algorithms. The signiﬁcance of this type of algorithm relatively to greedy algorithms,
such as OMP and OLS, is that a previously activated column can be deactivated as
soon as the quadratic error does not signiﬁcantly increase [MIL 02, Chapter 3].
OLS and SBR are summarized in Algorithm 2.1. At each iteration, OLS selects
the column ai (i /∈Q) that causes the largest decrease in the approximation error:
EQ∪i =
min
t∈
k+1 ∥f −AQ∪{i}t∥2
[2.14]
where k is the cardinality of the support Q. In the end, OLS gives approximations
Ax(k) of f for the consecutive values of the cardinality k. In practice, the practitioner
does not necessarily make an a priori decision regarding the choice of k but can stop
the algorithm when it states that qualitatively, the principal structures (discontinuities)
have been found or that the approximation of data f by Ax(k) becomes sufﬁciently
correct. We will make the latter choice below. To describe SBR, the notation Q • {i}
can be introduced to indicate an elementary replacement, that is the insertion (• = ∪)
or the removal (• = \) of a column. At each iteration, SBR searches for the elementary
replacement that enables the largest possible decrease in the criterion value J (x; λ).
SBR selects, therefore, the column ai (i ∈{1, . . . , n}) that leads to the smallest value
of:
JQ•{i}(λ) ≜EQ•{i} + λCard[Q • {i}]
[2.15]

46
Regularization and Bayesian Methods for Inverse Problems
Algorithm 2.1: SBR algorithm deﬁned as a descent method to minimize J (x; λ)
for ﬁxed λ [SOU 11]. • indicates the insertion (• = ∪) or the removal (• = \) of
a column in the list of the selected columns. SBR coincides with OLS [CHE 89]
for λ = 0. In this case, only insertion tests are carried out and the algorithm
is stopped when a maximum cardinality is attained, in other words, when the
approximation error EQ is lower than a given threshold.
Input: A,f, λ > 0
Output: support Q
Do Q = ∅and JQ(λ) = ∥f∥2;
while JQ(λ) decreases do
for every i ∈{1, . . . , n} do
calculate JQ•{i}(λ) deﬁned in [2.15]
Do Q ←Q • {ℓ} where ℓ∈arg mini JQ•{i}(λ);
Update JQ(λ) = JQ•{ℓ}(λ);
2.4.3. Joint detection of discontinuities
We show now that problem [2.10] can be written in the form [2.12] or [2.13].
The case of the vector variant using the penalty [2.11] instead of the ℓ0 cost will be
discussed in section 2.4.3.3. Let us recall that the data f(z) are represented by the
vector f = [f(z1), . . . , f(zm)]t ∈
m, where the position z of a discontinuity point
is described by the index i ∈{1, . . . , m} of the corresponding sample.
In order to minimize the criteria [2.9] and [2.10], classical sparse synthesis
algorithms cannot be directly used, because the regularization term does not take the
form λ ∥g∥0: it is a combination of terms such as λ ∥Dpg∥0 where Dp is a discrete
derivation operator of order p. With regard to the use of traditional sparse algorithms,
we have recourse to a change of variable, such as x = Dpg ⇐⇒g = Apx, where
Ap is a numerical integration operator. The effect of this change of variable is that
the ﬁrst term ∥f −g∥2 in [2.9] and [2.10] becomes ∥f −Apx∥2.
2.4.3.1. Construction of the dictionary
We address the detection of discontinuities as a problem of variable selection,
where a variable (i, p) is associated with both the position i and the order p of a
discontinuity. The dictionary is built in such a way that all types of searched
discontinuity can be present for a given position. Thus, for discontinuities of order p,

Analysis of Force-Volume Images in AFM Using Sparse Approximation
47
we construct the Toeplitz matrix Ap whose columns result from sampling on a
regular grid the monomial z →zp, by considering all possible discontinuity points:
Ap =
⎡
⎢⎢⎢⎢⎢⎢⎣
1p
0
· · ·
0
2p
1p
...
3p
2p
...
...
...
...
0
...
...
· · · 1p
mp (m −1)p · · · 2p
⎤
⎥⎥⎥⎥⎥⎥⎦
[2.16]
Ap is a numerical integration operator which consists of integrating P times a
pulse signal in position i, with the limit condition that the resulting signal is zero
before the position i. Figure 2.7 illustrates this deﬁnition in the cases p = 0, 1 and 2.
The columns of the matrices A0, A1 and A2 are, respectively, Heaviside’s discrete
functions, ramp functions and one-sided quadratic functions.
The dictionary enabling the detection of joint discontinuities of orders
p = 0, . . . , P −1 is ﬁnally obtained by appending all the dictionaries:
A =

A0 | A1 | . . . | AP −1
[2.17]
and the piecewise smoothing of a polynomial signal [2.10] is ﬁnally rewritten
as the approximation of f by Ax = P −1
p=0 Apxp, where xp represent the blocks
that constitute the vector x. The elements xp
i represent here the amplitude of the
discontinuities (similar to g(p+1)
p
(z) in [2.10]).
Signal a0
i
Signal a1
i
Signal a2
i
Figure 2.7. Signals ap
i related to the activation of a discontinuity of order
p in position i. Each signal is equal to 1 at the position i and its support
is equal to {i, . . . , m}

48
Regularization and Bayesian Methods for Inverse Problems
2.4.3.2. Selection of scalar variables
The selection of scalar variables is formulated by problems [2.12] and [2.13]. To
address this problem, we apply the OLS and SBR algorithms, respectively. OLS
provides ﬁrst coarse approximations with few discontinuity points (small values of
k), and then ﬁner approximations with more discontinuity points. The stopping
criterion chosen for OLS is the following. It is considered that the main
discontinuities have been found when the approximation error ∥f −Ax∥2 is less
than a threshold. The value of the threshold is automatically adjusted as a function of
the variance of the measurement noise, itself estimated from the experimental data
(in the ﬂat area of the force curves, where the force is theoretically zero).
Unlike OLS, SBR stops in a ﬁnite number of iterations and no adjustment is
necessary. The practical difﬁculty lies in setting up λ that we perform in an empirical
manner. SBR provides approximations progressively ﬁner when λ decreases. At
equal level of sparsity (for solutions whose supports are of same cardinality), SBR
provides more accurate approximations than OLS for a more signiﬁcant computation
time (related to the number of elementary replacements that have been performed)
[SOU 11].
2.4.3.3. Selection of vector variables
In the previous approach, the term ∥x∥0 counts the number of discontinuities for
each order and for each position. The detection of discontinuities at different orders
and for a same position (for example, it is imposed to simultaneously detect a jump
and a change of derivative in the signal for the same position i) consists of selecting
(activating) several columns of the matrix A simultaneously. In order to
mathematically formulate the selection of vector variables, the notation xi ∈
p is
used to gather all the coordinates of x related to a same position i and the vector x is
reordered as the concatenation of the vectors xi of size P: x = {x1, . . . , xm}. The
calculation of the number of selected positions i is written:
∥x∥∞,0 ≜
m

i=1
|∥xi∥∞|0
where |∥xi∥∞|0 is equal to 1 if xi ̸= 0, and 0 otherwise. The selection of vector
variables is ﬁnally formulated as the minimization of criteria:
min
∥x∥∞,0⩽k ∥f −Ax∥2
[2.18]
or:
min
x {∥f −Ax∥2 + λ ∥x∥∞,0}
[2.19]

Analysis of Force-Volume Images in AFM Using Sparse Approximation
49
where k controls the maximum number of authorized discontinuity points. This
enables the selection of at most k vector variables, or kp scalar variables.
The algorithms used to minimize [2.18] and [2.19] are simple adaptations of OLS
and SBR. At each iteration, the concern is now to test all possible addtions and
withdrawals of a single vector variable, that is, the simultaneous addition of P scalars
discontinuities in a position i (xi ̸= 0) or, if the position i is already active, testing
the simultaneous withdrawal of the active discontinuities (xi = 0).
2.5. Real data processing
The force-volume images have been acquired by an MFP3D-BIO microscope
(Asylum Res. Tech., Atomic Force F&E GmbH, Mannheim). The AFM tips, conical
in shape, have been built by Veeco (MLCT-AUNM, Veeco Instruments SAS,
Dourdan). Before proceeding with the acquisition of data, the geometry of the tip is
controlled by a 3D vision system (TGT1, NT-MTD Company, Moscow). Each force
curve is obtained by adopting a velocity of 1 μm.s−1 during the approach and the
retraction of the tip. The sample studied in the approach phase is a K-12 mutant of
the bacterium E. coli (E2152), supplied by the Institut Pasteur in Paris. The data
processed in the retraction phase are relative to the bacterium P. ﬂuorescens (see
Figure 2.3). This bacterium, present in water distribution networks, produces and
excretes exopolysaccharides. These complex sugars induce the formation of
signiﬁcant bioﬁlms, i.e., agglomerates of bacteria ﬁxed on the surfaces of water
distribution networks and which may induce many cases of nosocomial diseases.
2.5.1. Segmentation of a retraction curve: comparison of strategies
Figure 2.8 shows the comparative results of two methods of variable selection for
a retraction curve (we refer the readers to [DUA 10] for a more comprehensive study).
In both cases, we are simultaneously looking for the discontinuities of orders 0, 1 and
2 in the signal, which is equivalent to approximate it piecewise by a polynomial of
degree 2. The algorithm used is SBR, and the parameter λ is ﬁxed empirically as a
function of the quality of the desired approximation. The approximation by selection
of scalar variables does not generally detect discontinuities with different orders for
identical positions. On Figures 2.8(a) and (b), there are as many scalar variables as
discontinuity points (12 and 21 variables, respectively, that is 12 and 21 positions). The
vector approach yields only four and seven discontinuity points following the choice
of the parameter λ, which is equivalent (also) to select 12 and 21 scalar variables in
the dictionary. The intervals delimited by the discontinuity points make it possible to

50
Regularization and Bayesian Methods for Inverse Problems
identify in a more reliable manner the decreasing regions of the force curve in which
the FJC model applies.
3200
3250
3300
3350
3400
3450
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
 z (nm)
 F (nN)
Data
Approximation
Order 0
Order 1
Order 2
3200
3250
3300
3350
3400
3450
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
 z (nm)
 F (nN)
Data
Approximation
Order 0
Order 1
Order 2
a) k = 12 scalar discontinuities
b) k = 21 scalar discontinuities
3200
3250
3300
3350
3400
3450
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
 z (nm)
 F (nN)
Data
Approximation
Discontinuity points
3200
3250
3300
3350
3400
3450
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
 z (nm)
 F (nN)
Data
Approximation
Discontinuity points
c) k = 4 vector discontinuities
d) k = 7 vector discontinuities
Figure 2.8. Piecewise smoothing of a force curve by selecting scalar and vector variables,
using the SBR algorithm. The piecewise smooth signal is quadratic, in other words, we are
searching for discontinuities of orders p = 0, 1 and 2. The experimental force curve f(z) is
displayed with a dotted line and its approximation with a solid line. (a-b) Selection of scalar
variables, obtained with two different degrees of sparsity. (c-d) Selection of vector variables
for two levels of sparsity. The positions of discontinuity points are represented by a single circle
because three scalar discontinuities of orders 0, 1, and 2 are simultaneously present
2.5.2. Retraction curve processing
For the complete and systematic processing of a retraction curve, we apply the OLS
algorithm coupled to the selection of vector variables. Although SBR provides slightly
better results than OLS, in practice it is easier to deﬁne the degree of sparsity k, that
is the number of iterations of OLS rather than the parameter λ of SBR. k is controlled
automatically so that the residual ∥f −Ax(k)∥2 of the approximation of the data

Analysis of Force-Volume Images in AFM Using Sparse Approximation
51
with k variables is less than a ﬁxed threshold S, linked to the empirical variance of the
measurement noise that is calculated in the ﬂat part of the experimental signal (area
on the far right, where the force is theoretically zero). The results of the segmentation,
of the detection of the regions of interest (decreasing regions) and the ﬁtting of the
retraction curve by the FJC model, are presented in Figure 2.9 for the retraction curve
of Figure 2.3(d). For the segmentation, piecewise afﬁne smoothing of the signal was
used. It is observed that most of the decreasing regions are ﬁnely detected and that
the ﬁtting of the FJC model is very accurate. The algorithm has also been tested on a
very large number of real signals. In quantitative terms, it provides consistent results
with values known in the literature for the physical parameters of studied bacteria
[POL 11].
−3000
−2900
−2800
−2700
−2600
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
 z (nm)
 F (nN)
Data
Approximation
Discontinuity points
0
100
200
300
400
500
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
z−Z0 (nm)
 F (nN)
Data
Decreasing regions
a) Force curve segmentation
b) Detection of the regions of interest
0
100
200
300
400
500
−0.4
−0.35
−0.3
−0.25
−0.2
−0.15
−0.1
−0.05
0
z−Z0 (nm)
F (nN)
Data
FJC Model
Extrapolation
c) Fitting of the FJC model
Figure 2.9. Full processing of the retraction curve presented in Figure 2.3(d): segmentation
by the vectorized OLS algorithm a); detection of decreasing regions of the smooth signal b);
and ﬁtting the FJC model to the experimental data in each decreasing region c). The bold lines
represent the ﬁt of the FJC model in each region, and the thin lines indicate the extrapolation
of the model outside of the regions of interest (particularly toward the contact point z = Z0)

52
Regularization and Bayesian Methods for Inverse Problems
2.5.3. Force-volume image processing in the approach phase
Force-volume image processing is illustrated in the approach phase. For each
force curve of the image, the “vectorized” OLS algorithm is applied and seven
parameters are estimated by the procedure described in section 2.3.1, namely the
topography Z1, the indentation Δ2 which characterizes the transition between the
deformable and non-deformable interaction modes, the prefactor A and the Debye
length κ−1 for the electrostatic interaction, Young’s modulus E, the elasticity
constant kcell of the bacterium and its turgor pressure for the mechanical interaction.
We ﬁnally reconstruct seven 2D images of the physical parameters. Figure 2.10
depicts the image of the topography Z1 and that of Young’s modulus E. More
complete results, including the retraction phase, are available in [POL 11]. In the
retraction phase, the visualization of the results as images is more difﬁcult because
the number of regions of interest (and therefore the number of estimated physical
parameters Lc and ℓk) differs from one force curve to another.
5
10
15
20
25
30
5
10
15
20
25
30
0
200
400
600
800
5
10
15
20
25
30
5
10
15
20
25
30
0.2
0.4
0.6
0.8
1
1.2
1.4
a) Topography (nm)
b) Young modulus E (MPa)
Figure 2.10. Complete processing of a force-volume image in the approach phase. For a K-12
mutant sample of the bacterium Escherichia coli (E2152), the two images a) of the topography,
that is of the height of the surface, and b) of Young’s modulus characterizing the elasticity are
represented
2.6. Conclusion
The inverse problem presented in this chapter is the joint detection of the
discontinuities of different orders in a signal and its approximation by a piecewise
polynomial signal. By choosing an adapted dictionary, we formulated the problem as
a sparse approximation problem. To solve this problem by using an ℓ0 constraint is to
solve a discrete optimization problem in order to look for the position and the order
of the discontinuity points. OLS and SBR algorithms yield very precise segmentation
results, in a computation time of course a little more expensive than other simpler
sparse approximation algorithms, such as OMP and the algorithms that rely on the
relaxation of the ℓ0 pseudo-norm by the ℓ1 norm [SOU 11].

Analysis of Force-Volume Images in AFM Using Sparse Approximation
53
The ﬁrst presented (scalar) approach enables us to simultaneously detect the
discontinuities of different orders in a signal but without imposing their detection on
the same position (typically, a jump and a change of slope do not necessarily occur
simultaneously). As it is sometimes desirable to favor this simultaneous detection, we
have developed a vector variant in which the activation of discontinuities is imposed
for all the possible orders and at a given position for a unitary cost. This variant is
connected to the concept of group sparsity [KOW 09] where a neighborhood relation
is deﬁned between the columns of the dictionary and the simultaneous activation of
related columns is favored. We have shown the relevance of this approach for the
processing of force curves to demonstrate that it is possible to very ﬁnely segment the
experimental signals using a limited number of discontinuity points.
The ﬁtting of physical models in every segmented interval is carried out with least
squares. This is a conventional problem of continuous optimization whose variables
are the parameters of the physical models. As the number of parameters is small,
the application of local optimization methods is fast although it is necessary to take
precautions because certain overparameterized models may induce criteria with ﬂat
valleys or local minima.
As opposed to the independent processing of a group of force curves, a
perspective consists of envisaging a joint treatment. Therefore, the hypothesis will be
made that a force curve results from mixing signal sources where a source is linked
to a type of elementary interaction and the mixtures are linear combinations with
delayed signal sources. This model has been validated with experimental data in the
approach phase [SOU 08]. The joint processing of several force curves will ﬁnally
rely on a separation procedure of delayed sources, where each source is described by
a piecewise polynomial signal and the delays are linked to the topography of the
nano-object.
2.7. Bibliography
[BIN 88] BINNIG G., Atomic force microscope and method for imaging surfaces with atomic
resolution, US Patent no. US 4724318, February 1988.
[BLA 89] BLAKE A., “Comparison of the efﬁciency of deterministic and stochastic
algorithms for visual reconstruction”, IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. PAMI-11, no. 1, pp. 2–12, January 1989.
[BLU 07] BLUMENSATH T., DAVIES M.E., On the difference between orthogonal matching
pursuit and orthogonal least squares, research report, Edinburgh University, March 2007.
[BLU 08] BLUMENSATH
T.,
DAVIES
M.E.,
“Iterative
thresholding
for
sparse
approximations”, Journal of Fourier Analysis and Applications, vol. 14, no. 5, pp. 629–654,
December 2008.

54
Regularization and Bayesian Methods for Inverse Problems
[BUT 05] BUTT H.-J., CAPPELLA B., KAPPL M., “Force measurements with the atomic
force microscope: technique, interpretation and applications”, Surface Science Reports,
vol. 59, nos.1–6, pp. 1–152, October 2005.
[CHA 96] CHAMPAGNAT F., GOUSSARD Y., IDIER J., “Unsupervised deconvolution of
sparse spike trains using stochastic approximation”, IEEE Transactions on Signal
Processing, vol. 44, no. 12, pp. 2988–2998, December 1996.
[CHA 10] CHAMPAGNAT F., GOUSSARD Y., GAUTIER S., et al., “Deconvolution of spike
trains”,
in IDIER J., (ed.), Bayesian Approach to Inverse Problems, Chapter 5, ISTE,
London and John Wiley & Sons, New York, pp. 117–140, April, 2010.
[CHE 89] CHEN S., BILLINGS S.A., LUO W., “Orthogonal least squares methods and their
application to non-linear system identiﬁcation”, International Journal of Control, vol. 50,
no. 5, pp. 1873–1896, November 1989.
[CRO 09] CROSS S.E., JIN Y.-S., RAO J., et al., “Applicability of AFM in cancer detection”,
Nature Nanotechnology, vol. 4, pp. 72–73, 2009.
[DIE 95] DIERCKX P., Curve and Surface Fitting with Splines, Monographs on Numerical
Analysis, 2nd ed., Oxford University Press, New York, 1995.
[DUA 10] DUAN J., SOUSSEN C., BRIE D., et al., Restauration et séparation de signaux
polynômiaux par morceaux, Application à la microscopie de force atomique, PhD Thesis,
Université Henri Poincaré, Nancy, November 2010.
[ELA 10] ELAD M., Sparse and Redundant Representations: From Theory to Applications in
Signal and Image Processing, Springer, New York, 2010.
[FAN 01] FAN J., LI R., “Variable selection via nonconcave penalized likelihood and its
oracle properties”, Journal of Acoustical Society America, vol. 96, no. 456, pp. 1348–1360,
December 2001.
[GAB 07] GABORIAUD F., DUFRNE Y.F., “Atomic force microscopy of microbial cells:
application to nanomechanical properties, surface forces and molecular recognition forces”,
Colloids and Surfaces B: Biointerfaces, vol. 54, pp. 10–19, 2007.
[GOU 90] GOUSSARD Y., DEMOMENT G., IDIER J., “A new algorithm for iterative
deconvolution of sparse spike trains”, IEEE International Conference on Acoustic, Speech
and Signal Processing, Albuquerque, NM, pp. 1547–1550, April 1990.
[JAN 00] JANSHOFF A., NEITZERT M., OBERDRFER Y., et al., “Force spectroscopy
of molecular systems – single molecule spectroscopy of polymers and biomolecules”,
Angewandte Chemie International Edition, vol. 39, no. 18, pp. 3213–3237, September
2000.
[KAS 08] KASAS S., DIETLER G., “Probing nanomechanical properties from biomolecules
to living cells”, Pﬂugers Arch. – European Journal of Physiology special issue: “Atomic
Force Microscopy Enters Physiology”, vol. 456, no. 1, pp. 13–27, 2008.
[KOR 82] KORMYLO J.J., MENDEL J.M., “Maximum-likelihood detection and estimation
of Bernoulli-Gaussian processes”, IEEE Transactions on Information Theory, vol. 28,
pp. 482–488, 1982.

Analysis of Force-Volume Images in AFM Using Sparse Approximation
55
[KOW 09] KOWALSKI
M.,
“Sparse
regression
using
mixed
norms”,
Applied
and
Computational Harmonic Analysis, vol. 27, no. 3, pp. 303–324, September 2009.
[LIN 07] LIN D.C., DIMITRIADIS E.K., HORKAY F., “Robust strategies for automated AFM
force curve analysis – I. non-adhesive indentation of soft, inhomogeneous materials”,
Journal of Biomechanical Engineering, vol. 129, no. 3, pp. 430–440, June 2007.
[MIL 02] MILLER A.J., Subset Selection in Regression, 2nd ed., Chapman and Hall, London,
2002.
[NAT 95] NATARAJAN B.K., “Sparse approximate solutions to linear systems”, SIAM Journal
on Computing, vol. 24, no. 2, pp. 227–234, April 1995.
[NEE 09] NEEDELL D., TROPP J.A., “CoSaMP: iterative signal recovery from incomplete
and inaccurate samples”, Applied and Computational Harmonic Analysis, vol. 26, no. 3,
pp. 301–321, May 2009.
[PAT 93] PATI Y.C., REZAIIFAR R., KRISHNAPRASAD P.S., “Orthogonal matching pursuit:
recursive function approximation with applications to wavelet decomposition”, Proceedings
of 27th Asilomar Conference on Signals, Systems and Computers, vol.
1, pp. 40–44,
November 1993.
[POL 11] POLYAKOV P., SOUSSEN C., DUAN J., et al., , “Automated force volume image
processing for biological samples”, PLoS ONE, vol. 6, no. 4, pp. 1–19, April 2011.
[SEN 05] SEN S., SUBRAMANIAN S., DISCHER D.E., “Indentation and adhesive probing
of a cell membrane with AFM: theoretical model and experiments”, Biophysical Journal,
vol. 89, no. 5, pp. 3203–3213, November 2005.
[SMI 96] SMITH M.S., KOHN R., “Nonparametric regression using Bayesian variable
selection”, Journal of Econometrics, vol. 75, no. 2, pp. 317–343, December 1996.
[SOU 08] SOUSSEN C., BRIE D., GABORIAUD F., et al., “Modeling of force-volume images
in atomic force microscopy”, IEEE International Symposium on Biomedical Imaging, Paris,
May 2008.
[SOU 11] SOUSSEN C., IDIER J., BRIE D., et al., “From Bernoulli-Gaussian deconvolution
to sparse signal restoration”, IEEE Transactions on Signal Processing, vol. 59, no. 10,
pp. 4572–4584, October 2011.
[VEL 02] VELEGOL S.B., LOGAN B.E., “Contributions of bacterial surface polymers,
electrostatics, and cell elasticity to the shape of AFM force curves”, Langmuir, vol. 18,
no. 13, pp. 5256–5262, May 2002.
[VET 02] VETTERLI M., MARZILIANO P., BLU T., “Sampling signals with ﬁnite rate of
innovation”, IEEE Transactions on Signal Processing, vol. 50, no. 6, pp. 1417–1428, June
2002.
[WIE 94] WIESENDANGER R., Scanning Probe Microscopy and Spectroscopy: Methods and
Applications, Cambridge University Press, Cambridge, 1994.

56
Regularization and Bayesian Methods for Inverse Problems
[YAO 02] YAO X., WALTER J., BURKE S., et al., , “Atomic force microscopy and theoretical
considerations of surface properties and turgor pressures of bacteria”, Colloids and Surfaces
B: Biointerfaces, vol. 23, nos. 2–3, pp. 213–230, February 2002.
[ZIB 10] ZIBULEVSKY M., ELAD M., “ℓ2 −ℓ1 optimization in signal and image processing”,
IEEE Signal Processing Magazine, vol. 27, no. 3, pp. 76–88,May 2010.

3
Polarimetric Image Restoration by
Non-local Means
3.1. Introduction
Conventional optical imaging relies on the analysis of radiance values, possibly
coupled to wavelength analysis. This modality ignores the vectorial nature of light
waves and does not account for their polarization. However, it has been shown that
polarization gives valuable information about the interaction of the electromagnetic
wave with media. It carries essential information in the case where transparent or
semi-transparent objects are present in the scene, or even when the objective is to
access volumetric information hidden by light coming from the surface.
We deﬁne polarization imaging as the spatially distributed measurement of
polarization parameters. It is complementary to conventional imaging techniques for
a number of applications:
studies of biological tissues, non-destructive control,
clutter detection and three-dimensional (3D) shape reconstruction. A number of
imaging polarimeters have been built for a wide range of applications, ranging from
the biomedical ﬁeld [BUE 99, GIA 06, PIE 11] to remote sensing [GOL 99, GOL 02,
LIN 06, SHE 05, TYO 06], including metrology [MIY 02, MIY 04] and the studies
of propagation in the atmosphere [LEM 08].
Optical polarization imaging is performed by placing a variable polarization state
analyzer (PSA) in front of a camera. Each state of the PSA enables observing a
different polarized radiance image. Radiance (or channel) images acquired in such a
manner are related to the polarization parameters images through an observation
model speciﬁc to the PSA. Moreover,
the polarimetric image presents a
multidimensional structure where multicomponent information is attached to each
Chapter written by Sylvain FAISAN, François ROUSSEAU, Christian HEINRICH and Jihad
ZALLAT.

58
Regularization and Bayesian Methods for Inverse Problems
pixel. This multicomponent information is combined in the different channels,
corresponding to the different analysis states of the PSA, gathered by the camera.
Measurements are affected by systematic errors and noise. Systematic errors come
from the non-ideal character of the optical elements constituting the PSA as well as
from misalignment. A careful calibration procedure reduces the impact of these errors.
The noise comes from the sensor and from the observation conditions: the observation
is performed on narrow spectral bands and the narrowness of the spectral bandwidth
tends to degrade the signal-to-noise ratio.
Besides model imperfection and contamination of measurements by noise, there
are constraints on the set of admissible polarimetric images. Hence, inversion of
collected data requires much care and speciﬁc methods. We propose here an original
method, based on an “area”-oriented processing algorithm that enables restoring
polarimetric images while accounting for the bidimensional distribution of the
information
in
the
image.
This
method
complements
other
approaches
[SFI 11, VAL 09, ZAL 07, ZAL 08].
This chapter is organized in the following manner. Section 3.2 introduces light
polarization in the context of Stokes–Mueller formalism. The following section
focuses on the approaches that we propose for the estimation of Stokes vectors, ﬁrst
in the case of a single pixel and then in the case of an image, using an approach based
on non-local means. Results based on synthetic and real data are presented in
section 3.4.
3.2. Light polarization and the Stokes–Mueller formalism
The polarization state of any light wave is perfectly deﬁned by a real vector
(s0,s1,s2,s3)t that is known as a “Stokes vector”, where s0 represents the total
intensity of the light wave, s1 and s2 are related to the alignment of the axes of the
polarization ellipse with the horizontal axis and s3 is related to the oriented area of
the ellipse that is equal to ±πs3 and describes the circular part of the wave.
Any physically admissible Stokes vector must verify two constraints. The set of
admissible Stokes vectors is denoted as S. It is a subset of vectors of
4 that veriﬁes:
s0 ⩾0,
s2
0 −s2
1 −s2
2 −s2
3 ⩾0
[3.1]
The ﬁrst constraint accounts for the positivity of the intensity of a light wave, while
the second ensures that the intensity of the polarized part of the wave does not exceed
the total intensity.

Polarimetric Image Restoration by Non-local Means
59
The degree of polarization (DOP), the ellipticity ε and the orientation θ of the wave
are deﬁned by:
DOP =

s2
1 + s2
2 + s2
3
s0
,
ε = 1
2 sin−1

s3

s2
1 + s2
2 + s2
3

θ = arg
	s1 + is2
2

where i is such that i2 = −1.
The action of a linear optical system on a light wave characterized by its Stokes
vector s is described by the matrix equation s′ = Ms, where M is a real 4×4 matrix
known as Mueller’s matrix and where s′ is the Stokes vector of the resulting wave.
A Stokes imaging polarimeter measures the polarization state of the light
reﬂected and diffused by a scene. The measurement of the polarization of the light
from a scene relies essentially on the possibility of constructing a PSA that enables
accessing the Stokes vectors of the light coming from each pixel of the image. This is
achieved using birefringent elements such as rotating quarter-wave plates followed
by a ﬁxed linear polarizer (Figure 3.1). Elements with nematic liquid crystals are an
interesting alternative to rotating waveplates since they present the advantage to not
require any mechanical component and enable acquisition speed close to video rates.
Regardless of the modulation principle used, the measurement procedure remains
unchanged:
the PSA analyzes each outcoming Stokes vector by measuring its
projection onto K ⩾4 independent states. The complete set of K measurements
provides for each pixel a matrix equation that links the Stokes vector of this pixel to
the corresponding measured intensities. An effective calibration procedure is
required in order to extract the polarimetric images [ZAL 06].
Caméra
Objectif
F
P
Q
Objet
PS A
Camera
Lens
Object
Figure 3.1. Principle of a Stokes imaging polarimeter. F: interferential ﬁlter;
Q: mechanically or electrically controlled birefringent element; P: polarizer

60
Regularization and Bayesian Methods for Inverse Problems
The polarization state of the incident light on the charge-coupled device (CCD)
camera is described by the Stokes vector s′, which veriﬁes:
s′ = MPSAs
[3.2]
with:
MPSA =

i
Mi
where Mi are the Mueller matrices of the optical elements constituting the PSA and
s is the Stokes vector of the incident wave on the input diaphragm iris of the PSA.
Equation [3.2] can be explicitly written as a function of the elements mi,j of the
matrix MPSA such as:
⎛
⎜
⎜
⎝
s′
0
s′
1
s′
2
s′
3
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
m11 m12 m13 m14
m21 m22 m23 m24
m31 m32 m33 m34
m41 m42 m43 m44
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
s0
s1
s2
s3
⎞
⎟
⎟
⎠
Only the ﬁrst item s′
0 of the Stokes vector, namely the total intensity of the wave,
is accessible to a direct measurement. The polarization state s can be estimated by
performing K(K ⩾4) intensity measurements, denoted as o1,o2, . . . , ,oK, where
oi is the ith intensity measured at a pixel. Each of these intensities corresponds to a
different state of the PSA modulator (PSAk with k = 1,2, · · · ,K ). The relationship
between the measured intensities and the Stokes vector of the incident wave writes:
⎛
⎜
⎜
⎜
⎝
o1
o2
...
oK
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
m11,1 m12,1 m13,1 m14,1
m11,2 m12,2 m13,2 m14,2
...
...
...
...
m11,K m12,K m13,K m14,K
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎝
s0
s1
s2
s3
⎞
⎟
⎟
⎠(+ noise)
or also:
o = P s (+ noise)
where P is a K × 4 matrix. This polarimetric measurement matrix is a function of
the polarimetric properties of the optical elements that constitute the measurement
system. This matrix is known and corresponds to an effective modulation of the ﬂow
of photons. Measurements are achieved such that P is well-conditioned. Our goal is
to solve this inverse problem while accounting for physical admissibility constraints
and reducing the noise impact.

Polarimetric Image Restoration by Non-local Means
61
3.3. Estimation of the Stokes vectors
We begin by presenting in section 3.3.1 a new approach that enables us to
estimate a Stokes vector in a pixel from the observations that are associated with it.
In order to reduce the effects of noise, we propose thereafter to take into account
spatial information with a non-local means ﬁltering approach. The principle of this
ﬁltering is explained in section 3.3.2 and improvements are proposed in section 3.3.3.
These two sections consider the case of an image with a single channel. They are
therefore independent of the estimation of the Stokes vectors. They are a prerequisite
to section 3.3.4 where ﬁltering by non-local means is extended to estimate and to
ﬁlter the Stokes vectors. In particular, we will show that this is equivalent to
optimizing a quadratic criterion and that this optimization can be performed through
a two-stage procedure: a ﬁltering stage and an estimation stage.
3.3.1. Estimation of the Stokes vector in a pixel
3.3.1.1. Problem formulation
The Stokes vector is estimated by minimizing the quadratic error between the
measurements o observed in a pixel (it is assumed that o is of size K, K ⩾4) and
the radiances P s arising from the model:
!v = arg min
s
∥o −P s∥2
[3.3]
The measurement procedure is determined in a way such that matrix P is well-
conditioned. The criterion of equation [3.3] is therefore strictly convex and it has a
unique global minimizer:
!v = P †o
[3.4]
P † being the pseudoinverse of the matrix P , which is called “reduction matrix”. The
solution to [3.4] is denoted as !v and not !s to stress the fact that !v is not necessarily
a Stokes vector, since the physical constraint of equation [3.1] may not be satisﬁed.
To solve this problem, the following approach is generally used: the Stokes vector !s
is chosen equal to !v if !v satisﬁes the constraints of physical admissibility. Otherwise,
!s is the orthogonal projection of !v on the set of Stokes vectors. It is, however, more
appropriate to estimate !s by minimizing the quadratic error while constraining the
solution to be a Stokes vector. This writes:
!s = arg min
s∈S
∥o −P s∥2
[3.5]
where S is the set of admissible Stokes vectors. This set is convex and the
optimization problem is therefore also convex. As a result, any local optimum is the

62
Regularization and Bayesian Methods for Inverse Problems
global optimum. We propose thereafter an effective manner to solve this constrained
optimization problem .
3.3.1.2. Properties of the constrained optimization problem
Two properties of the set S of Stokes vectors will be exploited:
– S is a convex set of
4:
∀(s1,s2) ∈S × S,∀t ∈[0,1],
(1 −t)s1 + ts2 ∈S
– a Stokes vector s belongs to the boundary of S, denoted as ∂S, if and only if its
DOP is equal to 1, that is if and only if s2
0 = s2
1 + s2
2 + s2
3. We thus have:
∂S =
"
x2 + y2 + z2,x,y,z
#t
,x,y,z ∈
3

The ﬁrst property follows from the stability of the set of Stokes vectors by addition.
On the other hand, (1 −t)s1 and ts2 are Stokes vectors for any t in [0,1]. As a result,
S is a convex set of
4.
To justify the stability of S by addition, let us consider two Stokes vectors s and
s′. Their sum belongs to S if s0 + s′
0 ⩾0 and if the relationship (s0 + s′
0)2 ⩾
3
i=1(si + s′
i)2 holds. Considering the following inequalities:
s0 + s′
0 ⩾
$
%
%
&
3

i=1
s2
i +
$
%
%
&
3

i=1
s′
i
2
(since s and s′ ∈S)
$
%
%
&
3

i=1
s2
i +
$
%
%
&
3

i=1
s′
i
2 ⩾
$
%
%
&
3

i=1
(si + s′
i)2
(triangular inequality)
We have that s + s′ is a Stokes vector and S is stable by addition.
If !v ∈S, the unconstrained criterion [3.3] and the constrained criterion [3.5]
have the same minimizer. Otherwise, the criterion [3.5] has a single minimizer !s that
belongs to ∂S and !s is the unique point of ∂S verifying:
– the gradient of the criterion at the point !s is orthogonal to the boundary ∂S;
– the gradient at the point !s gives the direction of the interior of S.
These properties result from the convexity of the problem [3.5].
It can be noted that the two points mentioned earlier correspond to the
Kuhn–Tucker conditions. There is a single point that veriﬁes these conditions and it

Polarimetric Image Restoration by Non-local Means
63
is the global minimum. This means that the criterion and the inequality constraints
are invex of type I [HAN 99] and that a method of constrained optimization such as
sequential quadratic programming is guaranteed to converge to the global minimum.
However, in practice, we have observed that such a method converges relatively
slowly, leading either to a very signiﬁcant calculation time, or to a coarse
approximation of the minimizer (we have used the implementations of the
“optimization” toolbox of Matlab) that depends on the stopping conditions. To solve
this problem,
we propose here an algorithm that takes advantage of the
parameterization of ∂S. Rather than using a constrained optimization method, it is
then possible to directly optimize over ∂S.
3.3.1.3. Optimization algorithm
If !v ∈S (equation [3.4]), then !s = !v. Otherwise, the solution !s is to be searched
on the boundary ∂S. But the criterion ∥o −P s∥2 restricted on ∂S can admit local
minima. However, the general problem 3.5 is convex and the solution is obtained by
local descent.
We will consider an optimization algorithm comprising two modes (see
Algorithm 3.1):
– local descent on the boundary ∂S:
min
s1,s2,s3
o −P
	'
s2
1 + s2
2 + s2
3,s1,s2,s3

t
2
[3.6]
– local descent on the interior ˘S of S:
min
s∈˘S
∥o −P s∥2 with:˘S =

s,s0 >
'
s2
1 + s2
2 + s2
3

If !v /∈S, the descent is initialized over ∂S, in mode 1. When a local optimum is
found, the gradient orientation of the quadratic criterion ∥o −P s∥2 makes it
possible to determine whether this optimum is global (the gradient points inside S),
and the solution is then obtained, or local (the gradient points outside S). If the
optimum is local, the descent continues in mode 2. In this case, it is possible to
decrease the criterion by entering inside S. The boundary is met once again and the
descent continues in mode 1. Step 2 enables, if necessary, to reach a better attraction
basin for the criterion [3.6].
In practice, we have observed that the solution obtained after the ﬁrst mode 1
optimization always corresponds to the global minimum under the condition that the
optimization algorithm of mode 1 is initialized with the orthogonal projection of !v on

64
Regularization and Bayesian Methods for Inverse Problems
S (see Algorithm 3.1). However, by choosing a random initialization over ∂S, two
iterations are sometimes needed for the algorithm to converge.
Algorithm 3.1: Estimation of a Stokes vector in a pixel from K observations o
Input: o, P
Output: !s
Compute !v (see equation [3.4]);
if !v satisﬁes the admissibility constraint then
!s = !v
else
Deﬁne s as being equal to the orthogonal projection of !v on S;
Set Solution = 0;
while (Solution == 0) do
Determine s as the result of an optimization in mode 1 initialized in s;
if the gradient of the criterion of equation [3.3] at point s gives the
outward direction of S then
Determine s as the result of an optimization in mode 2 initialized in
s;
else
Set Solution = 1;
!s = s;
Let us ﬁnally note that the proposed approach is original and much simpler than
sequential quadratic programming techniques. As we have already mentioned, it has
also proved to be more effective. This approach is convenient since:
– the criterion to optimize is strictly convex, and S is a convex set of
4;
– the boundary ∂S can be easily parameterized.
The ﬁrst point helps to show that the constrained criterion has a single solution.
Either the analytical solution of the non-constrained criterion satisﬁes the constraints,
or the solution of the constrained criterion is to be searched for on the boundary of
the domain. In this case, it is possible to optimize directly on this boundary (without
being concerned with the constraints) using the second point.
3.3.2. Non-local means ﬁltering
Taking into account the spatial information to estimate the Stokes vectors makes
it possible to reduce the effects of the noise present in the data. In this context, we
propose to extend non-local means ﬁltering in order to estimate and to ﬁlter the Stokes
vectors. This section describes the principle of non-local means ﬁltering with the case
of an image having a single channel.

Polarimetric Image Restoration by Non-local Means
65
Buades et al. [BUA 05] have proposed a non-supervised denoising algorithm, non-
local means ﬁltering, whose principle consists of taking into account the repetition of
the textures or the patterns present in the images. They have shown that this approach
was performing better for natural scene images than state-of-the-art methods, such as
the minimization of the total variation or wavelet-based approaches. This strategy has
been studied and used in numerous applications such as non-local regularization in
the context of inverse problems [GIL 08, KIN 05, MIG 08, ROU 10]. The interested
reader could refer to [KAT 10] for a detailed review on local and non-local methods
of denoising/ﬁltering.
Let us consider the following additive model:
I(x) = f(x) + n(x)
where f is the noiseless image, n is a zero-mean and σ2 variance noise, I is the
observed image and x represents a pixel of the image. The support of the image is
denoted as Ω. The objective of a denoising algorithm is to estimate the image f from
the observed image I.
A weighted graph w is considered, which associates a weight w(x,y) with two
pixels x and y of the observed image I. This weighted graph w is a representation of
the non-local similarities of image I.
In [BUA 05], this graph w is used for denoising using the following averaging
strategy:
∀x ∈Ω, Inlm(x) =

y∈Ω w(x,y)I(y)

y∈Ω w(x,y)
[3.7]
where Inlm is the denoised version of I.
The estimation of the weights is carried out from patches. By using the notations
of [DEL 12], a patch Px of size (2k + 1) centered around the pixel x is deﬁned by:
Px =

I(x + τ),
τ ∈[−k,k]2
The graph w can then be calculated by using a distance between the intensities of
the patches [COU 08]:
w(x,y) = φ
	dP (x,y)2
2Nβ!σ2

where dP (x,y) is the distance between the patches Px and Py (in our case, we use
the Euclidean norm ∥·∥2); N is the number of pixels of the patch (N = (2k + 1)2);

66
Regularization and Bayesian Methods for Inverse Problems
φ is the kernel measuring the similarity between patches (Buades et al. [BUA 05]
have considered the case φ(x) = e−x), β is a regularization parameter and !σ is the
estimated standard deviation of the noise. Under the assumption of Gaussian noise, β
can be set to 1 (see [BUA 05] for justiﬁcations).
Finally, the standard deviation of the noise is estimated according to the residual
variance method [GAS 86]. For every pixel x of I, the pseudoresidual is deﬁned as:
εx =
(
4
5

I(x) −1
4

y∈N (x)
I(y)

where N(x) is the 4-neighbor set of the pixel x. The standard deviation of the noise
is estimated by using the robust estimator:
!σ = 1,4826 med
i
	εi −med
j (εj)


It can be mentioned that !σ is a parameter that reﬂects the compromise between
the reduction of the noise and the degradation of the information contained in the
image. As a matter of fact, this parameter has a signiﬁcant inﬂuence on the calculation
of the weight. Schematically, the method will not remove any pattern with intensity
variation greater than the estimated noise level. In order to reduce the computation
time, the search window of the pixel x is not usually deﬁned as the support of the
image as in [BUA 05], but rather as a smaller window centered in x, denoted as Ωx.
Thus, the sum of the denominator (or of the numerator) of equation [3.7] will include
Card(Ωx) terms, instead of Card(Ω) terms.
3.3.3. Adaptive non-local means ﬁltering
An important point of the methods of non-local means ﬁltering concerns the
deﬁnition of the weighted graph. With the objective of providing an algorithm that
best adapts to the contents of the image to denoise, we propose here three
contributions for the estimation of the graph, which are in relation with the function
φ, and with the automatic estimation of the size and the shape of the patch.
3.3.3.1. The function φ
In the original version of non-local means ﬁltering [BUA 05], the kernel is
deﬁned as φ(x) = e−x. Nevertheless, it has been shown that the use of the ﬁnite
support kernel provides more satisfactory results [GOO 08]. In particular, because of
the exponential form of the kernel used in [BUA 05], even very different patches
have positive weights. Although these weights are low, denoising cannot be
satisfactory due to all these small contributions.

Polarimetric Image Restoration by Non-local Means
67
In this work, the conventional kernel φ(x) = e−x has been used, but we propose to
eliminate the contributions of the aberrant patches by using a statistical approach. Let
us recall that if X1, . . . , XN are random independent reduced and centered Gaussian
variables, then the sum N
i=1 X2
i of their squares follows a chi-square distribution
with N degrees of freedom. Thus, under the hypothesis that the two patches Px and
Py are associated with the same uniform area and that the noise is a white Gaussian
noise with a variance σ2, we get:
Nβ dP (x,y)2
2Nβσ2
∼χ2(N)
[3.8]
We use this property to deﬁne a threshold such that the weights that are too small
will be considered as aberrant. Being given a p-value, a threshold t can be estimated
such that the probability that the random variable [3.8] exceeds t is equal to p. As
φ is a decreasing function in
+, then P(w(x,y) < φ(t/Nβ)) = p. The weights
w(x,y) lower than t0 = φ(t/Nβ) are then considered as aberrant values and set to
0. Thereafter, the p-value is set to 0.05. Such a thresholding procedure thus does not
consider, during the denoising process, the pairs of patches that are dissimilar.
3.3.3.2. Patches size and shape
Buades et al. proposed in the original version of their algorithm using square
patches of dimension (2k + 1) × (2k + 1) as well as a Gaussian kernel, so as to
reduce the inﬂuence of peripheral pixels. The parameter k is generally chosen equal
to 1, 2 or 3, leading thus to a patch size equal to 3 × 3, 5 × 5 or 7 × 7. The half-width
k is generally considered as a global parameter. This can result in a rather ineffective
procedure since the principle of redundant information which the ﬁltering approach
is based on can be dependent on the scale. Equally, the estimation of the appropriate
size of the patches is in direct relation to the estimation of their form. While the
square shape yields simple algorithms, the ﬁltering process can be inefﬁcient
alongside the edges. In order to solve this problem, one possibility is to consider
anisotropic supports (see [KAT 10], for example).
In this work, we propose ﬁrst of all an adaptive approach that modiﬁes the form
of the patches during the estimation of the weights w(x,y). Instead of using a square
patch [BUA 05], the shape of the patches is determined locally in a non-parametric
manner for every pair (x,y) of pixels. As shown in Figure 3.2, the support S of the
patch is decomposed in two disjoint parts, denoted as S1 and S2, such that
S = S1
) S2. S1 corresponds to the interior of S and S2 to its boundary. On the one
hand, all the pixels of S1 are used in the estimation of w(x,y) such as to obtain a
coherent local descriptor. On the other hand, the pixels of S2 can potentially be
considered as aberrant data and may not be used to calculate w(x,y). In order to
obtain a simple algorithm, a quarter of the pixels of S2 (corresponding to the most
dissimilar pixels of the two patches Px and Py) are not used for the estimation of

68
Regularization and Bayesian Methods for Inverse Problems
w(x,y). This approach thus enables constructing different forms of patches in a
simple, effective data-driven manner (remember, the objective of the preceding
section was not to consider aberrant patches, namely, to set the weight w(x,y) to
zero for a number of pixel pairs). In contrast, the objective is here not to take into
account certain pixels in the estimation of w(x,y).
Figure 3.2. Example of three patches of size 3 (left), 5 (center)
and 7 (right); S1 is in white and S2 in gray
In summary, the distance between patches can be expressed as:
dP (x,y)2 = arg min
S

τ∈[−k,k]2
S(x,y)(τ)(I(x + τ) −I(y + τ))2
[3.9]
where S(x,y)(τ), τ ∈[−k,k]2, deﬁnes the form of the patches Px and Py. It can be
mentioned that S(x,y)(τ) = 1 for all the pixels except for a quarter of the pixels of
S2 for which S(x,y)(τ) = 0. The computation of dP (x,y)2 ( [3.9]) is straightforward
using standard sorting algorithms.
Finally, it is known that the use of large patches results in a better performing
denoising process in the uniform areas, while small patches are more adapted to the
contrasted areas. We propose here a sequential process that enables automatically
deﬁning the size of the patch. The principle is to use the largest possible patch. We
initialize the algorithm with a patch of size (2kmax + 1) × (2kmax + 1) and estimate
the associated weights {w(x,y)}y∈Ωx. If an insufﬁcient number of similar patches
has been found in the search area Ωx, the size of the patch is reduced (it is expected
to observe more small similar patches than large similar patches) and the set of
weights {w(x,y)}y∈Ωx is recalculated. This procedure is repeated until a sufﬁcient
number of examples is found, or until the minimal size of the patch is reached.
In
practice,
the
minimal
number
of
similar
patches
(i.e.
Card{Py, y ∈Ωx | w(x,y) > 0}) is set to N0. Since the optimal size of the patches
is estimated for every pixel starting from a maximal value 2kmax + 1, kmax can
therefore arbitrarily be chosen with a large value. Nevertheless, kmax must be

Polarimetric Image Restoration by Non-local Means
69
relatively small to keep the computation time reasonable. In the case of conventional
images, a patch size of 9 × 9 (kmax = 4) is generally sufﬁcient to estimate the
similarity between two pixels. The determination of the weights is ﬁnally described
in Algorithm 3.2.
Algorithm 3.2: Weights determination at pixel x
Input: image I, maximum size of the patch kmax, search area centered around
the point x: Ωx
Output: weights w(x,.)
for k = kmax to 1 do
Threshold estimation t0 such that P(w(·,·) < t0) = 0.05;
for every y ∈Ωx do
Compute the weights w(x,y) according to equation [3.9];
Set w(x,y) equal to 0 if w(x,y) < t0;
Set w(x,x) to the maximal value of {w(x,y)}y∈Ωx;
if there are at least N0 values of w(x,·) strictly positive then
exit the loop
Finally, it can be noted that the value of N0 (the minimum number of similar
patches to ﬁnd) can have a direct impact on the results. If there are less than N0
similar patches, the size of the patch is reduced. Indeed, less than N0 observations
are not considered as sufﬁcient to efﬁciently denoise the images. For example, in a
uniform region with white noise and under the hypothesis that all the weights are
identical, the standard deviation of noise will be reduced by a factor 1/
√
N, if N
patches are considered. A small value of N0 can thus result in low-quality denoising
(there is a risk in considering very big patches, even if the number of examples is low).
Conversely, a large value of N0 will lead to favor patches with a small size. In practice,
the choice of N0 can vary according to the properties of the image under analysis. In
this work, N0 has been set to 40.
3.3.4. Application to the estimation of Stokes vectors
We extend here non-local means ﬁltering to the case of the estimation-ﬁltering of
Stokes vectors. Therefore, it is necessary to consider non-local means ﬁltering as a
weighted least-squares problem: under the hypothesis that the weights w(x,.) have
been normalized such that they sum up to 1, it can thus be written:
∀x ∈Ω,
Inlm(x) =

y∈Ωx
w(x,y)I(y) = arg min
a

y∈Ωx
w(x,y)(I(y) −a)2

70
Regularization and Bayesian Methods for Inverse Problems
In the context of the estimation of Stokes vectors, we have K (K ⩾4) intensity
images. Image I can therefore be considered as an image with K components:
I = [I1,I2,I3, . . . , IK]t
Thus, I(x) represents the K measurements associated with the pixel x such that
I(x) corresponds to the observation vector o of section 3.2 and section 3.3.1.
Denoising I can be carried out independently for every channel as follows:
∀x ∈Ω,
Inlm(x) =

y∈Ωx
Dx(y)I(y)
= arg min
a

y∈Ωx
(I(y) −a)tDx(y)(I(y) −a)
[3.10]
where Dx(y) is a diagonal matrix of size K ×K where the element dii (i = 1, . . . K)
is the normalized weight between the pixels x and y for the ith channel. Consequently,

y∈Ωx Dx(y) is the identity matrix.
Nevertheless, our objective is not to denoise the intensity images, but to estimate
the image of Stokes vectors !S. The output of the model is therefore not the vector a,
but the magnitude P s, with a parameter s. Equation [3.10] is then written as:
∀x ∈Ω, !S(x) = arg min
s

y∈Ωx
(I(y) −P s)tDx(y)(I(y) −P s)
[3.11]
Three comments can be made about equation [3.11]. First of all, it can be noted
that the estimation of the Stokes vector !S(x) at pixel x is not only carried out
considering the observations in this pixel but also the observations of the neighboring
pixels. Matrix Dx(y) determines the weight of the observation vector I(y) at pixel x.
The criterion of [3.11] enables therefore ﬁltering and estimating the Stokes vectors.
Next, if only the data associated with the pixel x are considered for the estimation
of the Stokes vector at this pixel (no spatial ﬁltering), then the matrix Dx(y) is the
null matrix if y ̸= x and the identity matrix otherwise. The criterion of equation [3.11]
is then equivalent to the criterion of equation [3.3].
The last comment is related to the estimation of Dx(y). Since dii is the
normalized weight between the pixel x and the pixel y for the ith channel, we use the
approach of section 3.3.3 to estimate dii. Nevertheless, let us note that the method is
versatile and other approaches could be used to estimate the weights. Thus, the diis
can be determined such that the ﬁltering procedure corresponds to median or
Gaussian ﬁltering. It is also possible to ensure that the weights between two pixels do
not depend on the channel by constraining the matrix Dx(y) to be proportional to
the identity matrix.

Polarimetric Image Restoration by Non-local Means
71
Let us ﬁnally note that the criterion of equation [3.11] can be more simply
expressed and in a more compact manner. Since the matrix Dx(y) is diagonal and
symmetric, the gradient of the criterion writes:
∇= −2P t 
y∈Ωx
Dx(y) (I(y) −P s)
= −2P t
 
y∈Ωx
Dx(y)I(y) −

y∈Ωx
Dx(y)P s

= −2P t
 
y∈Ωx
Dx(y)I(y) −P s

because 
y∈Ωx Dx(y) is the identity matrix. Finally, by deﬁnition of Inlm(x) (see
equation [3.10]), the gradient of the criterion can be written as:
∇= −2P t (Inlm(x) −P s)
It can be deducted that the problem [3.11] is equivalent to:
∀x ∈Ω,
!S(x) = arg min
s
∥Inlm(x) −P s∥2
[3.12]
This arises from the equality between the gradient of [3.12] and the gradient
of [3.11]. The two criteria are thereby equal up to an additive constant.
Criterion of [3.12] is interesting because it corresponds to denoising every
channel separately by using non-local means ﬁltering and then to estimating the
Stokes vector at every pixel in a separated manner. In practice, we optimize the
criterion of equation [3.12] while constraining the solution to be a Stokes vector by
using the approach seen in section 3.3.1:
∀x ∈Ω,
!S(x) = arg min
s∈S
∥Inlm(x) −P s∥2
[3.13]
As the criteria of equations [3.11] and [3.13] are equal up to an additive constant,
their minimization is equivalent under the physical admissibility constraint. Thus, we
can solve [3.11] under the physical admissibility constraint in two steps: a non-local
means ﬁltering step and an estimation [3.13] step.
If !S represents the image of the estimated Stokes vectors, the image !I deﬁned by
!I(x) = P !S(x) for every pixel x can be considered as a denoised version of I. For
K = 4, !I and Inlm differ only at pixels where the solution obtained by pseudoinverse
does not verify the physical admissibility constraint.

72
Regularization and Bayesian Methods for Inverse Problems
3.4. Results
3.4.1. Results with synthetic data
3.4.1.1. Synthetic data and context evaluation presentation
We synthesized a Stokes image of size 256 × 256. This image is composed of a
uniform background with s =

1,1/
√
3,1/
√
3,1/
√
3
t and a disc with a radius of
100
pixels
upon
which
the
Stokes
vectors
vary.
They
are
set
to
[1, cos 2φ cos 2χ, sin 2φ cos 2χ, sin 2χ]t, where 2φ (respectively, 2χ) is constant for
every column (respectively, every line) of the image and varies alongside the
horizontal diameter from 0.2 to 18 degrees (respectively, along the vertical diameter
from 4.5 to 40.5 degrees). We point out that all Stokes vectors have a DOP equal to 1.
This will highlight the relevance of the proposed approach when the pseudoinverse
solution does not verify the physical admissibility conditions. Such a phenomenon is
likely to be met in the presence of noise (it is likely to observe such a phenomenon
when the noise will be added to the images). Let Sgt =

Sgt
1 ,Sgt
2 ,Sgt
3 ,Sgt
4
t, the image
of the Stokes vectors generated (“gt” stands for ground truth). The associated intensity
images Igt =

Igt
1 ,Igt
2 ,Igt
3 ,Igt
4
t are ﬁnally deﬁned as follows at each pixel:

Igt
1 (x),Igt
2 (x),Igt
3 (x),Igt
4 (x)
t = P

Sgt
1 (x),Sgt
2 (x),Sgt
3 (x),Sgt
4 (x)
t
with:
P =
⎛
⎜
⎜
⎝
1.0000 −0.0535
0.2251
0.9729
1.0000 −0.7464 −0.4351
0.5036
1.0000 −0.7464
0.4351 −0.5036
1.0000 −0.0535 −0.2251 −0.9729
⎞
⎟
⎟
⎠
[3.14]
Finally, Gaussian white noise with a variance σ2 is added to the four images Igt
i
(i = 1, . . . 4). The images thus obtained are represented as I = [I1,I2,I3,I4]t.
Since the ground truth is known, the precision of the estimation can be evaluated
by comparing the estimated images (images of Stokes vectors !S or intensity images
!I) with the original images. A ﬁrst assessment criterion is obtained by comparing the
original image Igt with its estimation !I through the peak signal-to-noise ratio (PSNR):
PSNR(Igt,!I) = 10 log10
⎛
⎜
⎝
d2
1
4 NP
4
j=1 α2
j

x
"
Igt
j (x) −!Ij(x)
#2
⎞
⎟
⎠
[3.15]

Polarimetric Image Restoration by Non-local Means
73
where αj is estimated such that the dynamics of αjIgt
j is d (e.g. 255) and where NP is
the number of pixels. In addition, a second assessment criterion, based on the Stokes
vectors, is used as:
e(Sgt, !S) = 100
$
%
%
& 1
NP

x
∥Sgt(x) −!S(x)∥2
∥Sgt(x)∥2
[3.16]
Four methods, M1, M2, M3 and MP , are evaluated:
– in M1, !S(x) is estimated by pseudoinverse (which gives !I = I);
– in M2, !S(x) is estimated by pseudoinverse, but the solution is projected
orthogonally onto the set of Stokes vectors if the admissibility constraints are not
veriﬁed;
– in M3, !S(x) is estimated using the approach described in section 3.3.1, namely
by minimizing the energy of the error, while constraining the solution to be a Stokes
vector;
– MP (see equation [3.13]) is the approach proposed with a search window size
equal to 11 × 11 and a maximal patch size set to 9 × 9.
It can be noted that methods M1, M2 and M3 do not use any spatial ﬁltering.
3.4.1.2. Results
Figure 3.1 indicates the values of PSNR (equation [3.15]), as well as the
quantiﬁcation of the Stokes vector error of the Stokes vectors (equation [3.16]),
which have been obtained with methods M1, M2, M3 and MP , for different values
of σ2.
The results of Table 3.1 clearly show the relevance of accounting for the spatial
information to estimate Stokes vectors. The values of the PSNR (Table 3.1(a)) are
obtained with the proposed approach (MP ) with an improvement of about 15–20 dB
in comparison with the other approaches and the quantiﬁcation of the Stokes vector
error regarding the Stokes vectors is decreased by a factor of 8–10. In addition, among
the methods that do not account for spatial information, M3 is the one that yields
the most satisfactory results. It indicates that the conventional method based on an
orthogonal projection onto the set Stokes vectors (method M2) is not a successful
approach. It is preferable to decrease as much as possible the error between the data
and their prediction, while constraining the solution to be physically admissible.
Finally, Figure 3.3 summarizes the overall performance of the proposed approach.
The Stokes vectors of the original image Sgt (Figure 3.3(a)), those estimated from I
(σ2 = 0,01) with method M1 (Figure 3.3(b)) and with the proposed approach MP

74
Regularization and Bayesian Methods for Inverse Problems
(Figure 3.3(c)) are represented using the Poincaré sphere (a Stokes vector s is
represented in
3 by the point with coordinates (s1/s0, s2/s0, s3/s0) such that this
point belongs to the unit sphere known as the Poincaré sphere). The two regions of
Sgt can easily be observed on the left part of Figure 3.3: the ﬁrst is a homogeneous
region (which is associated with a single Stokes vector), whereas the second is not
homogeneous (the Stokes vectors vary inside the region). The results obtained with
method M1 provide results that are not physically admissible (they do not belong to
the sphere) and it is difﬁcult to distinguish between the two regions. On the contrary,
the results with the proposed approach enable ﬁltering the noise, while constraining
the solution to be physically admissible. The two regions can then be easily observed.
σ2
M1
M2
M3
MP
0.001 24.09 24.17 24.81 43.49
0.002 21.08 21.16 21.86 40.33
0.005 17.12 17.21 17.98 35.85
0.010 14.08 14.15 15.02 32.31
0.020 11.07 11.13 12.15 29.06
0.050
7.09
7.14
8.46 25.24
σ2
M1
M2
M3
MP
0.001
5.09
4.94
4.67 0.54
0.002
7.20
6.98
6.55 0.79
0.005 11.34 10.95 10.22 1.31
0.010 16.09 15.47 14.32 1.95
0.020 22.76 21.73 19.86 2.86
0.050 36.02 33.86 30.06 4.35
a)
b)
Table 3.1. Values of the PSNR a) and the quantiﬁcation of the Stokes vector error
b) obtained with four methods and for different values of σ2. Methods M1, M2
and M3 do not use any spatial ﬁltering. MP is the proposed approach. The values
in bold correspond to the best results
Figure 3.3. Poincaré sphere upon which the original Stokes vectors Sgt are represented a)
those estimated from I (σ2 = 0.01) with method M1 b) and with the proposed approach MP
c). A Stokes vector s is represented in
3 by the point with coordinates (s1/s0, s2/s0, s3/s0).
The left-right and the top-down axes are, respectively, associated with s2/s0 and s3/s0, the
last axis being associated with s1/s0. Thus, the Stokes vector associated with a right-handed
circular polarization, s = (1,0,0,1)t, is represented by the point with coordinates (0,0,1),
namely the North Pole of the unity sphere (see the point right-hand circular polarization (RCP)
of the ﬁgure)

Polarimetric Image Restoration by Non-local Means
75
3.4.1.3. Signiﬁcance of the proposed method for the estimation of the weights
A contribution of this work is to propose an adaptive algorithm to determine the
weights, which are then used during the denoising process (see section 3.3.3). Let us
recall that at each pixel x, the size and shape of the patch are estimated. In addition, the
weights that are too low are set to 0 with the aim not to consider the too dissimilar pairs
of patches. We propose here to compare the proposed approach with the traditional
method [BUA 05]. We thus propose to deﬁne four approaches (T1 to T4) that are
simpliﬁed versions of the proposed method, such that the weights are estimated in
[BUA 05] (the size of the patch is a square of ﬁxed size and the contribution of aberrant
patches is not suppressed). In all the cases, the size of the search window has been set
to 11 × 11, while the size of the patch has been set to 3 × 3 for T1, to 5 × 5 for T2, to
7 × 7 for T3 and to 9 × 9 for T4. The results are given in Table 3.2.
σ2
T1
T2
T3
T4
MP
0.001 40.24 40.68
38.98 37.41
43.49
0.002 37.23 37.57
36.17 35.06
40.33
0.005 33.00
33.24 32.26 31.44
35.85
0.010 29.85 30.41
29.62 28.89
32.31
0.020 26.92 27.80
27.25 26.66
29.06
0.050 23.18 24.54
24.39 24.12
25.24
σ2
T1
T2
T3
T4
MP
0.001 0.79 0.76 0.91 1.08
0.54
0.002 1.13 1.08 1.24 1.40
0.79
0.005 1.82 1.74 1.93 2.10
1.31
0.010 2.59 2.41 2.61 2.83
1.95
0.020 3.64 3.25 3.43 3.65
2.86
0.050 5.57 4.70 4.75 4.88
4.35
a)
b)
Table 3.2. Values of PSNR a) and quantiﬁcation of the Stokes vector error b) obtained with
the four simpliﬁed methods (T1, T2, T3 and T4) of the proposed approach (MP ) for different
values of σ2. The values in bold correspond to the best results among {T1,T2,T3,T4}, without
considering the proposed approach MP that always provides the best results
First of all, we may note that the proposed approach gives better results than
methods T1, T2, T3 and T4, and this for all experiments carried out. This highlights
the performance of the method proposed for the determination of weights. Finally,
the results of Table 3.2 show that the size of the patch can have an important impact
on the results. In particular, neither T1 nor T4 enables obtaining the best results
(among T1, T2, T3 and T4). This emphasizes the beneﬁt of the adaptive strategy that
has been presented.
3.4.2. Results with real data
We have tested our algorithm on a real image composed of a ﬁgurine, a cube
and a rubber ball. The setup has been mounted on a diffusing metal support. The
scene thus composed has been illuminated by two bright white light sources (white
light-emitting diodes (LEDs)). In order to increase the polarized part of the light, an
elliptical polarizer has been placed in front of one of the two lamps. The setup has
been imaged with a Stokes polarimeter at the wavelength of 460 nm. We have used a
16-bit digital camera with an interferential ﬁlter of 10 nm width.

76
Regularization and Bayesian Methods for Inverse Problems
Figure 3.4. DOP (ﬁrst row), ellipticity (second row) and orientation (third row) of the Stokes
vector estimated with the proposed method (left) and by pseudoinverse (right)

Polarimetric Image Restoration by Non-local Means
77
Figure 3.5. DOP of the Stokes vector estimated with the proposed method
a) and by pseudoinverse b). A close-up of the character’s foot clearly enables
highlighting the denoising
The matrix P is equal to:
P =
⎛
⎜
⎜
⎝
1,0000
0,6792 −0,2235
0,6991
1.0000 −0.7075
0.6437
0.2918
1.0000 −0.4085 −0.8130 −0.4150
1.0000
0.5244
0.5303 −0.6662
⎞
⎟
⎟
⎠
[3.17]
Figure 3.4 represents the DOP (ﬁrst row), the ellipticity (second row) and the
orientation (third row) of the Stokes vector estimated with the proposed method (left)
and by inversion using the pseudoinverse (right). The pertinence of our approach
appears clearly by examining more closely the local variations of the polarimetric
signature. For example, Figure 3.5 shows a close-up view of the DOP map on the foot
of the ﬁgurine. When observing the images of the other characteristics (ellipticity and
orientation), it also clearly appears that our approach outperforms by far the naive
inversion method. The characteristics obtained with our approach are far less noisy.
Finally, the numerical values correspond to the physical admissibility conditions.
3.5. Conclusion
Polarization imaging carries information missing in conventional optical intensity
imaging. It is therefore interesting for a large number of applications. However, the
exploitation of polarimetric data requires particular care, for two reasons. On the one
hand, some vectors may not meet the physical admissibility criteria. On the other
hand, the measurements are carried out on very narrow spectral bands, which makes
the obtained intensity images relatively noisy.
The conventional pseudoinverse approaches do not account for physical
admissibility constraints. We have proposed to estimate physically admissible Stokes
vectors, in the context of a convex optimization problem. In order to reduce the
inﬂuence of noise degrading the measurements, we have shown how it is possible to

78
Regularization and Bayesian Methods for Inverse Problems
extend the methods by non-local means to estimate the Stokes vectors. In this
context, we have shown that estimating Stokes vectors by using a non-local means
ﬁltering approach is equivalent to ﬁrst denoise each intensity image with this type of
ﬁltering and second to estimate, pixel by pixel, a Stokes vector from the denoised
images. Finally, we have observed that the method obtained was much more efﬁcient
than the conventional methods.
3.6. Bibliography
[BUA 05] BUADES A., COLL B., MOREL J., “A review of image denoising algorithms, with
a new one”, Multiscale Modeling & Simulation, vol. 4, no. 2, pp. 490–530, 2005.
[BUE 99] BUENO J.-M., ARTAL P., “Double-pass imaging polarimetry in the human eye”,
Optics Letters, vol. 24, no. 1, pp. 64–66, 1999.
[COU 08] COUPÉ P., YGER P., PRIMA S., et al., “An optimized blockwise nonlocal means
denoising ﬁlter for 3D magnetic resonance images”, IEEE Transactions on Medical
Imaging, vol. 27, no. 4, pp. 425–441, 2008.
[DEL 12] DELEDALLE C.-A., DUVAL V., SALMON J., “Non-local methods with shape-
adaptive patches (NLM-SAP)”, Journal of Mathematical Imaging and Vision, vol. 43, no. 2,
pp. 103–120, June 2012.
[GAS 86] GASSER T., SROKA L., STEINMETZ C., “Residual variance and residual pattern in
nonlinear regression”, Biometrika, vol. 73, no. 3, pp. 625–633, 1986.
[GIA 06] GIATTINA S.,
COURTNEY B.,
HERZ P., et al., “Assessment of coronary
plaque collagen with polarization sensitive optical coherence tomography (PS-OCT)”,
International Journal of Cardiology, vol. 107, no. 3, pp. 400–409, 2006.
[GIL 08] GILBOA G., OSHER S., “Nonlocal operators with applications to image processing”,
Multiscale Modeling & Simulation, vol. 7, no. 3, pp. 1005–1028, 2008.
[GOL 99] GOLDSTEIN D., CHENAULT D., “Conference on polarization – Measurement,
analysis, and remote sensing II”, Proceedings SPIE, vol. 3754, 1999.
[GOL 02] GOLDSTEIN D., CHENAULT D., “Spectropolarimetric reﬂectometer”, Optical
Engineering, vol. 41, no. 5, pp. 1013–1020, 2002.
[GOO 08] GOOSSENS B., LUONG H., PIZURICA A., et al., “An improved non-local
means algorithm for image denoising”, International Workshop on Local and Non-Local
Approximation in Image Processing (LNLA2008), Lausanne, Switzerland, 25–29 August
2008.
[HAN 99] HANSON M., “Invexity and the Kuhn-Tucker theorem”, Journal of Mathematical
Analysis and Applications, vol. 236, pp. 594–604, 1999.
[KAT 10] KATKOVNIK V., FOI A., EGIAZARIAN K., et al., “From local kernel to nonlocal
multiple-model image denoising”, International Journal of Computer Vision, vol. 86, no. 1,
pp. 1–32, 2010.

Polarimetric Image Restoration by Non-local Means
79
[KIN 05] KINDERMAN S., OSHER S., JONES P., “Deblurring and denoising of images by
nonlocal functionals”, Multiscale Modeling & Simulation, vol. 4, no. 4, pp. 1091–1115,
2005.
[LEM 08] LEMASTER D., CAIN S., “Multichannel blind deconvolution of polarimetric
imagery”, Journal of the Optical Society of America (A), vol. 25, no. 9, pp. 2170–2176,
2008.
[LIN 06] LIN S., YEMELYANOV K., PUGH E., et al., “Separation and contrast enhancement
of overlapping cast shadow components using polarization”, Optics Express, vol. 14, no. 16,
pp. 7099–7108, 2006.
[MIG 08] MIGNOTTE M., “A non-local regularization strategy for image deconvolution”,
Physical Review Letters, vol. 29, no. 16, pp. 2206–2212, 2008.
[MIY 02] MIYAZAKI D., SAITO M., SATO Y., et al., “Determining surface orientations of
transparent objects based on polarization degrees in visible and infrared wavelengths”,
Journal of the Optical Society of America (A), vol. 19, no. 4, pp. 687–694, 2002.
[MIY 04] MIYAZAKI D., KAGESAWA M., IKEUCHI K., “Transparent surface modeling from
a pair of polarization images”, IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 26, no. 1, pp. 73–82, 2004.
[PIE 11] PIERANGELO A., BENALI A., ANTONELLI M., et al., “Ex-vivo characterization
of human colon cancer by Mueller polarimetric imaging”, Optics Express, vol. 19, no. 2,
pp. 1582–1593, 2011.
[ROU 10] ROUSSEAU
F.,
“A
non-local
approach
for
image
super-resolution
using
intermodality priors”, Medical Image Analysis, vol. 14, no. 4, pp. 594–605, 2010.
[SFI 11] SFIKAS G., HEINRICH C., ZALLAT J., et al., “Recovery of polarimetric stokes
images by spatial mixture models”, Journal of the Optical Society of America (A), vol. 28,
no. 3, pp. 465–474, 2011.
[SHE 05] SHELL J., SCHOTT J., “A polarized clutter measurement technique based on
the governing equation for polarimetric remote sensing in the visible to near infrared”,
Proceedings of SPIE (Targets and Backgrounds XI: Characterization and Representation),
vol. 5811, pp. 34–45, 2005.
[TYO 06] TYO J., GOLDSTEIN D., CHENAULT D., et al., “Polarization in remote sensing –
introduction”, Applied Optics, vol. 45, no. 22, pp. 5451–5452, 2006.
[VAL 09] VALENZUELA J., FESSLER J., “Joint reconstruction of stokes images from
polarimetric measurements”, Journal of the Optical Society of America (A), vol. 26, no. 4,
pp. 962–968, 2009.
[ZAL 06] ZALLAT J., AINOUZ S., STOLL M.-P., “Optimal conﬁgurations for imaging
polarimeters: impact of image noise and systematic errors”, Journal of Optics A, vol. 8,
no. 9, pp. 807–814, 2006.

80
Regularization and Bayesian Methods for Inverse Problems
[ZAL 07] ZALLAT J., HEINRICH C., “Polarimetric data reduction: a Bayesian approach”,
Optics Express, vol. 15, no. 1, pp. 83–96, 2007.
[ZAL 08] ZALLAT J.,
HEINRICH C.,
PETREMAND M.,
“A Bayesian approach for
polarimetric data reduction: the Mueller imaging case”, Optics Express, vol. 16, no. 10,
pp. 7119–7133, 2008.

4
Video Processing and Regularized
Inversion Methods
4.1. Introduction
This chapter focuses on some of the problems of image sequence processing (in
short it will be referred to as “video processing”) such as optical ﬂow estimation,
stereovision, and superresolution (SR). These problems are similar to inverse
problems in that they concern the estimation of a large number of variables partially
and indirectly observable. They are therefore addressed by a large number of authors
with the tools of regularized inversion, by dimension reduction or using a penalty
term [DEM 89, IDI 08]. In the context of regularization by penalty term, they have
resulted in methodological or algorithmic contributions whose scope has sometimes
extended to the whole domain of inverse problems. For instance, the CNG method
proposed by Blake and Zisserman [BLA 87],
or Chambolle’s TV criteria
optimization algorithm [CHA 04] can be recalled as two contributions originally
motivated by surface reconstruction from stereovision.
While these problems are therefore well connected to other contexts of inversion
with regard to the method of regularization or optimization algorithmics, they imply
choices and approximations concerning the data ﬁtting term. Such operations depart
somewhat from the ideal of direct modeling, such as it could be conceived by a PhD
student at the Groupe Problèmes Inverses (Inverse Problems Group) of the L2S: the
modeling of the relationship, based on physical equations, between the variable of
interest and the observable data. In other words, the complexity of the modeling of
the dynamic interaction of an imaging sensor and its environment leads to choose, in
a largely ad hoc fashion, the observation equation. Such a situation does not occur
only in video processing. It may be compared, for instance, with the modeling of
Chapter written by Guy LE BESNERAIS and Frédéric CHAMPAGNAT.

82
Regularization and Bayesian Methods for Inverse Problems
ultrasound echography or seismology by a one-dimensional (1D) convolution of
spike trains. Generally way, it can be noticed that in many inversion problems, there
is a preprocessing step (or even a pipeline of preprocessing operations) which
enables approaching the problem with some well-known statement. The distance
between a “rigorous” physical model and a relationship of circumstance is perhaps
more important in video processing than in other areas. However, this brings us to
focus this chapter on the building of the observation relationship rather than on the
selection of the regularization term.
This chapter is illustrated by three applications: motion estimation from images,
multiview stereovision and SR, which are quickly presented in the next section. In all
three cases, a common problem of image registration is revealed whose formalization
is the focus of section 4.3. To conclude, section 4.4 shows achievements that are based
on choices and approximations of the data ﬁtting term to obtain effective techniques
in a given application context.
Two reading levels are possible for this chapter. The reader interested in the
general processing methodology can focus on sections 4.2 and 4.3. Section 4.4
develops certain techniques dedicated to each of the applications and is rather
directed to readers already familiar with these issues.
4.2. Three applications
4.2.1. PIV and estimation of optical ﬂow
Particle Image Velocimetry (PIV) is an experimental technique of ﬂuid mechanics
that enables the measurement of velocity ﬁelds [RAF 07]. Figure 4.1 presents a PIV
image of a ﬂow passing upon a ramp. To make ﬂuid phenomena visible, the ﬂow is
seeded by an injection of particles, often oil droplets, and illuminated by a laser sheet
projected through the area of interest.
A high performance camera (fast, sensitive and with a good resolution) enables
the acquisition of pairs of images at two very close instants. Only one of these two
images is presented in Figure 4.1, as well as a close-up (in the red frame): unresolved
particles appear as bright spots. The analysis of a pair of PIV images, essentially by
tracking groups of particles from one frame to the next, provides the estimation of the
ﬁeld of apparent displacements (in the image plane) due to the ﬂow. This estimation
is converted into motion in the laser plane by means of a geometric transformation
identiﬁed in a preliminary calibration step. The result, velocity ﬁeld in m/s and local
vorticity in color, is presented in Figure 4.1, with a close-up of the boundary layer
separation near the ramp (red frame), where recirculation vortices can be measured.

Video Processing and Regularized Inversion Methods
83
Figure 4.1. Top row: PIV image and estimated velocity ﬁeld. Bottom row: zoom on the framed
area. High speed PIV data obtained in the S19Ch wind tunnel (ONERA) by B. Gardarin and
G. Losfeld. Processing: FOLKI-SPIV (ONERA, DAFE and DTIM departments). For a color
version of this ﬁgure, see www.iste.co.uk/giovannelli/regularization.zip
The main problem is therefore to associate the particles from one image to the
other in order to estimate their movement. In fact, the particles are indistinguishable
because they are unresolved and signiﬁcant in density. A group of particles is then
considered that appear in an area of the image (usually a square area called an
interrogation window) and this same group is searched, such as a rigid set, in the
following image. The most common techniques use a correlation calculation with
Fast Fourier Transform (FFT). The displacement found is assigned to the central
pixel of the interrogation window. By performing these operations on a regular tiling
of the image, a ﬁeld of displacement vectors is therefore estimated, whose spatial
resolution is linked to the size (in pixels) of the chosen interrogation window.
In practice, the calculation of the vector ﬁeld is often carried out with a smaller
step than the window size, the limit being one pixel. In this limiting case, another
way to see the estimation is to consider that a dense vector ﬁeld is searched for,
which describes the apparent motion of each pixel from a measurement instant to the
next. This vector ﬁeld is commonly called optical ﬂow in computer vision. The
estimation of the optical ﬂow from two images is an under-determined problem: for

84
Regularization and Bayesian Methods for Inverse Problems
each pixel, a single piece of information is available (its gray level, or rather the
difference between the gray level and its value at the next instant) and two
components of the displacement are searched for. The use of interrogation windows
can be viewed as a spatial regularization by dimension reduction: for each window,
including typically 16 × 16 pixels, a single displacement vector with two components
is estimated. The model of the underlying vector ﬁeld is therefore regarded as locally
constant. Such a hypothesis is in contradiction with the fact of searching for a vector
for each pixel: this contradiction can lead to a divergence in an iterative estimation
procedure. In this context, section 4.4.1 presents a dense converging estimation
algorithm, based on an original ﬁrst-order expansion of the data term.
Another approach is to search for the optical ﬂow to optimize a “pixelwise”
registration criterion such that:
C(u) =

s
(I1(s) −I2(s + u(s)))2 ds
[4.1]
where u ∈
2 designates the searched optical ﬂow, I1 and I2 are the image
intensities and s, the position in the image plane. The interpretation of this expression
as a data ﬁtting criterion is not obvious, as we will see in section 4.3. This criterion is
supplemented by a regularization term, for example a term expressing the energy of
the gradients of the optical ﬂow as proposed by Horn and Schunk [HOR 81]. This
type of approach has known all sorts of extensions since the 1980s and continues to
give rise to a large number of works, as reﬂected in the activity of the comparative
evaluation site of optical ﬂow techniques of the Middlebury College [BAK 07].
4.2.2. Multiview stereovision
Stereovision is a photogrammetric technique for the estimation of geometrical
quantities relating to a 3D scene (distances and angles) from two views taken from
two different positions. Multiview stereovision is often mentioned in a slightly
excessive manner, when more than two views of the scene are available. This is the
case described in Figure 4.2 in which a video recording of an urban area ﬁlmed from
a helicopter can be seen. In the top left corner, a reference view extracted from the
sequence is presented that corresponds to the mid-point of the trajectory followed by
the aircraft; the trajectory is represented at the bottom by three positions of the
camera. The map of elevations presented in the top right is in the geometry of the
reference view. It has been estimated by processing the images in the sequence and
knowing the internal parameters (focal length, etc.) of the camera used, as well as its
trajectory (position and orientation) in the coordinate frame of the reference view.
Note that the sequence is recorded by ﬁxing a point of the scene during the
displacement in such a way as to maximize the common part of the ﬁelds of view.

Video Processing and Regularized Inversion Methods
85
The details of the scene located in the center of the ﬁeld are therefore always visible,
unless a transitional occlusion caused by a closer building occurs. The apparent
trajectory of most pixels throughout the sequence can be estimated in the plane of the
image reference, which constitutes a form of generalization with more than two
views of the optical ﬂow problematic presented earlier. Under the hypothesis of a
rigid scene, these trajectories result from the movements of the camera (assumed as
known) and from the 3D structure of the scene — in other words, they are parallax
trajectories.
Reference 
image
Camera trajectory
Figure 4.2. 3D reconstruction from a side view oblique aerial sequence [LEB 08, SAN 05].
Top left, the reference image; top right, map of elevations relative to an average plan of the
ground (linear scale of gray levels between −50 and 50 m); bottom, graph of the geometric
conﬁguration of the acquisition and of the 3D reconstruction

86
Regularization and Bayesian Methods for Inverse Problems
The problem can be directly expressed as the estimation of the 3D structure of the
scene. One approach consists of deﬁning a reference view – for example the central
view presented in gray in the diagram in Figure 4.2. A point Q of the scene visible in
this view can be deﬁned by a position in pixel s which deﬁnes a ray in 3D space and
by a scalar indicating the position of the point Q on this ray. This scalar, for example,
can correspond to an altitude h relative to a reference plane (the “ﬂoor plan”). The set
of these scalars forms a map of elevations in the geometry of the reference image as
the one presented in the top right corner of Figure 4.2. For a ﬁxed map of elevations,
it is possible, as we will see more speciﬁcally in section 4.3.3, to write a data ﬁtting
criterion – the data being here the images of the sequence. This criterion expresses the
likelihood of the trajectory formed by the images of the point Q, and uses a hypothesis
of consistency of the intensity over time. Its optimization leads to a global registration
of all the images in the sequence.
In practice, it is necessary to introduce a spatial regularization, which can be done
for example with a criterion ℓ1 (total variation), allowing the smoothing of “ﬂat” areas
and preserving the surface discontinuities.
If the preservation of the contours is a key point of the estimation, it is not only
linked
to
the
choice
of
an
adapted
regularization.
In
stereovision,
some
discontinuities of the surface describing the scene are associated with occlusions (or
semi-occlusions). This is the case of the right edge of the tower which is at the center
of the ﬁeld in Figure 4.2. The pixels to the right of this contour correspond to points
located at the rear of the tower, on the ground. When the observer moves (to the left),
these points will be hidden by the tower. They are called semi-occulted, that is, they
are not visible over the whole sequence. The rigorous consideration of the partial
visibility of the points is very complicated, since visibility depends on the unknown
3D structure of the scene. As we will see in section 4.4.2, an ad hoc modiﬁcation of
the data ﬁtting criterion facilitates a good localization of the discontinuities, for
certain acquisition conﬁgurations like the one in Figure 4.2.
4.2.3. Superresolution and non-translational motion
SR consists of using the temporal redundancy of a video to estimate an image (or
a video with lower frame rate) with better quality and, in particular, better resolved.
The principle is to use inter-frame motions to improve the spatial sampling of the
scene. Most works assume that these motions are global translations of the whole
image support: it is the context considered in the SR performance model proposed in
Chapter 5 of the present book.
Figure 4.3 illustrates a more complex case in which inter-frame motions are not
limited to a translation. The input aerial sequence, denoted as low-resolution (LR)
sequence, presents a substantial forward motion, visible by comparing the three

Video Processing and Regularized Inversion Methods
87
frames extracted from the sequence presented in the upper row. On the bottom row,
we present on the left the spatial zoom (by bilinear interpolation) of the last frame of
the sequence (the closest to the scene), and on the right, the result of SR processing
combining all the frames of the sequence. The gain in quality and spatial resolution
provided by SR is clear; in particular, the correction of aliasing effects that affected
the horizontal building on the left side of the ﬁeld can be noted.
Figure 4.3. SR example on a sequence presenting a forward motion. On top: low-resolution
images extracted from the input sequence; bottom left: spatial zoom on the last frame of the
sequence; bottom right: SR results
Modeling this estimation problem again goes through the selection of a reference
view, the last image in the case of Figure 4.3, and through the registration of all the
frames of the sequence. If the motion model is not translational, it is usually
parametric, (see section 4.3). The problem can then be written as a linear system to
invert. According to the discretization and approximation choices,
different
formulations can be obtained,
leading to various precision/processing speed
compromises, that we will discuss in section 4.4.3.

88
Regularization and Bayesian Methods for Inverse Problems
4.3. Dense image registration
Image registration is central to the aforementioned applications. This section aims
to give a more precise problem statement and to present the two most often used ways
of practical implementation.
We consider the problem of dense registration as opposed to approaches based on
feature matching (edges, interest points, etc.) and estimation of a registration model
from their coordinates. Here, the registration model is directly searched for by
comparing the intensities of the two images. In the literature, this approach, that we
call dense registration,
is sometimes designated as a direct method or as
gradient-based registration (because optimization makes use of the spatial gradient
of the images).
Let us note I1 and I2, the two images to register, whose observation equations can
be written as:
* I1(m) = I1(mΔ) + B1(m)
I2(m) = I2(mΔ) + B2(m)
[4.2]
where the application s
→
Ii(s) designates the continuous image before
discretization, that is, the intensity received in any point s = [x,y]t ∈S of a portion
S of the image plane called image support. This support is sampled with a step Δ
that corresponds, for example, to the distance between the detectors in a digital
camera. The double index m = [m,n]t belongs to a grid G ⊂
2. Further in the text,
the conventional value Δ = 1 is used when this parameter is not necessary.
The main hypothesis of the registration is the conservation of the image intensity,
that supposes that the differences between the images before discretization are
exclusively due to an apparent motion ﬁeld s →u(s) that affects the whole image
support:
I1(s) = I2(s + u(s))
[4.3]
It is easy to see the limitations of this model. First, global illumination can evolve
from one image to the next, because of a dynamic gain correction of the sensor for
example. In this case, these illumination effects are corrected by searching, in
conjunction with the registration, for intensity correction parameters. Next, the local
exceptions to the conservation of intensity are numerous: occlusions, objects that
move out or come into the ﬁeld, reﬂections, etc. Nevertheless, in most cases, these
phenomena are localized and can be rejected by robust procedures. In the end, it is
considered that [4.3] can be veriﬁed on most of the image support.

Video Processing and Regularized Inversion Methods
89
The optical ﬂow u can be parametric and written as a deformation model W(s,w)
depending of a parameter vector w:
I1(s) = I2(W(s,w))
[4.4]
A very common model is the afﬁne transformation of the spatial coordinates:
s′ = W(s,w) = As + t
[4.5]
which depends on six parameters and one of its special cases is the global translation
(A = 0).
The observation equations of the apparent motion ﬁeld under the conservation of
intensity hypothesis can therefore be written as:
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
I1(m) = I1(mΔ) + B1(m),
∀m ∈G
I2(m) = I2(mΔ) + B2(m),
∀m ∈G
with I1(s) = I2(s + u(s)),
∀s ∈S
(optical ﬂow)
or I1(s) = I2(W(s,w)),
∀s ∈S
(parametric model)
[4.6]
They appear both as nonlinear in parameters (u or w) and implicit, because the
motion occurs via the received intensities I1 and I2. On the one hand, these
continuous images are themselves indirectly observed with an imperfect sensor,
which introduces noise and above all sampling. However, as discussed in Chapter 5
concerning SR, the imaging sensors are generally under-sampled.
On the other hand, the observability of the motion ﬁeld, or in other words the
“conditioning” of the estimation of u (or w), depends on the properties of the images.
As a trivial example, the motion is not observable when the images are constant. More
generally, the estimation depends on the local “textural” content of the image that
varies signiﬁcantly from a region to the other. In measurement techniques of motion
ﬁelds by imaging such as the PIV (see section 4.2.1), these observability problems are
solved by artiﬁcially increasing the texture by “seeding”. Seeding consists of adding
motion markers such as coloring agents, smoke, dust or oil droplets before acquiring
the images.
The problem [4.6] is virtually never tackled as such, but simpliﬁed to enable the
implementation of conventional strategies of data processing.

90
Regularization and Bayesian Methods for Inverse Problems
4.3.1. Direct formulation
A ﬁrst approach, which we will call direct formulation, consists of using image
interpolation, that is, to approach I2 with a simple estimator such as:
/I2(s) =

m
I2(m)γm(s −m)
∀s ∈S
[4.7]
with most of the time, and due to reasons of implementation costs, an interpolation
using a homogeneous kernel (γm = γ), separable and of limited support. The most
used models of interpolation are the bilinear model, the bicubic model and the cubic
B-spline model [UNS 99].
A direct nonlinear relationship can then be deﬁned such that:
I1(m) = /I2(m + u(m)) + B(m)
[4.8]
By modeling, most often due to the lack of a better solution, the noise term by a
white Gaussian vector, a quadratic registration criterion can be written as:
C(u) =

m∈G

I1(m) −/I2(m + u(m))
2
[4.9]
This is an instance of the criterion outlined in [4.1], written for the samples of
an observable image (I1) and by giving a precise meaning to the intervention of the
optical ﬂow in the second image. Let us note that the asymmetry of the criterion [4.9]
is hardly natural: a number of works therefore use the symmetrized criterion:

m∈G
/I1(m + u(m)/2) −/I2(m −u(m)/2)
2
[4.10]
Then the estimated motion ﬁeld is not in the geometry of either one of the images
but corresponds to an intermediate geometry (in stereovision, it is called cyclopean
geometry).
Finally, it can be noted that robust energies are often used as:

m∈G
φA

I1(m) −/I2(m + u(m))

[4.11]
φA being a potential growing more slowly than the quadratic for large deviations. This
behavior enables the rejection of the areas for which the hypothesis of conservation of
the intensity is false (glare, occlusions, missing particles in PIV, etc.).

Video Processing and Regularized Inversion Methods
91
The criteria [4.9], [4.10] and [4.11] are nonlinear least squares 1, in general
optimized by using an iterative Gauss–Newton scheme. The Jacobian expression is
inferred from the shape of the interpolation kernel γ which intervenes in [4.7].
These approaches can easily be extended to the case of the deformation of images
by replacing /I2(s + u) by /I2(W(s,w)) in the previous expressions, and using
composition formulas to obtain the Jacobian in w. In fact, the precursor of these
vision approaches is the article of Lucas and Kanade [LUC 81], about the estimation
of an afﬁne deformation model [4.5] by iterative registration of local windows, in a
context of stereovision.
4.3.2. Variational formulation
Another way to approach the implicit problem [4.6] is to search for a motion ﬁeld
or a deformation dependent on a continuous space variable. Formal expressions of the
resulting functional registration can then be written as:

S
(I1(s) −I2(s + u(s))2 ds
[4.12]
Based on the Euler–Lagrange equation which deﬁnes the minimum of
energy [4.12], an update equation of the ﬂow is derived. At this stage only, a
discretization scheme is applied to this equation (for example ﬁnite differences)
where ideal continuous images are replaced by observed discrete images, to obtain an
algorithm for numerical computation of the estimator. This approach, which we will
call variational formulation, is the most widely used in computer vision. A precursor
is Horn and Schunck’s work [HOR 81] concerning the estimation of optical ﬂow,
which uses a linearized version of the criterion [4.12] for small displacements:

S

I1(s) −I2(s) −∇I2(s)tu(s)
2 ds
[4.13]
Let us observe that in these approaches, the energy criterion of reference is not
explicit, in the sense that [4.12] and [4.13] are never calculated (nor their gradient)
during the iterations, and that the estimator is deﬁned as the stopping point of the
algorithm.
1. This is also the case of the minimization of a robust energy such as [4.11] due to the use of iterative
reweighted least squares (IRLS) methods.

92
Regularization and Bayesian Methods for Inverse Problems
4.3.3. Extension of direct formulation for multiview processing
In the context of multiview processing, a set of images {I1, . . . , IP } must be
registered on a reference view I0. The direct criterion can be written as:
C(u1, . . . , uP ) =
P

p=1

m∈G

I0(m) −/Ip(m + uk(m))
2
but has no practical interest, since it is separable and under-determined. According
to the contexts, the number of variables is decreased by using spatial models (that is
according to m) or temporals (that is according to p).
In the context of SR, it is generally assumed that the model of spatial motion is
parametric and we are led to solve P independent problems of registration:
Cp(wp) =

m∈G

I0(m) −/Ip(W(m,wp))
2
In practice, the afﬁne model [4.5], or a global translational motion, are used.
In multiview stereovision, as explained in section 4.2.2, the hypothesis of parallax
motion gives a spatially separable and temporally parametric model, that is, a criterion
such as C(h) = 
m Cm(h(m)) with:
Cm(h(m)) =
P

p=1

I0(m) −/Ip(H(m,h(m)))
2
[4.14]
where the searched surface m →h(m) is a map of elevations in the geometry of the
reference image I0 and H a homographic model in m described in [LEB 08].
4.4. A few achievements based on direct formulation
4.4.1. Dense optical ﬂow by correlation of local window
4.4.1.1. Lucas–Kanade exact approach
The objective is to estimate the motion (supposedly rigid) of an interrogation
window of radius R centered around a pixel k = [k,l]t, Fk = {m = [m,n]t ∈G |
|m −k| ⩽R and |n −l| ⩽R} by optimizing a registration criterion using a direct
formulation:
Ck(u(k)) =

m∈Fk
˜I2(m + u(k)) −I1(m)
2

Video Processing and Regularized Inversion Methods
93
In an iterative algorithm such as Gauss–Newton, a sequence of motion ﬁelds
u0 = 0, u1, . . . , uη is calculated. At iteration η + 1, the criterion is expanded to the
ﬁrst-order by setting u(k) = uη(k) + δu(k) and the resulting linearized criterion is
minimized over δu(k):

m∈Fk
˜I2(m + u0(k)) −I1(m) + ∇˜I2(m + u0(k))tδu(k)
2
[4.15]
It leads to a 2 × 2 system revealing the empirical covariance matrix of the spatial
gradient ˜I2 on the window Fk [LEB 05].
In this section, we describe the effective and convergent implementation of the
dense version of this estimation, in which an optical ﬂow vector is calculated in all the
pixels k ∈G by optimizing the global criterion:
C(u) =

k∈G
Ck(u(k))
[4.16]
Since this criterion is separable, the ﬁrst solution simply consists of performing
the Gauss–Newton iterations independently for all the pixels k ∈G. But this requires
interpolating and deriving the image I2 several times for each pixel. Considering
the pixel k: the calculation of the correction δu(k) in k uses ˜I2(k + u0(k)) and
∇˜I2(k + u0(k)). But the calculation of the correction at the neighboring pixel k+ =
[k,l + 1]t uses the values ˜I2(k + u0(k+)) and ∇˜I2(k + u0(k+)). Similarly all the
pixels located at less than a window radius from k use interpolations at the pixel k
for different offsets. Overall, it is necessary to interpolate (2R + 1)2 times the image
(and its gradients), except if the initial ﬁeld u0(k) is constant. This algorithm, which
we will subsequently call exact LK (for Lucas–Kanade), is therefore very costly.
4.4.1.2. IWS scheme
It is possible to obtain an algorithm that approaches exact LK at a much lower cost.
In the following, we introduce this algorithm by starting with the initial step u0 = 0
prior to describe the iteration itself. First of all, we can write the global criterion in the
following form:
C(u) =

k∈G

m∈G
F(m −k)
˜I2(m + u(k)) −I1(m)
2
[4.17]
where F is a weighted kernel attached to the window F; it is a simple indicator
function or a truncated Gaussian kernel if it is desired to strengthen the inﬂuence of
the central pixels. Let us note that this expression is not a convolution since the
residuals to the right depend on k by the intermediary of the optical ﬂow in the center
of the window u(k).

94
Regularization and Bayesian Methods for Inverse Problems
The iteration consists of replacing the residuals by a linearized expression
according to u(k) = uη(k) + δu(k). With regard to the initial step, u0 = 0 and it is
obtained:

k

m
F(m −k)
˜I2(m) −I1(m) + ∇˜I2(m)tδu(k)
2
By using a “spatio-temporal” notation, that is, by grouping the spatial and temporal
derivatives of the interpolated image in three-dimension vectors:
∇3˜I(m) ≜
0
∇˜I2(m)
˜I2(m) −I1(m)
1
[4.18]
and the spatial and temporal increments:
δw(k) ≜
0
δu(k)
1
1
[4.19]
the following expression of the linearized criterion [4.17] is obtained:

k
δw(k)t
F ∗∇3˜I(∇3˜I)t
(k) δw(k)
[4.20]
What does the form of equation [4.20] tell us? That the linearized criterion is
separable pixel by pixel, and that the coefﬁcients of each pixelwise quadratic
polynomial can be calculated by 2D convolutions of images by the kernel F. The
implementation of the initial registration, described in [LEB 05], is therefore
extremely effective.
During the iterations, when searching for an increment of the motion ﬁeld δu
relative to a previous ﬁeld uη non-zero, this ﬁeld appears in the residuals of the
window centered in k, (see [4.15]), and the convolutive form [4.20] is lost. An
intuitive approach is to distort the image ˜I2 with the previous ﬁeld to obtain:
˜Iη
2 (m) = ˜I2(m + uη(m)), ∀m ∈G
and then to consider the registration of I1 and of the “warped” image ˜Iη
2 as an initial
registration relative to a zero displacement, which enables the retrieval of a convolutive
form such as [4.20]:

k
δw(k)t
F ∗∇3˜Iη(∇3˜Iη)t
(k) δw(k)
[4.21]
with
∇3˜Iη(m) ≜
0
∇˜Iη
2 (m)
˜Iη
2 (m) −I1(m)
1

Video Processing and Regularized Inversion Methods
95
and the fast implementation which follows to identify in each k, with [4.19], the
increment δu(k) = u(k) −uη(k). By reiterating the process, it is obtained what
will be called an Iterative Warping Scheme (IWS) scheme, which approaches exact
LK and is signiﬁcantly less costly since the multiple interpolations and derivations in
each pixel are avoided.
The IWS scheme underlies most of the iterative LK methods published until
[LEB 05]. IWS searches for the increment u(k) −uη(k) and uses the linearization:
˜I2(m + uη(m)) −I1(m) + ∇˜I2(m + uη(m))t(u(k) −uη(k))
However, this linearization is false, since ˜I2 is developed in uη(m): for a non-
constant image, the approximation is all the more false as m moves away from k,
that is, when moving away from the center of the window. In fact, the IWS scheme
results in parasite oscillations of the estimated ﬂow that amplify when the size of the
integration window is increased, as it is shown in the results of Figure 4.4. These
effects have been the subject of several works in the PIV community and are analyzed
in more detail in [LEC 11].
4.4.1.3. FOLKI algorithm
To avoid this problem, the residuals can be developed in u(k) −uη(m), which
gives:
˜I2(m + uη(m)) −I1(m) + ∇˜I2(m + uη(m))t(u(k) −uη(m))
By deﬁning a modiﬁed spatio-temporal gradient and by using the notation:
∇3 ˜J η(m) =
0
∇˜Iη
2 (m)
˜Iη
2 (m) −I1(m) −∇˜Iη
2 (m)tuη(m)
1
a convolutive form of the linearized criterion [4.17] is obtained:

k
w(k)t
F ∗∇3 ˜J η(∇3 ˜J η)t
(k) w(k)
[4.22]
which is written in terms of the complete ﬂow w = [ut,1]t and not of the increment
as in [4.21]. This iterative scheme is at the origin of an optical ﬂow estimation
algorithm named FOLKI, for Flot Optique par Lucas–Kanade Iteratif (Iterative
Optical Flow by Lucas–Kanade): pseudo-code can be found in [LEB 05], along with
different variants and empirical convergence results. A version dedicated to
Graphical Processing Units (GPU) has been programmed by A. Plyer at
ONERA/DTIM in 2008: this code is extremely fast. It allows, for example, the

96
Regularization and Bayesian Methods for Inverse Problems
estimation of the optical ﬂow on two megapixels (full HD) videos in real time, that is
at 40 ms/frame.
Figure 4.4 presents some estimation results with IWS and FOLKI using the
example Army, for a Gaussian window with a standard deviation σ = 2 on the one
hand (left column), and for a rectangular window of size 9 × 9 on the other hand
(right column). The instability of the IWS scheme becomes manifest for a Gaussian
window, but remains limited by the small weights of the boundaries; however, the use
of a rectangular window results in an explosive estimation variance. FOLKI beneﬁts
on the contrary from the superior extension of the rectangular window and gives
smoother results, in accordance with the intuition.
Figure 4.4. Flow estimation with the example Army (available on the site vision.middlebury.
edu/ﬂow) by iterative LK: IWS scheme (at the top) and FOLKI (bottom). Gaussian window
with a standard deviation σ = 2 (left) or rectangular of size 9 × 9 (right). The representation
uses the coding with the color of the reference [BAK 07]. For a color version of the ﬁgure, see
www.iste.co.uk/giovannelli/regularization.zip
The FOLKI algorithm has been improved and extended to the context of PIV
processing in collaboration with the DAFE department from ONERA, to obtain a
GPU code named FOLKI-SPIV. This code achieves calculation times unprecedented
in the ﬁeld of PIV while providing precise, comparable, or even better results, than
current commercial PIV codes [CHA 11].
Finally, recent works concern the association of this type of algorithm, adapted to
the use of a spatial integration window with the registration term [4.16], with a spatial
regularization term of the Horn and Schunck type. This “double regularization” could
help to achieve better resolution/estimation variance compromises in PIV [FEZ 11].

Video Processing and Regularized Inversion Methods
97
4.4.2. Occlusion management in multiview stereovision
4.4.2.1. A regularized approach of the elevation estimation
We have described in section 4.2.2 the geometric model of 3D reconstruction that
we have adopted in the context of multiview stereovision and proposed a ﬁrst data-
ﬁdelity criterion spatially separable [4.14]. It can be shown that this criterion is biased
due to the special role of the reference image. Another possibility is to restart, for
a hypothesis of 3D point (m,h), from the image intensity vector collected over the
whole sequence:
/I1:P (m,h) = {/Ip(H(m,h))}1⩽p⩽P
[4.23]
expression that uses direct formulation. In the context of the intensity conservation
model, the hypothesis that the observed 3D surface goes through the point (m,h)
can be conﬁrmed by the fact that the components of the vector /I1:P (m,h) are close
to one another, or for instance that their empirical standard deviation, that we note
!σ(/I1:P (m,h)), is small.
According to this principle, an elevation map m →!h(m) is searched for in
reference geometry to minimize the regularized criterion:
J (h) =

m
!σ(/I1:P (m,h(m))) + λ

m′∈V4(m)
|h(m) −h(m′)|
[4.24]
with !σ(I), the empirical standard deviation of the vector I. The second term is a
normalized penalty ℓ1 of the ﬁnite differences of order 1 of the map h, calculated
over a four-nearest neighborhood V4(m). Algorithms computing the minimum cut in
a graph (graph-cut) permit to ﬁnd an optimal discrete solution, using a quantiﬁcation
of the elevation map in J levels {h1 < h2 < . . . < hJ} [LEB 02, LEB 08].
4.4.2.2. Occlusion management by typical visibility
The data-ﬁdelity term of the criterion [4.24] ignores occlusion issues: some areas
of the scene are not visible in some views, because they are hidden by other
structures. Figure 4.5 presents on the left the elevation map minimizing [4.24], that is
to be compared to the one presented in Figure 4.2 obtained with a technique that
takes occlusions into account. It can be observed that the occlusions are relatively
few but they have an important inﬂuence on the localization of the borders of the
superstructures.
The ﬁrst approach of this problem consists of explicitly taking into account the
occlusions by adding discrete variables; for example, a vector set of visibility v(m,h)
with dimension P such that vp(m,h) = 1, if the 3D detail spotted by the pair (m,h)

98
Regularization and Bayesian Methods for Inverse Problems
is visible in the view p and 0 otherwise. The data-ﬁdelity criterion appearing in [4.24]
is replaced by a term such that:

m
!σ(/Iv(m,h)(m,h(m)))
where the notation /Iv indicates the vector in which only the components for which
the visibility vk = 1 are preserved. The minimization of this data-ﬁdelity term is
much more complex than that of [4.24] because of the dependence of the visibility
vectors to the current surface h. It is necessary to adopt an iterative approach in
which the update order of the different parts of the scene plays an important role: to
decide the presence of an opaque element in (m,h) implies a masking deﬁned by the
shadow cone of this detail in the view p. In 2001, Kang et al. [KAN 01] have
proposed an iterative optimization technique, however their work emphasized the
difﬁculty to obtain a correct solution to this problem. Other more elaborated
techniques and usable for bigger angular explorations have been reviewed in Martial
Sanfourche’s thesis [SAN 05].
Figure 4.5. Occlusion problems. On the left, elevation map obtained by minimizing the
criterion [4.24], to compare the reconstruction, taking account of the occlusion problems that
was presented in Figure 4.2; on the right, visibility conﬁguration map, gray: total visibility,
black: half-sequence left, white : half-sequence right [LEB 08, SAN 05]
A second approach, known as radiometric approach, consists of deciding upon
the visibility based only on the quality of the radiometric criterion associated with the
3D position and with the considered view. The terms appearing in the ﬁtting criterion
of [4.24] are selected, to retain only the best residuals. The simplest technique
consists of taking a robust criterion, for example using a median rather than an
average. Following Nakamura’s works et al. [NAK 96] in multiview stereoscopy with
an acquisition device based on a matrix of cameras and those of Kang et al.

Video Processing and Regularized Inversion Methods
99
[KAN 01] in image sequence processing, we have proposed to systematize the
radiometric approach using the notion of “typical visibility”. The principle is to
assume that an approximate description of the geometrical conﬁguration can be made
with a few typical visibility vectors, which are suitable for most points of the scene.
For a side view conﬁguration shown in Figure 4.2, a three typical visibility
“dictionary” can be used: total visibility, partial left and partial right:
T :
⎧
⎪
⎪
⎨
⎪
⎪
⎩
vtotal = {1, . . . , P}
vleft = {1, . . . , P/2}
vright = {P/2, . . . , P}
[4.25]
The left and right partial visibilities, proposed in [KAN 01], correspond to the
fact that on a lateral sequence of limited angular exploration, when a detail is hidden
by an obstacle, for example one of the superstructures, it does not usually reappear
again. Next, there are several solutions to take into account these typical visibilities
[SAN 05]. A simple procedure consists of making them concurrent by using the
following criterion:
J (h) =

m
min
v∈T {wv!σ(/Iv(m,h))} + λ

m′∈V4(m)
|h(m) −h(m′)|
[4.26]
The weight wv takes into account the fact that the different typical visibilities do
not retain the same number of views. With the dictionary T from [4.25], we use the
weights {1,0; 1,4; 1,4}, the result obtained was presented in Figure 4.2. The image
on the right in Figure 4.5 represents the typical visibilities chosen on the output of
the optimization for each pixel: in gray, the pixels that are considered fully visible,
in black, those considered as visible on the left half-sequence and in white, visible
on the right half-sequence. Even on real sequences of poor quality, the decisions are
often consistent with intuition: the zones visible on the left appear on the right of the
buildings (and vice versa); on the edges of the ﬁeld of view, partial overlap between
images leads to partial visibility. The spatial regularization imposed by the second
term of [4.26] also inﬂuences these decisions and encourages homogeneous decision
areas.
4.4.3. Direct models for SR
For a sequence of observed images I0, . . . , IP of a same object deﬁned by a
distribution of intensity O, the continuous direct model is written as:
Ip(m) = Ip(mΔ) + Bp(m)
[4.27]
Ip = h ∗(O ◦Wp)
[4.28]

100
Regularization and Bayesian Methods for Inverse Problems
where h = hdet ∗hopt is the point spread function (PSF) of the camera that results from
the detector response hdet and from the optical response hopt.
Wp is a parametric geometric transformation obtained by the composition of the
transformation outcoming from the registration of the LR image Ip on the reference
image I0 (see section 4.3.3) and from a change of scale related to the desired SR
factor, deﬁned later in this chapter. In the following, we assume that the object to be
reconstructed is deﬁned in the geometry of the LR reference image I0, or, equivalently,
that the object describes the scene from the view point of the sensor at time 0 – this
implies that the transformation W0 is a simple change of scale.
The discretization of [4.28] can now be considered based on the decomposition of
the object on a basis of shifted functions:
/O(s) =

k
O(k) γ(s −kΔ′)
[4.29]
The high-resolution (HR) step Δ′ = Δ/L is chosen with respect to sampling step
Δ of the LR reference image to correspond to an SR factor of L.
The purpose of the SR is to estimate the coefﬁcients vector o from the intensities
of the LR images. The observation equation is formally written as:
i =
⎡
⎢⎢⎢⎣
i0
i1
...
iP
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
A0
A1
...
AP
⎤
⎥⎥⎥⎦o +
⎡
⎢⎢⎢⎣
b0
b1
...
bP
⎤
⎥⎥⎥⎦= Ao + b
[4.30]
The aim of this section is to present various ways to form this huge system:
typically, the SR with a factor of 3 over a sequence of twenty 512 × 512 images leads
to a matrix A with dimensions ﬁve million by two million.
Inversion of this ill-conditioned system can then be placed in a conventional
framework of regularization by penalty, that is, by optimizing a criterion such as:
J (o) = ∥y −Ao∥2 + λR(o)
[4.31]
The regularization terms conventionally employed rely on the ℓ2 norm of spatial
derivatives of the SR image, or on the use of ℓ2-ℓ1 norms. The optimization should
be carried out with algorithms adapted to the size of the system – typically ﬁrst-order
techniques such as gradient descent or conjugate gradient.
The matrix A0 describing the relationship between the LR reference image i0
and the object o is deducted from the convolution matrix H related to the instrument

Video Processing and Regularized Inversion Methods
101
response 2 by keeping a line out of L. Indeed, the component of A0 describing the
contribution of the SR pixel k to the LR pixel m is A0(m,k) = (h∗γ)(mΔ−kΔ′).
For
other
LR
images,
the
observation
relationship
reveals
the
spatial
transformation Wp and the component of Ap describing the contribution of the SR
pixel k to the BR pixel m is written as:
Ap(m,k) =

2 γ(Wp(s) −kΔ′) h(mΔ −s) ds
[4.32]
For a sufﬁciently regular transformation, this coefﬁcient can also be written in the
form:
Ap(m,k) =

2 γ(s −kΔ′) h(mΔ −W−1
p (s)) J−1
Wp ds
[4.33]
These two expressions correspond to two modeling logics illustrated in Figure 4.6.
The expression [4.32] refers to the sensor geometry and assumes that the object is
deformed (see the diagram in the center of Figure 4.6). On the contrary, according to
the expression [4.33], a transformation is applied to the sensor to place it in the object
geometry, according to the principle illustrated in the diagram on the right.
Figure 4.6. The principles of the two direct SR models are represented here for rotational
movement. On the left, the reference situation: LR image (gray coarse grid representing the
LR detectors) and SR image (black ﬁne grid). In the center, a direct model in sensor geometry,
the rotation is applied to the SR grid. On the right, a direct model in object geometry where the
sensor grid is displaced
4.4.3.1. A calculation in object geometry
When h and γ are indicator functions, the aim is to calculate the inﬂuence of
each detector in the geometry of the SR object and to sum the basic contributions
associated with the support of each basis function γ(· −kΔ′) intercepted. The partial
2. H is Toeplitz-block-Toeplitz [IDI 08]

102
Regularization and Bayesian Methods for Inverse Problems
contributions can be calculated as a function of the area of the intersection of the
HR pixel and the detector. In the case of afﬁne movements, these intersections are
convex polygons which can be estimated by a clipping algorithm such as Sutherland–
Hodgman (see en.wikipedia.org/wiki/Sutherland-Hodgman_algorithm).
In the case where γ is an indicator function, h is a Gaussian response and the
motion is homographic, Capel [CAP 01] proposes an other accurate calculation
of [4.33] based on a local afﬁne approximation of the homography. This
approximation allows to maintain the Gaussian nature of the detector response
transferred into the geometry of the object, and facilitates the calculation of the
integral.
In the two previous approaches the calculation of the components Ak(m,k)
remains a very expensive operation: most of the practical SR techniques use coarser
but fast approximations.
For example, the Schultz and Stevenson’s model [SCH 96] approximates the
transformation that affects the sensor m in [4.33] by a translation written in integer
steps of the SR grid: W−1
p (s) ≈s + Δ′km, for s in the support of the LR pixel m.
This approximation is depicted in Figure 4.7. It is easy to see that the matrix Ap can
then be obtained by selecting some rows of the convolution matrix H. Moreover, for
an instrument response modeled by an indicator function, the obtained observation
matrix has components in {0,1} 3. All these techniques are based on the convolution
matrix H; they do not consider the change of geometry of the sensor itself, which
leads to bias. This problem can be understood by noting that in the schematic
diagram of Figure 4.7, some SR pixels do not contribute to any LR pixel and others
contribute to several LR pixels at a time.
Figure 4.7. Direct model of SR in object geometry with an approximation known as Schultz
and Stevenson’s [SCH 96]: large scale movements are taken into account by shifting the
center of the detector, but the geometry of the sensor itself is left unchanged
3. In the case of the indicator function, it is further possible to model non-integer translations by
performing simple calculations of fractions of HR pixels which are intercepted by the sensor.

Video Processing and Regularized Inversion Methods
103
4.4.3.2. A calculation in sensor geometry
The most accurate discretization model used in practice is called the model of Elad
and Feuer (EF) in [ROC 06], but appears in previous works from Irani et al. [IRA 93].
This model is placed in the sensor geometry, according to the principle of the central
diagram in Figure 4.6, and decomposes the problem into three matrix operations:
Ap = DHWp
[4.34]
which are a spatial transformation (or warp) Wp, an HR convolution H and a
decimation D of a factor L.
The difﬁculty is postponed to the discretization of the warp, or, alternatively, to the
decomposition of the warped image:
Op ≜O ◦Wp
[4.35]
onto a basis of functions aligned with the sensor geometry:
/Op(s) =

k
Op(k) γ(s −kΔ′)
[4.36]
The easiest way to obtain the coefﬁcients is to use [4.29] op(k) = /
O(Wp(k)),
which is equivalent to ensuring the equality [4.35] only on the nodes of the grid. It is
well-known that this type of “ad hoc” interpolation generates aliasing effects that can
bias the direct model. This point is also emphasized by D. Capel [CAP 01]. However,
for non-translational motions, it is in general a far better approximation than that of
Schultz and Stevenson’s model.
In [ROC 06], which focuses on the case of an afﬁne motion [4.5], it is proposed to
deﬁne a representation such as [4.35] for B-splines bases and to compute the
coefﬁcients to obtain the best approximation of O ◦Wp in ℓ2. The calculation is
performed for a 0-order B-spline basis and uses a decomposition of the 2D afﬁnity in
2 successive 1D afﬁne transforms – in row and then in column.
4.4.3.3. Shift-and-add model
An special case is that of LR motions restricted to translations of integer
multiples of the SR pixel size: in this case the warp matrix which appears in [4.34]
becomes a binary matrix which deﬁnes a permutation of the components of o. By
choosing periodic boundary conditions, the matrices H and Wp of system [4.34]
become circulant-block-circulant matrices and they commute:
Ap = DH(c)W (c)
p
= DW (c)
p H(c)
[4.37]

104
Regularization and Bayesian Methods for Inverse Problems
In this expression, the exponent (c) indicates that circulant versions of the operators
are being used. In other words, writing the problem in sensor geometry or in image
geometry yields identical solutions.
The signiﬁcance of this particular framework is to allow us to write, from the
second expression of [4.37], the problem in two steps:
i = DW (c)
p z
[4.38]
z = H(c)o
[4.39]
The operator which appears in the equation [4.38] is a permutation followed by a
decimation. Its generalized inverse is stable and corresponds to very simple shift and
averaging operations of the values of LR images on an HR grid: this process is known
(slightly excessively) as shift-and-add (SA). The estimation of the SR image then
returns to the inversion of [4.39] by regularized deconvolution, typically by using a
Wiener’s ﬁlter. The resulting method will be called SAW, for Shift-and-Add+Wiener 4.
Methods such as SAW based on the equations [4.38]–[4.39] are very often used
in practice for their computational efﬁciency, even outside the context of global
translation – even though they may then lead to degraded results. The ﬁeld of
application of these methods is illustrated in Figure 4.8, which presents several
comparisons between the results of spatial interpolation, of inversion based on a
speciﬁc model such as [4.34] and of the SAW method. This image is extracted from
Antoine Letienne’s thesis [LET 10] which presents a comparative performance study
as well as some theoretical results of equivalence between these methods. The ﬁrst
two columns of Figure 4.8 relate to cases where an important number of LR images
is available (between 25 and 90) with a low or medium SNR: SAW is then almost
similar to the accurate model. The third column corresponds to a situation with a
high SNR but with a very small number of LR images (nine images). In this case, the
SAW reconstruction is affected by residual aliasing effects and appears signiﬁcantly
worse than the SR outcome using a more accurate model.
4.5. Conclusion
This chapter demonstrates the signiﬁcance of methodologies derived from
regularized inversion for video processing and more generally for low level vision, to
the extent that a preliminary reﬂection is conducted on an adequate observation
model.
4. In practice, it is often necessary to ﬁll the gaps corresponding to the nodes of the SR grid which have
not been observed. Care is taken such that the gaps have limited expansion by a suitable choice of the SR
factor knowing the number of LR images available, in order to implement simple interpolations.

Video Processing and Regularized Inversion Methods
105
Figure 4.8. Comparison of SR methods. From top to bottom: interpolated LR image, SR image
by regularized inversion of the model [4.34], SR image by SAW – SR factor set to 3. From
left to right: example with low SNR and a large number of LR images (89 images) (FLIR ATS
sequence); medium SNR and 25 images (“car” sequence); high SNR and low number of images
(nine images) (FLIR ATS sequence)
We have proposed two practical approaches of the central problem of image
registration, and then presented practical achievements based on a direct formulation.
The latter simply consists of working on an intensity distribution deﬁned on
continuous space variables by means of a model of spatial interpolation by kernels.
Via this formulation, useful observation models can be written that lead to known
algorithmic structures (optimization of regularized criteria, descent methods, graph-
cut): examples have been given in the contexts of multiview stereovision and of SR.
In the case of the estimation of optical ﬂow by window registration, we have
presented
an
original
algorithm
which
belongs
to
the
family
of
iterative
Gauss–Newton methods. This algorithm has given rise to an extremely efﬁcient GPU
implementation.

106
Regularization and Bayesian Methods for Inverse Problems
4.6. Bibliography
[BAK 07] BAKER S., SCHARSTEIN D., LEWIS J., et al., “A database and evaluation
methodology for optical ﬂow”, Proceedings International Conference on Computer Vision,
2007.
[BLA 87] BLAKE A., ZISSERMAN A., Visual Reconstruction, The MIT Press, Cambridge,
1987.
[CAP 01] CAPEL D.P., Image mosaicing and super-resolution, PhD Thesis, Visual Geometry
Group, Oxford University, UK, 2001.
[CHA 04] CHAMBOLLE A., “An algorithm for total variation minimization and applications”,
Journal of Mathematical Imaging and Vision, vol. 20, pp. 89–97, January 2004.
[CHA 11] CHAMPAGNAT F., PLYER A., LE BESNERAIS G., et al., “Fast and accurate
PIV computation using highly parallel iterative correlation maximization”, Experiments in
Fluids, vol. 50, pp. 1169–1182, 2011.
[DEM 89] DEMOMENT G., “Image reconstruction and restoration: overview of common
estimation structures and problems”, IEEE Transactions on Acoustics, Speech and Signal
Processing, vol. ASSP–37, no. 12, pp. 2024–2036, December 1989.
[FEZ 11] FEZZANI R., Approches parallèles pour l’estimation du ﬂot optique par méthode
variationnelle, PhD Thesis, UPMC, Paris, France, June 2011.
[HOR 81] HORN B., SCHUNK B., “Determining optical ﬂow”, Artiﬁcial Intelligence, vol. 17,
pp. 185–204, 1981.
[IDI 08] IDIER J., (ed.), Bayesian Approach to Inverse Problems, ISTE, London and John
Wiley & Sons, New York, 2008.
[IRA 93] IRANI M., PELEG S., “Motion analysis for image enhancement:
resolution,
occlusion, and transparency”, Journal of Visual Communication and Image Representation,
vol. 4, pp. 324–335, 1993.
[KAN 01] KANG S.B., SZELISKI R., CHAI J., “Handling occlusion in dense multi-view
stereo”, IEEE Conference on Computer Vision and Pattern Recognition, Kauai, Hawaii,
vol. 1, pp. 103–110, December 2001.
[LEB 02] LE BESNERAIS G., DUPLAQUET M.-L., “Traitement de séquences d’images
aériennes en visée latérale pour la reconstruction 3D”, Bulletin de la SFPT, vol. 166, pp. 27–
33, June 2002.
[LEB 05] LE BESNERAIS G., CHAMPAGNAT F., “Dense optical ﬂow estimation by iterative
local window registration”, IEEE International Conference on Image Processing, Genoa,
Italy, vol. 1, pp. 137–140, September 2005.
[LEB 08] LE BESNERAIS G., SANFOURCHE M., CHAMPAGNAT F., “Dense height map
estimation from oblique aerial image sequences”, Computer Vision Image Understanding,
vol. 109, no. 2, pp. 204–225, February 2008.
[LEC 11] LECLAIRE B., LE SANT Y., LE BESNERAIS G., et al., “On the stability and spatial
resolution of image deformation PIV methods”, PIV’11, Kobe, Japan, July 2011.

Video Processing and Regularized Inversion Methods
107
[LET 10] LÉTIENNE A., Super-résolution: développement d’algorithmes rapides et évaluation
de performance, PhD Thesis, University of Paris 13, Villetaneuse, France, June 2010.
[LUC 81] LUCAS B., KANADE T., “An iterative image registration technique with an
application to stereo vision”, 7th International Joint Conference on Artiﬁcial Intelligence,
pp. 674–679, 1981.
[NAK 96] NAKAMURA Y., MATSUURA T., SATOH K., et al., “Occlusion detectable stereo –
occlusion patterns in camera matrix”, IEEE Conference on Computer Vision and Pattern
Recognition, San Francisco (USA), pp. 371–378, June 1996.
[RAF 07] RAFFEL M., WILLERT C.E., WERELEY S.T., et al., Particle Image velocimetry: A
Practical Guide, Springer, New York, 2007.
[ROC 06] ROCHEFORT G., CHAMPAGNAT F., LE BESNERAIS G., et al., “An improved
observation model for super-resolution under afﬁne motion”, IEEE Transactions on Image
Processing, vol. 15, no. 11, pp. 3325–3337, November 2006.
[SAN 05] SANFOURCHE M., Traitement de séquences d’images pour l’estimation jointe de la
structure et du mouvement, Application à l’imagerie aérienne, PhD Thesis, Cergy-Pontoise
University, Cergy, France, April 2005.
[SCH 96] SCHULTZ R.R., STEVENSON R.L., “Extraction of high-resolution frames from
video sequences”, IEEE Transactions on Image Processing, vol. 5, no. 6, pp. 996–1011,
June 1996.
[UNS 99] UNSER M., “Splines – a perfect ﬁt for signal and image processing”, Proceedings
of the IEEE, vol. 16, no. 6, pp. 22–38, November 1999.


5
Bayesian Approach in Performance
Modeling: Application to Superresolution
5.1. Introduction
5.1.1. The hiatus between performance modeling and Bayesian inversion
A performance model is a formula that quantiﬁes the usefulness of an algorithm
based on a few parameters characterizing the algorithm and the set of signals on which
it is likely to be applied. These models are widespread in detection theory; many
examples can be found in [VAN 68]. Another common example is the Cramér–Rao
bound in orthodox statistics [REN 06, VAN 68].
Conversely, few contributions to performance modeling (PM) can be found in the
area of Bayesian inversion, and none in this book besides this chapter. Why this hiatus
between Bayesian inversion and PM?
Several kinds of explanations can be advanced. The ﬁrst explanation consists of
saying that PM is never useful. In simple cases, it leads to trivial theoretical results
that do not bring much in practice. A common case is the use of the mean square
error (MSE) in linear Gaussian restoration: the MSE is the sum of frequency by
frequency errors in the useful band, but it does not account for the perceptual quality
of the results. In more complex cases (deconvolution by non-quadratic regularization,
detection/estimation, etc.), it is very complicated to generate a theoretical result from
the PM of nonlinear algorithms used in practice. The second explanation relies on
theoretical arguments: in the Bayesian context, if the optimal estimator can be
calculated for some prior distribution and a given cost function, PM has no more
Chapter written by Frédéric CHAMPAGNAT, Guy LE BESNERAIS and Caroline KULCSÁR.

110
Regularization and Bayesian Methods for Inverse Problems
interest, since, in any event, the best possible performance is obtained in the context
of the chosen prior model.
Thus, the question escapes the theoretical ﬁeld and the necessary performance
evaluation becomes a question for the end-user, which is then generally addressed in
an empirical manner. As an example, during the preliminary studies of the SPOT5
satellite imager, the Centre National d’Etudes Spatiales (CNES), the French Space
Agency, selected the best algorithms by requesting from photogrammeters a ranking
of the output image quality for representative image data.
However, PM is a signiﬁcant issue, for example, for the design of systems based
on imaging. Such systems are installed onboard air carriers (e.g. airplanes,
helicopters and drones) to ensure the functions of object detection and recognition, or
to
aid
navigation.
Design
studies
go
through
algorithmic
selection
and
characterization stages using PM, necessarily theoretical at the origin, since the
sensor does not exist yet. An essential aspect is the determination of the application
ﬁeld, that is the space of input data where the algorithm will be able to provide its
best performance, as well as the boundaries beyond which the performances break
down. This description of the application ﬁeld must reveal intuitive indices because
the input spaces of typical interest to us are of very large dimension (megapixel
images and gigavoxel volumes). Again, PM can provide elegant solutions in this
respect.
We show in the following sections of this chapter that a simple reinterpretation of
the Bayesian paradigm helps to consider it as a natural framework for PM. We then
apply this methodology to the modeling of superresolution (SR) algorithms.
5.1.2. Chapter organization
We begin section 5.2 by providing details about a typical performance evaluation
prototype, its characteristics and limitations to motivate the use of a performance
model. We show that the performance evaluation prototype ﬁnds a direct
formalization in Bayesian statistics, providing, therefore, a very general theoretical
framework for PM.
We continue in section 5.3 by showing how these principles are embodied in the
analysis of a particular algorithmic class, namely SR.
SR techniques are rooted in aliasing, thus section 5.3.1 recalls the origin of
aliasing in optical imaging. The known behavior of SR techniques, such as found in
the literature, is recalled in section 5.3.2. Our performance model and its general
features are the subject of section 5.3.4. We propose in section 5.4 a few examples of
the use of this model in the analysis of the behavior of SR methods.

Bayesian Approach in Performance Modeling: Application to Superresolution
111
Section 5.5 addresses the question of the practical usefulness of the proposed PM
with regard to the SR processing of real data, in this case bar-code images, which
deviate from the chosen prior.
Section 5.6 concludes the chapter and indicates some recent declinations of the
proposed methodology.
5.2. Performance modeling and Bayesian paradigm
The evaluation of low-level processing on outputs of image sensors requires the
production of images representative of both the application context, “the scene”, and
the quality of the sensors (e.g. ﬁeld-of-view, resolution, contrast and noise). In the
context of the design of embedded systems, the characteristics of the sensors are also
to be determined. Inputs of the image processing stage, therefore, depend on a sensor
model whose parameters may vary. In practice, it is common to use an “end-to-end”
performance evaluation tool, that is modeling the scene, the sensors and processing.
5.2.1. An empirical performance evaluation tool
Figure 5.1 gives a schematic diagram of a Monte Carlo “end-to-end” performance
evaluation for the function of detecting an object hidden in an inhomogeneous
background. The general principle is to process a representative set of data, formed
from a representative set of background images and object images passed through a
sensor model (see Figure 5.1). The result of the process is then compared with the
desired result. In practice, this operation is repeated on a wide variety of background
images and object images (shown in Figure 5.1 by the background and object
databases) and the similarity measure between processing output and desired output
is averaged in order to compute a “performance measure”. The performance measure
ﬁgured here is the curve of “receiver operating characteristic” (ROC) that
summarizes the behavior of the system regarding two fundamental characteristics of
the detection: detection probability and false alarm rate.
5.2.2. Usefulness and limits of a performance evaluation tool
By varying the processing inputs by means of scene and sensor descriptors, the
issues of application ﬁeld description mentioned in section 5.1 are dealt with, whereas
if the processing is varied, then benchmarking, that is the comparison of algorithms,
can be carried out.
The comparison of algorithms can be done using objective performance measures,
and with the Internet, the advantage of a collaborative effect on a large scale can be
exploited. An example may be given such as, for computer vision applications, the

112
Regularization and Bayesian Methods for Inverse Problems
evaluation site of Middlebury College (http://vision.middlebury.edu), whose concern
is, among others, stereo reconstruction algorithms and optical ﬂow.
Figure 5.1. Synoptic view of a Monte Carlo performance assessment with an object detection
problem. Processing is applied to a set of representative images formed from a database (DB)
and a sensor model, and the result is compared to an expected output to derive statistics from
the deviations: the “performance measures”
The analysis of the application ﬁeld requires an exploration of the potential
outcomes that can be very expensive: in a simulation such as the one shown in
Figure 5.1, the space of possibilities is represented by the set of parameters
controlling the scene, the sensor and the algorithm. The dimension of this space, its
topology, and the unit cost of a basic performance measure computation can make
the approach unusable in practice. In addition, the production of such a tool is a
costly operation in itself because it involves a wide spectrum of technical ﬁelds (for
example, for an aerial detection system: atmospheric physics, sensor physics, signal
and image processing, control, planning, etc.) with a large risk of overmodeling,
since the experts of each domain naturally look for as much sharpness as possible in
their ﬁeld. Overmodeling has the effect of signiﬁcantly increasing the dimension of
the space of the simulation parameters, which is catastrophic for the parametric
exploration.
For these reasons, the derivation of more compact models directly expressing
performance without going through repeated evaluations of input instances is a very
interesting ﬁeld of research. The creation of such models is done at the price of major
simpliﬁcations of sensor models and data processing. If the performance prediction
model obtained is less accurate than the ﬁne evaluation tool previously mentioned, it
can help in contrast to quickly study the trends and compromises that will be
validated by the ﬁne evaluation tool in a later phase of development.
Therefore, performance models are less likely to predict an operating performance
value than to simplify the analysis of the processing behavior, or even the coupled
behavior of the sensor and processing. The remainder of this chapter is written from
this perspective.

Bayesian Approach in Performance Modeling: Application to Superresolution
113
5.2.3. Bayesian formalism
The Bayesian statistics formalization of a performance evaluation process, such as
the one shown in Figure 5.1, involves the following basic parts:
– the distribution of occurrences of the quantity of interest x, for example, in the
form of a probability density p(x); this distribution models the “object database” of
Figure 5.1;
– the distribution expressing the relationship between the observations y and x, for
example, under the form of the conditional density p(y|x); this distribution implies
an observation generation model and corresponds to the “sensor model” and to the
“background database” of Figure 5.1;
– the estimator x(y) is a function of the observations obtained on output at the
“processing” box of Figure 5.1;
– a measure of the estimation error cost, or “risk”, R(x,x(y)), which corresponds
to the “similarity measure” of Figure 5.1.
Given these parts, the performance measure is an average of the unit risks
weighted by their probability, that is the risk expectation on the occurrences of x and
the measures y weighted by their joint probability:
E[R(x,x(y))] =

R(x,x(y))p(x,y) dx dy
[5.1]
Summarizing, the Bayesian risk approach is a straightforward mathematical
modeling of the diagram of performance evaluation method depicted in Figure 5.1.
This abstraction is useful in some cases, which we are exploring in the remainder
of this chapter, where it is possible to express analytically the integrals involved in the
calculation of the average risk.
When the direct evaluation of the average risk is impracticable, approximations can
be used, as, for example, in [VAN 68, REN 06, LEV 08], or Monte Carlo techniques
can be used, which efﬁciently sample the occurrence space [TIC 98].
5.3. Superresolution techniques behavior
In this section, we illustrate the use of probabilistic models and associated
performance models to understand the trade-offs involved in the techniques of SR, as
proposed in [CHA 09b].
SR consists of combining several images of the same scene to build a better quality
image in terms of resolution, contrast and noise. A brief overview of this topic is
proposed in section 5.3.1.

114
Regularization and Bayesian Methods for Inverse Problems
One of the speciﬁcities of SR regarding other “low level” processes is that the
discretization step of the sought solution is smaller than the native sampling step of
the images. The ratio of the sampling step over the discretization step, denoted M in
the following text, is commonly called the SR factor. This parameter has a key role
in the ﬁrst studies on SR performance [BAK 02, LIN 04] because it is associated with
(and unfortunately often mistaken for) the concept of gain in resolution.
The characteristic feature of our works is, on the contrary, not to focus on this
parameter but to make it even completely disappear from the performance analysis.
Our logic is the following: there are already a number of parameters that affect the
quality of result of SR; these are the parameters of the acquisition conditions and the
parameters of the processing structure. M is one of the latter parameters, and it is
possible to set M without impacting the other choices while taking M as large as
possible: considering M = ∞that is a reconstruction with continuous variables.
This non-standard choice yields a surprisingly compact mathematical expression
of the MSE of SR reconstruction in terms of the scene, sensor and processing
parameters. This expression is well suited to the qualitative analysis of the behavior
of SR and to the achievement of original theoretical results. Explaining the
dependency of the performance with regard to processing is also an originality
compared to similar works in SR [ROB 06, ROB 08]:
these are focused on
Cramér–Rao bound analysis and do only characterize an optimal processing behavior
(see the discussion in section 5.2.3).
5.3.1. Superresolution
The concept of SR is linked to a speciﬁc feature of optical images: aliasing.
Aliasing occurs in video in the high-frequency areas of the images. Aliasing is the
result of a design choice, as explained by Fales [FAL 88]:
“ ... the spatial-frequency response of optical apertures (to incoherent
radiation) cannot approximate an ideal bandpass response. Instead, it is
constrained to be the autocorrelation of the optical transmittance, or
pupil, function so that the spatial-frequency response tends to decrease
smoothly with increasing frequency. This nonideal bandpass response of
optical apertures inevitably leads to a tradeoff between aliasing and
blurring in the image-gathering and image-restoring processes; aliasing
can be made negligible only at the expense of excessive blur...”
Thus, the formation of optical images naturally implies a trade-off between blur
and aliasing for the designer of imaging systems, and this trade-off usually leads us
to accept some aliasing. Aliasing limits the practical usefulness of the techniques of
deconvolution in conventional imaging, except in special cases (defocusing, motion
or exotic imaging concepts, etc.) and was initially largely ignored by the imaging

Bayesian Approach in Performance Modeling: Application to Superresolution
115
community [FAL 88]. The 1990s have witnessed the development under the
“superresolution” label of a number of techniques that use several images of a video
to create better quality images in terms of contrast, noise and resolution. Figure 5.2
gives an illustration of the signiﬁcance of SR obtained by our algorithms (see
Chapter 4), on an infrared video provided by the company FLIR-ATS.
Our works are representative of the class of “reconstruction-based” SR techniques
according to the terminology proposed by Park et al. [PAR 03]: starting from a
sequence of low-resolution (LR) images:
– motion between images is estimated by registration, or even optical ﬂow
techniques;
– the motion parameters are combined with a precalibrated point spread function
(PSF) to deﬁne a linear forward model linking the observed LR images to the desired
SR image;
– the SR image is obtained by the regularized inversion of the direct model: the
reconstruction.
These different steps are discussed in Chapter 4. Subsequently, we are interested
in the characterization of the performance of this type of technique.
5.3.2. SR methods performance: known facts
Relatively little work has been dedicated to the analysis of the behavior of SR
techniques:
[BAK 02,
LIN 04,
ROB 06,
ROB 08,
VAN 07,
WAN 05].
The
conclusions of these works are summarized later in this section by classifying them
according to three topics: the need for regularization, inﬂuential parameters and
registration error.
5.3.2.1. Condition number of forward model and regularization
All the authors agree to recognize the poorly conditioned character of the forward
model. By means of different formalisms, Wang and Qi [WAN 05] and Robinson and
Milanfar [ROB 06] showed that this bad conditioning is mainly due to the limited
bandwidth of the PSF, and to a lesser extent, due to pathological conﬁgurations of
relative motions between images. A simple example of such conﬁgurations is the case
of two very sightly shifted images [PRA 07, VAN 07, WAN 05]. The consequence of
this bad conditioning is the need for regularization [PRA 07, ROB 06, WAN 05].
5.3.2.2. Inﬂuential parameters
The parameters whose inﬂuence is predominant are the number of images, the
signal-to-noise ratio (SNR) and the SR factor M already mentioned at the beginning
of section 5.3. This factor is one of the most discussed topics in SR, ﬁrst, because it

116
Regularization and Bayesian Methods for Inverse Problems
conditions the numerical complexity and, second, because it is related to resolution
improvement [CHA 06]. Although the SR forward model has a condition number
that increases rapidly with M [BAK 02], an SR reconstruction can be deﬁned for any
value of this factor in a regularized framework. Of course, when an SR factor is
chosen greater than a threshold M ∗to be deﬁned, the SR image obtained by
regularization does not display any gain in detail: this is just an interpolated version
from a reconstruction obtained with M ∗(see [LIN 04]). In [CHA 09b], we show by
means of an experiment on real data that the choice of the SR factor is not trivial and
that if it is chosen too low it can have a limiting impact on the targeted performance.
To avoid this pitfall, we have proposed in [CHA 06, CHA 09b] to remove this
parameter in PM by using stochastic models of images with continuous variables.
Figure 5.2. Superresolution: application in infrared imaging. In the top left corner, one
640 × 512 frame from a video, in MWIR band, provided by the company FLIR-ATS. The
camera tracks the front of a moving ferry and the sequence contains several difﬁculties: noise,
insufﬁcient resolution to identify the ferry, rigid and non-translational movements (ﬂag of the
ferry and harbor in the background). At the top, on the right, result of an SR algorithm. Besides
improving contrast, a resolution improvement effect can be noted as certiﬁed by the closeups
of the identiﬁcation numbers of the ferry, bottom images. SR facilitates the unambiguous
identiﬁcation of the name of the ferry “SEA SHARP” and helps to partially resolve the number
underneath

Bayesian Approach in Performance Modeling: Application to Superresolution
117
The other important parameters are the PSF [ROB 06, ROB 08, VAN 07], and the
distribution of motion shifts [CHA 09b, PRA 07, ROB 06, VAN 07]. Regarding the
PSF, the important point is the ratio between the PSF cutoff frequency and the
sampling frequency that quantiﬁes the aliasing rate. The distribution of the shifts is
crucial:
the best performance will be obtained with shifts leading to uniform
sampling while redundant shifts will only lead to minimal gains (this situation is
more likely with low numbers of images).
5.3.2.3. Registration error
Registration introduces nonlinearity in the SR reconstruction. Taking this
nonlinearity into account makes the SR analysis more complex. Only Lin’s [LIN 04]
and Robinson’s [ROB 06] works calculate error bounds that account for a registration
error
term;
this
aspect
is
otherwise
neglected
in
other
works
[BAK 02, CHA 06, PRA 07, WAN 05]. Robinson [ROB 06] uses a Cramér-Rao
Bound (CRB) formalism to predict a performance degradation up to 25 % due to the
registration error. From our point of view, this conclusion is exaggerating the
effective impact of the registration error,
because it ignores the fact that
regularization counterbalances its effect [CHA 08, LEE 03, PIC 08]: this idea is the
basis of the works of Lee and Kang [LEE 03] and can also be deduced from Pickup
et al.’s [PIC 08, table 1] work. Moreover, Robinson neglected the registration error in
his recent works of codesign for a mobile phone camera (see [ROB 08]).
5.3.3. An SR experiment
We present in this section the SR characterization results based on real data so as
to motivate our choice of modeling, especially the choice to work with continuous
variable objects. The data used consist of a video of 50 LR frames of the resolution
test pattern “reswedges”, available at www.bealecorner.com/trv900/respat/. This test
pattern is composed of three bundles of lines horizontally juxtaposed. The size of the
second bundle is exactly half of the ﬁrst bundle, and the third bundle is also half of the
second bundle, which enables the measurement of a relative resolution.
The acquisition is made possible due to a 640 × 480 Marlin AVT B/W video
camera, with a 12 mm focal length lens at a distance of 2.7 m from the pattern, so that
it corresponds to a 150 × 110 pixel rectangular area in the image plane. The pattern
has been moved manually during shooting: the motion between images is, therefore,
essentially a translation, but uncontrolled rotations are observed, with angles between
−0.8 and 0.8. The resolution pattern and a closeup of the ﬁrst LR image of the
sequence are presented in Figure 5.3. The right side of this ﬁgure represents the
interpolated LR image and shows that the camera does not resolve entirely the
second bundle (called “2”) and the third bundle (called “4”) is completely aliased.
The SR technique used is a variant of that of Hardie et al. [HAR 98], with the
difference that ours uses an afﬁne motion model. Each frame of the LR sequence is

118
Regularization and Bayesian Methods for Inverse Problems
registered with respect to the ﬁrst. Then, the reconstruction is achieved by regularized
inversion of the forward model. Due to the regularization, the size of the SR
reconstruction is not constrained by the number of LR images used. In practice, we
set the SR factor M and we adapt the regularization parameter so as to minimize the
RMSE deﬁned hereafter. The observed variation of the best regularization parameter
with regard to M is low.
The implementation of the SR algorithm requires us to specify a PSF: this is
empirically obtained from an image of a black/white step transition captured by the
camera with a small angle between the step and the matrix of pixels. Our estimation
method is a variant of that originally proposed by Reichenbach and Park [REI 91]
and resumed in [CAP 01] and resulted in an isotropic Gaussian model of standard
deviation σh = 0.35 pixels, which corresponds to a signiﬁcant aliasing. With such a
σh, the bulk of the signal spectrum at more than twice the Nyquist frequency is
ﬁltered out in the acquisition of the images; it will thus not be possible to restore the
signal at more than twice the Nyquist frequency with a linear technique. Increasing
the resolution twice can, therefore, be considered as a theoretical bound in the
context of these tests.
Figure 5.3. SR with the “reswedges” test pattern. Left: ideal image of the pattern. Bundle “4”
is twice as thin as bundle “2”, itself two times as ﬁne as bundle “1”. The dashed lines indicate
the boundary of the high-frequency region used for the calculation of the MSE. The size of the
image is 1,479 × 943 pixels. Right: LR image interpolated
The SR reconstruction space depends on the factor M; in order to deﬁne a
common space, all the comparisons between reconstructions carried out subsequently

Bayesian Approach in Performance Modeling: Application to Superresolution
119
are relative to the geometry of the high-resolution (HR) test pattern in Figure 5.3 by
registering and interpolating SR reconstructions before calculating the square root of
the empirical MSE (root-mean-square error – RMSE). The interpolator is a bicubic
spline and only high-frequency areas of the test pattern, delimited by the dashed line
on the left side of Figure 5.3, are considered in the RMSE so as to emphasize the
frequency extrapolation obtained by SR.
Figure 5.4 compares the LR interpolated image to an SR reconstruction with 40
LR images and an SR factor M
= 4. We can give a coarse measure of the
improvement of the resolution obtained by SR with the help of the pattern. On the
LR interpolated image, bundle “2” is not completely resolved: around the 1.4 mark,
the number of dark lines of the bundle decreases from ﬁve before the mark to four
afterward, which emphasizes aliasing. In the SR reconstruction, this phenomenon
occurs around the same mark 1.4, but in the last bundle (indicated “4”). A coarse
measure of the resolution improvement factor is, therefore, 2, which is consistent
with the theoretical bound mentioned above. The SR method used in this example,
therefore, fulﬁlls its objective.
Figure 5.4. SR experiment with the “reswedges” test pattern. Left: bicubic
interpolation of the ﬁrst LR image. Right: SR with 40 images and M = 4
An SR method using M
= 2 should thus be sufﬁcient, since the observed
resolution improvement is equal to 2. Unfortunately, this is not the case, as shown in
Figure 5.5 where the performance of SR is compared with a variable M. The upper
part of Figure 5.5 presents an SR reconstruction with M = 2 (left), M = 3 (at the
center) and M = 4 (right), while the lower part is a detail of bundle “2” of the upper

120
Regularization and Bayesian Methods for Inverse Problems
part. It is observed that bundle “2” with M = 2 is not as well resolved as with M = 3
or M = 4. In addition, the reconstruction with M = 4 has a slightly better contrast
than that obtained with M = 3.
Figure 5.5. SR performance with 20 LR images and a variable M. Upper part:
SR
reconstructions with M = 2 (left), M = 3 (center) and M = 4 (right). Lower part: detail of
the bundle “2” of the upper part. Bundle “2” is not restored as precisely with M = 2 as with
M = 3 or M = 4
The qualitative results observed in Figure 5.5 are conﬁrmed by the use of a
quantitative index of reconstruction quality. Figure 5.6 presents the RMSE between
the reconstructed images and the pattern calculated for different values of M and a
variable number of LR images.
As expected, the performance in terms of RMSE improves with the number of
processed LR images, but the improvement becomes negligible beyond 20 images. In
addition, Figure 5.6 shows that the choice of M has an impact on the SR performance:
SR reconstructions with M = 2 appear always worse than those obtained with M = 3,

Bayesian Approach in Performance Modeling: Application to Superresolution
121
the latter being slightly worse than that obtained with M = 4. This inﬂuence is far
from negligible: with M = 2, it takes 20 images to achieve the level of performance
obtained with only eight images and M = 4.
0
5
10
15
20
25
30
35
40
45
60
65
70
75
80
85
number of LR images
RMSE
 
 
M=2
M=3
M=4
Figure 5.6. RMSE between SR reconstructions and test patterns for different
M and an increasing number of LR images
Let us summarize the main outcomes of this experiment:
1) With the help of the test pattern used, we have shown that the obtained
resolution improvement was close to the theoretical bound, using standard techniques
to calibrate the PSF, to register the images, and using a linear SR reconstruction.
2) We have proposed a practical evaluation method of the RMSE using a HR
reference test pattern. This RMSE appears to be sensitive not only to coarse aliasing
errors (difference between M = 2 and M = 3 at the bottom of Figure 5.5) but also
to more subtle variations in contrast (difference between M = 3 and M = 4 at the
bottom of Figure 5.5).
3) We have shown that the choice of M has a signiﬁcant impact on the empirical
performance of SR.
Based on the previous experiment, it is clear that M must be chosen sufﬁciently
large to avoid limiting the maximal attainable performance. Conversely, the
performance improvement by increasing the value of M becomes rapidly negligible,
and an SR image built with a large M appears as the interpolation of an SR image
reconstructed with a lower M. During the practical implementation of an SR
technique, the choice of M is actually a compromise between performance,
execution time and available memory. With regard to PM, the choice of a

122
Regularization and Bayesian Methods for Inverse Problems
“continuous signal/discrete data” context eliminates the inﬂuence of the parameter
M to focus on the other inﬂuential parameters: PSF, SNR, number of images.
5.3.4. Performance model and properties
Our goal now is to account for the previous characteristics using a performance
model that results from the combination of scene,
sensor,
processing and
performance measure models, on the basis of the principle of Figure 5.1 and its
mathematical modeling [5.1].
5.3.4.1. Data and processing models
Our performance model relates to the SR reconstruction methods (see
section 5.3.2) and is based on three main hypotheses:
(H1) motion between images is known: this assumption simpliﬁes a lot error
analysis and is justiﬁed in many practical cases where registration can be obtained
with very good accuracy [ROB 08, ROB 09]. Nevertheless, our analysis also applies
to registration biases such as rounding (see section 5.4.2);
(H2) the forward model is based on translation motion, as most of the works in SR
PM;
(H3) the inversion of the direct model uses a quadratic regularization:
the
reconstruction is, therefore, linear. This excludes techniques based on non-quadratic
regularization [FAR 04, SCH 96], but the improvement brought by SR already
appears with linear techniques.
Let us associate with hypotheses H1 and H2 (known as translational motion) a
continuous scene model denoted by x. The formation model of the image data is
shown in Figure
5.7. The data are modeled by noisy versions of the shifted and
sampled signal, leading to the observation equation:
zk
n = h ∗x(nΔ −τk) + bk
n, n = 1, . . . , N, k = 1, . . . , K
[5.2]
The measurement noise bk
n is assumed to be a white Gaussian of variance rb, τk is
the shift of the signals with regard to a common reference, h is the PSF and Δ is the
sampling step. The latter is chosen equal to 1 thereafter. The PSF h is characterized by
a cutoff frequency of νc. In the following, ¯νc = Δνc denotes the cutoff frequency in
units of sampling frequency. In the case of numerical applications, we have considered
the Gaussian approximation of the PSF [CAP 01, NGU 01, PAT 01, SHI 05]. The
Gaussian standard deviation should then be lower than Δ, otherwise the aliasing is
negligible and the problem comes down to a deconvolution.

Bayesian Approach in Performance Modeling: Application to Superresolution
123
Figure 5.7. Data model block-diagram. The continuous input signal x suffers
a delay τk before being ﬁltered, sampled and noised
by the sensor to form a discrete LR signal zk
The model is presented in one-dimension (1D) to alleviate the notations, but all
the formalisms are directly generalized in N-D (see [CHA 09b]) with 1D numerical
applications [CHA 06] and two-dimensional (2D) [CHA 09b].
The hypotheses (H2) and (H3) lead to a linear and space-invariant reconstruction
structure.
Associated
with
a
discrete
observation
structure
and
continuous
reconstruction, this yields an estimator with a general form:
x(u) =

k,n
wk(u −n)zk
n
[5.3]
where w(u)
=
[w1(u), . . . , wK(u)] denotes the reconstruction ﬁlters. The
reconstruction block diagram associated with [5.3] is given in Figure 5.8.
Expression [5.3] is representative of many SR methods. Let us ﬁrst cite that of
[UR 92], based on the generalized sampling theorem [PAP 84]. Then, all the linear
SR techniques under the hypothesis of translation motion [HAR 97, IRA 91, KIM 90,
NGU 01, TSA 84] can, except near the boundaries, be put into the following form:
x(m/M) =

k,n
wk(m/M −n)zk
n, ∀m ∈
[5.4]
that is, therefore, a special case of [5.3]. In this sense, [5.3] is representative of all the
linear SR techniques under the hypothesis of translational motion, while eliminating
the problem of the choice of M

124
Regularization and Bayesian Methods for Inverse Problems
+
×
×
×
z1
zk
zK
reconstruction
w1(·)
wk(·)
wK(·)
ˆx(·)
Figure 5.8. Block diagram of the image reconstruction model. Each observed signal
zk is multiplied by a Dirac comb before being ﬁltered by a reconstruction ﬁlter
and then recombined with the other signals
5.3.4.2. Input models
In accordance with the methodology previously introduced (see in particular,
Figure 5.1 and section 5.2) in order to deﬁne error statistics, it is still necessary to
deﬁne the distribution of the inputs. A description in terms of autocorrelation
function, or equivalently in terms of power spectral density (PSD), is sufﬁcient here.
We consider later on generic PSD associated with correlation functions featuring
exponential decay, largely used as image models [HAR 07, ROB 08, SHI 06b,
SHI 06a]. Another family of PSDs is proposed by Champagnat et al. [CHA 09a] to
model bar resolution test targets.
In 1D, this correlation is expressed by rx(u) = (ρx/2α) exp(−α|u|) with a PSD
Φx(ν) = ρx/(α2 + 4π2ν2), where ρx is a power scale factor, while α is associated
with the cutoff frequency of the PSD (α is, therefore, a bandwidth parameter, see
Figure 5.9).
We also consider the limit α →0+, which converges toward a PSD with a single
parameter, invariant by scaling change, of the form Φx(ν) = λ|ν|−r with r ∈{2,3}.
In 1D, the particular case r = 2 is the Wiener process already at the center of the
developments of [CHA 06]. When α →0+ the variance and all moments of order two
of the process tend toward inﬁnity, but the variogram γx [KEN 94, KÜN 87] of the
process tends toward:
γx(u) = E (x(u) −x(0))2 = ρx|u|
[5.5]

Bayesian Approach in Performance Modeling: Application to Superresolution
125
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
10
1
10
−2
10
0
10
2
10
4
10
6
10
8
10
10
ν
φx(ν)
 
 
ν = νs/2 = 0.5
α = 0
α = 0.01
α = 0.1
α = 1
Figure 5.9. PSD associated with exponential decay correlations. Log-log graph of Φx(ν) =
100/(α2 + 4π2ν2), for α = 0, 10−2, 10−1, 1. The Nyquist frequency νs/2 is represented by a
vertical dotted line. Except where α = 0, Φx has a low-frequency shape, with a cut at α/2π.
The case α = 0 is a good approximation of the behavior for |ν| > α/2π
Therefore, at the limit when α →0+, ρx can be interpreted as the growth rate of
the image signal or contrast. This amount can be estimated by the empirical means of
discrete gradients [CHA 06].
5.3.4.3. Performance measure
These model elements (scene, sensor and processing) being speciﬁed, the last step
consists of giving a performance measure. In the adopted Gaussian context, the logical
choice – which is that of most works on SR performance – is the MSE. We begin by
deﬁning a pointwise MSE:
σ2(u) ≜E (x(u) −x(u))2.
[5.6]
To get a measure of overall performance throughout the whole support, it is
necessary to spatially average the pointwise MSE [5.6]. By noting that the pointwise
MSE is periodic with a period 1 [CHA 09b], a measure of overall performance is
obtained, afterward referred to as MSE, in the form:
¯σ2 ≜
 1
0
σ2(u) du
[5.7]
One of the main results of [CHA 09b] is a compact expression of this MSE in the
Fourier domain.

126
Regularization and Bayesian Methods for Inverse Problems
THEOREM.– Given Φx, the PSD of x, H, the Fourier transform (FT) of the PSF h
and W , the FT of reconstruction ﬁlters w, then the MSE ¯σ2 [5.7] is expressed as:
¯σ2 =


Φx(ν) −2Re

W †(ν)H†(ν)Φx(ν)vν

+ W †(ν)ΦνW (ν)

dν
[5.8]
where:
Φν ≜

n
Φy(ν + n)vν+nv†
ν+n + rbI,
Φy ≜Φx|H|2,
vν ≜[exp(2iπντ1), . . . , exp(2iπντK)]t
Expression [5.8] is generalized without any problem to N-D, simply by replacing
the scalars ν, n and τk by N-vectors, and appropriate products by inner products.
5.3.4.4. Discussion
Expression [5.8] makes it possible to analyze in a theoretical way the behavior
of the SR and, as discussed in the remainder of this section, to rediscover behaviors
observed from experimental results.
To our knowledge, this expression has no equivalence in the literature of SR
performance analysis, on the one hand, because of its simplicity, and on the other
hand, because it includes the processing component.
The simplicity of this performance measure [5.8] makes it possible to obtain
formal convergence results asymptotic in the number of images, [CHA 09b], and also
to qualitatively analyze the behavior of the SR. It should be noted that the CRB
results [ROB 06], while following the same objective, have a complexity that limits
their practical use.
With regard to the second point, we notice that previous works, such as
[CHA 06, LIN 04, ROB 06, ROB 08], propose error bounds assuming optimal
processing, whereas [5.8] also depends on processing. From this MSE expression,
the optimal ﬁlter can be calculated [CHA 09b] and its performance can be evaluated,
but it is also possible to study the suboptimal SR processing. It should be recalled
that
many
SR
techniques
are
suboptimal,
as,
for
example,
in
[ELA 01, NGU 01, SCH 96, SHI 06b] (see also Chapter 4). The MSE proposed here
also represents an original tool for the evaluation of these techniques, or for the
improvement of some of them.
5.4. Application examples
Based on [5.8], we can perform two types of analysis. First, we set typical values
to the components of the performance: sensor response, translations, PSD and noise,

Bayesian Approach in Performance Modeling: Application to Superresolution
127
so as to numerically calculate the MSE and to explore the typical behaviors. Second,
we can deduce formulas from qualitative analyses and general theorems.
We then illustrate both types of analysis, ﬁrst for the study of the behavior of the
MSE by increasing the number of processed images, and then for the characterization
of a conventional suboptimal procedure, the rounding of the estimated translations.
5.4.1. Behavior of the optimal ﬁlter with regard to the number of images
Expression [5.8] shows a dependency of the MSE with regard to the translations
τ. Figure 5.10 shows a signiﬁcant dispersion between two extreme cases: (1) the case
where images are shifted by a integer number of pixels that gives the maximal error
and (2) the case where images are shifted by 1/K, where K is the number of images,
which generally yields the minimal error; the latter case is called equispaced. These
intuitive results are formally proved in [CHA 09b].
2
4
6
8
10
12
14
16
18
20
7
8
9
10
11
12
13
K
MSE
 
 
equispaced
mean
median
integer pixel shift
Figure 5.10. MSE evolution depending on the number of images K. The dashed and dotted
curves are the mean and the median of the distribution, while the solid line curve represents
the equispaced case, with minimum error. h is Gaussian with a standard deviation σh = 0.5,
Φx = 100/4π2ν2, rb = 1
When considering the MSE statistics for a random uniform distribution of shifts
over [0, 1), it is shown that the case of maximal MSE becomes a very rare event as
K increases and that the distribution of MSE tends to concentrate on its minimal
value, that is to say, the equispaced case. This behavior, illustrated in Figure 5.10,
is in fact very general: we show under very broad conditions in [CHA 09b] that this
convergence always takes place. Furthermore, it is tempting to explain the general

128
Regularization and Bayesian Methods for Inverse Problems
behavior of the MSE according to K by that of the equispaced case MSE, whose
expression is the following:
σ2
0(τ e) = 2
 +∞
νc
Φx
+

[−νc, νc]
Φx
	
1 −
Φy
Φy + rb/K + 
n̸=0 Φy(ν + Kn)

[5.9]
where τ e = [τ1, . . . , τK] is the vector of equispaced shifts that is of the form
τk = k/K. The denominator of the last integral of [5.9] is composed of three terms.
If the last two terms cancel out, the second integral is also canceled out and the signal
is fully restored in the band [−νc, νc]. Thus,
the two contributors to the
reconstruction error are clearly identiﬁed: noise (term rb/K) and aliasing (term

n̸=0 Φy(ν + Kn)).
The decay rates of the two terms are very different: with our hypotheses about the
PSD and the PSF (see sections 5.3.4.1 and 5.3.4.2), the aliasing term:

n̸=0
Φy(ν + Kn)
decreases much more quickly than the noise term rb/K. For a low noise level, it can,
therefore, be expected that the decay in K of the MSE is faster for low values of K
due to the decrease in aliasing, and that it then slows down toward a decrease in rb/K:
this behavior has been empirically observed by [LIN 04].
Figure 5.11. The three curves show the MSE of the equispaced case on the basis of the
number of signals for different levels of noise: rb = 0.1 (solid line), rb = 1 (dashed line),
rb
=
10
(dashed
line).
The
PSF
h
is
Gaussian
with
a
standard
deviation
σh = 0.25, Φx(ν) = 100/4π2ν2
This behavior is illustrated in Figure 5.11 in which the MSE is represented as a
function of K in the equispaced case for different noise powers. With regard to the

Bayesian Approach in Performance Modeling: Application to Superresolution
129
convergence results mentioned in section 5.4.1, all the curves converge toward the
same limit: Figure 5.11 shows that this convergence can be slow, especially in the
case of low SNR. It can also be noticed in Figure 5.11 that the higher the SNR, the
faster it gets to approximate a value close to the optimum. Therefore, two different
SR operating regimes can be distinguished: with a high SNR, the regime is one of
“dealiasing”, this is the typical regime scheme sought by the SR. In contrast, with a
low SNR, the regime is rather one of “denoising” by temporal averaging, for which
the use of SR techniques is not required. Averaging after motion compensation would
be just as effective in this context.
5.4.2. Characterization of an approximation: shifts rounding
Many SR techniques do not use the estimated shifts directly, but values rounded in
such a manner to get shifts which are an integer multiple of HR pixels (see Chapter 4).
For example, with an SR factor M = 4, rounding is carried out to the nearest pixel
quarter. This rounding is performed to simplify the algorithm: the “warp matrices”
involved in [CAP 01, ELA 97, HAR 98] become as a matter of fact binary if the shifts
are rounded to the nearest HR pixel.
We have assessed the impact of such a practice on real data, using test patterns that
we imaged under the conditions described in section 5.3.3.
For each SR factor M = 2, 3, 4, the estimated motion vectors are rounded off to
the closest 1/M pixel. The test pattern is then reconstructed by SR with an increasing
number of images. The results shown in Figure 5.12 are expressed in terms of RMSE,
that is to say, the square root of the mean squared error measured between the HR test
pattern and the image reconstructed from the observations.
In Figure 5.12, the lowest curve corresponds to the best result with M = 4 without
rounding. Not surprisingly, the rounding approximation improves when M increases.
However, even with a large M, rounding can generate local artifacts, as can be seen,
for example, in [HAR 07]’s article.
This behavior can be predicted by means of our performance model: Figure 5.13
illustrates
the
performance
gap
between
the
optimal
ﬁlter
and
different
approximations obtained by rounding. Performance improves with the rounding
precision and the curve representing the tenth of rounding a pixel is not much
different from the optimal curve. In all cases, a general decrease in MSE with K can
be observed but it is slower than in the case of the optimal ﬁlter. In particular, a
non-monotonic behavior is observed when rounding to the third or to the quarter of a
pixel; this is similar with the non-monotonic behavior to the half-pixel in Figure 5.12.
This non-monotonic behavior is a characteristic of suboptimality.

130
Regularization and Bayesian Methods for Inverse Problems
0
5
10
15
20
25
30
35
40
45
60
65
70
75
80
85
K
RMSE
trunc 0
trunc 1/2
trunc 1/3
trunc 1/4
Figure 5.12. Evolution as a function of the number of images K of the square root of the mean
squared error (RMSE) of the estimation. Estimated motion (◦) and rounded with different
rounding precisions: half-pixel (□), third of a pixel (⋄) and quarter of a pixel (+)
Figure 5.13. MSE evolution with K images for random shifts for the optimal ﬁlter (solid line)
and suboptimal ﬁlters, with different levels of precision: third of a pixel (−−), quarter of a pixel
(−·) and tenth of a pixel (◦). 2D case, Φx(ν) = 100/((0,25)2 + ||ν||2)3/2, h is Gaussian with
σh = 1/
√
2π, rb = 1
5.5. Real data processing
By carrying out Bayesian PM, the usual criticism about Bayesian estimation can
be expected: what is the sensitivity to the a priori model? In other words, we have
an error formula derived from an a priori model, what can be derived about the
performance obtained in practice during the processing of real signals that do not
exactly satisfy the model? In practice, two problems arise:
– instantiate an error formula such as [5.8] in a concrete performance measure,
associated with quantities accessible to experience;

Bayesian Approach in Performance Modeling: Application to Superresolution
131
– connect the theoretical scene model to real signals families characterized by
measurable statistics, that is to say, characterize the effective application ﬁeld of the
performance model [5.8].
The practical case considered in this section concerns the processing of signals
extracted from barcodes [CHA 06]. Figure 5.14 presents several images of labels that
show barcodes and illustrates the ﬁeld of view/resolution compromise that must be
carried out when designing the barcode reader. This compromise can be changed by
the use of a 1D SR processing using several barcode lines, which makes it possible to
read the barcode on the wide-ﬁeld underresolved image.
As a matter of fact, it is possible to show that a 1D declination of the SR
observation model [5.2] applies to each line of a barcode image. The reconstruction
formalism of a continuous input based on discrete data has been applied in [CHA 06]
to these 1D signals. The chosen reconstruction criterion results from the penalization
of a least squares criterion on equation [5.2] by means of a Tikhonov term of order 1:
Jτ,μ(x(·)) =
K

k=1
N

n=1

zk
n −h ∗x(nΔ −τk)
2 + μ

|x′|2
[5.10]
deﬁned on the functional space:
H =

x(·)|

|x′|2 < +∞,

|h(u −·)x(·)| < +∞, ∀u

[5.11]
such that all the written quantities have a meaning. The optimum of this criterion is
unique and is expressed as a ﬁnite linear combination of functions depending on h
and sampling points. When h is a gate function of size Δ (that is the detector is
predominant in the PSF), the solution is piecewise polynomial of order 2 and
continuously differentiable. The discontinuity points of order 2 are related to the
sampling points. The calculation of the polynomial segments involves the inversion
of a sparse KN × KN band matrix: an example of reconstruction is presented in
Figure 5.18. If the SR result is not exempt from the Gibbs phenomenon, the
information required for the recognition of the barcode is well restored by this simple
linear processing.
In addition, this solution is equivalent to a solution with a minimal MSE for Wiener
processes, that is to say of PSD in 1/|ν|2. This property helps to derive a performance
model of the process resulting from the minimization of [5.10].

132
Regularization and Bayesian Methods for Inverse Problems
5.5.1. A concrete measure to improve the resolution: the RER
We present a performance indicator more intuitive than the MSE that helps to
simply answer the question “how many images are necessary to double, triple, ... the
resolution?”.
In order to introduce this indicator, let us recall Figure 5.14 and the experimental
images. The barcode imaged from a distance in Figure 5.14-right, will not be resolved
unless the sensor is placed closer, for example, 2.5 times, which gives the image 5.14-
left. In this context of a fronto-parallel scene, it is equivalent to getting 2.5 times
closer or to using a sensor “with a resolution 2.5 times higher”, that is with detectors
2.5 times smaller (assuming the SNR is constant).
Figure 5.14. Compromise between ﬁeld and resolution: two images of a packing box and
closeups on barcodes. The high resolution (HR) image on the left is taken from a distance
twice and a half shorter than that on the right, of low resolution (LR). The ﬁeld is wider on the
right, but the barcode is unreadable
If the MSE is used as an SR reconstruction quality criterion, the two (theoretical)
curves can be plotted as presented in Figure 5.15: the MSE decreases when the number
of LR signals used for the reconstruction increases. But the MSE also varies with the
distance to the scene: if the distance is reduced, the object is better resolved and the
reconstruction is better, so the MSE decreases. The theoretical curves presented in
Figure 5.15, therefore, indicate that for a SNR of 10, an equivalent quality is obtained
with 16 images or getting 2.5 times closer.

Bayesian Approach in Performance Modeling: Application to Superresolution
133
0
2
4
6
8
10
12
14
16
18
20
0
0.02
0.04
0.06
0.08
0.1
0.12
MSE
Closeup factor (R) or number of signals (K) 
 
 
MSE = f(R)
MSE = f(K)
Figure 5.15. Theoretical MSE curves for a SNR of 10, as a function of the number
of signals (or of a closeup factor R). Processing, for example, 16
signals is equivalent in terms of MSE to getting 2.5 times closer to the scene
Figure 5.16. Theoretical RER curves for different SNR values, and empirical
curves obtained from barcode type signals (+). The number of signals is
in abscissa, and the RER (or closeup factor) is in ordinate
This process helps to deﬁne the theoretical resolution enhancement ratio (RER)
curves: these are curves without marks presented in Figure 5.16. These curves
illustrate the gain in terms of closeup factor as the number of processed LR images
increases. This gain is, of course, variable depending on the SNR.

134
Regularization and Bayesian Methods for Inverse Problems
But the following questions immediately arise:
– How a value of SNR can be set in a practical situation to know which curve of
Figure 5.16 to refer to?
– How this logic of choice for the SNR can experimentally be validated so as
to obtain the overlap between the theoretical curves and the empirical performance
measures?
5.5.2. Empirical validation and application ﬁeld
Validation consists of replacing formal MSE measures by empirical MSE based
on signals reconstructed by the 1D SR process derived from the optimization of the
criterion [5.10].
Based on a continuous signal x∗, binary and piecewise constant, constructed from
the binary sequence associated with the barcode from Figure 5.14-left, different
observation conﬁgurations can be simulated (moving the barcode away from the
sensor, number of LR signals, subpixel position and SNR) which generate observed
samples. These samples are used to reconstruct a continuous signal x(u) as the
minimum of the criterion [5.10], and the empirical MSE of x is calculated with
regard to the “ideal” barcode x∗. With the help of these empirical MSE curves, the
process described in Figure 5.15 is used to obtain the empirical RER curves.
The reconstruction criterion [5.10] uses a ﬁrst derivative regularization, which
corresponds to taking a Wiener process as an a priori scene model, characterized by
a linear variogram (see section 5.3.4.2). In this context, the theoretical SNR is
deﬁned as the ratio of the growth rate of the variogram rx over the noise variance rb.
The practical determination of rb does not raise any problem: this value is either set
in the simulation, or measured from images. In practice, rx is estimated from slope
statistics of the empirical variogram of the model signal x∗. This results, for example,
in estimating rx by the squared mean of the samples of a numerical derivation, which
is a traditional indicator of the texture content of the image. A more stable technique
consists of adjusting to the neighborhood of the origin a linear model with an
empirical variogram (see Figure 5.19).
Recalling the example in Figure 5.14-right, the SNR was estimated at 22. For this
value of SNR, the RER of Figure 5.17 indicates that by processing 11 signals (that is
11 lines of the LR barcode image, Figure 5.14-right) a closeup factor of about 2.5 is
obtained, which is illustrated in Figure 5.18.
In Figure 5.16, the empirical curves have been superimposed on the theoretical
MSE
curves,
calculated
using
an
increasing
number
of
barcode
lines
in
Figure 5.14-right. A good concordance of empirical and theoretical results can be

Bayesian Approach in Performance Modeling: Application to Superresolution
135
observed, with a noticeable difference only in the highest SNRs, which are not
realistic. Thus, the empirical evaluations based on barcode signals are in good
agreement with the theoretical results based on a Wiener process model. We explain
this result by the behavior of the empirical variogram of a barcode. Barcodes present,
in the vicinity of zero, a variogram close to that of a Wiener process (see
Figure 5.19). This behavior is not so surprising if it is recalled that a barcode can be
seen like a square-wave pulse: such a pulse has a linear variogram near the origin
then thresholded. In the Fourier domain, the PSD of this pulse has an asymptotic
behavior in 1/ν2 characteristic of the Wiener process.
Figure 5.17. Empirical RER (dashed line) and theoretical (solid line) for a SNR = 22
Figure 5.18. On top: barcode obtained by approaching 2.5 times.
At the bottom: reconstruction obtained by superresolution of 11 lines
of the barcode, at a distance (low-resolution signal)
Figure 5.19. Variogram of a Wiener process (continuous line)
and empirical variogram of a barcode signal (dashed)

136
Regularization and Bayesian Methods for Inverse Problems
In summary, although the barcodes class does not ﬁt perfectly to the class of
Wiener processes, it is sufﬁciently close, on the one hand, for the optimal process
associated with the Wiener process to give good results in practice (see Figure 5.18),
and, on the other hand, for performance models to apply (see Figure 5.16).
This observation illustrates perfectly the concept of the application ﬁeld
introduced in section 5.1: the reconstruction algorithms by Tikhonov regularization
of order 1 [5.10] have an application ﬁeld constituted by linear variogram signals and
extension linear variogram signals in the vicinity of the origin, such as barcodes.
5.6. Conclusion
The Bayesian paradigm is typically introduced as a framework for the
interpretation of the construction of optimal estimators. The purpose of this chapter is
to highlight the signiﬁcance of an alternative interpretation that facilitates the PM of
algorithms.
We apply this principle to the behavior analysis of a class of video processing
methods, namely SR, by explaining the different components of the model: input,
sensor, algorithm and performance measurement. The results presented illustrate the
ability of the model that we have proposed to conﬁrm previous observations or to
predict typical SR behaviors, optimal or suboptimal, and to analyze the dependence
of the performance with respect to the inﬂuential parameters (SNR, number of images
and SR factor).
We then addressed the question of the implementation of the proposed
performance model to the processing of real images. Based on the study of a
particular case, we have proposed an intuitive indicator measuring the potential of an
SR process to increase resolution. Moreover, we were able to characterize the
application ﬁeld of the 1D SR linear techniques used in this example. This approach
could be followed for 2D SR procedures and for other families of images.
The logical follow-up to these works on PM is the design of imaging systems. It
is more precisely concerned with the codesign of systems combining sensors and
processing, that is to say, the joint optimization of a comprehensive performance
model based on the parameters of the optical system and the processing.
With regard to improving image quality, the work reported in [CHA 09b] applies
the error model presented in this chapter to the codesign of an embedded imager
incorporating an SR process for observation missions at a distance. Other functions
can
be
addressed
with
the
same
methodology.
A
codesign
study
of
a
three-dimensional (3D) imager (that is generating a depth map) associating a passive
single-channel sensor with a depth-from-defocus process can be found in particular in
[TRO 12].

Bayesian Approach in Performance Modeling: Application to Superresolution
137
5.7. Bibliography
[BAK 02] BAKER S., KANADE T., “Limits on super-resolution and how to break them”, IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 9, pp. 1167–1183,
September 2002.
[CAP 01] CAPEL D.P., Image mosaicing and super-resolution, PhD Thesis, Visual Geometry
Group, Oxford University, 2001.
[CHA 06] CHAMPAGNAT F., KULCSÁR C., LE BESNERAIS G., “Continuous super-resolution
for recovery of 1-D image features:
algorithm and performance modeling”, IEEE
Conference on Computer Vision and Pattern Recognition, vol. 1, pp. 916–926, 2006.
[CHA 08] CHAMPAGNAT F., LE BESNERAIS G., KULCSÁR C., Performance modeling of
regularized, linear, spatially invariant super-resolution methods, Report no. RT 1/03010
DTIM, ONERA, 2008.
[CHA 09a] CHAMPAGNAT F., CORNIC P., HERBIN S., Etude EXDRO: contribution DTIM au
rapport ﬁnal, Report no. 6/14108 DPRS/DTIM, ONERA, 2009.
[CHA 09b] CHAMPAGNAT F., LE BESNERAIS G., KULCSÁR C., “Statistical performance
modeling for super-resolution:
a discrete data-continuous reconstruction framework”,
Journal of the Optical Society of America, vol. 26, no. 7, pp. 1730–1746, July 2009.
[ELA 97] ELAD M., FEUER A., “Restoration of a single superresolution image from
several blurred, noisy, and undersampled measured images”, IEEE Transactions on Image
Processing, vol. 6, no. 12, pp. 1646–1658, December 1997.
[ELA 01] ELAD M., HEL-OR Y., “A fast super-resolution reconstruction algorithm for pure
translational motion and common space-invariant blur”, IEEE Transactions on Image
Processing, vol. 10, no. 8, pp. 1187–1193, October 2001.
[FAL 88] FALES C.L., HUCK F.O., MCCORMICK J.A., et al., “Wiener restoration of sampled
image data: an end-to-end analysis”, Journal of the Optical Society of America (A), vol. 5,
no. 3, pp. 300–314, March 1988.
[FAR 04] FARSIU S., ROBINSON M.D., ELAD M., et al., “Fast and robust multiframe super-
resolution”, IEEE Transactions on Image Processing, vol. 13, no. 10, pp. 1327–1343,
October 2004.
[HAR 97] HARDIE R.C., BARNARD K.J., ARMSTRONG E.E., “Joint MAP registration
and high-resolution image estimation using a sequence of undersampled images”, IEEE
Transactions on Image Processing, vol. 6, no. 12, pp. 1621–1633, December 1997.
[HAR 98] HARDIE R.C., BARNARD K.J., BOGNAR J.G., et al., “High-resolution image
reconstruction from a sequence of rotated and translated frames and its application to an
infrared imaging system”, Optical Engineering, vol. 37, no. 1, pp. 247–260, January 1998.
[HAR 07] HARDIE R.C., “A fast image super-resolution algorithm using an adaptive wiener
ﬁlter”, IEEE Transactions on Image Processing, vol. 16, no. 12, pp. 2953–2964, December
2007.
[IRA 91] IRANI M., PELEG S., “Improving resolution by image registration”, Computer
Vision and Graphics and Image Processing, vol. 52, no. 3, pp. 231–239, May 1991.

138
Regularization and Bayesian Methods for Inverse Problems
[KEN 94] KENT J., MARDIA K., “The link between kriging and thin plate splines”, in
KELLY F. (ed.), Probability, Statistics and Optimization: A Tribute to Peter Whittle,
Chapter 24, Wiley, Chichester, pp. 325–339, 1994.
[KIM 90] KIM S., BOSE N., VALENZUELA H., “Recursive reconstruction of high resolution
image from noisy undersampled multiframes”, IEEE Transactions on Acoustics, Speech
and Signal Processing, vol. 38, no. 6, pp. 1013–1027, June 1990.
[KÜN 87] KÜNSCH H.R., “Intrinsic autoregressions and related models on the two-
dimensional lattice”, Biometrika, vol. 74, no. 3, pp. 517–524, 1987.
[LEE 03] LEE E.S., KANG M.G., “Regularized adaptive high-resolution image reconstruction
considering inaccurate subpixel registration”, IEEE Transactions on Image Processing,
vol. 12, no. 7, pp. 526–837, July 2003.
[LEV 08] LEVIN A., FREEMAN W., DURAND F., “Understanding camera trade-offs through
a Bayesian analysis of light ﬁeld projections”, European Conference on Computer Vision,
vol. Part IV, Marseille, France, pp. 88–101, 2008.
[LIN 04] LIN Z., SHUM H.-Y., “Fundamental limits of reconstruction-based superresolution
algorithms under local translation”, IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 26, no. 1, pp. 83–97, January 2004.
[NGU 01] NGUYEN
N.,
MILANFAR
P.,
GOLUB
G.,
“A
computationally
efﬁcient
superresolution image reconstruction algorithm”, IEEE Transactions on Image Processing,
vol. 10, no. 4, pp. 573–583, April 2001.
[PAP 84] PAPOULIS A., Probability, Random Variables and Stochastic Processes, McGraw-
Hill, New York, 2nd ed., 1984.
[PAR 03] PARK S.C., PARK M.K., KANG M.G., “Super-resolution image reconstruction:
a technical overview”, IEEE Signal Processing Magazine, vol. 20, no. 3, pp. 21–36, May
2003.
[PAT 01] PATTI A.J., ALTUNBASAK Y., “Artifact reduction for set theoretic super resolution
image reconstruction with edge adaptative constraints and higher-order interpolants”, IEEE
Transactions on Image Processing, vol. 10, no. 1, pp. 179–186, January 2001.
[PIC 08] PICKUP L.C., CAPEL D.P., ROBERTS S.J., et al., “Bayesian methods for image
super-resolution”, The Computer Journal, vol. 52, no. 1, pp. 101–113, 2008.
[PRA 07] PRASAD S., “Digital superresolution and the generalized sampling theorem”,
Journal of the Optical Society of America (A), vol. 24, no. 2, pp. 311–325, February 2007.
[REI 91] REICHENBACH S., PARK S., “Small convolution kernels for high-ﬁdelity image
restoration”, IEEE Transactions on Signal Processing, vol. 39, pp. 2263–2274, 1991.
[REN 06] RENAUX A., Contribution à l’analyse des performances d’estimation en traitement
statistique du signal, PhD Thesis, École normale supérieure de Cachan, 2006.
[ROB 06] ROBINSON D.,
MILANFAR P.,
“Statistical performance analysis of super-
resolution”, IEEE Transactions on Image Processing, vol. 15, no. 6, pp. 1413–1428, June
2006.
[ROB 08] ROBINSON D., STORK D.G., “Joint digital-optical design of superresolution
multiframe imaging systems”, Applied Optics, vol. 47, no. 10, pp. 11–20, April 2008.

Bayesian Approach in Performance Modeling: Application to Superresolution
139
[ROB 09] ROBINSON D., FARSIU S., MILANFAR P., “Optimal registration of aliased images
using variable projection with applications to superresolution”, The Computer Journal,
vol. 52, no. 1, pp. 31–42, 2009.
[SCH 96] SCHULTZ R.R., STEVENSON R.L., “Extraction of high-resolution frames from
video sequences”, IEEE Transactions on Image Processing, vol. 5, no. 6, pp. 996–1011,
June 1996.
[SHI 05] SHIMIZU M., OKUTOMI M., “Sub-pixel estimation error cancellation on area-based
matching”, International Journal of Computer Vision, vol. 63, no. 3, pp. 207–224, July
2005.
[SHI 06a] SHI J., REICHENBACH S., “Image interpolation by two-dimensional parametric
cubic convolution”, IEEE Transactions on Image Processing, vol. 15, no. 7, pp. 1857–1870,
July 2006.
[SHI 06b] SHI J., REICHENBACH S.E., HOWE J.D., “Small-kernel superresolution methods
for microscanning imaging systems”, Applied Optics, vol. 6, no. 45, pp. 1203–1214,
February 2006.
[TIC 98] TICHAVSKY P., MURAVCHIK C.H., NEHORAI A., “Posterior Cramér-Rao bounds
for discrete-time nonlinear ﬁltering”, IEEE Transactions on Signal Processing, vol. 46,
no. 5, pp. 1386–1396, May 1998.
[TRO 12] TROUVÉ P., Conception conjointe optique/traitement pour imageur compact à
capacité 3D, PhD Thesis, École Centrale de Nantes, December 2012.
[TSA 84] TSAI R.Y., HUANG T.S., “Multiframe image restoration and registration”, HUANG
T., (ed.), Advances in Computer Vision and Image Processing: Image Reconstruction from
Incomplete Observations, Chapter 7, JAI Press, Greenwich, pp. 317–339, 1984.
[UR 92] UR H., GROSS D., “Improved resolution from sub-pixel shifted pictures”, Computer
Vision and Graphics and Image Processing, vol. 54, pp. 181–186, March 1992.
[VAN 68] VAN TREES H.L., Detection, Estimation and Modulation Theory, Part 1, John
Wiley, New York, 1968.
[VAN 07] VAN EEKEREN A.W.M., SCHUTTE K., OUDEGEEST O.R., et al., “Performance
evaluation of super-resolution reconstruction methods on real-world data”, EURASIP
Journal on Advances in Signal Processing, vol. 2007, no. 1, pp. 043953, available at
http://asp.eurasipjournals.com/content/2007/1/043953, 2007.
[WAN 05] WANG Z., QI F., “Analysis of multiframe super-resolution reconstruction for image
anti-aliasing and deblurring”, Image and Vision Computing, vol. 23, pp. 393–404, 2005.


6
Line Spectra Estimation
for Irregularly Sampled Signals
in Astrophysics
6.1. Introduction
The detection and the characterization of oscillations that are present in time
series are questions raised by many data analysis problems. In some ﬁelds of
observational science,
the acquisition is subject to an irregular process of
measurement. This is the case, for example, of astronomical observations: objects of
interest can be periodically unobservable due to the alternation of day and night or
seasons, and adverse weather conditions cause gaps in temporal coverage. In more
general terms, the sampling times may be irregularly spaced due to instrumental
constraints. This kind of sampling makes the problem of oscillation frequency
estimation particularly complex. In the Fourier domain, ideal sampling at times
{tn}n=1,..., N leads to the convolution of the spectrum being sought by a spectral
window Ws(f):
ys(t) = y(t) ×
N

n=1
δ(t −tn)
FT
−→Ys(f) = Y (f) ⋆Ws(f), Ws(f) =
N

n=1
ei2πftn.
For regular sampling tn = (n −1)Ts, the spectral window is a periodized sinc
function Ws(f) = eiπ(N−1)fTssin(πfNTs)/sin(πfTs), with a main lobe of width
2/NTs. This expression is no longer valid with irregular sampling, where the
window does not have simple properties – it should be noted, however, that the
half-width of the main lobe remains of the order of the inverse of the duration of the
Chapter written by Sébastien BOURGUIGNON and Hervé CARFANTAN.

142
Regularization and Bayesian Methods for Inverse Problems
observation. Moreover, it can be shown that the periodic lack of data generates
secondary lobes in |Ws(f)| at the corresponding frequency, whose amplitude
increases with the proportion of gaps in the temporal coverage. The transition to an
irregular sampling scheme also causes loss of periodicity of Ws(f), which is at the
origin of spectral aliasing. Figure 6.1 illustrates these different aspects, showing the
windows corresponding to various types of sampling. The gaps due to the alternation
of day and night cause, in particular, secondary lobes in |Ws(f)| exceeding 60% of
its maximum (achieved at zero). On the informational level, the estimation problem
is all the more difﬁcult as the convolution window has a complex shape.
Regular sampling
Spectral window |Ws(f)|/N
0
1
2
3
4
5
−0.5
0
0.5
−30
−20
−10
0
10
20
30
0.2
0.4
0.6
0.8
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
Irregular sampling
Spectral window |Ws(f)|/N
0
1
2
3
4
5
−0.5
0
0.5
−30
−20
−10
0
10
20
30
0.2
0.4
0.6
0.8
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
Irregular sampling + holes
Spectral window |Ws(f)|/N
0
1
2
3
4
5
−0.5
0
0.5
−30
−20
−10
0
10
20
30
0.2
0.4
0.6
0.8
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
Time (d)
Frequency (c/d)
Frequency (c/d)
Figure 6.1. Sampling schemes (left) and moduli of associated spectral windows (center and
close-up around f = 0 on the right). Time is expressed in day (d) and frequency in cycles per
day (c/d). At the top, regular sampling at Ts = 20 c/d. At the center, irregular sampling. Below,
irregular sampling with daily gaps. In each case, N = 100 data items
This chapter studies different approaches to the estimation of sinusoidal
oscillations, formalized as a deconvolution problem in the Fourier domain. The
searched spectrum then corresponds to a set of lines, with unknown frequencies and
magnitudes.
In
terms
of
spectral
analysis,
these
methods
can
be
called
high-resolution methods, in the sense that they can, in theory, exceed the Fourier
resolution, corresponding to the width of the main lobe of Ws. They are also part of
non-parametric methods, aimed at directly recovering the entire Fourier spectrum of
the signal. This formulation is particularly suitable for the analysis of time series in
astronomy, where the modeling of data in the form of multi-sinusoidal oscillations is
often only an approximation:
a frequent case concerns the perturbation of the
oscillations by a low-frequency component, corresponding to a continuous spectrum.
On the other hand, multiple preprocessing is often made during data reduction,

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
143
inevitably generating a number of errors. A non-parametric approach seems therefore
more appropriate in terms of robustness to model errors than methods based on more
constrained models. Let us note again that the irregular nature of sampling causes
loss of geometric structures in the observation models, so that some parametric
methods developed in the regular sampling case are not applicable here.
Figure 6.2 shows a particularly complex artiﬁcial data set, in which the results of
the different presented methods will be illustrated. The sampling and the oscillation
frequencies reproduce the radial velocity curve of the Herbig star HD 104237, which
was observed over ﬁve nights in April 1999 at the South African Astronomical
Observatory [BOU 07a]. This star shows several pulse modes between 30 and 37 c/d
and the spectral window has highly marked side lobes at ± 1 c/d, such that the
Fourier spectrum presents several local maxima at frequencies that do not correspond
to oscillations.
Time series
Spectral window
0
1
2
3
4
6
8
10
12
14
−5
0
5
0
0.5
1
Time (d)
Frequency (c/d)
Fourier spectrum
Fourier spectrum (close-up)
0
10
20
30
40
50
0
0.2
0.4
0.6
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
Frequency (c/d)
Frequency (c/d)
Figure 6.2. Simulated data for a difﬁcult spectral analysis problem. (Top) Time series and
spectral window. (Bottom) Modulus of the Fourier spectrum of the data (solid line) and true
lines (×), on the left for frequencies between 0 and 50 c/d, on the right, close-up between 30
and 37 c/d
The methods discussed in this chapter are divided into several groups. A number
of generalizations of the periodogram using irregular sampling have been proposed in
the astrophysics community since the 1970s [FER 81, LOM 76]. Section 6.2 is
dedicated to them, where they are interpreted in the context of the maximum
likelihood estimation of sinusoidal model parameters. Section 6.3 explicitly
approaches spectral analysis as a deconvolution problem, in which the objective
consists of recovering the Fourier spectrum discretized on an arbitrarily ﬁne grid. The
problem being generally underdetermined, obtaining a satisfactory solution requires

144
Regularization and Bayesian Methods for Inverse Problems
the injection of a priori information, which is expressed in terms of spectral sparsity.
Estimation is then described as the sparse approximation of the data in a dictionary of
sinusoids where complexity, both informational and algorithmic, exceeds that of
standard problems of this area. The following two sections examine different
approaches presented from this perspective. Several iterative (greedy) methods are
analyzed and compared in section 6.4, where techniques based on the principle of the
CLEAN algorithm, frequently used in astronomy, are also presented. Section 6.5
examines in a thorough fashion the estimation by means of the minimization of a
least squares criterion penalized by the ℓ1-norm. The probabilistic modeling of
sparsity by a Bernoulli–Gaussian (BG) model is ﬁnally studied in section 6.6. With
regard to the previous approaches, which represent the standards of sparse
approximation methods, a more thorough exploitation of the data is proposed using
Markov chain Monte-Carlo algorithms (MCMC), at the expense of a higher
computational cost.
6.2. Periodogram, irregular sampling and maximum likelihood
Let us consider the complex harmonic model with frequency ν and amplitude α:
∀n ∈{1, . . . , N},
yn = αei2πνtn + ϵn
⇐⇒
y = α e(ν) + ϵ,
[6.1]
where
y
=
[y1, . . . , yN]t
contains
the
data
sampled
at
times
tn,
e(ν) = [ei2πνt1, . . . , ei2πνtN ]t and ϵ = [ϵ1, . . . , ϵN]t is a random perturbation term.
For the sake of simplicity, we consider independent and identically distributed (iid)
perturbations 1, Gaussian, centered, with variance σ2. The maximum likelihood
estimation of (α,ν) with respect to the maximum likelihood is therefore equivalent to
minimizing the quadratic error:
(!α,!ν) = arg min
α,ν
JQ(α,ν),
JQ(α,ν) = ∥y −αe(ν)∥2 ,
where the linearity in α makes it possible to write, with ﬁxed ν, !α(ν) = e(ν)†y/N.
By injecting this solution into JQ, it is straightforward to show that the frequency !ν
maximizes:
PS(ν) = 1
N
e(ν)†y
2 = 1
N


n
yne−i2πνtn

2
= 1
N |Ys(ν)|2 .
This last expression deﬁnes the periodogram of the data (Schuster periodogram).
Maximizing the frequency by maximizing PS therefore corresponds to the maximum
likelihood estimation of the frequency of model [6.1].
1. All the developments presented here, based on the likelihood of the data, can be extended without any
major difﬁculty to the case of non-iid Gaussian perturbations, a case frequently encountered in astrophysics,
for example due to the variation of the atmospheric conditions or to the observation of the same object from
several instruments.

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
145
For data with real values, the Fourier spectrum Ys(ν) is Hermitian and PS(ν)
admits two maxima around !ν and −!ν. It may seem more natural to use the sinusoidal
model:
∀n ∈{1, . . . , N},
yn = a cos(2πνtn) + b sin(2πνtn) + ϵn ⇐⇒y = Γ(ν)s + ϵ
[6.2]
noting s = [a, b]t the associated amplitudes and Γ(ν) the matrix composed of the N
lines [cos(2πνtn), sin(2πνtn)]. By a process analogous to the complex harmonic
case, the maximum likelihood estimation of the pair (s, ν) is equivalent to
minimizing: 2
∥y −Γ(ν)!s(ν)∥2 = ∥y∥2 −ytΓ(ν)

Γ(ν)tΓ(ν)
−1 Γ(ν)ty
[6.3]
with respect to ν. When sampling is regular tn = (n −1)Ts, and for frequencies
νk in the form ±k/(NTs), k ∈
, namely, for the frequencies corresponding to the
deﬁnition grid of the discrete Fourier transform (DFT), the vectors [cos(2πνktn)]n
and [sin(2πνktn)]n are orthogonal: Γ(νk)tΓ(νk) = NI2; thus, !νk maximizes:
1
N
Γ(νk)ty
2 = 1
N
"
yn cos(2πνktn)
#2
+
"
yn sin(2πνktn)
#2
= 1
N


yne−i2πνktn

2
,
which corresponds to the periodogram PS evaluated at frequencies νk. With irregular
sampling, or for frequencies outside the DFT grid, the two columns of Γ(ν) are no
longer orthogonal: the minimizer of [6.3] no longer has a simple interpretation
involving the periodogram. Lomb [LOM 76] has proposed a trick that consists of, at
a
given
frequency
ν,
introducing
in
model
[6.2]
an
offset
τν = (4πν)−1Arctan {
n sin(4πνtn)/ 
n cos(4πνtn)} to the time axis origin,
such that the two columns of Γ(ν) are orthogonal. The minimization of [6.3] is then
simpliﬁed in !ν = arg maxν PL(ν), with:
PL(ν) = (
n yn cos(2πν(tn −τν)))2
2 
n cos2(2πν(tn −τν))
+ (
n yn sin(2πν(tn −τν)))2
2 
n sin2(2πν(tn −τν)) ,
which deﬁnes the Lomb periodogram. The latter provides an expression of !ν as a
maximizer of a simple analytical criterion, without the need for inverting a 2 × 2
system for any frequency ν. The optimum is then computed on a grid. It should be
noted that if the periodogram PS corresponds to the convolution (in modulus) of the
spectrum of the signal by the spectral window, this is no longer the case of PL.
2. It is straightforward to show that for ﬁxed ν, s(ν) =

Γ(ν)tΓ(ν)
−1Γ(ν)ty.

146
Regularization and Bayesian Methods for Inverse Problems
If the Lomb periodogram is commonly used in astronomy, a more complete
version, which is proposed by Ferraz-Mello [FER 81], has surprisingly remained
unknown. This latter consists of including a continuous component to the
model [6.2]. As a matter of fact, even if the data values are in average zero, a
continuous component may still be present, particularly when the sampling scheme
has periodic gaps with frequencies close to ν: it is then possible to sample in a more
frequent manner the upper (or lower) part of the sine wave. In this case, a model
without continuous component can lead to erroneous estimation of the oscillation
frequency [FER 81]. By means of an orthogonalization approach similar to the
previous case, Ferraz-Mello then obtains a criterion solely depending on the
frequency, whose maximization – here again on a grid – achieves the maximum
likelihood estimation of the parameters of the associated model.
These different periodograms are interpreted as measures of likelihood for a
model with a single sinusoidal oscillation. Their maximization therefore has the
properties of maximum likelihood estimation, namely that they are asymptotically
(as the number of data items tends toward inﬁnity) unbiased and with a minimal
variance. It is interesting to observe that the modiﬁed versions of the periodogram,
taking into account the sampling irregularity, have been developed for the processing
of astrophysical data, notably where the presence of periodic gaps makes the problem
difﬁcult. In the case of a model with several oscillations:
yn =
K

k=1
ak cos(2πνktn) + bk sin(2πνktn) + ϵn,
[6.4]
the estimation based on the maxima of the periodogram no longer admits rigorous
statistical basis. If the frequencies νk are sufﬁciently separated, the identiﬁcation of
these maxima may, however, yield satisfactory results. The presence of strong side
lobes in the spectral window (see section 6.1), on the other hand, makes it more
uncertain to use the periodogram, which can show high peaks that do not correspond
to the existence of lines, as shown in the example in Figure 6.2. In critical
conﬁgurations of the frequencies νk relative to sampling, the interferences between
the contributions of the different lines can even lead to a global maximum of the
periodogram at a frequency not present in the data.
6.3. Line spectra models: spectral sparsity
Different methods have been proposed by the astrophysics community, whose
principle consists of successively extracting the peaks in the periodogram, by
“cleaning” at each iteration the latter from the contribution of the last estimated line.
The algorithm is stopped when none of the peaks is any longer statistically
signiﬁcant,
hence
the
name
prewhitening
given
to
these
methods

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
147
[FER 81, GRA 73, ROB 87]. Other approaches have been proposed, notably based
on works carried out on deconvolution of spike trains in geophysics [TAY 79],
discussing spectral analysis in non-parametric form. The estimation of the whole
Fourier spectrum of the data, discretized on a grid of frequencies, is then written as a
linear inverse problem, where the sparse nature of the sought spectrum has to be
taken into account [BOU 07a, CIU 01, DEM 99, DEM 02, SAC 98].
For 15 years, numerous works of research have been developed around the notion
of sparsity of signals and images where they are expressed, in an exact or
approximate manner, as a linear combination of a low number of elementary signals
(the atoms) taken in a redundant collection (the dictionary). Although mainly prior to
the development of this thematics, the approaches described above can be seen as
sparse approximation methods, where the dictionary is composed of sinusoidal
functions. This section formalizes the relationship between the estimation of spectral
lines and sparse approximation, insisting on the speciﬁcities of the model, caused
notably by the irregular nature of the sampling scheme. Addressing the estimation
from – now largely numerous – works on sparsity thus provides a new insight, with
new tools and algorithms. Nevertheless, the complexity of the problem reveals the
limits, in the case of inverse problems, of the “generic” tools of sparse estimation,
both at the level of theoretical properties and that of associated algorithms.
6.3.1. An inverse problem with sparsity prior information
Instead of searching for a small number K of sine waves with any frequencies
νk, we consider here a multi-sinusoidal model, with a large number P of frequencies
regularly discretized between 0 and a maximal frequency fmax. The model is then
written as:
yn = a0 +
P

p=1
ap cos(2πfptn) + bp sin(2πfptn) + ϵn,
with fp = p
P fmax, [6.5]
where the frequencies νk of model [6.4] are searched in the form of νk = (pk/P)fmax
with pk an integer between 0 and P. Both models are therefore equivalent if we add
the constraints apk ̸= 0, bpk ̸= 0 and ap = bp = 0 for p ̸∈{pk}k=1, ..., K. In contrast
to model [6.4] whose parameters are the order K, the K frequencies and the 2K

148
Regularization and Bayesian Methods for Inverse Problems
corresponding amplitudes, model [6.5], considering ﬁxed frequencies and a known
order, becomes linear with the only unknowns ap, bp. It can be written 3:
yn =
1
√
N
P

p=−P
xpei2πfptn + ϵn
⇐⇒
y = Wx + ϵ,
[6.6]
where W =

1
√
N ei2πfptnp=−P,..., P
n=1,..., N
and x ∈
2P +1 contains the corresponding
amplitudes. The introduction of the factor 1/
√
N in the dictionary W helps here to
have columns wp of unit norm, a property that will be useful later. In regular sampling,
the maximal frequency is limited by Shannon’s theorem. As can be seen in section 6.1,
the irregular nature of sampling causes loss of periodicity of the spectral window.
Spectral aliasing is then pushed away to much higher frequencies than in the case of
regular sampling of the N data items. The irregularity of sampling is in this sense an
advantage, allowing the estimation of the spectrum on a much wider range [EYE 99].
The deﬁnition of the largest frequency range exempt of aliasing is a complex issue
and is not necessarily required: very high frequencies do not always have a physical
sense and the coverage of a broad domain might unnecessarily increase the number of
unknowns of model [6.6] and hence the cost of the associated algorithms. A practical
solution consists of choosing a value of fmax in accordance with the physics of data,
ensuring that the spectral window does not present any periodicity in the considered
band.
The estimation, from measurements y, of the spectrum x discretized on the
frequency grid {(p/P)fmax}p=−P,..., P is an inverse problem with a linear model,
referred to as non-parametric spectral analysis, in the sense that the model does not
present any explicit dependency in the frequency parameters νk and in the order K of
model [6.4]. In the case of sinusoidal oscillations, the problem then becomes that of
the estimation of a line spectrum, where most of the amplitudes are zero. Therefore, a
sparse approximation x of the data y is searched for in the dictionary W. It should
be noted that the gain brought by the linearity occurs at the expense of the
discretization of the frequency space. The constraints of a high-resolution (allowing
close frequencies to be separated) analysis and with high precision (limiting the
effect of approximation of the frequencies of the model on a grid) therefore require
the ﬁnest reconstruction grid possible, namely a high number of unknowns 2P + 1.
The dictionary W is usually very redundant, with many more columns (number of
atoms) than lines (number of data items). Let us also note that the simplest solution
to problem [6.6], the generalized inverse solution of y = Wx of minimal ℓ2-norm, is
not satisfactory: ﬁrst, this solution is very sensitive to noise in the data, the operator
W being ill-conditioned; on the other hand, it does not include zero values, since the
ℓ2-norm is not a measure of sparsity.
3. This formulation is actually more general than [6.5], since it includes the case of data with complex
values.

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
149
It should ﬁnally be observed that the usual problems of sparse representation in a
dictionary W aim to approximate the data y by Wx (with an underlying sparsity
hypothesis of x). Here, however, the spectral analysis problem relates directly to the
amplitude vector x. It is therefore important to limit the effects of an inaccurate
discretization on the estimator. It is then necessary to use a ﬁne discretization of the
frequencies, making the problem more complex both at the algorithmic and
informational level. Some a posteriori corrections will also be considered for the
approach with ℓ1-norm of section 6.5, as well as a model extension for the
probabilistic approach of section 6.6, which can improve frequency localization.
6.3.2. Difﬁculties in terms of sparse approximation
Abundant works concerning “denoising” and “compression” during the last two
decades have relied on the sparsity hypothesis that assumes that, for certain classes
of signals and images, the essential information is concentrated in a small number
of coefﬁcients, expressed in an appropriate transformed space (see, e.g, [MAL 09]).
Under a model such as [6.6] where W represents this transformation, often redundant
(x being of larger dimension than y), the sparsest solution is sought, namely with the
least non-zero components, which correctly ﬁts the data:
min
x ∥x∥0 such that ∥y −Wx∥2 ⩽τ, where ∥x∥0 = Card{k | xk ̸= 0}.
[6.7]
This large-dimensional combinatorial optimization problem can generally not be
solved exactly and two classes of suboptimal approaches are often considered:
– greedy methods, which build a sparse solution by selecting components in an
iterative manner;
– relaxation of problem [6.7] in a numerically computable approximation, the most
commonly used consisting of substituting the ℓ0-pseudonorm by the ℓ1-norm: ∥x∥1 =

k |xk|.
The characterization of the solutions obtained by these approaches in relation to the
solution of [6.7] has been the subject of numerous works and conditions of equivalence
have been proposed, based on near-orthogonality measures of W:
– mutual coherence is deﬁned by μ(W) = maxk̸=ℓ|w†
kwℓ|. The solutions
obtained by greedy [TRO 04] and by ℓ1 relaxation [FUC 04] methods correspond to
the solution of [6.7] if it satisﬁes ∥x∥0 < (1 + 1/μ(W))/2;
– the restricted isometry constant δK is the smallest value such that for any
combination WK of K columns of W and ∀u ∈
K, (1−δK)∥u∥2 ≤∥WKu∥2 ≤
(1+δK)∥u∥2. The ℓ1 −ℓ0 equivalence is then ensured for solutions with less than M
non-zero components if δM + δ2M + δ3M < 1 for noise-free data, and close solutions
in an ℓ2-norm sense are obtained if δ3M + 3δ4M < 2 in noisy cases [CAN 06];

150
Regularization and Bayesian Methods for Inverse Problems
– the exact recovery coefﬁcient (ERC) coefﬁcient is written for a solution !x of [6.7]
with support Ω = {p|!xp ̸= 0} as: ERC(Ω) = 1 −maxℓ̸∈Ω ∥W+
Ωwℓ∥1, with
W+
Ω = (W†
ΩWΩ)−1W†
Ω. In the noiseless case, a positive value of ERC(Ω) ensures
the convergence of the greedy methods toward !x [SOU 11, TRO 04] and ensures
that the solution obtained by ℓ1 relaxation corresponds to the solution of [6.7] for
an adequate value of τ [TRO 06].
For spectral analysis, the structure of the dictionary is imposed by the sampling
{tn}n and by the frequency discretization grid. In the case of regular sampling with
fmax set at Nyquist limit and for spectrum x with the same size as data y, W is the
inverse orthogonal DFT operator. In the general case, we have:
{W†W}k,ℓ= 1
N

exp
	
i2π k −ℓ
P
fmaxtn

= 1
N Ws
	k −ℓ
P
fmax

,
where Ws is the spectral window (see section 6.1). The mutual coherence then reads
μ(W) = maxp̸=0 1
N |Ws( p
P fmax)|. For high-resolution analysis, i.e. high P μ(W)
corresponds to the sampling of |Ws| in its main lobe, and can therefore be very close
to 1. When the temporal coverage includes gaps, the spectral window has strong side
lobes and the sampling of Ws in these lobes can also cause a high value of μ(W),
typically greater than 0.5. Therefore, the condition on μ(W) is only valid for ∥x∥0 ⩽
1, which ensures that greedy algorithms such as ℓ1 relaxation provide the solution
to problem [6.7] only if it includes at most one sine wave. Concerning the restricted
isometry constants, it can be shown that, with uk = uk+1 = 1, uℓ= 0 for ℓ̸= k:
δ2 ⩾∥Wu∥2
∥u∥2
−1 = 1
N
N

n=1
cos
	
2π fmax
P
tn

.
For high P, the frequency fmax/P is very low and δ2 is close to 1. As a result,
sufﬁcient conditions of correct estimation are not satisﬁed, even for a single sine
wave. Finally, the ERC does not take any usable form for spectral analysis but, in
practice, it always takes a negative value in difﬁcult cases. Let us ﬁnally note that the
properties based on the restricted isometry constant and the ERC rely on the fact that
the desired solution is exactly sparse in the dictionary W, which is not the case for
spectral analysis, where the true frequencies can be located outside the discretization
grid of x.
In summary, the sampling complexity and the need for high resolution make the
Fourier dictionary very correlated and the classic characterizations of the criteria and
algorithms based on sparsity do not apply. For example, the sampling scheme of data
in Figure 6.2, with maximum frequency fmax = 50 c/d and frequency discretization
with P = 4,000 positive frequencies, corresponding to the conﬁguration considered
for our simulations, yields the mutual coherence of the dictionary μ(W) = 0.99, the

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
151
isometric constant δ2 ⩾0.98 and ERC = −0.74 (calculated for the sparse solution at
the grid frequencies that are closest to the true frequencies).
6.4. Prewhitening, CLEAN and greedy approaches
Adaptive pursuit algorithms or greedy algorithms, popularized by the works of
Mallat and Zhang [MAL 93], construct a sparse solution in an iterative manner. A
number of variants have been proposed, whose performances can be evaluated from a
perspective of compromise between the quality of the solution and the computational
complexity. If the interpretation of these methods in the context of sparse
representations is fairly recent, they were ﬁrst proposed much earlier: in the 1960s
for variable selection in statistics (see, e.g., [MIL 02]), in the 1970s for prewhitening
techniques [GRA 73] and the CLEAN method [HÖG 74], which can be seen as a
reﬁnement of a matching pursuit (MP) strategy. This section recalls the principle of
the most common greedy algorithms (MP, orthogonal MP (OMP), orthogonal least
squares (OLS)), compares their performance and highlights their limitations on the
problem of spectral analysis with irregular sampling. Two approaches aiming to
improve their performance are then presented. The ﬁrst is an extension of the OLS
algorithm based on the explicit minimization of a sparsity criterion, where each
iteration also enables the elimination of components [SOU 11]. The second proposes
a relaxation of the greedy methods, only removing at each iteration, a fraction of the
detected component: this is the principle of the CLEAN method, whose application
to spectral analysis dates back to the mid-1980s [ROB 87]. These algorithms are
mainly presented using the vocabulary proper to sparse approximation. In the case of
spectral analysis, the dictionary W corresponds to the N × (2P + 1) matrix of
complex exponentials, sampled at times {tn} and discretized at frequencies
{(p/P)fmax}p=−P,..., P , and whose columns constitute the atoms.
6.4.1. Standard greedy algorithms
Greedy methods construct a solution, for which at iteration n, the signal y is
approximated with the sum of n atoms of the dictionary, indexed by Ωn:
/yn =

k∈Ωn
xkwk = WΩnxn, with WΩn = [wk]k∈Ωn and xn = [xk]k∈Ωn.
Based on a zero initialization /y0 = 0, each iteration thus consists of:
i) the selection of an atom wk(n) from the approximation of the previous iteration,
deﬁning the new support Ωn = Ωn−1 ∪{k(n)};

152
Regularization and Bayesian Methods for Inverse Problems
ii) the update of amplitudes xn of the selected atoms;
iii) the update of the approximation /yn and of the residual error rn = y −/yn.
The algorithm is stopped when none of the atoms is any longer signiﬁcant in the
residual rn. The stopping criterion controls therefore the number of components of
the solution and plays a similar role to that of the regularization parameter of
penalized approaches of section 6.5. Throughout this chapter, the different methods
will be adjusted in such a way as to provide solutions producing statistically
equivalent residuals: for an estimated spectrum !x and a residual r = y −W!x, the
solution will be chosen such that r is of the order of the noise level ϵ. If the noise
variance σ2 is known, ∥ϵ∥2 /σ2 follows a χ2 distribution with N degrees of freedom.
We
will
then
choose
!x
such
that
the
error
∥r∥2 /σ2
≃
ξ,
where
Pr(u ⩽ξ | u ∼χ2
N) < η, for a probability η ﬁxed at 95%.
6.4.1.1. Matching pursuit
The MP algorithm [MAL 93] selects at each iteration the atom having the
highest correlation with the residual, and estimates the associated amplitude with the
correlation coefﬁcient:
i) k(n) = arg maxk |w†
krn−1|
and ii)
xk(n) = w†
krn−1.
The cost of an iteration is therefore limited to that of the projection W†rn−1. This
choice can be interpreted as the maximum likelihood estimation of the atom
xk(n)wk(n)
under
the
model
(and
under
the
hypothesis
of
iid
Gaussian
perturbations):
rn−1 = xkwk + ϵ.
In the case of spectral analysis, the iteration therefore selects the frequency
maximizing the periodogram of the residual rn−1 discretized on the frequency grid.
However, as explained in section 6.2, the maximization of the periodogram is only
statistically valid in the case of a model with a single noisy sinusoid. The estimation
of the ﬁrst atom (with r0 = y) is therefore equivalent to assuming such a model on
y, whereas the next iteration of MP searches for a new sine wave in the residual,
which was supposed to only contain noise at the previous iteration. This approach is
therefore sensitive to interferences between the different frequencies, in the same
way as is the estimation of a multi-sinusoidal model from the maximum of the
periodogram. Thus, on critical spacing of frequencies, such an approach can perform
an erroneous selection, generating error propagation in subsequent iterations.
Figure 6.3 shows the results obtained with the data of Figure 6.2, presenting several
false alarms as well as the non-detection of two of the ﬁve frequencies of the signal.

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
153
6.4.1.2. Orthogonal matching pursuit
The OMP algorithm (see, e.g., [BLU 07]) performs the same selection as MP, but
reﬁnes the amplitudes estimation by minimizing the approximation error:
ii) xn = arg minun ∥y −WΩnun∥2.
The term orthogonal comes from the fact that at each iteration, /yn represents the
best approximation of y by means of a linear combination of atoms of WΩn with
respect to the Euclidean norm. The approximation after n iterations thus yields a lower
residual with OMP than with MP, for a slightly higher computation cost due to the
inversion of an n × n system at iteration n. However, the main limits of MP, due to
the selection stage, also affect OMP with fewer false alarms with OMP due to a better
estimation of the magnitudes, as illustrated in Figure 6.3.
6.4.1.3. Orthogonal least squares
The OLS algorithm (see, e.g., [BLU 07]) is a greedy method whose selection step
is more complex: the new atom is the one which, when added to the previous ones,
minimizes the approximation error of the new model. The selection then amounts to
calculating the optimal amplitudes corresponding to all models integrating each new
potential atom, and to choosing the one providing the lowest error:
i) and ii) {k(n),xn} = arg mink,un ∥y −WΩnun∥2 with Ωn = Ωn−1 ∪{k}.
This is a ﬁner and more expensive approach than OMP, requiring at each iteration
the inversion of a number of n × n systems equal to the number of unknowns not
yet selected 4. With regard to spectral analysis, unlike MP and OMP, not only the
frequency maximizing the periodogram of the residual is tested, but the model with n
frequencies (including n −1 ﬁxed) approximating at best the data is searched for. As
illustrated in Figure 6.3, the selection is made more robust, even if it is not completely
exempt of error propagation problems identiﬁed in previous sections.
6.4.2. A more complete iterative method: single best replacement
The single best replacement (SBR) algorithm, which was recently introduced in
[SOU 11] (see also Chapter 2, section 2.4.2), is an iterative approach aiming to solve
the
ℓ0
problem
[6.7],
by
seeking
to
minimize
a
criterion
such
as
∥y −Wx∥2 + λ ∥x∥0 . Each iteration proposes to add a new component or to
withdraw a previously selected component, by choosing the operation that most
decreases this criterion. It should be noted that, unlike the ℓ1-norm penalization
approach that will be examined in section 6.5, this approach falls under the category
of combinatorial optimization problems. By questioning the detections made in
4. Inversions can nevertheless be performed recursively, see, for example, [SOU 11].

154
Regularization and Bayesian Methods for Inverse Problems
previous iterations, this algorithm is more robust than standard greedy algorithms
faced with the critical problems caused by sampling – at the expense, of course, of an
increase of the computational cost. Figure 6.3 illustrates this behavior, where a better
frequency localization is obtained with regard to OLS. In practice, a homotopic
version of the algorithm has been used [DUA 09], where the parameter λ decreases
until the residual of the estimation is at the noise level, as for the previous approaches
(see p. 152).
Matching Pursuit
Orthogonal Matching Pursuit
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
1
2
3
4
5
6
7
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
1 2
3
4
5
6
Orthogonal Least Squares
Single Best Replacement
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
1
2
3
4
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
CLEAN (detected freq.)
CLEAN (after convolution)
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
Frequency (c/d)
Frequency (c/d)
Figure 6.3. Results of the greedy algorithms for the spectral analysis of the data of Figure 6.2:
modulus of the Fourier spectrum of the data (dotted), true lines (×), estimated lines (◦) and
estimation residual (solid line). For MP, OMP and OLS, the order of the frequencies’ detection
is indicated. In the case of CLEAN, the detected frequencies are represented as well as the ﬁnal
spectrum obtained after reconvolution with a Gaussian kernel (solid line)
6.4.3. CLEAN-based methods
In order to reduce the sensitivity of prewhitening methods (equivalent to MP) to
false alarms, Roberts et al. [ROB 87] have suggested a trick originally proposed for
the reconstruction of radioastronomical images. The objective is, at stage ii), to only
withdraw a fraction γ < 1 from the contribution of the component selected at stage i).
This trick limits the effects of a bad selection on subsequent iterations. In practice,
the estimation of a line is achieved after several iterations (since each iteration only

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
155
removes a fraction of the line), which can lead to frequencies slightly shifted between
them. The ﬁnal spectrum is presented as the convolution of the different extracted
components by a kernel with the same width as the main lobe of the spectral window,
in order to give it a smooth appearance. A strictly sparse spectrum is therefore no
longer obtained. The result of such a method is illustrated in Figure 6.3. If the solution
is all the more stabilized as γ is low, as a result, the computational cost also increases.
Thus, with a typical value of γ = 10 %, 29 iterations are required to remove 95% of
the contribution of one component.
6.5. Global approach and convex penalization
We are here interested in regularization approaches by convex penalization
[DEM 89, IDI 08], where the estimator minimizes a criterion of the form:
!x = arg min
x
J(x), J(x) = 1
2 ∥y −Wx∥2 + λΦ(x).
[6.8]
The quadratic function ΦΠ(x) = x†Πx represents the simplest regularization,
for which !x has an explicit expression and is linear with respect to the data. In the
case of high-resolution spectral analysis, Giovannelli and Idier [GIO 01] have shown
that the estimator can be written as the DFT of the windowed and zero-padded data,
where the window is speciﬁed by the form of Π 5. This type of regularization is
hardly exploitable in our case, where the complex nature of the sampling makes any
data windowing operation delicate. More importantly, with the same arguments as
those invalidating the use of generalized inversion (see section 6.3.1), quadratic
regularization cannot provide a sparse solution: the presence of zeros is necessarily
linked to a thresholding stage, explicit or not, and therefore intrinsically originates
from a nonlinear estimation process.
In order to strengthen the sparse nature of the spectrum, different penalties have
been proposed in the form Φ(x) = 
p ϕ(|xp|), reﬂecting, on the one hand, the a
priori independence between the various spectral components, and, on the other
hand, the fact that sparsity relates to the modulus of the amplitudes and not to their
phase. Several functions ϕ(u) with a slower growth than the quadratic function, such
as the log-Cauchy function ln(1 + u2/2σ2) [DEM 02, SAC 98] or the branch of the
hyperbola
√
s2 + u2 [CIU 01], have thus been used for the estimation of spectra with
a “peaked” appearance. Such functions lead to a continuously differentiable 6
criterion J, paving the way for numerous algorithmic solutions with regard to
optimization. On the other hand, the minimization of [6.8] provides a sparse solution
5. This property, however, is only valid for regular sampling.
6. It should be noted that the ﬁrst works in this direction [SAC 98] use non-convex penalties, that can
generate local minima in the criterion. Such an effect is especially to be avoided with irregular sampling,
where it was observed that it worsens the multimodal nature of the likelihood function.

156
Regularization and Bayesian Methods for Inverse Problems
in the strict sense (namely with zero components) if and only if the penalization
function ϕ is not differentiable at 0 [MOU 99]. Used since the 1970s in spike train
deconvolution [ALL 94, TAY 79], penalization with the ℓ1 norm (ϕ(u) = |u|) ﬁts
into this framework. This norm is also mentioned by Demoment and Idier relative to
spectral analysis in a Bayesian context [DEM 99], in the form of a Laplace prior
distribution. More recently, such penalization has been the subject of numerous
works, both theoretical and applied, in the context of sparse approximations. The
remainder of this section examines the properties of the ℓ1 penalization estimation,
corresponding to a convex relaxation of the ℓ0-norm problem [6.7], for spectral
analysis:
!x = arg min
x
J1(x),
J1(x) = 1
2 ∥y −Wx∥2 + λ ∥x∥1
[6.9]
6.5.1. Signiﬁcance of ℓ1 penalization in
In criterion [6.9], the ℓ1 penalization relates to the modulus of the complex
amplitudes
xp.
For
a
model
with
real
variables
such
as
[6.5],
where
xp,p⩾0 = (ap −ibp)/2 and xp,p<0 = (ap + ibp)/2, the equivalent penalization
function is
√
a2
p + b2
p and is no longer an ℓ1-norm. It is important to note here the
difference in models between the used penalization and the penalization of ℓ1 with
real variables, as in |ap| + |bp|. While the ﬁrst jointly penalizes both the real and the
imaginary parts of the same complex amplitude (and therefore jointly generates zero
or non-zero values for ap and bp), the second penalizes independently ap and bp,
which is less satisfactory in terms of a priori modeling: if a spectral line is present at
the frequency fp, both real and imaginary parts of the associated amplitude are
generally non-zero. A detailed study of this difference can be found in [BOU 07b].
6.5.2. Existence and uniqueness
Criterion [6.9], sum of two convex functions, is convex and therefore admits a
global minimum. However, in the frequent case where there are more unknowns than
available data items, the quadratic part, as well as the penalization term, are not
strictly convex. It is thus not directly possible to prove the uniqueness of the
minimizer on a strict convexity argument, but only claim that the set of minimizers is
itself also convex. Let us note that the results of the many comparisons between
standard ℓ0-norm and ℓ1-norm problems (see section 6.3.2), which relate the
equivalence of both problems to the uniqueness of the sparsest solution, do not
provide any answer to the problem of high-resolution spectral analysis. As a matter
of fact, these results are based on a near orthogonality measure of the dictionary,
while we have seen in section 6.3.2 that, in our case, the latter may be highly
correlated.

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
157
Another way to characterize uniqueness of the solution is based on the unique
representation property (URP), which assumes that any set of N columns of W
is linearly independent, N being the number of data items. It has been shown in
[BOU 07b] that for any matrix satisfying the URP, problem [6.9] admits at most one
solution with less than N/2 non-zero components. The URP property of the matrix
W relative to spectral analysis is also examined, where the following conclusions are
established:
– any matrix W constructed from regular sampling with fmax less than or equal
to the Nyquist limit satisﬁes the URP;
– in irregular sampling, for a ﬁxed P and given times {tn}, the values of fmax for
which W does not satisfy the URP are isolated points of
+.
Consequently, if the solution of problem [6.9] has fewer than N/2 components, its
uniqueness can generally be asserted.
6.5.3. Minimizer and regularization parameter analytical characterization
The extension to the complex case of a result established in [ALL 94] with real
variables enables us to establish the following condition [BOU 07a]:
!x minimizes J1 ⇐⇒
 ∀p such that xp = 0, |!rp| < λ
∀p such that xp ̸= 0, !rp = λei arg(xp) ,
where !r = W†(y −W!x). This characterization of the optimality makes it possible
to strictly test the convergence of the optimization procedure. It also provides the
information about the meaning of the parameter λ, which is the maximum value
reached by |!r|, where |!r|2 is the periodogram of the estimation residual y −W!x,
sampled on the reconstruction grid.
6.5.4. Amplitude bias and a posteriori corrections
The penalization with the ℓ1-norm yields a sparse spectrum;
therefore,
frequencies are identiﬁed at the grid positions where !x is non-zero. However, the
estimation of the associated amplitudes is biased. Indeed, Fuchs [FUC 04] gives the
expression of bias from the second condition of optimality of the preceding
paragraph: by indexing with ⋆the non-zero components of !x, this condition can be
written as W†
⋆(y −W⋆!x⋆) = λei arg(!
x⋆). If the number of components in !x⋆is less
than N/2 (which is the case, in practice, where a limited number of lines are
desired), the matrix W⋆satisﬁes the URP condition (see section 6.5.2) and has full
rank. This system can then be inverted in the form:
!x⋆=

W†
⋆W⋆
−1W†
⋆y −λ

W†
⋆W⋆
−1ei arg(!
x⋆).

158
Regularization and Bayesian Methods for Inverse Problems
The ﬁrst term of the right member corresponds to the unbiased estimation by least
squares of the amplitudes of !x⋆under the model y = W⋆x⋆+ϵ, and the second term
is a bias term that can be eliminated once the solution !x⋆is computed.
It should be noted that, in practice, a line is sometimes detected by the two closest
frequencies on the grid. In addition, an a posteriori frequency correction, for example
by simply taking the barycenter of the two neighboring frequencies weighted by their
estimated amplitudes, allows us to improve the accuracy of the estimated frequencies.
6.5.5. Hermitian symmetry and speciﬁcity of the zero frequency
For real-valued data, the spectrum x is Hermitian: x−p = x∗
p. If such a constraint
does not appear in previous developments, it is, however, possible to show [CIU 01]
that criterion [6.9] admits a Hermitian minimizer. As a matter of fact, if !x is a solution,
then the symmetric spectrum !xH such as !xH
p = !x⋆
−p is also a solution since J1(!x) =
J1(!xH). By convexity, the solution (!x + !xH)/2 is Hermitian and also a solution.
Moreover, if the minimizer is unique (which is usually the case, see section 6.5.2),
then !x satisﬁes the desired Hermitian property, without imposing it in an explicit
manner.
In addition, we have seen in section 6.2 that it was important to integrate the
estimation of a continuous component with irregular sampling. However, ℓ1-norm
penalization induces a bias on the amplitudes (see section 6.5.4). If it is a posteriori
possible to correct this bias, a biased continuous component estimation can interfere
with the detection of other frequency components. It is thus more natural to not
penalize, in criterion [6.9], the amplitude x0 corresponding to f0 = 0.
6.5.6. Optimization algorithms
A very large number of algorithms have been proposed, in the context of sparse
approximations, for the optimization of criterion [6.9]. A large majority takes the
form of projected gradient algorithms, alternating between a stage of descent, guided
by the gradient of the quadratic part of the criterion and a thresholding
step [DAU 04]. These methods have been developed in relation to large-size
problems (in particular, for images), where sparsity is often expressed in a widely
incoherent dictionary. This property conditions the complexity of optimization: the
closer the dictionary to orthogonality, the more signiﬁcant the descent step will be
and the more effective gradient-based approaches will be. With regard to spectral
analysis, the size of the problem remains limited by the one-dimensional nature of
the data. Moreover, time series in astrophysics rarely exceed 10,000 points. In
contrast,
the Fourier dictionary is extremely correlated due to the required
high-frequency resolution and the shape of the spectral window. This is a common

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
159
feature to all spike deconvolution problems where obstacles, informational but also
computational, originate from the difﬁculty in separating close peaks. The algorithms
mentioned above can then appear to be pathologically slow. It should also be noted
that the impossibility to use a problem formulation with an ℓ1-norm involving real
variables excludes any quadratic programming-based optimization approach as well
as the powerful homotopic methods [OSB 00].
Conversely,
an algorithmic scheme such as iterative coordinate descent,
alternating minimizations with respect to every complex variable, proved particularly
effective for this problem [BOU 07b]. Indeed, if this strategy proves to be generally
inefﬁcient, it allows here several properties of criterion [6.9] to be exploited:
– each minimization has a low-computational-cost analytical solution;
– the solution being sparse, the number of non-zero components drops quickly
during the iterations. A selective scan of the coordinates can then be performed,
frequently updating the non-zero components;
– the non-differentiable part of the criterion being separable in the unknowns, this
strategy is guaranteed to converge to the minimal value of the criterion.
6.5.7. Results
Figure 6.4 illustrates the results obtained with the data in Figure 6.2. On the top
left, the “coarse” minimizer of criterion [6.9] is represented, showing a good
localization of the four main lines, with, however, a duplication of frequencies and
some artifacts of low amplitude. As can be seen on the top right, accounting for a
stage involving the correction of the amplitudes allows these artifacts and most
duplications to be removed. Finally, re-estimating duplicated frequencies further
improves frequency localization as well as the associated amplitudes, as it is shown
in the bottom picture. In each case, the regularization parameter is adjusted so that
the residual estimation is at the noise level, as described in section 6.4.1. It is
important to note that the results on the last two ﬁgures are not obtained from the
results of the ﬁrst one: the amplitude bias removal and then the reduction of the
frequency discretization error, both allow us to reﬁne the model and, for the same
level of estimation residual, to use a higher regularization parameter, thus removing
the artifacts.
6.6. Probabilistic approach for sparsity
The Bayesian interpretation of criterion [6.8] amounts to assuming, for the
penalization Φ(x) = 
p ϕ(|xp|), that the amplitudes xp are random iid variables
according to an a priori distribution p(xp) ∝exp

−λσ−2ϕ(|xp|)

[IDI 08]. The
strict decrease at 0 of this distribution, therefore the non-differentiability of the

160
Regularization and Bayesian Methods for Inverse Problems
criterion at any point containing zeros, is a necessary and sufﬁcient condition for the
minimizer,
corresponding
to
the
maximum
a
posteriori
estimator,
to
be
sparse [MOU 99]. Moreover, continuous a priori distributions do not explicitly
model sparsity, which corresponds to the presence of a probability mass at 0. This
section discusses the estimation of line spectra from a Bernoulli-Gaussian (BG)
model, which has been introduced for spike train deconvolution in geophysics
[KOR 82]. The latter model describes the presence or absence of a component in any
point by means of binary variables, and is therefore structurally sparse.
“Coarse” estimator ([6.9] minimizer)
With amplitude correction
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
Frequency (c/d)
Frequency (c/d)
With amplitude and frequency correction
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
Frequency (c/d)
Figure 6.4. Results of ℓ1 penalization for the spectral analysis of the data in Figure 6.2:
modulus of the Fourier spectrum of the data (dashed line), true lines (×), estimated lines (◦) and
estimation residual (solid line). (Top left) “Coarse” estimator. (Top right) Estimator obtained
by correcting the amplitudes. At the bottom, solution after correction of frequency duplication
6.6.1. Bernoulli–Gaussian model for spectral analysis
The BG model was originally introduced for real variables. In the case of spectral
analysis where the unknowns are complex, an extension has been proposed in
[DUB 95, DUB 96] with a circular BG model. In order to simplify the equations, the
real-valued formulation [6.5] has been used:
yn = a0 +

p
ap cos(2πfptn) + bp sin(2πfptn) + ϵn ⇐⇒y = Hs + ϵ
where H is the N × (2P + 1) matrix of coefﬁcients hn,0 = 1, hn,2p = cos(2πfptn)
and hn,2p+1 = sin(2πfptn) for p = 1, . . . , P and s = [a0,a1,b1, . . . , aP ,bP ]t gathers
the set of amplitudes. Let sp = [ap,bp]t denote the vector of amplitudes associated
with frequency fp for p ⩾1. Introducing Bernoulli variables, with a probability mass

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
161
at 0, allows us to explicitly account for sparsity: these variables equal 1 when the
corresponding amplitude is non-zero and 0 when it is zero. Thus, the BG model is
deﬁned in the form (q,s), where:
– q = [q0, . . . , qP ] is a sequence of iid Bernoulli variables: Pr(qp = 1) = γ, the a
priori probability of the presence of a line at frequency fp;
– for p ⩾1, conditionally to the presence of a line, the amplitudes ap and bp
are assumed to be independent of each other, with the same Gaussian distribution
N(0, σ2
1), such that p(sp | qp = 1) = g2(sp,σ2
1I2), where gN(x,Σ) represents the
N-dimensional centered Gaussian distribution, with covariance matrix Σ. The case
where amplitudes are zero is then written as p(sp | qp = 0) = δ2(s), the two-
dimensional delta function;
– in accordance with developments of section 6.2, it is important, in irregular
sampling, to systematically take a continuous component into account. Therefore,
q0 = 1 and p(a0) = g1(a0,σ2
1) is subsequently imposed.
The a priori BG distribution then takes the following form:
p(q,s) = Pr(q)p(s | q) = γNq(1 −γ)P −Nqg2P +1(s,Σs|q), with Nq =

p
qp,
where Σs|q is the diagonal matrix formed by the blocks of covariances σ2
1, qpσ2
1I2,
p = 1, . . . , P and using the convention δ2(s) = g2(s,0). For the concerned model, the
likelihood is Gaussian, centered in y −Hs, and with covariance matrix σ2IN. Bayes’
theorem gives then the a posteriori distribution p(q,s | y) ∝gN(y−Hs,σ2IN)p(q,s).
6.6.2. A structure adapted to the use of MCMC methods
The exploitation of p(q,s | y), mixing discrete and continuous random variables,
is a complex problem. Its maximization in (q,s) relates essentially to combinatorial
optimization and, for realistically sized problems, cannot be obtained in an exact
manner. Many works have considered suboptimal approaches, such as [KOR 82] for
spike trains deconvolution or [DUB 96] for spectral analysis. In the irregular
sampling case, however, the presence of multiple local modes in the likelihood (see
section 6.2) makes optimization even more difﬁcult. Indeed, the single most likely
replacement (SMLR) algorithm and its variants, proposed in the previously
mentioned works, can get stuck in these local modes, in a similar manner to the
iterative methods described in section 6.4. In addition, the probabilistic hierarchical
structure of the model, as well as the computing power now available, enable a more
complete exploration of p(q,s | y), by means of stochastic simulation methods. This
is the procedure adopted here, based on the works of Chen et al. [CHE 96], who were
the ﬁrst to use MCMC methods for this model.

162
Regularization and Bayesian Methods for Inverse Problems
If a sequence

q(m), s(m)
m=1,...,M of samples of random variables distributed
according to the distribution of interest p(q,s | y) is available, richer information can
be accessed than by optimization. In particular, the estimation of the a posteriori mean
of the Bernoulli parameters:
!q = 1
M

m
q(m) ∈[0,1]M
provides a measurement of the detection probability of a line at each point of the
spectrum [BOU 06]. Technically, the use of MCMC methods also avoids adjusting the
hyperparameters θ = [γ, σ, σ1] of the model, the estimation of which can be included
in the procedure (see section 6.6.4).
6.6.3. An extended BG model for improved accuracy
The main obstacle of this approach lies in its computational cost, which depends
crucially on the number of parameters involved, namely the size of the frequency grid.
This cost is much higher than that of the optimization approaches of section 6.5 and
a grid of similar size cannot be considered. Dublanchet’s works [DUB 96], for an
optimization-based approach with this same model, have introduced frequency offset
variables, in the form:
yn = a0 +

p
ap cos (2π(fp −dfp)tn) +bp sin (2π(fp −dfp)tn) +ϵn
⇐⇒
y = Hdfs + ϵ,
where
matrix
Hdf
now
contains
some
unknown
parameters,
with
offset
df = [df1, . . . , dfP ], with dfp ∈[0,fmax/P[. It should be noted that qp is now
interpreted as the probability of the presence of a line in the interval ]fp−1,fp] and
that at most one line is present in this interval. The resolution of the model, deﬁned as
its power of separation between two close lines, remains therefore limited to the size
of these intervals, that is fmax/P. Its frequency accuracy, on the other hand, is now
that of continuous-valued parameters.
6.6.4. Stochastic simulation and estimation
We consider the simulation of variables according to the a posteriori distribution
including the hyperparameters:
p(q,s,df,θ | y) ∝gN(y −Hdfs,σ2In) p(q,s | γ,σ2
1) p(df) p(γ) p(σ2) p(σ2
1),

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
163
where:
– the offsets df are also sampled, assuming uniform a priori distributions in
[0,fmax/P[;
– the Bernoulli parameter γ follows a uniform a priori distribution over [0,1];
– variances σ2 and σ2
1 follow non-informative a priori Jeffreys distributions
p(σ2) ∝σ−2, p(σ2
1) ∝σ−2
1 . Using these conjugate (degenerate) distributions, one
can easily generate samples of σ2 and σ2
1 according to their conditional a posteriori
distributions [ROB 96].
The conditional distributions in each of the parameters (qp,sp) are BG
distributions according to which it is also easy to generate samples. A possible
simulation scheme therefore relies on Gibbs sampling: random samples are generated
successively according to the distribution of each variable conditionally to all
others, ﬁxed at their last generated value. Metropolis–Hastings-based stages are
then incorporated for the generation of the parameters dfp, for which it is not
easy to exploit conditional distributions [ROB 96]. Details of implementation can
be found in [BOU 06]. After convergence, the estimation is performed from
samples(q(m),s(m),df (m),θ(m))m in the following manner:
i) the a posteriori mean of the Bernoulli sequence !q is estimated from (q(m))m.
Similarly, the hyperparameters are estimated as the mean !θ of θ(m);
ii) the mean !q is thresholded, in order to only retain the lines detected with a
probability greater than a threshold α, which depends on the desired conﬁdence level
(typically, α = 95 %). We then denote !qα the indicator sequence of the selected
components: !qα
p = 1 and !qp ⩾α, 0 otherwise;
iii) the means and variances of frequency offsets are computed from samples df (m)
where q(m) = !qα;
iv) the associated amplitudes are ﬁnally estimated:
they follow a multivariate
Gaussian distribution, whose mean and covariance matrix are analytically given from
the values of q = !qα and θ = !θ.
6.6.5. Results
Figure 6.5 on the left shows the results of the estimation on the data in Figure 6.2
obtained by assuming the noise level σ2 to be known, for P = 500 frequency
intervals. The four main lines are correctly detected, providing a quality similar to the
results from ℓ1 penalization and from SBR with the same informational conditions.
With respect to previous approaches, the estimation provides here signiﬁcant
additional information: detection probability and uncertainty on the values of the
detected frequencies and the associated amplitudes. This approach also allows a fully
unsupervised estimation including the estimation of σ2 in the procedure:
the

164
Regularization and Bayesian Methods for Inverse Problems
corresponding result on the ﬁgure to the right is a little less accurate, but remains
efﬁcient given the complexity of the problem.
With known σ2
With estimated σ2
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
100%
100%
100%
100%
30
31
32
33
34
35
36
37
0
0.2
0.4
0.6
91%
100%
100%
100%
Frequency (c/d)
Frequency (c/d)
Figure 6.5. Results of a Bernoulli-Gaussian modeling approach for spectral analysis of the
data of Figure 6.2. Each detected line is represented by a rectangle centered on the estimated
value and whose dimensions represent the standard deviation associated with the frequency
and the amplitude estimation. Above each line, the associated detection probability is indicated.
Modulus of the Fourier spectrum of the data (dashed line), true lines (×) and estimation residual
(solid line)
6.7. Conclusion
The identiﬁcation of multi-sinusoidal models from irregularly sampled data is a
complex problem from the informational perspective, in particular when signal
frequencies interfere with those of the possible periodic gaps in the temporal
coverage. Formalized in the Fourier domain, it corresponds to a spike train
deconvolution problem where the impulse response – the complex-valued spectral
window – may present strong side lobes. We have studied several types of approach
that can be described from the perspective of the sparse approximation of the data in
a dictionary of sinusoidal functions (or of complex exponentials) whose frequencies
are discretized on a grid. Greedy approaches traditionally used in this area are easily
defeated in the case where the spectral window generates false alarms that propagate
in the procedure. A more complete iterative algorithm based on SBR reduces this
problem by allowing the removal of components that were incorrectly detected over
the previous iterations. The standard alternative to greedy approaches by ℓ1
penalization only provides really satisfactory results if it is performed jointly with
additional reﬁnements, relative to the removal of amplitude bias and to a correction
of the duplicating effect of the estimated lines caused by the discretization step
(necessarily inaccurate) of the frequency axis. Finally, probabilistic modeling of
sparsity associated with an MCMC algorithm allows more comprehensive results to
be accessed, combining detection probabilities and uncertainties in the estimated
parameters, information of prime importance to the astrophysicist at the price of a
much higher computational cost. However, it should be noted that based on the data
example introduced at the beginning of this chapter, none of the methods ﬁnds the

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
165
low-amplitude line at approximately 31 c/d, except at the expense of multiple false
alarms.
We have also shown the limits of many of the tools developed in the framework
of sparse approximation, both at the level of theoretical solution characterizations
and of the associated computation methods. These tools have proved inappropriate
for this sparse estimation problem, which does not share the same purpose: the point
of interest here relates directly to the quality of the sparse solution !x, where sparse
approximation is more focused on the quality of the approximation of data W!x.
Moreover, the choice of a discretization of the space of unknowns as ﬁne as possible,
necessary in order to obtain a high-resolution estimator, constrains the dictionary to
an extremely correlated structure, and confers to the resulting estimation problem an
ill-posed nature.
At the astrophysical level, many recent or upcoming instrumental projects intend
to acquire high-quality measurements, for which multi-sinusoidal models are no
longer adapted. This is particularly the case of the COROT satellite for which the
data acquired from space present a gap-free temporal coverage over a very long
period (several months) and with very fast sampling, as well as a very low noise
level. Such time-series analysis then no longer poses the problems examined in this
chapter but raises new methodological questions, which require accounting for more
realistic models, for example describing ﬁnite duration and/or damped oscillations.
6.8. Bibliography
[ALL 94] ALLINEY S., RUZINSKY S.A., “An algorithm for the minimization of mixed l1
and l2 norms with application to Bayesian estimation”, IEEE Transactions on Signal
Processing, vol. 42, no. 3, pp. 618–627, March 1994.
[BLU 07] BLUMENSATH T., DAVIES M.E., On the difference between orthogonal matching
pursuit and orthogonal least squares, Research report, Edinburgh University, March 2007.
[BOU 06] BOURGUIGNON S., CARFANTAN H., “Spectral analysis of irregularly sampled data
using a Bernoulli-Gaussian model with free frequencies”, IEEE International Conference
on Acoustic, Speech and Signal Processing, Toulouse, May 2006.
[BOU 07a] BOURGUIGNON S., CARFANTAN H., “SparSpec:
a new method for ﬁtting
multiple sinusoids with irregularly sampled data”, Astronomy and Astrophysics, vol. 462,
pp. 379–387, January 2007.
[BOU 07b] BOURGUIGNON S., CARFANTAN H., IDIER J., “A sparsity-based method for the
estimation of spectral lines from irregularly sampled data”, IEEE Journal of Selected Topics
in Signal Processing, vol. 1, no. 4, pp. 575–585, December 2007.
[CAN 06] CANDÈS E.J., ROMBERG J.K., TAO T., “Stable signal recovery from incomplete
and inaccurate measurements”, Communications on Pure and Applied Mathematics,
vol. 59, no. 8, pp. 1207–1223, August 2006.

166
Regularization and Bayesian Methods for Inverse Problems
[CHE 96] CHENG Q.,
CHEN R.,
LI T.-H.,
“Simultaneous wavelet estimation and
deconvolution of reﬂection seismic signals”, IEEE Transactions on Geoscience and Remote
Sensing, vol. 34, pp. 377–384, March 1996.
[CIU 01] CIUCIU P., IDIER J., GIOVANNELLI J.-F., “Regularized estimation of mixed spectra
using a circular Gibbs-Markov model”, IEEE Transactions on Signal Processing, vol. 49,
no. 10, pp. 2201–2213, October 2001.
[DAU 04] DAUBECHIES I., DEFRISE M., DE MOL C., “An iterative thresholding algorithm
for linear inverse problems with a sparsity constraint”, Communications on Pure and
Applied Mathematics, vol. 57, no. 11, pp. 1413–1457, 2004.
[DEM 89] DEMOMENT G., “Image reconstruction and restoration: overview of common
estimation structures and problems”, IEEE Transactions on Acoustics, Speech and Signal
Processing, vol. ASSP-37, no. 12, pp. 2024–2036, December 1989.
[DEM 99] DEMOMENT G., IDIER J., “Approche bayésienne pour la résolution des problèmes
inverses en imagerie”, in BONNET M., (ed.), Problèmes inverses: de l’expérimentation à la
modélisation, Collection Arago, OFTA, Tec & Doc edition, Paris, vol. 22, pp. 59–77, 1999.
[DEM 02] DEMOMENT G., “Problèmes inverses en traitement du signal et de l’image”,
Journal de Physique IV, vol. 12, no. 1, pp. 3–34, March 2002.
[DUA 09] DUAN J., SOUSSEN C., BRIE D., et al., “A continuation approach to estimate
a solution path of mixed L2-L0 minimization problems”, Proceedings of the SPARS
Workshop, Saint-Malo, France, April 2009.
[DUB 95] DUBLANCHET F., DUVAUT P., IDIER J., et al., “EXPULSE COMPLEXE,
estimation bayésienne de sinusodes par déconvolution de la transformée de Fourier discrète
du signal”, Actes du 15e colloque GRETSI, Juan-les-Pins, France, pp. 37–40, September
1995.
[DUB 96] DUBLANCHET F., Contribution de la méthodologie bayésienne à l’analyse spectrale
de raies pures et à la goniométrie haute résolution, Doctoral Thesis, Paris-Sud University,
Orsay, October 1996.
[EYE 99] EYER L., BARTHOLDI P., “Variable stars: which Nyquist frequency?”, Astronomy
and Astrophysics Supplement Series, vol. 135, pp. 1–3, February 1999.
[FER 81] FERRAZ-MELLO S., “Estimation of periods from unequally spaced observations”,
Astronomical Journal, vol. 86, pp. 619–624, April 1981.
[FUC 04] FUCHS J.-J., “On sparse representations in arbitrary redundant bases”, IEEE
Transactions on Information Theory, vol. 50, no. 6, pp. 1341–1344, June 2004.
[GIO 01] GIOVANNELLI J.-F., IDIER J., “Bayesian interpretation of periodograms”, IEEE
Transactions on Signal Processing, vol. 49, no. 7, pp. 1988–1996, July 2001.
[GRA 73] GRAY D.F., DESIKACHARY K., “A new approach to periodogram analyses”,
Astrophysical Journal, vol. 181, pp. 523–530, April 1973.
[HÖG 74] HÖGBOM
J.A.,
“Aperture
synthesis
with
a
non-regular
distribution
of
interferometer baselines”, Astronomy and Astrophysics Supplement, vol. 15, pp. 417–426,
1974.

Line Spectra Estimation for Irregularly Sampled Signals in Astrophysics
167
[IDI 08] IDIER J., (ed.), Bayesian Approch to Inverse Problems, ISTE, London and John Wiley
& Sons, New York, 2008.
[KOR 82] KORMYLO J.J., MENDEL J.M., “Maximum-likelihood detection and estimation
of Bernoulli-Gaussian processes”, IEEE Transactions on Information Theory, vol. 28,
pp. 482–488, 1982.
[LOM 76] LOMB N.R., “Least-squares frequency analysis of unequally spaced data”,
Astrophysics and Space Science, vol. 39, pp. 447–462, February 1976.
[MAL 93] MALLAT S., ZHANG Z., “Matching pursuits with time-frequency dictionaries”,
IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397–3415, December 1993.
[MAL 09] MALLAT S., A Wavelet Tour of Signal Processing: The Sparse Way, Elsevier/
Academic Press, Amsterdam, 2009.
[MIL 02] MILLER A.J., Subset Selection in Regression, 2nd ed., Chapman and Hall, London,
2002.
[MOU 99] MOULIN P., LIU J., “Analysis of multiresolution image denoising schemes using
generalized Gaussian and complexity priors”, IEEE Transactions on Information Theory,
vol. 45, pp. 909–919, April 1999.
[OSB 00] OSBORNE M.R., PRESNELL B., TURLACH B.A., “A new approach to variable
selection in least squares problems”, IMA Journal of Numerical Analysis, vol. 20, no. 3,
pp. 389–403, 2000.
[ROB 87] ROBERTS D.H., LEHAR J., DREHER J.W., “Time series analysis with Clean – part
one – derivation of a spectrum”, Astronomical Journal, vol. 93, pp. 968989, April 1987.
[ROB 96] ROBERT C.P., Méthodes de Monte-Carlo par chaînes de Markov, Economica, Paris,
1996.
[SAC 98] SACCHI M.D., ULRYCH T.J., WALKER C.J., “Interpolation and extrapolation using
a high-resolution discrete Fourier transform”, IEEE Transactions on Signal Processing,
vol. 46, no. 1, pp. 31–38, January 1998.
[SOU 11] SOUSSEN C.,
IDIER J.,
BRIE D.,
DUAN J.,
“From Bernoulli-Gaussian
deconvolution to sparse signal restoration”, IEEE Transactions on Signal Processing,
vol. 59, no. 10, pp. 4572–4584, October 2011.
[TAY 79] TAYLOR H., BANKS S., MCCOY F., “Deconvolution with the l1 norm”, Geophysics,
vol. 44, no. 1, pp. 39–52, January 1979.
[TRO 04] TROPP. J.A., “Greed is good: algorithmic results for sparse approximation”, IEEE
Transactions on Information Theory, vol. 50, no. 10, pp. 2231–2242, October 2004.
[TRO 06] TROPP J.A., “Just relax: convex programming methods for identifying sparse
signals”, IEEE Transactions on Information Theory, vol. 52, no. 3, pp. 1030–1051, March
2006.


7
Joint Detection-Estimation
in Functional MRI
7.1. Introduction to functional neuroimaging
The objective of functional neuroimaging is the characterization of the brain in
action. Its most conventional use consists of assigning a task to an individual while
simultaneously measuring a signal induced by brain activity.
Several investigation modalities allow the neuroscientist to probe the operation of
the brain both in healthy volunteers and in patients, whether the pathology is
neurodegenerative (Alzheimer), psychiatric (schizophrenia) or neurological (stroke,
epilepsy). Without going into detail of a taxonomy of imaging modalities, it is
noteworthy to give a general overview (see Figure 7.1) and distinguish those that
allow two fundamental questions to be answered in a simpliﬁed form: “where” and
“when” do brain processes occur? In fact, the brain is a complex structure where
specialization and functional integration coexist so well that it would be imprudent to
picture cognitive functions as very localized specialized modules. Among the tools of
functional neuroimaging, we brieﬂy summarize the following:
– functional magnetic resonance imaging (fMRI) consists of measuring in a
non-invasive manner, BOLD signals 1 which reﬂect the rate of oxygenation of
blood in the brain [OGA 90]. Through a neurovascular response mechanism, the
ﬂow of oxygenated blood increases in the regions whose neuronal metabolism
increases, in much more signiﬁcant fashion than necessary. Because of its good spatial
resolution (around the mm), by using this method, it is possible to identify the brain
regions speciﬁcally involved in a given task. Since the 2000s and the emergence of
event-related fMRI, which consists of presenting stimuli in an isolated manner in
Chapter written by Philippe CIUCIU, Florence FORBES, Thomas VINCENT and LotﬁCHAARI.
1. Blood Oxygenated Level-Dependent.

170
Regularization and Bayesian Methods for Inverse Problems
time, it is possible to recover the dynamics of the BOLD signal with a temporal
resolution of the order of the second. This is still slower than the dynamics of
cognitive processes (a few tens to hundreds of milliseconds). The choice of fMRI
as an investigation method is multiple: on the one hand, with the advent of very high-
ﬁeld imagers 2, of parallel imaging [CHA 11b, PRU 99], and compressive sampling
techniques [BOY 12, LUS 07], signiﬁcant gains are still expected in spatio-temporal
resolution. On the other hand, the emergence of new contrast agents (CEST, USPIO)
could help to achieve a molecular resolution of the order of that attained in PET;
Figure 7.1. Comparison of different brain imaging modalities in terms of spatio-temporal
resolution and invasiveness degree. For a color version of the ﬁgure, see
www.iste.co.uk/giovannelli/regularization.zip
– positron emission tomography (PET) consists of measuring the modiﬁcations
of blood ﬂow through a radioactive tracer that must ﬁrst be injected intravenously.
The diffusion of the tracer and the modulation of the blood ﬂow being relatively
slow phenomena, this technique does not give access to the dynamics of neural
mechanisms. Its invasiveness and the low temporal resolution of the observed
biological phenomena make it an increasingly less used technique nowadays in
functional imaging, supplanted by fMRI;
– electroencephalography (EEG) was the ﬁrst non-invasive neuroimaging method,
developed in 1929, by the neurologist Hans Berger. Unlike previous methods, it is a
measure of the electrical activity. EEG is relatively spatially inaccurate but it does
offer a good temporal resolution, limited only by the speed of the measurement
electronics (5 kHz) [SWA 98]. The conventional approach consists of measuring
evoked potentials: repeating the same stimulation a large number of times, it is
possible to bring forward positive and negative waves characteristic of the different
stages of the information processing procedure (for example, evoked potentials N100,
P300, etc.) [HAL 98];
2. MRI at 11.7 T expected end of 2015 at NeuroSpin.

Joint Detection-Estimation in fMRI
171
– magnetoencephalography (MEG) provides information relatively similar to the
EEG, but it measures magnetic ﬁelds induced by the activity of the brain. The
signiﬁcance of MEG lies in the fact that, unlike electric ﬁelds, magnetic ﬁelds are
virtually undistorted by their passage through organic tissues (notably the interface
between the cerebrospinal ﬂuid and the skull). Just like with EEGs, it is possible
with the solution of a linear inverse problem to reconstruct the sources of the
electromagnetic signal on the cortical surface. This helps to identify with an accuracy
of half a centimeter the regions where evoked ﬁelds are issued [SEK 05]. These spatial
localization techniques have expanded with the help of regularization tools [BAI 97]
and the advent of efﬁcient optimization algorithms [BAU 11, GRA 11].
A key step in the choice of imaging modality lies in the neuroscientiﬁc question
which must be answered. If the objective is to ensure the temporal decoding of
neuronal activity, the best choice seems to be MEG or EEG by default. However, if
spatial localization is being prioritized, fMRI seems more sensible. Beyond this
choice, obtaining a reliable description of the neuronal activity can be improved, ﬁrst
in designing more efﬁcient measurement tools (imagers, sensors), then by improving
analysis
methods
to
correct
inaccuracies
or
measurement
artifacts
(robust
reconstruction,
denoising,
distortions,
etc.) or by considering more realistic
mathematical models of underlying physiological processes.
This chapter falls precisely under the scope of restoration and of object detection
problematics related to brain activity from noisy fMRI signals, by exposing a
Bayesian approach to the joint detection-estimation (JDE) of cerebral activity, that is
for solving a spatio-temporal inverse problem (3D+time). The JDE approach calls
into question the conventional paradigm of activations detections in fMRI by
estimating the impulse response of the neurovascular system rather than assuming
that it is known and constant through the whole brain.
Afterwards, we present in section 7.2 the problematics of JDE, then inversion is
addressed in a Bayesian framework in section 7.3. Two alternative inference patterns,
that is, stochastic and variational, are then presented in section 7.4 and 7.5,
respectively. A comparison of the results provided by these two approaches, both
with simulated and real data, is achieved in section 7.6, thus initiating a discussion
regarding the contributions of each formalism at the end of the chapter.
7.2. Joint detection-estimation of brain activity
7.2.1. Detection and estimation: two interdependent issues
The objective of the intra-subject fMRI analysis is two-fold: on the one hand, it
aims to locate the neuronal activity evoked by cognitive tasks of interest during an
experimental paradigm, that is to say, to detect voxels activated by stimulation, and

172
Regularization and Bayesian Methods for Inverse Problems
on the other hand, it aims to estimate the dynamics of the neurovascular system
which establishes the link between stimulation and measured BOLD signals, by
characterizing the hemodynamic response function (HRF) whose canonical form is
described in Figure 7.2. The objective is therefore to create activation maps as well as
a local description of the response induced by stimulation, to better understand
neurovascular coupling but also to contribute to a dynamic pattern of brain
functioning, by considering, for example, the spatial variability of the time taken to
reach the peak of the response or by attempting to infer temporal scheduling
relationships between activation states.
The usual approach to address these two issues is based on multiple regression,
still known as the general linear model (GLM) [FRI 95]. In this context, an
experimental design matrix is built, that is to say, a set of regressors representing the
activation canonical time courses. These time courses are built as the convolution of
canonical HRF (see Figure 7.2) with the binary signal deducted from each
experimental condition present in the paradigm, that is from the signal encoding
stimuli occurrences speciﬁc to this condition (for example, presentation of a known
face): each “1” indicates an instance of the stimulus. The detection is then performed
voxel-wise by estimating the GLM parameters in the maximum likelihood sense, and
sufﬁcient normalized statistics (e.g. Student test) are then calculated in order to detect
activations (difference in activity or contrast between two experimental conditions as
the activity in response to familiar versus unfamiliar faces) by rejecting the zero
hypothesis, according to which contrast is zero with a speciﬁed false positive rate.
In the GLM framework, hemodynamics estimation can only be carried out in
regions detected as activated. However, this required detection, if based on a
predeﬁned canonical hemodynamic model, will in turn produce biased estimation. It
appears that these two tasks are highly interdependent:
a good HRF model is
necessary to build the linear model and to correctly detect activations while a reliable
estimation of the HRF is only available in correctly detected areas. Although it is
possible
to
inject
some
temporal
ﬂexibility
within
the
linear
model
[CAS 08, HEN 00] by adding regressors (for example, functions base) [WOO 04a],
derivation and dispersion of the canonical HRF [HEN 00], ﬁnite impulse response
(FIR) model [GOU 00], etc.), it is customary in GLM approaches to use only a
hemodynamic model ﬁxed for the whole brain, even though this is not consistent
with our knowledge of the BOLD signal (see section 7.2.2). Furthermore, estimating
the HRF makes sense only in regions activated by a speciﬁc task, which induces a
preselection bias and does not reﬂect the hemodynamic activity of the whole brain.
An approach to the voxel-wise HRF estimation [CIU 03, MAR 03] sometimes allows
the activation states to be inferred in a second phase. However, this methodology
appears to be not very robust on occasion because of nuisance and noise components
prominent in the BOLD signal [CIU 04].

Joint Detection-Estimation in fMRI
173
Figure 7.2. Canonical HRF and its main characteristics. This response performs a delay as
well as a slow variation transformation of the input stimulation signal. The initial dip or early
depletion is not always observed and reﬂects an immediate extraction of oxygen due to neuronal
excitation
The idea of performing a JDE has originally appeared in [MAK 05, MAK 08],
where the problem of the reproducibility of the HRF is treated spatially by adopting a
3D brain parcellation (see Figure 7.3). Thus the objective is to bring out a spatial
scale where this hemodynamic ﬁlter, embodied by the HRF, can be locally assumed
as constant, so as to make its estimation the most robust possible, unlike voxel-wise
attempts [DEP 08]. It is now necessary to formulate the physiological hypotheses
underlying the retained model.
7.2.2. Hemodynamics physiological hypotheses
The signiﬁcant characteristics of the BOLD signal synthesized here consists of all
the a priori knowledge about the observed signal [LOG 01].
H1) The signal is very noisy: variations in hemodynamic activity evoked by the
paradigm represent only a change in the order of 5% with respect to the baseline at 3
Tesla.
H2) There exists a spatial correlation of BOLD signals, due to observed
physiological phenomena and network neuronal functioning. These phenomena have
a wider spatial support than the spatial resolution of the data.

174
Regularization and Bayesian Methods for Inverse Problems
Figure 7.3. Coronal,
sagittal (on top),
axial and 3D entire brain (bottom) views
of an intra-subject parcellation, with a color code for a parcel, superposed to the anatomical
MRI in grayscale as well as to the cortical surface (gray mesh). For a color version of the ﬁgure,
see www.iste.co.uk/giovannelli/regularization.zip
H3) The signal presents a short and long-range temporal correlation. The
short-range or high-frequency component comes naturally from the duration of
physiological phenomena that exceed the sampling period. Among these, some are of
interest as related to the paradigm, others reﬂect physiological regulations that are
difﬁcult to explicitly model, due to their unexpected nature. Therefore, they fall
under the scope of noise modeling in the form of a temporally autocorrelated random
process [WOO 01]. Long-range correlation comes partly from artifacts: respiratory
and above all cardiac pulsations have higher frequencies than those of the
measurement of the signal. The Shannon–Nyquist condition is not satisﬁed, thus
inducing spectral aliasing of cardiac and respiratory components in the low
frequencies of the acquired signals. In its very low frequency part, the long range
component is also associated with intrinsic cerebral activity of interest that can be
highlighted in resting-state fMRI in rest functional networks (for example, the default
mode) [CIU 12, CIU 14, LIN 01, ZAR 97].
H4) If neurovascular coupling can be physiologically described by the
neuro-astro-vascular unit, the origin of the measured BOLD signal remains unclear
and depends on variations of several factors:
blood volume, blood ﬂow and
deoxyhemoglobin concentration,
without being able to clearly establish the
contributions of each one of them [EKS 10, LOG 01].

Joint Detection-Estimation in fMRI
175
H5) Hemodynamics vary spatially, both within a same subject [HAN 04, MIE 00]
and between subjects [AGU 98, BUC 98]. Inter-subject variations are even more
pronounced when populations that can differ in age [RIC 03], in pathology or in
therapeutics [DES 99] are considered.
The balloon model [BUX 98] and its extensions [BUX 04, RIE 04] establish the
ﬁnest description of the neurovascular coupling from a physiological point of view,
with respect to hypotheses H1-H5. Governed by systems of nonlinear differential
equations with unknown parameters, these models suffer from identiﬁability
problems and proved to be difﬁcult to put into practice in fast multicondition
event-related paradigms. In this chapter we consider a convolutive model, easier to
handle but also able to cover a wider range of paradigms. Before describing this
alternative approach, we ﬁrst recall the main temporal features of the BOLD signal.
7.2.3. Spatially variable convolutive model
Stationarity: The BOLD response is assumed to be stationary in time, that is not
varying in different trials of the same stimulus. If given x(t), a stimulus presented at
time t, and h(x(t)), the evoked hemodynamic response, then stationarity indicates:
h(x(t + Δt)) = h(x(t)) for Δt, an interstimulus interval, such as Δt > 3 s.
Conversely, response saturation phenomena are involved and the response becomes
sublinear [BUC 98, DAL 97].
From a physiological point of view, this hypothesis is justiﬁed by the construction
of the paradigm where care is taken to pseudo-randomly distribute trials to prevent
learning or anticipating and limit the duration of the experiment to avoid fatigue in
the subject. A second justiﬁcation comes from the raised cognitive question: the
acquisition of a reproducible response across the trials.
Linearity: The vascular system is assumed to be linear which amounts to assume on
the one hand, that h(ax(t)) = ah(x(t)) for an amplitude a > 0 of the stimulus, and on
the other hand, that it is additive with respect to experimental conditions, noted here
x1 and x2: h(x1(t)+x2(t)) = h(x1(t))+h(x2(t)). However, a number of saturation
phenomena exist. Thus, for a doubling of the intensity of sound stimulus, the response
in the auditory cortex only doubles in a restricted volume range in decibels.
These hypotheses lead to a convolutional system, which is, in addition, justiﬁed
by the choice of a simple model but respecting the hypotheses H1-H4. In order to
ensure a certain ﬂexibility, a linear FIR ﬁlter is retained so as to formulate its
identiﬁcation as a nonparametric estimation problem. On the other hand, in order to
account for H5 while ensuring a parsimonious character and therefore a larger

176
Regularization and Bayesian Methods for Inverse Problems
robustness
of
estimation,
we
have
proposed
the
following
restrictions
[MAK 05, MAK 08]: “a single HRF shape by region characterizes the hemodynamic
system, local variations of activity at the voxel level are reduced to amplitude
modulations of the HRF only. The model therefore decouples the form of the
hemodynamic ﬁlter of its amplitude, inducing a bilinear system”.
n ∈
∗
N = {1, . . . , N}
Scans index
j ∈
∗
J
Voxels index
RT
Repetition time: data temporal resolution
= {Vj}j=1:J
Voxels set. For whole brain analysis,
J ≈5.104. For regional analysis, J ≈500.
m ∈
∗
M
Experimental conditions index
yj = (yj,n)n=1:N ∈
N
fMRI signal measured at voxel Vj
bj = (bj,n)n=1:N ∈
N
Noise measurement at voxel Vj
Y = (yj)j=1:J
N × J acquired data matrix for
ℓj = (ℓj,n)n=1:N ∈
Q
Low frequency drift coefﬁcients at Vj
L = (ℓj)j=1:J
Q × N drift coefﬁcients matrix for
qm = (qm
j )j=1:J
Activation states for
associated with the condition m
Q = (qm)m=1:M
State matrix for all conditions and
am = (am
j )j=1:J
NRL for
associated with the condition m
A = (am)m=1:M
NRL matrix for all conditions and
xm = (xdt
m)dt=0:Δt: N×T R
Δt
Sequence encoding the arrival times of stimuli m sampled on a
resolution grid Δt
hγ = (hdΔt
γ
)d=0:D
HRF with Δt the sampling period
Table 7.1. Notation table
We are now introducing the notations used subsequently in the text. Table 7.1
indicates the variables associated with quantities of interest using the following
conventions: vectors 3 and matrices are noted in lowercase and uppercase bold,
respectively (e.g. x and X). Scalars are noted in lowercase regular characters and
sets with double-bars (e.g.
), transposition is symbolized by t.
7.2.4. Regional generative model
The approach assumes a prior partitioning of data, that is to say, parcellation
guaranteeing hemodynamic homogeneity within each parcel. Thus assuming the
brain is previously subdivided into P
=
(Pγ)γ=1:Γ parcels, each having
homogeneous functional properties from the perspective of hemodynamics. The
proposed model will therefore be inferred parcel-wise, each one independent from
the others.
3. Always in column.

Joint Detection-Estimation in fMRI
177
Each parcel Pγ is deﬁned by a connected set of voxels
γ. It has a unique HRF
hγ characteristic of Pγ, which is thus considered identical for all voxels. Activation
levels A are unique to each voxel Vj and each experimental condition m (see BOLD
response stationarity hypothesis). Hence, in each voxel Vj ∈
γ, the following
generative model can be derived, shown in Figure 7.4:
yj = Sjhγ + P ℓj + bj,
with
Sj =
M

m=1
am
j Xm
[7.1]
where Sjhγ is the sum of the stimulus components induced from the BOLD signal.
Matrix Xm = (xn−dΔt
m
)d=0:D
n=1:N is binary, of dimensions N × (D + 1), and code
occurrences of stimuli of the m-th condition. The parameter Δt is the sampling
period of the unknown HRF hγ in Pγ. The scalar am
j is the amplitude of the response
at voxel Vj for the condition m. These weights model the transition between
stimulation
and
vascular
response.
They
can
therefore
be
considered
as
“pre-vascular”. In addition,
since it is admitted that neuronal excitation is
time-locked to the occurrence of the stimulus and at the origin of the vascular
response, amplitudes A are therefore referred to as “neural response levels” (NRL).
Matrix P is an orthogonal basis of low frequency functions of size N × Q. At each
voxel, a weight vector ℓj is attached in order to estimate the drift. The set of these
vectors within Pγ is grouped in L. Finally, bj is the noise vector in Vj whose
structure is detailed in section 7.3.1.




   
h

	









 

J



   

	


J

J

J

J
 J

J



J
j
1


Pl
PlJ
b
bJ
y


y

J

 



	





	

J

 


Figure 7.4. Parcel-wise model of the BOLD signal. The size of each parcel Pγ is typically of
some hundreds of voxels. The number M of experimental conditions of a paradigm usually
varies between 1 and 5; in this illustration, M = 2. This model supports asynchronous
paradigms for which the arrival times of the stimuli do not necessarily correspond to the
acquisition times. The NRL (a1
j,a2
j) are voxel-speciﬁc whereas the HRF hγ is constant for
the parcel Pγ but varies from one parcel to another (not shown). It can be sampled at a rate of
0.5 s for a duration ranging typically from 20 s to 25 s (for example D = 51). The coefﬁcients
ℓj usually focus on a few composantes (Q = 4)

178
Regularization and Bayesian Methods for Inverse Problems
7.3. Bayesian approach
Bayesian formalism requires embedding the problem in a probabilistic framework
and the main object of interest is the joint a posteriori density of all the unknown
parameters knowing observations Y . This latter allows the proposal of estimators of
the unknowns, particularly for (hγ,A) that are the most relevant from a cognitive
perspective. It is deﬁned by:
p(hγ,A,L,Θ | Y ) ∝p(Y | hγ,A,L,θ0) p(A | θA) p(hγ | θh)p(L | θℓ) p(Θ) [7.2]
where Θ gathers all the hyperparameters of the model: θ0 contains those related to the
noise model and θx concerns those related to the unknown x ∈{hγ,A,L}. This joint
density requires the expression of the likelihood term as well as the a priori deﬁnition
of the modeled variables.
7.3.1. Likelihood
The likelihood is the density of the observations for a given set of parameters of
the concerned model. It quantiﬁes the ability of the model to ﬁt the observed data and
therefore is directly dependent on the noise model hypotheses. Even if the structure
of the noise is spatially correlated [WOO 04b], this dependency is negligible and
fMRI
time-series
are
considered
spatially
independent
but
non-identically
distributed. Moreover, given the nature of data acquisition (see H2), noise is
supposed to be temporally autocorrelated and in this case, an AR(1) process is
considered to be as the following [WOO 01]: bj ∼N(0,Γj) with Γj = σ2
j Λ−1
j
and
where Λj is tridiagonal symmetric and depends on the AR parameter ρj [MAK 08]:
(Λj)1,1
=
(Λj)N,N
=
1, (Λj)p,p
=
1 + ρ2
j for p
=
2
:
N −1 and
(Λj)p+1,p = (Λj)p,p+1 = −ρj for p = 1 : N −1. These parameters are supposed to
be variable from a voxel Vj
to another given their dependence to tissue
[WOO 04a, PEN 07]. The likelihood is then written as:
p(Y | hγ,A,L,θ0) ∝
Jγ

j=1
|Γj|−1/2 exp
"
−1
2yt
jΓ−1
j yj
#
[7.3]
where θ0 = (θ0,j)j=1:Jγ, θ0,j = (ρj,σ2
j ) and yj = yj −P ℓj −Sjhγ.
7.3.2. A priori distributions
In the Bayesian approach, a priori distributions are introduced for the unknown
(A, hγ, L) and for the hyperparameters Θ.

Joint Detection-Estimation in fMRI
179
7.3.2.1. Hemodynamic response function (HRF)
Following [CIU 03, MAR 03], an HRF with smooth variations is expected, its
a priori density is a multivariate Gaussian distribution whose variance–covariance
matrix expresses a constraint on the second derivative: hγ ∼N(0,vhR) with
R = (Dt
2D2)−1. The aim is to penalize large slope variations. Boundary constraints
are also introduced in the form h0 = hDΔt = 0. The a priori retained for variance vh
corresponds to the Jeffreys a priori: p(vh) = v−1/2
h
. On the sole basis of this, in an
inactive parcel where the signal of interest is close to zero, it can be derived that the
estimated hemodynamic response will be of very low amplitude, and therefore also
its slope. Thus, it will have a strong likelihood. Several solutions are conceivable to
avoid in the case of inactive signals the estimation of an unrealistic HRF that could
subsequently lead to a false activation detection. One of them consists of replacing
the mean Gaussian vector by imposing it to a shape similar to the canonical
hemodynamics ﬁlter. Another consists of detecting the active parcels after inference,
that is to say, for which at least one experimental condition is relevant in the sense
that it delivers signiﬁcant evoked response or positive NRLs [BAK 12].
7.3.2.2. Neural response levels (NRL)
In accordance with the principle of maximum entropy [ROB 07, p. 109], the
independence
of
the
NRLs
is
postulated
between
conditions:
p(A | θa)
=

m p(am | θm) with θa
=
(θm)m=1:M grouping the set of
hyperparameters for the m-th condition. Mixture models are introduced to segment
activated voxels from non-activated voxels. Given qm
j is the allocation variables that
encode the activated (qm
j
= 1) or non-activated (qm
j
= 0) state for the condition m
within
voxel
Vj.
NRLs
remain
independent
conditionally
on
qm:
p(am | qm,θm) = 
j p(am
j | qm
j ,θm).
Spatial mixture models (SMM) introduced in [VIN 10] allow accounting for a
certain spatial correlation between neighboring voxels in order to facilitate the
detection of clusters of activation rather than isolated voxels. In SMMs, the mixture
weights are implicit and controlled only by the local interaction relationship between
latent variables. The marginal distribution of the NRLs, not factorable over voxels, is
written as follows:
p(am | θm) =

qm∈{0,1}Jγ
Pr(qm | θm)
Jγ

j=1
p(am
j | qm
j ,θm).
[7.4]
Fortunately, as we will see in section 7.4, its explicit expression is not necessary
since only conditional distributions are useful in the sampling scheme.

180
Regularization and Bayesian Methods for Inverse Problems
Spatial correlation is directly taken into account in the activation probability by
means of a hidden Ising ﬁeld 4 on the variables qm, according to previous works
[HIG 98, SMI 03]. Here, the a priori ﬁeld on qm is expressed in the form:
Pr(qm | βm) = Z(βm)−1 exp

βmU(qm)

where U(qm) =

j∼k
I(qm
j = qm
k ) [7.5]
and I(A) = 1 if A true and I(A) = 0 otherwise. The notation j ∼k means that the
sum extends over all pairs (Vj,Vk) of neighboring voxels. The neighborhood system
can be 3D in the cerebral volume intersecting parcel Pγ or 2D along the cortical
surface. In this chapter, we only discuss the 3D case using 6-connectivity. The
extensions to 18 and 26 neighbors are direct. In [7.5], we do not consider external
ﬁelds in order not to promote an a priori state. Nevertheless, previous works have
showed that anatomical information could be modeled through an external ﬁeld such
that to increase the likelihood of activation (class 1) in gray matter [SMI 03]. The
parameter βm > 0 controls the level of spatial regularization: a large value of βm
associates high probabilities to homogeneous conﬁgurations, that is to say, containing
voxels of the same class. It should be noted that activation patterns within a parcel Pγ
are likely to vary from a condition m to the other. That is why different parameters
βm are considered. The partition function (FP) Z(·) of the Markov ﬁeld is written as:
Z(βm) =

qm∈{0,1}Jγ
exp

βmU(qm)

[7.6]
and ensures the normalization of the probability Pr(qm | βm). In the following, we
assume that (am
j | qm
j
= i) ∼N(μi,m,vi,m), for i = 0,1. We impose μ0,m = 0 for
the average of NRLs in inactive voxels leading to the hyperparameters vector θm =
[v0,m,μ1,m,v1,m,βm] for each condition m.
We should observe that a similar formulation such as Bernoulli–Gaussian has
also been proposed in fMRI in [SMI 03]. This situation corresponds to the case
of degenerate mixture, that is to say, v0,m = 0. However, this formulation is too
coarse, as different conﬁgurations of activation can appear in the current parcel and
the mixture parameters θm for the condition m are supposed to adapt themselves to
this latter set:
– in the case where all voxels are activated as a result of the condition m, the
proposed model is too rich and therefore the estimation of v0,m tends toward 0. Once
again, the BG model reappears without having imposed it;
– in the case where all voxels are inactivated, the estimate of μ1,m is close to 0
and the two mixture classes are superimposed. There again, the model is too rich, and
4. It should be noted that if the aim is to manage deactivations, 3-class Potts ﬁelds are substituted to the
Ising ﬁelds (see [RIS 11] for details).

Joint Detection-Estimation in fMRI
181
the approach developed in [BAK 12] for the automatic selection of relevant conditions
provides an effective solution to this problem. Considering a BG model would not add
anything in this case, because it would also be redundant;
– in the intermediate case where only a part of the voxels is activated in response
to the condition m, the proposed mixture model can adapt itself to heterogeneous
conﬁgurations and leads to consider as inactive weakly activated voxels with regard to
the others present in the parcel. It is in this kind of conﬁgurations that the BG model
is questioned because it can lead to false positives. The introduction of a variance
v0,m ̸= 0 therefore helps us not to bring the estimation μ1,m close to 0 by excluding
activated voxels (those whose evoked activity is too low);
– in the presence of light deactivations 5, that is negative NRLs, the BG model is
also considered in default and there again, the proposed formulation, more ﬂexible,
adapts itself to this conﬁguration provided that the number of deactivations and their
amplitude is low faced with those of activations, in order to keep μ1,m > 0;
– richer three-class mixture models and considering support distributions in
+
or
−for NRLs of activated and deactivated voxels have been successfully tested
in [MAK 08] but the induced numerical complexity does not easily consider the
introduction of spatial models with an unsupervised estimation of the parameters.
It is interesting to observe that the spatial regularization introduced in A is
nonquadratic due to the introduction of a compound model of (A,Q); it therefore
enables raising the activation boundaries on the sulcus walls within the cortex, in a
simpler manner than through the use of convex nonquadratic regularization directly
applied to A. The reason is due to the fact that the automatic estimation of the
regularization level βm does only involve the a priori Ising or Potts ﬁeld whose
partition function calculation can be tabulated in advance because the variables Q are
hidden: they are therefore not intervening in the observation model [7.1]. Moreover,
the regularization remains separable through the experimental conditions, whereas a
direct regularization on each vector am would induce nonseparability due to the
shape of the observation model.
7.3.2.3. Mixture hyperparameters
We consider a priori conjugate distributions for variances v0,m and v1,m, that is
to say, inverse-gamma distributions, IG(av0, bv0) and IG(av1, bv1), identical for all
conditions m. The adjustment of the meta-hyperparameters (av0,1, bv0,1) must be
carried out by taking care to be the least informative as possible. Besides the fact that
the a posteriori conditional distribution remains conjugate inverse-gamma, the
signiﬁcance of this choice lies in the fact that it remains proper. Thus, sampling
conditional a posteriori distributions of variances v·,m is still possible even when one
5. Well known phenomena brought forward in the functional network of the mode by default [CIU 12,
FOX 07, GRE 03], where the level of activity decreases as a result of the action of a stimulus.

182
Regularization and Bayesian Methods for Inverse Problems
of the mixture classes is empty or consists of a single element, in contrast to the
situation generated by the use of non-informative Jeffreys a priori, that is of a
distribution p(v0,m) = v−1/2
0,m . For the same reasons, a proper a priori N(aμ1, bμ1) is
retained for μ1,m. The choices of constants (aμ1, bμ1) are done to express diffuse
densities, that is slightly informative.
7.3.2.4. Noise and derivatives
The parameters of noise and derivative, θ0 and L respectively, are supposed to be
spatially independent: p(θ0,L | vℓ) = 2
j p(θ0,j) p(ℓj | vℓ) and without a priori
information,
the
following
is
chosen:
ℓj
∼
N(0,vℓIQ) 6
and
p(ρj,σ2
j ) = σ−1
j I(|ρj| < 1) in order to ensure the stability of the AR(1) process for
the noise [KAY 88]. As with vh, we choose a non-informative Jeffreys a priori for
vℓ: p(vℓ) = v−1/2
ℓ
.
7.3.3. A posteriori distribution
From the equations [7.2]–[7.3], and deﬁned a priori distributions, we get the
a posteriori distribution:
p(hγ,A,L,Q,Θ | Y ) ∝p(Y | hγ,A,L,θ0) p(A,Q | θA) p(hγ | vh)
p(L | vℓ) p(θ0) p(θA) p(vh) p(vℓ)
that is developed as follows:
p(hγ,A,L,Q,Θ | Y ) ∝v−D/2
h
v−JγQ/2
ℓ
Jγ

j=1
(1 −ρ2
j)1/2σ−N−1
j
I(|ρj| < 1)
exp
⎛
⎝−ht
γR−1hγ
2vh
−
Jγ

j=1

yt
jΛjyj
2σ2
j
+ ∥ℓj∥2
2vℓ
⎞
⎠
[7.7]
M

m=1
p(am | qm, θm)p(qm, θm)
It appears in [7.7] that this a posteriori distribution is speciﬁc to Pγ through the
HRF hγ. This density is however too complex to allow the analytical calculation of
an estimator. Consequently, we exploit in section 7.4 stochastic simulation tools, still
called Markov Chain Monte-Carlo (MCMC) methods to simulate samples from [7.7].
A variational alternative is then presented in section 7.5.
6. Where IQ is the size Q identity matrix.

Joint Detection-Estimation in fMRI
183
7.4. Scheme for stochastic MCMC inference
The inference scheme relies on a hybrid Gibbs-Metropolis sampler in which
conditional a posteriori distributions are sampled in turn, either directly (Gibbs) or
using an instrumental distribution (Metropolis-Hastings). The algorithm is detailed in
[VIN 10, Table I]. After convergence of the Markov chain, the quantities of interest
are then estimated with regard to the a posteriori (PM for Posterior Mean) as
follows: ∀x ∈{hγ,A,Θ}: xMP = (Tc −T0)−1 Tc
t=T0+1 x(t), where T0 deﬁnes the
warming period and Tc, the convergence number of iterations. For detection, we use
the
estimator
of
the
marginal
maximum
a
posteriori:
(qm
j ) MMAP = arg maxi Pr(qm
j = i | yj).
Brieﬂy, we present the two stages of the simulation of hγ and of A, in order to
highlight the links between the variational and stochastic schemes.
7.4.1. HRF and NRLs simulation
The
a
posteriori
distribution
p(hγ | Y ,A,L,Θ)
is
Gaussian
and
reads
N(μhγ,Σhγ) 7:
Σ−1
hγ = v−1
h R−1 +
Jγ

j=1
St
jΓ−1
j Sj,
μhγ = Σhγ
Jγ

j=1
St
jΓ−1
j

yj −P ℓj

.
[7.8]
Similarly, by an argument of conjugation, the a priori distribution [7.4] of the
NRLs, A being a Gaussian mixture and the likelihood of A being a Gaussian when
hγ is ﬁxed, the a posteriori marginal density p(A | Y ,hγ,L,Θ) is also a Gaussian
mixture. Given the introduction of Q in the sampling scheme 8 and the spatial
correlation model retained in [7.5], the simulation of A is simpliﬁed and factorized:
p(A | Y ,Q,hγ,Θ) = 
j p(aj | yj,qj,hγ,Θ). Within the voxel Vj, the last point is to
successively consider the different experimental conditions m ∈
∗
M and to simulate
according to p(am
j | qm
j
= i,yj, · · · ) = N

μm
i,j,vm
i,j

. The identiﬁcation of the
parameters (μm
i,j,vm
i,j) of the Gaussian distributions leads to:
vm
i,j =

v−1
i,m + gt
mΓ−1
j gm
−1,
μm
i,j = vm
i,j

gt
mΓ−1
j em,j + i μi,mv−1
i,m

[7.9]
where gm = Xmhγ and em,j = yj −P ℓj −
m′̸=m am′
j gm′ = yj + gm. The
identiﬁcation of the weights λm
i,j of the a posteriori mixture is detailed in [VIN 10,
Appendix B].
7. The authors report an error in the expression of Σ−1
h
in [MAK 08, equation [B.1]].
8. See details in [VIN 10, Appendix B].

184
Regularization and Bayesian Methods for Inverse Problems
7.4.2. Unsupervised spatial and spatially adaptive regularization
Within a parcel Pγ, unsupervised spatial regularization consists of automatically
adjusting the vector β from data Y . With the proposed Gibbs sampler, this step is
performed by probabilizing β and by adding a sampling step of p(β | Q), which
depends on p(qm | βm) and on the a priori p(β):
p(β | Q) =
M

m=1
p(βm | qm) ∝
M

m=1
Z(βm)−1 exp

βmU(qm)

p(βm)
[7.10]
where p(βm) is chosen truncated on an interval [0,βmax] such that to avoid phase
transition phenomena. The distribution [7.10] depends on Z(·), independent of m.
As a result, the estimation of Z(·) remains a prerequisite to any sampling attempt
of p(βm | qm). In [VIN 10], a Metropolis–Hastings algorithm was implemented to
perform this step. The acceptance likelihood of a candidate value β(c)
m is written as:
α(β(t)
m →β(c)
m ) = min(1,Am
t→c) where the acceptance ratio Am
t→c is given by:
Am
t→c = p(β(c)
m |q(t)
m )g(β(t)
m |β(c)
m )
p(β(t)
m |q(t)
m )g(β(c)
m |β(t)
m )
= Z(β(t)
m )
Z(β(c)
m )
exp
"
(β(c)
m −β(t)
m )U((qm)(t))
#
Bm
t→c
with Bm
t→c function the instrumental distribution g. The exact evaluation of Z(β) in a
reasonable time is impossible for conventional image sizes. Its accurate estimation is
accessible by adopting a signiﬁcant sampling scheme on a discrete grid of values of
β. However, the computational cost remains important during a whole brain analysis
involving multiple parcels for each of which the FP of the hidden ﬁeld must be
estimated. As a matter of fact, these plots have all different sizes and geometries as is
illustrated in Figure 7.5.
To overcome this difﬁculty, an FP extrapolation scheme is implemented, relying
on a few reference partition functions to adapt itself to geometry variations. The
algorithmic details are available in [RIS 11, VIN 10].
7.5. Alternative variational inference scheme
7.5.1. Motivations and foundations
The exact Bayesian analysis of the JDE model is difﬁcult and has lead to the
approximated calculation of the a posteriori distribution [7.7] with the help of an
MCMC procedure, whose asymptotic convergence is ensured by a number of
conditions
simple
to
verify
[GEM 84,
HAS 70].
However,
difﬁculties
of
implementation may appear due to the extended calculation time, the need to
establish a convergence diagnosis that is sometimes sophisticated for simulation
algorithms [BRO 98] and the additional work cost required to address model

Joint Detection-Estimation in fMRI
185
selection issues based on the samples of the a posteriori distribution [MAR 07].
These considerations have prompted us to develop deterministic approximations, as a
matter of fact variational, of the distribution [7.7]. Unlike MCMC schemes,
variational calculation techniques are generally not accurate even asymptotically but
their computational ﬂexibility often justiﬁes their use.
size
density
Figure 7.5. Variability of parcels resulting from real data, forming the input
parcellation of the JDE approach. Left: histogram of parcels sizes;
right: illustration of a few parcels with varied geometries
The main idea here consists of approaching the target distribution, that is to say,
the a posteriori distribution with a distribution for which the calculations inherent to
a maximization likelihood algorithm can be achieved, such as the Expectation
Maximization (EM) algorithm [DEM 77]. The identiﬁcation of the best distribution
approximating the target distribution is performed relatively to the Kullback–Leibler
divergence by imposing additional constraints when the target distribution is not
directly computable. The most common procedure thus consists of assuming a
product form for the approximating distribution as we will now illustrate in the JDE
model.
In addition to the approximation based on a variational principle, the difference
with the previous approach resides in the lack of a priori of the parameters. Here, we
consider a non-Bayesian framework with missing variables. The variables of interest
considered as missing are A, hγ, Q, whereas L and Θ hold the status of parameters
estimated by maximum likelihood in an iterative scheme such as EM [DEM 77]. It
should be noted that from the perspective of the probabilistic process, there is no
difference between a missing variable and a parameter with an a priori distribution
such that it is easy to incorporate it to certain parameters if necessary. We will
illustrate this ﬂexibility in the following by adding an a priori to the spatial
regularization parameters βm.

186
Regularization and Bayesian Methods for Inverse Problems
7.5.2. Variational EM algorithm
We are looking for an approximation in the form p = pA pHγ pQ of the target
a posteriori distribution fHγAQ = p(hγ,A,Q | Y ; Θ) 9 minimizing the Kullback–
Leibler divergence D(p||fHγAQ):
D(p || fHγAQ) =

p(hγ,A,Q) ln
p(hγ,A,Q)
fHγAQ(hγ,A,Q) dhγ dA dQ
[7.11]
or equivalently by maximizing the free energy F(p; Θ)
=
ln p(Y ; Θ)−
D(p || fHγAQ) [NEA 98]. The terms of the optimal distribution then verify:
Am
t→c = p(β(c)
m |q(t)
m )g(β(t)
m |β(c)
m )
p(β(t)
m |q(t)
m )g(β(c)
m |β(t)
m )
= Z(β(t)
m )
Z(β(c)
m )
exp

(β(c)
m −β(t)
m )U((qm)(t))

Bm
t→c
which is again simpliﬁed due of the product form but results in formulas that remain
coupled and for which explicit direct solutions for pA, pHγ and pQ are not available.
However, the above formulation has the advantage of leading to an iterative solution
in which one of the terms pA is successively updated, pHγ and pQ as follows, the two
others being ﬁxed:
stage E-H: p(r)
Hγ = arg max
pHγ
F(p(r−1)
A
pHγ p(r−1)
Q
; Θ(r−1))
[7.12]
stage E-A: p(r)
A = arg max
pA
F(pA p(r)
Hγ p(r−1)
Q
; Θ(r−1))
[7.13]
stage E-Q: p(r)
Q = arg max
pQ
F(p(r)
A
p(r)
Hγ pQ; Θ(r−1))
[7.14]
by noting p(r−1)
A
, p(r−1)
Q
and Θ(r−1), common solutions and parameters at r −1.
The expressions [7.12]–[7.14] can then be written in terms of the Kullback–Leibler
divergence whose properties enables the identiﬁcation of solution distributions as
follows:
p(r)
Hγ(hγ) ∝exp

Ep(r−1)
A
p(r−1)
Q

ln p(hγ | Y ,A,Q; Θ(r−1)	
[7.15]
p(r)
A (A) ∝exp

Ep(r)
Hγp(r−1)
Q

ln p(A | Y ,hγ,Q; Θ(r−1))
	
[7.16]
p(r)
Q (Q) ∝exp

Ep(r)
A p(r)
Hγ

ln p(Q | Y ,hγ,A; Θ(r−1))
	
.
[7.17]
9. The use of the semicolon makes it possible to distinguish unknown deterministic parameters from
random variables.

Joint Detection-Estimation in fMRI
187
With respect to the estimation of the parameters Θ, updates are performed
according to:
Θ(r) = arg max
Θ
Ep(r)
A p(r)
Hγp(r)
Q

ln p(Y ,hγ,A,Q ; Θ)

[7.18]
Distribution expressions p(r)
Hγ, p(r)
A and p(r)
Q prove to be explicit, as well as those
of part of the parameters Θ. In particular, distributions p(r)
Hγ and p(r)
A are Gaussian. In
the following text, to alleviate notations, we eliminate the exponent r and we note
pHγ
=
N(mHγ,VHγ) and pA
=

j pAj with pAj
=
N(mAj,VAj). The
expressions of mHγ and VHγ are similar to those obtained with the MCMC
procedure in equation [7.8]. The terms which in [7.8] depend on am
j are replaced by
their expectation with respect to the distribution pA. From [7.15], it is obtained:
mHγ = VHγ
Jγ

j=1
St
jΓ−1
j (yj −Pℓj)
[7.19]
V −1
Hγ = v−1
h R−1 +
Jγ

j=1
 
m,m′
vAm
j Am′
j Xt
mΓ−1
j Xm′ + St
jΓ−1
j
Sj
	
[7.20]
where Sj = M
m=1 mAm
j Xm. Notations mAm
j and vAm
j Am′
j
respectively represent
components m and (m,m′) of the mean vector and of the covariance matrix of the
current distribution pAj. In the case of the distribution pAj deﬁned in
M, it yields:
mAj = VAj
 
i=0,1
Δijμi + 
GtΓ−1
j (yj −P ℓj)
	
[7.21]
VAj =
 
i=0,1
Δij + 
Hj
	−1
[7.22]
where μi = [μi,1, . . . , μi,M]t, 
G = EpHγ

G

with G = [g1, . . . , gM] (the column m
of

G is denoted as gm
=
XmmHγ),
Δij
=
diag[pQm
j (i)/vi,m] and

Hj = EpHγ

GtΓ−1
j G

, a matrix M × M whose element (m, m′) is:
EpHγ

gt
mΓ−1
j gm′
=EpHγ

gm
tΓ−1
j EpHγ

gm′
+ tr

Γ−1
j covpHγ (gm,gm′)

= gt
mΓ−1
j gm′ + tr

Γ−1
j XmVHγXt
m′

.
In this case, the similarity with updates obtained by MCMC is less obvious. In
MCMC as a matter of fact, am
j are simulated in turns and conditionally to qm
j
and

188
Regularization and Bayesian Methods for Inverse Problems
other am′
j . In the variational EM algorithm (VEM), marginals are calculated and
integration is performed with regard to the other variables (qm
j ). However, a way to
show consistency with the moments [7.9] of the conditional Gaussian distribution
p(am
j | qm
j
= i,yj, . . .) is to assume the equivalent of qm
j
= i, that is to say,
/pQm
j (i) = 1 and /pQm
j (1 −i) = 0. In equation [7.22] of the variance, the m-th
diagonal
term
of

i Δij
+
3
Hj
is
then
equal
to
v−1
i,m + /gt
mΓ−1
j /gm + tr(Γ−1
j XmVHγXm
t). It can be observed in the ﬁrst two terms
that an expression similar to [7.9] reappears with a third additional term.
In
the
case
of
the
mean,
the
m-th
diagonal
term
of

i Δijμi + /
GtΓ−1
j (yj −P ℓj) in [7.21] is i μi,m v−1
i,m + /gt
mΓ−1
j (yj −P ℓj). Thus,
the second factor appears once again in the expression [7.9] when replacing gm by
/gm and up to the term 
m′̸=m am′
j gm′. It is in this last term that conditioning by
am′
j , non-existing in the variational formulation, is expressed, as well as the
interaction between the different conditions. It is carried out naturally by means of
conditioning by am′
j
in MCMC whereas that is achieved by more complex matrix
expressions and additional terms in VEM.
With respect to /pQ, the variables (am, qm) constitute independent pairs whose
respective a priori distributions are hidden Ising models respectively with interaction
parameter βm, without external ﬁeld and with Gaussian emission distributions. It
follows that /pQ(Q) has a product form:
/pQ(Q)
=
2
m /pQm(qm) with
/pQm(qm) = f(qm | am = mAm; μ.,m,v.,m,βm) where the right member represents
the conditional distribution for a joint distribution noted f which is the distribution of
a hidden Ising ﬁeld modiﬁed relatively to the a priori hidden Ising ﬁeld (am,qm).
The modiﬁcation consists of replacing the observations am
j
by their mean mAm
j
(which become the new observations) and in adding an external ﬁeld for j ∈Jγ and
i ∈{0,1}, αi,j = −vAm
j Am
j /vi,m, the remaining regularization parameter βm.
Nonetheless, the expression of the modiﬁed hidden Ising is not available explicitly
due to the partition function. However, it is still possible to ﬁnd a variational
approximation, also known as mean-ﬁeld, as speciﬁed in [CEL 03]. It is tantamount
to consider the achieved approximation: /pQm(qm) ≈2
j /pQm(qm
j | {˜qm
k ,k ∼j})
with ˜qm a ﬁeld of ﬁxed values that veriﬁes a ﬁxed point equation to solve. This
solves the problem because the above conditional distributions for the hidden Ising
ﬁeld /pQm are now calculable. Furthermore, by applying the mean-ﬁeld principle, the
˜qm
k are interpreted as the mean values of the hidden Ising ﬁelds at each site k. Other
approximations are possible (see [CEL 03]).

Joint Detection-Estimation in fMRI
189
Relatively to the update of the parameters Θ, the expression [7.18] gives rise to
four independent updates. The ﬁrst two are self-explanatory. It should be noted that
¯pim = 
j pQm
j (i) for i ∈{0,1}. It gives:
μi,m =
Jγ

j=1
pQm
j (i)
¯pim
mAm
j ,
vi,m =
Jγ

j=1
pQm
j (i)
¯pim

(mAm
j −μi,m)2 + vAj
mAj
m

vh = (D −1)−1EpHγ

ht
γR−1hγ

= (D −1)−1(mt
HγR−1mHγ + tr(VHγR−1))
= (D −1)−1tr((VHγ + mHγmHγ
t)R−1).
With respect to Gaussian parameters and for a comparison with MCMC
procedures, the a posteriori distributions obtained for these parameters can be
referred to such as detailed in [VIN 10, Appendix A]. Recalling the notations in
[VIN 10], when in the above formulas pQm
j (1) = 1 is given for j such that qm
j = 1 in
the MCMC procedure, μ1,m = 
j∈C1,m mAm
j /J1,m is met once more. This is
consistent with the expression of η1,m (see [VIN 10, equation [A.4]) when replacing
am
j by mAm
j (in variational, the hyperparameter aμ1 does not have to be included).
Respectively
to
variances,
it
gives
vi,m
=

j∈Ci,m((mAm
j
−μi,m)2+
vAm
j Am
j )/Ji,m, which up to the term vAm
j Am
j
is consistent with the mean (see
[VIN 10, equation [A.4]) of the inverse-gamma distribution vi,m in the MCMC
procedure. Also the fact that in MCMC, conditioning is achieved for these
calculations by am
j ,
the observed differences are due to the presence of
hyperparameters and to a priori distributions that do not exist in the VEM algorithm
discussed here.
In the VEM formulation, there is an expression consistent with the simulation of
vh in MCMC (see [MAK 08]). While vh is conditionally simulated with hγ ﬁxed
to its current value in MCMC, the expressions in hγ are replaced by their expected
values relatively to the current approximating distribution in VEM.
The other two updates (interaction parameters β and noise parameters) require an
iterative maximization procedure. With respect to parameters βm, the approximation
created above [CEL 03] leads to an equation for which a gradient-based algorithm
can be used. A tendency toward the overestimation of these parameters can then be
observed. This can be partly compensated by introducing an a priori distribution
p(βm) aiming to reduce the estimated value βm. Thus, we illustrate the possibility
mentioned previously to incorporate, as with the MCMC case, a priori distributions

190
Regularization and Bayesian Methods for Inverse Problems
of the parameters. More speciﬁcally, if it is assumed that p(βm) is an exponential
distribution with a parameter λm, it gives:
βm = arg max
β′
m
E/pQm [ln p(qm | β′
m)p(β′
m)]
= arg max
β′
m
{−ln Z(β′
m) + β′
m(

j∼k
E/pQm

I(qm
j = qm
k )

−λm)}
If derivation is carried out with respect to βm, the outcome is again the
conventional expression detailed in [CEL 03] in which the constant λm is subtracted
from the usual quantity 
j∼k E/pQm

I(qm
j = qm
k )

representing the mean number of
homogeneous cliques of the approximating distribution. It is easy to see that this
subtraction has the effect of decreasing the value of βm estimate as desired.
With regard to the parameters {ℓj,σ2
j ,Λj,j = 1..Jγ}, they satisfy a ﬁxed-point
equation of that we do not fully detail. In the AR(1) case, it can be shown that:
ℓj = (P tΓ−1
j P )−1P tΓ−1
j

yj −/SjmHγ

= F1(ρj)
[7.23]
Hence a similarity with [MAK 08, equation [B.2]] can be observed in [7.23] when
replacing hγ and A with mHγ and mA. In a similar manner, it can be shown that the
optimal values verify two other relationships σ2
j = F2(ρj,ℓj) and ρj = F3(ρj,σ2
j ).
This then allows these different relationships to combine to estimate ρj as a solution
to a ﬁxed point equation and to derive then ℓj and σj.
7.6. Comparison of both types of solutions
In order to compare the two methods, a number of simulations as well as
experiments on real data have been carried out [CHA 11a, CHA 13].
7.6.1. Experiments on simulated data
First, we have simulated data from the equation [7.1] and the p(A | Q)
distribution with a matrix P deﬁned as a discrete cosine transform basis, white
Gaussian noise (σ2
j = 0,5, Λj = IN) and M = 2 experimental conditions with
variable contrast-to-noise ratios (CNR). More speciﬁcally, we have set: μ1,1 = 2,8,
v1,1 = 0,3 and μ1,2 = 1,8, v1,2 = 0,4, such that μ1,1/v1,1 > μ1,2/v1,2. The other
variances v0,· are set to 0.3. Using this mixture, the artiﬁcial NRLs are generated
conditionally to synthetic binary images of size 20 × 20 representing activated and
non-activated pixels (Figure 7.6). In addition, the initial paradigm is constituted of 15
stimuli for each of the conditions. Simulated data are thus constituted of 152 scans
and sampled at a rate of TR = 1 s.

Joint Detection-Estimation in fMRI
191
NRL
Labels
Truth
MCMC/VEM
MCMC
VEM
m = 1
m = 2
Figure 7.6. On the left: NRLs simulated and estimated by MCMC and VEM
(very similar results); on the right: a posteriori probability maps obtained by the
approximation pQm (VEM) and by the MMAP estimator qm (MCMC)
The two MCMC and VEM procedures are then applied to these data. We are under
the scope of the true noise model, that is to say, white Gaussian as in simulations.
Both approaches produce very similar NRLs. A slight difference is observed in the a
posteriori activation probabilities of the condition with a low CNR (m = 2). These
probabilities are given by pQm
j (1) in the variational case and by ˆqm
j (1) deﬁned in
section 7.4 in the MCMC case. This difference suggests a gain in robustness in favor
of the variational approach. The estimated levels of spatial regularization also differ
with !β1 = 0,78, !β2 = 0,92 for MCMC and !β1 = 1,04, !β2 = 1,08 for VEM.
For a more quantitative comparison, additional simulations have been performed
with varying stimuli densities (from ﬁve to thirty), variable noise variances and
different temporal correlation models for the noise (AR structure for Λi). The results
are shown in Figure 7.7 that shows (a) the evolution of the mean squared error (MSE)
of NRLs estimated based on the number of stimuli for simulations following an
AR(2) noise model such that the estimation assumes a white Gaussian noise as
previously.
Figure 7.7 shows that for a low stimulus density, that is to say, for a low
signal-to-noise ratio (SNR), the variational version is more robust. In the case of
higher densities (higher than 20) the two approaches behave in the same manner.
However,
Figure 7.7(b) and 7.7(c) show for two densities from different
stimulations (ﬁve and ﬁfteen) the HRFs estimated with respect to the canonical HRF
used for simulation. The main features (peak value, time-to-peak and undershoot) are
correctly estimated by the two approaches. However, Figure 7.7(b) shows that for a
low stimuli density, the variational approach is less accurate than its stochastic
counterpart at the undershoot level. This observation is also conﬁrmed when the
three above characteristics of the true HRF used in the simulations are varied.

192
Regularization and Bayesian Methods for Inverse Problems
However, when estimations and simulations are made with the same noise model, the
differences observed between the two approaches are minimal and not signiﬁcant.
MSE
%Δ BOLD signal
ground truth
ground truth
Number of stimuli
Time (s)
Time (s)
a)
b): 5 stimuli
c): 15 stimuli
Figure 7.7. a) Evolution of the MSE of the NRLs as a function of the number of stimuli,
b)-c) ground truth and HRF estimated with VEM inferences and MCMC
for two different stimuli densities, using an AR(2) noise
Figure 7.8(a) and 7.8(b) show the evolution of the MSE of NRLs depending on the
SNR when the variance of the noise and autocorrelation are respectively varied.
In the latter case, the two parameters of the AR(2) model are modiﬁed in such a
way as to maintain a stable AR process. Similarly to what has been already observed
in [CAS 08], for a given SNR, a larger autocorrelation implies an increase of the MSE
more signiﬁcant than the increase of the noise variance, and this is for both approaches.
Moreover, the two methods behave in the same manner over a large range of values
for SNR ⩾5.5.
Finally, the most notable advantage of the variational approach reside in
computation times. On a Core 2 - 2,26 GHz - 2 Gb RAM Intel architecture, results
have been obtained approximately thirty times faster.
MSE
MSE
Input SNR (dB)
Input SNR (dB)
a)
b)
Figure 7.8. Evolution of the MSE of the NRLs as a function of the input SNR (AR(2) noise)
a) by varying noise variance and b) the correlation level of AR(2) noise

Joint Detection-Estimation in fMRI
193
7.6.2. Experiments on real data
We have also considered real fMRI data acquired from a 3T MRI (Siemens Trio)
with a gradient echoEPI sequence (TE = 30 ms/TR = 2.4 s/FOV = 192 mm2) and
a paradigm resulting from a Localizer protocol [PIN 07]. The acquisition carried out
includes a single session of N = 128 3D volumes with a 2×2×3 mm3 resolution. The
paradigm includes 10 conditions (heard and read sentences, aurally induced and read
calculations, left and right clicks aurally and visually induced, horizontal and vertical
checkerboards) divided into 60 stimuli.
We have focused on the contrast calculation-sentence (combined auditory
and visual stimuli) taking into account, the differences of activations induced by the
calculation and the sentence conditions in the left intraparietal sulcus, subdivided into
17 parcels for JDE analyses. The choice of this region lies in the fact that it is likely
to induce an HRF which deviates from the canonical shape. An extended version of
these results is presented in [CHA 13].
Figure 7.9 shows that NRLs estimated by both approaches are very similar and
follow in a satisfactory manner the underlying sulcal anatomy. It should be noted
here that only the most activated slice is visualized in Figure 7.9, where we also show
the two estimations (MCMC and VEM) of the HRF in the most activated parcel that
includes approximately 200 voxels. The HRF are similar in both approaches and
clearly deviate from the canonical shape particularly at the level of the time-to-peak
and at the undershoot, that is, postactivation depletion. The variational approach
generates an HRF which oscillates more at the level of this depletion but this is
tempered by the fact that in a general fashion, the estimation of the HRF tail is less
reliable than the peak for which the BOLD signal level is more important. In
addition, the event-related nature of the paradigm under consideration here is not
adapted to precisely study the characteristics of the tail of the HRF. With a slow
event-related paradigm for which responses do not overlap in time, our tests show
that these oscillations disappear. A pragmatic solution proposed to solve this issue in
the framework of fast event-related paradigms consists of introducing in the design of
the paradigm zero events, that is to say, slightly longer periods without stimulation,
such as to allow time for the hemodynamics response to recover its baseline. Another
way consists of restraining the HRF model using a semi-parametric approach as in
[GEN 00, WOO 04a].
On the other hand, the estimated spatial regularization levels, as shown in the maps
at the bottom of Figure 7.9, are signiﬁcantly different from one approach to the other,
with stronger estimated levels for VEM. However, for the most activating parcel, it
can be noted that for both approaches there is coherence with the contrast achieved:
the value of !β is stronger for the calculation condition than for the sentence
condition. Finally, respectively to computation times, in the study of this parcel, a
same gain by a factor of 30 for the variational approach as for simulations can be
observed.

194
Regularization and Bayesian Methods for Inverse Problems
MCMC
VEM
%Δ BOLD signal
time (s.)
!βcalc.
!βsent.
!βcalc.
!βsent.
Figure 7.9. Coronal (top row, exterior), sagittal (top row, center) and axial (middle row) views
of the calculation-sentence contrast estimated by MCMC JDE (on the left) and VEM (on
the right). At the center: HRF estimated by MCMC (in green) and by VEM (in blue) and
HRF canonical in dashed. Bottom row maps of spatial regularization levels estimated for the
conditions calculation and sentence by MCMC (left) and by VEM (right). For a color
version of the ﬁgure, see www.iste.co.uk/giovannelli/regularization.zip
7.7. Conclusion
The experiments described in the previous section aimed essentially to compare
both VEM and MCMC approaches proposed for the estimation of the JDE model.
More detailed results with the JDE approach itself and its comparison with other
models can also be found in [BAD 11, BAD 13].
The results of section 7.6 conﬁrm the expected advantages of the variational
approach, namely the simplicity of implementation, the speed of calculations, etc.
With respect to the comparison with the MCMC procedure, what appears as
particularly advantageous is the simplicity of the convergence criterion in variational
and the possibility to relatively easily extend the procedure to more complex models
including for example AR noises of higher order or a model of neural habituation.

Joint Detection-Estimation in fMRI
195
Aspects
of
model
selection
are
also
accessible
if
considering
penalized
likelihood-based
criteria
whose
variational
approach
can
easily
provide
an
approximation [FOR 03].
With regard to performance, they are often very similar for the two inference
schemes, if the interest is as here toward pointwise estimators. However, signiﬁcant
differences appear in the uncertainty measures of these estimators, that is to say, in
the estimation variances.
This result with pointwise estimators may seem surprising to the extent where only
the MCMC procedure offers guarantees of theoretical convergence but in reality it can
hide different situations. On the one hand, this does not exclude that the variational
approach has also in some cases the same convergence properties but so far there exist
no fairly general results about the quality of these approximations. On the other hand,
it is not excluded that simpliﬁcations introduced in the variational approach of the
factorization of the target distribution induce a greater robustness regarding certain
model errors (response stationarity, noise, etc.).
In the speciﬁc case of the JDE, a more in-depth study by means of simulations
could be considered in order to try to better identify at which stage of the model the
variational approximation is the most active and potentially indicate if this action is
prone to important errors or not. Another interesting aspect observed in our
simulations, and which is not contradictory to the approximation aspect, is the
greater robustness of the variational approach to model errors.
Finally, to complete the comparison of approaches and the evaluation of the
potential of the variational solution, it would be interesting to carry out a group
analysis with it such as the one achieved with the MCMC procedure in
[BAD 11, BAD 13].
7.8. Bibliography
[AGU 98] AGUIRRE G., ZARAHN E., D’ESPOSITO M., “The variability of human, BOLD
hemodynamic responses”, Neuroimage, vol. 8, pp. 360–369, 1998.
[BAD 11] BADILLO S., VINCENT T., CIUCIU P., “Impact of the joint detection-estimation
approach on random effects group studies in fMRI”, IEEE International Symposium on
Biomedical Imaging, pp. 376–380, Chicago, IL, April 2011.
[BAD 13] BADILLO S., VINCENT T., CIUCIU P., “Group-level impacts of within- and
between-subject hemodynamic variability in fMRI,” NeuroImage, vol. 82, pp. 433-448,
2013.
[BAI 97] BAILLET S., GARNERO L., “A Bayesian approach to introducing anatomo-
functional priors in the EEG/MEG inverse problem”, IEEE Transactions on Biomedical
Engineering, vol. 44, no. 5, pp. 374–385, May 1997.

196
Regularization and Bayesian Methods for Inverse Problems
[BAK 12] BAKHOUS C., FORBES F., VINCENT T., et al., “Adaptive experimental condition
selection in event-related fMRI”, IEEE International Symposium on Biomedical Imaging,
pp. 1755–1758, Barcelona, Spain, May 2012.
[BAU 11] BAUSCHKE H.H., COMBETTES P.L., Convex Analysis and Monotone Operator
Theory in Hilbert Spaces, CMS books in Mathematics, Springer, 2011.
[BOY 12] BOYER C., CIUCIU P., WEISS P., et al., “HYR2PICS: Hybrid regularized
reconstruction for combined parallel imaging and compressive sensing in MRI”, IEEE
International Symposium on Biomedical Imaging, pp. 66–69, Barcelona, Spain, May 2012.
[BRO 98] BROOKS S.P., GELMAN A., “General methods for monitoring convergence of
iterative simulations”, Journal of Computational and Graphical Statistics, vol. 7, no. 4,
pp. 434–455, 1998.
[BUC 98] BUCKNER R., “Event-related fMRI and the hemodynamic response”, Human Brain
Mapping, vol. 6, nos. 5–6, pp. 373–377, 1998.
[BUX 98] BUXTON R.B., WONG E.C., FRANK L.R., “Dynamics of blood ﬂow and
oxygenation changes during brain activation: the balloon model”, Magnetic Resonance in
Medicine, vol. 39, pp. 855–864, June 1998.
[BUX 04] BUXTON R.B., ULUDAG K., DUBOWITZ D.J., et al., “Modeling the hemodynamic
response to brain activation”, Neuroimage, vol. 23, no. 1, pp. S220–S233, 2004.
[CAS 08] CASANOVA R., RYALI S., SERENCES J., et al., “The impact of temporal
regularization on estimates of the BOLD hemodynamic response function: a comparative
analysis”, Neuroimage, vol. 40, no. 4, pp. 1606–1618, May 2008.
[CEL 03] CELEUX G., FORBES F., PEYRARD N., “EM procedures using mean ﬁeld-
like approximations for Markov model-based image segmentation”, Pattern Recognition,
vol. 36, pp. 131–144, 2003.
[CHA 11a] CHAARI L., FORBES F., VINCENT T., et al., “Variational solution to the joint
detection estimation of brain activity in fMRI”, MICCAI, LNCS 6892, Toronto, Canada,
Springer, Berlin, pp. 260–268, September 2011.
[CHA 11b] CHAARI L., PESQUET J.-C., BENAZZA-BENYAHIA A., et al., “A wavelet-
based regularized reconstruction algorithm for SENSE parallel MRI with applications to
neuroimaging”, Medical Image Analysis, vol. 15, no. 2, pp. 185–201, 2011.
[CHA 13] CHAARI L., VINCENT T., FORBES F., et al., “Fast joint detection-estimation
of evoked brain activity in event-related fMRI using a variational approach,” IEEE
Transactions on Medical Imaging, vol. 32, no. 5, pp. 821–837, 2013.
[CIU 03] CIUCIU P., POLINE J.-B., MARRELEC G., et al., “Unsupervised robust non-
parametric estimation of the hemodynamic response function for any fMRI experiment”,
IEEE Transactions on Medical Imaging, vol. 22, no. 10, pp. 1235–1251, October 2003.
[CIU 04] CIUCIU P., IDIER J., ROCHE A., et al., “Outlier detection for robust region-
based estimation of the hemodynamic response function in event-related fMRI”, IEEE
International Symposium on Biomedical Imaging, pp. 392–395, Arlington, VA, April 2004.
[CIU 08] CIUCIU P., ABRY P., RABRAIT C., et al., “Log wavelet leaders cumulant based
multifractal analysis of EVI fMRI time series: evidence of scaling in ongoing and evoked

Joint Detection-Estimation in fMRI
197
brain activity”, IEEE Journal of Selected Topics in Signal Processing, vol. 2, no. 6, pp. 929–
943, December 2008.
[CIU 12] CIUCIU P., VAROQUAUX G., ABRY P., et al., “Scale-free and multifractal time
dynamics of fMRI signals during rest and task”, Frontiers in Physiology, vol. 3, no. 186,
pp. 1–18, June 2012.
[CIU 14] CIUCIU P., ABRY P., HE J.B., “Interplay between functional connectivity and scale-
free dynamics in intrinsic fMRI networks,” NeuroImage, vol. 95, pp. 248– 263, 2014.
[DAL 97] DALE A.M., BUCKNER R.L., “Selective averaging of rapidly presented individual
trials using fMRI”, Human Brain Mapping, vol. 5, pp. 329–340, 1997.
[DEP 08]
DE PASQUALE F., DEL GRATTA C., ROMANI G., “Empirical Markov Chain Monte
Carlo Bayesian analysis of fMRI data”, Neuroimage, vol. 42, no. 1, pp. 99–111, August
2008.
[DES 99] D’ESPOSITO M., ZARAHN E., AGUIRRE G.K., et al., “The effect of normal aging
on the coupling of neural activity to the BOLD hemodynamic response”, Neuroimage,
vol. 10, no. 1, pp. 6–14, July 1999.
[DEM 77] DEMPSTER A.P., LAIRD N.M., RUBIN D.B., “Maximum likelihood from
incomplete data via the EM algorithm”, Journal of the Royal Statistical Society B, vol. 39,
pp. 1–38, 1977.
[EKS 10] EKSTROM A., “How and when the fMRI BOLD signal relates to underlying neural
activity: The danger in dissociation”, Brain Research Reviews, vol. 62, no. 2, pp. 233–244,
2010.
[FOR 03] FORBES F., PEYRARD N., “Hidden Markov random ﬁeld selection criteria based
on mean ﬁeld-like approximations”, IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 25, no. 8, pp. 1089–1101, 2003.
[FOX 07] FOX M.D., RAICHLE M.E., “Spontaneous ﬂuctuations in brain activity observed
with functional magnetic resonance imaging”, Nature Reviews Neuroscience, vol. 8, no. 9,
pp. 700–11, September 2007.
[FRI 95] FRISTON K., HOLMES A.P., POLINE J.-B., et al., “Analysis of fMRI time-series
revisited”, Neuroimage, vol. 2, pp. 45–53, 1995.
[GEM 84] GEMAN S., GEMAN D., “Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images”, IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. PAMI-6, no. 6, pp. 721–741, November 1984.
[GEN 00] GENOVESE C., “A Bayesian time-course model for functional magnetic resonance
imaging data (with discussion)”, Journal of American Statistical Association, vol. 95,
pp. 691–719, 2000.
[GOU 00] GOUTTE C., NIELSEN F.A., HANSEN L.K., “Modeling the haemodynamic
response in fMRI using smooth ﬁlters”, IEEE Transactions on Medical Imaging, vol. 19,
no. 12, pp. 1188–1201, December 2000.
[GRA 11] GRAMFORT A., STROHMEIER D., HAUEISEN J., et al., “Functional brain imaging
with M/EEG using structured sparsity in time-frequency dictionaries”, in SZÉKELY G.,
HAHN H.K., (eds.), IPMI, Springer, Berlin, vol. 6801, pp. 600–611, 2011.

198
Regularization and Bayesian Methods for Inverse Problems
[GRE 03] GREICIUS M.D., KRASNOW B., REISS A.L., et al., “Functional connectivity in
the resting brain: a network analysis of the default mode hypothesis”, Proceedings of the
National Academy of Sciences of the United States of America, vol. 100, no. 1, pp. 253–258,
January 2003.
[HAL 98] HALGREN E., MARINKOVIK K., CHAUVEL P., “Generators of the late cognitive
potentials in auditory and visual oddball tasks”, Electroencephalography and Clinical
Neurophysiology, vol. 102, no. 2, pp. 156–164, 1998.
[HAN 04] HANDWERKER D.A., OLLINGER J.M., D’ESPOSITO M., “Variation of BOLD
hemodynamic responses across subjects and brain regions and their effects on statistical
analyses”, Neuroimage, vol. 21, pp. 1639–1651, 2004.
[HAS 70] HASTINGS W.K., “Monte Carlo sampling methods using Markov chains and their
applications”, Biometrika, vol. 57, no. 1, pp. 97–109, January 1970.
[HEN 00] HENSON R., ANDERSSON J., FRISTON K., “Multivariate SPM application to basis
function characterisations of event-related fMRI responses”, Neuroimage, vol. 11, pp. 468,
2000.
[HIG 98] HIGDON D.M., “Auxiliary variable methods for Markov chain Monte Carlo with
applications”, Journal of American Statistical Association, vol. 93, no. 442, pp. 585–595,
June 1998.
[KAY 88] KAY S.M., Modern Spectral Estimation, Prentice-Hall, Englewood Cliffs, NJ,
1988.
[LIN 01] LINKENKAER-HANSEN K., NIKOULINE V.V., PALVA J.M., et al., “Long-range
temporal correlations and scaling behavior in human brain oscillations”, The Journal of
Neuroscience, vol. 21, no. 4, pp. 1370–1377, February 2001.
[LOG 01] LOGOTHETIS N.K.,
PAULS J.,
AUGATH M.,
et al.,
“Neurophysiological
investigation of the basis of the fMRI signal”, Nature, vol. 412, no. 6843, pp. 150–157,
July 2001.
[LUS 07] LUSTIG M., DONOHO D., PAULY J., “Sparse MRI: the application of compressed
sensing for rapid MR imaging”, Magnetic Resonance in Medecine, vol. 58, no. 6, pp. 1182–
1195, December 2007.
[MAK 05] MAKNI S., CIUCIU P., IDIER J., et al., “Joint detection-estimation of brain activity
in functional MRI: a multichannel deconvolution solution”, IEEE Transactions on Signal
Processing, vol. 53, no. 9, pp. 3488–3502, September 2005.
[MAK 08] MAKNI S., IDIER J., VINCENT T., et al., “A fully Bayesian approach to the parcel-
based detection-estimation of brain activity in fMRI”, Neuroimage, vol. 41, no. 3, pp. 941–
969, July 2008.
[MAR 03] MARRELEC G., BENALI H., CIUCIU P., et al., “Robust Bayesian estimation of
the hemodynamic response function in event-related BOLD MRI using basic physiological
information”, Human Brain Mapping, vol. 19, no. 1, pp. 1–17, May 2003.
[MAR 07] MARIN J.-M.,
ROBERT C.P., Bayesian Core:
A Practical Approach to
Computational Bayesian Statistics, Springer, New York, 2007.

Joint Detection-Estimation in fMRI
199
[MIE 00] MIEZIN F.M., MACCOTTA L., OLLINGER J.M., et al., “Characterizing the
hemodynamic response: effects of presentation rate, sampling procedure, and the possibility
of ordering brain activity based on relative timing”, Neuroimage, vol. 11, pp. 735–759,
2000.
[NEA 98] NEAL R., HINTON G., “A view of the EM algorithm that justiﬁes incremental,
sparse and other variants”, in JORDAN M., (ed.), Learning in Graphical Models, pp. 355–
368, MIT Press, Cambridge, MA, 1998.
[OGA 90] OGAWA S., LEE T., KAY A., et al., “Brain magnetic resonance imaging with
contrast dependent on blood oxygenation”, Proceedings of the National Academy of
Sciences of the United States of America, vol. 87, no. 24, pp. 9868–9872, 1990.
[PEN 07] PENNY W., FLANDIN G., TRUJILLO-BARETO N., “Bayesian comparison of
spatially regularised general linear models”, Human Brain Mapping, vol. 28, no. 4, pp. 275–
293, 2007.
[PIN 07] PINEL P., THIRION B., MÉRIAUX S., et al., “Fast reproducible identiﬁcation and
large-scale databasing of individual functional cognitive networks”, BMC Neuroscience,
vol. 8, no. 1, p. 91, 2007.
[PRU 99] PRUESSMANN K.P., WEIGER M., SCHEIDEGGER M., et al., “SENSE: Sensitivity
encoding for fast MRI”, Magnetic Resonance in Medecine, vol. 42, pp. 952–962, November
1999.
[RIC 03] RICHTER W., RICHTER M., “The shape of the fMRI BOLD response in children
and adults changes systematically with age”, Neuroimage, vol. 20, no. 2, pp. 1122–1131,
2003.
[RIE 04] RIERA J.J., WATANABE J., KAZUKI I., et al., “A state-space model of the
hemodynamic approach:
nonlinear ﬁltering of BOLD signals”, Neuroimage, vol. 21,
pp. 547–567, 2004.
[RIS 11] RISSER L., VINCENT T., FORBES F., et al., “Min-max extrapolation scheme for fast
estimation of 3D Potts ﬁeld partition functions. Application to the joint detection-estimation
of brain activity in fMRI”, Journal of Signal Processing Systems, vol. 65, no. 3, pp. 325–
338, December 2011.
[ROB 07] ROBERT C.P., The Bayesian Choice: From Decision-Theoretic Foundations to
Computational Implementation, Springer, New York, 2007.
[SEK 05] SEKIHARA K., SAHANI M., NAGARAJAN S.S., “Localization bias and spatial
resolution of adaptive and non-adaptive spatial ﬁlters for MEG source reconstruction”,
Neuroimage, vol. 25, no. 4, pp. 1056–1067, 2005.
[SMI 03] SMITH M., PTZ B., AUER D., et al., “Assessing brain activity through spatial
Bayesian variable selection”, Neuroimage, vol. 20, pp. 802–815, 2003.
[SWA 98] SWARTZ B., GOLDENSOHN E., “Timeline of the history of EEG and associated
ﬁelds”, Electroencephalography and Clinical Neurophysiology, vol. 102, no. 2, pp. 173–
176, 1998.
[VIN 10] VINCENT T., RISSER L., CIUCIU P., “Spatially adaptive mixture modeling for
analysis of within-subject fMRI time series”, IEEE Transactions on Medical Imaging,
vol. 29, no. 4, pp. 1059–1074, April 2010.

200
Regularization and Bayesian Methods for Inverse Problems
[WOO 01] WOOLRICH M., RIPLEY B., BRADY M., et al., “Temporal autocorrelation in
univariate linear modeling of fMRI data”, Neuroimage, vol. 14, no. 6, pp. 1370–1386,
December 2001.
[WOO 04a] WOOLRICH M., JENKINSON M., BRADY J., et al., “Fully Bayesian spatio-
temporal modeling of fMRI data”, IEEE Transactions on Medical Imaging, vol. 23, no. 2,
pp. 213–231, February 2004.
[WOO 04b] WOOLRICH M., JENKINSON M., BRADY J.M., et al., “Constrained linear basis
set for HRF modeling using variational Bayes”, Neuroimage, vol. 21, no. 4, pp. 1748–1761,
2004.
[ZAR 97] ZARAHN E., AGUIRRE G.K., D’ESPOSITO M., “Empirical analysis of BOLD
fMRI statistics. I. Spatially unsmoothed data collected under null-hypothesis conditions”,
Neuroimage, vol. 5, no. 3, pp. 179–197, April 1997.

8
MCMC and Variational Approaches for
Bayesian Inversion in Diffraction Imaging
8.1. Introduction
The term “Bayesian diffraction imaging” should be understood here in the sense
of an “inverse scattering problem” in which the objective is to reconstruct the image
of an unknown object (a mapping of its physical parameters such as dielectric
permittivity) from the scattered ﬁeld measurements resulting from its interaction with
a known probing wave (known as incident wave). This type of problem is found in
many imaging and non-destructive testing applications. It corresponds to the
situation when looking for a good trade-off between image resolution (smallest
observable detail) and incident wave penetration in the probed medium leads to
choosing a wavelength of the latter in the “resonance” domain, such that it is of the
order of magnitude of the characteristic dimensions or of the inhomogeneities of the
object under test. The wave–object interaction results in a signiﬁcant diffraction
phenomenon. This is the case for the two applications considered here, where the
probing waves are electromagnetic waves with wavelengths in the microwave and the
optical domains, where the characteristic dimensions of the sought object are 1 cm
and 1 μm, respectively.
The solution of an inverse problem obviously requires previous construction of a
forward model expressing the scattered ﬁeld as a function of the parameters of the
sought object. Consideration of these diffraction phenomena in this model is done by
using integral domain representations of electric ﬁelds. The forward model is then
described by two coupled integral equations, whose discrete versions are obtained
using the method of moments and whose inversion yields a nonlinear problem.
Chapter written by Hacheme AYASSO, Bernard DUCHÊNE and Ali MOHAMMAD-DJAFARI.

202
Regularization and Bayesian Methods for Inverse Problems
With respect to inversion, at the beginning of the 1980s, accounting for the
diffraction phenomena was the subject of much attention in the ﬁeld of acoustic
imaging for applications in geophysics [DEV 84], in non-destructive testing or
biomedical imaging [MUE 80]. It led to techniques known as “diffraction
tomography”, a term which according to Schatzberg and Devaney [SCH 92]
designates
“applications
that
employ
diffracting
waveﬁelds
in
tomographic
reconstruction processes”, but that generally involved reconstruction processes based
on the generalized projection-slice theorem. This corresponds to an extension to the
case of the diffraction of the projection-slice theorem of conventional computed
tomography (CT) whose forward model is given by the Radon transform. This
theorem is based on Born or Rytov’s ﬁrst-order linear approximations. As a result,
the term “diffraction tomography” was paradoxically used to describe reconstruction
techniques adapted to weakly scattering environments that do not provide
quantitative information on high-contrast dielectric objects [AZI 83, SLA 84], such
as those encountered in the applications considered here, where multiple diffraction
cannot be ignored.
In addition to this major drawback, there is also the fact that the solution of these
techniques is limited because evanescent waves are not taken into consideration.
These limitations led the researchers to develop in the 1990s inversion algorithms
that are capable of dealing with nonlinear problems for microwave imaging and more
recently for optical imaging [BEL 03]. Many studies have focused on the
development of deterministic methods,
among which we must mention the
Newton–Kantorovich method [JOA 91], the modiﬁed gradient method (MGM,
[KLE 92]) or contrast-source inversion (CSI, [BER 97]), where the solution is sought
using iterative minimization by a gradient method of a cost function that expresses
the difference between the scattered ﬁeld and the estimated model output. However,
it is well-known that inverse diffraction problems, and especially nonlinear inversion
problems are also ill-posed, which means that their resolution requires regularization
that generally consists of the introduction of prior information about the sought
object. In this case, for example, we look for artiﬁcial objects that are composed of
homogeneous and compact regions made of a ﬁnite number of different materials,
and with the aforementioned deterministic methods, it is not easy to take into account
this prior information because it must be introduced into the cost function.
Instead, the probabilistic framework of the Bayesian estimation [IDI 08], on
which we developed the model presented here, is especially well suited for this
situation. The prior information is appropriately accounted for by a probabilistic
Gauss–Markov–Potts model [PIE 03, TIE 94]. The marginal contrast distribution is
modeled as a mixture of Gaussian laws [FÉR 02], where each Gaussian distribution
represents a class of materials and the compactness of the regions is taken into
account using a hidden Markov model. An unsupervised joint approach is adopted
for estimating the searched unknowns and all the parameters introduced into the prior
model.

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
203
Two iterative algorithms are proposed. The ﬁrst and rather conventional algorithm
is called the Monte-Carlo Markov chain methods (MCMC); it consists of expressing
all the joint posterior or conditional distributions of all the unknowns and then of using
the Gibbs sampling algorithm [ROB 04] for estimating the posterior mean (PM) of the
unknown values. This algorithm gives good results, but it is computationally intensive
mainly because the Gibbs sampling requires a signiﬁcant number of samples.
The second algorithm is based on variational Bayesian approximation (VBA)
[SMÍ 06]). The latter was ﬁrst introduced in the ﬁeld of Bayesian interference for
applications in neural networks [HIN 93], for learning graphic models [JOR 99] and
estimating model parameters [JAA 00]. Its appearance in the ﬁeld of inverse
problems is relatively recent, starting with source separation [CHO 02] and image
restoration [LIK 04]. It consists of approximating the joint posterior distribution of
all
the
unknowns
by
a
separable
free-form
distribution
minimizing
the
Kullback–Leibler divergence [KUL 51] with respect to the posterior distribution,
which has interesting properties for optimization and leads to an implicit parameter
optimization scheme. Once the approximating distribution is constructed, the
estimator can easily be obtained.
A solution to this functional optimization problem can be found in terms of
exponential distributions whose shape parameters are estimated iteratively. It must be
noted that at each iteration the expression of these parameters is similar to the one
that we would have obtained, if a gradient method was used to solve the optimization
problem. Moreover, the gradient and the step size have an interpretation in terms of
statistical moments (means, variances, etc.) [SAT 01].
Both algorithms introduced here are applied to two quite different conﬁgurations.
The conﬁguration related to microwave imaging is nearly-optimal:
we have
near-complete data at different frequencies. This means that scattered ﬁelds are
measured around the object for several illumination directions and at multiple
frequencies. The conﬁguration used in optical imaging is much less favorable since
the data are aspect-limited and only available at a single frequency. This means that
illuminations and measurements can only be performed in a limited angular sector.
This limited aspect of the data reinforces the ill-posedness of the inverse problem and
makes the introduction of prior information essential. However, we can see that in
both cases, satisfactory results are obtained.
This chapter consists of four parts. The ﬁrst part describes the experimental
conﬁgurations corresponding to the two studied applications: microwave imaging
and optical imaging. In the second part, we develop a modelization of the forward
problem, its discrete formulation and its validation against laboratory controlled data.
The third part deals with the Bayesian approach to inversion and description of the
MCMC and VBA algorithms. The fourth section presents the results obtained using

204
Regularization and Bayesian Methods for Inverse Problems
the two algorithms in microwave imaging and optical imaging and draws some
conclusions.
8.2. Measurement conﬁguration
The experimental setups considered in this chapter were developed at the Fresnel
Institute (Marseille). They are presented in more detail in [GEF 05] for microwave
imaging and in [MAI 10] for optical imaging. Both devices have common
characteristics that we model as follows. The objects are considered as cylinders with
an inﬁnite extension along the Oz axis and an arbitrary cross-section Ω invariant
along this axis. These objects are illuminated by a harmonic incident wave with an
angular frequency ω whose implicit temporal dependence is chosen as exp(-i ωt).
This incident wave can be regarded as a plane wave whose electric ﬁeld is polarized
parallel to the Oz axis so that a two-dimensional (2D) conﬁguration is considered in
a transverse magnetic polarization case that leads to a scalar formulation of the
electric ﬁelds. The different environments are assumed to be non-magnetic and
lossless (however, it must be noted that even if environments are lossy, it does not
cause any particular problem) and are characterized by their propagation constants
km
(m
=
1,2
or
Ω)
such
that
k2
m
=
ω2ε0εmμ0,
where
ε0
(ε0 = 8.854 × 10−12 F.m−1) and μ0 (μ0 = 1.256 × 10−6 H.m−1) are the dielectric
permittivity and magnetic permeability of vacuum and εm is the dielectric
permittivity relative to the environment Dm. The object is assumed to be contained in
the test domain D (D ⊂D1) in which we deﬁne a normalized contrast function χ,
such that χ(r) = (k2(r) −k2
1)/k2
1
= ε(r) −1. The latter accounts for the
electromagnetic parameters of the object and is null outside of Ω.
8.2.1. The microwave device
The microwave measuring system consists of a network analyzer coupled with
two horn antennas, of which one is for emission and the other is for reception. The
device operates at Nf = 9 frequencies ranging from 2 to 10 GHz. The studied object
M consists of three dielectric cylinders with parallel axes and circular cross-sections,
two of which are made of plastic and are identical, with a diameter of 3 cm and a
relative permittivity of εΩ = 3, and the third consists of foam and has a diameter of 8
cm and a relative permittivity of εΩ = 1.45 (Figure 8.1). This object is placed in air
(medium D1) and is illuminated at an angle of incidence θ1 that may vary around the
object. Thus, Nv = 18 views are performed at varying θ1, each view consisting of a
scattered ﬁeld measurement at a ﬁxed distance of r = 1.67 m for Nr = 241 different
observation directions θ, uniformly distributed in a range of 240◦(θ ∈θ1 ± 120◦).

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
205
8.2.2. The optical device
The optical measuring system consists of a helium-neon laser coupled with a
microscope operating in reﬂection mode and equipped with an interferometer that
can provide an accurate estimation of the measured ﬁeld phase. This device functions
at a frequency of 473.6 THz, which corresponds to a wavelength of 633 nm in
vacuum. The sought object is a typical version of the ones encountered in the ﬁeld of
nanotechnology. It consists of two resin rods with rectangular cross-sections,
arranged in parallel on a silicon substrate, whose dimensions are so large compared
to the rod sections that it can be regarded as having an inﬁnite extension. The
measurement conﬁguration can be modeled as follows (Figure 8.2): an object O
consisting of resin, whose relative permittivity is εΩ = 2.66, is located at the top
layer of a stratiﬁed medium,
consisting of two half-spaces separated by a
semi-inﬁnite interference plane γ12. The top half-space D1 is ﬁlled with air (ε1 = 1),
while the lower half-space D2 consists of silicon with a relative permittivity of
ε2 = 15.07. The rods are of the same height (0.14 μm), and their widths are 1 μm and
0.5 μm, respectively, and they are spaced 0.5 μm apart from each other. This object is
illuminated at an incidence θ1 varying in an interval of ±32◦. Thus, Nv = 8 views
are realized with a variable θ1, with each view consisting of measurements of the
reﬂected scattered ﬁeld at a ﬁxed distance r for Nr = 611 different observation
angles θ in a range of ±46◦.
3 cm
8 cm
2 cm
3 cm
 plastic
 plastic
 foam
M
Ω
x
y
θ
D1
air
θ1
incident wave
observation
D
Figure 8.1. Measurement conﬁguration for microwave imaging (a)
and object M geometry (b)
The data collected at the Fresnel Institute using these experimental conﬁgurations
(courtesy of K. Belkebir and M. Saillard for the microwave data and G. Maire,
K. Belkebir and A. Sentenac for the optical data) will allow us to validate our

206
Regularization and Bayesian Methods for Inverse Problems
forward model and test our inversion algorithms without committing an “inverse
crime” such as deﬁned by Colton and Kress [COL 92] that would consist of testing
the latter on synthetic data obtained by solving the forward problem with the help of
a method closely related to the one used to solve the inverse problem.
γ
12
silicon
1 μm
0.14 μm
resin
0.5 μm
O
0.5 μm
resin
Ω
x
y
θ
silicon
D2
air
γ
12
θ1
incident wave
observation
D
D1
Figure 8.2. The measurement conﬁguration for optical imaging
a) and the object O geometry b)
8.3. The forward model
The forward model is based on a domain integral representation of electric ﬁelds
obtained by applying Green’s theorem to the Helmholtz wave equations satisﬁed by
the ﬁelds and by accounting for the continuity and the radiation conditions [CHE 95,
COL 92]. This yields two coupled integral equations, the ﬁrst of which is called the
coupling equation (or the state equation) and which links the total electric ﬁeld E in
D to the Huygens type sources w(r′) induced in the object by the incident wave, with
w(r′) = χ(r′)E(r′), and χ is the contrast function. This equation can be written as:
E(r) = Einc(r) + k2
1

D
G(r,r′)w(r′) dr′,r ∈D
[8.1]
Einc and G(r,r′), which will be shown in more detail later, represent the incident
ﬁeld and Green’s function in the considered medium. The second equation is a
Fredholm integral equation of the ﬁrst kind, called the observation equation. It links
the scattered ﬁeld Edif observed in the measurement domain S to the induced sources
w(r′):
Edif(r) = k2
1

D
G(r,r′)w(r′) dr′,r ∈S
[8.2]
This model describes the two conﬁgurations considered here. They only differ in
the incident ﬁeld and Green’s function expressions because in one case (microwave)

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
207
the imaged object is located in a homogeneous medium, while in the other case
(optical) it is placed in a stratiﬁed medium.
8.3.1. The microwave case
Since the object is placed in a homogeneous medium, the incident ﬁeld and
Green’s function expressions are fairly simple. Thus, the incident ﬁeld, which is a
plane wave propagating in the direction θ1, can be written at point r = (x, y) as:
Einc(r) = exp(ik1(x cos θ1 + y sin θ1)),
[8.3]
while Green’s function G(r,r′), which represents the radiation of a line-source placed
in r′ and observed in r in the absence of an object, is given by:
G(r,r′) = i
4H1
0(k1 |r −r′|),
[8.4]
where H1
0 is the Hankel function of order zero of the ﬁrst kind.
8.3.2. The optical case
As the object is placed in a stratiﬁed medium, these expressions become more
complicated because they must take into account the reﬂections on the interface γ12.
Thus, the incident ﬁeld becomes:
Einc(r) = exp(−ik1(x cos θ1 + y sin θ1)) + R exp(ik1(x cos θ1 −y sin θ1))
R = k1 cos θ1 −k2 cos θ2
k1 cos θ1 + k2 cos θ2
, with θ2 such that k1 sin θ1 = k2 sin θ2
while Green’s function is expressed in the spectral domain associated with y, the axis
parallel
to
γ12,
as
a
spectrum
of
plane
waves
with
variable
incidence
[LES 91, SOU 96]:
G(r,r′) = 1
2π
 +∞
−∞
g(x,x′,α) exp (iα(y −y′)) dα
[8.5]
Each plane wave is reﬂected or transmitted at the interface γ12 and ﬁnally the
contributions of elementary plane waves are summed up at the observation point. Here,
accounting for the fact that the source point (r′) and the observation point (r) are both
located in the medium D1, the spectrum of plane waves g(x,x′,α) can be written as:
g(x, x′, α) =
i
2β1
	
exp(iβ1 |x −x′|) + β1 −β2
β1 + β2
exp(iβ1(x + x′))

[8.6]
βm =

k2m −α2 ,
ℑm(βm) ≥0 ,
m = 1, 2

208
Regularization and Bayesian Methods for Inverse Problems
It consists of two terms: the ﬁrst term represents the direct contribution, i.e. the
spectral expansion of Green’s function of the free space in the medium D1 [8.4], and
the second term takes into consideration the reﬂections on the interface γ12.
8.3.3. The discrete model
Assuming that both the contrast χ and the incident ﬁeld Einc are known, the
resolution of the forward problem consists of ﬁrst solving coupling equation [8.1] for
determining the sources w and then solving observation equation [8.2] to determine
the scattered ﬁeld Edif. This is done by using the discrete versions of these equations,
obtained by applying the method-of-moments with pulse basis functions and Dirac
distributions as test functions [GIB 07]. This implies that the domain D is partitioned
into ND elementary pixels small enough so that we can consider the electric ﬁeld and
the contrast as constant over each of them. Rewriting equation [8.1] to describe the
induced sources, we get the following two linear systems:
w(ri) = χ(ri)Einc(ri) + χ(ri)
ND

j=1
HD
ijw(rj),
i = 1, . . . , ND
[8.7]
Edif(θn) =
ND

j=1
HS
njw(rj),
n = 1, . . . , Nr
[8.8]
where the elements of the matrix HS
nj and HD
ij are obtained from the integration of
Green’s observation and coupling functions, respectively, over the elementary pixels.
8.3.3.1. The observation matrix
The calculation of the elements HS
nj does not pose particular problems because
in the microwave case, as well as in the optical case, an approximate analytical
expression of HS
nj can be obtained. Thus in the microwave case, it is common to
replace the integral of the square pixel by an integral over a disk of the same area
[RIC 65], and this yields:
HS
nj = iπΔk1
2
H1
0(k1 |rn −rj|) J1(k1Δ)
[8.9]
where J1 is the Bessel function of the ﬁrst kind and Δ = 2a/√π, where a is the
half-side of the pixel.
In the optical case, as the scattered ﬁeld is measured at points r = (r,θ) ∈S
located in the far ﬁeld, with a ﬁxed r and in direction θ such that x > x′. Hence, an
approximate analytical expression Go of G can be obtained in the spatial domain using

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
209
the stationary phase method [CLE 66] by introducing the asymptotic expansion of H1
0
for large arguments:
Go(θ,r′) = iQ(θ,x′)exp (−i(α(θ)y′ + π/4))
√8πk1
Q(θ,x′) = exp(−iβ1(θ)x′) + β1(θ) −β2(θ)
β1(θ) + β2(θ) exp(iβ1(θ)x′)
[8.10]
α(θ) = k1 sin(θ),
βm(θ) =
'
k2m −α(θ)2
for m = 1,2
where a constant term exp(−ik1r)/√r, accounted for elsewhere in the normalization
constants, was omitted. By integration, this leads to the coefﬁcients:
HS
nj = i
(
2k3
1
π
sin (β1(θn)a) sin (α(θn)a)
α(θn)β1(θn)
Q(θn,xj) exp
"
−i
"
α(θn)yj + π
4
##
8.3.3.2. The coupling matrix
Regarding the coupling matrix elements HD
ij, in the microwave case they are
obtained similarly to those of the observation matrix, but with a different expression
for the diagonal terms obtained when the source and the observation points are
colocated:
HD
ij =
*
iπΔk1H1
1(k1Δ)/2 −1
if i = j
iπΔk1H1
0(k1 |ri −rj|)J1(k1Δ)/2
otherwise
[8.11]
In the optical case, things are a bit more complicated because the Green’s
function G is known in the spectral domain and, for calculating coupling matrix
elements, the approximation [8.10] cannot be used because the observation point is
located in the near ﬁeld. However, knowing G in the spectral domain might not
necessarily be disadvantageous because this implies solving equation [8.7] by a
conjugate gradient fast Fourier transform method (CG-FFT –[SAR 86]), which
signiﬁcantly saves time by calculating the convolution and correlation products
appearing in this equation in the spectral domain and by solving the system using a
conjugate gradient method; such calculations are detailed in [LES 91] for a similar
conﬁguration to the one considered here. It must be noted that the two terms of the
Green’s function [8.6] must be treated separately because the direct contribution
presents a singularity at r = r′. The elements HDs
ij corresponding to the singular
contribution are given by [8.11] and are known in the spatial domain. Regarding the
coefﬁcients HDns
ij
corresponding to the non-singular part of the Green’s function,
their spectral counterparts hDns
ij
can be written as:
hDns
ij
= 2ik2
1 sin(αa) sin(β1a)
αβ2
1
β1 −β2
β1 + β2
exp (iβ1(xi + xj))
[8.12]

210
Regularization and Bayesian Methods for Inverse Problems
8.3.4. Validation of the forward model
Figure 8.3 presents forward model comparison results for the objects M and O.
The domain D is divided into ND square pixels with half-side a and is illuminated at a
frequency f in the direction θ1 such that ND = 64×64, a = 1.17 mm, f = 4 GHz and
θ1 = 40◦for M and ND = 512×32, a = 3.7 nm, f = 473.6 THz and θ1 = −22.24◦
for O. Generally, the scattered ﬁelds are relatively well described. However, it must
be noted that the ﬁelds measured with the optical device are very noisy, especially
near the specular directions, and that the data are missing in their immediate vicinity.
This can be explained by the fact that the scattered ﬁeld is negligible compared to
the incident ﬁeld. Therefore, it is difﬁcult to measure accurately in those directions
because it is obtained from the difference between the total ﬁeld and the incident ﬁeld.
-180
-90
0
90
180
100
160
220
280
340
-50
-40
-30
-20
-10
100
160
220
280
340
-180
-90
0
90
180
-40
-20
0
20
40
-15
-5
5
15
-40
-20
0
20
40
observation angle (°)
observation angle (°)
modulus (dB)
phase (°)
M
M
O
O
observation angle (°)
observation angle (°)
Figure 8.3. The magnitude (top) and the phase (bottom) of the calculated
scattered ﬁelds (dotted gray line) and the measured ones (continuous black line)
for the object M (left) and for the object O (right)

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
211
8.4. Bayesian inversion approach
Now, we focus on solving the inverse problem of estimating the contrast χ from
the scattered ﬁeld measurements Edif, with the incident ﬁeld Einc being known. It
should be noted that the induced sources w, also unknown, should be estimated
simultaneously with the contrast. First, we must deﬁne two vectors, ϵ and ξ, which
represent all the errors, i.e., the measurement uncertainties and the model errors due
to discretization and other approximations. Let us assume that these errors are i.i.d
Gaussian, centered and with variances ρ2
ϵ and ρ2
ξ, respectively. Accounting for the
different views (v = 1, . . . , Nv) and the different frequencies (microwave case
f = 1, . . . , Nf) using the index u = (1, . . . , Nv × Nf) and introducing the errors
deﬁned previously, the forward discrete model (equations [8.7] and [8.8]) can be
rewritten using matrix notation as follows:
Edif
u = HS
uwu + ϵu
[8.13]
wu = χEinc
u + χHD
f wu + ξu
[8.14]
Edif
u , Einc
u
and wu are the complex vectors that contain the scattered ﬁelds (the
data), the incident ﬁelds and the induced sources corresponding to the different views
and different frequencies; χ is a real vector that contains the contrast values at the
pixel centers; HS
u and HD
f are operators mapping L2(D) to L2(S) and L2(D) to
itself, respectively, and that are represented by large matrices whose elements are
HS
nj and HD
ij, respectively.
As noted previously, it is now necessary to take into consideration the prior
information available on the sought object. We know that it consists of a ﬁnite
number Nk of different materials. This prior information is introduced with the help
of a hidden variable z(r) associated with each pixel r. This label deﬁnes the different
material classes, and the pixels belonging to a given class k can be characterized by a
contrast distributed according to a Gaussian law:
p(χ(r) | z(r) = k) = N(mk,ρ2
k),
k = 1, . . . , Nk
[8.15]
with a mean value mk and a variance ρ2
k. Prior information that the different materials
are divided into compact regions is modeled by using a Markov–Potts ﬁeld for z which
describes the spatial dependence between neighboring pixels:
p(z | Υ) = 1
Ξ exp
⎛
⎝Υ
2
ND

i=1

rj∈V(ri)
δ [z(ri) −z(rj)]
⎞
⎠
[8.16]
where Ξ is a normalization constant, Υ is a parameter that determines the degree of
dependence or correlation between neighboring labels (here, Υ
=
2 and the

212
Regularization and Bayesian Methods for Inverse Problems
probability p(z | Υ) will be simply denoted as p(z)), δ(0) = 1 and δ(t) = 0 if t ̸= 0.
V(ri) is the neighborhood of ri, which in this case consists of the four closest pixels.
The probability distributions deﬁned previously involve a number of different
parameters, such as ρ2
ϵ, ρ2
ξ, mk and ρ2
k (k = 1, . . . , Nk), that from now on will be
called hyperparameters and are denoted by a vector ψ. However, the different means
mϱ
and
variances
ρ2
ϱ
(ϱ
=
χ,w
or
k)
are
grouped
into
vectors
mϱ = {mϱ(ri), i = 1, 2, . . . , ND or Nk}) and vϱ = {ρ2
ϱ(ri), i = 1, 2, . . . , ND or
Nk}), respectively, and in diagonal matrices Mϱ = Diag (mϱ) and Vϱ = Diag (vϱ)
built up from the elements of these vectors. It can be noted that an unsupervised
approach is adopted: the contrast χ, the induced currents w, the segmentation z and
the hyperparameters of the model ψ are estimated together. By using the Bayesian
rule, we obtain:
p(χ,w,z,ψ | Edif) ∝p(Edif | w,ψ) p(w | χ,ψ) p(χ | z,ψ) p(z) p(ψ)
[8.17]
In this equation, p(χ | z,ψ) and p(z) are given by equations [8.15] and [8.16],
whereas p(Edif | w,ψ) and p(w | χ,ψ) are obtained by the observation and coupling
equations, respectively. These distributions can be written as:
p(Edif | w,ψ) =

u
	
1
2πρ2ϵ

Nr/2
exp
	
−1
2ρ2ϵ
Edif
u −HS
uwu
2
S

p(wu | χ,ψ) ∝exp

−1
2ρ2
ξ
wu −χ Einc
u −χ HD
f wu
2
D

[8.18]
where ∥·∥A represents the norm associated with the scalar product ⟨·,·⟩A in L2(A)
(A = S or D). As for p(ψ), it is a collection of conjugate priors [BER 94]. This
means that the variances and the means follow the inverse-gamma (IG) and Gaussian
distributions, respectively:
p(ρ2
ϱ) = IG(ηϱ,φϱ) ∝ρ−2(ηϱ+1)
ϱ
exp

−φϱ/ρ2
ϱ

,
ϱ = ϵ, ξ, k
p(mk) = N(μk,τk),
k = 1, . . . , Nk
[8.19]
with the meta-hyperparameters (η, φ, μ, τ) ﬁxed in a way that satisﬁes an
uninformative distribution, i.e. ﬂat.
All the terms on the right-hand side of equation [8.17] are known, and this allows
us to obtain the left-hand side, i.e. the unnormalized posterior joint distribution of all
the unknowns. From this expression, different inferences can be made about these
unknowns. The conventional way is to deﬁne a point estimator, for example, the
maximum a posteriori (MAP) or the PM, but it is usually very difﬁcult to obtain
tractable expressions for such estimators. Therefore, the posterior distribution needs
to be approximated, either numerically using an MCMC sampling algorithm, or

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
213
analytically, using the VBA method, which will be discussed in more detail
thereafter.
8.4.1. The MCMC sampling method
Among the MCMC sampling methods [ROB 04], one consists of generating
samples according to the conditional posterior distribution, which corresponds to a
Gibbs sampling algorithm. We must, therefore, determine a number of these
conditional distributions.
The
distributions
p(Vk | Edif, χ,w,z,mk),
p(mk | Edif,
χ,w,z,Vk),
p(ρ2
ϵ | Edif, χ,w,z) and p(ρ2
ξ | Edif, χ,w,z) can easily be sampled because the choice
of conjugate priors for hyperparameters allows them to remain in the same family,
i.e. Gaussian for means and inverse-gamma for variances:
p(ρ2
ϱ | Edif, χ,w,z) ∝p(Edif | χ,w,ρ2
ϱ) p(w | χ,ρ2
ϱ) p(ρ2
ϱ) p(χ,z)
∝p(ρ2
ϱ) p(χ,z)

u
p(Edif
u | χ,wu,ρ2
ϱ) p(wu | χ,ρ2
ϱ)
= IG

ˇηϱ,ˇφϱ

,
ϱ = (ϵ, ξ)
[8.20]
p(Vk | Edif, χ,w,z,mk) = p(ρ2
k | Edif, χ,w,z,mk) = IG(ˇηk,ˇφk)
[8.21]
p(mk | Edif, χ,w,z,Vk) = p(mk | ρ2
k,Edif, χ,w,z) = N(ˇμk, ˇτk)
[8.22]
where, with the following notation:
Rk = {r; z(r) = k},nk = Card(Rk),χk =

Rk
χ(r)
nk
, s2
k =

Rk
(χ(r) −mk)2
nk
the parameters in the conditional posterior distribution can be written as:
ˇμk = nkχk + μkρ2
k/τk
nk + ρ2
k/τk
,
ˇτk =
ρ2
k
nk + ρ2
k/τk
,
ˇηk = ηk + nk
2
ˇφk = φk + nk s2
k
2
,
ˇηϵ = ηϵ + Nu(Nr + ND)
2
ˇφϵ = φϵ + 1
2

u
Edif
u −HS
uwu
2
S + 1
2γ

u
χEinc
u −wu + χHD
f wu
2
D
ˇηξ = ηξ + NuND
2
ˇφξ = φξ + 1
2

u
χEinc
u −wu + χHD
f wu
2
D
with γ = ρ2
ξ/ρ2
ϵ.

214
Regularization and Bayesian Methods for Inverse Problems
The posterior distribution of the segmentation p(z | Edif, χ,w,ψ) is a Markov
ﬁeld with the same neighborhood as before (four pixels); the sampling of this
distribution can be achieved using a two-step procedure [BES 74]. First of all, the
pixels are divided up like a chessboard. It must be noted that zb and zn are the white
and black pixels, respectively. The four neighbors of each white pixel are black and
vice versa. So given black pixels zn, white pixels zb are independent and can be
sampled simultaneously and vice versa. The sampling of p(z | Edif, χ,w,ψ) is then
carried out using a Gibbs sampling algorithm, by alternatively drawing samples of zb
given zn and zn given zb.
Regarding wu, its posterior distribution can be written as:
p(wu | Edif
u ,χ,z,ψ) ∝p(Edif
u | wu,ρ2
ϵ) p(wu | χ,ρ2
ϵ) ∝exp
	
−Ju(wu)
2ρ2ϵ

[8.23]
with Ju(wu) =
Edif
u −HS
uwu
2
S + 1
γ
χEinc
u −wu + χHD
f wu
2
D
Thus, the posterior distribution of wu is Gaussian. Its sampling requires the
knowledge of its mean and covariance matrix. The calculation of the latter requires
the inversion of a large matrix. To avoid this, the sampling of this distribution is
achieved using a perturbed gradient method [ORI 12], which consists of minimizing
the criterion Ju (see equation [8.23]).
We can obtain the distribution p(χ | Edif, w, z,ψ) using the same method:
p(χ | Edif, w,z,ψ) ∝exp
	
−1
2ρ2
ξ

u
∥χ Eu −wu∥2
D −1
2B(χ)

[8.24]
where Eu = Einc
u
+ HD
f wu and B(χ) = (χ −mχ)t V−1
χ (χ −mχ), while
superscript t indicates the vector transpose.
8.4.2. The VBA method
The idea here is to approximate the posterior distribution p(w,χ,z,ψ | Edif) using
a free-form separable distribution q(w,χ,z,ψ) that minimizes the Kullback–Leibler
divergence KL(q||p) =
4
q ln (q/p). First, we must deﬁne the separation form:
q(w,χ,z,ψ) =

i
q(wi)

j
q(χj)

l
q(zl)

k
q(ρ2
k)q(mk)q(ρ2
ϵ)q(ρ2
ξ)
[8.25]

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
215
Then, the optimal form of q is found such that it minimizes the Kullback–Leibler
divergence. With the chosen forward model (equations [8.13] and [8.14]), the
Gaussian distributions deﬁned by [8.18] and the conjugate distributions [8.19] chosen
for
the
hyperparameters,
this
approach
leads
to
the
following
parametric
distributions:
q(w) = N( ˜mw,˜Vw),
q(χ) = N( ˜mχ,˜Vχ),
q(z) =

r
˜ζk(r)
q(mk) = N(˜μk,˜τ k),
q(ρ2
k) = IG(˜ηk,˜φk),
k = 1, . . . , Nk
[8.26]
q(ρ2
ϱ) = IG(˜ηϱ,˜φϱ),
ϱ = ϵ,ξ
where tilded parameters are mutually dependent and iteratively calculated. The
expressions of these parameters at iteration n are detailed further on. It must be noted
that superscript (n −1) is omitted for clarity reasons, which means that the values of
these superscript-less parameters are those obtained at iteration (n−1). We obtain the
following expressions for the distributions of:
– the hidden ﬁeld z:
q(z) ∝

r
exp
"
−1
2
˜ζk(r)
#
[8.27]
˜ζk(r) = Ψ(˜ηk) + ln ˜φk + ρ−2
k

( ˜mχ(r) −˜μk)2 + ˜τk + ˜vχ(r)

−Υ

r′∈V(r)
˜ζk(r′)
where Ψ is the digamma function and where the overline indicates the expectation of
the variable with respect to q (i.e. ¯a = Eq(a));
– the variance of the observation noise ρ2
ϵ:
q(ρ2
ϵ) = IG(˜ηϵ,˜φϵ),
˜φϵ = φϵ + Nr/2
[8.28]
˜ηϵ = ηϵ+ 1
2
"
∥Edif∥2
S +
HS ˜mw
2
S −2ℜe

(Edif)†HS ˜mw

+∥HS2˜vw∥1
#
where the exponent † denotes the conjugate transpose, ∥·∥1 denotes the L1 norm and
where the elements of HS2 are the squares of the elements of HS;
– the variance of the coupling noise ρ2
ξ:
q(ρ2
ξ) = IG(˜ηξ,˜φξ),
˜φξ = φξ + ND/2
[8.29]
˜ηξ = ηξ + 1
2
"
∥˜mw∥2
D + ∥˜Vw∥1 + ∥( ˜M2
χ + ˜Vχ)E2∥1 −2ℜe
 ˜m†
χwE∗#
where ∗denotes the complex conjugate and ˜M2
χ = ˜M∗
χ ˜Mχ;

216
Regularization and Bayesian Methods for Inverse Problems
– the variances of the classes ρ2
k:
q(ρ2
k) = IG(˜ηk,˜φk),
k = 1, . . . , Nk,
˜φk = φk +
ND

i=1
˜ζk(ri)/2
[8.30]
˜ηk = ηk + 1
2
ND

i=1
˜ζk(ri)

˜m2
χ(ri) + ˜vχ(ri) + ˜μ2
k + ˜τk −2˜μk ˜mχ(ri)

– the means of the classes mk:
q(mk) = N(˜μk,˜τk),
k = 1, . . . , Nk
˜τk =
	
τ −1
k
+ ρ−2
k
ND

i=1
˜ζk(ri)

−1
,
˜μk = ˜τk
	μk
τk
+ ρ−2
k
ND

i=1
˜ζk(ri) ˜mχ(ri)

[8.31]
– the induced currents w:
q(w) =
ND

i=1
q(w(ri)) = N( ˜mw,˜Vw),
˜Vn
w =
"
Diag
"
ρ−2
ϵ
ΓS + ρ−2
ξ
ΓD
χ
##−1
˜mn
w = ˜mw +
"
˜Vn
w
# 	
ρ−2
ϵ HS† 
Edif −HS ˜mw

+ ρ−2
ξ
"
˜MχEinc + ˜MχHD ˜mw
−˜mw −HD† ˜YχEinc + HD† ˜M†
χ ˜mw −HD† ˜YχHD ˜mw
#
[8.32]
where ˜Yχ = ( ˜M2
χ + ˜Vχ) and ΓS and ΓD
χ are such that:
ΓS(rj) =
Nr

i=1
|HS
ij|2
ΓD
χ (rj) = 1 −2ℜe

HD
jj ˜mχ(rj)

+ (| ˜mχ(rj)|2 + ˜vχ(rj))
ND

i=1
|HD
ij|2
– the contrast χ:
q(χ) =
ND

i=1
q(χ(ri)) = N( ˜mχ,˜Vχ)
[8.33]
˜mn
χ = (˜Vn
χ)
	
k
ρ−2
k ˜μk˜ζk + ρ−2
ξ wE∗

,
˜Vn
χ =
"
ρ−2
ξ
E2 + V−1
χ
#−1

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
217
where wE∗, the expectation of the vector wE∗, is such that:
wE∗(ri) =
Nu

1
Einc∗
u
(ri) ˜mw(ri) + ˜mw(ri)
ND

j=1
HD∗
ij ˜m∗
w(rj) + HD∗
ii
˜vw(ri)
and V−1
χ
and E2 are the diagonal matrices whose elements can be written as:

V−1
χ

ii =

k
˜ζk(ri)ρ−2
k ,

E2
ii =
Nu

1
Einc
u (ri)
2 + A
A = 2ℜe

Einc ∗(ri)HD
ii ˜mw(ri)

+

ND

j=1
HD
ij ˜mw(rj)

2
+
ND

j=1
HD
ij
2˜vw(rj)
8.4.3. Initialization, progress and convergence of the algorithms
The two algorithms described previously are iterative and require an initialization
of the different variables (χ(0),w(0),z(0),ψ(0)). The initial estimate of the sources
w(0) is obtained by back-propagation of the scattered ﬁeld from the measurement
domain S onto the test domain D [DUC 04]:
w(0)
u
= ΓHS†
u Edif
u
[8.34]
where Γ is a constant determined by minimizing 
u
Edif
u −ΓHS
uHS†
u Edif
u
2
S and
HS† is the adjoint operator of HS mapping L2(S) to L2(D), such that:
5
wu,HS†
u Edif
u
6
D =
5
HS
uwu,Edif
u
6
S
MCMC
VBA
Step Variable
Sampling of
Eq.
Updating
Eq.
1
z(n)
p(z | Edif,χ,ψ)
˜ζn
k
[8.27]
2
ρ2ϵ
(n)
p(ρ2
ϵ | Edif, χ, 
w,z(n))
(8.20)
˜φn
ϵ and ˜ηn
ϵ
[8.28]
3
ρ2
ξ
(n)
p(ρ2
ξ | Edif, χ, 
w,z(n))
(8.20)
˜φn
ξ and ˜ηn
ξ
[8.29]
4
V(n)
k
p(Vk | Edif, χ, 
w,z(n))
(8.21)
˜φn
k and ˜ηn
k
[8.30]
5

m(n)
k
p(mk | Edif, χ, 
w,z(n),V(n)
k ) (8.22)
˜τ n
k and ˜μn
k
[8.31]
6

w(n)
p(w | Edif, χ,z(n),ψ
(n))
(8.23)
˜Vn
w and ˜mn
w [8.32]
7
χ(n)
p(χ | Edif, 
w(n),z(n),ψ
(n))
(8.24) ˜Vn
χ and ˜mn
χ [8.33]
Table 8.1. MCMC and VBA algorithms and reference equations

218
Regularization and Bayesian Methods for Inverse Problems
The ﬁeld E(0)
u
results directly from the coupling equation:
E(0)
u
= Einc
u + HD
f w(0)
u
[8.35]
and χ(0) is then obtained by minimizing the error in the constitutive relation of the
induced sources w = χE, or, more accurately, by minimizing the regularized
criterion:

u
"χ E(0)
u
−w(0)
u
2
D + σu ∥χ∥2
D
#
[8.36]
where σu is a regularization constant empirically set at σu = 0,1
Einc
u
 in the optical
case and at zero in the microwave case. It must be noted that the presence of a
regularization term is required in the optical case because arbitrarily small values of
E(0)
u
can be obtained near the interface γ12 which would lead to inﬁnite initial values
for the contrast in the absence of regularization. Taking into account that the contrast
is real and positive leads to:
χ(0) =

u ℜe(w(0)
u E⋆(0)
u
)

u
"E(0)
u
2 + σu
#
χ≥0
[8.37]
where
χ≥0 = 1 if χ ≥0 and
χ≥0 = 0 if χ < 0. Once χ(0) and w(0) are known, the
segmentation z and the hyperparameters can be initialized by means of a segmentation
method. The K-means algorithm [MAC 67] is used here with an empirical estimator
to estimate the classes and their centers so that their variances are minimal, given the
number of classes.
Once initialized, the algorithms can proceed as summarized in Table 8.1. Based
on the contrast χ(n−1), the sources w(n−1), the hidden ﬁeld z(n−1) and the
hyperparameters ψ(n−1) determined at iteration (n −1), the variables of the second
column are either sampled according to the distributions indicated in the third
column given by equations designated in the fourth column (MCMC algorithm), or
updates will be made based on the shape parameters of the ﬁfth column, calculated
by means of the equations of the sixth column (VBA algorithm).
Steps 1–7 are iterated until convergence is reached. Several methods exist for
evaluating convergence (for example, an autocorrelation test for MCMC or the
negative free energy for the VBA). Here, we opted for an empirical criterion based on
the evolution of the hyperparameters. The latter is illustrated in Figure 8.4 through
some of them. In the case of the VBA algorithm, there is no minimum number of
iterations to be performed, and in the case considered here, convergence is reached
after nearly a hundred iterations (see Figure 8.4 (bottom)). However, with the MCMC

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
219
algorithm, theoretically, many iterations are carried out, ﬁrst without keeping track of
the sampling until a certain level of convergence is reached (which corresponds to the
burn-in time of the Gibbs sampler) and then recording them by calculating their
means and variances. However, in the studied case, we can see that variables and
hyperparameters do not change signiﬁcantly after approximately 250 iterations (see
Figure 8.4 (top)); therefore the maximum number of iterations has been set to 500
and the PM is estimated based on the last hundred samples.
Figure 8.4. The evolution of the hyperparameters for the MCMC (top) and the VBA (bottom)
algorithms applied to the microwave [left, mean of the class 2 (foam: gray line:) and 3 (plastic:
black line)] and optical [right, mean (gray line) and variance (black line) of class 2 (resin)]
conﬁgurations

220
Regularization and Bayesian Methods for Inverse Problems
8.5. Results
Figure 8.5 shows the results obtained with the two algorithms described
previously for the objects M and O. In the microwave case, the test domain D
consists
of
51 × 51
pixels
with
the
half-side
a
=
1.75
mm,
i.e.
D = 17.85 × 17.85 cm2, while in the optical case it is 32 × 512 pixels with the
half-side a = 3.7 nm, i.e. D = 0.237 × 3.789 μm2. In all cases, the two algorithms
are able to reconstruct the homogeneous regions that correspond to different
materials with relatively accurate contrast values, as shown in Figure 8.5 (bottom)
which shows the reconstructed proﬁles along the axis of symmetry of the object M,
or at a height of x = 0.1 μm for the object O, compared to the real proﬁles. The
shape of the reconstructed objects is sometimes slightly different from the real shape,
but the results are far more accurate than the images obtained using deterministic
methods such as CSI [ABU 05, AYA 12] or MGM [DUB 04, MAI 09].
8.6. Conclusions
In this chapter, we addressed microwave and optical imaging as inverse scattering
problems that are known as nonlinear ill-posed problems. The objects considered
here have a signiﬁcant dielectric contrast that rules out the use of small perturbation
approximation, such as the Born or Rytov ones, which linearizes the inverse problem.
The latter is a nonlinear problem modeled by two coupled integral equations linking
the measured scattered ﬁeld to the sources induced within the object by the incident
wave, while the induced sources and the contrast are unknown.
As for the ill-posedness, the inverse problem must be regularized prior to its
resolution and this regularization generally consists of introducing prior information
about the sought solution. The provision of such information is particularly necessary
in the case of optical imaging, where the aspect-limited data enhances the
ill-posedness of the inverse problem. The latter consists of imaging objects composed
of a ﬁnite number of different materials, which is an important prior information.
This means that the unknown image is composed of a ﬁnite number of homogeneous
regions. This prior information is taken into account using the Gauss–Markov–Potts
model for the contrast distribution, developed in a Bayesian estimation framework.
The Bayesian approach presented here gives better results, both in microwave and
optical imaging, compared to those obtained using deterministic iterative methods,
such as CSI or MGM. Moreover, compared to the latter, it has the advantage of
providing not only an estimation of the contrast distribution but also the segmentation
into regions and the values of the parameters (means and variances) in each class. In
some applications, this segmentation is even more important than the reconstruction
itself.

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
221
-7.5
-2.5
2.5
7.5
MCMC
x (μm)
x (μm)
y (μm)
VBA
y (μm)
0
0.5
1
1.5
2
-1.5
-0.5
0.5
1.5
y (μm)
χ
actual profile
MCMC
VBA
0
0.05
0.1
0.15
0.2
0
0.3
0.6
0.9
1.8
1.2
1.5
y (cm)
x (cm)
0
0.4
0.8
2
1.2
1.6
MCMC
M
O
y (cm)
x (cm)
VBA
0
0.5
1
1.5
2
2.5
-7.5
-2.5
2.5
7.5
y (cm)
χ
actual profile
MCMC
VBA
-7.5
-2.5
2.5
7.5
-7.5
-2.5
2.5
7.5
-1.5
-0.5
0.5
1.5
-1.5
-0.5
0.5
1.5
0
0.05
0.1
0.15
0.2
-7.5
-2.5
2.5
7.5
Figure 8.5. Contrast of objects M (left) and O (right) obtained with MCMC (top) and VBA
(middle) and reconstructed proﬁles along the dashed lines (bottom)

222
Regularization and Bayesian Methods for Inverse Problems
Finally, two variants of this approach have been examined: the conventional
method, called MCMC, based on the Gibbs sampling algorithm that results in a
numerical approximation of the joint posterior of the unknowns, and the VBA, which
makes possible analytical approximation of the latter. Although these two variants
lead to comparable results, the latter converges much faster than the former which is
important when studying complicated conﬁgurations like the one considered in
optical imaging, since working with objects located in a stratiﬁed medium requires a
signiﬁcant amount of calculations.
8.7. Bibliography
[ABU 05] ABUBAKAR A., VAN DEN BERG P.M., HABASHY T., “Application of the
multiplicative regularized contrast source inversion method on TM- and TE-polarized
experimental Fresnel data”, Inverse Problems, vol. 21, no. 6, pp. S65–S80, 2005.
[AYA 12] AYASSO H., DUCHÊNE B., MOHAMMAD-DJAFARI A., “Optical diffraction
tomography within a variational Bayesian framework”, Inverse Problems in Science and
Engineering, vol. 20, no. 1, pp. 59–73, 2012.
[AZI 83] AZIMI M., KAK A.C., “Distorsion in diffraction tomography caused by multiple
scattering”, IEEE Transactions on Medical Imaging, vol. 2, no. 4, pp. 176–195, 1983.
[BEL 03] BELKEBIR K., SENTENAC A., “High-resolution optical diffraction microscopy”,
Journal of the Optical Society of America (A), vol. 20, no. 7, pp. 1223–1229, 2003.
[BER 94] BERNARDO J.M., SMITH A.F.M., Bayesian Theory, Wiley, Chichester, UK, 1994.
[BER 97] VAN DEN BERG P.M., KLEINMAN R.E., “A contrast source inversion method”,
Inverse Problems, vol. 13, no. 6, pp. 1607–1620, 1997.
[BES 74] BESAG J.E., “Spatial interaction and the statistical analysis of lattice systems (with
discussion)”, Journal of the Royal Statistical Society B, vol. 36, no. 2, pp. 192–236, 1974.
[CHE 95] CHEW W., Waves and Fields in Inhomogeneous Media, IEEE Press, New York,
1995.
[CHO 02] CHOUDREY R., Variational methods for Bayesian independent component analysis,
PhD Thesis, Oxford University, 2002.
[CLE 66] CLEMMOW P.C., The Plane Wave Spectrum Representation of Electromagnetic
Fields, Pergamon Press, Oxford, 1966.
[COL 92] COLTON D., KRESS R., Inverse Acoustic and Electromagnetic Scattering Theory,
Springer Verlag, New York, 1992.
[DEV 84] DEVANEY A.J., BEYLKIN G., “Diffraction tomography using arbitrary source-
receiver surfaces”, Ultrasonic Imaging, vol. 6, pp. 181–193, 1984.
[DUB 04] DUBOIS A., BELKEBIR K., SAILLARD M., “Retrieval of inhomogeneous targets
from experimental frequency diversity data”, Inverse Problems, vol. 21, no. 6, pp. S65–S79,
2004.

MCMC and Variational Approaches for Bayesian Inversion in Diffraction Imaging
223
[DUC 04] DUCHÊNE B., JOISEL A., LAMBERT M., “Nonlinear inversions of immersed
objects using laboratory-controlled data”, Inverse Problems, vol. 20, no. 6, pp. S81–S98,
2004.
[FÉR 02] FÉRON O., MOHAMMAD-DJAFARI A., “Image fusion and joint segmentation using
an MCMC algorithm”, Journal of Electronic Imaging, vol. 14, no. 2, pp. 1–12, 2002.
[GEF 05] GEFFRIN J.-M., SABOUROUX P., EYRAUD C., “Free space experimental scattering
database continuation: experimental set-up and measurement precision”, Inverse Problems,
vol. 21, no. 6, pp. S117–S130, 2005.
[GIB 07] GIBSON W.C., The Method of Moments in Electromagnetics, Chapman & Hall/CRC,
Boca Raton, 2007.
[HIN 93] HINTON G.E., VAN CAMP D., “Keeping the neural networks simple by minimizing
the description length of the weights”, 6th Annual Conference on Computational Learning
Theory, ACM, New York, pp. 5–13, 1993.
[IDI 08] IDIER J., (ed.), Bayesion Approach to Inverse Problems, ISTE, London and John
Wiley & Sons, New York, 2008.
[JAA 00] JAAKKOLA T.S., JORDAN M.I., “Bayesian parameter estimation via variational
methods”, Statistics and Computing, vol. 10, no. 1, pp. 25–37, 2000.
[JOA 91] JOACHIMOWICZ N., PICHOT C., HUGONIN J.-P., “Inverse scattering: an iterative
numerical method for electromagnetic imaging”, IEEE Transactions on Antennas and
Propagation, vol. 39, no. 12, pp. 1742–1752, December 1991.
[JOR 99] JORDAN M.I., GHAHRAMANI Z., JAAKKOLA T.S., et al., “An introduction to
variational methods for graphical models”, Machine Learning, vol. 37, no. 2, pp. 183–233,
1999.
[KLE 92] KLEINMAN R.E., VAN DEN BERG P.M., “A modiﬁed gradient method for
two-dimensional problems in tomography”, Journal of Computational and Applied
Mathematics, vol. 42, pp. 17–35, 1992.
[KUL 51] KULLBACK S., LEIBLER R.A., “On information and sufﬁciency”, The Annals of
Mathematical Statistics, vol. 22, pp. 79–86, 1951.
[LES 91] LESSELIER D., DUCHÉNE B., “Buried, 2-D penetrable objects illuminated by line-
sources: FFT-based iterative computations of the anomalous ﬁeld”, in SARKAR T., (ed.),
Application of Conjugate Gradient Methods to Electromagnetics and Signal Analysis,
Elsevier, New York, vol. 5, pp. 400–438, 1991.
[LIK 04] LIKAS A.C., GALATSANOS N.P., “A variational approach for Bayesian blind image
deconvolution”, IEEE Transactions on Signal Processing, vol. 52, no. 8, pp. 2222–2233,
2004.
[MAC 67] MACQUEEN J., “Some methods for classiﬁcation and analysis of multivariate
observations”, 5th Berkeley Symposium on Mathematical Statistics and Probability,
Berkeley, CA, pp. 281–297, 1967.
[MAI 09] MAIRE G., DRSEK F., GIRARD J., et al., “Experimental demonstration of
quantitative imaging beyond Abbe’s limit with optical diffraction tomography”, Physical
Review Letters, vol. 102,p. 213905, 2009.

224
Regularization and Bayesian Methods for Inverse Problems
[MAI 10] MAIRE G., GIRARD J., DRSEK F., et al., “Experimental inversion of optical
diffraction tomography data with a nonlinear algorithm in the multiple scattering regime”,
Journal of Modern Optics, vol. 57, no. 9, pp. 746–755, 2010.
[MUE 80] MUELLER R., KAVEH M., INVERSON R., “A new approach to acoustic
tomography using diffraction techniques”, in METHERELL A., (ed.), Acoustical Imaging,
Plenum Press, New York, vol. 8, pp. 615–628, 1980.
[ORI 12] ORIEUX F., FÉRON O., GIOVANNELLI J.-F., “Sampling high-dimensional Gaussian
distributions for general linear inverse problems”, IEEE Signal Processing Letters, vol. 19,
no. 5, pp. 251–254, 2012.
[PIE 03] PIECZYNSKI W., “Modèles de Markov en traitement d’images”, Traitement du
Signal, vol. 20, no. 3, pp. 255–278, 2003.
[RIC 65] RICHMOND J., “Scattering by a dielectric cylinder of arbitrary cross-section shape”,
IEEE Transactions on Antennas and Propagation, vol. 13, no. 3, pp. 334–341, 1965.
[ROB 04] ROBERT C.P., CASELLA G., Monte Carlo Statistical Methods, Springer Texts in
Statistics, Springer Verlag, New York, 2nd ed., 2004.
[SAR 86] SARKAR T.K., ARVAS E., RAO S.M., “Application of FFT and the conjugate
gradient method for the solution of electromagnetic radiation from electrically large and
small conducting bodies”, IEEE Transactions on Antennas and Propagation, vol. 34, no. 5,
pp. 635–640, 1986.
[SAT 01] SATO M., “Online model selection based on the variational Bayes”, Neural
Computation, vol. 13, no. 7, pp. 1649–1681, July 2001.
[SCH 92] SCHATZBERG A., DEVANEY A., “Super-resolution in diffraction tomography”,
Inverse Problems, vol. 8, no. 1, pp. 149–164, 1992.
[SLA 84] SLANEY M., KAK A.C., LARSEN L.E., “Limitations of imaging with ﬁrst-order
diffraction tomography”, IEEE Transactions on Microwave Theory and Techniques, vol. 32,
no. 8, pp. 860–874, August 1984.
[SMÍ 06] SMÍDL V., QUINN A., The Variational Bayes Method in Signal Processing, Springer
Verlag, Berlin, 2006.
[SOU 96] SOURIAU L., DUCHÊNE B., LESSELIER D., et al., “A modiﬁed gradient approach
to inverse scattering for binary objects in stratiﬁed media”, Inverse Problems, vol. 12, no. 4,
pp. 463–481, 1996.
[TIE 94] TIERNEY L., “Markov chain for exploring posterior distribution”, Annals of
Statistics, vol. 22, no. 4, pp. 1701–1762, December 1994.

9
Variational Bayesian Approach and
Bi-Model for the
Reconstruction-Separation of Astrophysics
Components
9.1. Introduction
A critical issue currently in astrophysics is the determination of the structure of
the universe just after the Big Bang. This knowledge involves the study of
ﬂuctuations of the cosmic microwave background in order to extract information
related to fossil energy. Two satellites, Herschel and Planck, carrying space
telescopes, were sent in 2009 in order to measure this background. Thus, the Planck
satellite is dedicated to the accurate measurement of the cosmic background. The
image obtained by this satellite covers the entire universe but this result is obtained at
the expense of spatial resolution. One of the major problems, if the objective is to
study the cosmic microwave background, is the presence of a foreground made up of
stars, galaxies, etc.
The problem that is of concern to us in this chapter is consequently the separation
of the two components in a satellite image, in order to allow the separate study of the
cosmic microwave background and the foreground, thus of point sources. The
objective is to develop algorithms that facilitate the separation of medium- and
low-energy point sources because the tools developed by the European Space Agency
(ESA) are not able to do so. The idea is to use data from the Herschel telescope that
have a much better resolution to create a map of point sources and a map of the
background. This map then helps to correct the measurements of the Planck satellite.
Chapter written by Thomas RODET, Aurélia FRAYSSE and Hacheme AYASSO.

226
Regularization and Bayesian Methods for Inverse Problems
In signal processing terms, this problem can be formalized as both a separation
and a superresolution problem applied to large-sized data. Moreover, as we will
detail later, the problem of component separation is ill-posed in the sense that there is
no information in the data distinguishing a regular component,
the cosmic
background, from an impulse component, the foreground. The separation in this case
is only possible through the addition of a priori discriminating information. The
consequence of this state of affairs is that the hyperparameters of the method are
relatively difﬁcult to manually adjust. That is why we will subsequently develop an
unsupervised approach to overcome this adjustment. The objective is therefore to
build an over-resolved and unsupervised inversion method, and to jointly solve a
problem of separation between a correlated extended source and a separable impulse
source. In addition, the developed method must be fast and effective with large-sized
data. Typically, for data from the Planck satellite, there are up to 100 million data
items and 50 million unknowns to process.
Then, we will use a Bayesian methodology to solve this problem, and more
precisely a fully Bayesian approach that will also facilitate the estimation of
hyperparameters. Therefore, we will use the a priori information linked to the
problem. This will enable us, by means of Bayes theorem, to determine the a
posteriori distribution. In general, this a posteriori distribution is known only up to a
normalization constant. This a posteriori distribution will then allow us to calculate a
point estimator. In general, this estimator is built by looking for the maximum or
mean a posteriori. When searching for the maximum a posteriori, the problem is one
of the general non-convex optimizations. In the case of the a posteriori mean, an
integral should be calculated. Because non-convex optimization is difﬁcult in
practice, a large number of recent works have rather focused on the estimator of the a
posteriori mean using Stochastic Markov chain Monte Carlo (MCMC) methods
[MAZ 05, ROB 97, ROB 04]. The concept of MCMC methods is to generate a chain
of samples which asymptotically converge toward samples following the target
distribution (a posteriori distribution). Then, only the samples obtained from the a
posteriori distribution are retained after a certain period of time, which is called
burn-in time. By empirically averaging a large number of these samples, a very good
estimator of the a posteriori mean is obtained. There are mainly two types of MCMC
algorithms: the Metropolis-Hastings algorithms and Gibbs sampling. The principle
of Metropolis–Hasting’s methods is to generate a result with a simpler distribution
that can be easily sampled. This sample is then accepted with a certain probability.
The main limitation of this method is the dimension of the space to explore which
can become important when the number of sources increases. Therefore, it is almost
impossible to ﬁnd a propositional distribution close enough to the target distribution
with respect to a problem of large dimensions. With regard to the Gibbs sampler, the
principle is to obtain samples with conditional distributions. When the chain has
converged, the algorithm generates samples under the joint a posteriori distribution.
In a problem with a large number of unknowns, this algorithm converges extremely

Variational Bayesian Approach and Bi-Model
227
slowly if each unknown is conditionally sampled in turn. On the other hand, it
becomes effective when it is possible to jointly sample a large number of unknowns,
which is the case if the joint distribution is separable or if it is Gaussian with an
easily invertible covariance matrix. For example, in the case of deconvolution
problems where the covariance matrix is diagonalizable by Fourier transform, there
are
many
fully
Bayesian
approaches
developed
with
this
method
[GIO 08, MAZ 05, ORI 10]. On the other hand, it is extremely difﬁcult to effectively
generate a large-dimensional sample set using a correlation distribution if the
covariance matrix has no particular structure that makes it easily invertible or if the
inverse of the covariance matrix is not hollow. Moreover, these MCMC approaches
require the exploration of the possibilities space. This step requires a large number of
calculations for obtaining samples that may have a low probability.
MacKay [MAC 03] suggested an alternative to these sampling methods, called
variational Bayesian approach [MAC 03]. This approach was also introduced in
another form in the computer science community [JOR 99]. In the signal processing
ﬁeld, this methodology has been applied to various topics such as the separation of
sources [CHO 02, MIS 00], the choice of model in order to decompose a signal as a
sum of AutoRegressive (AR) signals [DEM 07, MOR 11],
the reduction of
hyperspectral images [BAL 08], problems of deconvolution [BAB 09, CHA 08] and
the problem of myopic deconvolution due to a motion blur [DEM 08, FER 06]. The
variational Bayesian approach is less generic than the MCMC approaches, and more
approximative in theory, but it is more effective in practice especially in large
dimensions. It is limited because it does only solve inverse problems when the a
posteriori distribution belongs to a family of conjugate distributions with respect to
the likelihood. But since the calculations are analytical, the speed of convergence is
clearly faster than MCMC approaches. The objective of this methodology is to
approach the posterior distribution with separable distributions. The separable
function is closest to the posterior distribution by minimizing the Kullback–Leiber
divergence. It is therefore necessary to solve an optimization problem in an
inﬁnite-dimensional space, namely a functional space. In practice, functional
minimization is carried out using each separable component in turn, in an alternate
manner. This minimization is a functional version of the Gauss–Seidel minimization
algorithm in a ﬁnite-dimensional space [NOC 00]. In the case of a ﬁnite-dimensional
space, it is known that this type of algorithm converges slowly when the number of
unknowns is very large. In the same way, variational Bayesian algorithms are less
efﬁcient when the approaching function is factored into a large number of separable
functions.
This shift from variational Bayesian approaches to large-sized problems is made
possible due to recent works, which is detailed in [FRA 11, FRA 12], and that will be
summarized here. The idea is to transpose conventional gradient descent approaches
with an optimal step, which is well known in ﬁnite-dimensional spaces [NOC 00], into
the probability distribution space, which is of inﬁnite dimension.

228
Regularization and Bayesian Methods for Inverse Problems
We have here applied this methodology to solving the inverse problem of
astrophysical imaging, where the direct model is linear in the presence of Gaussian
noise. Therefore, we use a Gaussian correlated a priori to estimate the extended
source and a separable a priori promote the sparsity to estimate the impulse
component.
This
bi-model
has
already
been
introduced
in
the
works
of
[CIU 99, DEM 04, GIO 05].
With regard to the sparse component, many approaches reveal a regularization
term such as ℓ1 [CHE 96] whose Bayesian interpretation is the determination of a
maximum a posteriori (MAP) using Laplace a priori. More generally, sparse a priori
is inserted using heavy-tailed distributions. These distributions are, for example, the
Laplace distribution, the Bernoulli–Gauss distribution [GOU 90, RAB 07], ﬁnite
Gaussian mixtures [ICH 06, SNO 02], the Cauchy distribution, α-stable distributions
and more generally, some distributions from the family of inﬁnite Gaussian mixtures
(Gaussian scale mixture (GSM)) [CHA 04, WAI 00]. Later on in the chapter, we
have chosen to model sparse information using student’s t-distributions, which can
also be written in the form of GSMs. This choice is justiﬁed for two reasons. First,
these distributions may
be
conjugate with
respect
to
Gaussian
likelihood.
Furthermore, the degree of sparsity in this case can be adjusted with a parameter. As
in Chantas et al. [CHA 08], an extended problem is deﬁned where hidden variables
correspond to the variances of the Gaussian a priori.
In section 9.2, we will remind the variational Bayesian methodology, and then will
expose the methodology based on an exponential gradient (section 9.3). In section 9.4,
we will apply this methodology to our bi-model in order to determine the a posteriori
distribution, and then we will detail the necessary calculations for implementing the
algorithm in section 9.5. We will ﬁnish by putting this approach into practice, both
with simulated and real data, before concluding.
9.2. Variational Bayesian methodology
Here, we describe the conventional variational Bayesian approach introduced by
MacKay [MAC 03]. In this section, in order to to achieve a general presentation of
the approaches, we will denote by y ∈
M the vector of dimension M containing
the data and by w ∈
N the vector gathering all the unknowns to estimate, whether
they are hidden or not. Variational Bayesian methodology aims to approach the
posterior distribution with a separable approximation distribution that will be denoted
as q(w) = 2
i qi(wi).
The degree of separability is generally chosen as a function of the complexity of
the a posteriori dependencies. In the present case here, i.e. large-dimensional
problems, the choice of a strong separability is virtually imposed by the difﬁculty of
calculating the statistics with a large-sized vectorial distribution of order 2 [FRA 12].

Variational Bayesian Approach and Bi-Model
229
Once the choice of separability is made, the probability density function q is
determined as close as possible
to the a posteriori with respect to the
Kullback–Leibler divergence (for more detail, see [AYA 10a, SMI 06]). As this
approximating distribution is assumed to be separable, we can carry out the
calculations analytically. The following optimal approximating distribution is thus
obtained:
∀i ∈{1, . . . , N},
qi(wi) = 1
Ki
exp
	
⟨ln p(y,w)⟩2
j̸=i qj(wj)

[9.1]
where Ki is a normalizing constant and ⟨w⟩q =
4
N wq(w)dw.
Clearly, it can be noted that this solution is analytical but unfortunately it does
not have an explicit form. In order to obtain the optimum in a practical manner, it is
necessary to implement a ﬁxed-point iterative algorithm:
qk+1
i
(wi) = 1
Ki
exp
	
⟨ln p(y,w)⟩2
j̸=i qk
j (wj)

[9.2]
The procedure for obtaining the approximating distribution thus corresponds to an
optimization alternated by groups of coordinates summarized in Algorithm 9.1.
Algorithm 9.1: Conventional variational Bayesian algorithm
Initialization (q0);
repeat
for all i ∈{1, . . . , Ngroup} do
Calculation of qk+1
i
(wi) knowledge qk
j (wj) using [9.2];
Estimate the free energy F(qk);
until convergence;
It is well known in the case of ﬁnite-dimensional optimization that this type of
approach becomes very slow when the number of groups of coordinates is high. This
implies that this type of approach becomes practically untractable when the problem
to solve is very large. This is why we will look at another approach, based on
exponentiated gradient introduced by Kivinen [KIV 97]. The advantage of this latter
approach is that it seems well suited to large-dimensional problems.
9.3. Exponentiated gradient for variational Bayesian
The purpose of exponentiated gradient for variational Bayesian is to update all
the approximating distributions simultaneously. Consequently, it relies on gradient-
based approaches with non-optimal step, well known in the resolution of optimization

230
Regularization and Bayesian Methods for Inverse Problems
problems in ﬁnite dimension. But here in this case, we want to ﬁnd an approximating
distribution to minimize the Kullback–Leibler divergence.
We ﬁrst begin by introducing the functional to optimize, and then will deﬁne the
type of differential used as well as the functional space to which it relates. Then the
used descent direction, as well as the deﬁnition of the descent step, will be exposed.
The following relationship between the evidence of the M model and the
Kullback–Leibler distance to be minimized can easily be found; the demonstration is
found in [CHO 02] or in [SMI 06]:
ln p(y | M) = F(q(w)) + KL(q(w)||p(w | y,M))
where:
F(q(w)) =

N ln
	p(y,w | M)
q(w)

q(dw)
=

N ln p(y,w)q(dw) −

N ln q(w)q(dw)
[9.3]
is the negative free energy. The ﬁrst term on the left corresponds to the expected
value of the logarithm of the joint distribution with regard to q and the right term
corresponds to the entropy q of the approximating distribution. For a data set y, the
evidence of the model is constant. To ﬁnd the approximating distribution q, it is
possible to either minimize the Kullback–Leibler divergence or maximize the
negative free energy. From a practical point of view, it is better to maximize the
negative free energy because it involves the joint distribution of p(y,w | M) and not
the a posteriori distribution p(y | w,M). The joint distribution is easier to handle
because there is no need to know the partition function of the a posteriori distribution
which is most often difﬁcult to calculate.
Our functional optimization problem is to maximize the negative free energy. In
order to solve this issue, we have chosen to make use of the space of signed Radon
measures denoted R [RUD 87]. Special attention is particularly given to the space
R(
)N = R(
) × R(
) × . . . × R(
), which is the Cartesian product of N spaces
of signed Radon measures in
. The advantage of using signed measures is that they
deﬁne a Banach space, involving the norm of the total variation ∥ν∥TV.
In this functional space, the optimization problem is formalized as follows. We are
searching for:
q(w)opt = arg max
q∈R(
)N F(q(w))
[9.4]

Variational Bayesian Approach and Bi-Model
231
To determine the optimal probability density function, we use the Fréchet
differential of the functional F(q(w)). This differential is deﬁned by the linear
application dF(q(w); . ) : R(
)N →
verifying:
∀ν ∈R(
)N,
lim
t→0
1
t ∥F(q(w) + tν(w)) −F(q(w)) −t dF(q(w); ν(w))∥= 0
In addition, in the present case, our differential can be expressed as:
∀ν ∈R(
)N,
dF(q(w); ν(w)) =

N df(q,w) ν(dw)
=
N

i=1

dif(qi,wi) νi(dwi)
[9.5]
In this case:
df(q,w) =

i
dif(qi,wi)
Based on the previous deﬁnitions, the Fréchet functional differential given by
equation
[9.3]
can
be
calculated.
These
calculations
are
detailed
in
[FRA 11, FRA 12]. Thus, we have ∀i ∈{1, . . . , N}, ∀w ∈
N
dif(qi,wi) = ⟨ln p(y,w)⟩2
j̸=i qj(wj) −ln qi(wi) −1
[9.6]
In order to ensure the positivity of the density q, an update process has been chosen
that uses exponentiated gradient by analogy with [KIV 97] in ﬁnite dimension:
qk+1(w) = Kqk(w) exp(df(q,w))αk
[9.7]
In addition,
the constant K
has been added in order to ensure that
4
N qk+1(dw) = 1. We have also introduced a descent step αk which appears as a
power in equation [9.7] because the update is multiplicative. It should be noted that if
the logarithm of equation
[9.7] is taken, an additive update structure with a
multiplicative step is found once more, such as in conventional gradient descent. If
the step is correctly chosen and sufﬁcient, it helps to ensure the convergence of the
algorithm to the overall minimum of the functional, the free energy being a concave
functional in the broad sense. The statement of the convergence theorem and its proof
are found in [FRA 12].

232
Regularization and Bayesian Methods for Inverse Problems
By replacing equation [9.6] in equation [9.7], the updated equation of the
approximating distributions is obtained:
qk+1(w) = Kqk(w)
⎛
⎜
⎜
⎝

i
exp
	
⟨ln p(y,w)⟩2
j̸=i qk
j (wj)

qk
i (wi)
⎞
⎟
⎟
⎠
αk
= Kqk(w)

i
qr
i (wi)
qk
i (wi)
αk
[9.8]
with:
qr
i (wi) = exp
	
⟨ln p(y,w)⟩2
j̸=i qk
j (wj)

[9.9]
At each iteration, obtaining the descent step αk requires solving the following
scalar optimization problem:
αk+1 = arg max
α∈
F

qk(w) exp ( df(q,w))α
[9.10]
In practice, the criterion is too complicated to obtain an optimal solution.
Therefore, a suboptimal step is determined by calculating the optimal step of the
second-order Taylor’s expansion of the criterion.
Algorithm 9.2: Exponentiated gradient for variational Bayesian
Initialization (q0 ∈R(
)N);
repeat
Calculation of qr
i (wi);
Calculation of αk;
Update qk+1 using [9.8];
until Convergence;
9.4. Application: reconstruction-separation of astrophysical components
9.4.1. Direct model
As discussed in section 9.1, we are searching for an infrared image composed of
point sources, stars and galaxies unresolved by the instrument, and of an extended
component that corresponds to the cosmic microwave background. This bi-model has
already been introduced in [CIU 99, DEM 04, GIO 05]. It is, therefore, considered

Variational Bayesian Approach and Bi-Model
233
that our image of interest x ∈
N consists of two components: a smooth image s and
a peak image p. The model of the sky is then written as:
x = s + p
The concern is here in the data from the telescope Herschel and more particularly
from the far-infrared imager (λ = 250 μm or λ = 350 μm or λ = 500 μm) SPIRE.
The observations are given by several horizontal and / or vertical scans of the skies.
This imager is composed of distributed bolometric detectors according to a hexagonal
grid. In addition, there is downsampling due to the footprint of each bolometer. The
sky measurement is carried out by means of a subpixel sky scanning procedure. The
model also takes into account the optical response of the instrument, given by the sum
of the response of the mirrors and the cones that operate as antennae allowing the
concentration of the electromagnetic wave. It is considered that the response of the
telescope corresponds to the convolution of the sky by a impulse response assumed
as known. This direct model has been developed in the context of Franois Orieux’s
thesis; for more detail on this point, see [ORI 09, ORI 12]. The data are also corrupted
by an error ϵ. The relationship that links data, denoted y ∈
M, to the sky x is,
therefore, as follows:
y = Hx + ϵ = H(s + p) + ϵ
[9.11]
This error ϵ is divided into two parts: a reproducible (offset) measurement error o
which is estimable and an error that is considered as centered white Gaussian noise b
with an inverse variance (precision) γb. Without introducing too much modeling error,
it can be assumed that the offsets are independent from one bolometer to another and
that they are identical for each bolometer during the acquisition phase. The dimension
Nscan × Nbolometers of the offset vector is, therefore, much lower than the number of
measurements. In addition, the measurement strategy makes data redundant, each sky
pixel being seen several times by the (bolometers) sensors at different times. All this
contribute to the fact that the estimation problem of the offsets is well posed. If an
offsets replication matrix R is included, which duplicates the offsets in order to have
a value by measurement, equation [9.11] becomes:
y = H(s + p) + Ro + b
[9.12]
Since the noise is Gaussian, the likelihood of s, p and o is determined using
equation [9.12]:
Pr(y | p,s,o,γb) ∝exp
"
−γb
2 ∥y −Hs −Hp −Ro∥2#
[9.13]

234
Regularization and Bayesian Methods for Inverse Problems
9.4.2. A Priori distributions
These a priori distributions are going to model the information that will be
introduced to separate the two components (impulse and smooth). As a result, it can
be easily seen according to [9.12] that there is indetermination between s and p. The
separation between the two components will consequently only due to the a priori
information. Therefore, it is necessary that this a priori information is sufﬁciently
discriminating. Here, a correlated Gaussian distribution was chosen for the smooth
component s and a separable heavy-tailed distribution for the impulse component p.
Another important property for the choice of our a priori is that they are conjugate
with the likelihood. This property ensures that the a posteriori corresponds to known
distributions and thus that its cumulants are easily calculable. These constraints have
led us to the following choices:
– the a priori distribution of the offsets o is an iid Gaussian distribution with a
mean om and an inverse variance γo:
p(o | om,γo) ∝exp
"
−γo
2 ∥o −om∥2#
– the a priori distribution in the case of s is a correlated Gaussian distribution with
an inverse variance γs and a covariance matrix (DtD)−1:
p(s | γs) ∝exp
"
−γs
2 ∥Ds∥2#
where D is the matrix of ﬁnite differences;
– the a priori distribution for p is an iid student’s t-distribution:
p(p | ν,λ) ∝

i
"
1 + λ
ν p2
i
#−ν+1
2
We have chosen this heavy-tailed distribution rather than a Laplace [CHE 96]
or α-stable distribution because it can be written as a GSM involving a gamma
distribution:
p(pi | ν,λ) =

p(pi | ai,λ)p(ai | ν) dai
with:
p(pi | ai,λ) = N(0,(λai)−1)
p(ai | ν) = Gamma
"ν
2,ν
2
#

Variational Bayesian Approach and Bi-Model
235
REMARK.– By using this extended problem, the student’s t-distribution is deﬁned as
an inﬁnite centered Gaussian mixture. By replacing the gamma distribution by another
distribution, it is possible to obtain other types of probabilities, such as the Laplace
distribution (see [WAI 00]).
The concept presented by Chanta et al. [CHA 08] for the solution of inverse
problems is to introduce hidden variables a corresponding to the inverse variance of
the Gaussian distributions. Therefore, the problem to be solved is an extended
problem where the smooth component s, the impulse component p and the inverse
variance a of this impulse component are estimated. Often, we decide to solve an
extended problem, having more unknowns and where we must also estimate hidden
variables a. We estimate now three times more unknowns than data items. On the
other hand, there is an a posteriori Gaussian for s, p and an a posteriori gamma for
a:
p(s,p,a | γs,ν,λ) = p(s | γs)p(p,a | ν,λ) = p(s | γs)p(p | a,λ)p(a | ν)
[9.14]
It should be noted here that discrete hidden variables could have been included.
This would have led us to a Gaussian mixture as in [AYA 10b, ICH 06], or Bernoulli-
Gaussian [GOU 90, KOW 11, RAB 07].
Now that we have deﬁned a priori conjugate distributions with respect to the
likelihood, it can be ensured that conditional a posteriori distributions will belong to
known families. The application of the variational Bayesian methodology presented
in section 9.2 is, therefore, possible. This means that the optimization of the
approximation will be done by means of the parameters of these distributions, which
facilitates the implementation of the methods.
9.4.3. A posteriori distribution
To obtain the a posteriori distribution, we use Bayes theorem. By introducing the
chosen model M, the extended a posteriori distribution with respect to (s,p,a,o) is
derived from equations [9.13] and [9.14]:
p(s,p,a,o | y,γb,γs,ν,λ,om,γo,M) ∝exp
"1
2

γb ∥y −Hs −Hp −Ro∥2
+γs ∥Ds∥2 +γo ∥o −om∥2 #
i
√ai exp
"
−1
2p2
i aiλ
#
a
ν
2 −1
i
exp
"
−1
2νai
#
[9.15]

236
Regularization and Bayesian Methods for Inverse Problems
9.5. Implementation of the variational Bayesian approach
9.5.1. Separability study
Based on the expression of the a posteriori [9.15] distribution, it can be easily
established that the conditional distribution p(a | s,p,y,o,γb,γs,ν,λ,M) is separable,
unlike the conditional distributions of s, p and o. In addition, we will process a
signiﬁcant volume of data (4 million data items) and we also have a very large
number of unknowns, namely three times 700,000 unknowns. This is why we have
chosen to use approximation distributions all separable to reduce the computational
cost and the size of the required memory. As a result, the storage of the vectorial
method imposes at least the storage of a covariance matrix of size N 2. We will,
therefore, apply the methodologies presented in sections 9.2 and 9.3. By taking
w = (s,p,a,o), the approximation q is written in this case as:
q(s,p,a,o) =

i
qsi(si)

j
qpj(pj)

k
qak(ak)

l
qol(ol)
[9.16]
It should be remembered that the objective is to ﬁnd the approximation q
maximizing the negative free energy equation [9.3]. We will thus use an alternate
procedure described in Algorithm 9.3.
Algorithm 9.3: Inversion algorithm of the bi-model
Initialization (q0
s, q0
p, q0
a, q0
o);
repeat
Update of the distribution of a(qk
p) with a conventional variational Bayesian
approach (Algorithm 9.1);
Update of the distribution of p(qk
s, qk
o, qk+1
a
) by exponentiated gradient
(Algorithm 9.2);
Update of the distribution of s(qk
o, qk+1
p
) by exponentiated gradient
(Algorithm 9.2);
Update of the distribution of o(qk+1
s
, qk+1
p
) by exponentiated gradient
(Algorithm 9.2);
until Convergence;
9.5.2. Update of the approximation distributions
Since we have chosen conjugate a priori distributions, we thus know that the
optimal distribution is necessarily of the same family as the a posteriori distribution.
We, therefore, choose approximating distributions q in the same family as the a
posteriori distribution. Hence, it is concluded that the approximating distributions

Variational Bayesian Approach and Bi-Model
237
q(s) and q(p) will be Gaussians while the approximating distribution q(a) will
belong to the family of gamma distributions. As we have seen in the previous section,
we have chosen the maximum degree of separability in order to limit the cost of
calculations. The approximation distributions at iteration k are, therefore, deﬁned as
follows:
qk
or(or) = N(mok(r), Υo 2
k (r)),
qk
si(si) = N(msk(i), Υs 2
k (i))
qk
pj(pj) = N(mpk(j), Υp 2
k (j)),
qk
al(al) = Gamma(αk(l),γk(l))
with mo, ms, respectively, mp is the mean vector of the approximating Gaussian
distributions of o, s, respectively, p and Υo, Υs, respectively, Υp is the standard
deviation vector of the Gaussian approximations of o, s ,respectively, p.
9.5.2.1. Update of the a distribution
By examining the a posteriori distribution of equation [9.15], it can be observed
that the conditional distribution of a is already separable. The update of ai can thus be
done in an independent manner in the “conventional” variational Bayesian approach.
In this case, Algorithm 9.2 is unnecessary. For this reason, we will use Algorithm 9.1.
Equations [9.2] and [9.15] give us ∀i ∈{1, . . . , N}:
˜qk+1
i
(ai) = 1
Ki
exp
	
⟨ln p(y,s,p,a)⟩2
j̸=i qk
j (aj)qk(s)qk(p)qk(o)

∝exp
	
ν −1
2
ln ai −
 	p2
i aiλ
2
+ ai ν
2

 
l
qk
l (pl)

j̸=i
˜qk
j (aj) dp da

∝exp
	
ν −1
2
ln ai −ai ν
2 −
 p2
i aiλ
2
qk
i (pi) dpi

∝exp
	
ν −1
2
ln ai −ai ν
2 −ai λ
2

Υp 2
k (i) + mp2
k(i)

∝a
ν
2 −1
2
i
exp
	
−ai
	
ν
2 + λ
2

Υp 2
k (i) + mp2
k(i)


[9.17]
The equations to update the parameters of the gamma distribution can be derived:
∀i ∈{1, . . . , N},
αk+1(i) = ν
2 + 1
2,
[9.18]
γk+1(i) = ν
2 + λ

Υp 2
k (i) + mp2
k(i)

2
[9.19]

238
Regularization and Bayesian Methods for Inverse Problems
9.5.2.2. Update of the p distribution
Unlike the previous case, the conditional posterior distribution p is not separable;
we will hence use Algorithm 9.2 in order to update our approximating distribution.
The ﬁrst step of this algorithm is to calculate the intermediate density qr
p of
equation [9.9] which belongs to the same family as qp. For all i in {1, . . . , N}:
qr
pi(pi) = exp
	
⟨ln p(y,p,s,a)⟩2
j̸=i qk
pj (pj)qk(s)qk(o)qk+1(a)

∝exp
	
−
 	γb
2 ∥y −Hs −Hp −Ro∥2 + λp2
i ai
2


j̸=i
qk
pj(pj)qk(s)qk(o)qk+1(a) dp ds do da

∝exp

−
 	γb
2 ptHtHp −γbptHt(y −Hs −Rmok) + λp2
i ai
2


j̸=i
qk
pj(pj)qk(s)qk+1(a) dp ds da

∝exp
	
−γb
2

p2
i diag(HtH) −2pi

Ht(y −Hmsk −Rmok)

i
+ 2pi(HtH −diag(HtH))mpk

+ λ p2
i αk+1(i)
2γk+1(i)

[9.20]
where diag(A) is a vector made up of the diagonal elements of the matrix A. It can
be noted that qr
p remains Gaussian with a mean mpr and a variance Υp 2
r, these
parameters taking the following values:
Υp 2
r (i) =
	
γb diag(HtH)i + λ p2
i αk+1(i)
γk+1(i)

−1
[9.21]
mpr(i) =
Υp 2
r (i)γb

Ht(y −Hmsk −Rmok) −(HtH −diag(HtH))mpk

i
The second step consists of determining the distribution qk+1 as a function of the
descent step αk. By inserting the expression of qr
p in equation [9.8] and by using

Variational Bayesian Approach and Bi-Model
239
calculations detailed in [FRA 12], the equations to update qp can be obtained as a
function of αk:
Υp 2
αk (i) =
Υp 2
r (i) Υp 2
k (i)
Υp 2
r (i) + α(Υp 2
k (i) −Υp 2
r (i))
[9.22]
mpαk(i) = mpk(i) Υp 2
r (i) + αk

mpr(i) Υp 2
k (i) −mpk(i) Υp 2
r (i)

Υp 2
r (i) + αk

Υp 2
k (i) −Υp 2
r (i)

The last step prior to the update of the distribution consists of determining a
descent step αk. Therefore, a suboptimal step is determined by achieving a Taylor
expansion of order 2 of α →F(qα) and the optimal step is calculated from this
expansion. The details of the calculation of the step as well as of the result are found
in [FRA 12].
9.5.2.3. Update of the s distribution
As in the previous section, the conditional posterior of s is not separable, and we
will use Algorithm 9.2.
Let us start by calculating the intermediate density qr
s. For all i in {1, . . . , N}:
qr
si(si) = exp
	
⟨ln p(y,p,s,a)⟩2
j̸=i qk
sj (sj)qk(o)qk+1(p)qk+1(a)

∝exp

−
 "γb
2 ∥y −Hs −Hp −Ro∥2 + γs
2 ∥Ds∥2#

j̸=i
qk
sj(sj) qk(o) qk+1(p) qk+1(a) ds do dp da

∝exp

−
 "γb
2 stHtHs −γbstHt(y −Hp −Rmok) + γs
2 ∥Ds∥2#

j̸=i
qk
sj(sj) qk+1(p) qk+1(a) ds dp da

∝exp
"
−γb
2

s2
i diag(HtH) −2si

Ht(y −Hmpk+1 −Rmok)

i
+ 2si(HtH −diag(HtH))msk

+ γs(s2
i diag(DtD) + 2si

DtDmsk −diag(DtD)msk

i
#
[9.23]

240
Regularization and Bayesian Methods for Inverse Problems
It should be noted that qr
s remains Gaussian with a mean msr and a variance Υs 2
r
where:
Υs 2
r (i) =

γbdiag(HtH)iγsdiag(DtD)i
−1
[9.24]
msr(i) = Υs 2
r (i)γb
"
Ht(y −Hmpk+1 −Rmok)
−(HtH −diag(HtH))msk −γs
γb
(DtD −diag(DtD))msk
#
i
[9.25]
It should be noted here that the variance is constant which facilitates the
calculation. The remainder of the calculations is similar to that given in
section 9.5.2.2.
9.5.2.4. Update of the o distribution
By using the same process as in the previous sections, the equations of the next
update of the parameters of the intermediate distribution are obtained:
Υo 2
r (i) =

γbdiag(RtR)i + γo
−1
[9.26]
mor(i) = Υo 2
r (i)γb
"
Rt(y −Hmpk −Hmsk) + γs
γb
om
#
i
[9.27]
9.6. Results
This section aims to show, on the one hand, the effectiveness of the exponentiated
gradient approach to solve in a non-supervised manner problems of large dimensions
and, on the other hand, the power of separation between the foregrounds (impulse
component)
and
the
cosmic
microwave
background
(regular
component).
Furthermore, we are ﬁrst going to simulate data in the conﬁguration of real data, that
is to say that there are 4 million observations to estimate 700,000 unknowns. We have
simulated from an extended emission map measured by Herschel, to which point
sources were added. After this validation, we have applied our approach to the real
data; it will be seen that the contribution of our approach in terms of resolution and
artifacts correction is obvious.
First, one of the points that we have not addressed in this chapter, due to lack of
space, is the adjustment of the hyperparameters. This adjustment is particularly
tedious in the case of the reconstruction and the joint separation of two components
because there are then four parameters to adjust, namely γb,γo,γs and λ. It is
considered here that the shape parameter of the student’s t-distribution ν does not
need to be adjusted; it sufﬁces to chose a value that helps to favor the impulses. We
have, therefore, developed an approach where the four hyperparameters cited above
are estimated jointly in both maps s, p and in the offsets o.

Variational Bayesian Approach and Bi-Model
241
9.6.1. Simulated data
We present here two numerical experiments in which we have built a real picture
from two components, and then we have generated data through our direct model
by adding a white Gaussian noise identically distributed. In these cases, we have not
added any offset o in order to facilitate the analysis of the results.
The ﬁrst experiment is a realistic simulation. As a result, a map reconstructed
through the ofﬁcial code of the Herschel consortium was considered as a real extended
emission map, based on real data (see Figure 9.1(a)). Then we have added sources that
are slightly expanded, namely a disk with a diameter of ﬁve over-resolved pixels (see
Figure 9.2(a)). We have then simulated data using our direct model and then we have
added iid Gaussian noise with a standard deviation of 0.035, which corresponds to the
level of noise observed in real data.
The second numerical experiment has the objective to test the operation of the
approach when the extended component is zero and to verify that the position of the
sources is once more well traced and that photometry, that is to say, the energy of the
source, is guaranteed.
Figure 9.1. Extended component s: on the left, real map used to simulate
data; on the right, map rebuilt with our approach
9.6.1.1. Realistic simulation
A very important point for variational Bayesian approaches is initialization. In
this example, we have initialized the mean mp to zero and ms with the map
obtained from the deconvoluted ESA algorithm. The variances have been initialized
from an empirical estimation of the noise variance σ2
b obtained by the residual
resulting from the initialization of the mean. The value of Υp is then taken as equal
to σb and that of Υs equal to σb/10. After convergence, when the negative free

242
Regularization and Bayesian Methods for Inverse Problems
energy evolution is below a speciﬁc threshold, we obtain the results of Figures 9.1(b)
and 9.2(b). It is observed that the extended component has been correctly separated
from the impulse component. Beyond the separation of sources, the quality of the
reconstructed image also happens to increase. To obtain this result, we sum up the
extended component with the impulse component and we compare the result
obtained, Figure 9.3(d), with the method provided by the ESA, Figure 9.3(b), and a
method where the likelihood is identical to our approach but where a Gaussian a
priori is used in the image gradients, Figure 9.3(c). It is clearly seen that the a priori
using the impulses facilitates the maximum exploitation of the over-resolved model.
In addition, the impulse component is very close to the real one (see Figure 9.3(a)).
Figure 9.2. Impulse component: on the left, real map used to simulate data;
on the right, map rebuilt with our approach
9.6.1.2. Study of the peaks reconstruction
This second study has two objectives: ﬁrst, to determine the behavior of the
method in the absence of an extended component and, second, to obtain quantitative
results about the estimation quality of the peaks, their positions and the respect of
photometry (conserved energy). In this example, the image of point sources has been
created by randomly choosing the source positions and their amplitudes (see
Figure 9.4(a)).
The rebuilt extended component is negligible because its energy is 3 against 190
in the case of the impulse map. In addition, the energy of the peak map is
concentrated on a few values while that of the extended map is distributed over all the
pixels of the image (15 against 700,000). No source has thus been explained by the
extended component. It can be observed in Figure 9.4(a) that the positions of the
peaks are all correctly estimated. The value of the sources is also very correctly
estimated (see Figure 9.4(b)). This last point is very important for astrophysicists

Variational Bayesian Approach and Bi-Model
243
because the amplitude distribution of the sources is a component that helps to
determine cosmological parameters.
Figure 9.3. Comparison of estimators over the sum of the two components:
a) real map, b) reconstruction with the ofﬁcial ESA pipeline, c) reconstruction
without bi-model and with a smooth a priori and d) reconstruction using a bi-model
We have also studied the quality of the estimation of the impulses by plotting a cut
through an impulse (see Figure 9.5). First, it can be observed that we have chosen the
impulses with a certain extension such that they are more visible on the images (see
black curve from Figure 9.5). On this same graph are plotted the ofﬁcial ESA method,
the version of our over-resolved algorithm using only a priori smoothing, and our new
approach. It can be noted that the over-resolved approach with a priori smoothing can
improve the resolution of the image but it also creates jumps (Gibbs phenomenon), the
smoothing a priori poorly modeling the impulse sources. Our new approach facilitates
the substantial increase in the impulses resolution while eliminating jumps phenomena
around the peaks.

244
Regularization and Bayesian Methods for Inverse Problems
200
400
600
800
300
350
400
450
500
550
600
650
700
750
a)
 
 
Estimated
True
0
5
10
15
0
100
200
300
400
500
600
b)
Figure 9.4. Reconstruction of an image of point sources:
a) spatial distribution of sources, square: real position and cross: estimated;
b) amplitude of the 13 sources in an arbitrary order
10
20
30
40
50
60
70
80
90
100
−2
0
2
4
6
8
10
12
True sky
Coadd
Smooth
Smooth+Point
Figure 9.5. A source proﬁle: (solid line) real sky, (squarer line) ESA’s algorithm,
(line with cross) reconstruction without bi-model with a smoothing a priori,
(line with triangle) reconstruction using a bi-model
9.6.2. Real data
The data used are from the Herschel satellite from the SPIRE instrument. This
instrument is a far-infrared radiation imager (the mean wavelength is 250 μm). The

Variational Bayesian Approach and Bi-Model
245
ﬁrst results are very encouraging. This approach is very effective because we jointly
estimate the offsets, the map of pulses, the extended map, the noise variance, the
impulse map variance and the variance of the smoothing map. Each map is composed
of 700,000 unknowns, and overall 4 million data items are processed. The computation
time is only 2,000 s on a desktop computer (Intel core 2 Duo 2.66 GHz). In addition,
when comparing the result obtained by our approach (see Figure 9.6(b)) with regard
to the result obtained by the ESA’s algorithm (see Figure 9.6(a)), it can be observed
that the resolution is signiﬁcantly higher. Our method, due to the joint estimation of
the offsets, also eliminates systematic effects (oscillations on the edge of the image).
Figure 9.6. Visualization of the sum of the two components based on the real reconstructed
data: on the left, reconstruction by means of the ESA’s algorithm; on the right, variational
Bayesian reconstruction with bi-model
Figure 9.7. Reconstruction of real data with our approach: on the left,
impulse component; on the right, extended component
In addition to the gain in resolution, we have separated the point sources or those
slightly extended from a continuous background. These sources are distant galaxies
not resolved by the instrument. It can be noted that some of these unresolved sources

246
Regularization and Bayesian Methods for Inverse Problems
are explained by two or three impulses. Finally, due to the joint estimation of the
sources and the extended map, it is possible to detect sources with much lower
amplitude than with approaches such as CLEAN. As these approaches are based on
the estimation of a maximum in a residual map, low sources are detected when the
extended map has a signiﬁcant level, which is not the case when the level of the
extended map is low. Within this context, our method is of particular signiﬁcance
because it overcomes this problem.
9.7. Conclusion
In this chapter, we have shown how the Bayesian approaches, and more
particularly the approach based on the exponentiated gradient adapted for variational
Bayesian, can be used in the context of the reconstruction of images from space
telescopes. More particularly, the method detailed here can solve inverse problems of
very large dimensions. As a result, maps with 700,000 pixels are rebuilt here using a
desktop computer, and even 50 million pixels on supercomputers. We have shown
how this recent approach can solve an over-resolved estimation problem jointly for
the separation of two components: a smooth component representing the cosmic
microwave background radiation and an impulse component, namely the foreground.
This approach was implemented within the framework of a non-supervised algorithm
by exploiting the very sparse nature of the map of impulse sources. The results with
simulated data correspond to our expectations, and the results from real data are very
good because we have managed to model the predominant effects which degrade
reconstructions, in particular the presence of offset in the bolometers. This Bayesian
approach is very promising because it facilitates the extension of non-supervised
approaches exploiting sparsity in problems of very large dimensions.
9.8. Bibliography
[AYA 10a] AYASSO H., Une approche bayésienne de l’inversion. Application à l’imagerie de
diffraction dans les domaines micro-onde et optique, PhD Thesis, University of Paris-Sud,
December 2010.
[AYA 10b] AYASSO
H.,
MOHAMMAD-DJAFARI
A.,
“Joint
NDT
image
restoration
and segmentation using Gauss-Markov-Potts prior models and variational Bayesian
computation”, IEEE Transactions on Image Processing, vol. 19, no. 9, pp. 2265–2277,
September 2010.
[BAB 09] BABACAN S.D., MOLINA R., KATSAGGELOS A.K., “Variational Bayesian blind
deconvolution using a total variation prior”, IEEE Transactions on Image Processing,
vol. 18, no. 1, pp. 12–26, 2009.
[BAL 08] BALI N., MOHAMMAD-DJAFARI A., “Bayesian approach with hidden Markov
modeling and mean ﬁeld approximation for hyperspectral data analysis”,
IEEE
Transactions on Image Processing, vol. 17, no. 2, pp. 217–225, February 2008.

Variational Bayesian Approach and Bi-Model
247
[CHA 04] CHAMPAGNAT F., IDIER J., “A connection between half-quadratic criteria and EM
algorithms”, IEEE Signal Processing Letters, vol. 11, no. 9, pp. 709–712, September 2004.
[CHA 08] CHANTAS G., GALATSANOS N., LIKAS A., et al., “Variational Bayesian image
restoration based on a product of t-distributions image prior”, IEEE Transactions on Image
Processing, vol. 17, no. 10, pp. 1795–1805, October 2008.
[CHE 96] CHEN S.S., DONOHO D.L., SAUNDERS M.A., Atomic decomposition by basis
pursuit, research report, Stanford University, February 1996.
[CHO 02] CHOUDREY R., Variational methods for Bayesian independent component analysis,
PhD Thesis, Oxford University, 2002.
[CIU 99] CIUCIU P., IDIER J., GIOVANNELLI J.-F., “Analyse spectrale non paramétrique
haute résolution”, Actes du 17e colloque GRETSI, Vannes, pp. 721–724, September 1999.
[DEM 04] DE MOL C., DEFRISE M., “Inverse imaging with mixed penalties”, Proceedings
URSI Symposium on Electromagnetic Theory, Ed. PLUS Univ. Pisa, Pisa, Italy. Available
at http://www.ee.bgu.ac.il/∼specmeth/EMT04/pdf/session_3/3_10_03.pdf, pp. 798–800,
2004.
[DEM 07] DEMOMENT G., Sujet de TP du M2R ATSI: Apprentissage dans un ensemble ou
calcul bayésien variationnel, 2007.
[DEM 08] DEMOMENT G., Séminaire interne GPI: Histoire de jonquilles ou de J.P.E.G.1
en J.P.E.G.2, (1) Joint Photographic Experts Group, (2) Jardiniers Parkinsonniens et
Estimateurs Gérontologues, 2008.
[FER 06] FERGUS R., SINGH B., HERTZMANN A., et al., , “Removing camera shake from a
single photograph”, ACM Transactions on Graphics, vol. 25, no. 3, pp. 787–794, July 2006.
[FRA 11] FRAYSSE A., RODET T., “A gradient-like variational Bayesian algorithm”, IEEE
Workshop on Statistical Signal Processing, Nice, France, no. S17.5, pp. 605–608, June
2011.
[FRA 12] FRAYSSE A., RODET T., A measure-theoretic variational Bayesian algorithm for
large dimensional problems, Report , LSS, 2012.
[GIO 05] GIOVANNELLI J.-F., COULAIS A., “Positive deconvolution for superimposed
extended source and point sources”, Astronomy and Astrophysics, vol. 439, pp. 401–412,
2005.
[GIO 08] GIOVANNELLI J.-F., “Unsupervised Bayesian convex deconvolution based on a ﬁeld
with an explicit partition function”, IEEE Transactions on Image Processing, vol. 17, no. 1,
pp. 16–26, January 2008.
[GOU 90] GOUSSARD Y.,
DEMOMENT G.,
MONFRONT F., “Maximum a posteriori
dectection-estimation of Bernoulli-Gaussian processes”, Mathematics in Signal Processing
II, Clarendon Press, Oxford, 1990.
[ICH 06] ICHIR M., MOHAMMAD-DJAFARI A., “Hidden Markov models for wavelet-based
blind source separation”, IEEE Transactions on Image Processing, vol. 15, no. 7, pp. 1887–
1899, July. 2006.

248
Regularization and Bayesian Methods for Inverse Problems
[JOR 99] JORDAN M.I., GHAHRAMANI Z., JAAKKOLA T.S., et al., “An introduction to
variational methods for graphical models”, Machine Learning, vol. 37, no. 2, pp. 183–233,
1999.
[KIV 97] KIVINEN J., WARMUTH M., “Exponentiated gradient versus gradient descent for
linear predictors”, Information and Computation, vol. 132, no. 1, pp. 1–63, 1997.
[KOW 11] KOWALSKI M., RODET T., “An unsupervised algorithm for hybrid/morphological
signal decomposition”, IEEE International Conference on Acoustic, Speech and Signal
Processing, Prague, May 2011.
[MAC 03] MACKAY D.J.C., Information Theory, Inference, and Learning Algorithms,
Cambridge University Press, Cambridge, 2003.
[MAZ 05] MAZET
V.,
Développement
de
méthodes
de
traitement
de
signaux
spectroscopiques: estimation de la ligne de base et du spectre de raies, PhD Thesis,
Henri Poincaré University, Nancy 1, 2005.
[MIS 00] MISKIN J., Ensemble learning for independent component analysis, PhD Thesis,
Cambridge University, 2000.
[MOR 11] MORTON K., TORRIONE P., COLLINS L., “Variational Bayesian learning for
mixture autoregressive models with uncertain-order”, IEEE Transactions on Signal
Processing, vol. 59, no. 6, pp. 2614–2627, 2011.
[NOC 00] NOCEDAL J., WRIGHT S.J., Numerical Optimization, Series in Operations
Research, Springer, New York, 2000.
[ORI 09] ORIEUX F., Inversion bayésienne myope et non-supervisée pour l’imagerie sur-
résolue. Application à l’instrument SPIRE de l’observatoire spatial Herschel, PhD Thesis,
University of Paris-Sud 11, November 2009.
[ORI 10] ORIEUX F., GIOVANNELLI J.-F., RODET T., “Bayesian estimation of regularization
and point spread function parameters for Wiener-Hunt deconvolution”, Journal of the
Optical Society of America (A), vol. 27, no. 7, pp. 1593–1607, 2010.
[ORI 12] ORIEUX F., GIOVANNELLI J.-F., RODET T., et al., “Super-resolution in map-
making based on a physical instrument model and regularized inversion. Application to
SPIRE/Herschel”, Astronomy and Astrophysics, vol. 539, p. A38 (16 p.), March 2012.
[RAB 07] RABASTE O., CHONAVEL T., “Estimation of multipath channels with long impulse
response at low SNR via an MCMC method”, IEEE Transactions on Signal Processing,
vol. 55, no. 4, pp. 1312–1325, April 2007.
[ROB 97] ROBERT C.P., Simulations par la méthode MCMC, Economica, Paris, 1997.
[ROB 04] ROBERT C.P., CASELLA G., Monte Carlo Statistical Methods, 2nd ed., Springer
Texts in Statistics, Springer Verlag, New York, 2004.
[RUD 87] RUDIN W., Real and Complex Analysis, McGraw-Hill, New York, 1987.
[SMI 06] SMIDL V., QUINN A., The Variational Bayes Method in Signal Processing, Springer,
New York, 2006.

Variational Bayesian Approach and Bi-Model
249
[SNO 02] SNOUSSI H., MOHAMMAD-DJAFARI A., “Bayesian unsupervised learning for
source separation with mixture of Gaussians prior”, International Journal of VLSI Signal
Processing Systems, vol. 37, nos. 2–3, pp. 263–279, 2002.
[WAI 00] WAINWRIGHT M.J., SIMONCELLI E.P., “Scale mixtures of Gaussians and the
statistics of natural images”, in SOLLA S., LEEN T., MÜLLER K.-R., (eds.), Advances
in Neural Information Processing Systems 12, MIT Press, Cambridge, pp. 855–861, 2000.


10
Kernel Variational Approach for Target
Tracking in a Wireless Sensor Network
10.1. Introduction
Sensor networks have been raising a growing interest within the scientiﬁc and
industrial communities for several years [SAY 05]. A network of intelligent sensors
can be deﬁned as an integrated system that includes, on the one hand, the means to
achieve a measurement, on the other hand the means to process this measurement
and disseminate the information up to a higher-level system or a human. The sensors
are provided with wireless technologies and are able to communicate in a
peer-to-peer, dynamic and instantaneous, reconﬁgurable fashion depending on the
evolution of the population of sensors, without central hierarchy, and thus to form a
net-shaped structure. This distributed mode presents the advantage of being
particularly robust to external attacks and to the failure of sensors since it is
envisaged that the loss of components does not undermine the effectiveness of the
network as a whole. Also, the formulation of decision problems and of resolution
algorithms changes completely since each network node, composed of an intelligent
miniaturized sensor and having a limited energy capacity, is entrusted with the
mission of achieving the measurements and taking local partial decisions. These
latter are progressively reﬁned by the sensors that surround it. At the present time,
these new technologies are found in a large number of applications that increasingly
tend to become more numerous as the integration capabilities (micro and
nanosystems) increase. As a matter of fact, rapid deployment, reduced cost and fault
tolerance in sensor networks are the characteristics that make them a valuable tool in
several areas of application, such as in military activities, telecommunications, the
biomedical ﬁeld, etc. (see Figure 10.1).
Chapter written by Hichem SNOUSSI, Paul HONEINE and Cédric RICHARD.

252
Regularization and Bayesian Methods for Inverse Problems
Figure 10.1. Potential applications of wireless sensor networks
The functions performed by wireless sensor networks have to adapt to the
constraints of digital communications and energy limitations. In fact, in these
systems, data processing is based on the distributed and cooperative nature of sensors
constituting the network for a more secure decision. The nodes that constitute the
network are autonomous and have for that purpose an energy reserve available,
whose renewal may prove impossible, which limits their life duration. Each of the
nodes must be able to process the sensed data, to take a local decision and to
communicate autonomously to neighboring nodes, to which it is connected. This
cooperation is intended to ensure that the best decisions are taken, despite the limits
in terms of energy consumption and of processing power. In this context, it is
essential that the proposed solutions are cooperative, both at the level of the
communication mode and for the processing of acquired information.
10.2. State of the art: limitations of existing methods
The problem of monitoring a mobile object (passive target or active sensor) is
generally resolved in a Bayesian framework based on a state model. The state model
contains two equations: an equation reﬂecting the a priori that is already available
about the trajectory of the target and a second equation linking the unknown state of
the system to the observations that the sensors are collecting. Bayesian ﬁltering
consists of estimating the a posteriori distribution of the state xt of the system
(position of the target) knowing all the observations of the sensors having detected
the target. In the context of this work, the ﬁltering is considered distributed (without
central hierarchy) where only a few sensors, considered relevant, are enabled to
ensure an effective target tracking.
Recent work has been devoted to the implementation of Bayesian ﬁltering in a
wireless sensor network. Considering the nonlinearity of the observation equations,
Bayesian ﬁltering is implemented by a sequential Monte Carlo method (particle
ﬁltering) [DOU 02]. The popularity of this type of methods is mainly due to their
ﬂexibility to process nonlinear/non-Gaussian state models. However, particle ﬁltering

Kernel Variational Approach for Target Tracking in a Wireless Sensor Network
253
requires exchanging a signiﬁcant number of particles (randomly simulated positions)
when the sensors ensuring the tracking transfer control to other more relevant sensors
during the next time frame. For this reason, approximations have been considered to
accommodate the energy constraints. In [IHL 05], an approximation of the distribution
of the particles of the KD-tree type has been considered. In [SHE 05], a collaborative
strategy based on the approximation of the Gaussian mixture type and implemented
by an Expectation-Maximization (EM) algorithm has been proposed. Conversely to
the KD-tree approximation, the Gaussian mixture approximation does not allow the
control of the propagation of the approximation error. The KD-tree approach allows
the control of a trade-off between the approximation error, and the constraints of
digital communications. However, the two approaches induce the propagation of
successive approximations errors. Recently, a variational collaborative approach has
been proposed in [SNO 06]. It consists of an online updating of an approximated
form of the a posteriori distribution. This approach allows an adaptive compression
of the non-Gaussian distribution of the system state. The communication between the
sensors is therefore done without loss since the exchange is based only on sending a
single Gaussian. The main drawback of all the tracking approaches presented above is
the fact that they do not take account for a factor far more important in wireless sensor
networks: observation modality. The following points must be taken into account:
– due to costs reasons, the sensors are in general not ﬁtted with sophisticated
capture systems;
– the actual operating conditions of the sensors are not generally known. It follows
that the observation function is not known and changes with the environment of the
sensor;
– the sensors can deteriorate with time (normal wear or incidents).
For example, the proximity model which is largely used in the literature is that of
the Received Signal Strength Indicator (RSSI) [PAT 04]. The RSSI is based on a
parametric model whose parameters must be learned according to the environment of
the sensor. The performance of tracking and localization are very sensitive to the
relevance of this model and to its ﬁxed parameters. As a matter of fact, the RSSI
represents the shape of the likelihood which is used in Bayesian ﬁltering. The
deviation of this likelihood function causes a severe performance degradation of the
tracking.
One
way
to
avoid
this
problem
is
the
use
of
binary
sensors
[DJU 05, TEN 07, TEN 10a]. A binary sensor is based on the comparison of the
RSSI with respect to a ﬁxed threshold to decide if the target is in the vicinity or not of
the sensor. The main defect of this approach is the loss of a signiﬁcant amount of
information by imposing a threshold to the RSSI (binary quantiﬁcation).

254
Regularization and Bayesian Methods for Inverse Problems
10.3. Model-less target tracking
The target tracking problem could be resolved in a Bayesian framework. If x(t) is
the position of the target to estimate at time t and yt,m is the observation raised by the
sensor m at time t, the state model is written as:
*
xt ∼p(xt | xt−1)
yt,m = gm(xt) + ϵt
where the function gm(·) links the position of the target to the observation of sensor
m. In general, this function is assumed to be known in state-of-the-art tracking
methods. However, it can be noted that in practice, the function g cannot be perfectly
known. In addition, this function, even when it is valid, has parameters which vary
according to the geographical positions of the sensors. Figure 10.2 shows, for
example, the Received Signal Strength (RSSI) exchanged between two Mica2 sensors
(marketed by Crossbow) according to their distance. The inaccuracy of this model,
which is largely used in the literature, can be noted. In addition, a simple rotation of
the sensors can completely change the parameters of the RSSI curve.




	




	











Distance (m)
Figure 10.2. Inaccuracy of the model linking the distance to the radio power
exchanged between two Mica2 sensors (Crossbow – www.xbow.com).
For a color version of the ﬁgure, see www.iste.co.uk/giovannelli/regularization.zip
In this work, we propose a tracking methodology without any information on the
observation model. The relationship between the observed data and the position of
the target is learned in a local way only from data measured by the sensors. The
principle of the method consists of exploiting the powers exchanged between the
sensors themselves in addition to the data obtained from the observation of the target.
As a matter of fact, as the positions of the sensors are known, the additional data of
the exchanged powers makes it possible to obtain what is called as learning data in

Kernel Variational Approach for Target Tracking in a Wireless Sensor Network
255
the machine learning community [SCH 01] (see Figure 10.3). The problem of
tracking is reformulated as a problem of matrix completion. The resolution of this
matrix completion leads to a linear/Gaussian relationship between the observations
and the unknown state of the system (geographical position). The linearity of the
relationship obtained makes it possible to solve the problem of tracking in a very
efﬁcient and quick manner from an implementation point of view. In fact, two
algorithms can be used: Kalman ﬁltering or variational ﬁltering. Compared to its
typical implementation [SNO 06], variational ﬁltering, due to the linearity of the
relationship between the observations and the system state, is becoming very fast.
Tracked target
Activated sensor
Figure 10.3. The sensors (with known positions) can measure
their own RSSI =⇒learning data!
The advantages of this method compared to conventional approaches can be
summarized in the following points:
– tracking without observation model through the local learning of the relationship
between the position of the target and the observations;
– robustness in relation to a non-stationary environment (see Figure 10.4);
– robustness in relation to the deviation of a model even if it is known (degradation
of the sensor, etc.);
– quick implementation of the variational ﬁltering due to the linearity of the
equations obtained by the matrix completion.
10.3.1. Construction of the likelihood by matrix regression
This section is dedicated to the technical aspects of the local construction of a
linear and Gaussian likelihood model by exploiting the measured data between
sensors with known positions. In order to establish a link with the problem of matrix
regression, the observed data are considered as similarity data between the sensor
and the target. Our unique starting hypothesis on the nature of the observations is the
fact that the similarity data measured between the sensors and the targets or between

256
Regularization and Bayesian Methods for Inverse Problems
the sensors themselves is a strictly decreasing function of the distance. No model is
assumed to be known.
Region 1
Sensor
Passive target
Region 2
Figure 10.4. Tracking robustness in non-stationary environments:
the parameters of the observation model can vary depending on the region
where the sensors are deployed
At each moment t, it is assumed that a set of n sensors {s(t)
1 , ...,s(t)
n } are selected
to estimate the position of the target xt. In addition, it is assumed that the similarity
data measured between the sensors and the target and between the sensors
themselves are available at the level of a sensor selected for the update of the ﬁltering
process (see section 10.3.2). The inter-sensor similarity data play the role of learning
data which will be exploited without knowledge of the model which binds the
distances (see Figure 10.3). By following the principle of kernel trick, commonly
used in the machine learning community, similarity data are considered as scalar
products in reproducing kernel Hilbert space (RKHS). In other words, the similarity
measure between a sensor s(t)
i
and another sensor s(t)
j
is regarded as the scalar
euclidean product between their representatives φ(s(t)
i ) and φ(s(t)
j ) in the RKHS:
K(s(t)
i , s(t)
j ) =
5
φ(s(t)
i ), φ(s(t)
j )
6
.
According to this formulation, the matrix N × N (with N = n + 1) of similarity
data corresponds to the kernel matrix K (measured and therefore perfectly known)
whose elements are deﬁned as follows:
⎧
⎪
⎨
⎪
⎩
(K)i,j = K(s(t)
i , s(t)
j )
1 ⩽i ̸= j ⩽n
(K)i,n+1 = K(s(t)
i , xt)
1 ⩽i ⩽n
(K)l,l = c = const.
1 ⩽l ⩽n + 1

Kernel Variational Approach for Target Tracking in a Wireless Sensor Network
257
Since the position of the target is unknown, the matrix G (N × N) formed by the
Euclidean inner product of {s(t)
1 , s(t)
2 , . . . , s(t)
n , xt} has a number of unknown entries
corresponding to the scalar products between the sensors with known positions and
the target with an unknown position. The objective of matrix completion consists of
estimating the missing entries of the matrix G while exploiting a correlation form with
the complete matrix K. While dividing the matrix G in four blocks Gtt, Gtp, Gpt and
Gpp corresponding respectively to the scalar products sensor/sensor, sensor/target,
target/sensor and target/target, the problem of matrix completion can be illustrated by
the following diagram:
Ktt
Ktp
Kpt
Kpp
7
89
:
K
→
Gtt
Gtp
Gpt
Gpp
7
89
:
G
[10.1]
where the objective is the prediction of the unknown blocks (in gray) Gtp, Gpt and
Gpp while estimating a relation between the blocks (known) Ktt and Gtt. Let us note
that in our problem of tracking only the block Gtp is of interest to us since it contains
the scalar products between the sensors and the target:
Gtp = [(s(t)
1 )txt, . . . , (s(t)
i )txt, . . . , (s(t)
n )txt]t
[10.2]
It is useful to note that the matrix Gtp is linear in comparison with the unknown
position of the target xt. This property will be exploited for an effective
implementation of the variational ﬁlter in section 10.3.2.
In order to resolve the problem of matrix completion, a method of matrix
regression has been proposed in [YAM 07]. It is essentially based on the formulation
of the problem in the RKHS space. According to this approach, instead of predicting
the missing block Gtp, it is proposed to extend this method to calculate a probability
distribution of Gtp. Using the same reasoning as [YAM 07], it can be shown that the
relation between the blocks of the complete matrix K and the blocks of the
incomplete matrix G is written as follows:
Gtt = KttAKtt + Ψtt
[10.3]
Gtp = KttAKtp + Ψtp
[10.4]
Gpp = KptAKtp + Ψpp
[10.5]

258
Regularization and Bayesian Methods for Inverse Problems
where A is an unknown matrix and Ψ = (ϵi,j)i,j=1..N is a matrix (N × N) of
Gaussian variables (iid) with the same variance σ2
ij = σ2.
According to the above statistics formulation of the regression problem, it can be
shown that, knowing the matrices Gtt,Ktt and Ktp, the matrix Gtp is Gaussian with
the mean and the covariance given by the following expressions:
* μg = GttK−1
tt Ktp
Σg = σ2(KptK−2
tt Ktp + 1)In
[10.6]
where In is the n × n identity matrix.
The Gaussian distribution of the vector Gtp is one of the key-points of this
approach. Indeed, while noting S = [s(t)
1 ,s(t)
2 , . . . ,s(t)
n ]t the matrix n × 2 of the
positions of the selected sensors at the instant t, the Gaussian aspect of Gtp can be
expressed by the following relation:
Gtp = Sxt = GttK−1
tt Ktp + γt
[10.7]
where γt is a Gaussian noise of mean zero and diagonal covariance matrix Σg deﬁned
by [10.6]. The expression [10.7] can be considered as the statistical model resulting
from linking the observed data and the positions of the moving objects (passive targets
or mobile sensors). This model will play the role of the likelihood function when we
will implement the Bayesian ﬁltering (see next section). The quantity GttK−1
tt Ktp in
the term on the right of the equation [10.7] can be interpreted as the sufﬁcient statistics
summarizing all the data available at the current instant t.
10.3.2. Variational ﬁltering for the tracking of mobile objects
In the following, the likelihood function will be based on the linear Gaussian
model [10.7] obtained by the method of matrix regression described in the previous
section. The dynamics of the system state xt is described by a continuous Gaussian
mixture model (mean-scale mixture). According to this model, introduced in
[VER 03] regarding target visual tracking, the hidden state xt ∈
nx follows a
Gaussian distribution of random mean μt and random precision matrix λt. The mean
follows a Gaussian random-walk, reﬂecting the temporal correlation of the trajectory
of the hidden state of the system and the precision matrix follows a Wishart
distribution:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
μt ∼N(μt | μt−1,¯λ)
λt ∼W¯n(λt | ¯S)
xt ∼N(xt | μt,λt)
[10.8]

Kernel Variational Approach for Target Tracking in a Wireless Sensor Network
259
where the hyperparameters ¯λ, ¯n and ¯S are respectively the precision matrix of the
random-walk, the degree of freedom and the precision matrix of the Wishart
distribution. It should be noted that the randomness of the mean and of the precision
matrix induces an a priori marginal distribution whose tail behavior can be adjusted
in a simple manner according to the values of the hyperparameters [BAR 77]. In
addition, a heavy-tailed distribution allows an effective monitoring of trajectories
presenting abrupt jumps. In a matter of fact, the transition marginal distribution is
obtained by integrating with regard to the mean and to the random precision matrix:
p(xt | xt−1) =

N(xt | μt, λt)p(μt, λt | xt−1) dμt dλt
[10.9]
where the integration with respect to the precision matrix brings forward the class of
generalized
hyperbolic
distributions
(or
scale
mixtures)
introduced
by
Barndorff–Nielsen [BAR 77]. The value of the degree of freedom ¯n of the Wishart
distribution strongly conditions the behavior of the tails of the marginal [10.9]
distribution.
Figure
10.5
shows
examples
of
scalar
generalized
hyperbolic
distributions obtained by varying its parameters. The ﬂexibility of this family of laws
to cover very varied behaviors in tails of the probability distributions can be noted. It
should also be noted that, despite the linearity of the likelihood function, the
application of the Kalman ﬁlter is impossible because of the nonlinearity of the
transition dynamic [10.8].
Depending on the transition model [10.8], the increased hidden status becomes
αt = (xt, μt, λt). Instead of approximating the ﬁltering distribution p(αt | y1..t) by
a set of weighted particles (particulate ﬁltering [DJU 05, DOU 00]), the principle of
the online variational approach consists of approaching this distribution by another
simpler functional q(αt) by minimizing the Kullback–Leibler divergence compared
to the true ﬁltering distribution:
DKL(q∥p) =

q(αt) ln
q(αt)
p(αt | y1..t) dαt
[10.10]
By imposing a separable form (non-parametric) q(αt) = q(xt) q(μt) q(λt) and
minimizing the Kullback–Leibler [10.10] divergence with the tools of variational
calculus, the following iterative procedure can be obtained:
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
q(xt) ∝exp
5
ln p(y1..t,αt)
6
q(μt)q(λt) ∝N(xt | x∗
t ,Γ∗
t )
q(μt) ∝exp
5
ln p(y1..t,αt)
6
q(xt)q(λt) ∝N(μt | μ∗
t ,λ∗
t )
q(λt) ∝exp
5
ln p(y1..t,αt)
6
q(xt)q(μt) ∝Wn∗(λt | S∗
t )
[10.11]

260
Regularization and Bayesian Methods for Inverse Problems
where the parameters are updated in an iterative fashion according to the following
diagram:
x∗
t = Γ∗−1
t
(StΣ−1
g GttK−1
tt Ktp + ⟨λt⟩⟨μt⟩)
[10.12]
Γ∗
t = StΣ−1
g S + ⟨λt⟩
[10.13]
μ∗
t = λ∗−1
t
(⟨λt⟩⟨xt⟩+ λp
t μp
t )
[10.14]
λ∗
t = ⟨λt⟩+ λp
t
[10.15]
n∗= ¯n + 1
[10.16]
S∗
t = (⟨xtxt
t⟩−⟨xt⟩⟨μt⟩t −⟨μt⟩⟨xt⟩t + ⟨μtμt
t⟩+ ¯S
−1)−1
[10.17]
μp
t = μ∗
t−1
[10.18]
λp
t = (λ∗−1
t−1 + ¯λ−1)−1
[10.19]










































a)
b)
Figure 10.5. Examples of generalized hyperbolic laws: a) hyperbolic case; b) Cauchy case.
The densities are in the ﬁrst line and the log-densities in the second line. The dotted line
corresponds to the Gaussian distribution with the same mean and the same variance

Kernel Variational Approach for Target Tracking in a Wireless Sensor Network
261
Let us note that all the variables have mathematical expectation easy to calculate:
⟨xt⟩= x∗
t ,
⟨xtxt
t⟩= Γ∗−1
t
+ x∗
t x∗T
t ,
⟨μt⟩= μ∗
t ,
⟨μtμt
t⟩= λ∗−1
t
+ μ∗
t μ∗T
t ,
⟨λt⟩= n∗S∗
t
REMARK 10.1.– In previous works [SNO 06, VER 03], for a likelihood function with
a general form p(yt | xt), the position of the target does not have a simple analytical
distribution q(xt). To calculate its mean and its covariance, Monte Carlo simulations
were necessary. However, in this work, due to the linear and Gaussian form of the
likelihood, the mean and the covariance of xt are calculated with an exact expression.
REMARK 10.2.– It should be noted that the calculation of q(αt) is implemented in
a sequential manner (in time) based only on the knowledge of q(μt−1). As a matter
of fact, taking into account the separable form from the distribution at the previous
instant t −1, the ﬁltering distribution is written:
p(αt | y1:t) ∝p(yt | xt)p(xt,λt | μt)

p(μt | μt−1)q(αt−1) dαt−1
∝p(yt | xt)p(xt,λt | μt)

p(μt | μt−1)q(μt−1) dμt−1
where only the integration with respect to μt−1 is used due to the separable form of
q(αt−1). This is also one of the key points of this approach where the time dependency
is limited to the functional of a single component. In a decentralized context, the
communication between two units in charge of the update of the ﬁltering distribution is
limited to sending q(μt−1) which thus represents the sufﬁcient statistics. In addition, a
simple calculation allows us to show that this function is a Gaussian one and therefore
that the communication between two successive leading nodes resumes to sending the
mean, and the covariance. Therefore, the classical approach [IHL 05] consisting of
updating ﬁrst the probability densities and second approximating them is no longer
necessary. This joint data processing and the approximation of the sufﬁcient statistics
is the key point of the approach presented here.
10.4. Simulation results
In this section, we illustrate the effectiveness and robustness of the kernel VBA
(DD-VF) algorithm for monitoring a moving target in a wireless sensor network,
comparing it to the traditional variational ﬁlter with a known observation model. We
have considered a trajectory composed of two sinusoids in a 2D ﬁeld (see
Figure 10.6), for a period of 200 time units. A sudden change in trajectory is
simulated at the moment ta = 100 in order to test the ability of the algorithm to track
the target in the difﬁcult case of discontinuous trajectory. A set of 500 sensors is
randomly deployed on a surface of 120 × 120 m2. Each sensor has a coverage of

262
Regularization and Bayesian Methods for Inverse Problems
20 m. At each time t, the known matrices Ktt and Ktp (algorithm inputs) are
simulated according to the following stationary model:

Ktt(i, j) = exp{−∥s(t)
i
−s(t)
j ∥/2σ2} + ϵ(t)
ij ,
1 ⩽i, j ⩽n
Ktp(j) = exp{−∥s(t)
j
−x∗
t ∥/2σ2} + ϵ(t)
j ,
1 ⩽j ⩽n
[10.20]
where s(t)
m = (sm
1 ,sm
2 ) and x∗
t = (x1,x2) are the positions of the active sensor and the
target at time t. The parameter σ is ﬁxed at 10 and ϵ(t)
ij designates the noise caused by
the modeling error, the measurement noise and the interference of neighboring signals.
The noise variance depends on the distance between sensors and the distance between
the sensor and the target.
The matrix Gtt is formed of scalar products ⟨s(t)
i ,s(t)
j ⟩, for i, j = 1, . . . , n between
the known positions of the sensors.
The maximum number of sensors activated at each moment is ﬁxed at 10. The
selection protocol is based on the calculation of the predictive distribution of the
target position. It is shown in [SNO 06], based on variational calculus, that this
distribution is Gaussian, which makes the protocol very simple to implement. More
details on the protocols for the selection of the sensors as well as the comparison of
their performance could be found in [TEN 07, TEN 10b].
The hyperparameters of the transition model (a priori model) have been set as
follows:
¯λ = 10−2I, ¯n = 1, ¯S = 102I
which induce a very ﬂexible and non-informative a priori model. It should be noted
that in conventional tracking methods, a linear transition model based on velocity and
acceleration is often used. In this work, the model is mainly general, which makes it
usable in other applications.
The proposed tracking algorithm is applied to estimate the online position of the
target without any knowledge of the observation [10.20] model. Figure 10.6 shows the
estimated trajectory of the target. The position of the target is estimated by a posteriori
estimation (ˆxt = ⟨xt⟩) depending on the approximated ﬁltering density q(xt). Special
attention should be given to the precision of the tracking assessed according to the
mean square error EQM = 
t ∥xt −ˆxt∥2/T = 0.29. On the same Figure 10.6, the
10 sensors selected to process the information are marked by circles for four instants:
t = 40, t = 80, t = 160 and t = 190. Emphasis should be placed on the ability of the
algorithm to select the most relevant sensors based on a compact Gaussian form of the
predictive distribution.

Kernel Variational Approach for Target Tracking in a Wireless Sensor Network
263














Figure 10.6. Model-less variational ﬁltering in a cooperative sensors network: the estimated
positions are in blue and the real positions in red. The 10 selected sensors are drawn in circles,
for four given instants, t = 40, t = 80, t = 160 and t = 190. For a color version of the ﬁgure,
see www.iste.co.uk/giovannelli/regularization.zip
For purposes of comparison, the conventional VBA (VF) algorithm [SNO 06] is
applied to track the target in the same conditions above, with ten sensors selected at
each moment. The VF algorithm is applied using the true observation [10.20] model
having been used to generate simulation data. Figure 10.7 shows the result of the
tracking of the moving target. The mean square error is evaluated at EQM = 1.3. It
should be noted that the conventional VBA algorithm is less efﬁcient than the proposed
algorithm which does not use the observation model. This difference in performance
can be explained by the fact that the proposed algorithm exploits in addition data from
the inter-sensors RSSI. This difference in performance may widen further when the
conventional algorithm uses an incorrect observation model or when the deployment
ﬁeld of the sensors is nonstationary. As a matter of fact, the proposed algorithm is
based on the training data collected by the sensors to estimate a local observation
model, which makes it very robust compared to the nonstationarity of this model or
the inﬂuence of the degradation of the sensors.

264
Regularization and Bayesian Methods for Inverse Problems














Figure 10.7. Conventional variational ﬁltering in a cooperative sensor network: the estimated
positions are in blue and the real positions in red. The 10 selected sensors are drawn in circles,
for four given instants, t = 40, t = 80, t = 160 and t = 190. For a color version of the ﬁgure,
see www.iste.co.uk/giovannelli/regularization.zip
10.5. Conclusion
In this chapter, we have described, in a summary manner, one of our works on
decentralized information processing in wireless sensor networks. This work presents
a new methodology exploiting both the consistency of the Bayesian approach as well
as a learning technique in order to overcome the lack of a reliable observation model.
The exploitation of signals exchanged between the sensors of known positions is
formulated by a matrix (completion) regression problem in a Hilbert space with
reproducing kernel. The Gaussian shape of the obtained likelihood function allows
the quick and effective implementation of the variational ﬁlter. This technique can be
applied for both target tracking and self-localization of mobile sensors.
10.6. Bibliography
[BAR 77] BARNDORFF-NIELSEN
O.,
“Exponentially decreasing distributions for the
logarithm of particle size”, Proceedings of the Royal Society of London A, vol. 353, pp. 401–
419, 1977.

Kernel Variational Approach for Target Tracking in a Wireless Sensor Network
265
[DJU 05] DJURI ´C P., VEMULA M., BUGALLO M., “Tracking with particle ﬁltering in tertiary
wireless sensor networks”, IEEE International Conference on Acoustic, Speech and Signal
Processing, Philadelphia, PA, March 2005.
[DOU 00] DOUCET A., GODSILL S., ANDRIEU C., “On sequential Monte Carlo sampling
methods for Bayesian ﬁltering”, Statistics and Computing, vol. 10, no. 3, pp. 197–208,
2000.
[DOU 02] DOUCET A., VO B., ANDRIEU C., et al., “Particle ﬁltering for multitarget tracking
and sensor management”, International Conference on Information Fusion, vol. 1, pp. 474–
481, March 2002.
[IHL 05] IHLER A., FISHER J., WILLSKY A., “Particle ﬁltering under communications
constraints”, IEEE Workshop on Statistical Signal Processing, pp. 89–94, July 2005.
[PAT 04] PATWARI N., HERO A., “Manifold learning algorithms for localization in wireless
sensor networks”, IEEE International Conference on Acoustic, Speech and Signal
Processing, vol. 3, pp. 857–860, May 2004.
[SAY 05] SAYEED A.M., ESTRIN D., POTTIE G.J., et al., “Special issue on self-
organizing distributed collaborative sensor networks”, IEEE Journal on Selected Areas in
Communications, vol. 23, no. 4, 2005.
[SCH 01] SCHÖLKOPF B., SMOLA A., Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond, The MIT Press, Cambridge, 2001.
[SHE 05] SHENG X., HU Y., RAMANATHAN P., “Distributed particle ﬁlter with GMM
approximation for multiple targets localization and tracking ion wireless sensor network”,
IEEE/ACM International Conference on Information Processing in Sensor Networks,
pp. 181–188, April 2005.
[SNO 06] SNOUSSI H., RICHARD C., “Ensemble learning online ﬁltering in wireless sensor
networks”, IEEE International Conference on Communications Systems, 2006.
[TEN 07] TENG J., SNOUSSI H., RICHARD C., “Prediction-based proactive cluster target
tracking protocol for binary sensor networks”, IEEE Symposium on Signal Processing and
Information Technology, 2007.
[TEN 10a] TENG J., SNOUSSI H., RICHARD C., “Decentralized variational ﬁltering for target
tracking in binary sensor networks”, IEEE Transactions on Mobile Computing, 2010.
[TEN 10b] TENG J., SNOUSSI H., RICHARD C., “Prediction-based cluster management for
target tracking in wireless sensor networks”, International Journal of Wireless and Mobile
Computing, 2010.
[VER 03] VERMAAK J., LAWRENCE N., PEREZ P., “Variational inference for visual
tracking”, IEEE Conference on Computer Vision and Pattern Recognition, June 2003.
[YAM 07] YAMANISHI Y., VERT J.-P., “Kernel matrix regression”, Proceedings of the 12th
International Conference on Applied Stochastic Models and Data Analysis (ASMDA 2007),
2007.


11
Entropies and Entropic Criteria
11.1. Introduction
This chapter focuses on the notions of entropy and of maximum entropy
distribution which will be characterized according to different perspectives. Beyond
links with applications in engineering and physics, it will be shown that it is possible
to build regularization functionals based on the use of a maximum entropy technique,
which can then possibly be employed as ad hoc potentials in data inversion problems.
The chapter begins with an overview of the key properties of information
measures, and with the introduction of various concepts and deﬁnitions. In particular,
the Rényi divergence is deﬁned, the concept of escort distribution is presented, and
the principle of maximum entropy that will be subsequently used will be commented
on. A conventional engineering problem is then presented, the problem of source
coding, and it shows the beneﬁt of using measures with a different length than the
standard measure, and in particular an exponential measure, which leads to a source
coding theorem whose minimum bound is a Rényi entropy. It is also shown that
optimal codes can easily be calculated with escort distributions. In section 11.4, a
simple state transition model is introduced and examined. This model leads to an
equilibrium distribution deﬁned as a generalized escort distribution, and as a
by-product leads once again to a Rényi entropy. The Fisher information ﬂow along
the curve deﬁned by the generalized escort distribution is examined and connections
with the Jeffreys divergence are achieved. Finally, various arguments are obtained
which, in this framework, lead to an inference method based on the minimization of
the Rényi entropy under a generalized mean constraint, that is to say, taken with
regard to the escort distribution. From section 11.5.3, the main concern is about the
minimization of the Rényi divergence subject to a generalized average constraint.
The optimal density that solves this problem, and the value of the corresponding
Chapter written by Jean-François BERCHER.

268
Regularization and Bayesian Methods for Inverse Problems
optimal divergence are given and characterized. The main properties of any entropy
that may be related are deﬁned and characterized. Finally, it is shown how to
practically calculate these entropies and how it can be envisaged to use them for
solving linear problems.
11.2. Some entropies in information theory
The concept of information plays a major role in a number of scientiﬁc and
technical ﬁelds and in their applications. Moreover, information theory, “the Shannon
way”, meets the theories of physics, mutually fertilizing each other;
these
interactions have been exploited by Jaynes [JAY 57a, JAY 57b] since 1957, are
discussed for example by Brillouin [BRI 62] and more recently in the fascinating
work of [MER 10]. We will further give a simple model of phase transition that
yields a Rényi entropy.
A fundamental question in information theory is of course the measure, or the
deﬁnition, of the information. Several approaches are possible. The ﬁrst is pragmatic
and accepts as a measure of valid information the measures that appear by
themselves when solving a practical problem. The second is axiomatic, which starts
with a certain number of reasonable properties or postulates, and then carries on with
the mathematical derivation of the functions that exhibit these properties. This is the
point of view adopted originally by Shannon,
in his fundamental article
[SHA 48a, SHA 48b], and that has led to a number of subsequent developments,
among which [ACZ 75] and [ACZ 84] will be cited (where the author warns against
the excesses of generalizations:
“I wish to urge here caution with regard to
generalizations in general, and in particular with regard to those introduced through
characterizations. (...) There is a large number of “entropies” and other
“information
measures”
and
their
“characterizations”,
mostly
formal
generalizations of (1), (19), (16), (24), (17), (23) etc. popping up almost daily in the
literature. It may be reassuring to know that most are and will in all probability be
completely useless.”
Similarly, Rényi himself [CSI 06, REN 65] stressed that only quantities that can
actually be used in concrete problems should be considered as information measures,
in agreement with the pragmatic approach (As a matter of fact, if certain quantities
are deduced from some natural postulates (from “ﬁrst principles”) these certainly
need for their ﬁnal justiﬁcation the control whether they can be effectively used in
solving concrete problems).
11.2.1. Main properties and deﬁnitions
We recall here the main properties used for the characterizations of information
measures. If P, Q, R refer to discrete probability distributions for n events, with pk

Entropies and Entropic Criteria
269
the probability associated with the k-th event k = 1, . . . , n, then noting H(P) =
H(p1,p2, . . . , pn) the information measure related to distribution events P, the main
properties are as follows:
P1) symmetry: H(p1,p2, . . . , pn) does not depend on the order of the events;
P2) H(p,1 −p) is a continuous function of p;
P3) H(1/2,1/2) = 1;
P4) recursion (branching):
Hn+1(p1q1,p1q2,p2, . . . , pn) = Hn(p1,p2, . . . , pn) + p1H2(q1,q2);
P5) expansibility: Hn+1(p1,p2, . . . , pn,0) = Hn(p1,p2, . . . , pn);
P6) subadditivity: H(PQ) ⩽H(P) + H(Q)
(and additivity in the independent case: H(PQ) = H(P) + H(Q));
P7) conditional subadditivity: H(PQ|R) ⩽H(P|R) + H(Q|R);
P8) generalized recursion:
Hn+1(p1q1,p1q2,p2, . . . , pn) = Hn(p1,p2, . . . , pn) + m(p1)H2(q1,q2).
Simple consequences
The ﬁrst four postulates are Faddeev’s axioms [FAD 56], that sufﬁce to uniquely
characterize the Shannon entropy:
H(P) = −
n

i=1
pi ln pi
[11.1]
If the recursion postulate is evaluated but an additivity requirement is added, then
the class of possible solutions is much wider, and includes in particular the Rényi
entropy, which will be referred to further in the text. The replacement of the P4
recursion by a general recursion postulate, P8, with m(p1p2) multiplicative
m(p1p2) = m(p1)m(p2) and especially m(p) = pq leads to the entropy of order q:
Hq(P) =
1
21−q −1
 n

i=1
pq
i −1

[11.2]
which was introduced by [HAV 67], independently by Darczy [DAR 70], and then
rediscovered in the ﬁeld of statistical physics by C. Tsallis [TSA 88]. For q ⩾1,
these entropies are subadditive, but are not additive. In the case of q = 1, by

270
Regularization and Bayesian Methods for Inverse Problems
l’Hôpital’s rule, the entropy of order q = 1 is none other than the Shannon entropy. In
statistical physics, a signiﬁcant community has formed around the study of
non-extensive thermodynamics (non-additive in fact) [TSA 09] based on the use of
the Tsallis entropy, on associated maximum entropy distributions and on the
extension of classical thermodynamics.
In Faddeev’s axiomatics, Rényi [REN 61] has proposed to replace the recursion
postulate by the additivity property, and add a property of mean entropy, which
speciﬁes that the entropy of the union of two incomplete probability distributions is
equal to the weighted average of the two entropy distributions. When the mean being
used is an arithmetic mean, the only solution is the Shannon entropy. However, by
using an exponential mean, the entropy that appears is a Rényi entropy:
Hq(P) =
1
1 −q ln
n

i=1
pq
i
[11.3]
Another way to apprehend the Rényi entropy is to note that the Shannon entropy is
the arithmetic mean, with weights pi, of the basic information Ii = −ln pi associated
with the different events. By replacing the arithmetic mean by a Kolmogorov–Nagumo
average, the entropy becomes:
Hψ(p1, . . . , pn) = ψ−1 "
piψ(−ln pi)
#
Under an additional additivity condition and under the condition limp→0 Hψ(p,
1 −p) = 0, this entropy is either the Shannon entropy, the Rényi entropy, with q ⩾
0. Again, by l’Hospital’s rule, the Shannon entropy is met once again for q = 1.
Furthermore, for q = 0, the Rényi entropy becomes the Hartley entropy, the logarithm
of the number of events of nonzero probability.
11.2.2. Entropies and divergences in the continuous case
In the continuous case, the deﬁnition used for the Shannon entropy associated with
a density f(x) is:
H[f] = −

f(x) ln f(x) dx
[11.4]
However, it should be noted that this expression only results from the transition
to the limit of the discrete case up to an additive constant tending to inﬁnity (see
for example [PAP 81]). Therefore, the concern is rather about differential entropy.
However, Jaynes has lucidly noted that, since [JAY 63, p. 202], it is necessary to
introduce a measure m(x) accounting for “points density” shifting the procedure to

Entropies and Entropic Criteria
271
the limit; this measure conferring in addition a coordinate change invariance to the
resulting information, which is not the case of [11.4]. The corresponding differential
entropy then takes the form:
H[f] = −

f(x) ln f(x)
m(x) dx
[11.5]
This form is similar to a Kullback–Leibler divergence [KUL 59] (or I-divergence
in Csiszr’s terminology) between two probability distributions with densities f(x) and
g(x) relative to a common measure μ(x), and which is deﬁned by:
D(f||g) =

f(x) ln f(x)
g(x) dμ(x)
[11.6]
by assuming g absolutely continuous relatively to f, and with the convention
0 ln 0 = 0. When g is uniform, with respect to μ, the Kullback divergence becomes,
in absolute value, a μ-entropy. In the case where μ is the Lebesgue measure, the
differential Shannon [11.5] entropy appears once again; in the discrete case, if μ is
the counting measure, then [11.1] will appear again. It is easily shown, by application
of Jensen’s inequality, that the Kullback divergence is deﬁned as non-negative,
D(f||g) ⩾0 with equality if and only if f = g. Thus, it can be understood as a
distance between distributions, although it is not symmetric and does not check the
triangle inequality.
In the same way, continuous versions of the Rényi and Tsallis entropies can be
deﬁned. For an entropy index q ̸= 1:
Sq[f] =
1
1 −q
	
f(x)q dμ(x) −1

[11.7]
is the Tsallis entropy and:
Hq[f] =
1
1 −q ln

f(x)q dμ(x)
[11.8]
that of Rényi. These two entropies are tantamount to the Shannon entropy for q = 1.
Divergence can also be associated with them; for example, the Rényi divergence:
Dq(f||g) =
1
q −1 ln

f(x)qg(x)1−q dμ(x)
[11.9]
which is also deﬁned as non-negative (by Jensen’s inequality), and is reduced to the
Kullback divergence when q →1.

272
Regularization and Bayesian Methods for Inverse Problems
11.2.3. Maximum entropy
The principle of maximum entropy is widely used in physics, and can rely on a
large number of arguments: counts, axioms, etc. The principle has been particularly
highlighted by Jaynes [JAY 57a] “Information theory provides a constructive criterion
for setting up probability distributions on the basis of partial knowledge, and leads
to a type of statistical inference which is called the maximum entropy estimate. It
is the least biased estimate possible on the given information; i.e., it is maximally
noncommittal with regard to missing information”, and we will conﬁne ourselves here
to recalling its relevance in terms of statistics, by following Ellis [ELL 99] (theorem 2
for example).
If fN is the empirical distribution corresponding to the collection of N random
variables according to a density distribution g relatively to μ, then the probability Q
of ﬁnding fN in a set B is roughly (see Ellis [ELL 99] for more correct formulations),
and for large N:
Q (fN ∈B) ≈exp
"
−N inf
P ∈B D(f||g)
#
[11.10]
Thus, it can be derived, by iterating reasoning on subsets of B, that the absolutely
predominant distribution in B is the one that achieves the minimum Kullback distance
to g: there is concentration of all the probability on the closest distribution to g. A
minimum distance Kullback principle can thus be derived, or equivalently, if g is
uniform, a principle of maximum entropy. Among all the distributions of a set B, the
density that minimizes D(f||g) should be selected. When the point of interest is, as
in statistical physics, the probability of ﬁnding an empirical mean xN, that is to say,
the mean under fN, in a set C, then a result with large level 1 deviations is obtained,
which indicates that:
Q (xN ∈C) ≈exp
"
−N inf
x∈C F(x)
#
[11.11]
where F(x) is the rate function F(x) = infP :x=E[X] D(P||μ). This result thus
suggests to select the most probable element, that which achieves the minimum of
F(x) on C. The shift from problematics of distributions to problematics of means is
known as the contraction principle.
11.2.4. Escort distributions
We will also use in the remainder of this chapter, the notion of escort distribution.
These escort distributions have been introduced as a tool in the context of
multifractals [BEC 95, CHH 89],
with interesting connections with standard
thermodynamics. Escort distributions are proving useful in source coding, where they

Entropies and Entropic Criteria
273
enable optimal code words to be obtained whose mean length is bounded by a Rényi
entropy [BER 09]. This is what we will present in 11.3.3. We will then ﬁnd these
escort distributions in the framework of a state transition problem, section 11.4.
If f(x) is a probability density, then its escort of order q ⩾0 is:
fq(x) =
f(x)q
4
f(x)q dμ(x)
[11.12]
provided that the informational generating function Mq[f] =
4
f(x)q dμ(x) is ﬁnite.
We can easily see that if fq(x) is the escort of f(x), then f(x) is itself the escort of
order 1/q of fq(x). When q decreases, the escort comes closer to a uniform
distribution whereas when q increases, density modes are ampliﬁed. This can be
speciﬁed: as a matter of fact, it can be shown in the compact support case that
D(fq||U) > D(f||U) for q > 1, and that D(fq||U) < D(f||U) for q < 1, which
means that fq is further away from the uniform than f when q > 1 and closer
otherwise.
The concept of escort distribution can also be expanded in order to take into
account two densities f(x) and g(x) according to:
fq(x) =
f(x)qg(x)1−q
4
f(x)qg(x)1−q dμ(x)
[11.13]
when Mq[f, g] =
4
f(x)qg(x)1−q dμ(x) < ∞. This generalized escort distribution is
simply a weighted geometrical mean of f(x) and g(x). Of course, if g(x) is a uniform
measure whose support includes that of f(x), then the generalized escort is reduced
to the standard escort [11.12]. This generalized escort appears in the analysis of the
effectiveness of hypotheses tests [CHE 52] and allows the best possible exponent to be
deﬁned in the error probability [COV 06, Chapter 11]. When q varies, the generalized
escort describes a curve that connects f(x) and g(x). Finally, we will call generalized
moments the moments taken with respect to an escort distribution: the generalized
moment of order p associated with the standard escort of order q will be:
mp,q[f] =

|x|pfq(x) dx =
4
|x|pf(x)q dμ(x)
4
f(x)q dμ(x)
[11.14]
11.3. Source coding with escort distributions and Rényi bounds
In this section, the advantage of the Rényi entropy and escort distributions is
illustrated within the framework of source coding, one of the fundamental problems
of information theory. After a very brief reminder of the context of source coding, a

274
Regularization and Bayesian Methods for Inverse Problems
source coding theorem is described linking a new measure of mean length and the
Rényi entropy. It is then shown that it is possible to practically calculate the optimal
codes by using the concept of escort distribution. Details about these elements as
well as other results are given in [BER 09].
11.3.1. Source coding
In source coding, we consider a set X = {x1, x2, . . . , xN} of symbols generated
by a source with respective probabilities pi where N
i=1 pi = 1. The role of source
coding is to associate with each symbol xi a code word ci, with length li, expressed
with an alphabet of D elements. It is well-known that if the lengths verify the Kraft–
Mac Millan inequality:
N

i=1
D−li ⩽1
[11.15]
then there exists a uniquely decodable code with these elementary lengths. In
addition,
any
uniquely
decodable
code
satisﬁes
the
Kraft–Mac
Millan
inequality [11.15]. Shannon’s source coding theorem indicates that the mean length
¯L of word codes is bounded from below by the source entropy, H1(p), and that the
best uniquely decodable code satisﬁes:
H1(p) ⩽¯L =

i
pili < H1(p) + 1
[11.16]
where the logarithm used in the Shannon entropy is calculated in base D, and noted
logD. This result indicates that the Shannon entropy H1(p) is a fundamental limit to
the minimal mean length for any code built for the source. The lengths of the optimal
word codes are given by:
li = −logD pi
[11.17]
The characteristic of these optimal codes is that they assign the shortest words to
the most probable symbols and the longest words to the rarest symbols.
11.3.2. Source coding with Campbell measure
It is well known that the Huffman algorithm provides a preﬁx code that minimizes
the mean length and approaches the optimal length limits li = −logD pi. However,
other forms of length measurement have also been considered. In particular the ﬁrst,
that of Campbell [CAM 65], is fundamental. It has been seen, by relation [11.17], that

Entropies and Entropic Criteria
275
the lowest probabilities lead to longer word codes. However, the cost of using a code
is not necessarily a linear function of its length, and it is possible that the addition of
a letter to a long word is much more expensive than the addition of the same letter
to a short word. This led Campbell to propose a new measure of mean length, by
introducing an exponential penalty of the lengths of the word codes. This length, the
Campbell length, is a generalized Kolmogorov–Nagumo average associated with an
exponential function:
Cβ = 1
β logD
N

i=1
piDβli
[11.18]
with β > 0. The remarkable result of Campbell is that, in the same way as the Shannon
entropy places a lower bound on the average length of the word codes, the Rényi
entropy of order q, with q = 1/(β + 1), is the lower bound of the mean Campbell
length [11.18]:
Cβ ⩾Hq(p)
[11.19]
A simple demonstration of the result is given in [BER 09]. It is easy to see that
equality is obtained for:
li = −logD Pi = −logD

pq
i
N
j=1 pq
j

[11.20]
Clearly, the lengths li obtained in this way can be made smaller than the optimal
Shannon lengths, by choosing a quite small parameter q, which then tends to
standardize the distribution, then actually enhancing the lowest probabilities. Thus,
the procedure effectively penalizes the longest word codes and provides word codes
of different lengths than Shannon’s, with eventually shorter word codes associated
with the low probabilities.
11.3.3. Source coding with escort mean
For the usual mean length measure ¯L = 
i pili, there is a linear combination of
the elementary length, weighted by probabilities pi. In order to increase the impact of
the most important lengths associated with low probabilities, the Campbell length uses
an exponential of the elementary lengths. Another idea is to modify the weights in the
linear combination, so to increase the importance of the words with low probabilities.
A simple way to achieve this is to standardize the initial probability distribution, and

276
Regularization and Bayesian Methods for Inverse Problems
to use the weights achieved by this new distribution rather than pi. Naturally, this leads
to use a mean taken with an escort distribution:
Mq =
N

i=1
pq
i

j pq
j
li =
N

i=1
Pili
[11.21]
In the case of the imaginary source that would have a distribution P the standard
statistics mean is Mq, and Shannon’s classical source coding theorem can immediately
be applied:
Mq ⩾H1(P)
[11.22]
with equality if:
li = −logD Pi
[11.23]
or exactly the lengths [11.20] obtained for Campbell’s measure. The simple relation
li = −logD Pi obtained for the minimization of Mq under the constraint supplied
by Kraft–Mac Millan’s inequality has an immediate but important application. As
a matter of fact, it simply sufﬁces to provide the escort distribution P rather than
the initial distribution p to an standard encoding algorithm, for example a Huffman
algorithm, to obtain an optimized code for the Campbell length Cβ, or in a similar
manner, for the measurement of length Mq. Table 11.1 gives a simple example with
D = 2 : we have used a standard Huffman algorithm, with the initial distribution, then
its escorts of order q = 0.7 and q = 0.4.
pi
q = 1
q = 0,7
q = 0,4
0,48 0
0
00
0,3
10
10
01
0,1
110
1100
100
0,05 1110
1101
101
0,05 11110
1110
110
0,01 111110 11110
1110
0,01 111111 11111
1111
Table 11.1. Example of binary codes, for different values of q
It is important to note that speciﬁc algorithms have been developed for the mean
Campbell length. The above connection provides an easy and immediate alternative.
Another important point is that these codes have practical applications: they are
optimal for the minimization of the probability of buffer overﬂowing [HUM 81] or,
with q > 1, for maximizing the probability of receiving a message in a single send of
limited size.

Entropies and Entropic Criteria
277
11.4. A simple transition model
In the previous section, we have seen emerging and appreciated the signiﬁcance
of the Rényi entropy and escort distributions for a source coding problem. In this
section, we will show that these two quantities are also involved in an equilibrium, or
a transition model framework, between two states. It has actually been noted that
extended thermodynamics, associated with the Tsallis and Rényi entropies, seems
particularly
relevant
in
the
case
of
deviations
from
the
conventional
Boltzmann–Gibbs equilibrium. This then suggests amending the conventional
formulation of the conventional approach of maximum entropy (or of the the
minimum of divergence) and imagining an equilibrium characterized by two (and no
longer a single) distributions: rather than selecting the nearest distribution of a
reference distribution under a mean constraint, an intermediary distribution pq(x) is
desired, in a sense that needs clariﬁcation, between two references p0(x) and p1(x).
This construction, as well as some of its consequences, are also described in
[BER 12].
11.4.1. The model
Let us consider two density states with probabilities p0(x) and p1(x) at point x
of the phase space, and search for an intermediate state according to the following
scenario. The initial state system p0, subject to a generalized force, is moved and held
at a distance η = D(p||p0) of p0. However, the system is attracted toward a ﬁnal
state p1. As a result, the new intermediate state pq is chosen such that it minimizes
its divergence from the attractor p1 while being maintained at a distance η of p0. As
illustrated in Figure 11.1, the intermediate probability density is “aligned” with p0
and p1 and at the intersection with the set D(p||p0) = η, a circle of radius η centered
on p0. More speciﬁcally, by taking densities relative to the Lebesgue measure, the
problem can be formulated as follows:
⎧
⎨
⎩
minp D(p||p1)
under D(p||p0) = η
and
4
p(x) dx = 1
[11.24]
The solution is given by the following proposition.
PROPOSITION 11.1.– If q is a real positive that D(pq||p0) = η and if Mq(p1,p0) =
4
p1(x)qp0(x)1−q dx < ∞, then, the solution of problem [11.24] is given by:
pq(x) =
p1(x)qp0(x)1−q
4
p1(x)qp0(x)1−q dx
[11.25]
REMARK.– When p0 is uniform and with compact support, the standard escort
distribution [11.12] is met once again. If media is not compact and uniform

278
Regularization and Bayesian Methods for Inverse Problems
distribution improper, it is possible to simply change the formulation by taking as a
constraint a ﬁxed entropy H(p) = −η, and then the escort distribution is obtained.
η
D(p||p1)
D(p||p0)
p0
p1
pq
p
a) Cas η < D(p1||p0)
η
D(p||p1)
D(p||p0)
p0
p1
pq
p
b) Cas η > D(p1||p0)
Figure 11.1. Equilibrium between the states p0 and p1:
the search for the equilibrium
distribution is carried out in all of the distributions within a difference ﬁxed at p0, D(p||p0) = η,
and at a minimal Kullback distance of p1. The resulting equilibrium distribution pq, the
generalized escort distribution is “aligned” with p0 and p1, and at the intersection of the set
D(p||p0) = η
Let us evaluate the divergence D(p||pq). For all densities p such that constraint
D(p||p0) = η is satisﬁed, it yields:
D(p||pq) =

p(x) ln p(x)
pq(x) dx =

p(x) ln p(x)qp(x)1−q
p1(x)qp0(x)1−q dx + ln Mq(p1,p0)
= q

p(x) ln p(x)
p1(x) dx + (1 −q)

p(x) ln p(x)
p0(x) dx + ln Mq(p1,p0)
= q D(p||p1) + (1 −q)η + ln Mq(p1,p0)
[11.26]
By taking p = pq, the last equality becomes:
D(pq||pq) = q D(pq||p1) + (1 −q)η + ln Mq(p1,p0)
[11.27]
Finally by subtracting [11.26] and [11.27], it gives:
D(p||pq) −D(pq||pq) = q (D(p||p1) −D(pq||p1))
[11.28]
Since q ⩾0 and D(p||pq) ⩾0 with equality if and only if p = pq, it ﬁnally yields
D(p||p1) ⩾D(pq||p1) which proves proposition 11.1.

Entropies and Entropic Criteria
279
When η varies, the function q(η) is increasing, with still D(pq||p0) = η. For
η = 0 it gives q = 0 and for η = D(p1||p0) it gives q = 1. Therefore, when q varies,
pq deﬁnes a curve that links p0 (q = 0) to p1 (q =1), and beyond for q > 1, see
Figure 11.1.
REMARK 11.1.– It is also interesting to note that results have shown that work
dissipated during a transition can be expressed as a Kullback–Leibler divergence
[PAR 09]. In this context, with a Hamiltonian pair following the impulse, the
constraint D(p||pk) = η, k = 0 where 1, can be interpreted as a bound on the
average work dissipated during the transition from p to pk.
11.4.2. The Rényi divergence as a consequence
Finally, it is interesting to note that the Rényi divergence appears as a byproduct
of our construction. As a matter of fact, as a direct consequence of [11.27] and of the
deﬁnition of the Rényi divergence [11.9], the minimum Kullback information can be
expressed as:
D(pq||p1) =
	
1 −1
q

(η −Dq(p1||p0))
[11.29]
By taking a uniform measure for p0, the Rényi entropy is revealed.
D(pq||p1) =
	
1 −1
q

(η + Hq [p1])
[11.30]
The Kullback–Leibler divergence is not symmetrical. From the beginning,
Kullback and Leibler have introduced a symmetrical version, returning again to the
Jeffreys divergence. In our case, this Jeffreys divergence is a simple afﬁne function of
the Rényi divergence:
J(p1,pq) = D(p1||pq) + D(pq||p1) = (q −1)2
q
(Dq(p1||p0) −η)
[11.31]
This equality is a simple consequence of relation [11.26], with p = p1, and
relation [11.27]. It can be noted, as a signiﬁcant consequence, that the minimization
of the Jeffreys divergence between p1 and pq under certain constraints, is therefore
equivalent to the minimization of the Rényi divergence with the same constraints.
11.4.3. Fisher information for the parameter q
The generalized escort distribution pq deﬁned a curve indexed by q linking
distributions p0 and p1 for q = 0 and q = 1. It is interesting to evaluate the attached
information to the parameter q of the generalized distribution. This Fisher
information is given by:
I(q) =

1
pq(x)
	dpq(x)
dq

2
dx =
 dpq(x)
dq
ln p1(x)
p0(x) dx
[11.32]

280
Regularization and Bayesian Methods for Inverse Problems
where the right term is obtained using the relation:
dpq(x)
dq
= pq(x)
	
ln p1(x)
p0(x) −d ln Mq
dq

[11.33]
and the fact that:

dpq(x)
dq
dx =
d
dq

pq(x) dx = 0
by Leibniz’s rule. It can also be shown that this Fisher information is equal to the
variance, with respect to the distribution pq, of the likelihood ratio.
Finally, it is possible to identify the integral of the Fisher information along the
curve, the “energy” of the curve, at the Jeffreys divergence. More speciﬁcally, the
following proposal is given.
PROPOSITION 11.2.– The integral of the Fisher information, from q = r to q = s is
proportional to the Jeffreys divergence between pr and ps:
(s −r)
 s
r
I(q) dq = J(ps,pr) = D(ps||pr) + D(pr||ps)
[11.34]
With r = 0 and s = 1, it therefore yields that:
 1
0
I(q) dq = J(p1,p0) = D(p1||p0) + D(p0||p1)
[11.35]
To demonstrate [11.34], it is sufﬁcient to integrate [11.32]:
 s
r
I(q) dq =
 s
r

dpq(x)
dq
ln p1(x)
p0(x) dx dq
=

(ps(x) −pr(x)) ln p1(x)
p0(x) dx
Taking into account the fact that ln ps/pr = (s −r) ln p1/p0, we then get [11.34].
Finally, if θi, i = 1..M is a set of intensive variables depending on q, then
d ln p
dq
= M
i=1
∂ln p
∂θi
dθi
dq and the Fisher information q can be expressed according to
the Fisher information matrix of θ. In these conditions, and for the generalized escort
distribution, the result is that the “thermodynamic divergence” for the transition is
none other than the Jeffreys divergence [11.35]:
J =
 1
0
I(q) dq =
M

i=1
M

j=1
 1
0
dθi
dq [I(θ)]i,j
dθ
dq dq
= D(p1||p0) + D(p0||p1)
[11.36]

Entropies and Entropic Criteria
281
11.4.4. Distribution inference with generalized moment constraint
Let us assume now that the p1 distribution is imperfectly known, but that additional
information is available under the form of a mean value, achieved with distribution pq.
This mean is the generalized mean [11.14], which is used in non-extensive statistical
physics; here, it has the clear interpretation of a mean obtained from the equilibrium
distribution pq. The problem that arises now is thus the determination of the most
general distribution compatible with this constraint.
The idea of minimizing the divergence to p1 can be retained as in the problem
[11.24] which has led us to the equilibrium distribution with generalized escort.
Since the Kullback divergence is directed, the direction will be retained by
minimizing D(pq||p1) for q < 1 and D(p1||pq) for q > 1. In both cases, the
divergence is expressed as an afﬁne function of the Rényi divergence Dq(p1||p0),
see [11.29], and these minimizations are ﬁnally equivalent to the minimization of the
Rényi divergence under the generalized mean constraint.
Similarly, the concern could be about the minimization of the symmetric Jeffreys
divergence between pq and p1. However, we have noted in [11.31] that this is also
expressed as a simple afﬁne function of the Rényi divergence. Its minimization is
therefore equivalent to the minimization of the Rényi divergence under a generalized
mean constraint.
Finally, the Jeffreys divergence J(p1,pq) is proportional to the thermodynamic
divergence, the integral of the Fisher information, as shown in [11.34], for q > 1
as well as for q < 1. Therefore, the minimization of the thermodynamic divergence
between pq and p1 is also equivalent to the minimization of the Rényi divergence.
These different arguments very legitimately lead us to search for distribution p1
as the distribution minimizing the Rényi divergence of index q, under the generalized
mean constraint.
11.5. Minimization of the Rényi divergence and associated entropies
In the previous section, we have described a framework naturally yielding the
concepts of Rényi information, escort distributions and generalized moments. In
addition, we have derived an inference distribution method: the minimization of the
Rényi information, with information available in the form of generalized moments.
In this section, we will ﬁrst give the expression for the density that minimizes the
Rényi divergence, then we will describe some properties of the associated partition
functions. Finally, we will show how new entropic functionals can be derived a few
examples of which will be given. Some of these results, but also some extensions,
can be referred to in [BER 08].

282
Regularization and Bayesian Methods for Inverse Problems
11.5.1. Minimization under generalized moment constraint
We will ﬁrst consider a generalized moment of any order [11.14], whose
expression is mentioned below:
mp,q[f] =

|x|pfq(x) dμ(x) =
4
|x|pf(x)qg(x)1−q dμ(x)
4
f(x)qg(x)1−q dμ(x)
[11.37]
The problem is then considered:
Fq(m) =
⎧
⎨
⎩
minf Dq(f||g)
under m = mp,q[f]
and
4
f(x) dμ(x) = 1
[11.38]
The minimum obtained is of course a function of m, that will be noted Fq(m).
It is a contracted version of the Rényi divergence, which deﬁnes an “entropy” in the
space of possible means m. In [BER 11], we have considered a more general problem,
in which the indices of the generalized moment and of the Rényi divergence are not
identical. In any case, the result obtained here is as follows.
PROPOSITION 11.3.– The density Gγ which achieves the minimum in the
problem [11.38] is given by:
Gγ(x) =
1
Zν(γ,¯xp) (1 −(1 −q)γ (|x|p −¯xp))ν
+ g(x)
[11.39]
or equivalently by:
G¯γ(x) =
1
Zν(¯γ) (1 −(1 −q)¯γ|x|p)ν
+ g(x)
[11.40]
with ν = 1/(1 −q), ¯xp an eventual translation parameter, γ and ¯γ selected scaling
parameters chosen such that the generalized constraint moment is satisﬁed, and
ﬁnally where (x)+
= max(0,x). Quantities Zν(γ,¯xp) and Zν(¯γ) are partition
functions that allow the standardization of the density. For q = 1, density Gγ(x)
becomes an exponential density:
Gγ(x) =
1
Zν(γ) exp (−γ (|x|p −¯xp)) g(x)
[11.41]
with respect to g(x).
In the case p = 2, a Gaussian density is thus found once again. Density Gγ is
sometimes called “generalized Gaussian”. It should be noted once more that γ and ¯γ
are determined by the relation:
¯γ =
γ
1 + γ
ν ¯xp
[11.42]
In the case of expression [11.40], the demonstration is proposed here. The
approach is rather similar in the case of density [11.39].

Entropies and Entropic Criteria
283
As in [BER 11], let A(¯γ) = 1/Z(¯γ). It immediately yields:

f qG1−q
¯γ
dμ(x) = A(¯γ)1−qMq[f, g] ×

(1 −(1 −q)¯γ|x|p) +
f qg1−q
Mq[f, g] dμ(x)
⩾A(¯γ)1−q (1 −(1 −q)¯γ mp,q[f]) Mq[f, g]
[11.43]
with Mq[f, g] =
4
f ag1−q dμ(x), where mp,q[f] refers to the generalized moment,
and where the inequality results from the fact that the support (1 −(1 −q)¯γ|x|p)+can
be included in that of f qg1−q. From [11.43] it directly gives, with f = G¯γ:
M1[G¯γ] = 1 = A(¯γ)1−q (1 −(1 −q)¯γ mq,p[G¯γ]) Mq[G¯γ, g]
[11.44]
Thus, for all distributions f of generalized moment mp,q[f] = m and for ¯γ such
that G¯γ has the same moment mp,q[G¯γ] = m, then the combination of [11.43]
and [11.44] results in:

f qG1−q
¯γ
dμ ⩾Mq[f, g]
Mq[G¯γ, g]
Finally, the Rényi divergence of order q can thus be expressed as:
Dq(f||G¯γ) = ln
	
f qG1−q
¯γ
dμ(x)

1
q−1
[11.45]
⩽ln
	 Mq[f, g]
Mq[G¯γ, g]

1
q−1
= Dq(f||g) −Dq(G¯γ||g)
[11.46]
By the non-negativity of the divergence, it thus ensues that:
Dq(f||g) ⩾Dq(G¯γ||g)
[11.47]
for all distributions f of generalized mp,q[f] = mp,q[G¯γ] = m, and with equality if
and only if f = G¯γ.
11.5.2. A few properties of the partition functions
Some important properties of partition functions Zν(γ,¯xp) associated with the
optimal density Gγ are given here (see [BER 08]). These properties will be essential
for the characterization of entropic functionals Fq(x). Eν refers to the statistic mean
taken relatively to the optimum density distribution [11.39], with ν = 1/(1 −q). It is
also important to realize, from now on, that the escort density of order q of [11.39] is
none other than this same density Gγ but with an exponent ν −1, such that:
mp,q[G¯γ] = Eν−1[X]
[11.48]
The successive partition functions are linked by:
Zν(γ,xp) = Eν−1
;
1 −γ
ν (|x|p −xp))
<
Zν−1(γ,x)
[11.49]

284
Regularization and Bayesian Methods for Inverse Problems
As a direct result, it can be seen that Zν(γ,xp) = Zν−1(γ,xp) if and only if
xp = Eν−1 [|X|p] .
Using Leibniz’s rule, the derivative with respect to γ can be obtained and is given
by:
d
dγ Zν(γ,xp) =
	
−Eν−1 [|X|p −xp] + γ dxp
dγ

Zν−1(γ,xp)
[11.50]
under the condition that xp is really differentiable with respect to γ. Similarly:
d
dxp
Zν(γ,xp) =
	
−dγ
d¯xp
Eν−1 [|X|p −xp] + γ

Zν−1(γ,xp)
[11.51]
Thus, if xp = Eν−1 [|X|p] , then taking into account the equality of the partition
functions of rank ν and ν −1, it gives:
d
dγ ln Zν(γ,xp) = γ dxp
dγ
[11.52]
or even:
d
dxp
ln Zν(γ,xp) = γ
[11.53]
On the other hand, when xp is an independent parameter of γ, say xp = m, then:
d2Zν(γ,m)
dγ2
= ν −1
ν
Eν−2

(X −m)2
Zν−2(γ,m)
[11.54]
and similarly:
d2Zν(γ,m)
dm2
= ν −1
ν
γ2Eν−2

(X −m)2
Zν−2(γ,m)
[11.55]
which, considering the fact that (ν −1)/ν = q > 0, the fact that partition functions
are strictly positive, shows that if xp = m and γ are independent, then the partition
Zν(γ,m) is convex in its two variables.
Finally, the solution of problem [11.38] can be expressed, that is Fq(m), from the
partition function. By direct calculation, it actually yields:
Dq(Gγ||g) =
1
q −1 ln Zqν(γ,¯xp) −
q
q −1 ln Zν(γ,¯xp)
[11.56]
which is simply reduced to:
Fq(m) = Dq(Gγ||g) = −ln Zν(γ,m) = −ln Zν−1(γ,m)
[11.57]
for the value of γ such that the constraint is satisﬁed, or mp,q[G¯γ] = Eν−1[X] =
¯xp = m.

Entropies and Entropic Criteria
285
11.5.3. Entropic functionals derived from the Rényi divergence
Thus, the solution of the minimization problem of the Rényi divergence of order q
viewed as a function of constraint, deﬁnes an “entropic functional”. Different
functionals will be associated with the several speciﬁcations of the reference density
g(x), as well as with the various values of the index q. We will see that the functions
in question present interesting properties. Therefore, a set of functions is potentially
available that can be eventually used as objective functions or regularization terms.
The main characterization of Fq(m) is as follows.
PROPOSITION 11.4.– The entropy Fq(m), deﬁned by [11.38], is non-negative, with
a single minimum mg, the average of g, and Fq(mg) = 0. The entropy is a pseudo-
convex function for q ∈[0,1) and strictly convex for q ⩾1.
The Rényi divergence Dq(f||g) is always non-negative, and zero only for f = g.
Since functionals Fq(m) are deﬁned as the minimum of the divergence Dq(f||g),
they are always non-negative. Based on [11.53], it gives
d
dx ln Zν(γ,x) = γ. Thus,
functionals Fq(x) are only presenting a single singular point in γ = 0. For this value
of γ, it yields Gγ=0 = g, and Dq(g||g) = 0. Under these conditions, Fq(x) has a
unique minimum for x = mg, the mean of g, and Fq(mg) = 0. Therefore, it follows
that Fq(x) is unimodal and does not present any points of inﬂection with a horizontal
tangent; this is sufﬁcient to claim that Fq(x) is pseudo-convex, as referred to by
Mangasarian [MAN 87]. Let us now examine the convexity for q ⩾1. If fq is the
generalized
escort
distribution
given
by
[11.13],
then
the
equality
Dq(f||g) = D1/q(fq||g) holds. Subsequently, searching for the distribution f that
achieves the minimum of Dq(f||g) with a generalized mean constraint, that is to say,
taken with respect to fq, is equivalent to searching the distribution fq that minimizes
D1/q(fq||g), under a standard moment constraint. In these circumstances, given p1
and p2 the two densities that minimize D1/q(fq||g) under constraints x1 = Efq[X]
and x2 = Efq[X]. Then, Fq(x1) = D1/q(p||g), and Fq(x2) = D1/q(p2||g). In the
same way, given Fq(μx1 + (1 −μ)x2) = D1/q( ˆP||Q), where ˆP is the optimal escort
distribution of mean μx1 + (1 −μ)x2. Distributions ˆP and μp1 + (1 −μ)p2 then
have the same mean. Thus, when D1/q(fq||g) is a strictly convex function fq, that is
to say for q ⩾1 it follows that D1/q( ˆP||g) < μD1/q(p1||g) + (1 −μ)D1/q(p2||g),
or Fq(μx1 + (1 −μ)x2) < μFq(x1) + (1 −μ)Fq(x2) and entropy Fq(x) is a
strictly convex function.
Even provided with this interesting characterization, a practical signiﬁcant
problem still remains: how to analytically or numerically determine the entropies
Fq(x) for a reference density g and a given index entropy q. The problem amounts to
determining the parameter γ such that the optimal generalized mean density [11.39]
has a speciﬁed value m. A simple manner of proceeding consists of recalling the fact

286
Regularization and Bayesian Methods for Inverse Problems
that if ¯xp is a ﬁxed parameter m, independent of γ, then the derivative relation
[11.50] is reduced to:
d
dγ Zν(γ,m) = (m −Eν−1 [|X|p]) Zν−1(γ,m)
[11.58]
Therefore, it can be seen that it sufﬁces to search for the extrema of the partition
function Zν(γ,m) to obtain a γ such that m = Eν−1 [|X|p] . Since we have seen
that Zν(γ,m), with ﬁxed m, is strictly convex, then this extremum is unique and is
a minimum. Finally, the value of the entropy is simply given by [11.57]: Fq(m) =
−ln Zν(γ,m).
The search for the expression of Fq(m) therefore requires us to compute the
partition function then to solve
d
dγ Zν(γ,m) = 0 with respect to γ. With the
exception of a few special cases, this resolution does not seem analytically possible,
and entropy Fq(m) is given implicitly. In the particular case where g is a Bernoulli
measure, it is possible to obtain an analytic expression for Fq(m), this for any q > 0.
For other reference densities g, it is possible to obtain analytic expressions when
q →1. These points are detailed in [BER 08], where in addition, different densities
of reference g are studied, and corresponding entropies numerically evaluated
according to the scheme previously mentioned. As an example, the numerical results
obtained in the case where p = 1 and a uniform density in the interval [0,1] are given
in Figure 11.2. For q ⩾1, a family of convex functions is correctly obtained over
(0,1), minimum for the mean of g, or 0.5, as we have indicated above. For q < 1, a
family of non-negative, unimodal functions, also minimal in x = mg = 0.5 is
obtained.
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
x
q=1.02
q=2
q=3
q=10
a) Case q > 1
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
x
q=0.01
q=0.6
q=0.98
b) Case q ∈(0,1)
Figure 11.2. Entropy Fq(x) for a uniform reference,
respectively for q ⩾1 and q ∈]0,1[

Entropies and Entropic Criteria
287
11.5.4. Entropic criteria
Based on the previous ﬁgures, it is apparent that the minimization of Fq(x) under
certain constraints automatically provides a solution in the interval (0,1). In addition,
the parameter q can be used to adjust the curvature of the function or the penalty on
the boundaries of the domain. It is thus interesting to use these entropies when the
purpose is to solve inverse problems. More speciﬁcally, an entropy criterion can be
used such as Fq(x) as objective function. The whole of this section will be restricted
to the case p = 1. When considering a linear inverse problem y = Ax, with xk the
components of x, then this can be formulated as:
minx

k Fq(xk)
under y = Ax
[11.59]
This then corresponds to selecting among possible solutions the solution whose
components are of minimum entropy. It should be noted that it is assumed here,
implicitly, that the criterion is separable into its components. In reality, if we deﬁne
Fq(x) as the Rényi divergence under the generalized mean constraint, then, even
when assuming that components are independent, it yields a density on x similar
to [11.39], which is not separable. In order to obtain a separable criterion, which is
both more consistent with intuition and easier to use, we amend the formulation by
searching the density product that achieves the minimum of the Rényi divergence
under generalized mean constraint, which leads effectively to the separable criterion.
Thus, the previous problem [11.59] can also be read as:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
minx
⎧
⎨
⎩
minf Dq(f||g)
under f = 2
k fk
and x = Efq[X]
under y = Ax
[11.60]
where Efq[X] refers to the generalized mean, that is to say, taken with respect to the
escort distribution of order q. The point of concern here is therefore a “maximum
entropy” problem which consists of selecting a solution x, seen as the generalized
mean of a minimum Rényi divergence distribution with a reference density g.
Entropies 
k Fq(xk) being pseudo-convex, it is known that minimization under
linear constraints leads to a single minimum (see for example [CAM 08, theorem
4.4.1]). Now let us examine how it is possible to obtain a solution of [11.59], even in
the case where the entropies have no explicit expression. The solution corresponds to
a stationary point of the Lagrangian L(λ,x) associated with problem [11.59], and the
objective is therefore to solve:
min
x max
λ
L(λ,x) = min
x max
λ

k
Fq(xk) + λt (y −Ax)
[11.61]
= min
x max
λ

k
Fq(xk) −ckxk + λty
[11.62]

288
Regularization and Bayesian Methods for Inverse Problems
with ck =

λtA

k. Using the fact that:
Fq(xk) = −ln Zν(γ∗,xk) = −inf
γ ln Zν(γ,xk)
as we have seen in section 11.5.3, it thus gives:
min
x max
λ
L(λ,x) = min
x max
λ

k
−ln Zν(γ∗,xk) −ckxk + λty
[11.63]
= max
λ
λty −

k
max
xk (ln Zν(γ∗,xk) + ckxk)
[11.64]
However, by relation [11.53], it follows:
d
dxk
(ln Zν(γ∗,xk) + ckxk) = γ∗+ ck
[11.65]
which yields γ∗= −ck, and xk is the associated generalized mean. Finally, the
concern is therefore about solving:
max
λ
λty −

k
(ln Zν(−ck,xk) + ckxk)
[11.66]
where, for any ck, the corresponding generalized mean xk can be calculated as the
unique solution of the problem:
xk = arg min
x
(ln Zν(−ck,x) + ckx)
It is thus possible to solve problem [11.59] which provides a unique “maximum
Rényi entropy” solution to the inverse linear problem y = Ax, problem where various
constraints can be included, including support, through the reference density g, and
where the form of the criteria can be adjusted by means of the index entropy q.
In the case where data y would be imperfect, it is possible to minimize the entropy
criterion under a constraint provided by a statistic (for example, residual χ2) rather
than with an exact constraint. It is also possible to use the entropy criterion with a
data-ﬁdelity term.
In the case where q = 1, the Rényi divergence is reduced to the Kullback
divergence, the generalized moments to the usual moments, and the optimal
density to an exponential density [11.41] with respect to g. Under these
conditions, the log-partition function is written ln Z∞(−ck,xk)
=
−ckxk+
ln
4
exp (ckxk) g(xk) dμ(xk), problem [11.66] becomes:
max
λ
λty −

k
ln

exp (ckxk) g(xk) dμ(xk)
and the optimal solution is given by the derivative of the log-partition function with
respect to ck. This latter approach has been developed in works supervised by Guy
Demoment [LEB 99].

Entropies and Entropic Criteria
289
11.6. Bibliography
[ACZ 75] ACZÉL J., DAROCZY Z., On Measures of Information and their Characterizations,
Academic Press, New York, 1975.
[ACZ 84] ACZÉL J., “Measuring information beyond communication theory?
Why some
generalized information measures may be useful, others not”, Aequationes Mathematicae,
vol. 27, no. 1, pp. 1–19, March 1984.
[BEC 95] BECK C., SCHLOEGL F., Thermodynamics of Chaotic Systems: An Introduction,
Cambridge Nonlinear Science Series (Book 4), Cambridge University Press, p. 308, 24
February, 1995.
[BER 08] BERCHER J.-F., “On some entropy functionals derived from Rényi information
divergence”, Information Sciences, vol. 178, no. 12, pp. 2489–2506, June 2008.
[BER 09] BERCHER J.-F., “Source coding with escort distributions and Rényi entropy
bounds”, Physics Letters A, vol. 373, no. 36, pp. 3235–3238, August 2009.
[BER 11] BERCHER
J.-F.,
“Escort entropies and divergences and related canonical
distribution”, Physics Letters A, vol. 375, no. 33, pp. 2969–2973, August 2011.
[BER 12] BERCHER J.-F., “A simple probabilistic construction yielding generalized entropies
and divergences, escort distributions and q-Gaussians”, Physica A: Statistical Mechanics
and its Applications, vol. 391, no. 19, pp. 4460–4469, October 2012.
[BRI 62] BRILLOUIN L., Science and Information Theory, Academic Press, New York, 2nd
ed., 1962.
[CAM 65] CAMPBELL L.L., “A coding theorem and Rényi’s entropy”, Information and
Control, vol. 8, no. 4, pp. 423–429, 1965.
[CAM 08] CAMBINI A., MARTEIN L., “Generalized Convexity and Optimization: Theory
and Applications”, Lecture Notes in Economics and Mathematical Systems, Springer,
Berlin, vol. 616, p. 248, November 21, 2008.
[CHE 52] CHERNOFF H., “A measure of asymptotic efﬁciency for tests of a hypothesis based
on the sum of observations”, Annals of Mathematical Statistics, vol. 23, no. 4, pp. 493–507,
1952.
[CHH 89] CHHABRA A., JENSEN R.V., “Direct determination of the f(α) singularity
spectrum”, Physical Review Letters, vol. 62, no. 12, p. 1327, March 1989.
[COV 06] COVER T.M., THOMAS J.A., Elements of Information Theory, Wiley-InterScience,
New York, 2nd ed., 2006.
[CSI 06] CSISZR I., “Stochastics: information theory”, HORVTH J., (ed.), A Panorama of
Hungarian Mathematics in the Twentieth Century I, vol. 14, pp. 523–535, Springer, Berlin,
Heidelberg, 2006.
[DAR 70] DARCZY Z., “Generalized information functions”, Information and Control,
vol. 16, no. 1, pp. 36–51, March 1970.
[ELL 99] ELLIS R.S., “The theory of large deviations: from Boltzmann’s 1877 calculation
to equilibrium macrostates in 2D turbulence”, Physica D, vol. 133, no. 1–4, pp. 106–136,
September 1999.

290
Regularization and Bayesian Methods for Inverse Problems
[FAD 56] FADDEEV D., “On the concept of entropy of a ﬁnite probabilistic scheme”, Uspekhi
Matematicheskikh Nauk, vol. 11, no. 1, no. 67, pp. 227–231, 1956.
[HAV 67] HAVRDA J., CHARVT F., “Quantiﬁcation method of classiﬁcation processes.
concept of structural a-entropy”, Kybernetika, vol. 3, pp. 30–35, 1967.
[HUM 81] HUMBLET P., “Generalization of Huffman coding to minimize the probability of
buffer overﬂow”, IEEE Transactions on Information Theory, vol. 27, no. 2, pp. 230–232,
1981.
[JAY 57a] JAYNES E.T., “Information theory and statistical mechanics”, Physical Review A,
vol. 106, no. 4, pp. 620–630, May 1957.
[JAY 57b] JAYNES E.T., “Information theory and statistical mechanics. II”, Physical Review
A, vol. 108, no. 2, pp. 171–190, October 1957.
[JAY 63] JAYNES E.T., “Information theory and statistical mechanics”, 1962 Brandeis
Summer
Institute
in
Theoretical
Physics,
vol.
3,
pp.
182–218,
FORD
K.W.,
BENJAMIN W.A., Inc., New York, 1963, reissued in JAYNES E.T. Papers on Probability,
Statistics and Statistical Physics, in ROSENCRANTZ R.D., (ed.), Synthese Library,
vol. 138, Reidel, Dordrecht, 1963, Second paperbound ed., Kluwer Academic Publishers,
1989.
[KUL 59] KULLBACK S., Information Theory and Statistics, John Wiley & Sons, New York,
1959 (reissued by Dover Publications, 1997).
[LEB 99] LE BESNERAIS G., BERCHER J.-F., DEMOMENT G., “A new look at entropy for
solving linear inverse problems”, IEEE Transactions on Information Theory, vol. 45, no. 5,
pp. 1565–1578, 1999.
[MAN 87] MANGASARIAN O.L., Nonlinear Programming, SIAM, January 1987.
[MER 10] MERHAV N., “Statistical physics and information theory”, Foundations and Trends
in Communications and Information Theory, vol. 6, nos. 1–2, pp. 1–212, 2010.
[PAP 81] PAPOULIS A., “Maximum entropy and spectral estimation:
A review”, IEEE
Transactions on Acoustics, Speech and Signal Processing, vol. 29, no. 6, pp. 1176–1186,
December 1981.
[PAR 09] PARRONDO J.M.R., VAN DEN BROECK C., KAWAI R., “Entropy production and
the arrow of time”, New Journal of Physics, vol. 11, no. 7, July 2009.
[REN 61] RÉNYI A., “On measures of entropy and information”, 4th Berkeley Symposium on
Mathematical Statistics and Probability, Berkeley, CA, pp. 547–561, 1961.
[REN 65] RÉNYI A., “On the foundations of information theory”, Review of the International
Statistical Institute, vol. 33, no. 1, pp. 1–14, 1965.
[SHA 48a] SHANNON C., “A mathematical theory of communication”, The Bell System
Technical Journal, vol. 27, no. 3, pp. 379–423, 1948.
[SHA 48b] SHANNON C., “A mathematical theory of communication”, The Bell System
Technical Journal, vol. 27, no. 4, pp. 623–656, 1948.

Entropies and Entropic Criteria
291
[TSA 88] TSALLIS C., “Possible generalization of Boltzmann-Gibbs statistics”, Journal of
Statistical Physics, vol. 52, no. 1, pp. 479–487, July 1988.
[TSA 09] TSALLIS C., Introduction to Nonextensive Statistical Mechanics, Springer, New
York, 2009.


 
List of Authors
Hacheme AYASSO 
GIPSA-Lab 
University of Grenoble 
France 
 
Jean-François BERCHER 
LIGM 
ESIEE Paris 
University Paris-Est Marne-la-Vallée 
France 
 
Sébastien BOURGUIGNON 
IRCCyN 
École Centrale de Nantes 
France 
 
David BRIE 
CRAN 
University of Lorraine 
Nancy 
France 
 
Hervé CARFANTAN 
IRAP – OMP 
UPS, CNRS 
University of Toulouse 
France 
 
 
 
Lotfi CHAARI 
IRIT 
Toulouse 
France 
 
Frédéric CHAMPAGNAT 
ONERA DTIM 
Palaiseau 
France 
 
Philippe CIUCIU 
NeuroSpin 
CEA Saclay 
Gif-sur-Yvette 
France 
 
Bernard DUCHÊNE 
L2S 
Gif-sur-Yvette 
France 
 
Sylvain FAISAN 
ICube 
University of Strasbourg 
France 
  
 
 
 
 

294     Regularization and Bayesian Methods for Inverse Problems 
Florence FORBES 
INRIA Grenoble 
France 
 
Grégory FRANCIUS 
LCPME 
Villers-lès-Nancy 
France 
 
Aurélia FRAYSSE 
L2S 
Gif-sur-Yvette 
France 
 
Jean-François GIOVANNELLI 
IMS 
University of Bordeaux 1 
France 
 
Yves GOUSSARD 
École Polytechnique de Montréal 
Canada 
 
Christian HEINRICH 
ICube 
University of Strasbourg 
France 
 
Paul HONEINE 
UMR-STMR 
Institut Charles Delaunay 
University of Technology of Troyes 
France 
 
Jérôme IDIER 
IRCCyN 
École Centrale de Nantes 
France 
 
 
 
 
 
 
 
Caroline KULCSÁR 
Laboratoire Charles Fabry 
Institut d’Optique, CNRS 
University Paris-Sud 
Palaiseau 
France 
 
Guy LE BESNERAIS 
ONERA DTIM 
Palaiseau 
France 
 
Ali MOHAMMAD-DJAFARI 
L2S 
Gif-sur-Yvette 
France 
 
Cédric RICHARD 
Laboratoire Lagrange 
Observatoire de la Côte d’Azur 
University Sophia-Antipolis 
Nice 
France 
 
Thomas RODET 
L2S 
Gif-sur-Yvette 
France 
 
François ROUSSEAU 
ICube 
University of Strasbourg 
France 
 
Hichem SNOUSSI 
UMR-STMR 
Institut Charles Delaunay 
University of Technology of Troyes 
France 
 
 
 
 
 
 

List of Authors       295 
Charles SOUSSEN 
CRAN 
University of Lorraine 
Nancy 
France 
 
Thomas VINCENT 
LJK 
INRIA Grenoble 
France 
 
Jihad ZALLAT 
ICube 
University of Strasbourg 
France 
 
 

 

Index
A
algorithm
EM, 185, 253
FOLKI, 83, 95, 96
greedy, 44, 45, 150, 151, 153, 154
Huffman, 274, 276
IPOPT, 17
MCMC, 144, 194, 203, 213, 218
MP, 152, 153
OLS, 45, 48–50, 52, 153, 154
OMP, 45, 52, 151, 153, 154
SBR, 45, 46, 48–50, 52, 153, 163, 164
SMLR, 45, 161
SPS, 6, 13, 17
TRIOT, 13, 17, 18
variational EM (VEM), 186, 188, 189,
191, 194
astrophysics, 143, 225
atomic force microscopy, 31, 32
B, C
Bayes theorem, 161, 226, 235
Bayesian rule, 212
bi-model, 225
CLEAN, 144, 151, 154, 246
correlation
long-range, 174
spatial, 173, 179, 180, 183
temporal, 174, 258
D
deconvolution, 45, 104, 109, 114, 122,
142–147, 156–164, 227
degree of polarization, 59, 62, 72, 76, 77
dictionary, 43–47, 52, 53, 144, 147–151,
156, 158, 164, 165
Fourier, 150, 158
polynomial, 31, 46
Toeplitz, 43
distribution
conjugate, 163, 215, 227, 228, 235
Wishart, 258, 259
divergence
Jeffreys, 267, 279–281
Kullback(-Leibler), 185, 186, 203, 214,
215, 227, 229, 230, 259, 271, 279,
281, 288
Rényi, 267, 271, 279, 281–283, 285,
287, 288
E
entropy
Rényi, 267–270, 273–275, 277, 279
Shannon, 269–271, 274, 275
Tsallis, 270, 271
escort distribution, 267, 272–274,
276–281, 285, 287
exponentiated gradient, 229, 231,
240, 246

298
Regularization and Bayesian Methods for Inverse Problems
F, G
ﬁeld
Markov, 180, 214
Markov-Potts, 211
fMRI, 169–171, 174, 176, 178, 193
force curve, 33
force spectroscopy, 32, 33, 37, 40
Gauss-Newton, 91, 93, 105
Gauss-Seidel, 227
generalized moment, 273, 282, 283
Green’s function, 206–209
H
hemodynamics, 172, 173, 175, 176, 179,
193
hyperparameter, 11, 15, 17, 18, 22, 25, 27,
162, 163, 178–181, 212, 213, 215, 218,
219, 226, 240, 259
I
image registration, 82, 84, 86–88, 90–92,
94, 96, 100, 105, 115, 117, 122
imaging, 201
electromagnetic (MEG, EEG), 171
fonctional, 169
force-volume, 35, 52
microwave, 202–205, 220
optical, 2, 57, 110, 202–206
PIV, 82, 83, 89, 90, 95, 96
polarization, 57
integral representation, 206
inverse crime, 16, 206
irregular sampling, 141–161
IWS, 93, 95, 96
K, L
Kuhn-Tucker conditions, 62
length
Campbell, 275, 276
Debye, 38, 40, 52
Kuhn, 39
Shannon, 275
M
matrix
completion, 255, 257
covariance, 3, 93, 161, 163, 187, 214,
227, 234, 236, 258
Mueller, 59, 60
projection, 3, 7, 8, 10, 14
reduction, 61
Toeplitz, 43, 47, 100
maximum
a posteriori (MAP), 10, 212, 228
marginal (MMAP), 183
entropy, 179, 267, 270, 272, 277, 287,
288
likelihood, 5, 143–146, 152, 185
means
a posteriori (PM), 162, 163,
183, 212
posterior, 203
posterior (PM), 219
non-local, 58, 61, 64–66, 69, 71, 78
microwave imaging, 202–205
model
Bernoulli-Gaussian, 45, 144, 160, 163,
228, 235
electrostatic, 38
FJC, 39
Gauss-Markov-Potts, 202, 220
Hertz, 38
Hooke, 38
Markov, 202
multi-sinusoidal, 142, 147, 152, 164,
165
WLC, 39
mono/polyenergetic radiation, 4, 5, 7,
7–11, 13–16, 21–26
mutual coherence, 150
N
network
neural, 203
sensors, 251, 252, 261, 263, 264
neuroimaging, 169, 170
non-linear inverse problem, 201
O
optical
tomography, 201

Index
299
ﬂow, 81, 83–85, 89–91, 93, 95, 96,
105, 112, 115
system, 59
optimization
combinatorial, 149, 153
constrained, 62, 63
convex, 11, 61, 77, 287
discrete, 52
functional, 203, 230
local, 53
non-convex, 226
P
parsimony, 26, 32, 41–45, 48, 50, 52, 175
periodogram, 143–146, 152, 153, 157
piecewise smoothing, 41
polarization state, 58–60
polymer, 31, 32, 35, 36, 39, 40
problem
ill-posed, 2, 26, 27, 165, 202, 203, 220
inverse, 1, 26, 52, 65, 81, 147, 171,
201, 203, 206, 211, 220, 227, 235,
246, 287
linear, 147, 148, 171, 228, 268,
287, 288
non-linear, 27, 89–91, 109, 175,
201, 202, 220
poorly-stated, 226
pursuit, 151–154
R
radiance, 57, 61
reconstruction, 148, 220, 240, 242, 246
3D, 1, 7, 26, 27, 85
tomographic, 2, 5, 12
reproducing kernel, 256, 264
S
sampling
Gibbs, 163, 203, 213, 214, 219, 222
Metropolis-Hastings, 163, 183, 184,
226
segmentation, 40, 41, 51, 52, 212, 218, 220
source coding, 267, 272–274, 277
sparsity, 144–165, 228, 246
spectral analysis, 142–165
stereovision, 81, 82, 84, 86, 90–92, 97, 105
superresolution, 81, 89, 92, 105, 110, 113,
114, 119, 135
T
theorem
Green, 206
projection-slice, 202
tomography, 201
diffraction, 2, 27
emission, 2, 5, 27, 170
transmission, 2, 5
X-ray, 1
tracking, 252–258, 262–264
transform
Fourier, 126, 141, 145, 227
Radon, 202
V, X
variable selection, 43, 44, 46, 48–50
variational algorithm (VBA), 217, 219
variational Bayesian, 184–190, 203, 227,
228, 232, 255–258
video
processing, 81, 82, 104
rate, 59
recording, 84
X-rays, 2, 3, 9, 7, 10, 15, 16, 21, 22, 24, 25


Other titles from  
 
in 
Digital Signal and Image Processing 
2014 
AUGER François 
Signal Processing with Free Software: Practical Experiments 
BLANCHET Gérard, CHARBIT Maurice 
Digital Signal and Image Processing using MATLAB 
Volume 1 – Fundamentals – 2nd edition 
FANET Hervé 
Medical Imaging Based on Magnetic Fields and Ultrasounds 
MOUKADEM Ali, OULD Abdeslam Djaffar, DIETERLEN Alain 
Time-Frequency Domain for Segmentation and Classification of Non-
stationary Signals: The Stockwell Transform Applied on Bio-signals and 
Electric Signals 
NDAGIJIMANA Fabien 
Signal Integrity: From High Speed to Radiofrequency Applications 
PINOLI Jean-Charles 
Mathematical Foundations of Image Processing and Analysis –  
Volumes 1 and 2 

TUPIN Florence, INGLADA Jordi, NICOLAS Jean-Marie 
Remote Sensing Imagery 
VLADEANU Calin, EL ASSAD Safwan 
Nonlinear Digital Encoders for Data Communications 
2013 
GOVAERT Gérard, NADIF Mohamed 
Co-Clustering 
DAROLLES Serge, DUVAUT Patrick, JAY Emmanuelle 
Multi-factor Models and Signal Processing Techniques: Application to 
Quantitative Finance 
LUCAS Laurent, LOSCOS Céline, REMION Yannick 
3D Video: From Capture to Diffusion 
MOREAU Eric, ADALI Tulay 
Blind Identification and Separation of Complex-valued Signals 
PERRIN Vincent 
MRI Techniques 
WAGNER Kevin, DOROSLOVACKI Milos 
Proportionate-type Normalized Least Mean Square Algorithms 
FERNANDEZ Christine, MACAIRE Ludovic, ROBERT-INACIO Frédérique 
Digital Color Imaging 
FERNANDEZ Christine, MACAIRE Ludovic, ROBERT-INACIO Frédérique 
Digital Color: Acquisition, Perception, Coding and Rendering 
NAIT-ALI Amine, FOURNIER Régis 
Signal and Image Processing for Biometrics 
OUAHABI Abdeljalil 
Signal and Image Multiresolution Analysis 

2011 
CASTANIÉ Francis 
Digital Spectral Analysis: Parametric, Non-parametric and Advanced 
Methods 
DESCOMBES Xavier 
Stochastic Geometry for Image Analysis 
FANET Hervé 
Photon-based Medical Imagery 
MOREAU Nicolas 
Tools for Signal Compression 
2010 
NAJMAN Laurent, TALBOT Hugues 
Mathematical Morphology 
2009 
BERTEIN Jean-Claude, CESCHI Roger 
Discrete Stochastic Processes and Optimal Filtering / 2nd edition 
CHANUSSOT Jocelyn et al. 
Multivariate Image Processing 
DHOME Michel 
Visual Perception through Video Imagery 
GOVAERT Gérard 
Data Analysis 
GRANGEAT Pierre 
Tomography 
MOHAMAD-DJAFARI Ali 
Inverse Problems in Vision and 3D Tomography 
SIARRY Patrick 
Optimisation in Signal and Image Processing 

2008 
ABRY Patrice et al.  
Scaling, Fractals and Wavelets 
GARELLO René 
Two-dimensional Signal Analysis 
HLAWATSCH Franz et al. 
Time-Frequency Analysis 
IDIER Jérôme 
Bayesian Approach to Inverse Problems 
MAÎTRE Henri 
Processing of Synthetic Aperture Radar (SAR) Images 
MAÎTRE Henri 
Image Processing 
NAIT-ALI Amine, CAVARO-MENARD Christine 
Compression of Biomedical Images and Signals 
NAJIM Mohamed 
Modeling, Estimation and Optimal Filtration in Signal Processing 
QUINQUIS André 
Digital Signal Processing Using Matlab 
2007 
BERTEIN Jean-Claude, CESCHI Roger 
Discrete Stochastic Processes and Optimal Filtering 
BLOCH Isabelle 
Information Fusion in Signal and Image Processing 
GLAVIEUX Alain 
Channel Coding in Communication Networks 
OPPENHEIM Georges et al. 
Wavelets and their Applications 

2006 
CASTANIÉ Francis 
Spectral Analysis 
NAJIM Mohamed 
Digital Filters Design for Signal and Image Processing 
 

