E l e c t r o n i c
J
o
u
r n a l
o
f
P
r o b a b i l i t y
Electron. J. Probab. 27 (2022), article no. 142, 1–42.
ISSN: 1083-6489 https://doi.org/10.1214/22-EJP867
On mixing of Markov chains: coupling, spectral
independence, and entropy factorization*
Antonio Blanca†
Pietro Caputo‡
Zongchen Chen§
Daniel Parisi¶
Daniel ätefankoviˇc||
Eric Vigoda**
Abstract
For general spin systems, we prove that a contractive coupling for an arbitrary
local Markov chain implies optimal bounds on the mixing time and the modiﬁed log-
Sobolev constant for a large class of Markov chains including the Glauber dynamics,
arbitrary heat-bath block dynamics, and the Swendsen-Wang dynamics. This reveals
a novel connection between probabilistic techniques for bounding the convergence
to stationarity and analytic tools for analyzing the decay of relative entropy. As a
corollary of our general results, we obtain O(n log n) mixing time and ⌦(1/n) modiﬁed
log-Sobolev constant of the Glauber dynamics for sampling random q-colorings of an
n-vertex graph with constant maximum degree ∆when q > (11/6 −✏0)∆for some
ﬁxed ✏0 > 0. We also obtain O(log n) mixing time and ⌦(1) modiﬁed log-Sobolev
constant of the Swendsen-Wang dynamics for the ferromagnetic Ising model on an
n-vertex graph of constant maximum degree when the parameters of the system
lie in the tree uniqueness region. At the heart of our results are new techniques
for establishing spectral independence of the spin system and block factorization
of the relative entropy. On one hand we prove that a contractive coupling of any
local Markov chain implies spectral independence of the Gibbs distribution. On the
other hand we show that spectral independence implies factorization of entropy for
arbitrary blocks, establishing optimal bounds on the modiﬁed log-Sobolev constant of
the corresponding block dynamics.
*Supported in part by NSF grants CCF-1850443, CCF-2007287, and CCF-2007022.
†Department of Computer Science and Engineering, Penn State, University Park, PA 16801, USA.
E-mail: ablanca@cse.psu.edu
‡Department of Mathematics, University of Roma Tre, Largo San Murialdo 1, 00146 Roma, Italy.
E-mail: caputo@mat.uniroma3.it
§Department of Mathematics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA.
E-mail: zongchen@mit.edu
¶Department of Mathematics, University of Roma Tre, Largo San Murialdo 1, 00146 Roma, Italy.
E-mail: daniel.parisi@uniroma3.it
||Department of Computer Science, University of Rochester, Rochester, NY 14627, USA.
E-mail: stefanko@cs.rochester.edu
**Department of Computer Science, University of California, Santa Barbara, CA 93106, USA.
E-mail: vigoda@ucsb.edu

Coupling, spectral independence, and entropy factorization
Keywords: MCMC; mixing time; spectral independence; Swendsen-Wang; log-Sobolev.
MSC2020 subject classiﬁcations: 60J10; 82B20; 68Q87.
Submitted to EJP on December 20, 2021, ﬁnal version accepted on October 17, 2022.
1
Introduction
Spectral independence is a powerful new approach for proving fast convergence of
Markov chain Monte Carlo (MCMC) algorithms. The technique was introduced by Anari,
Liu, and Oveis Gharan [ALO20] to establish rapid mixing of the Glauber dynamics by
utilizing high-dimensional expanders. For a spin system deﬁned on a graph G = (V, E),
the Glauber dynamics is the simple single-site update Markov chain which updates the
spin at a randomly chosen vertex in each step. The mixing time is the number of steps to
reach close to the stationary distribution.
Our paper addresses two broad questions. First, what are the implications of spectral
independence? In particular, does it imply fast convergence for other Markov chains
beyond the simple Glauber dynamics? We prove that it does: we show that spectral
independence implies optimal mixing time bounds and modiﬁed log-Sobolev constants
for a broad class of chains, including all possible heat-bath block dynamics and the
Swendsen-Wang dynamics. Our proof utilizes recent work on entropy factorization [CP21,
BCP+21].
Our second question is when does spectral independence hold, and how does it relate
to traditional proof approaches, such as coupling techniques? Here again we prove a
general result, showing that a contractive coupling for any local Markov chain implies
spectral independence. This immediately yields stronger than state of the art mixing
time bounds for a variety of chains. In addition, it provides an intriguing conceptual
connection between the coupling method and modiﬁed log-Sobolev inequalities as we
describe below.
There are two broad approaches for establishing fast convergence of MCMC algo-
rithms: probabilistic or analytic techniques. Probabilistic techniques primarily utilize
the coupling method; a popular example is the path coupling method which has be-
come a fundamental tool in theoretical computer science [BD97]. In contrast, analytic
techniques establish decay to equilibrium by means of functional inequalities such as
Poincaré or log-Sobolev inequalities, which correspond to decay of variance and relative
entropy respectively. In particular, the so-called modiﬁed log-Sobolev inequality is often
a powerful analytic tool in establishing tight bounds on the mixing time, while the weaker
Poincaré inequality provides control on the spectral gap; see, e.g., [DS96, Mar99, BT06].
These two approaches—probabilistic or analytic—appeared disparate. While coupling
techniques have been used to prove Poincaré inequalities, there are no clear relations
between the probabilistic approach and log-Sobolev inequalities. Here we establish
a strong connection by proving that coupling inequalities in the form of bounds on
the Ollivier-Ricci curvature of the Markov chain imply entropy decay, and hence the
associated modiﬁed log-Sobolev inequality holds; see Section 2 for deﬁnitions. In the
context of spin systems on bounded-degree graphs, this conﬁrms a remarkable (and more
general) conjecture of Peres and Tetali (see Conjecture 3.1 in [ELL17] and Remark 1.14).
We refer to [CDPP09, EHMT17, Con22, Sal21] for further relevant works on the relations
between curvature and entropy in Markov chains.
Our technical contributions apply in the general setting of q-state spin systems. This
is a convenient setting to capture a wide family of distributions deﬁned on graphs,
including the equilibrium distribution of undirected graphical models. We now introduce
some relevant notation and refer to Section 2 for a formal deﬁnition of general spin
systems. Let G = (V, E) be an n-vertex graph, and let ∆denote the maximum degree of
EJP 27 (2022), paper 142.
Page 2/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
G. For integer q ≥2 the state space of the model is the set ⌦= {σ 2 [q]V : µ(σ) > 0} of
assignments with positive weight in the Gibbs distribution µ.
Canonical examples of a spin system include the Ising model (with q = 2 spin values)
and the Potts model (with q ≥3); in these models, for an inverse temperature parameter
β, a conﬁguration σ 2 ⌦has probability µ(σ) / exp(βM(σ)) where M(σ) is the number
of edges of G which are monochromatic in σ. The Ising/Potts model is ferromagnetic
when β > 0 and antiferromagnetic when β < 0. The hard-core model is another spin
system deﬁned on the set of independent sets of G weighted by a parameter λ > 0; each
independent set σ has probability proportional to λ|σ| in the Gibbs distribution. The
q-colorings model, where the Gibbs distribution is uniform over the collection of proper
vertex q-colorings of G, is also a classical spin system.
The Glauber dynamics is the simplest MCMC approach for sampling from the Gibbs
distribution µ. The transitions of the Markov chain (Xt) update the spin at a randomly
chosen vertex in each step. From Xt 2 ⌦, we choose a random vertex x, set Xt+1(y) =
Xt(y) for all y 6= x, and the spin Xt+1(x) is chosen from the marginal distribution at x
conditional on the current spins on N(x), the neighborhood of x. The mixing time is the
number of steps, from the worst initial state, to get close to the stationary distribution;
see Section 2.
Our results apply more broadly to the general class of heat-bath block dynamics.
Let B = {B1, . . . , B`} be any collection of sets (or blocks) such that V = [iBi and let
↵= (↵B)B2B be a probability distribution on B. A step of the heat-bath block dynamics
operates by choosing a block B 2 B with probability ↵B and updating the conﬁguration
in B with a sample from the Gibbs distribution conditional on the conﬁguration on
V \ B. Note that the Glauber dynamics corresponds to setting the blocks to individual
vertices with uniform weights, and for a bipartite graph, the even-odd chain (also known
as the alternating scan dynamics) corresponds to uniform weighting for two blocks
corresponding to the two parts. By extending the weight to ↵B = 0 if B /2 B we think
of ↵as a distribution over all subsets of V and speak of the ↵-weighted heat-bath block
dynamics.
Given ↵, deﬁne the minimum “coverage probability” of a vertex by
δ = δ(↵) = min
x2V
X
B:B3x
↵B.
(1.1)
We say that the block dynamics have optimal mixing when there exists a constant C such
that for all weights ↵the mixing time of the ↵-weighted heat-bath block dynamics is at
most Cδ(↵)−1 log n. Similarly, we say that the block dynamics have optimal entropy decay
if the modiﬁed log-Sobolev constant of the ↵-weighted heat-bath block dynamics is at
least δ(↵)/C. Note that the constant C may depend on the parameters deﬁning the spin
system and on the maximum degree ∆, but it does not depend on n and it is independent
of the choice of weights ↵. In this generality, these bounds are optimal up to the value of
the constant C. Indeed, for the Glauber dynamics we have δ(↵) = 1/n and the mixing
time matches the ⌦(n log n) lower bound established by Hayes and Sinclair [HS07] for
bounded-degree graphs. Moreover, by restricting to test functions of a single spin it is
not hard to check that the spectral gap of the ↵-weighted block dynamics is always at
most δ(↵), and therefore the lower bound δ(↵)/C on the modiﬁed log-Sobolev constant
of the block dynamics is optimal up to the multiplicative constant 1/C; see e.g. [BT06]
for standard relations between spectral gap and modiﬁed log-Sobolev constant.
1.1
Applications
We begin with a few examples of applications of our results. We then delve into our
general technical contributions in subsequent subsections. We note that all of these
EJP 27 (2022), paper 142.
Page 3/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
applications follow immediately from previous coupling proofs together with our new
technical contributions.
For q-colorings of graphs with maximum degree ∆, Jerrum [Jer95] proved that the
Glauber dynamics has O(n log n) mixing time when q > 2∆. Jerrum’s result was improved
to q > 11
6 ∆in [Vig00] and further improved to q > ( 11
6 −✏0)∆for some small ✏0 ⇡10−5 > 0
by Chen et al. [CDM+19] by analyzing a Markov chain referred to as the ﬂip dynamics;
this implied O(n2) mixing time of the Glauber dynamics. We obtain O(n log n) mixing
time of the Glauber dynamics, which is asymptotically optimal [HS07], and also obtain
optimal bounds on the log-Sobolev and modiﬁed log-Sobolev constants.
Theorem 1.1. For q-colorings on an n-vertex graph of maximum degree ∆, when q >
( 11
6 −✏0)∆, where ✏0 ⇡10−5 > 0 is a ﬁxed constant, the Glauber dynamics has mixing time
O(n log n) and log-Sobolev and modiﬁed log-Sobolev constants ⌦(1/n). More generally,
under these assumptions all block dynamics have optimal mixing and optimal entropy
decay.
For the ferromagnetic Ising model, Mossel and Sly [MS13] established optimal mixing
time bounds of O(n log n) for the Glauber dynamics on any graph of maximum degree
∆in the tree uniqueness region; that is, for all β < βc(∆), where βc(∆) := ln(
∆
∆−2)
is the threshold of the uniqueness/non-uniqueness phase transition on the ∆-regular
tree. Our general results allow us to extend this to arbitrary heat-bath block dynamics
and to the Swendsen-Wang dynamics [SW87]. The latter is a particularly interesting
Markov chain which utilizes the random-cluster representation of the ferromagnetic
Potts model to perform global updates in a single step. This non-local nature makes tight
analysis of the Swendsen-Wang dynamics challenging. In [BCV20], it was shown that the
mixing time of Swendsen-Wang dynamics on any graph of maximum degree ∆in the tree
uniqueness region is O(n). Our general results imply a bound of O(log n) on the mixing
time of the Swendsen-Wang dynamics and a bound of ⌦(1) on the corresponding modiﬁed
log-Sobolev constant in the same tree uniqueness region. As shown in [BCP+21] for the
special case of the d-dimensional integer lattice Zd, these estimates are optimal up to a
multiplicative constant. Our results also yield new optimal bounds on the log-Sobolev
and modiﬁed log-Sobolev constants for the Glauber dynamics in the same setting.
We also obtain improved results for the ferromagnetic Potts model. Unlike the Ising
model, for the ferromagnetic Potts model known rapid mixing results for the Glauber
dynamics do not reach the tree uniqueness threshold. The best known results [Hay06,
Ull14, BGP16] imply that the Glauber dynamics mixes in O(n log n) steps when β < β0
where β0 = max
" 2
∆, 1
∆ln( q−1
∆)
 
. In addition, [BGP16] showed poly(n) mixing of the
Glauber dynamics for β < β1 where β1 = (1−o(1)) ln q
∆−1, the o(1) term tends to 0 as q ! 1;
see Remark 4.16 for more details. These results yield polynomial mixing time bounds for
the Swendsen-Wang dynamics in the corresponding regimes of β. Note the critical point
for the uniqueness threshold on the tree was established by Häggström [Häg96] and it
behaves as βu =
ln q
∆−1 + O(1); see [BGP16]. In both regimes, we prove optimal bounds
for the mixing time and (modiﬁed) log-Sobolev constant of the Glauber dynamics and
also for the Swendsen-Wang dynamics.
Theorem 1.2. For the ferromagnetic Ising model with β < βc(∆) on any n-vertex graph
of maximum degree ∆≥3, all heat-bath block dynamics have optimal mixing and optimal
entropy decay, and the Swendsen-Wang dynamics has optimal mixing time O(log n) and
optimal modiﬁed log-Sobolev constant ⌦(1). For the ferromagnetic Potts model the same
results hold when β < max{β0, β1}.
EJP 27 (2022), paper 142.
Page 4/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
1.2
Spectral independence deﬁnitions
A central concept in our work is spectral independence, which was introduced by
Anari, Liu and Oveis Gharan [ALO20] to establish polynomial mixing time bounds for
the Glauber dynamics. To formally deﬁne spectral independence it will be important to
consider the effect of pinnings which can informally be viewed as boundary conditions.
For U ⇢V , let ⌦U = {⌧2 [q]U : 9σ 2 ⌦, σU = ⌧} denote the set of assignments to U
with valid extensions on the remaining vertices. In particular, ⌦x denotes the set of
all valid spin assignments for the vertex x under µ. A pinning is a ﬁxed assignment
⌧on some U ⇢V where ⌧2 ⌦U. We write µ⌧for the Gibbs measure µ(· | σU = ⌧)
obtained by conditioning on the given ⌧. In the presence of a pinning ⌧on U ⇢V , the
deﬁnition of the Glauber dynamics remains the same with the assignment ⌧on U ﬁxed
(see Remark 4.2 for a deﬁnition). Let T = [U⇢V ⌦U denote the collection of all pinnings,
and X = {(x, a) : x 2 V, a 2 ⌦x} for the set of all feasible vertex-spin pairs.
The spectral independence approach considers the following matrices which capture
the pairwise inﬂuence of vertices. For a pair of vertices x, y and a pair of spins a, a0, it is
the inﬂuence of the spin a at x on the marginal probability of a0 at y.
Deﬁnition 1.3 (ALO inﬂuence matrix). The ALO inﬂuence matrix J 2 RX⇥X is deﬁned
by J(x, a; x, a0) = 0 and
J(x, a; y, a0) = µ(σy = a0 | σx = a) −µ(σy = a0)
for x 6= y.
Moreover, for a pinning ⌧2 T , J⌧denotes the inﬂuence matrix with respect to the
conditional measure µ⌧.
Note that [ALO20] deﬁned the inﬂuence matrix only for q = 2 in a slightly different
form and the deﬁnition was later generalized to all q ≥2 by two independent works
[CGvV21, FGYZ21] in different ways. In this paper we use the deﬁnition from [CGvV21]
which is more suitable for our applications in Section 4 for establishing spectral indepen-
dence, but we could also work with the deﬁnition from [FGYZ21] with some additional
effort. Since J is self-adjoint the eigenvalues of J are real. Let λ1(J) ≥0 denote its
largest eigenvalue (the eigenvalue zero always exists since all row sums of J vanish).
Deﬁnition 1.4 (Spectral independence). We say that a spin system is ⌘-spectrally inde-
pendent if for all pinnings ⌧2 T we have λ1(J⌧) ⌘.
There is one additional property of the Gibbs distribution that will be relevant to us;
namely, that the marginal probability for any vertex is lower bounded by a constant b.
This property is typically trivial to satisfy for some constant b = b(∆) > 0. We write ⌦⌧
x
for the set of spin values that are allowed at x in the presence of the pinning ⌧.
Deﬁnition 1.5 (Marginal boundedness). We say that the spin system is b-marginally
bounded if for all pinnings ⌧, all x 2 V , all a 2 ⌦⌧
x we have µ⌧(σx = a) ≥b.
1.3
Consequences of spectral independence
The spectral independence approach has been quite powerful as it led to rapid
mixing results for the hard-core model in the tree uniqueness region [ALO20], for any
2-spin antiferromagnetic spin system in the tree uniqueness region [CLV20], and for
colorings [CGvV21, FGYZ21] it matched the best known parameter bounds using other
algorithmic approaches.
Moreover, recent work of Chen et al. [CLV21] shows that
spectral independence implies optimal mixing of the Glauber dynamics in all of these
cases as stated in the following theorem.
Theorem 1.6 ([CLV21]). For an arbitrary spin system on a graph of maximum degree ∆,
if the system is ⌘-spectrally independent and b-marginally bounded, then there exists
a constant C = C(b, ⌘, ∆) > 0 such that the mixing time of the Glauber dynamics for
EJP 27 (2022), paper 142.
Page 5/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
the spin system is at most Cn log n where n is the number of vertices, and the modiﬁed
log-Sobolev constant of the Glauber dynamics is at least 1/(Cn). Moreover, the constant
C satisﬁes C =
$ ∆
b
%O(1+ ⌘
b ).
We note that we obtain an incremental improvement in the mixing time bound in this
theorem, improving the exponent in the constant C from O(1 + ⌘/b2) (see Theorem 1.9
in [CLV21]) to O(1 + ⌘/b).
The key step in the proof of Theorem 1.6 is the implication
Spectral Independence
=)
Approximate Tensorization of Entropy.
(1.2)
Approximate tensorization of entropy for a Gibbs distribution µ, as deﬁned in [CMT15],
says that there exists a constant C ≥1, such that for any function f : ⌦! R+,
Ent(f) C
X
x2V
µ[Entx(f)].
(1.3)
Here µ[f] = P
σ2⌦µ(σ)f(σ) and Ent(f) = µ[f log(f/µ[f])] denote the mean and entropy
of f with respect to the measure µ, respectively, whereas µ[Entx f] = µ[f log(f/µx[f])] is
the expected value according to µ of the conditional entropy ⌧7! Ent(f|⌧) for ⌧a spin
conﬁguration on V \ {x}; see Section 2 for the detailed deﬁnitions. The inequality (1.3)
generalizes the well known tensorization property of product measures: if µ is product,
then (1.3) holds with C = 1. In general, approximate tensorization is easily seen to imply
the desired bounds on the modiﬁed log-Sobolev constant and the mixing time of the
Glauber dynamics; see e.g. [CMT15]. In the setting of spin systems on the lattice Zd,
approximate tensorization estimates are known to hold under the so-called strong spatial
mixing condition; this follows from the logarithmic Sobolev inequalities established in
[SZ92, MO94, Ces01, DPPP02].
We provide an alternative proof of some of the key steps for the implication (1.2).
The analogous result in [CLV21] is proved in the framework of simplicial complexes and
generalizes the result of [CGM21] for homogeneous strongly log-concave distributions;
see also [HS19] for related results.
Our proof, which is provided in Section 5, is
completely framed in the setting of spin systems and is devoid of any work on simplicial
complexes. This new approach may be conceptually simpler to some readers, and it
enables us to present a self-contained proof of our main results. As a byproduct we
obtain the aforementioned improvement in the constant C in Theorem 1.6.
One of our main results in this paper is the following substantial extension of (1.2):
Spectral Independence
=)
General Block Factorization of Entropy.
(1.4)
The notion of general block factorization of entropy, recently introduced in [CP21], is a
generalization of the approximate tensorization, and is useful for analyzing more general
classes of Markov chains. Let ↵= (↵B)B⇢V be an arbitrary probability distribution over
subsets of V , and set δ(↵) = minx2V
P
B:B3x ↵B as in (1.1). General block factorization
of entropy holds with constant C if for all weights ↵, for all f : ⌦! R+:
δ(↵) Ent f C
X
B⇢V
↵B µ[EntB f],
(1.5)
where µ[EntB f] = µ[f log(f/µBf)] is the expected value of the conditional entropy
⌧7! Ent(f|⌧) for ⌧a spin conﬁguration on V \B. Approximate entropy tensorization (1.3)
is the special case when ↵B = 1/n for every block of size 1 and ↵B = 0 for larger blocks.
The choice of the constant δ(↵) in this inequality is motivated by the fact that when
µ is a product measure then (1.5) holds with C = 1, in which case it is known as the
EJP 27 (2022), paper 142.
Page 6/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Shearer inequality; see [CMT15]. The block factorization of entropy is a statement
concerning the equilibrium distribution µ which has deep consequences for several
natural sampling algorithms. In particular, it implies optimal mixing and optimal entropy
decay for arbitrary block dynamics (see Lemma 2.8 below) and constitutes a key concept
in the proof of Theorem 1.1 and Theorem 1.2. The precise formulation of (1.4) and its
corollaries is as follows.
Theorem 1.7. For an arbitrary spin system on a graph of maximum degree ∆, if the
system is ⌘-spectrally independent and b-marginally bounded, then general block fac-
torization of entropy (1.5) holds with constant C = C(b, ⌘, ∆). Moreover, all heat-bath
block dynamics have optimal mixing and optimal entropy decay. The constant C satisﬁes
C =
$ 2
b
%O(∆(1+ ⌘
b )).
Recall, for the Glauber dynamics δ(↵) = 1/n, and hence, using the simple facts
recalled in Lemma 2.8 below, one recovers Theorem 1.6 as a special case of the above
result. As another example, for a bipartite graph, Theorem 1.7 implies O(log n) mixing
time of the even-odd dynamics where at each step, according to a coin ﬂip, all the odd
sites or all the even sites are updated at once.
When ↵is the uniform distribution over all subsets of a given size `, we refer to (1.5)
as the `-uniform block factorization of entropy or `-UBF for short.
In [CLV21], an
important step in the proof of Theorem 1.6 is establishing `-UBF with ` ⇠✓n for some
✓2 (0, 1). To prove Theorem 1.7 for arbitrary blocks we establish that `-UBF implies
general block factorization of entropy; see Theorem 1.16 for a detailed statement and
Figure 1 for a high-level overview.
Recent work of Blanca et al. [BCP+21] utilizes block factorization of entropy into the
even and odd sublattices of Zd to obtain tight mixing time bounds for the Swendsen-
Wang dynamics on boxes of Zd in the high-temperature region. Following the approach
presented in [BCP+21] and using our general result in Theorem 1.7, here we prove
optimal mixing time of the Swendsen-Wang dynamics when spectral independence holds
on arbitrary bounded-degree graphs. This can be formalized in the following statement,
which is a key ingredient in the proof of Theorem 1.2.
Theorem 1.8. For the ferromagnetic Ising and Potts models on a graph of maximum
degree ∆, if the system is ⌘-spectrally independent and b-marginally bounded, then
there exists a constant C = C(b, ⌘, ∆) such that the mixing time of the Swendsen-Wang
dynamics is at most C log n and the modiﬁed log-Sobolev constant is at least C−1. The
constant C satisﬁes C =
$ 2
b
%O(∆(1+ ⌘
b )).
1.4
Establishing spectral independence
The above results show the power of spectral independence as it implies optimal
mixing time bounds for a wide variety of Markov chains. We next address when spectral
independence holds and how it relates to classical conditions that imply fast mixing. The
next series of results prove in a general context that when there exists a contractive
coupling then spectral independence holds.
Let d denote an arbitrary metric on ⌦. A simple example is the Hamming metric, which
for conﬁgurations σ, ⌧2 ⌦is deﬁned to be dH (σ, ⌧) = |{x 2 V : σx 6= ⌧x}|. There are two
types of more general metrics that we will consider: those within a constant factor of
the Hamming metric and vertex-weighted Hamming metric for arbitrary weights. For
γ ≥1, a metric d on ⌦is said to be γ-equivalent to the Hamming metric (or γ-equivalent
for simplicity) if for all σ, ⌧2 ⌦,
1
γ dH (σ, ⌧) d(σ, ⌧) γdH (σ, ⌧) ;
EJP 27 (2022), paper 142.
Page 7/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
that is, a γ-equivalent metric is an arbitrary metric where every distance is within a
factor γ of the Hamming distance. In contrast, we can generalize the Hamming distance
by considering arbitrary weights for the vertices. Let w : V ! R+ be an arbitrary
positive weight function. The w-weighted Hamming metric between two conﬁgurations
σ, ⌧2 ⌦is deﬁned to be
dw(σ, ⌧) =
X
x2V
w(x)1{σx 6= ⌧x}.
In particular, if wx = 1 for all x then dw is just the usual Hamming metric. Note there are
no constraints on the weights except that they are positive; in particular, the weights
can be a function of n.
We will often consider a class P = {P ⌧: ⌧2 T } of Markov chains associated with µ,
where each P ⌧is a Markov chain with stationary distribution µ⌧and ⌧2 T is a pinning;
for example, P can be the family of Glauber dynamics for all µ⌧’s. In coupling proofs,
the goal is to design a coupling so that for an arbitrary pair of states the chains contract
with respect to some distance metric after the coupled transition. Roughly speaking, for
2 (0, 1), we say that µ is -contractive with respect to (w.r.t.) a collection P of Markov
chains and a metric d if one step of every chain P ⌧contracts the distance by a factor 
in expectation. This is formalized in the following deﬁnition.
Deﬁnition 1.9 (-Contraction). Let P denote a collection of Markov chains associated
with µ and let d be a metric on ⌦. For 2 (0, 1) we say that µ is -contractive w.r.t. P
and d if for all ⌧2 T , all X0, Y0 2 ⌦⌧, there exists a coupling (X0, Y0) ! (X1, Y1) for P ⌧
such that:
E[d(X1, Y1)|X0, Y0] d(X0, Y0).
The following result shows that spectral independence holds if the Glauber dynamics
has a contractive coupling.
Theorem 1.10.
(1) If µ is -contractive w.r.t. the Glauber dynamics and an arbitrary
w-weighted Hamming metric, then µ is spectrally independent with constant ⌘=
2
(1−)n. In particular, if 1 −✏
n, then ⌘2
✏.
(2) If the metric in (1) is not a weighted Hamming metric but instead an arbitrary
γ-equivalent metric, then ⌘=
2γ2
(1−)n. In particular, if 1 −✏
n, then ⌘2γ2
✏.
Note that a -contractive coupling for the Hamming distance immediately implies
O(n log n) mixing time of the Glauber dynamics (see, e.g., [BD97, LP17]). But the above
theorem offers two additional features. First, it allows arbitrary weights w and the re-
sulting bound on the mixing time does not depend on the ratio of maxx w(x)/ minx w(x),
whereas a coupling argument, such as the one utilized in path coupling [BD97], yields a
mixing time bound which depends on this ratio. Second, as discussed in the previous
theorems, spectral independence (together with the easily satisﬁed marginal bounded-
ness) implies optimal bounds on the mixing time and entropy decay rate for arbitrary
heat-bath block dynamics.
We can extend Theorem 1.10 by replacing the Glauber dynamics with arbitrary
Markov chains. In particular, we consider a general class of Markov chains which we
call the select-update dynamics. In each step, the select-update dynamics picks a block
B 2 B randomly (with a distribution that may depend on the current conﬁguration), and
updates all vertices in B using the current conﬁguration (and the pinning if there is
one). Note that no assumptions are made on how to pick or update the blocks; the only
requirement is that the dynamics converges to the correct stationary distribution. If
the chain selects a block B from a ﬁxed distribution over B and updates B using the
conditional marginal distribution on B (under the pinning if applicable), then this is
the standard heat-bath block dynamics that we introduced earlier; hence, the select-
update dynamics is much more general than the weighted heat-bath block dynamics.
EJP 27 (2022), paper 142.
Page 8/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Another example of the select-update dynamics is the ﬂip dynamics for sampling random
colorings of a graph; see Section 4.3.1.
We deﬁne M = maxB2B |B| to be the maximum block size and D to be the maximum
probability of a vertex being selected in any step of the chain; see (4.12) for the precise
deﬁnition of D.
Theorem 1.11. If µ is -contractive w.r.t. arbitrary select-update dynamics and an
arbitrary γ-equivalent metric, then µ is spectrally independent with constant ⌘= 2γ2DM
1−
.
Theorem 1.11 generalizes Theorem 1.10(2) since M = 1 and D = 1/n for the Glauber
dynamics. If we further assume that the select-update dynamics updates each connected
component of a block independently, then the maximum block size M can be replaced
by the maximum component size of a block; see Remark 4.10. See also Theorem 4.8
for a stronger statement involving arbitrary Markov chains, where DM is replaced by
the maximum expected distance of two chains when pinning a single vertex. This more
general statement potentially applies to chains with unbounded block sizes, including
the Swendsen-Wang dynamics.
It is worth remarking that, as a corollary of Theorem 1.11 we obtain that a coupling
argument for the select-update dynamics where the maximum block size is constant (and
D/(1 −) = O(1)) implies O(n log n) mixing time of the Glauber dynamics, together with
the optimal mixing and optimal entropy decay for arbitrary heat-bath block dynamics.
Moreover, as a corollary of Theorem 1.10 we obtain that the Dobrushin uniqueness
condition implies spectral independence.
The Dobrushin uniqueness condition is a
classical condition in statistical physics which considers the following dependency
matrix.
Deﬁnition 1.12 (Dobrushin uniqueness condition). The Dobrushin dependency (or inﬂu-
ence) matrix R 2 RV ⇥V is deﬁned by R(x, x) = 0 and
R(x, y) = max {dTV (µy(· | σ), µy(· | ⌧)) : (σ, ⌧) 2 Sx,y}
for x 6= y
where Sx,y is the set of all pairs of conﬁgurations on V \ {y} that can differ only at x.
The Dobrushin uniqueness condition holds if the maximum column sum of R is at most
1 −✏for some ✏> 0.
The Dobrushin dependency matrix for the entry R(x, y) considers the worst case pair
of conﬁgurations on the entire neighborhood of y which differ at x. If x is not a neighbor
of y then R(x, y) = 0. Hence, the Dobrushin uniqueness condition states that for all y,
P
x2N(y) R(x, y) < 1. In contrast, the ALO inﬂuence matrix considers the inﬂuence of
a disagreement at x on a vertex y (which is not necessarily a neighbor) and no other
vertices are ﬁxed, although one needs to consider all pinnings to establish spectral
independence, so the notions are incomparable at ﬁrst glance.
Using Theorem 1.10 we prove that the Dobrushin uniqueness condition implies spec-
tral independence. Moreover, our result holds under generalizations of the Dobrushin
uniqueness condition. Hayes [Hay06] generalized it to the following spectral condition:
if kRk2 1 −✏for some ✏> 0, then the mixing time of the Glauber dynamics is O(n log n).
This was further generalized by Dyer et al. [DGJ09] to arbitrary matrix norms. We
prove spectral independence when the spectral radius %(R) < 1, which is the strongest
statement of this type as the spectral radius is no larger than any matrix norm; see
Remark 4.4 for a more detailed discussion.
Theorem 1.13. If the Dobrushin dependency matrix R satisﬁes %(R) 1 −✏for some
✏> 0, then µ is spectrally independent with constant ⌘= 2/✏.
Previously, Marton [Mar19] (see also [GSS19, SS20]) showed that the spectral con-
dition in Theorem 1.13 implies approximate tensorization of entropy and thus optimal
EJP 27 (2022), paper 142.
Page 9/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
bounds on the modiﬁed log-Sobolev constant for the Glauber dynamics. However, the
approach in these works does not imply block factorization of entropy as in our case.
Remark 1.14. Our deﬁnition of -contraction is equivalent to the statement that the
Markov chain has coarse Ollivier-Ricci curvature at least 1 −> 0 with respect to
the metric d [Oll09]. Combining Theorem 1.10 with Theorem 1.7 we obtain a proof
of the following version of the Peres-Tetali conjecture: if the Glauber dynamics has
Ollivier-Ricci curvature at least ✏/n > 0 then the Glauber dynamics has a modiﬁed
log-Sobolev constant at least c/n and any ↵-weighted heat-bath block dynamics has a
modiﬁed log-Sobolev constant at least c δ(↵), for some constant c = c(✏, b, ∆) > 0, where
δ(↵) is deﬁned in (1.1). Replacing Theorem 1.10 with its generalization Theorem 1.11
we obtain the same conclusion under the much milder assumption that there exists
some -contractive select-update dynamics satisfying DM/(1 −) = O(1). The original
Peres-Tetali conjecture in the setting of random walks on graphs is that if there exists a
graph metric d such that the random walk has Ollivier-Ricci curvature at least λ > 0 with
respect to d then the random walk has modiﬁed log-Sobolev constant at least cλ > 0, for
some universal constant c > 0; see Conjecture 3.1 in Eldan et al. [ELL17].
1.5
Proof overviews for main results
In this section, we sketch the proof of our key technical results. We begin in Sec-
tion 1.5.1 with an overview of our proof that spectral independence implies optimal
bounds for arbitrary block dynamics and the Swendsen-Wang dynamics (namely, Theo-
rems 1.7 and 1.8). In Section 1.5.2 we highlight the proofs of Theorems 1.10, and 1.11
that a contractive coupling for an arbitrary local dynamics implies spectral independence.
1.5.1
Optimal mixing under spectral independence: Theorems 1.7 and 1.8
We begin with the high-level idea for the proof that spectral independence implies
optimal mixing for arbitrary heat-bath block dynamics, and then we describe the key
ideas to obtain optimal mixing for the Swendsen-Wang dynamics.
Recall from Section 1.3 that to establish optimal mixing for an arbitrary choice of
block dynamics it sufﬁces to prove general block factorization (GBF); see Lemma 2.8 for
more details. Previous results show that spectral independence implies `-uniform block
factorization (`-UBF) with ` = d✓ne for any ﬁxed ✓2 (0, 1); see [CLV21] and Theorem 5.1.
Note, `-UBF refers to the block factorization where the weights ↵are uniform over all
subsets of size `; see Deﬁnition 1.15 below. The key step in the proof of Theorem 1.7
is to show that `-UBF, with ` = d✓ne and ✓sufﬁciently small, implies general block
factorization (GBF).
We begin with the formal deﬁnition of `-UBF. For a positive integer ` n, let
$V
`
%
denote the collection of all subsets of V of size `.
Deﬁnition 1.15 (Uniform Block Factorization). We say that the spin system µ satisﬁes the
`-uniform block factorization (`-UBF) of entropy with constant Cubf if for all f : ⌦! R+
`
n Ent(f) Cubf · 1
$n
`
%
X
S2(
V
`)
µ[EntS(f)].
(1.6)
We prove the following theorem that `-UBF (for sufﬁciently small choice of `) implies
general block factorization (GBF).
Theorem 1.16. For an arbitrary b-marginally bounded spin system on a graph of maxi-
mum degree ∆, if d✓ne-UBF holds with constant Cubf and 0 < ✓b2(∆+1)
4e∆2 , then GBF holds
with constant Cgbf = Cubf ⇥O
$
(✓b2)−1 log(1/b)∆3%
.
EJP 27 (2022), paper 142.
Page 10/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
This and the already known d✓ne-UBF implies Theorem 1.7 from the introduction:
Proof of Theorem 1.7. For a spin system which is ⌘-spectrally independent and also
b-marginally bounded, d✓ne-UBF holds with constant Cubf = ( 1
✓)O( ⌘
b ) (see Theorem 5.1).
Then, taking ✓= b2(∆+1)
4e∆2 , Theorem 1.16 implies that GBF holds with constant Cgbf =
O
⇣
4e∆5
b2(∆+2) log(1/b)
⌘
⇥
⇣
4e∆2
b2(∆+1)
⌘O( ⌘
b )
, and it thus follows that Cgbf =
$ 2
b
%O(∆(1+ ⌘
b )).
Hence, the key novelty in the proof of Theorem 1.7 is Theorem 1.16. To establish
Theorem 1.16 we consider a special case of GBF, which we call k-partite factorization
of entropy. Recall that a graph G of maximum degree ∆is k-partite, with k ∆+ 1.
Let {V1, ..., Vk} denote the independent sets Vi ⇢V corresponding to a k-partition of G.
Theorem 1.16 follows immediately from the following factorization statements.
Lemma 1.17. Suppose that for an arbitrary b-marginally bounded spin system on a
graph of maximum degree ∆, d✓ne-UBF holds with constant Cubf and ✓b2(∆+1)
4e∆2 . Then,
Ent(f) KCubf
k
X
i=1
µ[EntVi(f)],
(1.7)
where the constant K satisﬁes K = O(∆2(✓b2)−1 log(1/b)). We refer to inequality (1.7)
as a k-partite factorization of entropy with constant KCubf.
Lemma 1.18. Suppose that for an arbitrary spin system on a graph of maximum degree
∆, k-partite factorization of entropy holds with constant C.
Then, GBF holds with
constant Ck.
We comment brieﬂy on how we prove these two lemmas; their actual proofs are
provided in Section 3. The main idea behind the proof of Lemma 1.17 can be roughly
explained as follows. If the sets S in (1.6) were all independent sets, then a suitable
decomposition of the entropy functional would imply the desired conclusion. Using a
tensorization argument from [CP21], the same conclusion would continue to hold if S
only contained connected components of bounded size. However, even if ✓is small, a
uniformly random set S with |S| = d✓ne is likely to have components of size ⇥(log n).
On the other hand, locally the expected component size is bounded if ✓is sufﬁciently
small. The challenge in obtaining optimal bounds is thus to use the expected local
component size instead of the maximum component size. To achieve this we combine
ideas from [CP21] and [CLV21] together with a new conditioning argument. The proof of
Lemma 1.18 is simpler, and relies on the fact that GBF holds on each of the independent
sets Vi; this is a consequence of the weighted Shearer inequality for the Shannon entropy
(see, e.g., Lemma 4.2 in [CP21]).
Finally, to prove Theorem 1.8, that is the optimal mixing results for the SW dynamics,
our strategy is based on establishing the spin/edge factorization of entropy, a notion
introduced in [BCP+21], for the “joint” Edwards-Sokal coupling; see [ES88, Gri06] and
Section 6 below. The spin/edge factorization of entropy was shown in [BCP+21, Lemma
1.8] to imply O(log n) mixing of the SW dynamics on any graph. To prove Theorem 1.8,
we show that k-partite factorization of entropy for µ implies spin/edge factorization of
entropy. This requires a nontrivial adaptation of the corresponding result established
in [BCP+21] in the special case of bipartite graphs. The proof of Theorem 1.8 is provided
in Section 6.
1.5.2
Spectral independence via contractivity: Theorems 1.10 and 1.11
Here we outline our proofs of Theorems 1.10 and 1.11. We establish spectral indepen-
dence by showing that the maximum absolute row sum of the ALO inﬂuence matrix is
EJP 27 (2022), paper 142.
Page 11/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
bounded. Consider the case without pinnings for simplicity. We would like to upper
bound, for each (x, a) 2 X , the quantity
S(x, a) =
X
(y,a0)2X
|J(x, a; y, a0)| =
X
(y,a0)2X: y6=x
|⌫(σy = a0) −µ(σy = a0)|
where ⌫= µ(· | σx = a) is the conditional distribution under the pinning σx = a. Upper
bounds on S(x, a) (and analogous results with pinnings) would then imply spectral
independence. The ﬁrst step is to deﬁne a 2-Lipschitz function f : ⌦! R, w.r.t. the
Hamming metric dH, such that S(x, a) = E⌫f −Eµf.
In particular, it follows that
S(x, a) 2W1,dH(⌫, µ) where W1,dH(⌫, µ) represents the 1-Wasserstein distance; we refer
to Section 4.1 for relevant deﬁnitions. The important intuition here is that it sufﬁces
to upper bound some statistical distance between the two distributions µ and ⌫= µ(· |
σx = a). In other words, to deduce spectral independence one only needs to show that
every pinning σx = a would disturb the distribution µ on a limited scale, in terms of the
Wasserstein distance.
Up to now, we have not yet applied our assumptions on contractivity of the distribu-
tion µ. Our next step is to show W1,dH(⌫, µ) = O(1) for contractive µ. To achieve this, we
generalize a result from previous works [BN19, RR19] to bound the Wasserstein distance
of two distributions; see Lemma 4.3. Roughly speaking, we show that, assuming contrac-
tivity, the stationary distributions of two Markov chains are close to each other if the two
chains are close in one step. Previous results in [BN19, RR19] were specialized for the
binary product space and the Glauber dynamics. Here, we establish our Lemma 4.3 for
any ﬁnite state space and any Markov chain. This result is of independent interest and
may ﬁnd applications in other problems.
We point out that after the ﬁrst version of the present paper appeared, Kuikui
Liu posted the article [Liu21] where he independently established similar results to
Theorem 1.11, which also implied Theorem 1.1 for the Glauber dynamics. In [Liu21]
the author concluded a version of Theorem 1.13 as well, but required the stronger
assumption that the row sum of the Dobrushin dependency matrix is bounded. Using
our Theorem 1.10 we only require a bound on the spectral radius which is the weakest
assumption of this type; see Remark 4.4.
Paper organization
The organization of the paper is demonstrated in Figure 1. After
giving preliminaries in Section 2, in Section 3 we prove that uniform block factorization
implies general block factorization of entropy.
In Section 4, we establish spectral
independence if the distribution admits a contractive Markov chain. In Section 5, we
reformulate the result of [CLV21] showing that spectral independence implies uniform
block factorization; our new proof avoids abstract simplicial complexes and gives a
slightly better constant. We also show in Section 5 that spectral independence implies
approximate subadditivity of entropy, see Theorem 5.1. Finally, in Section 6 we show
optimal mixing and optimal entropy decay of the Swendsen-Wang dynamics if k-partite
factorization holds, which can be in turn deduced from spectral independence.
2
Preliminaries
2.1
Spin systems
We begin with the formal deﬁnition of general q-state spin systems.
Let q ≥2
be an integer and write [q] = {1, . . . , q}. Let G = (V [ @V, E [ @E) be an undirected
graph where @V denotes the boundary set of the induced subgraph G0 = (V, E), and
@E consists of all edges between V and @V . A q-spin system on G with a boundary
condition ⇠2 [q]@V is parameterized by nonnegative symmetric matrices Axy 2 Rq⇥q
+
,
EJP 27 (2022), paper 142.
Page 12/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Spectral
Independence
General Block
Factorization
(Path)
Coupling
Uniform Block
Factorization
k-Partite
Factorization
Approximate
Subadditivity
Contraction for
Glauber Dynamics
(E.g., Dobrushin
Uniqueness Condition)
Contraction for Select-
Update Dynamics (E.g.,
Flip Dynamics for
Colorings
q
∆≥11
6 −✏0)
Any Block Dynamics
(Including Glauber)
(Ising/Potts)
Swendsen-Wang
Optimal Mixing and
Optimal Entropy Decay
Thm. 1.10
Thm. 1.11
Eq. (5.1)
Eq. (5.2)
[CLV21]
Lem. 1.17
Lem. 1.18
[CP21]
Thm. 6.1
[BCP+21]
Figure 1: Organization of the paper
{x, y} 2 E [ @E, representing the nearest neighbor interactions, and vectors Bx 2 Rq
+,
x 2 V , representing the external ﬁelds. A conﬁguration σ 2 [q]V has weight:
w(σ) =
Y
{x,y}2E
Axy(σx, σy)
Y
{x,y}2@E
x2V, y2@V
Axy(σx, ⇠y)
Y
x2V
Bx(σx).
Let ⌦= {σ 2 [q]V : w(σ) > 0} denote the collection of all feasible conﬁgurations and let
ZG = P
σ2⌦w(σ) denote the partition function. We assume that ⌦6= ;; i.e., the boundary
condition ⇠is feasible. Finally, the Gibbs distribution µ is given by, for σ 2 ⌦,
µ(σ) = w(σ)/ZG.
Recall the notion of pinning from Section 1.2 which we brieﬂy repeat here for
convenience. For U ⇢V , we use the notation σU = (σx)x2U and let ⌦U = {⌧2 [q]U :
9σ 2 ⌦, σU = ⌧} be the set of all feasible pinnings on U. Note, for x 2 V , ⌦x is the
set of feasible spin assignments for vertex x. Denote the collection of all pinnings by
T = [U⇢V ⌦U and denote the set of all feasible vertex-spin pairs by X = {(x, a) : x 2
V, a 2 ⌦x}. For ⌧2 ⌦U, let µ⌧denote the conditional Gibbs distribution µ(· | σU = ⌧).
We also write µ⌧
⇤= µ⌧if ⌧2 ⌦V \⇤and use the notation µ⇤: ⌦V \⇤3 ⌧7! µ⌧
⇤for the
associated mapping. Following a standard convention, with slight abuse of notation we
sometimes consider µ⇤as a map on the whole set ⌦V , that is µ⇤: ⌦V 3 σ 7! µσ
⇤in such
a way that µσ
⇤= µσ0
⇤for all σ, σ0 2 ⌦V which coincide on V \ ⇤.
For a pinning ⌧2 ⌦U for U ⇢V , let ⌦⌧= {σ 2 ⌦: σU = ⌧} denote the corresponding
state space; i.e., ⌦⌧is the support of µ⌧. We also deﬁne ⌦⌧
W = {' 2 [q]W : 9σ 2 ⌦⌧, σW =
'} for W ⇢V \ U and X ⌧= {(x, a) : x 2 V \ U, a 2 ⌦⌧
x}. We say ⌦⌧is connected if the
graph on ⌦⌧with edges connecting pairs at Hamming distance 1 is connected. The
distribution µ over ⌦is said to be totally-connected if for every ⌧2 T , the set ⌦⌧is
connected. Throughout this paper, we will assume the distribution µ is totally-connected
as this is necessary for the Glauber dynamics to be ergodic for all conditional measures
µ⌧.
We recall some classical examples of spin system. The Ising/Potts model at inverse
temperature β 2 R corresponds to the interaction Axy(a, a0) = exp (β1(a = a0)) and
Bx(a) = exp (h(a)) where h 2 Rq is a vector of external ﬁelds, with q = 2 for the Ising
EJP 27 (2022), paper 142.
Page 13/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
model and q ≥3 for the Potts model. The hard-core (or independent sets) model with
parameter λ > 0 is obtained with q = 2, Axy(a, a0) = 0 if a = a0 = 1 and Axy(a, a0) = 1
otherwise, and Bx(a) = λ if a = 1 and Bx(a) = 1 if a = 2.
The q-colorings model
corresponds to Axy(a, a0) = 1(a 6= a0) and Bx(a) = 1. Note that the Ising/Potts models
with any β and h, as well as the hard-core model with any λ > 0, and the q-colorings
when q ≥∆+ 2 are totally-connected spin systems.
2.2
Mixing time, entropy, and log-Sobolev inequalities
Let P be the transition matrix of an ergodic Markov chain with ﬁnite state space
⌦and stationary distribution µ. Let P t(X0, ·) denote the distribution of the chain after
t steps starting from the initial state X0 2 ⌦. The mixing time Tmix(P) of the chain is
deﬁned as
Tmix(P) = max
X02⌦min
"
t ≥0 : kP t(X0, ·) −µktv 1/4
 
,
where k · kTV denotes total variation distance.
In this paper, we rely on functional inequalities related to entropy to bound the mixing
time. For a function f : ⌦7! R, let µ[f] = P
σ2⌦µ(σ)f(σ) and Varµ(f) = µ[f 2] −µ[f]2
denote its mean and variance with respect to µ. Likewise, for f : ⌦! R+, the entropy
of f with respect to µ is deﬁned as
Ent(f) = µ

f · log
✓f
µ[f]
◆-
= µ[f · log f] −µ[f] · log µ[f].
(2.1)
When f ≥0 is such that µ[f] = 1, then Ent(f) = H(fµ | µ) equals the relative entropy,
or Kullback-Leibler divergence, of the distribution fµ with respect to µ.
For real functions f, g on ⌦, the Dirichlet form associated to the pair (P, µ) is deﬁned
as
DP (f, g) = hf, (1 −P)giµ,
(2.2)
where hf, giµ = µ[fg] denotes the scalar product in L2(µ). When P is reversible, i.e.,
µ(σ)P(σ, ⌧) = µ(⌧)P(⌧, σ), one has
DP (f, g) = 1
2
X
σ,⌧2⌦
µ(σ)P(σ, ⌧)(f(σ) −f(⌧))(g(σ) −g(⌧)).
(2.3)
Deﬁnition 2.1. The pair (P, µ) satisﬁes the (standard) log-Sobolev inequality (LSI) with
constant s if for all f ≥0:
DP (
p
f,
p
f) ≥s Ent(f).
(2.4)
It satisﬁes the modiﬁed log-Sobolev inequality (MLSI) with constant % if for all f ≥0:
DP (f, log f) ≥% Ent(f).
(2.5)
It satisﬁes the (discrete time) relative entropy decay with rate δ > 0 if for all distributions
⌫:
H(⌫P | µ) (1 −δ)H(⌫| µ).
(2.6)
In this paper we focus on the entropy decay inequality (2.6) which may be seen as the
discrete time analog of the modiﬁed log-Sobolev inequality. We recall some well known
facts about its relation to the other two inequalities and its implications for mixing times.
EJP 27 (2022), paper 142.
Page 14/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Lemma 2.2. If (P, µ) satisﬁes the standard LSI with constant s > 0 then it satisﬁes
the MLSI with constant % = 2s. If it satisﬁes the discrete time relative entropy decay
with rate δ > 0, then it satisﬁes the MLSI with constant % = δ. Finally, if it satisﬁes the
discrete time relative entropy decay with rate δ > 0, then
Tmix(P) 1 + δ−1[log(8) + log log(1/µ⇤)] ,
(2.7)
where µ⇤= minσ2⌦µ(σ).
We refer to e.g. [BCP+21, Section 2] for a proof. If P is positive semi-deﬁnite, and
therefore (P, µ) is reversible, then one can additionally show that the standard LSI with
constant s implies the discrete time relative entropy decay with rate δ = s (see [BCP+21,
Lemma 2.8], whose statement erroneously omitted the requirement that P is positive
semi-deﬁnite).
2.3
Some basic properties of entropy
To compute the relative entropy with respect to a pinned measure µ⌧
⇤it is convenient
to use the notation
Ent⇤(f) = µ⇤[f log (f/µ⇤[f])] ,
(2.8)
with the understanding that if we evaluate the left hand side at a given pinning ⌧on
⇤c = V \ ⇤we then evaluate the expectations in the right hand side with respect to
µ⌧
⇤. To emphasize the dependence on the pinning we sometimes write Ent⌧
⇤(f). The
expectation µ[Ent⇤f] is obtained by averaging with respect to µ over the pinning ⌧on
⇤c, and satisﬁes
µ[Ent⇤(f)] =
X
⌧2⌦⇤c
µ(σ⇤c = ⌧) Ent⌧
⇤(f) = µ [f log (f/µ⇤[f])] .
(2.9)
The following lemma summarizes a standard decomposition of the relative entropy; see
e.g. [CP21, Lemma 3.1] for a proof.
Lemma 2.3. For any ⇤⇢V , for any f : ⌦! R+:
Ent(f) = µ [Ent⇤(f)] + Ent (µ⇤[f]).
(2.10)
More generally, for any ⇤0 ⇢⇤1 ⇢· · · ⇢⇤w ⇢V , for any f : ⌦! R+:
w
X
i=1
µ
⇥
Ent⇤i(µ⇤i−1[f])
⇤
= µ [Ent⇤w(µ⇤0[f])] .
(2.11)
The following monotonicity property of the entropy functional is an immediate conse-
quence of the previous lemma.
Lemma 2.4. For all A ⇢B ⇢V ,
µ[EntA(f)] µ[EntB(f)] .
(2.12)
Next, we recall the deﬁnition of general block factorization of entropy.
Deﬁnition 2.5. The spin system is said to satisfy the general block factorization of
entropy with constant C if for all f ≥0, for all probability distribution ↵over subsets of
V ,
δ(↵) Ent f C
X
B⇢V
↵B µ[EntB f],
(2.13)
where δ(↵) = minx2V
P
B: B3x ↵B.
EJP 27 (2022), paper 142.
Page 15/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
We will often consider independent sets ⇤of V , that is sets of vertices whose induced
subgraph in G has no edge; in those cases, µ⇤is a product measure µ⇤= ⌦x2⇤µx and
the following lemma will be useful.
Lemma 2.6. Fix ⇤⇢V and suppose that µ⇤is a product measure µ⇤= ⌦x2⇤µx. Then,
for any distribution ↵over the subsets of ⇤, and any f : ⌦! R+:
δ(↵) Ent⇤(f) 
X
B⇢⇤
↵B µ⇤[EntB(f)] ,
(2.14)
that is µ⇤satisﬁes the general block factorization of entropy with constant C = 1.
The above statement is a simple consequence of the well known weighted version
of Shearer inequality for the Shannon entropy; see e.g. [CHV20, Theorem 6.2] for a
proof of the latter, and see e.g. [CMT15, Proposition 2.6] to go from Shannon entropy to
relative entropy.
The following properties will also be used.
Lemma 2.7. Let ⇤= A [ B and assume that µ⇤is a product µ⇤= µA ⌦µB. Then, for all
f ≥0:
Ent⇤(µB(f)) = µ⇤[EntA(µB(f))],
(2.15)
and for all U ⇢B,
µ⇤[EntA(µB(f))] µ⇤[EntA(µU(f))].
(2.16)
Proof. From the decomposition in Lemma 2.3 it follows that
Ent⇤(µB(f)) −µ⇤[EntA(µB(f))] = Ent⇤(µAµB(f)) = Ent⇤(µ⇤(f)) = 0.
This proves (2.15). To prove (2.16) notice that by deﬁnition
µ⇤[EntA(µB(f))] = µ⇤

µB(f) log
✓
µB(f)
µAµB(f))
◆-
.
For any U ⇢B, µB(f) = µBµU(f) and the product structure µ⇤= µA ⌦µB implies the
commutation relation µAµBµU = µBµAµU. Therefore,
µ⇤[EntA(µB(f))] = µ⇤

µBµU(f) log
✓µBµU(f)
µBµAµU(f)
◆-
= µ⇤

µU(f) log
✓
µBµU(f)
µBµAµU(f))
◆-
= µ⇤

µA

µU(f) log
✓µBµU(f)
µAµBµU(f)
◆--
.
It remains to observe that
µA

µU(f) log
✓µBµU(f)
µAµBµU(f)
◆-
EntA(µU(f)).
The latter estimate follows from the well known variational principle
EntA(g) = sup {µA(gh) , µA(eh) 1}
(2.17)
valid for any A and any function g ≥0; see, e.g. [Led99, Proposition 2.2].
EJP 27 (2022), paper 142.
Page 16/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
2.4
Implications of block factorization
Fix a probability distribution ↵over subsets of V and observe that the ↵-weighted
heat bath block dynamics deﬁned in the introduction is the Markov chain with transition
matrix P↵on ⌦such that for any real function f
P↵f =
X
B⇢V
↵B µB(f) .
(2.18)
To clarify the above notation, if we evaluate the left hand side at a spin conﬁguration
σ 2 ⌦then each for each B the term µBf in the right hand side is given by µ⌧
Bf where
⌧= σV \B. If ↵B = n−1 1(|B| = 1), then (2.18) is the Glauber dynamics for µ.
The ↵-weighted heat bath block dynamics (2.18) deﬁnes a reversible pair (P↵, µ).
Moreover, its Dirichlet form satisﬁes
D↵(f, g) =
X
B⇢V
↵B µ[f(1 −µB)g] =
X
B⇢V
↵B µ [CovB(f, g)] ,
(2.19)
where CovB(f, g) = µB [(f −µBf)(g −µBg)] denotes the covariance functional.
Lemma 2.8. If the spin system satisﬁes the general block factorization with constant C
then for all ↵the Markov chain (P↵, µ) satisﬁes
1. the modiﬁed log-Sobolev inequality with constant % = δ(↵)
C ;
2. the discrete time relative entropy decay with rate δ = δ(↵)
C ;
3. Tmix(P↵) 1 +
C
δ(↵)[log(8) + log log(1/µ⇤)], where µ⇤= minσ2⌦µ(σ).
Proof. In view of Lemma 2.7 it is sufﬁcient to prove item 2. We note that the relative
entropy decay with rate δ is equivalent to the entropy contraction
Ent(P↵f) (1 −δ) Ent(f),
(2.20)
for all f ≥0. By convexity of x 7! x log x one has
Ent(P↵f) = µ[P↵f log(P↵f)] −µ[f] log µ[f]

X
B
↵B µ[µB(f) log(µB(f))] −µ[f] log µ[f] =
X
B
↵B Ent(µB(f)).
(2.21)
From the decomposition in Lemma 2.3 it follows that
Ent(P↵f) Ent(f) −
X
B
↵Bµ[EntB(f)].
(2.22)
By Deﬁnition 2.5, P
B ↵Bµ[EntB(f)] ≥(δ(↵)/C) Ent(f), and therefore
Ent(P↵f) (1 −δ(↵)/C) Ent(f).
3
Uniform block factorization implies general block factorization
We provide in this section the proofs of Lemmas 1.18 and 1.17. Recall that these are
the key ingredients for proving Theorem 1.16.
Proof of Lemma 1.18. Let ↵= (↵B)B⇢V be a probability distribution over the subsets
of V .
Observe that for all j = 1, ..., k and all ⌧2 ⌦V \Vj, µ⌧
Vj is a product measure
on ⌦⌧
Vj. Therefore, we can apply Lemma 2.6 with ⇤= Vj and ˆ↵= (ˆ↵U)U⇢Vj, where
ˆ↵U = !−1 P
B⇢V ↵B1(Vj \ B = U) and ! = P
B⇢V ↵B1(Vj \ B 6= ;). We get
δ(ˆ↵) Ent⌧
Vj(f) 
X
U⇢Vj
ˆ↵U µ⌧
Vj[EntU(f)] = !−1 X
B⇢V
↵B µ⌧
Vj[EntVj\B(f)].
(3.1)
EJP 27 (2022), paper 142.
Page 17/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Observe that
!δ(ˆ↵) = ! min
x2Vj
X
U⇢Vj:U3x ˆ↵U = min
x2Vj
X
B⇢V :B3x ↵B ≥δ(↵),
and from (2.12) we have µ[EntVj\B(f)] µ[EntB(f)]. Hence, taking expectation in (3.1)
with respect to µ we obtain
δ(↵) µ[EntVj(f)] 
X
B⇢V
↵B µ[EntB(f)].
Summing over j we have, for all f : ⌦! R+,
δ(↵)
k
X
j=1
µ[EntVj(f)] 
k
X
j=1
X
B⇢V
↵B µ[EntB(f)],
and since by assumption k-partite factorization of entropy holds with constant C, we
have
δ(↵)Ent(f) C
k
X
j=1
X
B⇢V
↵B µ[EntB(f)] C k
X
B⇢V
↵B µ[EntB(f)].
Hence, GBF holds with constant Ck.
Proof of Lemma 1.17. Since d✓ne-UBF holds by assumption, setting C = Cubf one has
Ent(f) C
✓E [µ [EntS(f)]] ,
(3.2)
where S is a random set with uniform distribution over all subsets of V of cardinality
d✓ne, and E denotes the corresponding expectation.
Let S1, S2, . . . denote the connected components of S in G (taken in some arbitrary
order) and for i > 1 let S<i = [i−1
j=1Sj. Then µS<i+1 has the product structure µS<i+1 =
⌦i
j=1µSj. By Lemmas 2.3 and 2.7, one has the decomposition
µ [EntS(f)] =
X
i≥1
µ
⇥
EntS<i+1(µS<i(f))
⇤
=
X
i≥1
µ[EntSi(µS<i(f))],
(3.3)
where we have used Eq. (2.15) with A = Si and B = S<i. For ⌧2 ⌦V \Si, let Γ(Si, ⌧) be
the optimal constant so that
Ent⌧
Si(µS<i(f)) Γ(Si, ⌧)
k
X
j=1
µ⌧
Si
⇥
EntVj\Si(µS<i(f))
⇤
.
Let Γ(Si) = max⌧2⌦V \Si Γ(Si, ⌧). Then,
µ [EntS(f)] 
X
i≥1
Γ(Si)
k
X
j=1
µ
⇥
EntVj\Si(µS<i(f))
⇤
.
We observe next that for all j = 1, ..., k one has
µ
⇥
EntVj\Si(µS<i(f))
⇤
µ
⇥
EntVj\Si(µVj\S<i(f))
⇤
.
(3.4)
To see this, we apply Lemma 2.7 with A = Vj \ Si, B = S<i and U = Vj \ S<i. Since
µS<i+1 = ⌦i
j=1µSj the assumptions for that lemma are satisﬁed and we obtain (3.4) from
Eq. (2.16).
EJP 27 (2022), paper 142.
Page 18/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Summarizing, we have obtained
Ent(f) C
✓
k
X
j=1
E
2
4X
i≥1
Γ(Si) µ
⇥
EntVj\Si(µVj\S<i(f))
⇤
3
5 .
(3.5)
We show next that for all j = 1, ..., k
E
2
4X
i≥1
Γ(Si) µ
⇥
EntVj\Si(µVj\S<i(f))
⇤
3
5 C0µ
⇥
EntVj(f)
⇤
,
(3.6)
with C0 = O
⇣
log(1/b)
b2
∆2⌘
. Combined with (3.5), this concludes the proof of the lemma.
Let us ﬁx j and let v1, v2, . . . denote an ordering of the sites in Vj \ S such that
v1, ..., v|Vj\S1| is an ordering of Vj \ S1, v|Vj\S1|+1, ..., v|Vj\S1|+|Vj\S2| is an ordering of
Vj \ S2 and so on. Since, for all i ≥1, µVj\Si is a product measure, Lemmas 2.3 and 2.7
(as in (3.3)) imply
µ
⇥
EntVj\Si(µVj\S<i(f))
⇤
=
|Vj\S1|+···+|Vj\Si|
X
h=|Vj\S1|+···+|Vj\Si−1|+1
µ [Entvh(%vh(f))] ,
where %vh is the conditional distribution obtained from µ by freezing the spins at all the
sites outside Vj, together with all the sites vh, vh+1, . . . , v|Vj\S|.
Using this decomposition and rearranging one ﬁnds
E
2
4X
i≥1
Γ(Si) µ
⇥
EntVj\Si(µVj\S<i(f))
⇤
3
5
(3.7)
= E
2
4X
i≥1
Γ(Si)
|Vj\S1|+···+|Vj\Si|
X
h=|Vj\S1|+···+|Vj\Si−1|+1
µ [Entvh(%vh(f))]
3
5
= E
"X
h
µ [Entvh(%vh(f))] Γ(S(vh))
#
,
(3.8)
where S(vh) denotes the (unique) connected component of S containing vh. Notice that
for each realization of S, µVj\S is a product measure and so one has from Lemmas 2.3
and 2.7 that
X
h
µ [Entvh(%vh(f))] = µ
⇥
EntVj\S(f))
⇤
µ
⇥
EntVj(f)
⇤
;
the inequality follows from (2.12).
Observe that each term µ[Entvh(%vh(f))], as well as the sequence {vh}, depends on
the realization S only through Vj \ S. Therefore,
E
"X
h
µ [Entvh(%vh(f))] Γ(S(vh))
#
= E
"X
h
µ [Entvh(%vh(f))] E [Γ(S(vh)) | Vj \ S]
#
,
where E [Γ(S(vh)) | Vj \ S] is the conditional expectation of Γ(S(vh)) given the realization
Vj \ S. Therefore, (3.6) follows if we prove that
max
W ⇢Vj max
v2W E [Γ(S(v)) | Vj \ S = W] C0.
(3.9)
Now, for a b marginally bounded spin system, it follows from Lemma 4.2 in [CLV21]
and (2.12) that
Γ(S(v)) ⇣|S(v)|3z|S(v)|,
EJP 27 (2022), paper 142.
Page 19/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
where ⇣= ⇣(b) = 3 log(1/b)
2b2
and z = 1/b2. Thus,
max
W ⇢Vj max
v2W E [Γ(S(v)) | Vj \ S = W] ⇣· max
W ⇢Vj max
v2W E
h
|S(v)|3z|S(v)| | Vj \ S = W
i
.
(3.10)
To bound the expectation on the right-hand-side of (3.10), we consider the graph G2
with vertex set V and edge set E [ E2, where E is the edge set of G and E2 is the set of
all pairs of vertices with a common neighbor in G. Note that G2 has maximum degree
∆2. Let Av(a) be the collection of subsets of vertices U ⇢V such that |U| ≥a, v 2 U and
the induced subgraph G2[U] of U in G2 is connected.
Now, let us ﬁx the set W = Vj \S and the vertex v 2 W and let S2 := (S(v)\V6=j) ⇢S,
where V6=j := S
i:i6=j Vi. We claim that when the event {|S(v)| = a} occurs for some
a 2 N, then S2 2 Av(
a
∆+1). Indeed, G2[S2] is connected, since S(v) is connected in G and
removing the vertices in Vj from S(v) will not disconnect S2 in G2. Moreover, since each
vertex in S(v) \ Vj is a neighbor of some vertex in S(v) \ V6=j and the maximum degree
of G is ∆, one has ∆|S(v) \ V6=j| ≥|S(v) \ Vj|, and so
a = |S(v) \ Vj| + |S(v) \ V6=j| (∆+ 1)|S(v) \ V6=j|,
which implies that |S2| = |S(v)\V6=j| ≥a/(∆+1). Given S, let T2(v) denote the connected
component of S in G2 containing v, and note that S2 ⇢T2(v). Then, for any W ⇢Vj,
v 2 W and integer a ≥1 we get
P (|S(v)| = a | Vj \ S = W) P
✓
9 S0 2 Av
✓
a
∆+ 1
◆
; S0 ⇢S
◆
P
✓
|T2(v)| ≥
a
∆+ 1
◆
.
(3.11)
To estimate the size of the connected component T2(v) one can use Lemma 4.3 from
[CLV21], which implies that for any integer m ≥1,
P (|T2(v)| = m) `
n(2e∆2✓)m−1.
(3.12)
Indeed, the only difference with respect to Lemma 4.3 from [CLV21] is that we have
maximum degree ∆2 here instead of ∆. In particular, if 2e∆2✓1/2, using `
n 2✓,
P
✓
|T2(v)| ≥
a
∆+ 1
◆
4✓(2e∆2✓)b
a
∆+1 c−1 ∆−2(2e∆2✓)b
a
∆+1 c.
(3.13)
It follows that
E
h
|S(v)|3z|S(v)| | Vj \ S = W
i
=
X
a≥1
a3za · P (|S(v)| = a | Vj \ S = W)
(3.14)
∆−2 X
a≥1
a3(2e∆2✓z∆+1)b
a
∆+1 c C1∆2,
(3.15)
for some absolute constant C1 provided that 2e∆2✓z∆+1 1/2. The last bound can
be seen e.g. by writing the sum over a as a sum over ` and the by summing over
(` −1)(∆+ 1) a `(∆+ 1) −1. This implies that
max
W ⇢Vj max
v2W E
h
|S(v)|3z|S(v)| | Vj \ S = W
i
C1∆2.
Hence, (3.9) and (3.6) hold with C0 = C1⇣∆2, and consequently k-partite factorization
holds with constant CubfC1⇣∆2/✓.
EJP 27 (2022), paper 142.
Page 20/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
4
Spectral independence for contractive distributions
In this section we establish our main results that a contractive distribution is spec-
trally independent. These results in particular connect classic probabilistic approach for
establishing fast mixing of Markov chains such as coupling with recent developments
utilizing spectral independence. We ﬁrst consider a special case of Theorem 1.10 con-
cerned with Glauber dynamics and Hamming metric in Section 4.1; this will serve as a
concrete example to illustrate our approach for establishing spectral independence. In
Section 4.2, we consider arbitrary metric and prove Theorem 1.10. Finally, we consider
general Markov chains and metrics in Section 4.3 and prove Theorem 1.11.
4.1
Warm-up: contraction for Glauber dynamics and Hamming metric
In this section, we prove a simpler version of Theorem 1.10, which already gives the
main idea of our proof approach for establishing spectral independence. We show that,
if the distribution µ is contractive w.r.t. the Glauber dynamics and the Hamming metric,
then it is spectrally independent.
Theorem 4.1. If µ is -contractive w.r.t. the Glauber dynamics and the Hamming metric
for some 2 (0, 1), then µ is spectrally independent with constant ⌘=
2
(1−)n.
In
particular, if 1 −✏/n, then ⌘2/✏.
Remark 4.2. In this paper, the Glauber dynamics P ⌧
gl for the conditional distribution µ⌧
with a pinning ⌧on U ⇢V is deﬁned as follows: in each step the chain picks a vertex
x 2 V u.a.r. and updates its spin conditioned on all other vertices and ⌧. In particular, all
pinned vertices in U are allowed to be selected and when this happens the conﬁguration
will remain the same (no updates will be made). This setting can make our theorem
statements and proofs easier to understand, and will not harm our results since we only
consider these chains for the purpose of analysis rather than actually running them.
Alternatively, we can deﬁne the Glauber dynamics ˜P ⌧
gl for µ⌧in the following way: in each
step an unpinned vertex x 2 V \ U is selected u.a.r. and updated accordingly. Note that
˜P ⌧
gl is faster than P ⌧
gl and the contraction rate of ˜P ⌧
gl depends on the number of unpinned
vertices. If we assume µ⌧is `-contractive w.r.t. ˜P ⌧
gl and dH where ` = |V \ U|, then an
analog of Theorem 4.1 can show that µ is spectrally independent with
⌘=
max
`=1,...,n
⇢
2
(1 −`)`
;
.
However, in actual applications such as under the Dobrushin uniqueness condition in
Section 4.2.1, the contraction rate satisﬁes ` 1 −✏/`, so we eventually get ⌘2/✏
just as from Theorem 4.1.
Recall that for any pinning ⌧2 T we let µ⌧be the conditional distribution over ⌦⌧
given ⌧, and the ALO inﬂuence matrix J⌧is a square matrix indexed by X ⌧and deﬁned
as J(x, a; x, a0) = 0 and
J⌧(x, a; y, a0) = µ⌧(σy = a0 | σx = a) −µ⌧(σy = a0) for x 6= y.
The distribution µ is said to be ⌘-spectrally independent if λ1(J⌧) ⌘for all pinning ⌧.
Our goal is to upper bound the maximum eigenvalue of the ALO inﬂuence matrix J⌧
for a given pinning ⌧. In fact, to make notations simpler we will only consider the case
where there is no pinning; the proof is identical by replacing ⌦, µ, J with ⌦⌧, µ⌧, J⌧when
an arbitrary pinning ⌧is given. To upper bound λ1(J), a standard approach that has
been applied in previous works [ALO20, CLV20, CGvV21, FGYZ21, CLV21] is to upper
bound the inﬁnity norm of J. More speciﬁcally, for each (x, a) 2 X we deﬁne
S(x, a) =
X
(y,a0)2X
|J(x, a; y, a0)|
(4.1)
EJP 27 (2022), paper 142.
Page 21/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
to be the sum of absolute inﬂuences of a given pair (x, a). The quantity S(x, a) can be
thought of as the total inﬂuence of (x, a) on all other vertex-spin pairs. If one can show
S(x, a) ⌘for all (x, a) 2 X , then it immediately follows that
λ1(J) kJk1 =
max
(x,a)2X S(x, a) ⌘.
Hence, it sufﬁces to prove a suitable upper bound on S(x, a). Fix (x, a) 2 X , and deﬁne
the distribution ⌫= µ(· | σx = a); namely, ⌫is the conditional distribution of µ with the
pinning σx = a. The key observation we make here is that the quantity S(x, a) can be
viewed as the difference of the expectation of some function f under the two measures
µ and ⌫. More speciﬁcally, we deﬁne
f(σ) =
X
(y,a0)2X
t(x, a; y, a0) 1{σy=a0},
(4.2)
where
t(x, a; y, a0) = sgn(J(x, a; y, a0)) =
8
>
<
>
:
+1,
J(x, a; y, a0) > 0;
−1,
J(x, a; y, a0) < 0;
0,
J(x, a; y, a0) = 0.
With this deﬁnition it follows that
S(x, a) =
X
(y,a0)2X
t(x, a; y, a0)J(x, a; y, a0)
=
X
(y,a0)2X
t(x, a; y, a0)µ(σy = a0 | σx = a) −t(x, a; y, a0)µ(σy = a0)
= E⌫f −Eµf.
Therefore, the absolute sum of inﬂuences S(x, a) describes, in some sense, the “distance”
of the two distributions ⌫and µ measured by f.
To be more precise about our last statement, we review some standard deﬁnitions
about the Wasserstein distance. Let (⌦, d) be a ﬁnite metric space. We say a function
f : ⌦! R is L-Lipschitz w.r.t. the metric d if for all σ, ⌧2 ⌦we have
|f(σ) −f(⌧)| Ld(σ, ⌧).
For every function f : ⌦! R, we let Ld(f) be the optimal Lipschitz constant of f w.r.t.
the metric d; i.e., Ld(f) = inf{L ≥0 : f is L-Lipschitz w.r.t. d}. For a pair of distributions
µ and ⌫on ⌦, the 1-Wasserstein distance w.r.t. the metric d between µ and ⌫is deﬁned as
W1,d(µ, ⌫) =
inf
⇡2C(µ,⌫) E⇡[d(σ, ⌧)],
where C(µ, ⌫) denotes the set of all couplings of µ, ⌫(i.e., ⇡(·, ·) 2 C(µ, ⌫) is a joint
distribution over ⌦⇥⌦with the marginals on the ﬁrst and second coordinates being µ
and ⌫respectively) and (σ, ⌧) is distributed as ⇡; equivalently, the 1-Wasserstein distance
can be represented in the following functional form, which follows from Kantorovich-
Rubinstein duality [Vil21],
W1,d(µ, ⌫) =
sup
f:⌦!R
Ld(f)1
Eµf −E⌫f.
(4.3)
Observe that, the function f deﬁned by (4.2) is 2-Lipschitz w.r.t. the Hamming metric
dH; to see this, if σ, ⌧2 ⌦and dH (σ, ⌧) = k then by the deﬁnition of f we have |f(σ) −
f(⌧)| 2k. Therefore, we deduce from (4.3) that
S(x, a) = E⌫f −Eµf LdH(f) W1,dH(⌫, µ) 2W1,dH(⌫, µ).
EJP 27 (2022), paper 142.
Page 22/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
That means, if one can show W1,dH(⌫, µ) C for µ and ⌫= µ(· | σx = a) for any pair
(x, a), then λ1(J) 2C and the ⌘-spectral independence with ⌘= 2C would follow.
The following lemma, which generalizes previous works [BN19, RR19], will be used
to bound the Wasserstein distance of two distributions and may be interesting of its own.
Roughly speaking, it claims that if µ, ⌫are the stationary distributions of two Markov
chains P, Q (e.g., Glauber dynamics) respectively, and if µ is contractive w.r.t. P and the
two chains P, Q are “close” to each other in one step, then the Wasserstein distance
between ⌫and µ is small. The special case where ⌦= {+, −}n and P, Q are both the
Glauber dynamics appeared in [BN19, Theorem 3.1] and [RR19, Theorem 2.1], but here
we do not make any assumption on the state space or the chains, which is crucial to our
applications in Section 4.3.1.
Lemma 4.3. Let (⌦, d) be a ﬁnite metric space. Let µ, ⌫be two distributions over ⌦, and
P, Q be two Markov chains on ⌦with stationary distributions µ, ⌫respectively. If µ is
-contractive w.r.t. the chain P and the metric d, then for every f : ⌦! R we have
|Eµf −E⌫f| Ld(f)
1 −E⌫[W1,d(P(σ, ·), Q(σ, ·))]
where P(σ, ·) is the distribution after one step of the chain P when starting from σ and
similarly for Q(σ, ·). As a consequence,
W1,d(µ, ⌫) 
1
1 −E⌫[W1,d(P(σ, ·), Q(σ, ·))] .
We remark that Lemma 4.3 holds in a very general setting, and (⌦, d) can be any
ﬁnite metric space. It shows that if two Markov chains are close to each other, then their
stationary distributions must be close to each other, under the assumption that one of
the chains is contractive.
Proof of Lemma 4.3. The proof imitates the arguments from [BN19, RR19]. Assume for
now that P is irreducible; this is a conceptually easier case and we will consider general
P later. Since P is irreducible, let h be the principal solution to the Poisson equation
(I −P)h = ¯f where ¯f = f −Eµf; that is,
h =
1
X
t=0
P t ¯f.
(4.4)
See Lemma 2.1 in [BN19] and the references in that paper for backgrounds on the
Poisson equation. We then have
E⌫f −Eµf = E⌫¯f = E⌫[(I −P)h] = E⌫[(Q −P)h]
where the last equality is due to ⌫= ⌫Q. For each σ 2 supp(⌫) ⇢⌦, we deduce from (4.3)
that
((Q −P)h)(σ) = EQ(σ,·)h −EP (σ,·)h Ld(h) W1,d(Q(σ, ·), P(σ, ·)).
It remains to bound the Lipschitz constant of h. For σ, ⌧2 ⌦,
|h(σ) −h(⌧)| 
1
X
t=0
@@(P t ¯f)(σ) −(P t ¯f)(⌧)
@@
=
1
X
t=0
@@EP t(σ,·) ¯f −EP t(⌧,·) ¯f
@@
Ld(f)
1
X
t=0
W1,d(P t(σ, ·), P t(⌧, ·))
EJP 27 (2022), paper 142.
Page 23/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
where the last inequality again follows from (4.3). Since µ is -contractive w.r.t. P and d,
for all σ, ⌧2 ⌦and every integer t ≥1 we have
W1,d(P t(σ, ·), P t(⌧, ·)) td(σ, ⌧).
We then deduce that
|h(σ) −h(⌧)| Ld(f)
1
X
t=0
td(σ, ⌧) = Ld(f)
1 −d(σ, ⌧).
This implies that Ld(h) Ld(f)/(1 −) and the lemma then follows.
Next, we show how to remove the assumption that P is irreducible. Observe that in
the proof above we only need the irreducibility of P to guarantee that the function h
given by (4.4) is well-deﬁned; i.e., the series on the right-hand side of (4.4) is convergent.
The rest of the proof does not require the irreducibility of P . In fact, one can deduce the
convergence of (4.4) solely from the contraction of P . Note that for all σ 2 ⌦,
@@P t ¯f(σ)
@@ =
@@P t ¯f(σ) −EµP t ¯f
@@
=
@@@@@P t ¯f(σ) −
X
⌧2⌦
µ(⌧)P t ¯f(⌧)
@@@@@

X
⌧2⌦
µ(⌧)
@@P t ¯f(σ) −P t ¯f(⌧)
@@
where the ﬁrst equality follows from EµP t ¯f = Eµ ¯f = 0. Since ⌦is ﬁnite, to show
that (4.4) is convergent for all σ 2 ⌦, it sufﬁces to show that for all σ, ⌧2 ⌦the series
P1
t=0
@@P t ¯f(σ) −P t ¯f(⌧)
@@ is convergent. Actually, our proof before has already showed
that
1
X
t=0
@@P t ¯f(σ) −P t ¯f(⌧)
@@ Ld(f)
1 −d(σ, ⌧) < 1
using only the contraction of P , where we have Ld(f) < 1 and supσ,⌧2⌦d(σ, ⌧) < 1
because ⌦is ﬁnite.
Therefore, the lemma remains true without the assumption of
irreducibility of P .
Given Lemma 4.3, we can now complete the proof of Theorem 4.1.
Proof of Theorem 4.1. For every (x, a) 2 X , we deduce from Lemma 4.3 that
S(x, a) = E⌫f −Eµf LdH(f)
1 −E⌫[W1,dH(P(σ, ·), Q(σ, ·))]
(4.5)
where S(x, a) is given by (4.1), f is given by (4.2), P is the Glauber dynamics for µ, and
Q is the Glauber dynamics for ⌫= µ(x,a) = µ(· | σx = a) (we use (x, a) to denote the
pinning σx = a). We claim that for every σ 2 ⌦(x,a),
W1,dH(P(σ, ·), Q(σ, ·)) 1
n.
(4.6)
To see this, let σ1 and σ2 be the conﬁgurations after one step of P and Q respectively
when starting from σ. We can couple σ1 and σ2 by picking the same vertex to update
in the Glauber dynamics. If the picked vertex is not x, then we can make σ1 = σ2;
meanwhile, if x is picked, which happens with probability 1/n, then dH(σ1, σ2) 1 where
the discrepancy is caused by the pinning σx = a. Therefore, the 1-Wasserstein distance
between σ1 and σ2 is upper bounded by 1/n; this justiﬁes our claim. Combining LdH(f) 
2 and (4.6), we obtain from (4.5) that S(x, a) 
2
(1−)n for each (x, a); consequently,
λ1(J) 
2
(1−)n. The same argument holds for µ⌧under any pinning ⌧as well, and
spectral independence then follows.
EJP 27 (2022), paper 142.
Page 24/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
4.2
Contraction for Glauber dynamics and general metrics
In this section, we generalize the Hamming metric assumption in Theorem 4.1 to
any weighted Hamming metric or any metric equivalent to Hamming, which establishes
Theorem 1.10. We restate it here for convenience.
Theorem 1.10.
(1) If µ is -contractive w.r.t. the Glauber dynamics and an arbitrary
w-weighted Hamming metric, then µ is spectrally independent with constant ⌘=
2
(1−)n. In particular, if 1 −✏
n, then ⌘2
✏.
(2) If the metric in (1) is not a weighted Hamming metric but instead an arbitrary
γ-equivalent metric, then ⌘=
2γ2
(1−)n. In particular, if 1 −✏
n, then ⌘2γ2
✏.
We prove the two cases of Theorem 1.10 separately. We ﬁrst consider the weighted
Hamming metric. Recall that for a positive weight function w : V ! R+, the w-weighted
Hamming metric d = dw is given by
dw(σ, ⌧) =
X
x2V
w(x)1{σx 6= ⌧x} for σ, ⌧2 ⌦.
In particular, if w(x) = 1 for all x then d is the usual Hamming metric.
Unfortunately, the proof of Theorem 4.1 does not work directly in this scenario. The
reason is that the right-hand side of (4.5), with dH replaced by d = dw now, can be as
large as O(wmax/wmin) (more speciﬁcally, Ld(f) = O(1/wmin) and W1,d(P(σ, ·), Q(σ, ·)) =
O(wmax)), which can be unbounded since we are not making any assumption on w.
To deal with this, we need to take the vertex weights into account when deﬁning the
function f and, more importantly, when deﬁning the absolute sum of inﬂuences S(x, a).
Proof of Theorem 1.10(1). For ease of notation we may assume that there is no pinning;
the proof remains the same with an arbitrary pinning ⌧. For ﬁxed (x, a) 2 X , we deﬁne
the w-weighted sum of absolute inﬂuences given by
Sw(x, a) =
X
(y,a0)2X
w(y) |J(x, a; y, a0)|.
(4.7)
Such weighted sums were considered in [CLV20, Lemma 22] to deduce spectral indepen-
dence. We claim that if Sw(x, a) ⌘w(x) for all (x, a) 2 X for some ⌘> 0, then λ1(J) ⌘.
To see this, let ˜w 2 R|X|
+
with ˜w(x, a) = w(x) and let W = diag( ˜w); the assumption of the
claim then implies that kW −1JWk1 ⌘and thus λ1(J) = λ1(W −1JW) ⌘. Therefore,
it sufﬁces to upper bound the ratio Sw(x, a)/w(x).
Let ⌫= µ(x,a) = µ(· | σx = a) be the conditional distribution with pinning σx = a, and
deﬁne
fw(σ) =
X
(y,a0)2X
w(y) t(x, a; y, a0) 1{σy=a0}
(4.8)
where t(x, a; y, a0) = sgn(J(x, a; y, a0)). Observe that Ld(fw) 2 and
Sw(x, a) = E⌫fw −Eµfw.
It then follows from Lemma 4.3 that
Sw(x, a) 
2
1 −E⌫[W1,d(P(σ, ·), Q(σ, ·))]
where P, Q are the Glauber dynamics for µ, ⌫respectively. For every σ 2 ⌦(x,a) we have
W1,d(P(σ, ·), Q(σ, ·)) w(x)
n
,
EJP 27 (2022), paper 142.
Page 25/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
since if we couple the conﬁgurations σ1, σ2 after one step of P, Q respectively by picking
the same vertex to update, then d(σ1, σ2) = w(x) only when the site x is picked, and
σ1 = σ2 otherwise. Therefore, we get Sw(x, a) 
2w(x)
(1−)n for every (x, a) 2 X , implying
that λ1(J) 
2
(1−)n. The same argument works for µ⌧under any pinning ⌧as well, which
establishes spectral independence.
Next we consider the second part of Theorem 1.10. Recall that a metric d on ⌦is
said to be γ-equivalent (to the Hamming metric) for some γ > 1 if for all σ, ⌧2 ⌦
1
γ dH (σ, ⌧) d(σ, ⌧) γdH (σ, ⌧) .
To prove the second part, we follow the proof approach for Theorem 4.1, and in particular
the right-hand side of (4.9) below (analogous to (4.5)) can be upper bounded using the
γ-equivalence.
Proof of Theorem 1.10(2). For every (x, a) 2 X , we deduce from Lemma 4.3 that
S(x, a) = E⌫f −Eµf Ld(f)
1 −E⌫[W1,d(P(σ, ·), Q(σ, ·))]
(4.9)
where S(x, a) and f are deﬁned by (4.1), (4.2) respectively, and P, Q are the Glauber
dynamics for µ and ⌫= µ(x,a) = µ(· | σx = a) respectively. Since d is γ-equivalent, for all
σ, ⌧2 ⌦we have
|f(σ) −f(⌧)| 2dH (σ, ⌧) 2γd(σ, ⌧);
this shows Ld(f) 2γ. Meanwhile, by the deﬁnition of 1-Wasserstein distance for every
σ 2 ⌦(x,a) we have
W1,d(P(σ, ·), Q(σ, ·)) = inf {E⇡[d(σ, ⌧)] | ⇡2 C(P(σ, ·), Q(σ, ·))}
γ inf {E⇡[dH(σ, ⌧)] | ⇡2 C(P(σ, ·), Q(σ, ·))} = γW1,dH(P(σ, ·), Q(σ, ·)) γ
n
where the last inequality is (4.6). Thus, we obtain from (4.9) that S(x, a) 
2γ2
(1−)n. The
rest of the proof is the same as Theorem 4.1.
4.2.1
Application: Dobrushin uniqueness condition
As an application of Theorem 1.10, we show that the Dobrushin uniqueness condition, as
well as its generalizations [Hay06, DGJ09], implies spectral independence. Recall that
the Dobrushin dependency matrix R is a |V | ⇥|V | matrix deﬁned as R(x, x) = 0 and
R(x, y) = max {dTV (µy(· | σ), µy(· | ⌧)) : (σ, ⌧) 2 Sx,y} for x 6= y
where Sx,y is the set of pairs of conﬁgurations on V \ {y} that differ at most at x. Denote
the spectral radius of a square matrix M by %(M). If M is nonnegative, then %(M) is an
eigenvalue of M by the Perron-Frobenius theorem. We prove Theorem 1.13 from the
introduction.
Theorem 1.13. If the Dobrushin dependency matrix R satisﬁes %(R) 1 −✏for some
✏> 0, then µ is spectrally independent with constant ⌘= 2/✏.
Remark 4.4. If kRk1 < 1, then the Glauber dynamics mixes rapidly by a simple applica-
tion of the path coupling method of Bubley and Dyer [BD97]. The same is true under the
Dobrushin uniqueness condition, i.e., when kRk1 < 1. Hayes [Hay06] generalized the
condition to the spectral norm kRk2 < 1. Dyer, Goldberg, and Jerrum [DGJ09] further
improved it to kRk < 1 for any matrix norm (where the mixing time depends logarithmly
EJP 27 (2022), paper 142.
Page 26/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
on the norm of the all-one matrix). Our condition %(R) < 1 in Theorem 1.13 is technically
better than previous works since for a nonnegative matrix R one has %(R) kRk for any
matrix norm, and the inequality can be strict for all norms when R is not irreducible;
see [DGJ09] for related discussions. Finally, we point out that if R is symmetric then
%(R) = kRk2.
It is known that the Glauber dynamics is contractive for some weighted Hamming
metric if the weight vector satisﬁes a spectral condition related to R.
Lemma 4.5 ([DGJ09, Lemma 20]). If w 2 RV
+ is a positive vector such that Rw (1−✏)w
entrywisely, then µ is (1−✏/n)-contractive w.r.t. the Glauber dynamics and the w-weighted
Hamming metric d = dw.
The following fact about nonnegative matrices is helpful.
Lemma 4.6 ([Mey00, Example 7.10.2]). If M, N 2 Rn⇥n
+
are two nonnegative square
matrices such that M N entrywisely, then %(M) %(N).
We give below the proof of Theorem 1.13.
Proof of Theorem 1.13. Consider ﬁrst the case that there is no pinning. If the Dobrushin
dependency matrix R is irreducible, then the right principal eigenvector w associated
with the eigenvalue %(R) satisﬁes Rw = %(R)w (1 −✏)w and w > 0 by the Perron-
Frobenius theorem. Hence, Lemma 4.5 and (the proof of) Theorem 1.10(1) immediately
yield λ1(J) 2/✏. However, if R is reducible, we cannot use the principal eigenvector
directly since it may have zero entries. We instead consider the matrix Rδ = R + δO
where O is the all-one matrix and δ > 0 is a tiny constant. Let wδ be the right principal
eigenvector of Rδ associated with the eigenvalue %(Rδ). Since Rδ is irreducible, wδ > 0 by
the Perron-Frobenius theorem. Moreover, Rwδ Rδwδ = %(Rδ)wδ. Since limδ!0 Rδ = R,
we have limδ!0 %(Rδ) = %(R); see, e.g., Remark 3.4 in [Ale13]. Thus, %(Rδ) < 1 for
sufﬁciently small δ. Then by Lemma 4.5 and Theorem 1.10(1), for δ small enough, we
have λ1(J) 2/(1 −%(Rδ)). Taking δ ! 0 and using the assumption that %(R) 1 −✏,
we obtain λ1(J) 2/✏.
Next, consider the conditional measure µ⌧with a pinning ⌧on a subset U ⇢V . Let
R⌧be the Dobrushin dependency matrix for µ⌧; note that by deﬁnition R⌧(x, y) = 0 if
x 2 U or y 2 U, and R⌧(x, y) R(x, y) for all x, y 2 V . We deduce from Lemma 4.6 that
%(R⌧) %(R) 1 −✏and thus this is reduced to the no-pinning case. Therefore, we get
λ1(J⌧) 2/✏for all ⌧and spectral independence then follows.
4.3
Contraction for general Markov chains and general metrics
In this section, we generalize Theorem 4.1 to arbitrary “local” Markov chains and
arbitrary metrics close to the Hamming metric. In particular, we prove Theorem 1.11.
Consider a collection of Markov chains P = {P ⌧: ⌧2 T } associated with µ, where
each P ⌧is a Markov chain on ⌦⌧with stationary distribution µ⌧. Intuitively, one can
think of P as the same dynamics applied to all conditional distributions µ⌧; for example,
P can be the collection of Glauber dynamics for all µ⌧’s. We are particularly interested
in local dynamics; these are Markov chains that make local updates on the conﬁguration
in each step, e.g., Glauber dynamics for spin systems or ﬂip dynamics for colorings.
Alternatively, we can describe local dynamics as those insensitive to pinnings; that is,
if the dynamics is applied to both µ and µ(x,a) with a pinning σx = a, then with high
probability there is no difference in the two chains or the discrepancy caused by the
pinning will not propagate. This motivates the following deﬁnition.
Deﬁnition 4.7. We say a collection P of Markov chains associated with µ is Φ-local if for
any two adjacent pinnings ⌧2 T and ⌧0 = ⌧[ (x, a) where (x, a) 2 X ⌧(i.e., ⌧0 combines ⌧
EJP 27 (2022), paper 142.
Page 27/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
and the pinning σx = a), and for all σ 2 ⌦⌧0, we have
W1,dH(P ⌧(σ, ·), P ⌧0(σ, ·)) Φ.
We show that for such local dynamics contraction implies spectral independence.
Theorem 4.8. If µ is -contractive w.r.t. a Φ-local collection P of Markov chains and a
γ-equivalent metric d for some 2 (0, 1), then µ is spectrally independent with constant
⌘= 2γ2Φ
1−.
Proof. The proof is similar to that of Theorems 4.1 and 1.10(2). For an arbitrary pinning
⌧and (x, a) 2 X ⌧, we deﬁne
S⌧(x, a) =
X
(y,a0)2X ⌧
|J⌧(x, a; y, a0)|
(4.10)
and
f ⌧(σ) =
X
(y,a0)2X ⌧
t⌧(x, a; y, a0) 1{σy=a0}
(4.11)
where t⌧(x, a; y, a0) = sgn(J⌧(x, a; y, a0)); these deﬁnitions are analogous to (4.1) and (4.2)
with pinning ⌧. Let ⌧0 = ⌧[ (x, a). Then we deduce from Lemma 4.3 that
S⌧(x, a) = Eµ⌧0 f ⌧−Eµ⌧f ⌧Ld(f ⌧)
1 −Eµ⌧0
h
W1,d(P ⌧(σ, ·), P ⌧0(σ, ·))
i
.
As shown in the proof of Theorem 1.10(2), since d is γ-equivalent to the Hamming metric
we have Ld(f ⌧) γLdH(f ⌧) 2γ and for all σ 2 ⌦⌧0 we have
W1,d(P ⌧(σ, ·), P ⌧0(σ, ·)) γW1,dH(P ⌧(σ, ·), P ⌧0(σ, ·)) γΦ
using the Φ-locality of P. Therefore, we obtain that S⌧(x, a) 2γ2Φ
1−for all (x, a) 2 X ⌧.
This yields λ1(J⌧) 2γ2Φ
1−and spectral independence follows.
To better understand local dynamics, we consider a very general type of Markov
chains which we call select-update dynamics; examples include the Glauber dynamics,
heat-bath block dynamics, and ﬂip dynamics. Let B be a collection of blocks associated
with the select-update dynamics and ﬁx some pinning ⌧. Given the current conﬁguration
σt 2 ⌦⌧, the next conﬁguration σt+1 is generated as follows:
1. Select: Select a block B 2 B from some distribution pt over B;
2. Update: Resample the conﬁguration on B from some distribution ⌫t
B.
We try to make weakest assumptions on the selection rule pt and the update rule ⌫t
B:
the selection distribution pt is allowed to depend on the current conﬁguration σt but
is independent of the pinning ⌧, and the update distribution ⌫t
B is allowed to depend
on the whole current conﬁguration σt and the part of the pinning ⌧contained in B. In
particular, the heat-bath block dynamics is a special case of the select-update dynamics:
the selection rule pt = ↵is a ﬁxed distribution over B and the update rule ⌫t
B is the
marginal distribution on B conditioned on σt outside B and the pinning ⌧in B.
Remark 4.9. The assumption that the selection rule pt is independent of the pinning
⌧is not necessary, but it is helpful for stating and proving our theorems and does
not weaken our results. Roughly speaking, we only require that the collection of the
select-update dynamics is the same dynamics applied to all µ⌧’s, and the selection rule
pt can be conditioned on containing at least one unpinned vertex. See the discussions in
Remark 4.2 for the Glauber dynamics.
EJP 27 (2022), paper 142.
Page 28/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
We write PB for a collection of select-update dynamics associated with µ. Denote the
maximum block size of B by
M = max
B2B |B|,
and the maximum probability of a vertex being selected in Step 1 by
D = max
pt max
x
X
B2B:x2B
pt(B),
(4.12)
where we maximize over all selection rules pt that can occur. We can show that the
select-update dynamics PB is Φ-local with Φ = DM; using this and Theorem 4.8 we
establish Theorem 1.11, which we restate here for convenience.
Theorem 1.11. If µ is -contractive w.r.t. arbitrary select-update dynamics and an
arbitrary γ-equivalent metric, then µ is spectrally independent with constant ⌘= 2γ2DM
1−
.
Proof. It sufﬁces to show that the select-update dynamics PB is Φ-local with Φ = DM;
the theorem would then follows immediately from Theorem 4.8. Consider two adjacent
pinnings ⌧and ⌧0 = ⌧[ (x, a) where (x, a) 2 X ⌧. For σ 2 ⌦⌧0, let σ1 and σ2 be the two
conﬁgurations obtained from σ after one step of P ⌧and P ⌧0 respectively. We couple
σ1 and σ2 by picking the same block B 2 B in Step 1 of the select-update dynamics. If
x /2 B, then we have σ1 = σ2. Meanwhile, if x 2 B, which happens with probability at
most D, we have dH (σ1, σ2) |B| M. Therefore,
W1,dH(P ⌧(σ, ·), P ⌧0(σ, ·)) DM.
This establishes the (DM)-locality for PB.
Remark 4.10. If we further assume that in Step 2 the select-update dynamics resamples
a block independently for each of its components (i.e., the update rule ⌫t
B is a product
distribution over all components of the induced subgraph G[B]), then in Theorem 1.11
the maximum block size M can be replaced by the maximum component size of all
blocks.
4.3.1
Application: ﬂip dynamics for colorings
In this section we establish spectral independence for colorings utilizing Theorem 1.11.
Theorem 4.11. Let ✏0 ⇡10−5 > 0 be a ﬁxed constant. Let ∆, q ≥3 be integers and
q > ( 11
6 −✏0)∆. Then there exists ⌘= ⌘(∆, q) > 0 such that the following holds.
Let µ be the uniform distribution over all proper q-colorings of a graph G = (V, E) of
maximum degree at most ∆. Then µ is spectrally independent with constant ⌘.
To apply Theorem 1.11, we need a contractive Markov chain for sampling colorings of
a graph. Vigoda considered the ﬂip dynamics [Vig00] and showed that it is contractive
for the Hamming metric when the number of colors q >
11
6 ∆. Recently, [CDM+19]
improved the bound to q > ( 11
6 −✏0)∆for a ﬁxed tiny constant ✏0 ⇡10−5, using variable-
length coupling or an alternative metric. Our result on spectral independence builds
upon contraction results for the ﬂip dynamics.
We ﬁrst describe the ﬂip dynamics. Let ⌦be the set of all proper q-colorings of G. Fix
a pinning ⌧on U ⇢V . For a coloring σ 2 ⌦, a vertex x 2 V , and a color a 2 [q], denote by
Lσ(x, a) the bicolored component containing x with colors a and σx; that is, the set of all
vertices which can be reached from x through an alternating (σx, a)-colored path. Given
the coloring σt at time t, the ﬂip dynamics with pinning ⌧generates the next coloring
σt+1 as follows:
1. Pick a vertex x 2 V u.a.r. and a color a 2 [q] u.a.r.;
EJP 27 (2022), paper 142.
Page 29/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
2. If Lσt(x, a) contains a pinned vertex (i.e., Lσt(x, a) \ U 6= ;), then σt+1 = σt;
3. If all vertices in Lσt(x, a) are free (i.e., Lσt(x, a) \ U = ;), then ﬂip the two colors
of Lσt(x, a) with probability ps/s where s = |Lσt(x, a)|.
The ﬂip dynamics is speciﬁed by the ﬂip parameters {ps}1
s=1. In [Vig00] and the
recent improvement [CDM+19], the ﬂip parameters are chosen in such a way that ps = 0
for all s ≥7; i.e., in each step at most six vertices change their colors. We set the
ﬂip parameters as in Observation 5.1 from [CDM+19], where the authors established
contraction of the ﬂip dynamics using the path coupling method.
Lemma 4.12 ([CDM+19]). Under the assumptions of Theorem 4.11, there exists a
constant ✏= ✏(∆, q) > 0 and a 2-equivalent metric d such that µ is (1 −✏/n)-contractive
w.r.t. the ﬂip dynamics and the metric d.
We remark that the pinning ⌧induces a list coloring instance where each unpinned
vertex has a color list to choose its color from, and the results of [CDM+19] generalize
naturally to list colorings. Also, in this paper we assume that the ﬂip dynamics may
pick a pinned vertex and stay at the current coloring. This does not weaken our results
since we only consider the ﬂip dynamics for analysis rather than actually running it; see
Remark 4.2 addressing the same issue for the Glauber dynamics and also Remark 4.9
for general select-update dynamics.
We give below the proof of Theorem 4.11.
Proof of Theorem 4.11. Observe that the ﬂip dynamics belongs to the class of select-
update dynamics, where the associated B is the collection of connected subsets of
vertices.
Since the ﬂip parameters satisfy ps > 0 only for s 6, we have M 6.
Moreover, we have D ∆6/n since a vertex x is in the selected bicolored component
Lσt(y, a) only if dist(x, y) 5, which happens with probability at most ∆6/n. The theorem
then follows from Lemma 4.12 and Theorem 1.11.
We conclude here with the proof of Theorem 1.1.
Proof of Theorem 1.1. By Theorem 4.11 the uniform distribution µ of proper colorings
is spectrally independent. Then the results follows immediately from Theorem 1.7.
4.3.2
Application: block dynamics for Potts model
Here we apply Theorems 4.1 and 1.11 to the ferromagnetic Potts model to establish
spectral independence.
Theorem 4.13. Let ∆≥3 and q ≥2 be integers. Let µ be the Gibbs distribution of the
q-state ferromagnetic Potts model with inverse temperature parameter β on a graph
G = (V, E) of maximum degree at most ∆. Then, the following holds:
1. If β < max
" 2
∆, 1
∆ln( q−1
∆)
 
, then µ is spectrally independent with constant ⌘=
⌘(β, ∆).
2. For any δ > 0 there exists c = c(δ, ∆) > 0 such that, if β 
ln q−c
∆−1+δ then µ is spectrally
independent with constant ⌘= ⌘(δ, β, ∆).
To prove this theorem, we need the following results from [Ull14] and [BGP16]
regarding the contraction of the Glauber dynamics and of the heat-bath block dynamics
with a speciﬁc choice of blocks.
Lemma 4.14 ([Ull14, Corollary 2.14] & [BGP16, Proposition 2.2]). Under the assumptions
in Part 1 of Theorem 4.13, there exists a constant ✏= ✏(β, ∆) such that µ is (1 −✏
n)-
contractive w.r.t. the Glauber dynamics and the Hamming metric.
EJP 27 (2022), paper 142.
Page 30/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Lemma 4.15 ([BGP16, Theorem 2.7]). Under the assumptions in Part 2 of Theorem 4.13,
there exists a collection of blocks B = {Bx}x2V satisfying x 2 Bx, |Bx| = O(1/δ) and
G[Bx] connected for all x, such that µ is (1 −
1
2n)-contractive w.r.t. the ↵-weighted heat-
bath block dynamics for B and the Hamming metric, where ↵is the uniform distribution
over B.
Remark 4.16. To be more precise, [BGP16] shows that the conclusion of Lemma 4.15 is
true when β, q, and the maximum block size M = maxx2V |Bx| satisfy
β
✓
∆−1 + 1
M
◆
+ 3M(ln ∆+ ln M) ln q.
(4.13)
Thus, for any δ > 0, by taking M = dδ−1e and c = 3M(ln ∆+ ln M), our assumption
β 
ln q−c
∆−1+δ in Part 2 of Theorem 4.13 implies (4.13). Moreover, if we take, say, M ⇡pln q
(namely, δ ⇡1/pln q), then c = o(ln q) and our assumption becomes β (1 −o(1)) ln q
∆−1
where o(1) tends to 0 as q ! 1; this gives the bound β1 in Theorem 1.2 from the
introduction.
Theorem 4.13 is an immediate consequence of Lemmas 4.14, 4.15 and the results
proved in this section.
Proof of Theorem 4.13. Part 1 follows directly from Lemma 4.14 and Theorem 4.1. For
Part 2, we note that the block dynamics from Lemma 4.15 corresponds to a select-update
dynamics with M = O(1/δ) and D = ∆O(1/δ)/n; to see the bound on D, we observe that
if x 2 By for some y then the graph distance between x and y is at most |By| = O(1/δ)
since G[By] is connected, and hence the number of blocks containing x is at most ∆O(1/δ).
The theorem then follows from Lemma 4.15 and Theorem 1.11.
We end this section with the proof of Theorem 1.2.
Proof of Theorem 1.2. For Ising model, spectral independence is known in the whole
uniqueness region [CLV20]. For Potts model, Theorem 4.13 establishes spectral inde-
pendence in the corresponding parameter regimes. The theorem then follows from
Theorems 1.7 and 1.8.
5
Spectral independence and entropy factorization
The goal of this section is to reformulate in the setting of spin systems some of the
key facts that were derived in [CLV21] and the references therein in the framework of
simplicial complexes. This specialization yields some minor simpliﬁcation in the main
proofs, and may be of use for later reference. The approach consists in exploiting a
recursive scheme which allows one to derive a global contraction estimate by analysing
the spectral norm of a local operator. This is reminiscent of the recursive approach
developed in [CCL03, CM03, Cap04], where similar ideas were used to derive spectral
gap estimates for a class of conservative spin systems. The argument here is more
robust and, unlike the one in [CCL03, CM03, Cap04], it does not rely on symmetries of
the underlying measures.
We ﬁrst introduce some notation. Let f be a function of the spin conﬁguration σ in
the whole region V , and U ⇢V = [n] a subset of vertices. Recall the notation µV \U
for the conditional distribution given the spins in U, and write Av|U|=` for the uniform
average over all sets U ⇢[n] with ` vertices. We are going to prove the following result
that was established in [CLV21].
EJP 27 (2022), paper 142.
Page 31/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Theorem 5.1. If the spin system is ⌘-spectrally independent and b-marginally bounded
then there exists a constant C = O(1 + ⌘
b ) such that for any ` = {1, . . . , n −1} and for all
f ≥0:
n
` Av|U|=` Ent(µV \Uf) C Entf.
(5.1)
Moreover, for any ✓2 (0, 1], there exists C =
$ 1
✓
%O( ⌘
b ) such that for ` = d✓ne:
`
n Entf C Av|⇤|=` µ [Ent⇤f] .
(5.2)
We remark that, when ` = 1, (5.1) takes the form of an approximate subadditivity
statement:
X
x2V
Ent(fx) C Ent(f),
(5.3)
with constant C = O(1+ ⌘
b ). Here the functions fx are deﬁned by fx = µ(f | σx) = µV \{x}f.
When µ(f) = 1 then ⌫= fµ is a probability measure and, if µx denotes the marginal
of µ on x, then fxµx gives the marginal of ⌫on x. The inequality (5.3) is known to be
equivalent to a Brascamp-Lieb type inequality for the measure µ; see [CLL04, CC09].
For a general discussion of subadditivity of entropy, Brascamp-Lieb type inequalities,
and their applications, see for instance [BCLM11] and the references therein. On the
other hand (5.2) is the uniform block factorization statement `-UBF with ` = d✓ne; see
Deﬁnition 1.15.
We articulate the proof in two steps. The ﬁrst is a recursive scheme which allows one
to go from a local inequality to a global one; see Lemma 5.3. The second step is a control
of the local inequality; see Lemma 5.4.
5.1
Setting up the recursion
If U ⇢V , and ⌧= ⌧U a conﬁguration of spins on U, recall that we use notation
µ⌧= µ(· | ⌧) for the conditional distribution µV \U when the spins on U are given by ⌧.
Moreover, we write µ⌧,x = µ(· | ⌧[σx) if we additionally condition on the spin σx at vertex
x /2 U and similarly for µ⌧,x,y = µ(· | ⌧[ σx [ σy) for x, y /2 U, so that e.g. the expression
µ⌧[Entµ⌧,x,yf] indicates the entropy of f with respect to µ(· | ⌧[ σx [ σy),
Entµ⌧,x,yf = µ⌧,x,y[f log(f/µ⌧,x,y(f))]
averaged over the two spins σx, σy sampled according to µ⌧. Deﬁne the constants ↵k,
k = 0, . . . , n −2, as the largest numbers such that the inequalities
(1 + ↵k)Avx/2U Entµ⌧(µ⌧,x(f)) Avx,y /2U Entµ⌧(µ⌧,x,y(f)) ,
(5.4)
hold for all k = 0, . . . , n −2, for all U ⇢[n] with |U| = k, for all conﬁgurations ⌧on U and
for all functions f ≥0. The symbol Avx/2U denotes the uniform average over all n −k
vertices x /2 U, and Avx,y /2U stands for the uniform average over all (n −k)(n −k −1)
pairs (x, y) with x, y /2 U and x 6= y. We refer to (5.4) as the local inequality, since
for each choice of x, y, the distributions involved are concerned with the spins at two
vertices only.
Remark 5.2. Fix x, y /2 U. Using µ⌧,xf = µ⌧,xµ⌧,x,yf, from Lemma 2.3 we have the
decomposition
Entµ⌧(µ⌧,x,y(f)) = Entµ⌧(µ⌧,x(f)) + µ⌧[Entµ⌧,x(µ⌧,x,y(f)] .
EJP 27 (2022), paper 142.
Page 32/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
In particular, Entµ⌧(µ⌧,x,y(f)) ≥Entµ⌧(µ⌧,x(f)) and therefore (5.4) is always true with
↵k = 0. If µ is a product measure then the subadditivity of entropy for product measures
gives
Entµ⌧(µ⌧,x,y(f)) ≥Entµ⌧(µ⌧,x(f)) + Entµ⌧(µ⌧,y(f)),
which implies the validity of (5.4) with ↵k = 1 for all k = 0, . . . , n −2.
The recursion is based on the following statement, which rephrases [CLV21, Theorem
5.4].
Lemma 5.3. Let ↵k, k = 0, . . . , n −2, be deﬁned by (5.4). Then, for all functions f ≥0,
Av|U|=jEnt(µV \Uf) (1 −j)Ent(f),
j = 1, . . . , n −1,
(5.5)
where
j =
Pn−1
i=j Γi
Pn−1
i=0 Γi
,
Γi =
i−1
Y
k=0
↵k ,
Γ0 = 1.
Proof. The claim (5.5) follows from the fact that for all k = 1, . . . , n −1:
Av|U|=k Ent(µV \Uf) δkAv|U|=k+1 Ent(µV \Uf) ,
δk =
Pk−1
i=0 Γi
Pk
i=0 Γi
,
(5.6)
since Av|U|=n Ent(µV \Uf) = Ent(f), and δjδj+1 · · · δn−1 = (1 −j).
To prove (5.6), note that it holds for k = 1 with δ1 = 1/(1 + ↵0) = Γ0/(Γ0 + Γ1) by the
assumption (5.4) at ⌧= ;. Next, we suppose it holds for 0 < k −1 < n −1 and show it for
k. For any |U| = k + 1 and U 0 ⇢U with |U 0| = k −1, setting {x, y} = U \ U 0 and letting
⌧= ⌧U 0 be the conﬁguration on U 0, as in Lemma 2.3 we have the decomposition
Ent(µV \Uf) = Ent(µ(µV \Uf | ⌧U 0)) + µ
⇥
Ent(µV \Uf | ⌧U 0)
⇤
= Ent(µV \U 0f) + µ [Entµ⌧(µ⌧,x,yf)] .
(5.7)
Averaging we obtain
Av|U|=k+1Ent(µV \Uf) = Av|U 0|=k−1Ent(µV \U 0f)
+ Av|U 0|=k−1Avx,y /2U 0µ [Entµ⌧(µ⌧,x,yf)] .
(5.8)
In the same way
Av|U|=kEnt(µV \Uf) = Av|U 0|=k−1Ent(µV \U 0f)
+ Av|U 0|=k−1Avx/2U 0µ [Entµ⌧(µ⌧,xf)] .
(5.9)
From (5.4),
Av|U|=k+1Ent(µV \Uf) −Av|U 0|=k−1Ent(µV \U 0f)
(5.10)
≥(1 + ↵k−1)Av|U 0|=k−1Avx/2U 0µ [Entµ⌧(µ⌧,xf)]
= (1 + ↵k−1)
⇥
Av|U|=kEnt(µV \Uf) −Av|U 0|=k−1Ent(µV \U 0f)
⇤
.
Therefore,
Av|U|=k+1Ent(µV \Uf) ≥(1 + ↵k−1)Av|U|=kEnt(µV \Uf) −↵k−1Av|U 0|=k−1Ent(µV \U 0f).
By the inductive assumption (5.6) at k −1 we have
Av|U|=k+1Ent(µV \Uf) ≥(1 + ↵k−1 −↵k−1δk−1)Av|U|=kEnt(µV \Uf)
= δ−1
k Av|U|=kEnt(µV \Uf).
EJP 27 (2022), paper 142.
Page 33/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
5.2
Estimating the local coefﬁcients
The next step is an estimate on the coefﬁcients ↵k appearing in (5.4).
Lemma 5.4. If the spin system is ⌘-spectrally independent and b-marginally bounded
then the local inequality (5.4) holds with
↵k ≥1 −
2⌘
b(n −k −1).
(5.11)
Proof. Fix U ⇢V , |U| = k n −2 and ⌧= ⌧U. We may assume µ⌧(f) = 1, which implies
µ⌧(µ⌧,x,y(f)) = µ⌧(µ⌧,x(f)) = 1 for all x, y /2 U. For simplicity, we write Avx,y and Avx
for the averages Avx,y /2U and Avx/2U. Observe that
Avx,y Entµ⌧(µ⌧,x,y(f)) −2Avx Entµ⌧(µ⌧,x(f))
= Avx,y µ⌧[µ⌧,x,y(f) log µ⌧,x,y(f) −µ⌧,x(f) log µ⌧,x(f) −µ⌧,y(f) log µ⌧,y(f)]
(5.12)
= Avx,y µ⌧

µ⌧,x,y(f) log
µ⌧,x,y(f)
µ⌧,x(f)µ⌧,y(f)
-
.
(5.13)
Using a log(a/b) ≥a −b for all a, b ≥0,
Avx,y Entµ⌧(µ⌧,x,y(f)) −2Avx Entµ⌧(µ⌧,x(f))
≥1 −Avx,y µ⌧[µ⌧,x(f)µ⌧,y(f)]
= −Avx,y µ⌧[(µ⌧,x(f) −1)(µ⌧,y(f) −1)] .
(5.14)
We may rewrite
Avx,y µ⌧[(µ⌧,x(f) −1)(µ⌧,y(f) −1)]
=
1
n −k −1
X
(x,a)2X
⌫(x, a)'(x, a)[J⌧'](x, a),
(5.15)
where
'(x, a) = µ⌧(f | σx = a) −1 = [µ⌧,x(f)](a) −1,
X is the set of all pairs (x, a) where x 2 V \ U (if U is the set where ⌧= ⌧U is speciﬁed)
and a 2 [q], ⌫denotes the probability measure on X obtained by setting
⌫(x, a) =
1
n −k µ⌧(σx = a),
and J⌧: X ⇥X 7! R denotes the inﬂuence matrix from Deﬁnition 1.3. Note that in the
derivation of (5.15) we have used the fact that for each ﬁxed y /2 U one has
X
a02[q]
⌫(y, a0)'(y, a0) =
1
n −k µ⌧(µ⌧,y(f) −1) = 0.
Observe that J⌧is self-adjoint in L2(X, ⌫):
⌫(x, a)J⌧(x, a; y, a0) = ⌫(y, a0)J⌧(y, a0; x, a).
(5.16)
In particular, its eigenvalues are real. Let ⌘≥0 denote its largest eigenvalue (the
eigenvalue zero always exists since all row sums of J⌧vanish). Letting h·, ·i denote the
scalar product in L2(X, ⌫) we have h , J⌧ i ⌘h ,  i for all  2 L2(X, ⌫). Therefore,
Avx,y µ⌧[(µ⌧,x(f) −1)(µ⌧,y(f) −1)]
=
1
n −k −1h', J⌧'i 
⌘
n −k −1h', 'i
=
⌘
n −k −1 Avx µ⌧⇥
(µ⌧,x(f) −1)2⇤
=
⌘
n −k −1 Avx Varµ⌧(µ⌧,x(f)).
(5.17)
EJP 27 (2022), paper 142.
Page 34/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Recalling (5.14) we have shown
Avx,y Entµ⌧(µ⌧,x,y(f)) −2Avx Entµ⌧(µ⌧,x(f))
≥−
⌘
n −k −1 Avx Varµ⌧(µ⌧,x(f)).
(5.18)
Next, observe that for every ﬁxed x /2 U, setting h⌧(σx) = [µ⌧,x(f)](σx):
Varµ⌧(µ⌧,x(f)) =
X
a
µ⌧(σx = a)(h⌧(a) −1)2
1
b
 X
a
µ⌧(σx = a)|h⌧(a) −1|
!2
where b = minx/2U mina µ⌧(σx = a), as in Deﬁnition 1.5, with the minimum over a
restricted to spin values that are allowed at x, that is such that µ⌧(σx = a) > 0, and we
have used P
i a2
i (P
i ai)2 for all ai ≥0. Pinsker’s inequality shows that
X
a
µ⌧(σx = a)|h⌧(a) −1| 
q
2 Entµ⌧(µ⌧,x(f)).
It follows that
Varµ⌧(µ⌧,x(f)) 2
b Entµ⌧(µ⌧,x(f)).
(5.19)
Inserting (5.19) into (5.18) concludes the proof.
5.3
Proof of Theorem 5.1
From Lemma 5.3, we see that (5.1) holds with C = n
` (1 −`). From Lemma 5.4 if
follows that
↵k ≥max{1 −R/(n −k −1), 0},
R = d2⌘/be.
Using this bound in the deﬁnition of the coefﬁcients ` and rearranging, see Section 2.2
of [CLV21], it is not hard to see that for any 1 ` n −1:
` ≥(n −` −1) · · · (n −` −R)
(n −1) · · · (n −R)
.
(5.20)
In particular,
n
` (1 −`) n
`
✓
1 −(n −` −1) · · · (n −` −R)
(n −1) · · · (n −R)
◆
.
Remarkably, the expression in the right hand side above is decreasing with `, and
therefore it is always less than R + 1, its value at ` = 1. This shows that (5.1) holds with
C R + 1 = O(1 + ⌘
b ).
To prove (5.2), we start with the decomposition
Av|⇤|=` µ [Ent⇤f] = Ent(f) −Av|U|=n−` Ent
⇥
µV \Uf
⇤
,
which follows from Lemma 2.3. Therefore Lemma 5.3 implies that (5.2) holds with
C =
`
n n−` . Using (5.20) we see that
`
n n−`
(n −1) · · · (n −R)
(` −1) · · · (` −R) .
In particular, if ` = d✓ne with ✓2 (0, 1] ﬁxed, then for all sufﬁciently large n one has
`
n n−` ( 1
✓)O(R). This ends the proof of Theorem 5.1.
EJP 27 (2022), paper 142.
Page 35/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
6
Optimal mixing of the SW dynamics
In this section, we show that for ferromagnetic Potts models, the k-partite factor-
ization of entropy, as deﬁned in (1.7), implies optimal mixing of the Swendsen-Wang
(SW) dynamics. Since we have already established that, for any spin system, k-partite
factorization is implied by spectral independence, we then deduce Theorem 1.8 from the
introduction.
We again take G = (V, E) to be an n-vertex graph of maximum degree ∆and µ to be
the Potts distribution on G with conﬁguration space ⌦= [q]V . The SW dynamics takes
a spin conﬁguration, transforms it into a “joint” spin-edge conﬁguration, performs a
step in the joint space, and then drops the edges to obtain a new Potts conﬁguration.
Formally, from a Potts conﬁguration σt 2 [q]V , a transition σt ! σt+1 of the SW dynamics
is deﬁned as follows:
1. Let Mt = M(σt) denote the set of monochromatic edges in σt.
2. Independently for each edge e 2 Mt, keep e with probability p = 1 −exp(−β) and
remove e with probability 1 −p. Let At ⇢Mt denote the resulting subset.
3. In the subgraph (V, At), independently for each connected component C (including
isolated vertices), choose a spin sC uniformly at random from [q] and assign to each
vertex in C the spin sC. This spin assignment deﬁnes σt+1.
It will be useful for us to consider the “joint” Edwards-Sokal distribution for G with
parameters p 2 [0, 1] and integer q ≥2. Let ⌦j = ⌦⇥{0, 1}E be the set of “joint” spin-edge
conﬁgurations (σ, A) consisting of a spin assignment to the vertices σ 2 ⌦and a subset
of edges A ⇢E. The Edwards-Sokal measure assigns to each (σ, A) 2 ⌦j a probability
given by
⌫(σ, A) = 1
Zj
p|A|(1 −p)|E|−|A|1(σ ⇠A),
(6.1)
where σ ⇠A means that A ⇢M(σ) (i.e., every edge in A is monochromatic in σ) and
Zj is the corresponding normalizing constant or partition function. When p = 1 −e−β,
the “spin marginal” of ⌫is precisely the Potts distribution µ and ZG = Zj, and the “edge
marginal” of ⌫corresponds to the random-cluster measure; see, e.g., [FK72, Gri06] for
extensive background on these measures.
Before stating our results, we stipulate some notation. We write
Ent⌫(f) = ⌫[f log(f/⌫(f))]
for the entropy of the function f : ⌦j ! R+ with respect to ⌫. For a ﬁxed conﬁguration
σ 2 ⌦and subset of edges A ⇢E, Ent⌫(f | σ) and Ent⌫(f | A) denote the entropy of f with
respect to the conditional measures ⌫(· | σ) and ⌫(· | A), respectively. More precisely, for a
given σ 2 ⌦, ⌫(· | σ) is the measure ⌫conditioned on the event that the spin conﬁguration
is equal to σ, and for a given A ⇢E, ⌫(· | A) is the measure ⌫conditioned on the event that
the edge conﬁguration is equal to A. In this way, Ent⌫(f | σ) and Ent⌫(f | A) are functions
of σ and A, respectively, and ⌫[Ent⌫(f | σ)], ⌫[Ent⌫(f | A)] denote the corresponding
expectations with respect to ⌫. The main result in this section is stated as follows.
Theorem 6.1. Suppose µ satisﬁes the k-partite factorization of entropy with constant
Cpar; see Eq. (1.7). Then, there exists a constant C = C(Cpar, β, ∆) such that for all
f : ⌦j 7! R+
Ent⌫(f) C (⌫[Ent⌫(f | σ)] + ⌫[Ent⌫(f | A)]) .
(6.2)
The constant C satisﬁes C = Cpar ⇥O(β∆2eβ∆).
EJP 27 (2022), paper 142.
Page 36/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
We call (6.2) the spin/edge factorization of entropy with constant C for the joint
measure ⌫. The main motivation for this inequality is the result established in [BCP+21,
Lemma 1.8] that on any n-vertex graph, spin/edge factorization with constant C implies
that the SW dynamics has discrete time entropy decay with rate δ = 1/C, and therefore,
by Lemma 2.2, satisﬁes Tmix = O(log n). Theorem 1.8 from the introduction now follows
immediately.
Proof of Theorem 1.8. For the Potts model one has eβ∆= O(1/b). Therefore, the results
follows from Theorem 5.1, Lemma 1.17, Theorem 6.1 and [BCP+21, Lemma 1.8].
Let {V1, ..., Vk} be the k-partition of G, where k ∆+ 1, as in Section 3. For all j 2 [k]
let ⌫(· | σV c
j , A) denote the measure ⌫conditioned on σV c
j = {σv, v /2 Vj} and A ⇢E. We
use Ent⌫(f | σV c
j , A) for the corresponding conditional entropy and ⌫
h
Ent⌫(f | σV c
j , A)
i
for its expectation with respect to ⌫. Theorem 6.1 will follow from the following lemmas.
Lemma 6.2. For all f : ⌦j 7! R+ and all j 2 [k] we have
⌫[Ent⌫(f | A)] ≥⌫
h
Ent⌫(f | σV c
j , A)
i
.
Lemma 6.3. There exists a constant δ1 > 0 such that, for all f : ⌦j 7! R+ and all j 2 [k],
⌫[Ent⌫(f | σ)] + ⌫
h
Ent⌫(f | σV c
j , A)
i
≥δ1 ⌫
h
Ent⌫(f | σV c
j )
i
.
The constant δ1 satisﬁes 1/δ1 = O(β∆eβ∆).
Lemma 6.4. If µ satisﬁes the k-partite factorization with constant Cpar, then for all
f : ⌦j 7! R+,
k
X
j=1
⌫
h
Ent⌫(f | σV c
j )
i
≥δ2Ent⌫(f),
where δ2 =
1
Cpar .
Proof of Theorem 6.1. By combining the bounds from Lemmas 6.2, 6.3 and 6.4 we get
⌫[Ent⌫(f | σ) + Ent⌫(f | A)] ≥δ1δ2
k Ent⌫(f),
(6.3)
and so, using also k ∆+ 1, the spin/edge factorization holds with constant
C =
k
δ1δ2
= Cpar ⇥O(β∆2eβ∆).
We turn to the proof of Lemmas 6.2, 6.3 and 6.4. In the special case of bipartite
graphs these correspond to Lemmas 4.3, 4.4 and 4.5 in [BCP+21], respectively. For
Lemmas 6.2 and 6.4 the adaptation to our setting is straightforward. The proof of
Lemma 6.3, the core of the argument, requires some modiﬁcation. The main difference
with the proof in [BCP+21] is in the deﬁnition of the measures ⌫x(· | σV c
j ) below, since in
the bipartite case one only needs to consider the measures ⌫x(· | σV c
j ) for x 2 Vj while
here one needs to deﬁne ⌫x(· | σV c
j ) for all x 2 V . Once this is taken care of, however, the
proof proceeds essentially in the same way.
Proof of Lemma 6.2. This is an instance of the same monotonicity already seen in
Lemma 2.4. In this particular case, it follows from the argument in the proof of Lemma
4.3 in [BCP+21] by simply substituting σO with σV c
j in that proof.
EJP 27 (2022), paper 142.
Page 37/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
Proof of Lemma 6.3. Let us ﬁx j 2 [k], and an arbitrary ordering of the independent sets
V1, . . . , Vk, such that Vj is the lowest independent set, that is Vj < Vi for all i 6= j. We use
xy to denote the edge {x, y}, and view the edge conﬁguration A as a vector in {0, 1}E.
Clearly, if xy 2 E then x 2 Vi and y 2 V` for some i 6= `. For any x 2 V we write N(x)
for the set of neighbors of x which belong to a higher independent set, that is if x 2 Vi
then y 2 N(x) iff xy 2 E and y 2 V` for some V` > Vi. Note that, since Vj is the lowest
independent set, if x 2 Vj then N(x) coincides with the set of all neighbors of x. The
main observation here is that, by deﬁnition of the measure ⌫, for any ﬁxed conﬁguration
σV c
j of spins on V c
j , the conditional probability ⌫(· | σV c
j ) is a product measure
⌫(· | σV c
j ) =
O
x2V
⌫x(· | σV c
j ),
(6.4)
where the single measures ⌫x(· | σV c
j ), x 2 V , are described as follows. For each x 2 Vj,
⌫x(· | σV c
j ) is the law on {1, . . . , q} ⇥{0, 1}N(x) obtained by picking the spin of site x
according to the Potts measure on x conditioned on the spin of its neighbors in V c
j and
then, independently for every y 2 N(x) with σx = σy by taking Axy a Bernoulli(p) random
variable, and for every y 2 N(x) with σx 6= σy by setting Axy = 0. For x 2 V c
j instead, the
single measure ⌫x(· | σV c
j ) is the law on {1, . . . , q} ⇥{0, 1}N(x) obtained by taking a Dirac
mass on {1, . . . , q} according to the assigned spin value σx, and such that independently
for every y 2 N(x) with σx = σy, Axy is a Bernoulli(p) random variable, and for every
y 2 N(x) with σx 6= σy one has Axy = 0. Note that, by construction, if x 2 V c
j then the
spins σx, σy, for y 2 N(x), are all assigned once we condition on σV c
j .
The measure ⌫(· | σV c
j , A), obtained by further conditioning on a valid conﬁguration of
all edge variables A compatible with the ﬁxed spins σV c
j , is again a product measure:
⌫(· | σV c
j , A) =
O
x2V
⌫x(· | σV c
j , A),
(6.5)
where ⌫x(· | σV c
j , A) is deﬁned as follows. If x 2 Vj, ⌫x(· | σV c
j , A) is the probability measure
on {1, . . . , q} ⇥{0, 1}N(x) that is uniform in the spin variable if x has no incident edges
in A, and is concentrated on the unique admissible value given σV c
j and A otherwise,
and it is a Dirac mass in the edge variables according to A. If x 2 V c
j , ⌫x(· | σV c
j , A) is a
Dirac mass on {1, . . . , q} ⇥{0, 1}N(x) according to the assigned spin value σx and edge
variables A.
Next, we note that ⌫(· | σ) is a product of Bernoulli(p) random variables over all
monochromatic edges in σ, while it is concentrated on Axy = 0 on all remaining edges.
Therefore we may write
⌫(· | σ) =
O
x2V
⌫x(· | σ),
(6.6)
where ⌫x(· | σ) is the probability measure on {1, . . . , q} ⇥{0, 1}N(x) given by a Dirac mass
at the assigned spin σx and the product of Bernoulli(p) variables on all edges xy such
that y 2 N(x) and σx = σy, and a Dirac mass at Axy = 0 if y 2 N(x) and σx 6= σy.
We write Entx(· | σV c
j ), Entx(· | σV c
j , A), Entx(· | σ) for the entropies with respect to the
distributions ⌫x(· | σV c
j ), ⌫x(· | σV c
j , A), ⌫x(· | σ) respectively. The next key observation is
that, for every site x, there is a local factorization of entropies in the following sense.
There exists a constant δ1 2 (0, 1] such that 1/δ1 = O(β∆eβ∆), and such that for all
functions f ≥0 and all σ and x 2 V ,
⌫x
h
Entx(f | σ) | σV c
j
i
+ ⌫x
h
Entx(f | σV c
j , A) | σV c
j
i
≥δ1 Entx(f | σV c
j ).
(6.7)
EJP 27 (2022), paper 142.
Page 38/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
In the case x 2 Vj this follows exactly as in Lemma 4.7 from [BCP+21] for bipartite
graphs and is thus omitted.
If instead x 2 V c
j then, recalling that by construction
⌫x(· | σV c
j ) is a Dirac mass on the spin value at x and a product measure on the edge
variables at xy, y 2 N(x), one has ⌫x(·|σV c
j ) = ⌫x(·|σ) and therefore
Entx(f | σV c
j ) = Entx(f | σ) = ⌫x
h
Entx(f | σ) | σV c
j
i
,
and thus δ1 can be taken to be 1 in this case.
Next, we would like to lift inequality (6.7) to the product measure ⌫(· | σV c
j ) =
⌦x2V ⌫x(· | σV c
j ). Let x = 1, . . . , n denote an arbitrary ordering of the sites x 2 V . For
all x 2 V we let Ax 2 {0, 1}N(x) be the random variable corresponding to the state
of the edges xy such that y 2 N(x). We write ⇠x = (σx, Ax) for the pair of variables
corresponding to any x 2 V . Note that, under the conditional distribution ⌫(· | σV c
j ), the
random variables ⇠x, x 2 V , are independent. Thus, we may write
Ent⌫(f | σV c
j ) =
n
X
x=1
⌫
h
Entx(gx−1 | σV c
j ) | σV c
j
i
,
(6.8)
where gx = ⌫
h
f | σV c
j , ⇠x+1, . . . , ⇠n
i
, g0 = f and gn = ⌫
h
f | σV c
j
i
.
This identity is an
instance of the decomposition in Lemma 2.3.
Putting together (6.7) and (6.8) yields
δ1 Ent⌫(f | σV c
j ) 
n
X
x=1
⌫
h
⌫x
h
Entx(gx−1 | σ) | σV c
j
i
+ ⌫x
h
Entx(gx−1 | σV c
j , A) | σV c
j
i
| σV c
j
i
=
n
X
x=1
⌫
h
Entx(gx−1 | σ) + Entx(gx−1 | σV c
j , A) | σV c
j
i
.
(6.9)
To conclude the proof we can now proceed exactly as in the proof of Lemma 4.8 from
[BCP+21]. We obtain the following two inequalities:
n
X
x=1
⌫
h
Entx(gx−1 | σ) | σV c
j
i
⌫
h
Ent⌫(f | σ) | σV c
j
i
,
n
X
x=1
⌫
h
Entx(gx−1 | σV c
j , A) | σV c
j
i
⌫
h
Ent⌫(f | σV c
j , A) | σV c
j
i
.
These two inequalities combined with (6.9) yield that
δ1 Ent⌫(f | σV c
j ) ⌫
h
Ent⌫(f | σ) | σV c
j
i
+ ⌫
h
Ent⌫(f | σV c
j , A) | σV c
j
i
.
(6.10)
The desired result follows by taking expectations with respect to ⌫in (6.10).
Proof of lemma 6.4. From the deﬁnition of conditional entropy as well as the fact that
⌫(· | σVj, σV c
j ) = ⌫(· | σ), we get
Ent⌫(f | σV c
j ) = Ent⌫
⇣
⌫[f | σ] | σV c
j
⌘
+ ⌫
h
Ent⌫(f | σ) | σV c
j
i
.
(6.11)
(see eq. (4.5), (4.6) from Lemma 4.5 in [BCP+21]). Now, since the function ⌫[f | σ]
depends only on the spin conﬁguration σ, one has the identity
k
X
j=1
⌫
h
Ent⌫(⌫[f | σ] | σV c
j )
i
=
k
X
j=1
µ
h
Ent(⌫[f | σ] | σV c
j )
i
,
(6.12)
EJP 27 (2022), paper 142.
Page 39/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
where the entropy in the right hand side is with respect to µ and not with respect to ⌫.
Since k-partite factorization holds by assumption,
k
X
j=1
µ
h
Ent(⌫[f | σ] | σV c
j )
i
≥δ2 Ent (⌫[f | σ]) ,
(6.13)
where δ2 = 1/Cpar. By taking functions depending only on σVj for a single Vj one easily
sees that Cpar must be at least 1. Then, taking expectation and summing over j in (6.11),
and combining with (6.12) and (6.13), we get
k
X
j=1
⌫
h
Ent⌫(f | σV c
j )
i
≥δ2 Ent⌫(⌫[f | σ]) + k ⌫[Ent⌫(f | σ)] .
Using the simple decomposition Ent⌫(f) = Ent⌫(⌫[f | σ]) + ⌫[Ent⌫(f | σ)], and the fact
that δ2 1 k, we conclude that
k
X
j=1
⌫
h
Ent⌫(f | σV c
j )
i
≥δ2 Ent⌫(f).
References
[Ale13] A. Alexanderian. On continuous dependence of roots of polynomials on coefﬁcients. Tech-
nical notes, 2013.
[ALO20] N. Anari, K. Liu, and S. Oveis Gharan. Spectral independence in high-dimensional ex-
panders and applications to the hardcore model. In Proceedings of the 61st Annual IEEE Sym-
posium on Foundations of Computer Science (FOCS), pages 1319–1330, 2020. MR4232133
[BCLM11] F. Barthe, D. Cordero-Erausquin, M. Ledoux, and B. Maurey. Correlation and Brascamp-
Lieb inequalities for Markov semigroups. International Mathematics Research Notices,
10:2177–2216, 2011. MR2806562
[BCP+21] A. Blanca, P. Caputo, D. Parisi, A. Sinclair, and E. Vigoda. Entropy decay in the Swendsen-
Wang dynamics. In Proceedings of the 53rd Annual ACM Symposium on Theory of Computing
(STOC), 2021. MR4398940
[BCV20] A. Blanca, Z. Chen, and E. Vigoda. Swendsen-Wang dynamics for general graphs in the
tree uniqueness region. Random Structures & Algorithms, 56(2):373–400, 2020. MR4060350
[BD97] R. Bubley and M. E. Dyer. Path coupling: a technique for proving rapid mixing in Markov
chains. In Proceedings of the 38th Annual IEEE Symposium on Foundations of Computer
Science (FOCS), pages 223–231, 1997.
[BGP16] M. Bordewich, C. Greenhill, and V. Patel. Mixing of the Glauber dynamics for the ferro-
magnetic Potts model. Random Structures & Algorithms, 48(1):21–52, 2016. MR3432570
[BN19] G. Bresler and D. Nagaraj. Stein’s method for stationary distributions of Markov chains
and application to Ising models. The Annals of Applied Probability, 29(5):3230–3265, 2019.
MR4019887
[BT06] S. Bobkov and P. Tetali. Modiﬁed logarithmic Sobolev inequalities in discrete settings.
Journal of Theoretical Probability, 19(2):289–336, 2006. MR2283379
[Cap04] P. Caputo. Spectral gap inequalities in product spaces with conservation laws. Stochastic
analysis on large scale interacting systems, 39:53–88, 2004. MR2073330
[CC09] E. A. Carlen and D. Cordero-Erasquin. Subadditivity of the entropy and its relation to
Brascamp-Lieb type inequalities. Geometric and Functional Analysis, 19:373–405, 2009.
MR2545242
[CCL03] E. A. Carlen, M. C. Carvalho, and M. Loss. Determination of the spectral gap for Kac’s mas-
ter equation and related stochastic evolution. Acta Mathematica, 191:1–54, 2003. MR2020418
EJP 27 (2022), paper 142.
Page 40/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
[CDM+19] S. Chen, M. Delcourt, A. Moitra, G. Perarnau, and L. Postle. Improved bounds for
randomly sampling colorings via linear programming. In Proceedings of the 30th Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2216–2234, 2019. MR3909603
[CDPP09] P. Caputo, P. Dai Pra, and G. Posta. Convex entropy decay via the Bochner-Bakry-Emery
approach. In Annales de l’IHP Probabilités et statistiques, volume 45, pages 734–753, 2009.
MR2548501
[Ces01] F. Cesi. Quasi-factorization of the entropy and logarithmic Sobolev inequalities for Gibbs
random ﬁelds. Probability Theory and Related Fields, 120(4):569–584, 2001. MR1853483
[CGM21] M. Cryan, H. Guo, and G. Mousa. Modiﬁed log-Sobolev inequalities for strongly log-
concave distributions. The Annals of Probability, 49(1):506–525, 2021. MR4203344
[CGvV21] Z. Chen, A. Galanis, D. ätefankoviˇc, and E. Vigoda. Rapid mixing for colorings via
spectral independence. In Proceedings of the 32nd Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 1548–1557, 2021. MR4262527
[CHV20] E. Csóka, V. Harangi, and B. Virág. Entropy and expansion. In Annales de l’Institut Henri
Poincaré, Probabilités et Statistiques, volume 56, pages 2428–2444. Institut Henri Poincaré,
2020. MR4164843
[CLL04] E. A. Carlen, E. H. Lieb, and M. Loss. A sharp analog of Young’s inequality on SN
and related entropy inequalities. The Journal of Geometric Analysis, 14:487–520, 2004.
MR2077162
[CLV20] Z. Chen, K. Liu, and E. Vigoda. Rapid mixing of Glauber dynamics up to uniqueness via
contraction. In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer
Science (FOCS), pages 1307–1318, 2020. MR4232132
[CLV21] Z. Chen, K. Liu, and E. Vigoda. Optimal mixing of Glauber dynamics: Entropy factorization
via high-dimensional expansion. In Proceedings of the 53rd Annual ACM Symposium on
Theory of Computing (STOC), 2021. MR4398939
[CM03] P. Caputo and F. Martinelli. Relaxation time of anisotropic simple exclusion processes
and quantum Heisenberg models. The Annals of Applied Probability, 13(2):691–721, 2003.
MR1970283
[CMT15] P. Caputo, G. Menz, and P. Tetali. Approximate tensorization of entropy at high tempera-
ture. Annales de la Faculté des sciences de Toulouse: Mathématiques, 24(4):691–716, 2015.
MR3434252
[Con22] G. Conforti. A probabilistic approach to convex (')-entropy decay for Markov chains. The
Annals of Applied Probability, 32(2):932–973, 2022. MR4414692
[CP21] P. Caputo and D. Parisi. Block factorization of the relative entropy via spatial mixing.
Communications in Mathematical Physics, 388:793–818, 2021. MR4334247
[DGJ09] M. Dyer, L. A. Goldberg, and M. Jerrum. Matrix norms and rapid mixing for spin systems.
The Annals of Applied Probability, 19(1):71–107, 2009. MR2498672
[DPPP02] P. Dai Pra, A. M. Paganoni, and G. Posta. Entropy inequalities for unbounded spin
systems. The Annals of Probability, 30(4):1959–1976, 2002. MR1944012
[DS96] P. Diaconis and L. Saloff-Coste. Logarithmic Sobolev inequalities for ﬁnite Markov chains.
The Annals of Applied Probability, 9(3):695–750, 1996. MR1410112
[EHMT17] M. Erbar, C. Henderson, G. Menz, and P. Tetali. Ricci curvature bounds for weakly
interacting Markov chains. Electronic Journal of Probability, 22:1–23, 2017. MR3646066
[ELL17] R. Eldan, J. R. Lee, and J. Lehec. Transport-entropy inequalities and curvature in discrete-
space Markov chains. In A Journey Through Discrete Mathematics, pages 391–406. 2017.
MR3726607
[ES88] R. G. Edwards and A. D. Sokal. Generalization of the Fortuin-Kasteleyn-Swendsen-Wang
representation and Monte Carlo algorithm. Physical review D, 38(6):2009, 1988. MR0965465
[FGYZ21] W. Feng, H. Guo, Y. Yin, and C. Zhang. Rapid mixing from spectral independence beyond
the Boolean domain. In Proceedings of the 32nd Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 1558–1577, 2021. MR4262528
[FK72] C. M. Fortuin and P. W. Kasteleyn. On the random-cluster model I. Introduction and relation
to other models. Physica, 57(4):536–564, 1972. MR0359655
EJP 27 (2022), paper 142.
Page 41/42
https://www.imstat.org/ejp

Coupling, spectral independence, and entropy factorization
[Gri06] G. R. Grimmett. The Random-Cluster Model, volume 333. Springer-Verlag, Berlin, 2006.
MR2243761
[GSS19] F. Götze, H. Sambale, and A. Sinulis. Higher order concentration for functions of weakly
dependent random variables. Electronic Journal of Probability, 24, 2019. MR4003138
[Häg96] O. Häggström. The random-cluster model on a homogeneous tree. Probability Theory and
Related Fields, 104(2):231–253, 1996. MR1373377
[Hay06] T. P. Hayes. A simple condition implying rapid mixing of single-site dynamics on spin
systems. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer
Science (FOCS), pages 39–46, 2006.
[HS07] T. P. Hayes and A. Sinclair. A general lower bound for mixing of single-site dynamics on
graphs. Annals of Applied Probability, 17(3):931–952, 2007. MR2326236
[HS19] J. Hermon and J. Salez. Modiﬁed log-Sobolev inequalities for strong-Rayleigh measures.
Preprint, arXiv:1902.02775, 2019.
[Jer95] M. Jerrum. A very simple algorithm for estimating the number of k-colorings of a low-
degree graph. Random Structures & Algorithms, 7(2):157–165, 1995. MR1369061
[Led99] M. Ledoux. Concentration of measure and logarithmic Sobolev inequalities. Séminaire de
Probabilités XXXIII, pages 120–216, 1999. MR1767995
[Liu21] K. Liu. From Coupling to Spectral Independence and Blackbox Comparison with the
Down-Up Walk. In Proceedings of APPROX-RANDOM, 2021. MR4366587
[LP17] D. A. Levin and Y. Peres. Markov chains and mixing times. American Mathematical Society,
2017. MR3726904
[Mar99] F. Martinelli. Lectures on Glauber dynamics for discrete spin models. In Lectures on
probability theory and statistics, pages 93–191. Springer, 1999. MR1746301
[Mar19] K. Marton. Logarithmic Sobolev inequalities in discrete product spaces. Combinatorics,
Probability & Computing, 28(6):919–935, 2019. MR4015662
[Mey00] C. D. Meyer. Matrix analysis and applied linear algebra, volume 71. SIAM, 2000.
MR1777382
[MO94] F. Martinelli and E. Olivieri. Approach to equilibrium of Glauber dynamics in the one phase
region. I. Communications in Mathematical Physics, 161(3):447–486, 1994. MR1269387
[MS13] E. Mossel and A. Sly. Exact thresholds for Ising–Gibbs samplers on general graphs. The
Annals of Probability, 41(1):294–328, 2013. MR3059200
[Oll09] Y. Ollivier. Ricci curvature of Markov chains on metric spaces. Journal of Functional
Analysis, 256(3):810–864, 2009. MR2484937
[RR19] G. Reinert and N. Ross. Approximating stationary distributions of fast mixing Glauber
dynamics, with applications to exponential random graphs. The Annals of Applied Probability,
29(5):3201–3229, 2019. MR4019886
[Sal21] J. Salez. Cutoff for non-negatively curved Markov chains. Preprint, arXiv:2102.05597,
2021.
[SS20] H. Sambale and A. Sinulis. Logarithmic Sobolev inequalities for ﬁnite spin systems and
applications. Bernoulli, 26(3):1863–1890, 2020. MR4091094
[SW87] R. H. Swendsen and J. S. Wang. Nonuniversal critical dynamics in Monte Carlo simulations.
Physical Review Letters, 58:86–88, 1987.
[SZ92] D. W. Stroock and B. Zegarlinski. The equivalence of the logarithmic Sobolev inequality
and the Dobrushin-Shlosman mixing condition. Communications in Mathematical Physics,
144(2):303–323, 1992. MR1152374
[Ull14] M. Ullrich. Rapid mixing of Swendsen-Wang dynamics in two dimensions. Dissertationes
Mathematicae, 502:1–65, 2014. MR3222829
[Vig00] E. Vigoda. Improved bounds for sampling colorings. Journal of Mathematical Physics,
41(3):1555–1569, 2000. MR1757969
[Vil21] C. Villani. Topics in optimal transportation, volume 58. American Mathematical Society,
2021. MR1964483
EJP 27 (2022), paper 142.
Page 42/42
https://www.imstat.org/ejp

