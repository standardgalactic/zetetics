Navigating the Grey Area:
Expressions of Overconﬁdence and Uncertainty in Language Models
Kaitlyn Zhou
Stanford University
katezhou@stanford.edu
Dan Jurafsky
Stanford University
jurafsky@stanford.edu
Tatsunori Hashimoto
Stanford University
thashim@stanford.edu
Abstract
Despite increasingly ﬂuent, relevant, and co-
herent language generation, major gaps remain
between how humans and machines use lan-
guage. We argue that a key dimension that is
missing from our understanding of language
models (LMs) is the model’s ability to inter-
pret and generate expressions of uncertainty.
Whether it be the weatherperson announcing
a chance of rain or a doctor giving a diagno-
sis, information is often not black-and-white
and expressions of uncertainty provide nuance
to support human-decision making.
The in-
creasing deployment of LMs in the wild mo-
tivates us to investigate whether LMs are capa-
ble of interpreting expressions of uncertainty
and how LMs’ behaviors change when learn-
ing to emit their own expressions of uncer-
tainty. When injecting expressions of uncer-
tainty into prompts (e.g., "I think the answer
is..."), we discover that GPT3’s generations
vary upwards of 80% in accuracy based on
the expression used. We analyze the linguistic
characteristics of these expressions and ﬁnd a
drop in accuracy when naturalistic expressions
of certainty are present. We ﬁnd similar ef-
fects when teaching models to emit their own
expressions of uncertainty, where model cali-
bration suffers when teaching models to emit
certainty rather than uncertainty.
Together,
these results highlight the challenges of build-
ing LMs that interpret and generate trustwor-
thy expressions of uncertainty.
1
Introduction
As natural language systems are increasingly used
in real-life scenarios, it is becoming important to
not only produce ﬂuent and correct answers but
also to properly communicate uncertainties. From
weather reports to doctors’ ofﬁces, humans are con-
stantly integrating expressions of uncertainty into
decision-making processes. Expressions of uncer-
tainty inform us of the conﬁdence, source, and
limitations of information, helping us make big
and small decisions like bringing an umbrella or
starting a course of chemotherapy. Despite substan-
tial literature on methods for quantifying statistical
uncertainty, there has been comparatively less atten-
tion on how such linguistic uncertainties might in-
teract with the natural language generation system,
resulting in a lack of understanding of this critical
component of how models interact with natural lan-
guage. Our work makes progress in answering this
question by asking: are language models (LMs)
capable of interpreting expressions of uncertainty
and how do LMs’ behaviors change when trained
to emit their own expressions of uncertainty?
Naturalistic expressions of uncertainty cover a
broad range of discourse acts such as signaling hes-
itancy, attributing information, or acknowledging
limitations. While prior work has explored lin-
guistically calibrating model generations, this has
primarily focused on learning the mapping between
the internal probabilities of a model and a verbal
or numerical ordinal output (Kadavath et al., 2022;
Lin et al., 2022; Mielke et al., 2022). Our work, by
contrast, seeks to understand and incorporate non-
uni-dimensional linguistic features such as hedges
or epistemic markers (e.g., It could be ...), factive
verbs (e.g., We realize it’s...), and evidential mark-
ers (e.g., Wikipedia says it’s...) — building our
understanding of how these additional properties
and evidentiality impact natural language genera-
tion.
We begin with an investigation of how LMs in-
terpret uncertainty in prompts, followed by a small
study on how LMs behave when generating their
own uncertainties via in-context learning. We focus
on studying this question in the question-answering
(QA) setting, due to its real-world relevance and
existing benchmarks. The ﬁrst experiments are
conducted in a zero-shot setting, making it possible
to analyze and isolate the effects of uncertainty in
prompting, while the second experiments exam-
ine how learning to express uncertainty impacts
arXiv:2302.13439v1  [cs.CL]  26 Feb 2023

generation in QA tasks.
In both sets of experiments, we ﬁnd shortcom-
ings when expressions of high certainty are used,
both in accuracy and calibration.
In zero-shot
prompting, we ﬁnd that there are systematic losses
in accuracy when expressions of certainty are used
to strengthen prepositions (i.e., "We’re 100% cer-
tain..."). In the second in-context learning scenario,
we ﬁnd that teaching the model to emit weakeners
rather than strengtheners results in better calibra-
tion without sacriﬁcing accuracy. We then discuss
designing linguistically calibrated models, espe-
cially given the potential downfalls of models emit-
ting highly certain language.
Our work offers four key contributions:
• We provide a framework and carry out analy-
sis on how expressions of uncertainty interact
with large language models.
• We introduce a typology of expressions of
uncertainty to evaluate how linguistic features
impact LM generation.
• We demonstrate how model accuracy suffers
when models use expressions of certainty
(e.g., factive verbs) or idiomatic language
(e.g., ‘I’m 100% certain’)
• Our results in in-context learning suggest that
GPT3 struggles to emit expressions of cer-
tainty in a calibrated manner but that expres-
sions of uncertainty might lead to better cali-
bration.
Together, our ﬁndings illustrate the need for
model analysis through the lens of uncertainty,
especially as LMs become deployed in real-life
decision-making settings.
2
Expressions of Certainty and
Uncertainty: Linguistic Background
There is a broad literature on linguistic markers
that relate to certainty and uncertainty, both for
weakening category membership and weakening
speaker commitment to the truth value of a propo-
sition (see Related Work). For convenience in this
paper we broadly group these linguistic devices
into weakeners and strengtheners.
The most widely studied weakeners are hedges,
ﬁrst deﬁned by Lakoff (1975) as related to mod-
ifying or weakening the category membership of
a predicate or nominal. The most central kind of
hedges are approximators (e.g., somewhat, kind
Strengtheners
(Certainty)
Weakeners
(Uncertainty)
Plausibility Shields
“I think it’s...”
Approximators
“Around ...”
Factive Verbs
“He realized it’s...”
“I’m certain it’s...”
“They acknowledged it’s...”
“Allegedly, it’s...”
Evidentials
Figure 1: Lexical Features of Expressions of Uncer-
tainty. Uncertainty classiﬁcation partly adapted from
(Prince et al., 1982). Certainty markers are strength-
eners which contain factive verbs. Evidential markers
can be both expressions of certainty and uncertainty
(strengtheners or weakeners). Personal pronouns and
reference to sources are additional dimensions of ex-
pressions of (un)certainty not shown in this diagram.
of, about, approximately), which hedge proposi-
tional content. Another class of weakeners that
some (like Prince et al. (1982)) but not others clas-
sify under hedges are plausibility shields (Prince
et al., 1982) which express a speaker’s lower level
of commitment (e.g., I think, I believe).
Strengtheners are liguistic constructions that
mark certainty. We use the term strengtheners both
to refer to strengthening speaker commitment to
truth value, and to strengthening category mem-
bership. Strengtheners include boosters or intensi-
ﬁers like "I am certain" or "Undoubtedly" (Hyland,
2005, 2014).
Whereas boosters can assert certainty or truth,
a second kind of strengthening construction, the
factive verb, is used to presuppose certainty or
truth. Factive verbs like "know", "realize", or "un-
derstand" (Kiparsky and Kiparsky, 1970) presup-
pose the truth of the complement sentence. The
statement “X realizes Y" presupposes that Y is true,
(and also asserts that X is aware of it). By contrast,
“X believes Y" makes no presupposition about the
truth of Y. Thus the sentence “He realizes [Madrid
is the capital of France]" is infelicitous, because
“realize" as a factive verb presupposes the truth of
its complement clause, which in this case is false
(Madrid is not the capital of France).
We note the existence of a third class of mark-
ers that can be used to mark both certainty and
uncertainty by directly indicating the source of
the information. Such expressions (like Accord-
ing to Wikipedia, or I heard or I saw) are called
evidential marker, linguistic signals that tells the
hearer where the information came from (Aikhen-

vald, 2004). One subtype of evidential markers,
quotative markers, are used when reported infor-
mation overtly references a source (e.g., According
to research in the latest issue of Nature, Two re-
cent studies demonstrate that...). Speciﬁcally, we
examine when references are citing a source (e.g.,
Wikipedia says) versus when the source is unspeci-
ﬁed or very indirect (e.g., They said). We’ll refer
to this former case as sourced as a shorthand for
indicating that a source is mentioned.
Finally, ﬁrst-person personal pronouns (e.g.,
I, we, our) can be used to mark subjectivity and
uncertainty in expressions like "I think".
2.1
Expressions of Uncertainty Typology
We compiled a list of ﬁfty expressions of verbal un-
certainty from both crowd-workers and the authors,
focusing on the use of weakeners, strengtheners,
plausibility shields, factives, evidential markers,
mentions of sources, and personal pronouns. We
then coded each expression with the linguistic fea-
tures above. The ﬁnal list (Table B) allows us to
systematically analyze how expressions of uncer-
tainty impact language modeling in the QA setting.
Figure 1 shows a diagram of how each of these
features relates to certainty and uncertainty in our
coding scheme (details in Appendix B).
3
Methods
Our work studies LM expressions of uncertainty in
the context of open-ended question answering. In
both of our settings, we insert linguistic expressions
of uncertainty into a QA question through hand-
crafted templates (Figure 2), and query the LM
using this modiﬁed question answering prompt.
In the ﬁrst setting, we use zero-shot promoting
with expressions of uncertainty to elicit answers.
We inject verbal uncertainties into trivia questions,
converting a prompt like "What is the capital of
France?" to, "What is the capital of France, I think
it’s...".
In the second setting, we create an in-context
learning prompt with nearly ﬁfty samples that con-
tain question-answer pairs, along with an expres-
sion of (un)certainty for any (un)certain examples.
For example, if the model has high conﬁdence in an
answer, the sample might appear in the in-context
learning prompt as, "What’s the capital of France?
Paris. I’m sure". And vice versa for low-conﬁdence
answers.
Q: What is the 
capital of France?
A: I think it’s...
“Paris.”
PROMPT
GENERATION
Q: What is the 
capital of France?
A: I’m 90% 
certain it’s...
Q: What is the 
capital of Italy?
A: Rome. I’m sure.
“Paris.”
Q: What is the 
capital of France? 
A: Paris. I’m sure. 
Single prompt 
which contains a 
series of calibrated 
in-context learning 
examples
“I’m sure” is 
appended to 
high confidence 
answers
“London. 
I’m sure.”
Zero-shot Injection of
Expressions of Uncertainty
Multi-shot Prompting to
Emit Expressions of Uncertainty
Verbal
Numerical
Q: What is the 
capital of Albania? 
A: Tirana.
Q: What is the 
capital of the UK?
A:
Figure 2: Two key settings to analyze expressions of un-
certainty. The ﬁrst uses zero-shot promoting and injects
verbal and numerical uncertainties into trivia questions.
The second uses multi-shot in-context learning to teach
models to emit expressions of certainty or uncertainty
(not shown here) in its generation.
3.1
Datasets
We perform our analysis across four question an-
swering datasets: TriviaQA, a standard dataset that
has been used on a variety of QA related tasks
(Joshi et al., 2017); Natural Questions (closed-
book) which were aggregated queries to Google
(Kwiatkowski et al., 2019a); CountryQA, which
we constructed using names of countries and
capitals in the form of "What is the capital of
Afghanistan?"; Jeopardy questions which were
crawled from a fan-created database, J! Archive.1
We treat this as an open-ended question answering
task with a single-token answer by subsampling
the datasets down to questions which have a single
token answer based on GPT3’s vocabulary. This
avoids the potential confounder of generating subto-
kens for OOV answers (which allows us to more
accurately measure the probability placed on the
gold answers).
We use a random subset of 200 questions for
three of our datasets and calculate 95% conﬁdence
intervals using bootstrap resampling. For Coun-
tryQA, there are 53 questions whose answers were
in vocabulary, of which we sample 50. We test each
of our ﬁfty templates across this subset of questions
1https://j-archive.com/

3.2
LM, Prompting, and Evaluation Details
We study OpenAI’s GPT3 model (davinci 175B) as
it is a commonly used large language model with
reasonable statistical calibration properties (Liang
et al., 2022a). For the generated tokens, we take
the sum of the probability assigned to the gold
answer(s) to be the probability-on-gold. When
calculating accuracy, we generate 10 tokens and if
any of the tokens matches the answer, we’ll count
that as a correct generation. This is done to not
unfairly disadvantage templates that are prone to
generating words prior to emitting the answer (e.g.,
"Allegedly, it’s said to be...") (details in A.1).
4
The Impact of Uncertainty on
Language Generation
Information in real-life text is rarely in black-and-
white, and expressions of uncertainty are neces-
sary in supporting decision-making processes. In
this section, we investigate how GPT3’s generation
changes based on the expressions of uncertainty in
its prompt (Figure 2, top) and whether some expres-
sions of uncertainty can have systematic effects on
model behavior.
We begin this section by studying two hypothe-
ses for how LMs might respond to expressions of
uncertainty. The ﬁrst hypothesis is that a model
“knows what it knows” and regardless of the un-
certainty templates used, the answer remains the
same. In other words, the model is robust to adding
expressions of uncertainty in its input. The second
hypothesis is that models will respond differently
based on the uncertainty cues, and ideally in a cali-
brated manner. Under this hypothesis, a conﬁdent
template would be more likely to produce the cor-
rect response than a low conﬁdence template. This
hypothesis would be consistent with prior work
which has shown that LMs can generate language
in the style of diverse personas (Lee et al., 2022;
Park et al., 2022).
4.1
Variation in GPT3 Responses Across
Verbal Uncertainties
We evaluate GPT3 using zero-shot prompting on
our four datasets and ﬁnd that our results do not
support the ﬁrst hypothesis: we ﬁnd that GPT3
is highly sensitive to uncertainty cues, and accu-
racy changes signiﬁcantly depending on the uncer-
tainty expression used. Across our datasets, we
ﬁnd that accuracies can change by up to 80% on
the exact same set of questions. This is especially
pronounced in CountryQA where a template like,
"We realize it’s.." achieves 14% accuracy while
many other templates result in perfect accuracy. In
TriviaQA, when prompted with a template such as
"I’m certain it’s..." the model’s accuracy is 42% but
when prompted with "I would need to double check
but maybe it’s...", accuracy actually increases to
56%. Our ﬁndings illustrate that expressions of
uncertainty affect language generation and that the
changes resulting from these expressions have sub-
stantive impact on overall accuracy.
Returning to our second hypothesis, we seek to
understand if GPT3’s responses to QA answers
are linguistically calibrated based on the expres-
sion of uncertainty used in the prompt. Surpris-
ingly, we ﬁnd that weakeners perform signiﬁcantly
better than strengtheners across all four of our
datasets. The average accuracy among weaken-
ers across all four datasets is 47% compared to
40% among strengtheners. This effect is especially
large in CountryQA where the accuracy gap is 17%.
This effect is driven by the use of factive verbs in
strengtheners (as nearly all uses of factive verbs
in our templates are strengtheners)2, and the use
of factive verbs consistently results in signiﬁcant
losses in accuracy (Figure 3). In other words, when
the template presupposes the truth, accuracy drops.
This ﬁnding contradicts the second hypothesis,
as we might have expected expressions of certainty
to improve performance, not hurt it. This is par-
ticularly concerning as conﬁdent prompts which
intuitively seem like they might result in better gen-
erations actually lead to worse generations.
Furthermore, we ﬁnd that in three of our four
datasets, the use of evidential markers signiﬁcantly
improves performance. In fact, some of the best
performing templates include evidential markers
with a source. The top ten performing prompts for
each dataset are listed in Appendix C.
The results across the other linguistic features
are mixed. Across the four datasets, there is not a
consistent improvement from the use of plausibility
shields, sources, or personal pronouns. (Figure 7).
4.2
A Redistribution of Probability Mass
When Prompted with Weakeners
What explains our surprising ﬁnding that templates
with weakeners outperform templates with strength-
eners? One hypothesis could be that weakeners are
changing the underlying probability distribution of
2The exception being "I vaguely remember it’s...".

CountryQA
Jeopardy
NaturalQA
TriviaQA
Datasets
0.0
0.2
0.4
0.6
0.8
1.0
Accuracy
Accuracy by Factive Verbs
Factive
Not Factive
CountryQA
Jeopardy
NaturalQA
TriviaQA
Datasets
0.0
0.2
0.4
0.6
0.8
Accuracy
Accuracy by Evidential Marker
Evidential
Not Evidential
Figure 3: Signiﬁcant and consistent accuracy losses for templates with factive verbs (left). Use of evidential
markers signiﬁcantly improves accuracy in three out of our four datasets (right). 95% CI calculated using bootstrap
resampling.
the potential answers, placing more weight on the
correct generations.
If weakeners change the answer distribution, we
might expect weakeners to induce an increase in
the probability-on-gold (which is deﬁned as the
sum of the probabilities placed on all of the answer
aliases). We calculate the average probability-on-
gold among all correctly predicted answers and ﬁnd
this is not the case. In fact, the probability-on-gold
from templates with weakeners is slightly lower
than the probability-on-gold from templates with
strengtheners. This is true across three of our four
datasets NaturalQA (42% vs 45%), JeopardyQA
(47% vs 51%), and TriviaQA (53% vs 55%).
Furthermore, we ﬁnd that weakeners led to a ﬂat-
tening of the distribution of probability mass across
answers, compared to strengtheners. We look at
the entropy of the probability distribution of top to-
kens not counting the top prediction; essentially the
uncertainty among all but the top candidate. This
entropy is signiﬁcantly higher among weakeners
than strengtheners (Table 1). Our ﬁnding suggests
that the increase in accuracy of weakeners is not
due to an increase in answer conﬁdence, but rather
when a weakener is used, the model responds by
placing probability more evenly across each of the
remaining possible options.
4.3
Expressions of Uncertainty Compared to
the Standard Prompting Method
Lastly, we ﬁnd that in certain cases, the use of
expressions of uncertainty might actually lead to
better performance than the standard prompting
method (i.e., just simply using "Q: <question> A:").
Dataset
weakeners
strengtheners
TriviaQA
2.980 ± 0.01
2.917 ± 0.01
CountryQA
3.078 ± 0.02
2.875 ± 0.03
Jeopardy
3.170 ± 0.01
3.089 ± 0.01
NaturalQA
3.167 ± 0.01
3.106 ± 0.01
Table 1: Average entropy of the probability distribution
of alternative tokens among weakeners and strengthen-
ers. Across all four datasets, entropy is higher among
weakeners, an indication the model places probability
more evenly across the alternative answers. 95% CI
calculated using standard error.
In TriviaQA and the template "Online says it’s..."
achieves an accuracy of 66% compared to 63%
achieved by the standard method. In Natural Ques-
tions, there are seven templates that outperform
the standard method, six of which are expressions
of uncertainty. Although these results vary across
datasets, we see promising results suggesting that
including uncertainty may not only help human
decision makers, it may also improve the absolute
accuracy of the model.
4.4
The Impact of the Degree of Uncertainty
on Performance
Results from Section 4.1 illustrate that GPT3’s gen-
eration is highly sensitive to uncertainty in prompts,
and certainty seems to lead to diminished accuracy.
Here, we extend these results to study uncertainty
at a more ﬁne-grained level, allowing us to ask if
the degree of uncertainty could play a role in the
accuracy of model generation.

0
20
40
60
80
100
Percent
0.875
0.900
0.925
0.950
0.975
Accuracy
0.0
10.0
30.0
50.0
70.0
90.0
100.0
CountryQA
Mean
0
20
40
60
80
100
Percent
0.45
0.50
0.55
Accuracy
0.0
10.0
30.0
50.0
70.0
90.0
100.0
TriviaQA
Mean
0
20
40
60
80
100
Percent
0.20
0.22
0.24
0.26
0.28
Accuracy
0.0
10.0
30.0
50.0
70.0
90.0
100.0
NaturalQA
Mean
0
20
40
60
80
100
Percent
0.300
0.325
0.350
0.375
0.400
Accuracy
0.0
10.0
30.0
50.0
70.0
90.0
100.0
Jeopardy Questions
Mean
Figure 4: The X-axis indicates the percentage that was injected into the verbal uncertainty. The Y-axis indicates the
accuracy across numerical uncertainties. Note the consistent drop in accuracy between 90% and 100% uncertainty
and the increase in accuracy between 0% and 10% uncertainty.
Introducing Numerical Values
To study the
role of uncertainty at a more ﬁne-grained level
rather than just certain (strengthener/factive) and
uncertain (weakener), we introduce numerical val-
ues into our verbal expressions of uncertainty. The
setup of our task changes from “What is the capital
of Belarus? I’m sure it’s...” to “What is the capital
of Belarus? I’m 90% sure it’s...”. We use a set
of seven expressions covering a range of numer-
ical uncertainty expressions, including those that
use personal pronouns to weaken uncertainty ("I’m
90% certain...") and those which indicate uncer-
tainty probabilistically but without mentioning the
self ("70% chance it’s..."). We also downsample
our test set to 50 questions per dataset and evaluate
each template at 0%, 10%, 30%, 50%, 70%, 90%
and 100% intervals.
4.4.1
100% Certainty is not 100% Accurate
Our ﬁndings for numerical uncertainties extend our
earlier analysis by enabling us to obtain more ﬁne-
grained results on whether a model is linguistically
calibrated, and how much certainty causes perfor-
mance degradation.
First, we ﬁnd that numerical uncertainties are
poorly calibrated. For example, the prompt ”I’m
90% sure it’s...” in TriviaQA only produces the
correct answer 57% of the time. Formally, we
evaluate the expected calibration error (ECE) and
ﬁnd poor values ranging from 0.50 to 0.30, 0 being
the best (Figures 4).
Second, consistent with our ﬁndings from Sec-
tion 4, we ﬁnd that certainty does hurt model per-
formance. However, we ﬁnd this is true only at the
extremes. We see performance on models peaks
usually between 70% and 90% in numerical val-
ues but drops in accuracy when using 100% in the
prompt. Across all four datasets, with seven tem-
plates each, 21 out of the 28 templates which use
100% numerical values had a lower probability-
on-gold than templates which used 90% numerical
values (Figure 8). Additionally, at the other ex-
treme, when 0% is used in templates, there is also
a drastic drop to accuracy.
The Effect of Hyperbolic Language
What
could be causing these results? We hypothesize
that this could be caused by the use of hyperbolic or
exaggerated language in the training set, in which
numbers are used non-literally. When someone
says “I’m 100% certain there was pie left”, they
don’t necessarily mean they are 100% certain is
there pie — but rather are speaking loosely to em-
phasize their strong belief there was pie. Similarly,
saying “I have zero conﬁdence my car will start”
isn’t a literally calculated value but just an expres-
sion of doubt in their car’s abilities. We hypothe-
size that models somehow recognize the idiomatic,
non-literal uses of these extreme values, resulting
in lower performances on tasks when introduced
into prompts.

Another confounder in how model’s interpret
numerical values could be the distribution of nu-
merical frequencies in typical model training data.
Querying the Pile (Gao et al., 2020), a popular
dataset used to train GPT-like models, we ﬁnd that
there are drastic imbalances in the use of percent-
ages in training datasets. There are signiﬁcant
spikes in frequency at the upper extremes (50%,
95% and 100%) (Figure 5). This might be happen-
ing as humans might naturally exaggerate values
or use colloquial terms to describe conﬁdence. The
presence of scientiﬁc language like "95% conﬁ-
dence interval" could be another possible source
of imbalance. Although spoken natural language
also includes rounded percentages, the use of only
textual data might be further exacerbating this bias.
Figure 5: Visualization of the Frequency of percentages
found in the pile. Note the peaks at the extremes, (0, 50,
10), and peaks at every 10 and 5 intervals from the ﬁrst
1,000,000 samples queried from the Pile dataset using
the HuggingFace API.
5
When LMs Emit Their Own
Uncertainty
Having studied the impact of uncertainty as part of
the prompt, we turn to our second motivating ques-
tion and ask: how does model performance change
when models learn to emit their own expressions
of uncertainty? This question is becoming increas-
ingly important as researchers have studied training
models to generate expressions of uncertainty (Lin
et al., 2022; Kadavath et al., 2022; Mielke et al.,
2022). Here, we study how model performance
changes based on in-context learning examples.
Speciﬁcally, we follow Lin et al. (2022)’s method
in few-shot learning with 50 samples which has
been shown to be nearly as effective as ﬁne-tuning
on datasets that are magnitudes larger. To ensure
that our in-context learning dataset covers a range
of conﬁdence levels, our dataset contains 48 sam-
ples whose probability-on-gold is uniformly dis-
tributed (in buckets of 10) between 0 and 100.
5.1
Experiment Details
To study how LMs respond when emitting their
own uncertainty, we follow Lin et al. (2022)’s setup
but modify it for the strengtheners and weakeners,
which are inherently non-numerical (Figure 2). In
our setting, instead of teaching a model to output
a percentage conﬁdence, we teach it to output a
strengthener when the conﬁdence is above a thresh-
old and nothing otherwise.3 Conversely, when we
study weakeners, we teach it to output a weakener
when the probability is below a threshold and noth-
ing otherwise.
As an example, consider the question “What is
the capital of France”. We record the LM’s proba-
bility over Paris (the probability-on-gold) and ap-
pend “I’m sure” to the in-context example if the
model’s conﬁdence was above 0.5. We repeat this
for all the in-context examples to obtain our in-
context learning training set.
5.2
Prompting Perturbations
Recent work has shown the drastic differences that
appear based on simple changes to prompting setup
(Suzgun et al., 2022; Lu et al., 2021). We design
our in-context learning samples with various per-
turbations to ensure robustness in our results.
We select high performing weakeners and
strengtheners from Section 4 and experiment with
appending expressions of uncertainty after the an-
swer (e.g., “Paris. I think”) (Table 9).4 We then
use three different sample orderings to perturb our
learning samples: ascending and descending order
of probability-on-gold and random ordering.5 Fi-
nally, we experiment with a variety of thresholds
(0.3, 0.5, 0.7, 0.9) for determining when expres-
sions of (un)certainty should be inserted into the
example. These perturbations are done on a small
scale to help identify the best hyper-parameters
(threshold, placement, and ordering) to use.
We ﬁnd that varying the threshold across does
not drastically change accuracy (with all methods
attaining ∼83% in accuracy), although the thresh-
old does signiﬁcantly impact the balance of the
training datasets. Similarly, we ﬁnd limited differ-
ences in the ordering of the samples. With these
3We choose to emit nothing rather than emit an expression
of uncertainty, as we wish to isolate the effect of each linguistic
expression of uncertainty.
4Here, we exclude expressions with attribution shields for
concerns of false attribution, more on this in the discussion.
5In the ascending and descending orders, all the samples
in the beginning or all the samples at the end will include
expressions of (un)certainty.

results, we choose a threshold of 0.5 (creating a
balanced dataset) and random ordering (simplest
setting) as our hyper-parameters for our remain-
ing scenarios and analysis, which we test on 100
TriviaQA questions.
5.3
Results
Gains in Model Calibration When Learning
Uncertainty
Overall, GPT3 has a limited abil-
ity to learn naturalistic expressions of uncertainty
in a calibrated manner. We measured calibration
based on whether models successfully emit the ex-
pressions of uncertainty when the probability of the
top token above or below our training threshold. In
our setup, when learning to emit certainty, answers
with probability-on-gold of greater than 0.5 had
a strengthener and answers with less than 0.5 had
nothing. Therefore, in its generation when the prob-
ability on the top token is greater than 0.5, we’d
expect the model to also generate a strengthener
and vice versa for weakeners. We measure whether
the model successfully generates strengtheners and
weakeners through the F1 score, and ﬁnd that the
template with the highest macro-F1 score for un-
certainty templates to be 0.56 compared to 0.53 for
certainty templates.6 This is close to the random
guessing baseline on this test set which results in
an F1 score of 0.45.
To illustrate the difference in calibration between
uncertainty and certainty, we can look at the aver-
age accuracy when a model emits an expression or
not. When learning to express weakeners, the gen-
eration of a weakener results in an accuracy of 74%
but this increases to 83% when the model doesn’t
generate a weakener. This is the intended behavior,
with the model hedging answers it is more likely to
get incorrect. However, when teaching the model to
emit strengtheners, the generation of strengtheners
does not lead to a signiﬁcant increase in accuracy
(79% with or without an emission of strengtheners).
This means that when the model emits certainty,
the answer is not more likely to be correct, creating
a concerning issue for linguistic calibration (Table
2).
Modeling Changes in Entropy
Despite low
model calibration when emitting expressions of cer-
tainty and uncertainty, we ﬁnd that the underlying
entropy of the probability distribution of the gen-
erated answer is well-calibrated to the expressions
6Average F1 scores being .52 average for uncertainty and
.49 average of certainty
Entropy
Emitting
Not Emitting
Uncertainty
0.699*
0.522
Certainty
0.461
0.617*
Control
N/A
0.541
Accuracy
Emitting
Not Emitting
Uncertainty
0.738
0.829*
Certainty
0.799
0.789
Control
N/A
0.78
Table 2: Entropy is higher when uncertainty is be-
ing expressed and also higher when certainty is not
expressed. Accuracy is also higher when uncertainty
is not expressed but accuracy is not signiﬁcant higher
when certainty is expressed (an indication or poor cal-
ibration). *Signiﬁcantly higher value calculated using
two-sample t-test, p < 0.05.
of uncertainty. Analyzing the top ﬁve predictions
for each token, we ﬁnd that when teaching mod-
els weakeners, the entropy of the distribution of
potential generations is higher when a weakener is
emitted and lower when it is not. The inverse is
true when teaching models strengtheners, entropy
is lower when strengtheners are emitted and higher
when it is not. Although the calibration scores are
not strong for either uncertainty or certainty, we see
promising behaviors in the entropy of the model’s
top generations. When emitting weakeners, the
model places more consideration on alternative an-
swers and less when emitting strengtheners.
Sensitivity to Placement of Template
Finally,
we test how model performance differs based on
the placement of the templates. The simple design
difference of probing for the answer before (e.g.,
“I think it’s Paris.”) or after (e.g., “Paris. I think.”)
an expression of uncertainty can have a signiﬁcant
difference in performance. In our tables, we re-
fer to these places as preﬁxes (before) and sufﬁxes
(after). We ﬁnd that when appending expressions
of uncertainty as a preﬁx, the generation is signif-
icantly worse for accuracy (63% vs 80%). This
is also correlated with probability-on-gold being
lower in preﬁxed templates (40% vs 67%). An
explanation for this might be that the probability
of generating the correct answer will be lower if
generated after a phrase like “I think it’s...” rather
than just generated immediately after the question.
Our work suggests that ordering effects may be
important when addressing accuracy-calibration

trade-offs in LMs and that there are accuracy gains
when prompting the model to respond with answers
as soon as possible.
Template
Certainty
Prob
Top 1
Preﬁx
Uncertain
0.388
0.592
Certain
0.407
0.674
Sufﬁx
Uncertain
0.674
0.800
Certain
0.673
0.792
Control
N/A
0.673
0.780
Table 3: Average probability on generated token and
top 1 accuracy across preﬁx and sufﬁx templates.
6
Related Work
While scholars have studied model uncertainty,
prior work has focused on more accurately ex-
tracting model conﬁdence (Kuhn et al., 2023; Sun
et al., 2022; Gleave and Irving, 2022), measuring
(Kwiatkowski et al., 2019b; Radford et al., 2019;
Liang et al., 2022b) and improving model calibra-
tion (Jiang et al., 2021; Desai and Durrett, 2020a;
Jagannatha and Yu, 2020; Kamath et al., 2020;
Kong et al., 2020). However, the community has
found mixed results on the calibration of neural
model (Minderer et al., 2021; Carrell et al., 2022);
for example, Desai and Durrett (2020b) shows
that pre-trained transformers are relatively well-
calibrated meanwhile Wang et al. (2020) found
severe mis-calibration in neural machine transla-
tion. Another line of work also explore the trade-
off between model performance and calibration
(Stengel-Eskin and Van Durme, 2022).
Closest to our work, Mielke et al. (2022) pro-
pose solutions to reducing model overconﬁdence
through linguistic calibration, Kadavath et al.
(2022) experiment with models’ ability emit self-
conﬁdence after ﬁnding that models are relatively
well-calibrated, and Lin et al. (2022) teach mod-
els to be linguistically calibrated when answering
math questions. We build on these works by incor-
porating other aspects of naturalistic expressions
of uncertainty such as evidentials and factive verbs
into the analysis of NLG systems.
In addition, semanticists and computational lin-
guists have long studied speaker commitment fac-
tors such as factivity (Karttunen, 1971; Degen and
Tonhauser, 2022) and projection (Simons et al.,
2010), and more recent work include corpora
like the CommitmentBank (De Marneffe et al.,
2019) which offers naturally occurring examples,
as well as new experimental paradigms to investi-
gate speaker commitment (Degen et al., 2019). A
wide variety of scholars have examined computa-
tional issues in factuality, veridicality, and commit-
ment (Saurí and Pustejovsky, 2009; de Marneffe
et al., 2012; Stanovsky et al., 2017; Rudinger et al.,
2018; Jiang and de Marneffe, 2021, inter alia) as
well as bias (Pryzant et al., 2020; Patel and Pavlick,
2021) and speciﬁc devices like hedges (Prokoﬁeva
and Hirschberg, 2014; Raphalen et al., 2022), and
modality (Pyatkin et al., 2021).
We see our work as a bridge between these two
areas of speaker commitment and natural language
generation, by telling us how models interpret and
generate speaker commitment through expressions
of certainty and uncertainty.
7
Discussion and Conclusion
In this work, we analyzed how naturalistic expres-
sions of uncertainty impact model behavior both
for prompting and in-context learning. We ﬁnd a
drop in accuracy when naturalistic expressions of
certainty (i.e., strengtheners and factive verbs) are
used in zero-shot prompting; this is also true with
numerical uncertainty when idioms like "I’m 100%
certain" are used. We also ﬁnd calibration gains
when teaching models to express weakeners rather
than strengtheners. Moving forward, we present a
number of recommendations for the community for
NLG and naturalistic expressions of uncertainty.
A Shift to Uncertainty
Beyond calibration,
teaching models to only emit expressions of un-
certainty when they are unsure, rather than when
they are sure, could be a safer design choice for
human-computer interactions.
Prior work has
shown cases where AI-assisted decision-making
performed worse than human decision-making
alone, suggesting an over-reliance on AI, even
when systems are wrong (Jacobs et al., 2021; Bus-
sone et al., 2015). This leads us to suspect that
teaching models to emit expressions of certainty
could further exacerbate these challenges — es-
pecially given how poorly calibrated and brittle
the models are. Current literature has focused on
emitting expressions of conﬁdence in a calibrated
manner, but what remains to be investigated is how
humans interpret generated naturalistic expressions.
As a recommendation, we urge the community to
focus on training models to emit expressions of

uncertainty meanwhile further work is conducted.
The Limitations of Written Text
The analysis
of how GPT3 uses expressions of uncertainty high-
lights another limitation of using just written text
for training language generation. We suspect that
the use of weakeners in spoken language will vary
greatly from that of written language and that
cues such as vocal disﬂuencies could also impact
a model’s interpretation and generation of uncer-
tainty. Future work could explore the interaction of
LMs with spoken expressions of uncertainty.
Veriﬁed Attribution
We encourage the commu-
nity to explore how to integrate attributions of un-
certainty in a veriﬁed manner. In our experiments,
we chose not to experiment with generating attri-
butions as we could not guarantee their validity.
However, some of the best-performing templates
from Section 4 include phrases like "Wikipedia
says...". We hypothesize there is potential to at-
tribute knowledge in a way that supports model
transparency and encourages user trust and think-
ing. However, we would caution the community
away from arbitrarily generating attributions as it
could provide downstream users with a false, yet
highly believable attribution.
Using Expressions of Uncertainty to Sow Doubt
Lastly, we warn the community about the potential
dangers of using expressions of uncertainty mali-
ciously. Imagine a model arbitrarily responding
with weakeners when asked to answer controver-
sial topics (e.g., "Well, supposedly it’s true..." or
"Wikipedia claims it’s happening..."). Beyond the
helpful ways in which expressions of uncertainty
can support human decision-making, expressions
of harm can be used by LMs to undermine individu-
als and to cast doubt on controversial or subjective
questions.
As the community expands the capabilities of
NLG to general language that is more natural, it is
critical we prepare for the opportunity and harms
that may arise from naturalistic expressions of un-
certainty.
Acknowledgements
Thank you so much to Tolúlo.pé. Ògúnrè.mí, Yiwei
Luo, Myra Cheng, Rishi Bommasani, and Omar
Shaikh for their helpful feedback and advice!
References
Alexandra Y Aikhenvald. 2004. Evidentiality. OUP
Oxford.
Adrian
Bussone,
Simone
Stumpf,
and
Dympna
O’Sullivan. 2015. The role of explanations on trust
and reliance in clinical decision support systems. In
2015 International Conference on Healthcare Infor-
matics, pages 160–169.
Annabelle Carrell, Neil Mallinar, James Lucas, and
Preetum Nakkiran. 2022. The calibration general-
ization gap. arXiv preprint arXiv:2210.01964.
Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it happen?
the pragmatic complexity of veridicality assessment.
Computational Linguistics, 38(2):301–333.
Marie-Catherine De Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
In proceedings of Sinn und Bedeutung, volume 23,
pages 107–124.
Judith Degen and Judith Tonhauser. 2022. Are there
factive predicates? an empirical investigation. Lan-
guage, 98(3):552–591.
Judith Degen, Andreas Trotzke, Gregory Scontras, Eva
Wittenberg, and Noah D Goodman. 2019. Deﬁnitely,
maybe: A new experimental paradigm for investigat-
ing the pragmatics of evidential devices across lan-
guages. Journal of Pragmatics, 140:33–48.
Shrey Desai and Greg Durrett. 2020a.
Calibra-
tion of pre-trained transformers.
arXiv preprint
arXiv:2003.07892.
Shrey Desai and Greg Durrett. 2020b. Calibration of
pre-trained transformers. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 295–302, Online.
Association for Computational Linguistics.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027.
Adam Gleave and Geoffrey Irving. 2022.
Uncer-
tainty estimation for language reward models. arXiv
preprint arXiv:2203.07472.
Ken Hyland. 2005. Stance and engagement: A model
of interaction in academic discourse.
Discourse
studies, 7(2):173–192.
Ken Hyland. 2014.
Disciplinary discourses: Writer
stance in research articles. In Writing: Texts, pro-
cesses and practices, pages 99–121. Routledge.

Maia Jacobs, Melanie F Pradier, Thomas H McCoy Jr,
Roy H Perlis, Finale Doshi-Velez, and Krzysztof Z
Gajos. 2021. How machine-learning recommenda-
tions inﬂuence clinician treatment selections: the ex-
ample of antidepressant selection. Translational psy-
chiatry, 11(1):108.
Abhyuday Jagannatha and Hong Yu. 2020. Calibrat-
ing structured output predictors for natural language
processing. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 2078–2092, Online. Association for Computa-
tional Linguistics.
Nanjiang Jiang and Marie-Catherine de Marneffe.
2021.
He thinks he knows better than the doc-
tors: BERT for event factuality fails on pragmatics.
Transactions of the Association for Computational
Linguistics, 9:1081–1097.
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham
Neubig. 2021. How can we know when language
models know? on the calibration of language mod-
els for question answering. Transactions of the Asso-
ciation for Computational Linguistics, 9:962–977.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatﬁeld Dodds, Nova DasSarma,
Eli Tran-Johnson, et al. 2022.
Language models
(mostly) know what they know.
arXiv preprint
arXiv:2207.05221.
Amita Kamath, Robin Jia, and Percy Liang. 2020. Se-
lective question answering under domain shift. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5684–
5696, Online. Association for Computational Lin-
guistics.
Lauri Karttunen. 1971. Some observations on factiv-
ity.
Research on Language & Social Interaction,
4(1):55–69.
Paul Kiparsky and Carol Kiparsky. 1970. FACT, pages
143–173. De Gruyter Mouton, Berlin, Boston.
Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie
Lyu, Tuo Zhao, and Chao Zhang. 2020.
Cali-
brated language model ﬁne-tuning for in- and out-of-
distribution data. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1326–1340, Online. As-
sociation for Computational Linguistics.
Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.
Semantic uncertainty: Linguistic invariances for un-
certainty estimation in natural language generation.
arXiv preprint arXiv:2302.09664.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, et al. 2019a.
Natural questions: a
benchmark for question answering research. Trans-
actions of the Association for Computational Lin-
guistics, 7:453–466.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019b. Natu-
ral questions: a benchmark for question answering
research. Transactions of the Association of Compu-
tational Linguistics.
George Lakoff. 1975. Hedges: A study in meaning cri-
teria and the logic of fuzzy concepts. In Contempo-
rary Research in Philosophical Logic and Linguis-
tic Semantics: Proceedings of a Conference Held at
the University of Western Ontario, London, Canada,
pages 221–271. Springer.
Young-Jun Lee, Chae-Gyun Lim, Yunsu Choi, Ji-Hui
Lm, and Ho-Jin Choi. 2022. PERSONACHATGEN:
Generating personalized dialogues using GPT-3. In
Proceedings of the 1st Workshop on Customized
Chat Grounding Persona and Knowledge, pages 29–
48, Gyeongju, Republic of Korea. Association for
Computational Linguistics.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-
mar, Benjamin Newman, Binhang Yuan, Bobby Yan,
Ce Zhang, Christian Cosgrove, Christopher D. Man-
ning, Christopher Ré, Diana Acosta-Navas, Drew A.
Hudson, Eric Zelikman, Esin Durmus, Faisal Lad-
hak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue
Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng,
Mert Yuksekgonul, Mirac Suzgun, Nathan Kim,
Neel Guha, Niladri Chatterji, Omar Khattab, Peter
Henderson, Qian Huang, Ryan Chi, Sang Michael
Xie, Shibani Santurkar, Surya Ganguli, Tatsunori
Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav
Chaudhary, William Wang, Xuechen Li, Yifan Mai,
Yuhui Zhang, and Yuta Koreeda. 2022a.
Holistic
evaluation of language models.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-
mar, et al. 2022b. Holistic evaluation of language
models. arXiv preprint arXiv:2211.09110.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
Teaching models to express their uncertainty in
words. arXiv preprint arXiv:2205.14334.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian
Riedel, and Pontus Stenetorp. 2021.
Fantastically
ordered prompts and where to ﬁnd them: Overcom-
ing few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786.

Sabrina J Mielke, Arthur Szlam, Emily Dinan, and
Y-Lan Boureau. 2022.
Reducing conversational
agents’ overconﬁdence through linguistic calibra-
tion. Transactions of the Association for Computa-
tional Linguistics, 10:857–872.
Matthias Minderer, Josip Djolonga, Rob Romijnders,
Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin
Tran, and Mario Lucic. 2021. Revisiting the calibra-
tion of modern neural networks. Advances in Neural
Information Processing Systems, 34:15682–15694.
Joon Sung Park, Lindsay Popowski, Carrie Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S Bern-
stein. 2022. Social simulacra: Creating populated
prototypes for social computing systems.
In Pro-
ceedings of the 35th Annual ACM Symposium on
User Interface Software and Technology, pages 1–
18.
Roma Patel and Ellie Pavlick. 2021. “was it “stated” or
was it “claimed”?: How linguistic bias affects gener-
ative language models. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 10080–10095, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
E. Prince, C. Bosk, and J. Frader. 1982. On hedging in
physician-physician discourse. Di Pietro, R.J., Ed.,
Linguistics and the Professions, pages 83–97.
Anna Prokoﬁeva and Julia Hirschberg. 2014. Hedging
and speaker commitment. In 5th Intl. Workshop on
Emotion, Social Signals, Sentiment & Linked Open
Data, Reykjavik, Iceland.
Reid Pryzant, Richard Diehl Martinez, Nathan Dass,
Sadao Kurohashi, Dan Jurafsky, and Diyi Yang.
2020. Automatically neutralizing subjective bias in
text. In Proceedings of the AAAI conference on arti-
ﬁcial intelligence, volume 34, pages 480–489.
Valentina Pyatkin, Shoval Sadde, Aynat Rubinstein,
Paul Portner, and Reut Tsarfaty. 2021. The possible,
the plausible, and the desirable: Event-based modal-
ity detection for language processing. In Proceed-
ings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 953–965.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019.
Lan-
guage models are unsupervised multitask learners.
OpenAI blog, 1(8):9.
Yann Raphalen, Chloé Clavel, and Justine Cassell.
2022. “You might think about slightly revising the
title”: Identifying hedges in peer-tutoring interac-
tions. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2160–2174, Dublin, Ire-
land. Association for Computational Linguistics.
Rachel Rudinger, Aaron Steven White, and Benjamin
Van Durme. 2018. Neural models of factuality. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pages 731–744, New
Orleans, Louisiana. Association for Computational
Linguistics.
Roser Saurí and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
resources and evaluation, 43:227–268.
Mandy Simons, Judith Tonhauser, David Beaver, and
Craige Roberts. 2010. What projects and why. In
Semantics and linguistic theory, volume 20, pages
309–327.
Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy
Puzikov, Ido Dagan, and Iryna Gurevych. 2017. In-
tegrating deep linguistic features in factuality pre-
diction over uniﬁed datasets. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
352–357, Vancouver, Canada. Association for Com-
putational Linguistics.
Elias Stengel-Eskin and Benjamin Van Durme. 2022.
Calibrated interpretation: Conﬁdence estimation in
semantic parsing. arXiv preprint arXiv:2211.07443.
Meiqi Sun, Wilson Yan, Pieter Abbeel, and Igor Mor-
datch. 2022. Quantifying uncertainty in foundation
models via ensembles. In NeurIPS 2022 Workshop
on Robustness in Sequence Modeling.
Mirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-
sky. 2022.
Prompt-and-rerank:
A method for
zero-shot and few-shot arbitrary textual style trans-
fer with small language models.
arXiv preprint
arXiv:2205.11503.
Shuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu.
2020.
On the inference calibration of neural ma-
chine translation. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 3070–3079, Online. Association for
Computational Linguistics.

A
Appendix
Additional Details
A.1
Additional Details from Section 4
The model generates up to 10 tokens for each ques-
tion and returns the top predictions per token. For
Section 4 we use OpenAI’s researcher API and
retrieve the top 50 most probable predictions per
token. For Section 5 we use the standard API and
retrieve the top 5 most probable predictions per
token.
We are careful that our prompts do not end with
trailing white space (" ") as recommended by Ope-
nAI in order to prompt the best generations. We
also use the delimiters "Q:" and "A:" to signal the
start and end of questions and answers.7
B
Amazon Mechanical Turk Results
We use Amazon Mechanical Turk to crowd-source
some additional expressions of uncertainty.
A
screenshot of the task is included below. Work-
ers were ﬁltered to be have HITs greater than 99
and to have at least 500 approved HITs. Given
the simplicity of the task, we estimated it would
take users a minute or two to complete the task, a
paid users $0.35 USD for the task which results in
roughly $10.50 USD to $21.00 USD an hour. We
collected a total of 9 samples of 5 examples each.
The authors then read, ﬁltered, and modiﬁed the
examples to follow the overall linguistic structure
of the other templates.
Figure 6: Screenshot of the Crowdsourced Example
C
Top 10 Templates from Section 4
7In CountryQA and TriviaQA, an additional new line char-
acter was added before "A:". To reduce accruing additional
costs, NaturalQA and Jeopardy results were not rerun to match
this exact template.
CountryQA
Jeopardy
NaturalQA
TriviaQA
Datasets
0.0
0.2
0.4
0.6
0.8
1.0
Accuracy
Accuracy by Plausibility Shield
Plausibility
Not Plausibility
CountryQA
Jeopardy
NaturalQA
TriviaQA
Datasets
0.0
0.2
0.4
0.6
0.8
Accuracy
Accuracy by Source
Source
No Source
CountryQA
Jeopardy
NaturalQA
TriviaQA
Datasets
0.0
0.2
0.4
0.6
0.8
Accuracy
Accuracy by Personal Pronouns
Personal Pronoun
Not Personal Pronoun
Figure 7: The use of plausibility shields, sources, and
personal pronouns are mixed, without signiﬁcant con-
sistent improvements or drops in accuracy. 95% CI cal-
culated using bootstrap resampling.

Template
Strengtheners
Shield
Evidential Marker
Factive Verb
Source
1P
Apparently it’s
Weakener
None
Evidential
Not Factive
No Source
No
Presumably it’s
Weakener
None
Evidential
Not Factive
No Source
No
Rumor says it it’s
Weakener
None
Evidential
Not Factive
No Source
No
Allegedly it’s
Weakener
None
Evidential
Not Factive
No Source
No
I was told it’s
Weakener
None
Evidential
Not Factive
No Source
Yes
I’ve heard it’s
Weakener
None
Evidential
Not Factive
No Source
Yes
They told me it’s
Weakener
None
Evidential
Not Factive
No Source
Yes
Wikipedia suggests it’s
Weakener
None
Evidential
Not Factive
Source
No
Online says it’s
Weakener
None
Evidential
Not Factive
Source
No
The internet says it’s
Weakener
None
Evidential
Not Factive
Source
No
Wikipedia claims it’s
Weakener
None
Evidential
Not Factive
Source
No
Wikipedia says it’s
Weakener
None
Evidential
Not Factive
Source
No
I read on the internet it’s
Weakener
None
Evidential
Not Factive
Source
Yes
I read on Wikipedia it’s
Weakener
None
Evidential
Not Factive
Source
Yes
I read online it’s
Weakener
None
Evidential
Not Factive
Source
Yes
To the best of
my knowledge it’s
Weakener
Plausibility
Evidential
Not Factive
No Source
Yes
As far as I’m aware it’s
Weakener
Plausibility
Evidential
Not Factive
No Source
Yes
I vaguely remember it’s
Weakener
Plausibility
Evidential
Not Factive
No Source
Yes
It could be
Weakener
Plausibility
Not Evidential
Not Factive
No Source
No
Considering all
the options it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
No
It probably is
Weakener
Plausibility
Not Evidential
Not Factive
No Source
No
Maybe it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
No
Perhaps it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
No
It should be
Weakener
Plausibility
Not Evidential
Not Factive
No Source
No
I don’t know maybe it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
Yes
I suppose it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
Yes
I would need to
double check but maybe it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
Yes
I wouldn’t put
money on it but maybe it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
Yes
I’m not an expert but maybe it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
Yes
I think it’s
Weakener
Plausibility
Not Evidential
Not Factive
No Source
Yes
I feel like it should be
Weakener
Plausibility
Not Evidential
Not Factive
No Source
Yes
It is known that it’s
Strengthener
None
Evidential
Factive
No Source
No
The most recent
evidence shows it’s
Strengthener
None
Evidential
Factive
Source
No
The rules state it’s
Strengthener
None
Evidential
Factive
Source
No
Two recent studies
demonstrate it’s
Strengthener
None
Evidential
Factive
Source
No
Wikipedia
acknowledges it’s
Strengthener
None
Evidential
Factive
Source
No
Wikipedia conﬁrms it’s
Strengthener
None
Evidential
Factive
Source
No
Our lab has shown it’s
Strengthener
None
Evidential
Factive
Source
Yes
Evidently it’s
Strengthener
None
Evidential
Not Factive
No Source
No
According to the
latest research it’s
Strengthener
None
Evidential
Not Factive
Source
No
We can see in the
textbook that it’s
Strengthener
None
Evidential
Not Factive
Source
Yes
It must be
Strengthener
None
Not Evidential
Factive
No Source
No
We realize it’s
Strengthener
None
Not Evidential
Factive
No Source
Yes
We understand it’s
Strengthener
None
Not Evidential
Factive
No Source
Yes
We know it’s
Strengthener
None
Not Evidential
Factive
No Source
Yes
Undoubtedly it’s
Strengthener
None
Not Evidential
Not Factive
No Source
No
With 100% conﬁdence it’s
Strengthener
None
Not Evidential
Not Factive
No Source
No
I’m certain it’s
Strengthener
None
Not Evidential
Not Factive
No Source
Yes
I am 100% sure it’s
Strengthener
None
Not Evidential
Not Factive
No Source
Yes
It’s
None
None
Not Evidential
Not Factive
No Source
No
Table 4: Full list of expressions of uncertainty coded for six linguistic features. *Claims is a neg-factive but in our
schema, will just be considered not a factive verb. (Saurí and Pustejovsky, 2009)

Template
Top 1 Accuracy
0
Online says it’s
0.660
1
Standard Method
0.625
2
Wikipedia conﬁrms it’s
0.600
3
Wikipedia suggests it’s
0.595
4
The internet says it’s
0.585
5
Wikipedia claims it’s
0.575
6
Wikipedia says it’s
0.575
7
We can see in the textbook that it’s
0.565
8
I would need to double check but maybe it’s
0.555
9
Rumor says it it’s
0.550
Table 5: Top 10 Templates For TriviaQA
Template
Top 1 Accuracy
0
Standard Method
1.0
1
I read on Wikipedia it’s
1.0
2
It’s
1.0
3
It should be
1.0
4
Allegedly it’s
1.0
5
I’m not an expert but maybe it’s
1.0
6
I wouldn’t put money on it but maybe it’s
1.0
7
Presumably it’s
1.0
8
I read online it’s
1.0
9
I read on the internet it’s
1.0
Table 6: Top 10 Templates For CountryQA
Template
Top 1 Accuracy
0
Standard Method
0.450
1
It must be
0.390
2
It’s
0.380
3
It could be
0.370
4
The internet says it’s
0.370
5
Online says it’s
0.360
6
With 100% conﬁdence it’s
0.350
7
Undoubtedly it’s
0.345
8
Wikipedia says it’s
0.345
9
Wikipedia conﬁrms it’s
0.325
Table 7: Top 10 Templates for Jeopardy

Template
Top 1 Accuracy
0
Wikipedia claims it’s
0.340
1
Wikipedia says it’s
0.335
2
Online says it’s
0.310
3
Wikipedia suggests it’s
0.305
4
The internet says it’s
0.300
5
Wikipedia conﬁrms it’s
0.300
6
I read on Wikipedia it’s
0.295
7
Presumably it’s
0.275
8
Standard Method
0.275
9
I think it’s
0.270
Table 8: Top 10 Templates for NaturalQA
0
20
40
60
80
100
Percent
0.75
0.80
0.85
0.90
0.95
1.00
Accuracy
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
CountryQA
 0% chance it's
 0% probability it's
 I am 0% certain it's
 I am 0% confident it's
 I am 0% convinced it's
 I am 0% sure it's
 With 0% confidence it's
0
20
40
60
80
100
Percent
0.35
0.40
0.45
0.50
0.55
0.60
0.65
Accuracy
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
TriviaQA
 0% chance it's
 0% probability it's
 I am 0% certain it's
 I am 0% confident it's
 I am 0% convinced it's
 I am 0% sure it's
 With 0% confidence it's
0
20
40
60
80
100
Percent
0.10
0.15
0.20
0.25
0.30
Accuracy
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
NaturalQA
 0% chance it's
 0% probability it's
 I am 0% certain it's
 I am 0% confident it's
 I am 0% convinced it's
 I am 0% sure it's
 With 0% confidence it's
0
20
40
60
80
100
Percent
0.15
0.20
0.25
0.30
0.35
0.40
0.45
Accuracy
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
0.0
10.0
30.0
50.0
70.0
90.0
100.0
Jeopardy Questions
 0% chance it's
 0% probability it's
 I am 0% certain it's
 I am 0% confident it's
 I am 0% convinced it's
 I am 0% sure it's
 With 0% confidence it's
Figure 8: Variation in probability-on-gold across numerical uncertainties. Note the consistent drop in accuracy
between 90% and 100% uncertainty and the increase in accuracy between 0% and 10% uncertainty.

Expression
Sufﬁx
Preﬁx
Uncertainty
Undoubtedly.
Undoubtedly it’s
Uncertainty
With 100% conﬁdence.
With 100% conﬁdence it’s
Uncertainty
We know it.
We know it’s
Uncertainty
Evidently.
Evidently it’s
Uncertainty
It must be.
It must be
Certainty
I think.
I think it’s
Certainty
It could be.
It could be
Certainty
But I would need to double check.
I would need to double check but maybe it’s
Certainty
I suppose.
I suppose it’s
Certainty
But I wouldn’t put money on it.
I wouldn’t put money on it but maybe it’s
Table 9: List of Templates Used for Section 5

