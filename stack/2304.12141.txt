Variational Diffusion Auto-encoder:
Deep Latent Variable Model with Unconditional
Diffusion Prior
Georgios Batzolis ∗
DAMTP
University of Cambridge
Cambridge CB3 0WA
gb511@cam.ac.uk
Jan Stanczuk ∗
DAMTP
University of Cambridge
Cambridge CB3 0WA
js2164@cam.ac.uk
Carola-Bibiane Schönlieb
DAMTP
University of Cambridge
Cambridge CB3 0WA
cbs31@cam.ac.uk
Abstract
Variational auto-encoders (VAEs) are one of the most popular approaches to deep
generative modeling. Despite their success, images generated by VAEs are known
to suffer from blurriness, due to a highly unrealistic modeling assumption that the
conditional data distribution p(x|z) can be approximated as an isotropic Gaussian.
In this work we introduce a principled approach to modeling the conditional data
distribution p(x|z) by incorporating a diffusion model. We show that it is possible
to create a VAE-like deep latent variable model without making the Gaussian
assumption on p(x|z) or even training a decoder network. A trained encoder and an
unconditional diffusion model can be combined via Bayes’ rule for score functions
to obtain an expressive model for p(x|z). Our approach avoids making strong
assumptions on the parametric form of p(x|z), and thus allows to signiﬁcantly
improve the performance of VAEs.
1
Introduction
Variational Autoencoders (VAEs) Kingma und Welling (2013) have proven to be a powerful tool
for unsupervised learning, allowing for the efﬁcient modeling and generation of complex data
distributions. However, VAEs have important limitations, including difﬁculty in capturing the
underlying structure of high-dimensional data and generating blurry images Zhao u. a. (2017). These
problems emerge due to an unrealistic modeling assumption that the conditional data distribution
p(x|z) can be approximated as a Gaussian distribution. Moreover, instead of sampling from p(x|z)
the model simply outputs the mean of the distribution, which results in an undesirable smoothing
effect. In this work we propose to relax this limiting assumption by modeling p(x|z) in a ﬂexible way
by leveraging the capabilities of diffusion models. We show that an encoder network modeling p(z|x)
can be easily combined with an unconditional diffusion model trained on p(x) to yield a model for
p(x|z).
Diffusion models Sohl-Dickstein u. a. (2015); Ho u. a. (2020) have recently emerged as a promising
technique for generative modeling, which uses the time reversal of a diffusion process, to estimate
∗Equal contibution.
Preprint. Under review.
arXiv:2304.12141v1  [cs.LG]  24 Apr 2023

the data distribution p(x). Diffusion models proved to be incredibly successful in capturing complex
high-dimensional distributions achieving state-of-the-art performance in many tasks such as image
synthesis Dhariwal und Nichol (2021) and audio generation Kong u. a. (2020). Recent works show
that diffusion models can capture effectively complex conditional probability distributions and apply
them to solve problems such as in-painting, super resolution or image-to-image translation Batzolis
u. a. (2022); Saharia u. a. (2021).
Motivated by the success of diffusion models in learning conditional distributions, recent works
Preechakul u. a. (2022); Yang und Mandt (2023) have explored applications of conditional diffusion
models as a decoders in a VAE framework by training them to learn p(x|z). We improve upon this
line of research by showing that the diffusion decoder is obsolete. Instead one can combine the
encoder with an unconditional diffusion model via the Bayes’ rule for score functions to obtain a
model for p(x|z). This approach has several important advantages
1. It avoids making unrealistic Gaussian assumption on p(x|z). Therefore signiﬁcantly im-
proves the performance compared to original VAE avoiding blurry samples.
2. Since the diffusion model used in our approach is unconditional, i.e. it does not depend on
the latent factor, our method can leverage existing powerful pre-trained diffusion models
and combine them with any encoder network. Moreover the diffusion component can be
always easily replaced for a better model without the need to retrain the encoder. This is
in contrast to prior approaches which used conditional diffusion models, which have to be
trained speciﬁcally to a given encoder.
3. By using the Bayes’ rule for score functions we can separate training the prior from train-
ing the encoder and improve the training dynamics. This allows our method to achieve
performance superior to approaches based on conditional diffusion models.
Moreover we derive a novel lower-bound on the data likelihood p(x), which can be used to optimise
the encoder in this framework.
We showcase the performance of our method on CIFAR10 Krizhevsky (2012) and CelebA Liu u. a.
(2015) datasets.
2
Background
2.1
Variational Autoencoders
Variational Autoencoders (VAEs) Kingma und Welling (2013); Rezende u. a. (2014) is a deep latent
variable model which approximates the data generating distribution p(x). In VAEs it is assumed that
the data generating process can be represented by ﬁrst generating a latent variable z according to p(z)
and then generating the corresponding data point x according to the conditional data distribution
p(x|z). The model is parameterized by two neural networks: the encoder eφ and the decoder dθ.
VAEs make the following parametric assumptions:
• The latent prior distribution p(z) is assumed to be a standard Gaussian N(0, I).
• The data posterior distribution pθ(x|z) is parameterised as a Gaussian N(µx
θ(x), Σx
θ(x))
where the mean µx
θ(x) and the diagonal covariance matrix Σx
θ(x) are outputs of the decoder
network dθ : z 7→(µx
θ(x), Σx
θ(x)). However in many implementations an additional
simpliﬁcation is made: Σx
θ(x) is assumed to be isotropic σx
θ I or even the identity matrix I.
This induces latent posterior pθ(z|x) and marginal likelihood distributions pθ(x) which are both
intractable. Therefore VAEs introduce a variational approximation
• The latent posterior distribution pθ(z|x) is approximated with a variational distribution
qφ(z|x) which is parameterised as a Gaussian N(µz
φ(x), Σz
φ(x)) where the mean µz
φ(x)
and the diagonal covariance matrix Σz
φ(x) are outputs of the encoder network eφ : x 7→
(µz
φ(x), Σz
φ(x)).
As mentioned before the exact log-likelihood ln pθ(x) is intractable, however using the variational
distribution qφ(z|x) one obtains a tractable lower bound known as the evidence lower bound (ELBO)
2

or variational lower bound:
ln pθ(x) −DKL(qφ(z|x) ∥pθ(z|x)) = Ez∼qφ(z|x)[ln pθ(x|z)] −DKL(qφ(z|x) ∥p(z))
Finally the model can be trained by maximizing the ELBO over all data points or by minimizing:
LELBO(θ, φ) := −Ex∼p(x)

Ez∼qφ(z|x)[ln pθ(x|z)] −DKL(qφ(z|x) ∥p(z))

(1)
via a SGD based optimization method. This has the effect of both maximizing the data log-likelihood
ln pθ(x) and minimizing DKL(qφ(z|x) ∥pθ(z|x)), and therefore pushing the variational distribution
qφ(z|x) towards the posterior pθ(z|x).
It is a well known problem that the KL penalty term in 2 leads to signiﬁcant over-regularization and
poor convergence. Therefore many implementations use the following modiﬁed β-ELBO objective
instead Higgins u. a. (2016):
LELBO(θ, φ, β) := −Ex∼p(x)

Ez∼qφ(z|x)[ln pθ(x|z)] −βDKL(qφ(z|x) ∥p(z))

(2)
where β ∈(0, 1). The resulting model is often referred to as β-VAE 2.
2.2
Score-based diffusion models
Setup: In Song u. a. (2020) score-based Hyvärinen (2005) and diffusion-based Sohl-Dickstein u. a.
(2015); Ho u. a. (2020) generative models have been uniﬁed into a single continuous-time score-based
framework where the diffusion is driven by a stochastic differential equation. This framework relies
on Anderson’s Theorem Anderson (1982), which states that under certain Lipschitz conditions on the
drift coefﬁcient f : Rnx × R −→Rnx and on the diffusion coefﬁcient G : Rnx × R −→Rnx × Rnx
and an integrability condition on the target distribution p(x0) a forward diffusion process governed
by the following SDE:
dxt = f(xt, t)dt + G(xt, t)dwt
(3)
has a reverse diffusion process governed by the following SDE:
dxt = [f(xt, t) −G(xt, t)G(xt, t)T ∇xtln pXt(xt)]dt + G(xt, t)d ¯wt,
(4)
where ¯wt is a standard Wiener process in reverse time.
The forward diffusion process transforms the target distribution p(x0) to a diffused distribution
p(xT ) after diffusion time T. By appropriately selecting the drift and the diffusion coefﬁcients of the
forward SDE, we can make sure that after sufﬁciently long time T, the diffused distribution p(xT )
approximates a simple distribution, such as N(0, I). We refer to this simple distribution as the prior
distribution, denoted by π. The reverse diffusion process transforms the diffused distribution p(xT )
to the data distribution p(x0) and the prior distribution π to a distribution pSDE. The distribution
pSDE is close to p(x0) if the diffused distribution p(xT ) is close to the prior distribution π. We get
samples from pSDE by sampling from π and simulating the reverse SDE from time T to time 0.
Sampling: To get samples by simulating the reverse SDE, we need access to the time-dependent
score function ∇xtln p(xt). In practice, we approximate the time-dependent score function with a
neural network sθ(xt, t) ≈∇xtln p(xt) and simulate the reverse SDE presented in equation 5 to map
the prior distribution π to pSDE
θ
.
dxt = [f(xt, t) −G(xt, t)G(xt, t)T sθ(xt, t)]dt + G(xt, t)d ¯wt,
(5)
If the prior distribution is close to the diffused distribution and the approximated score function is
close to the ground truth score function, the modeled distribution pSDE
θ
is provably close to the target
distribution p(x0). This statement is formalised in the language of distributional distances in the work
of Song u. a. (2021).
Training: A neural network sθ(xt, t) can be trained to approximate the score function ∇xtln p(xt)
by minimizing the weighted score matching objective
2It is worth noting that choosing β ̸= 1 is equivalent to choosing a ﬁxed isotropic covariance Σx
θ(x) = σI
for appropriate value of σ, instead of the identity matrix. We refer to Rybkin u. a. (2021) for details
3

LSM(θ, λ(·)) :=
1
2Et∼U(0,T )
xt∼p(xt)
[λ(t) ∥∇xtln p(xt) −sθ(xt, t)∥2
2]
(6)
where λ : [0, T] −→R+ is a positive weighting function.
However, the above quantity cannot be optimized directly since we don’t have access to the ground
truth score ∇xtln p(xt). Therefore in practice, a different objective has to be used Hyvärinen (2005);
Vincent (2011); Song u. a. (2020). In Song u. a. (2020), the weighted denoising score-matching
objective is used, which is deﬁned as
LDSM(θ, λ(·)) :=
1
2E
t∼U(0,T )
x0∼p(x0)
xt∼p(xt|x0)
[λ(t) ∥∇xtln p(xt|x0) −sθ(xt, t)∥2
2]
(7)
The difference between DSM and SM is the replacement of the ground truth score which we do not
know by the score of the perturbation kernel which we know analytically for many choices of forward
SDEs. The choice of the weighted DSM objective is justiﬁed because the weighted DSM objective is
equal to the SM objective up to a constant that does not depend on the parameters of the model θ.
The reader can refer to Vincent (2011) for the proof.
Weighting function: The choice of the weighting function is also important, because it determines
the quality of score-matching in different diffusion scales. In the case of uniform diffusion i.e. when
G(xt, t) = g(t)I for g : R −→R a principled choice for the weighting function is λ(t) = g(t)2. This
weighting function is called the likelihood weighting function Song u. a. (2021), because it ensures
that we minimize an upper bound on the Kullback–Leibler divergence from the target distribution to
the model distribution by minimizing the weighted DSM objective with this weighting. The previous
statement is implied by the combination of inequality 8 which is proven in Song u. a. (2021) and the
relationship between the DSM and SM objectives.
DKL(p(x0) ∥pSDE
θ
(x0)) ≤LSM(θ, g(·)2)
+DKL(p(xT ) ∥π)
(8)
Other weighting functions have also yielded very good results Kingma u. a. (2021) with particular
choices of forward SDEs. However, we do not have theoretical guarantees that alternative weightings
would yield good results with arbitrary choices of forward SDEs.
Conditional diffusion models: The continuous score-matching framework can be extended to
conditional generation, as shown in Song u. a. (2020). Suppose we are interested in p(x|z), where x is
a target data and z is a condition. Again, we use the forward diffusion process (Equation 3) to obtain
a family of diffused distributions p(xt|z) and apply Anderson’s Theorem to derive the conditional
reverse-time SDE
dx = [f(xt, t) −G(xt, t)G(xt, t)T ∇xtln p(xt|z)]dt + G(xt, t)d ¯wt.
(9)
Now we need to learn the conditional score ∇xt ln p(xt|z) in order to be able to sample from p(x|z)
using reverse-time diffusion.
The conditional denoising estimator (CDE) is a way of estimating ∇xt ln p(xt|z) using the denoising
score matching approach Vincent (2011); Song u. a. (2020). In order to approximate ∇xt ln p(xt|z),
the conditional denoising estimator minimizes
1
2Et∼U(0,T )
x0,z∼p(x0,z)
xt∼p(xt|x0)
[λ(t) ∥∇xtln p(xt|x0) −sθ(xt, z, t)∥2
2].
(10)
In Batzolis u. a. (2022) it has been shown that this is equivalent to minimizing
1
2Et∼U(0,T )
xt,z∼p(xt,z)
[λ(t) ∥∇xt ln p(xt|z) −sθ(xt, z, t)∥2
2]
and that under mild assumptions sθ(xt, z, t) is a consistent estimator of the conditional score
∇xt ln p(xt|z).
4

Figure 1: Graphical overview of our method. The time-dependent encoder network eφ induces the
encoder distribution qφ(z|xt, t) ≈pt(z|xt). The data x0 is encoded with the encoder into a latent
vector z by sampling qφ(z|x0, 0). Then the reconstruction ˆx0 is obtained by running the conditional
reverse diffusion process using the approximate conditional data score sθ,φ(xt, z, t) ≈∇xt ln p(xt|z).
The model sθ,φ(xt, z, t) is obtained by adding the score of unconditional diffusion model sθ(xt, t) ≈
∇xt ln p(xt) and the score of the encoder distribution ∇xt ln qφ(z|xt, t) ≈∇xt ln p(z|xt). The latter
can be computed via automatic differentiation with respect to the input xt.
3
Method
3.1
Problems with conventional VAEs
As discussed before, VAEs model the conditional data distribution pθ(x|z) as a Gaussian distribution
with mean µx
θ(z) and covariance Σx
θ(z) learned by the decoder network dθ. Moreover, in practice it
is often assumed that Σx
θ = I. Under these assumptions maximizing the conditional log-likelihood
ln pθ(x|z) = −∥z −µx
θ(z)∥2
2
2
+ d
2 ln 2π
is equivalent to minimizing the L2 reconstruction error. There are several reasons why this model is
inadequate when dealing with certain data modalities such as images:
• Samples from pθ(x|z) would look like noisy images and therefore instead of sampling the
distribution pθ(x|z), as a principled model should, VAEs simply output the mean µx
θ(z),
which leads to undesirable smoothing and blurry samples Zhao u. a. (2017).
• It is equivalent to L2 pixel-wise error, which aligns very poorly with human perception and
semantics of the image Zhang u. a. (2018). See appendix A.2 for a detailed discussion.
3.2
Conditional Diffusion Models as decoders
To mitigate above problems and avoid making the unrealistic Gaussian assumption about p(x|z) one
can train a conditional diffusion model. Such approaches have been explored in Preechakul u. a.
(2022); Yang und Mandt (2023). The conditional diffusion model sθ(xt, z, t) is trained jointly with an
encoder network eφ : x0 7→z to approximate the conditional data score ∇xt ln p(xt|z) by minimizing
the objective 10. This signiﬁcantly improves upon original VAE framework by avoiding the Gaussian
assumption and alleviates the problem of blurry samples. However, the approach leads to complex
optimization dynamics requiring careful choices of architecture to ensure convergence.
5

3.3
Score VAE: Encoder with unconditional diffusion model as prior
We propose a further development to the above idea by leveraging Baye’s rule for scores and using the
structure of ∇xt ln p(xt|z). We can separate training the prior from training the encoder and improve
the training dynamics. By Bayes’s rule for scores we have:
∇xt ln p(xt|z) = ∇xt ln p(z|xt) + ∇xt ln p(xt)
This means that we can decompose the conditional data score ∇xt ln p(xt|z) into the data prior score
∇xt ln p(xt) and the latent posterior score ∇xt ln p(z|xt). The data prior score can be approximated
by an unconditional diffusion model sθ(xt, t) in the data space, allowing us to leverage powerful
pretrained diffusion models. The latent posterior score ∇xt ln p(z|xt) is approximated by a time-
dependent encoder network eφ(xt, t). We will discuss the details of modeling the latent posterior
score in the next section. Once we have the model for data prior and latent posterior scores we can
combine them to obtain the conditional data score. Then we obtain a complete latent variable model.
The data x can be encoded to latent representation z using the encoder network and then it can by
reconstructed by simulating the conditional reverse diffusion process using the conditional data score
∇xt ln p(xt|z).
This method has several advantages over the conditional diffusion approach, while preserving the
beneﬁts of having a powerful and ﬂexible model for p(x|z). In the conditional diffusion case the
score model sθ(xt, z, t) has to be trained jointly with the encoder network eφ. Moreover, sθ(xt, z, t)
has to implicitly learn two distributions. Firstly it has to approximate p(z|x) to understand how eφ
encodes the information about x into z and secondly it has to model the prior p(x) to ensure realistic
reconstructions. In our approach these two tasks are clearly separated and delegated to two separate
networks. Therefore, the diffusion model does not need to re-learn the encoder distribution. Instead
the prior and the encoder distributions are combined in a principled analytical way via the Bayes’
rule for score functions.
Moreover, the unconditional prior model sθ(xt, t) can be trained ﬁrst, independently of the encoder.
Then we freeze the prior model and train just the encoder network. This way we always train only
one network at a time, what allows for improved training dynamics.
Additionally, the data prior network can be always replaced by a better model without the need to
retrain the encoder.
3.4
Modeling the latent posterior score ∇xt ln p(z|xt)
The latent posterior score is induced by the encoder network. First similarly to VAEs we impose a
Gaussian parametric model at t = 0:
pφ(z|x0) = N(z; µz
φ(x0), σz
φ(x0)I)
where µz
φ(x0), σz
φ(x0) are the outputs of the encoder network. This together with the transition kernel
p(xt|x0) determines the distribution pφ(z|xt), which is given by
pφ(z|xt) = Ex0∼p(x0|xt)[pφ(z|x0)]
(11)
The above is computationally intractable since sampling from pt(x0|xt) would require solving
the reverse SDE multiple times during each training step. Therefore we consider a variational
approximation to the above distribution
qt,φ(z|xt) = N(z; µφ(xt, t), σφ(xt, t))
(12)
and learn parameters φ such that qt,φ(z|xt) ≈pt(z|xt).
The choice of the above variational family is justiﬁed by the following observations:
1. At time t = 0 the true distribution belongs to the family, since p(z|x0) is Gaussian. Moreover
since for small t the distribution pt(x0|xt) is very concentrated around x0 it is apparent from
equation 11 that pφ(z|xt) is approximately Gaussian.
2. At time t = 1 the true distribution can be well approximated by a member of the variational
family. This is because a noisy sample x1 no longer contains information about z, therefore
p1(z|x1) ≈p(z). And since we are training with KL loss p(z) will be approximately
Gaussian.
Finally, we can use automatic differentiation to compute ∇xt ln qt,φ(z|xt) which is our model for the
latent posterior score ∇xt ln p(z|xt).
6

3.5
Encoder Training Objective
Let sθ(xt, t) ≈∇xt ln p(xt) be a score function of a pre-trained unconditional diffusion model.
Let eφ : (xt, t) 7→(µz
φ(xt, t), σz
φ(xt, t)I) be the encoder network, which deﬁnes the variational
distribution qt,φ(z|xt) = N
 z; µz
φ(xt, t), σz
φ(xt, t)I

and ∇xt ln qt,φ(z|xt) which approximates
∇xt ln p(z|xt). By the Bayes’ rule for score functions the neural approximation of the conditional
data score ∇xt ln p(xt|z) is given by
sθ,φ(xt, z, t) := sθ(xt, t) + ∇xt ln qt,φ(z|xt)
We train the encoder by maximizing the marginal data log-likelihood ln pθ,φ(x). In Appendix A.1 we
show that minimizing the following training objective (with β = 1) is equivalent to maximizing the
marginal data log-likelihood ln pθ,φ(x),
Lβ(φ) := Ex0∼p(x0)
1
2E
t∼U(0,T )
xt∼pt(xt|x0)
z∼q0,φ(z|xt)

g(t)2 ∥∇xtln pt(xt|x0) −sθ,φ(xt, z, t)∥2
2

+βDKL
 q0,φ(z|x0) ∥p(z)

.
4
Experiments
We trained our method on Cifar10 and CelebA 64 × 64 using β = 0.01. We present a quantitative
comparison of our method to a β-VAE trained with the same β value in Tables 1 and 2. We present a
qualitative comparison in Figures 2, 3 and a more extensive qualitative comparison in Figures 5, 6 in
the Appendix C. To ensure fair comparison, we designed the models for our method and β-VAE with
a very similar architecture and almost the same number of parameters. More experimental details
are provided in the Appendix B. In Cifar10, ScoreVAE performs better than VAE according to both
L2 and LPIPS and generates realistic non-blurry reconstructions. The same holds true for CelebA
with the only difference that VAE achieves a slightly better L2 score. One should bear in mind that
the VAE is trained to speciﬁcally minimize L2 and that L2 is not a good metric for assessing the
performance of VAEs. The last point is explained in more detail in the Appendix A.2.
Table 1: Cifar10
L2
LPIPS
VAE (ConvNet)
3.410
0.269
ScoreVAE (ConvNet)
2.634
0.125
Table 2: CelebA 64 × 64
L2
LPIPS
VAE (ResNet)
6.97
0.217
ScoreVAE (ResNet)
7.322
0.158
5
Conclusions
In this paper, we present a method which improves the variational auto-encoder framework by using
diffusion model to avoid the unrealistic Gaussian assumption on conditional data distribution p(x|z).
We use the Bayes’s rule for score functions to show that an encoder combined with a pre-trained
unconditional diffusion model can yield a very good model for p(x|z). Thus, we show that provided
that one has access to a pre-trained diffusion model for the data distribution, one can train a VAE, by
training only an encoder by optimizing a novel lower-bound on the data likelihood, which we derived.
Our method yields improved performance compared to the vanilla β-VAE, as it yields non-blurry
consistent reconstructions.
References
[Anderson 1982]
ANDERSON, Brian D.: Reverse-time diffusion equation models. In: Stochas-
tic Processes and their Applications 12 (1982), Nr. 3, S. 313–326. – URL https://www.
sciencedirect.com/science/article/pii/0304414982900515. – ISSN 0304-4149
7

Original
ScoreVAE
VAE
Figure 2: CelebA 64 × 64
Original
ScoreVAE
VAE
Figure 3: Cifar10
[Batzolis u. a. 2022]
BATZOLIS, Georgios ; STANCZUK, Jan ; SCHÖNLIEB, Carola-Bibiane ;
ETMANN, Christian: Non-Uniform Diffusion Models. 2022. – URL https://arxiv.org/abs/
2207.09786
[Deng u. a. 2009]
DENG, Jia ; DONG, Wei ; SOCHER, Richard ; LI, Li-Jia ; LI, Kai ; FEI-FEI, Li:
ImageNet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer
Vision and Pattern Recognition, 2009, S. 248–255
[Dhariwal und Nichol 2021]
DHARIWAL, Prafulla ; NICHOL, Alex: Diffusion Models Beat GANs
on Image Synthesis. 2021. – URL https://arxiv.org/abs/2105.05233
[Higgins u. a. 2016]
HIGGINS, Irina ; MATTHEY, Loïc ; PAL, Arka ; BURGESS, Christopher P. ;
GLOROT, Xavier ; BOTVINICK, Matthew M. ; MOHAMED, Shakir ; LERCHNER, Alexander: beta-
VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In: International
Conference on Learning Representations, 2016
[Ho u. a. 2020]
HO, Jonathan ; JAIN, Ajay ; ABBEEL, Pieter: Denoising Diffusion Probabilistic
Models. 2020. – URL https://arxiv.org/abs/2006.11239
[Hyvärinen 2005]
HYVÄRINEN, Aapo: Estimation of Non-Normalized Statistical Models by
Score Matching. In: Journal of Machine Learning Research 6 (2005), Nr. 24, S. 695–709. – URL
http://jmlr.org/papers/v6/hyvarinen05a.html
[Kingma u. a. 2021]
KINGMA, Diederik P. ; SALIMANS, Tim ; POOLE, Ben ; HO, Jonathan:
Variational Diffusion Models. 2021. – URL https://arxiv.org/abs/2107.00630
[Kingma und Welling 2013]
KINGMA, Diederik P. ; WELLING, Max: Auto-Encoding Variational
Bayes. 2013. – URL https://arxiv.org/abs/1312.6114
[Kong u. a. 2020]
KONG, Zhifeng ; PING, Wei ; HUANG, Jiaji ; ZHAO, Kexin ; CATANZARO,
Bryan:
DiffWave: A Versatile Diffusion Model for Audio Synthesis. 2020. – URL https:
//arxiv.org/abs/2009.09761
[Krizhevsky 2012]
KRIZHEVSKY, Alex: Learning Multiple Layers of Features from Tiny Images.
In: University of Toronto (2012), 05
[Liu u. a. 2015]
LIU, Ziwei ; LUO, Ping ; WANG, Xiaogang ; TANG, Xiaoou: Deep Learning Face
Attributes in the Wild. In: Proceedings of International Conference on Computer Vision (ICCV),
December 2015
[Preechakul u. a. 2022]
PREECHAKUL, Konpat ; CHATTHEE, Nattanat ; WIZADWONGSA, Suttisak ;
SUWAJANAKORN, Supasorn: Diffusion Autoencoders: Toward a Meaningful and Decodable
Representation. 2022
8

[Rezende u. a. 2014]
REZENDE, Danilo J. ; MOHAMED, Shakir ; WIERSTRA, Daan: Stochastic
Backpropagation and Approximate Inference in Deep Generative Models. 2014
[Rybkin u. a. 2021]
RYBKIN, Oleh ; DANIILIDIS, Kostas ; LEVINE, Sergey: Simple and Effective
VAE Training with Calibrated Decoders. 2021
[Saharia u. a. 2021]
SAHARIA, Chitwan ; HO, Jonathan ; CHAN, William ; SALIMANS, Tim ;
FLEET, David J. ; NOROUZI, Mohammad: Image Super-Resolution via Iterative Reﬁnement. 2021.
– URL https://arxiv.org/abs/2104.07636
[Sohl-Dickstein u. a. 2015]
SOHL-DICKSTEIN, Jascha ; WEISS, Eric A. ; MAHESWARANATHAN,
Niru ; GANGULI, Surya: Deep Unsupervised Learning using Nonequilibrium Thermodynamics.
2015. – URL https://arxiv.org/abs/1503.03585
[Song u. a. 2021]
SONG, Yang ; DURKAN, Conor ; MURRAY, Iain ; ERMON, Stefano: Maximum
likelihood training of score-based diffusion models. In: Advances in Neural Information Processing
Systems 34 (2021), S. 1415–1428
[Song u. a. 2020]
SONG, Yang ; SOHL-DICKSTEIN, Jascha ; KINGMA, Diederik P. ; KUMAR,
Abhishek ; ERMON, Stefano ; POOLE, Ben: Score-based generative modeling through stochastic
differential equations. In: arXiv preprint arXiv:2011.13456 (2020)
[Stanczuk u. a. 2021]
STANCZUK, Jan ; ETMANN, Christian ; KREUSSER, Lisa M. ; SCHÖNLIEB,
Carola-Bibiane: Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein
Distance). 2021. – URL https://arxiv.org/abs/2103.01678
[Vincent 2011]
VINCENT, Pascal: A Connection Between Score Matching and Denoising Autoen-
coders. In: Neural Computation 23 (2011), Nr. 7, S. 1661–1674
[Yang und Mandt 2023]
YANG, Ruihan ; MANDT, Stephan: Lossy Image Compression with
Conditional Diffusion Models. 2023
[Zhang u. a. 2018]
ZHANG, Richard ; ISOLA, Phillip ; EFROS, Alexei A. ; SHECHTMAN, Eli ;
WANG, Oliver: The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. 2018
[Zhao u. a. 2017]
ZHAO, Shengjia ; SONG, Jiaming ; ERMON, Stefano: Towards Deeper Under-
standing of Variational Autoencoding Models. 2017. – URL https://arxiv.org/abs/1702.
08658
A
Appendix
A.1
Full derivation of the training objective
In this section we derive the maximum likelihood training objective for the encoder network. Let
sθ(xt, t) be a score function of a pre-trained unconditional diffusion model and let eφ : (xt, t) 7→
(µz
φ(xt, t), σz
φ(xt, t)I) be the encoder network. The neural approximation of the conditional data
score is given by
sθ,φ(xt, z, t) := sθ(xt, t) + ∇xt ln qt,φ(z|xt)
where qt,φ(z|xt) = N
 z; µz
φ(xt, t), σz
φ(xt, t)I

.
Recall that by variational lower bound, for any x0, z and distribution q(z|x0) we have
ln pθ,φ(x0) ≥Ez∼q(z|x0)[ln pθ,φ(x0|z)] −DKL(q(z|x0) ∥p(z))
(13)
Moreover, by (Song u. a., 2021, Theorem 3) for any x0 and z we have
ln pθ,φ(x0|z) ≥LDSM(x0, z)
(14)
where
LDSM(x0, z) := Ext∼pt(xt|x0)[ln π(xt)] −E
t∼U(0,T )
xt∼pt(xt|x0)

g(t)2 ∥∇xtln pt(xt|x0) −sθ,φ(xt, z, t)∥2
2

+E
t∼U(0,T )
xt∼pt(xt|x0)

g(t)2 ∥∇xtln pt(xt|x0)∥2
2 + 2∇xt · f(xt, t)

9

Putting both together we obtain:
ln pθ,φ(x0) ≥Ez∼q0,φ(z|x0)[ln pθ,φ(x0|z)] −DKL(q0,φ(z|x0) ∥p(z))
≥Ez∼q0,φ(z|x0)[LDSM(x0, z)] −DKL(q0,φ(z|x0) ∥p(z))
after removing terms which don’t depend on the parameters of the model and taking average over the
data-points, we obtain the following training objective
L(φ) := Ex0∼p(x0)
1
2E
t∼U(0,T )
xt∼pt(xt|x0)
z∼q0,φ(z|xt)

g(t)2 ∥∇xtln pt(xt|x0) −sθ,φ(xt, z, t)∥2
2

+DKL
 q0,φ(z|x0) ∥p(z)

It follows from the above considerations that
arg min
φ
L(φ) = arg max
φ
Ex∼p(x)[ln pθ,φ(x)]
or in other words that minimizing the objective L(φ) is equivalent to maximizing the marginal data
log-likelihood ln pθ,φ(x).
Finally just like in β-VAE we ﬁnd that in practice it is helpful to introduce a hyper-parameter
β ∈[0, 1] which controls the strength of KL regularization. We deﬁne our ﬁnal training objective as:
Lβ(φ) := Ex0∼p(x0)
1
2E
t∼U(0,T )
xt∼pt(xt|x0)
z∼q0,φ(z|xt)

g(t)2 ∥∇xtln pt(xt|x0) −sθ,φ(xt, z, t)∥2
2

+βDKL
 q0,φ(z|x0) ∥p(z)

A.2
Pixel-wise L2 is not a perceptual metric
As we discussed in the previous section choosing a Gaussian model for p(x|z) in VAEs leads to a
term in the loss function, which is equivalent to the L2 reconstruction error. While L2 is a common
dissimilarity metric it is a very bad choice for certain data modalities such as images. This is because
L2 distance measures the differences between corresponding pixel intensities, but does not take into
account human perception. Thus, two images may have a small L2 distance but still appear visually
different, or vice versa (see Figure 4). The metric does not consider the hierarchical and contextual
information that humans use when perceiving images. In particular small spatial shifts, rotations or
cropping can lead to large L2 distances even if the images are perceptually similar.
B
Experimental details
B.1
ScoreVAE
The pretrained diffusion models for all experiments are based on the DDPM architecture Ho u. a.
(2020). We used 128 base ﬁlters and attention at resolution 16 × 16 for all experiments. For Cifar10,
we set the channel multiplier array to [1, 2, 2, 2] and the number of ResNet blocks to 4. For CelebA
64 × 64, we set the channel multiplier array to [1, 1, 2, 2, 3] and the number of ResNet blocks to 2.
We used 0.1 dropout rate for Cifar10 and 0 dropout rate for CelebA. We used the beta-linear variance
preserving forward process with the same parameters as the ones used by Song u. a. (2020) and
trained the diffusion model using the weighted denoising score matching objective with the simple
weighting, i.e., λ(t) = σ(t)2, where σ(t) is the standard deviation of the perturbation kernel. We
used the Adam optimizer and EMA rate 0.999. Finally, we set the learning rate to 2e−4 for Cifar10
and 1e−4 for CelebA.
The time dependent encoder for the Cifar10 experiment is a simple convolutional network that
consists of a sequence of blocks of convolutions followed by the GELU activation function. The
ﬁnal activation is ﬂattened and concatenated to the time tensor. A ﬁnal linear layer maps the time
10

Reference image
Slightly altered,
semantically very
similar image
Image with lower
euclidean distance
from reference than
altered image
Cropping
Cropping
Cropping
Darkening
Flipping
Rotation
Figure 4: Examples where the L2-distance is not a (semantically) meaningful distance between
images. In the left column, a reference image from ImageNet (Deng u. a., 2009) is shown. In the
middle column, a slight alteration is applied, whose result a human observer would consider to be
very close to the reference image. In the right column, another image from the ImageNet data set is
displayed, which to the human observer is very different from the reference image, but which has
a lower L2-distance to the reference image than the altered image. The numbers above the images
indicate the L2-distance to the reference image. Figure taken from Stanczuk u. a. (2021).
augmented ﬂattened tensor to the latent dimension. The time dependent encoder for CelebA is based
heavily on the DDPM architecture. We removed the upsampling part of the U-NET and removed the
skip connections. The downscaled tensor is ﬂattened and mapped to the latent dimension with an
additional linear layer. We used the Adam optimizer and EMA rate 0.999. We set the learning rate to
2e−4 for Cifar10 and 1e−4 for CelebA. We trained the Cifar10 encoder for 1.4M iterations and the
CelebA encoder for 300K iterations.
11

B.2
VAE
For VAE we used exactly the same encoder architectures as in the Score VAE (except they were
not conditioned on time). For each choice of encoder we created a mirror decoder with symmetric
architecture. In Cifar10 the decoder starts by reshaping the the ﬂat latent vector into a tensor which
is then passed through a sequence of transposed convolutions which exactly mirror the structure of
the encoder. In CelebA we used a decoder consisting of the upsampling part of the DDPM U-NET.
The Cifar10 model was trained for 11M iterations, while the CelebA model was trained for 600K
iterations.
12

C
Extended qualitative evaluation
Original
ScoreVAE
VAE
Figure 5: CelebA 64 × 64
Original
ScoreVAE
VAE
Figure 6: Cifar10
13

