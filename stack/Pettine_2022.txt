Human latent-state generalization through prototype learning with
discriminative attention
Warren Woodrich Pettine1, Dhruva V. Raman2, A. David Redish3,a, and John D. Murray1,a
1Yale School of Medicine, Department of Psychiatry
2University of Cambridge, Department of Engineering
2University of Minnesota, Department of Neuroscience
aThese authors share senior authorship
December 5, 2021
Abstract
People cannot access the latent causes giving rise to experience. How then do they approximate the high-
dimensional feature space of the external world with lower-dimensional internal models that generalize
to novel examples or contexts? Here, we developed and tested a theoretical framework that internally
identiﬁes states by feature regularity (i.e., prototype states) and selectively attends to features according
to their informativeness for discriminating between likely states.
To test theoretical predictions, we
developed experimental tasks where human subjects ﬁrst learn through reward-feedback internal models
of latent states governing actions associated with multi-feature stimuli.
We then analyzed subjects’
response patterns to novel examples and contexts. These combined theoretical and experimental results
reveal that the human ability to generalize actions involves the formation of prototype states with ﬂexible
deployment of top-down attention to discriminative features.
These cognitive strategies underlie the
human ability to generalize learned latent states in high-dimensional environments.
1

Introduction
The high-dimensional sensory environment we experience is structured by underlying latent states [1, 2].
Internal models formed from these latent states must generalize to new observations and contexts.
For
example, people must not only form internal models of nutritious and poisonous fruits, but must also
generalize to all cases of discriminating between them. Previous models of latent-state learning focused
on conditions where latent states are deﬁned by the underlying reward probability associated with stimuli
whose features are all informative [2–6]. However, latent states in the world are often rules organizing the vast
number of feature dimensions we experience into lower-dimensional causal states (e.g., deﬁning a poisonous
fruit by its color and shape rather than by the position of the sun). How do people use experiences caused
by latent states to learn generalizable internal models? Furthermore, how does top-down attention ﬁlter
distracting feature dimensions when generalizing to novel latent-state examples?
The ﬁeld of category learning has proposed several models of how features can be organized into internal
states [7]. With “prototype” or “exemplar” models, new observations are compared against either an idealized
state prototype [8,9], or against individual past state examples (exemplars) [10–13]. Under both these models,
a new observation’s state membership is assigned to the state it most closely resembles. Importantly, both
prototype and exemplar states exist in a feature space structured according to the regularity of features across
past examples (prototype shown Fig. 1A, left). An alternative approach is to deﬁne a decision boundary
that separates each state in feature space [14–16]. Instead of matching a new example to the state it most
resembles, a discriminative model deﬁnes state membership by the new example’s location on a decision
boundary map (Fig. 1A, right). The experimental literature ﬁnds support, depending on task demands, for
prototype, exemplar, and discriminative strategies in category learning [7], but is largely silent on how these
internal category states are formed through experience.
To investigate the learning of latent states in high-dimensional environments, we developed an instru-
mental reinforcement learning computational framework, then designed and deployed with human subjects
a series of generalization tasks. In our framework, an agent forms prototype states and, when faced with
a decision, allocates top-down attention to feature dimensions which are informative for discriminating be-
tween the most likely states. In the ﬁrst experiment, we found that human subjects apply discriminative
attention when generalizing to novel latent-state examples. This experiment establishes that humans do
not exclusively rely on prototype states.
In the second experiment, we found that human subjects use
previously-uninformative features when generalizing to novel comparisons between learned states. This es-
tablishes that humans do not learn states exclusively deﬁned by discriminative boundaries; rather they either
learn prototype states, or memorize individual exemplars. In the third experiment, we found that humans
form prototype, rather than exemplar, states. Taken together, these experiments suggest a framework where
humans learn prototype states, then ﬂexibly attend to discriminatively-informative feature dimensions when
selecting the state that determines an action. These cognitive strategies may underlie the human ability to
learn internal states that ﬂexibly generalize to novel observations in high-dimensional environments.
Results
A model of prototype state learning with ﬂexible discriminative attention
Both the prototype/exemplar models and discriminative models predict speciﬁc failure modes when gener-
alizing action selection to novel observations or contexts. Prototype/exemplar model generalization failure
is akin to distractor sensitivity. These models consider essential for state deﬁnition any features that are
regular (i.e., consistent) across past examples – even if their regularity is incidental and not a true deﬁning
property of the latent state. When a novel example of a known latent state diﬀers from past examples in
an incidental feature, this can lead to the inappropriate creation of a new internal state (i.e., a failure to
generalize an existing internal state; Fig. 1B,C). Generalization failure in the discriminative model can result
from overly-parsimonious inclusion of which features predict a state. A discriminative model’s internal state
includes only features useful for discriminating between alternative states. An agent can learn to discriminate
latent states in separate contexts, and then encounter a novel context where all latent states are present. If
the essential discriminative feature in that novel context was non-discriminative during initial learning, the
agent can confuse the known states, thereby failing to generalize appropriately (Fig. 1D). Importantly, these
2

failure modes are complementary and dissociable: a discriminative model will successfully generalize where
a state-matching model fails, and vice versa.
These generalization failures provide insight for our development of a “prototype states with discrimi-
native attention” (ProDAtt) model that learns latent states through instrumental reinforcement learning.
ProDAtt model agents form prototype states deﬁned by the mean and covariance of the state’s past ex-
amples. When faced with a decision, ProDAtt model agents use a discriminative component to allocate
top-down attention [17–19] according to which dimensions maximally diﬀer between the prototypes states.
Attention-ﬁltered prototype states are then used to estimate the current state, which is used to select an
action.
On a given trial, a ProDAtt model agent encounters a stimulus vector (Fig. 2, top). The agent ﬁrst
uses a Bayesian surprise metric [20] to determine the distance from the stimulus vector to each known state-
prototype (less-surprising means closer). If no state is less-surprising than a threshold, the agent identiﬁes
the stimulus as a new state. If more than one state is below the threshold, the agent creates a candidate set of
states (Fig. 2(1)) and then uses individual features’ mutual information to identify feature dimensions that
maximally discriminate between the set of candidate states (“Discriminative Attention,” Fig. 2(2)) [21].
The mutual information of each feature dimension determines its discriminative attention weights.
The
attention weights are modulated by the integrated reward history, such that heightened reward volatility
increases overall attention. After scaling all feature values by these attention weights (Fig. 2(3)) the agent
recalculates the surprise metric. If multiple states are below a second surprise threshold, the agent selects
the least surprising (i.e. closest; Fig. 2(4)), left). If no state is below the second surprise threshold, the
agent creates a new state (Fig. 2(4), right). Each state learns a set of values for the available actions in
the task. Once a state has been selected, that state’s values are probabilistically used to choose an action
(Fig. 2(5)). Additional components, such as the updating of state prototype, or the integration of negative
reward history are described in the Methods.
How does the ProDAtt model balance the learning of new latent states with the generalization of known
states to novel examples or contexts? First, the model’s states are represented and used in a way that
exploits advantages of both prototype and discriminative learning: state prototypes maintain a parsimonious
memory of salient features available for generalization to novel contexts contexts, while discriminative top-
down attention ﬁlters salient features that could distract from generalization. Next, the model’s surprise
metric interacts with these mechanisms to provide a versatile threshold for estimating likely states and
creating new states. The initial surprise metric is calculated using state prototypes alone, and deﬁnes a set
of candidate states. This allows the model to consider all known states that could potentially generalize to a
novel context. The ﬁnal surprise metric is calculated after the context-speciﬁc non-discriminative dimensions
have been removed. Therefore, depending on the context the same stimulus could either generalize to an
existing state or prompt creation of a new state. Finally, reinforcement learning feedback allows the ProDAtt
model to infer the latent states in the environment by grouping stimuli with similar causal statistics.
Previous latent-state models struggle with multi-state instrumental learning
Having identiﬁed speciﬁc failure modes for diﬀerent approaches to latent-state learning and created a model
that bridges these approaches, we compared the ProDAtt model to four previous latent-state learning models
[2,3,5,6].
We tested the models using two basic multi-state instrumental learning tasks. The ﬁrst task involved
two stimuli and two actions, where each stimulus was deterministically rewarded for a distinct action (Fig.
S1A, top). The second task involved two latent states (with two stimuli each) organized according to latent
rules. The latent states were again deterministically rewarded for distinct actions (Fig. S1A, bottom).
We found that all previous models struggled to learn even the single-stimulus state task. The previous
models designed for Pavlovian learning performed at chance [5,6], while the models designed for bandit tasks
performed just above chance [2,3]. The ProDAtt model, however, quickly learned both tasks and performed
without errors (Fig. S1B and C). This demonstrates previous latent-state learning models are insuﬃcient
for instrumental learning tasks which the ProDAtt model quickly masters (see Discussion).
3

Experiment 1: Subjects use discriminative attention
We hypothesized that the ProDAtt model captures key aspects of human behavior during multi-feature
latent-state learning. To test this hypothesis, we used ProDAtt model to design a series of tasks which
produce speciﬁc model predictions for choice behavior during learning and generalization. We characterized
performance on these tasks using multiple computational models, including the ProDAtt. We also imple-
mented them in online behavioral experiments for human subjects recruited through Amazon Mechanical
Turk (see Methods). Finally, we compared choice behavior from subjects to the predictions of diﬀerent
models.
Given that previous models of latent-state learning generally lacked a discriminative component, we
designed Experiment 1 to determine the necessity of the discriminative mechanism, and whether it is used
by human subjects. A pure prototype model that deﬁnes states based on the regularity of feature dimensions
can fail to generalize if a new observation varies in a highly-regular but non-informative dimension (Fig. 3A,
left). In the same scenario, a discriminative boundary should facilitate easy generalization (Fig. 3A, right).
To investigate this prediction, we designed an action generalization reinforcement learning task. On each
trial a human subject via mTurk or simulated agent encounters a stimulus and chose between one of four
actions (Fig. 3B, left). After a subject selects an action, they either receive a reward (green crystal; Fig. 3B,
middle/top) or a reward is omitted (empty crystal; Fig. 3B, middle/bottom). In the ﬁrst block, subjects
learn state-action reward contingencies. Subsequently, uncued, subjects enter a generalization block, with
novel stimuli that diﬀer in a non-discriminative dimension (sequence in Fig. 3B, right). The latent states
are conﬁgured in such a way that one dimension (color) deﬁnes the ﬁrst latent state, a separate dimension
(shape) deﬁnes a second latent state, and two dimensions (color and shape) deﬁne the third. All initially
learned stimuli have the same texture (Fig. 3C, left; Table 1). Each of these three latent states are rewarded
for a distinct action (A, B or C). A fourth action (D) is left unrewarded to measure exploratory behavior.
The novel stimuli introduced in the generalization block are identical to the previous stimuli, except for their
texture. These novel stimuli are members of the same latent states as their pairs in the initially learned
stimuli (Fig. 3C, right; Table 1).
We used the feature dimensions of this experiment’s simple stimuli as a proxy for how subjects deﬁne
states. For example, a pure prototype model would theoretically value features by regularity across past ob-
servations. Thus, a theoretical model of pure prototype state formation predicts that the non-discriminative
dimension (texture) separates initially learned stimuli from novel generalization stimuli (Fig. 3D, left). In
contrast, a model with a discriminative component ignores the non-discriminative dimension when deter-
mining the feature distance between stimuli. Thus in the decision process, initially learned stimuli and their
associated novel generalization stimuli overlap in feature space after attention is applied (Fig. 3D, right).
This not to say subjects fail to perceive the non-discriminative dimension, but rather a discriminative com-
ponent removes it from ﬁnal evaluation.
We deployed the task with human subjects (n = 71) and with two simulated variations of our latent-state
learning model. The ﬁrst variant was the ProDAtt model (as described in Fig. 2). The second variant was
a pure prototype state model – that is, it lacked discriminative attention. This allowed us to investigate if a
discriminative component improves generalization ability relative to a model that purely relies on prototype-
matching.
To characterize generalization from choice behavior, we deﬁned two generalization metrics. During the
initial learning block, action D was never rewarded. Furthermore, during the initial learning block the other
dimensions (color and shape) each uniquely formed states associated with speciﬁc actions (A and B). In
this action generalization task, subjects learned state-action associations through exploration. If subjects
questioned whether the novel generalization stimuli (with diﬀerent texture) were distinct from the initially
learned states, this could manifest as increased exploration of action D (Fig. 3E, left).
Similarly, if subjects failed to fully generalize from initially learned stimuli to novel generalization stimuli,
we expected an increased error rate for novel generalization stimuli. To quantify this, we compared the error
rate between learned and novel stimuli that diﬀered only along the non-discriminative dimension (texture)
(Fig. 3E, right). A meaningful diﬀerence in error rates would indicate the subject failed to fully apply their
initially learned states to novel examples.
We found the ProDAtt model, prototype state model and human subjects learned state-action contin-
gencies over the course of the initial block (Fig. 3F, left column). During the generalization block, human
4

subjects and the ProDAtt model performed above chance on novel stimuli from the earliest trials; however,
the prototype state model fell to chance on novel stimuli and needed to learn them over the course of the
block (Fig. 3F, right column). While it took the models signiﬁcantly more trials to learn during the initial
block (approximately 400 versus 140 for human subjects), this is to be expected given the required “burn-in”
trials (n = 75) for the learning of each state, as occurs in previous models [2,3,5,6].
Successful generalization by human subjects and the ProDAtt model was reﬂected in our two generaliza-
tion metrics (Fig. 3G). Human subjects explored action D signiﬁcantly less than chance (BF=1.832e+32,
p<0.001; Fig. 3G, right margin). While the proportion of errors on novel stimuli was signiﬁcantly greater
than that for initially learned stimuli (p<0.001), the Bayes factor was not very strong (46.295), and the
eﬀect size was small (0.3966; Fig. 3G, top margin). Taken together, these results provide strong support for
subjects utilizing discriminative attention when deciding between states.
Experiment 2: subjects form states that incorporate non-discriminative feature
dimensions
Having established that human subjects use discriminative attention during state estimation, the natural
next question is whether they learn states exclusively deﬁned by discriminative boundaries. Furthermore, if
states are deﬁned by discriminative boundaries and two states only partially overlap in deﬁned boundaries,
which set of boundaries are used to select between them?
To investigate these questions, we designed a set generalization task where latent states must be learned
separately (diﬀerent sets), and then combined.
To successfully generalize when the sets are combined,
subjects must attend to a feature dimension that was non-discriminative when the sets were initially learned
(Fig. 4A). That is, the conﬁguration of features is such that a purely discriminative model of latent-state
learning will fail to assign weight to a feature that is later essential for discriminating between the two sets.
The set generalization task in Experiment 2 is structured in three blocks (Fig. 4B). During the ﬁrst
block, subjects learn Set 1 latent states separable by two dimensions (color and shape), each rewarded for
a distinct action (action A for magenta circles and action B for orange squares). After block 1, subjects
are instructed to put aside what they had learned and to learn entirely new states along with entirely new
actions (action C and action D). In block 2, Set 2 latent states are distinguished only by single dimension
(shape: circle with action C, and square with action D). Finally, during block 3 generalization, subjects
are instructed that all actions are available, and they will encounter all previously learned stimuli (stimuli
described in Table 2).
There are two ways subjects using a discriminative model could apply set boundaries during generaliza-
tion. Under the ﬁrst discriminative model, we assume that subjects evaluate just dimensions informative
for both sets (in this case, shape; Fig. 4C, left). Under the second discriminative model, we assume that
when considering a stimulus’ state membership, subjects use the discriminative boundary for the state in
question. By this, we mean that when subjects evaluate if a stimulus belongs to a state learned during Set
1, they evaluate both dimensions composing the boundary learned during Set 1 (color and shape). Then,
when evaluating if a stimulus belongs to a state learned during Set 2, they evaluate the single dimension of
the boundary learned during Set 2 (shape; Fig. 4C, middle).
If subjects purely form discriminative states, they will exhibit a speciﬁc error type we call a “discriminative
error.” This error occurs when, due to exclusion of previously non-discriminative feature dimensions, the
model feature distance between two perceptually distinct stimuli is zero (Fig. 4D). In contrast, a prototype
model relies on feature regularity to allocate feature weight. Thus, it incorporates the highly-regular, non-
discriminative dimension (texture) into its state deﬁnitions. This results in clear separation between states
of each set (Fig. 4C, right), which in turn facilitates block 3 generalization.
To simulate a pure discriminative model, we built an actor-critic reinforcement learning framework,
with a three-layer neural network constituting the actor and critic (see Methods). This neural network is
capable of learning, through simple action-reward feedback, nonlinear mappings between inputs and actions.
Furthermore, as the dimensionality of the intermediate layer is an order of magnitude higher than the
dimensionality of the inputs, it is possible that the network could learn to represent states as linearly
separable in novel contexts [22,23]. Though we expected the readout layer to selectively weigh discriminative
dimensions, the use of a neural network allowed for alternative possibilities.
To simulate prototype learning, we used two versions of the ProDAtt model.
The ﬁrst version was
5

identical in all parameters and mechanisms to that of Experiment 1. When this version generalized perfectly,
we modiﬁed it slightly to increase confusion between similar internal states (see Methods).
Examining the learning curve, we found the neural network was able to learn both sets over the course of
blocks 1 and 2. When transitioning from block 1 to block 2, there was a brief dip below chance level before
the neural network learned Set 2. This was due to the neural network carrying assumptions from Set 1 into
the second block, which it then had to update as it learned eﬀective Set 2 state-action associations. When it
transitioned to the generalization block, there was again a transient, above chance dip in performance before
it recovered to the same peaks levels seen in block 1 and block 2 (Fig. 4E, top).
The ProDAtt model without increased state confusion learned the Set 1 states during block 1. It also fell
below chance after the block 2 transition as it formed new internal states corresponding to the two new latent
states. During the transition to block 3, there was no dip in performance, indicating perfect generalization
(Fig. 4E, middle).
The ProDAtt model with increased state confusion learned the ﬁrst two sets exactly the the same as the
model without increased state confusion. This is to be expected, as their parameters were identical during
those blocks. During block 3 (when the increased state confusion changes were implemented), there was a
drop in performance (Fig. 4E, middle).
Human subjects (n = 43) also learned both sets perfectly, with a brief below-chance dip in performance
when transitioning from block 1 to block 2. Importantly, during generalization, human subjects showed a
dip in performance, followed by prolonged reduced performance (Fig. 4E, bottom). Reduced performance
during block three (when all four actions were accessible) indicates some form of cognitive constraint on task
performance (e.g., working working capacity, or stimulus-state mapping precision).
To better quantify generalization performance, we calculated the proportion of errors during each stim-
ulus’ ﬁrst block 3 appearance. As was reﬂected in the learning curves, we found all models and human
subjects generalized better than predicted by chance (one-sample t-test p<0.001; Fig. 4F)
We then examined the proportion of discriminative errors – those the task was designed to elicit if
subjects use a discriminative model for state deﬁnition. Only the neural network produced a signiﬁcant
number of discriminative errors during generalization (one-sample t-test, p<0.001). To emphasize, human
subjects produced no more discriminative errors than expected by chance (p>0.05; Fig. 4G). This provides
ﬁrm evidence that human subjects do not form states based on purely discriminative dimensions.
We then analyzed human subjects’ patterns of errors across each stimulus, and how they compared to
each theoretical model’s predicted feature distance. From each theoretical model’s feature subspace for each
state (Fig. 4C), we estimated the feature distance between stimuli according to that model’s subspace (see
Methods). In addition to the discriminative and prototype models described, we also considered the case
where subjects memorized each stimulus (feature distance matrices in Fig. S2A). Using each model’s feature
distance matrix, we then inferred a predicted confusion matrix for errors produced by each model (Fig. S2B;
see Methods).
For the analysis of error patterns, we started by inspecting speciﬁc stimulus errors and how they compared
to qualitative model predictions. If humans subjects use the ﬁrst discriminative model, the distribution of
overall errors and discriminative errors should be uniform. Examining the overall pattern of errors, we found
that subjects displayed signiﬁcantly diﬀerent proportions of errors across stimuli (ANOVA F=46.8, p<0.001;
Fig. S2C).
We then examined how the overall pattern of errors corresponded to error patterns predicted by theoretical
models. To assess this, we regressed observed confusion matrices against each theoretical model’s predicted
confusion matrices (scatter plots in Fig. S2D). We then used the residual variance (R2) as an indication of
ﬁt quality (see Methods). To compute a p-value, we generated a null distribution by permuting stimulus
rows of the confusion matrix (see Methods). Though regression of the neural network confusion matrix by
all theoretical models was signiﬁcant (p<0.05), the R2 values were higher for the discriminative models (ﬁrst
discriminative model R2 = 0.924, second discriminative model R2 = 0.847) than for the prototype model
(R2 = 0.434), or stimulus-memorization model (R2 = 0.30; Fig. S3A, top; Fig. 4I, top). This indicates that,
as expected, the confusion matrix expressed by the neural network was best ﬁt by discriminative models.
Fits of the confusion matrix produced by the ProDAtt model with increased state confusion was signiﬁcant
only for the theoretical prototype model, and for memorization of individual stimuli (p<0.001 for both;
Fig. S3B, top; Fig. 4I, middle). The R2 values for the prototype (R2 =0.961) and stimulus-memorization
(R2 = 0.956) models were close, providing an initial indication that this experiment struggled to diﬀerentiate
6

between those models.
For human subjects, we found neither of the two discriminative-only models produced an observed R2
greater than chance (p>0.05). However, regression coeﬃcients for the prototype model’s predicted confu-
sion matrix (R2 = 0.949) and the confusion matrix from stimulus-memorization (R2 = 0.879) were both
signiﬁcantly better than chance (p<0.05; Fig. S2E; Fig. 4I, bottom).
This set generalization experiment was designed to assess if subjects learn states purely deﬁned by
discriminative feature dimensions, and if so the speciﬁcs of which discriminative boundaries are used. Given
that subjects’ overall level of discriminative errors was at chance and the regression of subject error patterns
against predictions of both discriminative models was insigniﬁcant, these experimental results support that
learning of states is not done by purely discriminative boundaries. Even the richer hypothesis regarding
discriminative state formation (involving two features) did not account for human performance. Thus, the
states that subjects learn must involve some sort of regular, non-discriminative features.
However, this
experiment is equivocal as to whether subjects form prototype states, or memorize individual stimuli.
Experiment 3: subjects form prototype, not exemplar states
Experiment 1 found that subjects use discriminative attention when selecting between states; Experiment
2 found that subjects learn states deﬁned by commonly encountered, but non-discriminative feature dimen-
sions. These experiments leave open the question, however, of whether human subjects form idealized state
prototypes or remember individual state examples. For Experiment 3, we sought to diﬀerentiate between
the latter possibilities.
There is considerable debate in category learning as to whether, or in what task settings, humans compare
new observations to prototypes, or to individual category exemplars [9–13,24–29,29–32]. Under a prototype
model, the distance to a new observation is calculated relative to a single state prototype for each state.
Under an exemplar model, the distance of a new observation is calculated for every prior state example. The
observation’s distance from the state is a combination of individual distances (Fig. 5A).
To investigate exemplar state learning, we developed the exemplar states with discriminative attention
(ExDAtt) model. To do this, we modiﬁed the ProDAtt model by replacing the surprise calculation for a
prototype with a surprise calculation for each state-exemplar (see Methods). We found the ExDAtt model
was able to learn the action generalization task of Experiment 1 and that the discriminative component
was also necessary for seamless generalization (Fig.
S4A). Furthermore, the ExDAtt model learned the
set generalization task of Experiment 2 and the increased state confusion modiﬁcations were necessary to
generate generalization errors (Fig. S4B). The ExDAtt model with increased state confusion generalized
to initial encounters, and produced no more discriminative errors than predicted by chance (Fig. S4C).
Though confusion matrix regression was signiﬁcant for the second discriminative model, along with the
prototype and stimulus-memorization models, the R2 values for the prototype (R2 = 0.952) and individual
stimulus memorization models (R2 = 0.932) was much greater than that for the second discriminative model
(R2 = 0.203; Fig.
S4D). These results demonstrate exemplar latent-state learning is possible, and our
algorithm is a reasonable model for how it occurs.
In Experiment 3, we modiﬁed the conﬁguration of stimuli in the set generalization task to produce
predictions that distinguish a prototype state model from a model utilizing individual exemplars (Fig. 5B;
Table 3). Speciﬁcally, features are arranged such that, for stimuli within a latent state, a prototype model’s
feature distance is equal between stimuli, while an exemplar model’s feature distance is unequal (Fig. S5A).
This creates an “exemplar diﬀerence” in errors predicted by the prototype and exemplar models (Fig. 5C).
Importantly, actions producing exemplar diﬀerences are distinct from actions associated with discriminative
errors. Thus, we can investigate both phenomenon in the same task.
Similar to Experiment 2, all computational models (neural network , ProDAtt model and ExDAtt model)
and human subjects (n = 40) performed better than chance when ﬁrst encountering stimuli during block 3
(t-test p<0.001; Fig. 5D; Fig. S5B). This indicated they all learned some state representation of the task
and carried that into the ﬁnal generalization setting.
Again, similar to Experiment 2, only the neural network produced a signiﬁcant number of discriminative
errors (t-test for neural network p<0.001; ProDAtt model, ExDAtt model and humans p>0.05; Fig. 5E). The
modiﬁed set generalization task thus also can determine if subjects use discriminative state representations.
We then examined the exemplar diﬀerence results (Fig. 5C). As expected, the neural network showed
7

no signiﬁcant exemplar diﬀerence (0.015, t-test p>0.05), and the ExDAtt model produced a signiﬁcant
exemplar diﬀerence (0.037, p<0.001). Surprisingly, we found the exemplar diﬀerence was also signiﬁcant for
the ProDAtt model (0.058, p<0.001). The ProDAtt model uses the covariance of features across past trials
to identify feature regularity. Given its method of calculating covariance, the ProDAtt model did not entirely
ignore irregular features (e.g. shape in the state associated with action A). That fact made the ProDAtt
model slightly biased towards confusing stimuli based on their individual features. While it suggests that
the ProDAtt model will beneﬁt from more sophisticated estimation of the covariance structure, it also gave
us conﬁdence in the sensitivity of the exemplar diﬀerence metric.
Applying the exemplar diﬀerence metric to human performance, we found no signiﬁcant eﬀect (0.01,
p>0.05). To emphasize, examining the speciﬁc error type that motivated task design, human subjects were
not signiﬁcantly biased in favor of memorizing individual stimuli, or of forming exemplar states.
We then ﬁt the theoretical confusion matrices (Fig. S5G) to the simulated models’ and human subjects’
behavior. Theoretical model ﬁts to the simulations were as expected, with the neural network best ﬁt by the
discriminative model, the ProDAtt model by the prototype model and the ExDAtt model by the exemplar
model (Fig. 5G; Fig. S6A-C).
When ﬁtting human subject confusion matrices, only the theoretical prototype and exemplar R2 values
were above chance (p<0.001; S6D). The magnitude of the theoretical prototype model R2 was much greater
than the exemplar (R2 = 0.878 versus R2 = 0.714; Fig. 5G, right Fig. S5D). Therefore, assuming errors are
inversely proportional to the feature distance, human behavior is best ﬁt by a prototype model.
Experiment 3 was designed to diﬀerentiate whether subjects form prototype states or exemplar states
(similar to memorizing individual stimuli). Results from our analyses support the hypothesis that human
subjects learn prototype states and do not memorize exemplars.
Discussion
We found that human generalization of learned actions involves both the formation of prototype states
and ﬂexible application of discriminative attention.
Moreover, the ProDAtt model provides a plausible
mechanistic account of this process.
Previous latent-state models recapitulate key phenomena of Pavlovian learning and single-active-state
instrumental learning [2,3,5,6]. Why then do some fail entirely on multi-state instrumental learning, while
others show promise?
First, the previous models learned states with identical sensory information, but
unique (latent) reward statistics. While the ProDAtt model is amenable to such tasks, here we investigated
tasks where the latent states are deﬁned by rules organizing a high-dimensional environment. Next, previous
models able to perform multi-state instrumental learning were initially designed for bandit tasks, and (just as
the ProDAtt model) condition action values on the selected state [2,3]. That is, successful models engage in
a two-step process where in the ﬁrst step agents estimate a state, and in the second step the selected state’s
action values drive choice. As a consequence, when agents receive a reward or a reward is omitted, they
can easily attribute that result to a speciﬁc state – facilitating the formation of distinct states. Models that
failed the basic instrumental learning task were designed for Pavlovian learning and follow a Bayes-optimal
choice strategy where the action values used in the ﬁnal choice process are the combination of action values
from multiple states, weighted by each state’s likelihood [5, 6]. That is, agents compute the likelihood of
each state, and then the ﬁnal action values are a weighted combination of each states’ action values. As
implemented in the previous models, this led to diﬃculty attributing action outcomes to unique states.
Given the small number (eight) and relatively simple stimuli used in Experiments 2 and 3, and that
performance at the end of the initial two blocks reached 100%, subjects could have easily adopted a strategy
where they memorized the category membership of each individual stimulus. Indeed, a number of experi-
mental studies support this exemplar approach [7]. Moreover success of the ExDAtt model demonstrated
this strategy is a mechanistically viable form of latent-state learning. However, subjects in Experiment 3
showed a strong bias towards prototype learning, not memorization of exemplars.
This is likely due to
the memory load of the set generalization tasks. Stable performance noticeably fell during the third block
(generalization) when subjects had access to all four actions and encountered all eight stimuli. A common
issue for exemplar models is the memory required to maintain a large bank of every previously encountered
example [33,34]. As the environment becomes richer, the memory requirement increases drastically. A state
8

prototype, however, can be represented by summary statistics. Thus, a prototype model of latent-state
learning has lower memory demands. Our results support that cognitive constraints push subjects towards
learning state prototypes, rather than memorizing each encountered example.
Of particular relevance to neuroscience are the ProDAtt model’s predictions for how top-down attention
shifts as subjects form states, identify candidate states, and make a ﬁnal decision. Many groups have found
representation of state information in brain areas such as the orbitofrontal cortex [35–44], medial prefrontal
cortex [45–48] and hippocampus [40,49–54]. The ProDAtt speciﬁcally predicts that top-down attention to
state variables will be nonspeciﬁc during early encounters with state examples and will narrow to highly-
regular feature dimensions as states are formed. Moreover, neural activity will rapidly shift based on the set
of possible states from which a subject can select. That is, a state example will elicit diﬀerent patterns of
top-down attention depending on the alternatives available to a subject.
Our ﬁndings and models are also highly relevant to altered learning and decision-making in psychiatric
disorders. Processes such as state formation, state-selection and generalization are key to the symptomology
of multiple disorders. In schizophrenia, there is a clear misuse of the statistical regularities of latent states
in the environment to form internal states. This could be due to either an issue with state creation, or the
attribution of action rewards [55–57]. Our computational framework can be used to design and interpret tasks
investigating both possibilities. Individuals with obsessive compulsive disorder might eﬀectively generate
initial states, but struggle to update their state representations and action values over time [58–60]. These
scenarios are mechanistically separable in our framework. Finally, the behavioral rigidity observed in autism
spectrum disorder may result from deﬁciencies in prototype formation [61] or use of top-down attention [62].
As demonstrated here, behavioral tasks can be designed to investigate potential mechanisms giving rise to
these phenotypes.
There are two signiﬁcant limitations of our work. First, our models calculate feature distance by assuming
a linear mapping between our deﬁnition of stimulus features and the features subjects use to create states.
That is, the models assume human subjects use color and shape to deﬁne states, whereas subjects could form
states based on a near inﬁnite number of external or internal factors. With this in mind, we intentionally
deﬁned latent states using basic feature primitives (size, color, shape and texture) commonly used to describe
objects in the world. The high ﬁdelity with which models predicted subject data also gave us conﬁdence
that subjects either use the assumed features, or use features that correlate highly with those assumed.
The next limitation pertains to our models’ use of two ﬁxed surprise thresholds: one to generate a
candidate list of states, and one for ﬁnal state estimation.
Yet, it is highly likely these thresholds are
task-dependent or vary across individuals.
Indeed, treating these thresholds as free variables may have
improved the correspondence between model and human behavior.
However, to demonstrate the same
model can perform tasks requiring either discriminative boundaries or prototype formation, we ﬁxed the
surprise thresholds across all simulations. Given the impact of these thresholds on state formation, selection
and observable behavior, future work could investigate the eﬀects of changing surprise thresholds, and
mechanisms by which thresholds are dynamically tuned.
In this work, we proposed, and validated in human subjects, a theoretical framework by which subjects
generalize through learning prototype latent states and ﬂexibly applying discriminative attention.
Our
results suggest future investigations in the ﬁelds of learning theory, machine learning, systems neuroscience
and psychiatry.
Acknowledgments
This research was partly supported by NIH grants R01MH112746 (JDM), R01MH112688 and P50MH119569
(ADR), as well as by an appointment to the Intelligence Community Postdoctoral Research Fellowship Pro-
gram at Yale University, administered by Oak Ridge Institute for Science and Education (ORISE) through
an interagency agreement between the U.S. Department of Energy and the Oﬃce of the Director of Na-
tional Intelligence (ODNI) (WWP). We thank Vishwa Gouder, Adriana Di Martino, and Daniel Ehrlich for
comments and discussion.
9

Methods
Previous latent state models
We tested several previous models of latent-state learning. These included the Redish 2017 model [2], the
Gershman and Niv 2012 model [3], the Gershman et al. 2017 model [5] and the Cochran and Cisler 2019
model [6]. To implement these models, we used a code package made publicly available by Cochran and
Cisler [6].
Prototype states with discriminative attention model
Overview
Our prototype states with discriminative attention (ProDAtt) is an extension of a previous
latent-state reinforcement learning model used to study the phenomenon of extinction and renewal [2]. The
ProDAtt balances the learning of internal agent states (s) that represent latent-states in the environment
with the learning of actions within those states (Q(s, a)). To accomplish both state estimation and new-
state creation, it uses a prototype-clustering method where newly encountered cue examples (c(t)) are either
classiﬁed as a member of the least-surprising state (a.k.a. cluster), or if the surprise for any state prototype
(µ) exceeds a threshold (υ), it forms a new state. The clustering is a two-step process. In the ﬁrst step, a set
of candidate states ({i ∈S}) are identiﬁed as any whose surprise does not exceed a certain threshold (υC).
In this stage, features are weighed using only the prototype precision matrix (IP ). The mutual information
(MI) of each feature is then computed according to a feature’s capacity for maximally diﬀerentiating the
candidate states. This MI is used to create a set of top-down attention weights (IMI). The use of attention
is modulated by reward history (¯δ) such that consistent deviation from expected reward increases the overall
level of attention to features in the environment. After modulation, the attention weights are applied and
surprise is recalculated.
Each state then learns its own set of action-values through a Rescorla-Wagner
update. Our overview is expanded in what follows.
Identifying a candidate list of states
When presented with a stimulus example vector c(t), the agent ﬁrst compares it to the bank of known states
using the equation,
Zi(c(t)) = (c(t) −µi),
(1)
where µi is the prototype for the speciﬁc state. It then uses the vector Zi(c(t)) to calculate a single radial
distance from the internal agent-state prototype using,
D2
i (c(t)) = Zi(c(t))′IP,iZi(c(t))
(2)
where D2
i (c(t)) is the radial distance and IP,i is the precision matrix (inverse of covariance Σi) for that
state. Given there is no history dependence in the states of the task, we assumed a uniform prior over the
probability of observing speciﬁc internal-states. Thus,
P(si|c(t))P(c(t)) =
1
p
sπnc|Σi|
exp

−1
2D2
i (c(t))

,
(3)
and
P(c(t)|si) ∝P(si|c(t))P(c(t)).
(4)
It then calculates the surprise index by taking the negative log of each posterior such that,
F(si) = −ln(P(c(t)|si)).
(5)
The agent compares the level of surprise for each state to a ﬁxed threshold υC = 184. States whose surprise
is below the threshold are included in the candidate set of states. This threshold was determined empirically
then held consistent through all simulations.
10

Creation of new states
If, at any stage of the process, no state’s surprise is below the speciﬁed threshold, the agent creates a new
state:
N = |{i : F(si) < υ}|
if N < 1, create new state.
(6)
When a new state is created, its precision matrix IP is initialized to 25 along the diagonal. The initial µi
is centered on the novel observation. New internal-states are given a “burn in,” time of n trials (n = 75)
before these values are updated as described in section .
Discriminative attention
If the set of candidate states is longer than one, the agent performs a discriminative step to identify a new set
of attention weights. This is done by calculating the MI of each individual cue, k. First the agent calculates
the entropy of cue k for each state {i ∈S} using the equation,
H(Ci,k) = p(si)
X
t∈si
p(ck)log2
 p(ck(t))

,
(7)
where Ci is a matrix of all past trials’ (t) cue vectors in the memory of state si, and Ci,k is a slice of feature
k for each example. The agent then calculates the total entropy of each cue with the equation,
H(Ck) =
X
t
p(ck)log2
 p(ck(t))

,
(8)
where Ck is a vector containing all memories of feature k across every trial t for states {i ∈S}. The agent
then uses these values to calculate the MI of each cue with equation,
IMI = I(Ck, S) = H(Ck) −
X
i
H(Ci,k).
(9)
IMI is a vector containing the MI for each feature across states {i ∈S}. Prior to incorporation into the
surprise calculation, the agent modulates the MI by the reward history (¯δ, described in Section ). The logic
is that, if the current set of internal agent states do not accurately correspond to the latent-states in the
world, attention to all cues should increase. This modulation is accomplished through the equation,
wA =

1 −min
 1, min(0, ¯δ −ξDB,floor)/(ξDB,ceil −ξDB,floor)

∗IMI
+ min
 1, min(0, ¯δ −ξDB,floor)/(ξDB,ceil −ξDB,floor)

,
(10)
where the slope ranges from 1
2 to 1 within the range of (ξDB,floor, ξDB,ceil), and the intercept is restricted
by (ξDB,floor, ξDB,ceil). In all simulations, ξDB,floor = −4 and ξDB,ceil = −12.
Final state estimation
The discriminative attention weights are incorporated by replacing Equation 1 with,
Zi(c(t)) = wA(c(t) −µi),
(11)
where wA is multiplied element-wise. The surprise is then calculated as described in the rest of Section .
For ﬁnal estimation, however, the surprise threshold was empirically determined and ﬁxed at υF = 2.3. If
more than one internal state is less than the surprise threshold, the agent estimates a state through use of
the softmax decision function,
P
 select state s(t)|P(c(t)|si)

= exp(βSP(c(t)|si)/
X
i
exp(βstateP(c(t)|si),
(12)
where βS governs the explore/exploit trade-oﬀamong states. After initial tuning, βS was ﬁxed at 50.
11

Action selection and value updating
Once the agent selects a state s(t) for a given trial, the agent then uses a softmax function over action-values
to select among actions:
P(select action a|s(t)) = exp(βV alueQ(s(t), a))/
X
a
exp(βvalueQ(s(t), a)).
(13)
The balance of explore/exploit among actions Q(s(t), a) is dictated by βV alue (ﬁxed at 15). After selecting
an action, the agent is reward R(t) = 1, or the reward is omitted R(t) = 0. The agent then calculates a
reward prediction error (RPE) using the basic Rescorla-Wagner rule,
δ = R(t) −Q(s(t −1), a).
(14)
It updates the state action-value according to,
Q(s(t), a) = Q(s(t −1), a) + ηδ.
(15)
Importantly, the agent tracks the running negative reward history using a fast timescale variable ¯δfast
and a slow timescale variable ¯δslow.
This functionally enables the agent to distinguish between volatile
environments and stable environments with low reward probabilities. Both values are computed using the
equation,
¯δ = ξ0¯δ(t −1) + ξ1⌈δ⌉,
(16)
where ξ0 is a leak term, and ⌈δ⌉is an integration term for the negatively rectiﬁed RPE. To compute ¯δfast
and ¯δslow, we ﬁrst set a baseline timescale using ξ0baseline = 0.99, and ξ1baseline = 1.50. To shift the timescale
while maintaining the asymptote, we used a variable γ implemented as,
ξ0shift = ξ0 + γ
ξ1shift = ξ1 ∗(1 −ξ0shift)/(1 −ξ0).
(17)
We then calculated the fast and slow timescales using, γfast = −0.009, and γslow = 0.004. ¯δeffective is then
determined trial-by-trial with,
¯δeffective = ⌈¯δfast −¯δslow⌉.
(18)
Updating of state prototype
When the agent is rewarded, it updates the prototype of selected state si with the new example. If si had
been selected for fewer than n trials, only the action-values Q(si, a) are updated. After n trials, the prototype
vector µi is updated with a simple averaging of prior internal-state observations j ∈Ci (going back n trials).
The matrix Ci contains only those trials classiﬁed as internal-state si that were rewarded. The prototype
cue vector µi is calculated with the equation,
µi = 1
n
t
X
t−n
Ci(t).
(19)
Feature dimension prototype weights are determined by calculating the covariance of internal-state cue
observations matrix C. This is done with the equation,
Σi =
1
n −1
t
X
t−n
(Ci(t) −µi)(Ci(t) −µi)′.
(20)
The precision matrix IP,i is then calculated from the covariance using,
IP,i = Σ−1
i .
(21)
12

Prototype state model
In the initial experiment, a model is presented that does not include the discriminative attention step as
described in Section . Instead, Equation 1 is used in the decision process described in Section . The algorithm
is otherwise identical to the ProDAtt.
Increased state confusion
We increased the level of state confusion two ways. First, to encourage greater stochasticity in state estima-
tion, we set βS = 4. However, this modiﬁcation of the inverse temperature alone would only increase overall
randomness. To encourage a bias in errors towards states the model considered more similar, we used the
equation,
di(t) =
 (1 −ξbl)(c(t) −µi) + ⃗ξbl
ξbe,
(22)
where ξbl linearly blurs the state representation, thereby decreasing memory precision. The nonlinearity
provided by ξbe stratiﬁes the distance such that more-similar states are perceived as closer. In the prototype
model, ξbl = .77 and ξbe = 2. Variable di(t) then replaces (c(t) −µi) in Equations 1 and 11. Furthermore,
as agents are instructed that no new states are available in block 3, the state creation process is turned oﬀ.
Exemplar state with discriminative attention model
To create an exemplar states with discriminative attention (ExDAtt), we replaced the radial distance calcu-
lation as described in Section . When doing so, we looked to concepts described in the generalized context
model (GCM) [10–13], a commonly used exemplar model of categorization. In this model, a distance metric
is computed between a new stimulus and each example of a category. A nonlinearity is applied to amplify
that distance and then category distance is determined by averaging its individual example distances.
Agents in our tasks encounter noisy representations of stimuli. To induce robustness to small amounts
of noise, exemplar agents take the absolute value of prior examples and use that to group prior trials by
example. This set of trials (with noise included) is used to calculate a µi,j and precision matrix IP,i,j for
each example. To ensure stability in the calculation of the precision matrix, IP,i,j is only computed if an
example had been encountered more than 15 times. Otherwise, the precision matrix is set to the new-state
default value. For similar reasons, the trial history is increased from n = 75 (for prototype) to n = 100.
Agents then use µi,j and IP,i,j to calculate F(si,j). This step provides a similar nonlinearity to that in the
GCM. Finally, agents compute F(si) using the averaging equation,
F(si) =
P
j njF(si,j)
n
,
(23)
where nj indicates the number of times an example is observed, and n the total number of examples. As
this method changes the minimum activation values, we empirically ﬁxed υC = 345. The rest of the model
is as described in prior sections. When increasing state confusion we set ξbl = .44 and ξbe = 5.
Neural network actor-critic reinforcement learning
We used a neural network (NN) to implement an actor-critic reinforcement learning model through the
TensorFlow Keras package [63]. The input layer size corresponds to the length of the input vector, and
then feeds into a common layer of 124 ReLu units. An actor network receives inputs from the common
layer, and its size corresponds to the number of actions. The critic, composed of a single unit, also receives
common layer inputs. While the critic network’s role is to predict the expected reward on a given trial, the
actor network learns a value function that minimizes the diﬀerence between the critic’s prediction and the
experienced reward. That diﬀerence is computed using a Huber loss function, which then back-propagates
through gradient decent using the Adam algorithm with a learning rate of 0.01.
13

Experimental tasks
Three tasks were designed to test ProDAtt predictions. Stimuli used in the tasks varied along four dimensions:
color, shape, texture and size.
Each task was implemented both for computational models and human
subjects.
For the computational models, stimuli were composed of vectors where sections of the vectors were one-
hot for a particular feature. For example, the portion of the vector for color might be [1, 0, 0] for orange, [0,
1, 0] for magenta, or [0, 0, 1] for blue. The portion of the vector for shape might be [1, 0, 0] for circle, [0, 1,
0] for square, or [0, 0, 1] for star. Thus, the vector for an orange star would be [0, 1, 0, 0, 0, 1]. Noise was
randomly added to stimulus vectors, drawn from the distribution N(0, 0.05). The sum of a stimulus vector
was always always the same within an experiment, except for the small amount of included noise.
Stimuli were presented semi-randomly in cycles. During a cycle, each potential stimulus was presented a
single time. Within cycles, stimulus order was randomized.
One and two stimulus multi-state state instrumental learning
To test the previous latent-state models we used simple instrumental learning tasks. In each version of the
task, there were two states and two actions. Each action was deterministically mapped to a unique state.
The tasks diﬀer in the number of stimuli composing each state (a single stimulus or two stimuli grouped by
a latent rule). On a given trial, agents were presented the stimulus, and then select an action. Reward was
then provided or omitted according to the latent-state.
Online task deployment
Task implementation and recruitment
We implemented the tasks as a webapp using the Django
framework, and hosted them online through Microsoft Azure. We then recruited subjects through Amazon
Mechanical Turk (mTurk).
On the recruitment page, subjects were informed of the purpose and risks
associated with participation, as well as the potential beneﬁts. When they accepted, they were provided a
link to our website, where they ﬁrst entered their mTurk ID, and answered a few basic demographic questions
(age, gender, education level).
For Experiment 1, 200 human subjects were recruited, 141 completed session, 71 passed the performance
threshold. For Experiment 2, 100 were recruited, 63 completed session, 43 passed the performance threshold.
For Experiment 3, 100 were recruited, 61 completed session and 40 passed the performance threshold.
Human subject compensation
Human subjects received $2 for completing the tutorial (whether passing
or not), $4 for completing the task and $4 for achieving a performance level above 60%.
Tutorial
For all tasks, subjects ﬁrst participated in a tutorial that instructed them on associating actions
(A and B) with sets of stimuli. Actions during the tutorial were probabilistically rewarded. The ﬁrst set was
rewarded 90% for action A and 0% for action B. The second set was reversed, such that A was rewarded 90%
and B 0%. The third set had a mixed reward contingency, such that action A was rewarded 70% and B 30%.
After moving through several screens that interactively and explicitly taught them the reward contingencies,
they were tested for 10 trials where stimuli from the sets were randomly interleaved. After each trial, they
received feedback both about reward, and that trial’s action-reward probabilities. To move onto the main
task, they had to select on 8 out of 10 trials the option with the largest reward probability. If they failed
to pass on the ﬁrst go, they were allowed to try a second time. If they failed the second time, they were
given a payment token and sent back to mTurk. The tutorial was implemented with this structure for three
reasons: 1) instruct subjects on task structure; 3) prepare subjects to form states that can be associated
with multiple probabilistically rewarded actions; and 3) to screen out bots.
General task structure
Each experiment was framed as subjects being “space pirates,” who had to
learn which actions were able to activate alien artifacts. On each trial, they were presented a stimulus and
attempted to activate it through actions associated with diﬀerent key presses. Action-key associations were
randomized across subjects. There was no time limit on their response. After responding, they were shown
14

an outcome screen. If they chose the correct action, they were shown a glowing green energy crystal along
with the following text:
Rewarded! The power was growing...
If they chose the incorrect action, they were shown an empty energy crystal, along with the following text:
Nothing happens. Bummer.
Every ten trials, subjects were shown a screen displaying how much energy they had accumulated, as well
as how much was possible. At the completion of the tasks, subjects were provided a randomly generated
payment token to enter on the mTurk site.
Experiment 1: action generalization
Human subjects and agents (henceforth jointly referred to as “subjects”) were trained on stimulus-action
contingencies using an initial set of stimuli, and then needed to generalize learned actions to stimuli that
varied along a noninformative feature dimension. Stimuli were composed of the following dimensional com-
binations: colors were orange, magenta or blue; shapes were circle, square or star; and texture was dots or
diagonal lines. All stimuli were size large. There were four available actions: A, B, C or D.
Sessions were composed of two blocks: initial learning, then generalization. During the initial learning
block, subjects learned that all orange stimuli were rewarded for action A, stars were rewarded for action
B, and circles for action C. Action D was never rewarded. Stimuli during initial learning shared a texture
(dots), which was noninformative. In the generalization block, novel stimuli were introduced identical to
those of the ﬁrst block, but diﬀering from the initially learned stimuli in their texture (diagonal lines; full
description in Table 1).
To be included in analysis, subjects had to have successfully completed the tutorial, the full task session,
and exited the initial learning block with a minimum performance threshold of 70% correct. These criteria
ensured we included subjects who had formed at least some eﬀective state-representation of the task.
Human subject task implementation
To facilitate learning, orange stars (which ﬁt the rules for both
action A and action B) were excluded. During the initial learning block, there were 16 cycles of stimulus
presentation, resulting in 128 trials. Prior to beginning the session, subjects were provided the following
message (where “A” was replaced by the key on the keyboard, etc.):
You stumbled upon a treasure trove of alien artifacts! To activate the alien artifacts, you might
Shake (press A), Slap (press B), Kick (press C) or Bite (press D). It could be that all the actions
were useful, or it could be that just a few were useful. There also might be patterns. You have
to ﬁgure it out by trial and error. Good luck!
During the generalization block, there were 5 cycles, resulting in 80 trials. The transition from the initial
learning block to generalization occurred without notice to subjects.
Model task implementation
For simulations, models were trained on 222 cycles for the initial learning
block (with 9 stimuli, 1998 total trials), and 111 for the generalization block (18 stimuli, 1998 trials). This
was standardized across model implementations. In ﬁgures showing learning curves, total session length was
truncated.
We allowed all simulations (discriminative and prototype) access to all four actions during training
on the set generalization tasks. This was done to give the NN a chance to develop a richer state-action
representation. If we restricted the discriminative NN’s action space during the initial set-learning blocks
(only actions A and B during Set 1, then only actions C and D in Set 2), the NN would fail to generalize
for trivial reasons.
15

Set generalization tasks
Subjects were trained sequentially on two separate sets of stimuli (Set 1 and Set 2), with each set involving
distinct actions (actions A and B in Set 1, actions C and D in Set 2). The sets were composed of four unique
stimuli, and each action was rewarded for two unique stimuli. During the generalization block, stimuli from
both sets were mixed together.
Human subjects who reached a performance level of 70% correct on the ﬁnal 10 trials of blocks 1 and 2
were included in analysis. This ensured they formed at least some eﬀective representation of the states they
could then use during generalization.
Version 1 (Experiment 2)
Version 1 was designed to determine if subjects were using a discriminative
model of state deﬁnition, and if so the type of model.
Set 1 contained two stimuli rewarded for action A that shared dimensions color (magenta), shape (circles)
and texture (dots), but unique in size (large or small). Action B stimuli in Set 1 also shared those dimensions
(orange squares with dot texture), and were also unique in size (large or small).
Thus, the dimensions
informative for actions in Set 1 were color (magenta versus orange) and shape (circle versus square).
Set 2 was composed of stimuli rewarded for actions C or D. Action C stimuli diﬀered in color (magenta
and blue), but shared the dimensions for shape (circle), size (large) and texture (diagonal lines). Action
D stimuli diﬀered from action C stimuli in their shape (square).
Thus, the dimension informative for
determining actions in Set 2 was shape (see Table 2 for full description).
Version 2 (Experiment 3)
Version 2 was designed both to identify if subjects were utilizing a discrimi-
native model, and to determine if behavior was driven by individual stimuli (an exemplar model), or state
prototypes. To accomplish this, the distribution of stimulus features across sets was designed to maximally
diﬀerentiate model predictions.
Set 1 contained two stimuli rewarded for action A that shared features shape (circles) and texture (dots),
but diﬀered in size (large or small) and color (magenta or orange). Action B stimuli in Set 1 shared shape
(square), texture (dots) and size (large), but diﬀered in color (magenta or orange). Thus, the dimension
informative for actions in Set 1 was shape (circle versus square).
Set 2 contained two stimuli rewarded for action C that shared features shape (circles), size (large) and
texture (diagonal lines), but diﬀered in color (magenta or blue). Action D stimuli in Set 2 shared shape
(square) and texture (diagonal lines), but diﬀered in color (magenta or blue) size (small or large). Thus,
the dimension informative for actions in Set 2 was also shape (circle versus square; see Table 3 for full
description).
Human subject implementation
Subjects experienced 12 randomized stimulus cycles in block 1 of (48
total trials), 12 cycles in block 2 of Set 2 (48 total trials), and then 10 cycles of combined Set 1 and Set
2 during the generalization block (80 total trials). Prior to beginning block 1 (Set 1), subject received the
following instructions:
Welcome to Planet Waz-up, home to the long-deceased Waz civilization.
Here you will ﬁnd
artifacts that were activated with either a shake (press A) or a slap (press B). You’ll have to
ﬁgure out what works!
Prior to block 2 (Set 2), they received the instructions:
Your work on Planet Waz-up was complete!
After hopping in your spaceship, you traveled
to Planet Oh-Kay. Here once lived the proud species Oh. Their artifacts operate completely
diﬀerently, and were activated with either a kick (press C) or a bite (press C). Forget what you
learned on Planet Waz-up. Planet Oh-Kay’s artifacts have their own rules!
Prior to block 3 (generalization), they received this instruction:
Good work space pirate! You learned of planet Blabla, the only place in the galaxy where both
the Waz and Oh once lived! Here you discover artifacts from both civilizations. That means
you can shake (press A) or a slap (press B), kick (press C), or a bite (press C). Go collect some
energy!
16

Model task implementation
The latent state RL models experienced 200 cycles (800 total trials) of
block 1 and 2, then 50 cycles of generalization (400 total). The discriminative NN experienced 250 cycles of
block 1 and 2 (1000 trials each block), then 125 cycles of generalization (1000 total cycles). For the models,
all actions (A, B, C and D) were available throughout the entire session.
Behavioral analyses
Learning curves
On each trial, we determined if a subject selected the most-rewarded action. If so, a 1 was recorded for that
trial, otherwise a 0 was recorded. This provided a per-trial vector for each subject. To obtain population
measures, we computed the mean and standard deviation of these vectors on a per-trial basis. To apply
smoothing, we used a forward window of 5 trials.
Initial generalization performance
We identiﬁed the ﬁrst trial a stimulus appeared during the generalization block, and whether the subject
choose the most-rewarded action. That value was averaged for each subject. From that, we obtained a
population mean and standard deviation.
Action generalization, speciﬁc analyses
Comparison of learned versus novel stimuli
We focused on the generalization block, when all stimuli
were present. For each novel stimulus, we paired it with the initially learned stimulus that diﬀered only in
the noninformative dimension (texture). For example, two large magenta circles where the novel stimulus
had a striped texture and the initially learned stimulus had a dots texture. For each member of the pair,
calculated the error rate as the proportion of trials during the generalization block where the largest action
was not chosen. The diﬀerence was taken as a simple subtraction of the initially learned stimulus’ error rate
minus the novel stimulus’ error rate.
To obtain a single measure for each subject, we averaged this value for a subject across all stimulus pairs.
This provided a distribution across subjects for which we could compute the deviation from zero.
P(Choose D | Novel Stimulus, First Encounter)
We identiﬁed the ﬁrst trial a stimulus appeared
during the generalization block, and whether the subject choose action D. That value was averaged for each
subject. From that, we obtained a population mean and standard deviation.
Set generalization, speciﬁc analyses
Theoretical model feature distances
After completing the ﬁrst two blocks of learning (Set 1 and Set
2), each model provides distinct predictions as to the separation between stimuli in feature space. These
distances in turn make speciﬁc predictions as to errors when the sets were combined during the generalization
block. The space is, by its nature, directional. We indicate this such that, “presented stimulus,” refers to
the stimulus on the screen and “perceived stimulus,” refers to stimuli as they were represented perceptually
by the subject.
For the ﬁrst set generalization task’s two discriminative models (ﬁrst discriminative model and second
discriminative model), we determined feature distance by applying the learned decision boundaries. The
discriminative model that learned only shape (ﬁrst discriminative model) provided a relatively simple process.
If stimuli were of diﬀerent shape, their feature distance was 1, otherwise it was 0. Dimensions such as color,
size or texture were irrelevant. The process was a bit more complicated for the discriminative model that
learned both color and shape in the Set 1, then only shape in Set 2 (second discriminative model). To
compute feature distance for this model, we used the decision boundaries that were learned for the set under
question. For example, when the model considers the set membership of stimulus 6 (large blue circle with
diagonal texture), it considers the decision boundaries of the set under question. As the Set 1 boundary
involves two dimensions (color and shape), stimulus 6 is a distance of 1 from action A, and two from action
B. By applying these rules, we produced matrices of feature distance for both discriminative models. The
17

feature distance matrix for the ﬁrst discriminative model was symmetric, while the second discriminative
model produces an asymmetric feature distance matrix.
The prototype model compares a presented stimulus with an internal bank of prototypes. Thus, the
feature distance depends on both the stimulus presented, and the prototype to which it was being compared.
For example, stimuli forming the state rewarded for action A share color (magenta), shape (circle) and
texture (dots), but diﬀer in size (large and small). Thus, their prototype places high weight on color, shape
and texture, but low on shape. Because of that, two stimuli have the same color (orange), shape (square)
and texture (dots) but diﬀer in shape (large and small) will have the same feature distance from the “action
A state.” Working through each stimulus-prototype comparison produced a feature distance matrix for the
prototype model. Due to the asymmetries in prototype state feature weights, the matrix was asymmetric.
For the ﬁrst set generalization task, we also computed the feature distances of each stimulus to each other
stimulus. This was done by simply adding up the number of features that diﬀered between two stimuli. By
its nature, the matrix was symmetric.
To create the theoretical model feature distance matrices for the second set generalization task, we used
the same principels.
Discriminative errors
The feature distance matrices generate speciﬁc predictions for the errors a subject
produces, from the stimuli under each model which are perceptually closest. These produces stimulus-wise
predictions of distinct a error type for the ﬁrst discriminative model and second discriminative model we call
a “discriminative error.” For example, under the discriminative models, stimuli associated with action B (3
and 4) are an feature distance of 0 with stimuli of action D. Thus, one would expect a heavy bias in errors
of action B stimuli to be for action D. However, the ﬁrst discriminative model and second discriminative
model are not identical in the other direction (stimuli 7 and 8 erring with action B). This is because the
dual decision boundaries predicted for the second discriminative model model increase the feature distance
for those stimuli. Thus, one predicts a large number of stimuli 7 and 8 discriminative errors for the ﬁrst
discriminative model, but not for the second discriminative model.
Model error predictions: chance control
To generate the control for each of these model error pattern
predictions, we compute how likely an agent behaving randomly was to make that type of error. For example,
with the shape-only discriminative model, simply by chance one would expect 33% of errors made by an
action A stimulus to be action C. Thus the chance level for a discriminative error was 33%. Applying this
method to the patterns predicted by each model produced the level of chance for each error type under each
model.
Confusion matrices
We created confusion matrices of each subject’s errors. Rows in the subject’s con-
fusion matrix indicate the presented stimulus, and columns indicate selected actions on error trials. When
calculating the confusion matrix for individual subjects, this involved simply ﬁlling the matrix based on error
actions.
The predicted confusion matrices required a simplifying assumption. As actions were associated with two
stimuli, any error could reﬂect confusion with one of two stimuli whose feature distances from the presented
stimulus might be unequal.
We therefore assumed that on error trials subjects confused the presented
stimulus based on the minimum feature distance to the chosen action.
As a control, we produced a confusion matrix where the errors were evenly distributed.
Residual variance of confusion matrix explained by predictions
We computed the residual variance
(R2) by linearly regressing vectorized subject confusion matrices against the vectorized confusion matrices
predicted by each model. Due to the sparsity of confusion matrices for individuals, we pooled errors across
subjects. Chance was determined by computing the correlation between the chance confusion matrix and
each model. For each theoretical model, we generated a null distribution of residual variance estimates by
permuting the rows (among a stimulus) of the subjects’ confusion matrix 1,000 times, and regressing it
against predictions from the the model. We then used the tail of that distribution to compute a P-value.
18

Exemplar Diﬀerence
To calculate the exemplar diﬀerence, we exploited the fact the prototype feature
distance of stimuli from the same state to another state in the other set will be equal, while its exemplar
feature distance in that same condition will be unequal. Thus, a prototype model predicts that the proportion
of errors will be the same for that action, while the exemplar model predicts that they will be signiﬁcantly
diﬀerent.
To compute this, we ﬁrst normalized the rows of each subject’s confusion matrix by the total number of
errors in that row. We then computed the diﬀerence in errors on the relevant action for each latent-state’s
stimuli. For the stimuli of Set 1 latent-state action A, that meant the diﬀerence in the proportion of errors
where they selected action D (and vice versa). For the stimuli of Set 1 latent-state action B, that meant
proportion of errors where they selected action C (and vice versa). This diﬀerence was directional, such that
the proportion of errors for the stimulus with the further exemplar feature distance was subtracted from the
proportion of errors for the stimulus with the closer exemplar feature distance. Therefore, a positive value
indicates a bias towards the exemplar model. The diﬀerence was then averaged for each subject across the
four latent-states. That produced a distribution of exemplar diﬀerences.
Data and code availability
Upon publication, behavioral datasets will be released via a public dataset repository, and code for task
implementations, data analysis, and model simulation will be released via a public code repository
(https://github.com/murraylab).
19

References
[1] Gershman, S. J., Blei, D. M. & Niv, Y. Context, learning, and extinction. Psychological Review 117,
197–209 (2010).
[2] Redish, A. D., Jensen, S., Johnson, A. & Kurth-Nelson, Z. Reconciling reinforcement learning models
with behavioral extinction and renewal: Implications for addiction, relapse, and problem gambling.
Psychological Review 114, 784–805 (2007).
[3] Gershman, S. J. & Niv, Y.
Exploring a latent cause theory of classical conditioning.
Learning &
Behavior 40, 255–268 (2012).
[4] Collins, A. & Koechlin, E. Reasoning, Learning, and Creativity: Frontal Lobe Function and Human
Decision-Making. PLOS Biology 10, e1001293 (2012).
[5] Gershman, S. J., Monﬁls, M.-H., Norman, K. A. & Niv, Y. The computational nature of memory
modiﬁcation. eLife 6, e23763 (2017).
[6] Cochran, A. L. & Cisler, J. M. A ﬂexible and generalizable model of online latent-state learning. PLOS
Computational Biology 15, e1007331 (2019).
[7] Ashby, F. G. & Maddox, W. T. Human Category Learning. Annual Review of Psychology 56, 149–178
(2005).
[8] Minda, J. P. Prototypes in category learning: The eﬀects of category size, category structure, and
stimulus complexity. Journal of Experimental Psychology: Learning, Memory, and Cognition 27, 775
(2001).
[9] Smith, D. J. & Minda, J. P. Thirty categorization results in search of a model. Journal of Experimental
Psychology: Learning, Memory, and Cognition 26, 3–27 (2000).
[10] Medin, D. L. & Schaﬀer, M. M. Context theory of classiﬁcation learning. Psychological Review 85,
207–238 (1978).
[11] Estes, W. K. Array models for category learning. Cognitive Psychology 18, 500–549 (1986).
[12] Nosofsky, R. M. Choice, similarity, and the context theory of classiﬁcation. Journal of Experimental
Psychology: Learning, Memory, and Cognition 10, 104–114 (1984).
[13] Nosofsky, R. M. Attention, similarity, and the identiﬁcation–categorization relationship. Journal of
experimental psychology: General 115, 39 (1986).
[14] Ashby, F. G. Multidimensional models of categorization. In Multidimensional models of perception and
cognition, Scientiﬁc psychology series, 449–483 (Lawrence Erlbaum Associates, Inc, Hillsdale, NJ, US,
1992).
[15] Maddox, W. T. & Ashby, F. G. Comparing decision bound and exemplar models of categorization.
Perception & Psychophysics 53, 49–70 (1993).
[16] Niv, Y. et al. Reinforcement Learning in Multidimensional Environments Relies on Attention Mecha-
nisms. Journal of Neuroscience 35, 8145–8157 (2015).
[17] Corbetta, M. & Shulman, G. L. Control of goal-directed and stimulus-driven attention in the brain.
Nature Reviews Neuroscience 3, 201–215 (2002).
[18] Noudoost, B., Chang, M. H., Steinmetz, N. A. & Moore, T. Top-down control of visual attention.
Current Opinion in Neurobiology 20, 183–190 (2010).
[19] Baluch, F. & Itti, L. Mechanisms of top-down attention. Trends in Neurosciences 34, 210–224 (2011).
20

[20] Friston, K. The free-energy principle: a uniﬁed brain theory? Nature Reviews Neuroscience 11, 127–138
(2010).
[21] Myung, I. J. Maximum Entropy Interpretation of Decision Bound and Context Models of Categorization.
Journal of Mathematical Psychology 38, 335–365 (1994).
[22] Barak, O., Rigotti, M. & Fusi, S. The Sparseness of Mixed Selectivity Neurons Controls the General-
ization–Discrimination Trade-Oﬀ. Journal of Neuroscience 33, 3844–3856 (2013).
[23] Bernardi, S. et al. The Geometry of Abstraction in the Hippocampus and Prefrontal Cortex. Cell 183,
954–967.e21 (2020).
[24] Ashby, F. G. & Waldron, E. M. On the nature of implicit categorization. Psychonomic Bulletin &
Review 6, 363–378 (1999).
[25] Dopkins, S. & Gleason, T. Comparing exemplar and prototype models of categorization. Canadian
Journal of Experimental Psychology/Revue canadienne de psychologie exp´erimentale 51, 212–230 (1997).
[26] McKinley, S. C. & Nosofsky, R. M. Investigations of exemplar and decision bound models in large, ill-
deﬁned category structures. Journal of Experimental Psychology: Human Perception and Performance
21, 128–148 (1995).
[27] McKinley, S. C. & Nosofsky, R. M. Selective attention and the formation of linear decision boundaries.
Journal of Experimental Psychology: Human Perception and Performance 22, 294–317 (1996).
[28] Nosofsky, R. M. & Zaki, S. R. Exemplar and prototype models revisited: Response strategies, selective
attention, and stimulus generalization. Journal of Experimental Psychology: Learning, Memory, and
Cognition 28, 924–940 (2002).
[29] Smith, J. D. Prototypes in the mist: The early epochs of category learning. Journal of Experimental
Psychology: Learning, Memory, and Cognition 24, 1411 (1998).
[30] Smith, J. D. Straight talk about linear separability. Journal of Experimental Psychology: Learning,
Memory, and Cognition 23, 659 (1997).
[31] Smith, J. D.
Exemplar Theory’s Predicted Typicality Gradient Can Be Tested and Disconﬁrmed.
Psychological Science 13, 437–442 (2002).
[32] Stanton, R. D., Nosofsky, R. M. & Zaki, S. R. Comparisons between exemplar similarity and mixed
prototype models using a linearly separable category structure.
Memory & Cognition 30, 934–944
(2002).
[33] Lewandowsky, S. Working memory capacity and categorization: Individual diﬀerences and modeling.
Journal of Experimental Psychology: Learning, Memory, and Cognition 37, 720–738 (2011).
[34] Hoﬀmann, J. A., von Helversen, B. & Rieskamp, J. Pillars of judgment: How memory abilities aﬀect
performance in rule-based and exemplar-based judgments. Journal of Experimental Psychology: General
143, 2242–2261 (2014).
[35] Wallis, J. D. Orbitofrontal Cortex and Its Contribution to Decision-Making. Annual Review of Neuro-
science 30, 31–56 (2007).
[36] Abe, H. & Lee, D.
Distributed Coding of Actual and Hypothetical Outcomes in the Orbital and
Dorsolateral Prefrontal Cortex. Neuron 70, 731–741 (2011).
[37] Steiner, A. & Redish, A. D. The Road Not Taken: Neural Correlates of Decision Making in Orbitofrontal
Cortex. Frontiers in Neuroscience 6, 131 (2012).
[38] Wilson, R. C., Takahashi, Y. K., Schoenbaum, G. & Niv, Y. Orbitofrontal cortex as a cognitive map of
task space. Neuron 81, 267–279 (2014).
21

[39] Stott, J. J. & Redish, A. D. A functional diﬀerence in information processing between orbitofrontal
cortex and ventral striatum during decision-making behaviour. Philosophical Transactions of the Royal
Society B: Biological Sciences 369, 20130472 (2014).
[40] Wikenheiser, A. M. & Schoenbaum, G.
Over the river, through the woods: cognitive maps in the
hippocampus and orbitofrontal cortex. Nature Reviews Neuroscience 17, 513–523 (2016).
[41] Chan, S. C. Y., Niv, Y. & Norman, K. A.
A Probability Distribution over Latent Causes, in the
Orbitofrontal Cortex. Journal of Neuroscience 36, 7817–7828 (2016).
[42] Schuck, N., Cai, M., Wilson, R. & Niv, Y. Human Orbitofrontal Cortex Represents a Cognitive Map
of State Space. Neuron 91, 1402–1412 (2016).
[43] Niv, Y. Learning task-state representations. Nature Neuroscience 22, 1544–1553 (2019).
[44] Zhou, J. et al. Evolving schema representations in orbitofrontal ensembles during learning. Nature 1–6
(2020).
[45] Hyman, J. M., Ma, L., Balaguer-Ballester, E., Durstewitz, D. & Seamans, J. K. Contextual encoding by
ensembles of medial prefrontal cortex neurons. Proceedings of the National Academy of Sciences 109,
5086–5091 (2012).
[46] Powell, N. J. & Redish, A. D. Representational changes of latent strategies in rat medial prefrontal
cortex precede changes in behaviour. Nature Communications 7, 12830 (2016).
[47] Leong, Y. C., Radulescu, A., Daniel, R., DeWoskin, V. & Niv, Y.
Dynamic Interaction between
Reinforcement Learning and Attention in Multidimensional Environments. Neuron 93, 451–463 (2017).
[48] Akam, T. et al. The Anterior Cingulate Cortex Predicts Future States to Mediate Model-Based Action
Selection. Neuron 109, 149–163.e7 (2021).
[49] O’keefe, J. & Nadel, L. The hippocampus as a cognitive map (Oxford university press, 1978).
[50] Redish, A. D. Beyond the cognitive map. The MIT Press, Cambridge (1999).
[51] Shohamy, D. & Turk-Browne, N. B. Mechanisms for widespread hippocampal involvement in cognition.
Journal of Experimental Psychology: General 142, 1159–1170 (2013).
[52] Kelemen, E. & Fenton, A. A. Coordinating diﬀerent representations in the hippocampus. Neurobiology
of Learning and Memory 129, 50–59 (2016).
[53] Stachenfeld, K. L., Botvinick, M. M. & Gershman, S. J. The hippocampus as a predictive map. Nature
Neuroscience 20, 1643–1653 (2017).
[54] Nieh, E. H. et al. Geometry of abstract learned knowledge in the hippocampus. Nature 1–5 (2021).
[55] Barch, D. M. & Ceaser, A. Cognition in schizophrenia: core psychological and neural mechanisms.
Trends in Cognitive Sciences 16, 27–34 (2012).
[56] Joyce, D. W., Averbeck, B. B., Frith, C. D. & Shergill, S. S.
Examining belief and conﬁdence in
schizophrenia. Psychological Medicine 43, 2327–2338 (2013).
[57] Nassar, M. R., Waltz, J. A., Albrecht, M. A., Gold, J. M. & Frank, M. J. All or nothing belief updating
in patients with schizophrenia reduces precision and ﬂexibility of beliefs. Brain (2021).
[58] Maia, T. V. & Cano-Colino, M.
The Role of Serotonin in Orbitofrontal Function and Obsessive-
Compulsive Disorder. Clinical Psychological Science 3, 460–482 (2015).
[59] Ferreira, G. M., Y¨ucel, M., Dawson, A., Lorenzetti, V. & Fontenelle, L. F. Investigating the role of
anticipatory reward and habit strength in obsessive-compulsive disorder. CNS Spectrums 22, 295–304
(2017).
22

[60] Seow, T. X. F. & Gillan, C. M. Transdiagnostic Phenotyping Reveals a Host of Metacognitive Deﬁcits
Implicated in Compulsivity. Scientiﬁc Reports 10, 2883 (2020).
[61] Mercado, E., Chow, K., Church, B. A. & Lopata, C. Perceptual category learning in autism spectrum
disorder: Truth and consequences. Neuroscience & Biobehavioral Reviews 118, 689–703 (2020).
[62] Miller, H. L., Ragozzino, M. E., Cook, E. H., Sweeney, J. A. & Mosconi, M. W. Cognitive Set Shifting
Deﬁcits and Their Relationship to Repetitive Behaviors in Autism Spectrum Disorder.
Journal of
Autism and Developmental Disorders 45, 805–815 (2015).
[63] Abadi, M. et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems.
arXiv:1603.04467 [cs] (2016).
23

Main Figures
Figure 1: Prototype and discriminative models of latent-state learning produce distinct generalization
failures (A) A prototype model (left) of latent-state learning deﬁnes states based on past examples. This produces
state clusters whose weights on individual feature dimensions (shape of the cluster) is determined by the covariance
of features.
A discriminative model (right) deﬁnes states based on feature dimensions that are informative for
separating one state from another. This produces state boundaries deﬁned by those dimensions most informative for
discrimination. (B) Common grocery store items can be considered latent states of which one encounters only variable
examples. Each item is associated with sets of actions. (C) A pure prototype model can fail to generalize when a
highly regular, yet noninformative feature changes in novel examples or contexts. During initial learning, the store
section is highly regular across examples (left), and thus it is considered informative (middle). When this feature is
altered in novel examples, a pure prototype model struggles to generalize the rewarded action (right). (D) A purely
discriminative model can fail to generalize when previously noninformative features become informative in novel
comparisons. During learning in the initial contexts, store section was non-discriminative (left). Thus, the distance
is zero between otherwise-identical alternatives learned in separate contexts (middle). This leads to generalization
failure when these alternatives are brought together in a novel context (right).
24

Figure 2: A model of latent-state instrumental reinforcement learning using prototype states with
discriminative attention (1) When an agent encounters a stimulus, it ﬁrst compares the stimulus to its internal
bank of states, determining how surprised it would be if the stimulus is an exemplar of that speciﬁc state. States that
are less surprising than a threshold (i.e. more likely) are included in a set of state candidates. (2) Using this set of
candidate states, the agent calculates the mutual information (MI) of each feature, identifying feature dimensions that
maximally discriminate between states. (3) The agent then scales the features by their MI – applying discriminative
attention. (4) Surprise is re-calculated using the newly-scaled feature weights. If the level of surprise is below a
threshold, the state with the lowest surprise is selected (left). If no state is below the surprise threshold, a new state
is created (right). (5) Each state learns a set of action values that are used for action-selection. In this case, the state
“chocolate cereal” has been selected, and the model uses the action values to choose between adding milk or yogurt.
Following feedback, the agent updates its state representation and other tracking variables.
25

Figure 3: Experiment 1: Human subjects use discriminative attention when generalizing to novel
examples. (A) A model relying on feature regularity to deﬁne states can fail to generalize if a new observation
diﬀers in a highly regular, but noninformative feature dimension (left). However, a model utilizing discriminative
boundaries easily generalizes in this scenario (right). (B) Human subjects and simulated agents learn the latent states
underlying stimulus-action-reward associations. On a given trial, they encounter an “alien artifact” activated with one
of four actions. After selecting an action, they are provided feedback with the outcome. (C) The stimuli forming the
states, grouped by their rewarded action. Initially learned stimuli are on the left, and the novel generalization stimuli
on the right. Action D is never rewarded. (D) The theoretical feature distance for novel stimuli diﬀers depending
on the application of discriminative attention. Without discriminative attention (left), novel stimuli are separated
from the initially learned stimuli by their texture. With attention scaling (right) novel stimuli are projected into the
same feature space as the initially learned stimuli. (E) During generalization the choice of action D for novel stimuli
indicates exploration of a new state (left). The diﬀerence in error rates for discriminatively-identical stimuli quantiﬁes
generalization of a state to novel examples (right). (F) (Left) Computational simulations and humans learned the
task during the initial block [prototype states with discriminative attention (ProDAtt); prototype state model (PS)].
(Right) Generalization block where “learned” refers to performance on the initially learned stimuli, and “novel”
performance on novel generalization stimuli.
(G) Human subjects and the model with discriminative deduction
exhibited statistically signiﬁcant diﬀerences in error rates between initially learned and novel stimuli (p<0.001), but
with a moderate Bayes factor (46.295) and small eﬀect size (0.3966). Subjects explored the novel action signiﬁcantly
less than chance (p<0.001) with a large Bayes factor (1.832e+34). The model without discriminative deduction,
however, exhibited a large diﬀerence in error rates, and often explores action D.
26

Figure 4: Experiment 2: Human subjects use initially non-discriminative feature dimensions when
generalizing to novel contexts. (A) When a purely discriminative model learns states, it deﬁnes states according
to feature dimensions that are maximally informative for decisions.
It can fail to generalize when a dimension
non-discriminative during initial learning becomes the key discriminative dimension for later decisions. (B) A set
generalization task where subjects learn distinct state-action contingencies in two separate blocks, before generalizing
to novel selection among all states. Stimuli vary along the dimensions color, shape, texture and size. (C) Theoretically,
a pure discriminative model that learns only shape (D1M) will form a single decision boundary that fails to distinguish
states falling on the same side of the decision boundary. A pure discriminative model that learns the informativeness
of color and shape in the ﬁrst block will form a decision boundary for Set 1 states involving color and shape, and a
separate decision boundary for Set 2 states involving only shape (D2M). A model that forms prototype states will
place high weight on the regularity of individual state features, resulting in distinct states. (D) A pure discriminative
model will produce a distinct “discriminative error,” where the key feature dimension separating Set 1 from Set 2
stimuli was initially learned as noninformative. (E) Model simulations and human subjects learn the task. Blue
is pure discriminative neural network; purple the normal prototype states with discriminative attention (ProDAtt);
orange, a version of the ProDAtt with increased state confusion (ISC) during block 3; green is human subjects. Shown
are the population mean and SEM. Chance level diﬀers between simulations and humans in the ﬁrst two sets because
the models were given access to all actions. (F) Models and human subjects perform signiﬁcantly better than chance
on the ﬁrst occurrence of each stimulus during generalization. (G) Only the neural network simulation produces
more discriminative errors than predicted by chance. (H) Feature distances of stimulus 4 to state-actions for each
theoretical model, and stimulus 8 for the D2M. (I) The residual variance of theoretical models ﬁt to simulations and
human subject behavior. P is prototype model; M is memorization of individual stimuli.
27

Figure 5: Experiment 3: Human subjects form prototype states, rather than exemplar states. (A)
While a prototype model compares new observations against an idealized prototype, an exemplar model compares it to
all previous observations of that state. The orange and brown are examples of distinct states; dashed lines represent
distance. To reduce clutter only a few lines are shown. (B) A set generalization task where the conﬁguration of
stimulus feature produces dissociable theoretical predictions between discriminative, prototype and exemplar models.
(C) Based on the conﬁguration of features, stimuli of the same latent state are equally distant for a prototype
model, but of diﬀerent distances for an exemplar model (causing the “exemplar diﬀerence” in errors). (D) Human
subjects and models generalize above chance. Blue is the discriminative neural network (NN), orange the prototype
states with discriminative attention (ProDAtt) with increased state confusion (ISC), brown the exemplar states
with discriminative attention (ExDAtt) with ISC, and green is human subjects. (E) Human subjects do not show
a signiﬁcant number of discriminative errors (t-test p>0.05).
(F) Human subjects and the discriminative neural
network do not show an exemplar diﬀerence (t-test p>0.05). The prototype and exemplar algorithms both show a
signiﬁcant exemplar diﬀerence (t-test p<0.001). (G) The human subject confusion matrix has the highest R2 for
theoretical predictions of the prototype model.
28

Supplemental Figures
Figure S1: Previous latent-state models struggle with instrumental learning. (A) We implemented two
versions of a simple multi-state instrumental learning task. In the ﬁrst task, there were two states and two actions, with
each state deterministically rewarded for a unique action. The second task had the same action-reward contingencies,
but involved two stimuli for each state, where color was the latent rule determining state-membership. (B) Of the
previous models, only the Gershman 2012 model [3] and the Redish 2007 model [2] were able to perform the tasks,
with the Gershman 2012 model struggling to learn two stimulus states. Shown are the mean and SEM for 100 agents
trained with each model. (C) Performance in the last 100 trials reﬂects prior models’ diﬃculty with generalization.
Of those models, the Redish 2007 performs consistently above chance. It was used as the basis of the prototype states
with discriminative attention (ProDAtt), which performs perfectly on both tasks.
29

Figure S2: Theoretical prediction and human behavior on Experiment 2. (A) Theoretical model feature
distances. For each model, one can compute the theoretical feature distance between stimuli after initial set learning.
D1M refers to a discriminative model learning one feature dimension boundary across all Sets; D2M to a discriminative
model learning two boundaries for Set 1, and a single boundary for Set 2. (B) Human subject, chance and theoretical
confusion matrices. By taking the minimum distance for each action, one can create theoretical confusion matrix
distribution whereby perceptually further stimuli are less likely to be selected and perceptually closer stimuli are
more likely to be selected.
Shown are the observed confusion matrix pooled across subjects, a confusion matrix
assuming random choice, and the theoretical confusion matrices predicted by each model.
(C) Human subjects
display asymmetry across stimuli in overall errors during block 3 (ANOVA p<0.001). (D) Human subject confusion
as a function of each theoretical model. (E) Regression between human subject data is signiﬁcant (p<0.05) for the
theoretical prototype model and the theoretical memorization of stimuli, but not signiﬁcant for either theoretical
discriminative model. Permuting the rows in the pooled-subject confusion matrix 1,000 times, then calculating the
correlation between that matrix and each model generates control distributions. In each distribution, the vertical
dashed black line is the mean, the yellow vertical line correlation with the chance confusion matrix and the green
vertical line correlation with the observed confusion matrix.
30

Figure S3: Simulation behavior on Experiment 2. (A) Pure discriminative neural network similar to Fig.
S2D. The neural network is signiﬁcant for all models. (B) Same for the ProDAtt with increased state confusion. The
ProDAtt is signiﬁcant for all models but the D1M.
31

Figure S4: Exemplar latent-state reinforcement learning model with discriminative deduction suc-
cessfully performs Experiments 1 and 2.
(A) (Left) Experiment 1 learning curves for the exemplar states
with discriminative attention (ExDAtt) and the exemplar state model (ES) (as in Fig.
3F). (Right) The model
with discriminative attention generalizes, while that without does not (as in Fig. 3G). (B) Experiment 2 learning
curves for the model, and a version with increased state confusion (ISC) during generalization (as in Fig. 4E). (C)
The model with ISC eﬀectively generalizes, and produces chance-level discriminative errors (as in Fig. 4F,G). (D)
The residual variance explained by the theoretical discriminative models is at chance level, while for the theoretical
prototype and memorized-stimuli residual variance are above chance (as in Fig. S3 and Fig. 4I). D1M refers to
a discriminative model learning a one feature dimension boundary across all Sets; D2M to a discriminative model
learning two boundaries for Set 1, and a single boundary for Set 2.
32

Figure S5:
Learning curves and theoretical models on Experiment 3.
(A) Theoretical model feature
distances after learning ﬁrst two sets, prior to generalization (similar to Fig. S2A). (B) Learning curves for simulations
and human subjects. (C) Confusion matrices of human subjects, random choice, and that predicted by each theoretical
model (similar to Fig. S2B).
33

Figure S6: Confusion matrix ﬁts, and rates of minimum feature distance errors on Experiment 3. (A)
Pure discriminative neural network. (B) Prototype states and discriminative attention RL. (C) Exemplar states and
discriminative attention RL. (D) Human subjects.
34

Supplemental Tables
Stimulus
Type
Action
Color
Shape
Size
Texture
1
Learned
A
Orange
Circle
Large
Dots
2
Learned
A
Orange
Square
Large
Dots
3
Learned
B
Magenta
Star
Large
Dots
4
Learned
B
Blue
Star
Large
Dots
5
Learned
C
Magenta
Circle
Large
Dots
6
Learned
C
Magenta
Square
Large
Dots
7
Learned
C
Blue
Circle
Large
Dots
8
Learned
C
Blue
Square
Large
Dots
9
Novel
A
Orange
Circle
Large
Diagonal
10
Novel
A
Orange
Square
Large
Diagonal
11
Novel
B
Magenta
Star
Large
Diagonal
12
Novel
B
Blue
Star
Large
Diagonal
13
Novel
C
Magenta
Circle
Large
Diagonal
14
Novel
C
Magenta
Square
Large
Diagonal
15
Novel
C
Blue
Circle
Large
Diagonal
16
Novel
C
Blue
Square
Large
Diagonal
Table 1: Stimulus properties and actions for Experiment 1.
Stimulus
Set
Action
Color
Shape
Size
Texture
1
1
A
Magenta
Circle
Large
Dots
2
1
A
Magenta
Circle
Small
Dots
3
1
B
Orange
Square
Large
Dots
4
1
B
Orange
Square
Small
Dots
5
2
C
Magenta
Circle
Large
Diagonal
6
2
C
Blue
Circle
Large
Diagonal
7
2
D
Magenta
Square
Large
Diagonal
8
2
D
Blue
Square
Large
Diagonal
Table 2: Stimulus properties and actions for Experiment 2.
Stimulus
Set
Action
Color
Shape
Size
Texture
1
1
A
Orange
Circle
Large
Dots
2
1
A
Magenta
Circle
Small
Dots
3
1
B
Magenta
Square
Large
Dots
4
1
B
Orange
Square
Large
Dots
5
2
C
Magenta
Circle
Large
Diagonal
6
2
C
Blue
Circle
Large
Diagonal
7
2
D
Magenta
Square
Small
Diagonal
8
2
D
Blue
Square
Large
Diagonal
Table 3: Stimulus properties and actions for Experiment 3.
35

