Do LLMs dream of elephants (when told not to)?
Latent concept association and associative memory
in transformers
Yibo Jiang1, Goutham Rajendran2,
Pradeep Ravikumar2, and Bryon Aragam3
1Department of Computer Science, University of Chicago
2Machine Learning Department, Carnegie Mellon University
3Booth School of Business, University of Chicago
Abstract
Large Language Models (LLMs) have the capacity to store and recall facts. Through
experimentation with open-source models, we observe that this ability to retrieve
facts can be easily manipulated by changing contexts, even without altering their
factual meanings. These findings highlight that LLMs might behave like an associative
memory model where certain tokens in the contexts serve as clues to retrieving facts. We
mathematically explore this property by studying how transformers, the building blocks
of LLMs, can complete such memory tasks. We study a simple latent concept association
problem with a one-layer transformer and we show theoretically and empirically that
the transformer gathers information using self-attention and uses the value matrix for
associative memory.
1
Introduction
What is the first thing that would come to mind if you were asked not to think of an ele-
phant? Chances are, you would be thinking about elephants. What if we ask the same thing
to Large Language Models (LLMs)? Obviously, one would expect the outputs of LLMs to be
heavily influenced by tokens in the context [Bro+20]. Could such influence potentially prime
LLMs into changing outputs in a nontrivial way? To gain a deeper understanding, we focus
on one specific task called fact retrieval [Men+22; Men+23] where expected output answers
are given. LLMs, which are trained on vast amounts of data, are known to have the capabil-
ity to store and recall facts [Men+22; Men+23; DCAT21; Mit+21; Mit+22; Dai+21]. This abil-
ity raises natural questions: How robust is fact retrieval, and to what extent does it depend
on semantic meanings within contexts? What does it reveal about memory in LLMs?
In this paper, we first demonstrate that fact retrieval is not robust and LLMs can be easily
fooled by varying contexts. For example, when asked to complete “The Eiffel Tower is in
the city of”, GPT-2 [Rad+19] answers with “Paris”. However, when prompted with “The
Eiffel Tower is not in Chicago. The Eiffel Tower is in the city of”, GPT-2 responds with
“Chicago”. See Figure 1 for more examples, including Gemma and LLaMA. On the other
hand, humans do not find the two sentences factually confusing and would answer “Paris”
in both cases. We call this phenomenon context hijacking. Importantly, these findings
suggest that LLMs might behave like an associative memory model. In which, tokens in
contexts guide the retrieval of memories, even if such associations formed are not inherently
semantically meaningful.
This associative memory perspective raises further interpretability questions about how
1
arXiv:2406.18400v1  [cs.CL]  26 Jun 2024

Figure 1: Examples of context hijacking for various LLMs, showcasing that fact retrieval is not
robust.
LLMs form such associations. Answering these questions can facilitate the development
of more robust LLMs. Unlike classical models of associative memory in which distance
between memory patterns are measured directly and the associations between inputs and
outputs are well-specified, fact retrieval relies on a more nuanced notion of similarity mea-
sured by latent (unobserved) semantic concepts. To model this, we propose a synthetic task
called latent concept association where the output token is closely related to sampled tokens
in the context but wherein similarity is measured via a latent space of semantic concepts.
We then investigate how a one-layer transformer [Vas+17], a fundamental component of
LLMs, can tackle this memory retrieval task in which various context distributions corre-
spond to distinct memory patterns. We demonstrate that the transformer accomplishes the
task in two stages: The self-attention layer gathers information, while the value matrix func-
tions as associative memory. Moreover, low-rank structure also emerges in the embedding
space of trained transformers. These findings provide additional theoretical validation for
numerous existing low-rank editing and fine-tuning techniques [Men+22; Hu+21].
Contributions
Specifically, we make the following contributions:
1. We systematically demonstrate context hijacking for various open source LLM models
including GPT-2 [Rad+19], LLaMA-2 [Tou+23] and Gemma [Tea+24], which show
that fact retrieval can be misled by contexts (Section 3), reaffirming that LLMs lack
robustness to context changes [Shi+23; Pet+20; CSH22; Yor+23; PE21].
2. We propose a synthetic memory retrieval task termed latent concept association,
allowing us to analyze how transformers can accomplish memory recall (Section 4).
Unlike classical models of associative memory, our task creates associations in a
latent, semantic concept space as opposed to directly between observed tokens. This
perspective is crucial to understanding how transformers can solve fact retrieval
problems by implementing associative memory based on similarity in the latent space.
3. We theoretically (Section 5) and empirically (Section 6) study trained transformers on
this latent concept association problem, showing that self-attention is used to aggre-
gate information while the value matrix serves as associative memory. And moreover,
we discover that the embedding space can exhibit a low-rank structure, offering ad-
ditional support for existing editing and fine-tuning methods [Men+22; Hu+21].
2

2
Literature review
Associative memory
Associative memory has been explored within the field of neu-
roscience [Hop82; Seu96; BYBOS95; Ska+94; SS22]. The most popular models among
them is the Hopfield network [Hop82] and its modern successors [Ram+20; Mil+22; Zha23]
are closely related to the attention layer used in transformers [Vas+17]. In addition, the
attention mechanism has also been shown to approximate another associative memory
model known as sparse distributed memory [BP21]. Beyond attention, Radhakrishnan
et al. [RBU20] and Jiang and Pehlevan [JP20] show that overparameterzed autoencoders
can implement associative memory as well. This paper studies fact retrieval as a form
of associative memory. Another closely related area of research focuses on memorization
in deep neural networks. Henighan et al. [Hen+23] shows that a simple neural network
trained on toy model will store data points in the overfitting regime while storing features
in the underfitting regime. Feldman [Fel20] and Feldman and Zhang [FZ20] study the
interplay between memorization and long tail distributions while Kim et al. [KKM22] and
Mahdavi et al. [MLT23] study the memorization capacity of transformers.
Interpreting transformers and LLMs
There’s a growing body of work on understand-
ing how transformers and LLMs work [LLR23; AZL23a; AZL23b; AZL24; EI+24; Tar+23b;
Tar+23a; Li+24], including training dynamics [Tia+23a; Tia+23b; She+24] and in-context
learning [Xie+21; Gar+22; Bai+24; Bai+24]. Recent papers have introduced synthetic
tasks to better understand the mechanisms of transformers [Cha22; Liu+22; Nan+23;
Zha+22; Zho+24], such as those focused on Markov chains [Bie+24; Ede+24; NDL24;
Mak+24]. Most notably, Bietti et al. [Bie+24] and subsequent works [CDB23; CSB24]
study weights in transformers as associative memory but their focus is on understanding
induction head [Ols+22b] and one-to-one map between input query and output memory. An
increasing amount of research is dedicated to understanding the internals of pre-trained
LLMs, broadly categorized under the term “mechanistic interpretability” [Elh+21; Ols+22a;
Gev+23; Men+22; Men+23; Jia+24; Raj+24; Has+24; Wan+22; McG+23; Gei+21; Gei+22;
Gei+24; Wu+24].
Knowledge editing and adversarial attacks on LLMs
Fact recall and knowledge
editing have been extensively studied [Men+22; Men+23; Has+24; Sak+23; DCAT21;
Mit+21; Mit+22; Dai+21; Zha+23; Tia+24; Jin+23], including the use of in-context learning
to edit facts [Zhe+23]. This paper aims to explore a different aspect by examining the
robustness of fact recall to variation in prompts. A closely related line of work focuses
on adversarial attacks on LLMs [see Cho+24, for a review]. Specifically, prompt-based
adversarial attacks [Xu+23; Zhu+23; Wan+23b] focus on the manipulation of answers
within specific classification tasks while other works concentrate on safety issues [Liu+23a;
PR22; Zou+23; Apr+22; Wan+23a; Si+22; Rao+23; SMR23; Liu+23b]. There are also
works showing LLMs can be distracted by irrelevant contexts in problem solving [Shi+23],
question answering [Pet+20; CSH22; Yor+23] and factual reasoning [PE21]. Although
phenomena akin to context hijacking have been reported in different instances, the goals
of this paper are to give a systematic robustness study for fact retrieval, offer a framework
for interpreting it in the context of associative memory, and deepen our understanding of
LLMs.
3
Context hijacking in LLMs
In this section, we run experiments on LLMs including GPT-2 [Rad+19], Gemma [Tea+24]
(both base and instruct models) and LLaMA-2-7B [Tou+23] to explore the effects of context
hijacking on manipulating LLM outputs. As an example, consider Figure 1. When we
prompt the LLMs with the context “The Eiffel Tower is in the city of”, all 4 LLMs output the
3

(a) Hijacking generically
(b) Hijacking based on Relation ID P190
Figure 2: Context hijacking can cause LLMs to output false target. The figure shows efficacy score
versus the number of prepends for various LLMs on the CounterFact dataset under two hijacking
schemes.
correct answer (“Paris”). However, as we see in the example, we can actually manipulate
the output of the LLMs simply by modifying the context with additional factual informa-
tion that would not confuse a human. We call this context-hijacking. Due to the different
capacities and capabilties of each model, the examples in Figure 1 use different hijacking
techniques. This is most notable on LLaMA-2-7B, which is a much larger model than the
others. Of course, as expected, the more sophisticated attack on LLaMA also works on
GPT-2 and Gemma. Additionally, the instruction-tuned version of Gemma can understand
special words like “not” to some extent. Nevertheless, it is still possible to systematically
hijack these LLMs, as demonstrated below.
We explore this phenomenon at scale with the CounterFact dataset introduced in [Men+22],
a dataset of difficult counterfactual assertions containing a diverse set of subjects, rela-
tions, and linguistic variations. CounterFact has 21,919 samples, each of which are
given by a tuple (p,o∗,o_,s,r). From each sample, we have a context prompt p with a
true target answer o∗(target_true) and a false target answer o_ (target_false), e.g. the
prompt p = “Eiffel Tower can be found in” has true target o∗= “Paris” and false target
o_ = “Guam”. Additionally, the main entity in p is the subject s (s = “Eiffel Tower”) and the
prompt is categorized into relations r (for instance, other samples with the same relation
ID as the example above could be of the form “The location of {subject} is”, “{subject} can
be found in”, “Where is {subject}? It is in”). For additional details on how the dataset was
collected, see [Men+22].
For a hijacking scheme, we report the Efficacy Score (ES) [Men+22], which is the proportion
of samples for which the token probabilities satisfy Pr[o_] > Pr[o∗] after modifying the
context, that is, the proportion of the dataset that has been successfully manipulated. We
experiment with two hijacking schemes for this dataset. We first hijack by prepending the
text “Do not think of {target_false}” to each context. For instance, the prompt “The Eiffel
Tower is in” gets changed to “Do not think of Guam. The Eiffel Tower is in”. In Figure 2a,
we see that the efficacy score drops significantly after hijacking. Here, we prepend the
hijacking sentence k times for k = 0,...,5 where k = 0 yields the original prompt. We see
that additional prepends decrease the score further.
In the second scheme, we make use of the relation ID r to prepend factually correct
sentences. For instance, one can hijack the example above to “The Eiffel Tower is not
located in Guam. The Eiffel Tower is in”. We test this hijacking philosophy on different
relation IDs. In particular, Figure 2b reports hijacking based on relation ID P190 (“twin
city”). And we see similar patterns that with more prepends, the ES score gets lower. It is
also worth noting that one can even hijack by only including words that are semantically
close to the false target (e.g., “France” for false target “French”). This suggests that context
hijacking is more than simply the LLM copying tokens from contexts. Additional details and
4

experiments for both hijacking schemes and for other relation IDs are in Appendix B.
These experiments show that context hijacking changes the behavior of LLMs, leading them
to output incorrect tokens, without altering the factual meaning of the context. It is worth
noting that similar fragile behaviors of LLMs have been observed in the literature in differ-
ent contexts [Shi+23; Pet+20; CSH22; Yor+23; PE21]. See Section 2 for more details.
Context hijacking indicates that fact retrieval in LLMs is not robust and that accurate
fact recall does not necessarily depend on the semantics of the context. As a result,
one hypothesis is to view LLMs as an associative memory model where special tokens
in contexts, associated with the fact, provide partial information or clues to facilitate
memory retrieval [Zha23]. To better understand this perspective, we design a synthetic
memory retrieval task to evaluate how the building blocks of LLMs, transformers, can
solve it.
4
Problem setup
In the context of LLMs, fact or memory retrieval, can be modeled as a next token prediction
problem. Given a context (e.g., “The capital of France is”), the objective is to accurately
predict the next token (e.g., “Paris”) based on the factual relation between context and the
following token.
Previous papers [Ram+20; Mil+22; BP21; Zha23] have studied the connection between
attention and autoassociative and heteroassociative memory. For autoassociative memory,
contexts are modeled as a set of existing memories and the goal of self-attention is to
select the closest one or approximations to it. On top of this, heteroassociative memory
[Mil+22; BP21] has an additional projection to remap each output to a different one,
whether within the same space or otherwise. In both scenarios, the goal is to locate the
closest pattern within the context when provided with a query (up to a remapping if it’s
heteroassociative).
Fact retrieval, on the other hand, does not strictly follow this framework. The crux of the
issue is that the output token is not necessarily close to any particular token in the context
but rather a combination of them and the “closeness” is intuitively measured by latent
semantic concepts. For example, consider context sentence “The capital of France is” with
the output “Paris”. Here, none of the tokens in the context directly corresponds to the word
“Paris”. Yet some tokens contain partial information about “Paris”. Intuitively, “capital”
aligns with the “isCapital” concept of “Paris”, while “France” corresponds to the “isFrench”
concept linked to “Paris” where all the concepts are latent. To model such phenomenon, we
propose a synthetic task called latent concept association where the output token is closely
related to tokens in the context and similarity is measured via the latent space.
4.1
Latent concept association
We propose a synthetic prediction task where for each output token y, tokens in the context
(denoted by x) are sampled from a conditional distribution given y. Tokens that are similar
to y will be favored to appear more in the context, except for y itself. The task of latent
concept association is to successfully retrieve the token y given samples from p(x|y). The
synthetic setup simplifies by not accounting for the sequential nature of language, a choice
supported by previous experiments on context hijacking (Section 3). We formalize this task
below.
To measure similarity, we define a latent space. Here, the latent space is a collection
of m binary latent variables Zi. These could be viewed as semantic concept variables.
Let Z = (Z1,...,Zm) be the corresponding random vector, z be its realization, and Z be
the collection of all latent binary vectors. For each latent vector z, there’s one associated
5

token t ∈[V] = {0,...,V −1} where V is the total number of tokens. Here we represent the
tokenizer as ι where ι(z) = t. In this paper, we assume that ι is the standard tokenizer
where each binary vector is mapped to its decimal number. In other words, there’s a one to
one map between latent vectors and tokens. Because the map is one to one, we sometimes
use latent vectors and tokens interchangeably. We also assume that every latent binary
vector has a unique corresponding token, therefore V = 2m.
Under the latent concept association model, the goal is to retrieve specific output tokens
given partial information in the contexts. This is modeled by the latent conditional distri-
bution:
p(z|z∗) = ωπ(z|z∗)+(1−ω)Unif(Z )
where
π(z|z∗) ∝
(
exp(−DH(z, z∗)/β)
z ∈N (z∗),
0
z ∉N (z∗).
Here DH is the Hamming distance, N (z∗) is a subset of Z \ {z∗} and β > 0 is the tem-
perature parameter. The use of Hamming distance draws a parallel with the notion of
distributional semantics in natural language: “a word is characterized by the company
it keeps” [Fir57]. In words, p(z|z∗) says that with probability 1−ω, the conditional dis-
tribution uniformly generate random latent vectors and with probability ω, the latent
vector is generated from the informative conditional distribution π(z|z∗) where the support
of the conditional distribution is N (z∗). Here, π represents the informative conditional
distribution that depends on z∗whereas the uniform distribution is uninformative and
can be considered as noise. The mixture model parameter ω determines the signal to noise
ratio of the contexts.
Therefore, for any latent vector z∗and its associated token, one can generate L context
token words with the aforementioned latent conditional distribution:
• Uniformly sample a latent vector z∗
• For l = 1,...,L −1, sample zl ∼p(z|z∗) and tl = ι(zl).
• For l = L, sample z ∼π(z|z∗) and tL = ι(z).
Consequently, we have x = (t1,..,tL) and y = ι(z∗). The last token in the context is generated
specifically to make sure that it is not from the uniform distribution. This ensures that the
last token can use attention to look for clues, relevant to the output, in the context. Let DL
be the sampling distribution to generate (x, y) pairs. The conditional probability of y given
x is given by p(y|x). With slight abuse of notation, given a token t ∈[V], we define N (t) =
N (ι−1(t)). we also define DH(t,t′) = DH(ι−1(t),ι−1(t′)) for any pair of tokens t and t′.
For any function f that maps the context to estimated logits of output labels, the training
objective is to minimize this loss of the last position:
E(x,y)∈DL[ℓ(f (x), y)]
where ℓis the cross entropy loss with softmax. The error rate of latent concept association
is defined by the following:
RDL(f ) = P(x,y)∼DL[argmax f (x) ̸= y]
And the accuracy is 1−RDL(f ).
4.2
Transformer network architecture
Given a context x = (t1,..,tL) which consists of L tokens, we define X ∈{0,1}V×L to be its
one-hot encoding where V is the vocabulary size. Here we use χ to represent the one-hot
6

encoding function (i.e., χ(x) = X). Similar to [LLR23; Tar+23a; Li+24], we also consider
a simplified one-layer transformer model without residual connections and normaliza-
tion:
f L(x) =
h
WETWV attn(WEχ(x))
i
:L
(4.1)
where
attn(U) = Uσ
³(WKU)T(WQU)
p
da
´
,
WK ∈Rda×d is the key matrix, and WQ ∈Rda×d is the query matrix and da is the attention
head size. σ : RL×L →(0,1)L×L is the column-wise softmax operation. WV ∈Rd×d is the
value matrix and WE ∈Rd×V is the embedding matrix. Here, we adopt the weight tie-in
implementation which is used for Gemma [Tea+24]. We focus solely on the prediction of
the last position, as it is the only one relevant for latent concept association. For conve-
nience, we also use h(x) to mean
£
attn(WEχ(x))
¤
:L, which is the hidden representation after
attention for the last position, and f L
t (x) to represent the logit for output token t.
5
Theoretical analysis
In this section, we theoretically investigate how a single-layer transformer can solve the
latent concept association problem. We first introduce a hypothetical associative memory
model that utilizes self-attention for information aggregation and employs the value matrix
for memory retrieval. This hypothetical model turns out to mirror trained transformers
in experiments. We also examine the role of each individual component of the network:
the value matrix, embeddings, and the attention mechanism. We validate our theoretical
claims in Section 6.
5.1
Hypothetical associative memory model
In this section, we show that a simple single-layer transformer network can solve the latent
concept association problem. The formal result is presented below in Theorem 1; first we
require a few more definitions. Let WE(t) be the t-th column of the embedding matrix WE.
In other words, this is the embedding for token t. Given a token t, define N1(t) to be the
subset of tokens whose latent vectors are only 1 Hamming distance away from t’s latent
vector: N1(t) = {t′ : DH(t′,t)) = 1}∩N (t). For any output token t, N1(t) contains tokens
with the highest probabilities to appear in the context.
The following theorem formalizes the intuition that a one-layer transformer that uses
self-attention to summarize statistics about the context distributions and whose value
matrix uses aggregated representations to retrieve output tokens can solve the latent
concept association problem defined in Section 4.1.
Theorem 1 (informal). Suppose the data generating process follows Section 4.1 where
m ≥3, ω = 1, and N (t) = V \{t}. Then for any ε > 0, there exists a transformer model given
by (4.1) that achieves error ε, i.e. RDL(f L) < ε given sufficiently large context length L.
More precisely, for the transformer in Theorem 1, we will have WK = 0 and WQ = 0. Each
row of WE is orthogonal to each other and normalized. And WV is given by
WV =
X
t∈[V]
WE(t)(
X
t′∈N1(t)
WE(t′)T)
(5.1)
A more formal statement of the theorem and its proof is given in Appendix A (Theo-
rem 7).
7

Intuitively, Theorem 1 suggests having more samples from p(x|y) can lead to a better recall
rate. On the other hand, if contexts are modified to contain more samples from p(x| ˜y)
where ˜y ̸= y, then it is likely for transformer to output the wrong token. This is similar to
context hijacking (see Section 5.5). The construction of the value matrix is similar to the
associative memory model used in [Bie+24; CSB24], but in our case, there is no explicit
one-to-one input and output pairs stored as memories. Rather, a combination of inputs are
mapped to a single output.
While the construction in Theorem 1 is just one way that a single-layer transformer can
tackle this task, it turns out empirically this construction of WV is close to the trained WV ,
even in the noisy case (ω ̸= 1). In Section 6.1, we will demonstrate that substituting trained
value matrices with constructed ones can retain accuracy, and the constructed and trained
value matrices even share close low-rank approximations. Moreover, in this hypothetical
model, a simple uniform attention mechanism is deployed to allow self-attention to count oc-
currences of each individual tokens. Since the embeddings are orthonormal vectors, there is
no interference. Hence, the self-attention layer can be viewed as aggregating information of
contexts. It is worth noting that, in different settings, more sophisticated embedding struc-
tures and attention patterns are needed. This is discussed in the following sections.
5.2
On the role of the value matrix
The construction in Theorem 1 relies on the value matrix acting as associative memory.
But is it necessary? Could we integrate the functionality of the value matrix into the
self-attention module to solve the latent concept association problem? Empirically, the
answer seems to be negative as will be shown in Section 6.1. In particular, when the
context length is small, setting the value matrix to be the identity would lead to subpar
memory recall accuracy.
This is because if the value matrix is the identity, the transformer would be more susceptible
to the noise in the context. To see this, notice that given any pair of context and output
token (x, y), the latent representation after self-attention h(x) must live in the polyhedron
Sy to be classified correctly where Sy is defined as:
Sy = {v : (WE(y)−WE(t))Tv > 0 where t ̸∈[V]\{y}}
Note that, by definition, for any two tokens y and ˜y, Sy ∩S ˜y = ;. On the other hand,
because of the self-attention mechanism, h(x) must also live in the convex hull of all the
embedding vectors:
CV = Conv(WE(0),...,WE(|V|−1))
In other words, for any pair (x, y) to be classified correctly, h(x) must live in the intersection
of Sy and CV. Due to the stochastic nature of x, it is likely for h(x) to be outside of this
intersection. The remapping effect of the value matrix can help with this problem. The
following lemma explains this intuition.
Lemma 2. Suppose the data generating process follows Section 4.1 where m ≥3, ω = 1 and
N (t) = {t′ : DH(t,t′)) = 1}. For any single layer transformer given by (4.1) where each row of
WE is orthogonal to each other and normalized, if WV is constructed as in (5.1), then the
error rate is 0. If WV is the identity matrix, then the error rate is strictly larger than 0.
Another intriguing phenomenon occurs when the value matrix is the identity matrix. In this
case, the inner product between embeddings and their corresponding Hamming distance
varies linearly. This relationship can be formalized by the following theorem.
Theorem 3. Suppose the data generating process follows Section 4.1 where m ≥3, ω = 1
and N (t) = V \{t}. For any single layer transformer given by (4.1) with WV being the identity
8

matrix, if the cross entropy loss is minimized so that for any sampled pair (x, y),
p(y|x) = ˆp(y|x) = softmax(f L
y (x))
there exists a > 0 and b such that for two tokens t ̸= t′,
〈WE(t),WE(t′)〉= −aDH(t,t′)+ b
5.3
Embedding training and geometry
The hypothetical model in Section 5.1 requires embeddings to form an orthonormal basis.
In the overparameterization regime where the embedding dimension d is larger than
the number of tokens V, this can be approximately achieved by Gaussian initialization.
However, in practice, the embedding dimension is typically smaller than the vocabulary size,
in which case it is impossible for the embeddings to constitute such a basis. Empirically, in
Section 6.2, we observe that with overparameterization (d > V), embeddings can be frozen
at their Gaussian initialization, whereas in the underparameterized regime, embedding
training is required to achieve better recall accuracy.
This raises the question: What kind of embedding geometry is learned in the underparam-
eterized regime? Experiments reveal a close relationship between the inner product of
embeddings for two tokens and the Hamming distance of these tokens (see Figure 3b and
Figure C.5 in Appendix C.2). Approximately, we have the following relationship:
〈WE(t),WE(t′)〉=
(
b0
t = t′
−aDH(t,t′)+ b
t ̸= t′
(5.2)
for any two tokens t and t′ where b0 > b and a > 0. One can view this as a combination
of the embedding geometry under Gaussian initialization and the geometry when WV is
the identity matrix (Theorem 3). Importantly, this structure demonstrates that trained
embeddings inherently capture similarity within the latent space. Theoretically, this
embedding structure (5.2) can also lead to low error rate under specific conditions on b0,b
and a, which is articulated by the following theorem.
Theorem 4 (Informal). Following the same setup as in Theorem 1, but embeddings obey
(5.2), then under certain conditions on a,b and if b0 and context length L are sufficiently
large, the error rate can be arbitrarily small, i.e. RDL(f L) < ε for any 0 < ε < 1.
The formal statement of the theorem and its proof is given in Appendix A (Theorem 8).
Notably, this embedding geometry also implies a low-rank structure. Let’s first consider
the special case when b0 = b. In other words, the inner product between embeddings and
their corresponding Hamming distance varies linearly.
Lemma 5. If embeddings follow (5.2) and b = b0 and N (t) = V \{t}, then rank(WE) ≤m+2.
When b0 > b, the embedding matrix will not be strictly low rank. However, it can still exhibit
approximate low-rank behavior, characterized by an eigengap between the top and bottom
singular values. This is verified empirically (see Figure C.9-C.12 in Appendix C.4).
5.4
The role of attention selection
As of now, attention does not play a significant role in the analysis. But perhaps unsurpris-
ingly, the attention mechanism is useful in selecting relevant information. To see this, let’s
consider a specific setting where for any latent vector z∗, N (z∗) = {z : z∗
1 = z1}\{z∗}.
Essentially, latent vectors are partitioned into two clusters based on the value of the first
latent variable, and the informative conditional distribution π only samples latent vectors
9

that are in the same cluster as the output latent vector. Empirically, when trained under
this setting, the attention mechanism will pay more attention to tokens within the same
cluster (Section 6.3). This implies that the self-attention layer can mitigate noise and
concentrate on the informative conditional distribution π.
To understand this more intuitively, we will study the gradient of unnormalized attention
scores. In particular, the unnormalized attention score is defined as:
ut,t′ = (WKWE(t))T(WQWE(t′))/
p
da.
Lemma 6. Suppose the data generating process follows Section 4.1 and N (z∗) = {z : z∗
1 =
z1}\{z∗}. Given the last token in the sequence tL, then
∇ut,tL ℓ(f L) = ∇ℓ(f L)T(WE)TWV (αt ˆptWE(t)−ˆpt
L
X
l=1
ˆptlWE(tl))
where for token t, αt = PL
l=1 1[tl = t] and ˆpt is the normalized attention score for token t.
Typically, αt is larger when token t and tL belong to the same cluster because tokens within
the same cluster tend to co-occur frequently. As a result, the gradient contribution to the
unnormalized attention score is usually larger for tokens within the same cluster.
5.5
Context hijacking and the misclassification of memory re-
call
In light of the theoretical results on latent concept association, a natural question arises:
How do these results connect to context hijacking in LLMs? In essence, for the latent concept
association problem, the differentiation of output tokens is achieved by distinguishing
between the various conditional distributions p(x|y). Thus, adding or changing tokens
in the context x so that it resembles a different conditional distribution can result in
misclassification. In Appendix C.5, we present experiments showing that mixing different
contexts can cause transformers to misclassify. This partially explains context hijacking
in LLMs (Section 3). On the other hand, it is well-known that the error rate is related
to the KL divergence between conditional distributions of contexts [Cov99]. The closer
the distributions are, the easier it is for the model to misclassify. Here, longer contexts,
primarily composed of i.i.d samples, suggest larger divergences, thus higher memory recall
rate. This is theoretically implied by Theorem 1 and Theorem 4 and empirically verified in
Appendix C.6. Such result is also related to reverse context hijacking (Appendix B) where
prepending sentences including true target words can improve fact recall rate.
6
Experiments
The main implications of the theoretical results in the previous section are:
1. The value matrix is important and has associative memory structure as in (5.1).
2. Training embeddings is crucial in the underparameterized regime, where embeddings
exhibit certain geometric structures.
3. Attention mechanism is used to select the most relevant tokens.
To evaluate these claims, we conduct several experiments on synthetic datasets. Additional
experimental details and results can be found in Appendix C.
10

(a) Value matrix training
(b) Embedding structure
(c) Attention Pattern
Figure 3: Key components of the single-layer transformer working together on the latent concept
association problem. (a) Fixing the value matrix WV as the identity matrix results in lower accuracy
compared to training WV . The figure reports average accuracy for both fixed and trained WV with
L = 64. (b) When training in the underparameterized regime, the embedding structure is approximated
by (5.2). The graph displays the average inner product between embeddings of two tokens against the
corresponding Hamming distance between these tokens when m = 8. (c) The self-attention layer can
select tokens within the same cluster. The figure shows average attention score heat map with m = 8
and the cluster structure from Section 5.4.
6.1
On the value matrix WV
In this section, we study the necessity of the value matrix WV and its structure. First, we
conduct experiments to compare the effects of training versus freezing WV as the identity
matrix, with the context lengths L set to 64 and 128. Figure 3a and Figure C.1 show that
when the context length is small, freezing WV can lead to a significant decline in accuracy.
This is inline with Lemma 2 and validates it in a general setting, implying the significance
of the value matrix in maintaining a high memory recall rate.
Next, we investigate the degree of alignment between the trained value matrix WV and the
construction in (5.1). The first set of experiments examines the similarity in functionality
between the two matrices. We replace value matrices in trained transformers with the
constructed ones like in (5.1) and then report accuracy with the new value matrix. As a
baseline, we also consider randomly constructed value matrix, where the outer product
pairs are chosen randomly (detailed construction can be found in Appendix C.1). Figure C.2
indicates that the accuracy does not significantly decrease when the value matrix is
replaced with the constructed ones. Furthermore, not only are the constructed value
matrix and the trained value matrix functionally alike, but they also share similar low-
rank approximations. We use singular value decomposition to get the best low rank
approximations of various value matrices where the rank is set to be the same as the number
of latent variables (m). We then compute smallest principal angles between low-rank
approximations of trained value matrices and those of constructed, randomly constructed,
and Gaussian-initialized value matrices. Figure C.3 shows that the constructed ones have,
on average, smallest principal angles with the trained ones.
6.2
On the embeddings
In this section, we explore the significance of embedding training in the underparamerized
regime and embedding structures. We conduct experiments to compare the effects of
training versus freezing embeddings with different embedding dimensions. The learn-
ing rate is selected as the best option from {0.01,0.001} depending on the dimensions.
Figure C.4 clearly shows that when the dimension is smaller than the vocabulary size
(d < V), embedding training is required. It is not necessary in the overparameterized
regime (d > V), partially confirming Theorem 1 because if embeddings are initialized from
a high-dimensional multi-variate Gaussian, they are approximately orthogonal to each
other and have the same norms.
The next question is what kind of embedding structures are formed for trained transformers
11

in the underparamerized regime. From Figure 3b and Figure C.5, it is evident that the
relationship between the average inner product of embeddings for two tokens and their
corresponding Hamming distance roughly aligns with (5.2). Perhaps surprisingly, if we plot
the same graph for trained transformers with a fixed identity value matrix, the relationship
is mostly linear as shown in Figure C.6, confirming our theory (Theorem 3).
As suggested in Section 5.3, such embedding geometry (5.2) can lead to low rank structures.
We verify this claim by studying the spectrum of the embedding matrix WE. As illustrated
in Appendix C.4, Figure C.9-C.12 demonstrate that there are eigengaps between top and
bottom singular values, suggesting low-rank structures.
6.3
On the attention selection mechanism
In this section, we examine the role of attention pattern by considering a special class of
latent concept association model as defined in Section 5.4. Figure 3c and Figure C.7 clearly
show that the self-attention select tokens in the same clusters. This suggests that attention
can filter out noise and focus on the informative conditional distribution π. We extend
experiments to consider cluster structures that depend on the first two latent variables
(detailed construction can be found in Appendix C.3) and Figure C.8 shows attention
pattern as expected.
7
Conclusions
In this work, we first presented the phenomenon of context hijacking in LLMs, which
suggested that fact retrieval is not robust against variations of contexts. This indicates
that LLMs might function like associative memory where tokens in contexts are clues to
guide memory retrieval. To investigate this perspective further, we devised a synthetic
task called latent concept association and examined theoretically and empirically how
single-layer transformers are trained to solve this task. These results provide further
insights into the inner workings of transformers and LLMs, and can hopefully stimulate
further work into interpreting and understanding the mechanisms by which LLMs predict
tokens and recall facts.
Acknowledgments
We thank Victor Veitch for insightful discussions that helped shape
the initial idea of this work. We acknowledge the support of AFRL and DARPA via FA8750-
23-2-1015, ONR via N00014-23-1-2368, NSF via IIS-1909816, IIS-1955532, IIS-1956330,
and NIH R01GM140467. We also acknowledge the support of the Robert H. Topel Faculty
Research Fund at the University of Chicago Booth School of Business.
References
[AZL23a]
Z. Allen-Zhu and Y. Li. Physics of language models: part 3.1, knowledge
storage and extraction. 2023. arXiv: 2309.14316 [cs.CL] (cit. on p. 3).
[AZL23b]
Z. Allen-Zhu and Y. Li. Physics of language models: part 3.2, knowledge
manipulation. 2023. arXiv: 2309.14402 [cs.CL] (cit. on p. 3).
[AZL24]
Z. Allen-Zhu and Y. Li. Physics of language models: part 3.3, knowledge
capacity scaling laws. 2024. arXiv: 2404.05405 [cs.CL] (cit. on p. 3).
[Apr+22]
G. Apruzzese, H. S. Anderson, S. Dambra, D. Freeman, F. Pierazzi, and K. A.
Roundy. "real attackers don’t compute gradients": bridging the gap between
adversarial ml research and practice. 2022. arXiv: 2212.14315 [cs.CR] (cit.
on p. 3).
[Bai+24]
Y. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. “Transformers as statisticians:
provable in-context learning with in-context algorithm selection”. Advances
in neural information processing systems (2024) (cit. on p. 3).
12

[BYBOS95]
R. Ben-Yishai, R. L. Bar-Or, and H. Sompolinsky. “Theory of orientation
tuning in visual cortex.” Proceedings of the National Academy of Sciences 9
(1995) (cit. on p. 3).
[Bie+24]
A. Bietti, V. Cabannes, D. Bouchacourt, H. Jegou, and L. Bottou. “Birth
of a transformer: a memory viewpoint”. Advances in Neural Information
Processing Systems (2024) (cit. on pp. 3, 8).
[BP21]
T. Bricken and C. Pehlevan. “Attention approximates sparse distributed
memory”. Advances in Neural Information Processing Systems (2021) (cit. on
pp. 3, 5).
[Bro+20]
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A.
Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G.
Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter,
C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C.
Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language
models are few-shot learners. 2020. arXiv: 2005.14165 [cs.CL] (cit. on p. 1).
[CDB23]
V. Cabannes, E. Dohmatob, and A. Bietti. “Scaling laws for associative mem-
ories”. arXiv preprint arXiv:2310.02984 (2023) (cit. on p. 3).
[CSB24]
V. Cabannes, B. Simsek, and A. Bietti. “Learning associative memories with
gradient descent”. arXiv preprint arXiv:2402.18724 (2024) (cit. on pp. 3, 8).
[Cha22]
F. Charton. “What is my math transformer doing?–three results on inter-
pretability and generalization”. arXiv preprint arXiv:2211.00170 (2022) (cit.
on p. 3).
[Cho+24]
A. G. Chowdhury, M. M. Islam, V. Kumar, F. H. Shezan, V. Jain, and A.
Chadha. “Breaking down the defenses: a comparative survey of attacks on
large language models”. arXiv preprint arXiv:2403.04786 (2024) (cit. on p. 3).
[Cov99]
T. M. Cover. Elements of information theory. 1999 (cit. on p. 10).
[CSH22]
A. Creswell, M. Shanahan, and I. Higgins. “Selection-inference: exploiting
large language models for interpretable logical reasoning”. arXiv preprint
arXiv:2205.09712 (2022) (cit. on pp. 2, 3, 5).
[Dai+21]
D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. “Knowledge neurons
in pretrained transformers”. arXiv preprint arXiv:2104.08696 (2021) (cit. on
pp. 1, 3).
[DCAT21]
N. De Cao, W. Aziz, and I. Titov. “Editing factual knowledge in language
models”. arXiv preprint arXiv:2104.08164 (2021) (cit. on pp. 1, 3).
[Dev83]
L. Devroye. “The equivalence of weak, strong and complete convergence in l1
for kernel density estimates”. The Annals of Statistics 3 (1983) (cit. on p. 18).
[Ede+24]
B. L. Edelman, E. Edelman, S. Goel, E. Malach, and N. Tsilivis. “The evolution
of statistical induction heads: in-context learning markov chains”. arXiv
preprint arXiv:2402.11004 (2024) (cit. on p. 3).
[Elh+21]
N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell,
Y. Bai, A. Chen, T. Conerly, et al. “A mathematical framework for transformer
circuits”. Transformer Circuits Thread (2021) (cit. on p. 3).
[EI+24]
M Emrullah Ildiz, Y. Huang, Y. Li, A. Singh Rawat, and S. Oymak. “From
self-attention to markov models: unveiling the dynamics of generative trans-
formers”. arXiv e-prints (2024) (cit. on p. 3).
[Fel20]
V. Feldman. “Does learning require memorization? a short tale about a
long tail”. In: Proceedings of the 52nd Annual ACM SIGACT Symposium on
Theory of Computing. 2020 (cit. on p. 3).
[FZ20]
V. Feldman and C. Zhang. “What neural networks memorize and why: discov-
ering the long tail via influence estimation”. Advances in Neural Information
Processing Systems (2020) (cit. on p. 3).
[Fir57]
J. Firth. “A synopsis of linguistic theory, 1930-1955”. Studies in linguistic
analysis (1957) (cit. on p. 6).
13

[Gar+22]
S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. “What can transformers
learn in-context? a case study of simple function classes”. Advances in Neural
Information Processing Systems (2022) (cit. on p. 3).
[Gei+21]
A. Geiger, H. Lu, T. Icard, and C. Potts. “Causal abstractions of neural
networks”. Advances in Neural Information Processing Systems (2021) (cit.
on p. 3).
[Gei+22]
A. Geiger, Z. Wu, H. Lu, J. Rozner, E. Kreiss, T. Icard, N. Goodman, and
C. Potts. “Inducing causal structure for interpretable neural networks”. In:
International Conference on Machine Learning. PMLR. 2022 (cit. on p. 3).
[Gei+24]
A. Geiger, Z. Wu, C. Potts, T. Icard, and N. Goodman. “Finding alignments be-
tween interpretable causal variables and distributed neural representations”.
In: Causal Learning and Reasoning. PMLR. 2024 (cit. on p. 3).
[Gev+23]
M. Geva, J. Bastings, K. Filippova, and A. Globerson. “Dissecting recall of
factual associations in auto-regressive language models”. arXiv preprint
arXiv:2304.14767 (2023) (cit. on p. 3).
[Has+24]
P. Hase, M. Bansal, B. Kim, and A. Ghandeharioun. “Does localization inform
editing? surprising differences in causality-based localization vs. knowledge
editing in language models”. Advances in Neural Information Processing
Systems (2024) (cit. on p. 3).
[Hen+23]
T. Henighan, S. Carter, T. Hume, N. Elhage, R. Lasenby, S. Fort, N. Schiefer,
and C. Olah. “Superposition, memorization, and double descent”. Trans-
former Circuits Thread (2023) (cit. on p. 3).
[Hop82]
J. J. Hopfield. “Neural networks and physical systems with emergent collec-
tive computational abilities.” Proceedings of the national academy of sciences
8 (1982) (cit. on p. 3).
[Hu+21]
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
W. Chen. Lora: low-rank adaptation of large language models. 2021. arXiv:
2106.09685 [cs.CL] (cit. on p. 2).
[JP20]
Y. Jiang and C. Pehlevan. “Associative memory in iterated overparameterized
sigmoid autoencoders”. In: International conference on machine learning.
PMLR. 2020 (cit. on p. 3).
[Jia+24]
Y. Jiang, G. Rajendran, P. Ravikumar, B. Aragam, and V. Veitch. “On the
origins of linear representations in large language models”. arXiv preprint
arXiv:2403.03867 (2024) (cit. on p. 3).
[Jin+23]
T. Jin, N. Clement, X. Dong, V. Nagarajan, M. Carbin, J. Ragan-Kelley,
and G. K. Dziugaite. “The cost of down-scaling language models: fact recall
deteriorates before in-context learning”. arXiv preprint arXiv:2310.04680
(2023) (cit. on p. 3).
[KKM22]
J. Kim, M. Kim, and B. Mozafari. “Provable memorization capacity of trans-
formers”. In: The Eleventh International Conference on Learning Representa-
tions. 2022 (cit. on p. 3).
[Li+24]
Y. Li, Y. Huang, M. E. Ildiz, A. S. Rawat, and S. Oymak. “Mechanics of
next token prediction with self-attention”. In: International Conference on
Artificial Intelligence and Statistics. PMLR. 2024 (cit. on pp. 3, 7).
[LLR23]
Y. Li, Y. Li, and A. Risteski. “How do transformers learn topic structure:
towards a mechanistic understanding”. In: International Conference on Ma-
chine Learning. PMLR. 2023 (cit. on pp. 3, 7).
[Liu+23a]
Y. Liu, G. Deng, Y. Li, K. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, and
Y. Liu. “Prompt injection attack against llm-integrated applications”. arXiv
preprint arXiv:2306.05499 (2023) (cit. on p. 3).
[Liu+23b]
Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang, and
Y. Liu. “Jailbreaking chatgpt via prompt engineering: an empirical study”.
arXiv preprint arXiv:2305.13860 (2023) (cit. on p. 3).
14

[Liu+22]
Z. Liu, O. Kitouni, N. S. Nolte, E. Michaud, M. Tegmark, and M. Williams.
“Towards understanding grokking: an effective theory of representation
learning”. Advances in Neural Information Processing Systems (2022) (cit. on
p. 3).
[LH17]
I. Loshchilov and F. Hutter. “Decoupled weight decay regularization”. arXiv
preprint arXiv:1711.05101 (2017) (cit. on p. 29).
[MLT23]
S. Mahdavi, R. Liao, and C. Thrampoulidis. “Memorization capacity of multi-
head attention in transformers”. arXiv preprint arXiv:2306.02010 (2023)
(cit. on p. 3).
[Mak+24]
A. V. Makkuva, M. Bondaschi, A. Girish, A. Nagle, M. Jaggi, H. Kim, and
M. Gastpar. Attention with markov: a framework for principled analysis of
transformers via markov chains. 2024. arXiv: 2402.04161 [cs.LG] (cit. on
p. 3).
[McG+23]
T. McGrath, M. Rahtz, J. Kramar, V. Mikulik, and S. Legg. “The hydra
effect: emergent self-repair in language model computations”. arXiv preprint
arXiv:2307.15771 (2023) (cit. on p. 3).
[Men+22]
K. Meng, D. Bau, A. Andonian, and Y. Belinkov. “Locating and editing factual
associations in gpt”. Advances in Neural Information Processing Systems
(2022) (cit. on pp. 1–4, 27).
[Men+23]
K. Meng, A. S. Sharma, A. Andonian, Y. Belinkov, and D. Bau. Mass-editing
memory in a transformer. 2023. arXiv: 2210.07229 [cs.CL] (cit. on pp. 1, 3).
[Mil+22]
B. Millidge, T. Salvatori, Y. Song, T. Lukasiewicz, and R. Bogacz. “Universal
hopfield networks: a general framework for single-shot associative memory
models”. In: International Conference on Machine Learning. PMLR. 2022
(cit. on pp. 3, 5).
[Mit+21]
E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. “Fast model
editing at scale”. arXiv preprint arXiv:2110.11309 (2021) (cit. on pp. 1, 3).
[Mit+22]
E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn. “Memory-based
model editing at scale”. In: International Conference on Machine Learning.
PMLR. 2022 (cit. on pp. 1, 3).
[Nan+23]
N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. “Progress
measures for grokking via mechanistic interpretability”. arXiv preprint
arXiv:2301.05217 (2023) (cit. on p. 3).
[NDL24]
E. Nichani, A. Damian, and J. D. Lee. How transformers learn causal struc-
ture with gradient descent. 2024. arXiv: 2402.14735 [cs.LG] (cit. on p. 3).
[Ols+22a]
C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan,
B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z.
Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt,
K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and
C. Olah. In-context learning and induction heads. 2022. arXiv: 2209.11895
[cs.LG] (cit. on p. 3).
[Ols+22b]
C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B.
Mann, A. Askell, Y. Bai, A. Chen, et al. “In-context learning and induction
heads”. arXiv preprint arXiv:2209.11895 (2022) (cit. on p. 3).
[PE21]
L. Pandia and A. Ettinger. “Sorting through the noise: testing robustness
of information processing in pre-trained language models”. arXiv preprint
arXiv:2109.12393 (2021) (cit. on pp. 2, 3, 5).
[PR22]
F. Perez and I. Ribeiro. “Ignore previous prompt: attack techniques for lan-
guage models”. arXiv preprint arXiv:2211.09527 (2022) (cit. on p. 3).
[Pet+20]
F. Petroni, P. Lewis, A. Piktus, T. Rocktäschel, Y. Wu, A. H. Miller, and S.
Riedel. “How context affects language models’ factual predictions”. arXiv
preprint arXiv:2005.04611 (2020) (cit. on pp. 2, 3, 5).
15

[Rad+19]
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. “Lan-
guage models are unsupervised multitask learners”. OpenAI blog 8 (2019)
(cit. on pp. 1–3).
[RBU20]
A. Radhakrishnan, M. Belkin, and C. Uhler. “Overparameterized neural net-
works implement associative memory”. Proceedings of the National Academy
of Sciences 44 (2020) (cit. on p. 3).
[Raj+24]
G. Rajendran, S. Buchholz, B. Aragam, B. Schölkopf, and P. Ravikumar.
“Learning interpretable concepts: unifying causal representation learning
and foundation models”. arXiv preprint arXiv:2402.09236 (2024) (cit. on p. 3).
[Ram+20]
H. Ramsauer, B. Schäfl, J. Lehner, P. Seidl, M. Widrich, T. Adler, L. Gruber,
M. Holzleitner, M. Pavlović, G. K. Sandve, et al. “Hopfield networks is all
you need”. arXiv preprint arXiv:2008.02217 (2020) (cit. on pp. 3, 5).
[Rao+23]
A. Rao, S. Vashistha, A. Naik, S. Aditya, and M. Choudhury. “Tricking llms
into disobedience: understanding, analyzing, and preventing jailbreaks”.
arXiv preprint arXiv:2305.14965 (2023) (cit. on p. 3).
[Sak+23]
M. Sakarvadia, A. Ajith, A. Khan, D. Grzenda, N. Hudson, A. Bauer, K.
Chard, and I. Foster. “Memory injections: correcting multi-hop reasoning
failures during inference in transformer-based language models”. arXiv
preprint arXiv:2309.05605 (2023) (cit. on p. 3).
[Seu96]
H. S. Seung. “How the brain keeps the eyes still”. Proceedings of the National
Academy of Sciences 23 (1996) (cit. on p. 3).
[SMR23]
M. Shanahan, K. McDonell, and L. Reynolds. “Role play with large language
models”. Nature 7987 (2023) (cit. on p. 3).
[She+24]
H. Sheen, S. Chen, T. Wang, and H. H. Zhou. “Implicit regularization of gra-
dient flow on one-layer softmax attention”. arXiv preprint arXiv:2403.08699
(2024) (cit. on p. 3).
[Shi+23]
F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Schärli, and D.
Zhou. “Large language models can be easily distracted by irrelevant context”.
In: International Conference on Machine Learning. PMLR. 2023 (cit. on pp. 2,
3, 5).
[Si+22]
W. M. Si, M. Backes, J. Blackburn, E. De Cristofaro, G. Stringhini, S. Zannet-
tou, and Y. Zhang. “Why so toxic? measuring and triggering toxic behavior in
open-domain chatbots”. In: Proceedings of the 2022 ACM SIGSAC Conference
on Computer and Communications Security. 2022 (cit. on p. 3).
[Ska+94]
W. Skaggs, J. Knierim, H. Kudrimoti, and B. McNaughton. “A model of the
neural basis of the rat’s sense of direction”. Advances in neural information
processing systems (1994) (cit. on p. 3).
[SS22]
J. Steinberg and H. Sompolinsky. “Associative memory of structured knowl-
edge”. Scientific Reports 1 (2022) (cit. on p. 3).
[Tar+23a]
D. A. Tarzanagh, Y. Li, C. Thrampoulidis, and S. Oymak. “Transformers as
support vector machines”. arXiv preprint arXiv:2308.16898 (2023) (cit. on
pp. 3, 7).
[Tar+23b]
D. A. Tarzanagh, Y. Li, X. Zhang, and S. Oymak. “Margin maximization in
attention mechanism”. arXiv preprint arXiv:2306.13596 (2023) (cit. on p. 3).
[Tea+24]
G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak,
L. Sifre, M. Rivière, M. S. Kale, J. Love, et al. “Gemma: open models based
on gemini research and technology”. arXiv preprint arXiv:2403.08295 (2024)
(cit. on pp. 2, 3, 7).
[Tia+24]
B. Tian, S. Cheng, X. Liang, N. Zhang, Y. Hu, K. Xue, Y. Gou, X. Chen, and H.
Chen. “Instructedit: instruction-based knowledge editing for large language
models”. arXiv preprint arXiv:2402.16123 (2024) (cit. on p. 3).
[Tia+23a]
Y. Tian, Y. Wang, B. Chen, and S. S. Du. “Scan and snap: understanding
training dynamics and token composition in 1-layer transformer”. Advances
in Neural Information Processing Systems (2023) (cit. on p. 3).
16

[Tia+23b]
Y. Tian, Y. Wang, Z. Zhang, B. Chen, and S. Du. “Joma: demystifying multi-
layer transformers via joint dynamics of mlp and attention”. arXiv preprint
arXiv:2310.00535 (2023) (cit. on p. 3).
[Tou+23]
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. “Llama 2: open foundation
and fine-tuned chat models”. arXiv preprint arXiv:2307.09288 (2023) (cit. on
pp. 2, 3).
[Vas+17]
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin. “Attention is all you need”. Advances in neural
information processing systems (2017) (cit. on pp. 2, 3).
[Wan+23a]
B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong, R.
Dutta, R. Schaeffer, et al. “Decodingtrust: a comprehensive assessment of
trustworthiness in gpt models”. arXiv preprint arXiv:2306.11698 (2023) (cit.
on p. 3).
[Wan+23b]
J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang,
W. Ye, X. Geng, et al. “On the robustness of chatgpt: an adversarial and out-
of-distribution perspective”. arXiv preprint arXiv:2302.12095 (2023) (cit. on
p. 3).
[Wan+22]
K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. “Inter-
pretability in the wild: a circuit for indirect object identification in gpt-2
small”. arXiv preprint arXiv:2211.00593 (2022) (cit. on p. 3).
[Wu+24]
Z. Wu, A. Geiger, T. Icard, C. Potts, and N. Goodman. “Interpretability
at scale: identifying causal mechanisms in alpaca”. Advances in Neural
Information Processing Systems (2024) (cit. on p. 3).
[Xie+21]
S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. “An explanation of in-context
learning as implicit bayesian inference”. arXiv preprint arXiv:2111.02080
(2021) (cit. on p. 3).
[Xu+23]
X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, and M. Kankanhalli.
“An llm can fool itself: a prompt-based adversarial attack”. arXiv preprint
arXiv:2310.13345 (2023) (cit. on p. 3).
[Yor+23]
O. Yoran, T. Wolfson, O. Ram, and J. Berant. “Making retrieval-augmented
language models robust to irrelevant context”. arXiv preprint arXiv:2310.01558
(2023) (cit. on pp. 2, 3, 5).
[Zha+22]
Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner.
“Unveiling transformers with lego: a synthetic reasoning task”. arXiv preprint
arXiv:2206.04301 (2022) (cit. on p. 3).
[Zha+23]
Z. Zhang, M. Fang, L. Chen, M.-R. Namazi-Rad, and J. Wang. “How do large
language models capture the ever-changing world knowledge? a review of
recent advances”. arXiv preprint arXiv:2310.07343 (2023) (cit. on p. 3).
[Zha23]
J. Zhao. “In-context exemplars as clues to retrieving from large associative
memory”. arXiv preprint arXiv:2311.03498 (2023) (cit. on pp. 3, 5).
[Zhe+23]
C. Zheng, L. Li, Q. Dong, Y. Fan, Z. Wu, J. Xu, and B. Chang. “Can we edit
factual knowledge by in-context learning?” arXiv preprint arXiv:2305.12740
(2023) (cit. on p. 3).
[Zho+24]
Z. Zhong, Z. Liu, M. Tegmark, and J. Andreas. “The clock and the pizza: two
stories in mechanistic explanation of neural networks”. Advances in Neural
Information Processing Systems (2024) (cit. on p. 3).
[Zhu+23]
K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye,
N. Z. Gong, Y. Zhang, et al. “Promptbench: towards evaluating the robust-
ness of large language models on adversarial prompts”. arXiv preprint
arXiv:2306.04528 (2023) (cit. on p. 3).
[Zou+23]
A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson. Univer-
sal and transferable adversarial attacks on aligned language models. 2023.
arXiv: 2307.15043 [cs.CL] (cit. on p. 3).
17

A
Additional Theoretical Results and Proofs
A.1
Proofs for Section 5.1
Theorem 1 can be stated more formally as follows:
Theorem 7. Suppose the data generating process follows Section 4.1 where m ≥3, ω = 1,
and N (t) = V \{t}. Assume there exists a single layer transformer given by (4.1) such that
a) WK = 0 and WQ = 0, b) Each row of WE is orthogonal to each other and normalized, and
c) WV is given by
WV =
X
i∈[V]
WE(i)(
X
j∈N1(i)
WE(j)T).
Then if L > max{
100m2 log(3/ε)
(exp(−1
β )−exp(−2
β ))2 ,
80m2|N (y)|
(exp(−1
β )−exp(−2
β ))2 } for any y, then
RDL(f L) ≤ε,
where 0 < ε < 1.
Proof. First of all, the error is defined to be:
RDL(f L) = P(x,y)∼DL[argmax f L(x) ̸= y]
= PyPx|y[argmax f L(x) ̸= y]
Let’s focus on the conditional probability Px|y[argmax f L(x) ̸= y].
By construction, the single layer transformer model has uniform attention. Therefore,
h(x) =
X
i∈N (y)
αiWE(i)
where αi = 1
L
PL
k=1 1{tk = i} which is the number of occurrence of token i in the sequence.
By the latent concept association model, we know that
p(i|y) = exp(−DH(i, y)/β)
Z
where Z = P
i∈N (y) exp(−DH(i, y)/β).
Thus, the logit for token y is
f L
y (x) =
X
i∈N1(y)
αi
And the logit for any other token ˜y is
f L
˜y (x) =
X
i∈N1( ˜y)
αi
For the prediction to be correct, we need
max
˜y
f L
y (x)−f L
˜y (x) > 0
By Lemma 3 of [Dev83], we know that for all ∆∈(0,1), if |N (y)|
L
≤∆2
20 , we have
P
¡
max
i∈N (y)|αi −p(i|y)| > ∆
¢
≤P
¡
X
i∈N (y)
|αi −p(i|y)| > ∆
¢
≤3exp(−L∆2/25)
18

Therefore, if L ≥max{ 25log(3/ε)
∆2
, 20|N (y)|
∆2
}, then with probability at least 1−ε, we have,
max
i∈N (y)|αi −p(i|y)| ≤∆
f L
y (x)−f L
˜y (x) =
X
i∈N1(y)
αi −
X
j∈N1( ˜y)
αj
=
X
i∈N1(y)
αi −
X
i∈N1(y)
p(i|y)+
X
i∈N1(y)
p(i|y)
−
X
j∈N1( ˜y)
p(j|y)+
X
j∈N1( ˜y)
p(j|y)−
X
j∈N1( ˜y)
αj
≥
X
i∈N1(y)
p(i|y)−
X
j∈N1( ˜y)
p(j|y)−2m∆
≥exp(−1
β)−exp(−2
β)−2m∆
Note that because of Lemma 10, there’s no neighboring set that is the superset of another.
Therefore as long as ∆<
exp(−1
β )−exp(−2
β )
2m
,
f L
y (x)−f L
˜y (x) > 0
for any ˜y.
Finally, if L > max{
100m2 log(3/ε)
(exp(−1
β )−exp(−2
β ))2 ,
80m2|N (y)|
(exp(−1
β )−exp(−2
β ))2 } for any y, then
Px|y[argmax f L(x) ̸= y] ≤ε
And
RDL(f L) = P(x,y)∼DL[argmax f L(x) ̸= y]
= PyPx|y[argmax f L(x) ̸= y] ≤ε
A.2
Proofs for Section 5.2
Lemma 2. Suppose the data generating process follows Section 4.1 where m ≥3, ω = 1 and
N (t) = {t′ : DH(t,t′)) = 1}. For any single layer transformer given by (4.1) where each row of
WE is orthogonal to each other and normalized, if WV is constructed as in (5.1), then the
error rate is 0. If WV is the identity matrix, then the error rate is strictly larger than 0.
Proof. Following the proof for Theorem 7, let’s focus on the conditional probability:
Px|y[argmax f L(x) ̸= y]
By construction, we have
h(x) =
X
i∈N1(y)
αiWE(i)
where αi = 1
L
PL
k=1 1{tk = i} which is the number of occurrence of token i in the sequence.
Let’s consider the first case where WV is constructed as in (5.1). Then we know that for
some other token ˜y ̸= y,
f L
y (x)−f L
˜y (x) =
X
i∈N1(y)
αi −
X
i∈N1( ˜y)
αi = 1−
X
i∈N1( ˜y)
αi
19

By Lemma 10, we have that for any token ˜y ̸= y,
f L
y (x)−f L
˜y (x) > 0
Therefore, the error rate is always 0.
Now let’s consider the second case where WV is the identity matrix. Let j be a token in the
set N1(y). Then there is a non-zero probability that context x contains only j. In that case,
h(x) = WE(j)
However, we know that by the assumption on the embedding matrix,
f L
y (x)−f L
j (x) = (WE(y)−WE(j))Th(x) = −∥WE(j)∥2 < 0
This implies that there’s non zero probability that y is misclassified. Therefore, when WV
is the identity matrix, the error rate is strictly larger than 0.
Theorem 3. Suppose the data generating process follows Section 4.1 where m ≥3, ω = 1
and N (t) = V \{t}. For any single layer transformer given by (4.1) with WV being the identity
matrix, if the cross entropy loss is minimized so that for any sampled pair (x, y),
p(y|x) = ˆp(y|x) = softmax(f L
y (x))
there exists a > 0 and b such that for two tokens t ̸= t′,
〈WE(t),WE(t′)〉= −aDH(t,t′)+ b
Proof. Because for any pair of (x, y), the estimated conditional probability matches the true
conditional probability. In particular, let’s consider two target tokens y1, y2 and context
x = (ti,...,ti) for some token ti such that p(x|y1) > 0 and p(x|y2) > 0, then
p(y1|x)
p(y2|x) = p(x|y1)p(y1)
p(x|y2)p(y2) = p(x|y1)
p(x|y2) = ˆp(x|y1)
ˆp(x|y2) = exp((WE(y1)−WE(y2))Th(x))
The second equality is because p(y) is the uniform distribution. By our construction,
p(x|y1)
p(x|y2) = p(ti|y1)L
p(ti|y2)L = exp((WE(y2)−WE(y1))Th(x)) = exp((WE(y1)−WE(y2))TWE(ti))
By the data generating process, we have that
L
β (DH(ti, y2)−DH(ti, y1)) = (WE(y1)−WE(y2))TWE(ti)
Let ti = y3 such that y3 ̸= y1, y3 ̸= y2, then
L
β DH(y3, y1)−WE(y1)TWE(y3) = L
β DH(y3, y2)−WE(y2)TWE(y3)
For simplicity, let’s define
Ψ(y1, y2) = L
β DH(y1, y2)−WE(y1)TWE(y2)
Therefore,
Ψ(y3, y1) = Ψ(y3, y2)
Now consider five distinct labels: y1, y2, y3, y4, y5. We have,
Ψ(y3, y1) = Ψ(y3, y2) = Ψ(y4, y2) = Ψ(y4, y5)
20

In other words, Ψ(y3, y1) = Ψ(y4, y5) for arbitrarily chosen distinct labels y1, y3, y4, y5.
Therefore, Ψ(t,t′) is a constant for t ̸= t′.
For any two tokens t ̸= t′,
L
β DH(t,t′)−WE(t)TWE(t′) = C
Thus,
WE(t)TWE(t′) = −L
β DH(t,t′)+C
A.3
Proofs for Section 5.3
Theorem 4 can be formalized as the following theorem.
Theorem 8. Following the same setup as in Theorem 7, but embeddings follow (5.2) then
if b > 0, ∆1 > 0, 0 < ∆<
exp(−1
β )−exp(−2
β )
2m
, L ≥max{ 25log(3/ε)
∆2
, 20|N (y)|
∆2
} for any y, and
0 < a <
2exp( 1
β)
(|V|−2)m2
and
b0 > max{
a(m−2)m+∆1
exp(−1
β)−exp(−2
β)−2m∆
+b,
(b −a)∆1 −|V|−2
2
abm2 exp(−1
β)+ |V|−2
2
a2(m−2)m2
1−|V|−2
2
am2 exp(−1
β)
}
we have
RDL(f L) ≤ε
where 0 < ε < 1.
Proof. Following the proof of Theorem 7, let’s also focus on the conditional probability
Px|y[argmax f L(x) ̸= y]
By construction, the single layer transformer model has uniform attention. Therefore,
h(x) =
X
i∈N (y)
αiWE(i)
where αi = 1
L
PL
k=1 1{tk = i} which is the number of occurrence of token i in the sequence.
For simplicity, let’s define αy = 0 such that
h(x) =
X
i∈[V]
αiWE(i)
Similarly, we also have that if L ≥max{ 25log(3/ε)
∆2
, 20|N (y)|
∆2
}, then with probability at least
1−ε, we have,
max
i∈[V]|αi −p(i|y)| ≤∆
Also define the following:
φk(x) =
X
j∈N1(k)
WE(j)T¡ X
i∈[V]
αiWE(i)
¢
vk(y) = WE(y)TWE(k)
21

Thus, the logit for token y is
f L
y (x) =
|V|−1
X
k=0
vk(y)φk(x)
Let’s investigate φk(x). By Lemma 9,
φk(x) =
X
i∈[V]
αi(
X
j∈N1(k)
WE(j)TWE(i))
= (b0 −b)
X
j∈N1(k)
αj +
X
i∈[V]
αi(−a(m−2)DH(k, i)+(b −a)m)
Thus, for any k1,k2 ∈[V],
φk1(x)−φk2(x) = (b0 −b)(
X
j1∈N1(k1)
αj1 −
X
j2∈N1(k2)
αj2)
+
X
i∈[V]
αia(m−2)(DH(k2, i)−DH(k1, i))
Because −m ≤DH(k2, i)−DH(k1, i) ≤m, we have
(b0 −b)(
X
j1∈N1(k1)
αj1 −
X
j2∈N1(k2)
αj2)−a(m−2)m
≤φk1(x)−φk2(x) ≤
(b0 −b)(
X
j1∈N1(k1)
αj1 −
X
j2∈N1(k2)
αj2)+ a(m−2)m
For prediction to be correct, we need
max
˜y
f L
y (x)−f L
˜y (x) > 0
This also means that
max
˜y
|V|−1
X
k=0
¡
vk(y)−vk( ˜y)
¢
φk(x) > 0
One can show that for any k, if ι−1( ˜k) = ι−1(y)⊗ι−1( ˜y)⊗ι−1(k) where ⊗means bitwise XOR,
then
vk(y)−vk( ˜y) = v ˜k( ˜y)−v ˜k(y)
(A.1)
First of all, if k = y, then ˜k = ˜y, which means
vk(y)−vk( ˜y) = v ˜k( ˜y)−v ˜k(y) = b0 + aDH(y, ˜y)−b
If k ̸= y, ˜y, then (A.1) implies that
DH(k, y)−DH(k, ˜y) = DH( ˜k, ˜y)−DH( ˜k, y)
We know that DH(k, y) is the number of 1s in ι−1(k)⊗ι−1(y) and,
ι−1( ˜k)⊗ι−1(y) = ι−1(y)⊗ι−1( ˜y)⊗ι−1(k)⊗ι−1(y) = ι−1( ˜y)⊗ι−1(k)
Similarly,
ι−1( ˜k)⊗ι−1( ˜y) = ι−1(y)⊗ι−1(k)
22

Therefore, (A.1) holds and we can rewrite f L
y (x)−f L
˜y (x) as
f L
y (x)−f L
˜y (x) =
|V|−1
X
k=0
¡
vk(y)−vk( ˜y)
¢
φk(x)
= (b0 −b + aDH(y, ˜y))(φy(x)−φ ˜y(x))
+
X
k̸=y, ˜y,DH(k,y)≥DH(k, ˜y)
a(DH(k, y)−DH(k, ˜y))(φk(x)−φ ˜k(x))
We already know that b0 > b > 0 and a > 0, thus, b0 −b + aDH(y, ˜y) > 0 for any pair y, ˜y.
We also want φy(x)−φ ˜y(x) to be positive. Note that
φy(x)−φ ˜y(x) ≥(b0 −b)(exp(−1
β)−exp(−2
β)−2m∆)−a(m−2)m
We need ∆<
exp(−1
β )−exp(−2
β )
2m
and for some positive ∆1 > 0, b0 needs to be large enough such
that
φy(x)−φ ˜y(x) > ∆1
which implies that
b0 >
a(m−2)m+∆1
exp(−1
β)−exp(−2
β)−2m∆
+ b
(A.2)
On the other hand, for k ̸= y, ˜y, we have
φk(x)−φ ˜k(x) ≥(b0 −b)(
X
j1∈N1(k)
αj1 −
X
j2∈N1( ˜k)
αj2)−a(m−2)m
≥(b0 −b)(−(m−1)exp(−1
β)−exp(−2
β)−2m∆)−a(m−2)m
≥(b0 −b)(−(m−1)exp(−1
β)−exp(−2
β)+exp(−2
β)−exp(−1
β))−a(m−2)m
≥−(b0 −b)mexp(−1
β)−a(m−2)m
Then, we have
f L
y (x)−f L
˜y (x) ≥(b0 −b + a)∆1 −|V|−2
2
³
(b0 −b)am2 exp(−1
β)+ a2(m−2)m2´
≥
³
1−|V|−2
2
am2 exp(−1
β)
´
b0 −(b −a)∆1 + |V|−2
2
abm2 exp(−1
β)−|V|−2
2
a2(m−2)m2
The lower bound is independent of ˜y, therefore, we need it to be positive to ensure the
prediction is correct. To achieve this, we want
1−|V|−2
2
am2 exp(−1
β) > 0
which implies that
a <
2exp( 1
β)
(|V|−2)m2
(A.3)
And finally we need
b0 >
(b −a)∆1 −|V|−2
2
abm2 exp(−1
β)+ |V|−2
2
a2(m−2)m2
1−|V|−2
2
am2 exp(−1
β)
(A.4)
23

To summarize, if b > 0, ∆1 > 0, 0 < ∆<
exp(−1
β )−exp(−2
β )
2m
, L ≥max{ 25log(3/ε)
∆2
, 20|N (y)|
∆2
} for any
y, and
0 < a <
2exp( 1
β)
(|V|−2)m2
and
b0 > max{
a(m−2)m+∆1
exp(−1
β)−exp(−2
β)−2m∆
+b,
(b −a)∆1 −|V|−2
2
abm2 exp(−1
β)+ |V|−2
2
a2(m−2)m2
1−|V|−2
2
am2 exp(−1
β)
}
we have
RDL(f L) ≤ε
where 0 < ε < 1.
Lemma 5. If embeddings follow (5.2) and b = b0 and N (t) = V \{t}, then rank(WE) ≤m+2.
Proof. By (5.2), we have that
〈WE(i),WE(j)〉= −aDH(i, j)+ b
Therefore,
(WE)TWE = −aDH + b11T
Let’s first look at DH which has rank at most m+1. To see this, let’s consider a set of m+1
tokens: {e0, e1,..., em} ⊆V where ek = 2k. Here e0 is associated with the latent vector of all
zeroes and the latent vector associated with ek has only the k-th latent variable being 1.
On the other hand, for any token i, we have that,
i =
X
k:ι−1(i)k=1
ek
In fact,
DH(i) =
X
k:ι−1(i)k=1
³
DH(ek)−DH(e0)
´
+ DH(e0)
where DH(i) is the i-th row of DH, and for each entry j of DH(i), we have that
DH(i, j) =
X
k:ι−1(i)k=1
³
DH(ek, j)−DH(e0, j)
´
+ DH(e0, j)
This is because
DH(ek, j)−DH(e0, j) =
(
+1 if ι−1(j)k = 0
−1 if ι−1(j)k = 1
Thus, we can rewrite DH(i, j) as
DH(i, j) =
X
k:ι−1(i)k=1
³
1[ι−1(i)k = 1,ι−1(j)k = 0]−1[ι−1(i)k = 1,ι−1(j)k = 1)]
´
+ DH(e0, j)
=
m
X
k=1
³
1[ι−1(i)k = 1,ι−1(j)k = 0]−1[ι−1(i)k = 1,ι−1(j)k = 1)]
´
+
m
X
k=1
³
1[ι−1(i)k = 0,ι−1(j)k = 1]+1[ι−1(i)k = 1,ι−1(j)k = 1)]
´
=
m
X
k=1
1[ι−1(i)k = 1,ι−1(j)k = 0]+1[ι−1(i)k = 0,ι−1(j)k = 1]
= DH(i, j)
24

Therefore, every row of DH can be written as a linear combination of {DH(e0),DH(e1),...,DH(em)}.
In other words, DH has rank at most m+1.
Therefore,
rank((WE)TWE) = rank(WE) ≤m+2.
Lemma 9. Let z(0) and z(1) be two binary vectors of size m where m ≥2. Then,
X
z:DH(z(0),z)=1
DH(z, z(1)) = (m−2)DH(z(0), z(1))+ m
Proof. For z such that DH(z, z(0)) = 1, we know that there are two cases. Either z differs
with z(0) on a entry but agrees with z(1) on that entry or z differs with both z(0) and z(1).
For the first case, we know that there are DH(z(0), z(1)) such entries. In this case, DH(z, z(1)) =
DH(z(0), z(1))−1. For the second case, DH(z, z(1)) = DH(z(0), z(1))+1.
Therefore,
X
z:DH(z,z(0))=1
DH(z, z(1))
= DH(z(0), z(1))(DH(z(0), z(1))−1)+(m−DH(z(0), z(1)))(DH(z(0), z(1))+1)
= (m−2)DH(z(0), z(1))+ m
Lemma 10. If m ≥3 and N (t) = V \{t}, then N1(t) ̸⊆N1(t′) for any t,t′ ∈[V].
Proof. For any token t, N1(t) contains any token t′ such that DH(t,t′) = 1 by the conditions.
Then given a set N1(t), one can uniquely determine token t. This is because for the set
of latent vectors associated with N1(t), at each index, there could only be one possible
change.
A.4
Proofs for Section 5.4
Lemma 6. Suppose the data generating process follows Section 4.1 and N (z∗) = {z : z∗
1 =
z1}\{z∗}. Given the last token in the sequence tL, then
∇ut,tL ℓ(f L) = ∇ℓ(f L)T(WE)TWV (αt ˆptWE(t)−ˆpt
L
X
l=1
ˆptlWE(tl))
where for token t, αt = PL
l=1 1[tl = t] and ˆpt is the normalized attention score for token t.
Proof. Recall that,
f L(x) =
h
WETWV attn(WEχ(x))
i
:L
= WETWV
L
X
l=1
exp(utl,tL)
Z
WE(tl)
where Z is a normalizing constant.
Define ˆptl =
exp(utl,tL )
Z
. Then we have
f L(x) = WETWV
L
X
l=1
ˆptlWE(tl)
25

Note that if tl = t then,
∂ˆptl
∂ut,tL
= ˆptl(1−ˆptl)
Otherwise,
∂ˆptl
∂ut,tL
= −ˆptl ˆpt
By the chain rule, we know that
∇ut,tL ℓ(f L) = ∇ℓ(f L)T(WE)TWV (
L
X
l=1
1[tl = t] ˆptlWE(t)−
L
X
l=1
ˆptl ˆptWE(tl))
Therefore,
∇ut,tL ℓ(f L) = ∇ℓ(f L)T(WE)TWV (αt ˆptWE(t)−ˆpt
L
X
l=1
ˆptlWE(tl))
where αt = PL
l=1 1[tl = t].
26

B
Additional experiments – context hijacking
In this section, we show the results of additional context hijacking experiments on the
CounterFact dataset [Men+22].
Reverse context hijacking
In Figure 2a, we saw the effects of hijacking by adding in
“Do not think of {target_false}.” to each context. Now, we measure the effect of the reverse:
What if we prepend “Do not think of {target_true}.” ?
Based on the study in this paper on how associative memory works in LLMs, we should
expect the efficacy score to increase. Indeed, this is what happens, as we see in Fig-
ure B.1.
Figure B.1: Prepending ‘Do not think of {target_true}.’ can increase the chance of LLMs to output
correct tokens. This figure shows efficacy score versus the number of prepends for various LLMs on
the CounterFact dataset with the reverse context hijacking scheme.
Hijacking based on relation IDs
We first give an example of each of the 4 relation IDs
we hijack in Table 1.
Table 1: Examples of contexts in Relation IDs from CounterFact
Relation ID r
Context p
True target o∗
False target o_
P190
Kharkiv is a twin city of
Warsaw
Athens
P103
The native language of Anatole France is
French
English
P641
Hank Aaron professionally plays the sport
baseball
basketball
P131
Kalamazoo County can be found in
Michigan
Indiana
Table 2: Examples of hijack and reverse hijack formats based on Relation IDs
Relation ID r
Context Hijack sentence
Reverse Context Hijack sentence
P190
The twin city of {subject} is not {target_false}
The twin city of {subject} is {target_true}
P103
{subject} cannot speak {target_false}
{subject} can speak {target_true}
P641
{subject} does not play {target_false}
{subject} plays {target_true}
P131
{subject} is not located in {target_false}
{subject} is located in {target_true}
Similar to Figure 2b, we repeat the hijacking experiments where we prepend factual
sentences generated from the relation ID. We use the format illustrated in Table 2 for
27

(a) Relation P103
(b) Relation P132
(c) Relation P190
(d) Relation P641
Figure B.2: Context hijacking based on relation IDs can result in LLMs output incorrect tokens.
This figure shows efficacy score versus the number of prepends for various LLMs on the CounterFact
dataset with hijacking scheme presented in Table 2.
the prepended sentences. We experiment with 3 other relation IDs and we see similar
trends for all the LLMs in Figure B.2a, B.2b, and B.2d. That is, the efficacy score drops for
the first prepend and as we increase the number of prepends, the trend of ES dropping
continues. Therefore, this confirms our intuition that LLMs can be hijacked by contexts
without changing the factual meaning.
Similar to Figure B.1, we experiment with reverse context hijacking where we give the
answers based on relation IDs, as shown in Table 2. We again experiment with the same
4 relation IDs and the results are in Figure B.3a - B.3d. We see that the efficacy score
increases when we prepend the answer sentence, thereby verifying the observations of this
study.
Hijacking without exact target words
So far, the experiments use prompts that either
contain true or false target words. It turns out, the inclusion of exact target words are
not necessary. To see this, we experiment a variant of the generic hijacking and reverse
hijacking experiments. But instead of saying “Do not think of {target_false}” or “Do not
think of {target_true}”. We replace target words with words that are semantically close. In
particular, for relation P1412, we replace words representing language (e.g., “French”) with
their associated country name (e.g., “France”). As shown in Figure B.4, context hijacking
and reverse hijacing still work in this case.
28

(a) Relation P103
(b) Relation P132
(c) Relation P190
(d) Relation P641
Figure B.3: Reverse context hijacking based on relation IDs can result in LLMs to be more likely to
be correct. This figure shows efficacy score versus the number of prepends for various LLMs on the
CounterFact dataset with the reverse hijacking scheme presented in Table 2.
C
Additional experiments and figures – latent concept
association
In this appendix section, we present additional experimental details and results from the
synthetic experiments on latent concept association.
Experimental setup
Synthetic data are generated following the model in Section 4.1.
Unless otherwise stated, the default setup has ω = 0.5, β = 1 and N (i) = V \{i} and L = 256.
The default hidden dimension of the one-layer transformer is also set to be 256. The model
is optimized using AdamW [LH17] where the learning rate is chosen from {0.01,0.001}.
The evaluation dataset is drawn from the same distribution as the training dataset and
consists of 1024 (x, y) pairs. Although theoretical results in Section 5 may freeze certain
parts of the network for simplicity, in this section, unless otherwise specified, all layers
of the transformers are trained jointly. Also, in this section, we typically report accuracy
which is 1−error.
C.1
On the value matrix WV
In this section, we provide additional figures of Section 6.1. Specifically, Figure C.1 shows
that fixing the value matrix to be the identity will negatively impact accuracy. Figure C.2
indicates that replacing trained value matrices with constructed ones can preserve accuracy
to some extent. Figure C.3 suggests that trained value matrices and constructed ones
share similar low-rank approximations. For the last two sets of experiments, we consider
randomly constructed value matrix, where the outer product pairs are chosen randomly,
defined formally as follows:
29

(a) Hijacking P1412
(b) Reverse hijacking P1412
Figure B.4: Hijacking and reverse hijacking experiments on relation P1412 show that context
hijacking does not require exact target word to appear in the context. This figure shows efficacy score
versus the number of prepends for various LLMs on the CounterFact dataset.
(a) L = 64
(b) L = 128
Figure C.1: Fixing the value matrix WV as the identity matrix results in lower accuracy compared to
training WV , especially for smaller context length L. The figure reports accuracy for both fixed and
trained WV settings, with standard errors calculated over 10 runs.
WV =
X
i∈[V]
WE(i)(
X
{j}∼Unif([V])|N1(i)|
WE(j)T)
C.2
On the embeddings
This section provides additional figures from Section 6.2. Figure C.4 shows that in the
underparameterized regime, embedding training is required. Figure C.5 indicates that
the embedding structure in the underparameterized regime roughly follows (5.2). Finally
Figure C.6 shows that, when the value matrix is fixed to the identity, the relationship
between inner product of embeddings and their corresponding Hamming distance is mostly
linear.
C.3
On the attention selection mechanism
This section provides additional figures from Section 6.3. Figure C.7-C.8 show that at-
tention mechanism selects tokens in the same cluster as the last token. In particular, for
Figure C.8, we extend experiments to consider cluster structures that depend on the first
two latent variables. In other words, for any latent vector z∗, we have
N (z∗) = {z : z∗
1 = z1 and z∗
2 = z2}\{z∗}
30

(a) m = 5
(b) m = 6
(c) m = 7
(d) m = 8
Figure C.2: When the value matrix is replaced with the constructed one in trained transformers,
the accuracy does not significantly decrease compared to replacing the value matrix with randomly
constructed ones. The graph reports accuracy under different embedding dimensions and standard
errors are over 5 runs.
31

(a) m = 5
(b) m = 6
(c) m = 7
(d) m = 8
Figure C.3: The constructed value matrix WV has similar low rank approximation with the trained
value matrix. The figure displays average smallest principal angles between low-rank approximations
of trained value matrices and those of constructed, randomly constructed, and Gaussian-initialized
value matrices. Standard errors are over 5 runs.
32

(a) m = 5
(b) m = 6
(c) m = 7
(d) m = 8
Figure C.4: In the underparameterized regime (d < V), freezing embeddings to initializations
causes a significant decrease in performance. The graph reports accuracy with different embedding
dimensions and the standard errors are over 5 runs. Red lines indicate when d = V.
(a) m = 7
(b) m = 8
Figure C.5: The relationship between inner products of embeddings and corresponding Hamming
distances of tokens can be approximated by (5.2). The graph displays the average inner product
between embeddings of two tokens against the corresponding Hamming distance between these
tokens. Standard errors are over 5 runs.
33

(a) m = 5
(b) m = 6
(c) m = 7
(d) m = 8
Figure C.6: The relationship between inner products of embeddings and corresponding Hamming
distances of tokens is mostly linear when the value matrix WV is fixed to be the identity. The graph
displays the average inner product between embeddings of two tokens against the corresponding
Hamming distance between these tokens. Standard errors are over 10 runs.
34

(a) m = 5
(b) m = 6
(c) m = 7
(d) m = 8
Figure C.7: The attention patterns show the underlying cluster structure of the data generating
process. Here, for any latent vector, we have N (z∗) = {z : z∗
1 = z1}\{z∗}. The figure shows attention
score heat maps that are averaged over 10 runs.
(a) m = 5
(b) m = 6
(c) m = 7
(d) m = 8
Figure C.8: The attention patterns show the underlying cluster structure of the data generating
process. Here, for any latent vector, we have N (z∗) = {z : z∗
1 = z1 and z∗
2 = z2}\{z∗}. The figure shows
attention score heat maps that are averaged over 10 runs.
35

(a) Sample 1
(b) Sample 2
(c) Sample 3
(d) Sample 4
Figure C.9: The spectrum of embedding matrix WE has eigengaps between the top and bottom
eigenvalues, indicating low rank structures. The figure shows results from 4 experimental runs.
Number of latent variable m is 7 and the embedding dimension is 32.
C.4
Spectrum of embeddings
We display several plots of embedding spectra (Figure C.9, Figure C.10, Figure C.11,
Figure C.12) that exhibit eigengaps between the top and bottom eigenvalues, suggesting
low-rank structures.
C.5
Context hijacking in latent concept association
In this section, we want to simulate context hijacking in the latent concept association model.
To achieve that, we first sample two output tokens y1 (true target) and y2 (false target)
and then generate contexts x1 = (t1
1,...,t1
L) and x2 = (t2
1,...,t2
L) from p(x1|y1) and p(x2|y2).
Then we mix the two contexts with rate pm. In other words, for the final mixed context
x = (t1,...,tL), tl has probability 1−pm to be t1
l and pm probability to be t2
l . Figure C.13
shows that, as the mixing rate increases from 0.0 to 1.0, the trained transformer tends
to favor predicting false targets. This mirrors the phenomenon of context hijacking in
LLMs.
C.6
On the context lengths
As alluded in Section 5.5, the memory recall rate is closely related to the KL divergences
between context conditional distributions. Because contexts contain mostly i.i.d samples,
longer contexts imply larger divergences. This is empirically verified in Figure C.14 which
demonstrates that longer context lengths can lead to higher accuracy.
36

(a) Sample 1
(b) Sample 2
(c) Sample 3
(d) Sample 4
Figure C.10: The spectrum of embedding matrix WE has eigengaps between the top and bottom
eigenvalues, indicating low rank structures. The figure shows results from 4 experimental runs.
Number of latent variable m is 7 and the embedding dimension is 64.
(a) Sample 1
(b) Sample 2
(c) Sample 3
(d) Sample 4
Figure C.11: The spectrum of embedding matrix WE has eigengaps between the top and bottom
eigenvalues, indicating low rank structures. The figure shows results from 4 experimental runs.
Number of latent variable m is 8 and the embedding dimension is 32.
37

(a) Sample 1
(b) Sample 2
(c) Sample 3
(d) Sample 4
Figure C.12: The spectrum of embedding matrix WE has eigengaps between the top and bottom
eigenvalues, indicating low rank structures. The figure shows results from 4 experimental runs.
Number of latent variable m is 8 and the embedding dimension is 64.
(a) m = 5
(b) m = 6
(c) m = 7
(d) m = 8
Figure C.13: Mixing contexts can cause misclassification. The figure reports accuracy for true target
and false target under various context mixing rate. Standard errors are over 5 runs.
38

(a) m = 5
(b) m = 6
(c) m = 7
(d) m = 8
Figure C.14: Increasing context lengths can improve accuracy. The figure reports accuracy across
various context lengths and dimensions. Standard errors are over 5 runs.
39

