Understanding Complex Systems
Stochastic Equations: 
Theory and Applications in 
Acoustics, Hydrodynamics,
Magnetohydrodynamics,
and Radiophysics, Volume 1
Valery I. Klyatskin
Basic Concepts, Exact Results,
and Asymptotic Approximations

Understanding Complex Systems
Founding Editor
Prof. Dr. J.A. Scott Kelso
Center for Complex Systems & Brain Sciences
Florida Atlantic University
Boca Raton FL, USA
E-mail: kelso@walt.ccs.fau.edu
Editorial and Programme Advisory Board
Dan Braha
New England Complex Systems, Institute and University of Massachusetts, Dartmouth
Péter Érdi
Center for Complex Systems Studies, Kalamazoo College, USA and Hungarian Academy of
Sciences, Budapest, Hungary
Karl Friston
Institute of Cognitive Neuroscience, University College London, London, UK
Hermann Haken
Center of Synergetics, University of Stuttgart, Stuttgart, Germany
Viktor Jirsa
Centre National de la Recherche Scientiﬁque (CNRS), Université de la Méditerranée, Marseille,
France
Janusz Kacprzyk
System Research, Polish Academy of Sciences, Warsaw, Poland
Kunihiko Kaneko
Research Center for Complex Systems Biology, The University of Tokyo, Tokyo, Japan
Scott Kelso
Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton, USA
Markus Kirkilionis
Mathematics Institute and Centre for Complex Systems, University of Warwick, Coventry, UK
Jürgen Kurths
Potsdam Institute for Climate Impact Research (PIK), Potsdam, Germany
Andrzej Nowak
Department of Psychology, Warsaw University, Poland
Linda Reichl
Center for Complex Quantum Systems, University of Texas, Austin, USA
Peter Schuster
Theoretical Chemistry and Structural Biology, University of Vienna, Vienna, Austria
Frank Schweitzer
System Design, ETH Zürich, Zürich, Switzerland
Didier Sornette
Entrepreneurial Risk, ETH Zürich, Zürich, Switzerland
For further volumes:
http://www.springer.com/series/5394

Understanding Complex Systems
Future scientiﬁc and technological developments in many ﬁelds will necessarily depend upon coming
to grips with complex systems. Such systems are complex in both their composition - typically many
different kinds of components interacting simultaneously and nonlinearly with each other and their envi-
ronments on multiple levels - and in the rich diversity of behavior of which they are capable.
The Springer Series in Understanding Complex Systems series (UCS) promotes new strategies and
paradigms for understanding and realizing applications of complex systems research in a wide variety of
ﬁelds and endeavors. UCS is explicitly transdisciplinary. It has three main goals: First, to elaborate the
concepts, methods and tools of complex systems at all levels of description and in all scientiﬁc ﬁelds,
especially newly emerging areas within the life, social, behavioral, economic, neuro and cognitive sci-
ences (and derivatives thereof); second, to encourage novel applications of these ideas in various ﬁelds
of engineering and computation such as robotics, nano-technology and informatics; third, to provide a
single forum within which commonalities and differences in the workings of complex systems may be
discerned, hence leading to deeper insight and understanding.
UCS will publish monographs, lecture notes and selected edited contributions aimed at communicat-
ing new ﬁndings to a large multidisciplinary audience.
Springer Complexity
Springer Complexity is an interdisciplinary program publishing the best research and academic-level
teaching on both fundamental and applied aspects of complex systems - cutting across all traditional dis-
ciplines of the natural and life sciences, engineering, economics, medicine, neuroscience, social and com-
puter science.
Complex Systems are systems that comprise many interacting parts with the ability to generate a new
quality of macroscopic collective behavior the manifestations of which are the spontaneous formation of
distinctive temporal, spatial or functional structures. Models of such systems can be successfully mapped
onto quite diverse “real-life” situations like the climate, the coherent emission of light from lasers, chem-
ical reaction-diffusion systems, biological cellular networks, the dynamics of stock markets and of the
internet, earthquake statistics and prediction, freeway trafﬁc, the human brain, or the formation of opin-
ions in social systems, to name just some of the popular applications.
Although their scope and methodologies overlap somewhat, one can distinguish the following main
concepts and tools: self-organization, nonlinear dynamics, synergetics, turbulence, dynamical systems,
catastrophes, instabilities, stochastic processes, chaos, graphs and networks, cellular automata, adaptive
systems, genetic algorithms and computational intelligence.
The two major book publication platforms of the Springer Complexity program are the monograph
series “Understanding Complex Systems” focusing on the various applications of complexity, and the
“Springer Series in Synergetics”, which is devoted to the quantitative theoretical and methodological foun-
dations. In addition to the books in these two core series, the program also incorporates individual titles
ranging from textbooks to major reference works.

Valery I. Klyatskin
Stochastic Equations:
Theory and Applications in
Acoustics, Hydrodynamics,
Magnetohydrodynamics,
and Radiophysics, Volume 1
Basic Concepts, Exact Results,
and Asymptotic Approximations
Translated from Russian
by A. Vinogradov
ABC

Valery I. Klyatskin
A.M. Obukhov Institute of
Atmospheric Physics
Russian Academy of Sciences
Moscow
Russia
ISSN 1860-0832
ISSN 1860-0840
(electronic)
ISBN 978-3-319-07586-0
ISBN 978-3-319-07587-7
(eBook)
DOI 10.1007/978-3-319-07587-7
Springer Cham Heidelberg New York Dordrecht London
Library of Congress Control Number: 2014941222
c⃝Springer International Publishing Switzerland 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied speciﬁcally for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of
this publication or parts thereof is permitted only under the provisions of the Copyright Law of the
Publisher’s location, in its current version, and permission for use must always be obtained from Springer.
Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations
are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of pub-
lication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any
errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect
to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

To Sonya Klyatskina

Preface
This monograph is revised and more comprehensive translation of my Russian
monograph in 2 volumes [56, 57]. For reasons of usability, the material is
divided into two practically independent volumes.
Volume 1 gives the theory of stochastic equations (including ordinary dif-
ferential equations, partial diﬀerential equations, boundary-value problems,
and integral equations) in terms of the functional analysis. The developed
approach yields exact solutions to stochastic problems for a number of models
of ﬂuctuating parameters among which are telegrapher’s and generalized tele-
grapher’s processes, Markovian processes with a ﬁnite number of states, Gaus-
sian Markovian processes, and functions of the above processes. Asymptotic
methods of analyzing stochastic dynamic systems, such as delta-correlated
random process (ﬁeld) approximation and diﬀusion and higher approxima-
tions are also considered. The material is supplemented with the sections
concerning dynamical and statistical descriptions of simplest hydrodynamic-
type systems. Sections dealing with the stochastic structure formation in
random media such as the clustering of parametrically excited random ﬁelds,
and statistical problems of turbulent dynamo (magnetic ﬁeld generation in
random ﬁelds) are also added in this Volume.
Working at this edition, I tried to take into account remarks and wishes of
readers about both style of the text and choice of speciﬁc problems. Diﬀer-
ent mistakes and misprints are corrected. The book is destined for scientists
dealing with stochastic dynamic systems in diﬀerent areas, such as acoustics,
hydrodynamics, magnetohydrodynamics, radiophysics, theoretical and math-
ematical physics, and applied mathematics; it can be also useful for senior
and postgraduate students. Now, a few words are due on the structure of the
text of this volume. It consists of four parts.
The ﬁrst part may be viewed as an introductory text. It takes up a few
typical physical problems to discuss their solutions obtained under random
disturbances of parameters aﬀecting the system behavior. More detailed for-
mulations of these problems and relevant statistical analysis can be found in
other parts of this Volume and in Volume 2.

VIII
Preface
The second part deals with the statistical description of random quantities,
processes, and ﬁelds and includes, in particular, some problems of statistical
topography of random ﬁelds and stochastic structure formation in random
ﬁelds.
The third part is devoted to the general theory of statistical analysis of
dynamic systems with ﬂuctuating parameters described by diﬀerential and
integral equations. This theory is illustrated by analyzing speciﬁc dynamic
systems.
The fourth part treats asymptotic methods of statistical analysis such
as the delta-correlated random process (ﬁeld) approximation, diﬀusion and
higher approximations.
It is worth noting that purely mathematical and physical papers devoted
to considered issues run into thousands. It would be physically impossible to
give an exhaustive bibliography. Therefore, in this book I conﬁne myself to
referencing those papers which are used or discussed in this book and also
recent review papers and papers with extensive bibliography on the subject.
Moscow
Valery I. Klyatskin

Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
VII
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . XVII
Part I:
Dynamical Description of Stochastic Systems
1
Examples, Basic Problems, Peculiar Features
of Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Ordinary Diﬀerential Equations: Initial-Value Problems . . . .
3
1.1.1
Particles under Random Velocity Field . . . . . . . . . . . . .
3
1.1.2
Particles under Random Forces. . . . . . . . . . . . . . . . . . . .
9
1.1.3
The Hopping Phenomenon . . . . . . . . . . . . . . . . . . . . . . .
10
1.1.4
Systems with Blow-Up Singularities. . . . . . . . . . . . . . . .
20
1.1.5
Oscillator with Randomly Varying Frequency
(Stochastic Parametric Resonance) . . . . . . . . . . . . . . . .
21
1.2
Linear Ordinary Diﬀerential Equations: Boundary-Value
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.2.1
Plane Waves in Layered Media: A Wave Incident
on a Medium Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.2.2
Plane Waves in Layered Media: Source Inside the
Medium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.2.3
Plane Waves in Layered Media: Two-Layer Model . . .
32
1.3
First-Order Partial Diﬀerential Equations. . . . . . . . . . . . . . . . .
33
1.3.1
Linear First-Order Partial Diﬀerential Equations . . . .
34
1.3.2
Quasilinear Equations. . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
1.3.3
Boundary-Value Problems for Nonlinear Ordinary
Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
1.3.4
Nonlinear First-Order Partial Diﬀerential
Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
1.4
Partial Diﬀerential Equations of Higher Orders . . . . . . . . . . . .
52

X
Contents
1.4.1
Fundamental Solutions of Wave Problems in Free
Space and Layered Media . . . . . . . . . . . . . . . . . . . . . . . .
52
1.4.2
Stationary Problems for Maxwell’s Equations
. . . . . .
59
1.4.3
The Helmholtz Equation (Boundary-Value
Problem) and the Parabolic Equation of
Quasi-Optics (Waves in Randomly Inhomogeneous
Media) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
1.4.4
The Navier–Stokes Equation: Random Forces in
Hydrodynamic Theory of Turbulence . . . . . . . . . . . . . .
67
1.4.5
Equations of Geophysical Hydrodynamics . . . . . . . . . .
79
2
Solution Dependence on Problem Type, Medium
Parameters, and Initial Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
2.1
Functional Representation of Problem Solution . . . . . . . . . . . .
81
2.1.1
Variational (Functional) Derivatives . . . . . . . . . . . . . . .
81
2.1.2
Principle of Dynamic Causality . . . . . . . . . . . . . . . . . . .
87
2.2
Solution Dependence on Problem’s Parameters . . . . . . . . . . . .
88
2.2.1
Solution Dependence on Initial Data . . . . . . . . . . . . . . .
88
2.2.2
Imbedding Method for Boundary-Value Problems . . .
90
3
Indicator Function and Liouville Equation . . . . . . . . . . . . . . .
95
3.1
Ordinary Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . .
95
3.2
First-Order Partial Diﬀerential Equations. . . . . . . . . . . . . . . . .
98
3.2.1
Linear Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.2.2
Quasilinear Equations. . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
3.2.3
General-Form Nonlinear Equations . . . . . . . . . . . . . . . .
109
3.3
Higher-Order Partial Diﬀerential Equations . . . . . . . . . . . . . . .
109
3.3.1
Parabolic Equation of Quasi-Optics . . . . . . . . . . . . . . . .
110
3.3.2
Random Forces in Hydrodynamic Theory of
Turbulence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
Part II:
Random Quantities, Processes, and Fields
4
Random Quantities and Their Characteristics . . . . . . . . . . .
117
5
Random Processes and Their Characteristics . . . . . . . . . . . .
125
5.1
General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
5.1.1
Random Process Typical Realization Curve . . . . . . . .
127
5.1.2
Statistics of Random Process Cross Points
with a Line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
5.2
Gaussian Random Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
5.3
Logarithmically Normal Random Process . . . . . . . . . . . . . . . . .
132
5.4
Discontinuous Random Processes . . . . . . . . . . . . . . . . . . . . . . . .
133
5.4.1
Poisson (Impulse) Random Process . . . . . . . . . . . . . . . .
134
5.4.2
Telegrapher’s Random Process . . . . . . . . . . . . . . . . . . . .
137
5.4.3
Generalized Telegrapher’S Random Process . . . . . . . . .
141

Contents
XI
5.5
Markovian Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
5.5.1
General Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
5.5.2
Characteristic Functional of the Markovian
Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
6
Random Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
6.1
Gaussian Vector Random Field . . . . . . . . . . . . . . . . . . . . . . . . . .
156
6.2
Statistical Topography of Random Processes and Fields . . . .
160
6.3
On the Criterion of Stochastic Structure Formation in
Random Media . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
7
Correlation Splitting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
7.1
General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
7.2
Gaussian Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
7.3
Gaussian Random Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
7.4
Poisson Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
7.5
Telegrapher’s Random Process . . . . . . . . . . . . . . . . . . . . . . . . . .
173
7.6
Generalized Telegrapher’s Random Process . . . . . . . . . . . . . . .
177
7.7
General-Form Markovian Processes . . . . . . . . . . . . . . . . . . . . . .
178
7.8
Delta-Correlated Random Processes . . . . . . . . . . . . . . . . . . . . .
181
7.8.1
General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
7.8.2
Asymptotic Meaning of Delta-Correlated Processes
and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184
Part III:
Stochastic Equations
8
General Approaches to Analyzing Stochastic Dynamic
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
8.1
Ordinary Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . .
193
8.2
Partial Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
8.2.1
Passive Tracer Transfer in Random Field
of Velocities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
8.2.2
Parabolic Equation of Quasi-Optics . . . . . . . . . . . . . . . .
200
8.2.3
Random Forces in the Theory of Hydrodynamic
Turbulence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
8.3
Stochastic Integral Equations (Methods of Quantum Field
Theory) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
8.3.1
Linear Integral Equations . . . . . . . . . . . . . . . . . . . . . . . .
203
8.3.2
Nonlinear Integral Equations. . . . . . . . . . . . . . . . . . . . . .
211
8.4
Completely Solvable Stochastic Dynamic Systems . . . . . . . . .
218
8.4.1
Ordinary Diﬀerential Equations . . . . . . . . . . . . . . . . . . .
218
8.4.2
Partial Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . .
232
8.5
Delta-Correlated Fields and Processes . . . . . . . . . . . . . . . . . . . .
237
8.5.1
General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
8.5.2
One-Dimensional Nonlinear Diﬀerential Equation . . . .
240

XII
Contents
8.5.3
Linear Operator Equation . . . . . . . . . . . . . . . . . . . . . . . .
243
8.5.4
Partial Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . .
252
9
Stochastic Equations with the Markovian Fluctuations
of Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
9.1
General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
9.2
Telegrapher’s Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
9.2.1
System of Linear Operator Equations . . . . . . . . . . . . . .
264
9.2.2
One-Dimensional Nonlinear Diﬀerential Equation . . . .
269
9.2.3
Particle in the One-Dimensional Potential Field . . . . .
271
9.2.4
Ordinary Diﬀerential Equation of the n-th Order . . . .
272
9.2.5
Statistical Interpretation of Telegrapher’s
Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
9.3
Generalized Telegrapher’s Process . . . . . . . . . . . . . . . . . . . . . . .
275
9.3.1
Stochastic Linear Equation . . . . . . . . . . . . . . . . . . . . . . .
276
9.3.2
One-Dimensional Nonlinear Diﬀerential Equation . . . .
280
9.3.3
Ordinal Diﬀerential Equation of the n-th Order . . . . .
281
9.4
Gaussian Markovian Processes . . . . . . . . . . . . . . . . . . . . . . . . . .
283
9.4.1
General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283
9.4.2
Stochastic Linear Equation . . . . . . . . . . . . . . . . . . . . . . .
284
9.4.3
Ordinal Diﬀerential Equation of the n-th Order . . . . .
285
9.4.4
The Square of the Gaussian Markovian Process . . . . .
288
9.5
Markovian Processes with Finite-Dimensional Phase
Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
9.5.1
Two-State Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290
9.6
Causal Stochastic Integral Equations . . . . . . . . . . . . . . . . . . . . .
291
9.6.1
General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
291
9.6.2
Telegrapher’s Random Process . . . . . . . . . . . . . . . . . . . .
293
9.6.3
Generalized Telegrapher’s Random Process . . . . . . . . .
295
9.6.4
Gaussian Markovian Process . . . . . . . . . . . . . . . . . . . . . .
297
Part IV:
Asymptotic and Approximate Methods for Analyzing
Stochastic Equations
10
Gaussian Random Field Delta-Correlated in Time
(Ordinary Diﬀerential Equations) . . . . . . . . . . . . . . . . . . . . . . . .
305
10.1 The Fokker–Planck Equation . . . . . . . . . . . . . . . . . . . . . . . . . . .
305
10.2 Transitional Probability Distributions . . . . . . . . . . . . . . . . . . . .
308
10.3 Applicability Range of the Fokker–Planck Equation . . . . . . . .
310
10.3.1 Langevin Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
311
10.4 The Simplest Markovian Random Processes. . . . . . . . . . . . . . .
315
10.4.1 System of Linear Equations . . . . . . . . . . . . . . . . . . . . . . .
315
10.4.2 Wiener Random Process . . . . . . . . . . . . . . . . . . . . . . . . .
316
10.4.3 Wiener Random Process with Shear . . . . . . . . . . . . . . .
318

Contents
XIII
10.4.4 Lognormal Random Process, Intermittency and
Dynamic Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
320
10.5 Logarithmic-Normal Random Fields, Intermittency and
Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
10.5.1 Logarithmic-Normal Random Fields . . . . . . . . . . . . . . .
329
10.5.2 Statistical Topography of the Lognormal Random
Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
10.6 Causal Integral Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
334
10.6.1 General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335
10.6.2 Statistical Averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
11
Methods for Solving and Analyzing the Fokker-Planck
Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
11.1 Integral Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
11.2 Steady-State Solutions of the Fokker–Planck Equation . . . . .
344
11.2.1 One-Dimensional Nonlinear Diﬀerential Equation . . . .
344
11.2.2 Hamiltonian Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
11.2.3 Systems of Hydrodynamic Type . . . . . . . . . . . . . . . . . .
347
11.3 Boundary-Value Problems for the Fokker-Planck Equation
(Hopping Phenomenon) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
355
11.3.1 Hopping Phenomenon in Regular Systems
. . . . . . . . .
355
11.3.2 Hopping Phenomena in Singular Systems . . . . . . . . . .
359
11.4 Asymptotic and Approximate Methods of Solving the
Fokker-Plank Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
363
11.4.1 Asymptotic Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . .
363
11.4.2 Method of Cumulant Expansions . . . . . . . . . . . . . . . . . .
364
11.4.3 Method of Fast Oscillation Averaging . . . . . . . . . . . . . .
365
12
Diﬀusion and Higher Approximations. . . . . . . . . . . . . . . . . . . .
377
12.1 General Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
12.2 Diﬀusion Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
12.2.1 Dynamics of a Particle . . . . . . . . . . . . . . . . . . . . . . . . . . .
379
12.3 Higher Approximations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
383
13
Some Other Approximate Approaches to the Problems
of Statistical Hydrodynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
391
13.1 Quasi-Elastic Properties of Isotropic and Stationary
Noncompressible Turbulent Media . . . . . . . . . . . . . . . . . . . . . . .
391
13.2 Sound Radiation by Vortex Motions . . . . . . . . . . . . . . . . . . . . .
396
13.2.1 Sound Radiation by Vortex Lines . . . . . . . . . . . . . . . . . .
398
13.2.2 Sound Radiation by Vortex Rings . . . . . . . . . . . . . . . . .
401
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
415

Mephistopheles (about algebra)
· · ·
I want to warn you:
careful you should
Be with this science
which is very tricky.
It can involve your brain into the chaos
of vain unnecessary transformations.
If you don’t manage
to embrace its basis,
You won’t be able indices to diﬀer.
· · ·
What’s important −
deepen into symbols
And, having mastered them,
you bravely step
Onto the path, which leads into the Kingdom
of Formulas, eternal and exact.
Kurd Lasswitz, Prost!,
www. gutenberg2000.de/lasswitz/prost/prost.htm
Translated by Alla Parollo.

Introduction
Diﬀerent areas of physics pose statistical problems in ever-greater numbers.
Apart from issues traditionally obtained in statistical physics, many applica-
tions call for including ﬂuctuation eﬀects into consideration. While ﬂuctua-
tions may stem from diﬀerent sources (such as thermal noise, instability, and
turbulence), methods used to treat them are very similar. In many cases, the
statistical nature of ﬂuctuations may be deemed known (either from physical
considerations or from problem formulation) and the physical processes may
be modeled by diﬀerential, integro-diﬀerential or integral equations.
Today the most powerful tools used to tackle complicated statistical prob-
lems are the Markov theory of random processes and the theory of diﬀusion-
type processes evolved from Brownian motion theory. Mathematical aspects
underlying these theories and their applications have been treated extensively
in academic literature and textbooks (see, for example, [17]), and therefore
we will not dwell on these issues in this treatise.
We will consider a statistical theory of dynamic and wave systems with
ﬂuctuating parameters. These systems can be described by ordinary diﬀe-
rential equations, partial diﬀerential equations, integro-diﬀerential equations
and integral equations. A popular way to solve such systems is by obtaining
a closed system of equations for statistical characteristics of such systems to
study their solutions as comprehensively as possible.
We note that often wave problems are boundary-value problems. When
this is the case, one may resort to the imbedding method to reformulate the
equations at hand into initial-value problems, thus considerably simplifying
the statistical analysis [50].
We shall dwell in depth on dynamic systems whose ﬂuctuating parameters
are Gaussian random processes (ﬁelds), although what we present in this
book is a general theory valid for ﬂuctuating parameters of any nature.
The purpose of this book is to demonstrate how diﬀerent physical problems
described by stochastic equations may be solved on the base of a general
approach. This treatment reveals interesting similarities between diﬀerent
physical problems.

XVIII
Introduction
Examples of speciﬁc physical systems outlined below are mainly borrowed
from statistical hydrodynamics, statistical radiophysics and acoustics because
of author’s research in these ﬁelds. However, similar problems and solution
techniques occur in such areas as plasma physics, solid-state physics, magne-
tohydrodynamics to name a few.
In stochastic problems with ﬂuctuating parameters, the variables are func-
tions. It would be natural therefore to resort to functional methods for their
analysis. We will use a functional method devised by Novikov [114] for Gaus-
sian ﬂuctuations of parameters in a turbulence theory and developed by the
author of this book [46, 48–50, 55, 61, 62] for the general case of dynamic
systems and ﬂuctuating parameters of arbitrary nature.
However, only a few dynamic systems lend themselves to analysis yielding
solutions in a general form. It proved to be more eﬃcient to use an asymptotic
method where the statistical characteristics of dynamic problem solutions
are expanded in powers of a small parameter which is essentially a ratio of
the random impact’s correlation time to the time of observation or to other
characteristic time scale of the problem (in some cases, these may be spatial
rather than temporal scales). This method is essentially a generalization of
the theory of Brownian motion. It is termed the delta-correlated random
process (ﬁeld) approximation. In Brownian motion theory, this approximation
is consistent with a model obtained by neglecting the time between random
collisions as compared to all other time scales.
For dynamic systems described by ordinary diﬀerential stochastic equations
with Gaussian ﬂuctuations of parameters, this method leads to a Markovian
problem solving model, and the respective equation for transition probability
density has the form of the Fokker–Planck equation. In this book, we will con-
sider in depth the methods of analysis available for this equation and its boun-
dary conditions. We will analyze solutions and validity conditions by way of
integral transformations. In more complicated problems described by partial
diﬀerential equations, this method leads to a generalized equation of Fokker–
Planck type in which variables are the derivatives of the solution’s charac-
teristic functional. For dynamic problems with non-Gaussian ﬂuctuations of
parameters, this method also yields Markovian type solutions. Under the cir-
cumstances, the probability density of respective dynamic stochastic equations
satisﬁes a closed operator equation. For example, systems with parameters
ﬂuctuating in a Poisson proﬁle are converted into the Kolmogorov–Feller type
of integro-diﬀerential equations.
In physical investigations, Fokker–Planck and similar equations are usu-
ally set up from rule of thumb considerations, and dynamic equations are
invoked only to calculate the coeﬃcients of these equations. This approach
is inconsistent, generally speaking. Indeed, the statistical problem is com-
pletely deﬁned by dynamic equations and assumptions on the statistics of
random impacts. For example, the Fokker–Planck equation must be a logical
sequence of the dynamic equations and some assumptions on the character of
random impacts. It is clear that not all problems lend themselves for reducing

Introduction
XIX
to a Fokker–Planck equation. The functional approach allows one to derive
a Fokker–Planck equation from the problem’s dynamic equation along with
its applicability conditions. In terms of formal mathematics, our approach
corresponds to that of R.L. Stratonovich (see, e.g., [126]).
For a certain class of random processes (Markovian telegrapher’s processes,
Gaussian Markovian process and the like), the developed functional approach
also yields closed equations for the solution probability density with allowance
for a ﬁnite correlation time of random interactions.
For processes with Gaussian ﬂuctuations of parameters, one may construct
a better physical approximation than the delta-correlated random process
(ﬁeld) approximation,—the diﬀusion approximation that allows for ﬁniteness
of correlation time radius. In this approximation, the solution is Markovian
and its applicability condition has transparent physical meaning, namely, the
statistical eﬀects should be small within the correlation time of ﬂuctuating
parameters. This book treats these issues in depth from a general standpoint
and for some speciﬁc physical applications.

Part I
Dynamical Description of Stochastic
Systems

Chapter 1
Examples, Basic Problems, Peculiar
Features of Solutions
In this chapter, we consider several dynamic systems described by diﬀerential
equations of diﬀerent types and discuss the features in the behaviors of solu-
tions to these equations under random disturbances of parameters. Here, we
content ourselves with problems in the simplest formulation. More complete
formulations will be discussed below, in the sections dealing with statistical
analysis of the corresponding systems.
1.1
Ordinary Diﬀerential Equations: Initial-Value
Problems
1.1.1
Particles under Random Velocity Field
In the simplest case, a particle under random velocity ﬁeld is described by
the system of ordinary diﬀerential equations of the ﬁrst order
d
dtr(t) = U(r, t),
r(t0) = r0,
(1.1)
where U(r, t) = u0(r, t) + u(r, t), u0(r, t) is the deterministic component of
the velocity ﬁeld (mean ﬂow), and u(r, t) is the random component. In the
general case, ﬁeld u(r, t) can have both nondivergent (solenoidal, for which
div u(r, t) = 0) and divergent (for which div u(r, t) ̸= 0) components.
We dwell on stochastic features of the solution to problem (1.1) for a
system of particles in the absence of mean ﬂow (u0(r, t) = 0). From Eq. (1.1)
formally follows that every particle moves independently of other particles.
However, if random ﬁeld u(r, t) has a ﬁnite spatial correlation radius lcor,
particles spaced by a distance shorter than lcor appear in the common zone
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
3
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_1, c
⃝Springer International Publishing Switzerland 2015

4
1
Examples, Basic Problems, Peculiar Features of Solutions
of infection of random ﬁeld u(r, t) and the behavior of such a system can
show new collective features.
For steady velocity ﬁeld u(r, t) ≡u(r), Eq. (1.1) reduces to
d
dtr(t) = u(r),
r(0) = r0.
(1.2)
This equation clearly shows that steady points r (at which u(r) = 0) remain
the ﬁxed points. Depending on whether these points are stable or unstable,
they will attract or repel nearby particles. In view of randomness of function
u(r), points r are random too.
It is expected that the similar behavior will be also characteristic of the
general case of the space-time random velocity ﬁeld of u(r,t).
If some points r remain stable during suﬃciently long time, then clusters
of particles (i.e., compact regions with elevated particle concentration, which
occur merely in rareﬁed zones) must arise around these points in separate
realizations of random ﬁeld u(r, t). On the contrary, if the stability of these
points alternates with instability suﬃciently rapidly and particles have no
time for signiﬁcant rearrangement, no clusters of particles will occur.
Simulations (see [76,123,142]) show that the behavior of a system of par-
ticles essentially depends on whether the random ﬁeld of velocities is nondi-
vergent or divergent. By way of example, Fig. 1.1a shows a schematic of the
evolution of the two-dimensional system of particles uniformly distributed
within the circle for a particular realization of the nondivergent steady ﬁeld
u(r).
Here, we use the dimensionless time related to statistical parameters of ﬁeld
u(r). In this case, the area of surface patch within the contour remains intact
and particles relatively uniformly ﬁll the region within the deformed contour.
The only feature consists in the fractal-type irregularity of the deformed
contour. This phenomenon—called chaotic advection —is under active study
now (see, e.g., [6]).
On the contrary, in the case of the divergent velocity ﬁeld u(r), particles
uniformly distributed in the square at the initial instant will form clusters dur-
ing temporal evolution. Results simulated for this case are shown in Fig. 1.1b.
We emphasize that the formation of clusters is purely the kinematic eﬀect.
This feature of particle dynamics disappears on averaging over an ensemble
of realizations of the random velocity ﬁeld.
To demonstrate the process of particle clustering, we consider the simplest
problem [70], in which random velocity ﬁeld u(r, t) has the form
u(r, t) = v(t)f(kr),
(1.3)
where v(t) is the random vector process and
f(kr) = sin(2kr)
(1.4)

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
5
a
−10
−5
−5
−5
−5
−5
−5
−5
−5
−10
−10
−10
−10
−10
−10
5
5
5
5
5
5
5
5
10
10
10
10
10
10
10
10
t = 0
t = 2
t = 3
t = 4
b
t = 0
t = 0.5
t = 1
t = 2.0
Fig. 1.1 Diﬀusion of a system of particles described by Eqs. (1.2) numerically
simulated for (a) nondivergent and (b) potential random steady velocity ﬁeld u(r)

6
1
Examples, Basic Problems, Peculiar Features of Solutions
is the deterministic function of one variable. Note that this form of function
f(r) corresponds to the ﬁrst term of the expansion in harmonic components
and is commonly used in numerical simulations [76,142].
In this case, Eq. (1.1) can be written in the form
d
dtr(t) = v(t) sin(2kr),
r(0) = r0.
In the context of this model, motions of a particle along vector k and in the
plane perpendicular to vector k are independent and can be separated. If we
direct the x-axis along vector k, then the equations assume the form
d
dtx(t) = vx(t) sin(2kx),
x(0) = x0,
d
dtR(t) = vR(t) sin(2kx),
R(0) = R0.
(1.5)
The solution of the ﬁrst equation in (1.5) is
x(t) = 1
k arctan

eT (t)tan(kx0)

,
(1.6)
where
T (t) = 2k
t

0
dτvx(τ).
(1.7)
Taking into account the equality following from (1.6)
sin(2kx) =
sin(2kx0)
e−T (t) cos2(kx0) + eT (t) sin2(kx0),
cos(2kx) = 1 −e2T (t) tan2(kx0)
1 + e2T (t) tan2(kx0),
(1.8)
we can rewrite the second equation in Eqs. (1.5) in the form
d
dtR(t|r0) = sin(2kx0)
vR(t)
e−T (t) cos2(kx0) + eT (t) sin2(kx0).
As a result, we have
R(t|r0) = R0 + sin(2kx0)
t

0
dτ
vR(τ)
e−T (τ) cos2(kx0) + eT (τ) sin2(kx0).
(1.9)
Consequently, if the initial particle position x0 is such that

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
7
kx0 = nπ
2 ,
(1.10)
where n = 0, ±1, · · · , then the particle will be the ﬁxed particle and r(t) ≡
r0.
Equalities (1.10) deﬁne planes in the general case and points in the one-
dimensional case. They correspond to zeros of the ﬁeld of velocities. Stability
of these points depends on the sign of function v(t), and this sign changes
during the evolution process. It can be expected that particles will be concen-
trated around these points if vx(t) ̸= 0, which just corresponds to clustering
of particles.
In the case of a nondivergent velocity ﬁeld, vx(t) = 0 and, consequently,
T (t) ≡0; as a result, we have
x(t|x0) ≡x0,
R(t|r0) = R0 + sin(2kx0)
t

0
dτvR(τ),
which means that no clustering occurs.
In numerical simulations, we will describe velocity v(t) by the model of
the Gaussian random delta-correlated vector ﬁeld with the parameters
⟨v(t)⟩= 0
⟨vi(t)vj(t′)⟩= 2σ2δijτ0δ(t −t′),
(1.11)
where σ2 is the variance of every component of the velocity and τ0 is its tem-
poral radius of correlation. Below, we will use the nondimensional variables
t →k2σ2τ0t,
x →kx,
⟨vi(t)vj(t′)⟩→2δijδ(t −t′).
(1.12)
Figure 1.2a shows a fragment of the realization of random process T (t)
obtained by numerical integration of Eq. (1.7) for a realization of random
process vx(t); we used this fragment for simulating the temporal evolution of
coordinates of four particles x(t), x ∈(0, π/2) initially located at coordinates
x0(i) = π
2
i
5 (i = 1, 2, 3, 4) (see Fig. 1.2b). Figure 1.2b shows that particles
form a cluster in the vicinity of point x = 0 at the dimensionless time t ≈4.
Further, at time t ≈16 the initial cluster disappears and new one appears in
the vicinity of point x = π/2. At moment t ≈40, the cluster appears again
in the vicinity of point x = 0, and so on. In this process, particles in clusters
remember their past history and signiﬁcantly diverge during intermediate
temporal segments (see Fig. 1.2c).
Thus, we see in this example that the cluster does not move from one
region to another; instead, it ﬁrst collapses and then a new cluster appears.
Moreover, the lifetime of clusters signiﬁcantly exceeds the duration of inter-
mediate segments. It seems that this feature is characteristic of the particular
model of the velocity ﬁeld and follows from steadiness of points (1.10).

8
1
Examples, Basic Problems, Peculiar Features of Solutions
−10
−20
10
10
10
0
0
0
20
20
30
30
40
40
13
14
15
16
17
0.5
0.5
1.0
1.0
1.5
1.5
x
x
T (t)
a
b
c
t
t
t
Fig. 1.2 (a) Segment of a realization of random process T (t) obtained by nu-
merically integrating Eq. (1.7) for a realization of random process vx(t); (b), (c)
x-coordinates simulated with this segment for four particles versus time.
As regards the particle diﬀusion along the y-direction, no cluster occurs
here.
Note that such clustering in a system of particles was found, to all appea-
rance for the ﬁrst time, in papers [104–107] as a result of simulating the
so-called Eole experiment with the use of the simplest equations of atmos-
pheric dynamics.
In this global experiment, 500 constant-density balloons were launched in
Argentina in 1970-1971; these balloons traveled at a height of about 12 km
and spread along the whole of the southern hemisphere.
Figure 1.3 shows the balloon distribution over the southern hemisphere for
day 105 from the beginning of this process simulation [106]; this distribution
clearly shows that balloons are concentrated in groups, which just corre-
sponds to clustering. Results of statistical processing of balloon arrangement
can be found, for example, in papers [16,110].

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
9
Fig. 1.3 Balloon distribution in the atmosphere for day 105 from the beginning of
process simulation.
1.1.2
Particles under Random Forces
The system of equations (1.1) describes also the behavior of a particle under
the ﬁeld of random external forces f(r, t). In the simplest case, the behavior
of a particle in the presence of linear friction is described by the diﬀerential
equation of the second order (Newton equation)
d2
dt2 r(t) = −λ d
dtr(t) + f(r, t),
r(0) = r0,
d
dtr(0) = v0,
(1.13)
or the system of diﬀerential equations of the ﬁrst order
d
dtr(t) = v(t),
d
dtv(t) = −λv(t) + f(r, t),
r(0) = r0,
v(0) = v0.
(1.14)
Results of numerical simulations of stochastic system (1.14) can be found in
Refs. [102,136].
The behavior of a particle under the deterministic potential ﬁeld in the
presence of linear friction and random forces is described by the system of
equations

10
1
Examples, Basic Problems, Peculiar Features of Solutions
d
dtr(t) = v(t),
d
dtv(t) = −λv(t) −∂U(r, t)
∂r
+ f(r, t),
r(0) = r0,
v(0) = v0,
(1.15)
which is the simplest example of Hamiltonian systems with linear friction.If
friction and external forces are absent and function U is independent of time,
U(r, t) = U(r), the system has an integral of motion
d
dtE(t) = const,
E(t) = v2
2 + U(r)
expressing energy conservation.
In statistical problems, equations of type (1.14), (1.15) are widely used to
describe the Brownian motion of particles.
In the general case, Hamiltonian systems are described by the system of
equations
d
dtr(t) = ∂H(r, v, t)
∂v
,
d
dtv(t) = −∂H(r, v, t)
∂r
,
r(0) = r0,
v(0) = v0,
(1.16)
where H(r, v, t) = H(r(t), v(t), t) is the Hamiltonian function. In the case
of conservative Hamiltonian systems, function H(r, v, t) has no explicit de-
pendence on time, H(r, v, t) = H(r, v), and the system has the integral of
motion
H(r, v) = const.
1.1.3
The Hopping Phenomenon
Now, we dwell on another stochastic aspect related to dynamic equations of
type (1.1); namely, we consider the hopping phenomenoncaused by random
ﬂuctuations.
Consider the one-dimensional nonlinear equation
d
dtx(t) = x

1 −x2
+ f(t),
x(0) = x0,
(1.17)
where f(t) is the random function of time. In the absence of randomness
(f(t) ≡0), the solution of Eq. (1.17) has two stable steady states x = ±1 and
one instable state x = 0. Depending on the initial value, solution of Eq. (1.17)
arrives at one of the stable states. However, in the presence of small random
disturbances f(t), dynamic system (1.17) will ﬁrst approach the vicinity of

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
11
one of the stable states and then, after the lapse of certain time, it will be
transferred into the vicinity of another stable state.
Note that Eq. (1.17) corresponds to limit process λ →∞in the equation
d2
dt2 x(t) + λ d
dtx(t) −λ
dU(x)
dx
+ f(t)
	
= 0,
that is known as the Duﬃng equation and is the special case of the one-
dimensional Hamiltonian system (1.15)
d
dtx(t) = v(t),
d
dtv(t) = −λ

v(t) −dU(x)
dx
−f(t)
	
(1.18)
with the potential function
U(x) = x2
2 −x4
4 .
In other words, Eq. (1.17) corresponds to great friction coeﬃcients λ.
Statistical description of this problem will be considered in Sect.11.3.1,
page 357. Additionally, we note that, in the context of statistical description,
reduction of the Hamiltonian system (1.18) to the ’short-cut equation’ is
called the Kramers problem (see Sect. 11.2.2, page 347).
It is clear that the similar behavior can occur in more complicated
situations.
Hydrodynamic-Type Nonlinear Systems
An important problem of studying large-scale processes in the atmosphere
considered as a single physical system consists in revealing the mechanism of
energy exchange between diﬀerent ’degrees of freedom’.
The analysis of such nonlinear processes on the base of simple models des-
cribed by a small number of parameters (degrees of freedom) is recently of
great attention [97,98,101]. In this connection, A.M. Obukhov [115,116,119]
(see also [14]) introduced the concept of hydrodynamic-type systems (HTS).
These systems have a ﬁnite number of parameters v1, · · · , vn, but the general
features of the dynamic equations governing system motions coincide with
those characteristic of the hydrodynamic equations of perfect incompressible
ﬂuid, including quadratic nonlinearity, energy conservation, and regularity
(phase volume invariance during system motions). The general description of
HTS is given in Sect. 11.3.1, page 357. Here, we dwell only on the dynamic
description of simplest HTS systems.
The simplest system of such type (S3) is equivalent to the Euler equations
in the dynamics of solids; it describes the known problem on ﬂuid motions
in an ellipsoidal cavity [13, 14, 23]. Any ﬁnite-dimensional approximation of

12
1
Examples, Basic Problems, Peculiar Features of Solutions
hydrodynamic equations also belongs to the class of HTS if it possesses the
above features.
To model the cascade mechanism of energy transformation in a turbulent
ﬂow, Obukhov [117] suggested a multistage HTS. Each stage of this system
consists of identical-scale triplets; at every next stage, the number of triplets
is doubled and the scale is decreased in geometrical progression with ratio
Q ≳1. As a result, this model describes interactions between the motions of
diﬀerent scales.
The ﬁrst stage consists of a singe triplet whose unstable mode v01 is excited
by an external force f0(t) applied to the system (Fig. 1.4a). The stable modes
of this triplet v1,1 and v1,2 are the unstable modes of two triplets of the second
stage; their stable modes v2,1, v2,2, v2,3, and v2,4 are, in turn, the unstable
modes of four triplets of the third stage; and so on (Fig. 1.4b).
a
f0(t)
f0(t)
v1,0(t)
v1,0(t)
v1,1(t)
v1,1(t)
v1,2(t)
v1,2(t)
b
v2,1(t)
v2,2(t)
v2,3(t)
v2,4(t)
Fig. 1.4 Diagrams of (a) three- and (b) seven-mode HTS
It should be noted however that physical processes described in terms of
macroscopic equations occur in actuality against the background of processes
characterized by shorter scales (noises). Among these processes are, for ex-
ample, the molecular noises (in the context of macroscopic hydrodynamics),
microturbulence (against the large-scale motions), and the eﬀect of truncated
(small-scale) terms in the ﬁnite-dimensional approximation of hydrodynamic
equations. The eﬀect of these small-scale noises should be analyzed in statis-
tical terms. Such a consideration can be performed in terms of macroscopic
variables. With this goal, one must include external random forces with cer-
tain statistical characteristics in the corresponding macroscopic equations.
The models considered here require additionally the inclusion of dissipative
terms in the equations of motion to ensure energy outﬂow to small-scale
modes.
Accordingly, the simplest hydrodynamic models that allow simulating ac-
tual processes are the HTS with random forces and linear friction.

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
13
An important problem appeared, for example, in the theory of climate
consists in the determination of a possibility of signiﬁcantly diﬀerent cir-
culation processes occurred under the same distribution of incoming heat,
i.e., the problem on the existence of diﬀerent regimes of motion of a given
hydrodynamic system under the same ’external conditions’. A quite natu-
ral phenomenon that should be considered in this context is the ’hopping’
phenomenon that consists in switching between such regimes of motion. Cha-
racteristic of these regimes is the fact that the duration of switching is small
in comparison with the lifetime of the corresponding regimes.
It is expedient to study these problems using the above simple models. The
corresponding systems of quadratically nonlinear ordinary diﬀerential equa-
tions may generally have several stable regimes, certain parameters describing
external conditions being identical. The hopping events are caused by varia-
tions of these conditions in time and by the eﬀect of random noises [24]. If the
system has no stable regimes, its behavior can appear extremely diﬃcult and
require statistical description as it is the case in the Lorentz model [98]. In
the case of availability of indiﬀerent equilibrium states, the system may allow
quasisteady-state regimes of motion [25]. Switching between such regimes can
be governed by the dynamic structure of the system, and system behavior
can appear ’stochastic’ in this case, too.
In what follows, we study these hopping phenomena with the use of the
simplest hydrodynamic models.
Dynamics of a Triplet (Gyroscope)
Consider ﬁrst the case of a single stage, i.e., the triplet in the regime of
forced motion (Fig. 1.4a). With this goal, we set force f0(t) = f0 = const and
assume the availability of dissipative forces acting on the stable modes of the
triplet. In this case, equations of motion have the form
d
dtv1,0(t) = μ

v2
1,1(t) −v2
1,2(t)

+ f0,
d
dtv1,1(t) = −μv1,0(t)v1,1(t) −λv1,1(t),
d
dtv1,2(t) = μv1,0(t)v1,2(t) −λv1,2(t).
(1.19)
If f0 > 0, component v1,1(t) vanishes with time, so that the motion of the
triplet (1.19) is described in the dimensionless variables
x =

 μ
f0
v1,0 −
λ
√μf0
,
y =

 μ
f0
v1,2,
τ =

μf0t
(1.20)

14
1
Examples, Basic Problems, Peculiar Features of Solutions
by the two-mode system
d
dtx(t) = −y2(t) + 1,
d
dty(t) = x(t)y(t).
(1.21)
This system has the integral of motion H1 = x2(t) + y2(t) −2 ln y(t). The
change of variables p(t) = x(t), q(t) = ln y(t) reduces the system to the
Hamiltonian form
d
dtp(t) = −∂H(p, q)
∂q
,
d
dtq(t) = ∂H(p, q)
∂p
with the Hamiltonian H(p, q) = p2(t)
2
+ 1
2e2q(t) −q(t).
Thus, the behavior of a system with friction (1.19) under the action of a
constant external force is described in terms of the Hamiltonian system.
Stationary points (0, 1) and (0, −1) of system (1.21) are the centers. If
H1 −1 ≪1, the period T1 of motion along closed trajectories around every of
these singular points is determined by the asymptotic formula (it is assumed
that the sign of y(t) remains intact during this motion)
T1 ≈
√
2π

1 + H1 −1
12

.
In the opposite limiting case H1 ≫1 (the trajectories are signiﬁcantly distant
from the mentioned centers), we obtain
T1 ≈
1
√H1
[2H1 + ln H1] .
Supplement now the dynamic system (1.19) with the linear friction acting
on component v01(t):
d
dtv1,0(t) = μ

v2
1,1(t) −v2
1,2(t)

−λv1,0(t) + f0,
d
dtv1,1(t) = −μv1,0(t)v1,1(t) −λv1,1(t),
d
dtv1,2(t) = μv1,0(t)v1,2(t) −λv1,2(t).
(1.22)
Introducing again the dimensionless variables
t →t/λ,
v1,0(t) →λ
μv0(t),
v1,2(t) →λ
μv1(t),
v1,1(t) →λ
μv2(t),
we arrive at the system of equations

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
15
d
dtv0(t) = v2
2(t) −v2
1(t) −v0(t) + R,
d
dtv1(t) = v0(t)v1(t) −v1(t),
d
dtv2(t) = −v0(t)v2(t) −v2(t),
(1.23)
where quantity R = μf0
λ2 is the analog of the Reynolds number.
Dynamic system (1.23) has steady-state solutions that depend now on
parameter R, and R = Rcr = 1 is the critical value.
For R < 1, the system has the stable steady-state solution
v1 = v2 = 0,
v0 = R.
For R > 1, this solution becomes unstable with respect to small distur-
bances of parameters, and the steady-state regimes
v0 = 1,
v2 = 0,
v1 = ±
√
R −1
(1.24)
become available. Here, we have an element of randomness because compo-
nent v1 can be either positive or negative, depending on the amplitude of
small disturbance.
Assume now that all components of the triplet are acted on by random
forces. This means that system (1.23) is replaced with the system of equations
d
dtv0(t) = v2
2(t) −v2
1(t) −v0(t) + R + f0(t),
d
dtv1(t) = v0(t)v1(t) −v1(t) + f1(t),
d
dtv2(t) = −v0(t)v2(t) −v2(t) + f2(t).
(1.25)
This system describes the motion of a triplet(gyroscope) with the linear
isotropic friction, which is driven by the force acting on the instable mode
and having both regular (R) and random (f(t)) components. Such a situation
occurs, for example, for a ﬂuid moving in the ellipsoidal cavity [13,14,23].
For R > 1, dynamic system (1.25) under the action of random disturbances
will ﬁrst reach the vicinity of one of the stable states (1.24), and then, after
the lapse of certain time, it will be hoppingly set into the vicinity of the other
stable state. Figure 1.5 shows the results of simulations of this phenomenon
for R = 6 and diﬀerent realizations of random force f(t), whose components
were simulated as the Gaussian random processes. (See Sect. 11.3.1, page 347
for the statistical description of this problem.)

16
1
Examples, Basic Problems, Peculiar Features of Solutions
−1
−2
3
0
1
2
2
4
4
6
88
10
12
14
t
√
5
−
√
5
Fig. 1.5 Hopping phenomenon simulated from system (1.25) for R = 6 and vari-
ance σ = 0.1 (the solid and dashed lines show components v0(t) and v1(t), respec-
tively).
Thus, within framework of the dynamics of the ﬁrst stage, the hopping
phenomenon can occur only due to the eﬀect of external random forces acting
on all modes.
Hopping between Quasisteady-State Regimes
The simplest two-stage system can be represented in the form
d
dtv1,0(t) = v2
1,1(t) −v2
1,2(t) + 1,
d
dtv1,1(t) = −v1,0(t)v1,1(t) + Q

v2
2,1(t) −v2
2,2(t)

,
d
dtv1,2(t) = −v1,0(t)v1,2(t) + Q

v2
2,3(t) −v2
2,4(t)

,
d
dtv2,1(t) = −Qv1,1(t)v2,1(t),
d
dtv2,2(t) = Qv1,1(t)v2,2(t),
d
dtv2,3(t) = −Qv1,2(t)v2,3(t),
d
dtv2,4(t) = Qv1,2(t)v2,4(t),
(1.26)

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
17
where we used the dimensionless variables similar to (1.20). Only the com-
ponents v1,0(t), v1,2(t), v2,3(t) and v2,4(t) survive for f0 > 0 (see Fig. 1.6).
These components satisfy the system of equations
f0(t)
v1,0(t)
v1,2(t)
v2,3(t)
v2,4(t)
Fig. 1.6 Diagram of the excited seven-mode HTS
d
dtv1,0(t) = −v2
1,2(t) + 1,
d
dtv1,2(t) = −v1,0(t)v1,2(t) + Q

v2
2,3(t) −v2
2,4(t)

,
d
dtv2,3(t) = −Qv1,2(t)v2,3(t),
d
dtv2,4(t) = Qv1,2(t)v2,4(t)
that has the integral of motion
v2,3(t)v2,4(t) = I = const.
Introducing notation
x(t) = v1,0(t),
y(t) = v1,2(t),
z(t) = v2,4(t)
v2,3(t),
we arrive at the system of three equations
d
dtx(t) = −y2(t) + 1,
d
dty(t) = x(t)y(t) + QI
 1
z(t) −z(t)

,
(1.27)
d
dtz(t) = y(t)z(t)
that describes the behavior of the seven-mode model (1.26).

18
1
Examples, Basic Problems, Peculiar Features of Solutions
Inclusion of the second stage signiﬁcantly changes the dynamics of the ﬁrst
stage. Indeed, the initial values of components v2,3 and v2,4 being arbitrarily
small, they nevertheless determine certain value of the integral of motion I,
and, by virtue of Eqs. (1.27), variable y will repeatedly change the sign.
Consider the case of small values of constant I in more detail. Figure 1.7
shows the numerical solution of Eqs. (1.27) with the initial conditions x =
0.05, y = 1, and z = 1 at Q =
√
8 and I = 10−20. As may be seen, two types
of motion are characteristic of system (1.27) for small values of constant I;
namely, ’fast’ motions occur in a small vicinity of either closed trajectory of
the Hamiltonian system (1.21) near the plane z = 1, and relatively rare hop-
ping events occur due to the changes of sign of variable y at z ∼I or z ∼−1
I .
Every such hopping event signiﬁcantly changes the parameters of fast motion
trajectories of system (1.27), so that the motion of the system acquires the
features characteristic of the dynamic systems with strange attractors (see,
e.g., [98]).
a
b
1
2
0
0
0
1
1
2
−1
−1
−2
−20
−10
10
10
20
t
log z
y
y
100
50
0.5
0.5
−0.5
−0.5
x
Fig. 1.7 (a) Time-dependent behavior of components of system (1.27) (curve 1 for
y and curve 2 for log z) and (b) projection of the phase trajectory of system (1.27)
on plane (x, y)
To describe ’slow’ motions, we introduce variables X and Y by the formulas
x(t) = X(t)x1(t),
y(t) = Y (t)y1(t),

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
19
where (x1(t), y1(t)) is a solution to Eqs. (1.21). According to Eqs. (1.27), X(t),
Y (t), and z(t) satisfy the equations
d
dtX(t) =
1
x1(t)

X2(t) −Y 2(t)

y2
1(t) + 1 −X(t)

,
d
dtY (t) = (X(t) −1) Y (t)x1(t) + QI
y1(t)
 1
z(t) −z(t)

,
d
dtz(t) = 2QY (t)y1(t)z(t).
Averaging this equations in slow varying quantities over the period1 T1 =
√
2π, we obtain
d
dtX(t) = 0,
d
dtY (t) = QI

1
Z(t) −Z(t)

,
d
dtZ(t) = 2QY (t)Z(t),
(1.28)
where Z = z(t). The system (1.28) has the integral of motion
H2 = Y 2(t) + I

1
Z(t) + Z(t)

,
(1.29)
so that we can use the corresponding variables to rewrite it in the Hamiltonian
form, as it was done for the system (1.21). The motion of system (1.28) along
closed trajectories around stationary point (0, 1) is characterized by the half-
period T2 (the time between hopping events) given by the formula
T2 = 2
Q
Z2

Z1
dZ
Z

H2 −I
 1
Z + Z
 =
1
Q

I

r +
√
r2 −1
K
⎛
⎝

2
√
r2 −1
r +
√
r2 −1
⎞
⎠,
where Z1 and Z2 are the roots of Eq. (1.29) at Y = 0, r = 2H2/I, and K(z)
is the complete elliptic integral of the ﬁrst kind. For small I, we have
T2 ≈
1
Q√H2
ln
4H2
I

.
(1.30)
Figure 1.8 shows the numerical solution of Eqs. (1.28) with the initial con-
ditions Y = 1, Z = 1 (they correspond to the initial conditions used earlier
for solving Eqs. (1.27)), constants Q and I also being coincident with those
1 This expression for T1 appears to be quite adequate for the solution shown in
Fig. 1.7. On closed trajectories of system (1.21) near which this solution passes,
the values of Hamiltonian do not exceed 2. Because these trajectories are lo-
cated in small vicinities of critical points (0, 1) and (0, −1) of this system, we
performed averaging assuming that y1(t) = 1 and 
1/y1(t) = 1.

20
1
Examples, Basic Problems, Peculiar Features of Solutions
used for solving Eqs. (1.27). The comparison of curves in Figs. 1.7 and 1.8
shows that system (1.28) satisfactorily describes hopping events in system
(1.27). The values of half-period T2 determined by Eq. (1.30) and obtained
from the numerical integration of Eqs. (1.27) are 33.54 and 33.51, respectively.
We note that system (1.27) has an additional characteristic time T3 ∼1/Q,
whose meaning is the duration of the hopping event.
1
2
0
0
1
2
−1
−2
−20
−10
10
10
20
t
log Z
Y
50
Fig. 1.8 Time-dependent behavior of components of system (1.28) (curve 1 for
Y and curve 2 for log Z )
1.1.4
Systems with Blow-Up Singularities
The simplest stochastic system showing singular behavior in time is described
by the following equation commonly used in the statistical theory of waves
d
dtx(t) = −λx2(t) + f(t),
x(0) = x0,
λ > 0,
(1.31)
where f(t) is the random function of time.
In the absence of randomness (f(t) = 0), the solution to Eq. (1.31) has the
form
x(t) =
1
λ (t −t0),
t0 = −1
λx0
.

1.1
Ordinary Diﬀerential Equations: Initial-Value Problems
21
For x0 > 0, we have t0 < 0, and solution x(t) monotonically tends to zero
with increasing time. On the contrary, for x0 < 0, solution x(t) reaches −∞
within a ﬁnite time t0 = −1/λx0, which means that the solution becomes
singular and shows the blow-up behavior. In this case, random force f(t)
has insigniﬁcant eﬀect on the behavior of the system. The eﬀect becomes
signiﬁcant only for positive parameter x0.
x(t)
x0
0
t
Fig. 1.9 Typical realization of the solution to Eq. (1.31)
Here, the solution, slightly ﬂuctuating, decreases with time as long as it
remains positive. On reaching suﬃciently small value x(t), the impact of
force f(t) can cause the solution to hop into the region of negative values of
x, where it reaches the value of −∞within a certain ﬁnite time.
Thus, in the stochastic case, the solution to problem (1.31) shows the blow-
up behavior for arbitrary values of parameter x0 and always reaches −∞
within a ﬁnite time t0. Figure 1.9 schematically shows the temporal realiza-
tion of the solution x(t) to problem(1.31) for t > t0; its behavior resembles a
quasi-periodic structure.
1.1.5
Oscillator with Randomly Varying Frequency
(Stochastic Parametric Resonance)
In the above stochastic examples, we considered the eﬀect of additive random
impacts (forces) on the behavior of systems. The simplest nontrivial system
with multiplicative (parametric) impact can be illustrated using the stochas-
tic parametric resonance as an example. Such a system is described by the
second-order equation

22
1
Examples, Basic Problems, Peculiar Features of Solutions
d2
dt2 x(t) + ω2
0[1 + z(t)]x(t) = 0,
x(0) = x0,
d
dtx(0) = v0,
(1.32)
where z(t) is the random function of time. This equation is characteristic
of almost all ﬁelds of physics. It is physically obvious that dynamic system
(1.32) is capable of parametric excitation, because random process z(t) has
harmonic components of all frequencies, including frequencies 2ω0/n (n =
1, 2, · · · ) that exactly correspond to the frequencies of parametric resonance
in the system with periodic function z(t), as it is the case, for example, in
the Mathieu equation.
1.2
Linear Ordinary Diﬀerential Equations:
Boundary-Value Problems
In the previous section, we considered several dynamic systems described by
a system of ordinary diﬀerential equations with given initial values. Now,
we consider the simplest linear boundary-value problem, namely, the steady
one-dimensional wave problem.
1.2.1
Plane Waves in Layered Media: A Wave
Incident on a Medium Layer
Let the layer of inhomogeneous medium occupies the part of space L0 < x <
L and let the unit-amplitude plane wave u0 (x) = e−ik(x−L) is incident on
this layer from the region x > L (Fig. 1.10a).
a
b
x
x
L
L
ε(x)
ε(x)
TLeik(L0−x)
RLe−ik(L−x)
eik(L−x)
L0
L0
x0
T1eik(L0−x)
T2e−ik(L−x)
Fig. 1.10 (a) Plane wave incident on the medium layer and (b) source inside the
medium layer

1.2
Linear Ordinary Diﬀerential Equations: Boundary-Value Problems
23
The waveﬁeld satisﬁes the Helmholtz equation,
d2
dx2 u(x) + k2(x)u(x) = 0,
(1.33)
where
k2(x) = k2[1 + ε(x)]
and function ε(x) describes medium inhomogeneities. We assume that ε(x) =
0, i.e., k(x) = k outside the layer; inside the layer, we set ε(x) = ε1(x) + iγ,
where ε1(x) is the real part responsible for wave scattering in the medium
and the imaginary part γ ≪1 describes the absorption of the wave in the
medium.
In region x > L, the waveﬁeld has the structure
u(x) = e−ik(x−L) + RLeik(x−L),
where RL is the complex reﬂection coeﬃcient. In region x < L0, the structure
of the waveﬁeld is
u(x) = TLeik(L0−x),
where TL is the complex transmission coeﬃcient. Boundary conditions for
Eq. (1.33) are the continuity conditions for the ﬁeld and the ﬁeld derivative
at layer boundaries; they can be written as follows
u(L) + i
k
du(x)
dx

x=L
= 2,
u(L0) −i
k
du(x)
dx

x=L0
= 0.
(1.34)
Thus, the waveﬁeld in the layer of an inhomogeneous medium is described
by the boundary-value problem (1.33), (1.34). Dynamic equation (1.33) co-
incides in form with Eq. (1.32). Note that the problem under consideration
assumes that function ε(x) is discontinuous at layer boundaries. We will call the
boundary-value problem (1.33), (1.34) the unmatched boundary-value prob-
lem. In such problems, wave scattering is caused not only by medium inhomo-
geneities, but also by discontinuities of function ε(x) at layer boundaries.
If medium parameters (function ε1(x)) are speciﬁed in the statistical form,
then solving the stochastic problem (1.33), (1.34) consists in obtaining sta-
tistical characteristics of the reﬂection and transmission coeﬃcients, which
are related to the waveﬁeld values at layer boundaries by the relationships
RL = u(L) −1,
TL = u(L0),
and the waveﬁeld intensity
I(x) = |u(x)|2
inside the inhomogeneous medium. Determination of these characteristics
constitutes the subject of the statistical theory of radiative transfer.

24
1
Examples, Basic Problems, Peculiar Features of Solutions
Note that, for x < L, from Eq. (1.33) follows the equality
kγI(x) = d
dxS(x),
where energy-ﬂux density S(x) is determined by the relationship
S(x) = i
2k

u(x) d
dxu∗(x) −u∗(x) d
dxu(x)

.
By virtue of boundary conditions, we have S(L) = 1 −|RL|2 and S(L0) =
|TL|2.
0
1
2
2.5
5 Dx
Fig. 1.11 Dynamic localization phenomenon simulated for two realizations of
medium inhomogeneities
For non-absorptive media (γ = 0), conservation of energy-ﬂux density is
expressed by the equality
|RL|2 + |TL|2 = 1.
(1.35)
Consider some features characteristic of solutions to the stochastic
boundary-value problem (1.33), (1.34). On the assumption that medium in-
homogeneities are absent (ε1(x) = 0) and absorption γ is suﬃciently small,
the intensity of the waveﬁeld in the medium slowly decays with distance
according to the exponential law
I(x) = |u(x)|2 = e−kγ(L−x).
(1.36)
Figure 1.11 shows two realizations of the intensity of a wave in a suﬃciently
thick layer of medium. These realizations were simulated for two realizations
of medium inhomogeneities [137]. The diﬀerence between them consists in
the fact that the corresponding functions ε1(x) have diﬀerent signs in the

1.2
Linear Ordinary Diﬀerential Equations: Boundary-Value Problems
25
middle of the layer at a distance of the wavelength. This oﬀers a possibility
of estimating the eﬀect of a small medium mismatch on the solution of the
boundary problem. Omitting the detailed description of problem parameters,
we mention only that this ﬁgure clearly shows the prominent tendency of
a sharp exponential decay (accompanied by signiﬁcant spikes toward both
higher and nearly zero-valued intensity values), which is caused by multiple
reﬂections of the wave in the chaotically inhomogeneous random medium
(the phenomenon of dynamic localization). Recall that absorption is small
(γ ≪1), so that it cannot signiﬁcantly aﬀect the dynamic localization.
It is well known that the introduction of new function
ψ (x) = i
k
d
dx ln u (x)
reduces the second-order equation (1.33) to two ﬁrst-order equations, and
this function satisﬁes the closed equation following from Eq. (1.33):
d
dxψ (x) = ik

ψ2 (x) −1 −ε(x)

,
ψ(L0) = 1.
(1.37)
From the condition at boundary x = L follows that
u (L) =
2
1 + ψ (L)
and, consequently, the reﬂection coeﬃcient is determined from the solution
to Eq. (1.37) by the formula
RL = 1 −ψ (L)
1 + ψ (L).
Introducing the function
R(x) = 1 −ψ (x)
1 + ψ (x),
ψ(x) = 1 −R(x)
1 + R(x),
we can rewrite Eq. (1.37) in the form of the equation
d
dxR(x) = 2ikR(x) + i
2kε(x) (1 + R(x))2 ,
R(L0) = 0
(1.38)
whose solution at x = L coincides with the reﬂection coeﬃcient, i.e.,
RL = R (L) .
In terms of function R(x), the waveﬁeld u(x) inside the medium is now
expressed by the equality

26
1
Examples, Basic Problems, Peculiar Features of Solutions
u (x) = [1 + R (L)] exp
⎡
⎣ik
L

x
dξ 1 −R (ξ)
1 + R (ξ)
⎤
⎦.
(1.39)
Figure 1.12a shows the traditional procedure of solving the problem. One
solves Eq. (1.38) ﬁrst and then reconstructs the waveﬁeld by the formula
(1.39). This is the well known approach called the sweep method. However,
it is inappropriate for analyzing statistical problems.
x
x
x
x
L
L
L
L
x0
x0
L0
L0
L0
L0
a
b
c
d
u(x)
ux
ux
u(L)
u(x; L)
u(x; L)
uL
uL
R(L)
G(x; x0; L)
G(x0; x0)
G(x; x0)
u(x; x)
u(x; x0)
R1(L)
R2(L)
Fig. 1.12 Schematic of solving boundary problem (1.33), (1.34) by (a) the sweep
method and (b) the imbedding method and boundary-value problem (1.49) by (c)
the sweep method and (d) the imbedding method
Alternatively, the waveﬁeld inside the medium can be represented in the
form
u(x) = u1(x) + u2(x),
d
dxu(x) = −ik[u1(x) −u2(x)],
where u1(x) and u2(x) are the complex contradirectional modes. Because
these modes are related to the waveﬁeld by the expressions
u1 (x) = 1
2

1 + i
k
d
dx

u (x) ,
u1 (L) = 1,
u2 (x) = 1
2

1 −i
k
d
dx

u (x) ,
u2 (L0) = 0,
(1.40)
we can rewrite the boundary-value problem (1.33), (1.34) in the form

1.2
Linear Ordinary Diﬀerential Equations: Boundary-Value Problems
27
 d
dx + ik

u1 (x) = −ik
2 ε (x) [u1 (x) + u2 (x)] ,
u1 (L) = 1,
 d
dx −ik

u2 (x) = −ik
2 ε (x) [u1 (x) + u2 (x)] ,
u2 (L0) = 0.
(1.41)
Note that function R (x) introduced earlier is expressed in terms of modes
u1 (x) and u2 (x) simply as the ratio
R (x) = u2 (x)
u1 (x).
The imbedding method oﬀers a possibility of reformulating the boundary-
value problem (1.33), (1.34) to the dynamic problem with the initial values
for parameter L (this parameter corresponds to the geometrical position of
the layer right-hand boundary) by considering the solution to the boundary-
value problem as a function of parameter L [49,50,52]. On such reformulation,
the reﬂection coeﬃcient RL satisﬁes the Riccati equation
d
dLRL = 2ikRL + ik
2 ε(L) (1 + RL)2 ,
RL0 = 0
(1.42)
that coincides, naturally, with Eq. (1.41), and the waveﬁeld in the medium
layer u(x) ≡u(x; L) satisﬁes the linear equation
∂
∂Lu(x; L) = iku(x; L) + ik
2 ε(L) (1 + RL) u(x; L),
u(x; x) = 1 + Rx,
(1.43)
which can be derived, for example, by diﬀerentiating Eq. (1.39) with respect
to parameter L. Figure 1.12b shows the procedure of solving the problem in
this formulation. Comparing this procedure with that of the sweep method
(Fig. 1.12a), we see that solving procedure has changed the direction, and
namely this fact will oﬀer a possibility of constructing the statistical descrip-
tion of the solution to the problem in the stochastic formulation.
The equation for the squared modulus of the reﬂection coeﬃcient WL =
|RL|2 follows from Eq. (1.42):
d
dLWL = −2kγWL −ik
2 ε1(L) (RL −R∗
L) (1 −WL) ,
WL0 = 0.
(1.44)
Note that condition WL0 = 1 will be the initial value to Eq. (1.44) in the
case of totally reﬂecting boundary at L0. In this case, the wave incident on
the layer of a non-absorptive medium (γ = 0) is totally reﬂected from the
layer, i.e., WL = 1, so that the reﬂection coeﬃcient can be written in the form
RL = eiφL. For the phase of the reﬂection coeﬃcient, we have the dynamic
equation following from Eq. (1.42)

28
1
Examples, Basic Problems, Peculiar Features of Solutions
d
dLφL = 2k + kε1(L) (1 + cos φL) .
(1.45)
It governs phase variations in the whole range of values (−∞, +∞). At the
same time, the equation for the waveﬁeld (1.43) depends only on trigonomet-
ric functions of the phase of reﬂection coeﬃcient. For this reason, it would
be desirable to deal with the phase varying in interval (−π, π). We can do
this by introducing new function zL = tan(φL/2). This function satisﬁes the
dynamic equation of type (1.31)
d
dLzL = k

1 + z2
L

+ kε1(L),
and its solution shows the singular behavior.
In the general case of arbitrarily reﬂecting boundary L0, the steady-state
(independent of L) solution WL = 1 corresponding to the total reﬂection
of incident wave formally exists for a half-space (L0 →−∞)
ﬁlled with
non-absorptive random medium, too. This solution is actually realized in the
statistical problem with probability equal to one [49,50,52].
It is obvious that the division of the ﬁeld into contradirectional modes
(1.40) is of arbitrary nature; this is nothing more than the mathematical
technique that reduces the second-order equation (1.33) to two ﬁrst-order
equations with simplest boundary conditions.
If, in contrast to the above problem, we assume that function k(x) is
continuous at boundary x = L, i.e., if we assume that the wave number in
the free half-space x > L is equal to k(L), then boundary conditions (1.34)
of problem (1.33) will be replaced with the conditions
u(L) +
i
k(L)
du(x)
dx

x=L
= 2,
u(L0) −
i
k(L0)
du(x)
dx

x=L0
= 0.
(1.46)
We will call the boundary-value problem (1.33), (1.46) the matched boundary-
value problem. In this case, it is convenient to represent the waveﬁeld in the
form
u (x) = u1 (x) + u2 (x) ,
du(x)
dx
= −ik (x) [u1 (x) −u2 (x)] ,
where the complex contradirectional modes u1(x) and u2(x) are now related
to the waveﬁeld by the expressions

1.2
Linear Ordinary Diﬀerential Equations: Boundary-Value Problems
29
u1 (x) = 1
2

1 +
i
k (x)
d
dx

u(x),
u1 (L) = 1,
u2 (x) = 1
2

1 −
i
k (x)
d
dx

u(x),
u2 (L0) = 0
and satisfy the boundary-value problem
 d
dx + ik (x)

u1(x) = −k′ (x)
k (x) [u1 (x) −u2 (x)] ,
u1 (L) = 1,
 d
dx −ik (x)

u2(x) =
k′ (x)
k (x) [u1 (x) −u2 (x)] ,
u2 (L0) = 0,
where k′(x) = dk(x)
dx . Function R(x) = u2(x)/u1(x) is now described by the
Riccati equation
d
dxR(x) = 2ikR(x) + k′(x)
2k

1 −R2(x)

,
R (L0) = 0,
(1.47)
and the reﬂection coeﬃcient is determined in terms of the solution to
Eq. (1.47) from the relationship
RL = R(L).
In the case of suﬃciently small function ε(x), we can rewrite Eq. (1.47) in
the form
dR(x)
dx
= 2ikR(x) + 1
4ε′ (x)

1 −R2(x)

,
where the derivative of function ε(x) appears as distinct from Eq. (1.41).
Note that, for the matched boundary-value problem (1.33), (1.46), the
equations of the imbedding method have the form
d
dLRL = 2ikRL + 1
4ε′ (L)

1 −R2
L

,
RL0 = 0,
∂
∂Lu (x, L) = 2iku (x, L) + 1
4ε′ (L) (1 −RL) u (x, L) ,
u (x, x) = 1 + Rx.
(1.48)
1.2.2
Plane Waves in Layered Media: Source Inside
the Medium
The ﬁeld of a point source located in the layer of random medium is described
by the similar boundary-value problem for Green’s function of the Helmholtz
equation:

30
1
Examples, Basic Problems, Peculiar Features of Solutions
d2
dx2 G(x; x0) + k2[1 + ε(x)]G(x; x0) = 2ikδ(x −x0),
G(L; x0) + i
k
dG(x; x0)
dx

x=L
= 0,
G(L0; x0) −i
k
dG(x; x0)
dx

x=L0
= 0.
(1.49)
Outside the layer, the solution has here the form of outgoing waves
(Fig. 1.10b)
G(x; x0) = T1eik(x−L)
(x ≥L),
G(x; x0) = T2e−ik(x−L0)
(x ≤L0).
Note that, for the source located at layer boundary x0 = L, this problem
coincides with the boundary-value problem (1.33), (1.34) on the wave incident
on the layer, which yields
G(x; L) = u(x; L).
The solution to the boundary-value problem (1.49) has the structure
G (x; x0) = G (x0; x0)
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
exp

ik
x0

x
ψ1 (ξ) dξ

, x0 ≥x,
exp

ik
x
x0
ψ2 (ξ) dξ

x0 ≤x,
(1.50)
where the ﬁeld at the source location, by virtue of the derivative gap condition
dG(x; x0)
dx

x=x0+0
−dG(x; x0)
dx

x=x0−0
= 2ik,
is determined by the formula
G (x0; x0) =
2
ψ1 (x0) + ψ2 (x0)
and functions ψi(x) satisfy the Riccati equations
d
dxψ1 = ik

ψ2
1 −1 −ε (x)

,
ψ1 (L0) = 1,
d
dxψ2 = −ik

ψ2
2 −1 −ε (x)

,
ψ2 (L) = 1.
(1.51)
Figure 1.12c shows the procedure of solving this problem by the sweep
method. One solves two equations (1.51) ﬁrst and then reconstructs the wave-
ﬁeld using Eq. (1.50).

1.2
Linear Ordinary Diﬀerential Equations: Boundary-Value Problems
31
Introduce new functions Ri(x) related to functions ψi(x) by the formula
ψi(x) = 1 −Ri (x)
1 + Ri (x),
i = 1, 2.
With these functions, the waveﬁeld in region x < x0 can be written in the
form
G (x; x0) = [1 + R1 (x0)] [1 + R2 (x0)]
1 −R1 (x0) R2 (x0)
exp
⎡
⎣ik
x0

x
dξ 1 −R1 (ξ)
1 + R1 (ξ)
⎤
⎦,
(1.52)
where function R1(x) satisﬁes the Riccati equation (1.38).
For x0 = L, expression (1.52) becomes
G (x; L) = u(x; L) = [1 + R1 (L)] exp
⎡
⎣ik
L

x
dξ 1 −R1 (ξ)
1 + R1 (ξ)
⎤
⎦,
(1.53)
so that parameter R1(L) = RL is the reﬂection coeﬃcient of the plane wave
incident on the layer from region x > L. In a similar way, quantity R2(x0)
is the reﬂection coeﬃcient of the wave incident on the medium layer (x0, L)
from the homogeneous half-space x < x0 (i.e., from region with ε = 0).
Using Eq. (1.53), we can rewrite Eq. (1.52) in the form
G (x; x0) =
1 + R2 (x0)
1 −R1 (x0) R2 (x0)u (x; x0) ,
x ≤x0,
where u(x; x0) is the waveﬁeld inside the inhomogeneous layer (L0, x0) in the
case of the incident wave coming from the free half-space x > x0.
Thus, for x < x0, the ﬁeld of the point source is proportional to the
waveﬁeld generated by the plane wave incident on layer (L0, x0) from the
free half-space x > x0. The layer segment (x0, L) aﬀects only parameter
R2(x0).
Note that, considering the waveﬁeld as a function of parameter L (i.e.,
setting G(x; x0) ≡G(x; x0; L)), we can use the imbedding method to obtain
the following system of equations with initial values:

32
1
Examples, Basic Problems, Peculiar Features of Solutions
∂
∂LG(x; x0; L) = ik
2ε(L)u(x0; L)u(x; L),
G(x; x0; L)|L=max(x,x0) =

u (x; x0) , x ≥x0
u (x0; x) , x ≤x0 ,
∂
∂Lu(x; L) = ik {1 + ε(L)u(L; L)} u(x; L),
u(x; L)|L=x = u(x; x),
d
dLu(L; L) = 2ik[u(L; L) −1] + ik
2ε(L)u2(L; L),
u(L0; L0) = 1.
(1.54)
Here, two last equations describe the waveﬁeld appearing in the problem on
the wave incident on the medium layer. Figure 1.12d shows the procedure of
solving this problem.
1.2.3
Plane Waves in Layered Media: Two-Layer
Model
Investigators often faces with multidimensional situations in which one wave
mode can originate other wave mode due to dependence of problem parame-
ters on spatial coordinates. Sometimes, such problems allow a parametriza-
tion by selecting certain direction and dividing the medium in this direction
into the layers characterized by discrete values of certain parameters, whereas
other parameters may vary continuously in these layers. As an example, we
mention the large-scale and low-frequency motions in Earth’s atmosphere
and ocean, such as the Rossby waves. These waves can be described within
the framework of the quasi-geostrophic model that describes the atmosphere
and ocean as thin multilayer ﬁlms characterized in the vertical direction by
thicknesses and densities of layers [120]. At the same time, other parameters
H1
H2
H0
x
L
ε(x)
L0
Fig. 1.13 Two-layer model of medium

1.3
First-Order Partial Diﬀerential Equations
33
vary continuously in these layers. It is quite possible that the reason of the
local property of the Rossby waves consists in the spatial variation of bottom
topography inhomogeneities in the horizontal plane. The simplest one-layer
model is equivalent to the one-dimensional Helmholtz equation and describes
barotropic motions of the medium; the two-layer model (Fig. 1.13) includes
additionally the baroclinic eﬀects [28,54,75].
In the context of two-layer media, the simplest model describing the pro-
pagation of interacting waves is the system of equations [27]
d2
dx2 ψ1 + k2ψ1 −α1F (ψ1 −ψ2) = 0,
d2
dx2 ψ2 + k2 [1 + ε(x)] ψ2 + α2F (ψ1 −ψ2) = 0,
(1.55)
where parameters α1 = 1/H1, α2 = 1/H2 (H1 and H2 are the thicknesses
of the top and bottom layers), parameter F characterizes wave interaction,
and function ε(x) describes medium inhomogeneities in the bottom layer.
Boundary conditions for system (1.55) are the radiation conditions at inﬁnity.
Note that parameter F characterizing the medium parametrization in the
vertical direction appears in system (1.55) as some sort of the horizontal
scale responsible for generation of an additional wave. System (1.55) describes
wave interaction (and, in particular, dependence of parameters αi on layer
thicknesses) in conformity with problems of geophysical hydrodynamics. For
other problem types, the form of these relationships can change, which only
slightly concerns the essence of the problem. The only essential point is the
fact that wave interaction is the linear interaction.
Transition to the one-layer model is performed by setting F = 0, ψ1 = 0
which transforms the corresponding wave equation to the Helmholtz equation
(1.33). Limit process H1 →
0 also results in the transition to the one-
layer model; in this case, ψ1 = ψ2. However, one can bear in mind that
limit processes L0 →−∞(transition to the half-space) and Hi →0 do not
commute in statistical problems. In this case, layer thicknesses Hi must be
ﬁnite though arbitrarily small.
1.3
First-Order Partial Diﬀerential Equations
Consider now several dynamic systems (dynamic ﬁelds) described by partial
diﬀerential equations.

34
1
Examples, Basic Problems, Peculiar Features of Solutions
1.3.1
Linear First-Order Partial Diﬀerential
Equations
Diﬀusion of Density Field under Random Velocity Field
In the context of linear ﬁrst-order partial diﬀerential equations, the simplest
problems concern the equation of continuity for the density (concentration)
of a conservative tracer and the transfer of a nonconservative passive tracer
by a random velocity ﬁeld U(r, t):
 ∂
∂t + ∂
∂r U(r, t)

ρ(r, t) = 0,
ρ(r, 0) = ρ0(r),
(1.56)
 ∂
∂t + U(r, t) ∂
∂r

q(r, t) = 0,
q(r, 0) = q0(r).
(1.57)
The conservative tracer is a tracer whose total mass remains intact
M0 =

drρ(r, t) =

drρ0(r)
(1.58)
We can use the method of characteristics to solve the linear ﬁrst-order
partial diﬀerential equations (1.56), (1.57). Introducing characteristic curves
(particles)
d
dtr(t) = U(r, t),
r(0) = r0,
(1.59)
we can write these equations in the form
d
dtρ(t) = −∂U(r, t)
∂r
ρ(t),
ρ(0) = ρ0(r0),
d
dtq(t) = 0,
q(0) = q0(r0).
(1.60)
This formulation of the problem corresponds to the Lagrangian description,
while the initial dynamic equations (1.56), (1.57) correspond to the Eulerian
description.
Here, we introduced the characteristic vector parameter r0 in the system
of equations (1.59), (1.60). With this parameter, Eq. (1.59) coincides with
Eq. (1.1) that describes particle dynamics in random velocity ﬁeld.
The solution of the system of equations (1.59), (1.60) depends on the initial
value r0,
r(t) = r(t|r0),
ρ(t) = ρ(t|r0),
(1.61)
which we will isolate by the vertical bar symbol.
The ﬁrst equality in Eq. (1.61) can be considered as an algebraic equation
in characteristic parameter; the solution of this equation

1.3
First-Order Partial Diﬀerential Equations
35
r0 = r0(r, t)
exists because divergence j(t|r0) = det ∥∂ri(t|r0)/∂r0k∥is diﬀerent from zero.
Consequently, we can write the solution of the initial equation (1.56) in the
form
ρ(r, t) = ρ(t|r0(r, t)) =

dr0ρ(t|r0)j(t|r0)δ (r(t|r0) −r) .
From Eq. (1.59) follows the equation for divergence j(t|r0)
d
dtj(t|r0) = ∂U(r, t)
∂r
j(t|r0),
j(0) = 1.
(1.62)
Correlating it with Eq. (1.60), we see that
ρ(t|r0) = ρ0(r0)
j(t|r0)
(1.63)
and, consequently, the density ﬁeld can be rewritten in the form of the
equality
ρ(r, t) =

dr0ρ(t|r0)j(t|r0)δ (r(t|r0) −r) =

dr0ρ0(r0)δ (r(t|r0) −r)
(1.64)
that states the relationship between the Lagrangian and Eulerian characte-
ristics. For the position of the Lagrangian particle, the delta-function ap-
peared in the right-hand side of this equality is the
indicator function(see
the next chapter). Consequently, averaging this equality over an ensemble of
realizations of random velocity ﬁeld, we obtain the well-known relationship
between the average density in the Eulerian description and the one-time
probability density P(t, r|r0) = ⟨δ (r(t|r0) −r)⟩of the Lagrangian particle
(see, e.g., [112]):
⟨ρ(r, t)⟩=

dr0ρ0(r0)P (t, r|r0) .
(1.65)
For a nondivergent velocity ﬁeld (div U(r, t) = 0), both particle divergence
and particle density are conserved, i.e.,
j(t|r0) = 1,
ρ(t|r0) = ρ0(r0),
q(t|r0) = q0(r0).
Consider now stochastic features of the solutions to problem (1.56). A
convenient way of analyzing random ﬁeld dynamics consists in using topogra-
phic concepts. Indeed, in the case of the nondivergent velocity ﬁeld, temporal
evolution of the contour of constant concentration ρ = const coincides with
the dynamics of particles in this velocity ﬁeld and, consequently, coincides
with the dynamics shown in Fig. 1.1a, page 5. In this case, the area within
the contour remains constant and, as it is seen from Fig. 1.1a, the pattern

36
1
Examples, Basic Problems, Peculiar Features of Solutions
becomes highly indented, which is manifested in gradient sharpening and the
appearance of contour dynamics for progressively shorter scales. In the other
limiting case (the divergent velocity ﬁeld), the area within the contour tends
to zero, and the ﬁeld of density condenses forming clusters. One can ﬁnd
examples simulated for this case in papers [76,142]. These features of particle
dynamics disappear on averaging over an ensemble of realizations.
Cluster formation in the Eulerian description can be traced using the ran-
dom velocity ﬁeld of form (1.3), (1.4), page 4. If vx(t) ̸= 0, then concentration
ﬁeld in Lagrangian description ρ(t|r0) in the particular case of uniform (inde-
pendent of r) initial distribution ρ0(r) = ρ0 can be described by the following
equation
d
dtρ(t) = −2kvx(t) cos(2kx)ρ(t),
ρ(0) = ρ0,
which can be rewritten, by virtue of Eq. (1.6), page 6, in the form
d
dtρ(t|r0) = −2kvx(t)1 −e2T (t) tan2(kx0)
1 + e2T (t) tan2(kx0)ρ(t|r0),
(1.66)
where function T (t) is given by Eq. (1.7), page 6. Integrating Eq. (1.66), we
obtain the Lagrangian representation of the velocity ﬁeld in the framework
of the model under consideration
ρ(t|x0)/ρ0 =

e−T (t)cos2(kx0) + eT (t)sin2(kx0)

.
Eliminating characteristic parameter x0 with the use of equalities
sin2 (kx0) =
e−T (t) sin2(kx(t))
eT (t) cos2(kx(t)) + e−T (t) sin2(kx(t)),
cos2 (kx0) =
eT (t) cos2(kx(t))
eT (t) cos2(kx(t)) + e−T (t) sin2(kx(t)),
(1.67)
following from Eq. (1.6), page 6, we pass to the Eulerian description
ρ(r, t)/ρ0 =
1
eT (t) cos2(kx) + e−T (t) sin2(kx).
(1.68)
Expression (1.68) shows that the density ﬁeld is low everywhere exclud-
ing the neighborhoods of points kx = nπ
2 , where ρ(x, t)/ρ0 = e±T (t) and is
suﬃciently high if random factor T (t) has appropriate sign.
Thus, in the problem under consideration, the cluster structure of the
density ﬁeld in the Eulerian description is formed in the neighborhoods of
points
kx = nπ
2
(n = 0, ±1, ±2, . . .).

1.3
First-Order Partial Diﬀerential Equations
37
Note that Eulerian density ﬁeld (1.68) averaged over spatial variables is in-
dependent of random factor T (t),
ρ(x, t)/ρ0 = 1,
and the average square of the density mainly grows with time
(ρ(x, t)/ρ0)2 = 1
2

eT (t) + e−T (t)
,
Figure 1.14 shows the Eulerian concentration ﬁeld 1 + ρ(r, t)/ρ0 and its
space-time evolution calculated by Eq. (1.68) in the dimensionless space-time
variables (the density ﬁeld is added with a unity to avoid the diﬃculties of
dealing with nearly zero-valued concentrations in the logarithmic scale). This
ﬁgure shows successive patterns of concentration ﬁeld rearrangement toward
narrow neighborhoods of points x ≈0 and x ≈π/2, i.e., the formation of
clusters, in which relative density is as high as 103−104, while relative density
is practically zero in the whole other space. Note that the realization of the
density ﬁeld passes through the initial homogeneous state at the instants t
such that T (t) = 0. As is seen from ﬁgures, the lifetimes of such clusters
coincide on the order of magnitude with the time of cluster formation.
ρ/ρ0 + 1
ρ/ρ0 + 1
ρ/ρ0 + 1
ρ/ρ0 + 1
x
x
x
x
0
0
0
0
0.4
0.4
0.4
0.4
0.8
0.8
0.8
0.8
1.2
1.2
1.2
1.2
1.6
1.6
1.6
1.6
1
1
1
1
10
10
10
10
100
100
100
100
a
b
c
d
t = 1
t = 2
t = 3
t = 4
t = 5
t = 6
t = 7
t = 8
t = 9
t = 10
t = 16
t = 17
t = 18
t = 19
t = 20
t = 21
t = 22
t = 23
t = 24
t = 25
1000
1000
1000
1000
10000
10000
10000
Fig. 1.14 Space-time evolution of the Eulerian density ﬁeld given by Eq. (1.68)

38
1
Examples, Basic Problems, Peculiar Features of Solutions
This model provides an insight into the diﬀerence between the diﬀusion
processes in divergent and nondivergent velocity ﬁelds. In nondivergent (in-
compressible) velocity ﬁelds, particles (and, consequently, density ﬁeld) have
no time for attracting to stable centers of attraction during the lifetime of
these centers, and particles slightly ﬂuctuate relative their initial location. On
the contrary, in the divergent (compressible) velocity ﬁeld, lifetime of stable
centers of attraction is suﬃcient for particles to attract to them, because
the speed of attraction increases exponentially, which is clearly seen from
Eq. (1.68).
From the above description, it becomes obvious that dynamic equation
(1.56) considered as a model equation describing actual physical phenom-
ena can be used only on ﬁnite temporal intervals. A more complete analy-
sis assumes the consideration of the ﬁeld of tracer concentration gradient
p(r, t) = ∇ρ(r, t) that satisﬁes the equation
 ∂
∂t + ∂
∂rU(r, t)

pi(r, t) = −pk (r, t) ∂Uk(r, t)
∂ri
−ρ(r, t)∂2U(r, t)
∂ri∂r
,
p(r, 0) = p0(r) = ∇ρ0(r).
(1.69)
In addition, one should also include the eﬀect of the dynamic diﬀu-
sion (with the dynamic diﬀusion coeﬃcient μ) that smooths the mentioned
sharpen of the gradient; this eﬀect is described by the linear second-order
partial diﬀerential equation
 ∂
∂t + ∂
∂rU(r, t)

ρ(r, t) = μΔρ(r, t),
ρ(r, 0) = ρ0(r).
(1.70)
In conclusion of this Section I give a pattern taken from Internet (Fig. 1.15)
that shows the cluster structure of the Universe and is seemingly a direct
consequence of clustering the cosmic matter in random velocity ﬁelds.
Fig. 1.15 Cluster structure of the Universe

1.3
First-Order Partial Diﬀerential Equations
39
Diﬀusion of Magnetic Field in a Random Velocity Field
The diﬀusion of such passive ﬁelds as the scalar density (particle concen-
tration) ﬁeld and the magnetic ﬁeld is an important problem of the theory
of turbulence in magnetohydrodynamics. Here, the basic stochastic equa-
tions are the continuity equation (1.56) for density ﬁeld ρ(r, t) (see previous
section) and the induction equation for nondivergent magnetic ﬁeld H(r, t)
(div H(r, t) = 0) [82]
∂
∂tH(r, t) = curl [U(r, t) × H(r, t)] ,
H(r, 0) = H0(r).
(1.71)
In Eq. (1.71), U(r, t) is the hydrodynamic velocity ﬁeld and pseudovector
C = A×B is the vector product of vectors A and B, Ci = εijkAjBk, where
εijk is the pseudotensor such that εijk = 0 if indices i, j, and k are not all
diﬀerent and εijk = 1 or εijk = −1, if indices i, j, and k are all diﬀerent and
form cyclic or anticyclic sequence (see, e.g., [7]). The operator
curl C(r, t) = [∇× C(r, t)],
curl C(r, t)|i = εijk
∂
∂rj
Ck(r, t)
is called the vortex of ﬁeld C(r, t).
In magnetohydrodynamics, velocity ﬁeld U(r, t) is generally described by
the Navier–Stokes equation complemented with the density of extraneous
electromagnetic forces
f(r, t) = 1
4π [curl H(r, t) × H(r, t)] .
Nevertheless we, as earlier, will consider velocity ﬁeld U(r, t) as random ﬁeld
whose statistical parameters are given.
Remark 1.1. Important Formulas of the Vector Analysis
The product of two pseudotensors is a tensor, and, in the case of pseu-
dotensors ε, we have the following equality
εilmεjpq = δijδlpδmq +δipδlqδmj +δiqδljδmp−δijδlqδmp−δipδljδmq −δiqδlpδmj,
(1.72)
Setting j = m (repeated indices assume summation), we obtain
εilmεmpq = (d −2)(δipδlq −δiqδlp),
(1.73)
where d is the dimension of space, so that the above convolution becomes
zero in the two-dimensional case.
Thus, the double vector product is given by the formula
[C × [A × B]]i = εilmεmpqClApBq = CqAiBq −CpApBi.
(1.74)

40
1
Examples, Basic Problems, Peculiar Features of Solutions
If ﬁelds C, A, and B are the conventional vector ﬁelds, Eq. (1.74) assumes
the form
[C × [A × B]] = (C · B) A −(C · A) B.
(1.75)
In the case of operator vector ﬁeld C = ∇= ∂
∂r, Eq. (1.74) results in the
expression
curl [A(r) × B(r)] =
 ∂
∂r · B(r)

A(r) −
 ∂
∂r · A(r)

B(r).
(1.76)
Note that, if vector ﬁeld A is an operator in Eq. (1.74), A = ∇= ∂
∂r, then
we have
[C(r) × curl B(r)] = Cq(r) ∂
∂rBq(r) −

C(r)· ∂
∂r

B(r)
and, in particular,
[B(r) × curl B(r)] = 1
2
∂
∂r B2(r) −

B(r)· ∂
∂r

B(r).
(1.77)
♦
Using Eq. (1.76), we can rewrite Eq. (1.71) in the form
 ∂
∂t + ∂
∂ru(r, t)

H(r, t) =

H(r, t) · ∂
∂r

u(r, t),
H(r, 0) = H0(r).
(1.78)
Dynamic system (1.78) is a conservative system, and magnetic ﬁeld ﬂux

dr H(r, t) remains constant during evolution.
We are interested in the evolution of magnetic ﬁeld in space and time from
given smooth initial distributions and, in particular, simply homogeneous
ones, H0(r) = H0. Clearly, at the initial evolutionary stages of the process,
the eﬀects of dynamic diﬀusion are insigniﬁcant, and Eq. (1.78) describes
namely this case. Further stages of evolution require consideration of the
eﬀects of dynamic diﬀusion; these eﬀects are described by the equation
 ∂
∂t + ∂
∂r u(r, t)

H(r, t) =

H(r, t) · ∂
∂r

u(r, t) + μHΔH(r, t), (1.79)
where μH is the dynamic diﬀusion coeﬃcient for the magnetic ﬁeld.
Note that, similarly to the case of tracer density, velocity ﬁled model (1.3),
(1.4), page 4 allows obtaining the magnetic ﬁeld in an explicit form if dynamic

1.3
First-Order Partial Diﬀerential Equations
41
diﬀusion can be neglected [67]. With the use of this model, the induction
equation for homogeneous initial condition (1.78) assumes the form
 ∂
∂t + vx(t) sin(2kx) ∂
∂x

H(x, t)=2k cos(2kx) [v(t)Hx(x, t) −vx(t)H(x, t)] ,
H(x, 0) = H0,
from which follows that the x-component of the magnetic ﬁeld remains con-
stant (Hx(x, t) = Hx0), and the existence of the Hx0 component causes the
appearance of an additional source of magnetic ﬁeld in the transverse (y, z)-
plane
 ∂
∂t + vx(t) sin(2kx) ∂
∂x

H⊥(r, t) = 2k cos(2kx) [v⊥(t)Hx0 −vx(t)H⊥(x, t)] ,
H⊥(x, 0) = H⊥0.
(1.80)
Equation (1.80) is a partial diﬀerential equation, and we can solve it using
the method of characteristics (the Lagrangian description). The characteris-
tics satisfy the equations
d
dtx(t|x0) = vx(t) sin(2kx(t|x0)), x(0|x0) = x0,
d
dtH⊥(t|x0) = 2k cos(2kx|x0) [v⊥(t)Hx0 −vx(t)H⊥(t|x0)] ,
H⊥(0|x0) = H⊥0,
(1.81)
where the vertical bar separates the dependence on characteristic parame-
ter x0.
The ﬁrst equation in Eqs. (1.81) describes particle diﬀusion, and its solu-
tion has the form
x(t|x0) = 1
k arctan

eT (t)tan(kx0)

,
where function T (t) is given by Eq. (1.7), page 6. The solution of the equation
in the magnetic ﬁeld has the form
H⊥(t|x0) =

e−T (t)cos2(kx0) + eT (t)sin2(kx0)

H⊥0
+ 2k

e−T (t)cos2(kx0) + eT (t)sin2(kx0)

×
t

0
dτ

e−T (τ)cos2(kx0) −eT (τ)sin2(kx0)


e−T (τ)cos2(kx0) + eT (τ)sin2(kx0)
2 vx(τ)v⊥(τ)Hx0.
Eliminating now characteristic parameter x0 with the use of Eqs. (1.67), page
36, we pass to the Eulerian description

42
1
Examples, Basic Problems, Peculiar Features of Solutions
H⊥(x, t) = ρ(x, t)
ρ0
H⊥0
+ 2kHx0
t

0
dτ

eT (t)−T (τ)cos2(kx) −e−T (t)+T (τ)sin2(kx)


eT (t)−T (τ)cos2(kx) + e−T (t)+T (τ)sin2(kx)
2 vx(τ)v⊥(τ),
(1.82)
where the density of passive tracer ρ(x, t) is described by Eq. (1.68). Making
now the change of integration variables t−τ = λ in Eq. (1.82), we can rewrite
it as
H⊥(x, t) = ρ(x, t)
ρ0
H⊥0
+2kHx0
t

0
dλ

eT (t)−T (τ)cos2(kx) −e−T (t)+T (τ)sin2(kx)


eT (t)−T (τ)cos2(kx) + e−T (t)+T (τ)sin2(kx)
2 vx(t−λ)v⊥(t−λ),
where
T (t) −T (τ) =
t

τ
dξvx(ξ) =
t−τ

0
dηvx(t −η) =
λ

0
dηvx(t −η).
Hence, dealing with the one-time statistical characteristics of magnetic ﬁeld,
we can replace vx(t −λ) with vx(λ) in view of stationarity of the velocity
ﬁeld (see section 4.2.1, 5.3 page 125) and rewrite Eq. (1.82) in a statistically
equivalent form,
H⊥(x, t) = ρ(x, t)
ρ0
H⊥0 + 2kHx0
t

0
dτ

eT (τ)cos2(kx) −e−T (τ)sin2(kx)


eT (τ)cos2(kx) + e−T (τ)sin2(kx)
2 vx(τ)v⊥(τ).
(1.83)
The ﬁrst term describes magnetic ﬁeld clustering like the density ﬁeld clus-
tering if H⊥0 ̸= 0. The second term describes the generation of a magnetic
ﬁeld in the transverse (y, z)-plane due to the presence of an initial ﬁeld Hx0.
At H⊥0 = 0, this term, proportional to the square of the random velocity
ﬁeld, determines the situation. Like the density ﬁeld, the structure of this
ﬁeld is also clustered.
Figures 1.16, 1.17 present the calculated space–time evolution of a real-
ization of energy of the magnetic ﬁeld generated in the transverse plane,
E(x, t) = H2
⊥(x, t), in dimensionless variables (see 1.12, page 7) at H⊥0 = 0
for the same realization of the random process T (t) as that presented previ-
ously in Fig. 1.2 a.

1.3
First-Order Partial Diﬀerential Equations
43
First of all, we note that the total energy of the generated magnetic ﬁeld
concentrated in the segment [0, π/2] increases rapidly with time (Fig. 1.16a).
20
18
16
14
12
10
8
6
4
0
10
20
30
40
t
log E(t)
a
1, 5
1
0, 5
0
0
10
20
30
40
t
x
xxx
b
Fig. 1.16 (a) Temporal evolution of the total magnetic energy in segment [0, π/2]
and (b) cluster structure in the (x, t)-plane
                         
0.5
1
1.5
x
0
0
20
40
100E(x, t)
E(t)
60
80
100
a
0.5
1
1.5
x
0
log E(x, t)
3
4
5
6
7
8
9
10
11
12
b
Fig. 1.17 Dynamics of disappearance of a cluster at point 0 and appearance of a
cluster at point π/2. The circles, triangles, and squares mark the curves correspon-
ding to time instants t = 10.4, 10.8, and 11.8, respectively.
A general space–time structure of the magnetic energy clustering is shown
in Fig. 1.16b. This structure was calculated in the following way. The coordi-
nates of points xi are plotted along the x-axis and the time is plotted along
the t-axis. The points are marked as white squares (unseen) if they contain
less than 1 of the energy available in the entire layer at current t and as black
squares if they contain more than 1 of the energy available in the entire layer

44
1
Examples, Basic Problems, Peculiar Features of Solutions
at the time in question. There are a total of 40000 points (100 steps in x and
400 steps in time).
A more detailed pattern of the evolution of clusters with time is presented
in Fig. 1.17. Figure 1.17a shows the percentage ratio of the generated mag-
netic energy ﬁeld contained in a cluster to the total energy in the layer at
the time under consideration; Fig. 1.17b shows the dynamics of the ﬂow of
magnetic energy perturbations from one boundary of the region to the other.
Fig. 1.18 Artist’s interpretation depicting the new view of the heliosphere (left).
Computer simulation of the magnetic reconnection in the heliosheats, which look
like bubbles, or sausages (right).
I illustrate structure formation in magnetic ﬁeld by the extract from an
internet-page: What does puzzle astrophysicists so strongly?
Contrary to hypotheses formed for ﬁfty years, at the boundary of planetary
system observers encountered a boiling foam of locally magnetized areas each
of hundreds of millions kilometers in extent, which form a non-stationary
cellular structure in which magnetic ﬁeld lines are permanently breaking and
recombining to form new areas—magnetic "bubbles" (see ﬁg. 1.18).2
1.3.2
Quasilinear Equations
Consider now the simplest quasilinear equation for scalar quantity q(r, t),
which we write in the form
 ∂
∂t + U(t, q) ∂
∂r

q(r, t) = Q (t, q) ,
q(r, 0) = q0(r),
(1.84)
where we assume for simplicity that functions U(t, q) and Q(t, q) are explic-
itly independent of spatial variable r.
2 Nancy Atkinson, Voyagers Find Giant Jacuzzi-like Bubbles at Edge of Solar
System, http://www.universetoday.com/86446/
voyagers-find-giant-jacuzzi-like-bubbles-at-edge-of-solar-system/

1.3
First-Order Partial Diﬀerential Equations
45
Supplement Eq. (1.84) with the equation for the gradient p(r, t)
=
∇q(r, t), which follows from Eq. (1.84), and the equation of continuity for
conserved quantity I(r, t):
 ∂
∂t + U(t, q) ∂
∂r

p(r, t) + ∂{U(t, q)p(r, t)}
∂q
p(r, t) = ∂Q(t, q)
∂q
p(r, t),
∂
∂tI(r, t) + ∂
∂r {U(t, q)I(r, t)} = 0,
I(r, 0) = I0(r).
(1.85)
From Eqs. (1.85) follows that

drI(r, t) =

drI0(r).
(1.86)
In terms of characteristic curves determined from the system of ordinary
diﬀerential equations, Eqs. (1.84) and (1.85) can be written in the form
d
dtr(t) = U (t, q) ,
d
dtq(t) = Q (t, q) , r (0) = r0, q (0) = q0 (r0) ,
d
dtp(t) = −∂{U (t, q) p(t)}
∂q
p(t) + ∂Q(t, q)
∂q
p(t),
p (0) = ∂q0(r0)
∂r0
,
d
dtI(t) = −∂{U (t, q) p(t)}
∂q
I(t),
I (0) = I0 (r0) .
(1.87)
Thus, the Lagrangian description considers the system (1.87) as the initial-
value problem. In this description, the two ﬁrst equations form the closed
system that deﬁnes characteristic curves .
Expressing now characteristic parameter r0 in terms of t and r, one can
write the solution to Eqs. (1.84) and (1.85) in the Eulerian description as
q (r, t) =

dr0q (t|r0) j (t|r0) δ (r (t|r0) −r) ,
I (r, t) =

dr0I (t|r0) j (t|r0) δ (r (t|r0) −r) .
(1.88)
The feature of the transition from the Lagrangian description (1.87) to the
Eulerian description (1.88) consists in the general appearance of ambiguities,
which yields discontinuous solutions. These ambiguities are related to the fact
that the divergence j (t|r0) = det
    
∂
∂r0k
ri (t|r0)
    can now vanish at certain
moments.
Quantities I(t|r0) and j(t|r0) are not independent. Indeed, integrating
I (r, t) in Eq. (1.88) over r and taking into account Eq. (1.86), we see that
there exists the evolution integral

46
1
Examples, Basic Problems, Peculiar Features of Solutions
j (t|r0) = I0 (r0)
I (t|r0),
(1.89)
from which follows that zero-valued divergence j(t|r0) is accompanied by the
inﬁnite value of conservative quantity I(t|r0).
It is obvious that all these results can be easily extended to the case in
which functions U(r, t, q) and Q(r, t, q) explicitly depend on spatial variable
r and Eq. (1.84) itself is the vector equation. As a particular physical example,
we consider the equation for the velocity ﬁeld V (r, t) of low-inertia particles
moving in the hydrodynamic ﬂow whose velocity ﬁeld is u(r, t) (see, e.g.,
[100])
 ∂
∂t + V (r, t) ∂
∂r

V (r, t) = −λ [V (r, t) −u(r, t)] .
(1.90)
We will assume this equation the phenomenological equation.
In the general case, the solution to Eq. (1.90) can be nonunique, it can
have discontinuities, etc. However, in the case of asymptotically small inertia
property of particles (parameter λ →∞), which is of our concern here, the
solution will be unique during reasonable temporal intervals. Note that, in the
right-hand side of Eq. (1.90), term F (r, t) = λV (r, t) linear in the velocity
ﬁeld V (r, t) is, according to the known Stokes formula, the resistance force
acting on a slowly moving particle. If we approximate the particle by the
sphere of radius a, parameter λ will be λ = 6πaη/mp, where η is the coeﬃcient
of dynamic viscosity and mp is the mass of the particle (see, e.g., [79,83]).
From Eq. (1.90) follows that velocity ﬁeld V (r, t) is the divergent ﬁeld even
if hydrodynamic ﬂow is nondivergent (div u(r, t) = 0). As a consequence,
particle number density n(r, t) in nondivergent hydrodynamic ﬂows, which
satisﬁes the linear equation of continuity
 ∂
∂t + ∂
∂rV (r, t)

n(r, t) = 0,
n(r, 0) = n0(r)
(1.91)
similar to Eq. (1.56), shows the cluster behavior.
For large parameters λ →∞(inertialess particles), we have
V (r, t) ≈u(r, t),
(1.92)
which means that particle number density n(r, t) shows no cluster behavior
in nondivergent hydrodynamic ﬂows.
The ﬁrst-order partial diﬀerential equation (1.90) (Eulerian description)
is equivalent to the system of ordinary diﬀerential characteristic equations
(Lagrangian description)
d
dtr(t) = V (r(t), t) ,
r(0) = r0,
d
dtV (t) = −λ [V (t) −u (r(t), t)] ,
V (0) = V 0(r0),
(1.93)

1.3
First-Order Partial Diﬀerential Equations
47
that describes the diﬀusion of a particle under the random external force and
linear friction and coincide with Eq. (1.14), page 9.
Conventional statistical description usually assumes that ﬂuctuations of
the hydrodynamic velocity ﬁeld are suﬃciently small. For this reason, we
can linearize the system of equations (1.93) and rewrite it in the form (for
simplicity, we assume that the mean ﬂow is absent and use the zero-valued
initial conditions)
d
dtr(t) = v(t),
d
dtv(t) = −λ [v(t) −f(t)] ,
r(0) = 0,
v(0) = 0,
(1.94)
the stochastic solution to which has the form
v(t) = λ
t

0
dτe−λ(t−τ)f(τ),
r(t) =
t

0
dτ

1 −e−λ(t−τ)
f(τ).
Note that the closed linear equation of the ﬁrst order in velocity v(t) is called
the Langevin equation.
Now, we turn back to Eq. (1.90). Setting λ = 0, we arrive at the equation
 ∂
∂t + V (r, t) ∂
∂r

V (r, t) = 0,
V (r, 0) = V 0(r)
(1.95)
called the Riemann equation. It describes free propagation of the nonlinear
Riemann wave. The solution to this equation obviously satisﬁes the transcen-
dental equation
V (r, t) = V 0 (r −tV (r, t)) .
Example. The One-Dimensional Riemann Equation
In the special case of the one-dimensional equation
 ∂
∂t + q(x, t) ∂
∂x

q(x, t) = 0,
q(x, 0) = q0(x),
(1.96)
the solution can be drawn in both implicit and explicit form. This equation
coincides with Eq. (1.84) at G(t, q) = 0, U(t, q) = q(x, t).
The method of characteristics applied to Eq. (1.96) gives
q(t|x0) = q0(x0),
x(t|x0) = x0 + tq0(x0),
so that the solution of Eq. (1.96) can be written in the form of the transcen-
dental equation
q(x, t) = q0 (x −tq(x, t))
from which follows an expression for spatial derivative

48
1
Examples, Basic Problems, Peculiar Features of Solutions
p(x, t) = ∂
∂xq(x, t) =
q′
0(x0)
1 + tq′
0(x0),
(1.97)
where
x0 = x −tq(x, t)
and
q′
0 (x0) =
d
dx0
q0 (x0) .
The function p(x, t) by itself satisﬁes here the equation
 ∂
∂t + q(x, t) ∂
∂x

p(x, t) = −p2(x, t),
p(x, 0) = p0(x) = q′
0(x).
(1.98)
For completeness, we give the equation of continuity for the density ﬁeld
ρ(x, t)
 ∂
∂t + q(x, t) ∂
∂x

ρ(x, t) = −p(x, t)ρ(x, t),
ρ(x, 0) = ρ0(x),
(1.99)
and its logarithm χ(x, t) = ln ρ(x, t)
 ∂
∂t + q(x, t) ∂
∂x

χ(x, t) = −p(x, t),
χ(x, 0) = χ0(x),
(1.100)
which are related to the Riemann equation (1.96). The solution to Eq. (1.96)
has the form
ρ(x, t) =
ρ0(x0)
1 + tp0(x0) =
ρ0(x −tq(x, t))
1 + tp0(x −tq(x, t)).
If q′
0(x0) < 0, then derivative ∂
∂xq(x, t) and solution of Eq. (1.96) becomes
discontinuous. For times prior to t0, the solution is unique and representable
in the form of a quadrature. To show this fact, we calculate the variational
derivative (for variational derivative deﬁnitions and the corresponding oper-
ation rules, see Chapter 2)
δq (x, t)
δq0 (x0) =
1
1 + tq′
0 (x0)δ (x −tq (x, t) −x0) .
Because q(x, t) = q0(x0) and x −tq0(x0) = x0, the argument of delta
function vanishes at x = F(x0, t) = x0 + tq0(x0). Consequently, we have
δq (x, t)
δq0 (x0) = δ(x −F(x0, t)) = 1
2π
∞

−∞
dk eik(x−x0)−iktq0(x0).
We can consider this equality as the functional equation in variable q0(x0).
Then, integrating this equation with the initial value

1.3
First-Order Partial Diﬀerential Equations
49
q (x, t)|q0(x0)=0 = 0,
in the functional space, we obtain the solution of the Riemann equation in
the form of the quadrature (see e.g., [121])
q (x, t) =
i
2πt
∞

−∞
dk
k
∞

−∞
dξeik(x−x0) 
e−iktq0(ξ) −1

.
The mentioned ambiguity can be eliminated by considering the Burgers
equation
∂
∂tq(x, t) + q(x, t) ∂
∂xq(x, t) = μ ∂2
∂x2 q(x, t),
q(x, 0) = q0(x)
(it includes the molecular viscosity and also can be solved in quadratures)
followed by the limit process μ →0.
♦
1.3.3
Boundary-Value Problems for Nonlinear
Ordinary Diﬀerential Equations
Note that, using the imbedding method (see Chapter 2), the boundary-value
problems for nonlinear ordinary diﬀerential equations also can be reduced to
quasilinear equations. This is the case, for example, for the nonlinear vector
boundary-value problem
d
dtx(t) = U (t, x(t)) ,
deﬁned on segment t ∈[0, T ] with boundary conditions
Gx(0) + Hx(T ) = v,
where G and H are constant matrices. Consider the solution of this problem
as a function of parameters T and v, i.e., x(t) = x(t; T, v). Then, function
R(T, v) = x(T ; T, v) as a function of parameters T and v is described by the
quasilinear vector equation [26,50]
 ∂
∂T + [HU (T, R (t, v))] ∂
∂v

R (T, v) = U (T, R (t, v))
with boundary condition for T →0
R (T, v)|T =0 = (G + H)−1 v,

50
1
Examples, Basic Problems, Peculiar Features of Solutions
and function x(t; T, v) itself satisﬁes the linear equation
∂xi(t; T, v)
∂T
= −HklUl (T, R(T, v)) ∂xi(t; T, v)
∂vk
with boundary condition
x(t; T, v)|T =t = R(t, v).
1.3.4
Nonlinear First-Order Partial Diﬀerential
Equations
In the general case, a nonlinear scalar ﬁrst-order partial diﬀerential equation
can be written in the form
∂
∂tq(r, t) + H (r, t, q, p) = 0,
q (r, 0) = q0 (r) ,
(1.101)
where p(r, t) = ∇q(r, t).
In terms of the Lagrangian description, this equation can be rewritten in
the form of the system of characteristic equations (see, e.g., [135]):
d
dtr(t|r0) = ∂
∂pH (r, t, q, p) ,
r(0|r0) = r0;
d
dtp(t|r0) = −
 ∂
∂r + p ∂
∂q

H (r, t, q, p) ,
p(0|r0) = p0(r0);
d
dtq(t|r0) =

p ∂
∂p −1

H (r, t, q, p) ,
q(0|r0) = q0(r0).
(1.102)
Now, we supplement Eq. (1.101) with the equation for the conservative
quantity I(r, t)
∂
∂tI(r, t) + ∂
∂r
∂H (r, t, q, p)
∂p
I(r, t)
	
= 0,
I (r, 0) = I0 (r) .
(1.103)
From Eq. (1.103) follows that

drI(r, t) =

drI0(r).
(1.104)
In the Lagrangian description, the corresponding quantity satisﬁes the equa-
tion
d
dtI(t|r0) = −∂2H (r, t, q, p)
∂r∂p
I(r, t),
I (0|r0) = I0 (r0) ,

1.3
First-Order Partial Diﬀerential Equations
51
so that the solution to Eq. (1.103) has the form
I (r, t) = I (t|r0 (t, r)) =

dr0I (t|r0) j (t|r0) δ (r (t|r0) −r) ,
(1.105)
where j (t|r0) = det ∥∂ri (t|r0) /∂r0j∥is the divergence (Jacobian).
Quantities I(t|r0) and j(t|r0) are related to each other. Indeed, substitut-
ing Eq. (1.105) for I(r, t) in Eq. (1.104), we see that there exists the evolution
integral
j (t|r0) = I0 (r0)
I (t|r0),
and Eq. (1.105) assumes the form
I (r, t) =

dr0I0 (r0) δ (r (t|r0) −r) .
Example. Hamilton–Jacobi Equation
In the case of function H(r, t, q, p) speciﬁed as
H (r, t, q, p) = 1
2p2 (r, t) + U (r, t) ,
Eqs. (1.102) correspond to the Hamilton equations
d
dtr(t) = p(t),
d
dtp(t) = −∂
∂rU (r, t) ,
d
dtq(t) = −U (r, t) ,
whereas Eq. (1.101) becomes the Hamilton–Jacobi equation
∂
∂tq(r, t) + U (r, t) + p2 (r, t) = 0,
q (r, 0) = q0 (r)
and function p(r, t) = ∇q(r, t) satisﬁes the quasilinear equation
 ∂
∂t + p (r, t) ∂
∂r

p (r, t) + ∂
∂rU (r, t) = 0,
p (r, 0) = ∇q0 (r) .
♦

52
1
Examples, Basic Problems, Peculiar Features of Solutions
1.4
Partial Diﬀerential Equations of Higher Orders
1.4.1
Fundamental Solutions of Wave Problems in
Free Space and Layered Media
Here we discuss several properties of fundamental solutions (Green’s func-
tions) of wave equations in free space and layered media following mono-
graph [50] and papers [31,52].
Free Space
First of all, we consider Green’s function of the one-dimensional Helmholtz
equation
d2
dx2 g(x; x0) + k2g(x; x0) = δ(x −x0).
(1.106)
The solution of Eq. (1.106) satisfying radiation condition for x →±∞has
the form
g(x; x0) = g(x −x0) =
1
2ikeik|x−x0|.
(1.107)
The modulus |x−x0| appeared in the right-hand side of Eq. (1.107) by virtue
of the fact that Eq. (1.106) is the equation of the second order in variable
x. However, if we ﬁx mutual order of observation points and source, then
Green’s function will satisfy the equality (for deﬁniteness, we assume that
x0 > x)
∂
∂x0
g(x −x0) = ikg(x −x0)
that, being supplemented with the initial condition
g(x −x0)|x0=x = g(0) =
1
2ik,
can be considered the ﬁrst-order diﬀerential equation.
Thus, the order of the equation for Green’s function decreases if source
and observation points obey certain order. This property is generic of wave
problems (factorization property of wave equations) and follows from the fact
that the wave radiated in direction x < x0 (or x > x0) travels in free space
without changing the direction.
In the general case, Green’s function satisﬁes the second-order operator
equation
 ∂2
∂x2 +!
M 2(η)
	
g(x −x0, η −η0) = δ(x −x0)g(η −η0),
(1.108)

1.4
Partial Diﬀerential Equations of Higher Orders
53
where operator !
M(η) acts on the temporal and other spatial variables denoted
by η. For example, operator !
M 2(η) in Eq. (1.106) is the number !
M 2(η) = k2.
Structurally, Green’s function is similar to Eq. (1.107),
g(x −x0, η −η0) = ei|x−x0|
M(η)g(0, η −η0) = ei|x−x0|
M(−η0)g(0, η −η0).
(1.109)
As a consequence, it can be described for x < x0 by the operator equation of
the ﬁrst order in variable x (or x0)
∂
∂x0
g(x −x0, η −η0) = −∂
∂xg(x −x0, η −η0)
= i!
M(η)g(x −x0, η −η0) = i!
M(−η0)g(x −x0, η −η0)
with the initial condition
g(x −x0, η −η0)|x0=x = g(0, η −η0) ≡g(η −η0).
For x > x0, the equations are similar.
The solution of Eq. (1.108) is continuous in x, but its derivative with res-
pect to x is discontinuous at the point of source location x = x0
∂
∂xg(x −x0, η −η0)

x=x0+0
−∂
∂xg(x −x0, η −η0)

x=x0−0
= δ(η −η0).
(1.110)
Substituting Eq. (1.109) in Eq. (1.110), we obtain the expression
2i!
M(η)g(0, η −η0) = δ(η −η0).
(1.111)
In the general case, operator !
M(η) can be considered as an integral op-
erator. Indeed, action of operator !
M(η) on arbitrary function f(η) is repre-
sentable in the form
!
M(η)f(η) =
∞

−∞
dξ !
M(η)δ(η −ξ)f(ξ) =
∞

−∞
dξ M(η −ξ)f(ξ),
where the kernel of the integral operator is deﬁned by the equality
M(η −ξ) = !
M(η)δ(η −ξ).
(1.112)
The inverse operator !
M −1(η) also can be introduced by the corresponding
choice of kernel M −1(η −ξ).
Applying
operator
!
M(η)
to
Eq. (1.111),
we
obtain,
according
to
Eq. (1.112), the kernel of the integral operator in the form

54
1
Examples, Basic Problems, Peculiar Features of Solutions
M(η −η0) = 2i!
M 2(η)g(0, η −η0).
(1.113)
The kernel of the inverse integral operator
M −1(η −η0) = !
M −1(η)δ(η −η0) = 2ig(0, η −η0).
(1.114)
is obtained by applying the inverse operator !
M −1(η) to Eq. (1.111).
Thus, kernels of integral operators !
M(η) and !
M −1(η) are expressed in
terms of the wave equation fundamental solution.
Consider now several speciﬁc wave problems.
1. We represent the Helmholtz equation in the form
 ∂2
∂x2 +ΔR+k2

g(x −x0, R −R0) = δ(x −x0)δ(R −R0),
(1.115)
where vector R denotes the coordinates in the plane perpendicular to the
x-axis.
The solution of Eq. (1.115) satisfying radiation conditions at inﬁnity has
the form
g(r −r0) = −
1
4π|r −r0|eik|r−r0|,
r = {x, R}.
Function g(r) can be represented in the integral form
g(x, R) =
1
8iπ2

dq

k2 −q2 exp
"
i

k2 −q2|x| + iqR
#
,
from which follows that operator !
M(R) has in this case the form
!
M(R) =

k2 + ΔR,
!
M(R0) =

k2 + ΔR0,
and the corresponding kernels of the integral operators are given, according
to Eq. (1.113) and (1.114), by the expressions
M(R) = 2i

k2+ΔR

g(R) = −
i
2πR2
 1
R−ik

eikR,
M −1(R) = −
i
2πReikR.
(1.116)
In the two-dimensional case, we have
g(r −r0) = −i
4H(1)
0
(k|r −r0|)
(r = {x, y}),
where H(1)
0
(k|r|) is the Hankel function. As a consequence, kernels of the
corresponding integral operators

1.4
Partial Diﬀerential Equations of Higher Orders
55
!
M(y) =
$
k2 + ∂2
∂y2 ,
!
M −1(y) =
1
$
k2 + ∂2
∂y2
are given by the expressions
M(y) =
k
2|y|H(1)
1 (k|y|),
M −1(y) = 1
2H(1)
0 (k|y|).
(1.117)
As we mentioned earlier, in the one-dimensional case, operators !
M and
!
M −1 are simply the numbers.
2. We represent the nonstationary wave equation in the form of Eq. (1.108)
 ∂2
∂x2 +ΔR−1
c2
∂2
∂t2

g(x−x0, R −R0, t−t0) = δ(x−x0)δ(R −R0)δ(t−t0).
(1.118)
In this case, operator !
M 2(η) is the diﬀerential operator
!
M 2(R, t) = ΔR −1
c2
∂2
∂t2 .
In the three-dimensional case, the solution of Eq. (1.118) satisfying radiation
conditions (the retarded solution) has the form
g(x, R, t) = −c
2πθ(t)δ

c2t2 −x2−R2
,
where θ(t) is the Heaviside step function . As a consequence, kernels of the
corresponding integral operators are given by the formulas
M(R, t) =
i
πctθ(t) ∂
∂tδ

c2t2−R2
,
M −1(R, t) = −ic
π θ(t)δ

c2t2 −R2
.
(1.119)
In the two-dimensional case,
g(x, y, t) = −c
2π
θ

ct −

x2 + y2


c2t2 −x2 −y2
= −c
2π
θ

c2t2 −x2 −y2

c2t2 −x2 −y2
and, consequently,
M(y, t) =
i
πctθ(t) ∂
∂t
θ

c2t2 −y2

c2t2 −y2 ,
M −1(y, t) = −ic
π
θ (ct −y)

c2t2 −y2 . (1.120)
In the one-dimensional case,
g(x, t) = −c
2θ(ct −|x|)

56
1
Examples, Basic Problems, Peculiar Features of Solutions
and, consequently,
M(t) = i
cδ′(t),
M −1(t) = −icθ(t).
(1.121)
Here, we considered certain properties of the fundamental solutions (Gre-
en’s functions) of wave equations describing the ﬁeld of the point source in
the unbounded free space. Note that the similar analysis for problems on the
point source ﬁeld in a ﬁnite layer of free or layered space diﬀers from the
above analysis only in insigniﬁcant details.
Layered Space
For a layered medium in which ε(x, y, z) = ε(z), wave equations can be
factorized because waves spread in plane (x, y) and do not scatter in the
backward direction.
We denote G(1)(z; z0) the point source ﬁeld in the one-dimensional space.
This function satisﬁes the equation
 d2
dz2 +k2(z)

G(1)(z; z0) = δ(z −z0),
whose solution can be represented in the operator form,
G(1)(z; z0) = %L−2(z)δ(z −z0),
where
%L2(z) = d2
dz2 + k2(z).
In the two-dimensional space, the wave ﬁeld of the point source is described
by Green’s function G(2)(x, z; z0) satisfying the equation
 ∂2
∂x2 +%L2(z)

G(2)(x, z; z0) = δ(x)δ(z −z0).
The solution of this equation has the form
G(2)(x, z; z0) = ei|x|L(z)G(2)(0, z; z0),
(1.122)
where function G(2)(0, z; z0) describes the wave ﬁeld on axis x = 0. The
discontinuity of derivative ∂
∂xG(2)(x, z; z0) at x = 0 is given by the expression
∂
∂xG(2)(x, z; z0)

x=+0
−∂
∂xG(2)(x, z; z0)

x=−0
= δ(z −z0).

1.4
Partial Diﬀerential Equations of Higher Orders
57
Being combined with Eq. (1.122), this discontinuity yields the equality
2i%L(z)G(2)(0, z; z0) = δ(z −z0),
(1.123)
from which follows that
G(2)(0, z; z0) = 1
2i
%L−1(z)δ(z −z0).
(1.124)
Applying now operator %L2(z) to Eq. (1.123), we obtain the equality
%L2(z)G(2)(0, z; z0) = 1
2i
%L(z)δ(z −z0).
(1.125)
We can consider operators %L(z) and %L−1(z) as the integral operators; in
this case, Eq. (1.125), (1.124) deﬁne the kernels of these operators. With this
fact in mind, we see that Eq. (1.124) is the nonlinear integral equation in
function G(2)(0, z; z0) describing the wave ﬁeld on axis x = 0,
∞

−∞
dξG(2)(0, z; ξ)G(2)(0, ξ; z0) = −1
4G(1)(z; z0),
where G(1)(z; z0) is Green’s function of the one-dimensional problem.
In the three-dimensional case, Green’s function of layered medium satisﬁes
the equation
 ∂2
∂x2 + ∂2
∂y2 +%L2(z)

G(3)(x, y, z; z0) = δ(x)δ(y)δ(z −z0).
We represent the solution to this equation in the form
G(3)(x, y, z; z0) = ei|x|L(y,z)G(3)(0, y, z; z0),
(1.126)
where %L(y, z) =
∂2
∂y2 + %L2(z) and function G(3)(0, y, z; z0) describes the
wave ﬁeld in plane (y, z). The condition of discontinuity of derivative
∂
∂xG(3)(x, y, z; z0) in plane x = 0 yields the operator equality
G(3)(0, y, z; z0) = 1
2i
%L−1(y, z)δ(y)δ(z −z0),
which can be rewritten in terms of the Hankel function of the ﬁrst kind
G(3)(0, y, z; z0) = −i
4H(1)
0

y%L(z)

δ(z −z0).
Using the Hankel function integral representation

58
1
Examples, Basic Problems, Peculiar Features of Solutions
H(1)
0 (βμ) = 1
iπ
∞

0
dx
x exp

iμ
2

x+β2
x
	
,
we obtain that function G(3)(0, y, z; z0) is related to the solution of the
parabolic equation
∂
∂tu(t, z; z0) = i
2k
%L2(z)u(t, z; z0),
u(0, z; z0) = δ(z −z0)
with respect to auxiliary parameter t by the quadrature
G(3)(0, y, z; z0) = −1
4π
∞

0
dt
t exp

i k
2ty2

u(t, z; z0),
or by the expression
G(3)(0, y, z; z0) = −1
4π
∞

0
dt
t exp

i k
2t

y2 + t2	
ψ(t, z; z0),
where function ψ(t, z; z0) is the solution to the parabolic equation
∂
∂tψ(t, z; z0) = i
2k
 ∂2
∂z2 + k2(z) −k2

ψ(t, z; z0),
ψ(0, z; z0) = δ(z −z0).
(1.127)
In view of arbitrary direction of the x-axis, we obtain that, for y > 0,
function
G(3)(x, y, z; z0) = G(3)(ρ, z; z0),
where ρ2 = x2 + y2, deﬁnes Green’s function in the whole of the space,
G(3)(x, y, z; z0) = −1
4π
∞

0
dt
t exp

i k
2t

x2 + y2 + t2	
ψ(t, z; z0).
(1.128)
Integrating Eq. (1.128) ﬁrst over y and x, we obtain the corresponding
integral representations of two- and one-dimensional Green’s functions
G(2)(x, z; z0) =
1
2ik
 k
2πi
1/2 ∞

0
dt
√
t exp

i k
2t

x2 + t2	
ψ(t, z; z0),
(1.129)

1.4
Partial Diﬀerential Equations of Higher Orders
59
G(1)(z; z0) =
1
2ik
∞

0
dt exp

ikt
2
	
ψ(t, z; z0).
(1.130)
1.4.2
Stationary Problems for Maxwell’s Equations
In the steady inhomogeneous medium, propagation of a monochromatic elec-
tromagnetic wave of frequency ω is described by Maxwell’s equations (see,
e.g., [130])
curl E(r) = ikH(r), curl H(r) = −ikε(r)E(r), div{ε(r)E(r)} = 0,
(1.131)
where E(r) and H(r) are the electric and nondivergent magnetic strengths,
ε(r) is the dielectric permittivity of the medium, and k = ω/c = 2π/λ is the
wave number (λ is the wavelength and c is the velocity of wave propagation).
Here, we assumed that magnetic permeability μ = 1, medium conductivity
σ = 0, and speciﬁed temporal factor e−iωt for all ﬁelds.
Equations (1.131) can be rewritten in the form of the equation closed in
the electric ﬁeld E(r)

Δ + k2ε(r)

E(r) = −∇(E(r)∇ln ε(r)) .
(1.132)
In this case, the magnetic ﬁeld H(r) is calculated by the equality
H(r) = 1
ik curl E(r).
(1.133)
We restrict ourselves with electromagnetic wave propagation in media with
weakly ﬂuctuating dielectric permittivity. We set
ε(r) = 1 + ε1(r),
where ε1(r) stands for small ﬂuctuations of dielectric permittivity (⟨ε1(r)⟩=
0). Smallness of ﬂuctuations ε1(r) assumes that
⟨|ε1(r)|⟩≪1 .
With this assumption, Eq. (1.132) can be rewritten in the simpliﬁed form

Δ + k2
E(r) = −k2ε1(r)E(r) −∇(E(r)∇ε1(r)) .
(1.134)
Using the theory of perturbations, Tatarskii [128] and Kravtsov [77] esti-
mated light wave depolarization at propagation paths of about 1 km in the
conditions of the actual atmosphere and showed that depolarization is very
small. In these conditions, we can neglect the last term in the right-hand side

60
1
Examples, Basic Problems, Peculiar Features of Solutions
of Eq. (1.134). As a result, the problem reduces in fact to the scalar Helmholtz
equation

Δ + k2
U(r) = −k2ε1(r)U(r).
(1.135)
For Eq. (1.135) be meaningful, one must formulate boundary conditions and
specify the source of radiation.
1.4.3
The Helmholtz Equation (Boundary-Value
Problem) and the Parabolic Equation of
Quasi-Optics (Waves in Randomly
Inhomogeneous Media)
Let the layer of inhomogeneous medium occupies spatial segment L0 < x < L
and let the point source is located at point (x0, R0), where R0 stands for the
coordinates in the plane perpendicular to the x-axis. In this case, the ﬁeld
inside the layer G (x, R; x0, R0) satisﬁes the equation for Green’s function
 ∂2
∂x2 + ΔR + k2 [1 + ε (x, R)]
	
G (x, R; x0, R0) = δ (x −x0) δ (R −R0) ,
(1.136)
where k is the wave number, ΔR = ∂2/∂R2, and ε1(r) = ε(x, R) is the
deviation of the refractive index (or dielectric permittivity) from unity.
Let ε(x, R) = 0 outside the layer. Then, the waveﬁeld outside the layer
satisﬁes the Helmholtz equation
 ∂2
∂x2 + ΔR + k2
	
G (x, R; x0, R0) = 0,
and continuity conditions for functions G and ∂G/∂x at layer boundaries.
Furthermore, the solution to Eq. (1.136) must satisfy the radiation conditions
for x →±∞.
The waveﬁeld outside the layer can obviously be represented in the form
G (x, R; x0, R0) =
⎧
⎨
⎩

dq T1 (q) exp

−i

k2 −q2 (x −L0) + iqR

, x ≤L0;

dq T2 (q) exp

i

k2 −q2 (x −L) + iqR

,
x ≥L.
Consequently, the boundary condition for Eq. (1.136) at x = L0 can be writ-
ten as
 ∂
∂x + i

k2 + ΔR

G (x, R; x0, R0)

x=L0
= 0.
(1.137)
Similarly, the boundary condition at x = L has the form

1.4
Partial Diﬀerential Equations of Higher Orders
61
 ∂
∂x −i

k2 + ΔR

G (x, R; x0, R0)

x=L
= 0.
(1.138)
In the case of space inﬁnite in coordinates R, operator
√
k2 + ΔR appeared
in Eqs. (1.137), (1.138) can be deﬁned in terms of the Fourier transform.
Alternatively, this operator can be also treated as the linear integral operator
whose kernel is expressed in terms of Green’s function for free space (see
sect. 1.4.1).
Thus, the ﬁeld of the point source in inhomogeneous medium is described
by the boundary-value problem (1.136)–(1.138). This problem is equivalent
to the integral equation
G (x, R; x0, R0) = g (x −x0, R −R0)
+
L

L0
dx′

dR′g

x −x′, R −R′
ε

x′, R′
G

x′, R′; x0, R0

,
(1.139)
where g(x, R) is Green’s function in the free space. In the three-dimensional
case, we have
g (x, R) = −1
4πreikr,
r =

x2 + R2.
The integral representation of this Green’s function is as follows
g (x, R) =

dq g (q) exp
"
i

k2 −q2 |x| + iqR
#
,
g (q) =
1
8iπ2
k2 −q2 .
(1.140)
It can be shown that operator √k2 + ΔR applied to arbitrary function
F(R) acts as the integral operator

k2 + ΔRF (R) =

dR′K

R −R′
F

R′
(1.141)
whose kernel is
K

R −R′
=

k2 + ΔRδ

R −R′
= 2i

k2 + ΔR

g

0, R −R′
.
(1.142)
The corresponding kernel of the inverse operator is
L

R −R′
=

k2 + ΔR
−1/2 δ

R −R′
= 2ig

0, R −R′
.
(1.143)
If the point source resides at the layer boundary x0 = L, then the waveﬁeld
inside the layer L0 < x < L satisﬁes the equation

62
1
Examples, Basic Problems, Peculiar Features of Solutions
 ∂2
∂x2 + ΔR + k2 [1 + ε (x, R)]
	
G (x, R; L, R0) = 0
(1.144)
with the boundary conditions following from conditions (1.137), (1.138)
 ∂
∂x + i

k2 + ΔR

G (x, R; L, R0)

x=L0
= 0,
 ∂
∂x −i

k2 + ΔR

G (x, R; L, R0)

x=L
= −δ(R −R0).
(1.145)
Boundary-value problem (1.144), (1.145) can be reduced to an equivalent
integral equation
G (x, R; L, R0) = g (x −L, R −R0)
+
L

L0
dx′

dR′g

x −x′, R −R′
ε

x′, R′
G

x′, R′; L, R0

(1.146)
coinciding with Eq. (1.139) for x0 = L.
If the wave u0(x, R) is incident on the layer from region x > L (in the
negative direction of the x-axis), then the waveﬁeld U(x, R) inside the layer
satisﬁes the Helmholtz equation
 ∂2
∂x2 + ΔR + k2 [1 + ε (x, R)]
	
U (x, R) = 0,
(1.147)
with the boundary conditions
 ∂
∂x + i

k2 + ΔR

U (x, R)

x=L0
= 0,
 ∂
∂x −i

k2 + ΔR

U (x, R)

x=L
= −2i

k2 + ΔRu0 (L, R) .
(1.148)
Similarly to the one-dimensional case, we can represent ﬁeld U(x, R) in
the form
U (x, R) = u1 (x, R) + u2 (x, R) ,
∂
∂xU (x, R) = −ik

k2 + ΔR {u1 (x, R) + u2 (x, R)} ,
(1.149)
where we replaced function U(x, R) with the sum of two functions u1(x, R)
and u2(x, R) corresponding to the waves propagating in the negative and
positive directions of the x-axis, respectively. These functions are related to
ﬁeld U(x, R) through the expressions

1.4
Partial Diﬀerential Equations of Higher Orders
63
u1 (x, R) =
i
2√k2 + ΔR
 ∂
∂x −i

k2 + ΔR

U(x, R),
u2 (x, R) = −
i
2√k2 + ΔR
 ∂
∂x + i

k2 + ΔR

U(x, R)
(1.150)
following from Eq. (1.149).
Diﬀerentiating Eq. (1.150) with respect to x and using Eq. (1.147), we ob-
tain the system of equations in functions u1(x, R) and u2(x, R); the corres-
ponding boundary conditions are derived from Eq. (1.148)
 ∂
∂x + i

k2 + ΔR

u1(x, R) = −
ik2
2√k2 + ΔR
{ε(x, R)U (x, R)} ,
 ∂
∂x −i

k2 + ΔR

u2(x, R) =
ik2
2√k2 + ΔR
{ε(x, R)U (x, R)} ,
u1(L, R) = u0(L, R),
u2(L0, R) = 0.
(1.151)
Function u2(x, R) describes the wave propagating in the direction inverse
to the direction of the incident wave, i.e., it describes the backscattered ﬁeld.
Neglecting the backscattering eﬀects, i.e., setting u2(x, R) = 0, we obtain
the generalized parabolic equation
 ∂
∂x + i

k2 + ΔR

U(x, R) = −
ik2
2
√
k2 + ΔR
{ε(x, R)U (x, R)} ,
U(L, R) = u0(L, R).
(1.152)
valid for waves scattered by arbitrary angles (less than π/2). In the case of
small-angle scattering (ΔR ≪k2), we represent ﬁeld U(x, R) in the form
U(x, R) = e−ik(x−L)u(x, R).
If we assume that the wave is incident on the inhomogeneous medium from
half-space x < 0 (i.e., if we replace L −x with x), then Eq. (1.152) reduces
to the parabolic equation of quasi-optics,
∂
∂xu(x, R) = i
2kΔRu(x, R) + ik
2 ε(x, R)u(x, R),
u(0, R) = u0(R),
(1.153)
which concerns the wave propagation in media with large-scale three-
dimensional inhomogeneities responsible for small-angle scattering. It was
successfully used in many problems on wave propagation in Earth’s atmos-
phere and ocean.

64
1
Examples, Basic Problems, Peculiar Features of Solutions
There is the waste literature on derivation and basing of both parabolic and
generalized parabolic equations. Appendix of Vol. 2 gives such a derivation in
terms of the imbedding method.
Introducing the amplitude-phase representation of the waveﬁeld in
Eq. (1.153) by the formula
u(x, R) = A(x, R)eiS(x,R),
we can write the equation for the waveﬁeld intensity I(x, R) = |u(x, R)|2 in
the form
∂
∂xI(x, R) + 1
k ∇R {∇RS(x, R)I(x, R)} = 0,
I(0, R) = I0(R).
(1.154)
From this equation follows that the power of a wave in plane x = const is
conserved in the general case of arbitrary incident wave beam:
E0 =

I(x, R)dR =

I0(R)dR.
Equation (1.154) coincides in form with Eq. (1.56), page 34. Consequently,
we can treat it as the equation of transfer of conservative tracer in the poten-
tial velocity ﬁeld. However, this tracer can be considered the passive tracer
only in the geometrical optics approximation, in which case the phase of the
wave, the transverse gradient of the phase
p(x, R) = 1
k ∇RS(x, R),
and the matrix of the phase second derivatives
uij(x, R) = 1
k
∂2
∂Ri∂Rj
S(x, R)
characterizing the curvature of the phase front S(x, R) = const satisfy the
closed system of equations [48,49]
∂
∂xS(x, R) + k
2 p2(x, R) = k
2 ε(x, R),
 ∂
∂x + p(x, R)∇R

p(x, R) = 1
2∇Rε(x, R),
 ∂
∂x + p(x, R)∇R

uij(x, R) + uik(x, R)ukj(x, R) = 1
2
∂2
∂Ri∂Rj ε(x, R).
(1.155)
In the general case, i.e., with the inclusion of diﬀraction eﬀects, this tracer
becomes the active tracer.
According to the material of the previous section, realizations of intensity
must show cluster behavior, which manifests itself in the appearance of caustic

1.4
Partial Diﬀerential Equations of Higher Orders
65
structures. An example demonstrating the appearance of waveﬁeld caustic
structures is given in Fig. 1.19, which is a fragment of the photo on the back of
the cover — the ﬂyleaf — of book [122] that shows the transverse section of the
laser beam propagating in turbulent atmosphere (see also papers [18,19,30]
and Chapter 20, page 272, Vol. 2 for the results of laboratory investigations
and simulations).
Fig. 1.19 Transverse section of a laser beam in turbulent medium
We mention that Eq. (1.153) with parameter x considered as time t coin-
cides in form with the Schr¨odinger equation with a random potential. In a
similar way, the nonlinear parabolic equation describing self-action of a har-
monic wave ﬁeld in multidimensional random media,
∂
∂xu(x, R) = i
2kΔRu(x, R) + ik
2 ε(x, R; I(x, R))u(x, R),
u(0, R) = u0(R)
(1.156)
coincides in form with the nonlinear Schr¨odinger equation. Consequently,
clustering of wave ﬁeld energy must occur in this case too, because Eq. (1.154)
is formally independent of the shape of function ε(x, R; I(x, R)). In partic-
ular, random initial condition u0(R) yields the caustic structure formation
even if ε(x, R) = 0. In this case, Eqs. (1.153) and (1.156) are simpliﬁed and
assume the form
∂
∂xu(x, R) = i
2kΔRu(x, R),
u(0, R) = u0(R)
(1.157)
that allows an analytical solution,
u(x, R) = exp
 ix
2k ΔR
	
u0(R) =
k
2πix

dR′ exp
 ik
2x(R −R′)2
	
u0(R′).
(1.158)

66
1
Examples, Basic Problems, Peculiar Features of Solutions
For the plane incident wave, the initial condition to Eqs. (1.157) has the
form |u0(R)| = 1, i.e., u0(R) = eiS0(R), where S0(R) is the ﬁeld of the ran-
dom initial phase. Here, the spatial ﬂuctuations in the initial distribution of
the phase are transformed into the caustic structure of the wave ﬁeld inten-
sity (random phase screen), which is well known and is regularly observed
both in water pools and shallow waters (see ﬁg. 1.20).
Fig. 1.20 Caustics in a pool and in a sea
For more detailed statistical description of this phenomenon, see Chap-
ter 20, page 271 in Volume 2.
Interest in equation (1.153), (1.156) and (1.157) is motivated by the prob-
lem of so-called rogue waves (see e.g. [39]). These waves, undoubtedly, arise
because of water mass clustering.
Replacing the imaginary-valued time it with t in the Schr¨odinger equation
with a random potential z(r, t), we arrive at the real diﬀusion equation
d
dtf(r, t) = z(r, t)f(r, t) + μfΔf(r, t),
f(r, 0) = f0(r),
(1.159)
where μf is the dynamic diﬀusivity coeﬃcient for the ﬁeld f(r, t). It is note-
worthy that Eq. (1.159), containing the terms responsible for random repro-
duction and diﬀusion, is also relevant for problems occurring in biology and
the kinetics of chemical and nuclear reactions (see, e.g., [108]).
Note that at the initial stage of diﬀusion, the solution of problem (1.159)
is given by function
f(r, t) = f0(r)ew(r,t),
(1.160)
where w(r, t) =
t

0
dτ z(r, τ).
Consider now the geometrical optics approximation (1.155) for parabo-
lic equation (1.153). In this approximation, the equation for the phase of
the wave is the Hamilton–Jacobi equation and the equation for the trans-
verse gradient of the phase (1.155) is the closed quasilinear ﬁrst-order partial

1.4
Partial Diﬀerential Equations of Higher Orders
67
diﬀerential equation, and we can solve it by the method of characteristics
(see, e.g., [135]). Equations for the characteristic curves (rays) have the form
d
dxR(x) = p(x),
d
dxp(x) = 1
2∇Rε(x, R),
(1.161)
and the waveﬁeld intensity and matrix of the phase second derivatives along
the characteristic curves will satisfy the equations
d
dxI(x) = −I(x)uii(x),
d
dxuij(x) + uik(x)ukj(x) = 1
2
∂2
∂Ri∂Rj
ε(x, R).
(1.162)
Equations (1.161) coincide in appearance with the equations for a particle
under random external forces in the absence of friction (1.14) and form the
system of the Hamilton equations .
In the two-dimensional case (R = y), Eqs. (1.161), (1.162) become signiﬁ-
cantly simpler and assume the form
d
dxy(x) = p(x),
d
dxp(x) = 1
2
∂
∂y ε(x, y),
d
dxI(x) = −I(x)u(x),
d
dxu(x) + u2(x) = 1
2
∂2
∂y2 ε(x, y).
(1.163)
The last equation for u(x) in Eqs. (1.163) is similar to Eq. (1.31) whose solu-
tion shows the singular behavior. The only diﬀerence between these equations
consists in the random term that has now more complicated structure. Nev-
ertheless, it is quite clear that solutions to stochastic problem (1.163) will
show the blow-up behavior; namely, function u(x) will reach minus inﬁnity
and intensity will reach plus inﬁnity at a ﬁnite distance. Such a behavior of a
waveﬁeld in randomly inhomogeneous media corresponds to random focusing,
i.e., to the formation of caustics, which means the appearance of points of
multivaluedness (and discontinuity) in the solutions to quasilinear equation
(1.155) for the transverse gradient of the waveﬁeld phase.
1.4.4
The Navier–Stokes Equation: Random Forces in
Hydrodynamic Theory of Turbulence
Consider now the turbulent motion model that assumes the presence of ex-
ternal forces f(r, t) acting on the ﬂuid. Such a model is evidently only imagi-
nary, because no actual analogues exist for these forces. However, assuming that
forces f(r, t) ensure an appreciable energy income only to large-scale velocity

68
1
Examples, Basic Problems, Peculiar Features of Solutions
components, we can expect that, within the concepts of the theory of local
isotropic turbulence, the imaginary nature of ﬁeld f(r, t) will only slightly af-
fect statistical properties of small-scale turbulent components [112]. Conse-
quently, this model is quite appropriate for describing small-scale properties
of turbulence.
Motion of an incompressible ﬂuid under external forces is governed by the
Navier–Stokes equation
 ∂
∂t + u(r, t) ∂
∂r

u(r, t) = −1
ρ0
∂
∂rp(r, t) + νΔu(r, t) + f(r, t),
∂
∂ru(r, t) = 0,
∂
∂r f(r, t) = 0.
(1.164)
Here, ρ0 is the density of the ﬂuid, ν is the kinematic viscosity, and pressure
ﬁeld p(x, t) is expressed in terms of the velocity ﬁeld at the same instant by
the relationship
p(r, t) = −ρ0

Δ−1 (r, r′) ∂2 (ui(r′, t)uj(r′, t))
∂r′
i∂r′
j
dr′,
(1.165)
where Δ−1(r, r′) is the integral operator inverse to the Laplace operator
(repeated indices assume summation).
Note that the linearized equation (1.164)
∂
∂tu(r, t) = νΔu(r, t) + f(r, t),
u(r, 0) = 0
(1.166)
describes the problem on turbulence degeneration for t →∞. The solution
to this problem can be drawn in an explicit form:
u(r, t) =
t

0
dτ eν(t−τ)Δf(r, τ) =
t

0
dτ eντΔ

dr′ δ(r −r′)f(r′, t −τ)
=
1
(2π)3
t

0
dτ eντΔ

dr′

dq eiq(r−r′)f(r′, t −τ)
=
1
(2π)3
t

0
dτ

dr′

dq e−ντq2eiqr′f(r −r′, t −τ)
=

dr′
t

0
dτ
(4πντ)3/2 exp

−r′2
4ντ
	
f(r −r′, t −τ).
(1.167)

1.4
Partial Diﬀerential Equations of Higher Orders
69
The linear equation (1.166) is an extension of the Langevin equation in
velocity (1.94), page 47 to random ﬁelds.
Neglecting the eﬀect of viscosity and external random forces in Eq. (1.164),
we arrive at the equation
∂
∂tu(r, t) + (u(r, t) · ∇) u(r, t) = −1
ρ∇p(r, t)
(1.168)
that describes dynamics of perfect ﬂuid and is called the Euler equation.
Using Eq. (1.77), page 40, this equation can be rewritten in terms of the
velocity ﬁeld vortex ω = curl u(r, t) as
∂
∂tω(r, t) = curl [u(r, t) × ω(r, t)] ,
ω(r, 0) = ω0(r),
(1.169)
or
 ∂
∂t + ∂
∂ru(r, t)

ω(r, t) =

ω(r, t) · ∂
∂r

ω(r, t),
ω(r, 0) = ω0(r).
(1.170)
These equations coincide with Eqs. (1.71) and (1.78), page 40 appeared in
the problem on the diﬀusion of magnetic ﬁeld.
However, this coincidence is only formal, because diﬀerent boundary con-
ditions to these problems results in drastically diﬀerent behaviors of ﬁelds
ω(r, t) and H(r, t) as functions of the velocity ﬁeld u(r, t).
If we substitute Eq. (1.165) in Eq. (1.164) to exclude the pressure ﬁeld,
then we obtain in the three-dimensional case that the Fourier transform of
the velocity ﬁeld with respect to spatial coordinates
%ui(k, t) =

drui(r, t)e−ikr,
ui(r, t) =
1
(2π)3

dk%ui(k, t)eikr,
(%u∗
i (k, t) = %ui(−k, t)) satisﬁes the nonlinear integro-diﬀerential equation
∂
∂t %ui (k, t) + i
2

dk1

dk2Λαβ
i
(k1, k2, k) %uα (k1, t) %uβ (k2, t)
−νk2%ui (k, t) = %fi (k, t) ,
(1.171)
where
Λαβ
i
(k1, k2, k) =
1
(2π)3 {kαΔiβ (k) + kβΔiα (k)} δ(k1 + k2 −k),
Δij (k) = δij −kikj
k2
(i, α, β = 1, 2, 3),
and %f(k, t) is the spatial Fourier harmonic of external forces,

70
1
Examples, Basic Problems, Peculiar Features of Solutions
%f(k, t) =

drf(r, t)e−ikr,
f(r, t) =
1
(2π)3

dk%f(k, t)eikr.
A speciﬁc feature of the three-dimensional hydrodynamic motions consists
in the fact that the absence of external forces and viscosity-driven eﬀects is
suﬃcient for energy conservation.
It appears convenient to describe the stationary turbulence in terms of the
space–time Fourier harmonics of the velocity ﬁeld
%ui(K) =

dx
∞

−∞
dtui(x, t)e−i(kx+ωt),
ui(x, t) =
1
(2π)4

dk
∞

−∞
dω%ui(K)ei(kx+ωt),
where K is the four-dimensional wave vector {k, ω} and ﬁeld
%u∗
i (K) = %ui(−K)
because ﬁeld ui(r, t) is real. In this case, we obtain the equation for compo-
nent %ui(K) by accomplishing the Fourier transformation of Eq. (1.171) with
respect to time:
(iω + νk2)ui (K)+ i
2

d4K1

d4K2Λαβ
i
(K1, K2, K) uα (K1) uβ (K) = fi (K) ,
(1.172)
where
Λαβ
i
(K1, K2, K) = 1
2π Λαβ
i
(k1, k2, k) δ(ω1 + ω2 −ω),
and %fi(K) are the space–time Fourier harmonics of external forces. The
obtained Eq. (1.172) is the integral (and not integro-diﬀerential) nonlinear
equation.
Waves on Sea Surface
Consider now the behavior of water boundary at the sea surface. In this
case, equations of hydrodynamics must be supplemented with the kinematic
boundary condition on the free sea surface z = ξ(R, t), where R denotes the
coordinates in the plane perpendicular to the vertical axis z (Fig. 1.21), which
has the form
d
dtξ(R, t) = wz(R, z; t)

z=ξ(R,t)
.
(1.173)

1.4
Partial Diﬀerential Equations of Higher Orders
71
Here, d
dtξ(R, t) is the total derivative of the displacement of water surface
and wz(R, z; t) is the vertical component of the hydrodynamic velocity of
medium.
−H
H(R)
R
0
ξ(R, t)
z
Fig. 1.21 Disturbance of water surface
If we denote the horizontal component of the velocity ﬁeld of the medium
as u(R, z, t), then we can consider condition (1.173) as a closed stochastic
quasilinear eqiation in the kinematic approximation, i.e., at given statistical
characteristics of the velocity ﬁeld {u(R, z, t), w(R, z; t)},
∂ξ(R, t)
∂t
+ ui(R, ξ(R, t), t)∂ξ(R, t)
∂Ri
= wz(R, ξ(R, t); t).
(1.174)
At the same time, Eq. (1.174) describes generation of waves on the sea
surface which are driven by the vertical component of the hydrodynamic
velocity ﬁeld. Diﬀerentiating Eq. (1.173) with respect to R, we obtain an
equation in the gradient of surface displacement pk(R, t) = ∂ξ(R, t)
∂Rk
, which
is a characteristic of surface slopes,
∂pk(R, t)
∂t
+

∂ui(R, z; t)
∂Rk

z=ξ(R,t)
+ ∂ui(R, ξ(R, t); t)
∂z
pk(R, t)

pi(R, t)
+ui(R, ξ(R, t), t)∂pi(R, t)
∂Rk
= ∂wz(R, z; t)
∂Rk

z=ξ(R,t)
+ ∂wz(R, ξ(R, t); t)
∂z
pk(R, t).
(1.175)
Note that the problem considered here has the second boundary condition
associated with inhomogeneity of bottom topography (see Fig. 1.21). In the
scope of the kinematic approximation, this boundary condition appears in the
functional form; namely, the variational derivatives of the solution ξ(R, t) and
p(R, t) assume the form

72
1
Examples, Basic Problems, Peculiar Features of Solutions
δξ(R, t)
δu(R′, z′, t′) ∼θ (z′ −H(R)) θ (t −t′) ,
δp(R, t)
δu(R′, z′, t′) ∼θ (z′ −H(R)) θ (t −t′) ,
(1.176)
where θ (z) is the Heaviside theta function.
Note that some huge waves—called also rogue waves—are occasionally ob-
served on the sea and ocean surface (see, e.g., [39]).
Figures 1.22 and 1.23 show three photos of an unusual narrow and lengthy
immobile wave of about 4-5-m height observed on June 11, 2006 near Kam-
chatka Paciﬁc coast at a distance of 1-1.5 km from the shoreline.
a
Fig. 1.22 Rogue wave. (a) Side view.
b
c
Fig. 1.23 Rogue wave. Front view of (b) the head of the wave and (c) the middle
of the wave.
The author of these photos, M.M. Sokolovsky described this phenomenon
in the following words: "It was certainly a strange wave, because it appeared
and then disappeared a few times. No waves were observed around this wave,
peace and quite."
What was this structure formation? Was it a rogue wave? No answer yet...

1.4
Partial Diﬀerential Equations of Higher Orders
73
Plane Motion under the Action of a Periodic Force
Consider now the two-dimensional motion of an incompressible viscous ﬂuid
U(r, t) = {u(r, t), v(r, t)} in plane r = {x, y} under the action of a spatially
periodic force directed along the x-axis,
fx(r, t) = γ sin py (γ > 0).
Such a ﬂow is usually called the Kolmogorov ﬂow (stream). The corresponding
motion is described by the system of equations
∂u
∂t + ∂u2
∂x + ∂uv
∂y = −1
ρ
∂P
∂x + νΔu + γ sin py,
∂v
∂t + ∂uv
∂x + ∂v2
∂y = −1
ρ
∂P
∂y + νΔv,
∂u
∂x + ∂v
∂y = 0,
(1.177)
where P(r, t) is the pressure, ρ is the density, and ν is the kinematic viscosity.
See papers [118,119] for the laboratory modeling of the Kolmogorov ﬂow.
The system of the Navier–Stokes and continuity equations (1.177) has
the steady-state solution that corresponds to the laminar ﬂow at constant
pressure along the x-axis and has the following form
us-s(r) =
γ
νp2 sin py,
vs-s(r) = 0,
Ps-s(r) = const.
(1.178)
Introducing scales of length p−1, velocity p−2υ−1γ, and time pνγ−1 and using
dimensionless variables, we reduce system (1.177) to the form
∂u
∂t + ∂u2
∂x + ∂uv
∂y = −∂P
∂x + 1
RΔu + 1
R sin y,
∂v
∂t + ∂uv
∂x + ∂v2
∂y = −∂P
∂y + 1
RΔv,
∂u
∂x + ∂v
∂y = 0,
(1.179)
where R =
γ
ν2p3 is the Reynolds number. In these variables, the steady-state
solution has the form
us-s(r) = sin y,
vs-s(r) = 0,
Ps-s(r) = const.

74
1
Examples, Basic Problems, Peculiar Features of Solutions
Introducing ﬂow function ψ(r, t) by the relationship
u(r, t) = ∂
∂yψ(r, t),
v(r, t) = −∂
∂xψ(r, t),
we obtain that it satisﬁes the equation
 ∂
∂t −Δ
R

Δψ −∂ψ
∂x
∂Δψ
∂y
+ ∂ψ
∂y
∂Δψ
∂x
= 1
R cos y
(1.180)
and
ψs-s(r) = −cos y.
It was shown [103,138] that, in the linear problem formulation, the steady-
state solution (1.178) corresponding to the laminar ﬂow is unstable with res-
pect to small disturbances for certain values of parameter R. These distur-
bances rapidly increase in time getting the energy from the ﬂow (1.178); this
causes the Reynolds stresses described by the nonlinear terms in Eq. (1.180)
to increase, which results in decreasing the amplitude of laminar ﬂow until
certain new steady-state ﬂow (called usually the secondary ﬂow) is formed.
Represent the hydrodynamic ﬁelds in the form
u(r, t) = U(y, t) + u(r, t),
v(r, t) = v(r, t),
P(r, t) = P0 + P(r, t),
ψ(r, t) = Ψ(y, t) + ψ(r, t).
Here, U(y, t) is the new proﬁle of the steady-state ﬂow to be determined to-
gether with the Reynolds stresses and the tilde denotes the corresponding
ultimate disturbances. Abiding by the cited works, we will consider distur-
bances harmonic in variable x with wavelength 2π/α (α > 0). The new ﬂow
proﬁle U(y, t) is the result of averaging with respect to x over distances of
about a wavelength.
One can easily see that, for α ≥1, the laminar ﬂow (1.178) is unique and
stable for all R [138] and instability can appear only for disturbanceswith α < 1.
According to the linear theory of stability, we will consider ﬁrst the non-
linear interaction only between the ﬁrst harmonic of disturbances and the
mean ﬂow and neglect the generation of higher harmonics and their mutual
interactions and interaction with the mean ﬂow.
We represent all disturbances in the form
ϕ(r, t) = ϕ(1)(y, t)eiαx + ϕ(−1)(y, t)e−iαx,

ϕ(r, t) = u(r, t), v(r, t), P(r, t), ψ(r, t)

,
where quantity ϕ(−1)(y, t) is the complex conjugated of ϕ(1)(y, t). Then, using
this representation in system (1.179) and eliminating quantities
P(r, t) and
u(r, t), we obtain the system of equations in mean ﬂow U(y, t) and distur-
bances v(1)(y, t) [14,43]:

1.4
Partial Diﬀerential Equations of Higher Orders
75
∂
∂tU + i
α

v(−1) ∂2v(1)
∂y2
−v(1) ∂2v(−1)
∂y2

= 1
R
∂2U
∂y2 + 1
R sin y,
 ∂
∂t −Δ
R

Δv(1) + iα

UΔv(1) −v(1) ∂2U
∂y2

= 0.
(1.181)
The second equation in system (1.181) is the known Orr–Sommerfeld equa-
tion. A similar system can be derived for the ﬂow function.
To examine the stability of the laminar regime (1.178), we set
U(y) = sin y
in the second equation of system (1.181) to obtain
 ∂
∂t −Δ
R

Δv(1)(y, t) + iα sin y[1 + Δ]v(1)(y, t) = 0.
(1.182)
Representing disturbances v(1)(y, t) in the form
v(1)(y, t) =
∞
&
n=−∞
v(1)
n eσt+iny
(1.183)
and substituting this representation in Eq. (1.182), we arrive at the recurrent
system in quantities v(1)
n
2
α

α2 + n2 
σ + α2 + n2
R

v(1)
n
+ v(1)
n−1

α2 −1 + (n −1)2
−v(1)
n+1

α2 −1 + (n + 1)2
= 0,
n = −∞, · · · , +∞.
(1.184)
The analysis of system (1.184) showed [103,138] that, under certain restric-
tions on wave number α and Reynolds number R, positive values of parame-
ter σ can exist, i.e., the solutions are unstable. The corresponding dispersion
equation in σ has the form of an inﬁnite continued fraction, and the critical
Reynolds number is Rcr =
√
2 for α →0. In other words, long-wave distur-
bances along the applied force appear to be most unstable. For this reason,
we can consider parameter α as a small parameter of the problem at hand and
integrate the Orr–Sommerfeld equation asymptotically. We will not dwell on
details of this solution. Note only that the components of eigenvector
"
v(1)
n
#
of problem (1.184) have diﬀerent orders of magnitude in parameter α. For
example, all components of vector
"
v(1)
n
#
with n = ±2, ±3, · · · will have
an order of α4 at least. As a result, we can conﬁne ourselves to the most
signiﬁcant harmonics with n = 0, ±1, which is, in essence, equivalent to the
Galerkin method with the trigonometric coordinate functions. In this case,

76
1
Examples, Basic Problems, Peculiar Features of Solutions
U(y, t) = U(t) sin y,
and the equation in v(1) assumes the form
 ∂
∂t −Δ
R

Δv(1)(y, t) + iαU(t) sin y[1 + Δ]v(1)(y, t) = 0.
Substituting the expansion
v(1)(y, t) =
1
&
n=−1
v(1)
n (t)einy
in Eqs. (1.181), we obtain that functions
U(t),
z0(t) = v(1)
0 (t),
z+(t) = v(1)
1 (t) + v(1)
−1(t),
z−(t) = v(1)
1 (t) −v(1)
−1(t)
2
satisfy the system of equations [14,43]
 d
dt + 1
R

U(t) = 1
R −4
αz0(t)z−(t),
 d
dt + α2
R

z0(t) = αU(t)z−(t),
 d
dt + 1
R

z−(t) = α
2 U(t)z0(t),
 d
dt + 1
R

z+(t) = 0.
(1.185)
Equation in quantity z+(t) is independent of other equations; as a con-
sequence, the corresponding disturbances can only decay with time. Three
remaining equations form the simplest three-component hydrodynamic-type
system (see Sect. 1.1.3, page 13). As was mentioned earlier, this system
is equivalent to the dynamic system describing the motion of a gyroscope
with anisotropic friction under the action of an external moment of force
relative to the unstable axis. An analysis of system (1.185) shows that, for
R < Rcr =
√
2, it yields the laminar regime with U = 1, zi = 0. For R >
√
2,
this regime becomes unstable, and new regime (the secondary ﬂow) is formed
that corresponds to the mean ﬂow proﬁle and steady-state Reynolds stresses
U =
√
2
R ,
4
αz0z−= R −
√
2
R2
,
z+ = 0,

v(1)
0
2
= R −
√
2
2
√
2R2 ,
v(1)
1
= α
√
2v(1)
0 ,
α ≪1,
R ≥
√
2.

1.4
Partial Diﬀerential Equations of Higher Orders
77
Turning back to the dimensional quantities, we obtain
U(y) =
√
2υp sin py,
⟨uv⟩= −
γ
p
R −
√
2
R
cos py.
(1.186)
Note that the amplitude of the steady-state mean ﬂow is independent of
the amplitude of exciting force. Moreover, quantity v(1)
0
can be both positive
and negative, depending on the signs of the amplitudes of small initial dis-
turbances.
Flow function of the steady-state ﬂow has the form
ψ1(x, y) = −
√
2
R cos y −2
αv(1)
0
√
2α sin y cos αx + sin αx

.
Figure 1.24 shows the current lines
α cos y +
√
2α sin y cos αx + sin αx = C
of ﬂow (1.186) at R = 2Rcr = 2
√
2 (v(1)
0
> 0). In addition, Fig. 1.24 shows
schematically the proﬁle of the mean ﬂow. As distinct from the laminar solu-
tion, systems of spatially periodic vortices appear here, and the tilt of longer
axes of these vortices is determined by the sign of the derivative of the mean
ﬂow proﬁle with respect to y.
Flow (1.186) was derived under the assumption that the nonlinear inter-
actions between diﬀerent harmonics of the disturbance are insigniﬁcant in
comparison with their interactions with the mean ﬂow. This assumption will
hold if ﬂow (1.186) is, in turn, stable with respect to small disturbances.
C = −1
C = −1
0
0
x
y
y
αx
π
π
π
−π
−π
−π
π
2
π
2
−π
2
−π
2
−3π
2
C = 0
C = 0
C = 0
C = 1 + α
C = 1 −α
2
√
3α
2
√
3α
2
√
3α
2
√
2α
2
√
2α
4√α
4√α
Fig. 1.24 Mean velocity and current lines of the secondary ﬂow at R = 2Rcr = 2
√
2
(v0 > 0)

78
1
Examples, Basic Problems, Peculiar Features of Solutions
The corresponding stability analysis can be carried out by the standard pro-
cedure, i.e., by linearizing the equation for ﬂow function (1.180) relative to
ﬂow (1.186) [43]. The analysis shows that ﬂow (1.186) is stable if we restrict
ourselves to the harmonics of the types same as in solution (1.186).
However, the solution appears to be unstable with respect to small-scale
disturbances. In this case, the nonlinear interaction of inﬁnitely small dis-
turbances governs the motion along with the interaction of the disturbances
with the mean ﬂow. Moreover, we cannot here content ourselves with a ﬁ-
nite number of harmonics in x-coordinate and need to consider the inﬁnite
series. As regards the harmonics in y-coordinate, we, as earlier, can limit the
consideration to the harmonics with n = 0, ±1.
Represent the ﬂow function in the form
ψ(x, y, t) = ψ−1(x, t)e−iy + ψ0(x, t) + ψ1(x, t)eiy
(ψ∗
1(x, t) = ψ−1(x, t)),
(1.187)
where ψi(x, t) are the periodic functions in x-coordinate with a period 2π/α.
Substituting Eq. (1.187) in Eq. (1.180), neglecting the terms of order α3 in
interactions of harmonics and the terms of order α2 in the dissipative terms
of harmonics ψ±1, and introducing new functions
ψ+(x, t) = ψ−1 + ψ1
2
,
ψ−(x, t) = ψ−1 −ψ1
2i
,
we obtain the system of equations
 ∂
∂t + 1
R

ψ+ −ψ−
∂ψ0
∂x = −1
2R,
 ∂
∂t + 1
R

ψ−+ ψ+
∂ψ0
∂x = 0,
 ∂
∂t −1
R
∂2
∂x2

ψ0 + 2

ψ−
∂ψ+
∂x −ψ+
∂ψ−
∂x

= 0.
This system of equations takes into account the whole inﬁnite series of har-
monics in x-coordinate and extends the system of gyroscopic-type equations
(1.185) to the inﬁnite-dimensional case. Its characteristic feature consists in
the absence of steady-state solutions periodic in x-coordinate (except the
solution corresponding to the laminar ﬂow).
Figure 1.25 shows an example of the Kolmogorov ﬂow obtained experi-
mentally (see, e.g., [13]).

1.4
Partial Diﬀerential Equations of Higher Orders
79
Fig. 1.25 An example of the Kolmogorov ﬂow obtained experimentally
a
H0
H0
ρ0
x
x
h(x)
h(x)
b
H1
H2
ρ1
ρ2
Fig. 1.26 Diagrammatic views of (a) one-layer and (b) two-layer models of hydro-
dynamic ﬂows
1.4.5
Equations of Geophysical Hydrodynamics
Consider now the description of hydrodynamic ﬂows on the rotating Earth in
the so-called quasi-geostrophic approximation [120]. In the simplest case of the
one-layer model, the incompressible ﬂuid ﬂow in the two-dimensional plane
R = (x, y) is described by the stream function that satisﬁes the equation
∂
∂tΔψ(R, t) + β0
∂
∂xψ(R, t) = J {Δψ(R, t) + h(R); ψ(R, t)} ,
ψ(R, 0) = ψ0(R),
(1.188)
where parameter β0 is the derivative of the local Coriolis parameter f0 with
respect to latitude, J{ψ, ϕ} is the Jacobian of two functions ψ(R, t) and
ϕ(R, t)
J {ψ(R, t); ϕ(R, t)} = ∂ψ(R, t)
∂x
∂ϕ(R, t)
∂y
−∂ϕ(R, t)
∂x
∂ψ(R, t)
∂y
,
and function h(R) = f0h(R)/H0 is the deviation of bottom topography h(R)
relative to its average thickness H0 (Fig. 1.26a).

80
1
Examples, Basic Problems, Peculiar Features of Solutions
The velocity ﬁeld is expressed in terms of the stream function by the
relationship
v (R, t) =

−∂ψ (R, t)
∂y
, ∂ψ (R, t)
∂x

.
Note that, under the neglect of Earth’s rotation and eﬀects of underly-
ing surface topography, Eq. (1.188) reduces to the standard equation of two-
dimensional hydrodynamics (see, e.g., [83])
∂
∂tΔψ(R, t) + J {Δψ(R, t)); ψ(R, t)} ,
ψ(R, 0) = ψ0(R).
(1.189)
Equation (1.188) describes the barotropic motion of a ﬂuid. In the more
general case of baroclinic motions, investigation is usually carried out within
the framework of the two-layer model of hydrodynamic ﬂows described by
the system of equations [120]
∂
∂t [Δψ1 −α1F (ψ1 −ψ2)] + β0 ∂
∂xψ1 = J {Δψ1 −α1F (ψ1 −ψ2) ; ψ1} ,
∂
∂t [Δψ2 −α2F (ψ2 −ψ1)] + β0 ∂
∂xψ2 = J {Δψ2 −α2F (ψ2 −ψ1) + f0α2h; ψ2} ,
(1.190)
where additional parameters
F = f 2
0 ρ/g(Δρ),
Δρ/ρ = (ρ2 −ρ1)/ρ0 > 0
are introduced and α1 = 1/H1 and α2 = 1/H2 are the inverse thicknesses of
layers (Fig. 1.26b).
Among the particular cases of Eqs. (1.188), (1.190) are the equations ob-
tained by neglecting Earth’s rotation (two-dimensional hydrodynamics) but
with allowance for bottom topography and the linearized quasi-geostrophic
equations similar to Eq. (1.55) that describe the eﬀect of topography on pro-
pagation of the Rossby waves.

Chapter 2
Solution Dependence on Problem Type,
Medium Parameters, and Initial Data
Below, we considered a number of dynamic systems described by both or-
dinary and partial diﬀerential equations. Many applications concerning re-
search of statistical characteristics of the solutions to these equations require
the knowledge of the solution dependence (generally, in the functional form)
on the medium parameters appeared in the equation as coeﬃcients and initial
values. Some properties appear common of all such dependencies, and two of
them are of special interest in the context of statistical descriptions. We illus-
trate these dependencies by the example of the simplest problem, namely, the
system of ordinary diﬀerential equations (1.1) that describes particle dyna-
mics under random velocity ﬁeld and which we reformulate in the form of
the nonlinear integral equation
r(t) = r0 +
t

t0
dτU(r(τ), τ).
(2.1)
The solution to Eq. (2.1) functionally depends on vector ﬁeld U(r′, τ) and
initial values r0, t0.
2.1
Functional Representation of Problem Solution
2.1.1
Variational (Functional) Derivatives
Recall ﬁrst the general deﬁnition of a functional. One says that a functional
is given if a rule is ﬁxed that associates a number to every function from
certain function family. Below, we give some examples of functionals:
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
81
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_2, c
⃝Springer International Publishing Switzerland 2015

82
2
Solution Dependence on Problem Type, Medium Parameters
(a)
F[ϕ(τ)] =
t2

t1
dτa(τ)ϕ(τ),
where a(t) is the given (ﬁxed) function and limits t1 and t2 can be both ﬁnite
and inﬁnite. This is the linear functional.
(b)
F[ϕ(τ)] =
t2

t1
t2

t1
dτ1dτ2B(τ1, τ2)ϕ(τ1)ϕ(τ2),
where B(t1, t2) is the given (ﬁxed) function. This is the quadratic functional.
(c)
F[ϕ(τ)] = f (Φ[ϕ(τ)]) ,
where f(x) is the given function and quantity Φ[ϕ(τ)] is the functional.
ϕ(τ)
ϕ(τ) + δϕ(τ)
ϕ(τ)
t −Δτ/2
t + Δτ/2
τ
t
Fig. 2.1 To deﬁnition of variational derivative
Estimate the diﬀerence between the values of a functional calculated for
functions ϕ(τ) and ϕ(τ) + δϕ(τ) for t −Δτ
2
< τ < t + Δτ
2
(see Fig. 2.1).
The variation of a functional is deﬁned as the linear (in δϕ(τ)) portion of
the diﬀerence
δF[ϕ(τ)] = {F [ϕ(τ) + δϕ(τ)] −F[ϕ(τ)]} .
The limit

2.1
Functional Representation of Problem Solution
83
δF[ϕ(τ)]
δϕ(t)dt = lim
Δτ→0
δF[ϕ(τ)]

Δτ
dτδϕ(τ)
(2.2)
is called the variational (or functional) derivative (see, e.g., [112]).
For short, we will use notation δF[ϕ(τ)]
δϕ(t)
instead of δF[ϕ(τ)]
δϕ(t)dt .
Note that, if we use function δϕ(τ) = αδ(τ), where δ(τ) is the Dirac
delta function, then Eq. (2.2) can be represented in the form of the ordinary
derivative
δF[ϕ(τ)]
δϕ(t)
= lim
α→0
d
dαF[ϕ(τ) + αδ(τ −t)].
The variational derivative of functional F[ϕ(τ)] is again the functional of
ϕ(τ), which depends additionally on point t as a parameter. As a result, this
variational derivative will have two types of derivatives; one can diﬀerentiate
it in the ordinary sense with respect to parameter t and in the functional sense
with respect to ϕ(τ) at point τ = t′ thus obtaining the second variational
derivative of the initial functional
δ2F[ϕ(τ)]
δϕ(t′)δϕ(t) =
δ
δϕ(t′)
δF[ϕ(τ)]
δϕ(t)

.
The second variational derivative will now be the functional of ϕ(τ) depen-
dent on two points t and t′, and so forth.
Determine the variational derivatives of functionals (a), (b ), and (c).
In the case (a), we have
δF[ϕ(τ)] = F[ϕ(τ) + δϕ(τ)] −F[ϕ(τ)] =
t+ Δτ
2

t−Δτ
2
dτa(τ)δϕ(τ).
If function a(t) is continuous on segment Δτ, then, by the average theorem,
δF[ϕ(τ)] = a(t′)

Δτ
dτδϕ(τ),
where point t′ belongs to segment

t −Δτ
2 , t + Δτ
2

. Consequently,
δF[ϕ(τ)]
δϕ(t)
= lim
Δτ→0 a(t′) = a(t).
(2.3)
In the case (b), we obtain similarly

84
2
Solution Dependence on Problem Type, Medium Parameters
δF[ϕ(τ)]
δϕ(t)
=
t2

t1
dτ [B(τ,t)+B(t, τ)] ϕ(τ)
(t1 < t < t2).
Note that function B(τ1, τ2) can always be assumed a symmetric function of
its arguments here.
In the case (c), we have
F[ϕ(τ) + δϕ(τ)] = f (Φ[ϕ(τ)]) + ∂f(Φ[ϕ(τ)])
∂Φ
δΦ[ϕ(τ)] + · · ·
= F[ϕ(τ)] + ∂f(Φ[ϕ(τ)])
∂Φ
δΦ[ϕ(τ)] + · · ·
and, consequently,
δ
δϕ(t)f (Φ[ϕ(τ)]) = ∂f(Φ[ϕ(τ)])
∂Φ
δ
δϕ(t)Φ[ϕ(τ)].
(2.4)
Consider now functional Φ[ϕ(τ)] = F1[ϕ(τ)]F2[ϕ(τ)]. We have
δΦ[ϕ(τ)] = F 1[ϕ(τ) + δϕ(τ)]F 2[ϕ(τ) + δϕ(τ)]−F 1[ϕ(τ)]F 2[ϕ(τ)]
= F1[ϕ(τ)]δF2[ϕ(τ)] + F2[ϕ(τ)]δF1[ϕ(τ)]
and, consequently,
δ
δϕ(t)F1[ϕ(τ)]F2[ϕ(τ)] = F1[ϕ(τ)]
δ
δϕ(t)F2[ϕ(τ)] + F2[ϕ(τ)]
δ
δϕ(t)F1[ϕ(τ)].
(2.5)
We can deﬁne the expression for the variational derivative of functional
ϕ(τ0) with respect to function ϕ(t) by the formal relationship
δϕ(τ0)
δϕ(t) = δ(τ0 −t).
(2.6)
Formula (2.6) can be proved, for example, by considering the linear functional
of the form
F[ϕ(τ)] =
1
√
2πσ
∞

−∞
dτϕ(τ) exp

−(τ −τ0)2
2σ2
	
.
(2.7)
According to Eq. (2.3), the variational derivative of this functional has the
form
δ
δϕ(t)F[ϕ(τ)] =
1
√
2πσ exp
'
−(t −τ0)2
2σ2
(
.
(2.8)

2.1
Functional Representation of Problem Solution
85
Performing now formal limit process σ →0 in Eq. (2.7) and (2.8), we obtain
the desired formula (2.6). Moreover,
δF[ϕ(τ)]
δϕ(t)
= ∂F[ϕ(τ)]
∂ϕ(τ)
δϕ(τ)
δϕ(t) = ∂F[ϕ(τ)]
∂ϕ(τ) δ(τ −t).
Formula (2.6) is very convenient for functional diﬀerentiation of functionals
explicitly dependent on ϕ(τ). Indeed, for the quadratic functional (b), we
have
δ
δϕ(t)
t2

t1
t2

t1
dτ1dτ2B(τ1, τ2)ϕ(τ1)ϕ(τ2)
(2.5)
=
t2

t1
t2

t1
dτ1dτ2B(τ1, τ2)
δϕ(τ1)
δϕ(t) ϕ(τ2) + ϕ(τ1)δϕ(τ2)
δϕ(t)

(2.6)
=
t2

t1
dτ [B(t, τ)+B(τ, t)] ϕ(τ)
(t1 < t < t2).
Consider the functional
F[ϕ(τ)] =
t2

t1
dτL

τ, ϕ(τ), dϕ(τ)
dτ

as another example. In this case,
δ
δϕ(t) F [ϕ(τ)]
(2.4)
=
t2

t1
dτ
⎡
⎢⎢⎣
∂L

τ, ϕ(τ), dϕ(τ)
dτ
	
∂ϕ(τ)
+
∂L

τ, ϕ(τ), dϕ(τ)
dτ
	
∂˙ϕ(τ)
d
dτ
⎤
⎥⎥⎦
δϕ(τ)
δϕ(t)
(2.6)
=
t2

t1
dτ
⎡
⎢⎢⎣
∂L

τ, ϕ(τ), dϕ(τ)
dτ
	
∂ϕ(τ)
+
∂L

τ, ϕ(τ), dϕ(τ)
dτ
	
∂˙ϕ(τ)
d
dτ
⎤
⎥⎥⎦δ(τ −t)
=

−d
dt
∂
∂˙ϕ(t) +
∂
∂ϕ(t)
	
L

t, ϕ(t), dϕ(t)
dt
	
,
where ˙ϕ(t) = d
dtϕ(t) if point t belongs to interval (t1, t2).
Just as a function can be expanded in the Taylor series, a functional
F[ϕ(τ) + η(τ)] can be expanded in the functional Taylor series in function
η(τ)

86
2
Solution Dependence on Problem Type, Medium Parameters
F[ϕ(τ) + η(τ)] = F[ϕ(τ)] +
∞

−∞
dtδF[ϕ(τ)]
δϕ(t)
η(t)
+ 1
2!
∞

−∞
∞

−∞
dt1dt2
δ2F[ϕ(τ)]
δϕ(t1)δϕ(t2)η(t1)η(t2) + · · · .
(2.9)
Note that the operator expression
1 +
∞

−∞
dtη(t)
δ
δϕ(t) + 1
2!
∞

−∞
∞

−∞
dt1dt2η(t1)η(t2)
δ2
δϕ(t1)δϕ(t2) + · · ·
= 1 +
∞

−∞
dtη(t)
δ
δϕ(t) + 1
2!
⎡
⎣
∞

−∞
dtη(t)
δ
δϕ(t)
⎤
⎦
2
+ · · ·
(2.10)
can be written shortly as the operator
exp
⎧
⎨
⎩
∞

−∞
dtη(t)
δ
δϕ(t)
⎫
⎬
⎭,
(2.11)
whose action should be treated precisely in the sense of expansion (2.10).
Using this operator, we can rewrite Eq. (2.9) in the form
F[ϕ(τ) + η(τ)] = e
∞

−∞
dtη(t)
δ
δϕ(t) F[ϕ(τ)],
(2.12)
which enables us to interpret operator (2.11) as the functional shift operator.
Consider now functional F[t; ϕ(τ)] dependent on parameter t. We can
diﬀerentiate this functional with respect to t and determine its variational
derivative with respect to ϕ(t′), as well. One can easily see that these opera-
tions commute, i.e., the equality
∂
∂t
δF[t; ϕ(τ)]
δϕ(t′)
=
δ
δϕ(t′)
∂F[t; ϕ(τ)]
∂t
(2.13)
holds. If the domain of τ is independent of t, the validity of Eq. (2.13) is
obvious. Otherwise, for example, for functionals F[t; ϕ(τ)] with 0 ≤τ ≤t,
the validity of Eq. (2.13) can be checked on by expanding functional F[t; ϕ(τ)]
in the functional Taylor series.

2.1
Functional Representation of Problem Solution
87
2.1.2
Principle of Dynamic Causality
Vary Eq. (2.1) with respect to ﬁeld U (r, t). Assuming that the initial position
r0 is independent of ﬁeld U, we obtain the equation linear in variational
derivative (the linear variational diﬀerential equation)
δri (t)
δUj (r, t′) =δijδ (r −r (t′)) θ (t′ −t0) θ (t −t′)+
t

t0
dτ ∂Ui (r (τ) , τ)
∂rk
δrk (τ)
δUj (r, t′),
(2.14)
where δ (r −r′) is the Dirac delta function, and θ (z) is the Heaviside step
function. From Eq. (2.14) follows that
δri (t)
δUj (r, t′) = 0
for t′ > t or t′ < t0,
(2.15)
which means that solution r(t) to the dynamic problem (2.1) as a
functional of ﬁeld U (r, t′) depends on U (r, t′) only for t0
<
t′
<
t. Consequently, function r(t) will remain unchanged if ﬁeld U (r, t′)
varies
outside
the
interval
(t0, t),
i.e.,
for
t′
<
t0
or
t′
>
t.
We will call condition (2.15) the dynamic causality condition.
Taking this condition into account, we can rewrite Eq. (2.14) in the form
δri (t)
δUj (r, t′) =δijδ (r −r (t′)) θ (t′ −t0) θ (t −t′)+
t

t′
dτ ∂Ui (r (τ) , τ)
∂rk
δrk (τ)
δUj (r, t′).
(2.16)
As a consequence, limit t →t′ + 0 yields the equality
δri (t)
δUj (r, t′)

t=t′+0
= δijδ (r −r (t′)) .
(2.17)
Integral equation (2.16) in variational derivative is obviously equivalent to
the linear diﬀerential equation with the initial value
∂
∂t

δri (t)
δUj (r, t′)

= ∂Ui (r (t) , t)
∂rk

δrk (t)
δUj (r, t′)

,
δri (t)
δUj (r, t′)

t=t′
= δijδ (r −r (t′)) .
(2.18)
The dynamic causality condition is the general property of problems des-
cribed by diﬀerential equations with initial values. The boundary-value prob-
lems possess no such property. Indeed, in the case of problem (1.33), (1.34),
page 23 that describes propagation of a plane wave in a layer of inhomo-
geneous medium, waveﬁeld u(x) at point x and reﬂection and transmission

88
2
Solution Dependence on Problem Type, Medium Parameters
coeﬃcients depend functionally on function ε(x) for all x of layer (L0, L).
However, using the imbedding method, we can convert this problem into the
initial-value problem with respect to an auxiliary parameter L and make use
the causality property in terms of the equations of the imbedding method.
2.2
Solution Dependence on Problem’s Parameters
2.2.1
Solution Dependence on Initial Data
Here, we will use the vertical bar symbol to isolate the dependence of solution
r(t) to Eq. (2.1) on the initial parameters r0 and t0:
r (t) = r (t|r0, t0) , r0 = r (t0|r0, t0) .
Let us diﬀerentiate Eq. (2.1) with respect to parameters r0k and t0. As
a result, we obtain linear equations for Jacobi’s matrix
∂
∂r0k
ri (t|r0, t0) and
quantity
∂
∂t0
ri (t|r0, t0)
∂ri (t|r0, t0)
∂r0k
= δik +
t

t0
dτ ∂Ui (r (τ) , τ)
∂rj
∂rj (τ|r0, t0)
∂r0k
,
∂ri (t|r0, t0)
∂t0
= −Ui (r0(t0), t0) +
t

t0
dτ ∂Ui (r (τ) , τ)
∂rj
∂rj (τ|r0, t0)
∂t0
.
(2.19)
Multiplying now the ﬁrst of these equations by Uk (r0 (t) , t), summing over
index k, adding the result to the second equation, and introducing the vector
function
Fi (t|r0, t0) =
 ∂
∂t0
+ U (r0, t0) ∂
∂r0

ri (t|r0, t0) ,
we obtain that this function satisﬁes the linear homogeneous equation
Fi (t|r0, t0) =
t

t0
dτ ∂Ui (r (τ) , τ)
∂rk
Fk (τ|r0, t0) .
(2.20)
Diﬀerentiating this equation with respect to time, we arrive at the ordinary
diﬀerential equation
∂
∂tFi (t|r0, t0) = ∂Ui (r (t) , t)
∂rk
Fk (t|r0, t0)

2.2
Solution Dependence on Problem’s Parameters
89
with the initial condition Fi (t0|r0, t0) = 0 at t = t0, which follows from
Eq. (2.20); as a consequence, we have Fi (t|r0, t0) ≡0. Therefore, we obtain
the equality
 ∂
∂t0
+ U (r0, t0) ∂
∂r0

ri (t|r0, t0) = 0,
(2.21)
which can be considered as the linear partial diﬀerential equation with the
derivatives with respect to variables r0, t0 and the initial value at t0 = t
r (t|r0, t) = r0.
(2.22)
The variable t appears now in problem (2.21), (2.22) as a parameter.
Equation (2.21) is solved using the time direction inverse to that used in
solving problem (1.1), page 3; for this reason, we will call it the backward
equation .
Equation (2.21) with the initial condition (2.22) can be rewritten as the
integral equation
r(t|r0, t0) = r0 +
t

t0
dτ

U (r0, τ)
∂
∂r0

r(t|r0, τ).
(2.23)
Varying now Eq. (2.23) with respect to function Uj(r′, t′), we obtain the
integral equation
δri(t|r0, t0)
δUj(r′, t′) = δ(r0 −r′)θ(t′ −t0)θ(t −t′)∂ri(t|r0, t′)
∂rj0
+
t

t0
dτ

U (r0, τ)
∂
∂r0
 δri(t|r0, τ)
δUj(r′, t′) ,
(2.24)
from which follows that
δri (t|r0, t0)
δUj (r′, t′) = 0,
if t′ > t
or
t′ < t0,
which means that function r(t|r0, t0) also possesses the property of dynamic
causality with respect to parameter t0 (2.15)—which is quite natural—and
Eq. (2.24) can be rewritten in the form (for t0 < t′ < t)
δri(t|r0, t0)
δUj(r′, t′) = δ(r0 −r′)∂ri(t|r0, t′)
∂rj0
+
t′

t0
dτ

U (r0, τ)
∂
∂r0
 δri(t|r0, τ)
δUj(r′, t′) .
(2.25)
Setting now t′ →t0 + 0, we obtain the equality

90
2
Solution Dependence on Problem Type, Medium Parameters
δri (t|r0, t0)
δUj (r, t′)

t′=t0+0
= δ (r0 −r) ∂ri (t|r0, t0)
∂r0j
.
(2.26)
2.2.2
Imbedding Method for Boundary-Value
Problems
Consider ﬁrst boundary-value problems formulated in terms of ordinary dif-
ferential equations. The imbedding method (or invariant imbedding method,
as it is usually called in mathematical literature) oﬀers a possibility of re-
ducing boundary-value problems at hand to the evolution-type initial-value
problems possessing the property of dynamic causality with respect to an
auxiliary parameter.
The idea of this method was ﬁrst suggested by V.A. Ambartsumyan (the
so-called Ambartsumyan invariance principle)
[2–4] for solving the equa-
tions of linear theory of radiative transfer. Further, mathematicians grasped
this idea and used it to convert boundary-value (nonlinear, in the general
case) problems into evolution-type initial-value problems that are more con-
venient for simulations. Several monographs (see, e.g., [8, 10, 38]) deal with
this method and consider both physical and computational aspects.
Consider the dynamic system described in terms of the system of ordinary
diﬀerential equations
d
dtx(t) = F (t, x(t)) ,
(2.27)
deﬁned on segment t ∈[0, T ] with the boundary conditions
gx(0) + hx(T ) = v,
(2.28)
where g and h are the constant matrixes.
Dynamic problem (2.27), (2.28) possesses no dynamic causality property,
which means that the solution x(t) to this problem at instant t functionally
depends on external forces F (τ, x(τ)) for all 0 ≤τ ≤T . Moreover, even
boundary values x(0) and x(T ) are functionals of ﬁeld F (τ, x(τ)). The ab-
sence of dynamic causality in problem (2.27), (2.28) prevents us from using
the known statistical methods of analyzing statistical characteristics of the
solution to Eq. (2.27) if external force functional F (t, x) is the random space-
and time-domain ﬁeld. Introducing the one-time probability density P(t; x)
of the solution to Eq. (2.27), we can easily see that condition (2.28) is insuﬃ-
cient for determining the value of this probability at any point. The boundary
condition imposes only certain functional restriction.
Note that the solution to problem (2.27), (2.28) parametrically depends on
T and v, i.e., x(t) = x(t; T, v). Abiding by paper [26], we introduce functions
R(T, v) = x(T ; T, v),
S(T, v) = x(0; T, v)

2.2
Solution Dependence on Problem’s Parameters
91
that describe the boundary values of the solution to Eq. (2.27).
Diﬀerentiate Eq. (2.27) with respect to T and v. We obtain two linear
equations in the corresponding derivatives
d
dt
∂xi(t; T, v)
∂T
= ∂Fi(t, x)
∂xl
∂xl(t; T, v)
∂T
,
d
dt
∂xi(t; T, v)
∂vk
= ∂Fi(t, x)
∂xl
∂xl(t; T, v)
∂vk
.
(2.29)
These equations are identical in form; consequently, we can expect that their
solutions are related by the linear expression
∂xi(t; T, v)
∂T
= λk(T, v)∂xi(t; T, v)
∂vk
(2.30)
if vector quantity λ(T, v) is such that boundary conditions (2.28) are satisﬁed
and the solution is unique. To determine vector quantity λ(T, v), we ﬁrst set
t = 0 in Eq. (2.30) and multiply the result by matrix g; then, we set t = T
and multiply the result by matrix h; and, ﬁnally, we combine the obtained
expressions. Taking into account Eq. (2.28), we obtain
g ∂x(0; T, v)
∂T
+ h ∂x(t; T, v)
∂T

t=T
= λ(T, v).
In view of the fact that
∂x(t; T, v)
∂T

t=T
= ∂x(T ; T, v)
∂T
−∂x(t; T, v)
∂t

t=T
= ∂R(T, v)
∂T
−F (T, R(T, v))
(with allowance for Eq. (2.27)), we obtain the desired expression for quantity
λ(T, v),
λ(T, v) = −hF (T, R(T, v)) .
(2.31)
Expression (2.30) with parameter λ(T, v) deﬁned by Eq. (2.31), i.e., the
expression
∂xi(t; T, v)
∂T
= −hklFl (T, R(T, v)) ∂xi(t; T, v)
∂vk
,
(2.32)
can be considered as the linear diﬀerential equation; one needs only to sup-
plement it with the corresponding initial condition
x(t; T, v)|T =t = R(t, v)
assuming that function R(T, v) is known.
The equation for this function can be obtained from the equality
∂R(T, v)
∂T
= ∂x(t; T, v)
∂t

t=T
+ ∂x(t; T, v)
∂T

t=T
.
(2.33)

92
2
Solution Dependence on Problem Type, Medium Parameters
The right-hand side of Eq. (2.33) is the sum of the right-hand sides of
Eq. (2.27) and (2.30) at t = T . As a result, we obtain the closed nonlinear
(quasilinear) equation
∂R(T, v)
∂T
= −hklFl (T, R(T, v)) ∂R(T, v)
∂vk
+ F (T, R(T, v)) .
(2.34)
The initial condition for Eq. (2.34) follows from Eq. (2.28) for T →0
R(T, v)|T =0 = (g + h)−1v.
(2.35)
Setting now t = 0 in Eq. (2.29), we obtain for the secondary boundary
quantity S(T, v) = x(0; T, v) the equation
∂S(T, v)
∂T
= −hklFl (T, R(T, v)) ∂S(T, v)
∂vk
(2.36)
with the initial condition
S(T, v)|T =0 = (g + h)−1v
following from Eq. (2.35).
Thus, the problem reduces to the closed quasilinear equation (2.34) with
initial value (2.35) and linear equation (2.30) whose coeﬃcients and initial
value are determined by the solution of Eq. (2.34).
In the problem under consideration, input 0 and output T are symmetric.
For this reason, one can solve it not only from T to 0, but also from 0 to T .
In the latter case, functions R(T, v) and S(T, v) switch the places.
An important point consists in the fact that, despite the initial problem
(2.27) is nonlinear, Eq. (2.30) is the linear equation, because it is essentially
the equation in variations. It is Eq. (2.34) that is responsible for nonlinearity.
Note that the above technique of deriving imbedding equations for
Eq. (2.27) can be easily extended to the boundary condition of the form [26]
g (x(0)) + h (x(T )) +
T

0
dτK (τ, x(τ)) = v,
where g (x), h (x) and K (T , x) are arbitrary given vector functions.
If function F (t, x) is linear in x, Fi (t, x) = Aij(t)xj(t), then boundary-
value problem (2.27), (2.28) assumes the simpler form
d
dtx(t) = A (t) x(t),
gx(0) + hx(T ) = v,
and the solution of Eq. (2.30), (2.34) and (2.36) will be the function linear in
v

2.2
Solution Dependence on Problem’s Parameters
93
x(t; T, v) = X(t; T )v.
(2.37)
As a result, we arrive at the closed matrix Riccati equation for matrix R(T ) =
X(T ; T )
d
dT R(T ) = A(T )R(T ) −R(T )hA(T )R(T ),
R(0) = (g + h)−1.
(2.38)
As regards matrix X(t, T ), it satisﬁes the linear matrix equation with the
initial condition
∂
∂T X(t; T ) = −X(t; T )hA(T )R(T ),
X(t; T )T =t = R(t).
(2.39)
Note that, for particular linear boundary problems of wave propagation,
the direct derivation of imbedding equations from the speciﬁc problem state-
ment is usually more convenient (see Appendix to Volume 2).

Chapter 3
Indicator Function and Liouville
Equation
Modern apparatus of the theory of random processes is able of constructing
closed descriptions of dynamic systems if these systems meet the condition
of dynamic causality and are formulated in terms of linear partial diﬀerential
equations or certain types of integral equations (see Chapter 5). One can
use indicator functions to perform the transition from the initial, generally
nonlinear system to the equivalent description in terms of the linear partial
diﬀerential equations. However, this approach results in increasing the dimen-
sion of the space of variables. Consider such a transition using the dynamic
systems described in Chapter 1.
3.1
Ordinary Diﬀerential Equations
Assume that a stochastic problem is described by the system of equations
(1.1), page 3
d
dtr(t) = U (r(t), t) ,
r(t0) = r0.
(3.1)
We introduce the scalar function
ϕ(r, t) = δ(r(t) −r),
(3.2)
which is concentrated on the section of the random process r(t) by a given
plane r(t) = r and is usually called the indicator function.
Diﬀerentiating Eq. (3.2) with respect to time t, we obtain, using Eq. (3.1),
the equality
∂
∂tϕ(r, t) = −dr(t)
dt
∂
∂r δ(r(t) −r) = −U (r(t), t) ∂
∂rδ(r(t) −r)
= −∂
∂r [U (r(t), t) δ(r(t) −r)] .
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
95
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_3, c
⃝Springer International Publishing Switzerland 2015

96
3
Indicator Function and Liouville Equation
Using then the probing property of the delta-function
U (r(t), t) δ(r(t) −r) = U (r, t) δ(r(t) −r),
we obtain the linear partial diﬀerential equation
 ∂
∂t + ∂
∂rU(r, t)

ϕ(r, t) = 0,
ϕ(r, t0) = δ(r0 −r)
(3.3)
equivalent to the initial system. This equation is called the Liouville equa-
tion. In terms of phase points moving in phase space {r, t}, this equation
corresponds to the equation of continuity.
Remark 3.1. Speciﬁcs of Working with Delta-Function
The above derivation procedure of the Liouville equation clearly shows the
necessity of distinguishing the function r(t) and the parameter r. In this con-
nection, the correct working with delta-function assumes that the argument
of function r(t) was explicitly written in all intermediate calculations (cf. the
forms of Eqs. (3.1) and (1.1), page 3). Neglecting this rule results usually in
errors.
♦
The transition from system (3.1) to Liouville equation (3.3) entails en-
largement of the phase space (r, t), which, however, has the ﬁnite dimension.
Note that Eq. (3.3) coincides in form with the equation of tracer transfer
by the velocity ﬁeld U(r, t) (1.56); the only diﬀerence consists in the initial
values.
Equation (3.3) can be rewritten in the form of the linear integral equation
ϕ(r, t) = δ(r −r0) −
t

t0
dτ ∂
∂r {U(r, τ)ϕ(r, τ)} .
Varying this equation with respect to function Ui(r′, t′) with the use of
Eq. (2.6), page 84, we obtain the integral equation
δϕ(r, t)
δUj(r′, t′) = −
t

t0
dτ ∂
∂ri
 δUi(r, τ)
δUj(r′, t′)ϕ(r, τ) + Ui(r, τ) δϕ(r, τ)
δUj(r′, t′)

= −θ(t −t′)θ(t′ −t0) ∂
∂rj

δ

r −r′
ϕ(r, t′)

−
t

t0
dτ ∂
∂r

U(r, τ) δϕ(r, τ)
δUj(r′, t′)

,
(3.4)
where θ(t) is, as earlier, the Heaviside step function. As a result, we have
δϕ(r, t)
δUj(r′, t′) ∼θ(t −t′)θ(t′ −t0),
and the variational derivative satisﬁes the condition

3.1
Ordinary Diﬀerential Equations
97
δϕ(r, t)
δUj(r′, t′) = 0
if
t′ < t0
or
t′ > t,
which expresses the condition of dynamic causality for the Liouville equation
(3.3).
For t0 < t′ < t, Eq. (3.4) can be written in the form
δϕ(r, t)
δUj(r′, t′) = −∂
∂rj
{δ (r −r′) ϕ(r, t′)} −
t

t′
dτ ∂
∂r

U(r, τ) δϕ(r, τ)
δUj(r′, t′)
	
,
(3.5)
from which follows that
δϕ(r, t)
δUi(r′, t′)

t=t′+0
= −∂
∂ri
{δ (r −r′) ϕ(r, t′)} .
(3.6)
Note that equality (3.6) can be derived immediately from Eq. (2.17),
page 87. Indeed, by deﬁnition of function ϕ(r, t), variational derivative
δϕ(r, t)/δUi(r′, t′) has the form
δϕ(r, t)
δUj(r′, t′) =
δ
δUj(r′, t′)δ(r(t) −r) = −∂
∂ri
δ(r(t) −r)
δri(t)
δUj(r′, t′).
(3.7)
As a consequence, for t = t′ + 0, we have the equality
δϕ(r, t)
δUj(r′, t′)

t=t′+0
= −∂
∂ri
δ(r(t′) −r)
δri(t)
δUj(r′, t′)

t=t′+0
= −∂
∂rj
{δ(r(t′) −r)δ (r′ −r)} = −∂
∂rj
{ϕ(r, t′)δ (r′ −r)} .
(3.8)
The solution to Eq. (3.1) and, consequently, function (3.2) depends on
the initial values t0, r0. Indeed, function r(t) = r(t|r0, t0) as a function of
variables r0 and t0 satisﬁes the linear ﬁrst-order partial diﬀerential equation
(2.21). The equations of such type also allow the transition to the equations
in the indicator function ϕ(t; r|t0, r0) (see the next section); in the case under
consideration, this equation is again the linear ﬁrst-order partial diﬀerential
equation including the derivatives with respect to variables r0 and t0,
 ∂
∂t0
+ U(r0, t0) ∂
∂r0

ϕ(r, t|r0, t0) = 0,
ϕ(r, t|r0, t) = δ(r0 −r).
(3.9)
Equation (3.9) can be called the backward Liouville equation. At the same
time, as a consequence of the dynamic causality, the variational derivative of
indicator function ϕ(r, t|r0, t0),
δϕ(r, t|r0, t0)
δUj(r′, t′)
=
δ
δUj(r′, t′)δ(r(t|r0, t0) −r) = −∂
∂ri δ(r(t|r0, t0) −r)δri(t|r0, t0)
δUj(r′, t′) ,

98
3
Indicator Function and Liouville Equation
can be expressed in view of Eq. (2.26), page 90 as follows
δϕ(r, t|r0, t0)
δUj(r′, t′)

t′=t0+0
= −∂
∂ri
δ(r(t|r0, t0) −r) δri(t|r0, t0)
δUj(r′, t′)

t′=t0+0
= −δ(r0 −r′)∂ri(t|r0, t0)
∂r0j
∂ϕ(r, t|r0, t0)
∂ri
.
(3.10)
On the other hand,
∂ϕ(r, t|r0, t0)
∂r0j
= −∂ri(t|r0, t0)
∂r0j
∂ϕ(r, t|r0, t0)
∂ri
,
and Eq. (3.10) assumes the ﬁnal closed form
δϕ(r, t|r0, t0)
δUj(r′, t′)

t′=t0+0
= δ(r0 −r′)∂ϕ(r, t|r0, t0)
∂r0j
.
(3.11)
Note that, in the general case of the Hamilton systems described by
Eq. (1.16), page 10, the indicator function ϕ(r, v, t) = δ(r(t) −r)δ(v(t) −v)
satisﬁes the Liouville equation (equation of continuity in {r, v}-space)
∂ϕ(r, v, t)
∂t
=

−∂
∂rk
∂H(r, v, t)
∂vk
+
∂
∂vk
∂H(r, v, t)
∂rk
	
ϕ(r, v, t)
that can be rewritten in the form
∂ϕ(r, v, t)
∂t
= {H(r, v, t), ϕ(r, v, t)} ,
(3.12)
where
{H, ϕ} =
∂H
∂rk
∂ϕ
∂vk
−∂H
∂vk
∂ϕ
∂rk

is the Poisson bracket of functions H(r, v, t) and ϕ(r, v, t).
3.2
First-Order Partial Diﬀerential Equations
If the initial-value problem is formulated in terms of partial diﬀerential equa-
tions, we always can convert it to the equivalent formulation in terms of the
linear variational diﬀerential equation in the inﬁnite-dimensional space (the
Hopf equation) [33–35] (see also [48, 49, 112]). For some particular types of
problems, this approach is simpliﬁed. Indeed, if the initial dynamic system
satisﬁes the ﬁrst-order partial diﬀerential equation (either linear as Eq. (1.56),
page 34, or quasilinear as Eq. (1.84), page 44, or in the general case nonlinear

3.2
First-Order Partial Diﬀerential Equations
99
as Eq. (1.101), page 50), then the phase space of the corresponding indicator
function will be the ﬁnite-dimension space [48, 49], which follows from the
fact that ﬁrst-order partial diﬀerential equations are equivalent to systems of
ordinary (characteristic) diﬀerential equations. Consider these cases in more
details.
3.2.1
Linear Equations
Backward Liouville Equation
First of all, we consider the derivation of the backward Liouville equation in
indicator function
ϕ(r, t|r0, t0) = δ (r (t|r0, t0) −r)
considered as a function of parameters (r0, t0). Diﬀerentiating function
ϕ(r, t|r0, t0) with respect to t0 and r0j and taking into account Eq. (2.21),
page 89, we obtain the expressions
∂ϕ(r, t|r0, t0)
∂t0
= ∂ϕ(r, t|r0, t0)
∂ri
U (r0, t0) ∂ri (t|r0, t0)
∂r0
,
∂ϕ(r, t|r0, t0)
∂r0
= −∂ϕ(r, t|r0, t0)
∂ri
∂ri(t|r0, t0)
∂r0
.
(3.13)
Taking scalar product of the second equation in (3.13) with U (r0, t0) and
summing the result with the ﬁrst one, we arrive at the closed equation
 ∂
∂t0
+ U (r0, t0) ∂
∂r0

ϕ(r, t|r0, t0) = 0,
ϕ(r, t|r0, t) = δ(r −r0), (3.14)
which is just the backward Liouville equation (3.9) given earlier without
derivation.
Rewrite Eq. (3.14) as the linear integral equation
ϕ(r, t|r0, t0) = δ(r −r0) +
t

t0
dτU(r0, τ) ∂
∂r0
ϕ(r, t|r0, τ).
(3.15)
According to this equation, variational derivative δϕ(r, t|r0, t0)
δUj(r′, t′)
satisﬁes the
linear integral equation

100
3
Indicator Function and Liouville Equation
δϕ(r, t|r0, t0)
δUj(r′, t′)
= δ(r0 −r′)θ(t′ −t0)θ(t −t′)∂ϕ(r, t|r0, t′)
∂r0j
+
t

t0
dτU(r0, τ) ∂
∂r0
δϕ(r, t|r0, τ)
δUj(r′, t′) ,
from which follows that
δϕ(r, t|r0, t0)
δUj(r′, t′)
∼θ(t′ −t0)θ(t −t′),
i.e., the condition of dynamic causality holds. For t0 < t′ < t, this equation
is simpliﬁed to the form
δϕ(r, t|r0, t0)
δUj(r′, t′)
= δ(r0 −r′)∂ϕ(r, t|r0, t′)
∂r0j
+
t′

t0
dτU(r0, τ) ∂
∂r0
δϕ(r, t|r0, τ)
δUj(r′, t′) ,
and we arrive, in the limit t′ = t0+0, at Eq. (3.11) derived earlier in a diﬀerent
way.
Density Field of Passive Tracer under Random Velocity Field
Consider the problem on tracer transfer by random velocity ﬁeld in more
details. The problem is formulated in terms of Eq. (1.56), page 34 that we
rewrite in the form
 ∂
∂t + U(r, t) ∂
∂r

ρ(r, t) + ∂U(r, t)
∂r
ρ(r, t) = 0,
ρ(r, 0) = ρ0(r).
(3.16)
To describe the density ﬁeld in the Eulerian description, we introduce the
indicator function
ϕ(r, t; ρ) = δ(ρ(t, r) −ρ),
(3.17)
which is similar to function (3.2) and is localized on surface ρ(r, t) = ρ =
const in the three-dimensional case or on a contour in the two-dimensional
case. An equation for this function can be easily obtained either immediately
from Eq. (3.16), or from the Liouville equation in the Lagrangian description.
Indeed, diﬀerentiating Eq. (3.17) with respect to time and using dynamic
equation (3.16) and probing property of the delta-function, we obtain the
equation
∂
∂tϕ(r, t; ρ) = ∂U(r, t)
∂r
∂
∂ρρϕ(r, t; ρ) + U(r, t)∂ρ(r, t)
∂r
∂
∂ρϕ(r, t; ρ).
(3.18)

3.2
First-Order Partial Diﬀerential Equations
101
However, this equation is not closed because the right-hand side includes
term ∂ρ(r, t)/∂r that cannot be explicitly expressed through ρ(r, t).
On the other hand, diﬀerentiating function (3.17) with respect to r, we
obtain the equality
∂
∂r ϕ(r, t; ρ) = −∂ρ(r, t)
∂r
∂
∂ρϕ(r, t; ρ).
(3.19)
Eliminating now the last term in Eq. (3.18) with the use of (3.19), we obtain
the closed Liouville equation in the Eulerian description
 ∂
∂t + U(r, t) ∂
∂r

ϕ(r, t; ρ) = ∂U(r, t)
∂r
∂
∂ρ [ρϕ(r, t; ρ)]
ϕ(r, 0; ρ) = δ(ρ0(r) −ρ).
(3.20)
Remark 3.2. The simplest model of velocity ﬁeld
Note that the Liouville equation (3.20) becomes the one-dimensional equa-
tion in the case of velocity ﬁeld described by the simplest model (1.17), page
10 – U(x, t) = v(t)f(x) (x is the dimensionless variable),
 ∂
∂t + vx(t)f(x) ∂
∂x

ϕ(x, t; ρ) = vx(t)∂f(x)
∂x
∂
∂ρ [ρϕ(x, t; ρ)] ,
ϕ(x, 0; ρ) = δ(ρ0(x) −ρ).
(3.21)
♦
To obtain more complete description, we consider the extended indicator
function including both density ﬁeld ρ(r, t) and its spatial gradient p(r, t) =
∇ρ(r, t)
ϕ(r, t; ρ, p) = δ (ρ(r, t) −ρ) δ (p(r, t) −p) .
(3.22)
Diﬀerentiating Eq. (3.22) with respect to time, we obtain the equality
∂
∂tϕ(r, t; ρ, p) = −
 ∂
∂ρ
∂ρ(r, t)
∂t
+ ∂
∂pi
∂pi(r, t)
∂t

ϕ(r, t; ρ, p).
(3.23)
Using now dynamic equation (3.16) for density and Eq. (1.69), page 38 for
the density spatial gradient, we can rewrite Eq. (3.23) as the equation
∂
∂tϕ(r, t; ρ, p) = ∂
∂ρ
∂U(r, t)
∂r
ρ + U(r, t)p

ϕ(r, t; ρ, p)
+ ∂
∂pi

U(r, t)∂pi(r, t)
∂r
+ ∂U(r, t)
∂r
pi + pk ∂Uk(r, t)
∂ri
+ ρ∂2U(r, t)
∂ri∂r

ϕ(r, t; ρ, p),
(3.24)
which is not closed because of the term ∂pi(r, t)/∂r in the right-hand side.

102
3
Indicator Function and Liouville Equation
Diﬀerentiating function (3.22) with respect to r, we obtain the equality
∂
∂rϕ(r, t; ρ, p) = −

p ∂
∂ρ + ∂pi(r, t)
∂r
∂
∂pi

ϕ(r, t; ρ, p).
(3.25)
Now, multiplying Eq. (3.25) by U(r, t) and adding the result to Eq. (3.24),
we obtain the closed Liouville equation for the extended indicator function
 ∂
∂t + U(r, t) ∂
∂r

ϕ(r, t; ρ, p)
=
∂Uk(r, t)
∂ri
∂
∂pi pk + ∂U(r, t)
∂r
 ∂
∂ρρ + ∂
∂pp

+ ∂2U(r, t)
∂ri∂r
∂
∂pi ρ

ϕ(r, t; ρ, p),
(3.26)
with the initial value
ϕ(r, 0; ρ, p) = δ (ρ0(r) −ρ) δ (p0(r) −p) .
In the case of nondivergent velocity ﬁeld (div U(r, t) = 0), the Liouville
equation (3.26) is simpliﬁed to the form
 ∂
∂t + U(r, t) ∂
∂r

ϕ(r, t; ρ, p) = ∂Uk(r, t)
∂ri
∂
∂pi
pkϕ(r, t; ρ, p),
ϕ(r, 0; ρ, p) = δ (ρ0(r) −ρ) δ (p0(r) −p) .
(3.27)
Derive now Eq. (3.20) starting from the Lagrangian description of the
dynamic system. In the Lagrangian representation, the behavior of a passive
tracer is described in terms of ordinary diﬀerential equations (1.59), (1.60),
and (1.62), page 35. Using these equations, one can easily derive the linear
Liouville equation in the corresponding phase space for the function
ϕLag(r, ρ, j, t|r0) = δ(r(t|r0) −r)δ(ρ(t|r0) −ρ)δ(j(t|r0) −j),
(3.28)
which explicitly assumes that the solution to the initial dynamic equations is
a function of the Lagrangian coordinates r0. This equation has the form
 ∂
∂t + ∂
∂r U(r, t)

ϕLag(r, ρ, j, t|r0) = ∂U(r, t)
∂r
 ∂
∂ρρ −∂
∂j j

ϕLag(r, ρ, j, t|r0)
(3.29)
with the initial value
ϕLag(r, ρ, j, 0|r0) = δ(r0 −r)δ(ρ0(r0) −ρ)δ(j −1).
Taking into account the equality

3.2
First-Order Partial Diﬀerential Equations
103
δ(r(t|r0) −r) =
1
∥∂rα/∂r0β∥δ(r0 −r0(t, r)) =
1
j(t|r0)δ(r0 −r0(t, r)),
we can rewrite function (3.28) in the form
ϕLag(r, ρ, j, t|r0) = 1
j δ(r0 −r0(t, r))δ(j(t|r0) −j)ϕ(r, t; ρ),
where ϕ(r, t; ρ) is the indicator function (3.17). Consequently,
ϕ(r, t; ρ) =

dr0
∞

0
jdjϕLag(r, ρ, j, t|r0).
(3.30)
Multiplying Eq. (3.29) by j and integrating the result over j and r0, we
obtain the corresponding Liouville equation in the Eulerian representation,
which coincides with Eq. (3.20).
In the case of nondivergent velocity ﬁeld, Eqs. (3.16), (3.20), and (3.29)
coincide. Fundamental diﬀerences appear only if the potential component is
available in the velocity ﬁeld.
Note that solutions to dynamic problems have the one-time and one-point
probability densities that coincide with the corresponding indicator functions
averaged over an ensemble of realizations
P(r, ρ, j, t|r0) = ⟨ϕLag (r, ρ, j, t|r0)⟩,
P(r, t; ρ) = ⟨ϕ (r, t; ρ)⟩,
P(r, t; ρ, p) = ⟨ϕ (r, t; ρ, p)⟩.
This point explains the special interest to indicator functions in the statistical
analyzing of dynamics of systems. In addition, indicator functions provide a
good deal of data on geometric structure of random ﬁelds, which can be
obtained using statistical topography of random ﬁelds (see Sect. 6.2, page
160).
Magnetic Field Diﬀusion under Random Velocity Field
Let us introduce the indicator function of magnetic ﬁeld H(r, t),
ϕ(r, t; H) = δ(H(r, t) −H).
Diﬀerentiating this function with respect to time with the use of dynamic
equation (1.78), page 40 and probing property of the delta function, we arrive
at the equation

104
3
Indicator Function and Liouville Equation
∂
∂tϕ(r, t; H) = −∂Hi(r, t)
∂t
∂
∂Hi ϕ(r, t; H)
= −

H(r, t)∂ui(r, t)
∂r
−Hi(r, t)∂u(r, t)
∂r

∂
∂Hi ϕ(r, t; H)
+ u(r, t)∂Hi(r, t)
∂r
∂
∂Hi ϕ(r, t; H) = −∂
∂Hi

H ∂ui(r, t)
∂r
−Hi ∂u(r, t)
∂r

ϕ(r, t; H)
+ u(r, t)∂Hi(r, t)
∂r
∂
∂Hi ϕ(r, t; H),
which is unclosed because of the last term. However, since
∂
∂rϕ(r, t; H) = −∂Hi(r, t)
∂r
∂
∂Hi
δ(H(r, t) −H)
and, hence,
−u(r, t) ∂
∂rϕ(r, t; H) = u(r, t)∂Hi(r, t)
∂r
∂
∂Hi
δ(H(r, t) −H),
we obtain the desired closed Liouville equation [67]
 ∂
∂t + u(r, t) ∂
∂r

ϕ(r, t; H) = −∂
∂Hi

H ∂ui(r, t)
∂r
−Hi
∂u(r, t)
∂r

ϕ(r, t; H)
(3.31)
with the initial condition ϕ(r, 0; H) = δ(H0(r) −H).
Remark 3.3. The Simplest Model of Velocity Field
Note that, in the case of velocity ﬁeld described by the simplest model
(1.17), page 10 – u(x, t) = v(t)f(x) (x is the dimensionless variable), the
Liouville equation (3.31) with homogeneous initial conditions becomes the
equation
∂
∂tϕ(x, t; H) =

−∂
∂xf(x) + f ′(x)

1 +
∂
∂H H

vx(t)ϕ(x, t; H)
−f ′(x)Hx0vi(t) ∂
∂Hi
ϕ(x, t; H)
(3.32)
with the initial condition ϕ(x, 0; H) = δ(H0 −H).
♦
3.2.2
Quasilinear Equations
Consider now the simplest quasilinear equation for scalar quantity q(r, t)
(1.84), page 44

3.2
First-Order Partial Diﬀerential Equations
105
 ∂
∂t + U(t, q) ∂
∂r

q(r, t) = Q (t, q) ,
q(r, 0) = q0(r).
(3.33)
In this case, an attempt of deriving a closed equation for the indicator
function ϕ(r, t; q) = δ(q(r, t) −q) on the analogy of the linear problem will
fail. Here, we must supplement Eq. (3.33) with Eq. (1.85), page 45 for the
gradient ﬁeld p(r, t) = ∇q(r, t)
 ∂
∂t + U(t, q) ∂
∂r

p(r, t) + ∂{U(t, q)p(r, t)}
∂q
p(r, t) = ∂Q(t, q)
∂q
p(r, t)
(3.34)
and consider the extended indicator function
ϕ(r, t; q, p) = δ(q(r, t) −q)δ(p(r, t) −p).
(3.35)
Diﬀerentiating Eq. (3.35) with respect to time and using Eqs. (3.33) and
(3.34), we obtain the equation
∂
∂tϕ(r, t; q, p) = −
 ∂
∂q
∂q(r, t)
∂t
+
∂
∂pk
∂pk(r, t)
∂t

ϕ(r, t; q, p)
=
 ∂
∂q [pU(t, q) −Q(t, q)] +
∂
∂pk
U(t, q)∂pk(r, t)
∂r
	
ϕ(r, t; q, p)
+
∂
∂pk

pk

p∂U(t, q)
∂q
−∂Q(t, q)
∂q

ϕ(r, t; q, p),
(3.36)
which is not closed, however, because of the term ∂pk(r, t)/∂r in the right-
hand side.
Diﬀerentiating function (3.35) with respect to r, we obtain the equality
∂
∂rϕ(r, t; q, p) = −

p ∂
∂q + ∂pk(r, t)
∂r
∂
∂pk

ϕ(r, t; q, p),
(3.37)
from which follows that
∂
∂pk
∂pk(r, t)
∂r
ϕ(r, t; q, p) = −
 ∂
∂r + p ∂
∂q

ϕ(r, t; q, p).
Consequently, Eq. (3.36) can be rewritten in the closed form
 ∂
∂t + U(t, q) ∂
∂r

ϕ(r, t; q, p) =

p∂U(t, q)
∂q
−∂
∂q Q(t, q)

ϕ(r, t; q, p)
+
∂
∂pk

pk

p∂U(t, q)
∂q
−∂Q(t, q)
∂q

ϕ(r, t; q, p),
(3.38)

106
3
Indicator Function and Liouville Equation
which is just the desired Liouville equation for the quasilinear equation (3.33)
in the extended phase space {q, p} with the initial value
ϕ(r, 0; q, p) = δ(q0(r) −q)δ(p0(r) −p).
Note that the equation of continuity for conserved quantity I(r, t)
∂
∂tI(r, t) + ∂
∂r {U(t, q)I(r, t)} = 0,
I(r, 0) = I0(r)
can be combined with Eqs. (3.33), (3.34). In this case, the indicator function
has the form
ϕ(r, t; q, p, I) = δ(q(r, t) −q)δ(p(r, t) −p)δ(I(r, t) −I)
and derivation of the closed equation for this function appears possible in
space {q, p, I}, which follows from the fact that, in the Lagrangian descrip-
tion, quantity inverse to I(r, t) coincides with the divergence.
Note that the Liouville equation in indicator function
ϕ(x, t; q, p, χ) = δ (q(x, t) −u) δ (p(x, t) −p) δ (χ(x, t) −χ)
in the case of the one-dimensional Riemann equation (1.96), page 47 in func-
tion q(x, t) complemented with the equations for the gradient ﬁeld p(x, t) =
∂q(x, t)
∂x
— (1.98) and the logarithm of the density ﬁeld χ(x, t) = ln ρ(x, t) —
(1.100) assumes the form of the equation
 ∂
∂t + q ∂
∂x

ϕ(x, t; q, p, χ) =

p + ∂
∂pp2 + p ∂
∂χ

ϕ(x, t; q, p, χ)
(3.39)
with the initial condition
ϕ(x, 0; q, p, χ) = δ(q0(x) −q)δ(p0(x) −p)δ (χ0(x) −χ) .
The solution to Eq. (3.39) can be represented in the form
ϕ(x, t; q, p, χ) = exp

−t

q ∂
∂x −p ∂
∂χ
	
ϕ(x, t; q, p, χ),
where function ϕ(x, t; q, p) satisﬁes the equation
 ∂
∂t −p2 ∂
∂p

ϕ(x, t; q, p, χ) = 3pϕ(x, t; q, p, χ)
(3.40)
with the initial condition
ϕ(x, 0; q, p, χ) = δ(q0(x) −q)δ(p0(x) −p)δ (χ0(x) −χ) .

3.2
First-Order Partial Diﬀerential Equations
107
Solving now Eq. (3.40) by the method of characteristics, we obtain the
expression
ϕ(x, t; q, p, χ) =
1
(1 −pt)3 δ(q0(x) −q)δ

p0(x) −
p
1 −pt

× δ (χ0(x) −ln (1 + tp0(x)) −χ) ,
that can be rewritten in the form
ϕ(x, t; q, p, χ) = [1 + tp0(x)] δ(q0(x) −q)δ

p0(x)
1 + tp0(x) −p

× δ (χ0(x) −ln (1 + tp0(x)) −χ) .
(3.41)
Thus, we obtain the solution to Eq. (3.39) in the ﬁnal form
ϕ(x, t; q, p, χ) = exp

−qt ∂
∂x
	
[1 + tp0(x)] δ(q0(x) −q)δ

p0(x)
1 + tp0(x) −p

× δ (χ0(x) −ln (1 + tp0(x)) −χ) ,
(3.42)
in agreement with solutions on page 47.
Waves on Sea Surface
Consider now problem (1.174), page 71 that describes generation of sea sur-
face displacements under the eﬀect of random vertical velocity of the sea
medium.
Introduce the indicator function of surface displacement
ϕ(R, t; ξ, p) = δ (ξ(R, t) −ξ)
(3.43)
and the joined indicator function of surface displacement and gradient
ϕ(R, t; ξ, p) = δ (ξ(R, t) −ξ) δ (p(R, t) −p) .
(3.44)
Then, using equality
∂ϕ(R, t; ξ)
∂t
= −∂ξ(R, t)
∂t
∂
∂ξ ϕ(R, t; ξ)
and taking into account the equation for the ﬁeld of water surface displace-
ment (Eq. (1.174), page 71), we obtain an equation in function (3.43)
∂ϕ(R, t; ξ)
∂t
−∂ξ(R, t)
∂Ri
∂
∂ξ ui(R, ξ; t)ϕ(R, t; ξ) = −∂
∂ξ w(R, ξ; t)ϕ(R, t; ξ),
(3.45)

108
3
Indicator Function and Liouville Equation
which is nonclosed because it contains an additional unknown function,
namely, the gradient of the water surface displacement.
Equation for the joined indicator function ϕ(R, t; ξ, p) has the form
∂ϕ(R, t; ξ, p)
∂t
= −∂ξ(R, t)
∂t
∂ϕ(R, t; ξ, p)
∂ξ
−∂pk(R, t)
∂t
∂ϕ(R, t; ξ, p)
∂pk
. (3.46)
Using Eq. (1.175), page (71) that describes the gradient of surface displace-
ment pk(R, t) = ∂ξ(R, t)
∂Rk
, we can rewrite Eq. (3.46) in the form
∂ϕ(R, t; ξ, p)
∂t
= ∂
∂ξ [ui(R, ξ, t)pi −w(R, ξ; t)] ϕ(R, t; ξ, p)
+
∂
∂pk
∂ui(R, ξ; t)
∂Rk
pi + ∂ui(R, ξ; t)
∂z
pkpi

ϕ(R, t; ξ, p)
−
∂
∂pk
∂w(R, ξ; t)
∂Rk
+ ∂w(R, ξ; t)
∂z
pk

ϕ(R, t; ξ, p)
+ ∂2ξ(R, t)
∂Rk∂Ri
ui(R, ξ(R, t); t) ∂
∂pk
ϕ(R, t; ξ, p),
(3.47)
which is nonclosed again, but containes now the second spatial derivative of
water surface displacement.
In order to close Eq. (3.47), we calculate spatial derivative
∂
∂Ri
ϕ(R, t; ξ, p) = −∂ξ(R, t)
∂Ri
∂
∂ξ ϕ(R, t; ξ, p) −∂2ξ(R, t)
∂Ri∂Rk
∂
∂pk
ϕ(R, t; ξ, p)
(3.48)
and use this expression to eliminate the last term in Eq. (3.47). As a result,
we obtain the closed Liouville equation of the form
∂ϕ(R, t; ξ, p)
∂t
+ ui(R, ξ; t)
 ∂
∂Ri
+ pi
∂
∂ξ

ϕ(R, t; ξ, p)
= ∂
∂ξ [piui(R, ξ; t) −w(R, ξ; t)] ϕ(R, t; ξ, p)
+
∂
∂pk
∂ui(R, ξ; t)
∂Rk
pi + ∂ui(R, ξ; t)
∂ξ
pkpi

ϕ(R, t; ξ, p)
−
∂
∂pk
∂w(R, ξ; t)
∂Rk
+ ∂w(R, ξ; t)
∂ξ
pk

ϕ(R, t; ξ, p).
(3.49)

3.3
Higher-Order Partial Diﬀerential Equations
109
3.2.3
General-Form Nonlinear Equations
Consider now the scalar nonlinear ﬁrst-order partial diﬀerential equation in
the general form (1.101), page 50
∂
∂tq(r, t) + H (r, t, q, p) = 0,
q (r, 0) = q0 (r) ,
(3.50)
where p(r, t) = ∂q(r, t)/∂r. In order to derive the closed Liouville equation
in this case, we must supplement Eq. (3.50) with equations for vector p(r, t)
and second derivative matrix Uik(r, t) = ∂2q(r, t)/∂ri∂rk.
Introduce now the extended indicator function
ϕ(r, t; q, p, U, I) = δ(q(r, t) −q)δ(p(r, t) −p)δ(U(r, t) −U)δ(I(r, t) −I),
(3.51)
where we included for generality an additional conserved variable I(r, t) sat-
isfying the equation of continuity (1.103)
∂
∂tI(r, t) + ∂
∂r
∂H (r, t, q, p)
∂p
I(r, t)
	
= 0,
I (r, 0) = I0 (r) .
(3.52)
Equations (3.50), (3.52) describe, for example, wave propagation in in-
homogeneous media within the frames of the geometrical optics approxi-
mation of the parabolic equation of quasi-optics. Diﬀerentiating function
(3.51) with respect to time and using dynamic equations for functions
q(r, t), p(r, t), U(r, t) and I(r, t), we generally obtain an unclosed equation
containing third-order derivatives of function q(r, t) with respect to spatial
variable r. However, the combination
 ∂
∂t + ∂H (r, t, q, p)
∂p
∂
∂r

ϕ(r, t; q, p, U, I)
will not include the third-order derivatives; as a result, we obtain the closed
Liouville equation in space {q, r, U, I} [48,49].
3.3
Higher-Order Partial Diﬀerential Equations
If the initial dynamic system includes higher-order derivatives (e.g., Laplace
operator), derivation of a closed equation for the corresponding indicator
functionbecomes impossible. In this case, only the variational diﬀerential
equation (the Hopf equation) can be derived in the closed form for the func-
tional whose average over an ensemble of realizations coincides with the cha-
racteristic functional of the solution to the corresponding dynamic equation.

110
3
Indicator Function and Liouville Equation
Consider such a transition using the partial diﬀerential equations considered
in Chapter 1 as examples.
3.3.1
Parabolic Equation of Quasi-Optics
The ﬁrst example concerns wave propagation in a random medium within
the frames of the linear parabolic equation (1.153), page 63
∂
∂xu(x, R) = i
2k ΔRu(x, R)+ ik
2 ε(x, R)u(x, R),
u(0, R) = u0(R). (3.53)
Equation (3.53) as the initial-value problem possesses the property of dyna-
mic causality with respect to parameter x. Indeed, rewrite this equation in
the integral form
u(x, R) = u0(R) + i
2k
x

0
dξΔRu(ξ, R) + ik
2
x

0
dξε(ξ, R)u(ξ, R).
(3.54)
Function u(x, R) is a functional of ﬁeld ε(x, R), i.e.
u(x, R) = u

x, R; ε

x, R

.
Varying Eq. (3.54) with respect to function ε(x′, R′), we obtain the integral
equation in variational derivative δu(x, R)
δε(x′, R′)
δu(x, R)
δε(x′, R′) = ik
2 θ(x −x′)θ(x′)δ(R −R′)u(x′, R)
+ ik
2
x

0
dξε(ξ, R) δu(ξ, R)
δε(x′, R′) + i
2k
x

0
dξΔR
δu(ξ, R)
δε(x′, R′).
(3.55)
From this equation follows that
δu(x, R)
δε(x′, R′) = 0
if
x < x′
or
x′ < 0,
(3.56)
which just expresses the dynamic causality property. As a consequence of this
fact, we can rewrite Eq. (3.55) in the form (for 0 < x′ < x)

3.3
Higher-Order Partial Diﬀerential Equations
111
δu(x, R)
δε(x′, R′) = ik
2 δ(R −R′)u(x′, R)
+ ik
2
x

x′
dξε(ξ, R) δu(ξ, R)
δε(x′, R′) + i
2k
x

x′
dξΔR
δu(ξ, R)
δε(x′, R′)
(3.57)
from which follows the equality
δu(x, R)
δε(x −0, R′) = ik
2 δ(R −R′)u(x, R).
(3.58)
Consider now the functional
ϕ[x; v(R′), v∗(R′)] = exp

i

dR′ 
u(x, R′)v(R′) + u∗(x, R′)v∗(R′)
	
,
(3.59)
where waveﬁeld u(x, R) satisﬁes Eq. (3.53) and u∗(x, R) is the complex con-
jugated function. Diﬀerentiating (3.59) with respect to x and using dynamic
equation (3.53) and its complex conjugate, we obtain the equality
∂
∂xϕ[x; v(R′), v∗(R′)] = −1
2k

dR [v(R)ΔRu(x, R)
−v∗(R)ΔRu∗(x, R)] ϕ[x; v(R′), v∗(R′)]
−k
2

dRε(x, R) [v(R)u(x, R) −v∗(R)u∗(x, R)] ϕ[x; v(R′), v∗(R′)],
which can be written as the variational diﬀerential equation
∂
∂xϕ[x; v(R′), v∗(R′)] = ik
2

dRε(x, R)!
M(R)ϕ[x; v(R′), v∗(R′)]
+ i
2k

dR

v(R)ΔR
δ
δv(R) −v∗(R)ΔR
δ
δv∗(R)

ϕ[x; v(R′), v∗(R′)]
(3.60)
with the initial value
ϕ[0; v(R′), v∗(R′)] = exp

i

dR′ 
u0(R′)v(R′) + u∗
0(R′)v∗(R′)
	
,
(3.61)
and with the Hermitian operator
!
M(R) = v(R)
δ
δv(R) −v∗(R)
δ
δv∗(R).
Equation (3.60) is equivalent to the input equation (3.53).

112
3
Indicator Function and Liouville Equation
Rewrite problem (3.60) with the initial condition (3.61) in the integral
form
ϕ[x; v( R), v∗( R)] = ϕ[0; v( R), v∗( R)]
+ ik
2
x

0
dξ

dRε(ξ, R)!
M(R)ϕ[ξ; v( R), v∗( R)]
+ i
2k
x

0
dξ

dR

v(R)ΔR
δ
δv(R) −v∗(R)ΔR
δ
δv∗(R)

ϕ[ξ; v( R), v∗( R)]
(3.62)
and vary the resulting equation with respect to function ε(x′, R′). Then we
obtain that variational derivative δϕ[x; v( R), v∗( R)]
δε(x′, R′)
satisﬁes the integral
equation
δϕ[x; v( R), v∗( R)]
δε(x′, R′)
= θ(x −x′)θ(x′)ik
2
!
M(R′)ϕ[x′; v( R), v∗( R)]
+ ik
2
x

0
dξ

dRε(ξ, R)!
M(R)δϕ[ξ; v( R), v∗( R)]
δε(x′, R′)
+ i
2k
x

0
dξ

dR

v(R)ΔR
δ
δv(R) −v∗(R)ΔR
δ
δv∗(R)
 δϕ[ξ; v( R), v∗( R)]
δε(x′, R′)
.
(3.63)
In view of the dynamic causality property, we can rewrite it in the form of
the equation (for 0 < x′ < x)
δϕ[x; v( R), v∗( R)]
δε(x′, R′)
= ik
2
!
M(R′)ϕ[x′; v( R), v∗( R)]
+ ik
2
x

x′
dξ

dRε(ξ, R)!
M(R)δϕ[ξ; v( R), v∗( R)]
δε(x′, R′)
+ i
2k
x

x′
dξ

dR

v(R)ΔR
δ
δv(R) −v∗(R)ΔR
δ
δv∗(R)
 δϕ[ξ; v( R), v∗( R)]
δε(x′, R′)
(3.64)

3.3
Higher-Order Partial Diﬀerential Equations
113
from which follows the equality
δ
δε(x −0, R)ϕ[x; v( R), v∗( R)] = ik
2
!
M(R)ϕ[x; v( R), v∗( R)].
(3.65)
3.3.2
Random Forces in Hydrodynamic Theory of
Turbulence
Consider now integro-diﬀerential equation (1.171), page 69 for the Fourier
harmonics %u(k, t) of the solution to the Navier–Stokes equation (1.164)
∂
∂t %ui (k, t) + i
2

dk1

dk2Λαβ
i
(k1, k2, k) %uα (k1, t) %uβ (k2, t)
−νk2%ui (k, t) = %fi (k, t) ,
(3.66)
where
Λαβ
i
(k1, k2, k) =
1
(2π)3 {kαΔiβ (k) + kβΔiα (k)} δ(k1 + k2 −k),
Δij (k) = δij −kikj
k2
(i, α, β = 1, 2, 3),
and %f(k, t) is the spatial Fourier harmonic of external forces.
Function %ui (k, t) satisﬁes the principle of dynamic causality, which means
that the variational derivative of function
%ui (k, t) = %ui

k, t; %fα

k, t

(considered as a functional of ﬁeld %
f (k, t)) with respect to ﬁeld %
f

k′, t′
vanishes for t′ > t and t′ < 0,
δ%ui (k, t)
δ %fj

k′, t′ = 0,
if
t′ > t
or
t′ < 0,
and, moreover,
δ%ui (k, t)
δ %fj

k′, t −0
 = δijδ(k −k′).
(3.67)
Consider the functional
ϕ[t; z] = ϕ[t; z(k′)] = exp

i

dk′%u(k′, t)z(k′)
	
.
(3.68)

114
3
Indicator Function and Liouville Equation
Diﬀerentiating this functional with respect to time t and using dynamic
equation (3.66), we obtain the equality
∂
∂tϕ[t; z] = 1
2

dkzi(k)

dk1

dk2Λαβ
i
(k1, k2, k) uα (k1, t) uβ (k2, t) ϕ[t; z]
−i

dkzi(k)

νk2ui (k, t) −fi (k, t)

ϕ[t; z],
which can be rewritten in the functional space as the linear Hopf equation
containing variational derivatives
∂
∂tϕ[t; z] = −1
2

dkzi(k)

dk1

dk2Λαβ
i
(k1, k2, k)
δ2ϕ[t; z]
δzα (k1) δzβ (k2)
−

dkzi(k)

νk2
δ
δzi (k) −i %fi (k, t)
	
ϕ[t; z]
(3.69)
with the given initial condition at t = 0.
This means that Eq. (3.69) satisfy the condition of dynamic causality; a
consequence of this condition is the equality
δ
δ%f(k, t −0)
ϕ[t; z] = iz(k)ϕ[t; z].
(3.70)
In a similar way, considering the space-time harmonics of the velocity ﬁeld
%ui(K), where K is the four-dimensional wave vector {k, ω} and %u∗
i (K) =
%ui(−K) because ﬁeld ui(r, t) is real, we obtain the nonlinear integral equation
(1.172), page 70
(iω + νk2)ui (K)+ i
2

d4K1

d4K2Λαβ
i
(K1, K2, K) uα (K1) uβ (K) = fi (K) ,
(3.71)
where
Λαβ
i
(K1, K2, K) = 1
2π Λαβ
i
(k1, k2, k) δ(ω1 + ω2 −ω),
and %fi(K) are the space-time Fourier harmonics of external forces. In this
case, dealing with the functional
ϕ[z] = ϕ[z(K′)] = exp

i

d4K′%u(K′)z(K′)
	
,
(3.72)
we derive the linear variational integro-diﬀerential equation of the form
(iω + νk2) δϕ[z]
δzi (K) = −1
2

d4K1

d4K2Λαβ
i
(K1, K2, K)
δϕ[z]
δzα (K1) δzβ (K2)
+ i fi (K) ϕ[z].
(3.73)

Part II
Random Quantities, Processes, and
Fields

Chapter 4
Random Quantities and Their
Characteristics
Prior to consider statistical descriptions of the problems mentioned in Part
1, we discuss basic concepts of the theory of random quantities, processes,
and ﬁelds.
This chapter concerns those basic properties of random quantities, pro-
cesses, and ﬁelds that are widely used in analyzing dynamic systems with
ﬂuctuating parameters, but only slightly elucidated in textbooks. Here, we
will follow monographs [46, 48, 49], in which one can also found the funda-
mental references concerning the problem.
The probability for a random quantity ξ to fall in interval−∞< ξ < z is
the monotonic function
F(z) = P(−∞< ξ < z) = ⟨θ(z −ξ)⟩ξ ,
F(∞) = 1,
(4.1)
where
θ(z) =

1, if z > 0,
0, if z < 0
is the Heaviside step function and ⟨· · · ⟩ξ denotes averaging over anensemble
of realizations of random quantity ξ. This function is called the probability
distribution function or the integral distribution function. Deﬁnition (4.1)
reﬂects the real-world procedure of ﬁnding the probability according to the
rule
P(−∞< ξ < z) = lim
N→∞
n
N ,
where n is the integer equal to the number of realizations of event ξ < z in
N independent trials. Consequently, the probability for a random quantity ξ
to fall into interval z < ξ < z + dz, where dz is the inﬁnitesimal increment,
can be written in the form
P(z < ξ < z + dz) = p(z)dz,
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
117
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_4, c
⃝Springer International Publishing Switzerland 2015

118
4
Random Quantities and Their Characteristics
where function p(z) called the probability density is represented by the for-
mula
p(z) = d
dz P(−∞< ξ < z) = ⟨δ(z −ξ)⟩ξ ,
(4.2)
where δ(z) is the Dirac delta function. In terms of probability density p(z),
the integral distribution function is expressed by the formula
F(z) = P(−∞< ξ < z) =
z

−∞
dξp(ξ),
(4.3)
so that
p(z) > 0,
∞

−∞
dzp(z) = 1.
Multiplying Eq. (4.2) by arbitrary function f(z) and integrating over the
whole domain of variable z, we express the mean value of the arbitrary func-
tion of random quantity in the following form
⟨f(ξ)⟩ξ =
∞

−∞
dzp(z)f(z).
(4.4)
The characteristic function deﬁned by the equality
Φ(v) =
,
eivξ-
ξ =
∞

−∞
dzeivzp(z)
is a very important quantity that exhaustively describes all characteristics of
random quantity ξ. The characteristic function being known, we can obtain
the probability density (via the Fourier transform)
p(x) = 1
2π
∞

−∞
dvΦ(v)e−ivx,
moments
Mn = ⟨ξn⟩=
∞

−∞
dzp(z)zn =
 d
idv
n
Φ(v)

v=0
,
cumulants (or semi-invariants)
Kn =
 d
idv
n
Θ(v)

v=0
,

4
Random Quantities and Their Characteristics
119
where Θ(v) = ln Φ(v), and other statistical characteristics. In terms of mo-
ments and cumulants of random quantity ξ, functions Θ(v) and Φ(v) are the
Taylor series
Φ(v) =
∞
&
n=0
in
n!Mnvn,
Θ(v) =
∞
&
n=1
in
n!Knvn.
(4.5)
In the case of multidimensional random quantity ξ = {z1, · · · , zn}, the
exhaustive statistical description assumes the multidimensional characteristic
function
Φ(v) =
,
eivξ-
ξ ,
v = {v1, · · · , vn}.
(4.6)
The corresponding joined probability density for quantities ξ1, · · · , ξn is the
Fourier transform of characteristic function Φ(v), i.e.,
P(x) =
1
(2π)n

dvΦ(v)e−ivx,
x = {x1, · · · , xn}.
(4.7)
Substituting function Φ(v) deﬁned by Eq. (4.6) in Eq. (4.7) and integrating
the result over v, we obtain the obvious equality
P(x) = ⟨δ(ξ −x)⟩ξ = ⟨δ(ξ1 −x1) · · · δ(ξn −xn)⟩
(4.8)
that can serve the deﬁnition of the probability density of random vector
quantity ξ.
In this case, the moments and cumulants of random quantity ξ are deﬁned
by the expressions
Mi1,··· ,in =
∂n
in∂vi1 · · · ∂vin
Φ(v)

v=0
,
Ki1,··· ,in =
∂n
in∂vi1 · · · ∂vin
Θ(v)

v=0
,
where Θ(v) = ln Φ(v), and functions Θ(v) and Φ(v) are expressed in terms
of moments Mi1,··· ,in and cumulants Ki1,··· ,in via the Taylor series
Φ(v) =
∞
&
n=0
in
n!Mi1,··· ,invi1 · · · vin,
Θ(v) =
∞
&
n=1
in
n!Ki1,··· ,invi1 · · · vin.
(4.9)
Note that, for quantities ξ assuming only discrete values ξi(i = 1, 2, · · · )
with probabilities pi, formula (4.8) is replaced with its discrete analog
pk = ⟨δz,ξk⟩,
where δi,k is the Kronecker delta (δi,k = 1 for i = k and 0 otherwise).
Consider now statistical average ⟨ξf(ξ)⟩ξ, where f(z) is arbitrary deter-
ministic function such that the above average exists. We calculate this average
using the procedure that will be widely used in what follows. Instead of f(ξ),
we consider function f(ξ + η), where η is arbitrary deterministic quantity.

120
4
Random Quantities and Their Characteristics
Expand function f(ξ + η) in the Taylor series in powers of ξ„ i.e., represent
it in the form
f(ξ + η) =
∞
&
n=0
1
n!f (n)(η)ξn = exp

ξ d
dη
	
f(η),
where we introduced the shift operator with respect to η. Then we can write
the equality
⟨ξf(ξ + η)⟩ξ =
.
ξ exp

ξ d
dη
	/
ξ
f(η)
=
.
ξ exp

ξ d
dη
	/
ξ
.
exp

ξ d
dη
	/
ξ
.
exp

ξ d
dη
	/
ξ
f(η) = Ω
 d
idη

⟨f(ξ + η)⟩ξ ,
(4.10)
where function
Ω(v) =
,
ξeiξv-
ξ
⟨eiξv⟩ξ
=
d
idv ln Φ(v) =
d
idv Θ(v),
and Φ(v) is the characteristic function of random quantity ξ. Using now the
Taylor series (4.5) for function Θ(v), we obtain function Ω(v) in the form of
the series
Ω(v) =
∞
&
n=0
in
n!Kn+1vn.
(4.11)
Because variable η appears in the right-hand side of Eq. (4.10) only as the
term of sum ξ + η, we can replace diﬀerentiation with respect to η with
diﬀerentiation with respect to ξ (in so doing, operator Ω(d/idξ) should be
introduced into averaging brackets) and set η = 0. As a result, we obtain the
equality
⟨ξf(ξ)⟩ξ =
.
Ω
 d
idξ

f(ξ)
/
ξ
,
which can be rewritten, using expansion (4.11) for Ω(v), as the series in
cumulants Kn
⟨ξf(ξ)⟩ξ =
∞
&
n=0
1
n!Kn+1
.dnf(ξ)
dξn
/
ξ
.
(4.12)
Note that, setting f(ξ) = ξn−1 in Eq. (4.12), we obtain the recurrence
formula that relates moments and cumulants of random quantity ξ in the
form

4
Random Quantities and Their Characteristics
121
Mn =
n
&
k=1
(n −1)!
(k −1)!(n −k)!KkMn−k
(M0 = 1,
n = 1, 2, · · · ).
(4.13)
In a similar way, we can obtain the following operator expression for sta-
tistical average ⟨g(ξ)f(ξ)⟩ξ
⟨g(ξ + η1)f(ξ + η2)⟩ξ = exp

Θ
 d
idη1 +
d
idη2

−Θ
 d
idη1

−Θ
 d
idη2

× ⟨g(ξ + η1)⟩ξ ⟨f(ξ + η2)⟩ξ .
(4.14)
In the particular case g(z) = eωz, where parameter ω assumes complex values
too, we obtain the expression
,
eωξf(ξ + η)
-
ξ = exp

Θ
1
i

ω + d
dη

−Θ
 d
idη
	
⟨f(ξ + η)⟩ξ . (4.15)
To illustrate practicability of the above formulas, we consider two types of
random quantities ξ as examples.
1. Let ξ be the Gaussian random quantity with the probability density
p(z) =
1
√
2πσ exp

−z2
2σ2
	
.
Then, we have
Φ(v) = exp

−v2σ2
2
	
,
Θ(v) = −v2σ2
2
,
so that
M1 = K1 = ⟨ξ⟩= 0,
M2 = K2 = σ2 =
,
ξ2-
,
Kn>2 = 0.
In this case, the recurrence formula (4.13) assumes the form
Mn = (n −1)σ2Mn−2,
n = 2, · · · ,
(4.16)
from which follows that
M2n+1 = 0,
M2n = (2n −1)!!σ2n.
For averages (4.12) and (4.15), we obtain the expressions
⟨ξf(ξ)⟩ξ = σ2
.df(ξ)
dξ
/
ξ
,
,
eωξf(ξ)
-
ξ = exp
ω2σ2
2
	 ,
f(ξ + ωσ2)
-
ξ .
(4.17)
Additional useful formulas can be derived from Eqs. (4.17); for example,

122
4
Random Quantities and Their Characteristics
,
eωξ-
ξ = exp
ω2σ2
2
	
,
,
ξeωξ-
ξ = ωσ2 exp
ω2σ2
2
	
,
and so on.
If random quantity ξ has a nonzero mean value, ⟨ξ⟩̸= 0, the probability
density is given by the expression
P(z) =
1
√
2πσ2 exp

−(ξ −⟨ξ⟩)2
2σ2
	
.
(4.18)
In this case, the characteristic function assumes the form
Φ(v) = exp

iv ⟨ξ⟩−v2σ2
2
	
,
Θ(v) = iv ⟨ξ⟩−v2σ2
2
,
and, consequently, moments and cumulants are given by the expressions
M1 = K1 = ⟨ξ⟩, M2 = ⟨ξ⟩2 + σ2, K2 = σ2, Kn>2 = 0.
The corresponding integral distribution function (4.3) assumes the form
F(z) =
1
√
2πσ2
z

−∞
dξ exp

−(ξ −⟨ξ⟩)2
2σ2
	
=
1
√
2π
z−⟨ξ⟩
σ

−∞
dy exp

−y2
2
	
= Pr
z −⟨ξ⟩
σ

,
(4.19)
where function
Pr (z) =
1
√
2π
z

−∞
dx exp

−x2
2
	
(4.20)
is known as the probability integral. It is clear that
Pr(∞) = 1
and
Pr(0) = 1
2.
(4.21)
For negative z (z < 0), this function satisﬁes the relationship
Pr (−|z|) =
1
√
2π
∞

|z|
dx exp

−x2
2
	
= 1 −Pr (|z|) .
(4.22)
Asymptotic behavior of probability integral for z →±∞can be easily
derived from Eqs. (4.20) and (4.22); namely,

4
Random Quantities and Their Characteristics
123
Pr (z) ≈
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1 −
1
z
√
2π exp

−z2
2
	
, z →∞,
1
|z|
√
2π exp

−z2
2
	
z →−∞.
(4.23)
If we deal with the random Gaussian vector whose components are ξi
(⟨ξi⟩ξ = 0, i = 1, · · · , n), then the characteristic function is given by the
equality
Φ(v) = exp

−1
2Bijvivj
	
,
Θ(v) = −1
2Bijvivj,
where matrix Bij = ⟨ξiξj⟩and repeated indices assume summation. In this
case, Eq. (4.17) is replaced with the equality
⟨ξf(ξ)⟩ξ = B
.df(ξ)
dξ
/
ξ
.
(4.24)
2. Let ξ ≡n be the integer random quantity governed by Poisson distri-
bution
pn = ¯nn
n! e−¯n,
where ¯n is the average value of quantity n. In this case, we have
Φ(v) = exp
0
¯n

eiv −1
1
,
Θ(v) = ¯n

eiv−1

.
The recurrence formula (4.13) and Eq. (4.12) assume for this random quantity
the forms
Ml = ¯n
l−1
&
k=0
(l −1)!
k!(l −1 −k)!Mk ≡¯n
2
(n + 1)l−13
, ⟨nf(n)⟩= ¯n ⟨f(n + 1⟩.
(4.25)

Chapter 5
Random Processes and Their
Characteristics
5.1
General Remarks
If we deal with random function (random process) z(t), then all statistical
characteristics of this function at any ﬁxed instant t are exhaustively descri-
bed in terms of the one-time probability density
P(z, t) = ⟨δ (z(t) −z)⟩
(5.1)
dependent parametrically on time t by the following relationship
⟨f(z(t))⟩=
∞

−∞
dzf(z)P(z, t).
Note that the singular Dirac delta function
ϕ(z, t) = δ (z(t) −z)
appeared in Eq. (5.1) in angle brackets of averaging is called the indicator
function.
The integral distribution function for this process, i.e. the probability of
the event that process z(t) < Z at instant t, is calculated by the formula
F(t, Z) = P(z(t) < Z) =
Z

−∞
dzP(z, t)
from which follows that
F(t, Z) = ⟨θ(Z −z(t))⟩,
F(t, ∞) = 1,
(5.2)
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
125
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_5, c
⃝Springer International Publishing Switzerland 2015

126
5
Random Processes and Their Characteristics
where θ(z) is the Heaviside step function equal to zero for z < 0 and unity
for z > 0.
Similar deﬁnitions hold for the two-time probability density
P(z1, t1; z2, t2) = ⟨ϕ(z1, t1; z2, t2)⟩
and for the general case of the n-time probability density
P(z1, t1; · · · ; zn, tn) = ⟨ϕ(z1, t1; · · · ; zn, tn)⟩,
where
ϕ(z1, t1; · · · ; zn, tn) = δ(z(t1) −z1) · · · δ(z(tn) −zn).
is the n-time indicator function.
Process z(t) is called stationary if all its statistical characteristics are in-
variant with respect to arbitrary temporal shift, i.e., if
P(z1, t1 + τ; · · · ; zn, tn + τ) = P(z1, t1; · · · ; zn, tn).
(5.3)
In particular, the one-time probability density of stationary process is at all
independent of time, and the correlation function depends only on diﬀerence
of times,
Bz(t1, t2) = ⟨z(t1)z(t2)⟩= Bz(t1 −t2).
Temporal scale τ0 characteristic of correlation function Bz(t) is called the
temporal correlation radius of process z(t). We can determine this scale, say,
by the equality
∞

0
⟨z(t + τ)z(t)⟩dτ = τ0
,
z2(t)
-
.
(5.4)
Note that the Fourier transform of stationary process correlation function
Φz(ω) =
∞

−∞
dtBz(t)eiωt
is called the temporal spectral function (or simply temporal spectrum).
An exhaustive description of random function z(t) can be given in terms
of the characteristic functional
Φ[v(τ)] =
4
exp
⎧
⎨
⎩i
∞

−∞
dτv(τ)z(τ)
⎫
⎬
⎭
5
,
(5.5)
where v(t) is arbitrary (but suﬃciently smooth) function. Functional Φ[v(τ)]
being known, one can determine such characteristics of random function z(t)

5.1
General Remarks
127
as mean value ⟨z(t)⟩, correlation function ⟨z(t1)z(t2)⟩, n-time moment func-
tion ⟨z(t1) · · · z(tn)⟩, etc.
Indeed, expanding functional Φ[v(τ)] in the functional Taylor series, we
obtain the representation of characteristic functional in terms of the moment
functions of process z(t):
Φ[v(τ)] =
∞
&
n=0
in
n!
∞

−∞
dt1 · · ·
∞

−∞
dtnMn(t1, · · · , tn)v(t1) · · · v(tn),
Mn(t1, · · · , tn) = ⟨z(t1) · · · z(tn)⟩= 1
in
δn
δv(t1) · · · δv(tn)Φ[v(τ)]

v=0
.
Consequently, the moment functions of random process z(t) are expressed
in terms of variational derivatives of the characteristic functional. See Sect.
2.1.1, page 81 for variational derivative deﬁnitions and the corresponding
operation rules.
Represent functional Φ[v(τ)] in the form Φ[v(τ)] = exp{Θ[v(τ)]}. Func-
tional Θ[v(τ)] also can be expanded in the functional Taylor series
Θ[v(τ)] =
∞
&
n=1
in
n!
∞

−∞
dt1 · · ·
∞

−∞
dtnKn(t1, · · · , tn)v(t1) · · · v(tn),
(5.6)
where function
Kn(t1, · · · , tn) = 1
in
δn
δv(t1) · · · δv(tn)Θ[v(τ)]

v=0
is called the n-th order cumulant function of random process z(t).
Characteristic functionals ensure an exhaustive description of random pro-
cesses. However, even one-time probability densities provide certain informa-
tion about temporal behavior of random processes for arbitrary long temporal
intervals.
5.1.1
Random Process Typical Realization Curve
Following works [53, 71], we discuss ﬁrst the concept of typical realization
curve of random process z(t). This concept concerns the fundamental features
of the behavior of a separate process realization as a whole for temporal
intervals of arbitrary duration.
A feature common of all random functions (processes) z(t) consists in the
fact that their particular realizations contain irregular spikes (ﬂuctuations)
both positive and negative relative certain deterministic curve called the typi-
cal realization curve

128
5
Random Processes and Their Characteristics
{
{
{
z∗(t)
z(t)
t
t1
t2
Δt1
Δt2
Δt3
Fig. 5.1 To the deﬁnition of the typical realization curve of a random process
The typical realization curve of random process z(t) is deﬁned as the deter-
ministic curve z∗(t), which is the median of the integral distribution function
(5.2) and is determined as the solution to the algebraic equation
F (t, z∗(t)) =
z∗(t)

−∞
dz P(z, t) = 1
2.
(5.7)
This means, on the one hand, that for any t probabilities
P{z(t) > z∗(t)} = P{z(t) < z∗(t)} = 1/2.
On the other hand, a speciﬁc property of this curve as the median is that, for
any temporal interval (t1, t2), random process z(t) entwines about curve z∗(t)
in a way to force the identity of average times during which the inequalities
z(t) > z∗(t) and z(t) < z∗(t) hold (Fig. 5.1):
,
Tz(t)>z∗(t)
-
=
,
Tz(t)<z∗(t)
-
= 1
2 (t2 −t1) .
(5.8)
Indeed, integrating Eq. (5.7) over temporal interval (t1, t2), we obtain
t2

t1
dtF (t, z∗(t)) = 1
2(t2 −t1).
(5.9)
On the other hand, in view of deﬁnition of the integral distribution function
(5.2), the integral in the right-hand side of Eq. (5.9) can be represented as

5.1
General Remarks
129
t2

t1
dtF (t, z∗(t)) = ⟨T (t1, t2)⟩,
(5.10)
where T (t1, t2) =
N
6
1
Δtk is the combined time during which the realization
of process z(t) appears below curve z∗(t) in interval (t1, t2). Combining
Eqs. (5.9) and (5.10), we obtain Eq. (5.8).
Curve z∗(t) can signiﬁcantly diﬀer from any particular realization of pro-
cess z(t) and cannot describe possible magnitudes of spikes. Nevertheless,
the deﬁnitional domain of typical realization curve z∗(t) of random process
z(t) derived from the one-time probability density is the whole temporal axis
t ∈(0, ∞). This property common of all random processes is called inter-
mittency. For more detailed discussion of this concept, see Sect. 10.4.4, page
320.
Consideration of speciﬁc random processes allow obtaining an additional
information concerning the realization spikes relative to this curve.
5.1.2
Statistics of Random Process Cross Points with
a Line
The one-time probability density (5.1) of random process z(t) is a result of
averaging the singular indicator function over an ensemble of realizations of
this process. This function is concentrated at points at which process z(t)
crosses line z = const. Because the cross points are determined as roots of
algebraic equation
z(tn) = z
(n = 0, 1, · · · , ∞),
we can rewrite the indicator function in the following form
ϕ(z, t) =
n
&
k=1
1
|p(tk)|δ(t −tk),
where p(t) = d
dtz(t).
The number of cross points by itself is obviously a random quantity des-
cribed by the formula
n(t, z) =
t

−∞
dτ|p(τ)|ϕ(τ; z).
(5.11)
As a consequence, the average number of points where process z(t) crosses
line z = const can be described in terms of the correlation between the process

130
5
Random Processes and Their Characteristics
derivative with respect to time and the process indicator function, or in terms
of joint one-time probability density of process z(t) and its derivative with
respect to time d
dtz(t).
In a similar way, we can determine certain elements of statistics related to
some other special points (such as points of maxima or minima) of random
process z(t).
Consider several examples of random processes.
5.2
Gaussian Random Process
We start the discussion with the continuous processes; namely, we consider
the Gaussian random process z(t) with zero-valued mean (⟨z(t)⟩= 0) and
correlation function B(t1, t2) = ⟨z(t1)z(t2)⟩. The corresponding characteristic
functional assumes the form
Φ[v(τ)] = exp
⎧
⎨
⎩−1
2
∞

−∞
∞

−∞
dt1dt2B(t1, t2)v(t1)v(t2)
⎫
⎬
⎭.
(5.12)
Only one cumulant function (the correlation function)
K2(t1, t2) = B(t1, t2)
is diﬀerent from zero for this process, so that
Θ[v(τ)] = −1
2
∞

−∞
∞

−∞
dt1dt2B(t1, t2)v(t1)v(t2).
(5.13)
Consider the nth-order variational derivative of functional Φ[v(τ)]. It satis-
ﬁes the following line of equalities:
δn
δv(t1) · · · δv(tn)Φ[v(τ)] =
δn−1
δv(t2) · · · δv(tn)
δΘ[v(τ)]
δv(t1) Φ[v(τ)]
=
δ2Θ[v(τ)]
δv(t1)δv(t2)
δn−2
δv(t3) · · · δv(tn)Φ[v(τ)] +
δn−2
δv(t3) · · · δv(tn)
δΘ[v(τ)]
δv(t1)
δΦ[v(τ)]
δv(t2) .
Setting now v = 0, we obtain that moment functions of the Gaussian
process z(t) satisfy the recurrence formula
Mn(t1, · · · , tn) =
n
&
k=2
B(t1, t2)Mn−2(t2, · · · , tk−1, tk+1, · · · , tn).
(5.14)

5.2
Gaussian Random Process
131
From this formula follows that, for the Gaussian process with zero-valued
mean, all moment functions of odd orders are identically equal to zero and
the moment functions of even orders are represented as sums of terms which
are the products of averages of all possible pairs z(ti)z(tk).
If we assume that function v(τ) in Eq. (5.13) is diﬀerent from zero only in
interval 0 < τ < t, the characteristic functional
Φ[t; v(τ)] =

exp
⎛
⎝i
t

0
dτz(τ)v(τ)
⎞
⎠

= exp
⎧
⎨
⎩−
t

0
dτ1
τ1

0
dτ2B(τ1, τ2)v(τ1)v(τ2)
⎫
⎬
⎭
(5.15)
becomes a function of time t and satisﬁes the ordinary diﬀerential equation
d
dtΦ[t; v(τ)] = −v(t)
t

0
dt1B(t, t1)v(t1)Φ[t; v(τ)],
Φ[0; v(τ)] = 1.
(5.16)
To obtain the one-time characteristic function of the Gaussian random
process at instant t, we specify function v(τ) in Eq. (5.12) in the form
v(τ) = vδ(τ −t).
Then, we obtain
Φ(v, t) =
2
eivz(t)3
=
∞

−∞
dz P(z, t)eivz = exp

−1
2σ2(t)v2
	
,
(5.17)
where σ2(t) = B(t, t). Using the inverse Fourier transform of (5.17), we obtain
the one-time probability density of the Gaussian random process
P(z, t) = 1
2π
∞

−∞
dv Φ(v, t)e−ivz =
1

2πσ2(t)
exp

−
z2
2σ2(t)
	
.
(5.18)
Note that, in the case of stationary random process z(t), variance σ2(t) is
independent of time t, i.e., σ2(t) = σ2 = const.
Density P(z, t) as a function of z is symmetric relative to point z = 0,
P(z, t) = P(−z, t).
(5.19)
If mean value of the Gaussian random process is diﬀerent from zero, then
we can consider process z(t) −⟨z(t)⟩to obtain instead of Eq. (5.18) the ex-
pression
P(z, t) =
1

2πσ2(t)
exp
'
−(z −⟨z(t)⟩)2
2σ2(t)
(
(5.20)
and the corresponding integral distribution function assumes the form

132
5
Random Processes and Their Characteristics
F(z, t) =
1

2πσ2(t)
z

−∞
dz exp
'
−(z −⟨z(t)⟩)2
2σ2(t)
(
= Pr
z −⟨z(t)⟩
σ(t)

,
where probability integral Pr(z) is deﬁned by Eq. (4.20), page 122.
In view of Eq. (4.21), page 122, the typical realization curve (5.7) of the
Gaussian random process z(t) in this case coincides with the mean value of
process z(t)
z∗(t) = ⟨z(t)⟩.
(5.21)
5.3
Logarithmically Normal Random Process
The so-called logarithmically normal (lognormal) random process y(t)
y(t) = ez(t),
whose logarithm is the Gaussian random process, is described by the one-time
probability density P(y, t) of the form
P(y, t) = 1
y P(z = ln y, t) =
1
y

2πσ2(t)
exp
'
−ln2 
e−⟨z(t)⟩y

2σ2(t)
(
and, consequently, the corresponding integral distribution function assumes
the form
F(y, t) =
y

−∞
dy′ P(y′, t) =
1

2πσ2(t)
y

−∞
dy′
y′ exp
'
−ln2 
e−⟨z(t)⟩y′
2σ2(t)
(
=
1

2πσ2(t)
ln[e−⟨z(t)⟩y]

−∞
dz exp

−
z2
2σ2(t)
	
= Pr
7
ln

e−⟨z(t)⟩y

σ(t)
8
,
where probability integral Pr(z) is deﬁned by Eq. (4.20), page 122.
In view of Eq. (4.21), page 122, the right-hand side of this equality turns
into 1/2 if
e−⟨z(t)⟩y(t) = 1,
which means that the typical realization curve y∗(t) of the lognormal random
process y(t) is given by the equality
y∗(t) = e⟨z(t)⟩= e⟨ln y(t)⟩.
(5.22)

5.4
Discontinuous Random Processes
133
Note that, if we know the moment functions of random process y(t) as
functions of time, i.e., if we know functions ⟨yn(t)⟩(n = 1, 2, · · · ), we know
also the statistical characteristics of random process z(t) = ln y(t). Indeed,
⟨yn(t)⟩=
2
en ln y(t)3
= exp

n ⟨ln y(t)⟩+ n2
2 σ2
ln y(t)
	
and, consequently,
⟨ln y(t)⟩= lim
n→0
1
n ln ⟨yn(t)⟩,
σ2
ln y(t) = lim
n→∞
2
n2 ln ⟨yn(t)⟩.
(5.23)
5.4
Discontinuous Random Processes
Consider now some examples of discontinuous processes. The discontinuous
processes are the random functions that change their time-dependent beha-
vior at discrete instants t1, t2, · · · given statistically. The description of dis-
continuous processes requires ﬁrst of all either the knowledge of the statistics
of these instants, or the knowledge of the statistics of number n(0, t) of in-
stants ti falling in time interval (0, t). In the latter case, we have the equality
n(0, t) = n(0, t′) + n(t′, t),
0 ≤t′ ≤t.
The quantity n(0, t) by itself is a random process, and Fig. 5.2 shows its
possible realization.
t
0
1
2
3
4
t1
t2
t3
t4
n(0, t)
Fig. 5.2 A possible realization of process n(0, t)
The set of points of discontinuity t1, t2, · · · of process z(t) is called the
stream of points. In what follows, we will consider the Poisson stationary
stream of points in which the probability of falling n points in interval (t1, t2)
is speciﬁed by the Poisson formula

134
5
Random Processes and Their Characteristics
Pn(t1,t2)=n =

n(t1, t2)
n
n!
e−n(t1,t2)
(5.24)
with the mean number of points in interval (t1, t2) given by the formula
n(t1, t2) = ν|t1 −t2|,
where ν is the mean number of points per unit time. It is assumed here that
the numbers of points falling in nonoverlapping intervals are statistically in-
dependent and the instants at which points were fallen in interval (t1, t2)
under the condition that their total number was n are also statistically in-
dependent and uniformly distributed over the interval (t1, t2). The length of
the interval between adjacent points of discontinuity satisﬁes the exponential
distribution.
The Poisson stream of points is an example of the Markovian processes
(see Sect. 5.5).
Note that quantity (5.24)
P(n, t) = ⟨δ (n(0, t) −n)⟩,
which is the probability density of falling n points in time interval (0, t),
satisﬁes as a function of parameter t the recurrence equations
d
dtP(n, t) = −ν [P(n −1, t) −P(n, t)] ,
P(n, 0) = 0
(n = 1, 2, · · · ),
d
dtP(0, t) = −νP(0, t),
P(0, 0) = 1.
(5.25)
Consider now random processes whose points of discontinuity form Poisson
streams of points. Currently, three types of such processes — Poisson process,
telegrapher’s process, and generalized telegrapher’s process — are mainly used
in the model problems of physics. Below, we focus our attention on these
processes.
5.4.1
Poisson (Impulse) Random Process
The Poisson (impulse) random process z(t) is the process described by the
formula
z(t) =
n
&
i=1
ξig(t −ti),
(5.26)
where random quantities ξi are statistically independent and distributed
with probability density p(ξ); random points tk are uniformly distributed on

5.4
Discontinuous Random Processes
135
interval (0, T ), so that their number n obeys the Poisson law with parameter
¯n = νT ; and function g(t) is the deterministic function that describes the
pulse envelope.
Calculate the characteristic functional of this process
Φ[t; v(τ)] =
4
exp
⎧
⎨
⎩i
t

0
dτ v(τ)z(τ)
⎫
⎬
⎭
5
z
.
(5.27)
We will average Eq. (5.27) in two stages (see, e.g., [46]). At ﬁrst, we perform
averaging over random quantity ξ and positions of random points tk:
Φ[t; v(τ)] =
44
exp
⎧
⎨
⎩i
t

0
dτ v(τ)
n
&
k=1
ξkg(τ −tk)
⎫
⎬
⎭
5
ξi,tk
5
n
=
4⎡
⎣1
T
T

0
dt′
∞

−∞
dξp(ξ) exp
⎧
⎨
⎩iξ
t

0
dτ v(τ)g(τ −t′)
⎫
⎬
⎭
⎤
⎦
n5
n
=
4⎡
⎣1
T
T

0
dt′ W
⎛
⎝
t

t′
dτ v(τ)g(τ −t′)
⎞
⎠
⎤
⎦
n5
n
,
(5.28)
where W(v) =
 ∞
−∞
dξ p(ξ)eiξv is the characteristic function of random quan-
tity ξ. Let t < T . Then Eq. (5.28) can be rewritten in the form
Φ[t; v(τ)] =
4⎡
⎣1
T
t

0
dt′ W
⎛
⎝
t

t′
dτ v(τ)g(τ −t′)
⎞
⎠+ T −t
T
⎤
⎦
n5
n
.
(5.29)
Average now Eq. (5.29) over Poisson’s distribution pn = e−n
n! (n)n of random
quantity n:
Φ[t; v(τ)] =
∞
 
0
e−n
n! (n)n
⎡
⎣1
T
t

0
dt′ W
⎛
⎝
t

t′
dτ v(τ)g(τ −t′)
⎞
⎠+ T −t
T
⎤
⎦
n
n
= exp
⎧
⎨
⎩−n + n
T
⎡
⎣
t

0
dt′ W
⎛
⎝
t

t′
dτ v(τ)g(τ −t′)
⎞
⎠+ T −t
⎤
⎦
⎫
⎬
⎭.
As a result, we obtain the following expression for the characteristic
functional of Poisson’s random process

136
5
Random Processes and Their Characteristics
Φ[t; v(τ)] = exp
⎧
⎨
⎩ν
t

0
dt′
⎡
⎣W
⎛
⎝
t

t′
dτv(τ)g(t −t′)
⎞
⎠−1
⎤
⎦
⎫
⎬
⎭.
(5.30)
Consequently, functional Θ[t; v(τ)] is given by the formula
Θ[t; v(τ)] = ln Φ[t; v(τ)] = ν
t

0
dt′
∞

−∞
dξp(ξ)
⎧
⎨
⎩exp
⎡
⎣iξ
t

t′
dτv(τ)g(t −t′)
⎤
⎦−1
⎫
⎬
⎭
(5.31)
and cumulant functions assume the form
Kn(t1, · · · , tn) = ν ⟨ξn⟩
min{t1,··· ,tn}

0
dt′g(t −t′) · · · g(tn −t′).
We consider two types of Poisson’s processes important for applications.
1. Let g(t) = θ(t) =
 1, t > 0,
0, t < 0, , i.e., z(t) =
n6
i=1
ξiθ(t −ti). In this case,
Kn(t1, · · · , tn) = ν ⟨ξn⟩min{t1, · · · , tn}.
If additionally ξ = 1, then process z(t) ≡n(0, t), and we have
Kn(t1, · · · , tn) = ν min{t1, · · · , tn},
Θ[t; v(τ)] = ν
t

0
dt′
⎧
⎨
⎩exp
⎡
⎣i
t

t′
dτv(τ)
⎤
⎦−1
⎫
⎬
⎭.
(5.32)
2. Let now g(t) = δ(t). In this case, process
z(t) =
n
&
i=1
ξiδ(t −ti)
is usually called the shot noise process. This process is a particular case of
the delta-correlated processes (see Sect. 7.8, page 181). For such a process,
functional Θ[t; v(τ)] is
Θ[t; v(τ)] = ν
t

0
dτ
∞

−∞
dξp(ξ)
"
eiξv(τ) −1
#
(5.33)
and cumulant functions assume the forms
Kn(t1, · · · , tn) = ν ⟨ξn⟩δ(t1 −t2)δ(t2 −t3) · · · δ(tn−1 −tn).

5.4
Discontinuous Random Processes
137
Note that the Poisson process z(t) with arbitrary given impulse function
g(t) can be always expressed in terms of the Poisson delta-correlated random
process zδ(t) by the formula
z(t) =
t

0
dτg(t −τ)zδ(τ).
(5.34)
5.4.2
Telegrapher’s Random Process
Consider now statistical characteristics of telegrapher’s random process
(Fig. 5.3) deﬁned by the formula
z(t) = a(−1)n(0,t)
(z(0) = a,
z2(t) ≡a2),
(5.35)
where n(t1, t2) is the random sequence of integers equal to the number of
points of discontinuity in interval (t1, t2).
z(t)
−a
t
a
Fig. 5.3 A possible realization of telegrapher’s random process
We consider two cases.
1. We will assume ﬁrst that amplitude a is a deterministic quantity.
For the two ﬁrst moment functions of process z(t), we have the expressions
⟨z(t)⟩= a
∞
&
n(0,t)=0
(−1)n(0,t)Pn(0,t) = ae−2n(0,t) = ae−2νt,
⟨z(t1)z(t2)⟩= a2 2
(−1)n(0,t1)+n(0,t2)3
= a2 2
(−1)n(t2,t1)3
= a2e−2n(t2,t1) = a2e−2ν(t1−t2)
(t1 ≥t2).

138
5
Random Processes and Their Characteristics
The higher moment functions for t1 ⩾t2 ⩾· · · ⩾tn satisfy the recurrence
relationship
Mn(t1, · · · , tn) = ⟨z(t1) · · · z(tn)⟩= a2 %
(−1)n(0,t1)+n(0,t2)+n(0,t3)+···+n(0,tn)&
= a2 %
(−1)n(t2,t1)& %
(−1)n(0,t3)+···+n(0,tn)&
= ⟨z(t1)z(t2)⟩Mn−2(t3, · · · , tn).
(5.36)
This relationship is very similar to Eq. (5.14) for the Gaussian process with
correlation function B(t1, t2). The only diﬀerence is that the right-hand side
of Eq. (5.36) coincides with only one term of the sum in Eq. (5.14), namely,
with the term that corresponds to the above order of times.
Consider now the characteristic functional of this process
Φa[t; v(τ)] =
4
exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
,
where index a means that amplitude a is the deterministic quantity. Expand-
ing the characteristic functional in the functional Taylor series and using
recurrence formula (5.36), we obtain the expansion
Φa[t; v(τ)] =
∞
&
n=0
in
n!
t

0
· · ·
t

0
dt1 · · · dtnMn(t1, · · · , tn)v(t1) · · · v(tn)
= 1 + ia
t

0
dt1e−2νt1v(t1) −a2
t

0
dt1
t1

0
dt2e−2ν(t1−t2)v(t1)v(t2)
×
∞
&
n=2
in
t2

0
dt3 · · ·
tn−1

0
dtnMn(t3, · · · , tn)v(t3) · · · v(tn).
(5.37)
The sum in the right-hand side of Eq. (5.37) can be expressed in terms of the
characteristic functional; as a result, we obtain the integral equation
Φa[t; v(τ)] = 1 + ia
t

0
dt1e−2νt1v(t1) −a2
t

0
dt1
t1

0
dt2e−2ν(t1−t2)v(t1)v(t2)Φa[t2; v(τ)].
(5.38)
Diﬀerentiating Eq. (5.38) with respect to t, we obtain the integro-differential
equation
d
dtΦa[t; v(τ)] = iae−2νtv(t) −a2v(t)
t

0
dt1e−2ν(t−t1)v(t1)Φa[t1; v(τ)]. (5.39)

5.4
Discontinuous Random Processes
139
No general solution to Eq. (5.39) is known. It can be shown that this equation
is equivalent to the second-order diﬀerential equation
 d2
dt2 +

2ν + d ln v(t)
dt
 d
dt + a2v(t)
	
Φa[t; v(τ)] = 0,
Φa[0; v(τ)] = 1,
d
dtΦa[t; v(τ)]

t=0
= iav(0).
2. Let now amplitude a be the random quantity with probability density
p(a). To obtain the characteristic functional of process z(t) in this case, we
should average Eq. (5.39) with respect to random amplitude a. In the general
case, such averaging cannot be performed analytically. Analytical averaging
of Eq. (5.39) appears possible only if probability density of random amplitude
a has the form
p(a) = 1
2 [δ (a −a0) + δ (a + a0)]
with ⟨a⟩= 0 and
,
a2-
= a2
0 (in fact, this very case is what is called usually
telegrapher’s process). As a result, we obtain the integro-diﬀerential equation
d
dtΦ[t; v(τ)] = −a2
0v(t)
t

0
dt1e−2ν(t−t1)v(t1)Φ[t1; v(τ)]
(5.40)
equivalent to the second-order equation
 d2
dt2 +

2ν + d ln v(t)
dt
 d
dt + a2
0v(t)
	
Φ[t; v(τ)] = 0,
d
dtΦ[t; v(τ)]

t=0
= 0.
Φ[0; v(τ)] = 1,
(5.41)
Note that in the special case of v(t) ≡v, Eq. (5.41) can be solved analyti-
cally, and the solution has the form
Φ[t; v] =

exp
⎧
⎨
⎩iv
t
	
0
dτz(τ)
⎫
⎬
⎭

= e−νt
⎧
⎪
⎨
⎪
⎩
cosh

ν2 −a2
0v2t+
ν

ν2 −a2
0v2
sinh

ν2 −a2
0v2t
⎫
⎪
⎬
⎪
⎭
.
(5.42)
One can easily see that this expression is the one-time characteristic function
of random process ξ(t) =
 t
0
dτz(τ).
Now, we dwell on an important limit theorem concerning telegrapher’s
random processes.
Remark 5.4. Limiting
Theorem
for
Telegrapher’S
Random
Processes.

140
5
Random Processes and Their Characteristics
Consider the random process
ξN(t) = z1(t) + · · · + zN(t),
where all zk(t) are statistically independent telegrapher’s processes with zero-
valued means and correlation functions
⟨z(t)z(t + τ)⟩= σ2
N e−α|τ|.
In this case, the characteristic functional of process zk(t) satisﬁes Eq. (5.40)
d
dtΦ[t; v(τ)] = −σ2
N v(t)
t

0
dt1e−α(t−t1)v(t1)Φ[t1; v(τ)],
from which follows that Φ[t; v(τ)] →1 for N →∞.
For the characteristic functional of random process ξN(t), we have the
expression
ΦN[t; v(τ)] =
4
exp
⎧
⎨
⎩i
t

0
dτξN(τ)v(τ)
⎫
⎬
⎭
5
= {Φ[t; v(τ)]}N .
Consequently, it satisﬁes the equation
d
dt ln ΦN[t; v(τ)] = −σ2v(t)
t

0
dt1e−α(t−t1)v(t1)Φ[t1; v(τ)]
Φ[t; v(τ)] .
In the limit N →∞, we obtain the equation
d
dt ln Φ∞[t; v(τ)] = −σ2v(t)
t

0
dt1e−α(t−t1)v(t1),
which means that process ξ(t) = limN→∞ξN(t) is the Gaussian random pro-
cess with the exponential correlation function
⟨ξ(t)ξ(t + τ)⟩= σ2e−α|τ|,
i.e., the Gaussian Markovian process (see Sect. 4.3). Thus, process ξN(t) for
ﬁnite N is the ﬁnite-number-of-states process approximating the Gaussian
Markovian process. This approximation appears practicable for studying var-
ious functions of the Gaussian Markovian processes rather than only the
Gaussian Markovian processes by themselves. As an example, for process
z(t) = x2(t) −
,
x2(t)
-
,

5.4
Discontinuous Random Processes
141
where x(t) is the Gaussian Markovian process with the exponential correla-
tion function, the ﬁnite-series approximation assumes the form
zN(t) =
N
&
i̸=j=1
zi(t)zj(t),
z(t) = lim
N→∞zN(t).
This representation is much more convenient for analyzing stochastic equa-
tions than the immediate use of processes x(t) and z(t).
♦
5.4.3
Generalized Telegrapher’S Random Process
Consider now generalized telegrapher’s process deﬁned by the formula
z(t) = an(0,t).
(5.43)
Here, n(0, t) is the sequence of integers described above and quantities ak are
assumed statistically independent with distribution function p(a). Figure 5.4
shows a possible realization of such a process.
z(t)
t
a1
a2
a3
a4
a5
a6
a7
a8
a0
Fig. 5.4 A possible realization of generalized telegrapher’s random process
For process z(t), we have
⟨z(t)⟩=
∞
&
k=0
,
akδk,n(0,t)
-
= ⟨a⟩,

142
5
Random Processes and Their Characteristics
⟨z(t1)z(t2)⟩=
∞
&
k=0
∞
&
k=0
⟨akal⟩
,
δk,n(0,t1)δl,n(0,t2)
-
=
,
a2-
' ∞
&
k=0
,
δk,n(0,t1)
- ,
δ0,n(t2,t1)
-
+ 1 −
∞
&
k=0
,
δk,n(0,t2)
- ,
δ0,n(t2,t1)
-
(
=
,
a2-
e−ν(t1−t2) +
,
a2- 
1 −e−ν(t1−t2)
(t1 ⩾t2),
and so on. In addition, the probability of absence of points of discontinuity
in interval (t2, t1) is given by the formula
Pn(t2,t1)=0 =
,
δ0,n(t2,t1)
-
= e−ν|t1−t2|.
For such a process, no relationship similar to Eq. (5.37) can be obtained,
and derivation of the equation for the characteristic functional is essentially
based on the fact tat this process is the Markovian process. The resulting
equation is the integro-diﬀerential equation
Φ[t, v(τ)] =
4
exp
⎧
⎨
⎩ia
t

0
dτv(τ)
⎫
⎬
⎭
5
e−νt
+ ν
t

0
dt1e−ν(t−t1)
4
exp
⎧
⎨
⎩ia
t

t1
dτv(τ)
⎫
⎬
⎭
5
Φ[t1, v(τ)].
(5.44)
The ﬁrst term in the right-hand side of Eq. (5.44) corresponds to the absence
of points of discontinuity in interval (0, t), and the second term corresponds
to the situations in which the number of points of discontinuity in interval
(0, t) can vary from one to inﬁnity. Here, time t1 is the instance at which the
last point of discontinuity appears.
Note that, for probability density of the form
p(a) = 1
2[δ(a −a0) + δ(a + a0)],
Eq. (5.44) coincides (after replacing ν with ν/2) with the equation for teleg-
rapher’s process. This fact is quite expectable, because, unlike telegrapher’s
process, the process z(t) considered here can change the sign at a point of
discontinuity with a probability of 1/2, which just results in doubling the
mean time between the discontinuities.
Earlier, we noted that the Poisson stream of points and processes based on
these streams are the Markovian processes. Below, we consider this important
class of random processes in detail.

5.5
Markovian Processes
143
5.5
Markovian Processes
5.5.1
General Properties
In the foregoing section, we considered the characteristic functional that des-
cribes all statistical characteristics of random process z(t). Speciﬁcation of
the argument of the functional in the form
v(t) =
n
&
k=1
vkδ(t −tk)
transforms the characteristic functional into the joint characteristic function
of random quantities zk = z(tk)
Φn(v1, · · · , vn) =
4
exp
'
i
n
&
k=1
vkz(tk)
(5
,
whose Fourier transform is the joint probability density of process z(t) at
discrete instants
Pn(z1, t1; · · · ; zn, tn) = ⟨δ(z(t1) −z1) · · · δ(z(tn) −zn)⟩.
(5.45)
Assume that the above instants are ordered according to the line of in-
equalities
t1 ≥t2 ≥· · · ≥tn.
Then, by deﬁnition of the conditional probability, we have
Pn(z1, t1; · · · ; zn, tn) = pn(z1, t1|z2, t2; · · · ; zn, tn)Pn−1(z2, t2; · · · ; zn, tn),
(5.46)
where pn is the conditional probability density of the value of process z(t) at
instant t1 under the condition that function z(t) was equal to zk at instants
tk for k = 2, · · · , n (z(tk) = zk, k = 2, · · · , n). If process z(t) is such that the
conditional probability density for all t1 > t2 is unambiguously determined
by the value z2 of the process at instant t2 and is independent of the previous
history, i.e., if
pn(z1, t1|z2, t2; · · · ; zn, tn) = p(z1, t1|z2, t2),
(5.47)
then this process is called the Markovian process, or the memoryless process.
In this case, function
p(z, t|z0, t0) = ⟨δ(z(t) −z)|z(t0) = z0⟩
(t > t0)
(5.48)
is called the transition probability density. Setting t = t0 in Eq. (5.48), we
obtain the equality

144
5
Random Processes and Their Characteristics
p(z, t0|z0, t0) = δ(z −z0).
Substituting expression (5.46) in Eq. (5.45), we obtain the recurrence for-
mula for the n-time probability density of process z(t). Iterating this formula,
we ﬁnd the relationship of probability density Pn with the one-time probabi-
lity density (t1 ≥t2 ≥· · · ≥tn)
Pn(z1, t1; · · · ; zn, tn) = p(z1, t1|z2, t2) · · · p(zn−1, tn−1|zn, tn)P(tn, zn).
(5.49)
Thus, only two functions — transition probability density p(z, t|z0, t0) and
one-time probability density P(t, z) — are suﬃcient to exhaustively des-
cribe all statistical characteristics of the Markovian process z(t). It appears
that transition probability density as a function of its arguments satisﬁes
the nonlinear integral equation called the Smolukhovsky equation (or the
Kolmogorov–Chapman equation). In the context of the derivation of this equa-
tion, we note the following fact: if process z(t) assumes values z(t0) = z0,
z(t1) = z1, z(t) = z at ﬁxed instants t0 < t1 < t, then the coordination
condition
∞

−∞
dz1P3(z, t; z1, t1; z0, t0) = P2(z, t; z0, t0).
(5.50)
holds. Substituting now P3 and P2 expressed in the form of Eq. (5.49) in
Eq. (5.50), we obtain the desired equation
p(z, t; z0, t0) =
∞

−∞
dz1p(z, t|z1, t1)p(z1, t1; z0, t0).
(5.51)
Integrating Eq. (5.51) over z0, we obtain the linear integral equation for the
one-time probability density P(z, t)
P(z, t) =
∞

−∞
dz1p(z, t|z1, t1)P(z1, t1).
(5.52)
Integral equations (5.51) and (5.52) oﬀer a possibility of deriving dif-
ferential or integro-diﬀerential equation for simple Markovian processes.
The simplest Markovian processes with continuous time can be classiﬁed as
follows:
1) discrete processes,
2) continuous processes, and
3) discrete-continuous processes that can undergo discontinuous varia-
tions at certain instants and behave as continuous processes between these
instants.

5.5
Markovian Processes
145
Discrete Markovian Process
Consider the discrete Markovian process z(t). This assumes that the process
can take on only discrete values z1, · · · , zn and switching between the val-
ues occurs at random time instants. We introduce the transition probability
density
pij(t, t0) = ⟨δ(z(t) −zi)|z(t0) = zj⟩,
&
i
pij(t, t0) = 1
(t0 < t),
(5.53)
which is the conditional probability of the event that process z(t) assumes
value zi at instant t under the condition that its value at instant t0 was zj.
It is obvious that
pij(t0, t0) = δij.
(5.54)
For short temporal intervals Δt →0, we have
pij(t + Δt, t) = δij + aij(t)Δt + o(Δt),
(5.55)
where aij(t)Δt is the transition probability from state zj at instant t to state
zi during time Δt. It is assumed that
aij(t) > 0
(i ̸= j),
ajj(t) = −
&
i(i̸=j)
aij(t),
(5.56)
because normalization condition (5.53) must hold.
Using Eq. (5.55), one can easily show from the Smolukhovsky equation
(5.51) that probability pij(t, t0) satisﬁes the system of linear differential equa-
tions
d
dtpij(t, t0) =
n
&
k=1
aik(t)pkj(t, t0)
(i, j = 1, · · · , n).
(5.57)
Representing the one-time probability Pi(t) in the form
Pi(t) =
&
j
pij(t, t0)p0
j,
(5.58)
where p0
j are the initial probabilities of states (p0
j = Pj(t0)), we obtain that
this one-time probability satisﬁes the system of equations
d
dtPi(t) =
n
&
k=1
aik(t)Pk(t),
Pi(t0) = p0
i .
(5.59)
Consider three examples as illustrations of the above consideration.
1. Let random process z(t) = n(0, t) represents the number of discontinu-
ities occurred in interval (0, t) at random instants (see Fig. 5.2 for a possible
realization of this process). It is assumed that process z(t) takes on only

146
5
Random Processes and Their Characteristics
integer values 0, 1, 2, · · · , and it is obvious that
pij(t, t0) = 0
for
i < j,
t ≥t0.
Assuming additionally that, in temporal interval (t, t + Δt), the probability
of one change of state is νΔt + o(Δt) and the probability of the absence of
discontinuities is 1 −νΔt + o(Δt) and neglecting the possibility of two and
more changes of state in this interval (these assumptions are just the assump-
tions that govern the Poisson stream of instants at which the discontinuities
appear), we can write the system of equations (5.59) for this process. In the
case under consideration, this system assumes the form
d
dtP0(t) = −νP0(t),
P0(0) = 1,
d
dtPi(t) = −ν [Pi(t) −Pi−1(t)] ,
Pi̸=0(0) = 0
(5.60)
and coincides with the system of equations (5.25) at i = n.
2. As the second example, we consider the simplest Markovian process
with the ﬁnite number of states, namely, telegrapher’s random process that
can take on only two values z(t) = ±a. In the foregoing section, we considered
this process from another viewpoint. Here, we assume that the probabilities
of transitions (a →−a) and (−a →a) during short interval Δt coincide and
are νΔt + o(Δt), the corresponding probabilities of state preservation during
interval Δt are 1 −νΔt + o(Δt), and probabilities of initial states are p0
a and
p0
−a = 1 −p0
a. In this case, the transition probabilities satisfy the system of
equations (5.57) with parameters
a11 = a22 = −ν,
a12 = a21 = ν.
The solution of this system is (τ = t −t0)
p11(τ) = p22(τ) = 1
2

1 + e−2ντ
,
p12(τ) = p21(τ) = 1
2

1 −e−2ντ
.
(5.61)
Expressions for the one-time probabilities are obtained similarly:
P1(τ) = 1
2 +

p0
a−1
2

e−2ντ,
P2(τ) = 1
2 −

p0
a−1
2

e−2ντ.
(5.62)
If process z(t) had at the initial instant the ﬁxed value z(t0) = a, then
p0
a = 1 and Eqs. (5.62) assume the form
P1(τ) = 1
2

1 + e−2ντ
,
P2(τ) = 1
2

1 −e−2ντ
.
(5.63)
For t →∞, these probability distributions tend to steady-state values
P1,2(∞) = 1/2 and the process behavior tends to steady-state regime.

5.5
Markovian Processes
147
If p0
a = p0
−a = 1/2 at the initial instant, the process z(t) is always steady.
Note that, in the case of telegrapher’s process, formulas (5.61) can be
combined in one formula; namely,
p(z, t|z0, t0) = δ(z −z0)P1(τ) + δ(z + z0)P2(τ),
(5.64)
where P1(τ) and P2(τ) are given by Eqs. (5.63) and τ = t−t0. Differentiating
Eq. (5.64) with respect to time, we obtain the equation for the transition
probability density p(z, t|z0, t0)
∂
∂tp(z, t|z0, t0) = −ν {p(z, t|z0, t0) −p(−z, t|z0, t0)}
(5.65)
with the initial value p(z, t0|z0, t0) = δ(z −z0).
Thus, the transition probability density p(z, t|z0, t0) of telegrapher’s pro-
cess satisﬁes the linear operator equation
∂
∂tp(z, t|z0, t0) = %L(z)p(z, t|z0, t0),
(5.66)
where operator %L(z) is deﬁned by the equality
%L(z)f(z) = −ν {f(z) −f(−z)} .
(5.67)
Note that this is the property characteristic of all Markovian processes.
However, the equation for the transition probability density not always allows
the compact representation such as (5.66). In the general case of arbitrary
Markovian process with a ﬁnite number of states, operator %L(z) is matrix
||aij|| appeared in Eq. (5.57) and probability density p(z, t|z0, t0) itself is
the matrix function. In this case, any realization of process z(t) satisﬁes the
identity
[z(t) −z1][z(t) −z2] · · · [z(t) −zn] = 0.
(5.68)
Opening the brackets in Eq. (5.68), we see that diﬀerent powers of process
z(t) satisfy the algebraic relationship
zn(t) = (z1 + · · · + zn)zn−1(t) + · · · + (−1)n+1z1z2 · · · zn.
(5.69)
In the case of telegrapher’s random process, i.e., the process with two possible
states z(t) = ±a, identity (5.69) reduces to z2(t) = a2, which appears to be
very useful for analyzing stochastic equations whose parameters ﬂuctuate by
the law of telegrapher’s process.
3. Consider now generalized telegrapher’s process as an example of the
spasmodic process. This process is deﬁned by Eq. (5.43), and its transition
probability density has the form

148
5
Random Processes and Their Characteristics
p(z, t|z0, t0) = ⟨δ(z(t) −z)|z(t0) = z0⟩
= δ(z −z0)Pn(0,t)=0 + ⟨δ(z −a)⟩a
0
Pn(0,t)=1 + Pn(0,t)=2 + · · ·
1
.
(5.70)
Taking into account the normalization condition
∞
&
n=0
Pn(0,t)=n = 1, we obtain
the ﬁnal expression in the form
p(z, t|z0, t0) = δ(z −z0)P0(t, t0) + pa(z) {1 −P0(t, t0)} ,
(5.71)
where P0(t, t0) = e−ν(t−t0) is the probability of absence of jumps within
temporal interval (t0, t) and pa(z) is the probability of the event that random
quantity a assumes value z.
The one-time probability distribution of process z(t) is obviously the
steady-state distribution
P(z, t) = pa(z).
(5.72)
It is obvious that quantity (5.71) satisﬁes, as a function of variable t, the
diﬀerential equation
∂
∂tp(z, t|z0, t0) = −ν {p(z, t|z0, t0) −pa(z)} ,
(5.73)
which can be rewritten in the operator form
∂
∂tp(z, t|z0, t0) = %L(z)p(z, t|z0, t0),
(5.74)
where %L(z) in this particular case is the integral operator
%L(z)f(z) = −ν
⎧
⎨
⎩f(z) −pa(z)
∞

−∞
dz′f(z′)
⎫
⎬
⎭.
(5.75)
Continuous Markovian Processes
Consider now continuous Markovian processes. In this case, the transition
probability density p(z, t|z0, t0) satisﬁes the operator equation (this equation
is a consequence of the Smolukhovsky equation (5.51))
∂
∂tp(z, t|z0, t0) =
∞
&
n=1
(−1)n
n!
∂n
∂zn [Bn(z, t)p(z, t|z0, t0)] ,
(5.76)
where functions Bn(z, t) are determined by the equalities

5.5
Markovian Processes
149
Bn(z, t) = lim
Δt→0
1
Δt ⟨{z(t + Δt) −z(t)}n |z(t)⟩
= lim
Δt→0
1
Δt
∞

−∞
dz {z(t + Δt) −z(t)}n p(z, t + Δt|z, t).
(5.77)
A consequence of Eq. (5.76) is the similar equation for the one-time probabi-
lity density P(z, t) = ⟨δ(z(t) −z)⟩:
∂
∂tP(t, z) =
∞
&
n=1
(−1)n
n!
∂n
∂zn [Bn(z, t)P(t, z)] .
Continuous processes for which all coeﬃcients Bn in Eq. (5.76) with n ≥3
vanish form an important particular class. Markovian processes having this
property are called the diﬀusion processes. In the context of such processes,
Eq. (5.76) assumes the form
∂
∂tp(z, t|z0, t0) = −∂
∂z [B1(z, t)p(z, t|z0, t0)] + 1
2
∂2
∂z2 [B2(z, t)p(z, t|z0, t0)] .
(5.78)
This equation is called the Fokker–Planck equation and functions B1(z, t)
and B2(z, t) are called the drift and diﬀusion coeﬃcients, respectively.
In the particular case of B2(z, t) = const and B1(z, t) = −B1z, Markovian
process z(t) is the Gaussian process with the exponential correlation function
⟨z(t)z(t + τ)⟩= σ2(t)e−B1|τ|.
In this case Eq. (5.78) is replaced with the equation
∂
∂tp(z, t|z0, t0) = %L(z)p(z, t|z0, t0),
where operator
%L(z) = B1
∂
∂z z + 1
2B2
∂2
∂z2 .
(5.79)
Note that the converse is also valid; namely, any Gaussian process with the
exponential correlation function is the Markovian process.
Discrete-Continuous Markovian Processes
Consider now the one-dimensional discrete-continuous Markovian process.
Two cases are possible here: the case of a purely discontinuous (spasmodic)
process and the case of a process varying both continuously and discontinu-
ously. In the ﬁrst case, two functions — q(z, t) and u(z, z′, t) — characterize
random process z(t). The meaning of these functions is as follows: within the

150
5
Random Processes and Their Characteristics
short temporal interval (t, t + Δt), the probability for the process to preserve
its previous value is 1−q(z, t)Δt and the probability for the process to change
its value from z to z′′ is u(z, z′, t)ΔtΔz′ (here, z′ < z′′ < z′+Δz′). Of course,
the normalization condition
∞

−∞
dz′u(z, z′, t) = q(z, t).
(5.80)
is assumed additionally. For this process, a consequence of the Smolukhovsky
equation (5.51) is the integro-diﬀerential equation
∂
∂tp(z, t|z0, t0) = −q(z, t)p(z, t|z0, t0) +
∞

−∞
dz′u(z, z′, t)p(z′, t|z0, t0), (5.81)
which is called the Kolmogorov–Feller equation. The equation for the one-
time probability density has the similar form.
If, in addition to jumps, the process allows continuous variation, then the
right-hand side of Eq. (5.81) is added with the right-hand side of Eq. (5.78).
Note that random process z(t) = n(0, t) (the number of jumps within tem-
poral interval (0, t)) that we considered earlier is a special case of spasmodic
processes; correspondingly, diﬀerence-diﬀerential equations (5.60) are special
cases of the integro-diﬀerential equation (5.81).
It is obvious that Eq. (5.74) for generalized telegrapher’s process is the Kol-
mogorov–Feller equation (5.81) with specially deﬁned parameters q(z, t) = ν
and u(z, z′, t) = νpa(z).
Up to this point, we dealt with the one-dimensional processes; it is clear
however that all results remain valid for multidimensional processes, i.e., for
vector random functions z(t). In particular, the transition probability density
p(z, t|z0, t0) = ⟨δ(z(t) −z)|z(t0) = z0)⟩
will satisfy the linear operator equation
∂
∂tp(z, t|z0, t0) = %L(z)p(z, t|z0, t0).
(5.82)
Note additionally that transition probability density p(z, t|z0, t0) satisﬁes,
as a function of its arguments, not only Eq. (5.82) (we will call this equation
the forward equation), but also the equation with respect to variable t0
∂
∂t0
p(z, t|z0, t0) = %L+(z0)p(z, t|z0, t0),
(5.83)

5.5
Markovian Processes
151
which we will call the backward equation. Here, %L+(z0) is the operator conju-
gated to operator %L(z). Equation (5.83) is convenient for analyzing the prob-
lems that deal with dependencies on initial locations of space-time points.
We mentioned earlier that two functions — transition probability density
p(z, t|z0, t0) and one-time probability density P(z, t) — are suﬃcient to ex-
haustively describe all statistical characteristics of the Markovian process
z(t). Nevertheless, statistical analysis of stochastic equations requires additi-
onally the knowledge of the characteristic functional of random process z(t).
5.5.2
Characteristic Functional of the Markovian
Process
For the Markovian process z(t), no closed equation can be derived in the
general case for the characteristic functional Φ[t; v(τ)] = ⟨ϕ[t; v(τ)]⟩, where
ϕ[t; v(τ)] = exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭.
Instead, we can derive the closed equation for the functional
Ψ[z, t; v(τ)] = ⟨δ(z(t) −z)ϕ[t; v(τ)]⟩
(5.84)
describing correlations of process z(t) with its prehistory. The characteris-
tic functional Φ[t; v(τ)] can be obtained from functional Ψ[z, t; v(τ)] by the
formula
Φ[t; v(τ)] =
∞

−∞
dzΨ[z, t; v(τ)].
(5.85)
To derive the equation for functional Ψ[z, t; v(τ)], we note that the follow-
ing equality
ϕ[t; v(τ)] = 1 + i
t

0
dt1z(t1)v(t1)ϕ[t1; v(τ)]
(5.86)
holds. Substituting Eq. (5.86) in Eq. (5.84), we obtain the expression
Ψ[z, t; v(τ)] = P(z, t) + i
t

0
dt1v(t1) ⟨δ(z(t) −z)z(t1)ϕ[t1; v(τ)]⟩,
(5.87)
where P(z, t) = ⟨δ(z(t) −z)⟩is the one-time probability density of random
quantity z(t).
We rewrite Eq. (5.87) in the form

152
5
Random Processes and Their Characteristics
Ψ[z, t; v(τ)] = P(z, t) + i
t

0
dt1v(t1)
∞

−∞
dz1z1 ⟨δ(z(t) −z)δ(z(t1) −z1)ϕ[t1; v(τ)]⟩.
(5.88)
Taking into account the fact that process z(t) is the Markovian process, we
can perform averaging in (5.88) to obtain the closed integral equation
Ψ[z, t; v(τ)] = P(z, t) + i
t

0
dt1v(t1)
∞

−∞
dz1z1p(z, t|z1, t1)Ψ[z1, t1; v(τ)],
(5.89)
where p(z, t|z1, t1) is the transition probability density.
We note that the integral equation similar to Eq. (5.89) can be derived also
for the functional
Ψ[z, t′, t; v(τ)] = ⟨δ(z(t′) −z)ϕ[t; v(τ)]⟩
(t′ ≥t).
(5.90)
This equation has the form
Ψ[z, t′, t; v(τ)] = P(z, t′) + i
t

0
dt1v(t1)
∞

−∞
dz1z1p(z, t′|z1, t1)Ψ[z1, t1; v(τ)].
(5.91)
Integrating Eq. (5.89) with respect to z, we obtain an additional re-
lationship between the characteristic functional Φ[t; v(τ)] and functional
Ψ[z, t; v(τ)]. This relationship has the form
1
iv(t)
d
dtΦ[t; v(τ)] =
∞

−∞
dz1z1Ψ[z1, t; v(τ)] = Ψ[t; v(τ)].
(5.92)
Multiplying Eq. (5.89) by z and integrating the result over z, we obtain
the relationship between functionals Ψ[t; v(τ)] and Ψ[z, t; v(τ)]
Ψ[t; v(τ)] = ⟨z(t)⟩+ i
t

0
dt1v(t1)
∞

−∞
dz1 ⟨z(t)|z1, t1⟩Ψ[z1, t1; v(τ)].
(5.93)
Equation (5.89) is in the general case a complicated integral equation
whose explicit form depends on functions P(z, t) and p(z, t|z1, t1), i.e., on
parameters of the Markovian process. Preliminarily diﬀerentiating this equa-
tion with respect to t and using Eq. (5.66), we can convert it into the integro-
diﬀerential equation

5.5
Markovian Processes
153
d
dtΨ[z, t; v(τ)] = izv(t)Ψ[z, t; v(τ)] + %L(z)Ψ[z, t; v(τ)],
Ψ[z, 0; v(τ)] = P(z, 0).
(5.94)
In this case, functional Ψ[z, t′, t; v(τ)] (5.90) as a function of variable t′ satis-
ﬁes the equation with the initial value at t′ = t
d
dt′ Ψ[z, t′, t; v(τ)] = %L(z)Ψ[z, t′, t; v(τ)]
(t′ > t),
Ψ[z, t, t; v(τ)] = Ψ[z, t; v(τ)].
(5.95)
Thus, Eq. (5.94) together with Eqs. (5.92) and (5.93) forms the starting
point for the determination of the characteristic functional of the Markovian
process.
We demonstrate this fact using the processes considered earlier as
examples.
For telegrapher’s process, Eq. (5.64) gives
⟨z(t)|z1, t1⟩= z1e−2ν(t−t1),
⟨z(t)⟩= 0,
and we obtain Eq. (4.24).
Consider now generalized telegrapher’s process. By virtue of Eq. (5.75),
Eq. (5.94) for functional Ψ[t, z; v(τ)] assumes in this case the form
d
dtΨ[z, t; v(τ)] = {izv(t) −ν} Ψ[z, t; v(τ)] + νpa(z)Φ[t; v(τ)],
Ψ[z, 0; v(τ)] = pa(z).
(5.96)
Deriving Eq (5.96), we used equality (5.85). Solving Eq. (5.96) in functional
Ψ[z, t; v(τ)], we relate it to the characteristic functional
Ψ[z, t; v(τ)] = pa(z) exp
⎧
⎨
⎩−νt + iz
t

0
dτv(τ)
⎫
⎬
⎭
+ νpa(z)
t

0
dt1Φ[t1; v(τ)] exp
⎧
⎨
⎩−ν(t −t1) + iz
t

t1
dτv(τ)
⎫
⎬
⎭.
(5.97)
Integrating Eq. (5.97) over z, we obtain the closed integral equation for the
characteristic functional Φ[t, v(τ)]

154
5
Random Processes and Their Characteristics
Φ[t, v(τ)] =
4
exp
⎧
⎨
⎩ia
t

0
dτv(τ)
⎫
⎬
⎭
5
a
e−νt
+ ν
t

0
dt1e−ν(t−t1)
4
exp
⎧
⎨
⎩ia
t

t1
dτv(τ)
⎫
⎬
⎭
5
a
Φ[t1, v(τ)],
(5.98)
which coincides with Eq. (5.44).
Multiplying Eq. (5.97) by arbitrary function F(z) and integrating the result
over z, we obtain the equality
4
F (z(t)) exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
=
4
F(a) exp
⎧
⎨
⎩ia
t

0
dτv(τ)
⎫
⎬
⎭
5
a
e−νt
+ ν
t

0
dt1e−ν(t−t1)
4
F(a) exp
⎧
⎨
⎩ia
t

t1
dτv(τ)
⎫
⎬
⎭
5
a
Φ[t1, v(τ)].
(5.99)
In the particular case of F(z) = z, Eq. (5.99) can be reduced to the integro-
diﬀerential equation for the characteristic functional Φ[t, v(τ)]
d
iv(t)dtΦ[t, v(τ)] =
4
a exp
⎧
⎨
⎩ia
t

0
dτv(τ)
⎫
⎬
⎭
5
a
e−νt
+ ν
t

0
dt1e−ν(t−t1)
4
a exp
⎧
⎨
⎩ia
t

t1
dτv(τ)
⎫
⎬
⎭
5
a
Φ[t1, v(τ)],
(5.100)
which is equivalent to Eq. (5.98).
In the case of generalized telegrapher’s process, we can additionally estab-
lish the relationship between functionals Ψ[z, t′, t; v(τ)] and Ψ[z, t; v(τ)]. This
relationship has the form (t′ ≥t)
Ψ[z, t′, t; v(τ)] = Ψ[z, t; v(τ)]e−ν(t′−t) + pa(z)Φ[t; v(τ)][1 −e−ν(t′−t)]. (5.101)

Chapter 6
Random Fields
For random ﬁeld f(x, t), the one- and n-point probability densities are deﬁned
similarly to those for random processes
P(x, t; f) = ⟨ϕ(x, t; f)⟩,
P(x1, t1, f1; · · · ; xn, tn, fn) = ⟨ϕ(x1, t1, f1; · · · ; xn, tn, fn)⟩,
(6.1)
where the indicator functions are deﬁned as follows:
ϕ(x, t; f) = δ(f(x, t) −f),
ϕ(x1, t1, f1; · · · ; xn, tn, fn) = δ (f(x1, t1) −f1) · · · δ (f(xn, tn) −fn) .
(6.2)
For clarity, we use here variables x and t as spatial and temporal coordinates;
however, in many physical problems, some preferred spatial coordinate can
play the role of the temporal coordinate.
Random ﬁeld f(x, t) is called the spatially homogeneous ﬁeld if all its sta-
tistical characteristics are invariant relative to spatial translations by arbit-
rary vector a, i.e., if
P(x1 + a, t1, f1; · · · ; xn + a, tn, fn) = P(x1, t1, f1; · · · ; xn, tn, fn).
In this case, the one-point probability density P(x, t; f) = P(t; f) is indepen-
dent of x, and the spatial correlation function Bf(x1, t1; x2, t2) depends on
the diﬀerence x1 −x2:
Bf(x1, t1; x2, t2) = ⟨f(x1, t1)f(x2, t2)⟩= Bf(x1 −x2; t1, t2).
If random ﬁeld f(x, t) is additionally invariant with respect to rotation of
all vectors xi by arbitrary angle, i.e., with respect to rotations of the reference
system, then ﬁeld f(x, t) is called the homogeneous isotropic random ﬁeld.
In this case, the correlation function depends only on length |x1 −x2|:
Bf(x1, t1; x2, t2) = ⟨f(x1, t1)f(x2, t2)⟩= Bf(|x1 −x2|; t1, t2).
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
155
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_6, c
⃝Springer International Publishing Switzerland 2015

156
6
Random Fields
The corresponding Fourier transform of the correlation function with res-
pect to the spatial variable deﬁnes the spatial spectral function (called also
the angular spectrum)
Φf(k, t) =

dxBf(x, t)eikx,
and the Fourier transform of the correlation function of random ﬁeld f(x, t)
stationary in time and homogeneous in space deﬁnes the space-time spectrum
Φf(k, ω) =

dx
∞

−∞
dtBf(x, t)ei(kx+ωt).
In the case of isotropic random ﬁeld f(x, t), the space-time spectrum ap-
pears isotropic in the k-space:
Φf(k, ω) = Φf(k, ω).
The characteristic functional and the n-th order cumulant functions of
scalar random ﬁeld f(x, t) are deﬁned similarly
Φ[v(x′, τ)] =
4
exp
⎧
⎨
⎩i

dx
∞

−∞
dtv(x,t)f(x, t)
⎫
⎬
⎭
5
= exp {Θ[v(x′, τ)]} ,
Mn(x1, t1, · · · , xn, tn) = 1
in
δn
δv(x1, t1) · · · δv(xn, tn)Φ[v(x′, τ)]

v=0
,
Kn(x1, t1, · · · , xn, tn) = 1
in
δn
δv(x1, t1) · · · δv(xn, tn)Θ[v(x′, τ)]

v=0
.
In the case of vector random ﬁeld f(x, t), we must assume that v(x,t) is
the vector function.
6.1
Gaussian Vector Random Field
Consider now the basic space–time statistical characteristics of the vector
Gaussian random ﬁeld u(r, t) with zero-valued mean and correlation tensor
Bij(r, t; r′, t′) = ⟨ui(r, t)uj(r′, t′)⟩.
For deﬁniteness, we will identify this vector ﬁeld with the velocity ﬁeld.
In the general case, random ﬁeld u(r, t) is assumed to be the divergent
(div u(r, t) ̸= 0) Gaussian ﬁeld statistically homogeneous and possessing

6.1
Gaussian Vector Random Field
157
spherical symmetry (but not possessing reﬂection symmetry) in space and
stationary in time with correlation and spectral tensors (τ = t −t1)
Bij(r −r1, τ) = ⟨ui(r, t)uj(r1, t1)⟩=

dk Eij(k, τ)eik(r−r1),
Eij(k, τ) =
1
(2π)d

dr Bij(r, τ)e−ikr,
where d is the dimension of space. In view of the assumed symmetry con-
ditions, correlation tensor Bij(r −r1, τ) has the structure [7] (r −r1 →r)
Bij(r, τ) = Biso
ij (r, τ) + C(r, τ)εijkrk,
(6.3)
where the isotropic portion of the correlation tensor is expressed as follows
Biso
ij (r, τ) = A(r, τ)rirj + B(r, τ)δij.
Here, εijk is the pseudotensor described in Sect. 1.3.1, page 39. This pseu-
dotensor is used, for example, to determine the velocity vortex ﬁeld
ω(r, t) = curl u(r, t) = [∇× u(r, t)]
(6.4)
whose components have the form
ωi(r, t) = εijk
∂uk(r, t)
∂rj
.
Note that, in the two-dimensional case, vector ω(r, t) has a single compo-
nent orthogonal to velocity ﬁeld u(r, t); as a result, quantity u(r, t) · ω(r, t)
called velocity ﬁeld helicity vanishes,
u(r, t) · ω(r, t) = 0.
Expression (1.72), page 39 allows obtaining the following representations
for the vortex ﬁeld correlation and spectral tensors [7]:
⟨ωi(r, t)ωj(r′, t′)⟩= −δijΔrBll(r −r′, t −t′)
+ ∂2Bll(r −r′, t −t′)
∂ri∂rj
+ ΔrBij(r −r′, t −t′),
Ωij(k, t −t′) = (δijk2 −kikj)Ell(k, t −t′) −k2Eij(k, t −t′).
Convolving them with respect to indices i and j and setting r = r′, we obtain
the expressions for the correlation and spectral functions of the vortex ﬁeld

158
6
Random Fields
⟨ωi(r, t)ωi(r, t′)⟩= −ΔrBll(0, t −t′) = −⟨u(r, t)Δru(r, t′)⟩,
Ωii(k, t −t′) = k2Eii(k, t −t′).
(6.5)
Note that the procedure similar to that used to calculate vortex ﬁeld vari-
ance and spectrum allows additionally calculating average helicity of the velo-
city ﬁeld
χ(r, t) = u(r, t) · ω(r, t) = u(r, t) · curl u(r, t)
in the three-dimensional case in the absence of reﬂection symmetry. Indeed,
the use of Eq. (1.73), page 39 results in the following expression for statisti-
cally averaged quantity ⟨ωi(r, t)uj(r, t)⟩
'
ωi(r, t)up(r′, t)
(
r=r′ = εijk
)∂uk(r, t)
∂rj
up(r′, t)
*
r=r′
= εijk ∂
∂rj Bkp(r, 0)r=0
= lim
r→0 εijk ∂
∂rj C(r, 0)εkpmrm = C(0, 0)εijmεmpj = 2C(0, 0)δip
and, hence, the average helicity of the Gaussian random ﬁeld in the three-
dimensional case is
⟨χ(r, t)⟩= 6C(0, 0).
(6.6)
The isotropic portion of correlation tensor corresponds to the spatial spec-
tral tensor of the form
Eij(k, τ) = Es
ij(k, τ) + EP
ij(k, τ),
where the spectral components of the tensor of velocity ﬁeld have the follow-
ing structure
Es
ij(k, τ) = Es(k, τ)

δij −kikj
k2

,
Ep
ij(k, τ) = Ep(k, τ)kikj
k2 .
Here, Es(k, τ) and Ep(k, τ) are the solenoidal and potential components of
the spectral density of velocity ﬁeld, respectively.
Deﬁne now function Bij(r) as the integral of correlation function (6.3)
over time, i.e.,
Bij(r) =
∞

0
dτBij(r, τ) = Biso
ij (r) + C(r)εijkrk.
(6.7)
Then, Bij(0) = D0δij and, hence, quantity
Bii(0) = D0d = τ0σ2
u =

dk [(d −1) Es(k) + Ep(k)]
(6.8)

6.1
Gaussian Vector Random Field
159
deﬁnes temporal correlation radius of the velocity ﬁeld τ0. Here, σ2
u =
Bii(0, 0) =
,
u2(r, t)
-
is the variance of the velocity ﬁeld, and functions Es(k)
and Ep(k) are deﬁned as follows
Es(k) =
∞

0
dτ Es(k, τ),
Ep(k) =
∞

0
dτ Ep(k, τ).
(6.9)
The use of correlation function (6.7) appreciably simpliﬁes calculations
dealing with spatial derivatives of the velocity ﬁeld. Indeed, we have
∂Bij(0)
∂rk
= C(0)εijk,
(6.10)
−∂2Bij(0)
∂rk∂rl
=
Ds
d(d + 2) [(d + 1)δklδij −δkiδlj −δkjδli]
+
Dp
d(d + 2)

δklδij + δkiδlj + δkjδli

,
(6.11)
∂3Bkp(0)
∂rn∂rm∂rj
= −2α (εkpjδnm + εkpmδnj + εkpnδmj) ,
(6.12)
and, consequently,
∂3Bkp(0)
∂r2∂rj
= −2α(d + 2)εkpj,
where
Ds =

dk k2Es(k) =
1
d −1
∞

0
dτ ⟨ω(r, t + τ)ω(r, t)⟩,
Dp =

dk k2Ep(k) =
∞

0
dτ
.∂u(r, t + τ)
∂r
∂u(r, t)
∂r
/
,
C(r) = C(0) −αr2,
(6.13)
and ω(r, t) is the velocity ﬁeld vortex.
As we noted earlier, characteristic functionals ensure the exhaustive des-
cription of random processes and ﬁelds. However, even one-time probability
densities provide certain information about temporal behavior and spatial
structure of random processes for arbitrary long temporal intervals. The ideas
of statistical topography of random processes and ﬁelds can assist in obtain-
ing this information.

160
6
Random Fields
6.2
Statistical Topography of Random Processes and
Fields
The term statistical topography was seemingly for the ﬁrst time introduced in
book [141], though the underlying ideas of this approach can be traced back
to much earlier works (see, e.g., books [1, 11] and review [36] with detailed
reference lists on the problem).
Similarly to common topography of mountain ranges, the statistical to-
pography studies the systems of contours (level lines in the two-dimensional
case and surfaces of constant values in the three-dimensional case) speciﬁed
by the equality f(r, t) = f = const. Figure 6.1 shows examples of realizations
of two random ﬁelds characterized by diﬀerent statistical structures.
a
b
−2
−2
−1
0
0
1
1
1
2
2
2
2
3
3
4
4
60
60
60
60
60
60
60
60
80
80
80
80
100
100
100
100
100
100
100
100
120
120
120
120
140
140
140
140
140
140
140
140
Fig. 6.1 Realizations of the ﬁelds governed by (a) Gaussian and (b) lognormal
distributions(a) and the corresponding topographic level lines. The bold curves in
the bottom patterns show level lines corresponding to levels 0 (a) and 1 (b).
For analyzing a system of contours (in this section, we will deal for sim-
plicity with the two-dimensional case and assume r = R), we introduce the
singular indicator function (6.2) concentrated on these contours.
The convenience of function (6.2) consists, in particular, in the fact that
it allows simple expressions for quantities such as the total area of regions
where f(R, t) > f (i.e., within level lines f(R, t) = f) and the total mass of
the ﬁeld within these regions [72]

6.2
Statistical Topography of Random Processes and Fields
161
S(t; f) =

θ(f(R, t) −f)dR =
∞

f
df ′

dRϕ(t, R;f ′),
M(t; f) =

f(R, t)θ(f(R, t) −f)dR =
∞

f
f ′df ′

dRϕ(t, R; f ′).
As we mentioned earlier, the mean value of indicator function (6.2) over
an ensemble of realizations determines the one-time (in time) and one-point
(in space) probability density
P(R, t; f) = ⟨ϕ(R, t; f)⟩= ⟨δ (f(R, t)−f)⟩.
Consequently, this probability density immediately determines ensemble-
averaged values of the above expressions.
If we include into consideration the spatial gradient
p(R, t) = ∇f(R, t),
we can obtain additional information on details of the structure of ﬁeld
f(R, t). For example, quantity
l(t; f) =

dR |p(R, t)| δ(f(R, t) −f) =
=
dl
(6.14)
is the total length of contours [84 – 88] and extends formula (5.11) to random
ﬁelds.
The integrand in Eq. (6.14) is described in terms of the extended indicator
function
ϕ(R, t; f, p) = δ (f(R, t) −f) δ (p(R, t)−p) ,
(6.15)
so that the average value of total length (6.14) is related to the joint one-time
probability density of ﬁeld f(R, t) and its gradient p(R, t), which is deﬁned
as the ensemble average of indicator function (6.15), i.e., as the function
P(R, t; f, p) = ⟨δ (f(R, t) −f) δ (p(R, t) −p)⟩.
Inclusion of second-order spatial derivatives into consideration allows esti-
mating the total number of contours f(R, t) = f = const by the approximate
formula (neglecting unclosed lines) [127]
N(t; f) = Nin(t; f)−Nout(t; f) = 1
2π

dRκ(t, R; f) |p(R, t)| δ (f(R, t) −f) ,
where Nin(t; f) and Nout(t; f) are the numbers of contours for which vector
p is directed along internal and external normals, respectively; and κ(R, t; f)
is the curvature of the level line.

162
6
Random Fields
Recall that, in the case of the spatially homogeneous ﬁeld f(R, t), the cor-
responding probability densities P(R, t; f) and P(R, t; f, p) are independent
of R. In this case, statistical averages of the above expressions without inte-
gration over R characterize – if exits – the corresponding speciﬁc (per unit
area) values of these quantities.
So, the speciﬁc mean area ⟨shom(t; f)⟩over which the random ﬁeld f(R, t)
exceeds a given level f, coincides with the probability of the event f(R, t) > f
at any spatial point, i.e.,
⟨shom(t; f)⟩= ⟨θ(f(R, t) −f)⟩= P{f(R, t) > f}
and therefore the mean speciﬁc area oﬀers a geometric interpretation of the
probability of the event f(R, t) > f, which is apparently independent of point
R.
In Chapter 1 we noted that, in a number of cases, stochastic structures of
the type of clustering can be formed in stochastic dynamic systems.
Clustering of a ﬁeld is identiﬁed as the emergence of compact areas with
large ﬁeld values against the residual background of areas where these values
are fairly low. Naturally, statistical averaging completely destroys all data on
clustering.
The notion of clustering by itself is related to the spatial behavior of a
random ﬁeld in separate realizations! Clustering either exists or not exists.
Consider now the conditions of occurrence of stochastic structure forma-
tion. Phenomenon of clustering random ﬁelds can be detected and described
only on the basis of the ideas of statistical topography.
6.3
On the Criterion of Stochastic Structure
Formation in Random Media
It is clear that, for a positive ﬁeld f(R, t), the condition of clustering with
a probability one, i.e., almost in all realizations, is formulated in the gen-
eral case as simultaneous tendency of fulﬁllment of the following asymptotic
equalities for t →∞
⟨S(t; f)⟩→0,
⟨M(t; f)⟩→

d R ⟨f(R, t)⟩.
On the contrary, simultaneous tendency of fulﬁllment of the asymptotic
equalities for t →∞
⟨S(t; f)⟩→∞,
⟨M(t; f)⟩→

d R ⟨f(R, t)⟩
corresponds to the absence of structure formation.

6.3
On the Criterion of Stochastic Structure Formation in Random Media
163
In the case of a spatially homogeneous ﬁeld f(R, t), conditions of clustering
are reduced to the tendency of asymptotic equalities for t →∞
⟨shom(t; f)⟩= P{f(R, t) > f} →0,
⟨mhom(t; f)⟩→⟨f(t)⟩.
Absence of clustering corresponds to the tendency of asymptotic equalities
⟨shom(t; f)⟩= P{f(R, t) > f} →1,
⟨mhom(t; f)⟩→⟨f(t)⟩.
Thus, in spatially homogeneous problems, clustering is the physical phe-
nomenon realized with probability one (i.e., occurred in almost all realizations
of a positive random ﬁeld) generated by a rare event whose probability tends
to zero. Namely availability of these rare events is the trigger that starts the
process of structure formation, and structure formation itself is the property
of random medium [61–66].
In the conditions of developed clustering, the random ﬁeld is simply absent
in the most part of space!
As for setup time of such spatial structure formation, it depends on limiting
behavior of the right-hand expressions in all above asymptotic equalities.
Stochastic structure formation is governed by diﬀusion of random ﬁeld
f(R, t) in its phase space {f}. This means that clustering of such a positive
ﬁeld f(R, t) is inherent in dynamic ﬁelds of arbitrary nature, and we can say
that structure formation is a law of nature for such ﬁelds.
The above criterion of ’ideal’ clustering (analogously to ideal hydrody-
namic) describes dynamics of cluster formation in the dynamic systems des-
cribed in general by the ﬁrst-order partial diﬀerential equations (Eulerian
description). This ideal structure originates in the form of very thin belts (in
the two-dimensional case) or very thin tubes (in the three-dimensional case).
We note that ﬁrst-order partial diﬀerential equations can be solved in the
general case by the method of characteristics. This corresponds to the La-
grangian description of dynamic systems. Moreover, the characteristic curves,
which are described by ordinary diﬀerential equations, can of course have var-
ious peculiarities and even singularities. The conditions for such peculiarities
to arise in the Lagrangian description are not directly related to the phe-
nomenon of clustering in space and time, i.e., in the Eulerian description. A
more detailed description of this fact is given in Volume 2, Part 2.
As for actual physical systems, various additional factors come to play
with time; they are related to generation of random ﬁeld spatial derivatives
like spatial diﬀusion or diﬀraction, which deform the pattern of clustering,
but not dispose it.
In particular, a possible situation can occur when the probability density
rapidly approaches its steady-state regime P(R; f) for t →∞. In this case,
functionals like ⟨S(f)⟩and ⟨M(f)⟩cease to describe further deformation of
the clustering pattern, and we must study temporal evolution of functionals
related to the spatial derivatives of ﬁeld f(R, t), like the number of contours.

164
6
Random Fields
a
b
Fig. 6.2 Photo of cluster structure of cloudy sky: (a) nearly "ideal" clustering and
(b) "deformed" clustering
a
b
Fig. 6.3 The lava lakes in the depths of Nyiragongo Crater (a), and Kilauea Crater
(b)
As illustrations of ’ideal’ and ’deformed’ clustering in Nature we cite photos
of cluster structure of cloudy sky1 (Fig. 6.2), and the lava lakes boiling in
the depths of Nyiragongo Crater (in the heart of the Great Lakes region
of Africa) and in the depth of Kilauea Crater (Hawaiian National Volcano
Park) in Figs. 6.32. The problem of wave propagation in random media yields
other illustrations of the phenomenon of clustering (see Fig. 1.19, page 65
in Chapter 1, and similar patterns in Chapter 20 of Volume 2, which were
obtained in both laboratory experiments and numerical simulations).
1 These photos were obtained by V.A. Dovzhenko on June 15 and August 2, 2013
at the coast of the Azov Sea (Russia) at 21:00.
2 Photos in Figs. 6.3 can be found at http://bigpicture.ru/?p=128340,
http://pacificislandparks.com/2010/01/20/more-amazing-lava-
lake-photos/

Chapter 7
Correlation Splitting
7.1
General Remarks
For simplicity, we content ourselves here with the one-dimensional random
processes (extensions to multidimensional cases are obvious). We need the
ability of calculating correlation ⟨F[z(τ)]R[z(τ)]⟩, where F[z(τ)] is the func-
tional explicitly dependent on process z(t) and R[z(τ)] is the functional
that can depend on process z(t) both explicitly and implicitly. To cal-
culate this average, we consider auxiliary functionals F [z(τ) + η1(τ)] and
R [z(τ) + η2(τ)], where ηi(t) are arbitrary deterministic functions, and calcu-
late the correlation ⟨F[z(τ) + η1(τ)]R[z(τ) + η2(τ)]⟩. The correlation of in-
terest will be obtained by setting ηi(τ) = 0 in the ﬁnal result.
We can expand the above auxiliary functionals in the functional Taylor
series with respect to z(τ). The result can be represented in the form
F [z(τ) + η1(τ)] = e
∞

−∞
dτz(τ)
δ
δη1(τ) F [η1(τ)] ,
R [z(τ) + η2(τ)] = e
∞

−∞
dτz(τ)
δ
δη2(τ) R [η2(τ)] ,
where we introduced the functional shift operators. With this representation,
we can obtain the following expression for the correlation
⟨F[z(τ) + η1(τ)]R[z(τ) + η2(τ)]⟩
= exp

Θ
1
i

δ
δη1(τ) +
δ
δη2(τ)

−Θ
1
i
δ
δη1(τ)

−Θ
1
i
δ
δη2(τ)
	
× ⟨F[z(τ) + η1(τ)]⟩⟨R[z(τ) + η2(τ)]⟩.
(7.1)
Here Θ[v(τ)] = ln Φ[v(τ)] and Φ[v(τ)] is the characteristic functional of ran-
dom process z(t) (See Eq. (5.5)).
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
165
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_7, c
⃝Springer International Publishing Switzerland 2015

166
7
Correlation Splitting
This formula expresses the average of the product of functionals through
the product of averages of the functionals themselves. The main problem here
consists in calculating the action of the functional operator
exp

Θ
1
i

δ
δη1(τ) +
δ
δη2(τ)

−Θ
1
i
δ
δη1(τ)

−Θ
1
i
δ
δη2(τ)
	
on the product of average functionals.
In a number of statistical problems, the intensity of parameter ﬂuctuation
can be considered small. In these situations, we can expand functional F[z(τ)]
in the Taylor series with respect to process z(τ) and content themselves
with the linear term of the expansion. In the case of the linear functional
F[z(τ)] ≡z(t′), we obtain the following expression for the correlation
⟨z(t′)R[z(τ) + η(τ)]⟩= Ω

t′;
δ
iδη(τ)

⟨R[z(τ) + η(τ)]⟩,
where functional
Ω[t′; v(τ)] =
4
z(t′) exp
⎧
⎨
⎩i
∞

−∞
dτz(τ)v(τ)
⎫
⎬
⎭
5
4
exp
⎧
⎨
⎩i
∞

−∞
dτz(τ)v(τ)
⎫
⎬
⎭
5
=
δ
iδv(t′)Θ[v(τ)].
Setting now η(τ) = 0, we obtain the expression
⟨z(t′)R[z(τ)⟩=
.
Ω

t′;
δ
iδz(τ)

R[z(τ)]
/
.
(7.2)
If we expand functional Θ[v(τ)] in the functional Taylor series (5.6),
page 127
Ω[t′; v(τ)] =
∞
&
n=0
in
n!
∞

−∞
dt1 · · ·
∞

−∞
dtnKn+1(t′, t1, · · · , tn)v(t1) · · · v(tn),
then expression (7.2) assumes the form
'
z(t′)R[z(τ)]
(
=
∞
 
n=0
1
n!
∞

−∞
dt1 · · ·
∞

−∞
dtnKn+1(t′, t1, · · · , tn)
)
δnR[z(τ)]
δz(t1) · · · δz(tn)
*
.
(7.3)
Note that, if functional R[z(τ)] has the form of the power monomial
R[z(τ)] = z(t1) · · · z(tn),

7.1
General Remarks
167
then Eq. (7.3) recursively relates the n-point moment of process z(t) to its
cumulants.
If process z(t) is simply random quantity z, operator

dt
δ
δz(t) reduces
to the ordinary derivative d
dz and Eq. (7.3) grades into Eq. (4.12), page 120.
Thus, Eq. (7.3) extends Eq. (4.12) to random processes.
In physical problems satisfying the condition of dynamical causality in
time t, statistical characteristics of the solution at instant t depend on the
statistical characteristics of process z(τ) for 0 ≤τ ≤t, which are completely
described by the characteristic functional
Φ[t; v(τ)] = exp {Θ[t; v(τ)]} =
4
exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
.
In this case, the obtained formulas hold also for calculating statistical averages
⟨z(t′)R[t; z(τ)]⟩for t′ < t, τ ≤t, i.e., we have the equality
⟨z(t′)R[t; z(τ)]⟩=
.
Ω

t′, t;
δ
iδz(τ)

R[t; z(τ)]
/
(0 < t′ < t),
(7.4)
where
Ω[t′, t; v(τ)] =
δ
iδv(t′)]Θ[t; v(τ)]
=
∞
&
n=0
in
n!
t

0
dt1 · · ·
t

0
dtnKn+1(t′, t1, · · · , tn)v(t1) · · · v(tn).
(7.5)
For t′ = t −0, formula (7.4) holds as before, i.e.
⟨z(t)R[t; z(τ)]⟩=
.
Ω

t, t;
δ
iδz(τ)

R[t; z(τ)]
/
.
(7.6)
However, expansion (7.5) not always gives the correct result in the limit
t′ →t −0 (which means that the limiting process and the procedure of
expansion in the functional Taylor series can appear non-commutable). In
this case,
Ω[t, t; v(τ)] =
4
z(t) exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
4
exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
=
d
iv(t)dtΘ[t; v(τ)],
(7.7)

168
7
Correlation Splitting
and statistical averages in Eqs. (7.4) and (7.6) can be discontinuous at t′ =
t −0.
Consider several examples of random processes.
7.2
Gaussian Process
In the case of the Gaussian random process z(t), all formulas obtained in
the previous section become signiﬁcantly simpler. In this case, the loga-
rithm of characteristic functional Φ[v(τ)] is given by Eq. (5.13), page 130
(we assume that the mean value of process z(t) is zero); as a consequence,
Eq. (7.1) assumes the form
⟨F[z(τ) + η1(τ)]R[z(τ) + η2(τ)]⟩= exp
⎧
⎨
⎩
∞

−∞
∞

−∞
dτ1dτ2B(τ1, τ2)
δ2
δη1(τ1)δη2(τ2)
⎫
⎬
⎭
× ⟨F[z(τ) + η1(τ)]⟩⟨R[z(τ) + η2(τ)]⟩.
(7.8)
We can easily calculate variational derivative of Eq. (7.8) with respect to
η1(τ) (this operation reduces to functional shift) and set η1(τ) = 0. As a
result, we obtain the equality
⟨F[z(τ)]R[z(τ) + η(τ)]⟩=

F
⎡
⎣z(τ) +
∞

−∞
dτ1B(τ, τ1)
δ
δη(τ1)
⎤
⎦

⟨R[z(τ) + η(τ)]⟩.
(7.9)
Let F[z(τ)] = z(t) for example. Then Eq. (7.9) assumes the form
⟨z(t)R[z(τ) + η(τ)]⟩=
∞

−∞
dτ1B(t, τ1)
δ
δη(τ1) ⟨R[z(τ) + η(τ)]⟩.
(7.10)
Replacing now diﬀerentiation with respect to η(τ) by diﬀerentiation with
respect to z(τ) and setting η(τ) = 0, we obtain the equality
⟨z(t)R[z(τ)]⟩=
∞

−∞
dτ1B(t, τ1)
.
δ
δz(τ1)R[z(τ)]
/
(7.11)
commonly known in physics as the Furutsu–Novikov formula [20,114]. Note
that this formula can be obtained by partial integration in appropriate func-
tional space [15].
One can easily obtain the multi-dimensional extension of Eq. (7.11); it can
be written in the form
⟨zi1,··· ,in(r)R[z]⟩=

dr′ ⟨zi1,··· ,in(r)zj1,··· ,jn(r′)⟩
.
δR[z]
δzj1,··· ,jn(r′)
/
, (7.12)

7.2
Gaussian Process
169
where r stands for all continuous arguments of random vector ﬁeld z(r) and
i1, · · · , in are the discrete (index) arguments. Repeated index arguments in
the right-hand side of Eq. (7.12) assume summation.
If we set F[z(τ)] = exp
'
i
∞

−∞
dτ z(τ)v(τ)
(
in Eq. (7.9), then we obtain,
at η(τ) = 0, the equality

exp
⎧
⎨
⎩i
∞

−∞
dτ z(τ)v(τ)
⎫
⎬
⎭R[z(τ)]

=Φ[v(τ)]

R
⎡
⎣z(τ) + i
∞

−∞
dτ1B(τ, τ1)v(τ1)
⎤
⎦

,
(7.13)
in which random process z(τ) within the averaging brackets in the right-hand
side is added with the deterministic imaginary term. Formulas (7.11), (7.12)
and (7.13) extend formulas (4.17), (4.24), page 123 to the Gaussian random
processes.
If random process z(τ) is deﬁned only on time interval [0, t], then functional
Θ[t, v(τ)] will assume the form
Θ[t, v(τ)] = −1
2
t

0
t

0
dτ1dτ2B(τ1, τ2)v(τ1)v(τ2),
(7.14)
and functionals Ω[t′, t; v(τ)] and Ω[t, t; v(τ)] will be the linear functionals
Ω[t′, t; v(τ)] =
δ
iδv(t′)Θ[t, v(τ)] = i
t

0
dτB(t′, τ)v(τ),
Ω[t, t; v(τ)] =
d
iv(t)dtΘ[t, v(τ)] = i
t

0
dτB(t, τ)v(τ).
(7.15)
As a consequence, Eqs. (7.4), (7.6) will assume the form
⟨z(t′)R[t, z(τ)]⟩=
t

0
dτB(t′, τ)
.δR[z(τ)]
δz(τ)
/
(t′ ⩽t)
(7.16)
that coincides with Eq. (7.11) if the condition
δR[t; z(τ)]
δz(τ)
= 0
for
τ < 0,
τ > t
(7.17)
holds. Note that Eq. (7.13) assumes in this case the form

170
7
Correlation Splitting
4
exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭R[t; z(τ)]
5
=
Φ[t; v(τ)]
4
R
⎡
⎣t; z(τ) + i
t

0
dτ1B(τ, τ1)v(τ1)
⎤
⎦
5
,
(7.18)
where Φ[t; v(τ)] is the characteristic functional of the Gaussian random
process z(t).
The corresponding extension of multidimensional formula (7.12) to the
case of functional R[t; z(r, τ)] causal in time has the form
⟨zi1,··· ,in(r, t)R[t; z(r, τ)]⟩=

dr′
t

0
dt′ ⟨zi1,··· ,in(r, t)zj1,··· ,jn(r′, t′)⟩
. δR[t; z(r, τ)]
δzj1,··· ,jn(r′, t′)
/
.
(7.19)
7.3
Gaussian Random Fields
We consider Gaussian random ﬁelds in more detail because they play very
important role in physical applications.
A statistical quantity such as the joint characteristic functional (generating
functional) of two correlated Gaussian ﬁelds u(r, t) and v(r, t) with zero
means has the following structure

e

dr′
∞

−∞
dt′ [κ1(r′,t′)v(r′,t′)+κ(r′,t′)u(r′,t′)]
= exp
⎧
⎨
⎩
1
2

dr′dr′′
∞

−∞
dt′dt′′ 
vi(r′, t′)vj(r′′, t′′)

κ1i(r′, t′)κ1j(r′′, t′′)
+2 vi(r′, t′)uj(r′′, t′′) κ1i(r′, t′)κj(r′′, t′′) + ui(r′, t′)uj(r′′, t′′) κi(r′, t′)κj(r′′, t′′)
.
Varying this expression with respect to κ1l at point (r, t) (i.e., calculating
variational derivative
δ
iδκ1l(r, t)) and setting κ1 = 0, we obtain

7.3
Gaussian Random Fields
171
4
vl(r, t)e
 dr′
∞

−∞
dt′ u(r′,t′)κ(r′,t′)5
=

dr′′
∞

−∞
dt′′ ⟨vl(r, t)uj(r′′, t′′)⟩κj(r′′, t′′)
4
e

dr′
∞

−∞
dt′ u(r′,t′)κ(r′,t′)5
.
Using this expression and the functional shift operator, we can represent
correlation ⟨vl(r, t)F[u(r, τ) + η(r, τ)]⟩in the form
⟨vl(r, t)F [u(r, τ) + η(r, τ)]⟩=

vl(r, t)e

dr′
∞

−∞
dt′ u(r′,t′)
δ
δη(r′,t′)

F [η(r, τ)]
=

dr′′
∞

−∞
dt′′ 
vl(r, t)uj(r′′, t′′)
δ
δηj(r′′, t′′)

e

dr′
∞

−∞
dt′ u(r′,t′)
δ
δη(r′,t′)

F [η(r, τ)]
=

dr′
∞

−∞
dt′ 
vl(r, t)uj(r′, t′)
δ
δηj(r′, t′) ⟨F [u(r, τ) + η(r, τ)]⟩.
Setting now η(r, τ) = 0, we arrive at a formula that generalizes the Furutsu-
Novikov (7.12) to two correlated Gaussian ﬁelds
⟨vl(r, t)F [u(r, τ)]⟩=

dr′
∞

−∞
dt′ vl(r, t)uj(r′, t′) 
δ
δuj(r′, t′)F [u(r, τ)]

.
(7.20)
Here, ﬁeld v(r, t) is, generally speaking, arbitrary. In particular, it can
depend on ﬁeld u(r, t). For example, if v(r, t) = ∂u(r, t)
∂rk
, then we obtain
the expression
)∂ul(r, t)
∂rk
F[u(r, τ)]
*
=

dr′
∞

−∞
dt′ ∂⟨ul(r, t)uj(r′, t′)⟩
∂rk
)
δ
δuj(r′, t′)F[u(r, τ)]
*
that, in the case of the Gaussian ﬁeld u(r, t) homogeneous in space and
stationary in time reduces to the equality
 ∂ul(r, t)
∂rk
F [u(r, τ)]

=

dr′
∞

−∞
dt′ ∂Blj(r −r′, t −t′)
∂rk

δ
δuj(r′, t′)F [u(r, τ)]

,
(7.21)
where Blj(r −r′, t −t′) = ⟨ul(r, t)uj(r′, t′)⟩is the correlation tensor of ran-
dom vector ﬁeld u(r, t).
Similar expressions can be derived for both other spatial derivatives of ﬁeld
u(r, t) and temporal derivatives of this ﬁeld.
If condition of dynamic causality holds, formulas (7.20) and (7.21) are
simpliﬁed and assume the form

172
7
Correlation Splitting
⟨vl(r, t)F[u(r, τ)]⟩=

dr′
t

0
dt′ ⟨vl(r, t)uj(r′, t′)⟩
.
δ
δuj(r′, t′)F[u(r, τ)]
/
,
(7.22)
)∂ul(r, t)
∂rk
F[u(r, τ)]
*
=

dr′
t

0
dt′ ∂Blj(r −r′, t −t′)
∂rk
)
δ
δuj(r′, t′)F[u(r, τ)]
*
.
(7.23)
7.4
Poisson Process
The Poisson process is deﬁned by Eq. (5.26), page 134, and its characteristic
functional logarithm is given by Eq. (5.30). In this case, formulas (7.5) and
(7.7) for functionals Ω[t′, t; v(τ)] and Ω[t, t; v(τ)] assume the forms
Ω[t′, t; v(τ)]=
δ
iδv(t′)Θ[t, v(τ)] = −i
t′

0
dτg(t′−τ) ˙W
⎛
⎝
t

τ
dτ1v(τ1)g(τ1 −τ)
⎞
⎠,
Ω[t, t; v(τ)] =
d
iv(t)dtΘ[t, v(τ)] = −i
t

0
dτg(t−τ) ˙W
⎛
⎝
t

τ
dτ1v(τ1)g(τ1 −τ)
⎞
⎠,
(7.24)
where ˙W(v) = dW(v)
dv
= i
 ∞
−∞
dξξp(ξ)eiξv.
Changing the integration order, we can rewrite equalities (7.24) in the form
Ω[t′, t; v(τ)] = i
∞

−∞
dξξp(ξ)
t′

0
dτg(t′ −τ) exp
⎧
⎨
⎩iξ
t

τ
dτ1v(τ1)g(τ1 −τ)
⎫
⎬
⎭
(t′ ⩽t).
(7.25)
As a result, we obtain that correlations of the Poisson random process z(t)
with functionals of this process are described by the expression
'
z(t′)R[t; z(τ)]
(
= ν
∞

−∞
dξξp(ξ)
t′

0
dτ ′g(t′ −τ ′)
'
R[t; z(τ) + ξg(τ −τ ′)]
(
(t′ ⩽t).
(7.26)
As we mentioned earlier, random process n(0, t) describing the number of
jumps during temporal interval (0, t) is the special case of the Poisson process.
In this case, p(ξ) = δ(ξ −1) and g(t) = θ(t), so that Eq. (7.26) assumes the
extra-simple form

7.5
Telegrapher’s Random Process
173
⟨n(0, t)R[t; n(0, τ)]⟩= ν
t′

0
dτ ⟨R[t; n(0, τ) + θ(t′ −τ)]⟩
(t′ ⩽t).
(7.27)
Equality (7.27) extends formula (4.25) for the Poisson random quantities to
the Poisson random processes.
7.5
Telegrapher’s Random Process
Now, we dwell on telegrapher’s random process deﬁned by formula (5.35),
page 137
z(t) = a(−1)n(0,t),
(7.28)
where a is the deterministic quantity. The n-th order moment functions of this
process satisfy recurrence equation (5.36) from which immediately follows the
relationship
⟨z(t1)z(t2)R[z(τ)]⟩= ⟨z(t1)z(t2)⟩⟨R[z(τ)]⟩,
(7.29)
which holds for arbitrary functional R[z(τ)] under the condition that τ ≤
t2 ≤t1 [9]. The proof of Eq. (7.29) consists in expanding functional R[z(τ)]
in the Taylor series in z(τ) and using formula (5.36).
Let now quantity a be the random quantity with probability distribution
density
P(a) = 1
2 [δ(a −a0) + δ(a + a0)] .
(7.30)
In this case M2k+1 = 0 and, in addition to Eq. (7.29), the equality [9]
⟨F[z(τ1)]z(t1)z(t2)R[z(τ2)]⟩
= ⟨F[z(τ1)⟩] ⟨z(t1)z(t2)⟩⟨R[z(τ2)]⟩+ ⟨F[z(τ1)]z(t1)⟩⟨z(t2)R[z(τ2)]⟩
(7.31)
holds for any τ1 ≤t1 ≤t2 ≤τ2 and arbitrary functionals F[z(τ1)] and
R[z(τ2)]. Indeed, in terms of the Taylor expansion in z(τ), functional R[z(τ2)]
can be represented in the form R[z(τ2)] =
&
2k
+
&
2k+1
, where the ﬁrst sum
consists of terms with the even number of co-factors z(τ) and the second
sum consists of terms with the odd number of such co-factors. Then, taking
into account Eq. (5.36) and equalities
⟨R[z(τ2)]⟩=
4&
2k
5
,
⟨z(t2)R[z(τ2)]⟩=
4
z(t2)
&
2k+1
5
,
we obtain Eq. (7.31).

174
7
Correlation Splitting
Formula
(7.31)
allows
another
representation.
Denote
functional
F[z(τ1)]z(t1) as F[t1; z(τ1)], where τ1 ≤t1, and functional z(t2)R[z(τ2)] as
R[t2; z(τ2)], where t1 ≤t2 ≤τ2. Then, Eq. (7.31) can be written in the form
⟨F[t1; z(τ1)]R[t2; z(τ2)]⟩= ⟨F[t1; z(τ1)]⟩⟨R[t2; z(τ2)]⟩
+ 1
a2
0
e−2ν(t2−t1) ⟨z(t1)F[t1; z(τ1)]⟩⟨z(t2)R[t2; z(τ2)]⟩.
(7.32)
Because functionals F[z(τ1)] and R[z(τ2)] in Eq. (7.31) are arbitrary func-
tionals, functionals F[t1; z(τ1)] and R[t2; z(τ2)] in (7.32) are also arbitrary
functionals.
Formulas (7.29) and (7.31) include bilinear combinations of process z(t),
which is not always practicable.
As we mentioned earlier, calculation of correlator ⟨z(t)R[t; z(τ)]⟩for τ ≤t
assumes the knowledge of the characteristic functional of process z(t); unfor-
tunately, the characteristic functional is unavailable in this case. We know
only Eqs. (5.39) and (5.40), page 139 that describe the relationship between
the functional
Ψ[t; v(τ)] =
1
iv(t)
d
dtΦ[t; v(τ)] =
4
z(t)e
i
t
0
dτz(τ)v(τ)
5
and the characteristic functional Φ[t; v(τ)] in the form
4
z(t)e
i
t
0
dτz(τ)v(τ)
5
=
ae−2νt + ia2
t

0
dt1e−2ν(t−t1)v(t1)
4
e
i
t1

0
dτz(τ)v(τ)
5
.
This relationship is suﬃcient to split correlator ⟨z(t)R[t; z(τ)]⟩, i.e., to express
it immediately in terms of the average functional ⟨R[t; z(τ)]⟩.
Indeed, the equality
⟨z(t)R[t; z(τ) + η(τ)]⟩=
4
z(t)e
t
0
dτz(τ)
δ
δη(τ)
5
R[t; η(τ)]
= aR[t; η(τ)]e−2νt + a2
t

0
dt1e−2ν(t−t1)
δ
δη(t1) ⟨R[t; z(τ)θ(t1 −τ) + η(τ)]⟩,
(7.33)
where η(t) is arbitrary deterministic function, holds for any functional
R[t; z(τ)] for τ ≤t. The ﬁnal result is obtained by the limit process η →0:

7.5
Telegrapher’s Random Process
175
⟨z(t)R[t; z(τ)]⟩= aR[t; 0]e−2νt + a2
t

0
dt1e−2ν(t−t1)
.
δ
δz(t1)
R[t, t1; z(τ)]
/
,
(7.34)
where functional R[t, t1; z(τ)] is deﬁned by the formula
R[t, t1; z(τ)] = R[t; z(τ)θ(t1 −τ + 0)].
(7.35)
In the case of random quantity a distributed according to probability
density distribution (7.30), additional averaging (7.34) over a results in the
equality
⟨z(t)R[t; z(τ)]⟩= a2
0
t

0
dt1e−2ν(t−t1)
.
δ
δz(t1)
R[t, t1; z(τ)]
/
.
(7.36)
Formula (7.36) is very similar to the formula for splitting the correlator of
the Gaussian process z(t) characterized by the exponential correlation func-
tion (i.e., the Gaussian Markovian process) with functional R[t; z(τ)]. The
diﬀerence consists in the fact that the right-hand side of Eq. (7.36) depends
on the functional that is cut by the process z(τ) rather than on functional
R[t; z(τ)] itself.
Note that correlation ⟨z(t′)R[t; z(τ)]⟩,where t′ ≥t and τ < t, can be rep-
resented in the form
⟨z(t)R[t; z(τ)]⟩= 1
a2
0
⟨z(t′)z(t)z(t)R[t; z(τ)]⟩.
As a consequence, the equality
⟨z(t′)R[t; z(τ)]⟩= e−2ν(t′−t) ⟨z(t)R[t; z(τ)]⟩
(t′ ⩾t)
(7.37)
holds according to formula (7.29).
In a similar way, we can obtain the expression
⟨z(t′)R[t0, t; z(τ)]⟩= e−2ν(t0−t′) ⟨z(t0)R[t0, t; z(τ)]⟩
(t′ ⩾t),
(7.38)
where t′ ≤t0 ≤τ ≤t and
⟨z(t0)R[t0, t; z(τ)]⟩= a2
0
t

t0
dt1e−2ν(t1−t0)
.
δ
δz(t1)R[t0, t, ; z(τ)θ(τ −t1 + 0]
/
.
(7.39)
In the case of the general-form correlator ⟨z(ξ)R[t0, t; z(τ)]⟩, where t0 ⩽
ξ ⩽t and t0 ⩽τ ⩽t, one can show [48, 49] the validity of the following
equality:

176
7
Correlation Splitting
⟨z(ξ)R[t0, t; z(τ)]⟩= a2
0
ξ
	
t0
dt1e−2ν(ξ−t1)
 δR[t0, t, ; z1(τ)θ(τ −ξ) + z2(τ)θ(t1 −τ + 0]
δz2(t1)

+ a2
0
t
	
ξ
dt1e−2ν(t1−ξ)
 δR[t0, t, ; z1(τ)θ(ξ −τ) + z2(τ)θ(τ −t1 + 0]
δz2(t1)

,
(7.40)
where z1(t) and z2(t) are statistically independent telegrapher’s processes
characterized by the same correlation function
⟨z(t)z(t′)⟩= a2
0e−2ν|t−t′|.
The limits of integration t0 and t in Eq. (7.40) can assume arbitrary values
from −∞to ∞. At ξ = t or ξ = t0, Eq. (7.40) grades into Eq. (7.36) or (7.39),
respectively.
If we set v(τ) = v and R[t0, t; z(τ)] = exp
'
iv
t
t0
dτz(τ)
(
in Eq. (7.40) and
take into account Eq. (5.42), page 139, we obtain the expression
4
z(ξ) exp
⎧
⎨
⎩iv
t

t0
dτz(τ)
⎫
⎬
⎭
5
= iva2
0
λ e−ν(t−t0)

sinhλ(t −t0) + 2ν
λ sinh λ(t −ξ) sinh λ(ξ −t0)
	
,
(7.41)
where λ =
>
ν2 −a2
0v2.
Let us diﬀerentiate Eq. (7.34) with respect to time t. Taking into account
the fact that there is no need in diﬀerentiating with respect to the upper limit
of the integral in the right-hand side of Eq. (7.34), we obtain the expression
[124,125]
 d
dt + 2ν

⟨z(t)R[t; z(τ)]⟩=
.
z(t) d
dtR[t; z(τ)]
/
(7.42)
called usually the diﬀerentiation formula.
One essential point should be noticed. Functional R[t; z(τ)] in diﬀerentia-
tion formula (7.42) is an arbitrary functional and can simply coincide with
process z(t −0). In the general case, the realization of telegrapher’s process
is a generalized function. The derivative of this process is also a generalized
function (the sequence of delta-functions), so that
z(t) d
dtz(t) ̸= 1
2
d
dtz2(t) ≡0
in the general case. These generalized functions, as any generalized functions,
are deﬁned only in terms of functionals constructed on them. In the case

7.6
Generalized Telegrapher’s Random Process
177
of our interest, such functionals are the average quantities denoted by angle
brackets ⟨· · · ⟩, and the above diﬀerentiation formula describes the diﬀerential
constraint between diﬀerent functionals related to random process z(t) and
its one-sided derivatives for t →t −0, such as dz/dt, d2z/dt2. For example,
formula (7.42) allows derivation of equalities, such as
.
z(t) d
dτ z(τ)
/
τ=t−0
= 2ν
,
z2-
,
.
z(t) d2
dτ 2 z(τ)
/
τ=t−0
= 4ν2 ,
z2-
.
It is clear that these formulas can be obtained immediately by diﬀerentiating
the correlation function ⟨z(t)z(t′)⟩with respect to t′ (t′ < t) for t′ →t −0.
Above, we considered the correlation of random process z(t) with a func-
tional of this process. If we deal with an arbitrary function of telegrapher’s
process F(z(t)), then, clearly, the equality
F (z(t)) = F(a) + F(−a)
2
+ F(a) −F(−a)
2a
z(t)
(7.43)
will hold, and all results valid for telegrapher’s process z(t) will be valid (with
small variations) for process F(z(t)).
7.6
Generalized Telegrapher’s Random Process
Consider now generalized telegrapher’s process described by Eq. (5.43),
page 141. In this case, functional
Ψ[t; v(τ)] =
1
iv(t)
d
dtΦ[t; v(τ)] =
4
z(t) exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
is related to the characteristic functional Φ[t; v(τ)] of process z(t) by
Eq. (5.100)
4
z(t) exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
=
4
a exp
⎧
⎨
⎩ia
t

0
dτv(τ)
⎫
⎬
⎭
5
a
e−νt
+ ν
t

0
dt1e−ν(t−t1)
4
a exp
⎧
⎨
⎩ia
t

t1
dτv(τ)
⎫
⎬
⎭
5
a
Φ[t1, v(τ)],
As in the case of telegrapher’s process, this formula allows one to express
correlator ⟨z(t)R[t; z(τ)]⟩, where R[t; z(τ)] is arbitrary functional, in terms of

178
7
Correlation Splitting
the mean value of the functional. Indeed, if we replicate operations used for
telegrapher’s process, we obtain the equality
⟨z(t)R[t; z(τ) + η(τ)]⟩= Ψ

t;
δ
iδη(τ)

R[t; η(τ)] = ⟨aR[t; η(τ) + a]⟩a e−νt
+ ν
t

0
dt1e−ν(t−t1) ⟨aR [t; η(τ) + aθ(τ −t1) + z (τ) θ(t1 −τ)]⟩a,z ,
(7.44)
where η(τ) is arbitrary function and random quantity a is distributed with
probability density p(a) and is statistically independent of process z(t). Set-
ting now η(τ) = 0, we obtain the ﬁnal expression
⟨z(t)R[t; z(τ)]⟩= ⟨aR[t; a]⟩a e−νt
+ ν
t

0
dt1e−ν(t−t1) ⟨aR [t; aθ(τ −t1) + z (τ) θ(t1 −τ)]⟩a,z .
(7.45)
Note additionally that, in the case of generalized telegrapher’s process,
correlation ⟨F(z(t))R[t; z(τ)]⟩, where F(z) is arbitrary function, is described,
in view of Eq. (5.99), by the formula similar to Eq. (7.45)
⟨F (z(t)) R[t; z(τ)]⟩= ⟨F (a) R[t; a]⟩a e−νt
+ ν
t

0
dt1e−ν(t−t1) ⟨F (a) R [t; aθ(τ −t1) + z (τ) θ(t1 −τ)]⟩a,z .
(7.46)
7.7
General-Form Markovian Processes
Processes such as mentioned telegrapher’s processes are the simplest exam-
ples of the Markovian processes. Here, we consider what consequences can
be derived for the correlation of functionals from the sole assumption that
process z(t) is the Markovian process.
In the case of the general-form Markovian process z(t), we have no equa-
tion for the characteristic functional. We have only integral equation (5.89),
page 152 for the functional
Ψ[t, z; v(τ)] =
4
δ (z(t)−z) exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5

7.7
General-Form Markovian Processes
179
that describes statistical relationship of process z(t) at instant t with its
prehistory
Ψ[t, z; v(τ)] = P(t, z) + i
t

0
dt1v(t1)
∞

−∞
dz1z1p(z, t|z1, t1)Ψ[t1, z1; v(τ)],
(7.47)
where P(t, z) is the one-time probability density and p(z, t|z0, t0) is the tran-
sition probability density of process z(t). In this case, we can again use the
method described above to obtain the expression for the correlator
⟨δ (z(t)−z) R[t; z(τ) + η(τ)]⟩
(τ ⩽t)
in the form of the integral equality with variational derivatives
⟨δ (z(t)−z) R[t; z(τ) + η(τ)]⟩= Ψ

t, z;
δ
iδη(τ)

R[t; η(τ)],
(7.48)
where η(t) is arbitrary function.
For the Markovian processes z(t), functions P(t, z) and p(z, t|z0, t0) satisfy
linear operator equations (5.66), page 147
∂
∂tP(t, z) = %L(z)P(t, z),
∂
∂tp(z, t|z0, t0) = %L(z)p(z, t|z0, t0),
(7.49)
where %L(z) is the integro-diﬀerential operator with respect to variable z.
Let us diﬀerentiate Eq. (7.48) with respect to time t and take into account
that variational derivative
δ
δη(t1)R[t; η(τ)] ∼θ(t −t1) by the deﬁnition of
functional R[t; z(τ)], so that we have no need in diﬀerentiating the integral in
Eq. (7.47) with respect to the upper limit (we can set it to ∞). An additional
point consists in the fact that the diﬀerentiation operation commutes with
the variational diﬀerentiation operation (see Eq. (2.13), page 86):
∂
∂t
δ
δη(t1)R[t; η(τ)] =
δ
δη(t1)
∂
∂tR[t; η(τ)].
Taking into account Eqs. (7.49), we obtain the formula for the derivative of
the correlation of interest with respect to time (function η(t) can be set to
zero) [47–49]
∂
∂t ⟨δ (z(t) −z) R[t; z(τ)]⟩=
.
δ (z(t) −z) ∂
∂tR[t; z(τ)]
/
+%L(z) ⟨δ (z(t) −z) R[t; z(τ)]⟩.
(7.50)
Multiply now Eq. (7.50) by arbitrary function f(z) and integrate the result
over z. The result will be the diﬀerentiation formula

180
7
Correlation Splitting
∂
∂t ⟨f (z(t)) R[t; z(τ)]⟩=
.
f (z(t)) ∂
∂tR[t; z(τ)]
/
+
∞

−∞
dzf(z)%L(z) ⟨δ (z(t)−z) R[t; z(τ)]⟩,
(7.51)
that can be rewritten in the form
∂
∂t ⟨f (z(t)) R[t; z(τ)]⟩−
.
f (z(t)) ∂
∂tR[t; z(τ)]
/
=
2
R[t; z(τ)]

%L+(z)f(z)
3
,
(7.52)
where we introduced operator %L+(z) conjugated to operator %L(z).
Thus, Eqs. (7.50)–(7.52) govern the rules of diﬀerentiating with respect to
time the correlators of functions of the Markovian process z(t) with function-
als of this process.
Note that, if the mean value of process z(t) is equal to zero, the right-
hand side of Eq. (7.52) can be expressed in terms of the desired correlation
⟨z(t)R[t; z(τ)]⟩for all Markovian processes considered earlier. This is most
probably the reason of low practicability of this formula. However, for telegra-
pher’s and generalized telegrapher’s processes, Eq. (7.52) appears practicable
for analyzing linear stochastic equations. Indeed, for telegrapher’s process
z(t), the right-hand side of Eq. (7.52) for correlation ⟨z(t)R[t; z(τ)]⟩has the
form
−2ν ⟨z(t)R[t; z(τ)]⟩.
(7.53)
For generalized telegrapher’s process z(t), the right-hand side of Eq. (7.52)
under the condition that ⟨f(a)⟩= 0 assumes the form
−ν ⟨f (z(t)) R[t; z(τ)]⟩.
(7.54)
In the special case f(z) = z, Eq. (7.54) reduces to
−ν ⟨z(t)R[t; z(τ)]⟩.
(7.55)
Now, we dwell on some extensions of the above formulas. First of
all, we note that if we deal with the vector Markovian process z(t) =
{z1(t), · · · , zN(t)} described by operator %L(z), then functional
Ψ[t, z; v(τ)] =
4
δ (z(t) −z) exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
,
where v(t) = {v1(t), · · · , vN(t)}, satisﬁes the equation
∂
∂tΨ[t, z; v(τ)] =
"
%L(z)+izv
#
Ψ[t, z; v(τ)]
(7.56)

7.8
Delta-Correlated Random Processes
181
with the initial value Ψ[0, z; v(τ)] = P0(z). With this remark, we can easily
derive the formula of diﬀerentiating the correlator ⟨F(z(t))R[t; z(τ)]⟩with
respect to time; it assumes the form
∂
∂t ⟨F (z(t)) R[t; z(τ)]⟩=
.
F (z(t)) ∂
∂tR[t; z(τ)]
/
+
2
R[t; z(τ)]

%L+(z)F(z)
3
,
(7.57)
where %L+(z) is the operator conjugated to operator %L(z).
An important special case corresponds to the situation in which all com-
ponents of vector z(t) are statistically independent Markovian processes des-
cribed by the same operator %L(z); in this case, Eq. (7.57) reduces to the form
∂
∂t ⟨F (z(t)) R[t; z(τ)]⟩=
)
F (z(t)) ∂
∂tR[t; z(τ)]
*
+
N
 
k=1
%
R[t; z(τ)]
+
L+(zk)F(z)
,&
.
(7.58)
For example, all above Markovian processes having the exponential correla-
tion function ⟨z(t)z(t + τ)⟩=
,
z2-
e−α|τ| satisfy the equality
 ∂
∂t + αk

⟨z1(t) · · · zk(t)R[t; z(τ)]⟩=
.
z1(t) · · · zk(t) ∂
∂tR[t; z(τ)]
/
.
(7.59)
Formula (7.59) deﬁnes the rule of factoring the operation of diﬀerentiating
with respect to time out of angle brackets of averaging; in particular, we have
.
z1(t) · · · zk(t) ∂n
∂tn R[t; z(τ)]
/
=
 ∂
∂t + αk
n
⟨z1(t) · · · zk(t)R[t; z(τ)]⟩,
(7.60)
where k = 1, · · · , N.
7.8
Delta-Correlated Random Processes
7.8.1
General Remarks
Random processes z(t) that can be treated as delta-correlated processes are of
special interest in physics. The importance of this approximation follows ﬁrst
of all from the fact that it is physically obvious in the context of many phy-
sical problems; moreover, the corresponding dynamic systems always allow
obtaining closed equations for probability densities of the solutions to these
systems.
For the Gaussian delta-correlated (in time) process, correlation function
has the form
B(t1, t2) = ⟨z(t1)z(t2)⟩= B(t1)δ(t1 −t2),
(⟨z(t)⟩= 0).

182
7
Correlation Splitting
In this case, functionals Θ[t; v(τ)], Ω[t′, t; v(τ)], and Ω[t, t; v(τ)] introduced
earlier in Eqs. (7.14), (7.15), page 169 assume the forms
Θ[t; v(τ)] = −1
2
t

0
dτB(τ)v2(τ),
Ω[t′, t; v(τ)] = iB(t′)v(t′),
Ω[t, t; v(τ)] = i
2B(t)v(t),
and Eqs. (7.16) appear signiﬁcantly simpler
⟨z(t′)R[t; v(τ)]⟩= B(t′)
.
δ
δz(t′)R[t; v(τ)]
/
(0 < t′ < t),
⟨z(t)R[t; v(τ)]⟩= 1
2B(t)
.
δ
δz(t)R[t; v(τ)]
/
.
(7.61)
Formulas (7.61) show that statistical averages of the Gaussian delta-
correlated process considered here are discontinuous at t′ = t. This disconti-
nuity is completely dictated by the fact that this process is delta-correlated;
if process is not delta-correlated, no discontinuity occurs (see Eq. (7.16),
page 169).
The Poisson delta-correlated random process corresponds to the limit pro-
cess
g(t) →δ(t).
In this case, the logarithm of the characteristic functional has the simple form
(see (5.33), page 136); as a consequence, Eqs. (7.25), page 172 for functionals
Ω[t′, t; v(τ)] and Ω[t, t; v(τ)] assume the forms
Ω[t′, t; v(τ)] = i
∞

−∞
dξξp(ξ)eiξv(t′)
(t′ < t),
Ω[t, t; v(τ)] =
ν
iv(t)
∞

−∞
dξξp(ξ)

eiξv(t) −1

= ν
∞

−∞
dξp(ξ)
ξ

0
dηeiηv(t),
and we obtain the following expression for the correlation of the Poisson
random process z(t) with a functional of this process
⟨z(t′)R[t; z(τ)]⟩= ν
∞

−∞
dξξp(ξ) ⟨R[t; z(τ) + ξδ(τ −τ ′)]⟩
(t′ ⩽t),
⟨z(t)R[t; z(τ)]⟩= ν
∞

−∞
dξp(ξ)
ξ

0
dη ⟨R[t; z(τ) + ηδ(t −τ)]⟩.
(7.62)

7.8
Delta-Correlated Random Processes
183
These expressions also show that statistical averages are discontinuous at
t′ = t. As in the case of the Gaussian process, this discontinuity is completely
dictated by the fact that this process is delta-correlated.
In the general case of delta-correlated process z(t), we can expand the
logarithm of characteristic functional in the functional Taylor series
Θ[t; v(τ)] =
∞
&
n=1
in
n!
t

0
dτKn(τ)vn(τ),
(7.63)
where cumulant functions assume the form
Kn(t1, · · · , tn) = Kn(t1)δ(t1 −t2) · · · δ(tn−1 −tn).
As can be seen from Eq. (7.63), a characteristic feature of these processes
consists in the validity of the equality
˙Θ[t; v(τ)] = ˙Θ[t; v(t)]

˙Θ[t; v(τ)] = d
dtΘ[t; v(τ)]

,
(7.64)
which is of fundamental signiﬁcance. This equality shows that, in the case
of arbitrary delta-correlated process, quantity Θ[t; v(τ)] appears not a func-
tional, but simply a function of time t. In this case, functionals Ω[t′, t; v(τ)]
and Ω[t, t; v(τ)] are
Ω[t′, t; v(τ)] =
∞
&
n=0
in
n!Kn+1(t′)vn(t′)
(t′ < t) ,
Ω[t, t; v(τ)] =
∞
&
n=0
in
(n + 1)!Kn+1(t)vn(t),
and formulas for correlation splitting assume the forms
⟨z(t′)R[t; z(τ)]⟩=
∞
&
n=0
in
n!Kn+1(t′)
.δnR[t; z(τ)]
δzn(t′)
/
(t′ < t) ,
⟨z(t)R[t; z(τ)]⟩=
∞
&
n=0
in
(n + 1)!Kn+1(t)
.δnR[t; z(τ)]
δzn(t)
/
.
(7.65)
These formulas describe the discontinuity of statistical averages at t′ = t in
the general case of delta-correlated processes.
Note that, for t′ > t, delta-correlated processes satisfy the obvious equality
⟨z(t′)R[t; z(τ)]⟩= ⟨z(t′)⟩⟨R[t; z(τ)]⟩.
(7.66)
Now, we dwell on the concept of random delta-correlated (in time) ﬁelds.

184
7
Correlation Splitting
We will deal with vector ﬁeld f(x, t), where x describes the spatial
coordinates and t is the temporal coordinate. In this case, the logarithm
of characteristic functional can be expanded in the Taylor series with coeﬃ-
cients expressed in terms of cumulant functions of random ﬁeld f(x, t) (see
Sect. 3.2). In the special case of cumulant functions
Ki1,··· ,im
n
(x1, t1; · · · , xn, tn)=Ki1,··· ,im
n
(x1, · · · , xn; t1)δ(t1 −t2) · · · δ(tn−1 −tn),
(7.67)
we will call ﬁeld f(x, t) the random ﬁeld delta-correlated in time t. In this
case, functional Θ[t; ψ(x′, τ)] assumes the form
Θ[t; ψ(x′, τ)] =
∞
&
n=1
in
n!
t

0
dτ

· · ·

dx1 · · · dxnKi1,··· ,im
n
(x1, · · · , xn; τ)
× ψi1(x1, τ) · · · ψim(xn, τ),
(7.68)
An important feature of this functional is the fact that it satisﬁes the equality
similar to Eq. (7.64):
˙Θ[t; ψ(x′, τ)] = ˙Θ[t; ψ(x′, t)].
(7.69)
7.8.2
Asymptotic Meaning of Delta-Correlated
Processes and Fields
The nature knows no delta-correlated processes. All actual processes and
ﬁelds are characterized by a ﬁnite temporal correlation radius, and delta-
correlated processes and ﬁelds result from asymptotic expansions in terms of
their temporal correlation radii.
We illustrate the appearance of delta-correlated processes using the sta-
tionary Gaussian process with correlation radius τ0 as an example. In this
case, the logarithm of the characteristic functional is described by the expres-
sion
Θ[t; v(τ)] = −
t

0
dτ1v(τ1)
τ1

0
dτ2B
τ1 −τ2
τ0

v(τ2).
(7.70)
Setting τ1 −τ2 = ξτ0, we transform Eq. (7.70) to the form
Θ[t; v(τ)] = −τ0
t

0
dτ1v(τ1)
τ1/τ0

0
dξB (ξ) v(τ1 −ξτ0).
Assume now that τ0 →0. In this case, the leading term of the asymptotic
expansion in parameter τ0 is given by the formula

7.8
Delta-Correlated Random Processes
185
Θ[t; v(τ)] = −τ0
∞

0
dξB (ξ)
t

0
dτ1v2(τ1)
that can be represented in the form
Θ[t; v(τ)] = −Beﬀ
t

0
dτ1v2(τ1),
(7.71)
where
Beﬀ=
∞

0
dτB
 τ
τ0

= 1
2
∞

−∞
dτB
 τ
τ0

.
(7.72)
Certainly, asymptotic expression (7.71) holds only for the functions v(t)
that vary during times about τ0 only slightly, rather than for arbitrary func-
tions v(t). Indeed, if we specify this function as v(t) = vδ(t −t0), asymp-
totic expression (7.71) fails; in this case, we must replace Eq. (7.70) with the
expression
Θ[t; v(τ)] = −1
2B(0)v2
(t > t0)
corresponding to the characteristic function of process z(t) at a ﬁxed time
t = t0.
Consider now correlation ⟨z(t)R[t; z(τ)]⟩given, according to the Furutsu–
Novikov formula (7.16), page 169, by the expression
⟨z(t)R[t; z(τ)]⟩=
t

0
dt1B
t −t1
τ0
 .
δ
δz(t1)R[t; z(τ)]
/
.
The change of integration variable t −t1 →ξτ0 transforms this expression to
the form
⟨z(t)R[t; z(τ)]⟩= τ0
t/τ0

0
dξB (ξ)
.
δ
δz(t −ξτ0)R[t; z(τ)]
/
,
(7.73)
which grades for τ0 →0 into the equality obtained earlier for the Gaussian
delta-correlated process
⟨z(t)R[t; z(τ)]⟩= Beﬀ
.
δ
δz(t)R[t; z(τ)]
/
provided that the variational derivative in Eq. (7.73) varies only slightly dur-
ing times of about τ0.

186
7
Correlation Splitting
Thus, the approximation of process z(t) by the delta-correlated one is
conditioned by small variations of functionals of this process during times of
about process’ temporal correlation radius.
Consider now telegrapher’s and generalized telegrapher’s processes. In the
case of telegrapher’s process, the characteristic functional satisﬁes Eq. (5.40),
page 139. The correlation radius of this process is τ0 = 1/2ν, and, for ν →∞
(τ0 →0), this equation grades for suﬃciently smooth functions v(t) into the
equation
d
dtΦ[t; v(τ)] = −a2
0
2ν v2(t)Φ[t; v(τ)],
(7.74)
which corresponds to the Gaussian delta-correlated process. If we additionally
assume that a2
0 →∞and
lim
ν→∞a2
0/2ν = σ2
0,
then Eq. (7.74) appears independent of parameter ν. Of cause, this fact does
not mean that telegrapher’s process looses its telegrapher’s properties for
ν →∞. Indeed, for ν →∞, the one-time probability distribution of process
z(t) will as before correspond to telegrapher’s process, i.e., to the process
with two possible states. As regards the correlation function and higher-
order moment functions, they will possess for ν →∞all properties of delta-
functions in view of the fact that
lim
ν→∞
"
2νe−2ν|τ|#
=

0,
if
τ ̸= 0,
∞,
if
τ = 0.
Such functions should be considered the generalized functions; their delta-
functional behavior will manifest itself in the integrals of them (see, e.g., [22]).
As Eq. (7.74) shows, the limit process ν →∞is equivalent for these quantities
to the replacement of process z(t) by the Gaussian delta-correlated process.
This situation is completely similar to the approximation of the Gaussian
random process with a ﬁnite correlation radius τ0 by the delta-correlated
process for τ0 →0.
In a similar way, we obtain that generalized telegrapher’s process
whose characteristic functional satisﬁes integro-diﬀerential equation (5.100),
page 154 is governed for ν →∞and suﬃciently smooth functions v(t) by the
equation (here, we assume ⟨a⟩= 0 for simplicity)
d
dtΦ[t; v(τ)] = −
,
a2-
ν
v2(t)Φ[t; v(τ)],
which again corresponds to the Gaussian delta-correlated process.
Consider the square of the Gaussian stationary process, i.e., process z(t) =
ξ2(t), where ξ(t) is the Gaussian process with parameters
⟨ξ(t)⟩= 0,
⟨ξ(t1)ξ(t2)⟩= B(t1 −t2),

7.8
Delta-Correlated Random Processes
187
as a more complicated example.
Let as calculate the characteristic functional of this process
Φ[t; v(τ)] = ⟨ϕ[t; ξ(τ)]⟩,
ϕ[t; ξ(τ)] = exp
⎧
⎨
⎩i
t

0
dτv(τ)ξ2(τ)
⎫
⎬
⎭.
(7.75)
The characteristic functional of process z(t) satisﬁes the stochastic equation
d
dtΦ[t; v(τ)] = iv(t)
,
ξ2(t)ϕ[t; ξ(τ)]
-
.
(7.76)
Consider quantity Ψ(t1, t)
=
⟨ξ(t1)ξ(t)ϕ[t; ξ(τ)]⟩. According to the
Furutsu–Novikov formula (7.11), page 168,
Ψ(t1, t) =
t

0
dt′B(t1 −t′)
.
δ
δξ(t′)ξ(t)ϕ[t; ξ(τ)]
/
.
(7.77)
Calculating now the variational derivative in the right-hand side of Eq. (7.77)
with the use of the explicit expression for functional ϕ[t; ξ(τ)], we obtain the
integral equation for function Ψ(t1, t)
Ψ(t1, t) = B(t1 −t)Φ[t; v(τ)] + 2i
t

0
dτB(t1 −τ)v(τ)Ψ(τ, t).
(7.78)
Function Ψ(t1, t) is representable in the form Ψ(t1, t) = S(t1, t)Φ[t; v(τ)],
where function S(t1, t) satisﬁes the linear integral equation
S(t1, t) = B(t1 −t) + 2i
t

0
dτB(t1 −τ)v(τ)S(τ, t).
(7.79)
As a consequence, characteristic functional Φ[t; v(τ)] can be represented in
the form
Φ[t; v(τ)] = exp
⎧
⎨
⎩i
t

0
dτv(τ)S(τ, τ)
⎫
⎬
⎭.
(7.80)
Thus, the expansion of quantity S(t, t) in the functional Taylor series in
v(τ) determines the cumulants of process z(t) = ξ2(t). Because Eq. (7.79)
is the linear integral equation, we can represent its solution as the iterative
series S(t, t) =
∞
6
n=0
S(n)(t, t), where

188
7
Correlation Splitting
S(n)(t, t) = (2i)n
t

0
dτ1 · · ·
t

0
dτn v(τ1) · · · v(τn)B(t−τ1)B(τ1−τ2) · · · B(τn−t).
(7.81)
If function v(t) varies slowly during correlation time τ0 of process ξ(t)
(which means that we omit from consideration the one-time characteristic
functions of process z(t) = ξ2(t)), we can proceed to the limit τ0 →0. As a
result, we obtain the expressions
S(0)(t, t) = B(0),
S(n)(t, t) = (2i)nvn(t)
∞

0
· · ·
∞

0
dτ1 · · · dτnB(τ1)B(τ1 −τ2) · · · B(τn),
(7.82)
from which follows that process z(t) = ξ2(t) in this limit can be consid-
ered the delta-correlated (in time t) random process. The eﬀective expansion
parameter of quantity S(t, t) in series (7.81) is in this case β = τ0B(0)v(t).
If β ≪1, we can content ourselves with the ﬁrst term of series (7.82), which
corresponds to the standard perturbation theory. But if β ∼1, one needs
take into account the whole series for function S(t, t).
The Gaussian Markovian process ξ(t) with correlation function B(τ) =
σ2e−α|τ|, where α = 1/τ0, allows a more detailed analysis. In this case, integ-
ral equation (7.79) assumes for t1 < t the form
S(t1, t) = σ2e−α(t−t1) + 2iσ2
t

0
dτe−α|t1−τ|v(τ)S(τ, t).
(7.83)
The solution to this equation as a function of parameter t can be described
as the initial-value problem
∂
∂tS(t1, t) = {−α + 2iv(t)S(t, t)} S(t1, t),
S(t1, t)|t=t1 = S(t1, t1),
d
dtS(t, t) = −2α

S(t, t) −σ2
+ 2iv(t)S2(t, t),
S(t, t)|t=0 = σ2.
(7.84)
The asymptotic solution of the latter equation for α →∞
(τ0 →0) has the
form
S(t, t) = 1 −

1 −4iσ2τ0v(t)
2iτ0v(t)
,
S(n)(t, t) = [2iτ0v(t)]n σ2(n+1) (2n −1)!!
(n + 1)! ,
which coincides with the solution of Eq. (7.84) under the assumption that
function v(t) is a constant. As a consequence, the logarithm of the characte-
ristic functional in this asymptotic case is given by the formula Θ[t; v(τ)] =

7.8
Delta-Correlated Random Processes
189
1
2τ0
t

0
dτ

1 −

1 −4iσ2τ0v (τ)

, and cumulants of process z(t) = ξ2(t) have
the forms (K1 = σ2)
Kn(t1, · · · , tn) = (2τ0)n−1σ2n(2n −3)!!δ(t1 −t2) · · · δ(tn−1 −tn),
which correspond to Eqs. (7.82); in this case, we have
In =
∞

0
dτ1 · · ·
∞

0
dτn exp {−τ1 −|τ1 −τ2| −· · · −|τn−1 −τn| −τn}= (2n −1)!!
(n + 1)! .
Process z(t) = zξ(t), where ξ(t) is the Gaussian delta-correlated process
with parameters ⟨ξ(t)⟩= 0, ⟨ξ(t)ξ(t′)⟩= 2σ2τ0δ(t −t′), and quantity z is
the random quantity with probability density p(z), is an example of the
process that cannot be considered the delta correlated process. In this case,
the characteristic functional is given by the equality
Φ[t; v(τ)] =

exp
⎧
⎨
⎩iz
t

0
dτv(τ)ξ(τ)
⎫
⎬
⎭

z,ξ
=
∞

−∞
dzp(z) exp
⎧
⎨
⎩−z2σ2τ0
t

0
dτv2(τ)
⎫
⎬
⎭,
(7.85)
and process z(t) cannot be considered the delta-correlated process, because
it does not satisfy Eq. (7.64) despite its second cumulant has the form
K2(t1, t2) = ⟨z(t1)z(t2)⟩= 2
,
z2-
σ2τ0δ(t1 −t2).
This follows from the fact that process z(t) is formed as the product of two
processes — process z with the inﬁnite correlation radius and process ξ(t)
with the zero-valued correlation radius.
Now, we proceed to the direct statistical analysis of stochastic dynamic
systems.

Part III
Stochastic Equations

Chapter 8
General Approaches to Analyzing
Stochastic Dynamic Systems
In this chapter, we will consider basic methods of determining statistical
characteristics of solutions to stochastic equations.
Consider a linear (diﬀerential, integro-diﬀerential, or integral) stochastic
equation. Averaging of such an equation over an ensemble of realizations of
ﬂuctuating parameters will not result generally in a closed equation for the
corresponding average value. To obtain the closed equation, we must deal
with an additional extended space whose dimension appears to be inﬁnite
in most cases. This approach makes it possible to derive the linear equation
for average quantity of interest, but this equation will contain variational
derivatives.
Consider some special types of dynamic systems.
8.1
Ordinary Diﬀerential Equations
Let dynamics of vector function x(t) is described by the ordinary diﬀerential
equation
d
dtx(t) = v(x, t) + f(x, t),
x(t0) = x0.
(8.1)
Here, function v(x, t) is the deterministic function and f(x, t) is the random
function.
The solution to Eq. (8.1) is a functional of f(y, τ)+v(y, τ) with τ ∈(t0, t),
i.e.,
x(t) = x[t; f(y, τ) + v(y, τ)].
From this fact follows the equality
δ
δfj(y, τ)F (x(t)) =
δ
δvj(y, τ)F (x(t)) = ∂F(x(t))
∂xi
δxi(t)
δfj(y, τ)
valid for arbitrary function F(x). In addition, we have
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
193
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_8, c
⃝Springer International Publishing Switzerland 2015

194
8
General Approaches to Analyzing Stochastic Dynamic Systems
δ
δfj(y,t −0)xi(t) =
δ
δvj(y,t −0)xi(t) = δijδ (x(t)−y) .
The corresponding Liouville equation for the indicator function
ϕ(x, t) = δ(x(t) −x)
follows from Eq. (8.1) and has the form
∂
∂tϕ(x, t) = −∂
∂x {[v(x, t) + f(x, t)] ϕ(x, t)} , ϕ(x, t0) = δ(x −x0),
(8.2)
from which follows the equality
δ
δf(y, t −0)ϕ(x, t) =
δ
δv(y,t −0)ϕ(x, t) = −∂
∂x {δ(x −y)ϕ(x, t)} .
(8.3)
Using this equality, we can rewrite Eq. (8.2) in the form, which may look at
ﬁrst sight more complicated
 ∂
∂t + ∂
∂xv(x, t)

ϕ(x, t) =

dyf(y, t)
δ
δv(y, t)ϕ(x, t).
(8.4)
Consider now the one-time probability density for solution x(t) of Eq. (8.1)
P(x, t) = ⟨ϕ(x, t)⟩= ⟨δ(x(t) −x)⟩.
Here, x(t) is the solution of Eq. (8.1) corresponding to a particular realization
of random ﬁeld f(x, t), and angle brackets ⟨· · · ⟩denote averaging over an
ensemble of realizations of ﬁeld f(x, t).
Averaging Eq. (8.4) over an ensemble of realizations of ﬁeld f(x, t), we
obtain the expression
 ∂
∂t + ∂
∂xv(x, t)

P(x, t) =

dy
δ
δv(y, t) ⟨f(y, t)ϕ(x, t)⟩.
(8.5)
Quantity ⟨f(y, t)ϕ(x, t)⟩in the right-hand side of Eq. (8.5) is the correlator
of random ﬁeld f(y, t) with function ϕ(x, t), which is a functional of random
ﬁeld f(y, τ) and is given either by Eq. (8.2), or by Eq. (8.4).
The characteristic functional
Φ[t, t0; u(y, τ)] =
4
exp
⎧
⎨
⎩i
t

t0
dτ

dyf(y, τ)u(y, τ)
⎫
⎬
⎭
5
= exp {Θ[t, t0; u(y, τ)]}
exhaustively describes all statistical characteristics of random ﬁeld f(y, τ)
for τ ∈(t0, t).

8.1
Ordinary Diﬀerential Equations
195
We split correlator ⟨f(y, t)ϕ(x, t)⟩using the technique of functionals. In-
troducing functional shift operator with respect to ﬁeld v(y, τ), we represent
functional ϕ[x, t; f(y, τ) + v(y, τ)] in the operator form
ϕ[x, t; f(y, τ) + v(y, τ)] = exp
⎡
⎣
t

t0
dτ

dyf(y, τ)
δ
δv(y, τ)
⎤
⎦ϕ[x, t; v(y, τ)].
With this representation, the term in the right-hand side of Eq. (8.5) assumes
the form

dy
δ
δvj(y, t)
4
fj(y, t) exp
⎧
⎨
⎩
t

t0
dτ

dy′f(y′, τ)
δ
δv(y′, τ)
⎫
⎬
⎭
5
4
exp
⎧
⎨
⎩
t

t0
dτ

dy′f(y′, τ)
δ
δv(y′, τ)
⎫
⎬
⎭
5
P(x, t)
= ˙Θt

t, t0;
δ
iδv(y, τ)

P(x, t),
where we introduced the functional
˙Θt[t, t0; u(y, τ)] = d
dt ln Φ[t, t0; u(y, τ)].
Consequently, we can rewrite Eq. (8.5) in the form
 ∂
∂t + ∂
∂xv(x, t)

P(x, t) = ˙Θt

t, t0;
δ
iδv(y, τ)

P(x, t).
(8.6)
Equation (8.6) is the closed equation with variational derivatives in the
functional space of all possible functions {v(y , τ)}. However, for a ﬁxed
function v(x, t), we arrive at unclosed equation [47–49]
 ∂
∂t + ∂
∂xv(x, t)

P(x, t) =
.
˙Θt

t, t0;
δ
iδf(y, t)

ϕ[x, t; f(y, τ)]
/
,
P(x, t0) = δ(x −x0).
(8.7)
Equation (8.7) is the exact consequence of the initial dynamic equation
(8.1). Statistical characteristics of random ﬁeld f(x, t) appear in this equa-
tion only through functional ˙Θt[t, t0; u(y, τ)] whose expansion in the func-
tional Taylor series in powers of u(x, τ) depends on all space-time cumulant
functions of random ﬁeld f(x, t).

196
8
General Approaches to Analyzing Stochastic Dynamic Systems
Note that equation for the one-time probability density P(x, t) preserve
the form of Eq. (8.7) even for more general integro-diﬀerential equation
d
dtxi(t) = vi(x, t) +

dyDij(x, y, t)fj(y, t),
x(t0) = x0,
in which case the variational derivative has the form
δ
δfj(y,t −0)xi(t) = Dij(x(t), y, t).
As we mentioned earlier, Eq. (8.7) is not closed in the general case with
respect to function P(x, t), because quantity
˙Θt

t, t0;
δ
iδf(y, τ)

δ(x(t) −x)
appeared in averaging brackets depends on the solution x(t) (which is
a functional of random ﬁeld f(y, τ)) for all times t0 < τ < t. How-
ever, in some cases, the variational derivative in Eq. (8.7) can be expres-
sed in terms of ordinary diﬀerential operators. In such conditions, equa-
tions like Eq. (8.7) will be the closed equations in the corresponding
probability densities. The corresponding examples will be given below.
Note that Eq. (8.2) is the forward Liouville equation and describes the
evolution of indicator function
ϕ(x, t) = ϕ(x, t|x0, t0) = δ (x(t) −x|x(t0) = x0)
in time t. For this reason, we can call Eq. (8.7) the forward equation for
probability density.
In Chapter 3, we obtained the backward Liouville equation (3.9), page 97
for the indicator function, which describes the evolution of dynamic system
(8.1) in terms of initial values t0 and x0. In our case, this equation has the
form,
 ∂
∂t0
+v(x0,t0) ∂
∂x0

ϕ(x, t|x0, t0) = −f(x0, t0) ∂
∂x0
ϕ(x, t|x0, t0),
ϕ(x, t|x0, t) = δ(x −x0).
(8.8)
Averaging Eq. (8.8) over an ensemble of realizations of ﬁeld f(x0, t0) and
performing the procedure similar to that used in derivation of Eq. (8.7), we
obtain the equation for probability density
P(x, t|x0, t0) = ⟨δ(x, t|x0, t0)⟩
in the form

8.1
Ordinary Diﬀerential Equations
197
 ∂
∂t0 + v(x0, t0) ∂
∂x0

P(x, t|x0, t0) =
)
˙Θt0

t, t0;
δ
iδf(y, τ)

δ (x(t|x0, t0) −x)
*
,
P(x, t|x0, t) = δ(x −x0),
(8.9)
where
˙Θt0[t, t0; u(y, τ)] =
d
dt0
ln Φ[t, t0; u(y, τ)].
Equation (8.9) describes the evolution of the probability density as a function
of initial parameters {x0, t0}; for this reason, we can call it the backward
equation .
The forward and backward equations are equivalent. The forward equation
appears to be more convenient for studying the behavior of statistical cha-
racteristics of solutions to Eq. (8.1) in time domain. The backward equation
is more convenient for studying the statistical characteristics that concern
the residence of random process x(t) within certain region of space, such as
residence duration within the region and time of arrival at region boundary.
Indeed, the probability of residence of random process x(t) in spatial region
V is given by the integral
G(t; x0, t0) =

V
dxP(x, t|x0, t0),
which, according to Eq. (8.9), will satisfy the equation
 ∂
∂t0 + v(x0, t0) ∂
∂x0

G(t|x0, t0) = ˙Θt0

t, t0;
δ
iδf(y, τ)
 
V
dxδ (x(t|x0, t0) −x) ,
(8.10)
G(t|x0, t0) =
 1 (x0 ∈V )
0 (x0 /∈V ) .
This equation must be supplemented with the boundary conditions following
from the nature of each particular problem and depend on region V and
region boundaries.
Proceeding in a similar way, one can obtain the equation similar to Eq. (8.7)
for the m-time probability density that refers to m diﬀerent instants t1 < t2 <
· · · < tm
Pm(x1, t1; · · · ; xm, tm) = ⟨ϕm(x1, t1; · · · ; xm, tm)⟩,
(8.11)
where the indicator function is deﬁned by the equality
ϕm(x1, t1; · · · ; xm, tm) = δ(x(t1) −x1) · · · δ(x(tm) −xm).
Diﬀerentiating Eq. (8.11) with respect to time tm and using then dynamic
equation (8.1), one can obtain the equation

198
8
General Approaches to Analyzing Stochastic Dynamic Systems
 ∂
∂tm
+
∂
∂xm
v(xm,tm)

Pm(x1, t1; · · · ; xm, tm)
=
.
˙Θtm

tm, t0;
δ
iδf(y, τ)

ϕm(x1, t1; · · · ; xm, tm)
/
.
(8.12)
No summation over index m is performed here. The initial value to Eq. (8.12)
can be derived from Eq. (8.11). Setting tm = tm−1 in Eq. (8.11), we obtain
the equality
Pm(x1, t1; · · · ; xm, tm−1) = δ(xm −xm−1)Pm−1(x1, t1; · · · ; xm−1, tm−1),
which just determines the initial value for Eq. (8.12).
8.2
Partial Diﬀerential Equations
Above, we considered statistical description of dynamic systems starting from
the Liouville equation (8.2) that matches the ordinary diﬀerential equation
(8.1). It is quite obvious that the derivation procedure of Eqs. (8.6), (8.7),
(8.12), and the like can be applied to other dynamic systems speciﬁed in
terms of linear equations both in ﬁnite- and inﬁnite-dimension spaces, i.e., in
terms of partial diﬀerential equations of the ﬁrst and higher orders. Below,
we consider the use of these equations by the speciﬁc examples, such as
passive tracer transfer in random ﬁeld of velocities Eqs. (1.56), page 34, (3.16),
and (3.20), page 101; wave propagation in random media described within
the framework of the parabolic equation of quasi-optics (1.153), page 63;
and hydrodynamic turbulence evolution described by the integro-diﬀerential
equation (1.171), page 69.
8.2.1
Passive Tracer Transfer in Random Field of
Velocities
The ﬁrst example deals with Eq. (3.20), page 101
 ∂
∂t + U(r, t) ∂
∂r

ϕ(r, t; ρ) = ∂U(r, t)
∂r
∂
∂ρρϕ(r, t; ρ)
(8.13)
for indicator function
ϕ(r, t; ρ) = δ (ρ(r, t) −ρ) .

8.2
Partial Diﬀerential Equations
199
We assume that U(r, t) = u0(r , t) + u(r, t), where u0(r, t) is the deter-
ministic component of the velocity ﬁeld (mean ﬂow) and u(r, t) is the random
component.
A consequence of Eq. (8.13) is the equality
δϕ(r, t; ρ)
δUj(r′,t −0) =
δϕ(r, t; ρ)
δuj(r′, t −0) =
δϕ(r, t; ρ)
δu0j(r′, t −0)
=

−δ(r −r′) ∂
∂rj
+∂δ(r −r′)
∂rj
∂
∂ρρ
	
ϕ(r, t; ρ).
(8.14)
Statistical characteristics of random ﬁeld u(r, t) can be exhaustively des-
cribed in terms of the characteristic functional
Φ[t; ψ(r′, τ)]=
4
exp
⎧
⎨
⎩i
t

0
dτ

dr′u(r′, τ)ψ(r′, τ)
⎫
⎬
⎭
5
=exp {Θ[t; ψ(r′, τ)]} .
Now, we average Eq. (8.13) over an ensemble of realizations of random
ﬁeld u(r, t). Then, replicating derivation of Eq.(8.7) and taking into account
Eq. (8.14), we obtain that the one-time probability density
P(r, t; ρ) = ⟨ϕ(r, t; ρ)⟩= ⟨δ (ρ(r, t) −ρ)⟩
satisﬁes the equation
 ∂
∂t + u0(r, t) ∂
∂r

P(r, t; ρ) = ∂u0(r, t)
∂r
∂
∂ρρP(r, t; ρ)
+
.
dru(r, t)
δ
δu0(r, t −0)ϕ(r, t; ρ)
/
,
(8.15)
whose last term can be rewritten in the form
)
dru(r, t)
δ
δu0(r, t −0)ϕ[r, t; ρ; u + u0]
*
=

dru(r, t)
δ
δu0(r, t −0) exp
⎧
⎨
⎩
t

0
dτ

dr′u(r′, τ)
δ
δu0(r′, τ)
⎫
⎬
⎭

ϕ[r, t; ρ; u0]
=

d
dt exp
⎧
⎨
⎩
t

0
dτ

dr′u(r′, τ)
δ
δu0(r′, τ)
⎫
⎬
⎭

ϕ[r, t; ρ; u0]
= ˙Θt

t;
δ
iδu0(r′, τ)

⟨ϕ[r, t; ρ; u + u0]⟩= ˙Θt

t;
δ
iδu0(r′, τ)

P(r, t; ρ),
where

200
8
General Approaches to Analyzing Stochastic Dynamic Systems
˙Θt [t; ψ(r′, τ)] = d
dt ln
4
exp
⎧
⎨
⎩i
t

0
dτ

dr′u(r′, τ)ψ(r′, τ)
⎫
⎬
⎭
5
is the derivative of the characteristic functional logarithm of random ﬁeld
u(r, t).
Thus, expression (8.15) can be represented as the functional linear varia-
tional diﬀerential equation in the functional space of functions {u0(r, t)}
 ∂
∂t + u0(r, t) ∂
∂r

P(r, t; ρ)
= ∂u0(r, t)
∂r
∂
∂ρρP(r, t; ρ) + ˙Θt

t;
δ
iδu0(r′, τ)

P(r, t; ρ),
(8.16)
However, if we deal with a ﬁxed mean ﬂow u0(r , t) (e.g., u0(r, t) = 0),
then Eq. (8.16) assumes the form of an unclosed equality
 ∂
∂t + u0(r, t) ∂
∂r

P(r, t; ρ)
= ∂u0(r, t)
∂r
∂
∂ρρP(r, t; ρ) +
.
˙Θt

t;
δ
iδu(r′, τ)

ϕ(r, t; ρ)
/
.
(8.17)
8.2.2
Parabolic Equation of Quasi-Optics
The second example deals with wave propagation in random medium within
the framework of the linear parabolic equation (1.153), page 63
∂
∂xu(x, R) = i
2k ΔRu(x, R) + ik
2ε(x, R)u(x, R),
u(0, R) = u0(R). (8.18)
In this case, functional (3.59)
ϕ[x; v(R′), v∗(R′)] = ϕ[x; v, v∗] = exp

i

dR′ -
u(x, R′)v(R′)+u∗(x, R′)v∗(R′)
.
is described by the variational diﬀerential equation (the Hopf equation)
(3.60), page 111
∂
∂xϕ[x; v, v∗] = ik
2

dR′ε(x, R′)!
M(R′)ϕ[x; v, v∗]
+ i
2k

dR′

v(R′)ΔR′
δ
δv(R′) −v∗(R′)ΔR′
δ
δv∗(R′)
	
ϕ[x; v, v∗]
(8.19)

8.2
Partial Diﬀerential Equations
201
equivalent to the initial equation (8.18). Here, !
M(R′) is the Hermitian
operator
!
M(R′) = v(R′)
δ
δv(R′) −v∗(R′)
δ
δv∗(R′).
A consequence of Eq. (8.19) is the equality
δ
δε(x−0, R′)ϕ[x; v, v∗] = ik
2
!
M(R′)ϕ[x; v, v∗].
(8.20)
Averaging Eq. (8.19) over an ensemble of realizations of random ﬁeld
ε(x, R) and replicating derivation of Eq. (8.7) and (8.17), we obtain that
the characteristic functional of the solution to problem (8.18)
Φ[x; v(R′), v∗(R′)] = Φ[x; v, v∗] =
,
ϕ[x; v(R′), v∗(R′)]
-
satisﬁes the variational derivative equation
∂
∂xΦ[x; v, v∗] =
.
˙Θx

x;
δ
iδε(ξ, R′)

ϕ[x; v,v∗]
/
+ i
2k

dR′

v(R′)ΔR′
δ
δv(R′) −v∗(R′)ΔR′
δ
δv∗(R′)
	
Φ[x; v, v∗],
(8.21)
where
˙Θx

x; ψ(ξ, R′)

= d
dx ln
4
exp
⎧
⎨
⎩i
x

0
dξ

dR′ε(ξ, R′)ψ(ξ, R′)
⎫
⎬
⎭
5
is the derivative of the characteristic functional logarithm of random ﬁeld
ε(x, R).
8.2.3
Random Forces in the Theory of Hydrodynamic
Turbulence
In Chapter 1, page 69 we obtained that stationary and homogeneous hydro-
dynamic turbulence can be described in terms of the Fourier transform of the
velocity ﬁeld ( %u∗
i (k, t) = %ui(−k, t))
%ui(k, t) =

drui(r, t)e−ikr,
ui(r, t) =
1
(2π)3

dk%ui(k, t)eikr,
which satisﬁes the nonlinear integro-diﬀerential equation (1.171)

202
8
General Approaches to Analyzing Stochastic Dynamic Systems
∂
∂t ui (k, t)+ i
2

dk1

dk2Λαβ
i
(k1, k2, k) uα (k1, t) uβ (k2, t) −νk2ui (k, t)= fi (k, t) ,
(8.22)
where
Λαβ
i
(k1, k2, k) =
1
(2π)3 {kαΔiβ (k) + kβΔiα (k)} δ(k1 + k2 −k),
Δij (k) = δij −kikj
k2
(i, α, β = 1, 2, 3),
and %f(k, t) is the spatial Fourier harmonics of the external force.
A speciﬁc feature of the three-dimensional hydrodynamic motions consists
in the existence of the integral of energy under the condition that external
forces and eﬀects related to the molecular viscosity are absent.
Furthermore, in Chapter 2, page 114 we obtained that functional
ϕ[t; z(k′)] = ϕ[t; z] = exp

i

dk′%u(k′, t)z(k′)
	
satisﬁes the linear variational diﬀerential Hopf equation in functional space
(3.69)
∂
∂tϕ[t; z] = −

dkzi(k)

νk2
δ
δzi (k) −i %fi (k, t)
	
ϕ[t; z]
−1
2

dkzi(k)

dk1

dk2Λαβ
i
(k1, k2, k)
δ2ϕ[t; z]
δzα (k1) δzβ (k2).
(8.23)
A consequence of Eq. (8.23) is the equality
δ
δ%f(k, t −0)
ϕ[t; z] = iz(k)ϕ[t; z].
(8.24)
Average Eq. (8.23) over an ensemble of realizations of random force %f(k, t).
Then, replicating derivation of Eq. (8.7), we obtain that characteristic func-
tional of the velocity ﬁeld
Φ[t; z(k′)] = Φ[t; z] =
,
ϕ[t; z(k′)]
-
,
satisﬁes the unclosed variational diﬀerential equation
∂
∂t Φ[t; z]=

˙Θt
 
t;
δ
iδf(κ, τ)
!
ϕ[t; z]

−

dkzi(k)
 1
2

dk1

dk2Λi,αβ(k1, k2, k)
δ2
δzα(k1)δzβ(k2) +νk2
δ
δzi(k)

Φ[t; z],
(8.25)
where

8.3
Stochastic Integral Equations (Methods of Quantum Field Theory)
203
˙Θt [t; ψ(κ, τ)] = d
dt ln
4
exp
⎧
⎨
⎩i
t

0
dτ

dκf(κ, τ)ψ(κ, τ)
⎫
⎬
⎭
5
is the derivative of the characteristic functional logarithm of the random
ﬁeld of external force %f(k, t). The equation for the characteristic functional
Φ[t; z(k′)] describes all one-time statistical characteristics of the velocity ﬁeld.
8.3
Stochastic Integral Equations (Methods of
Quantum Field Theory)
Problems discussed in the above sections allow deriving the closed (or un-
closed) statistical description in functional space due to the fact that every
of these problems can be formulated in terms of some system of diﬀeren-
tial equations of the ﬁrst-order with respect to time and initial values at
t = 0. Such systems satisfy the causality condition formulated in Sect. 2.1.2,
page 87 which reads as follows: problem solution at instant t depends only
on ﬂuctuations of system parameters for times preceding this instant and is
independent of ﬂuctuations for posterior times.
Problems formulated in terms of integral equations that cannot be gene-
rally reduced to the system of diﬀerential equations also can satisfy the causal-
ity condition.
However, prior to consider this class of stochastic problems, we dwell on
general methods of statistical description of dynamic systems, which are bor-
rowed from the quantum ﬁeld theory. The essence of these methods consists in
constructing a perturbation series for statistical characteristics of quantity of
interest and analyzing the result with the use of the methods developed in the
quantum ﬁeld theory. It appears convenient to represent each term of these
perturbation series diagrammatically (in the form of the so-called Feynman
diagrams) and associate every diagram element with certain function or op-
erator; as a result, each diagram corresponds to certain analytical expression.
We will not consider the diagram technique as such (for its exhaustive descrip-
tion in the context of statistical problems, see, e.g., monographs [112,130]);
instead, we derive the basic results directly, using the functional methods
described above [48,49].
8.3.1
Linear Integral Equations
The input stochastic equation is the linear integral (or integro-differential)
equation for Green’s function

204
8
General Approaches to Analyzing Stochastic Dynamic Systems
S(r, r′) = S0(r, r′)+

dr1

dr2

dr3S0(r, r1)Λ(r1, r2, r3)f(r2)S(r3, r′),
(8.26)
where r denotes all arguments of functions S and f, including the index
arguments that assume summation instead of integration. It is assumed that
function f(r) is the random ﬁeld and function S0(r, r′) is Green’s function
of the problem without parameter ﬂuctuations, i.e., at f(r) = 0. In some
problems, quantity Λ(r1, r2, r3) can be an operator; the notation of Eq. (8.26)
assumes that this operator acts on all factors appeared to the right from
it. For example, the nonlinear system of ordinary diﬀerential equations can
be reduced to the equation like Eq. (8.26) by constructing the equivalent
linear stochastic partial diﬀerential equation (the Liouville equation) whose
characteristics correspond to the solution of the nonlinear system. In this
case, function S is Green’s function of the stochastic Liouville equation and
quantity Λ is the diﬀerential operator. For problems formulated in terms of
systems of linear equations, quantity Λ(r1,r2,r3) is a function.
Below, we will assume for simplicity that quantity Λ(r1,r2,r3) is not an
operator, but a function. The consideration of operator quantity Λ(r1,r2,r3)
involves only insigniﬁcant diﬀerences. Indeed, if quantity Λ(r1,r2,r3) is an
operator, we can reduce the problem to the problem under consideration
by introducing delta-functions with arguments coinciding with variables on
which this operator is acting and adding the corresponding integrations.
Equation (8.26) can be represented in the symbolic form
S = S0 + S0ΛfS,
(8.27)
where integration is assumed with respect to all arguments of function
Λ({ri}).
We can solve Eq. (8.27) by the iteration method with function S0(r, r′) as
the zero-order approximation. As a result, we obtain the solution in the form
of a series that we represent in the symbolic form again
S = {1 + S0Λf + S0ΛfS0Λf + · · · } S0 =
∞
&
n=0
(S0Λf)nS0.
(8.28)
The same iteration series represents the solution of the integral equation
S = S0 + SΛfS0.
Consequently Eq. (8.27) is equivalent to the equation
S(r, r′) = S0(r, r′)+

dr1

dr2

dr3S(r, r1)Λ(r1, r2, r3)f(r2)S0(r3, r′).
(8.29)
The solution to Eqs. (8.26) and (8.29) is the functional of ﬁeld f(r), i.e.,

8.3
Stochastic Integral Equations (Methods of Quantum Field Theory)
205
S(r, r′) = S[r, r′; f(r)].
It is not diﬃcult to show that Eqs. (8.26) and (8.29) are equivalent to the
variational diﬀerential equation in functional space {f(r)}
δ
δf(r0)S[r, r′; f(r)] =

dr1

dr2S[r, r1; f(r)]Λ(r1, r0, r2)S[r2, r′; f(r)]
(8.30)
with the initial value
S[r, r′; f(r)]f=0 = S0(r, r′).
Indeed, varying Eq. (8.27) for S(r, r′) with respect to function f(r0), we
obtain the equation
δ
δf S = S0ΛδS + S0Λf δ
δf S,
where δ denotes the delta-function of the corresponding arguments. The solu-
tion to this equation can be represented as the iteration series
δ
δf S =
0
1 + S0Λf + (S0Λf)2 + . . .
1
S0ΛδS.
Taking into account the iteration series (8.28) for S, we obtain the desired
formula (8.30).
Averaging now the obtained iteration series (8.28) over an ensemble of
realizations of ﬁeld f(r), we obtain function ⟨S(r, r′)⟩in the form of the it-
eration series dependent on all moment functions of ﬁeld f( r). Rearranging
the terms of this series, we can then express the right-hand side of the expan-
sion in terms of function ⟨S(r, r′)⟩itself. This rearrangement produces new
unknown functions speciﬁed by the corresponding iteration series and called,
by analogy with the quantum ﬁeld theory, the mass and vertex functions.
Consider instead of Eq. (8.26) the auxiliary equation
S[r, r′; f + η] = S0(r, r′)
+

dr1

dr2

dr3S0(r, r1)Λ(r1, r2, r3)[f(r2) + η(r2)]S[r3, r′; f + η],
(8.31)
where η(r) is arbitrary deterministic function. We can ﬁnd the desired func-
tion S(r,r′) by setting η(r) = 0 in Eq. (8.31), i.e.,
S(r, r′) = S[r, r′; f(r)] = S[r, r′; f(r) + η(r)]η=0.
Let us average Eq. (8.31). Splitting the correlator ⟨fS⟩by formula (7.2),
page 153, we obtain the equation

206
8
General Approaches to Analyzing Stochastic Dynamic Systems
G[r, r′; η] = S0(r, r′)+

dr1

dr2

dr3S0(r, r1)Λ(r1, r2, r3)η(r2)G[r3, r′; η]
+

dr1

dr2

dr3S0(r, r1)Λ(r1, r2, r3)
)
Ωr2

δ
iδf(r)

S[r3, r′;f+η]
*
.
(8.32)
Here, functional Ωr [v(r)] is given by the formula
Ωr =
δ
iδv(r)Θ [v(r)] ,
functional
Θ [v(r)] = ln
.
exp

i

drf(r)v(r)
	/
is the logarithm of the characteristic functional of random ﬁeld f(r), and
G[r, r′; η(r)] = ⟨S[r, r′; f(r) + η(r)]⟩.
Taking into account the fact that functional S[r,r′; f(r) + η(r)] is the
functional of argument f(r) + η(r), we can replace variational diﬀerentia-
tion with respect to f(r) by diﬀerentiation with respect to η(r) and rewrite
Eq. (8.32) in the form of the closed variational diﬀerential equation similar
to the Schwinger equation of the quantum ﬁeld theory
G[r, r′; η] = S0(r, r′)+

dr1

dr2

dr3S0(r, r1)Λ(r1, r2, r3)η(r2)G[r3, r′; η]
+

dr1

dr2

dr3S0(r, r1)Λ(r1, r2, r3)Ωr2

δ
iδη(r)

G[r3, r′; η].
(8.33)
We can solve Eq. (8.33) for functional G[r,r′; η(r)] by the iteration method
with function S0(r,r′) as the zero-order approximation. Setting η(r) = 0 in
the resulting expansion, we obtain the iteration series for function ⟨S(r, r′)⟩.
To simplify further presentation, we rewrite Eq. (8.33) in the symbolic form
(the corresponding complete expressions can be easily restored at every step)
G = S0 + S0Λ

η + Ω
 δ
iδη

G.
(8.34)
We introduce the inverse functional G−1, such that
G−1G = 1,
GG−1 = 1.
(8.35)
Here, the unity is understood as the corresponding delta-function. In addi-
tion, we introduce the functional
Γ = −δG−1
δη
,
(8.36)

8.3
Stochastic Integral Equations (Methods of Quantum Field Theory)
207
which we call the vertex functional.
Varying Eq. (8.35) with respect to ﬁeld η, we obtain the equality
δG
δη = GΓG,
(8.37)
whose substitution in Eq. (8.34) results in the equation
G = S0 + S0ΛηG + S0QG.
(8.38)
We call the quantity
Q = Λ

Ω
 δ
iδη

G

G−1
(8.39)
the mass functional.
Multiplying now Eq. (8.38) by G−1 from the right and by S−1
0
from the left
(and integrating over the corresponding arguments), we obtain the equation
for functional G−1
S−1
0
−G−1 = Λη + Q.
(8.40)
Varying Eq. (8.40) with respect to ﬁeld η, we obtain the equation for func-
tional Γ
Γ = Λ + δ
δη Q.
(8.41)
The system of functional equations (8.38) and (8.41) is closed in func-
tionals G and Γ. An additional point is Eq. (8.37) that relates the solutions
to these equations. We can solve Eq. (8.41) for Γ by the iteration method
with quantity Λ as the zero-order approximation. If we use formula (8.37) to
express variational derivatives of functional G with respect to η, we obtain
the integral equations for Γ and G with inﬁnite number of terms every of
which includes no functionals other than G and Γ. Setting now η(r) = 0, we
can obtain the closed system of integral equations. In particular, Eq. (8.38)
assumes the following form
⟨S⟩= S0 + S0Q ⟨S⟩,
⟨S⟩= S0 + ⟨S⟩QS0,
(8.42)
and is called the Dyson equation.
Now, we consider in more detail the case of the Gaussian random ﬁeld
f(r) with correlation function B(r,r ′) = ⟨f(r)f(r′)⟩. In this case
Ωr [v(r)] = i

dr′B(r, r′)v(r′),
the mass functional assumes the form
Q = ΛBGΓ,
(8.43)
and Eqs. (8.38) and (8.41) assume the form

208
8
General Approaches to Analyzing Stochastic Dynamic Systems
G = S0 + S0ΛηG + S0ΛBGΓG,
Γ = Λ + ΛBGΓGΓ + ΛBGδΓ
δη .
(8.44)
Setting now η = 0, we obtain the closed system of equations
⟨S⟩= S0 + S0Q ⟨S⟩
(Dyson equation),
Q = ΛB ⟨S⟩Γ,
Γ = Λ + ΛB ⟨S⟩Γ ⟨S⟩Γ + · · ·
( Γ = Γ|η=0).
(8.45)
System of equations (8.45) is very complicated and low understood. The
simplest way of its simpliﬁcation consists in cutting the inﬁnite series in
the equation for Γ. If we content ourselves with the ﬁrst term, we obtain the
closed nonlinear equation (the Kraichnan approximation )
⟨S⟩= S0 + S0QKr ⟨S⟩,
QKr = ΛB ⟨S⟩Λ.
(8.46)
Further, if we replace ⟨S⟩in the expression for the mass function QKr with
S0, we obtain the linear equation (the Bourret approximation)
⟨S⟩= S0 + S0ΛBS0Λ ⟨S⟩.
(8.47)
Functional Γ and, consequently, function Γ are closely related to quantity
⟨SΛS⟩. Indeed, in view of formula (8.30), we can rewrite expression (8.37) at
η = 0 in the form
⟨SΛS⟩= ⟨S⟩Γ ⟨S⟩.
(8.48)
Thus, diﬀerent approximations for function Γ are equivalent to certain
hypotheses about splitting the correlation ⟨SΛS⟩. The Kraichnan approxi-
mation (8.46) corresponds to the equality
⟨SΛS⟩= ⟨S⟩Λ ⟨S⟩,
while the Bourret approximation (8.47) is equivalent to the requirement that
⟨SΛS⟩= S0Λ ⟨S⟩.
Splitting the correlation ⟨SΛS⟩by formula (7.9), page 168, we obtain the
general operator expression
⟨SΛS⟩= G

iB δ
δη

ΛG[η]|η=0,
which is in essence equivalent to the introduction of the vertex function.

8.3
Stochastic Integral Equations (Methods of Quantum Field Theory)
209
Note that the knowledge of functional G[r, r′; η(r)] is equivalent, in the
case of the Gaussian ﬁeld f(r), to the knowledge of the functional
Φ[r, r′; v(r)] =
2
S(r, r′)ei

drf(r)v(r)3
describing all statistical correlations between function S(r,r′) and ﬁeld f(r).
Indeed, according to formula (7.13), page 169, we can rewrite functional
Φ[r,r′; v(r)] in the form
Φ[r, r′; v(r)] =
2
ei

drf(r)v(r)3 .
S

r, r′;f(r)+i

dr1B(r, r1)
/
,
wherefrom we obtain the equality
Φ[r, r′; v(r)] =
2
ei

drf(r)v(r)3
G

r, r′;i

dr1B(r, r1)

.
To complete the picture, we dwell now on the so-called renormalization
method. The point is that, even if the mass function is known, the Dyson
equation (8.45) is a very complicated integral equation, which only rarely
can be solved analytically. At the same time, the Dyson equation with the
simpliﬁed mass function can be easily solved in a number of cases. The renor-
malization method lies in rearranging the Dyson equation into the integral
equation in which function S0(r,r′) is replaced with the solution to the sim-
pliﬁed problem.
Denote S the solution of the Dyson equation (8.45) with the simpliﬁed
mass function Q. Then, function S will satisfy the equation
S = S0 + S0 QS.
(8.49)
In view of the fact that Eq. (8.49) is linear in S, it is obvious that it can be
rewritten in the form
S = S0 + S QS0 = (1 + S Q)S0,
(8.50)
where 1 is the unit operator.
To exclude function S0(r,r′) from the Dyson equation (8.45), we apply
operator (1 + S Q) to it. Then, we obtain, in view of Eq. (8.50), the equation
⟨S⟩= S + S{Q −Q} ⟨S⟩.
(8.51)
Now, we can solve Eq. (8.51) by the iteration method with function S as the
zero-order approximation.
At Q = 0, function S = S0, and we turn back to the Dyson equation
(8.45). It is obvious that the above derivation of Eq. (8.51) holds not only for

210
8
General Approaches to Analyzing Stochastic Dynamic Systems
the Gaussian ﬁeld f(r), but for any arbitrary ﬁeld f(r), because the form of
the Dyson equation is independent of the type of ﬁeld f(r).
Now, we dwell on the general-form Dyson equation (8.38). Note that we
can represent functional Ω[v] in terms of the Taylor series in powers of v
Ω[v] =
∞
&
n=1
1
n!Kn+1vn,
where Kn are the cumulant functions of random ﬁeld f(r). As a consequence,
we can represent the mass functional (8.39) in the form
Q = Λ
∞
&
n=0
1
n!Kn+1
 δn
δηn G
	
G−1,
where variational derivatives of functional G with respect to η are calculated
by formula (8.37). In this case, the Dyson equation has very complicated
structure. The standard ways of simplifying this equation are quite similar
to those used in the case of the Gaussian parameter ﬂuctuations.
If we set Γ = Λ, expression (8.37) assumes the form
δG
δη = GΛG
and, consequently,
δnG
δηn = n!(GΛ)nG =
∞

0
dλe−λ(GλΛ)nG.
In this case, we arrive at the generalized Kraichnan equation
⟨S⟩= S0 + S0QKr ⟨S⟩,
QKr = Λ
∞
&
n=0
Kn+1{⟨S⟩Λ}n = Λ
∞

0
dλe−λΩ[⟨S⟩λΛ].
(8.52)
If we replace ⟨S⟩in operator QKr in Eq. (8.52) with S0, we obtain the
generalized Bourret equation
QB = Λ
∞

0
dλe−λΩ[S0λΛ],
which coincides with the so-called one-group approximation
for the Dyson
equation.

8.3
Stochastic Integral Equations (Methods of Quantum Field Theory)
211
Above, we considered the derivation of the equation for averaged Green’s
function (the Dyson equation). In a similar way, we can derive the equation
for the correlation function
Γ(r, r′; r1, r′
1) = ⟨S(r, r′)S∗(r1, r′
1)⟩.
With this goal in view, we multiply Eq. (8.26) by S∗(r1,r′
1) and average
the result over an ensemble of realizations of random ﬁeld f(r). In this way,
we obtain the equation
Γ = S0 ⟨S∗⟩+ S0Λ ⟨fSS∗⟩.
(8.53)
Taking into account the Dyson equation (8.42)
⟨S⟩= 1+ ⟨S⟩QS0,
we apply operator {1 + ⟨S⟩Q} to Eq. (8.53). As a result, we obtain the equa-
tion
Γ = ⟨S⟩⟨S∗⟩+ ⟨S⟩{Λ ⟨fSS∗⟩−QΓ} .
(8.54)
In standard notation, Eq. (8.54) assumes the form
Γ (r, r′; r1, r′
1) = S(r, r′) S∗(r1, r′
1)
+

dr2

dr′
2

dr3

dr′
3 ⟨S(r, r2)⟩S∗(r1, r′
2) K(r2, r′
2; r3, r′
3)Γ (r3, r′; r′
3, r′
1),
(8.55)
and is called the Bete–Salpeter equation . Function K(r2,r′
2;r3,r′
3) is called
the kernel of the intensity operator.
The simplest approximation to this equation — the so-called ladder appro-
ximation — corresponds to function K(r2,r′
2;r3,r′
3) of the form
K(r2, r′
2; r3, r′
3) = δ(r2 −r3)δ(r′
2 −r′
3)Bf(r2, r′
2),
(8.56)
where Bf(r2,r′
2) = ⟨f(r2)f(r′
2)⟩is the correlation function of ﬁeld f(r).
8.3.2
Nonlinear Integral Equations
Consider now integral equation (8.26) extended to the case of an equation
with quadratic nonlinearity. There are two possible cases. In the ﬁrst — sim-
plest — case, the solution can be expressed through the integral of the solution
to the linear equation with respect to an auxiliary parameter; the second —
more complex — case describes the space-time structure of hydrodynamic tur-
bulence and is described by the integro-diﬀerential equation (1.172), page 70.

212
8
General Approaches to Analyzing Stochastic Dynamic Systems
The Simplest Nonlinear Integral Equation
Consider the nonlinear equation
S(r, r′) = S0(r, r′) +

dr1

dr2

dr3S(r, r1)Λ(r1, r2, r3)f(r2)S(r3, r′).
(8.57)
Along with this equation we draw once again Eq. (8.26) and mark its solution
by index Λ
SΛ(r, r′)=S0(r, r′) +

dr1

dr2

dr3S0(r, r1)Λ(r1, r2, r3)f(r2)SΛ(r3, r′).
(8.58)
As was mentioned earlier, the solution to Eq. (8.58) can be represented as the
iteration series
SΛ =
∞
&
n=0
(S0Λf)nS0.
It is obvious that the solution to integral equation (8.57) has a similar itera-
tion structure
S =
∞
&
n=0
An(S0Λf)nS0
(8.59)
with an additional numeric parameter An. This parameter can be easily found
from the quadratic equation
y = 1 + λy2
whose solution is
y = 1 −
√
1 −4λ
2λ
=
∞
&
n=0
Anλn.
Consequently,
An = 2n (2n −1)!!
(n + 1)! = 22n+1
π
Γ(n + 1/2)Γ(3/2)
Γ(n + 2)
= 22n+1
π
B

n + 1
2, 3
2

,
where B(γ, δ) is the beta-function whose integral representation is
B(γ, δ) =
1

0
dppγ−1(1 −p)δ−1.
As a result, we have
An = 2
π
1

0
dp (4p)n

1 −p
p
.

8.3
Stochastic Integral Equations (Methods of Quantum Field Theory)
213
Substituting this expression in Eq. (8.59), we obtain
S = 2
π
1

0
dp

1 −p
p
∞
&
n=0
(S04pΛf)nS0= 2
π
1

0
dp

1 −p
p
S4pΛ.
(8.60)
Thus, the solution to the nonlinear equation (8.57) is expressed through the
solution to the linear equation (8.58) as the integral with respect to an aux-
iliary parameter [113].
Space-Time Description of Hydrodynamic Turbulence
Consider now the nonlinear integral equation (1.172), page 70 for the space-
time harmonics of the turbulent velocity ﬁeld
(iω+νk2)ui (K)+ i
2

d4K1

d4K2Λαβ
i
(K1, K2, K) uα (K1) uβ (K2) = fi (K) ,
(8.61)
where
Λαβ
i
(K1, K2, K) =
1
(2π)3 {kαΔiβ (k) + kβΔiα (k)} δ(k1 + k2 −k)δ(ω1 + ω2 −ω),
Δij (k) = δij −kikj
k2
(i, α, β = 1, 2, 3).
Here, K is the four-dimensional wave vector with components {k, ω} and
%fi (K) are the space-time Fourier harmonics of external forces; in view of the
fact that ui(x, t) is the real-valued ﬁeld, we have
%u∗
i (K) = %ui(−K).
Equation (8.61) can be juxtaposed with the equivalent linear variational
diﬀerential Hopf equation
(iω + νk2)
δ
δzi (K)ϕ[z] = i %fi (K) ϕ[z]
−1
2

d4K1

d4K2Λαβ
i
(K1, K2, K)
δ2ϕ[z]
δzα (K1) δzβ (K2)
(8.62)
for the functional
ϕ[z] = exp

i

d4K′%u

K′
z

K′	
.
(8.63)
Averaging Eq. (8.62) over an ensemble of realizations of the external force
ﬁeld %f(K), we obtain the equation

214
8
General Approaches to Analyzing Stochastic Dynamic Systems
(iω + νk2)
δ
δzi (K)Φ[z] = i
2
%fi (K) ϕ[z]
3
−1
2

d4K1

d4K2Λαβ
i
(K1, K2, K)
δ2Φ[z]
δzα (K1) δzβ (K2).
(8.64)
for the characteristic functional
Φ[z] = ⟨ϕ[z]⟩.
We will now assume that the random ﬁeld of external force %f(K) is the
Gaussian ﬁeld homogeneous in space and stationary in time whose diﬀerent
statistical characteristics are determined by the space-time spectral function
2
%fi(K1) %fj(K2)
3
= 1
2δ4 (K1 + K2) Fij(K1).
Because %f(K) is the nondivergent (solenoidal) ﬁeld, we have
Fij(K) = Δij(k)F(K),
where F(K) is the space-time spectrum of external forces.
Splitting the correlator in the right-hand side of Eq. (8.64) by the Furutsu–
Novikov formula, we can rewrite Eq. (8.64) in the form
(iω + νk2)
δ
δzi (K)Φ[z] = −1
2Fij(K)

d4K1zα (K1) Gαj [K1, −K; z]
−1
2

d4K1

d4K2Λαβ
i
(K1, K2, K)
δ2Φ[z]
δzα (K1) δzβ (K2),
(8.65)
where we introduced the new functional
Gij

K, K′; z

=
4
δui(K)
δ %fj(K′)
ϕ[z]
5
.
We can obtain the equation for quantity δui(K)/δ %fj(K′) by varying
Eq. (8.61),
(iω+νk2) δ%ui(K)
δ %fj(K′)
+i

d4K1

d4K2Λαβ
i
(K1, K2, K) %uα (K1) δ%uβ(K2)
δ %fj(K′)
= δijδ4 
K −K′
,
(8.66)
As a consequence, functional Gij

K, K′; z

satisﬁes the equation

8.3
Stochastic Integral Equations (Methods of Quantum Field Theory)
215
(iω + νk2)Gij

K, K′; z

= δijδ4 
K −K′
Φ[z]
−

d4K1

d4K2Λαβ
i
(K1, K2, K)
δ
δzα(K1)Gβj

K2, K′; z

.
(8.67)
The system of functional equations (8.65) and (8.67) for Φ[z] and
Gij

K, K′; z

is closed and completely governs the statistical description
of the velocity ﬁeld [42] (see also [46,49,112]).
It is easy to show that average energy income of the velocity ﬁeld harmonics
due to work of the external force is given by the expression
2
%ui (K) %fj

K′3
= 1
2Fjl

K′
4
δ%ui (K)
δ %fl

−K′
5
= 1
2Fjα

K′
Giα

K, −K′; 0

,
(8.68)
which deﬁnes the physical meaning of quantity Gij

K, K′; z

as the func-
tional that describes correlations between the velocity ﬁeld and the rate of
energy income (force power). Here, quantity δ%ui (K) /δ %fl

K′
can be consid-
ered as some sort of Green’s function for Eq. (8.61). Indeed, if we add some
deterministic force η (K) to the right-hand side of Eq. (8.61), then the solu-
tion of the resulting equation will be a functional of argument %f (K)+η (K),
i.e.,
%ui (K) = %ui

K; %f

K′
+ η

K′
.
(8.69)
Expand solution (8.69) in the functional series in η (K)
%ui (K) = %ui

K; %f

K′
+

d4K′ δ%ui (K)
δηj

K′

η=0
ηj

K′
+ · · ·
= %u0
i (K) +

d4K′ δ%ui (K)
δ %fj

K′ηj

K′
+ · · · .
(8.70)
The ﬁrst term of the expansion is simply the solution of problem (8.61). The
second term describes the dynamic system response on the inﬁnitely small
deterministic force, and quantity δ%ui (K) /δ %fj

K′
appears the analog of
Green’s function for linear systems. Averaging Eq. (8.70) over an ensemble of
realizations of random forces and taking into account the equality
,
%u0
i (K)
-
=
0, we obtain the expression for the system average response
⟨%ui (K)⟩=

d4K′
4
δ%ui (K)
δ %fj

K′
5
ηj

K′
+ · · · .
Turn back to the system of equations (8.65) and (8.67) and represent
functionals Φ[z] and Gij

K, K′; z

in the form

216
8
General Approaches to Analyzing Stochastic Dynamic Systems
Φ[z] = eφ[z],
Gij

K, K′; z

= Sij

K, K′; z

eφ[z].
Then, equations for functionals φ[z] and Sij

K, K′; z

assume the form
δ
δzi (K)φ[z]
= −1
2

d4K′

d4K1S0
iγ

K, K′
Fγj(K′)zα (K1) Gαj [K1, −K; z]
−1
2

d4K′

d4K1

d4K2S0
iγ

K, K′
Λαβ
γ (K1, K2, K)
×

δ2φ[z]
δzα (K1) δzβ (K2) +
δ2φ[z]
δzα (K1)
δφ[z]
δzβ (K2)
	
,
(8.71)
Sij

K, K′; z

= S0
ij

K, K′
−

d4K′′

d4K1

d4K2S0
iγ

K, K′′
Λαβ
γ

K1, K2, K′′
×

δφ[z]
δzα(K1)Gβj

K2, K′; z

+
δ
δzα(K1)Gβj

K2, K′; z
	
,
(8.72)
where
S0
ij

K, K′
=

iω + νk2−1 δijδ4 
K −K′
.
The last equation is analogous to the Schwinger equation in the quantum
ﬁeld theory.
Note that expansion of functional φ[z] in the functional Taylor series in
z (K) determines the velocity ﬁeld cumulants, and expansion of functional
Sij

K, K′; z

determines the correlators between Green’s function analog
Gij

K, K′
= δui (K)
δfj

K′ and the velocity ﬁeld.
If only the behavior of the velocity correlation function is of interest, the
system of functional equations (8.71), (8.72) appears redundant, and we can
ﬁlter out useless information by representing the spectral function of velo-
city in terms of a speciﬁc perturbation series. To construct such a series, we
introduce, as in the linear case, quantity S−1
ij

K, K′; z

by the formula

d4K′Sij

K, K′; z

S−1
jδ

K′, K′′; z

= δiδδ4 
K −K′′
.
(8.73)
One can easily see that the relationship

d4K′S−1
ij

K, K′; z

Sjδ

K′, K′′; z

= δiδδ4 
K −K′′
(8.74)

8.3
Stochastic Integral Equations (Methods of Quantum Field Theory)
217
will also hold.
Introduce additionally the three-index functional
Γ jδ
γ

P , K′, K′′; z

=
δ
δzγ (P )S−1
jδ

K′, K′′; z

,
(8.75)
which is similar to the mass operator vertex portion in the quantum ﬁeld
theory. Varying Eq. (8.73) with respect to zγ (P ), we can express δS/δz in
terms of S and Γ
δ
δzγ (P )Siμ [K, Q; z]
= −

d4K′

d4K′′Sij

K, K′; z

Γ jδ
γ

P , K′, K′′; z

Sδμ

K′′, Q; z

.
(8.76)
Using Eq. (8.76), we can rewrite Eq. (8.72) in the form
Sij

K, K′; z

= S0
ij

K, K′
−

d4P

d4K1

d4K2S0
iγ (K, P ) Λαβ
γ
(K1, K2, P )
×

δφ[z]
δzα(K1) Gβj

K2, K′; z

−

d4K′

d4K′′Sβσ

K2, K′; z

× Γ σν
α

K1, K′, K′′; z

Sνj

K′′, K′; z
 
.
(8.77)
Setting z = 0 in Eq. (8.77), we obtain the equation that interconnects quan-
tities S|z=0 and Γ|z=0 and is similar to the Dyson equation of the quantum
ﬁeld theory (δφ/δz = 0 at z = 0).
Multiplying Eq. (8.77) by S−1 from the right and by S−1
0
from the left,
integrating over the corresponding arguments, and varying the result with
respect to z, we obtain the following functional equation for Γ
Γ μρ
γ [P 3, P 2, P 1; z] =

d4K1Λα,ρ
μ (K1, P 1, P 2)
δ2φ[z]
δzα (K1) δzγ (P 3)
−

d4K1

d4K2

d4K′Λα,β
μ
(K1, P 1, P 2)
×
δ
δzγ(P 3)
0
Sβσ

K2, K′; z

Γ σρ
α

K, K′, P 2; z
1
.
(8.78)
The system of equations (8.71), (8.77), and (8.78) is closed; however, the
solutions to this system are interconnected additionally by the relationship
(8.76).
If we will now construct the perturbation series with absolute terms of
Eqs. (8.71) and (8.78) as the zero-order approximations and will express

218
8
General Approaches to Analyzing Stochastic Dynamic Systems
appearing variations of S with respect to z using relationship (8.76), then we
obtain the space-time velocity spectrum and function Γ|z=0 in the form of the
inﬁnite series, every term of which includes these very functions. These series
will be integral equations with inﬁnite number of terms and, being combined
with Eq. (8.72) at z = 0, they form the closed system of equations for quan-
tities δ2φ/δzδz, S|z=0, and Γ|z=0. However, explicit representation of even a
few terms of these series is hardly possible because of cumbersome rearrange-
ments and complicated structure of functional equations (8.71), (8.77), and
(8.78). The reader can ﬁnd an analysis of possible simpliﬁcations in Ref. [42]
(see also [46,49,112]).
8.4
Completely Solvable Stochastic Dynamic Systems
Consider now several dynamic systems that allow suﬃciently adequate sta-
tistical analysis for arbitrary random parameters.
8.4.1
Ordinary Diﬀerential Equations
Multiplicative Action
As the ﬁrst example, we consider the vector stochastic equation with initial
value
d
dtx(t) = z(t)g(t)F (x),
x(0) = x0,
(8.79)
where g(t) and Fi(x), i = 1, · · · , N are the deterministic functions and z(t)
is the random process whose statistical characteristics are described by the
characteristic functional
Φ[t; v(τ)] =
4
exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
= eΘ[t;v(τ)].
Equation (8.79) has a feature that oﬀers a possibility of determining sta-
tistical characteristics of its solutions in the general case of arbitrarily dis-
tributed process z(t). The point is that introduction of new ’random’ time
T =
t

0
dτz(τ)g(τ)
reduces Eq. (8.79) to the equation looking formally deterministic

8.4
Completely Solvable Stochastic Dynamic Systems
219
d
dT x(T ) = F (x),
x(0) = x0,
so that the solution to Eq. (8.79) has the following structure
x(t) = x(T ) = x
⎛
⎝
t

0
dτz(τ)g(τ)
⎞
⎠.
(8.80)
Varying Eq. (8.80) with respect to z(τ) and using Eq. (8.79), we obtain the
equality
δ
δz(τ)x(t) = g(τ) d
dT x(T ) = g(τ)F (x(t)).
(8.81)
Thus, the variational derivative of solution x(t) is expressed in terms of the
same solution at the same time. This fact makes it possible to immediately
write the closed equations for statistical characteristics of problem (8.79).
Let us derive the equation for the one-time probability density
P(x, t) = ⟨δ(x(t) −x)⟩.
It has the form
∂
∂tP(x, t) =
.
˙Θt

t;
δ
iδz(τ)

δ (x(t)−x)
/
.
(8.82)
Consider now the result of operator δ/δz(τ) applied to the indicator func-
tion ϕ(x, t) = δ(x(t) −x). Using formula (8.81), we obtain the expression
δ
δz(τ)δ (x(t)−x) = −g(τ) ∂
∂x {F (x)ϕ(x, t)} .
Consequently, we can rewrite Eq. (8.82) in the form of the closed operator
equation
∂
∂tP(x, t) = ˙Θt

t; ig(τ) ∂
∂xF (x)

P(x, t),
P(x, 0) = δ(x −x0),
(8.83)
whose particular form depends on the behavior of process z(t).
For the two-time probability density
P(x, t; x1, t1) = ⟨δ(x(t) −x)δ(x(t1) −x1)⟩
we obtain similarly the equation (for t > t1 )
∂
∂tP(x, t; x1, t1) = ˙Θt

t; ig(τ)
 ∂
∂xF (x) + θ(t1−τ) ∂
∂x1 F (x1)

P(x, t; x1, t1)
(8.84)
with the initial value

220
8
General Approaches to Analyzing Stochastic Dynamic Systems
P(x, t1; x1, t1) = δ(x −x1)P(x1, t1),
where function P(x1, t1) satisﬁes Eq. (8.83).
One can see from Eq. (8.84) that multidimensional probability density can-
not be factorized in terms of the transition probability (see Sect. 3.3), so that
process x(t) is not the Markovian process. The particular forms of Eqs. (8.83)
and (8.84) are governed by the statistics of process z(t).
If z(t) is the Gaussian process whose mean value and correlation function
are
⟨z(t)⟩= 0,
B(t, t′) = ⟨z(t)z(t′)⟩,
then functional Θ[t; v(τ)] has the form
Θ[t; v(τ)] = −1
2
t

0
dt1
t

0
dt2B(t1, t2)v(t1)v(t2)
and Eq. (8.83) assumes the following form
∂
∂tP(x, t) = g(t)
t

0
dτB(t, τ)g(τ) ∂
∂xj
Fj(x) ∂
∂xk
Fk(x)P(x, t)
(8.85)
and can be considered as the extended Fokker–Planck equation .
The class of problems formulated in terms of the system of equations
d
dtx(t) = z(t)F (x) −λx(t),
x(0) = x0,
(8.86)
where F (x) are the homogeneous polynomials of power k, can be reduced to
problem (8.79). Indeed, introducing new functions
x(t) = x(t)e−λt,
we arrive at problem (8.79) with function g(t) = e−λ(k−1)t. In the important
special case with k = 2 and functions F (x) such that xF (x) = 0, the system
of equations (8.79) describes hydrodynamic systems with the linear friction
(see, e.g., [14,23]). In this case, the interaction between the components ap-
pears random.
If λ = 0, energy conservation holds in hydrodynamic systems for any
realization of process z(t). For t →∞, there is the steady-state probability
distribution P(x), which is, under the assumption that no additional integrals
of motion exist, the uniform distribution over sphere x2
i = E0. If additional
integrals of motion exist (as it is the case for ﬁnite-dimensional approxima-
tion of the two-dimensional motion of ﬂuid), the domain of the steady-state
probability distribution will coincide with the phase space region allowed by
the integrals of motion.

8.4
Completely Solvable Stochastic Dynamic Systems
221
Note that, in the special case of the Gaussian process z(t) appeared in the
one-dimensional linear equation of type Eq. (8.86)
d
dtx(t) = −λx(t) + z(t)x(t),
x(0) = 1,
which determines the simplest logarithmic-normal random process, we obtain,
instead of Eq. (8.85), the extended Fokker–Planck equation
 ∂
∂t + λ ∂
∂xx

P(x, t) =
t

0
dτB(t, τ) ∂
∂xx ∂
∂xxP(x, t),
P(x, 0) = δ(x −1).
(8.87)
Additive Action
Consider now the class of linear equations
d
dtx(t) = A(t)x(t) + f(t),
x(0) = x0,
(8.88)
where A(t) is the deterministic matrix and f(t) is the random vector function
whose characteristic functional Φ[t; v(τ)] is known.
For the probability density of the solution to Eq. (8.88), we have
∂
∂tP(x, t) = −∂
∂xi
(Aik(t)xkP(x, t))+
.
˙Θt

t;
δ
iδf(τ)

δ (x(t)−x)
/
. (8.89)
In the problem under consideration, the variational derivative δx(t)
δf(τ) also
satisﬁes (for τ < t) the linear equation with the initial value
d
dt
δ
δf(τ)xi(t) = Aik(t)
δ
δf(τ)xk(t),
δ
δfl(τ)xi(t)

t=τ
= δil.
(8.90)
Equation (8.90) has no randomness and governs Green’s function Gil(t, τ) of
homogeneous system (8.88), which means that
δ
δfl(τ)xi(t) = Gil(t, τ).
As a consequence, we have
δ
δfl(τ)δ(x(t) −x) = −∂
∂xk
Gkl(t, τ)δ(x(t) −x),
and Eq. (8.89) appears converted into the closed equation

222
8
General Approaches to Analyzing Stochastic Dynamic Systems
∂
∂tP(x, t) = −∂
∂xi
(Aik(t)xkP(x,t)) + ˙Θt

t; iGkl(t, τ) ∂
∂xk

P(x, t). (8.91)
From Eq. (8.91) follows that any moment of quantity x(t) will satisfy a
closed linear equation that will include only a ﬁnite number of cumulant
functions whose order will not exceed the order of the moment of interest.
For the two-time probability density
P(x, t; x1, t1) = ⟨δ(x(t) −x)δ(x(t1) −x1)⟩,
we quite similarly obtain the equation
∂
∂tP(x, t; x1, t1) = −∂
∂xi
(Aik(t)xkP(x,t; x1,t1))
+ ˙Θt

t; i {Gkl(t, τ) + Gkl(t1, τ)} ∂
∂xl

P(x, t; x1, t1)
(t > t1)
(8.92)
with the initial value
P(x, t1; x1, t1) = δ(x −x1)P(x1, t1),
where P(x1, t1) is the one-time probability density satisfying Eq. (8.91). From
Eq. (8.92) follows that x(t) is not the Markovian process. The particular form
of Eq. (8.91) and (8.92) depends on the structure of functional Φ[t; v(τ)], i.e.,
on the random behavior of function f(t).
For the Gaussian vector process f(t) whose mean value and correlation
function are
⟨f(t)⟩= 0,
Bij(t, t′) = ⟨fi(t)fj(t′)⟩
Eq. (8.91) assumes the form of the extended Fokker–Planck equation
∂
∂tP (x, t) = −∂
∂xi
(Aik(t)xkP (x,t)) +
t

0
dτBjl(t, τ)Gkj(t, t)Gml(t, τ)
∂2
∂xk∂xm
P (x, t).
(8.93)
Consider the dynamics of a particle under random forces in the presence
of friction [69] as an example of such a problem.
Inertial Particle under Random Forces
The simplest example of particle diﬀusion under the action of random exter-
nal force f(t) and linear friction is described by the linear system of equations
(1.94), page 47

8.4
Completely Solvable Stochastic Dynamic Systems
223
d
dtr(t) = v(t),
d
dtv(t) = −λ [v(t) −f(t)] ,
r(0) = 0,
v(0) = 0.
(8.94)
The stochastic solution to Eqs. (8.94) has the form
v(t) = λ
t

0
dτe−λ(t−τ)f(τ),
r(t) =
t

0
dτ

1 −e−λ(t−τ)
f(τ).
(8.95)
In the case of stationary random process f(t) with the correlation tensor
⟨fi(t)fj(t′)⟩= Bij(t −t′)
and temporal correlation radius τ0 determined from the relationship
∞

0
dτBii(τ) = τ0Bii(0),
Eq. (8.94) allows obtaining analytical expressions for correlators between par-
ticle velocity components and coordinates
⟨vi(t)vj(t)⟩= λ
t

0
dτBij(τ)

e−λτ −e−λ(2t−τ)
,
1
2
d
dt ⟨ri(t)rj(t)⟩= ⟨ri(t)vj(t)⟩=
t

0
dτBij(τ)

1 −e−λt 
1 −e−λ(t−τ)
.
(8.96)
In the steady-state regime, when λt ≫1 and t/τ0 ≫1, but parameter λτ0
can be arbitrary, particle velocity is the stationary process with the correla-
tion tensor
⟨vi(t)vj(t)⟩= λ
∞

0
dτBij(τ)e−λτ,
(8.97)
and correlations ⟨ri(t)vj(t)⟩and⟨ri(t)rj(t)⟩are as follows
⟨ri(t)vj(t)⟩=
∞

0
dτBij(τ),
⟨ri(t)rj(t)⟩= 2t
∞

0
dτBij(τ).
(8.98)
If we additionally assume that λτ0 ≫1, the correlation tensor grades into
⟨vi(t)vj(t)⟩= Bij(0),
(8.99)

224
8
General Approaches to Analyzing Stochastic Dynamic Systems
which is consistent with (8.94), because v(t) ≡f(t) in this limit.
If the opposite condition λτ0 ≪1 holds, then
⟨vi(t)vj(t)⟩= λ
∞

0
dτBij(τ).
This result corresponds to the delta-correlated approximation of random pro-
cess f(t).
Probability Distribution Function
Introduce now the indicator function of the solution to Eq. (8.94)
ϕ(r, v, t) = δ (r(t) −r) δ (v(t) −v) ,
which satisﬁes the Liouville equation
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

ϕ(r, v, t) = −λf(t) ∂
∂v ϕ(r, v, t),
ϕ(r, v; 0) = δ (r) δ (v) .
(8.100)
The mean value of the indicator function ϕ(r, v, t) over an ensemble of
realizations of random process f(t) is the joint one-time probability density
of particle position and velocity
P(r, v, t) = ⟨ϕ(r, v; t)⟩= ⟨δ (r(t) −r) δ (v(t) −v)⟩f .
Averaging Eq. (8.100) over an ensemble of realizations of random process
f(t), we obtain the unclosed equation
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) = −λ ∂
∂v ⟨f(t)ϕ(r, v, t)⟩,
P(r, v; 0) = δ (r) δ (v) .
(8.101)
This equation contains correlation ⟨f(t)ϕ(r, v, t)⟩and is equivalent to the
equality
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) =
. .
Θ

t;
δ
iδf(τ)

ϕ(r, v, t)
/
,
P(r, v; 0) = δ (r) δ (v) ,
(8.102)

8.4
Completely Solvable Stochastic Dynamic Systems
225
where functional Θ [t; v(τ)] is related to the characteristic functional of ran-
dom process f(t)
Φ [t; ψ(τ)] =
4
exp
⎧
⎨
⎩i
t

0
dτ ψ(τ)f(τ)
⎫
⎬
⎭
5
= eΘ[t;ψ(τ)]
by the formula
.
Θ [t; ψ(τ)] = d
dt ln Φ [t; ψ(τ)] = d
dtΘ [t; ψ(τ)] .
Functional Θ[t; ψ(τ)] can be expanded in the functional power series
Θ[t; ψ(τ)] =
∞
&
n=1
in
n!
t

0
dt1 · · ·
t

0
dtnK(n)
i1,··· ,in(t1, · · · , tn)ψi1(t1) · · · ψin(tn),
where functions
K(n)
i1,··· ,in(t1, · · · , tn) = 1
in
δn
δψi1(t1) · · · δψin(tn) Θ[t; ψ(τ)]

ψ=0
are the n-th order cumulant functions of random process f(t).
Consider the variational derivative
δ
δfj(t′)ϕ(r, v, t) = −
 ∂
∂rk
δrk(t)
δfj(t′) +
∂
∂vk
δvk(t)
δfj(t′)

ϕ(r, v, t).
(8.103)
In the context of dynamic problem (8.94), the variational derivatives of func-
tions r(t) and v(t) in Eq. (8.103) can be calculated from Eqs. (8.95) and have
the forms
δvk(t)
δfj(t′) = λδkje−λ(t−t′),
δrk(t)
δfj(t′) = δkj

1 −e−λ(t−t′)
.
(8.104)
Using Eq. (8.104), we can now rewrite Eq. (8.103) in the form
δ
δf(t′)ϕ(r, v; t) = −

1 −e−λ(t−t′) ∂
∂r + λe−λ(t−t′) ∂
∂v
	
ϕ(r, v, t),
after which Eq. (8.102) assumes the closed form
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t)
=
.
Θ

t; i

1 −e−λ(t−t′) ∂
∂r + λe−λ(t−t′) ∂
∂v
	
P(r, v, t),
(8.105)

226
8
General Approaches to Analyzing Stochastic Dynamic Systems
P(r, v; 0) = δ (r) δ (v) .
Note that from Eq. (8.105) follows that equations for the n-th order mo-
ment functions include cumulant functions of orders not higher than n.
The Gaussian Process f(t)
Assume now that f(t) is the Gaussian stationary process with the zero mean
value and correlation tensor
Bij(t −t′) = ⟨fi(t)fj(t′)⟩.
In this case, the characteristic functional of process f(t) is
Φ [t; ψ(τ)] = exp
⎧
⎨
⎩−1
2
t

0
t

0
dt1dt2Bij(t1 −t2)ψi(t1)ψj(t2)
⎫
⎬
⎭,
functional
.
Θ[t; ψ(τ)] is given by the formula
.
Θ [t; ψ(τ)] = −ψi(t)
t

0
dt′Bij(t −t′)ψj(t′),
and Eq. (8.105) appears an extension of the Fokker–Planck equation
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) = λ2
t

0
dτBij(τ)e−λτ
∂2
∂vi∂vj
P(r, v, t)
+ λ
t

0
dτBij(τ)

1 −e−λτ
∂2
∂vi∂rj
P(r, v, t),
(8.106)
P(r, v; 0) = δ (r) δ (v) .
Equation (8.106) is the exact equation and remains valid for arbitrary
times t. From this equation follows that r(t) and v(t) are the Gaussian func-
tions. For moment functions of processes r(t) and v(t), we obtain in the
ordinary way the system of equations

8.4
Completely Solvable Stochastic Dynamic Systems
227
d
dt ⟨ri(t)rj(t)⟩= 2 ⟨ri(t)vj(t)⟩,
 d
dt + λ

⟨ri(t)vj(t)⟩= ⟨vi(t)vj(t)⟩+ λ
t

0
dτBij(τ)

1 −e−λτ
,
 d
dt + 2λ

⟨vi(t)vj(t)⟩= 2λ2
t

0
dτBij(τ)e−λτ.
(8.107)
From system (8.107) follows that steady-state values of all one-time correla-
tors for λt ≫1 and t/τ0 ≫1 are given by the expressions
⟨vi(t)vj(t)⟩= λ
∞

0
dτBij(τ)e−λτ,
⟨ri(t)vj(t)⟩= Dij,
⟨ri(t)rj(t)⟩= 2tDij,
(8.108)
where
Dij =
∞

0
dτBij(τ)
(8.109)
is the spatial diﬀusion tensor, which agrees with expressions (8.97) and (8.98).
Remark 8.1. Temporal Correlation Tensor and Temporal Correla-
tion Radius of Process v(t).
We can additionally calculate the temporal correlation radius of velocity
v(t), i.e., the scale of correlator ⟨vi(t)vj(t1)⟩. Using equalities (8.104), we
obtain for t1 < t the equation
 d
dt + λ
	
⟨vi(t)vj(t1)⟩= λ2
t1

0
dt′Bij(t −t′)e−λ(t1−t′) = λ2eλ(t−t1)
t

t−t1
dτBij(τ)e−λτ
(8.110)
with the initial value
⟨vi(t)vj(t1)⟩|t=t1 = ⟨vi(t1)vj(t1)⟩.
(8.111)
In the steady-state regime, i.e., for λt ≫1 and λt1 ≫1, but at ﬁxed
diﬀerence (t −t1), we obtain the equation with initial value (τ = t −t1)
 d
dτ + λ

⟨vi(t + τ)vj(t)⟩= λ2eλτ
∞

τ
dτ1Bij(τ1)e−λτ1,
⟨vi(t + τ)vj(t)⟩τ=0 = ⟨vi(t)vj(t)⟩.
(8.112)

228
8
General Approaches to Analyzing Stochastic Dynamic Systems
One can easily write the solution to Eq. (8.112); however, our interest here
concerns only the temporal correlation radius τv of random process v(t). To
obtain this quantity, we integrate Eq. (8.112) with respect to parameter τ
over the interval (0, ∞). The result is
λ
∞

0
dτ ⟨vi(t + τ)vj(t)⟩= ⟨vi(t)vj(t)⟩+ λ
∞

0
dτ1Bij(τ1)

1 −e−λτ1
,
and we, using Eq. (8.108), arrive at the expression
τv
,
v2(t)
-
= Dii = τ0Bii(0),
(8.113)
i.e.,
τv = τ0Bii(0)
⟨v2(t)⟩=
τ0Bii(0)
λ
∞

0
dτBii(τ)e−λτ
=
⎧
⎨
⎩
τ0 for
λτ0 ≫1,
1/λ for
λτ0 ≪1.
(8.114)
♦
Integrating Eq.(8.106) over r, we obtain the closed equation for the proba-
bility density of particle velocity
 ∂
∂t −λ ∂
∂v v

P(v, t) = λ2
t

0
dτBij(τ)e−λτ
∂2
∂vi∂vj
P(r, v, t),
P(r, v; 0) = δ (v) .
The solution to this equation corresponds to the Gaussian process v(t) with
correlation tensor (8.96), which follows from the fact that the second equation
of system (8.94) is closed. It can be shown that, if the steady-state probabi-
lity density exists under the condition λt ≫1, then this probability density
satisﬁes the equation
−∂
∂vvP(v, t) = λ
∞

0
dτBij(τ)e−λτ
∂2
∂vi∂vj
P(v, t),
and the rate of establishing this distribution depends on parameter λ.
The equation for the probability density of particle coordinate P(r, t) can-
not be derived immediately from Eq. (8.106). Indeed, integrating Eq. (8.106)
over v, we obtain the equality
∂
∂tP(r, t) = −∂
∂r

vP(r, v, t)dv,
P(r, 0) = δ (r) .
(8.115)
For function

vkP(r, v, t)dv, we have the equality

8.4
Completely Solvable Stochastic Dynamic Systems
229
 ∂
∂t + λ
 
vkP(r, v, t)dv = −∂
∂r

vkvP(r, v, t)dv
−λ
t

0
dτBkj(τ)

1 −e−λτ ∂
∂rj
P(r, t),
(8.116)
and so on, i.e., this approach results in an inﬁnite system of equations.
Random function r(t) satisﬁes the ﬁrst equation of system (8.94) and, if
we would know the complete statistics of function v(t) (i.e., the multi-time
statistics), we could calculate all statistical characteristics of function r(t).
Unfortunately, Eq. (8.106) describes only one-time statistical quantities, and
only the inﬁnite system of equations similar to Eqs. (8.115), (8.116), and so
on appears equivalent to the multi-time statistics of function v(t). Indeed,
function r(t) can be represented in the form
r(t) =
t

0
dt1v(t1),
so that the spatial diﬀusion coeﬃcient in the steady-state regime assumes, in
view of Eq. (8.113), the form
1
2
d
dt
,
r2(t)
-
=
∞

0
dτ ⟨v(t + τ)v(t)⟩= τv
,
v2(t)
-
= Dii = τ0Bii(0),
(8.117)
from which follows that it depends on the temporal correlation radius τv and
the variance of random function v(t).
However, in the case of this simplest problem, we know both variances
and all correlations of functions v(t) and r(t) (see Eqs. (8.96)) and, conse-
quently, can draw the equation for the probability density of particle coordi-
nate P(r; t). This equation is the diﬀusion equation
∂
∂tP(r; t) = Dij(t)
∂2
∂ri∂rj
P(r, t),
P(r, 0) = δ (r) ,
where
Dij(t) = 1
2
d
dt ⟨ri(t)rj(t)⟩= 1
2 {⟨ri(t)vj(t)⟩+ ⟨rj(t)vi(t)⟩}
=
t

0
dτBij(τ)

1 −e−λt 
1 −e−λ(t−τ)

230
8
General Approaches to Analyzing Stochastic Dynamic Systems
is the diﬀusion tensor (8.96). Under the condition λt ≫1, we obtain the
equation
∂
∂tP(r; t) = Dij
∂2
∂ri∂rj
P(r, t),
P(r, 0) = δ (r)
(8.118)
with the diﬀusion tensor (8.109).
Note that conversion from Eq. (8.106) to the equation for the probability
density of particle coordinate (8.118) with the diﬀusion coeﬃcient (8.109)
corresponds to the so-called Kramers problem (see, e.g., [131]).
Delta-Correlated Approximation (λτ0 ≪1)
Under the assumption that λτ0 ≪1, where τ0 is the temporal correlation
radius of process f(t), Eq. (8.106) becomes simpler
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) = λ2
t

0
dτBij(τ)
∂2
∂vi∂vj
P(r, v, t),
P(r, v; 0) = δ (r) δ (v)
and corresponds to the approximation of random function f(t) by the delta-
correlated process. If t ≫τ0, we can replace the upper limit of the integral
with the inﬁnity and proceed to the standard diﬀusion Fokker–Planck equa-
tion
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) = λ2Dij
∂2
∂vi∂vj
P(r, v, t),
P(r, v; 0) = δ (r) δ (v)
(8.119)
with the diﬀusion tensor (8.109). In this approximation, the combined random
process {r(t), v(t)} is the Markovian process.
Under the condition λt ≫1, there are the steady-state equation for the
probability density of particle velocity
−λ ∂
∂v vP(v) = λ2Dij
∂2
∂vi∂vj
P(v)
and the nonsteady-state equation for the probability density of particle co-
ordinate
∂
∂tP(r; t) = Dij
∂2
∂ri∂rj
P(r, t),
P(r, 0) = δ (r) .

8.4
Completely Solvable Stochastic Dynamic Systems
231
Another Asymptotic Limit (λτ0 ≫1)
Consider now the limit λτ0 ≫1. In this case, we can rewrite Eq. (8.106) in
the form
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) = λBij(0)

1 −e−λt
∂2
∂vi∂vj
P(r, v, t)
−Bij(0)

1 −e−λt
∂2
∂vi∂rj
P(r, v, t) + λ
t

0
dτBij(τ)
∂2
∂vi∂rj
P(r, v, t),
P(r, v; 0) = δ (r) δ (v) .
Integrating this equation over r, we obtain the equation for the probability
density of particle velocity
 ∂
∂t −λ ∂
∂v v

P(v, t) = λBij(0)

1 −e−λt
∂2
∂vi∂vj
P(v, t),
P(v; 0) = δ (v)
and, under the condition λt ≫1, we arrive at the steady-state Gaussian
probability density with variance
⟨vi(t)vj(t)⟩= Bij(0).
As regards the probability density of particle position, it satisﬁes, under
the condition λt ≫1, the equation
∂
∂tP(r; t) = Dij
∂2
∂ri∂rj
P(r, t),
P(r, 0) = δ (r)
(8.120)
with the same diﬀusion coeﬃcient as previously. This is a consequence of the
fact that Eq. (8.113) is independent of parameter λ. Note that this equation
corresponds to the limit process λ →∞in Eq. (8.94)
d
dtr(t) = v(t),
v(t) = f(t),
r(0) = 0.
In the limit λ →∞(or λτ0 ≫1), we have the equality
v(t) ≈f(t),
(8.121)
and all multi-time statistics of random functions v(t) and r(t) will be
described in terms of statistical characteristics of process f(t). In partic-
ular, the one-time probability density of particle velocity v(t) is the Gaussian

232
8
General Approaches to Analyzing Stochastic Dynamic Systems
probability density with variance ⟨vi(t)vj(t)⟩= Bij(0), and the spatial diﬀu-
sion coeﬃcient is
D = 1
2
d
dt
,
r2(t)
-
=
∞

0
dτBii(τ) = τ0Bii(0).
As we have seen earlier, in the case of process f(t) such that it can be cor-
rectly described in the delta-correlated approximation (i.e., if λτ0 ≪1), the
approximate equality (8.121) appears inappropriate to determine statistical
characteristics of process v(t). Nevertheless, Eq. (8.120) with the same diﬀu-
sion tensor remains as before valid for the one-time statistical characteristics
of process r(t), which follows from the fact that Eq. (8.117) is valid for any
parameter λ and arbitrary probability density of random process f(t).
Above, we considered several types of stochastic ordinary diﬀerential equa-
tions that allow obtaining closed statistical description in the general form.
It is clear that similar situations can appear in dynamic systems formulated
in terms of partial diﬀerential equations.
8.4.2
Partial Diﬀerential Equations
First of all, we note that the ﬁrst-order partial diﬀerential equation
 ∂
∂t + z(t)g(t) ∂
∂xF (x)

ρ(r, t) = 0,
(8.122)
is equivalent to the system of ordinary diﬀerential equations (8.79), page 218
and, consequently, also allows the complete statistical description for arbit-
rary given random process z(t). Solution ρ(r, t) to Eq. (8.122) is a functional
of random process z(t) of the form
ρ(x, t) = ρ[x, t; z(τ)] = ρ(x, T [t; z(τ)])
with functional
T [t; z(τ)] =
t

0
dτ z(τ)g(τ),
(8.123)
so that Eq. (8.122) can be rewritten in the form
 ∂
∂T + ∂
∂xF (x)

ρ(x, T ) = 0.
The variational derivative is expressed in this case as follows

8.4
Completely Solvable Stochastic Dynamic Systems
233
δ
δz(τ)ρ(x, t) =
δ
δz(τ)ρ(x, T [t; z(τ)]) = dρ(x, T )
dT
δT [t; z(τ)]
δz(τ)
= θ(t −τ)g(τ)dρ(x, T )
dT
= −θ(t −τ)g(τ) ∂
∂xF (x)ρ(x, t).
Consider now the class of nonlinear partial diﬀerential equations whose
parameters are independent of spatial variable x,
 ∂
∂t + z(t) ∂
∂x

q(x, t) = F

t, q, ∂q
∂x, ∂
∂x ⊗∂q
∂x, . . .

,
(8.124)
where z(t) is the vector random process and F is the deterministic function.
Solution to this equation is representable in the form
q(x, t) = exp
⎧
⎨
⎩−
t

0
dτ z(τ) ∂
∂x
⎫
⎬
⎭Q (x, t) = Q
⎛
⎝x −
t

0
dτ z(τ), t
⎞
⎠, (8.125)
where we introduced the shift operator. Function Q(x, t) satisﬁes the equation
exp
⎧
⎨
⎩−
t

0
dτ z(τ) ∂
∂x
⎫
⎬
⎭
∂Q(x, t)
∂t
= F
⎛
⎝t,
⎡
⎣exp
⎧
⎨
⎩−
t

0
dτ z(τ) ∂
∂x
⎫
⎬
⎭Q
⎤
⎦,
⎡
⎣exp
⎧
⎨
⎩−
t

0
dτ z(τ) ∂
∂x
⎫
⎬
⎭
∂Q
∂x
⎤
⎦,
∂
∂x ⊗
⎡
⎣exp
⎧
⎨
⎩−
t

0
dτ z(τ) ∂
∂x
⎫
⎬
⎭
∂Q
∂x
⎤
⎦, . . . ,
⎞
⎠.
(8.126)
The shift operator can be factored out from arguments of function
F

t, q, ∂q
∂x, ∂
∂x ⊗∂q
∂x, . . .

,
and Eq. (8.126) assumes the form of the deterministic equation
∂Q(x, t)
∂t
= F

t, Q, ∂Q
∂x , ∂
∂x ⊗exp ∂Q
∂x , . . .

.
(8.127)
Thus, the variational derivative can be expressed in the form
δq(x, t)
δz(τ)
=
δQ

x −
t
0
dτ z(τ), t

δz(τ)
= −θ(t −τ) ∂
∂xq(x, t),
(8.128)

234
8
General Approaches to Analyzing Stochastic Dynamic Systems
i.e., the variational derivatives of the solution to problem (8.124) are expres-
sed in terms of the spatial derivatives.
Consider the Burgers equation with random drift z(t) as a speciﬁc example:
∂
∂tv(x, t) +

(v + z(t)) ∂
∂x

v(x, t) = ν ∂2
∂x2 v(x, t).
(8.129)
In this case, the solution has the form
v(x, t) = exp
⎧
⎨
⎩−
t

0
dτ z(τ) ∂
∂x
⎫
⎬
⎭V (x, t) = V
⎛
⎝x −
t

0
dτ z(τ), t
⎞
⎠,
where function V (x, t) satisﬁes the standard Burgers equation
∂
∂tV (x, t) +

V (x, t) ∂
∂x

V (x, t) = ν ∂2
∂x2 V (x, t).
(8.130)
In this example, the variational derivative is given by the expression
δ
δzi(τ)v(x, t) =
δ
δzi(τ)V
⎛
⎝x −
t

0
dτ z(τ), t
⎞
⎠= −θ(t −τ) ∂
∂xi
v(x, t).
(8.131)
In the case of such problems, statistical characteristics of the solution can
be determined immediately by averaging the corresponding expressions con-
structed from the solution to the last equation. Proceeding in this way, one
obtains that the desired function, say, function ⟨q(x, t)⟩, satisﬁes a closed
equation containing derivatives of all orders with respect to x.
Consider two examples.
One-Dimensional Diﬀusion of Passive Tracers
Consider the one-dimensional diﬀusion of passive tracers in random velocity
ﬁeld. This problem is formulated in terms of the equation
∂
∂tρ(x, t) + v(t) ∂
∂xf(x)ρ(x, t) = 0,
(8.132)
where we will assume that v(t) is the stationary random Gaussian process
with parameters
⟨v(t)⟩= 0,
B(t −t′) = ⟨v(t)v(t′)⟩

Bv(0) =
,
v2(t)
-
and f(x) is the deterministic function. The indicator function ϕ(x, t; ρ) =
δ (ρ(x, t) −ρ) for Eq. (8.132) satisﬁes the equation

8.4
Completely Solvable Stochastic Dynamic Systems
235
 ∂
∂t+v(t)f(x) ∂
∂x

ϕ(x, t; ρ) = v(t)df(x)
dx
∂
∂ρρϕ(x, t; ρ),
ϕ(x, 0; ρ) = δ(ρ0(x) −ρ).
We rewrite this equation in the form
∂
∂tϕ(x, t; ρ) = −v(t)
 ∂
∂xf(x)−df(x)
dx

1 + ∂
∂ρρ
	
ϕ(x, t; ρ),
ϕ(x, 0; ρ) = δ(ρ0(x) −ρ).
(8.133)
Averaging Eq. (8.133) over an ensemble of realizations of random process
v(t), we obtain the expression
∂
∂tP(x, t; ρ) = −
t

0
dt′B(t −t′)
 ∂
∂xf(x)−df(x)
dx

1 + ∂
∂ρρ
 )
δ
δv(t′)ϕ(t,x; ρ)
*
,
(8.134)
P(x, 0; ρ) = δ(ρ0(x) −ρ).
The solution to Eq. (8.133) has the form
ϕ(x, t; ρ) = ϕ(x, T (t); ρ),
where T (t) =
 t
0
dτv(τ) is the new (random) time and function ϕ(x, T ; ρ) as
a function of its arguments satisﬁes the deterministic equation
∂
∂T ϕ(x, T ; ρ) = −
 ∂
∂xf(x)−df(x)
dx

1 + ∂
∂ρρ
	
ϕ(x, T ; ρ),
ϕ(x, 0; ρ) = δ(ρ0(x) −ρ).
(8.135)
Consequently,
δ
δv(t′)ϕ(x, t; ρ) = ∂ϕ(x, T ; ρ)
∂T
δ
δv(t′)T (t) = ∂ϕ(x, T ; ρ)
∂T
θ(t −t′)
= −θ(t −t′)
 ∂
∂xf(x) −df(x)
dx

1 + ∂
∂ρρ
	
ϕ(x, t; ρ),
where θ(t) is the Heaviside step function (1 for t > 0 and 0 for t < 0), and
Eq. (8.134) assumes the form of a closed equation

236
8
General Approaches to Analyzing Stochastic Dynamic Systems
∂
∂tP(x, t; ρ) =
t

0
dt′B(t −t′)
 ∂
∂xf(x) −df(x)
dx

1 + ∂
∂ρρ
	
×
 ∂
∂xf(x) −df(x)
dx

1 + ∂
∂ρρ
	
P(x, t; ρ),
P(x, 0; ρ) = δ(ρ0(x) −ρ).
Burgers Equation with Random Drift
Consider the one-dimensional Burgers equation with random drift
∂
∂tq(x, t) + (q + z(t)) ∂
∂xq(x, t) = ν ∂2
∂x2 q(x, t).
(8.136)
In this case, we have for the variational derivative of q(x, t) with respect to
z(τ)
δ
δz(τ)q(x, t) =
δ
δz(τ)Q
⎛
⎝x −
t

0
dτz(τ), t
⎞
⎠= −θ(t −τ) ∂
∂xq(x, t).
(8.137)
Assume now that random process z(t) is the Gaussian process stationary in
time and described by correlation function B(t−t′). Let us average Eq. (8.136)
over an ensemble of realizations of process z(t) to obtain
∂
∂t ⟨q(x, t)⟩+ 1
2
∂
∂x
,
q2(x, t)
-
+ ∂
∂x ⟨z(t)q(x, t)⟩= ν ∂2
∂x2 ⟨q(x, t)⟩.
(8.138)
We split the correlators in the left-hand side of this equation using formulas
(see Sect. 7.2, page 168)
⟨q[z(τ) + η1(τ)]q[z(τ) + η2(τ)]⟩
= exp
⎧
⎨
⎩
t

0
dτ1
t

0
dτ2B(τ1 −τ2)
δ2
δη1(τ1)δη2(τ2)
⎫
⎬
⎭⟨q[z(τ) + η1(τ)]q[z(τ) + η2(τ)]⟩,
⟨z(t)q(x, t)⟩=
t

0
dτB(t −τ)
.
δ
δz(τ)q(x, t)
/
.
In view of Eq. (8.137) we can represent these formulas in the form

8.5
Delta-Correlated Fields and Processes
237
q2(x, t) = exp
⎧
⎨
⎩2
t

0
dτ(t −τ)B(τ)
∂2
∂η1∂η2
⎫
⎬
⎭⟨q(x + η1, t)⟩⟨q(x + η2, t)⟩|η=0
=
∞
%
n=0
2n
n!
⎡
⎣
t

0
dτ(t −τ)B(τ)
⎤
⎦
n  ∂n
∂xn ⟨q(x, t)⟩
!2
,
⟨z(t)q(x, t)⟩= −
t

0
dτB(τ) ∂
∂x ⟨q(x, t)⟩.
As a result, Eq. (8.138) becomes the closed equation
∂
∂t ⟨q(x, t)⟩+ 1
2
∂
∂x
∞
&
n=0
2n
n!
⎡
⎣
t

0
dτ(t −τ)B(τ)
⎤
⎦
n  ∂n
∂xn ⟨q(x, t)⟩
2
=
⎛
⎝ν +
t

0
dτB(τ)
⎞
⎠∂2
∂x2 ⟨q(x, t)⟩,
(8.139)
However, in contrast to (8.136), this equation depends on all derivatives with
respect to spatial variable x [48,49].
Unfortunately, there is only limited number of equations that allow suﬃ-
ciently complete analysis. In the general case, the analysis of dynamic systems
appears possible only on the basis of various asymptotic and approximate
techniques. In physics, techniques based on approximating actual random
processes and ﬁelds by the ﬁelds delta-correlated in time are often and suc-
cessfully used.
8.5
Delta-Correlated Fields and Processes
8.5.1
General Remarks
In the case of random ﬁeld f(x, t) delta-correlated in time, the following
equality holds (see Sect. 7.8, page 181)
˙Θt[t, t0; v(y, τ)] ≡˙Θt[t, t0; v(y, t)]
and situation becomes signiﬁcantly simpler. The fact that ﬁeld f(x, t) is
delta-correlated means that

238
8
General Approaches to Analyzing Stochastic Dynamic Systems
Θ[t, t0; v(y, τ)] =
∞
&
i=1
in
n!
t

t0
dτ

dy1 · · ·

dynKi1,··· ,in
n
(y1, · · · , yn; τ)
× vi1(y1, τ) · · ·vin(yn, τ),
which, in turn, means that ﬁeld f(x, t) is characterized by cumulant functions
of the form
Ki1,··· ,in
n
(y1, t1; · · · ; yn, tn)=Ki1,··· ,in
n
(y1, · · · , yn;t1)δ(t1−t2) · · · δ(tn−1−tn).
In view of Eq. (8.3), page 194 Eqs. (8.7), (8.9), and (8.12) appear to be in
this case the closed operator equations in functions P(x, t), p(x, t|x0, t0), and
Pm(x1, t1; · · · ; xm, tm). Indeed, Eq. (8.7), page 195 is reduced to the equation
 ∂
∂t + ∂
∂xv(x, t)

P(x,t) = ˙Θt

t, t0; i ∂
∂xδ(y −x)

P(x, t),
P(x, 0) = δ(x −x0),
(8.140)
whose concrete form is governed by functional Θ[t, t0; v(y, τ)], i.e., by sta-
tistical behavior of random ﬁeld f(x, t). Correspondingly, Eq. (8.12) for the
m-time probability density is reduced to the operator equation (t1≤t2 ≤
· · · ≤tm)
 ∂
∂t +
∂
∂xm
v(xm, tm)

Pm(x1,t1; · · · ; xm, tm)
= ˙Θtm

tm, t0; i
∂
∂xm
δ(y −xm)

Pm(x1, t1; · · · ; xm, tm),
(8.141)
Pm(x1, t1; · · · ; xm, tm−1) = δ(xm −xm−1)Pm−1(x1, t1; · · · ; xm−1, tm−1).
We can seek the solution to Eq. (8.141) in the form
Pm(x1, t1; · · · ; xm, tm)=p(xm, tm|xm−1, tm−1)Pm−1(x1, t1; · · · ; xm−1, tm−1). (8.142)
Because all diﬀerential operations in Eq. (8.141) concern only tm and xm, we
can substitute Eq. (8.142) in Eq. (8.141) to obtain the following equation for
the transition probability density
 ∂
∂t + ∂
∂xv(x, t)

p(x, t|x0,t0) = ˙Θt

t, t0; i ∂
∂xδ(y −x)

p(x, t|x0, t0),
p(x, t|x0, t0)|t→t0 = δ(x −x0).
(8.143)

8.5
Delta-Correlated Fields and Processes
239
Here, we denoted variables xm and tm as x and t and variables xm−1 and
tm−1 as x0 and t0.
Using formula (8.142) (m −1) times, we obtain the relationship
Pm(x1, t1; · · · ; xm, tm) = p(xm, tm|xm−1, tm−1) · · · p(x2, t2|x1, t1)P(x1, t1),
(8.144)
where P(x1, t1) is the one-time probability density governed by Eq. (8.140).
Equality (8.144) expresses the many-time probability density in terms of the
product of transition probability densities, which means that random process
x(t) is the Markovian process. The transition probability density is deﬁned
in this case as follows:
p(x, t|x0, t0) = ⟨δ(x(t) −x)|x0, t0⟩.
Special models of parameter ﬂuctuations can signiﬁcantly simplify the ob-
tained equations.
For example, in the case of the Gaussian delta-correlated ﬁeld f(x, t), the
correlation tensor has the form (⟨f(x, t)⟩= 0)
Bij(x, t; x′, t′) = 2δ(t −t′)Fij(x, x′; t).
Then, functional Θ[t, t0; v(y, τ)] assumes the form
Θ[t, t0; v(y, τ)] = −
t

t0
dτ

dy1

dy2Fij(y1, y2; τ)vi(y1, τ)vj(y2, τ),
and Eq. (8.140) reduces to the Fokker–Planck equation
 ∂
∂t +
∂
∂xk
[vk(x,t) + Ak(x,t)]

P(x, t) =
∂2
∂xk∂xl
[Fkl(x, x,t)P(x,t)] ,
(8.145)
where
Ak(x, t) =
∂
∂x′
l
Fkl(x, x′;t)

x′=x
.
Note that Eq. (8.9), page 197 in this case assumes the form of the backward
Fokker–Planck equation (see, e.g., [21])
 ∂
∂t0
+ [vk(x0,t0)+Ak(x0,t0)]
∂
∂x0k

P(x, t|x0, t0)
= −Fkl(x0, x0; t0)
∂2
∂x0k∂x0l
P(x, t|x0, t0)
(8.146)
with initial condition

240
8
General Approaches to Analyzing Stochastic Dynamic Systems
P(x, t|x0, t) = δ(x −x0).
In view of the special role that the Gaussian delta-correlated ﬁeld f(x, t)
plays in physics, we give an alternative and more detailed discussion of this
approximation commonly called the approximation of the Gaussian delta-
correlated ﬁeld in Chapter 10, page 305.
For random ﬁeld f(x, t) related to the delta-correlated Poisson process
(see Sect. 7.8 page 181) one can obtain the forward and backward equations
of type of the Kolmogorov–Feller equation.
We illustrate the above general theory by the examples of several equations.
8.5.2
One-Dimensional Nonlinear Diﬀerential
Equation
Consider the one-dimensional stochastic equation
d
dtx(t) = f(x, t) + z(t)g(x, t),
x(0) = x0,
(8.147)
where f(x, t) and g(x, t) are the deterministic functions and z(t) is the random
function of time. For indicator function ϕ(x, t) = δ(x(t) −x), we have the
Liouville equation
 ∂
∂t + ∂
∂xf(x, t)

ϕ(x, t) = −z(t) ∂
∂x {g(x, t)ϕ(x, t)} ,
so that the equation for the one-time probability density P(x, t) has the form
 ∂
∂t + ∂
∂xf(x, t)

P(x, t) =
.
˙Θt

t,
δ
iδz(τ)

ϕ(x, t)
/
.
In the case of the delta-correlated random process z(t), the equality
˙Θt[t, v(τ)] = ˙Θt[t, v(t)]
holds. Taking into account the equality
δ
δz(t −0)ϕ(x, t) = −∂
∂x {g(x, t)ϕ(x, t)} ,
we obtain the closed operator equation
 ∂
∂t + ∂
∂xf(x, t)

P(x, t) = ˙Θt

t, i ∂
∂xg(x, t)

P(x, t).
(8.148)
For the Gaussian delta-correlated process , we have

8.5
Delta-Correlated Fields and Processes
241
Θ[t, v(τ)] = −1
2
t

0
dτB(τ)v2(τ),
(8.149)
and Eq. (8.148) assumes the form of the Fokker–Planck equation
 ∂
∂t + ∂
∂xf(x, t)

P(x, t) = 1
2B(t) ∂
∂xg(x, t) ∂
∂xg(x, t)P(x, t).
(8.150)
For the Poisson delta-correlated process z(t), we have
Θ[t, v(τ)] = ν
t

0
dτ
⎧
⎨
⎩
∞

−∞
dξp(ξ)eiξv(τ) −1
⎫
⎬
⎭,
(8.151)
and Eq. (8.148) reduces to the form
 ∂
∂t + ∂
∂xf(x, t)

P(x, t) = ν
⎧
⎨
⎩
∞

−∞
dξp(ξ)e−ξ ∂
∂x g(x,t)−1
⎫
⎬
⎭P(x, t).
(8.152)
If we set g(x, t) = 1, Eq. (8.147) assumes the form
d
dtx(t) = f(x, t) + z(t),
x(0) = x0.
In this case, the operator in the right-hand side of Eq. (8.152) is the shift op-
erator, and Eq. (8.152) assumes the form of the Kolmogorov–Feller equation
 ∂
∂t + ∂
∂xf(x, t)

P(x, t) = ν
∞

−∞
dξp(ξ)P(x −ξ, t) −νP(x, t).
Deﬁne now g(x, t) = x, so that Eq. (8.147) reduces to the form
d
dtx(t) = f(x, t) + z(t)x(t),
x(0) = x0.
In this case, Eq. (8.152) assumes the form
 ∂
∂t + ∂
∂xf(x, t)

P(x, t) = ν
⎧
⎨
⎩
∞

−∞
dξp(ξ)e−ξ ∂
∂x x−1
⎫
⎬
⎭P(x, t).
(8.153)
To determine the action of the operator in the right-hand side of
Eq. (8.153), we expand it in series in ξ

242
8
General Approaches to Analyzing Stochastic Dynamic Systems
"
e−ξ ∂
∂x x −1
#
P(x, t) =
∞
&
n=1
(−ξ)n
n!
 ∂
∂xx
n
P(x, t)
and consider the action of every term.
Representing x in the form x = eϕ, we can transform this formula as
follows (the fact that x is the alternating quantity is insigniﬁcant here)
∞
&
n=1
(−ξ)n
n!
e−ϕ ∂n
∂ϕn eϕP(eϕ, t) = e−ϕ "
e−ξ ∂
∂ϕ −1
#
eϕP(eϕ, t) =
e−ξP(eϕ−ξ, t) −P(eϕ, t).
Reverting to variable x, we can represent Eq. (8.153) in the ﬁnal form of the
integro-diﬀerential equation similar to the Kolmogorov–Feller equation
 ∂
∂t + ∂
∂xf(x, t)

P(x, t) = ν
∞

−∞
dξp(ξ)e−ξP(xe−ξ, t) −νP(x, t).
In Chapter 3, page 137, we mentioned that formula
x(t) =
t

0
dτg(t −τ)z(τ)
relates the Poisson process x(t) with arbitrary impulse function g(t) to the
Poisson delta-correlated random process z(t). Let g(t) = e−λt. In this case,
process x(t) satisﬁes the stochastic diﬀerential equation
d
dtx(t) = −λx(t) + z(t)
and, consequently, both transition probability density and one-time probabi-
lity density of this process satisfy, according to Eq. (8.152), the equations
∂
∂tp(x, t|x0, t0) = %L(x)p(x, t|x0, t0),
∂
∂tP(x, t) = %L(x)P(x, t),
where operator
%L(x) = λ ∂
∂xx + ν
⎧
⎨
⎩
∞

−∞
dξp(ξ)e−ξ ∂
∂x −1
⎫
⎬
⎭.
(8.154)

8.5
Delta-Correlated Fields and Processes
243
8.5.3
Linear Operator Equation
Consider now the linear operator equation
d
dtx(t) = %A(t)x(t) + z(t) %B(t)x(t),
x(0) = x0,
(8.155)
where %A(t) and %B(t) are the deterministic operators (e.g., diﬀerential opera-
tors with respect to auxiliary variables or regular matrixes). We will assume
that function z(t) is the random delta-correlated function.
Averaging system (8.155), we obtain, according to general formulas,
d
dt ⟨x(t)⟩= %A(t) ⟨x(t)⟩+
.
˙Θt

t,
δ
iδz(t)

x(t)
/
.
(8.156)
Then, taking into account the equality
δ
δz(t −0)x(t) = %B(t)x(t)
that follows immediately from Eq. (8.155), we can rewrite Eq. (8.156) in the
form
d
dt ⟨x(t)⟩= %A(t) ⟨x(t)⟩+ ˙Θt

t, −i %B

⟨x(t)⟩.
(8.157)
Thus, in the case of linear system (8.155), equations for average values also
are the linear equations.
We can expand the logarithm of the characteristic functional Θ[t; v(τ)] of
delta-correlated processes in the functional Fourier series
Θ[t; v(τ)] =
∞
&
n=1
in
n!
t

0
dτKn(τ)vn(τ),
(8.158)
where Kn(t) determine the cumulant functions of process z(t). Substituting
Eq. (8.158) in Eq. (8.157), we obtain the equation
d
dt ⟨x(t)⟩= %A(t) ⟨x(t)⟩+
∞
&
n=1
1
n!Kn(t)

%B(t)
n
⟨x(t)⟩.
(8.159)
If there exists power l such that %Bl(t) = 0, then Eq. (8.159) assumes the form
d
dt ⟨x(t)⟩= %A(t) ⟨x(t)⟩+
l−1
&
n=1
1
n!Kn(t)

%B(t)
n
⟨x(t)⟩.
(8.160)
In this case, the equation for average value depends only on a ﬁnite number of
cumulants of process z(t). This means that there is no necessity in knowledge

244
8
General Approaches to Analyzing Stochastic Dynamic Systems
of probability distribution of function z(t) in the context of the equation
for average value; suﬃcient information includes only certain cumulants of
process and knowledge of the fact that process z(t) can be considered as the
delta-correlated random process. Statistical description of an oscillator with
ﬂuctuating frequency is a good example of such system in physics.
Stochastic Parametric Resonance
Consider statistical description of an oscillator with ﬂuctuating frequency
(1.32), page 22 as an example of simple linear dynamic system that allows a
suﬃciently complete analysis. The problem on such an oscillator is formulated
in terms of the initial-value problem for the second-order diﬀerential equation
d2
dt2 x(t) + ω2
0[1 + z(t)]x(t) = 0,
x(0) = x0,
d
dtx(0) = y0,
(8.161)
which is equivalent to the system of equations
d
dtx(t) = y(t),
d
dty(t) = −ω2
0[1 + z(t)]x(t),
x(0) = x0,
y(0) = y0.
(8.162)
For system (8.162), indicator function
ϕ(x, y, t) = δ (x(t) −x) δ (y(t) −y)
satisﬁes the Liouville equation
 ∂
∂t + y ∂
∂x −ω2
0x ∂
∂y

ϕ(x, y, t) = ω2
0z(t)x ∂
∂yϕ(x, y, t).
The joint one-time probability density of solutions to system (8.162) is deﬁned
by the equality P(x, y, t) = ⟨ϕ(x, y, t)⟩and satisﬁes the operator equation
 ∂
∂t + y ∂
∂x −ω2
0x ∂
∂y

P(x, y, t) =
. .
Θt

t;
δ
iδz(τ)

ϕ(x, y, t)
/
,
(8.163)
where
.
Θt [t; v(τ)] = d
dtΘ [t; v(τ)], and Θ [t; v(τ)] is the logarithm of the cha-
racteristic functional of process z(t),

8.5
Delta-Correlated Fields and Processes
245
Θ [t; v(τ)] = ln
4
exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
.
In the case of delta-correlated process z(t), the joint one-time probability
density of solutions to system (8.162) satisﬁes the simpliﬁed operator equation
 ∂
∂t + y ∂
∂x −ω2
0x ∂
∂y

P(x, y, t) =
. .
Θt

t;
δ
iδz(t)

ϕ(x, y, t)
/
,
(8.164)
which, in view of the equality
δ
δz(t −0)ϕ(x, y, t) = ω2
0x ∂
∂y ϕ(x, y, t)
immediately following from the Liouville equation, can be represented as the
closed operator equation
 ∂
∂t + y ∂
∂x −ω2
0x ∂
∂y

P(x, y, t) =
.
Θt

t; −iω2
0x ∂
∂y

P(x, y, t).
(8.165)
Equation (8.165) oﬀers a possibility of deriving closed systems of equations
for moments of arbitrary orders. This possibility follows from the fact that the
operator in the right-hand side of Eq. (8.165) depends only on homogeneous
combination x ∂
∂y whose action cannot increase the order of the moment
under consideration, which is, of cause, a consequence of linearity of the
initial system of equations (8.162). Hence, the equations for moments will
depend only on the process z(t) cumulants whose orders are smaller or equal
to the order of the moment of interest.
Indeed, consider the vector quantity
Ak(t) = xk(t)yN−k(t)
(k = 0, . . . , N).
One can derive from system (8.162) that this quantity satisﬁes the stochastic
equation
d
dtAk(t) = kAk−1(t) −ω2
0(N −k)[1 + z(t)]Ak+1(t)
(k = 0, . . . , N),
which corresponds to the linear operator equation (8.155), page
243 with
constant matrixes
%Aij = iδi,j+1 −ω2
0(N −i)δi,j−1,
%Bij = −ω2
0(N −i)δi,j−1.
It is obvious that the square of matrix %Bij is
%B2
ij = −ω4
0(N −i)(N −j + i)δi,j−2

246
8
General Approaches to Analyzing Stochastic Dynamic Systems
and so on for higher powers; consequently, for power N + 1 we have
%BN+1 ≡0.
According to (8.159), page 243, averages ⟨Ak(t)⟩(k = 0, · · · , N) satisfy
the equation
d
dt ⟨Ak(t)⟩= k ⟨Ak−1(t)⟩−ω2
0(N −k) ⟨Ak+1(t)⟩+
N
&
n=1
1
n!Kn

%Bn
kl ⟨Al(t)⟩,
(8.166)
where Kn are the cumulants of random process z(t) and the summation is
truncated at n = N because, as was mentioned, the equation for average
value can depend only on the cumulants of process z(t) whose orders are
smaller or equal to N. In particular, the ﬁrst moments of the solution to
the system of stochastic equations (8.162) for the delta-correlated process
z(t) are independent of ﬂuctuations of system parameters in view of the
equality K1 = 0, and the second moments satisfy the system of equations
that coincides with the system derived for the Gaussian ﬂuctuations of system
parameters.
In the case of the delta-correlated process z(t), we can additionally obtain
the correlation functions of solutions to system of equations (8.162). Indeed,
multiplying system (8.162) by x(t′), where t′ < t, and averaging the result
over an ensemble of realizations of process z(t), we obtain the closed system
d
dt ⟨x(t)x(t′)⟩= ⟨y(t)x(t′)⟩,
d
dt ⟨y(t)x(t′)⟩= −ω2
0 ⟨x(t)x(t′)⟩,
(8.167)
because
δ
δz(t −0)x(t)x(t′) = 0.
The initial values of this system are as follows
⟨x(t)x(t′)⟩t=t′ =
,
x2(t′)
-
,
⟨y(t)x(t′)⟩t=t′ = ⟨x(t′)y(t′)⟩.
(8.168)
The system of equations for the other pair of correlation functions for t > t′
is derived similarly
d
dt ⟨x(t)y(t′)⟩= ⟨y(t)y(t′)⟩,
d
dt ⟨y(t)y(t′)⟩= −ω2
0 ⟨x(t)y(t′)⟩.
(8.169)
The corresponding initial conditions are
⟨x(t)y(t′)⟩t=t′ = ⟨x(t′)y(t′)⟩,
⟨y(t)y(t′)⟩t=t′ =
,
y2(t′)
-
.
(8.170)
Solutions to systems of equations (8.167) and (8.169) with the respective
initial values (8.168) and (8.170) have the form

8.5
Delta-Correlated Fields and Processes
247
⟨x(t)x(t′)⟩=
,
x2(t′)
-
cos ω0(t −t′) + 1
ω0
⟨x(t′)y(t′)⟩sin ω0(t −t′),
⟨y(t)x(t′)⟩= −ω0
,
x2(t′)
-
sin ω0(t −t′) + ⟨x(t′)y(t′)⟩cos ω0(t −t′),
⟨x(t)y(t′)⟩= ⟨x(t′)y(t′)⟩cos ω0(t −t′) + 1
ω0
,
y2(t′)
-
sin ω0(t −t′),
⟨y(t)y(t′)⟩= −ω0 ⟨x(t′)y(t′)⟩sin ω0(t −t′) +
,
y2(t′)
-
cos ω0(t −t′).
(8.171)
Gaussian Delta-Correlated Fluctuations of Parameters
For
the
Gaussian
stationary
delta-correlated process
z(t),
functional
Θ [t; v(τ)] is given by the formula

⟨z(t)⟩= 0, ⟨z(t)z(t′)⟩= 2σ2τ0δ(t −t′)

Θ [t; v(τ)] = −σ2τ0
t

0
dτv2(τ),
where σ2 is the variance and τ0 is the temporal correlation radius of process
z(t), so that Eq. (8.165) assumes the form of the Fokker–Planck equation
 ∂
∂t + y ∂
∂x −ω2
0x ∂
∂y

P(x, y, t) = Dω2
0x2 ∂2
∂y2 P(x, y, t),
P(x, y, 0) = δ (x −x0) δ (y −y0) ,
(8.172)
where D = σ2τ0ω2
0 is the diﬀusion coeﬃcient in space {x, y/ω0}.
Let us derive the equations for the two ﬁrst moments of solutions to system
(8.162).
For average values of x(t) and y(t), we obtain the system of equations
d
dt ⟨x(t)⟩= ⟨y(t)⟩,
d
dt ⟨y(t)⟩= −ω2
0 ⟨x(t)⟩,
x(0) = x0,
y(0) = y0
(8.173)
that coincides with system (8.162) without ﬂuctuations, which agrees with
the above discussion. Consequently, we have
⟨x(t)⟩= x0 cos ω0(t −t′) + 1
ω0
y0 sin ω0(t −t′),
⟨y(t)⟩= −ω0x0 sin ω0(t −t′) + y0 cos ω0(t −t′).
(8.174)

248
8
General Approaches to Analyzing Stochastic Dynamic Systems
The second moments of quantities x(t) and y(t) satisfy the system of equa-
tions
d
dt
,
x2(t)
-
= 2 ⟨x(t)y(t)⟩,
d
dt ⟨x(t)y(t)⟩=
,
y2(t)
-
−ω2
0
,
x2(t)
-
,
d
dt
,
y2(t)
-
= −2ω2
0 ⟨x(t)y(t)⟩+ Dω2
0
,
x2(t)
-
.
(8.175)
From this system, we can derive the closed third-order equation for any
particular moment. For example, for quantity ⟨U(t)⟩=
,
x2(t)
-
that describes
the average potential energy of an oscillator, we obtain the equation
d3
dt3 ⟨U(t)⟩+ 4ω2
0
d
dt ⟨U(t)⟩−4Dω2
0 ⟨U(t)⟩= 0,
(8.176)
which corresponds to the following stochastic initial-value problem for U(t) =
x2(t)
d3
dt3 U(t) + 4ω2
0
d
dtU(t) + 2ω2
0

z(t) d
dtU(t) + d
dtz(t)U(t)

= 0,
U(0) = x2
0,
d
dtU(t)

t=0
= 2x0y0,
d2
dt2 U(t)

t=0
= 2y2
0 −ω2
0[1 + z(0)]x2
0
(8.177)
that can also be obtained immediately from system (8.162).
To simplify the calculations, we will assume that the initial values of system
(8.162) have the form
x(0) = 0,
y(0) = ω0.
(8.178)
Assuming that the problem has a small parameter related to the intensity of
process z(t) ﬂuctuations, we can approximately (to terms of order of D/ω0)
represent the solution to system (8.175) in the form
,
x2(t)
-
= 1
2

eDt −e−Dt
2

cos(2ω0t) + 3D
4ω0
sin(2ω0t)
	
,
⟨x(t)y(t)⟩= ω0
4

2e−Dt
2 sin(2ω0t) + D
ω0

eDt −e−Dt
2 cos(2ω0t)
	
,
,
y2(t)
-
= ω2
0
2

eDt + e−Dt
2

cos(2ω0t) −D
4ω0
sin(2ω0t)
	
.
(8.179)

8.5
Delta-Correlated Fields and Processes
249
Thus, solution (8.179) of system of equations (8.175) has terms increasing
with time, which corresponds to statistical parametric build-up of ﬂuctuations
in dynamic system (8.162) at the expense of frequency ﬂuctuations. In the
case of weak ﬂuctuations, the increment of ﬂuctuations is
μ = D
(D/ω0 ≪1).
From Eqs. (8.179) follows that solutions to statistical problem (8.162) have
two characteristic temporal scales t1 ∼1/ω0 and t2 ∼1/D. The ﬁrst tempo-
ral scale corresponds to the period of oscillations in system (8.162) without
ﬂuctuations (fast processes), and the second scale characterizes slow vari-
ations of statistical characteristics, which appear due to ﬂuctuations (slow
processes). The ratio of these scales is small:
t1/t2 = D/ω0 ≪1.
We can explicitly obtain slow variations of statistical characteristics of
processes x(t) and y(t) by excluding fast motions by means of averaging the
corresponding quantities over the period T = 2π/ω0. Denoting such averaging
with the overbar, we have
⟨x2(t)⟩= 1
2eDt,
⟨x(t)y(t)⟩= 0,
⟨y2(t)⟩= ω2
0
2 eDt.
Stochastic Problem with Linear Friction
If we add the linear friction to the system of equations (8.162), i.e., if we
consider the dynamic system
d
dtx(t) = y(t),
d
dty(t) = −2γy(t) −ω2
0[1 + z(t)]x(t),
(8.180)
then the corresponding Fokker–Planck equation will have the form
 ∂
∂t −2γ ∂
∂yy + y ∂
∂x −ω2
0x ∂
∂y

P(x, y, t) = Dω2
0x2 ∂2
∂y2 P(x, y, t)
and the system of equations for the second moments assumes, instead of
(8.175), the form
d
dt
,
x2(t)
-
= 2 ⟨x(t)y(t)⟩,
d
dt ⟨x(t)y(t)⟩=
,
y2(t)
-
−2γ ⟨x(t)y(t)⟩−ω2
0
,
x2(t)
-
,
d
dt
,
y2(t)
-
= −4γ
,
y2(t)
-
−2ω2
0 ⟨x(t)y(t)⟩+ Dω2
0
,
x2(t)
-
.

250
8
General Approaches to Analyzing Stochastic Dynamic Systems
For this system, we will seek the solution proportional to eλt. The corres-
ponding characteristic equation for λ assumes then the form
λ3 + 6γλ2 + 4(ω2
0 + 2γ2)λ + 4ω2
0(2γ −D) = 0.
As is known, the necessary and suﬃcient conditions of solution stability
(which means the absence of roots λk with positive real parts) is formulated as
the Routh–Hurwitz condition, which is equivalent in our case to the inequality
D < 2γ. Thus, if this condition is not satisﬁed, i.e., if
2γ < D,
(8.181)
the second moments grow in time exponentially, which means the occurrence
of the statistical parametric excitation of second moments. Note that condi-
tions of the statistical parametric excitation diﬀer for diﬀerent moments. For
example, the condition of exciting the fourth moments appears weaker than
condition (8.181) and has the form [122]
D > 2γ
3
ω2
0 + 3γ2
ω2
0 + 6γ2 .
For the stochastic parametric oscillator with friction, we can consider the
problem on the steady-state regime that steadies under the action of random
forces statistically independent of frequency ﬂuctuations. This problem is
formulated as the stochastic system of equations
d
dtx(t) = y(t),
d
dty(t) = −2γy(t) −ω2
0[1 + z(t)]x(t) + f(t),
(8.182)
where f(t) is the Gaussian process statistically independent of process z(t);
it is assumed that f(t) is the delta-correlated process with the following
parameters
⟨f(t)⟩= 0,
⟨f(t)f(t′)⟩= 2σ2
fτfδ(t −t′),
where σ2
f is the variance and τf is the temporal correlation radius of process
f(t).
The one-time probability density of the solutions to stochastic system
(8.182) satisﬁes the Fokker–Planck equation
 ∂
∂t −2γ ∂
∂yy + y ∂
∂x −ω2
0x ∂
∂y

P(x, y, t)
= Dω2
0x2 ∂2
∂y2 P(x, y, t) + σ2
fτf
∂2
∂y2 P(x, y, t),
(8.183)
and, consequently, we have
⟨x(t)⟩= 0,
⟨y(t)⟩= 0.

8.5
Delta-Correlated Fields and Processes
251
Equations for the second moments form in this case the system
d
dt
,
x2(t)
-
= 2 ⟨x(t)y(t)⟩,
d
dt ⟨x(t)y(t)⟩=
,
y2(t)
-
−2γ ⟨x(t)y(t)⟩−ω2
0
,
x2(t)
-
,
(8.184)
d
dt
,
y2(t)
-
= −4γ
,
y2(t)
-
−2ω2
0 ⟨x(t)y(t)⟩+ Dω2
0
,
x2(t)
-
+ 2σ2
fτf,
whose steady-state solution exists for t →∞if the condition (8.181) is sat-
isﬁed. This solution behaves as follows
⟨x(t)y(t)⟩= 0,
,
x2(t)
-
=
σ2
fτf
ω2
0(D −2γ),
,
y2(t)
-
=
σ2
fτf
D −2γ .
Poisson Delta-Correlated Fluctuations of Parameters
Functional Θ[t; v(τ)] of the Poisson delta-correlated random process z(t) is
given by Eq. (5.33)
Θ[t; v(τ)] = ν
t

0
dτ
∞

−∞
dξp(ξ)

eiξv(τ) −1

,
so that Eq. (8.164) assumes the form of the Kolmogorov–Feller equation
 ∂
∂t + y ∂
∂x −ω2
0x ∂
∂y

P(x, y, t) = ν
∞

−∞
dξp(ξ)P(x, y+ξω2
0x, t)−νP(x, y, t).
(8.185)
For suﬃciently small parameter ξ, the logarithm of the characteristic func-
tional grades into the expression
Θ[t; v(τ)] = −ν
,
ξ2-
t

0
dτv2(τ),
and Eq. (8.185) grades into the Fokker–Planck equation (8.172) with the dif-
fusion coeﬃcient
D = 1
2ν
,
ξ2-
ω2
0x.

252
8
General Approaches to Analyzing Stochastic Dynamic Systems
8.5.4
Partial Diﬀerential Equations
Statistical Interpretation of Solutions to Deterministic Equations
In a number of cases, solutions to many deterministic problems can be treated
as a result of averaging certain functionals over random trajectories. Such
interpretation can be useful in the context of various applications.
Let us derive the conditions under which such interpretation is applicable
to some simple equations.
Consider the problem formulated as the initial-value problem for the par-
tial diﬀerential equation
∂
∂tu(r, t) = −q(r, t)u(t, r) + Q(∇, t)u(r, t),
u(r, 0) = u0(r).
(8.186)
Along with Eq. (8.186), we consider the ﬁrst-order partial diﬀerential equation
∂
∂tφ(r, t) = −q(r, t)φ(t, r) + z(t)∇φ(r, t),
φ(r, 0) = u0(r)
(8.187)
whose solution has the form
φ[r, t; z(τ)] = u0
⎛
⎝r +
t

0
dτz(τ)
⎞
⎠exp
⎧
⎨
⎩−
t

0
dτq
⎛
⎝τ,r +
t

τ
dτ ′z(τ ′)
⎞
⎠
⎫
⎬
⎭.
(8.188)
We will assume that z(t) is the random function delta-correlated in time
t with characteristic functional Φ[t; v(τ)]. Averaging Eq. (8.187) over an en-
semble of realizations z(t), we obtain the equation
∂
∂t ⟨φ(r, t)⟩= −q(r, t) ⟨φ(r, t)⟩+
.
˙Θt

t,
δ
iδz(t)

φ(r, t)
/
.
(8.189)
Taking into account the equality
δ
δz(t −0)φ(r, t) = ∇φ(r, t),
which is a consequence of the initial dynamic equation (8.187), we can rewrite
Eq. (8.189) in the form
∂
∂t ⟨φ(r, t)⟩= −q(r, t) ⟨φ(r, t)⟩+ ˙Θt [t, −i∇] ⟨φ(r, t)⟩.
(8.190)
Comparing now Eq. (8.190) with Eq. (8.186), we can see that
u(r, t) = ⟨φ[r, t; z(τ)]⟩z
(8.191)

8.5
Delta-Correlated Fields and Processes
253
if
Q(∇, t) = ˙Θt[t, −i∇].
(8.192)
In this case, we can treat Eq. (8.191) as the solution to Eq. (8.186) written in
the form of the continual integral.
In addition, we can give the operator form of Eq. (8.191) by introducing
the functional shift operator
u(r, t) = ⟨φ[r, t; z(τ) + v(τ)]⟩z|v=0 = Φ

t,
δ
iδv(τ)

φ[r, t; v(τ)]

v=0
,
(8.193)
where Φ[t; v(τ)] is the characteristic functional of process z(t).
For the Gaussian process z(t), we have
Θ[t, v(τ)] = −1
2
t

0
dτB(τ)v2(τ)

Q(∇, t) = 1
2B(t)Δ,
B(t)> 0

.
As a consequence, we obtain the well-known result that the solution to the
diﬀusion equation
∂
∂tu(r, t) = −q(r, t)u(t, r) + 1
2B(t)Δu(r, t),
u(0, r) = u0(r)
(8.194)
can be treated as the result of averaging the functional φ[r, t; z(τ)] over the
Gaussian delta-correlated process z(t), i.e.
u(r, t) =
4
u0
⎛
⎝r +
t

0
dτz(τ)
⎞
⎠exp
⎧
⎨
⎩−
t

0
dτq
⎛
⎝r +
t

τ
dτ ′z(τ ′), τ
⎞
⎠
⎫
⎬
⎭
5
z
= exp
⎧
⎨
⎩−1
2
t

0
dτB(τ)
δ2
δv2(τ)
⎫
⎬
⎭
×
⎧
⎨
⎩u0
⎛
⎝r +
t

0
dτv(τ)
⎞
⎠exp
⎡
⎣−
t

0
dτq
⎛
⎝r +
t

τ
dτ ′v(τ ′), τ
⎞
⎠
⎤
⎦
⎫
⎬
⎭
v=0
.
For the Poisson random process of the type of shot noise
z(t) =
∞
&
i=1
ξiδ(t −ti),
we have the functional

254
8
General Approaches to Analyzing Stochastic Dynamic Systems
˙Θt[t, v(t)] = ν

dξp(ξ)eiξv(t)−1
	
,
and, consequently,
Q(∇, t) = ν

dξ p(ξ)eξ∇−1
	
.
Consequently, the solution to the integro-diﬀerential equation
∂
∂tu(r, t) = −q(r, t)u(r, t) + ν

dξp(ξ)u(r, t + ξ) −νu(r, t),
u(r, 0) = u0(r)
(8.195)
can be represented as the result of averaging the functional φ[ r, t; z(τ)], i.e.,
in the form
u(r, t) =
4
u0
⎛
⎝r +
t

0
dτz(τ)
⎞
⎠exp
⎧
⎨
⎩−
t

0
dτq
⎛
⎝r +
t

τ
dτ ′z(τ ′), τ
⎞
⎠
⎫
⎬
⎭
5
z
,
if function p(ξ) in Eq. (8.195) can be interpreted as the probability density
of random quantity ξ.
If p(ξ) = δ(ξ−r0), then Eq. (8.195) assumes simpler form
∂
∂tu(r, t) = −q(r, t)u(r, t) + ν [u(r + r0, t) −u(r, t)] ,
u(r, 0) = u0(r).
(8.196)
In this case, Eqs. (8.195) and (8.196) have the form of the transfer equation.
If a dynamic system is formulated in terms of partial diﬀerential equations
of orders higher than the ﬁrst and ﬂuctuations of system parameters are delta-
correlated, we can obtain closed equations, but for characteristic functionals
in functional spaces. Consider two examples of such equations.
Parabolic Equation of Quasi-Optics
If we assume that ﬁeld ε(x, R) in the linear parabolic equation (1.153),
page 63
∂
∂xu(x, R) = i
2kΔRu(x, R) + ik
2ε(x, R)u(x, R),
u(0, R) = u0(R)

8.5
Delta-Correlated Fields and Processes
255
is the homogeneous delta-correlated random ﬁeld, then Eq. (8.21) for the
characteristic functional Φ[x; v, v∗] of the solution to this equation assumes
the form
∂
∂xΦ[x; v, v∗] =
.
˙Θx

x;
δ
iδε(x, R′)

ϕ[x; v,v∗]
/
+ i
2k

dR′

v(R′)ΔR′
δ
δv(R′)−v∗(R′)ΔR′
δ
δv∗(R′)
	
Φ[x; v, v∗].
Taking into account Eq. (8.20), we can represent the last equation in the
closed operator form [46,48,49]
∂
∂xΦ[x; v, v∗] = ˙Θx

x,k
2
!
M(R′)

Φ[x; v, v∗)]
+ i
2k

dR′

v(R′)ΔR′
δ
δv(R′)−v∗(R′)ΔR′
δ
δv∗(R′)
	
Φ[x; v, v∗],
(8.197)
where !
M(R′) is given by the formula
!
M(R′) = v(R′)
δ
δv(R′) −v∗(R′)
δ
δv∗(R′)
and functional
˙Θx

x; ψ(ξ, R′)

= d
dx ln
4
exp
⎧
⎨
⎩i
x

0
dξ

dR′ε(ξ, R′)ψ(ξ, R′)
⎫
⎬
⎭
5
is the derivative of the logarithm of the characteristic functional of ﬁled
ε(x, R).
Equation (8.197) yields the equations for the moment functions of ﬁled
u(x, R),
Mm,n(x; R1, · · · , Rm; R′
1, · · · , R′
n) = u(x, R1) · · · u(x, Rm)u∗(x, R′
1) · · · u∗(x, R′
n)
(for m = n, these functions are usually called the coherence functions of order
2n), which are a consequence of the linearity of the initial dynamic equation
(8.13). These equations have the form

256
8
General Approaches to Analyzing Stochastic Dynamic Systems
∂
∂xMm,n = i
2k
7 m
&
p=1
ΔRp −
n
&
q=1
ΔR′
q
8
Mm,n
+ ˙Θx

x, 1
k
7 m
&
p=1
δ(R′ −Rp) −
n
&
q=1
δ(R′ −R′
q)
8
Mm,n.
(8.198)
If we assume now that ε(x, R) is the homogeneous Gaussian delta-
correlated ﬁeld with the correlation function
Bε(x, R) = A(R)δ(x),
A(R) =
∞

−∞
dxBε(x, R),
so that functional Θ

x; ψ(ξ, R′)

has the form
Θ

x; ψ(ξ, R′)

= −1
2
x

0
dξ

dR′

dRA(R′ −R)ψ(ξ, R′)ψ(ξ, R),
then Eq. (8.197) assumes the closed operator form [46,48,49,122,129]
∂
∂xΦ[x; v, v∗] = −k2
8

dR′

dR A(R′ −R)!
M(R′)!
M(R)Φ[x; v, v∗]
+ i
2k

dR′

v(R′)ΔR′
δ
δv(R′)−v∗(R′)ΔR′
δ
δv∗(R′)
	
Φ[x; v, v∗],
(8.199)
and Eqs. (8.198) for the moment functions of waveﬁeld u(x, R) assume the
form
∂
∂x Mm,n =
i
2k
⎛
⎝
m
%
p=1
ΔRp −
n
%
q=1
ΔR′q
⎞
⎠Mm,n −k2
8 Q(R1, . . . , Rm; R′
1, . . . , R′
m)Mm,n,
(8.200)
where
Q(R1, · · · , Rm; R′
1, · · · , R′
m)
=
m
&
i=1
m
&
j=1
A(Ri −Rj)−2
m
&
i=1
n
&
j=1
A(Ri −R′
j)+
n
&
i=1
n
&
j=1
A(R′
i −R′
j).
(8.201)
Remark 8.2. Another Derivation of Eqs. (8.198) and (8.200).

8.5
Delta-Correlated Fields and Processes
257
In the case of the delta-correlated ﬂuctuations of medium parameters, there
is another, physically clearer way of deriving Eqs. (8.198) and (8.200) for the
moment functions of waveﬁeld u(x, R) [45,46].
As was mentioned earlier, ﬁeld u(x, R) depends functionally only on the
preceding values of ﬁeld ε(ξ, R′), i.e., for ξ ⩽x. However, in the general
case, there is statistical relationship between ﬁeld u(x, R) and subsequent
values of ﬁeld ε(ξ, R′) for ξ ⩾x. In the approximation of the delta-correlated
ﬂuctuations of medium parameters, this statistical relationship disappears,
and ﬁelds u(ξi, R) for ξi < x are independent of ε(ηj, R′) for ηj > x not
only functionally, but also statistically; i.e., for ξi < x; ηj > x, the following
equality holds:
4?
i,j
u(ξi, Ri)ε(ηj, Rj)
5
=
4?
i
u(ξi, Ri)
5 4?
j
ε(ηj, Rj)
5
.
(8.202)
Using Eq. (8.202), we can easily obtain the equations for statistical mo-
ments of ﬁeld u(x, R). We derive the equation for average ﬁeld ⟨u(x, R)⟩as
an example. With this goal in view, we rewrite the initial stochastic equation
(8.13) in the form of the integral equation
u(x, R) = u0(R) exp
⎧
⎨
⎩ik
2
x

0
dξε(ξ, R)
⎫
⎬
⎭
+ i
2k
x

0
dξ exp
⎧
⎪
⎨
⎪
⎩
ik
2
x

ξ
dηε(η, R)
⎫
⎪
⎬
⎪
⎭
ΔRu(ξ, R).
(8.203)
Averaging Eq. (8.203) over an ensemble of realizations of random ﬁeld ε(ξ, R),
we take into account Eq. (8.202) to obtain the closed integral equation
⟨u(x, R)⟩= u0(R)
4
exp
⎧
⎨
⎩ik
2
x

0
dξε(ξ, R)
⎫
⎬
⎭
5
+ i
2k
x

0
dξ
4
exp
⎧
⎪
⎨
⎪
⎩
ik
2
x

ξ
dηε(η, R)
⎫
⎪
⎬
⎪
⎭
5
ΔR ⟨u(ξ, R)⟩.
(8.204)
To transform the integral equation into the diﬀerential equation, we use the
fact that the equality

exp
⎧
⎨
⎩i k
2
x

0
dξε(ξ, R)
⎫
⎬
⎭

=

exp
⎧
⎪
⎨
⎪
⎩
i k
2
ξ

0
dηε(η, R)
⎫
⎪
⎬
⎪
⎭
 
exp
⎧
⎪
⎨
⎪
⎩
i k
2
x

ξ
dηε(η, R)
⎫
⎪
⎬
⎪
⎭

,

258
8
General Approaches to Analyzing Stochastic Dynamic Systems
holds in the case of the delta-correlated ﬂuctuations of medium parameter
for any point 0 ⩽ξ ⩽x. Thus, introducing function
Φ(x, R) =
4
exp
⎧
⎨
⎩ik
2
x

0
dηε(η, R)
⎫
⎬
⎭
5
,
we can rewrite Eq. (8.204) in the form
⟨u(x, R)⟩= u0(R)Φ(x, R) + i
2k
x

0
dξ Φ(x, R)
Φ(ξ, R) ΔR ⟨u(ξ, R)⟩,
(8.205)
from which easily follows the diﬀerential equation for ⟨u(x, R)⟩
∂
∂x ⟨u(x, R)⟩= i
2kΔR ⟨u(x, R)⟩+ ⟨u(x, R)⟩∂
∂x ln Φ(x, R)
coinciding with Eq. (8.198) for m = 1, n = 0. Equations for the higher-order
moments of ﬁeld u(x, R) can be derived similarly.
♦
Random Forces in Hydrodynamic Turbulence
In the case of the hydrodynamic equation (8.22), page 202 under the assump-
tion that random ﬁeld f(x, t) is homogeneous in space and stationary and
delta-correlated in time, Eq. (8.25), page 202 for the characteristic functional
of the Fourier transform of the velocity ﬁeld
Φ[t; z(k′)] = Φ[t; z] =
,
ϕ[t; z(k′)]
-
=
.
exp

i

dk′z(k′)%u(k′, t)
	/
assumes the form
∂
∂t Φ[t; z] =

˙Θt
 
t;
δ
iδf(κ, t)
!
ϕ[t; z]

−

dkzi(k)
 1
2

dk1

dk2Λi,αβ(k1, k2, k)
δ2
δzα(k1)δzβ(k2) + νk2
δ
δzi(k)Φ[t; z]

,
(8.206)
where
˙Θt [t; ψ(κ, τ)] = d
dt ln
4
exp
⎧
⎨
⎩i
t

0
dτ

dκ %
f(κ, τ)ψ(κ, τ)
⎫
⎬
⎭
5

8.5
Delta-Correlated Fields and Processes
259
is the derivative of the logarithm of the characteristic functional of external
forces %
f(k, t). By virtue of equality
δ
δ %
f(k,t −0)
ϕ[t; z] = iz(k)ϕ[t; z] (see
Eq. (8.24), page 202), we can rewrite Eq. (8.206) in the form of the closed
equation
∂
∂t Φ[t; z] = ˙Θt [t; z(k)] Φ[t; z]
−

dkzi(k)
 1
2

dk1

dk2Λi,αβ(k1, k2, k)
δ2
δzα(k1)δzβ(k2) + νk2
δ
δzi(k)

Φ[t; z].
(8.207)
If we assume now that f(x, t) is the Gaussian random ﬁeld homogeneous
and isotropic in space and stationary in time with the correlation tensor
Bij(x1 −x2, t1 −t2) = ⟨fi(x1, t1)fj(x2, t2)⟩,
then the ﬁeld %
f(k, t) will also be the Gaussian stationary random ﬁeld with
the correlation tensor
2
%fi(k, t + τ) %fj(k′, t)
3
= 1
2Fij(k, τ)δ(k + k),
where Fij(k, τ) is the spatial spectrum of the external force given by the
formula
Fij(k, τ) = 2(2π)3

dxBij(x, τ)e−ikx.
In view of the fact that forces are spatially isotropic, we have Fij(k, τ) =
F(k, τ)Δij(k). As long as ﬁeld %
f(k, t) is delta-correlated in time, we have
F(k, τ) = F(k)δ(τ), so that functional Θ [t; ψ(κ, τ)] is given by the formula
Θ [t; ψ(κ, τ)] = −1
4
t

0
dτ

dκ F(κ)Δij(κ)ψi(κ, τ)ψj(−κ, τ),
and Eq. (8.207) assumes the closed form [114]
∂
∂t Φ[t; z] = −1
4

dkF (k)Δij(k)zi(k)zj(−k)Φ[t; z]
−

dkzi(k)
 1
2

dk1

dk2Λi,αβ(k1, k2, k)
δ2
δzα(k1)δzβ(k2) + νk2
δ
δzi(k)

Φ[t; z].
(8.208)
Equation (8.208) plays the role of the Fokker–Planck equation of the problem
under consideration. The unknown in this equation is the characteristic func-
tional, and this fact distinguishes this equation from the standard equation

260
8
General Approaches to Analyzing Stochastic Dynamic Systems
of this type, where the unknown is the probability density expressed as the
Fourier transform of this functional.
Another distinction consists in the fact that Eq. (8.208) is the diﬀusion
equation in the inﬁnite-dimensional space, because of which it is the varia-
tional diﬀerential equation. The diﬀusion coeﬃcient can be diﬀerent for dif-
ferent wave components; it is given by the spectral tensor of external forces
F(k)Δij(k).

Chapter 9
Stochastic Equations with the
Markovian Fluctuations of Parameters
In the preceding chapter, we dealt with the statistical description of dynamic
systems in terms of the general methods that assumed the knowledge of the
characteristic functional of ﬂuctuating parameters. However, this functional
is unknown in most cases, and we are forced to resort either to assumptions
on the model of parameter ﬂuctuations, or to asymptotic approximations.
The methods based on approximating the ﬂuctuating parameters with
the Markovian random processes and ﬁelds with a ﬁnite temporal correla-
tion radius are widely used. Such approximations can be found, for example,
as solutions to the dynamic equations with delta-correlated ﬂuctuations of
parameters. Consider such methods in greater detail by the examples of the
Markovian random processes [47–49].
9.1
General Remarks
Consider stochastic equations of the form
d
dtx(t) = f (t, x, z(t)) ,
x(0) = x0,
(9.1)
where f (t, x, z(t)) is the deterministic function. The vector process
z(t) = {z1(t), · · · , zn(t)}
is the Markovian vector process whose transition probability density satisﬁes
the equation (see Chapter 3, page 150)
∂
∂tp(z, t|z0, t0) = %L(z)p(z, t|z0, t0).
In this equation, operator %L(z) is called the kinetic operator.
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
261
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_9, c
⃝Springer International Publishing Switzerland 2015

262
9
Stochastic Equations with the Markovian Fluctuations of Parameters
Our task consists in the determination of statistical characteristics of the
solution to Eq. (9.1) from known statistical characteristics of process z(t), for
example, from the kinetic operator %L(z).
In the general case of arbitrary Markovian process z(t), we cannot judge
about process x(t). We can only assert that the joint process {x(t), z(t)}
is the Markovian process. Indeed, as we showed in Chapter 4, page 179 the
following diﬀerentiation formula
d
dt ⟨δ (z(t) −z) R[t; z(τ)]⟩=
.
δ (z(t) −z) d
dtR[t; z(τ)]
/
+%L(z) ⟨δ (z(t) −z) R[t; z(τ)]⟩,
(9.2)
holds for arbitrary functional R[t;z(τ)], τ ≤t if z(t) is the Markovian process.
Multiplying Eq. (9.2) by arbitrary function F(z) and integrating the result
over z, we obtain another representation of the diﬀerentiation formula
d
dt ⟨F (z(t)) R[t; z(τ)]⟩=
.
F (z(t)) d
dtR[t; z(τ)]
/
+
2
R[t; z(τ)]

%L+(z)F (z(t))
3
,
(9.3)
where %L+(z) is the operator conjugated to operator %L(z).
Now, we specify functional R[t;z(τ)] in the form of the indicator function
R[x, t; z(τ)] = δ(x(t) −x),
where x(t) is the solution to Eq. (9.1). In this case, function R[x, t; z(τ)]
satisﬁes the equation
∂
∂tR[x, t; z(τ)] = −∂
∂xi
fi(t, x, z)R[x, t; z(τ)],
which is the stochastic Liouville equation for our problem. Note that the
correlator
⟨δ(z(t) −z)R[x, t; z(τ)]⟩= P(x, z, t)
appears in this case to be the one-time joint probability density of processes
x(t) and z(t). Consequently, the diﬀerentiation formula (9.2) assumes the
form of the closed equation for the one-time probability density
∂
∂tP(x, z, t) = −∂
∂xi
fi(t, x, z)P(x, z, t) + %L(z)P(x, z, t).
(9.4)
It is obvious that the transition probability density of the joint process
{x(t), z(t)} also satisﬁes Eq. (9.4), which means that process {x(t), z(t)} is
the Markovian process. If we would able to solve Eq. (9.4), then we could
integrate the solution over z to obtain the probability density of the solution

9.2
Telegrapher’s Processes
263
to Eq. (9.1), i.e., function P(x, t). In this case, process x(t) would not be the
Markovian process.
There are several types of processes z(t) that allow obtaining equations
for density P(x, t) without solving Eq. (9.4) for P(x, z, t). Among these pro-
cesses, we mention ﬁrst of all the telegrapher’s and generalized telegrapher’s
processes, Markovian processes with ﬁnite number of states, and Gaussian
Markovian processes. Below, we discuss these processes in more detail as
examples of processes widely used in diﬀerent applications.
9.2
Telegrapher’s Processes
Recall that telegrapher’s random process z(t) (the two-state, or binary pro-
cess) is deﬁned by the equality
z(t) = a(−1)n(0,t),
where random quantity a assumes values a = ± a0 with probabilities 1/2
and n(t1, t2), t1 < t2 is the Poisson integer-valued process with mean value
n(t1, t2) = ν|t1 −t2|.
Telegrapher’s process z(t) is stationary in time and its correlation function
⟨z(t)z(t′)⟩= a2
0e−2ν|t−t′|
has the temporal correlation radius τ0 = 1/ (2ν).
For splitting the correlation between telegrapher’s process z(t) and arbit-
rary functional R[t; z(τ)], where τ ≤t, we obtained the relationship (7.36),
page 175
⟨z(t)R[t; z(τ)]⟩= a2
0
t

0
dt1e−2ν(t−t1)
.
δ
δz(t1)
R[t; z(τ)]
/
,
(9.5)
where functional R[t; z(τ)] is given by the formula
R[t, t1; z(τ)] = R[t; z(τ)θ(t1 −τ + 0)]
(t1 < t).
(9.6)
Formula (9.5) is appropriate for analyzing stochastic equations linear in
process z(t). Let functional R[t; z(τ)] is the solution to a system of diﬀe-
rential equations of the ﬁrst order in time with initial values at t = 0.
Functional R[t, t1; z(τ)] will also satisfy the same system of equations with
product z(t)θ(t1 −t) instead of z(t). Consequently, we obtain that functional
R[t, t1; z(τ)] = R[t; 0] for all times t > t1; moreover, it satisﬁes the same

264
9
Stochastic Equations with the Markovian Fluctuations of Parameters
system of equations for absent ﬂuctuations (i.e., at z(t) = 0) with the initial
value R[t1, t1; z(τ)] = R[t1; z(τ)].
Another formula convenient in the context of stochastic equations linear
in random telegrapher’s process z(t) concerns the diﬀerentiation of the cor-
relation of this process with arbitrary functional R[t; z(τ)] (τ ≤t) (7.42),
page 176
d
dt ⟨z(t)R[t; z(τ)]⟩= −2ν ⟨z(t)R[t; z(τ)]⟩+
.
z(t) d
dtR[t; z(τ)]
/
.
(9.7)
In addition, we have the equality
⟨z(t′)R[t; z(τ)]⟩= e−2ν|t−t′| ⟨z(t)R[t; z(τ)]⟩,
t′ ⩾t,
τ ⩽t.
(9.8)
Formula (9.7) determines the rule of factoring the diﬀerentiation operation
out of averaging brackets
.
z(t) dn
dtn R[t; z(τ)]
/
=
 d
dt + 2ν
n
⟨z(t)R[t; z(τ)]⟩.
(9.9)
We consider some special examples to show the usability of these formulas.
It is evident that both methods give the same result. However, the method
based on the diﬀerentiation formula appears to be more practicable.
9.2.1
System of Linear Operator Equations
The ﬁrst example concerns the system of linear operator equations
d
dtx(t) = %A(t)x(t) + z(t) %B(t)x(t),
x(0) = x0,
(9.10)
where %A(t) and %B(t) are certain diﬀerential operators (they may include dif-
ferential operators with respect to auxiliary variables). If operators %A(t) and
%B(t) are matrixes, then Eqs. (9.10) describe the linear dynamic system.
Average Eqs. (9.10) over an ensemble of random functions z(t). The result
will be the vector equation
d
dt ⟨x(t)⟩= %A(t) ⟨x(t)⟩+ %B(t)ψ(t),
⟨x(0)⟩= x0,
(9.11)
where we introduced new functions
ψ(t) = ⟨z(t)x(t)⟩.

9.2
Telegrapher’s Processes
265
We can use formula (9.7) for these functions; as a result, we obtain the
equality
d
dtψ(t) = −2νψ(t) +
.
z(t) d
dtx(t)
/
.
(9.12)
Substituting now derivative dx/dt (9.10) in Eq. (9.12), we obtain the equation
for the vector function ψ(t)
 d
dt + 2ν

ψ(t) = %A(t)ψ(t) + %B(t)
,
z2(t)x(t)
-
.
(9.13)
Because z2(t) ≡a2
0 for telegrapher’s process, we obtain ﬁnally the closed
system of linear equations for vectors ⟨x(t)⟩and ψ(t)
d
dt ⟨x(t)⟩= %A(t) ⟨x(t)⟩+ %B(t)ψ(t),
⟨x(0)⟩= x0,
 d
dt + 2ν

ψ(t) = %A(t)ψ(t) + a2
0 %B(t) ⟨x(t)⟩,
ψ(0) = (0).
(9.14)
If operators %A(t) and %B(t) are the time-independent matrixes A and B,
we can solve system (9.14) using the Laplace transform. After the Laplace
transform, system (9.14) becomes the algebraic system of equations
(pE −A) ⟨x⟩p −Bψp = x0,
[(p + 2ν) E −A]ψp −a2
0B ⟨x⟩p = 0,
(9.15)
where E is the unit matrix. From this system, we obtain solution ⟨x⟩p in the
form
⟨x⟩p =

(pE −A) −a2
0B
1
(p + 2ν)E −AB
−1
x0.
(9.16)
Stochastic Parametric Resonance
Consider the problem of statistical description of an oscillator with ﬂuctu-
ating frequency as a simple example of the linear dynamic system (9.10).
This problem is formulated as the second-order equation (1.32), page 22 with
initial values
d2
dt2 x(t) + ω2
0[1 + z(t)]x(t) = 0,
x(0) = x0,
d
dtx(t)

t=0
= y0,
(9.17)
which is equivalent to the system of equations

266
9
Stochastic Equations with the Markovian Fluctuations of Parameters
d
dtx(t) = y(t),
d
dty(t) = −ω2
0[1 + z(t)]x(t),
x(0) = x0,
y(0) = y0.
(9.18)
If our interest concerns only the average value of the solution to statistical
problem (9.17), we can deal without rewriting it in the form of the system of
equations (9.18). Averaging Eq. (9.17) over an ensemble of realizations z(t),
we obtain the unclosed equation
 d2
dt2 + ω2
0

⟨x(t)⟩+ ω2
0 ⟨z(t)x(t)⟩= 0.
(9.19)
To split the correlator in the right-hand side of Eq. (9.19), we multiply
Eq. (9.17) by function z(t) and average the result to obtain the equation
.
z(t)
 d2
dt2 + ω2
0

x(t)
/
+ ω2
0a2
0 ⟨x(t)⟩= 0.
(9.20)
Deriving Eq. (9.20), we took into account that quantity z2(t) ≡a2
0 is not
random in the case of telegrapher’s process.
Then, we use the rule of factoring the derivative out of averaging brackets
(9.9), page 264 to rewrite Eq. (9.20) in the form
 d
dt + 2ν
2
+ ω2
0

⟨z(t)x(t)⟩+ ω2
0a2
0 ⟨x(t)⟩= 0.
(9.21)
Now, Eqs. (9.19) and (9.21) form the closed system of equations.
From Eq. (9.21), we obtain
⟨z(t)x(t)⟩= ω0a2
0
t

0
dt′e−2ν(t−t′) sin ω0(t −t′) ⟨x(t′)⟩.
Consequently, Eq. (9.19) can be represented in the form of the integro-
diﬀerential equation
 d2
dt2 + ω2
0

⟨x(t)⟩+ ω3
0a2
0
t

0
dt′e−2ν(t−t′) sin ω0(t −t′) ⟨x(t′)⟩= 0.
(9.22)
We can again use the Laplace transform to solve either the system of equa-
tions (9.19) and (9.21) or Eq. (9.22); in both cases, the solution has the form

9.2
Telegrapher’s Processes
267
⟨x⟩p = F(p)
L(p + 2ν)
L(p)L(p + 2ν) −ω4
0a2
0
= F(p)
1
p2 + ω2
0 −
ω4
0a2
0
(p + 2ν)2 + ω2
0
,
(9.23)
where
F(p) = px0 + y0,
L(p) = p2 + ω2
0.
Under the conditions
ω0 ≪2ν,
ω2
0a2
0
4ν2 ≪1,
solution (9.23) grades into the Laplace transform of Eq. (8.174), i.e., corre-
sponds to the Gaussian random process z(t) delta-correlated in time.
Consider now the problem on the second moments of the solution to
Eq. (9.17). Here, the use of system of equations (9.18) appears necessary.
In a way similar to the above derivation of the system of equations (9.19)
and (9.21), we obtain the system of six equations for second moments
d
dt
,
x2(t)
-
= 2 ⟨x(t)y(t)⟩,
d
dt ⟨x(t)y(t)⟩=
,
y2(t)
-
−ω2
0
,
x2(t)
-
−ω2
0
,
z(t)x2(t)
-
,
d
dt
,
y2(t)
-
= −2ω2
0 ⟨x(t)y(t)⟩−2ω2
0 ⟨z(t)x(t)y(t)⟩,
 d
dt + 2ν
 ,
z(t)x2(t)
-
= 2 ⟨z(t)x(t)y(t)⟩,
 d
dt + 2ν

⟨z(t)x(t)y(t)⟩=
,
z(t)y2(t)
-
−ω2
0
,
z(t)x2(t)
-
−a2
0
,
x2(t)
-
,
 d
dt + 2ν
 ,
z(t)y2(t)
-
= −2ω2
0

⟨z(t)x(t)y(t)⟩−a2
0 ⟨x(t)y(t)⟩

.
(9.24)
System of equations (9.24) allows one to obtain closed systems for every
unknown function
,
x2(t)
-
, ⟨x(t)y(t)⟩, and
,
y2(t)
-
. For example, the average
value of the potential energy ⟨U(t)⟩, where U(t) = x2(t), satisﬁes the closed
system of two equations (every of which is the third-order equation)

268
9
Stochastic Equations with the Markovian Fluctuations of Parameters
d3
dt3 ⟨U(t)⟩+ 4ω2
0
d
dt ⟨U(t)⟩+ 4ω2
0
 d
dt + ν

⟨z(t)U(t)⟩= 0,
 d
dt + 2ν
 ' d
dt + 2ν
2
+ 4ω2
0
(
⟨z(t)U(t)⟩+ 4ω2
0a2
0
 d
dt + ν

⟨U(t)⟩= 0.
(9.25)
It is clear that we could obtain system (9.25) without deriving the com-
plete system of equations (9.24). Indeed, random quantity U(t) satisﬁes the
stochastic third-order equation (8.177)
d3
dt3 U(t) + 4ω2
0
d
dtU(t) + 2ω2
0

z(t) d
dtU(t) + d
dtz(t)U(t)

= 0
(9.26)
with the initial value that can generally depend on process z(t) and its deriva-
tives. Averaging Eq. (9.26) over an ensemble of random process realizations
and using rule (9.9), page 264 to factor the derivative out of averaging brack-
ets, we obtain the ﬁrst equation of system (9.25). Then, multiplying Eq. (9.26)
by z(t) and using again the rule (9.9), we obtain the second equation of sys-
tem (9.25).
Systems of equations (9.24) and (9.25) can be solved using the Laplace
transform. For example, in the case of the conditions x(0) = x0, y(0) = y0,
we have
U(0) = x2
0,
d
dtU(t)

t=0
= 0,
d2
dt2 U(t)

t=0
= 2y2
0,
(9.27)
and we obtain the solution of Eqs. (9.25) in the form
⟨U⟩p = 2y2
0
L(p + 2ν)
L(p)L(p + 2ν) −a2
0M 2(p),
L(p) = p

p2 + 4ω2
0

,
M(p) = 4ω2
0

p2 + ν

.
(9.28)
In the limiting case of great parameters ν and a2
0, but ﬁnite ratio a2
0/2ν =
σ2τ0, we obtain from the second equation of system (9.25)
⟨z(t)U(t)⟩= −ω2
0σ2τ0
ν
⟨U(t)⟩.
Consequently, average potential energy ⟨U(t)⟩satisﬁes in this limiting case
the closed third-order equation
d3
dt3 ⟨U(t)⟩+ 4ω2
0
d
dt ⟨U(t)⟩−4ω4
0σ2τ0 ⟨U(t)⟩= 0,
which coincides with Eq. (8.176) and corresponds to the Gaussian delta-
correlated process z(t).

9.2
Telegrapher’s Processes
269
The system of equations for correlation functions ⟨x(t)x(t′)⟩and ⟨y(t)x(t′)⟩
for t > t′ can be obtained in a way similar to the derivation of Eqs. (9.24); it
has, obviously, the form
d
dt ⟨x(t)x(t′)⟩= ⟨y(t)x(t′)⟩,
d
dt ⟨y(t)x(t′)⟩= −ω2
0 ⟨x(t)x(t′)⟩−ω2
0 ⟨z(t)x(t)x(t′)⟩,
 d
dt + 2ν

⟨z(t)x(t)x(t′)⟩= ⟨z(t)y(t)x(t′)⟩,
 d
dt + 2ν

⟨z(t)y(t)x(t′)⟩= −ω2
0 ⟨z(t)x(t)x(t′)⟩−ω2
0a2
0 ⟨x(t)x(t′)⟩.
The initial values for this system are obtained as the solution to system (9.24)
at t = t′. In a similar way, one can derive the second pair of equations for
correlation functions ⟨x(t)y(t′)⟩, ⟨y(t)y(t′)⟩for t > t′. In the limit ν →∞,
a2
0 →∞, but ﬁnite ratio a2
0/2ν = σ2τ0, we revert to systems of equations
(8.167) and (8.169), which correspond to the Gaussian delta-correlated pro-
cess z(t).
9.2.2
One-Dimensional Nonlinear Diﬀerential
Equation
Consider now the nonlinear one-dimensional equation
d
dtx(t) = f(x, t) + z(t)g(x, t),
x(0) = x0.
(9.29)
In this case, the indicator function ϕ(x, t) = δ(x(t)−x) satisﬁes the stochastic
Liouville equation
∂
∂tϕ(x, t) = −∂
∂xf(x, t)ϕ(x, t) −z(t) ∂
∂xg(x, t)ϕ(x, t).
(9.30)
Averaging Eq. (9.30) over an ensemble of realizations of functions z(t)
yields the equation for the probability density of solutions to Eq. (9.29)
P(x, t) = ⟨ϕ(x, t)⟩in the form
∂
∂tP(x, t) = −∂
∂xf(x, t)P(x, t) −∂
∂xg(x, t)Ψ(x, t),
(9.31)
where we introduced new function

270
9
Stochastic Equations with the Markovian Fluctuations of Parameters
Ψ(x, t) = ⟨z(t)ϕ(x, t)⟩.
Since solution to Eq. (9.30) is a functional of process z(t), we can apply
formula (9.7), page 264 to Ψ(x, t) to obtain the equality
 d
dt+2ν

Ψ(x, t) =
.
z(t) ∂
∂tϕ(x, t)
/
.
(9.32)
Substitution of the right-hand side of Eq. (9.30) in Eq (9.32) yields the equa-
tion
 d
dt + 2ν

Ψ(x, t) = −∂
∂xf(x, t)Ψ(x, t) −∂
∂xg(x, t)
,
z2(t)ϕ(x, t)
-
,
(9.33)
and we obtain the closed system of equations
∂
∂tP(x, t) = −∂
∂xf(x, t)P(x, t) −∂
∂xg(x, t)Ψ(x, t),
 d
dt + 2ν

Ψ(x, t) = −∂
∂xf(x, t)Ψ(x, t) −a2
0
∂
∂xg(x, t)P(x, t).
(9.34)
If functions f(x, t) and g(x, t) are independent of time, the steady-state
probability distribution satisﬁes (if it exists) the equations
f(x)P(x) = −g(x)Ψ(x),

2ν + d
dxf(x)

Ψ(x) = −a2
0
d
dxg(x)P(x).
(9.35)
Eliminating function Ψ(x), we obtain the ﬁrst-order diﬀerential equation [47–
49]

2ν + d
dxf(x)
 f(x)
g(x) P(x) = a2
0
d
dxg(x)P(x),
whose solution can be represented in the form of the quadrature (|f(x)| <
a0|g(x)|)
P(x) =
C|g(x)|
a2
0g2(x) −f 2(x) exp
2ν
a2
0

dx
f(x)
a2
0g2(x) −f 2(x)
	
,
(9.36)
where the positive constant C is determined from the normalization condi-
tion.
Note that, in the limit ν →∞and a2
0 →∞under the condition that
a2
0τ0 = const (τ0 = 1/ (2ν)), probability distribution (9.36) grades into the
expression
P(x) =
C
|g(x)| exp
2ν
a2
0

dx f(x)
g2(x)
	

9.2
Telegrapher’s Processes
271
corresponding to the Gaussian delta-correlated process z(t), i.e., the Gaussian
process with the correlation function
⟨z(t)z(t′)⟩= 2a2
0τ0δ(t −t′).
To obtain an idea of system dynamics under the condition of ﬁnite corre-
lation radius of process z(t), we consider the simple example with g(x) = 1,
f(x) = −x and a0 = 1. In this case, we obtain from Eq. (9.36) the probability
distribution
P(x) =
1
B(ν, 1/2)(1 −x2)ν−1
(|x| < 1),
(9.37)
where B(ν, 1/2) is the beta-function. This distribution has essentially diﬀer-
ent behaviors for ν > 1, ν = 1 and ν < 1, which is schematically shown in
Fig. 9.1.
x
x
x
P(x)
P(x)
P(x)
ν > 1
ν = 1
ν < 1
−1
−1
−1
1
1
1
Fig. 9.1 Steady-state probability distribution (9.37) of the solution to the equation
d
dtx(t) = −x + z(t) versus parameter ν
One can easily see that this system resides mainly near state x = 0 if
ν > 1, and near states x = ±1 if ν < 1. In the case ν = 1, we obtain the
uniform probability distribution on segment [−1, 1].
9.2.3
Particle in the One-Dimensional Potential Field
Another example of nonlinear system concerns the one-dimensional motion
of a particle in the ﬁled U(x) under the condition that random forces have a
ﬁnite temporal correlation radius. We will describe the motion of the particle
by the stochastic system of equations
d
dtx(t) = y(t),
d
dty(t) = −dU(x)
dx
−λy(t) + μz(t),
(9.38)
where function z(t) is assumed to be telegrapher’s process (z2(t) = 1). Simi-
larly to the derivation of Eq. (9.34), we obtain the operator equation for the

272
9
Stochastic Equations with the Markovian Fluctuations of Parameters
steady-state joint probability density of particle coordinate x and velocity y
%L(x, y)P(x, y) = μ2 ∂
∂y
1
2ν + %L(x, y)
∂
∂yP(x, y),
(9.39)
where %L(x, y) is the Liouville operator,
%L(x, y) = y ∂
∂x −dU(x)
dx
∂
∂y −λ ∂
∂y y.
For ν →∞, Eq. (9.39) grades into the steady-state Fokker–Planck equation
%L(x, y)P(x, y) = μ2
2ν
∂2
∂y2 P(x, y),
whose solution is the Gibbs distribution
P(x, y) = C exp

−β
y2
2 + U(x)
	

β = 2νλ
μ2

.
(9.40)
But in the general case, Eq. (9.39) describes the deformation of distribution
(9.40) because of ﬁnite correlation time τ0 = 1/ (2ν) of process z(t). Equation
(9.39) can be rewritten in the form of the partial diﬀerential equation

2ν + %L
2 %L −λ

2ν + %L

%L
	
P(x, y)
+

−μ2 
2ν + %L
 ∂2
∂y2 + μ2 ∂2
∂x∂y + ∂2U(x)
∂x2
%L
	
P(x, y) = 0.
(9.41)
Deriving Eq. (9.41), we used the diﬀerentiation formula for the inverse oper-
ator %L−1(α)
∂
∂α
%L−1(α) = −%L−1(α)∂%L(α)
∂α
%L−1(α).
Equation (9.41) is rather complicated, and it hardly can be solved for
arbitrary ﬁeld U(x). However, one can easily see that solution to Eq. (9.41)
will not be a function of sole particle energy as it is the case in Eq. (9.40);
in addition, particle coordinate and velocity will be statistically dependent
quantities.
9.2.4
Ordinary Diﬀerential Equation of the n-th
Order
Let now operators %A(t) and %B(t) in Eq. (9.10) be the matrices, i.e.,

9.2
Telegrapher’s Processes
273
d
dtx(t) = A(t)x(t) + z(t)B(t)x(t),
x(0) = x0.
If only one component is of our interest, we can obtain for it the operator
equation
%L
 d
dt

x(t) +
n−1
&
i+j=0
bij(t) di
dti z(t) dj
dtj x(t) = f(t),
(9.42)
where operator
%L
 d
dt

=
n
&
i=0
ai(t) di
dti
and n is the order of matrices A and B in Eq. (9.10).
The initial values for x are included in function f(t) through the corres-
ponding derivatives of the delta function. Note that function f(t) can depend
additionally on derivatives of random process z(t) at t = 0, i.e., f(t) is also
the random function statistically related to process z(t).
Averaging Eq. (9.42) over an ensemble of realizations of process z(t) with
the use of formula (9.9), we obtain the equation
%L
 d
dt

⟨x(t)⟩+ M
 d
dt, d
dt + 2ν

⟨z(t)x(t)⟩= ⟨f(t)⟩,
(9.43)
where
M [p, q] =
n−1
&
i+j=0
bij(t)piqj.
However, Eq. (9.43) is unclosed because of the presence of correlator
⟨z(t)x(t)⟩. Multiplying Eq. (9.42) by z(t) and averaging the result, we ob-
tain the equation for correlator ⟨z(t)x(t)⟩
%L
 d
dt + 2ν

⟨z(t)x(t)⟩+ a2
0M
 d
dt + 2ν, d
dt

⟨x(t)⟩= ⟨z(t)f(t)⟩.
(9.44)
System of equations (9.43) and (9.44) is the closed system. If functions
ai(t) and bij(t) are independent of time, this system can be solved using the
Laplace transform. This solution is as follows:
⟨x⟩p =
%L(p + 2ν) ⟨f⟩p −M[p, p + 2ν] ⟨zf⟩p
%L(p)%L(p + 2ν) −a2
0M[p + 2ν, p]M[p, p + 2ν]
.
(9.45)
Note that Eq. (9.26) considered earlier is a special case of Eq. (9.42) and,
consequently, Eq. (9.28) is a special case of Eq. (9.45).

274
9
Stochastic Equations with the Markovian Fluctuations of Parameters
9.2.5
Statistical Interpretation of Telegrapher’s
Equation
In the preceding chapter, page 252, we showed that solutions to certain class
of partial diﬀerential equations can be treated as the result of averaging
certain functional over the random process delta-correlated in time. A similar
situation occurs for telegrapher’s random process.
Consider the initial-value problem for the wave equation with linear friction
 ∂2
∂t2 + 2ν ∂
∂t −v2 ∂2
∂x2

F(x, t) = 0,
F(x, 0) = ϕ(x),
∂
∂tF(x, t)

t=0
= ψ(x).
(9.46)
We can rewrite Eq. (9.46) as the integro-diﬀerential equation
∂
∂tF(x, t) = ψ(x)e−2νt + v2
t

0
dt1e−2ν(t−t1) ∂2
∂x2 F(x, t).
(9.47)
Introduce now the auxiliary stochastic equation
∂
∂tf(x, t) = ψ(x)e−2νt + vz(t) ∂
∂xf(x, t),
f(x, 0) = ϕ(x),
(9.48)
where z(t) is telegrapher’s process (z2 = 1). From the above material obvi-
ously follows that
F(x, t) = ⟨f(x, t)⟩z .
The solution to Eq. (9.48) has the form
f(x, t) = ϕ
⎛
⎝x + v
t

0
dτz(τ)
⎞
⎠+
t

0
dt1e−2νt1ψ
⎛
⎝x + v
t

t1
dτz(τ)
⎞
⎠.
Consequently, the solution to Eq. (9.46) can be represented as the statistical
average over random process z(t):
F(x, t) =
4
ϕ
⎛
⎝x + v
t

0
dτz(τ)
⎞
⎠
5
z
+
t

0
dt1e−2νt1
4
ψ
⎛
⎝x + v
t

t1
dτz(τ)
⎞
⎠
5
z
.

9.3
Generalized Telegrapher’s Process
275
9.3
Generalized Telegrapher’s Process
Generalized telegrapher’s process is deﬁned by the formula
z(t) = an(0,t),
(9.49)
where n(t1, t2), t1 < t2 is the integer-valued Poisson random process sta-
tistically independent of random quantities ai, which are also statistically
independent and have probability density p(a).
Generalized telegrapher’s process z(t) is stationary in time and its corre-
lation function
⟨z(t)z(t′)⟩=
,
a2-
e−ν|t−t′|
is characterized by the temporal correlation radius τ0 = 1/ν.
As in the case of telegrapher’s process, two alternative methods are appro-
priate for analyzing stochastic equations whose parameter ﬂuctuations can
be described by generalized telegrapher’s process.
The ﬁrst method immediately deals with the formula (7.45), page 178 for
splitting the correlation of process z(t) with arbitrary functional R[t; z(τ)] of
this process,
⟨z(t)R[t; z(τ)]⟩= ⟨aR[t; a]⟩e−υt + a2
0
t

0
dt1e−υ(t−t1) 2
a R[t, t1; a, z(τ)]
3
,
(9.50)
where functional R[t, t1; a, z(τ)] is given by the formula
R[t, t1; a, z(τ)] = R[t; aθ(τ −t1 + 0) + z(τ)θ(t1 −τ + 0)],
(9.51)
and random quantity a is statistically independent of process z(t).
In contrast to telegrapher’s process, the second method appears here more
formal and deals with the diﬀerentiation formula (7.52), (7.54), page 180 that
has the form
 d
dt + ν

⟨F (z(t)) R[t; z(τ)]⟩=
.
F (z(t)) d
dtR[t; z(τ)]
/
,
(τ ≤t)
(9.52)
under the condition that ⟨F(z(t))⟩= ⟨F(a)⟩a = 0. In particular, we have the
formula
 d
dt + ν
n
⟨F (z(t)) R[t; z(τ)]⟩=
.
F (z(t)) dn
dtn R[t; z(τ)]
/
(9.53)
deﬁning the rule of factoring the diﬀerential operator out of averaging brack-
ets.
The further analysis becomes simpler if we deﬁne function F(z(t)) as
follows:

276
9
Stochastic Equations with the Markovian Fluctuations of Parameters
F(t) = F(z(t)) =
1
1+λz(t) −C0(λ),
(9.54)
where
Ck(λ) =
.
ak
1+λa
/
a
(9.55)
and λ is arbitrary parameter. This function F(t) satisﬁes the identity
z(t)F(t) = −1
λF(t) + C1(t) −z(t)C0(λ).
(9.56)
Now, we consider several examples of working according to the above for-
malisms.
9.3.1
Stochastic Linear Equation
First of all, we consider, as in the previous section, the stochastic linear
equation (9.10), page 264 assuming that linear operators %A(t) and %B(t) in
this equation are constant matrices A and B. In this case, the equation for
the mean value ⟨x(t)⟩is
d
dt ⟨x(t)⟩= A ⟨x(t)⟩+ B ⟨z(t)x(t)⟩.
(9.57)
Using Eq. (9.50), we can rewrite this equation in the form
d
dt ⟨x(t)⟩= A ⟨x(t)⟩+B ⟨ax[t; a]⟩e−νt+νB
t

0
dt1e−ν(t−t1) ⟨ax[t, t1; a, z(τ)]⟩.
(9.58)
According to Eq. (9.51), functional x[t, t1; a, z(τ)] satisﬁes the equation
d
dtx(t) = Ax(t) + aBx(t)
(t > t1)
(9.59)
with the initial value
x[t1, t1; a, z(τ)] = x[t1; z(τ)].
(9.60)
Hence, functional x[t, t1; a, z(τ)] has the form
x[t, t1; a, z(τ)] = e(A+aB)(t−t1)x(t1),
and Eq. (9.58) turns into the integro-diﬀerential equation

9.3
Generalized Telegrapher’s Process
277
d
dt ⟨x(t)⟩= A ⟨x(t)⟩+ e−νtB
2
ae(A+aB)t3
x0
+ νB
t

0
dt1e−ν(t−t1) 2
ae(A+aB)(t−t1)3
⟨x(t1)⟩
(9.61)
with the initial value ⟨x(0)⟩= x0.
We can easily solve Eq. (9.61) using the Laplace transform. The solution
has the form
⟨x⟩p = (E −νC)−1Cx0,
(9.62)
where
C =
,
{(p + ν)E −A −aB} −1-
a
and E is the unit matrix.
Use now the alternative method for splitting the correlator in Eq. (9.57).
According to the diﬀerentiation formula (9.52), we have
 d
dt + ν

⟨F (t) x(t)⟩=
.
F (t) d
dtx(t)
/
= A ⟨F(t)x(t)⟩+ B ⟨z(t)F(t)x(t)⟩.
(9.63)
Using then Eq. (9.56), we can rewrite Eq. (9.63) as the identity
 d
dt + ν

E −A + 1
λB

⟨F (t) x(t)⟩= BC1(λ) ⟨x(t)⟩−BC0(λ) ⟨z(t)x(t)⟩.
(9.64)
Performing the Laplace transform of Eqs. (9.57) and (9.64), we obtain the
unclosed system of equations
(pE −A) ⟨x⟩p −B ⟨zx⟩p = x0,

(p + ν) E −A + 1
λB

⟨F (t) x(t)⟩p = BC1(λ) ⟨x⟩p −BC0(λ) ⟨zx⟩p ,
(9.65)
which is valid for arbitrary λ. For 1
λ = [A −(p + ν)E]B−1, the left-hand side
of the second equation vanishes, and we obtain the algebraic relationship
between ⟨x⟩p and ⟨zx⟩p; together with the ﬁrst equation of system (9.65),
this relationship yields the solution that coincides with Eq. (9.62).
Stochastic Parametric Resonance
We consider the statistical description of solution to problem (9.17), page 265
as a speciﬁc example. Averaging Eq. (9.17) over an ensemble of realizations
of generalized telegrapher’s process z(t), we obtain the unclosed equation

278
9
Stochastic Equations with the Markovian Fluctuations of Parameters
 d2
dt2 + ω2
0

⟨x(t)⟩+ ω2
0 ⟨z(t)x(t)⟩= 0,
(9.66)
with the initial values ⟨x(0)⟩= x0,
.dx(t)
dt
/
t=0
= y0.
To split the correlator appeared in (9.66), we multiply Eq. (9.17), page 265
by function F (z(t)) and average the result. Using then formula (9.52),
page 275 that deﬁnes the rule of factoring the derivative out of averaging
brackets, we obtain the equation
 d
dt + ν
2
+ ω2
0

⟨F (z(t)) x(t)⟩+ ω2
0 ⟨z(t)F (z(t)) x(t)⟩= 0
(9.67)
with zero-valued initial values ⟨F (z(t)) x(0)⟩= 0,
.
F (z(t)) dx(t)
dt
/
t=0
=
0.
The further analysis becomes simpler if we use function F (z(t)) in form
(9.54), page 276 and rewrite Eq. (9.67) as follows:
+ d
dt + ν
	2
+ ω2
0

1 −1
λ
	,
⟨F (z(t)) x(t)⟩+ ω2
0C1(λ) ⟨x(t)⟩−ω2
0C0(λ) ⟨z(t)x(t)⟩= 0.
(9.68)
Performing the Laplace transform of Eqs. (9.66) and (9.68), we obtain

p2 + ω2
0

⟨x⟩p + ω2
0 ⟨zx⟩p = y0 + px0,

(p + ν)2 + ω2
0

1 −1
λ

⟨Fx⟩p + +ω2
0C1(λ) ⟨x⟩p −ω2
0C0(λ) ⟨zx⟩p = 0.
(9.69)
In Eqs. (9.68) and (9.69), parameter λ is arbitrary parameter. Now, we
specify it as follows:
λ = λp =
ω2
0
L(p + ν),
L(p) = p2 + ω2
0.
(9.70)
In this case, the ﬁrst term in the second equation of system (9.69) vanishes,
and we obtain the relationship between correlator ⟨zx⟩p and average solution
to Eq. (8.161) ⟨x⟩p in the form
⟨zx⟩p = C1(p)
C0(p) ⟨x⟩p ,
(9.71)
where
Ck(p) =
.
ak
L(p + ν) + aω2
0
/
a
.

9.3
Generalized Telegrapher’s Process
279
Substituting Eq. (9.71) in the ﬁrst equation of system (9.69), we obtain
the solution in the form
⟨x⟩p =
y0 + px0
L(p) + ω2
0
C1(p)
C0(p)
.
(9.72)
As was noted earlier, the mean value of the solution to problem (9.17),
page 265 can be obtained with the use of the other — alternative and more
intuitive — method. Using Eq. (9.50), page 275, we can rewrite Eq. (9.66) in
the form
 d2
dt2 + ω2
0

⟨x(t)⟩= −ω2
0 ⟨ax[t; a]⟩a e−νt −νω2
0
t

0
dt1e−ν(t−t1) ⟨ax[t, t1; a, z(τ)]⟩a ,
(9.73)
where functional x[t, t1; a, z(τ)] satisﬁes the equation
 d2
dt2 + ω2
0

x(t) + ω2
0ax(t) = 0
with the initial values
x[t, t1; a, z(τ)]t=t1 = x(t1),
d
dt x[t, t1; a, z(τ)]

t=t1
=
d
dt1
x(t1)
and x[t; a] = x[t, 0; a, z(τ)].
The solution to this equation is as follows
x[t, t1; a, z(τ)] = x(t1) cos

ω0
√
1 + a(t −t1)

+ dx(t1)
dt1
sin

ω0
√1 + a(t −t1)

ω0
√1 + a
,
and, consequently, Eq. (9.28) can be rewritten in the closed form
 d2
dt2 +ω2
0
	
⟨x(t)⟩=−x0ω2
0e−νt 
a cos

ω0
√
1 + at

a−y0ω2
0e−νt

asin ω0
√1 + at
ω0
√1 + a

a
−νω2
0
t

0
dt1e−ν(t−t1) ⟨x(t1)⟩

a cos

ω0
√
1 + a(t −t1)

a
−νω2
0
t

0
dt1e−ν(t−t1) d ⟨x(t1)⟩
dt1

asin ω0
√1 + a(t −t1)
ω0
√1 + a

a
.
(9.74)
Equation (9.74) can be easily solved using the Laplace transform; the result
coincides with Eq. (9.72).

280
9
Stochastic Equations with the Markovian Fluctuations of Parameters
9.3.2
One-Dimensional Nonlinear Diﬀerential
Equation
Consider now the one-dimensional equation (9.29), page 269
d
dtx(t) = f(x) + z(t)g(x),
x(0) = x0
assuming that z(t) is generalized telegrapher’s process and functions f(x) and
g(x) are independent of time. In this case, the indicator function satisﬁes the
Liouville equation (9.30), page 269 that assumes here the form
∂
∂tϕ(x, t) = −∂
∂xf(x)ϕ(x, t) −z(t) ∂
∂xg(x)ϕ(x, t).
(9.75)
Averaging Eq. (9.75), we obtain the equation for the one-time probability
density
∂
∂tP(x, t) + ∂
∂xf(x)P(x, t) = −∂
∂xg(x) ⟨z(t)ϕ(x, t)⟩
= −e−νt ∂
∂xg(x) ⟨aϕ[x, t; a]⟩−ν ∂
∂xg(x)
t

0
dt1e−ν(t−t1) ⟨aϕ[x, t, t1; a, z(τ)]⟩.
(9.76)
Functional ϕ[x, t, t1; a, z(τ)] will satisfy now the equation
∂
∂t ϕ(x, t) = −∂
∂xf(x)ϕ(x, t) −a ∂
∂xg(x)ϕ(x, t)
with the initial value ϕ(x, t1) = ϕ(x, t1). In the operator form, the solution
to this equation will be
ϕ(x, t) = e−(t−t1) ∂
∂x [f(x)+ag(x)]ϕ(x, t1).
Hence, we can rewrite the equation for the probability density (9.76) in the
closed operator form
 ∂
∂t + ∂
∂xf(x)

P(x, t) = −e−νt ∂
∂xg(x)
2
ae−t ∂
∂x [f(x)+ag(x)]3
ϕ(x, 0)
−ν ∂
∂xg(x)
t

0
dt1e−ν(t−t1) 2
ae−(t−t1) ∂
∂x [f(x)+ag(x)]3
P(x, t1).
(9.77)

9.3
Generalized Telegrapher’s Process
281
The steady-state probability distribution (if it exists) satisﬁes the operator
equation
f(x)P(x) = −νg(x)
∞

0
dτe−ντ 2
ae−τ d
dx [f(x)+ag(x)]3
P(x)
that can be rewritten as follows:
f(x)P(x) = −νg(x)
4
a
ν + d
dx[f(x) + ag(x)]
5
P(x).
(9.78)
To convert Eq. (9.78) to the diﬀerential equation, we must specify the proba-
bility distribution of random quantity a.
Assume for example that quantity a is characterized by suﬃciently small
intensity of ﬂuctuations and ⟨a⟩= 0. Then, expanding the operator in the
right-hand side of Eq. (9.78) in powers of a and neglecting all terms higher
than
,
a2-
, we obtain the operator equation
f(x)P(x) = −ν
,
a2-
g(x)
1
ν + d
dxf(x)
d
dx
1
ν + d
dxf(x)
P(x).
(9.79)
If we represent now function P(x) in the form
P(x) =

ν + d
dxf(x)

ψ(x),
then we obtain the second-order diﬀerential equation for function ψ(x)

ν + d
dxf(x)
 f(x)
g(x)

ν + d
dxf(x)

ψ(x) = −ν
,
a2- d
dxg(x)ψ(x).
(9.80)
For ν →∞, we can expand the mean value in the right-hand side of
Eq. (9.78) in powers of 1/ν and obtain the steady-state Fokker–Planck equa-
tion
f(x)P(x) = g(x) ν
⟨a2⟩
d
dxg(x)P(x)
corresponding to the Gaussian delta-correlated process z(t).
9.3.3
Ordinal Diﬀerential Equation of the n-th Order
Consider now Eq. (9.42), page 273

282
9
Stochastic Equations with the Markovian Fluctuations of Parameters
%L
 d
dt

x(t) +
n−1
&
i+j=0
bij(t) di
dti z(t) dj
dtj x(t) = f(t)
with generalized telegrapher’s process z(t). For simplicity, we will assume
that initial values for Eq. (9.42) are independent of z(t) and coeﬃcients ai of
operator %L (9.151) and bij are constants.
Averaging Eq. (9.42) with the use of formula (9.53), page 275, we obtain
the equation
%L
 d
dt

⟨x(t)⟩+ M
 d
dt, d
dt + ν

⟨z(t)x(t)⟩= f(t),
(9.81)
where M [p, q] =
n−1
&
i+j=0
bij(t)piqj, as before.
Consider now correlator ⟨F(t)x(t)⟩, where F(t) is given by Eq. (9.54). In
accordance with the diﬀerentiation formula (9.53), this function satisﬁes the
equation
L
 d
dt + ν
	
⟨F (t)x(t)⟩=

F (t)L
 d
dt
	
x(t)

= −
n−1
%
i+j=0
bij(t)

F (t) di
dti z(t) dj
dtj x(t)

−
n−1
%
i+j=0
bij(t)
 d
dt + ν
	i 
z(t)F (t) dj
dtj x(t)

.
(9.82)
Using now Eq. (9.56), we can rewrite the right-hand side of Eq. (9.82) in
the form
−
n−1
&
i+j=0
bij(t)
 d
dt+ν
i
−1
λ
.
F(t) dj
dtj x(t)
/
+ C1(λ) dj
dtj ⟨x(t)⟩−C0(λ)
.
z(t) dj
dtj x(t)
/
= 1
λM
 d
dt + ν, d
dt + ν

⟨F(t)x(t)⟩
−C1(λ)M
 d
dt + ν, ν

⟨x(t)⟩+ C0(λ)M
 d
dt + ν, d
dt + ν

⟨z(t)x(t)⟩.
(9.83)
Consequently, Eq. (9.82) assumes the form

%L
 d
dt + ν

−1
λM
 d
dt + ν, d
dt + ν
	
⟨F(t)x(t)⟩
= −C1(λ)M
 d
dt + ν, d
dt

⟨x(t)⟩+ C0(λ)M
 d
dt + ν, d
dt + ν

⟨z(t)x(t)⟩
(9.84)

9.4
Gaussian Markovian Processes
283
with the initial value ⟨F(t)x(t)⟩|t=0 = 0.
Perform now the Laplace transform of Eqs. (9.81) and (9.84). As a result,
we obtain the algebraic system of equations
%L (p) ⟨x⟩p + M [p, p + ν] ⟨zx⟩p = f(p),

%L (p + ν) −1
λM [p + ν, p + ν]
	
⟨Fx⟩p
= −C1(λ)M [p + ν, p] ⟨x⟩p + C0(λ)M [p + ν, p + ν] ⟨zx⟩p .
(9.85)
Equations (9.85) are valid for arbitrary λ. If we set
λ = λp = M[p + ν, p + υ]/L(p + ν),
(9.86)
the second equation of system (9.85) becomes the algebraic relationship bet-
ween ⟨zx⟩p and ⟨x⟩p:
⟨z(t)x(t)⟩p =
C1(p)
C0(p)
M [p + ν, p]
M [p + ν, p + ν] ⟨x⟩p ,
(9.87)
where
Ck(p) =
4
ak
%L(p + ν) + aM [p + ν, p + ν]
5
a
.
Substituting (9.87) in the ﬁrst equation of system (9.85), we obtain the
algebraic equation for ⟨x⟩p, whose solution has the form [78]
⟨x⟩p = f(p)

%L(p) + M [p + ν, p] M [p, p + ν]
M [p + ν, p + ν]
C1(p)
C0(p)
−1
.
(9.88)
9.4
Gaussian Markovian Processes
Here, we consider several examples associated with the Gaussian Markovian
processes.
9.4.1
General Remarks
Deﬁne random process z(t) by the formula
z(t) = z1(t) + · · · + zN(t),
(9.89)

284
9
Stochastic Equations with the Markovian Fluctuations of Parameters
where zi(t) are statistically independent telegrapher’s processes with corre-
lation functions
⟨zi(t)zj(t′)⟩= δij
,
z2-
e−α|t−t′|
(α = 2ν).
If we set
,
z2-
= σ2/N, then this process grades for N →∞into the
Gaussian Markovian process with correlation function (see Remark 3.1, page
139)
⟨zi(t)zj(t′)⟩=
,
z2-
e−α|t−t′|.
Thus, process z(t) (9.89) approximates the Gaussian Markovian process in
terms of the Markovian process with a ﬁnite number of states.
It is evident that the diﬀerentiation formula and the rule of factoring the
derivative out of averaging brackets assume for process z(t) the forms
 d
dt + αk

⟨z1(t) · · · zk(t)R[t; z(τ)]⟩=
.
z1(t) · · · zk(t) d
dtR[t; z(τ)]
/
,
.
z1(t) · · · zk(t) dn
dtn R[t; z(τ)]
/
=
 d
dt + αk
n
⟨z1(t) · · · zk(t)R[t; z(τ)]⟩,
(9.90)
where R[t; z(τ)] is arbitrary functional of process z(t) (τ ≤t).
9.4.2
Stochastic Linear Equation
Consider again Eq. (9.10), which we represent here in the form
d
dtx(t) = %A(t)x(t) + [z1(t) + · · · + zN(t)] %B(t)x(t),
x(0) = x0,
(9.91)
and introduce vector-function
Xk(t) = ⟨z1(t) · · · zk(t)x(t)⟩,
k = 1, · · · , N;
X0(t) = ⟨x(t)⟩.
(9.92)
Using formula (9.90) for diﬀerentiating correlations (9.92) and Eq. (9.91),
we obtain the recurrence equation for xk(t) (k = 0, 1, · · · , N)
 d
dt + αk

Xk(t) = 
A(t)Xk(t) +
%
z1(t) · · · zk(t)[z1(t) + · · · + zN(t)] B(t)x(t)
&
= 
A(t)Xk(t) + k
'
z2( B(t)Xk−1(t) + (N −k) B(t)Xk+1(t),
(9.93)
with the initial value
Xk(0) = x0δk,0.
Thus, the mean value of the solution to system (9.91) satisﬁes the closed sys-
tem of (N + 1) vector equations. If operators %A(t), %B(t) are time-independent

9.4
Gaussian Markovian Processes
285
matrices, system (9.93) can be easily solved using the Laplace transform. It is
clear that such a solution will have the form of a ﬁnite segment of continued
fraction. If we set
,
z2-
= σ2/N and proceed to the limit N →∞, then random
process (9.89) will grade, as was mentioned earlier, into the Gaussian Marko-
vian process and solution to system (9.93) will assume the form of the inﬁnite
continued fraction.
9.4.3
Ordinal Diﬀerential Equation of the n-th Order
Consider stochastic equation (9.42), page 273
%L
 d
dt

x(t) +
n−1
&
i+j=0
bij(t) di
dti z(t) dj
dtj x(t) = f(t)
with random process z(t) given by Eq. (9.89) and introduce, as in the previous
example, functions
Xk(t) = ⟨z1(t) · · · zk(t)x(t)⟩,
k = 1, · · · , N;
X0(t) = ⟨x(t)⟩.
To obtain equations for these functions, we multiply Eq. (9.42) by
z1(t) · · · zk(t) and average the result over an ensemble of realizations of zi(t).
Using Eqs. (9.90), we obtain that function Xk(t) satisﬁes the closed system
of recurrence equations
%L
 d
dt + αk

Xk(t) = Fk(t) −k
,
z2-
M
 d
dt + αk, d
dt + α(k −1)

Xk−1(t)
−(N −k)
,
z2-
M
 d
dt + αk, d
dt + α(k + 1)

Xk+1(t),
(9.94)
where
Fk(t) = ⟨z1(t) · · · zk(t)f(t)⟩.
If operator %L and functions bij are independent of time t, the Laplace
transform reduces system (9.94) to the algebraic system
%L (p + αk) Xk(p) = Fk(p) −k
,
z2-
M [p + αk, p + α(k −1)] Xk−1(p)
−(N −k)
,
z2-
M [p + αk, p + α(k + 1)] Xk+1(p).
(9.95)
In the special case of function f(t) independent of zk(t), when Fk(p) =
f(p)δk,0, Eq. (9.95) can be easily solved, and the solution has the form of the
ﬁnite segment of continued fraction
X0(p) = f(p)K0(p),
Kl(p) =
1
Al(p) −Bl(p)Kl+1(p),
(9.96)

286
9
Stochastic Equations with the Markovian Fluctuations of Parameters
where
Al(p) = %L (p + αl) ,
Bl(p) =
,
z2-
(N −l)(l + 1)M [p + αl, p + α(l + 1)] M [p + α(l + 1), p + αl] .
(9.97)
If N = 1, i.e., if we deal with only one telegrapher’s process, the solution
(9.96), (9.97) assumes the form of Eq. (9.45), page 273, which corresponds
to the two-level continued fraction.
If we set
,
z2-
= σ2/N and proceed to the limit N →∞, we obtain
solution ⟨x(p)⟩for the Gaussian Markovian process z(t) in the form of the
inﬁnite continued fraction (9.96) with parameters [78]
Al(p) = %L (p + αl) ,
Bl(p) = σ2(l + 1)M [p + αl, p + α(l + 1)] M [p + α(l + 1), p + αl] .
(9.98)
Stochastic Parametric Resonance
We illustrate the above material by the example of statistical description of
solution to problem (9.17), page
265 for the Gaussian Markovian process
z(t).
We introduce function
Xl(t) = ⟨z1(t) · · · zl(t)x(t)⟩,
(9.99)
where x(t) is the solution to problem (9.17). Multiplying then Eq. (9.17) by
product z1(t) · · · zl(t), averaging the result over an ensemble of realizations
of all processes zi(t), and using Eq. (9.90), we obtain the recurrent equality
%L
 d
dt + αl

Xl(t) + ω2
0
,
z2-
lXl−1 + ω2
0(N −l)Xl+1 = 0, (l = 0, · · · , N),
(9.100)
where
%L
 d
dt + αl

=
 d
dt + αl
2
+ ω2
0.
Equality (9.100) can be considered as the closed system of N equations with
the initial values
X0(0) = 0,
d
dtX0(t)

t=0
= y0.
Performing the Laplace transform, we obtain recurrent algebraic system
of equations
L(p + αl)Xl(p) + ω2
0
,
z2-
Xl−1(p) + ω2
0(N −l)Xl+1(p) = F(p)δl,0,
(9.101)

9.4
Gaussian Markovian Processes
287
where F(p) = y0 + px0. Now, we set
Xl(p) = −ω2
0
,
z2-
lKl(p)Xl−1
(9.102)
for l ̸= 0. Substituting Eq. (9.102) in Eq. (9.101), we obtain the ﬁnite segment
of continued fraction
Kl(p) =
1
L(p + αl) −ω4
0 ⟨z2⟩(N −l)(l + 1)Kl+1(p),
(9.103)
and the solution to problem (8.161), page 244 is
⟨x⟩p = X0(p) = F(p)K0(p).
(9.104)
At N = 1, equality (9.104) grades into Eq. (9.23) for single telegrapher’s
process and corresponds to the two-level continued fraction (9.103).
Setting
,
z2-
= σ2/N and proceeding to the limit N →∞, we obtain
the solution for the Gaussian Markovian process in the form of the inﬁnite
continued fraction (9.104), where
Kl(p) =
1
L(p + αl) −ω4
0σ2(l + 1)Kl+1(p).
(9.105)
The second moments of the solution to problem (8.161), page 244 can be
considered similarly. For example, considering potential energy U(t) = x2(t)
that satisﬁes dynamic equation (9.26) with initial values (9.27), page 268, we
obtain the mean value ⟨U(t)⟩in the form of the ﬁnite segment of continued
fraction (in the case of N telegrapher’s processes)
⟨U⟩p = 2y2
0K0(p),
Kl(p) =
1
Al(p) −Bl(p)Kl+1(p),
(9.106)
where
Al(p) = (p + αl)

(p + αl)2 + 4ω2
0

,
Bl(p) = 4
,
z2-
ω4
0(l + 1)(N −l)

2p + α(2l + 1)2
.
At N = 1, we obtain the solution (9.28), page 268 corresponding to single
telegrapher’s process. Setting
,
z2-
= σ2/N and proceeding to the limit N →
∞, we obtain the solution for the Gaussian Markovian process in the form of
the inﬁnite continued fraction (9.106), where
Al(p) = (p+αl)

(p + αl)2 + 4ω2
0

,
Bl(p) = 4σ2ω4
0(l+1)

2p + α(2l + 1)2
.

288
9
Stochastic Equations with the Markovian Fluctuations of Parameters
9.4.4
The Square of the Gaussian Markovian Process
The ﬁnite-dimensional approximation of the Gaussian Markovian process
(9.89), page 283 is practicable for describing ﬂuctuations of dynamic systems
of the form F(z(t)), where z(t) is the Gaussian Markovian process, too.
For example, for system F (z(t)) = z2(t) −
,
z2(t)
-
, the ﬁnite-dimensional
approximation has the form (see Remark 3.1, page 139)
F (z(t)) =
N
&
i̸=k
zi(t)zk(t).
In this case, the mean value of the solution to system of equations (9.10),
page 264 (we assume that operators %A(t) and %B(t) are matrices)
d
dtx(t) = Ax(t) +
N
&
i̸=k
zi(t)zk(t)Bx(t)
(9.107)
will satisfy the closed system of ([N/2] −1) vector equations in functions
Xn(t) = ⟨z1(t) · · · z2n(t)x(t)⟩,
n = 1, · · · , [N/2];
X0(t) = ⟨x(t)⟩.
Here, [N/2] is the integer part of N/2.
It is obvious that functions Xn(t) satisfy the equations
 d
dt + 2αn

Xn(t) −AXn(t) = B
4
z1(t) · · · z2n(t)
N
&
i̸=k
zi(t)zk(t)x(t)
5
.
(9.108)
The further analysis is similar to the derivation of system of equations
(9.94). Divide the sum over i and k in the right-hand side of Eq. (9.108) into
four regions (Fig. 9.2).
In region (1), both functions zi(t) and zk(t) will be extinguished by the
corresponding functions of product
z1(t) · · · z2n(t).
The number of such terms is 2n(2n −1); consequently, in region (1), the
right-hand side of Eq. (9.108) assumes the form
2n(2n −1)
,
z2-2 BXn−1(t)
In region (2), none of functions zi(t) and zk(t) is extinguished, and we obtain
that the corresponding term in the right-hand side of Eq. (9.108) has the form
(N −2n)(N −2n −1)BXn+1(t)

9.5
Markovian Processes with Finite-Dimensional Phase Space
289
(1)
(2)
(3)
(4)
2n
2n
1
1
2
2
N
N
i
k
Fig. 9.2 Schematic of index division in sum (9.108)
In regions (3) and (4), only one of functions zi(t) and zk(t) is extinguished.
The number of such terms is 4n(N −2n), so that the corresponding term in
the right-hand side of Eq. (9.108) has the form
4n(N −2n)
,
z2-
BXn(t)
As a result, Eq. (9.108) assumes the form of the closed system of recurrence
equations
 d
dt + 2αn

E −A −4n(N −2n)
,
z2-
B

Xn(t)
= 2n(2n −1)
,
z2-2 BXn−1(t) + (N −2n)(N −2n −1)BXn+1(t),
where n = 0, 1, · · · , [N/2]. It is obvious that, for constant matrices A and B,
the solution to this system again has the form of a ﬁnite segment of continued
fraction. The simplest approximations with N = 2 and N = 3 give the closed
systems of only two vector equations.
9.5
Markovian Processes with Finite-Dimensional
Phase Space
All considered processes — telegrapher’s and generalized telegrapher’s pro-
cesses and process z(t) = z1(t) + · · · + zN(t), where zi(t) are statistically
independent telegrapher’s processes, — are the special cases of the Markovian

290
9
Stochastic Equations with the Markovian Fluctuations of Parameters
processes z(t) with a ﬁnite number of states (or with the ﬁnite-dimensional
phase space). We assume that possible values of process z(t) are in the gen-
eral case z1, · · · , zn. As a result, all realizations of process z(t) satisfy the
identity
(z(t) −z1) (z(t) −z2) · · · (z(t) −zn) ≡0,
and, consequently,
zn(t) = (z1 + · · · + zn)zn−1(t) + (−1)n−1z1 · · · zn.
(9.109)
In this case, the mean value of the solution to the system of equations
d
dtx(t) = %A(t)x(t) + z(t) %B(t)x(t),
x(0) = x0
(9.110)
will again satisfy a closed system of equations. Indeed, averaging Eq. (9.110)
and repeatedly using the diﬀerentiation formula (9.3), page 262 for correlators
,
zk(t)x(t)
-
(k = 1, · · · , n −1),
we reach function ⟨zn(t)x(t)⟩at the last step. Because this function is expres-
sed in terms of the functions of preceding steps (see Eq. (9.109)), we obtain
the closed system of vector equations of the n-th order.
9.5.1
Two-State Process
Consider the process with two states z1, z2 and respective transition prob-
abilities ν and μ as an example. In this case, Eq. (9.109) assumes the form
z2(t) = (z1 + z2)z(t) −z1z2.
(9.111)
Averaging Eq. (9.110), we obtain

E d
dt −%A(t)

⟨x(t)⟩= %B(t) ⟨z(t)x(t)⟩,
⟨x(0)⟩= x0.
(9.112)
According to Eq. (9.3), page 262, correlation ⟨z(t)x(t)⟩is given by the formula
d
dt ⟨z(t)x(t)⟩=
.
z(t) d
dtx(t)
/
+
2
x(t)

%L+(z)z(t)
3
,
where the kinetic and conjugated operators are the following matrices (see
page 145)
%L(z) =
 −ν μ
ν −μ

,
%L+(z) =
 −ν ν
μ −μ

.
(9.113)

9.6
Causal Stochastic Integral Equations
291
Because the action of operator %L+(z) on z(t) is representable in the form
%L+(z)z(t) =

−ν ν
μ −μ
 
z1
z2

=

−ν (z1 −z2)
μ (z1 −z2)

= (νz2+μz1)−(ν+μ)z(t),
we can rewrite the equation for correlation ⟨z(t)x(t)⟩as follows

E
 d
dt + μ + ν

−%A(t) −(z1 + z2) %B(t)
	
⟨z(t)x(t)⟩
=
"
(νz2 + μz1)E −z1z2 %B(t)
#
⟨x(t)⟩.
(9.114)
Equations (9.112) and (9.114) form the closed system of two vector equations.
Note that, in the special case of the scalar equation with parameters A = 0
and B(t) = iv(t), the solution to Eq. (9.110) is
x(t) = exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭,
so that the mean value of this solution coincides with the characteristic func-
tional of random process z(t)
Φ[t; v(τ)] = ⟨x(t)⟩=
4
exp
⎧
⎨
⎩i
t

0
dτz(τ)v(τ)
⎫
⎬
⎭
5
.
In this case, we can obtain the diﬀerential equation for functional Φ[t; v(τ)]
by eliminating function ⟨z(t)x(t)⟩from Eqs. (9.112) and (9.114):
d2
dt2 Φ[t; v(τ)] +

μ + ν −
1
v(t)
dv(t)
dt
−iv(t)(z1 + z2)
 d
dtΦ[t; v(τ)]
−

iv(t)(νz2 + μz1) + z1z2v2(t)

Φ[t; v(τ)] = 0.
(9.115)
9.6
Causal Stochastic Integral Equations
9.6.1
General Remarks
Consider the simplest one-dimensional integral equation
S(t, t′) = g(t −t′)θ(t −t′) + Λ
t

0
dτg(t −τ)z(τ)S(τ, t′),
(9.116)

292
9
Stochastic Equations with the Markovian Fluctuations of Parameters
where z(t) is the random function of time, g(t −t′) is the deterministic func-
tion, Λ is a constant parameter, and θ(t) is the Heaviside step function . It-
erating this equation, we can see that its solution S(t, t′) depends on random
function z(τ) only for t′ ≤τ ≤t, which means that the causality condition
δ
δz(τ)S(t, t′) = 0
for
τ < t′,
τ > t
holds. In addition, S(t, t′) ∼θ(t −t′).
Average Eq. (9.116) over an ensemble of realizations of function z(t). In
the case of stationary process z(t), function
⟨S(t, t′)⟩= ⟨S(t −t′)⟩,
and the result of averaging assumes the form
⟨S(t −t′)⟩= g(t −t′)θ(t −t′) + Λ
t

0
dτg(t −τ)
τ

0
dτ ′Q(τ −τ ′) ⟨S(τ ′ −t′)⟩,
(9.117)
where Q(t) ∼θ(t) is the mass function deﬁned by the equality
Λ ⟨z(t)S(t, t′)⟩=
t

0
dτQ(t −τ) ⟨S(τ −t′)⟩.
Performing the Laplace transform in Eq. (9.117) with respect to t −t′, we
obtain
⟨S⟩p = g(p) + g(p)Q(p) ⟨S⟩p ,
(9.118)
where
Λ ⟨z(t)S(t, t′)⟩p = Q(p) ⟨S⟩p .
(9.119)
Note that if the integral equation (9.116) can be reduced to the diﬀerential
equation
L
 d
dt

S(t, t′) = Λz(t)S(t, t′) + δ(t −t′),
then g(p) = L−1(p).
According to the material of Sect. 5.3, the structure of function Q(p) can
be determined from the auxiliary equation
S(t, t′) = g(t −t′)θ(t −t′) + Λ
t

0
dτg(t −τ) [z(τ) + η(τ)] S(τ, t′),
(9.120)
where η(τ) is arbitrary deterministic function. If we average Eq. (9.120) and
denote the solution to averaged equation G[t, t′; η(τ)], then vertex function

9.6
Causal Stochastic Integral Equations
293
Γ(t, t1, t′) = Γ(t −t1, t1 −t′) will be given by the equality
Γ(t, t1, t′) = −
δ
δη(t1)G−1[t, t′; η]

η=0
.
The variational derivative δG/δη at η(τ) = 0 can be expressed in terms of
vertex function Γ(t, t1, t′) and average Green’s function by the relationship
(8.37), page 207
δ
δη(t1)G[t, t′; η]

η=0
=

dτ1

dτ 2 ⟨S(t −τ1)⟩Γ(τ1 −t1, t1 −τ2) ⟨S(τ2 −t′)⟩,
(9.121)
where the domain of integration is deﬁned by the condition of positiveness of
all arguments. Performing the Laplace transform in (9.121) with respect to
(t −t1) and (t1 −t′), we obtain the equality
δG
δη (p, q) = ⟨S⟩p Γ(p, q) ⟨S⟩q
(9.122)
that makes it possible to determine the Laplace transform of the vertex func-
tion. In this case, the mass function is expressed through the characteristic
functional of process z(t).
Variational derivative δG/δη in the right-hand side of Eq. (9.122) can be
obtained by varying Eq. (9.120) with respect to η(t1) followed by setting
η(t) = 0 and averaging the obtained equation. If Eq. (9.120) can be averaged
analytically, variational derivative δG/δη can be obtained by varying the
averaged equation with respect to η(t).
Consider the realization of the above scheme for diﬀerent processes z(t).
9.6.2
Telegrapher’s Random Process
Let z(t) is telegrapher’s process with correlation function
⟨z(t)z(t′)⟩=
,
z2-
e−α|t−t′|.
Averaging Eq. (9.120), we obtain
G(t, t′) = g(t −t′)θ(t −t′) + Λ
t

0
dτg(t −τ)η(τ)G(τ, t′)
+ Λ
t

0
dτg(t −τ)
2
z(τ)S(τ, t′)
3
.
(9.123)

294
9
Stochastic Equations with the Markovian Fluctuations of Parameters
Equation (9.123) is unclosed because it contains new unknown func-
tion
2
z(τ)S(τ, t′)
3
. To obtain the equation for this function, we multiply
Eq. (9.120) by z(t) and average the result
2
z(t)S(t, t′)
3
= Λ
t

0
dτg(t −τ)η(τ)
2
z(t)S(τ, t′)
3
+ Λ
t

0
dτg(t −τ)
2
z(t)z(τ)S(τ, t′)
3
.
(9.124)
Taking into account formula (7.37), page 175
⟨z(t)R[t′; z(τ)]⟩= e−α(t−t′) ⟨z(t′)R[t′; z(τ)]⟩,
(9.125)
which is valid for arbitrary functional R[t′; z(τ)] for t′ ≤t, we obtain the
equation (z2(t) ≡
,
z2-
)
2
z(t)S(t, t′)
3
= Λ
t

0
dτg(t −τ)η(τ)e−α(t−τ) 2
z(τ)S(τ, t′)
3
+ Λ
,
z2-
t

0
dτg(t −τ)e−α(t−τ)G(τ, t′).
(9.126)
System of equations (9.123) and (9.126) is the closed system. Setting η = 0
in this system and performing then the Laplace transform with respect to
(t −t′), we obtain the algebraic system
⟨S⟩p = g(p) + Λg(p) ⟨zS⟩p ,
⟨zS⟩p = Λ
,
z2-
g(p) ⟨S⟩p ,
(9.127)
whose solution is as follows:
⟨S⟩p =
g(p)
1 −Λ2 ⟨z2⟩g(p)g(p + α),
⟨zS⟩p =
Λ
,
z2-
g(p)g(p + α)
1 −Λ2 ⟨z2⟩g(p)g(p + α).
(9.128)
According to Eq. (9.119), the mass function Q(p) is
Q(p) = Λ2 ,
z2-
g(p + α).
(9.129)
In order to determine the vertex function , we vary Eqs. (9.123) and (9.126)
with respect to η(t1), set η(t) = 0, and perform the Laplace transform with
respect to (t −t1) and (t1 −t′). As a result, we obtain the algebraic system

9.6
Causal Stochastic Integral Equations
295
δG
δη (p, q) = Λg(p) ⟨S⟩p + Λg(p)
4
z δ S
δη
5
p,q
,
4
z δ S
δη
5
p,q
= Λg(p + α) ⟨zS⟩q + Λ
,
z2-
g(p + α)δG
δη (p, q),
(9.130)
whose solution is
δG
δη (p, q) = Λ ⟨S⟩p
0
1 + Λ2 ,
z2-
g(p + α)g(q + α)
1
⟨S⟩q ,
(9.131)
where we used Eq. (9.127). Comparing Eq. (9.131) with Eq. (9.122), we obtain
the vertex function in the form
Γ(p, q) = Λ
0
1 + Λ2 ,
z2-
g(p + α)g(q + α)
1
.
(9.132)
9.6.3
Generalized Telegrapher’s Random Process
Let z(t) is the generalized telegrapher’s process. Averaging Eq. (9.120),
we obtain Eq. (9.123). Then we should derive the equation for function
2
Fλ(t)S(t, t′)
3
, where
Fλ(t) =
1
1 + λz(t) −C0(λ),
Ck(λ) =
.
ak
1+λa
/
a
(⟨Fλ(t)⟩a = 0) ,
and λ is arbitrary parameter. Multiplying Eq. (9.120) by Fλ(t) and averaging
the result, we obtain
-
Fλ(t)S(t, t′)
.
= Λ
t

0
dτg(t −τ)e−α(t−τ)η(τ)
-
Fλ(t)S(τ, t′)
.
−Λ
t

0
dτg(t−τ)e−α(t−τ)
 1
λ
-
Fλ(τ)S(τ, t′)
.
−C1(λ)G(τ, t′) + C0(λ)
-
z(τ)S(τ, t′)
.
.
(9.133)
Deriving Eq. (9.133), we used the equality
⟨Fλ(t)R[t′; z(τ)]⟩= e−α(t−t′) ⟨Fλ(t′)R[t′; z(τ)]⟩,
which is valid for arbitrary functional R[t; z(τ)] of random process z(t) for
t′ ⩾t, and the identity (9.128)
z(t)F(t) = −1
λF(t) + C1(t) −z(t)C0(λ).

296
9
Stochastic Equations with the Markovian Fluctuations of Parameters
To determine the mass function, we set η(t) = 0 in Eqs. (9.123) and (9.133)
and perform the Laplace transform. As a result, we obtain the system of
equations
⟨S⟩p = g(p) + Λg(p) ⟨zS⟩p ,
Fλ(t)S(t, t′)
p

1 + Λ
λ g(p + α)

= Λg(p + α)
/
C1(λ) ⟨S⟩p −C0(λ) Fλ(t)S(t, t′)
p
0
valid for arbitrary λ. Setting
λ = λp = −Λg(p + α),
we obtain the algebraic relationship between ⟨zS⟩p and ⟨S⟩p
⟨z(t)S(t, t′)⟩p = ⟨S⟩p
C1(λp)
C0(λp)
(9.134)
and, consequently,
⟨S⟩p =
g(p)
1 −Λg(p)C1(λp)
C0(λp)
.
(9.135)
Using (9.134), function ⟨Fλ(t)S(t, t′)⟩p for arbitrary λ can be represented
in the form
⟨Fλ(t)S(t, t′)⟩p = −
λλp ⟨S⟩p [C1(λ)C0(λp) −C0(λ)C1(λp)]
(λ−λp) C0(λp)
.
(9.136)
In this case, the mass function , as follows from Eq. (9.134), is
Q(p) = ΛC1(λp)
C0(λp).
(9.137)
To determine the vertex function , we vary Eqs. (9.123) and (9.133) with
respect to η(t1), set η(t) = 0, and perform the Laplace transform. As a result,
we obtain the system
δG
δη (p, q) = Λg(p) ⟨S⟩q + Λg(p)
4
z δ S
δη
5
p,q
,
4
Fλ
δ S
δη
5
p
=

1 + Λ
λ g(p + α)
	
Λg(p + α)FλSq
+ Λg(p + α)
⎧
⎨
⎩C1(λ)δG
δη (p, q) −C0(λ)
4
z δ S
δη
5
p,q
⎫
⎬
⎭.
(9.138)

9.6
Causal Stochastic Integral Equations
297
Then, setting λ = λp in Eq. (9.138), we obtain the algebraic system in
δG
δη (p, q) and
4
z δ S
δη
5
p,q
, whose solution can be represented as follows
δG
δη (p, q) = Λ ⟨S⟩p

1 + Λ g(p + α)g(q + α)
g(p + α) −g(q + α)
C1(λp)
C0(λp) −C1(λq)
C0(λq)
	
⟨S⟩q ,
(9.139)
where we used Eqs. (9.135), (9.136). Consequently, the vertex function is
Γ(p, q) = Λ

1 + Λ g(p + α)g(q + α)
g(p + α) −g(q + α)
C1(λp)
C0(λp) −C1(λq)
C0(λq)
	
.
(9.140)
If the probability distribution of quantity a has the form
p(a) = 1
2 [δ(a −a0)+δ(a + a0)] ,
then C1(λ)/C0(λ) = −λa2
0, and we turn back to telegrapher’s process with
parameter
,
z2-
= a2
0.
If a is the continuous random quantity with zero-valued mean and suﬃ-
ciently small variance, then
C0(λ) ≈1,
C1(λ) ≈−λ
,
a2-
,
and the vertex function assumes the form
Γ(p, q) = Λ
0
1 + Λ2 ,
a2-
g(p + α)g(q + α)
1
.
(9.141)
However, Eq. (9.141) is valid only if obvious inequalities
|λ2|
,
a2-
≪1
(λ = λp, λq)
(9.142)
are satisﬁed.
9.6.4
Gaussian Markovian Process
Let z(t) is the Gaussian Markovian process with correlation function
⟨z(t)z(t)⟩=
,
z2-
e−α|t−t′|.
This process can be obtained from the process with a ﬁnite number of states
(see Remark 3.1, page 139)
ξN = z1(t) + · · · + zN(t),

298
9
Stochastic Equations with the Markovian Fluctuations of Parameters
where zi(t) are the statistically independent telegrapher’s processes with
,
z2-
= σ2/N, using limit process N →∞.
So, we consider Eq. (9.120) with z(t) = ξN(t) and introduce functions
Gl(t, t′) =
2
z1(t) · · · zl(t)S(t, t′)
3
(G0(t, t′) = G(t, t′)) .
(9.143)
Multiplying Eq. (9.120) by product z1(t) · · · zN(t), averaging the result, and
using
Eq. (9.125), we can obtain the recurrence equation for l = 0, 1, · · · , N
Gl(t, t′) = g(t −t′)θ(t −t′)δl,0 + Λ
t

0
dτg(t −τ)η(τ)e−αl(t−τ)Gl(τ, t′)
+ Λ
t

0
dτg(t −τ)e−αl(t−τ) 0
l
,
z2-
Gl−1(τ, t′) + (N −l)Gl+1(τ, t′)
1
.
(9.144)
Setting η(t) = 0 and performing the Laplace transform with respect to (t−t′),
we obtain the algebraic recurrence equation
Gl(p) = g0(p)δl,0 + Λgl(p)
0
l
,
z2-
Gl−1(p) + (N −l)Gl+1(p)
1
,
(9.145)
where gl(p) = g(l + αl).
The solution to Eq. (9.145) has the form of a ﬁnite segment of the continued
fraction
Gl(p) = Λgl(p)l
,
z2-
Kl(p)Gl−1(p),
l = 1, · · · , N,
(9.146)
where
Kl(p) =
1
1 −γl(p)Kl+1(p),
γl(p) = Λ2 ,
z2-
(l + 1)(N −l)gl(p)gl+1(p).
(9.147)
Consequently,
Gl(p) = Λl ,
z2-l l! {gl(p)Kl(p)}!g0(p)K0(p),
(9.148)
where fl! stands for the product f1 · · · fl. Taking into account the fact that
⟨ξN(t)S(t, t′)⟩p = NG1(p),
we obtain the expression for the mass function :
QN(p) = Λ2N
,
z2-
g1(p)K1(p).
(9.149)

9.6
Causal Stochastic Integral Equations
299
Setting now
,
z2-
= σ2/N and proceeding to the limit N →∞, we obtain
the mass function for the Gaussian Markovian process:
Q(p) = Λ2σ2g1(p)K1(p),
(9.150)
where K1(p) is the inﬁnite continued fraction (9.147) with parameter
γl(p) = Λ2σ2(l + 1)gl(p)gl+1(p).
(9.151)
Calculating the vertex function in the cases of telegrapher’s and generali-
zed telegrapher’s processes, we straightforwardly followed the procedure valid
for arbitrary integral equations. The goal of that consideration was to illus-
trate the general procedure. In the case of Eq. (9.116), we can immediately
obtain the expression for the vertex function if only the solution to the Dyson
equation is known. Indeed, according to Eqs. (9.121) and (8.30), page 205, we
have the following relationship for Eq. (9.116)
Λ ⟨S(t, t0)S(t0, t′)⟩=

dτ1

dτ2 ⟨S(t −τ1)⟩Γ(τ1 −t1, t1 −τ2) ⟨S(τ2 −t′)⟩.
(9.152)
Let now random process z(t) is a function of process ξN(t). Then, we can
split the correlator in the left-hand side of Eq. (9.152) using formula (7.32),
which assumes in this case the form
'
S(t, t0)S(t0, t′)
(
=
N
 
k=0
Ck
N
1
⟨z2⟩k ⟨z1(t0) · · · zk(t0)S(t, t0)⟩
'
z1(t0) · · · zk(t0)S(t0, t′)
(
.
(9.153)
Performing the Laplace transform with respect to (t −t0) and (t0 −t′), we
obtain the equality
⟨SS⟩p,q =
N
&
k=0
Ck
N
1
⟨z2⟩k Gk(p)Gk(q),
(9.154)
where function Gk(p) is given by Eq. (9.143). Consequently, we obtain the
following expression for the vertex function Γ(p, q)
Γ(p, q) = 1
Λ
N
&
k=0
Ck
N
1
⟨z2⟩k
Gk(p)Gk(q)
G0(p)G0(q) .
(9.155)
For z(t) = ξN(t), functions Gk(p) are given by Eq. (9.148), and we obtain
ΓN(p, q) = Λ

1 +
N
&
k=0
Λ2k ,
z2-k
N!k!
(N −k)! {gk(p)gk(q)Kk(p)Kk(q)}!

.
(9.156)
At N = 1, we turn back to the case of single telegrapher’s process, and
Eq. (9.156) grades into (9.132).

300
9
Stochastic Equations with the Markovian Fluctuations of Parameters
Setting
,
z2-
= σ2/N and proceeding to the limit N →∞, we obtain the
vertex function for the Gaussian Markovian process in the form of the inﬁnite
series
Γ(p, q) = Λ

1 +
∞
&
k=0
k!Λ2kσ2k {gk(p)gk(q)Kk(p)Kk(q)}!

,
(9.157)
whose terms include inﬁnite continued fractions (9.147) with parameter
(9.151). Two ﬁrst terms of series (9.157) are as follows
Γ(p, q) = Λ

1 + Λ2σ2g1(p)g1(q)K1(p)K1(q) + · · ·

.
Now, we dwell on approximations commonly used in analyzing stochastic
integral equations.
First of all, we consider the Gaussian Markovian process. In this case, the
mass function is related to the vertex function by the formula
Q(t −t′) = Λσ2
 
dτ1dτ2e−α(t−τ) ⟨S(t −τ1)⟩Γ(τ1 −τ, τ −t′),
(9.158)
where the domain of integration is deﬁned by the condition of positiveness of
all arguments. Performing the Laplace transform in Eq. (9.158) with respect
to (t −t′), we obtain the equality
Q(p) = Λσ2 ⟨S⟩p+α Γ(p + α, p).
(9.159)
The Kraichnan approximation corresponds to the replacement of vertex
function Γ(p + α, p) in Eq. (9.158) by Λ, and the Bourret approximation
assumes additionally substitution of ⟨S⟩p+α with g1(p).
The solution to the Dyson equation (9.118) depends primarily on the poles
and other signiﬁcant singularities of function g1(p). Denote p0 the singular
point of this function. Then, if the condition
Λ2σ2|g1(p0)|2|K1(p0)|2 ≪1
(9.160)
holds, we can neglect all terms of series (9.157) excluding the ﬁrst one.
Functions K1(p0) themselves depend on parameter β2 = Λ2|g1(p0)|2, and
|K1(p0)| ∼1 for β2 ≪1.
Thus, we can replace vertex function Γ(p, q) by Λ under the condition that
β2 = Λ2|g1(p0)|2 ≪1.
(9.161)
Earlier, we showed that function ⟨S⟩p0+α also has a small parameter. In the
ﬁrst approximation with respect to this small parameter, the mass function
is
Q(p) = Λ2g1(p),
(9.162)

9.6
Causal Stochastic Integral Equations
301
which corresponds to the Bourret approximation. Thus, the Kraichnan appro-
ximation fails in the context of this problem, whereas the Bourret approxi-
mation represents the ﬁrst term of the asymptotic expansion of the solution
in the above small parameter.
Note that the mass function in the Bourret approximation (9.162) coincides
with the mass function for telegrapher’s process (9.129).
The limit process α →∞in the solutions obtained for all above processes
results in the Gaussian delta-correlated process with correlation function
⟨z(t)z(t′)⟩= 2σ2τ0δ(t −t′),
τ0 = 1/α.
It is clear that this solution can be obtained immediately from Eq. (9.116).

Part IV
Asymptotic and Approximate Methods
for Analyzing Stochastic Equations

Chapter 10
Gaussian Random Field
Delta-Correlated in Time (Ordinary
Diﬀerential Equations)
In the foregoing chapters, we considered in detail the general methods of
analyzing stochastic equations. Here, we give an alternative and more detailed
consideration of the approximation of the Gaussian random delta-correlated
(in time) ﬁeld in the context of stochastic equations and discuss the physical
meaning of this widely used approximation.
10.1
The Fokker–Planck Equation
Let vector function x(t) = {x1(t), x2(t), · · · , xn(t)} satisﬁes the dynamic
equation
d
dtx(t) = v(x, t) + f(x, t),
x(t0) = x0,
(10.1)
where vi(x, t) (i = 1, · · · , n) are the deterministic functions and fi(x, t) are
the random functions of (n + 1) variable that have the following properties:
(a) fi(x, t) is the Gaussian random ﬁeld in the (n+ 1) -dimensional space
(x, t);
(b) ⟨fi(x, t)⟩= 0.
For deﬁniteness, we assume that t is the temporal variable and x is the
spatial variable.
Statistical characteristics of ﬁeld fi(x, t) are completely described by cor-
relation tensor
Bij(x, t; x′, t′) = ⟨fi(x, t)fj(x′, t′)⟩.
Because Eq. (10.1) is the ﬁrst-order equation with the initial value, its
solution satisﬁes the dynamic causality condition
δ
δfj(x′, t′)xi(t) = 0
for
t′ < t0
and
t′ > t,
(10.2)
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
305
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_10, c
⃝Springer International Publishing Switzerland 2015

306
10
Gaussian Random Field Delta-Correlated in Time
which means that solution x(t) depends only on values of functions fj(x, t′)
for times t′ preceding time t, i.e., t0 ≤t′ ≤t. In addition, we have the
following equality for the variational derivative
δ
δfj(x′, t −0)xi(t) = δijδ(x(t) −x′).
(10.3)
Nevertheless, the statistical relationship between x(t) and function values
fj(x, t′′) for consequent times t′′ > t can exist, because such function values
fj(x, t′′) are correlated with values fj(x, t′) for t′ ≤t. It is obvious that the
correlation between function x(t) and consequent values fj(x, t′′) is appre-
ciable only for t′′ −t ≤τ0, where τ0 is the correlation radius of ﬁeld f(x, t)
with respect to variable t.
For many actual physical processes, characteristic temporal scale T of func-
tion x(t) signiﬁcantly exceeds correlation radius τ0 (T ≫τ0); in this case,
the problem has the small parameter τ0/T that can be used to construct an
approximate solution.
In the ﬁrst approximation with respect to this small parameter, one can
consider the asymptotic solution for τ0 →0. In this case values of function
x(t′) for t′ < t will be independent of values f(x, t′′) for t′′ > t not only
functionally, but also statistically. This approximation is equivalent to the
replacement of correlation tensor Bij with the eﬀective tensor
Beﬀ
ij (x, t; x′, t′) = 2δ(t −t′)Fij(x, x′; t).
(10.4)
Here, quantity Fij(x, x′; t) is determined from the condition that integrals of
Bij(x, t; x′, t′) and Beﬀ
ij (x, t; x′, t′) over t′ coincide
Fij(x, x′; t) = 1
2
∞

−∞
dt′Bij(x, t; x′, t′),
which just corresponds to the passage to the Gaussian random ﬁeld delta-
correlated in time t.
Introduce the indicator function
ϕ(x, t) = δ(x (t) −x),
(10.5)
where x(t) is the solution to Eq. (10.1), which satisﬁes the Liouville equation
 ∂
∂t + ∂
∂xv(x, t)

ϕ(x, t) = −∂
∂xf(x, t)ϕ(x, t)
(10.6)
and the equality
δ
δfj(x′, t −0)ϕ(x, t) = −∂
∂xj
{δ(x −x′)ϕ(x, t)} .
(10.7)

10.1
The Fokker–Planck Equation
307
The equation for the probability density of the solution to Eq. (10.1)
P(x, t) = ⟨ϕ(x, t)⟩= ⟨δ(x(t) −x)⟩
can be obtained by averaging Eq. (10.6) over an ensemble of realizations of
ﬁeld f(x, t),
 ∂
∂t + ∂
∂xv(x, t)

P(x, t) = −∂
∂x ⟨f(x, t)ϕ(x, t)⟩.
(10.8)
We rewrite Eq. (10.8) in the form
 ∂
∂t + ∂
∂xv(x, t)

P(x, t) = −∂
∂xi

dx′
t

t0
dt′Bij(x, t; x′, t′)
. δϕ(x, t)
δfj(x′, t′)
/
,
(10.9)
where we used the Furutsu–Novikov formula (7.12), page 168
⟨fk(x, t)R[t; f(y, τ)]⟩=

dx′

dt′Bkl(x, t; x′, t′)
.δR[t; f(y, τ)]
δ fl(x′, t′)
/
(10.10)
for the correlator of the Gaussian random ﬁeld f(x, t) with arbitrary func-
tional R[t; f(y, τ)] of it and the dynamic causality condition (10.2).
Equation (10.9) shows that the one-time probability density of solution
x(t) at instant t is governed by functional dependence of solution x(t) on
ﬁeld f(x′, t) for all times in the interval (t0, t).
In the general case, there is no closed equation for the probability density
P(x, t). However, if we use approximation (10.4) for the correlation func-
tion of ﬁeld f(x, t), there appear terms related to variational derivatives
δϕ[x, t; f(y, τ)]/δfj(x′, t′) at coincident temporal arguments t′ = t −0,
 ∂
∂t + ∂
∂xv(x, t)

P(x, t) = −∂
∂xi

dx′Fij(x, x′; t)
.
δϕ(x, t)
δfj(x′, t −0)
/
.
According to Eq. (10.7), these variational derivatives can be expressed im-
mediately in terms of quantity ϕ[x, t; f(y, τ)]. Thus, we obtain the closed
Fokker-Planck equation
 ∂
∂t +
∂
∂xk
[vk(x, t) + Ak(x, t)]

P(x, t) =
∂2
∂xk∂xl
[Fkl(x, x; t)P(x, t)] ,
(10.11)
where
Ak(x, t) =
∂
∂x′
l
Fkl(x, x′; t)

x′=x
.

308
10
Gaussian Random Field Delta-Correlated in Time
Equation (10.11) should be solved with the initial condition P(x, t0) =
δ(x−x0), or with a more general initial condition P(x, t0) = W0(x) if the ini-
tial conditions are also random, but statistically independent of ﬁeld f(x, t).
The Fokker-Planck equation (10.11) is a partial diﬀerential equation and
its further analysis essentially depends on boundary conditions with respect
to x whose form can vary depending on the problem at hand.
Consider the quantities appeared in Eq. (10.11). In this equation, the terms
containing Ak(x, t) and Fkl(x, x′; t) are stipulated by ﬂuctuations of ﬁeld
f(x, t). If ﬁeld f(x, t) is stationary in time, quantities Ak(x) and Fkl(x, x′)
are independent of time. If ﬁeld f(x, t) is additionally homogeneous and
isotropic in all spatial coordinates, then Fkl(x, x, t) = const, which corre-
sponds to the constant tensor of diﬀusion coeﬃcients, and Ak(x, t) = 0 (note
however that quantities Fkl(x, x′; t) and Ak(x, t) can depend on x because
of the use of a curvilinear coordinate system).
10.2
Transitional Probability Distributions
Turn back to dynamic system (10.1) and consider the m-time probability
density
Pm(x1, t1; · · · ; xm, tm) = ⟨δ(x(t1) −x1) · · · δ(x(tm) −xm)⟩
(10.12)
for m diﬀerent instants t1 < t2 < · · · < tm. Diﬀerentiating Eq. (10.12) with
respect to time tm and using then dynamic equation (10.1), dynamic causality
condition (10.2), deﬁnition of function Fkl(x, x′; t), and the Furutsu–Novikov
formula (10.10), one can obtain the equation similar to the Fokker–Planck
equation (10.11),
∂
∂tm
Pm(x1, t1; · · · ; xm, tm)+
n

k=1
∂
∂xmk
[vk(xm, tm) + Ak(xm, tm)] Pm(x1, t1; · · · ; xm, tm)
=
n
&
k=1
n
&
l=1
∂2
∂xmk∂xml
[Fkl(xm, xm; tm)Pm(x1, t1; · · · ; xm, tm))] .
(10.13)
No summation over index m is performed here. The initial value to
Eq. (10.13) can be determined from Eq. (10.12). Setting tm = tm−1 in (10.12),
we obtain
Pm(x1, t1; · · · ; xm, tm−1) = δ(xm −xm−1)Pm−1(x1, t1; · · · ; xm−1, tm−1).
(10.14)
Equation (10.13) assumes the solution in the form
Pm(x1, t1; · · · ; xm, tm) = p(xm, tm|xm−1, tm−1)Pm−1(x1, t1; · · · ; xm−1, tm−1).
(10.15)

10.2
Transitional Probability Distributions
309
Because all diﬀerential operations in Eq. (10.13) concern only tm and xm,
we can ﬁnd the equation for the transitional probability density by substitut-
ing Eq. (10.15) in Eqs. (10.13) and (10.14):
 ∂
∂t +
∂
∂xk [vk(x, t) + Ak(x, t)]

p(x, t|x0, t0) =
∂2
∂xk∂xl [Fkl(x, x; t)p(x, t|x0, t0)]
(10.16)
with the initial condition p(x, t|x0, t0)|t→t0 = δ(x −x0), where
p(x, t|x0, t0) = ⟨δ(x(t) −x)|x(t0) = x0⟩.
In Eq. (10.16) we denoted variables xm and tm as x and t, and variables
xm−1 and tm−1 as x0 and t0.
Using formula (10.15) (m −1) times, we obtain the relationship
Pm(x1, t1; · · · ; xm, tm) = p(xm, tm|xm−1, tm−1) · · · p(x2, t2|x1, t1)P(x1, t1),
(10.17)
where P(x1, t1) is the one-time probability density governed by Eq. (10.11).
Equality (10.17) expresses the multi-time probability density as the product
of transitional probability densities, which means that random process x(t)
is the Markovian process.
Equation (10.11) is usually called the forward Fokker–Planck equation .
The backward Fokker–Planck equation (it describes the transitional proba-
bility density as a function of the initial parameters t0 and x0) can also be
easily derived.
Indeed, we obtained the backward Liouville equation (3.9), page 97 for
indicator function
 ∂
∂t0
+ v(x0, t0) ∂
∂x0

ϕ(x, t|x0, t0) = −f(x0, t0) ∂
∂x0
ϕ(x, t|x0, t0),
(10.18)
with the initial value ϕ(x, t|x0, t) = δ(x −x0).
This equation describes the dynamic system evolution in terms of ini-
tial parameters t0 and x0. From Eq. (10.18) follows the equality similar to
Eq. (10.7),
δ
δfj(x′, t0 + 0)ϕ(x, t|x0, t0) = δ(x −x′)
∂
∂x0j
ϕ(x, t|x0, t0).
(10.19)
Averaging now the backward Liouville equation (10.18) over an ensemble of
realizations of random ﬁeld f(x, t) with the eﬀective correlation tensor (10.4),
using the Furutsu–Novikov formula (10.10), and relationship (10.19) for the
variational derivative, we obtain the backward Fokker–Planck equation (see
also [21])

310
10
Gaussian Random Field Delta-Correlated in Time
 ∂
∂t0
+ [vk(x0, t0) + Ak(x0, t0)]
∂
∂x0k

p(x, t|x0, t0)
= −Fkl(x0, x0; t0)
∂2
∂x0k∂x0l
p(x, t|x0, t0),
(10.20)
p(x, t|x0, t) = δ(x −x0).
The forward and backward Fokker–Planck equations are equivalent. The
forward equation is more convenient for analyzing the temporal behavior of
statistical characteristics of the solution to Eq. (10.1). The backward equation
appears to be more convenient for studying statistical characteristics related
to initial values, such as the time during which process x(t) resides in certain
spatial region and the time at which the process arrives at region’s boundary.
In this case the probability of the fact that random process x(t) rests in
spatial region V is given by the integral
G(t; x0, t0) =

V
dxp(x, t|x0, t0),
which, according to Eq. (10.20), satisﬁes the closed equation
 ∂
∂t0
+ [vk(x0, t0) + Ak(x0, t0)]
∂
∂x0k

G(t; x0, t0)
= −Fkl(x0, x0; t0)
∂2
∂x0k∂x0l
G(t; x0, t0),
(10.21)
G(t; x0, t0) =
'
1 (x0 ∈V ) ,
0 (x0 /∈V ) .
For Eq. (10.21) we must formulate additional boundary conditions, which
depend on characteristics of both region V and its boundaries.
10.3
Applicability Range of the Fokker–Planck
Equation
To estimate the applicability range of the Fokker–Planck equation, we must
include into consideration the ﬁnite-valued correlation radius τ0 of ﬁeld
f(x, t) with respect to time. In this case, the equation for the probability
density (10.11) is replaced with the equation
%EP(x, t) = −∂
∂xk
S′(x, t),

10.3
Applicability Range of the Fokker–Planck Equation
311
where %E — is the operator appeared in the left-hand side of Eq. (10.11) in
which quantity Fkl(x, x′, t) is replaced with
Fkl(x, x′, t) =
t

0
dt′Bkl(x, x′, t)
and term S′(x, t) includes corrections to the factor of the probability ﬂux
density because of ﬁniteness of τ0. For τ0 →0, we turn back to Eq. (10.11).
Thus, smallness of parameter τ0/T is the necessary, but generally not suf-
ﬁcient condition in order that one could describe statistical characteristics
of the solution to Eq. (10.1) using the approximation of the delta-correlated
random ﬁeld of which a consequence is the Fokker–Planck equation. Every
particular problem requires more detailed investigations. Below, we give a
more physical method called the diﬀusion approximation . This method also
leads to the Markovian property of the solution to Eq. (10.1); however, it
considers to some extent the ﬁnite value of the temporal correlation radius.
Here, we emphasize that the approximation of the delta-correlated ran-
dom ﬁeld does not reduce to the formal replacement of random ﬁeld f(x, t)
in Eq. (10.1) with the random ﬁeld with correlation function (10.4). This
approximation corresponds to the construction of an asymptotic expansion
for temporal correlation radius τ0 of ﬁled f(x, t) approaching to zero. It is
in such limit process that exact average quantities like ⟨f(x, t)R[t; f(x′, τ)]⟩
grade into the expressions obtained by the formal replacement of the corre-
lation tensor of ﬁeld f(x, t) with the eﬀective tensor (10.4).
10.3.1
Langevin Equation
We illustrate the above material by the example of the Langevin equation
that allows an exhaustive statistical analysis [51]. This equation has the form
d
dtx(t) = −λx(t) + f(t),
x(t0) = 0
(10.22)
and assumes that the suﬃciently ﬁne smooth function f(t) is the stationary
Gaussian process with zero-valued average and correlation function
⟨f(t)f(t′)⟩= Bf(t −t′).
For any particular realization of random force f(t), the solution to
Eq. (10.22) has the form
x(t) =
t

t0
dτf(τ)e−λ(t−τ).

312
10
Gaussian Random Field Delta-Correlated in Time
Consequently, this solution x(t) is also the Gaussian process with the para-
meters
⟨x(t)⟩= 0,
⟨x(t)x(t′)⟩=
t

t0
dτ1
t′

t0
dτ2Bf(τ1 −τ2)e−λ(t+t′−τ1−τ2).
In addition, we have, for example,
⟨f(t)x(t)⟩=
t−t0

0
dτBf(τ)e−λτ.
Note that the one-time probability density P(x, t) = ⟨δ(x(t) −x)⟩for
Eq. (10.22) satisﬁes the equation
 ∂
∂t −λ ∂
∂xx

P(x, t) =
t−t0

0
dτBf(τ)e−λτ ∂2
∂x2 P(x, t),
P(x, t0) = δ(x),
which rigorously follows from Eq. (8.93), page 222. As a consequence, we
obtain
d
dt
,
x2(t)
-
= −2λ
,
x2(t)
-
+ 2
t−t0

0
dτBf(τ)e−λτ.
For t0 →−∞, process x(t) grades into the stationary Gaussian process
with the following one-time statistical parameters (⟨x(t)⟩= 0)
σ2
x =
,
x2(t)
-
= 1
λ
∞

0
dτBf(τ)e−λτ,
⟨f(t)x(t)⟩=
∞

0
dτBf(τ)e−λτ.
In particular, for exponential correlation function Bf(t),
Bf(t) = σ2
fe−|τ|/τ0,
we obtain the expressions
⟨x(t)⟩= 0,
,
x2(t)
-
=
σ2
fτ0
λ(1 + λτ0),
⟨f(t)x(t)⟩=
σ2
fτ0
1 + λτ0
,
(10.23)
which grade into the asymptotic expressions
,
x2(t)
-
=
σ2
fτ0
λ
,
⟨f(t)x(t)⟩= σ2
fτ0
(10.24)
for τ0 →0.

10.3
Applicability Range of the Fokker–Planck Equation
313
Multiply now Eq. (10.22) by x(t). Assuming that function x(t) is suﬃ-
ciently ﬁne function, we obtain the equality
x(t) d
dtx(t) = 1
2
d
dtx2(t) = −λx2(t) + f(t)x(t).
Averaging this equation over an ensemble of realizations of function f(t), we
obtain the equation
1
2
d
dt
,
x2(t)
-
= −λ
,
x2(t)
-
+ ⟨f(t)x(t)⟩,
(10.25)
whose steady-state solution (it corresponds to the limit process t0 →−∞
and τ0 →0)
,
x2(t)
-
= 1
λ ⟨f(t)x(t)⟩
coincides with Eq. (10.23) and (10.24).
Taking into account the fact that δx(t)/δf(t −0) = 1, we obtain the same
result for correlation ⟨f(t)x(t)⟩by using the formula
⟨f(t)x(t)⟩=
t

−∞
dτBf(t −τ)
.
δ
δf(τ)x(t)
/
(10.26)
with the eﬀective correlation function
Beﬀ
f (t) = 2σ2
fτ0δ(t).
Earlier, we mentioned that statistical characteristics of solutions to dyna-
mic problems in the approximation of the delta-correlated random process
(ﬁeld) coincide with the statistical characteristics of the Markovian processes.
However, one should clearly understand that this is the case only for statis-
tical averages and equations for these averages. In particular, realizations
of process x(t) satisfying the Langevin equation (10.22) drastically diﬀer
from realizations of the corresponding Markovian process. The latter satis-
ﬁes Eq. (10.22) in which function f(t) in the right-hand side is the ideal
white noise with the correlation function Bf(t) = 2σ2
fτ0δ(t); moreover, this
equation must be treated in the sense of generalized functions, because the
Markovian processes are not differentiable in the ordinary sense. At the same
time, process x(t) — whose statistical characteristics coincide with the cha-
racteristics of the Markovian process — behaves as suﬃciently ﬁne function
and is diﬀerentiable in the ordinary sense. For example,
x(t) d
dtx(t) = 1
2
d
dtx2(t),
and we have for t0 →−∞in particular

314
10
Gaussian Random Field Delta-Correlated in Time
.
x(t) d
dtx(t)
/
= 0.
(10.27)
On the other hand, in the case of the ideal Markovian process x(t) satis-
fying (in the sense of generalized functions) the Langevin equation (10.22)
with the white noise in the right-hand side, Eq. (10.27) makes no sense at all,
and the meaning of the relationship
.
x(t) d
dtx(t)
/
= −λ
,
x2(t)
-
+ ⟨f(t)x(t)⟩
(10.28)
depends on the deﬁnition of averages. Indeed, if we will mean Eq. (10.28) as
the limit of the equality
.
x(t + Δ) d
dtx(t)
/
= −λ ⟨x(t)x(t + Δ)⟩+ ⟨f(t)x(t + Δ)⟩
(10.29)
for Δ →0, the result will be essentially diﬀerent depending on whether we
use limit processes Δ →+0, or Δ →−0. For limit process Δ →+0, we have
lim
Δ→+0 ⟨f(t)x(t + Δ)⟩= 2σ2
fτ0,
and, taking into account Eq. (10.26), we can rewrite Eq. (10.29) in the form
.
x(t + 0) d
dtx(t)
/
= σ2
fτ0.
(10.30)
On the contrary, for limit process Δ →−0, we have
⟨f(t)x(t −0)⟩= 0
because of the dynamic causality condition, and Eq. (10.29) assumes the form
.
x(t −0) d
dtx(t)
/
= −σ2
fτ0.
(10.31)
Comparing Eq. (10.27) with Eqs. (10.30) and (10.31), we see that, for the ideal
Markovian process described by the solution to the Langevin equation with
the white noise in the right-hand side and commonly called the Ohrnstein–
Ulenbeck process , we have
.
x(t + 0) d
dtx(t)
/
̸=
.
x(t −0) d
dtx(t)
/
̸= 1
2
d
dt
,
x2(t)
-
.
Note that equalities (10.30) and (10.31) can also be obtained from the
correlation function
⟨x(t)x(t + τ)⟩=
σ2
fτ0
λ
e−λ|τ|

10.4
The Simplest Markovian Random Processes
315
of process x(t).
To conclude with the discussion of the approximation of the delta-
correlated random process (ﬁeld), we emphasize that, in all further examples,
we will treat the sentence like ’dynamic system (equation) with the delta-
correlated parameter ﬂuctuations’ as the asymptotic limit in which these
parameters have temporal correlation radii small in comparison with all cha-
racteristic temporal scales of the problem under consideration.
10.4
The Simplest Markovian Random Processes
There are only few Fokker–Planck equations that allow an exact solution.
First of all, among them are the Fokker–Planck equations corresponding to
the stochastic equations that are themselves solvable in the analytic form.
Such problems often allow determination of not only the one-time and tran-
sitional probability densities, but also the characteristic functional and other
statistical characteristics important for practice.
10.4.1
System of Linear Equations
Consider the system of linear equations for the components of vector function
x(t)
d
dtx(t) = Ax(t) + f(t),
x(t0) = x0
(10.32)
with constant matrix A. In Sect. 10.3.1, we considered in detail a special one-
dimensional case of this equation — the Langevin equation . We will assume
functions fi(t) the Gaussian functions delta-correlated in time, i.e., we set
⟨fi(t)fj(t′)⟩= 2Bijδ(t −t′).
The solution to system of equations (10.32) has the form
x(t) = e(t−t0)Ax0 +
t

t0
dτe(t−t0)Af(τ),
so that quantity x(t) is the Gaussian vector function with the parameters
⟨x(t)⟩= e(t−t0)Ax0,

316
10
Gaussian Random Field Delta-Correlated in Time
σ2
ij(t, t′)=⟨[xi(t) −⟨xi(t)⟩][xj(t′) −⟨xj(t′)⟩]⟩=
t

t0
dτ
"
e(t−t0)ABe(t−t0)AT #
ij .
(10.33)
where AT is the transposed matrix of A.
We can easily see in this case that the Gaussian probability distribution
with parameters (10.33) satisﬁes the Fokker–Planck equation for the transi-
tional probability density p(x, t|x0, t0)
 ∂
∂t +
∂
∂xi
Aikxk

p(x, t|x0, t0) =
∂
∂xi
Bik
∂
∂xk
p(x, t|x0, t0)
(10.34)
corresponding to stochastic system (10.32).
We note that Eq. (10.34) by itself also can be easily solved by the Fourier
transform with respect to spatial coordinates.
The simplest special case of Eq. (10.32) is the equation that deﬁnes the
Wiener random process . In view of the signiﬁcant role that such processes
plays in physics (for example, they describe the Brownian motion of particles)
, we consider the Wiener process in detail.
10.4.2
Wiener Random Process
The Wiener random process is deﬁned as the solution to the stochastic equa-
tion
d
dtw(t) = z(t),
w(0) = 0,
where z(t) is the Gaussian process delta-correlated in time and described by
the parameters
⟨z(t)⟩= 0,
⟨z(t)z(t′)⟩= 2σ2τ0δ(t −t′).
The solution to this equation
w(t) =
t

0
dτz(τ)
(10.35)
is the continuous Gaussian nonstationary random process with the parame-
ters
⟨w(t)⟩= 0,
⟨w(t)w(t′)⟩= 2σ2τ0 min(t, t′).
As a consequence, its characteristic functional has the form

10.4
The Simplest Markovian Random Processes
317
w(t)
0
0
0,5
1
1,5
2
2
2,5
−4
4
−2
Dt
Fig. 10.1 Realization of the Wiener process (10.35)
Φ[t; v(τ)] =
4
exp
⎧
⎨
⎩i
t

0
dτw(τ)v(τ)
⎫
⎬
⎭
5
= exp
⎧
⎨
⎩−σ2τ0
t

0
dτ1
t

0
dτ2v(τ1)v(τ2) min(τ1, τ2)
⎫
⎬
⎭.
(10.36)
Figure 10.1 shows a simulated realization of the Wiener process (10.35).
Note that the increment of process w(t) on the temporal interval (t1, t2)
w(t1; t2) = w(t2) −w(t1) =
t2

t1
dτz(τ)
has, like process w(t) itself, the Gaussian statistics with the parameters
⟨w(t1; t2)⟩= 0,
,
[w(t1; t2)]2-
= 2σ2τ0|t2 −t1|.
The Wiener random process w(t) is the Gaussian continuous process with
independent increments. This means that increments of process w(t) on the
nonoverlapping intervals (t1; t2) and (t3; t4) are statistically independent.
The characteristic functional of process

318
10
Gaussian Random Field Delta-Correlated in Time
w(t0; t0 + t) =
t0+t

t0
dτz(τ)
coincides with the characteristic functional of process w(t). This means that
realizations of processes w(t) and w(t0; t0 + t) are statistically equivalent for
any given parameter t0. Thus, dealing solely with process realizations, we
cannot decide to which process these realizations belong. In addition, pro-
cesses w(t) and w(−t) are also statistically equivalent, which means that the
Wiener random process is the time-reversible process in the sense speciﬁed
above.
An additional — fractal — property inheres in realizations of the Wiener
process. According to this property, realizations of the Wiener process w(at)
(compressed in time for a > 1) are statistically equivalent to realizations of
process a1/2w(t) (elongated in amplitude). The fractal property of the Wiener
process can be treated also as statistical equivalence of realizations of process
w(t) and realizations of process w(at)/a1/2, which is compressed both in time
t and amplitude, because their characteristic functionals coincide.
10.4.3
Wiener Random Process with Shear
Consider a more general process that includes additionally the drift depen-
dent on parameter α
w(t; α) = −αt + w(t),
α > 0.
Process w(t; α) is the Markovian process, and its probability density
P(w, t; α) = ⟨δ(w(t; α) −w)⟩
satisﬁes the Fokker–Planck equation
 ∂
∂t −α ∂
∂w

P(w, t; α) = D ∂2
∂w2 P(w, t; α),
P(w, 0; α) = δ(w),
(10.37)
where D = σ2τ0 is the diﬀusion coeﬃcient. The solution to this equation has
the form of the Gaussian distribution
P(w, t; α) =
1
2
√
πDt
exp

−(w + αt)2
4Dt
	
.
(10.38)
The corresponding integral distribution function deﬁned as the probability
of the event that w(t; α) < w is given by the formula

10.4
The Simplest Markovian Random Processes
319
F(w, t; α) =
w

−∞
dwP(w, t; α) = Pr
7
w
√
2Dt
+ α

t
2D
8
,
(10.39)
where function Pr(z) is the error function (4.20), page 122. In this case, the
curve of typical realization of the Wiener random process with shear is the
linear function of time in accordance with Eqs. (5.22), page 132
w∗(t; α) = −αt.
In addition to the initial value, supplement Eq. (10.37) with the boundary
condition
P(w, t; α)|w=h = 0,
(t > 0).
(10.40)
This condition breaks down realizations of process w(t; α) at the instant they
reach boundary h. For w < h, the solution to the boundary-value prob-
lem (10.37), (10.40) (we denote it as P(w, t; α, h)) describes the probability
distribution of those realizations of process w(t; α) that survived instant t,
i.e., never reached boundary h during the whole temporal interval. Corre-
spondingly, the norm of the probability density appears not unity, but the
probability of the event that t < t∗, where t∗is the instant at which process
w(t; α) reaches boundary h for the ﬁrst time
h

−∞
dwP(w, t; α, h) = P(t < t∗).
(10.41)
Introduce the integral distribution function and probability density of ran-
dom instant at which the process reaches boundary h
F(t; α, h) = P(t∗< t) = 1 −P(t < t∗) = 1 −
h

−∞
dwP(w, t; α, h),
P(t; α, h) = ∂
∂tF(t; α, h) = −∂
∂wP(w, t; α, h)|w=h.
(10.42)
If α > 0, process w(t; α) moves on average out of boundary h; as a result,
probability P(t < t∗) (10.41) tends for t →∞to the probability of the event
that process w(t; α) never reaches boundary h. In other words, limit
lim
t→∞
h

−∞
dwP(w, t; α, h) = P (wmax(α) < h)
(10.43)
is equal to the probability of the event that the process absolute maximum

320
10
Gaussian Random Field Delta-Correlated in Time
wmax(α) =
max
t∈(0,∞) w(t; α)
is less than h. Thus, from Eq. (10.43) follows that the integral distribution
function of the absolute maximum wmax(α) is given by the formula
F(h; α) = P (wmax(α) < h) = lim
t→∞
h

−∞
dwP(w, t; α, h).
(10.44)
After we solve boundary-value problem (10.37), (10.40) by using, for ex-
ample, the reﬂection method, we obtain
P(w, t; α, h)=
1
2
√
πDt

exp

−(w + αt)2
4Dt

−exp

−hα
D −(w −2h + αt)2
4Dt
	
.
(10.45)
Substituting this expression in Eq. (10.42), we obtain the probability density
of instant t∗at which process w(t; α) reaches boundary h for the ﬁrst time
P(t; α, h) =
1
2Dt
√
πDt
exp

−(h + αt)2
4Dt
	
.
Finally, integrating Eq. (10.45) over w and setting t →∞, we obtain, in
accordance with Eq. (10.44), the integral distribution function of absolute
maximum wmax(α) of process w(t; α) in the form [52,71]
F(h; α) = P (wmax(α) < h) = 1 −exp

−hα
D
	
.
(10.46)
Consequently, the absolute maximum of the Wiener process has the expo-
nential probability density
P(h; α) = ⟨δ (wmax(α) −h)⟩= α
D exp

−hα
D
	
.
The Wiener random process oﬀers a possibility of constructing other pro-
cesses convenient for modeling diﬀerent physical phenomena. In the case of
positive quantities, the simplest approximation of such kind is the logarithmic-
normal (lognormal) process. Consider this process in greater detail.
10.4.4
Lognormal Random Process, Intermittency
and Dynamic Localization
We deﬁne the lognormal process (logarithmic-normal process) by the formula

10.4
The Simplest Markovian Random Processes
321
y(t; α) = ew(t;α) = exp
⎧
⎨
⎩−αt +
t

0
dτz(τ)
⎫
⎬
⎭,
(10.47)
where z(t) is the Gaussian white noise process with the parameters
⟨z(t)⟩= 0,
⟨z(t)z(t′)⟩= 2σ2τ0δ(t −t′).
The lognormal process satisﬁes the stochastic equation
d
dty(t; α) = {−α + z(t)} y(t; α),
y(0; α) = 1.
We note that changing the sign of parameter α in (10.47) is statistically
equivalent to switch to the process 1/y(t; α) [61,62].
α > 0
0
0
0.5
1
1
1.5
2
2
2.5
3
4
Dt
y(t)
α < 0
0
0
0.5
1
1.5
2
2.5
5
10
15
20
25
30
Dt
y(t)
Fig. 10.2 Realizations of lognormal processes y(t) for parameters α > 0 and α < 0
for the parameter ratio |α|/D = 1
Figure 10.2 displays realizations of lognormal random processes y(t) in
Eq. (10.47) for positive and negative parameters α for the parameter ratio
|α|/D = 1 (the dashed curves show the functions exp{−Dt} and exp{Dt},
correspondingly). The ﬁgure shows the presence of rare but strong ﬂuctua-
tions relative to the dashed curves towards both large values and zero. Such a
property of random processes is called intermittency. The curve with respect
to which the ﬂuctuations are observed is referred to as the typical realization
curve (see Sect. 5.1.1, page 127).
The one-time probability density of the lognormal process is given by the
formula

322
10
Gaussian Random Field Delta-Correlated in Time
P(y, t; α) = ⟨δ (y(t; α) −y)⟩=
2
δ

ew(t;α) −y
3
= 1
y ⟨δ (w(t; α) −ln y)⟩= 1
y P(w, t; α)|w=ln y ,
where P(w, t; α) is the one-time probability density of the Wiener process
with a drift, which is given by Eq. (10.38), so that
P(y, t; α) =
1
2y
√
πDt
exp
'
−(ln y + αt)2
4Dt
(
=
1
2y
√
πDt
exp

−ln2 (yeαt)
4Dt
	
,
(10.48)
where D = σ2τ0.
Note that the one-time probability density of random process y(t; α) =
1/y(t; α) is also lognormal and is given by the formula
P(y, t; α) =
1
2y
√
πDt
exp

−ln2 (ye−αt)
4Dt
	
,
(10.49)
which coincides with Eq. (10.48) with parameter α of opposite sign. Corre-
spondingly, the integral distribution functions are given, in accordance with
Eq. (10.39), by the expressions
F(y, t; α) = P (y(t; α) < y) = Pr

1
√
2Dt
ln

ye±αt
,
(10.50)
where Pr(z) is the probability integral (4.20), page 122.
Figures 10.3 show the curves of the lognormal probability densities (10.48)
and (10.49) for α/D = 1 and dimensionless times τ = Dt = 0.1 and 1. Figure
10.4 shows these probability densities at τ = 1 in logarithmic scale.
Structurally, these probability distributions are absolutely diﬀerent. The
only common feature of these distributions consists in the existence of long
0.5
0.5
1.0
1.0
1.5
1.5
2.0
2.0
y
P(y)
a
τ = 0.1
τ = 1
0.2
0.4
0.6
0.8
1
2
3
4
y
P(y)
b
τ = 0.1
τ = 1
Fig. 10.3 Logarithmic-normal probability densities (10.48) (a) and (10.49) (b) for
α/D = 1 and dimensionless times τ = 0.1 and 1

10.4
The Simplest Markovian Random Processes
323
P(y)
P(y)
−2.0 −1.5 −1.0 −0.5
0.5
0.5
2.0
1.5
1, 0
1, 0
Fig. 10.4 Probability densities of processes y(t; α) (solid line) and y(t; α) (dashed
line) at τ = 1 in common logarithmic scale
ﬂat tails that appear in distributions at τ = 1; these tails increase the role of
high peaks of processes y(t; α) and y(t; α) in the formation of the one-time
statistics.
Having only the one-time statistical characteristics of process y(t; α), one
can obtain a deeper insight into the behavior of realizations of process y(t; α)
on the whole interval of times (0, ∞) [52,71]. In particular,
(1) The lognormal process y(t; α) is the Markovian process and its one-
time probability density (10.48) satisﬁes the Fokker–Planck equation
 ∂
∂t −α ∂
∂yy

P(y, t; α) = D ∂
∂yy ∂
∂yyP(y, t; α),
P(y, 0; α) = δ(y −1).
(10.51)
From Eq. (10.51), one can easily derive the equations for the moment func-
tions of process y(t; α); solutions to these equations are given by the formulas
⟨yn(t; α)⟩= en(n−α/D)Dt,
.
1
yn(t; α)
/
= en(n+α/D)Dt,
n = 1, 2, · · ·
(10.52)
from which follows that moments exponentially grow with time.
From Eq. (10.51), one can easily obtain the equality
α = −lim
t→+∞
∂⟨ln y(t)⟩
∂t
.
(10.53)
Remark 10.1. Lyapunov Characteristic Index
Note that many investigators give great attention to the approach based
on the Lyapunov analysis of stability of solutions to deterministic ordinary
diﬀerential equations

324
10
Gaussian Random Field Delta-Correlated in Time
d
dtx(t) = A(t)x(t).
This approach deals with the upper limit of problem solution
λx(t) =
lim
t→+∞
1
t ln |x(t)|
called the characteristic index of the solution. In the context of this approach
applied to stochastic dynamic systems, these investigators often use statistical
analysis at the last stage to interpret and simplify the obtained results; in
particular, they calculate statistical averages such as
,
λx(t)
-
=
lim
t→+∞
1
t ⟨ln |x(t)|⟩.
(10.54)
♦
Consequently, parameter α (10.53) is the Lyapunov characteristic index of
the lognormal random process y(t; α) (see, e.g., [58–60]).
(2) From the integral distribution function, one can calculate the typical
realization curve of lognormal process y(t; α); this curve appears the expo-
nentially decaying curve
y∗(t; α) = e−αt,
(10.55)
which agrees with Eq. (5.22), page 132.
Consequently, the exponential increase of moments is caused by deviations
of process y(t; α) from the typical realization curve y∗(t; α) towards both large
and small values of y.
At α/D = 1, the average value of process y(t; D) is independent of time
and is equal to unity. Despite this fact, according to Eq. (10.50), the proba-
bility of the event that y < 1 for Dt ≫1 rapidly approaches the unity by the
law
P (y(t; D) < 1) = Pr
7
Dt
2
8
= 1 −
1
√
πDt
e−Dt/4,
i.e., the curves of process realizations run mainly below the level of the process
average ⟨y(t; D)⟩= 1, though namely large peaks of the process govern the
behavior of statistical moments of process y(t; D).
Here, we have a clear contradiction between the behavior of statistical
characteristics of process y(t; α) and the behavior of process realizations.
(3) The behavior of realizations of process y(t; α) on the whole temporal
interval can also be evaluated with the use of the p -majorant curves Mp(t, α)
whose deﬁnition is as follows [52, 71]. We call the majorant curve the curve
Mp(t, α) for which inequality y(t; α) < Mp(t, α) is satisﬁed for all times t
with probability p, i.e.,
P {y(t; α) < Mp(t, α) for all t ∈(0, ∞)} = p.

10.4
The Simplest Markovian Random Processes
325
The above statistics (10.46) of the absolute maximum of the Wiener pro-
cess with a drift w(t; α) makes it possible to outline a wide enough class of
the majorant curves. Indeed, let p be the probability of the event that the
absolute maximum wmax(β) of the auxiliary process w(t; β) with arbitrary
parameter β in the interval 0 < β < α satisﬁes inequality w(t; β) < h = ln A.
It is clear that the whole realization of process y(t; α) will run in this case
below the majorant curve
Mp(t, α, β) = Ae(β−α)t.
(10.56)
with the same probability p. As may be seen from Eq. (10.46), the proba-
bility of the event that process y(t; α) never exceeds majorant curve (10.56)
depends on this curve parameters according to the formula
p = 1 −A−β/D.
This means that we derived the one-parameter class of exponentially de-
caying majorant curves
Mp(t, α, β) =
1
(1 −p)D/β e(β−α)t.
(10.57)
Notice the remarkable fact that, despite statistical average ⟨y(t; D)⟩re-
mains constant (⟨y(t; D)⟩= 1) and higher-order moments of process y(t; D)
are exponentially increasing functions, one can always select an exponen-
tially decreasing majorant curve (10.57) such that realizations of process
y(t; D) will run below it with arbitrary predetermined probability p < 1. In
particular, inequality (τ = Dt)
y(t; D) < M1/2(t, D, D/2) = M(τ) = 4e−τ/2
(10.58)
is satisﬁed with probability p = 1/2 for any instant t from interval (0, ∞).
Figure 10.5 schematically shows the behaviors of a realization of process
y(t; D) and the majorant curve (10.58). This schematic is an additional fact
in favor of our conclusion that the exponential growth of moments of process
y(t; D) with time is the purely statistical eﬀect caused by averaging over the
whole ensemble of realizations.
Note that the area below the exponentially decaying majorant curves has a
ﬁnite value. Consequently, high peaks of process y(t; α), which are the reason
of the exponential growth of higher moments, only insigniﬁcantly contribute
to the area below realizations; this area appears ﬁnite for almost all real-
izations, which means that the peaks of the lognormal process y(t; α) are
suﬃciently narrow .
(4) In this connection, it is of interest to investigate immediately the
statistics of random area below realizations of process y(t; α)

326
10
Gaussian Random Field Delta-Correlated in Time
y(τ)
τ
1
1
2
2
3
3
4
M(τ)
Fig. 10.5 Schematic behaviors of a realization of process y(t; D) and majorant
curve M(τ) (10.58)
Sn(t; α) =
t

0
dτyn(τ; α).
(10.59)
This function satisﬁes the system of stochastic equations
d
dtSn(t; α) = yn(t; α),
Sn(0; α) = 0,
d
dty(t; α) = {−α + z(t)} y(t; α),
y(0; α) = 1,
(10.60)
so that the two-component process {y(t; α), Sn(t; α)} is the Markovian pro-
cess whose one-time probability density
P(Sn, y, t; α) = ⟨δ (Sn(t; α) −Sn) δ (y(t; α) −y)⟩
and transition probability density satisfy the Fokker–Planck equation
 ∂
∂t + yn ∂
∂Sn
−α ∂
∂y y

P(Sn, y, t; α) = D ∂
∂yy ∂
∂yyP(Sn, y, t; α),
P(Sn, y, 0; α) = δ(Sn)δ(y −1).
(10.61)

10.4
The Simplest Markovian Random Processes
327
Unfortunately, Eq. (10.61) cannot be solved analytically, which prevents
us from studying the statistics of process Sn(t; α) exhaustively. However, for
the one-time statistical averages of process Sn(t; α), i.e., averages at a ﬁxed
instant, the corresponding statistics can be studied in suﬃcient detail.
With this goal in view, we rewrite Eq. (10.59) in the form
Sn(t; α) =
t

0
dτ exp
⎧
⎨
⎩−nατ + n
τ

0
dτ1z(τ1)
⎫
⎬
⎭
=
t

0
dτ exp
⎧
⎨
⎩−nα(t −τ) + n
t−τ

0
dτ1z(t −τ −τ1)
⎫
⎬
⎭,
from which follows that quantity Sn(t; α) in the context of the one-time
statistics is statistically equivalent to the quantity
Sn(t; α) =
t

0
dτe
−nα(t−τ)+n
t−τ

0
dτ1z(τ+τ1)
.
(10.62)
Diﬀerentiating now Eq. (10.62) with respect to time, we obtain the statis-
tically equivalent stochastic equation
d
dtSn(t; α) = 1 −n{α −z(t)}Sn(t; α),
Sn(0; α) = 0
whose one-time statistical characteristics are described by the one-time
probability density P(Sn, t; α) = ⟨δ (Sn(t; α) −Sn)⟩that satisﬁes the Fokker–
Planck equation
 ∂
∂t +
∂
∂Sn
−nα ∂
∂Sn
Sn

P(Sn, t; α) = n2D ∂
∂Sn
Sn
∂
∂Sn
SnP(Sn, y, t; α).
(10.63)
As may be seen from Eq. (10.63), random integrals
Sn(α) =
∞

0
dτyn(τ; α)
are distributed according to the steady-state probability density
P(Sn; α) =
1

n2D
α/nD Γ
 α
D

S1+α/D
n
exp

−
1
n2DSn
	
,
where Γ(z) is the gamma function . In the special case n = 1, quantity

328
10
Gaussian Random Field Delta-Correlated in Time
S(α) = S1(α) =
∞

0
dτy(τ; α)
has the following probability density
P(S; α) =
1
Dα/DΓ
 α
D

S1+α/D exp

−1
DS
	
.
(10.64)
If we set now α = D, then the steady-state probability density and the
corresponding integral distribution function will have the form
P(S; D) =
1
DS2 exp

−1
DS
	
,
F(S; D) = exp

−1
DS
	
.
(10.65)
The time-dependent behavior of the probability density of random process
S(t, α) =
∞

t
dτy(τ; α)
(10.66)
gives an additional information about the behavior of realizations of process
y(t; α) with time t. The integral in the right-hand side of Eq. (10.66) can be
represented in the form
S(t, α) = y(t; α)
∞

0
dτ exp
⎧
⎨
⎩−ατ +
τ

0
dτ1z(τ1 + t)
⎫
⎬
⎭.
(10.67)
In Eq. (10.67), random process y(t; α) is statistically independent of the
integral factor, because they depend functionally on process z(τ) for nonover-
lapping intervals of times τ; in addition, the integral factor by itself appears
statistically equivalent to random quantity S(α). Consequently, the one-time
probability density P(S, t; α) =
2
δ

S(t; α) −S
3
of random process S(t, α)
is described by the expression
P(S, t; α) =
∞

0
∞

0
dydSδ(yS −S)P(y, t; α) =
∞

0
dy
y P(y, t; α)P
7 S
y ; α
8
,
(10.68)
where P(y, t; α) is the one-time probability density of lognormal process
y(t; α) (10.47) and P
7 S
y ; α
8
is the probability density (10.64) of random
area.
The corresponding integral distribution function

10.5
Logarithmic-Normal Random Fields, Intermittency and Clustering
329
F(S, t; α) = P

S(t; α) < S

=

S

0
dSP (S, t; α)
is given by the integral
F(S, t; α) =
∞

0
dyP(y, t; α)F
7 S
y ; α
8
,
where F(S; α) is the integral distribution function of random area S(t; α). In
the special case α = D, we obtain, according to Eq. (10.48) and (10.65), the
expression
F(S, t; D) =
1
2
√
πDt
∞

0
dy
y exp
'
−ln2 
yeDt
4Dt
−
y
D S
(
from which follows that the probability of the event that inequality S(t; D) <
S is satisﬁed monotonously tends to unity with increasing Dt for any pre-
determined value of D S. This is an additional evidence in favor of the fact
that every separate realization of the lognormal process tends to zero with in-
creasing Dt, though moment functions of process y(t; α) show the exponential
growth caused by large spikes.
10.5
Logarithmic-Normal Random Fields,
Intermittency and Clustering
10.5.1
Logarithmic-Normal Random Fields
Consider now a positive lognormal random ﬁeld f(r, t) whose one-point
probability density
P(r, t; f) = ⟨δ (f(r, t )−f)⟩
is determined by the equation
∂
∂tP(r, t; f) =

D0
∂2
∂r2 + α ∂
∂f f + Df
∂
∂f f ∂
∂f f
	
P(r, t; f)
(10.69)
with initial condition P(r, 0; f) = δ (f −f0(r)), where D0 is the diﬀusion
coeﬃcient in r-space, and coeﬃcients α and Df characterize the diﬀusion in
the f-space. The parameter α can be either positive, negative, or equal to
zero (critical case). In the context of one-point characteristics, changing the
sign of α means switch from the ﬁeld f(r, t) to the ﬁeld f(r, t) = 1/f(r, t).

330
10
Gaussian Random Field Delta-Correlated in Time
The solution to this equation has the form
P(r, t; f) =
1
2f

πDft exp

D0t ∂2
∂r2
	
exp

−ln2 [feαt/f0(r)]
4Dft
	
.
(10.70)
For a positive conservative random ﬁeld f(r, t) with

dr f(r, t)
=

dr f0(r), we have α = Df, and Eq. (10.69) assumes the form
∂
∂tP(r, t; f) =

D0
∂2
∂r2 + α ∂2
∂f 2 f 2

P(r, t; f).
(10.71)
Of course, any random ﬁeld always has the intermittency property. Indeed,
time evolution of f(r, t) at any ﬁxed spatial point r is a random process to
which all the above applies.
In the case of the problem statistically homogeneous in space and corres-
ponding to the initial ﬁeld distribution f0(r) = f0, all the one-point statistical
characteristics of the ﬁeld f(r, t) are independent of point r, and positivity
of parameter α = −lim
t→∞
∂⟨ln f(r, t)⟩
∂t
implies that, at any point of space, re-
alizations of a lognormal ﬁeld f(r, t) decrease with time despite the presence
of rare large outbursts characteristic of the lognormal process. The charac-
teristic decay time of the ﬁeld is t ∼1/α. But if this ﬁeld decreases almost
everywhere, it must somewhere concentrate, which means that clustering must
occur. For a negative value of α, the ﬁeld grows at each ﬁxed point of space.
In this case, probability density (10.70) is independent of r and is described
by the equation
∂
∂tP(t; f) =

α ∂
∂f f + Df
∂
∂f f ∂
∂f f
	
P(t; f),
P(0; f) = δ (f −f0) ,
(10.72)
whose solution is given by the formula
P(t; f) =
1
2f

πDft exp

−ln2 [feαt/f0]
4Dft
	
.
(10.73)
Hence, the one-point statistical characteristics of random ﬁeld f(r, t) in
a spatially homogeneous problem are statistically equivalent to the statisti-
cal characteristics of the lognormal process f(t; α) with probability density
(10.73). A characteristic feature of this distribution is the appearance of a
long gently sloping " tail" for Dft ≫1, which means the enhanced role of
large outbursts of the process f(t; α) in forming the one-time statistics. For
this distribution, all moment functions of any order n increase exponentially
in time. In particular, for n = 1 and Df > α, the mean is given by
⟨f(r, t)⟩= f0e(Df −α)t,

10.5
Logarithmic-Normal Random Fields, Intermittency and Clustering
331
f(r, 0)
r
r
1
t
t1
f ∗(t)
α > 0
f(r, t1)
f(r, 0)
r
r
1
t
t1
f ∗(t)
α < 0
f(r, t1)
Fig. 10.6 Schematic behavior of realizations of random ﬁeld f(r, t) for α > 0 and
α < 0
and α is the Lyapunov characteristic index.
The papers [139, 140] mentioned in Sect. 5.1.1, 5.1.1 page 127 treat the
intermittency of parametrically excited random ﬁelds (namely, the presence
of great but rare outbursts) as the structure formation in the space of this
ﬁeld.
However, the above material clearly shows that this inference is contrary to
fact because intermittency takes place always, while the structure formation
(in the form of clustering) occurs only under certain conditions. Moreover, if
statistically homogeneous ﬁeld f(r, t) is a lognormal ﬁled, ﬁeld 1/f(r, t) will
be also the lognormal ﬁeld and will have great rare outbursts. But if one of
these ﬁelds shows clustering, the other does not show it though intermittency
is characteristic of both these ﬁelds.
Figure 10.6 schematically shows realizations of random ﬁeld f(r, t) with
diﬀerent signs of parameter α.
10.5.2
Statistical Topography of the Lognormal
Random Fields
Analyzing one-point statistical characteristics of a spatially homogeneous
problem, it is generally convenient to take into account that random ﬁeld

332
10
Gaussian Random Field Delta-Correlated in Time
f(r, t) is statistically equivalent to certain random process f(t) with the
same statistical characteristics.
Knowing the one-point probability density of random ﬁeld f(r, t) (10.70),
we can also obtain some general information about the spatial structure of
f(r, t). In particular, functionals of f(r, t) such as the total mean volume
(in the three-dimensional case) or area (in the two-dimensional case) of the
domain in which f(r, t) > f and the total mean "mass" of the ﬁeld conﬁned
within that domain are given by the formulas
⟨V (t, f)⟩=

dr
∞

f
df ′ P(r, t; f ′),
⟨M(t, f)⟩=

dr
∞

f
df ′ f ′P(r, t; f ′).
The values of these functionals are independent of the diﬀusion in the r space
(of the coeﬃcient D0), and in the case of probability distribution (10.70), we
obtain the expressions
⟨V (t, f)⟩=

dr Pr

1
√
2Dt
ln
f0(r)
f
e−αt

,
⟨M(t, f)⟩= e(D−α)t

dr f0(r) Pr

1
√
2Dt
ln
f0(r)
f
e(2D−α)t

,
(10.74)
where the probability integral Pr(z) is deﬁned in Eq. (4.20), page 122.
Using asymptotic forms (4.23), page 123 of function Pr(z), we can study
the time evolution of these functionals. Namely, the asymptotic form of the
mean volume as t →∞decreases in time for α > 0
⟨V (t, f)⟩≈1
α
$
D
πf α/Dte−α2t/4D

dr
>
f α/D
0
(r).
For α < 0, the mean volume occupies all the space as t →∞.
The asymptotic form of the total mean "mass" as t →∞is (in the most
interesting case α < 2D)
⟨M(t, f)⟩≈e(D−α)t

drf0(r)
⎡
⎣1 −
1
(2D −α)
1
D
πt

f
f0(r)
	(2D−α)/D
e−(2D−α)2t/4
⎤
⎦.
Therefore, for α > 0, all the mean "mass" is collected in clusters in the
limit t →∞.
For homogeneous initial conditions, the integrands in (10.74) represent
speciﬁc values of the volume of large outbursts and their total "mass" , i.e.,
the corresponding values per unit volume,

10.5
Logarithmic-Normal Random Fields, Intermittency and Clustering
333
⟨vhom(t, f)⟩=⟨θ (f(r, t) −f)⟩=P{f(r, t) > f} = Pr

1
√
2Dt
ln
f0
f e−αt

,
⟨mhom(t, f)⟩=f0e(D−α)t Pr

1
√
2Dt
ln
f0
f e(2D−α)t

.
(10.75)
If we choose the section level f
> f0, then ⟨vhom(0, f)⟩= 0 and
⟨mhom(0, f)⟩= 0 at the initial instant. After that, spatial perturbations of
random ﬁeld f(r, t) occur, and we have the asymptotic expressions as t →∞
(for 2D > α)
⟨vhom(t, f)⟩= P{f(r, t) > f} ≈
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
1
α
$
D
πt
f0
f
α/D
e−α2t/4D (α > 0),
1 −1
|α|
$
D
πt
 f
f0
|α|/D
e−α2t/4D (α < 0),
(10.76)
⟨mhom(t, f)⟩≈f0e(D−α)t
⎡
⎣1 −
1
(2D −α)

D
πt
 f
f0
(2D−α)/D
e−(2D−α)2t/4D
⎤
⎦.
(10.77)
Hence, for α > 0, the speciﬁc total volume tends to zero, and the speciﬁc
total "mass" contained in that volume tends to the mean "mass" of the en-
tire space, which corresponds to the criterion for structure formation with
probability one for "ideal clustering" of the ﬁeld f(r, t) under consideration.
In these conditions, random ﬁeld f(r, t) is practically absent in the most part
of space. In addition, at each ﬁxed point of space, the characteristic decay
time of the ﬁeld is αt ∼1, and the characteristic time of cluster structure
formation of the ﬁeld is αt ∼max
0
4ξ, 4ξ/(2ξ −1)21
, where ξ = D/α.
For α < 0, clustering is absent, and only the general increase of random
ﬁeld f(r, t) occurs everywhere in the space. In this case, therefore, chaos
remains chaos! Only clustering of the zeros of the ﬁeld f(r, t) occurs.
Remark 10.2. We Have the Following Theorem:
In a statistically homogeneous problem, a conservative positive parametri-
cally excited random log-normal ﬁeld always experiences clustering with proba-
bility one, i.e., for almost all realizations of this ﬁeld.
Indeed, we then have f(r, t) = eln f(r,t), and therefore
⟨f(r, t)⟩=
2
eln f(r,t)3
= exp

⟨ln f(r, t)⟩+ 1
2σ2
ln f(r,t)
	
,
where σ2
ln f(r,t) is the variance of the random ﬁeld ln f(r, t). In view of con-
servativity
⟨ln f(r, t)⟩+ 1
2σ2
ln f(r,t) = ln f0,

334
10
Gaussian Random Field Delta-Correlated in Time
and the typical realization curve is
f ∗(r, t) = e⟨ln f(r,t)⟩= f0 exp−αt
with the Lyapunov characteristic index
α = lim
t→∞
1
2tσ2
ln f(r,t) > 0.
The problem is to calculate it from the corresponding dynamic equation.
Here, as noted above, α = D for a conservative f(r, t) (see Eq. (10.71)).
Therefore, the characteristic time of cluster structure formation αt ∼4, which
is four times greater than the characteristic decay time of the ﬁeld at almost
every point of space.
♦
So, we have demonstrated that if clustering is realized with probability one
(i.e., in almost all realizations of a random ﬁeld), then it arises from very rare
events whose probability tends to zero. In this connection, I cannot but note
that a point commonly accepted in many works suggests that, for an event
to happen, it is required that this event was most probable. For example,
in a recent paper [37], a hypothesis of the origin of life from the perspective
of physics was suggested as a result of calculations of some probabilities (on
the base of random graphs and chains): "Life can be brieﬂy described as a
result of a game process, an interplay between a part of the system and its
environment. During the game, this part acquired an ability to remember the
probabilities of gains and losses in previous rounds, which gave it a chance to
exist in the following ones" . I cannot agree with the idea that the origin of
life is a game process. I believe that the origin of life is an event happened
with probability one [63].
10.6
Causal Integral Equations
In problems discussed earlier, we succeeded in deriving the closed statistical
description in the approximation of the delta-correlated random ﬁeld due
to the fact that every of these problems corresponded to a system of the
ﬁrst-order (in temporal coordinate) diﬀerential equations with given initial
values at t = 0. The solutions to such systems satisfy the dynamic causality
condition, which means that the solution at instant t depends only on system
parameter ﬂuctuations for preceding times and is independent of ﬂuctuations
for consequent times.
However, problems described in terms of integral equations that generally
cannot be reduced to a system of diﬀerential equations also can satisfy the
causality condition.

10.6
Causal Integral Equations
335
10.6.1
General Remarks
In this case, the parent stochastic equation is the linear integral equation for
Green’s function
S(r, r′) = S0(r, r′)+

dr1

dr2

dr3S0(r, r1)Λ(r1, r2, r3)f(r2)S(r3, r′),
(10.78)
where r denotes all arguments of functions S(r, r′) and f(r) including the
index arguments that assume summation instead of integration. It is assumed
here that function f(r) is a random ﬁeld and function S0(r, r′) is Green’s
function for the problem with absent parameter ﬂuctuations, i.e., for f(r) =
0. We will assume additionally that quantity Λ({ri}) is a function.
The solution to Eq. (10.78) is a functional of ﬁeld f(r), i.e.
S(r, r′) = S[r, r′; f(r)]
and Eq. (10.78) appears to be equivalent to the functional equation that con-
tains the variational derivative in functional space {f(r)} (8.30), page 205
δ
δf(r0)S[r, r′; f(r)] =

dr1

dr2S[r, r1; f(r)]Λ(r1, r0, r2)S[r2, r′; f(r)]
(10.79)
and satisﬁes the initial value
S[r, r′; f(r)]f=0 = S0(r, r′).
Now, we separate the temporal coordinate t in Eq. (10.78), i.e., rewrite it
in the form
S(r, t; r′, t′) = S0(r, t; r′, t′)
+

dr1

dr2

dr3

dτS0(r, t; r1, τ)Λ(r1, r2, r3)f(r2, τ)S(r3, τ; r′, t′).
(10.80)
In what follows, we will assume that
S0(r, t; r′, t′) = g(r, t; r′, t′)θ(t −t′),
where θ(t) is the Heaviside step function . In this case, the solution to
Eq. (10.80) also has the form
S(r, t; r′, t′) = G(r, t; r′, t′)θ(t −t′),
where function G(r, t; r′, t′) is described by the causal (in time) integral
equation

336
10
Gaussian Random Field Delta-Correlated in Time
G(r, t; r′, t′) = g(r, t; r′, t′)
+

dr1

dr2

dr3
t

t′
dτg(r, t; r1, τ)Λ(r1, r2, r3)f(r2, τ)G(r3, τ; r′, t′).
(10.81)
Function G(r, t; r′, t′) is a functional of ﬁeld f(r, τ), i.e.,
G(r, t; r′, t′) = G[r, t; r′, t′; f(r, τ)].
Consequently, the dynamic causality condition has the form
δ
δf(r0, τ)G(r, t; r′, t′) = 0
for
τ < t′
and
τ > t.
In this case, the variational derivative is given, in view of Eq. (10.79), by
the expression
δ
δf(r0, t0)G(r, t; r′, t′)=

dr1

dr2G(r, t; r1, t0)Λ(r1, r0, r2)G(r2, t0; r′, t′),
(10.82)
from which follows that
δ
δf(r0, t −0)G(r, t; r′, t′)=

dr1

dr2g(r, t; r1, t)Λ(r1, r0, r2)G(r2, t; r′, t′).
(10.83)
10.6.2
Statistical Averaging
Let now random ﬁeld f(r, t) is the Gaussian random ﬁeld whose average
value is zero. In this case, assignment of the correlation function
B(r, t; r′, t′) = ⟨f(r, t)f(r′, t′)⟩.
describes all statistical characteristics of the ﬁeld. For statistically homoge-
neous and stationary random ﬁeld f(r, t), we have
B(r, t; r′, t′) = B(r −r′, t −t′).
Averaging Eq. (10.81) over an ensemble of realizations of ﬁeld f(r, t), we
obtain the equation

10.6
Causal Integral Equations
337

G(r, t; r′, t′)

= g(r, t; r′, t′)
+

dr1

dr2

dr3
t

t′
dτg(r, t; r1, τ)Λ(r1, r2, r3)

f(r2, τ)G(r3, τ; r′, t′)

.
(10.84)
Using the Furutsu–Novikov formula (10.10), page 307 to split the correla-
tion in the right-hand side of Eq. (10.84), we obtain

G(r, t; r′, t′)

= g(r, t; r′, t′) +

dr1

dr2

dr3
t

t′
dτg(r, t; r1, τ)Λ(r1, r2, r3)
×

dr0
τ

t′
dt0B(r2 −r0, τ −t0)

δ
δf(r0, t0)G(r3, τ; r′, t′)

.
(10.85)
If we use Eq. (10.82) for the variational derivative, we obtain that Eq. (10.85)
assumes the form of the equality
⟨G(r, t; r′, t′)⟩= g(r, t; r′, t′) +

dr1

dr2

dr3
t

t′
dτg(r, t; r1, τ)
× Λ(r1, r2, r3)

dr0
τ

t′
dt0B(r2 −r0, τ −t0)
×

dr′

dr” ⟨G(r3, τ; r′, t0)Λ(r′, r0, r”)G(r”, t0; r′, t′)⟩.
(10.86)
Now, the correlation function of ﬁeld G(r, t; r′, t′) appears in the right-hand
side of Eq. (10.86).
If we tend the temporal correlation radius of random ﬁeld f(r, t) to zero,
τ0 →0, then Eq. (10.86) is simpliﬁed and assumes, for t ≫τ0, the form of
the closed integral equation
'
G(r, t; r′, t′)
(
= g(r, t; r′, t′)

dr1

dr2

dr3
t

t′
dτg(r, t; r1, τ)Λ(r1, r2, r3)
×

dr0F(r2 −r0)

dr′

dr′g(r3, τ; r′, τ)Λ(r′, r0, r′) ⟨G(r′, τ; r′, t′)⟩,
where F(r) =
 ∞
0
dtB(r, t). This result is equivalent to the introduction of
the eﬀective correlation function of random ﬁeld f(r, t) in Eq. (10.85)
B(r, t) = 2F(r)δ(t),
F(r) =
∞

0
dtB(r, t)

338
10
Gaussian Random Field Delta-Correlated in Time
and the use of Eq. (10.83) instead of (10.82), which just corresponds to the
delta-correlated approximation for random ﬁeld f(r, t) in time.
The equation for the correlation function of solution to Eq. (10.81) can
be derived in a similar way. For short, we illustrate this derivation by the
simplest example of the one-dimensional causal equation (t > t′)
G(t; t′) = g(t; t′) + Λ
t

t′
dτg(t; τ)z(τ)G(τ; t′),
(10.87)
where we assume that z(t) is the Gaussian delta-correlated random function
with the parameters
⟨z(t)⟩= 0,
⟨z(t)z(t′)⟩= 2Dδ(t −t′)
(D = σ2
zτ0).
Averaging then Eq. (10.87) over an ensemble of realizations of random func-
tion z(t), we obtain the equation
⟨G(t; t′)⟩= g(t; t′) + Λ
t

t′
dτg(t; τ) ⟨z(τ)G(τ; t′)⟩.
(10.88)
Taking into account Eq. (10.83) that assumes here the form
δ
δz(t)G(t; t′) = g(t; t)ΛG(t; t′),
(10.89)
we can rewrite the correlation in the right-hand side of Eq. (10.88) in the
from
⟨z(τ)G(τ; t′)⟩= D
.
δ
δz(τ)G(τ; t′)
/
= ΛDg(τ; τ) ⟨G(τ; t′)⟩.
As a consequence, Eq. (10.88) grades into the closed integral equation for
average Green’s function
⟨G(t; t′)⟩= g(t; t′) + Λ2D
t

t′
dτg(t; τ)g(τ; τ) ⟨G(τ; t′)⟩,
(10.90)
which, according to the general-form derivation technique, has the form of
the Dyson equation

10.6
Causal Integral Equations
339
⟨G(t; t′)⟩= g(t; t′) + Λ
t

t′
dτg(t; τ)
τ

t′
dτ ′Q(τ; τ ′) ⟨G(τ ′; t′)⟩,
⟨G(t; t′)⟩= g(t; t′) + Λ
t

t′
dτ ⟨G(t; τ)⟩
τ

t′
dτ ′Q(τ; τ ′)g(τ ′; t′),
(10.91)
with the mass function
Q(τ; τ ′) = Λ2Dg(τ; τ)δ(τ −τ ′).
(10.92)
Derive now the equation for the correlation function
Γ(t, t′; t1, t′
1) = ⟨G(t; t′)G∗(t1; t′
1)⟩
(t > t′,
t1 > t′
1),
where G∗(t; t′) is complex conjugated Green’s function. With this goal in
view, we multiply Eq. (10.87) by G∗(t1; t′
1) and average the result over an
ensemble of realizations of random function z(t). The result is the equation
that can be symbolically represented as
Γ = g ⟨G∗⟩+ Λg ⟨zGG∗⟩.
(10.93)
Taking into account the Dyson equation (10.91)
⟨G⟩= {1 + ⟨G⟩Q}g,
we apply operator {1 + ⟨G⟩Q} to Eq. (10.93). As a result, we obtain the
symbolic-form equation
Γ = ⟨G⟩⟨G∗⟩+ ⟨G⟩Λ {⟨zGG∗⟩−QΓ} ,
which can be represented in common variables in the form
Γ(t, t′; t1, t′
1) = ⟨G(t; t′)⟩⟨G∗(t1; t′
1)⟩
+ ΛD
t

0
dτ ⟨G(t; τ)⟩
.δG(τ; t′)
δz(τ)
G∗(t1; t′
1) + 2G(τ; t′)δG∗(t1; t′
1)
δz(τ)
/
−Λ2D
t

0
dτ ⟨G(t; τ)⟩g(τ; τ)Γ(τ, t′; t1, t′
1).
(10.94)
Deriving Eq. (10.94), we used additionally Eq. (7.61), page 182 for splitting
correlators between the Gaussian delta-correlated process z(t) and functionals
of this process

340
10
Gaussian Random Field Delta-Correlated in Time
⟨z(t′)R[t; z(τ)]⟩=
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
D
.
δ
δz(t)R[t; z(τ)]
/
(t′ = t,
τ < t),
2D
.
δ
δz(t′)R[t; z(τ)]
/
(t′ < t,
τ < t).
Taking into account formulas (10.89) and (10.79) of which the latter
assumes in our case the form
δ
δz(τ)G∗(t1; t′
1) = ΛG∗(t1; τ)G∗(τ; t′
1),
we can rewrite Eq. (10.94) as
Γ(t, t′; t1, t′
1) = ⟨G(t; t′)⟩⟨G∗(t1; t′
1)⟩
+ 2|Λ|2D
t

0
dτ ⟨G(t; τ)⟩⟨G∗(t1; τ)G(τ; t′)G∗(τ; t′
1)⟩.
(10.95)
Now, we take into account the fact that function G∗(t1; τ) functionally
depends on random process z(τ) for τ ≥τ while functions G(τ; t′) and
G∗(τ; t′
1) depend on it for τ ≤τ. Consequently, these functions are sta-
tistically independent in the case of the delta-correlated process z(τ), and we
can rewrite Eq. (10.95) in the form of the closed equation (t1 ≥t)
Γ(t, t′; t1, t′
1)=
'
G(t; t′)
( '
G∗(t1; t′
1)
(
+2|Λ|2D
t

0
dτ ⟨G(t; τ)⟩⟨G∗(t1; τ)⟩Γ(τ; t′; τ; t′
1),
(10.96)
which corresponds to the Bete–Salpeter equation (8.55), page 211 with the
intensity operator kernel
K(τ1, τ′; τ2, τ′′) = 2|Λ|2Dδ(τ1 −τ ′)δ(τ2 −τ ′′)δ(τ1 −τ2).
(10.97)
Thus, for the one-dimensional causal equation (10.87), the ladder approxi-
mation appears the exact equality in the case of the delta-correlated process
z(t).

Chapter 11
Methods for Solving and Analyzing the
Fokker-Planck Equation
The Fokker–Planck equations for the one-time probability density (10.11),
page 307 and for the transitional probability density (10.16), page 309 are the
partial diﬀerential equations of parabolic type, so that we can use methods of
the theory of mathematical physics equations to solve them. In this context,
the basic methods are such as the method of separation of variables, the
Fourier transformation with respect to spatial coordinates, and other integral
transformations.
11.1
Integral Transformations
Integral transformations are very practicable for solving the Fokker–Planck
equation. Indeed, earlier we mentioned the convenience of the Fourier trans-
formation in Eq. (10.11), page (307) if the tensor of diﬀusion coeﬃcients
Fkl(x, x; t) is independent of x. Diﬀerent integral transformations related
to eigenfunctions of the diﬀusion operator
%L =
∂2
∂xk∂xl
Fkl(x, x; t)
can be used in other situations.
For example, in the case of the Legendre operator
%L = ∂
∂x(x2 −1) ∂
∂x,
it is quite natural to use the integral transformation related to the Legendre
functions. This transformation is called the Meler–Fock transform (see, e.g.,
[12]) and is deﬁned by the formula
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
341
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_11, c
⃝Springer International Publishing Switzerland 2015

342
11
Methods for Solving and Analyzing the Fokker-Planck Equation
F(μ) =
∞

1
dxf(x)P−1/2+iμ(x)
(μ > 0),
(11.1)
where P−1/2+iμ(x) is the complex index Legendre function of the ﬁrst kind,
which satisﬁes the equation
d
dx(x2 −1) d
dxP−1/2+iμ(x) = −

μ2 + 1
4

P−1/2+iμ(x).
(11.2)
The inversion of the transform (11.1) has the form
f(x) =
∞

0
dμ μ tanh(πμ)F(μ)P−1/2+iμ(x)
(1 ≤x ≤∞),
(11.3)
where F(μ) is given by formula (11.1).
Another integral transformation called the Kontorovich–Lebedev transform
(see, e.g., [12]), is related to diﬀusion operator
%L = ∂
∂xx2 ∂
∂x
and has the form
F(τ) =
∞

0
dxf(x)Kiτ(x)
(τ > 0),
(11.4)
where Kiτ(x) is the imaginary index McDonalds function of the ﬁrst kind,
which satisﬁes the equations

x2 d2
dx2 + x d
dx −x2 + τ 2

Kiτ(x) = 0,
 d
dxx2 d
dx −x d
dx

Kiτ(x) =

x2 −τ 2
Kiτ(x).
(11.5)
The corresponding inversion has the form
f(x) =
2
π2x
∞

0
dτ sinh(πτ)F(τ)Kiτ (x).
(11.6)
As a concrete example, consider the Fokker–Planck equation (x ≥1)

11.1
Integral Transformations
343
∂
∂tp(x, t|x0, t0) = D ∂
∂x(x2 −1) ∂
∂xp(x, t|x0, t0),
p(x, t0|x0, t0) = δ(x −x0).
(11.7)
Multiplying Eq. (11.7) by P−1/2+iμ(x), integrating the result over x from 1
to ∞, and introducing function
p(t, μ) =
∞

1
dxp(x, t|x0, t0)P−1/2+iμ(x),
we obtain the equation
∂
∂tp(t, μ) = D
∞

1
dxP−1/2+iμ(x) ∂
∂x(x2 −1) ∂
∂xp(x, t|x0, t0)
(11.8)
with the initial value
p(t0, μ) = P−1/2+iμ(x0).
(11.9)
Integrating two times by parts in the right-hand side of Eq. (11.8) and using
the diﬀerential Legendre equation for function P−1/2+iμ(x) (11.2), we obtain
the ordinary diﬀerential equation in p(t, μ)
d
dtp(t, μ) = −D

μ2 + 1
4

p(t, μ),
whose solution satisfying the initial value (11.9) has the form
p(t, μ) = P−1/2+iμ(x0)e−D(μ2+ 1
4)(t−t0).
Using now inversion (11.3), we obtain the solution to Eq. (11.7) in terms
of the Meler–Fock integral
p(x, t|x0, t0) =
∞

0
dμ μ tanh(πμ)e−D(μ2+ 1
4)(t−t0)P−1/2+iμ(x)P−1/2+iμ(x0).
(11.10)
If x0 = 1 at the initial instant t0 = 0, we obtain the expression
P(x, t) =
∞

0
dμ μ tanh(πμ)e−D(μ2+ 1
4)(t−t0)P−1/2+iμ(x)
(11.11)
corresponding to the solution of the Fokker–Planck equation for the one-time
probability density (11.7) with the initial value
P(x, 0) = δ(x −1).

344
11
Methods for Solving and Analyzing the Fokker-Planck Equation
11.2
Steady-State Solutions of the Fokker–Planck
Equation
In previous sections, we discussed the general methods of solving the Fokker–
Planck equation for both transition and one-time probability densities. How-
ever, the problem on the one-time probability density can have peculiarities
related to possible existence of the steady-state solution; in a number of cases,
such a solution can be obtained immediately. The steady-state solution, if it
exists, is independent of initial values and is the solution of the Fokker–Planck
equation in the limit t →∞.
There are two classes of problems for which the steady-state solution of
the Fokker–Planck equation can be easily found. These classes deal with
one-dimensional diﬀerential equations and with the Hamiltonian systems of
equations. Consider them in greater detail.
11.2.1
One-Dimensional Nonlinear Diﬀerential
Equation
The one-dimensional nonlinear systems are described by the stochastic equa-
tion
d
dtx(t) = f(x) + z(t)g(x),
x(0) = x0,
(11.12)
where z(t) is, as earlier, the Gaussian delta-correlated process with the para-
meters
⟨z(t)⟩= 0,
⟨z(t)z(t′)⟩= 2Dδ(t −t′)
(D = σ2
zτ0).
The corresponding Fokker–Planck equation has the form
 ∂
∂t + ∂
∂xf(x)

P(x, t) = D ∂
∂xg(x) ∂
∂xg(x)P(x, t).
(11.13)
The steady-state probability distribution P(x), if it exists, satisﬁes the
equation
f(x)P(x) = Dg(x) d
dxg(x)P(x)
(11.14)
(we assume that P(x) is distributed over the whole space, i.e., for −∞< x <
∞) whose solution is as follows
P(x) =
C
|g(x)| exp
 1
D

dx f(x)
g2(x)
	
,
(11.15)
where constant C is determined from the normalization condition

11.2
Steady-State Solutions of the Fokker–Planck Equation
345
∞

−∞
dxP(x) = 1.
In the special case of the Langevin equation (10.32), page 315
f(x) = −λx, g(x) = 1,
Eq. (11.15) grades into the Gaussian probability distribution
P(x) =

λ
2πD exp

−λ
2Dx2
	
.
(11.16)
11.2.2
Hamiltonian Systems
Another type of dynamic systems that allow obtaining the steady-state proba-
bility distribution is described by the Hamiltonian system with linear friction
and external random force
d
dtri(t) =
∂
∂pi
H({ri}, {pi}),
d
dtpi(t) = −∂
∂ri
H({ri}, {pi}) −λpi + f i(t),
(11.17)
where i = 1, 2, · · · , N,
H({ri}, {pi}) = p2
i
2 + U(r1, · · · , rN)
is the Hamiltonian; {ri}, {pi} denotes collections of all quantities r(t) and
p(t), i.e., {ri} = {r1, · · · , rN}, {pi} = {p1, · · · , pN}; λ is a constant coeﬃcient
(friction), and random forces fi(t) are the Gaussian delta-correlated random
vector functions with the correlation tensor
2
f α
i (t)f β
j (t′)
3
= 2Dδijδαβδ(t −t′),
D = σ2
fτ0.
(11.18)
Here, α and β are the vector indices.
System of equations (11.17) describes the Brownian motion of a system of
N interacting particles. The corresponding indicator function ϕ({ri}, {pi}, t)
satisﬁes the equation
∂
∂tϕ({ri}, {pi}, t) +
N
&
k=1
{H, ϕ}(k) −λ
N
&
k=1
∂
∂pk
{pkϕ} =
N
&
k=1
∂
∂pk
{f k(t)ϕ},

346
11
Methods for Solving and Analyzing the Fokker-Planck Equation
which is an extension of Eq. (3.12), page 98 and, hence, the Fokker–Planck
equation for the joint probability density of the solution to system (11.17)
has the form
∂
∂tP({ri}, {pi}, t) +
N
&
k=1
{H, P({ri}, {pi}, t)}(k)
−λ
N
&
k=1
∂
∂pk
{pkP({ri}, {pi}, t)} = D
N
&
k=1
∂2
∂p2
k
P({ri}, {pi}, t),
(11.19)
where
{ϕ, ψ}(k) = ∂ϕ
∂pk
∂ψ
∂rk
−∂ψ
∂pk
∂ϕ
∂rk
is the Poisson bracket for the k-th particle.
One can easily check that the steady-state solution to Eq. (11.19) is the
canonical Gibbs distribution
P({ri}, {pi}) = C exp

−λ
DH({ri}, {pi})
	
.
(11.20)
The speciﬁcity of this distribution consists in the Gaussian behavior with
respect to momenta and statistical independence of particle coordinates and
momenta.
Integrating Eq. (11.20) over all r, we can obtain the
Maxwell distribu-
tion that describes velocity ﬂuctuations of the Brownian particles. The case
U(r1, · · · , rN) = 0 corresponds to the Brownian motion of a system of free
particles (11.16).
If we integrate probability distribution (11.20) over momenta (velocities),
we obtain the Boltzmann distribution of particle coordinates
P({ri}) = C exp

−λ
DU({ri})
	
.
(11.21)
In the case of suﬃciently strong friction, the equilibrium distribution
(11.20) is formed in two stages. First, the Gaussian momentum distribution
(the Maxwell distribution) is formed relatively quickly and then, the spatial
distribution (the Boltzmann distribution) is formed at much slower rate. The
latter stage is described by the Fokker–Planck equation
∂
∂tP({ri}, t) = 1
λ
N
&
k=1
∂
∂rk
∂U({ri})
∂rk
+ D
λ
∂
∂rk

P({ri}, t)

,
(11.22)
which is usually called the Einstein–Smolukhovsky equation . Derivation of
Eq. (11.22) from the Fokker–Planck equation (11.19) is called the Kramers
problem (see, e.g., [131] and the corresponding discussion in Sect. 5.4.1, where

11.2
Steady-State Solutions of the Fokker–Planck Equation
347
dynamics of particles under a random force is considered as an example). Note
that Eq. (11.22) statistically corresponds to the stochastic equation
d
dtri(t) = −1
λ
∂
∂ri
U({ri}) + 1
λf i(t),
(11.23)
which, nevertheless, cannot be considered as the limit of Eq. (11.17) for λ →
∞.
In the one-dimensional case, Eqs. (11.17) are simpliﬁed and assume the
form of the system of two equations
d
dtx(t) = y(t),
d
dty(t) = −∂
∂xU(x) −λy(t) + f(t).
(11.24)
The corresponding steady-state probability distribution has the form
P(x, y) = C exp

−λ
DH(x, y)
	
,
H(x, y) = y2
2 + U(x).
(11.25)
11.2.3
Systems of Hydrodynamic Type
In Sect. 1.1.3, page 13, we considered general dynamics of simplest
hydrodynamic-type systems (HTS). Now, we consider these systems in terms
of the statistical description.
Hydrodynamic-type systems with linear friction are described by the dyna-
mic equations
d
dtvi(t) = Fi(v) −λ(i)vi(t)
(i = 1, · · · , N),
(11.26)
where λ(i) is the friction coeﬃcient of the i-th component of the N-
dimensional vector v 1 and Fi(v) is the function quadratic in v and having
the following properties:
(a) viFi(v) = 0,
energy conservation holds at λ(i) = 0: d
dtE(t) = 0, E(t) = v2
i (t)
2
;
(b)
∂
∂vi
Fi(v) = 0,
1 In the general case, the dissipative term in Eq. (11.26) has the form λikvk. How-
ever, we can always choose the coordinate system in which two positively deﬁned
quadratic forms—energy E = v2
i /2 and dissipation λikvivk—have the diagonal
representation. The form of Eq. (11.26) assumes the use of namely such a coor-
dinate system.

348
11
Methods for Solving and Analyzing the Fokker-Planck Equation
conditions of the Liouville theorem are satisﬁed at λ(i) = 0, and this equality
is the equation of incompressibility in the phase space.
Equilibrium Thermal Fluctuations in the Hydrodynamic-Type
Systems
Here, we dwell on a class of phenomena closely related to the Brownian
motion; namely, we dwell on equilibrium thermal ﬂuctuations in solids.
Microscopic equations describe the behavior of physical systems only in
terms of spatial scales large in comparison with the molecule free path in
the medium and temporal scales great in comparison with the time between
molecule collisions. This means that macroscopic equations adequately des-
cribe the behavior of systems only on average. However, in view of molecule
thermal motion, macroscopic variables are in general terms stochastic vari-
ables, and a complete macroscopic theory must describe not only system
behavior on average, but also the ﬂuctuations around the average.
Such a description can be performed in terms of macroscopic variables
by supplementing the corresponding macroscopic equations with the ’ex-
ternal forces’ speciﬁed as the Gaussian random ﬁelds delta-correlated in
time (this approach is closely related to the ﬂuctuation–dissipation theorem).
See [81,84,87] for the corresponding correlation theories of equilibrium ther-
mal ﬂuctuations in electrodynamics, hydrodynamics, and viscoelastic media.
In HTS, equilibrium thermal ﬂuctuations are described by Eq. (11.26) sup-
plemented with external forces fi(t)
d
dtvi(t) = Fi(v) −λ(i)vi(t) + fi(t)
(i = 1, · · · , N).
(11.27)
External forces are assumed to be the Gaussian random functions delta-
correlated in time with the correlation tensor of the form
⟨fi(t)fj(t′)⟩= 2δijσ2
(i)δ(t −t′)
(⟨f(t)⟩= 0).
(11.28)
Note that one can consider Eqs. (11.27) as the system describing the Brow-
nian motion in HTS. Such a treatment assumes that coeﬃcients λ(i) are cer-
tain eﬀective friction coeﬃcients. For example, at N = 3, system (11.27)
describes (in the velocity space) the rotary Brownian motion of a solid in a
medium, and quantities λ(i)vi(t) play the role of the corresponding resistance
forces. The probability density of solution v(t) to Eqs. (11.27), i.e., function
P(v, t) = ⟨δ(v(t) −v)⟩, satisﬁes the Fokker–Planck equation
∂
∂tP(v, t) = −∂
∂vi
"
[Fi(v) −λ(i)vi]P(v, t)
#
+ σ2
(i)
∂2
∂v2
i
P(v, t).
(11.29)

11.2
Steady-State Solutions of the Fokker–Planck Equation
349
The steady-state, initial data-independent solution to Eq. (11.29) must be-
have like the Maxwell distribution that corresponds to the uniform distribu-
tion of energy over the degrees of freedom
P(v) = C exp

−v2
i
2kT
	
,
(11.30)
where k is the Boltzmann constant and T is the equilibrium temperature in
the system.
Substituting Eq. (11.30) in Eq. (11.29), we obtain the expression for σ2
(i)
σ2
(i) = λ(i)kT
(11.31)
called the Einstein formula. Here, we used that, in view of conditions (a)
and (b),
∂
∂vi
{Fi(v)P(v)} = 0.
Thus, Eqs. (11.28) and (11.31) completely determine statistics of external
forces, and the nonlinear term in Eq. (11.27) plays no role for one-time ﬂuc-
tuations of v(t). Namely this feature constitutes the subject matter of the
ﬂuctuation–dissipation theorem (see [84,87]) in the context of HTS.
Note that Eqs. (11.27) with the correlation relationships (11.28) describe
not only equilibrium thermal ﬂuctuations in HTS, but also the interaction of
small-scale motions (such as microturbulence, for example) with the motions
of larger scales. If such an HTS allows the description in terms of phenomeno-
logical equations (11.27) with λ(i) = λ = const, σ2
(i) = σ2 = const, the steady-
state probability distribution of v will have the form similar to distribution
(11.30):
P(v) = C exp

−λ
2σ2 v2
i
	
.
(11.32)
Noises in Hydrodynamic-Type Systems under the Action of a
Regular Force
We illustrate statistical description of HTS by the example of the simplest
system with three degrees of freedom (S3), which is described by system of
equations (1.25), page 15:
d
dtv0(t) = v2
2(t) −v2
1(t) −v0(t) + R + f0(t),
d
dtv1(t) = v0(t)v1(t) −v1(t) + f1(t),
d
dtv2(t) = −v0(t)v2(t) −v2(t) + f2(t).
(11.33)

350
11
Methods for Solving and Analyzing the Fokker-Planck Equation
We assume that fi(t) are the Gaussian random forces delta-correlated in time
with the correlation tensor
⟨fi(t)fj(t′)⟩= 2δijσ2δ(τ).
In the absence of external forces, this system is equivalent to the Euler
equations of the dynamics of a gyroscope with isotropic friction, which is
excited by a constant moment of force relative to the unstable axis. The
steady-state solution to this system depends on parameter R (an analog to
the Reynolds number), and the critical value of this parameter is Rcr = 1.
For R < 1, it has the stable steady-state solution
v1 s-s = vs-s = 0,
v0 s-s = R.
(11.34)
For R > 1, this solution becomes unstable with respect to small distur-
bances of parameters, and new steady-state regime is formed:
v0 s-s = 1,
v2 s-s = 0,
v1 s-s = ±
√
R −1.
(11.35)
Here, we have an element of randomness because quantity v1 s-s can be either
positive or negative, depending on the amplitude of small disturbances.
As was mentioned earlier, the steady-state probability distribution at R =
0 has the form
P(v) = C exp

−v2
i
2σ2
	
.
(11.36)
Let now R ̸= 0 and R < 1. In this case, we can easily obtain that ﬂuc-
tuations of components relative to their steady-state values (vi = vi −vi s-s)
satisfy the system of equations
d
dtv0(t) = v2
2(t) −v2
1(t) −v0(t) + f0(t),
d
dtv1(t) = v0(t)v1(t) −(1 −R)v1(t) + f1(t),
d
dtv2(t) = −v0(t)v2(t) −(1 + R)v2(t) + f2(t).
(11.37)
Statistical characteristics of the solution to system (11.37) can be deter-
mined using the perturbation theory with respect to small parameter σ2. The
second moments of components vi will then be described by the linearized
system of equations (11.37); the second moment being determined, mean val-
ues can be obtained by averaging system (11.37) directly. The corresponding
steady-state variances of ﬂuctuations vi have the form [14,24,46]
,
v2
0(t)
-
= σ2,
,
v2
1(t)
-
=
σ2
1 −R,
,
v2
2(t)
-
=
σ2
1 + R.
(11.38)
Note that expressions (11.38) hold for R ≪1. With increasing R, both in-
tensity and temporal correlation radius of component v1 increase, while the

11.2
Steady-State Solutions of the Fokker–Planck Equation
351
intensity of quantity v2 decreases. In this process, maximum ﬂuctuations of
quantity v1 occur when the dynamic system gets over the critical regime.
Consider now the case of R > 1. The steady-state probability distribution
of component v1 has two maxima near v1 = ±
√
R −1 (they correspond to
the stable steady-state positions) and a minimum v1 = 0 that corresponds
to the unstable position. This probability distribution is formed by ensemble
averaging over realizations of random forces fi(t). In a single realization,
the system arrives with a probability of 1/2 at one of the stable positions
corresponding to distribution maxima. In this case, averaging over time or
ensemble averaging over the forces that bring the system to this state will
form the probability distribution near the maximum, and we can determine
statistical characteristics of the solution using the perturbation theory in
parameter σ2.
Let the system arrives at the state corresponding to the stable steady-state
position v2 s-s = 0, v1 s-s =
√
R −1. Then, ﬂuctuations relative to this state
will satisfy the system of equations
d
dtv0(t) = v2
2(t) −v2
1(t) −2
√
R −1v1(t) −v0(t) + f0(t),
d
dtv1(t) = v0(t)v1(t) +
√
R −1v0(t) + f1(t),
d
dtv2(t) = −v0(t)v2(t) −2v2(t) + f2(t).
(11.39)
For R ≫1, the second moments of ﬂuctuations vi are obtained from the
linearized system (11.39) and mean values are then determined by direct
averaging of Eqs. (11.39). Using this procedure, we obtain that, for R ≫1,
the steady-state variances and correlation coeﬃcients of triplet components
have the form
⟨v0(t)v2(t)⟩= ⟨v1(t)v2(t)⟩= 0
⟨v0(t)v1(t)⟩= −σ2
√
R
,
v2
0(t)
-
= 3σ2,
,
v2
1(t)
-
= 3
2σ2,
,
v2
2(t)
-
= σ2
2 .
(11.40)
As we mentioned earlier, these statistical characteristics correspond either
to averaging over time, or to ensemble averaging over realizations of forces
fi(t) that bring the system to the steady-state position. It should be noted
that if the system has arrived at one of the most probable states under the
action of certain realization of forces fi(t), it will be transferred into another
most probable state after the lapse of certain time T (the greater T , the less
σ2 and the greater R) in view of availability of suﬃciently great values of
fi(t). This process simulated for a realization of random forces is shown in
Fig. 1.5 on page 16.

352
11
Methods for Solving and Analyzing the Fokker-Planck Equation
Consider now ﬂuctuations of components vi(t) at critical regime (i.e., at
R = 1). Equations for the ﬂuctuations relative to the state v1 ст = v2 ст = 0,
v0 ст = R can be obtained by setting R = 1 either in Eq. (11.37), or in
Eq. (11.39). In this case, ﬂuctuations of component v2(t) can be described in
terms of the linearized equation (σ2 ≪1), and the corresponding steady-state
variance is given by the formula
,
v2
2(t)
-
= σ2
2 .
Components v0(t) and v1(t) are described by the nonlinear system of equa-
tions
d
dtv0(t) = −v2
1(t) −v0(t) + f0(t),
d
dtv1(t) = v0(t)v1(t) + f1(t),
(11.41)
and no linearization is allowed here.
Averaging system (11.41), we obtain the following relationships for steady-
state values of ﬂuctuations
⟨v0(t)v1(t)⟩= 0,
⟨v0(t)⟩= −
,
v2
1(t)
-
.
As we noted earlier, intensity of ﬂuctuations v1 increases in the critical
regime and, consequently, quantity ⟨v0(t)⟩increases. At the same time, the
variance of ﬂuctuation v0(t) remains nearly intact. Indeed, as follows from
Eqs. (11.41), the variance of steady-state ﬂuctuation v0(t) is given by the
formula
,
v2
0(t)
-
= 2σ2.
It becomes clear that ﬂuctuations v0(t) and v1(t) can be estimated from the
simpliﬁed system
d
dtv0(t) = −v2
1(t) −v0(t) + f0(t),
d
dtv1(t) = ⟨v0(t)⟩v1(t) + f1(t)
obtained by the replacement of quantity v0(t) in the second equation of sys-
tem (11.41) by ⟨v0(t)⟩.
Using this system, we obtain that steady-state ﬂuctuations v0(t) and v1(t)
satisfy the expressions
,
v2
1(t)
-
= −⟨v0(t)⟩= σ.
In a similar way, we obtain the estimate of the temporal correlation radius
of quantity v1(t):
τ ∼1/σ.

11.2
Steady-State Solutions of the Fokker–Planck Equation
353
Earlier, we considered the behavior of a triplet under the assumption that
random forces act on all variables. Let us see what will be changed if only one
unstable component v0(t) is aﬀected by the random force (we assume that the
force acting on component v0(t) has regular (R) and random components).
In this case, the dynamic system assumes the form (R > 1)
d
dtv0(t) = −v2
1(t) −v0(t) + R + f(t),
d
dtv1(t) = v0(t)v1(t) −v1(t).
(11.42)
In system (11.42), we omitted the terms related to component v2(t), because
one can easily see that it will not be excited in the problem at hand.
Represent component v0(t) as v0(t) = 1 + v0(t). The system of equations
(11.42) assumes then the form
d
dtv0(t) = −v2
1(t) −v0(t) + (R −1) + f(t),
d
dtv1(t) = v0(t)v1(t),
(11.43)
from which follows that temporal evolution of component v1(t) depends on
its initial value.
If v1(0) > 0, then v1(t) > 0 too. In this case, we can represent v1(t) in the
form
v1(t) = eϕ(t)
and rewrite the system of equations (11.43) in the Hamiltonian form (11.24)
d
dtv0(t) = −∂U(ϕ)
∂ϕ
−v0(t) + f(t),
d
dtϕ(t) = v0(t),
(11.44)
where U(ϕ) = 1
2e2ϕ−(R −1) ϕ. Here, variable ϕ(t) plays the role of particle’s
coordinate and variable v0(t) plays the role of particle’s velocity.
The solid line in Fig. 11.1 shows the behavior of function U(ϕ). At point
ϕ0 = ln
√
R −1, this function has a minimum
U(ϕ0) = 1
2(R −1) [1 −ln(R −1)]
corresponding to the stable equilibrium state v1 =
√
R −1. Thus, the steady-
state probability distribution of ϕ(t) and v0(t) is similar to the Gibbs distri-
bution (11.25)
P(v0, ϕ) = C exp

−1
DH(v0, ϕ)
	
,
H(v0, ϕ) = v2
0
2 + U(ϕ).
(11.45)

354
11
Methods for Solving and Analyzing the Fokker-Planck Equation
ϕ
U(ϕ)
U(ϕ0)
ϕ0
Fig. 11.1 Potential function U(ϕ). The dashed lines show curve U(ϕ) = 1
2 exp{2ϕ}
and straight line U(ϕ) = −(R −1)ϕ.
From Eq. (11.45) follows that, for R > 1, the steady-state probability dis-
tribution is composed of two independent steady-state distributions, of which
the distribution of component v0(t) of system (11.42) is the Gaussian distri-
bution
P(v0) =
1
√
2πD
exp

−(v0 −1)2
2D
	
,
(11.46)
and the distribution of quantity ϕ(t) is the non-Gaussian distribution. If
we turn back to variable v1(t), we obtain the corresponding steady-state
probability distribution in the form
P(v1) = const v
R−1
D −1
1
exp

−v2
1
2D
	
.
(11.47)
As may be seen from Eq. (11.47), no steady-state probability distribution
exists for component v1(t) in critical regime(R = 1), which contrasts with
the above case when random forces acted on all components of the triplet.
Earlier, we mentioned that two stages are characteristic of the formation
of distribution (11.45). First, the Maxwellian distribution with respect to
v0 (11.46) is rapidly formed and the distribution with respect to ϕ is then
formed at a much slower rate. The second stage is described by the Einstein–
Smolukhovsky equation (11.22) that, in terms of variable v1, assumes the
form
∂
∂tP(v1, t) =
∂
∂v1

v1

v2
1 −(R −1)

+ σ2v1
∂
∂v1
v1
	
P(v1, t).
(11.48)

11.3
Boundary-Value Problems for the Fokker-Planck Equation
355
This equation is the Fokker–Planck equation for the stochastic dynamic equa-
tion
d
dtv1(t) = −v1(t)

v2
1(t) −(R −1)

+ v1(t)f(t).
At critical regime R = 1, this equation assumes the form
d
dtv1(t) = −v3
1(t) + v1(t)f(t),
from which follows that, despite strong nonlinear friction, its solution is
stochastically unstable due to the speciﬁc form of the random term. Stochas-
tic stabilization occurs, as we saw earlier, due to inclusion of random forces
into the equations for other components.
11.3
Boundary-Value Problems for the Fokker-Planck
Equation (Hopping Phenomenon)
The Fokker–Planck equations are the partial diﬀerential equations and they
generally require boundary conditions whose particular form depends on the
problem under consideration. One can proceed from both forward and back-
ward Fokker–Planck equations, which are equivalent. Consider several exam-
ples.
11.3.1
Hopping Phenomenon in Regular Systems
Consider the nonlinear oscillator with friction described by the equation
d2
dt2 x(t) + λ d
dtx(t) + ω2
0x(t) + βx3(t) = f(t)
(β, λ > 0)
(11.49)
and assume that random force f(t) is the delta-correlated random function
with the parameters
⟨f(t)⟩= 0,
⟨f(t)f(t′)⟩= 2Dδ(t −t′)
(D = σ2
fτ0).
At λ = 0 and f(t) = 0, this equation is called the Duﬃng equation .
We can rewrite Eq. (11.49) in the standard form of the Hamiltonian system
in functions x(t) and v(t) = d
dtx(t),
d
dtx(t) = ∂
∂v H(x, v),
d
dtv(t) = −∂
∂xH(x, v) −λv + f(t),

356
11
Methods for Solving and Analyzing the Fokker-Planck Equation
where
H(x, v) = v2
2 + U(x),
U(x) = ω2
0x2
2
+ β x4
4
is the Hamiltonian.
According to Eq. (11.25), the steady-state solution to the corresponding
Fokker–Planck equation has the form
P(x, v) = C exp

−λ
DH(x, v)
	
.
(11.50)
It is clear that this distribution is the product of two independent distribu-
tions, of which one — the steady-state probability distribution of quantity
v(t) — is the Gaussian distribution and the other — the steady-state proba-
bility distribution of quantity x(t) — is the non-Gaussian distribution. Integ-
rating Eq. (11.50) over v, we obtain the steady-state probability distribution
of x(t)
P(x) = C exp

−λ
D
ω2
0x2
2
+ β x4
4
	
.
This distribution is maximum at the stable equilibrium point x = 0.
Consider now the equation
d2
dt2 x(t) + λ d
dtx(t) −ω2
0x(t) + βx3(t) = f(t)
(β, λ > 0).
(11.51)
In this case again, the steady-state probability distribution has the form
(11.50), where now
H(x, v) = v2
2 + U(x),
U(x) = −ω2
0x2
2
+ β x4
4 .
The steady-state probability distribution of x(t) assumes now the form
P(x) = C exp

−λ
D

−ω2
0x2
2
+ β x4
4
	
(11.52)
P(x)
−

ω2
0/β

ω2
0/β
x
Fig. 11.2 Probability distribution (11.52)

11.3
Boundary-Value Problems for the Fokker-Planck Equation
357
and has maxima at points
x = ±
>
ω2
0/β
and a minimum at point x = 0; the maxima correspond to the stable equi-
librium points of problem (11.51) for f(t) = 0 and the minimum, to the
instable equilibrium point. Figure 11.2 shows the behavior of the probability
distribution (11.52).
As we mentioned earlier, the formation of distribution (11.52) is described
by the Einstein–Smolukhovsky equation (11.22), which has in this case the
form
∂
∂tP(x, t) = 1
λ
∂
∂x
∂U(x)
∂x
P(x, t)

+ 1
λ2
∂2
∂x2 P(x, t).
(11.53)
This equation is statistically equivalent to the dynamic equation
d
dtx(t) = −1
λ
∂U(x)
∂x
+ 1
λf(t).
(11.54)
Probability distribution (11.52) corresponds to averaging over an ensemble
of realizations of random process f(t). If we deal with a single realization,
the system arrives at one of states corresponding to the distribution maxima
with a probability of 1/2. In this case, averaging over time will form the
probability distribution around the maximum position. However, after a lapse
of certain time T (the longer, the smaller D), the system will be transferred
in the vicinity of the other maximum due to the fact that function f(t) can
assume suﬃciently large values. For this reason, temporal averaging will form
probability distribution (11.52) only if averaging time t ≫T .
Introducing dimensionless coordinate x →
$
ω2
0
β x and time t →λ
ω2
0
t, we
can rewrite Eq. (11.53) in the form
∂
∂tP(x, t) = ∂
∂x
∂U(x)
∂x
P(x, t)

+ μ ∂2
∂x2 P(x, t),
(11.55)
where
μ = βD
λω4
0
,
U(x) = −x2
2 + x4
4 .
In this case, the equivalent stochastic equation (11.54) assumes the form of
Eq. (1.17), page 10
d
dtx(t) = −∂U(x)
∂x
+ f(t).
(11.56)
Estimate the time required for the system to switch from a most probable
state x = −1 to the other x = 1.

358
11
Methods for Solving and Analyzing the Fokker-Planck Equation
Let the system described by stochastic equation (11.56) was at a point
from the interval (a, b) at instant t0. The corresponding probability for the
system to leave this interval
G(t; x0, t0) = 1 −
b

a
dxp(x, t|x0, t0)
satisﬁes Eq. (10.21) following from the backward Fokker–Planck equation
(10.20), page 310, i.e., the equation
∂
∂t0
G(t; x0, t0) = ∂U(x0)
∂x0
∂
∂x0
G(t; x0, t0) −μ ∂2
∂x2
0
G(t; x0, t0)
with the boundary conditions
G(t; x0, t) = 0,
G(t; a, t0) = G(t; b, t0) = 1.
Taking into account the fact that G(t; x0, t0) = G(t −t0; x0) in our problem,
we can denote (t −t0) = τ and rewrite the boundary-value problem in the
form
∂
∂τ G(τ; x0) = ∂U(x0)
∂x0
∂
∂x0
G(τ; x0) −μ ∂2
∂x2
0
G(τ; x0),
G(0; x0) = 0, G(τ; a, ) = G(τ; b) = 1

lim
τ→∞G(τ; x0) = 0

.
(11.57)
From Eq. (11.57), one can easily see that average time required for the
system to leave interval (a, b)
T (x0) =
∞

0
dτ τ ∂G(τ; x0)
∂τ
satisﬁes the boundary-value problem
μd2T (x0)
dx2
0
−dU(x0)
dx0
dT (x0)
dx0
= −1,
T (a) = T (b) = 0.
(11.58)
Equation (11.58) can be easily solved, and we obtain that the average time
required for the system under random force to switch its state from x0 = −1
to x0 = 1 (this time is usually called the Kramers time ) is given by the
expression

11.3
Boundary-Value Problems for the Fokker-Planck Equation
359
T = 1
μ
1

−1
dξ
ξ

−∞
dη exp
 1
μ [U(ξ) −U(η)]
	
= C(μ)
μ
1

0
dξ exp
 1
μU(ξ)
	
,
(11.59)
where C(μ) =
∞

−∞
dξ exp
 1
μU(ξ)
	
. For μ ≪1, we obtain
T ≈
√
2π exp
 1
4μ
	
,
i.e., the average switching time increases exponentially with decreasing the
intensity of ﬂuctuations of the force.
Remark 11.1. Stochastic Resonance.
In addition to the Duﬃng stochastic equation (11.51), a great attention is
given recently to the equation
d2
dt2 x(t) + λ d
dtx(t) −ω2
0x(t) + βx3(t) = f(t) + A cos ω0t
(β, λ > 0)
and, in particular, to the eﬀect of an additional (except the noise) periodic im-
pact on the statistical characteristics of the solution to Eq. (11.51) (see, e.g.,
reviews [5] and [40]). In this case, there sometimes occurs the phenomenon
commonly called the stochastic resonance. In the context of this problem, the
physical meaning of the term ’resonance’ diﬀers from the generally accepted
one. Here, it reﬂects the fact that the response of the nonlinear stochastic
oscillator on an external action appears a non-monotonous (i.e., ’resonance’)
function of the intensity of stochastic noise f(t). In the case of the above
problem, such a stochastic resonance occurs if the periodic signal frequency
ω0 coincides with the frequency of system switching between two stable states
ω ∼1/T , which is called the Kramers frequency .
♦
11.3.2
Hopping Phenomena in Singular Systems
Consider now the singular stochastic problem described by Eq. (1.31),
page 20. We rewrite this equation in the form (λ = 1)
d
dtx(t) = −x2(t) + f(t),
x(0) = x0,
(11.60)
where we assume as earlier that random process f(t) is the Gaussian delta-
correlated process with the parameters
⟨f(t)⟩= 0,
⟨f(t)f(t′)⟩= 2Dδ(t −t′)
(D = σ2
fτ0).

360
11
Methods for Solving and Analyzing the Fokker-Planck Equation
In the absence of ﬂuctuations, the solution to Eq. (11.60) has the form
x(t) =
1
t −t0
,
t0 = −1
x0
.
If x0 > 0, the solution monotonously tends to zero. But if x0 < 0, the
solution arrives at the inﬁnite value within a ﬁnite time t0.
The solution of the statistical problem (11.60) is described by the forward
and backward Fokker–Planck equations (t −t0 = τ)
∂
∂τ p(x, τ|x0) = ∂
∂xx2p(x, τ|x0) + D ∂2
∂x2 p(x, τ|x0),
∂
∂τ p(x, τ|x0) = −x2
0
∂
∂x0
p(x, τ|x0) + D ∂2
∂x2
0
p(x, τ|x0).
(11.61)
Note that respective dimensions of quantities x, p(x, τ|x0) and D are
[x] = τ−1,
[D] = τ −3,
[p] = τ.
Consequently, we can reduce Eq. (11.61) to the following dimensionless form
∂
∂τ p(x, τ|x0) = ∂
∂xx2p(x, τ|x0) + ∂2
∂x2 p(x, τ|x0),
∂
∂τ p(x, τ|x0) = −x2
0
∂
∂x0
p(x, τ|x0) + ∂2
∂x2
0
p(x, τ|x0).
(11.62)
Now, we must formulate boundary conditions to Eq. (11.62). Two types of
boundary conditions are of the ﬁrst-hand interest.
Boundary conditions of the ﬁrst type correspond to the assumption that
curve x(t) stops at point t0 where it becomes equal to −∞. This means that
probability ﬂux density
J(τ, x) = x2p(x, τ|x0) + ∂
∂xp(x, τ|x0)
(11.63)
must vanish for x →∞, i.e.,
J(τ, x) →0,
for
x →∞;
p(x, τ|x0) →0,
for
x →−∞.
In this case, quantity
G(τ|x0) =
 ∞
−∞
dx p(x, τ|x0) ̸= 1

11.3
Boundary-Value Problems for the Fokker-Planck Equation
361
is the probability of the event that function x(t) remain ﬁnite along the whole
axis (−∞, ∞); in other words, this quantity is the probability of the absence
of singular point at instant t:
G(τ|x0) = P(t < t0).
Consequently, the probability of the appearance of singular point at instant
t is given by the equality
P(t > t0) = 1 −
∞

−∞
dxp(x, τ|x0),
and the corresponding probability density
p(τ|x0) = ∂
∂tP(t > t0) = −∂
∂τ
∞

−∞
dxp(x, τ|x0)
(11.64)
satisﬁes the equation
∂
∂τ p(τ|x0) = −x2
0
∂
∂x0
p(τ|x0)+ ∂2
∂x2
0
p(τ|x0),
lim
τ→0, τ→∞p(τ|x0) →0 (11.65)
following from the backward Fokker–Planck equation (11.62).
Estimate the average time
⟨T (x0)⟩=
∞

0
τdτp(τ|x0)
during which the system switches from state x0 to state (−∞). This time is
described by the equation following from Eq. (11.65)
−1 = −x2
0
d
dx0
⟨T (x0)⟩+ d2
dx2
0
⟨T (x0)⟩
(11.66)
the boundary conditions to which are formulated as ⟨T (x0)⟩→
0 for
x0 →−∞and ﬁniteness of ⟨T (x0)⟩for x0 →∞. This equation can be
easily integrated, and the result has the form
⟨T (x0)⟩=
x0

−∞
dξ
∞

ξ
dη exp
1
3

ξ3 −η3	
.
(11.67)
From Eq. (11.67), we obtain for the average time of switching between two
singular points (x0 →∞)

362
11
Methods for Solving and Analyzing the Fokker-Planck Equation
⟨T (∞)⟩= √π 121/6
3
Γ
1
6

≈4.976.
Additionally, we note that quantity
⟨T (0)⟩= 2
3 ⟨T (∞)⟩
is the average time of switching from state x0 = 0 to state x0 = −∞.
Drastically diﬀerent boundary conditions appear under the assumption
that function x(t) is discontinuous and deﬁned for all times t. If we assume
additionally that function value −∞at instant t →t0 −0 is immediately fol-
lowed by value ∞at instant t →t0+0, the boundary condition to Eq. (11.62)
will be the condition of continuity of probability density ﬂux (11.63), i.e., the
condition
J(τ, x)|x=−∞= J(τ, x)|x=+∞.
In this case, the steady-state probability density exists and is independent of
x0,
P(x) = J
x

−∞
dξ exp
1
3

ξ3 −x3	
,
(11.68)
where
J =
1
⟨T (∞)⟩
is the steady-state probability ﬂux density .
From (11.68) follows the asymptotic formula
P(x) ≈
1
⟨T (∞)⟩x2
(11.69)
for great x. This asymptotic is formed by discontinuities of function x(t).
Indeed, function x(t) behaves near the discontinuity as
x(t) =
1
t −tk
,
and the eﬀect of randomness appears insigniﬁcant. In this case, we have for
suﬃciently great t (t ≫⟨T (∞)⟩) and x
p(x, t|x0) =
∞
&
k=0
.
δ

x −
1
t −tk
/
= 1
x2
∞
&
k=0
⟨δ (t −tk)⟩
=
1
2πx2
∞

−∞
dωe−iωt
∞
&
k=0
,
eiωtk-
=
1
2πx2
∞

−∞
dωe−iωt
Φ0(ω)
1 −Φ(ω),

11.4
Asymptotic and Approximate Methods
363
where Φ0(ω) =
,
eiωt0-
is the characteristic function of the ﬁrst singular point,
and Φ(ω) =
,
eiωT -
is the characteristic function of the temporal interval
between the singularities. As a result, for t →∞, we obtain the asymptotic
expression
P(x) = −
1
2πix2 ⟨T (∞)⟩
∞

−∞
dωe−iωt
1
ω + i0 = J
x2 ,
coincident with Eq. (11.69).
11.4
Asymptotic and Approximate Methods of
Solving the Fokker-Plank Equation
If parameter ﬂuctuations of the dynamic system are suﬃciently small, the
Fokker–Planck equation can be analyzed using diﬀerent asymptotic and ap-
proximate methods. Consider in more detail three methods used in statistical
analysis most actively.
11.4.1
Asymptotic Expansion
First of all, one can formulate some convergence method with respect to small
parameters related to ﬂuctuating quantities. This is the standard procedure
for partial diﬀerential equations in which the small parameter appears as a
factor of the highest derivative. The schematic of such a method is as follows
(see, e.g., [21]).
Rewrite the Fokker–Planck equation in the form
∂
∂tP(x, t) + A(x, t; ε)P(x, t) + Bi(x, t; ε) ∂
∂xi
P(x, t)
= ε2Dij(x, t; ε)
∂2
∂xi∂xj
P(x, t),
P(x, 0) = p0(x),
(11.70)
where we introduced parameter ε2 that characterizes the intensity of ﬂuctua-
tions of dynamic system parameters. Representing the solution to Eq. (11.70)
in the form
P(x, t) = C(ε) exp

−1
ε2 φ(x, t; ε)
	
,
(11.71)
we obtain the nonlinear equation for function φ(x, t; ε)

364
11
Methods for Solving and Analyzing the Fokker-Planck Equation
∂
∂tφ(x, t; ε) −ε2A(x, t; ε) + Bi(x, t; ε) ∂
∂xi
φ(x, t; ε)
−ε2Dij(x, t; ε)
∂2
∂xi∂xj
φ(x, t; ε)
+ Dij(x, t; ε)
 ∂
∂xi
φ(x, t; ε)
  ∂
∂xj
φ(x, t; ε)

= 0,
(11.72)
whose solution can be sought in the form of the series in powers of ε2
φ(x, t; ε) = φ0(x, t) + ε2φ1(x, t) + · · · .
To derive the convergence method for Eq. (11.72), we substitute this ex-
pansion in Eq. (11.72), expand the equation coeﬃcients in series in ε2, and
group the terms with the corresponding powers of ε2. In particular, for func-
tion φ0(x, t), we obtain the equation
 ∂
∂t + Bi(x, t; 0) ∂
∂xi

φ0(x, t) + Dij(x, t; 0)
 ∂
∂xi φ0(x, t)
  ∂
∂xj φ0(x, t)

= 0,
which is the ﬁrst-order partial diﬀerential equation and can be solved by the
method of characteristics, for example. Function φ0(x, t) is the ﬁrst term
of the convergence method series; it describes the main singularity of the
Fokker–Planck equation. The next term φ1(x, t) describes the preexponential
factor, and constant C(ε2) in (11.71) can be obtained from the behavior of
the solution to Eq. (11.70) for t →0 and the corresponding initial value.
This convergence method holds only for a ﬁnite-duration initial stage of
evolution and fails in the limit t →∞. To analyze Eq. (11.70) in this limit,
this equation is usually rearranged to the form containing the self-adjoint
operator with respect to spatial variables, which has the discrete spectrum.
Consider now two approximate methods of solving the Fokker–Planck
equation.
11.4.2
Method of Cumulant Expansions
The ﬁrst method is called the method of cumulant expansions [99]. If we per-
form the Fourier transform of the Fokker–Planck equation (10.11), page 307
with respect to spatial variables x, i.e., turn from the probability density
of the solution to stochastic equations (10.1), page 305 to the characteristic
function
Φ(λ, t) =
2
eiλx(t)3
= eΘ(λ,t)
(11.73)
and expand this function in the Taylor series in powers of λ, we obtain that
the expansion coeﬃcients (i.e., one-time cumulants of random process x(t))
satisfy the inﬁnite system of nonlinear equations. The method of cumulant

11.4
Asymptotic and Approximate Methods
365
expansions considers this system neglecting all higher-order cumulants begin-
ning from some certain order (if this order is three, we arrive at the Gaussian
approximation, if four, the excess approximation, and so on). The retained
cumulants satisfy the closed nonlinear system of ordinary diﬀerential equa-
tions whose solution determines the time-dependent behavior of cumulants.
Note that monograph [99] suggests the general approach for deriving these
equations directly from stochastic equations (10.1), without considering the
Fokker–Planck equation (10.11), page 307 or the equation for the characte-
ristic function. A disadvantage of this method consists in the fact that the
neglect of the inﬁnite number of cumulants, as is known, impairs the proba-
bility distribution. In particular, such impaired distribution appears negative
in certain regions of spatial variables. Nevertheless, examples show that the
method of cumulant expansions adequately describes time-dependent beha-
vior of certain cumulants for a wide class of problems. It seems that this class
of problems is limited to the problems for which statistical characteristics of
the solution are analytic functions with respect to the intensity of random
actions. Most likely, this method will fail for problems characterized by the
nonanalytic behavior with respect to this parameter (such as problems on
the escape of the trajectory of a system out of certain spatial region and the
problem on the arrival at a given boundary).
11.4.3
Method of Fast Oscillation Averaging
Another approximate method widely used in the context of stochastic oscil-
lating systems is called the method of averaging over fast parameters. For
example, let a stochastic system is described by the dynamic equations
d
dtx(t) = A(x, φ) + z(t)B(x, φ),
d
dtφ(t) = C(x, φ) + z(t)D(x, φ),
(11.74)
where
φ(t) = ω0t + φ(t),
functions A(x, φ), B(x, φ), C(x, φ), and D(x, φ) are the periodic functions of
variable φ, and z(t) is the Gaussian delta-correlated process with the para-
meters
⟨z(t)⟩= 0,
⟨z(t)z(t′)⟩= 2Dδ(t −t′),
D = σ2τ0.
Variables x(t) and φ(t) can mean the vector module and phase, respec-
tively. The Fokker–Planck equation corresponding to system of equations
(11.74) has the form

366
11
Methods for Solving and Analyzing the Fokker-Planck Equation
∂
∂tP(x, φ, t) = −∂
∂xA(x, φ)P(x, φ, t) −∂
∂φC(x, φ)P(x, φ, t)
+ D
 ∂
∂xB(x, φ) + ∂
∂φD(x, φ)
2
P(x, φ, t).
(11.75)
Commonly, Eq. (11.75) is very complicated to immediately analyze the
joint probability density. We rewrite this equation in the form
∂
∂tP(x, φ, t) = −∂
∂xA(x, φ)P(x, φ, t) −∂
∂φC(x, φ)P(x, φ, t)
−D ∂
∂x
7
∂B2(x, φ)
2∂x
+ ∂B(x, φ)
∂φ
D(x, φ)
8
P(x, φ, t)
−D ∂
∂φ
7
∂D(x, φ)
∂x
B(x, φ) + ∂D2(x, φ)
2∂φ
)
8
P(x, φ, t)
+ D
 ∂2
∂x2 B2(x, φ) + 2 ∂2
∂x∂φB(x, φ)D(x, φ) + ∂2
∂φ2 D2(x, φ)
	
P(x, φ, t).
(11.76)
Now, we assume that functions A(x, φ) and C(x, φ) are suﬃciently small
and ﬂuctuation intensity of process z(t) is also small. In this case, statistical
characteristics of system of equations (11.74) only slightly vary during times
∼1/ω0. To study these small variations (accumulated eﬀects), we can average
Eq. (11.76) over the period of all oscillating functions. Assuming that function
P(x, φ, t) remains intact under averaging, we obtain the equation
∂
∂tP(x, φ, t) = −∂
∂xA(x, φ) P(x, φ, t) −∂
∂φC(x, φ) P(x, φ, t)
−D
/
∂
∂x
0
∂B2(x, φ)
2∂x
+ ∂B(x, φ)
∂φ
D(x, φ)
1
+ ∂
∂φ
∂D(x, φ)
∂x
B(x, φ)
2
P(x, φ, t)
+ D
 ∂2
∂x2 B2(x, φ) + 2 ∂2
∂x∂φB(x, φ)D(x, φ) + ∂2
∂φ2 D2(x, φ)

P(x, φ, t),
(11.77)
where the overbar denotes quantities averaged over the oscillation period.
Integrating Eq. (11.77) over φ, we obtain the Fokker–Planck equation for
function P(x, t)
∂
∂t P (x, t) = −∂
∂xA(x, φ) P (x, t) + D ∂
∂x
2
∂B2(x, φ)
2∂x
+ ∂B(x, φ)
∂φ
D(x, φ)
3
P (x, t)
+ D ∂
∂x B2(x, φ) ∂
∂x P (x, t).
(11.78)
Note that quantity x(t) appears to be the one-dimensional Markovian ran-
dom process in this approximation.

11.4
Asymptotic and Approximate Methods
367
If we assume that B(x, φ)D(x, φ) = ∂D(x, φ)
∂x
B(x, φ) = 0, and C(x, φ) =
const, D2(x, φ) = const in Eq. (11.77), then processes x(t) and φ(t) become
statistically independent, and process φ(t) becomes the Markovian Gaussian
process whose variance is the linear increasing function of time t. This means
that probability distribution of quantity φ(t) on segment [0, 2π] becomes uni-
form for large t (at C(x, φ) = 0).
As an illustration of using the above technique, we consider the problems
on the stochastic parametric resonance, diﬀusion of passive tracer in a random
hydrodynamic ﬂow, and noises in hydrodynamic ﬂows near the instability
threshold.
Stochastic Parametric Resonance
Consider the stochastic second-order equation equivalent to system of the
ﬁrst-order equations (8.180), page 249
d
dtx(t) = y(t),
d
dty(t) = −2γy(t) −ω2
0[1 + z(t)]x(t).
(11.79)
In Chapter 5, page 244, we discussed the general approach to this problem
in the case of the delta-correlated ﬂuctuations of frequency. Here, we will
assume that z(t) is the Gaussian random process with the parameters
⟨z(t)⟩= 0,
⟨z(t)z(t′)⟩= 2σ2τ0δ(t −t′).
Replace functions x(t) and y(t) with the variables — oscillation amplitude
A(t) and phase φ(t) — deﬁned by the formulas
x(t) = A(t) sin (ω0t + φ(t)) ,
y(t) = ω0A(t) cos (ω0t + φ(t)) .
(11.80)
Substituting Eq. (11.80) in system of equations (11.79), we obtain the system
of equations in functions A(t) and φ(t)
d
dtA(t) = −2γA(t) cos2 ψ(t) −ω0
2 z(t)A(t) sin (2ψ(t)) ,
d
dtφ(t) = 2γ sin (2ψ(t)) + ω0z(t) sin2 ψ(t),
(11.81)
where ψ(t) = ω0t+φ(t). Representing amplitude A(t) as A(t) = eu(t), we can
rewrite system (11.81) in the form

368
11
Methods for Solving and Analyzing the Fokker-Planck Equation
d
dtu(t) = −2γ cos2 ψ(t) −ω0
2 z(t) sin (2ψ(t)) ,
d
dtφ(t) = γ sin (2ψ(t)) + ω0z(t) sin2 ψ(t).
(11.82)
Consider now the joint probability density of the solution to system of
equations (11.81) P(u, φ, t) = ⟨ϕ(u, φ, t)⟩, where the indicator function
ϕ(u, φ, t) = δ (u(t) −u) δ (φ(t) −φ)
satisﬁes the Liouville equation
∂
∂tϕ(u, φ, t) = γ

2 ∂
∂u cos2 ψ(t) −∂
∂φ sin (2ψ(t))
	
ϕ(u, φ, t)
+ z(t)ω0
1
2
∂
∂u sin (2ψ(t)) −∂
∂φ sin2 ψ(t)
	
ϕ(u, φ, t).
(11.83)
Averaging now Eq. (11.83) over an ensemble of realizations of random delta-
correlated process z(t), using the Furutsu–Novikov formula that assumes in
this case the form
⟨z(t)ϕ(u, φ, t)⟩= σ2τ0
.
δ
δz(t −0)ϕ(u, φ, t)
/
,
and the equality
δ
δz(t −0)ϕ(u, φ, t) = ω0
1
2
∂
∂u sin (2ψ(t)) −∂
∂φ sin2 ψ(t)
	
ϕ(u, φ, t)
following from Eq. (11.83), we obtain the Fokker–Planck equation for the
probability density
∂
∂tP(u, φ, t) = γ

2 ∂
∂u cos2 ψ(t) −∂
∂φ sin (2ψ(t))
	
P(u, φ, t)
+ D
1
2
∂
∂u sin (2ψ(t)) −∂
∂φ sin2 ψ(t)
	2
P(u, φ, t),
where D = σ2τ0ω2
0. This equation can be rewritten in the form

11.4
Asymptotic and Approximate Methods
369
∂
∂tP(u, φ, t) = γ

2 ∂
∂u cos2 ψ(t) −∂
∂φ sin (2ψ(t))
	
P(u, φ, t)
+ D
 ∂
∂u cos (2ψ(t)) sin2 ψ(t) −2 ∂
∂φ sin3 ψ(t) cos ψ(t)
	
P(u, φ, t)
+ D
1
4
∂2
∂u2 sin2 (2ψ(t)) −
∂2
∂u∂φ sin (2ψ(t)) sin2 ψ(t) + ∂2
∂φ2 sin4 ψ(t)
	
× P(u, φ, t).
(11.84)
Assuming that absorption parameter γ is small in comparison with oscilla-
tion frequency ω0 (γ ≪ω0), we can average Eq. (11.84) over oscillation period
T = 2π/ω0 (the assumption that statistical characteristics only slightly vary
during times ∼T allows us to average solely trigonometric functions appeared
in the right-hand side of Eq. (11.84)) to obtain the equation for the avera-
ged (i.e., describing slow variations of statistical characteristics) probability
density
∂
∂tP(u, φ, t) = γ ∂
∂uP(u, φ, t) −D
4
∂
∂uP(u, φ, t)
+ D
8
∂2
∂u2 P(u, φ, t) + 3D
8
∂2
∂φ2 P(u, φ, t)
(11.85)
with the initial value
P(u, φ, 0) = δ(u −u0)δ(φ −φ0).
For example, in the case of initial values u0 = 0, φ0 = 0 corresponding to
x(0) = 0, y(0) = ω0, from Eq. (11.85) follows that statistical characteristics of
amplitude and phase (averaged over the oscillation period) are statistically
independent and the corresponding probability densities are the Gaussian
densities,
P(u, t) =
1

2πσ2u(t)
exp
'
−(u −⟨u(t)⟩)2
2σ2u(t)
(
,
P(φ, t) =
1
>
2πσ2
φ(t)
exp
'
−(φ −φ0)2
2σ2
φ(t)
(
,
(11.86)
where
⟨u(t)⟩= u0 −γt + D
4 t,
σ2
u(t) = D
4 t,
⟨φ(t)⟩= φ0,
σ2
φ(t) = 3D
4 t.
As an example of using the above expressions, consider expressions for
⟨x(t)⟩and
,
x2(t)
-
corresponding to initial values u0 = 0 and φ0 = 0.
For the average value, we have the expression

370
11
Methods for Solving and Analyzing the Fokker-Planck Equation
⟨x(t)⟩= ⟨A(t)⟩⟨sin (ω0t + φ(t))⟩= 1
2i
2
eu(t)3 2
eiω0t+iφ(t) −e−iω0t−iφ(t)3
= exp

⟨u(t)⟩+ 1
2σ2
u(t) −1
2σ2
φ(t)
	
sin(ω0t) = e−γt sin(ω0t)
(11.87)
coinciding with the problem solution in the case of absent ﬂuctuations.
For quantity
,
x2(t)
-
, we obtain the expression

x2(t)

=
-
e2u(t). 
sin2 (ω0t + φ(t))

= 1
2
-
e2u(t).
{1 −⟨cos 2(ω0t + φ(t))⟩}
= 1
2e2⟨u(t)⟩+2σ2
u(t) /
1 −e−2σ2
φ(t) cos(ω0t)
0
= 1
2e(D−2γ)t /
1 −e−3Dt/2 cos(2ω0t)
0
(11.88)
that coincides (in the absence of absorption) with Eq. (8.179), page 248 to
terms of order D/ω0 ≪1, and statistical parametric excitation of the system
occurs if the condition D > 2γ is satisﬁed.
As was mentioned earlier, the random amplitude has the lognormal proba-
bility distribution; consequently, its moment functions are given by the ex-
pression
⟨An(t)⟩=
2
enu(t)3
= An
0 exp

−nγt + 1
8n(n + 2)Dt
	
.
(11.89)
Under the condition 8γ < (n + 2)D, stochastic dynamic system (11.79) is
statistically excited beginning from the moment function of order n. Never-
theless, the typical realization curve of the random amplitude has the form
A∗(t) = A0e−(γ−D
4 )t,
and, under suﬃciently weak absorption , namely if
1 < 4 γ
D < 1 + 1
2n,
which is the case if n is suﬃciently great, the typical realization curve de-
creases exponentially with time, whereas all moment functions of random am-
plitude A(t) of order n and higher are exponentially increasing functions of
time. This means that statistics of random amplitude A(t) is formed by high
spikes above the exponentially decreasing typical realization curve, which
is a consequence of the fact that random amplitude A(t) is the lognormal
quantity.
Diﬀusion of Passive Tracer in a Random Hydrodynamic Flow
In Chapter 1, we gave examples of the formation of cluster structure of the
tracer ﬁeld in random velocity ﬁeld u(r, t) = v(t)f(r), where f(r) is the

11.4
Asymptotic and Approximate Methods
371
deterministic function and v(t) is the vector random Gaussian process. In
this case, the ﬁeld of tracer density satisﬁes Eq. (3.16), page 100. In terms of
the model under consideration, this equation assumes the form
 ∂
∂t + v(t) ∂
∂rf(r)

ρ(r, t) = 0,
ρ(r, 0) = ρ0(r).
We will assume that v(t) is the Gaussian random stationary vector process
with the parameters
⟨v(t)⟩= 0,
Bij(t −t′) = ⟨vi(t)vj(t′)⟩

Bv(0) =
,
v2(t)
-
.
In the approximation of delta-correlated process v(t), we have
Bij(t −t′) = 2σ2δijτ0δ(t −t′)
⎛
⎝σ2δijτ0 =
∞

0
dτBij(τ)
⎞
⎠,
(11.90)
where σ2 is the variance of the velocity ﬁeld and τ0 is the temporal correlation
radius of ﬁeld v.
In the Eulerian representation, the indicator function
ϕ(r, t; ρ) = δ(ρ(r, t) −ρ)
satisﬁes the Liouville equation (3.20), page 101; in the case under considera-
tion, this equation assumes the form
 ∂
∂t+v(t)f(r) ∂
∂r

ϕ(r, t; ρ) = v(t)∂f(r)
∂r
∂
∂ρρϕ(r, t; ρ),
ϕ(r, 0; ρ) = δ(ρ0(r) −ρ).
We rewrite this equation in the form
∂
∂tϕ(r, t; ρ) = −v(t)
 ∂
∂rf(r)−∂f(r)
∂r

1 + ∂
∂ρρ
	
ϕ(r, t; ρ).
(11.91)
Averaging Eq. (11.91) over an ensemble of random process v(t), we obtain
the equation for the probability density
∂
∂tP(r, t; ρ) = σ2τ0
 ∂2
∂r2 f 2(r) −

3+2 ∂
∂ρρ
 ∂
∂rf(r)∂f(r)
∂r
+
+ f(r)∂2f(r)
∂r2

1 + ∂
∂ρρ

+ ∂f(r)
∂r
∂f(r)
∂r

1 + ∂
∂ρρ
2(
P(r, t; ρ).
(11.92)

372
11
Methods for Solving and Analyzing the Fokker-Planck Equation
If characteristic scale of function f(r) variation with r is k−1 and these
variations show nearly periodic behavior (’fast variations’), then we can addi-
tionally average Eq. (11.92) over this scale to obtain the equation for ’slow’
spatial variations
∂
∂tP(r, t; ρ) = σ2τ0
'
f 2(r)Δ +
∂f(r)
∂r
2 ∂2
∂ρ2 ρ2
(
P(r, t; ρ).
(11.93)
Note that Eq. (11.93) coincides with Eq. (4.44), Vol. 2, page 39, which
means that, in the context of the one-time statistical characteristics, the
velocity ﬁeld model considered here is statistically equivalent to the model
of the Gaussian delta-correlated ﬁeld u(r, t). As a consequence, this model
of the velocity ﬁeld ﬂuctuations must also result in clustering of tracer if
v(t)∂f(r)/∂r ̸= 0, which was observed in simulations for the simplest func-
tion f(r) = sin 2(kr) (see the corresponding results on page 37).
Equation (11.93) becomes simpler if we consider this model using the x-
axis directed along vector k and assume that the initial distribution of density
is independent of r (ρ0(r0) = ρ0):
∂
∂tP(t; ρ) = 2k2σ2τ0
∂2
∂ρ2 ρ2P(t; ρ).
(11.94)
The solution of this equation corresponds to the logarithmically normal
probability distribution with the moment functions of the form
⟨ρn(r, t)⟩= ρn
0 e2n(n−1)k2σ2τ0t,
and, in accordance with Eq. (5.22), page 132, the typical realization curve is
the exponentially decaying curve
ρ∗(r, t) = ρ0e−2k2σ2τ0t.
In these conditions, the statistical ﬁeld of density can be represented in
the form of Eq. (1.68), page (36)
ρ(r, t)/ρ0 =
1
eT (t) cos2(kx) + e−T (t) sin2(kx),
where
T (t) = 2k
t

0
dτvx(τ)
is the ’random time’. Consequently, quantity ρ(r, t)/ρ0 averaged over fast
spatial variables is equal to unity,
ρ(r, t)/ρ0 = 1,

11.4
Asymptotic and Approximate Methods
373
and is independent of random time T (t).
In a similar way, we can obtain for example that
(ρ(r, t)/ρ0)2 = 1
2

eT (t) + e−T (t)
.
In the case of the Gaussian delta-correlated random process vx(t) with para-
meters (11.90), we have
2
(ρ(r, t)/ρ0)23
=
2
eT (t)3
= exp
1
2
,
T 2(t)
-	
= e4k2σ2τ0t,
which agrees with the logarithmically normal probability distribution of ﬁeld
ρ(r, t) (11.94).
Noises in Hydrodynamic Flows Near the Instability Threshold
In actuality, physical processes described by macroscopic hydrodynamic equa-
tions occur against a background of processes characterized by smaller scales
(noises), which require statistical description. Among these processes are, for
example, such as molecular noises in microscopic hydrodynamics, microtur-
bulence in respect of large-scale motions, and the eﬀect of rejected terms
in the ﬁnite-dimensional approximation of hydrodynamic equations. It ap-
pears that the eﬀect of such noises can be statistically described within the
framework of macroscopic variables, by supplementing the corresponding mi-
croscopic equations with random ’external forces’ (sources) with speciﬁed
statistical characteristics.
A characteristic feature of such formulation of the problem consists in
the existence of small parameter σ2, which is the ratio of noise energy to
the kinetic energy of macroscopic motion. The smallness of this parameter
drastically simpliﬁes the problem in a number of cases, because it allows
using a perturbation theory to describe the ﬂuctuations in a hydrodynamic
ﬂow far from the critical regime. The perturbation theory fails only in the
region, where the type of ﬂow changes (i.e., in the region, where a bifurcation
occurs in the solution of the corresponding dynamic problem). In this region,
one must consider the complete nonlinear problem, which is very diﬃcult
in the general case. Moreover, the issue of the nonlinear stability of ﬂows
is diﬃcult and poorly known even in the absence of ﬂuctuations. For this
reason, it is natural to limit ourselves to the consideration of simple models
with a ﬁnite number of degrees of freedom [44].
The simplest model is the Landau scheme of instability and secondary
ﬂows origination [80] (see also [83,112]). According to this scheme, near the
instability threshold, the velocity ﬁeld is representable in the form (the one-
dimensional instability)

374
11
Methods for Solving and Analyzing the Fokker-Planck Equation
u(x, t) = A(t)ϕ(x) + A∗(t)ϕ∗(x),
(11.95)
where, in the linear statement of the problem, A(t) = exp {γt −iωt} (γ < 0
corresponds to the undercritical regime, γ = 0 to the critical regime, and
γ > 0 to the overcritical regime) and ϕ(x) is the spatial eigenfunction of
the corresponding boundary-value problem. In addition, |γ| ≪|ω| near the
critical regime. According to Landau, the behavior of ﬂow for γ > 0 can be
considered on the basis of the equation
d
dt
I(t) = 2γI(t) −δI2(t)
(11.96)
in slowly varying quantity I(t) =

A(t)A(t)∗. This equation is called the
Landau equation; for δ > 0 it determines the steady-state secondary ﬂow
I = 2γ/δ. Here, we performed averaging over times t ∼1/ω.
In principle, hydrodynamic equations allow obtaining an equation for am-
plitude A(t) (with allowance for random noises such as, in particular, thermal
ﬂuctuations 2) in the form
d
dtA(t) = {· · · } + f(t),
d
dtA∗(t) = {· · · }∗+ f ∗(t),
(11.97)
where {· · · } denotes the terms of the third degree in A and A∗, asterisk
denotes the complex conjugated quantity, and f(t) is the corresponding ’ex-
ternal force’ acting on the instable mode. Within the framework of model
statement of the problem, we can consider forces f(t) and f ∗(t) as the zero-
mean Gaussian random forces characterized by the correlation functions of
the form
⟨f(t)f(t′)⟩= 2σ2
1hδ(t −t′),
⟨f ∗(t)f ∗(t′)⟩= 2σ2
1h∗δ(t −t′),
⟨f(t)f ∗(t′)⟩= 2σ2
0δ(t −t′),
(11.98)
where h is the complex constant appeared due to separation of random force
f(t) acting on the instable mode. Delta-correlated property is conditioned
here by the fact that the correlation radius of the force is much less than the
characteristic times of the corresponding dynamic problem.
Introduce probability density of quantity I(t) = A(t)A(t)∗:
P(I, t) = ⟨δ (I(t) −I)⟩.
In view of the relationships
2 The problem on thermal noises in simple hydrodynamic-type system was consid-
ered in Sect. 11.2.3, page 348.

11.4
Asymptotic and Approximate Methods
375
δA(t)
δf(t −0) =
δA∗(t)
δ∗f(t −0) = 1,
δA(t)
δf ∗(t −0) =
δA∗(t)
δf(t −0) = 0,
we obtain that probability density P(I, t) satisﬁes the equation
∂
∂tP(I, t) = −∂
∂I ⟨[A∗(t){· · · } + A(t){· · · }∗] δ (I(t) −I)⟩
+ 2σ2
0
∂
∂I I ∂
∂I P(I, t) + σ2
1
∂2
∂I2 ⟨[hA∗(t)A∗(t) + h∗A(t)A(t)] δ (I(t) −I)⟩.
(11.99)
Averaging Eq. (11.99) over times t ∼1/ω and taking into account the Lan-
dau equation (11.96), we obtain the Fokker–Planck equation for the probabi-
lity density P(I, t) of the slowly varying quantity I(t) in the following form
∂
∂tP(I, t) = −∂
∂I
0
2γI −δI2
P(I, t)
1
+ 2σ2
0
∂
∂I I ∂
∂I P(I, t).
(11.100)
The steady-state solution of Eq. (11.100) has the form
P(I) = C exp
 1
2σ2
0

2γI −δI2
2
	
,
from which follows that ﬂuctuations strongly increase in the critical regime
(γ = 0) because ⟨I⟩∼σ0 in this regime, in contrast to ⟨I⟩∼σ2
0 in the
undercritical region (γ < 0).
Relative to the temporal correlation ⟨A(t)A∗(t′)⟩, the model under consi-
deration tells nothing, because this quantity depends on the particular form
of the terms denoted {· · · } in Eqs. (11.97).

Chapter 12
Diﬀusion and Higher Approximations
12.1
General Remarks
Applicability of the approximation of delta-correlated random ﬁeld f(x, t)
(i.e., applicability of the Fokker–Planck equation) is restricted by the small-
ness of the temporal correlation radius τ0 of random ﬁeld f(x, t) with res-
pect to all temporal scales of the problem under consideration. The eﬀect
of the ﬁnite-valued temporal correlation radius of random ﬁeld f(x, t) can
be considered within the framework of the diﬀusion approximation (see,
e.g., [51, 52]). The diﬀusion approximation appears to be more obvious and
physical than the formal mathematical derivation of the approximation of
delta-correlated random ﬁeld. This approximation also holds for suﬃciently
weak parameter ﬂuctuations of the stochastic dynamic system and allows
describing new physical eﬀects caused by the ﬁnite-valued temporal correla-
tion radius of random parameters, rather than only obtaining the applicabil-
ity range of the delta-correlated approximation. The diﬀusion approximation
assumes that the eﬀect of random actions is insigniﬁcant during temporal
scales about τ0, i.e., the system behaves during these times as a free system.
Again, let vector function x(t) satisﬁes the dynamic equation (10.1),
page 305
d
dtx(t) = v(x, t) + f(x, t), x(t0) = x0,
(12.1)
where v(x, t) is the deterministic vector function and f(x, t) is the random
statistically homogeneous and stationary Gaussian vector ﬁeld with the sta-
tistical characteristics
⟨f(x, t)⟩= 0,
Bij(x, t; x′, t′) = Bij(x −x′, t −t′) = ⟨fi(x, t)fj(x′, t′)⟩.
Introduce the indicator function
ϕ(x, t) = δ(x(t) −x),
(12.2)
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
377
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_12, c
⃝Springer International Publishing Switzerland 2015

378
12
Diﬀusion and Higher Approximations
where x(t) is the solution to Eq. (12.1) satisfying the Liouville equation (10.6),
page 306
 ∂
∂t + ∂
∂xv(x, t)

ϕ(x, t) = −∂
∂xf(x, t)ϕ(x, t).
(12.3)
As earlier, we obtain the equation for the probability density of the solution
to Eq. (12.1)
P(x(t) = ⟨ϕ(x, t)⟩= ⟨δ(x(t) −x)⟩
by averaging Eq. (12.3) over an ensemble of realizations of ﬁeld f(x, t)
 ∂
∂t + ∂
∂xv(x, t)

P(x, t) = −∂
∂x ⟨f(x, t)ϕ(x, t)⟩,
P(x, t0) = δ(x −x0).
(12.4)
Using the Furutsu–Novikov formula (10.10), page 307
⟨fk(x, t)R[t; f(y, τ)]⟩=

dx′

dt′Bkl(x, t; x′, t′)
.
δ
δ fl(x′, t′)R[t; f(y, τ)]
/
valid for the correlation between the Gaussian random ﬁeld f(x, t) and arbit-
rary functional R[t; f(y, τ)] of this ﬁeld, we can rewrite Eq. (12.4) in the form
 ∂
∂t + ∂
∂xv(x, t)

P(x, t)=−∂
∂xi

dx′
t

t0
dt′Bij(x, t; x′, t′)
)
δ
δ fj(x′, t′)ϕ(x, t)
*
.
(12.5)
12.2
Diﬀusion Approximation
In the diﬀusion approximation, Eq. (12.5) is the exact equation, and the vari-
ational derivative and indicator function satisfy, within temporal scales of
about temporal correlation radius τ0 of random ﬁeld f(x, t), the system of
dynamic equations
∂
∂t
δϕ(x, t)
δfi(x′, t′) = −∂
∂x

v(x, t) δϕ(x, t)
δfi(x′, t′)
	
,
δϕ(x, t)
δfi(x′, t′)

t=t′ = −∂
∂xi
{δ(x −x′)ϕ(x, t′)} ,
(12.6)
∂
∂tϕ(x, t) = −∂
∂x {v(x, t)ϕ(x, t)} ,
ϕ(x, t)|t=t′ = ϕ(x, t′).
The solution to problem (12.5), (12.6) holds for all times t. In this case,
the solution x(t) to problem (12.1) cannot be considered as the Marko-
vian vector random process because its multi-time probability density can-
not be factorized in terms of the transition probability density. However, in

12.2
Diﬀusion Approximation
379
asymptotic limit t ≫τ0, the diﬀusion-approximation solution to the initial
dynamic system (12.1) will be the Markovian random process, and the corres-
ponding conditions of applicability are formulated as smallness of all statistical
eﬀects within temporal scales of about temporal correlation radius τ0.
12.2.1
Dynamics of a Particle
The use of the diﬀusion approximation in concrete physical problems will be
discussed in Part 4. Here, we illustrate this approximation by the example
of the dynamics of a particle with linear friction under random forces, which
is described by the stochastic system (1.14), page 9
d
dtr(t) = v(t),
d
dtv(t) = −λv(t) + f(r, t),
r(0) = r0,
v(0) = v0.
(12.7)
Introduce the indicator function for particle position and velocity
ϕ(r, v, t) = δ(r(t) −r)δ(v(t) −v).
This function satisﬁes the stochastic Liouville equation
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

ϕ(r, v, t) = −f(r, t) ∂
∂v ϕ(r, v, t).
(12.8)
Averaging Eq. (12.8) over an ensemble of realizations of ﬁeld f(r, t), we
obtain that the one-time probability density
P(r, v, t) = ⟨δ(r(t) −r)δ(v(t) −v)⟩
satisﬁes the equation
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) = −∂
∂v ⟨f(r, t)ϕ(r, v, t)⟩.
(12.9)
Taking into account the Furutsu–Novikov formula (10.10), page 307 we
can rewrite this equation in the form
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t)=−∂
∂vi

dr′
t

0
dt′Bij(r−r′, t−t′)
)δϕ(r, v, t)
δfj(r′, t′)
*
.
(12.10)
If we express the variational derivative in the right-hand side of Eq. (12.10)
in terms of the variational derivatives of functions r(t) and v(t), the equation
will assume the form

380
12
Diﬀusion and Higher Approximations
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) =
∂
∂vi

dr′
t

0
dt′Bij(r −r′, t −t′)
×
. ∂
∂rk
δrk(t)
δfj(r′, t′) +
∂
∂vk
δvk(t)
δfj(r′, t′)

ϕ(r, v, t)
/
.
(12.11)
The variational derivatives of functions r(t) and v(t) appeared in
Eq. (12.11) satisfy the system of equations following from Eq. (12.7) for t′ < t,
d
dt
δrk(t)
δfj(r′, t′) =
δvk(t)
δfj(r′, t′),
d
dt
δvk(t)
δfj(r′, t′) = −λ
δvk(t)
δfj(r′, t′) + ∂fk(r, t)
∂rl
δrl(t)
δfj(r′, t′)
(12.12)
with the initial values
δrk(t)
δfj(r′, t′)

t−t′
= 0,
δvk(t)
δfj(r′, t′)

t=t′
= δkjδ(r(t′) −r′).
(12.13)
The integral with respect to time in the right-hand side of Eq. (12.11)
depends mainly on the behavior of variational derivatives within the temporal
interval t −t′ ∼τ0. Assuming that the eﬀect of random forces is insigniﬁcant
within this temporal scale, we can omit the last term in the second equation
of system (12.12) to obtain the deterministic system of equations
d
dt
δrk(t)
δfj(r′, t′) =
δvk(t)
δfj(r′, t′),
d
dt
δvk(t)
δfj(r′, t′) = −λ
δvk(t)
δfj(r′, t′).
(12.14)
Nevertheless, the initial values (12.13) to this system remain random because
r(t′) is the stochastic function.
The solution to system (12.14) with initial values (12.13) is given by the
formulas
δvk(t)
δfj(r′, t′) = δkje−λ(t−t′)δ(r(t′) −r′),
δrk(t)
δfj(r′, t′) = 1
λδkj

1 −e−λ(t−t′)
δ(r(t′) −r′).
(12.15)
Assuming further that the dynamics of the particle is also only slightly af-
fected by random forces, we can express function r(t′) in Eq. (12.15) through
function r(t) that satisﬁes the simpliﬁed system of equations (12.7)
d
dtr(t) = v(t),
d
dtv(t) = −λv(t)
(12.16)
with the initial values

12.2
Diﬀusion Approximation
381
r(t)|t=t′ = r(t′),
v(t)|t=t′ = v(t′),
(12.17)
from which follows that
r(t′) = r(t) −1
λv

eλ(t−t′) −1

,
v(t′) = eλ(t−t′)v(t).
(12.18)
The above simplifying procedures of transition from Eqs. (12.12), (12.13)
to Eqs. (12.15) and from Eqs. (12.7) to Eqs. (12.18) form the basis of the
diﬀusion approximation in the context of the problem under consideration.
Using now Eqs. (12.15) and (12.18), we can rewrite Eq. (12.11) in the closed
form
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) = −∂
∂vi

dr′
t

0
dτBij(r −r′, τ)
×
 ∂
∂rj
1
λ

1 −e−λτ
+
∂
∂vj
e−λτ
	
δ

r −r′ −1
λ

eλτ −1

v

P(r, v, t).
(12.19)
The operator in braces commutes with the delta-function, and integration
over r′ results in the equation
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) =
∂
∂vi

D(1)
ij (v, t) ∂
∂vj + D(2)
ij (v, t) ∂
∂rj

P(r, v, t),
(12.20)
where we introduced the diﬀusion coeﬃcients
D(1)
ij (v, t) =
t

0
dτe−λτBij
 1
λ

eλτ −1

v, τ

,
D(2)
ij (v, t) = 1
λ
t

0
dτ

1 −e−λτ
Bij
 1
λ

eλτ −1

v, τ

.
(12.21)
Equation (12.20) for the one-time probability density is correct even for
times t < τ0. In this case, the solution {r(t), v(t)} to problem (12.7) will not
be the Markovian vector process because its multi-time probability density
cannot be factorized in terms of the transition probability density. Neverthe-
less, it will be the Markovian random process in asymptotic limit t ≫τ0. In
this limit, we can replace the upper limits of integrals in Eqs. (12.21) with
inﬁnity. This replacement results in the Fokker–Planck equation for the one-
time probability density
 ∂
∂t + v ∂
∂r −λ ∂
∂v v

P(r, v, t) =
∂
∂vi

D(1)
ij (v) ∂
∂vj + D(2)
ij (v) ∂
∂rj

P(r, v, t),
(12.22)
with the diﬀusion coeﬃcients

382
12
Diﬀusion and Higher Approximations
D(1)
ij (v) =
∞

0
dτe−λτBij
 1
λ

eλτ −1

v, τ

,
D(2)
ij (v) = 1
λ
∞

0
dτ

1 −e−λτ
Bij
 1
λ

eλτ −1

v, τ

.
(12.23)
Note that the approximation of delta-correlated random ﬁeld corresponds
to Eq. (12.20) with the diﬀusion coeﬃcients
D(1)
ij (v) =
∞

0
dτBij (0, τ) ,
D(2)
ij (v) = 0.
Integrating Eq. (12.22) over r, we arrive at the Fokker–Planck equation for
the one-time probability density of particle velocity
 ∂
∂t −λ ∂
∂v v

P(v, t) =
∂
∂vi
D(1)
ij (v) ∂
∂vj
P(v, t).
The steady-state probability density corresponding to the limit process
t →∞satisﬁes the equation
−λ ∂
∂v vP(v) =
∂
∂vi
D(1)
ij (v) ∂
∂vj
P(v),
(12.24)
whose solution essentially depends on the behavior of the diﬀusion coeﬃcient,
i.e., on the correlation function of random vector ﬁeld f(r, t). For example,
if we consider the one-dimensional case and specify the correlation function
Bf(x, t) in the form
Bf(x, t) = σ2
f exp

−|x|
l0
−|t|
τ0
	
,
where l0 and τ0 are the spatial and temporal correlation radii, respectively,
then we obtain that, for a suﬃciently small friction ( λτ0 ≪1), the solution
to Eq. (12.24) has the form [51]
P(v) = C exp
'
−λv2
2σ2
fτ0

1 + 2
3
|v|τ0
l0
(
.
(12.25)
For small particle velocity |v|τ0 ≪l0, probability distribution (12.25)
grades into the Gaussian distribution corresponding to the approximation of
the delta-correlated (in time) random ﬁeld f(x, t). However, in the opposite
limiting case |v|τ0 ≫l0, probability distribution (12.25) decreases signiﬁ-
cantly faster than in the case of the approximation of the delta-correlated (in
time) random ﬁeld f(x, t), namely,

12.3
Higher Approximations
383
P(v) = C exp
'
−λv2|v|
3σ2
fl0
(
,
(12.26)
which corresponds to the diﬀusion coeﬃcient decreasing according to the law
D(1) ∼1/|v| for great particle velocities. Physically, it means that the eﬀect
of random force f(x, t) on faster particles is signiﬁcantly smaller than on
slower ones.
Thus, the diﬀusion approximation lifts the basic restriction on smallness
of the temporal correlation radius τ0 remaining within the framework of the
Markovian process.
12.3
Higher Approximations
In some cases, the diﬀusion coeﬃcients can vanish in both approximation of
the delta-correlated random ﬁeld and diﬀusion approximation. Such a situa-
tion occurs, for example, when a particle moves in fast random waveﬁelds of
velocity [74] (see also [132–134]).
In this case, particle diﬀusion is described by the equation
d
dtr(t) = u(r, t),
r(0) = r0,
(12.27)
where u(r, t) is the statistically homogeneous and stationary random wave
vector ﬁeld such that ⟨u(r, t)⟩= 0 and the correlation tensor has the form
Bij(r, t) =

dkFij(k) cos {kr−ω(k)t} .
(12.28)
The spectral function Fij(k) is such that

dkFii(k) = σ2
u and ω = ω(k) > 0
is the equation of the dispersion curve for wave motions. For conventional
wave motions, the spectral functions of the velocity satisﬁes the condition
Φij(0) = 0, where
Φij(ω) =

dkFij(k)δ[ω −ω(k)],
so that the tensor diﬀusion coeﬃcient in the Fokker–Planck equation vanishes,
i.e.,
Dij =
∞

0
Bij(0, t)dt = 0.
The same diﬀusion coeﬃcient appears in the diﬀusion approximation for
t ≫τ0, where τ0 is the temporal correlation radius of the velocity ﬁeld.
Consequently, both approximation of the delta-correlated ﬁeld of velocity
and diﬀusion approximation give no ﬁnite result, and one needs to take into

384
12
Diﬀusion and Higher Approximations
account higher-order terms. The general procedure of constructing the theory
of successive approximations was developed in paper [73]. Here, we will adhere
to work [74].
Let the maximum of spectral function Fij(k) corresponds to a certain wave
number km, and the maximum of spectral function Φij(ω), to frequency ωm.
The corresponding spatial and temporal scales are l = 1/km and τ0 = 1/ωm.
Quantity ε = σuτ0/l appears usually small for actual waveﬁelds and can be
used as the basic small parameter of the problem, i.e., ε ≪1. This case will
be considered in Volume 2 in more detail.
Introduce now new ﬁeld u(r, t) characterized by unit variance and such
that u(r, t) = σuu(r, t), where σ2
u = Bii(0, 0) is the variance of velocities and
u(r, t) is the statistically homogeneous and stationary random wave ﬁeld with
mean value ⟨u(r, t)⟩= 0 and correlation tensor
⟨ui(r, t)uj(r′, t′)⟩= Bij(r −r′, t −t′)
(Bii(0, 0) = 1) .
Rewrite Eq. (12.27) in the form
d
dtr(t) = σu u(r, t),
r(0) = r0.
(12.29)
Introduce designations for the indicator function of the particle coordinate,
ϕ(r, t) = δ (r(t) −r) ,
(12.30)
and the ﬁrst and second variational derivatives that will be used later for
calculating statistical averages
δϕ(r, t)
δui(r′, t′) = σuSi(r, t; r′, t′),
δ2ϕ(r, t)
δui(r′, t′)δuj(r′′, t′′) = σ2
uSij(r, t; r′, t′; r′′, t′′).
(12.31)
The indicator function satisﬁes the stochastic Liouville equation
∂
∂tϕ(r, t) = −σu
∂
∂rk
{uk(r, t)ϕ(r, t)} ,
ϕ(r, 0) = δ(r −r0)
(12.32)
that can be rewritten in the integral form
ϕ(r, t) = δ(r −r0) −σu
∂
∂rk
t

0
dτ uk(r, τ)ϕ(r, τ).
(12.33)
For the ﬁrst variational derivative (12.31) (recall that it takes on nonzero
values only for t ≥t′), we obtain the stochastic integral equation

12.3
Higher Approximations
385
Si(r, t; r′, t′) = %Li(r, r′)ϕ(r, t′)θ(t −t′) −σu
∂
∂rk
t

t′
dτ uk(r, τ)Si(r, τ; r′, t′),
(12.34)
where operator %Lj(r, r′) acts on function f(r) according to the formula
%Li(r, r′)f(r) = −∂
∂ri
{δ(r −r′)f(r)} .
(12.35)
The stochastic integral equation for the second variational derivative is
obtained similarly:
Sij(r, t; r′, t′; r′′, t′′) = %Li(r, r′)Sj(r, t′; r′′, t′′)θ(t −t′)θ(t′ −t′′)
+ %Lj(r, r′′)Si(r, t′′; r′, t′)θ(t −t′′)θ(t′′ −t′)
−σu
∂
∂rl
t

max{t′,t′′}
dτ ul(r, τ)Sij(r, τ; r′, t′; r′′, t′′).
(12.36)
If we now average Eq. (12.32) over an ensemble of realizations of ﬁeld
{u(r, t)} and take into account the Furutsu–Novikov formula (10.10),
page 307, we obtain the equation for the probability density of particle posi-
tion P(r, t) = ⟨ϕ(r, t)⟩:
∂
∂tP(r, t) = −σ2
u
∂
∂rk

dr′
t

0
dt′Bki(r −r′, t −t′) ⟨Si(r, t; r′, t′)⟩,
P(r, 0) = δ(r −r0).
(12.37)
Integrating Eq. (12.37) with respect to time over the interval (t1, t),
where t1 < t, we obtain the equality
P(r, t)−P(r, t1) = −σ2
u
∂
∂rl
t

t1
dτ

dr′′
τ

0
dt′′Blj(r −r′′, τ −t′′)
'
Sj(r, τ; r′′, t′′)
(
.
(12.38)
Average now Eq. (12.34) over an ensemble of realizations of ﬁeld {uk(r, t)}.
Then we obtain the equation for quantity ⟨Si(r, t; r′, t′)⟩:
⟨Si(r, t; r′, t′)⟩= %Li(r, r′)P(r, t′)θ(t −t′)
−σ2
u
∂
∂rl
t

t′
dτ

dr′′
t

0
dt′′Blj(r −r′′, τ −t′′) ⟨Sij(r, τ; r′, t′; r′′, t′′)⟩.
(12.39)

386
12
Diﬀusion and Higher Approximations
We approximate function ⟨Sij(r, t; r′, t′′; r′′, t)⟩by the expression
⟨Sij(r, t; r′, t′′; r′′, t)⟩= %Li(r, r′) ⟨Sj(r, t′; r′′, t′′)⟩θ(t −t′)θ(t′ −t′′)
+ %Lj(r, r′′) ⟨Si(r, t′′; r′, t′)⟩θ(t −t′′)θ(t′′ −t′)
(12.40)
that corresponds to the neglect of the third-order variational derivatives
in Eq. (12.36). Using this approximation and Eq. (12.38), we can rewrite
Eq. (12.39) in the form of the closed integral equation:
⟨Si(r, t; r′, t′)⟩= %Li(r, r′)P(r, t)θ(t −t′)
+ σ2
u%Li(r, r′) ∂
∂rl
t

t′
dτ

dr′′
τ

0
dt′′Blj(r −r′′, τ −t′′) ⟨Sj(r, τ; r′′, t′′)⟩
−σ2
u
∂
∂rl
t

t′
dτ

dr′′
t′

0
dt′′Blj(r −r′′, τ −t′′)%Li(r, r′) ⟨Sj(r, t′; r′′, t′′)⟩
−σ2
u
∂
∂rl
t

t′
dτ

dr′′
τ

t′
dt′′Blj(r −r′′, τ −t′′)%Lj(r, r′′) ⟨Si(r, t′′; r′, t′)⟩.
(12.41)
Solving Eq. (12.41) in ⟨Si(r, t; r′, t′)⟩by the convergence method with res-
pect to parameter σ2
u to terms proportional to σ2
u (in this case, temporal
arguments ti of functions P(r, ti) can be replaced with t), we have
⟨Si(r, t; r′, t′)⟩=
"
%Li(r, r′)
+ σ2
u%Li(r, r′) ∂
∂rl
t

t′
dτ

dr′′
τ

0
dt′′Blj(r −r′′, t′′)%Lj(r, r′′)
−σ2
u
∂
∂rl
t

t′
dτ

dr′′
t′

0
dt′′Blj(r −r′′, τ −t′′)%Li(r, r′)%Lj(r, r′′)
−σ2
u
∂
∂rl
t

t′
dτ

dr′′
τ

t′
dt′′Blj(r −r′′, τ −t′′)%Lj(r, r′′) %Li(r, r′)
#
P(r, t).
(12.42)
Substituting Eq. (12.42) in Eq. (12.37) and integrating the result over all
spatial variables, we arrive at the equation of the third order in r (where
the terms containing the ﬁrst-order derivative and proportional to σ4
u can be
omitted)

12.3
Higher Approximations
387
∂
∂tP(r, t) = −σ2
u
t

0
dt′ ∂Bki(0, t′)
∂ri
∂
∂rk
P(r, t) + σ2
u
t

0
dt′Bki(0, t′)
∂2
∂rk∂ri P(r, t)
+ σ4
u
∂2
∂rk∂rl
t

0
dt′Bki(0, t −t′)
t

t′
dτ
t′

0
dt′′ ∂2Blj(0, τ −t′′)
∂ri∂rj
P(r, t)
+ σ4
u
∂2
∂rk∂rl
t

0
dt′ ∂2Bki(0, t −t′)
∂ri∂rj
t

t′
dτ
τ

t′
dt′′Blj(0, τ −t′′)P(r, t)
+ σ4
u
∂2
∂rk∂rj
t

0
dt′ ∂Bki(0, t −t′)
∂rl
t

t′
dτ
t′

0
dt′′ ∂Blj(0, τ −t′′)
∂ri
P(r, t)
+ σ4
u
∂2
∂rk∂ri
t

0
dt′ ∂Bki(0, t −t′)
∂rl
t

t′
dτ
τ

0
dt′′ ∂Blj(0, t′′)
∂rj
P(r, t)
+ σ4
u
∂2
∂rk∂rj
t

0
dt′ ∂2Bki(0, t −t′)
∂ri∂rl
t

t′
dτ
τ

0
dt′′Blj(0, t′′)P(r, t)
+ σ4
u
∂2
∂rk∂ri
t

0
dt′ ∂2Bki(0, t −t′)
∂rl∂rj
t

t′
dτ
τ

t′
dt′′Blj(0, τ −t′′)P(r, t)
−σ4
u
∂3
∂rk∂rl∂rj
t

0
dt′Bki(0, t −t′)
t

t′
dτ
t′

0
dt′′ ∂Blj(0, τ −t′′)
∂ri
P(r, t)
−σ4
u
∂3
∂rk∂rl∂ri
t

0
dt′ ∂Bki(0, t −t′)
∂rj
t

t′
dτ
τ

t′
dt′′Blj(0, τ −t′′)P(r, t)
−σ4
u
∂3
∂rk∂ri∂rj
t

0
dt′ ∂Bki(0, t −t′)
∂rl
t

t′
dτ
τ

0
dt′′Blj(0, t′′)P(r, t).
(12.43)
Generally, Eq. (12.43) cannot be considered as the equation for probability
density because the solution can be negative in the region of small values.
Nevertheless, the solution of Eq. (12.43) adequately describe statistical mo-
ments and, in this sense, Eq. (12.43) generalizes the Fokker–Planck equation.
Using now velocity ﬁeld in the spectral representation (12.28) and taking
into account properties of this representation, we can calculate integrals in
the coeﬃcients of Eq. (12.43) and obtain the equation for great times (t ≫τ0)
in the form

388
12
Diﬀusion and Higher Approximations
∂
∂tP(r, t) = −σ2
u

dk
ω(k)kiFki(k) ∂
∂rk
P(r, t)
+ σ4
u
π
2

dk1
 dk2
ω2
2
k1lk1jFki(k1)Flj(k2)δ (ω1 −ω2)
∂2
∂rk∂ri
P(r, t)
+ σ4
u
π
2

dk1
 dk2
ω2
2
k1lk2iFki(k1)Flj(k2)δ (ω1 −ω2)
∂2
∂rk∂rj
P(r, t),
(12.44)
where ω1 = ω(k1) and ω2 = ω(k2).
Equation (12.44) is now the Fokker–Planck equation and describes the
probability density of the position of a particle transferred by the statistically
homogeneous Gaussian wave velocity ﬁeld.
In the case of isotropic ﬂuctuations of ﬁeld u(r, t), Eq. (12.44) is simpliﬁed
and assumes the form
∂
∂tP(r, t) = D ∂2
∂r2 P(r, t)
corresponding to the Gaussian random vector process r(t) with mean value
⟨r(t)⟩= r0 and variance
σ2
r(t) =
2
(r(t) −r0)23
= 2dD t,
where d is the dimension of space and
D = σ4
u
π
2d

dk1
 dk2
ω2
2
k1lk1jFii(k1)Flj(k2)δ(ω1 −ω2)
is the diﬀusion coeﬃcient. In this case, the structure of the wave ﬁeld spectral
tensor has the form
Fki(k) = F s(k)

δik −kikk
k2

+ F p(k)kikk
k2 ,
where F s(k) and F p(k) are the solenoidal and potential components, respec-
tively and ω(k) ≡ω(k) ; as a result, the diﬀusion coeﬃcient is given by the
expression
D = σ4
u
π
2d
 dk1
ω2
1
k2
1Fii(k1)

dk2Fll(k2)δ(ω1 −ω2)
= σ4
u
π
2d
 dk1
ω2
1
k2
1 [F s(k1) (N −1) + F p(k1)]2

dk2δ(ω1 −ω2).
In the case of anisotropic medium, vector process r(t) shows spatial asym-
metry and the corresponding mean value and variance are given by the
expressions

12.3
Higher Approximations
389
⟨rm(t)⟩= r0m + tσ2
u

dk
ω(k)kiFmi(k),
σ2
r(t) =
'
r2(t) −⟨r(t)⟩2(
= tσ4
uπ

dk1
 dk2
ω2
2
k1lk1jFii(k1)Flj(k2)δ(ω1 −ω2)
+ tσ4
uπ

dk1
 dk2
ω2
2
k1lk2iFki(k1)Flk(k2)δ (ω1 −ω2) .
The diﬀusion coeﬃcient appears to be proportional to the square of the
velocity ﬁeld variance, rather than simply to the velocity ﬁeld variance. This
fact follows from the absence of wave–particle resonances in this problem,
which decreases the variance of the velocity of random particle drift by an
order of magnitude.

Chapter 13
Some Other Approximate Approaches to
the Problems of Statistical
Hydrodynamics
13.1
Quasi-Elastic Properties of Isotropic and
Stationary Noncompressible Turbulent Media
In Part 2, we analyzed statistics of solutions to the nonlinear equations of
hydrodynamics using the rigorous approach based on deriving and investigat-
ing the exact variational diﬀerential equations for characteristic functionals
of nonlinear random ﬁelds. However, this approach encounters severe diﬃcul-
ties caused by the lack of development of the theory of variational diﬀerential
equations. For this reason, many researchers prefer to proceed from more
habitual partial diﬀerential equations for diﬀerent moment functions of ﬁelds
of interest. The nonlinearity of the input dynamic equations governing ran-
dom ﬁelds results in the appearance of higher moment functions of ﬁelds of
interest in the equations governing any moment function. As a result, even
determination of average ﬁeld or correlation functions requires, in the strict
sense, solving an inﬁnite system of linked equations.
Thus, the main problem of this approach consists in cutting the mentioned
system of equations by means of one or another physical hypothesis. The
most known example of such a hypothesis is the Millionshchikov hypothesis
according to which the higher moment functions of even orders are expressed
in terms of the lower ones by the laws of the Gaussian statistics. The disad-
vantage of such approaches consists in the fact that the validity of hypothesis
usually cannot be proved; moreover, cutting the system of equations may
often yield physically contradictory results, namely, energy spectra of tur-
bulence can appear negative for certain wave numbers. Nevertheless, these
approximate approaches provide a deeper insight into physical mechanisms
of forming the statistics of strongly nonlinear random ﬁelds and make it pos-
sible to derive quantitative expressions for ﬁelds’ correlation functions and
spectra. It seems that the Millionshchikov hypothesis provides correct spectra
of the developed turbulence in viscous interval [112].
V.I. Klyatskin, Stochastic Equations: Theory and Applications in Acoustics,
391
Hydrodyn., Magnetohydrodyn., and Radiophys., Vol. 1, Understanding Complex Systems,
DOI: 10.1007/978-3-319-07587-7_13, c
⃝Springer International Publishing Switzerland 2015

392
13
Some Other Approximate Approaches to the Problems
We emphasize additionally that the mentioned approximate equations re-
veal many nontrivial eﬀects inherent in nonlinear random ﬁelds and having
no analogs in the behavior of deterministic ﬁelds and waves. Here, we illus-
trate these methods of analysis by the example of an interesting physical
eﬀect, which consists in the fact that average ﬂows of noncompressible ﬂuids
superimposed on the background of developed turbulent pulsations acquire
quasi-elastic wave properties. This eﬀect was ﬁrst mentioned by Moﬀatt [111]
who studied the reaction of turbulence on variations of the transverse gradi-
ent of average velocity. Moﬀatt noted that turbulent medium behaves in some
sense like an elastic medium; namely, variations of average velocity proﬁle of
a plane-parallel ﬂow satisfy the wave equation.
Let uT(r, t) is the random velocity ﬁeld of developed turbulence stationary
in time and homogeneous and isotropic in space (we assume that
,
uT(r, t)
-
=
0). In actuality, regular average ﬂows are imposed on turbulent pulsations of
ﬂuid. We denote U(r, t) the velocity ﬁeld of these regular ﬂows. Because the
turbulent pulsations and average ﬂows interact nonlinearly, we can represent
the total velocity ﬁeld in the form
u(r, t) = U(r, t) + uT(r, t) + u′(r, t),
(13.1)
Here, uT(r, t) is the unperturbed turbulent ﬁeld in the absence of average
ﬂows and u′(r, t) is the turbulent ﬁeld disturbance caused by the interaction
with the regular ﬂow (we assume that ⟨u′(r, t)⟩= 0).
Investigation of nonlinear interactions between regular ﬂows and turbulent
pulsations (and the analysis of turbulent pulsations uT(r, t) by themselves,
though) is very diﬃcult in the general case. However, in the case of weak
average ﬂows, i.e., under the condition that
U 2(r, t) ≪2T (t),
where T (t) = 1
2
2
uT(r, t)
23
is the average density of turbulent energy pul-
sation, we can discuss the eﬀect of turbulence on the average ﬂow evolution in
suﬃcient detail by considering the linear approximation in small disturbances
of ﬁelds U(r, t) and u′(r, t) and assuming known the spectrum of turbulent
pulsations uT(r, t).
The total velocity ﬁeld (13.1) and its turbulent component uT(r, t) satisfy
the Navier–Stokes equation (1.164), page 68. Hence, substituting Eq. (13.1)
in Eq. (1.164) and linearizing the equations in average ﬁeld U(r, t) and ﬂuc-
tuation component u′(r, t), we arrive at the approximate system of equations
∂
∂tUi(r, t) +
∂
∂rk
Tik(r, t) = −∂
∂ri
P(r, t),
(13.2)

13.1
Quasi-Elastic Properties
393
∂
∂tu′
i(r, t) +
∂
∂rk
[ui(r, t)uk(r, t) −⟨ui(r, t)uk(r, t)⟩]
+ uT
l (r, t)∂Ui(r, t)
∂rl
+ Ul(r, t)∂uT
i (r, t)
∂rl
= −∂
∂ri
p′(r, t),
(13.3)
where Tik(r, t) = ⟨ui(r, t)uk(r, t)⟩is the Reynolds stress tensor and P(r, t)
and p′(r, t) are the average and perturbed turbulent pressure components,
respectively. Here and below, we neglect the eﬀect of viscosity on dynamics
and statistics of perturbed ﬁelds U(r, t) and u′(r, t). In addition, we will bear
in mind that ﬁelds U(r, t) and u′(r, t) satisfy the incompressibility condition
(1.164)
∇U(r, t) = 0,
∇u′(r, t) = 0.
(13.4)
Being combined with Eq. (1.164), page 68 for turbulent pulsations uT(r, t),
Eq. (13.3) yields the following equation for the Reynolds stress tensor Tik(r, t)
∂
∂t Tik(r, t) +
∂
∂rl
⟨ui(r, t)uk(r, t)ul(r, t)⟩
+
-
uT
k (r, t)uT
l (r, t)
. ∂Ui(r, t)
∂rl
+
-
uT
i (r, t)uT
l (r, t)
. ∂Uk(r, t)
∂rl
+ Ul(r, t)∂

uT
i (r, t)uT
k (r, t)

∂rl
−
 
uT
k (r, t) ∂
∂ri
p′(r, t) + uT
i (r, t) ∂
∂rk
p′(r, t)
!
.
(13.5)
Here,
ui(r, t)uk(r, t)ul(r, t) = uT
i (r, t)uT
k (r, t)u′
l(r, t) + · · · .
Pressure pulsations p′ in this equations can be expressed in terms of per-
turbed velocities U(r, t) and u′(r, t),
p′(r, t) = −Δ−1(r, r′)

∂2
∂r′m∂r′n
[um(r′, t)un(r′, t) −⟨um(r′, t)un(r′, t)⟩]
+ 2∂uT
l (r′, t)
∂r′m
∂Um(r′, t)
∂r′
l
	
,
where Δ−1(r, r′) is the integral operator inverse to the Laplace operator.
Equations (13.2), (13.5), and the expression for pressure p′(r, t) form the
system of equations in average ﬁeld U(r, t) and stress tensor Tik(r, t). These
equations are not closed because they depend on higher-order velocity corre-
lators like the triple correlator
,
uT
i (r, t)uT
k (r, t)u′
l(r, t)
-
. Assuming that such
correlators only slightly aﬀect the dynamics of perturbations and taking into

394
13
Some Other Approximate Approaches to the Problems
account conditions (13.4) and statistical homogeneity of ﬁeld uT(r, t),
we
reduce
the
system
of
equations
(13.2),
(13.5)
to
the
form

τi(r, t) =
∂
∂rk
Tik(r, t)

∂
∂tUi(r, t) + τi(r, t) = −∂
∂ri
P(r, t),
(13.6)
∂
∂tτi(r, t) +
,
uT
k (r, t)uT
l (r, t)
- ∂2Ui(r, t)
∂rl∂rk
= 2 ∂
∂rk
.
uT
k (r, t) ∂
∂ri
Δ−1(r, r′)
∂uT
l (r′, t)
∂r′m
∂Um(r′, t)
∂r′
l
/
+
.
uT
i (r, t) ∂
∂rk
Δ−1(r, r′)
∂uT
l (r′, t)
∂r′m
∂Um(r′, t)
∂r′
l
/
.
(13.7)
This system of equations is closed in U(r, t). The coeﬃcients in the left-hand
side of Eq. (13.7) and the integral operator in the right-hand side can be
expressed in terms of the correlation tensor of vortex ﬁeld of unperturbed
turbulence uT(r, t)
,
uT
i (r, t)uT
j (r′, t)
-
=

dqΦij(q)eiq(r−r′).
(13.8)
In view of the fact that ﬁeld uT(r, t) is the solenoidal ﬁeld, we have
Φij(q) = Δij(q)F(q),
where
Δij(q) = δij −qiqj
q2 .
We will assume the energy spectrum of turbulent pulsations F(q) known.
From Eq. (13.8) follows in particular that the energy of turbulent pulsations
is expressed through the energy spectrum by the relationship
T = 1
2
2
uT(r, t)
23
= 4π
∞

0
dq q2F(q),
and coeﬃcients in the left-hand side of Eq. (13.7) are given by the formula
,
uT
k (r, t)uT
l (r, t)
-
= 2
3T δkl.
(13.9)
Finally, using Eq. (13.8), the right-hand side of Eq. (13.7) can be repre-
sented in the form

13.1
Quasi-Elastic Properties
395
−

dqeiqrUm(q, t)gim(q),
(13.10)
where U(q, t) is the spatial Fourier transform of average velocity ﬁeld U(r, t)
and tensor gim(q) is given by the formula
gim(p) = 2

dq qmplpk
(q + p)2 [(qi + pi) Δkl (q) + (qk + pk) Δil (q)] F(q).
(13.11)
From the obvious fact that tensor gim(p) is invariant relative rotations in
space follows that it must have the form
gim(p) = A(p)δim + B(p)pipm
p2 .
(13.12)
Moreover, the term proportional to B(p) disappears in Eq. (13.12) in view
of Eq. (13.4) (it expresses the property of incompressibility of average ﬁeld
U(r, t)) and the identity pU(p, t) = 0 following from Eq. (13.4), so that the
right-hand side (13.10) of Eq. (13.7) assumes the form
−

dqeiqrUi(q, t)A(q).
Substituting this expression in the right-hand side of Eq. (13.7) and taking
into account Eq. (13.9), we rewrite Eqs. (13.6), (13.7) in the form
∂
∂tUi(r, t) + τi(r, t) = −∂
∂ri
P(r, t),
∂
∂tτi(r, t) + 2
3T ΔUi(r, t) = −A(−i∇)Ui(r, t).
(13.13)
The kernel of the integral operator appeared in the second equation can be
obtained from comparison of Eq. (13.11) with (13.12). The result is as follows
A(p) =

dq
F(q)
(q2 + p2)

p2 −(qp)2
q2
 
q2 −(qp) −2(qp)2
p2

.
(13.14)
We can perform integration in Eq. (13.14) over angular coordinate to ob-
tain the expression
A(p) = 2πp2
∞

0
dq F(q)
'
2q2
3p2 −q4 −p4
4p4
+

q2 −p23
8qp5
ln

q + p
q −p

(
. (13.15)
We note additionally that Eq. (13.13) with allowance for Eq. (13.4) yields
the identity ΔP(r, t) = 0, so that P(r, t) = P0 = const. Then, eliminating

396
13
Some Other Approximate Approaches to the Problems
quantity τi(r, t) from system of equations (13.11) and (13.4), we arrive at a
single equation in the vector ﬁeld of average velocity U(r, t) of ﬂow
∂2
∂t2 U(r, t) −
2
3T Δ + A(−i∇)

U(r, t) = 0.
(13.16)
The corresponding dispersion equation
ω2(p) = 2
3T p2 −A(p)
can be rewritten in the form
ω2(p) = 2πp2
∞

0
dq F(q)f
q
p

,
where
f(x) = 2 −x2
3
+ x4 −1
8
−

x2 −1
3
16x
ln

1 + x
1 −x
 .
Function f(x) has the following asymptotics
f(x) =

2/3,
x ≪1,
4/15
x ≫1.
Moreover, the inequalities
2
3 ≥f(x) ≥4
15
hold for arbitrary x.
Thus, the time-dependent development of disturbances in average ﬂow
is governed by the hyperbolic equation (13.16). As a result, the turbulent
medium possesses certain quasi-elastic properties; namely, disturbances dif-
fuse in the turbulent medium as transverse waves showing dispersion pro-
perty. The phase and group velocities of these waves vary in the limits

2
3T ≥c ≥

4
15T.
13.2
Sound Radiation by Vortex Motions
In the previous section, we considered the physical eﬀect immediately related
to statistical averaging of the nonlinear system of hydrodynamical equations
in the case of an incompressible ﬂow. Here, we consider an eﬀect related
to weakly compressible media; namely, we consider the problem on sound
radiation by a weakly compressible medium. This problem corresponds to

13.2
Sound Radiation by Vortex Motions
397
the inclusion of random ﬁelds in the linearized equations of hydrodynamics.
Note that, within the framework of linear equations, parametric action of
turbulent medium yields the equations of acoustics with random refractive
index. We dealt with such problems in Chapter 13, Vol. 2
Turbulent motion of a ﬂow in certain ﬁnite spatial region excites acous-
tic waves outside this region. In the case of a weakly compressible ﬂow, this
sound ﬁeld is such as if it were generated by the static distribution of acous-
tic quadrupoles whose instantaneous power per unit volume is given by the
relationship
Tij(r, t) = ρ0vT
i (r, t)vT
j (r, t),
where ρ0 is the average density and vT
i (r, t) are the components of the velo-
city of ﬂow in turbulent region V inside which the turbulence is assumed
homogeneous and isotropic in space and stationary in time (we use the coor-
dinate system in which the whole of the ﬂow is quiescent). Turbulent motions
cause the ﬂuctuating waves of density ρ(r, t) that satisfy the wave equation
 ∂2
∂t2 −c2
0Δ

ρ(r, t) =
∂2
∂ri∂rj
Tij(r, t),
(13.17)
where c0 is the sound velocity in the homogeneous portion of the medium,
i.e., outside the region of turbulent motions.
The solution to this equation has the form of the retarded solution
ρ(r, t) =
1
4πc2
0
∂2
∂ri∂rj

V
dyTij

y, t−|r −y|
c0

.
For distances signiﬁcantly exceeding linear sizes of turbulent region V , this
solution can be reduced to the asymptotic expression
ρ(r, t) =
1
4πc4
0
rirj
x3

V
dy ¨Tij

y, t −|r −y|
c0

,
(13.18)
where ¨Tij (y, t) = ∂2
∂t2 Tij (y, t). Correspondingly, average energy ﬂux density
of acoustic wave excited by turbulent motions q(r, t) = c3
0
,
ρ2(r, t)
-
ρ0
is given
by the expression
q(r, t) =
1
16π2c5
0
rirjrkrl
r6

V
dy

V
dz
)
¨Tij

y, t−|r −y|
c0

¨Tkl

z, t −|r −z|
c0
*
.
(13.19)
Further calculations require the knowledge of the space–time correlators
of the velocity ﬁeld. The current theory of strong turbulence is incapable of
yielding the corresponding relationships. For this reason, investigators limit
themselves to various plausible hypotheses that allow complete calculations

398
13
Some Other Approximate Approaches to the Problems
to be performed (see, e.g., [89–91, 112]). In particular, it was shown that,
with allowance for incompressibility of ﬂow in volume V and the use of the
Kolmogorov–Obukhov hypothesis and a number of simplifying hypotheses for
splitting space-time correlators, from Eq. (13.19) follows that both average
energy ﬂux density and acoustic power are proportional to ∼M 5, where
M =

⟨v2(r, t)⟩
c0
is the Mach number (signiﬁcantly smaller than unity). We
note that this result can be explained purely hydrodynamically, by analyzing
vortex interactions in weakly compressible medium [41]. The simplest sound-
radiating vortex systems are the pair of vortex lines (radiating cylindrical
waves) and the pair of vortex rings (radiating spherical waves).
13.2.1
Sound Radiation by Vortex Lines
Consider two parallel vortex lines separated by distance 2h and characterized
by equal intensities κ = 1
2πξσ, where ξ is the vorticity (the size of the vortex
uniformly distributed over the area of inﬁnitely small section σ), so that the
circulation about each of vortex line is Γ = 2πκ. We will call these vortex
lines simply vortices. In a noncompressible ﬂow, these vortices revolve with
angular velocity ω =
κ
2h2 around the center of the line connecting these
vortices (see, ﬁg. 13.1a and e.g., [109]).
a
b
h(t)
h(t)
a1(t)
a2(t)
2h(t)
Fig. 13.1 Schematics of dynamics of a) vortex lines and (b) vortex rings
Select the coordinate system with the origin at a ﬁxed point and z-axis
along the vortex line. In this coordinate system, velocity potential ϕ0(r, t)
(v0(r, t) = −Re∇ϕ0(r, t)) and squared velocity v2
0(r, t) assume the forms
ϕ0(r, t) = ik ln

r2e2iθ−h2e2iωt
,
v2
0(r, t) =
4κ2r2
r4 + h4−2r2h2 cos 2(ωt−θ).
(13.20)

13.2
Sound Radiation by Vortex Motions
399
Here reiθ is the radius-vector of the observation point.
According to the Bernoulli equation, local velocity pulsations described by
Eq. (13.20) must produce the corresponding pressure pulsations; in the case
of weakly compressible medium, namely, under the condition that M ≪1,
where M =
κ
2hc0
is the Mach number and c0 is the sound velocity, these
pressure pulsations will propagate at large distances as sound waves.
Encircle the origin with a circle of radius R such that h ≪R ≪λ, where
λ is the sound wavelength. This is possible because
λ
h = π
M ≫1
for
M ≪1.
In region r < R, dynamics of ﬂow approximately coincides with the dyna-
mics of incompressible ﬂow. In other words, the dynamics of ﬂow is described
by Eqs. (13.20) in this region.
In region r > R, equations of motion coincide with the standard equations
of acoustics (see, e.g., [83])
 ∂2
∂t2 −c2
0Δ

ϕ(r, t) = 0,
p′(r, t) = ρ0
∂
∂tϕ(r, t).
(13.21)
Here, ϕ(r, t) is the potential of velocity in the acoustic wave, p′(r, t) is the
pressure in the wave, and ρ0 is the density of the medium.
Taking into account the fact that velocity v0(r, t) depends on t and θ only
in combination 2 (ωt −θ), we will seek the solution to Eq. (13.21) in the form
ϕ(r, t) = f(r)e2i(ωt−θ),
(13.22)
Substituting Eq. (13.22) in Eq. (13.21) and solving the resulting equation in
function f(r) with allowance for the radiation condition, we obtain
ϕ(r, t) = AH2
2 (2ωr/c0) e2i(ωt−θ),
(13.23)
where H2
2(z) is the Hankel function of the second kind and A is a constant.
For r ≫c0/2ω we have the standard divergent cylindrical wave with wave-
length λ = πc0/ω
ϕ(r, t) = A

 c
πωr exp{2i(ωt −ωr/c0 −θ −3π/8)}.
In the opposite case r ≪λ, we obtain
ϕ(r, t) = iA
π
 c
ωr
2
e2i(ωt−θ).
Potential ϕ(r, t) in region h ≪r ≪λ must coincide with the oscillating
portion of potential ϕ0(r, t) (see, e.g., [83]), i.e., with

400
13
Some Other Approximate Approaches to the Problems
ϕ(1)
0 (r, t) = −iκh2
r2 e2i(ωt−θ).
This condition yields the following expression for constant A
A = −πκM 2 = −πκ3
4h2c2
0
.
Consequently, potential ϕ(r, t) in the wave zone assumes the form
ϕ(r, t) = −κM 3/2

πh
r exp{2i(ωt −ωr/c0 −θ −3π/8)}.
(13.24)
The pressure in sound wave can be determined from potential (13.24) using
Eq. (13.21). The result is as follows
p′(r, t) = −2iκωρ0M 3/2

πh
r exp{2i(ωt −ωr/c0 −θ −3π/8)}.
The sound intensity (energy radiated per unit time) can be obtained by
integrating along the circle of radius R ≫λ
I = c0
ρ0
=
dl
,
|p′(r, t)| 2-
= 2π2ρ0M 4 κ3
h2 .
(13.25)
The radiated energy must coincide with the interaction energy of vortices
located in region r < R. Total energy in region r < R is
E = ρ0
2

dSv2
0(r, t).
(13.26)
Substituting Eq. (13.20) in (13.26) and discarding inﬁnite terms correspon-
ding to the energy of motion of vortices themselves (we assume vortices the
point vortices), we obtain the interaction energy in the form
E1 = 4πκ2ρ0 ln(R/h).
(13.27)
The interaction energy can vary only at the expense of varying the distance
between vortices (h = h(t)), because the circulation remains intact due to
the fact that we consider nonviscous medium.
Diﬀerentiating Eq. (13.27) with respect to time, we obtain energy variation
rate
I(t) = −4πρ0
κ2
h(t)
d
dth(t),
(13.28)
which is just transferred into the energy of acoustic waves. Using Eq. (13.25)
and (13.28), we obtain that the distance between vortices satisﬁes the equa-
tion

13.2
Sound Radiation by Vortex Motions
401
d
dth(t) = πκM 4
2h(t) .
(13.29)
Integrating Eq. (13.29) with allowance for the fact that
M = M(t) =
κ
2h(t)c0
,
we obtain
h(t) = h0

1 + 6πM 4
0ω0t
1/6 .
Thus, the intensity of radiated sound is proportional to M 4, I ∼M 4. It
is obvious that, in the case of statistically distributed system of vortex line
pairs, this estimate remains valid for certain portion of the plane.
13.2.2
Sound Radiation by Vortex Rings
In an incompressible ﬂow, a vortex ring of intensity κ causes the ﬂow to move
with a velocity
v(r, t) = κ
2 a
2π

0
dφs×r
r3 ,
which follows from the Biot–Savart law (see, e.g. [109]). Here, s is the unit
vector tangent to the vortex ring (it is directed along the vortex vector), a
is the radius of the ring, and r is the vector specifying observation point
position relative to points lying on the ring.
In the cylindrical coordinate system with origin at the center of ring and
z-axis directed along ring axis, we have
vR = κ
2 a
2π

0
dφcos φ
r3 ,
vθ = 0,
vz = κ
2 a
2π

0
dφa−Rcos φ
r3
,
(13.30)
where
r =

R2+z2+a2−2Racosφ
1/2 .
Here (R, θ, z) are the coordinates of the radius-vector of the observation point.
Let now we have two vortex rings of equal intensities and equal radii a0 at
distance 2h0. In this case, the front ring will increase in size, while the rear
ring will decrease and pursue the front ring (ﬁg. 13.1b). At certain instant,
it will penetrate through the front ring, and the rings switch places. This
phenomenon is called the game of vortex rings . In a weakly compressible
ﬂow, these motions of rings produce local regions of compression and rarefac-
tion; these regions propagate in the medium and, for large distances assume

402
13
Some Other Approximate Approaches to the Problems
the form of spherical acoustic waves. To determine the structure of radiated
sound, we must know relative motions of rings in an incompressible ﬂow.
Let rings have radii a1(t) and a2(t) and are separated by distance 2h(t)
at certain instant t. The rates of variations of ring radii are equal to radial
velocities that rings induce at each other, and the rate of variation of the
distance between the rings is equal to the diﬀerence of the z-components of
velocities induced by the rings. Consequently, we have
d
dta1(t) = 2κa2(t)h(t)
π

0
dφ
cos φ
|a1(t) −a2(t)|3 ,
d
dta2(t) = −2κa1(t)h(t)
π

0
dφ
cos φ
|a1(t) −a2(t)|3 ,
d
dth(t) = −κ
2

a2
1(t)−a2
2(t)

π

0
dφ
1
|a1(t) −a2(t)|3 ,
(13.31)
where
|a1(t) −a2(t)| =

a2
1(t)+a2
2(t) + 4h2(t) −2a1(t)a2(t) cos φ
1/2 .
Equations (13.31) should be solved with the initial conditions at t = 0
a1(0) = a2(0) = a0,
h(0) = h0.
The ﬁrst pair of equations immediately yields the relationship between
a1(t) and a2(t); namely,
a2
1(t) + a2
2(t) = 2a2
0.
(13.32)
This relationship shows that the moment of inertia of rings relative to the z
-axis is conserved.
The integrals in the right-hand side of Eq. (13.31) can be expressed in
terms of elliptic functions.
If rings are far from each other (γ = h0/a0 ≫1), they interact only slightly,
and we can assume that, in the ﬁrst approximation, they move independently
with the velocities determined by the ring areas. In the other limiting case
γ ≪1 (namely this case will be considered in what follows), the rings in-
teract actively. The integrals in the right-hand side of Eq. (13.31) are mainly
contributed by the neighborhood of point φ = 0. For this reason, we can
replace cos φ in the numerators of the ﬁrst pair of equations by unity. Thus
we obtain the second integral of motion
4h2(t) + [a1(t)−a2(t)]2 = 4h2
0.
(13.33)

13.2
Sound Radiation by Vortex Motions
403
Integral (13.33) means that the distance between points on diﬀerent rings
at the same polar angle is the conserved quantity.
In view of the existence of integrals (13.32) and (13.33), we can reduce sys-
tem (13.31) to a single equation in variable θ(t) determined by the equalities
a1(t) =
√
2a0 cos
π
4 −γ sin θ(t)

, a2(t) =
√
2a0 sin
π
4 −γ sin θ(t)

,
h(t) = h0 cos θ(t).
(13.34)
With this deﬁnition, conservation laws (13.32) and (13.33) are satisﬁed
automatically (in the ﬁrst order with respect to γ). Substituting Eq. (13.34)
in Eq. (13.31), expanding the result in the series, and calculating the integral,
we obtain
θ(t) =
κ
2h2
0
t.
Consequently, the ring radii and the distance between rings are given by the
expression
a1(t) = a0 (1 + γ sin(ωt)) ,
a2(t) = a0 (1 −γ sin(ωt)) ,
h(t) = h0 cos(ωt),
(13.35)
where
ω =
κ
2h2
0
,
(13.36)
as in the above case of vortex lines.
We note that the inﬁnitely thin rings move with an inﬁnite velocity. How-
ever, actual vortex rings move with a ﬁnite velocity signiﬁcantly smaller than
the sound velocity, but the dynamics of relative motion of rings only slightly
diﬀers from that we just obtained. The fact that angular velocity (13.36)
coincides with the corresponding angular velocity in the case of vortex lines
shows that the points lying on rings at equal polar angles revolve relative the
center point of the line connecting them at the rotational speed coinciding
with the rotational speed of vortex lines located at the same distance and
having the same intensity as the rings.
To study the structure of the sound radiated by the system of rings, we need
to know the velocity ﬁeld for large distances from the system. We associate
the coordinate system with the point located at the center of the segment
connecting the centers of the rings. The velocity of ﬂow outside the rings is
given by the formula
v(r, t) = κ
2 a1(t)
2π

0
dφs1×r1
r3
1
+ κ
2 a2(t)
2π

0
dφs2×r2
r3
2
.
(13.37)

404
13
Some Other Approximate Approaches to the Problems
Here, s1 and s2 are the unit vectors tangent to the vortex rings and r1 and r2
are the vectors specifying observation point position relative to points lying
on the rings. For large distances from the rings, the oscillating parts of the
velocity have the form (in the cylindrical coordinates)
v(1)
R (t) = 3κ
4
R

4z2 −R2
(R2 + z2)3/2 h

a2
1−a2
2

,
v(1)
θ (t) = 0,
v(1)
z (t) = 3κ
4
z

2z2 −3R2
(R2+z2)3/2 h

a2
1−a2
2

.
(13.38)
Introducing potential by the formula v(1) = −∇ϕ(1), we obtain
ϕ(1)(t) = −κ
4
R2 −2z2
(R2 + z2)5/2 h

a2
1 −a2
2

.
(13.39)
Substituting Eq. (13.35) in Eq. (13.39) and changing to spherical coordinates,
we can rewrite Eq. (13.39) in the complex form
ϕ(1)(t) = iκ
2 a3
0γ2 1 −3 cos2 θ
r3
e2iωt.
(13.40)
Now, we will proceed similar to the case of two vortex lines. We encircle
the origin by a sphere of radius L such that a0 ≪L ≪λ, where λ is the
wavelength of radiated sound waves. We will use the fact that the motion
of ﬂow inside the sphere approximately coincides with the motion of incom-
pressible ﬂow. Outside the sphere, the equations of motion will have the form
of Eq. (13.21) with the only diﬀerence that now Δ is the Laplace operator in
the three-dimensional space.
Represent potential ϕ in the form
ϕ(r, θ) = f(r, θ)e2iωt.
Substituting this expression in Eq. (13.21) and taking into account that we
must obtain divergent spherical waves for r ≫λ, we obtain f(r, θ) in the
form
f(r, θ) =
∞
&
n = 0
An
√r H(2)
n+1/2
2ωr
c0

Pn (cos θ) ,
where H(2)
n+1/2(z) is the Hankel function of the second kind and Pn(z) is the
Legendre polynomial . Comparing potential ϕ(r, θ) for r ≪λ with ϕ(1)(r, θ),
we obtain that n = 2 and
A2 = 2√π
3
κγ2a3
0ω5/2
c5/2
0
.

13.2
Sound Radiation by Vortex Motions
405
Consequently, potential and pressure in the wave zone have the forms
ϕ(r, θ) = 2κ
3

κ
2h0c0
2 a0
r

1 −3 cos2 θ

exp

2i

ωt−ωr
c0
−3π
4
	
,
p′(r, θ) = 4iκ
3

κ
2h0c0
2 a0ω
r ρ0

1 −3 cos2 θ

exp

2i

ωt−ωr
c0
−3π
4
	
,
so that angular distribution of the energy radiated per unit time is
I(θ) = 8π
9
κ3a2
0
h3
0

κ
2h0c0
5
ρ0

1 −3 cos2 θ
2 .
Vortex rings radiate energy as a quadrupole; the main portion of energy is
radiated in a cone about z-axis with corner angle 106◦in both positive and
negative z-directions. Integrating over θ, we obtain that total energy radiated
per unit time is given by the expression
I = 64π
45
κ3a2
0
h3
0

κ
2h0c0
5
ρ0.
Thus, intensity of sound radiated by a pair of vortex rings is proportional to
M 5. In the case of statistically distributed system of pairs of vortex rings, this
proportionality will remain valid for certain portion of space, which agrees
with estimates obtained in [89–91].

References
1. Adler, R.J.: The Geometry of Random Fields. Wiley, New York (1991)
2. Ambartsumyan, V.A.: Diﬀuse reﬂection of light by a foggy medium. Comptes
Rendus (Doklady) de l’USSR 38(8), 229–232 (1943)
3. Ambartsumyan, V.A.: On the problem of diﬀuse reﬂection of light. Journal of
Physics USSR 8(1), 65 (1944)
4. Ambartsumyan, V.A.: On the principle of invariance and its some applications.
In: Mnatsakanyan, M.A., Pickichyan, H.V. (eds.) Principle of Invariance and
its Applications, pp. 9–18. Armenian SSR Acad. of Sciences, Yerevan (1989)
5. Anishchenko, V.S., Neiman, A.B., Moss, F., Schimansky-Geier, L.: Stochastic
resonance: noise enhanced order. Physics–Uspekhi 42(1) (1999)
6. Aref, H.: The development of chaotic advection. Physics of Fluids 14, 1315–1325
(2002)
7. Batchelor, G.K.: Theory of Homogeneous Turbulence. Cambridge Univ. Press,
London (1953)
8. Bellman, R., Wing, G.M.: An Introduction to Invariant Imbedding. Classics
in Applied Mathematics, vol. 8. SIAM, Philadelphia (1992)
9. Bourret, R.C., Frish, U., Pouquet, A.: Brownian motion of harmonical oscil-
lator with stochastic frequency. Physica A 65(2), 303–320 (1973)
10. Casti, J., Kalaba, R.: Imbedding Methods in Applied Mathematics. Addison-
Wesley, Reading (1973)
11. Cramer, H., Leadbetter, M.R.: Stationary and Related Stochastic Processes.
Wiley, New York (1967)
12. Ditkin, V.A., Prudnikov, A.P.: Integral Transforms and Operation Calculus.
Pergamon Press, Oxford (1965)
13. Dolzhansky, F.V.: Fundamentals of Geophysical Hydrodynamics. In: Encyclo-
pedia of Mathematical Sciences. Mathematical Physics IV, vol. 103. Springer,
Heidelberg (2013)
14. Dolzhansky, F.V., Klyatskin, V.I., Obukhov, A.M., Chusov, M.A.: Nonlinear
Hydrodynamic Type Systems. Nauka, Moscow (1974) (in Russian)
15. Donsker, M.D.: On functional space integrals. In: Proc. Conf. Theory and
Appl. of Analysis in Funct. Space, vol. 17. The M.I.I. Press (1964)
16. Er-El, J., Peskin, R.L.: Relative diﬀusion of constant-level balloons in the
Southern Hemisphere. J. Atmos. Sci. 38(10), 2264–2274 (1981)
17. Feller, W.: An Introduction to Probability Theory and its Applications, vol. I,
3rd edn., vol. I, 2nd edn. Wiley, New York (1968)

408
References
18. Flatt´e, S.M., Wang, G.-Y., Martin, J.: Irradiance variance of optical waves
through atmospheric turbulence by numerical simulation and comparison with
experiment. JOSA A10, 2363–2370 (1993)
19. Flatt´e, S.M., Bracher, C., Wang, G.Y.: Probability - density functions of irradi-
ance for waves in atmospheric turbulence calculated by numerical simulation.
JOSA A11, 2080–2092 (1994)
20. Furutsu, K.: On the statistical theory of electromagnetic waves in a ﬂuctuating
medium. J. Res. NBS D-67, 303 (1963)
21. Gardiner, C.W.: Handbook of Stochastic Methods for Physics, Chemistry and
the Natural Sciences. Springer, Berlin (1985)
22. Gel’fand, I.M., Shilov, G.E.: Generalized Functions, vol. 1. Academic Press,
New York (1964)
23. Gledzer, E.B., Dolzhanskii, F.V., Obukhov, A.M.: Systems of hydrodynamical
type and their applications. Nauka, Moscow (1981) (in Russian)
24. Glukhovsky, A.B., Klyatskin, V.I.: Stochastic ‘noise’ in elementary nonlin-
ear ﬂuid-dynamic systems. Izvestiya, Atmospheric and Oceanic Physics 9(7),
381–386 (1973)
25. Gluhovsky, A., Klyatskin, V.I.: On dynamics of ﬂipover phenomena in simple
hydrodynamic models. Doklady, Earth Science Sections 237, 18–20 (1977)
26. Golberg, M.A.: Invariant imbedding and Riccati transformations. Appl. Math.
and Com. 1(1), 1–24 (1975)
27. Gryanik, N.V., Klyatskin, V.I.: Contribution to the statistical theory of wave
localization in a two-layer medium. JETP 84(6), 1106–1113 (1997a)
28. Gryanik, N.V., Klyatskin, V.I.: Localization of Rossby waves under the inﬂu-
ence of random cylindrical topography (two-layer model). Izvestiya, Atmos-
pheric and Oceanic Physics 33(6), 669–678 (1997b)
29. Gurbatov, S., Malakhov, A., Saichev, A.: Nonlinear Random Waves and Tur-
bulence in Nondispersive Media: Waves, Rays and Particles. Manchester Uni-
versity Press, Manchester (1991)
30. Gurvich, A.S., Kallistratova, M.A., Martvel’, F.E.: An investigation of strong
ﬂuctuations of light intensity in a turbulent medium at a small wave parame-
ter. Radiophys. & Quantum Electron. 20(7), 705–714 (1977)
31. Guzev, M.A., Klyatskin, V.I.: Approximation of the parabolic equation and
the waveﬁeld of a point source in a layered random medium. Waves in Random
Media 1(4), 275–286 (1991)
32. Guzev, M.A., Klyatskin, V.I., Popov, G.V.: Phase ﬂuctuations and locali-
zation length in layered randomly inhomogeneous media. Waves in Random
Media 2(2), 117–123 (1992)
33. Hopf, E.: Statistical hydromechanics and functional calculus. J. Ration. Mech.
Anal. 1, 87–123 (1952)
34. Hopf, E.: On the application of functional calculus to the statistical theory of
turbulence. In: Proc. Symp. Appl. Math., vol. 7, pp. 41–50. Am. Math. Society
(1957)
35. Hopf, E.: Remarks on the functional-analytic approach to turbulence. In: Proc.
Symp. Appl. Math., vol. 13, pp. 157–163. Am. Math. Society (1962)
36. Isichenko, M.B.: Percolation, statistical topography, and transport in random
media. Rev. Modern Phys. 64(4), 961–1043 (1992)
37. Ivanittskii, G.R.: 21st century: what is life from the perspective of physics?
Phys. Usp. 53, 327–356 (2010)

References
409
38. Kagiwada, H.H., Kalaba, R.: Integral Equations Via Imbedding Methods.
Addison-Wesley, Reading (1974)
39. Kharif, C., Pelinovskyy, E., Slyunaen, A.: Rogue Waves in the Ocean. Springer,
Berlin (2009)
40. Klimontovich, Y.L.: What are stochastic ﬁltration and stochastic resonance?
Physics–Uspekhi 42(1) (1999)
41. Klyatskin, V.I.: Sound radiation from vortex system. Izvestiya AN SSSR,
Mekhanika Zhidkosti i Gaza (6), 87–92 (1966)
42. Klyatskin, V.I.: Space-time description of stationary and homogeneous turbu-
lence. Fluid Dynamics 6(4), 655–661 (1971)
43. Klyatskin, V.I.: To the nonlinear theory of stability of periodic ﬂow. J. Appl.
Math. and Mech., 36(2) (1972)
44. Klyatskin, V.I.: Noise in the hydrodynamic ﬂow near the instability threshold.
Radiophys. & Quantum Electron. 17(4), 476–481 (1974)
45. Klyatskin, V.I.: Statistical theory of light propagation in a medium having ran-
dom inhomogeneities. Radiophys. & Quantum Electron. 18(1), 47–51 (1975a)
46. Klyatskin, V.I.: Statistical Description of Dynamical Systems with Fluctuating
Parameters. Nauka, Moscow (1975b) (in Russian)
47. Klyatskin, V.I.: Dynamic systems with parameter ﬂuctuations as telegraphic-
process type. Radiophys. & Quantum Electron. 20(4), 382–392 (1977)
48. Klyatskin, V.I.: Stochastic Equations and Waves in Randomly Inhomogeneous
Media. Nauka, Moscow (1980) (in Russian)
49. Klyatskin,
V.I.:
Ondes
et
´Equations
Stochastiques
dans
les
milieus
Al´eatoirement non Homog`enes. Les ´editions de Physique, Besan¸con-Cedex
(1985) (in French)
50. Klyatskin, V.I.: The Imbedding Method in Wave Propagation Theory. Nauka,
Moscow (1986) (in Russian)
51. Klyatskin, V.I.: Approximations by delta-correlated random processes and
diﬀusion approximation in stochastic problems. In: Kohler, W., White, B.S.
(eds.) Mathematics of Random Media. Lectures in Appl. Math, vol. 27,
pp. 447–476. AMS, Providence RI (1991)
52. Klyatskin, V.I.: The imbedding method in statistical boundary-value wave
problems. In: Wolf, E. (ed.) Progress in Optics, vol. XXXIII, pp. 1–128. North-
Holland, Amsterdam (1994a)
53. Klyatskin, V.I.: Statistical description of the diﬀusion of a passive tracer in a
random velocity ﬁeld. Physics–Uspekhi 37(5), 501–513 (1994b)
54. Klyatskin, V.I.: Localization of Rossby waves over a random cylindrical topog-
raphy of the ocean bottom. Atmospheric and Oceanic Physics 32(6), 757–765
(1996)
55. Klyatskin, V.I.: Diﬀusion and Clustering of Passive Tracers in Random Hy-
drodynamic Flows. Fizmatlit, Moscow (2005) (in Russian)
56. Klyatskin, V.I.: Stochastic equations (Theory and Applications in Acous-
tics, Hydrodynamics, and Radiophysics). Basic Concepts, Exact Results, and
Asymptotic Approximations, vol. 1. Fizmatlit, Moscow (2008) (in Russian)
57. Klyatskin, V.I.: Stochastic equations (Theory and Applications in Acoustics,
Hydrodynamics, and Radiophysics). Coherent Phenomena in Stochastic Dyna-
mic Systems, vol. 2. Fizmatlit, Moscow (2008) (in Russian)
58. Klyatskin, V.I.: Dynamic stochastic systems, typical realization curve, and
Lyapunov’s exponents. Atmospheric and Oceanic Physics 44(1), 18–32 (2008a)

410
References
59. Klyatskin, V.I.: Statistical topography and Lyapunov’s exponents in dynamic
stochastic systems. Physics-Uspekhi 51(4), 395–407 (2008b)
60. Klyatskin, V.I.: Modern methods for the statistical description of dynamical
stochastic sestems. Physics-Uspekhi 52(5), 514–519 (2009)
61. Klyatskin, V.I.: Lectures on Dynamics of Stochastic Systems. Elsevier, Boston
(2011)
62. Klyatskin, V.I.: Studies on Dynamics of Stochastic Systems. URSS, Moscow
(2012) (in Russian)
63. Klyatskin, V.I.: Spatial structures can form in stochastic dynamic systems due
to near-zero-probability events: (comment on “21st century: what is life from
the perspective of physics?"). Physics-Uspekhi 55(11), 1152–1154 (2012)
64. Klyatskin, V.I.: On the criterion of stochastic structure formation in random
media. In: Proceedings of the 4th International Interdisciplinary Chaos Sym-
posium, pp. 69–73. Springer, Berlin (2013)
65. Klyatskin, V.I.: Clastering of Random Positive Fiele as a Law of Nature. Teor.
Math. Phys. 176(3), 1252–1256 (2013)
66. Klyatskin, V.I.: On the Statistical Theory of Spatial Structure Formation in
Random Media. Russ. Journ. Math. Phys. 20(3), 295–314 (2013)
67. Klyatskin, V.I., Chkhetiani, O.G.: On the Diﬀusion and Clustering of a mag-
netic Field in Random Velocity Fields. JETP 109(2), 345–356 (2009)
68. Klyatskin, V.I., Elperin, T.: Clustering of the Low-Inertia Particle Number
Density Field in Random Divergence-Free Hydrodynamic Flows. JETP 95(2),
328–340 (2002a)
69. Klyatskin, V.I., Elperin, T.: Diﬀusion of Low-Inertia Particles in a Field
of Random Forces and the Kramers Problem. Atmospheric and Oceanic
Physics 38(6), 725–731 (2002b)
70. Klyatskin, V.I., Koshel’, K.V.: The simplest example of the development of a
cluster-structured passive tracer ﬁeld in random ﬂows. Physics–Uspekhi 43(7),
717–723 (2000)
71. Klyatskin, V.I., Saichev, A.I.: Statistical and dynamical localization of plane
waves in randomly layered media. Soviet Physics Usp. 35(3), 231–247 (1992)
72. Klyatskin, V.I., Saichev, A.I.: Statistical theory of the diﬀusion of a passive
tracer in a random velocity ﬁeld. JETP 84(4), 716–724 (1997)
73. Klyatskin, V.I., Tatarskii, V.I.: New method of successive approximations
in the peoblem of wave propagation in a medium with large-scale inhomo-
geneities. Radiophys. & Quantum Electron. 14(9), 1100–1111 (1971)
74. Klyatskin, V.I., Yakushkin, I.G.: Stochastic transport in random wave ﬁelds.
JETP 91(4), 736–747 (2000)
75. Klyatskin, V.I., Gryanik, N.V., Gurarie, D.: Localization of Rossby waves
under
the
inﬂuence
of
random
topography
(two-layer
model).
Wave
Motion 28(4), 333–352 (1998)
76. Koshel’, K.V., Aleksandrova, O.V.: Some resullts of numerical modeling of
passive scalar diﬀusion in randon velocity ﬁeld. Izvestiya, Atmospheric and
Oceanic Physics 35(5), 578–588 (1999)
77. Kravtsov, Y.A.: ‘Geometrical’ depolarization of light in a turbulent atmos-
phere. Radiophys. & Quantum Electron. 13(2), 217–220 (1970)
78. Kuzovlev, Y.E., Bochkov, G.N.: Operator methods of analysing stochastic
non-gaussian processes and systems. Radiophys. & Quantum Electron. 20(10),
1036–1044 (1977)
79. Lamb, H.: Hydrodinamics, 6th edn. Dover Publications, New York (1932)

References
411
80. Landau, L.D.: To the problem of turbulence. Dokl. Akad. Nauk SSSR 44(6),
339 (1944)
81. Landau, L.D., Lifshitz, E.M.: On hydrodynamic ﬂuctuations. JETP 32(3)
(1957)
82. Landau, L.D., Lifshitz, E.M.: Course of Theoretical Physics. Electrodynamics
of Continuous Media, vol. 8. Butterworth-Heinemann, Oxford (1984)
83. Landau, L.D., Lifshitz, E.M.: Fluid Mechanics, 2nd edn. Pergamon Press,
London (1987)
84. Levin, M.L., Rytov, S.M.: Theory of Steady-state Thermal Fluctuations in
Electrodynamics. Nauka, Moscow (1967) (in Russian)
85. Leontovich, M.A.: On one method of solving some problems about propagation
along the Earth surface. Izvestiya AN SSSR, Physika 8, 16 (1944)
86. Leontovich, M., Fock, V.A.: The solution of the electromagnetic wave propa-
gation problem along the Earth surface by the parabolic equation method.
Zh. ´Exper. Teor. Fiz. 16, 557–573 (1946)
87. Lifshits, E.M., Pitaevskii, L.P.: Statistical Physics, Pt. 2. Elsevier Science &
Technology, UK (1980)
88. Lifshits, I.M., Gredeskul, S.A., Pastur, L.A.: Introduction to the Theory of
Disordered Solids. John Wiley, New York (1988)
89. Lighthill, J.: On sound generated aerodynamically. 1. General theory. Proc.
Roy. Soc. A.211(1107), 564–587 (1952)
90. Lighthill, J.: On sound generated aerodynamicaly. II. Turbulence as a source
of sound. Proc. Roy. Soc. A.222(1148), 1–32 (1954)
91. Lighthill, J.: Sound generated aerodynamicaly. Proc. Roy. Soc. A.267(1329),
147–182 (1962)
92. Longuet-Higgins, M.S.: Statistical properties of a moving wave-form. Proc.
Cambridge Philos. Soc. 52, 234–245 (1956)
93. Longuet-Higgins, M.S.: On the velocity of the maxima in a moving wave-form.
Proc. Cambridge Philos. Soc. 53, 230–233 (1957a)
94. Longuet-Higgins, M.S.: The statistical analysis of a random, moving surface.
Philos. Trans. R. Soc., London, Ser. A249(966), 321–387 (1957b)
95. Longuet-Higgins, M.S.: Statistical properties of an isotropic random surface.
Philos. Trans. R. Soc., London, Ser. A250(975), 157–174 (1957c)
96. Longuet-Higgins, M.S.: The statistical geometry of random surfaces. In: Hy-
drodynamic Stability. In: Proc. 13th Symp. Appl. Math., pp. 105–142. Am.
Math. Soc. (1962)
97. Lorenz, E.N.: Maximum simpliﬁcation of the dynamic equations. Tellus 12(3),
243–254 (1960)
98. Lorenz, E.N.: Deterministic nonperiodic ﬂow. J. Atmos. Sci. 20(3), 130–141
(1963)
99. Malakhov, A.N.: Kumulant Analysis of Non-Gaussian Processes and Their
Transforms. Sov. Radio, Moscow (1978) (in Russian)
100. Maxey, M.R.: The Gravitational Settling of Aerosol Particles in Homogeneous
turbulence and random ﬂow ﬁeld. J. Fluid Mech. 174, 441–465 (1987)
101. McLaughlin, J.B., Martin, P.C.: Transition to turbulence in a statically
stressed ﬂuid system. Phys. Rev. A12(1), 186–203 (1975)
102. Mehlig, B., Wilkinson, M.: Coagulation by random velocity ﬁelds as a Kramers
Problem. Phys. Rev. Letters 92(25), 250602-1–250602-4 (2004)
103. Meshalkin, L.D., Sinai, Y.G.: Investigation of the stability of a stationary
solution of a system of equations for the plane movement of an incompressible
viscous ﬂow. J. Appl. Math. and Mech. 25(6), 1700–1705 (1961)

412
References
104. Mesinger, F.: Behavior of a very large number of constant-volume trajectories.
J. Atmos. Sci. 22(5), 479–492 (1965)
105. Mesinger, F.: Numerical integration of the primitive equations with a ﬂoat-
ing set of computation points: experiments with a barotropic global model.
Monthly Weather Review 99(1), 15–29 (1971)
106. Mesinger, F., Mintz, Y.: Numerical simulation of the 1970–1971 Eole experi-
ment. Numerical simulation of weather and climate. Technical Report No. 4,
Dep. Meteorology. Univ. of California, Los Angeles (1970a)
107. Mesinger, F., Mintz, Y.: Numerical simulation of the clustering of the constant-
volume balloons in the global domain. Numerical simulation of weather and
climate. Technical Report No. 5, Dep. Meteorology. Univ. of California, Los
Angeles (1970b)
108. Mikhailov, A.S., Uporov, I.V.: Critical phenomena in media with breeding,
decay, and diﬀusion. Sov. Phys. Usp. 27(9), 695–714 (1984)
109. Milne-Thomson, L.M.: Theoretical Hydrodynamics, 5th edn. Dover Publica-
tion, Inc., New York (1996)
110. Mirabel’, A.P., Monin, A.S.: On statistical properties of admixture mix-
ing by two-dimensional turbulence. Izvestiya, Atmospheric and Oceanic
Physics 19(9), 681–687 (1983)
111. Moﬀatt, H.K.: The interaction of turbulence with strong wind shear. In: Ya-
glom, A.M., Tatarskii, V.I. (eds.) Atmospheric Turbulence and Radio Wave
Propagation, pp. 139–154. Nauka, Moscow (1967)
112. Monin, A.S., Yaglom, A.M.: Statistical Fluid Mechanics. MIT Press, Cam-
bridge (1980)
113. Nalbandyan, O.G.: Possible approximations of the gaussian random process in
some nonlinear stochastic problems. Radiophys. & Quantum Electron. 20(5)
(1977)
114. Novikov, E.A.: Functionals and the random force method in turbulence theory.
Sov. Phys. JETP 20(5), 1290–1294 (1965)
115. Obukhov, A.M.: On integral invariants in the hydrodynamic-type systems.
Dokl. AN SSSR 184(2), 309–312 (1969)
116. Obukhov, A.M.: On invariant characteristics of systems of ﬂuid mechanical
type. Fluid Dynam. Trans. 5(2) (1971)
117. Obukhov, A.M.: On the problem of nonlinear interaction in ﬂuid dynamics.
Gerlands Beitr. Geophys. 82(4), 282–290 (1973)
118. Obukhov, A.M.: The Kolmogorov ﬂow and its laboratory modeling. Uspekhi
Mat. Nauk 38(4) (232), 101–111 (1983)
119. Obukhov, A.M.: Turbulence and Dynamics of Atmosphere. Leningrad,
Gidrometizdat (1988) (in Russian)
120. Pedlosky, J.: Geophysical Fluid Dynamics. Springer, New York (1982)
121. Pelinovskii, E.N.: Spectral analysis of simple waves. Radiophys. & Quantum
Electron. 19(3), 262–270 (1976)
122. Rytov, S.M., Kravtsov, Y.A., Tatarskii, V.I.: Principles of Statistical Radio-
physics, pp. 1–4. Springer, Berlin (1987-1989)
123. Saichev, A.I., Woyczynski, W.A.: Probability distributions of passive tracers
in randomly moving media. In: Molchanov, S.A., Woyczynski, W.A. (eds.)
Stochastic Models in Geosystems. IMA Volumes in Mathematics and its Ap-
plications, vol. 85, pp. 359–400. Springer, New York (1996)
124. Shapiro, V.E., Loginov, V.M.: ‘Formulae of diﬀerentiation’ and their use for
solving stochastic equations. Physica 91A, 563–574 (1978)

References
413
125. Shapiro, V.E., Loginov, V.M.: Dynamical Systems Under Random Inﬂuences.
Nauka, Novosibirsk (1983) (in Russian)
126. Stratonovich, R.L.: Topics in the Theory of Random Noise, vol. 1, 2. Gordon
and Breach, New York (1963, 1967)
127. Swerling, P.: Statistical properties of the contours of random surfaces. I.R.E.
Tranc. Inf. Theory IT-8, 315–321 (1962)
128. Tatarskii, V.I.: Depolarization of light by turbulent atmospheric inhomo-
geneties. Radiophys. & Quantum Electron. 10(12), 987–988 (1967)
129. Tatarskii, V.I.: Light propagation in a medium with random index refraction
inhomogeneities in the Markov process approximation. Sov. Phys. JETP 29(6),
1133–1138 (1969)
130. Tatarskii, V.I.: The Eﬀects of the Turbulent Atmosphere on Wave Propaga-
tion. National Technical Information Service. Springﬁeld, Va (1977)
131. Uhlenbeck, G.E.: The fundamental problems of statistical mechanics. Uspekhi
Fiz. Nauk 103(2), 275–318 (1971); Soviet Phys., Uspekhi
132. Weichman, P.B., Glazman, R.E.: Turbulent ﬂuctuation and transport of pas-
sive scalar by random wave ﬁelds. Phys. Rev. Letters 83(24), 5011–5018 (1999)
133. Weichman, P.B., Glazman, R.E.: Passive scalar transport by travelling wave
ﬁelds. J. Fluid Mech. 420, 147–200 (2000)
134. Weichman, P.B., Glazman, R.E.: Spatial variations of a passive tracer in a
random wave ﬁeld. J. Fluid Mech. 453, 263–287 (2002)
135. Whitham, G.B.: Linear and Nonlinear Waves. John Wiley & Sons Inc., New
York (1974)
136. Wilkinson, M., Mehlig, B.: Caustics in turbulent aerosols. Europhys. Let-
ters 71(2), 186–192 (2005)
137. Yaroshchuk, I.O.: Numerical Modeling of the Plane Wave Propagation in
Randomly Layered Linear and Nonlinear Media. Ph.D dissertation, Paciﬁc
Oceanological Institute, Vladivostok (1986)
138. Yudovich, V.I.: Example of the birth of secondary steady-state or periodic
ﬂow at stability failure of the laminar ﬂow of viscous noncompressible ﬂuid.
J. Appl. Math. and Mech. 29(3), 527–544 (1965)
139. Zel’dovich, Y.B., Molchanov, S.A., Ruzmaikin, A.A., Sokolov, D.D.: Inter-
mittency of passive ﬁelds in random media. Sov. Phys. JETP 62, 1188–1194
(1985)
140. Zel’dovich, Y.B., Molchanov, S.A., Ruzmaikin, A.A., Sokolov, D.D.: Intermit-
tency in random media. Sov. Phys. Usp. 30, 353–369 (1987)
141. Ziman, J.M.: Models of Disorder, The Theoretical Physics of Homogeneously
Disordered Systems. Cambr. Univ. Press, Cambridge (1979)
142. Zirbel, C.L., ¸Cinlar, E.: Mass transport by Brownian motion. In: Molchanov,
S.A., Woyczynski, W.A. (eds.) Stochastic Models in Geosystems. IMA Vol-
umes in Mathematics and its Applications, vol. 85, pp. 459–492. Springer,
New York (1996)

Index
Absorption
370
parameter
369
Ambartsumyan invariance principle
90
Approximation
Bourret
208, 300
delta-correlated Gaussian ﬁeld
240,
305–389
geometrical optics
64, 66, 109
Kraichnan
208, 300
ladder
211, 340
one-group
210
quasi-geostrophic
79
Asymptotic expansion
363
Backscattered ﬁeld
63
Bernoulli equation
399
Bete–Salpeter equation
211, 340
Biot–Savart law
401
Boltzmann constant
349
Boltzmann distribution
346
Boundary-value problem
22–33, 49,
60, 90–93
matched
28
unmatched
23
Brownian motion
10, 316, 345
Burgers equation
49, 234
with drift
234, 236
Caustic structure
65
Chaotic advection
4
Characteristic
curve
45
particles
34
rays
67
function
118–123
functional
126, 142, 151–154
Clustering
4, 7, 8, 162, 334
Coeﬃcient
reﬂection
23–32
transmission
23
Coriolis parameter
79
Description
Eulerian
34–44, 51, 100–109
Lagrangian
34–51, 100–109
Diﬀusion
approximation
311, 377–389
dynamic
40
in random ﬂows
34–44, 100–104,
198–200, 234–235, 370–373
in waves
383
particles
3–8, 222–232, 379–383
Distribution function
integral
117, 125, 128
probability
117
Duﬃng equation
11, 355, 359
Dynamic
causality
87, 89, 110, 167, 203
localization
25
Dyson equation
207, 211, 217, 299,
338
Einstein formula
349
Einstein–Smolukhovsky equation
346,
357
Eole experiment
8
Equation

416
Index
backward
89, 151, 197
continuity
34
forward
150, 196
transfer
64
transport
254
Euler equation
11, 69
Factorization property
52
Flow function
74
Fluctuation–dissipation theorem
349
Fokker–Planck equation
149, 230,
241, 251, 259, 305–375, 387, 388
backward
309, 360
extended
220–222, 226
forward
309, 360
steady-state
272
Formula
diﬀerentiation
176, 262
Furutsu–Novikov
168, 185, 187,
307, 378
Stokes
46
Fractal property of the Wiener process
318
Function
coherence
255
Dirac delta
83, 87, 118, 125
error
319
Gamma
327
Hankel
54, 399, 404
Heaviside
55, 87, 96, 117, 126, 235,
292, 335
indicator
35, 95–109, 109, 125–129,
306, 384
extended
101, 102, 105, 109
Legendre
341
mass
205, 294, 296, 298, 339
McDonalds
342
stream
79
vertex
205, 293–296, 299
Functional
characteristic
109
mass
207
vertex
207
Galerkin method
75
Game of vortex rings
401
Gibbs distribution
272, 346
Hamilton equation
51, 67
Hamilton–Jacobi equation
51
Hamiltonian function
10
Hamiltonian system
10, 345, 355
Helicity
158
Helmholtz equation
23–33, 52, 60–62
Hopf equation
98, 109, 114, 200, 202,
213
Hopping phenomenon
10
in regular systems
355
in singular systems
359
Hydrodynamic-type system
11–20,
347–355
Hydrodynamics
geophysical
79
turbulence
67, 113–114, 198,
201–203, 213–218, 258–260
Induction equation
39
Intermittency
129, 321
Jacobian
51, 79
Kinetic operator
261
Kolmogorov ﬂow
73
Kolmogorov–Chapman equation
144
Kolmogorov–Feller equation
150,
240–242, 251
Kramers
frequency
359
problem
11, 230, 346
time
358
Kronecker delta
119
Landau equation
374
Langevin equation
47, 311, 315
Legendre
equation
343
polynomial
404
Liouville equation
95–109, 306, 384
backward
97, 99–100, 309
Localization
334
Lorentz model
13
Lyapunov characteristic index
323
Mach number
398
Majorant curve
324–325
Markovian process
134, 142–154,
178–181, 262, 323
continuous
148
discrete
145
discrete-continuous
149

Index
417
ﬁnite-dimensional
289–291
Gaussian
140, 283–289, 297–301
Poisson
134, 136
telegrapher’s
134, 137–141, 146–
147, 153, 173–177, 263–274, 293,
295
generalized
134, 141–142, 147–
150, 153–154, 177–178, 275–283,
295, 297
Mathieu equation
22
Maxwell distribution
346, 349
Maxwell’s equation
59
Method
characteristic
163
imbedding
27, 49, 90
of characteristics
34, 47, 67
of cumulant expansions
364
of fast oscillation averaging
365–375
sweep
26
Millionshchikov hypothesis
391
Motion
baroclinic
80
barotropic
33, 80
of particle
6
of triplet (gyroscope) 15
Navier–Stokes equation
68, 113
Newton equation
9
Orr–Sommerfeld equation
75
Parabolic equation
58, 63, 66, 109,
110, 198, 200, 254
generalized
63
nonlinear
65
Parametric resonance
stochastic
21, 244–251, 265–269,
277–279, 286–287, 367–370
Phase screen
66
Poisson
bracket
346
distribution
123
formula
133
Probability density
118–161, 194, 198
ﬂux
360, 362
steady-state
362
steady-state
344–347, 354, 356, 362
transition
143–152, 309
Probability integral
122
Probing property of the delta-function
96
Pseudotensor
39
Pseudovector
39
Quasi-geostrophic model
32
Random ﬁeld
155, 161
delta-correlated
237, 240
Gaussian
306
Random process
125–142
delta-correlated
136, 181–189, 237,
240
Gaussian
181, 240, 247
Poisson
182, 240–242, 251
diﬀusion
149
discontinuous
133
Gaussian
130, 149, 168
lognormal (logarithmic-normal) 132,
320–334
Ohrnstein–Ulenbeck
314
Poisson
172
delta-correlated
137
Wiener
316–320, 325
Random quantity
117–123
Gaussian
121
Reynolds number
15, 73
Reynolds stress tensor
393
Riccati equation
27, 29, 30, 93
Riemann equation
47
Riemann wave
47
Rogue wave
72
Schr¨odinger equation
65
nonlinear
65
Schwinger equation
206, 216
Shot noise
253
Smolukhovsky equation
144, 145, 148,
150
Stochastic resonance
359
Transfer radiative
23
Transform
Fourier
61, 69, 118, 119, 143, 156,
201, 258, 316, 364, 395
Kontorovich–Lebedev
342
Laplace
292–301
Meler–Fock
341
Typical realization curve
128, 129,
132, 324

418
Index
Variational derivative
48, 81–88, 114,
127
Velocity vortex ﬁeld
157
Vortex
line
398
ring
401
Wave
absorption
23, 24
propagation
in 3D random media
59–67,
110–113, 198, 200–201, 254–258
in layered random media
22–33
rogue
66
Rossby
32, 80

