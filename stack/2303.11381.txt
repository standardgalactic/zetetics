MM-REACT
: Prompting ChatGPT for Multimodal Reasoning and Action
Zhengyuan Yang∗, Linjie Li∗, Jianfeng Wang∗, Kevin Lin∗, Ehsan Azarnasab∗, Faisal Ahmed∗,
Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang♠
Microsoft Azure AI
{zhengyang,lindsey.li,jianfw,keli,ehazar,fiahmed,zliu,ce.liu,nzeng,lijuanw}@microsoft.com
Visual Math and Text Reasoning
Multi-Image Reasoning
Open-World Concept Understanding
Video Summarization/Event Localization
Multi-Hop Document Understanding
Visual-Conditioned Joke/Meme
Spatial/Coordinate Understanding
Visual Planning and Prediction
What is “a” in the first equation?
What is y in the last equation?
The value of “a” in the first equation “5 
+ a = 7” is 2.
The value of “y” in the last equation 
“34 + y = 39” is 5.
How much in total did I spend on travel?
How much in total did I pay for taxes?
You spent $1299.90 on airfare and $43.83 
on an Uber ride for a total of $1343.73 on 
travel. 
You paid $323.23 in total for taxes. This 
includes $321.90 for taxes and fees for 
the flight purchase and $1.33 for sales 
tax for the Windmill Family Restaurant.
Excluding the regular cab, what is the average total fueling 
cost?
The average total fueling cost excluding 
the Ford F150 (Regular Cab) is $76.55. This 
is calculated by taking the sum of the total 
fueling cost of Toyota RAV4 and Honda 
Civic and dividing it by 2. ($82.56 + 
$70.55) / 2 = $76.55.
Where is the frisbee?
What is the relative position of the frisbee and the man?
The frisbee is located at coordinates 
(63,119,133,177).
The frisbee is located above the man in 
the image. The man is falling on the 
ground while the frisbee is in the air. The 
frisbee is also closer to the top of the 
image than the man..
What is funny about it?
The humor in the comic may come from 
the situation, the characters, or the 
dialogue. The person is feeling unwell 
and is asked if they are pregnant or have 
the coronavirus, which is a current and 
relevant topic. The humor may come 
from the absurdity of the situation or the 
unexpectedness of the question. 
What should I do after kneading the dough, before dividing it 
into pieces? For how long?
After kneading the dough, you should 
put it in a warm place before dividing it 
into pieces, according to the recipe in 
the image.
You should put the dough in a warm 
place for 1 hour.
Can you breakdown this video tutorial to step-by-step 
instructions, with start and end time?
Here's a step-by-step breakdown of the 
tutorial with start and end time.
What is the brand of these drink? Which 
one is unsweetened?
The brand of these 
drinks is “BETTER THAN 
MILK”. The unsweetened 
one is “BETTER THAN 
MILK organic OAT 
DRINK”.
Who are the celebrities in this image?
Keira Knightley and 
Natalie Portman are 
the celebrities in this 
image.
Is it safe to eat this mushroom?
Morel mushrooms are 
generally considered safe 
to eat when properly 
cooked. 
Add Lettuce
Add Tomato
Add Bacon
…
…
Figure 1. MM-REACT allocates specialized vision experts with ChatGPT to solve challenging visual understanding tasks through multi-
modal reasoning and action. For example, the system could associate information from multiple uploaded receipts and calculate the total
travel cost (“Multi-Image Reasoning”). We only highlight key information here and postpone full MM-REACT responses to Figures 4-14.
Abstract
We propose MM-REACT, a system paradigm that inte-
grates ChatGPT with a pool of vision experts to achieve
multimodal reasoning and action. In this paper, we deﬁne
and explore a comprehensive list of advanced vision tasks
that are intriguing to solve, but may exceed the capabilities
of existing vision and vision-language models. To achieve
such advanced visual intelligence, MM-REACT introduces
a textual prompt design that can represent text descriptions,
textualized spatial coordinates, and aligned ﬁle names for
dense visual signals such as images and videos.
MM-
REACT’s prompt design allows language models to accept,
associate, and process multimodal information, thereby fa-
cilitating the synergetic combination of ChatGPT and var-
ious vision experts.
Zero-shot experiments demonstrate
MM-REACT’s effectiveness in addressing the speciﬁed ca-
pabilities of interests and its wide application in different
scenarios that require advanced visual understanding. Fur-
thermore, we discuss and compare MM-REACT’s system
paradigm with an alternative approach that extends lan-
guage models for multimodal scenarios through joint ﬁne-
tuning. Code, demo, video, and visualization are available
at https://multimodal-react.github.io/.
∗Equal Contribution
♠Project Lead
1
arXiv:2303.11381v1  [cs.CV]  20 Mar 2023

1. Introduction
Recent years have seen signiﬁcant advancement for com-
puter vision, thanks to improved network architecture [4,9,
12,28], large-scale model training [8,29,35], and other fac-
tors. However, different vision problems typically require
different models, which often require manual selection and
composition of individual models for each use case. For ex-
ample, when determining if an image contains “people”, we
may choose the image tagging model [7,16,19] and check if
the predicted tag list contains “people”. If “people” exists,
we may select the celebrity model [20] to further understand
whether a celebrity appears and who he/she is.
One research direction is to combine the vision and
language modules as one end-to-end model, such as
Flamingo [2], PaLM-E [10], to provide a dialogue-based
experience to the end user. That is, the user can use natural
language to interact with the model around the image con-
tent. The vision module encodes vision signals into special
text tokens or features that the language module can un-
derstand, enabling the system to utilize the language mod-
ule for understanding user queries and providing responses.
However, these joint ﬁnetuning approaches require a large
amount of computing resources and annotated data to en-
able speciﬁc capabilities. In this work, we aim to combine
existing individual vision models with the language model
in a more ﬂexible manner to tackle complicated visual un-
derstanding problems, e.g., the ones illustrated in Figure 1.
Large language models (LLMs) [3,6], such as ChatGPT,
have shown impressive dialogue capability with text as both
input and output. Recent NLP research [11,24,26,34] (e.g.,
REACT [34]) demonstrates the effectiveness of integrating
external NLP tools, such as search engines and math calcu-
lators, with LLMs by proper instruction. Speciﬁcally, RE-
ACT [34] prompts an LLM to generate reasoning texts that
break down complex problems into intermediate steps, and
action texts that allocate NLP tools for solving these steps.
One example is that the LLM can suggest a text query to
a modern search engine to grab the latest internet informa-
tion, and return the user with the information that is not in
the pre-training corpus. Inspired by the efﬁcacy of reason-
ing and acting with LLMs and NLP tools, we explore the
integration of vision expert tools with LLMs.
To this end, we present MM-REACT, a system paradigm
that composes numerous vision experts with ChatGPT for
multimodal reasoning and action. To enable images and
videos as inputs, we use their ﬁle path as the input to Chat-
GPT. The ﬁle path functions as a placeholder, allowing
ChatGPT to treat it as a black box. Whenever a speciﬁc
property such as celebrity names or box coordinates is re-
quired, ChatGPT is expected to seek help from a speciﬁc
vision expert to identify the desired information. To inject
the knowledge of vision experts’ usages into ChatGPT, we
add instructions to ChatGPT prompts about each expert’s
capability, input argument type, and output type, along with
a few in-context examples for each expert. Additionally, a
special watchword is instructed such that we can use regex
expression matching to invoke the expert accordingly.
We show MM-REACT’s representative visual under-
standing capabilities in Figure 1.
For example, MM-
REACT could associate information from multiple up-
loaded receipts and calculate the total travel cost (“Multi-
Image Reasoning”), recognize and answer questions about
the “morel mushrooms” (“Open-World Concept Under-
standing”), and condense a long video into representative
thumbnails (“Video Summarization and Event Localiza-
tion”). These visual intelligence features are similar to those
found in recent models, such as multimodal GPT-4 [23] and
PaLM-E [10], but achieved via prompting instead of addi-
tional multimodal training. The MM-REACT system may
provide extra ﬂexibility in module upgrades, and may be
effective in certain visual understanding tasks by better uti-
lizing existing specialized vision experts, such as celebrity
recognition and dense captioning.
2. Related Work
LLMs Prompting Methods.
Large language models
(LLMs) [3,6] demonstrate a strong chain-of-thought (CoT)
capability [17, 31] that could break down complex prob-
lems into solvable intermediate steps. On the other hand,
research [1, 15, 22] shows that LLMs, when equipped with
a range of external NLP tools, can effectively serve as
action planners to select and utilize tools for problem-
solving, such as using search or mathematical tools to ad-
dress knowledge or math problems.
Nevertheless, LLMs for reasoning [17,31] and LLMs for
action [1, 15, 22] , when used independently, fail to solve
complex tasks that require breaking down the problem via
reasoning and solving sub-steps via planned actions. Re-
cent studies [11,24,26,34] have attempted to merge the ac-
tion and reasoning phases to enhance LLMs’ capabilities
in solving complicated tasks that require advanced plan-
ning and reasoning. One representative work, REACT [34],
treats reasoning text generation as an executable action and
achieves the synergetic combination of reasoning and action
for NLP tasks. In this work, we explore how to extend such
intriguing properties into multimodal scenarios by model-
ing thought and invoking vision tools as executable actions.
Vision+LLMs.
Our MM-REACT is related to the pre-
vious studies that extend language models to understand
visual inputs.
The representative framework adds a vi-
sion module to project visual inputs into representations
that the language model can understand. These represen-
tations can be either discrete text words [13, 30, 33, 36]
or continuous features projected into the textual feature
space [2, 10, 14, 18, 27].
Recent vision-language studies
2

wait for user input
image as <path>
ChatGPT responds
parse <model> and <path>
execute the action
respond to user
yes
no
output as text
observation
image caption
image tagging
dense caption
OCR
receipt
celebrity
Bing search
vision experts
…
This is an image. 
Assistant, what objects do you see in this 
image? <ImagePath>
Thought
Action request
Observation
Image Captioning
This is an image (694 x 720) with description a basketball player in a 
yellow jersey with a ball in his hand.
Dense Captioning
List of object descriptions, and their locations in this image:
a close up of a basketball (x1, y1, x2, y2)  = (63, 535, 190, 663)
a basketball player in a yellow jersey (x1, y1, x2, y2)  = (55, 19, 502, 700)
…
Object Tagger
List of object tags seen in this image:
sport, athletic game, basketball, person, sports uniform, ball, 
sportswear, basketball player, basketball moves, sports equipment, 
basketball court …
Face Detection
Detected faces, and their location in this image:
man (x1, y1, x2, y2)  = (365, 278, 439, 352)
man (x1, y1, x2, y2)  = (548, 308, 620, 380)
thought & action request?
Invoke vision experts over <ImagePath>
Action execution
Figure 2. Flowchart of MM-REACT for enhanced visual understanding with ChatGPT. The user input can be in the form of text, images,
or videos, with the latter two represented as ﬁle path strings. ChatGPT is instructed to say speciﬁc watchwords in action request if a vision
expert is required to interpret the visual inputs. Regular expression matching is applied to parse the expert’s name and the ﬁle path, which
are then used to call the vision expert (action execution). The expert’s output (observation) is serialized as text and combined with the
history to further activate ChatGPT. If no extra experts are needed, MM-REACT would return the ﬁnal response to the user. The right
ﬁgure shows a single-round vision expert execution, which is the component that constructs the full execution ﬂow illustrated in Figure 3.
explore the chain-of-thought capability [17, 31] in multi-
modal settings. MM-CoT [37] ﬁnetunes on the reasoning
chain annotated in the ScienceQA [21] dataset to achieve
the CoT capability in the science question answering task.
KOSMOS-1 [14] and PaLM-E [10] demonstrate the zero-
shot multimodal CoT capabilities with large-scale training.
Multimodal Reasoning and Action.
A key distinction
between MM-REACT and prior vision+LLM studies dis-
cussed above is that MM-REACT leverages LLMs’ high-
level planning abilities to allocate various vision experts,
rather than solely using LLMs for text generation condi-
tioned on visual inputs.
MM-REACT is closely related
to the recent concurrent work of Visual ChatGPT [32]
and ViperGPT [25]. In comparison, Visual ChatGPT [32]
primarily focuses on image generation and editing, while
our MM-REACT mainly focuses on visual understanding.
ViperGPT [25] instructs LLMs to generate Python code for
a one-round query answering. In contrast, MM-REACT is a
multi-round, dialogue-based system that may integrate the
strong QA model as one of its vision experts.
3. MM-REACT Prompting
The goal of MM-REACT is to compose numerous vision
experts to empower ChatGPT with visual understanding. A
vision expert is a computer vision model that takes an image
as input and interprets the content from different perspec-
tives. For instance, the image captioning expert generates a
natural description, the OCR expert extracts the scene text
in the image, the celebrity recognition model identiﬁes the
celebrity names, and the object detection model extracts the
salient object with bounding box locations. At present, one
may have to manually decide which vision experts to em-
ploy for speciﬁc use cases, and manually compose them. In-
stead, our target is to automate this process based on the re-
quirements presented by the user query in natural language.
ChatGPT is an artiﬁcial intelligence chatbot with text as
both input and output, without visual understanding. How-
ever, ChatGPT exhibits strong instruct learning capability,
which motivates us to instruct ChatGPT to properly deter-
mine which vision expert should be invoked and which im-
age should be processed.
Figure 2 shows the ﬂowchart of our MM-REACT sys-
tem. The terms thought and action request refer to the rea-
soning and action-oriented texts generated by ChatGPT to
break down the problem or invoke vision experts. Observa-
tion refers to the vision expert’s responses after the action
execution requested in the action request text. Next, we de-
tail each step in the ﬂowchart as follows.
3.1. User Input
As ChatGPT only accepts texts as input, the ﬁrst chal-
lenge is how to accommodate non-text inputs, such as mul-
tiple images and videos. Since most vision experts accept
3

the ﬁle path or URL, we use the path string to indicate non-
text inputs. The ﬁle path itself is meaningless and is essen-
tially a placeholder. Although no visual recognition task
can be performed directly with ﬁle paths, ChatGPT may
seek help from different vision experts to understand the
image content from different perspectives, e.g., identifying
the celebrity names of the detected person. By including the
provided ﬁle path in its text output, ChatGPT can indicate
which image should be processed by the vision expert.
3.2. ChatGPT Response
Given the user’s input, ChatGPT is expected to provide
two kinds of responses. The ﬁrst is to seek help from vision
experts, while the second is to respond to the user directly.
A key challenge is to set up a protocol such that we know
when to invoke the vision expert. Inspired by REACT [34],
we instruct ChatGPT to respond with certain watchwords,
such as “Assistant, what objects are there in the image?
<ﬁle path>”, if a speciﬁc vision expert is required. In our
implementation, we use the keyword “Assistant,” to distin-
guish whether a vision expert is required.
To further improve the performance, we encourage Chat-
GPT to show the thought (reasoning) process to highlight
why an external tool is required. It is also shown to be ben-
eﬁcial in NLP studies [34] to incorporate such reasoning.
3.3. Vision Experts
Given the action request from ChatGPT, we use the reg-
ular expression matching to parse the expert name and the
ﬁle path, and invoke the action (vision expert execution).
The expert’s output can be in different forms but is stan-
dardized into the text format such that ChatGPT can under-
stand it. For certain experts, such as the captioning model
or the celebrity model, it is straightforward to represent the
output as text. However, the standardization is less intuitive
for others. For example, the detection model outputs a list
of object names with bounding box locations. In this case,
we concatenate all the boxes, each of which is represented
as <object name, x1, y1, x2, y2>, where (x1,y1), (x2,y2)
are the coordinates of the top-left and bottom-right corners,
respectively. An additional text description is added to ex-
plain the meaning of the last four numerical values. In some
cases, we ﬁnd ChatGPT is capable of understanding these
coordinates, e.g., identifying which object is on the left.
The text-formed output from vision experts can be inter-
preted as the observation resulting from ChatGPT’s action
of invoking the vision expert. Combining observations with
the chat history, ChatGPT can further invoke additional ex-
perts or return the ﬁnal answer to the user. We provide ex-
amples of full execution ﬂows in Section 4.2 and Figure 3.
To inject the knowledge of various vision experts’ us-
ages, we add both instructions and in-context examples in
the preﬁx when prompting ChatGPT. Each expert is de-
scribed with the model name, a general description of its
capability, the input data format, and the output informa-
tion. After describing each expert, we add a few in-context
dialogue examples to enhance the performance. With the
injected knowledge, ChatGPT can effectively select one or
multiple vision experts to understand the images or videos
from different perspectives.
3.4. Extensibility
Our scheme is motivated by REACT, which invokes dif-
ferent tools in the NLP ﬁeld. As only the text is involved,
no speciﬁc design is required to incorporate other modal-
ities. In this work, we extend MM-REACT to the vision
domain. The key is to replace the non-text modality with a
path string, enabling ChatGPT to ask speciﬁc vision experts
to recognize the visual content. Therefore, we could further
extend MM-REACT to other modalities, such as speech and
audio.
Meanwhile, we can also easily incorporate more
tools by formatting their outputs in a text format. While
ChatGPT serves as the primary LLM in our main imple-
mentation, performance could be further enhanced through
the simple upgrade to a more powerful LLM, such as GPT-
4 [23] discussed in Section 4.5.
4. Experiments
4.1. Experiment Setup
We implement MM-REACT based on the LangChain
codebase [5] and reference ideas from ReAct [34]. We ac-
cess ChatGPT via the Azure “gpt-3.5-turbo” API that has a
token length limit of 4,096, and utilize vision experts pub-
licly available via the Azure Cognitive Services APIs1, in-
cluding the ones for image captioning, image tagging, dense
captioning, optical character recognition (OCR), and spe-
cialized recognition models for celebrities, receipts, etc. We
further expand the toolset with customized tools for spa-
tial understanding and image editing, and tools from other
modalities such as Bing search and PAL math.
4.2. MM-REACT’s Full Execution Flow
Figure 3 provides an example to illustrate MM-REACT’s
full execution ﬂow. We highlight the exact order to call
different models (i.e., executions) with numbered blue cir-
cles. The executions, highlighted by bold underlined text,
can be either a ChatGPT call (e.g., “ChatGPT:”) or the
execution of one or multiple selected vision experts (e.g.,
“Image Captioning”). We add a commentary text action
execution in dashed boxes to help understand the vision ex-
pert execution. The action execution is not an actual input or
output in the MM-REACT ﬂow. ChatGPT executions can
be used to generate thought (reasoning) and action texts that
1https://azure.microsoft.com/en- us/products/
cognitive-services/vision-services
4

Convert image to <ImagePath> and 
send to ChatGPT
Thought 1
ChatGPT: This is an image.
Action request 1
Assistant, what objects do you see 
in this image? <ImagePath>
Invoke vision experts, including 
Image Captioning, Dense 
Captioning, Object Tagger, Face 
Detection.
Observation 1
Image Captioning
Assistant: This is an image (694 x 
720) with description a basketball 
player in a yellow jersey with a ball in 
his hand.
Dense Captioning
List of object descriptions, and their 
locations in this image:
a close up of a basketball 
(x1,y1,x2,y2) = (63, 535, 190, 663)
a basketball player in a yellow jersey 
(x1,y1,x2,y2) = (55, 19, 502, 700)
…
Object Tagger
List of object tags seen in this image:
sport, athletic game, basketball, … 
Face Detection
Detected faces, and their location in 
this image:
man
(x1,y1,x2,y2)=(365,278,439,352)
man 
(x1,y1,x2,y2)=(548,308,620,380)
Thought 2
Action request 2
Observation 2
ChatGPT: There are two 
faces detected in the image.
Assistant, are there any celebrities  
in this image? <ImagePath>
Invoke Celebrity Recognition
Celebrity Recognition
This image contains celebrities
List of celebrities, and their 
location in this image:
Kobe Bryant 
(x1,y1,x2,y2)=(365,278,439,352)
Paul Pierce 
(x1,y1,x2,y2)=(548,308,620,380)
ChatGPT: This image contains two celebrities, 
Kobe Bryant and Paul Pierce. They are both 
basketball players.
How many championship rings did the player 
on the left win in his career?
Thought 3
Action request 3
Observation 3
ChatGPT: This question 
requires Bing Search
Assistant, Bing search how many 
championship rings did Kobe 
Bryant win in his career? 
<ImagePath>.
Bing Search
Assistant: Bryant, a shooting 
guard, and O’Neal, a centre, 
meshed into a remarkably 
effective combination, and, by the 
time Bryant was 23, the Lakers had 
won three consecutive NBA 
championships. Kobe Bryant After 
winning their third title in 2002, 
Bryant and the Lakers 
encountered difficulties. Kobe 
Bryant has won a total of 5 rings in 
20 seasons…
ChatGPT: Kobe Bryant won 5 championship 
rings in his career.
1
3
4
5
6
7
8
Action execution 1
Action execution 2
Action execution 3
2
Invoke Bing Search
Figure 3. An example of MM-REACT’s full execution ﬂow. The blue circles with numbered indices indicate the order in which dif-
ferent models are called (i.e., the executions). The executions, highlighted by bold underlined text, can be either a ChatGPT call (e.g.,
“ChatGPT:”) or running one or multiple selected vision experts (e.g., “Image Captioning”). We add a commentary text action execution
in dashed boxes to help understand the expert execution. Each ChatGPT execution takes the preceding text as input and generates the text
leading up to the next execution (e.g., “This is an image. Assistant, what . . . image? <ImagePath>” for Execution 1). Texts in gray rep-
resent MM-REACT’s thoughts or vision experts’ actions and outputs, which are invisible to users. This multimodal reasoning and action
process occurs behind the scene to gather the necessary information for generating ﬁnal responses to users, which are shown in black.
5

allocate vision experts
(invisible to users), or produce the
ﬁnal response to users
. Each ChatGPT execution takes
the preceding text as input and generates the text leading
up to the next execution (e.g., “This is an image. Assistant,
what objects do you see in this image? <ImagePath>” for
Execution 1). ChatGPT “learns” the proper text to gener-
ate based on the instructions and in-context examples in the
prompt preﬁx, as detailed in Section 3.3. Additional exam-
ples of the reasoning and execution procedures are in Fig-
ures 18-22.
4.3. MM-REACT Capabilities and Applications
Figures 4-14 show the representative capabilities and ap-
plication scenarios that MM-REACT demonstrates. Specif-
ically, we examine MM-REACT’s capabilities in visual
math and text reasoning (Figure 4), understanding visual-
conditioned jokes and memes (Figure 5), spatial and coor-
dinate understanding, visual planning and prediction (Fig-
ure 6), multi-image reasoning (Figure 7), multi-hop doc-
ument understanding on bar charts (Figure 8), ﬂoorplans
(Figure 9), ﬂowcharts (Figure 10), tables (Figure 11), open-
world concept understanding (Figure 12), and video analy-
sis and summarization (Figure 13, 14). We provide an ex-
ample of the unfolded steps in Figure 18.
4.4. Capability Comparison with PaLM-E
MM-REACT is a training-free scheme which composes
existing vision experts with ChatGPT, while PaLM-E [10]
trains a vision-language model which combines an image
encoder and a text decoder with dedicated datasets. Fig-
ures 15-17 shows our MM-REACT can achieve competitive
results to PaLM-E. We further illustrate the complete mul-
timodal reasoning and action procedures in Figures 21,22.
4.5. MM-REACT Extensibility
In Figure 23 and 24, we explore the enhancement of
MM-REACT’s LLM from ChatGPT (“gpt-3.5-turbo”) to
GPT-4 (language-only). We access the language-only GPT-
4 via the ChatGPT website and reference the multimodal
GPT-4 demo provided by OpenAI for comparison. These
examples demonstrate the beneﬁt of MM-REACT’s ex-
tensibility: MM-REACT equipped with GPT-4 correctly
answers the physics question (Figure 23), while the ver-
sion with ChatGPT (GPT-3.5) fails.
Furthermore, MM-
REACT is designed with the ﬂexibility to incorporate new
tools without training. Figure 25 provides a case study of
plugging an image editing tool from X-decoder [38] for
multi-round, dialogue-based image editing.
4.6. Limitations
We identify the following limitations. 1). Considering
the recognition capability in the wild, we ﬁnd it hard to sys-
tematically evaluate the performance with concrete accu-
racy numbers, due to a lack of annotated benchmarks. Thus,
it is worth investing efforts towards how to effectively eval-
uate such system’s performance. 2). The vision capability
is limited by the integrated vision experts. On one hand, the
integrated experts may make mistakes; on the other hand,
the system may fail if the necessary experts are missing. 3).
We inject the vision experts’ knowledge in the preﬁx, and
thus the number of experts is limited by the context window
(4096 tokens) of ChatGPT. 4). Visual signals are converted
to text words for ChatGPT understanding, which might not
be the optimal solution for certain vision tasks. 5). MM-
REACT requires manual prompt engineering. We expect
research work to automate this process, making the system
even easier to develop.
5. Conclusion
We have presented MM-REACT, a system paradigm
that
synergizes
multimodal
reasoning
and
action
to
tackle complicated visual understanding problems. MM-
REACT presents a simple and ﬂexible way to empower
LLMs with a pool of vision experts. Extensive zero-shot ex-
periments demonstrate MM-REACT’s capabilities in solv-
ing a wide range of challenging understanding tasks, such
as multi-image reasoning, multi-hop document understand-
ing, open-world concept understanding, video summariza-
tion, and more.
Acknowledgment
We would like to express our gratitude to Jianfeng Gao for his
valuable suggestions, as well as to Jianwei Yang for generously
providing the image editing tool utilizing the X-Decoder frame-
work.
References
[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-
otar, Omar Cortes, Byron David, Chelsea Finn, Keerthana
Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i
can, not as i say: Grounding language in robotic affordances.
arXiv preprint arXiv:2204.01691, 2022. 2
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katie Millican, Malcolm Reynolds, et al. Flamingo: a vi-
sual language model for few-shot learning. arXiv preprint
arXiv:2204.14198, 2022. 2
[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In NeurIPS, 2020. 2
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In ECCV, 2020. 2
[5] Harrison Chase.
Langchain.
https://langchain.
readthedocs.io/, 2023. 4
6

Figure 4. Case studies of MM-REACT’s capabilities and application scenarios: visual math and text reasoning.
7

[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 2
[7] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhip-
ing Luo, and Yantao Zheng. Nus-wide: a real-world web im-
age database from national university of singapore. In Pro-
ceedings of the ACM international conference on image and
video retrieval, pages 1–9, 2009. 2
[8] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr
Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner,
Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin,
et al. Scaling vision transformers to 22 billion parameters.
arXiv preprint arXiv:2302.05442, 2023. 2
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020. 2
[10] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey
Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,
Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
and Pete Florence. Palm-e: An embodied multimodal lan-
guage model. In arXiv preprint arXiv:2303.03378, 2023. 2,
3, 6, 20, 21, 22
[11] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei
Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
Pal:
Program-aided language models.
arXiv preprint
arXiv:2211.10435, 2022. 2
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR,
2016. 2
[13] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A
Smith, and Jiebo Luo.
Promptcap: Prompt-guided task-
aware image captioning. arXiv preprint arXiv:2211.09699,
2022. 2
[14] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
Owais Khan Mohammed, Qiang Liu, et al.
Language is
not all you need: Aligning perception with language mod-
els. arXiv preprint arXiv:2302.14045, 2023. 2, 3
[15] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor
Mordatch. Language models as zero-shot planners: Extract-
ing actionable knowledge for embodied agents. In Interna-
tional Conference on Machine Learning, pages 9118–9147.
PMLR, 2022. 2
[16] Mark J Huiskes and Michael S Lew. The mir ﬂickr retrieval
evaluation. In Proceedings of the 1st ACM international con-
ference on Multimedia information retrieval, pages 39–43,
2008. 2
[17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
Matsuo, and Yusuke Iwasawa. Large language models are
zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
2, 3
[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2:
Bootstrapping language-image pre-training with
frozen image encoders and large language models.
arXiv
preprint arXiv:2301.12597, 2023. 2
[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV, 2014. 2
[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
the IEEE international conference on computer vision, pages
3730–3738, 2015. 2
[21] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and
Ashwin Kalyan.
Learn to explain: Multimodal reasoning
via thought chains for science question answering.
arXiv
preprint arXiv:2209.09513, 2022. 3
[22] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse, Shantanu
Jain, Vineet Kosaraju, William Saunders, et al.
Webgpt:
Browser-assisted question-answering with human feedback.
arXiv preprint arXiv:2112.09332, 2021. 2
[23] OpenAI. Gpt-4 technical report. 2023. 2, 4
[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-
cedda, and Thomas Scialom. Toolformer: Language mod-
els can teach themselves to use tools.
arXiv preprint
arXiv:2302.04761, 2023. 2
[25] D´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. Vipergpt:
Visual inference via python execution for reasoning. arXiv
preprint arXiv:2303.08128, 2023. 3
[26] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and
Ashish Sabharwal.
Interleaving retrieval with chain-of-
thought reasoning for knowledge-intensive multi-step ques-
tions. arXiv preprint arXiv:2212.10509, 2022. 2
[27] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Es-
lami, Oriol Vinyals, and Felix Hill.
Multimodal few-
shot learning with frozen language models. arXiv preprint
arXiv:2106.13884, 2021. 2
[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS, 2017. 2
[29] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
Git: A generative image-to-text transformer for vision and
language. arXiv preprint arXiv:2205.14100, 2022. 2
[30] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou,
Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chen-
guang Zhu, Derek Hoiem, et al. Language models with im-
age descriptors are strong few-shot video-language learners.
In Advances in Neural Information Processing Systems. 2
[31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought
prompting elicits reasoning in large language models. arXiv
preprint arXiv:2201.11903, 2022. 2, 3
8

[32] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,
Zecheng Tang, and Nan Duan.
Visual chatgpt: Talking,
drawing and editing with visual foundation models. arXiv
preprint arXiv:2303.04671, 2023. 3
[33] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yu-
mao Lu, Zicheng Liu, and Lijuan Wang. An empirical study
of gpt-3 for few-shot knowledge-based vqa. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, volume 36,
pages 3081–3089, 2022. 2
[34] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
Karthik Narasimhan, and Yuan Cao.
React: Synergizing
reasoning and acting in language models.
arXiv preprint
arXiv:2210.03629, 2022. 2, 4
[35] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li,
Chunyuan Li,
et al.
Florence:
A new
foundation model for computer vision.
arXiv preprint
arXiv:2111.11432, 2021. 2
[36] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choro-
manski, Federico Tombari, Aveek Purohit, Michael Ryoo,
Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. So-
cratic models: Composing zero-shot multimodal reasoning
with language. arXiv preprint arXiv:2204.00598, 2022. 2
[37] Zhuosheng Zhang,
Aston Zhang,
Mu Li,
Hai Zhao,
George Karypis, and Alex Smola.
Multimodal chain-of-
thought reasoning in language models.
arXiv preprint
arXiv:2302.00923, 2023. 3
[38] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,
Chunyuan Li, Xiyang Dai, Jianfeng Wang, Lu Yuan, Nanyun
Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao. Gen-
eralized decoding for pixel, image and language. 2022. 6,
30
9

This image is a cartoon illustration of a 
squirrel holding a camera
Visual-conditioned Joke/Meme
Why would a person find this image funny?
The humor in this image is subjective and 
can vary from person to person. Some 
people might find the cartoon squirrel 
holding a camera funny because it is a cute 
and quirky illustration. Others might find it 
funny because it is a humorous take on the 
idea 
of 
a 
squirrel 
taking 
photos. 
Ultimately, the humor in this image is up 
to the interpretation of the individual 
viewer. So, it depends on the person’s 
sense of humor and personal preferences.
What is unusual about this image?
The unusual thing about this image is that 
a man is ironing clothes in the back of a 
yellow taxi. This is not a common sight and 
may be considered unusual or unique.
This image is likely a comic or cartoon 
illustration.
Is there text in the image? If so, can you 
read it and explain it to me?
The text in the image is a conversation 
between two people. One person says 
they don’t feel good and need to buy a 
test. The other person asks if the first 
person 
is 
pregnant 
or 
has 
the 
coronavirus. The text is likely part of a 
comic or cartoon illustration. The image is 
tagged with “@INNA_S_ART”, which may 
indicate the artist or creator of the image.
What is funny about it?
The humor in the comic may come from 
the situation, the characters, or the 
dialogue. The person is feeling unwell and 
is asked if they are pregnant or have the 
coronavirus, which is a current and 
relevant topic. The humor may come 
from the absurdity of the situation or the 
unexpectedness 
of 
the 
question. 
However, humor is subjective and what 
one person finds funny, another person 
may not. It is best to view the comic and 
decide for yourself if it is funny.
Figure 5. Case studies of MM-REACT’s capabilities and application scenarios: visual-conditioned joke/meme. We provide an example
of the unfolded multi-modal reasoning and action steps for meme understanding in Figure 18.
10

This image is likely a dog playing frisbee 
with a man.
Where is the frisbee in the image?
The frisbee is located at the top left 
corner of the image with coordinates 
(63, 119) and (133, 177). 
Where is the man in the image?
The man is located at the coordinates 
(66, 386) to (453, 604) in the image.
This image is a recipe for making bread at 
home. The ingredients include water, salt, 
flour, sugar, and yeast. The instructions 
include kneading the dough, dividing it 
into pieces, and baking it on each side 
until well done. The temperature is in 
Celsius.
What should I do after kneading the 
dough, before dividing it into pieces?
After kneading the dough, you should put 
it in a warm place before dividing it into 
pieces, according to the recipe in the 
image.
How long should I put in warm places?
Spatial/Coordinate Understanding
Visual Planning and Prediction
What is the spatial relation between the 
frisbee and the man?
The frisbee is located above the man in 
the image. The man is falling on the 
ground while the frisbee is in the air. 
The frisbee is also closer to the top of 
the image than the man..
You should put the dough in a warm 
place for 1 hour.
Illustration of detected 
bbox from MM-React
Figure 6. Case studies of MM-REACT’s capabilities and application scenarios: spatial/coordinate understanding and visual planning
and prediction.
11

How much did I spend on grocery?
You spent $25.97 on groceries at The Shop Store #100 in Chicago, IL. 
How much in total did I spend on travel?
You spent $1299.90 on airfare and $43.83 on an Uber ride for a total of $1343.73 on travel.
How much in total did I pay for taxes?
You paid $323.23 in total for taxes. This includes $321.90 for taxes and fees for the flight purchase 
and $1.33 for sales tax for the Windmill Family Restaurant expenses.
Multi-Image Reasoning
Receipt 1:
Receipt 2:
Receipt 3:
Receipt 4:
How much did I spend on dining out?
You spent $23.45 on dining out at the Windmill Family Restaurant. 
Figure 7. Case studies of MM-REACT’s capabilities and application scenarios: multi-image reasoning. For the multi-image inputs shown
above, we input one receipt image at a time. Once all four receipt images are provided as inputs to MM-REACT, we prompt it to answer
questions that require reasoning over multiple images.
12

Figure 8. Case studies of MM-REACT’s capabilities and application scenarios: multi-hop document understanding (bar charts). The
unfolded multi-modal reasoning and action steps of this example are detailed in Figure 19.
13

Figure 9. Case studies of MM-REACT’s capabilities and application scenarios: document understanding (ﬂoorplan).
14

Figure 10. Case studies of MM-REACT’s capabilities and application scenarios: document understanding (ﬂowchart). We provide an
example of the unfolded multi-modal reasoning and action steps for ﬂowchart understanding in Figure 20.
15

Figure 11. Case studies of MM-REACT’s capabilities and application scenarios: document understanding (table).
16

İ
Figure 12. Case studies of MM-REACT’s capabilities and application scenarios: open-world concept understanding.
17

Video Summarization/Event Localization
Please create a summary of the video in less than 10 sentences.
The speaker is making a BLT sandwich with grilled tomatoes and fried bacon. They are using fresh bread, 
mayo, Worcester sauce, lettuce, and tomatoes. They are sprinkling salt and pepper on top. The bread can 
be toasted or grilled. The speaker mentions that the few ingredients used in the sandwich will give it a lot 
of flavor. The speaker mentions that the bread is important and can be toasted or grilled. The final product 
is a BLT sandwich.
Can you breakdown this tutorial of making a BLT sandwich to step-by-step instructions, with start and end time?
Here's a breakdown of the start and end times for each step in the video:
1.
Heat a little olive oil in a pan and grill the tomatoes: 00:00:22 - 00:00:40
2.
Cook the bacon in a frying pan: 00:00:51 - 00:01:41
3.
Place the cooked bacon on a paper towel: 00:01:39 - 00:01:49
4.
Spread mayo and add Worcestershire sauce: 00:01:55 - 00:02:16
5.
Assemble the sandwich (lettuce, tomatoes, bacon): 00:02:22 - 00:03:11
a.
Layer lettuce: 00:02:44 - 00:02:49
b.
Add grilled tomatoes: 00:02:55 - 00:03:01
c.
Place bacon on top: 00:03:15 - 00:03:27
6.
Sprinkle salt and pepper: 00:03:08 - 00:03:11
7.
Top the sandwich with the other slice of bread: 00:03:29 - 00:03:34
I am trying to create a short version of the video, can you suggest a shorter segment for each step?
Here's a suggested shorter segment for each step:
1.
Heat a little olive oil in a pan and grill the tomatoes: 00:00:24 - 00:00:30
2.
Cook the bacon in a frying pan: 00:00:53 - 00:00:59
3.
Place the cooked bacon on a paper towel: 00:01:39 - 00:01:41
4.
Spread mayo and add Worcestershire sauce: 00:02:00 - 00:02:08
5.
Assemble the sandwich (lettuce, tomatoes, bacon): 00:02:44 - 00:03:01
a.
Layer lettuce: 00:02:44 - 00:02:47
b.
Add grilled tomatoes: 00:02:55 - 00:02:59
c.
Place bacon on top: 00:03:15 - 00:03:20
6.
Sprinkle salt and pepper: 00:03:08 - 00:03:11
7.
Top the sandwich with the other slice of bread: 00:03:29 - 00:03:34
1. Grill tomato
2.Fry Bacon
3.Drain Bacon
4. Spread Source
5a. Layer Lettuce
5b.Add Tomato
5c.Place Bacon
6.Sprinkle Salt/Pepper
Which segment is best for video thumbnail to attract more audience?
For a video thumbnail that attracts more audience, the segment 
showing the assembled sandwich with all its layers would be the 
most appealing. This image will showcase the delicious combination 
of ingredients and the final result of the recipe. The suggested 
segment for this is:
●
Assembled BLT sandwich: 00:03:29 - 00:03:34
This segment shows the completed sandwich just before adding the 
final slice of bread on top. You can take a screenshot of the 
sandwich from this segment or create a more stylized image based 
on it for your thumbnail.
Suggested Thumbnail at 00:03:29
Figure 13. Case studies of MM-REACT’s capabilities and application scenarios: video summarization/event localization.
18

Video Summarization/Event Localization
The video is about Kobe Bryant’s final game in the NBA. He scored 60 points and led the Lakers to a victory 
over the Utah Jazz. The crowd was out of their minds and the performance was considered one of the 
greatest in the history of the game. Kobe was fouled and made two free throws for 60 points. The video 
ends with the crowd applauding and Kobe’s final memory being a 60-point game and a victory over the 
Utah Jazz. Overall, it was a magical farewell performance for one of the greatest athletes ever.
How did Kobe win the last 2 points to get 60? Explain and locate the start and end time of relevant segment in 
this video.
Kobe Bryant won the last 2 points to reach 60 by making two free throws. He was fouled and went to the 
free-throw line. The relevant section starts at 00:04:02 with the mention of the foul, it ends at 00:04:39 
after Kobe successfully converts both free throws. 
Second Free throw at 00:04:31 - 00:04:34
First Free throw at 00:04:11 - 00:04:15
Please create a summary of the video in less than 10 sentences.
I want to make a short video to highlight the long deuce by Kobe if there is any in the video. Can you tell me 
roughly the start and end time of that.
The long deuce by Kobe Bryant is mentioned at timestamp 
00:03:28. The play leading up to this shot starts around 
00:03:21, and the audience’s reaction to the long deuce 
ends at approximately 00:03:32.
Figure 14. Case studies of MM-REACT’s capabilities and application scenarios: video summarization/event localization.
19

Visual-Conditioned Jokes
Please describe the image
This image is likely a dog jumping over 
a hurdle in a dog agility competition.
Describe the following <img>: A dog 
jumping over a hurdle at a dog show.
PaLM-E
Ours
I see (image omitted). Description: A cow sitting 
on a rug, Joke: Why is the cow sitting on a rug? 
Because it wants to be a cow-ch! .. I see <img>. 
Description: A donkey is carrying a dog, cat, and 
rooster. Joke: What do you call a donkey with a 
rooster on his back. A rooster booster.
Please write a joke for this image
Why did the cat stand on the dog’s 
back? Because the donkey was full!
Image Captioning, VQA
PaLM-E
Ours
Where is the coffee pot in the second image?
The coffee pot is located in the kitchen 
with a stove and cabinets. It is a silver 
coffee pot on a stove and its location is 
at (242, 230) to (295, 299).
Q: Where is <img1> in this scene <img2>? A: 
A stove top with a pot on it.
PaLM-E
Ours
Multi-Image Relationships
Image1
Image2
Illustration of detected 
bbox from MM-React
Figure 15. Comparison of MM-REACT with PaLM-E [10] on illustrated capabilities. We empirically show that text prompts are as
effective as expensive joint ﬁne-tuning in solving complicated vision problems.
20

End-to-End Egocentric Q&A, Dialogue
How many championship rings did the player on 
the left win in his career?
Kobe Bryant won 5 championship rings in his 
career.
Given <img>. Q: How many championship rings did the 
player on the left win in his career A: Let’s think step by 
step 1. kobe bryant is on the left. 2. kobe bryant has won
5 championship rings.
PaLM-E
Ours
Photo 1, at 10:30 am: <img1>. Photo 2, at 12:45 
pm: <img2>. Photo 3, at 3:45 pm: <img3>. Q: I 
forget, what did I have for lunch, and what time 
was it? A: Let’s think step by step. 1. you had a 
sandwich for lunch. 2. it was 12:45 pm
I forget, what did I have for lunch, 
and what time was it?
You had a sandwich for lunch at 
12:45pm.
Multimodal Chain-of-Thought
PaLM-E
Ours
Photo1 at 10:30am
Photo2 at 12:45pm
Photo3 at 4:45pm
Scene Text Reasoning
Given <img>. Q: I am getting just two custom pizza 
for me and my friend. How much should I pay in 
total? A: Let's think step by step. 1st step: 2 custom 
pizzas. 2nd step: 2 x $9.99. 3rd step: $19.98.
I am getting just two custom pizza for me and 
my friend. How much should I pay in total?
Based on the information you provided, you are 
ordering two custom pizzas. The price of each 
custom pizza is $9.99. So, the total cost of two 
custom pizzas would be $9.99 x 2 = $19.98
Ours
PaLM-E
Figure 16. Comparison of MM-REACT with PaLM-E [10] on illustrated capabilities. We empirically show that text prompts are as
effective as expensive joint ﬁne-tuning in solving complicated vision problems.
21

Robot Perception, Dialogue, Planning
What will the robot do next?
I’m sorry, I cannot predict the future 
actions of the robot. The image only 
shows its current state.
Given <img> . Q: What will the robot do next? 
A: fall.
PaLM-E
Ours
When a human asks me to do a task, I will 
respond with the sequence of actions I would 
do to accomplish the task with only the items 
I see. Human: Use all of the ingredients you 
see to make a cake batter.
To make a cake batter using the ingredients I 
see in the image, I would do the following:
1. Gather all of the bags of flour and empty 
them into a large mixing bowl.
2. Add any other dry ingredients, such as 
sugar or baking powder, to the bowl.
3. Mix the dry ingredients together until they 
are well combined.
4. Add any liquids, such as milk or eggs, to 
the bowl and mix until the batter is smooth.
5. Pour the batter into a greased cake pan 
and bake according to the recipe instructions.
Physical Prediction
Ours
When a human asks me to do a task, I will respond 
with the sequence of actions I would do to 
accomplish the task with only the items I see. 
Human: Use all of the ingredients you see to make 
a cake batter. 1. crack egg. 2. put egg in bowl. 3. 
put flour in bowl. 4. put sugar in bowl. 5. mix. 6. 
put in pan. 7. bake. 8. eat. 9. clean up.
PaLM-E
Figure 17. Comparison of MM-REACT with PaLM-E [10] on illustrated capabilities. We empirically show that text prompts are as
effective as expensive joint ﬁne-tuning in solving complicated vision problems.
22

Convert image to <ImagePath> and 
send to ChatGPT
Thought 1
ChatGPT: This is an image.
Action 1
Assistant, what objects do you see in 
this image? <ImagePath>
Invoke vision experts, including 
image captioning, dense captioning, 
object tagger, face detection.
Observation 1
Image Captioning
Assistant: This is an image (444 x 444) 
with description a cartoon of a man 
and a woman.
Dense Captioning
List of object descriptions, and their 
locations in this image:
a cartoon of a woman with a green 
face (x1,y1,x2,y2) = (11, 152, 108, 344)
a cartoon of a woman with a green 
face (x1,y1,x2,y2) = (230, 155, 328, 
349)
…
Object Tagger
List of object tags seen in this image:
text, cartoon, animated cartoon, 
comics, human face, fiction, 
illustration, clipart, comic, funny
Thought 2
Action 2
Observation 2
ChatGPT: Yes, there is text in the 
image.
Assistant, what is the OCR texts  in 
this image? <ImagePath>
Invoke OCR
OCR
This image contains handwritten text
List of texts (words) seen in this 
image:
BEFORE …
I don’t feel good,
I need to buy
the test
NOW …
I don’t feel good,
I need to buy
the test
@INNA_S_ART
Are you pregnant?
Do you have the
coronavirus ?
ChatGPT: The text in the image is a conversation 
between two people. One person says they don’t 
feel good and need to buy a test. The other 
person asks if the first person is pregnant or has 
the coronavirus. The text is likely part of a comic 
or cartoon illustration. The image is tagged with 
“@INNA_S_ART”, which may indicate the artist or 
creator of the image.
Why is it funny?
ChatGPT: The humor in the comic may come 
from the situation, the characters, or the 
dialogue. The person is feeling unwell and is 
asked if they are pregnant or have the 
coronavirus, which is a current and relevant 
topic. The humor may come from the absurdity 
of the situation or the unexpectedness of the 
question. However, humor is subjective and what 
one person finds funny, another person may not. 
It is best to view the comic and decide for 
yourself if it is funny.
ChatGPT: This image is likely a comic or 
cartoon illustration.
Is there text in the image? If so, can you read 
it and explain it to me?
Figure 18. Unfolded multimodal reasoning and action steps for example of visual-conditioned joke/meme in Figure 5.
23

Convert image to <ImagePath> and 
send to ChatGPT
Thought 1
ChatGPT: This is an image.
Action 1
Assistant, what objects do you see in 
this image? <ImagePath>
Invoke vision experts, including image 
captioning, dense captioning, object 
tagger.
Observation 1
Image Captioning
This is an image (410 x 593) with 
description a graph of a number of cars.
Dense Captioning
List of object descriptions, and their 
locations in this image:
a purple rectangular object with white 
border (x1,y1,x2,y2) = (152, 169, 245, 
446)
a purple rectangular bar chart 
(x1,y1,x2,y2) = (25, 135, 365, 456)
…
Object Tagger
List of object tags seen in this image:
text, screenshot, purple, violet, graphic 
design, … 
Thought 2
Action 2
Observation 2
ChatGPT: This image is likely a bar 
chart or graph. 
Assistant, what is the OCR texts  in 
this image? <ImagePath>
Invoke OCR
OCR
This image contains text
List of texts (words) seen in this 
image:
$130.96
Ford
F150
(Regular Cab)
$82.56
Toyota
RAV4
$70.55
Honda
Civic
Total fueling cost
ChatGPT: This image is a bar chart of the total 
fueling cost of different cars. The cars are Ford 
F150 (Regular Cab), Toyota RAV4, and Honda Civic. 
The total fueling cost of Ford F150 (Regular Cab) is 
$130.96, the total fueling cost of Toyota RAV4 is 
$82.56, and the total fueling cost of Honda Civic is 
$70.55.
Excluding the regular cab, what is the 
average total fueling cost?
ChatGPT: The average total fueling cost excluding 
the Ford F150 (Regular Cab) is $76.55. This is 
calculated by taking the sum of the total fueling 
cost of Toyota RAV4 and Honda Civic and dividing 
it by 2. ($82.56 + $70.55) / 2 = $76.55.
Thought 3
Action 3
Observation 3
ChatGPT: This image is a chart of the total 
charging cost of different electric vehicles. The 
electric vehicles are Ford F150 Lightning 
(Estimate), Rivian R1T/S, and Tesla Model 3 
(Standard Range). The total charging cost of Ford 
F150 Lightning (Estimate) is $22.76, the total 
charging cost of Rivian R1T/S is $31.35, and the 
total charging cost of Tesla Model 3 (Standard 
Range) is $12.54.
ChatGPT: This is an image.
Assistant, what objects do you see 
in this image? <ImagePath2>
Invoke vision experts, including 
image captioning, dense 
captioning, object tagger.
Image Captioning
This is an image (400 x 297) with 
description a chart of cost comparison.
Dense Captioning
List of object descriptions, and their 
locations in this image:
a yellow rectangle with black text 
(x1,y1,x2,y2) = (154, 12, 245, 150)
a chart of cost per charge (x1,y1,x2,y2) = 
(1, 1, 391, 288)
…
Object Tagger
List of object tags seen in this image:
text, screenshot, line, yellow, number, 
… 
Thought 4
Action 4
Observation 4
ChatGPT: This image is likely a 
chart of cost comparison.
Assistant, what is the OCR texts  in 
this image? <ImagePath>
Invoke OCR
OCR
This image contains text
List of texts (words) seen in this image:
$22.76
Ford F150
Lightning
(Estimate)
$31.35
Rivian
R1T/S
$12.54
Tesla
Model 3
(Standard Range)
Total charging cost
Excluding Ford F150 Lightning, what is the 
average total charging cost?
ChatGPT: The average total charging cost 
excluding the Ford F150 Lightning (Estimate) is 
$22.44. This is calculated by taking the sum of 
the total charging cost of Rivian R1T/S and Tesla 
Model 3 (Standard Range) and dividing it by 2. 
($31.35 + $12.54) / 2 = $22.44.
Now how much cheaper is the average 
charging cost over the electric cars (including 
Ford F150 Lightning) than the average fueling 
cost over the gasoline cars (including Ford 
F150 (Regular Cab))? Let’s think step-by-step.
ChatGPT: The average charging cost over the 
three electric cars (including Ford F150 Lightning) 
is $18.66, which is calculated by taking the sum 
of the total charging cost of Ford F150 Lightning 
(Estimate), Rivian R1T/S, and Tesla Model 3 
(Standard Range) and dividing it by 3. ($22.76 + 
$31.35 + $12.54) / 3 = $18.66.
The average fueling cost over the three gasoline 
cars (including Ford F150 (Regular Cab)) is 
$101.07, which is calculated by taking the sum of 
the total fueling cost of Ford F150 (Regular Cab), 
Toyota RAV4, and Honda Civic and dividing it by 
3. ($130.96 + $82.56 + $70.55) / 3 = $101.07.
Therefore, the average charging cost is $82.41 
cheaper than the average fueling cost. This is 
calculated by subtracting the average charging 
cost from the average fueling cost. $101.07 - 
$18.66 = $82.41.
Convert image to <ImagePath> and 
send to ChatGPT
Figure 19. Unfolded multimodal reasoning and action steps for multi-hop document understanding (bar charts) in Figure 8.
24

Convert image to <ImagePath> and 
send to ChatGPT
Thought 1
ChatGPT: This is an image.
Action 1
Assistant, what objects do you see in 
this image? <ImagePath>
Invoke vision experts, including image 
captioning, dense captioning, object 
tagger.
Observation 1
Image Captioning
This is an image (682 x 1080) with 
description a diagram of a computer 
program.
Dense Captioning
List of object descriptions, and their 
locations in this image:
a white sign with green text 
(x1,y1,x2,y2) = (104, 641, 329, 748)
a green text on a white background 
(x1,y1,x2,y2) = (115, 948, 314, 1052)
…
Object Tagger
List of object tags seen in this image:
text, diagram, sketch, font, design
Thought 2
Action 2
Observation 2
ChatGPT: Let me ask for more 
information about the text.
Assistant, what is the OCR texts  in 
this image? <ImagePath>
Invoke OCR
OCR
This image contains text
List of texts (words) seen in this 
image:
Start
Input
num1
Input
num2
num1>num2
False
Display
num2
Stop
True
Display
num 1
ChatGPT: The flowchart appears to be a simple 
program that takes two inputs, num1 and num2, 
and compares them. If num1 is greater than 
num2, it displays num1. If num2 is greater than 
num1, it displays num2. The program starts with 
the “Start” label and ends with the “Stop” label.
Can you transform this flowchart into a 
simple program in python?
ChatGPT: Sure, I can try to write a simple 
program in python based on the flowchart. 
ChatGPT: This image is a flowchart diagram of a 
computer program. The text in the image is likely 
the labels for the different parts of the flowchart.
Can you read the text in this flowchart and 
explain it to me?
Figure 20. Unfolded multimodal reasoning and action steps for document understanding (ﬂowchart) in Figure 10.
25

…
…
…
…
Figure 21. Unfolded multimodal reasoning and action steps for example of multi-image relationships in Figure15.
26

Figure 22. Unfolded multimodal reasoning and action steps for example of scene text reasoning in Figure 16.
27

User  Answer question I.1.a. Think step-by-step.
GPT-4  To answer question I.1.a, we need to express the 
temperature T(x) in every point of the conductive bar, which is 
characterized by the abscissa x. We are given that T(0) = T0 and 
T(L) = Tb.
For a stationary regime, the heat equation for the 
one-dimensional heat conduction in the bar is:
d²T/dx² = 0
Integrating once, we get:
dT/dx = C₁
Integrating a second time:
T(x) = C₁x + C₂
To determine the constants C₁ and C₂, we use the boundary 
conditions:
T(0) = T0, which gives C₂ = T0
T(L) = Tb, which gives C₁ = (Tb - T0) / L
Finally, the expression for T(x) is:
T(x) = (Tb - T0) * (x / L) + T0
This represents a linear temperature profile along the conductive 
bar, with a slope of (Tb - T0) / L.
GPT-4 (Multimodal)
Ours w/ ChatGPT
To answer question I.1.a, one would need to use 
the principles of thermodynamics and heat 
transfer to determine the temperature at any 
point in the conductor bar. This would likely 
involve solving a differential equation to find the 
temperature distribution along the bar. The 
solution would depend on the specific properties 
of the bar, such as its length, cross-sectional area, 
and thermal conductivity, as well as the 
temperature of the bolometer and the 
temperature of the thermostat. The solution 
would also depend on the power of the incident 
radiation and the current of polarization. Once the 
temperature distribution is found, the 
temperature at any point in the bar can be 
determined by evaluating the solution at that 
point.
Answer question I.1.a. Think step-by-step.
Ours w/ GPT-4 (Language-only)
To answer question I.1.a, we'll consider the heat 
conduction equation along the thermal conductor bar. 
We're working under the assumptions of stationary 
mode and one-dimensional heat conduction with no 
lateral heat exchanges.
Start with Fourier's Law of heat conduction:
q = -k * (dT/dx)
where q is the heat flow (W/m²), k is the thermal 
conductivity (W/m·K), and dT/dx is the temperature 
gradient along the x-axis.
In a stationary regime, the heat flow q is constant 
throughout the bar. Thus, we have:
-k * (dT/dx) = constant
Integrate the equation with respect to x:
∫(dT/dx) dx = ∫(constant/k) dx
After integration, we get:
T(x) = (constant/k) * x + C
where C is the integration constant.
Apply the boundary conditions to find the constants:
T(0) = To => C = To
T(L) = Tb => (constant/k) * L + To = Tb
Solve for the constant:
constant = k * (Tb - To) / L
Substitute the constant back into the T(x) equation:
T(x) = (k * (Tb - To) / L) * x / k + To
T(x) = ((Tb - To) / L) * x + To
So, the temperature T(x) at any point x along the 
thermal conductor bar is given by:
T(x) = ((Tb - To) / L) * x + To
Figure 23. Case studies of MM-REACT’s extensibility. With the new release of GPT-4 (Language-only), we can upgrade ChatGPT in
MM-REACT to GPT-4 (Langauge-only) and further improve the results of MM-REACT. In Figure 26, we provide the unfolded multimodal
reasoning and action steps based on Ours w/ ChatGPT.
28

GPT-4 (Multimodal)
Ours w/ ChatGPT
Ours w/ GPT-4 (Language-only)
Can you write a template for this website in HTML? Replace the joke and punchline with a real one.
Figure 24. Case studies of MM-REACT’s extensibility. With the new release of GPT-4 (Language-only), we can upgrade ChatGPT in
MM-REACT to GPT-4 (Langauge-only) and further improve the results of MM-REACT.
29

Figure 25. Case studies of MM-REACT’s extensibility. MM-REACT is designed with the ﬂexibility to plug in new experts. In this
example, MM-REACT is enhanced with the image editing model from X-decoder [38] and can perform dialogue-based image editing.
30

…
Figure 26. Unfolded multimodal reasoning and action steps with ChatGPT to tackle physics problem in Figure 23.
31

