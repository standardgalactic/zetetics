Bayesian Vector Autoregressions and its Applications in
Macroeconomics
By
Hengwei Qi
Submitted to the Department of Economics and the
Graduate Faculty of the University of Kansas
in partial fulﬁllment of the requirements for the degree of
Doctor of Philosophy
Committee members
Professor Shu Wu, Chairperson
Professor Terry Soo
Professor Ted Juhl
Professor Jianbo Zhang
Professor Pym Manopimoke
Date defended:
May 13, 2015

The Dissertation Committee for Hengwei Qi certiﬁes
that this is the approved version of the following dissertation :
Bayesian Vector Autoregressions and its Applications in Macroeconomics
Professor Shu Wu, Chairperson
Date approved:
May 13, 2015
ii

Abstract
This dissertation presents three essays which are organized as chapters. Each chapter,
with its own feature, is focusing on parsimonious estimation of Vector Autoregressive
(VAR) models. The newly presented models are applied to some issues in macroeco-
nomics.
VARs are widely used in economic analysis and forecasting probably due to its easy
tractability and the power in exposing and exploring complicated dynamic process in
a modern economy such as shocks, channels and their links. Researchers sometimes
would like to include more observations in VARs in order to have a broad investiga-
tion into the economy. In addition, this analysis, in particular, currently has extended
to time variation in parameters so as to capture the possibly changing dynamics. These
two will inevitably cause parameter proliferation. The over-ﬁtting caused by hoping
including more variables and extending to time variation in parameters and wide ap-
plications because of tractability and detecting power motivate my research interest
in parsimonious modeling, estimation and applications as well as comparisons in this
dissertation.
In the ﬁrst chapter, I extend the previous parsimonious estimation on constant coefﬁ-
cients to time varying coefﬁcients. It is desirable since when time dimension is taken
into account, the number of parameters rises with time periods while number of obser-
vations still ﬁxed there, the same as in constant parameter estimation. I use stochastic
variable selection method over the time dimension, that is, the time varying dynamics
of each coefﬁcient in a TVP-VAR model is checked. I estimate the model via Bayesian
iii

method based on the sensitivity of variation of conditional likelihood when the coef-
ﬁcient considered in the model or not. If the coefﬁcient has more contribution to the
likelihood, the posterior tends to give more probability of staying in the system. Nev-
ertheless, if the parsimonious check is imposed on every coefﬁcient in the model, the
computational burden will substantially increase because this estimation is essential
a mixed model estimation such that every possible model needs to be checked. The
number of candidate models increases with time dimension. I suggest two ways to al-
leviate it. One is from model setting that we can check the variables we are interested
in collected together as a whole. In other words, we can do block checking rather than
single checking. The other is from the efﬁciency of the numerical computation which
is conducted in two dimensions. We construct large matrices to replace Kalman for-
ward ﬁlter and backward smoother in order to reduce the procedures in each iteration
in Bayesian simulation. On the other hand, the structure of the large matrix is sparse
which further make the computation efﬁcient. The single-checking-based TVP–VAR
with stochastic volatility is used to estimate the changes in monetary policy stance
and agents’ behavior to policy shocks over time. With the most parsimonious estima-
tion, I still cannot ﬁnd signiﬁcant changes both in policy stance and in the reaction of
economic agents to the non-systematic monetary policy shocks.
In the second chapter, I present a general parsimonious estimation on the time vary-
ing parameter VAR with stochastic volatility via factor idea. That is, the far lags
are driven by recent lags; the time variation in coefﬁcients on regressors is driven by
several factors and therefore the covariance matrix of innovations to the coefﬁcients
become reduced rank. Lastly, we use a latent factor, namely, the common volatility to
represent full stochastic volatility based on the empirical evidence that the estimated
volatilities of most macroeconomic variables share the similar pattern. Note that the
model I present concentrates on how to reduce the dimension of the parameters, not the
dimension of large data set such as factor models like dynamic factor models or factor
iv

augmented models. The model is estimated by Bayesian simulation. Each estimation
procedure or block is presented in this chapter. Based on the general treatment, the es-
timation procedures can be freely combined with some proper modiﬁcation depending
on the speciﬁc research object. I give an empirical analysis by the factor driven model.
The analysis is based on the typical small scale monetary VAR. Principal component
analysis shows that small scale TVP-VAR is still over-ﬁtting very much, can be driven
by several factors and that early lags are not suitable to drive far lags that will cause
dynamic contamination. Therefore parsimonious estimation via factors on time vary-
ing coefﬁcients and common volatility is used to estimate the small scale monetary
VAR. Focusing on agent response to monetary policy shocks, I cannot ﬁnd signiﬁcant
difference among different time periods.
In the last chapter, I consider a large Bayesian VAR that contains 28 variables. The
variables cover a broad range of the U.S economy including labor market, housing
market, bonds market and so on. High dimensional observations are desirable by re-
searchers because this setting can give a strong background of the whole economy, re-
ducing potential missing variables that are critical for the transmission of some shocks
of interest. A large number of endogenous variables will increase the degree of param-
eter proliferation. I use proper priors that can shrink values of coefﬁcients to conduct
an empirical analysis on monetary policy shocks, ﬁnancial shocks and uncertainty
shocks. I ﬁnd that for the effects of monetary policy shocks, the impulse response
functions are almost perfectly in line with theoretical predictions. For the ﬁnancial
shocks and uncertainty shocks, we analyze them jointly. Both positive ﬁnancial and
uncertainty shocks have negative effects on real activities, however, ﬁnancial shocks
have more persistent effects on these variables than uncertainty shocks. We also ﬁnd
that ﬁnancial variables care more for uncertainty shocks compared to ﬁnancial shocks.
v

Acknowledgements
I wish to express my deep and sincere gratitude to my advisor, Professor Shu Wu,
for his excellent guidance, care, patience, and for providing me with an excellent at-
mosphere for doing research. His invaluable suggestions and help hasten the cross of
Bayesian econometrics and macroeconomics in my dissertation.
I would like to thank Professor Terry Soo, in Department of Mathematics, for reading
this thesis word by word from grammar to contents and giving me lots of impor-
tant suggestions. Special thanks go to Professors Ted Juhl, Jianbo Zhang and Pym
Manopimoke for being in my dissertation committee.
Understanding the theory of Bayesian econometrics is one thing, putting it into prac-
tice is another thing. I would also like to thank Professor Dimitris Korobilis, at Univer-
sity of Glasgow, for his suggestion in email contact and code sharing and explanation
on his website. Without his code and suggestion, I cannot have a good and deep un-
derstanding of Bayesian theory and improve my code writing skill. I learn much from
his website.
Finally, I would like to express my deepest gratitude to all the people who have
helped me in my research and dissertation writing during the Ph.D life at University of
Kansas. Without them, I would never have been able to ﬁnish my dissertation. I owe
much to them.
vi

Contents
1
The Restricted Bayesian Vector Autoregressions and Monetary Policy Estimation
1
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Models in stochastic variable selection . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.1
Stochastic variable selection Bayesian VAR . . . . . . . . . . . . . . . . .
9
1.2.2
Stochastic variable selection partial TVP-VAR
. . . . . . . . . . . . . . .
11
1.2.3
Stochastic variable selection full TVP-VAR . . . . . . . . . . . . . . . . .
14
1.3
Bayesian estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.3.1
Stochastic volatility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.3.2
Stochastic selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.3.3
Exercise
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.4
Monetary policy in stochastic selection models
. . . . . . . . . . . . . . . . . . .
22
1.4.1
Data description
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.4.2
Lags and signiﬁcance . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
1.4.3
Impulse responses
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
1.4.4
Systematic or exogenous change . . . . . . . . . . . . . . . . . . . . . . .
32
1.4.4.1
Agents bahavior to monetary shocks
. . . . . . . . . . . . . . .
33
1.4.4.2
Systematic monetary policy . . . . . . . . . . . . . . . . . . . .
33
1.4.4.3
Non-systematic monetary policy shocks
. . . . . . . . . . . . .
42
1.4.4.4
An interpretation . . . . . . . . . . . . . . . . . . . . . . . . . .
42
1.5
Robust check
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
vii

1.6
Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
Appendix
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
A. The basic Kalman ﬁlter and state smoother . . . . . . . . . . . . . . . . . . . .
61
B. Modiﬁed Kalman ﬁlter and state smoother with breaks . . . . . . . . . . . . . .
63
C. Estimation of states without Kalman ﬁlter and state smoother . . . . . . . . . .
64
D. Efﬁcient estimation of SVS-Full-TVP-VAR
. . . . . . . . . . . . . . . . . . .
66
2
A General Parsimonious Estimation of Time-varying Vector Autoregressions
70
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.2
Model speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
2.3
Bayesian estimation procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
2.3.1
Draw a history of {bt}T
t=1
. . . . . . . . . . . . . . . . . . . . . . . . . .
80
2.3.1.1
Draw a history of
˜bt
	T
t=1 . . . . . . . . . . . . . . . . . . . . .
81
2.3.1.2
Draw Mbb0 as a whole . . . . . . . . . . . . . . . . . . . . . . .
82
2.3.1.3
Draw a history of {bt}T
t=1 . . . . . . . . . . . . . . . . . . . . .
84
2.3.2
Draw reduced rank covariance matrix Qb
. . . . . . . . . . . . . . . . . .
84
2.3.3
Draw constant matrix B
. . . . . . . . . . . . . . . . . . . . . . . . . . .
84
2.3.4
Draw constant matrix G1 . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
2.3.5
Draw structural impact matrix A . . . . . . . . . . . . . . . . . . . . . . .
87
2.3.6
Draw diagonal elements of S . . . . . . . . . . . . . . . . . . . . . . . . .
88
2.3.7
Draw common stochastic volatility {λt}T
t=1 . . . . . . . . . . . . . . . . .
88
2.3.8
Draw coefﬁcient F . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
2.3.9
Draw variance Qh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
2.4
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
2.4.1
Are factors important in time varying parameters?
. . . . . . . . . . . . .
94
2.4.2
Structural analysis and model evaluation . . . . . . . . . . . . . . . . . . . 102
2.5
Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Appendix
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
viii

A. Bayesian estimation of restricted linear regression model
. . . . . . . . . . . . 114
3
Structual Analysis in a Large Bayesian VAR
117
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
3.2
The literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
3.3
The model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
3.3.1
The likelihood of VAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
3.3.2
The priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
3.3.3
The posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
3.4
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
3.4.1
The data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.4.2
Empirical ﬁndings
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.5
Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Appendix
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
A. Data speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
ix

List of Figures
1.1
IRs for model 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
1.2
IRs for model 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
1.3
IR-inﬂation-comparison for model 3 . . . . . . . . . . . . . . . . . . . . . . . . .
36
1.4
IRs-unemployment rate-comparison for model 3 . . . . . . . . . . . . . . . . . . .
37
1.5
IRs-inﬂation-comparison for model 2
. . . . . . . . . . . . . . . . . . . . . . . .
38
1.6
IRs-unemployment rate-comparison for model 2 . . . . . . . . . . . . . . . . . . .
39
1.7
3D-IRs for model 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
1.8
3D-IRs for model 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
1.9
Policy response to inﬂation for model 3
. . . . . . . . . . . . . . . . . . . . . . .
43
1.10 Policy response to inﬂation for model 2
. . . . . . . . . . . . . . . . . . . . . . .
44
1.11 Policy response to unemployment rate for model 3
. . . . . . . . . . . . . . . . .
45
1.12 Policy response to unemployment rate for model 2
. . . . . . . . . . . . . . . . .
46
1.13 Volatility for each variable in model 3 . . . . . . . . . . . . . . . . . . . . . . . .
47
1.14 Posterior probability for each coefﬁcient in policy rule
. . . . . . . . . . . . . . .
49
1.15 Posterior probability of each coefﬁcient in inﬂation equation . . . . . . . . . . . .
50
1.16 Poterior probability of coefﬁcient in unemployment rate equation . . . . . . . . . .
51
1.17 Posterior probability of each regime . . . . . . . . . . . . . . . . . . . . . . . . .
54
1.18 IRs of inﬂation and their difference between different regimes
. . . . . . . . . . .
56
1.19 IRs of unemployment rate and their difference between different regimes
. . . . .
57
1.20 IRs of interest rate to inﬂation shocks
. . . . . . . . . . . . . . . . . . . . . . . .
58
x

1.21 IRs of interest rate to unemployment shocks . . . . . . . . . . . . . . . . . . . . .
59
2.1
Contribution and cumulative contribution for lag 1 . . . . . . . . . . . . . . . . . .
96
2.2
Contribution and cumulative contribution for lag 2 . . . . . . . . . . . . . . . . . .
97
2.3
Contribution and cumulative contribution for lag 3 . . . . . . . . . . . . . . . . . .
98
2.4
Contribution and cumulative contribution for lag 4 . . . . . . . . . . . . . . . . . .
99
2.5
TVP-VAR-CV-lag1-factor7-IR-inﬂation . . . . . . . . . . . . . . . . . . . . . . . 103
2.6
TVP-VAR-CV-lag1-factor7-IR-unemployment rate . . . . . . . . . . . . . . . . . 104
2.7
TVP-VAR-CV-lag2-factor3-IR-inﬂation . . . . . . . . . . . . . . . . . . . . . . . 105
2.8
TVP-VAR-CV-lag2-factor3-IR-unemployment rate . . . . . . . . . . . . . . . . . 106
2.9
TVP-VAR-CV-lag3-factor3-IR-inﬂation . . . . . . . . . . . . . . . . . . . . . . . 108
2.10 TVP-VAR-CV-lag3-factor3-IR-unemployment rate . . . . . . . . . . . . . . . . . 109
2.11 TVP-VAR-CV-lag4-factor3-IR-inﬂation . . . . . . . . . . . . . . . . . . . . . . . 110
2.12 TVP-VAR-CV-lag4-factor3-IR-unemployment rate . . . . . . . . . . . . . . . . . 111
2.13 Common volatility in different model settings . . . . . . . . . . . . . . . . . . . . 112
3.1
IRs to monetary policy shocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
3.2
IRs to uncertainty shocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
3.3
IRs to ﬁnancial shocks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
xi

List of Tables
1.1
Exercise result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.2
Lag check1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
1.3
Lag check2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.4
Lag check3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.5
Lag check4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.6
Lag check5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.7
Lag check6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.8
Lag check7
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.9
Lag check8
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.10 Lag check9
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.11 Lags in model 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.1
Contribution for each factor via principal component analysis on covariance matrix
Qb for TVP-VAR-CV from lag one to lag two . . . . . . . . . . . . . . . . . . . .
95
2.2
Contribution for each factor via principal component analysis on covariance matrix
Qb for TVP-VAR-CV from lag three to lag four . . . . . . . . . . . . . . . . . . .
95
3.1
Data speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
xii

Chapter 1
The Restricted Bayesian Vector
Autoregressions and Monetary Policy
Estimation
1

1.1
Introduction
Many economists have found that there is strong evidence that the U.S. economy experienced
higher and more volatile unemployment and inﬂation in the last century 70’s and the early 80’s
than the following years. It’s natural to relate the U.S. monetary policy to the bad performance
and ask the question of whether high inﬂation and slow growth during that period were due to bad
policy or bad luck. The literature generally gives two main explanations. One (e.g. Blanchard
and Simon, 2001; Koop, Leon-Gonzalez and Strachan, 2009; Primiceri, 2005; Sims and Zha,
2006; Stock and Watson, 2002a ) focuses on exogenous shocks, which have been much more
volatile in the 1970’s and early 1980’s than in the rest of the sample. They argued that the variance
of the exogenous shocks has changed over time and that this alone may explain many apparent
changes in monetary policy. This is ‘bad luck’ story, i.e. in the 1970s volatility was high, whereas
later policymakers had the good fortune of the Great Moderation of the business cycle. Koop,
Leon-Gonzalez and Strachan (2009) and Primiceri (2005) using the method of time-varying vector
autoregression, they still ﬁnd exogenous shocks play an overwhelming role. Therefore, I put them
in this respect. The second emphasizes the changes in the transmission mechanism—the way
macroeconomic variables respond to shocks. Particular attention gives to monetary policy reaction
function. Some authors (e.g. Boivin and Giannoni, 2006, Cogley and Sargent, 2001, and Lubik
and Schorfheide, 2004)) have argued that the way the Fed reacted to inﬂation has changed over
time. They think that the Fed was less active in ﬁghting inﬂation pressures under the chairmanship
of Burns than under Volker and Greenspan. This is ‘bad policy’ story.
However, these explanations are controversial. For example, Bernanke and Mihov (1998),
Hanson (2003), Leeper and Zha (2003) found that litter evidence of changes in the systematic part
of monetary policy and Sims (1999, 2001) found no evidence of unidirectional drifts in policy
toward a more active behavior.
From the methodological perspective, many versions of Bayesian Vector Autoregressions (BVAR)
in this ﬁeld are well developed. Here, I just pick up some representatives. Sims and Zha (2006)
used Bayesian VAR with multivariate stochastic volatility but no time-varying parameters. Cogley
2

and Sargent (2001) developed a time-varying parameters VAR model without stochastic volatil-
ity. Later they (2005) further extend a Bayesian VAR model with time-varying parameters and
multivariate stochastic volatility. The volatility however is too restrictive. This model allows the
simultaneous relations among variables are time invariant i.e. covariance is constant and vari-
ance is dynamic. In impulse response analysis, it can be shown that this restriction implies that
a shock to the ith variable has an effect on the jth variable which is constant over time. In some
macroeconomic applications, relationships might not be the case. Boivin (2001) considered time
varying simultaneous relations, but ignored potential heteroscedasticity of innovations. Ciccarelli
and Rebucci (2003) extend a t-distribution of errors based on the Boivin (2001) model; Finally,
Primiceri (2005) got the time varying parameters and loose multivariate volatility together. Loose
multivariate volatility means no restriction imposed on the covariance and variance parameters in
error matrix, totally time-varying now. This ﬂexible approach can be regarded as today’s standard
time varying VAR workhorse and is widely used in empirical macroeconomics.
It seems surprising, however natural that all models above and further modiﬁcations are all
Bayesian. First, if the model is a time invariant parameter VAR, in terms of classical estimation,
for instance, a VAR involving 5 dependent variables with 4 lags contains 105 parameters. Gen-
erally, in a quarterly macroeconomic data set, the number of observations on each variable might
be a few hundred and for some reason, usually a segment, not all range is picked up for a speciﬁc
analysis. That implies that maybe a few data is shared for each parameter estimation—the informa-
tion density is too low. If the model is extended to a time variant one, apparently more parameters
will be created. Think how many observations of data we should have to make the estimation and
further features of interest like forecasts and impulse responses precise. It’s impossible! Second, if
the variance of the time varying coefﬁcients is small, the classical maximum likelihood estimator
of this variance has a point mass at zero. This is so called pile-up problem (Sargent and Bhargava,
1983; Shephard and Harvey, 1990; Stock and Watson, 1998). The third, the maximum likelihood
method often meets optimization problem when model dimensionality is multiple and no longer
linear. Such a complicated model will highly probably have a multivariate likelihood function of
3

regions containing many local peaks and it’s difﬁcult to distinguish which one is global. More-
over, if these peaks are very narrow, the likelihood may reach particularly high values, not at all
representative of the model’s ﬁt on a wider and more interesting parameter region. Altogether, rich
parameters and potential problems in maximum likelihood with limited observations of data sets
make the classical estimation not suitable for some researches. A Bayesian method is naturally
introduced. In a Bayesian setting, not as many observations are needed, of course the more, the
better; Given a uninformative prior together with likelihood function, posterior mean and variance
of parameters and features of interest can be obtained from convergent therefore stable distribu-
tions by using Markov chain Monte Carlo (MCMC) method. The Gibbs sampler is a variant of
MCMC and widely used in these models. It consists of drawing from lower dimensional condi-
tional posteriors as opposed to the high dimensional joint posterior of the whole parameter set.
Bayesian vector autoregressions with Markov Chain Monte Carlo are a good and suitable ap-
proach to estimate parameters, especially in the form of time variation. However, implicitly it is
assumed that all parameters of every draw from some stable posterior distributions are efﬁcient
and should exist in the model. Simply speaking, all parameters in this Bayesian model are given
signiﬁcant before you start estimate, i.e., every one is counted in. In reality, it’s not the case. Let’s
compare it with classical vector autoregressions ﬁrst. In the context of classical estimation, the
usual way to test a coefﬁcient in a model whether or not signiﬁcant relative to zero is t-test or
F-test. We get the p-value of the coefﬁcient and then compare it with signiﬁcant level in some
distribution to decide reject or accept the null hypothesis. Anyhow, a speciﬁc way of testing pa-
rameter signiﬁcance is already at hand in the ﬁeld of classical estimation. Nevertheless, when
the same concern comes to Bayesian VAR and determine which coefﬁcient is signiﬁcant by some
methodology, how to deal with this problem? I am very interested in it.
To my best knowledge, I only ﬁnd two articles about this issue in Bayesian ﬁeld. One is Kuo
and Mallick (1997), the other is Korobilis (2013b). The former presented the coefﬁcients to be
precisely zero if the indicator of the corresponding coefﬁcient is zero (for instance, γ j is the corre-
sponding indicator for β j, when βj = 0 if γ j = 0); The latter has extended the use of such methods
4

to VARs for forecasting considering possible economic structure change and policy regime switch
that has been well documented in related literature, especially in the ﬁeld of monetary policy and
business cycles. A Markov chain Monte Carlo method (I will give details later) is created to obtain
the posterior probability of each indicator after many times of iterations, therefore the posterior
probability of parameter corresponding to this indicator. A criteria probability is set to compare
with the probability of each indicator, namely, corresponding parameter. If the posterior prob-
ability of the parameter is greater than the criteria probability, then the parameter is said to be
signiﬁcant with high conﬁdence and should stay in the Bayesian model. It looks like a test as in
classical estimation.
This restricted vector autoregression method chosen by indicators has three advantages. First,
variable selection is automatic, meaning that along with estimates of parameters we get associ-
ated probabilities of inclusion of each parameter in the ‘best model’. For instance, in a VAR of
5 dependent variables with 4 lags I used before, the indicators tells us which elements of coefﬁ-
cient matrices should be included or excluded from the ﬁnal optimal model, thus implementing
a selection among all possible VAR model combinations, without the need to estimate each and
everyone of these models. Second, this form of Bayesian variable selection is independent of the
prior assumptions about parameter matrices. That is, if the researcher had deﬁned any desirable
prior for her parameters of the unrestricted model, adopting the variable selection restriction needs
no other modiﬁcation than one extra block in the posterior sampler that draws from the conditional
posterior of the indicators. Finally, unlike other proposed stochastic search variable selection al-
gorithms for VAR models (George et al., 2008), this form of variable selection may be adopted in
many nonlinear extensions of VAR models.
The contribution of this paper is threefold. One is on methodology. In Korobilis (2013b), he
extends stochastic variable selection method to constant parameter VAR and limited time-varying
parameter VAR without stochastic volatility, respectively. This type of time variation in the context
of TVP-VAR framework in Korobilis (2013b) only refer to one of coefﬁcients either keep existing
and time varying over time or not. Here, based on his model, I extend the potential time variation
5

to each and every coefﬁcient, namely, it allows each and every coefﬁcient of TVP-VAR to shift
over time with corresponding posterior probability of present or not, but not necessarily the next
time. Whether or not the coefﬁcient is time varying is totally determined by data, period by pe-
riod. Furthermore, I could also add loose stochastic volatility, i.e. both variance and covariance
parameters of innovation matrix are time-variant – some econometricians call this volatility model
heteroskedastic TVP-VAR – with several extra blocks embedded in original iteration route. One
thing worth mention is that since the method looks into determining whether every and each co-
efﬁcient in the TVP-VAR exist or not over time, the computational burden is high and thus time
consuming. To increase computational efﬁciency, we use the precision based algorithm of Chan
and Jeliazkov (2009) to replace typical procedure of Kalman ﬁlter and Smoother (forward ﬁltering
and backward smoothing) for an estimation of linear and gaussian state-space models. The second
is that perhaps the ﬁrst one implements the stochastic variable selection time-varying parameters
autoregression with stochastic volatility on monetary policy analysis. We ﬁnd that, more precisely
than extant literature, there are no signiﬁcant systematic shifts in monetary stance and economic
agents’ behavior due to exogenous monetary policy shock. The last but not the least is that we give
an Bayesian statistic proof of the stability of small monetary VAR system. This is done by two
steps. we ﬁrst check variable signiﬁcance over time due to proliferation of parameters in VAR sys-
tem under limited data availability; second, we only focus on large breaks because generally, only
the large ones have high potential to be candidates of structural change.1 Since typical TVP-VAR
is a type of model whose latent parameters vary every time, the accounting of small breaks may
take over some strength from original true large breaks, which may cause misleading. We solve
this problem with a modiﬁed TVP-VAR augmented with a discrete Markov process.
The paper is organized as follows. In the second section, we present three stochastic variable
selection models step by step from static to time variation and from without stochastic volatility
to with it. We give basic Bayesian estimation procedure for each model and more importantly,
also explore their internal relations among these models; The third section discusses estimation
1Note that breaks are not necessarily equivalent to systematic changes. Systematic changes imply some ‘deep
parameters’ have changed in DSGE models.
6

issues on stochastic volatility and stochastic variable selection as well as efﬁcient implementation
of computation; In section 4, we use these models to estimate and evaluate whether there were
regime switches in monetary policy and structural changes in the U.S. economy, then give our
ﬁndings and interpretation; Section 5 conducts robust check and the last section 6 concludes.
1.2
Models in stochastic variable selection
This section gives three models that are mutually relevant and become ﬂedged step by step, mod-
eling gradually close to economic reality. I present a basic description of each model. The ﬁrst is
stochastic variable selection time-invariant parameter model, namely stochastic variable selection
Bayesian VAR for which I use SVS-BVAR for short. The second is stochastic variable selection
partial time-varying parameter model, in which setup parameters either exist all time or not. I
use SVS-Partial-TVP-VAR for short for this model. The last, newly developed in this chapter,
is stochastic variable selection full time-varying parameter model where parameters either exist
or not period by period. I call it SVS-Full-TVP-VAR. The three models can be imbeded with a
stochastic volatility part, taking into account dynamics of exogenous shocks.
At the starting point, I should clarify some notations associated with these models.2 A usual
form of VAR should be transformed into a reduced form for convenient analysis. The VAR(p) of
p−lag and M dimensionality with constant parameters can be written as:
yt = c+A1yt−1 +···+Apyt−p +εt
(1.1)
where yt is an M ×1 vector, c is an M ×1 constant, {Ai}p
i=1 are M ×M matrices, and εt is an M ×1
vector of shocks following normal distribution εt ∼N(0,Σ) , for t = 1,...,T. The (1.1) can be
transformed into a compact form
yt = Ztβ +εt
(1.2)
2These notations apply to all the three chapters.
7

where Zt = IM [1,y
′
t−1,...y
′
t−p] and β = vec([c,A1,...Ap]
′), with  and vec Kronecker product
and column stacking operator, respectively.
Above is just simple expression for writing convenience. If in terms of estimation, i.e. nesting
all data together in big matrices, two forms are found. One is Canova (2007) and others; the other
is, for example, Kadiyala and Karlsson (1997). The former arises if we use an TM × 1 vector
y which stack all T obsevations on the ﬁrst dependent variables, then all T observations on the
second dependent variables, etc., i.e. y = [y1:T ′
1
,...,y1:T ′
M ]′ where y1:T
i
denote observations of yi in a
column from time 1 to time T. The latter arises if we deﬁne Y to be T ×M matrix which stacks the
T observations on each dependent variable in columns next to one other, i.e. Y =

y1:T
1 ,...,y1:T
M

.
ε and E denote stackings of the errors in a manner conformable for y and Y, respectively. Deﬁne
xt = [1,y
′
t−1,...,y
′
t−p]
′ and X = [xt,...,xT]
′. Note that if we let K = 1 + M · p be the number of
coefﬁcients in each equation of the VAR, the X is a T × K matrix. Finally, if A = [c,A1,...,Ap]
′,
we deﬁne β = vec(A) that is a K ·M ×1 vector which stacks all the VAR coefﬁcients including the
intercepts into a column vector. With all these deﬁnitions, we can write the VAR either as
Y = XA+E KK version
(1.3)
or
y = (IM X)β +ε Canova version
(1.4)
where y = vec(Y) from above deﬁnitions and ε ∼N(0,ΣIT).3
If parameters are time variant, the form of VAR accordingly becomes
yt = ct +A1,tyt−1 +···+Ap,tyt−p +ut
(1.5)
or compactly
yt = Ztβt +ut
3Equation (1.4) can be derived from equation (1.3) via vec(ABC) = (C′ A)vec(B).
8

where Zt = IM 

1,y′
t−1,...,y′
t−p

, βt = vec
 [ct,A1,t,...,Ap,t]′
and ut ∼i.i.dN(0,Σt) for t =
1,...,T. The stochastic volatility of Σt is introduced in the next subsection. Other details and
modiﬁcations will be given in speciﬁc models.
Below we get into the stochastic variable selection models with notations deﬁned above.
1.2.1
Stochastic variable selection Bayesian VAR
The VAR model in simple form of (1.2) can be written as
yt = Ztθ +εt
(1.6)
where θ = Γ β, Γ = diag(γ) = diag([γ1,...,γKM]
′
) and εt ∼i.i.dN(0,Σ). We denote γj the jth
element of the vector γ, which is also jth diagonal element of the matrix Γ, and γ−j the vector γ
with the jth element removed. Priors for parameters in the model are set as
β ∼NMK(β,V)
γ j|γ−j ∼Bernoulli(1,π0,j)
Σ ∼IWishart(S,ν)
where β, V, π0,j, ν and S are hyperparameters set by researchers.
Given the priors above, the full conditional posteriors are
1. Sample β from the posterior density
β|γ,β,Σ,y,Z ∼NMK
  ¯β, ¯V

9

where
¯V =
 
V −1+
T
∑
t=1
Z⋆′
t Σ−1Z⋆
t
!−1
¯β = ¯V
 
V −1β+
T
∑
t=1
Z⋆′
t Σ−1yt
!
and Z⋆
t = ZtΓ.
2. Sample γj from the posterior density
γ j|γ−j,β,Σ,y,Z ∼Bernoulli
 1, ¯π j

preferably in random order j, where ¯πj =
l0 j
l0 j+l1 j , with
l0j = p(y|θj,γ−j,γj = 1)π0j
and
l0j = p(y|θj,γ−j,γj = 0)(1−π0j)
The expression p(y|θ j,γ−j,γ j = 1) and p(y|θ j,γ−j,γj = 0) are conditional likelihood ex-
pressions. Here we deﬁne θ ⋆to be equal to θ but with the jth element θj = βj in the case
of γj = 1. Similarly, we deﬁne θ ⋆⋆to be equal to θ but with the jth element θ j = 0 when
γj = 0. Then in terms of the likelihood of simple form of VAR for each period, namely
equation (1.6), we can write l0j, l1j analytically as
l0j = exp(−1
2
T
∑
t=1
(yt −Ztθ ⋆)′Σ−1(yt −Ztθ ⋆))π0 j
(1.7)
l1j = exp(−1
2
T
∑
t=1
(yt −Ztθ ⋆⋆)′Σ−1(yt −Ztθ ⋆⋆))(1−π0 j)
(1.8)
10

3. Sample Σ from the posterior density
Σ|β,γ,y,Z ∼IWishart(¯ν, ¯S)
where
¯ν = T +ν
and
¯S = S+
T
∑
t=1
(yt −Ztθ)(yt −Ztθ)
′
4. Go back to setp 1 again, start the next iteration.
This model is fundamental to the following model extensions in three aspects. Firstly, the essen-
tial part that determines the posterior probability of presence for each coefﬁcient is the difference
between the two conditional likelihood l0j if the coefﬁcient exist and the one l1j if not. When
l0 j dominate l1j, the jth coefﬁcient shows up with high posterior possibility supported by data;
Secondly, do not forget the role of prior probability π0 j for each coefﬁcient βj in that prior also
give information to the posterior outcome. When it is assigned with very informative prior due to
some economic theory, say, a very low value for π0j, the information involved in the conditional
likelihood l0j, to some extent, will be weakened, while l1j will be strengthened; Lastly, the Markov
Chain Monte Carlo (MCMC) method based on this model is ﬂexble to accomodate other modiﬁ-
cation, such as time-varying parameters, stochastic volatility widely existed in the time series.
1.2.2
Stochastic variable selection partial TVP-VAR
In this model, compared with the model SVS-BVAR above, the difference is that coefﬁcients are
time variant of the type as in Korobilis (2013b). This time variation as mentioned previously for
some coefﬁcient either exists for the whole time period or not; that is each indicator refers to a
coefﬁcient from period one to the last period, which is illustrated in details below. The model
11

speciﬁcation is
yt = Ztθt +εt
(1.9)
βt = βt−1 +ut
(1.10)
where θt = Γβt, Γ = daig(γ) = diag([γ1,...,γKM]′), εt ∼N (0,Σ) and ut ∼N (0,Q) which are
uncorrelated with each other at all leads and lags. The priors for this models are
β0 ∼NMK

β,V

γj|γ−j ∼Bernoulli
 1, ¯π0j

Q ∼IW

ξ,R

Σ ∼IW (S,ν)
Estimating these parameters means sampling sequentially from the following conditonal densityies
1. Sample βt|βt−1,Q,Σ,yt,Z⋆
t for all t, where Z⋆
t = ZtΓ, using the Carter and Kohn (1994) ﬁlter
and smoother for state-space models. For details on this, please refer to appendix of Prim-
iceri (2005). This step, for computational efﬁciency, could be replaced by precision based
algorithm of Chan and Jeliazkov (2009), taking full advantage of sparse matrix computation
in commonly used maths and econometrics software.
2. Sample γj from the density
γ j|γ−j,β,Σ,y,Z ∼Bernoulli
 1, ¯π j

12

preferably in random order j, where ¯πj =
l0 j
l0 j+l1 j , with
l0j = p(y|θ 1:T
j
,γ−j,γ j = 1)π0 j
l0j = p(y|θ 1:T
j
,γ−j,γ j = 0)(1−π0j)
The expression p(y|θ 1:T
j
,γ−j,γj = 1) and p(y|θ 1:T
j
,γ−j,γj = 0) are conditional likelihoods,
where θ 1:T
j
=

θ1,j,...,θt,j,...,θT,j

. Deﬁne θ ⋆
t to be equal to θt but with the jth element
θt,j = βt,j when γ j = 1 for all t = 1,...,T. Similarly, we deﬁne θ ⋆⋆
t
to be equal to θt but with
jth element θt,j = 0, namely when γj = 0, for all t = 1,...,T. Then in the case of TVP-VAR
likelihood of the model, we can write l0j, l1j analytically as
l0j = exp(−1
2
T
∑
t=1
(yt −Ztθ ⋆
t )′Σ−1(yt −Ztθ ⋆
t ))π0 j
(1.11)
l1j = exp(−1
2
T
∑
t=1
(yt −Ztθ ⋆
t )′Σ−1(yt −Ztθ ⋆
t ))(1−π0j)
(1.12)
3. Sample Q from the posterior density
Q|β,γ,Σ,y,Z ∼IW
  ¯ξ, ¯R

where
¯ξ = T +ξ −1
and
R = ¯R+
T
∑
t=2
(βt −βt−1)(βt −βt−1)
′
4. Sample Σ from the posterior density
Σ|β,γ,y,Z ∼IWishart(¯ν, ¯S)
13

where
¯ν = T +ν
and
¯S = S+
T
∑
t=1
(yt −Ztθt)(yt −Ztθt)
′
5. Go back to step 1, start a new iteration.
One thing to note from step 2 above is that the indicator γ j associate with coefﬁcient βjt from
t = 1,...,T, i.e. Γ matrix is invariant over time. This model speciﬁcation extending from constant
coefﬁcients to time varying coefﬁcient may be good to forecast, but probably too restrictive for
modeling time series dynamics as some coefﬁcients may appear and disappear mutually during
whole time, not simply keeping existing or not. A nature idea is to assign time variant indicator
to each coefﬁcient in a TVP-VAR model, to capture possible distinct dynamic feature for each
coefﬁcient. We will refer to model 3 below.
1.2.3
Stochastic variable selection full TVP-VAR
This model is based on the stochastic variable selection partial time-varying parameter VAR (Ko-
robilis, 2013b), i.e. the second model above, and incorporates the multivariate stochastic volatility.
The speciﬁcation of the model has the same formula as equation (1.9) and equation (1.10) but with
time dimension θt = Γtβt, Γt = diag(γt) = diag

[γ1,t,...,γKM.t]
′
and εt ∼N (0,Σt). γj,t is indic-
tor for corresponding β j,t, either 0 or 1, meaning including and excluding the corresponding β j.t
respectively, for j = 1,...,KM and t = 1,...,T. γ j,t follows Benoulli distribution independently.
There are two differences in model 3, compared with model 2. The ﬁrst is the indicator for each
coefﬁcient. The indicator γ now associate with not only identity j, but also with time t. That is,
every coefﬁcient in TVP-VAR now has its own indicator, unlike that β 1:T
j
share the same indicator
γ j in model 2; The second, error variacne covariance matrix Σt, displays time variant, i.e. stochastic
volatility. The covariance matrix typically can be decompoed into the form of Σt = A−1
t
DtDtA−1′
t
,
14

where At is a lower triangular matrix with value of ones on the main diagonal
At =


1
0
···
0
a21,t
1
...
...
...
...
...
0
aM1,t
···
aM(M−1),t
1


and Dt is the diagnoal matrix with elements di,t = exp
 1
2hi,t

, for i = 1,...,M. Here, the exponen-
tialization makes values of diagonal elements always positive. Therefore Dt is the matrix
Dt =


d1,t
0
···
0
0
d2,t
···
0
...
...
...
...
0
···
0
dM,t


=


e
1
2h1,t
0
···
0
0
e
1
2h2,t
···
0
...
...
...
0
0
···
0
e
1
2hM,t


If we ﬁrst stack the unrestricted elements of At below main diagonal by rows into a M(M−1)
2
×1
vector as at =

a21,t,a31,t,a32,t,...,an(n−1)
′
for n = 2,...,M, and ht = [h1,t,...,hM,t]
′, then βt, at,
and ht follow independent random walks and the whole speciﬁcation of the model in state space
form is
yt = Ztθt +A−1
t
Dtυt
(1.13)
βt = βt−1 +ut
at = at−1 +ςt
ht = ht−1 +ηt
where υt ∼N (0,IM) and θt = Γtβt.
The random walk setting presents the advantages of focusing on permanent shifts and reducing
the number of parameters in the estimation procedure.
15

The innovations in the three state equations are


ut
ςt
ηt


∼i.i.dN


Q
0
0
0
S
0
0
0 W


where S can be block diagonal (Primiceri, 2005) or full matrix (De Wind and Gambetti, 2014).4
As for the sampling process for this model, details like Kalman ﬁlter and smoother are dele-
gated to the technical appendix. Without loss of generality, I give the general expression for this
model. It sufﬁces to note two points. First , l0j and l1j in the above two models now change to
l0 jt and l1jt due to that the indicators γ j,t have been assigned to every coefﬁcient in the TVP model
over j = 1,...,KM and t = 1,...,T. The conditional likelihood (approximate to) becomes:
l0js = π0js exp
(
−1
2
"
T
∑
t̸=s
(yt −Ztθt)′ Σ−1
t
(yt −Ztθt)+(ys −Zsθ ⋆
s )′ Σ−1
s
(ys −Zsθ ⋆
s )
#)
l1js =
 1−π0js

exp
(
−1
2
"
T
∑
t̸=s
(yt −Ztθt)′ Σ−1
t
(yt −Ztθt)+(ys −Zsθ ⋆⋆
s )′ Σ−1
s
(ys −Zsθ ⋆⋆
s )
#)
where θ ⋆
s is deﬁned to be equal to θs = Γsβs, but θjs is equal to βj,s, i.e. the corresponding indicator
is γj,s = 1 for s = 1,...,T and for j = 1,...,KM; Similarly, θ ⋆⋆
s
is when θ j,s is zero in the case
of γ j,s = 0 for s = 1,...,T and for j = 1,...,KM. Then the probability for indicator γj,s = 1 in
Bernoulli drawing is ¯πj,s =
l0 js
l0 js+l1 js . Note that the identity j and the time s are both randomly
picked up.5 Second, it is about the prior setting. Generally, there are two choices. Usually, the
priors on the time-varying parameters are:
β ∼N
 0,4Inβ

4De Wind and Gambetti (2014) prove that standard Kalman ﬁlter can still be used in the second state space model
for at instead of equation by equation causing block diagonal in S as in Primiceri (2005).
5An efﬁcient estimation is developed in appendix following Chan and Jeliazkov (2009).
16

l ∼N (0,4Inl)
h ∼N (0,4Inh)
The subscript presents dimensions of each parameter. The priors on their error covariacne are:
Q ∼IW

1+nβ,

(kQ)2  1+nβ

Inβ

S ∼IW

1+nl,

(kS)2 (1+nl)Inl

W ∼IW

1+nh,

(kW)2 (1+nh)Inh

where the hyperparamets are set to kQ = 0.01, kS = 0.1, kW = 0.01. One can also construct the
priors using a training sample (Primiceri, 2005). In particular, assume that ˆθols and V
  ˆθols

are the
mean and variance respectively of the OLS estimate of Θ = (β,l,h) based on a VAR with constant
parameters using an initial training sample.6 Then the priors can be written as
β ∼N (βols,4V (βols))
l ∼N (lols,4V (lols))
h ∼N (hols,4V (hols))
and errors of covariance matrices are
6The priors can also be constructed via Bayesian estimation with noninformative priors.
17

Q ∼IW

1+nβ,

(kQ)2  1+nβ

V (βols)

S ∼IW

1+nl,

(kS)2 (1+nl)V (lols)

W ∼IW

1+nh,

(kW)2 (1+nh)V (hols)

Other notations are the same as above.
One thing should be mentioned again is that the model 3 presented here is the general case.
This means that modiﬁcation or restriction can be applied to the general case. If the investigation
of parameter signiﬁcance via γ j,t in model 3 is restricted to γ j and without stochastic volatility, then
model 3 is reduced to model 2. If further shutting off time variations on coefﬁcients, it is model 1.
On the other hand, as we discussed stochastic search method is associated with mixture models,
the possible choices will become proliferative when the target model involves many coefﬁcients,
especially in TVP models (over identity and over time). This will give rise to great computational
burden. Therefore, in order to reduce the burden, an indicator that controls for a set of coefﬁcients
could be used instead of one for one. Actually, the model 2 has used the idea that each indicator
γ j corresponds to β j over the whole period, namely β 1:T
j
. I call it block checking. The property
of indicators over each time and each identity in model 3 makes the investigation of any single
coefﬁcient (theoretically) possible if pursuing the best model, but causes dramatically increased
computational cost in practice such that sometimes the estimation is infeasible. This is single
checking. Hence, efﬁcient estimation of stochastic variable selection model is required. I will
discuss it in the next section.
18

1.3
Bayesian estimation
In this section, we focus on two points that are important for estimation and numerical computation.
One is on stochastic volatility; The other is on stochastic variable selection.
1.3.1
Stochastic volatility
Stochastic volatility is a method modeling dynamic process for error term. Once stochastic volatil-
ity (SV afterwards) is used, generally, the econometric model involves a nonlinear or/and non-
gaussian state space part.
Jacquier, Polson and Rossi (1994) propose to, based on Carlin et
al.(1992), use Metropolis step via a single move to draw stochastic volatility; here, we follows
Primiceri (2005), using mixture normal distribution to approximate irregular distribution (speciﬁ-
cally, log
 x2 (1)

), suggested by Kim, Shephard and Chib (1998).
The method involves two steps. Firstly, we draw sT from f
 sT|yT,Ξ,hT
, where sT demon-
strate which normal is chosen among mixture normal distributions each period; yT is all the obser-
vation from y1 to yT, the same for log volatility states hT and Ξ represents other parameters and
hyperparameters. This conditional posterior is discrete distribution with seven points or ten points
as support, see Kim et al. (1998) for seven normal distributons and Omori et al. (2007) further for
ten, respectively; Secondly, draw hT from f
 hT|yT,Ξ,sT
given all s1 to sT from previous step.
This step is very important, as it transforms non-gaussian to a given gaussian based on identity
st each period. Hence, (nonlinear) non-gaussian state space model has been ‘cut’ into linear and
gaussian state space model each period and naturally, multi-move Gibbs sampling of Carter and
Kohn (1994) can be used, which makes chain mix well quickly and therefore reduce computational
time, as opposed to single move of Jacquier et al. (1994).7
Details on this SV approximation method can be found in appendix. Some issues such as why
draw identity sT as ﬁrst step and comparison with other methods dealing with SV, reader of interest
are referred to Del Negro and Primiceri (2013), who conﬁrm that performance of this method is
7In Chapter 2, the single move of Jacquier et al. (1994) is used for nonliner state space model.
19

quite good and quantitatively same as the results of methods exactly modeling SV.
1.3.2
Stochastic selection
In section 2, we have given the mechanism of stochastic selection (SS hereafter) in (TVP) VAR
models and sketch the steps how to draw posteriors from full conditional distributions. In this
subsection, we focus on implementation of this stochastic selection in computation, especially for
model 3.
As we discussed in model 3, in section 2, compared with model 1 and model 2, all models
share the essentially same rationale for SS, but computation burden dramatically increases from
model 1 to model 3. Take model 3 for example. After incorporating SV and indicator for each and
every coefﬁcient (over identity and over time) in the TVP-VAR model, the model becomes very
complicated though it allows for all possibilities. Considering a small TVP-VAR, typically with
three endogenous variables and only two lags – usually used in monetary VAR and requiring more
endogenous variables and lags such as oil analyses in Baumeiser and Peersman (2010) – and with
sample period usually covering around T = 150 quarterly data set, we need to compute 2∧(21∗150)
models only for stochastic selection part, not including SV, for just one iteration in the Bayesian
estimation circle.
To make the implementation of SS feasible, two ways can be used. One is block checking,
namely, the selection is based on a set of variables rather than single variable such as model 2.
Apparently, this method can reduce the number of candidate models, but with the concern of
missing potential better choices. Single checking is required if the best model is preferred in some
context. This suggests the second way – an efﬁcient computational method that is prepared for the
general single checking. We use the efﬁcient simulation method of Chan and Jeliazkov (2009).
The basic idea is that we nest potential iterations in each Bayesian circle into a quiet large and
sparse matrix to reduce number of iterations and therefore to speed up computation.
For example, we can construct large sparse matrices for a linear normal state space model with-
out using typical steps of Kalman forward and backward recursion and directly obtain a draw of
20

β 1:T = [β ′
1,...,β ′
t ,···β ′
T]′ at a time from a posterior conditional distribution. Thus it is the trans-
formation from conventional estimation of TVP-VAR to estimation of constant parameter VAR via
large and sparse matrices. On the other hand for SS portion, we also construct large sparse matrix
for calculating conditional likelihood for model selection instead of iterating over whole time pe-
riod. That is, corresponding to the draw of β 1:T, a large diagonal matrix Γ = Blkdiag

{Γt}T
t=1

is used where Γt = diag
 [γ1,t,...,γMK,t]′
. After this improvement of implementation in compu-
tation, we ﬁnd we can cut off two thirds of time originally used for not doing this due to taking the
advantage of matrix computation faster than iteration and large sparse matrix actually accounting
for small space to save. For details, please refer to appendix for this chapter.
1.3.3
Exercise
Why we need to use stochastic variable selection models to do empirical analysis? Make the
models parsimonious and close to reality. Here, I use an artiﬁcial data set to show this advantage
of this approach by a simple exercise. For simplicity, I generate a 5 variable VAR with only T = 50
time series observations, one lag and no constant. The unrestricted OLS estimates perform poorly,
while variable selection gives better estimates. This is because we restrict irrelevant variables, and
then there are more degrees of freedom to estimate the parameters on the relevant variables.
Suppose that the coefﬁcient matrix in the version of Kadiyala and Karlsson (1997) mentioned
above is an identity matrix and that error covariance matrix is randomly chosen only positive
deﬁnite for sure. Initial data y1 is from a standard uniform distribution. Let’s look at table 1.1.
The ﬁrst column contains posterior probatility in mean when γ j = 1, namely, the probability of
coefﬁcient β j in the VAR(1) model. Posterior mean of βj is in column 2. Column 3 contains
coefﬁcients estimated by ordinary least square method (OLS). The true parameters are in the last
column. The prior probability π0j for each parameter βj is set to 0.5. That’s because one has
no idea of including or excluding each parameter, say, perhaps in or out in the model half by half.
Hence, Korobilis (2013b) regards it as ‘noninformative prior’ and criteria probability. Column 3, in
contrast to last column – true values of coefﬁcients – demonstrates that the estimation performance
21

is bad and many inefﬁcient variables contained in. Column 1 gives posterior probability for each
coefﬁcient that match well with true value: when true value is 1, all posterior probability is 1;
while true value is 0, almost all the corresponding posterior probabilities are extremely low, seldom
above 5% contrast with 50% as criteria. The corresponding mean of each coefﬁcient in column 2,
compared with column 3, generally, is more close to true value because the indicators give more
information to coefﬁcients that exist. Obviously, the exercise shows great beneﬁt on Bayesian VAR
regression.
1.4
Monetary policy in stochastic selection models
1.4.1
Data description
I use a quaterly U.S. data set on the inﬂation rate, unemployment rate and the short interest rate.
Inﬂation is the annual percentage change in a chain-weighted GDP price index; unemployment
rate is seasonlly adjusted civilian unemployment, all workers over age 16; short rate is 3−month
Treasury bill rate. They are denoted by yt = (πt,ut,rt)
′collected in a vector. The sample runs
from 1953Q1 to 2006Q3. We choose this sample period due to two reasons. One is that this
time span, covering Great Inﬂation, Monetary Targeting, and Great Moderation, is widely used in
the monetary policy literature, especially for those focusing on issue of monetary policy regimes
switching during this possible policy change time period. Therefore it is nature for us to choose
the same span to compare our ﬁndings and results with previous ones. The other is that after recent
global economic and ﬁnancial crisis of 2008, though the Great Recession has gone but still in the
process of slow recovery, the Fed has switched to and continued near zero rate policy until full
recovery, which apparently an abrupt change in monetary instrument.
These three variables we choose in our analysis are commonly used in New Keynesian VAR
literature. They are simple but representative. Examples of papers which use these, or similar
variables, include Cogley and Sargent (2001, 2005), Koop, Leon-Gonzalez and Strachan (2009)
and Primiceri (2005).
22

Restrict
Bayesian
OLS
True
1
0.984388
0.668605
1
0.025
-0.02115
-0.26653
0
0.0268
0.017219
-0.3257
0
0.348
0.063925
-0.3257
0
0.0288
0.01453
-0.1153
0
0.0925
-0.27718
-0.40878
0
1
0.791886
0.671789
1
0.028
0.015616
0.024751
0
0.0412
-0.03159
-0.03933
0
0.0326
0.042137
-0.07599
0
0.0158
0.05577
0.069901
0
0.017
0.073861
0.117344
0
1
0.787047
0.753396
1
0.212
-0.00324
-0.0383
0
0.0166
-0.03478
0.116155
0
0.163
-0.00202
0.09937
0
0.0474
0.033059
-0.02569
0
0.058
-0.02369
-0.00919
0
1
0.8962
0.92836
1
0.019
0.091339
-0.11235
0
0.0564
0.098384
0.350941
0
0.027
-0.02018
0.24663
0
0.0336
-0.01635
0.340174
0
0.0176
0.049076
0.187969
0
1
0.856714
0.90082
1
Table 1.1: Exercise result
Notes: The ﬁrst column contains posterior probability in mean when γ j = 1, namely, the probability
of coefﬁcient βj in the VAR(1) model; The second column has posterior mean of βj; The third
column is OLS estimate of β j; The fourth column is for true value of β j that is used to construct
the artiﬁcial data.
23

1.4.2
Lags and signiﬁcance
In this subsection, I discuss this issue focusing on Model 1, namely static SVS-BVAR. Though its
simplicity, it can specify this issue very well and shed light on the other two models.
Due to the property of stochastic selection models we discussed in section 2, this class of
models itself is able to freely choose what coefﬁcients enter or exit the system. In this way, if
arbitrarily choose observables and lags, letting the data speak, in a Bayesian framework, one can
ﬁnally ﬁnd the best model associated with its coefﬁcient signiﬁcance and lags.
I ﬁrst use model 1 (SVS-BVAR) to estimate the full range of the data set. Inﬂation, unem-
ployment rate and 3-month treasury bill rate are typically used in small scale monetary VAR. Of
course, more variables such as real GDP growth, money base, exchange rate and so on can also
be nested into models and let the data decide if they are signiﬁcantly interact each other and over
time following the idea. Before this paper, to my best knowledge, Canova and Ganbetti (2009),
Cogley and Sargent (2001, 2005), Koop et al. (2009) and Primiceri (2005) all directly give two
lags without any check. The advantage of SVS model is that we can ﬁnd the signiﬁcant variables
meanwhile the best lag. I ﬁrst set the uninformative prior probability π0j = 0.5, meaning the fair
change of including or exluding the corresponding coefﬁcient. The possible number of lags, say
four lags, needs to be ﬁnally checked by the data.
In Table 1.2, the second column contains posterior probabilities of lagged coefﬁcients for inﬂa-
tion. The third column is for unemployment, the last is for interest rate. The bold font probabilities
are much higher than 50%, which implies the corresponding lagged variables should stay in this
VAR model. We can also ﬁnd after 3 lags, no signiﬁcant dependents left. It says that the right lag
may be 3. But a question arises from the prior choice. In table 1.2, I choose 0.5 as a prior for in-
dicators, which perhaps lower the probability of including some variables, making some variables
left the table that might stay in. To avoid this possibility, I gradually increase the prior from 0.5
to 0.75 by an increase of 5% step by step. Except that the short rate of lag 3 in policy equation
some time not very signiﬁcant but very close to criteria probability, other variables from table 1.3
to table 1.7 are very stable. Then, the second question comes out that I set prior for each variable
24

π0j = 0.5
Lag = 4
πt
ut
rt
c
0.171889
0.9844
0.090889
πt−1
1
0.017244
0.994911
ut−1
0.072044
1
0.984111
rt−1
0.007156
0.016889
1
πt−2
1
0.018111
0.7271556
ut−2
0.034044
1
0.975822
rt−2
0.006844
0.062489
0.032867
πt−3
0.030333
0.015756
0.111333
ut−3
0.042244
0.035956
0.083067
rt−3
0.003689
0.86116
0.692844
πt−4
0.014622
0.054178
0.173044
ut−4
0.026467
0.0164
0.113711
rt−4
0.005711
0.054644
0.035489
Table 1.2: Lag check1
the same every time. This may not be the case in reality. Therefore, I use standard uniform dis-
tribution for each prior. Table 1.8 shows the result is the same only with that interest rate of lag
3 for unemployment equation disappears. Actually, uniform distribution probably potentially can
lower or raise prior for some variables and this may make the result controversial. Hence, relatively
speaking prior of 50% is an appropriate choice. We also ﬁnd that the third lag of interest rate is
very isolated and this may be caused by more lags I choose. When lags are reduced to three, see
table 1.9, signiﬁcant variables are the same as previous results within four lags. When reduced to
lag of two, the result is the same as those with more lags. Altogether, coefﬁcients of signiﬁcance
found in table 1.2 is not prior sensitive and coefﬁcients on fourth lag always keep insigniﬁcant.
Now, I can say, lag of 3 is a good choice for the static VAR model using this data set.
Following the same spirit, we can also ﬁnd the signiﬁcance and therefore choose the best lags.
Table 1.11 lists the signiﬁcance of coefﬁcients for model 2 (SVS-Partial-TVP-VAR).8 Note that
value in each cell corresponds to the signiﬁcance of the coefﬁcients over the whole sample pe-
riod as a whole, i.e., β 1:T
j
, while Fig 1.14 to Fig 1.16 for model 3 (SVS-Full-TVP-VAR) gives the
signiﬁcance of each coefﬁcient in each equation, βj,t, for j = 1,...,KM and for t = 1,...,T, indi-
8Acutully for model 2, we ﬁnd three lags are best choice. However, for convenience and consistence of comparison
of results in the related literature, we choose two lags usually seen for quarterly monetary small VAR.
25

π0j = 0.55
πt
ut
rt
c
0.273067
0.98576
0.120022
πt−1
1
0.024378
0.99062
ut−1
0.209089
1
0.78644
rt−1
0.005044
0.0668
1
πt−2
1
0.041689
0.78473
ut−2
0.163756
1
0.78473
rt−2
0.007133
0.040356
0.046889
πt−3
0.0396
0.045089
0.180644
ut−3
0.039133
0.055067
0.083978
rt−3
0.004489
0.817
0.547133
πt−4
0.014733
0.057511
0.152489
ut−4
0.030556
0.0406
0.122422
rt−4
0.004244
0.07311
0.037556
Table 1.3: Lag check2
π0j = 0.6
πt
ut
rt
c
0.299644
0.99658
0.128133
πt−1
1
0.017156
0.98989
ut−1
0.202867
1
0.951
rt−1
0.009511
0.049978
1
πt−2
1
0.0162
0.7812
ut−2
0.086553
0.9672
0.9286
rt−2
0.015333
0.0338
0.046511
πt−3
0.046911
0.067889
0.180244
ut−3
0.089156
0.094422
0.112422
rt−3
0.0096
0.83849
0.78979
πt−4
0.02644
0.0746
0.217378
ut−4
0.053822
0.030222
0.140511
rt−4
0.011711
0.071578
0.040956
Table 1.4: Lag check3
26

π0j = 0.65
πt
ut
rt
c
0.4738
0.99347
0.160111
πt−1
1
0.022978
0.98916
ut−1
0.417044
1
0.9962
rt−1
0.010889
0.073869
1
πt−2
1
0.034956
0.79469
ut−2
0.241578
1
0.9668
rt−2
0.013289
0.068956
0.058978
πt−3
0.057911
0.043311
0.212667
ut−3
0.098689
0.074711
0.198711
rt−3
0.007244
0.8608
0.68487
πt−4
0.027556
0.042622
0.204711
ut−4
0.097222
0.037644
0.2132
rt−4
0.008511
0.068667
0.073844
Table 1.5: Lag check4
π0j = 0.7
πt
ut
rt
c
0.674244
0.98831
0.220867
πt−1
1
0.03444
0.97342
ut−1
0.72776
1
0.78644
rt−1
0.010711
0.080044
1
πt−2
1
0.038022
0.7376
ut−2
0.504911
1
0.92064
rt−2
0.010889
0.104978
0.07422
πt−3
0.052822
0.041067
0.222467
ut−3
0.149333
0.061311
0.210133
rt−3
0.008222
0.88473
0.74669
πt−4
0.035867
0.040178
0.230178
ut−4
0.181156
0.030689
0.287578
rt−4
0.009378
0.0838
0.060111
Table 1.6: Lag check5
27

π0j = 0.75
πt
ut
rt
c
0.676533
0.99118
0.238756
πt−1
1
0.0366
0.99551
ut−1
0.6782
1
0.98373
rt−1
0.018956
0.101644
1
πt−2
1
0.54556
0.78229
ut−2
0.344467
0.9658
0.9364
rt−2
0.014333
0.085689
0.105933
πt−3
0.067822
0.063178
0.344244
ut−3
0.241533
0.126867
0.250044
rt−3
0.0118
0.85351
0.746289
πt−4
0.034489
0.0628
0.302933
ut−4
0.159533
0.063089
0.346
rt−4
0.009822
0.092244
0.096644
Table 1.7: Lag check6
π0 j ∼U (0,1)
πt
ut
rt
c
0.020733
0.99653
0.013222
πt−1
1
0.009
0.96131
ut−1
0.037489
1
0.96469
rt−1
0.000956
0.023689
1
πt−2
1
0.062667
0.63662
ut−2
0.035689
0.94331
0.84693
rt−2
0.014289
0.111756
0.000778
πt−3
0.000956
0.036289
0.349956
ut−3
0.001578
0.9794
0.174378
rt−3
0.002978
0.151089
0.8954
πt−4
0.025911
0.529983
0.424889
ut−4
0.6608
0.279311
0.117289
rt−4
0.001289
0.263667
0.059067
Table 1.8: Lag check7
28

π0j = 0.5
πt
ut
rt
c
0.1501
0.999
0.0994
πt−1
1
0.0099
0.9816
ut−1
0.0094
1
1
rt−1
0.0031
0.1093
1
πt−2
1
0.0074
0.7273
ut−2
0.0082
1
1
rt−2
0.0042
0.116
0.0323
πt−3
0.0314
0.0105
0.2294
ut−3
0.0082
0.0145
0.0528
rt−3
0.0022
0.8137
0.8235
Table 1.9: Lag check8
π0j = 0.5
πt
ut
rt
c
0.1152
0.9789
0.0984
πt−1
1
0.1765
0.9918
ut−1
0.0124
1
1
rt−1
0.0023
0.1378
1
πt−2
1
0.149
0.6464
ut−2
0.0096
1
1
rt−2
0.0062
0.5454
0.0872
Table 1.10: Lag check9
29

π0j = 0.5
πt
ut
rt
ct
1
0.4620
0.9550
πt−1
1
0.2170
0.9460
ut−1
1
1
1
rt−1
0.3250
0.0370
1
πt−2
1
0.8810
0.7050
ut−2
1
1
1
rt−2
0.3430
0
1
Table 1.11: Lags in model 2
vidually. We shall come back to them in the next subsections when discussing possible structrual
changes in agents’ behavior and monetary policy implementation.
1.4.3
Impulse responses
Next, let’s check impulse responses. First, we compare impulse responses of unrestriced Bayesian
VAR with static SVS-BVAR of model 1. The unrestricted Bayesian VAR is equivalent to setting
prior that all parameters are in the model with probability of one in terms of SVS-BVAR. All
parameters in SVS-BVAR are set with prior probability of 50%.
Recursive identiﬁcation of exogenous monetary policy shocks is used in which short rate is
placed in the last order such that inﬂation and unemployment rate can impact policy immediately
while policy rate affect them with one lag. The size of monetary shock is normalized to one
percentage.
The result shows that in the ﬁrst several periods, for unrestriced model, responses of unem-
ployment rate get down, which contracts with economic theory and unsurprisingly, typically seen
in small scale monetary VAR literature. However, when stochastic selection is imposed, it im-
proves completely, getting rise directly after lags of 3 periods due to the infﬂuence of irrelevant
variables alleviated by stochastic selection. Interest rate response to inﬂation in unrestricted model
is modest, after around 5 periods starting above 1, then gradually get down to zero around the 21st
period. Responses of interest rate in model with stochastic selection, are quite strong, until after
24 periods, it still stays close to 2 at median, which means in a long run respect Taylor rule is very
30

powerful that’s different with the result for unrestricted model.9
Now let’s look at impulse responses (IRs hereafter) of variables to monetary policy shocks in
model 2 and model 3. Both models are imposed with multivariate stochastic volatility in order to
jointly analyze systematic and non-systematic changes. Time variation of coefﬁcients on regressors
and stochastic volatility have extended the IRs with dynamic property over time as opposed to static
model 1.
We randomly choose three period 1975 Q1, 1981 Q3 and 1996 Q1 to represent three chairman-
ships respectively. Figure 1.2, corresponding to model 3, shows the impulse response functions
of inﬂation, unemployment and three month short rate to one percentage monetary policy shock,
with solid line representing median, dashed lines 16th and 84th percentile respectively of posterior
IR distribution. All the responses perfectly satisfy economic theory. There are no price puzzle typ-
ically seen in a small scale monetary VAR which can be weakened after adding forward looking
prices or using large data set (Bernanke et al., 2005); Unemployment rates also increase quickly
under contracting monetary policy. All the responses under recursive identiﬁcation are in line with
that under agnostic identiﬁcation of sign restriction of Uhlig (2005). If comparing the same re-
sponses with that under model 2, in Figure 1.1, and unrestricted TVP-VAR, we ﬁnd that there are
‘price puzzle’ and ‘unemployment abnormal’, though no longer signiﬁcant in model 2 – the sim-
ilar improvement in rich data set model such as factor augmented models, but seldom completely
diminishing. This means that even though in a small scale VAR model, however, with coefﬁcient
restrictions, we can still obtain theory consistent results that are difﬁcult without restriction. On the
other hand, we also ﬁnd error bands of IRs for model 3 in Figure 1.2 have substantially narrowed as
opposed to those of IRs for model 2 in Figure 1.1. A reasonable explanation for this is that model
3 has restrictions both over identity and time, while model 2 not allowed for time dimension.
9Since the static typical small scale VAR is widely analysed, we do not provide ﬁgues for the save of space.
31

1.4.4
Systematic or exogenous change
An important thing we need to consider in monetary policy issue is that there might be structural
changes in monetary policy with different chairmanships of Burns, Volcker and Greenspan, though
some believe, some not. Related literature has given large number of evidence about this issue,
though until now such dispute still exists there in theoretical and empirical study and this one
sometimes more or less associated with other topics like sources or causes of the Great Inﬂation
or the Great Moderation. One thing we need to conﬁrm is that generally there are two periods
suitable for monetary policy test. One is from 1963 Q1 to 1973 Q3 corresponding approximately
to the period of rising inﬂation before the Volcker chairmanship. The period 1982 Q4 to 2006
Q3 corresponds to the Volcker and Greenspan chairmanships excluding the years of Monetary
Targeting, for which the Taylor rule might not represent an appropriate description of systematic
monetary policy (see, for instance, Hanson, 2006; Sims and Zha, 2006).
Recently, time varying parameter VAR with stochastic volatility (see, among many other, Cog-
ley and Sargent, 2005; Cogley, Primiceri, and Sargent, 2010; Koop, Leon-Gonzalez, and Strachan,
2009; Koop and Korobilis, 2013) has been widely used in empirical analysis in business cycle, pol-
icy change, forecast and so on, because this kind of models can capture time variation properties
of coefﬁcient and volatility that probably reﬂect structural changes in a gradual manner rather than
abrupt one like Markov regime switch model with probability or threshold regime switch model
when threshold variable is above or below threshold value. On the other hand, SV can investigate
some potential exogenous or non-systematic changes during sample period that is not capable for
model without SV. However, these TVP models all associate with parameter proliferation problem
due to the extension of parameters to time dimension even if in a small scale VAR, which might
give incorrect properties and possible wrong inferences. Therefore, TVP-VAR with stochastic
variable selection is a good tool to analyze such problems relevant with systematic changes via
time varying coefﬁcients and non-systematic changes via stochastic volatility.
32

1.4.4.1
Agents bahavior to monetary shocks
We investigate whether structure has changed in the agents’ behavior to monetary policy shocks.
The magnitude of monetary policy is standardized to one percentage in each period. We compare
the difference among these three periods mentioned above.
Intuitively, in Figure 1.1 and Figure 1.2, IRs of inﬂation and unemployment did not seem
change much. However, we need a way to precisely estimate the difference. Following Primiceri
(2005) and others, we compute the difference of IRs between every two periods mutually among
the three periods in every iteration after burin-in during Bayesian estimation and therefore obtain
the posterior distribution of the IR difference .
Figure 1.3 and Figure 1.4, for model 3, show that there are no signiﬁcant differences between
these periods no matter for response of inﬂation or unemployment, because they are all insigniﬁ-
cant with zero line. This means that economic agents have not altered their behavior and hence no
structure changes in the agents in response to non systematic monetary policy shocks. The same
results can also be found in model 2, see Figure 1.5 and Figure 1.6, and in unrestricted TVP-VAR,
but with larger error band compared with model 3 for the reason we have discussed before.10
Since arbitrarily picking up three periods, we can not guarantee that the ‘insigniﬁcant change
in agent behavior’ always hold during the whole sample period. The 3-D impulse responses of
inﬂation and unemployment rate over the whole period respectively, in Figure 1.8 for model 3, are
given, showing that it is reasonable to believe that economic agents keep the same way in decision
making. Figure 1.7 for model 2 also support this conclusion with impuse responses appearing
more smooth.
1.4.4.2
Systematic monetary policy
We check if the implementation of monetary policy has changed. Long run response of policy
rate to inﬂation shocks and unemployment shocks is used to represent monetary policy stance.11
10For saving space, we do not give ﬁgures for unrestricted TVP-VAR from which they deliver the results in line
with the literature.
11In standard New Keynesian DSGE models, the monetary policy rule follows this similar type:
33

3
6
9
12 15 18 21
−2
−1
0
1
IR of inflation, 1975:Q1
3
6
9
12 15 18 21
−4
−2
0
2
4
IR of unemployment, 1975:Q1
3
6
9
12 15 18 21
−4
−2
0
2
4
IR of interest rate, 1975:Q1
3
6
9
12 15 18 21
−2
−1
0
1
IR of inflation, 1981:Q3
3
6
9
12 15 18 21
−4
−2
0
2
4
IR of unemployment, 1981:Q3
3
6
9
12 15 18 21
−4
−2
0
2
4
IR of interest rate, 1981:Q3
3
6
9
12 15 18 21
−2
−1
0
1
IR of inflation, 1996:Q1
3
6
9
12 15 18 21
−4
−2
0
2
4
IR of unemployment, 1996:Q1
3
6
9
12 15 18 21
−4
−2
0
2
4
IR of interest rate, 1996:Q1
Figure 1.1: IRs for model 2
Notes: The model 2 is SVS-Partial-TVP-VAR with SV. The ﬁrst column plots impulse responses
of inﬂation to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The second column
plots impulse responses of unemployment rate to monetary policy shocks in 1975 Q1, 1981 Q3
and 1996 Q1. The third column plots impulse responses of interst rate to monetary policy shocks
in 1975 Q1, 1981 Q3 and 1996 Q1. Solid line represents posterior median of impluse response
distribution with dashed lines of 16th percentile and 84th percentile, respectively.
34

3
6
9 12 15 18 21
−0.5
0
0.5
IR of inflation, 1975:Q1
3
6
9 12 15 18 21
−0.2
0
0.2
IR of unemployment, 1975:Q1
3
6
9 12 15 18 21
−1
0
1
2
IR of interest rate, 1975:Q1
3
6
9 12 15 18 21
−0.5
0
0.5
IR of inflation, 1981:Q3
3
6
9 12 15 18 21
−0.2
0
0.2
IR of unemployment, 1981:Q3
3
6
9 12 15 18 21
−1
0
1
2
IR of interest rate, 1981:Q3
3
6
9 12 15 18 21
−0.5
0
0.5
IR of inflation, 1996:Q1
3
6
9 12 15 18 21
−0.2
0
0.2
IR of unemployment, 1996:Q1
3
6
9 12 15 18 21
−0.5
0
0.5
1
IR of interest rate, 1996:Q1
Figure 1.2: IRs for model 3
Notes: The model 3 is SVS-Full-TVP-VAR with SV. The ﬁrst column plots impulse responses of
inﬂation to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The second column plots
impulse responses of unemployment rate to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996
Q1. The third column plots impulse responses of interst rate to monetary policy shocks in 1975
Q1, 1981 Q3 and 1996 Q1. Solid line represents posterior median of impluse response distribution
with dashed lines of 16th percentile and 84th percentile, respectively.
35

3
6
9
12
15
18
21
−0.4
−0.3
−0.2
−0.1
0
median IR of inflation to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.06
−0.04
−0.02
0
0.02
0.04
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.1
−0.05
0
0.05
0.1
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.1
−0.05
0
0.05
0.1
1996 Q1−1975 Q1
Figure 1.3: IR-inﬂation-comparison for model 3
Notes: The model 3 is SVS-Full-TVP-VAR with SV. The upper left panel plots median impulse
responses of inﬂation to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remain-
ing three panels plot difference of responses between every two periods mutually, with solid line
representing median and dashed lines for 16th percentile and 84th percentile respectively.
36

3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
0.06
0.08
IR of unemployment to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.06
−0.04
−0.02
0
0.02
0.04
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.1
−0.05
0
0.05
0.1
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.1
−0.05
0
0.05
0.1
1996 Q1−1975 Q1
Figure 1.4: IRs-unemployment rate-comparison for model 3
Notes: The model 3 is SVS-Full-TVP-VAR with SV. The upper left panel plots median impulse
responses of unemployment rate to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1.
The remaining three panels plot difference of responses between every two periods mutually, with
solid line representing median and dashed lines for 16th percentile and 84th percentile respectively.
37

3
6
9
12
15
18
21
−1
−0.8
−0.6
−0.4
−0.2
0
median IR of inflation to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.2
−0.1
0
0.1
0.2
0.3
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.4
−0.2
0
0.2
0.4
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.4
−0.2
0
0.2
0.4
0.6
1996 Q1−1975 Q1
Figure 1.5: IRs-inﬂation-comparison for model 2
Notes: The model 2 is SVS-Partial-TVP-VAR with SV. The upper left panel plots median impulse
responses of inﬂation to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remain-
ing three panels plot difference of responses between every two periods mutually, with solid line
representing median and dashed lines for 16th percentile and 84th percentile respectively.
38

3
6
9
12
15
18
21
−0.5
0
0.5
1
1.5
IR of unemployment to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.4
−0.2
0
0.2
0.4
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−1
−0.5
0
0.5
1
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−1
−0.5
0
0.5
1
1996 Q1−1975 Q1
Figure 1.6: IRs-unemployment rate-comparison for model 2
Notes: The model 2 is SVS-Partial-TVP-VAR with SV. The upper left panel plots median impulse
responses of unemployment rate to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1.
The remaining three panels plot difference of responses between every two periods mutually, with
solid line representing median and dashed lines for 16th percentile and 84th percentile respectively.
39

1965 1970 1975 1980 1985 1990 1995 2000 2005
5
10
15
20
−0.8
−0.6
−0.4
−0.2
Impulse Responses of Inflation to Monetary Policy Shock, 3D
1965 1970 1975 1980 1985 1990 1995 2000 2005
5
10
15
20
0
0.5
1
Impulse Responses of Unemployment to Monetary Policy Shock, 3D
Figure 1.7: 3D-IRs for model 2
Notes: The model 2 is SVS-Partial-TVP-VAR with SV. The upper panel plots median impulse
responses of inﬂation to monetary policy shocks over the whole sample period. The lower panel
plots median impulse responses of unemployment rate to monetary policy shocks over the whole
sample period.
40

1965 1970 1975 1980 1985 1990 1995 2000 2005
5
10
15
20
−0.3
−0.2
−0.1
0
Impulse Responses of Inflation to Monetary Policy Shock, 3D
1965 1970 1975 1980 1985 1990 1995 2000 2005
5
10
15
20
−0.05
0
0.05
Impulse Responses of Unemployment to Monetary Policy Shock, 3D
Figure 1.8: 3D-IRs for model 3
Notes: The model 3 is SVS-Full-TVP-VAR with SV. The upper panel plots median impulse re-
sponses of inﬂation to monetary policy shocks over the whole sample period. The lower panel
plots median impulse responses of unemployment rate to monetary policy shocks over the whole
sample period.
41

Responses for 5 quarters, 10 quaters and 15 quarters are examined for policy strength.
We ﬁrst look at policy response to inﬂation shock. In Figure 1.9, we ﬁnd that around after
middle 1970s, policy sensitivity to inﬂation has increased after 5 quaters, 10 quaters and 15 quaters,
but not signiﬁcant with a line always through the whole error band respectively. This line can be
above one or below one. Figure 1.10 have the same property, however, with very broad conﬁdence
band. Therefore, we can only reach that there are no signiﬁcant structural change in policy rule
which is consistent with Primiceri (2005) and Sims and Zha (2006).
Monetary reaction to unemployment shocks has signiﬁcantly enhanced after 5 quaters and 10
quaters, but become insigniﬁcant after 15 quaters in Figure 1.11 for model 3 over the sample
period. This ﬁnding can not be captured in Figure 1.12 for model 2 as they are all not signiﬁcant.
Generally speaking, we can not conclude that policy stance to unemployment has signiﬁcantly
changed.
1.4.4.3
Non-systematic monetary policy shocks
The last panel of Figure 1.13 plots the dynamic process of stochastic volatility for nonsystematic
policy with narrow band for model 3. This path is well conﬁrmed in the literature that the volatility
during era of the Great Inﬂation is much higher than the episode of the Great Moderation, and
the ﬂutuation in the period of monetary targeting of chairmanship of Volk is dramatically volatile.
Together with the previous analysis, we only ﬁnd volatility of exogenous shocks have changed
substantially.
1.4.4.4
An interpretation
Altogether, even though with restricted TVP-VAR via stochastic variable selection in model 2 and
model 3, We still have the almost the same conclusion that there are no signiﬁcant switches in
agents’ behavior and monetary policy stance, as opposed to which, exogenous shocks have altered
strongly.
rt = ρrrt−1 +(1−ρr)(ρππt +ρyyt)+εM
t
where long run response ρπ and ρy represent monetary policy stance.
42

1970
1980
1990
2000
0.5
1
1.5
inflation shock after 0Qs
1970
1980
1990
2000
0.6
0.8
1
1.2
1.4
1.6
1.8
inflation shock after 5Qs
1970
1980
1990
2000
0.8
1
1.2
1.4
1.6
inflation shock after 10Qs
1970
1980
1990
2000
0.6
0.8
1
1.2
1.4
inflation shock after 15Qs
Figure 1.9: Policy response to inﬂation for model 3
Notes: The model 3 is SVS-Full-TVP-VAR with SV. The upper left panel plots immediate im-
pulse responses of policy rate to inﬂation shocks over the whole sample period. The remaining
panels plot impulse responses of policy rate to inﬂation shocks in the 5th , 10th, and 15th period
respectively over the whole sample period.
43

1970
1980
1990
2000
0
0.1
0.2
0.3
0.4
0.5
inflation shock after 0Qs
1970
1980
1990
2000
0.5
1
1.5
inflation shock after 5Qs
1970
1980
1990
2000
−0.5
0
0.5
1
1.5
2
inflation shock after 10Qs
1970
1980
1990
2000
−1.5
−1
−0.5
0
0.5
1
inflation shock after 15Qs
Figure 1.10: Policy response to inﬂation for model 2
Notes: The model 2 is SVS-Partial-TVP-VAR with SV. The upper left panel plots immediate
impulse responses of policy rate to inﬂation shocks over the whole sample period. The remaining
panels plot impulse responses of policy rate to inﬂation shocks in the 5th , 10th, and 15th period
respectively over the whole sample period.
44

1970
1980
1990
2000
−2
−1.5
−1
−0.5
unemployment shock after 0Qs
1970
1980
1990
2000
−2
−1.5
−1
−0.5
unemployment shock after 5Qs
1970
1980
1990
2000
−1.5
−1
−0.5
unemployment shock after 10Qs
1970
1980
1990
2000
−1.2
−1
−0.8
−0.6
−0.4
−0.2
unemployment shock after 15Qs
Figure 1.11: Policy response to unemployment rate for model 3
Notes: The model 3 is SVS-Full-TVP-VAR with SV. The upper left panel plots immediate impulse
responses of policy rate to unemployment rate shocks over the whole sample period. The remaining
panels plot impulse responses of policy rate to unemployment rate shocks in the 5th , 10th, and 15th
period respectively over the whole sample period.
45

1970
1980
1990
2000
−1
−0.8
−0.6
−0.4
unemployment shock after 0Qs
1970
1980
1990
2000
−3.5
−3
−2.5
−2
−1.5
unemployment shock after 5Qs
1970
1980
1990
2000
−3
−2
−1
0
1
unemployment shock after 10Qs
1970
1980
1990
2000
−2
−1
0
1
2
3
unemployment shock after 15Qs
Figure 1.12: Policy response to unemployment rate for model 2
Notes: The model 2 is SVS-Partial-TVP-VAR with SV. The upper left panel plots immediate
impulse responses of policy rate to unemplyment rate shocks over the whole sample period. The
remaining panels plot impulse responses of policy rate to unemployment rate shocks in the 5th ,
10th, and 15th period respectively over the whole sample period.
46

1965
1970
1975
1980
1985
1990
1995
2000
2005
0.2
0.4
0.6
0.8
Posterior median of the standard deviation of residuals in Inflation equation
1965
1970
1975
1980
1985
1990
1995
2000
2005
0.2
0.4
0.6
0.8
1
Posterior median of the standard deviation of residuals in Unemployment equation
1965
1970
1975
1980
1985
1990
1995
2000
2005
0.5
1
1.5
2
2.5
Posterior median of the standard deviation of residuals in Interest Rate equation
Figure 1.13: Volatility for each variable in model 3
47

A possible reason we believe is that although restricted TVP-VAR of model 2 and model 3,
especially for model 3, have signiﬁcantly improved the parameter proliﬁcation problem and re-
duced uncertainty, but this merit is not strong enough to overturn the view of stability of structures
existing both in economic agents and policy authority. The prior probability for each coefﬁcient
in each equation is set to π0jt = 0.5. Fig 1.14 to Fig 1.16 shows the posterior signiﬁcance of each
coefﬁcient along the sample period. Two points can be found that ﬁrst, variables always react to
its own ﬁrst lag with probability of one, and then become week around 50% with special case for
inﬂation (equation 1) where it also strongly response to its own lag 2 during some periods of the
Great Inﬂation and money base targeting; Second, other lags, no matter domestic or foreign, ﬂuc-
tuate with the mild and relatively the same magnitude around half percentage. These two points
are consistent with empirical literature such as settings of shrinkage priors ( for example, the Min-
nesota prior, see Doan, Litterman and Sims, 1984, Litterman, 1986 and its extension, see Banbura,
Giannone and Reichlin, 2010, among others).
From these ﬁgures, there is no prominent jump of probability, except for their own lags, for
each coefﬁcient in each equation over sample period. That is, every coefﬁcient always plays almost
the same (relative) importance in this system. Structural change needs abrupt shift and keep it for
a period or gradual increase or decease to over some critical threshold point. We can not ﬁnd such
style in these ﬁgures.
1.5
Robust check
Every TVP-VAR model has the nature that its latent state coefﬁcients have to change or break
every period due to its transition equation dynamics and this change has to be smoothed because
of the backward recursion as described in Carter and Kohn (1994) in estimation after observing all
the data. Hence there is a situation in which some state variables of small variations will more or
less take over and narrow down the weight originally played by those of large variation in order
to make the estimation smooth. Under such kind of operation, Primiceri (2005) discussed this
48

1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
policy rule c/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
inf−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
u−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0.6
0.8
1
r−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
inf−2/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
u−2/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
r−2/%
Figure 1.14: Posterior probability for each coefﬁcient in policy rule
49

1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
eq1 c/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0.6
0.8
1
inf−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
u−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
r−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
inf−2/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0.4
0.6
0.8
u−2/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
r−2/%
Figure 1.15: Posterior probability of each coefﬁcient in inﬂation equation
50

1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
eq2 c/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
inf−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0.6
0.8
1
u−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
r−1/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
inf−2/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
u−2/%
1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.5
1
r−2/%
Figure 1.16: Poterior probability of coefﬁcient in unemployment rate equation
51

concern. In this case, misleading could take place and this is potential missing in the proof via Fig
1.14 to Fig1.16 deriving from model 3 with also such kind of smoothness operation. Probably it is
one reason for Sims and Zha (2006) to choose Markov regime switch models to detect structural
changes occurring in an economy.
The above concern motivates the aim of the robust check in this section. Given that large scale
breaks have high potential to be candidates of systematic changes while non-large-breaks not,
then naturally, designing models good at being able to seize large breaks and meanwhile defense
inﬂuence from not large variations is a necessary direction. Stepping further, if we still can not
ﬁnd systematic change under the large beak, it implies that it’s more impossible to infer structural
alterations in other non-large-break periods, because these non-large-breaks are more less qualiﬁed
to be considered as possible structural switches. This deletes the above concern, complements and
closes the proof that the TVP-VAR system is stable for the data set. Therefore there were no
structural changes in monetary policy and economic behavior, and exogenous shocks naturally
play the role of accounting for the empirical dynamic difference between the ‘Great Inﬂation’ and
the ‘Great Moderation’ .
Recently, following Bauwens, Koop, Korobilis and Rombouts (2014), Korobilis (2013b) de-
velop a TVP-VAR model involving discrete Markov process used for forecasting. We use this
model to do our robust check.
The basic idea is that the latent coefﬁcients in this TVP model are subject to several possible
regime states which follows Markov process, meaning the regime – the corresponding coefﬁcient
– is not necessary to move every period. The model is formulated as follows modiﬁed on the
previous TVP models:
yt = Ztθst +εt
(1.14)
θst = Γβst
(1.15)
52

βst = βst−1 +ut
(1.16)
where εt ∼N (0,R), ut ∼N (0,Q) and st ∈[1,...,M +1] is Markov process of order one with block
diagonal transition matrix of the form


p11
p12
0
···
0
0
p22
p23
...
...
...
...
...
...
0
0
pMM
pM,M+1
0
···
0
0
pM+1


This model speciﬁes there is a break between t and t + 1, namely βst+1 ̸= βst due to ut ̸= 0 if
only if st ̸= st+1, otherwise βst+1 = βst and the state process st only goes forward and never comes
back without memory.12 This deﬁnes the new transition equation. Not every βst is time varying,
only when regime switch happens that does deserve. In other words, not always, only the limited
variations in βstuniquely correspond to several large breaks (herein M breaks) that have relatively
more importance than other non-large-breaks in the data set.
The assumption of regime switching ‘without memory’ (the block diagonal transition matrix)
seems very strong, probably not in line with empirical evidence, but it is very suitable for us to
detect large breaks – the candidates of structural changes – in posterior respect meanwhile ﬁltering
out small variation not qualiﬁed as ‘breaks’ that however always involved in TVP models causing
potential contamination of inference. In a word, the model only paying attention to large breaks
has been already sufﬁcient for us to ﬁnd large breaks.
We priorly set two breaks, namely three regimes for U.S. sample set.13 Fig 1.17 plots the
posterior probability for each regime. It is evident that only two regimes, regime 1 and regime 2
12The ‘never comes back without memory’ is based on the idea that it is reasonable to regard it as a ‘new break’
if ut is large enough and also on compatibility of estimation after incorporating discrete Markov (regime switching)
process into TVP models for smoother part, which can be found in the appendix of Bayesian estimation of the model.
13We also tried three breaks but denied by the data.
53

1965
1970
1975
1980
1985
1990
1995
2000
2005
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Probability for each regime
 
 
regime 1
regime 2
regime 3
Figure 1.17: Posterior probability of each regime
are strongly supported by the data. For regime 3, it is zero percentage before around 2003, after that
2% signiﬁcantly denied by the data. Now let’s look at the dynamic relationship between regime 1
and regime 2. Regime 1 strongly dominated with posterior probability of value one before the end
of 1980s, then after cross point around 1981, the regime 2 took the place of regime 1 dramatically
climbing up to probability near one till the end of 2006 Q2 with the trivial role of regime 3 of
extremely low level support. The information showing up in Fig 1.17 is not surprising, conforming
with the literature that the year of 1981, among monetary targeting project in the chairmanship of
Volcker, is the threshold point that divides the sample into two regimes. The property of TVP-VAR
with regime switching that is able to pick up large breaks clearly catch the signiﬁcant switch of
monetary policy.14
14Note that the TVP-VAR with regime switch in this paper does not belong to the category of conventional state
space models with regime switching in which regime switch occurs on parameters. See chapter 10 of Kim and Nelson
(1999).
54

Has the economy changed before and after monetary targeting policy in the aim to attacking
high inﬂation? Fig 1.18 and Fig 1.19 give the ‘NO’ answer. Take regime 3 into account, though
it is trivial, to give a full piture. Differences among these three regimes for reactions of inﬂation
and unemployment to monetary policy shocks are not signiﬁcant with zero, which is in line with
ﬁndings from Model 2 and Model 3. Monetary policy stance is also investigated. In Fig 1.20,
in response to inﬂation shocks, the Fed’s reaction had no signiﬁcant differences during every two
regimes, no matter in short run or long run. The same result is also found in response to unem-
ployment shocks in Fig 1.21. Altogether, there were no systematic changes in Monetary policy
and agents’ behavior.
Since in large breaks systematic changes are not found, the concern in model 2 and model 3
about possible inﬂuence of small variation on large variation is neglectable.
1.6
Concluding remarks
In this paper, we present models for dealing with the problem of parameter proliferation – as-
sociated with potential incorrect inferences – in VAR models (under usual limited data access).
They are static SVS-BVAR (Model 1), SVS-Partial-TVP-VAR (Model 2) and SVS-Full-TVP-VAR
(Model 3). For the latter two models, I incorporate multivariate SV so as to investigate systematic
and non-systematic changes jointly. Actually, the three models are consistent each other; Model
1 and Model 2 are special cases of Model 3 after relaxing some restrictions, as we discussed in
section 2.
With these models and U.S. quarterly data, we analyzed whether there were systematic switches
in U.S monetary stance and economic agents’ behavior. After investigating long run responses of
policy rate to inﬂation and unemployment shocks, respectively, we ﬁnd that there were insignif-
icant changes in systematic monetary policy along the whole sample period. We also evaluated
agents’ behavior examined by monetary policy shocks in three arbitrary periods, then extended to
all the sample period and still no signiﬁcant responses were found. We payed more attention on
55

3
6
9
12
15
18
21
−0.05
0
0.05
0.1
0.15
IRs of inflation to MP shocks
 
 
regime 1
regime 2
regime 3
3
6
9
12
15
18
21
−0.15
−0.1
−0.05
0
0.05
regime 2 − regime 1
3
6
9
12
15
18
21
−0.1
−0.05
0
0.05
0.1
regime 3 − regime 2
3
6
9
12
15
18
21
−0.2
−0.1
0
0.1
0.2
regime 3 − regime 1
Figure 1.18: IRs of inﬂation and their difference between different regimes
Notes: The upper left panel plots median impulse responses of inﬂation to monetary policy shocks
in regime 1, regime 2, and regime 3. The remaining three panels plot difference of responses
between every two regimes mutually, with solid line representing median and dashed lines for 16th
percentile and 84th percentile respectively.
56

3
6
9
12
15
18
21
−0.05
0
0.05
0.1
0.15
IRs of unemployment to MP shocks
 
 
regime 1
regime 2
regime 3
3
6
9
12
15
18
21
−0.1
−0.05
0
0.05
0.1
regime 2 − regime 1
3
6
9
12
15
18
21
−0.1
0
0.1
0.2
0.3
regime 3 − regime 2
3
6
9
12
15
18
21
−0.1
0
0.1
0.2
0.3
regime 3 − regime 1
Figure 1.19: IRs of unemployment rate and their difference between different regimes
Notes: The upper left panel plots median impulse responses of unemployment rate to monetary
policy shocks in regime 1, regime 2, and regime 3. The remaining three panels plot difference of
responses between every two regimes mutually, with solid line representing median and dashed
lines for 16th percentile and 84th percentile respectively.
57

3
6
9
12
15
18
21
0
0.5
1
1.5
IRs of interest rate to Inflation shocks
 
 
regime 1
regime 2
regime 3
3
6
9
12
15
18
21
−1
−0.5
0
0.5
1
regime 2 − regime 1
3
6
9
12
15
18
21
−1
−0.5
0
0.5
regime 3 − regime 2
3
6
9
12
15
18
21
−1
−0.5
0
0.5
1
regime 3 − regime 1
Figure 1.20: IRs of interest rate to inﬂation shocks
Notes: The upper left panel plots median impulse responses of interest rate to inﬂation shocks
in regime 1, regime 2, and regime 3. The remaining three panels plot difference of responses
between every two regimes mutually, with solid line representing median and dashed lines for 16th
percentile and 84th percentile respectively.
58

3
6
9
12
15
18
21
−3
−2
−1
0
IRs of interest rate to unemployment shocks
 
 
regime 1
regime 2
regime 3
3
6
9
12
15
18
21
−1
−0.5
0
0.5
regime 2 − regime 1
3
6
9
12
15
18
21
−1
−0.5
0
0.5
1
regime 3 − regime 2
3
6
9
12
15
18
21
−2
−1
0
1
regime 3 − regime 1
Figure 1.21: IRs of interest rate to unemployment shocks
Notes: The upper left panel plots median impulse responses of interest rate to unemployment
shocks in regime 1, regime 2, and regime 3. The remaining three panels plot difference of responses
between every two regimes mutually, with solid line representing median and dashed lines for 16th
percentile and 84th percentile respectively.
59

model 2 and model 3 due to their time variation property in tracking dynamic process. With the
model 2 of block checking and model 3 of single checking together, in case of possible missing
and misleading in model 2, We ﬁnd insigniﬁcance alterations in behavior of both economic agents
and policy authority.
One thing worth noting is that ﬁgures for model 3 indeed have seized some changes in the
policy implementation with obviously narrow error band as opposed to model 2, however, these
beneﬁts resulting from stochastic selection controlling signiﬁcance of each variable along time
period have not overturn the ﬁndings in model 2. Posterior probability for every coefﬁcient over
time dimension delivers the plausible reason that except for its own lag of one, relative importance
of interaction among variables over time is generally stable that can be found from Fig 1.14 to Fig
1.16.
The concern that large variation can be weakened by small variation due to smoothness op-
eration in TVP-VAR estimation gives rise to sensitivity check via a modiﬁed TVP-VAR model
involved with discrete Markov process. We ﬁnd the concern can be ignored. Hence it is believable
to conclude that since the system is indeed stable by statistical proof, the empirical dynamic change
before and after Volcker monetary targeting can only be attributed to exogenous shocks, not deter-
mined by the system. This conclusion consistent with Primiceri (2005) and Sims and Zha (2006).
On the other hand, the stability, namely the equality of relative importance of each variable in the
typical small scale monetary TVP-VAR system is not sensitive to the choice of models of parsimo-
nious restrictions, making the concern about over parameterization and smoothness typically seen
in TVP-VAR system neglectable.
In this sense, the value of the paper lies not only in examining and conﬁrming one of views
in the literature using other different models, but also giving the underlying reason as well as
statistical proof why their view can still hold even though with more enriched models such as
stochastic variable selection models presented in this paper.
These models can apply to other topics like ﬁscal policy change, ﬁnancial market ﬂuctuation
and so on that are left for our future research.
60

Appendix
A. The basic Kalman ﬁlter and state smoother
Consider a typical state space model, ignoring the inﬂuence of exogenous variables15:
yt = Xtβt +ut
(1.17)
βt = Fβt−1 +εt
(1.18)
where yt is an n × 1 observables, Xt is a n × k matrix of regressors, βt is a k × 1 unobserable
vector state following the dynamic process in (1.18); ut ∼N (0,Rt) and εt ∼N (0,Q) as well as
cov(ut,εs) = 0 for ∀t and ∀s. Usually, equation (1.17) is called measurement or observation equa-
tion and (1.18) the transition or evolution or state equation.
Denote βt|s and Vt|s conditional mean and conditional variance of βt based on information set
up to and including t, respectively. The forward ﬁlter, with initial moments β0|0 and V0|0, consists
of two steps:
Prediction
βt|t−1 = Fβt−1|t−1
Vt|t−1 = FVt−1|t−1F
′ +Q
Updating
Kt = Vt|t−1X
′
t

XtVt|t−1X
′
t +Rt
−1
βt|t = βt|t−1 +Kt
 yt −Xtβt|t−1

Vt|t = Vt|t−1 −KtXtVt|t−1
15Including exogenous variables in measurement equation and/or transition equation does not affect the derivation
of Kalman ﬁlter and state smoother.
61

this process proceeds from t = 0 to t = T. At the conclusion of the forward recursion, draw βT
from N
 βT|T,VT|T

.
With βT regarded as observation, backward smoother starts from t = T −1 to t = 1 with
βt|t+1 = βt|t +Vt|tF′V −1
t+1|t
 βt+1 −Fβt|t

Vt|t+1 = Vt|t −Vt|tF′V −1
t|t+1FVt|t
then βt is drawn from posterior N
 βt|t+1,Vt|t+1

after observing all the data.
62

B. Modiﬁed Kalman ﬁlter and state smoother with breaks
Consider a modiﬁed state space model
yt = Xtβst +ut
βst = βst−1 +εt
in which setting, the time varying coefﬁcients βt depend on latent state st that follows a discrete
Markov process. For details of the model speciﬁcation, please refer to section 5 in this chapter.
For the forward ﬁlter part,
βt|t−1 = βt−1|t−1
Vt|t−1 =







Vt−1|t−1 +Q
if st−1 ̸= st
Vt−1|t−1
otherwise
Kt = Vt|t−1X
′
t

XtVt|t−1X
′
t +Rt
−1
βt|t = βt|t−1 +Kt
 yt −Xtβt|t−1

Vt|t = Vt|t−1 −KtXtVt|t−1
For the backward smoother part,
βt|t+1 =







βt|t +Vt|tV −1
t+1|t
 βt+1 −βt|t

if st+1 ̸= st
βt|t
otherwise
Vt|t+1 =







Vt|t −Vt|tV −1
t|t+1Vt|t
if st+1 ̸= st
Vt|t
otherwise
63

C. Estimation of states without Kalman ﬁlter and state smoother
Consider the general state space model in (1.17) and (1.18) in A1. Stack the two equations period
by period and collect all the data and states together, one obtains
y = Xβ +u
(1.19)
where y =


y1
...
yT


, X =


X1
···
0
...
...
...
0
···
XT


, β =


β1
...
βT


, u =


u1
...
uT


,
with u ∼N (0,R) and R = Blkdiag

{Rt}T
t=1

.
The transition equation becomes
Hβ = ε
(1.20)
where
H =


Iq
F
Iq
F
Iq
...
F
Iq


and


ε1
...
εT


∼N (0,S) with
S =


D
Q
Q
...
Q


64

and initial state β1 ∼N (0,D).
Equation (1.20) can be further written as
β ∼N
 0,K−1
(1.21)
with precision matrix K = H′S−1H. (1.21) is regarded as the prior for β constructed from the
structure of transition equation (1.18). Thus all the state varibles nested in β can be drawn at a
time without forward ﬁltering and backward smoothing:
β ∼N( ¯β, ¯P−1
β )
with
¯Pβ = X′R−1X +K
¯β = ¯P−1
β
 X′R−1y

Note that K and R are both block banded and sparse matrices, therefore so is ¯Pβ.
65

D. Efﬁcient estimation of SVS-Full-TVP-VAR
Consider a standard TVP-VAR with SV:
yt = ct +A1,tyt−1 +···+Ap,tyt−p +ut
(1.22)
where yt is an n×1 vector of observed endogenous variables, ct is an n×1 vector of time varying
constants, {Ai,t}p
i=1 are n×n matrices of time varying autoregressive parameters, and ut is an n×1
vector of shocks following normal distribution ut ∼N(0,Rt) , for t = 1,...,T. The Rt with SV has
the same structure as in the section 2.3 this chapter.
Let βt = vec([ct,A1,t,...Ap,t]
′) denote the vector of time varying parameters each period of
dimension k × 1, with k = n(1+np) and vec column stacking operator. The law of motion of βt
follows random walk process
βt = βt−1 +εt
(1.23)
which is assumed that εt ∼N (0,Q).
Rewrite (1.22) into the form of
yt = Xtβt +ut
(1.24)
where Xt = In  [1,y
′
t−1,...y
′
t−p], with  Kronecker product. Then stacking (1.24) over time to
pool all the data together, one obtains
y = Xβ +u
where y =


y1
...
yT


, X =


X1
···
0
...
...
...
0
···
XT


, β =


β1
...
βT


, u =


u1
...
uT


,
with u ∼N (0,R) and R = Blkdiag

{Rt}T
t=1

.
The dynamic process of βt is also stacked into
Hβ = ε
(1.25)
66

where
H =


Iq
Iq
Iq
Iq
Iq
...
Iq
Iq


and


ε1
...
εT


∼N (0,S) with
S =


D
Q
Q
...
Q


and initial state β1 ∼N (0,D).
From (1.25), the prior of whole states is distributed as
β ∼NkT
 0,K−1
with precision matrix K = H′S−1H . The prior is derived from the random walk structure seen in
matrix H.
After above several steps of transformation, a stochastic variable selection TVP-VAR with SV
is ﬁnally expressed as
y = Xθ +u
(1.26)
θ = Γβ
with prior β ∼N
 0,K−1
and Γ = diag

{Γt}T
t=1

in which Γt = diag
 γ1,t,...,γk,t

. Apparently,
67

the large square matrix Γ contains all the indicators γj,t over identity and time corresponding to the
large column vectorβ stacking all the states βt.
The Bayesian estimation procedure is similar to the benchmark model of SVS-BVAR due to
the exclusion of Kalman forward ﬁltering and backward smoothing. Here, the steps associated
with stochastic selection are given:
1. Draw β from the posterior density
β ∼NkT( ¯β, ¯P−1
β )
with
¯Pβ = X⋆′R−1X⋆+K
¯β = ¯P−1
β
 X⋆′R−1y

and X⋆= XΓ.
2. Sample γl from the posterior density
γl ∼Bernoulli(1, ¯πl)
where l = 1,...,kT, ¯πl =
l0l
l0l+l1l , with
l0l = p(y|θ,γ−l,γl = 1)π0l
and
l0l = p(y|θ,γ−l,γl = 0)(1−π0l)
The expression p(y|θ,γ−l,γl = 1) and p(y|θ,γ−l,γl = 0) are conditional likelihood expres-
sions. Here we deﬁne θ ⋆to be equal to θ but with the lth element θl = βl in the case of
γl = 1. Similarly, we deﬁne θ ⋆⋆to be equal to θ but with the lth element θl = 0 when γl = 0.
68

Then in terms of likelihood of (1.26), we can write l0l, l1l analytically as
l0l = exp(−1
2(y−Xθ ⋆)′R−1(y−Xθ ⋆))πl
(1.27)
l1l = exp(−1
2(y−Xθ ⋆⋆)′R−1(y−Xθ ⋆⋆))(1−πl)
(1.28)
Note that the order lth is randomly picked up, which is equivalent to assigning randomness
to both identity j and time t, for j = 1,...,k and t = 1,...,T, respectively.
3. Draw Rt with stochastic volatility that contains the same blocks and steps as in Primiceri
(2005) and Del Negro and Primiceri (2013) or De Wind and Gambetti (2014).
4. Go back to setp 1 again, start the next iteration.
69

Chapter 2
A General Parsimonious Estimation of
Time-varying Vector Autoregressions
70

2.1
Introduction
Since the pioneering work of Sims (1980), the vector autoregressive (VAR) models have become
popular and widely used in economic and policy analysis. The most importance of vector au-
toregressive models is that it provides a tool to analyse the dynamic relationship among multiple
macroeconomic variables by allowing all variables in a vector to response to all variables at all
lags, as in a economy practioners and economists not only care about the intertemporal relations,
but also focus on dynamic processes among macroeconomic variables which can lead them to con-
duct economic analyses and forecasting. Simplicity, tractability and properties of VARs such as
impulse response functions and variance decompotion make the original complicated problem of
exploring inter-relationship among different macro observations straightforward. Therefore, VAR
models gradually become more and more popular and dominate the empirical analysis in macroe-
conomics. For instance, researchers typically use VARs to ﬁnd some empirical realities, then rely-
ing on these ﬁndings to establish dynamic stochastic general equilibrium models (DSGE), trying
to interpret the underlying rationales.
Econometric models are always evolutionary with the requirement to converge as close as pos-
sible to economic reality. The U.S. economy has experienced the Great Depression – the 1930s,
the Great Inﬂation - 1970s and early 1980s, the Great Moderation from middle 1980s to 2006 that
most industrialized economies also have the similar experience, and recent the Great Recession
since the global ﬁnancial crisis in 2008 and still on the way to the recovery. Except for the charac-
teristics of business cycles, the U.S. economy also has experienced four chairmanships of Federal
Reserve, Burns (1970-1978), Volker (1979-1987), Greenspan (1987-2006) and Bernanke (2006-
2014). Did they conduct the same monetary policy or not? What’s the relationship between their
policies and the alternations of the above mentioned business cycles? Besides monetary policy,
are there other factors inﬂuencing the business cycle and have these factors changed over different
economic stages? To answer such questions and issues require that the conventional assumption
of the constant coefﬁcients in VARs might be poor and have to be relaxed to be time varying. The
great decline in volatility – the property of the Great Moderation – in most macro-variables in the
71

U.S. and in most industrialized economies led to an increasing focus on appropiate modeling of
the error covariance matrix in VARs and this led to the incorporation of multivariate stochastic
volatility in many recent empirical papers. Hence, the time varying coefﬁcients on regressors and
stoachstic volatility on covariance matrix of forecast errors become the standard analysing tool
in applied macroeonomics. In model settings, from Cogley and Sargent (2001) with only time
variance on coefﬁcients on regressors, based on which, Cogley and Sargent (2005) extended the
early model to stochastic volatility on covariance matrix of forecast errors with some restrictions,
ﬁnally Primiceri (2005) developed a model with sufﬁcient ﬂexibility on all the parameters that can
be considered as the today’s standard TVP-VAR workhouse.
The time verying parameter VARs with stochastic volatility is not a parsimonious model. This
kind of model increase paramters dramatically. Suppose a VAR with n dimention and p lags and
a constant. The number of coefﬁcients on regressors follows k = n(1 + n · p) and the number of
parameters on covariance matrix of errors is m = n(n+1)
2
. When n and p increase, the total number
of paramters of this model will increase nonlinearly and causes the so called problem of parameter
proliferation, given that we usually have limited length of macro data sets. In addtion, satisfying
the recent research requirement of extending the constant parameters to time varying ones, there
will be T, the net sample periods in this model, time paths for the k +m parameters and the newly
created, associated covariance matrices of innovations to the dynamic process of above parameters.
In all, this TVP-VAR has total parameters of Kp (n, p,T) = T(k + m) + k(k+1)
2
+ n(n+1)
2
+ r(r+1)
2
where r = n(n−1)
2
. With limited sample span, over-parameterization, more or less, is inevitable,
especially in medium and large scale TVP-VAR models. This explains why in practice, small
scale models with short lags are often seen such as moetary policy analysis typically with three
variables and two lags.
Over parameterization problem in TVP-VARs with stochastic volatiliy strongly limits the num-
ber of variables and lags that can be incorporated in the model. Nevertheless, for many applica-
tions a large set of variables and more lags are necessary. In a modern economy, a large number
of variables work together and react each other. A variation of one variable will cause ﬂuctua-
72

tions of other variables and these dynamics will typically continue for a period. If some important
varibles are missing in the model, it probably will give rise to the corresponding missing of tran-
sition channels or shocks. A typical case is ‘price puzzle’ that will cause misunderstanding and
misleading in economic and policy analysis that usually seen in a small scale monetary analysis.
Banbura, Giannone, and Reichlin (2010), Carriero, Kapetanios, and Marcellino (2011) and Koop
(2013) demonstrate that a system of 15-20 variables performs better than small systems in point
forecasting and structual analysis.
As for the number of lags, for a quarterly data, Blanchard and Perotti (2002) argue that at
least 4 lags can catch the dynamic interaction of economy and ﬁscal policy. For a monthly data,
Uhlig (2005) use 13 lags to analyse effects of monetary policy shock. Alessandri and Mumtaz
(2014) investigate the effects of uncertainty shock under different ﬁnancial regimes, also use 13
lags for a monthly data set. Generally, more variables and lags can capture potentially possible
inter-reactions among different variables.
Two conﬂicts arise. The ﬁrst conﬂict is between preferred more variables and more lags and
the parameter proliferation, which becomes even stronger under time varying parameter framwork
that is often desired and required in current empirical time series analysis. As discussed above, the
problem of over parameterization is alway accompanying and become serious with the increase
of dimension of observations, number of lags and sample periods. The other conﬂict is between
estimation, computational burden and tractability. The TVP-VAR with stochastic volatility model
essentially is a combination of different state space models. The time varying parameters on regres-
sors and covariance matrix actully are state or latent variables in state equations that drive dynamic
process in measurement equations given other parameters and data. Note that the incorporation of
stochastic volatility typically involves nonlinear and non-gaussian state space blocks which abso-
lutely require high estimation skill and increase computational burden. Large data set, long lags
as well as time varying parameters make the estimation and computation much complicated and
sometimes hard to deal with.
The challenge facing the economists is how to build models or modify conventional models,
73

that are ﬂexible enough to capture the main dynamic process in the data set, but not to cause se-
rious over-parameterization, thus making the estimation tractable and result believable. There are
two branches in the literature. The ﬁrst one is on shrinkage.1 The starting point is to set priors
that impose restrictions on the parameters, say shrinking to zero. The Minnesota Prior (see Doan,
Litterman and Sims, 1984 and Litterman, 1986) probably is the most typical one among them. The
basic idea of the prior is to make the VARs shrink to random walk, with stronger shrinkage for
coefﬁcients on longer lags and across variables. Further development of this prior includes impos-
ing restrictions on sum of coefﬁcients and cointegration, see Banbura et.al. (2010). Note that the
dimension of data set n has not changed, only using some particular priors to shrink to the desired
values of parameters. The second branch is based on factor idea that has been applied to differ-
ent directions. Since large data set is desirable and available in empirical analysis, several factors
that extracted from large data set could signiﬁcally decrease the model dimension n and alleviate
over parameterization problem replying on the assumption that the bulk of dynamic inter relations
within a large data set can be explained by several common factors (Forni et al., 2002 and Stock
and Watson, 2002b). The factors that reduce the dimension and VARs that explore dynamic inter-
relationship motivate the combination of the two methods. Bernanke, Boivin and Eliasz (2005) and
Stock and Watson (2005) have combined the two models, the so called factor augmented vector
autoregressive models (FAVARs). Del Negro and Otrok (2008) and Korobilis (2013a) extended
the FAVAR models to have time varying features on parameters, hence the TVP-FAVARs. Using
also the factor idea, De Wind and Gambetti (2014) focus on the factors that drive time varying
parameters instead of those that drive large data set in factor models and factor augmented VAR
models. They ﬁnd that a q dimensional factors can sufﬁciently capture the bulk of time variation in
the k dimensional latent parameters in the full rank TVP-VAR model and sometimes q ≪k. That
is the covariance matrix of the innovations to the time varying parameters is reduced rank rather
than full rank. Simply speaking, this model tranforms the original k sources of variations, the same
dimentison as the number of time varying parameters, to q sources, the number of factors. Kim
1Using shrinkage prior in large Bayesian VAR will be discussed in Chapter 3.
74

and Yamamoto (2012) proposed a new approach to apply the factor idea. They argue that since
more lags sometimes are necessary to capture the dynamic process among variables, especially in
monthly data, recent lags could be qualiﬁed to drive longer lags due to that current variables are
more affected by recent lags in a standard VAR framwork. That is the constant vector and several
coefﬁcient matrices on the recent lags are the dynamic sources driving the variation of the stan-
dard TVP-VAR model. Factor idea can also be used in the time variation, namely the stochastic
volatility, on covariance matrix of forecast errors in VAR. Carriero, Clark and Marcellino (2012)
proposed a computationally effective way to model stochastic volatility to greatly speed up com-
putations for smaller VAR models and make estimation tractable for larger models. The newly
presented method links to the observation that the pattern of estimatied volatilities in empirical
analysis is often very similar across variables. They used a common unobserved factor – com-
mon volatility – to drive the individual volatility in standard TVP model with stachastic volatility.
They ﬁnd that common volatility model signiﬁcantly improves model ﬁt and forecasting accuracy
compared to constant volatility. Alessandri and Mumtaz (2014) by a VAR model and Mumtaz
and Theodoridis (2014) via a dynamic factor model also use common volatility to study effects of
‘endogenous’ uncertainty (generated from the model itself) on the U.S. economy.
As we discussed above, TVP-VAR with SV is not a parsimonious model which almost always
company over parameterization concern that could lead to misunderstanding and misleading in
economic inferences. A natural solution to this estimation is to make the model parsimonious and
tractable, meanwhile to be able to catch main dynamic characteristics among these variables.
In this chapter we propose a general parsimonious estimation method based on factor idea
that collects all the advantages of the above mentioned models. This model has the ability to
solve all the possible sources of parameter proliferation such as number of lags, dimension of
latent coefﬁcients on regressors and complicated stochastic volatility, and therefore can reduce
the computation burden, making the estimation tractable whatever small or large models. That’s
why we call it ‘general parsimonious estimation’. This model is also ﬂexible enough that could
pay different attention to different sources of possible over parameterization. For instance, if the
75

number of lags is small enough and suitable for some economic analysis, the model only need to
focus on the dimension of latent coefﬁcients and stochastic volatility.
Three papers are relevant to our method. Kim and Yamamoto (2012) is only on reduced lags
without possible factors driving latent coefﬁcients and stochastic volatility; De Wind and Gam-
betti (2014) applies latent factor both on latent coefﬁcients and stochastic volatiliy in estimation
procedure, but the complication of estimation of stochastic volatiliy still there; Carriero, Clark and
Marcellino (2012) use a latent factor as common volatility under constant coefﬁcients.
We apply the ‘general parsimonious estimation’ presented in this chapter to small scale mon-
etary VAR. Speciﬁcally, we increase the number of lags from one to four to create possible over
parameterization enviroment. We implement principal component analysis on the covariance ma-
trix of innovations to latent coefﬁcients. The results show that even though with only one lag, the
VAR model is still not parsimonious; as the number of lags increases, the problem of over ﬁtting
become serious and several factors are enough to capture the amount of variation that is present in
full rank model. The common volatility from the model with one lag to the model with four lags
works very well that coincides with the feature of U.S. business cycle. We also checked economic
agents’ reaction to monetary policy shocks under the parsimonious estimation and ﬁnd that there
are no signiﬁcant changes in the responses to non-systematic monetary policy in line with Prim-
iceri (2005) and Sims and Zha (2006). These evidences suggest that parsimonious estiamtion is
not only good at alleviating over ﬁtting, but also suitable for structural analysis.
The Chapter 2 is organized as follows. In section 2, we present the model speciﬁcation that
is based on factor idea to conduct a parsimonious estimation. In section 3, Bayesian estiamtion
procedure is given step by step and we also point out that the free combination of different blocks in
estimation is equivalent to the simple verison of the presented general model seen in the literature.
Comparison with conventional model or estimation is also conducted to demonstrate the advantage
of reducing over-parameterization and at the same time also increase the estimation efﬁciency.
Section 4 in an empirical analysis, gives strong evidences of over-ﬁtting even in a small scale
TVP-VAR and evaluates the performance of the factor driven model under the setting when the
76

extent of over parameterization becomes more and more serious. Finally, Section 5 conludes.
2.2
Model speciﬁcation
In this section, We present the general model that imposes all the possible factors that drive lags,
latent coefﬁcients and stochastic volatility. Of course, when it applies to an empirical analysis,
appropriate settings should be chosen by researchers depending on the sepciﬁc objects.
Suppose yt is an n×1 vector that follows VAR(p) process
yt = ct +B1,tyt−1 +···+Bp,tyt−p +ut
(2.1)
where ct is an n×1 time varying constant, {Bi,t}p
i=1 are n×n matrices of time varying autoregres-
sive parameters, and ut is an n × 1 vector of forecast errors. The errors are assumed to indepen-
dently and identically follow normal distribution ut ∼N (0,Σt).
Let’s ﬁrst look at the stochastic volatility part of Σt. Following Carriero et. al. (2012), we
assume that
Σt = A−1DtA−1′
(2.2)
where A is a lower triangluar matrix with values of ones on the main diagonal. The volatility
process is deﬁned as
Dt = λtS
(2.3)
S = diag
 [1,s2,...,sn]′
(2.4)
log(λt) = F ·logλt−1 +ηt
(2.5)
where ηt ∼iidN (0,Qh). h = log(λt) follows AR(1) process that is common to all variables and
drives the time variation in the entire covariance matrix of the VAR errors. The ﬁrst element
of diagonal variance matrix S – the loading – is normalized to one for identiﬁcation of common
77

volatility λt.
Next we can rewrite equation (2.1) using the above volatility structure in the form of
yt = BtXt +λ
1
2t A
−1S
1
2εt
(2.6)
where Bt = [ct,B1,t,...,Bp,t] and Xt =

1,y′
t−1,...,y′
t−p
′
. Following Kim and Yamamoto (2012),
the time varying coefﬁcients on regressors can be decomposed into
Bt = B+ ¯BtG
(2.7)
where ¯Bt and G are n×r and r×(np+1) respectively. For the model to be properly identiﬁed, we
assume that
¯B1 = 0 and G = [Ir G1]
(2.8)
we as usual assume that bt = vec

¯B
′
t

follows random walk
bt = bt−1 +ubt
(2.9)
where ubt ∼iidN (0,Qb). Note that from ¯BtG = [ ¯Bt ¯BtG1], ¯Bt should have the similar time variation
to the elements in the ﬁrst r columns of Bt matrix of the unrestricted model.
Finally, we further assume that the nr×1 time varying paramter bt is driven by q factors where
q ≤nr (De Wind and Gambetti, 2014). This means that the covariance matrix Qb is of less than full
rank. Decompose the covariance matrix Qb = ΛbΛ
′
b where Λb is a nr ×q matrix as factor loadings
implying that rank(Qb) = q. The transition equation (2.9) can be written as
bt = bt−1 +Λbυt
(2.10)
where correspondingly υt is a q×1 shocks that follows υt ∼N
 0,Iq

. The above equation (2.10)
implies that △bt is on the column space of Λb but bt is not necessarily in the column space of
78

Λb. In other words, changes in time varying parameters are driven by factors while levels are not
necessary since there are more forces determining the economy than forces changing the economy.
Thus the law of motion of (2.10) can be further written as
bt = Pbbt +Mbbt
(2.11)
where Pb = Λb

Λ
′
bΛb
−1
Λ
′
b is a projection matrix onto the column space of Λb that contains the
changing part of the bt; Mb = Inr −Pb is the projection matrix onto the left null space of Λb that
sizes the time invariant part of bt. Deﬁning ˜bt =

Λ
′
bΛb
−1
Λ
′
bbt as driving factors, then
bt = Λb˜bt +Mbb0
(2.12)
and the law of motion in terms of the underlying factors follows
˜bt = ˜bt−1 +υt
(2.13)
via premultiplying equation (2.10) by

Λ
′
bΛb
−1
Λ
′
b both sides.
The model speciﬁcation consists of three parts. Equations from (2.1) to (2.5) describe a com-
mon factor that drive the volatilities of all the variables in yt. This factor application is due to two
reasons. One is from the observation that the pattern of estimated volatilities in empirical analysis
is often very similar across variables. As discussed above, the U.S. economy has experienced two
episodes of the ‘Great Inﬂation’ and the ‘Great Moderation’, respectively. In the former period,
most macroeconomic variables had very high volatility; while in the latter period, modest volatiliy
was shared by most macro-variables. The other is that it greatly reduces the computational budern
in which nonlinear and nongaussian state space involoved in full stochastic volatility part has to
be transformed to linear gaussian state space via seven (Kim, Shephard and Chib, 1998) or ten
(Omori, Chib, Shephard and Nakajima, 2007) mixture normals. In addition, the length of history
of common volatility is always ﬁxed at T, independent of n, while the full volatility is determined
79

by both T and n, i.e. n(n+1)
2
·T.
Equations (2.6) - (2.9) give the factor idea of using early lags to drive whole lag coefﬁcients.
Many empirical analysis, especially in ﬁeld of forecasting, have evidenced that recent lags are more
relevant than long lags affecting current variables which justﬁes our model setting. The last four
equations (2.10) - (2.13) present another interpretation of dynamic process of latent time varying
coefﬁcients that only limited factors drive the the bulk of variation in time varying coefﬁcients. De
Wind and Gambetti (2014) give the emipical evidence and theoretical support for this setting.
One thing worth mention is that the three portions of the general-setting model is not neces-
sarily connected together. They can be freely combined or independently exist with respect to
the research object ones are conducting. For instance, if relaxing one of factor restrictions, the
corresponding part becomes the standard setting. This can be seen clearly in Bayesian estimation
procedure in the next section.
2.3
Bayesian estimation procedure
In this section, we describe the Bayesian estimation procedure step by step. In each step or, pre-
cisely speaking, in each block, after some appropriate transformation, the estimation ﬁnally re-
duces to standard Bayesian estimation such as linear regression models and state space models
either linear or nonlinear.
2.3.1
Draw a history of {bt}T
t=1
We need some transformation to construct a state space model for bt. Substituting the decomposi-
tion equation (2.7) into measurement equation (2.6), one obtains
yt = BXt + ¯BtGXt +ut
(2.14)
80

It can be further written as via column stacking operator both sides
y⋆
t = Zb,tbt +ut
(2.15)
where y⋆
t = yt −BXt, Zb,t =

In (GXt)
′
, bt = vec

¯B
′
t

and ut ∼N (0,Σt);  denotes Kronecker
product and vec is column stacking operator.2The composition of bt = Λb˜bt + Mbb0 in equation
(2.12) then is plugged into (2.15) to obtain
y⋆
t = Zb,tΛb˜bt +Zb,tMbb0 +ut
(2.16)
Clearly, We ﬁnd that drawing the history of bt is divided into three steps: i) draws of ˜bt, ii) draws
of Mbb0 and iii) draws of bt based on i) and ii).
2.3.1.1
Draw a history of
˜bt
	T
t=1
The state space model for ˜bt based on equation (2.16) is orgonized as
y⋆⋆
t
= Z⋆
b,t ˜bt +ut
(2.17)
with the law of motion in (2.13)
˜bt = ˜bt−1 +υt
where y⋆⋆
t = y⋆
t −Zb,tMbb0, Z⋆
b,t = Zb,tΛb, ut ∼N (0,Σt) and υt ∼N
 0,Iq

due to the decomposition
of Qb. Since the above two equations form the standard linear and gaussian state space model,
posterior draws of {bt}T
t=1 can be sampled by the algorithm of Carter and Kohn (1994).
Standard Kalman ﬁlter and a smoother apply to the linear and gaussian state space model for
˜bt. We herein give the basic description. The ﬁlter goes forward until T and obtain a draw from
˜bT ∼N
 ˜bT|T,VT|T

in the last period; Then based on the draw of ˜bT as an obervation, the ﬁlter
goes backward into a smooth process until the ﬁrst period. That is ˜bt ∼N
 ˜bt|t+1,Vt|t+1

based
2vec(ABC) =

AC
′
vec

B
′
81

on previous draw as a new observation successively for t = T −1,...,1. The forward recursive
formulae are given by
Vt|t−1 = Vt−1|t−1 +Iq
Kt = Vt|t−1Z⋆′
b,t

Z⋆
b,tVt|t−1Z⋆′
b,t +Σt
−1
˜bt|t = ˜bt−1|t−1 +Kt
 y⋆⋆
t −Z⋆
b,t ˜bt−1|t−1

Vt|t = Vt|t−1 −Kt
 Z⋆
b,tVt|t−1

where the notation x|t is used to condition on the information set up to and including time t. Note
that the initialization of the recursion follows from the prior distribution on b0|0 ∼N (b0,V0). By the
deﬁnition of ˜bt = Rbbt where Rb =

Λ
′
bΛb
−1
Λ
′
b, then prior distribution becomes ˜b0|0 ∼N
 ˜b0, ˜V0

where correspondingly ˜b0 = Rbb0 and ˜V0 = RbV0R
′
b. The backward recursion, namely the smoother
has
˜bt|t+1 = ˜bt|t +Vt|tV −1
t|t+1
 ˜bt+1 −˜bt|t

Vt|t+1 = Vt|t −Vt|tV −1
t|t+1Vt|t
Finally, ˜bt premultiplying by Λb obtain the time varying part of bt following equation (2.12).
2.3.1.2
Draw Mbb0 as a whole
After drawing of time varying part, now we consider the time invariant part of bt. Equation (2.16)
can be organized as
y⋆⋆
t
= Zb,tδ +ut
(2.18)
where y⋆⋆
t
= y⋆
t −Zb,tΛb˜bt and δ = Mbb0. The above equation can be interpreted as a restricted
linear regression, as the vector coefﬁcient δ from its deﬁnition is on the column space of Mb
82

naturally with the restriction Pbδ = 0 or equivalently Rbδ = 0.
Stacking (2.18) with the above restriction, it displays as
y⋆⋆= Zbδ +u
(2.19)
Rbδ = 0
where y⋆⋆=
h
y
′
1,...,y
′
T
i′
, Zb =
h
Z
′
b,1,...,Z
′
b,T
i′
, u =
h
u
′
1,...,u
′
T
i′
as well as u ∼N (0,Σ⋆) with
Σ⋆= Blkdiag([Σ1,...,ΣT]) in which covariance matrix each period is placed on the main diagonal.
Details on the estimation of the restricted linear regression is delegated to appendix in this chapter.
The basic idea to dealing with the Bayesian estimation with restrctions on the parameters is to
think of the restrictons as another prior information and incoporate it into the posterior. Since
b0|0 ∼N (b0,V0), the prior for δ becomes δ ∼N (δ,V δ) where δ = Mbb0 and V δ = MbV0M
′
b. The
posterior of δ also follows normal distribution
δ ∼N
  ¯δ, ¯Vδ

with
¯δ =

Inr −˜VδR
′
b

Rb ˜VδR
′
b
−1
Rb

˜δ
¯Vδ =

Inr −˜VδR
′
b
 Rδ ˜VδRb
−1 Rb

˜Vδ
where the posterior mean of ˜δ and posterior variance of ˜Vδ are from the standard Bayesian estima-
tion of linear unrestricted regression
˜δ =

Z
′
bΣ⋆−1Zb +V −1
δ
−1
˜Vδ = ˜δ

Z
′
bΣ⋆−1y⋆⋆+V −1
δ δ

83

2.3.1.3
Draw a history of {bt}T
t=1
The ﬁnal step is straightforward to sum the time varying part and time invariant part together, i.e.
bt = Λb˜bt + Mbb0 which yields bt draw for t = 1,...,T. This completes the drawing from the
posterior distribution of {bt}T
t=1 via a two separate gibbs sampling.
2.3.2
Draw reduced rank covariance matrix Qb
The posterior distribution of Qb only depends on the history of all bts. Sampling is based on the
following state equation (2.9)
bt = bt−1 +ub,t
The posterior distribution of Qb follows inverse wishart distribution given the prior distribution of
the same type
Qb ∼SIW
  ¯Qb, ¯ν

with
¯Qb =
T
∑
t=2
(bt −bt−1)(bt −bt−1)
′ +Qb
¯ν = ν +T −1
where Qb and ν are scale matrix and degree of freedom respectively for prior inverse wishart
distribution of reduced rank Qb.
2.3.3
Draw constant matrix B
Go back to equation (2.14), place unrelated term to the left hand side and obtain the linear regres-
sion associated with matrix B
y⋆
t = BXt +ut
(2.20)
84

where y⋆
t = yt −¯BtGXt and ut ∼N (0,Σt) with ut = λ
1
2t A−1S
1
2εt. Transpose the above equation and
divide by √λt both sides, we obtain
y⋆′
t /
p
λt =

Xt/
p
λt

B
′ +u
′
t/
p
λt
(2.21)
Stacking the above equation row by row
Y ⋆= XB
′ +U
(2.22)
then stacking the equation column by column, we obtain the ﬁnal equation form for our Bayesian
estimation
y⋆= Ξb+u
(2.23)
where Ξ = In X, b = vec

B
′
and u = vec(U) with u ∼N (0,ΣIT).3
The aim of the above steps is to transform the heterogeneous linear regression to homogeneous
linear regression model. With respective to (2.23), standard normal posterior distribution of b can
be found with also the normal prior distribution
b ∼N
 ¯b, ¯Vb

with
¯b = ¯Vb

Ξ
′ (ΣIT)−1 y⋆+V −1
b b

¯Vb =

Ξ
′ (ΣIT)−1 Ξ+V −1
b
−1
where prior follows b ∼N (b,V b). Finally transform column vector b back to matrix B.
3vec(ABC) =

C
′ A

vec(B)
85

2.3.4
Draw constant matrix G1
We still focus on equation (2.14). Place the term BXt to the left hand side based on previous draw
of B and obtain
y⋆
t = ¯BtX1,t + ¯BtG1X2,t +ut
where y⋆
t = yt −BXt and the decomposition of Xt into X1,t and X2,t is due to the structure G = [Ir G1].
We further get the linear regression associated with G1by column stacking operator towards the
above equation both sides
y⋆⋆
t
= Wtg1 +ut
(2.24)
where y⋆⋆
t
= y⋆
t −¯BtX1,t, Wt =

¯Bt X
′
2,t

, g1 = vec

G
′
1

and ut ∼N (0,Σt). The same as (2.21),
heteroscedasticity can be removed through dividing both sides of (2.24) by √λt. Here we skip this
step, directly transform the (2.24) into a large matrix form
y⋆⋆= Wg1 +u
(2.25)
where y⋆⋆=
h
y⋆⋆′
1 ,...,y⋆⋆′
T
i′
, W =
h
W
′
1,...,W
′
T
i′
, ut =
h
u
′
1,...,u
′
T
i′
and u ∼N (0,Σ⋆) with Σ⋆=
Blkdiag([Σ1,...,ΣT]).
The posterior of g1 follows normal with variance and mean given by ¯Vg1 and ¯g1 respectively
g1 ∼N ( ¯g1, ¯Vg1)
¯g1 = ¯Vg1

W
′Σ⋆−1y⋆⋆+V −1
g1 g1

¯Vg1 =

W
′Σ⋆−1W +V −1
g1
−1
and prior follows g1 ∼N

g1,V g1

.
86

2.3.5
Draw structural impact matrix A
To draw impact matrix A, we concentrate on equation (2.6). Take unrelated term to the left hand
side, obtaining
Aˆyt = λ
1
2t S
1
2εt
(2.26)
where ˆyt = yt −BtXt. Since the recursive structure of A – lower triangular matrix with value of
ones on the main diagonal, we can estimate (2.26) individually
ˆyi,t = −ˆy
′
−i,tαi +λ
1
2t s
1
2
i εt
(2.27)
for i = 2,...,n and for t = 1,...,T. ˆy−i,t collect elements from ˆy1,t to ˆyi−1,t and αi nest corre-
sponding row elements in matrix A. To estimate the above equation, divide by √λtst both sides to
remove the error heteroscedasticity
ˆyi,t/
p
λtst =

−y−i,t/
p
λtst

αi +εt
Stack them row by row to obtain
ˆyi = X−iαi +ε
(2.28)
where ε ∼N (0,IT).
The posterior of αi follows normal with mean and variance
αi ∼N ( ¯αi, ¯Vα)
with
¯αi = ¯Vα

X
′
−i ˆyi +V −1
α αi

¯Vα =

X
′
−iX−i +V −1
α
−1
and prior αi ∼N (αi,V α).
87

2.3.6
Draw diagonal elements of S
Using equation (2.27), divide both sides by √λt and obtain
ˆyi,t/
p
λt =

−y−i,t/
p
λt

α +s
1
2
i εt
again stack row by row
ˆyi = X−iαi +ε
(2.29)
where ε ∼N (0,siIT) for i = 2,...,n.4
Given the prior si ∼IG(a,b) where IG denotes inverse gamma distribution, the posterior fol-
lows
si ∼IG
 ¯a, ¯b

with
¯a = a+ T
2
¯b = b+ ε
′ε
2
Note that the step 5 of drawing αi can be merged into step 6 only focusing on equation (2.29).
2.3.7
Draw common stochastic volatility {λt}T
t=1
Unlike bt, the latent factor λt is not in a linear and gaussian state space model given other pa-
rameters and hyperparamters. Thus typical way of multiple drawing of latent variables of Carter
and Kohn (1994) algorithm is no longer suitable for λt. Following Jacquier et.al. (1994), single
drawing date by date is used for the nonlinear model. Carlin et.al (1992) show that conditional
distribution of state variables in a general state space model can be written as product of three
4i starting from 2 is for the identiﬁcation of common (latent factor) volatility. See section 2 on the model speciﬁ-
cation.
88

terms
f
 ht|Θ,yT
∝f (ht|ht−1)· f (ht+1|ht)· f (yt|ht,Θ)
(2.30)
which is the starting point of sampling of the latent factor ht = log(λt). The ﬁrst two terms on
the right hand side can be further written as f (ht|ht+t,ht−1) ∝f (ht|ht−1) · f (ht+1|ht).5 Hence
conditional posterior of ht is now a product of two terms
f
 ht|Θ,yT
∝f (ht|ht−1,ht+1)· f (yt|ht,Θ)
(2.31)
That is the target distribution from which draws of ht come.
Nonlinearity of the state space model on measurement equation makes the posterior target does
not have an analytical form, therefore metropolis algorithm is required. We choose f (ht|ht−1,ht+1)
as proposal since it is part of target distribution with the same support and the most importance
is that the proposal is analytical due to that the law of motion of the common factor is linear
and gaussian. The basic idea is that one can regard ht as paramters to be estimated, ht+1 as the
obserbation driven by ht and ht−1 as prior information about ht. This can be seen explicitly in
successive two periods
ht = F ·ht−1 +ηt
ht+1 = F ·ht +ηt+1
from the ﬁrst equation one can set prior for ht ∼N (F ·ht−1,Qh) while in the second equation ht
is coefﬁcient to be estimated and ht+1is an observation. From the standard Bayesian estiamtion of
linear regression model, one is familiar with
• For t = 1,...,T −1,
ht ∼N (Fht−1,Qh)
5 f (ht|ht−1,ht+1) = f(ht−1,ht,ht+1)
f(ht−1,ht+1) = f(ht+1|ht,ht−1)·f(ht|ht+1)
f(ht+1|ht−1)
∝f (ht+1|ht)· f (ht|ht−1)
89

ht|ht−1,ht+1 ∼N (uh,Qh)
(2.32)
with
uh = Vh

F
′Q−1
h ht+1 +Q−1
h Fht−1

Vh =
 F′Q−1
h F +Q−1
h
−1
• For t = 0,
h0 ∼N (uh,V h)
since
h1 = F ·h0 +η1
then
h0 ∼N ( ¯uh, ¯Vh)
(2.33)
¯uh = ¯Vh

F
′Q−1
h h1 +V −1
h uh

¯Vh =

F
′Q−1
h F +V −1
h
−1
• For t = T,
hT|hT−1 ∼N
 uT,h,VT,h

(2.34)
uT,h = VT,h
 Q−1
h FhT−1

VT,h =

F
′Q−1
h F +Q−1
h
−1
With the above proposals at hand (2.32) - (2.34), the date by date independence metropolis is
implemented as follows
1. When t = 0, draw h0 from (2.33) given prior for h0.
2. When t = 1,...,T −1, a) draw a candidate h⋆
t from (2.32); b) compute the acceptance ratio
90

(probability) r = min

f(yt|h⋆t ,Ξ)
f(yt|hold
t
,Ξ), 1

where f (yt|ht,Ξ) is the likelihood of the observation
t; c) for each h⋆
t , draw a value u from the Uniform (0,1) distribution. If u ≤r, accept h⋆
t as
hnew
t
. Otherwise, still keep hold
t
in period t.
3. When t = T, a) draw a candidate h⋆
T from (2.34); b) compute the acceptance ratio (probabil-
ity) r = min

f(yT |h⋆
T ,Ξ)
f(yT |hold
T ,Ξ), 1

where f (yT|hT,Ξ) is the likelihood of the observation T; c)
for each h⋆
T, draw a value u from the Uniform (0,1) distribution. If u ≤r, accept h⋆
T as hnew
T
.
Otherwise, still keep hold
T
in period T.
4. Repeat step 1 to step 3 date by date in each iteration.
We complete the drawing of ht from t = 1 to t = T. Note that though there is no explicit backward
smoother procedure as in the algorithm of Carter and Kohn (1994) for linear gaussian model, the
draws of ht are stll based on the information set of all the observations as they directly come from
posterior condition f
 h1,...,hT|yT,Ξ

instead of two seperate steps of forward Kalman ﬁlter and
a backforward smoother with the analytical expression in linear gaussian model.
2.3.8
Draw coefﬁcient F
Give the previous draws of {ht}T
t=1with equation (2.5)
ht = F ·ht−1 +ηt
where ηt ∼N (0,Qh). By typical column stacking implementation
yh = F ·xh +η
(2.35)
the posterior of F has
F ∼N
 ¯uF, ¯QF

91

with
¯uF = ¯QF

Q−1
h x
′
hxh +Q−1
F uF

¯QF =

Q−1
h x
′
hyh +Q−1
F
−1
where prior F ∼N

uF,QF

.
2.3.9
Draw variance Qh
Still focusing on equation (2.35) with prior distribution of inverse gamma Qh ∼IG(c,d), the pos-
terior becomes
Qh ∼IG
 ¯c, ¯d

with
¯c = c+ T
2
¯d = d + η
′η
2
The above nine steps or nine blocks consist of one iteration in Bayesian estimation via Markov
chain Monte Carlo (MCMC). The MCMC simulation involves Gibbs sampling and Metropolis
Hasting algorithm. After discarding some burn-in iterations, the draw of parameters and hyper-
parameters from each conditional posterior is equivalent to one from joint posterior from which
bayesain inference can be conducted.
The characteristic of the model is that we use factor idea on lags, coefﬁcients and volatility
to make the TVP-VAR model parsimonious. We use ﬁrst several lags to drive long lags that are
identiﬁed by G matrix, see (2.8); We use a latent factor, namely the common volatility to represent
the volatilities of the whole observations that is identiﬁed by S matrix, see (2.4 ) and ﬁnally several
factors could drive the latent coefﬁcients by decomposing covariance matrix Qb into reduced rank
that can be seen in (2.10 ) and (2.11). The difference of our model with conventional factor models
or factor augmented models is that we focus on reducing dimension of parameters rather than on
92

dimension of observations in a rich data set.
In the next section, we apply this model to monetary policy analysis to evaluate the model
performance and make some inferences.
2.4
Empirical analysis
In this section, we ﬁrst analyse whether there are enough evidences for factor driving of the dy-
namic process of the time varying coefﬁcients in the TVP-VAR model. We apply the general
factor-driven model to a typical small scale monetary VAR. We however still ﬁnd strong evidences
in supporting factor driving. Then we turn to structrual analysis on the agents’ response to mone-
tary policy shock. No signiﬁcant difffences are found.
The same data is used as in Chapter 1. It contains three variables, namely inﬂation rate, unem-
ployment rate and short rate of 3-month treasury bill rate which cover the period from 1953 Q1 to
2006 Q3 before the 2008 ﬁnancial crisis after that unconventional monetary policy was conducted.
The reason we choose this data set is threefold. First, it is a good description of workings of the
economy in real activity, nominal variable and monetary policy that explicitly correspond to IS
curve, Philips Curve and policy rule in a typical small scale DSGE model. Thus this data set is
widely used in policy and business cycle analysis and also a subset of medium or large scale data
set for such analysis; Second, due to the well known problem of parameter proliferation in TVP-
VAR with or without SV, researchers usually tend to use such kind of small data set to weeken this
concern in order to make their ﬁndings or/and conclusions belivable. Based on this arrangement,
we try to ask is there still over-parameterization in this small scale model they specially choose and
if it is, is it strong enough; Lastly, since the data is widely used, it is convenient for us to conduct
comparison with extant literature.
93

2.4.1
Are factors important in time varying parameters?
In this subsetion, we test whether there is enough evidence supporting over ﬁtting in the small
scale TVP-VAR. That is the precondition and starting point for our factor-based model. The test
is based on two models that all impose no factor restriction on time varying coefﬁcients on re-
gressors, namely let the coefﬁcients freely ﬂuctuate. The only difference is that one with full
stochastic volatility (Primiceri, 2005) and the other common volatility (Carriero et al., 2012). In-
cluding stochastic volatility in the model is because shutting off the volatility channel probably
will cause misunderstanding of the dynamic process.6 If the test is implemented in a model that
can potentially cause the amount of time variation originating from the part of volatiliy incorrectly
transferring to the part of time varying coefﬁcients on regressors – that is the time varying coef-
ﬁcients now have more variation than they should have, even though we ﬁnd evidence on over
parameterization, the result is still questionable. Testing results from both models with SV are
very similar, here we only present the results from the model with common volatility for the save
of space.7
The TVP-VAR with common volatility (TVP-VAR-CV for short hereafter) is implemented
with one lag to four lags with common volatility respectively. We conduct pincipal component
analysis of the covariance matrix of innovations to time varying coefﬁcients Qb in each model.
Table 2.1 and Table 2.2 give the contribution of each factor for each model with lag from one to
four. They are given in terms of mean and median in percentage in descending order. Some ﬁgures
do not list in the tables because contributions are already too small.
In the model with only 1 lag, it has only 12 time varying coefﬁcients each period and therefore
the most less over-ﬁtting model. Principal component analysis shows that factor driving is still very
obvious and can not be ignored. From Table 2.1 under the Lag 1 title, we ﬁnd that the ﬁrst factor
6Consider a AR(1) process yt = ρyt−1+ut where ut ∼N
 0,σ2
. The unconditional variance of yt is var(yt) =
σ2
1−ρ2
. If the volatility σt is increasing with time, but the model is estimated on time varying coefﬁcient ρt with constant σ,
we will ﬁnd the persistent ρt of the model is increasing that contradicts the reality.
7The empirical test on the TVP-VAR with full SV is also conducted. The results are almost exactly the same as
common SV which conﬁrms the ﬁnding in Carrero et al. (2012) that for the typical U.S. data set, common SV is a
good representation for full SV.
94

TVP-VAR-CV
Lag 1
Lag 2
Principal component
Mean %
Median %
Mean %
Median %
1
63.8684
63.5164
96.0390
96.1510
2
18.2649
18.9576
1.6741
1.0047
3
11.4011
11.0741
0.5968
0.7234
4
3.0299
2.9678
0.4953
0.4322
5
1.2880
1.3223
0.3128
0.2725
6
0.9465
0.9347
0.2257
0.2507
7
0.5503
0.5536
0.1662
0.2011
8
0.3539
0.3742
0.1353
0.1691
9
0.2276
0.2251
0.0903
0.1464
10
0.0463
0.0457
11
0.0146
0.0213
12
0.0085
0.0071
Table 2.1: Contribution for each factor via principal component analysis on covariance matrix Qb
for TVP-VAR-CV from lag one to lag two
TVP-VAR-CV
Lag 3
Lag 4
Principal component
Mean %
Median %
Mean %
Median %
1
95.1109
93.5693
93.7272
91.1517
2
1.8860
1.7230
3.1857
2.3241
3
0.8257
1.0010
0.9423
1.1121
4
0.5718
0.5718
0.4749
1.0458
5
0.3397
0.5330
0.4012
0.6961
6
0.2658
0.4804
0.2641
0.6427
7
0.1683
0.3620
0.2270
0.4846
8
0.1612
0.3288
9
10
11
0.0712
0.2264
12
0.0728
0.1361
Table 2.2: Contribution for each factor via principal component analysis on covariance matrix Qb
for TVP-VAR-CV from lag three to lag four
95

2
4
6
8
10
12
65
70
75
80
85
90
95
100
cumulative percentage
 
 
mean
median
2
4
6
8
10
12
10
20
30
40
50
60
contribution for each factor
 
 
mean
median
Figure 2.1: Contribution and cumulative contribution for lag 1
Notes: This is TVP-VAR-CV with 1 lag and 12 coefﬁcients on regressors each period. The left
panel plots cumulative contribution for all the factors via principal component analysis of covari-
ance matrix Qb. The right panel plots contribution for each factor in descending order via principal
component analysis of covariance matrix Qb. The solid line represents posterior mean, while
dashed line posterior median.
96

5
10
15
20
96.5
97
97.5
98
98.5
99
99.5
cumulative percentage
 
 
mean
median
5
10
15
20
10
20
30
40
50
60
70
80
90
contribution for each factor
 
 
mean
median
Figure 2.2: Contribution and cumulative contribution for lag 2
Notes: This is TVP-VAR-CV with 2 lag and 21 coefﬁcients on regressors each period. The left
panel plots cumulative contribution for all the factors via principal component analysis of covari-
ance matrix Qb. The right panel plots contribution for each factor in descending order via principal
component analysis of covariance matrix Qb. The solid line represents posterior mean, while
dashed line posterior median.
97

5
10
15
20
25
30
94
95
96
97
98
99
100
cumulative percentage
 
 
mean
median
5
10
15
20
25
30
10
20
30
40
50
60
70
80
90
contribution for each factor
 
 
mean
median
Figure 2.3: Contribution and cumulative contribution for lag 3
Notes: This is TVP-VAR-CV with 3 lag and 30 coefﬁcients on regressors each period. The left
panel plots cumulative contribution for all the factors via principal component analysis of covari-
ance matrix Qb. The right panel plots contribution for each factor in descending order via principal
component analysis of covariance matrix Qb. The solid line represents posterior mean, while
dashed line posterior median.
accounts for 64% contribution of the variation of all coefﬁcients. The second takes 18% much less
than the ﬁrst and the third 11% with a moderate amount of interpretation. This dynamic process
can be found in Fig 2.1 in which the left panel gives the cumulative contribution and the right panel
shows the contribution for each factor where solid line denotes mean and dashed line median. It
is easy to observe that the ﬁrst three factors, all above 10%, together account for near 95% and
the ﬁrst six factors together almost 100%. After the sixth factor, all the remaining factors with
very little explanation can be ignored which is evidenced by the right panel of Fig 2.1. Generally
speaking, the samll scale model with the shortest lag indicates that it is still very over ﬁtting, at
least half of the principals should be discarded.
98

10
20
30
92
93
94
95
96
97
98
99
100
cumulative percentage
 
 
mean
median
10
20
30
10
20
30
40
50
60
70
80
90
contribution for each factor
 
 
mean
median
Figure 2.4: Contribution and cumulative contribution for lag 4
Notes: This is TVP-VAR-CV with 4 lag and 39 coefﬁcients on regressors each period. The left
panel plots cumulative contribution for all the factors via principal component analysis of covari-
ance matrix Qb. The right panel plots contribution for each factor in descending order via principal
component analysis of covariance matrix Qb. The solid line represents posterior mean, while
dashed line posterior median.
99

Let’s go to two lags. The two columns under the title of Lag 2 strongly suggest that, in Table2.1,
the ﬁrst factor is large enough with 96% to interpret almost all the variation in the part of time
varying coefﬁcients, while the second factor, in contrast to the model of one lag, dramatically
decline to less than 2%. This can be evidenced by the right panel of Fig 2.2. After the second
factor, all the remaining factors have contributions close to zero. When the model is set to three
lags, the result is very similar that the ﬁrst accounts for more than 95%, the second less than 2%
and from the third onward, all factors are very near zero, see Fig 2.3 intuitively. The model of
four lags is also the same case no matter in mean or median even though the number of factor
candidates raising to 39, which can be directly observed in Fig 2.4. Models of two, three and four
lags share the same property that only the ﬁrst factor account for most and two or three together
for almost all the variation, while the model with one lag needs more factors where the tangent of
cumulative percentage curve in Fig 2.1 is less steeper than others in Fig 2.2 to Fig 2.4.
At least three ﬁndings and implications can be derived. The ﬁrst, there are strong evidence of
factor driving of dynamic process in time varying coefﬁcients no matter in the model of one lag or
of four lags. This indicates that in a typical model settings of TVP-VARs, the number of dynamic
sources, also the same number of coefﬁcients should be dramatically reduced.
The second, following the ﬁrst, it further conﬁrms researchers’ concern that TVP-VAR is not
a parsimonious model even it is set in a small scale with short lags which are in line with Cogley
and Sargent (2005) and De Wind and Gambetti (2014). Small scale TVP-VAR models still need
factor-driven estimation.
The third, we ﬁnd signiﬁcant difference in the style of factor driving in model of one lag and
model of more lags. In the context of the typical three observations, one-lag model needs more
factors than more-lag models and with the increase of the number of lags, the style of (almost)
one factor leading is consistent from two-lag model to four-lag model.8 The cosistency of one
factor driving means that the sources of variation in the model have been already fully identiﬁed,
implying that more than enough lags will cause serious over ﬁtting. A reasonable explanation is
8We also tried lags more than four, the results are quantitatively the same.
100

that if a model does not have enough lags, its dynamic inter relationship among variables could be
messed up and the factors extracted from it distribute on a broad range (see again the left panel of
Fig 2.1) such that economic implication is difﬁcult to give, displaying merely some purely statistic
properties. Nevertheless, if with enough lags, two lags above, the inter relationship can be fully
released and expressed, therefore the factor driving analysis could imply some important structural
interpretation.9 If more than enough, since important factors have been fully extracted, naturally
serious over ﬁtting will arise, comparing Fig 2.4 with Fig 2.2 on the right panel.
The above three variable empirical test consistently demonstrates that almost only one im-
portant force inﬂuence the dynamics of the economy, conﬁrming again that there are much more
forces determining the economy than those changing the economy. The result of empirical test
is in line with dynamic stochastic general equilibrium model (DSGE). If a DSGE model can be
transfomed into a VAR form under some conditions (Fernandez-Villaverde et al. 2006, Morris,
2012 and Ravenna, 2007), the coefﬁcients on the regressors must be the functions of ‘deep param-
eters’ of preference, technoloy and policy rule in the DSGE model and therefore the coefﬁcients
in the transformed VAR are cross equation restricted.10 Changes in one deep parameter will cause
changes in almost all the coefﬁcents in the VAR. This provides theoretical support for the results
of the above empirical test and recommend a factor-driven VAR model setting.
Since we ﬁnd strong factor leading evidence, even in a small scale TVP-VAR, empirically and
theoretically, another question will be naturally asked can the model with factor driven speciﬁed
in section 2 be used in structrual economic analysis and is it consistent or inconsistent with the
literature. We answer these questions in the next subsection.
9The ‘structural interpretation’ does not mean structural identiﬁcation associated with structural shocks in VARs.
It only mean several important factors already sufﬁciently drive the dynamic process of the economy.
10A general representation of a log-linearized DSGE model has a state space form. A state space corresponds to
a VARMA form (see, e.g., Aoki, 1990). But the VARMA does not necessarily can be transformed to a VAR form.
Fernandez-Villaverde et al. (2006) give the VAR(∞) expression for a DSGE under some conditions; Ravenna (2007)
for VAR(p) and Morris (2012) for VAR(1)under required conditions. Giacomini (2013) give a good literature review
on the relationship between DSGE and VAR models.
101

2.4.2
Structural analysis and model evaluation
The above empirical tests for the three variables model - inﬂation rate, unemployment rate and
3-month treasury rate - suggest that factors should be imposed on the time varying coefﬁcients
and two lags are enough for fully capturing the dynamic process in the data and therefore the time
variation in the latent coeffﬁcients. Since the empirical tests have already found that the factor
leading style for the ﬁrst lag is quite different with the style for from two to four lags, we can not
use early lags – the ﬁrst lag and constant here – to drive the remaining long lags. Hence the factor
settings on lags should be skipped under this empirical context.11
We implement the analysis in a model setting factors only on time varying coefﬁcients and
stochastic volatility. That is, we estimate the model only involving factors on coefﬁcients and
volatility, imposing no restriction on lags. As discussed in section 3, though the model gives a
general factor treatment to every part of TVP-VARs, where the factor restriction should be used
depends on the speciﬁc research object.
In this section we have two purposes. One is to investigate whether agents’ responses to mone-
tary policy shocks have changed or not over sample period and the second is to evaluate the model
performance under different lag settings. The ﬁrst is to check after factor extracting whether the
model is still capable of structural analysis and the second is to check whether the model is ﬂexible
enough to deal with the case when the extent of over ﬁtting goes strong as the number of lags
incease.
These two are jointly conducted together. Our analysis is based on above empirical tests. Dates
chosen for comparison are 1975 Q1, 1981 Q3 and 1996 Q1. They are somewhat representative of
the typical economic conditions of the chairmanships of Burns, Volcker and Greenspan, but apart
from that, they are choosen arbitrarily. Enough number of factors is chosen for each model accord-
ing to Table 2.1 and Table 2.2 so as to avoid the potential concern that the bulk of variation not fully
captured by less than enough factors could affect inference and might lead wrong understanding.
11We have estimated the model of the factor settings on lags, namely the general factor-driven model presented in
section 2. The result is messy to conduct instructive analysis. A reasonable interpretation is that the dynamic inter
relationship among the variables has been distorted by the the constant and the ﬁrst lag.
102

3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
median IR of inflation to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.1
−0.05
0
0.05
0.1
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.1
−0.05
0
0.05
0.1
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.2
−0.1
0
0.1
0.2
1996 Q1−1975 Q1
Figure 2.5: TVP-VAR-CV-lag1-factor7-IR-inﬂation
Notes: This is TVP-VAR with 1 lag, 7 factors driving time variation in coefﬁcients on regressors
and common volatility. The upper left panel plots median impulse responses of inﬂation to mone-
tary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remaining three panels plot difference
of responses between every two periods mutually, with solid line representing median and dashed
lines for 16th percentile and 84th percentile respectively.
103

3
6
9
12
15
18
21
0
0.02
0.04
0.06
0.08
IR of unemployment to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.05
0
0.05
0.1
0.15
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.05
0
0.05
0.1
0.15
1996 Q1−1975 Q1
Figure 2.6: TVP-VAR-CV-lag1-factor7-IR-unemployment rate
Notes: This is TVP-VAR with 1 lag, 7 factors driving time variation in coefﬁcients on regressors
and common volatility. The upper left panel plots median impulse responses of unemployment rate
to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remaining three panels plot
difference of responses between every two periods mutually, with solid line representing median
and dashed lines for 16th percentile and 84th percentile respectively.
104

3
6
9
12
15
18
21
−0.06
−0.04
−0.02
0
median IR of inflation to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.02
−0.01
0
0.01
0.02
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
1996 Q1−1975 Q1
Figure 2.7: TVP-VAR-CV-lag2-factor3-IR-inﬂation
Notes: This is TVP-VAR with 2 lag, 3 factors driving time variation in coefﬁcients on regressors
and common volatility. The upper left panel plots median impulse responses of inﬂation to mone-
tary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remaining three panels plot difference
of responses between every two periods mutually, with solid line representing median and dashed
lines for 16th percentile and 84th percentile respectively.
105

3
6
9
12
15
18
21
−0.05
0
0.05
0.1
0.15
IR of unemployment to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.02
−0.01
0
0.01
0.02
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.02
0
0.02
0.04
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
1996 Q1−1975 Q1
Figure 2.8: TVP-VAR-CV-lag2-factor3-IR-unemployment rate
Notes: This is TVP-VAR with 2 lag, 3 factors driving time variation in coefﬁcients on regressors
and common volatility. The upper left panel plots median impulse responses of unemployment rate
to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remaining three panels plot
difference of responses between every two periods mutually, with solid line representing median
and dashed lines for 16th percentile and 84th percentile respectively.
106

For model of one lag, we choose 7 factors since 7 factors can cover all the important factors
that together accounting for near 100% evidenced by Table 2.1 and Fig 2.1. In Fig 2.5 upper left
panel, we ﬁnd strong ‘price puzzle’ in each period respectively, a typical ﬁnding in small scale
monetary VAR. The remaining three graphs show the median of the distribution of the difference
of inﬂation responses between every two periods mutually with lower 16th percentile and upper
84th percentile conﬁdence band. There are no signiﬁcant difference among three periods in that
every coﬁdence band contains a zero line. The response of unemployment rate increase, see Fig
2.6, in each period consistent with economic theory. The difference of unemployment responses
between every two periods are not signiﬁcant with zero. As for the model with two lags, we
choose 3 factors in order to cover as much time variation as possible even though the ﬁrst factor
has already taken 96% seen in Table 2.1. The price puzzle disappears, in Fig 2.7, for each period;
Inﬂation decline after positive monetary policy shock. The difference of inﬂation responses every
two periods is not signiﬁcant in the remaining panel. Unemployment rate increase in each period
and no signiﬁcant difference mutually in Fig 2.8. When the model goes from 2 lags to 3 lags with
the same 3 factors as they have the same factor driving style, we ﬁnd that, in both Fig 2.9 and
Fig 2.10, the results are almost the same: inﬂation went down and unemployment went up each
period, and the dynamic process in each response function is signiﬁcantly no difference. Though
the model is of more over-parameterization, the parsimonious estimation of factor driving gives the
qualitatively and quantitatively the same results. How about the model of four lags when it become
further over-ﬁtting with 39 coefﬁcients? We assign again 3 factors and still ﬁnd the qualitatively
and quantitatively the same results in Fig 2.11 and Fig 2.12.12
The common volatility for each model in Fig 2.13 illustrates almost the same dynamic property
of the U.S. business cycle. The level of commom volatility was climbing during the 1970s up to the
peak in early 1980s during which Fed Chairman Volcker implemented monetary targeting policy,
after that from middle 1980s the volatility declined and stayed on a low level in a whole 1990s and
12We assign 3 factors to the portion of time varying coefﬁcients for models of 3 lags and 4 lags in order to make
sure most amount of variation can be covered. Actually, one factor for 2 lags, 3 lags and 4 lags already works well
respectively.
107

3
6
9
12
15
18
21
−0.1
−0.08
−0.06
−0.04
−0.02
0
median IR of inflation to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.01
0
0.01
0.02
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.02
−0.01
0
0.01
0.02
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
1996 Q1−1975 Q1
Figure 2.9: TVP-VAR-CV-lag3-factor3-IR-inﬂation
Notes: This is TVP-VAR with 3 lag, 3 factors driving time variation in coefﬁcients on regressors
and common volatility. The upper left panel plots median impulse responses of inﬂation to mone-
tary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remaining three panels plot difference
of responses between every two periods mutually, with solid line representing median and dashed
lines for 16th percentile and 84th percentile respectively.
108

3
6
9
12
15
18
21
−0.1
0
0.1
0.2
0.3
IR of unemployment to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.02
−0.01
0
0.01
0.02
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.02
0
0.02
0.04
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
1996 Q1−1975 Q1
Figure 2.10: TVP-VAR-CV-lag3-factor3-IR-unemployment rate
Notes: This is TVP-VAR with 3 lag, 3 factors driving time variation in coefﬁcients on regressors
and common volatility. The upper left panel plots median impulse responses of unempoyment rate
to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remaining three panels plot
difference of responses between every two periods mutually, with solid line representing median
and dashed lines for 16th percentile and 84th percentile respectively.
109

3
6
9
12
15
18
21
−0.2
−0.15
−0.1
−0.05
0
median IR of inflation to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.02
0
0.02
0.04
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.05
0
0.05
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
1996 Q1−1975 Q1
Figure 2.11: TVP-VAR-CV-lag4-factor3-IR-inﬂation
Notes: This is TVP-VAR with 4 lag, 3 factors driving time variation in coefﬁcients on regressors
and common volatility. The upper left panel plots median impulse responses of inﬂation to mone-
tary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remaining three panels plot difference
of responses between every two periods mutually, with solid line representing median and dashed
lines for 16th percentile and 84th percentile respectively.
110

3
6
9
12
15
18
21
−0.1
0
0.1
0.2
0.3
IR of unemployment to MP shocks
 
 
1975 Q1
1981 Q3
1996 Q1
3
6
9
12
15
18
21
−0.05
0
0.05
1981 Q3 − 1975 Q1
3
6
9
12
15
18
21
−0.05
0
0.05
1996 Q1 − 1981 Q3
3
6
9
12
15
18
21
−0.04
−0.02
0
0.02
0.04
1996 Q1−1975 Q1
Figure 2.12: TVP-VAR-CV-lag4-factor3-IR-unemployment rate
Notes: This is TVP-VAR with 4 lag, 3 factors driving time variation in coefﬁcients on regressors
and common volatility. The upper left panel plots median impulse responses of unemployment rate
to monetary policy shocks in 1975 Q1, 1981 Q3 and 1996 Q1. The remaining three panels plot
difference of responses between every two periods mutually, with solid line representing median
and dashed lines for 16th percentile and 84th percentile respectively.
111

1970
1980
1990
2000
0.4
0.6
0.8
1
vol (1 lags−7 factors)
1970
1980
1990
2000
0.4
0.6
0.8
1
vol (2 lags−3 factors)
1970
1980
1990
2000
0.4
0.6
0.8
1
vol (3 lags−3 factors)
1970
1980
1990
2000
0.4
0.6
0.8
vol (4 lags−3 factors)
Figure 2.13: Common volatility in different model settings
early 2000s, then gradully increased at the end of the sample.
Connecting the above analysis together, we ﬁnd that i) in the model of one lag even with enough
7 factors capturing almost all the variation in coefﬁcients, we still ﬁnd price puzzle. As discussed
in empirical test via principal component analysis, one lag can not sufﬁciently release and probably
distort the dynamic inter relationships among variables. This is a problem of lags, not a problem
of factor driving. Therefore, using the ﬁrst lag to drive the other lags is not a good choice. ii)
When having enough lags and even more lags to make the model more over parameterization, the
factor driving model works very well on every setting, giving almost the same results on impulse
response function and volatility. All these fully demonstrate that the factor-driven model is ﬂexible
enough to capture main driving forces and give consistent interpretation in the context of the model
with over-ﬁtting problem, especially when the extent of over parameterization is very serious (the
model with 4 lags).
112

2.5
Concluding remarks
In this chapter, we present a model that gives a general treatment via factor driving on the three
parts of conventional TVP-VAR model, namely, the part of time varying coefﬁcients on regressors,
the part of lags and the part of stochastic volatility. These three portions are sources of parameter
proliferation. The general model uses several factors to drive the dynamic process of all the time
varying coefﬁcients; uses the ﬁrst several lags to drive other remaining lags; and uses a latent factor
to drive the volatilities of all variables, i.e. the common volatility. These factor drivings can be
jointly or separately used depending on speciﬁc object one meets.
We also provide a Bayesian estimation procedure on this general model step by step, which
can be nested, divided and modiﬁed according to the factor driving portion one needs.
We ﬁnally conduct an empirical analysis on the typical small scale monetary TVP-VAR. Strong
evidences of over parameterization are found over one lag to four lags. This is the starting point
of our general parsimonious treatment of the standard TVP-VAR model. Empirical analysis shows
that the factor-driven model is strong enough and ﬂexible enough to capture the main driving forces
even in a serious over-ﬁtting setting and still give consistent results.
A point should be pointed out is that the factor-idea-based model must stand on the precondition
that the dynamic process among data set is correctly expressed. If not, the model is estimable, but
not instructive.
The future research could be applying the model to large data set or conducting forecasting
exercise.
113

Appendix
A. Bayesian estimation of restricted linear regression model
Here, we consider a multivariate linear regression case.
Consider that the linear regression model has the form:
yt = Xtβ +ut
where yt is an n×1 vector of regressands, Xt is n×k matrix of regressors, β is k×1 corresponding
parameters and ut ∼N (0,Σt). Stacking the above equation over time periods and holding all the
data together, one can obtain
y = Xβ +u
where y =
h
y
′
1,....y
′
T
i′
, X =
h
X
′
1,...,X
′
T
i′
and u =
h
u
′
1,...,u
′
T
i′
with u ∼N (0,Ω) and Ω=
Blkdiag

{Σt}T
t=1

.
We ﬁrst consider Bayesian estimation of unrestricted linear regression model. Set prior for β:
β ∼N

β,V β

(2.36)
then the posterior is
β ∼N
  ¯β, ¯Vβ

with
¯β = ¯Vβ

X
′Ω−1y+V −1
β β

(2.37)
¯Vβ =

X
′Ω−1X +V −1
β
−1
(2.38)
114

Now consider linear restriction on coefﬁcients
Rβ = 0
(2.39)
where R is a q × k matrix with rank(R) = q and q ≤k. The posterior of β can be derived by
pooling the linear regression model, the prior information and the linear restriction condition above
together:
y = Xβ +u
(2.40)
β = β +v
(2.41)
0 = Rβ +η
(2.42)
where v ∼N(0,V β) and η ∼N
 0, 1
λ Iq

. Equation (2.41) is prior inforamtion in equation (2.36)
and equation (2.42) is linear restriction of (2.39) when λ →∞. The posterior of restricted β for a
given λ can be obtained by conducting generalized least square estimation of the pooled regression
model (2.40) - (2.42):
βres (λ) ∼N

˜β (λ), ˜V(λ)

with
˜β (λ) =

X
′Ω−1X +V −1
β +λR
′R
−1 
X
′Ω−1y+V −1
β β

(2.43)
˜V (λ) =

X
′Ω−1X +V −1
β +λR
′R
−1
(2.44)
Following De Wind and Gambetti (2014), (2.44) is further expanded, by matrix inversion lemma,
to13
˜V (λ) =

X′Ω−1X +V −1
β
−1
−

X′Ω−1X +V −1
β
−1
R′

λ −1Iq +R

X′Ω−1X +V −1
β
−1
R′
−1
R

X′Ω−1X +V −1
β
−1
13Matrix inversion lemma: (A+BCD)−1 = A−1 −A−1B
 C−1 +DA−1B
−1 DA−1
115

Since equation (2.37) and equation (2.38), posterior of βres (λ) for a given λ can be expressed in
terms of unrestricted posterior of β:
˜β (λ) =

Ik −¯VβR
′ 
λ −1Iq +R ¯VβR
′−1
R

¯β
˜V (λ) =

Ik −¯VβR
′ 
λ −1Iq +R ¯VβR
′
R

¯Vβ
With above prepared expression, letting λ →∞, the posterior of βres follows
βres ∼N

˜βres, ˜Vres

with
˜βres =

Ik −¯VβR
′ 
R ¯VβR
′−1
R

¯β
˜Vres =

Ik −¯VβR
′ 
R ¯VβR
′−1
R

¯Vβ
and ¯β is from (2.37), ¯Vβ from (2.38).
116

Chapter 3
Structual Analysis in a Large Bayesian VAR
117

3.1
Introduction
In previous two chapters, We have studied time-varying parameters vector autoregressive regres-
sion models with stochastic volatility or common volatility. As we know that vector autoregessive
models are unrestricted models and relatively easily tractable with good properties such as impulse
response function and variance decompositon for the future dynamics or historical decomposition
for the past, therefore it is widely used by practioners, reseachers and policy-makers in economic,
policy ananlysis and forecasting. In addition, since endogenous variables in a vector inter act
each other that can capture very complicated relationships in an economy which economists are
interested in, and most importantly some of the relationship among those variabes are still vague,
disputable or even unknown, such as typiclly the effects of monetary policy on the performance
of economy, the relationship between ﬁnancial market and real economy, and recently the uncer-
tainty shocks that is paid more and more attention after global ﬁnancial crisis of 2008 and in the
subsequent still slow recovery, VARs are used again and again to ﬁnd inner possible relationships
that have not been explored by theoretical models and thus VARs are also good instructions for the
direction of theoretical analysis.
Nevertheless, everything has its two sides. VAR is not a parsimonious model due to its unre-
stricted structures that are very suitable to expose blackboxes in economy but it is hard to estimate
in parctice when the number of observations in a vector increase. Its advantege cause its disad-
vantage. Typically, in empirical analysis, a VAR has three or ﬁve, at most ten variables, but seen
otherwise without any restrictions, no matter in static VAR models or TVP-VAR models as we
have discussed in the previous chapers. Chapter 1 focuses on the signiﬁcance of the time varying
parameters over each identity and each time via stochastic variable selection and provide a method
to estimate them efﬁciently in practice; Chapter 2 lies in reducing dimension of the TVP-VAR with
SV via factor idea. Namely, we use factors to represent and reduce number of lags, coefﬁcients on
regressors and volatilities in order to make a parsionious estimation with potentional over-ﬁtting
problem. That is different with the ever increasing literature that mainly on reducing the dimension
of obsevations when one meet rich data enviroment such as facor models begining with Geweke
118

(1977) and factor augmented VAR recommended by Bernake, Boivin and Eliasz (2005) and Stock
and Watson (2005).
In this chapter, we still look at vector autoregressive models in constant parameters, rather than
time variant, discussed in Chapter 1 and Chapter 2. The VAR model with constant parameters
probably is the most widely used one in paractise. Economist and practioners have again and agian
proved that VARs with constant parameters still do a good job in economic analysis, policy mak-
ing and forecasting compared to other type of VARs like TVP-VAR or regime switching VAR.
Banbura, Giannone and Reichlin (2010) argue that vector regression with Bayesian shrinkage is
an appropriate tool for large dynamic models. Building on the results of De Mol and co-authors
(2008), they show that when the degree of shrinkage is set in relation to the cross-sectional dimen-
sion, the forecasting performance of small monetary VARs can be improved by adding additional
maroeconomic variables and sectoral information. In addition, VARs with shrinkage produce cred-
ible impulse responses and are suitable for structural analysis.
Small VARs have the limitation that the information incorporated in the model is very small,
while in real economic world, we have hundreds of maroeconomic data published by governtment
agency and research institution. For example, central banks usually obseve and investigate a large
amount of data in decision making of their policy; Economic agency in an economy can not deter-
mine their ﬁnal behavior within small number of observations. A small information-involved VAR
sometimes cause misleading and misunderstanding, a typical case is of ‘prize puzzle’ due to the
missing of forward looking variables in VAR analysis. However, if a VAR with more variables will
inevitably lead parameter proliferation. As suggested by Banbura et al. (2010), the way to deal
with the problem is to impose proper priors of shrinkage on the parameters. On the priors with
property of shrinkage, the parameters that are important for the variables will be strenghen while
less important will shrink to zero or a limit value.
Follwing Banbura et al. (2010) with proper priors, we conduct an empirical analysis on three
important shocks: monetary policy shock, uncertainty shock and ﬁnancial shock. Monetary pol-
icy shock and its transmission mechanism, as we known, are very important to central banks and
119

economic agencies. It reﬂects central banks’ stance to the current economic performance and what
kind of policy they conduct, and meanwhile how agencies react to such stance and policy imple-
mentaion. The second - uncertainty shocks and the third - ﬁnancial shocks recently are more and
more jointly considered, in paticular, in the current situation of slow recovery. Researchers ﬁnd
that uncertainty and ﬁnancial market are strongly tangled each other. They are both channels and
shocks. They are both potentially important drivers in business cycle and display strong nonlinear-
ity in different economic stages. We use a large data set which contains 28 variables that cover a
broad range of an economy such as goods market, labor market, ﬁnancial market and so on. By
the large VAR, we jointly identify and investigate the three shocks and their transmisson. We ﬁnd
some interesting results via Bayesian estimation with shrinkage priors.
This chapter is organized as follows. In the second section, we discuss some issues on the large
Bayesian VAR and the inter relationship among ﬁnancial market, uncertainty and monetary policy.
In section 3, we give the theoretical background of the prior on shrinkage, model speciﬁcation and
posterior in terms of large cross-sectional data set. Section 4 conducts empirical analysis and gives
some important ﬁndings we obtain and implications we infer from the large Bayesian VAR. The
last section concludes.
3.2
The literature
In this section, some issues will be discussed in current literature on the model and on the shocks
and their transmission, respectively.
We ﬁrst look at the model.
In econometric literature on VAR models, a very important issue is how to deal with the po-
tential over parameterization problem inherited in VARs and the accessibility of high dimensional
data set. This two typically join togther. Researchers would like to include more observations
in the dynamic model in order to expose and explore complicated inner relationships that samll
scale model is unable to do, at the same time if more variables are incorporated, the number of
120

parameters will nonlinearly, dramatically proliferate, making the estimation and inference infeasi-
ble even large data is availabe. Generally, there are three branches in dealing with this dilemma.
One is on reducing data dimension by factors. Since the Geweke (1977), factor modes have been
the most common way of achieving this goal. Applications such as Forni and Reichlin (1998),
Stock and Watson (1999, 2002b), Bernake and Boivin (2003), have popularized factor methds
among macroeconomists. Bernake, Boivin and Eliasz (2005) and Stock and Watson (2005) have
combined factor methods with VAR methods. Del Negro and Otrok (2008) and Korobilis (2013a)
provide further time varying parameter extension to these models. The second is on lowering
the dimension of parameters of the model still using the factor idea. Indeed, factors that drive
large observations can substantially limit the over-ﬁtting problem, which is specially prominent in
constant-parameter VARs, however, when it comes to time varying ones, the number of parame-
ters in the case of constant-paramer model will be multiplied by the periods of sample size. The
number of parameters again becomes very large even in a small scale TVP-VAR as discussed in
Chapter 2. To deal with the problem, Kim and Yamamoto (2012) use coefﬁcients on recent lags
as factors to drive distant lags; De Wind and Gambetti (2014) decompose the covariance matrix of
innovations to time varying parameters and extract several factors to drive whole dynamic process
of all the time varying coefﬁcients. Note that the covariance matrix associated with the parame-
ters become reduced rank; Carriero, Clark and Marcellino (2012) present a model applying latent
factor, i.e. common volatility to represent fully stochatic volatility on each variable justﬁed by the
observation that most maroeconomic variables share very similar pattern of estimated volatility.
They are all trying to limit parameter dimension in the context of time varying coefﬁcient fram-
work. The last is on shrinking the parameters via imposing priors. Probably the most classic one
is Minnesota prior (see Doan, Litterman and Sims, 1984 and Litterman, 1986). The basic idea of
the prior is that it makes model implement like a random walk process. There are also other priors
on stochastic search variable selection (SSVS) (see George, Sun and Ni, 2008). This paper focuses
on the third branch, namely, setting suitable priors to shrink the parameters of the large VAR via
Bayesian method. For the ﬁrst and the third branch, Koop and Korobilis (2010) have given a good
121

survey on them.
The second issue is about the empirical analysis. Here, we discuss ﬁnancial market and uncer-
tainty jointly. The literature has identiﬁed at least three channels through which uncertainty shocks
impose impact on economic activity. First, unceratin can affect the behavior of ﬁrms (Bernake,
1983; Bloom, 2009). A key concept in this framwork is irreversibility in investment. If invest-
ment decisions are irreversible, ﬁrms must take investment decisions that trade of the extra returns
from early commitment agianst the beneﬁt of having more informatioin by waiting. Bernake’s real
options framwork captures the notion that when uncertainty is high, the option value of waiting
increases as it may be beneﬁcial for ﬁrms to wait and acquire more information before deciding
to invest in a real asset. The second, higher uncertainty may induce households to save more as
higher uncertainty about future income will delay consumer spending, in particular on durable
goods (Romer, 1990). The last channel is via ﬁnancial maket for which a more recent strand of
research places ﬁnancial rather than real frictions at the center of the transmission mechanism
(Arellano et al., 2012; Christiano et al., 2014; Gilchrist et al., 2014). If ﬁnancial contracts are
subject to agency problem or moral hazard probem, a rise in economic uncertainty increase the
premium on external ﬁnance, leading to an increase in the cost of capital faced by ﬁrms or borrow-
ers and thus a fall in investment. These three channels mixed together, especially the last channel
makes that uncertainty and ﬁnancial market should be considered together.
In empirical works, Beetsma and Giuliodori (2012) use linear VARs via rolling windows and
show that the impact of uncertainty shocks on output in the US has decreased over the last ﬁve
decades. Hartmann et at. (2012) use a regime switch (with ﬁxed probability) model to estimate
the links between ﬁnancial stress and macroeconomic variables. They ﬁnd that ﬁnancial shocks
have more serious impact on real variables in high ﬁnancial stress regime than in normal times.
Bijsterbosch and Guerin (2014) also use a regime swithing model to study regime-dependent re-
lationship between uncertainty and economic activity. They ﬁnd that only the third regime - the
highest uncertainty regime are associated with a weaker growth performacne and sharp decline in
stock price.
122

Caggiano et al. (2014) use instead a smooth - transition VAR (with continous probability)
where parameters are allowed to depend on the state of the business cycle. They ﬁnd that, using
U.S. quarterly post-war data, uncertainty shocks have a stronger impact on unemployment during
recessions. The above researches only independently study the uncertainty or ﬁnancial conditions,
not jointly together. It is crucial if the ‘ﬁnancial view’ of the third channel discussed above plays
a very important role. To our best knowledge, the Alessandri and Mumtaz (2014) probably the
ﬁrst to study the joint relationship of uncertainty and ﬁnancial condition to check the importance
of the ‘ﬁnancial view’ in literature. Their model is very innovative in twofolds. First, the effects
of uncertainty is dependent on the different ﬁnancial conditions which is determined by a threhold
variable that represents ﬁnancial conditions on different stage rather than business cycle stages
deﬁned by NBER. Second, the uncertainty is generated endogenously from the common stachastic
volatility as we discussed in Chaper 2 and the common volatility can be regarded as uncertainty
proxy that is placed in the regressor, then the volatility generated from the model itself have the
effects on the endogenous variables. This model is so called volatility in mean. The idea of using a
single volatility process in a multivariate model has been introduced by Carriero et al. (2012) while
volatility in mean effects are studied in the context of otherwise linear VAR models by Mumtaz
and Thedoridis (2012), Mumtaz and Surio (2013) and Mumtaz and Zanetti (2013).
The above linear or nonliner VAR models as tool to study uncertainty and ﬁnancial market
jointly or indepently all belong to small scale VAR models. Caggiano et al. (2012) have four
variables, namely the uncertainty proxy of VIX index and inﬂation, umeployment and federal
funds rates. Alessandri and Mumtazi (2014) also use four variables including a ﬁnancial condition
proxy. Bloom (2009), perhaps the most, contained eight observations. Except for the Banbura et al.
(2010), large Bayesian VAR models are seldom used in structual analysis though recommended
by them in terms of large information of great view. We only ﬁnd three works related with it.
Gupta et al. (2012) study the effects of monetary policy on housing sector dynamics in a large
sacle Bayesian VAR with 143 monthly macrovariables. Auer (2014) consider another direction of
monetary policy shock on foreign investment income from a large VAR with 32 variables. Sanjani
123

(2014) use quarterly data of 34 variables to study ﬁnancial frictions. In this chapter, we use a
large data set of 28 observables to jointly identify and analyse the effect of each structual shock
on different dimension of the U.S. economy. In the next section, the large Bayesian VAR model is
presented and the priors on shrinkage are discussed.
3.3
The model
3.3.1
The likelihood of VAR
Let’s consider a VAR model with p lags. It can be typically written as
yt = c+B1yt−1 +···+Bpyt−p +ut
(3.1)
where yt is an n × 1 vector containing n endogenous observations; c is also n × 1 constant term;
coefﬁcients on lags are from B1 to Bt−p with corresponding dimension n × n and ut is residual
follows ut ∼iddN (0,Σ). The above equation is the expression for each period, it can be rewritten
in the form of nesting all the data, that is in a compact form
Y = XB+U
(3.2)
where equation (3.2) is obtained with matrices xt =
h
y
′
t−1,...,y
′
t−p,1
i′
, then the X = [x1,...,xT]
′
and accordingly B = [B1,...,Bt−p,c]
′ as well as U = [u1,...,uT]
′. By column stacking operator of
both sides of equation (3.2), we further have the form
y = (In X)β +u
(3.3)
where y = vec(Y), β = vec(B), u = vec(U) and u ∼N (0,ΣIT) with vec column stacking op-
erator and  Kronecker product. The regressors in the brackets are obtained by vec(X · B · I).1
1vec(ABC) =

C
′ A

vec(B)
124

Note that y = [y1;...;yn] where yi is a T ×1 vector collecting all observations belonging to i, there-
fore y is a Tn × 1 column. u has the similar structure as y. The likelihood of (3.3) based on the
multivariate normal distribution of u is written as
f (y|β,Σ) ∝|Σ|
T
2 exp
1
2u
′ (ΣIT)−1 u

by the transformation formula tr(ABC) = vec

A
′′
(I B)vec(C), the above is equal to
f (y|β,Σ) ∝|Σ|
T
2 exp
1
2tr

(Y −XB)
′ (Y −XB)Σ−1
(3.4)
The above likelihood after leaving out the constant term that does not affect ﬁnding distribution
kernels, can be decomposed into two components: f (β|Σ,y) and f (Σ|y). The former follows
multi-variate normal distribution and the latter follows inverse wishart distribution. By the equation
that (Y −XB)
′ (Y −XB) =
 Y −X ˆB
′  Y −X ˆB

+
  ˆB−B
′
X
′X
  ˆB−B

, where ˆB =

X
′X
−1
X
′Y
is the least square estimate of B, equation (3.4) gives
f (y|β,Σ) ∝|Σ|−k
2 exp

−1
2tr
  ˆB−B

X
′X
  ˆB−B

Σ−1
(3.5)
|Σ−T−K
2 |exp

−1
2tr
 Y −X ˆB
′  Y −X ˆB

Σ−1
(3.6)
the ﬁrst equation (3.5) is the kernel of the matrix normal distribution and the second (3.6) is the
kernel of the inverse wishart distribution. It is more convenient to rewrite the matrix normal distri-
bution in terms of the multivariate normal distribution, so that
β|Σ,y ∼N

ˆβ,Σ

X
′X
−1
(3.7)
Σ|y ∼IW
  ˆS,T −k −n−1

(3.8)
125

where ˆβ = vec
  ˆB

, ˆS =
 Y −X ˆB
′  Y −X ˆB

and k = np+1
From the above derivation, we know that the likelihood function of a VAR is a product of two
conditional distributions.
3.3.2
The priors
First we introduce the Minnesota prior. The basic idea of Minnesota prior is that all the equations
are ‘centered’ around the random walk with drift, which means that Yt = c+Yt−1 +ut. This means
diagonal of B1 shrink to one and the remaining elements of coefﬁcient matrix from B1 to Bp
towards to zero. Any element of coefﬁcient matrices independently follows a normal distribution
satisfying the moment conditions
E
h
(Bk)i j
i
=







δi,
j = i,k = 1
0
otherwise
(3.9)
V
h
(Bk)i j
i
=







λ 2
k2 ,
j = i
ν λ 2
k2
σ2
i
σ2
j ,
otherwise
(3.10)
which reﬂects that diagonal elements of coefﬁcient matrix on the ﬁrst lag centered on δi. If δi
shrinks to 1, it is random walk; If δi converges to0, it is white noise. The diagonal covariance
matrix V on each Bk implies two beliefs that more recent lags are more important than more
distant lags which can be seen that as k →p, the magnitude of the variance, i.e. the range of
ﬂucuation become narrower and narrower and accordingly coefﬁcients on far lags shrink more
quickly to zero and that own lags of a variable have more powerful interpretation than foreign lags
which can be found on the value of ν ∈(0,1). In the original version of Minnesota prior, it has
a hyper parameter to control tightness on each line of (3.10). In Banbura et al. (2010), the hyper
parameter on the ﬁrst line has been normalized, therefore ν takes the range. We follow the way of
Banbura. Finally, let’s look at the λ which controls the overall tightness of the prior distribution.
126

The λ shows up on both lines of (3.10), which means that as λ →0, the posterior is equivalent
to the prior and the data is perfectly dominated by prior information; when λ →∞, the very ﬂat
prior, then the posterior equals likelihood and prior plays no role. De Mol et al. (2008) show that
the parameters should be shrunk more so as to alleviate over-ﬁtting when the number of variables
increases. σ2
i /σ2
j shows the scale and variability of the data. These values can be set by residual
variance on AR(p) regression on each variable in a large data set.
The original Minnesota prior assumes that covariance matrix Σ of ut is ﬁxed and diagonal. If
one is interested in structural analysis, Σ should be imposed as a full matrix to allow for possible
correlation among residual of different variables. To overcome the problem, following Kadiyala
and Kalsson (1977), Banbura et al. (2010) impose a normal inverse wishart prior on (β,Σ) under
the condition that ν = 1. This prior can retain the principles of the Minnesota prior. The normal
inverse wishart prior has the form
vec(B)|Σ
∼
N (vec(B0),ΣΩ0)
(3.11)
Σ
∼
IW (S0,α0)
(3.12)
Since (3.7) and (3.8) are also the same distributions, the product of (3.11) and (3.12) is the like-
lihood of prior information. By this link, imposing prior on likelihood is equivalent to adding
dummy observations to the original data set which demonstrates the advantage of natural con-
jugate prior of normal inverse wishart prior. Following (3.7) and (3.8), the prior parameters B0,
Ω0, S0 and α0 can be expressed by dummy observations: B0 =

X
′
dXd
−1
X
′
dYd, Ω0 =

X
′
dXd
−1
,
S0 = (Yd −XdB0)
′ (Yd −XdB0) and α0 = Td −k −n−1 where k = np+1.
It can be shown that the dummy variables Xd and Yd involving Minnesota moments of (3.9) and
127

(3.10) are
Yd =
















diag(δ1σ1,...,δnσ)/λ
0n(p−1)×n
···
diag(σ,...,σn)
···
01×n
















Xd =












Jp  diag(σ1,...,σn)/λ
0np×1
...
0n×np
0n×1
...
01×np
ε












where Jp = diag(1,2,..., p).
We also include an additonal prior, which implements a so-called ’inexact differencing’ of the
data. More preciselty, rewrite the VAR equation (3.1) in an error correction form:
△yt = c+Πyt−1 +B⋆
1△yt−1 +···+B⋆
p−1△yt−p+1 +ut
where B⋆
s = −Bs+1 −···−Bp, s = 1,..., p−1 and Π = B1 +···+Bp −In.
A VAR in ﬁrst difference implies the restriction Π = 0 or A1 +···+Ap = In. We follow Doan
et al. (1984) and set a prior that shrinks Π to zeros. Specially, we set a prior that is centered at 1 for
sum of coefﬁcients on the own lags for each variable, and at 0 for the sum of coefﬁcients on other
variables’ lags. This prior introduces correlations among the coefﬁcients on each variable in each
equation. The tightness of this prior on the sum of ‘coefﬁcients’ is controlled by the hyparameter
τ. As τ goes inﬁnity, the prior becomes diffuse, whereas as it goes to zero, we approach the case
of exact differencing, which implies the presence of a unit root in each equation. In the literature,
it is usually implemented by adding the following dummy observations:
128

Y ⋆
d = diag(δ1µ1,...,δnµn)/τ
(3.13)
X⋆
d =
  11×p

 diag(δ1µ1,...,δnµn)/τ
0n×1

(3.14)
The incorporation of this sum of coefﬁcients prior serves to increase accuracy of forecast and
avoid conﬁdence band explosiveness in long horizons for impulse response functions.
3.3.3
The posterior
As discussed above, implementation of priors is equivalent to adding dummy observations trans-
formed from priors, now the equation (3.2) is augmented to
Y ⋆= X⋆B+U
(3.15)
where T ⋆= T +Td +T ⋆
d , Y ⋆=

Y
′,Y
′
d,Y ⋆′
d
′
and X⋆=

X
′,X
′
d,X⋆′
d

with Td = n(p+1)+1, T ⋆
d = n.
To ensure the existence of the prior expectation of Σ, it is necessary to add an improper prior
Σ ∼|Σ|−(n+3)/2. After that the posterior has the form
vec(B)|Σ,Y ∼N

vec

˜B,Σ

X⋆′X⋆−1
Σ|Y ∼IW
 ˜Σ,Td +T ⋆
d +2+T −k

where ˜B =

X⋆′X⋆−1
X⋆′Y ⋆and ˜Σ =
 Y ⋆−X⋆˜B
′  Y ⋆−X⋆˜B

which are typical setting for normal
inverse wishart distribution as seen in (3.7) and (3.8).
3.4
Empirical analysis
In this section, we apply the large Bayesian VAR model with priors of shrinkage to a large data set
which contains 28 variables covering a broad range of the U.S. economy. We focus three shocks
which are monetary policy shocks, uncertainty shocks and ﬁnancial shocks.
129

3.4.1
The data
Since large Bayesian VAR with proper shrinkage priors is able to deal with large data set, the
whole information in the data can be directly used to explore complicated dynamic process among
different variables avoiding possible missing variable bias. Following Sanjani (2014), we select 28
variables that generally can give a comprehensive description of the economy of U.S.
This data covers labor market, housing market, labor market, government bonds market, cor-
porate bonds market and so on. To identify uncertainty shock and ﬁnancial shock, we include
VIX index proxy for uncertainty measure and Chicago Fed national ﬁnancial condition index for
ﬁnancial condition measure.
As for uncertaity, three types of measure can be found: ﬁnancial market indicators, survey
based measures including forecast dispersion measures and media measures based on the number
of citations of a speciﬁc term. Other measures are more microeconomic in nature and based on
various indicators of dispersion at individual company or industry level. In this paper, we choose
the VIX index due to two reasons. First it is widely used as standard uncertainty measure (Bloom,
2009) and second it covers long period since 1962 the third quarter.
For the ﬁnancial conditons, the Chicago Fed national ﬁnancial condition index is chosed. It
is a real-time indicator of ﬁnancial distress constructed and maintained by the Chicago Fed and
described extensively in Brave and Butters (2012). The index extracted using dynamic factor
analysis from a set of 120 series that describe a broad range of monetary, debt and equity markets
as well as the leverage of the ﬁnancial industry. Another advantage of the data is that it covers the
longest periods since 1973 the ﬁrst quarter.
We use quarterly data from 1973 the ﬁrst quarter to 2009 the last quarter that covers the longest
period for all the 28 variables.
3.4.2
Empirical ﬁndings
We use 28 quarterly data form 1973 Q1 to 2009 Q4 with 2 lags in large VAR in Bayesian estima-
tion. We also tried more lags, however, the results are robust.
130

The identiﬁcation of the monetary policy shocks, uncertainty shocks and ﬁnancial shocks are
implemented in a recursive manner. For the identiﬁcation of monetary policy shocks, the observa-
tions are divided into three blocks. The ﬁrst one is slow moving block that contain variables not
sensitive in response to monetary policy shocks and react with one period lag. The second block
only contains the instrument of conventional monetary policy, namely the effctive fedral fund rate.
The last block contains variables that are very sensitive to the changes in monetary policy – the
surprise. All the variables in the third block are ﬁnancial variables that cover government bonds,
corporate bonds, stock , exchange rate markets.
As for the identiﬁcation of the uncertainty shock, I place VIX index for uncertainty measure
in the ﬁrst place in the ﬁrst block, that is VIX is the ﬁrst variable among all the observations.
Caggiano et al. (2012) in a small scale nonlinear VAR for the U.S economy and Kamber et al.
(2013) in a factor augmented VAR model for small open economy of New Zealand, both place
uncertainy measure of VIX in the ﬁrst. In order to make the uncertainty measure as compatible
as possible with the identiﬁcation restriction - uncertainty is not affected immediately by other
shocks, following Kamber et al. (2013), the quarterly data is constructed by only choosing the
ﬁrst month of the quarter. Regarding ﬁnancial condition variable, it is ordered the last in the third
block. That is, the national ﬁnancial condition index is the last one among all the observations in
the sense that this indicator responses to all the variables contemporarily. In practice, identiﬁcation
of these shocks are conducted by cholesky decomposition of the covariance matirx of the residuals
in the large VAR.
Let’s ﬁrst look at the effects of monetary policy shocks on the economy. Figue 3.1 plots the
response functions in median for all the 28 variables with dashed lines representing conﬁdence
interval between 16th percentile and 84th percentile. From the responses, it is reasonable to believe
that the non-systematic monetary policy is well identiﬁed. One standard deviation monetary policy
shock cause almost all the responses in line with economic theory. Non-systematic monetary
shocks increase uncertainty that is recently empasized by Baker et al. (2013) in the current slow
recovery stage after the global ﬁnancial crisis of 2008. Both real GDP and industrial production of
131

different output measure decline in response to tightening policy. Capacity utilization gets down in
response to high interest rate. Residential investment responses much stronger than non-resdential
investment due to that they are more sensitive to ﬁnancing cost. Consumer conﬁdence is also
depressed by the tightening policy. With the increase in federal fund rate - the benchmark rate,
rates of all the government bonds and corporate bonds rise. Stock price declines as expected. U.S
dollar appreciates due to high return in international capital market. M1 and M2 decline reﬂecting
liquidity. Finally, tighten monetary policy elevates ﬁnancial stress.
Then we look at the uncertainy shocks in Fig 3.2. Uncertainty shocks have countercyclical
effect on the economy. Capacity utilization decline. Real output, consumption, investment all
get down consistent with different economic theory on uncertainty. What is interesting is that
comparing with monetary shock, nonresidential investment decline facing futural uncertaity while
residential investment seems nonsensitive to the uncertainty relative to borrowing cost caused by
tightening monetary policy. Uncertainty also depresses consumer conﬁdence. In respose to un-
certainty shock, federal fund rate lowers in order to stimulate the economic activity. Decline in
federal funds rate decreases government bonds rates while increase the Baa return. This means
that with high uncertainty, the effects of requiring high risk premium dominate the decline in short
rates for corporate bonds with low credit rating. M1 and M2 is improved by Fed policy. Stock
price decline and stay on a low level for the whole ﬁve years. Uncertainty cause national ﬁnancial
conditon worse than before which is evidenced by the last graph in Fig 3.2.
As discussed in section 1 and section 2, ﬁnancial market and uncertaity links together and affect
each other. On the main diagonal of Fig 3.3, ﬁnancial shocks increase the uncertainty. Financial
shocks affect real activity for a long period. Compared response of industrial production in Fig 3.3
with that in Fig 3.2, the deline in IP caused by ﬁnancial shock last for ﬁve years, while the effect
caused by uncertainy recover back after three or four quarters. We can ﬁnd the same case in real
GDP, employment, hours worked, capacity utilization and nonresidential investment as opposed
to uncertainty shocks. Again residential investment becomes sensitive with ﬁnancial tightness –
the same response with positive monetary shocks. In addtion, ﬁnancial shocks seem to have no
132

signiﬁcant effects on ﬁnancial variables except for ﬁnancial condition index itself; In contract,
uncertainty shocks have strong effects on ﬁnancial variables over government bonds, corporate
bonds, money, stock and foreign exchange rate markets.
In sum, through the large Bayesian VAR with data set covering broad range of the U.S. econ-
omy, we have three ﬁndings. First, increase in uncertainty cause tight ﬁnancial conditions and tight
ﬁnancial conditons lead high uncertainty (see the main diagonal of Fig 3.2 and Fig 3.3); Second,
both ﬁnancial and uncertaity shocks cause counter-cyclical effects on real activity but ﬁnancial
shocks have the effects kept for a long time; The last is that ﬁnancial variables are much more
sensitive to uncertainty shocks compared to ﬁnancial shocks.
3.5
Concluding remarks
In this chapter, we switch from time varying parameter VAR to constant parameter VAR in a rich
data environment. With the proper priors of shrinkage property imposed on the coefﬁcients and
estimated by Bayesian method, the model conducts a good performance in structual analysis.
We jointly identiﬁed three shocks which are monetary policy shocks, ﬁnancial shocks and un-
certainty shocks in a structural analysis. For the effects of monetary shocks, the impulse response
functions are in line with theoretical predictions. For the ﬁnancial shocks and uncertainty shocks,
we analyse them together. Financial condition and uncertainty affect each other. Tight ﬁnancial
condition elevates uncertainty and in turn, high uncertainty exacerbates ﬁnancial condition. Both
positive ﬁnancial and uncertainty shocks have negative effects on real activities, however, ﬁnancial
shocks have more persistent effects on these real variables than uncertainty shocks. We also ﬁnd
that ﬁnancial variables care more for uncertainty shocks compared to ﬁnancial shocks.
The empirical analysis implies that in a theoretical framwork such as DSGE model, the ﬁnan-
cial market and uncertainty should be combined together beacuse they are mutually as shocks and
channels, affecting each other and multiple dimensions of the economy evidenced by the large
Bayesian VAR.
133

Figure 3.1: IRs to monetary policy shocks
10 20 30 40
−0.05
0
0.05
VIX
10 20 30 40
−0.01
0
0.01
RGDP
10 20 30 40
−0.02
0
0.02
Com P
10 20 30 40
−0.02
0
0.02
IP
10 20 30 40
−0.01
0
0.01
Emp: total
10 20 30 40
−0.01
0
0.01
Emp: serv
10 20 30 40
−0.01
0
0.01
Consp
10 20 30 40
−0.05
0
0.05
Res.Inv
10 20 30 40
−0.05
0
0.05
NonResInv
10 20 30 40
−2
0
2
Cap Util
10 20 30 40
−0.5
0
0.5
Cons Confid
10 20 30 40
−0.02
0
0.02
Emp Hor
10 20 30 40
−5
0
5
x 10
−3
Compensation
10 20 30 40
−1
0
1
FFR
10 20 30 40
−1
0
1
3mTB
10 20 30 40
−0.5
0
0.5
6mTB
10 20 30 40
−0.5
0
0.5
1yTB
10 20 30 40
−0.5
0
0.5
5yTB
10 20 30 40
−0.5
0
0.5
10yTB
10 20 30 40
−0.5
0
0.5
AAA
10 20 30 40
−0.5
0
0.5
BAA
10 20 30 40
−0.02
0
0.02
M1
10 20 30 40
−0.01
0
0.01
M2
10 20 30 40
−0.05
0
0.05
S&P500
10 20 30 40
−0.05
0
0.05
Eff ex−rate
10 20 30 40
−0.05
0
0.05
NW1
10 20 30 40
−0.05
0
0.05
NW2
10 20 30 40
−0.5
0
0.5
FCI
Notes: Impulse responses of 28 variables to monetary policy shocks with solid line representing
posterior median and dashed lines covering conﬁdence interval between 16th percentile and 84th
percentile.
134

Figure 3.2: IRs to uncertainty shocks
10 20 30 40
−0.2
0
0.2
VIX
10 20 30 40
−5
0
5
x 10
−3
RGDP
10 20 30 40
−0.02
0
0.02
Com P
10 20 30 40
−0.01
0
0.01
IP
10 20 30 40
−5
0
5
x 10
−3
Emp: total
10 20 30 40
−5
0
5
x 10
−3
Emp: serv
10 20 30 40
−5
0
5
x 10
−3
Consp
10 20 30 40
−0.05
0
0.05
Res.Inv
10 20 30 40
−0.02
0
0.02
NonResInv
10 20 30 40
−1
0
1
Cap Util
10 20 30 40
−0.5
0
0.5
Cons Confid
10 20 30 40
−0.01
0
0.01
Emp Hor
10 20 30 40
−0.01
0
0.01
Compensation
10 20 30 40
−0.5
0
0.5
FFR
10 20 30 40
−0.5
0
0.5
3mTB
10 20 30 40
−0.5
0
0.5
6mTB
10 20 30 40
−0.5
0
0.5
1yTB
10 20 30 40
−0.2
0
0.2
5yTB
10 20 30 40
−0.2
0
0.2
10yTB
10 20 30 40
−0.2
0
0.2
AAA
10 20 30 40
−0.2
0
0.2
BAA
10 20 30 40
−0.02
0
0.02
M1
10 20 30 40
−0.01
0
0.01
M2
10 20 30 40
−0.05
0
0.05
S&P500
10 20 30 40
−0.05
0
0.05
Eff ex−rate
10 20 30 40
−0.05
0
0.05
NW1
10 20 30 40
−0.05
0
0.05
NW2
10 20 30 40
−0.2
0
0.2
FCI
Notes: Impulse responses of 28 variables to uncertainty shocks with solid line representing pos-
terior median and dashed lines covering conﬁdence interval between 16th percentile and 84th per-
centile.
135

Figure 3.3: IRs to ﬁnancial shocks
10 20 30 40
−0.02
0
0.02
VIX
10 20 30 40
−5
0
5
x 10
−3
RGDP
10 20 30 40
−0.01
0
0.01
Com P
10 20 30 40
−0.01
0
0.01
IP
10 20 30 40
−0.01
0
0.01
Emp: total
10 20 30 40
−5
0
5
x 10
−3
Emp: serv
10 20 30 40
−5
0
5
x 10
−3
Consp
10 20 30 40
−0.02
0
0.02
Res.Inv
10 20 30 40
−0.02
0
0.02
NonResInv
10 20 30 40
−0.5
0
0.5
Cap Util
10 20 30 40
−0.5
0
0.5
Cons Confid
10 20 30 40
−0.01
0
0.01
Emp Hor
10 20 30 40
−5
0
5
x 10
−3
Compensation
10 20 30 40
−0.2
0
0.2
FFR
10 20 30 40
−0.2
0
0.2
3mTB
10 20 30 40
−0.1
0
0.1
6mTB
10 20 30 40
−0.2
0
0.2
1yTB
10 20 30 40
−0.2
0
0.2
5yTB
10 20 30 40
−0.2
0
0.2
10yTB
10 20 30 40
−0.2
0
0.2
AAA
10 20 30 40
−0.2
0
0.2
BAA
10 20 30 40
−0.02
0
0.02
M1
10 20 30 40
−5
0
5
x 10
−3M2
10 20 30 40
−0.05
0
0.05
S&P500
10 20 30 40
−0.02
0
0.02
Eff ex−rate
10 20 30 40
−0.05
0
0.05
NW1
10 20 30 40
−0.02
0
0.02
NW2
10 20 30 40
−0.2
0
0.2
FCI
Notes: Impulse responses of 28 variables to ﬁnancial shocks with solid line representing posterior
median and dashed lines covering conﬁdence interval between16th percentile and 84th percentile.
136

Appendix
A. Data speciﬁcation
Notes: The data set consists of 28 U.S. quarterly variables from 1973 Q1 to 2009 Q4. Except for
consumer conﬁdence index, effective exchange rate index and VIX volatility index, other data can
be found from Federal Reserve Economic Data - FRED - St. Louis Fed. The consumer conﬁdence
index is from OECD. The effective exchage rate is from BIS. For the VIX index, the stock market
volatility is measured by realized volatility before 1986 and by Black - Scholes implied volatility
after 1986 (Choi, 2013).2 The quarterly data of VIX is constructed by choosing the ﬁrst month of
each quarter following Kamber et al. (2013) to allow for lag effect in response to other shocks. For
the other quaterly data, they are all constructed by monthly average for a speciﬁc quarter. The ‘#’
column lists the order of the variables in the large VAR. In the ‘Tcode’ column, 1 means level, 2
means log level and ‘sa’ represents seasonally adjusted as well as ‘nsa’ not seasonally adjusted. In
the last ‘Id’ column, ‘f’ implies fast moving and ‘s’ slow moving.
2The monthly VIX index is kindly provided by Sangyup Choi.
137

Table 3.1: Data speciﬁcation
#
Mnemonic
Description
Tcode
Id
1
VIX
CBOE Volatility Index
2 nsa
s
2
RGDP
Real gross domestic product
2 sa
s
3
PPI
Producer Price Index: ﬁnished goods
2 sa
s
4
IP: total
Industrial Production Index
2 sa
s
5
Emp: total
All employees: total private
2 sa
s
6
Emp: services
All employees: service providing
2 sa
s
7
consp
Real personal consumption expenditures
2 sa
s
8
Res.Inv
Residential private domestic investment
2 sa
s
9
NonRes.Inv
Nonresidential private domestic investment
2 sa
s
10
Cap Util
Capacity utilization: total industry
1 sa
s
11
Cons Conﬁd
Consumer conﬁdence index (OECD)
1 nsa
s
12
Emp. Hours
Hours of all persons: nonfarm
2 sa
s
13
Real Comp/Hour
Real compensation per hour: nonfarm
2 sa
s
14
FFR
Effective Federal Funds Rate
1 nsa
-
15
3mTB
Treasury Bill rate 3 months: second market
1 nsa
f
16
6mTB
Treasury Bill rate 6 months: second market
1 nsa
f
17
1yTB
Treasury Bill rate 1 year: constant maturity rate
1 nsa
f
18
5yTB
Treasury Bill rate 5 year: constant maturity rate
1 nsa
f
19
10yTB
Treasury Bill rate 10 year: constant maturity rate
1 nsa
f
20
AAA yield
AAA corporate bond yield
1 nsa
f
21
BAA yield
BAA corporate bond yield
1 nsa
f
22
M1
M1 money stock
2 sa
f
23
M2
M2 money stock
2 sa
f
24
S&P 500
S&P’s common stock price index
2 nsa
f
25
Ex rate
Effective exchange rate index (BIS)
2 nsa
f
26
NW 1
Market value of equities outstanding
2 nsa
f
27
NW 2
Owners’ equity in household real estate
2 nsa
f
28
FCI
Chicago Fed national ﬁnancial condition index
1 nsa
f
138

References
Alessandri, P. & Mumtaz, H. (2014). Financial regimes and uncertainty shocks. Working paper,
No.729, Queen Mary University of London, School of Economics and Finance.
Aoki, N. (1990). State space modeling of time series. Cambridge Univ Press.
Arellano, C., Bai, Y., & Kehoe, P. (2010).
Financial markets and ﬂuctuations in uncertainty.
Federal Reserve Bank of Minneapolis Working Paper.
Auer, S. (2014). Monetary policy shocks and foreign investment income: evidence from a large
bayesian var. SNB working papers, Swiss National Bank.
Baker, S. R., Bloom, N., & Davis, S. J. (2013). Measuring economic policy uncertainty. Chicago
Booth research paper.
Ba´nbura, M., Giannone, D., & Reichlin, L. (2010). Large bayesian vector auto regressions. Journal
of Applied Econometrics, 25(1), 71–92.
Baumeister, C. & Peersman, G. (2013). Time-varying effects of oil supply shocks on the u.s.
economy. American Economic Journal: Macroeconomics, 5(4), 1–28.
Bauwens, L., Koop, G., Korobilis, D., & Rombouts, J. V. (2014). The contribution of structural
break models to forecasting macroeconomic series. Journal of Applied Econometrics, early
online publication.
Beetsma, R. & Giuliodori, M. (2012). The changing macroeconomic response to stock market
volatility shocks. Journal of Macroeconomics, 34(2), 281–293.
139

Bernanke, B. S. (1983). Irreversibility, uncertainty, and cyclical investment. The Quarterly Journal
of Economics, 98(1), 85–106.
Bernanke, B. S. & Boivin, J. (2003). Monetary policy in a data-rich environment. Journal of
Monetary Economics, 50(3), 525–546.
Bernanke, B. S., Boivin, J., Tom, D., & Eliasz, P. S. (2005). Measuring the effects of mone-
tary policy: A factor-augmented vector autoregressive (favar) approach. Quarterly Journal of
Economics, 120(1), 387–422.
Bernanke, B. S. & Mihov, I. (1998). Measuring monetary policy. Quarterly Journal of Economics,
113(3), 869–902.
Bijsterbosch, M. & Guérin, P. (2013). Characterizing very high uncertainty episodes. Economics
Letters, 121(2), 239–243.
Blanchard, O. & Perotti, R. (2002).
An empirical characterization of the dynamic effects of
changes in government spending and taxes on output. The Quarterly Journal of Economics,
117(4), 1329–1368.
Blanchard, O. & Simon, J. (2001). The long and large decline in u.s. output volatility. Brookings
papers on economic activity, 2001(1), 135–174.
Bloom, N. (2009). The impact of uncertainty shocks. Econometrica, 77(3), 623–685.
Boivin, J. (2001). The fed’s conduct of monetary policy: Has it changed and does it matter?
Columbia Business School, mimeo.
Boivin, J. & Giannoni, M. P. (2006). Has monetary policy become more effective? The Review of
Economics and Statistics, 88(3), 445–462.
Brave, S. & Butters, A. (2012). Diagnosing the ﬁnancial system: ﬁnancial conditions and ﬁnancial
stress. International Journal of Central Banking, 8(2), 191–239.
140

Caggiano, G., Castelnuovo, E., & Groshenny, N. (2014). Uncertainty shocks and unemployment
dynamics in u.s. recessions. Journal of Monetary Economics, 67, 78–92.
Canova, F. (2007). Methods for applied macroeconomic research, volume 13. Princeton University
Press.
Canova, F. & Gambetti, L. (2009). Structural changes in the u.s. economy: Is there a role for
monetary policy? Journal of Economic dynamics and control, 33(2), 477–490.
Carlin, B. P., Polson, N. G., & Stoffer, D. S. (1992). A monte carlo approach to nonnormal and
nonlinear state-space modeling. Journal of the American Statistical Association, 87(418), 493–
500.
Carriero, A., Clark, T. E., & Marcellino, M. G. (2012). Common drifting volatility in large bayesian
vars. CEPR Discussion Paper, No. 8894.
Carriero, A., Kapetanios, G., & Marcellino, M. (2011). Forecasting large datasets with bayesian
reduced rank multivariate models. Journal of Applied Econometrics, 26(5), 735–761.
Carter, C. K. & Kohn, R. (1994). On gibbs sampling for state space models. Biometrika, 81(3),
541–553.
Chan, J. C. & Jeliazkov, I. (2009). Efﬁcient simulation and integrated likelihood estimation in state
space models. International Journal of Mathematical Modelling and Numerical Optimisation,
1(1), 101–120.
Choi, S. (2013). Are the effects of bloom’s uncertainty shocks robust? Economics Letters, 119(2),
216–220.
Christiano, L. J., Motto, R., & Rostagno, M. (2014). Risk shocks. The American Economic Review,
104(1), 27–65.
Ciccarelli, M. & Rebucci, A. (2003). Measuring contagion with a bayesian, time-varying coefﬁ-
cient model. IMF working paper.
141

Cogley, T., Primiceri, G. E., & Sargent, T. J. (2010). Inﬂation-gap persistence in the u.s. American
Economic Journal: Macroeconomics, 2(1), 43–69.
Cogley, T. & Sargent, T. J. (2001). Evolving post-world war ii u.s. inﬂation dynamics. NBER
Macroeconomics Annual, 16(1), 331–373.
Cogley, T. & Sargent, T. J. (2005). Drifts and volatilities: monetary policies and outcomes in the
post wwii u.s. Review of Economic dynamics, 8(2), 262–302.
De Mol, C., Giannone, D., & Reichlin, L. (2008). Forecasting using a large number of predictors:
Is bayesian shrinkage a valid alternative to principal components?
Journal of Econometrics,
146(2), 318–328.
De Wind, J. & Gambetti, L. (2014).
Reduced-rank time-varing vector autoregressions.
CPB
Discussion paper, N0. 270, CPB Netherlands Bureau for Economic Policy Analysis.
Del Negro, M. & Otrok, C. (2008). Dynamic factor models with time-varying parameters: mea-
suring changes in international business cycles. FRB of New York Staff Report, No.326.
Del Negro, M. & Primiceri, G. E. (2013). Time-varying structural vector autoregressions and
monetary policy: a corrigendum. FRB of New York Staff Report, No.619.
Doan, T., Litterman, R., & Sims, C. (1984). Forecasting and conditional projection using realistic
prior distributions. Econometric reviews, 3(1), 1–100.
Fernandez-Villaverde, J., Rubio-Ramirez, J. F., Sargent, T. J., & Watson, M. W. W. (2007). Abcs
(and ds) of understanding vars. American Economic Review, 97(3), 1021–1026.
Forni, M., Hallin, M., Lippi, M., & Reichlin, L. (2000). The generalized dynamic-factor model:
Identiﬁcation and estimation. Review of Economics and statistics, 82(4), 540–554.
Forni, M. & Reichlin, L. (1998). Let’s get real: a factor analytical approach to disaggregated
business cycle dynamics. The Review of Economic Studies, 65(3), 453–473.
142

George, E. I., Sun, D., & Ni, S. (2008). Bayesian stochastic search for var model restrictions.
Journal of Econometrics, 142(1), 553–580.
Geweke, J. (1977). The dynamic factor analysis of economic time series. Amsterdam: North-
Holland.
Giacomini, R. (2013). The relationship between dsge and var models. Advances in Econometrics,
31.
Gilchrist, S., Sim, J. W., & Zakrajšek, E. (2014). Uncertainty, ﬁnancial frictions, and investment
dynamics. NBER Working paper, No.20038.
Gupta, R., Jurgilas, M., Kabundi, A., & Miller, S. M. (2012). Monetary policy and housing sec-
tor dynamics in a large-scale bayesian vector autoregressive model. International Journal of
Strategic Property Management, 16(1), 1–20.
Hanson, M. S. (2006). Varying monetary policy regimes: A vector autoregressive investigation.
Journal of Economics and Business, 58(5), 407–427.
Hartmann, P., Hubrich, K., Kremer, M., & Tetlow, R. J. (2014). Melting down: Systemic ﬁnancial
instability and the macroeconomy. Available at SSRN 2462567.
Jacquier, E., Polson, N. G., & Ross (1994). Bayesian analysis of stochastic volatility models.
Journal of Business & Economic Statistics, 12(4), 371–389.
Kadiyala, K. & Karlsson, S. (1997). Numerical methods for estimation and inference in bayesian
var models. Journal of Applied Econometrics, 12(2), 99–132.
Kamber, G., Karagedikli, O., Ryan, M., & Vehbi, T. (2013). International spill-overs of uncertainty
shocks: Evidence from a favar. Reserve Bank of New Zealand, mimeo.
Kim, C.-J. & Nelson, C. R. (1999). State space models with regime switching: classical and
gibbs-sampling approach with applications. MIT Press.
143

Kim, D. & Yamamoto, Y. (2013). Time instability of the u.s. monetary system: Multiple break
tests and reduced rank tvp-var. Discussion Paper.
Kim, S., Shephard, N., & Chib, S. (1998). Stochastic volatility: likelihood inference and compari-
son with arch models. The Review of Economic Studies, 65(3), 361–393.
Koop, G. & Korobilis, D. (2010). Bayesian multivariate time series methods for empirical macroe-
conomics. Foundations and trends(R) in Econometrics, 3(4), 267–358.
Koop, G. & Korobilis, D. (2013). Large time-varying parameter vars. Journal of Econometrics,
177(2), 185–198.
Koop, G., Leon-Gonzalez, R., & Strachan, R. W. (2009). On the evolution of the monetary policy
transmission mechanism. Journal of Economic Dynamics and Control, 33(4), 997–1017.
Koop, G. M. (2013). Forecasting with medium and large bayesian vars. Journal of Applied Econo-
metrics, 28(2), 177–203.
Korobilis, D. (2013a). Assessing the transmission of monetary policy using time-varying parameter
dynamic factor models. Oxford Bulletin of Economics and Statistics, 75(2), 157–179.
Korobilis, D. (2013b).
Var forecasting using bayesian variable selection.
Journal of Applied
Econometrics, 28(2), 204–230.
Kuo, L. & Mallick, B. (1998). Variable selection for regression models. Sankhy¯a: The Indian
Journal of Statistics, Series B, 60(1), 65–81.
Leeper, E. M. & Zha, T. (2003). Modest policy interventions. Journal of Monetary Economics,
50(8), 1673–1700.
Litterman, R. B. (1986). A statistical approach to economic forecasting. Journal of Business &
Economic Statistics, 4(1), 1–4.
144

Lubik, T. A. & Schorfheide, F. (2004). Testing for indeterminacy: an application to u.s. monetary
policy. American Economic Review, 94(1), 190–217.
Morris, S. D. (2012). Var (1) representation of dsge models. University of California, San Diego
working paper.
Mumtaz, H. & Surico, P. (2013). Policy uncertainty and aggregate ﬂuctuations. CEPR Discussion
Paper, No.8894.
Mumtaz, H. & Thedoridis, K. (2012). The international transmission of volatility shocks. Bank of
England Working paper, No. 463.
Mumtaz, H. & Thedoridis, K. (2014). The changing transmission of uncertainty shocks in the u.s.:
an empirical analysis. Working paper, No. 735, Queen Mary University of London, School of
Economics and Finance.
Mumtaz, H. & Zanetti, F. (2013). The impact of the volatility of monetary policy shocks. Journal
of Money, Credit and Banking, 45(4), 535–558.
Omori, Y., Chib, S., Shephard, N., & Nakajima, J. (2007). Stochastic volatility with leverage: Fast
and efﬁcient likelihood inference. Journal of Econometrics, 140(2), 425–449.
Primiceri, G. E. (2005). Time varying structural vector autoregressions and monetary policy. The
Review of Economic Studies, 72(3), 821–852.
Ravenna, F. (2007). Vector autoregressions and reduced form representations of dsge models.
Journal of monetary economics, 54(7), 2048–2064.
Romer, C. D. (1990). The great crash and the onset of the great depression. The Quarterly Journal
of Economics, 105(3), 597–624.
Sargan, J. D. & Bhargava, A. (1983). Maximum likelihood estimation of regression models with
ﬁrst order moving average errors when the root lies on the unit circle. Econometrica: Journal
of the Econometric Society, 51(3), 799–820.
145

Shephard, N. G. & Harvey, A. C. (1990). On the probability of estimating a deterministic compo-
nent in the local level model. Journal of time series analysis, 11(4), 339–347.
Sims, C. A. (1980). Macroeconomics and reality. Econometrica: Journal of the Econometric
Society, 48(1), 1–48.
Sims, C. A. (1999). Drift and breaks in monetary policy. Manuscript, Princeton University.
Sims, C. A. (2001). Stability and instability in u.s. monetary policy behavior. Princeton University,
photocopy.
Sims, C. A. & Zha, T. (2006). Were there regime switches in us monetary policy? The American
Economic Review, 96(1), 54–81.
Stock, J. H. & Watson, M. W. (1998). Asymptotically median unbiased estimation of coefﬁcient
variance in a time varying parameter model. Journal of the American Statistical Association,
93(441), 349–358.
Stock, J. H. & Watson, M. W. (1999). Forecasting inﬂation. Journal of Monetary Economics,
44(2), 293–335.
Stock, J. H. & Watson, M. W. (2002a). Has the business cycle changed and why? NBER Macroe-
conomics Annual, 17, 159–230.
Stock, J. H. & Watson, M. W. (2002b). Macroeconomic forecasting using diffusion indexes. Jour-
nal of Business & Economic Statistics, 20(2), 147–162.
Stock, J. H. & Watson, M. W. (2005). Implications of dynamic factor models for var analysis.
NBER Working paper, No.11467.
Taheri Sanjani, M. (2014). Financial frictions in data: Evidence and impact. IMF working paper.
Uhlig, H. (2005). What are the effects of monetary policy on output? results from an agnostic
identiﬁcation procedure. Journal of Monetary Economics, 52(2), 381–419.
146

