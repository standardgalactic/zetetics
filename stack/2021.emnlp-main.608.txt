Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7699–7715
November 7–11, 2021. c⃝2021 Association for Computational Linguistics
7699
Constrained Language Models Yield Few-Shot Semantic Parsers
Richard Shin, Christopher H. Lin, Sam Thomson, Charles Chen,
Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls,
Dan Klein, Jason Eisner, Benjamin Van Durme
Microsoft Semantic Machines
sminfo@microsoft.com
Abstract
We explore the use of large pretrained lan-
guage models as few-shot semantic parsers.
The goal in semantic parsing is to generate a
structured meaning representation given a nat-
ural language input. However, language mod-
els are trained to generate natural language.
To bridge the gap, we use language models
to paraphrase inputs into a controlled sublan-
guage resembling English that can be automat-
ically mapped to a target meaning representa-
tion. Our results demonstrate that with only
a small amount of data and very little code to
convert into English-like representations, our
blueprint for rapidly bootstrapping semantic
parsers leads to surprisingly effective perfor-
mance on multiple community tasks, greatly
exceeding baseline methods also trained on the
same limited data.
1
Introduction
Large pretrained language models (LMs) like GPT-
3 (Brown et al., 2020) have shown increasingly
impressive few-shot performance by formulating
tasks as text-to-text generation problems (Raffel
et al., 2020; Brown et al., 2020). Given only a
trained LM and a short textual prompt that de-
scribes and/or exempliﬁes a task, one can produce
surprisingly accurate models for a variety of natu-
ral language processing problems. However, task-
speciﬁc semantic parsing does not naturally ﬁt into
this paradigm because such parsers typically use
custom meaning representations that are unlikely
to already exist on the web, let alone exist in large
enough quantities to affect the parameters of these
LMs. We leverage two key insights to overcome
this barrier: (1) since LMs excel at generating natu-
ral language, we should formulate semantic parsing
as paraphrasing into a controlled sublanguage (Be-
rant and Liang, 2014; Marzoev et al., 2020) and (2)
autoregressive LMs can be efﬁciently constrained
to search over only valid paraphrases, so the sub-
language does not need to be learned from scratch.
In particular, following Berant and Liang (2014),
we envision a developer for some new domain
ﬁrst writing a synchronous context-free grammar
(SCFG) that deﬁnes the space of supported (and
well-formed) meaning representations along with
canonical natural language constructions that ex-
press them. Such a grammar maps between canon-
ical natural language forms and domain-speciﬁc
meaning representations, so that a separate LM-
based system can focus entirely on mapping an
unconstrained utterance u to a canonical (but still
natural) form c. Furthermore, the grammar can
be used to constrain this LM-based system so that
the LM is only allowed to generate canonical ut-
terances (i.e., utterances that correspond to well-
formed meaning representations).
Given such a grammar, an LM, and a handful
of examples for priming the LM for the task of in-
terest, our approach immediately yields a working
semantic parser. While we do not expect the accu-
racies of our models to reach state-of-the-art perfor-
mance when compared to models trained on large
amounts of task-speciﬁc examples, the ability to
rapidly prototype semantic parsers in new domains
can be immensely helpful for developers, both by
facilitating quick construction of a minimum vi-
able product and by enabling the bootstrapping
of new data collection through human-in-the-loop
processes (Duan et al., 2016).
We report results on the Overnight (Wang
et al., 2015), Break (Wolfson et al., 2020) and
SMCalFlow (Semantic Machines et al., 2020)
datasets1 using GPT-2 (Radford et al., 2019),
GPT-3 (Brown et al., 2020), and BART (Lewis
et al., 2020) as the underlying LMs.
Our re-
sults demonstrate that our solution:
(1) deliv-
ers greater accuracy when LMs target natural
language-like representations, (2) is further im-
proved through the use of explicit decoder con-
1Each of these preexisting datasets uses English inputs, but
their output representations vary.

7700
Canonical Utterance
What time am I brewing coffee with Megan and Megan and Megan?
Language Model
SCFG
Language Model
Constrained Decoding
Meaning Representation
Natural Utterance
Figure 1: Our proposed workﬂow for semantic parsing with a pretrained language model. Given a few examples
(not shown) and a natural user utterance (blue, italic), a pretrained language model generates paraphrased utterances
(purple). A grammar constrains the search over paraphrases to only canonical utterances, and the highest-scoring
canonical paraphrase is mechanically converted to a task-speciﬁc meaning representation (pink).
straints; and (3) performs surprisingly well with
very few examples, suggesting a new frontier for
rapidly prototyping semantic parsers. The code
and grammars developed in this work are pub-
licly available at https://github.com/microsoft/
semantic_parsing_with_constrained_lm.
2
Background
Autoregressive Language Models.
A language
model deﬁnes an (estimated) probability distribu-
tion over sequences of tokens w = w1, . . . , wn.
Autoregressive LMs factorize this distribution as:
p(s) =
n
Y
i=1
p(wi | w1, . . . , wi−1).
(1)
Unlike a cloze model such as BERT (Devlin et al.,
2019), an LM enables text generation, and an au-
toregressive LM makes it efﬁcient to generate in-
crementally. LMs like GPT-2 (Radford et al., 2019)
and GPT-3 (Brown et al., 2020) are trained by max-
imizing their likelihood on large web corpora.
It has been shown that autoregressive LMs are
powerful at performing tasks not obviously con-
nected to pure language modeling. For example,
Raffel et al. (2020) showed that an LM was able to
extend the prompt “Translate English to German:
That is good.” with the correct translation “Das ist
gut.” Brown et al. (2020) used “few-shot” prompts
that included several examples of inputs followed
by target outputs, with the actual task input ap-
pended at the end. In both cases, the task deﬁned
by the prompt is carried out by asking the language
model to generate the subsequent text. Even with-
out task-speciﬁc ﬁne-tuning, this approach has al-
ready yielded reasonable results (see e.g., Radford
et al., 2018; Brown et al., 2020; Gao et al., 2020).
This has wide implications, indicating we may
be able to carry out various tasks simply by design-
ing the prompts that we feed to pretrained LMs,
removing the expense of training task-speciﬁc mod-
els. There already exist multiple approaches to
prompt design, like choosing appropriate examples
to include in the prompt (e.g., Liu et al., 2021a)
or reformulating the prompts into more human-
friendly forms (i.e., closer to natural language;
Schick and Sch¨utze, 2020a). More related to our
work, prompt-guided semantic parsing relates to
ideas in example-based machine translation dating
back to work by Nagao (1984), that have been re-
cently revisited in the context of semantic parsing
with retrieve-and-edit by Hashimoto et al. (2018).
Fine-tuning can still be used with these mod-
els to perform various tasks (Li and Liang, 2021;
Liu et al., 2021b; Schick and Sch¨utze, 2020b). Al-
though ﬁne-tuning requires additional training, the
ﬁne-tuned system can be more efﬁcient at inference
time, as it is no longer necessary to select training
examples to precede the test input.
Semantic Parsing as Paraphrasing.
We adopt
the insight from Berant and Liang (2014) that se-
mantic parsing can make use of triples (natural
utterance u, canonical utterance c, meaning repre-
sentation m), where the parser maps u 7→c 7→m.
By design, it is easy to map c 7→m and vice-versa.
Our innovation is to prompt and constrain an LM
so as to make it map u 7→c. This approach can
exploit newly available large pretrained LMs.
Previous work in parsing as paraphrase has not
used generative LMs for the u 7→c step. Rather, it
has mapped u 7→c by obtaining candidate c values
in some way and then scoring them according
to whether they paraphrase u, using a semantic
equivalence model that scores (u, c) pairs. For

7701
example, Berant and Liang (2014) mapped from u
directly to many candidate meanings m, and then
evaluated the corresponding canonical utterances
c against u.
Wang et al. (2015) and Marzoev
et al. (2020) generated candidate c values (along
with their meanings m) from a grammar of legal
canonical utterances, but incrementally ﬁltered
the bottom-up or top-down generation by scoring
the partial candidates against u. Our procedure
swaps the roles of the grammar and u. We use u
to generate the candidate c values by prompting a
large LM with u, and then incrementally ﬁlter the
left-to-right generation by assessing whether the
partial candidates ﬁt the canonical grammar. This
places the LM in the driver’s seat. The large LM
that we use for paraphrase generation is trained
on much more data than the specialized paraphrase
scoring models used in prior work.
Bootstrapping a Semantic Parser.
One line
of prior work on quickly bootstrapping a se-
mantic parser has focused on creating synthetic
training examples from a grammar developed by
hand (Campagna et al., 2019; Weir et al., 2020;
Marzoev et al., 2020; Campagna et al., 2020) or
derived automatically from existing data (Jia and
Liang, 2016; Yu et al., 2020). Wang et al. (2015)
described an approach to bootstrapping that uses
a grammar to generate canonical forms, which are
paraphrased by crowdworkers to produce training
data “overnight.” Xu et al. (2020) extended this
work by generating paraphrases for training data
by ﬁltering examples generated from a grammar.
In this paper we take the approach of using the
grammar as a constraint, with an eye towards en-
abling bootstrapping through human-in-the-loop
semantic parsing, where humans quickly annotate
data by manually correcting parses from an initial
prototype (Duan et al., 2016; He et al., 2016; Yao
et al., 2019; Elgohary et al., 2021). With this mo-
tivation in mind we report accuracy at K, deﬁned
as the rate in which an annotator would ﬁnd the
correct parse when selecting among K options.
3
Approach
We propose a method for semantic parsing using
large pre-trained LMs that requires little to no
task-speciﬁc training.
For the prompt-based
few-shot setting, we use the 175-billion-parameter
GPT-3 model (Brown et al., 2020) as our LM
because at the time of writing it was the largest
available LM that provided an accessible API.2
Our goals are to show the approach is good enough
to be practical, and to conﬁrm our claim that large
LMs are better used to generate text that looks
more like natural language rather than an artiﬁcial
programming language.
Our approach consists of two parts: (1) LM prim-
ing, either through dynamic prompt creation or
ﬁne-tuning, and (2) constrained decoding, ensuring
well-formed output under the target representation.
Dynamic Prompt Creation.
The prompt we
feed to GPT-3 is designed so that it contains a small
representative set of examples mapping utterances
to their desired outputs. As mentioned in §1, we tar-
get rapid prototyping and so, for each task that we
tackle we assume access to 1,000 or fewer training
examples. Each example is a pair (ui, ti) where
ui is an utterance and ti is the target output for
that utterance, speciﬁed as either the original mean-
ing representation, mi, or our canonical linguistic
representation, ci, which can then be translated to
mi. Given a test input utterance u = “how long is
the weekly standup”, for example, a dynamically
constructed prompt looks something like:
Let’s translate what a human user says into
what a computer might say.
Human: when is the weekly standup
Computer: start time of weekly standup
Human: what date is the weekly standup
Computer: date of weekly standup
...
Human: how long is the weekly standup
Computer:
Intuitively, we want the examples used to be similar
to the test utterance u so GPT-3 can learn how to
generate the target output based on just the prompt.
We propose to also use GPT-3 for selecting the
examples to include in the prompt. Consider a train-
ing example, (ui, ti). We quantify its relevance to
the test input u as p(u | ui), computed directly
using GPT-3.3 For each test utterance u, we sort all
training examples by this metric, and construct the
prompt from the P most relevant examples. Note
that the GPT-3 API accepts at most 2,048 tokens
(after sub-word tokenization) and thus, if using P
exceeds this limit, we reduce P accordingly. For
2https://openai.com/blog/openai-api.
3During development we also considered using S-
RoBERTa (Reimers and Gurevych, 2019) and LASER
(Artetxe and Schwenk, 2019) for estimating relevance in-
stead of GPT-3, but we did not observe differences signiﬁcant
enough to motivate the additional complexity.

7702
example, to generate a 40-token output we need to
limit the prompt size to 2,008 tokens.
Fine-tuning.
An alternative to few-shot prompt-
ing is to ﬁne-tune the LM on each task using just
the utterance as input. Since the GPT-3 API avail-
able to us does not support ﬁne-tuning, we use the
next largest model of the same type, GPT-2 XL.4
We also ﬁne-tune BART (Lewis et al., 2020), a
pretrained sequence-to-sequence model with a bidi-
rectional encoder and autoregressive decoder. As
BART is trained to generate sentences given cor-
rupted versions of those sentences, it is perhaps
particularly suited for generating paraphrases.
We use the same set of examples to ﬁne-tune that
we would otherwise use as candidates for prompt
creation, ﬁne-tuning an LM to do well at mapping
utterance ui to the target output ti; no other ex-
amples are included in the prompt. When the tar-
get is a structured representation, this amounts to
sequence-to-sequence semantic parsing. When the
target output is natural language, this might be
called text rewriting or sentential paraphrasing.
Constrained Decoding.
The input to the LM is a
prompt p, which always contains the utterance u to
be parsed. In the non–ﬁne-tuned case it is preceded
by dynamically constructed examples as described
above. Given p, we use an LM to generate a contin-
uation t and take this as the output. As mentioned
in §2, we assume that each target task speciﬁes a
way to constrain the generated continuation to guar-
antee a well-formed output for that task. Formally,
we assume that each task provides a nextTokens
function which, for any token sequence s, returns
the set of all tokens that can immediately follow s
in the target output language. We then use the LM
to produce the output t by extending the prompt p
using a length-normalized variant of beam search
(Murray and Chiang, 2018; Wu et al., 2016). At
each step of the search, we ﬁlter the set of valid
continuations using nextTokens.
4
Case Studies
In the following sections we present multiple case
studies to evaluate our approach. Each studies a
different task and follows the same workﬂow: a
Deﬁnition of the task and the meaning representa-
tion it uses; a Framing of the representation into our
4We tried using GPT-2 with few-shot prompts and no ﬁne-
tuning but the results were sufﬁciently poor that we did not
explore further.
proposal, including a description of nextTokens;
an Experimental Setup with task-speciﬁc details;
and Results, where our experiments evaluate our
ability to predict the original meaning representa-
tion m, either as u 7→m or as u 7→c 7→m.
4.1
Overnight
Deﬁnition.
Wang et al. (2015) constructed the
Overnight semantic parsing dataset, which con-
tains a total of 13,682 examples across eight dif-
ferent domains exhibiting a variety of linguistic
phenomena and semantic structures. The underly-
ing task aims to map natural language utterances
to database queries.
The authors initially gen-
erated pairs (ci, mi) of canonical utterances and
corresponding queries (in the form of Lisp-like
S-expressions) using a hand-crafted SCFG. They
then used crowdsourcing to paraphrase each ci into
a more natural-sounding utterance ui. An exam-
ple ui is shown below, followed by the canonical
representation ci and meaning representation mi:
which january 2nd meetings is alice attenting [sic]
meeting whose date is jan 2 and whose attendee is alice
(call listValue (call ﬁlter
(call ﬁlter (call getProperty
(call singleton en.meeting) (string !type))
(string date) (string =) (date 2015 1 2))
(string attendee) (string =) en.person.alice))
The resulting (ui, ci, mi) triples were used to train
a semantic parser that mapped u 7→(c, m).
Framing.
The publicly available release of the
Overnight dataset conveniently contains all of the
(ci, mi) pairs generated by enumerating SCFG
derivation trees up to a certain depth. For some
of these, the natural language paraphrase ui is also
available. For these, we can directly use mi as
the meaning representation for our setup, and ci
as the canonical representation. Furthermore, we
implement the nextTokens function from §3 by
building a large trie that contains all of the ci or mi
strings (depending on whether our experimental
system is attempting to map u 7→c or u 7→m).
This trie allows us to quickly look up all the ways
in which a valid preﬁx of a (depth-limited) c or m
string can be extended to produce a longer valid
preﬁx. In the case of m, it enforces not only syn-
tactic well-formedness but also type safety.
Experimental Setup.
For each domain, we
simulate the low-data prototyping regime by using
only 200 training examples, randomly selected
from the 640–3,535 examples provided in the

7703
Model
Train n Basketball Blocks Calendar Housing Publications Recipes Restaurants Social
GPT-3 Constrained Canonical
200
0.859
0.634
0.792
0.741
0.776
0.792
0.840
0.687
BARTf Constrained Canonical
200
0.847
0.581
0.845
0.725
0.758
0.773
0.831
0.731
GPT-2f Constrained Canonical
200
0.836
0.549
0.804
0.640
0.752
0.787
0.762
0.726
Cao et al. (2019)
200
0.772
0.429
0.613
0.550
0.696
0.671
0.639
0.566
Cao et al. (2019)
640–3535
0.880
0.652
0.807
0.767
0.807
0.824
0.840
0.838
BERT-LSTM (Xu et al., 2020)
640–3535
0.875
0.624
0.798
0.704
0.764
0.759
0.828
0.819
AutoQA (Xu et al., 2020)
0†
0.739
0.549
0.726
0.709
0.745
0.681
0.786
0.615
Table 1: Denotation accuracies on Overnight. f indicates models that have been ﬁne-tuned on the training examples.
For results above the line, we use n = 200 randomly-sampled training examples; the ﬁrst three lines are our
systems, while for Cao et al. (2019), we ran their training code on the same 200. The results below the line come
from prior work using many more training examples. †AutoQA was trained on a large set of >400,000 synthetic
utterances u created from Overnight’s canonical utterances by automated paraphrasing.
Model
Train n Basketball Blocks Calendar Housing Publications Recipes Restaurants Social
GPT-3 Constrained Canonical
200
0.80*
0.62*
0.82*
0.71*
0.79*
0.84*
0.89*
0.72*
GPT-3 Constrained Meaning
200
0.68*
0.53*
0.68*
0.58*
0.63*
0.75*
0.78*
0.63*
GPT-3 Unconstrained Canonical
200
0.76*
0.46*
0.68*
0.56*
0.58*
0.74*
0.74*
0.55*
GPT-3 Unconstrained Meaning
200
0.56*
0.39*
0.50*
0.42*
0.46*
0.66*
0.58*
0.48*
GPT-3 Constrained Canonical
20
0.80*
0.55*
0.67*
0.68*
0.81*
0.60*
0.76*
0.67*
BARTf Constrained Canonical
200
0.85
0.58
0.85
0.73
0.76
0.77
0.83
0.73
BARTf Constrained Meaning
200
0.83
0.56
0.77
0.75
0.79
0.76
0.81
0.69
BARTf Unconstrained Canonical
200
0.83
0.56
0.80
0.67
0.72
0.75
0.81
0.65
BARTf Unconstrained Meaning
200
0.82
0.55
0.76
0.71
0.77
0.73
0.80
0.63
Table 2: Variations of our method using GPT-3 and BART. “*” denotes accuracies computed on a smaller test set
randomly sampled from the full set due to the computational cost of using GPT-3.
Overnight training set; with GPT-3, we also try 20
training examples as a more extreme case. For each
evaluation example, we create the GPT-3 prompt
by selecting up to P = 20 training examples.
When using constrained decoding, we perform
beam search with a beam size of 10. For uncon-
strained decoding with GPT-3, we use the API
to greedily sample (using a softmax temperature
of 0.0) from the prompt until we reach a newline
character; we also try beam search with beam size
10, but to save on computation costs, we do so
only for the calendar domain. For parity, we report
results using greedy search for unconstrained
decoding with models other than GPT-3.
Results.
Table 1 shows our main results on the
full test sets in Overnight. As in prior work we com-
pute the denotation accuracy, checking whether
execution of the predicted m against a database
returns the gold answer, rather than exact match
accuracy. We compare against the current state-of-
the-art method from Cao et al. (2019) also trained
on only 200 examples (see Appendix D.1 for de-
tails).
Table 1 also includes results using all training ex-
amples, from Cao et al. (2019) and Xu et al. (2020);
and AutoQA, which uses only synthetic utterances
created by automatically paraphrasing the canon-
1
2
3
4
5
6
7
8
9
10
K
0.6
0.7
0.8
0.9
1.0
Denotation accuracy
CC (200)
CM (200)
UC (200)
UM (200)
CC (20)
Figure 2: Denotation accuracy @K on the Calendar
subdomain of Overnight with GPT-3. Unlike Table 1,
all conditions use beam search; Constrained Canoni-
cal (CC), Constrained Meaning (CM), Unconstrained
Canonical (UC), and Unconstrained Meaning (UM),
using (200) or (20) training examples.
ical utterances. On some of the domains, such as
Calendar, Housing, and Restaurants, we obtain sim-
ilar numbers as the state-of-the-art approach using
7 to 13 times less training data.
Our method with GPT-3 performs the best
among models trained on only 200 examples, ap-
proaching the performance of the models trained on
all training examples. BART and GPT-2, when ﬁne-
tuned on the 200 examples, also perform quite well.
BART outperforms GPT-2 despite having fewer
parameters, suggesting that its denoising training
objective is particularly effective for paraphrasing.

7704
Given that ﬁne-tuning was necessary for decent
performance for GPT-2, we expect that ﬁne-tuning
GPT-3 may improve its performance even further –
when it becomes practical to do so.
Table 2 shows that both constrained decoding
and the use of English-like canonical utterances
rather than Lisp-like logical forms substantially
increases the accuracy. This same pattern holds for
BART and GPT-2 as well. Using only 20 training
examples generally decreases accuracy by a modest
amount, but surprisingly not on all domains.
Figure 2 shows accuracy@K on the calendar do-
main, where the GPT-3 parser is scored as correct
on an input if any output in its top K hypotheses is
correct. The accuracy@5 of Constrained Canonical
is 0.98, even though this is only a rapid prototype
trained on 200 examples.
4.2
Break
Deﬁnition.
Break (Wolfson et al., 2020) pairs
natural language questions with programs in the
question decomposition meaning representation
(QDMR). Each program is a sequence of database
queries in a controlled natural language, where
each query can use the return values of previ-
ous queries. The utterances u are questions sam-
pled from many existing language understanding
datasets.5 Crowdworkers decomposed each ques-
tion ui into a sequence mi of queries speciﬁed as
strings. The string of each step was restricted to: (i)
words and their inﬂections appearing in the ques-
tions, (ii) 66 pre-deﬁned function words (e.g., “if”,
“on”, or “for each”), and (iii) tokens that refer to
results from the previous step. This resulted in
44,321 train, 7,760 development, and 8,069 test
examples. An example is shown below, including
our canonical representation (deﬁned next) and the
QD meaning representation:
What color are a majority of the objects?
(colors of (objects)) where (number of (objects for each
(colors of (objects))) is highest)
1. objects
2. colors of #1
3. number of #1 for each #2
4. #2 where #3 is highest
Framing.
For our canonical representation ci,
we mechanically and invertibly map the QDMR
5Break covers semantic parsing (Price, 1990; Zelle and
Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018), reading
comprehension (Talmor and Berant, 2018; Yang et al., 2018;
Dua et al., 2019; Abujabal et al., 2019), and visual question
answering (Johnson et al., 2017; Suhr et al., 2019).
Model
Train n nem
Wolfson et al.
44,321 0.42
Coleman & Reneau
44,321 0.42
GPT-3 Constrained Canonical
1,000 0.32*
GPT-3 Constrained Canonical
100 0.24*
GPT-3 Constrained Canonical
25 0.20*
GPT-3 Constrained Canonical
200 0.31*
GPT-3 Constrained Meaning
200 0.24*
GPT-3 Unconstrained Canonical
200 0.20*
GPT-3 Unconstrained Meaning
200 0.17*
GPT-3 Constrained Canonical
200 0.24
BARTf Constrained Canonical
200 0.22
BARTf Constrained Meaning
200 0.22
BARTf Unconstrained Canonical
200 0.18
BARTf Unconstrained Meaning
200 0.19
Table 3: NEM accuracy on the Break dataset, where n
is the number of training examples used in each case.
Entries drawn from the task leaderboard are included
as reference points. “*” denotes accuracies on a random
sample on validation. f indicates ﬁne-tuned models.
mi into a single-line format that more closely re-
sembles a detailed English request, as illustrated
above. We implement nextTokens by restricting
the allowed tokens to: (i) words or their inﬂections
that appear in the questions, (ii) the pre-deﬁned set
of function words, and (iii) opening and closing
parentheses. A string is considered valid if its to-
kens belong to one of these three categories, and
any parentheses used are balanced.
Experimental Setup.
The Break leaderboard6
reports four metrics, with a focus on normalized ex-
act match accuracy (NEM), deﬁned as exact match
accuracy after QDMR canonicalization. All four
metrics followed consistent relative trends in our
experiments; we focus on NEM for brevity and clar-
ity. We sampled n ∈{25, 100, 200, 1000} items
uniformly at random from the training set to simu-
late varying amounts of data in the low-data, rapid
prototyping regime. For each evaluation example,
we create the prompt by selecting up to P = 20 of
the n available training examples.
Results.
Table 3 shows the results.
Similar
to the ﬁrst case study (§4.1), we observe that
our Constrained Canonical approach obtains
competitive accuracy despite using relatively few
training examples. We can see that the canonical
representation is easier to predict than the meaning
representation, even though QDMR was already
designed to be more natural than the original rep-
resentations of the various Break datasets. We also
see that constrained decoding results in further im-
6https://leaderboard.allenai.org/break.

7705
1
2
3
4
5
6
7
8
9
10
K
0.25
0.30
0.35
0.40
0.45
NEM
CC (1000)
CM (1000)
CC (200)
CM (200)
Figure 3: NEM @K on a sample from Break for GPT-3
Constrained Canonical (CC) and Constrained Meaning
(CM) with 200 training examples.
provements, leading to gains of 7–11% in absolute
accuracy. All our methods outperform a standard
seq2seq baseline (BART Unconstrained Meaning)
trained to predict the meaning representation on
the same number of training examples. For con-
strained decoding of canonical utterances, we see
steady improvements as we increase the number
of training examples from 25 up to 1,000. Figure 3
illustrates accuracy over the top-K predictions of
the model, with results consistent with §4.1.
4.3
SMCalFlow
Deﬁnition.
SMCalFlow (Semantic Machines
et al., 2020) is a large dataset for task-oriented
dialogue,
spanning
the
domains
of
events,
weather, places, and people. User utterances u
in SMCalFlow are paired with rich executable
dataﬂow programs m featuring API calls, function
composition, complex constraints, and references
to the programs from previous dialogue turns.
The dataset contains 133,821 training examples
(ui, mi). Examples have a history of previous dia-
logue turns, which we ignore here in order to align
our approach with the previous sections.7 The fol-
lowing example shows the canonical representation
(deﬁned next) and the meaning representation:
What did I set as my response status for the team meeting?
my response status of ﬁnd event called something like
“team meeting”
(Yield :output
(:responseStatus (singleton (:results
(FindEventWrapperWithDefaults
:constraint (Constraint[Event]
:subject (?∼= #(String “team meeting”))))))))
Framing.
Unlike the previous datasets, SM-
CalFlow does not come with a grammar. For such
a complex dataset, writing a grammar post-hoc that
7Ignoring dialogue history hurts performance relative to
prior work: history could be incorporated into a prompt in
future work that strives for state of the art.
can produce ﬂuent, natural English is challenging.
At the same time, SMCalFlow is representative
of the rich semantic parsing tasks our proposal is
meant to help rapidly prototype hence its inclusion.
In order to map between m and a canonical ut-
terance c, we built an SCFG over (c, m′) pairs,
where m′ is a transformed intermediate represen-
tation that is more SCFG-friendly than m (see Ap-
pendix A for details). While our transformation
and SCFG allow us to map m 7→m′ 7→c deter-
ministically (to construct training examples (ui, ci)
for the prompt), some simple guessing models are
required in the reverse direction c 7→m′ 7→m
(to convert GPT-3’s linguistic output to the desired
SMCalFlow representation), since our canonical
utterances c are occasionally ambiguous and since
m′ omits some information about coreferent nodes.
From this SCFG, we extract two CFGs that de-
ﬁne the well-formed sequences c and m′, respec-
tively. As we generate a preﬁx from left to right,
we incrementally parse it using Earley’s algorithm
(Earley, 1970). nextTokens inspects the state of
the incremental parser to return precisely the set of
next tokens that are allowed by the CFG.8
Experimental Setup.
We gather 300 training ex-
amples from the SMCalFlow training set through
stratiﬁed sampling (see Appendix C) to simulate
a scenario where examples of different kinds are
written by a domain developer in the course of de-
veloping annotation guidelines. We also uniformly
sample a set of 100 examples and use stratiﬁed
sampling for a set of 150 examples from the SM-
CalFlow validation set to assist in grammar devel-
opment and hyperparameter tuning. We use a beam
size of 10. For some GPT-3 experiments, we uni-
formly sample an evaluation set of 200 examples
from the SMCalFlow validation set.
Results.
Results are shown in Table 4 and Fig-
ure 4. Note that we always evaluate on the orig-
inal meaning representation. We ﬁnd similar rel-
ative differences as in previous tasks: targeting a
more natural representation and constraining the
decoding improves results. Our methods also sig-
niﬁcantly outperforms a standard sequence to se-
quence baseline (BART Unconstrained Meaning)
trained to predict meaning representations.
8To robustly handle tokenization mismatches between the
pretrained LM and grammar, we effectively transform the
grammar such that terminals are single characters. Details in
Appendix A.

7706
Model
Train n Accuracy
Semantic Machines et al. (2020)
133,821
0.73
GPT-3 Constrained Canonical
300
0.33*
GPT-3 Constrained Meaning
300
0.25*
GPT-3 Unconstrained Canonical
300
0.26*
GPT-3 Unconstrained Meaning
300
0.20*
GPT-3 Constrained Canonical
300
0.32
BARTf Constrained Canonical
300
0.42
BARTf Constrained Meaning
300
0.37
BARTf Unconstrained Canonical
300
0.40
BARTf Unconstrained Meaning
300
0.30
Table 4: Performance on SMCalFlow. “*” indicates
evaluation on a random sample of validation. f are ﬁne-
tuned models.
1
2
3
4
5
6
7
8
9
10
K
0.35
0.40
0.45
0.50
0.55
0.60
Exact Match
GPT-3 CC
BART CC
BART CM
Figure 4: Accuracy@K of three of our models on SM-
CalFlow.
More Data.
To analyze the effect of training
data size, we also evaluate our BART Constrained
Canonical model on stratiﬁed training set sizes of
1k, 10k, 50k and on the full SMCalFlow training
set (≈120k). For comparison, we also consider
the current state-of-the-art model on SMCalFlow
(VACSP; Platanios et al., 2021) in the same set-
tings. Figure 5 shows the results. Recall that for
all experiments we do not use any dialogue context
and so the performance of VACSP is lower than the
performance reported by Platanios et al. (2021).
Our proposed method outperforms VACSP in
300
1k
10k
50k 120k
# Train Examples
0.3
0.4
0.5
0.6
0.7
Accuracy
BART CC
VACSP
Figure 5: Accuracy of our best model on SMCalFlow
at different training set sizes, compared to the recent
state of the art model by Platanios et al. (2021).
low data regimes, further supporting the intuitions
behind our approach. With more training data the
beneﬁts of constrained decoding and an initialized
decoder become less important. Future work could
assess the relative impact of sampling examples
from the grammar for use in pretraining a model
such as VACSP, contrasting with using the same
grammar as constraints on paraphrase decoding.
5
Discussion
Empirically, we demonstrated that (i) constrained
decoding is better than unconstrained and (ii) con-
trolled natural languages are better than meaning
representations when used with a pre-trained LM.
The beneﬁt of (i) is readily observable because un-
constrained decoding can produce not-quite-correct
answers. For example, GPT-3 constrained decod-
ing maps the Overnight example show me any
meetings labeled as important which are also three
hours long to the correct canonical utterance meet-
ing that is important and whose length is three
hours, whereas unconstrained decoding yields the
non-canonical utterance meeting whose label is
important and whose length is three hours.
The effect of (ii) is harder to isolate, though we
found some suggestive examples, e.g., for the in-
put utterance meetings that are not attended by
alice, our method led to the correct meeting whose
attendee is not alice. In contrast, constrained pre-
diction of the meaning representation dropped the
negation (using = instead !=), producing the mean-
ing representation for meeting whose attendee is al-
ice and is important. We speculate that constrained
GPT-3 was more willing to preserve the input word
not than to produce !=. More impressively, in
Break, our method correctly interpreted the novel
bigram as many, mapping Are there as many matte
objects as metallic objects? to ((number of (matte
objects)) is same as (number of (metallic objects)).
In contrast, constrained prediction of the QDMR
led to the wrong predicate, whose canonical ut-
terance would be ((number of (matte objects)) is
higher than (number of (metallic objects)).
6
Further Related Work
Motivated by tasks where a user requires certain
phrases to be present or absent in the output of a
text generation system, researchers have explored
increasingly more efﬁcient approaches to restrict-
ing valid paths in beam search such that they satisfy
externally provided constraints (e.g., Hokamp and

7707
Liu, 2017; Anderson et al., 2017; Post and Vilar,
2018; Hu et al., 2019). Grammar-constrained de-
coding restricts some or all of a successful transduc-
tion path to result in a sequence parseable under
a grammar. Such techniques were used in task-
oriented speech recognition systems (Moore et al.,
1997),9 where it was assumed a user knew the pre-
cise way to phrase commands. In contemporary
settings we retain the notion of a parser support-
ing task-speciﬁc features, where we would like to
enjoy the beneﬁts of a grammar in terms of laying
out prescribed functionality but without constrain-
ing the user’s linguistic forms. Constraining neural
semantic parsing decoders has been explored by
Yin and Neubig (2017) and Krishnamurthy et al.
(2017), among others, for generating structured
forms rather than paraphrases. Herzig et al. (2021)
predict intermediate semantic representations with
stronger structural correspondence to natural lan-
guage than m, replacing the role of c in our ap-
proach with a modiﬁed meaning representation m′.
Like the closely related problem of machine
translation (Wong and Mooney, 2006; Andreas
et al., 2013), semantic parsing has recently been
driven by encoder-decoder neural architectures
(starting with Dong and Lapata, 2016; Jia and
Liang, 2016; Koˇcisk´y et al., 2016). More recently,
Chen et al. (2020) used pre-trained LMs, includ-
ing BART, to initialize both the encoder and the
decoder of a semantic parser. In concurrent work,
Desai et al. (2021) reports gains on Chen et al.
(2020) by modifying a target representation to be
more natural language-like. We argue that LMs
are better suited for generating natural language
directly rather than task-speciﬁc meaning represen-
tations, using experiments designed to contrast the
proﬁciency of LMs on these two output modalities.
Finally, Wu et al. (2021) concurrently proposed
a similar solution to our own. We independently
conﬁrm positive results on Overnight, with new
studies on Break and SMCalFlow. In contrast to
their primary focus on the unsupervised setting, our
experiments were largely concerned with the few-
shot scenario. We consider it reasonable to expect
small hundreds of examples from a domain expert
when building a real world parser, and our results
suggest that this obviates the concerns of Wu et al.
9Prior to recent advances it was believed that “practical
application of speech recognition technology requires a vo-
cabulary and grammar tailored to the particular application,
since for high accuracy the recognizer must be restricted as
to what sequences of words it will consider” – Moore et al.
on initially tuning a paraphrase model beyond what
current off-the-shelf pretraining methods provide.
7
Conclusion
We wish to rapidly develop semantic parsers in
new domains. To this end, we have demonstrated
that constrained decoding of powerful language
models can enable the paraphrasing of user utter-
ances into a controlled sublanguage, which may
then be mapped to a task-speciﬁc representation.
With small hundreds of examples we are able to
quickly bootstrap models for a variety of datasets,
enabling future work that explores human in the
loop interactions for iterative model reﬁnement.
References
Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed
Yahya, and Gerhard Weikum. 2019.
ComQA:
A community-sourced dataset for complex factoid
question answering with paraphrase clusters.
In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 307–317,
Minneapolis, Minnesota.
Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. 2017. Guided open vocabulary im-
age captioning with constrained beam search.
In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
936–945, Copenhagen, Denmark.
Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 47–52, Soﬁa, Bulgaria.
Mikel Artetxe and Holger Schwenk. 2019.
Mas-
sively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond.
Transac-
tions of the Association for Computational Linguis-
tics, 7:597–610.
Jonathan Berant and Percy Liang. 2014. Semantic pars-
ing via paraphrasing. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1415–
1425, Baltimore, Maryland.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,
Sandhini Agarwal,
Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Ben-
jamin Chess, Jack Clark, Christopher Berner, Sam

7708
McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. 2020.
Language models are few-
shot learners.
Computing Research Repository,
arXiv:2005.14165.
Giovanni Campagna, Agata Foryciarz, Mehrad Morad-
shahi, and Monica Lam. 2020. Zero-shot transfer
learning with synthesized data for multi-domain dia-
logue state tracking. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 122–132, Online.
Giovanni Campagna, Silei Xu, Mehrad Moradshahi,
Richard Socher, and Monica S. Lam. 2019. Genie:
A generator of natural language semantic parsers for
virtual assistant commands. In Proceedings of the
40th ACM SIGPLAN Conference on Programming
Language Design and Implementation (PLDI).
Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li, and Kai
Yu. 2019. Semantic parsing with dual learning. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 51–64,
Florence, Italy. Association for Computational Lin-
guistics.
Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke
Zettlemoyer, and Sonal Gupta. 2020. Low-resource
domain adaptation for compositional task-oriented
semantic parsing. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 5090–5100, Online. As-
sociation for Computational Linguistics.
Shrey Desai, Akshat Shrivastava, Alexander Zotov, and
Ahmed Aly. 2021. Low-resource task-oriented se-
mantic parsing via intrinsic modeling.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota.
Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Stanovsky, Sameer Singh, and Matt Gardner. 2019.
DROP: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 2368–2378, Min-
neapolis, Minnesota.
Manjuan Duan, Ethan Hill, and Michael White. 2016.
Generating disambiguating paraphrases for struc-
turally ambiguous sentences.
In Proceedings of
the 10th Linguistic Annotation Workshop held in
conjunction with ACL 2016 (LAW-X 2016), pages
160–170, Berlin, Germany. Association for Compu-
tational Linguistics.
Jay Earley. 1970. An efﬁcient context-free parsing al-
gorithm.
Communications of the ACM, 13(2):94–
102.
Ahmed Elgohary, Chris Meek, Matthew Richardson,
Adam Fourney, Gonzalo Ramos, and Ahmed H.
Awadallah. 2021.
NL-EDIT: Correcting semantic
parse errors through natural language interaction. In
NAACL 2021.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.
Making pre-trained language models better few-
shot learners.
Computing Research Repository,
arXiv:2012.15723.
Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren,
and Percy Liang. 2018. A retrieve-and-edit frame-
work for predicting structured outputs. In 32nd Con-
ference on Neural Information Processing Systems.
Luheng He, Julian Michael, Mike Lewis, and Luke
Zettlemoyer. 2016. Human-in-the-loop parsing. In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pages
2337–2342, Austin, Texas. Association for Compu-
tational Linguistics.
Jonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin
Guu, Panupong Pasupat, and Yuan Zhang. 2021. Un-
locking compositional generalization in pre-trained
models using intermediate representations.
Chris Hokamp and Qun Liu. 2017.
Lexically con-
strained decoding for sequence generation using grid
beam search.
In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1535–1546,
Vancouver, Canada.
J. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick
Xia,
Tongfei Chen,
Matt Post,
and Benjamin
Van Durme. 2019. Improved lexically constrained
decoding for translation and monolingual rewriting.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 839–850,
Minneapolis, Minnesota.
Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany.
Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Fei-Fei Li, C. Lawrence Zitnick, and Ross
Girshick. 2017.
Clevr: A diagnostic dataset for
compositional language and elementary visual rea-
soning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

7709
Tom´aˇs Koˇcisk´y, G´abor Melis, Edward Grefenstette,
Chris Dyer,
Wang Ling,
Phil Blunsom,
and
Karl Moritz Hermann. 2016. Semantic parsing with
semi-supervised sequential autoencoders.
In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1078–
1087, Austin, Texas.
Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1516–1526, Copen-
hagen, Denmark.
Mike
Lewis,
Yinhan
Liu,
Naman
Goyal,
Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871–7880, Online. Association
for Computational Linguistics.
Fei Li and H. V. Jagadish. 2014.
NaLIR: An inter-
active natural language interface for querying rela-
tional databases.
In International Conference on
Management of Data, SIGMOD.
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:
Optimizing continuous prompts for generation.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2021a. What
makes good in-context examples for GPT-3? Com-
puting Research Repository, arXiv:2101.06804.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt
understands, too.
Alana Marzoev, Samuel Madden, M. Frans Kaashoek,
Michael Cafarella, and Jacob Andreas. 2020. Unnat-
ural language processing: Bridging the gap between
synthetic and natural language data. In Proceedings
of the First Workshop on Natural Language Inter-
faces (NLI).
R. C. Moore, J. Dowding, H. Bratt, J. M. Gawron,
Y. Gorfu, and A. Cheyer. 1997.
CommandTalk:
A spoken-language interface for battleﬁeld simu-
lations.
In Fifth Conference on Applied Natural
Language Processing, pages 1–7, Washington, DC,
USA.
Kenton Murray and David Chiang. 2018. Correcting
length bias in neural machine translation. In Pro-
ceedings of the Third Conference on Machine Trans-
lation: Research Papers, pages 212–223, Brussels,
Belgium.
Makoto Nagao. 1984.
A Framework of Mechanical
Translation between Japanese and English by Anal-
ogy Principle. In A. Elithorn and R. Banerji, editors,
ARTIFICIAL AND HUMAN INTELLIGENCE, chap-
ter 11. Elsevier Science Publishers.
Emmanouil Antonios Platanios, Adam Pauls, Subhro
Roy, Yuchen Zhang, Alexander Kyte, Alan Guo,
Sam Thomson, Jayant Krishnamurthy, Jason Wolfe,
Jacob Andreas, and Dan Klein. 2021.
Value-
agnostic conversational semantic parsing.
In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 3666–
3681, Online. Association for Computational Lin-
guistics.
Matt Post and David Vilar. 2018. Fast lexically con-
strained decoding with dynamic beam allocation for
neural machine translation. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 1314–1324, New Orleans, Louisiana.
P. J. Price. 1990. Evaluation of spoken language sys-
tems: the ATIS domain. In Speech and Natural Lan-
guage: Proceedings of a Workshop Held at Hidden
Valley, Pennsylvania, June 24-27,1990.
Alec Radford, Karthik Narasimhan, Tim Salimans, ,
and Ilya Sutskever. 2018. Improving language un-
derstanding by generative pre-training. Technical re-
port.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. Techni-
cal report, OpenAI.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020.
Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search, 21(140):1–67.
Nils Reimers and Iryna Gurevych. 2019.
Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
3982–3992, Hong Kong, China. Association for
Computational Linguistics.
Timo Schick and Hinrich Sch¨utze. 2020a. Exploiting
cloze questions for few shot text classiﬁcation and
natural language inference.
Computing Research
Repository, arXiv:2001.07676.
Timo Schick and Hinrich Sch¨utze. 2020b. It’s not just
size that matters: Small language models are also
few-shot learners.
Semantic Machines, Jacob Andreas, John Bufe, David
Burkett, Charles Chen, Josh Clausman, Jean Craw-
ford, Kate Crim, Jordan DeLoach, Leah Dorner, Ja-
son Eisner, Hao Fang, Alan Guo, David Hall, Kristin
Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Sm-
riti Jha, Dan Klein, Jayant Krishnamurthy, Theo

7710
Lanman, Percy Liang, Christopher H. Lin, Ilya
Lintsbakh, Andy McGovern, Aleksandr Nisnevich,
Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth,
Subhro Roy, Jesse Rusak, Beth Short, Div Slomin,
Ben Snyder, Stephon Striplin, Yu Su, Zachary
Tellman, Sam Thomson, Andrei Vorobev, Izabela
Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang,
and Alexander Zotov. 2020. Task-oriented dialogue
as dataﬂow synthesis. Transactions of the Associa-
tion for Computational Linguistics, 8:556–571.
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,
Huajun Bai, and Yoav Artzi. 2019.
A corpus for
reasoning about natural language grounded in pho-
tographs. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 6418–6428, Florence, Italy.
Alon Talmor and Jonathan Berant. 2018. The web as
a knowledge-base for answering complex questions.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 641–651, New Or-
leans, Louisiana.
Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 1332–1342,
Beijing, China.
Nathaniel Weir, Prasetya Utama, Alex Galakatos, An-
drew Crotty, Amir Ilkhechi, Shekar Ramaswamy,
Rohin Bhushan, Nadja Geisler, Benjamin Hattasch,
Steffen Eger, Carsten Binnig, and Ugur Cetintemel.
2020. DBPal: A fully pluggable NL2SQL training
pipeline. In Proceedings of SIGMOD.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
ner, Yoav Goldberg, Daniel Deutch, and Jonathan
Berant. 2020.
Break it down: A question under-
standing benchmark. Transactions of the Associa-
tion for Computational Linguistics, 8:183–198.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Confer-
ence, pages 439–446, New York City, USA.
Shan Wu, Bo Chen, Chunlei Xin, Xianpei Han, Le Sun,
Weipeng Zhang, Jiansong Chen, Fan Yang, and Xun-
liang Cai. 2021.
From paraphrasing to semantic
parsing:
Unsupervised semantic parsing via syn-
chronous semantic decoding. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 5110–5121, Online. As-
sociation for Computational Linguistics.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le,
Mohammad Norouzi,
Wolfgang Macherey,
Maxim Krikun,
Yuan Cao,
Qin Gao,
Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin John-
son, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,
Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith
Stevens, George Kurian, Nishant Patil, Wei Wang,
Cliff Young, Jason Smith, Jason Riesa, Alex Rud-
nick, Oriol Vinyals, Greg Corrado, Macduff Hughes,
and Jeffrey Dean. 2016. Google’s neural machine
translation system: Bridging the gap between human
and machine translation.
Silei Xu, Sina Semnani, Giovanni Campagna, and
Monica Lam. 2020. AutoQA: From databases to QA
semantic parsers with only synthetic training data.
In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 422–434, Online.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018. HotpotQA: A dataset for
diverse, explainable multi-hop question answering.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2369–2380, Brussels, Belgium.
Ziyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. 2019.
Model-based Interactive Semantic Parsing: A Uni-
ﬁed Framework and A Text-to-SQL Case Study. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 5447–
5458, Hong Kong, China. Association for Computa-
tional Linguistics.
Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 440–450, Vancouver, Canada.
Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin
Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev,
Richard Socher, and Caiming Xiong. 2020. Grappa:
Grammar-augmented pre-training for table seman-
tic parsing.
Computing Research Repository,
arXiv:2009.13845.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao,
Shanelle Roman,
Zilin Zhang,
and Dragomir Radev. 2018.
Spider:
A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-SQL task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3911–3921, Brussels, Belgium.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming.
In AAAI’96:
Proceedings of the
Thirteenth National Conference on Artiﬁcial Intelli-
gence, volume 2, pages 1050–1055, Portland, OR.

7711
A
SMCalFlow SCFG
We
use
a
synchronous
context-free
gram-
mar (SCFG) to convert between SMCalFlow
meaning representations m and canonical English
representations c. Mapping m 7→c is necessary
in order to convert the SMCalFlow dataset into
prompt examples (ui, ci), while mapping c 7→m
is necessary to convert the predicted canonical
English paraphrases back into a target meaning
representation. In this section, we review SCFGs
and discuss in general how they can be used to
map between canonical utterances and meaning
representations. We describe speciﬁc issues that
arose in the case of SMCalFlow, and how we
handled them.
These techniques may also be
useful in other domains.
A.1
Context Free Grammars
A context free grammar (CFG) is a 4-tuple
(V, Σ, R, v0) where V is a set of nonterminal sym-
bols, Σ is a set of terminal symbols, R = {V ×
(V ∪Σ)∗} is a set of rules, and v0 ∈V is the start-
ing nonterminal. A CFG is speciﬁed by writing a
list of rules that expand a nonterminal v ∈V into
a string of nonterminals and terminals,
v →σ0v1σ1 · · · vnσn,
where vi ∈V ∗, σi ∈Σ. The language L deﬁned by
a CFG consists of all strings that can be generated
by using the rules to recursively expand nonter-
minals starting from the start nonterminal v0 until
there are no nonterminals left. A string s ∈L can
be parsed into one or more parse trees, which de-
scribe the expansions that could have been used to
generate s. A string is ambiguous if there is more
than one possible parse for it, and a grammar is am-
biguous if any string in its language is ambiguous.
Grammars that attempt to cover natural language
tend to be highly ambiguous, but in our setting an
unambiguous grammar is preferable.
An SCFG can be thought of as two CFGs that
share nonterminals, but have their own set of ter-
minals. Instead of specifying a single expansion,
each rule speciﬁes two expansions, a source and a
target expansion, which are synchronized by using
the same nonterminals:
v →σ0v1σ1 · · · vnσn , τ0v1τ2 · · · vnτn
The two expansions must use the same n nontermi-
nals, although the form above may be generalized
to allow these nonterminals to appear in different
orders in the source and target expansions. The set
of rules and their source expansions deﬁnes a CFG
and a language C, and the set of rules and their
target expansions deﬁnes a CFG and a language
M. Because each expansion’s nonterminals are the
same in any given rule, given an SCFG and a string
c ∈C, we can parse c to obtain a parse tree, and
then use this parse tree to generate its correspond-
ing string m ∈M. While one set of expansions
is termed the source and the other the target, we
can also reverse the process and translate a string
m ∈M to a string c ∈C. It is this ability to pair
two languages together that we use to map between
canonical and meaning representations.
A.2
SCFG for Semantic Parsing
Now suppose we have some semantic pars-
ing domain with a set F of functions.
Each
function
f
∈
F
has
a
type
signature
f(af
1 : T f
1 , . . . , af
n : T f
n ) →T f, where Tf is the
return type and af
i and T f
i are the name and type
of the ith argument. For simplicity, we treat con-
stants of the domain as 0-ary functions, writing
them without the parentheses.
In the case of SMCalFlow, we had to reconstruct
the type signatures for the functions in the dataset,
as they were not provided with the dataset release.
For each function, we specify a corresponding
English template E(f) = σf
0af
1σf
1 · · · af
nσf
n, where
each σf
i is a possibly empty10 string of English
text. Again, we may generalize to allow the ai to
be ordered differently in E(f) than in f.
We can deﬁne an SCFG that maps between pro-
grams and English for this domain by writing down
the rules
T f →σf
0T f
1 σf
1 · · · T f
n σf
n , f(T f
1 , . . . , T f
n )
for all f ∈F. Let T denote the set of types.
For example, consider a toy domain where we
can buy colored shapes.
We have types T
=
{Command, CShape, Shape}, and functions
for returning shapes, coloring those shapes, and
10For example, in SMCalFlow, our template for the function
Execute($intension) is simply $intension.

7712
buying the colored shapes:
F = {buy($o: CShape) →Command,
toRed($s: Shape) →CShape,
toGreen($s: Shape) →CShape,
square →Shape,
triangle →Shape}
We could write English templates:
E(buy) = Buy a $o
E(toRed) = red $s
E(toGreen) = green $s
E(square) = box
E(triangle) = triangle
The resulting SCFG for our toy domain would be:
Command
→Buy a CShape , buy(CShape) (1)
CShape
→red Shape , toRed(Shape)
(2)
CShape
→green Shape , toGreen(Shape)(3)
Shape
→box , square
(4)
Shape
→triangle , triangle
(5)
where we have bolded the nonterminals. Now
given a canonical English utterance like Buy a
green box, we can parse it to produce the parse
tree (1 (3 (4)), which we can then use to gener-
ate the program buy(toGreen(square)).
A.3
Ambiguity
Ideally, the mappings c 7→m and m 7→c would
be 1-1 mappings, but an SCFG does not guarantee
this. In the case of our SMCalﬂow SCFG, each
meaning representation does have only a single
parse—as one would expect for code in a formal
language—so m 7→c is deterministic. Unfortu-
nately, a canonical utterance c may have multiple
parses, leading to different meanings m.
The ﬁrst reason for ambiguity arises directly
from the ambiguity of English. For example, does
Create a meeting after the meeting with Bob mean
“After the meeting, create a meeting with Bob” or
“After the meeting with Bob, create a meeting”?
While one could attempt to wordsmith the tem-
plates to eliminate this kind of ambiguity, doing so
can quickly become unscalable for large domains.
The other reason that ambiguity occurs is
that we allow templates to not contain any En-
glish literals, allowing a parser to loop arbi-
trarily many times on a nonterminal.
For ex-
ample, consider the templates for the type co-
ercion functions toRecipient($person) and
personFromRecipient($recipient).
Since
these functions coerce types in a way that English
leaves implicit, our English templates for these two
functions do not contain any English literals. This
leads to SCFG rules like
Recipient
→Person ,
toRecipient(Person)
Person
→Recipient ,
personFromRecipient(Recipient)
Person
→Bob , Bob
In this case, given the canonical utterance Bob, one
can repeatedly apply the ﬁrst two rules any number
of times, producing inﬁnitely many parses.
We could solve both problems by weighting
the rules in the grammar, and picking the lowest-
weight parse. Since data is available, we can also
train a model to predict the correct parse. However,
we ﬁnd that in practice, (1) limiting the allowable
recursion in the grammar so that the grammar can
only produce a ﬁnite number of parses and then
(2) using some heuristic rules to pick among those
ﬁnite set of parses, is both simple and works well.
To limit the recursion in the grammar, we ﬁrst
deﬁne a graph induced by the SCFG, where nodes
represent nonterminals and a directed edge from
node ns to nd represents usage of the nonterminal
nd in a rule for ns. We greedily ﬁnd the set N of
the minimal set of nodes that covers all the cycles
in this graph. Then we make D copies of every
nonterminal v1, . . . , vd for all v ∈V , and for every
rule
v →→σ1
0v1σ1
1 · · · vnσ1
n , σ2
0v1σ2
1 · · · vnσ2
n
we replace it with D copies of every rule where
copy d of a rule uses copy d + 1 of a nonterminal
vi if vi ∈N:
vd →σ1
0d(v1)σ1
1 · · · d(vn)σ1
n , σ2
0d(v1)σ2
1 · · · d(vn)σ2
n
where d(vi) = vd+1
i
if vi ∈N and vd
i otherwise.
For our experiments, we set D = 10, which we
ﬁnd covers all the examples that we use.
Then, to select from a ﬁnite set of parses,
we generate the corresponding program for each
parse, and use a set of heuristic rules to discard
programs that we know are incorrect.
These
rules include discarding programs that call Yield
multiple times, as in (Yield :output (Yield
:output ...)), and discarding programs that

7713
SMCalflow 
Original 
Representation
Intermediate
Representation
Remove reentrancies
Add new functions
Language 
Model
Canonical English
Natural English
SCFG for 
Intermediate 
Representation
Intermediate 
Representation (IR)
Add reentrancies
Remove new functions
IRs
Disambiguate
IR
Figure 6: Our pipeline for SMCalﬂow. We ﬁrst convert SMCalﬂow’s original representation to an intermediate
representation, upon which we induce an SCFG. This SCFG is used to generate pairs of natural and canonical
English utterances, which is used to train a language model to predict a canonical English utterance given a natural
one. Predicted canonical English utterances are then mapped back into intermediate meaning representations,
which can then be transformed back into the original representation.
call CreatePreflightEventWrapper without
calling CreateCommitEventWrapper. In prac-
tice we ﬁnd that our heuristic rules can recover
the correct parse 90% of the time.
A.4
Character-level Parsing
When writing English templates, it would be incon-
venient to ensure that the terminals of the gram-
mar line up exactly with the tokens used by the
language model. Different LMs sometimes use
subtly different tokenizers, and it would be espe-
cially inconvenient to write a different grammar
for each LM. In order to handle differences be-
tween the LM’s tokenizer and the terminals of the
grammar, we effectively treat the grammar as one
whose terminals are all single characters. Then
to implement nextTokens, we advance each LM-
proposed token character-by-character and return
the set of tokens who, after being fully consumed,
still have live Earley chart items. By catering to
the LM’s preferred tokenization, we ensure that the
LM’s likelihood after incremental search matches
the likelihood the LM would have assigned had it
been given the full string to begin with.
B
Intermediate Representation
While we have described how to build an SCFG
for mapping between meaning representations
and canonical representations, we still have two
problems. The ﬁrst problem is that unfortunately as
constructed, the SCFG cannot handle reentrancies,
where expressions are cached in variables inside
let expressions and then used multiple times.
The second problem arises from the fact that it is
impossible to engineer the English templates in a
way that they produce natural English utterances
for every possible composition of functions. For
example, our English template for get($object,
$path) is $path of $object, which produces
ﬂuent English when getting the start time of an
event, like in “start time of event”.
However,
consider the program needed to deleting an
event: (DeleteCommitEventWrapper :event
(DeletePreflightEventWrapper :id (get
(constraint[Event]) #(Path "id"))). The
SCFG would translate this program into “delete id
of event” when we would prefer something closer
to “delete event.”
To solve both these problems, instead of induc-
ing an SCFG based on the original SMCalﬂow rep-
resentation, we instead ﬁrst transform SMCalﬂow
into an intermediate representation that 1) does not
contain any reentrancies and 2) replaces common
program fragments with calls to macros, and in-
duce an SCFG on that the resulting intermediate
representation. See Figure 6 for a visualization of
our entire process.
B.1
Reentrancies
To remove reentrancies, given an expression of
the form (let (var binding) (body)) where
the body contains usages of var, we replace
the ﬁrst usage of var (in postorder traversal)
with binding and all other usages into calls
to (referWithinTurn T) where T
∈
T is
the return type of the body expression and
referWithinTurn is a new function that retrieves
the most “salient” evaluation of type T from else-
where in the program for the current utterance.
Given a program p in the intermediate repre-
sentation, to convert a call to (referWithinTurn
T) back into a let expression (to map from the
intermediate representation back to the original),
we ﬁnd the ﬁrst expression e of type T in p (in
postorder traversal), and replace p with the let
expression (let (x e) sub(p, e, x)), where
sub replaces all expressions e in p with x. Note that
this transformation is lossy. By picking the ﬁrst
expression that matches, it is possible to make mis-
takes, but we ﬁnd in practice that such a heuristic

7714
is often good enough.
B.2
Macros
To reduce the number of unnatural sound-
ing utterances produced by our grammar, we
deﬁne macros to capture common program
fragments, and then replace those fragments
with calls to those macros.
For example, we
deﬁne
a
macro
DeleteWrapper($event),
which we use to replace fragments that look
like
(DeleteCommitEventWrapper :event
(DeletePreflightEventWrapper :id (get
$event #(Path "id"))).
After
deﬁning
macros, we add the macros to the set of functions
F and corresponding English templates. In the
case of DeleteWrapper, we write the template
delete $event.
In total, we deﬁne 15 new
functions and ﬁnd that they signiﬁcantly help
ﬂuentize the resulting English. When translating
from the intermediate representation back to the
original SMCalﬂow representation, we can remove
these new functions by simply replacing them with
their deﬁnitions.
C
Stratiﬁed Datasets
The motivation for our stratiﬁed datasets is to try
and imitate what a small dataset over SMCalFlow
or similar would look like had it been collected with
a small target size in mind (i.e., collect and anno-
tate 100 representative dialogue turns). In this case,
we expect that domain developers would do their
best to guarantee that each supported SMCalFlow
functionality appears in at least one example. Such
functionalities can be described by the functions
that are used in the respective programs. Therefore,
our goal is to produce small subsets of the origi-
nal large dataset (∼120,000 dialogue turns), which
guarantee that each supported function appears in
at least k examples (k = 1 in our experiments).
The procedure we used to do this consists of three
steps that we describe in the following paragraphs.
Function Histogram Extraction.
We ﬁrst ex-
tract function histograms for each example in our
data. This step consists of collecting all the func-
tion signatures that appear in each example and
then constructing a histogram over these signatures
for our train, validation, and test datasets.
Rare Functions Filtering.
SMCalFlow contains
some examples that use very rare functions. These
examples seem to be the result of annotation errors
or incomplete data migrations and thus we do not
want to include them in our stratiﬁed datasets. Note
that including them would also render complete
coverage of the data (in terms of functionality) im-
possible with only 100 or 300 examples. Therefore,
in this step we: (i) use the extracted histograms to
collect all functions that appear less than n times
in the training data (n = 10 in our experiments),
(ii) remove all examples that contain any of these
functions across all of the train, validation, and
test data, and (iii) update the dataset histograms to
reﬂect the ﬁltered data distributions.
Stratiﬁed Sampling.
Our goal in this step is to
use the function histograms and sample subsets
of the ﬁltered datasets which guarantee that each
function appears in at least k examples in each
sample. Let m be the total number of examples
in the dataset we are sampling from, and let f
be the total number of functions after the previous
ﬁltering step is applied. We formulate our sampling
problem as a mixed-integer program (MIP):
max
x
x⊤c,
OBJECTIVE
(6)
s.t.
x⊤1 = s,
TARGET SIZE
(7)
x⊤H ≥k,
COVERAGE
(8)
where x ∈{0, 1}m denotes whether or not an
example is included in the subset we are sam-
pling, c ∈Rm is a random vector sampled
from the standard Gaussian distribution, s is the
target dataset size, and H
∈
{0, 1}m×f de-
notes the function membership for each exam-
ple (i.e., Hij = 1 speciﬁes that example i con-
tains function j). In our experiments we used the
open-source JOpt solver, which can be found at
https://github.com/blubin/jopt.
D
Overnight
D.1
Our reproduction of Cao et al. (2019)
In order to investigate how a state-of-the-art
method for Overnight performs when it is only
given 200 training examples for each domain, we
downloaded the code from https://github.com/
rhythmcao/semantic-parsing-dual and made the
following modiﬁcations:
• We used 200 training examples for each do-
main, the same examples as used in experi-
ments with our methods.
• Overnight does not have an ofﬁcial develop-
ment set. Rather than holding out 20% of the

7715
Model
Train n Basketball Blocks Calendar Housing Publications Recipes Restaurants Social
GPT-2f Constrained Canonical
200
0.836
0.549
0.804
0.640
0.752
0.787
0.762
0.726
GPT-2f Constrained Meaning
200
0.831
0.516
0.732
0.677
0.727
0.778
0.768
0.671
GPT-2f Unconstrained Canonical
200
0.798
0.509
0.762
0.603
0.720
0.745
0.705
0.632
GPT-2f Unconstrained Meaning
200
0.821
0.506
0.708
0.646
0.671
0.755
0.753
0.626
Table 5: Accuracy on Overnight dataset using GPT-2 XL.
200 training examples (the default method-
ology) as a development set to use for early
stopping, we randomly sampled a develop-
ment set with a size of 20% of the original
training set, from the original training set with
the 200 chosen earlier excluded.
• We increased the total number of max epochs
before stopping to 200, from 100. The code
evaluates the model after each epoch on the
development set, and chooses the snapshot
that performed best on the development set.
D.2
Miscellaneous details
For our GPT-2, GPT-3, and BART experiments
using meaning representations as the target
output, we removed all instances of the string
edu.stanford.nlp.sempre.overnight.SimpleWorld.
from the meaning representations,
as it is
redundant.
E
Finetuning Experiments
For our ﬁnetuning experiments, we use BART-large
model which has 406 million parameters, and the
GPT2-XL model which has 1.5 billion parame-
ters. We train each model using the causal LM
loss for 20,000 steps, where we linearly warmup
the learning rate for the ﬁrst 1000 steps, and then
reduce the learning rate by a factor of 0.999 every
t steps. For choosing hyperparameters, we perform
a grid search by choosing the maximum learning
rate from the set {10−5, 10−6} and t from the set
{2, 4, 6, 8}. The best hyperparameters were chosen
based on performance on a development set. We
use a batch size of 32, clip gradient norm at 10, and
set a minimum learning rate threshold of 10−9.
We add some additional experimental results
using ﬁnetuned GPT-2 XL in Tables 5 and 6.
Model
n nem
Coleman & Reneau
44,321 0.42
Wolfson et al. (2020)
44,321 0.29
Arad & Sapir
44,321 0.16
BARTf Unconstrained Meaning
200 0.10
GPT-2f Constrained Canonical
200 0.18
GPT-2f Constrained Meaning
200 0.17
GPT-2f Unconstrained Canonical
200 0.13
GPT-2f Unconstrained Meaning
200 0.13
Table 6: NEM accuracy on the Break dataset using
GPT-2 XL.
F
Computing Infrastructure
For the GPT-3 experiments, we used OpenAI’s
GPT-3 API hosted on Microsoft Azure. For the
ﬁnetuning experiments, we used NVIDIA DGX-2
machines contaning NVIDIA Tesla V100 GPUs.
G
Further Discussion
A common thread among all of our datasets, and
arguably semantic parsing in general, is that an-
notation subtleties cause problems for automated
methods and annotators alike. For example, on the
Calendar subset of Overnight, we found that of our
best model’s 18 errors, 8 were legitimately wrong,
7 were annotation errors that the model actually
got right, and 3 differed only by equality strictness
– which is often left ambiguous in natural language.
For example, for the input: tell me the all meetings
begins after 10am or 3pm, the annotated canonical
form in the data is: meeting whose start time is at
least 10am or 3pm, but our system predicted: meet-
ing whose start time is larger than 10am or 3pm.
We would expect low interannotator agreement on
this subtle distinction (≥vs. >), just as we would
expect GPT-3 to perform poorly. As another exam-
ple, on the Calendar subdomain of Overnight, our
best model’s denotation accuracy @K saturated at
0.98 when K ≥5; but we found that the 2 remain-
ing errors were caused by annotation mistakes on
utterances that the model correctly interpreted.

