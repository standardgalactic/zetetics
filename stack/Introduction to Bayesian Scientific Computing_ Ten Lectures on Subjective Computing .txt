
Surveys and Tutorials in the
Applied Mathematical Sciences
Volume 2
Editors
S.S. Antman, J.E. Marsden, L. Sirovich

Surveys and Tutorials in the Applied Mathematical Sciences
Volume 2
Editors
S.S. Antman, J.E. Marsden, L. Sirovich
Mathematics is becoming increasingly interdisciplinary and developing stronger interactions
with ﬁelds such as biology, the physical sciences, and engineering. The rapid pace and
development of the research frontiers has raised the need for new kinds of publications: short,
up-to-date, readable tutorials and surveys on topics covering the breadth of the applied math-
ematical sciences. The volumes in this series are written in a style accessible to researchers,
professionals, and graduate students in the sciences and engineering. They can serve as intro-
ductions to recent and emerging subject areas and as advanced teaching aids at universities. In
particular, this series provides an outlet for material less formally presented and more antic-
ipatory of needs than ﬁnished texts or monographs, yet of immediate interest because of the
novelty of their treatments of applications, or of the mathematics being developed in the con-
text of exciting applications. The series will often serve as an intermediate stage of publication
of material which, through exposure here, will be further developed and reﬁned to appear later
in one of Springer’s more formal series in applied mathematics.
Surveys and Tutorials in the Applied Mathematical Sciences
1. Chorin/Hald: Stochastic Tools in Mathematics and Science
2. Calvetti/Somersalo: Introduction to Bayesian Scientiﬁc Computing: Ten Lectures on
Subjective Computing

Daniela Calvetti Erkki Somersalo
Introduction to Bayesian
Scientiﬁc Computing
Ten Lectures on Subjective Computing

Daniela Calvetti
Erkki Somersalo
Department of Mathematics
Institute of Mathematics
Case Western Reserve University
Helsinki University of Technology
Cleveland OH, 44106–7058
P.O. Box 1100
daniela.calvetti@case.edu
FI-02015 TKK
Finland
erkki.somersalo@tkk.ﬁ
Editors:
S.S. Antman
J.E. Marsden
L. Sirovich
Department of Mathematics
Control and Dynamical
Laboratory of Applied
and
System, 107-81
Mathematics
Institute for Physical Science
California Institute
Department of
and Technology
of Technology
Bio-Mathematical Sciences
University of Maryland
Pasadena, CA 91125
Mount Sinai School of Medicine
College Park,
USA
New York, NY 10029-6574
MD 20742-4015
marsden@cds.caltech.edu
USA
USA
chico@camelot.mssm.edu
ssa@math.umd.edu
Mathematics Subject Classiﬁcation (2000): 62F15, 65C60, 65F10, 65F22, 65C40, 15A29
Library of Congress Control Number: 2007936617
ISBN 978-0-387-73393-7
e-ISBN 978-0-387-73394-4
Printed on acid-free paper.
c⃝2007 Springer Science+Business Media, LLC
All rights reserved. This work may not be translated or copied in whole or in part without the written
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY
10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in connection
with any form of information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks and similar terms, even if they are
not identiﬁed as such, is not to be taken as an expression of opinion as to whether or not they are subject
to proprietary rights.
Printed in the United States of America. (MVY)
9 8 7 6 5 4 3 2 1
springer.com

Dedicated to the legacy of
Bruno de Finetti and Luigi Pirandello:
Cos´ı `e (se vi pare).

Preface
The book of nature, according to Galilei, is written in the language of mathe-
matics. The nature of mathematics is being exact, and its exactness is under-
lined by the formalism used by mathematicians to write it. This formalism,
characterized by theorems and proofs, and syncopated with occasional lem-
mas, remarks and corollaries, is so deeply ingrained that mathematicians feel
uncomfortable when the pattern is broken, to the point of giving the im-
pression that the attitude of mathematicians towards the way mathematics
should be written is almost moralistic. There is a deﬁnition often quoted,
“A mathematician is a person who proves theorems”, and a similar, more
alchemistic one, credited to Paul Erd˝os, but more likely going back to Alfr´ed
R´enyi, stating that “A mathematician is a machine that transforms coﬀee into
theorems1”. Therefore it seems to be the form, not the content, that charac-
terizes mathematics, similarly to what happens in any formal moralistic code
wherein form takes precedence over content.
This book is deliberately written in a very diﬀerent manner, without a
single theorem or proof. Since morality has its subjective component, to para-
phrase Manuel Vasquez Montalban, we could call it Ten Immoral Mathemat-
ical Recipes2. Does the lack of theorems and proofs mean that the book is
more inaccurate than traditional books of mathematics? Or is it possibly just
a sign of lack of coﬀee? This is our ﬁrst open question.
Exactness is an interesting concept. Italo Calvino, in his Lezioni Ameri-
cane3, listed exactness as one of the values that he would have wanted to take
along to the 21st century. Exactness, for Calvino, meant precise linguistic ex-
1 That said, academic mathematics departments should invest on high quality coﬀee
beans and decent coﬀee makers, in hope of better theorems. As Paul Tur´an, a
third Hungarian mathematician, remarked, “weak coﬀee is ﬁt only to produce
lemmas”.
2 M. V. Montalban: Ricette immorali (orig. Las recetas inmorales, 1981), Feltrinelli,
1992.
3 I. Calvino: Lezioni Americane, Oscar Mondadori, 1988.

VIII
Preface
pression, but in a particular sense. To explain what he meant by exactness, he
used a surprising example of exact expression: the poetry of Giacomo Leop-
ardi, with all its ambiguities and suggestive images. According to Calvino,
when obsessed with a formal language that is void of ambiguities, one loses
the capability of expressing emotions exactly, while by liberating the language
and making it vague, one creates space for the most exact of all expressions,
poetry. Thus, the exactness of expression is beyond the language. We feel the
same way about mathematics.
Mathematics is a wonderful tool to express liberally such concepts as quali-
tative subjective beliefs, but by trying to formalize too strictly how to express
them, we may end up creating beautiful mathematics that has a life of its
own, in its own academic environment, but which is completely estranged to
what we initially set forth. The goal of this book is to show how to solve prob-
lems instead of proving theorems. This mischievous and somewhat provocative
statement should be understood in the spirit of Peter Lax’ comment in an in-
terview given on the occasion of his receiving the 2005 Abel Prize4: “When a
mathematician says he has solved the problem he means he knows the solu-
tion exists, that it’s unique, but very often not much more.” Going through
mathematical proofs is a serious piece of work: we hope that reading this book
feels less like work and more like a thought-provoking experience.
The statistical interpretation, and in particular the Bayesian point of view,
plays a central role in this book. Why is it so important to emphasize the
philosophical diﬀerence between statistical and non-statistical approaches to
modelling and problem solving? There are two compelling reasons.
The ﬁrst one is very practical: admitting the lack of information by mod-
elling the unknown parameters as random variables and encoding the nature
of uncertainty into probability densities gives a great freedom to develop the
models without having to worry too much about whether solutions exist or are
unique. The solution in Bayesian statistics, in fact, is not a single value of the
unknowns, but a probability distribution of possible values, that always exists.
Moreover, there are often pieces of qualitative information available that sim-
ply do not yield to classical methods, but which have a natural interpretation
in the Bayesian framework.
It is often claimed, in particular by mathematician in inverse problems
working with classical regularization methods, that the Bayesian approach is
yet another way of introducing regularization into problems where the data
are insuﬃcient or of low quality, and that every prior can be replaced by an
appropriately chosen penalty. Such statement may seem correct in particular
cases when limited computational resources and lack of time force one to use
the Bayesian techniques for ﬁnding a single value, typically the maximum a
posteriori estimate, but in general the claim is wrong. The Bayesian frame-
work, as we shall reiterate over and again in this book, can be used to produce
4 M. Raussen and C. Skau: Interview with Peter D. Lax. Notices of the AMS 53
(2006) 223–229.

Preface
IX
particular estimators that coincide with classical regularized solutions, but the
framework itself does not reduce to these solutions, and claiming so would be
an abuse of syllogism5.
The second, more compelling reason for advocating the Bayesian approach,
has to do with the interpretation of mathematical models. It is well under-
stood, and generally accepted, that a computational model is always a simpliﬁ-
cation. As George E. P. Box noted, “all models are wrong, some are useful”. As
computational capabilities have grown, an urge to enrich existing models with
new details has emerged. This is particularly true in areas like computational
systems biology, where the new paradigm is to study the joint eﬀect of a huge
number of details6 rather than using the reductionist approach and seeking
simpliﬁed lumped models whose behavior would be well understood. As a con-
sequence, the computational models contain so many model parameters that
hoping to determine them based on few observations is simply unreasonable.
In the old paradigm, one could say that there are some values of the model
parameters that correspond in an “optimal way” to what can be observed.
The identiﬁability of a model by idealized data is a classic topic of research in
applied mathematics. From the old paradigm, we have also inherited the faith
in the power of single outputs. Given a simplistic electrophysiological model of
the heart, a physician would want to see the simulated electrocardiogram. If
the model was simple, for example two rotating dipoles, that output would be
about all the model could produce, and no big surprises were to be expected.
Likewise, given a model for millions of neurons, the physician would want to
see a simulated cerebral response to a stimulus. But here is the big diﬀerence:
the complex model, unlike the simple dipole model, can produce a continuum
of outputs corresponding to ﬁctitious data, never measured by anybody in the
past or the future. The validity of the model is assessed according to whether
the simulated output corresponds to what the physician expects. While when
modelling the heart by a few dipoles, a single simulated output could still
make sense, in the second case the situation is much more complicated. Since
the model is overparametrized, the system cannot be identiﬁed by available or
even hypothetical data and it is possible to obtain completely diﬀerent outputs
simply by adjusting the parameters. This observation can lead researchers to
state, in frustration, “well, you can make your model do whatever you want,
so what’s the point”7. This sense of hopelessness is exactly what the Bayesian
approach seeks to remove. Suppose that the values of the parameters in the
5 A classic example of analogous abuse of logic can be found in elementary books
of logic: while it is true that Aristotle is a Greek, it is not true that a Greek is
Aristotle.
6 This principle is often referred to as emergence, as new unforeseen and qualita-
tively diﬀerent features emerge as a sum of its parts ( cf. physics −→chemistry
−→life −→intelligence). Needless to say, this holistic principle is old, and can
be traced back to ancient philosophers.
7 We have actually heard this type of statement repeatedly from people who refuse
to consider the Bayesian approach to problem solving.

X
Preface
complex model have been set so that the simulated output is completely in
conﬂict with what the experts expect. The reaction to such output would be to
think that the current settings of the parameters “must” be wrong, and there
would usually be unanimous consensus about the incorrectness of the model
prediction. This situation clearly demonstrates that some combinations of the
parameter values have to be excluded, and the exclusion principle is based on
the observed data (likelihood), or in lack thereof, on the subjective belief of
an expert (prior). Thanks to this exclusion, the model can no longer do what-
ever we want, yet we have not reduced its complexity and thereby its capacity
to capture complex, unforeseen, but possible, phenomena. By following the
principle of exclusion and subjective learned opinions, we eﬀectively narrow
down the probability distributions of the model parameters so that the model
produced plausible results. This process is cumulative: when new information
arrives, old information is not rejected, as is often the case in the infamous
“model ﬁtting by parameter tweaking”, but included as prior information.
This mode of building models is not only Bayesian, but also Popperian8 in
the wide sense: data is used to falsify hypotheses thus leading to the removal
of impossible events or to assigning them as unlikely, rather than to verify
hypotheses, which is in itself a dubious project. As the classic philosophic
argument goes, producing one white swan, or, for that matter, three, does
not prove the theory that all swans are white. Unfortunately, deterministic
models are often used in this way: one, or three, successful reconstructions are
shown as proof of a concept.
The statistical nature of parameters in complex models serves also another
purpose. When writing a complex model for the brain, for instance, we expect
that the model is, at least to some extent, generic and representative, and thus
capable of explaining not one but a whole population of brains. To our grace,
or disgrace, not all brains are equal. Therefore, even without a reference to
the subjective nature of information, a statistical model simply admits the
diversity of those obscure objects of our modelling desires.
This books, which is based on notes for courses that we taught at Case
Western Reserve University, Helsinki University of Technology, and at the
University of Udine, is a tutorial rather than an in-depth treatise in Bayesian
statistics, scientiﬁc computing and inverse problems. When compiling the bib-
liography, we faced the diﬃcult decision of what to include and what to leave
out. Being at the crossroad of three mature branches of research, statistics, nu-
merical analysis and inverse problems, we were faced with three vast horizons,
as there were three times as many people whose contributions should have
been acknowledged. Since compiling a comprehensive bibliography seemed a
herculean task, in the end Occam’s razor won and we opted to list only the
books that were suggested to our brave students, whom we thank for feed-
back and comments. We also want to thank Dario Fasino for his great hos-
8 See A. Tarantola: Inverse problems, Popper and Bayes, Nature Physics 2
492–494,(2006).

Preface
XI
pitality during our visit to Udine, Rebecca Calvetti, Rachael Hageman and
Rossana Occhipinti for help with proofreading. The ﬁnancial support of the
Finnish Cultural Foundation for Erkki Somersalo during the completion of
the manuscript is gratefully acknowledged.
Cleveland – Helsinki,
Daniela Calvetti
May 2007
Erkki Somersalo

Contents
1
Inverse problems and subjective computing . . . . . . . . . . . . . . . .
1
1.1
What do we talk about when we talk about random variables?
2
1.2
Through the formal theory, lightly . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
How normal is it to be normal? . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2
Basic problem of statistical inference . . . . . . . . . . . . . . . . . . . . . . 21
2.1
On averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.2
Maximum Likelihood, as frequentists like it . . . . . . . . . . . . . . . . . 31
3
The praise of ignorance: randomness as lack of information
39
3.1
Construction of Likelihood. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2
Enter, Subject: Construction of Priors. . . . . . . . . . . . . . . . . . . . . . 48
3.3
Posterior Densities as Solutions of Statistical Inverse Problems
55
4
Basic problem in numerical linear algebra . . . . . . . . . . . . . . . . . 61
4.1
What is a solution? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.2
Direct linear system solvers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
4.3
Iterative linear system solvers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.4
Ill-conditioning and errors in the data . . . . . . . . . . . . . . . . . . . . . . 77
5
Sampling: ﬁrst encounter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
5.1
Sampling from Gaussian distributions . . . . . . . . . . . . . . . . . . . . . . 92
5.2
Random draws from non-Gaussian densities. . . . . . . . . . . . . . . . . 99
5.3
Rejection sampling: prelude to Metropolis-Hastings . . . . . . . . . . 102
6
Statistically inspired preconditioners. . . . . . . . . . . . . . . . . . . . . . . 107
6.1
Priorconditioners: specially chosen preconditioners . . . . . . . . . . . 108
6.2
Sample-based preconditioners and PCA model reduction . . . . . 118

XIV
Contents
7
Conditional Gaussian densities and predictive envelopes . . . 127
7.1
Gaussian conditional densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
7.2
Interpolation, splines and conditional densities . . . . . . . . . . . . . . 134
7.3
Envelopes, white swans and dark matter . . . . . . . . . . . . . . . . . . . 144
8
More applications of the Gaussian conditioning . . . . . . . . . . . . 147
8.1
Linear inverse problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
8.2
Aristotelian boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . 151
9
Sampling: the real thing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
9.1
Metropolis–Hastings algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
10
Wrapping up: hypermodels, dynamic priorconditioners
and Bayesian learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
10.1 MAP estimation or marginalization? . . . . . . . . . . . . . . . . . . . . . . . 189
10.2 Bayesian hypermodels and priorconditioners . . . . . . . . . . . . . . . . 193
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199

1
Inverse problems and subjective computing
The rather provocative subtitle of this book, Ten Lectures on Subjective Com-
puting, is intended to evoke the concoction of two relatively well established
concepts, subjective probability and scientiﬁc computing. The former is con-
cerned usually with Bayesian statistics; the latter with the design and ap-
plication of numerical techniques to the solution of problems in science and
engineering. This book provides an oblique view of numerical analysis and
statistics, with the emphasis on a class of computational problems usually
referred to as inverse problems.
The need for scientiﬁc computing to solve inverse problems has been rec-
ognized for a long time. The need for subjective probability, surprisingly, has
been recognized even longer. One of the founding fathers1 of the Bayesian
approach to inverse problems, in addition to reverend Bayes2 himself, is
Laplace3, who used Bayesian techniques, among other things, to estimate
the mass of planets from astronomical observations.
To understand the connection between inverse problems and statistical
inference, consider the following text book deﬁnitions:
•
Inverse problem: The problem of retrieving information of unknown
quantities by indirect observations.
1 In those times potential founding mothers were at home raising children and
cooking meals.
2 Thomas Bayes (1702–1761), in his remarkable, posthumously published , An Es-
say towards solving a Problem in the Doctrine of Chances, Phil. Tras. Royal Soc.
London 53, 370–418 (1763), ﬁrst analyzed the question of inverse probability, that
is, how to determine the probability of an event from observation of outcomes.
This has now become a core question of modern science.
3 Pierre-Simon Laplace (1749–1827): French mathematician and natural scientist,
one of the developers of modern probability and statistics, among other achieve-
ments. His M´emoire sur la probabilit´e des causes par les ´ev`enemens from 1774
shows the Bayes formula in action as we use it today. The English translation as
well as a discussion of the Memoir can be found in Stigler, S.M.: Laplace’s 1774
Memoir on Inverse Probability, Statistical Science, 1, 359–363 (1986).

2
1 Inverse problems and subjective computing
•
Statistical inference: The problem of inferring properties of an un-
known distribution from data generated from that distribution.
At ﬁrst sight, the scopes seem to be rather diﬀerent. The key words, how-
ever, are “information”, “unknown” and “distribution”. If a quantity is un-
known, the information about its value is incomplete. This is an ideal setting
for introducing random variables, whose distribution tells precisely what infor-
mation is available and, by complementarity, what is lacking. This deﬁnition
of randomness is at the very core of this book4.
Statistics provides a ﬂexible and versatile framework to study inverse prob-
lems, especially from the computational point of view, as we shall see. Nu-
merical methods have long been used in statistics, in particular in what is
known as computational statistics. One of the goals of this book is to enable
and nurture a ﬂow in the opposite direction. Paraphrasing the famous appeal
of J.F.Kennedy, “Ask not what numerical analysis can do for statistics; ask
what statistics can do for numerical analysis”. Hence, this book addresses a
mixture of topics that are usually taught in numerical analysis or statisti-
cal inference courses, with the common thread of looking at them from the
point of view of retrieving information from indirect observations by eﬀective
numerical methods.
1.1 What do we talk about when we talk about random
variables?
The concepts of random events and probabilities assigned to them have an
intuitively clear meaning5 and yet have been the subject of extensive philo-
sophical and technical discussions. Although it is not the purpose of this book
to plunge into this subject matter, it is useful to review some of the central
concepts and explain the point of view that we follow.
A random event is the complement of a deterministic event in the sense
that if a deterministic event is one whose outcome is completely predictable,
by complementarity the outcome of a random event is not fully predictable.
Hence here randomness means lack of information. The degree of information,
or more generally, our belief about the outcome of a random event, is expressed
in terms of probability.
That said, it is clear that the concept of probability has a subjective com-
ponent, since it is a subjective matter to decide what is reasonable to believe,
and what previous experience the belief is based on. The concept of proba-
bility advocated in this book is called subjective probability6, which is often
4 A harbinger view to inverse problems and statistical inference as a uniﬁed ﬁeld
can be found in the signiﬁcant article by Tarantola, A. and Valette, B.: Inverse
problems = quest for information, J. Geophys. 50, 159–170 (1982).
5 As Laplace put it, probability is just “common sense reduced to calculations”.
6 An inspirational reference of this topic, see [Je04].

1.1 What do we talk about when we talk about random variables?
3
a synonym for Bayesian probability. To better understand this concept, and
the customary objections against it, let us consider some simple examples.
Example 1.1: Consider the random event of tossing a coin. It is quite
natural to assume that the odds, or relative probabilities of complementary
events, of getting heads or tails are equal. This is formally written as
P(heads) = P(tails) = 1
2,
where P stands for the probability. Such assumption is so common to be
taken almost as truth, with no judgemental component in it, similarly as
it was once dealt with the ﬂatness of the Earth. If, however, somebody
were to disagree, and decided to assign probability 0.4, say, to heads and
0.6 to tails, a justiﬁcation would certainly be required. Since this is meant
as a trivial example of testing a scientiﬁc theory, there must be a way to
falsify the theory7. The obvious way to proceed is to test it empirically:
a coin is tossed a huge number of times, generating a large experimental
sample. Based on this sample, we can assign experimental probabilities to
heads and tails. The outcome of this experiment will favor one theory over
another.
The previous example is in line with the frequentist deﬁnition of probabil-
ity: the probability of an event is its relative frequency of occurrence in an
asymptotically inﬁnite series of repeated experiments. The statistics based on
this view is often referred to as frequentist statistics8, and the most common
justiﬁcation is its objective and empirical nature. But is this really enough of
a validation?
Example 1.2: The concept of randomness as lack of information is use-
ful in situations where the events in question are non-repeatable. A good
example is betting on soccer. A bookmaker assigns odds to the outcomes
of a football game of Palermo versus Inter9, and the amount that a bet
wins depends on the odds. Since Palermo and Inter have played each other
several times in the past, the new game is hardly an independent real-
ization of the same random event. The bookmaker tries to set the odds
so that, no matter what the outcome is, his net loss is non-positive. The
7 This requirement for a scientiﬁc theory goes back to Karl Popper (1902–1994),
and it has been criticized for not reﬂecting the way scientiﬁc theories work in
practice. Indeed, a scientiﬁc theory is usually accepted as a probable explanation,
if it successfully resists attempts to be falsiﬁed, as the theory of ﬂat earth or
geocentric universe did for centuries. Thus, scientiﬁc truth is really a collective
subjective matter.
8 Frequentist statistics is also called Fisherian statistics, after the English statisti-
cian and evolutionary biologist Ronald Fisher (1890–1962).
9 or, if you prefer, an ice hockey game between Canada and Finland, or a baseball
game between the Indians and the Yankees.

4
1 Inverse problems and subjective computing
subjective experience of the bookmaker is clearly essential for the success
of his business.
This example is related to the concept of coherence and the deﬁnition of
probability as worked out by De Finetti10 In its simplest form, assume that
you have a random event with two possible outcomes, “A” and “B”, and that
you are betting on them against your opponent according to the following
rules. If you correctly guess the outcome, you win one dollar; otherwise you
pay one dollar to your opponent. Based on your information, you assign the
prices, positive but less than one dollar, for betting on each of the outcomes.
After your have decided the prices, your opponent decides for which outcome
you have to bet. The right price for your bets should therefore be such that
whichever way the result turns out, you feel comfortable with it.
Obviously, if the frequentist’s information about the outcome of earlier
events is available, it can be used, and it is likely to help in setting the odds,
even if the sample is small. However, the probabilities are well deﬁned even if
such sample does not exist. Setting the odds can also be based on computa-
tional modelling.
Example 1.3: Consider a variant of the coin tossing problem: a thumb-
tack is tossed, and we want its probability of landing with the spike up.
Assuming that no tacks are available to generate an experimental sample,
we may try to model the experiment. The convex hull of the tack is a cone,
and we may assume that the odds of having the spike up is the ratio of the
bottom area to the surface area of the conical surface. Hence, we assign a
probability based on a deterministic model. Notice, however, that whether
the reasoning we used is correct or not is a judgemental issue. In fact, it is
possible also to argue that the odds for spike up or spike down are equal,
or we may weigh the odds based on the uneven mass distribution. The
position of the spike at landing depends on several factors, some of which
may be too complicated to model.
In the last example, the ambiguity is due to the lack of a model that would
explain the outcome of the random event. The use of computers has made it
possible to simulate frequentist’s data, leading to a new way of setting the
odds for events.
Example 1.4: Consider the growth of bacteria in a petri dish, and assume
that we have to set the probability for the average bacteria density to be
above a certain given level. What we may do is to set up a growth model,
partitioning the petri dish area in tiny squares and writing a stochastic
model, a “Game of Life11”, for instance along the following lines. Start
10 Bruno De Finetti (1906–1985): Italian mathematician and statistician, one of the
developers of the subjective probability theory. De Finetti’s writings on the topic
are sometimes rather provocative.
11 This term refers to the cellular automaton (1970) of John Conway, a British
mathematician.

1.2 Through the formal theory, lightly
5
with assigning a probability for the culture to spread from an occupied
square to an empty square. If an occupied square is surrounded on all
sides by occupied squares, there is a certain probability that competition
will kill the culture in that square in the next time step. Once the model
has been set up, we may run a simulation starting from diﬀerent initial
conﬁgurations, thus creating a ﬁctitious simulated sample on which to
base our bet. Again, since the model is just a model, and the intrinsic
probabilities of the model are based on our belief, the subjective component
is evident. If in doubt on the validity of the model, we may add a random
error component to account for our lack of belief in the model. In this way,
we introduce a hierarchy of randomness, which turns out to be a useful
methodological approach.
Let’s conclude this discussion by introducing the notion of objective chance.
There are situations where it is natural to think that the occurrence of cer-
tain phenomena has a probability that is not judgemental. The ﬁrst example
in this chapter could be one of them, as is the tossing of a fair die, where
“fair” is deﬁned by means of a circular argument: if a die gives diﬀerent odds
to diﬀerent outcomes, the die is not fair. The current interpretation of quan-
tum mechanics contains an implicit assumption of the existence of objective
chance: the square of the absolute value of the solution of the wave equation
deﬁnes a probability density, and so the theory predicts the chance. Quantum
mechanics, however, is not void of the subjective component via either the
interpretation or the interference of the observing subject.
1.2 Through the formal theory, lightly
Regardless of the interpretation, the theory of probability and randomness
is widely accepted and presented in a standard form. The formalization of
the theory is largely due to Kolmogorov12. Since our emphasis is on some-
thing besides theoretical considerations, we will present a light version of the
fundamentals, spiced with examples and intuitive reasoning. For a rigorous
discussion, we shall refer to any standard text on probability theory.
Let’s begin with some deﬁnitions of quantities and concepts which will be
used extensively later. We will not worry too much here about the conditions
necessary for the quantities introduced to be well deﬁned. If we write an
expression, such as an integral, we will assume that it exists, unless otherwise
stated. Questions concerning, e.g., measurability or completeness are likewise
neglected.
12 Andrej Nikolaevich Kolmogorov (1903–1987): a Russian mathematician, often
refereed to as one of the most prominent ﬁgures in the 20th century mathemat-
ics and the father of modern probability theory – with emphasis on the word
“theory”.

6
1 Inverse problems and subjective computing
Deﬁne Ω to be a probability space equipped with a probability measure P
that measures the probability of events E ⊂Ω. In other words, Ω contains
all possible events in the form of its own subsets.
We require that
0 ≤P(E) ≤1,
meaning that we express the probability of an event as a number between 0
and 1. An event with probability 1 is the equivalent of a done deal13; one with
probability 0 the exact opposite.
The probability is additive in the sense that if A ∩B = ∅, A, B ⊂Ω,
P(A ∪B) = P(A) + P(B).
Since Ω contains all events,
P(Ω) = 1,
(“something happens”)
and
P(∅) = 0.
(“nothing happens”)
Two events, A and B, are independent, if
P(A ∩B) = P(A)P(B).
In this case, the probability of event A and of event B is the product of the
probability of A times the probability of B. Since a probability is a number
smaller than or equal to one, the probability of A and B can never be larger
than the probability of either A or B. For example, the probability of getting
a head when tossing a fair coin is 1
2. If we toss the coin twice, the probability
of getting two heads is 1
2 · 1
2 = 1
4
The conditional probability of A on B is the probability that A happens
provided that B happens,
P(A | B) = P(A ∩B)
P(B)
.
The probability of A conditioned on B is never smaller than the probability
of A and B, since P(B) ≤1, and it can be much larger if P(B) is very
small. That corresponds to the intuitive idea that the conditional probability
is the probability on the probability subspace where the event B has already
occurred. It follows from the deﬁnition of independent events that, if A and
B are mutually independent,
P(A | B) = P(A),
P(B | A) = P(B).
The probability space is an abstract entity that is a useful basis for the-
oretical considerations. In practice, it is seldom constructed explicitly, and it
is usually circumvented by the consideration of probability densities over the
state space.
13 meaning something which certainly happens.

1.2 Through the formal theory, lightly
7
Example 1.5: Consider the problem of tossing a die. The state space, i.e.,
the set of all possible outcomes, is S =

1, 2, 3, 4, 5, 6

. In this case, we
may deﬁne the probability space as coinciding with the state space, Ω = S.
A particular die is then modeled as a mapping
X : Ω →S,
i →i,
1 ≤i ≤6.
and the probabilities of random events, using this particular die, are
P

X(i) = i

= wi, with
6

i=1
wi = 1.
Here and in the sequel, we use the curly brackets with the probability
when the event, i.e., the subspace of Ω, is given in the form of a descriptive
condition such as X(i) = i. The weights wj deﬁne a probability density
over the state space.
The above mapping X is a random variable. Since in this book we are
mostly concerned about phenomena taking on real values, or more generally,
values in Rn, we assume that the state space is R. Therefore we restrict our
discussion here to probability densities that are absolutely continuous with
respect to the Lebesgue measure over the reals14.
Given a probability space Ω, a real valued random variable X is a mapping
X : Ω →R
which assigns to each element ω of Ω a real value X(ω). We call x = X(ω),
ω ∈Ω, a realization of X. In summary, a random variable is a function, while
its realizations are real numbers.
For each B ⊂R,
μX(B) = P(X−1(B)) = P

X(ω) ∈B

.
is called the probability distribution of X, i.e., μX(B) is the probability of
the event x ∈B. The probability distribution μX(B) measures the size of
the subspace of Ω mapped onto B by the random variable X. The probability
distribution is linked to the probability density πX via the following integration
process:
μX(B) =

B
πX(x)dx.
(1.1)
From now on, unless it is necessary to specify the dependency on the random
variable, we write simply
πX(x) = π(x).
14 Concepts like absolute continuity of measures will play no role in this book, and
this comment has been added here in anticipation of “friendly ﬁre” from our
fellow mathematicians.

8
1 Inverse problems and subjective computing
Analogously to what we did in Example 1.5, we identify the probability
space Ω and the state space R. It is tempting to identify the probability mea-
sure P with the Lebesgue measure. The problem with this is that the measure
of R is inﬁnite, thus the Lebesgue measure cannot represent a probability
measure. It is possible – but rather useless for our goals – to introduce an
integrable weight on R, deﬁning a probability measure over R, and to inte-
grate with respect to this weight. Since such complication would bring mostly
confusion, we stick to the intuitive picture of the Lebesgue measure as the ﬂat
reference probability distribution.
Given a random variable X, its expectation is the center of mass of the
probability distribution,
E

X

=

R
xπ(x)dx = x,
while its variance is the expectation of the squared deviation from the
expectation,
var

X

= σ2
X = E

(X −x)2
=

R
(x −x)2π(x)dx.
It is the role of the variance to measure how distant from the expectation the
values taken on by the random variable are. A small variance means that the
random variable takes on mostly values close to the expectation, while a large
variance means the opposite.
The expectation and variance are the ﬁrst two moments of a probability
density function. The kth moment is deﬁned as
E

(X −x)k
=

R
(x −x)kπ(x)dx.
The third and fourth moments are closely related to the skewness and kurtosis
of the density.
Example 1.6: The expectation of a random variable, in spite of its name,
is not necessarily the value that we should expect a realization to take on.
Whether this is the case or not depends on the distribution of the random
variable. Let Ω = [−1, 1], the probability P be uniform, in the sense that
P(I) = 1
2

I
dx = 1
2|I|,
I ⊂[−1, 1].
and consider the two random variables
X1 : [−1, 1] →R,
X1(ω) = 1
∀ω ∈R,
and
X2 : [−1, 1] →R,
X2(ω) =

2, ω ≥0
0, ω < 0

1.2 Through the formal theory, lightly
9
It is immediate to check that
E

X1

= E

X2

= 1,
and that while all realizations of X1 take on the expected value, this is
never the case for the realizations of X2. This simple consideration serves
as a warning that describing a random variable only via its expectation
may be very misleading. For example, although females make up roughly
half of the world’s population, and males the other half, we do not expect
a randomly picked person to have one testicle and one ovary! By the same
token, why should we expect by default a random variable to take on its
expected value?
The joint probability density of two random variables, X and Y , πXY (x, y)
= π(x, y), is
P

X ∈A, Y ∈B

= P

X−1(A) ∩Y −1(B)

=
 
A×B
π(x, y)dxdy
= the probability of the event that x ∈A
and, at the same time, y ∈B.
We say that the random variables X and Y are independent if
π(x, y) = π(x)π(y),
in agreement with the deﬁnition of independent events.
The covariance of X and Y is the mixed central moment
cov(X, Y ) = E

(X −¯x)(Y −¯y)

.
It is straightforward to verify that
cov

X, Y

= E

XY

−E

X

E

Y

.
The correlation coeﬃcient of X and Y is
corr

X, Y

= cov

X, Y

σXσY
,
σX =
	
var(X), σY =
	
var(Y ).
or, equivalently, the correlation of the centered normalized random variables,
corr

X, Y

= E
 
X 
Y

,

X = X −¯x
σX
,

Y = Y −¯y
σY
.
It is an easy exercise to verify that
E
 ˜X

= E
 ˜Y

= 0,
var
 ˜X

= var
 ˜Y

= 1.

10
1 Inverse problems and subjective computing
The random variables X and Y are uncorrelated if their correlation coeﬃcient
vanishes, i.e.,
cov

X, Y

= 0.
If X and Y are independent variables they are uncorrelated, since
E

(X −x)(Y −y)

= E

X −x

E

Y −y

= 0.
The viceversa does not necessarily hold. Independency aﬀects the whole den-
sity, not just the expectation.
Two random variables X and Y are orthogonal if
E

XY

= 0.
In that case
E

(X + Y )2
= E

X2
+ E

Y 2
.
Given two random variables X and Y with joint probability density π(x, y),
the marginal density of X is the probability of X when Y may take on any
value,
π(x) =

R
π(x, y)dy.
The marginal of Y is deﬁned analogously by
π(y) =

R
π(x, y)dx.
In other words, the marginal density of X is simply the probability density of
X, regardless of Y .
The conditional probability density of X given Y is the probability density
of X assuming that Y = y. By considering conditional probabilities at the
limit when Y belongs to an interval shrinking to a single point y, one may
show that the conditional density is
π(x | y) = π(x, y)
π(y) ,
π(y) ̸= 0.
Observe that, by the symmetry of the roles of X and Y , we have
π(x, y) = π(x | y)π(y) = π(y | x)π(x),
(1.2)
leading to the important identity,
π(x | y) = π(y | x)π(x)
π(y)
,
(1.3)
known as the Bayes formula. This identity will play a central role in the rest
of this book.

1.2 Through the formal theory, lightly
11
The interpretations of the marginal and conditional densities are repre-
sented graphically in Figure 1.1.
Conditional density leads naturally to conditional expectation or condi-
tional mean,
E

X | y

=

R
xπ(x | y)dx.
To compute the expectation of X via its conditional expectation, observe
that
E

X

=

xπ(x)dx =

x
 
π(x, y)dy

dx,
and, substituting (1.2) into this expression, we obtain
E

X

=

x
 
π(x | y)π(y)dy

dx
=
  
xπ(x | y)dx

π(y)dy =

E

X | y

π(y)dy.
(1.4)
So far, we have only considered real valued univariate random variables.
A multivariate random variable is a mapping
X =
⎡
⎢⎣
X1
...
Xn
⎤
⎥⎦: Ω →Rn,
where each component Xi is a real-valued random variable. The probability
density of X is the joint probability density π = πX : Rn →R+ of its
components, and its expectation is
x =

Rn xπ(x)dx ∈Rn,
or, componentwise,
xi =

Rn xiπ(x)dx ∈R,
1 ≤i ≤n.
The covariance matrix is deﬁned as
cov(X) =

Rn(x −x)(x −x)Tπ(x)dx ∈Rn×n,
or, componentwise,
cov(X)ij =

Rn(xi −xi)(xj −xj)π(x)dx ∈Rn×n,
1 ≤i, j ≤n.

12
1 Inverse problems and subjective computing
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
2
4
0.2
0.4
0.6
0.8
0
1
2
3
4
5
0
2
4
0.2
0.4
0.6
0.8
Fig. 1.1.
Conditional density (left) and marginal density (right). The ellipsoids
in the top panels are equiprobability curves of the joint probability density π(x, y).
The lower panels show a 3D representation of the densities.
The covariance matrix is symmetric and positive semi-deﬁnite. While the sym-
metry is implicit in the deﬁnition, the positive semi-deﬁniteness follows be-
cause, for any v ∈Rn, v ̸= 0,
vTcov(X)v =

Rn

vT(x −x)

(x −x)Tv

π(x)dx
(1.5)
=

Rn

vT(x −x)
2π(x)dx ≥0.
The expression (1.5) measures the variance of X in the direction of v. If the
probability density is not degenerate in any direction, in the sense that the
values of X −x are not restricted to any proper subspace, then the covariance
is positive deﬁnite.
The diagonal entries of the covariance matrix are the variances of the
individual components of the random variable X. Denoting by x′
i ∈Rn−1 the
vector x with the ith component deleted, we have

1.2 Through the formal theory, lightly
13
cov(X)ii =

Rn(xi −xi)2π(x)dx =

R
(xi −xi)2
 
Rn−1 π(xi, x′
i)dx′
i




=π(xi)
dxi
=

R
(xi −xi)2π(xi)dxi = var(Xi).
The marginal and conditional probabilities for multivariate random variables
are deﬁned by the same formulas as in the case of real valued, or univariate,
random variables.
We conclude this section with some examples of expectations.
Example 1.7: Random variables waiting for the train: assume that every
day, except on Sundays, a train for your destination leaves every S minutes
from the station. On Sundays, the interval between trains is 2S minutes.
You arrive at the station with no information about the trains time table.
What is your expected waiting time?
Deﬁne a random variable, T = waiting time, whose distribution on working
days is
T ∼π(t | working day) = 1
S χS(t),
χS(t) =

1, 0 ≤t < S,
0 otherwise
On Sundays, the distribution of T is
T ∼π(t | Sunday) = 1
2S χ2S(t).
If you are absolutely sure that it is a working day, the expected waiting
time is
E

T | working day

=

tπ(t | working day)dt = 1
S
 S
0
tdt = S
2 .
On Sundays, the expected waiting time is clearly double.
If you have no idea which day of the week it is15, you may give equal
probability to each day. Thus,
π

working day

= 6
7,
π

Sunday

= 1
7.
Since you are interested only in the waiting time for your train, regardless
of the day of the week, you marginalize over the days of the week and
obtain
E

T

= E

T | working day

π

working day

+ E

T | Sunday

π

Sunday

= 3S
7 + S
7 = 4S
7 .
Here we have used formula (1.4), with the integral replaced by a ﬁnite sum.
15 The probability of this to happen seems to be much larger for mathematicians
than for other individuals.

14
1 Inverse problems and subjective computing
The following example demonstrates how from simple elementary prin-
ciples one can construct a probability density that has the characteristics
of objective chance, i.e., the resulting probability density follows inarguably
from those principles. Naturally, the elementary principles may always be
questioned.
Example 1.8: Distributions and photon counts. A weak light source emits
photons that are counted with a CCD (Charged Coupled Device). The
counting process N(t),
N(t) = number of particles observed in [0, t] ∈N
is an integer-valued random variable.
To set up a statistical model, we make the following assumptions:
1. Stationarity: Let Δ1 and Δ2 be any two time intervals of equal length,
and let n be a non-negative integer. Assume that
P

n photons arrive in Δ1

= P

n photons arrive in Δ2

.
2. Independent increments: Let Δ1, . . . , Δn be non-overlapping time in-
tervals and k1, . . . , kn non-negative integers. Denote by Aj the event
deﬁned as
Aj = kj photons arrive in the time interval Δj.
We assume that these events are mutually independent,
P

A1 ∩· · · ∩An

= P

A1} · · · P{An}.
3. Negligible probability of coincidence: Assume that the probability of
two or more events occurring at the same time is negligible. More
precisely, assume that N(0) = 0 and
lim
h→0
P

N(h) > 1

h
= 0.
This condition can be interpreted as saying that the number of counts
increases at most linearly.
If these assumptions hold, then it can be shown16 that N is a Poisson
process:
P

N(t) = n

= (λt)n
n!
e−λt,
λ > 0.
We now ﬁx t = T= recording time, deﬁne a random variable N = N(T),
and let θ = λT. We write
N ∼Poisson(θ).
16 For the proof, see, e.g., [Gh96].

1.2 Through the formal theory, lightly
15
We want to calculate the expectation of this Poisson random variable.
Since the discrete probability density is
π(n) = P

N = n

= θn
n! e−θ,
θ > 0,
(1.6)
and our random variable takes on discrete values, in the deﬁnition of ex-
pectation instead of an integral we have an inﬁnite sum, that is
E

N

=
∞

n=0
nπ(n) = e−θ
∞

n=0
nθn
n!
= e−θ
∞

n=1
θn
(n −1)! = e−θ
∞

n=0
θn+1
n!
= θ e−θ
∞

n=0
θn
n!
  
=eθ
= θ.
We calculate the variance of a Poisson random variable in a similar way,
writing ﬁrst
E

(N −θ)2
= E

N 2} −2θ E

N

  
=θ
+θ2
= E

N 2
−θ2
=
∞

n=0
n2π(n) −θ2,
and, substituting the expression for π(n) from(1.6), we get
E

(N −θ)2
= e−θ
∞

n=0
n2 θn
n! −θ2 = e−θ
∞

n=1
n
θn
(n −1)! −θ2
= e−θ
∞

n=0
(n + 1)θn+1
n!
−θ2
= θe−θ
∞

n=0
nθn
n! + θe−θ
∞

n=0
(θ)n
n!
−θ2
= θe−θ
(θ + 1)eθ
−θ2
= θ,
that is, the mean and the variance coincide.

16
1 Inverse problems and subjective computing
1.3 How normal is it to be normal?
A special role in stochastics and statistics is played by normal or normally
distributed random variables. Another name for normal is Gaussian. We start
by giving the deﬁnition of normal random variable and then proceed to see
why normality is so widespread.
A random variable X ∈R is normally distributed, or Gaussian,
X ∼N(x0, σ2),
if
P

X ≤t

=
1
√
2πσ2
 t
−∞
exp

−1
2σ2 (x −x0)2

dx.
It can be shown that
E{X} = x0,
var(X) = σ2.
We now extend this deﬁnition to the multivariate case. A random variable
X ∈Rn is Gaussian if its probability density is
π(x) =

1
(2π)ndet(Γ)
1/2
exp

−1
2(x −x0)TΓ −1(x −x0)

,
where x0 ∈Rn, and Γ ∈Rn×n is symmetric positive deﬁnite. The expectation
of this multivariate Gaussian random variable is x0 and its covariance matrix
is Γ. A standard normal random variable is a Gaussian random variable with
zero mean and covariance the identity matrix.
Gaussian random variables arise naturally when macroscopic measure-
ments are averages of individual microscopic random eﬀects. They arise com-
monly in the modeling of pressure, temperature, electric current and lumi-
nosity. The ubiquity of Gaussian random variables can be understood in the
light of the Central Limit Theorem, which we are going to state here for
completeness, without presenting a proof17.
Central Limit Theorem: Assume that real valued random variables
X1, X2, . . . are independent and identically distributed (i.i.d.), each with
expectation μ and variance σ2. Then the distribution of
Zn =
1
σ√n

X1 + X2 + · · · + Xn −nμ

converges to the distribution of a standard normal random variable,
lim
n→∞P

Zn ≤x

= 1
2π
 x
−∞
e−t2/2dt.
17 The proof can be found in standard text books, e.g., [Gh96].

1.3 How normal is it to be normal?
17
Another way of thinking about the result of the Central Limit Theorem is
the following. If
Yn = 1
n
n

j=1
Xj,
and n is large, a good approximation for the probability distribution of Y is
Yn ∼N

μ, σ2
n

.
As an example of the usefulness of this result, consider a container ﬁlled
with gas. The moving gas molecules bombard the walls of the container, the
net eﬀect being the observed pressure of the gas. Although the velocity dis-
tribution of the gas molecules is unknown to us, the net eﬀect, which is the
average force per unit area is, by the Central Limit Theorem, Gaussian.
Example 1.9: Consider again the CCD camera of the previous example
with a relatively large number of photons. We may think of dividing the
counter unit into smaller sub-counters, so that the total count is the sum of
the sub-counts. Since all sub-counters are identical and the photon source
is the same for each of them, it is to be expected that the total count is
a sum of identical Poisson processes. Thus it is reasonable to expect that,
when the mean θ is large, the Poisson distribution can be approximated
well by a Gaussian distribution.
If this reasoning is correct, the approximating Gaussian distribution should
have asymptotically the same mean and variance as the underlying Poisson
distribution. In Figure 1.2 we plot the Poisson probability density
n →θn
n! e−θ = πPoisson(n | θ)
versus the Gaussian approximation
x →
1
√
2πθ
exp

−1
2θ(x −θ)2

= πGaussian(x | θ, θ),
for increasing values of the mean. The approximation, at least visually,
becomes more accurate as the mean increases. Notice, however, that since
the Poisson distribution is skewed, matching the means does not mean
that the maxima are matched, nor does it give a measure for the goodness
of the ﬁt.
To obtain a quantitative measure for the goodness of the approximation,
we may consider metric distances between the probability densities. Several
metrics for the distance between probability distributions exist. One often
used in engineering literature,
distKL

πPoisson( · | θ), πGaussian( · | θ, θ)

=
∞

n=0
πPoisson(n | θ) log

πPoisson(n | θ)
πGaussian(n | θ, θ)

,

18
1 Inverse problems and subjective computing
0
5
10
15
20
0
0.05
0.1
0.15
0.2
Mean = 5
Mean = 5
0
5
10
15
20
0
0.05
0.1
0.15
0.2
Mean = 10
0
10
20
30
40
50
0
0.02
0.04
0.06
0.08
0.1
0.12
Mean = 15
0
10
20
30
40
50
0
0.02
0.04
0.06
0.08
0.1
0.12
Mean = 20
Fig. 1.2.
Poisson distributions (dots) and their Gaussian approximations (solid
line) for various values of the mean θ.
is known as the Kullback–Leibler distance.
0
10
20
30
40
50
10
−3
10
−2
10
−1
Fig. 1.3. The Kullback-Leibler distance of the Poisson distribution from its Gaus-
sian approximation as a function of the mean θ in logarithmic scale. The horizontal
line indicates where the distance has dropped to one tenth of its value at θ = 1.
Figure 1.3 shows the Kullback-Leibler distance as a function of the param-
eter θ. The often quoted rule of thumb is that when θ ≥15, it is reasonable
to approximate the Poisson distribution with a Gaussian.
The Central Limit Theorem justiﬁes in many circumstances the use of
Gaussian
approximations.
As
we
shall
see,
normal
distributions
are

1.3 How normal is it to be normal?
19
computationally quite convenient. There is also a cognitive dimension which
plays an important role in the widespread use of Gaussian approximations, as
illustrated by the following example.
Example 1.10: An experienced radiologist decides to quantify her qualita-
tive knowledge of typical radiographs of the human thorax. The extensive
collection of her memories of such radiographs all display the same typical
features: a heart, two lungs, the ribcage. It seems like in her mind the col-
lective memories of the radiographs have melted into an average image18,
with the understanding that there could be slight variations around it.
Thus an eﬃcient way to summarize her qualitative experience in quanti-
tative terms is to describe the average image and the degree of variation
around it. It is not unreasonable then to write a Gaussian model for a typ-
ical thoracic radiograph, as an expression of the mental averaging process.
This description is clearly subjective, as it relies on personal experiences.
Exercises
1. Consider the following modiﬁcation of Example 1.7. Instead of every S
minutes, the train leaves from the station every day at the same, unevenly
distributed times, tj, 1 ≤j ≤n. Assume that you have the train time table
but no watch19, nor any idea of what time of the day it is. What is the
expected waiting time in this case?
2. Consider a vector valued random variable,
A = Xe1 + Y e2,
where e1 and e2 are the orthogonal Cartesian unit vectors, and X and Y
are real valued random variables,
X, Y ∼N(0, σ2).
The random variable
R = ∥A∥
is then distributed according to the Rayleigh distribution,
18 The cognitive dimension of averaging visual features has clearly emerged in studies
where digitized images of human faces are averaged and then assessed visually
by people. It turns out that an average face is typically found attractive, an
easily justiﬁed outcome from the evolutionary perspective. The result seems to
suggest that humans, in some sense, recognize the average, and, therefore, the
assumption that the brain averages images is not unnatural. See, e.g., Langlois,
J.H. and Roggman, L.A.: Attractive faces are only average. Psychological Science
1, 115–121 (1990).
19 The situation is not as bad as having a loaf of bread with a hard crust and no
teeth to eat it with.

20
1 Inverse problems and subjective computing
R ∼Rayleigh(σ2).
Derive the analytic expression of the Rayleigh distribution and write a
Matlab program that generates points from the Rayleigh distribution.
Make a plot of the distribution and a histogram of the points you gener-
ated.
3. Calculate the third and fourth moments of a Gaussian density.

2
Basic problem of statistical inference
The deﬁnition of statistical inference given at the beginning of Chapter 1
suggests that the fundamental problem of statistical inference can be stated
as follows:
Given a set of observed realizations of a random variable X,
S =

x1, x2, . . . , xN

,
xj ∈Rn,
(2.1)
we want to infer on the underlying probability distribution that gives rise
to the data S.
An essential portion of statistical inference deals with statistical modelling
and analysis, which often includes the description and understanding of the
data collection process. Therefore, statistical inference lies at the very core of
scientiﬁc modeling and empirical testing of theories.
It is customary to divide the various approaches in two categories;
•
Parametric approach: It is assumed that the underlying probability distri-
bution giving rise to the data can be described in terms of a number of
parameters, thus the inference problem is to estimate these parameters, or
possibly their distributions.
•
Non-parametric approach: No assumptions are made about the speciﬁc
functional form of the underlying probability density. Rather, the approach
consists of describing the dependency or independency structure of the
data, usually in conjunction with numerical exploration of the probability
distribution.
In classical statistical problems, parametric models depend on relatively
few parameters such as the mean and the variance. If the data set is large, this
is equivalent to reducing the model, in the sense that we attempt to explain
a large set of values with a smaller one. However, when we apply parametric
methods to inverse problems, this may not be the case. The parametric model

22
2 Basic problem of statistical inference
typically depends on the unknowns of primary interest, and the number of
degrees of freedom may exceed the size of the data.
Nonparametric models have the advantage of being less sensitive to mis-
modelling the genesis of the data set. The disadvantages include the fact that,
in the absence of a synthetic formula, it is often hard to understand the factors
that aﬀect the data.
The basic problem of statistical inference as stated above is non-committal
with respect to the interpretation of probability. It is interesting to notice
that, even within the parametric modelling approach, the methodology of
statistical inference is widely diﬀerent, depending on whether one adheres to
the frequentist point of view of probability, or to the Bayesian paradigm1. In
the framework of parametric models, one may summarize the fundamental
diﬀerence by saying that a frequentist believes in the existence of underlying
true parameter values, while in the Bayesian paradigm, anything that is not
known, including the parameters, must be modeled as a random variable,
randomness being an expression of lack of information, or, better said, of
ignorance2. Hence, there is a Platonist aspect in the frequentist’s approach,
while the Bayesian approach is closer to the Aristotelian concept of learning
as starting from a clean slate. As Aristotle learned a lot from his mentor
Plato, we can also learn a lot from the frequentist’s approach, which will be
our starting point.
2.1 On averaging
The most elementary way to summarize a large set of statistical data is to
calculate its average. There is a rationale behind this, summarized by the Law
of Large Numbers. This law has several variants, one of which is the following.
Law of Large Numbers: Assume that X1, X2, . . . are independent and
identically distributed random variables with ﬁnite mean μ and variance
σ2. Then
lim
n→∞
1
n

X1 + X2 + · · · + Xn

= μ
almost certainly.
The expression almost certainly means that, with probability one, the aver-
ages of any realizations x1, x2, . . . of the random variables X1, X2, . . . converge
towards the mean. This is good news, since data sets are always realizations.
Observe the close connection with the Central Limit Theorem, which states
the convergence in terms of probability densities.
1 We recommend the reader to pick up any statistics journal and appreciate the
remarkable diﬀerence in style of writing, depending on the approach of the author.
2 We use the word ignorance in this case to indicate lack of knowledge, which goes
back to its original Greek meaning.

2.1 On averaging
23
The following example is meant to clarify how much, or how little, averages
over samples are able to explain the underlying distribution.
Example 2.1: Given a sample S of vectors xj ∈R2, 1 ≤j ≤n, in the
plane, we want to set up a parametric model for it, assuming that the points
are independent realizations of a normally distributed random variable,
X ∼N(x0, Γ),
with unknown mean x0 ∈R2 and covariance matrix Γ ∈R2×2. We express
this by writing the probability density of X in the form
π(x | x0, Γ) =
1
2πdet(Γ)1/2 exp

−1
2(x −x0)TΓ −1(x −x0)

,
i.e., the probability density has the known form provided that x0 and Γ
are given. The inference problem is therefore to estimate the parameters
x0 and Γ.
As the Law of Large Numbers suggests, we may hope that n is suﬃciently
large to justify an approximation of the form
x0 = E

X

≈1
n
n

j=1
xj = x0.
(2.2)
To estimate the covariance matrix, observe ﬁrst that if X1, X2, . . . are
independent and identically distributed, so are f(X1), f(X2), . . . for any
function f : R2 →Rk, therefore we may apply the Law of Large Numbers
again. In particular, we may write
Γ = cov(X) = E

(X −x0)(X −x0)T
≈E

(X −x0)(X −x0)T
(2.3)
≈1
n
n

j=1
(xj −x0)(xj −x0)T = Γ.
The estimates ˆx and ˆΓ from formulas (2.2) and (2.3) are often referred to
as the empirical mean and covariance, respectively.
In the ﬁrst numerical simulation, let us assume that the Gaussian paramet-
ric model can indeed explain the sample, which has in fact been generated
by drawing from a two dimensional Gaussian density.
The left panel of Figure 2.1 shows the equiprobability curves of the original
Gaussian probability density, which are ellipses. A scatter plot of a sample
of 200 points drawn from this density is shown in the right panel of the same
ﬁgure. We then compute the eigenvalue decomposition of the empirical
covariance matrix,

Γ = UDU T,
(2.4)

24
2 Basic problem of statistical inference
Fig. 2.1.
The ellipses of the equiprobability curves of the original Gaussian dis-
tribution (left), and on the right, the scatter plot of the random sample and the
eigenvectors of the empirical covariance matrix, scaled by two times the square root
of the corresponding eigenvalues. These square roots represent the standard devia-
tions of the density in the eigendirections.
where U ∈R2×2 is an orthogonal matrix whose columns are the eigenvec-
tors scaled to have unit length, and D ∈R2×2 is the diagonal matrix of
the eigenvalues of 
Γ,
U =

v1 v2

,
D =
λ1
λ2

,

Γvj = λjvj
j = 1, 2.
The right panel of Figure 2.1 shows the eigenvectors of ˜Γ scaled to have
length twice the square root of the corresponding eigenvalues, applied at
the empirical mean. The plot is in good agreement with our expectation.
To better understand the implications of adopting a parametric form for
the underlying density, we now consider a sample from a density that is
far from Gaussian. The equiprobability curves of this non-Gaussian den-
sity are shown in the left panel of Figure 2.2. The right panel shows the
scatter plot of a sample of 200 points drawn from this density. We calcu-
late the empirical mean, the empirical covariance matrix and its eigenvalue
decomposition. As in the Gaussian case, we plot the eigenvectors scaled
by a factor twice the square root of the corresponding eigenvalues, applied
at the empirical mean. While the mean and the covariance estimated from
the sample are quite reasonable, a Gaussian model is clearly not in agree-
ment with the sample. Indeed, if we would generate a new sample from
a Gaussian density with the calculated mean and covariance, the scatter
plot would be very diﬀerent from the scatter plot of the sample.
In two dimensions, it is relatively easy to check whether a Gaussian ap-
proximation is reasonable by visually inspecting scatter plots. Although
in higher dimensions, we can look at the scatter plots of two-dimensional

2.1 On averaging
25
Fig. 2.2. The equiprobability curves of the original non-Gaussian distribution (left)
and the sample drawn from it (right), together with the scaled eigenvectors of the
empirical covariance matrix applied at the empirical mean.
projections and try to assess if the normality assumption is reasonable3, it
is nonetheless preferable to develop systematic methods to investigate the
goodness of the Gaussian approximation. Below, we present one possible
idea.
Let π(x) denote the Gaussian probability density,
π ∼N(x0, Γ),
where the mean and the covariance are obtained from a given sample.
Consider sets of the form
Bα =

x ∈R2 | π(x) ≥α

,
α > 0.
Since π is a Gaussian density, the set Bα is either empty (α large) or the
interior of an ellipse. Assuming that π is the probability density of the
random variable X, we can calculate the probability that X is in Bα by
evaluating the integral
P

X ∈Bα

=

Bα
π(x)dx.
(2.5)
We call Bα the credibility ellipse4 with credibility p, 0 < p < 1, if
P

X ∈Bα

= p.
(2.6)
3 As the legendary Yogi Berra summarized in one of his famous aphorisms, “you
can observe a lot of things just by looking at them”.
4 We choose here to use the Bayesian concept of credibility instead of the frequentist
concept of conﬁdence, since it is more in line with the point of view of this book.
Readers conﬁdent enough are welcome to choose diﬀerently.

26
2 Basic problem of statistical inference
The above equation relates α and p, and we may solve it for α as a function
of p,
α = α(p).
We may expect that, if the sample S of size n is normally distributed, the
number of points inside Bα(p) is approximately pn,
#

xj ∈Bα(p)

≈pn.
(2.7)
A large deviation from this suggests that the points of S are not likely to
be realizations of a random variable whose probability density is normal.
0
20
40
60
80
100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Gaussian
Non−gaussian
Fig. 2.3. Percentage of sample points inside the credibility ellipse of the approxi-
mating Gaussian distribution plotted against the sample size.
To calculate the number of sample points within Bα, we start by noticing
that, replacing Γ with its eigenvalue decomposition (2.4),
(x −x0)T Γ −1(x −x0) = (x −x0)TUD−1U T(x −x0)
= ∥D−1/2U T(x −x0)∥2,
where
D−1/2 =

1/√λ1
1/√λ2

.
After the change of variable
w = f(x) = W(x −x0),
W = D−1/2U T,
the integral (2.5) becomes

2.1 On averaging
27

Bα
π(x)dx =
1
2π

det( Γ)
1/2

Bα
exp

−1
2(x −x0)T Γ −1(x −x0)

dx
=
1
2π

det( Γ)
1/2

Bα
exp

−1
2∥W(x −x0∥2

dx
= 1
2π

f(Bα)
exp

−1
2∥w∥2

dw,
since
dw = det(W)dx =
1
√λ1λ2
dx =
1
det( Γ)1/2 dx.
The equiprobability curves for the density of w are circles centered at the
origin,
f(Bα) = Dδ =

w ∈R2 | ∥w∥< δ

for some δ > 0 that depends on α. The value of δ can be determined
directly from (2.6). In fact, passing to radial coordinates (r, θ), we have
1
2π

Dδ
exp

−1
2∥w∥2

dw =
 δ
0
exp

−1
2r2

rdr
= 1 −exp

−1
2δ2

= p,
which implies that
δ = δ(p) =

2 log

1
1 −p

.
To see if a sample point xj lies inside the p-credibility ellipse, it suﬃces to
check if
∥wj∥< δ(p),
wj = W(xj −x0),
1 ≤j ≤n,
holds.
Figure 2.3 shows the plots of the function
p →1
n#

xj ∈Bα(p)

for two samples, one generated from the Gaussian density of Figure 2.1 and
the other from the non-Gaussian one of Figure 2.2. The non-Gaussianity
of the latter sample can be deduced from this plot, in particular when the
value of p is small.
We conclude with the portion of a Matlab code that was used for the calcu-
lations. In the code, S is a 2×n matrix whose columns are the coordinates
of the sample points. The generation of the data is not discussed here,
since drawing samples from a given density will be discussed at length in
Chapters 5 and 9.

28
2 Basic problem of statistical inference
n = length(S(1,:));
% Size of the sample
xmean = (1/n)*(sum(S’)’);
% Mean of the sample
CS = S - xmean*ones(1,n);
% Centered sample
Gamma = 1/n*CS*CS’;
% Covariance matrix
% Mapping the the x-sample to w-sampe
[V,D] = eig(Gamma);
% Eigenvalue decomposition
W = diag([1/sqrt(D(1,1));1/sqrt(D(2,2))])*V’;
WS = W*CS;
% w-sample matrix
% Calculating the percentage of scatter points inside
% the credibility ellipses
normWS2 = sum(WS.^2);
rinside = zeros(11,1);
rinside(11) = n;
for j = 1:9
delta2 = 2*log(1/(1-j/10));
rinside(j+1) = sum(normWS2<delta2);
end
rinside = (1/n)*rinside;
plot([0:10:100],rinside,’k.-’,’MarkerSize’,12)
Notice that in the code the sample covariance is calculated from (2.3)
instead of using the mathematically equivalent formula
Γ = 1
n
n

j=1
xjxT
j −
x0
xT
0 .
(2.8)
Although for the present example either formula could have been used,
in higher dimensional problems, and in particular if the sample is highly
correlated, the computational performance of the two formulas might be
quite diﬀerent. The matrix obtained from (2.3) is more likely to be nu-
merically positive deﬁnite, while in some cases the matrix generated using
formula (2.8) may be so severely aﬀected by round-oﬀerrors to become
numerically singular or indeﬁnite.
Averaging of data is a common practice in science and engineering. The
motivation for averaging may be to summarize an excessive amount of the
data in comprehensible terms, but often also noise reduction5, which is brieﬂy
discussed in the following example.
5 Unfortunately, in many cases the motivation is just to represent the data disguised
as statistical quantities, while in reality the original data would be a lot more
informative.

2.1 On averaging
29
Example 2.2: Assume that we have a measuring device whose output
is a noisy signal. This means that the signal generated contains a noise
component. In order to estimate the noise level, we perform calibration
by attaching the measurement device to a dummy load whose theoretical
noiseless output we know. Subtracting the theoretical noise from the actual
output of the device we obtain a pure noise signal. Assume that the device
samples the continuous output signal, and the sampled signal is a vector of
given length n. When we perform the calibration measurement, the output
is thus a noise vector. In setting up a parametric model, we assume that
the noise vector x ∈Rn is a realization of a random variable,
X : Ω →Rn.
To estimate the noise level, we assume that the components of X are mu-
tually independent, identically distributed random variables. The average
noise and its variance, based on a single measurement vector, can be esti-
mated via the formulas
x0 = 1
n
n

j=1
xj,
σ2 = 1
n
n

j=1
(xj −x0)2.
Assume that, after having performed the calibration measurement, we re-
alize that the signal-to-noise ratio of our device is so poor that important
features of the signal are cluttered under the noise. In an eﬀort to reduce
the noise level, we repeat the measurement several times and average the
noisy signals, in the hope of reducing the variance of the noise component.
How many measurements do we need to reach a desired signal-to-noise ra-
tio? If x(k) ∈Rn denotes the noise vector in the kth repeated measurement,
the noise vector in the averaged signal is
x = 1
N
N

k=1
x(k) ∈Rn.
Assuming that the repeated measurements are mutually independent and
the measurement conditions from one measurement to another do not
change, we may still view the vector x as an average of independent iden-
tically distributed random variables. Since the Central Limit Theorem as-
serts that asymptotically, as N goes to inﬁnity, the average is Gaussian and
its variance goes to zero like σ2/N, to obtain a signal whose noise vari-
ance is below a threshold value τ 2, we need to repeat the measurements
N times, where
σ2
N < τ 2.
To demonstrate the noise reduction by averaging, we generate Gaussian
noise vectors of length n = 50 with variance σ2 = 1, and average them. In

30
2 Basic problem of statistical inference
Figure 2.4 we show averages of 1, 5 and 25 of such signals. We also indicate
the noise level, estimated as
noise level = 2 σ
√
N
,
where N is the number of averaged noise vectors and σ is the standard
deviation of the single noise vectors.
0
10
20
30
40
50
−3
−2
−1
0
1
2
3
N = 1
0
10
20
30
40
50
−3
−2
−1
0
1
2
3
N = 5
0
10
20
30
40
50
−3
−2
−1
0
1
2
3
N = 25
Fig. 2.4.
Averaged noise signals. The shading indicates where the noise level is
twice the estimated standard deviation, which in this case is 2/
√
N.
The mean and variance estimation by averaging relies solely on the formu-
las suggested by the Law of Large Numbers, and no information concerning
the estimated distribution is used. The following section adds more complex-
ity to the problem and discusses maximum likelihood estimators, a central
concept in frequentist statistics.

2.2 Maximum Likelihood, as frequentists like it
31
2.2 Maximum Likelihood, as frequentists like it
Assume that we can observe the realizations of a random variable X which
depends, in turn, on a parameter θ, which we cannot observe but whose values
are really what we are interested in. The parameter can be a real number, a
vector, or an array of real numbers. We interpret the sample S as a collection
of realizations of a random variable
X ∼π(x | θ),
θ ∈Rk,
where we use the notation of conditional probability density, anticipating the
Bayesian framework, where the parameter is also a realization of a random
variable,6 although, for the time being, we take the frequentists’ perspective
that there is an underlying “true” value of the parameter to be estimated. In
frequentist statistics one estimates parameters, while in the Bayesian statistics
one infers on their distributions based on one’s own beliefs and on available
information.
We assume that the observed values xj of X are independent realizations.
This means that the outcome of each single xj is not aﬀected by the knowledge
of the other outcomes. Since it may be confusing to talk about independent
realizations of the same random variable X, we specify that what we really
mean is that we think that there are N independent identically distributed
random variables X1, X2, . . . , XN, and that xj is a realization of Xj. There-
fore, referring to a single variable X is just a convenient shorthand notation7.
The assumption of independence implies that
π(x1, x2, . . . , xN | θ) = π(x1 | θ)π(x2 | θ) · · · π(xN | θ),
or, brieﬂy,
π(S | θ) =
N

j=1
π(xj | θ),
where S is the sample (2.1) at our disposal. This can be seen as a measure,
in some sense, of the probability of the outcome S, given the value of the
parameter θ. The Maximum Likelihood (ML) estimator of θ is deﬁned in a
very intuitive way as the parameter value that maximizes the probability of
the outcomes xj, 1 ≤j ≤N:
θML = arg max
N

j=1
π(xj | θ),
6 The notation more in line with the frequentist tradition would be πθ(x).
7 One can think of X as a die that is cast repeatedly, and Xk as the process of
casting the die the kth time.

32
2 Basic problem of statistical inference
provided that such maximizer exists. In other words, without making any
assumptions about the parameter to estimate, we take the value that makes
the available output most likely8.
We now observe the Maximum Likelihood estimator at work in a couple
of applications. We begin with the case where we believe that the data are
realizations of independent identically distributed Gaussian random variables,
whose expectation and variance we want to estimate.
Example 2.3: Consider a real random variable X with a Gaussian distri-
bution
π(x | x0, σ2) =
1
√
2πσ2 exp
 1
2σ2 (x −x0)2

,
whose unknown expectation x0 and variance σ2 we want to estimate from
a collection {x1, . . . , xN} of realizations of X. We begin with collecting the
parameters x0 and σ2 that we want to estimate into the vector θ ∈R2,
θ =

x0
σ2

=

θ1
θ2

,
and we deﬁne the likelihood function to be
N

j=1
π(xj | θ) =

1
2πθ2
N/2
exp
⎛
⎝−1
2θ2
N

j=1
(xj −θ1)2
⎞
⎠
= exp
⎛
⎝−1
2θ2
N

j=1
(xj −θ1)2 −N
2 log

2πθ2

⎞
⎠
= exp (−L(S | θ)) .
The maximum likelihood estimate of θ is the maximizer of exp (−L(S | θ)),
or, equivalently, the minimizer of L(S | θ), the negative of the log-likelihood
function.
It follows from the diﬀerentiability of the map θ →L(S | θ) that the
minimizer must be a solution of the equation
∇θL(S | θ) = 0.
Since
8 Here, we are edging dangerous waters: it is not uncommon to say that the maxi-
mum likelihood estimator is the “most likely” value of the parameter in the light
of the data, which is, of course, nonsense unless the parameter is also a ran-
dom variable. This is the key demarcation line between the frequentist and the
Bayesian interpretation.

2.2 Maximum Likelihood, as frequentists like it
33
∇θL(S | θ) =
⎡
⎢⎢⎢⎣
∂L
∂θ1
∂L
∂θ2
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎢⎣
−1
θ2
N

j=1
xj + N
θ2
θ1
−1
2θ2
2
N

j=1
(xj −θ1)2 + N
2θ2
⎤
⎥⎥⎥⎥⎦
,
solving the ﬁrst equation for x0 we have

x0 = θML,1 = 1
N
N

j=1
xj,
which, substituted in the second equation, gives

σ2 = θML,2 = 1
N
N

j=1
(xj −θML,1)2.
Thus, the Maximum Likelihood estimate of θ1 is the arithmetic mean of
the sample, and the Maximum Likelihood estimate of the variance is the
average of the squares of the distances of the data from the estimated
expectation. Comparing these ﬁndings to the results of the previous sub-
section we notice that the Maximum Likelihood estimators in the Gaussian
case coincide with the experimental mean and variance formulas suggested
by the Law of Large Numbers.
Let us now consider an example dealing with a discrete probability density.
Example 2.4: Consider a random variable K which has a Poisson distri-
bution with parameter θ,
π(k) = θk
k! e−θ,
and a sample S = {k1, · · · , kN}, kn ∈N, of independent realizations. The
likelihood density is
π(S | θ) =
N

n=1
π(kn) = e−Nθ
N

n=1
θkn
kn! ,
and its negative logarithm is
L(S | θ) = −log π(S | θ) =
N

n=1

θ −kn log θ + log kn!

.
As in the previous example, we maximize the likelihood by minimizing L.
Setting the derivative of the likelihood with respect to θ to zero

34
2 Basic problem of statistical inference
∂
∂θL(S | θ) =
N

n=1

1 −kn
θ

= 0,
(2.9)
yields
θML = 1
N
N

n=1
kn.
Since θ is the mean of the Poisson random variable, the Maximum Like-
lihood estimator is, once again, what the Law of Large Numbers would,
asymptotically, propose. A legitimate question which might now arise is
whether the Law of Large Numbers is really all we need for estimating
mean and variance. A simple reasoning shows that this is not the case.
In fact, for the Poisson distribution, the Law of Large Numbers would
suggest the variance approximation
var(K) ≈1
N
N

n=1
⎛
⎝kn −1
N
N

j=1
kj
⎞
⎠
2
,
which is diﬀerent from the Maximum Likelihood estimate of the variance
θML derived above.
It is instructive to see what the Maximum Likelihood estimator would give
if, instead of the Poisson distribution, we use its Gaussian approximation
with mean and variance equal to θ, an approximation usually advocated
for large values of θ. Writing
N

n=1
π(kn | θ) ≈
 1
2πθ
N/2
exp
"
−1
2θ
N

n=1
(kn −θ)2
#
=
 1
2π
N/2
exp
"
−1
2
$
1
θ
N

n=1
(kn −θ)2 + N log θ
%#
,
it follows that we can approximate θML by minimizing
L(S | θ) = 1
θ
N

n=1
(kn −θ)2 + N log θ.
To solve this minimization problem we set
∂
∂θL(S | θ) = −1
θ2
N

n=1
(kn −θ)2 −2
θ
N

n=1
(kn −θ) + N
θ = 0,
or, equivalently,
−
N

n=1
(kn −θ)2 −2
N

n=1
θ(kn −θ) + Nθ = Nθ2 + Nθ −
N

n=1
k2
n = 0,

2.2 Maximum Likelihood, as frequentists like it
35
from which it follows that
θML ≈
"
1
4 + 1
N
N

n=1
k2
n
#1/2
−1
2.
(2.10)
Thus the approximate Maximum Likelihood estimate obtained from the
Gaussian approximation diﬀers from what the exact density gave.
The following example is a ﬁrst step towards the interpretation of inverse
problems in terms of statistical inference and highlights certain important
characteristic features of inverse problems and Maximum Likelihood estima-
tors.
Example 2.5: Let X be a multivariate Gaussian random variable
X ∼N(x0, Γ),
(2.11)
where x0 ∈Rn is unknown and Γ ∈Rn×n is a known symmetric positive
deﬁnite matrix. Furthermore, assume that x0 depends on hidden parame-
ters z ∈Rk through a linear equation,
x0 = Az,
A ∈Rn×k,
z ∈Rk,
(2.12)
and that we want to estimate z from independent realizations of X.
The model (2.12) can be interpreted as the observation model for z in the
ideal, noiseless case. Since the world is less than ideal, in general we observe
noisy copies of x0. To rephrase the problem, we write an observation model
X = Az + E,
E ∼N(0, Γ),
(2.13)
from which it follows that
E

X

= Az + E

E

= Az = x0,
and
cov(X) = E

(X −Az)(X −Az)T
= E

EET
= Γ,
showing that the model (2.13) is in agreement with the assumption (2.11).
The probability density of X, given z, is
π(x | z) =
1
(2π)n/2det(Γ)1/2 exp

−1
2(x −Az)TΓ −1(x −Az)

.
Given N independent observations of X,

x1, . . . , xN

,
xj ∈Rn,
the likelihood function is

36
2 Basic problem of statistical inference
N

j=1
π(xj | z) ∝exp
⎛
⎝−1
2
N

j=1
(xj −Az)TΓ −1(xj −Az)
⎞
⎠,
where we introduce the notation ”∝” to mean “equal up to a multiplicative
constant of no interest”. The likelihood function can be maximized by
minimizing the negative of the log-likelihood function,
L(S | z) = 1
2
N

j=1
(xj −Az)TΓ −1(xj −Az)
= N
2 zT
ATΓ −1A

z −zT

ATΓ −1
N

j=1
xj

+ 1
2
N

j=1
xT
j Γ −1xj.
By computing the gradient of L(S | z) with respect to z,
∇zL(S | z) = N

ATΓ −1A

z −ATΓ −1
N

j=1
xj,
(2.14)
and setting it equal to zero, we ﬁnd that the Maximum Likelihood estima-
tor zML is the solution of the linear system

ATΓ −1A

z = ATΓ −1x,
where
x = 1
N
N

j=1
xj,
provided that the solution exists. The existence of the solution of the linear
system (2.14) depends on the properties of the matrix A ∈Rn×k.
In the particular case of a single observation, S =

x

, the search for the
Maximum Likelihood estimator reduces to the minimization of
L(x | z) = (x −Az)TΓ −1(x −Az).
By using the eigenvalue decomposition of the covariance matrix,
Γ = UDU T,
or, equivalently,
Γ −1 = W TW,
W = D−1/2U T,
we can express the likelihood in the form
L(x | z) = ∥W(Az −x)∥2.

2.2 Maximum Likelihood, as frequentists like it
37
Hence, the search for the Maximum Likelihood estimator is reduced to
the solution of a weighted least squares problem9 with weight matrix W ∈
Rn×n.
The last example shows that the maximum likelihood problem can be
formulated naturally as a basic problem of linear algebra, namely as the search
for the solution, in some sense, of a linear system of the type Az = x. This
linear system does not necessarily have a solution, and if it does, the solution
needs not be unique, or may be extremely sensitive to errors in the data.
In the context of inverse problems, linear systems typically exhibit several of
these problems, and the frequentist statistical framework, in general, provides
no clue of how to overcome them. In this case, numerical analysis works for
statistics, instead of statistics working for numerical analysis.
In the forthcoming chapters, we will investigate ﬁrst what numerical lin-
ear algebra can do to overcome the challenges that arise. Later, we add the
subjective aspect and see what statistical inference can indeed do for linear
algebra.
Exercises
1. A classical example in dynamical systems and chaos theory is the iteration
of the mapping
f : [0, 1] →[0, 1],
x →4x(1 −x).
Although fully deterministic, this function can be used to generate a sam-
ple that is chaotic; that is, which looks like a random sample, in the
following way. Starting from a value x1 ∈[0, 1], x1 ̸= 0, 1/2, 1, generate
the sample S =

x1, x2, . . . , xN

recursively by letting
xj+1 = f(xj),
then calculate its mean and variance, and, using the Matlab function hist,
investigate the behavior of the distribution as the sample size N increases.
2. Consider the Rayleigh distribution,
π(x) = x
σ2 exp

−x2
2σ2

,
x ≥0,
discussed in Exercise 2 of Chapter 1. Given σ2, generate a sample S =

x1, x2, . . . , xN

from the Rayleigh distribution. If the sample had come
9 One should probably use the term generalized weighted least squares problem,
since usually, in the literature, the term weighted least squares problem refers to a
problem where the matrix W is diagonal, i.e., each individual equation is weighted
separately.

38
2 Basic problem of statistical inference
from a log-normal distribution, then the sample S′ =

w1, w2, . . . , wN

,
wj = log xj, would be normally distributed. Write a Gaussian parametric
distribution and estimate its mean and variance using the sample S. Inves-
tigate the validity of the log-normality assumption following the procedure
of Example 2.1, that is, deﬁne the credibility intervals around the mean
and count the number of sample points within the credibility intervals
with diﬀerent credibilities.
3. This exercise is meant to explore in more depth the meaning of the Maxi-
mum Likelihood estimator. Consider the Rayleigh distribution for which,
unlike in the case of the normal distribution, the maximizer of π(x) and
the center of mass do not coincide. Calculate the maximizer and the center
of mass, and check your result graphically.
Assuming that you have only one sample point, x1, ﬁnd the Maximum
Likelihood estimator for σ2. Does x1 coincide with the maximizer or the
mean?

3
The praise of ignorance: randomness as lack of
information
“So you don’t have a unique answer to your questions?”
“Adson, if I had, I would teach theology in Paris.”
“Do they always have a right answer in Paris?”
“Never”, said William, ”but there they are quite conﬁdent of their
errors.”
(Umberto Eco: The Name of the Rose)
In the previous chapter the problem of statistical inference was consid-
ered from the frequentist’s point of view: the data consist of a sample from a
parametric probability density and the underlying parameters are determin-
istic quantities that we seek to estimate based on the data. In this section,
we adopt the Bayesian point of view: randomness simply means lack of in-
formation. Therefore any quantity that is not known exactly is regarded as
a random variable. The subjective part of this approach is clear: even if we
believed that an underlying parameter corresponds to an existing physical
quantity that could, in principle, be determined and therefore is conceptually
a deterministic quantity, the lack of the subject’s information about it justiﬁes
modeling it as a random variable. This is the general guiding principle that
we will follow, applying it with various degrees of rigor1.
When applying statistical techniques to inverse problems, the notion of
parameter needs to be extended and elaborated. In classical statistics, param-
eters are often regarded as tools, like, for example, the mean or the variance,
which identify a probability density. It is not uncommon that even in that
context, parameters may have a physical2 interpretation, yet they are treated
as abstract parameters. In inverse problems, parameters are more often by
1 After all, as tempting as it may sound, we don’t want to end up teaching theology
in Paris.
2 The word “physical” in this context may be misleading in the sense that it makes
us think of physics as a discipline. The use of the word here is more general, almost
a synonym of “material” or “of observable nature”, as opposed to something that
is purely abstract.

40
3 The praise of ignorance: randomness as lack of information
deﬁnition physical quantities, but they appear as statistical model parameters
deﬁning probability densities. Disquisitions about such diﬀerences in inter-
pretation may seem unimportant, but these very issues often complicate the
dialogue between statisticians and “inversionists”.
Example 3.1: Consider the general inverse problem of estimating a quan-
tity x ∈Rn that cannot be observed directly, but for which indirect obser-
vations of a related quantity y ∈Rm are available. We may, for example,
want to know the concentrations of certain chemical species (variable x)
in a gas sample, but for some reason, we cannot measure them directly;
instead, we observe spectral absorption lines of light that passes through
the specimen (variable y). A mathematical model describing light absorp-
tion by a mixture of diﬀerent chemical compounds ties these quantities
together. The fact that the variables that we are interested in are concen-
tration values already carries a priori the information that they cannot
take on negative values. In addition, knowing where the sample is taken
from, regardless of the subsequent measurement, we may have already a
relatively good idea of what to expect to be found in the gas sample.
In fact, the whole process of measuring may be performed to conﬁrm a
hypothesis about the concentrations.
In order to set up the statistical framework we need to express the distri-
bution of y in terms of the parameter x. This is done by constructing the
likelihood model. The design of the prior model takes care of incorporating
any available prior information.
As the preliminary example above suggests, the statistical model for in-
verse problems comprises two separate parts:
•
The construction of the likelihood model;
•
The construction of the prior model,
both of which make extensive use of conditioning and marginalization. When
several random variables enter the construction of a model, by means of con-
ditioning we can take into consideration one unknown at the time pretending
that the others are given. This allows us to construct complicated models step
by step. For example, if we want the joint density of x and y, we can write
the density of y for ﬁxed x and then deal with the density of x alone,
π(x, y) = π(y | x)π(x).
If, on the other hand, some of the variables appearing in the model are of no
interest, we can eliminate them from the density by marginalizing them, that
is by integrating them out. For example, if we have the joint density π(x, y, v)
but we are not interested in v, we can marginalize it as follows:
π(x, y) =

π(x, y, v)dv.

3.1 Construction of Likelihood
41
The parameter v of no interest is often referred to as noise or as a nuisance
parameter.
As statistics and probability are “common sense reduced to calculations”,
there is no universal prescription for the design of priors or likelihoods, al-
though some recipes seems to be used more often than others. These will be
discussed next.
3.1 Construction of Likelihood
In the previous section, the parametric likelihood density was viewed as a
probability density from which, presumably, the observed data was gener-
ated. In the Bayesian context, the meaning of the likelihood is the same,
with the only diﬀerence that parameters are seen as realizations of random
variables. When discussing inverse problems, the unknown parameters always
include the variables that we are primarily interested in. Hence, we can think
of the likelihood as of the answers to the following question: If we knew the
unknown x and all other model parameters deﬁning the data, how would the
measurements be distributed?
Since the construction of the likelihood starts from the assumption that, if
x were known, the measurement y would be a random variable, it is important
to understand the source of its randomness. Randomness being synonymous
of lack of information, it suﬃces to analyze what makes the data deviate
from the predictions of our observation model. The most common sources of
deviations are
1. measurement noise in the data;
2. incompleteness of the observation model.
The probability density of the noise can, in turn, depend on unknown pa-
rameters, as will be demonstrated later in the examples. The second source of
randomness is more complex, as it includes errors due to discretization, model
reduction and more generally, all the shortcomings of a computational model,
that is the discrepancy between the model and “reality” – in the heuristic
sense of the word3.
Example 3.2: In inverse problems, it is very common to use additive mod-
els to account for measurement noise, as we did in the previous chapter.
Assume that x ∈Rn is the unknown of primary interest, that the ob-
servable quantity y ∈Rm is ideally related to x through a functional
dependence,
y = f(x),
f : Rn →Rm,
(3.1)
3 Writing a model for the discrepancy between the model and reality implicitly,
and arrogantly, assumes that we know the reality and we are able to tell how the
model fails to describe it. Therefore, the word “reality” is used in quotes, and
should be understood as the “most comprehensive description available”.

42
3 The praise of ignorance: randomness as lack of information
and that we are very certain of the validity of the model. The available
measurement, however, is corrupted by noise, which we attribute to exter-
nal sources or to instabilities in the measuring device, hence not dependent
on x. Therefore we write the additive noise model,
Y = f(X) + E,
where E : Ω →Rm is the random variable modeling the noise. Observe
that since X and Y are unknown, they are interpreted as random vari-
ables – hence upper case letters –, leading to a stochastic extension of the
deterministic model (3.1).
Let us denote the distribution of the error by
E ∼πnoise(e).
Since we assume that the noise does not depend on X, ﬁxing X = x does
not change the probability distribution of E. More precisely
π(e | x) = π(e) = πnoise(e).
If, on the other hand, X is ﬁxed, the only randomness in Y is due to E.
Therefore
π(y | x) = πnoise(y −f(x)),
(3.2)
that is, the randomness of the noise is translated by f(x), as illustrated in
Figure 3.1.
In this example we assume that the distribution of the noise is known.
Although this is a common assumption, in practice the distribution of the
noise is seldom known. More typically, the noise distribution itself depends
on unknown parameters θ, thus
πnoise(e) = πnoise(e | θ),
hence equation (3.2) becomes
π(y | x, θ) = πnoise(y −f(x) | θ).
To illustrate this, assume that the noise E is zero mean Gaussian with
unknown variance σ2,
E ∼N(0, σ2I),
where I ∈Rm×m is the identity matrix. The corresponding likelihood
model is then
π(y | x, σ2) =
1
(2π)m/2σm exp

−1
2σ2 ∥y −f(x)∥2

,
with θ = σ2. If the noise variance is assumed known, we usually do not
write the dependency explicitly, instead using the notation

3.1 Construction of Likelihood
43
f(x)
Fig. 3.1. Additive noise: the noise around the origin is shifted to a neighborhood
of f(x) without otherwise changing the distribution.
π(y | x) ∝exp

−1
2σ2 ∥y −f(x)∥2

,
hence ignoring the normalizing constant.
In the previous example we started from an ideal deterministic model
and added independent noise. This is not necessarily always the case, as the
following example shows: the forward model may be intrinsically probabilistic.
Example 3.3: Assume that our measuring device consists of a collector
lens and a counter of photons emitted from N sources, with average photon
emission per observation time equal to xj, 1 ≤j ≤N, and that we want
to estimate the total emission from each source over a ﬁxed time interval.
We take into account the geometry of the lens by assuming that the total
photon count is the weighted sum of the individual contributions. When the
device is above the jth source, it collects the photons from its immediate
neighborhood. If the weights are denoted by ak, with the index k measuring
the oﬀset to the left of the current position, the expected count is
yj = E

Yj

=
L

k=−L
akxj−k,
where the weights aj are determined by the geometry of the lens and the
index L is related to the width of the lens, as can be seen in Figure 3.2.
Here it is understood that xj = 0 if j < 1 or j > N.
Considering the ensemble of all source points at once, we can write
y = E

Y

= Ax,

44
3 The praise of ignorance: randomness as lack of information
xj
xj−k
yj
a0
ak
yj+n
xj+n
Fig. 3.2. The expected contribution from the diﬀerent sources.
where A ∈RN×N is the Toeplitz matrix
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a0 a−1 · · · a−L
a1 a0
...
...
...
a−L
aL
...
...
...
a0 a−1
aL · · ·
a1 a0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The parameter L deﬁnes the bandwidth of the matrix.
If the sources are weak, the observation model just described is a photon
counting process. We may think that each Yj is a Poisson process with
mean yj,
Yj ∼Poisson

(Ax)j

,
that is,
π(yj | x) =
(Ax)yj
j
yj!
exp

−(Ax)j

.
Observe that, in general, there is no guarantee that the expectations (Ax)j
are integers. If we assume that consecutive measurements are independent,
the random variable Y ∈RN has probability density
π(y | x) =
N

j=1
π(yj | x) =
N

j=1
(Ax)yj
j
yj!
exp

−(Ax)j

.
We express this relation simply by writing

3.1 Construction of Likelihood
45
Y ∼Poisson(Ax).
A model sometimes used in the literature assumes that the photon count
is relatively large. In that case, using the Gaussian approximation of the
Poisson density discussed in Chapter 1, the likelihood model becomes
π(y | x) ≈
N

j=1

1
2π(Ax)j
1/2
exp

−
1
2(Ax)j

yj −(Ax)j
2

(3.3)
=

1
(2π)Ldet(Γ(x))
1/2
exp

−1
2(y −Ax)TΓ(x)−1(y −Ax)

,
where
Γ(x) = diag

Ax

.
We could try to ﬁnd an approximation of the Maximum Likelihood esti-
mate of x by maximizing (3.3). However this maximization is nontrivial,
since the matrix Γ appearing in the exponent and in the determinant is
itself a function of x. In addition, a typical problem arising with convolu-
tion kernels is that the sensitivity of the problem to perturbations in the
data is so high that even if we were able to compute the maximizer, it may
be meaningless. Attempts to solve the problem in a stable way by means
of numerical analysis have led to classical regularization techniques. The
statistical approach advocated in this book makes an assessment of what
we believe of a reasonable solution and incorporates this belief in the form
of probability densities.
It is instructive to see what the Poisson noise looks like, and to see how
Poisson noise diﬀers from Gaussian noise with constant variance. In Figure 3.3,
we have plotted a piecewise linear average signal x and calculated a realization
of a Poisson process with x as its mean. It is evident that the higher the mean,
the higher the variance, in agreement with the fact that the mean and the
variance are equal. By visual inspection, Poisson noise could be confused with
multiplicative noise, which will be discussed in the next example.
Before the example, let us point out a fact that is useful when composing
probability densities. Assume that we have two random variables X and Y in
Rn that are related via a formula
Y = f(X),
where f is a diﬀerentiable function, and that the probability distribution of
Y is known. We write
π(y) = p(y),
to emphasize that the density of Y is given as a particular function p. It
would be tempting to deduce that the density of X, denoted by π(x), is
obtained by substituting y = f(x) in the function p. This, in general, is

46
3 The praise of ignorance: randomness as lack of information
0
0.2
0.4
0.6
0.8
1
0
10
20
30
40
50
60
70
0
0.2
0.4
0.6
0.8
1
−15
−10
−5
0
5
10
15
20
Fig. 3.3.
Left panel: the average piecewise linear signal and a realization of a
Poisson process, assuming that the values at each discretization point are mutually
independent. Right panel: the diﬀerence between the noisy signal and the average.
not true, since probability densities represent measures rather than functions.
The proper way to proceed is therefore to take the Jacobian of the coordinate
transformation into consideration, by writing
π(y)dy = p(y)dy = p(f(x))|det(Df(x))|dx,
where Df ∈Rn×n is the diﬀerential of f. Now we may identify the density of
X as being
π(x) = p(f(x))|det(Df(x))|.
(3.4)
In the following example, we make use of this formula.
Example 3.4: Consider a noisy ampliﬁer that takes in a signal f(t) and
sends it out ampliﬁed by a constant factor α > 1. The ideal model for the
output signal is
g(t) = αf(t),
0 ≤t ≤T.
In practice, however, it may happen that the ampliﬁcation factor is not
constant but ﬂuctuates slightly around a mean value α0. We write a dis-
crete likelihood model for the output by ﬁrst discretizing the signal. Let
xj = f(tj),
yj = g(tj),
0 = t1 < t2 < · · · < tn = T.
Assuming that the ampliﬁcation at t = tj is aj, we have a discrete model
yj = ajxj,
1 ≤j ≤n,
and, replacing the unknown quantities by random variables, we obtain the
stochastic extension
Yj = AjXj,
1 ≤j ≤n,
which we write in vector notation as
Y = A.X,
(3.5)

3.1 Construction of Likelihood
47
with the dot denoting componentwise multiplication of the vectors A, X ∈
Rn. Assume that A as a random variable is independent of X, as is the
case, for instance, if the random ﬂuctuations in the ampliﬁcation are due
to thermal phenomena. If A has the probability density
A ∼πnoise(a),
to ﬁnd the probability density of Y , conditioned on X = x, we ﬁx X and
write
Aj = Yj
xj
,
1 ≤j ≤n.
Applying formula (3.4), we obtain
π(y | x) =
1
x1x2 · · · xn
πnoise
&y.
x
'
,
(3.6)
where the dot denotes that the division of the two vectors is component-
wise.
Let us consider a special example where we assume that all the variables
are positive, and A is log-normally distributed, i.e., the logarithm of A is
normally distributed. For simplicity, we assume that the components of A
are mutually independent, identically distributed, hence
Wi = log Ai ∼N(w0, σ2),
w0 = log α0.
To ﬁnd an explicit formula for the density of A, we note that if w = log a,
where the logarithm is applied componentwise, we have
dw =
1
a1a2 · · · an
da,
thus the probability density of A is
πnoise(a) ∝
1
a1a2 · · · an
exp

−1
2σ2 ∥log a −w0∥2

=
1
a1a2 · · · an
exp

−1
2σ2 ∥log
 a.
α0

∥2

.
By substituting this formula in (3.6), we ﬁnd that
π(y | x) ∝
1
y1y2 · · · yn
exp

−1
2σ2 ∥log

y.
(α0.x)

∥2

.
Before moving onto the design of priors, let’s look at an example with two
diﬀerent sources of noise.

48
3 The praise of ignorance: randomness as lack of information
Example 3.5: In this example, we assume that the photon counting con-
volution device of Example 3.3 adds a noise component to the collected
data. More precisely, we have an observation model of the form
Y = Z + E,
where Z ∼Poisson(Ax) and E ∼N(0, σ2I).
To write the likelihood model, we begin with assuming that X = x and
Zj = zj are known. Then
π(yj | zj, x) ∝exp

−1
2σ2 (yj −zj)2

.
Observe that x does not appear explicitly here, but is, in fact, a hidden
parameter that aﬀects the distribution of Zj. From the formula for the
conditional probability density it follows that
π(yj, zj | x) = π(yj | zj, x)π(zj | x),
π(zj | x) =
(Ax)zj
j
zj!
exp (−(Ax)j) .
Since the value of zj is not of interest here, we can marginalize it out. In
view of the fact that zj takes on only integer values greater than or equal
to zero, we write:
π(yj | x) =
∞

zj=0
π(yj, zj | x)
∝
∞

zj=0
π(zj | x)exp

−1
2σ2 (y −zj)2

,
which gives us the likelihood as a function of the variable of interest x. It
is possible to replace the summation by using the Gaussian approximation
for the Poisson distribution, but since the calculations are quite tedious
and the ﬁnal form is not so informative, we do not pursue that here.
3.2 Enter, Subject: Construction of Priors
The prior density expresses what we know4, or more generally, believe about
the unknown variable of interest prior to taking the measurements into
account. The choice of a prior
accounts for the subjective portion of the
4 The use of the word knowing could be replaced by being certain of, conforming
with the notion of Wittgenstein on certainty: being certain of something does not
imply that things are necessarily that way, which is the tragedy of science and
the salvation of the art.

3.2 Enter, Subject: Construction of Priors
49
procedure. In fact, what we believe a priori about the parameters biases the
search so as to favor solutions which adhere to our expectation. How signiﬁ-
cantly the prior is guiding our estimation of the unknowns depends in part on
the information contents of the measurements: in the absence of good mea-
sured data, the prior provides a signiﬁcant part of the missing information
and therefore has a great inﬂuence on the estimate. If, on the other hand, the
measured data is highly informative, it will play a big role in the estimation
of the unknowns, leaving only a lesser role for the prior, unless we want to go
deliberately against the observations. We actually use priors more extensively
than we are generally ready to admit, and in variety of situations: often, when
we claim not to know what to expect, our prior is so strong to rule out right
away some of the possible outcomes as meaningless, and, in fact, the concept
of meaningless only exists as a counterpart to meaningful, and its meaning is
usually provided by a prior.
Example 3.6: Here we give a trivial example of hidden priors: assume that
you want somebody to pick “a number, any number”. If that somebody is
a normal5 human being, the answer may be 3, 7, 13 – very popular choices
in Judeo-Christian tradition – or 42 – a favorite number in the science
ﬁction subculture6. You may think that you did not have any prior on
what to expect, until you ask a space alien in disguise who, taking your
question very seriously, starts to recite 40269167330954837..., continuing
this litany of digits for over a week. Although eventually the stream of
digits will end, as the number is ﬁnite after all, you start to wonder if a
lifetime will be enough to ﬁnish a round of the game. Quite soon, probably
much before the end of the week, you realize that this is not at all what you
had in mind with ”picking a number”: the response is deﬁnitely against
your prior. And in fact, you discover that not only your prior was there,
but it was quite strong too: there are inﬁnitely more numbers that would
take more than a week – or a lifetime, for that matter – to say, than those
you expected as an answer7!
Another, more pertinent example comes from the ﬁeld of inverse problems.
5 Preferably not from a math department, where people enjoy to come up with the
most intricate answers for the simplest of questions, a tendency which becomes
exacerbated when the answer involves numbers.
6 In the subculture of mathematicians, even more exotic answers are likely, such
as the Hardy–Ramanujan number, known also as the taxicab number, 1729. The
point here is to notice that to obtain numbers no more complex than this, it takes
some of the best number-minded brains. The human bias towards small numbers
is enormous.
7 Jose Luis Borges has an elegant short story, The book of sand, about a book with
an endless number of pages. Opening the book and reading the page number
would be tantamount to choosing “any” number. Borges, possibly aware of the
problem, leaves the page numbering a mystery.

50
3 The praise of ignorance: randomness as lack of information
Example 3.7: Your orthopedic surgeon has asked you to take along the
MRI slides of your knee in the next visit, but, on your way out of the house,
you accidentally took the envelope with the MRI slides of the brain. As
soon as the slides are taken out of the envelope, the doctor knows that you
have grabbed the wrong ones, in spite of having just told you not to have
any idea what to expect to see in the slides.
The prior beliefs that we often have are qualitative, and it may be quite
a challenge to translate them into quantitative terms. How do we express in
a prior that we expect to see an MRI of the neck, or how do we describe
quantitatively what radiologists mean by “habitus of a malignant tumor”?
Clearly, we are now walking along the divide between art and science.
The obvious way to assign a prior is by stating that the unknown param-
eter of interest follows a certain distribution. Since the prior expresses our
subjective belief about the unknown, our belief is a suﬃcient justiﬁcation for
its being. Note that the reluctance to choosing any speciﬁc prior, usually for
fear of its subjective nature, de facto leads often to a claim that all values
are equally likely, which, apart of its mathematical shortcomings, leaves it
entirely up to the likelihood to determine the unknown. The results may be
catastrophic.
The priors that we apply in day to day life are often inspired by our
previous experience in similar situations. To see how we can proceed in the
construction of a prior, we consider some examples.
Example 3.7: Assume that we want to determine the level x of hemoglobin
in blood by near-infrared (NIR) measurements at a patient’s ﬁnger. If we
have a collection of hemoglobin values measured directly from the patient’s
blood,
S =

x1, . . . , xN

,
we can think of them as realizations of a random variable X with an
unknown distribution. As explained in Chapter 2, there are two possible
ways for extracting information about the underlying distribution from S:
•
The non-parametric approach looks at a histogram based on S and
tries to infer what the underlying distribution is.
•
The parametric approach proposes a parametric model, then computes
the Maximum Likelihood estimate of the model parameters from the
sample S.
Let us assume, for example, that a Gaussian model is proposed,
X ∼N(x0, σ2).
We know from Chapter 2 that the Maximum Likelihood estimates for x0
and σ2 are,
x0,ML = 1
N
N

j=1
xj,

3.2 Enter, Subject: Construction of Priors
51
and
σ2
ML = 1
N
N

j=1
(xj −x0,ML)2,
respectively. We then assume that any future value x of the random vari-
able is a realization from this Gaussian distribution. Thus, we postulate
that:
•
The unknown X is a random variable, whose probability distribution,
called the prior distribution, is denoted by πprior(x),
•
Guided by our prior experience, and assuming that a Gaussian prior is
justiﬁable, we use the parametric model
πprior(x) =
1
√
2πσ2 exp

−1
2σ2 (x −x0)2

,
with x0 and σ2 determined experimentally from S by the formulas
above.
Methods in which prior parameters are estimated empirically, either from
previous observations or simultaneously with the unknown from the current
data, are called empirical Bayes methods.
The prior may be partly based on a physical, chemical or biological model,
as in the following example.
Example 3.8: Consider a petri dish with a culture of bacteria whose
density we want to estimate. For simplicity, let’s assume that we have a
rectangular array of squares, where each square contains a certain num-
ber of bacteria, as illustrated in Figure 3.4, and that we are interested in
estimating the density of the bacteria from some indirect measurements8.
We begin with setting up a model based on our belief about bacterial
growth. For example, we may assume that the number of bacteria in a
square is approximately the average of bacteria in the neighboring squares,
xj ≈1
4(xleft,j + xright,j + xup,j + xdown,j),
(3.7)
see Figure 3.4 for an explanation. Since the squares at the boundary of the
petri dish have no neighbors in some directions, we need to modify (3.7)
to account for their special status. The way in which we will handle this
turns out to be related to the choice of boundary conditions for partial
diﬀerential equations. For example, we can assume that xj = 0 in pixels
outside the square.
Let N be the number of pixels and A ∈RN×N be the matrix with jth row
8 Aside from the fact that direct count of bacteria would be impossible, it would
amount to moments of unmatched enjoyment.

52
3 The praise of ignorance: randomness as lack of information
xj
xdown
xup
xleft
xright
Fig. 3.4.
Square array of bacteria. On the right we zoom in on a neighborhood
system.
(up)
(down)
(left)
(right)
A(j, : ) =

0 · · · 1/4 · · · 1/4 · · · 1/4 · · ·
1/4 · · · 0

,
with the understanding that for boundary and corner pixels, some of the
columns may be missing, corresponding to the assumption that no contri-
bution from the outside of the array is coming.
If we were absolutely certain of the validity of the model (3.7), we would
replace the approximate sign with strict equality. In other words we would
assume that
x = Ax.
(3.8)
which clearly does not work! In fact, after rewriting (3.8) as
(I −A)x = 0,
it immediately follows that x = 0, since I −A is an invertible matrix.
Therefore, to relax the model (3.8) by admitting some uncertainty, we
deﬁne X to be a random variable and write a stochastic model
X = AX + R,
(3.9)
where R expresses the uncertainty of the averaging model. The lack of
information about the values of R is encoded in its probability density,
R ∼πmod.error(r),
and, by writing R = X −AX, we deﬁne that the prior density for X is
πprior(x) ∝πmod.error(x −Ax).
Equation (3.9) deﬁnes an autoregressive Markov model, and in this context
R is referred to as an innovation process. In particular, if R is Gaussian
with mutually independent, identically distributed components,

3.2 Enter, Subject: Construction of Priors
53
R ∼N(0, σ2I),
the prior for X is of the form
πprior(x | σ2) =

1
2πσ2
N/2
exp

−1
2σ2 ∥x −Ax∥2

=

1
2πσ2
N/2
exp

−1
2σ2 ∥Lx∥2

,
where
L = I −A.
We remark that if we have no natural way of ﬁxing the value of σ2, es-
timating it on the basis of observations is part of the larger estimation
problem. One possible line of thought for choosing the variance σ2 could
be as follows: suppose that you do expect to ﬁnd about M bacteria in the
whole dish. That means that, in the average, one square contains about
m = M/N bacteria. The ﬂuctuation around the postulated average value
in a single square is probably not much more than m, so a reasonable
choice could be σ ≈m.
We conclude this example by showing a Matlab code to construct the
matrix A. Since this matrix is sparse, it is advisable to construct it as
a sparse matrix. Sparse matrices are deﬁned in Matlab by three vectors
of equal length. The ﬁrst one contains the indices of the rows with non-
vanishing entries, the second one the column indices and the third one the
actual non-vanishing values. These vectors are called rows, cols and vals
in the code below.
n = 50;
% Number of pixels per directions
% Creating an index matrix to enumerate the pixels
I = reshape([1:n^2],n,n);
% Right neighbors
Icurr = I(:,1:n-1);
Ineigh = I(:,2:n);
rows = Icurr(:);
cols = Ineigh(:);
vals = ones(n*(n-1),1);
% Left neighbors
Icurr = I(:,2:n);
Ineigh = I(:,1:n-1);

54
3 The praise of ignorance: randomness as lack of information
rows = [rows;Icurr(:)];
cols = [cols;Ineigh(:)];
vals = [vals;ones(n*(n-1),1)];
% Upper neighbors
Icurr = I(2:n-1,:);
Ineigh = I(1:n-1,:);
rows = [rows;Icurr(:)];
cols = [cols;Ineigh(:)];
vals = [vals;ones(n*(n-1),1)];
% Lower neighbors
Icurr = I(1:n-1,:);
Ineigh = I(2:n,:);
rows = [rows;Icurr(:)];
cols = [cols;Ineigh(:)];
vals = [vals;ones(n*(n-1),1)];
A = 1/4*sparse(rows,cols,vals);
L = speye(n^2) - A;
Observe that L is, in fact, a second order ﬁnite diﬀerence matrix built
from the mask
⎡
⎣
−1/4
−1/4
1
−1/4
−1/4
⎤
⎦,
hence L is, up to a scaling constant, a discrete approximation of the Laplacian
−Δ = −∂2
∂x2
1
−∂2
∂x2
2
in the unit square. Indeed, if
xj = f(pj),
where pj is a point in the jth pixel, the ﬁnite diﬀerence approximation of the
Laplacian of f can be written in the form
−Δf(pj) ≈4
h2

Lx

j,
where h is the length of the discretization step. At the boundaries, we assume
that f extends smoothly by zero outside the square.
The prior model derived in the last example corresponds to what is often
referred to as second order smoothness prior. The reason for the name is that

3.3 Posterior Densities as Solutions of Statistical Inverse Problems
55
this prior assigns a higher probability to vectors x ∈RN corresponding to
discrete approximations of functions with a small second derivative, since the
exponential is larger when the negative exponent is smaller, that is, when
Lx has a small norm. This is the case, in general, for vectors x which are
discretizations of smooth functions. We remark that a Gaussian smoothness
prior does not exclude the occurrence of large jumps between adjacent pixels,
but it gives them an extremely low probability.
Before leaving, temporarily, the theme of prior distributions, we want to re-
port on a typical discussion which could occur after a presentation of Bayesian
solutions of inverse problems on how to estimate a gray scale image from a
blurred and noisy copy. In the image processing literature, this is referred to as
a denoising and deblurring problem. The image can be represented as a pixel
matrix, each entry assigning a gray scale value to the corresponding pixel. As-
sume that, after stacking the pixel values in one long vector, a Gaussian prior
has been constructed for the image vectors. A typical question is: “Can you
really assume that the prior density is Gaussian9?” While at ﬁrst sight such
a question may seem reasonable, once properly analyzed, it becomes rather
obscure. In fact, the question is based on the Platonic view that there is a
true prior. But, true prior of what, we may ask. All possible images, maybe?
Most certainly not, and in fact all possible images is a useless category, as is
the category of all possible realizations of all random process, or all possible
worlds. Maybe the set should be restricted to all possible images that can be
realized in the particular application that we have in mind? But such cate-
gory is also useless, unless we specify its genesis by describing, for instance,
the probability distribution of the images. And at this point we face the classic
problem of which came ﬁrst, the hen or the egg. A more reasonable question
would therefore be, what the given prior implies. As we shall see, a possible
way of exploring the implications of a prior distribution is to use sampling
techniques.
3.3 Posterior Densities as Solutions of Statistical Inverse
Problems
Within the previous sections of this chapter we introduced the main actors in
our Bayesian play, the prior and the likelihood. From now on, the marginal
density of the unknown of primary interest will be identiﬁed as prior density,
and denoted by πprior. Notice that both the likelihood and the prior may
contain parameters whose values we are not very conﬁdent about; the natural
Bayesian solution is to regard them as random variables, too.
If we let X denote the random variable to be estimated and Y the random
variable that we observe, from the Bayes formula we have that
9 Note that when saying that the prior is Gaussian, we do not intend a prior with
independent equally distributed components, so the obvious arguments referring
to non-negativity of the image do not necessarily apply here.

56
3 The praise of ignorance: randomness as lack of information
π(x | y) = πprior(x)π(y | x)
π(y)
,
y = yobserved.
(3.10)
The conditional density π(x | y), called the posterior density, expresses the
probability of the unknown given that the observed parameters take on the
values given as the data of the problem and our prior belief. In a Bayesian
statistical framework, the posterior density is the solution of the inverse prob-
lem.
Two questions are now pertinent. The ﬁrst one is how to ﬁnd the posterior
density and the second how to extract information from it in a form suitable for
our application. The answer to the ﬁrst question has already been given: from
the prior and the likelihood, the posterior can be assembled via the Bayes
formula. The answer to the second question will be given in the remaining
chapters. As a prelude to the ensuing discussion, let us consider the following
example.
Example 3.9: Consider a linear system of equations with noisy data,
y = Ax + e,
x ∈Rn, y, e ∈Rm, A ∈Rm×n,
and let
Y = AX + E
be its stochastic extension, where X, Y and E are random variables. A
very common assumption is that X and E are independent and Gaussian,
X ∼N(0, γ2Γ),
E ∼N(0, σ2I),
where we have assumed that both random variables X and E have zero
mean. If this is not the case, the means can be subtracted from the ran-
dom variable, and we arrive at the current case. The covariance of the
noise indicates that each component of Y is contaminated by independent
identically distributed random noise. We assume that the matrix Γ in the
prior is known. The role of the scaling factor γ will be discussed later. The
prior density is therefore of the form
πprior(x) ∝exp

−1
2γ2 xTΓ −1x

,
The likelihood density, assuming that the noise level σ2 is known, is
π(y | x) ∝exp

−1
2σ2 ∥y −Ax∥2

.
It follows from the Bayes formula that the posterior density is
π(x | y) ∝πprior(x)π(y | x)
∝exp

−1
2γ2 xTΓ −1x −
1
2σ2 ∥y −Ax∥2

= exp (−V (x | y)) ,

3.3 Posterior Densities as Solutions of Statistical Inverse Problems
57
where
V (x | y) = 1
γ2 xTΓ −1x +
1
2σ2 ∥y −Ax∥2.
Since the matrix Γ is symmetric positive deﬁnite, so is its inverse, thus
admitting a symmetric, e.g., Cholesky, factorization:
Γ −1 = RTR.
With this notation,
xTΓ −1x = xTRTRx = ∥Rx∥2,
and we deﬁne
T(x) = 2σ2V (x | y) = ∥y −Ax∥2 + δ2∥Rx∥2,
δ = σ
γ .
(3.11)
This functional T, sometimes referred to as the Tikhonov functional, plays
a central role in the classical regularization theory.
The analogue of the Maximum Likelihood estimator in the Bayesian setting
maximizes the posterior probability of the unknowns and is referred to as
the Maximum A Posteriori (MAP) estimator:
xMAP = arg max π(x | y).
Note that
xMAP = arg min V (x | y),
V (x | y) = −log π(x | y).
In this particular case we have that
xMAP = arg min

∥y −Ax∥2 + δ2∥Rx∥2
.
(3.12)
When the posterior density is Gaussian, the Maximum A Posteriori esti-
mate coincides with the Conditional Mean (CM), or Posterior Mean esti-
mate,
xCM =

xπ(x | y)dy.
It is immediate to check that if we have R = I and let γ increase with-
out bounds, the parameter δ goes to zero and the Maximum A Posteriori
estimator reduces formally to the Maximum Likelihood estimator, namely
the estimation of x is entirely based on the likelihood density.
In our discussion of the Maximum Likelihood estimation in Chapter 2,
Example 2.4, we saw that its calculation can be reduced to solving a system
of linear equations
Ax = y,
(3.13)

58
3 The praise of ignorance: randomness as lack of information
possibly in the least squares sense. It was also pointed out that even when
the matrix A is square, if it is numerically singular the calculation of the
Maximum Likelihood estimate becomes problematic. Classical regularization
techniques are based on the idea that an ill-posed problem is replaced by
a nearby problem that is well-posed. Tikhonov regularization, for example,
replaces the linear system (4.1) with the minimization problem
min

∥y −Ax∥2 + δ2∥Rx∥2
,
(3.14)
where the ﬁrst term controls the ﬁdelity of the solution to the data and the
second one acts as a penalty. The matrix R is typically selected so that large
∥Rx∥corresponds to an undesirable feature of the solution. The role of the
penalty is to keep this feature from growing unnecessarily. A central problem
in Tikhonov regularization is how to choose the value of the parameter δ
judiciously. A Tikhonov regularized solution is the result of a compromise
between ﬁtting the data and eliminating unwanted features. As we have seen,
this ﬁts well into the Bayesian framework, since the penalty can be given a
statistical interpretation via the prior density.
A question which comes up naturally at this point is if and how Tikhonov
regularization, or MAP estimation, can avoid the numerical problems that
make Maximum Likelihood estimation generally infeasible for noisy data. The
answer can be found by writing the functional to be minimized in (3.14) in
the form
∥y −Ax∥2 + δ2∥Rx∥2 =
((((

y
0

−

A
δR

x
((((
2
,
which reveals that the Maximum A Posteriori estimate is the least squares
solution of the linear system

A
δR

x =

y
0

.
While the original problem (3.13) may be ill-posed, the augmentation of the
matrix A by δR considerably reduces the ill-posedness of the problem. The
quality of the solution depends on the properties of the matrix R.
Exercises
1. A patient with a lump in the breast undergoes a mammography. The
radiologist who examines her mammogram believes that the lump is ma-
lignant, that is, the result of the mammogram is positive for malignancy.
The radiologist’s record of true and false positives is shown in the table
below:
Malignant Benign
Positive mammogram 0.8
0.1
Negative mammogram 0.2
0.9

3.3 Posterior Densities as Solutions of Statistical Inverse Problems
59
The patient, with the help of the internet, ﬁnds out that in the whole
population of females, the probability of having a malignant breast tumor
at her age is 0.5 percent. Without thinking much further, she takes this
number as her prior belief of having cancer.
(a) What is the probability of the mammogram result being positive for
malignancy?
(b) What is the conditional probability of her having a malignant tumor,
considering the fact that the mammogram’s result was positive?
(c) What problems are there with her selection of the prior, others than
the possible unreliability of internet?
2. A person claims to be able to guess a number from 1 to 10 which you are
asked to think of. He has a record of having succeeded 8 times out of 10.
You question the claim and in your mind assign a certain probability x of
him having indeed such gift, but decide to give the poor devil a chance and
let him try to guess the number between 1 and 10 that you are thinking.
He guesses correctly, but you are still not convinced. In other words, even
after his successful demonstration, you still think that he is a swindler and
that the probability of such an extraordinary gift is less than 0.5. How low
must your prior belief x have been for this to happen?
3. Rederive the result of Example 3.4 in a slightly diﬀerent way: By taking
the logarithm of both sides of equation (3.5), we obtain
log Y = log X + log A = log X + W,
where W is normally distributed. Write the likelihood density for log Y
conditioned on X = x and derive the likelihood for Y .


4
Basic problem in numerical linear algebra
We have seen in the previous chapter that the computation of both the Max-
imum Likelihood and the Maximum A Posteriori estimators often requires
solving a linear system of equations. In this chapter we review methods for
the eﬃcient solution of linear systems of equations in the classical or in the
least squares sense, and we give some insight into the complications that
might arise. In this chapter we review methods of traditional linear algebra
for solving ill-conditioned problems, the purpose being to prepare the way for
statistical methods to solve problems in numerical linear algebra. The Trojan
horse that makes this possible is the Bayesian framework, prior and likelihood
as its head and tail, bringing into the numerical algorithms information that
would not be otherwise usable.
4.1 What is a solution?
The main problem of numerical linear algebra that is of concern to us is how
to compute the solution of a linear system
Ax = b
A ∈Rm×n,
x ∈Rn,
b ∈Rm.
(4.1)
Since the linear system (4.1) is not necessarily square, and, even when square,
not necessarily solvable, the word “solution” may have diﬀerent meanings
depending on the values of m, n and on the properties of A and b. In particular,
1. If n > m, the problem is underdetermined, i.e., there are more unknowns
than linear equations to determine them;
2. If n = m, the problem is formally determined, i.e., the number of un-
knowns is equal to the number of equations;
3. If n < m, the problem is overdetermined, i.e., the number of equations to
satisfy exceeds that of the unknowns.
We now discuss the usual meaning of the word “solution” in each of these
three cases, a graphical illustration of which can be seen in Figure 4.1.

62
4 Basic problem in numerical linear algebra
Fig. 4.1.
Meaning of a solution. Left: minimum norm solution of an underdeter-
mined system (n = 2, m = 1). Middle: exact solution of a non-singular system
(n = m = 2). Right: least squares solution of an overdetermined system (n = 2,
m = 3).
If m < n, the number of unknowns exceeds the number of equations. Since,
in general, we can expect that at most as many unknowns as linear equations
can be determined, at least m −n unknowns cannot be determined without
additional information about the solution. One way to solve this underdeter-
mined linear system is to ﬁrst ﬁx the values of m −n of the unknowns, then
to determine the remaining ones as functions of the assigned values. Another
common procedure for the solution of underdetermined linear systems is to
look for a minimum norm solution, intended as a solution vector of minimal
Euclidean norm,
xMN = arg min

∥x∥| Ax = b

.
If m = n, we have as many linear equations as unknowns, but this only
guarantees that a solution exists and is unique if the matrix A is nonsingu-
lar. If A is not invertible and the system is consistent, some of the equations
are redundant, in the sense that they are linear combinations of the remain-
ing ones, or the system may be inconsistent. In the ﬁrst case the system is
eﬀectively underdetermined.
If m > n, the number of linear equations exceeds the number of unknowns
and, in general, we cannot expect to ﬁnd a vector x such that Ax = b. In
general, we seek a vector x such that
xLS = arg min ∥b −Ax∥2,
(4.2)
where ∥· ∥is the Euclidean vector norm. The minimizer, if it exists, is called
the least squares solution of (4.1) because it minimizes the sum of the squares
of the components of the residual error
r = b −Ax.
Algorithms for solving overdetermined systems are discussed below.
The choice of the best algorithm for solving (4.1) typically depends on
several factors, with the size of the problem playing an important role. In

4.2 Direct linear system solvers
63
general, we distinguish algorithms for solving linear systems of equations into
two broader classes: direct methods and iterative methods.
4.2 Direct linear system solvers
Direct methods solve (4.1) by ﬁrst factorizing the matrix A and then solv-
ing simpler linear systems. Since typically the factorization of the matrix A
requires
•
The matrix A to be explicitly available;
•
Suﬃcient memory allocation for the matrix A and its factors;
•
A number of ﬂoating point operations proportional to the cube of the
dimension of the problem;
they are most suitable for linear systems of small or medium dimensions.
Furthermore, direct methods are of the “all or nothing” kind, in the sense
that if the process is not completed, no partial information about the solution
is available. Although we shall not present here a systematic treatment of
direct methods, which can be found in any linear algebra textbook and is not
a central topic for these notes, the main algorithmic points are summarized
below in the form of examples.
Example 4.1: If (4.1) is a square system, i.e., m = n, a standard pro-
cedure is to perform a decomposition of the matrix A into a product of
triangular matrices and to solve the resulting subproblems by forward and
back substitution. If A is nonsingular, the algorithm referred to as Gaus-
sian elimination with partial pivoting produces a factorization of the form
PA = LU
where L and U are the lower triangular and the upper triangular matrices,
L =
⎡
⎢⎢⎢⎣
1
∗1
...
...
∗· · · ∗1
⎤
⎥⎥⎥⎦, ,
U =
⎡
⎢⎢⎢⎣
∗∗· · · ∗
∗
∗
... ...
∗
⎤
⎥⎥⎥⎦,
where the asterisks are placeholders for possibly non-zero elements and P
is a permutation matrix. Having computed the LU-decomposition of A, we
then solve the two triangular systems
Ly = Pb,
Ux = y.
If the matrix A is symmetric positive deﬁnite, the linear system (4.1) can
be solved by ﬁrst computing the Cholesky factorization of A:
A = RTR

64
4 Basic problem in numerical linear algebra
where R is upper triangular, then solving
RTy = b,
Rx = y.
Since the Cholesky factorization is only deﬁned for symmetric positive def-
inite matrices, this solution method, which requires only half of the storage
and of the computations for the matrix factorization, can only be applied
to a restricted class of linear systems. When attempting to compute the
Cholesky factorization of a symmetric matrix that is not positive deﬁnite,
the algorithm breaks down while trying to compute the square root of a
negative number. It is important to remember that matrices which are
mathematically symmetric positive deﬁnite may fail to be numerically so.
This is true in particular when the matrix is of large dimensions and it
arises from the discretization of an ill-posed problem. The errors accumu-
lated during the process of computing the Cholesky factorization may make
the matrix numerically singular or indeﬁnite, thus causing the algorithm
to break down prior to completion.
Let’s now discuss how to solve overdetermined linear systems of equations.
Example 4.2: Consider an overdetermined linear system
Ax = b,
A =
⎡
⎢⎢⎢⎢⎢⎣
∗· · · ∗
∗· · · ∗
...
...
∗· · · ∗
∗· · · ∗
⎤
⎥⎥⎥⎥⎥⎦
∈Rm×n,
m > n.
If the columns of A are linearly independent, the minimizer of (4.2) exists
and is unique. Multiplying both sides of the above equation by AT from
the left, we obtain the normal equations,
ATAx = ATb.
(4.3)
This is a square nonsingular linear system, whose coeﬃcient matrix ATA ∈
Rn×n is symmetric and positive deﬁnite. In fact,
xTATAx = (Ax)TAx = ∥Ax∥2 ≥0,
and this quadratic form is zero only if Ax = 0. But since Ax is a linear
combination of the columns of A, which by assumption are linearly inde-
pendent, Ax = 0 only if x = 0. The normal equation can be solved by
using the Cholesky factorization.
If the columns of A are linearly independent, the solution of the normal
equation (4.3) is the least squares solution,
xLS =

ATA
−1ATb = A†b.

4.2 Direct linear system solvers
65
The matrix A† ∈Rn×m is called the pseudoinverse of A. It is easy to
check that if A is square and invertible, the pseudoinverse coincides with
the usual inverse.
Forming and solving the normal equations is not necessarily the best way
to compute the solution of a least squares problem, in particular when the
coeﬃcient matrix is ill-conditioned and the right-hand side is contaminated
by errors. These issues will be discussed later in this chapter. For now,
we will just remark that the compression of the information contained
in the matrix A occurring when forming ATA is likely to introduce into
the system additional errors which can compromise the accuracy of the
solution.
In the case of ill-conditioned least squares problems it might be preferable
to use the QR factorization of the matrix A,
A = QR,
where Q ∈Rm×m is an orthogonal matrix, that is, Q−1 = QT, and R is
upper triangular,
R =
⎡
⎢⎢⎢⎢⎢⎣
∗· · · ∗
... ...
∗
⎤
⎥⎥⎥⎥⎥⎦
=

R1
0

∈Rm×n,
with R1 ∈Rn×n. Multiplying both sides of the linear system from the left
by the transpose of Q and replacing A by its QR factorization, we arrive
at the linear triangular system
Rx = QTb,
which can be partitioned as

R1x
0

=

QTb

1

QTb

2

.
Clearly, the last m −n equations are either satisﬁed or not, depending on
the right hand side b; the variable x ∈Rn plays no role in that. The ﬁrst
n equations can be solved by backsubstitution, if the diagonal elements of
R1 are non-zero.
The pseudoinverse is useful also when analyzing underdetermined linear
systems and, as a special case, formally determined linear systems with non-
invertible square matrix, as we will see in the following example.
Example 4.3: A useful tool for analyzing linear systems is the singular
value decomposition (SVD) of the matrix A,

66
4 Basic problem in numerical linear algebra
A = UDV T,
A ∈Rm×n.
Here U ∈Rm×m and V ∈Rn×n are orthogonal matrices, that is, U −1 =
U T, V −1 = V T and D ∈Rm×n is a diagonal matrix of the form
D =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
d1
d2
...
dn
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
, if m > n,
or
D =
⎡
⎢⎢⎢⎣
d1
d2
...
dm
⎤
⎥⎥⎥⎦, if m < n,
or, brieﬂy,
D = diag

d1, d2, . . . , dmin(m,n)

,
dj ≥0.
We may assume that the singular values d1, d2, . . . are in decreasing order.
Using the singular value decomposition it is possible to transform any
linear system into a diagonal one by two changes of variables,
Dx′ = b′,
x′ = V Tx,
b′ = U Tb.
(4.4)
In the overdetermined case, the diagonal linear system is of the form
b′
j =
 djx′
j, 1 ≤j ≤n,
0,
n < j ≤m.
Whether the m −n equations are satisﬁed or not depends entirely of the
right hand side b. Whether we can satisfy the ﬁrst n equations, on the
other hand, depends on the singular values dj. Assume that the ﬁrst p
singular values are positive, and the rest of them vanish, i.e.,
d1 ≥d2 ≥· · · ≥dp > dp+1 = dp+2 = · · · dn = 0.
Clearly the only possible choice for the ﬁrst p components of the solution
vector x is
x′
j = b′
j
dj
,
1 ≤j ≤p,
while the remaining components of x′ can be chosen arbitrarily. In the ab-
sence of additional information about the desired solution, it is customary,
adhering to the minimum norm solution paradigm, to set them to zero.
This is mathematically equivalent to deﬁning the solution of the linear
system to be

4.3 Iterative linear system solvers
67
x = A†b,
where
A† = V D†U T,
D† = diag
 1
d1
, . . . , 1
dp
, 0, . . . , 0

∈Rm×n.
(4.5)
Here we use the pseudoinverse notation because when the columns of A are
linearly independent, (4.5) is equivalent to the deﬁnition of pseudoinverse.
The deﬁnition (4.5), however, is more general: in the case where m < n, it
gives the minimum norm solution.
Although from the theoretical point of view the singular value decompo-
sition is a very versatile tool to analyze linear systems, it can only be used
for problems of small dimensions, because of its elevated computational cost.
The singular value decomposition will be revisited when the ill–conditioning
of linear systems is discussed.
4.3 Iterative linear system solvers
Many of the applications that we have in mind give rise to linear systems
of large dimensions, whose solution requires the use of iterative methods.
Iterative linear system solvers become of interest when
•
the dimensions of the linear system are very large;
•
the matrix A is not explicitly given;
•
we only want an approximate solution of the linear system.
The second case is typical in applications where the action of a linear
mapping is easy to calculate, i.e, for any given vector x, the output of x →Ax
is easily available. Examples of such matrix-free applications can be found in
signal and image processing. The following example illustrates the use of Fast
Fourier Transform (FFT) for computing convolution integrals without forming
convolution matrices.
Example 4.4: Consider a signal, f(t), 0 ≤t ≤T, that has been sampled
at discrete time instances,
xj = f(tj),
tj = j T
n ,
0 ≤j ≤n −1.
Its convolution with a kernel h is deﬁned as an integral
g(t) =
 T
0
h(t −s)f(s)ds,
0 ≤t < T,
which can be discretized and evaluated as a matrix product similarly as in
Example 3.3. An alternative way is to use Fourier transformation. Working

68
4 Basic problem in numerical linear algebra
with Fourier series of periodic functions, let us extend the input signal to a
T-periodic function, fT , and modify the convolution operator accordingly,
g(t) =
 ∞
−∞
h(t −s)fT (s)ds
=
 ∞
−∞
h(s)fT (t −s)ds,
0 ≤t < T,
so that the output g is also T-periodic. The kth Fourier coeﬃcient of the
output signal,
g(k) = 1
T
 T
0
exp

−i2π
T tk

g(t)dt
=
 ∞
−∞
h(s)
$
1
T
 T
0
exp

−i2π
T (t −s)k

fT (t −s)dt
%
exp

−i2π
T sk

ds
= f(k)
 ∞
−∞
h(s)exp

−i2π
T sk

ds
= h(k) f(k),
is then the product of the kth Fourier coeﬃcient of f and the Fourier
transform of h at k,
h(k) =
 ∞
−∞
h(s)exp

−i2π
T sk

ds.
The convolved output signal is then expressed as a Fourier series,
g(t) =
∞

k=−∞
g(k)exp

i2π
T tk

=
∞

k=−∞
h(k) f(k)exp

i2π
T tk

.
Therefore, the convolution can be computed by componentwise multipli-
cation in frequency space.
The implementation of the Fourier transform and of the inverse Fourier
transform can be done using the Discrete Fourier Transform (DFT) and
its fast and memory eﬃcient implementation, the Fast Fourier Transform.
As an example, we apply the FFT-based convolution to convolving a box-
car function with a Gaussian convolution kernel h,
h(t) =
1
	
2πγ2 exp

−1
2γ2 t2

,
whose Fourier transform can be calculated analytically,

4.3 Iterative linear system solvers
69
h(k) =
1
	
2πγ2
 ∞
−∞
exp

−1
2γ2 t2

exp

−i2π
T tk

dt
= exp
"
−1
2
2πγ
T k
2#
.
We leave as an exercise (see the end of this chapter) to verify that the
convolution can be implemented in Matlab as follows:
% Input signal
n = 64;
T = 3;
t = linspace(0,T,n)’;
x = (t>1).*(t<2);
% Fourier transform of the Gaussian kernel
xi = (2*pi*gamma)/T*[[0:n/2],[-n/2+1:-1]]’;
h = exp(-0.5*xi.^2);
% Convolution h*x
hx = real(ifft(h.*fft(x)));
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
Input
Output
Conv. kernel
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
Input
Output
Conv. kernel
Fig. 4.2. Convolution with a Gaussian kernel computed with the FFT algorithm.
The input signal in the example shown in the right panel is not vanishing at the
right edge, hence its periodic extension causes an artefact at the left edge.
A couple of comments concerning the code above are in order. First, note
that the components of the vector xi are ordered according to how the
Fourier coeﬃcients are returned by the Matlab function FFT; see the ex-
ercise at the end of this chapter. Second, notice that when calculating
the ﬁnal result, the Matlab function real is used to eliminate imaginary
components of the output that are due to numerical round-oﬀ.

70
4 Basic problem in numerical linear algebra
A second comment concerns the periodic extension of the input signal. In
Figure 4.2, we have plotted the output signals corresponding to an input
that vanishes near the end points of the interval and to one that does not
vanish. In the latter example, the non-locality of the convolution operator
causes an artifact at the left end of the output signal. To avoid this artifact,
the input signal must be ﬁrst padded with zero outside the interval [0, T],
and the FFT convolution must be then applied to the extended signal.
This procedure prevents the convolution kernel from reading a signal that
comes from the preceding or following period of fT . Further properties of
the FFT, concerning, for example, the ordering of the components, can be
found in the exercises at the end of this chapter.
The philosophy behind solving a linear system via an iterative method is
quite diﬀerent from the one behind the use of direct solvers. Instead of the
all-or-nothing approach of direct methods, iterative methods, starting from
an initial guess x0, compute a sequence
x1, x2, . . . , xk . . .
of improved approximate solutions. The matrix A does not need to be explic-
itly formed, but we must be able to compute its product and possibly the
product of its transpose with an arbitrary vector. This ﬂexibility with respect
to the matrix A makes iterative linear solvers the methods of choice when the
matrix A is either not explicitly known, or its storage would require a lot of
memory, but its action on a vector can be computed easily and eﬀectively,
like in the FFT example above.
In general, we stop iterating when either a maximum number of iterations
has been reached, or some convergence criterion has been satisﬁed. Usually
iterative linear solvers stop when the norm of the residual error
rk = b −Axk
has been suﬃciently reduced.
The kth Krylov subspace associated with the matrix A and the vector b is
deﬁned to be
Kk(A, b) = span{b, Ab, . . . , Ak−1b}.
Iterative methods which seek the kth approximate solution in a Krylov sub-
space of order k are called Krylov subspace iterative methods.
One of the ﬁrst Krylov subspace methods to be introduced, in 1952, was
the Conjugate Gradient (CG) method. This iterative method in its original
formulation can be applied only to solve linear systems with a symmetric
positive deﬁnite matrix, although some variants for singular matrices have
also been proposed. The popularity of the CG method is due to the fact that
it requires only one matrix-vector product per iteration, and the memory
allocation is essentially independent of the number of iterations. The kth
iterate computed by the CG method minimizes the A norm of the error,

4.3 Iterative linear system solvers
71
xk = arg
min
x∈Kk(A,b) ∥x −x∗∥2
A,
where x∗stands for the presumably existing but, evidently, unknown exact
solution of the system, and the A–norm is deﬁned by
∥z∥2
A = zTAz.
Fortunately, even though we do not know x∗and hence cannot evaluate the
functional to be minimized, an algorithm for ﬁnding the minimizer xk is avail-
able. A detailed derivation of the algorithm is not presented here, but instead
we give an outline of the idea behind it.
At each step, the CG algorithm searches for the scalar α which minimizes
the function
α →∥xk−1 + αpk−1 −x∗∥2
A,
where xk−1 is the approximate solution from the previous iteration and pk−1
is a search direction. The minimizer turns out to be
αk =
∥rk−1∥2
pT
k−1Apk−1
,
and the new approximate solution is obtained by updating the previous one,
xk = xk−1 + αk−1pk−1.
The selection of the search directions is crucial. At the ﬁrst iteration, we search
along
p0 = r0 = b −Ax0,
the residual associated with the initial guess. At each subsequent iteration,
the new search direction is A–conjugate to the previous ones, that is, pk must
satisfy
pT
k Apj = 0,
0 ≤j ≤k −1.
At ﬁrst sight, this requirement looks quite complicated and its numerical
implementation time-consuming. It turns out, however, that the new search
direction can always be chosen in the subspace spanned by the previous di-
rection and the most recent residual vector, that is,
pk = rk + βkpk−1,
rk = b −Axk,
where the parameter βk is chosen so that the A–conjugacy is satisﬁed. After
some algebraic manipulation it turns out that
βk =
∥rk∥2
∥rk−1∥2 ,
We are ready to outline how to organize the computations for the Conju-
gate Gradient method. For simplicity, the initial guess is set to zero here.

72
4 Basic problem in numerical linear algebra
The CG Algorithm: Given the right hand side b, initialize:
x0 = 0;
r0 = b −Ax0;
p0 = r0;
Iterative algorithm:
for k = 1, 2, . . . until the stopping criterion is satisﬁed
α =
∥rk−1∥2
(pT
k−1Apk−1);
xk = xk−1 + αpk−1;
rk = rk−1 −αApk−1;
β =
∥rk∥2
∥rk−1∥2 ;
pk = rk + βpk−1;
end
If the matrix A is not symmetric positive deﬁnite the Conjugate Gradi-
ent method will break down. An algorithm which can be applied to the more
general case where the matrix A is not necessarily even square is the Con-
jugate Gradient method for Least Squares (CGLS). This iterative method is
mathematically equivalent to applying the CG method to normal equations
ATAx = ATb,
without ever forming the matrix ATA. The CGLS method is computationally
more expensive than the CG method, requiring two matrix-vector products
per iteration, one with A, one with AT, but its memory allocation is essentially
independent of the number of iterations. It can be shown that the kth CGLS
iterate solves the minimization problem
xk = arg
min
x∈Kk(ATA,ATb) ∥b −Ax∥2,
or, equivalently, is characterized by
Φ(xk) =
min
x∈Kk(ATA,ATb) Φ(x),
where
Φ(x) = 1
2xTATAx −xTATb.
The minimization strategy is very similar to that of the Conjugate Gradient
method. The search for the minimizer is done by performing sequential linear
searches along the ATA–conjugate directions,
p0, p1, . . . , pk−1,

4.3 Iterative linear system solvers
73
where ATA–conjugate means that the vectors pj satisfy the condition
pT
j ATApk = 0,
j ̸= k.
The iterate xk is determined from xk−1 and pk−1 by the formula
xk = xk−1 + αk−1pk−1,
where the coeﬃcient αk−1 ∈R solves the minimization problem
αk−1 = arg min
α∈R Φ(xk−1 + αpk−1).
Introducing the residual error of the normal equations for the CGLS
method associated with xk,
rk = ATb −ATAxk,
the passage from the current search direction to the next is given by
pk = rk + βkpk−1,
where we choose the coeﬃcient βk so that pk is ATA-conjugate to the previous
search directions,
pT
k ATApj = 0,
1 ≤j ≤k −1.
It can be shown that
βk =
∥rk∥2
∥rk−1∥2 .
The quantity
dk = b −Axk
is called the discrepancy associated with xk. The discrepancy and the residual
of the normal equations are related to each other via the equation
rk = ATdk.
Note that the norms of the discrepancies form a non-increasing sequence,
while the norms of the solutions form a non-increasing one,
∥dk+1∥≤∥dk∥,
∥xk+1∥≥∥xk∥.
We are now ready to describe how the calculations for the CGLS method
should be organized. Here, again, the initial guess is set to zero.
The CGLS Algorithm: Given the right hand side b, initialize:
x0 = 0;
d0 = b −Ax0;
r0 = ATd0;
p0 = r0;

74
4 Basic problem in numerical linear algebra
y0 = Ap0;
Iteration:
for k = 1, 2, . . . until the stopping criterion is satisﬁed
α = ∥rk−1∥2
∥yk−1∥2
xk = xk−1 + αpk−1;
dk = dk−1 −αyk−1;
rk = ATdk;
β =
∥rk∥2
∥rk−1∥2 ;
pk = rk + βpk−1;
yk = Apk;
end
A more recent addition to the family of Krylov subspace iterative meth-
ods, the Generalized Minimal RESidual (GMRES) method, can be applied
to any linear system provided that the coeﬃcient matrix A is invertible. The
computational cost per iteration of the GMRES method is low, only requiring
one matrix-vector product, but its memory allocation grows by one vector of
the size of x at each iteration.
The kth iterate of the GMRES method solves the minimization problem
xk = arg
min
x∈Kk(A,b) ∥b −Ax∥,
a property which gives the method its name. Since for the GMRES method
the residual vector is also the discrepancy, from this characterization of the
iterates of the GMRES it follows immediately that
∥dk∥≤∥dk−1∥≤. . . ≤∥d0∥= ∥b∥,
because the Krylov subspaces where the minimization problems are solved
as the iterations progress form a nested sequence. To understand how the
GMRES computations are organized, we begin with an algorithm for com-
puting an orthonormal basis of Kk(A, b) directly. This is accomplished by the
following algorithm, called the Arnoldi process.
Consider ﬁrst the Krylov subspace K1(A, b) =

b

. Clearly, an orthonor-
mal basis for this subspace consists of one single vector v1,
v1 =
1
∥b∥b,
b ̸= 0.
Observe that if b = 0, x = 0 is the solution and no iterations are needed.
Assume now that we have found an orthonormal basis for Kk(A, b),

4.3 Iterative linear system solvers
75

v1, v2, . . . , vk

.
To augment this set with one additional vector, orthogonal to the previous
ones and spanning the extra direction of Kk+1(A, b), introduce the vector
w = Avk,
and search for a new vector vk+1 of the form
vk+1 = w −
k

j=1
hjkvj.
We impose that the new vector is orthogonal to the previous ones, by requiring
that it satisﬁes the condition
0 = vT
j vk+1 = vT
j w −hjk,
1 ≤j ≤k,
which, in turn, deﬁnes the weights hjk. Finally we scale the vector vk+1 to
have norm one. Algorithmically, we write the Arnoldi process as follows:
Arnoldi Process:
v1 =
b
∥b∥;
for j = 1, . . . , k
w = Avj;
for i = 1, 2, . . . , j
hij = wTvi;
w = w −hijvj;
end
hj+1,j = ∥w∥;
vj+1 =
w
∥w∥;
end
Once the Arnoldi process has been completed, the vectors vj form an
orthonormal basis of the kth Krylov subspace, that is
vT
i vj =

1, i = j
0, i ̸= j
and
span{v1, v2, . . . , vk+1} = Kk+1(A, b).
We collect the vectors vi in the matrix
Vj = [v1 v2 . . . vj], j = k, k + 1,

76
4 Basic problem in numerical linear algebra
and the weights hij in the matrix H,
Hk =
⎡
⎢⎢⎢⎢⎢⎣
h11 h12
h1k
h21 h22
h2k
... ...
hkk
hk+1,k
⎤
⎥⎥⎥⎥⎥⎦
.
A matrix of this form is referred to as a Hessenberg matrix.
It follows from the Arnoldi process that these matrices satisfy the Arnoldi
relation,
AVk = Vk+1Hk.
Once an orthonormal basis for Kk(A, b) is available, we can proceed to
solve the minimization problem. Using the Arnoldi relation and the fact that
the vectors vj are orthonormal, we deduce that
min
x∈Kk(A,b) ∥Ax −b∥= min
y∈Rk ∥AVky −b∥
= min
y∈Rk ∥Vk+1(Hky −∥b∥e1)∥
= min
y∈Rk ∥Hky −∥b∥e1∥.
If yk is the least squares solution of the system
Hky = ∥b∥e1,
which, in turn, can be written in terms of the pseudoinverse (Hk)† of Hk,
yk = ∥b∥(Hk)†e1,
then the solution xk to the original minimization problem is
xk = Vkyk.
We now present an outline of how the computations for the GMRES method
should be organized. In this version, the Arnoldi process has been built into
the algorithm.
The GMRES Algorithm: Initialize:
x0 = 0;
r0 = b −Ax0;
H = [ ]; (empty matrix)
V = [ ];
v1 =
r0
∥r0∥;
j = 0;

4.4 Ill-conditioning and errors in the data
77
Iteration: for k = 1, 2, . . . until the stopping criterion is satisﬁed
j = j + 1;
Add one column to V :
V = [V, vj];
wj = Avj;
Add one row and one column to H:
for i = 1, . . . , j
hij = wT
j vi;
wj = wj −hijvi;
end
hj+1,j = ∥wj∥;
if hj+1,j = 0
No independent directions found; stop iteration
else
vj+1 =
wj
hj+1,1
;
end
y = ∥b∥H†e1;
xj = V y;
end
The three iterative methods described can be found as built-in function in
Matlab. Each one of their calling sequences allows us to either pass the matrix
A as an argument, or to specify a function which computes the product of
an arbitrary vector with the matrix A or with its transpose. The choice of
the maximum number of steps is an integral part of the algorithm, as is the
choice of the tolerance value which decides when the approximate solution
is suﬃciently accurate. If these ﬁelds are not speciﬁed, default values are
used. All these iterative methods allow the use of preconditioning, a practice
whose aim is usually to improve the convergence rate of the method. Since
preconditioners will be used to import the statistical perspective into the
calculations, we will discuss them at length in Chapter 6.
4.4 Ill-conditioning and errors in the data
The solution of linear systems of equations coming from real life applications,
with real data as a right-hand side vector, often aims at determining the causes
of observed eﬀects. Depending on the type of linear relation between the cause
x and the eﬀect b expressed by the linear system, and on the discretization
used to represent the problem in terms of matrices and vectors, the matrix A
in (4.1) may become moderately to severely ill-conditioned. Ill-conditioning is
best understood in terms of the singular value decomposition of A,
A = UDV T.

78
4 Basic problem in numerical linear algebra
Consider the least squares solution of the problem (4.1) deﬁned via the pseu-
doinverse,
xLS = 1
d1
V diag

1, d1
d2
, . . . , d1
dr
, 0, . . . , 0

U Tb.
If b is contaminated by noise so is U Tb, whose kth component is multiplied by
the ratio d1/dk. As this ratio increases with k, the solution of the linear system
may be so dominated by ampliﬁed error components to become meaningless.
We say that the matrix A is ill-conditioned if its condition number
cond(A) = d1
dr
is very large. What very large means in practice depends on the application.
In general, condition numbers of the order of 105 · · · 106 start becoming of
concern.
The eﬀect of having singular values of diﬀerent orders of magnitude is
illustrated by a simple example.
Example 4.5: In this elementary two-dimensional model, the matrix A is
deﬁned via its eigenvalue decomposition,
A =

v1 v2
  1
0.1
 vT
1
vT
2

,
where the eigenvectors v1 and v2 are
v1 =

cos θ
sin θ

,
v2 =

sin θ
−cos θ

,
θ = π
6 .
Observe that the condition number of A is only 10, so the example should
be understood merely as an illustration.
In Figure 4.3, we have plotted the eigenvectors of a matrix A, scaled by
the corresponding eigenvalues and applied at a point x∗. The point x∗is
then mapped to the point y∗= Ax∗, shown in the right panel of the ﬁgure.
We may think of y∗as the errorless data and x∗as the true solution of the
equation
Ax = y∗.
What happens to the solution if we perturb the errorless solution y∗by
a small amount? Consider two diﬀerent perturbed data points y1 and y2,
plotted in the right panel of Figure 4.3. The preimages xj that satisfy
Axj = yj,
j = 1, 2,
are shown in the left panel of Figure 4.3. More generally, we have also plot-
ted a shaded ellipse, which is the preimage of the disc on the left. Although
the distance of y1 and y2 from y∗is the same, the distance of the preimages
from x∗are very diﬀerent. In fact, in the eigendirection corresponding to
the small eigenvalue, the sensitivity to data errors is tenfold compared to
that in the direction corresponding to the larger eigenvalue.

4.4 Ill-conditioning and errors in the data
79
The solution of ill-conditioned linear systems by direct methods is usu-
ally not advisable, since the ”all-or-nothing” approach is not able to separate
the meaningful part of the computed solution from the ampliﬁed error compo-
nents. The design of algorithms for the solution of linear systems with singular
values spanning a broad range and clustering around the origin, often referred
to as linear discrete ill-posed problems, is an active area of research in inverse
problems.
x*
x1
x2
A
y*
y1
y2
A−1
Fig. 4.3. Ill-conditioning and inversion. The noisy data points y1 and y2 are equally
close to the noiseless vector y∗= Ax∗, but this is not the case for the distance of
their preimages x1 = A−1y1 and x2 = A−1y2 from x∗.
When using iterative methods for the solution of this class of linear sys-
tems, a semiconvergence behavior can often be observed. At ﬁrst the iterates
seems to converge to a meaningful solution, but as the iterations proceed,
they begin to diverge. This phenomenon is illustrated ﬁrst using the above
two-dimensional toy problem.
Example 4.6: Starting from the two-dimensional linear system of Exam-
ple 4.5, consider the additive noise model
B = Ax + E,
E ∼N(0, σ2I),
and generate a sample of data vectors, b1, b2, . . . , bn. Each linear system
Ax = bi with right hand side a vector in the sample is then solved with
the Conjugate Gradient method. As expected, only two iteration steps are
needed. In Figure 4.4, we show the approximate solutions obtained after
one (left panel) and two (right panel) iteration steps.
The results reveal some typical features of iterative solvers. In fact, the
correction step determined in the ﬁrst iteration aims at reducing the resid-
ual error in the singular direction associated with the larger singular value,

80
4 Basic problem in numerical linear algebra
Fig. 4.4. Approximate solutions with 100 right hand sides contaminated by random
error, after one iteration step (left) and two steps (right).
while the correction determined in the second iteration aims at reducing
the residual error in the singular direction associated with the smaller sin-
gular value. The higher sensitivity of the system to noise in the latter
direction is a reason why it is often said that the ﬁrst iteration is mostly
ﬁtting the informative part of the signal, the second the noise.
This last example, in spite of its simplicity, shows how the idea of capturing
the solution before the ampliﬁed noise takes over by truncating the iteration
prior to reaching convergence came about. By equipping an iterative method
with a stopping rule eﬀective at ﬁltering the ampliﬁed noise from the com-
puted solution, we can make the problem less sensitive to perturbations in the
data. The process of stabilizing the solution of a discrete ill-posed problem
is usually referred to as regularization. The process of regularizing by pre-
maturely stopping the iteration process is called regularization by truncated
iteration.
The point where the iterations start to ﬁt the noise, or when the conver-
gence switches to divergence, varies from one iterative method to the other,
but in general it seems to occur sooner for the GMRES method.
The design of a good stopping rule is an important and diﬃcult task. We
outline here how this problem can be approached in a fully deterministic way,
prior to revisiting the issue from a Bayesian perspective.
Assume for the moment that we are considering a linear system with an
invertible square matrix A, and that all our information about the noise in
the right hand side amounts to an estimate of its norm. Denoting by x∗the
exact solution of the system with noiseless data b∗, we may write
Ax = b∗+ ε = Ax∗+ ε = b,
where ε represents the inaccuracy of the data, and we have the approximate
information that

4.4 Ill-conditioning and errors in the data
81
∥ε∥≈η,
with η > 0 known. The fact that
∥A(x −x∗)∥= ∥ε∥≈η
seems to suggest that, within the limits of our information, any solution x
that satisﬁes the condition
∥Ax −b∥≤τη
(4.6)
should be a good approximate solution. Here the fudge factor τ > 1, whose
value is close to one, safeguards against underestimating the norm of the noise.
The idea of using (4.6) to decide how much a problem should be regularized
is called the Morozov discrepancy principle. We now present a few examples
illustrating the use of this principle in classical deterministic regularization.
Example 4.7: In this example we consider regularization by truncated
iteration with iterative solvers applied to a linear system
b = Ax + e,
which is the discretization of a deconvolution problem. The matrix A is
of Toeplitz form and it is the discretization of a Gaussian kernel. The
following Matlab code can be used to generate the matrix:
n = 64;
%
Number of discretization points
T = 3;
%
Length of the interval
t = linspace(0,T,n)’;
gamma = 0.2;
% Width parameter
hh = (T/n)*1/(sqrt(2*pi*gamma^2))*
exp(-1/(2*gamma^2)*t.^2);
A = toeplitz(hh,hh);
Since this matrix A is extremely ill-conditioned, with condition number
cond(A) ≈5.7 × 1018,
we expect direct solvers without regularization to be completely useless for
the solution of this problem. To verify this, we take as true input signal x∗
the boxcar function of Example 4.4, calculate the errorless data, b = Ax∗,
and solve the equation b = Ax by a direct solver. The result, shown in
Figure 4.5, is indeed meaningless even if the only source of error was just
numerical rounding of the data.
We investigate next how iterative solvers perform. In fact, conﬁdent in
their performance, we add Gaussian random error with standard devia-
tion σ = 10−4 to the errorless data. We start with the CG method, which
can be applied to this problem because we know that the matrix A is

82
4 Basic problem in numerical linear algebra
0
0.5
1
1.5
2
2.5
3
−30
−20
−10
0
10
20
30
40
Fig. 4.5. The solution of the deconvolution problem by direct linear system solver
using errorless data corresponding to the boxcar input.
mathematically positive deﬁnite. To illustrate the semiconvergence phe-
nomenon, in Figure 4.6 we plot the norm of the solution error and the
norm of the discrepancy against the iteration index
k →∥xk −x∗∥,
k →∥rk∥= ∥b −Axk∥,
k = 0, 1, 2, . . .
In real applications only the norm of the residual error can be calculated,
because the error of the approximation is not available unless we know x∗,
the true solution1. The sequence of approximate solutions computed by
the CG iterations are shown in Figure 4.6.
0
5
10
15
10
−1
10
0
10
1
10
2
0
5
10
15
10
−6
10
−4
10
−2
10
0
10
2
Fig. 4.6. The norm of the error (left) and of the discrepancy (right) of the Conjugate
Gradient iterates as functions of the iteration index. The dashed line indicates the
norm of the noise in the right hand side.
From the plot of the error and residual norms it is clear that the norm
of the residual error associated with the fourth iterate x4 is at the level
1 and, to state the obvious, if we knew x∗we would not need to estimate it!

4.4 Ill-conditioning and errors in the data
83
of the norm of the noise. If we apply the Morozov discrepancy principle,
we stop around the fourth or ﬁfth iteration. The semiconvergence is quite
noticeable in this example: at the beginning, the error decreases but after
the thirteenth iteration, the noise starts to take over and the error starts to
grow. To see how the iterations proceed, we plot the ﬁfteen ﬁrst iterations
in Figure 4.7.
0
5
10
15
20
0
0.5
1
1.5
2
2.5
3
−1
−0.5
0
0.5
1
1.5
2
t
Iteration
Fig. 4.7.
The ﬁrst ﬁfteen Conjugate Gradient iterates. The true input (boxcar
function) is plotted in the foreground. Notice how the last iterate oscillates due to
noise ampliﬁcation.
For comparison, we repeat the procedure using the GMRES iteration. The
results are displayed in Figures 4.8 and 4.9.
0
5
10
15
10
−1
10
0
10
1
10
2
0
5
10
15
10
−8
10
−6
10
−4
10
−2
10
0
10
2
Fig. 4.8. The error (left) and the discrepancy (right) norm of the GMRES iteration
as functions of the iteration index. The dashed line indicates the norm of the noise
in the right hand side.

84
4 Basic problem in numerical linear algebra
0
5
10
15
20
0
0.5
1
1.5
2
2.5
3
−0.5
0
0.5
1
1.5
t
Iteration
Fig. 4.9. The ﬁrst ﬁfteen GMRES iterates. The true input is plotted in the fore-
ground. The last iterate oscillates due to the noise ampliﬁcation.
Now we discuss the regularization properties of using a prior in the infer-
ence problem.
Example 4.8: We have seen that the Maximum Likelihood estimator with
a single noisy observation leads to the minimization problem
xLS = arg min ∥Ax −b∥,
while the Maximum A Posteriori estimator for this linear model with a
Gaussian prior, leads to
xMAP = arg min
((((

A
δR

x −

b
0
(((( ,
where R is proportional to the Cholesky factor of the inverse of the prior
covariance matrix, as explained at the end of Chapter 3. We also saw that
the MAP estimate is the minimizer of the Tikhonov functional.
To understand how the regularization properties are aﬀected by the in-
troduction of the prior, consider for simplicity the case where the prior
covariance is proportional to the unit matrix and m ≥n. That is, the
system is either formally determined or overdetermined.
The normal equations corresponding to the Maximum A Posteriori mini-
mization problem are
 A
δI
T  A
δI

x =
 A
δI
T  b
0

,
or, equivalently,

4.4 Ill-conditioning and errors in the data
85

ATA + δ2I

x = ATb.
To study the ill-conditioning of this system, we replace A with its singular
value decomposition
A = UDV T,
in the left hand side of the normal equations to obtain
ATA + δ2I = V DTDV T + δ2I
= V

DTD + δ2I

V T.
We remark that
DTD + δ2I = diag

d2
1 + δ2, . . . , d2
n + δ2
,
hence the prior adds a positive multiple of the identity to the matrix to
reduce the ill-conditioning of the problem. In fact, we can write the Max-
imum A Posteriori solution in terms of the singular values as
xMAP =
n

j=1
dj
d2
j + δ2 (U Tb)jvj,
where vj is the jth column of the matrix V .
A large portion of the classical regularization literature is concerned with
the selection of the regularization parameter δ in the Tikhonov functional.
One of the most common ways of determining it is to use the Morozov
discrepancy principle. Writing the solution of the normal equations as
xδ =

ATA + δ2I
−1ATb,
the discrepancy principle suggests that we should choose the largest δ > 0
for which
∥Axδ −b∥≤τη
holds. Of course, if the dimensions of the linear system are large, it may be
unfeasible to compute xδ repeatedly, and more eﬃcient ways of estimating
the value of the parameter have to be developed. We do not discuss those
issues here.
As a concrete application, consider the problem of numerical diﬀerentia-
tion. Let f : [0, 1] →R be a piecewise smooth function, f(0) = 0, and f ′(t)
be its derivative where deﬁned. Assume that we have noisy observations
of f at a few discrete point, that is
yj = f(tj) + ej,
tj = 1
nj,
1 ≤j ≤n,
where ej denotes the noise. We want to estimate the derivative f ′ at the
n discrete points tj. To set up our model, we write a piecewise constant
approximation of the derivative,

86
4 Basic problem in numerical linear algebra
0
0.2
0.4
0.6
0.8
1
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Noiseless
Noisy
Fig. 4.10.
The noiseless data (thin) and data corrupted by noise with standard
deviation σ = 0.02 (thick).
10
−6
10
−4
10
−2
10
0
10
−10
10
−5
10
0
10
5
δ1
δ2
δ3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
δ1 =0.026367
δ2 =0.054556
δ3 =0.11288
Fig. 4.11. The norm of the discrepancy with diﬀerent values of the regularization
parameter δ (left). The horizontal dashed line corresponds to the discrepancy level
τη, where τ = 1.5. The right panel displays reconstructions with three diﬀerent
values of δ, indicated in the left panel in diﬀerent gray levels. The true signal is a
boxcar function of height 3 on the interval [1/3, 2/3].
f ′(t) = xj
for tj−1 < t ≤tj, 1 ≤j ≤n,
where t0 = 0. From the approximation
f(tj) =
 tj
0
f ′(t)dt ≈1
n
j

k=1
xk,
we derive the discrete model
y = Ax + e,
where the matrix A is the lower triangular matrix

4.4 Ill-conditioning and errors in the data
87
A = 1
n
⎡
⎢⎢⎢⎣
1
1 1
...
...
1 1 · · · 1
⎤
⎥⎥⎥⎦.
The Matlab code for generating the matrix A is
col = ones(n,1);
row = zeros(1,n); row(1) = 1;
A = (1/n)*toeplitz(col,row);
We calculate A of size 128 × 128. The condition number of such matrix is
not particularly bad, only a few hundreds, but if the noise is signiﬁcant,
the solution without regularization is quite useless.
We generate a piecewise linear noiseless signal which corresponds to the
integral of a boxcar function, and add noise to it, see Figure 4.10. Then
we solve the regularized problem with diﬀerent values of δ, ranging in
the interval 10−6 · · · 1. In Figure 4.11, we have plotted the norm of the
discrepancy
dk = ∥Axδk −b∥
for a few values of δ = δk. We select three of these values, close to τη,
where η is the norm of the particular realization of the noise vector and
τ = 1.5. In the same ﬁgure we also plot the corresponding estimates of
the derivatives. We see that above the noise limit, the solution starts to
lose details – an eﬀect called overregularization in the literature – while
below the noise is strongly present – an eﬀect called underregularization.
The discrepancy principle seems to give a reasonable solution in this case.
In the deterministic setting, when the norm of the noise is not known,
some of the stopping criteria proposed have been based on the Generalized
Cross Validation. Another strategy is to detect of ampliﬁed noise components
in the computed solution by monitoring the growth of its norm via the so
called L-curve. We will not discuss these methods here.
As a ﬁnal comment, consider the normal equations corresponding to an
ill-posed system. Let A ∈Rm×n be a matrix that has the singular value
decomposition
A = UDV T,
D = diag(d1, d2, . . . , dmin(n,m)),
implying that the matrix ATA appearing in the normal equations admits the
decomposition
ATA = V DT U TU
  
=I
DV T = V DTDV T.
Here, the matrix DTD ∈Rn×n is diagonal, and the diagonal entries which are
the eigenvalues of ATA are d2
1, . . . , d2
min(n,m), augmented with trailing zeros

88
4 Basic problem in numerical linear algebra
if m < n. From this analysis, it is obvious that the condition number of
the matrix may be dramatically larger than that of A, which leads to the
conclusion that forming the matrix ATA may not be always advisable.
Exercises
1. If f is a sample from a T-periodic continuous signal, we approximate
numerically its Fourier coeﬃcients by writing
f(k) = 1
T
 T
0
exp

−i2π
T tk

f(t)dt
≈1
T
T
n
n−1

j=0
exp

−i2π
T tjk

f(tj)
= 1
n
n−1

j=0
W −jk
n
xj,
Wn = exp

i2π
n

.
The transformation
Cn →Cn,
x →
⎡
⎣1
n
n−1

j=0
W −jk
n
xj
⎤
⎦
1≤k≤n
,
is called the Discrete Fourier Transform, and we denote it by DFT(x).
This linear operation allows a fast numerical implementation that is based
on the structure of the matrix multiplying x. The implementation is called
Fast Fourier Transform (FFT).
Show that if x(ℓ)
j
is a discretization of the single frequency signal
f (ℓ)(t) = exp

i2π
T ℓt

,
ℓ= 0, ±1, ±2, . . . ,
we get
DFT(x(ℓ))k =

1,
k = ℓ(mod n),
0
otherwise.
Deduce, in particular, that if f is a band limited signal of the form
f(t) =
n/2

ℓ=−n/2+1
f(ℓ)f (ℓ)(t),
the corresponding discretized signal

4.4 Ill-conditioning and errors in the data
89
x =
n/2

ℓ=−n/2+1
f(ℓ)x(ℓ),
has discrete Fourier transform
DFT(x) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
f(0)
f(1)
...
f(n/2)
f(−n/2 + 1)
...
f(−1)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(4.7)
In other words, the DFT returns the Fourier coeﬃcients exactly but per-
muting the order as indicated above.
2. Based on the result of the previous exercise, show that if the spectrum of a
signal is not contained in a band of width n, the spectrum calculated with
the DFT with n discretization points is aliased, i.e., the Fourier coeﬃcients
that are oﬀset by n are added together; see Figure 4.12 for an illustration
of this phenomenon.
0
−n/2+1
n/2
0
−n/2+1
n/2
Fig. 4.12. True spectrum of the signal that is sampled with frequency n (left), and
the resulting aliased spectrum based on too low sampling frequency (right).
3. The Inverse Discrete Fourier Transform and denoted by IDFT, is deﬁned
as
IDFT(y)k =
n−1

j=0
W jk
n yj,
0 ≤k ≤n −1.
Show that

90
4 Basic problem in numerical linear algebra
IDFT

DFT(x)

= DFT

IDFT(x)

= x.
4. Write a Matlab code for implementing the Conjugate Gradient, CGLS
and GMRES methods. Include the option that the intermediate iterates
xk can be saved and returned as output, as these are important later to
understand the use of iterative solvers as regularization method.

5
Sampling: ﬁrst encounter
The basic problem of statistical inference is to infer on an unknown distribu-
tion based on a sample that is believed to come from that distribution. In this
section, we start our journey in the opposite direction. That is, we want to
generate a sample from a given distribution. Why is such a task interesting
or important? There are at least two reasons.
It has been demonstrated on several occasions in the previous chapters that
drawing samples from a given distribution helps us understand and visualize
the problems that are being studied. In particular, when we write a prior
probability density starting from a prior belief that from the beginning is
not quantitative, we need to investigate whether the prior distribution really
corresponds to what we believe. A useful way to check the quality of the prior
is to draw a random sample and look at the realizations. This could be called
a visual inspection of the qualities of the prior.
The second important use of sampling is related to integration. Suppose
that we have a probability density π and we are asked to estimate the integral
I =

f(x)π(x)dx.
Estimating integrals by numerical quadratures is a classical problem: one
deﬁnes quadrature points xj and corresponding weights wj dictated by the
quadrature rule, then writes the approximation
I ≈
N

j=1
wjf(xj).
Problems arise if the dimensionality of the variable x ∈Rn is high. For in-
stance, approximating integrals by Gauss quadrature of order k over a rect-
angle in Rn containing the support of f requires k discretization points per
direction, i.e., in the above formula N = kn. It is quite obvious that for n
large, the computational burden required is beyond reasonable. So how does

92
5 Sampling: ﬁrst encounter
sampling help? Assume that instead of the quadrature points, we draw a sam-
ple S =

x1, x2, . . . , xN

of mutually independent realizations xj ∈Rn that
are distributed according to π. Then, the Law of Large Numbers tells us that
lim
N→∞
1
N

f(x1) + f(x2) + · · · + f(xN)




=IN
= I
almost certainly, and the Central Limit Theorem states that asymptotically,
var

IN −I

≈var(f(X))
N
,
that is, regardless of the dimensionality of the problem, the error goes to zero
like 1/
√
N. There is another reason why sampling helps: to apply quadrature
rules, one has to ﬁnd ﬁrst a reasonable region that contains the essential part
of the density π. If n is large, this may be a frustrating task not to mention
that ﬁnding such region is often already a piece of valuable information and
object of the statistical inference.
Integration based on random sampling is called Monte Carlo integration.
Although it looks very attractive, it has indeed its share of diﬃculties; oth-
erwise one might be led to believe that the world is too perfect. One of the
problems is how to draw the sample. We start studying this problem in this
section, and we will return to it later with more sophisticated tools at our
disposal.
There are two elementary distributions that we take for granted in the
sequel. We assume that we have access to
•
a random generator from the standard normal distribution,
π(x) =
1
√
2π exp

−1
2x2

,
•
a random generator from the uniform distribution over the interval [0, 1],
π(x) = χ[0,1](x).
These random generators in Matlab are called with the commands randn and
rand, respectively.
5.1 Sampling from Gaussian distributions
To generate random draws from a Gaussian density, we transform the density
into the standard normal density by a process called whitening. If X be a
multivariate Gaussian random variable, X ∼N(x0, Γ), with the probability
density

5.1 Sampling from Gaussian distributions
93
π(x) =

1
(2π)ndet(Γ)
1/2
exp

−1
2(x −x0)TΓ −1(x −x0)

,
and
Γ −1 = RTR
is the Cholesky decomposition of the inverse of Γ, the probability density of
X can be written as
π(x) =

1
(2π)ndet(Γ)
1/2
exp

−1
2∥R(x −x0)∥2

.
Deﬁne a new random variable,
W = R(X −x0),
(5.1)
whose probability density is
P

W ∈B

= P

R(X −x0) ∈B

= P

X ∈R−1(B) + x0

=

1
(2π)ndet(Γ)
1/2 
R−1(B)+x0
exp

−1
2∥R(x −x0)∥2

dx.
The change of variable,
w = R(x −x0),
dw = |det(R)|dx,
and the observation that
1
det(Γ) = det(Γ −1) = det(RT)det(R) = (det(R))2 ,
lead to the formula
P

W ∈B

=
1
(2π)n/2

B
exp

−1
2∥w∥2

dw,
i.e., W is Gaussian white noise1
W ∼N(0, I).
The transformation (5.1) is therefore called whitening of X, and the Cholesky
factor R of the inverse of the covariance the whitening matrix.
If the whitening matrix is known, random draws from a general Gaussian
density can be generated as follows:
1 The name white noise has its roots in signal processing: if we think of X as a
signal sequence of length n, its FFT is also a random variable with covariance the
identity. The interpretation of this assertion is that all frequencies of X have the
same average power, so when we “play” a realization of X, it sounds like noise
with high and low frequencies in balance.

94
5 Sampling: ﬁrst encounter
1. Draw w ∈Rn from the Gaussian white noise density,
2. Find x ∈Rn by solving the linear system
w = R(x −x0).
The process is put into action in following example.
Example 5.1: This example not only demonstrates the generation of ran-
dom draws by whitening but also discusses the construction of structural
prior densities. It is often the case that we seek to estimate a parame-
ter vector that we believe belongs to a class characterized by a structure,
whose exact details may not be fully speciﬁed. It is natural to think that,
if we know the structure, importing the information via the prior would
simplify the task of computing the solution of the problem and improve
its quality.
To clarify this notion and to explain how to construct a structural prior,
we go back to the bacteria density estimation, discussed in Example 3.8.
As previously, assume that the medium where the bacteria live is a square
array subdivided into square subunits, but now instead of one, we have
two diﬀerent cultures, which causes the density of bacteria to be diﬀerent
in diﬀerent regions of the square, see Figure 5.1. We also assume that the
areas of the two cultures are not perfectly isolated, and this aﬀects the
density of bacteria around the boundary between these regions.
xj
ε xdown
xup
xleft
xright
Fig. 5.1. A square array with two diﬀerent subdomains (left). The coupling con-
stant ε determines the strength of the coupling across the borderline (right).
As before, we assume that the density of bacteria in each square inside
one type of culture varies smoothly, thus if Xj is the number of bacteria in
the jth square, we write a stochastic model
Xj = 1
4(Xup + Xdown + Xleft + Xright) + R;
(5.2)

5.1 Sampling from Gaussian distributions
95
where R is the innovation term. This formula holds for all interior pixels
in both cultures. Consider a borderline pixel, whose “up”, “right” and
“left” neighbors are in the same culture, but whose “down” neighbor is in
the other culture, see Figure 5.1. Since the density in the “down” square is
presumably diﬀerent, it should not enter the averaging process in the same
way as that of the other neighbors. However, since the isolation between
the two cultures is not perfect, it should not be completely left out either.
Therefore we perform a weighted averaging, where densities from the other
culture are given a lower weight,
Xj =
1
3 + ε(Xup + εXdown + Xleft + Xright) + R.
(5.3)
The factor ε expresses how strongly, or weakly, coupled we believe the
two cultures to be; the closer to zero it is the more uncoupled. Similarly,
for corner pixels with two neighbors in the other culture, we will use the
averaging
Xj =
1
2 + 2ε(Xup + εXdown + εXleft + Xright) + R.
(5.4)
Assuming that the innovation term R in all three cases (5.2)–(5.4) is Gaus-
sian with variance σ2 and is independent from pixel to pixel as in Example
3.8, we arrive at the Gaussian prior model
πprior(x | σ2) ∝exp

−1
2σ2 ∥Lεx∥2

= exp

−1
2σ2 xT
LT
ε Lε

x

,
where
Lε = I −Aε ∈RN×N
follows (5.2) – (5.4).
The inverse of the covariance matrix is immediately available: in fact, we
have
Γ −1 = 1
σ2 LT
ε Lε.
To calculate the whitening matrix, we could compute the Cholesky factor
of this matrix. However, since Lε is invertible, we do not need to compute
anything: in fact, the random variable deﬁned as
W = 1
σ LεX
is Gaussian white noise!
How does one form the matrices Aε and Lε? If we have already calculated
the matrix A of Example 3.8, the modiﬁcation is almost trivial. To see
this, consider the situation illustrated in Figure 5.1: the jth pixel has all
neighbors but one in the same domain. This means that the jth row of the
matrix A is modiﬁed as follows:

96
5 Sampling: ﬁrst encounter
(up)
(down)
(left)
(right)
A(j, : ) =

0 · · · 1/4 · · · 1/4 · · · 1/4 · · ·
1/4 · · · 0

−→Aε(j, : ) =
4
3 + ε

0 · · · 1/4 · · · ε/4 · · · 1/4 · · ·
1/4 · · · 0

,
i.e., the elements that point to neighbors belonging to diﬀerent domain
are multiplied by ε. This leads to a simple algorithm: ﬁrst, the pixels are
reordered so that ﬁrst come the pixels in the ﬁrst domain, then the pixels
in the second domain. We write a partitioning
x =
x(1)
x(2)

,
where x(j) is a vector containing the pixel values in domain j, j = 1, 2,
and we partition the matrix A accordingly,
A =

A11 A12
A21 A22

.
The diagonal blocks A11 and A22 contain the averaging coeﬃcients within
the domains, while the oﬀ-diagonal ones contain the coeﬃcients across the
domain boundaries. Therefore, we have
Aε = Dε

A11 εA12
εA21 A22




= 
Aε
,
where Dε is a diagonal matrix whose diagonal is the inverse of the row
sum of the matrix 
Aε.
Below is one possible way of constructing the matrix Lε.
n = 50;
% Number of pixels per direction
% Define the structure: a rectangle R = [k1,k2]x[k1,k2]
k1 = 15; k2 = 35;
% Create an index vector Iin that contains indices
% to squares within the inner domain. The remaining
% indices are in Iout.
I = [1:n^2];
I = reshape(I,n,n);
Iin = I(k1:k2,k1:k2);
Iin = Iin(:);

5.1 Sampling from Gaussian distributions
97
Iout = setdiff(I(:),Iin);
% Smoothness prior with Dirichlet boundary without the
% structure: Calculate A as in Example 3.8 (omitted
% here).
% Defining the weak coupling constant epsilon and
% uncoupling
epsilon = 0.1;
A(Iin,Iout) = epsilon*A(Iin,Iout);
A(Iout,Iin) = epsilon*A(Iout,Iin);
% Scaling by a diagonal matrix
d = sum(A’)’;
A = spdiags(1 ./d,0,speye(n^2))*A;
L = speye(n^2) - A;
In order to avoid the unnecessary crowding of the computer memory, we
deﬁne directly the factor of the inverse of the covariance matrix as a sparse
matrix. The role of the index sets Iin and Iout is fundamental, because
it deﬁnes the spatial location of the structure. In our example, the in-
dices of the pixels corresponding to the inner structure belong to Iin and
the indices of the pixels outside this structure are in Iout. Note how we
construct Iout from Iin using the logical Matlab command setdiff.
Once the structural whitening matrix has been computed, we can perform
random draws from the corresponding prior with diﬀerent values of ε. Here
is a section of Matlab code that does the random draws and plots them as
surfaces and as images.
ndraws = 4;
for j = 1:ndraws
w = randn(n*n,1);
x = L\w;
figure(j)
surfl(flipud(reshape(x,n,n)))
shading interp
colormap(gray)
figure(ndraws+j)
imagesc((reshape(x,n,n))
axis(’square’)
axis(’off’)
shading flat
colormap(0.95*(1-gray))
end

98
5 Sampling: ﬁrst encounter
Fig. 5.2. Four random draws from the structural smoothness prior with the cou-
pling parameter ε = 0.1.

5.2 Random draws from non-Gaussian densities
99
Figure 5.2 shows four random draws from a Gaussian density with a struc-
tural prior constructed as described above and coupling parameter equal
to ε = 0.1.
5.2 Random draws from non-Gaussian densities
We have thus seen that random draws from Gaussian distributions seem to
be under control. But what about non-Gaussian ones? As a prelude to the
Markov Chain Monte Carlo (MCMC) techniques to be introduced later, let
us look at two model algorithms. They are based on the following ideas:
(a) Draw the random sample directly from the actual distribution,
(b) Find a fast approximate way of drawing, and correct later the sample
in some way.
The former approach is attractive, but it turns out to be costly unless the
probability density is one of standard form (e.g. normal) for which eﬀective
random generators are available. The success of the second approach depends
entirely of how smart proposal distribution we are able to design, and might
require a lot of hand tuning.
We start with the approach (a), restricting the discussion, for the time
being, to one-dimensional densities. Let X be a real valued random variable
whose probability density is π(x). For simplicity, we assume that in some
interval, ﬁnite or inﬁnite, π(x) > 0, and π(x) = 0 possibly only at isolated
points. Deﬁne the cumulative distribution function,
Φ(z) =
 z
−∞
π(x)dx.
Clearly, Φ is non-decreasing since π ≥0, and 0 ≤Φ ≤1. In fact, the assump-
tion about strict positivity of π implies that Φ is strictly increasing in some
interval.
Let us deﬁne a new random variable,
T = Φ(X).
(5.5)
We claim that T ∼Uniform([0, 1]). To see this, observe ﬁrst that, due to the
monotonicity of Φ,
P

T < a

= P

Φ(X) < a

= P

X < Φ−1(a)

,
0 < a < 1.
On the other hand, by the deﬁnition of the probability density,
P

X < Φ−1(a)

=
 Φ−1(a)
−∞
π(x)dx =
 Φ−1(a)
−∞
Φ′(x)dx.

100
5 Sampling: ﬁrst encounter
After the change of variable
t = Φ(x),
dt = Φ′(x)dx,
we arrive at
P

T < a

=
 Φ−1(a)
−∞
Φ′(x)dx =
 a
0
dt = a,
which is the very deﬁnition of T being a random variable with uniform dis-
tribution.
π(x)
Φ(x)
t
x
1
Fig. 5.3. Draw of x from the distribution π(x). Here, t ∼Uniform([0, 1]).
We have now an algorithm to draw from the distribution π, graphically
illustrated in Figure 5.3:
1. Draw t ∼Uniform([0, 1]),
2. Calculate x = Φ−1(t).
This algorithm, sometimes referred to as the Golden Rule or Inverse Cu-
mulative Distribution Rule, is very useful also in multivariate cases. Attractive
and innocent as it looks, its implementation can present some problems, as
the following example shows.
Example 5.2: Consider a one-dimensional normal distribution with a
bound constraint,
π(x) ∝πc(x)exp

−1
2x2

,
where
πc(x) =

1, if x > c,
0, if x ≤c
c ∈R,
from which we want to generate a sample.
The cumulative distribution function is

5.2 Random draws from non-Gaussian densities
101
Φ(z) = C
 z
c
e−x2/2dx,
where C > 0 is a normalizing constant,
C =
 ∞
c
e−x2/2dx
−1
.
The function Φ has to be calculated numerically. Fortunately, there are
routines readily available to do the calculation: in Matlab, the built-in
error function, erf, is deﬁned as
erf(t) =
2
√π
 t
0
e−s2ds.
We observe that
Φ(z) = C
 z
0
−
 c
0

e−x2/2dx =
√
2C
" z/
√
2
0
−
 c/
√
2
0
#
e−s2ds
=
)π
2 C
&
erf(z/
√
2) −erf(c/
√
2)
'
.
On the other hand, since
C =
)π
2 C
&
1 −erf(c/
√
2)
'−1
,
we have
Φ(z) =

erf(z/
√
2) −erf(c/
√
2)


1 −erf(c/
√
2)

.
What about the inverse? Writing
Φ(x) = t,
after some algebraic manipulation, we ﬁnd that
erf(z/
√
2) = t

1 −erf(c/
√
2)

+ erf(c/
√
2).
Again, fortunately there are eﬀective algorithms to calculate the inverse of
the error function, for example erfinv in Matlab:
z = Φ−1(t) =
√
2erﬁnv

t

1 −erf(c/
√
2)

+ erf(c/
√
2)

.
Hence, random generation in Matlab is very simple:
a = erf(c/sqrt(2));
t = rand;
z=sqrt(2)*erfinv(t*(1-a)+a);

102
5 Sampling: ﬁrst encounter
Here, a warning is in order: if the bound c is large, the above program
does not work. The reason is that the error function saturates quickly to
unity, so when c is large, numerically a = 1, and the above algorithm gives
numerical noise as a value of z. When working with large values of c, one
needs to modify the code, but we do not discuss the modiﬁcation here.
Rather, we leave the modiﬁcation as an exercise.
The Golden rule can be applied to integer valued densities, too, as the
next example demonstrates.
Example 5.3: Consider the problem of drawing from the Poisson distri-
bution,
π(n) = π(n | θ) = θn
n! e−θ,
n = 0, 1, 2, · · · ,
whose cumulative distribution is also discrete,
Φ(k) = e−θ
k

n=0
θn
n! .
The random draw is done now via the following steps:
1. Draw t ∼Uniform([0, 1]),
2. Find the smallest integer k such that Φ(k) > t.
The following is a segment of Matlab code that performs the random draw:
t = rand;
Phi = exp(-theta);
p = exp(-theta);
n = 0;
while Phi<=t
p = p * (theta /(n + 1));
Phi = Phi + p;
n = n + 1;
end
k = n;
This algorithm was used in Chapter 3 to produce Figure 3.3.
5.3 Rejection sampling: prelude to Metropolis-Hastings
The Golden Rule can be easily extended to multivariate distributions pro-
vided that the components are mutually independent. If this is not the case,
more advanced techniques need to be introduced. This leads us to consider
the alternative (b) in the previous section, drawing from an easy surrogate
distribution and then later correcting the sample in some way. As a prelude
to this, we consider ﬁrst a rather primitive idea.

5.3 Rejection sampling: prelude to Metropolis-Hastings
103
Example 5.4: Consider the distribution of Example 5.1, which is a Gaus-
sian density with a bound constraint. The most straightforward approach
for generating a sample from this distribution would probably be the fol-
lowing trial-and-error algorithm:
1. Draw x from the normal distribution N(0, 1),
2. If x > c, accept, otherwise reject.
It turns out that this simple algorithm is a particular case of what is known
as the rejection sampling algorithm. It works, but it can be awfully slow.
To understand why, consider the acceptance rate for diﬀerent values of c.
If c = 0, in the average every other proposal will be accepted, since the
underlying normal distribution is symmetric. If c < 0, the acceptance rate
will be higher than 50%. On the other hand, if c > 0, it will be smaller. In
general, the probability of hitting the domain of acceptance {x > c} is
I(c) =
1
√
2π
 ∞
c
e−x2/2dx,
and I(c) →0 superexponentially as c →∞. For instance, if c = 3, the
acceptance rate is less than 0.1%.
The situation gets worse for multidimensional problems. Consider for ex-
ample a multivariate distribution in Rn,
π(x) ∝π+(x)exp

−1
2|x|2

,
x ∈Rn,
where
π+(x) =
1, if xj > 0 for all j,
0, else
.
What is the acceptance rate using the trial-and-error algorithm? The nor-
mal distribution is symmetric around the origin, so any sign combination
is equally probable. Since
P{xj > 0} = 1
2,
1 ≤j ≤n,
the rate of successful proposals is
P{x1 > 0, x2 > 0, . . . , xn > 0} =
n

j=1
P{xj > 0} =
1
2
n
,
and it is clear that even the innocent looking positivity constraint puts us
out of business if n is large. The central issue therefore becomes how to
modify the initial random draws, step 1 above, to improve the acceptance.
Inspired by the trial-and-error drawing strategy of the previous example,
we discuss next an elementary sampling strategy that works – at least in
theory – also for multidimensional distributions.

104
5 Sampling: ﬁrst encounter
Assume that the true probability density π(x) is known up to a multi-
plicative constant, i.e., we have Cπ(x) with C > 0 unknown, and we have a
proposal distribution, q(x) that is easy to use for random draws – for instance,
a Gaussian distribution. Furthermore, assume that the proposal distribution
satisﬁes the condition
Cπ(x) ≤Mq(x)
for some M > 0.
The following algorithm is a prototype for the Metropolis-Hastings algorithm
that will be introduced later:
1. Draw x from the proposal distribution q(x),
2. Calculate the acceptance ratio
α = Cπ(x)
Mq(x),
0 < α ≤1.
3. Flip α–coin: draw t ∼Uniform([0, 1]), and accept x if α > t, otherwise
reject.
The last step says simply that x is accepted with probability α, rejected
with probability 1 −α, thus the name α–coin.
Why does the algorithm work? What we need to show is that the distri-
bution of the accepted x is π. To this end, let us deﬁne the event
A = event that a draw from q is accepted,
regardless of what the draw is. The distribution of the accepted samples drawn
from q, i.e., the distribution of the sample that we have just generated, is
π(x | A) = distribution of x provided that it is accepted.
To calculate this distribution, we use the Bayes formula,
π(x | A) = π(A | x)q(x)
π(A)
.
(5.6)
Above, the prior of x is obviously q since we draw it from q. One interpretation
of this is that before we test the acceptance, we believe that q is the correct
distribution. What are the other densities appearing above?
The likelihood, the probability of acceptance provided that x is given, is
clearly the acceptance rate,
π(A | x) = α = Cπ(x)
Mq(x).
The marginal density of the acceptance is, in turn,

5.3 Rejection sampling: prelude to Metropolis-Hastings
105
π(A) =

π(x, A)dx =

π(A | x)q(x)dx
=

Cπ(x)
Mq(x)q(x)dx = C
M

π(x)dx
= C
M .
Putting all the pieces in (5.6), we have
π(x | A) =

Cπ(x)/Mq(x)

q(x)
C/M
= π(x),
exactly as we wanted.
We conclude this section by checking that the simple example of Gaussian
distribution with a bound constraint is really a version of a rejection sampling
algorithm. In that case, the true distribution, up to a normalizing constant,
is
Cπ(x) = πc(x)exp

−1
2x2

,
the proposal distribution is Gaussian,
q(x) =
1
√
2π exp

−1
2x2

,
and the scaling constant M can be chosen as M =
√
2π so that
Mq(x) = exp

−1
2x2

≥Cπ(x).
The acceptance rate is then
α = Cπ(x)
Mq(x) = πc(x) =

1, if x > c,
0, if x ≤c.
Flipping the α–coin in this case says that we accept automatically if x > c
(α = 1) and reject otherwise (α = 0), which is exactly what our heuristic
algorithm was doing.
Exercises
1. Test numerically how large the parameter c in Example 5.2 may be with-
out the algorithm breaking down. Implement an alternative algorithm
along the following guidelines: select ﬁrst an upper limit M > c such
that if x > M, π(x)/π(c) < δ, where δ > 0 is a small cut-oﬀparame-
ter. Then divide the interval [c, M] in n subintervals, compute a discrete
approximation for the cumulative distribution function and approximate
numerically Φ−1(t), t ∼Uniform([0, 1]). Compare the results with the
algorithm of Example 5.2 when c is small.

106
5 Sampling: ﬁrst encounter
2. Implement the rejection sampling algorithm when π is the truncated Gaus-
sian density as in Example 5.2 and the proposal distribution an exponen-
tial distribution, q(x) = βexp(−β(x −c)), x ≥c. Try diﬀerent values for
β and investigate the acceptance rate as a function of it.

6
Statistically inspired preconditioners
In Chapter 4 we discussed iterative linear system solvers and their appli-
cation to ill-conditioned inverse problems, following a traditional approach:
numerical linear algebra helps in solving problems in computational statis-
tics. Characteristic of this approach is that when solving an ill-conditioned
linear system Ax = b, the remedies for ill-posedness usually depend on the
properties of the matrix A, not on what we expect the solution x to be like. It
is interesting to change perspective, and devise a strategy for importing statis-
tical ideas into numerical linear algebra. Statistically inspired preconditioners
provide a method for doing this.
The choice of a particular iterative method for the solution of a linear
system of equations is usually dictated by two considerations: how well the
solution can be represented in the subspace where the approximate solution
is sought, and how fast the method will converge. While the latter issue is
usually given more weight in the general context of solving linear systems
of equations, the former is more of interest to us. In fact, the subjective
component in solving a linear system is why we are solving the particular
system, and what we expect the solution to look like, which is the essence of
Bayesian statistics.
The solution of a linear system by iterative solvers is sought in a subspace
that depends on the matrix deﬁning the system. If we want to incorporate
prior information about the solution into the subspaces, evidently we have to
modify the linear system using the prior information. This is the principal
idea in this chapter.
We start by brieﬂy reviewing some of the basic facts and results about
preconditioners which we will be using later. Given a linear system of equa-
tions
Ax = b,
A ∈Rn×n,
(6.1)
with A invertible, and a nonsingular matrix M ∈Rn×n, it is immediate to
verify that the linear system
M −1Ax = M −1b
(6.2)

108
6 Statistically inspired preconditioners
has the same solution as (6.1). Likewise, if R ∈Rn×n is a nonsingular matrix,
the linear system
AR−1w = b,
Rx = w,
(6.3)
has also the same solution x as (6.1).
The convergence rate of an iterative method for the solution of a linear
system of equations (6.1) depends typically on the spectral properties of the
matrix A. Thus, if we replace the linear system (6.1) with (6.2) or (6.3), the
rate of convergence will depend on the spectral properties of M −1A or AR−1,
respectively, instead of on the spectral properties of A. It is therefore clear
that if M or R are well chosen, the convergence of an iterative method for
(6.2) or (6.3), respectively, may be much faster than for (6.1).
The matrix M (R, respectively) is called a left (right) preconditioner and
the linear system (6.2) (or (6.3)) is referred to as the left (right) precondi-
tioned linear system. Naturally, it is possible to combine both right and left
preconditioners and consider the system
M −1AR−1w = M −1b,
Rx = w.
In the statistical discussion to ensue, the left and right preconditioners will
have diﬀerent roles: one will be related to the likelihood, the other to the prior.
6.1 Priorconditioners: specially chosen preconditioners
In general, the closer the matrix A is to the identity, the easier it becomes
for a Krylov subspace iterative method to suﬃciently reduce the norm of the
residual error. Thus, for the construction of good preconditioners it is desirable
to ﬁnd a matrix M or R such that AR−1 or M −1A is close to an identity.
Since the application of an iterative method to a preconditioned linear
system will require the computation of matrix-vector products of the form
M −1Ax or AR−1w, followed by the solution of linear systems of the form
Rx = w in the case of right preconditioners, it is also important that these
computations can be done eﬃciently.
In the general case, the question of whether we should use left or right
preconditioning depends only on the iterative method that we are using and
on whether or not there are reasons to keep the right-hand side of the original
linear system unchanged. Furthermore, the best general purpose precondition-
ers are those which yield the fastest rate of convergence with the least amount
of work.
The situation becomes a little diﬀerent when dealing with linear systems of
equations with an ill-conditioned matrix and a right-hand side contaminated
by noise. Since, as we have seen in Chapter 4, as the iteration number increases
and ampliﬁed noise components start to corrupt the computed solution, it is
important to avoid preconditioners which speed up the convergence of the
noise.

6.1 Priorconditioners: specially chosen preconditioners
109
In Chapter 4 we also saw that a linear system’s sensitivity to noise was
related to the smallest eigenvalues, or more generally, the smallest singular val-
ues. In an eﬀort to accelerate the rate of convergence of the iterative methods,
while keeping the noise in the right hand side from overtaking the computed
solution, preconditioners which cluster only the largest eigenvalues of the ma-
trix A, while leaving the smaller eigenvalues alone, have been proposed in the
literature. The problem with this class of preconditioners is that the separa-
tion of the spectrum of A into the eigenvalues which should be clustered and
those which should be left alone may be diﬃcult if there is no obvious gap in
the spectrum. Furthermore, the separation of the spectrum should depend on
the noise level. Moreover, ﬁnding the spectral information of the matrix may
be computationally challenging and costly.
One goal in the general preconditioning strategy is to gain spectral infor-
mation about the operator and to modify it to obtain an equivalent system
with better convergence properties. In our case, however, the main interest is
in the properties of the unknowns, some of which we may know a priori, and
the preconditioner is the Trojan horse which will carry this knowledge into
the algorithm. Thus, instead of getting our inspiration about the selection
of preconditioners from the general theory about iterative system solvers, we
examine them in the light of linear Gaussian models that are closely related
also to Tikhonov regularization.
The design of regularizing preconditioners starts traditionally from
Tikhonov regularization, which is a bridge between statistical and non-
statistical theory of inverse problems. Instead of seeking a solution to the
ill-conditioned system (6.1), the strategy in Tikhonov regularization is to re-
place the original problem by a nearby minimization problem,
xδ = arg min

∥Ax −b∥2 + δ2∥Rx∥2
,
(6.4)
where the penalty term is selected in such a way that for the desired solution,
the norm of Rx is not excessively large, and the regularization parameter δ is
chosen, e.g., by the discrepancy principle, if applicable.
The simplest version of Tikhonov regularization method – sometimes, er-
roneously, called the Tikhonov regularization method, while calling (6.4) a
generalized Tikhonov regularization1 – is to choose R = I, the identity ma-
trix, leading to the minimization problem
xδ = arg min

∥Ax −b∥2 + δ2∥x∥2
.
(6.5)
As we have indicated in Chapter 4, an alternative to Tikhonov regularization
(6.5) is to use iterative solvers with early truncation of the iterations. In
particular, when the CGLS method is employed, the norms of the iterates
1 Andrei Nikolaevich Tikhonov (1906–1993), who had a great impact on various
areas of mathematics, was originally a topologist, and, loyal to his background,
thought of regularization in terms of compact embeddings.

110
6 Statistically inspired preconditioners
form a non-decreasing sequence, yielding a straightforward way of monitoring
the penalty term ∥x∥.
So what could be an alternative to the more general regularization strategy
(6.4)? It is obvious that if R is a nonsingular square matrix, the regularization
strategy (6.4) is equivalent to
wδ = arg min

∥AR−1w −b∥2 + δ2∥w∥2
,
Rxδ = wδ,
and the natural candidate for an alternative strategy to Tikhonov regulariza-
tion would be a truncated iterative solver with right preconditioner R.
To understand the eﬀects of choosing preconditioners in this fashion, we
go back to the Bayesian theory, which was our starting point to arrive at
Tikhonov regularization. We consider the linear additive model
B = AX + E,
E ∼N(0, σ2I),
X ∼N(0, Γ),
whose posterior probability density is
π(x | b) ∝exp

−1
2σ2 ∥b −Ax∥2 −1
2xTΓ −1x

.
Let
Γ = UDU T,
be the eigenvalue decomposition of the covariance matrix Γ, where U is an
orthogonal matrix whose columns are the eigenvectors of Γ and the diagonal
matrix
D = diag

d1, d2, . . . , dn

,
d1 ≥d2 ≥· · · ≥dn,
contains the eigenvalues in decreasing order. After introducing the symmetric
factorization
Γ −1 = UD−1/2 D−1/2U T



=R
= RTR,
we can write the posterior in the form
π(x | b) ∝exp

−1
2σ2

∥b −Ax∥2 + σ2∥Rx∥2
,
from which it can be easily seen that the Maximum A Posteriori estimate is
the Tikhonov regularized solution with δ = σ and
R = D−1/2U T.
(6.6)
This formulation lends itself naturally to a geometric interpretation of
Tikhonov penalty. Since
Rx =
n

j=1
1
	
dj
(uT
j x),

6.1 Priorconditioners: specially chosen preconditioners
111
Fig. 6.1. The light ellipse is the preimage of a disc in the mapping A, the darker
one is an equiprobability ellipse deﬁned by the prior. The eigenvectors of the prior
covariance matrix are also shown.
the penalty term eﬀectively pushes the solution towards the eigenspace cor-
responding to largest eigenvalues of the covariance matrix by penalizing the
solution for the growth of those components uT
j x that are divided by small
numbers
	
dj.
Example 6.1: We illustrate graphically what is going on, considering the
2 × 2 toy example discussed in Examples 4.5 and 4.6, equipped with a
Gaussian prior. In Figure 6.1 we have plotted the preimage of a small
disc around the noiseless data (light ellipse), as in Example 4.5, and in
addition a shaded equiprobability ellipse of the prior density. By the very
deﬁnition of prior, we believe that the solution could a priori have large
variance in the direction of the major semiaxis of this ellipse, which is
also the eigendirection corresponding to the larger eigenvalue d1 of the
covariance matrix, while the variance should be small in the direction of the
minor semiaxis, which is the eigendirection corresponding to the smaller
eigenvalue d2.
We solve the original linear system
Ax = bj,
bj = b∗+ ej,
1 ≤j ≤N,
with a sample of noisy right hand sides, with the noise realizations ej
drawn from a Gaussian white noise density. We choose to solve the problem
iteratively, using CGLS iteration and ignoring the prior. We then repeat
the process, this time solving the preconditioned linear system
AR−1w = bj,
Rx = w,
bj = b∗+ ej,
1 ≤j ≤N,
with the same data set and using CGLS iteration. The results are shown
in Figures 6.2–6.3. We observe that after two iterations the solutions are

112
6 Statistically inspired preconditioners
the same in both cases. This is not a surprise, since for a regular two-
dimensional problem, the CGLS method converges in two iterations, and
the original and the modiﬁed problems are equivalent. Diﬀerences are vis-
ible, however, after the ﬁrst iteration. The preconditioning reduced the
variability of the solution signiﬁcantly in the direction of the small eigen-
value of the prior. Hence, the eﬀect of preconditioning was to produce
approximate solutions that correspond qualitatively to what we believed
a priori of the solution. Thus, we may say that if we wanted to solve the
problem with truncated CGLS and take just one iteration step, qualita-
tively the use of the preconditioner paid oﬀ.
Fig. 6.2. Approximate solutions of the linear systems in Example 6.1, correspond-
ing to a sample of 200 noisy realizations of the right hand side, obtained after one
(left) and two (right) iterations of the CGLS method.
Inspired by the small example discussed above, consider now a general
linear model with Gaussian prior and Gaussian additive noise,
B = AX + E,
X ∼N(x0, Γ),
E ∼N(e0, Σ),
and X and E mutually independent. After introducing a symmetric factor-
ization, e.g. the Cholesky decompositions, of the inverses of the covariance
matrices,
Γ −1 = RTR,
Σ−1 = STS,
(6.7)
the posterior density can be expressed as
π(x | b)
∝exp

−1
2(b −Ax −e0)TΣ−1(b −Ax −e0) −1
2(x −x0)TΓ T(x −x0)

= exp

−1
2

∥S(b −Ax −e0)∥2 + ∥R(x −x0)∥2
,

6.1 Priorconditioners: specially chosen preconditioners
113
Fig. 6.3. Approximate solutions of the linear systems in Example 6.1, correspond-
ing to a sample of 200 noisy realizations of the right hand side, obtained after one
(left) and two (right) iterations of the CGLS method, using the whitening matrix
of the prior as priorconditioner.
and by deﬁning
w = R(x −x0),
(6.8)
we observe that
∥S(b −Ax −e0)∥2 + ∥R(x −x0)∥2 = ∥y −SAR−1w∥2 + ∥w∥2,
where
y = S(b −Ax0 −e0).
(6.9)
The derivation above suggests the following regularized algorithm for estimat-
ing the solution x:
1. Calculate the factorizations (6.7) of the noise and prior covariances, and
the vector (6.9).
2. Approximate the solution of the transformed system
SAR−1w = y,
R(x −x0) = w
using a suitable iterative solver.
We call the matrices S and R the left and right priorconditioners. Observe
that the left priorconditioner is related to the structure of the noise, while the
right priorconditioner carries information from the prior.
We emphasize that the priorconditioners are the whitening operators of X
and E, respectively. If, for simplicity, we assume that x0 = 0, e0 = 0, and let
W = RX,
Y = SE,
then

114
6 Statistically inspired preconditioners
E

WW T
= RE

XXT
RT = RΓRT.
Since
Γ = (RTR
−1 = R−1R−T,
it follows that
RΓRT = I,
showing that W is white noise. The same is true for Y .
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
2
2.5
3
True signal
Convolution kernel
Data
Fig. 6.4. The true signal, convolution kernel and the four data points. The error
bars indicate plus/minus two standard deviations of the noise in the data.
Example 6.2: In this example, we consider the deconvolution of a signal
from very sparse and noisy data. Suppose that we have a continuous signal
f supported on the unit interval [0, 1] with f(0) = 0, of which we know
four blurred and noisy samples,
g(sj) =
 1
0
a(sj −t)f(t)dt + ej,
1 ≤j ≤4.
The relatively narrow Gaussian convolution kernel is shown with the data
and signal that was used to generate it in Figure 6.4. The task is to estimate
the signal f from this data.
After discretizing the problem by subdividing the interval [0, 1] into n equal
subintervals and letting xk = f(tk), where tk is one of the division points,
we obtain the linear system
b = Ax + e,
where b ∈R4, bj = g(sj), represents the data. Here we choose n = 128,
leading to a badly underdetermined system.

6.1 Priorconditioners: specially chosen preconditioners
115
Since the matrix A ∈R4×128 is non-square, of the three iterative methods
discussed in Section 4, only the CGLS method can be applied. For compar-
ison, two versions, one with and one without a priorconditioner are tested.
Therefore, let us ﬁrst deﬁne a prior. In this example, the prior belief is
that the true signal is not oscillating excessively, and that f(0) = 0. A
stochastic model expressing our belief that the value at t = tk cannot be
very diﬀerent from the value at t = tk−1, can be expressed as
Xk = Xk−1 + Wk,
1 ≤k ≤n,
X0 = 0.
(6.10)
We model the uncertainties Wk by mutually independent zero mean Gaus-
sian random variables, whose standard deviations express how much we
believe that the adjacent values may diﬀer from one another. If we believe
that the range of the signal values are of the order one, a reasonable value
for the standard deviations is of the order 1/n. The equation (6.10) can
be written in the matrix form as
RX = W,
where the matrix R is the ﬁrst order ﬁnite diﬀerence matrix
R =
⎡
⎢⎢⎢⎣
1
−1
1
... ...
−1 1
⎤
⎥⎥⎥⎦∈Rn×n.
Assuming that the standard deviations of the variables Wk are all equal
to γ, we arrive at a ﬁrst order smoothness prior model,
πprior(x) ∝exp

−1
2γ2 ∥Rx∥2

,
Since the matrix R is invertible, it can be used as priorconditioner.
In Figure 6.5, we have plotted the ﬁrst four iterations of the CGLS algo-
rithm with and without priorconditioner. The eﬀect of the priorcondition-
ing is evident. The eigenvectors of the prior covariance matrix associated
with large eigenvalues correspond to signals with small jumps, i.e., Rx
is small. Therefore the priorconditioned solutions show fewer oscillations
than those obtained without priorconditioning.
In the light of the example above, it is worthwhile to make a small com-
ment concerning the usefulness of priorconditioning. In the context of Bayesian
models for linear Gaussian random variables, the Maximum A Posteriori es-
timate xMAP satisﬁes
xMAP = arg min
((((

A
δR

x −

b
0
(((( ,
δ = σ
γ ,

116
6 Statistically inspired preconditioners
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
True
k=1
k=2
k=3
k=4
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
True
k=1
k=2
k=3
k=4
Fig. 6.5.
The ﬁrst few CGLS iterations without the priorconditioner (left) and
with the preconditioner (right). Observe that, without the priorconditioner, the ap-
proximate solution after one iteration does not change anymore, thus the solutions
corresponding to diﬀerent iterations are indistinguishable. The priorconditioned ap-
proximate solutions seem to stabilize after three iterations.
which might lead us to argue that the correct Bayesian estimator is the least
squares solution of the linear system

A
δR

x =

b
0

,
(6.11)
which can be computed using an iterative solver. This argument is correct,
of course, with the caveat that while the priorconditioned CGLS does not re-
quire that the value of γ, or, equivalently, of δ, is known, the MAP estimate
does. If we do not know it, its estimation becomes part of the Bayesian inverse
problem and requires additional work. This is also the case if we interpret δ
as Tikhonov regularization parameter. Therefore we conclude that the poten-
tially diﬃcult step of selecting the value of the prior parameter is ﬁnessed by
priorconditioning, where it is traded with the selection of a truncation index.
Another worthwhile comment concerns the dimensions of the matrices
and the available selection of iterative solvers. Assuming that the matrix A is
square, the priorconditioned problem can be solved by the GMRES method,
which is, in many cases, more eﬃcient than the CGLS method. If, however,
instead of the priorconditioned problem we decide to solve the MAP estimate
from (6.11), the selection of applicable iterative solver is narrower.
So far, we have only discussed right preconditioning. It is interesting to
consider also the advantages of left preconditioning. For this purpose we return
to our two-dimensional problem.
Example 6.3: Consider a 2 × 2 linear Gaussian model,
B = Ax∗+ E,
E ∼N(0, Σ).
where the matrix A is as in Examples 4.5 and 4.6, but instead of being
Gaussian white, the additive noise has a covariance matrix diﬀerent from

6.1 Priorconditioners: specially chosen preconditioners
117
A
A−1
Fig. 6.6.
Two diﬀerent equiprobability ellipses of the data (right) and the corre-
sponding preimages in the mapping A (left).
unity. With additive Gaussian white noise, the equiprobability curves of
B are circles around the mean Ax∗because the variance of the noise is
the same in all directions. In this example, we assume that the covariance
matrix of the noise has a spectral factorization of the form
Σ = V diag(s2
1, s2
2)V T,
where V is a matrix whose columns are the eigenvectors of the matrix Σ
scaled to have unit length. Due to the symmetry of the covariance matrix,
the eigenvectors are orthogonal and we can write V in the form
V =

cos ϕ −sin ϕ
sin ϕ
cos ϕ

.
If we assume that s1 > s2, the equiprobability curves of B are no longer
circles but ellipses, with principal semi-axes of length s1 and s2 whose
orientations depend on the angle ϕ.
In the right panel of Figure 6.6, we have plotted two equiprobability el-
lipses, corresponding to s1 = 1, s2 = 0.2, and ϕ = π/4 in the ﬁrst (lighter
ellipse) and ϕ = 3π/4 in the second case (darker ellipse). The correspond-
ing preimages of these ellipses under the mapping A are shown in the left
panel of the same ﬁgure. We conclude that by changing the angle ϕ, which
does not change the noise level but only its covariance structure, the sensi-
tivity of the inverse problem to observation noise changes signiﬁcantly. In
fact, the (darker) preimage of the noise ellipse corresponding to ϕ = 3π/4
is remarkably more elongated than the one corresponding to ϕ = π/4,
indicating that in one direction the sensitivity to noise has been increased
signiﬁcantly. This example demonstrates that it is certainly not enough to
look at the noise level alone, as is customary in the non-statistical theory,
but that the covariance, and more generally the probability density of the
noise is also very important.

118
6 Statistically inspired preconditioners
A natural question which arises at this point is what all this has to do with
priorconditioning. In fact, the preimages of the noise ellipses are exactly the
preimages of a disc under the mapping S−1A. Hence, left preconditioning
with the whitener of the noise takes automatically the noise covariance into
account, and the classical reasoning using noise level only as a measure of
discrepancy becomes justiﬁed. In particular, this simple step justiﬁes the
use the discrepancy principle as a stopping criterion for priorconditioned
iterative methods, as if we had additive white noise in the model.
6.2 Sample-based preconditioners and PCA model
reduction
The discussion above suggests how to construct preconditioners when infor-
mation about the statistics of the solution and of the noise is available. In
many applications, however, instead of the statistics of the solution we have
access to a collection of typical solutions. A central question in statistical
modelling of inverse problems is how to construct informative and reasonable
prior densities. Here we discuss the problems of the estimation of prior densi-
ties from available samples and we warn about overly committal priors that
are biasing towards a reasonable but too conservative solution. In fact, while
the prior should favor the typical or normal solutions that we are expecting
to see, at the same time it should not exclude the presence of abnormalities or
anomalies that might in fact be, e.g., in medical imaging, what we are really
interested in.
The problem that we are considering here is how to estimate a quantity
x from observations b that are related via a computational model. Assume
that we have access to a sample believed to consist of realizations of the
random variable X representing the unknown of primary interest, as well as
to the corresponding set of data, believed to be realizations of the observable
random variable B. We call the sample of corresponding pairs a training set
and denote it by
S0 =

(x1, b1), (x2, b2), . . . , (xN, bN)

,
xj ∈Rn, bj ∈Rm,
with N ≥n. The noise level of the data may or may not be known. The set S0
may have been constructed from simulations or it may be the result of actual
experimental recordings. In medical applications, for example, the learning
set could consist of indirect, noninvasive measurements bj combined with in-
formation obtained during subsequent surgical interventions. We assume that
the vectors xj are discretized approximations of the quantities of interest, and
that the discretization level is ﬁxed and determined by the grid in which we
seek to solve the inverse problem.
If the sample is representative and N is suﬃciently large, it is possible
to estimate the probability density of the underlying variable X using this

6.2 Sample-based preconditioners and PCA model reduction
119
sample. The problem of estimating a probability density based on a sample
is the classical problem of frequentist statistics, so our approach has a non-
Bayesian ﬂavor.
Given a sample of true realizations, it may be unrealistic to assume that
they are normally distributed, and in fact, the validity of such assumption
may be tested. However, to construct priorconditioners we seek a Gaussian
approximation of the prior density. The Gaussian distributions are completely
characterized by the mean and the covariance. Estimates of the mean and the
covariance of X can be obtained from the available sample, as
x∗= E

X

≈1
N
N

j=1
xj = 
x∗,
Γ = E

XXT
−x∗xT
∗≈1
N
N

j=1
xjxT
j −
x∗
xT
∗= 
Γ.
(6.12)
Higher order moments can be estimated as well, and they can be used to
assess the ﬁdelity of the Gaussian approximation.
In the applications that we have in mind, the vectors xj represent typical
features of the random variable X. This often means that the vectors are
not very dissimilar. Consequently, the space spanned by the realizations may
be a proper subspace even if N ≥n, thus 
Γ may be rank deﬁcient or of
ill-determined rank2.
Assume ﬁrst that 
Γ is a satisfactory approximation of Γ. Without loss of
generality, we may assume that the mean of X vanishes. Let

Γ = V DV T,
V = [v1, v2, . . . , vn],
D = diag (d1, d2, . . . , dn),
(6.13)
be the singular value decomposition of the matrix 
Γ. Assume that only the
ﬁrst r singular values are larger that a preset tolerance, and set the remaining
ones to zero, i.e., d1 ≥d2 ≥. . . ≥dr > dr+1 = . . . = dn = 0 and partition V
accordingly,
V =
V0 V1

,
V0 = [v1, . . . , vr],
V1 = [vr+1, . . . , vn].
By using the orthogonality of V , we have
I = V V T =

V0 V1
 
V T
0
V T
1

= V0V T
0 + V1V T
1 ,
and therefore we may split X as
2 By ill-determined rank we mean that the eigenvalues, or more generally, singular
values, of the matrix go to zero, but there is no obvious cutoﬀlevel below which
the eigenvalues could be considered negligible. This type of ill-posedness is often
very diﬃcult to handle in numerical analyis.

120
6 Statistically inspired preconditioners
X = V0(V T
0 X) + V1(V T
1 X) = V0X0 + V1X1,
X0 ∈Rr, X1 ∈Rn−r.
To estimate the expected size of X1, we write
E

∥X1∥2
=
n

j=1
E

(X1)2
j

= trace

E

X1XT
1

,
and further, by expressing X1 in terms of X we have
E

X1XT
1

= V T
1 E

XXT
V1 = V T
1 ΓV1.
By approximating Γ ≈
Γ and substituting the right hand side of (6.13), it
follows that
ΓV1 = Γ

vr+1 vr+2 · · · vn

=
 Γvr+1 Γvr+2 · · · Γvn

=
 dr+1vr+1 dr+2vr+2 · · · dnvn

,
and, from the orthogonality of the vectors vj,
E

∥X1∥2
= trace
⎡
⎢⎣
dr+1
...
dn
⎤
⎥⎦=
n

j=r+1
dj = 0,
which is equivalent to saying that, within the validity of the approximation of
Γ by 
Γ, X = V0X0 with probability one. Therefore, we may write a reduced
model of the form
B = AX + E = AV0

=A0
X0 + E
(6.14)
= A0X0 + E,
A0 ∈Rm×r,
X0 ∈Rr,
whose model reduction error has zero probability of occurrence. The above
model is equivalent to the Principal Component Analysis (PCA) model3. Prin-
cipal Component Analysis is a popular and often eﬀective way of reducing the
dimensionality of a problem when it is believed that lots of the model param-
eters are redundant.
To better understand the properties of the reduced model, we introduce
the matrix
D0 = diag

d1, d2, . . . , dr

∈Rr×r,
and deﬁne the new random variable
W0 = D−1/2
0
X0 = D−1/2
0
V T
0 X ∈Rr,
3 See, e.g., [Jo02].

6.2 Sample-based preconditioners and PCA model reduction
121
where
D−1/2
0
=

D1/2
0
−1 = diag

d−1/2
1
, d−1/2
2
, . . . , d−1/2
r

.
The covariance matrix of the variable W0 is then
E

W0W T
0

= D−1/2
0
V T
0 ΓV0D−1/2
0
.
Since
V T
0 ΓV0 = D0,
it follows that
cov

W0

= Ir×r,
i.e., W0 is r–variate white noise. The whitened PCA model for X can then be
written as
B = AV0D1/2
0
W0 + E,
X = V0D1/2
0
W0,
(6.15)
where the matrix V0D1/2
0
∈Rn×r acts as a whitening preconditioner. Notice
that here we use the term preconditioner rather loosely, since the matrix is
not even square unless r = n.
This formulation of the PCA model emphasizes the fact that
X = V0D1/2
0
W0,
being a linear combination of the columns of the matrix V0, is automatically
in the subspace spanned by the singular vectors v1, . . . , vr. Hence, the PCA
model is a hard subspace constraint, since it forces the solution into a low
dimensional space.
The obvious appeal of the PCA reduced model (6.14) in either its original
or whitenend version (6.15), is that it takes full advantage of the prior infor-
mation. When r ≪n, the computational work may decrease substantially. It
should be pointed out, however, that the subspace spanned by V0 may contain
singular vectors of A that correspond to small singular values, thus the PCA
reduced model may still exhibit high sensitivity to noise.
From the point of view of applications, the PCA model reduction is not
void of problems. The ﬁrst question which arises is related to the approxi-
mation Γ ≈
Γ. In general, it is very diﬃcult to assess to what extent the
training set is a suﬃcient sample. If the vectors xj are drawn independently
from the same density, by the Central Limit Theorem the approximation
error of Γ is asymptotically Gaussian with variance decreasing as O(1/N).
However, the error associated with a given N can perturb signiﬁcantly the
singular values used to determine the truncation parameter r and, conse-
quently, the PCA subspace may be unable to represent essential features of
interest.
A more severe problem, certainly from the point of view of medical appli-
cations, is the PCA reduced model inability to reproduce outliers. Assume,
for instance, that the training set represents thoracic intersection images. The

122
6 Statistically inspired preconditioners
majority of the images corresponds to normal thoraxes, while the few outliers
representing anomalies, tumors, for example, that might be in the set have a
negligible eﬀect in the averaging process. As a consequence, the anomalous fea-
tures will not be represented by the PCA subspace vectors, when the purpose
of the imaging process might have been, in fact, to detect these anomalies. So
we might have a perfect method of reproducing something that we know, but
the essential information that we are looking for is discarded! One possible
way to overcome this problem is discussed in the following example.
Example 6.4: Assume that we believe a priori that the unknown consists
of a regular part and an anomalous part that may or may not be present.
We write a stochastic model,
X = Xr + V Xa,
(6.16)
where Xr and Xa are the anomalous and regular parts, respectively, and
V is a random variable that takes on values zero or one. We assume that
Xr and Xa are mutually independent. Furthermore, assume that
E{Xr} = xr,
E{Xa} = xa,
and
Cov(Xr) = Γr,
Cov(Xa) = Γa.
The probability of the occurrence of the anomaly is assumed to be α, so
P{V = 1} = α,
P{V = 0} = 1 −α.
The mean and the covariance matrices may have been estimated based on
an empirical sample. The role of the anomalous part in the model is to
avoid a too committal prior that leaves out interesting details from the
estimate.
To apply the priorconditioning technique, we calculate a Gaussian approx-
imation for the prior density of X. Therefore, the mean and the covariance
need to be calculated. We use the discrete version of the formula (1.4) and
write
E{X} = E{X | V = 1}P{V = 1} + E{X | V = 0}P{V = 0}
= (xr + xa)α + xr(1 −α) = xr + αxa = x.
Similarly, we calculate the covariance. In the calculation below, we assume
for simplicity that xa = 0, leading to
Cov(X) = E{(X −xr)(X −xr)T}
= E{(X −xr)(X −xr)T | V = 1}P{V = 1}
+ E{(X −xr)(X −xr)T | V = 0}P{V = 0}

6.2 Sample-based preconditioners and PCA model reduction
123
= αE{(Xr + Xa −xr)(Xr + Xa −xr)T}
+ (1 −α)E{(Xr −xr)(Xr −xr)T}
= α(Γr + Γa) + (1 −α)Γr
= Γr + αΓa.
It is left as an exercise to show that if the mean of the anomalous part is
non-vanishing, the covariance is
Cov(X) = Γr + αΓa + α(1 −α)xaxT
a .
(6.17)
Consider now a deconvolution example where we estimate f : [0, 1] →R
from
g(sj) =
 1
0
a(sk −t)f(t)dt + ej,
0 ≤sj ≤1,
where sj = (j −1)/(n −1), 1 ≤j ≤n and n = 200. The kernel a is
Gaussian and the noise vector is zero mean, normally distributed white
noise. We discretize the problem using grid points that coincide with the
data points sj, leading to the linear model
b = Ax + e.
We assume a priori that the true signal f, deﬁned over the unit interval
[0, 1] is under regular conditions a boxcar function, and we have an interval
estimate for both its height and location,
fr(t) = hχ[a,b](t),
where χ[a,b] is a characteristic function of the interval [a, b], and
1 ≤h ≤1.2,
0.2 ≤a ≤0.25,
0.65 ≤b ≤0.7.
To produce the regular part of the mean and covariance matrix, we gener-
ate a sample of 20 000 vectors x that are discretizations of boxcar functions
fr above,
h ∼Uniform([1, 1.2]), a ∼Uniform([0.2, 0.25]), b ∼Uniform([0.65, 0.7]).
We then assume that the regular part may be contaminated by a rare
anomaly of small support, and this anomaly is likely to appear in the
non-vanishing part of fr. Therefore, we write a model for the anomaly,
fa(t) = Cexp

−1
2γ2 (t −t0)2

,
and to generate a sample of anomalies, we assume probability distributions

124
6 Statistically inspired preconditioners
C ∼Uniform([0.1, 0.5]), t0 ∼Uniform([0.4, 0.5]), γ ∼Uniform([0.005, 0.03]).
Based on a sample of 20 000 vectors of discretizations of Gaussians with
parameters drawn from these distribution, we calculate the mean and co-
variance of the anomalous part.
We now compute the eigenvalue decomposition of the covariance of the
regular part,
Γr = UrDrU T
r .
The matrix Γr is of ill-determined rank. We discard the eigenvalues that
are below a threshold τ = 10−10 and replace them by zeros. Let 
Dr denote
the thresholded diagonal matrix. We then have
Γr ≈Ur 
DrU T
r = (Ur 
D1/2
r
)



=Rr
( 
D1/2
r
U T
r ) = RrRT
r .
Similarly, we deﬁne the covariance matrix Γ deﬁned by formula (6.17),
with α = 0.5, and write the decomposition
Γ = RRT.
Observe that if W is a white noise vector, then the random variable deﬁned
by
X = x + RW
is normally distributed with mean x and covariance Γ. This leads us to
consider the priorconditioned problem of solving the equation
b −Ax = ARw,
x = x + Rw,
and we may apply iterative solvers with the discrepancy principle as a
stopping criterion.
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
Non−anomalous
Anomalous
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
Non−anomalous
Anomalous
Fig. 6.7. The true signals used for data generation (left) and the noisy data. The
signals are oﬀset slightly to improve the visualization.

6.2 Sample-based preconditioners and PCA model reduction
125
To test the idea, we generate two signals, one with and one without the
anomaly, and calculate the corresponding noisy data. The signals and the
data are shown in Figure 6.7. The additive noise has a standard deviation
0.5% of the maximum value of the noiseless signal that would correspond
to the non-anomalous mean xr.
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
t
Iteration
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
−1
0
1
2
3
t
Iteration
Fig. 6.8.
The evolution of the CGLS iterations with the priorconditioner based
on the learning sample with no anomalies and with the data coming from a non-
anomalous source (left) and from a source with anomaly (right). The true signals
are plotted on the foreground.
We test the priorconditioned CGLS algorithm in four diﬀerent cases. First,
we use only the non-anomalous part of the covariance matrix and test
the algorithm both with the non-anomalous and the anomalous data. The
results are shown in Figure 6.8. Evidently, the performance of the algorithm
is good when the data comes from a non-anomalous source, while there
is a strong edge eﬀect visible when the data comes from the anomalous
signal.
We than add the anomalous part to the covariance and run again the al-
gorithm both with the anomalous and non-anomalous data. The results
can be seen in Figure 6.9. Evidently, the addition of the anomalous part
improves the performance with the anomalous data, but as importantly,
it does not produce signiﬁcant “false positive” artifact in the solution cor-
responding to the non-anomalous data.
In the previous example, the parameter α represents the probability of oc-
currence of an anomaly which may be a rare event, e.g., the occurrence of a
tumor in a radiograph, so one might think that it should be chosen very small,
unlike in the example, and the eﬀect of the anomalous part must therefore
become negligible. However, we must bear in mind that the prior is based on
subjective judgment. If a patient is sent for an X–ray, it is likely that there
is already a suspicion of something abnormal in the radiograph. Hence, the
prior density is a conditional density, and the expectation of the anomaly may
well be taken rather high.

126
6 Statistically inspired preconditioners
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
t
Iteration
0
1
2
3
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
t
Iteration
Fig. 6.9. The priorconditioner contains an anomalous part, the data coming from
an anomalous source (left) and from a non-anomalous one (right). The true signals
are plotted on the foreground.
Exercises
1. Given a n–variate random variable X with mean x ∈Rn, show that
E{(X −a)(X −a)T} = Cov(X) + (x −a)(x −a)T,
where a ∈Rn is any vector. This is the oﬀ-centered covariance of the
variable X.
2. Consider the Example 6.4 when the anomalous part has a non-vanishing
mean xa. Using the result of Exercise 1, show that in this case, the variance
of X is given by the formula (6.17).

7
Conditional Gaussian densities and predictive
envelopes
From the computational point of view, a fundamental problem in Bayesian
statistics is how to move between the marginal, conditional and joint prob-
ability distributions. To demonstrate various ways of doing this, we discuss
some classical problems from the statistical point of view, and show that not
only is the Bayesian analysis able to give the classical solutions, but it also
provides means of assessing how conﬁdent we should feel about them, in the
light of the data and our prior belief. In many applications, the latter aspect
is important enough to provide a reason to switch to the Bayesian framework.
In the context of linear inverse problems with additive Gaussian noise
and Gaussian prior, we observed that the joint probability density and the
posterior density are also Gaussian. In the derivation of the posterior density
we assumed that the noise and the unknown to be estimated were mutually
independent. This assumption is fairly common, and it can often be justiﬁed,
for example, when the source of the noise is exogenous. There are important
cases, however, when this assumption does not hold, and the concept of noise
should be deﬁned in more general terms. The following examples are meant
to clarify what we mean by this.
Example 7.1: Assume that we are recording electric or magnetic ﬁelds
outside a person’s skull, trying to infer on the cortical activity. The subject
is shown images and the resulting electromagnetic activity on the visual
cortex is the unknown of primary interest. Inevitably, when the images are
shown, the subject moves the eyes, thereby causing activity on the motor
cortex. The muscles that control the eye movements also produce a weak
electromagnetic signal. Since these events are evoked by the showing of
the images, we cannot assume that the highly correlated signals from the
motor cortex and from the muscles movements are independent noise, yet
it is natural to treat them as noise over which to marginalize the posterior
probability density.
In some cases we would also like to treat deﬁciencies in the computational
model as noise, as the following example indicates.

128
7 Conditional Gaussian densities and predictive envelopes
Example 7.2: Consider a discrete linear model,
Y = AX + E,
where X ∈Rn and E ∈Rm are mutually independent Gaussian random
variables. Assume that, for some reason, we decide to reduce the model
by using the Principal Component Analysis introduced in the previous
chapter. We then write
X = V0 (V T
0 X)
  
=X0
+V1 (V T
1 X)
  
=X1
= V0X0 + V1X1,
where
V =
V0 V1

,
V0 ∈Rn×k, V1 ∈Rn×(n−k)
is a partitioning of an orthogonal matrix V ∈Rn×n, and we obtain the
reduced model
Y = AV0

=A0
X0 + AV1X1 + E



= 
E
= A0X0 + 
E.
In the traditional PCA model reduction, the columns of the orthogonal
matrix V are the eigenvectors of the prior covariance matrix. In that case,
X0 and X1 are independent and, consequently, X0 and 
E are independent.
If, however, the matrix V is a sample based estimate of the covariance ma-
trix, the columns of V may not be the eigenvectors of the prior covariance,
and the independence is no longer guaranteed.
The point of these two examples is that the likelihood density is not al-
ways obtained from the observation model and the noise density alone, but
sometimes the prior density needs to be taken into account as well1.
The central question of this chapter can be stated as follows: assuming that
X is a multivariate Gaussian random variable, and some of its components are
ﬁxed, what is the distribution of the remaining components? Answering this
question will help constructing the likelihood in situations like those described
in the examples above, as well as provide other means to analyze posterior
densities.
7.1 Gaussian conditional densities
Let X ∈Rn be a Gaussian random variable, and assume for simplicity that its
ﬁrst k components are unknown, 0 < k < n, while the remaining ones are ﬁxed
to known values. To write the density of the k ﬁrst components conditioned
1 This is yet another instance where the claim that Bayesian statistics equals fre-
quentist statistics plus prior is simply incorrect.

7.1 Gaussian conditional densities
129
on the knowledge of the last n −k ones, we start with a partitioning of X of
the form
X =
 X1
X2
 ∈Rk
∈Rn−k ,
and we express the probability density of X as the joint density of X1 and
X2,
π(x) = π(x1, x2).
The probability density of x1, under the condition that x2 is known, is given
by
π(x1 | x2) ∝π(x1, x2),
x2 = x2,known.
Observe that if we are investigating a Gaussian linear model with additive
noise, and the model is X1 = AX2 + E, the conditional density above gives
the likelihood. If instead we write X2 = AX1 + E, the conditional density is
the posterior density.
Example 7.3: Let us consider the simplest possible case where n = 2,
and assume that the real valued random variables X and Y are zero mean
Gaussian variables with symmetric positive deﬁnite joint covariance matrix
Γ ∈R2×2,
Γ =
a b
b c

,
(7.1)
where
a = E

X2
,
b = E

XY

,
c = E

Y 2
.
The joint probability distribution is then
π(x, y) ∝exp

−1
2

x y

Γ −1
x
y

.
We want to ﬁnd the explicit form of the conditional density π(x | y) in
terms of the components of Γ. Before expressing Γ −1 in terms of the
components of Γ, let us write
Γ −1 = B =
p r
r q

,
where the symmetry of B follows from the symmetry of Γ. With this
notation, the exponent of the joint density becomes

x y

Γ −1
x
y

= px2 + 2rxy + qy2,
and after completing the square,
px2 + 2rxy + qy2 = p

x + ry
p
2
+

q −r2
p

y2,

130
7 Conditional Gaussian densities and predictive envelopes
we have
π(x | y) ∝exp

−1
2(px2 + 2rxy + qy2)

= exp

−1
2p

x + ry
p
2
−
1
2

q −r2
p

y2



independent of x

∝exp
"
−1
2p

x + ry
p
2#
,
since the term independent of x contributes only in the form of a normal-
izing constant. We conclude that the probability density of x conditioned
on y is a Gaussian density with mean and variance
¯x = −ry
p ,
σ2 = 1
p,
respectively. We are now ready to take a second look at Γ −1, since, after
all, we may want to express the quantities p, q and r in terms of the
components of Γ.
Observe that since Γ is positive deﬁnite, that is,
vTΓv > 0
for all v ̸= 0,
by choosing v = e1 =

1
0
T or v = e2 =

0
1
T, we have
a = eT
1 Γe1 > 0,
c = eT
2 Γe2 > 0.
Let us now return to Γ −1. It is elementary to verify that
Γ −1 =
1
ac −b2

c −b
−b
a

,
where ac −b2 = det(Γ) ̸= 0. Expressing the determinant of Γ in the form
ac −b2 = a

c −b2
a




=
a
= c

a −b2
c




=
c
,
we notice that, if

a = c −b2
a ̸= 0,

c = a −b2
c ̸= 0,
we can also express it in the form

7.1 Gaussian conditional densities
131
det(Γ) = a
a = c
c.
We now write the inverse of Γ in terms of these newly deﬁned quantities.
Since
a
det(Γ) = 1

a,
c
det(Γ) = 1

c ,
and
b
det(Γ) = b
a
a = b
c
c,
we have that
Γ −1 =

1/
a
−b/(a
a)
−b/(c
c)
1/
c

=

p r
r q

.
(7.2)
Note that from (7.2) we have two alternative expressions for r,
r = −b
a
a = −b
c
c.
Clearly the key players in calculating the inverse of Γ are the quantities 
a
and 
c, which are called the Schur complements of a and c, respectively. In
terms of the Schur complements, the conditional density can be written as
π(x | y) ∝exp

−1
2σ2 (x −¯x)2

,
with mean
¯x = −r
py = b

aa
ay = b
ay,
and variance
var(x) = σ2 = 1
p = 
a,
respectively.
We are now ready to consider the general multivariate case, trying to
mimic what we did in the simple case. Assuming that x ∈Rn is a zero
mean Gaussian random variable with covariance matrix Γ ∈Rn×n, the joint
probability density of x1 ∈Rk and x2 ∈Rn−k is
π(x1, x2) ∝exp

−1
2xTΓ −1x

.
(7.3)
To investigate how this expression depends on x1 when x2 is ﬁxed, we start
by partitioning the covariance matrix
Γ =
Γ11 Γ12
Γ21 Γ22

∈Rn×n,
(7.4)
where

132
7 Conditional Gaussian densities and predictive envelopes
Γ11 ∈Rk×k,
Γ22 ∈R(n−k)×(n−k),
k < n,
and
Γ12 = Γ T
21 ∈Rk×(n−k).
Proceeding as in the two dimensional example, we denote Γ −1 by B, and
we partition it according to the partition of Γ,
Γ −1 = B =

B11 B12
B21 B22

∈Rn×n.
(7.5)
We then write the quadratic form appearing in the exponential of (7.3) as
xTBx = xT
1 B11x1 + 2xT
1 B12x2 + xT
2 B22x2
(7.6)
=

x1 + B−1
11 B12x2
TB11

x1 + B−1
11 B12x2

+ xT
2

B22 −B21B−1
11 B12

x2



independent of x1
.
This is the key equation when considering conditional densities. Observing
that the term in (7.6) that does not depend on x1 contributes only to a
multiplicative constant, we have
π(x1 | x2) ∝exp

−1
2

x1 + B−1
11 B12x2
TB11

x1 + B−1
11 B12x2

.
We therefore conclude that the conditional density is Gaussian,
π(x1 | x2) ∝exp

−1
2(x1 −¯x1)TC−1(x1 −¯x1)

,
with mean
¯x1 = −B−1
11 B12x2
and covariance matrix
C = B−1
11 .
To express these quantities in terms of the covariance matrix Γ, we need to
introduce multivariate Schur complements. For this purpose we now derive a
formula similar to (7.2) for general n×n matrices. Let us consider a symmetric
positive deﬁnite matrix Γ ∈Rn×n, partitioned according to (7.5).
Since Γ is positive deﬁnite, both Γ11 and Γ22 are also. In fact, for any
x1 ∈Rk, x1 ̸= 0,
xT
1 Γ11x1 =

xT
1 0
 
Γ11 Γ12
Γ21 Γ22
 
x1
0

> 0,
showing the positive deﬁniteness of Γ11. The proof that Γ22 is positive deﬁnite
is analogous.
To compute Γ −1 using the block partitioning, we solve the linear system

7.1 Gaussian conditional densities
133
Γx = y
in block form, that is, we partition x and y as
x =
x1
x2
 ∈Rk
∈Rn−k ,
y =
y1
y2
 ∈Rk
∈Rn−k ,
and write
Γ11x1 + Γ12x2 = y1,
(7.7)
Γ21x1 + Γ22x2 = y2.
(7.8)
Solving the second equation for x2, which can be done because Γ22, being
positive deﬁnite, is also invertible, we have
x2 = Γ −1
22

y2 −Γ21x1

,
and substituting it into the ﬁrst equation, after rearranging the terms, yields

Γ11 −Γ12Γ −1
22 Γ21

x1 = y1 −Γ12Γ −1
22 y2.
We deﬁne the Schur complement of Γ22 to be

Γ22 = Γ11 −Γ12Γ −1
22 Γ21.
With this notation,
x1 = 
Γ −1
22 y1 −
Γ −1
22 Γ12Γ −1
22 y2.
(7.9)
Similarly, by solving (7.7) for x1 ﬁrst and plugging it into (7.8), we may express
x2 as
x2 = 
Γ −1
11 y2 −
Γ −1
11 Γ21Γ −1
11 y1
(7.10)
in terms of the Schur complement of Γ11,

Γ11 = Γ22 −Γ21Γ −1
11 Γ12.
Collecting (7.9) and (7.10) into

x1
x2

=
$

Γ −1
22
−
Γ −1
22 Γ12Γ −1
22
−
Γ −1
11 Γ21Γ −1
11

Γ −1
11
% 
y1
y2

,
we deduce that
Γ −1 =
$

Γ −1
22
−
Γ −1
22 Γ12Γ −1
22
−
Γ −1
11 Γ21Γ −1
11

Γ −1
11
%
=

B11 B12
B21 B22

,
which is the formula that we were looking for.

134
7 Conditional Gaussian densities and predictive envelopes
In summary, we have derived the following result: the conditional density
π(x1 | x2) is a Gaussian probability distribution whose conditional mean (CM)
and conditional covariance are
¯x1 = −B−1
11 B12x2 = Γ12Γ −1
22 x2,
and
C = B−1
11 = 
Γ22,
respectively.
Now we present some examples of how to use of this result.
7.2 Interpolation, splines and conditional densities
As an application of the computation of conditional densities for Gaussian
distributions, and also as an introduction to other topics that will be discussed
later, consider the following classical interpolation problem:
Find a smooth function f deﬁned on an interval [0, T], that satisﬁes the
constraints
f(tj) = bj ± ej,
1 ≤j ≤m,
where 0 = t1 < t2 < · · · < tm = T. We refer to the bj as the data and to the
ej as error bounds2.
Since in the statement of the problem no exact deﬁnition of smoothness
or of the error bounds are given, we will interpret them later subjectively.
A classical solution to this problem uses the cubic splines, i.e., curves that
between the interpolation points are third order polynomials, glued together
at the observation points so that the resulting piecewise deﬁned function and
its derivatives, up to the second order, are continuous. The Bayesian solution
that we are interested in is based on conditioning and, it turns out, can be
orchestrated so that the realization with highest posterior probability corre-
sponds to a spline.
As a prelude, consider a simpliﬁed version of the problem, which assumes
that only the values at the endpoints are given, and that they are known
exactly,
f(0) = b1,
f(T) = b2.
(7.11)
An obvious solution to this interpolation problem is a linear function passing
through the data points. In the following construction, this trivial solution
will be shown to be also the most probable realization with respect to the
prior that we introduce. As it turns out, in this simpliﬁed case the use of
Schur complements can be avoided, while they become useful when the data
is collected also in interior points.
2 This expression is borrowed from the engineering literature, where it is common
to refer to measurements as mean ± standard deviation to mean that an error
with the indicated standard deviation is expected in the measured data.

7.2 Interpolation, splines and conditional densities
135
We start by discretizing the problem: divide the support of the function
into n equal subintervals, and let
xj = f(sj),
sj = jh,
h = T
n ,
0 ≤j ≤n.
The smoothness of the solution is favored by imposing a prior condition that
the values of the function at interior points are not very diﬀerent from the
average of the adjacent values, that is,
xj ≈1
2(xj−1 + xj+1),
1 ≤j ≤n −1.
Similarly to the construction in Example 3.8, we write a stochastic Gaussian
prior model for the random variable X ∈Rn of the form
Xj = 1
2(Xj−1 + Xj+1) + Wj,
1 ≤j ≤n −1,
and the innovation vector W ∈Rn−1 with components Wj is modelled as a
multivariate Gaussian white noise process,
W ∼N(0, γ2I).
The variance γ2 is a measure of how much we believe in the averaging model.
Notice that forcing the mean value assumption with no innovation term would
give the linear interpolation solution.
In vector form, the prior model can be written as
LX = W,
where L is the second order ﬁnite diﬀerence matrix,
L = 1
2
⎡
⎢⎢⎢⎣
−1
2 −1
−1
2 −1
... ... ...
−1
2 −1
⎤
⎥⎥⎥⎦∈R(n−1)×(n+1).
(7.12)
To discuss the conditional density, let us partition the entries of x in the
two vectors
x′ =
⎡
⎢⎢⎢⎣
x1
x2
...
xn−1
⎤
⎥⎥⎥⎦∈Rn−1,
x′′ =
 x0
xn

∈R2.
The probability density corresponding to the prior model above is then
π(x′ | x′′) ∝π(x′, x′′) = π(x) ∝exp

−1
2γ2 ∥Lx∥2

,

136
7 Conditional Gaussian densities and predictive envelopes
where
x′′ = x′′
known =

b1
b2

= b.
Thus, the density deﬁnes the probability distribution of the interior points
assuming that the boundary values are given, and since its maximum occurs
at the solution of the equation Lx = 0, implying that x is a discretization of
a linear function, it favors solutions with small oscillations.
We now apply the conditioning formulas and write the conditional density
explicitly in terms of x′. Assume that by permuting the entries of x, the two
observed values are the last two components of x, write x in the form
x =
 x′
x′′

.
and partition the matrix L accordingly, after having permuted the columns
to follow the ordering of the components of x,
L =
L1 L2

,
L1 ∈R(n−1)×(n−1),
L2 ∈R(n−1)×2.
It can be checked that the matrix L1 is invertible. Since
Lx = L1x′ + L2x′′ = L1(x′ + L−1
1 L2x′′),
it follows that the conditional probability density
π(x′ | x′′) ∝exp

−1
2γ2 ∥L1(x′ + L−1
1 L2x′′)∥2

= exp

−1
2(x′ + L−1
1 L2x′′)T
 1
γ2 LT
1 L1

(x′ + L−1
1 L2x′′)

,
is Gaussian,
π(x′ | x′′ = b) ∼N(x′, Γ ′),
with mean and covariance
x′ = −L−1
1 L2b,
Γ ′ = γ2(LT
1 L1)−1,
respectively.
Before considering the case where we have more data points, assume that,
instead of the exact data (7.11), we know the data up to an additive error.
We interpret the error bar condition in the problem setup as stating that the
additive error is Gaussian
X0 = b1 + E1,
Xn = b2 + E2,
Ej ∼N(0, σ2
j ),
which implies that the marginal density of X′′ = [X1 Xn]T is

7.2 Interpolation, splines and conditional densities
137
π(x′′) ∝exp

−1
2(x′′ −b)TC−1(x′′ −b)

,
where
C =

σ2
1
σ2
2

.
Since now the determination of both x′ and x′′ from the data is of concern –
x′′ is no longer known exactly – we write the joint probability density as
π(x) = π(x′, x′′) = π(x′ | x′′)π(x′′),
or, explicitly,
π(x) ∝exp

−1
2γ2 ∥L1(x′ + L−1
1 L2x′′)∥2 −1
2(x′′ −b)TC−1(x′′ −b)

.
To better understand the signiﬁcance of this formula, some matrix manipu-
lations are required. We simplify the notations by assuming that γ = 1, a
condition which is easy to satisfy simply by scaling L by 1/γ.
Let’s begin by writing
∥L1(x′ + L−1
1 L2x′′)∥2 = ∥Lx∥2 =
((((
L1 L2
  x′
x′′
((((
2
,
and factorizing the matrix C−1 as
C−1 = KTK,
K =
1/σ1
1/σ2

,
so that
(x′′ −b)TC−1(x′′ −b) = ∥K(x′′ −b)∥2
=
((((

0 K
  x′
x′′

−Kb
((((
2
.
We can now write the probability density of X as
π(x) ∝exp
"
−1
2
((((

L1 L2
  x′
x′′
((((
2
−1
2
((((

0 K
  x′
x′′

−Kb
((((
2#
= exp
"
−1
2
((((

L1 L2
0
K
 
x′
x′′

−

0
Kb
((((
2#
= exp

−1
2∥
L(x −x)∥2

,
where

138
7 Conditional Gaussian densities and predictive envelopes

L =
 L1 L2
0
K

,
and
x = 
L−1

0
Kb

.
(7.13)
We therefore conclude that the probability density of X is Gaussian
π(x) ∼N(x, Γ),
with the mean x from (7.13) and covariance
Γ = (
LT
L)−1.
We return now to the original interpolation problem and add data points
inside the interval [0, T]. For simplicity, let us assume that the data points tj
coincide with discretization points sk so that, in the case of exact data,
xkj = f(tj) = bj,
1 ≤j ≤m.
(7.14)
Partition x as
x =

x′
x′′

,
x′′ =
⎡
⎢⎣
xk1
...
xkm
⎤
⎥⎦=
⎡
⎢⎣
b1
...
bm
⎤
⎥⎦,
permuting the elements of x so that the components corresponding to the
data are the last ones.
Among the several possible ways to modify the procedure discussed above
so that it can be applied to more general problems, we choose one which
extends easily to higher dimensions. Let L ∈R(n−1)×(n+1) be the matrix
(7.12). After permuting the columns of L to correspond to the ordering of the
elements of x, partition L according to the partition of x,
L =

L1 L2

,
L1 ∈R(n−1)×(n−m+1),
L2 ∈R(n−1)×m.
Observe that since L1 is no longer a square matrix, it is obviously not invertible
and we cannot proceed as in the simple case when the data was given at the
endpoints only. Therefore let
B = LTL =
B11 B12
B21 B22

,
where
B11 = LT
1 L1 ∈R(n−m+1)×(n−m+1),
B22 = LT
2 L2 ∈Rm×m,
and
B12 = LT
1 L2 = BT
21 ∈R(n−m+1)×m.

7.2 Interpolation, splines and conditional densities
139
Similarly as in the previous section, the conditional density can be written in
the form
π(x′ | x′′ = b) ∝exp

−1
2(x′ + B−1
11 B12b)TB11(x′ + B−1
11 B12b)

.
If the observations contain additive Gaussian error,
Xkj = bj + Ej,
Ej ∼N(0, σ2
j ),
we introduce the covariance matrix
C =
⎡
⎢⎣
σ2
1
...
σ2
m
⎤
⎥⎦,
and write
π(x′′) ∝exp

−1
2(x′′ −b)TC−1(x′′ −b)

,
leading to a joint probability density of the form
π(x) = π(x′ | x′′)π(x′′)
∝exp

−1
2(x′ + B−1
11 B12x′′)TB11(x′ + B−1
11 B12x′′)
−1
2(x′′ −b)TC−1(x′′ −b)

.
To extract the covariance and mean from this expression, the terms need
to be rearranged. After a straightforward but tedious exercise in completing
the square, we ﬁnd that
π(x) ∝exp
1
2(x −x)TΓ −1(x −x)

,
where
Γ =
B11
B12
B21 B21B−1
11 B12 + C−1
−1
,
(7.15)
and
x = Γ

0
b

.
(7.16)
The mean x is a good candidate for a single solution curve to the interpolation
problem.
Consider now a computed example where the formulas derived above are
applied.

140
7 Conditional Gaussian densities and predictive envelopes
Example 7.4: The support of the function is a unit interval, i.e., T = 1,
and the number of data points is m = 6. We divide the interval into
n = 60 equal subintervals. To ﬁx the parameter γ, recall that it conveys
our uncertainty of how well the function obeys the mean value rule. Thus,
if we assume that a typical increment |xj −xj−1| over a subinterval is of
order 1/n, we may be conﬁdent that the prior is not too committal if γ is
of the same order of magnitude. Therefore in our computed example, we
set γ = 1/n.
0
0.2
0.4
0.6
0.8
1
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
−1.5
−1
−0.5
0
0.5
1
1.5
2
Fig. 7.1. Mean and credibility envelopes of two standard deviations with noiseless
data (left) and with noisy data (right), the error bars indicating an uncertainty of
two standard deviations.
Given the values bj, we calculate the mean and covariance of x in the two
cases when the values are exactly known, and where they are normally
distributed with variance σ2
j .
In Figure 7.1, we have plotted the solution of the interpolation problem
when the data is exact in the left panel, and normally distributed in the
right panel, where the error bars of two standard deviations are also in-
dicated. In both cases, we have plotted the mean curve and a pointwise
predictive output envelope corresponding to two standard deviations, which
is the shaded area around the mean curve is bounded by the curves
sj →xj ± 2Γ 1/2
jj ,
with the understanding that in the case of exact data, at data points xj
coincides with the data and the variance vanishes. Comparing the two plots
in Figure 7.1, we notice that the mean curves with exact and noisy data are
similar while the predictive envelopes are rather diﬀerent. Clearly, when
forcing the solutions to pass through the data points, the uncertainty in
the intervals between data points increases signiﬁcantly.
The importance of taking into consideration the information carried by
the predictive envelopes become even more clear when we consider a realis-
tic interpolation problem. A typical example is the problem of interpolating

7.2 Interpolation, splines and conditional densities
141
geographic temperature distributions. Starting from a sparse network of ob-
servation stations where the temperature is measured, we interpolate the
temperature in the surrounding domain. The conditioning approach de-
rived above in the one dimensional case extends in a natural way to higher
dimensions.
Example 7.5: Consider the interpolation problem in two dimensions
where we want to ﬁnd a smooth surface with a square support that passes
through a given set of points in R3. Assume that the domain is discretized
into N = n × n pixels, and X ∈RN is a random vector representing the
surface as a pixelized image. We assume that some of the pixel values are
ﬁxed,
Xj = bj,
j ∈I2 ⊂I =

1, 2, . . . , N

,
I1 = I \ I2,
and we want to ﬁnd the conditional distribution of the vector of the re-
maining pixels,
X′′ =

Xj

j∈I1,
conditioned on
X′ =

Xj

j∈I2 = b.
We assume a priori that X is smooth and nearly vanishing at the bound-
aries, encoding this belief in a smoothness prior
π(x) ∝exp

−1
2γ2 ∥Lx∥2

,
where L is the second order ﬁnite diﬀerence operator constructed in Ex-
ample 3.8, corresponding to the mask
⎡
⎣
−1/4
−1/4
1
−1/4
−1/4
⎤
⎦.
This prior model conveys the idea that each pixel value is the mean of the
four neighboring pixels plus a random white noise innovation, the surface
values being extended by zeros outside the image view.
Instead of applying the formulas that have been derived in the one dimen-
sional case, let us look directly the Matlab code used to solve the problem,
assuming that the matrix L has already been computed.
B = L’*L;
% Conditioning: I2 defines the points that are known,
% I1 points to free pixels
I = reshape([1:n^2],n,n);
I2 = [I(15,15) I(15,25) I(15,40) I(35,25)];

142
7 Conditional Gaussian densities and predictive envelopes
x2 = [15;-20;10;10];
I1 = setdiff(I(:),I2);
% Conditional covariance of x1
B11 = B(I1,I1);
B12 = B(I1,I2);
% Calculating and plotting the conditional mean value
% surface
x1mean = -B11\(B12*x2);
xplot = zeros(n^2,1);
xplot(I1) = x1mean;
xplot(I2) = x2;
figure(1)
surfl(reshape(xplot,n,n)), shading flat, colormap(gray)
% Generate a few random draws from the distribution
R = chol(B11);
% Whitening matrix
ndraws = 2;
for j = 1:ndraws
xdraw1 = x1mean + R\randn(length(I1),1);
xdraw = zeros(n^2,1);
xdraw(I1) = xdraw1;
xdraw(I2) = x2;
figure(1+j)
imagesc(flipud(reshape(xdraw,n,n))), shading flat
axis(’square’), colormap(gray)
end
% Plotting the standard deviation surface
STDs = zeros(length(I1),1);
for j = 1:length(I1)
ej = zeros(length(I1),1); ej(j)=1;
STDs(j) = norm(R’\ej);
end
STDsurf = zeros(n^2,1);
STDsurf(I1) = STDs;
figure(ndraws + 2)

7.2 Interpolation, splines and conditional densities
143
surfl(reshape(varsurf,n,n))
shading flat, colormap(gray)
0
20
40
60
0
10
20
30
40
50
−30
−20
−10
0
10
20
30
0
20
40
60
0
10
20
30
40
50
0
2
4
6
8
10
12
Fig. 7.2. The conditional mean surface (left) with the ﬁxed values marked by dots.
The surface plotted in the right panel shows the square root of the diagonal entries
of the conditional covariance matrix, which gives the standard deviation of each
pixel. Notice that the standard deviation of the ﬁxed pixels is, of course, zero. Also
notice the eﬀect of continuation by zero beyond the boundaries.
Fig. 7.3.
Two random draws from the conditional density. The data points are
marked by dots.
The mean and variance surfaces are shown in Figure 7.2, and two random
draws are displayed in Figure 7.3. The standard deviation surface plot gives
an indication of how much credibility the predictions of the temperature

144
7 Conditional Gaussian densities and predictive envelopes
between the observation points have, based on the data and the prior
model. In Figure 7.3, two random draws from the conditional density are
displayed as gray scale images.
7.3 Envelopes, white swans and dark matter
In the previous section, we pointed out that the Bayesian analysis of the in-
terpolation problem gave not only a probable solution, but also credibility
envelopes around it. These envelopes are particularly important when the
models are used to make predictions about the behavior of quantities that
cannot be observed. An isolated predicted value, curve or surface can be use-
less, or even misleading unless we have some way of assessing how much it
should be trusted3. In the history of medicine, for instance, there are sev-
eral examples of catastrophic results as the consequence of single predictive
outputs made from carefully collected and analyzed experimental data.
In this section we return to the philosophical argument that producing
one, two or even three white swans does not prove that all swans are white.
Predictions are always extrapolations: the most interesting predictions are for
quantities or setting for which no data are available. Therefore, an obvious
choice to demonstrate the importance of predictive envelopes is to see them
in action on a simple extrapolation problem.
Example 7.6: One of the fundamental beliefs in physics is that the grav-
itational force exerted by a mass on a test particle is a linear function of
the particle’s mass. This theory has been tested with objects of relatively
small size and it seems to hold with high accuracy. But the statement that
it holds universally for all masses is a rather bold extrapolation of local lin-
earity. Considering that the estimated mass of the universe is of the order
of 1060, the mass of the solar system is of the order 1030, our experience on
gravity is limited to a quite small range. At this point one may wonder if
the dilemma of the missing dark matter could not, after all, be explained
by adding a small nonlinear correction to the linear gravitational theory.
To test this, let’s assume that we perform 500 measurements of masses
and associated gravitational force of the Earth, (mj, Fj), 1 ≤j ≤500. In
our Gedankenexperiment4, the masses are chosen randomly in the interval
between 10−3 and 104 kilograms. To put the linear theory of gravitational
3 In most of the non-statistical inverse problems literature, the performance of
the methods proposed is demonstrated with one dubious output. To avoid the
misunderstanding that we say this to blame others, we remark that in numerous
articles written by the authors, a wealth of examples adhering to the “dubious
output paradigm” can be found!
4 The German term for thought experiment, that was strongly advocated by Ein-
stein, goes back to the Danish physicist and chemist Hans Christian Ørsted (1777–
1851), a keen reader of the philosophy of Immanuel Kant.

7.3 Envelopes, white swans and dark matter
145
force in pole position, we calculate the simulated data using the linear
formula
Fi = gmi + ei,
where g = 9.81 m/s2, and draw the error ei from a Gaussian, normally dis-
tributed with standard deviation σ = 10−10, which means that we assume
a tremendous accuracy of the data.
Being slightly doubtful about the validity of the linear theory of gravity, we
consider the possibility of enriching it with a possible quadratic correction,
to obtain the new model
F(m) = x1m + x2m2.
We express, however, a strong conﬁdence in the validity of the linear model
via the prior model
πprior(x) ∝exp

−1
2γ2 (|x1 −g|2 + |x2|2)

,
γ = 10−14,
stating that, by any reasonable measure, we are almost certain of the
validity of the linear model with gravitational acceleration equal to g.
The question that we now ask is: Given the strong belief in the linear model
and the data, what does this model predict the gravitational force exerted by
the mass of the Earth to be for masses in an interval [mmin, mmax] which
is beyond the observable and measurable range?
To answer this question we write a likelihood model, admitting that the
measurements may contain a negligible Gaussian error of standard devia-
tion σ given above. Letting
A =
⎡
⎢⎢⎢⎣
m1 m2
1
m2 m2
2
...
...
mn m2
n
⎤
⎥⎥⎥⎦, F =
⎡
⎢⎢⎢⎣
F1
F2
...
Fn
⎤
⎥⎥⎥⎦,
the likelihood is of the form
π(F | x) ∝exp

−1
2σ2 ∥Ax −F∥2

,
leading to the posterior density
π(x | F) ∝exp

−1
2σ2 ∥Ax −F∥2 −
1
2γ2 ∥x −x∥2

= exp
"
−1
2σ2
((((
 A
δI

xc −
Fc
0
((((
2#
δ = σ
γ ,
where

146
7 Conditional Gaussian densities and predictive envelopes
x =
g
0

,
xc = x −x,
Fc = F −Ax.
Since we are interested in the predictions of this model, we generate a
large sample of vectors xj, 1 ≤j ≤N and the corresponding sample of
prediction curves,
m →xj
1m + xj
2m2,
mmin ≤m ≤mmax.
Then, for each value of m we calculate intervals that contains 90% of the
values xj
1m + xj
2m2, 1 ≤j ≤N. These intervals deﬁne the 90% pointwise
predictive output envelope of the model.
0
2
4
6
8
10
x 10
8
0
0.5
1
1.5
2
x 10
10
Fig. 7.4. Predicted mean and 90% pointwise predictive envelope of extrapolation
problem.
The curve corresponding to the posterior mean together with the predictive
output envelope is plotted in Figure 7.4. The number of sample points in
this example is N = 1000, and the interval of extrapolation is [1, 109].
Despite the strong prior belief in the linear model and the good quality of
the data – which should have favored the linear model that was used to
generate it – the belief envelope is amazingly wide, and orders of magnitude
away from mean extrapolation.
Although the model is not taking into account the evidence we might have
of the linearity of the gravity, at this point one might wonder if the dark
matter is but a white swan.
The themes discussed in this chapter will be further developed in the next
chapter and the exercises are postponed to the end of that chapter.

8
More applications of the Gaussian conditioning
In the previous chapter we developed tools to calculate the Gaussian condi-
tional probability densities using partitioning of the joint covariance matrix
and Schur complements. In this chapter we apply these tools to a few more
complex examples. We begin with illustrating how to analyze linear Gaussian
models with additive noise, a procedure with a wide range of applicability,
and which will serve as a basis to treat more general cases.
8.1 Linear inverse problems
Assume that we have a linear model,
B = AX + E,
where the pair (X, E) is Gaussian. Although it is rather common to assume
that X and E are mutually independent, in general this is neither necessary
nor easily justiﬁed, as we pointed out in the previous chapter. The question
that we address here is the calculation of the conditional density π(x | y). We
recall that when X and E are mutually independent, the Bayes formula states
that
π(x | y) ∝πprior(x)πnoise(b −Ax).
Assuming that X ∼N(x0, Γ), E ∼N(e0, Σ), and that the inverses of the
covariance matrices have symmetric factorizations,
Γ −1 = LTL,
Σ−1 = STS,
we can write the posterior density in the form
π(x | y) ∝exp

−1
2∥L(x −x0)∥2 −1
2∥S(b −e0 −Ax)∥2

.
This useful expression for the posterior density is not the most general one.
As it was pointed out in Examples 7.1 and 7.2, the mutual independence of

148
8 More applications of the Gaussian conditioning
the unknown X and of the noise E is a special condition that cannot always
be assumed.
To derive a more general form, let us assume for simplicity that X and
E have zero mean. If this is not the case, we can always consider instead
the variables X −x0 and E −e0, where x0 and e0 are the means. With this
assumption, B has also mean zero:
E

B

= AE

X

+ E

E

= 0.
Let us deﬁne the covariance matrices,
Γxx = E

XXT
∈Rn×n,
Γxb = Γ T
bx = E

XBT
∈Rn×m,
(8.1)
Γbb = E

BBT
∈Rm×m,
and the joint covariance matrix,
E

X
B
 
XT BT *
=

Γxx Γxb
Γbx Γbb

∈R(n+m)×(n+m).
To calculate the posterior covariance, we note that it was shown earlier that
the conditional density π(x | y) is a Gaussian probability distribution with
center point
xCM = ΓxbΓ −1
bb b,
(8.2)
and with covariance matrix the Schur complement of Γbb.
Γpost = 
Γbb.
(8.3)
To calculate the covariances (8.1) in terms of the covariances of X and E,
denote, as before,
Γxx = E

XXT
= Γprior,
Γee = E

EET
= Γnoise.
and the cross covariances of X and E by
Γxe = Γ T
ex = E

XET
.
(8.4)
Notice that the matrices (8.4) are often assumed to be zero as a consequence
of the independence of X and E. In general, we obtain
Γxb = E

X(AX + E)T
= E

XXT
AT + E

XET
= ΓpriorAT + Γxe.
Similarly,

8.1 Linear inverse problems
149
Γbb = E

(AX + E)(AX + E)T
= AE

XXT
AT + AE

XET
+ E

EXT
AT + E

EET
= AΓpriorAT + AΓxe + ΓexAT + Γnoise.
Therefore the expression for the midpoint and the covariance of the pos-
terior density, i.e., the conditional mean and covariance,
xCM =

ΓpriorAT + Γxe

AΓpriorAT + AΓxe + ΓexAT + Γnoise
−1b,
and
Γpost = Γprior
−

ΓpriorAT + Γxe

AΓpriorAT + AΓxe + ΓexAT + Γnoise
−1
AΓprior + Γex

,
are more cumbersome than when using the formulas (8.2) and (8.3).
In the special case where X and E are independent and therefore the cross
correlations vanish, the formulas simplify considerably,
xCM = ΓpriorAT
AΓpriorAT + Γnoise
−1b,
(8.5)
Γpost = Γprior −ΓpriorAT
AΓpriorAT + Γnoise
−1AΓprior.
(8.6)
The formulas (8.5) and (8.6) require that the prior and noise covariances
are given. Sometimes - like in the case of smoothness priors - instead of these
matrices, their inverses are available, and in some applications the inverse is
sparse while the matrix itself is dense. Fortunately, there are also formulas in
terms of the inverses. If X and E are independent and
Bprior = Γ −1
prior,
Bnoise = Γ −1
noise.
we can write the posterior as
π(x | y) ∝π(x)π(y | x)
∝exp

−1
2xTBpriorx −1
2(b −Ax)TBnoise(b −Ax)

.
Collecting the quadratic and linear terms above together, and completing the
square we get
xTBpriorx + (b −Ax)TBnoise(b −Ax)
= xT
Bprior + ATBnoiseA



=Bpost

x −xTATBnoiseb −bTBnoiseAx + bTBnoiseb
=

x −B−1
postATBnoiseb
TBpost(x −B−1
postATBnoiseb

+
bT
· · ·

b



independent of x
.

150
8 More applications of the Gaussian conditioning
Since the last term contributes only to the normalizing constant which is of
no interest here, we write
π(x | y) ∝exp

−1
2

x −B−1
postATBnoiseb
TBpost(x −B−1
postATBnoiseb

,
from which we derive the alternative formulas
xCM = B−1
postATBnoiseb =

Bprior + ATBnoiseA)−1ATBnoiseb
=

Γ −1
prior + ATΓ −1
noiseA)−1ATΓ −1
noiseb,
(8.7)
and
Γpost = B−1
post =

Γ −1
prior + ATΓ −1
noiseA)−1.
(8.8)
It is not immediately clear that these formulas are equivalent to (8.5) and
(8.6). In fact, the proof requires a considerable amount of matrix manipula-
tions and is left as an exercise.
Example 8.1: Assume that both X and E are Gaussian white noise, i.e.,
Γprior = γ2I and Γnoise = σ2I. Then we have the two diﬀerent expressions
for the conditional mean:
xCM = (ATA + δ2I)−1ATb = AT(AAT + δ2I)−1b,
δ = σ
γ .
(8.9)
The former expression is already familiar: it is in fact the Tikhonov regu-
larized solution of the linear system Ax = b. The latter is also a classical
formula, the Wiener ﬁltered solution of the linear system1. One may ask
which formula should be used. If A ∈Rm×n, since
ATA + δ2I ∈Rn×n,
AAT + δ2I ∈Rm×m.
at ﬁrst it would seem that for underdetermined systems (m < n), the
Tikhonov regularized solution is more appropriate while for overdeter-
mined systems (n < m) one should favor Wiener ﬁltering. The decision
is not so clear cut, because the complexity of the computation depends
on many other factors, than just the dimensionality of the problem, e.g.,
the sparsity and the structure of the matrices involved. Often it is advis-
able not to use any of the above formulas, but to notice that the posterior
density is
π(x | b) ∝exp

−1
2σ2 ∥Ax −b∥2 −
1
2γ2 ∥x∥2

= exp

−1
2σ2
((((

A
δI

x −

b
0
((((

.
1 The formula goes back to the signal processing work of Norbert Wiener (1894–
1964) from the fourties, and it is classically derived via projection techniques in
probability space.

8.2 Aristotelian boundary conditions
151
Therefore the conditional mean is the least squares solution of the system
 A
δI

x =
 b
0

,
which can often be computed eﬀectively using iterative solvers.
Before applying the above results to our model problems, we discuss
boundary eﬀects that play an important role in inverse problems of imag-
ing and signal analysis. In the following section, we show how the selection of
suitable boundary conditions for a truncated signal or image can be dealt with
from a statistical perspective, with techniques very similar to those introduced
for the interpolation problem.
8.2 Aristotelian boundary conditions
To put boundary conditions into context, let us start by considering a one-
dimensional deblurring problem, where the objective is to estimate the func-
tion f : [0, 1] →R from the observations of a noisy, blurred version of it,
modelled as
g(sj) =
 1
0
A(sj, t)f(t)dt + e(sj),
1 ≤j ≤m,
with A a smoothing kernel. The prior belief is that the signal f is smooth. No
particular information concerning the boundary behavior of f at the endpoints
of the interval is available.
Discretize the model by dividing the support of the signal into n equal
subintervals, let
xj = f(tj),
tj = j
n,
0 ≤j ≤n.
and consider the inverse problem of recovering x from the data.
To construct a prior density that favors non-oscillatory solutions, we pro-
ceed as in Section 7.2, and write a stochastic model,
Xj = 1
2(Xj−1 + Xj+1) + Wj,
1 ≤j ≤n −1,
where the innovation processes Wj are mutually independent, Gaussian ran-
dom variables with zero mean and variance γ2. In matrix form, the stochastic
model can be written as
LX = W,
where L ∈R(n−1)×(n+1) is the second order ﬁnite diﬀerence matrix (7.12),and
W ∼N(0, γ2I),
W ∈Rn−1.

152
8 More applications of the Gaussian conditioning
A natural candidate for the prior density for X, referred to as the preprior
here, would be
πpre(x) ∝exp

−1
2γ2 ∥Lx∥2

= exp

−1
2γ2 xTBx

,
where
B = LTL ∈R(n+1)×(n+1).
However, since it can be shown that2
rank (B) = n −1,
B is not invertible, hence πpre is not a proper density in the sense that we
cannot ﬁnd a symmetric positive deﬁnite covariance matrix Γ such that
exp

−1
2γ2 xTBx

= exp

−1
2xTΓ −1x

.
In the attempt to compensate for the rank-deﬁciency of B, let us see what
happens if we augment L by adding a ﬁrst and last row,
1
2
⎡
⎢⎢⎢⎣
1 −2
1
1 −2
1
... ... ...
1 −2 1
⎤
⎥⎥⎥⎦→1
2
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−2
1
1 −2
1
1 −2
1
... ... ...
1 −2
1
1 −2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
= LD ∈R(n+1)×(n+1).
The augmented matrix LD is symmetric positive deﬁnite, hence
πD(x) ∝exp

−1
2γ2 ∥LDx∥2

is a proper probability density.
To understand what the augmentation of L to LD means from the point
of view of statistical information, we look at random draws from this density.
After making the change of variable
W = 1
γ LDX,
and denoting by w a realization of W, we have that
exp

−1
2γ2 ∥LDx∥2

= exp

−1
2∥w∥2

,
that is, W is Gaussian white noise in Rn+1. Therefore we can generate random
draws from πD by performing the following steps:
2
This is not obvious, and the proof requires a fair amount of linear algebraic
manipulations.

8.2 Aristotelian boundary conditions
153
1. Draw a realization w of W,
w ∼N(0, I),
(w = randn(n + 1, 1));
2. Find the corresponding realization x of X by solving the linear system
LDx = γw
A few of these random draws are shown in Figure 8.1.
0
0.2
0.4
0.6
0.8
1
−5
0
5
Fig. 8.1. Five random draws from the density πD.
It is clear from Figure 8.1 that
•
At the endpoints of the support all random draws almost vanish;
•
The maximum variability in the draws seems to occur at the center of the
support interval.
To understand why the endpoints of the random draws all approach zero,
compute the variance of X at the jth pixel:
var

Xj

= E

X2
j

=

Rn+1 x2
jπD(x)dx.
(8.10)
Writing
xj = eT
j x,
ej =

0 0 · · · 1 · · · 0
T,
substituting it in (8.10) and remembering that, from our construction of the
density, the inverse of the covariance matrix of X is (1/γ2)LT
DLD, we have
that
var

Xj

= eT
j
 
Rn xxTπD(x)dx



=E

XXT

ej

154
8 More applications of the Gaussian conditioning
= γ2eT
j

LT
DLD
−1ej = γ2eT
j L−1
D L−T
D ej
= γ2∥L−T
D ej∥2.
0
0.2
0.4
0.6
0.8
1
−5
0
5
(σD)j
Fig. 8.2. Five random draws from the density πD and the standard deviation curve.
In Figure 8.2 the calculated standard deviation curve is plotted along with
the random draws of Figure 8.1. We deduce that by augmenting L into LD
we have implicitly forced the variance to nearly vanish at the endpoints. In
fact, the construction of LD is equivalent to extending x by zero outside the
interval, modifying the stochastic model so that
X0 = 1
2X1 + W0 = 1
2(X−1

=0
+X1) + W0,
and
Xn = 1
2Xn−1 + Wn = 1
2(Xn−1 + Xn+1
  
=0
) + Wn.
While this may be very well justiﬁed in a number of applications and might
have been supported by our prior belief about the solution, it is deﬁnitely
not a universally good way to handle boundary points. In fact, if we do not
know and we have no reason to believe that the signal approaches zero at
the boundary, it is better to admit our ignorance3 about the behavior of the
solution at the boundary and let the data decide what is most appropriate.
To free the endpoints of the interval from assumptions that we cannot
support, let’s go back to L ∈R(n−1)×(n+1). Instead of augmenting L into LD,
we use the idea of conditioning of Section 7.2 and proceed as follows:
3 Quoting Stan Laurel’s character in the classic movie Sons of the Desert, “Honesty
[is] the best politics!”

8.2 Aristotelian boundary conditions
155
•
First express the believed smoothness at the interior points by the condi-
tional density,
π(x1, x3, . . . , xn−1 | x0, xn) ∝exp

−1
2γ2 ∥Lx∥2

,
conditioned on the values at the endpoints;
•
Admit that we only pretended to know the values at endpoints, but did
not;
•
Free the endpoints by modelling them as random variables.
The conditional probability density of the interior points was derived in
Section 7.2. After permuting and partitioning the components of x so that
x =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x1
x2
...
xn−1
−−−
xn
x0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
 x′
x′′

,
and rearranging the matrix L accordingly,
L =

L1 L2

,
L1 ∈R(n−1)×(n−1),
L2 ∈R(n−1)×2,
the conditional density can be written in the form
π(x′ | x′′) ∝exp

−1
2γ2 ∥L1(x′′ −L−1
1 L2x′′)∥2

.
Assume now that the boundary points X0 and Xn are independent, zero mean
Gaussian random variables with variance β2, that is
π(x′′) ∝exp

−1
2β2 ∥x′′∥2

= exp

−1
2∥Kx′′∥2

,
where
K = 1
β I ∈R2×2.
As shown in Section 7.2, the density of X is
π(x) = π(x′ | x′′)π(x′′) = exp

−1
2γ2 ∥LAx∥2

,
where LA ∈R(n+1)×(n+1) is the invertible matrix
LA =

L1 L2
0 δK

,
δ = γ
β .

156
8 More applications of the Gaussian conditioning
Permuting the columns and rows of LA to correspond the natural order of the
pixels of x, (1, 2, . . . , n, 0) →(0, 1, . . . , n), we ﬁnd that
LA = 1
2
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
2δ
0
−1 2 −1
−1 2 −1
... ... ...
−1 2 −1
0 2δ
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
If we want a prior density that expresses an approximately equal lack of
knowledge4 about the pixel values in the entire interval, the ﬁnal step is to
choose δ so that the variance is approximately the same at every point of the
interval, i.e.,
variance at j = 0 ≈variance at j = [n/2] ≈variance at j = n,
where [n/2] denotes the integer part of n/2. Observe that since the variance
of Xj with respect to the prior density deﬁned by the matrix LA is
(σA)2
j = γ2∥L−T
A ej∥2,
the condition above becomes
∥L−T
A e0∥2 = ∥L−T
A e[n/2]∥2 = ∥L−T
A en∥2.
To compute δ in an eﬃcient manner, since away from the endpoints the vari-
ance of the random draws is fairly insensitive to our boundary assumptions,
we can use the approximation
(σA)2
j ≈(σD)2
j,
j =

n/2

,
and set
β = (σD)n/2 = γ∥L−T
D e[n/2]∥,
to get
δ = γ
β =
1
∥L−T
D e[n/2]∥.
We call the prior obtained in this manner Aristotelian boundary prior
because, like the Greek philosopher Aristotle5 who believed that knowledge
4 May be it would be more honest to say ignorance here also, but we opted for lack
of knowledge to keep the moral high.
5 Unfortunately Aristotle has a bad reputation in the scientiﬁc community, prob-
ably because many of his scientiﬁc speculations were eventually proved to be
incorrect. Here we pay our tribute to Aristotle’s philosophy of knowledge as a
layered process, rather than to his scientiﬁc intuition. In fairness, though, it is
safe to say that what we believe to be scientiﬁcally sound today will not hold any
longer two thousand years from now, assuming that there is anybody to witness
the consequences of today’s sound science.

8.2 Aristotelian boundary conditions
157
is a process which starts from a clean slate and is built up, layer after layer,
as we experience nature, we also start with minimal assumptions about the
boundary behavior of the solution and let the data determine what it should
be.
Figure 8.3 shows a few random draws from the density obtained from the
matrix LA, together with the computed standard deviation curve. Note that
now the variance of the values at the endpoints is similar to the variance of
the values in the interior of the support. We remark that these random draws
represent only what we believe a priori about our solution, before taking the
measured data into consideration. Once we introduce the measured data via
the likelihood, the values of the entries of the vector x will settle, in view of
the prior belief and of the measurements.
0
0.2
0.4
0.6
0.8
1
−10
−5
0
5
10
(σA)j
Fig. 8.3.
Five random draws from the density πA together with the standard
deviation curve.
With the Aristotelian smoothness prior density in hand, we return to the
deconvolution problem introduced in the beginning of this section. Let
yj = g(sj) =
 1
0
A(sj, t)f(t)dt + e(sj),
1 ≤j ≤m,
where the blurring kernel is
A(s, t) = c(s)t(1 −t)e−(t−s)2/2w2,
w = 0.03,
and the scaling function c is chosen so that maxt∈[0,1](K(s, t)) = 1. In Fig-
ure 8.4 the kernel t →A(s, t) is plotted for two diﬀerent values of s.
We assume that the sampling points are
sj = (j −1)/(m −1),
m = 40.
and that the noise is normally distributed white noise with variance σ2.

158
8 More applications of the Gaussian conditioning
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Fig. 8.4. The blurring kernels with s = s1 and s = s20.
The problem is discretized by dividing the interval into n = 100 subinter-
vals of equal length, as in the construction of the Aristotelian prior, yielding
a matrix equation
y = Ax + e,
where A ∈Rm×(n+1) has entries
Ai,j = 1
nA(si, tj).
We compute the posterior probability densities corresponding to the prior
densities πD and πA. The parameter γ in the prior is set to γ = 1/n, which
corresponds to the assumption that the expected increments in the innovation
process correspond to increments that are needed to cover a dynamic range
of order of magnitude of unity. The noise level σ is assumed to be 3% of the
noiseless signal.
In Figure 8.5 we have plotted the conditional means corresponding to both
the Dirichlet (left panel) and the Aristotelian (right panel) smoothness priors,
and the corresponding pointwise predictive output envelopes corresponding to
two standard deviations. The true input is also plotted.
The results reveal several features of the priors. First, we observe that
with the Dirichlet smoothness prior, the conditional mean and the credibility
envelopes fail to follow the true input, which obviously is in conﬂict with the
prior belief that it vanishes at the endpoints. The Aristotelian prior produces
a better conditional mean estimate in this respect. On the other hand, the
Aristotelian predictive envelope is considerably wider towards the end of the
interval than in its interior. There is a natural explanation for this: by looking
at the blurring kernels, we observe that they convey much less information
about the signal at the endpoints of the interval than in the interior, there-
fore the posterior density at the endpoints is dominated by the prior. We

8.2 Aristotelian boundary conditions
159
0
0.2
0.4
0.6
0.8
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
−1.5
−1
−0.5
0
0.5
1
1.5
Fig. 8.5.
The conditional means and two standard deviation predictive output
envelopes using the Dirichlet smoothness prior (left) and the Aristotelian smoothness
prior (right). The bold curve is the true input used to generate the data.
may conclude that the Dirichlet prior gives a false sense of conﬁdence at the
endpoints, while the Aristotelian prior admits the lack of information6.
Exercises
This exercise shows how incorrect boundary values in the prior may have
global eﬀects on the solution of an inverse problem.
We consider the inverse Laplace transform: estimate the function f from
the observations
g(sj) =
 ∞
0
e−sjtf(t)dt + ej,
1 ≤j ≤m.
The Laplace transform is notoriously ill–conditioned.
1. Assume that the function f is supported on the interval [0, 1], calculate
the matrix A which is the discretized Laplace transform by using the
piecewise constant approximation,
 1
0
e−sjtf(t)dt ≈1
n
n

k=1
e−sjtkf(tk),
tk = k −1
n −1, 1 ≤k ≤n,
and choosing the data points sj to be logarithmically equidistant, i.e.,
log10(sj) = 4
 j −1
m −1 −1

,
1 ≤j ≤m.
Calculate the condition number of the matrix A for diﬀerent values of n.
6 This example veriﬁes the words of Thomas Gray: “Where ignorance is bliss, ’tis
folly to be wise.”

160
8 More applications of the Gaussian conditioning
2. Calculate the LA matrix as explained in this section.
3. Assume that you know that the signal f takes on values roughly between
−2 and 2. Scale the LA matrix in such a way that random draws from the
corresponding smoothness prior,
π(x) ∝exp

−1
2α∥LAx∥2

take on values in the target interval. Test your scaling with random draws.
4. Having set up the prior, assume that the true f is a step function,
f(t) =

1, when 0 ≤t < a
0, when t ≥a
,
0 < a < 1.
Calculate analytically the noiseless data, then add Gaussian noise to it.
Choose a low noise level, otherwise the results are rather bad.
5. Calculate the data also by using your matrix A and estimate the dis-
crepancy that is due to the discretization. Is it larger or smaller than the
additive arror? Notice that the level of the discretization error depends on
the number of discretization points. The data that is generated by the for-
ward model and is subsequently used to solve the inverse problem is often
referred to as the inverse crime data, since it corresponds unrealistically
well to the model one uses7.
6. Calculate the conditional mean estimate and the covariance matrix. Plot
the predictive output envelope of two posterior standard deviations.
7. Repeat the calculation using LD instead of LA. Notice that the wrong
boundary value propagates also away from the boundary.
8. The step function corresponds badly to the smoothness prior assumption.
Repeat the calculations with data that comes from a smooth function that
takes on a non-zero value at the boundary.
9. Try the solutions with the inverse crime data. Do you notice any diﬀer-
ence?
10. Show that the formulas (8.5)–(8.6) and (8.7)–(8.8) are equivalent. This
may be a tedious task!
7 In this book, we have not paid much attention to the issue of inverse crimes,
albeit it is a very serious issue when model credibilities are assessed.

9
Sampling: the real thing
In Chapter 5, where we ﬁrst introduced random sampling, we pointed out
that sampling is used primarily to explore a given probability distribution
and to calculate estimates for integrals via Monte Carlo integration. It was
also indicated that sampling from a non-Gaussian probability density may be
a challenging task. In this section we further develop the topic and introduce
Markov Chain Monte Carlo (MCMC) sampling. We start the discussion with
a small example concerning Markov chains.
Example 9.1: Consider a small network consisting of nodes and directed
links of the type that we could obtain as the result of an internet search: a
set of keywords are given to a search engine that ﬁnds a set of internet sites
containing those keywords. The sites found are the nodes of the network
and each site might contain links to other sites. If the node A contains a
link to the site B which is also a node in the network, the nodes are linked
to each others by a directed link from A to B. Observe that the site B
need not have a link to site A. Figure 9.1 shows a small prototype of such
a network.
If we want to list the network nodes in a descending order of importance, as
some search engines do, then the natural question which arises is whether
it is possible to deduce from the network structure alone which node is the
most important.
At ﬁrst one might think that the most important node is the one most
referenced by the other nodes. But the question is more complex: in fact,
there may be several insigniﬁcant nodes referring to one particular link,
while there are several prominent ones referring to each other, forming
thus a large cluster of relatively well referenced nodes. Therefore a better
deﬁnition of importance could be based on random walk: when surﬁng the
network randomly, always going from one node to another by selecting one
of the directed links randomly, the most visited node would seem to be the
most important one. Notice that the visits are done randomly, since we
were not allowed to pay attention to the contents of the sites.

162
9 Sampling: the real thing
1
2
3
4
5
1/3
1/3
1/3
1
2
3
4
5
Fig. 9.1. The network (left) and the transition probabilities for moving from node
4 (right).
To explore the network, we write a transition matrix, deﬁning the prob-
abilities of transition between the nodes. In the right panel of Figure 9.1
we consider the probabilities of diﬀerent moves from node 4. Since there
are three links starting from this node, the transition probabilities of each
one of them is equal to 1/3. The transition matrix corresponding to this
network is
P =
⎡
⎢⎢⎢⎢⎣
0
1
0
1/3
0
1/2 0
0
1/3
0
0
0
0
0 1/2
1/2 0 1/2
0
1/2
0
0 1/2 1/3
0
⎤
⎥⎥⎥⎥⎦
.
The kth column of P contains the transition probabilities from the kth
node. Hence, if we start from node 1, the probability density of the next
state is
π2 = Pπ1 =
⎡
⎢⎢⎢⎢⎣
0
1/2
0
1/2
0
⎤
⎥⎥⎥⎥⎦
,
π1 =
⎡
⎢⎢⎢⎢⎣
1
0
0
0
0
⎤
⎥⎥⎥⎥⎦
.
Observe that the choice of π1 reﬂects the fact that initially we are at node
1 with certainty. Similarly, the probability density after n steps is
πn = Pπn−1 = P 2πn−2 = · · · = P nπ1.
Deﬁne a sequence of random variables Xn ∈R5 as follows: the components
of Xn all vanish except one, and the index of the non-vanishing component
is drawn from the density πn. Hence, in the previous example, the possible
realizations of the random variable X2 are

9 Sampling: the real thing
163
x2 =
⎡
⎢⎢⎢⎢⎣
0
1
0
0
0
⎤
⎥⎥⎥⎥⎦
= e2
or
x2 =
⎡
⎢⎢⎢⎢⎣
0
0
0
1
0
⎤
⎥⎥⎥⎥⎦
= e4,
and
P

X2 = e2

= P

X2 = e4

= 1/2.
If we want to move one step further, we have
π3 = P 2π1 =
⎡
⎢⎢⎢⎢⎣
2/3
1/6
0
0
1/6
⎤
⎥⎥⎥⎥⎦
.
These are the probabilities of the third state, assuming that we know that
the sequence starts at node 1, i.e.,
π(x3 | x1) = π3,
x1 = e1.
But what if we know also the second state? Suppose that X2 = x2 = e2.
Then, evidently, the probability distribution of X3 becomes
π(x3 | x1, x2) = Pe2 = e1,
x1 = e1, x2 = e2.
The relevant point here is that this result does not depend on the value of
X1: whichever way we came to the second node X2 = e2, the probability
density of X3 is always Pe2, that is,
π(x3 | x1, x2) = π(x3 | x2).
More generally, if we know the state Xn = xn, the next state has density
Pxn regardless of the more remote past states. We may write
π(xn+1 | x1, x2, . . . , xn) = π(xn+1 | xn).
(9.1)
We say that a discrete time stochastic process

X1, X2, . . .

is a Markov
process if it has the property (9.1). This condition is often expressed by
saying that “tomorrow depends on the past only through today”. We have
encountered previously Markov processes when constructing smoothness
priors. There the Markov property referred to spatial neighbors instead of
temporal predecessors.
Consider now the question of how to assess the importance of the nodes.
One way is to start a realization of the Markov chain at some node, say,
x1 = e1, generate a sample,
S =

x1, x2, . . . , xN

,

164
9 Sampling: the real thing
0
1
2
3
4
5
6
0
0.1
0.2
0.3
0.4
0.5
k=100
k=500
k=1500
Fig. 9.2.
The relative visiting frequencies at diﬀerent nodes with three diﬀerent
chain lengths N.
by drawing xj from the probability density Pxj−1 and compute the visiting
frequency of the diﬀerent nodes.
Figure 9.2 shows, in histogram form, the percentages of visitations of the
diﬀerent nodes,
hj = 1
N #

xk | xk = ej

,
for diﬀerent values of N. The histograms seem to indicate that the ﬁrst
node is the most important, according to this criterion.
Finally, let us search for a more eﬃcient way of reaching the result that we
obtained above by random walk. Assume that, as the number of iterations
grows, the probability distribution converges towards a limit distribution,
lim
n→∞P nπ1 = π∞.
Is there a simple way to ﬁnd the limit distribution? By writing
πn+1 = Pπn,
and taking the limit on both sides, we obtain
π∞= Pπ∞.
(9.2)
In other words, the limit distribution, if it exists, is the normalized eigen-
vector of the transition matrix P corresponding to the unit eigenvalue. We
say that the limit distribution is an invariant distribution of the transi-
tion matrix P, since it remains unchanged under the action of P. Thus, to
classify the node according to their importance, it suﬃces to ﬁnd the eigen-
vector corresponding to the eigenvalue one. In Figure 9.3, we have plotted
the components of this normalized eigenvector and the visiting histogram

9 Sampling: the real thing
165
corresponding to N = 1500 random steps. The matching between the val-
ues of the components of the eigenvector and the relative frequencies of
visitation of the diﬀerent nodes is quite striking, and it increases with N.
0
1
2
3
4
5
6
0
0.1
0.2
0.3
0.4
0.5
k=1500
Eigenvector
Fig. 9.3. The visiting histogram with N = 1500 and the components of the eigen-
vector corresponding to eigenvalue one.
It is worth mentioning that since one is the largest eigenvalue of the tran-
sition matrix, its corresponding eigenvector can be computed by the power
method. The most popular web search engines order the results of their
search in a similar fashion.
In the previous example, we started with the transition matrix P and
wanted to ﬁnd the associated invariant probability distribution. Suppose now
that π∞is known instead, and that we want to generate a sample distributed
according to π∞. A natural way to proceed would be to generate a realization
of the Markov chain using the matrix P starting from some initial point.
After a few iterations, the elements of the chain will be distributed according
to π∞. The plan is perfect except for a small detail: we have access to π∞by
assumption but we don’t know P! Taking care of this detail is indeed the core
question in Markov Chain Monte Carlo!
Since, in general, we are interested in problems in which the random vari-
ables take on values in Rn, we are now going to set the stage for Markov
Chain Monte Carlo with non-discrete state space. We start by deﬁning the
concept of random walk in Rn which, as the name suggests, is a process of
moving around by taking random steps. The most elementary random walk
can be deﬁned as follows:
1. Start at a point of your choice x0 ∈Rn.
2. Draw a random vector w1 ∼N(0, I) and set x1 = x0 + σw1.
3. Repeat the process: Set xk+1 = xk + σwk+1, wk+1 ∼N(0, I).

166
9 Sampling: the real thing
Using the random variables notation, the location of the random walk at
time k is a realization of the random variable Xk, and we have an evolution
model
Xk+1 = Xk + σWk+1,
Wk+1 ∼N(0, I).
The conditional density of Xk+1, given Xk = xk, is
π(xk+1 | xk) =
1
(2πσ2)n/2 exp

−1
2σ2 ∥xk −xk+1∥2

= q(xk, xk+1).
The function q is called the transition kernel and it is the continuous equivalent
of the transition matrix P in the discrete state space example. Since
q(x0, x1) = q(x1, x2) = · · · = q(xk, xk+1) = · · · ,
i.e., the step is always equally distributed independently of the value of k, the
kernel is called time invariant.
Clearly, the process above deﬁnes a chain

Xk, k = 0, 1, · · ·} of random
variables, each of them having a probability density of its own. The chain is
a discrete time stochastic process with values in Rn and has the particular
feature that the probability distribution of each variable Xk+1 depend on the
past only through the previous element Xk of the chain. This can be expressed
in terms of the conditional densities as
π(xk+1 | x0, x1, . . . , xk) = π(xk+1 | xk).
As in the discrete case of the previous example, a stochastic process with this
property is called a Markov chain.
Example 9.2: To understand the role of the transition kernel, consider a
Markov chain deﬁned by a random walk model in R2,
Xk+1 = Xk + σWk+1,
Wk+1 ∼N(0, C),
(9.3)
where C ∈R2×2 is a symmetric positive deﬁnite matrix, whose eigenvalue
decomposition we write as
C = UDU T.
(9.4)
The inverse of C can be decomposed as
C−1 = UD−1U T =

UD−1/2 
D−1/2U T



=L
,
so the transition kernel can be written as
q(xk | xk+1) = π(xk+1 | xk) ∝exp

−1
2σ2 ∥L(xk −xk+1)∥2

.

9 Sampling: the real thing
167
We may write the random walk model (9.3) in an alternative way as
Xk+1 = Xk + σL−1Wk+1,
Wk+1 ∼N(0, I),
(9.5)
where the random step is whitened.
To demonstrate the eﬀect of the covariance matrix, let
U =
cos θ −sin θ
sin θ
cos θ

,
θ = π
3 ,
and
D = diag(s2
1, s2
2),
s1 = 1, s2 = 5.
In the light of the random walk model (9.5), the random steps have a
component about ﬁve times larger in the direction of the second eigenvector
e2 than in the ﬁrst eigenvector e1, where
e1 =

cos θ
sin θ

,
e2 =

−sin θ
cos θ

.
The left panel of Figure 9.4 shows three random walk realizations with
the covariance matrix C = I starting from the origin of R2 and choosing
the step size σ = 0.1. In the right panel, the covariance matrix is chosen
as above. Obviously, by judiciously choosing the transition kernel, we may
guide the random walk quite eﬀectively.
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−15
−10
−5
0
5
10
15
−15
−10
−5
0
5
10
15
Fig. 9.4.
Three realizations of random walks. Left: The covariance matrix of the
random step is σ2I, with the standard deviation σ = 0.1, and the number of steps
in each realization is N = 500. Right: The covariance is C given in (9.4). The line
segments mark the eigendirections of the covariance matrix.

168
9 Sampling: the real thing
Consider now an arbitrary transition kernel q. Assume that X is a random
variable whose probability density is given. To emphasize the particular role
of this density, we denote it by π(x) = p(x). Suppose that we generate a new
random variable Y by using a given kernel q(x, y), that is,
π(y | x) = q(x, y).
The probability density of this new variable Y is found by marginalization,
π(y) =

π(y | x)π(x)dx =

q(x, y)p(x)dx.
If the probability density of Y is equal to the probability density of X, i.e.,

q(x, y)p(x)dx = p(y),
we say that p is an invariant density of the transition kernel q. The classical
problem in the theory of Markov chains can then be stated as follows: Given
a transition kernel, ﬁnd the corresponding invariant density.
When using Markov chains to sample from a given density, we are actually
considering the inverse problem: Given a probability density p = p(x), gener-
ate a sample that is distributed according to it. If we had a transition kernel
q with invariant density p, generating such sample would be easy: starting
from x0, draw x1 from q(x0, x1), and repeat the process starting from x1. In
general, given xk, draw xk+1 from q(xk, xk+1). After a while, the xk’s gener-
ated in this manner are more and more distributed according to p. This was
the strategy for generating the sample in the discrete state space problem of
Example 9.1.
So the problem we are facing now is: Given a probability density p, ﬁnd a
kernel q such that p is its invariant density.
Probably the most popular technique for constructing such transition ker-
nel is the Metropolis–Hastings method.
9.1 Metropolis–Hastings algorithm
Consider the following, more general, Markov process: starting from the cur-
rent point x ∈Rn, either
1. Stay put at x with probability r(x), 0 ≤r(x) < 1, or
2. Move away from x using a transition kernel R(x, y).
Since R by deﬁnition is a transition kernel, the mapping y →R(x, y)
deﬁnes a probability density, hence

Rn R(x, y)dy = 1.

9.1 Metropolis–Hastings algorithm
169
Denoting by A the event of moving away from x, and by ¬A the event of not
moving, the probabilities of the two events are
P

A

= 1 −r(x),
P

¬A

= r(x).
Now, given the current state X = x, we want to know what is the proba-
bility density of Y generated by the above strategy. Let B ⊂Rn, and consider
the probability of the event Y ∈B. It follows that
P

Y ∈B | X = x

= P

Y ∈B | X = x, A

P

A

+ P

Y ∈B | X = x, ¬A

P

¬A

,
in other words, Y can end up in B either through a move or by staying put.
The probability of arriving in B through a move is obviously
P

Y ∈B | X = x, A

=

B
R(x, y)dy.
On the other hand, arriving in B without moving can only happen if x already
is in B. Therefore,
P

Y ∈B | X = x, ¬A

= χB(x) =

1, if x ∈B,
0, if x /∈B ,
where χB is the characteristic function of B.
In summary, the probability of reaching B from x is
P

Y ∈B | X = x

= (1 −r(x))

B
R(x, y)dy + r(x)χB(x).
Since we are only interested in the probability of Y ∈B, we marginalize over
x and calculate the probability of Y ∈B regardless of the initial position,
P

Y ∈B

=

P

Y ∈B | X = x

p(x)dx
=

p(x)

B
(1 −r(x))R(x, y)dy

dx +

χB(x)r(x)p(x)dx
=

B

p(x)(1 −r(x))R(x, y)dx

dy +

B
r(x)p(x)dx
=

B

p(x)(1 −r(x))R(x, y)dx + r(y)p(y)

dy.
Recalling that, by deﬁnition,
P

Y ∈B

=

B
π(y)dy,

170
9 Sampling: the real thing
the probability density of Y must be
π(y) =

p(x)(1 −r(x))R(x, y)dx + r(y)p(y).
Our goal is then to ﬁnd a kernel R such that π(y) = p(y), that is
p(y) =

p(x)(1 −r(x))R(x, y)dx + r(y)p(y),
or, equivalently,
(1 −r(y))p(y) =

p(x)(1 −r(x))R(x, y)dx.
(9.6)
For simplicity, let us denote
K(x, y) = (1 −r(x))R(x, y),
and observe that, since R is a transitional kernel,

K(y, x)dx = (1 −r(y))

R(y, x)dx = 1 −r(y).
Therefore, condition (9.6) can be expressed in the form

p(y)K(y, x)dx =

p(x)K(x, y)dx,
which is called the balance equation. This condition is satisﬁed, in particular,
if the integrands are equal,
p(y)K(y, x) = p(x)K(x, y).
This condition is known as the detailed balance equation. The Metropolis-
Hastings algorithm is simply a technique of ﬁnding a kernel K that satisﬁes
it.
The Metropolis-Hastings algorithm starts by selecting a proposal distri-
bution, or candidate generating kernel q(x, y), chosen so that generating a
Markov chain with it is easy. It is mainly for this reason that a Gaussian
kernel is a very popular choice.
If q satisﬁes the detailed balance equation, i.e., if
p(y)q(y, x) = p(x)q(x, y),
we let r(x) = 0, R(x, y) = K(x, y) = q(x, y), and we are done, since the
previous analysis shows that p is an invariant density for this kernel. If, as is
more likely to happen, the detailed balance equation is not satisﬁed, the left
hand side is larger or smaller than the right hand side. Assume, for the sake
of deﬁniteness, that

9.1 Metropolis–Hastings algorithm
171
p(y)q(y, x) < p(x)q(x, y).
(9.7)
To enforce the detailed balance equation we can deﬁne the kernel K to be
K(x, y) = α(x, y)q(x, y),
where α is a suitable correcting factor, chosen so that
p(y)α(y, x)q(y, x) = p(x)α(x, y)q(x, y).
(9.8)
Since the kernel α need not be symmetric, we can choose
α(y, x) = 1,
and let the other correcting factor be determined from (9.8):
α(x, y) = p(y)q(y, x)
p(x)q(x, y) < 1.
Observe that if the sense of inequality (9.7) changes, we simply interchange
the roles of x and y, letting instead α(x, y) = 1. In summary, we deﬁne K as
K(x, y) = α(x, y)q(x, y),
α(x, y) = min

1, p(y)q(y, x)
p(x)q(x, y)
*
.
The expression for K looks rather complicated and it would seem that gen-
erating random draws should be all but simple. Fortunately, the drawing can
be performed in two phases, as in the case of rejection sampling, according to
the following procedure:
1. Given x, draw y using the transition kernel q(x, y).
2. Calculate the acceptance ratio,
α(x, y) = p(y)q(y, x)
p(x)q(x, y).
3. Flip the α–coin: draw t ∼Uniform([0, 1]); if α > t, accept y, otherwise
stay put at x.
To show why this works, let’s consider, again, the probability that the
proposal is accepted,
π(A | y) = probability of acceptance of y, which is = α(x, y),
and
π(y) = probability density of y, which is = q(x, y).
Then, from the Bayes formula,
π(y, A) = probability density of accepted y’s
= π(A | y)π(y) = α(x, y)q(x, y)
= K(x, y),

172
9 Sampling: the real thing
which is exactly what we wanted to prove.
We will now identify some convenient features of the algorithm while fol-
lowing it into action.
Example 9.3: Consider the following probability density in R2,
π(x) ∝exp

−1
2σ2 ((x2
1 + x2
2)1/2 −1)2 −
1
2δ2 (x2 −1)2

,
(9.9)
where
σ = 0.1,
δ = 1,
whose equiprobability curves are shown in Figure 9.5.
Fig. 9.5. The equiprobability curves of the original density (9.9)
We ﬁrst explore this density with a random walk sampler. For this purpose
consider the white noise random walk proposal,
q(x, y) =
1
	
2πγ2 exp

−1
2γ2 ∥x −y∥2

.
When the transition kernel is symmetric, i.e.,
q(x, y) = q(y, x),
the Metropolis-Hastings algorithm is particularly simple, because
α(x, y) = π(y)
π(x).
In this case the acceptance rule can be stated as follows: if the new point has
higher probability, accept it immediately; if not, accept with probability

9.1 Metropolis–Hastings algorithm
173
determined by the ratio of the probabilities of the new point and the old
one.
We start the Markov chain from the origin, x0 = (0, 0), and to illustrate
how it progresses, we adopt the plotting convention that each new accepted
point is plotted by a dot. If a proposal is rejected and we remain in the
current position, the current position is plotted with a larger dot, so that
the area of the dot is proportional to the number of rejections. A Matlab
code that generates the sample can be written as follows:
nsample = 500;
% Size of the sample
Sample = zeros(2,nsample);
count = 1;
x = [0;0];
% Initial point
Sample(:,1) = x;
lprop_old = logpdf(x);
% logpdf = log of the
prob.density
while count < nsample
% draw candidate
y = x + step*randn(2,1);
% check for acceptance
lprop =
logpdf(y);
if lprop - lprop_old > log(rand);
% accept
accrate = accrate + 1;
x = y;
lprop_old = lprop;
end
count = count+1;
Sample(:,count+1) = x;
end
We remark that in the actual computation, the acceptance ratio is calcu-
lated in logarithmic form: we accept the move x →y if
log p(y) −log p(x) > log t,
t ∼Uniform([0, 1]).
The reason for proceeding in this manner is to avoid numerical problems
with overﬂow or underﬂow in the computation of the ratio of p(y) and
p(x).
In our ﬁrst exploration of the density, we decide to move rather conserva-
tively, taking very small steps by setting γ = 0.02. A plot of the ﬁrst 500
points generated by the algorithm is shown in Figure 9.6.
It is clear from the plot that, after 500 draws, the sampler has not even
started to explore the density. In fact, almost the entire sample is needed
to move from the initial point to the numerical support of the density. This
initial tail, which has nothing to do with the actual probability density, is

174
9 Sampling: the real thing
Fig. 9.6.
The Metropolis-Hastings sample with step size γ = 0.02. The sample
mean is marked by the cross hair.
usually referred to as the burn-in of the sample. It is normal procedure in
MCMC sampling methods to discard the beginning of the sample to avoid
that the burn-in aﬀects the estimates that are subsequently calculated from
the sample. In general, it is not easy to decide a priori how many points
should be discarded.
The second observation is that the acceptance rate is high: almost nine
out of ten proposed moves are accepted. A high acceptance rate usually
indicates that the chain is moving too conservatively, and longer steps
should be used to get better coverage of the distribution.
Motivated by the observed high acceptance rate, we then increase the step
by a factor of hundred, choosing γ = 2. The results of this modiﬁcation can
be seen in Figure 9.7. Now the acceptance rate is only 7%, meaning that
most of the time the chain does not move and the proposals are rejected.
Notice that the big dots in the ﬁgure indicate points from which the chain
does not want to move away. The burn-in eﬀect is practically absent, and
the estimated conditional mean is much closer to the actual mean what
one could expect. The result seems to suggest that too low acceptance rate
is better than too high.
By playing with the steplength, we may be able to tune the proposal dis-
tribution so as to achieve an acceptance rate between 20% and 30%, which
is often advocated as optimal.
Figure 9.8 shows the results obtained with γ = 0.5, yielding an acceptance
rate of approximately 26%. We see that the points are rather well dis-
tributed over the support of the probability density. The estimated mean,
however, is not centered, indicating that the size of the sample is too small.

9.1 Metropolis–Hastings algorithm
175
Fig. 9.7. The Metropolis-Hastings sample with the step size γ = 2. The computed
sample mean is marked by the cross hair.
Fig. 9.8. The Metropolis-Hastings sample with the step size γ = 0.5.
The previous example shows that the choice of the proposal distribution
has an eﬀect on the quality of the sample thus generated. While in the two
dimensional case it is fairly easy to assess the quality of the sampling strategy
by simply looking at the scatter plot of the sample, in higher dimensions
this approach becomes impossible, and more systematic means are needed to
analyze the sample. No deﬁnitive measures for the quality can be given, and
we merely graze this rather complex topic here.
The Central Limit Theorem gives an idea of where to look for a measure of
the quality of the sample. Remember that according to the Central Limit The-
orem, the asymptotic convergence rate of a sum of N independently sampled,

176
9 Sampling: the real thing
identically distributed random variables is 1/
√
N. While the goal of Markov
Chain Monte Carlo sampling is to produce a sample that is asymptotically
drawn from the limit distribution, taking care, for the most part, of the iden-
tically distributed aspect, the part independent is more problematic. Clearly,
since the sample is a realization of a Markov chain, complete independency
of the sample points cannot be expected: every draw depends at least on the
previous element in the chain. This dependency has repercussions on the con-
vergence of Monte Carlo integrals. Suppose that on the average, only every
kth sample point can be considered independent. Then, by the asymptotic
law of the Central Limit Theorem, we may expect a convergence rate of the
order of
	
k/N, a rate that is painfully slow if k is large. Therefore, when de-
signing the Metropolis-Hastings strategies, we should be particularly careful,
in choosing the step length in the proposal distribution so that the correlation
length is small.
Let us begin with visual inspection of samples. Suppose that we have a
sampling problem in n spatial dimensions. While we cannot inspect a scatter
plot, we may always look at the sample histories of individual component,
plotting each individual component as a function of the sample index. The
ﬁrst question is then what a good sample history looks like.
0
1
2
3
4
5
6
7
8
9
10
x 10
4
−5
0
5
Fig. 9.9. A Gaussian white noise signal of length 100 000.
A typical example of a sample with completely uncorrelated elements is a
white noise signal: at each discrete time instant, the sample is drawn indepen-
dently. A Gaussian white noise signal is, in fact, a realization of a Gaussian
multivariate vector with covariance σ2I. Each component, independent of the
remaining ones, represents a realization of one time instance. Figure 9.9 shows
a realization of a white noise signal. It looks like a “fuzzy worm”. This gives a
visual description for a MCMCer of what is meant by saying that good sam-
ple histories should look like fuzzy worms. This visual description, although
rather vague, is in fact, quite useful to quickly assess the quality of a sample.
To obtain a more quantitative measure of the quality of a sample, we may
look at its correlation structure. The autocorrelation of a signal is a useful

9.1 Metropolis–Hastings algorithm
177
tool to analyze the independency of realizations. Let zj, 0 ≤j ≤N denote a
discrete time ﬁnite segment of a signal. Assume, for simplicity, that the signal
has zero mean. After augmenting the signal with trailing zeroes to an inﬁnite
signal, 0 ≤j < ∞, consider the discrete convolution,
hk =
∞

j=0
zj+kzj,
k = 0, 1, 2 . . .
When k = 0, the above formula returns the total energy of the signal,
h0 =
N

j=1
z2
j = ∥z∥2.
If z is a white noise signal and k > 0, the random positive and negative
contributions cancel out, and hk ≈0. This observation gives a natural tool to
analyze the independency of the components in a sample: plot
k →hk =
1
∥z∥2 hk,
k = 0, 1, . . .
and estimate the correlation length from the rate of decay of this sequence.
The quantity hk is the autocorrelation of z with lag k.
In the following example we demonstrate how to use of this idea.
Example 9.4: Consider the horseshoe distribution of the previous exam-
ple, and the Metropolis–Hastings algorithm with a white noise proposal
distribution. We consider three diﬀerent step sizes, γ = 0.02 which is way
too small, γ = 0.8 which is in a reasonable range and, ﬁnally, γ = 5,
which is clearly too large. We generate a sample

x1, x2, . . . , xN

of size
N = 100 000, calculate the mean,
x = 1
N
N

j=1
xj,
and the lagged autocorrelations of the centered components,
hi,k =
1
∥z∥2
N−k

j=1
zj+kzj,
zj = (xj −x)i,
i = 1, 2.
In Figure 9.10 we have plotted the sample history of the ﬁrst component
with the diﬀerent step sizes. Visually, the most reasonable step size pro-
duces a sample history which is similar to the white noise sample, while
with the smallest step size there is a strong low frequency component that
indicates a slow walk around the density. The too large step size, on the
other hand, causes the sampler to stay put for long periods of times with
an adverse eﬀect on the independency of the samples.

178
9 Sampling: the real thing
0
1
2
3
4
5
6
7
8
9
10
x 10
4
−1.5
−1
−0.5
0
0.5
1
1.5
0
1
2
3
4
5
6
7
8
9
10
x 10
4
−1.5
−1
−0.5
0
0.5
1
1.5
0
1
2
3
4
5
6
7
8
9
10
x 10
4
−1.5
−1
−0.5
0
0.5
1
1.5
Fig. 9.10.
The sample histories of the ﬁrst component of our random variable
with diﬀerent step sizes in the proposal distribution. From top to bottom, γ = 0.02,
γ = 0.8 and γ = 5.
In Figure 9.11, the corresponding autocorrelations for both components
are shown. This ﬁgure conﬁrms the fact that the correlation length, or
the period after which the sample points can be considered insigniﬁcantly
correlated, is much shorter with the step length γ = 0.8 than in the extreme
cases.
In the previous examples, we used Gaussian white noise random walks as
proposal distributions. It is quite clear that this proposal takes very poorly
into account the fact that the density that we are sampling from is very

9.1 Metropolis–Hastings algorithm
179
0
50
100
0
0.2
0.4
0.6
0.8
1
 = x1
 = x2
0
50
100
0
0.2
0.4
0.6
0.8
1
 = x1
 = x2
0
50
100
0
0.2
0.4
0.6
0.8
1
 = x1
 = x2
Fig. 9.11. The autocorrelations of the components with the diﬀerent proposal step
sizes. From left to right,γ = 0.02, γ = 0.8 and γ = 5.
anisotropic. In the next example we show how the shape of the density can
be taken into account when designing the proposal distribution.
Consider the problem of estimating the random variable X from the ob-
servation
B = f(X) + E,
E ∼N(0, σ2I),
where f is a non-linear function and we assume a priori that X is a Gaussian
and independent of E. For simplicity, let X ∼N(0, γ2I). The posterior density
in this case is
π(x | b) ∼exp

−1
2γ2 ∥x∥2 −
1
2σ2 ∥b −f(x)∥2

.
and we want to explore it by using the Metropolis-Hastings algorithm.
−2
−1
0
1
2
−4
−3
−2
−1
0
1
2
3
4
Fig. 9.12. The equiprobability curves of likelihood density x →π(bobserved | x) in
Example 9.5.

180
9 Sampling: the real thing
Example 9.5: Assume that X ∈R2 and f is given by
f(x) = f(x1, x2) =

x3
1 −x2
x2/5

.
The likelihood density, x →π(y | x) , with
b = bobserved =

−0.2
0.1

,
has been plotted in Figure 9.12. If the variance γ2 of the prior is large, the
likelihood and the posterior density are essentially equal.
Since the standard Gaussian white noise proposal density does not take
into account the fact that the numerical support of the posterior density
is shaped like a relatively narrow ridge, it proposes steps with equally
long components, on average, along the ridge and perpendicularly to it.
Although for the present two-dimensional problem, a white noise ran-
dom walk proposal might still be performing well, in more realistic, high-
dimensional problems, this proposal could be unacceptably expensive,
since long proposal steps would be almost always rejected, while short
steps would explore the distribution too slowly. An alternative way would
be to use local Gaussian approximation of the density as proposal. Assum-
ing that x is the current point, begin with writing a linear approximation
for f,
f(x + w) ≈f(x) + Df(x)w,
from which we construct a quadratic approximation of the exponent of the
posterior density,
1
2γ2 ∥x + w∥2 +
1
2σ2 ∥b −f(x + w)∥2
≈
1
2γ2 ∥x + w∥2 +
1
2σ2 ∥b −f(x) −Df(x)w∥2
= wT
 1
2γ2 I +
1
2σ2

Df(x)
TDf(x)




=H(x)
w + lower order terms.
The matrix H(x) carries information about the shape of the posterior
density in a neighborhood of the base point x. Figure 9.13 shows the ellipses
wTH(x)w = constant,
calculated at two diﬀerent base points.
We can now use
q(x, y) ∝exp

−1
2δ2 (x −y)TH(x)(x −y)


9.1 Metropolis–Hastings algorithm
181
−2
−1
0
1
2
−4
−3
−2
−1
0
1
2
3
4
Fig. 9.13. Variance ellipsoids of the proposal distribution at two diﬀerent points.
The step control parameter in this plot is δ = 0.2, i.e., we have wTH(x)w = δ2 = 0.04
at the boundary of each ellipse.
as a proposal distribution, but we need to remember that the distribution
is no longer symmetric, since the matrix H depends on x. Therefore, in
the process of updating x we compute the factorization
H(x) = L(x)TL(x),
and we write
y = x + δL(x)−1w,
w ∼N(0, I).
The tuning parameter δ > 0 can be used to adjust the step size so as to
achieve the desired acceptance rate. Furthermore, to calculate the accep-
tance ratio α, we need to evaluate the diﬀerential Df(y) at the proposed
point y, since
q(y, x) ∝exp

−1
2δ2 (y −x)TH(y)(y −x)

.
We remark that if y is accepted, the value of the diﬀerential can be used
for the subsequent linear approximation, based at y.
The process of updating the proposal distribution while the sampling al-
gorithm moves on is called adaptation. The adaptation strategy explained in
the previous example is suitable for low dimensional problems, but usually
too costly for high dimensional problems and, in practice, the adaptation is
normally based on the sample that has been generated so far: the empiri-
cal covariance of the sample is calculated and used as the covariance of the
proposal distribution, see the exercises at the end of this chapter. A deeper
discussion of diﬀerent variants of MCMC algorithms is beyond the scope of
this book.

182
9 Sampling: the real thing
Exercises
1. Write a Matlab code based on the adaptation of Example 9.5. Compare
the acceptance, autocorrelation and convergence speed of the algorithm
with a random walk Metropolis-Hastings algorithm where the proposal
distribution is a Gaussian white noise distribution.
2. To get an idea of more realistic adaptive methods, try the following: Using
a white noise proposal distribution, generate ﬁrst a sample {x1, . . . , xN},
then calculate the empirical covariance matrix C of this sample. Continue
the sampling using a proposal distribution
q(x −y) ∝exp

−1
2(x −y)TC−1(x −y)

.
Investigate the performance of this new proposal.

10
Wrapping up: hypermodels, dynamic
priorconditioners and Bayesian learning
In the previous chapters we have seen how priors give us a way to bring into
numerical algorithms our belief about the solution that we want to compute.
Since the most popular distributions used as priors depend themselves on pa-
rameters, a natural question to ask is how these parameters are chosen. While
in some cases the parameter choice might be natural and easy to justify, in
some of the applications where the subjective framework gives big improve-
ments over a deterministic approach, the values which should be chosen for
these parameters is not clear, and therefore we regard them as random vari-
ables. The procedure of introducing a distribution for the parameters deﬁning
the prior is sometimes referred to as hypermodel in the statistics literature.
Depending on the complexity of the problem, it may happen that the parame-
ters deﬁning the distribution of the parameters of the priors are also modelled
as random variables, thus introducing a nested chain of hypermodels.
To illustrate this, let’s consider ﬁrst an example from signal processing.
Example 10.1: Assume that we want to recover a signal that is known
to be continuous, except for a possible jump discontinuity at a known
location. The natural question which arises is how we can design a prior
which conveys this belief.
More speciﬁcally, let f : [0, 1] →R denote the continuous signal, and let
xj = f(tj),
tj = j
n,
0 ≤j ≤n,
denote its discretization. For simplicity, let us assume that f(0) = x0 = 0.
We denote the vector of the unknown components by x,
x =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦∈Rn.
A good candidate for a prior that favors signals with small variations is a
Gaussian ﬁrst order smoothness prior,

184
10 Wrapping up
πprior(x) ∝exp

−1
2γ2 ∥Lx∥2

,
where the matrix L is the ﬁrst order ﬁnite diﬀerence matrix,
L =
⎡
⎢⎢⎢⎣
1
−1
1
... ...
−1 1
⎤
⎥⎥⎥⎦∈Rn×n.
This prior corresponds to a Markov model,
Xj = Xj−1 + Wj,
Wj ∼N(0, γ2),
(10.1)
that is, each discrete value of the signal is equal to the previous value, up
to a white innovation process that has a ﬁxed variance γ2. The value of
the variance γ2 of the innovation term is a reﬂection of how large a jump
we expect the signal could have.
If we believe that in the interval [tk−1, tk] the signal could have a much
larger jump than at other locations, it is therefore natural to replace the
Markov model (10.1) at j = k with a model with a larger variance,
Xk = Xk−1 + Wk,
Wk ∼N

0, γ2
δ2

,
where δ < 1 is a parameter controlling the variance of the kth innovation.
It is easy to verify that this modiﬁcation leads to a prior model
πprior(x) ∝exp

−1
2γ2 ∥D1/2Lx∥2

,
where D1/2 ∈Rn×n is a diagonal matrix,
D1/2 = diag

1, 1, · · · , δ, · · · , 1

,
δ < 1,
the location of δ being ﬁxed and known.
To see the eﬀect of this modiﬁcation of the standard smoothness prior
model, we generate random draws from this distribution, setting γ = 1
and n = 100, meaning that over each subinterval except the kth one,
we expect random increments of the order ∼1. We then modify the kth
standard deviation, k = 40, to be equal to 1/δ. In Figure 10.1, we show four
random draws from the prior with values δ = 0.1 (left) and δ = 0.02 (right).
The outcomes conﬁrm that the expected size of the jump is controlled by
the size of the parameter 1/δ, since
E

(Xk+1 −Xk)2
=
&γ
δ
'2
.

10 Wrapping up
185
0
0.2
0.4
0.6
0.8
1
−20
−10
0
10
20
30
0
0.2
0.4
0.6
0.8
1
−40
−20
0
20
40
60
80
100
Fig. 10.1.
Random draws from the prior distribution with δ = 0.1 (left) and
δ = 0.02 (right) as the control parameter of the step size in the subinterval around
t = 0.4.
The construction of the prior in the previous example was simple, because
the prior information was given in a quantitative way: We believed a priori
that
1. the signal could have exactly one jump,
2. the location of the possible jump was known within the resolution of the
discretized support of the signal, and
3. an estimate for the range of the size of the jump, that is, its variance, was
available.
A much more challenging, and undoubtedly more common situation in prac-
tical applications, is that the only prior information available is inherently
qualitative. For instance, a geophysicist doing electromagnetic proﬁling of the
Earth may summarize what is known a priori about the electric conductivity
proﬁle as a function of depth with a statement of the type: “I’m expecting to
see a mildly varying conductivity proﬁle, but some discontinuities may occur.”
Thus the prior belief is clearly qualitative, since it is not expressed in terms of
numbers and mathematical formulas. The counterpart in our example would
be to expect a smooth signal, which could have a few jumps of unknown mag-
nitude at some unknown locations. The challenge is now to build a prior that
allows the data to decide how many, how large and where the jumps are. To
see how we can proceed, we revisit Example 10.1.
Example 10.2: In Example 10.1 we assumed that the signal increments
Xj −Xj−1 were equally distributed except for one. We can allow all incre-
ments to be diﬀerent by replacing (10.1) with
Xj = Xj−1 + Wj,
Wj ∼N

0, 1
θj

,
θj > 0.
This leads to a prior of the form

186
10 Wrapping up
πprior(x) ∝exp

−1
2∥D1/2Lx∥2

,
(10.2)
where D1/2 ∈Rn×n is the diagonal matrix,
D1/2 = diag

θ1/2
1
, θ1/2
2
, · · · , θ1/2
n

.
This model clearly accepts increments of diﬀerent size, the sizes being
determined by the parameters θj. If we knew the values of the θj, then we
could proceed as in Example 10.1. However, in general this is not the case,
thus we have just traded our uncertainty about the size and the location
of the jumps for the uncertainty about the value of the parameters of the
prior.
Among the questions raised by this example, a particularly intriguing -
and interesting - one is what happens if we treat the parameters θj of the
prior as variables, and whether they could be determined by the data. Before
answering, we need to be concerned about a quantity that normally does not
interest us, namely the normalizing constant of the prior density. We start
with computing the integral of the exponential on the right side of equation
(10.2). Letting
I =

Rn exp

−1
2∥D1/2Lx∥2

dx
=

Rn exp

−1
2xTLTDLx

dx,
and
R = D1/2L,
we have
I =

Rn exp

−1
2∥Rx∥2

dx.
Since the matrix R ∈Rn×n is invertible, the change of variable
Rx = z,
|det(R)|dx = dz
leads to the formula
I =
1
|det(R)|

Rn exp

−1
2∥z∥2

dz
= (
√
2π)n
|det(R)|.
The determinant of R can be expressed in terms of the parameters θj, since
det(LTDL) = det(RTR) = det(R)2,

10 Wrapping up
187
and therefore the normalized prior density is
πprior(x) =

det(LTDL)
(2π)n
exp

−1
2∥D1/2Lx∥2

.
The dependency of the normalizing constant on the parameter vector θ is
implicitly given via the determinant. It is possible to solve the explicit de-
pendency, but since the computation is rather complicated, we skip it here.
The point that we want to make here is that the price that we pay to treat
the parameter θ also as an unknown along with x is that the normalizing
constant, that depends on θ, cannot be neglected.
Let’s now express the fact that the prior depends on the parameter θ by
writing it as the conditional density
πprior(x | θ) ∝det(LTDL)1/2exp

−1
2∥D1/2Lx∥2

,
D1/2 = diag(θ)1/2,
(10.3)
where the notation emphasizes the idea that the right side is the prior provided
that the prior parameter θ is known.
Before further pursuing the idea of determining both x and θ from the data,
we discuss the computation of the determinant in the normalizing constant.
Consider the random variables Wj that deﬁne the innovations, or random
increments, in the Markov model for the signal. They are assumed to be
mutually independent, hence their probability density, assuming the variances
known, is
π(w) ∝
n

j=1
exp

−1
2θjw2
j

= exp

−1
2∥D1/2w∥2

.
Unlike the normalizing constant of the density of x, the normalizing constant
of this density has a simple explicit expression in terms of the entries of the
parameter vector θ. Indeed, since D is diagonal and the determinant of a
diagonal matrix is simply the product of its diagonal elements, we have that
π(w | θ) ∝(θ1θ2 · · · θn)1/2exp

−1
2∥D1/2w∥2

= exp
⎛
⎝−1
2∥D1/2w∥2 + 1
2
n

j=1
log θj
⎞
⎠.
Furthermore, since it is easy to construct a bijection between the incre-
ments wj and the discretized signal values xj by letting
wj = xj −xj−1,
x0 = 0,
or, in matrix notation,
w = Lx,

188
10 Wrapping up
where L is the ﬁrst order ﬁnite diﬀerence matrix. Conversely, the signal can
be recovered from the increments by observing that
xj =
j

k=1
wk,
or, in the matrix notation,
x = Bw,
where
B = L−1 =
⎡
⎢⎢⎢⎣
1
1 1
...
...
1 1 . . . 1
⎤
⎥⎥⎥⎦.
The immediate consequence of this observation is that, from the point of view
of information, it does not make any diﬀerence whether we express the prior
belief in terms of the signal or in terms of its increments. However, writing
the signal in terms of the increments simpliﬁes considerably the expression for
the prior. For this reason, we make the increments the unknowns of primary
interest.
The next issue that needs to be addressed is what is believed of the prior
for the parameters θj that will be treated themselves as random variables.
Consider a Gaussian smoothness prior for the increments and write a Bayesian
hypermodel,
π(w, θ) = πprior(w | θ)πhyper(θ),
where the hyperprior πhyper(θ) expresses our belief about θ. In the case of
our signal example, a reasonable hyperprior would allow some of the θj to
deviate strongly from the average, a situation which would correspond to the
qualitative information that the signal might contain jumps. This suggests
a fat tailed probability density that allows outliers, such as the exponential
density with a positivity constraint
πhyper(θ) ∝π+(θ) exp
⎛
⎝−γ
2
n

j=1
θj
⎞
⎠,
where π+(θ) is one if all components of θ are positive, and vanishes otherwise,
and γ > 0 is a hyperparameter.
Assuming that we have a linear observation model of the form
g(sj) = Af(tj) + ej,
1 ≤j ≤m,
whose discretized version is
b = Ax + e = ABw + e,
A ∈Rm×n,

10.1 MAP estimation or marginalization?
189
where the additive noise e is white Gaussian with variance σ2, the likelihood
model becomes
π(b | w) ∝exp

−1
2σ2 ∥b −ABw∥2

.
Thus, from the Bayes formula it follows that the posterior distribution is of
the form
π(w, θ | b) ∝π(b | w)π(w, θ)
∝exp
⎛
⎝−1
2σ2 ∥b −ABw∥2 −1
2
n

j=1
θjw2
j −γ
2
n

j=1
θj + 1
2
n

j=1
log θj
⎞
⎠.
10.1 MAP estimation or marginalization?
In general, once we have an expression for the posterior, we can proceed to
estimate the variable of primary interest by computing either the Maximum
A Posteriori estimate or the Conditional Mean estimate.
In the literature, a third way, ﬁrst estimating the prior parameters from
marginal distributions and subsequently using them to estimate the unknowns
of primary interest, is often advocated. More speciﬁcally, the third way in this
case would amount to
1. Calculating the marginal distribution
π(θ | y) =

π(w, θ | y)dw,
often referred to as the evidence of the parameter θ;
2. Estimating θ by maximizing the evidence π(θ | y),
θ = argmaxπ(θ | y);
3. Estimating w from the conditional density
π(w | θ, y) ∝π(w, θ | y).
Technical reasons make it cumbersome to implement the third way1. In
fact, since both the prior and the likelihood of w are Gaussian, after some
algebraic manipulations we arrive at the following analytic expression for the
marginal density of θ,
1 Like in the political scene of the last century, the expression third way indicates
a break from the two leading ways. Historical third ways include the Frank-
furt School and eurocommunism. Their implementation was also cumbersome for
technical reasons.

190
10 Wrapping up
π(θ | b)
∝

exp
⎛
⎝−1
2σ2 ∥b −ABw∥2 −1
2
n

j=1
θjw2
j −γ
2
n

j=1
θj + 1
2
n

j=1
log θj
⎞
⎠dw
∝
1
det(σ2(AB)TAB + D)1/2 exp
⎛
⎝−γ
2
n

j=1
θj + 1
2
n

j=1
log θj
⎞
⎠,
where D = diag(θ). Unfortunately this expression requires the evaluation of
a determinant, a step that we want to avoid. Therefore we resort to the ﬁrst
way, namely to the computation of the Maximum A Posteriori estimate.
To compute the Maximum A Posteriori estimate, we ﬁrst write the poste-
rior in the form
π(w, θ | b) ∝exp
⎛
⎝−1
2∥b −ABw∥2 −1
2∥D1/2w∥2 −γ
2
n

j=1
θj + 1
2
n

j=1
log θj
⎞
⎠,
then observe that the Maximum A Posteriori estimate is the minimizer of the
functional
F(w, θ) = 1
2
((((
 AB
D1/2

w −

b
0
((((
2
−1
2
n

j=1
log θj + γ
2
n

j=1
θj,
which can be found by solving alternatingly two simple minimization prob-
lems:
1. Fix the current estimate of the parameter of the prior, θ = θc, and update
w by minimizing the ﬁrst term of F(w, θc). The new value of w is the
solution, in the least squares sense, of the linear system
 AB
D1/2

w =

b
0

,
D = diag(θc).
2. Given the current estimate of w = wc, update θ by minimizing F(wc, θ).
The new value of θ is the solution of the equation
∇λF(wc, θ) = 0,
for which we have the analytic expression
θj =
1
w2
c,j + γ .

10.1 MAP estimation or marginalization?
191
Since 1/γ is an upper bound for θj, the hyperparameter γ eﬀectively
bounds from above the prior parameters θj. In the following example we
apply this algorithm to a deblurring problem, and discuss the inﬂuence of the
hyperparameter γ on the results.
Example 10.3: Consider a one-dimensional deconvolution problem. Let I
be the interval I = [0, 3], and a(s, t) a Gaussian blurring kernel
a(s, t) = exp

−1
2δ2 (s −t)2

,
s, t ∈I,
whose width is controlled by the parameter δ. We discretize the integral
by dividing the interval of integration into n = 150 identical subintervals
and set the width of the blurring so that the full width half value (FWHV)
is 50 pixels by choosing
δ =
50
2√2 log 2n.
and add Gaussian white noise with standard deviation 5% of the maxi-
mum of the noiseless blurred signal. We test the algorithm on a piecewise
constant signal, shown along with the blurred noisy data in Figure 10.2.
0
0.5
1
1.5
2
2.5
3
−1
−0.5
0
0.5
1
0
0.5
1
1.5
2
2.5
3
−1
−0.5
0
0.5
1
Fig. 10.2.
The true input signal (left) and the blurred noisy signal (right). The
noise level is 5% of the maximum of the noiseless signal.
Figure 10.3 shows the results obtained with the algorithm described above,
as well as the approximate solutions computed during the iterations. The
initial values for the θj were chosen as
θ0,j = 1
γ ,
γ = 10−5.
It is clear that after a few iterations the entries of θ away from the jumps
take on very high values, and that the method has no problems with de-
tecting the discontinuities even from a strongly blurred and rather noisy

192
10 Wrapping up
0
5
10
15
0
1
2
3
−2
−1
0
1
2
t
Iteration
0
5
10
15
0
1
2
3
0
5
10
x 10
4
t
Iteration
Fig. 10.3.
The evolution of the sequential iterations of the signal (left) and the
prior parameter θ (right). The value of the hyperparameter is γ = 10−5.
0
0.5
1
1.5
2
2.5
3
−1.5
−1
−0.5
0
0.5
1
1.5
1/γ =1e+001
0
0.5
1
1.5
2
2.5
3
−1.5
−1
−0.5
0
0.5
1
1.5
1/γ =1e+002
0
0.5
1
1.5
2
2.5
3
−1.5
−1
−0.5
0
0.5
1
1.5
1/γ =1e+006
0
0.5
1
1.5
2
2.5
3
−20
−10
0
10
20
30
1/γ =1e+008
Fig. 10.4.
The MAP estimates computed with various values of γ. The two es-
timates in the top row correspond to values of γ which are too large, while in the
estimate on the bottom right the parameter is too small. Observe the diﬀerence in
the scale of the bottom right ﬁgure.

10.2 Bayesian hypermodels and priorconditioners
193
signal. Figure 10.4 illustrates the role the value of the hyperparameter γ:
although the range of suitable values of γ is rather wide, ranging from
10−7 to 10−3, a large γ restricts the dynamical range of the parameters
θj excessively and the solution remains too smooth, while when γ is too
small the probability density of the smoothness parameters θj behaves as
an improper density and the MAP estimation fails. The eﬀect of γ on the
quality of the reconstructed signal is more critical when the amount of
noise is large, hence the information about the location of the jumps is
harder to extract from the data.
The previous example conforms to the rule of thumb that, while the choice
of the hyperparameter is important, the method is rather robust since the
interval of reasonable values covers several orders of magnitudes.
Observe that one could have arrived at the above iterative method via a
diﬀerent reasoning: after each update w, we re-evaluate the prior information
and update the prior density by changing θ. This process is sometimes referred
to as Bayesian learning.
10.2 Bayesian hypermodels and priorconditioners
The computation of the Maximum A Posteriori estimate using a Bayesian
hypermodel can be recast in terms of priorconditioners. This reformulation
is particularly attractive for large scale problems, where the updating of w
using direct methods may become unfeasible, or if the matrix A is not given
explicitly. In the general setting, right priorconditioners have been introduced
in the approximation of the Maximum A Posteriori estimate using iterative
methods for the solution of the associated linear system in the least squares
sense. Since the right priorconditioner comes from the factorization of the
inverse of the covariance matrix of the prior, in the case of hypermodels it
will have to be updated each time a new estimate of the parameters of the
prior becomes available. This implies that, in the computation of the Maxi-
mum A Posteriori estimate, we solve repeatedly a linear system of equations
by iterative methods using a right preconditioner which conveys the latest
information about the underlying prior.
More speciﬁcally, let Dc be the diagonal matrix constructed from the cur-
rent approximation θc of θ and consider the problem of minimizing
1
2∥ABw −b∥2 + 1
2∥D1/2
c
w∥2
with respect to w. Since D1/2
c
is invertible, we can introduce
z = D1/2
c
w,
w = D−1/2
c
z,
and compute an approximate solution z+ of the linear system

194
10 Wrapping up
ABD−1/2
c
z = b
by an iterative linear system solver. Letting
w+ = D−1/2
c
z+
and
x+ = Bw+,
we can update the parameters of the prior according to the formula
θ+,j =
1
w2
+,j + γ ,
and compute a new priorconditioner D1/2
+
= diag{θ+,1, . . . , θ+,n), to be used
in the iterative solution of the linear system. It should be pointed out that
the ill-conditioning of the linear system that we solve and the presence of
noise in the right hand side require that we stop iterating before the ampli-
ﬁed noise components dominate the computed solution, and that we use an
iterative solver suited for numerically inconsistent systems. We ﬁnally remark
that the updating of the priorconditioner in the context of Bayesian hyper-
priors is philosophically in line with the idea of ﬂexible preconditioning for
the GMRES method, where the change of the preconditioner reﬂects new ac-
quired knowledge about the system. We conclude by revisiting Example 10.1,
where we now use priorconditioned iterative methods for the restoration of
the blurred and noisy signal.
0
5
10
0
1
2
3
−1
0
1
2
t
Iteration
0
5
10
0
1
2
3
0
5
10
x 10
4
t
Iteration
Fig. 10.5. The evolution of the dynamically priorconditioned CGLS iterations of
the signal (left) and the prior parameter θ (right).
Given the blurred and noisy signal of Example 10.1, we use the CGLS
iterative method for the solution of the linear system
ABw = b

10.2 Bayesian hypermodels and priorconditioners
195
with right preconditioner
D1/2 = diag{θ1/2
1
, . . . , θ1/2
n }.
In our computation we use the CGLS algorithm, allowing up to 15 iteration
steps to reduce the residual below a tolerance of 10−3. At the end of each
CGLS sweep we use the new approximation of the signal increments to
update the θj, hence the associated priorconditioner. The actual number
of iteration steps needed to attained the desired residual reduction remains
consistently below the maximum allowed, and decreases with each sweep.
Figure 10.5 shows that the evolution of the computed solution is analogous
to that obtained when computing the Maximum A Posteriori estimate
directly.


References
[GR96]
Gilks, W.R., Richardson, S. and Spiegelhalter, D.J.: Markov Chain Monte
Carlo in Practice. Chapman&Hall/CRC Press, Boca Raton (1996)
[Gh96]
Ghahramani, S.: Fundamentals of Probability. Prentice Hall (1996)
[GV89]
Golub, G. and van Loan, C.F.:Matrix Computations. The John Hopkins
University Press, Baltimore (Third edition) (1989).
[Ha98]
Hansen, P.C.: Rank-Deﬁcient and Discrete Ill-Posed Problems. SIAM,
Philadelphia (1998)
[Je04]
Jeﬀrey, R.: Subjective Probability: The Real Thing. Cambridge University
Press, Cambridge (2004)
[Jo02]
Joliﬀe, I.T.: Principal Component Analysis. Springer Verlag, New York
(Second edition) (2002)
[KS05]
Kaipio, J. and Somersalo, E.: Statistical and Computational Inverse Prob-
lems. Springer Verlag, New York (2005)
[PT01]
Press, S.J. and Tanur, J.M.: The Subjectivity of Scientists and the Bayesian
Approach. John Wiley&Sons, Inc., New York (2001)
[Sa03]
Saad, Y.: Iterative Methods for Sparse Linear Systems. SIAM, Philadelphia
(Second edition) (2003)
[Ta05]
Tarantola, A.: Inverse Problem Theory. SIAM, Philadelphia (New edition)
(2005)
[TB97]
Trefethen, L.N. and Bau, D.: Numerical Linear algebra. SIAM, Philadel-
phia (1997)


Index
A–conjugate direction, 71
ATA–conjugate direction, 72
α–coin, 104, 171
absolutely continuous, 7
acceptance rate, 103, 174
acceptance ratio, 104, 171
adaptation, 181
aliasing, 89
anomalies, 118, 122
approximate solution, 70
Arnoldi process, 74, 75
Arnoldi relation, 76
autocorrelation, 176
autoregressive Markov model, 52
averaging, 22
bacteria, 4, 51, 94
balance equation, 170
bandwidth, 44
Bayes formula, 10, 55
Bayesian probability, 3
burn-in, 174
calibration, 29
candidate generating kernel, 170
CCD, Charged Coupled Device, 14, 17
Central Limit Theorem, 16, 29, 92, 121,
175
CG algorithm, 72
CGLS algorithm, 73
Cholesky decomposition, 93
Cholesky factorization, 57, 63
condition number, 78, 81
conditional covariance, 134
conditional density, 56
conditional expectation, 11
conditional mean, 11, 134
Conditional Mean estimate, 57, 189
conditional probability, 6
conditional probability density, 10, 48
conditioning, 40
conﬁdence, 25
Conjugate Gradient method, 70
Conjugate Gradient method for Least
Squares, 72
convergence criterion, 70
convergence rate, 108
convolution, 67
correlation coeﬃcient, 9
correlation length, 176
counting process, 14
covariance, 9
covariance matrix, 11
credibility, 25
credibility ellipse, 25
credibility interval, 38
cumulative distribution function, 99
deconvolution, 81, 114, 123, 157, 191
detailed balance equation, 170
direct menthod, 63
discrepancy, 73
Discrete Fourier Transform (DFT), 68
discrete time stochastic process, 163
eigenvalue decomposition, 23, 36, 78,
110, 124

200
Index
empirical Bayes methods, 51
empirical covariance, 23
empirical mean, 23
equiprobability curves, 24, 25, 27
error function, 101
error function, inverse of, 101
event, 6
evidence, 189
expectation, 8
expectation, Poisson process, 15
extrapolation, 144
false positive, 58, 125
Fast Fourier Transform (FFT), 67
ﬁnite diﬀerence matrix, ﬁrst order, 115,
184, 188
ﬁnite diﬀerence matrix, second order,
54, 135
formally determined problem, 61
frequentist statistics, 3
fudge factor, 81
Gaussian approximation, 17, 19, 24, 45,
48, 119, 180
Gaussian distribution, 16, 32
Gaussian elimination, 63
General Minimal Residual method, 74
GMRES algorithm, 76
Golden Rule, 100
gravitational force, 144
hard subspace constraint, 121
Hessenberg matrix, 76
hidden parameter, 35
hyperparameter, 188, 192
ignorance, 22, 154
ill-conditioned matrix, 81
ill-conditioning, 77
ill-determined rank, 119
independent events, 6
independent random variables, 9
independent realizations, 31
independent, identically distributed
(i.i.d.), 16, 29, 47, 52
innovation process, 52
interpolation, 134
invariant density, 168
invariant distribution, 164
inverse crime, 160
Inverse Cumulative Distribution Rule,
100
Inverse Discrete Fourier Transform
(IDFT), 89
inverse problem, 1, 41, 49, 55
inverse problem, linear, 147
iterative method, 63
iterative solver, 67
Jacobian, 46
joint probability density, 9, 40
knowledge, 48
Krylov subspace, 70
Krylov subspace iterative methods, 70,
108
Kullback–Leibler distance, 18
lack of information, 3, 39
Laplace transform, 159
Law of Large Numbers, 22, 23, 30, 33,
34, 92
least squares solution, 62, 64, 76, 78
likelihood, 41
likelihood function, 32
limit distribution, 164
linear system, 56, 58, 61, 67, 107, 111,
132, 153, 193, 194
log-likelihood function, 32
log-normal distribution, 38, 47
marginal density, 10, 55
marginalization, 40
Markov chain, 166
Markov Chain Monte Carlo, 99, 161
Markov process, 163
Matlab code, 27, 53, 69, 81, 87, 96, 97,
101, 102, 141, 173
Maximum A Posteriori estimate, 115,
189
Maximum A Posteriori estimator, 57,
84
Maximum Likelihood, 31
Maximum Likelihood estimate, 45
Maximum Likelihood estimator, 31, 50,
57, 84
Metropolis-Hastings, 102
minimization problem, 32, 34, 36, 57,
70, 72–74, 76, 109, 116, 193

Index
201
minimum norm solution, 62, 67
mixed central moment, 9
model reduction, 120, 128
moment, kth, 8
Monte Carlo integration, 92
Morozov discrepancy principle, 81, 85
multivariate Gaussian random variable,
35
multivariate random variable, 11
noise reduction, 28
noise, additive, 42
noise, multiplicative, 45
noise, Poisson, 45
non-parametric, 21, 50
normal distribution, 16
normal equations, 64, 72, 85, 87
normal random variable, 16
nuisance parameter, 41
numerical diﬀerentiation, 85
odds, 3
oﬀ-centered covariance, 126
orthogonal random variables, 10
outlier, 121
overdetermined problem, 61, 150
overregularization, 87
parametric, 21, 50
Paris, 39
penalty term, 58, 110
pointwise predictive output envelope,
140, 158
Poisson density, 17, 33, 45
Poisson distribution, 102
Poisson process, 14, 44
positive deﬁnite, 28, 64
positive semi-deﬁnite, 12
posterior density, 55
Posterior Mean estimate, 57
preconditioner, 107
preconditioner, left (right), 108
preconditioning, 77
preprior, 152
Principal Component Analysis (PCA),
118, 128
prior distribution, 51
prior information, 40
priorconditioner, 108, 194
probability density, 6, 7
probability distribution, 7
probability measure, 6
probability space, 6
proposal distribution, 99, 104, 170
pseudoinverse, 65, 67, 76, 78
QR factorization, 65
qualitative vs. quantitative information,
50
random variable, 2, 7
random walk, 161
Rayleigh distribution, 19, 37
reality, 41
regularization, 57, 80
regularization by truncated iteration, 80
regularization parameter, 85
rejection sampling algorithm, 103
residual error of the normal equations,
73
sample covariance, 28
sample history, 176
sampling, 91
Schur complement, 131, 133, 148
search direction, 71
semiconvergence, 79, 83
sensitivity, 80
singular value decomposition, 65, 77,
85, 87, 119
smoothness prior, ﬁrst order, 115, 183,
188
smoothness prior, second order, 54, 141,
160
standard normal random variable or
distribution, 16
statistical inference, 2, 21
stopping rule, 80
structural prior, 94
subjective probability, 1, 2
surrogate distribution, 102
Tikhonov functional, 57
Tikhonov regularization, 58, 85, 109,
116, 150
time invariant kernel, 166
Toeplitz matrix, 44
training set, 118

202
Index
transition kernel, 166
transition matrix, 162
transition probability, 162
truncation index, 116
uncorrelated random variables, 10
underdetermined problem, 61, 114, 150
underregularization, 87
variance, 8
variance, Poisson process, 15
weighted least squares problem, 37
white noise, 93
whitened PCA model, 121
whitening, 92
whitening matrix, 93
Wiener ﬁltering, 150

