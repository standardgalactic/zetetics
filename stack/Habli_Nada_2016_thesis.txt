Nonparametric Bayesian modelling in Machine Learning
Nada Habli
Thesis submitted to the Faculty of Graduate and Postdoctoral Studies
in partial fulﬁllment of the requirements for the degree of Master of Science in
Mathematics 1
Department of Mathematics and Statistics
Faculty of Science
University of Ottawa
c⃝Nada Habli, Ottawa, Canada, 2016
1The M.Sc. program is a joint program with Carleton University, administered by the Ottawa-
Carleton Institute of Mathematics and Statistics

Abstract
Nonparametric Bayesian inference has widespread applications in statistics and ma-
chine learning. In this thesis, we examine the most popular priors used in Bayesian
non-parametric inference.
The Dirichlet process and its extensions are priors on
an inﬁnite-dimensional space. Originally introduced by Ferguson (1983), its conju-
gacy property allows a tractable posterior inference which has lately given rise to
a signiﬁcant developments in applications related to machine learning. Another yet
widespread prior used in nonparametric Bayesian inference is the Beta process and
its extensions. It has originally been introduced by Hjort (1990) for applications in
survival analysis. It is a prior on the space of cumulative hazard functions and it
has recently been widely used as a prior on an inﬁnite dimensional space for latent
feature models.
Our contribution in this thesis is to collect many diverse groups of nonpara-
metric Bayesian tools and explore algorithms to sample from them. We also explore
machinery behind the theory to apply and expose some distinguished features of these
procedures. These tools can be used by practitioners in many applications.
ii

Acknowledgements
There are many people who provided much support and guidance throughout the
lengthy course of this thesis to whom I am thankful.
First and foremost I am deeply indebted to my supervisor Dr. Mahmoud Zare-
pour who guided and inspired my work. I would like to express my sincere gratitude
to him for redirecting my interest of research and introducing me to machine learn-
ing. It turns out to be a perfect ﬁt to my background study in Computer Science. I
attribute the level of my Masters degree to his encouragement and eﬀort and without
him this thesis would not have been completed or written. It was a privilege to have
studied and researched under the guidance of this word-class scientist and professor.
Besides my supervisor, I am grateful to Dr. Luai Al Labadi for his continuous
support and advice. He was a past PhD student of Dr. Zarepour and most of his
contributions are in the area of nonparametric Bayesian inference. I sincerely appre-
ciate his support over the phone and the back and forth emails we shared together.
Without his support I would not be able accomplish this much in my thesis.
To my dear colleagues in lab B03 of the department of Mathematics, in partic-
ular Maryam, Jo-Ann, Farid, Sheikh, Erv´e, Rachid, Hicham, Jason and Ibrahim. I
thank you for the opportunity of getting to know such broad-minded, wise and fun
researchers and for your assistance throughout my studies. I have been blessed with
a friendly and cheerful group of fellow students who provided a much needed form of
escape from my studies and for helping me keep things in perspective.
iii

Acknowledgements
iv
I acknowledge and greatly appreciate the ﬁnancial support from the University of
Ottawa for the Admission Scholarship as well as the Swartzen Memorial Scholarship
oﬀered by the department of Mathematics.
Last but not least, I would like especially to thank my family which consists
of my mother, my father, my brother and my sisters.
My hard-working parents
have sacriﬁced their lives for my siblings and me while providing unconditional love
and care.
I love them so much, and I would not have made it this far without
them. My brother, Omar and his wife Irialis have been my greatest support during
the diﬃcult moment in my life and I love them dearly. They have given me their
unequivocal support throughout for which my sincere expression of thanks likewise
does not suﬃce. I know I always have my family to count on when times are rough.

Dedication
To my gorgeous lovely children, Maya, Adam, Jad and Issam.
I love you all deeply and dearly.
v

Contents
List of Figures
ix
List of Tables
xiii
1
Introduction
1
2
L´evy random variables and processes
4
2.1
L´evy process . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.2
Characteristic function
. . . . . . . . . . . . . . . . . . . . . . .
5
2.2.1
Inﬁnitely divisibility
. . . . . . . . . . . . . . . . . . . . . .
5
2.2.2
L´evy-Khintchine deﬁnition . . . . . . . . . . . . . . . . . . .
6
2.2.3
L´evy-Itˆo deﬁnition
. . . . . . . . . . . . . . . . . . . . . . .
7
2.2.4
Transformation of Poisson Random measure . . . . . . . . .
12
2.3
L´evy random variables . . . . . . . . . . . . . . . . . . . . . . . .
14
2.3.1
Poisson random variable . . . . . . . . . . . . . . . . . . . .
14
2.3.2
Gamma random variable . . . . . . . . . . . . . . . . . . . .
15
2.3.3
Stable random variable . . . . . . . . . . . . . . . . . . . . .
17
3
Gamma and Dirichlet Process
19
3.1
Gamma Process . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.1.1
Deﬁnition of Gamma process
. . . . . . . . . . . . . . . . .
19
3.1.2
Series representation of the Gamma process
. . . . . . . . .
20
vi

CONTENTS
vii
3.1.3
Approximation of the Gamma process
. . . . . . . . . . . .
21
3.2
Dirichlet process . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.2.1
Deﬁnition of the Dirichlet distribution
. . . . . . . . . . . .
28
3.2.2
Deﬁnition of the Dirichlet process . . . . . . . . . . . . . . .
30
3.2.3
Series representation of the Dirichlet process . . . . . . . . .
31
3.2.4
Approximation of the Dirichlet process . . . . . . . . . . . .
32
4
Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
45
4.1
Two-parameter Poisson-Dirichlet process
. . . . . . . . . . . . .
45
4.1.1
Deﬁnition of the two-parameter Poisson-Dirichlet process
.
45
4.1.2
Approximation of the two-parameter Dirichlet process . . . .
47
4.2
Normalized inverse-Gaussian process (NIGP) . . . . . . . . . . .
49
4.2.1
Deﬁnition of the normalized inverse-Gaussian process (NIGP)
51
4.2.2
Series representation of the NIGP . . . . . . . . . . . . . . .
52
4.2.3
Stick-breaking representation of the NIGP . . . . . . . . . .
52
4.2.4
Approximation of the NIGP . . . . . . . . . . . . . . . . . .
53
4.2.5
Al Labadi & Zarepour approximation of the NIGP
. . . . .
54
5
Beta process
58
5.1
Beta process . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
5.1.1
Deﬁnition of the Beta process . . . . . . . . . . . . . . . . .
58
5.1.2
Series representation of the Beta process . . . . . . . . . . .
59
5.1.3
Stick-breaking representation of the Beta process
. . . . . .
60
5.1.4
Finite Approximation of the Beta process
. . . . . . . . . .
61
5.2
Beta-Bernoulli process . . . . . . . . . . . . . . . . . . . . . . . .
66
5.2.1
Deﬁnition of the Beta-Bernoulli process . . . . . . . . . . . .
66
5.2.2
Series representation of the Beta-Bernoulli process . . . . . .
67

CONTENTS
viii
5.2.3
Beta process conjugate prior for the Bernoulli process . . . .
68
5.3
Applications in Latent Feature Model . . . . . . . . . . . . . . .
70
5.3.1
Matrix Z . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
5.3.2
Updating the matrix Z using Methodology I . . . . . . . . .
73
5.3.3
Updating the matrix Z using Methodology II
. . . . . . . .
76
5.3.4
Nonparametric Latent Feature Models for Link Prediction
.
84
5.3.5
Basic model . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
A Deﬁnitions of background knowledge
86
Bibliography
88

List of Figures
3.1 One sample path of the Gamma process, GBond ∼GP(a, H) using
Bondeson (1982) approximation. We choose a = 5 and H ∼t(5).
By choosing ϵ = 0.0001, and following the stopping rule in (3.1.4),
we get n = 30. The x-axis represents the set of i.i.d. atoms gener-
ated from θi ∼H in increasing order and the y-axes represents the
corresponding Gamma process. We display in the same plot, the
weights in (3.1.5) as vertical lines at the corresponding atoms t = θi
for i = 1, . . . , n.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.2 One sample path of the Gamma process using Zarepour & Al Labadi
(2012) approximation. For comparison purposes we use the same
parameter used in Figure 3.1, particularly we choose a = 5 and
H ∼t(5).
Choosing ϵ = 0.0001 and using the stopping rule in
(3.1.8), we get n = 14. The plot shows as well the weights calculated
in (3.1.7) as vertical lines.
. . . . . . . . . . . . . . . . . . . . . . .
25
ix

LIST OF FIGURES
x
3.3 Ten sample paths of the Gamma process approximation with a = 5,
H = t(5) and ϵ = 0.0001.
The plot at the top of the ﬁgure
uses the Bondesson (1982) Gamma approximation and the plot at
the bottom uses the Zarepour and Al Labadi (2012) Gamma pro-
cess approximation. The truncation values for Bondeson (1982) are
n = 45, 25, 42, 40, 39, 41, 7, 52, 39 and 27, one value for each path.
Whereas the truncation values for Zarepour and Al Labadi (2012)
are found to be n = 14, 10, 10, 10, 11, 12, 8, 11, 10 and 9, for the same
tolerance value ϵ = 0.0001.
. . . . . . . . . . . . . . . . . . . . . . .
27
3.4 One sample path of the Dirichlet process approximated using Bon-
desson (1982) approximation. We choose a = 5 and H ∼t(5). For
ϵ = 0.0001, we get n = 25. Vertical lines at diﬀerent location θ(i)
represent the weights in calculated (3.2.4).
. . . . . . . . . . . . . .
34
3.5 Ten sample paths of the Dirichlet process using Bondesson (1982)
approximation with a = 5, H ∼t(5).
For ϵ = 0.0001, we get
n = 26, 31, 23, 26, 27, 36, 25, 28, 27 and 24. . . . . . . . . . . . . . . .
35
3.6 One sample path of Sethuraman (1994) Dirichlet process approxima-
tion, P Seth((−∞, t]) ∼DP(a, H). We choose a = 5 and H ∼t(5).
For ϵ = 0.0001 and using the stopping rule in (3.2.8), we get n = 45.
37
3.7 Ten sample paths of the Dirichlet process approximated by Sethura-
man (1994) stick breaking approach. We choose a = 5 and H ∼t(5).
For ϵ = 0.0001 and using the stopping rule in (3.2.8), the values of
n for each path is n = 30, 38, 57, 44, 54, 52, 21, 31, 43 and 54. . . . . .
38
3.8 One sample path of the Dirichlet process approximated by Zarepour
& Al Labadi (2012) approximation of the Dirichlet process with
a = 5, H ∼t(5). For ϵ = 0.0001, we get n = 12 . . . . . . . . . . . .
41

LIST OF FIGURES
xi
3.9 Ten paths of Zarepour & Al Labadi (2012) approximation of Dirich-
let process with a = 5, H ∼t(5). For ϵ = 0.0001 the value of n
is equal to n = 15, 11, 13, 10, 7, 10, 8, 6, 7 and 7, one for each sample
path. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.10 Solid step functions show the Dirichlet process prior using the Zare-
pour & Al Labadi (2012)’s approximation with a = 5, H ∼t(5) and
n = 1000. The solid line is the actual cumulative distribution of
the data set which is in our case Normal(−2, 1). Top to bottom
plot shows the posterior distribution of the Dirichlet process after
observing m = 5, 20 and 200 data points respectively.
. . . . . . . .
44
4.1 One sample path of the two parameter Poisson-Dirichlet process
with H ∼t(5), α = 0.5 and a = 1. the x-axis shows atoms gen-
erated from θi ∼H in increasing order and the y-axes represent
the resulting two parameter Poisson-Dirichlet process. Vertical lines
represent the intensity of the weights calculated from (4.1.2). . . . .
50
4.2 The plot at the top shows one sample path of the NIGP(1, t(5))
using the stick breaking approach with n = 100. The choice of n in
this plot is chosen to be relatively large. The vertical lines show the
weights in (4.2.4). The plot at the bottom depicts one sample path
of NIGP(1, t(5)). Choosing ϵ = 0.0001, we get n = 3 based on the
stopping rule in (4.2.5). . . . . . . . . . . . . . . . . . . . . . . . . .
55
4.3 One sample path of the normalized inverse Gaussian process with
H ∼t(5), a = 1 and n = 50. The NIGP(t(5), 1) is sampled using
Al Labadi & Zarepour (2014) approximation
. . . . . . . . . . . . .
57

LIST OF FIGURES
xii
5.1 One sample path of the Beta process B ∼BP(0.8,Uniform(0, 1)).
The Beta process is approximated using Al Labadi & Zarepour
(2014) algorithm ( Algorithm B) with n = 15. The plot shows as
well the weights in (5.1.4) by vertical lines at the associated atoms θi.
63
5.2 The plot at the top shows ten sample paths of the Beta process
with c = 1 and B0 ∼Uniform(0, 1). The plot at the bottom shows
ten sample paths of the Beta process with same base measure B0
but with c = 20. We use Algorithm B to approximate the Beta
process in both plots with n = 100. The dashed line connected by
dots in both plots represents the cumulative distribution of the base
measure B0 ∼Uniform(0, 1). . . . . . . . . . . . . . . . . . . . . . .
65
5.3 The plot at the top depicts one sample path of the Beta process with
c = 1, B0 ∼Uniform(0, 1). The Beta process is approximated using
Algorithm B with n = 15. The vertical lines shows the intensity of
the weights in (5.1.4). The plot at the bottom shows 10 draws of
the Bernoulli processes, one per line, with base measure the Beta
process (displayed at the top of the ﬁgure). . . . . . . . . . . . . . .
69
5.4 The plot at the top shows one draw of the Beta process B ∼
BP(1, Uniform(0, 1)) approximated by Algorithm B with n = 15.
The plot at the bottom shows 10 draws of the Beta-Bernoulli pro-
cess with base measure B. Draws are represented in the plot by
dots at each pair of the form (bk = 1, θi) generated from the Beta-
Bernoulli process. The plot at the bottom shows as well one updated
draw of the Beta-Bernoulli process given the 10 other observations.
Triangle pointed down represent the update contributed by the dis-
crete base, and triangle pointed up represent the update contributed
by the continuous part of the updated Beta process. . . . . . . . . .
83

List of Tables
5.1 The table shows the ﬁrst nine set of pairs (pk, θk)1≤k≤9 extracted
from a draw of the Beta process, B ∼BP(1, Uniform(0, 1)). The
Beta process is approximated using Algorithm B with n = 15. . . . .
71
5.2 The table depicts the ﬁrst nine values (bk, θk)1≤k≤9 extracted from a
draw of a Bernoulli process with base measure B ∼BP(1, Uniform(0, 1)).
Recall that bk ∼Binomial(pk), where pk is the probability displayed
in Table 5.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
5.3 The table shows some preliminary values of the pairs (pCont
k
, θ
′
k) ex-
tracted from BCont ∼BP(11, 1
11Uniform(0, 1)). . . . . . . . . . . . .
79
5.4 The table depicts some preliminary values of the pairs (bCont
k
, θ
′
k)
extracted from S ∼BeP(BCont), the Beta-Bernoulli process with
base measure BCont. . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5.5 The table shows the set of pairs (pDisc
k
, θk)1≤k≤3 such that pDisc
k
is
calculated based on (5.3.6). . . . . . . . . . . . . . . . . . . . . . . .
81
5.6 The table shows the set of pairs (bDisc
k
, θk)1≤k≤3, where bDisc
k
∼
Bernoulli(pDisc
k
). . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
xiii

Chapter 1
Introduction
In most data analysis that involves statistical inference, we often observe some set of
data where we wish to ﬁt a statistical model to be able to infer about its characteristic.
Such characteristic can be as simple as estimating the mean of the data or as complex
as estimating its entire distribution. Regardless of the complexity of the information
we want to extract from the data, we need to construct a statistical model that ﬁts
the data. This requires estimating a set of parameters that govern the underlying
physical setting of the measured data. There exist two main and distinct approaches
to tackle this problem, namely Frequentist and Bayesian statistics.
The Frequentist approach to statistics considers probability as a limiting long run
frequency. In particular, data are a repeatable random sample where we believe that
the underlying parameters remain constant (or ﬁxed) during this repeatable process.
On the other hand, the Bayesian approach to statistics considers the parameters φ as
being random, hence they are assigned a prior distribution p(φ). The observed data
X is then used to update our prior belief for each unknown parameter via the Bayes
rule p(φ|X) ∝p(X|φ)p(φ), where p(φ|X) is known as the posterior distribution. A
parametric Bayesian inference is used when the set of parameters governing the data
is ﬁnite. However, this could be restrictive as a model when we observe more and
1

1. Introduction
2
more data. We want to have a model that grows in complexity when we observe more
data. One way to overcome this problem is to use a non-parametric approach. In the
Bayesian framework, this approach allows us to put a prior on an inﬁnite dimensional
parameter space. The choice of a prior distribution has been carefully and widely
discussed in Bayesian inference. The main reason is that we need to construct a prior
which will lead to predictive models such that we know how to sample from the prior
and posterior distribution. The most practical and useful priors are conjugate priors.
We say that the prior is a conjugate prior for the likelihood if the posterior has the
same distributional form as the prior distribution. The conjugacy property is very
useful from a computation point of view because it will be straight forward to sample
from the posterior.
In this thesis, we highlight many diverse groups of nonparametric Bayesian priors
and explore algorithms to sample from these them. The outline of this thesis is as
follows: In Chapter 2 we review some preliminary theory for a L´evy random variable
and its characteristic function. We introduce an example of L´evy random variables
along with their L´evy measures. In Chapter 3 we introduce two well known priors; the
Gamma process and the Dirichlet process. The former has been recently applied to
exchangeable models of sparse graphs in Caron & Fox (2014) and for non-parametric
ranking models in Caron et al. (2013). It also has been used as a prior for inﬁnite-
dimensional latent indicator matrices in Titsias (2008).
The latter application is
one of the earliest Bayesian non-parametric approach to infer on latent (or hidden)
feature models, in particular when each feature occurs multiple times for a data point,
in contrast of being simply binary. The Dirichlet process is a prior on an inﬁnite-
dimensional space. It has commonly been used as prior on latent class models, in
particular in clustering and mixture models. For more details refer to Teh and Jordan
(2013) and Teh (2010). In Chapter 4, we describe in detail the two-parameter Poisson
Dirichlet process and the normalized inverse Gaussian process. The former is also
known as the Pitman-Yor process and is a generalization of the Dirichlet process.

1. Introduction
3
Unlike the Dirichlet and Beta process, the two-parameter Dirichlet process does not
have the conjugacy property.
We present the main contribution of this thesis in
Chapter 5. We ﬁrst introduce a diﬀerent approximation of the Beta process along with
algorithms to sample from it. We then introduce an extension to the Beta process
known as the Beta-Bernoulli process.
We describe two methodologies to sample
from the Beta-Bernoulli process where one of them has been known in the Computer
Science community by the Indian buﬀet process (IBP) (Ghahramani & Griﬃths, 2005)
and the other method is our contribution in this thesis. We describe a new sampling
technique which focuses on the accuracy and eﬃciency of the approximated Beta-
Bernoulli process. Finally, we present the sampling technique of the Beta-Bernoulli
process on a simulated example.

Chapter 2
L´evy random variables and
processes
2.1
L´evy process
In this chapter, we present some preliminary discussion on L´evy processes and their
applications in nonparametric Bayesian inference.
Deﬁnition 2.1.1 (L´evy process) A stochastic process X = {Xt : t ≥0} deﬁned
on a probability space (Ω, F, P) is called a L´evy process if the following properties
hold:
1. The paths of X are P-right continuous with left limits;
2. Xt starts at 0, i.e., P(X0 = 0) = 1 a.s.
3. Xt has independent increments, i.e., the random variables Xt0, Xt1 −Xt0, . . .,
Xtn −Xtn−1 are independent for all 0 ≤t0 < t1 < · · · < tn, for n ≥1;
4. Xt has stationary increments, i.e., Xt+s −Xt
d= Xs for all s, t ≥0; and
4

2. L´evy random variables and processes
5
5. Xt is stochastically continuous, i.e., for all s ≥0, Xt
p→Xs as t →s, or
equivalently
lim
t→s Pr{|Xt −Xs| > ϵ} = 0
.
Throughout this thesis, ”
d= ” and ”
p→” denote equality in distribution and conver-
gence in probability, respectively. Note that the ﬁfth property does not imply that
the sample paths are continuous.
Deﬁnition 2.1.2 (Subordinators) Let X = {Xt : t ≥0} be a L´evy process deﬁned
on a probability space (Ω, F, P). Then X is called a subordinator if the following
properties hold:
1. Xt is a L´evy process deﬁned on R+;
2. Xt is a.s. non-negative; and
3. Xt is a.s. non-decreasing.
2.2
Characteristic function
2.2.1
Inﬁnitely divisibility
Deﬁnition 2.2.1 (Inﬁnitely Divisible) A real-valued random variable Θ has an
inﬁnitely divisible distribution if for each n = 1, 2, . . . there exist a sequence of i.i.d.
random variable Θ1,n, . . . , Θn,n such that
Θ
d= Θ1,n + · · · + Θn,n.
Based on this deﬁnition, one way to determine whether a given random variable
has an inﬁnitely divisible distribution is by checking its characteristic exponent. As-
suming Θ has the characteristic exponent ψ(u) := −log E(eiuΘ) for all u ∈R, then

2. L´evy random variables and processes
6
Θ is an inﬁnitely divisible distribution if, for all n ≥1, there exist a characteristic
exponent of a probability distribution ψn such that ψ(u) = nψn(u) for all u ∈R.
From the deﬁnition of a L´evy process, we conclude that for any t > 0, Xt is
a random variable belonging to the class of inﬁnitely divisible distributions. This
follows from the fact that Xt has stationary independent increments and therefore for
any n = 1, 2, . . .,
Xt
d= Xt/n + (X2t/n −Xt/n) + · · · + (Xt −X(n−1)t/n).
Suppose now that for all u ≥0 and t ≥0, we deﬁne
ψt(u) = −log E(eiuXt).
(2.2.1)
Then, using (2.2.1), for any m, n ≥0, we can easily get,
nψ1(u) = ψn(u) = mψn/m(u).
Hence, for any t > 0 rational,
ψt(u) = tψ1(u),
(2.2.2)
where ψ1 is the characteristic exponent of X1. If t is a irrational, choosing a decreasing
sequence of rational {tn : n ≥1} such that tn ↓t as n →∞along with the a.s. right
continuity of Xt implies right continuity of exp{−ψt(u)}. In which case, (2.2.2) holds
for all t ≥0.
2.2.2
L´evy-Khintchine deﬁnition
Theorem 1 (L´evy-Khintchine Theorem) [Fristedt and Gray, 1996] There is a
one-to-one correspondence between all inﬁnitely divisible distributions (and therefore
L´evy processes) Xt and the set of triples (a, σ, ν) where a ∈R, σ ∈R+, and ν is a
measure concentrated on R\{0} satisfying
R
(1 ∧x2)ν(dx) < ∞, such that for all
E[eiuXt] = exp(−ψ(u, t)),

2. L´evy random variables and processes
7
where
ψ(u, t) = −iaut + tσ2u2
2
+ t
Z
R\{0}
(1 −eiux + itu1(|x|<1))ν(dx).
(2.2.3)
In (2.2.3) ν is called the L´evy measure.
For subordinator, there is a one-to-one
correspondence between all inﬁnitely divisible functions on R+ and pairs (b, ν) such
that for all t, the Laplace transform of Xt is
E[e−uXt] = exp[−but −t
Z ∞
0
(1 −e−ux)ν(dx)],
where b ∈R+ and ν is a measure on R+ satisfying
R
(x ∧1)ν(dx) < ∞.
Here and throughout this thesis, we deﬁne (x ∧1) := min(x, 1).
2.2.3
L´evy-Itˆo deﬁnition
Theorem 2 (L´evy-Itˆo decomposition) [Fristedt and Gray, 1996] Let Xt be a L´evy
process on R with triple (a, σ, ν) as described in Deﬁnition 1. Let (Y, W) be an inde-
pendent pair where W is standard Brownian motion and Y is a Poisson point process
in (0, ∞)×(R\{0}) whose intensity measure is λ×ν where λ is the Lebesgue measure.
Then, there exists a sequence of ϵk ↓0 such that
Xt
d= at + σWt + lim
k→∞
 Z
(−∞,−ϵk)]∪[ϵk,∞)
yY ((0, t] × dy) −
Z
(−∞,−ϵk]∪[ϵk,∞)
yν(dy)

.
When Xt is a subordinator, then ν is a measure satisfying
R ∞
0 (x ∧1)ν(dx) < ∞, and
the L´evy-Itˆo decomposition simpliﬁes to
Xt
d= bt +
Z
yY ((0, t] × dy),
(2.2.4)
where the pair (b, ν) is described in Deﬁnition 1 and Y is a Poisson process in (0, ∞)×
(0, ∞) whose intensity measure is λ × ν.
We reinterpret Xt in (2.2.4) to be X(0, t], the measure assigned to the interval
(0, t]. Since Xt can be constructed from a non-negative Poisson process, X is a random
measure. In general, we deﬁne X(A) =
R
A dXt for any Borel set A.

2. L´evy random variables and processes
8
Also, using the convention of the nonparametric Bayesian community, we redeﬁne
ν to be what was previously the product measure λ × ν. By doing so, we can allow
λ to be diﬀerent from the Lebesgue measure on some space Ω. If λ is not a multiple
of Lebesgue measure, the resulting process will not be a L´evy process since it will
not have stationary increments. Instead, it will deﬁned to be a completely random
measure.
Deﬁnition 2.2.2 (Completely random measure) [Kingman, 1967] A random mea-
sure Θ is a completely random measure if, for any ﬁnite collection A1, . . . , An of
disjoint sets, the random variables Θ(A1), . . . , Θ(An) are independent.
Deﬁnition 2.2.3 (Random measure) [Resnick, 1987] Let E be a Polish space and
B(E) be the Borel σ-algebra generated by the open sets in E. A measure µ is called
Radon if µ(K) < ∞for any compact set K in E. Let M+(E) be the space of Radon
measures in E. Let M+(E) be the smallest σ-algebra of subsets of M+(E) making
the maps µ →µ(f) =
R
f(x)dµ(x) from M+(E) to R measurable for all functions
f ∈C+
K(E), where C+
K(E) denotes the set of continuous functions f : E →[0, ∞) with
compact support. Note that, M+(E) is the Borel σ-algebra generated by the topology
of vague convergence. A random measure on E is any measurable map ξ deﬁned on
a probability space (Ω, F, P) with values in (M+(E), M+(E)).
Deﬁnition 2.2.4 (Point process) Let E be a locally compact space with a countable
basis. Let E be a Borel σ-algebra of subsets of E. Let (θi)i≥1 be a countable collection
of not necessary distinct points of E. A point measure on E is a measure m of the
following form:
m =
∞
X
i=1
δθi,
where, δθi denotes the Dirac measure at θi, i.e., δθi(A) = 1 if θi ∈A and 0 otherwise
for a set A ∈E . If K ∈E is compact then m(K) < ∞(i.e., m is Radon meaning
the measure of compact sets is always ﬁnite). Take Mp(E) as the space of all point

2. L´evy random variables and processes
9
measures deﬁned on E and Mp(E) be the smallest σ-algebra containing all sets of
the form {m ∈Mp(E) : m(A) ∈B} for A ∈E , B ∈B([0, ∞)). Alternatively,
Mp(E) is the smallest σ-algebra making all evaluation maps m →m(A) measurable
for all A ∈E . A point process ξ on E is a measurable map from the probability
space (Ω, F, P) →(Mp(E), Mp(E)). Therefore, a point process is a random element
of Mp(E). The probability law of the point process ξ is the measure P ◦ξ−1 = P[ξ ∈·]
on Mp(E).
The Laplace functional is a useful tool to determine the distribution of point processes.
It is important to notice that the Laplace functional of a random measure uniquely
determines the distribution of any random measure.
Deﬁnition 2.2.5 (Laplace Functional of Point Process) Let Q be a probability
measure on (Mp(E), Mp(E)) (where Mp(E) and Mp(E) are as constructed in Deﬁni-
tion 2.2.3). The Laplace transform of Q is the map ψ which takes non-negative Borel
functions on E into [0, ∞) deﬁned by
ψ(f) =
Z
Mp(E)

exp

−
Z
E
f(x)m(dx)

Q(dm).
If ξ : (Ω, F) →(Mp(E), Mp(E)) is a point process, the Laplace functional of ξ is the
Laplace transform of the law of ξ(f):
ψξ(f) = E(exp{−ξ(f)}) =
Z
Ω
exp{−ξ(ω, f)}P(dω)
=
Z
Mp(E)

exp

−
Z
E
f(x)m(dx)

Pξ(dm).
Deﬁnition 2.2.6 (Poisson random measure) Let µ be a Radon measure on E , a
point process ξ is called a Poisson point process or a Poisson random measure with
mean measure µ, denoted by PRM(µ), if ξ satisﬁes:
1. For A ∈E
P[ξ(A) = k] =





e−µ(A)(µ(A))k
k!
,
if µ(A) < ∞
0
if µ(A) = ∞.

2. L´evy random variables and processes
10
2. For any k ≥1, if A1, . . . , Ak are mutually disjoint sets in E , then ξ(A1), . . . , ξ(Ak)
are independent random variables.
Therefore, ξ is a Poisson random measure if the random number of points in a set A
has a Poisson distribution with parameter µ(A) and the number of points in disjoint
sets are independent random variables.
Proposition 1 Let ξ a PRM(µ), the Laplace functional of PRM(µ) uniquely deter-
mines the law of ξ. It is given for any measurable positive function, f ≥0, by
ψξ(f) = exp

−
Z
E
(1 −e−f(x))µ(dx)

.
For proof, see proposition 3.6 of Resnick (1987).
Proposition 2 Let ξ1 and ξ2 be two independent Poisson random measures on (E, B(E))
with mean measure µ1 and µ2 respectively. Then, the random measure ξ = ξ1 + ξ2 is
also a Poisson random measure with mean measure µ = µ1 + µ2.
Proof:
E(e−ξ(f)) = E(e−ξ1(f)−ξ2(f))
= E(e−ξ1(f))E(e−ξ2(f))
= exp

−
Z
E
(1 −e−f(x))µ1(dx)

exp

−
Z
E
(1 −e−f(x))µ2(dx)

= exp

−
Z
E
(1 −e−f(x))(µ1 + µ2)(dx)

.
Throughout this thesis, we deﬁne
Γi = E1 + · · · + Ei,
(2.2.5)

2. L´evy random variables and processes
11
where (Ek)k≥1 is a sequence of independent and identically distributed random vari-
ables with an exponential distribution of mean 1.
Theorem 3 Let ξ ∼PRM(λ) where λ is the Lebesgue measure on R. Then ξ can
be written as follows
ξ =
∞
X
i=1
δΓi.
Proof:
Using the recursive technique in Banjevic & Zarepour (2002), we have for
any non-negative function f
ξ(f) =
∞
X
i=1
f(Γi + t)
M(u, t) = E[e−uξ] = E(e−u P∞
i=1 f(Γi+t))
= E(E(e−u P∞
i=1 f(Γi+t)|Γ1 = s))
=
Z ∞
t
e−uf(s+t)E(e−u P∞
i=1 f(Γi+s+t))e−sds
=
Z ∞
t
e−uf(s+t)M(u, t + s)e−sds.
Now using the change of variable s + t = v and multiplying both sides by e−t, we get
M(u, t) =
Z ∞
t
e−uf(v)M(u, v)e−(v−t)dv
e−tM(u, t) =
Z ∞
t
e−uf(v)M(u, v)e−vdv.
Diﬀerentiating both sides with respect to t, we get
−e−tM(u, t) + DM(u, t)e−t = −e−uf(t)M(u, t)e−t
DM(u, t) = (1 −e−uf(t))M(u, t)
DM(u, t)
M(u, t) = (1 −e−uf(t))
M(u, t) = exp

−
Z ∞
t
(1 −e−uf(s))ds

.

2. L´evy random variables and processes
12
Now take t = 0 to get
M(u, 0) = exp

−
Z ∞
0
(1 −e−uf(s))ds

.
2.2.4
Transformation of Poisson Random measure
The next proposition shows that mapping points of a Poisson point process yields
new Poisson point process with a certain representation for its mean measure.
Proposition 2.2.7 (Proposition 3.7 of Resnick, 1987) Let Ei, i = 1, 2 be two
locally compact spaces with countable bases. Let Ei, i = 1, 2 be the associated σ-ﬁelds.
Let T : (E1, E1) →(E2, E2) be measurable. If ξ is a PRM(µ) on E1, then ˜ξ = ξ ◦T −1
is a PRM (˜µ) on E2 such that ˜µ = µ ◦T −1. If ξ has the representation
ξ =
∞
X
i=1
δXi
then
˜ξ = ξ ◦T −1 =
∞
X
i=1
δT(Xi).
The next proposition shows that starting from PRM, we may construct a new
PRM whose points live in a higher dimensional space.
Proposition 2.2.8 (Proposition 3.8 of Resnick, 1987) Let Ei, i = 1, 2 be two
locally compact spaces with countable bases. Suppose
∞
X
i=1
δXi
is a PRM(µ) on E1. Suppose (θi)i≥1 are i.i.d. random elements on E2 with common
probability distribution F, and suppose the Poisson process and (θi)i≥1 are deﬁned on

2. L´evy random variables and processes
13
the same probability space and are independent. Then the point process on E1 × E2,
∞
X
i=1
δ(Xi,θi),
is a PRM with mean measure µ × F.
Let ξ = P∞
i=1 δ(Γi,θi) such that θi
i.i.d.
∼H and Γi is independent of θi then for A and B
in Ω, we have
E[e−uξ(A×B)] = exp

−
Z
B
Z
A
(1 −e−ux)ν(dx)dH(θ)

.
Theorem 4 (Banjevic & Zarepour, 2002) Suppose X is a subordinator with char-
acteristic function
Ψ(x) = exp

−
Z ∞
0
(1 −exp(ixu))dν(u)

,
−∞< x < ∞
where ν is a positive, continuous and non-increasing L´evy measure deﬁned on (0, ∞)
such that ν(x) =
R ∞
x dν(u) and
Z ∞
ϵ
ν−1(u)du < ∞,
for each ϵ > 0
in which ν−1(u) = sup{x : ν(x) ≤u}. Then X has the almost sure representation
X =
∞
X
i=k
ν−1(Γk).
Theorem 5 (Campbell’s Theorem) [Kingman, 1993] Let Y be a Poisson process
on Ω× (0, ∞) with mean measure ν. Then Y has a sum representation
Y =
∞
X
i=1
xiδθi.
We have that Y is absolutely convergent if and only if
Z
Ω
Z ∞
0
x.ν(dθ, dx) < ∞.

2. L´evy random variables and processes
14
2.3
L´evy random variables
In this section, we introduce some examples of L´evy random variables and their
characteristic.
2.3.1
Poisson random variable
Let ξ be a Poisson random variable with probability distribution deﬁned as follows,
for each λ > 0
µλ({k}) = e−λλk/k!
k = 0, 1, 2, . . . .
Using the deﬁnition of characteristic function, an easy calculation shows that
E[eiuξ] =
X
k≥0
eiukµλ({k})
=
X
k≥0
[eiu]ke−λλk
k!
= e−λ
∞
X
k=0
(λeiu)k
k!
= e−λeλeiu
= e−λ(1−eiu)
=

e−λ
n(1−eiu)
n
(2.3.1)
In here, (2.3.1) is the characteristic function of the sum of n independent Poisson
processes, each of which with parameter λ/n. Therefore, the L´evy-Khintchine deﬁ-
nition states that there exist a triple (a, σ, ν) such that the characteristic exponent
ψ(u) satisﬁes the equation in (2.2.3). Indeed, we see that the characteristic exponent
of the Poisson random variable has the same form of (2.2.3) with a = σ = 0 and
ν = λδ1, where δ1 is the Dirac measure at 1. The Poisson random variable can be
written as
ξ(·) =
∞
X
i=1
δΓi(·).

2. L´evy random variables and processes
15
Refer to Theorem 3 for the proof.
2.3.2
Gamma random variable
Let X be a Gamma random variable with probability measure deﬁned as follows, for
α, β > 0
µα,β(dx) =
βα
Γ(α)xα−1e−βxdx
Using the characteristic function we get,
E[eiuX] =
Z ∞
0
eiuxµα,β(dx)
=
1
(1 −iu/β)α
=
 
1
(1 −iu/β)α/n
!n
(2.3.2)
From (2.3.2) we can conclude that a Gamma random variable is inﬁnitely divisible
and therefore the L´evy-Khintchine deﬁnition state that there exists a triple (a, σ, ν)
such that ψ(u) satisﬁes (2.2.3). The following helps us ﬁnd the values of (a, σ, ν).
Theorem 6 (Frullani integral) Let a, b > 0 such that a < b. If f ′(x) is continuous
and the integral converges, then
Z ∞
0
f(ax) −f(bx)
x
dx = [f(0) −f(∞)] log
b
a

.
Lemma 2.3.1 For all α, β > 0 and z ∈C such that the real part of z is in (−∞, 0)
we have
1
(1 −z/β)α = exp

−
Z ∞
0
(1 −ezx)αx−1e−βxdx

.
Proof:
Using the Frullani integral with f(x) = αe−x, it follows
exp

−
Z ∞
0
(1 −ezx)αx−1e−βxdx

= exp

−
Z ∞
0
αe−βx −αe−(β−z)x
x


2. L´evy random variables and processes
16
= exp

−
Z ∞
0
f(βx) −f((β −z)x)
x

= exp

−

[f(0) −f(∞)] log
β −z
β
 
= exp

−α log
β −z
β
 
= exp

log

β
β −z
α
=

1
1 −z/β
α
For simplicity and without loss of generality, in this thesis, we take β = 1. The
characteristic exponent of a Gamma random variable X is
ψ(u) = −log E(eiuX)
= −log

1
(1 −iu)α

= −log(e−
R ∞
0 (1−eiux)αx−1e−xdx)
= α
Z ∞
0
(1 −eiux)1
xe−xdx
for θ ∈R.
Therefore,
σ = 0,
and for ν concentrated on (0, ∞), we have
ν(dx) = αx−1e−xdx
(2.3.3)
a = −
Z 1
0
xν(dx)
(2.3.4)
The choice of a in the L´evy-Khintchine formula is the necessary quantity to cancel
the term coming from
R
iu1(|x|<1)ν(dx) in (2.2.3). We can show that any Gamma
random variable can be written as
X(·) =
∞
X
i=1
ν−1(Γi)δθi(·).

2. L´evy random variables and processes
17
See Ferguson and Klass (1972) and Banjevic & Zarepour (2002) for detail and proof.
2.3.3
Stable random variable
Deﬁnition 2.3.2 A random variable X is said to have a stable distribution if for all
n ≥2, an > 0 and bn ∈R,
X1 + X2 + · · · + Xn
d= anX + bn,
(2.3.5)
where X1, X2, . . . , Xn are independent copies of X.
It can be proven that an = n1/α for 0 < α ≤2. The value α is known as the index of
stability. Any stable random variable is inﬁnitely divisible, this follows by subtracting
bn/n from each of the independent copies X1, X2, . . . , Xn
X1 + X2 + · · · + Xn
d= anX + bn
n
X
k=1
Xk −bn/n
an

d= X
Deﬁnition 2.3.3 A stable random variable denoted by Sα(c, β, µ), with an index of
stability α ∈(0, 2], c ≥0, −1 ≤β ≤1, µ ∈R has a characteristic exponents as
follows:
ψ(u) =



−cα|u|α(1 −iβ(sign u) tan πα
2 ) + iµu
if α ̸= 1
c|u|(1 + iβ 2
π(sign u) ln |u| + iµu
if α = 1
where
sign(u) =









1
if u > 0
0
if u = 0
−1
if u < 0
To make a connection with the L´evy-Khintchine formula, one should have σ = 0,
a = µ −
R
R x1(|x|<1)ν(dx) in (2.2.3) and
ν(dx) =



P
x1+αdx
if x ∈(0, ∞)
Q
|x|1+αdx
if x ∈(−∞, 0)

2. L´evy random variables and processes
18
where for P, Q ≥0
c = P + Q,
and
β =



P−Q
P+Q
if α ∈(0, 1) ∪(1, 2)
0
if α = 1
(P = Q).
A random variable with symmetric α-stable distribution, denoted by Xα can be writ-
ten as a series representation as follows, with 0 < α < 2:
Xα(·) =
∞
X
i=1
ϵiΓ−1/α
i
δθi(·),
where {ϵi} is an i.i.d. sequence with
p(ϵi = 1) = 1/2
p(ϵi = −1) = 1/2.

Chapter 3
Gamma and Dirichlet Process
3.1
Gamma Process
The Gamma process plays a crucial role in nonparametric Bayesian inference.
It
has gained widespread adoption when computational techniques allowed it to be
more practically applicable. It has been used as prior to many applications in dif-
ferent ﬁelds such as exchangeable models of sparse graphs (Caron, Fran¸cois and Fox,
Emily B, 2014), nonparametric ranking models (Caron, Fran¸cois and Teh, 2013) and
inﬁnite-dimensional latent indicator matrices (Titsias, Michalis L, 2008). A normal-
ized Gamma process which will be introduced in future sections, called the Dirichlet
process has played a central role in nonparametric Bayesian inference.
3.1.1
Deﬁnition of Gamma process
Deﬁnition 3.1.1 (Gamma process) The Gamma process, denoted by G ∼GP(a, H),
is a completely random measure on [0, ∞] × Ωwith L´evy measure
ρ(dx, dθ) = N(dx)dH(θ),
19

3. Gamma and Dirichlet Process
20
where
N(dx) = ae−ax
x dx.
Here, a > 0 is called the concentration parameter and H the base measure.
Recall from Chapter 2 that the L´evy measure of the Gamma random variable is
deﬁned as follows
N(x) = a
Z ∞
x
u−1e−udu, for x > 0.
(3.1.1)
Note that N(dx) := dN(x).
The Gamma process is the conjugate prior for the non-negative integer valued-
Poisson random measure.
Suppose we observe m Poisson random measures such
that
P1, . . . , Pm|G ∼PRM(G)
G ∼GP(a, H).
Following Thibaux (2008) notation, we update our posterior belief of G as follows
G|P1, . . . , Pm ∼GP(c∗, H∗),
where
c∗= c + m
H∗=
c
c + mH +
m
c + m
Pm
i=1 Pi
m
.
For more details refer to Wolpert and Ickstadt (1998a).
3.1.2
Series representation of the Gamma process
From Ferguson and Klass (1972), the Gamma process G ∼GP(a, H), can be written
as a sum representation based on the Gamma L´evy measure as follows:
GFerg(·) =
∞
X
i=1
N −1(Γi)δθi(·),
(3.1.2)

3. Gamma and Dirichlet Process
21
where {θi}i≥1 is a sequence of i.i.d. random variable with common distribution H
independent of {Γi}i≥1. The sequence {Γi}i≥1 is deﬁned as in chapter 2.
The terms in the Ferguson and Klass (1972) sum representation are relatively
diﬃcult to compute since there is no closed form for the inverse of the L´evy measure.
Moreover, there are inﬁnite terms in (3.1.2) to calculate.
3.1.3
Approximation of the Gamma process
Bondesson (1982) introduces a sum representation of the Gamma process which avoids
the need to work with L´evy measures. It is shown that
GBond(·) =
∞
X
i=1
exp

−Γi
a

Eiδθi(·),
(3.1.3)
where, {Ei}i≥1 is a sequence of i.i.d. Exponential random variable with mean 1 and
{θi}i≥1 is a sequence of i.i.d. random variable with common distribution H. In here,
{Ei}i≥1 and {θi}i≥1 are independent. The Bondesson (1982) sum representation of
the Gamma process can be approximated by truncating the higher order in (3.1.3).
Following the same truncation approach used for the Dirichlet process deﬁned by
Muliere & Tradella (1998), we let
n = inf{i : exp

−Γi
a

Ei < ϵ},
(3.1.4)
for ϵ ∈(0, 1). The Bondesson (1982) sum approximation is deﬁned as
GBond(·) =
n
X
i=1
exp

−Γi
a

Eiδθi(·).
(3.1.5)
Note that the weights in (3.1.5) are not monotonically decreasing almost surely.
This phenomena is more obvious by looking at Figure 3.1. Note that the vertical
lines in this ﬁgure represent the weights calculated in (3.1.5) which are clearly not
monotonically decreasing.
We will discuss the eﬃciency of diﬀerent algorithm in
further sections.

3. Gamma and Dirichlet Process
22
Figure 3.1 depicts one sample path of the Gamma process GBond((−∞, t]) ∼
GP(a, H) for t ∈Ω= R using Bondesson (1982) approximation. We choose a = 5
and H ∼t(5), a t-distribution with ﬁve degrees of freedom. By choosing ϵ = 0.0001,
we get n = 30. The value of n is calculated based on the stopping rule in (3.1.4). The
plot shows as well the weights in (3.1.5) at locations t = θ(i) as vertical lines. The
more intense the weight is, the higher the vertical line becomes. Note that a jump in
the Gamma process occurs at t = θ(i) every time there is a visible weight at the same
location t = θ(i), for i = 1, . . . , n. Thus, a more intense weight results in a higher
jump in the Gamma process. Note that θ(1) < · · · < θ(n) are the ordered statistics for
θ1, . . . , θn, such that θi
i.i.d.
∼t(5).
Zarepour and Al Labadi (2012) derive a ﬁnite sum approximation of the Gamma
process Gn, which converges a.s. to the Ferguson and Klass (1972) sum representation.
To see this, let Gn ∼Gamma(a/n, 1), i.e:
Gn(x) = Pr(Xn > n) =
Z ∞
x
1
Γ(a/n)e−tta/n−1dt,
(3.1.6)
and
G−1
n (y) = inf{x : Gn(x) ≥y},
0 < y < 1.
Using the fact that n/Γ(α/n) = α/Γ(α/n+1) and n/Γ(α/n) →α, we have for x > 0,
nGn(x) =
n
Γ(α/n)
Z ∞
x
e−ttα/n−1dt →α
Z ∞
x
e−tt−1dt = N(x).
Note that for every x > 0, nGn(x) is a sequence of monotone functions converging to
a continuous monotone function, therefore
G−1
n (x/n) →N −1(x).
By taking x = Γi and from the fact that Γn+1/n →1 almost surely as n →∞, we
have
G−1
n
 Γi
Γn+1

a.s.
→N −1(Γi).

3. Gamma and Dirichlet Process
23
Figure 3.1: One sample path of the Gamma process, GBond ∼GP(a, H)
using Bondeson (1982) approximation.
We choose a = 5 and H ∼t(5).
By choosing ϵ = 0.0001, and following the stopping rule in (3.1.4), we get
n = 30. The x-axis represents the set of i.i.d. atoms generated from θi ∼H in
increasing order and the y-axes represents the corresponding Gamma process.
We display in the same plot, the weights in (3.1.5) as vertical lines at the
corresponding atoms t = θi for i = 1, . . . , n.
The Gamma process can be approximated as follows:
GZar&Al Lab
n
(·) =
n
X
i=1
G−1
n
 Γi
Γn+1

δθi(·)
a.s.
→G(·) =
∞
X
i=1
N −1(Γi)δθi(·).
(3.1.7)

3. Gamma and Dirichlet Process
24
The next algorithm describes the set of rules to sample from the Zarepour and Al
Labadi (2012) approximation of the Gamma process.
Algorithm A: An approximation of the Gamma process
1. Choose a relatively large positive integer n.
2. Generate θi
i.i.d.
∼H, for i = 1, . . . , n.
3. Generate n+1 independent exponential distributions with mean 1, Ei
i.i.d.
∼Exp(1).
Set Γi = Pi
k=1 Ek, for i = 1, . . . , n + 1.
4. Computing G−1
n (Γi/Γn+1) is equivalent as computing the quantile of the gamma
distribution, Gamma(a/n, 1) evaluated at (1 −Γ1/Γn+1).
Note that the approximation of Zarepour and Al Labadi (2012) is not based on a
stopping rule. It is more an asymptotic result. Nevertheless, a suggested stopping
rule for n is proposed for comparison purposes as follows:
n = inf

j : G−1
j
 Γj
Γj+1

< ϵ

.
(3.1.8)
Figure 3.2 shows one sample path of the Gamma process using Zarepour & Al
Labadi (2012) in (3.1.7). We take a = 5 and the base measure H ∼t(5). By choosing
ϵ = 0.0001 and using the stopping rule deﬁned in (3.1.8), we get n = 14. Figure 3.2
plots as well the corresponding weights calculated in (3.1.7) which are represented in
vertical lines at the corresponding location t = θi. Notice that the weights are now
monotonically decreasing almost surely.

3. Gamma and Dirichlet Process
25
Figure 3.2: One sample path of the Gamma process using Zarepour & Al
Labadi (2012) approximation.
For comparison purposes we use the same
parameter used in Figure 3.1, particularly we choose a = 5 and H ∼t(5).
Choosing ϵ = 0.0001 and using the stopping rule in (3.1.8), we get n = 14.
The plot shows as well the weights calculated in (3.1.7) as vertical lines.

3. Gamma and Dirichlet Process
26
It is worth mentioning that the approximation of Zarepour & Al Labadi (2012)
for the Gamma process is very eﬃcient from a computation point of view. Moreover,
the algorithm will be as eﬃcient as Bondesson (1982) with much smaller (almost half)
values of n. Nevertheless, the weights are monotonically decreasing which ensure that
there will not be an intense weight after the last negligible weights calculated (or ob-
served). This phenomena is not guaranteed in the Bondesson (1982) Gamma process
approximation.
The plot at the top of Figure 3.3 shows ten diﬀerent paths of the Gamma process
approximated using the Bondesson (1982) sum approximation with concentration
parameter a = 5 and base measure H ∼t(5).
For each path, the values of the
truncation n is equal to 45, 25, 42, 40, 39, 41, 7, 52, 39 and 27. We use the truncation
rule in (3.1.4) with ϵ = 0.0001. The plot at the bottom of Figure 3.3 shows ten
diﬀerent paths of the Gamma process with the same parameters, a and H, but this
time based on the Zarepour and Al Labadi (2012) Gamma process approximation.
For each path, the values n = 14, 10, 10, 10, 11, 12, 8, 11, 10 and 9 are calculated based
on the stopping rule in (3.1.8) with the same tolerance value ϵ. As discussed earlier,
the weights of the Zarepour and Al Labadi (2012) approximation of the Gamma
process are decreasing almost surely, thus the plot at the bottom shows decreasing
jumps (more intense jumps at the beginning and gradually decreases toward the end).

3. Gamma and Dirichlet Process
27
Figure 3.3: Ten sample paths of the Gamma process approximation with
a = 5, H = t(5) and ϵ = 0.0001. The plot at the top of the ﬁgure uses the
Bondesson (1982) Gamma approximation and the plot at the bottom uses
the Zarepour and Al Labadi (2012) Gamma process approximation.
The
truncation values for Bondeson (1982) are n = 45, 25, 42, 40, 39, 41, 7, 52, 39
and 27, one value for each path. Whereas the truncation values for Zarepour
and Al Labadi (2012) are found to be n = 14, 10, 10, 10, 11, 12, 8, 11, 10 and
9, for the same tolerance value ϵ = 0.0001.

3. Gamma and Dirichlet Process
28
3.2
Dirichlet process
It is well known that the beta distribution, denoted by Beta(a, b), is used as a conju-
gate prior for a binomial model. More concretely, let
f(X|p) ∼Binomial(n, p)
g(p) ∼Beta(a, b).
By using Bayes’ theorem, f(p|X) ∝f(X|p)g(p), the posterior distribution of p be-
comes
f(p|X = x) ∼Beta(a + x, n + a −x).
3.2.1
Deﬁnition of the Dirichlet distribution
Deﬁnition 3.2.1 (Dirichlet Distribution) Let P = (p1, p2, . . . , pk−1) be a random
vector, such that pi ≥0 for i = 1, 2, . . . , k and p1 + · · · + pk = 1.
In addition,
suppose that a = (a1, . . . , ak), with αi > 0 for i = 1, 2, . . . , k, and let a0 = Pk
i=1 ai.
Then, P is said to have a Dirichlet distribution with parameter a, denoted by P ∼
Dir(a1, . . . , ak), if its density function is given by
f(p1, . . . , pk−1|a1, . . . , ak) =
Γ(a0)
Qk
i=1 Γ(ai)
k−1
Y
i
pai−1
i

1 −
k−1
X
i=1
pi
ak−1
,
over the simplex
S = {(p1, p2, . . . , pk−1) : pi ≥0,
k−1
X
i=1
pi ≤1},
where Γ(x) denotes the Gamma function.
When k = 2, the Dirichlet distribution reduces to the beta distribution, which has
the density function
f(p; a, b) = Γ(a + b)
Γ(a)Γ(b)pa−1(1 −p)b−1
p ∈(0, 1)
a, b > 0.

3. Gamma and Dirichlet Process
29
Similar to the beta distribution, the Dirichlet distribution Dir(a1, . . . , ak) is used
as a conjugate prior for the Multinomial distribution given by
f(x1, . . . , xk−1|p1, . . . , pk−1) =
n!
x1! . . . xk−1!(n −Pk−1
i=1 xi)!
× px1−1
1
. . . pxk−1−1
k−1

1 −
k−1
X
i=1
pi
n−Pk−1
i=1 xi
.
In this case, the posterior distribution of P = (p1, . . . , pk−1) is
f(p1, . . . , pk−1|X1 = x1, . . . , Xk−1 = xk−1) ∼Dir(a1 + x1, . . . , ak + xk),
where xk = n −Pk−1
i=1 xi.
The following is a general review of properties of the Dirichlet distribution.
1. If {Gi}1≤i≤k are independent random variables such that Gi ∼Gamma(ai, 1), i =
1, 2, · · · , k, then
 
G1
Pk
i=1 Gi
,
G2
Pk
i=1 Gi
, . . . ,
Gk−1
Pk
i=1 Gi
!
∼Dir(a1, . . . , ak).
(3.2.1)
2. If (p1, . . . , pk−1) ∼Dir(a1, . . . , ak) and r1, . . . , rl are integers such that
0 < r1 < · · · < rl = k −1, then
 p(1,r1), p(r1+1,r2), . . . , p(rl−1+1,rl)

∼Dir
 a(1,r1), a(r1+1,r2), . . . , a(rl−1+1,rl), ak

,
where, for i, j = 1, · · · , k
p(i,j) = pi + pi+1 + · · · + pj
a(i,j) = ai + ai+1 + · · · + aj.
3. If (p1, . . . , pk−1) ∼Dir(a1, . . . , ak) and a0 = Pk
i=1 ai, then
E[(p1, . . . , pk−1)] =
a1
a0
, . . . , ak
a0

Cov(pi, pj) =





−aiaj
a2
0(a0+1)
for i ̸= j
ai(a0−ai)
a2
0(a0+1) .
for i = j

3. Gamma and Dirichlet Process
30
4. If the prior distribution of (p1, . . . , pk−1) ∼Dir(a1, . . . , ak) and for the random
variable X, let Pr{X = j|p1, . . . , pk} = pj a.s.
for j = 1, . . . , k, then the
posterior distribution of (p1, . . . , pk−1|X = j) ∼Dir(aj
1, . . . , ai
k), where
aj
i =





ai
if i ̸= j
ai + 1
if i = j.
Note that this shows that the Dirichlet distribution is a conjugate prior for any
random variable with discrete probability distribution and with ﬁnite support.
3.2.2
Deﬁnition of the Dirichlet process
In this section we discuss how to place a conjugate prior for any probability measure
on a general probability space with any support Ω(i.e. Ω= R, Ω= R+ or Ω= Rp).
Ferguson (1973) introduced the Dirichlet process as a class of priors over an arbitrary
measurable space Ω, indexed by elements of a Borel σ-algebra F.
The Dirichlet
process characterized by a stochastic process along with its conjugacy property has
gained widespread adoption both in theory and practice.
Deﬁnition 3.2.2 (Dirichlet random variable) Let (Ω, F) be an arbitrary mea-
surable space and H be a probability measure on (Ω, F) . Let a > 0 be arbitrary. A
random probability measure P deﬁned on F is called a Dirichlet probability measure
with parameters a and H, denoted by P ∼DP(a, H), if for any ﬁnite measurable
partition {A1, . . . , Ak} of Ω, the joint distribution of the vector (P(A1), . . . , P(Ak)) ∼
Dir(aH(A1), . . . , aH(Ak)), k ≥2. We assume that if H(Aj) = 0, then P(Aj) = 0
with probability one.
For any measurable set A ∈F, P(A) ∼Beta{aH(A), a(1 −H(A))}. Thus,
i)
E(P(A)) =
aH(A)
aH(A) + a(1 −H(A)) = H(A)

3. Gamma and Dirichlet Process
31
V ar(P(A)) = H(A)(1 −H(A)
1 + a
.
ii) If P ∼DP(a, H) and X1, . . . , Xm is a sample from P, then the posterior distri-
bution of P|X1, . . . , Xm ∼DP(a(1), H(1)), where
a(1) = a + m,
H(1) =
a
a + mH +
m
a + m
 1
m

m
X
i=1
δXi.
For more details, interested reader can refer to Ferguson (1973).
From (i), it is clear that H plays the role of the centre of the process and hence
is called the base measure (or also known as the initial guess). Also, we can see that
as a gets larger the variance gets smaller. Therefore the distribution of P is more
tightly concentrated around its mean, H. Hence, the parameter a can be seen as the
concentration parameter. Note from (ii), the posterior base distribution H(1) is the
combination of the base distribution and the empirical distribution. The posterior
base distribution approaches the prior base measure H as a →∞. Also, it approaches
the empirical distribution as a →0. Note that from strong law of large numbers, we
get as m →∞, H(1) →F, where F = limm→∞
1
m
Pm
i=1 δXi.
3.2.3
Series representation of the Dirichlet process
Ferguson (1973) showed that the Dirichlet process can be deﬁned by using a sum
representation for processes with independent increments. These processes are based
on the arrival times of a homogeneous Poisson process. In fact, the Dirichlet process,
P ∼DP(a, H), can be represented as a normalized Gamma process, see similarity of
the self normalization in (3.2.1). Ferguson (1973) described the Dirichlet process by
normalizing the series representation of a Gamma random measure given in (3.1.2)

3. Gamma and Dirichlet Process
32
as follows
PFerg(·) =
∞
X
i=1
N −1(Γi)
P∞
i=1 N −1(Γi)δθi(·),
(3.2.2)
where N, is deﬁned in (3.1.1) and (θi)i≥1 is a sequence of i.i.d. random variables with
common distribution H. Notice that PFerg is a discrete random probability measure.
Similar to the Gamma process, sampling from the Dirichlet process based in
(3.2.2) is diﬃcult in practice since there is no closed form for the inverse of the
Gamma L´evy measure. Moreover, there are an inﬁnite number of terms in (3.2.2)
that must be computed.
3.2.4
Approximation of the Dirichlet process
The Bondesson (1982) sum representation of the Dirichlet process with parameters a
and H is deﬁned in the next theorem.
Theorem 7 (Bondesson 1982) Let (θi)i≥1 be a sequence of i.i.d. random variables
with common distribution H and let (Ei)i≥1 be a sequence of i.i.d. exponential random
variables with mean 1, independent of (Γi)i≥1 and (θi)i≥1. Then,
PBond(·) =
∞
X
i=1
e−Γi/aEi
P∞
i=1 e−Γi/aEi
δθi(·).
(3.2.3)
For more details, see Ishwaran & Zarepour (2002) and Zarepour & Al Labadi (2012).
Note that the Bondesson representation overcomes the problem of inverting the
Gamma L´evy measure. However, the inﬁnite number of terms to compute in (3.2.3)
make it diﬃcult to sample from the Dirichlet process.

3. Gamma and Dirichlet Process
33
One can approximate the Dirichlet process by using a truncation as follows:
PBond
n
(·) =
n
X
i=1
e−Γi/aEi
Pn
i=1 e−Γi/aEi
δθi(·).
(3.2.4)
The choice of n can be selected for a given tolerance value ϵ ∈(0, 1) by
n = inf
(
j :
e−Γj/aEj
Pj
i=1 e−Γj/aEj
< ϵ
)
.
(3.2.5)
Figure 3.4 shows the Bondesson (1982) approximation of Dirichlet process with a = 5
and base measure H ∼t(5).
It also shows the weights in (3.2.4) represented as
vertical lines at diﬀerent location θi. For a tolerance of ϵ = 0.0001 and following the
truncation rule in (3.2.5), we get n = 25. Note that unlike the Gamma process, the
weights of the Dirichlet process sum up to 1. It is worth mentioning that the weights
in (3.2.4), represented in the plot by vertical lines, are not monotonically decreasing
almost surely.

3. Gamma and Dirichlet Process
34
Figure 3.4: One sample path of the Dirichlet process approximated using
Bondesson (1982) approximation.
We choose a = 5 and H ∼t(5).
For
ϵ = 0.0001, we get n = 25. Vertical lines at diﬀerent location θ(i) represent
the weights in calculated (3.2.4).
Figure 3.5 shows ten diﬀerent sample paths of the Dirichlet process using Bon-
desson (1982) approximation.
In all paths, we take a = 5 and H ∼t(5).
For
ϵ = 0.0001 the truncation value n are calculated following (3.2.5), thus we get
n = 26, 31, 23, 26, 27, 36, 25, 28, 27 and 24.
The Ferguson (1973) and Bondesson (1982) sum representation for the Dirichlet
process is based on the normalized Gamma process. Sethuraman (1994) deﬁned the

3. Gamma and Dirichlet Process
35
Figure 3.5:
Ten sample paths of the Dirichlet process using Bondesson
(1982) approximation with a = 5, H ∼t(5).
For ϵ = 0.0001, we get
n = 26, 31, 23, 26, 27, 36, 25, 28, 27 and 24.
Dirichlet process by using a stick breaking representation instead. This representation
does not involve a normalization. Similar to the Bondesson (1982) sum representation,
the stick breaking representation avoids inverting the L´evy measure N in (3.1.1).

3. Gamma and Dirichlet Process
36
Theorem 8 (Sethuraman 1994) Let (Bi)i≥1 be a sequence of i.i.d. random vari-
ables with Beta(1, a) distribution. Deﬁne
p1 = B1,
pi = Bi
i−1
Y
k=1
(1 −Bk),
i ≥2.
Moreover, let (θi)i≥1 be a sequence of i.i.d. random variables with common distribution
H, independent of (pi)i≥1. Then
PSeth(·) =
∞
X
i=1
piδθi(·),
(3.2.6)
is a Dirichlet process with parameter a and H.
The Dirichlet process can be approximated by using Sethurman’s stick breaking ap-
proximation. This is done by truncating the higher order terms in (3.2.6). Let (Bi)i≥1,
(pi)i≥1 and (θi)i≥1 be deﬁned in Theorem 8 with the only diﬀerence that Bn = 1,
PSeth
n
(·) =
n
X
i=1
piδθi(·).
(3.2.7)
Note that by letting Bn = 1, the weights (p1, . . . , pn) sum up to 1 almost surely. For
more details, see Ishwaran and James (2001). Muliere and Tradella (1998) proposed
a stopping rule for n where, for ϵ ∈(0, 1)
n = inf{i : pi = (1 −B1) . . . (1 −Bi−1)Bi < ϵ}.
(3.2.8)
Figure 3.6 shows one path of the Dirichlet process using the Sethuraman (1994)
approximation. We use a = 5 and H ∼t(5) . Figure 3.6 depicts also the weights
in (3.2.7) at every location t = θ(i). Those weights are represented in the plot by
vertical lines. Using the stopping rule proposed by Muliere and Tradella (1998) in
(3.2.8) with ϵ = 0.0001, we get n = 45. Similar to the Dirichlet process approximated
by Bondeson (1982) (see Figure 3.4), the weights in the stick breaking approach are
not strictly decreasing, thus vertical lines in the graph are not strictly decreasing.

3. Gamma and Dirichlet Process
37
Figure 3.6: One sample path of Sethuraman (1994) Dirichlet process approx-
imation, P Seth((−∞, t]) ∼DP(a, H). We choose a = 5 and H ∼t(5). For
ϵ = 0.0001 and using the stopping rule in (3.2.8), we get n = 45.
Figure 3.7 shows ten diﬀerent paths of the Dirichlet process approximated by the
stick breaking approach of Sethurman (1994). For each path, the truncation values
n = 30, 38, 57, 44, 54, 52, 21, 31, 43 and 54 are calculated using (3.2.8). Note that the
values of n calculated for each path of the Dirichlet process approximation of Sethu-
raman (1994) is relatively higher comparing to the Dirichlet process approximated by
Bondeson (1982) (see Figure 3.5 for more details). In the next section, we will discuss
theoretically why the weights in (3.2.7) are not strictly decreasing. This makes the

3. Gamma and Dirichlet Process
38
stopping rule in (3.2.8) and (3.2.5) ineﬃcient to approximate the Dirichlet process.
Indeed, for both approximations, the stopping rules are overestimating the value of
n.
Figure 3.7: Ten sample paths of the Dirichlet process approximated by Sethu-
raman (1994) stick breaking approach. We choose a = 5 and H ∼t(5). For
ϵ = 0.0001 and using the stopping rule in (3.2.8), the values of n for each
path is n = 30, 38, 57, 44, 54, 52, 21, 31, 43 and 54.
Zarepour and Al Labadi (2012) prove that the weights in the Bondesson’s rep-
resentation (3.2.3) and in the Sethurman sum representation (3.2.7) are not strictly

3. Gamma and Dirichlet Process
39
decreasing almost surely. They prove that for i ≥1,
Pr
 e−Γi+1/aEi+1
P∞
i=1 e−Γi/aEi
<
e−Γi/aEi
P∞
i=1 e−Γi/aEi

= a
∞
X
k=0
(−1)k/(k + a),
and,
Pr{pi+1 < pi} = a
∞
X
k=0
(−1)k/(k + a),
(3.2.9)
where {pi}i≥1 are as deﬁned in Theorem 8.
When a = 1 the right hand side of
(3.2.9) is equal to 0.6931 and when a = 10 the probability is 0.5249. Therefore, for
almost all i, with certain values of a there is a non-negligible probability of having
non decreasing weights. Therefore, the suggested stopping rules in (3.2.8) and (3.2.5)
with respect to the suggested weights are not eﬃcient in simulating the Dirichlet
process. The weights are not monotonically decreasing, this phenomena will tend
to overestimate the truncation value n. Moreover, there is no guarantee that after
choosing the weights up to the value n, there will not be a non negligible weight.
Zarepour and Al Labadi (2012) proposed a monotonically decreasing approxima-
tion of the Dirichlet process by normalizing the ﬁnite sum Gn of the Gamma process
deﬁned in (3.1.7). This is deﬁned in the next theorem.
Theorem 9 (Zarepour & Al Labadi) Let (θi)i≥1 be an i.i.d. sequence of random
variables with common distribution H, independent of (Γi)i≥1, then as n →∞
PZar&Al Lab
n
=
n
X
i=1
G−1
n

Γi
Γn+1

Pn
i=1 G−1
n

Γi
Γn+1
δθi
a.s
→P Ferg,
(3.2.10)
where, Gn(x) is as deﬁned in (3.1.6).
Note that since G−1
n
is a decreasing function then G−1
n (Γi/Γn+1) > G−1
n (Γi+1/Γn+1).
This is coming from the fact that for any 1 ≤i ≤n, Γi/Γn+1 < Γi+1/Γn+1 al-
most surely. To sample from the Dirichlet process approximation of Zarepour and Al

3. Gamma and Dirichlet Process
40
Labadi (2012), we can use the set of rules describe in Algorithm A. But we would need
to normalize the weights of the Gamma process to get the weights of the Dirichlet
process.
Although the technique used by Zarepour & Al Labadi (2012) to approximate the
Dirichlet process is not based on a truncation method, they propose for comparison
purposes a random stopping rule similar to that given in (3.2.5). For a given tolerance
value of ϵ ∈(0, 1), n can be calculated as follows:
n = inf







j :
G−1
j

Γj
Γj+1

Pj
i=1 G−1
j

Γi
Γj+1
 < ϵ







.
(3.2.11)
Figure 3.8 shows one sample path of the Dirichlet process using Zarepour & Al Labadi
(2012) approximation with a = 5 and H ∼t(5). The truncation value is calculated
following (3.2.11). Thus for ϵ = 0.0001, we get n = 12. Vertical lines in the graph
state the weights in (3.2.10) at diﬀerent location t = θ(i).

3. Gamma and Dirichlet Process
41
Figure 3.8: One sample path of the Dirichlet process approximated by Zare-
pour & Al Labadi (2012) approximation of the Dirichlet process with a = 5,
H ∼t(5). For ϵ = 0.0001, we get n = 12
Figure 3.9 shows ten sample paths of the Dirichlet process approximated by
Zarepour and Al Labadi (2012). For comparison purposes, parameters a = 5, H ∼
t(5) are the same as in the Bondesson(1982) and Sethurman (1994) approximation of
the Dirichlet process discussed earlier. With ϵ = 0.0001 and using the stopping rule
in (3.2.11), we obtain n = 15, 11, 13, 10, 7, 10, 8, 6, 7 and 7, one for each sample path.
Note that the Zarepour & Al Labadi (2012) approximation of the Dirichlet process
uses less weights compared to the other two approximations.

3. Gamma and Dirichlet Process
42
Figure 3.9: Ten paths of Zarepour & Al Labadi (2012) approximation of
Dirichlet process with a = 5, H ∼t(5). For ϵ = 0.0001 the value of n is
equal to n = 15, 11, 13, 10, 7, 10, 8, 6, 7 and 7, one for each sample path.
In the following we present a simulated example showing how we can use the
Dirichlet process to estimate the distribution of an observed data. To see this more
concretely, let us consider an i.i.d. data set coming from a Normal distribution. Let
X1, . . . , Xm ∼P, where P ∼Normal(−2, 1). Note that in practice the actual dis-
tribution of the data is not known in advance. Therefore, we are trying to infer the
actual distribution in practice.

3. Gamma and Dirichlet Process
43
As a Bayesian approach, we need to put a prior guess on P. Consider using the
following prior
P ∼DP(5, t(5)),
then, as discussed in (ii), the posterior distribution of P becomes
P|X1, . . . , Xm ∼DP
 
(5 + m),
 
5
5 + mt(5) +
m
a + m
 1
m

m
X
i=1
δXi
!!
.
In Figure (3.10), the step functions with solid line describe ﬁve sample paths of the
Dirichlet process DP(5, t(5)) prior guess, approximated by Zarepour & Al Labadi
(2012) with n = 1000 (we use Algorithm A to sample from the Dirichlet process). In
here the solid line represents the actual cumulative distribution of the data set, we
have X1, . . . , Xm ∼Normal(−2, 1). The dotted step functions represent ﬁve diﬀerent
paths of the posterior distribution of the Dirichlet process approximated after observ-
ing m data points. From top to bottom, m is chosen to be 5, 20 and 200 respectively.
We can see how regardless of our prior guess, the posterior distribution will converge
to the actual distribution of the data set with increasing number of observation.

3. Gamma and Dirichlet Process
44
Figure 3.10: Solid step functions show the Dirichlet process prior using the
Zarepour & Al Labadi (2012)’s approximation with a = 5, H ∼t(5) and
n = 1000. The solid line is the actual cumulative distribution of the data set
which is in our case Normal(−2, 1). Top to bottom plot shows the posterior
distribution of the Dirichlet process after observing m = 5, 20 and 200 data
points respectively.

Chapter 4
Two-parameter Poisson-Dirichlet
and the normalized
inverse-Gaussian process
4.1
Two-parameter Poisson-Dirichlet process
The two parameter Poisson-Dirichlet process also known as the Pitman-Yor process
is a generalization of the Dirichlet process. It has also been used as prior in non-
parametric Bayesian inference. Let PH,α,a ∼PDP(H; α, a) denote a two parameter
Poisson-Dirichlet, the probability measure H is called the based measure, where α and
a are called the discount parameter and the concentration parameter, respectively.
4.1.1
Deﬁnition of the two-parameter Poisson-Dirichlet pro-
cess
Pitman and Yor (1997) introduce the stick-breaking deﬁnition of the two-parameter
Poisson-Dirichlet process deﬁned on an arbitrary measurable space (Ω, F).
45

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
46
Deﬁnition 4.1.1 Let 0 ≤α < 1, a > −α and (βi)i≥1 be a sequence of independent
random variables with Beta(1 −α, a + iα) distribution. Deﬁne
p
′
1 = β1,
p
′
i = βi
i−1
Y
j=1
(1 −βj),
i ≥2.
Let (θi)i≥1 be a sequence of i.i.d.
random variables with common distribution H,
independent of (βi)i≥1 and p1 ≥p2 ≥· · · be the sorted values of (p
′
i)i≥1. Then the
random probability measure
PH,α,a(·) =
∞
X
i=1
piδθi(·)
(4.1.1)
is called a two-parameter Poisson-Dirichlet process with parameters α, a and H.
Note that for the special case when α = 0, (βi)i≥1 become a sequence of independent
random variables with Beta(1, a). Thus, the two-parameter Poisson-Dirichlet process
PH,0,a becomes simply the Dirichlet process. On the other hand, when a = 0, Pitman
and Yor (1997) show that the two-parameter Poisson-Dirichlet process PH,α,0, becomes
the normalized non-negative Stable law process with index α ∈(0, 1).
Note that for any measurable subset A of Ω, the two-parameter Poisson-Dirichlet
process has the following properties,
E(PH,α,a(A)) = H(A),
V ar(PH,α,a(A)) = H(A)(1 −H(A))1 −α
1 + θ .
For more details on the calculation of the moments for the two-parameter Poisson-
Dirichlet process, interested reader can refer to Carlton (1999).
Similar to the Dirichlet process, the base measure H plays the role of the center
of the process. Whereas, both α and a govern the variability of PH,α,a around its base
measure H.
The next theorem derives the posterior distribution of PH,α,a given the data set.
Note that unlike the Dirichlet and Beta process, the two-parameter Dirichlet process
does not have the conjugacy property.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
47
Theorem 10 Let X1, . . . , Xm be a sample from PH,α,a. Let n be the number of dis-
tinct X′
is, X
′
j be the jth distinct Xi and mj be the number of Xi equal to X
′
j. Then
PH,α,a|X1, . . . , Xm
d=
n
X
j=1
WjδX′
j + Wn+1PH,α,a+nα,
where PH,α,a+nα ∼PDP(H, α, a + nα) and (W1, . . . , Wn+1) ∼Dir(m1 −α, . . . , mn −
α, a + nα) and such that they are conditionally independent given X1, . . . , Xm.
4.1.2
Approximation of the two-parameter Dirichlet process
The inﬁnite sum in (4.1.1) makes it diﬃcult in practice to draw a sample from the
Poisson-Dirichlet process. An approximation of the two-parameter Poisson-Dirichlet
process can be done by truncating the higher order terms in the sum (4.1.1) (Al
Labadi and Zarepour (2014)).
Let (βi)i≥1, (pi)i≥1 and α are as deﬁned earlier, with βn = 1 (Ishwaran and James,
2001). The random probability measure
PH,α,a(·) =
n
X
i=1
piδθi(·)
(4.1.2)
is the ﬁnite approximation of the two-parameter Poisson-Dirichlet process. Mimicking
the same stopping rule n proposed by Muliere and Tradella (1998) for the Dirichlet
process,
n = inf{i : p
′
i = (1 −β1) · · · (1 −βi−1)βi < ϵ},
for ϵ ∈(0, 1).
Al Labadi and Zarepour (2014) show that the weights, (p
′
i)i≥1, in (4.1.2) before
ordering them are not strictly decreasing almost surely (see Lemma 1 of Al Labadi
and Zarepour (2014) for the proof).
Pitman and Yor (1997) propose a diﬀerent interesting approach to construct the
two-parameter Poisson Dirichlet process. This is described in the next proposition.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
48
Proposition 4.1.2 (Pitman and Yor 1997, Proposition 22) For 0 < α < 1
and a > 0, suppose (p1(0, a), p2(0, a), . . .) has distribution PD(0, a) and (p1(α, 0),
p2(α, 0), . . .) has distribution PD(α, 0).
Independent of (p1(0, a), p2(0, a), . . .), let
(pi
1(α, 0), pi
2(α, 0), . . .), i = 1, 2, . . . be a sequence of independent copies of (p1(α, 0),
p2(α, 0), . . .). Let (pi)i≥1 be the descending order statistics of {pi(0, a)pi
j(α, 0), i, j =
1, 2, . . . }. Then (p1, p2, . . . ) has a PD(α, a) distribution.
Note that the weights of the two-parameter Poisson-Dirichlet process is con-
structed based on two diﬀerent choices of parameters. One with α = 0 which corre-
spond to the Dirichlet process PH,0,a and another when a = 0 which correspond to
the normalized Stable law process PH,α,0. The index of the Stable law process α is
in (0, 1). Therefore an approximation of the two-parameter Poisson-Dirichlet process
would require to draw independently a sample from the Dirichlet process and from
the normalized Stable law process.
Pitman and Yor (1997, Proposition 10) prove that the sum representation of the
normalized Stable law process can be written as follows
PH,α,0(·) =
∞
X
i=1
Γ−1/α
i
P∞
i=1 Γ−1/α
i
δθi(·).
Therefore, the approximation of the normalized Stable law process is
PH,α,0(·) =
n
X
i=1
Γ−1/α
i
Pn
i=1 Γ−1/α
i
δθi(·),
(4.1.3)
for large enough n. Note that the weights

Γ−1/α
i
/ Pn
i=1 Γ−1/α
i

1≤i≤n are not nec-
essarily strictly decreasing. Al Labadi and Zarepour (2014) approximate the two-
parameter Poisson-Dirichlet process by ﬁrst sampling a draw from the Dirichlet pro-
cess given in (3.2.10)(using Algorithm A), then sampling a sample path of the nor-
malized Stable law process in (4.1.3). Al Labadi and Zarepour (2014) compared their
approximation with the corresponding stick-breaking approximation given in (4.1.2).
Through simulation, Al labadi and Zarepour (2014) show that the two-parameter

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
49
Poisson-Dirichlet approximated using their approach concludes more precise results
compared to the one obtained using the stick-breaking approximation in (4.1.2).
Figure 4.1 shows one sample path of the two parameter Poisson-Dirichlet process.
We take H to be a t-distribution with 5 degrees of freedom, α = 0.5 and a = 1. The
graph shows as well the weights in (4.1.2) as vertical lines at diﬀerent locations t = θ(i).
4.2
Normalized inverse-Gaussian process (NIGP)
Analogous to the Dirichlet process, Lijoi, Mena and Pr¨unster (2005) deﬁne the nor-
malized inverse Gaussian process as a prior to use in Bayesian nonparametric infer-
ence.
Deﬁnition 4.2.1 The random vector (Z1, . . . , Zm) is said to have a normalized inverse-
Gaussian distribution with parameters (γ1, . . . , γm), where γi > 0 for all i, if it has
the joint density function
f(z1, . . . , zm) = e
P∞
i=1 γi Qm
i=1 γi
2m/2−1πm/2
× K−m/2
 v
u
u
t
m
X
i=1
γi2
zi
!
×

m
X
i=1
γi2
zi
−m/4
×
m
Y
i=1
z−3/2
i
× IS(z1, . . . , zm),
where K is the modiﬁed Bessel function of the third type, S = {(z1, . . . , zm) : zi ≥
0, Pm
i=1 zi = 1}, and IS represents the indicator function of the set S.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
50
Figure 4.1: One sample path of the two parameter Poisson-Dirichlet process
with H ∼t(5), α = 0.5 and a = 1.
the x-axis shows atoms generated
from θi ∼H in increasing order and the y-axes represent the resulting two
parameter Poisson-Dirichlet process. Vertical lines represent the intensity of
the weights calculated from (4.1.2).

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
51
4.2.1
Deﬁnition of the normalized inverse-Gaussian process
(NIGP)
Deﬁnition 4.2.2 Let E be a Polish space and B(E) (or Ω) be the Borel σ-algebra
generated by the open sets in E. A random probability measure PH,a = {PH,a(B)}B∈B(E)
is called a normalized inverse-Gaussian process on (E, B(E)) with parameter H
(a ﬁxed probability measure) and a > 0 (concentration parameter), if for any ﬁ-
nite measurable partition B1, . . . , Bm of B(E), the joint distribution of the vector
(PH,a(B1), . . . , PH,a(Bm)) has a normalized inverse-Gaussian distribution with param-
eter (aH(B1), . . . , aH(Bm)). We denote the normalized inverse-Gaussian process with
parameters a and H by NIGP(H, a).
Here are some basic properties of the normalized inverse-Gaussian process. For any
B ∈Ω,
E(PH,a(B)) = H(B),
V ar(PH,a(B)) = H(B)(1 −H(B))
ξ(a)
,
where
ξ(a) =
1
a2eaΓ(−2, a)
and Γ(−2, a) =
R ∞
a u−3e−udu.
Abramowitz and Stegun (1972) show that for large values of a, we have ξ(a) ≈a.
Therefore, similar to the two-parameter Poisson-Dirichlet process, the base measure
H plays the role of the center of the process, while a plays the role of the concentration
parameter.
The posterior distribution of the normalized inverse Gaussian process can be
found in Lijoi, Mena and Pr¨unster (2005). Note that its posterior distribution is not
a conjugacy prior, similar to the two parameter Poisson-Dirichlet process.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
52
4.2.2
Series representation of the NIGP
The normalized inverse-Gaussian process can be written as a series representation.
Let (θi)i≥1 be a sequence of i.i.d. random variables with common distribution H,
independent of Γi. The normalized inverse-Gaussian process has the exact sum rep-
resentation as follows:
PH,a(·) =
∞
X
i=1
L−1(Γi)
P∞
i=1 L−1(Γi)δθi(·),
where
L(x) =
a
√
2π
Z ∞
x
e−t/2t−3/2dt, for x > 0,
(4.2.1)
is the L´evy measure. For more details, the interested reader can refer to Ferguson
and Klass (1972) or Nieto-Barajas and Pr¨unster (2009).
4.2.3
Stick-breaking representation of the NIGP
Similar to the Gamma process, inverting the L´evy measure in (4.2.1) is diﬃcult in
practice. Favaro, Lijoi and Pr¨unster (2012) use a ”stick-breaking” approach to deﬁne
the normalized inverse-Gaussian process.
Let (Zi)i≥1 be i.i.d. random variables where Zi is 1/2-stable random variable
with scale parameter 1. Let X1 ∼GIG(a2, 1, −1/2), deﬁne
V1 =
X1
X1 + Z1
,
for i = 2, 3, . . ., given V1, . . . , Vi−1 and Xi ∼GIG

a2/ Qi−1
j=1(1 −Vj), 1, −i/2

, for
i ≥2
Vi =
Xi
Xi + Zi
,

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
53
The sequence (Xi)i≥1 and (Zi)i≥1 are independent from each other and GIG
denotes the generalized inverse-Gaussian distribution (see equation (2) of Favaro,
Lijoi and Pr¨unster (2012)). Let (θi)i≥1 be a sequence of i.i.d. random variables with
common distribution H independent of (Vi)i≥1 and let
p1 = V1,
pj = Vj
j−1
Y
i=1
(1 −Vi),
j ≥2,
(4.2.2)
then
PH,a(·) =
∞
X
i=1
piδθi(·),
(4.2.3)
is a normalized inverse-Gaussian process with parameters a and H.
4.2.4
Approximation of the NIGP
The normalized inverse-Gaussian process can be approximated by truncating the
higher order terms in the sum (4.2.3) as follows:
Pn,H,a(·) =
n
X
i=1
piδθi(·),
(4.2.4)
where (Vi)i≥1, (pi)i≥1 are as deﬁned in (4.2.2) independent of (θi)i≥1.
Note that
Vn|V1, . . . , Vn−1 = 1 is necessary to make the weights add to 1 almost surely.
A
stopping rule for choosing n is similar to the one proposed by Muliere and Tradella
(1998) for the Dirichlet process, that is for ϵ ∈(0, 1),
n = inf{i : pi = Vj
j−1
Y
i=1
(1 −Vi) < ϵ}.
(4.2.5)
The graph at the top of Figure 4.2 shows one sample path of the normalized
inverse-Gaussian process using the stick breaking approach. We take H to be a t-
distribution with 5 degrees of freedom, a = 1 and n = 100. The graph shows as well
the weights in (4.2.4) in vertical lines at diﬀerent locations t = θ(i). Note that the

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
54
choice of n in that graph is a relatively big value (no stopping rule has been applied for
this ﬁgure). The graph at the bottom shows one same sample path of the normalized
inverse-Gaussian process with same parameters a and H but with n = 3. The value
of n is calculated based on the stopping rule in (4.2.5) for epsilon=0.0001. Note that
the stopping rule for n is not eﬃcient because the weights are not monotonically
decreasing. Therefore it stops before reaching the full approximation of the NIGP.
Moreover, the assumption that Vn|V1, . . . , Vn−1 = 1 for making the weights add to 1
leads to an inaccurate approximation of the NIGP.
4.2.5
Al Labadi & Zarepour approximation of the NIGP
Similar to the Dirichlet process, Al Labadi and Zarepour (2014) derive a ﬁnite
sum representation of the normalized inverse-Gaussian process that converges almost
surely to the Ferguson and Klass (1972) representation.
Let (θi)i≥1 be a sequence of i.i.d. random variables with common distribution H,
independent of (Γi)i≥1, then as n →∞
P Lab
n,H,a =
n
X
i=1
L−1
n

Γi
Γn+1

Pn
i=1 L−1
n

Γi
Γn+1
δθi
a.s.
→PH,a =
∞
X
i=1
L−1(Γi)
P∞
i=1 L−1(Γi)δθi,
(4.2.6)
where
Ln(x) =
Z ∞
x
a
n
√
2πt−3/2 exp

−1
2
 a2
n2t + t

+ a
n

dt.
In here, L(x) is deﬁned in (4.2.1)
For the same reason discussed in Theorem 9, the weights in the sum approxi-
mation of the normalized inverse-Gaussian process (4.2.6) decrease monotonically for
any ﬁxed positive integer n. Recall that for any 1 ≤i ≤n, Γi/Γn+1 < Γi+1/Γn+1

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
55
Figure 4.2: The plot at the top shows one sample path of the NIGP(1, t(5))
using the stick breaking approach with n = 100. The choice of n in this plot
is chosen to be relatively large. The vertical lines show the weights in (4.2.4).
The plot at the bottom depicts one sample path of NIGP(1, t(5)). Choosing
ϵ = 0.0001, we get n = 3 based on the stopping rule in (4.2.5).

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
56
almost surely.
Note that L−1
n
is a decreasing function, therefore L−1
n (Γi/Γn+1) >
L−1
n (Γi+1/Γn+1) almost surely.
Figure 4.3 depicts one sample path of the normalized inverse-Gaussian process
using Al Labadi & Zarepour (2014) approximation. We take H to be a t-distribution
with 5 degree of freedom, a = 1 and n = 50. The graph shows as well the weights
in (4.2.4) in vertical lines at diﬀerent locations t = θ(i). Note that the weights are in
decreasing order.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
57
Figure 4.3: One sample path of the normalized inverse Gaussian process with
H ∼t(5), a = 1 and n = 50. The NIGP(t(5), 1) is sampled using Al Labadi
& Zarepour (2014) approximation

Chapter 5
Beta process
5.1
Beta process
5.1.1
Deﬁnition of the Beta process
In this section, we deﬁne the Beta process and some of its basic properties.
Deﬁnition 5.1.1 (Beta process) [Thibaux & Jordan (2007)]
A Beta process B ∼BP(c, B0) is a completely random measure whose L´evy measure
depends on two parameters, c and B0. The L´evy measure of the Beta process on
Ω× [0, 1] can be written as
ν(dθ, dp) = cp−1(1 −p)c−1dpB0(dθ),
where B0 is a diﬀuse ﬁnite measure on (Ω, F) and c > 0. The total mass of B0
denoted by γ = B0(Ω) is called the mass parameter.
Notice that B is only a ﬁnite measure but not necessarily a random probability
measure. One of the basic properties of the Beta process is that for any set S ∈Ω
E[B(S)] = B0(S)
and
V ar[B(S)] = B0(S)
c + 1
58

5. Beta process
59
See Hjort (1990) and Thibaux & Jordan (2007), for more details. Similar to the
Dirichlet process, c is called the concentration parameter and B0 is called the base
measure. Note that as c →∞, V ar[B(S)] →0, thus with higher value of c, the Beta
process is more tightly close to the base measure B0. In general the concentration
parameter c is a function of θ but in this thesis we focus on the case where c is a
constant.
5.1.2
Series representation of the Beta process
The Beta process with continuous base can be represented as a series representation
following Ferguson (1972) such that
BFerg(·) =
∞
X
i=1
ν−1(Γi)δθi(·),
(5.1.1)
where
ν(x) = cγ
Z 1
x
p−1(1 −p)c−1dp,
and (θi)i≥1 is a sequence of i.i.d. random variables with common distribution B0/γ
independent of Γi. Note that for any set S ∈Ω, the inﬁnite sum in (5.1.1) is ﬁnite
only if B0 is ﬁnite.
In the case when the base measure B0 is discrete of the form B0 = P
i qiδθi, then
B has atoms at the same locations θi and the Beta process is deﬁned as follows
B =
∞
X
i=1
piδθi
pi ∼Beta(cqi, c(1 −qi)),
(5.1.2)
for qi ∈(0, 1). When the base measure B0 is the combination of discrete and continu-
ous, then the Beta process is the sum of two independent contributions. More details
can be found in Thibaux & Jordan (2007).

5. Beta process
60
5.1.3
Stick-breaking representation of the Beta process
Sampling B in (5.1.1) directly from the inﬁnite Beta process is diﬃcult because the
inverse of the L´evy measure doesn’t have a simple closed form. Wolpert and Ick-
stadt (1998b) introduce the inverse L´evy measure algorithm, a generic technique for
pure-jump non negative L´evy processes that allows to sample from B. This technique
generates the weights in (5.1.1) in decreasing order but requires inverting the incom-
plete beta function at each step which is computationally intensive. Paisley, John W
and Zaas, Aimee K and Woods, Christopher W and Ginsburg, Geoﬀrey S and Carin,
Lawrence (2010) and Broderick, Tamara and Jordan, Michael I and Pitman, Jim and
others (2012) proposed the stick-breaking representation of the Beta process that
provides a simple recursive procedure for obtaining the weights in equation (5.1.1).
This approach provides an explicit representation of a draw B from the Beta process
that doesn’t require inverting the L´evy measure,
B =
∞
X
i=1
Ci
X
j=1
V (i)
i,j
i−1
Y
l=1
(1 −V (l)
i,j )δθi,j,
(5.1.3)
Ci ∼Poisson(γ)
V (l)
i,j
i.i.d.
∼Beta(1, c)
θi,j
i.i.d.
∼B0/γ.
This is an analogue of Sethuraman’s (1994) stick-breaking representation of the
Dirichlet process. The only diﬀerence is that the weights resulting from the stick
breaking representation of the Dirichlet process all come from one single stick, thus
they add up to one. This is not the case in the stick-breaking representation of the
Beta process where the weights come from diﬀerent unit intervals. Therefore, they
do not need to add to one. Nevertheless, the sum in (5.1.3) is ﬁnite almost surely.
The shortcoming of the stick-breaking representation is discussed in Al Labadi and
Zarepour (2015).

5. Beta process
61
5.1.4
Finite Approximation of the Beta process
Another yet eﬃcient way of generating the weights in (5.1.1) is by deriving the ﬁnite
approximation of the Beta process deﬁned by Paisley & Carin (2009) as follows
Bn =
n
X
i=1
pi,nδθi
pi,n
i.i.d.
∼Beta

cγn, c

1 −γn

θi
i.i.d.
∼B0/B0(Ω).
Here (pi,n)1≤i≤n and (θi)1≤i≤n are independent and γn →0 as n →∞.
Later Al Labadi & Zarepour (2015) prove that the ﬁnite approximation Bn con-
verges weakly to Ferguson (1972) representation of the Beta process deﬁne in (5.1.1).
In particular, they show, via proposition 3.21 of Resnick (1987), that as n →∞,
nP[p1,n ∈(x, 1)] =
nΓ(c)
Γ(cγn)Γ(c −cγn)
Z 1
x
pcγn−1(1 −p)c(1−γn)−1dp
v→ν(x) = cγ
Z 1
x
p−1(1 −p)c−1dp,
where, ”
v→” denote vague convergence. One choice of γn is when γn = γ/n. Notice
that for any x > 0, Γ(x) = Γ(x + 1)/x, therefore n/Γ(cγ/n) = cγ/Γ(cγ/n + 1) when
replacing x by cγ/n. Moreover, as n →∞
n
Γ(cγ/n) →cγ
and
Γ(c)
Γ(c −cγ/n) →1.
By deﬁning
νn =
Γ(c)
Γ(cγ/n)Γ(c −cγ/n)
Z 1
x
pcγ/n−1(1 −p)c(1−γ/n)−1dp,

5. Beta process
62
we have as n →∞,
nνn(x)
v→ν(x).
Moreover, they prove that as n →∞,
Bn =
n
X
i
ν−1
n
 Γi
Γn+1

δθi
a.s.
→B =
∞
X
i=1
ν−1(Γi)δθi.
(5.1.4)
Interested readers can refer to Theorem 4 of Al Labadi & Zarepour (2015).
Based on the above results, Al Labadi & Zarepour (2015) describe an eﬃcient algo-
rithm to generate sample from an approximation of the Beta process B ∼BP(c, B0).
Algorithm B gives details of the set of rules to sample from Beta process with con-
tinuous base.
Algorithm B: An approximation of the Beta process with continuous base
1. Choose a large positive integer n.
2. Generate θi
i.i.d.
∼B0/γ, for i = 1, · · · , n.
3. Generate Ei
i.i.d.
∼Exp(1) with p.d.f f(x) = e−xI(x > 0). Let Γi = Pi
k=1 Ek.
4. Compute (ν−1
n (Γi/Γn+1))1≤i≤n. This can be done by evaluating the quantile
function of the beta distribution, Beta(cγ/n, c(1 −γ/n)) at 1 −Γi/Γn+1.
Figure 5.1 shows one sample path of the Beta process with c = 0.8, B0 ∼Uniform(0, 1)
and n = 15 using Al Labadi & Zarepour (2014) algorithm. Similar to the Dirichlet
process, vertical lines show the intensity of the weights in (5.1.4) at location θ(i).
Note that the weights are monotonically decreasing, therefore we are almost surely
conﬁdent that there will not be an intense weight after the last weight observed.
Figure 5.2 depicts ten sample paths of the Beta process approximated by Al
Labadi & Zarepour (2014) algorithm (Algorithm B) where we choose n equal to
100.
The dashed line connected by dots represent the cumulative distribution of

5. Beta process
63
Figure 5.1: One sample path of the Beta process B ∼BP(0.8,Uniform(0, 1)).
The Beta process is approximated using Al Labadi & Zarepour (2014) algo-
rithm ( Algorithm B) with n = 15. The plot shows as well the weights in
(5.1.4) by vertical lines at the associated atoms θi.

5. Beta process
64
B0 ∼Uniform(0, 1). The plot at the top represent the Beta process with c = 1 and
B0 =Uniform(0, 1) where as the plot at the bottom represent the Beta process with
same base measure B0 but with c = 20. The concentration parameter c express the
strength of belief in B0, this is shown in Figure 5.2 where with increase value of c the
Beta process approaches to B0 ∼Uniform(0, 1). This behaviour support our previous
discussion on the basic properties of the Beta process.

5. Beta process
65
Figure 5.2: The plot at the top shows ten sample paths of the Beta process
with c = 1 and B0 ∼Uniform(0, 1). The plot at the bottom shows ten sample
paths of the Beta process with same base measure B0 but with c = 20. We
use Algorithm B to approximate the Beta process in both plots with n = 100.
The dashed line connected by dots in both plots represents the cumulative
distribution of the base measure B0 ∼Uniform(0, 1).

5. Beta process
66
5.2
Beta-Bernoulli process
Thibaux & Jordan (2007) show that the Beta process is the conjugate prior for the
Bernoulli process.
This conjugacy extends the conjugacy between the Beta and
Bernoulli distribution. In this section, we deﬁne the Bernoulli process and we in-
troduce an extension to the Beta process known by the Beta-Bernoulli process.
5.2.1
Deﬁnition of the Beta-Bernoulli process
Deﬁnition 5.2.1 (Bernoulli Process) Let H be a measure on Ω. The Bernoulli
process Y with base measure H, written Y ∼BeP(H), is a completely random vari-
able with L´evy measure
π(dθ, dp) = δ1(dp)H(dθ),
where δ1 is a measure concentrate at 1.
When H is a diﬀuse measure (or has no points of discontinuity), Y has an under-
lying Poisson process with intensity H. It can be proven that Y has the following
representation
Y =
K
X
k=1
δθk
(5.2.1)
K ∼Poisson(H(Ω))
θk
i.i.d.
∼H/H(Ω).
When H is discrete (or consist of ﬁxed points of discontinuity) of the form H =
P∞
k=1 pkδθk, then the Bernoulli process is deﬁned at the same locations θk as follows
Y =
∞
X
i=k
bkδθk
bk ∼Bernoulli(pk).
(5.2.2)

5. Beta process
67
When the base measure is the mixture of discrete and continuous, the Beta-Bernoulli
process is the superposition of two independent contributions.
The Beta process is useful as parameter for the Bernoulli process. When combining
both processes together such that the base measure of the Bernoulli process is chosen
to be the Beta process, the resulting process is known as the Beta-Bernoulli process.
Deﬁnition 5.2.2 (Thibaux and Jordan (2007)) The Beta-Bernoulli process, de-
noted by X ∼BeP(B), is a completely random measure with base measure the Beta
process. The model is deﬁned as follows
X|B
i.i.d.
∼BeP(B)
B ∼BP(c, B0),
where B0 and c are deﬁne in Deﬁnition 5.1.1.
5.2.2
Series representation of the Beta-Bernoulli process
A draw from the Beta process B ∼BP(c, B0) generates a set of atoms (pi, θi)i≥1,
where pi is the weight in (5.1.1) calculated at location θi. The Beta-Bernoulli has a
series representation as follows
X =
∞
X
i=1
biδθi,
(5.2.3)
where bi ∼Bernoulli(pi) at location θi.
Note that from the strong law of large numbers, we get
E[X] = E[E[X|B]] = E[B] = B0.
This property of the Beta-Bernoulli process ensure that the series representation in
(5.2.3) converges even though there is an inﬁnite terms.

5. Beta process
68
5.2.3
Beta process conjugate prior for the Bernoulli process
The Beta process is the conjugate prior for the Bernoulli process. Following Thibaux
& Jordan (2007) notation, suppose we observe N data points such that
X1, · · · , XN|B
i.i.d.
∼BeP(B)
B ∼BP(c, B0),
then applying Theorem 3.3 of Kim (1999) the posterior distribution of B given the
observed data X1, · · · , XN is
B|X1, . . . , XN ∼BP(c + N, Bn)
Bn =
c
c + N B0 +
1
c + N
N
X
i=1
Xi
Bn =
c
c + N B0 +
N
c + N
PN
i=1 Xi
N
.
(5.2.4)
As N →∞, the base measure approaches the empirical distribution of Xi. And
as c →∞the base measure approaches to the prior guess B0. We recall that the X′
is
are Beta-Bernoulli random measures not a random number.
The plot at the top of Figure 5.3 shows one sample path of the Beta process with
c = 1, B0 ∼Uniform(0, 1). We use Paisley & Carin (2009) approximation and we
follow Al Labadi & Zarepour (2015) algorithm with n = 15 to approximate the Beta
process. Vertical lines show the intensity of the weights in (5.1.4) at location θ(i). The
plot at the bottom of Figure 5.3 shows 10 draws from the Beta-Bernoulli process, one
per line. We use the Beta process displayed at the top of Figure 5.3 as the base
measure to the Bernoulli process. A draw is a set of points (bi, θi)1≤i≤15, such that
bi ∼Bernoulli (B{θi}). Thus, each line has a black dot at position θi with probability
(B{θi}), B ∼BP(1, Uniform(0, 1)).

5. Beta process
69
Figure 5.3: The plot at the top depicts one sample path of the Beta process
with c = 1, B0 ∼Uniform(0, 1). The Beta process is approximated using
Algorithm B with n = 15.
The vertical lines shows the intensity of the
weights in (5.1.4). The plot at the bottom shows 10 draws of the Bernoulli
processes, one per line, with base measure the Beta process (displayed at the
top of the ﬁgure).

5. Beta process
70
5.3
Applications in Latent Feature Model
The Latent feature model has been widely used in many applications in diﬀerent
disciplines, particularly in machine learning. It has been used to decompose the data
into a small number of components. Example of applications in nonparametric latent
feature model are well explained in Miller & Jordan (2009) and in Paisley & Carin
(2009).
In this section we describe two methodologies to sample from the posterior of a
latent feature model. The ﬁrst method is known as the Indian Buﬀet Process in the
Computer Science community. The second method is a new technique we propose to
sample from the posterior. This new technique focuses on eﬃciency. We describe in
details the sampling techniques in both methodologies in the next sections.
5.3.1
Matrix Z
The Beta-Bernoulli process has been widely used as a prior in applications on latent
feature models. For instance, suppose we observe N data points Z1, Z2, . . . , ZN such
that N is large. In the Bayesian framework, the goal in most applications on latent
feature models is to infer a binary matrix Z, often called factor loadings, where Z is
an N ×K matrix. The number of row N represents the number of observation and K
represents the number of latent features or attributes. The dimension of K is inﬁnite,
thus the rows of Z consist of an inﬁnite collection of factor loadings. The matrix
Z is constructed in a way such that if the ith observation possesses the kth feature
then Zik is equal to 1 otherwise it is equal to 0 where 1 ≤i ≤N and 1 ≤k ≤K.
Notice that each observation possesses one or more features. Therefore, each row of
Z ideally has multiple 1′s. When a data set is modelled with a latent feature, there
is a certain belief that the possession of any number of these features has an eﬀect on
the observed data. Moreover, there is a strong belief that when two entities have a
great number of features in common, those two entities will most probably have the

5. Beta process
71
same structure or behaviour. On most applications involving latent feature models,
the Beta-Bernoulli process Xi is mapped to Zi. The matrix Z with rows, Z1, . . . , ZN,
is constructed in a way such that θk labels the kth column in Z and pi represents the
weight that observation i possesses feature k. Thus, entry at position Zi,k is 1 with
probability pi. Note that by Campbell’s theorem, the base measure B of the Beta-
Bernoulli process has ﬁnite mass. Therefore, the columns of Z have a ﬁnite number
of non-zero entries. Storing only non-zero columns make Z a ﬁnite matrix which is
computationally manageable. It is important to note that columns of the matrix Z
are exchangeable.
To make the connection between the Beta-Bernoulli process and the matrix Z,
let us consider an example to describe this mapping. We extract the set of pairs
(pk, θk)1≤k≤n from the Beta process B ∼BP(1, Uniform(0, 1)) displayed in Figure 5.3
(the plot at the top). The ﬁrst 9 estimated values of (pk, θk) are reported in Table
5.1.
pk
0.73
0.66
0.24
0.0087
0.0024
0.0011
0.00027
0.00019
0.000064
θk
0.93
0.32
0.51
0.93
0.27
0.32
0.89
0.11
0.9
Table 5.1: The table shows the ﬁrst nine set of pairs (pk, θk)1≤k≤9 extracted
from a draw of the Beta process, B ∼BP(1, Uniform(0, 1)).
The Beta
process is approximated using Algorithm B with n = 15.
We extract from the ﬁrst draw of the Beta-Bernoulli process, X1 ∼BeP(B),
displayed in Figure 5.3 the set of pairs (bk, θk)1≤k≤9 such that bk ∼Bernoulli(pk). We
report the ﬁrst nine estimated values in Table 5.2.
For each set of pairs of the form (bk = 1, θk)1≤k≤n, θk is mapped in Z to represent
a new latent feature. In particular, we choose to map θ1 to label the ﬁrst column of
Z, and θ2 to label the second column of Z and so on. As we discussed earlier in this
section, columns of the matrix Z are exchangeable therefore labelling columns of Z

5. Beta process
72
bk
1
0
0
0
0
0
0
0
0
θk
0.93
0.32
0.51
0.93
0.27
0.32
0.89
0.11
0.9
Table 5.2: The table depicts the ﬁrst nine values (bk, θk)1≤k≤9 extracted from
a draw of a Bernoulli process with base measure B ∼BP(1, Uniform(0, 1)).
Recall that bk ∼Binomial(pk), where pk is the probability displayed in Table
5.1.
by θ′s can be done in a diﬀerent order. Note that from Table 5.1 there is three non
negligible weights. Then at most there will be three visible atoms θ′s and consequently
at most three labelled columns in Z representing a latent feature . Indeed, Table 5.2
shows only one pair of the form (bk = 1, θ), then we map θ in Z to represent a new
label.
Therefore, mapping X1 to Z1 gives
Z =
h
1
0
0
0
0
0
0
0
0
0
i
Note that the value of K (the number of column of Z) should be relatively large but
for illustration purposes we choose K = 10. We use the same mapping technique to
map X2, . . . , X10 in Z2, . . . , Z10. We get

5. Beta process
73
Z =


1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0


(5.3.1)
Note that zero’s column means that there is no label (or latent feature) associated to
that column yet.
5.3.2
Updating the matrix Z using Methodology I
Updating the matrix Z with a new observation, ZN+1 based on what have been
observed Z1, . . . , ZN is the key in most nonparametric Bayesian applications. Recall
that
B|X1, . . . , XN ∼BP
 
c + N,
c
c + N B0 +
1
c + N
N
X
k=1
Xk
!
.
In the context of latent feature modelled by the matrix Z, the posterior of the Beta
process has another meaning for its discrete base measure as follows
B|X1, . . . , XN ∼BP
 
c + N,
c
c + N B0 +
K
X
k=1
mN,k
c + N δθk
!
,
(5.3.2)
where, mN,k = PN
i=1 I(Zi,k = 1). In words, mN,k is the number of time among N,
feature k has been observed. Notice that the posterior base measure is constructed of

5. Beta process
74
two components, one continuous and another discrete, it is reasonable to sample from
each component independently since they do not overlap. Therefore, we can write
the Bernoulli process as the sum of two independent Bernoulli processes as follows
XN+1|X1, · · · , XN ∼BeP

c
c + N B0

+ BeP
 K
X
k=1
mN,k
c + N δθk
!
XN+1|X1, · · · , XN ∼
F
+
R,
where
F ∼BeP

c
c + N B0

R ∼BeP
 K
X
k=1
mN,k
c + N δθk
!
.
The algorithm to updated the matrix Z is deﬁne in Methodology I. It is worth
mentioning that this technique is known by the two-parameter (c, γ) generalization
of the Indian buﬀet process (Ghahramani & Griﬃths, 2005), a well know process in
the Computer Science community.
Methodology I
1. Sample a draw from F ∼BeP
 c
c+N B0

.
2. Map every pair (1, θ
′
k) extracted from F in step 1 to the matrix Z.
3. Sample independently a draw from R ∼BeP
PK
k=1
mN,k
c+N δθk

.
4. Map every pair of the form (bk = 1, θk) extracted from R in step 4 to the matrix
Z.
See next discussion for the details of Methodology I, especially for the mapping in
step 2 and 4.

5. Beta process
75
Updating the matrix Z from the Bernoulli process with continuous base
Updating the matrix Z with a new observation ZN+1 can be done in two stages.
First, it involves sampling from the two Bernoulli processes F and R independently,
then mapping each process independently to the row ZN+1. Sampling the Bernoulli
process F with continuous base measure can be done as examined in (5.2.1)
F ∼BeP

c
c + N B0

F =
K1
X
k=1
δθ′
k
where
K1 ∼Poisson

cγ
c + N

θ
′
k ∼B/B(Ω) = B0/γ.
1 ≤k ≤K1
Note that for each pair of the form (1, θ
′
k)1≤k≤K1 extracted from F, (θ
′
k)1≤k≤K1 are
the new label of features added to the matrix Z. Let k′
1, . . . , k′
K1 be K1 indices of zero
columns of Z, then we let (ZN+1,k′ = 1)k′
1≤k′≤k′
K1.
Updating matrix Z from the Bernoulli process with discrete base
Sampling the Bernoulli process R with discrete base measure can be done as discussed
in (5.2.2)
R ∼BeP
 K
X
k=1
mN,k
c + N δθk
!
,
R =
∞
X
k=1
bkδθk
bk ∼Bernoulli
 mN,k
c + N


5. Beta process
76
Note that θk in the discrete base measure represent an existing latent feature in Z.
Therefore at each non zero column k of Z, ZN+1,k = 1 with probability mN,k/(c + N).
5.3.3
Updating the matrix Z using Methodology II
In this section we describe an alternative way of updating the matrix Z. This can be
done by sampling ﬁrst from the posterior of the Beta process then from the Beta-
Bernoulli process.
B|X1, . . . , XN ∼BP
 
c + N,
c
c + N B0 +
K
X
k=1
mN,k
c + N δθk
!
,
B|X1, . . . , XN ∼BP

c + N,
c
c + N B0

+ BP
 
c + N,
K
X
k=1
mN,k
c + N δθk
!
B|X1, . . . , XN ∼
BCont
+
BDisc,
where
BCont ∼BP

c + N,
c
c + N B0

BDisc ∼BP
 
c + N,
K
X
k=1
mN,k
c + N δθk
!
.
Now sampling a new observation can be done as follows
XN+1|X1, · · · , XN ∼BeP(BCont) + BeP(BDisc)
XN+1|X1, · · · , XN ∼
S
+
T
,
where,
S ∼BeP(Bcont)
T ∼BeP(BDisc).
The following gives further details on the methodology.

5. Beta process
77
Methodology II
1.
(a) Sample a draw from BCont. Extract the set of pairs (pCont
k
, θ
′
k)1≤k≤n from
BCont.
(b) Sample a draw from S ∼BeP(BCont). Extract the set of pairs (bCont
k
, θ
′
k)1≤k≤n.
(c) Map every pair of the form (bCont
k
= 1, θ
′
k) into the Matrix Z.
2.
(a) Sample a draw from BDisc. Extract the set of pairs (pDisc
k
, θk)1≤k≤n.
(b) Sample a draw from T ∼BeP(BDisc). Extract the set of pairs (bDisc
k
, θk)1≤k≤n.
(c) Map every pair of the form (bDisc
k
= 1, θk) in Matrix Z.
Refer to the next discussion for details on updating the matrix Z using Methodology
II.
Updating the matrix Z from the Beta-Bernoulli with continuous base
Updating Z would ﬁrst involve sampling BCont and BDisc independently then sam-
pling from the two Beta-Bernoulli processes S and T independently. Following our
discussion in (5.1.1), the Beta process BCont with continuous base has a series repre-
sentation
BCont =
∞
X
k=1
pCont
k
δθ′
k.
Using Paisley & Carin (2009) ﬁnite approximation of the Beta process in (5.1.4), we
have
BCont
n
=
n
X
k=1
pCont
k,n δθ′
k
pCont
k,n
i.i.d.
∼Beta
c ∗γ∗
n
, c ∗

1 −γ∗
n

(5.3.3)
θ
′
k
i.i.d.
∼B∗
0/γ∗,
(5.3.4)

5. Beta process
78
where
c∗= c + N
γ∗=
c
c + N γ
B∗
0 =
c
c + N B0.
(5.3.5)
Replacing c∗, γ∗and B∗
0 in (5.3.3) and (5.3.4) we get,
BCont
n
=
n
X
k=1
pCont
k,n δθ′
k
pCont
k,n
i.i.d.
∼Beta
cγ
n , N + c −cγ
n

θ
′
k
i.i.d.
∼B0/γ,
The Beta-Bernoulli process S ∼BeP(BCont
n
) has series representation
S ∼BeP(BCont
n
)
S =
n
X
k=1
bCont
k
δθ′
k
bCont
k
∼Bernoulli(pCont
k,n ).
Note that when the Bernoulli process has a continuous base, the resulting process
generates new locations θ
′’s. We map every pair of the form (1, θ
′
k) in ZN+1 such that
every θ
′
k label a zero column of Z. Let k′ represent the indices of a zero column of Z,
then we let ZN+1,k′ = 1.
Going back to our simulated example shown in Figure 5.3, we approximate the
posterior of the Beta process with continuous base, BCont using Al Labadi & Zarepour
(2015) algorithm for n = 15 and with the updated parameters c∗= 11 and B∗
0 ∼
1
11Uniform(0, 1). We extract from BCont the set of pairs (pCont
k
, θ
′
k)1≤k≤15. Table 5.3
shows some preliminary values of those pairs. Recall that the weights generated by Al
Labadi & Zarepour (2015) are in decreasing order almost surely, therefore we omitted

5. Beta process
79
pCont
k
0.068
0.00088
0.0000065
0.00011
0.000057
θ
′
k
0.12
0.87
0.47
0.34
0.29
Table 5.3: The table shows some preliminary values of the pairs (pCont
k
, θ
′
k)
extracted from BCont ∼BP(11, 1
11Uniform(0, 1)).
the last 10 weights in the table because we are almost sure that there will not be a
non negligible weight beyond that point.
We sample a draw from S, the set of pairs (bCont
k
, θ
′
k) are reported in Table 5.4.
bCont
k
1
0
0
0
0
θ
′
k
0.12
0.87
0.47
0.34
0.29
Table 5.4: The table depicts some preliminary values of the pairs (bCont
k
, θ
′
k)
extracted from S ∼BeP(BCont), the Beta-Bernoulli process with base mea-
sure BCont.
Note from Table 5.4 that there is only one pair such that (bCont
k
= 1, θ
′
k = 0.12).
To map this pair in Z, we choose the fourth column (zero column) of Z to label this
new feature and we let Z11,4 = 1. Thus, we have

5. Beta process
80
Z =


1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0


Updating the matrix Z from the Beta-Bernoulli with discrete base
To ﬁnish updating the matrix Z we are left with sampling from the Beta process
BDisc ∼BP

c + N, PK
k=1
mN,k
c+N δθk

with discrete base BDisc
0
, then sampling from the
Beta-Bernoulli process T ∼BeB(BDisc). Following our discussion in (5.1.2), the Beta
process BDisc has series representation
BDisc
n
=
n
X
k=1
pDisc
k
δθk
BDisc
0
=
n
X
k=1
mN,k
c + N δθk
pDisc
k
∼Beta(mN,k, N −mN,k + c).
(5.3.6)
The Beta-Bernoulli process T has series representation
T =
∞
X
k=1
bDisc
k
δθk
bDisc
k
∼Bernoulli(pDisc
k
).
(5.3.7)

5. Beta process
81
We extract from the Beta-Bernoulli process T the set of pairs (bDisc
k
, θk)1≤k≤15.
Let k
′
1, . . . , k
′
n be the indices of column of Z such that there is at least on row con-
taining an atom 1, then we let (ZN+1,k′ = bDisc
k
)k′
1≤k′≤k′
n.
Going back to our simulated example, the ﬁrst three column of Z in (5.3.1) have
at least one row with an atom 1. Those columns are labelled previously by θ1, θ2
and θ3 respectively. Table 5.5 shows the estimated values of pDisc
k
calculated based
on (5.3.6), where Table 5.6 shows the estimated values of bDisc
k
calculated based on
(5.3.7).
pDisc
k
0.90
0.62
0.0055
θk
0.93
0.32
0.51
Table 5.5: The table shows the set of pairs (pDisc
k
, θk)1≤k≤3 such that pDisc
k
is
calculated based on (5.3.6).
bDisc
k
1
1
0
θk
0.93
0.32
0.51
Table 5.6: The table shows the set of pairs (bDisc
k
, θk)1≤k≤3, where bDisc
k
∼
Bernoulli(pDisc
k
).
The matrix Z is updated with the Beta-Bernoulli process T as follows:

5. Beta process
82
Z =


1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
1
0
1
0
0
0
0
0
0


Figure 5.4 shows a draw from the posterior of the Beta-Bernoulli process X11
given X1, . . . , X10. The draw are the set of pairs of the form (bCont
k
= 1, θ
′
k) and
(bDisc
k
= 1, θk). The triangle pointed down represent a sampling of the Beta-Bernoulli
process, T with discrete base measure and the triangle pointed up represent a sampling
of the Beta-Bernoulli process, S with continuous base measure.

5. Beta process
83
Figure 5.4: The plot at the top shows one draw of the Beta process B ∼
BP(1, Uniform(0, 1)) approximated by Algorithm B with n = 15. The plot at
the bottom shows 10 draws of the Beta-Bernoulli process with base measure
B. Draws are represented in the plot by dots at each pair of the form (bk =
1, θi) generated from the Beta-Bernoulli process. The plot at the bottom
shows as well one updated draw of the Beta-Bernoulli process given the 10
other observations. Triangle pointed down represent the update contributed
by the discrete base, and triangle pointed up represent the update contributed
by the continuous part of the updated Beta process.

5. Beta process
84
5.3.4
Nonparametric Latent Feature Models for Link Predic-
tion
Miller & Jordan (2009) introduce the nonparametric latent feature relational model
used for social network data. This model seeks to extract latent structure representing
the properties of individual entities from the observed data. In particular, we observe
the relationships (or links) between a set of entities in a network and we try to
predict unobserved links. For example, consider Facebook as our social network. In
such network, we only know some subset of people who are friends with and some
other who are not. The goal would be to predict which other people are likely to
become friend.
5.3.5
Basic model
Assume we observe the directed links between a set of N entities. Let Y be an N ×N
binary matrix that contains these links. That is Yij = 1 if entity i is linked to entity
j (i →j), Yij = 0 if entity i is not linked to entity j, and Yij is left empty if we
don’t observe any link. The link can stand for diﬀerent meanings such as ”send a
friend request or not”, ”friend or not”, ”colleague or not” or any other relationship.
Depending of the relation, the matrix Y can be symmetric or asymmetric. The model
decompose the binary matrix Y in two matrices Z and W. Where Z is a N × K
matrix; each row of Z corresponds to an entity and each column corresponds to a
feature such that Zik = 1 if entity i has feature k, otherwise take Zik = 0.
For
instance, we can have a separate feature for ”statistician”, ”female”, ”athlete”, and
”painter” and the presence or absence of each of these features is what deﬁnes each
person and determines their relationships. And W is a K × K real valued matrix
where the (k, k′) entry of W is wk,k′. The value wkk′ is the weight that aﬀects the
probability of having a link from entity i to entity j if both entity i has feature k and
entity j has feature k′. If we are looking at the relation ”send a friend request”, then

5. Beta process
85
the weight at the (statistician, athlete) entry of W would correspond to the weight
that a statistician would send a friend request to an athlete.
A positive weights
would correspond to an increased probability, a negative weight would correspond to
a decreased probability, and a zero weight would indicate that there is no correlation
between these two features and the observed relation. Thus, the observed relations
depend on binary valued latent features that inﬂuences its relations, weighted with a
set of known covariates.
Following the notation of Miller & Jordan (2009), the model is deﬁned as follows:
Yij ∼Bernoulli(σ(ZiWZT
j ))
B ∼BP(1, B0)
Z ∼BeP(B)
wkk′ ∼Normal(0, σ2
w),
where, σ(·) is a function that transforms values on (−∞, ∞) to (0, 1) such as the
sigmoid function σ(x) = 1/(1 + exp(−x)). Note that the Beta process B can be
approximated using Algorithm B. Thus B = Pn
i=1 pkδθk and Zik ∼Bernoulli(pk).
Another contribution in this thesis is to modify (improve) the mathematical notation
of what most Computer Scientist has adopted in machine learning community. The
matrix Z is the map of the Beta-Bernoulli process X ∼BeP(B) to Z we discussed
earlier in Section 5.3.1. It is worth mentioning that Miller & Jordan (2009) have put
an Indian Buﬀet Process IBP(α) prior on the matrix Z. Jordan (2007) proves that
the Beta process with concentration parameter c = 1 and base measure B0 is the
IBP(α), with α = B0(Ω).
Given the full set of observation Y , we wish to infer the posterior distribution of
the feature matrix Z and the weights W. This can be done by using Bayes’ theorem,
p(Z, W|Y ) ∝P(Y |Z, W)P(Z)P(W) with an independent prior on Z and W. For
details on inference, interested reader can refer to Miller & Jordan (2009).

Appendix A
Deﬁnitions of background
knowledge
In this appendix we discuss some properties of random measure which are mentioned
throughout this thesis.
Deﬁnition A.0.1 (Convergence of Random Measures) (Kallenberg, 1983) Let
E be a Polish space and B(E) be a Borel σ-algebra generated by the open sets in E.
A measure µ is called Radon if µ(K) < ∞for any compact set K ∈E. Let M+(E)
be the space of Radon measures in E. Let M+(E) be the smallest σ-algebra of subsets
of M+(E) making the maps µ →µ(f) =
R
f(x)dµ(x) from M+(E) to R measurable
for all functions f ∈C+
K(E), where C+
K(E) denotes the set of continuous functions
f : E →[0, ∞) with compact support. Note that, M+(E) is the Borel σ-algebra
generated by the topology of vague convergence. If µn, µ ∈M+(E), we say that (µn)n
converges vaguely to µ, if µn(f)
v→µ(f) for any f ∈C+
K(E).
A random measure on E is any measurable map ξ deﬁned on a probability space
(Ω, F, P) with values in (M+(E), M+(E)). If ξn and ξ are random measures on E,
we say that (ξn)n converges in distribution to ξ (we write ξn
d→ξ) if {P ◦ξ−1
n }n
converges weakly to P ◦ξ−1. By Theorem 4.2 of Kallenberg (1983), ξn
d→ξ if and
86

A. Deﬁnitions of background knowledge
87
only if ξn(f) →ξ(f), i.e.
Z
E
f(x)ξn(dx) →
Z
E
f(x)ξ(dx),
∀f ∈C+
K(E).
We say that (ξn)n converges vaguely almost surely to ξ (and we write ξn
a.s.
→ξ) if there
exist a set eΩ∈F with P(eΩ) = 1 such that ∀w′ineΩ,
ξn(w, ·)
v→ξ(w, ·), i.e.
Z
E
f(x)ξn(w, dx) →
Z
E
f(x)ξ(w, dx),
∀f ∈C+
K(E).
The space M+(E) endowed with the vague topology is a complete separable metric
space Resnick (1987). For more details about random measures refer to Kallenberg
(1983).

Bibliography
[1] Al Labadi, Luai, and Mahmoud Zarepour (2014). On simulations
from the two-parameter Poisson-Dirichlet process and the normalized inverse-
Gaussian process. Sankhya A 76.1, 158-176.
[2] Al Labadi, Luai (2012). On New Constructive Tools in Bayesian Nonpara-
metric Inference. PhD thesis, University of Ottawa .
[3] Labadi, Luai Al and Zarepour, Mahmoud (2015). On Approximations
of the Beta Process in Latent Feature Models. arXiv preprint arXiv:1411.3434.
[4] Al Labadi, Luai and Zarepour, Mahmoud and others (2013). On
asymptotic properties and almost sure approximation of the normalized inverse-
Gaussian process. Bayesian Analysis 8, 553–568.
[5] Abramowitz, Milton and Stegun, Irene A (1972) Handbook of Mathe-
matical Functions with Formulas, Graphs, and Mathematical Tables. National
Bureau of Standards Applied Mathematics Series 55. Tenth Printing. Dover
Publications, Mineola, New York.
[6] Banjevic, Dragan and Ishwaran, Hemant and Zarepour, Mahmoud
and others (2002). A recursive method for functionals of Poisson processes.
Bernoulli Society for Mathematical Statistics and Probability, Volume 8 295–
311.
88

BIBLIOGRAPHY
89
[7] Bert Fristedt and Lawrence Gray (1996). A Modern Approach to Prob-
ability Theory. Birkhauser Boston.
[8] Bondesson, Lennart (1982). On simulation from inﬁnitely divisible distri-
butions. Advances in Applied Probability 855–869.
[9] Broderick, Tamara and Jordan, Michael I and Pitman, Jim and
others (2012). Beta processes, stick-breaking and power laws. Bayesian anal-
ysis. International Society for Bayesian Analysis. Vol 7
439–476.
[10] Carlton,
Matthew Aaron (1999) Applications of the two-parameter
Poisson-Dirichlet distribution. PhD thesis, University of California, Los An-
geles
[11] Caron, Franc¸ois and Fox, Emily B (2014). Bayesian nonparametric mod-
els of sparse and exchangeable random graphs. arXiv preprint arXiv:1401.1137.
[12] Caron, Franc¸ois and Teh, Yee Whye and Murphy, Thomas Bren-
dan (2013). Bayesian nonparametric Plackett-Luce models for the analysis of
clustered ranked data. CoRR.
[13] Favaro, Stefano and Lijoi, Antonio and Pr¨unster, Igor (2009).
On the stick-breaking representation of normalized inverse Gaussian priors.
Biometrika, 99 663774.
[14] Favaro, Stefano and Lijoi, Antonio and Mena, Rams´es H and
Pr¨unster, Igor (2009). Bayesian non-parametric inference for species vari-
ety with a two-parameter Poisson–Dirichlet process prior. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 71
993–1008.
[15] Ferguson, Thomas S (1973). A Bayesian analysis of some nonparametric
problems. The Annals of Statistics 1
209–230.

BIBLIOGRAPHY
90
[16] Ferguson, Thomas S and Klass, Michael J (1972). A representation of
independent increment processes without Gaussian components. The Annals of
Mathematical Statistics 1
209–230.
[17] Ghahramani, Zoubin and Griffiths, Thomas L (2005). Inﬁnite latent
feature models and the Indian buﬀet process. Advances in Neural Information
Processing Systems 475–482.
[18] Hjort, Nils Lid (1990). Nonparametric Bayes estimators based on beta pro-
cesses in models for life history data. The Annals of Statistics, 18(3) 1259–1294.
[19] Ishwaran, Hemant and James, Lancelot F (2001). Gibbs sampling meth-
ods for stick-breaking priors. Journal of the American Statistical Association 96,
161–173.
[20] Ishwaran, Hemant and Zarepour, Mahmoud (2002). Exact and approx-
imate sum representations for the Dirichlet process. The Canadian Journal of
Statistics 30 269–283.
[21] Kallenberg, O (1983). Random Measures. Third edition. Akademie-Verlag,
Berlin.
[22] Kim, Yongdai (1999). Nonparametric Bayesian estimators for counting pro-
cesses. Annals of Statistics, JSTOR 562–588.
[23] Kingman, John Frank Charles (1992). Poisson processes. Oxford Univer-
sity Press, Volume 3.
[24] Kingman, John (1967). Completely random measures. Paciﬁc Journal of
Mathematics 21
59–78.

BIBLIOGRAPHY
91
[25] Lijoi, Antonio and Mena, Rams´es H and Pr¨unster, Igor (2005). Hi-
erarchical mixture modeling with normalized inverse-Gaussian priors. Journal
of the American Statistical Association, 100
1278–1291.
[26] Miller, Kurt (2011) A Bayesian Nonparametric Latent Feature Models. PhD
thesis, University of California, Berkeley.
[27] Miller, Kurt and Jordan, Michael I and Griffiths, Thomas L (2009)
Nonparametric latent feature models for link prediction. Advances in Neural
Information Processing Systems, 1276–1284.
[28] Muliere, Pietro and Tardella, Luca (1998). Approximating distribu-
tions of random functionals of Ferguson-Dirichlet priors. The Canadian Journal
of Statistics 26, 283–297.
[29] Nieto-Barajas, Luis E and Pr¨unster, Igor (2009). A sensitivity analysis
for Bayesian nonparametric density estimators. Statistica Sinica, 19 685–705.
[30] Paisley, John and Carin, Lawrence (2009). Nonparametric factor analy-
sis with beta process priors. Proceedings of the 26th Annual International Con-
ference on Machine Learning 777–784.
[31] Paisley, John W and Zaas, Aimee K and Woods, Christopher W
and Ginsburg, Geoffrey S and Carin, Lawrence (2010). A stick-
breaking construction of the beta process. Proceedings of the 27th International
Conference on Machine Learning (ICML-10) 847–854.
[32] Pitman, Jim and Yor, Marc (1997). The two-parameter Poisson-Dirichlet
distribution derived from a stable subordinator. The Annals of Probability 855–
900.
[33] Resnick, Sidney I (1987). Extreme values, regular variation, and point pro-
cesses. Springer-Verlag, New York.

BIBLIOGRAPHY
92
[34] Robert L. Wolpert and Katja Ickstadt. (1998b). Simulation of L´evy
random ﬁels. Practical Nonparametric and Semiparametric Bayesian Statistics
Lecture Notes in Statistics, 133 227–242.
[35] Sethuraman, J. (1994). A Constructive Deﬁnition of Dirichlet Priors. Statis-
tica Sinica 4, 639–650.
[36] Teh, Yee Whye and Jordan, Michael I (2010). Hierarchical Bayesian
nonparametric models with applications. Bayesian nonparametrics, Camb. Ser.
Stat. Probab. Math .
[37] Teh, Yee Whye (2010). Dirichlet process. Encyclopedia of machine learning,
Springer 280–287.
[38] Thibaux, Romain Jean (2008). Nonparametric Bayesian models for machine
learning. Thesis
[39] Thibaux, Romain and Jordan, Michael I (2007). Hierarchical beta pro-
cesses and the Indian buﬀet process. International conference on artiﬁcial in-
telligence and statistics 564–571.
[40] Titsias, Michalis K (2008). The inﬁnite gamma-Poisson feature model. Ad-
vances in Neural Information Processing Systems, 1513–1520.
[41] Wolpert, Robert L and Ickstadt, Katja (1998). Simulation of L´evy
random ﬁelds. Practical Nonparametric and Semiparametric Bayesian Statis-
tics, Springer 227–242.
[42] Zarepour, Mahmoud and Al Labadi, Luai (2012). On a rapid simulation
of the Dirichlet process. Statistics & Probability Letters 82.5, 916–924.

