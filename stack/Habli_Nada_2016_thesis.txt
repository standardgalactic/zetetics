Nonparametric Bayesian modelling in Machine Learning
Nada Habli
Thesis submitted to the Faculty of Graduate and Postdoctoral Studies
in partial fulÔ¨Ållment of the requirements for the degree of Master of Science in
Mathematics 1
Department of Mathematics and Statistics
Faculty of Science
University of Ottawa
c‚ÉùNada Habli, Ottawa, Canada, 2016
1The M.Sc. program is a joint program with Carleton University, administered by the Ottawa-
Carleton Institute of Mathematics and Statistics

Abstract
Nonparametric Bayesian inference has widespread applications in statistics and ma-
chine learning. In this thesis, we examine the most popular priors used in Bayesian
non-parametric inference.
The Dirichlet process and its extensions are priors on
an inÔ¨Ånite-dimensional space. Originally introduced by Ferguson (1983), its conju-
gacy property allows a tractable posterior inference which has lately given rise to
a signiÔ¨Åcant developments in applications related to machine learning. Another yet
widespread prior used in nonparametric Bayesian inference is the Beta process and
its extensions. It has originally been introduced by Hjort (1990) for applications in
survival analysis. It is a prior on the space of cumulative hazard functions and it
has recently been widely used as a prior on an inÔ¨Ånite dimensional space for latent
feature models.
Our contribution in this thesis is to collect many diverse groups of nonpara-
metric Bayesian tools and explore algorithms to sample from them. We also explore
machinery behind the theory to apply and expose some distinguished features of these
procedures. These tools can be used by practitioners in many applications.
ii

Acknowledgements
There are many people who provided much support and guidance throughout the
lengthy course of this thesis to whom I am thankful.
First and foremost I am deeply indebted to my supervisor Dr. Mahmoud Zare-
pour who guided and inspired my work. I would like to express my sincere gratitude
to him for redirecting my interest of research and introducing me to machine learn-
ing. It turns out to be a perfect Ô¨Åt to my background study in Computer Science. I
attribute the level of my Masters degree to his encouragement and eÔ¨Äort and without
him this thesis would not have been completed or written. It was a privilege to have
studied and researched under the guidance of this word-class scientist and professor.
Besides my supervisor, I am grateful to Dr. Luai Al Labadi for his continuous
support and advice. He was a past PhD student of Dr. Zarepour and most of his
contributions are in the area of nonparametric Bayesian inference. I sincerely appre-
ciate his support over the phone and the back and forth emails we shared together.
Without his support I would not be able accomplish this much in my thesis.
To my dear colleagues in lab B03 of the department of Mathematics, in partic-
ular Maryam, Jo-Ann, Farid, Sheikh, Erv¬¥e, Rachid, Hicham, Jason and Ibrahim. I
thank you for the opportunity of getting to know such broad-minded, wise and fun
researchers and for your assistance throughout my studies. I have been blessed with
a friendly and cheerful group of fellow students who provided a much needed form of
escape from my studies and for helping me keep things in perspective.
iii

Acknowledgements
iv
I acknowledge and greatly appreciate the Ô¨Ånancial support from the University of
Ottawa for the Admission Scholarship as well as the Swartzen Memorial Scholarship
oÔ¨Äered by the department of Mathematics.
Last but not least, I would like especially to thank my family which consists
of my mother, my father, my brother and my sisters.
My hard-working parents
have sacriÔ¨Åced their lives for my siblings and me while providing unconditional love
and care.
I love them so much, and I would not have made it this far without
them. My brother, Omar and his wife Irialis have been my greatest support during
the diÔ¨Écult moment in my life and I love them dearly. They have given me their
unequivocal support throughout for which my sincere expression of thanks likewise
does not suÔ¨Éce. I know I always have my family to count on when times are rough.

Dedication
To my gorgeous lovely children, Maya, Adam, Jad and Issam.
I love you all deeply and dearly.
v

Contents
List of Figures
ix
List of Tables
xiii
1
Introduction
1
2
L¬¥evy random variables and processes
4
2.1
L¬¥evy process . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.2
Characteristic function
. . . . . . . . . . . . . . . . . . . . . . .
5
2.2.1
InÔ¨Ånitely divisibility
. . . . . . . . . . . . . . . . . . . . . .
5
2.2.2
L¬¥evy-Khintchine deÔ¨Ånition . . . . . . . . . . . . . . . . . . .
6
2.2.3
L¬¥evy-ItÀÜo deÔ¨Ånition
. . . . . . . . . . . . . . . . . . . . . . .
7
2.2.4
Transformation of Poisson Random measure . . . . . . . . .
12
2.3
L¬¥evy random variables . . . . . . . . . . . . . . . . . . . . . . . .
14
2.3.1
Poisson random variable . . . . . . . . . . . . . . . . . . . .
14
2.3.2
Gamma random variable . . . . . . . . . . . . . . . . . . . .
15
2.3.3
Stable random variable . . . . . . . . . . . . . . . . . . . . .
17
3
Gamma and Dirichlet Process
19
3.1
Gamma Process . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.1.1
DeÔ¨Ånition of Gamma process
. . . . . . . . . . . . . . . . .
19
3.1.2
Series representation of the Gamma process
. . . . . . . . .
20
vi

CONTENTS
vii
3.1.3
Approximation of the Gamma process
. . . . . . . . . . . .
21
3.2
Dirichlet process . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.2.1
DeÔ¨Ånition of the Dirichlet distribution
. . . . . . . . . . . .
28
3.2.2
DeÔ¨Ånition of the Dirichlet process . . . . . . . . . . . . . . .
30
3.2.3
Series representation of the Dirichlet process . . . . . . . . .
31
3.2.4
Approximation of the Dirichlet process . . . . . . . . . . . .
32
4
Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
45
4.1
Two-parameter Poisson-Dirichlet process
. . . . . . . . . . . . .
45
4.1.1
DeÔ¨Ånition of the two-parameter Poisson-Dirichlet process
.
45
4.1.2
Approximation of the two-parameter Dirichlet process . . . .
47
4.2
Normalized inverse-Gaussian process (NIGP) . . . . . . . . . . .
49
4.2.1
DeÔ¨Ånition of the normalized inverse-Gaussian process (NIGP)
51
4.2.2
Series representation of the NIGP . . . . . . . . . . . . . . .
52
4.2.3
Stick-breaking representation of the NIGP . . . . . . . . . .
52
4.2.4
Approximation of the NIGP . . . . . . . . . . . . . . . . . .
53
4.2.5
Al Labadi & Zarepour approximation of the NIGP
. . . . .
54
5
Beta process
58
5.1
Beta process . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
5.1.1
DeÔ¨Ånition of the Beta process . . . . . . . . . . . . . . . . .
58
5.1.2
Series representation of the Beta process . . . . . . . . . . .
59
5.1.3
Stick-breaking representation of the Beta process
. . . . . .
60
5.1.4
Finite Approximation of the Beta process
. . . . . . . . . .
61
5.2
Beta-Bernoulli process . . . . . . . . . . . . . . . . . . . . . . . .
66
5.2.1
DeÔ¨Ånition of the Beta-Bernoulli process . . . . . . . . . . . .
66
5.2.2
Series representation of the Beta-Bernoulli process . . . . . .
67

CONTENTS
viii
5.2.3
Beta process conjugate prior for the Bernoulli process . . . .
68
5.3
Applications in Latent Feature Model . . . . . . . . . . . . . . .
70
5.3.1
Matrix Z . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
5.3.2
Updating the matrix Z using Methodology I . . . . . . . . .
73
5.3.3
Updating the matrix Z using Methodology II
. . . . . . . .
76
5.3.4
Nonparametric Latent Feature Models for Link Prediction
.
84
5.3.5
Basic model . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
A DeÔ¨Ånitions of background knowledge
86
Bibliography
88

List of Figures
3.1 One sample path of the Gamma process, GBond ‚àºGP(a, H) using
Bondeson (1982) approximation. We choose a = 5 and H ‚àºt(5).
By choosing œµ = 0.0001, and following the stopping rule in (3.1.4),
we get n = 30. The x-axis represents the set of i.i.d. atoms gener-
ated from Œ∏i ‚àºH in increasing order and the y-axes represents the
corresponding Gamma process. We display in the same plot, the
weights in (3.1.5) as vertical lines at the corresponding atoms t = Œ∏i
for i = 1, . . . , n.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.2 One sample path of the Gamma process using Zarepour & Al Labadi
(2012) approximation. For comparison purposes we use the same
parameter used in Figure 3.1, particularly we choose a = 5 and
H ‚àºt(5).
Choosing œµ = 0.0001 and using the stopping rule in
(3.1.8), we get n = 14. The plot shows as well the weights calculated
in (3.1.7) as vertical lines.
. . . . . . . . . . . . . . . . . . . . . . .
25
ix

LIST OF FIGURES
x
3.3 Ten sample paths of the Gamma process approximation with a = 5,
H = t(5) and œµ = 0.0001.
The plot at the top of the Ô¨Ågure
uses the Bondesson (1982) Gamma approximation and the plot at
the bottom uses the Zarepour and Al Labadi (2012) Gamma pro-
cess approximation. The truncation values for Bondeson (1982) are
n = 45, 25, 42, 40, 39, 41, 7, 52, 39 and 27, one value for each path.
Whereas the truncation values for Zarepour and Al Labadi (2012)
are found to be n = 14, 10, 10, 10, 11, 12, 8, 11, 10 and 9, for the same
tolerance value œµ = 0.0001.
. . . . . . . . . . . . . . . . . . . . . . .
27
3.4 One sample path of the Dirichlet process approximated using Bon-
desson (1982) approximation. We choose a = 5 and H ‚àºt(5). For
œµ = 0.0001, we get n = 25. Vertical lines at diÔ¨Äerent location Œ∏(i)
represent the weights in calculated (3.2.4).
. . . . . . . . . . . . . .
34
3.5 Ten sample paths of the Dirichlet process using Bondesson (1982)
approximation with a = 5, H ‚àºt(5).
For œµ = 0.0001, we get
n = 26, 31, 23, 26, 27, 36, 25, 28, 27 and 24. . . . . . . . . . . . . . . .
35
3.6 One sample path of Sethuraman (1994) Dirichlet process approxima-
tion, P Seth((‚àí‚àû, t]) ‚àºDP(a, H). We choose a = 5 and H ‚àºt(5).
For œµ = 0.0001 and using the stopping rule in (3.2.8), we get n = 45.
37
3.7 Ten sample paths of the Dirichlet process approximated by Sethura-
man (1994) stick breaking approach. We choose a = 5 and H ‚àºt(5).
For œµ = 0.0001 and using the stopping rule in (3.2.8), the values of
n for each path is n = 30, 38, 57, 44, 54, 52, 21, 31, 43 and 54. . . . . .
38
3.8 One sample path of the Dirichlet process approximated by Zarepour
& Al Labadi (2012) approximation of the Dirichlet process with
a = 5, H ‚àºt(5). For œµ = 0.0001, we get n = 12 . . . . . . . . . . . .
41

LIST OF FIGURES
xi
3.9 Ten paths of Zarepour & Al Labadi (2012) approximation of Dirich-
let process with a = 5, H ‚àºt(5). For œµ = 0.0001 the value of n
is equal to n = 15, 11, 13, 10, 7, 10, 8, 6, 7 and 7, one for each sample
path. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.10 Solid step functions show the Dirichlet process prior using the Zare-
pour & Al Labadi (2012)‚Äôs approximation with a = 5, H ‚àºt(5) and
n = 1000. The solid line is the actual cumulative distribution of
the data set which is in our case Normal(‚àí2, 1). Top to bottom
plot shows the posterior distribution of the Dirichlet process after
observing m = 5, 20 and 200 data points respectively.
. . . . . . . .
44
4.1 One sample path of the two parameter Poisson-Dirichlet process
with H ‚àºt(5), Œ± = 0.5 and a = 1. the x-axis shows atoms gen-
erated from Œ∏i ‚àºH in increasing order and the y-axes represent
the resulting two parameter Poisson-Dirichlet process. Vertical lines
represent the intensity of the weights calculated from (4.1.2). . . . .
50
4.2 The plot at the top shows one sample path of the NIGP(1, t(5))
using the stick breaking approach with n = 100. The choice of n in
this plot is chosen to be relatively large. The vertical lines show the
weights in (4.2.4). The plot at the bottom depicts one sample path
of NIGP(1, t(5)). Choosing œµ = 0.0001, we get n = 3 based on the
stopping rule in (4.2.5). . . . . . . . . . . . . . . . . . . . . . . . . .
55
4.3 One sample path of the normalized inverse Gaussian process with
H ‚àºt(5), a = 1 and n = 50. The NIGP(t(5), 1) is sampled using
Al Labadi & Zarepour (2014) approximation
. . . . . . . . . . . . .
57

LIST OF FIGURES
xii
5.1 One sample path of the Beta process B ‚àºBP(0.8,Uniform(0, 1)).
The Beta process is approximated using Al Labadi & Zarepour
(2014) algorithm ( Algorithm B) with n = 15. The plot shows as
well the weights in (5.1.4) by vertical lines at the associated atoms Œ∏i.
63
5.2 The plot at the top shows ten sample paths of the Beta process
with c = 1 and B0 ‚àºUniform(0, 1). The plot at the bottom shows
ten sample paths of the Beta process with same base measure B0
but with c = 20. We use Algorithm B to approximate the Beta
process in both plots with n = 100. The dashed line connected by
dots in both plots represents the cumulative distribution of the base
measure B0 ‚àºUniform(0, 1). . . . . . . . . . . . . . . . . . . . . . .
65
5.3 The plot at the top depicts one sample path of the Beta process with
c = 1, B0 ‚àºUniform(0, 1). The Beta process is approximated using
Algorithm B with n = 15. The vertical lines shows the intensity of
the weights in (5.1.4). The plot at the bottom shows 10 draws of
the Bernoulli processes, one per line, with base measure the Beta
process (displayed at the top of the Ô¨Ågure). . . . . . . . . . . . . . .
69
5.4 The plot at the top shows one draw of the Beta process B ‚àº
BP(1, Uniform(0, 1)) approximated by Algorithm B with n = 15.
The plot at the bottom shows 10 draws of the Beta-Bernoulli pro-
cess with base measure B. Draws are represented in the plot by
dots at each pair of the form (bk = 1, Œ∏i) generated from the Beta-
Bernoulli process. The plot at the bottom shows as well one updated
draw of the Beta-Bernoulli process given the 10 other observations.
Triangle pointed down represent the update contributed by the dis-
crete base, and triangle pointed up represent the update contributed
by the continuous part of the updated Beta process. . . . . . . . . .
83

List of Tables
5.1 The table shows the Ô¨Årst nine set of pairs (pk, Œ∏k)1‚â§k‚â§9 extracted
from a draw of the Beta process, B ‚àºBP(1, Uniform(0, 1)). The
Beta process is approximated using Algorithm B with n = 15. . . . .
71
5.2 The table depicts the Ô¨Årst nine values (bk, Œ∏k)1‚â§k‚â§9 extracted from a
draw of a Bernoulli process with base measure B ‚àºBP(1, Uniform(0, 1)).
Recall that bk ‚àºBinomial(pk), where pk is the probability displayed
in Table 5.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
5.3 The table shows some preliminary values of the pairs (pCont
k
, Œ∏
‚Ä≤
k) ex-
tracted from BCont ‚àºBP(11, 1
11Uniform(0, 1)). . . . . . . . . . . . .
79
5.4 The table depicts some preliminary values of the pairs (bCont
k
, Œ∏
‚Ä≤
k)
extracted from S ‚àºBeP(BCont), the Beta-Bernoulli process with
base measure BCont. . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5.5 The table shows the set of pairs (pDisc
k
, Œ∏k)1‚â§k‚â§3 such that pDisc
k
is
calculated based on (5.3.6). . . . . . . . . . . . . . . . . . . . . . . .
81
5.6 The table shows the set of pairs (bDisc
k
, Œ∏k)1‚â§k‚â§3, where bDisc
k
‚àº
Bernoulli(pDisc
k
). . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
xiii

Chapter 1
Introduction
In most data analysis that involves statistical inference, we often observe some set of
data where we wish to Ô¨Åt a statistical model to be able to infer about its characteristic.
Such characteristic can be as simple as estimating the mean of the data or as complex
as estimating its entire distribution. Regardless of the complexity of the information
we want to extract from the data, we need to construct a statistical model that Ô¨Åts
the data. This requires estimating a set of parameters that govern the underlying
physical setting of the measured data. There exist two main and distinct approaches
to tackle this problem, namely Frequentist and Bayesian statistics.
The Frequentist approach to statistics considers probability as a limiting long run
frequency. In particular, data are a repeatable random sample where we believe that
the underlying parameters remain constant (or Ô¨Åxed) during this repeatable process.
On the other hand, the Bayesian approach to statistics considers the parameters œÜ as
being random, hence they are assigned a prior distribution p(œÜ). The observed data
X is then used to update our prior belief for each unknown parameter via the Bayes
rule p(œÜ|X) ‚àùp(X|œÜ)p(œÜ), where p(œÜ|X) is known as the posterior distribution. A
parametric Bayesian inference is used when the set of parameters governing the data
is Ô¨Ånite. However, this could be restrictive as a model when we observe more and
1

1. Introduction
2
more data. We want to have a model that grows in complexity when we observe more
data. One way to overcome this problem is to use a non-parametric approach. In the
Bayesian framework, this approach allows us to put a prior on an inÔ¨Ånite dimensional
parameter space. The choice of a prior distribution has been carefully and widely
discussed in Bayesian inference. The main reason is that we need to construct a prior
which will lead to predictive models such that we know how to sample from the prior
and posterior distribution. The most practical and useful priors are conjugate priors.
We say that the prior is a conjugate prior for the likelihood if the posterior has the
same distributional form as the prior distribution. The conjugacy property is very
useful from a computation point of view because it will be straight forward to sample
from the posterior.
In this thesis, we highlight many diverse groups of nonparametric Bayesian priors
and explore algorithms to sample from these them. The outline of this thesis is as
follows: In Chapter 2 we review some preliminary theory for a L¬¥evy random variable
and its characteristic function. We introduce an example of L¬¥evy random variables
along with their L¬¥evy measures. In Chapter 3 we introduce two well known priors; the
Gamma process and the Dirichlet process. The former has been recently applied to
exchangeable models of sparse graphs in Caron & Fox (2014) and for non-parametric
ranking models in Caron et al. (2013). It also has been used as a prior for inÔ¨Ånite-
dimensional latent indicator matrices in Titsias (2008).
The latter application is
one of the earliest Bayesian non-parametric approach to infer on latent (or hidden)
feature models, in particular when each feature occurs multiple times for a data point,
in contrast of being simply binary. The Dirichlet process is a prior on an inÔ¨Ånite-
dimensional space. It has commonly been used as prior on latent class models, in
particular in clustering and mixture models. For more details refer to Teh and Jordan
(2013) and Teh (2010). In Chapter 4, we describe in detail the two-parameter Poisson
Dirichlet process and the normalized inverse Gaussian process. The former is also
known as the Pitman-Yor process and is a generalization of the Dirichlet process.

1. Introduction
3
Unlike the Dirichlet and Beta process, the two-parameter Dirichlet process does not
have the conjugacy property.
We present the main contribution of this thesis in
Chapter 5. We Ô¨Årst introduce a diÔ¨Äerent approximation of the Beta process along with
algorithms to sample from it. We then introduce an extension to the Beta process
known as the Beta-Bernoulli process.
We describe two methodologies to sample
from the Beta-Bernoulli process where one of them has been known in the Computer
Science community by the Indian buÔ¨Äet process (IBP) (Ghahramani & GriÔ¨Éths, 2005)
and the other method is our contribution in this thesis. We describe a new sampling
technique which focuses on the accuracy and eÔ¨Éciency of the approximated Beta-
Bernoulli process. Finally, we present the sampling technique of the Beta-Bernoulli
process on a simulated example.

Chapter 2
L¬¥evy random variables and
processes
2.1
L¬¥evy process
In this chapter, we present some preliminary discussion on L¬¥evy processes and their
applications in nonparametric Bayesian inference.
DeÔ¨Ånition 2.1.1 (L¬¥evy process) A stochastic process X = {Xt : t ‚â•0} deÔ¨Åned
on a probability space (‚Ñ¶, F, P) is called a L¬¥evy process if the following properties
hold:
1. The paths of X are P-right continuous with left limits;
2. Xt starts at 0, i.e., P(X0 = 0) = 1 a.s.
3. Xt has independent increments, i.e., the random variables Xt0, Xt1 ‚àíXt0, . . .,
Xtn ‚àíXtn‚àí1 are independent for all 0 ‚â§t0 < t1 < ¬∑ ¬∑ ¬∑ < tn, for n ‚â•1;
4. Xt has stationary increments, i.e., Xt+s ‚àíXt
d= Xs for all s, t ‚â•0; and
4

2. L¬¥evy random variables and processes
5
5. Xt is stochastically continuous, i.e., for all s ‚â•0, Xt
p‚ÜíXs as t ‚Üís, or
equivalently
lim
t‚Üís Pr{|Xt ‚àíXs| > œµ} = 0
.
Throughout this thesis, ‚Äù
d= ‚Äù and ‚Äù
p‚Üí‚Äù denote equality in distribution and conver-
gence in probability, respectively. Note that the Ô¨Åfth property does not imply that
the sample paths are continuous.
DeÔ¨Ånition 2.1.2 (Subordinators) Let X = {Xt : t ‚â•0} be a L¬¥evy process deÔ¨Åned
on a probability space (‚Ñ¶, F, P). Then X is called a subordinator if the following
properties hold:
1. Xt is a L¬¥evy process deÔ¨Åned on R+;
2. Xt is a.s. non-negative; and
3. Xt is a.s. non-decreasing.
2.2
Characteristic function
2.2.1
InÔ¨Ånitely divisibility
DeÔ¨Ånition 2.2.1 (InÔ¨Ånitely Divisible) A real-valued random variable Œò has an
inÔ¨Ånitely divisible distribution if for each n = 1, 2, . . . there exist a sequence of i.i.d.
random variable Œò1,n, . . . , Œòn,n such that
Œò
d= Œò1,n + ¬∑ ¬∑ ¬∑ + Œòn,n.
Based on this deÔ¨Ånition, one way to determine whether a given random variable
has an inÔ¨Ånitely divisible distribution is by checking its characteristic exponent. As-
suming Œò has the characteristic exponent œà(u) := ‚àílog E(eiuŒò) for all u ‚ààR, then

2. L¬¥evy random variables and processes
6
Œò is an inÔ¨Ånitely divisible distribution if, for all n ‚â•1, there exist a characteristic
exponent of a probability distribution œàn such that œà(u) = nœàn(u) for all u ‚ààR.
From the deÔ¨Ånition of a L¬¥evy process, we conclude that for any t > 0, Xt is
a random variable belonging to the class of inÔ¨Ånitely divisible distributions. This
follows from the fact that Xt has stationary independent increments and therefore for
any n = 1, 2, . . .,
Xt
d= Xt/n + (X2t/n ‚àíXt/n) + ¬∑ ¬∑ ¬∑ + (Xt ‚àíX(n‚àí1)t/n).
Suppose now that for all u ‚â•0 and t ‚â•0, we deÔ¨Åne
œàt(u) = ‚àílog E(eiuXt).
(2.2.1)
Then, using (2.2.1), for any m, n ‚â•0, we can easily get,
nœà1(u) = œàn(u) = mœàn/m(u).
Hence, for any t > 0 rational,
œàt(u) = tœà1(u),
(2.2.2)
where œà1 is the characteristic exponent of X1. If t is a irrational, choosing a decreasing
sequence of rational {tn : n ‚â•1} such that tn ‚Üìt as n ‚Üí‚àûalong with the a.s. right
continuity of Xt implies right continuity of exp{‚àíœàt(u)}. In which case, (2.2.2) holds
for all t ‚â•0.
2.2.2
L¬¥evy-Khintchine deÔ¨Ånition
Theorem 1 (L¬¥evy-Khintchine Theorem) [Fristedt and Gray, 1996] There is a
one-to-one correspondence between all inÔ¨Ånitely divisible distributions (and therefore
L¬¥evy processes) Xt and the set of triples (a, œÉ, ŒΩ) where a ‚ààR, œÉ ‚ààR+, and ŒΩ is a
measure concentrated on R\{0} satisfying
R
(1 ‚àßx2)ŒΩ(dx) < ‚àû, such that for all
E[eiuXt] = exp(‚àíœà(u, t)),

2. L¬¥evy random variables and processes
7
where
œà(u, t) = ‚àíiaut + tœÉ2u2
2
+ t
Z
R\{0}
(1 ‚àíeiux + itu1(|x|<1))ŒΩ(dx).
(2.2.3)
In (2.2.3) ŒΩ is called the L¬¥evy measure.
For subordinator, there is a one-to-one
correspondence between all inÔ¨Ånitely divisible functions on R+ and pairs (b, ŒΩ) such
that for all t, the Laplace transform of Xt is
E[e‚àíuXt] = exp[‚àíbut ‚àít
Z ‚àû
0
(1 ‚àíe‚àíux)ŒΩ(dx)],
where b ‚ààR+ and ŒΩ is a measure on R+ satisfying
R
(x ‚àß1)ŒΩ(dx) < ‚àû.
Here and throughout this thesis, we deÔ¨Åne (x ‚àß1) := min(x, 1).
2.2.3
L¬¥evy-ItÀÜo deÔ¨Ånition
Theorem 2 (L¬¥evy-ItÀÜo decomposition) [Fristedt and Gray, 1996] Let Xt be a L¬¥evy
process on R with triple (a, œÉ, ŒΩ) as described in DeÔ¨Ånition 1. Let (Y, W) be an inde-
pendent pair where W is standard Brownian motion and Y is a Poisson point process
in (0, ‚àû)√ó(R\{0}) whose intensity measure is Œª√óŒΩ where Œª is the Lebesgue measure.
Then, there exists a sequence of œµk ‚Üì0 such that
Xt
d= at + œÉWt + lim
k‚Üí‚àû
 Z
(‚àí‚àû,‚àíœµk)]‚à™[œµk,‚àû)
yY ((0, t] √ó dy) ‚àí
Z
(‚àí‚àû,‚àíœµk]‚à™[œµk,‚àû)
yŒΩ(dy)

.
When Xt is a subordinator, then ŒΩ is a measure satisfying
R ‚àû
0 (x ‚àß1)ŒΩ(dx) < ‚àû, and
the L¬¥evy-ItÀÜo decomposition simpliÔ¨Åes to
Xt
d= bt +
Z
yY ((0, t] √ó dy),
(2.2.4)
where the pair (b, ŒΩ) is described in DeÔ¨Ånition 1 and Y is a Poisson process in (0, ‚àû)√ó
(0, ‚àû) whose intensity measure is Œª √ó ŒΩ.
We reinterpret Xt in (2.2.4) to be X(0, t], the measure assigned to the interval
(0, t]. Since Xt can be constructed from a non-negative Poisson process, X is a random
measure. In general, we deÔ¨Åne X(A) =
R
A dXt for any Borel set A.

2. L¬¥evy random variables and processes
8
Also, using the convention of the nonparametric Bayesian community, we redeÔ¨Åne
ŒΩ to be what was previously the product measure Œª √ó ŒΩ. By doing so, we can allow
Œª to be diÔ¨Äerent from the Lebesgue measure on some space ‚Ñ¶. If Œª is not a multiple
of Lebesgue measure, the resulting process will not be a L¬¥evy process since it will
not have stationary increments. Instead, it will deÔ¨Åned to be a completely random
measure.
DeÔ¨Ånition 2.2.2 (Completely random measure) [Kingman, 1967] A random mea-
sure Œò is a completely random measure if, for any Ô¨Ånite collection A1, . . . , An of
disjoint sets, the random variables Œò(A1), . . . , Œò(An) are independent.
DeÔ¨Ånition 2.2.3 (Random measure) [Resnick, 1987] Let E be a Polish space and
B(E) be the Borel œÉ-algebra generated by the open sets in E. A measure ¬µ is called
Radon if ¬µ(K) < ‚àûfor any compact set K in E. Let M+(E) be the space of Radon
measures in E. Let M+(E) be the smallest œÉ-algebra of subsets of M+(E) making
the maps ¬µ ‚Üí¬µ(f) =
R
f(x)d¬µ(x) from M+(E) to R measurable for all functions
f ‚ààC+
K(E), where C+
K(E) denotes the set of continuous functions f : E ‚Üí[0, ‚àû) with
compact support. Note that, M+(E) is the Borel œÉ-algebra generated by the topology
of vague convergence. A random measure on E is any measurable map Œæ deÔ¨Åned on
a probability space (‚Ñ¶, F, P) with values in (M+(E), M+(E)).
DeÔ¨Ånition 2.2.4 (Point process) Let E be a locally compact space with a countable
basis. Let E be a Borel œÉ-algebra of subsets of E. Let (Œ∏i)i‚â•1 be a countable collection
of not necessary distinct points of E. A point measure on E is a measure m of the
following form:
m =
‚àû
X
i=1
Œ¥Œ∏i,
where, Œ¥Œ∏i denotes the Dirac measure at Œ∏i, i.e., Œ¥Œ∏i(A) = 1 if Œ∏i ‚ààA and 0 otherwise
for a set A ‚ààE . If K ‚ààE is compact then m(K) < ‚àû(i.e., m is Radon meaning
the measure of compact sets is always Ô¨Ånite). Take Mp(E) as the space of all point

2. L¬¥evy random variables and processes
9
measures deÔ¨Åned on E and Mp(E) be the smallest œÉ-algebra containing all sets of
the form {m ‚ààMp(E) : m(A) ‚ààB} for A ‚ààE , B ‚ààB([0, ‚àû)). Alternatively,
Mp(E) is the smallest œÉ-algebra making all evaluation maps m ‚Üím(A) measurable
for all A ‚ààE . A point process Œæ on E is a measurable map from the probability
space (‚Ñ¶, F, P) ‚Üí(Mp(E), Mp(E)). Therefore, a point process is a random element
of Mp(E). The probability law of the point process Œæ is the measure P ‚ó¶Œæ‚àí1 = P[Œæ ‚àà¬∑]
on Mp(E).
The Laplace functional is a useful tool to determine the distribution of point processes.
It is important to notice that the Laplace functional of a random measure uniquely
determines the distribution of any random measure.
DeÔ¨Ånition 2.2.5 (Laplace Functional of Point Process) Let Q be a probability
measure on (Mp(E), Mp(E)) (where Mp(E) and Mp(E) are as constructed in DeÔ¨Åni-
tion 2.2.3). The Laplace transform of Q is the map œà which takes non-negative Borel
functions on E into [0, ‚àû) deÔ¨Åned by
œà(f) =
Z
Mp(E)

exp

‚àí
Z
E
f(x)m(dx)

Q(dm).
If Œæ : (‚Ñ¶, F) ‚Üí(Mp(E), Mp(E)) is a point process, the Laplace functional of Œæ is the
Laplace transform of the law of Œæ(f):
œàŒæ(f) = E(exp{‚àíŒæ(f)}) =
Z
‚Ñ¶
exp{‚àíŒæ(œâ, f)}P(dœâ)
=
Z
Mp(E)

exp

‚àí
Z
E
f(x)m(dx)

PŒæ(dm).
DeÔ¨Ånition 2.2.6 (Poisson random measure) Let ¬µ be a Radon measure on E , a
point process Œæ is called a Poisson point process or a Poisson random measure with
mean measure ¬µ, denoted by PRM(¬µ), if Œæ satisÔ¨Åes:
1. For A ‚ààE
P[Œæ(A) = k] =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
e‚àí¬µ(A)(¬µ(A))k
k!
,
if ¬µ(A) < ‚àû
0
if ¬µ(A) = ‚àû.

2. L¬¥evy random variables and processes
10
2. For any k ‚â•1, if A1, . . . , Ak are mutually disjoint sets in E , then Œæ(A1), . . . , Œæ(Ak)
are independent random variables.
Therefore, Œæ is a Poisson random measure if the random number of points in a set A
has a Poisson distribution with parameter ¬µ(A) and the number of points in disjoint
sets are independent random variables.
Proposition 1 Let Œæ a PRM(¬µ), the Laplace functional of PRM(¬µ) uniquely deter-
mines the law of Œæ. It is given for any measurable positive function, f ‚â•0, by
œàŒæ(f) = exp

‚àí
Z
E
(1 ‚àíe‚àíf(x))¬µ(dx)

.
For proof, see proposition 3.6 of Resnick (1987).
Proposition 2 Let Œæ1 and Œæ2 be two independent Poisson random measures on (E, B(E))
with mean measure ¬µ1 and ¬µ2 respectively. Then, the random measure Œæ = Œæ1 + Œæ2 is
also a Poisson random measure with mean measure ¬µ = ¬µ1 + ¬µ2.
Proof:
E(e‚àíŒæ(f)) = E(e‚àíŒæ1(f)‚àíŒæ2(f))
= E(e‚àíŒæ1(f))E(e‚àíŒæ2(f))
= exp

‚àí
Z
E
(1 ‚àíe‚àíf(x))¬µ1(dx)

exp

‚àí
Z
E
(1 ‚àíe‚àíf(x))¬µ2(dx)

= exp

‚àí
Z
E
(1 ‚àíe‚àíf(x))(¬µ1 + ¬µ2)(dx)

.
Throughout this thesis, we deÔ¨Åne
Œìi = E1 + ¬∑ ¬∑ ¬∑ + Ei,
(2.2.5)

2. L¬¥evy random variables and processes
11
where (Ek)k‚â•1 is a sequence of independent and identically distributed random vari-
ables with an exponential distribution of mean 1.
Theorem 3 Let Œæ ‚àºPRM(Œª) where Œª is the Lebesgue measure on R. Then Œæ can
be written as follows
Œæ =
‚àû
X
i=1
Œ¥Œìi.
Proof:
Using the recursive technique in Banjevic & Zarepour (2002), we have for
any non-negative function f
Œæ(f) =
‚àû
X
i=1
f(Œìi + t)
M(u, t) = E[e‚àíuŒæ] = E(e‚àíu P‚àû
i=1 f(Œìi+t))
= E(E(e‚àíu P‚àû
i=1 f(Œìi+t)|Œì1 = s))
=
Z ‚àû
t
e‚àíuf(s+t)E(e‚àíu P‚àû
i=1 f(Œìi+s+t))e‚àísds
=
Z ‚àû
t
e‚àíuf(s+t)M(u, t + s)e‚àísds.
Now using the change of variable s + t = v and multiplying both sides by e‚àít, we get
M(u, t) =
Z ‚àû
t
e‚àíuf(v)M(u, v)e‚àí(v‚àít)dv
e‚àítM(u, t) =
Z ‚àû
t
e‚àíuf(v)M(u, v)e‚àívdv.
DiÔ¨Äerentiating both sides with respect to t, we get
‚àíe‚àítM(u, t) + DM(u, t)e‚àít = ‚àíe‚àíuf(t)M(u, t)e‚àít
DM(u, t) = (1 ‚àíe‚àíuf(t))M(u, t)
DM(u, t)
M(u, t) = (1 ‚àíe‚àíuf(t))
M(u, t) = exp

‚àí
Z ‚àû
t
(1 ‚àíe‚àíuf(s))ds

.

2. L¬¥evy random variables and processes
12
Now take t = 0 to get
M(u, 0) = exp

‚àí
Z ‚àû
0
(1 ‚àíe‚àíuf(s))ds

.
2.2.4
Transformation of Poisson Random measure
The next proposition shows that mapping points of a Poisson point process yields
new Poisson point process with a certain representation for its mean measure.
Proposition 2.2.7 (Proposition 3.7 of Resnick, 1987) Let Ei, i = 1, 2 be two
locally compact spaces with countable bases. Let Ei, i = 1, 2 be the associated œÉ-Ô¨Åelds.
Let T : (E1, E1) ‚Üí(E2, E2) be measurable. If Œæ is a PRM(¬µ) on E1, then ÀúŒæ = Œæ ‚ó¶T ‚àí1
is a PRM (Àú¬µ) on E2 such that Àú¬µ = ¬µ ‚ó¶T ‚àí1. If Œæ has the representation
Œæ =
‚àû
X
i=1
Œ¥Xi
then
ÀúŒæ = Œæ ‚ó¶T ‚àí1 =
‚àû
X
i=1
Œ¥T(Xi).
The next proposition shows that starting from PRM, we may construct a new
PRM whose points live in a higher dimensional space.
Proposition 2.2.8 (Proposition 3.8 of Resnick, 1987) Let Ei, i = 1, 2 be two
locally compact spaces with countable bases. Suppose
‚àû
X
i=1
Œ¥Xi
is a PRM(¬µ) on E1. Suppose (Œ∏i)i‚â•1 are i.i.d. random elements on E2 with common
probability distribution F, and suppose the Poisson process and (Œ∏i)i‚â•1 are deÔ¨Åned on

2. L¬¥evy random variables and processes
13
the same probability space and are independent. Then the point process on E1 √ó E2,
‚àû
X
i=1
Œ¥(Xi,Œ∏i),
is a PRM with mean measure ¬µ √ó F.
Let Œæ = P‚àû
i=1 Œ¥(Œìi,Œ∏i) such that Œ∏i
i.i.d.
‚àºH and Œìi is independent of Œ∏i then for A and B
in ‚Ñ¶, we have
E[e‚àíuŒæ(A√óB)] = exp

‚àí
Z
B
Z
A
(1 ‚àíe‚àíux)ŒΩ(dx)dH(Œ∏)

.
Theorem 4 (Banjevic & Zarepour, 2002) Suppose X is a subordinator with char-
acteristic function
Œ®(x) = exp

‚àí
Z ‚àû
0
(1 ‚àíexp(ixu))dŒΩ(u)

,
‚àí‚àû< x < ‚àû
where ŒΩ is a positive, continuous and non-increasing L¬¥evy measure deÔ¨Åned on (0, ‚àû)
such that ŒΩ(x) =
R ‚àû
x dŒΩ(u) and
Z ‚àû
œµ
ŒΩ‚àí1(u)du < ‚àû,
for each œµ > 0
in which ŒΩ‚àí1(u) = sup{x : ŒΩ(x) ‚â§u}. Then X has the almost sure representation
X =
‚àû
X
i=k
ŒΩ‚àí1(Œìk).
Theorem 5 (Campbell‚Äôs Theorem) [Kingman, 1993] Let Y be a Poisson process
on ‚Ñ¶√ó (0, ‚àû) with mean measure ŒΩ. Then Y has a sum representation
Y =
‚àû
X
i=1
xiŒ¥Œ∏i.
We have that Y is absolutely convergent if and only if
Z
‚Ñ¶
Z ‚àû
0
x.ŒΩ(dŒ∏, dx) < ‚àû.

2. L¬¥evy random variables and processes
14
2.3
L¬¥evy random variables
In this section, we introduce some examples of L¬¥evy random variables and their
characteristic.
2.3.1
Poisson random variable
Let Œæ be a Poisson random variable with probability distribution deÔ¨Åned as follows,
for each Œª > 0
¬µŒª({k}) = e‚àíŒªŒªk/k!
k = 0, 1, 2, . . . .
Using the deÔ¨Ånition of characteristic function, an easy calculation shows that
E[eiuŒæ] =
X
k‚â•0
eiuk¬µŒª({k})
=
X
k‚â•0
[eiu]ke‚àíŒªŒªk
k!
= e‚àíŒª
‚àû
X
k=0
(Œªeiu)k
k!
= e‚àíŒªeŒªeiu
= e‚àíŒª(1‚àíeiu)
=

e‚àíŒª
n(1‚àíeiu)
n
(2.3.1)
In here, (2.3.1) is the characteristic function of the sum of n independent Poisson
processes, each of which with parameter Œª/n. Therefore, the L¬¥evy-Khintchine deÔ¨Å-
nition states that there exist a triple (a, œÉ, ŒΩ) such that the characteristic exponent
œà(u) satisÔ¨Åes the equation in (2.2.3). Indeed, we see that the characteristic exponent
of the Poisson random variable has the same form of (2.2.3) with a = œÉ = 0 and
ŒΩ = ŒªŒ¥1, where Œ¥1 is the Dirac measure at 1. The Poisson random variable can be
written as
Œæ(¬∑) =
‚àû
X
i=1
Œ¥Œìi(¬∑).

2. L¬¥evy random variables and processes
15
Refer to Theorem 3 for the proof.
2.3.2
Gamma random variable
Let X be a Gamma random variable with probability measure deÔ¨Åned as follows, for
Œ±, Œ≤ > 0
¬µŒ±,Œ≤(dx) =
Œ≤Œ±
Œì(Œ±)xŒ±‚àí1e‚àíŒ≤xdx
Using the characteristic function we get,
E[eiuX] =
Z ‚àû
0
eiux¬µŒ±,Œ≤(dx)
=
1
(1 ‚àíiu/Œ≤)Œ±
=
 
1
(1 ‚àíiu/Œ≤)Œ±/n
!n
(2.3.2)
From (2.3.2) we can conclude that a Gamma random variable is inÔ¨Ånitely divisible
and therefore the L¬¥evy-Khintchine deÔ¨Ånition state that there exists a triple (a, œÉ, ŒΩ)
such that œà(u) satisÔ¨Åes (2.2.3). The following helps us Ô¨Ånd the values of (a, œÉ, ŒΩ).
Theorem 6 (Frullani integral) Let a, b > 0 such that a < b. If f ‚Ä≤(x) is continuous
and the integral converges, then
Z ‚àû
0
f(ax) ‚àíf(bx)
x
dx = [f(0) ‚àíf(‚àû)] log
b
a

.
Lemma 2.3.1 For all Œ±, Œ≤ > 0 and z ‚ààC such that the real part of z is in (‚àí‚àû, 0)
we have
1
(1 ‚àíz/Œ≤)Œ± = exp

‚àí
Z ‚àû
0
(1 ‚àíezx)Œ±x‚àí1e‚àíŒ≤xdx

.
Proof:
Using the Frullani integral with f(x) = Œ±e‚àíx, it follows
exp

‚àí
Z ‚àû
0
(1 ‚àíezx)Œ±x‚àí1e‚àíŒ≤xdx

= exp

‚àí
Z ‚àû
0
Œ±e‚àíŒ≤x ‚àíŒ±e‚àí(Œ≤‚àíz)x
x


2. L¬¥evy random variables and processes
16
= exp

‚àí
Z ‚àû
0
f(Œ≤x) ‚àíf((Œ≤ ‚àíz)x)
x

= exp

‚àí

[f(0) ‚àíf(‚àû)] log
Œ≤ ‚àíz
Œ≤
 
= exp

‚àíŒ± log
Œ≤ ‚àíz
Œ≤
 
= exp

log

Œ≤
Œ≤ ‚àíz
Œ±
=

1
1 ‚àíz/Œ≤
Œ±
For simplicity and without loss of generality, in this thesis, we take Œ≤ = 1. The
characteristic exponent of a Gamma random variable X is
œà(u) = ‚àílog E(eiuX)
= ‚àílog

1
(1 ‚àíiu)Œ±

= ‚àílog(e‚àí
R ‚àû
0 (1‚àíeiux)Œ±x‚àí1e‚àíxdx)
= Œ±
Z ‚àû
0
(1 ‚àíeiux)1
xe‚àíxdx
for Œ∏ ‚ààR.
Therefore,
œÉ = 0,
and for ŒΩ concentrated on (0, ‚àû), we have
ŒΩ(dx) = Œ±x‚àí1e‚àíxdx
(2.3.3)
a = ‚àí
Z 1
0
xŒΩ(dx)
(2.3.4)
The choice of a in the L¬¥evy-Khintchine formula is the necessary quantity to cancel
the term coming from
R
iu1(|x|<1)ŒΩ(dx) in (2.2.3). We can show that any Gamma
random variable can be written as
X(¬∑) =
‚àû
X
i=1
ŒΩ‚àí1(Œìi)Œ¥Œ∏i(¬∑).

2. L¬¥evy random variables and processes
17
See Ferguson and Klass (1972) and Banjevic & Zarepour (2002) for detail and proof.
2.3.3
Stable random variable
DeÔ¨Ånition 2.3.2 A random variable X is said to have a stable distribution if for all
n ‚â•2, an > 0 and bn ‚ààR,
X1 + X2 + ¬∑ ¬∑ ¬∑ + Xn
d= anX + bn,
(2.3.5)
where X1, X2, . . . , Xn are independent copies of X.
It can be proven that an = n1/Œ± for 0 < Œ± ‚â§2. The value Œ± is known as the index of
stability. Any stable random variable is inÔ¨Ånitely divisible, this follows by subtracting
bn/n from each of the independent copies X1, X2, . . . , Xn
X1 + X2 + ¬∑ ¬∑ ¬∑ + Xn
d= anX + bn
n
X
k=1
Xk ‚àíbn/n
an

d= X
DeÔ¨Ånition 2.3.3 A stable random variable denoted by SŒ±(c, Œ≤, ¬µ), with an index of
stability Œ± ‚àà(0, 2], c ‚â•0, ‚àí1 ‚â§Œ≤ ‚â§1, ¬µ ‚ààR has a characteristic exponents as
follows:
œà(u) =
Ô£±
Ô£≤
Ô£≥
‚àícŒ±|u|Œ±(1 ‚àíiŒ≤(sign u) tan œÄŒ±
2 ) + i¬µu
if Œ± Ã∏= 1
c|u|(1 + iŒ≤ 2
œÄ(sign u) ln |u| + i¬µu
if Œ± = 1
where
sign(u) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
1
if u > 0
0
if u = 0
‚àí1
if u < 0
To make a connection with the L¬¥evy-Khintchine formula, one should have œÉ = 0,
a = ¬µ ‚àí
R
R x1(|x|<1)ŒΩ(dx) in (2.2.3) and
ŒΩ(dx) =
Ô£±
Ô£≤
Ô£≥
P
x1+Œ±dx
if x ‚àà(0, ‚àû)
Q
|x|1+Œ±dx
if x ‚àà(‚àí‚àû, 0)

2. L¬¥evy random variables and processes
18
where for P, Q ‚â•0
c = P + Q,
and
Œ≤ =
Ô£±
Ô£≤
Ô£≥
P‚àíQ
P+Q
if Œ± ‚àà(0, 1) ‚à™(1, 2)
0
if Œ± = 1
(P = Q).
A random variable with symmetric Œ±-stable distribution, denoted by XŒ± can be writ-
ten as a series representation as follows, with 0 < Œ± < 2:
XŒ±(¬∑) =
‚àû
X
i=1
œµiŒì‚àí1/Œ±
i
Œ¥Œ∏i(¬∑),
where {œµi} is an i.i.d. sequence with
p(œµi = 1) = 1/2
p(œµi = ‚àí1) = 1/2.

Chapter 3
Gamma and Dirichlet Process
3.1
Gamma Process
The Gamma process plays a crucial role in nonparametric Bayesian inference.
It
has gained widespread adoption when computational techniques allowed it to be
more practically applicable. It has been used as prior to many applications in dif-
ferent Ô¨Åelds such as exchangeable models of sparse graphs (Caron, Fran¬∏cois and Fox,
Emily B, 2014), nonparametric ranking models (Caron, Fran¬∏cois and Teh, 2013) and
inÔ¨Ånite-dimensional latent indicator matrices (Titsias, Michalis L, 2008). A normal-
ized Gamma process which will be introduced in future sections, called the Dirichlet
process has played a central role in nonparametric Bayesian inference.
3.1.1
DeÔ¨Ånition of Gamma process
DeÔ¨Ånition 3.1.1 (Gamma process) The Gamma process, denoted by G ‚àºGP(a, H),
is a completely random measure on [0, ‚àû] √ó ‚Ñ¶with L¬¥evy measure
œÅ(dx, dŒ∏) = N(dx)dH(Œ∏),
19

3. Gamma and Dirichlet Process
20
where
N(dx) = ae‚àíax
x dx.
Here, a > 0 is called the concentration parameter and H the base measure.
Recall from Chapter 2 that the L¬¥evy measure of the Gamma random variable is
deÔ¨Åned as follows
N(x) = a
Z ‚àû
x
u‚àí1e‚àíudu, for x > 0.
(3.1.1)
Note that N(dx) := dN(x).
The Gamma process is the conjugate prior for the non-negative integer valued-
Poisson random measure.
Suppose we observe m Poisson random measures such
that
P1, . . . , Pm|G ‚àºPRM(G)
G ‚àºGP(a, H).
Following Thibaux (2008) notation, we update our posterior belief of G as follows
G|P1, . . . , Pm ‚àºGP(c‚àó, H‚àó),
where
c‚àó= c + m
H‚àó=
c
c + mH +
m
c + m
Pm
i=1 Pi
m
.
For more details refer to Wolpert and Ickstadt (1998a).
3.1.2
Series representation of the Gamma process
From Ferguson and Klass (1972), the Gamma process G ‚àºGP(a, H), can be written
as a sum representation based on the Gamma L¬¥evy measure as follows:
GFerg(¬∑) =
‚àû
X
i=1
N ‚àí1(Œìi)Œ¥Œ∏i(¬∑),
(3.1.2)

3. Gamma and Dirichlet Process
21
where {Œ∏i}i‚â•1 is a sequence of i.i.d. random variable with common distribution H
independent of {Œìi}i‚â•1. The sequence {Œìi}i‚â•1 is deÔ¨Åned as in chapter 2.
The terms in the Ferguson and Klass (1972) sum representation are relatively
diÔ¨Écult to compute since there is no closed form for the inverse of the L¬¥evy measure.
Moreover, there are inÔ¨Ånite terms in (3.1.2) to calculate.
3.1.3
Approximation of the Gamma process
Bondesson (1982) introduces a sum representation of the Gamma process which avoids
the need to work with L¬¥evy measures. It is shown that
GBond(¬∑) =
‚àû
X
i=1
exp

‚àíŒìi
a

EiŒ¥Œ∏i(¬∑),
(3.1.3)
where, {Ei}i‚â•1 is a sequence of i.i.d. Exponential random variable with mean 1 and
{Œ∏i}i‚â•1 is a sequence of i.i.d. random variable with common distribution H. In here,
{Ei}i‚â•1 and {Œ∏i}i‚â•1 are independent. The Bondesson (1982) sum representation of
the Gamma process can be approximated by truncating the higher order in (3.1.3).
Following the same truncation approach used for the Dirichlet process deÔ¨Åned by
Muliere & Tradella (1998), we let
n = inf{i : exp

‚àíŒìi
a

Ei < œµ},
(3.1.4)
for œµ ‚àà(0, 1). The Bondesson (1982) sum approximation is deÔ¨Åned as
GBond(¬∑) =
n
X
i=1
exp

‚àíŒìi
a

EiŒ¥Œ∏i(¬∑).
(3.1.5)
Note that the weights in (3.1.5) are not monotonically decreasing almost surely.
This phenomena is more obvious by looking at Figure 3.1. Note that the vertical
lines in this Ô¨Ågure represent the weights calculated in (3.1.5) which are clearly not
monotonically decreasing.
We will discuss the eÔ¨Éciency of diÔ¨Äerent algorithm in
further sections.

3. Gamma and Dirichlet Process
22
Figure 3.1 depicts one sample path of the Gamma process GBond((‚àí‚àû, t]) ‚àº
GP(a, H) for t ‚àà‚Ñ¶= R using Bondesson (1982) approximation. We choose a = 5
and H ‚àºt(5), a t-distribution with Ô¨Åve degrees of freedom. By choosing œµ = 0.0001,
we get n = 30. The value of n is calculated based on the stopping rule in (3.1.4). The
plot shows as well the weights in (3.1.5) at locations t = Œ∏(i) as vertical lines. The
more intense the weight is, the higher the vertical line becomes. Note that a jump in
the Gamma process occurs at t = Œ∏(i) every time there is a visible weight at the same
location t = Œ∏(i), for i = 1, . . . , n. Thus, a more intense weight results in a higher
jump in the Gamma process. Note that Œ∏(1) < ¬∑ ¬∑ ¬∑ < Œ∏(n) are the ordered statistics for
Œ∏1, . . . , Œ∏n, such that Œ∏i
i.i.d.
‚àºt(5).
Zarepour and Al Labadi (2012) derive a Ô¨Ånite sum approximation of the Gamma
process Gn, which converges a.s. to the Ferguson and Klass (1972) sum representation.
To see this, let Gn ‚àºGamma(a/n, 1), i.e:
Gn(x) = Pr(Xn > n) =
Z ‚àû
x
1
Œì(a/n)e‚àítta/n‚àí1dt,
(3.1.6)
and
G‚àí1
n (y) = inf{x : Gn(x) ‚â•y},
0 < y < 1.
Using the fact that n/Œì(Œ±/n) = Œ±/Œì(Œ±/n+1) and n/Œì(Œ±/n) ‚ÜíŒ±, we have for x > 0,
nGn(x) =
n
Œì(Œ±/n)
Z ‚àû
x
e‚àíttŒ±/n‚àí1dt ‚ÜíŒ±
Z ‚àû
x
e‚àítt‚àí1dt = N(x).
Note that for every x > 0, nGn(x) is a sequence of monotone functions converging to
a continuous monotone function, therefore
G‚àí1
n (x/n) ‚ÜíN ‚àí1(x).
By taking x = Œìi and from the fact that Œìn+1/n ‚Üí1 almost surely as n ‚Üí‚àû, we
have
G‚àí1
n
 Œìi
Œìn+1

a.s.
‚ÜíN ‚àí1(Œìi).

3. Gamma and Dirichlet Process
23
Figure 3.1: One sample path of the Gamma process, GBond ‚àºGP(a, H)
using Bondeson (1982) approximation.
We choose a = 5 and H ‚àºt(5).
By choosing œµ = 0.0001, and following the stopping rule in (3.1.4), we get
n = 30. The x-axis represents the set of i.i.d. atoms generated from Œ∏i ‚àºH in
increasing order and the y-axes represents the corresponding Gamma process.
We display in the same plot, the weights in (3.1.5) as vertical lines at the
corresponding atoms t = Œ∏i for i = 1, . . . , n.
The Gamma process can be approximated as follows:
GZar&Al Lab
n
(¬∑) =
n
X
i=1
G‚àí1
n
 Œìi
Œìn+1

Œ¥Œ∏i(¬∑)
a.s.
‚ÜíG(¬∑) =
‚àû
X
i=1
N ‚àí1(Œìi)Œ¥Œ∏i(¬∑).
(3.1.7)

3. Gamma and Dirichlet Process
24
The next algorithm describes the set of rules to sample from the Zarepour and Al
Labadi (2012) approximation of the Gamma process.
Algorithm A: An approximation of the Gamma process
1. Choose a relatively large positive integer n.
2. Generate Œ∏i
i.i.d.
‚àºH, for i = 1, . . . , n.
3. Generate n+1 independent exponential distributions with mean 1, Ei
i.i.d.
‚àºExp(1).
Set Œìi = Pi
k=1 Ek, for i = 1, . . . , n + 1.
4. Computing G‚àí1
n (Œìi/Œìn+1) is equivalent as computing the quantile of the gamma
distribution, Gamma(a/n, 1) evaluated at (1 ‚àíŒì1/Œìn+1).
Note that the approximation of Zarepour and Al Labadi (2012) is not based on a
stopping rule. It is more an asymptotic result. Nevertheless, a suggested stopping
rule for n is proposed for comparison purposes as follows:
n = inf

j : G‚àí1
j
 Œìj
Œìj+1

< œµ

.
(3.1.8)
Figure 3.2 shows one sample path of the Gamma process using Zarepour & Al
Labadi (2012) in (3.1.7). We take a = 5 and the base measure H ‚àºt(5). By choosing
œµ = 0.0001 and using the stopping rule deÔ¨Åned in (3.1.8), we get n = 14. Figure 3.2
plots as well the corresponding weights calculated in (3.1.7) which are represented in
vertical lines at the corresponding location t = Œ∏i. Notice that the weights are now
monotonically decreasing almost surely.

3. Gamma and Dirichlet Process
25
Figure 3.2: One sample path of the Gamma process using Zarepour & Al
Labadi (2012) approximation.
For comparison purposes we use the same
parameter used in Figure 3.1, particularly we choose a = 5 and H ‚àºt(5).
Choosing œµ = 0.0001 and using the stopping rule in (3.1.8), we get n = 14.
The plot shows as well the weights calculated in (3.1.7) as vertical lines.

3. Gamma and Dirichlet Process
26
It is worth mentioning that the approximation of Zarepour & Al Labadi (2012)
for the Gamma process is very eÔ¨Écient from a computation point of view. Moreover,
the algorithm will be as eÔ¨Écient as Bondesson (1982) with much smaller (almost half)
values of n. Nevertheless, the weights are monotonically decreasing which ensure that
there will not be an intense weight after the last negligible weights calculated (or ob-
served). This phenomena is not guaranteed in the Bondesson (1982) Gamma process
approximation.
The plot at the top of Figure 3.3 shows ten diÔ¨Äerent paths of the Gamma process
approximated using the Bondesson (1982) sum approximation with concentration
parameter a = 5 and base measure H ‚àºt(5).
For each path, the values of the
truncation n is equal to 45, 25, 42, 40, 39, 41, 7, 52, 39 and 27. We use the truncation
rule in (3.1.4) with œµ = 0.0001. The plot at the bottom of Figure 3.3 shows ten
diÔ¨Äerent paths of the Gamma process with the same parameters, a and H, but this
time based on the Zarepour and Al Labadi (2012) Gamma process approximation.
For each path, the values n = 14, 10, 10, 10, 11, 12, 8, 11, 10 and 9 are calculated based
on the stopping rule in (3.1.8) with the same tolerance value œµ. As discussed earlier,
the weights of the Zarepour and Al Labadi (2012) approximation of the Gamma
process are decreasing almost surely, thus the plot at the bottom shows decreasing
jumps (more intense jumps at the beginning and gradually decreases toward the end).

3. Gamma and Dirichlet Process
27
Figure 3.3: Ten sample paths of the Gamma process approximation with
a = 5, H = t(5) and œµ = 0.0001. The plot at the top of the Ô¨Ågure uses the
Bondesson (1982) Gamma approximation and the plot at the bottom uses
the Zarepour and Al Labadi (2012) Gamma process approximation.
The
truncation values for Bondeson (1982) are n = 45, 25, 42, 40, 39, 41, 7, 52, 39
and 27, one value for each path. Whereas the truncation values for Zarepour
and Al Labadi (2012) are found to be n = 14, 10, 10, 10, 11, 12, 8, 11, 10 and
9, for the same tolerance value œµ = 0.0001.

3. Gamma and Dirichlet Process
28
3.2
Dirichlet process
It is well known that the beta distribution, denoted by Beta(a, b), is used as a conju-
gate prior for a binomial model. More concretely, let
f(X|p) ‚àºBinomial(n, p)
g(p) ‚àºBeta(a, b).
By using Bayes‚Äô theorem, f(p|X) ‚àùf(X|p)g(p), the posterior distribution of p be-
comes
f(p|X = x) ‚àºBeta(a + x, n + a ‚àíx).
3.2.1
DeÔ¨Ånition of the Dirichlet distribution
DeÔ¨Ånition 3.2.1 (Dirichlet Distribution) Let P = (p1, p2, . . . , pk‚àí1) be a random
vector, such that pi ‚â•0 for i = 1, 2, . . . , k and p1 + ¬∑ ¬∑ ¬∑ + pk = 1.
In addition,
suppose that a = (a1, . . . , ak), with Œ±i > 0 for i = 1, 2, . . . , k, and let a0 = Pk
i=1 ai.
Then, P is said to have a Dirichlet distribution with parameter a, denoted by P ‚àº
Dir(a1, . . . , ak), if its density function is given by
f(p1, . . . , pk‚àí1|a1, . . . , ak) =
Œì(a0)
Qk
i=1 Œì(ai)
k‚àí1
Y
i
pai‚àí1
i

1 ‚àí
k‚àí1
X
i=1
pi
ak‚àí1
,
over the simplex
S = {(p1, p2, . . . , pk‚àí1) : pi ‚â•0,
k‚àí1
X
i=1
pi ‚â§1},
where Œì(x) denotes the Gamma function.
When k = 2, the Dirichlet distribution reduces to the beta distribution, which has
the density function
f(p; a, b) = Œì(a + b)
Œì(a)Œì(b)pa‚àí1(1 ‚àíp)b‚àí1
p ‚àà(0, 1)
a, b > 0.

3. Gamma and Dirichlet Process
29
Similar to the beta distribution, the Dirichlet distribution Dir(a1, . . . , ak) is used
as a conjugate prior for the Multinomial distribution given by
f(x1, . . . , xk‚àí1|p1, . . . , pk‚àí1) =
n!
x1! . . . xk‚àí1!(n ‚àíPk‚àí1
i=1 xi)!
√ó px1‚àí1
1
. . . pxk‚àí1‚àí1
k‚àí1

1 ‚àí
k‚àí1
X
i=1
pi
n‚àíPk‚àí1
i=1 xi
.
In this case, the posterior distribution of P = (p1, . . . , pk‚àí1) is
f(p1, . . . , pk‚àí1|X1 = x1, . . . , Xk‚àí1 = xk‚àí1) ‚àºDir(a1 + x1, . . . , ak + xk),
where xk = n ‚àíPk‚àí1
i=1 xi.
The following is a general review of properties of the Dirichlet distribution.
1. If {Gi}1‚â§i‚â§k are independent random variables such that Gi ‚àºGamma(ai, 1), i =
1, 2, ¬∑ ¬∑ ¬∑ , k, then
 
G1
Pk
i=1 Gi
,
G2
Pk
i=1 Gi
, . . . ,
Gk‚àí1
Pk
i=1 Gi
!
‚àºDir(a1, . . . , ak).
(3.2.1)
2. If (p1, . . . , pk‚àí1) ‚àºDir(a1, . . . , ak) and r1, . . . , rl are integers such that
0 < r1 < ¬∑ ¬∑ ¬∑ < rl = k ‚àí1, then
 p(1,r1), p(r1+1,r2), . . . , p(rl‚àí1+1,rl)

‚àºDir
 a(1,r1), a(r1+1,r2), . . . , a(rl‚àí1+1,rl), ak

,
where, for i, j = 1, ¬∑ ¬∑ ¬∑ , k
p(i,j) = pi + pi+1 + ¬∑ ¬∑ ¬∑ + pj
a(i,j) = ai + ai+1 + ¬∑ ¬∑ ¬∑ + aj.
3. If (p1, . . . , pk‚àí1) ‚àºDir(a1, . . . , ak) and a0 = Pk
i=1 ai, then
E[(p1, . . . , pk‚àí1)] =
a1
a0
, . . . , ak
a0

Cov(pi, pj) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
‚àíaiaj
a2
0(a0+1)
for i Ã∏= j
ai(a0‚àíai)
a2
0(a0+1) .
for i = j

3. Gamma and Dirichlet Process
30
4. If the prior distribution of (p1, . . . , pk‚àí1) ‚àºDir(a1, . . . , ak) and for the random
variable X, let Pr{X = j|p1, . . . , pk} = pj a.s.
for j = 1, . . . , k, then the
posterior distribution of (p1, . . . , pk‚àí1|X = j) ‚àºDir(aj
1, . . . , ai
k), where
aj
i =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
ai
if i Ã∏= j
ai + 1
if i = j.
Note that this shows that the Dirichlet distribution is a conjugate prior for any
random variable with discrete probability distribution and with Ô¨Ånite support.
3.2.2
DeÔ¨Ånition of the Dirichlet process
In this section we discuss how to place a conjugate prior for any probability measure
on a general probability space with any support ‚Ñ¶(i.e. ‚Ñ¶= R, ‚Ñ¶= R+ or ‚Ñ¶= Rp).
Ferguson (1973) introduced the Dirichlet process as a class of priors over an arbitrary
measurable space ‚Ñ¶, indexed by elements of a Borel œÉ-algebra F.
The Dirichlet
process characterized by a stochastic process along with its conjugacy property has
gained widespread adoption both in theory and practice.
DeÔ¨Ånition 3.2.2 (Dirichlet random variable) Let (‚Ñ¶, F) be an arbitrary mea-
surable space and H be a probability measure on (‚Ñ¶, F) . Let a > 0 be arbitrary. A
random probability measure P deÔ¨Åned on F is called a Dirichlet probability measure
with parameters a and H, denoted by P ‚àºDP(a, H), if for any Ô¨Ånite measurable
partition {A1, . . . , Ak} of ‚Ñ¶, the joint distribution of the vector (P(A1), . . . , P(Ak)) ‚àº
Dir(aH(A1), . . . , aH(Ak)), k ‚â•2. We assume that if H(Aj) = 0, then P(Aj) = 0
with probability one.
For any measurable set A ‚ààF, P(A) ‚àºBeta{aH(A), a(1 ‚àíH(A))}. Thus,
i)
E(P(A)) =
aH(A)
aH(A) + a(1 ‚àíH(A)) = H(A)

3. Gamma and Dirichlet Process
31
V ar(P(A)) = H(A)(1 ‚àíH(A)
1 + a
.
ii) If P ‚àºDP(a, H) and X1, . . . , Xm is a sample from P, then the posterior distri-
bution of P|X1, . . . , Xm ‚àºDP(a(1), H(1)), where
a(1) = a + m,
H(1) =
a
a + mH +
m
a + m
 1
m

m
X
i=1
Œ¥Xi.
For more details, interested reader can refer to Ferguson (1973).
From (i), it is clear that H plays the role of the centre of the process and hence
is called the base measure (or also known as the initial guess). Also, we can see that
as a gets larger the variance gets smaller. Therefore the distribution of P is more
tightly concentrated around its mean, H. Hence, the parameter a can be seen as the
concentration parameter. Note from (ii), the posterior base distribution H(1) is the
combination of the base distribution and the empirical distribution. The posterior
base distribution approaches the prior base measure H as a ‚Üí‚àû. Also, it approaches
the empirical distribution as a ‚Üí0. Note that from strong law of large numbers, we
get as m ‚Üí‚àû, H(1) ‚ÜíF, where F = limm‚Üí‚àû
1
m
Pm
i=1 Œ¥Xi.
3.2.3
Series representation of the Dirichlet process
Ferguson (1973) showed that the Dirichlet process can be deÔ¨Åned by using a sum
representation for processes with independent increments. These processes are based
on the arrival times of a homogeneous Poisson process. In fact, the Dirichlet process,
P ‚àºDP(a, H), can be represented as a normalized Gamma process, see similarity of
the self normalization in (3.2.1). Ferguson (1973) described the Dirichlet process by
normalizing the series representation of a Gamma random measure given in (3.1.2)

3. Gamma and Dirichlet Process
32
as follows
PFerg(¬∑) =
‚àû
X
i=1
N ‚àí1(Œìi)
P‚àû
i=1 N ‚àí1(Œìi)Œ¥Œ∏i(¬∑),
(3.2.2)
where N, is deÔ¨Åned in (3.1.1) and (Œ∏i)i‚â•1 is a sequence of i.i.d. random variables with
common distribution H. Notice that PFerg is a discrete random probability measure.
Similar to the Gamma process, sampling from the Dirichlet process based in
(3.2.2) is diÔ¨Écult in practice since there is no closed form for the inverse of the
Gamma L¬¥evy measure. Moreover, there are an inÔ¨Ånite number of terms in (3.2.2)
that must be computed.
3.2.4
Approximation of the Dirichlet process
The Bondesson (1982) sum representation of the Dirichlet process with parameters a
and H is deÔ¨Åned in the next theorem.
Theorem 7 (Bondesson 1982) Let (Œ∏i)i‚â•1 be a sequence of i.i.d. random variables
with common distribution H and let (Ei)i‚â•1 be a sequence of i.i.d. exponential random
variables with mean 1, independent of (Œìi)i‚â•1 and (Œ∏i)i‚â•1. Then,
PBond(¬∑) =
‚àû
X
i=1
e‚àíŒìi/aEi
P‚àû
i=1 e‚àíŒìi/aEi
Œ¥Œ∏i(¬∑).
(3.2.3)
For more details, see Ishwaran & Zarepour (2002) and Zarepour & Al Labadi (2012).
Note that the Bondesson representation overcomes the problem of inverting the
Gamma L¬¥evy measure. However, the inÔ¨Ånite number of terms to compute in (3.2.3)
make it diÔ¨Écult to sample from the Dirichlet process.

3. Gamma and Dirichlet Process
33
One can approximate the Dirichlet process by using a truncation as follows:
PBond
n
(¬∑) =
n
X
i=1
e‚àíŒìi/aEi
Pn
i=1 e‚àíŒìi/aEi
Œ¥Œ∏i(¬∑).
(3.2.4)
The choice of n can be selected for a given tolerance value œµ ‚àà(0, 1) by
n = inf
(
j :
e‚àíŒìj/aEj
Pj
i=1 e‚àíŒìj/aEj
< œµ
)
.
(3.2.5)
Figure 3.4 shows the Bondesson (1982) approximation of Dirichlet process with a = 5
and base measure H ‚àºt(5).
It also shows the weights in (3.2.4) represented as
vertical lines at diÔ¨Äerent location Œ∏i. For a tolerance of œµ = 0.0001 and following the
truncation rule in (3.2.5), we get n = 25. Note that unlike the Gamma process, the
weights of the Dirichlet process sum up to 1. It is worth mentioning that the weights
in (3.2.4), represented in the plot by vertical lines, are not monotonically decreasing
almost surely.

3. Gamma and Dirichlet Process
34
Figure 3.4: One sample path of the Dirichlet process approximated using
Bondesson (1982) approximation.
We choose a = 5 and H ‚àºt(5).
For
œµ = 0.0001, we get n = 25. Vertical lines at diÔ¨Äerent location Œ∏(i) represent
the weights in calculated (3.2.4).
Figure 3.5 shows ten diÔ¨Äerent sample paths of the Dirichlet process using Bon-
desson (1982) approximation.
In all paths, we take a = 5 and H ‚àºt(5).
For
œµ = 0.0001 the truncation value n are calculated following (3.2.5), thus we get
n = 26, 31, 23, 26, 27, 36, 25, 28, 27 and 24.
The Ferguson (1973) and Bondesson (1982) sum representation for the Dirichlet
process is based on the normalized Gamma process. Sethuraman (1994) deÔ¨Åned the

3. Gamma and Dirichlet Process
35
Figure 3.5:
Ten sample paths of the Dirichlet process using Bondesson
(1982) approximation with a = 5, H ‚àºt(5).
For œµ = 0.0001, we get
n = 26, 31, 23, 26, 27, 36, 25, 28, 27 and 24.
Dirichlet process by using a stick breaking representation instead. This representation
does not involve a normalization. Similar to the Bondesson (1982) sum representation,
the stick breaking representation avoids inverting the L¬¥evy measure N in (3.1.1).

3. Gamma and Dirichlet Process
36
Theorem 8 (Sethuraman 1994) Let (Bi)i‚â•1 be a sequence of i.i.d. random vari-
ables with Beta(1, a) distribution. DeÔ¨Åne
p1 = B1,
pi = Bi
i‚àí1
Y
k=1
(1 ‚àíBk),
i ‚â•2.
Moreover, let (Œ∏i)i‚â•1 be a sequence of i.i.d. random variables with common distribution
H, independent of (pi)i‚â•1. Then
PSeth(¬∑) =
‚àû
X
i=1
piŒ¥Œ∏i(¬∑),
(3.2.6)
is a Dirichlet process with parameter a and H.
The Dirichlet process can be approximated by using Sethurman‚Äôs stick breaking ap-
proximation. This is done by truncating the higher order terms in (3.2.6). Let (Bi)i‚â•1,
(pi)i‚â•1 and (Œ∏i)i‚â•1 be deÔ¨Åned in Theorem 8 with the only diÔ¨Äerence that Bn = 1,
PSeth
n
(¬∑) =
n
X
i=1
piŒ¥Œ∏i(¬∑).
(3.2.7)
Note that by letting Bn = 1, the weights (p1, . . . , pn) sum up to 1 almost surely. For
more details, see Ishwaran and James (2001). Muliere and Tradella (1998) proposed
a stopping rule for n where, for œµ ‚àà(0, 1)
n = inf{i : pi = (1 ‚àíB1) . . . (1 ‚àíBi‚àí1)Bi < œµ}.
(3.2.8)
Figure 3.6 shows one path of the Dirichlet process using the Sethuraman (1994)
approximation. We use a = 5 and H ‚àºt(5) . Figure 3.6 depicts also the weights
in (3.2.7) at every location t = Œ∏(i). Those weights are represented in the plot by
vertical lines. Using the stopping rule proposed by Muliere and Tradella (1998) in
(3.2.8) with œµ = 0.0001, we get n = 45. Similar to the Dirichlet process approximated
by Bondeson (1982) (see Figure 3.4), the weights in the stick breaking approach are
not strictly decreasing, thus vertical lines in the graph are not strictly decreasing.

3. Gamma and Dirichlet Process
37
Figure 3.6: One sample path of Sethuraman (1994) Dirichlet process approx-
imation, P Seth((‚àí‚àû, t]) ‚àºDP(a, H). We choose a = 5 and H ‚àºt(5). For
œµ = 0.0001 and using the stopping rule in (3.2.8), we get n = 45.
Figure 3.7 shows ten diÔ¨Äerent paths of the Dirichlet process approximated by the
stick breaking approach of Sethurman (1994). For each path, the truncation values
n = 30, 38, 57, 44, 54, 52, 21, 31, 43 and 54 are calculated using (3.2.8). Note that the
values of n calculated for each path of the Dirichlet process approximation of Sethu-
raman (1994) is relatively higher comparing to the Dirichlet process approximated by
Bondeson (1982) (see Figure 3.5 for more details). In the next section, we will discuss
theoretically why the weights in (3.2.7) are not strictly decreasing. This makes the

3. Gamma and Dirichlet Process
38
stopping rule in (3.2.8) and (3.2.5) ineÔ¨Écient to approximate the Dirichlet process.
Indeed, for both approximations, the stopping rules are overestimating the value of
n.
Figure 3.7: Ten sample paths of the Dirichlet process approximated by Sethu-
raman (1994) stick breaking approach. We choose a = 5 and H ‚àºt(5). For
œµ = 0.0001 and using the stopping rule in (3.2.8), the values of n for each
path is n = 30, 38, 57, 44, 54, 52, 21, 31, 43 and 54.
Zarepour and Al Labadi (2012) prove that the weights in the Bondesson‚Äôs rep-
resentation (3.2.3) and in the Sethurman sum representation (3.2.7) are not strictly

3. Gamma and Dirichlet Process
39
decreasing almost surely. They prove that for i ‚â•1,
Pr
 e‚àíŒìi+1/aEi+1
P‚àû
i=1 e‚àíŒìi/aEi
<
e‚àíŒìi/aEi
P‚àû
i=1 e‚àíŒìi/aEi

= a
‚àû
X
k=0
(‚àí1)k/(k + a),
and,
Pr{pi+1 < pi} = a
‚àû
X
k=0
(‚àí1)k/(k + a),
(3.2.9)
where {pi}i‚â•1 are as deÔ¨Åned in Theorem 8.
When a = 1 the right hand side of
(3.2.9) is equal to 0.6931 and when a = 10 the probability is 0.5249. Therefore, for
almost all i, with certain values of a there is a non-negligible probability of having
non decreasing weights. Therefore, the suggested stopping rules in (3.2.8) and (3.2.5)
with respect to the suggested weights are not eÔ¨Écient in simulating the Dirichlet
process. The weights are not monotonically decreasing, this phenomena will tend
to overestimate the truncation value n. Moreover, there is no guarantee that after
choosing the weights up to the value n, there will not be a non negligible weight.
Zarepour and Al Labadi (2012) proposed a monotonically decreasing approxima-
tion of the Dirichlet process by normalizing the Ô¨Ånite sum Gn of the Gamma process
deÔ¨Åned in (3.1.7). This is deÔ¨Åned in the next theorem.
Theorem 9 (Zarepour & Al Labadi) Let (Œ∏i)i‚â•1 be an i.i.d. sequence of random
variables with common distribution H, independent of (Œìi)i‚â•1, then as n ‚Üí‚àû
PZar&Al Lab
n
=
n
X
i=1
G‚àí1
n

Œìi
Œìn+1

Pn
i=1 G‚àí1
n

Œìi
Œìn+1
Œ¥Œ∏i
a.s
‚ÜíP Ferg,
(3.2.10)
where, Gn(x) is as deÔ¨Åned in (3.1.6).
Note that since G‚àí1
n
is a decreasing function then G‚àí1
n (Œìi/Œìn+1) > G‚àí1
n (Œìi+1/Œìn+1).
This is coming from the fact that for any 1 ‚â§i ‚â§n, Œìi/Œìn+1 < Œìi+1/Œìn+1 al-
most surely. To sample from the Dirichlet process approximation of Zarepour and Al

3. Gamma and Dirichlet Process
40
Labadi (2012), we can use the set of rules describe in Algorithm A. But we would need
to normalize the weights of the Gamma process to get the weights of the Dirichlet
process.
Although the technique used by Zarepour & Al Labadi (2012) to approximate the
Dirichlet process is not based on a truncation method, they propose for comparison
purposes a random stopping rule similar to that given in (3.2.5). For a given tolerance
value of œµ ‚àà(0, 1), n can be calculated as follows:
n = inf
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
j :
G‚àí1
j

Œìj
Œìj+1

Pj
i=1 G‚àí1
j

Œìi
Œìj+1
 < œµ
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
.
(3.2.11)
Figure 3.8 shows one sample path of the Dirichlet process using Zarepour & Al Labadi
(2012) approximation with a = 5 and H ‚àºt(5). The truncation value is calculated
following (3.2.11). Thus for œµ = 0.0001, we get n = 12. Vertical lines in the graph
state the weights in (3.2.10) at diÔ¨Äerent location t = Œ∏(i).

3. Gamma and Dirichlet Process
41
Figure 3.8: One sample path of the Dirichlet process approximated by Zare-
pour & Al Labadi (2012) approximation of the Dirichlet process with a = 5,
H ‚àºt(5). For œµ = 0.0001, we get n = 12
Figure 3.9 shows ten sample paths of the Dirichlet process approximated by
Zarepour and Al Labadi (2012). For comparison purposes, parameters a = 5, H ‚àº
t(5) are the same as in the Bondesson(1982) and Sethurman (1994) approximation of
the Dirichlet process discussed earlier. With œµ = 0.0001 and using the stopping rule
in (3.2.11), we obtain n = 15, 11, 13, 10, 7, 10, 8, 6, 7 and 7, one for each sample path.
Note that the Zarepour & Al Labadi (2012) approximation of the Dirichlet process
uses less weights compared to the other two approximations.

3. Gamma and Dirichlet Process
42
Figure 3.9: Ten paths of Zarepour & Al Labadi (2012) approximation of
Dirichlet process with a = 5, H ‚àºt(5). For œµ = 0.0001 the value of n is
equal to n = 15, 11, 13, 10, 7, 10, 8, 6, 7 and 7, one for each sample path.
In the following we present a simulated example showing how we can use the
Dirichlet process to estimate the distribution of an observed data. To see this more
concretely, let us consider an i.i.d. data set coming from a Normal distribution. Let
X1, . . . , Xm ‚àºP, where P ‚àºNormal(‚àí2, 1). Note that in practice the actual dis-
tribution of the data is not known in advance. Therefore, we are trying to infer the
actual distribution in practice.

3. Gamma and Dirichlet Process
43
As a Bayesian approach, we need to put a prior guess on P. Consider using the
following prior
P ‚àºDP(5, t(5)),
then, as discussed in (ii), the posterior distribution of P becomes
P|X1, . . . , Xm ‚àºDP
 
(5 + m),
 
5
5 + mt(5) +
m
a + m
 1
m

m
X
i=1
Œ¥Xi
!!
.
In Figure (3.10), the step functions with solid line describe Ô¨Åve sample paths of the
Dirichlet process DP(5, t(5)) prior guess, approximated by Zarepour & Al Labadi
(2012) with n = 1000 (we use Algorithm A to sample from the Dirichlet process). In
here the solid line represents the actual cumulative distribution of the data set, we
have X1, . . . , Xm ‚àºNormal(‚àí2, 1). The dotted step functions represent Ô¨Åve diÔ¨Äerent
paths of the posterior distribution of the Dirichlet process approximated after observ-
ing m data points. From top to bottom, m is chosen to be 5, 20 and 200 respectively.
We can see how regardless of our prior guess, the posterior distribution will converge
to the actual distribution of the data set with increasing number of observation.

3. Gamma and Dirichlet Process
44
Figure 3.10: Solid step functions show the Dirichlet process prior using the
Zarepour & Al Labadi (2012)‚Äôs approximation with a = 5, H ‚àºt(5) and
n = 1000. The solid line is the actual cumulative distribution of the data set
which is in our case Normal(‚àí2, 1). Top to bottom plot shows the posterior
distribution of the Dirichlet process after observing m = 5, 20 and 200 data
points respectively.

Chapter 4
Two-parameter Poisson-Dirichlet
and the normalized
inverse-Gaussian process
4.1
Two-parameter Poisson-Dirichlet process
The two parameter Poisson-Dirichlet process also known as the Pitman-Yor process
is a generalization of the Dirichlet process. It has also been used as prior in non-
parametric Bayesian inference. Let PH,Œ±,a ‚àºPDP(H; Œ±, a) denote a two parameter
Poisson-Dirichlet, the probability measure H is called the based measure, where Œ± and
a are called the discount parameter and the concentration parameter, respectively.
4.1.1
DeÔ¨Ånition of the two-parameter Poisson-Dirichlet pro-
cess
Pitman and Yor (1997) introduce the stick-breaking deÔ¨Ånition of the two-parameter
Poisson-Dirichlet process deÔ¨Åned on an arbitrary measurable space (‚Ñ¶, F).
45

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
46
DeÔ¨Ånition 4.1.1 Let 0 ‚â§Œ± < 1, a > ‚àíŒ± and (Œ≤i)i‚â•1 be a sequence of independent
random variables with Beta(1 ‚àíŒ±, a + iŒ±) distribution. DeÔ¨Åne
p
‚Ä≤
1 = Œ≤1,
p
‚Ä≤
i = Œ≤i
i‚àí1
Y
j=1
(1 ‚àíŒ≤j),
i ‚â•2.
Let (Œ∏i)i‚â•1 be a sequence of i.i.d.
random variables with common distribution H,
independent of (Œ≤i)i‚â•1 and p1 ‚â•p2 ‚â•¬∑ ¬∑ ¬∑ be the sorted values of (p
‚Ä≤
i)i‚â•1. Then the
random probability measure
PH,Œ±,a(¬∑) =
‚àû
X
i=1
piŒ¥Œ∏i(¬∑)
(4.1.1)
is called a two-parameter Poisson-Dirichlet process with parameters Œ±, a and H.
Note that for the special case when Œ± = 0, (Œ≤i)i‚â•1 become a sequence of independent
random variables with Beta(1, a). Thus, the two-parameter Poisson-Dirichlet process
PH,0,a becomes simply the Dirichlet process. On the other hand, when a = 0, Pitman
and Yor (1997) show that the two-parameter Poisson-Dirichlet process PH,Œ±,0, becomes
the normalized non-negative Stable law process with index Œ± ‚àà(0, 1).
Note that for any measurable subset A of ‚Ñ¶, the two-parameter Poisson-Dirichlet
process has the following properties,
E(PH,Œ±,a(A)) = H(A),
V ar(PH,Œ±,a(A)) = H(A)(1 ‚àíH(A))1 ‚àíŒ±
1 + Œ∏ .
For more details on the calculation of the moments for the two-parameter Poisson-
Dirichlet process, interested reader can refer to Carlton (1999).
Similar to the Dirichlet process, the base measure H plays the role of the center
of the process. Whereas, both Œ± and a govern the variability of PH,Œ±,a around its base
measure H.
The next theorem derives the posterior distribution of PH,Œ±,a given the data set.
Note that unlike the Dirichlet and Beta process, the two-parameter Dirichlet process
does not have the conjugacy property.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
47
Theorem 10 Let X1, . . . , Xm be a sample from PH,Œ±,a. Let n be the number of dis-
tinct X‚Ä≤
is, X
‚Ä≤
j be the jth distinct Xi and mj be the number of Xi equal to X
‚Ä≤
j. Then
PH,Œ±,a|X1, . . . , Xm
d=
n
X
j=1
WjŒ¥X‚Ä≤
j + Wn+1PH,Œ±,a+nŒ±,
where PH,Œ±,a+nŒ± ‚àºPDP(H, Œ±, a + nŒ±) and (W1, . . . , Wn+1) ‚àºDir(m1 ‚àíŒ±, . . . , mn ‚àí
Œ±, a + nŒ±) and such that they are conditionally independent given X1, . . . , Xm.
4.1.2
Approximation of the two-parameter Dirichlet process
The inÔ¨Ånite sum in (4.1.1) makes it diÔ¨Écult in practice to draw a sample from the
Poisson-Dirichlet process. An approximation of the two-parameter Poisson-Dirichlet
process can be done by truncating the higher order terms in the sum (4.1.1) (Al
Labadi and Zarepour (2014)).
Let (Œ≤i)i‚â•1, (pi)i‚â•1 and Œ± are as deÔ¨Åned earlier, with Œ≤n = 1 (Ishwaran and James,
2001). The random probability measure
PH,Œ±,a(¬∑) =
n
X
i=1
piŒ¥Œ∏i(¬∑)
(4.1.2)
is the Ô¨Ånite approximation of the two-parameter Poisson-Dirichlet process. Mimicking
the same stopping rule n proposed by Muliere and Tradella (1998) for the Dirichlet
process,
n = inf{i : p
‚Ä≤
i = (1 ‚àíŒ≤1) ¬∑ ¬∑ ¬∑ (1 ‚àíŒ≤i‚àí1)Œ≤i < œµ},
for œµ ‚àà(0, 1).
Al Labadi and Zarepour (2014) show that the weights, (p
‚Ä≤
i)i‚â•1, in (4.1.2) before
ordering them are not strictly decreasing almost surely (see Lemma 1 of Al Labadi
and Zarepour (2014) for the proof).
Pitman and Yor (1997) propose a diÔ¨Äerent interesting approach to construct the
two-parameter Poisson Dirichlet process. This is described in the next proposition.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
48
Proposition 4.1.2 (Pitman and Yor 1997, Proposition 22) For 0 < Œ± < 1
and a > 0, suppose (p1(0, a), p2(0, a), . . .) has distribution PD(0, a) and (p1(Œ±, 0),
p2(Œ±, 0), . . .) has distribution PD(Œ±, 0).
Independent of (p1(0, a), p2(0, a), . . .), let
(pi
1(Œ±, 0), pi
2(Œ±, 0), . . .), i = 1, 2, . . . be a sequence of independent copies of (p1(Œ±, 0),
p2(Œ±, 0), . . .). Let (pi)i‚â•1 be the descending order statistics of {pi(0, a)pi
j(Œ±, 0), i, j =
1, 2, . . . }. Then (p1, p2, . . . ) has a PD(Œ±, a) distribution.
Note that the weights of the two-parameter Poisson-Dirichlet process is con-
structed based on two diÔ¨Äerent choices of parameters. One with Œ± = 0 which corre-
spond to the Dirichlet process PH,0,a and another when a = 0 which correspond to
the normalized Stable law process PH,Œ±,0. The index of the Stable law process Œ± is
in (0, 1). Therefore an approximation of the two-parameter Poisson-Dirichlet process
would require to draw independently a sample from the Dirichlet process and from
the normalized Stable law process.
Pitman and Yor (1997, Proposition 10) prove that the sum representation of the
normalized Stable law process can be written as follows
PH,Œ±,0(¬∑) =
‚àû
X
i=1
Œì‚àí1/Œ±
i
P‚àû
i=1 Œì‚àí1/Œ±
i
Œ¥Œ∏i(¬∑).
Therefore, the approximation of the normalized Stable law process is
PH,Œ±,0(¬∑) =
n
X
i=1
Œì‚àí1/Œ±
i
Pn
i=1 Œì‚àí1/Œ±
i
Œ¥Œ∏i(¬∑),
(4.1.3)
for large enough n. Note that the weights

Œì‚àí1/Œ±
i
/ Pn
i=1 Œì‚àí1/Œ±
i

1‚â§i‚â§n are not nec-
essarily strictly decreasing. Al Labadi and Zarepour (2014) approximate the two-
parameter Poisson-Dirichlet process by Ô¨Årst sampling a draw from the Dirichlet pro-
cess given in (3.2.10)(using Algorithm A), then sampling a sample path of the nor-
malized Stable law process in (4.1.3). Al Labadi and Zarepour (2014) compared their
approximation with the corresponding stick-breaking approximation given in (4.1.2).
Through simulation, Al labadi and Zarepour (2014) show that the two-parameter

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
49
Poisson-Dirichlet approximated using their approach concludes more precise results
compared to the one obtained using the stick-breaking approximation in (4.1.2).
Figure 4.1 shows one sample path of the two parameter Poisson-Dirichlet process.
We take H to be a t-distribution with 5 degrees of freedom, Œ± = 0.5 and a = 1. The
graph shows as well the weights in (4.1.2) as vertical lines at diÔ¨Äerent locations t = Œ∏(i).
4.2
Normalized inverse-Gaussian process (NIGP)
Analogous to the Dirichlet process, Lijoi, Mena and Pr¬®unster (2005) deÔ¨Åne the nor-
malized inverse Gaussian process as a prior to use in Bayesian nonparametric infer-
ence.
DeÔ¨Ånition 4.2.1 The random vector (Z1, . . . , Zm) is said to have a normalized inverse-
Gaussian distribution with parameters (Œ≥1, . . . , Œ≥m), where Œ≥i > 0 for all i, if it has
the joint density function
f(z1, . . . , zm) = e
P‚àû
i=1 Œ≥i Qm
i=1 Œ≥i
2m/2‚àí1œÄm/2
√ó K‚àím/2
 v
u
u
t
m
X
i=1
Œ≥i2
zi
!
√ó

m
X
i=1
Œ≥i2
zi
‚àím/4
√ó
m
Y
i=1
z‚àí3/2
i
√ó IS(z1, . . . , zm),
where K is the modiÔ¨Åed Bessel function of the third type, S = {(z1, . . . , zm) : zi ‚â•
0, Pm
i=1 zi = 1}, and IS represents the indicator function of the set S.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
50
Figure 4.1: One sample path of the two parameter Poisson-Dirichlet process
with H ‚àºt(5), Œ± = 0.5 and a = 1.
the x-axis shows atoms generated
from Œ∏i ‚àºH in increasing order and the y-axes represent the resulting two
parameter Poisson-Dirichlet process. Vertical lines represent the intensity of
the weights calculated from (4.1.2).

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
51
4.2.1
DeÔ¨Ånition of the normalized inverse-Gaussian process
(NIGP)
DeÔ¨Ånition 4.2.2 Let E be a Polish space and B(E) (or ‚Ñ¶) be the Borel œÉ-algebra
generated by the open sets in E. A random probability measure PH,a = {PH,a(B)}B‚ààB(E)
is called a normalized inverse-Gaussian process on (E, B(E)) with parameter H
(a Ô¨Åxed probability measure) and a > 0 (concentration parameter), if for any Ô¨Å-
nite measurable partition B1, . . . , Bm of B(E), the joint distribution of the vector
(PH,a(B1), . . . , PH,a(Bm)) has a normalized inverse-Gaussian distribution with param-
eter (aH(B1), . . . , aH(Bm)). We denote the normalized inverse-Gaussian process with
parameters a and H by NIGP(H, a).
Here are some basic properties of the normalized inverse-Gaussian process. For any
B ‚àà‚Ñ¶,
E(PH,a(B)) = H(B),
V ar(PH,a(B)) = H(B)(1 ‚àíH(B))
Œæ(a)
,
where
Œæ(a) =
1
a2eaŒì(‚àí2, a)
and Œì(‚àí2, a) =
R ‚àû
a u‚àí3e‚àíudu.
Abramowitz and Stegun (1972) show that for large values of a, we have Œæ(a) ‚âàa.
Therefore, similar to the two-parameter Poisson-Dirichlet process, the base measure
H plays the role of the center of the process, while a plays the role of the concentration
parameter.
The posterior distribution of the normalized inverse Gaussian process can be
found in Lijoi, Mena and Pr¬®unster (2005). Note that its posterior distribution is not
a conjugacy prior, similar to the two parameter Poisson-Dirichlet process.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
52
4.2.2
Series representation of the NIGP
The normalized inverse-Gaussian process can be written as a series representation.
Let (Œ∏i)i‚â•1 be a sequence of i.i.d. random variables with common distribution H,
independent of Œìi. The normalized inverse-Gaussian process has the exact sum rep-
resentation as follows:
PH,a(¬∑) =
‚àû
X
i=1
L‚àí1(Œìi)
P‚àû
i=1 L‚àí1(Œìi)Œ¥Œ∏i(¬∑),
where
L(x) =
a
‚àö
2œÄ
Z ‚àû
x
e‚àít/2t‚àí3/2dt, for x > 0,
(4.2.1)
is the L¬¥evy measure. For more details, the interested reader can refer to Ferguson
and Klass (1972) or Nieto-Barajas and Pr¬®unster (2009).
4.2.3
Stick-breaking representation of the NIGP
Similar to the Gamma process, inverting the L¬¥evy measure in (4.2.1) is diÔ¨Écult in
practice. Favaro, Lijoi and Pr¬®unster (2012) use a ‚Äùstick-breaking‚Äù approach to deÔ¨Åne
the normalized inverse-Gaussian process.
Let (Zi)i‚â•1 be i.i.d. random variables where Zi is 1/2-stable random variable
with scale parameter 1. Let X1 ‚àºGIG(a2, 1, ‚àí1/2), deÔ¨Åne
V1 =
X1
X1 + Z1
,
for i = 2, 3, . . ., given V1, . . . , Vi‚àí1 and Xi ‚àºGIG

a2/ Qi‚àí1
j=1(1 ‚àíVj), 1, ‚àíi/2

, for
i ‚â•2
Vi =
Xi
Xi + Zi
,

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
53
The sequence (Xi)i‚â•1 and (Zi)i‚â•1 are independent from each other and GIG
denotes the generalized inverse-Gaussian distribution (see equation (2) of Favaro,
Lijoi and Pr¬®unster (2012)). Let (Œ∏i)i‚â•1 be a sequence of i.i.d. random variables with
common distribution H independent of (Vi)i‚â•1 and let
p1 = V1,
pj = Vj
j‚àí1
Y
i=1
(1 ‚àíVi),
j ‚â•2,
(4.2.2)
then
PH,a(¬∑) =
‚àû
X
i=1
piŒ¥Œ∏i(¬∑),
(4.2.3)
is a normalized inverse-Gaussian process with parameters a and H.
4.2.4
Approximation of the NIGP
The normalized inverse-Gaussian process can be approximated by truncating the
higher order terms in the sum (4.2.3) as follows:
Pn,H,a(¬∑) =
n
X
i=1
piŒ¥Œ∏i(¬∑),
(4.2.4)
where (Vi)i‚â•1, (pi)i‚â•1 are as deÔ¨Åned in (4.2.2) independent of (Œ∏i)i‚â•1.
Note that
Vn|V1, . . . , Vn‚àí1 = 1 is necessary to make the weights add to 1 almost surely.
A
stopping rule for choosing n is similar to the one proposed by Muliere and Tradella
(1998) for the Dirichlet process, that is for œµ ‚àà(0, 1),
n = inf{i : pi = Vj
j‚àí1
Y
i=1
(1 ‚àíVi) < œµ}.
(4.2.5)
The graph at the top of Figure 4.2 shows one sample path of the normalized
inverse-Gaussian process using the stick breaking approach. We take H to be a t-
distribution with 5 degrees of freedom, a = 1 and n = 100. The graph shows as well
the weights in (4.2.4) in vertical lines at diÔ¨Äerent locations t = Œ∏(i). Note that the

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
54
choice of n in that graph is a relatively big value (no stopping rule has been applied for
this Ô¨Ågure). The graph at the bottom shows one same sample path of the normalized
inverse-Gaussian process with same parameters a and H but with n = 3. The value
of n is calculated based on the stopping rule in (4.2.5) for epsilon=0.0001. Note that
the stopping rule for n is not eÔ¨Écient because the weights are not monotonically
decreasing. Therefore it stops before reaching the full approximation of the NIGP.
Moreover, the assumption that Vn|V1, . . . , Vn‚àí1 = 1 for making the weights add to 1
leads to an inaccurate approximation of the NIGP.
4.2.5
Al Labadi & Zarepour approximation of the NIGP
Similar to the Dirichlet process, Al Labadi and Zarepour (2014) derive a Ô¨Ånite
sum representation of the normalized inverse-Gaussian process that converges almost
surely to the Ferguson and Klass (1972) representation.
Let (Œ∏i)i‚â•1 be a sequence of i.i.d. random variables with common distribution H,
independent of (Œìi)i‚â•1, then as n ‚Üí‚àû
P Lab
n,H,a =
n
X
i=1
L‚àí1
n

Œìi
Œìn+1

Pn
i=1 L‚àí1
n

Œìi
Œìn+1
Œ¥Œ∏i
a.s.
‚ÜíPH,a =
‚àû
X
i=1
L‚àí1(Œìi)
P‚àû
i=1 L‚àí1(Œìi)Œ¥Œ∏i,
(4.2.6)
where
Ln(x) =
Z ‚àû
x
a
n
‚àö
2œÄt‚àí3/2 exp

‚àí1
2
 a2
n2t + t

+ a
n

dt.
In here, L(x) is deÔ¨Åned in (4.2.1)
For the same reason discussed in Theorem 9, the weights in the sum approxi-
mation of the normalized inverse-Gaussian process (4.2.6) decrease monotonically for
any Ô¨Åxed positive integer n. Recall that for any 1 ‚â§i ‚â§n, Œìi/Œìn+1 < Œìi+1/Œìn+1

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
55
Figure 4.2: The plot at the top shows one sample path of the NIGP(1, t(5))
using the stick breaking approach with n = 100. The choice of n in this plot
is chosen to be relatively large. The vertical lines show the weights in (4.2.4).
The plot at the bottom depicts one sample path of NIGP(1, t(5)). Choosing
œµ = 0.0001, we get n = 3 based on the stopping rule in (4.2.5).

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
56
almost surely.
Note that L‚àí1
n
is a decreasing function, therefore L‚àí1
n (Œìi/Œìn+1) >
L‚àí1
n (Œìi+1/Œìn+1) almost surely.
Figure 4.3 depicts one sample path of the normalized inverse-Gaussian process
using Al Labadi & Zarepour (2014) approximation. We take H to be a t-distribution
with 5 degree of freedom, a = 1 and n = 50. The graph shows as well the weights
in (4.2.4) in vertical lines at diÔ¨Äerent locations t = Œ∏(i). Note that the weights are in
decreasing order.

4. Two-parameter Poisson-Dirichlet and the normalized inverse-Gaussian
process
57
Figure 4.3: One sample path of the normalized inverse Gaussian process with
H ‚àºt(5), a = 1 and n = 50. The NIGP(t(5), 1) is sampled using Al Labadi
& Zarepour (2014) approximation

Chapter 5
Beta process
5.1
Beta process
5.1.1
DeÔ¨Ånition of the Beta process
In this section, we deÔ¨Åne the Beta process and some of its basic properties.
DeÔ¨Ånition 5.1.1 (Beta process) [Thibaux & Jordan (2007)]
A Beta process B ‚àºBP(c, B0) is a completely random measure whose L¬¥evy measure
depends on two parameters, c and B0. The L¬¥evy measure of the Beta process on
‚Ñ¶√ó [0, 1] can be written as
ŒΩ(dŒ∏, dp) = cp‚àí1(1 ‚àíp)c‚àí1dpB0(dŒ∏),
where B0 is a diÔ¨Äuse Ô¨Ånite measure on (‚Ñ¶, F) and c > 0. The total mass of B0
denoted by Œ≥ = B0(‚Ñ¶) is called the mass parameter.
Notice that B is only a Ô¨Ånite measure but not necessarily a random probability
measure. One of the basic properties of the Beta process is that for any set S ‚àà‚Ñ¶
E[B(S)] = B0(S)
and
V ar[B(S)] = B0(S)
c + 1
58

5. Beta process
59
See Hjort (1990) and Thibaux & Jordan (2007), for more details. Similar to the
Dirichlet process, c is called the concentration parameter and B0 is called the base
measure. Note that as c ‚Üí‚àû, V ar[B(S)] ‚Üí0, thus with higher value of c, the Beta
process is more tightly close to the base measure B0. In general the concentration
parameter c is a function of Œ∏ but in this thesis we focus on the case where c is a
constant.
5.1.2
Series representation of the Beta process
The Beta process with continuous base can be represented as a series representation
following Ferguson (1972) such that
BFerg(¬∑) =
‚àû
X
i=1
ŒΩ‚àí1(Œìi)Œ¥Œ∏i(¬∑),
(5.1.1)
where
ŒΩ(x) = cŒ≥
Z 1
x
p‚àí1(1 ‚àíp)c‚àí1dp,
and (Œ∏i)i‚â•1 is a sequence of i.i.d. random variables with common distribution B0/Œ≥
independent of Œìi. Note that for any set S ‚àà‚Ñ¶, the inÔ¨Ånite sum in (5.1.1) is Ô¨Ånite
only if B0 is Ô¨Ånite.
In the case when the base measure B0 is discrete of the form B0 = P
i qiŒ¥Œ∏i, then
B has atoms at the same locations Œ∏i and the Beta process is deÔ¨Åned as follows
B =
‚àû
X
i=1
piŒ¥Œ∏i
pi ‚àºBeta(cqi, c(1 ‚àíqi)),
(5.1.2)
for qi ‚àà(0, 1). When the base measure B0 is the combination of discrete and continu-
ous, then the Beta process is the sum of two independent contributions. More details
can be found in Thibaux & Jordan (2007).

5. Beta process
60
5.1.3
Stick-breaking representation of the Beta process
Sampling B in (5.1.1) directly from the inÔ¨Ånite Beta process is diÔ¨Écult because the
inverse of the L¬¥evy measure doesn‚Äôt have a simple closed form. Wolpert and Ick-
stadt (1998b) introduce the inverse L¬¥evy measure algorithm, a generic technique for
pure-jump non negative L¬¥evy processes that allows to sample from B. This technique
generates the weights in (5.1.1) in decreasing order but requires inverting the incom-
plete beta function at each step which is computationally intensive. Paisley, John W
and Zaas, Aimee K and Woods, Christopher W and Ginsburg, GeoÔ¨Ärey S and Carin,
Lawrence (2010) and Broderick, Tamara and Jordan, Michael I and Pitman, Jim and
others (2012) proposed the stick-breaking representation of the Beta process that
provides a simple recursive procedure for obtaining the weights in equation (5.1.1).
This approach provides an explicit representation of a draw B from the Beta process
that doesn‚Äôt require inverting the L¬¥evy measure,
B =
‚àû
X
i=1
Ci
X
j=1
V (i)
i,j
i‚àí1
Y
l=1
(1 ‚àíV (l)
i,j )Œ¥Œ∏i,j,
(5.1.3)
Ci ‚àºPoisson(Œ≥)
V (l)
i,j
i.i.d.
‚àºBeta(1, c)
Œ∏i,j
i.i.d.
‚àºB0/Œ≥.
This is an analogue of Sethuraman‚Äôs (1994) stick-breaking representation of the
Dirichlet process. The only diÔ¨Äerence is that the weights resulting from the stick
breaking representation of the Dirichlet process all come from one single stick, thus
they add up to one. This is not the case in the stick-breaking representation of the
Beta process where the weights come from diÔ¨Äerent unit intervals. Therefore, they
do not need to add to one. Nevertheless, the sum in (5.1.3) is Ô¨Ånite almost surely.
The shortcoming of the stick-breaking representation is discussed in Al Labadi and
Zarepour (2015).

5. Beta process
61
5.1.4
Finite Approximation of the Beta process
Another yet eÔ¨Écient way of generating the weights in (5.1.1) is by deriving the Ô¨Ånite
approximation of the Beta process deÔ¨Åned by Paisley & Carin (2009) as follows
Bn =
n
X
i=1
pi,nŒ¥Œ∏i
pi,n
i.i.d.
‚àºBeta

cŒ≥n, c

1 ‚àíŒ≥n

Œ∏i
i.i.d.
‚àºB0/B0(‚Ñ¶).
Here (pi,n)1‚â§i‚â§n and (Œ∏i)1‚â§i‚â§n are independent and Œ≥n ‚Üí0 as n ‚Üí‚àû.
Later Al Labadi & Zarepour (2015) prove that the Ô¨Ånite approximation Bn con-
verges weakly to Ferguson (1972) representation of the Beta process deÔ¨Åne in (5.1.1).
In particular, they show, via proposition 3.21 of Resnick (1987), that as n ‚Üí‚àû,
nP[p1,n ‚àà(x, 1)] =
nŒì(c)
Œì(cŒ≥n)Œì(c ‚àícŒ≥n)
Z 1
x
pcŒ≥n‚àí1(1 ‚àíp)c(1‚àíŒ≥n)‚àí1dp
v‚ÜíŒΩ(x) = cŒ≥
Z 1
x
p‚àí1(1 ‚àíp)c‚àí1dp,
where, ‚Äù
v‚Üí‚Äù denote vague convergence. One choice of Œ≥n is when Œ≥n = Œ≥/n. Notice
that for any x > 0, Œì(x) = Œì(x + 1)/x, therefore n/Œì(cŒ≥/n) = cŒ≥/Œì(cŒ≥/n + 1) when
replacing x by cŒ≥/n. Moreover, as n ‚Üí‚àû
n
Œì(cŒ≥/n) ‚ÜícŒ≥
and
Œì(c)
Œì(c ‚àícŒ≥/n) ‚Üí1.
By deÔ¨Åning
ŒΩn =
Œì(c)
Œì(cŒ≥/n)Œì(c ‚àícŒ≥/n)
Z 1
x
pcŒ≥/n‚àí1(1 ‚àíp)c(1‚àíŒ≥/n)‚àí1dp,

5. Beta process
62
we have as n ‚Üí‚àû,
nŒΩn(x)
v‚ÜíŒΩ(x).
Moreover, they prove that as n ‚Üí‚àû,
Bn =
n
X
i
ŒΩ‚àí1
n
 Œìi
Œìn+1

Œ¥Œ∏i
a.s.
‚ÜíB =
‚àû
X
i=1
ŒΩ‚àí1(Œìi)Œ¥Œ∏i.
(5.1.4)
Interested readers can refer to Theorem 4 of Al Labadi & Zarepour (2015).
Based on the above results, Al Labadi & Zarepour (2015) describe an eÔ¨Écient algo-
rithm to generate sample from an approximation of the Beta process B ‚àºBP(c, B0).
Algorithm B gives details of the set of rules to sample from Beta process with con-
tinuous base.
Algorithm B: An approximation of the Beta process with continuous base
1. Choose a large positive integer n.
2. Generate Œ∏i
i.i.d.
‚àºB0/Œ≥, for i = 1, ¬∑ ¬∑ ¬∑ , n.
3. Generate Ei
i.i.d.
‚àºExp(1) with p.d.f f(x) = e‚àíxI(x > 0). Let Œìi = Pi
k=1 Ek.
4. Compute (ŒΩ‚àí1
n (Œìi/Œìn+1))1‚â§i‚â§n. This can be done by evaluating the quantile
function of the beta distribution, Beta(cŒ≥/n, c(1 ‚àíŒ≥/n)) at 1 ‚àíŒìi/Œìn+1.
Figure 5.1 shows one sample path of the Beta process with c = 0.8, B0 ‚àºUniform(0, 1)
and n = 15 using Al Labadi & Zarepour (2014) algorithm. Similar to the Dirichlet
process, vertical lines show the intensity of the weights in (5.1.4) at location Œ∏(i).
Note that the weights are monotonically decreasing, therefore we are almost surely
conÔ¨Ådent that there will not be an intense weight after the last weight observed.
Figure 5.2 depicts ten sample paths of the Beta process approximated by Al
Labadi & Zarepour (2014) algorithm (Algorithm B) where we choose n equal to
100.
The dashed line connected by dots represent the cumulative distribution of

5. Beta process
63
Figure 5.1: One sample path of the Beta process B ‚àºBP(0.8,Uniform(0, 1)).
The Beta process is approximated using Al Labadi & Zarepour (2014) algo-
rithm ( Algorithm B) with n = 15. The plot shows as well the weights in
(5.1.4) by vertical lines at the associated atoms Œ∏i.

5. Beta process
64
B0 ‚àºUniform(0, 1). The plot at the top represent the Beta process with c = 1 and
B0 =Uniform(0, 1) where as the plot at the bottom represent the Beta process with
same base measure B0 but with c = 20. The concentration parameter c express the
strength of belief in B0, this is shown in Figure 5.2 where with increase value of c the
Beta process approaches to B0 ‚àºUniform(0, 1). This behaviour support our previous
discussion on the basic properties of the Beta process.

5. Beta process
65
Figure 5.2: The plot at the top shows ten sample paths of the Beta process
with c = 1 and B0 ‚àºUniform(0, 1). The plot at the bottom shows ten sample
paths of the Beta process with same base measure B0 but with c = 20. We
use Algorithm B to approximate the Beta process in both plots with n = 100.
The dashed line connected by dots in both plots represents the cumulative
distribution of the base measure B0 ‚àºUniform(0, 1).

5. Beta process
66
5.2
Beta-Bernoulli process
Thibaux & Jordan (2007) show that the Beta process is the conjugate prior for the
Bernoulli process.
This conjugacy extends the conjugacy between the Beta and
Bernoulli distribution. In this section, we deÔ¨Åne the Bernoulli process and we in-
troduce an extension to the Beta process known by the Beta-Bernoulli process.
5.2.1
DeÔ¨Ånition of the Beta-Bernoulli process
DeÔ¨Ånition 5.2.1 (Bernoulli Process) Let H be a measure on ‚Ñ¶. The Bernoulli
process Y with base measure H, written Y ‚àºBeP(H), is a completely random vari-
able with L¬¥evy measure
œÄ(dŒ∏, dp) = Œ¥1(dp)H(dŒ∏),
where Œ¥1 is a measure concentrate at 1.
When H is a diÔ¨Äuse measure (or has no points of discontinuity), Y has an under-
lying Poisson process with intensity H. It can be proven that Y has the following
representation
Y =
K
X
k=1
Œ¥Œ∏k
(5.2.1)
K ‚àºPoisson(H(‚Ñ¶))
Œ∏k
i.i.d.
‚àºH/H(‚Ñ¶).
When H is discrete (or consist of Ô¨Åxed points of discontinuity) of the form H =
P‚àû
k=1 pkŒ¥Œ∏k, then the Bernoulli process is deÔ¨Åned at the same locations Œ∏k as follows
Y =
‚àû
X
i=k
bkŒ¥Œ∏k
bk ‚àºBernoulli(pk).
(5.2.2)

5. Beta process
67
When the base measure is the mixture of discrete and continuous, the Beta-Bernoulli
process is the superposition of two independent contributions.
The Beta process is useful as parameter for the Bernoulli process. When combining
both processes together such that the base measure of the Bernoulli process is chosen
to be the Beta process, the resulting process is known as the Beta-Bernoulli process.
DeÔ¨Ånition 5.2.2 (Thibaux and Jordan (2007)) The Beta-Bernoulli process, de-
noted by X ‚àºBeP(B), is a completely random measure with base measure the Beta
process. The model is deÔ¨Åned as follows
X|B
i.i.d.
‚àºBeP(B)
B ‚àºBP(c, B0),
where B0 and c are deÔ¨Åne in DeÔ¨Ånition 5.1.1.
5.2.2
Series representation of the Beta-Bernoulli process
A draw from the Beta process B ‚àºBP(c, B0) generates a set of atoms (pi, Œ∏i)i‚â•1,
where pi is the weight in (5.1.1) calculated at location Œ∏i. The Beta-Bernoulli has a
series representation as follows
X =
‚àû
X
i=1
biŒ¥Œ∏i,
(5.2.3)
where bi ‚àºBernoulli(pi) at location Œ∏i.
Note that from the strong law of large numbers, we get
E[X] = E[E[X|B]] = E[B] = B0.
This property of the Beta-Bernoulli process ensure that the series representation in
(5.2.3) converges even though there is an inÔ¨Ånite terms.

5. Beta process
68
5.2.3
Beta process conjugate prior for the Bernoulli process
The Beta process is the conjugate prior for the Bernoulli process. Following Thibaux
& Jordan (2007) notation, suppose we observe N data points such that
X1, ¬∑ ¬∑ ¬∑ , XN|B
i.i.d.
‚àºBeP(B)
B ‚àºBP(c, B0),
then applying Theorem 3.3 of Kim (1999) the posterior distribution of B given the
observed data X1, ¬∑ ¬∑ ¬∑ , XN is
B|X1, . . . , XN ‚àºBP(c + N, Bn)
Bn =
c
c + N B0 +
1
c + N
N
X
i=1
Xi
Bn =
c
c + N B0 +
N
c + N
PN
i=1 Xi
N
.
(5.2.4)
As N ‚Üí‚àû, the base measure approaches the empirical distribution of Xi. And
as c ‚Üí‚àûthe base measure approaches to the prior guess B0. We recall that the X‚Ä≤
is
are Beta-Bernoulli random measures not a random number.
The plot at the top of Figure 5.3 shows one sample path of the Beta process with
c = 1, B0 ‚àºUniform(0, 1). We use Paisley & Carin (2009) approximation and we
follow Al Labadi & Zarepour (2015) algorithm with n = 15 to approximate the Beta
process. Vertical lines show the intensity of the weights in (5.1.4) at location Œ∏(i). The
plot at the bottom of Figure 5.3 shows 10 draws from the Beta-Bernoulli process, one
per line. We use the Beta process displayed at the top of Figure 5.3 as the base
measure to the Bernoulli process. A draw is a set of points (bi, Œ∏i)1‚â§i‚â§15, such that
bi ‚àºBernoulli (B{Œ∏i}). Thus, each line has a black dot at position Œ∏i with probability
(B{Œ∏i}), B ‚àºBP(1, Uniform(0, 1)).

5. Beta process
69
Figure 5.3: The plot at the top depicts one sample path of the Beta process
with c = 1, B0 ‚àºUniform(0, 1). The Beta process is approximated using
Algorithm B with n = 15.
The vertical lines shows the intensity of the
weights in (5.1.4). The plot at the bottom shows 10 draws of the Bernoulli
processes, one per line, with base measure the Beta process (displayed at the
top of the Ô¨Ågure).

5. Beta process
70
5.3
Applications in Latent Feature Model
The Latent feature model has been widely used in many applications in diÔ¨Äerent
disciplines, particularly in machine learning. It has been used to decompose the data
into a small number of components. Example of applications in nonparametric latent
feature model are well explained in Miller & Jordan (2009) and in Paisley & Carin
(2009).
In this section we describe two methodologies to sample from the posterior of a
latent feature model. The Ô¨Årst method is known as the Indian BuÔ¨Äet Process in the
Computer Science community. The second method is a new technique we propose to
sample from the posterior. This new technique focuses on eÔ¨Éciency. We describe in
details the sampling techniques in both methodologies in the next sections.
5.3.1
Matrix Z
The Beta-Bernoulli process has been widely used as a prior in applications on latent
feature models. For instance, suppose we observe N data points Z1, Z2, . . . , ZN such
that N is large. In the Bayesian framework, the goal in most applications on latent
feature models is to infer a binary matrix Z, often called factor loadings, where Z is
an N √óK matrix. The number of row N represents the number of observation and K
represents the number of latent features or attributes. The dimension of K is inÔ¨Ånite,
thus the rows of Z consist of an inÔ¨Ånite collection of factor loadings. The matrix
Z is constructed in a way such that if the ith observation possesses the kth feature
then Zik is equal to 1 otherwise it is equal to 0 where 1 ‚â§i ‚â§N and 1 ‚â§k ‚â§K.
Notice that each observation possesses one or more features. Therefore, each row of
Z ideally has multiple 1‚Ä≤s. When a data set is modelled with a latent feature, there
is a certain belief that the possession of any number of these features has an eÔ¨Äect on
the observed data. Moreover, there is a strong belief that when two entities have a
great number of features in common, those two entities will most probably have the

5. Beta process
71
same structure or behaviour. On most applications involving latent feature models,
the Beta-Bernoulli process Xi is mapped to Zi. The matrix Z with rows, Z1, . . . , ZN,
is constructed in a way such that Œ∏k labels the kth column in Z and pi represents the
weight that observation i possesses feature k. Thus, entry at position Zi,k is 1 with
probability pi. Note that by Campbell‚Äôs theorem, the base measure B of the Beta-
Bernoulli process has Ô¨Ånite mass. Therefore, the columns of Z have a Ô¨Ånite number
of non-zero entries. Storing only non-zero columns make Z a Ô¨Ånite matrix which is
computationally manageable. It is important to note that columns of the matrix Z
are exchangeable.
To make the connection between the Beta-Bernoulli process and the matrix Z,
let us consider an example to describe this mapping. We extract the set of pairs
(pk, Œ∏k)1‚â§k‚â§n from the Beta process B ‚àºBP(1, Uniform(0, 1)) displayed in Figure 5.3
(the plot at the top). The Ô¨Årst 9 estimated values of (pk, Œ∏k) are reported in Table
5.1.
pk
0.73
0.66
0.24
0.0087
0.0024
0.0011
0.00027
0.00019
0.000064
Œ∏k
0.93
0.32
0.51
0.93
0.27
0.32
0.89
0.11
0.9
Table 5.1: The table shows the Ô¨Årst nine set of pairs (pk, Œ∏k)1‚â§k‚â§9 extracted
from a draw of the Beta process, B ‚àºBP(1, Uniform(0, 1)).
The Beta
process is approximated using Algorithm B with n = 15.
We extract from the Ô¨Årst draw of the Beta-Bernoulli process, X1 ‚àºBeP(B),
displayed in Figure 5.3 the set of pairs (bk, Œ∏k)1‚â§k‚â§9 such that bk ‚àºBernoulli(pk). We
report the Ô¨Årst nine estimated values in Table 5.2.
For each set of pairs of the form (bk = 1, Œ∏k)1‚â§k‚â§n, Œ∏k is mapped in Z to represent
a new latent feature. In particular, we choose to map Œ∏1 to label the Ô¨Årst column of
Z, and Œ∏2 to label the second column of Z and so on. As we discussed earlier in this
section, columns of the matrix Z are exchangeable therefore labelling columns of Z

5. Beta process
72
bk
1
0
0
0
0
0
0
0
0
Œ∏k
0.93
0.32
0.51
0.93
0.27
0.32
0.89
0.11
0.9
Table 5.2: The table depicts the Ô¨Årst nine values (bk, Œ∏k)1‚â§k‚â§9 extracted from
a draw of a Bernoulli process with base measure B ‚àºBP(1, Uniform(0, 1)).
Recall that bk ‚àºBinomial(pk), where pk is the probability displayed in Table
5.1.
by Œ∏‚Ä≤s can be done in a diÔ¨Äerent order. Note that from Table 5.1 there is three non
negligible weights. Then at most there will be three visible atoms Œ∏‚Ä≤s and consequently
at most three labelled columns in Z representing a latent feature . Indeed, Table 5.2
shows only one pair of the form (bk = 1, Œ∏), then we map Œ∏ in Z to represent a new
label.
Therefore, mapping X1 to Z1 gives
Z =
h
1
0
0
0
0
0
0
0
0
0
i
Note that the value of K (the number of column of Z) should be relatively large but
for illustration purposes we choose K = 10. We use the same mapping technique to
map X2, . . . , X10 in Z2, . . . , Z10. We get

5. Beta process
73
Z =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
(5.3.1)
Note that zero‚Äôs column means that there is no label (or latent feature) associated to
that column yet.
5.3.2
Updating the matrix Z using Methodology I
Updating the matrix Z with a new observation, ZN+1 based on what have been
observed Z1, . . . , ZN is the key in most nonparametric Bayesian applications. Recall
that
B|X1, . . . , XN ‚àºBP
 
c + N,
c
c + N B0 +
1
c + N
N
X
k=1
Xk
!
.
In the context of latent feature modelled by the matrix Z, the posterior of the Beta
process has another meaning for its discrete base measure as follows
B|X1, . . . , XN ‚àºBP
 
c + N,
c
c + N B0 +
K
X
k=1
mN,k
c + N Œ¥Œ∏k
!
,
(5.3.2)
where, mN,k = PN
i=1 I(Zi,k = 1). In words, mN,k is the number of time among N,
feature k has been observed. Notice that the posterior base measure is constructed of

5. Beta process
74
two components, one continuous and another discrete, it is reasonable to sample from
each component independently since they do not overlap. Therefore, we can write
the Bernoulli process as the sum of two independent Bernoulli processes as follows
XN+1|X1, ¬∑ ¬∑ ¬∑ , XN ‚àºBeP

c
c + N B0

+ BeP
 K
X
k=1
mN,k
c + N Œ¥Œ∏k
!
XN+1|X1, ¬∑ ¬∑ ¬∑ , XN ‚àº
F
+
R,
where
F ‚àºBeP

c
c + N B0

R ‚àºBeP
 K
X
k=1
mN,k
c + N Œ¥Œ∏k
!
.
The algorithm to updated the matrix Z is deÔ¨Åne in Methodology I. It is worth
mentioning that this technique is known by the two-parameter (c, Œ≥) generalization
of the Indian buÔ¨Äet process (Ghahramani & GriÔ¨Éths, 2005), a well know process in
the Computer Science community.
Methodology I
1. Sample a draw from F ‚àºBeP
 c
c+N B0

.
2. Map every pair (1, Œ∏
‚Ä≤
k) extracted from F in step 1 to the matrix Z.
3. Sample independently a draw from R ‚àºBeP
PK
k=1
mN,k
c+N Œ¥Œ∏k

.
4. Map every pair of the form (bk = 1, Œ∏k) extracted from R in step 4 to the matrix
Z.
See next discussion for the details of Methodology I, especially for the mapping in
step 2 and 4.

5. Beta process
75
Updating the matrix Z from the Bernoulli process with continuous base
Updating the matrix Z with a new observation ZN+1 can be done in two stages.
First, it involves sampling from the two Bernoulli processes F and R independently,
then mapping each process independently to the row ZN+1. Sampling the Bernoulli
process F with continuous base measure can be done as examined in (5.2.1)
F ‚àºBeP

c
c + N B0

F =
K1
X
k=1
Œ¥Œ∏‚Ä≤
k
where
K1 ‚àºPoisson

cŒ≥
c + N

Œ∏
‚Ä≤
k ‚àºB/B(‚Ñ¶) = B0/Œ≥.
1 ‚â§k ‚â§K1
Note that for each pair of the form (1, Œ∏
‚Ä≤
k)1‚â§k‚â§K1 extracted from F, (Œ∏
‚Ä≤
k)1‚â§k‚â§K1 are
the new label of features added to the matrix Z. Let k‚Ä≤
1, . . . , k‚Ä≤
K1 be K1 indices of zero
columns of Z, then we let (ZN+1,k‚Ä≤ = 1)k‚Ä≤
1‚â§k‚Ä≤‚â§k‚Ä≤
K1.
Updating matrix Z from the Bernoulli process with discrete base
Sampling the Bernoulli process R with discrete base measure can be done as discussed
in (5.2.2)
R ‚àºBeP
 K
X
k=1
mN,k
c + N Œ¥Œ∏k
!
,
R =
‚àû
X
k=1
bkŒ¥Œ∏k
bk ‚àºBernoulli
 mN,k
c + N


5. Beta process
76
Note that Œ∏k in the discrete base measure represent an existing latent feature in Z.
Therefore at each non zero column k of Z, ZN+1,k = 1 with probability mN,k/(c + N).
5.3.3
Updating the matrix Z using Methodology II
In this section we describe an alternative way of updating the matrix Z. This can be
done by sampling Ô¨Årst from the posterior of the Beta process then from the Beta-
Bernoulli process.
B|X1, . . . , XN ‚àºBP
 
c + N,
c
c + N B0 +
K
X
k=1
mN,k
c + N Œ¥Œ∏k
!
,
B|X1, . . . , XN ‚àºBP

c + N,
c
c + N B0

+ BP
 
c + N,
K
X
k=1
mN,k
c + N Œ¥Œ∏k
!
B|X1, . . . , XN ‚àº
BCont
+
BDisc,
where
BCont ‚àºBP

c + N,
c
c + N B0

BDisc ‚àºBP
 
c + N,
K
X
k=1
mN,k
c + N Œ¥Œ∏k
!
.
Now sampling a new observation can be done as follows
XN+1|X1, ¬∑ ¬∑ ¬∑ , XN ‚àºBeP(BCont) + BeP(BDisc)
XN+1|X1, ¬∑ ¬∑ ¬∑ , XN ‚àº
S
+
T
,
where,
S ‚àºBeP(Bcont)
T ‚àºBeP(BDisc).
The following gives further details on the methodology.

5. Beta process
77
Methodology II
1.
(a) Sample a draw from BCont. Extract the set of pairs (pCont
k
, Œ∏
‚Ä≤
k)1‚â§k‚â§n from
BCont.
(b) Sample a draw from S ‚àºBeP(BCont). Extract the set of pairs (bCont
k
, Œ∏
‚Ä≤
k)1‚â§k‚â§n.
(c) Map every pair of the form (bCont
k
= 1, Œ∏
‚Ä≤
k) into the Matrix Z.
2.
(a) Sample a draw from BDisc. Extract the set of pairs (pDisc
k
, Œ∏k)1‚â§k‚â§n.
(b) Sample a draw from T ‚àºBeP(BDisc). Extract the set of pairs (bDisc
k
, Œ∏k)1‚â§k‚â§n.
(c) Map every pair of the form (bDisc
k
= 1, Œ∏k) in Matrix Z.
Refer to the next discussion for details on updating the matrix Z using Methodology
II.
Updating the matrix Z from the Beta-Bernoulli with continuous base
Updating Z would Ô¨Årst involve sampling BCont and BDisc independently then sam-
pling from the two Beta-Bernoulli processes S and T independently. Following our
discussion in (5.1.1), the Beta process BCont with continuous base has a series repre-
sentation
BCont =
‚àû
X
k=1
pCont
k
Œ¥Œ∏‚Ä≤
k.
Using Paisley & Carin (2009) Ô¨Ånite approximation of the Beta process in (5.1.4), we
have
BCont
n
=
n
X
k=1
pCont
k,n Œ¥Œ∏‚Ä≤
k
pCont
k,n
i.i.d.
‚àºBeta
c ‚àóŒ≥‚àó
n
, c ‚àó

1 ‚àíŒ≥‚àó
n

(5.3.3)
Œ∏
‚Ä≤
k
i.i.d.
‚àºB‚àó
0/Œ≥‚àó,
(5.3.4)

5. Beta process
78
where
c‚àó= c + N
Œ≥‚àó=
c
c + N Œ≥
B‚àó
0 =
c
c + N B0.
(5.3.5)
Replacing c‚àó, Œ≥‚àóand B‚àó
0 in (5.3.3) and (5.3.4) we get,
BCont
n
=
n
X
k=1
pCont
k,n Œ¥Œ∏‚Ä≤
k
pCont
k,n
i.i.d.
‚àºBeta
cŒ≥
n , N + c ‚àícŒ≥
n

Œ∏
‚Ä≤
k
i.i.d.
‚àºB0/Œ≥,
The Beta-Bernoulli process S ‚àºBeP(BCont
n
) has series representation
S ‚àºBeP(BCont
n
)
S =
n
X
k=1
bCont
k
Œ¥Œ∏‚Ä≤
k
bCont
k
‚àºBernoulli(pCont
k,n ).
Note that when the Bernoulli process has a continuous base, the resulting process
generates new locations Œ∏
‚Ä≤‚Äôs. We map every pair of the form (1, Œ∏
‚Ä≤
k) in ZN+1 such that
every Œ∏
‚Ä≤
k label a zero column of Z. Let k‚Ä≤ represent the indices of a zero column of Z,
then we let ZN+1,k‚Ä≤ = 1.
Going back to our simulated example shown in Figure 5.3, we approximate the
posterior of the Beta process with continuous base, BCont using Al Labadi & Zarepour
(2015) algorithm for n = 15 and with the updated parameters c‚àó= 11 and B‚àó
0 ‚àº
1
11Uniform(0, 1). We extract from BCont the set of pairs (pCont
k
, Œ∏
‚Ä≤
k)1‚â§k‚â§15. Table 5.3
shows some preliminary values of those pairs. Recall that the weights generated by Al
Labadi & Zarepour (2015) are in decreasing order almost surely, therefore we omitted

5. Beta process
79
pCont
k
0.068
0.00088
0.0000065
0.00011
0.000057
Œ∏
‚Ä≤
k
0.12
0.87
0.47
0.34
0.29
Table 5.3: The table shows some preliminary values of the pairs (pCont
k
, Œ∏
‚Ä≤
k)
extracted from BCont ‚àºBP(11, 1
11Uniform(0, 1)).
the last 10 weights in the table because we are almost sure that there will not be a
non negligible weight beyond that point.
We sample a draw from S, the set of pairs (bCont
k
, Œ∏
‚Ä≤
k) are reported in Table 5.4.
bCont
k
1
0
0
0
0
Œ∏
‚Ä≤
k
0.12
0.87
0.47
0.34
0.29
Table 5.4: The table depicts some preliminary values of the pairs (bCont
k
, Œ∏
‚Ä≤
k)
extracted from S ‚àºBeP(BCont), the Beta-Bernoulli process with base mea-
sure BCont.
Note from Table 5.4 that there is only one pair such that (bCont
k
= 1, Œ∏
‚Ä≤
k = 0.12).
To map this pair in Z, we choose the fourth column (zero column) of Z to label this
new feature and we let Z11,4 = 1. Thus, we have

5. Beta process
80
Z =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Updating the matrix Z from the Beta-Bernoulli with discrete base
To Ô¨Ånish updating the matrix Z we are left with sampling from the Beta process
BDisc ‚àºBP

c + N, PK
k=1
mN,k
c+N Œ¥Œ∏k

with discrete base BDisc
0
, then sampling from the
Beta-Bernoulli process T ‚àºBeB(BDisc). Following our discussion in (5.1.2), the Beta
process BDisc has series representation
BDisc
n
=
n
X
k=1
pDisc
k
Œ¥Œ∏k
BDisc
0
=
n
X
k=1
mN,k
c + N Œ¥Œ∏k
pDisc
k
‚àºBeta(mN,k, N ‚àímN,k + c).
(5.3.6)
The Beta-Bernoulli process T has series representation
T =
‚àû
X
k=1
bDisc
k
Œ¥Œ∏k
bDisc
k
‚àºBernoulli(pDisc
k
).
(5.3.7)

5. Beta process
81
We extract from the Beta-Bernoulli process T the set of pairs (bDisc
k
, Œ∏k)1‚â§k‚â§15.
Let k
‚Ä≤
1, . . . , k
‚Ä≤
n be the indices of column of Z such that there is at least on row con-
taining an atom 1, then we let (ZN+1,k‚Ä≤ = bDisc
k
)k‚Ä≤
1‚â§k‚Ä≤‚â§k‚Ä≤
n.
Going back to our simulated example, the Ô¨Årst three column of Z in (5.3.1) have
at least one row with an atom 1. Those columns are labelled previously by Œ∏1, Œ∏2
and Œ∏3 respectively. Table 5.5 shows the estimated values of pDisc
k
calculated based
on (5.3.6), where Table 5.6 shows the estimated values of bDisc
k
calculated based on
(5.3.7).
pDisc
k
0.90
0.62
0.0055
Œ∏k
0.93
0.32
0.51
Table 5.5: The table shows the set of pairs (pDisc
k
, Œ∏k)1‚â§k‚â§3 such that pDisc
k
is
calculated based on (5.3.6).
bDisc
k
1
1
0
Œ∏k
0.93
0.32
0.51
Table 5.6: The table shows the set of pairs (bDisc
k
, Œ∏k)1‚â§k‚â§3, where bDisc
k
‚àº
Bernoulli(pDisc
k
).
The matrix Z is updated with the Beta-Bernoulli process T as follows:

5. Beta process
82
Z =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
1
0
1
0
0
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Figure 5.4 shows a draw from the posterior of the Beta-Bernoulli process X11
given X1, . . . , X10. The draw are the set of pairs of the form (bCont
k
= 1, Œ∏
‚Ä≤
k) and
(bDisc
k
= 1, Œ∏k). The triangle pointed down represent a sampling of the Beta-Bernoulli
process, T with discrete base measure and the triangle pointed up represent a sampling
of the Beta-Bernoulli process, S with continuous base measure.

5. Beta process
83
Figure 5.4: The plot at the top shows one draw of the Beta process B ‚àº
BP(1, Uniform(0, 1)) approximated by Algorithm B with n = 15. The plot at
the bottom shows 10 draws of the Beta-Bernoulli process with base measure
B. Draws are represented in the plot by dots at each pair of the form (bk =
1, Œ∏i) generated from the Beta-Bernoulli process. The plot at the bottom
shows as well one updated draw of the Beta-Bernoulli process given the 10
other observations. Triangle pointed down represent the update contributed
by the discrete base, and triangle pointed up represent the update contributed
by the continuous part of the updated Beta process.

5. Beta process
84
5.3.4
Nonparametric Latent Feature Models for Link Predic-
tion
Miller & Jordan (2009) introduce the nonparametric latent feature relational model
used for social network data. This model seeks to extract latent structure representing
the properties of individual entities from the observed data. In particular, we observe
the relationships (or links) between a set of entities in a network and we try to
predict unobserved links. For example, consider Facebook as our social network. In
such network, we only know some subset of people who are friends with and some
other who are not. The goal would be to predict which other people are likely to
become friend.
5.3.5
Basic model
Assume we observe the directed links between a set of N entities. Let Y be an N √óN
binary matrix that contains these links. That is Yij = 1 if entity i is linked to entity
j (i ‚Üíj), Yij = 0 if entity i is not linked to entity j, and Yij is left empty if we
don‚Äôt observe any link. The link can stand for diÔ¨Äerent meanings such as ‚Äùsend a
friend request or not‚Äù, ‚Äùfriend or not‚Äù, ‚Äùcolleague or not‚Äù or any other relationship.
Depending of the relation, the matrix Y can be symmetric or asymmetric. The model
decompose the binary matrix Y in two matrices Z and W. Where Z is a N √ó K
matrix; each row of Z corresponds to an entity and each column corresponds to a
feature such that Zik = 1 if entity i has feature k, otherwise take Zik = 0.
For
instance, we can have a separate feature for ‚Äùstatistician‚Äù, ‚Äùfemale‚Äù, ‚Äùathlete‚Äù, and
‚Äùpainter‚Äù and the presence or absence of each of these features is what deÔ¨Ånes each
person and determines their relationships. And W is a K √ó K real valued matrix
where the (k, k‚Ä≤) entry of W is wk,k‚Ä≤. The value wkk‚Ä≤ is the weight that aÔ¨Äects the
probability of having a link from entity i to entity j if both entity i has feature k and
entity j has feature k‚Ä≤. If we are looking at the relation ‚Äùsend a friend request‚Äù, then

5. Beta process
85
the weight at the (statistician, athlete) entry of W would correspond to the weight
that a statistician would send a friend request to an athlete.
A positive weights
would correspond to an increased probability, a negative weight would correspond to
a decreased probability, and a zero weight would indicate that there is no correlation
between these two features and the observed relation. Thus, the observed relations
depend on binary valued latent features that inÔ¨Çuences its relations, weighted with a
set of known covariates.
Following the notation of Miller & Jordan (2009), the model is deÔ¨Åned as follows:
Yij ‚àºBernoulli(œÉ(ZiWZT
j ))
B ‚àºBP(1, B0)
Z ‚àºBeP(B)
wkk‚Ä≤ ‚àºNormal(0, œÉ2
w),
where, œÉ(¬∑) is a function that transforms values on (‚àí‚àû, ‚àû) to (0, 1) such as the
sigmoid function œÉ(x) = 1/(1 + exp(‚àíx)). Note that the Beta process B can be
approximated using Algorithm B. Thus B = Pn
i=1 pkŒ¥Œ∏k and Zik ‚àºBernoulli(pk).
Another contribution in this thesis is to modify (improve) the mathematical notation
of what most Computer Scientist has adopted in machine learning community. The
matrix Z is the map of the Beta-Bernoulli process X ‚àºBeP(B) to Z we discussed
earlier in Section 5.3.1. It is worth mentioning that Miller & Jordan (2009) have put
an Indian BuÔ¨Äet Process IBP(Œ±) prior on the matrix Z. Jordan (2007) proves that
the Beta process with concentration parameter c = 1 and base measure B0 is the
IBP(Œ±), with Œ± = B0(‚Ñ¶).
Given the full set of observation Y , we wish to infer the posterior distribution of
the feature matrix Z and the weights W. This can be done by using Bayes‚Äô theorem,
p(Z, W|Y ) ‚àùP(Y |Z, W)P(Z)P(W) with an independent prior on Z and W. For
details on inference, interested reader can refer to Miller & Jordan (2009).

Appendix A
DeÔ¨Ånitions of background
knowledge
In this appendix we discuss some properties of random measure which are mentioned
throughout this thesis.
DeÔ¨Ånition A.0.1 (Convergence of Random Measures) (Kallenberg, 1983) Let
E be a Polish space and B(E) be a Borel œÉ-algebra generated by the open sets in E.
A measure ¬µ is called Radon if ¬µ(K) < ‚àûfor any compact set K ‚ààE. Let M+(E)
be the space of Radon measures in E. Let M+(E) be the smallest œÉ-algebra of subsets
of M+(E) making the maps ¬µ ‚Üí¬µ(f) =
R
f(x)d¬µ(x) from M+(E) to R measurable
for all functions f ‚ààC+
K(E), where C+
K(E) denotes the set of continuous functions
f : E ‚Üí[0, ‚àû) with compact support. Note that, M+(E) is the Borel œÉ-algebra
generated by the topology of vague convergence. If ¬µn, ¬µ ‚ààM+(E), we say that (¬µn)n
converges vaguely to ¬µ, if ¬µn(f)
v‚Üí¬µ(f) for any f ‚ààC+
K(E).
A random measure on E is any measurable map Œæ deÔ¨Åned on a probability space
(‚Ñ¶, F, P) with values in (M+(E), M+(E)). If Œæn and Œæ are random measures on E,
we say that (Œæn)n converges in distribution to Œæ (we write Œæn
d‚ÜíŒæ) if {P ‚ó¶Œæ‚àí1
n }n
converges weakly to P ‚ó¶Œæ‚àí1. By Theorem 4.2 of Kallenberg (1983), Œæn
d‚ÜíŒæ if and
86

A. DeÔ¨Ånitions of background knowledge
87
only if Œæn(f) ‚ÜíŒæ(f), i.e.
Z
E
f(x)Œæn(dx) ‚Üí
Z
E
f(x)Œæ(dx),
‚àÄf ‚ààC+
K(E).
We say that (Œæn)n converges vaguely almost surely to Œæ (and we write Œæn
a.s.
‚ÜíŒæ) if there
exist a set e‚Ñ¶‚ààF with P(e‚Ñ¶) = 1 such that ‚àÄw‚Ä≤ine‚Ñ¶,
Œæn(w, ¬∑)
v‚ÜíŒæ(w, ¬∑), i.e.
Z
E
f(x)Œæn(w, dx) ‚Üí
Z
E
f(x)Œæ(w, dx),
‚àÄf ‚ààC+
K(E).
The space M+(E) endowed with the vague topology is a complete separable metric
space Resnick (1987). For more details about random measures refer to Kallenberg
(1983).

Bibliography
[1] Al Labadi, Luai, and Mahmoud Zarepour (2014). On simulations
from the two-parameter Poisson-Dirichlet process and the normalized inverse-
Gaussian process. Sankhya A 76.1, 158-176.
[2] Al Labadi, Luai (2012). On New Constructive Tools in Bayesian Nonpara-
metric Inference. PhD thesis, University of Ottawa .
[3] Labadi, Luai Al and Zarepour, Mahmoud (2015). On Approximations
of the Beta Process in Latent Feature Models. arXiv preprint arXiv:1411.3434.
[4] Al Labadi, Luai and Zarepour, Mahmoud and others (2013). On
asymptotic properties and almost sure approximation of the normalized inverse-
Gaussian process. Bayesian Analysis 8, 553‚Äì568.
[5] Abramowitz, Milton and Stegun, Irene A (1972) Handbook of Mathe-
matical Functions with Formulas, Graphs, and Mathematical Tables. National
Bureau of Standards Applied Mathematics Series 55. Tenth Printing. Dover
Publications, Mineola, New York.
[6] Banjevic, Dragan and Ishwaran, Hemant and Zarepour, Mahmoud
and others (2002). A recursive method for functionals of Poisson processes.
Bernoulli Society for Mathematical Statistics and Probability, Volume 8 295‚Äì
311.
88

BIBLIOGRAPHY
89
[7] Bert Fristedt and Lawrence Gray (1996). A Modern Approach to Prob-
ability Theory. Birkhauser Boston.
[8] Bondesson, Lennart (1982). On simulation from inÔ¨Ånitely divisible distri-
butions. Advances in Applied Probability 855‚Äì869.
[9] Broderick, Tamara and Jordan, Michael I and Pitman, Jim and
others (2012). Beta processes, stick-breaking and power laws. Bayesian anal-
ysis. International Society for Bayesian Analysis. Vol 7
439‚Äì476.
[10] Carlton,
Matthew Aaron (1999) Applications of the two-parameter
Poisson-Dirichlet distribution. PhD thesis, University of California, Los An-
geles
[11] Caron, Franc¬∏ois and Fox, Emily B (2014). Bayesian nonparametric mod-
els of sparse and exchangeable random graphs. arXiv preprint arXiv:1401.1137.
[12] Caron, Franc¬∏ois and Teh, Yee Whye and Murphy, Thomas Bren-
dan (2013). Bayesian nonparametric Plackett-Luce models for the analysis of
clustered ranked data. CoRR.
[13] Favaro, Stefano and Lijoi, Antonio and Pr¬®unster, Igor (2009).
On the stick-breaking representation of normalized inverse Gaussian priors.
Biometrika, 99 663774.
[14] Favaro, Stefano and Lijoi, Antonio and Mena, Rams¬¥es H and
Pr¬®unster, Igor (2009). Bayesian non-parametric inference for species vari-
ety with a two-parameter Poisson‚ÄìDirichlet process prior. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 71
993‚Äì1008.
[15] Ferguson, Thomas S (1973). A Bayesian analysis of some nonparametric
problems. The Annals of Statistics 1
209‚Äì230.

BIBLIOGRAPHY
90
[16] Ferguson, Thomas S and Klass, Michael J (1972). A representation of
independent increment processes without Gaussian components. The Annals of
Mathematical Statistics 1
209‚Äì230.
[17] Ghahramani, Zoubin and Griffiths, Thomas L (2005). InÔ¨Ånite latent
feature models and the Indian buÔ¨Äet process. Advances in Neural Information
Processing Systems 475‚Äì482.
[18] Hjort, Nils Lid (1990). Nonparametric Bayes estimators based on beta pro-
cesses in models for life history data. The Annals of Statistics, 18(3) 1259‚Äì1294.
[19] Ishwaran, Hemant and James, Lancelot F (2001). Gibbs sampling meth-
ods for stick-breaking priors. Journal of the American Statistical Association 96,
161‚Äì173.
[20] Ishwaran, Hemant and Zarepour, Mahmoud (2002). Exact and approx-
imate sum representations for the Dirichlet process. The Canadian Journal of
Statistics 30 269‚Äì283.
[21] Kallenberg, O (1983). Random Measures. Third edition. Akademie-Verlag,
Berlin.
[22] Kim, Yongdai (1999). Nonparametric Bayesian estimators for counting pro-
cesses. Annals of Statistics, JSTOR 562‚Äì588.
[23] Kingman, John Frank Charles (1992). Poisson processes. Oxford Univer-
sity Press, Volume 3.
[24] Kingman, John (1967). Completely random measures. PaciÔ¨Åc Journal of
Mathematics 21
59‚Äì78.

BIBLIOGRAPHY
91
[25] Lijoi, Antonio and Mena, Rams¬¥es H and Pr¬®unster, Igor (2005). Hi-
erarchical mixture modeling with normalized inverse-Gaussian priors. Journal
of the American Statistical Association, 100
1278‚Äì1291.
[26] Miller, Kurt (2011) A Bayesian Nonparametric Latent Feature Models. PhD
thesis, University of California, Berkeley.
[27] Miller, Kurt and Jordan, Michael I and Griffiths, Thomas L (2009)
Nonparametric latent feature models for link prediction. Advances in Neural
Information Processing Systems, 1276‚Äì1284.
[28] Muliere, Pietro and Tardella, Luca (1998). Approximating distribu-
tions of random functionals of Ferguson-Dirichlet priors. The Canadian Journal
of Statistics 26, 283‚Äì297.
[29] Nieto-Barajas, Luis E and Pr¬®unster, Igor (2009). A sensitivity analysis
for Bayesian nonparametric density estimators. Statistica Sinica, 19 685‚Äì705.
[30] Paisley, John and Carin, Lawrence (2009). Nonparametric factor analy-
sis with beta process priors. Proceedings of the 26th Annual International Con-
ference on Machine Learning 777‚Äì784.
[31] Paisley, John W and Zaas, Aimee K and Woods, Christopher W
and Ginsburg, Geoffrey S and Carin, Lawrence (2010). A stick-
breaking construction of the beta process. Proceedings of the 27th International
Conference on Machine Learning (ICML-10) 847‚Äì854.
[32] Pitman, Jim and Yor, Marc (1997). The two-parameter Poisson-Dirichlet
distribution derived from a stable subordinator. The Annals of Probability 855‚Äì
900.
[33] Resnick, Sidney I (1987). Extreme values, regular variation, and point pro-
cesses. Springer-Verlag, New York.

BIBLIOGRAPHY
92
[34] Robert L. Wolpert and Katja Ickstadt. (1998b). Simulation of L¬¥evy
random Ô¨Åels. Practical Nonparametric and Semiparametric Bayesian Statistics
Lecture Notes in Statistics, 133 227‚Äì242.
[35] Sethuraman, J. (1994). A Constructive DeÔ¨Ånition of Dirichlet Priors. Statis-
tica Sinica 4, 639‚Äì650.
[36] Teh, Yee Whye and Jordan, Michael I (2010). Hierarchical Bayesian
nonparametric models with applications. Bayesian nonparametrics, Camb. Ser.
Stat. Probab. Math .
[37] Teh, Yee Whye (2010). Dirichlet process. Encyclopedia of machine learning,
Springer 280‚Äì287.
[38] Thibaux, Romain Jean (2008). Nonparametric Bayesian models for machine
learning. Thesis
[39] Thibaux, Romain and Jordan, Michael I (2007). Hierarchical beta pro-
cesses and the Indian buÔ¨Äet process. International conference on artiÔ¨Åcial in-
telligence and statistics 564‚Äì571.
[40] Titsias, Michalis K (2008). The inÔ¨Ånite gamma-Poisson feature model. Ad-
vances in Neural Information Processing Systems, 1513‚Äì1520.
[41] Wolpert, Robert L and Ickstadt, Katja (1998). Simulation of L¬¥evy
random Ô¨Åelds. Practical Nonparametric and Semiparametric Bayesian Statis-
tics, Springer 227‚Äì242.
[42] Zarepour, Mahmoud and Al Labadi, Luai (2012). On a rapid simulation
of the Dirichlet process. Statistics & Probability Letters 82.5, 916‚Äì924.

