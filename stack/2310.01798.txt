LARGE LANGUAGE MODELS CANNOT SELF-CORRECT
REASONING YET
Jie Huang1,2,∗Xinyun Chen1 Swaroop Mishra1 Huaixiu Steven Zheng1 Adams Wei Yu1
Xinying Song1 Denny Zhou1
1Google DeepMind
2University of Illinois at Urbana-Champaign
ABSTRACT
Large Language Models (LLMs) have emerged as a groundbreaking technology
with their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of their
generated content. A contemporary methodology, self-correction, has been pro-
posed as a remedy to these issues. Building upon this premise, this paper critically
examines the role and efficacy of self-correction within LLMs, shedding light on
its true potential and limitations. Central to our investigation is the notion of in-
trinsic self-correction, whereby an LLM attempts to correct its initial responses
based solely on its inherent capabilities, without the crutch of external feedback.
In the context of reasoning, our research indicates that LLMs struggle to self-
correct their responses without external feedback, and at times, their performance
might even degrade post self-correction. Drawing from these insights, we offer
suggestions for future research and practical applications in this field.
1
INTRODUCTION
The rapid advancements in the domain of artificial intelligence have ushered in the era of Large
Language Models (LLMs). These models, characterized by their expansive parameter counts and
unparalleled capabilities in text generation, have showcased promising results across a multitude
of applications (OpenAI, 2023; Anil et al., 2023; Qin et al., 2023; Bubeck et al., 2023). However,
concerns about their accuracy, reasoning capabilities, and the safety of their generated content have
drawn significant attention from the community (Bang et al., 2023; Alkaissi & McFarlane, 2023;
Zheng et al., 2023; Shi et al., 2023; Carlini et al., 2021; Huang et al., 2022; Shao et al., 2023; Li
et al., 2023; Wei et al., 2023; Zhou et al., 2023b; Zou et al., 2023, inter alia).
Amidst this backdrop, the concept of “self-correction” has emerged as a promising solution, where
LLMs refine their responses based on feedback to their previous outputs (Madaan et al., 2023;
Welleck et al., 2023; Shinn et al., 2023; Kim et al., 2023; Bai et al., 2022; Ganguli et al., 2023;
Gao et al., 2023; Paul et al., 2023; Chen et al., 2023b; Pan et al., 2023, inter alia). However, the
underlying mechanics and efficacy of self-correction in LLMs remain underexplored. A fundamen-
tal question arises: If an LLM possesses the ability to self-correct, why doesn’t it simply offer the
correct answer in its initial attempt? This paper delves deeply into this paradox, critically examining
the self-correction capabilities of LLMs, with a particular emphasis on reasoning (Wei et al., 2022;
Zhou et al., 2023b; Huang & Chang, 2023).
To study this, we first define the concept of intrinsic self-correction, a scenario wherein the model
endeavors to rectify its initial responses based solely on its inherent capabilities, without the crutch
of external feedback. Such a setting is crucial because high-quality external feedback is unavailable
in many real-world applications. Moreover, it is vital to understand the intrinsic capabilities of
LLMs. Contrary to the optimism surrounding self-correction (Madaan et al., 2023; Kim et al., 2023;
Shinn et al., 2023; Pan et al., 2023, inter alia), our findings indicate that LLMs struggle to amend
their prior responses in this setting. In most instances, the performance post self-correction even
deteriorates. This observation is in contrast to prior research such as Kim et al. (2023); Shinn et al.
(2023). Upon closer examination, we observe that the improvements in these studies result from
∗Work
done
during
internship
at
Google
DeepMind.
Correspondence
to:
Jie
Huang
<jeffhj@illinois.edu>, Denny Zhou <dennyzhou@google.com>.
1
arXiv:2310.01798v1  [cs.CL]  3 Oct 2023

Large Language Models Cannot Self-Correct Reasoning Yet
using oracles to guide the self-correction process, and the improvements vanish when oracle labels
are not available. Transitioning to another facet of self-correction, we investigate the potential of
multi-agent debate (Du et al., 2023; Liang et al., 2023) as a means to improve reasoning. In this
method, multiple instances of an LLM critique each other’s responses. However, our results reveal
that its efficacy is no better than self-consistency (Wang et al., 2022) when considering an equivalent
number of responses, highlighting the limitations of such an approach.
On the other hand, while self-correction has limitations in enhancing reasoning, it does show
promising results in other tasks like altering the style or improving the appropriateness of responses
(Madaan et al., 2023; Bai et al., 2022; Ganguli et al., 2023). Thus, understanding the nuances and
identifying the underlying reasons is vital. To delve deeper, we perceive self-correction as a form of
post-hoc prompting, wherein the prompting involves the responses of LLMs. We assess scenarios
where self-correction is genuinely beneficial—specifically when it provides instructions or feedback
that standard prompting misses. Moreover, our analysis reveals that the enhancement attributed to
self-correction in certain tasks may stem from an ill-crafted initial instruction that is overshadowed
by a carefully-crafted feedback prompt. In such cases, integrating the feedback into the initial in-
struction or crafting a better initial prompt might yield better results and lower the inference cost.
In light of our findings, we provide insights into the nuances of LLMs’ self-correction capabilities,
urging the community to be skeptical of this idea and apply it with caution. Furthermore, we ini-
tiate discussions and encourage future research to delve into methods that can genuinely correct
reasoning, for instance, by incorporating external feedback.
2
BACKGROUND AND RELATED WORK
Broadly, the concept of self-correction can be traced back to the foundational principles of machine
learning and adaptive systems. Early work in neural networks was based on the iterative adjustment
of model parameters in response to prediction errors (Rumelhart et al., 1986; LeCun et al., 1998)—a
process that can be considered as rectifying errors using feedback from training data.
The advent of reinforcement learning introduced another dimension of self-correction.
In this
paradigm, agents optimize their strategies through interactions with their environment, perpetually
modifying their policies based on received rewards or penalties (Sutton & Barto, 2018). Expand-
ing upon this, a notable advancement for LLMs was the incorporation of Reinforcement Learning
from Human Feedback (RLHF) (Ouyang et al., 2022). Rather than solely relying on environmen-
tal feedback, RLHF incorporates human insights, addressing challenges in crafting precise reward
functions and mitigating unintended behaviors. Nonetheless, this feedback often lacks immediacy
as it traditionally integrates during training phases rather than in real-time. Furthermore, corrections
might not necessarily be reflected in specific queries during inference.
With LLMs’ evolution, the notion of inference-time self-correction gained prominence. The dis-
course on self-correction now pivots around whether these advanced models can recognize the ap-
propriateness or accuracy of their outputs and, if needed, provide refined answers (Bai et al., 2022;
Madaan et al., 2023; Welleck et al., 2023, inter alia). For example, in the context of mathematical
reasoning, an LLM might initially solve a complex problem but make an error in one of the calcu-
lation steps. In an ideal self-correction scenario, the model is expected to recognize the potential
mistake, revisit the problem, correct the error, and consequently produce a more accurate solution.
Yet, the definition of “self-correction” in this context varies across the literature, leading to ambigu-
ity. As the field evolves, various terms like “self-refine”, “self-critique”, “self-improve”, and others
emerge, each with its own specific context. A pivotal distinction lies in the source of feedback (Pan
et al., 2023): Is it purely internal, originating solely from the LLM, or does it draw from external
inputs? Internal feedback relies on the model’s inherent knowledge and parameters to reassess its
outputs. In contrast, external feedback incorporates inputs from humans, other models (Wang et al.,
2023; Paul et al., 2023, inter alia), or external tools and knowledge sources (Gou et al., 2023; Chen
et al., 2023b; Olausson et al., 2023; Gao et al., 2023, inter alia).
Given that high-quality external feedback is often unavailable—and acknowledging its evident ad-
vantages—we channel our investigation towards whether LLMs possess the inherent capability to
rectify their responses. Such an investigation is also essential for understanding the capabilities of
LLMs. Consequently, we focus on self-correction without any external or human feedback. We
2

Large Language Models Cannot Self-Correct Reasoning Yet
term this setting intrinsic self-correction. For brevity, unless explicitly stated otherwise (e.g., self-
correction with oracle feedback), all references to “self-correction” in the remainder of this paper
pertain to intrinsic self-correction.
3
CAN LARGE LANGUAGE MODELS SELF-CORRECT REASONING?
Reasoning is foundational to human cognition; it is our mechanism for making sense of the world,
drawing inferences, making decisions, and solving problems. For LLMs to be truly effective and
reliable tools, especially in critical applications such as decision-making, education, or healthcare,
they must exhibit robust reasoning capabilities (Huang & Chang, 2023).
With this in mind, we center our investigation on a pivotal query: Can large language models
self-correct their reasoning? To address this, we delve into existing research, carry out rigorous
experiments, and present comprehensive analyses.
3.1
SELF-CORRECTION WITH ORACLE FEEDBACK
In this section, we attempt to employ existing self-correction methods, adopting their setting (using
labels to guide the self-correction process), to examine their effectiveness in enhancing the perfor-
mance of LLMs on reasoning tasks.
3.1.1
EXPERIMENTAL SETUP
Benchmarks. We focus on diverse datasets that have shown significant improvements through self-
correction in the existing literature, including
• GSM8K (Cobbe et al., 2021): GSM8K comprises a test set of 1,319 linguistically diverse grade
school math word problems, curated by human problem writers. There is a notable improvement
of approximately 7% as evidenced by Kim et al. (2023) post self-correction.
• CommonSenseQA (Talmor et al., 2019): This dataset offers a collection of multi-choice questions
that test commonsense reasoning. An impressive increase of around 15% is showcased through
the self-correction process, as demonstrated by Kim et al. (2023). Following Kojima et al. (2022);
Kim et al. (2023), we utilize the dev set for our evaluation, which encompasses 1,221 questions.
• HotpotQA (Yang et al., 2018): HotpotQA is an open-domain multi-hop question answering
dataset.
Shinn et al. (2023) demonstrate significant performance improvement through self-
correction. We test models’ performance in a closed-book setting and evaluate them using the
same set as Shinn et al. (2023). This set contains 100 questions, with exact match serving as the
evaluation metric.
Prompts. Following Kim et al. (2023); Shinn et al. (2023), we apply a three-step prompting strategy
for self-correction: 1) prompt the model to perform an initial generation (which also serves as the
results for Standard Prompting); 2) prompt the model to review its previous generation and produce
feedback; 3) prompt the model to answer the original question again with the feedback.
For our experiments, we mostly adhere to the prompts from the source papers. For GSM8K and
CommonSenseQA, we integrate format instructions into the prompts of Kim et al. (2023) to facilitate
a more precise automatic evaluation (detailed prompts can be found in Appendix A). For HotpotQA,
we use the same prompt as Shinn et al. (2023).
Test Models. Our primary testing is conducted on GPT-3.5-Turbo, the model examined in Kim et al.
(2023); Shinn et al. (2023). Our goal is to reproduce the experiments described in the original papers.
However, the papers do not specify which version was used1. We opt for gpt-3.5-turbo-0613,
as we observed that the 0301 version almost always believes its initial response is correct, particu-
larly with the prompt in Kim et al. (2023). We also conduct tests on GPT-4, accessed on 2023/08/29,
aiming to test the self-correction capabilities of the latest and most powerful iteration of OpenAI’s
model. For GPT-3.5, we employ the full evaluation set mentioned earlier. For GPT-4, to reduce the
cost, we randomly sampled 200 questions for each dataset (100 for HotpotQA) for testing.
1The official implementation uses the continuously updated gpt-3.5-turbo kernel from OpenAI.
3

Large Language Models Cannot Self-Correct Reasoning Yet
Table 1: Results of GPT-3.5 and GPT-4 on reasoning benchmarks with the setting in Section 3.1.1.
GSM8K
CommonSenseQA
HotpotQA
GPT-3.5
Standard Prompting
75.9
75.8
26.0
Self-Correct (Oracle)
84.3
89.7
29.0
GPT-4
Standard Prompting
95.5
82.0
49.0
Self-Correct (Oracle)
97.5
85.5
59.0
Other Setup. We prompt the models to undergo a maximum of two rounds of self-correction, using
the default temperature (1.). Following Kim et al. (2023); Shinn et al. (2023); Welleck et al. (2023),
we use the correct label to determine when to stop the self-correction loop.
3.1.2
RESULTS
Table 1 summarizes the results. From these results, we observe significant performance improve-
ments, consistent with the findings presented in Kim et al. (2023); Shinn et al. (2023).
3.1.3
REFLECTION
Table 2: Comparison of Self-Correct (Oracle) with a
Random Baseline.
CommonSenseQA
GPT-3.5
GPT-4
Standard Prompting
75.8
82.0
Self-Correct (Oracle, round 2)
89.7
85.5
Random (Oracle, round 2)
87.9
91.0
Random (Oracle, round 4)
100
100
While we do not leverage any external
sources or tools in our experiments, we
follow previous works in using the correct
label to determine when to stop the self-
correction loop. In a realistic setting, es-
pecially when aiming to employ LLMs to
solve math problems, the correct answer
is unknown to us. As a result, the perfor-
mance improvement requires a more care-
ful consideration.
To substantiate this perspective, we design a baseline predicated on random guessing. In this base-
line, we continue to employ the correct label to determine when to stop; however, the corrective
action is not undertaken by LLMs but is derived from random guessing from the remaining op-
tions. CommonSenseQA, being a multiple-choice question dataset, presents five option candidates
for each question. If the accuracy of generation in round k (where the initial generation is round 0) is
denoted as x, the expected accuracy for the subsequent generation becomes x+(1−x)/(5−k). Ta-
ble 2 tabulates the results of this random baseline. After 2 rounds, its performance is on par with or
even better than self-correction, and after 4 rounds, it achieves a 100% accuracy. Yet, it is manifestly
clear that such a random baseline cannot be regarded as an efficacious correction methodology.2
Nevertheless, results achieved with the use of labels might still function as an oracle, indicating the
presence of a perfect verifier that can adjudicate the correctness of an answer. In tasks such as code
generation, this is feasible since we can utilize an executor and unit tests to ascertain if the generated
code operates successfully (Chen et al., 2023b). However, for reasoning tasks, like solving mathe-
matical problems, this setting seems counter-intuitive. If we are already in possession of the ground
truth, there seems to be little reason to deploy LLMs for problem-solving. This setting can, how-
ever, find relevance in scenarios where the goal is to understand the reasoning process—situations
where the answer might be known, but there is a desire to decipher the detailed reasoning or explore
divergent solutions.
3.2
INTRINSIC SELF-CORRECTION
Per the discussions in Section 3.1.3, since the idea that LLMs can self-correct their reasoning is
not supported by the evidence so far, we turn our focus to the results in the intrinsic self-correction
2For GSM8K, a similar random baseline might not exist, but the underlying rationale remains the same.
Additionally, we can design a baseline, for example, by generating a random number each time. After a
significant number of rounds, it may reach the correct answer, but such a kind of improvement is apparently
not meaningful. A more direct justification is: If we already know the answer, why do we need to do this?
4

Large Language Models Cannot Self-Correct Reasoning Yet
Table 3: Results of GPT-3.5 and GPT-4 on reasoning benchmarks with intrinsic self-correction.
# calls
GSM8K
CommonSenseQA
HotpotQA
GPT-3.5
Standard Prompting
1
75.9
75.8
26.0
Self-Correct (round 1)
3
75.1
38.1
25.0
Self-Correct (round 2)
5
74.7
41.8
25.0
GPT-4
Standard Prompting
1
95.5
82.0
49.0
Self-Correct (round 1)
3
91.5
79.5
49.0
Self-Correct (round 2)
5
89.0
80.0
43.0
May refer to Table 6 of Appendix B for results with different feedback prompts for GSM8K.
The results are consistent, and the variance is low across different feedback prompts.
GSM8K
No Change
74.7%
Incorrect  Incorrect
8.9%
Incorrect  Correct
7.6%
Correct 
 Incorrect
8.8%
CommonSenseQA
No Change
42.8%
Incorrect  Incorrect
11.6%
Incorrect  Correct
5.8%
Correct 
 Incorrect
39.8%
Figure 1: Analysis of the changes in answers after two rounds of self-correction. No Change: The
answer remains unchanged; Correct ⇒Incorrect: A correct answer is changed to an incorrect one;
Incorrect ⇒Correct: An incorrect answer is revised to a correct one; Incorrect ⇒Incorrect: An
incorrect answer is altered but remains incorrect.
setting as defined in Section 2. To achieve this, we simply remove the use of labels to determine
when to stop and evaluate the performance with two rounds of self-correction.
Table 3 reports the accuracies and the number of model calls. We observe that, after self-correction,
the model’s performance drops on all benchmarks.
3.2.1
WHY DOES THE PERFORMANCE NOT INCREASE, BUT INSTEAD DECREASE?
Empirical Analysis. Figure 1 summarizes the results of changes in answers after two rounds of
self-correction using GPT-3.5, with two examples illustrated in Figure 2. For GSM8K, 74.7% of
the time, the model retains its initial answer. Among the remaining instances, the model is more
likely to modify a correct answer to an incorrect one than to revise an incorrect answer to a correct
one. For CommonSenseQA, there is a higher chance that GPT-3.5 alters its answer. The primary
reason for this is that false answer options in CommonSenseQA often appear somewhat relevant to
the question, and using the self-correction prompt might bias the model to choose another option,
leading to a high “correct ⇒incorrect” ratio.3
Let’s take another look at the results presented in Table 1. These results use ground-truth labels to
prevent the model from altering a correct answer to an incorrect one. However, determining how to
prevent such mischanges is, in fact, the key to ensuring the success of self-correction.
Intuitive Explanation. If the model is well-aligned and paired with a thoughtfully designed initial
prompt, the initial response should already be optimal given the conditions of the prompt and the
3We omit the analysis on HotpotQA because the sample size used in the source paper is quite small, which
may not produce meaningful statistics.
5

Large Language Models Cannot Self-Correct Reasoning Yet
Terry eats 2 yogurts a day. They are currently on sale at 
4 yogurts for $5.00. How much does he spend on 
yogurt over 30 days?
... So, Terry spends $\boxed{75.00}$ on yogurt over 30 
days. (Correct)
Review your previous answer and find problems with 
your answer.
Upon reviewing my previous answer, I realized that I 
made a mistake in calculating Terry's spending on 
yogurt over 30 days. I incorrectly stated that Terry 
spends $2.50 per day for 30 days, resulting in a total of 
$75.00…
Based on the problems you found, improve your 
answer.
... Therefore, the final answer is Terry spends 
$\boxed{37.50}$ on yogurt over 30 days. (Incorrect)
Christina is planning a birthday party and needs .75 gift 
bags per invited guest, because 1/4 of attendees don't 
show up. She invited 16 friends. Gift bags are $2 each. 
How much will she spend?
... Therefore, Christina will spend $18 on gift bags. 
Answer: \boxed{18}. (Incorrect)
Review your previous answer and find problems with 
your answer.
Upon reviewing my previous answer, I recognize a 
mistake. I incorrectly stated that Christina needs 0.75 
gift bags per invited guest. The given information states 
that 1/4 of the attendees won't show up…
Based on the problems you found, improve your 
answer.
... Therefore, Christina will spend $24 on gift bags. 
Answer: \boxed{24}. (Correct)
Figure 2: Examples on GSM8K (Cobbe et al., 2021). Left: successful self-correction; Right: failed
self-correction. Full prompts and responses can be viewed in Figures 3 and 4 of Appendix A.
specific decoding algorithm. Introducing feedback can be viewed as adding an additional prompt,
potentially skewing the model towards generating a response that is tailored to this combined input.
In an intrinsic self-correction setting, on the reasoning tasks, this supplementary prompt may not
offer any extra advantage for answering the question. In fact, it might even bias the model away
from producing an optimal response to the initial prompt, resulting in a decrease in performance.
3.2.2
RESULTS OF OTHER PROMPTS AND RELATED STUDIES
One might wonder, is the self-correction prompt we tested suboptimal? Could other prompts lead
to an improvement in performance? The answer is: it is entirely plausible that we could identify a
prompt that enhances model performance on specific benchmarks. However, this no longer aligns
with the intrinsic self-correction setting discussed in this paper, similar to the discussion of true
few-shot setting (Perez et al., 2021). Such a search essentially leverages feedback from humans or
training examples. Furthermore, the same strategy can also be effectively applied to optimize the
initial prompts (Zhou et al., 2022; Yang et al., 2023), possibly achieving better performance without
necessitating additional model calls for self-correction.4 In Appendix B, we test different prompts
but find that the performance still does not improve.
Moreover, we are not the first to observe that self-correction does not necessarily improve reasoning
of LLMs. For instance, Madaan et al. (2023) demonstrate the promising potential of self-correction
across various tasks, yet mathematical reasoning shows negligible improvement. Other studies, such
as those by Gou et al. (2023) and Zhou et al. (2023a), which incorporate external feedback or tools,
find that self-correction without such feedback is a relatively weak baseline.
To conclude, our focus is not on addressing a question like, “Is there a self-correction prompt that
can bolster performance on specific benchmarks?” Such a query may not be particularly meaningful.
Instead, we are aiming to tackle a more foundational issue: “Are large language models really able
to self-correct their reasoning based solely on their inherent capabilities?”
3.3
MULTI-AGENT DEBATE AND SELF-CONSISTENCY
In examining the literature, we find that another potential approach for LLMs to self-correct their
answers involves allowing the models to critique and debate through multiple model calls (Du et al.,
4We delve deeper into this topic in Section 4.
6

Large Language Models Cannot Self-Correct Reasoning Yet
2023; Liang et al., 2023; Chen et al., 2023a). Du et al. (2023) implement a multi-agent debate
method by leveraging multiple instances of a single ChatGPT model and demonstrate significant
improvements on reasoning tasks. We adopt their method to test performance on GSM8K. For an
unbiased implementation, we use the exact same prompt as Du et al. (2023) and replicate their ex-
periment with the gpt-3.5-turbo-0301 model, incorporating 3 agents and 2 rounds of debate.
The only distinction is that, to reduce result variance, we test on the complete test set of GSM8K,
compared to their usage of 100 examples. For reference, we also report the results of self-consistency
(Wang et al., 2022), which prompts models to generate multiple responses and performs majority
voting to select the final answer.
Table 4:
Results of multi-agent debate and self-
consistency.
# responses
GSM8K
Standard Prompting
1
76.7
Self-Consistency
3
82.5
Multi-Agent Debate (round 1)
6
83.2
Self-Consistency
6
85.3
Multi-Agent Debate (round 2)
9
83.0
Self-Consistency
9
88.2
Table 4 presents the results5.
The
results indicate that both multi-agent
debate and self-consistency achieve
significant improvements over stan-
dard prompting.
However,
when
comparing multi-agent debate to self-
consistency, we observe that the per-
formance of multi-agent is only slightly
better than that of self-consistency with
the same number of agents (3 re-
sponses, the baseline also compared in
Du et al. (2023)). Furthermore, for self-
consistency with an equivalent number of responses, multi-agent debate significantly underperforms
simple self-consistency using majority voting.
In fact, rather than labeling the multi-agent debate as a form of “debate” or “critique”, it is more
appropriate to perceive it as a means to achieve “consistency” across multiple model generations.
Fundamentally, its concept mirrors that of self-consistency; the distinction lies in the voting mech-
anism, whether voting is model-driven or purely based on counts. The observed improvement is
evidently not attributed to “self-correction”, but rather to “self-consistency”. If we aim to argue that
LLMs can self-correct, it is preferable to exclude the effects of selection among multiple generations.
4
SELF-CORRECTION AS POST-HOC PROMPTING
Table 5: Results of Constrained Generation.
# calls CommonGen-Hard
Standard Prompting*
1
44.0*
Self-Correct*
7
67.0*
Standard Prompting*
1
53.0
Self-Correct*
7
61.1
Standard Prompting (ours)
1
81.8
Self-Correct*
7
75.1
* Prompts and results from Madaan et al. (2023).
In the preceding section, we observed
that
LLMs
face
challenges
in
self-
correcting their reasoning. Yet, there are
cases where self-correction has yielded
impressive results, as evidenced by stud-
ies presented in Madaan et al. (2023);
Bai et al. (2022); Ganguli et al. (2023).
Therefore, it is crucial to discern the
differences and pinpoint the underlying
causes.
To unravel this, it is important to grasp
the fundamental nature of self-correction. Based on its form, self-correction can be viewed as a type
of post-hoc prompting. The difference between it and standard prompting (which we term pre-hoc
prompting here) is that the prompting is conducted on top of the responses of LLMs. And we refer
to the process of improving such prompts as post-hoc prompt engineering.
Therefore, scenarios in which self-correction enhances model responses occur when it can provide
valuable instruction or feedback that pre-hoc prompting cannot. For instance, when the goal is to
make the response safer, it might be challenging to instruct a model to generate completely risk-free
responses in its first attempt using only pre-hoc prompting. In such situations, self-correction can
serve as a means to enhance the safety of responses through a fine-grained post-hoc examination
(Bai et al., 2022; Ganguli et al., 2023). However, for reasoning tasks, this may not be the case. A
5We denote the initial round as round 0, whereas Du et al. (2023) refers to it as round 1. The standard
deviation for Standard Prompting over 9 runs is 0.91.
7

Large Language Models Cannot Self-Correct Reasoning Yet
feedback prompt such as “Review your previous answer and find problems with your answer.” does
not necessarily provide tangible benefits for reasoning.
Furthermore, even when a significant performance improvement post self-correction is observed,
careful consideration of prompt design is essential. For example, if an response needs to meet
criteria that can be easily specified in the initial instruction (e.g., the output should contain certain
words, the generated code need to be highly efficient, the sentiment should be strongly negative),
instead of feeding these requirements as feedback in the post-hoc prompt, a more cost-effective
alternative strategy is to embed these requirements directly (and explicitly) into the pre-hoc prompt.
Taking the Constrained Generation (Generative Commonsense Reasoning) task in Madaan
et al. (2023) as an example, where the task involves generating coherent sentences us-
ing 20-30 input concepts:
instead of asking the model to identify missing concepts
and then guiding it to incorporate these concepts through feedback, we can simply in-
struct the model to include all the concepts from the outset—a requirement is not explic-
itly embedded in the pre-hoc prompt of Madaan et al. (2023) (refer to Figure 7 for the
prompt).
Based on this insight, we create a straightforward baseline by designing the
prompt as, e.g., “Concept: ['vest', ... , 'serve', 'winter']\n\nWrite a
reasonable paragraph that includes *ALL* of the above concepts.”6
Following Madaan et al. (2023), we use concept coverage as the metric. Since the exact version
of the model used in Madaan et al. (2023) is not detailed, we reference their results and attempt to
replicate their experiments using gpt-3.5-turbo-0613. We also experiment with their self-
correction approach to refine the outputs produced with our designed prompt. The results in Table 5
indicate the prompt (causally) crafted by us, “Standard Prompting (ours)”, outperforms the post-
self-correction results of Madaan et al. (2023). Additionally, when we employ their prompt to refine
our outputs, performance even diminishes.7
Again, our intention here is not to debate whether a post-hoc prompt exists that can outperform
the prompt we wrote casually. Our main objective is to encourage a more critical examination of
self-correction experiments. It is meaningless to employ a well-crafted post-hoc prompt to guide the
model in “self-correcting” a response generated through a poorly constructed pre-hoc prompt. For
a fair comparison, equal effort should be invested in both pre-hoc and post-hoc prompting.
Considering the substantial token costs associated with self-correction, researchers and developers
should deliberate on its real benefits compared to what can be achieved with a pre-hoc prompt.
5
DISCUSSION
Self-correction may still be beneficial for aligning responses with certain preferences. First, it
is important to reiterate that we are not claiming self-correction is useless. Self-correction can be
effectively employed to make responses align with specific preferences, such as altering the style
of responses or enhancing their safety (Bai et al., 2022; Ganguli et al., 2023; Madaan et al., 2023).
However, self-correction might not be effective when attempting to correct responses for tasks in
which LLMs find it challenging to identify errors in their initial responses or assess the correctness
of those responses. For instance, in the reasoning tasks studied in this paper, we did not observe any
improvement through self-correction.
Leveraging external feedback for correction.
In this paper, we focus on the intrinsic self-
correction setting.
However, when we leverage external feedback for correction, the narrative
changes. For instance, in the study by Gou et al. (2023), it is demonstrated that LLMs, when
interacting with various external tools such as search engines and calculators, can more effectively
verify and correct their responses. Chen et al. (2023b); Olausson et al. (2023); Pan et al. (2023)
show that by using an executor and unit tests to ascertain the successful operation of generated code,
LLMs can improve their generation and better solve reasoning tasks. Cobbe et al. (2021); Lightman
et al. (2023); Wang et al. (2023) train a verifier or a critique model on a high-quality dataset to verify
6The prompt was casually written by the author without any optimization, symbolizing a “true zero-shot
setting” (Perez et al., 2021).
7Besides, numerous outputs fail (e.g., formatting issues) when applying Madaan et al. (2023)’s prompt,
necessitating many regenerations—another potential drawback of self-correction.
8

Large Language Models Cannot Self-Correct Reasoning Yet
or refine LLMs’ outputs. Indeed, in everyday usage scenarios, when interacting with LLMs, we
often provide feedback, guiding them to produce the content we desire. Utilizing this type of feed-
back, though not perpetually accessible, to assist LLMs in correcting their responses is intuitively
beneficial, particularly when the feedback is of high quality.
Employing self-consistency as a method of self-verification. From the results in Table 1, we
observe that the oracle setting yields much higher accuracy. This suggests that within the LLM’s
search space, a correct answer might exist. If we could leverage a robust verification process to
guide the LLMs towards the right direction or steer them away from incorrect paths, outcomes could
be enhanced. One approach to achieve this is through external feedback, as previously discussed.
Another method is by leveraging self-consistency (Wang et al., 2022; Kadavath et al., 2022), as
explored in Section 3.3. An example of this approach is Tree of Thought (Yao et al., 2023), wherein
reasoning of LLMs is enhanced through step-wise verification paired with self-consistency.
Pre-hoc vs post-hoc prompting. As discussed in Section 4, the latency and cost of post-hoc prompt-
ing are higher than those of pre-hoc prompting. Therefore, if the “feedback” can be encoded in the
initial instruction, pre-hoc prompting is the more advantageous choice. Moreover, it is preferable to
place greater emphasis on pre-hoc prompt engineering than on post-hoc prompt engineering. How-
ever, in cases where we wish to leverage external feedback that must be given in conjunction with
responses, post-hoc prompting becomes a more suitable option.
Guidelines for comparison. Lastly, we would like to offer some guidelines for comparison regard-
ing self-correction. First, when comparing self-correction methods to other baselines, it is important
to report the inference cost, e.g., number of calls or tokens. Additionally, it is advisable to include
self-consistency with the same number of calls/responses as a baseline. Second, avoid using an
ill-designed pre-hoc prompt while crafting a carefully designed post-hoc prompt for improvement.
Third, when the methods leverage external feedback, it is essential to consider whether the feedback
is reasonable. Calling external tools automatically or receiving feedback from the environment can
be considered reasonable; however, using ground-truth labels to judge whether a response is correct
can only be regarded as an oracle.
6
CONCLUSION
Our research shows that LLMs are not yet capable of self-correcting their reasoning. This implies
that expecting these models to inherently recognize and rectify their inaccuracies might be overly
optimistic, at least with the current state of technology. More broadly, this underscores the need for
ongoing improvement and a multi-faceted approach when deploying LLMs in real-world applica-
tions. In light of these findings, it is imperative for researchers and practitioners to approach the
concept of self-correction with a discerning perspective, acknowledging its potential and recogniz-
ing its boundaries. By doing so, we can better equip this technique to address the limitations of
LLMs, steering their evolution towards enhanced accuracy and reliability.
7
LIMITATIONS AND BROADER IMPACT
Although our exploration spans a variety of self-correction strategies, prompts, and tasks, as we
mentioned earlier, it is plausible that there exist specific prompts or strategies that could enhance
the reasoning performance of models for particular benchmarks. However, searching such prompts
or strategies may inadvertently rely on external feedback, either from human insights or training
data. Additionally, such search efforts can be equally applied to the pre-hoc prompts (Zhou et al.,
2022; Yang et al., 2023). A potential outcome of our study might be that it discourages research
into self-correction for certain applications. Nonetheless, we are optimistic that our findings will
steer future research towards more practical applications of self-correction, especially those where
tangible benefits can be derived, potentially by leveraging external feedback.
Furthermore, several related works have already presented findings consistent with our observation:
that self-correction, when devoid of external feedback, serves as a relatively weak baseline (Gou
et al., 2023; Zhou et al., 2023a). Despite this, we have identified a prevailing ambiguity in the wider
community, with even domain experts being uncertain about the intricacies of when and how self-
correction operates. Some existing literature may inadvertently contribute to this confusion, either
9

Large Language Models Cannot Self-Correct Reasoning Yet
by relegating crucial details about label usage to less prominent sections or by failing to clarify
that their designed self-correction strategies actually incorporate external feedback. Our intention
in this paper is to amplify these concerns and offer a comprehensive overview of the state of “self-
correction” in LLMs. The title, “Large Language Models Cannot Self-Correct Reasoning Yet”, is
not an outright dismissal of self-correction techniques. Instead, it serves as a call to action, urging
researchers to approach this domain with a discerning and critical perspective. Furthermore, it
encourages future research to explore approaches that can genuinely enhance reasoning. For the
sake of clarity, we recommend omitting the term “self” from concepts like self-correction or self-
critique when the context does not strictly adhere to an intrinsic self-correction setting and adopting
less confusing terms like “correction with external tools/feedback”.
Finally, in light of our findings that LLMs struggle to self-correct reasoning based purely on their
inherent capabilities, we urge a more circumspect view on any unwarranted optimism (or fear)
regarding the autonomous evolution of LLMs (and AI systems) through self-improvement.
REFERENCES
Hussam Alkaissi and Samy I McFarlane. Artificial hallucinations in chatgpt: implications in scien-
tific writing. Cureus, 15(2), 2023.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.
arXiv preprint arXiv:2305.10403, 2023.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-
lessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia,
Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of
chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023.
S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-
mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data
from large language models. In USENIX Security Symposium, volume 6, 2021.
Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference
improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023a.
Xinyun Chen, Maxwell Lin, Nathanael Sch¨arli, and Denny Zhou. Teaching large language models
to self-debug. arXiv preprint arXiv:2304.05128, 2023b.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch.
Improv-
ing factuality and reasoning in language models through multiagent debate.
arXiv preprint
arXiv:2305.14325, 2023.
Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil˙e Lukoˇsi¯ut˙e, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for
moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan,
Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what
language models say, using language models. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477–16508, 2023.
10

Large Language Models Cannot Self-Correct Reasoning Yet
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint
arXiv:2305.11738, 2023.
Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A sur-
vey. In Findings of the Association for Computational Linguistics: ACL 2023. Association for
Computational Linguistics, 2023.
Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models
leaking your personal information? In Findings of the Association for Computational Linguis-
tics: EMNLP 2022, pp. 2038–2047, Abu Dhabi, United Arab Emirates, 2022. Association for
Computational Linguistics.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,
Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language mod-
els (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.
Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.
The ICML Workshop on Artificial Intelligence & Human Computer Interaction, 2023.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. Advances in neural information processing systems,
35:22199–22213, 2022.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-step
jailbreaking privacy attacks on chatgpt. ArXiv preprint, abs/2304.05197, 2023.
Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng
Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-
agent debate. arXiv preprint arXiv:2305.19118, 2023.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
arXiv:2305.20050, 2023.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement
with self-feedback. Advances in Neural Information Processing Systems, 2023.
Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-
Lezama. Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896,
2023.
OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback.
Advances in Neural Information Processing Systems, 35:
27730–27744, 2022.
Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang
Wang. Automatically correcting large language models: Surveying the landscape of diverse self-
correction strategies. arXiv preprint arXiv:2308.03188, 2023.
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West,
and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint
arXiv:2304.01904, 2023.
Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models.
Advances in neural information processing systems, 34:11054–11070, 2021.
11

Large Language Models Cannot Self-Correct Reasoning Yet
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi
Yang. Is chatgpt a general-purpose natural language processing task solver?
arXiv preprint
arXiv:2302.06476, 2023.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-
propagating errors. nature, 323(6088):533–536, 1986.
Hanyin Shao, Jie Huang, Shen Zheng, and Kevin Chen-Chuan Chang. Quantifying association
capabilities of large language models and its implications on privacy leakage. arXiv preprint
arXiv:2305.12707, 2023.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael
Sch¨arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.
In International Conference on Machine Learning, pp. 31210–31227. PMLR, 2023.
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu
Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Infor-
mation Processing Systems, 2023.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question
answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, 2019.
Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu,
Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A
critic for language model generation. arXiv preprint arXiv:2308.04592, 2023.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. In The Eleventh International Conference on Learning Representations, 2022.
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training
fail? arXiv preprint arXiv:2307.02483, 2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
Neural Information Processing Systems, 35:24824–24837, 2022.
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin
Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference
on Learning Representations, 2023.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun
Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,
and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question
answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, 2018.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv
preprint arXiv:2305.10601, 2023.
Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. Why does chatgpt fall short in providing
truthful answers. ArXiv preprint, abs/2304.10513, 2023.
Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,
Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code
interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a.
12

Large Language Models Cannot Self-Correct Reasoning Yet
Denny Zhou, Nathanael Sch¨arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-
mans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex
reasoning in large language models. In The Eleventh International Conference on Learning Rep-
resentations, 2023b.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and
Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh Interna-
tional Conference on Learning Representations, 2022.
Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial
attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.
13

Large Language Models Cannot Self-Correct Reasoning Yet
A
PROMPTS AND EXAMPLE OUTPUTS
Can you solve the following math problem? Christina is planning a
birthday party and needs .75 gift bags per invited guest, because 1/4 of
attendees don't show up. She invited 16 friends. Gift bags are $2 each.
How much will she spend? Explain your reasoning. Your final answer should
be a single numerical number, in the form \boxed{answer}, at the end of
your response.
If Christina invited 16 friends, then 1/4 of them won't show up, which
means there will be 16 * 1/4 = 4 friends who won't attend the party. To
determine how many gift bags to buy, we need to subtract the number of
friends who won't show up from the total number of invited guests. The
number of invited guests is 16 - 4 = 12. Since Christina needs 0.75 gift
bags per invited guest, we can multiply this quantity by the number of
invited guests to find the total number of gift bags needed, which is
0.75 * 12 = 9 gift bags. If each gift bag is $2, then Christina will
spend a total of 2 * 9 = $<<2*9=18>>18 on gift bags. Therefore, Christina
will spend $18 on gift bags. Answer: \boxed{18}. (Incorrect)
Review your previous answer and find problems with your answer.
Upon reviewing my previous answer, I recognize a mistake. I incorrectly
stated that Christina needs 0.75 gift bags per invited guest. The given
information states that 1/4 of the attendees won't show up, so the
correct calculation should take this into account. To calculate the
number of gift bags needed, we will multiply the number of invited guests
by the ratio of attendees who will actually show up (75%). The number of
invited guests is 16, and if 1/4 of them won't show up, then the ratio of
attendees who will show up is 1 - 1/4 = 3/4. Multiplying the number of
invited guests by the ratio of attendees who will show up gives us: 16 *
(3/4) = 12 gift bags needed. Since each gift bag costs $2, the total
amount Christina will spend on gift bags is: 12 * $2 = $<<12*2=24>>24.
Based on the problems you found, improve your answer. Please reiterate
your answer, with your final answer a single numerical number, in the
form \boxed{answer}.
Upon reviewing my previous answer, I recognize a mistake. I incorrectly
stated that Christina needs 0.75 gift bags per invited guest. The given
information states that 1/4 of the attendees won't show up, so the
correct calculation should take this into account. To calculate the
number of gift bags needed, we will multiply the number of invited guests
by the ratio of attendees who will actually show up (75%). The number of
invited guests is 16, and if 1/4 of them won't show up, then the ratio of
attendees who will show up is 1 - 1/4 = 3/4. Multiplying the number of
invited guests by the ratio of attendees who will show up gives us: 16 *
(3/4) = 12 gift bags needed. Since each gift bag costs $2, the total
amount Christina will spend on gift bags is: 12 * $2 = $<<12*2=24>>24.
Therefore, Christina will spend $24 on gift bags. Answer: \boxed{24}.
(Correct)
Figure 3: Example on GSM8K where self-correction changes an incorrect answer to a correct one.
Text in bold denotes user prompts.
14

Large Language Models Cannot Self-Correct Reasoning Yet
Can you solve the following math problem? Terry eats 2 yogurts a day.
They are currently on sale at 4 yogurts for $5.00. How much does he spend
on yogurt over 30 days? Explain your reasoning. Your final answer should
be a single numerical number, in the form \boxed{answer}, at the end of
your response.
First, we need to calculate how many sets of 4 yogurts Terry buys in 30
days. Since Terry eats 2 yogurts a day, he will need 2/4 = 0.5 sets of 4
yogurts per day. Next, we multiply the number of sets by the price of
each set to calculate how much Terry spends per day. Each set of 4
yogurts costs $5.00, so Terry spends 0.5 * $5.00 = $<<0.5*5=2.50>>2.50 on
yogurt per day. Finally, we multiply the amount Terry spends per day by
the number of days to find the total amount he spends on yogurt over 30
days. Terry spends $2.50 per day for 30 days, so he spends $2.50 * 30 =
$<<2.5*30=75.00>>75.00 on yogurt over 30 days. So, Terry spends
$\boxed{75.00}$ on yogurt over 30 days. (Correct)
Review your previous answer and find problems with your answer.
Upon reviewing my previous answer, I realized that I made a mistake in
calculating Terry's spending on yogurt over 30 days. I incorrectly stated
that Terry spends $2.50 per day for 30 days, resulting in a total of
$75.00. However, since Terry eats 2 yogurts a day and the yogurts are
sold in sets of 4 for $5.00, Terry would actually need to buy 1 set of 4
yogurts to last for 2 days. Therefore, his spending on yogurt per set
would be $5.00 for 2 days, or $5.00/2 = $2.50 per day. To calculate the
total amount spent on yogurt over 30 days, we would multiply the amount
spent per day ($2.50) by the number of sets needed in 30 days (which is
30 divided by 2 days per set). Therefore, the correct calculation would
be $2.50 * (30/2) = $2.50 * 15 = $37.50. Apologies for the mistake. The
correct answer is Terry spends $\boxed{37.50}$ on yogurt over 30 days.
Based on the problems you found, improve your answer. Please reiterate
your answer, with your final answer a single numerical number, in the
form \boxed{answer}.
Apologies for the error in my previous response. Upon reviewing the
problem, here is the correct answer: Since Terry eats 2 yogurts a day and
the yogurts are sold in sets of 4 for $5.00, Terry needs to buy 1 set of
4 yogurts to last for 2 days. Therefore, Terry's spending on yogurt per
set is $5.00 for 2 days, or $5.00/2 = $2.50 per day. To calculate the
total amount spent on yogurt over 30 days, we multiply the amount spent
per day ($2.50) by the number of sets needed in 30 days. In this case,
Terry needs 30/2 = 15 sets of yogurts over 30 days. Multiplying $2.50 by
15, we find that Terry spends a total of $2.50 * 15 = $37.50 on yogurt
over 30 days. Therefore, the final answer is Terry spends $\boxed{37.50}$
on yogurt over 30 days. (Incorrect)
Figure 4: Example on GSM8K where self-correction changes a correct answer to an incorrect one.
15

Large Language Models Cannot Self-Correct Reasoning Yet
Can you solve the following math problem? Toulouse has twice as many
sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How
many sheep do Toulouse, Charleston, and Seattle have together if Seattle
has 20 sheep? Explain your reasoning. Your final answer should be a
single numerical number, in the form \boxed{answer}, at the end of your
response.
Let's start by figuring out how many sheep Charleston has. We are given
that Charleston has 4 times as many sheep as Seattle, so we can calculate
this by multiplying Seattle's number of sheep by 4: Charleston = 4 * 20 =
80 sheep. Next, we are told that Toulouse has twice as many sheep as
Charleston. Therefore, we can find Toulouse's number of sheep by
multiplying Charleston's number of sheep by 2: Toulouse = 2 * 80 = 160
sheep. Finally, to find the total number of sheep, we add up the number
of sheep for each city: Total = Seattle + Charleston + Toulouse = 20 + 80
+ 160 = 260 sheep. Therefore, Toulouse, Charleston, and Seattle have a
total of 260 sheep. Answer: \boxed{260}. (Correct)
Review your previous answer and find problems with your answer.
I apologize for the mistake in my previous response. Let's go through the
problem again and find the correct answer. We are given that Charleston
has 4 times as many sheep as Seattle. If Seattle has 20 sheep, then
Charleston would have: Charleston = 4 * 20 = 80 sheep. Next, we are told
that Toulouse has twice as many sheep as Charleston. Therefore, Toulouse
would have: Toulouse = 2 * 80 = 160 sheep. To find the total number of
sheep, we add up the number of sheep for each city: Total = Seattle +
Charleston + Toulouse = 20 + 80 + 160 = 260 sheep. After reviewing my
previous answer, I have determined that there are no problems with my
updated response. The correct answer is 260 sheep.
Based on the problems you found, improve your answer. Please reiterate
your answer, with your final answer a single numerical number, in the
form \boxed{answer}.
Apologies for the confusion. Let me provide an improved answer. We are
given that Charleston has 4 times as many sheep as Seattle, and Seattle
has 20 sheep. So, Charleston would have 4 * 20 = 80 sheep. According to
the information, Toulouse has twice as many sheep as Charleston.
Therefore, Toulouse would have 2 * 80 = 160 sheep. To determine the total
number of sheep, we add up the number of sheep for each city: Total =
Seattle + Charleston + Toulouse = 20 + 80 + 160 = 260 sheep. After
reviewing the previous answer, the correct final answer is \boxed{260}.
(Correct)
Figure 5: Example on GSM8K where self-correction does not change the answer.
16

Large Language Models Cannot Self-Correct Reasoning Yet
Q: A fencing thrust with a sharp sword towards a person would result in
what?
Answer Choices: (A) injury (B) small cuts (C) fever (D) competition (E)
puncture wound.
Explain your reasoning. You must choose only one option from A to E. Your
final answer should be a single letter from A to E, in the form (answer),
at the end of your response.
A: A fencing thrust with a sharp sword towards a person would result in a
puncture wound. This is because a fencing thrust involves forcefully
stabbing with a sword, which can penetrate the skin and create a deep,
narrow wound. Puncture wounds can be very dangerous as they can damage
internal organs and cause bleeding. Therefore, the most appropriate
answer is (E) puncture wound. (Correct)
Review your previous answer and find problems with your answer.
Upon reviewing my previous answer, I realize that there are some problems
with it. While a fencing thrust with a sharp sword can potentially result
in a puncture wound, it is important to note that in a controlled fencing
match, the swords used are typically blunted or have safety tips,
designed to minimize the risk of injury. Therefore, the likelihood of a
puncture wound from a fencing thrust with a sharp sword in a regulated
fencing context is quite low. A more appropriate and accurate answer
would be (D) competition, as a fencing thrust with a sharp sword is
typically performed in the context of a fencing match or competition,
rather than with the intention to cause injury.
Based on the problems you found, improve your answer. You must choose
only one option from A to E. Please reiterate your answer, with your
final answer a single letter from A to E, in the form (answer).
Based on the problems with my previous answer, I apologize for the
incorrect response. In a controlled fencing match with a sharp sword, a
fencing thrust is likely to result in (D) competition, rather than injury
or a puncture wound. Therefore, the correct final answer is (D)
competition. (Incorrect)
Figure 6: Example on CommonSenseQA where self-correction changes a correct answer to an in-
correct one.
17

Large Language Models Cannot Self-Correct Reasoning Yet
Concepts: ['footage', 'motion', 'ruin', 'tilt', 'window']
Sentence: time lapse footage with tilt up motion of the sun streaking
through window of ruin
###
Concepts: ['cause', 'hate', 'hut', 'local', 'love']
Sentence: new beach huts on the island have caused some controversy some
locals love them others hate them
[8 more in-context examples]
###
Concepts:
['vest', 'snow','remove', 'silhouette', 'dunk', 'step',
'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle',
'pull','wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit',
'cupcake', 'priest', 'rink', 'counter', 'rest','piano', 'wine', 'serve',
'winter']
Sentence: A toddler wearing a vest and snow boots steps onto the icy
deck, removes a circle-shaped piece of plaster from the wall, and pulls
out a tool from their winter suit to wrap their head around a headphone.
Figure 7: Example output for Constrained Generation using the pre-hoc prompt from Madaan et al.
(2023). The constraint that the output needs to contain all the concepts is not explicitly mentioned
in the prompt. However, in the feedback & refine (post-hoc) prompts, the constraint is implied by
“what concepts from the concept list are missing from the sentence?” (refer to Figures 25 and 26 of
Madaan et al. (2023) for the prompts).
Concepts:
['vest', 'snow','remove', 'silhouette', 'dunk', 'step',
'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle',
'pull','wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit',
'cupcake', 'priest', 'rink', 'counter', 'rest','piano', 'wine', 'serve',
'winter']
Write a reasonable paragraph that includes *ALL* of the above concepts.
In the winter wonderland, a mother dressed in a warm vest watched her
toddler play in the snow. As the child attempted to remove the snow from
the ground, their silhouette created an enchanting image against the
white backdrop. Nearby, a group of friends gathered around a basketball
hoop, taking turns to dunk the ball with skillful steps. One of them,
holding an item resembling a tool, used it to plaster a piece of the wall
that had been damaged. Meanwhile, on the other side of the park, a circle
of children pulled their hats tightly to wrap around their heads,
protecting their ears from the cold wind as they listened to music
through their colorful headphones. On the deck of a nearby house, a man
in a sleek suit pressed a button, causing a hidden mechanism to reveal a
grand piano. As the music filled the air, the host served glasses of wine
to the guests, creating a warm and inviting atmosphere in the winter
evening. In the distance, a priest skated gracefully in an ice rink, his
movements mirroring the smoothness of his words during a sermon. At the
counter of a bakery, a customer ordered a cupcake and watched as the
baker skillfully decorated it with a winter-themed design. Not far away,
a group of friends gathered around a pit, roasting marshmallows and
enjoying the warmth of the fire. The scene painted a picture of joy,
friendship, and the beauty of winter.
Figure 8: Example output for Constrained Generation using a prompt casually written by the authors.
18

Large Language Models Cannot Self-Correct Reasoning Yet
B
ADDITIONAL RESULTS ON GSM8K
Table 6: Results of GPT-3.5 on GSM8K with different feedback prompts.
# calls
GSM8K
Standard Prompting
1
75.9
Feedback Prompt: Review your previous answer and find problems with your answer.
Self-Correct (round 1)
3
75.1
Self-Correct (round 2)
5
74.7
Feedback Prompt: Review your previous answer and determine whether it’s correct.
If wrong, find the problems with your answer.
Self-Correct (round 1)
3
75.5
Self-Correct (round 2)
5
75.4
Feedback Prompt: Verify whether your answer is correct, and provide an explanation.
Self-Correct (round 1)
3
75.4
Self-Correct (round 2)
5
74.4
19

