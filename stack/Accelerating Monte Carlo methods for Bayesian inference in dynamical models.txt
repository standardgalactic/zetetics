Accelerating Monte Carlo methods
for Bayesian inference in dynamical models
Johan Dahlin
Linköping studies in science and technology. Dissertations.
No. 1754

Linköping studies in science and technology. Dissertations.
No. 1754
Accelerating Monte Carlo
methods for Bayesian
inference in dynamical
models
Johan Dahlin

Cover illustration: A Markov chain generated by the Metropolis-Hastings algorithm with
an autoregressive proposal on a manifold given by a parametric function.
This thesis was typeset using the LATEX typesetting system originally developed by Leslie
Lamport, based on TEX created by Donald Knuth. The text is set in Garamond and Cabin.
The source code is set in Inconsolata. All plots are made using R (R Core Team, 2015)
together with colors from the RColorBrewer package (Neuwirth, 2014). Most simulations
are carried out in R and Python with the exception of Paper F and H.
Linköping studies in science and technology. Dissertations.
No. 1754
Accelerating Monte Carlo methods for Bayesian inference in dynamical models
Johan Dahlin
johan.dahlin@liu.se
liu@johandahlin.com
http://liu.johandahlin.com
Division of Automatic Control
Department of Electrical Engineering
Linköping University
SE–581 83 Linköping
Sweden
ISBN 978-91-7685-797-7
ISSN 0345-7524
Copyright (c) 2016 Johan Dahlin
Printed by LiU-Tryck, Linköping, Sweden 2016

Denna avhandling tillägnas min familj!


Abstract
Making decisions and predictions from noisy observations are two important and challeng-
ing problems in many areas of society. Some examples of applications are recommendation
systems for online shopping and streaming services, connecting genes with certain diseases
and modelling climate change. In this thesis, we make use of Bayesian statistics to construct
probabilistic models given prior information and historical data, which can be used for
decision support and predictions. The main obstacle with this approach is that it often
results in mathematical problems lacking analytical solutions. To cope with this, we make
use of statistical simulation algorithms known as Monte Carlo methods to approximate
the intractable solution. These methods enjoy well-understood statistical properties but are
often computational prohibitive to employ.
The main contribution of this thesis is the exploration of diﬀerent strategies for accelerating
inference methods based on sequential Monte Carlo (smc) and Markov chain Monte Carlo
(mcmc). That is, strategies for reducing the computational eﬀort while keeping or improv-
ing the accuracy. A major part of the thesis is devoted to proposing such strategies for the
mcmc method known as the particle Metropolis-Hastings (pmh) algorithm. We investigate
two strategies: (i) introducing estimates of the gradient and Hessian of the target to better
tailor the algorithm to the problem and (ii) introducing a positive correlation between the
point-wise estimates of the target.
Furthermore, we propose an algorithm based on the combination of smc and Gaussian
process optimisation, which can provide reasonable estimates of the posterior but with a
signiﬁcant decrease in computational eﬀort compared with pmh. Moreover, we explore the
use of sparseness priors for approximate inference in over-parametrised mixed eﬀects models
and autoregressive processes. This can potentially be a practical strategy for inference in
the big data era. Finally, we propose a general method for increasing the accuracy of the
parameter estimates in non-linear state space models by applying a designed input signal.
v


Populärvetenskaplig sammanfattning
Borde Riksbanken höja eller sänka reporäntan vid sitt nästa möte för att nå inﬂationsmålet?
Vilka gener är förknippade med en viss sjukdom? Hur kan Netﬂix och Spotify veta vilka ﬁlmer
och vilken musik som jag vill lyssna på härnäst?
Dessa tre problem är exempel på frågor där statistiska modeller kan vara användbara för
att ge hjälp och underlag för beslut. Statistiska modeller kombinerar teoretisk kunskap om
exempelvis det svenska ekonomiska systemet med historisk data för att ge prognoser av
framtida skeenden. Dessa prognoser kan sedan användas för att utvärdera exempelvis vad
som skulle hända med inﬂationen i Sverige om arbetslösheten sjunker eller hur värdet på
mitt pensionssparande förändras när Stockholmsbörsen rasar. Tillämpningar som dessa och
många andra gör statistiska modeller viktiga för många delar av samhället.
Ett sätt att ta fram statistiska modeller bygger på att kontinuerligt uppdatera en modell
allteftersom mer information samlas in. Detta angreppssätt kallas för Bayesiansk statistik
och är särskilt användbart när man sedan tidigare har bra insikter i modellen eller tillgång
till endast lite historisk data för att bygga modellen. En nackdel med Bayesiansk statistik är
att de beräkningar som krävs för att uppdatera modellen med den nya informationen ofta är
mycket komplicerade. I sådana situationer kan man istället simulera utfallet från miljontals
varianter av modellen och sedan jämföra dessa mot de historiska observationerna som ﬁnns
till hands. Man kan sedan medelvärdesbilda över de varianter som gav bäst resultat för att
på så sätt ta fram en slutlig modell. Det kan därför ibland ta dagar eller veckor för att ta
fram en modell. Problemet blir särskilt stort när man använder mer avancerade modeller
som skulle kunna ge bättre prognoser men som tar för lång tid för att bygga.
I denna avhandling använder vi ett antal olika strategier för att underlätta eller förbättra
dessa simuleringar. Vi föreslår exempelvis att ta hänsyn till ﬂer insikter om systemet och
därmed minska antalet varianter av modellen som behöver undersökas. Vi kan således redan
utesluta vissa modeller eftersom vi har en bra uppfattning om ungefär hur en bra modell
ska se ut. Vi kan också förändra simuleringen så att den enklare rör sig mellan olika typer
av modeller. På detta sätt utforskas rymden av alla möjliga modeller på ett mer eﬀektivt
sätt. Vi föreslår ett antal olika kombinationer och förändringar av beﬁntliga metoder för att
snabba upp anpassningen av modellen till observationerna. Vi visar att beräkningstiden i
vissa fall kan minska ifrån några dagar till någon timme. Förhoppningsvis kommer detta i
framtiden leda till att man i praktiken kan använda mer avancerade modeller som i sin tur
resulterar i bättre prognoser och beslut.
vii


Acknowledgments
Science is a co-operative enterprise, spanning the generations. It’s the pass-
ing of a torch from teacher to student to teacher. A community of minds
reaching back from antiquity and forward to the stars. – Neil deGrasse Tyson
This is my humble contribution to the collaboration that is science. This is my dent in the
universe! However, I could not have reached this point and written this thesis without the
support, encouragement and love from so many people over the years. We all have so much
to be grateful for. We often do not have the opportunity to express this and often take
things for granted. Therefore, please bare with me on the following pages in my attempt to
express my gratitude for all the people that made this journey possible.
To do a phd means that you spend ﬁve years on the boundary of your comfort zone. Some-
times, you are on the inside of the boundary but often you are just (or even further) outside
the boundary. The latter is an awesome place to be. There is nothing that develops you
more than when you stretch the limits of what you think that you can achieve. However,
staying at this place for a long time takes its toll and this is one of the reasons (except of
course learning how to do research) for having a guide and mentor along for the journey.
In my case, I got the opportunity to travel along my two amazing supervisors Thomas
Schön and Fredrik Lindsten. These guys are really great supervisors and they have skilfully
guided me along the way to obtain my phd. I am truly grateful for all the time, eﬀort and
energy that they have put into helping me develop as a researcher and as a person. Thomas
has helped me a lot with the long-term perspective with strategy, planning, collaborations
and research ideas. Fredrik has helped me with many good ideas, answering hundreds of
questions regarding the intricate working of algorithms and helped me iron out subtle
mistakes in papers and reports. Thank you also for all the nice times together outside of
work. Especially all the running, restaurant visits and team day dinners as Thomas’ place!
Along my journey, I crossed paths with Mattias Villani and Robert Kohn, who supported
and guided me almost as if I was one of their own phd students. I am very grateful for
our collaborations and the time, inspiration and knowledge you both have given me. A
special thanks goes to Robert for the invitation to visit him at unsw Business School in
Sydney, Australia. The autumn that I spent there was truly a wonderful experience in terms
of research as well as from a personal perspective. Thank you Robert for you amazing
hospitality, your patience and encouragement.
I would like to thank all my co-authors during my time at Linköping University for some
wonderful and fruitful collaborations: Christian Andersson Naesseth, Liang Dai, Daniel
Hultqvist, Daniel Jönsson, Manon Kok, Robert Kohn, Joel Kronander, Fredrik Lindsten,
Cristian Rojas, Jakob Roll, Thomas Schön, Andreas Svensson, Fredrik Svensson, Jonas
Unger, Patricio Valenzuela, Mattias Villani, Johan Wågberg and Adrian Wills. Furthermore,
many of these co-authors and Olof Sundin helped with proof-reading the thesis and con-
tributed with suggestions to improve it. All remaining errors are entirely my own.
To be able to write a good thesis you require a good working environment. Svante Gun-
narsson and Ninna Stensgård are two very important persons in this eﬀort. Thank you for
all your support and helpfulness in all matters to help create the best possible situation for
ix

x
Acknowledgments
myself and for all the other phd students. Furthermore, I gratefully acknowledge the ﬁ-
nancial support from the projects Learning of complex dynamical systems (Contract number:
637-2014-466) and Probabilistic modeling of dynamical systems (Contract number: 621-2013-
5524) and cadics, a Linnaeus Center, all funded by the Swedish Research Council. I would
also like to acknowledge Dr. Henrik Tidefelt and Dr. Gustaf Hendeby for constructing and
maintaining the LATEX-template in which this thesis is (partially) written.
Another aspect of the atmosphere at work is all my wonderful colleagues. My room mate
from the early years Michael Roth was always there to discuss work and to keep my streak of
perfectionism in check. My friendships with Jonas Linder and Manon Kok have also meant
a lot to me. We joined the group at the same time and have spent many hours together both
at work and during our spare time. Thank you for your positive attitudes and for all the
fun times exploring Beijing, Cape Town, Vancouver, Varberg and France together.
Furthermore, thank you Sina Khoshfetrat Pakazad for arranging all the nice bbqs and for
always being up for discussing politics. It is also important to get some fresh air and see the
sun during the long days at work. Hanna Nyqvist has been my loyal companion during
our many lunch walks together. Oskar Ljungqvist recently joined the group but has quickly
become a good friend. Thank you for all our lunch runs (jogs) together, for encouraging
and inspiring my running and for all the nice discussions!
Furthermore, I would like to thank my remaining friends and colleagues at the group.
Especially, (without any speciﬁc ordering) Christian Andersson Naesseth, Christian Lyzell,
Ylva Jung, Isak Nielsen, Daniel Petersson, Zoran Sjanic, Niklas Wahlström, Clas Veibäck
and Emre Özkan for all the fun things that we have done together. This includes everything
from wine trips in South Africa, wonderful food in France and ordering 30 dumplings
each in Beijing to hitting some shallows on open sea with a canoe, cross country skiing in
Chamonix and riding the local bus to the Great Wall of China. It has been great fun!
Along the years, I have also spent a fair amount of time at other research groups and with the
phd students within them. A big thanks goes out to the Systems and Control at Uppsala
University, Automatic Control at kth, Economics at unsw and Statistics and Machine
learning at Linköping University. Thank you for your hospitality and for all our research
discussions. I especially would like to thank Joel Kronander, Christian Larsson, Andreas
Svensson, Soma Tayamon, Patricio Valenzuela and Johan Wågberg for all the good times
travelling the world together.
Finally, my family and friends outside of work are a great source of support, inspiration
and encouragement. My family is always there when needed with love and kindness as well
as to help with all possible practical matters. My friends always provide refuge from work
when the stress levels are high and the motivation falters. Thank you all for believing in me
and in supporting me even when (at times) I did not myself. I hope we all can spend some
more time together in the years to come. I love you all and you mean the world to me!
Linköping, March 21, 2016
Johan Dahlin

Contents
I
Background
1
Introductory overview
3
1.1
Examples of applications
. . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.1.1
Reconstructing the temperature of pre-historic Earth . . . . . . .
5
1.1.2
Rendering photorealistic images . . . . . . . . . . . . . . . . . .
5
1.2
Main contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3
Thesis outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.4
Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2
Bayesian modelling and inference
17
2.1
Three examples of statistical data . . . . . . . . . . . . . . . . . . . . . .
18
2.2
Parametric models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.2.1
Linear regression and generalised linear models
. . . . . . . . . .
23
2.2.2
Autoregressive models
. . . . . . . . . . . . . . . . . . . . . . .
25
2.2.3
State space models
. . . . . . . . . . . . . . . . . . . . . . . . .
26
2.2.4
Mixed eﬀect models . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.3
Computing the posterior and making decisions
. . . . . . . . . . . . . .
29
2.4
Non-parametric models . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.4.1
Gaussian processes
. . . . . . . . . . . . . . . . . . . . . . . . .
36
2.4.2
Dirichlet processes . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.5
Outlook and extensions . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3
Monte Carlo methods
43
3.1
Empirical approximations . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.2
Three sampling strategies . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2.1
Independent Monte Carlo
. . . . . . . . . . . . . . . . . . . . .
45
3.2.2
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . . . .
48
3.2.3
Markov chain Monte Carlo . . . . . . . . . . . . . . . . . . . . .
59
3.3
Pseudo-marginal Metropolis-Hastings . . . . . . . . . . . . . . . . . . . .
67
3.4
Outlook and extensions . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
xi

xii
Contents
4
Strategies for accelerating inference
73
4.1
Increasing the amount of information in the data . . . . . . . . . . . . . .
74
4.2
Approximating the model . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.3
Improving the inference algorithm . . . . . . . . . . . . . . . . . . . . .
79
4.4
Outlook and extensions . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5
Concluding remarks
87
5.1
Summary of contributions
. . . . . . . . . . . . . . . . . . . . . . . . .
88
5.2
Some trends and ideas for future work . . . . . . . . . . . . . . . . . . .
90
5.3
Source code and data
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
Notation
95
Bibliography
99
II
Papers
A
Getting started with PMH for inference in non-linear models
117
1
Introductory overview
. . . . . . . . . . . . . . . . . . . . . . . . . . .
120
1.1
State space models
. . . . . . . . . . . . . . . . . . . . . . . . .
120
1.2
Bayesian inference
. . . . . . . . . . . . . . . . . . . . . . . . .
121
2
Related software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
3
Overview of the PMH algorithm . . . . . . . . . . . . . . . . . . . . . .
123
3.1
Constructing the Markov chain
. . . . . . . . . . . . . . . . . .
123
3.2
Approximating the acceptance probability . . . . . . . . . . . . .
124
4
Estimating the parameters in a linear Gaussian SSM . . . . . . . . . . . .
126
4.1
Implementing the particle ﬁlter
. . . . . . . . . . . . . . . . . .
128
4.2
Numerical illustration of state inference . . . . . . . . . . . . . .
131
4.3
Implementing particle Metropolis-Hastings
. . . . . . . . . . . .
133
4.4
Numerical illustration of parameter inference . . . . . . . . . . .
135
5
Application example: volatility estimation OMXS30 . . . . . . . . . . . .
137
6
Improving the PMH algorithm . . . . . . . . . . . . . . . . . . . . . . .
141
6.1
Initialisation
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
6.2
Diagnosing convergence
. . . . . . . . . . . . . . . . . . . . . .
142
6.3
Improving mixing . . . . . . . . . . . . . . . . . . . . . . . . . .
142
7
Outlook and conclusions . . . . . . . . . . . . . . . . . . . . . . . . . .
146
7.1
Improving the particle ﬁlter
. . . . . . . . . . . . . . . . . . . .
147
7.2
Improving particle Metropolis-Hastings . . . . . . . . . . . . . .
148
7.3
Additional software implementations
. . . . . . . . . . . . . . .
149
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
B
PMH using gradient and Hessian information
155
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
2
Particle Metropolis-Hastings
. . . . . . . . . . . . . . . . . . . . . . . .
159
2.1
MH sampling with unbiased likelihoods . . . . . . . . . . . . . .
159
2.2
Constructing PMH1 and PMH2 . . . . . . . . . . . . . . . . . .
161

Contents
xiii
2.3
Properties of the PMH1 and PMH2 proposals . . . . . . . . . . .
162
3
Estimation of the likelihood, gradient, and Hessian
. . . . . . . . . . . .
162
3.1
Auxiliary particle ﬁlter . . . . . . . . . . . . . . . . . . . . . . .
163
3.2
Estimation of the likelihood . . . . . . . . . . . . . . . . . . . .
164
3.3
Estimation of the gradient
. . . . . . . . . . . . . . . . . . . . .
164
3.4
Estimation of the Hessian
. . . . . . . . . . . . . . . . . . . . .
165
3.5
Regularisation of the estimate of the Hessian
. . . . . . . . . . .
167
3.6
Resulting SMC algorithm . . . . . . . . . . . . . . . . . . . . . .
168
4
Numerical illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
4.1
Estimation of the log-likelihood and the gradient
. . . . . . . . .
169
4.2
Burn-in and scale-invariance
. . . . . . . . . . . . . . . . . . . .
169
4.3
The mixing of the Markov chains at stationarity . . . . . . . . . .
173
4.4
Parameter inference in a Poisson count model . . . . . . . . . . .
175
4.5
Robustness in the lag and step size . . . . . . . . . . . . . . . . .
178
5
Discussion and future work . . . . . . . . . . . . . . . . . . . . . . . . .
178
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
C
Quasi-Newton particle Metropolis-Hastings
185
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
2
Particle Metropolis-Hastings
. . . . . . . . . . . . . . . . . . . . . . . .
189
3
Proposal for parameters . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
3.1
Zeroth and ﬁrst-order proposals (PMH0/1)
. . . . . . . . . . . .
190
3.2
Second-order proposal (PMH2)
. . . . . . . . . . . . . . . . . .
191
4
Proposal for auxiliary variables . . . . . . . . . . . . . . . . . . . . . . .
192
4.1
SMC-ABC algorithm . . . . . . . . . . . . . . . . . . . . . . . .
192
4.2
Estimation of the likelihood . . . . . . . . . . . . . . . . . . . .
193
4.3
Estimation of the gradient of the log-posterior . . . . . . . . . . .
193
5
Numerical illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
5.1
Linear Gaussian SSM . . . . . . . . . . . . . . . . . . . . . . . .
194
5.2
Modelling the volatility in coﬀee futures . . . . . . . . . . . . . .
197
6
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
A
Implementation details
. . . . . . . . . . . . . . . . . . . . . . . . . . .
199
B
Implementation details for quasi-Newton proposal . . . . . . . . . . . . .
201
C
Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
D
α-stable distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
D Accelerating pmMH using correlated likelihood estimators
209
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
2
Introducing correlation into the auxiliary variables . . . . . . . . . . . . .
213
3
Theoretical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
3.1
Setting up the model . . . . . . . . . . . . . . . . . . . . . . . .
216
3.2
Analysis by discretisation of the state space
. . . . . . . . . . . .
217
3.3
Tuning the CN proposal for the univariate case . . . . . . . . . .
218
4
Numerical illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
4.1
Gaussian IID model . . . . . . . . . . . . . . . . . . . . . . . . . 220

xiv
Contents
4.2
Stochastic volatility model with leverage . . . . . . . . . . . . . .
222
5
Conclusions and future work . . . . . . . . . . . . . . . . . . . . . . . .
225
A
Implementation details
. . . . . . . . . . . . . . . . . . . . . . . . . . .
226
A.1
Gaussian IID model . . . . . . . . . . . . . . . . . . . . . . . . .
226
A.2
Stochastic volatility model with leverage . . . . . . . . . . . . . .
226
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
E
Approximate Bayesian inference for models with intractable likelihoods
233
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
2
An intuitive overview of GPO-ABC . . . . . . . . . . . . . . . . . . . .
238
3
Estimating the log-posterior . . . . . . . . . . . . . . . . . . . . . . . . . 240
3.1
Particle ﬁltering with approximate Bayesian computations
. . . .
241
3.2
The estimator and its statistical properties . . . . . . . . . . . . .
243
4
Constructing the surrogate of the log-posterior . . . . . . . . . . . . . . .
244
4.1
Gaussian process prior . . . . . . . . . . . . . . . . . . . . . . .
244
4.2
Acquisition function . . . . . . . . . . . . . . . . . . . . . . . . 246
5
The GPO-ABC algorithm . . . . . . . . . . . . . . . . . . . . . . . . . .
247
5.1
Initialisation and convergence criteria
. . . . . . . . . . . . . . .
247
5.2
Extracting the Laplace approximation . . . . . . . . . . . . . . .
248
5.3
Convergence properties . . . . . . . . . . . . . . . . . . . . . . .
249
6
Numerical illustrations and real-world applications . . . . . . . . . . . . .
249
6.1
Stochastic volatility model with Gaussian log-returns . . . . . . .
249
6.2
Stochastic volatility model with α-stable log-returns . . . . . . . .
251
6.3
Modelling price dependencies between oil futures . . . . . . . . .
253
7
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
A
Implementation details
. . . . . . . . . . . . . . . . . . . . . . . . . . .
261
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
F
Hierarchical Bayesian approaches for robust inference in ARX models
267
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
2
Hierarchical Bayesia ARX models
. . . . . . . . . . . . . . . . . . . . .
271
2.1
Student’s t distributed innovations . . . . . . . . . . . . . . . . .
271
2.2
Parametric model order . . . . . . . . . . . . . . . . . . . . . . .
271
2.3
Automatic relevance determination
. . . . . . . . . . . . . . . .
272
3
Markov chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . .
273
4
Posteriors and proposal distributions . . . . . . . . . . . . . . . . . . . .
274
4.1
Model order . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
4.2
ARX coeﬃcients . . . . . . . . . . . . . . . . . . . . . . . . . .
275
4.3
ARX coeﬃcients variance
. . . . . . . . . . . . . . . . . . . . .
275
4.4
Latent variance variables . . . . . . . . . . . . . . . . . . . . . . 276
4.5
Innovation scale parameter . . . . . . . . . . . . . . . . . . . . .
277
4.6
Innovation DOF . . . . . . . . . . . . . . . . . . . . . . . . . .
277
5
Numerical illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
5.1
Average model performance
. . . . . . . . . . . . . . . . . . . .
277
5.2
Robustness to outliers and missing data
. . . . . . . . . . . . . . 280
5.3
Real EGG data . . . . . . . . . . . . . . . . . . . . . . . . . . .
282

Contents
xv
6
Conclusions and Future work . . . . . . . . . . . . . . . . . . . . . . . .
282
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
G Bayesian inference for mixed eﬀects models with heterogeneity
287
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
2
Bayesian mixture modelling . . . . . . . . . . . . . . . . . . . . . . . . .
291
2.1
Inﬁnite mixture model using a Dirichlet process . . . . . . . . . .
291
2.2
Finite mixture model . . . . . . . . . . . . . . . . . . . . . . . .
293
3
Sampling from the posterior
. . . . . . . . . . . . . . . . . . . . . . . .
294
3.1
Prior distributions
. . . . . . . . . . . . . . . . . . . . . . . . .
294
3.2
Gibbs sampling . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
3.3
Conditional posteriors . . . . . . . . . . . . . . . . . . . . . . .
296
4
Numerical illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
4.1
Mixture of Gaussians . . . . . . . . . . . . . . . . . . . . . . . .
298
4.2
Mixed eﬀects model with synthetic data . . . . . . . . . . . . . .
300
4.3
Mixed eﬀects model with sleep deprivation data . . . . . . . . . .
302
5
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
304
A
Implementation details
. . . . . . . . . . . . . . . . . . . . . . . . . . .
306
A.1
Mixture of Gaussians . . . . . . . . . . . . . . . . . . . . . . . .
306
A.2
Mixed eﬀects model with synthetic data . . . . . . . . . . . . . .
306
A.3
Mixed eﬀects model with sleep deprivation data . . . . . . . . . .
306
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
H Robust Input design for non-linear dynamical models
309
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
312
2
Problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
314
3
Describing the set of stationary processes . . . . . . . . . . . . . . . . . .
316
4
Estimation of the Fisher information matrix . . . . . . . . . . . . . . . .
319
4.1
Estimating the score function
. . . . . . . . . . . . . . . . . . .
319
4.2
The resulting estimator . . . . . . . . . . . . . . . . . . . . . . .
321
5
Final input design method
. . . . . . . . . . . . . . . . . . . . . . . . .
324
6
Input signal generation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
324
7
Numerical illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
7.1
Accuracy of information matrix estimation
. . . . . . . . . . . .
326
7.2
Input design for the LGSS model . . . . . . . . . . . . . . . . . .
328
7.3
Input design for a non-linear model
. . . . . . . . . . . . . . . .
330
8
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333

xvi
Contents

Part I
Background


1
Introductory overview
Modelling of dynamical systems is an integrate part of modern science. Two major applica-
tions are to describe some observed data and to make forecasts about the future behaviour
of a system. The latter is an essential ingredient in making decision from noisy observations
in many areas such as business, economics, engineering and medicine. A standard approach
for forecasting and decision making is to make use of probabilistic models (Ghahramani,
2015), which are created by combining some pre-deﬁned model with observations from the
true system. This approach is also known as data-driven modelling and is probably the most
popular alternative for decision support today.
The probabilistic model is usually constructed by making use of statistical inference. One
such framework is Bayesian statistics, which allows for sequentially updating the model as
more observations arrive. Another beneﬁt is that the uncertainty regarding the model can be
included when making decisions. However, a major problem with Bayesian inference is that
model updates, prediction and other computations often are posed as intractable integrals.
Hence, these cannot be computed in closed-form and approximations are required.
A typical example is to compute the mean of the so-called posterior distribution π(x |y),
which encodes our prior beliefs of some quantity x ∈X and the information in some data
denoted by y. The posterior mean can be computed by
Eπ[x] =

X
x π(x |y) dx,
where the dimension of the problem is determined by the dimension of x. Hence, this can
correspond to a high-dimensional integration problem, which is diﬃcult to approximate
using numerical methods such as curvatures.
Instead, Monte Carlo methods or variational inference are often applied to approximate the
update by statistical simulation and analytical approximations, respectively. In this thesis,
3

4
1
Introductory overview
we focus on the former family of methods, which is based on generating a large number of
random scenarios or outcomes. Hence, Monte Carlo algorithms are often computational
intensive and can require days or weeks to run. This is especially a problem for dynamical
models and this thesis is devoted to try to decrease the time that is required to implement
and execute these algorithms while keeping their accuracy.
Examples of applications
We begin by presenting a few applications where acceleration of Monte Carlo methods could
be important. As previously discussed, these methods are essential for Bayesian inference
in dynamical models, which themselves have applications in many diﬀerent ﬁelds. One
example is platooning, where trucks are driven as a group to reduce the air drag and therefore
to increase fuel-eﬃciency. This is possible by using e.g., model predictive control (mpc;
Mayne, 2014) to control the trucks to keep a certain distance, see Turri et al. (2015). Here,
an accurate model is important as it is used to forecast future outcomes. Bayesian modelling
can be useful in this setting as it also can take into account the uncertainty of model when
computing predictions.
In recommendation systems, probabilistic modelling is important to provide suggestions to
the user, see Stern et al. (2009). Many online services such as Netﬂix, Spotify and Amazon
are already using such systems to improve customer satisfaction and to increase sales. This
problem is interesting because companies such as Amazon and Google have a massive
collection of information at their disposal. However, the amount of information regarding
a particular user can be quite limited, especially when the user is new to the service. Finding
patterns connecting this user to other users on the site is therefore important to be able
to pool the data and to provide good suggestions. It is also useful to take the dynamics
into account as user preferences can change over time. This was one of the key insights
incorporated into the winning algorithm in the Netﬂix Prize competition. The winning
approach proposed by Koren (2009) was awarded one million us dollars by the company.
Climate change and global warming are two big challenges for human kind to solve during
this century. Bayesian inference is useful in this setting to e.g., pool the output from diﬀerent
climate models together as discussed by Monteleoni et al. (2011). Again, the ability to take
uncertainty into account is important in this setting as well, see Birks (1998). Most natural
disasters are quite rare and modelling them is diﬃcult due to the small amounts of data.
Bayesian methods can therefore be useful in this setting to estimate probabilities of rare
events such as wild ﬁres, see Xue et al. (2012)
Probabilistic modelling is also useful in genomics to ﬁght disease and other health problems,
see Bush and Moore (2012) and Rasmussen et al. (2011). A major challenge in this ﬁeld
is to ﬁnd patterns and structures connecting genes with e.g., cancer, diabetes and heart
disease. The massive amount of information makes inference diﬃcult as many sophisticated
methods are computationally prohibitive. However, this type of analysis could be useful
for personalised medicine and data-driven health care if the computational challenges can be
overcome, see Raghupathi and Raghupathi (2014). Another interesting application in this
ﬁeld is reconstruct the lineage of diﬀerent species using Phylogenetic trees, see Larget and
Simon (1999) and Bouchard-Côté et al. (2012).

1.1
Examples of applications
5
We continue with introducing two more applications of probabilistic modelling connected
to Monte Carlo methods in the subsequent sections. Two further examples are introduced
in Chapter 2 and these follow us throughout the introductory part of this thesis to illustrate
important concepts. Finally, more real-world examples are presented in the papers included
in Part ii of this thesis.
Reconstructing the temperature of pre-historic Earth
In palaeoclimatology, ice varve thickness data is an important source of information to
recreate the historical mean temperature on the Earth, which is useful for studying global
warming. In the upper part of Figure 1.1, we present the thickness of ice varves (layers of
sediments that are deposited from melting glaciers) from Shumway and Stoﬀer (2011). The
silt and sand that are accumulated during each year makes up one varve and changes in the
varve thickness indicate temperature changes. That is, thick varves are the result of warm
and wet weather, whereas the opposite holds for cold and dry weather.
The data set contains the thickness of 634 ice varves formed at a location in Massachusetts,
us between the years 9,883 and 9,250 bc. We note that the mean and the variance of
the thickness seem to vary during the period but the data is quite noisy. Therefore, we
would like to smooth the data to be able to determine if the variations in the thickness are
statistically signiﬁcant. In the middle part of Figure 1.1, we make use of a non-parametric
Bayesian regression model known as the Gaussian process (gp; Rasmussen and Williams,
2006), which is further discussed in Section 2.4.1.
In the lower part of of the same ﬁgure, we present the result from using a parametric state
space model (ssm) to smooth the data. We introduce the ssm in Section 2.2.3 and show how
to make use of Monte Carlo methods to estimate the parameters of the ssm in Sections 3.2.2
and 3.3. In Figure 1.1, the resulting estimates of the mean thickness are presented as a solid
line and the 95% conﬁdence intervals are presented as the shaded areas. From this analysis,
we conclude that there is no signiﬁcant change in the mean thickness of the ice varves during
this period.
We also note that the uncertainty in the parametric model is smaller and it better follows
the data. The reason for this is that the gp model usually assumes homoscedastic variance,
whereas the variance is allowed to change with time in the parametric model. However, the
non-parametric model is simpler to estimate and it usually takes a couple of seconds on a
modern computer. On the other hand, inference for the parameters in the ssm can take
about an hour to complete. Therefore, there is a need to develop faster inference methods
for non-linear ssms. However, there are other non-parametric models that do not assume
homoscedasticity (Le et al., 2005) and can handle heavy-tailed observations by assuming
Student’s t noise (Shah et al., 2014).
Rendering photorealistic images
In computer graphics, an important problem is to design good methods for rendering
objects into an existing scene. This is typically used for special eﬀects in Hollywood ﬁlms
and for advertisement. A standard approach for this is the image-based lighting (ibl) method

6
1
Introductory overview
0
50
100
150
year (BC)
ice varve thickness
9900
9800
9700
9600
9500
9400
9300
9200
0
50
100
150
year (BC)
predictive posterior
9900
9800
9700
9600
9500
9400
9300
9200
0
50
100
150
year (BC)
predicted thickness
9900
9800
9700
9600
9500
9400
9300
9200
Figure 1.1. Upper: the thickness of ice varves formed at a location in Massachusetts between
years 9,883 and 9,250 BC. A non-parametric model (middle) and parametric model (lower) of
the thickness presented with the mean value (solid) lines and 95% conﬁdence intervals (shaded
areas). The dots indicate the original data.

1.2
Main contribution
7
(Debevec, 1998; Pharr and Humphreys, 2010), where a model of how light behaves is used
in combination with information about the scene. The latter so-called environment map
is often a panoramic image taken by a high dynamic range (hdr), which can record much
larger variations in brightness than a standard camera. This is required to capture all the
diﬀerent light sources within the scene.
Two concrete examples of renderings using the ibl method are presented in Figures 1.2
and 1.3 taken from Kronander et al. (2014a) and Unger et al. (2013). Note that, we have
rendered several photorealistic objects into the two scenes, such as a sphere, helicopter and
some furniture. In Figure 1.3, we present the scene before (left) and after (right) adding
the rendered furniture. This is a real-world example from ikea catalogues in which scenes
are often rendered using this technique to decrease the cost. The alternative would be
to build kitchens and similar environments customized for diﬀerent countries. The ibl
method instead allows for taking an image of a basic set-up, which then can be augmented
by computer rendering to create diﬀerent country-speciﬁc variations of the complete scene.
To be able to make use of ibl, we additionally require a geometric description of the
objects to be rendered and the properties of the diﬀerent materials in the objects. All of this
information is then combined using the light transport equation (lte), which is a physical
model expressed as an integral of how light rays propagates through space and reﬂects oﬀ
surfaces. The lte model cannot be solved analytically, but it can be approximated using
Monte Carlo methods as it is an integral.
A further complication is that there are inﬁnitely many rays that bounce around in the scene
before they hit the pixels in the image plan. As a result, it is computationally infeasible to
simulate all the possible light rays. Instead, we need to ﬁnd an approach to only simulate
the ones that contributes the most to the brightness and colour of each pixel in the image
plane. This is especially a problem when we would like to render a sequence of images. A
possible solution could be to start from the solution from the previous frame and adapt it
to the new frame. If the environment maps are similar, this could lead to a decrease in the
total computational cost. Hence, strategies for accelerating Monte Carlo methods could be
useful in this context to improve the rendering times and decrease the cost of special eﬀects
in ﬁlms. For more information, see Kronander et al. (2014a) and Ghosh et al. (2006).
Main contribution
We have now seen some speciﬁc examples of when dynamical models and Monte Carlo
methods can be of use. As stated before, Monte Carlo methods are very useful and often
acts as an enabler to solve otherwise intractable or infeasible problems. However, the main
drawback is that the Monte Carlo methods have a large computational cost and this could
limit their practical utility. Hence, accelerating Monte Carlo methods is an important
endeavour with applications in may diﬀerent domains. There are many diﬀerent strategies
to attain this goal proposed in the literature. These include solving an approximate but
simpler problem, utilising parallel hardware such as graphical processing units (gpus) and
modifying the algorithms themselves. In this thesis, we focus on the ﬁrst and the third
approach by using a number of diﬀerent strategies. This eﬀort has resulted in:

8
1
Introductory overview
Frame 502
Figure 1.2. A helicopter and sphere rendered using sequential ibl (Kronander et al., 2014a).
The image is part of Kronander et al. (2014a) ﬁrst published in the Proceedings of the 22nd European Signal
Processing Conference (eusipco 2014) in 2014, published by eurasip.
Figure 1.3. Scene from Unger et al. (2013) before (left) and after (right) rendering by ibl.
These images are unaltered reproductions of the originals in Unger et al. (2013) and are used under a cc-nc-
sa licence (http://creativecommons.org/licenses/by-nc-sa/3.0/). The original work is available
via Elsevier at http://dx.doi.org/10.1016/j.cag.2013.07.001.

1.3
Thesis outline
9
• A number of new alternative versions of the particle Metropolis-Hastings (pmh)
algorithm, where we incorporate gradient and Hessian information about the target
into the proposal. This results in better behaviour during the burn-in, improved
mixing of the Markov chain and simpliﬁed tuning of the algorithm for the user.
(Papers B and C).
• A method for introducing a positive correlation into the auxiliary variables generated
in the pseudo-marginal Metropolis-Hastings (pmmh) algorithm for estimating the
target. This results in around three times better mixing of the Markov chain for some
models, which results in a similar decrease of the computational cost (Paper D).
• A method to perform approximate Bayesian inference in non-linear ssms, which is
especially useful when the likelihood is intractable. The proposed method gives sim-
ilar estimates compared with the pmh algorithm but can reduce the computational
time from days to about an hour. (Paper E).
• A pedagogical and self-contained introduction to the phm algorithm with supporting
software implementations in three diﬀerent languages (Paper A).
• An evaluation of approximate inference in mixed eﬀects models with a Bayesian
mixture model for the heterogeneity of the random eﬀects. (Paper G).
• A method for input design in non-linear ssms (Paper H). The proposed method
increases the accuracy in the parameter estimates by applying a carefully designed
input signal to the system.
• An evaluation of two Bayesian arx models capable of dealing with outliers by mod-
elling the observations as Student’s t distributed. The proposed inference methods
also include automatic model order selection (Paper F).
Thesis outline
The thesis consists of two parts. The ﬁrst part introduces some background material regard-
ing modelling of dynamical data and diﬀerent approaches for inference. We also highlight
some problems with existing approaches and propose a number of strategies to mitigate
these. These strategies are applied and evaluated in the second part of the thesis in a collec-
tion of scientiﬁc contributions both as peer-reviewed papers and technical reports.
Paper A
Paper A of this thesis is an edited version of
J. Dahlin and T. B. Schön. Getting started with particle Metropolis-Hastings
for inference in nonlinear models. Pre-print, 2015. arXiv:1511.01707v4.
Source code and data: https://github.com/compops/pmh-tutorial
Summary: We provide a gentle introduction to the pmh algorithm for parameter inference
in non-linear ssms. Throughout this paper, we develop an implementation of the pmh algo-
rithm in the statistical programming language R. We provide the reader with some intuition

10
1
Introductory overview
for how the algorithm operates and provide some solutions to numerical problems that
might occur in practice. Furthermore, we make use of the implementation for parameter
inference in models using real-world data and provide a small survey of the ﬁeld.
Background and contribution: The idea for the paper originated from Thomas Schön
during the spring of 2015. The main aim was to provide an overview of the pmh algorithm
together with step-by-step instructions on how to implement it in some common program-
ming languages. The paper was written during the autumn of 2015 and example code for
MATLAB, R and Python are provided via GitHub. The author of this thesis wrote most of
the paper, made all implementations in software and carried out all numerical experiments.
Paper B
Paper B of this thesis is an edited version of
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using
gradient and Hessian information. Statistics and Computing, 25(1):81–92, 2015b.
which is a development of the two earlier contributions:
J. Dahlin, F. Lindsten, and T. B. Schön. Second-order particle MCMC for
Bayesian parameter inference. In Proceedings of the 19th IFAC World Congress,
Cape Town, South Africa, August 2014a.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis Hastings using
Langevin dynamics. In Proceedings of the 38th International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), Vancouver, Canada, May 2013a.
Source code and data: https://github.com/compops/pmh-stco2015
Summary: We develop an extension to the standard pmh algorithm, which incorporates
information about the gradient and the Hessian of the log-posterior into the parameter
proposal. This information can be extracted from the output generated by the particle ﬁlter
when estimating the likelihood. The gradient information is used to add a drift in the pro-
posal towards areas of high posterior probability. The Hessian information is useful to scale
the step lengths of the proposal to improve the exploration of non-isotropic posteriors. We
provide numerical experiments that indicates that the novel proposal makes the algorithm
scale invariant, increases mixing and decreases the number of pilot runs.
Background and contribution: The idea for the ﬁrst paper originated from Fredrik Lind-
sten during the autumn of 2012. The paper was developed in three stages and resulted in a
journal publication after an invitation to a special issue connected to new results presented
at the workshop mcmski 2014. The ﬁrst paper only made use of gradient information and
was a proof of concept. Hessian information was added in the second paper and another
particle smoother was used to decrease the computational cost. In the ﬁnal paper, we in-
troduced an improved method for estimating the gradient and Hessian together with an
approach to handle cases when the estimate of the Hessian is not a valid covariance matrix.
The author of this thesis wrote most parts of the conference papers, about half of the journal
paper, made all implementations in software and carried out all numerical experiments. A
similar idea was developed independently by Nemeth et al. (2014) during the same period.

1.3
Thesis outline
11
Paper C
Paper C of this thesis is an edited version of
J. Dahlin, F. Lindsten, and T. B. Schön. Quasi-Newton particle Metropolis-
Hastings. In Proceedings of the 17th IFAC Symposium on System Identiﬁcation
(SYSID), pages 981–986, Beijing, China, October 2015c.
Source code and data: https://github.com/compops/qpmh2-sysid2015
Summary: We develop the ideas from Paper B further by constructing an estimate of the
Hessian directly from gradient information. This is useful as it often is diﬃcult to obtain
positive semi-deﬁnite estimates of the Hessian using particle smoothing. This problem can
be mitigated by increasing the number of particles but this results in that the computational
cost also increases. Instead, we propose to construct a local approximation of the Hessian
using ideas from quasi-Newton optimisations, which often results in a positive semi-deﬁnite
estimate. The novel approach only requires estimates of the gradient, which usually are
more accurate compared with the estimates of the Hessian. We make use of the algorithm
for inference in a challenging class of models known as ssms with intractable likelihoods.
The results indicate that the proposed algorithm can in some cases increase the mixing by a
factor of four, when the gradients can be accurately estimated.
Background and contribution: The idea for the paper originated from the author of this
thesis during the spring of 2014, when preparing an example in the presentation for the de-
fence of his Licentiate’s thesis. The proposed algorithm is an attempt to increase the mixing
of pmh when the likelihood is estimated using particle ﬁltering with approximate Bayesian
computations (abc). It was later used to compare with the approximate method proposed
in Paper E. The author of this thesis wrote most of the paper, made all implementations in
software and carried out all numerical experiments.
Paper D
Paper D of this thesis is an edited version of
J. Dahlin, F. Lindsten, J. Kronander, and T. B. Schön. Accelerating pseudo-
marginal Metropolis-Hastings by correlating auxiliary variables.
Pre-print,
2015a. arXiv:1512.05483v1.
Source code and data: https://github.com/compops/pmmh-correlated2015
Summary: The standard formulation of the pmmh algorithm makes use of independent
estimators for the value of the target distribution. However, in theory we can increase
the acceptance rate of the algorithm by introducing a positive correlation between two
consecutive target estimates. We explore this idea by introducing a Crank-Nicolson proposal
for the random variables which are used to construct the estimator of the target. We provide
some numerical experiments indicating that this small change in the pmmh algorithm can
increase mixing and allow for a decrease in the number of particles. The typical increase
in mixing results in that the number of iterations can be decreased to a third compared
with using non-correlated random variables. Furthermore, we can often also decrease the

12
1
Introductory overview
number of random variables in the estimator, which results in a further decrease of the
computational cost.
Background and contribution: The original idea for the paper originated from discussions
between the author of this thesis and Joel Kronander during the summer of 2015. The idea
was then extended and reﬁned during discussions with Fredrik Lindsten during the autumn
of 2015. The author of this thesis wrote about half of the paper, made all implementations
in software and carried out all numerical experiments. A similar idea was developed inde-
pendently by Deligiannidis et al. (2015) published on the pre-print library arXiv one day
before our own paper.
Paper E
Paper E of this thesis is an edited version of
J. Dahlin, M. Villani, and T. B. Schön. Eﬃcient approximate Bayesian inference
for models with intractable likelihoods. Pre-print, 2015d. arXiv:1506.06975v1.
which is the development of the two earlier contributions:
J. Dahlin and F. Lindsten. Particle ﬁlter-based Gaussian process optimisation
for parameter inference. In Proceedings of the 19th IFAC World Congress, Cape
Town, South Africa, August 2014.
J. Dahlin, T. B. Schön, and M. Villani. Approximate inference in state space
models with intractable likelihoods using Gaussian process optimisation. Tech-
nical Report LiTH-ISY-R-3075, Department of Electrical Engineering, Linköping
University, Linköping, Sweden, April 2014b.
Source code and data: https://github.com/compops/gpo-abc2015
Summary: We propose a method for approximate Bayesian inference in ssms with in-
tractable likelihoods. The posterior in this type of models can be approximated point-wise
using abc. However, the resulting sequential Monte Carlo algorithm with abc (smc-abc)
for approximating the likelihood in ssms often requires more particles than the standard
smc implementation to achieve reasonable accuracy. To decrease the resulting large com-
putational cost, we propose a combination of smc-abc and Gaussian process optimisa-
tion (gpo) to estimate the parameters by maximising a surrogate function mimicking the
posterior distribution. We provide numerical experiments indicating that the constructed
surrogate function is similar to the true posterior around the mode and results in similar
parameter estimates. Furthermore, the use of gpo can decrease the computational cost
with one or two orders of magnitude compared with the pmh algorithm.
Background and contribution: The original idea was proposed by Fredrik Lindsten during
the summer of 2013. In the ﬁrst paper, we combined gpo with a standard particle ﬁlter for
maximum likelihood estimation in ssms. During the fall of 2013, the author of this thesis
attended a course in Bayesian inference given by Mattias Villani. The idea of making use
of gpo in combination with abc was born during this course and resulted in a technical
report as part of the course project. This report was reworked and extended twice to its

1.3
Thesis outline
13
current form during the spring of 2015. The author of this thesis wrote most parts of the
papers, made all implementations in software and carried out all numerical experiments.
A similar idea was developed independently by Gutmann and Corander (2015) during the
same period.
Paper F
Paper F of this thesis is an edited version of
J. Dahlin, F. Lindsten, T. B. Schön, and A. Wills. Hierarchical Bayesian ARX
models for robust inference. In Proceedings of the 16th IFAC Symposium on
System Identiﬁcation (SYSID), Brussels, Belgium, July 2012b.
Source code and data: https://github.com/compops/rjmcmc-sysid2012
Summary: Gaussian innovations are the typical choice in most arx models but using
other distributions such as the Student’s t could be useful. We demonstrate that this choice
of distribution for the innovations provides an increased robustness to data anomalies, such
as outliers and missing observations. We consider these models in a Bayesian setting and
perform inference using numerical procedures based on Markov chain Monte Carlo (mcmc)
methods. These models include automatic order determination by two alternative methods,
based on a parametric model order and a sparseness prior, respectively. The methods and
the advantage of our choice of innovations are illustrated in three numerical studies using
both simulated data and real eeg data.
Background and contribution: The original idea was proposed by Fredrik Lindsten during
the autumn of 2011. It was the ﬁrst project undertaken by the author of this thesis during
his PhD studies. The author of this thesis wrote the latter half of the paper, made some
implementations in software and carried out most of the numerical experiments. The
eeg data was kindly provided by Eline Borch Petersen and Thomas Lunner at Eriksholm
Research Centre, Oticon A/S, Denmark.
Paper G
Paper G of this thesis is an edited version of
J. Dahlin, R. Kohn, and T. B. Schön. Bayesian inference for mixed eﬀects
models with heterogeneity. Technical Report LiTH-ISY-R-3091, Department
of Electrical Engineering, Linköping University, Linköping, Sweden, March
2016.
Source code and data: https://github.com/compops/panel-dpm2016
Summary: Mixture modelling is an important problem in many scientiﬁc ﬁelds. In this
paper, we are interested in modelling panel data, i.e., a few sequential observations gathered
from many individuals. This type of data sets provides little information about a speciﬁc
individual and the main challenge is to pool information from similar individuals to obtain
accurate estimates of the parameters of the model. We compare two diﬀerent approaches

14
1
Introductory overview
to pool the individuals together using a Dirichlet process mixture (dpm) and a ﬁnite mix-
ture model with a sparseness prior. In this setting, we can see the latter approach as an
approximation of the dpm, which results in simpler and sometimes more eﬃcient inference
algorithms. We conclude via numerical experiments that the posteriors obtained from the
two approaches are very similar. Therefore, the approximate model can be beneﬁcial for
inference in big data problems.
Background and contribution: The idea of the paper originated from discussions between
the author of this thesis and Robert Kohn during the autumn of 2014. Some preliminary
work was carried out during author’s PreDoc at University of New South Wales Business
School in Sydney, Australia. The present paper is the result of work during the spring
of 2016. The author of this thesis wrote most of the paper, made all implementations in
software and carried out all numerical experiments.
Paper H
Paper H of this thesis is an edited version of,
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. On robust input
design for nonlinear dynamical models. Automatica, 2016a. (provisionally
accepted).
which is a development of the earlier contribution
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. A graph/particle-
based method for experiment design in nonlinear systems. In Proceedings of
the 19th IFAC World Congress, Cape Town, South Africa, August 2014.
Summary: Input design is an important sub-ﬁeld of system identiﬁcation. Its main aim is
to to determine an input that maximises a scalar function of the Fisher information matrix.
In this work, we make use of graph theory to create a model for the input signal based on a
convex combination of diﬀerent basis inputs. The resulting input signal is given as a solution
to an optimisation problem, which depends on estimates of the Fisher information matrix
for each basis input. We develop a particle smoothing technique to obtain these estimates
in a more eﬃcient and accurate manner than previously. Finally, we present numerical
illustrations indicating that the use of the designed input decreases the uncertainty in the
estimates and improves the convergence speed of the expectation maximisation algorithm.
Background and contribution: The idea of the ﬁrst paper originated from discussions
between the author of these papers during the spring of 2013. The main aim was to combine
recent developments in particle smoothing with input design. This idea was implemented
in the ﬁrst paper as a proof of concept. It was later extended in a second paper with a robust
formulation and a better estimator of the Fisher information matrix. The author of this
thesis wrote parts of the sections regarding particle methods in two the papers, made all
particle-based implementations in software and carried out most of experiments.

1.4
Publications
15
Publications
Published work of relevance to this thesis are listed below in reverse chronological order.
Items marked with ⋆are included in Part ii.
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. Particle-based Gaus-
sian process optimization for input design in nonlinear dynamical models.
Pre-print, 2016b. arXiv:1603.05445v1.
⋆J. Dahlin, R. Kohn, and T. B. Schön. Bayesian inference for mixed eﬀects
models with heterogeneity. Technical Report LiTH-ISY-R-3091, Department
of Electrical Engineering, Linköping University, Linköping, Sweden, March
2016.
⋆P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. On robust input
design for nonlinear dynamical models. Automatica, 2016a. (provisionally
accepted).
⋆J. Dahlin and T. B. Schön. Getting started with particle Metropolis-Hastings
for inference in nonlinear models. Pre-print, 2015. arXiv:1511.01707v4.
⋆J. Dahlin, F. Lindsten, J. Kronander, and T. B. Schön. Accelerating pseudo-
marginal Metropolis-Hastings by correlating auxiliary variables.
Pre-print,
2015a. arXiv:1512.05483v1.
A. Svensson, J. Dahlin, and T. B. Schön.
Marginalizing Gaussian process
hyperparameters using sequential Monte Carlo. In Proceedings of the 6th IEEE
International Workshop on Computational Advances in Multi-Sensor Adaptive
Processing (CAMSAP), Cancun, Mexico, December 2015.
⋆J. Dahlin, F. Lindsten, and T. B. Schön. Quasi-Newton particle Metropolis-
Hastings. In Proceedings of the 17th IFAC Symposium on System Identiﬁcation
(SYSID), pages 981–986, Beijing, China, October 2015c.
M. Kok, J. Dahlin, , T. B. Schön, and A. Wills. Newton-based maximum
likelihood estimation in nonlinear state space models. In Proceedings of the
17th IFAC Symposium on System Identiﬁcation (SYSID), pages 398–403, Beijing,
China, October 2015.
T. B. Schön, F. Lindsten, J. Dahlin, J. Wågberg, C. A. Naesseth, A. Svensson,
and L. Dai. Sequential Monte Carlo methods for system identiﬁcation. In
Proceedings of the 17th IFAC Symposium on System Identiﬁcation (SYSID), pages
775–786, Beijing, China, October 2015.
⋆J. Dahlin, M. Villani, and T. B. Schön. Eﬃcient approximate Bayesian inference
for models with intractable likelihoods. Pre-print, 2015d. arXiv:1506.06975v1.
⋆J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using
gradient and Hessian information. Statistics and Computing, 25(1):81–92, 2015b

16
1
Introductory overview
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. A graph/particle-
based method for experiment design in nonlinear systems. In Proceedings of
the 19th IFAC World Congress, Cape Town, South Africa, August 2014
J. Dahlin and F. Lindsten. Particle ﬁlter-based Gaussian process optimisation
for parameter inference. In Proceedings of the 19th IFAC World Congress, Cape
Town, South Africa, August 2014.
J. Dahlin, F. Lindsten, and T. B. Schön. Second-order particle MCMC for
Bayesian parameter inference. In Proceedings of the 19th IFAC World Congress,
Cape Town, South Africa, August 2014a.
J. Dahlin. Sequential Monte Carlo for inference in nonlinear state space models.
Licentiate’s thesis no. 1652, Linköping University, May 2014.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis Hastings using
Langevin dynamics. In Proceedings of the 38th International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), Vancouver, Canada, May 2013a.
⋆J. Dahlin, F. Lindsten, T. B. Schön, and A. Wills. Hierarchical Bayesian ARX
models for robust inference. In Proceedings of the 16th IFAC Symposium on
System Identiﬁcation (SYSID), Brussels, Belgium, July 2012b.
Other publications not included in the thesis are:
J. Kronander, J. Dahlin, D. Jönsson, M. Kok, T. B. Schön, and J. Unger. Real-
time video based lighting using GPU raytracing. In Proceedings of the 2014
European Signal Processing Conference (EUSIPCO), Lisbon, Portugal, September
2014a.
J. Kronander, T. B. Schön, and J. Dahlin. Backward sequential Monte Carlo for
marginal smoothing. In Proceedings of the 2014 IEEE Statistical Signal Processing
Workshop (SSP), Gold Coast, Australia, July 2014b.
D. Hultqvist, J. Roll, F. Svensson, J. Dahlin, and T. B. Schön. Detection and
positioning of overtaking vehicles using 1D optical ﬂow. In Proceedings of the
IEEE Intelligent Vehicles (IV) Symposium, Dearborn, USA, June 2014.
J. Dahlin and P. Svenson. Ensemble approaches for improving community
detection methods. Pre-print, 2013. arXiv:1309.0242v1.
J. Dahlin, F. Lindsten, and T. B. Schön. Inference in Gaussian models with miss-
ing data using equalisation maximisation. Pre-print, 2013b. arXiv:1308.4601v1.
J. Dahlin, F. Johansson, L. Kaati, C. Mårtensson, and P. Svenson. A method for
community detection in uncertain networks. In Proceedings of International
Symposium on Foundation of Open Source Intelligence and Security Informatics
2012, Istanbul, Turkey, August 2012a.
J. Dahlin and P. Svenson. A method for community detection in uncertain
networks. In Proceedings of 2011 European Intelligence and Security Informatics
Conference, Athens, Greece, August 2011.

2
Bayesian modelling and inference
Bayesian modelling and inference is a popular and growing tool in statistics, machine learn-
ing and data mining. It is one of the two dominating perspectives used in probabilistic
modelling and has certain interesting features for handling over-ﬁtting, prior information
and uncertainty, which can be useful in applications. Bayesian statistics has its origins
with the English reverend Thomas Bayes [1701-1761]. He discussed the ﬁrst known use of
Bayesian inference in Bayes (1764) for the Bernoulli model with what is now known as a
uniform prior. However, the general formulation and many important theorems are due to
the French mathematician Pierre-Simon Laplace [1749-1827]. He proposed the well-known
theorem named after Bayes in Laplace (1886). As a consequence, Bayesian inference is also
known as Laplacian statistics or inverse probability.
However, the popularity of Bayesian inference faded during the early 20th century when
the English statistician Ronald Fisher [1890-1962] proposed the Frequentisitic paradigm
for statistical inference. This view is based on the optimisation of the likelihood function,
which was ﬁrst proposed in Fisher (1922). The resulting method is known as maximum
likelihood and can be carried out in closed-form for many interesting applications. This
in contrast with Bayesian inference, which often is analytically intractable and requires
approximations to compute estimates. This is perhaps the reason that Bayesian statistics
took the back seat in statistics for some time.
This changed with the advent of the electronical computer in the 1940s and onwards. Com-
putational methods known as statistical simulation started to be applied to approximate
the estimates obtained by Bayesian inference. Monte Carlo methods emerged as a good
alternative for solving integration problems in high dimensions. These methods eventually
found their use in Bayesian inference during the 1980s as most problems in this paradigm
are posed as problems of computing integrals.
17

18
2
Bayesian modelling and inference
Figure 2.1. The basic inference process.
This chapter provides an overview of Bayesian modelling and inference with the aim to
introduce the statistical inference process following the steps presented in Figure 2.1. The
ﬁrst step in any inference procedure is to collect data from the system of interest, e.g., a
machine, the weather, the stock market or a human body. To describe the data, we require
a statistical model which usually depends on a number of unknown parameters. In the last
step, we make use of the data to infer the parameters of interest. After the model has been
inferred, we can make use of it to make forecasts or for making decisions by taking the
uncertainty into account.
Furthermore, we introduce two examples of applications in this chapter and these are
analysed throughout the introductory part of this thesis. We also give an overview of non-
parametric methods, where the number of parameters is inﬁnite or grows with the number
of observations. This property makes this type of models more ﬂexible compared with
the aforementioned parametric type of models. We conclude this chapter by providing the
reader with an outlook and references for further study.
Three examples of statistical data
The ﬁrst step in constructing a probabilistic model of a phenomenon is to collect data from
the system, individual or some other source. In this section, we discuss three diﬀerent types
of data: (i) cross-sectional, (ii) time series and (iii) panel/longitudinal. We also presents two
data sets that are later used to exemplify modelling and inference using diﬀerent approaches.
Cross-sectional data
In cross-sectional data, we obtain a collection of observations y = {yi}n
i=1 from n diﬀerent
sources/individuals. Furthermore, we often assume that these observations are independent
from each other and that they are recorded at the same time or that the observations are
independent of time. Three examples of cross-sectional data are: (i) the length of students
in a class room, (ii) the monthly wages at a company and (iii) the chemical factors that
inﬂuence the quality of wine.
These observations are typically recorded together with additional information which is
assumed to be able to explain the outcome. In the wage example, we would like to take into
account the age, the educational background and the number of years that each person has
worked for the company. We usually refer to the observation yi as the dependent variable
and the additional information as the independent or explaining variables. The independent
variables are denoted by xi = {xij}p
j=1, where p is the number of diﬀerent attributes
recorded for each observation yi. A typical question that the statistician would like to

2.1
Three examples of statistical data
19
answer is which independent variables inﬂuence the observation and by how much. We
return to modelling this type of data using a regression model in Section 2.2.1.
Time series data
In time series data, we obtain multiple sequential observations {yt }T
t=1 from a single source
or individual. We typically assume that the observations are dependent and that the correla-
tion increases when the observations are closer (in time) to each other. The main objective
is therefore to capture this correlation using a statistical model. Three typical examples of
time series data are: (i) the ice varve thickness from Section 1.1.1, (ii) the price of coﬀee
beans in Papers C and F and (iii) the blood sugar level of a patient. Another type of time
series data is presented in Example 2.1.
We often assume that the current observation can be explained by previous observations.
However, we can also add independent variables as in the cross-sectional case. For the blood
sugar example, it can be useful to also take into account the amount of physical exercise,
what the patient has eaten and if he/she is a diabetic when trying to forecast the future
blood sugar level. We refer to these variables as the exogenous variables and denote them by
ut = {ut j}p
j=1. A typical problem that the statistician would like to solve is to determine the
value of yt+m given {yt, ut }T
t=1 for m > 0. That is, to make a so-called m-step predication
of the observation given all the previous observations and independent variables available
at the present. We return to model this type of data using two diﬀerent time series models
in Sections 2.2.2 and 2.2.3.
Example 2.1: How does unemployment aﬀect inﬂation?
Consider the scenario that the Swedish parliament has launched a big campaign against
unemployment. The unemployment rate is expected to decrease rapidly during the coming
24 months. At the same time the Swedish Riksbank (central bank) is worried that this
might increase the inﬂation rate above its two percent target. They would like us to analyse
this scenario by providing them with a forecast to determine if any action is required.
The reasoning of the Riksbank is based on the Phillips curve hypothesis proposed by
Phillips (1958). It states that there is an inverse relationship between unemployment and
inﬂation rates in the short run. That is, a rapid decrease in unemployment tends to correlate
with a increased rate of inﬂation. The intuition for this is that it is diﬃcult for companies
to attract workers if the unemployment rate is too low. As a consequence, the employees
gain bargaining power which results in increased wages and therefore increased inﬂation as
well. The opposite occurs when the unemployment rate is high as it is easy for companies
to recruit new workers. Therefore, no wage increases are required to attract new workers.
Furthermore, the Phillips curve assumes that there exists an equilibrium point in the un-
employment known as the non-accelerating inﬂation rate unemployment (nairu) or the
natural rate of unemployment. The reason for a non-zero nairu is the matching problem
on the labour market. That is, not all individuals can take any available position due to
e.g., geographical or educational constraints. The nairu determines if the inﬂation rate
increases or decreases given the current unemployment rate. The inﬂation increases if the

20
2
Bayesian modelling and inference
0
5
10
15
date
inﬂation rate
1987
1991
1995
1999
2003
2007
2011
2015
0
5
10
15
date
unemployment rate
1987
1991
1995
1999
2003
2007
2011
2015
Figure 2.2. The inﬂation rate (upper) and unemployment rate (lower) for Sweden during the
period January, 1987 to December, 2015. The purple areas indicate the ﬁnancial crises of 1991-1992
and 2008-2010. The data is obtained from Statistiska centralbyrån (SCB).

2.1
Three examples of statistical data
21
unemployment rate is smaller than the nairu and vice versa. Estimating the nairu is
therefore important for making predictions of the future inﬂation.
In Figure 2.2, we present the unemployment and inﬂation rates in Sweden between January,
1987 and December, 2015. The data is obtained from Statistiska centralbyrån1 (scb), which
is the governmental agency responsible for collecting statistics in Sweden. We note that the
inﬂation rate changed rapidly during the ﬁnancial crises in 1991-1992 and 2008-2010 and at
the same time the unemployment rate increased. This suggests that a negative correlation
between these two variables could exist. We would like to determine the support for this
claim using a probabilistic model, which also is required to make the forecast asked for by
the Riksbank.
We return to this data set in Example 2.3 on page 27.
Panel and longitudinal data
Panel data (also known as longitudinal data) can be seen as a combination of time series
and cross-sectional data. In this setting, we typically obtain a data set y = {{yit }n
i=1}T
t=1
for individual i at time t with T ≪n. We assume that the observations are independent
between individuals but correlated between observations of the same individual. For each
observation yit of individual i at time t, we usually record p independent variables denoted
by {xijt } for j = 1, . . ., p.
One example of panel data is socio-economic studies such as the Sozio-oekonomisches Panel
(soef2), where a selection of German households have been interviewed annually since
1984. Topics included in the annual interviews are economical factors such as employment
and earnings as well as social factors such as family composition, health and general life
satisfaction. Analysis of such data is important to e.g., investigate how household incomes
correlate with university attendance. This can be useful to guide interventions and policy
decisions considered by the government.
Two other applications of panel data were already discussed in Chapter 1: (i) genome-
wide association studies and (ii) recommendation systems. In application (i), scientists are
making use of rapid scanners to search for markers connected with a particular disease in a
dna sample, see Bush and Moore (2012), Zhang et al. (2010) and Yang et al. (2014). This
information is useful for diagnoses, treatment and prevention of e.g., cancer, diabetes and
heart disease. In application (ii), the dynamics of users’ preferences can be seen as panel
data, see Condliﬀet al. (1999) and Stern et al. (2009). We return to discussing how to model
panel data in Section 2.2.4.
Example 2.2: Voting behaviour in the US Supreme court
In February 2016, the conservative justice Antonin Scalia of the us Supreme Court justice
died and a replacement is therefore required. The us President Barack Obama is faced with
a dilemma to either appoint a new justice with the same ideological leaning or one who
is more liberal. The us Supreme Court is made up by nine diﬀerent justices, which are
1See http://www.scb.se/en_/ for more information.
2See http://www.diw.de/en/soep for more information.

22
2
Bayesian modelling and inference
case
1
171
Scalia
Kennedy
Thomas
Ginsburg
Breyer
Roberts
Alito
Sotomayor
Kagan
Figure 2.3. The rulings in T = 171 non-unanimous cases in the US Supreme Court during the
terms between 2010 and 2015. Liberal votes are indicated by coloured ﬁelds and conservative by
white for each justice and case.

2.2
Parametric models
23
nominated by the President and approved by the Senate. The nomination is an important
political decision as each justice often serves for the remainder of his/her life or until he/she
resigns. The political view of each justice can therefore inﬂuence rulings during many years.
How will appointing a more liberal judge aﬀect the rulings of the court?
We consider the data set provided by Spaeth et al. (2016) of T = 171 non-unanimous rules
from the terms between 2010 and 2015. At the time, the supreme court justices were: Kagan,
Sotomayor, Alito, Roberts, Breyer, Ginsburg, Thomas, Kennedy and Scalia. The vote of
each justice is categorised as either liberal (1) or conservative (0) depending on the topic at
hand. The data set is presented in Figure 2.3 for each justice and case, respectively.
We would like to model the relative level of conservatism/liberalism between the justices.
A quick look at the data seems to indicate that (i) Kagan, Sotomayor, Breyer and Ginsberg
seem to be more liberal and (ii) Alito, Thomas and Scalia seem to be more conservative.
However, we would like to verify this using a probabilistic model. This model can later be
used to simulate votes by each justice to estimate the ideological leaning of the court.
We return to this data set in Example 2.4 on page 28.
Parametric models
The next step in probabilistic modelling after the data has been collected is to choose a
suitable model structure. In this section, we present a few diﬀerent structures aimed at
modelling the three diﬀerent kinds of statistical data discussed in the previous section. All
of the models presented are members of the family of parametric models. Hence, we assume
that they are speciﬁed by a ﬁnite number of parameters denoted by θ ∈Θ ⊂Rp, where Θ
denotes the parameter space which we typically assume to be the p-dimensional real space.
The choice of model structure is often diﬃcult and greatly inﬂuences the predictions and
decisions made from the combination of the data and the model. Hence, model choice is
an important problem in statistics but it is not discussed at any length in this thesis. Two
approaches are likelihood ratio tests and Bayes factors, see Casella and Berger (2001) and
Robert (2007) for more information.
Linear regression and generalised linear models
Generalised linear models (glms) are the work-horse of statistical modelling of cross-
sectional data. This is a type of regression model, where we would like to infer the ob-
servation (dependent) variable given the independent variables. The latter are typically
referred to as the regressors in this model. The basic glm is given by the linear regression
model, which can be expressed by
yi = β0 +
p
X
j=1
βj xij + σei,
(2.1)
where it is typically assumed that errors are independent and distributed according to the
standard Gaussian distribution, i.e., ei ∼N(0, 1). Note that this implies that the noise

24
2
Bayesian modelling and inference
variance is constant for all the observations, i.e., the errors are homoscedastic. Furthermore,
we assume that the regressors are linearly independent of each other and with the error, i.e.,
E[xijei] = 0 for every i and j.
In this model, the parameters are given by θ = {β0:p, σ}, where β0 ∈R determines the
so-called intercept (or bias) of the model. The standard deviation of the noise is determined
by σ > 0. The remaining parameters β1:p ∈Rp determines the linear relationship between
the regressors and the observation. A standard method for estimating θ from data is to
make use of the least squares (ls) approach. The main objective in ls is to minimise the
squared prediction error, i.e.,
Dβls ≜argmin
β∈Θ
∥y −X β∥2
2,
(2.2)
where ∥· ∥2 denotes the L2-norm. Here, we introduce
y =

y1
y2
...
yn

,
X =

1
x11
x12
· · ·
x1p
1
x21
x22
· · ·
x2p
...
...
...
...
...
1
xn1
xn2
· · ·
xnp

,
β =

β0
β1
β2
...
βp

.
The solution to (2.2) can be computed using a closed-form expression (Casella and Berger,
2001) given by
Dβls =  X ⊤X−1 X ⊤y,
(2.3)
which we refer to as the normal equations. The noise variance σ2 can be estimated directly
using the standard sample estimator by
s2 = Dσ2 =
1
n −p −1
n
X
i=1

yi −Xi Dβls
2,
(2.4)
where Xi denotes row i of the matrix X. It is possible to show that the ls estimator is
the best linear un-biased estimator (blue) when the assumptions of the error term and the
independence between the error and the regressors are fulﬁlled. This result is known as the
Gauss-Markov theorem (Casella and Berger, 2001; Lehmann and Casella, 1998) and it holds
for other types of linear models as well.
An alternative to the least squares formulation (2.2) is the elastic net (Zou and Hastie, 2005;
Hastie et al., 2009). The resulting loss function is given by
Dβelastic ≜argmin
β∈Θ
∥y −X β∥2
2 + λ1∥β∥1 + λ2∥β∥2
2,
(2.5)
where ∥· ∥1 denotes the L1-norm and λ1, λ2 > 0 denote tuning parameters. This type of
loss function is sometimes referred to as regularised least squares (rls) and is particularly
useful for the case p > n, i.e., when we have more parameters than observations. We can
recover two important special cases from (2.5): (i) l1-rls when λ2 = 0 and (ii) l2-rls
when λ1 = 0.
The advantage of this alternative formulation is that it penalises the inclusion of regressors

2.2
Parametric models
25
that does not contribute to explain the observations. That is, these regression coeﬃcients are
shrunk towards zero and therefore the regressors are practically removed from the model.
This is a form of model selection, which as previously mentioned is a challenging issue.
Another popular name for (i) is the lasso (Tibshirani, 1996) and it has the property to
shrink regression coeﬃcients to be exactly zero. The drawback with the lasso is bad
performance when the regressions exhibit multicollinearity, i.e., linear dependence between
some regressors. In this case, a better alternative is (ii) known also as ridge regression intro-
duced in statistics by Hoerl and Kennard (1970). The drawback with this regularisation is
that is only shrinks the coeﬃcients towards zero and do not remove them completely. We
return to the use of regularisation for automatic model order selection in Chapter 4.
In the linear regression model, we assume that the response yi is a continuous random
variable. However, many other types of responses can be found in applications. Some
examples are: (a) binary response (success/failure), (b) Bernoulli response (no. successes in
M attempts) or (c) count response (no. occurrences during some time period). The glm is
a useful approach to model these kinds of observations. This is done by transforming the
linear predictor ηi = Xi β with a so-called link function ℎsuch that E[yi] = ℎ−1(ηi). For
example in (a), we can use the logistic function or the Gaussian cumulative distribution
function (cdf) to map the linear predictor onto the unit interval (0, 1).
Autoregressive models
There are a large number of diﬀerent models for time series data. The simplest is probably
the autoregressive process of order p denoted by ar(p), see e.g., Brockwell and Davis (2002).
This model can be expressed using densities by
yt |yt−p:t−1 ∼N *.
,
yt; µ +
p
X
k=1
φk
 yt−k −µ, σ2+/
-
,
(2.6)
where yt denotes the observation at time t. The model is speciﬁed by the parameters
θ = {µ, φ1:p, σ} and the noise is assumed to be independent and Gaussian. The latter
can be relaxed to account for outliers by assuming Student’s t distributed noise, which is
considered in Paper F. Note that we make use of densities to deﬁne the ar process, which is
slightly diﬀerent from the equation form of the ls model. However, it is possible to rewrite
the ar process on the diﬀerence form corresponding to the ls model (2.1) and vice versa.
In the ar model, the model order p ∈N inﬂuences the number of past observations
included into the model. Therefore, p together with φ determines the persistence and
correlation structure of the process. The mean of the observations is determined by µ ∈R
and the standard deviation of the noise is determined by σ > 0. We require that all the
poles of the characteristic polynomial
qp −
p
X
k=1
φkqp−k = 0,
lie within the unit circle to obtain a stable ar(p) process, i.e., it does not diverge to inﬁnity
when T increases. Here, q denotes the back shift (lag) operator.

26
2
Bayesian modelling and inference
Given the order p, we can reformulate the problem of estimating θ in (2.6) when µ = 0
as a ls problem using the observations y1:T . The estimates are obtained by rewriting the
model to obtain
y =

yp+1
yp+2
...
yT

,
X =

yp
yp−1
· · ·
y1
yp+1
yp
· · ·
y2
...
...
...
...
yT
yT −1
· · ·
yT −p

,
φ =

φ1
φ2
...
φp

.
In this case, the ls estimate corresponds to the maximum likelihood estimate. However,
it is also possible to estimate the parameters in a Bayesian setting. Furthermore, we can
apply regularisation for selecting the model order using a loss function similar to (2.5). We
investigate this in Paper F for ar with exogenous inputs (arx) models, where the known
input {ut }T
t=1 (possibly lagged) is included in (2.6).
State space models
In the ar model, we obtained direct observations of the quantity of interest y1:T . In some
cases, we cannot directly observe the cause of the observation as it is a function or random
variable depending on some latent variables. A standard model for time series data using
latent variables is the ssm, which is also known as the hidden Markov model (hmm). This
type of model is used e.g., in statistics (Brockwell and Davis, 2002; Langrock, 2011), control
(Ljung, 1999), econometrics (Durbin and Koopman, 2012) and ﬁnance (Tsay, 2005).
An ssm with latent states x0:T and observations y1:T can be expressed as
x0 ∼µθ(x0),
xt | xt−1 ∼fθ(xt | xt−1),
yt | xt ∼gθ(yt | xt),
(2.7)
where θ denotes the parameters of the model. Here, we assume that the model can be
described by probability density functions (pdfs) denoted µθ, fθ and gθ.
We say that the ssm is fully dominated (by the Lebesgue measure) when we can write the
model on the form in (2.7). That is, when we can ﬁnd a density for each of the Markov
kernels in the model. In practice, this can often be done when the states and observations
are real-valued and the state or observation equations are not deterministic. However, the
methods presented in this thesis can be applied even when the densities are degenerate (states
are deterministic) and when the states/observations are integers. The reason for adopting
the density formulation is to keep the notation simple and avoid the measure-theoretic
formulation of stochastic processes.
The parameters of interest in the ssm are the latent states x0:T and the parameters of the
densities θ. We refer to the problem of estimating the former as the state inference problem
and the latter as the parameter inference problem. For this type of model, we cannot form
a simple optimisation problem as for the ls or ar models as the states are not directly
observed. Instead, we require more advanced maximum likelihood or Bayesian inference
methods to solve these two inference problems jointly. We return to this in Section 2.3.

2.2
Parametric models
27
Example 2.3: How does unemployment aﬀect inﬂation? (cont. from p. 19)
We consider the Phillips curve with non-linear dynamics and rational expectations intro-
duced by Zhou (2013) to model the Swedish data and to make forecasts. Let yt and ut
denote the inﬂation rate and unemployment rate (in percent) at time t. Furthermore, let xt
denote the nairu (the equilibrium point in the unemployment rate), which changes with
time depending on previous rates of inﬂation and unemployment. We can write a slightly
modiﬁed version of this model as an ssm given by
x0 ∼N

x0; 2, 4

,
(2.8a)
xt | xt−1 ∼N

xt; φxt−1 + µ(xt−1), σ2
v(xt−1, ut−1)

,
(2.8b)
yt | xt ∼N

yt; yt−1 + β ut −xt
, σ2
e

.
(2.8c)
We introduce the mean function of the state process and its variance function given by
µ(xt−1) ≜α
f
1 + exp   −xt−1
g−1,
σ−1
v (xt−1, ut−1) ≜1 + exp
f
−ut−1 −xt−1
g
.
The parameters of this Phillips curve model are θ = {α, β, φ, σe}. Here, α ∈R and
φ ∈(−1, 1) determine the mean and persistence of the state, respectively. The inﬂation rate
is determined by β ∈R and σe > 0. The parameter β is of great interest to us as its sign
determines the correlation between the inﬂation and the unemployment gap (the diﬀerence
between the unemployment rate and the state). The Phillips curve hypothesis suggests that
this parameter is negative.
We continue by analysing the mean and the variance of the state process to obtain some
insight into the model and its dynamics. Note that the term µ(xt−1) makes the state process
mean-reverting. That is, the average value of the process is given by µ(xt−1) and therefore
the process occasionally reverts to this value. Furthermore, µ(xt−1) can vary between
µ →α, when xt−1 ≫0,
µ →α
2 , when xt−1 ≈0.
That is, if xt grows large, then the long-term mean of the process also grows, making
it diﬃcult for xt to decrease again at a later stage. Hence, the state is sticky and if the
unemployment rate is larger than the state of the process (the nairu) for a long time,
then the mean of the latter grows. An explanation for this eﬀect is that companies tend to
streamline their organisations at the same time as employees are laid oﬀ, which can increase
the matching problem.
For the noise standard deviation, we have
σv →0.5, when |ut−1 −xt−1| →0,
σv →1.0, when |ut−1 −xt−1| →∞,
so the process noise decreases as the unemployment rate ut−1 approach the nairu xt−1.
The reason for this is that the nairu (according to some economists) can be seen as
the equilibrium state of the economy. Therefore, the inﬂation rate does not change if
the unemployment rate is close to the latent state. However, it can increase when the
unemployment rate is lower than the nairu, i.e., if ut −xt < 0. This imposes the condition
that β < 0 for the negative correlation between inﬂation and unemployment to exist.
We return to this model in Example 3.3 on page 54.

28
2
Bayesian modelling and inference
Mixed effect models
In many panel data applications, we would like to separate the mean population behaviour
and the individual deviations from this mean. This can be done using a mixed eﬀects model,
where the population behaviour is captured using so-called ﬁxed eﬀects and individual devia-
tions are captured using random eﬀects. A mixed eﬀects model can be expressed by
yit = αt xit + βizit + eit,
(2.9)
where eit denotes some error term, e.g., a Gaussian iid random variable or an ar(1) process.
Here, αt ∈Rd denotes the time-dependent ﬁxed eﬀects and βi ∈Rp denotes the individual-
dependent random eﬀects. The design matrices xit and zit for the two eﬀects contain the
intercept and the relevant regressors. For a general introduction to mixed eﬀects models,
see Fitzmaurice et al. (2008) and Greene (2008).
A common assumption for the individual random eﬀects are
βi ∼N(βi; β0, Σβ0),
for some mean vector β0 ∈Rp and covariance matrix Σβ0 ∈Rp×p. However, this can be
restrictive in some applications when the distribution of the random eﬀects is multi-modal.
An alternative approach that we consider in Paper G is therefore to replace this assumption a
mixture of Gaussians. This generalisation allows for so-called heterogeneity in the individual
random eﬀects. The mixed eﬀects model diﬀers from the linear regression model as some of
its parameters vary between individuals and some can vary over time. Inference in mixed
eﬀects models is therefore more complicated than for the linear regression model (2.1).
Finally, it is also possible to make use of a model similar to (2.9) when the observations are
binary or integer. The resulting class of models is known as the generalised linear mixed
model (glmm), which is based on the same type of link functions as for the glm. glmms
are common in many diﬀerent applications ranging from marketing and econometrics to
health and medicine, see e.g., Baltagi (2008) and Fitzmaurice et al. (2008).
Example 2.4: Voting behaviour in the US Supreme court (cont. from p. 21)
Let the observation yit = 1 denote a liberal vote and yit = 0 denote a conservative vote of
judge i = 1, . . ., n in case t = 1, . . .,T . We model the votes using an item response model
(irm; Fox, 2010), which is similar to a glmm with a probit link function, given by
uit = αt,1
αt,2
 "−1
xi
#
+ eit,
yit =

1,
uit > 0,
0,
uit ≤0,
where αt,1 ∈R and αt,2 ∈R denote the diﬃculty and discrimination parameter of case
t respectively. Here, eit denotes an independent standard Gaussian random variable. The
quantity xi ∈R captures the relative liberal/conservative score for each justice.
Note that the score xi and the so-called utility uit are unknown latent variables in this
model. The task is therefore to reconstruct these latent variables from the observations.
However, this is diﬃcult as we do not know the parameters of the model αt.
We return to this model in Example 3.8 on page 63.

2.3
Computing the posterior and making decisions
29
Computing the posterior and making decisions
We have discussed the ﬁrst two steps of the inference process in some details. In this section,
we introduce Bayesian inference for determining the unknown parameter θ from the data.
The basic premise in Bayesian inference is that we treat the unknown parameter as a random
variable. This parameter is assumed to be distributed according to some density denoted
by p(θ). This object is known as the prior distribution of the parameter and encodes our
subjective beliefs before looking at the data. From this prior distribution, we would like to
compute the posterior distribution p(θ |y), where y denotes some data. This procedure is
known as the prior-posterior update and is carried out using Bayes’ theorem given by
p(θ |y) = p(y |θ)p(θ)
p(y)
∝pθ(y)p(θ).
(2.10)
Here, p(y |θ) ≜pθ(y) denotes the likelihood, which is a function of the data and summarises
all the information about θ available in the observations. This is known as the likelihood
principle in statistics. The posterior is therefore a combination of our prior beliefs about
θ and the information about the parameter that is available in the observations. After
computing the posterior, we can extract point estimates of θ and their uncertainties.
In maximum likelihood inference, it is assumed that the parameter θ is ﬁxed and that the
uncertainty comes from the data. The point estimate of θ is obtained by maximising the
likelihood function pθ(y). Moreover, it is possible to prove that this procedure gives the
desired result in the limit of inﬁnitely many observations (Lehmann and Casella, 1998),
i.e., that the estimate equals the true parameter. In Bayesian statistics, we do not rely on
asymptotic results as for the maximum likelihood estimator. However, it is known that the
maximum likelihood estimator is the best un-biased estimator in many cases and can be
diﬃcult to beat in terms of statistical accuracy. However, this does not necessarily hold true
when the number of observations is small.
Prior distributions
A major decision for the user of Bayesian statistics is the choice of p(θ). As previously men-
tioned, the prior distribution is often determined by expert knowledge about the current
setting. A common type of prior is the conjugate prior, i.e., when the prior and posterior
are given by the same type of distribution. This is a convenient choice for carrying out
inference as the prior-posterior update amounts to recomputing some suﬃcient statistics.
Conjugate priors can be found for many members of the exponential family of distribution
when the data is iid, see Robert (2007) and Bishop (2006).
In many other cases, we cannot make use of conjugacy for selecting the prior. Instead,
parametric distributions such as the Gaussian, Gamma and similar are used as prior distri-
butions. This often results in that approximations are required to compute the posterior
distribution. Two other popular alternatives for prior distributions are improper priors and
non-informative priors. In former, we cannot normalise the prior and express it as a density.
However, it is possible in some cases to obtain a valid posterior distribution, which inte-
grates to one. A popular example is the uniform distribution over the positive real numbers,
which forces the posterior to only have probability mass on this interval. This is useful to

30
2
Bayesian modelling and inference
encode stability properties in ssms and other dynamical models. Non-informative priors
try to introduce as small amount of prior information as possible. However, the construc-
tion of such priors is diﬃcult and just applying ﬂat priors in order to encode ignorance can
have unforeseen eﬀects on the posterior. For more information regarding non-informative
priors, see Robert (2007) and Gelman (1996).
The choice of prior can greatly inﬂuence the shape and properties of the posterior distri-
bution. Especially, when the amount of information in the data is small. It is therefore
advisable to make use of a couple of diﬀerent prior distributions and compare the resulting
posterior. This approach is advocated by Spiegelhalter (2004). Another approach is posterior
predictive checks, where data is simulated from the posterior and compared to the actual
observations. This can be done in-sample (on the data used for estimation) or out-of-sample
(on fresh data not used in the estimation). This is similar to cross-validation, which is useful
for model order selection and model validation in e.g., machine learning and system identi-
ﬁcation. For more information about posterior predictive checks, see Gelman et al. (1996)
and Gelman et al. (2013).
Finally, note that one of the strengths of Bayesian inference comes from the prior. For
example, we can make use of prior knowledge to narrow the possible range for the parameter.
This could be helpful in settings when the amount of data is limited or when we have
identiﬁability issues. Furthermore, priors can be used to promote smoothness and sparsity
in the parameter (vector), see Remark 2.5.
Remark 2.5 (Prior distribution for promoting smoothness and sparsity). Prior distributions are particu-
larly useful in promoting: (i) smoothness and (ii) sparsity in the parameter posterior. Smoothness is
an interesting property when modelling cross-sectional or time series data as the posterior estimates
should vary slowly and smoothly between nearby data points. An example of this is the gp regression
model introduced in Section 1.1.1 for modelling the thickness of ice varves. We return to gp regression
models and their applications in Section 2.4.
rls was introduced in the form of the elastic net (2.5) in Section 2.2.1 by adding two terms to the loss
function. A more intuitive approach to rls is to view it as Bayesian linear regression with speciﬁc
choices of prior distributions. Two choices for promoting sparsity are given by
p(βj) = L(βj; 0, σ),
p(βj) = N(βj; 0, σ2),
for j = 1, . . ., p. Here, L(βj; 0, σ) and N(βj; 0, σ2) denote the zero-mean Laplace and Gaussian
distributions with scale σ > 0, respectively. These two choices of priors corresponds to l1-rls
(lasso) and to l2-rls (ridge regression), respectively. As previously mentioned, the primary beneﬁt
of these priors is that they shrink regression coeﬃcients towards zero and therefore automatically
select the most important regressors. We return to the use of sparseness priors for model order
selection in Section 4.2 and in Papers F and G.
The likelihood
As previously mentioned, the likelihood contains all the information in the observations
about the parameter. The form of the likelihood is determined by the model of the data.

2.3
Computing the posterior and making decisions
31
For example if the data is assumed iid, then the likelihood is given by
pθ(y1:n) =
n
Y
i=1
pθ(yi),
where e.g., pθ(yi) = N(yi; µ, σ2) if the data has a Gaussian distribution. We can write
similar expressions for the linear regression model and the ar(p) process.
Example 2.6: Voting behaviour in the US Supreme court (cont. from p. 28)
We have that the observations are iid Bernoulli from the model. Hence, we can express
the likelihood by
p(y |θ) =
n
Y
i=1
T
Y
t=1
pyit
it
 1 −pyit
it
1−yit,
with the success probability pit is given by
pit = Φ
 αt,1
αt,2
 "−1
xi
#!
,
where Φ( · ) denotes the standard Gaussian cumulative distribution function (cdf). Using
the likelihood, we can compute the posterior if we assume a prior distribution for each of
the parameters {α1:T, u1:n,1:T, x1:n}. Here, we limit ourselves to computing the conditional
posterior for the liberal/conservative score using the prior xi ∼N(xi; 0, 1). From Bayes’
theorem (2.10), we obtain directly that
p(xi |y, α, u) ∝N(xi; 0, 1)
T
Y
t=1
N uit; −αt,1 + αt,2xi, 1,
which is the Gaussian prior for xi multiplied with a Gaussian likelihood. Here, we have
discarded all terms not depending on xi. It is possible to rewrite the posterior as a Gaussian
distribution with updated suﬃcient statistics. This is a result of the conjugacy between
the likelihood and the prior, see Robert (2007) and Gelman et al. (2013). The calculation
is done by completing the square in the exponent of the Gaussian density. The resulting
conditional posterior can be written as
p(xi |y, α, u) = N *.
,
xi; Σ−1
post
TX
t=1
αt,2(uit + αt−1), Σ−1
post+/
-
,
Σpost = 1 +
TX
t=1
α2
t,2.
It is possible to also compute conditional posteriors for u and α, see Albert (1992). This is
done in the next part of this example. It turns out that we can sample from the posterior
using Monte Carlo by iteratively sample from each of the three conditional posteriors given
the remaining parameters.
We return to this model in Example 3.8 on page 63.

32
2
Bayesian modelling and inference
For the ssm, we can express the likelihood by using the decomposition
pθ(y1:T ) = pθ(y1)
T
Y
t=2
pθ(yt |y1:t−1),
(2.11)
where pθ(yt |y1:t−1) denotes the so-called predictive likelihood. We can express the predictive
likelihood as the marginalisation given by
pθ(yt |y1:t−1) =

X 2
gθ(yt | xt) fθ(xt | xt−1)pθ(xt−1 |y1:t−1) dxt−1:t,
(2.12)
which follows from the Markov property of the ssm. However, we cannot evaluate this
integral in closed form for most ssms as the latent states and thereby pθ(xt−1 |y1:t−1) are
unknown. This can be done in two special cases: (a) when the state space is ﬁnite (when the
state only assumes a ﬁnite collection of values) and (b) when the ssm is linear and Gaussian
as discussed in Remark 2.7. Otherwise, the computation of the likelihood is analytically
intractable and approximations are required.
Remark 2.7 (Bayesian state inference in a linear Gaussian SSM). In this example, we make use of the
properties of the Gaussian distribution to solve the state inference problem exactly for the linear
Gaussian state space (lgss) model. We can express a scalar version of a lgss model by
xt | xt−1 ∼N

xt ; µ + φ(xt−1 −µ), σ2
v

,
yt | xt ∼N

yt ; xt, σ2
e

,
(2.13)
where the parameters are denoted by θ = {µ, φ, σv, σe}. From (2.12), we know that the ﬁltering
distribution πt(xt) ≜pθ(xt |y1:t) is required to compute the likelihood. We can compute πt(xt)
using the Bayesian ﬁltering recursion (Anderson and Moore, 2005) given by
πt(xt) =
gθ(yt | xt)
pθ(yt |y1:t−1)

X
fθ(xt | xt−1)πt−1(xt−1) dxt−1,
(2.14)
for 0 < t ≤T . From the structure of the lgss model, we assume that the prior distribution can
be denoted by πt−1(xt−1) = N(xt−1; Dxt−1|t−1, Pt−1|t−1). Here, Dxt−1|t−1 and Pt−1|t−1 denote the
ﬁltered state and its covariance both at time t −1, respectively.
We can then solve (2.14) by using the properties of the Gaussian distribution. The solution is a
recursion known as the Kalman ﬁlter (Kalman, 1960; Kailath et al., 2000). This is an iterative approach
with two steps: (i) the simulation step computes the predicted state estimate and its covariance and
(ii) the correction step computes the ﬁltered state estimate and its covariance. The simulation step
corresponds to simulating the system one time step forward according to the state process, which
during iteration t consists of
Dxt|t−1 = µ + φ(Dxt−1|t−1 −µ),
Pt|t−1 = φ2Pt−1|t−1 + σ2
v.
In the correction step, we compare the predicted state with the observations and correct the state
estimate accordingly by
Dxt|t = Dxt|t−1 + Kt(yt −Dxt|t−1),
Pt|t = Pt|t−1 −Pt|t−1Kt,
where Kt = Pt|t−1(Pt|t−1 + σ2e )−1 denotes the so-called Kalman gain. Here, we introduce the
predicted state estimate and the predicted covariance denoted by Dxt|t−1 and Pt|t−1, respectively.

2.3
Computing the posterior and making decisions
33
Finally, the posteriors for the ﬁltered and predicted states are given by xt|t ∼N(xt|t ; Dxt|t, Pt|t) and
xt|t−1 ∼N(xt|t−1; Dxt|t−1, Pt|t−1), respectively. After a run of the Kalman ﬁlter, we can compute the
likelihood of the lgss model given the parameters θ by
pθ(y1:T ) =
T
Y
t=1
N yt ; Dxt|t−1, Pt|t−1 + σ2
e
,
which follows from (2.12). The mean and variance of the initial state are typically assumed to be
known, e.g., Dx1|0 = µ and P1|0 = σ2v(1 −φ2)−1 in this particular model.
We continue by presenting some useful quantities connected with the likelihood, which are
used of in many of the papers included in this thesis. The ﬁrst quantity is known as the
score function and it is deﬁned as the gradient of the log-likelihood given by
S(θ′) = ∇log pθ(y)
θ=θ′.
(2.15)
The score function has a natural interpretation as the slope of the log-likelihood. Hence,
the score function is zero when evaluated at the true parameter vector, S(θ⋆) = 0. However,
this is not necessarily true when the number of observations is ﬁnite.
The second quantity is known as the observed information matrix and it is deﬁned by the
negative Hessian of the log-likelihood given by
J (θ′) = −∇2 log pθ(y)
θ=θ′.
(2.16)
The observed informations matrix can be seen as a measure of the total amount of infor-
mation available regarding θ in the data. That is, if the data is informative, the resulting
information matrix is large (according to some measure). Also, the information matrix
can geometrically be seen as the negative curvature of the log-likelihood. As such, we ex-
pect it to be positive deﬁnite (pd) at the maximum likelihood parameter estimate (c.f. the
second-derivative test in basic calculus).
Moreover, there exists a limiting behaviour for the observed information matrix, which
tends to the so-called expected information matrix as the number of data points approach
inﬁnity. This quantity (also known as the Fisher information matrix) is deﬁned as the
expected value of the observed information matrix given by
I(θ′) = −Ey
f
∇2 log pθ(y)
θ=θ′
g
= Ey

∇log pθ(y)
θ=θ′
2
,
(2.17)
where the expectation is evaluated with respect to the data. Note, that the expected infor-
mation matrix is independent of the data realisation, whereas the observed information is
dependent on the realisation. The expected information matrix is pd for all values of θ as
it according to the ﬁrst term in (2.17) can be seen as the variance of the score function.
Point estimates
The Bayesian parameter inference problem is completely described by (2.10) and everything
known about θ is encoded in the posterior. However, we are sometimes interested in
computing point estimates of the parameter vector. This is done by applying statistical
decision theory to make decisions about what information from the posterior to take into

34
2
Bayesian modelling and inference
Loss function
Bayes point estimator
Linear
L(θ, δ) = |θ −δ|
Posterior median
Quadratic
L(θ, δ) = (θ −δ)2
Posterior mean
0-1
L(θ, δ) = I(θ = δ)
Posterior mode / Maximum a-posteriori (map)
Table 2.1. Diﬀerent loss functions and the resulting Bayes point estimator.
account in the point-estimate. Consider a loss function L : Θ × Θ →R+, which takes the
parameter and its estimate as inputs and returns a real-valued positive loss. The expected
posterior loss (or posterior risk) is given by
ρ p(θ), δ |y =

Θ
L θ, δ(y)p(θ |y) dθ,
where δ(y) denotes the decision of the parameter estimate given the data. The Bayes estima-
tor is deﬁned as the minimising argument of the expected posterior loss,
δ⋆(y) = argmin
δ(y)∈Θ
ρ p(θ), δ |y.
Remark 2.8 (Some common loss functions). In Table 2.1, we present three diﬀerent Bayes estimators
resulting from diﬀerent choices of the loss function. For example when selecting the quadratic loss
function, we have
argmin
δ(y)∈Θ

Θ
(Dθ −θ)2p(θ |y) dθ.
Expansion and diﬀerentiation of the integral with respect to Dθ gives
∂
∂Dθ

Dθ2 −2Dθ

Θ
θp(θ |y) dθ +

Θ
θ2p(θ |y) dθ

= 0,
where the derivative is set to zero to obtain the optimum. Hence, we obtain
2Dθ −2

Θ
θp(θ |y) dθ = 0,
where the solution is given by
Dθ = Ey[θ] =

Θ
θ p(θ |y) dθ.
(2.18)
That is, the Bayes estimator is given by the posterior mean for the quadratic loss function. Similar
calculations can be done for other loss functions. Furthermore, selecting the 0-1 loss function together
with uniform priors recovers the maximum likelihood estimator. Finally, we note that other more
complicated loss functions can be of interest in practice. For example, it could be important to limit
the number of estimates smaller than the true value or the number of false positives.

2.3
Computing the posterior and making decisions
35
The computation in (2.18) is a common integration problem in Bayesian inference. It
turns out that most problems in Bayesian inference corresponds to intractable integration
problems. On the other hand, maximum likelihood inference often corresponds to solving
convex and non-convex optimisation problems.
Asymptotic properties
The statistical properties of the Bayes estimator depend in general on the choice of prior. It
is therefore challenging to state anything general regarding the properties of the parameter
estimates when n (or T ) is ﬁnite. However, the Bernstein-von-Mises theorem (Van der Vaart,
2000) states that under some mild regularity conditions the inﬂuence of the prior distri-
bution diminishes are the amount of information about θ increase. Note that, this only
occurs when the amount of informative observations increases. Moreover, the posterior
distribution concentrates to a Gaussian distribution centred around the true parameters, i.e.,
the asymptotic maximum likelihood estimate. Therefore, the Bayes estimator enjoys the
same strong asymptotic properties as the maximum likelihood estimator.
As a consequence, the Bayes estimator is consistent, asymptotically Gaussian and eﬃcient
under some regularity conditions. These conditions include that the parameter space is
compact and that the likelihood, score function and information matrix exist and are well-
behaved, see Lehmann and Casella (1998) and Casella and Berger (2001). An estimator is
said to be consistent if
Dθ
a.s.
−→θ⋆,
where θ⋆denotes the true parameters and when n →∞. That is, the estimate almost surely
(with probability one) converges to the true value of the parameter in the limit of inﬁnite
data. Furthermore as the estimator is asymptotically Gaussian, the error in the estimate
satisﬁes a central limit theorem (clt) given by
√n
Dθ −θ⋆
d
−→N

0, I−1(θ⋆)

,
(2.19)
when n →∞. This follows from a second-order Taylor expansion of the log-likelihood
around θ⋆. Note that, the expected information matrix I(θ) determines the asymptotic
accuracy of the estimate.
Lastly, we say that an estimator is eﬃcient if it attains the Cramér-Rao lower bound, which
means that no other consistent estimator has a lower mean square error (mse). That is, the
maximum likelihood estimator is the best un-biased estimator in the mse-sense and there
are no better un-biased estimators. This last property is appealing and one might be tempted
to say that this estimator is the best choice for parameter inference. However, this result is
only asymptotically valid. Therefore, other estimators (e.g., Bayes estimators) could have
better properties in the ﬁnite sample regime.
Finally, there can exist biased estimators with a smaller mse than the maximum likelihood
estimator. This is the result of the so-called bias-variance trade-oﬀ. One example that we
already encountered is the rls in Remark 2.5, where the estimates are biased but sometimes
enjoy a much smaller variance compared with the ls solution.

36
2
Bayesian modelling and inference
Non-parametric models
Bayesian non-parametrics (bnps; Hjort et al., 2010) is an active research ﬁeld in machine
learning and computational statistics. The models introduces in Section 2.2 are all paramet-
ric, i.e., the number of parameters p does not grow with the number of observations. In
non-parametric models, we assume that p grows with the number of observations, which
gives more ﬂexibility to the model as more observations are recorded.
Another perspective of bnps is that they are inﬁnite stochastic processes. Their construction
can be carried out by invoking the Kolmogorov extension theorem (Billingsley, 2012, p. 517).
This theorem states that any ﬁnite subset of an inﬁnite dimensional process is distributed
according to the marginal of that process. Hence, we can ﬁnd an inﬁnite stochastic process
by ﬁxing a process in a ﬁnite subset of points and applying the extension theorem. Conjugate
priors are often used to carry out the prior-posterior update. See Orbanz (2009) for how to
construct bnps by starting from ﬁnite-dimensional marginals.
In this section, we brieﬂy discuss two useful bnp models that we make use of in Papers E and
F. The ﬁrst model is the gp (Rasmussen and Williams, 2006) and it is useful for regression
and classiﬁcation. We already encountered the gp in the motivating example connected
with Figure 1.1. There are many more applications where gps are useful for modelling, e.g.,
airline delays (Hensman et al., 2013) and human faces (Titsias and Lawrence, 2010).
The second model is known as a Dirichlet process (dp; Ferguson, 1973, 1974) and it is useful
for clustering and to model probability measures (distributions). An overview of the use of
dps and other bnps is provided by Fox (2009). dps are employed to model piece-wise aﬃne
systems by Wågberg et al. (2015) with applications in automatic control. In economics,
Burda and Harding (2013) have applied dps to model the heterogeneity in the eﬀects of
research and development in companies. Finally, this type of model is also used for detecting
haplotypes in genomics as discussed by Xing et al. (2007). A haplotype is unit of genetic
information and inferring these from data is important to understand genetic variations in
populations of individuals.
Gaussian processes
gps have their origins in kriging methods (Cressie, 1993; Matheron, 1963) from spatial statis-
tics, where they are used to construct elevation maps from measurements. Mathematically,
a realisation of a gp is an inﬁnite long vector of real-valued random variables. Hence, we
can see this vector as a function and this is why gps can be used as priors over function
spaces. Formally, a gp is an inﬁnite-dimensional Gaussian distribution, where any ﬁnite
subset of points is jointly distributed according to a Gaussian distribution. We denote a
gp by GP(m, κ), where m(x) and κ denote the mean function and the covariance function
(kernel), respectively. These two functions fully speciﬁes the gp and can be deﬁned by
m(x) = E[ f (x)],
(2.20a)
κ(x, x′) = E
f  f (x) −m(x) f (x′) −m(x′)⊤g
,
(2.20b)
for some function f : Rp →R that we would like to model by the gp. Both m and κ are
considered to be prior choices and encode our prior beliefs about the data in terms of e.g.,

2.4
Non-parametric models
37
trends, cycles and smoothness. The mean function speciﬁes the average value of the process
and the covariance function speciﬁes the correlation between (nearby) samples.
In the left half of Figure 2.4, we present a realisation from a gp prior. Furthermore, we
indicate two pairs of points by dotted lines. In the right half of the same ﬁgure, we present
the covariance function corresponding to the points. In the green case, the covariance
function has a high correlation and therefore the probable range of values for x2 given
x1 is quite narrow. In the orange case, the distance between the points is larger and the
correlation in the covariance function is therefore smaller. This illustrates the connection
between realisations of the gp and the choice of covariance function.
In Figure 2.5, we present three realisations from two diﬀerent gp priors. Here, we make use
of the squared exponential (se) covariance function and the Matérn 3/2 covariance function,
see Rasmussen and Williams (2006) for details. The former encodes the assumption that
the function has an inﬁnite number of continuous derivatives. The latter only assumes
one continuous derivative and therefore the realisations are less smooth. We also vary the
length scale l, which encodes assumptions on the rate of change of the underlying function.
Typically, l is treated as a hyper-parameter, which is either estimated from the data or
determined by the user.
gps are useful for non-parametric/non-linear regression, where no particular functional
form is assumed for the regression function f ( · ). A non-parametric regression model can
be written as
yi = f (xi) + σeei,
(2.21)
with ei as a standard Gaussian random variable with standard deviation σe > 0. To utilise
the gp, we assume the prior distribution
f ∼GP(m, κ),
(2.22)
for the regression function. Hence, we have that both the prior (2.22) and the data likelihood
(2.21) are distributed according to Gaussian distributions. We can compute the posterior
distribution by Bayes’ theorem using the conjugate property given some data D = {x, y} =
{xi, yi}n
i=1. The resulting predictive distribution evaluated at some test point x⋆is given by
a Gaussian distribution with an updated mean and covariance function computed by
f (x⋆)D ∼N

x⋆; µ f (x⋆|D), σ2
f (x⋆|D)

,
(2.23a)
µ f (x⋆|D) = κ⊤
⋆
f
κ x, x + σ2
e In
g−1y,
(2.23b)
σ2
f (x⋆|D) = κ x⋆, x⋆
 −κ⊤
⋆
f
κ x, x + σ2
e In
g−1κ⋆+ σ2
e .
(2.23c)
Here, we introduce κ⋆= κ x⋆, x
to denote the covariance between the test value and the
sampling points. An example of a predictive gp posterior was given in Figure 1.1, where
the mean µ f is plotted as a solid line. The conﬁdence intervals are computed by using the
variance in the predictive posterior σ2
f . We return to using gps to model the posterior
distribution of an ssm in Chapter 4 and in Paper E.

38
2
Bayesian modelling and inference
-4
-2
0
2
4
-3
-2
-1
0
1
2
3
x
f(x)
x1
x2
-4
-2
0
2
4
-4
-2
0
2
4
-4
-2
0
2
4
-3
-2
-1
0
1
2
3
x
f(x)
x1
x2
-4
-2
0
2
4
-4
-2
0
2
4
Figure 2.4. A realisation from a gp prior with the se covariance function for two pairs of points
indicated by dotted lines.
-4
-2
0
2
4
-3
-2
-1
0
1
2
3
x
f(x)
SE, l=1
-4
-2
0
2
4
-3
-2
-1
0
1
2
3
x
f(x)
Matérn 3/2, l=1
-4
-2
0
2
4
-3
-2
-1
0
1
2
3
x
f(x)
SE, l=3
-4
-2
0
2
4
-3
-2
-1
0
1
2
3
x
f(x)
Matérn 3/2, l=3
Figure 2.5. Realisations from a gp prior with the se covariance function (purple) and the
Matérn covariance function (magenta) for two diﬀerent length scales l.

2.4
Non-parametric models
39
Dirichlet processes
A realisation G of a dp is a random discrete probability distribution in the form of an
empirical distribution, i.e.,
G(dθ) =
∞
X
k=1
wkδθk(dθ).
(2.24)
Here, the weights {wk}∞
k=1 and locations {θk}∞
k=1 are random variables. Furthermore, we
have that P∞
k=1 wk = 1 with probability 1, which is why G can be interpreted as a proba-
bility measure. Let DP(α,G0) denote a dp with concentration parameter α > 0 and base
measure G0. We say that G is distributed according to a dp if all of its marginal distributions
are Dirichlet distributed. This was proved by Ferguson (1973) and is in analogue with the
Gaussian marginals required for the gp. Hence, if G0 is a probability measure on the space
(Ω, F), we have that

G(A1),G(A2), . . .,G(AN )

∼D

αG0(A1), αG0(A2), . . ., αG0(AN )

,
(2.25)
for any ﬁnite (measurable) partition A1:N of Ω. Here, D(α) denotes the Dirichlet distribu-
tion with concentration parameter α > 0.
Note that the expected value of G is the base measure and therefore G has the same support
as G0. Moreover, G is discrete with probability one even if the base measure is continuous.
In Figure 2.6, we present two realisations from a dp using α = 1 (green) and α = 10
(orange) and the standard Gaussian as G0. We note that a larger concentration parameter
results in more similar weights, where we can almost guess the underlying base measure.
Conversely, most probability mass is allocated to a small number of components when the
concentration parameter is small.
We can recover these properties analytically by studying the predictive distribution of a dp.
Assume that we obtain some data generated from the model given by
G ∼DP(α,G0),
θi |G ∼G,
for i = 1, 2, . . .. The predictive distribution is given by the marginalisation
p(θ⋆|θ1:n) =

G(θ⋆)p(G |θ1:n)dG,
which is possible to carry out in closed-form. The result is a Pólya urn scheme discussed by
Blackwell and MacQueen (1973), which can be expressed mathematically by
θ⋆|θ1:n ∼
α
α + nG0 +
1
α + n
n
X
i=1
niδθi .
(2.26)
Here, ni denotes the number of parameters that are identical to θi, i.e.,
ni =
n
X
j=1
I [θi = θj],
where I [A] denotes the indicator function.

40
2
Bayesian modelling and inference
This Pólya urn scheme has an interesting and rather amusing interpretation known as the
Chinese restaurant process (crp). In this interpretation, we see each parameter θi as a guest
arriving to a Chinese restaurant. The ﬁrst guest choose a random dish from the menu and
sits down at some table. The second guest can either select a new random dish from the
menu or join the ﬁrst guest at his/her table and have the same dish. This continues on
forever and the probability of joining an existing table is proportional to the number of
guest ni already sitting at that table.
Hence, we can conclude from the crp and (2.26) that the dp is a discrete process with a
non-zero probability of ties. That is, that guests tend to cluster around the existing dishes in
the restaurant. Furthermore, we are more likely to sample from the base measure (choose
a new dish) if α ≫n, which means that the predictive posterior concentrates to the base
measure. If α ≪n, we often sample from the existing parameters, which gives many ties
and a strong clustering behaviour. This corresponds to that a few samples obtain most of
the probability mass as seen in Figure 2.6.
A third alternative view of a dps are to consider them as the results of a stick-breaking process
(sbp; Sethuraman, 1994). This is useful for generating realisations from a dp by using the
empirical distribution in (2.24). The weights and locations can be generated by a sbp, i.e.,
wk = Vk
k−1
Y
i=1
(1 −Vi),
Vk ∼B(1, α),
θk ∼G0,
where B(a, b) denotes the Beta distribution with shape parameters a > 0 and b > 0. The
name sbp comes from that wk can be seen as a part of a stick of unit length. The product
represents the length of the remaining stick at iteration k and Vk denotes the fraction that
is broken oﬀthe stick. In the left part of Figure 2.7, we present an illustration of the sbp.
At each iteration, we break oﬀa proportion of the remaining stick (green) given by Vk.
We collect the resulting pieces and combine them with samples from the base measure to
obtain the random probability distribution presented in the right part of the ﬁgure.
We can create a hierarchical model to utilise the discreteness of the dp for clustering. This
results in a dp mixture (dpm; Antoniak, 1974), which can be expressed as
G ∼DP(α,G0),
θk ∼G,
xk ∼p( · |θk),
(2.27)
for k = 1, . . ., n. That is, we generate data from a distribution which is parametrised by the
random parameters drawn from the random probability distribution generated by a dp. In
practice, we make use of a parametric distribution for the data and obtain a clustering model
as some xk shares the same parameters. We return to make use of dpms for modelling the
heterogeneity of the individual random eﬀects in a mixed eﬀects model in Paper G.
Outlook and extensions
In this chapter, we have presented a few popular parametric models for diﬀerent types of
data. Regression models are discussed in many diﬀerent textbooks and the interested reader
is referred to Hastie et al. (2009) and McCullagh and Nelder (1989) for more information.
Time series models such as ar and autoregressive moving average (arma) models with

2.5
Outlook and extensions
41
-3
-2
-1
0
1
2
3
0.00
0.05
0.10
0.15
0.20
0.25
θ
probability
-3
-2
-1
0
1
2
3
0.00
0.05
0.10
0.15
0.20
0.25
θ
probability
Figure 2.6. Realisations from a dp prior with a standard Gaussian as the base measure and the
concentration parameter α = 1 (green) and α = 10 (orange).
Figure 2.7. Left: illustration of the stick-breaking process in which a proportion Vk of the remain-
ing stick (green) is broken oﬀat each iteration. Right: Illustration of the resulting realisation of
the dp.

42
2
Bayesian modelling and inference
extensions are thoroughly introduced by Tsay (2005), Shumway and Stoﬀer (2011) and
Brockwell and Davis (2002). For more information about ssms and interesting extensions,
see Douc et al. (2014), Cappé et al. (2005) and Ljung (1999). Kalman ﬁltering is an important
topic for lgss models and a book long treatment is provided by Kailath et al. (2000).
Graphical models are another large family of models, where ssms corresponds to a speciﬁc
instance, see Paper A. This type of models is useful in modelling everything from images
to text documents. A good introduction to this subject is provided in the book by Koller
and Friedman (2009) and Chapter 8 in the book by Bishop (2006). A few more examples
of models are presented in the papers included in this thesis. For example, we make use of
so-called copula models (Nelsen, 2007) in Section 6.3 (page 253) of Paper E.
Finally, more information regarding Bayesian inferences are found in the books by Robert
(2007) and Gelman et al. (2013). The statistical properties of Bayes estimators are further
discussed in Lehmann and Casella (1998) and Berger (1985).

3
Monte Carlo methods
A common problem when making use of Bayesian inference for many models of interest
is analytical intractability. From Chapter 2, we know that this can be the result of an
intractable likelihood or due to the fact that the prior-posterior update cannot be carried
out in closed-form. In these situations, we have to resort to approximations which are
usually based on variational inference or statistical simulation. In this thesis, we focus on
the latter approach by using Monte Carlo methods. This family of methods make use
of random sampling for integration, optimisation or to sample from some complicated
probability distribution. In Chapter 2, we noted that many problems in Bayesian inference
can be expressed as integrals and therefore Monte Carlo methods are useful.
As discussed by Eckhardt (1987), Monte Carlo methods were ﬁrst introduced by the Polish-
American mathematician Stanislaw Ulam [1909-1984] in cooperation with the Hungarian-
American mathematician John von Neumann [1903-1957] at the Los Alamos Scientiﬁc
Laboratory in 1946. The ﬁrst application was to simulate neutron transports in the shielding
material used for nuclear weapons research. These methods were quickly disseminated into
physics and chemistry to simulate complicated phenomena.
More elaborate Monte Carlo methods based on the use of Markov chains were later pro-
posed by Metropolis et al. (1953) and extended by Hastings (1970). The resulting algorithm
is known as the Metropolis-Hastings (mh) algorithm and is a member of the larger family
of Markov chain Monte Carlo (mcmc) methods. Another important mcmc method is
known as Gibbs sampling and was proposed by Geman and Geman (1984).
In the late 1980s, Monte Carlo methods became a common tool to approximate the posterior
distribution for many interesting problems in statistics. Ever since, it has been an important
enabler for Bayesian inference and is usually taught in most courses on the subject. In the
beginning of the 1990s, sequential versions of Monte Carlo algorithms were proposed by
Stewart and McCarty (1992), Gordon et al. (1993) and Kitagawa (1996). These methods are
43

44
3
Monte Carlo methods
usually referred to as particle ﬁlters or sequential Monte Carlo (smc) methods. A useful
combination of mcmc and smc was proposed by Beaumont (2003) based on a heuristic
argument. The algorithm was later formalised and analysed by Andrieu and Roberts (2009)
and Andrieu et al. (2010). The resulting algorithms are known as pseudo-marginal and
particle mcmc algorithms.
In this chapter, we present a number of Monte Carlo methods together with their properties
and applications. The main aim of the chapter is to provide the reader with an understanding
of the opportunities and problems that are connected with each algorithm. In Chapter 4, we
outline some strategies to mitigate these problems, which are applied in the papers included
in this thesis.
We begin this chapter by introducing standard Monte Carlo based on a number of diﬀerent
approaches using independent samples. Moreover, we discuss smc, mcmc and the pseudo-
marginal Metropolis-Hastings (pmmh) algorithm for sampling from more complicated
models. Finally, we provide the reader with an outlook and references for further study.
Empirical approximations
Monte Carlo methods are a collection of statistical simulation methods based on sampling.
They are particularly useful for approximating high-dimensional integration problems. For
example, a common problem in Bayesian inference is to compute the expected value of
some integrable test function ϕ : X →R given by
π[ϕ] ≜Eπ
ϕ(x) =

X
ϕ(x) π(x) dx,
(3.1)
where π(x) denotes a (normalised) target distribution. In the basic vanilla formulation of
Monte Carlo methods, we assume that we can simulate iid particles (or samples) from the
target distribution. However, we do not require to be able to evaluate the target point-wise.
In what follows, we encounter Monte Carlo methods which require point-wise evaluation
of the target but not being able to simulate from it directly.
The ﬁrst step in computing a Monte Carlo estimate of (3.1) is to form an empirical approxi-
mation of the target distribution given by
DπN
mc(dx) =
N
X
i=1
δx(i)(dx),
(3.2)
using the particles {x(i)}N
i=1 generated from the target distribution π. Here, δx′(dx) denotes
the Dirac distribution placed at x = x′. The vanilla estimator follows from the second step
by combining (3.2) into (3.1) to obtain
DπN
mc[ϕ] ≜

X
ϕ(x) Dπ(dx) = 1
N
N
X
i=1
ϕ x(i),
(3.3)
which follows from the properties of the Dirac distribution.

3.2
Three sampling strategies
45
The main advantage of Monte Carlo methods over some of their alternatives are solid
statistical properties. The estimator is un-biased and strongly consistent by the strong law of
large numbers (slln), i.e.,
DπN
mc[ϕ]
a.s.
−→π[ϕ],
when N →∞. Moreover, it is possible to construct a central limit theorem (clt) for the
vanilla Monte Carlo estimator given by
√
N

DπN
mc[ϕ] −π[ϕ]

d
−→N

0, σ2
mc

,
σ2
mc ≜Vπ
ϕ < ∞,
when N →∞and ϕ(x) has a ﬁnite second moment. Hence, we see that the Monte Carlo
estimator is asymptotically un-biased with Gaussian errors. Furthermore, the variance of
the error decreases as 1/N independently of the dimension of the problem. This is one
of the main advantages of the Monte Carlo methods compared with common numerical
integration methods based on quadratures, see e.g., Stoer and Bulirsch (1993).
Three sampling strategies
The main diﬃculty with applying vanilla Monte Carlo to many interesting problems is gen-
erating good samples from the target distribution. In this section, we present three diﬀerent
approaches for generating samples or approximating (3.3) directly using: (i) independent
sampling, (ii) sequential sampling and (iii) Markov chain sampling.
Independent Monte Carlo
All Monte Carlo methods rely on generating random numbers. In practice, we cannot
generate truly random numbers using computers. Instead, we make use of pseudo-random
numbers, which are constructed to pass many statistical tests for randomness. In the follow-
ing, we refer to pseudo-random numbers simply as random numbers. A linear congruential
generator can be applied to generate uniform random numbers by
x(i) = ax(i−1) + b(mod m),
for i = 1, 2, . . .. Here, a, b and m are integers (usually large) determined by the speciﬁc
generator. For example, the programming language Python makes use of the Mersenne
Twister (Matsumoto and Nishimura, 1998), which has a period of 219937 −1. In the left part
of Figure 3.1, we present random samples generated by the Mersenne Twister on the unit
square, i.e., u(i) ∼U[0, 1]2. We note that the samples do not ﬁll the square evenly but seem
to concentrate in certain areas. This is a drawback with pseudo-random numbers and can
result in slow convergence rates when the dimension of the problem increases.
There is an alternative method for generating a random sample called quasi-random number
generators. In the right part of Figure 3.1, we present the output from one such algorithm
based on Sobol sequences (Sobol, 1967). The main beneﬁt with this type of sequence is
that it ﬁlls the space more evenly and this can improve convergence in many applications.
The main drawback is that analysis of estimators based on Sobol sequences are challenging
due to their deterministic nature, see Owen (2013). Quasi-random numbers are useful in

46
3
Monte Carlo methods
quantitative ﬁnance (Glasserman, 2004; Niederreiter, 2010) and for sequential Monte Carlo
sampling, see Section 3.4.
Quantile transformation
For many distributions, we can obtain random samples given uniform random numbers
and by using the inverse cdf method. We illustrate this by sampling from the exponential
distribution with rate λ > 0 for which the cdf is given by
G(x) = P(X ≤x) = 1 −exp(−λx),
when x ≥0 and zero otherwise. We can directly compute the inverse cdf as
G−1(p) = −log(1 −p)
λ
,
which is known as the quantile function evaluated at p ∈(0, 1). Hence, we can obtain an
exponentially distributed random number by
x(i) = −
log  u(i)
λ
,
where u(i) denotes a uniform random number. We present an illustration of the inverse
cdf method in the left part of Figure 3.2. Here, we can directly obtain the random sample
1.9 from the uniform random variable 0.86. It is also possible to sample from an empirical
cdf constructed from some data, which is presented in the right part of the same ﬁgure.
This is the basis of an useful approach to non-parametric statistics known as the bootstrap
method (Efron, 1979; Davison and Hinkley, 1997). The main beneﬁt with the bootstrap is
that it does not rely on asymptotics to compute conﬁdence intervals and to carry out tests.
We return to resampling schemes like the bootstrap in Section 3.2.2.
Importance sampling
One approach to approximate (3.3) using independent samples is importance sampling (Mar-
shall, 1956). This algorithm makes use of a proposal distribution to simulate from the
target, which is useful when direct simulation from the target is diﬃcult or impossible.
The proposal distribution q(x) is usually selected to be simple to simulate from and allow
for cheap point-wise evaluations. The discrepancy between the target and proposal is then
compensated for by an importance weight. The main idea is to rewrite (3.1) by
π[ϕ] =

X
ϕ(x) π(x) dx =

X
ϕ(x) π(x)
q(x)
|{z}
≜w(x)
q(x) dx = q[wϕ],
where w(x) denotes the importance weight. Note that, we require to be able to evaluate
the target point-wise, c.f. vanilla Monte Carlo. We can form an analogue estimator to (3.3)
by writing
DπN
is [ϕ] ≜
N
X
i=1
w(i)ϕ x(i),
(3.4)

3.2
Three sampling strategies
47
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
u1
u2
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
u1
u2
Figure 3.1. Pseudo-random samples from U[0, 1]2 generated using Mersenne Twister (left) and
Sobol sequences (right).
-6
-4
-2
0
2
4
6
0.0
0.2
0.4
0.6
0.8
1.0
x
P(x)
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
x
empirical cdf
Figure 3.2. Illustration of the quantile transformation method to generate random variables. A
uniform variable is generated corresponding to P(x) for which x is determined by quantile
transformation (dotted lines).

48
3
Monte Carlo methods
where x(i) ∼q(x) and w(i) ≜w(x(i)). In Figure 3.3, we present two diﬀerent cases where
importance sampling can be useful. In the left plot, we consider sampling from the entire
target using a similar Gaussian proposal which is simple to sample from. The diﬀerence
between the two distributions is indicated by the purple area. The main dissimilarity lies
in the right tail, which falls oﬀslower for the target than for the proposal. In the right plot,
we are interested in computing e.g., the probability of obtaining large values of the target.
Therefore, we construct a proposal that focuses on the upper tail behaviour. This is useful in
many applications where extreme values are important, e.g., in survival models, hydrology
and quantitative ﬁnance, see McNeil et al. (2010) and Embrechts et al. (1997).
For importance sampling to work, the support of q(x) has to contain the support of
ϕ(x)π(x), i.e., supp(ϕπ) ⊂supp(q). In that case, the estimator (3.4) inherits all of the
properties from the vanilla Monte Carlo estimator, i.e., it is un-biased, consistent and asymp-
totically Gaussian. However, the asymptotic variance can be computed by
σ2
is =

X
 ϕ(x)π(x)2
q(x)
dx −π[ϕ]2 =

X
 ϕ(x)π(x) −π[ϕ]q(x)2
q(x)
dx.
A good choice of q(x) should therefore minimise this expression, i.e., by using a proposal
proportional to ϕ(x)π(x). However, in practice this is often diﬃcult due to the requirement
that the proposal should be simple to simulate from and to evaluate point-wise.
In the following, we consider importance sampling from un-normalised target distributions
γ(x). In this case, we can write the target as π(x) = γ(x)Z−1 where the normalisation
constant Z is unknown. However, we can make use of importance sampling in this settings
as well. The main diﬀerence is that the weights are un-normalised and computed by
H
w(x) = γ(x)
q(x).
The resulting estimator is given by
DπN
snis[ϕ] ≜
N
X
i=1
H
w(i)
PN
j=1 H
w(j)
|      {z      }
≜w(i)
ϕ x(i),
(3.5)
which is known as the self-normalised importance sampling (snis) estimator. The estima-
tor (3.5) is strongly consistent and asymptotically Gaussian, see Owen (2013) for details.
However, it is biased for ﬁnite N and the bias is proportional to O(N −1).
Sequential Monte Carlo
In some applications, we would like to compute the expectation of a test function with
respect to a sequence of probability distributions {πt(x0:t)}T
t=0, where xt ∈X . As for
importance sampling, we assume that the target can be written as πt(x0:t) = γt(x0:t)Z−1
t ,
where γt(x0:t) denotes the un-normalised target and Zt denotes the normalisation constant.
Here, we assume that it is possible evaluate the un-normalised target point-wise but the

3.2
Three sampling strategies
49
-6
-4
-2
0
2
4
6
0.0
0.1
0.2
0.3
0.4
0.5
x
p(x)
-6
-4
-2
0
2
4
6
0.0
0.2
0.4
0.6
0.8
1.0
x
p(x)
Figure 3.3. Illustration of importance sampling of the target (green) using a proposal (orange).
The diﬀerence between the target and proposal densities is indicated by the purple area. Two
cases are considered: sampling the entire distribution (left) and its upper tail (right).
Figure 3.4. An illustration of the evolution of the target distribution πt(x0:t) over time.

50
3
Monte Carlo methods
normalisation constant can be unknown, c.f. snis. An illustration of the sequence of
targets is given in Figure 3.4.
This set-up is useful in online settings where observations arrives sequentially or when
the number of observations is large. Note that the online setting can also be artiﬁcially
introduced by so-called tempering methods where a sequential target can be obtain from a
static. A simple annealing scheme is often used for this and it consists of setting πt(x) =
πφt (x), where φt is a parameter varying from zero to one as t increases.
A powerful approach for taking advantage of the sequential structure in certain target
distributions is to make use of smc samplers (Del Moral et al., 2006). These methods
can be seen as an extension of importance sampling algorithms, where the proposal is
constructed sequentially. That is, we assume that the proposal can be expressed by
qt(x0:t) = qt−1(x0:t−1)qt(xt | xt−1) = q0(x0)
tY
r=1
qr(xr | xr−1).
Hence, we can apply importance sampling to ﬁrst sample x0 and then sequentially propose
samples conditioned on the previous, i.e., xr ∼qt(xr | x0:r−1) for r = 1, . . ., t. The resulting
importance weights are also computed sequentially by
wt(x0:t) = γt(x0:t)
qt(x0:t) = γt−1(x0:t−1)
qt−1(x0:t−1)
γt(x0:t)
γt−1(x0:t−1) qt(xt | x0:t−1).
This set-up is more eﬃcient compared with standard importance sampling when x is a
high-dimensional vector. This is due to the aforementioned problems with constructing
good proposals for the importance sampling algorithms. The beneﬁt with smc algorithms
is that each proposal only extends the state from one time step to another, which simpliﬁes
the construction of the proposal.
The resulting algorithm is known as sequential importance sampling (sis). The main draw-
back with this method is that the variance of the estimates increases rapidly with t. This
is the result of particle depletion, where only a single particle (or sample) carries all the
importance weight. Instead, we can introduce a resampling step to mitigate this eﬀect and
to only keep the particles with large weights. This is the major development in the ﬁrst
papers about particle ﬁltering also known as sequential importance sampling with resampling
(sir). Later, the sir algorithm was generalised to the smc algorithm, which can make use
of more elaborate proposal distributions.
The sir algorithm is based on carrying out three steps during each iteration: (i) resampling,
(ii) propagation and (iii) weighting. The output from each iteration is a particle system given
by the particles (samples) {x(i)
0:t }N
i=1 and their corresponding self-normalised importance
weights {w(i)
t }N
i=1. From this system, we can construct an empirical approximation of
πt(x0:t) by
DπN
t,smc(dx0:t) =
N
X
i=1
w(i)
t δx(i)
0:t (dx0:t),
(3.6)

3.2
Three sampling strategies
51
together with an estimator in analogue with (3.4) given by
DπN
t,smc[ϕ] ≜
N
X
i=1
w(i)
t ϕ x(i)
0:t
,
(3.7)
where w(i)
t
≜wt(x(i)
0:t). We proceed by brieﬂy presenting each step and refer the interested
reader to Section 3 (page 123) in Paper A, Doucet and Johansen (2011) and Del Moral et al.
(2006) for further details.
(Resampling) The resampling step multiplies particles with large importance weights and
discard particles with a small weight. This is done in a stochastic manner to focus the
attention of the sir algorithm to the relevant part of the state space, i.e., in locations with
high probability under the target distribution. This operation is carried out by sampling
ancestor indices denoted a(i)
t
for each of the particles. Here, a(i)
t
is interpreted as the index
of the particle at time t −1 from which particle i at time t originates from. This can be
expressed as simulating from a multinomial distribution with probabilities given by
P(a(i)
t
= j) = w(j)
t−1,
j = 1, . . ., N,
(3.8)
for i = 1, . . ., N . This operation is known as multinomial resampling as it corresponds
to sampling from the distribution with the same name. This can also be seen as from the
empirical cdf in the right of Figure 3.2 generated from the normalised particle weights.
In Figure 3.5, we present an illustration of the eﬀect of resampling and the meaning of the
ancestor indices. The green line indicates the surviving particle genealogy during a run of
the sir algorithm. We see that the the genealogy collapses into a single trajectory at time
t = 8. This corresponds to a single surviving particle and therefore only one sample in the
empirical approximation of the target. This is known as particle degeneracy, which results in
that estimates of expected values of test functions with respect to x0:t can suﬀer from a large
variance when t is large. However, expectations with respect to only xt can be estimated
using many samples, which results in a lower variance. The particle degeneracy problem is
a result of the fact that resampling discards particles with a non-zero probability at every
iteration of the algorithm.
To mitigate this problem, we can use alternative resampling schemes or particle smoothers
as discussed in Remark 3.2. Better alternatives to multinomial resampling are based on
stratiﬁed sampling from the cdf of the weights. In this thesis, we make use of systematic
resampling, which exhibits good properties in many applications. The interested reader is
referred to Douc and Cappé (2005), Hol et al. (2006) and Murray et al. (2015) for more
information and examples of resampling algorithms.
(Propagation) In the propagation step, we simulate the particle system from time t −1 one
step forward in time to obtain the particle system at time t. Each particle is propagated
using the proposal distribution by
x(i)
t
∼qt

xt | x a(i)
t
t−1

,
x(i)
0:t ≜
(
x a(i)
t
0:t−1, x(i)
t
)
,
(3.9)
for i = 1, . . ., N . There are many diﬀerent choices for qt( · ) and tailoring them for the

52
3
Monte Carlo methods
problem at hand is important to achieve eﬃcient algorithms. We return to this problem in
Section 3.4 and in Section 6.3 (page 142) of Paper A.
(Weighting) Each particle is assigned an importance weight computed by a weight function
deﬁned by
Wθ
 xt, x0:t−1
 ≜
γt(x0:t)
γt−1(x0:t−1) qt(xt | x0:t−1)wt−1.
The resulting un-normalised and normalised weights are computed by
H
w(i)
t
= Wθ

x(i)
t , x a(i)
t
0:t−1

,
w(i)
t
=
H
w(i)
t
PN
j=1 H
w(j)
t
,
(3.10)
for i = 1, . . ., N . The weights account for the discrepancy between the proposal and the
target distribution in analogue with importance sampling. Furthermore, it is possible to
estimate the unknown normalisation constant Z for the target by making use of the un-
normalised importance weights H
w(i)
t , see Del Moral et al. (2006) for details.
In Kronander et al. (2014a) and Svensson et al. (2015), we make use of smc for rendering
animations in computer graphics and marginalising hyperparameters in gp priors, respec-
tively. smc algorithms have also been proposed for inference in mixture models (Fearnhead,
2004; Ulker et al., 2010), ssms (see Remark 3.1) and graphical models (Naesseth et al., 2014).
Remark 3.1 (Particle ﬁltering). The ﬁltering solution to the state inference problem for the lgss
model is given by the recursion in Example 2.7 on page 32. Although, this recursion is intractable
for most ssms it can be approximated using smc methods. We refer to the resulting algorithm as the
particle ﬁlter (Gordon et al., 1993), which is an integral component in most of the papers included in
this thesis. The particle ﬁlter is the smc algorithm targeting the ﬁltering distribution in an ssm, i.e.,
πt(x0:t) = pθ(x0:t |y1:t). We can compute the mean of this distribution at time t by using ϕ(xt) = xt
in (3.7),
Dxt|t ≜DπN
t,smc[xt] =

X t+1
xt DpN
θ (x0:t |y1:t) dx0:t =
N
X
i=1
w(i)
t x(i)
t ,
(3.11)
where {{x(i)
t , w(i)
t }N
i=1}T
t=0 denotes the particle system generated by the particle ﬁlter.
The main problem is that we cannot evaluate pθ(x0:t |y1:t) directly due to an unknown normalisation
factor. Instead, we let the smc algorithm target γt(x0:t) = fθ(xt | xt−1)gθ(yt | xt)γt−1(x0:t−1), which
follows from the Bayesian ﬁltering recursions (2.14). A simple choice for the proposal in this setting
is given by
qt
 xt | x0:t−1
 = fθ(xt | xt−1),
Wθ
 xt, x0:t−1
 = gθ(yt | xt),
where the weight function follows directly from the target and the choice of proposal by (3.10). That
is, using the state dynamics as the proposal and the density of the observations as the weight function.
We refer to this version of the algorithm as the bootstrap particle ﬁlter (bpf).

3.2
Three sampling strategies
53
0
5
10
15
20
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
time
state
Figure 3.5. The particle genealogy generated by the resampling step in the smc algorithm.
The green line/dots are the particles that survive the repeated resampling steps. The discarded
particles are presented as grey dots.
1.0
1.5
2.0
2.5
3.0
date
NAIRU (bPF)
1987
1995
2003
2011
-2
0
2
4
6
8
10
date
unemployment gap (bPF)
1987
1995
2003
2011
1.0
1.5
2.0
2.5
3.0
date
unemployment rate (PMH)
1987
1995
2003
2011
-2
0
2
4
6
8
10
date
unemployment gap (PMH)
1987
1995
2003
2011
Figure 3.6. The estimated nairu (left) together with the estimated unemployment gap (right)
for Sweden during the period January, 1987 to December, 2015. The estimates obtain by a bpf
(Example 3.3) are indicated by purple/magenta and the estimates from the pmh algorithm
(Example 3.13) are indicated by green/brown.

54
3
Monte Carlo methods
The bpf can be applied to estimate the likelihood pθ(y1:T ), which is useful for parameter inference
in ssms. The predictive likelihood (2.12) can be approximated using the particle system by
DpN
θ (yt |y1:t−1) =

X
gθ(yt | xt)DpN
θ (xt |y1:t−1) dxt = 1
N
N
X
i=1
w(i)
t .
The estimator of the likelihood follows from the decomposition in (2.11) and is given by
DpN
θ (y1:T ) =
1
N T +1
T
Y
t=0
N
X
i=1
w(i)
t .
(3.12)
We refer the interested reader to Section 4.1 (page 128) in Paper A and Doucet and Johansen (2011)
for more information about the bPF and other particle ﬁltering algorithms.
Remark 3.2 (Particle smoothing). In Remark 2.7, we introduced the Bayesian ﬁltering recursions to
sequentially compute the marginal ﬁltering distribution πt(xt) ≜pθ(xt |y1:t). We already know that
the smc algorithm approximating πt(xt) is known as the particle ﬁlter. A related estimation problem
is to approximate the marginal smoothing distribution πT (xt) ≜pθ(xt |y1:T ) for some t ∈[0,T ].
The resulting smc algorithms are known as particle smoothers, which usually operates by extending
the forward pass in the particle ﬁlter with a backward update. This enables the algorithm to make use
of both past and future observations, which reduces the problem with particle degeneracy.
The simplest particle smoother is to make use of the bpf to approximate the joint smoothing distri-
bution πT (x1:T ) ≜pθ(x1:T |y1:T ). The main problem with this approach is that the path degeneracy
problem limits the accuracy of the estimate. Another similar approach is to make use of the ﬁxed-lag
(fl) particle smoother (Kitagawa and Sato, 2001), which is based on using the bPF to estimate the
ﬁxed-lag smoothing distribution πt(xt−∆:t) ≜pθ(xt−∆:t |y1:t) for some ∆> 0. In this thesis, we
make frequent use of this smoother as it has a low computational cost and a reasonable accuracy.
Some of the alternatives to fl particle smoothing are based on approximations of the Bayesian
smoothing recursion (Anderson and Moore, 2005) in which the aforementioned backward pass is
added. Two popular examples from this family of algorithms are the forward ﬁlter backward smoother
(ffbsm; Doucet et al., 2000) and the forward ﬁltering backward simulator (ffbsi; Godsill et al., 2004).
For an extensive survey of particle smoothers, see Lindsten and Schön (2013).
Example 3.3: How does unemployment aﬀect inﬂation? (cont. from p. 27)
We employ a bpf from Remark 3.1 to estimate the nairu using data from Sweden. This
corresponds to the proposal and weight function given by
qt
 xt | x0:t−1
 = N

xt; φxt−1 + µ(xt−1), σ2
v(xt−1, ut−1)

,
Wθ
 xt, x0:t−1
 = N

yt; yt−1 + β ut −xt
, σ2
e

,
with θ = {φ, α, β, σe} = {0.76, 0.43, 0.01, 0.28} and N = 100 particles. The resulting
estimate of the nairu is presented in the upper part of Figure 3.6 together with the unem-
ployment gap (the diﬀerence between the unemployment rate and the nairu). We note
that the unemployment gap is mostly positive during this period. The nairu seems to be
constant at two percent and therefore the unemployment needs to decrease considerably
for the inﬂation the increase.
Furthermore, we perform 100 Monte Carlo simulations (independent runs on the same
data) to estimate the log-likelihood of the data for this model. We record the mean estimate

3.2
Three sampling strategies
55
N = 50
N = 100
N = 250
N = 500
N = 1, 000
Mean
-45.90
-45.89
-45.88
-45.88
-45.89
Standard deviation
0.15
0.11
0.06
0.05
0.03
Table 3.1. The mean and variance of log-likelihood estimates in the Phillips curve model com-
puted using the bpf while varying the number of particles N .
and its variance while varying the number of particles N between 50 and 1, 000. The results
are presented in Table 3.1, where we note that the mean is essentially the same and the
variance decreases when N increases. This is an example of the general properties of the
log-likelihood estimator discussed in Remark 3.4.
We return to this model in Example 3.6 on page 57.
Statistical properties
The analysis of smc algorithms is rather complicated compared to standard Monte Carlo
estimators as the generated particle system does not consist of independent samples due
to the resampling step. However, there are many strong results regarding non-asymptotic
stability and asymptotic properties, see the book long treatments by Del Moral (2013) and
Del Moral (2004). Here, we only provide a short overview of the results summarised by
Crisan and Doucet (2002) and Doucet and Johansen (2011).
For the asymptotic settings, it is possible to show that the empirical distribution (3.6) and
the estimator in (3.7) are strongly consistent, i.e.,
lim
N →∞DπN
t,smc = πt,
DπN
t,smc[ϕ]
a.s.
−→πt[ϕ],
(3.13)
where the ﬁrst property holds almost surely (a.s.) and the second property holds for any
integrable test function ϕ when N →∞. However, note that DπN
t,smc[ϕ] is in general biased
for ﬁnite N , see Remark 3.4 for an important exception. It is also possible to derive a clt
for (3.7) given by
√
N
f
DπN
t,smc[ϕ] −πt[ϕ]
g
d
−→N

0, σ2
smc

,
when using multinomial resampling and where σ2
smc denotes the asymptotic variance com-
puted in Del Moral et al. (2006).
Furthermore, assume that the function ϕ(x) is bounded1 for all x ∈X and some additional
assumptions that are stated by Crisan and Doucet (2002). It then follows that the mse of
the estimator (when we make use of the bPF with multinomial resampling) can be upper
bounded for any N ≥1 by
E

DπN
t,smc[ϕ] −πt[ϕ]
2
≤Ct
∥ϕ∥2
N ,
(3.14)
1This is a rather restrictive assumption as it is not satisﬁed by the function ϕ(x) = x, which is used to compute
the estimate of the ﬁltered state Dxt|t in Remark 3.1.

56
3
Monte Carlo methods
where ∥· ∥denotes the supremum norm. Here, Ct denotes a function that possibly depends
on t but is independent of N .
Note that (3.14) implies that the smc algorithm is stable, i.e., that the variance of the
estimator does not blow up as t increases for any N ≥1. Intuitively, this could be a problem
as the smc algorithm makes approximations based on approximations. It turns out that the
resampling step takes care of this and prevents the rapid accumulation of errors to occur.
For more stability results, see Chopin (2004) and Whiteley (2013).
It is possible to relax the assumption that ϕ(x) should be bounded and that we only use
the bpf. The resulting upper bounds have a similar structure to (3.14) but with diﬀerent
functions replacing the constant Ct. It is also possible to establish uniform convergence
of the estimator if the ssm is fast mixing, i.e., forgets its past fast enough, see Crisan and
Doucet (2002) for details.
Remark 3.4 (Particle ﬁltering (cont. from p. 52)). It turns out that the likelihood estimator (3.12) based
on the bPF is un-biased for any N ≥1, c.f. with the state estimate in (3.13). Furthermore, under some
mixing assumptions for the ssm, the error of the estimate satisﬁes a clt given by
√
N
f
pθ(y1:T ) −DpN
θ (y1:T )
g
d
−→N

0, σ2
L

,
(3.15)
for some asymptotic variance σ2
L, see Proposition 9.4.1 in Del Moral (2004) or Pitt et al. (2012).
Note that the estimator for the log-likelihood is biased for a ﬁnite N , but strongly consistent and
asymptotically Gaussian. This follows from the second-order delta method (Casella and Berger, 2001).
Estimating additive functionals
In this section, we consider the use of smc algorithms to estimate the expected value of an
additive functional. This type of functional can be expressed as
Sθ(x0:T ) =
TX
t=1
ξθ,t(xt−1:t),
(3.16)
which means that a function that depends on the entire state trajectory can be decomposed
into a sum of functionals. Here, ξθ,t(xt−1:t) denotes some general functional that depends
on only two states of the trajectory. This type of additive functional occurs frequently in
ssms when computing functions that depend on the densities fθ(xt+1 | xt) and gθ(yt | xy)
due to the Markov property. The resulting expectation can be expressed by
πT [Sθ] =
TX
t=1

X 2
ξθ,t(xt−1:t)pθ(xt−1:t |y1:T ) dxt−1:t,
(3.17)
where pθ(xt−1:t |y1:T ) denotes the two-step smoothing distribution, which is analytically
intractable for a general ssm. However, it can be estimated by a particle ﬁltering or smooth-
ing, see Remark 3.2 and Poyiadjis et al. (2011). In Remark 3.5, we show how to make use
of (3.17) to estimate the score function and observed information matrix for an ssm. The
same setup can also be used in the expectation maximisation (em; Dempster et al., 1977;
McLachlan and Krishnan, 2008) algorithm as discussed by Del Moral et al. (2010).

3.2
Three sampling strategies
57
Remark 3.5 (Estimating the score function and information matrix for an SSM). The score function
(2.15) for an ssm can be estimated using the Fisher identity (Cappé et al., 2005) given by
S(θ′) =
 f
∇log pθ(x0:T , y1:T )
θ=θ′
g
pθ(x0:T |y1:T ) dx0:T ,
where log pθ(x0:T , y1:T ) denotes the complete data log-likelihood given by
log pθ(x0:T , y1:T ) = log µ(x0) +
T
X
t=1
f
log fθ(xt | xt−1) + log gθ(yt | xt)
g
.
(3.18)
This results in the additive functional
ξθ′,t(xt−1:t) = ∇log fθ(xt | xt−1)
θ=θ′ + ∇log gθ(yt | xt)
θ=θ′,
(3.19)
corresponding to part of the gradient of (3.18) evaluated at θ = θ′. The observed information matrix
(2.16) can be estimated using the Louis identity (Cappé et al., 2005) given by
J (θ′) =
f
S(θ′)
g2 −
f
∇2pθ(y1:T )
θ=θ′
g f
pθ(y1:T )
g−1,
(3.20)
where the second term can be expressed as
f
∇2pθ(y1:T )
θ=θ′
g f
pθ′(y1:T )
g−1 =
 f
∇log pθ(x0:T , y1:T )
θ=θ′
g2pθ(x0:T |y1:T ) dx0:T
+
 f
∇2 log pθ(x0:T , y1:T )
θ=θ′
g
pθ(x0:T |y1:T ) dx0:T .
Note that the ﬁrst term in (3.20) is the square of the score function, which can be estimated by
(3.19). The term E[∇2 log pθ(x0:T , y1:T )|y1:T ] can be estimated using an analogue additive functional.
However, this cannot be done for the remaining term as it consists of terms with the structure
ξθ,t(xt−1:t)ξθ,s(xs−1:s) for all s, t ∈{1, . . .,T }. We return to this problem and how to estimate these
two identities in Section 3 (page 162) of Paper B.
Example 3.6: How does unemployment aﬀect inﬂation? (cont. from p. 54)
From the previous, we know how to estimate the log-likelihood and the gradient of the
log-posterior. It is possible to make use of this information in a gradient ascent or quasi-
Newton algorithm for parameter inference by maximising the log-likelihood/posterior.
Unfortunately, this can be diﬃcult as the log-likelihood and the gradient can be quite noisy.
In Figure 3.7, we present estimates of these two quantities obtained using a bpf with N = 50
particles while varying φ. The gradient of the log-posterior is estimated using a fl particle
smoother, which makes use of the particle system generated by the bpf, see Remark 3.2.
We note that the log-likelihood estimate is quite noisy for this model and the optimum
around 0.75 is indistinguishable from the surroundings. However, the gradient estimate is
less noisy and it is zero at 0.81, which indicates the map estimate of φ.
Here, it could be possible to make use of a direct optimisation method for estimating φ
based on the gradient estimate, see Kok et al. (2015) and Poyiadjis et al. (2011). In other
models, the noise can be even larger and increasing N could make inference computationally
prohibitive. We return to discuss other remedies for this problem in Chapter 4.
We return to this model in Example 3.13 on page 70.

58
3
Monte Carlo methods
0.0
0.2
0.4
0.6
0.8
1.0
-50
-49
-48
-47
-46
-45
φ
log-likelihood
0.0
0.2
0.4
0.6
0.8
1.0
-400
-200
0
200
400
φ
gradient of log-posterior wrt φ
Figure 3.7. The estimates of the log-likelihood (upper) and the gradient of log p(θ |y) with
respect to φ (lower) in the Phillips curve model computed by the bpf and fl particle smoothing
when varying φ. The dotted vertical lines indicate the maximum likelihood estimate (upper)
and map estimate (lower) and the dotted horizontal line indicate the value zero for the gradient
of the log-posterior.

3.2
Three sampling strategies
59
Markov chain Monte Carlo
Another approach for sampling from complicated target distributions is to make use of
mcmc algorithms. The main idea is to construct a Markov chain with the target as its
stationary distribution. An advantage of mcmc compared to e.g. importance sampling is
that it can be easier to ﬁnd a good proposal distribution for an mcmc algorithm. The reason
for this is that it is possible to use the current state of the Markov chain to make a more
informed proposal for the next state.
To explain what this means, we need to introduce some essential concepts of Markov chains.
More general introductions to the subject are found in Meyn and Tweedie (2009) and
Billingsley (2012). Introductions to Markov chains for mcmc are available in Tierney
(1994) and Robert and Casella (2004).
A sequence of random variables {xk}K
k=0 is a Markov chain if
P[xk ∈A| x0:k−1] = P[xk ∈A| xk−1] =

A
R(xk−1, dxk) =

A
R(xk−1, xk) dxk,
where xk ∈X denotes the state of the Markov chain at time k and A denotes some
(measurable) subset of X . Here, the Markov kernel R : X ×X →[0, 1] assigns a probability
to any (measurable) subset given the current state xk−1.
In this thesis, we assume that R(xk−1, dxk) admits a density denoted R(xk−1, xk) with
respect to the Lebesgue measure. However, all results presented in the following carries
over to when the Markov kernel is not absolutely continuous, e.g., when the kernel is a
combination of a density and a Dirac distribution.
An important property of the Markov chain is the concept of an invariant distribution. We
say that a distribution µ is invariant to the Markov chain if
µ(xk) =

X
µ(xk−1)R(xk−1, xk) dxk−1,
(3.21)
which we write in short as µ = µR. The concept of invariance means that xk ∼µ for all
future values of k if xk−1 ∼µ. Hence if the above holds for any k, then the Markov chain
has entered its stationary regime. Note that, we make use of R as an operator or transform
in (3.21). Hence, we can see Markov kernel as a mapping between the distribution of xk−1
and the distribution of xk.
The invariance property is important as we are interested in sampling from a target distribu-
tion using a Markov chain with a speciﬁc stationary distribution. The remaining question
is how to construct R such that the stationary distribution matches the target π. It turns
out that a suﬃcient condition for the chain to have a stationary distribution µ is
µ(xk−1)R(xk−1, xk) = µ(xk)R(xk, xk−1),
for any xk−1, xk ∈X .
(3.22)
This condition is known as detailed balance and it ensures that the Markov chain is reversible.
That is, the statistics of the Markov chain are the same if the direction of time is reversed.
To conﬁrm this, we can integrate both sides to recover the invariance property in (3.21),

60
3
Monte Carlo methods
i.e.,

X
µ(xk−1)R(xk−1, xk) dxk−1 =

X
µ(xk)R(xk, xk−1) dxk−1
= µ(xk)

X
R(xk, xk−1) dxk−1
= µ(xk),
where the invariance property µR = µ is recovered. Here, the detailed balance (3.22) gives
the ﬁrst step and the property

X R(xk, xk−1) dxk−1 = 1 is used in the third step.
Another important concept in Markov chain theory is ergodicity, which is required later to
establish certain statistical properties of mcmc algorithms. Intuitively, an ergodic Markov
chain can visit any (interesting) part of the state space at any point in time. This is necessary
as we would like to make use of the Markov chain to sample the target. Hence, it must be
able to visit all parts of the target with a non-zero probability mass to be able to explore it
fully. We might end up with problems if the Markov chain is not ergodic as it then can get
stuck in certain parts of the state space and therefore not revisit all areas of the target with
some regularity.
It is possible to show that if the Markov chain is ergodic, then it also has a unique invariant
distribution satisfying
µ = lim
k→∞µ0Rk,
for (almost) any initial distribution µ0. Hence, we have a unique limiting distribution and
it is the stationary or invariant distribution of the Markov chain governed by the kernel R.
The requirements for ergodicity are irreducibility, aperiodicity and positive recurrence. We
say that a chain is (strongly) irreducible if

A
R(xk−1, xk) dxk > 0,
for any xk−1, xk ∈X . Hence, the Markov chain can reach any part of the state space in one
step. Aperiodicity roughly means that the Markov chain does not get stuck in cycles where
it returns in a number of steps with a certain period. Finally, a Markov chain is positive
recurrent if the expected number of visits to any subset of the state space is inﬁnite. The
opposite case is known as transience, where the expected number of returns to a certain
set is zero after M < ∞steps. We return to what these properties mean in practice when
introducing some speciﬁc examples of mcmc algorithms.
The remaining question is how to construct the Markov kernel R in practice such that it
has some sought target distribution π as its stationary distribution. That is, a ergodic kernel
R that fulﬁls detailed balance for π. We present two diﬀerent approaches: the mh algorithm
and Gibbs sampling. After this, we return to discussing how to make use of the Markov
chain generated by these approaches to estimate expectations of test functions with respect
to some target.

3.2
Three sampling strategies
61
Metropolis-Hastings
We can construct a Markov chain to sample from some target distribution π(x) = γ(x)Z−1
using the mh algorithm (Metropolis et al., 1953; Hastings, 1970). Here, we remind the reader
that γ denotes the un-normalised target distribution and Z denotes the often unknown
normalisation constant.
Remark 3.7 (Bayesian parameter inference using the MH algorithm). In many applications, we have
the parameters θ as the state and the target as the parameter posterior π(θ) = p(θ |y) given by
Bayes’ theorem (2.10). In this setting, the normalisation constant is the marginal likelihood p(y).
Furthermore, we have the un-normalised target γ(θ) = p(y |θ)p(θ), where p(y |θ) and p(θ) denote
the likelihood and the prior distribution, respectively.
The mh algorithm consists of an iterative scheme in which we propose a new state of the
Markov chain called the candidate state x′ from some proposal distribution q(x′| xk−1)
admitting a density with respect to the Lebesgue measure. The candidate state is then
accepted or rejected according to some acceptance probability. In summary, we carry out
the following operations during iteration k:
a) Sample the proposal x′ ∼q x′| xk−1

.
b) Set the next state
xk =

x′
with probability α xk−1, x′,
xk−1
with probability 1 −α xk−1, x′.
These steps are repeated until we obtain K samples from π denoted by {xk}K
k=1. The
acceptance probability is given by
α(xk−1, x′) = 1 ∧
π(x′)
π(xk−1)
q(xk−1 | x′)
q(x′| xk−1) = 1 ∧
γ(x′)
γ(xk−1)
q(xk−1 | x′)
q(x′| xk−1),
(3.23)
where a ∧b ≜min(a, b) and when the proposal can be expressed as a density. Note that
the normalisation constant Z cancels and only point-wise evaluation of the un-normalised
target is required for computing the acceptance probability.
Typically, the Markov chain is constructed by the mh algorithm such that it explores the
posterior distribution by local moves thus exploiting the previously accepted state. Hence,
it focuses its attention to areas of the state space in which the posterior assigns a relatively
large probability mass. We can see this from (3.23), a proposed state x′ is always accepted
(neglecting the inﬂuence of the proposal) if it results in larger value of the target compare
with x. Furthermore, we accept a proposed state with some probability if this results in a
small decrease of the target compared with the previous iteration.
This results in that the mh sampler both allows for the Markov chain to ﬁnd and for it to
explore areas of high posterior probability. Hence, the mh algorithm can possibly escape
local extrema if the target is multi-modal which is a problem for many local optimisation
algorithms used in numerical maximum likelihood inference. It is also easier to construct
a good proposal for a high-dimensional target using local moves as in the mh algorithm
compared with the importance sampling algorithm.

62
3
Monte Carlo methods
In the mh algorithm, we can express the Markov kernel by
R(xk−1, dxk) = α(xk−1, xk)q(xk | xk−1) dxk
+

1 −

X
α(xk−1, z)q(z | xk−1) dz

δxk−1(dxk),
(3.24)
which cannot be expressed as a density due to the fact that the Dirac distribution is not
absolutely continuous. Hence, we write the kernel and the proposal as measures in this case.
This kernel satisﬁes the detail balance condition (3.22), which can be veriﬁed by
π(xk−1)q(xk | xk−1)α(xk−1, xk) = π(xk−1)q(xk | xk−1)
"
1 ∧π(xk)
π(xk−1)
q(xk−1 | xk)
q(xk | xk−1)
#
,
= π(xk−1)q(xk | xk−1) ∧π(xk)q(xk−1 | xk),
= π(xk)q(xk−1 | xk)α(xk, xk−1),
by using the deﬁnition of the acceptance probability (3.23). This results in that the target
distribution is the stationary distribution of the Markov chain. Finally, the Markov chain
generated by the mh algorithm is ergodic if the proposal q(x′| x) > 0 for all x′, x ∈X .
That is, the probability of reaching any set of the state space in one or a few steps is non-zero.
Further details are provided by Tierney (1994) and Robert and Casella (2004).
The performance of the mh algorithm is dependent on the choice of the proposal and the
resulting acceptance rate. Two common choices for proposals are the independent Gaussian
proposal and Gaussian random walk given by
q(x′| x) = q(x′) = N(x′; µ, Σ),
q(x′| x) = N(x′; x, Σ),
for some mean vector µ and covariance matrix Σ. Note that the independent proposal
cannot exploit the previous accepted state, which can be problematic if the proposal does
not match the target well. If the match is good, it is always possible to instead use an
importance sampling algorithm, which is probably the better choice.
The Gaussian random walk is the typical choice in applications but a version based on the
Student’s t distribution is also common. The choice of Σ determines the performance of
the mh algorithm. If it is too small, the algorithm tends to accept proposed steps but as each
step is small this results in a large autocorrelation in the chain. Moreover, the probability
of accepting a large step is also low, which results in a large autocorrelation as well. In the
mcmc literature, this is referred to bad mixing of the Markov chain, see Figure 5 (page 136)
in Paper A for an illustration of good and bad mixing. A good choice of the proposal is
therefore crucial to obtain a small autocorrelation in the Markov chain and (as we shall see)
accurate estimates of the target.
Gibbs sampling
Another approach to construct a Markov kernel to sample the target distribution of interest
is to use Gibbs sampling (Geman and Geman, 1984). A major diﬀerence with the mh
algorithm is that all the draws from the proposal distribution are accepted by the algorithm.
However, this does not mean that Gibbs sampling is superior to other types of mcmc

3.2
Three sampling strategies
63
algorithms. It is actually possible to show that Gibbs sampling is a particular special case
of the mh algorithm, see Robert and Casella (2004, p. 381). Gibbs sampling can only be
implemented for some models that admit conditional distributions with a special structure.
It can also get stuck if some states are highly correlated with each other. The main advantage
is that no tuning is required by the user which make implementation easier.
In Gibbs sampling, we sample the full conditional distribution for each element of the state
vector while keeping the other elements ﬁxed to their current value. Hence, we repeat the
following step during iteration k:
xk,i ∼πi
 xki | xk,1, . . ., xk,i−1, xk−1,i+1, . . ., xk−1,p
,
for each element of the state i = 1, . . ., p. Here, we denote the full condition distribution
for xi of the target distribution π(x) as πi(xi | · ). The resulting Markov kernel is given by
R(xk−1, xk) =
p
Y
i=1
πi(xk,i | xk,1, . . ., xk,i−1, xk−1,i+1, . . ., xk−1,p).
(3.25)
It is possible to show (Robert and Casella, 2004, p. 345) that π(x) = π(x1, x2, . . ., xp) is
the invariant distribution of the Markov chain governed by (3.25). Furthermore, the kernel
generates an ergodic chain if the density π(x) satisﬁes the positivity condition. That is, if
π(x) > 0 for all xi such that πi(xi) > 0 for i = 1, 2, . . ., p. Here, we denote the marginal
density of xi under π(x) by πi(xi). In summary, we have that the Gibbs sampler generates
samples from the target distribution in its stationary regime.
Compared with the mh algorithm, the full conditionals depend on the problem at hand
and Gibbs sampling cannot be used for all models. It is common to make use of conjugate
priors to obtain some posterior π(x) with the necessary conditionals. In Example 3.8, we
show how to apply Gibbs sampling for inference in the model regarding the ideological
leanings of us Supreme Court justices.
In practice, blocking and partial collapsing are used to increase the performance. Blocking
means that we sample more than one parameter at the same time, e.g.,
xk,1, xk,2 ∼π1,2(xk,1, xk,2 | xk−1,3),
xk,3 ∼π3(xk,3 | xk,1, xk,2).
An example of partial collapsing is the update
xk,1 ∼π1(xk,1 | xk−1,2),
xk,2 ∼π2(xk,2 | xk,1),
xk,3 ∼π3(xk,3 | xk,1, xk,2).
Both approaches can result in a signiﬁcant increase in performance. However, the design
is problem speciﬁc and care needs to be taken to ensure that the sampler converge to the
desired target.
Example 3.8: Voting behaviour in the US Supreme court (cont. from p. 31)
We follow Martin et al. (2011) and assume the following priors
αt ∼N(αt; 02, 0.25I2),
xi ∼N(xi; 0, 1),
for each justice i = 1, . . ., n and case t = 1, . . .,T . From these choices of prior, we can
compute the conditional posteriors as presented in Example 2.6, see Albert (1992) for the

64
3
Monte Carlo methods
complete derivation. The resulting update for the Gibbs sampler is given by
u′
it ∼

tn[0,∞)(u′
it; −αt,1 + αt,2xi, 1)
if yit = 1,
tn(−∞,0](u′
it; −αt,1 + αt,2xi, 1)
if yit = 0,
α′
t ∼N(α′
t; mα,t, (X ⊤X)−1),
x′
i ∼N(x′
i; mx,i, σ−2
x ),
for each justice i = 1, . . ., n and case t = 1, . . .,T .
We introduce the notation X ≜[−1
x1:n]⊤for the design matrix. Furthermore, we use
ui = {uit }T
t=1 and introduce tn(a,b)(µ, σ2) to denote the truncated Gaussian distribution
on the interval (a, b) with location µ ∈R and scale σ > 0. Finally, we introduce the
following auxiliary quantities
mα,t = (X ⊤X)−1X ut,
mx,i = σ−2
x
TX
t=1
α′
t,2(uit + α′
t,1),
σ2
x = 1 +
TX
t=1
(α′
t,2)2,
where mα,t is the same as the ordinary ls estimate (2.3).
We make use of the command MCMCirt1d implemented in the R package MCMCpack (Martin
et al., 2011) for the inference. The Gibbs sampler is run for K = 50, 000 iterations (discarding
the ﬁrst 10, 000 as burn-in). In Figure 3.8, we present the resulting marginal posteriors for
x1:9. From this, we note that: (i) Ginsberg, Sotomayor, Breyer and Kagan are more liberal
and (ii) Scalia, Thomas and Altio are more conservative. This is the same result as we
predicted in Example 2.2 when ﬁrst introducing the data set.
We return to this model in Example 5.2 on page 88.
Statistical properties
In this section, we summarise some of the statistical results that mcmc algorithms relies
upon and brieﬂy mention their underlying assumptions. For more information about the
properties of mcmc algorithms in general, see e.g., Tierney (1994), Robert and Casella
(2004) and Meyn and Tweedie (2009). A natural estimator of π[ϕ] for any integrable test
function ϕ using the Markov chain generated by a mcmc algorithm is given by
DπK
mcmc[ϕ] = 1
K
K
X
k=1
ϕ(xk),
(3.26)
where xk denotes the state of the Markov chain at time step k with π as its stationary
distribution. Note that this intuitively means that time spent by the Markov chain in
particular region is proportional to the probability mass allocated to the region. Hence, we
can map the distribution of probability mass in a target distribution but observing how the
Markov chain spends its time in the state space by e.g., a histogram.
A practical problem is that the estimator (3.26) only holds for samples from the Markov
chain when it is in stationarity. That is, when the distribution of the Markov chain is the
limiting or stationary distribution, which usually is the target distribution of interest. In

3.2
Three sampling strategies
65
practice, it is diﬃcult to assert when this occurs and often the Markov chain does not reach
stationarity for many iteration.
However, it is common to run the Markov chain during a burn-in (or warm-up) phase and
discard the samples from this period. After the burn-in, we assume that all the samples are
from the target distribution. Note that there are a number of convergence tests and similar
to make use of for diagnostics. More information is found in Section 6 (page 142) of Paper A
and in Robert and Casella (2009, p. 242).
By the ergodic theorem (Tierney, 1994; Robert and Casella, 2004), we know that the estima-
tor (3.26) is strongly consistent, i.e.,
DπK
mcmc[ϕ]
a.s.
−→π[ϕ],
when K →∞. Note that this property does not follow directly from the slln as the
samples obtained for the target are not iid, due to the fact that the states of the Markov
chain are correlated.
Furthermore, it is possible to form a clt for the estimator with some additional assump-
tions, see Jones (2004). Usually, we assume that the Markov chain is uniformly ergodic,
i.e.,
∥Rk(x0, · ) −π∥tv < C ρ−k,
for some C < ∞, ρ > 1 and any x0 ∈X . Here, ∥· ∥tv denotes the total variational (tv)
norm given by
∥µ −π∥tv ≜1
2

X
|µ(x) −π(x)| dx,
when both densities are dominated by the Lebesgue measure. This means that for any
starting point x0, we converge to the stationary distribution with a geometric rate. This is
a stronger condition compared with geometric convergence, where the same convergence
holds for most x0 ∈X . Given uniform/geometric convergence, we can ﬁnd a clt given by
√
K
f
DπK
mcmc[ϕ] −π[ϕ]
g
d
−→N 0, σ2
mcmc
,
when K →∞. Here, σ2
mcmc denotes the variance of the estimator given by
σ2
mcmc = V
f
DπK
mcmc[ϕ]
g
= π Hϕ2(x0) + 2
∞
X
k=1
π
f
Hϕ(x0)Hϕ(xk)
g
,
where we introduce Hϕ(x) ≜ϕ(x) −π[ϕ(x)] for brevity. We can rewrite this as
σ2
mcmc = π Hϕ2(x0)
· iact x1:K
,
where the integrated autocorrelation time (iact) is given by
iact x1:K
 ≜1 + 2
∞
X
k=1
ρk
 x1:K
.

66
3
Monte Carlo methods
liberal/conservative score
density
-4
-2
0
2
4
0
1
2
3
4
Scalia
Kennedy
Thomas
Ginsburg
Breyer
Roberts
Alito
Sotomayor
Kagan
Figure 3.8. The estimates of marginal posteriors for x1:9 in the irm for the supreme court
justice data, which represent the liberal/conservative score for each justice.
φ
posterior estimate
0.5
0.6
0.7
0.8
0.9
1.0
0
2
4
6
8
α
posterior estimate
0.0
0.2
0.4
0.6
0.8
1.0
0.0
1.0
2.0
3.0
β
posterior estimate
-0.04
-0.02
0.00
0.02
0.04
0
20
40
60
80
σv
posterior estimate
0.20
0.25
0.30
0.35
0
10
20
30
40
50
Figure 3.9. Estimates of the parameter posterior distribution for φ (green), α (orange), β (pur-
ple) and σe (magenta) in the Phillips curve model using the Swedish data. The prior distribu-
tions are indicated by grey cures and the posterior means by dotted lines.

3.3
Pseudo-marginal Metropolis-Hastings
67
Here, ρk(x) denotes the k-lag autocorrelation function (acf) of ϕ(xk). In practice, we
have to estimate the iact by approximating it using the sample estimates, see Section 6.3
(page 142) in Paper A for more information.
The iact can be interpreted as the number of iterations between each independent sample
and it is connected with the mixing property. Hence, a small value of the iact means
that the Markov chain mixes well, nearby samples are almost uncorrelated and that the
asymptotic variance is small. Note that an importance sampling algorithm makes use of
independently proposed particles and its iact is therefore one. However, the non-equal
weights in the importance sampling estimator results in other ineﬃciencies.
Remark 3.9 (Optimal Gaussian random walk proposal for the MH algorithm). It is possible to com-
pute the optimal Gaussian random walk proposal for a Gaussian target distribution by analytically
minimising the iact. The resulting proposal (Roberts et al., 1997) is given by
q(x | x′) = N
 
x′; x, 2.382
p
DΣ
!
,
(3.27)
where DΣ denotes an estimate of the posterior covariance and p denotes the number of parameters
in the model. The covariance is often unknown but it can be estimated using pilot runs. However,
this is problematic as its estimation essentially means that we already have a Markov chain with good
mixing.
It is also possible to calculate the resulting acceptance probability, which is 0.234 in the case of a
Gaussian target when using the optimal proposal. The performance of (3.27) can be poor if the target
deviates from a Gaussian distribution, e.g., if the target is more heavy-tailed than a Gaussian or is
multi-modal. In this case, it is possible to ﬁnd better proposals. However by the Bernstein-von-Mises
theorem from Section 2.3, the target concentrates asymptotically to a Gaussian as the amount of data
increases and then the Gaussian assumption is valid.
Pseudo-marginal Metropolis-Hastings
In Section 3.2.3, we illustrated how to make use of Markov chains to sample from some
complicated target distribution denoted by π(x). In the mh algorithm, it is required that
we can evaluate π(x) or its un-normalised version γ(x) point-wise in the computation of
the acceptance probability (3.23). However, we cannot implement the mh algorithm when
this is not possible.
Instead, we can often make use of importance sampling or smc to obtain un-biased but
noisy estimates of the target. These estimators are exploited by Beaumont (2003) to approx-
imate the acceptance probability in the mh algorithm by replacing the unknown target
distribution by a non-negative and un-biased estimate. We refer to the resulting algorithm as
a pseudo-marginal mh (pmmh) algorithm. It is proved by Andrieu and Roberts (2009) that
this is a valid approach and that the pmmh algorithm has similar convergence properties as
its exact counterpart. For example, the distribution of the Markov chain convergences to
the sought target distribution.
Note that the assumption of non-negativity is important in what follows to obtain a so-
called exact approximation of the mh algorithm. That is, an algorithm that returns samples

68
3
Monte Carlo methods
from the sought posterior. As previously mentioned, smc algorithms can provide one such
valid estimator but ﬁnding such estimators is diﬃcult in general. Assume that we have
an un-biased estimator of some quantity α ∈R, then there does not exist any algorithm
that can provide an un-biased estimator of ℎ(α) ∈R+ for some non-constant function
ℎ: R →R+. This fundamental result was recently established by Jacob and Thiery (2015).
The implications of this result is that exact approximation of algorithms can be diﬃcult
using approximate Bayesian computations (abc). As a consequence, we might have to
scarify un-biasedness to obtain inference algorithms with computational tractability. We
return to discussing this problem in Section 4.2.
Remark 3.10 (Bayesian parameter inference in non-linear SSMs). From Chapter 2, we know that the
likelihood is intractable for a non-linear ssm. Therefore, we cannot evaluate the target distribution
or its un-normalised version as p(y |θ) is unknown. The key point with pmmh is that the un-biased
estimate from the bPF discussed in Remark 3.1 can be used as a plug-in estimator for p(y |θ).
One approach to show the validity of the pmmh algorithm is to consider it to be a standard
mh algorithm targeting an extended target distribution. To see this, we assume that there
exists a non-negative and un-biased estimator of the un-normalised target γ(x) such that
Eu
f
DγN (x | u)
g
=

U
DγN (x | u) m(u) du = γ(x),
(3.28)
where u ∈U denotes the multivariate random samples with density m(u) with respect to
the Lebesgue measure used to construct this estimator. Remember that the normalisation
constant Z cancels in the acceptance probability for the mh algorithm. Hence, it does not
matter that we can only estimate the value of the un-normalised target in this setting.
Remark 3.11 (The random samples u). In the pmmh algorithm, the variable u contains the random
variables used to estimate the target. These corresponds to samples from the proposal distribution
when an importance sampler is applied to estimate the target. In the smc algorithm, the random
variables are used in the resampling step and for propagation. Hence, we can see a smc algorithm
such as the bpf algorithm as a deterministic algorithm given u. In this case, we refer to the resulting
algorithm as the particle mh (pmh) algorithm. It was ﬁrst introduced by Fernández-Villaverde and
Rubio-Ramírez (2007) and later analysed by Andrieu et al. (2010).
Furthermore, we assume a proposal distribution for x and u such that
q(x′, u′| x, u) = qx(x′| u, x) qu(u′| u).
(3.29)
Note that the random variables u can be used when proposing the candidate state x′. We
will ﬁnd this useful in Section 4, when we propose extensions of the pmmh algorithm to
improve mixing.
Finally, we can introduce the mh algorithm operating on the extended space X × U with
the extended target given by
γ(x, u) = DγN (x | u) m(u).
(3.30)
As a result, we can recover the original target by the marginalisation given by the un-
biasedness property in (3.28). Note that the resulting mh algorithm with the target (3.30)

3.3
Pseudo-marginal Metropolis-Hastings
69
and proposal (3.29) has an acceptance probability given by
α(x, u, x′, u′) = 1 ∧DγN (x′| u′)
DγN (x | u)
q(x, u | x′, u′)
q(x′, u′| x, u).
(3.31)
This acceptance probability is the same as for the mh algorithm but replacing the target
with an un-biased estimator of it and including an extended proposal. Note that this is not
a formal proof of the validity and the interested reader is referred to Andrieu and Roberts
(2009) for a more detailed presentation.
To recapitulate, the pmmh algorithm consists of an iterative scheme in which we propose
x′ and u′ using (3.29) and accept with the probability given by (3.31). Hence, we carry out
the following operations during iteration k:
a) Sample the state proposal x′ ∼qx
 x′| xk−1, uk−1

.
b) Sample the auxiliary variable proposal u′ ∼qu
 u′| uk−1

.
c) Compute the estimate of the un-normalised target DγN (x′| u′).
d) Set the next state
{xk, uk} =

{x′, u′}
with probability α xk−1, uk−1, x′, u′,
{xk−1, uk−1}
with probability 1 −α xk−1, uk−1, x′, u′.
These steps are repeated until we obtain K samples from π(x, u) denoted by {xk, uk}K
k=1.
The statistical properties of the pmmh algorithms are similar to the mh algorithm. How-
ever, the performance of pmmh often depends on the number of samples N , which are used
to compute point-wise estimates of the un-normalised target. If N is too small, then the vari-
ance of the estimates are large and therefore we often get stuck with the Markov chain with
a resulting low acceptance rate. We also know that N is connected to the computational
cost of estimating the target.
Remark 3.12 (Optimal Gaussian random walk proposals and selecting N in the PMH algorithm). There
is a trade-oﬀbetween the number of pmmh/pmh iterations and the computational complexity in the
estimator of the target. The overall aim is to minimise the total computational cost of the algorithm.
This problem is analysed for the pmh algorithm by Doucet et al. (2015) and Pitt et al. (2012). Their
conclusion is to select N such that the variance of the target estimates is between 1.0 and 1.7 depending
on the eﬃciency of the proposal for θ.
Furthermore, it is possible to construct optimal Gaussian random walk proposals (minimising the
iact for a Gaussian target) for the pmh algorithm in analogue with the mh algorithm. This is
investigated by Sherlock et al. (2015) and the resulting proposal is given by
q(x′ | x) = N
 
x′; x, 2.5622
p
DΣ
!
,
(3.32)
where DΣ again denotes an estimate of the posterior covariance and p denotes the dimension of the
parameter vector. The resulting acceptance probability is 0.07 in the case of a Gaussian target when
using the optimal proposal. Compared with the mh algorithm, this acceptance rate is much smaller
and this is due to the noise in the likelihood estimator.

70
3
Monte Carlo methods
Example 3.13: How does unemployment aﬀect inﬂation? (cont. from p. 57)
We are now interested in estimating the parameter posterior π(θ) where θ = {φ, α, β, σe}.
To do this, we need to augment the model with prior distributions for each parameters,
where we choose
φ ∼tn(−1,1)(φ; 0.8, 0.12),
α ∼N(α; 0.5, 0.22),
β ∼N(β; 0, 0.12),
σe ∼G(σe; 2, 4),
where G(a, b) denote the Gamma distribution with expected value a/b.
To sample from the posterior, we employ the pmh algorithm with the bPF to provide
estimates of the likelihood. We make use of N = 1, 000 particles in the bpf, K = 15, 000
iterations in the pmh algorithm and discard the ﬁrst 5, 000 iterations as burn-in. We select
an independent proposal for u given by u ∼N(0, 1) and the random walk proposal in
(3.32) for θ. The posterior covariance is estimated using a pilot run and it is given by
DΣ = 10−3 ·

7
2
1
0
2
48
3
−1
1
3
1
0
0
−1
0
0.05

.
In Figure 3.9, we present the marginal parameter posterior for the four parameters in the
model. We note that the Phillips curve hypothesis is not supported by this data set as there
seems to be little support for that β is negative. Remember that the sign of β determines the
correlation between the inﬂation and unemployment rates. The Phillips curve hypothesis
stated that this correlation should be negative and this then implies a negative value for β.
Furthermore, it is possible to make use of the pmh algorithm to estimate the nairu by
averaging over the state of the Markov chain. This means, that we can compute the posterior
of the latent state with the parameters marginalised out. Hence, we take into account the
uncertainty in the parameter posterior when estimating the state. This is diﬀerent from the
approach in Example 3.3 on page 54, where the parameters are ﬁxed to a single value.
We present the resulting estimate of the nairu using pmh in the lower part of Figure 3.6
on page 53. Note that the estimates from the bPF and the pmh algorithms are similar.
We return to this model in Example 4.1 on page 83.
Outlook and extensions
We conclude this chapter with an outlook and some possible extensions to the methods that
we have introduced. As discussed in the beginning of the chapter, there are two diﬀerent
main approaches to approximate intractable posterior distributions. We have presented the
approach based on statistical simulation in the form of Monte Carlo methods. However,
variational inference can be useful in applications when speed is important. This approach
is often based on approximating the posterior by a Gaussian (or some other member of
the exponential family of distributions) and make use of simple updates in analogue with
conjugate priors. Good general introductions to these methods are provided in the books

3.4
Outlook and extensions
71
by Bishop (2006) and Murphy (2012). A recent survey aimed for statisticians is provided
by Blei et al. (2016).
The main diﬃculty in using importance sampling is to ﬁnd a suitable proposal distribution
to sample from. This is especially diﬃcult in high-dimensional problems, where smc and
mcmc can be better alternatives. However, there are interesting methods in the literature to
adapt and/or combine several proposal distributions in importance sampling, see Cornuet
et al. (2011) and Veach and Guibas (1995). This idea can also be used in smc algorithms as
proposed by Kronander and Schön (2014). Another approach to construct better proposals
for smc algorithm is introduced by Naesseth et al. (2015). Population Monte Carlo makes
use of similar ideas and can be an interesting alternative, see Cappé et al. (2004). It is
also possible to make use of smc algorithms for parameter inference in ssms and other
interesting models. One such algorithm is the smc2 algorithm proposed by Chopin et al.
(2013). Finally, Quasi-random numbers can be useful in both importance sampling and smc
to decrease the variance in the estimates, see Gerber and Chopin (2015).
Adaptive algorithms have also been developed for the mh and pmh algorithms, see Andrieu
and Thoms (2008) and Peters et al. (2010). These approaches are often based on the rule-
of-thumb for selecting the optimal proposal and computes the unknown covariance matrix
on-the-ﬂy. It is also possible to make use of information regarding the gradient and Hessian
of the log-posterior to tune the proposal. This approach was proposed by Girolami and
Calderhead (2011) for the mh algorithm and we return to it in the context of the pmh
algorithm in Chapter 4.
Finally, there are a large number of additional mcmc algorithms, which are not discussed
in this thesis. Hamiltonian mcmc (hmc; Duane et al., 1987) is a interesting approach
to sample from high-dimensional targets using simulated Hamiltonian dynamics, see Neal
(2010). A big challenge is to create a pseudo-marginal version of this algorithm for parameter
inference in latent variable models such as the ssm. Sequential mcmc has been discussed
by e.g., Brockwell et al. (2010) as an alternative to pmh. Slice sampling introduced by Neal
(2003) is also an interesting and popular alternative. A pseudo-marginal version of slice
sampling was recently proposed by Murray and Graham (2015). Finally, mcmc algorithms
can also be useful for optimisation and a pseudo-marginal scheme for this is proposed by
Finke (2015).


4
Strategies for accelerating inference
We know from the previous chapters that Monte Carlo methods are useful for enabling
Bayesian inference in many interesting models. A drawback with these approaches are that
a large number of samples from the posterior are usually required to obtain reasonable
accuracy. This is often not a problem for simpler Monte Carlo methods, where samples can
be generated eﬃciently. However, for more complicated models it can take considerable
time to generate a sample.
There are two main approaches to mitigate this problem: (i) decreasing the computational
cost of generating a sample or (ii) decreasing the required number of samples by making
better use of them. We explore both strategies in this chapter to accelerate inference, i.e.,
decrease the total time for the inference, for some problems. We later show how these
strategies are employed in the papers included in this thesis. Another aspect for the user is
the time and complexity of the implementation of an inference algorithm for a new model.
It can therefore be beneﬁcial to apply simpler methods that are quicker to implement and
tune than more elaborate algorithms that may be more eﬃcient but takes longer time to
get up an running. The total time for the entire inference can therefore be shorter in the
former case than in the latter, even if the advanced algorithm is more eﬃcient.
To statistical procedure to ﬁt a model to data consists of three steps as discussed in the
beginning of Chapter 2: collecting data, selecting a model and inferring the parameters of
the model given the data. In this chapter, we propose some strategies accelerating inference
in these three steps. That is, starting with how to generate and collect the data, then how
to change the model and lastly how to change the inference algorithm to make faster and
easier inference.
73

74
4
Strategies for accelerating inference
Increasing the amount of information in the data
The ﬁrst approach to accelerate inference is to make data more informative about the
parameters. If this is accomplished, we can obtain accurate estimates of the parameters
using a smaller amount of observations T . In the smc algorithm, this results in a linear1
decrease of the computational cost that also carries over to the pmh algorithm and similar
methods. In Paper H, we present results that illustrates how more informative data can
actually increase the convergence rate of the em algorithm.
A common approach to make data more informative is the use of an input signal which
excites the system. In the ﬁeld of input design, the main objective is to construct this input
u = {ut }T
t=1 such that the observations y = {yt }T
t=1 are as informative as possible. The
amount of information in the observations y is described by the Fisher information matrix
I(θ⋆), see (2.17), which is the curvature of the log-likelihood at the true parameter θ⋆. A
larger size of the Fisher information matrix also results in a smaller asymptotic variance of
the Bayes and maximum likelihood estimators, see (2.19). To quantify the size of this matrix,
we usually employ the logarithm of its determinant denoted by ℎ(I(θ⋆)) = log det I(θ⋆).
A suitable input can then be computed by
u⋆= u α⋆,
α⋆= argmax
α
ℎ I(θ⋆, u(α)),
for some family of inputs u(α) parametrised by α and where I(θ⋆, u) denotes the Fisher
information matrix when the input u is applied.
We encounter two main problems with this approach for ﬁnding u⋆namely: (i) how do we
parametrise a good input u(α) for a speciﬁc model and (ii) how do we compute/estimate
I(θ⋆, u). We revisit these problems in Paper H. For problem (i), we make use of graph
theory to construct a number of basis inputs. The optimal input signal is formed by ﬁnding
the convex combination of these basis inputs with weighting α such that the size of the
Fisher information matrix is maximised.
For problem (ii), the Fisher information matrix is estimated for each basis input by using
smc methods and the Fisher identity introduced in Remark 3.5. The major challenge is to
obtain accurate estimates of I(θ⋆, u) with a reasonable computational cost. We propose to
accomplish this by adapting an estimator introduced by Segal and Weinstein (1989) for the
Kalman ﬁlter to the smc framework. However, the Louis identity from Remark 3.5 can
also be used. The main problem with this alternative is that the estimates of the information
matrix often are negative deﬁnite. The estimates from the approach proposed by Segal and
Weinstein (1989) are always pd.
Approximating the model
The second approach that we consider to accelerate inference is to approximate the model
and then make use of standard algorithms for inference. Note that the approximation of the
model typically results in a bias in the estimates. However, it could potentially have a large
1Actually quadratic since the number of particles N typically needs to scale linearly with T , which results in a
computational cost proportional to nt ∝T 2.

4.2
Approximating the model
75
impact on the computational cost and can enable inference in otherwise intractable models.
Here, we discuss using: (a) sparseness priors for model order selection, (b) approximate
Bayesian computations for ssms with intractable likelihoods and (c) using a surrogate
function to approximate the log-likelihood and/or log-posterior.
Over-parametrised models with sparseness priors
Model order selection is an important problem in statistics. For example, it is a challenging
task to determine a suitable model order p of an ar process (2.6) to best ﬁt a given data
set. In Bayesian inference, we can introduce the model order as a parameter to be inferred
together with the parameters of the model. This type of inference can be carried out by
the reversible-jump Metropolis-Hastings (rjmh) algorithm introduced by Green (1995).
However, it can be challenging to ﬁnd proposal distributions for p and θ that result in rea-
sonable mixing. Furthermore, implementation can be challenging even for simple models.
See Paper F for a concrete example where we infer p for an arx process using rjmh.
An alternative approach is to make use of an over-parametrised model and employ sparseness
priors to penalise using more parameters than supported by the data. This is similar to the
use of regularisation in linear regression as discussed in Remark 2.5. The surprising result
is that this can provide consistent estimates in some cases as proven by Rousseau and
Mengersen (2011). That is, the posterior distributions of the parameters and the model
order are asymptotically the same compared with using an exact approach such as the rjmh
algorithm. The main beneﬁt is that sparsity in some cases can be induced by using conjugate
priors. This enables making use of Gibbs sampling, which can be simpler to implement
compared with the rjmh algorithm and can also result in better mixing. Hence, we can say
that this accelerates inference in some type of models, where the model order is unknown.
A concrete example of this is again model order selection in arx models as discussed in
Paper F. Here, we make use of ard priors to induce sparsity in this case. This corresponds
to an hierarchical model where a zero-mean Gaussian prior is selected for the ar coeﬃcients,
i.e., φk ∼N(0, σ2
0). Moreover, we assume that σ2
0 ∼ig(aσ
0 , bσ
0 ), where the shape aσ
0 > 0
and scale bσ
0 > 0 are so-called hyperparameters. This is similar to the l2-rls discussed in
Remark 2.5 on page 30 with the major diﬀerence that the prior variance is not ﬁxed but
estimated from the data.
Another example is mixture models to capture heterogeneity in the individual random
eﬀects in panel data. In the mixed eﬀects model (2.9), we assumed that the random eﬀects
were distributed according to a Gaussian distribution with some unknown mean and vari-
ance. In some applications, it would be interesting to allow for multi-modal distributions
of the random eﬀects where each mode captures the behaviour of a certain subgroup of the
individuals. Two illustrations of this are presented in Figure 4.1, where the distributions
clearly indicate that there are a number of clusters of random eﬀects in the data.
One possible approach to model the distributions of the random eﬀects is by using a Dirich-
let process mixture (dpm), see (2.27). However, inference in this type of models can be
challenging both from implementation perspective and because of poor mixing, see Hastie
et al. (2015). An alternative proposed by Ishwaran and Zarepour (2002) is to make use of an
over-parametrised Gaussian ﬁnite mixture model and put a sparseness prior on the number

76
4
Strategies for accelerating inference
of components. The resulting inference algorithm is easier to implement and performs
well in many problems, see e.g., Chapter 22 in Gelman et al. (2013). We make use of this
approach and compare with using dpm in Paper G. The main objective is to compare the
posterior estimates to validate if the ﬁndings in Rousseau and Mengersen (2011) carries over
to this settings.
Approximate Bayesian computations
For some models, it is not possible to approximate the posterior as the target cannot be
evaluated point-wise. This could be the result of that the likelihood cannot be expressed in
closed-form or that the computation cost is high. In an ssm, this problem corresponds to
it is not possible evaluate gθ(yt | xt) point-wise. For a bpf, this results in that the weights
cannot be computed. An example of an ssm with an intractable likelihood is when the
observations are modelled using an α-stable distribution (Nolan, 2003). This is a popular
choice in ﬁnance as discussed in Papers C and E.
It turns out that we can reformulate the ssms with intractable likelihoods by introducing a
small perturbation, which allows us to apply a standard particle ﬁlter. This approach is part
of a family of methods known as abc (Marin et al., 2012). For the particle ﬁlter, we can
construct a smc-abc algorithm (Jasra et al., 2012) to estimate the log-likelihood. In this
algorithm, we assume that the observations are perturbed by
y⋆
t = yt + ϵzt,
where ϵ ≥0 denote the tolerance parameter and zt denote a standard Gaussian random
variable. Hence, we make use of y⋆= {y⋆
t }T
t=1 as the observations from the model and
{xt, yt }T
t=1 as the latent states. This results in that it is only required to be able to generate
samples from gθ(yt | xt) and not evaluate it point-wise, which is usually less restrictive.
The weighting function for the resulting abc version of the bpf algorithm is given by
w(i)
t
= N(w(i)
t ; yt −y(i),⋆
t
, ϵ 2), where y(i),⋆
t
denotes a sample from gθ( · | x(i)
t ).
Note that we recover the original model when ϵ →0, see Dean and Singh (2011). In practice,
it is required that ϵ > 0 to balance the computational cost with the accuracy of the estimate.
We can employ standard parameter inference methods like the pmh algorithm to estimate
the parameters of the approximate model implied by the smc-abc algorithm. The main
problem with this is that the variance of the log-likelihood estimates often is larger than for
a standard smc algorithm. This can result in bad mixing in the pmh algorithm. We return
to this in Section 4.3 and in Section 5.2 (page 197) of Paper C.
Note that using abc can in itself lead to an acceleration of the inference when the likelihood
is computationally prohibitive to evaluate. This could be a problem when the number of
observations is large. Then evaluating the likelihood at every iteration of a mh algorithm
could be problematic. We return to discussing Bayesian methods for big data in Chapter 5.2.
Building a surrogate of the posterior
From Example 3.6, we know that the estimates of the log-target and its gradients can be
obtained by particle methods. However, they can be quite noisy when N is small and

4.2
Approximating the model
77
-6
-4
-2
0
2
4
6
0.0
0.1
0.2
0.3
0.4
0.5
β
density
-6
-4
-2
0
2
4
6
0.0
0.1
0.2
0.3
0.4
0.5
β
density
Figure 4.1. Illustration of two multi-modular distributions of the individual random eﬀects βi
in a mixed eﬀects model (2.9).
0.6
0.7
0.8
0.9
1.0
-48
-47
-46
-45
-44
φ
predictive posterior
0.6
0.7
0.8
0.9
1.0
-48
-47
-46
-45
-44
φ
predictive posterior
Figure 4.2. The predictive gp posterior for the log-likelihood estimates from the bpf in Ex-
ample 3.6 using 10 (left) and (20) random values of φ. The dashed lines indicate the true
log-likelihood and the shaded areas indicate the 95% credibility intervals.

78
4
Strategies for accelerating inference
increasing the number of particles also increases the computational cost of the particle
method. When the noise variance is small, it is possible to make direct use of these estimates
for computing the maximum likelihood estimate or the maximum a posteriori estimate of θ.
For example, by using ﬁnite diﬀerence approaches, such as in the simultaneous perturbation
and stochastic approximation (spsa; Spall, 1987, 1998) algorithm. The gradient estimate
can be utilised in a standard gradient ascent algorithm (Poyiadjis et al., 2011; Doucet et al.,
2013) and in a Newton algorithm together with an estimate of Hessian (Kok et al., 2015).
Another approach is to make use of gp regression to build a surrogate function of the
log-posterior. The surrogate should be smooth and cheap to evaluate, where both require-
ments are fulﬁlled by the gp predictive posterior. In Figure 4.2, we present an example
of this by revisiting the log-likelihood estimates generated in Example 3.6. We randomly
select 10 and 20 samples to create two surrogate functions using gp regression. We note
that predictive mean diﬀers slightly from the true log-likelihood (dashed line) around the
parameter estimate. Hence, optimising the mean function will give a similar result to the
mode of the posterior estimates in Figure 3.9.
After that the surrogate has been computed, it is easy to make use of a quasi-Newton
algorithm to ﬁnd the parameter estimates by optimising the predictive mean function.
However, the predictive covariance function is also informative as it enables us to construct
conﬁdence intervals. This information can be used to decide in which parameters the log-
posterior should be sampled in next. In Figure 4.2, it would be beneﬁcial to decrease the
predictive covariance around φ = 0.85 as a peak in the log-likelihood could be hiding there.
This is the idea behind the Bayesian optimisation (bo; Mockus et al., 1978) algorithm, which
is especially designed for optimising noisy and expensive objective functions. In this thesis,
we refer to the bo algorithm as the Gaussian process optimisation (gpo) algorithm as the
surrogate function is a gp predictive posterior. General introductions to bo are given by
Snoek et al. (2012), Lizotte (2008), Boyle (2007) and Osborne (2010).
We make use of gpo for parameter estimation in ssms using maximum likelihood in
Dahlin and Lindsten (2014) by combining it with the bpf algorithm for estimating the
log-likelihood. The results are encouraging and the convergence is faster than compared
with the spsa algorithm in the number of log-likelihood estimates. In Paper E, we revisit
the problem for ssms with intractable likelihoods. In this setting, we can make use of smc-
abc to compute estimates of the log-likelihood. However, the smc-abc algorithm often
requires a larger value of N compared with the standard smc algorithm to obtain reasonable
accuracy in the log-likelihood estimates. This results in poorly mixing and computationally
costly inference algorithms based on the pmh algorithm, which is discussed in Paper C.
Instead, we make use of the gpo algorithm in combination with smc-abc to estimate
a Laplace approximation (Gelman et al., 2013) of the log-posterior. This can be seen as
an approximate Bayesian inference algorithm or be applied as a method to ﬁnd a suitable
initialisation and parameter proposal for the pmh algorithm. This allows use to decrease
the number of required estimates of the log-likelihood from about 10, 000 to 350 comparing
with pmh in one example, which corresponds to decreasing the computational time from
days to half-an-hour. Note that the use of a gp as the surrogate incurs a small computational
overhead. However, the main computational cost in both the gpo and the pmh algorithms
is incurred by the smc algorithm applied for estimating the log-target.

4.3
Improving the inference algorithm
79
Improving the inference algorithm
The third approach to accelerate inference is to make changes to the inference algorithms.
Here, we discuss two alterations to the pmmh/pmh algorithm based on: (a) inducing
correlation in the estimates of the target and (b) tailoring the parameter proposal to the
target distribution.
Correlating auxiliary variables
Sometimes it is useful to extend the model with auxiliary parameters to make inference eas-
ier. One such approach is the pseudo-marginal algorithms presented in Section 3.3. In these
algorithms, we can compute estimates of the target by introducing the auxiliary variables u.
This makes it possible to use the mh algorithm to sample from target distributions, which
we cannot evaluate point-wise but can estimate using e.g., importance sampling.
If we revisit Example 3.13, we made use of an independent proposal for u, which basically
means that the particle ﬁlters are independent. From Remark 3.4, we know that the likeli-
hood estimates from the particle ﬁlter are noisy with a variance that decreases proportional
to
√
N . We can therefore increase N to decrease the variance in the likelihood and thus
increasing the mixing in the pmh algorithm (up to the mixing of the optimal algorithm).
However, this makes each iteration of the pmh algorithm slower as the computational
cost increases. An interesting question is then if it is possible to correlate the errors in the
likelihood estimates to increase the mixing without increasing N .
One approach for this is to realise that the particle ﬁlter is a deterministic algorithm given
the random variables u, which are used for resampling and propagating particles. We can
thus make the likelihood estimates positively correlated by inducing a correlation in u. A
random walk proposal for u is not a good choice as millions of random variables typically
are required in the particle ﬁlter. Therefore, a random walk would not explore the space U
eﬃciently. Instead, we propose in Paper D to make use of a Crank-Nicolson (cn; Beskos
et al., 2008; Cotter et al., 2013; Hairer et al., 2014) proposal for u. A cn proposal is a speciﬁc
ar(1) process (2.6) given by
q(u′| u) = N
 
u′;
q
1 −σ2uu, σ2
uINu
!
,
where Nu denotes the number of elements in u and σu > 0 denotes a parameter determined
by the user. Note that for the bpf algorithm, we have that Nu = (N + 1)T when using N
particles and systematic resampling in a scalar lgss model.
In Figure 4.3, we present the correlation in the likelihood estimates in two consecutive
iterations of the pmmh algorithm keeping θ ﬁxed. Here, we use T = 10 samples from a
lgss model (2.13) with θ = {0.0, 0.5, 1.0, 0.1} while varying N in the particle ﬁlter and σu
on the unit interval. We note that the correlation decreases from one to zero as we change
the value of σu. In the cn proposal, we obtain the independent proposal for u by setting
σu = 1. Hence, it is possible to control the correlation in the likelihood estimates by σu.
We apply the cn proposal for u in the pmmh algorithm in Paper D and investigate the
impact on the iact. We present numerical simulations that indicate a three-fold increase in

80
4
Strategies for accelerating inference
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
σu
correlation
N=1
N=2
N=5
N=10
N=20
N=50
Figure 4.3. The correlation between two consecutive log-likelihood estimates from Example 3.1
with N particles and random variables sampled from the cn proposal when varying σu.
-4
-2
0
2
4
0
1
2
3
4
5
µ
σ
Figure 4.4. Four random walks on the Riemann manifold induced by the standard Gaussian iid
model with n = 100 samples.

4.3
Improving the inference algorithm
81
the iact compared with when using an independent proposal for u. Note that only small
changes in the implementation are required to switch from an independent proposal for
u to the cn proposal. It is especially simple when all random variables can be generated
using quantile transformations, see Section 3.2.1. This small change allows us to decrease N
signiﬁcantly, which results in an acceleration of the algorithm in terms of computational
cost. However, the drawback is that u′ and u are stored in memory at all times, which can
be a limiting factor when N is large.
Furthermore, Deligiannidis et al. (2015) presents encouraging theoretical results indicating
that it is possible to select N ∝T α for α < 1. They provide a numerical experiment
illustrating that only a handful of particles is required to infer the parameters of a lgss
model given tens of thousands of observations. This is an impressive result and leads to a
speed-up by 100 times. However, when T is that large the Bernstein-von-Mises theorem
(see Section 2.3) kicks in and alternative methods are probably even faster. Also, they make
use of a fully-adapted particle ﬁlter, which is a particle ﬁltering using the optimal proposal
for particles and the optimal weighting function, see Paper A.
We make use of a bpf for estimating the target in Paper D. In that case, we do not obtain
such radical improvement in the eﬃciency of the algorithm. However, we still increase the
mixing with a factor of about three and outperform the standard pmmh algorithm using
an independent proposal for u.
Tailoring the proposal to the target geometry
In Section 3.2.3, we introduced the mh algorithm and presented two choices of proposals
namely the independent and random walk. Both proposals are known as blind proposals
as they do note take any information about the target into account when proposing the
candidate state x′. The convergence to the sought target distribution is instead ensured
by the accept/reject mechanism. An interesting question is therefore how to construct a
proposal that takes e.g., the gradient of the log-target into account. Here, we discuss one
such approach based on continuous time diﬀusion processes.
An interesting class of (continuous time) stochastic processes are the Itô diﬀusions governed
by the stochastic diﬀerential equation (sde) given by
dXt = b(Xt)dt + σ(Xt)dBt,
X0 = x0,
where b( · ) and σ( · ) denote the drift and the volatility, respectively. A possible choice for
these quantities is given by
b(Xt) = 1
2∇log π(Xt),
σ(Xt) = 1,
then the resulting process is known as a Langevin diﬀusion or Brownian dynamics. This
process has the interesting property that it has π as its stationary distribution and the
resulting process is ergodic, see Livingstone and Girolami (2014) and Roberts and Tweedie
(1996). Hence, for any initialisation a proposal based on this diﬀusion has the target as its
stationary distribution and samples obtained after the burn-in are from the sought target.

82
4
Strategies for accelerating inference
In practice, it is common to make a discretisation of the Langevin diﬀusion using a ﬁrst-
order Euler approximation to obtain
q(x′| x) = N
 
x′; x + ϵ 2
2 ∇log π(x), ϵ 2Ip
!
,
(4.1)
for some discretisation length ϵ > 0. We see from the Langevin proposal (4.1) that the
gradient of the log-target acts like a drift towards areas of high posterior probability. This
could in theory be useful for exploring the target distribution more eﬃciently. Note that the
gradient of the log-posterior is given by the gradient of the log-prior and the score function
(2.15). The resulting algorithm from combining this proposal with the mh algorithm is
known as the Metropolis adjusted Langevin algorithm (mala; Roberts and Stramer, 2003).
It is possible to show that the Markov kernel in the mala algorithm is (geometrically)
ergodic if the tails of the target are heavier than for a Gaussian distribution and lighter than
for an exponential distribution, see Roberts and Tweedie (1996).
A possible extension of (4.1) is to include a matrix Σ(x) to scale the gradient and determine
the variance of the proposal. This results in the proposal given by
q(x′| x) = N
 
x′; x + ϵ 2
2 Σ−1(x)∇log π(x), ϵ 2Σ−1(x)
!
,
(4.2)
where Σ(x) can be selected as any positive semi-deﬁnite matrix or function. An interesting
choice proposed by Girolami and Calderhead (2011) is to make use of
Σ(x) = −∇2 log π(z)
z=x,
which corresponds to a parameter dependent matrix. We refer to the mh algorithm with
this proposal as the (simpliﬁed) manifold mala (mmala) algorithm. This choice of Σ(x)
corresponds to sum of the negative Hessian of the log-prior and the observed information
matrix (2.16).
The main beneﬁt of the mmala algorithm is that the gradients are scaled by the curvature
of the log-posterior. This is useful as the proposal takes larger steps when the Markov chain
is far from the target mode and smaller steps as it gets closer. In Figure 4.4, we present
four diﬀerent Markov chains governed by (4.2). Here, we make use of a standard Gaussian
iid model as the target and would like to infer the mean µ and the standard deviation σ
from n = 100 samples. We see that the Markov chain behaves as expected with large step
lengths far from the mode, which decreases as the chain approaches the true parameters
{µ, σ} = {0, 1}.
The Markov chain governed by (4.2) can be seen as a random walk on a Riemann manifold,
see Girolami and Calderhead (2011) and Livingstone and Girolami (2014). They highlight
that there are other choices for Σ(x) that could be useful in practice. The Langevin proposal
can also be seen as a Laplace approximation of the posterior as discussed by Robert and
Casella (2004) and in Paper B. A third interpretation proposed in Paper C is that mala and
mmala corresponds to noisy gradient ascent and noisy Newton algorithms, respectively.
The major potential beneﬁt with using the mala and the mmala is that they can increase
the mixing in the Markov chain. In Section 3.2.3, we showed that the mixing is connected
with the asymptotic variance of the estimate of the target distribution. Increasing the mix-

4.3
Improving the inference algorithm
83
ing can therefore have a positive impact on the computational time of the algorithm and
accelerate inference. This as it is possible to decrease K and still obtain a similar accuracy
in the estimate. Less iterations of the mh algorithm leads to a decreased computational cost.
Another beneﬁt for the mmala algorithm is that it requires less tuning than a standard
random walk proposal. This as the user only needs to tune the step length ϵ and not an
entire covariance matrix as for the random walk proposal. This is a decrease from p2 tuning
parameters to 1 for the proposal, which potentially decreases the amount of user interaction.
In Papers B and C, we propose particle versions of the mala and mmala algorithms to
increase the mixing compared to the standard pmh algorithm from Section 3.3. The main
challenge is to obtain estimate of the intractable gradient and the Hessian with respect to
the log-target distribution. We employ fl particle smoothing from Remark 3.2 together
with the results from Remark 3.5 to estimate these quantities. The primary beneﬁt is that
this type of smoother does not increase the computational complexity of the algorithm
and gives reasonable accuracy. The latter is important for the performance of the proposed
algorithm as analysed by Nemeth et al. (2014).
Example 4.1: How does unemployment aﬀect inﬂation? (cont. from p. 70)
We refer to the particle version of mmala as the pmh algorithm of second-order (pmh2)
since it makes use of gradient and Hessian information regarding the target. The basic
algorithm for pmh2 is presented in Paper B. qpmh2 is presented in Paper C and makes
use of quasi-Newton techniques (Nocedal and Wright, 2006) to estimate the Hessian.
We return to the problem of estimating the parameter β in the Phillips curve model to
compare the pmh0 and the qpmh2 algorithms. Remember that the pmh0 algorithm
makes use of a random walk proposal such as (3.32), which does not include information
about the gradient and Hessian of the target. Furthermore, we make use of a post-processing
of the Markov chain known as zero-variance (zv; Mira et al., 2013; Papamarkou et al., 2014)
to decrease the variance in the estimates. The zv approach is based on the idea of control
variates in vanilla Monte Carlo sampling, see Robert and Casella (2004). We can apply zv
to decrease the variance in the estimate of the posterior mean by
Hβk = βk + ρ
2 G(βk),
ρ = −V[G(β1:K )]C[β1:K, G(β1:K )],
where G(βk) = ∇log p(θ |y)
β=βk denotes the gradient of the log-posterior evaluated at
βk. Hence, we can compute the mean of Hβ1:K to estimate the posterior mean, which has a
smaller variance as making use of β1:K directly.
In Figure 4.5, we present the trace plot, acf and the posterior estimates for: qpmh2,
qpmh2 with zv (qpmh2-zv) and the pmh0 algorithm from Example 3.13. Note that the
mixing is better for the two algorithms based on qpmh2, which can be seen in both the
trace and the acf. Note the spike in the acf at lag 100, which is due to the memory of
the quasi-Newton update for the Hessian, see Paper C. Moreover, note that the variance
in the posterior estimate from qpmh2-zv is smaller than the same estimate from qpmh2.
Hence, the zv post-processing of the Markov chain seems to have a beneﬁcial eﬀect.
We return to this model in Example 5.1 on page 87.

84
4
Strategies for accelerating inference
8000
8100
8200
8300
-0.04
-0.02
0.00
0.02
0.04
iteration
β
8000
8100
8200
8300
-0.04
-0.02
0.00
0.02
0.04
iteration
β
8000
8100
8200
8300
-0.04
-0.02
0.00
0.02
0.04
iteration
β
0
20
40
60
80
100
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
lag
acf of β
0
20
40
60
80
100
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
lag
acf of β
0
20
40
60
80
100
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
lag
acf of β
β
posterior estimate
-0.04
0.00
0.02
0.04
0
20
40
60
80
qPMH2
β
posterior estimate
-0.04
0.00
0.02
0.04
0
20
40
60
80
qPMH2-ZV
β
posterior estimate
-0.04
0.00
0.02
0.04
0
20
40
60
80
PMH0
Figure 4.5. The trace, acf and parameter posterior estimate for β in the Phillips curve model
using the Swedish data. The results are presented for: qpmh2 (green), qpmh2 with zv esti-
mator (brown) and pmh0 (grey). Dotted lines indicate conﬁdence intervals and estimates of
posterior means. The grey curves indicate the prior distribution.

4.4
Outlook and extensions
85
Outlook and extensions
There are many other potential approaches for accelerating inference in the algorithms.
Some of them are discussed in Chapter 5. To conclude this chapter, we would like to take
the opportunity to discuss some more speciﬁc ideas for accelerating the pmh algorithm.
For the pmh algorithm, we would like to highlight three diﬀerent interesting approaches
connected with the material in this chapter. The ﬁrst is to make use of surrogate modelling
of the target distribution, which allows for cheaper computations of the acceptance probabil-
ity. This idea is proposed by Meeds and Welling (2014) and they make use of the predictive
gp posterior similar to gpo for constructing the surrogate. An interesting direction for
future work is therefore to investigate the combination of gpo and pmmh algorithms
further for accelerating inference by decreasing the computational cost for each iteration.
The second approach is to alter the test function ϕ to decrease the variance in the resulting
estimates. In vanilla Monte Carlo, this approach is known as control variates and is a
useful method for variance reduction by using a known analytical approximation of the
target. In the pmh algorithm, we can make use of zv (Mira et al., 2013; Papamarkou et al.,
2014) to achieve the same objective as in Example 4.1. In principle, it is straight-forward to
directly apply zv approaches for any the pmh algorithms proposed in Papers B-D to further
accelerate the inference. This especially as zv methods require estimates of the gradient of
the log-posterior, which already are computed by the aforementioned algorithms. The third
approach is to relax the requirement of detailed balance, see e.g. Diaconis et al. (2000) for a
related reference.


5
Concluding remarks
We conclude the introductory part by discussing the two examples introduced in Chapter 2
one last time. We also summarise the contributions of the thesis in more technical terms
connected to the concepts and issues discussed in the previous chapters. Furthermore, we
give a summary of interesting trends and areas for future work in accelerating Bayesian
inference. Finally, we discuss reproducible research and open source code in connection
with this thesis and the papers included in it.
Example 5.1: How does unemployment aﬀect inﬂation? (cont. from p. 70)
We are now ready to make predictions of the future inﬂation rate given the model and
a change in the unemployment rate. In the left part of Figure 5.1, we present the future
expected change in the unemployment rate as stated in this ﬁctional scenario by the Swedish
parliament. We assume that this forecast is correct and it acts as the input to our Philips
curve model from Example 3.13.
To make the forecast, we draw random samples from the parameter posterior estimates and
simulate the system forward in time using the ssm. The result is presented in the right part
of Figure 5.1, where the purple line indicates the predictive mean. We see that the predicted
mean of the inﬂation rate approaches the two percent target during this period but that no
action needs to be taken at the moment. However, the uncertainty (purple area) is large and
better models are required for long-term forecasting. In practice, the Riksbank makes use of
elaborate ssms known as dynamic stochastic general equilibrium (dsge) models (Adolfson
et al., 2007a, 2013, 2007b). to forecast the future inﬂation, unemployment, gdp and other
macro-economic variables.
We conclude this example by noting that no strong negative correlation between the unem-
ployment rate and the inﬂation rate in Sweden during the period between 1987 and 2015.
We have also illustrated one simple approach for predicting future levels of inﬂation.
87

88
5
Concluding remarks
Example 5.2: Voting behaviour in the US Supreme court (cont. from p. 63)
We make use of the estimated model from Example 3.8 to investigate how the ideological
leaning of the court would change if justice Scalia is replaced by a justice with: (i) the same
ideological leaning or (ii) a slightly more liberal leaning. The latter choice is simulated by
subtracting one from all the samples from p(x1 |y) corresponding to the liberal/conserva-
tive score for justice Scalia. We make use of the model and the posteriors from Example 3.8
to simulate the number of liberal votes in 10, 000 cases. We sample the parameter for each
case αt from the aggregated posterior samples from the inference in Example 3.8.
In Figure 5.2 we present histograms of the number of liberal votes for the two scenarios.
The shaded area approximates the probability of a liberal outcome from the ruling of the
court. We conclude that the probability of a liberal majority changes from 0.42 to 0.47
when replacing Scalia with a more liberal justice. Hence, the court is almost balanced if the
President choose alternative (b) or slightly conservative if the President choose to nominate
according to alternative (a).
It is possible to repeat this procedure sequentially for each year to investigate how the
ideological leaning changes over time for each justice and for the entire Court. This can be
done by applying Kalman ﬁltering from Remark 2.7 for the score {xi}9
i=1, see Martin and
Quinn (2002) and Martin and Quinn (2007) for more information.
Summary of contributions
The contributions of this thesis strive to accelerate Bayesian inference in a number of
diﬀerent model classes. The acceleration could take the form of simpler implementation,
being able to decrease the number of iterations or generated samples/particles while keeping
the accuracy of the estimates or increasing information about the parameter in the data.
Most of these improvements are the result of methodological contributions for a number
of diﬀerent algorithms.
In Paper A, we provide the reader with a gentle introduction to the pmh algorithm with
plenty of references for further study. Source code is also provided to encourage the reader
to use pmh for his/her own research problems. We propose particle versions of the mala
and mmala in Paper B referred to as the pmh1 and the pmh2 algorithms, respectively.
The main challenge in these algorithms is to obtain accurate estimates of the gradient and
the Hessian of the log-posterior. We propose to make use of fl particle smoothing from
Remark 3.2 together with regularisation approaches from the optimisation literature to
solve this.
Furthermore, we propose a quasi-Newton scheme for estimating the Hessian in Paper C
using only gradient information. This is useful in models where the Hessian estimates are
corrupted by large amounts of noise. From numerical experiments in Papers B and C, we
conclude that adding gradient and Hessian information can increase the mixing, shorten
the burn-in phase and make the proposal scale-invariant. We also investigate the use of
correlated target estimates in the pmmh algorithm in Paper D. A considerable increase in
mixing can be obtained by changing the independent proposal for the random variables u

5.1
Summary of contributions
89
4
5
6
7
8
9
10
date
unemployment rate
2015
2016
2017
2018
-2
-1
0
1
2
3
4
date
inﬂation rate
2015
2016
2017
2018
Figure 5.1. The ﬁctional change in the unemployment rate during 24 months (left) and the
resulting predicted inﬂation rate (right) from the Philips curve model. The predictive mean and
95% conﬁdence intervals are indicated by the purple line and area, respectively. The dashed lines
indicate the start of the forecasts.
no. liberal votes
density
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
no. liberal votes
density
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
Figure 5.2. The predicted number of liberal votes when replacing justice Scalia with someone
with same leaning (left) or slightly more liberal (right). The shaded areas approximate the
probability of a liberal majority in an average case.

90
5
Concluding remarks
used to estimate the target to an autoregressive proposal, which sometimes only amounts
to changing a few lines of code.
The main drawback with mcmc algorithms is their the computational cost can be quite
large. This is especially a problem for pmh algorithms, which can take several days to
execute. It is therefore interesting to investigate alternative approximate methods to carry
out Bayesian parameter inference in ssms. In Dahlin and Lindsten (2014) and Paper E,
we propose a new algorithm based on the combination of particle ﬁltering and gpo for
maximum likelihood and approximate Bayesian inference in ssms. We demonstrate that the
resulting algorithm gives accurate results and can lead to a considerable speed-up compared
with alternative optimisation methods and the pmh algorithm.
In Paper F, we investigate the use of arx models with Student’s t innovations to handle
outliers and missing data. Furthermore, we make use of the rjmh algorithm for auto-
matically selecting the model order. This approach is compared with making use of an
over-parametrised arx model with a sparseness prior on the ar parameters. Inference in
the latter model can be carried out using Gibbs sampling. In the numerical experiments,
the two approaches give similar results in terms of predictions. Good performance is also
demonstrated for predicting real-world eeg data.
In Paper G, we make use of sparseness priors to simplify inference in dpm models applied
for modelling random eﬀects in panel data models. We investigate the ﬁndings by Rousseau
and Mengersen (2011) and show that similar posterior estimates can be obtained by switch-
ing the dpm model to an over-parametrised ﬁnite mixture model with a sparseness prior of
the mixture weights. This opens up for simpler implementation of inference algorithms in
such models and can increase the mixing.
Finally in Paper H, we propose a novel approach for input design in non-linear ssms. The
optimal input is obtained as the solution to a convex optimisation problem of basis inputs
computed by graph theory. The corresponding cost function depends on the Fisher infor-
mation matrix for each basis input. Therefore, we propose a novel approach for estimating
this quantity based on particle smoothing. We demonstrate that the input signal generated
by the proposed method can increase the accuracy and convergence rate of the expectation
maximisation algorithm applied for parameter inference.
Some trends and ideas for future work
In this section, we discuss some trends and ideas for future work to accelerate Bayesian
inference. Further ideas and discussions are provided in the conclusions for each of the
papers in Part ii of this thesis.
Efficient implementations
Most of the contributions in this thesis are based on improving existing algorithms. There-
fore, we have not discussed how to write eﬃcient implementation of them to decrease the
actual computational time. The main challenge with eﬃcient implementation is typically
that great care needs to be taken to optimise e.g., memory management. For instance, the

5.2
Some trends and ideas for future work
91
programming language Julia (Bezanson et al., 2014) has been designed with this in mind.
The main advantages are the excellent computational performance together with a simple
high-level syntax that resembles programming languages such as R (R Core Team, 2015) or
Python. This could make eﬃcient implementation of Monte Carlo algorithms easier in the
future and lead to accelerated inference.
A related trend is to provide a general solver, which can be applied to many diﬀerent types
of problems. Two examples of this are stan (Stan Development Team, 2015) and libbi
(Murray, 2013), where the user can deﬁne a model and the program then takes care of solving
the inference problem using hmc, mcmc and/or smc. This makes inference easier for the
user, who does not need to implement advanced algorithms and tune them on his/her own.
Stan is already used extensively in many interesting applications, e.g., in the recent detection
of gravitational waves by Abbott et al. (2016).
Similar software based on probabilistic programming is also becoming more popular as a
general tool for carrying out inference, see Wood et al. (2014) and Mansinghka et al. (2014).
Finally, gpu-implementations and cloud computing can also be useful for accelerating smc
and mcmc algorithms by carrying out the computations in a parallel manner, see e.g., Beam
et al. (2014), Neiswanger et al. (2014), Henriksen et al. (2012) and Murray et al. (2015).
Better particle smoothing and Bayesian online methods
Many of the algorithms presented in this thesis are based on smc methods for estimating the
log-likelihood or its gradient and Hessian. The eﬃcient implementation of these methods
depends on good proposal distributions. More work is required in this area to improve smc
methods for high-dimensional targets, see e.g., Naesseth et al. (2015), Rebeschini and van
Handel (2015) and Beskos et al. (2014).
Another approach is to improve the estimators for the gradient and the Hessian of the log-
posterior. This is especially important for the Hessian as it should be pd (psd). A problem
when using the Louis identity as in Example 3.5 and Paper B is that the resulting estimate is
often not psd and therefore not a valid covariance matrix. Some alternative estimators are
discussed in Papers C and G. However, more work is required to make pmh1/2 useful for a
larger class of ssms.
Finally, online algorithms for Bayesian inference are an important problem which currently
lacks a satisfactory solution. Oﬄine inference is provided by the mcmc algorithm, which
can be applied to a wide range of problems. However, it is diﬃcult to make use of mcmc
algorithms when the target distribution changes between iterations. A natural solution
is the use of particle ﬁltering and smc algorithms as proposed by Carvalho et al. (2010),
Storvik (2002) and Fearnhead (2002). However, such algorithms can suﬀer from particle
depletion/degeneracy, which results in that the estimators suﬀer from a large variance.
Scalable Bayesian inference
The amount of data generated by people, machines and sensors increases drastically for every
year. Therefore, some like to refer to the current age as the era of big data. This represents
new challenges for Bayesian inference algorithms to handle both the large amount of data

92
5
Concluding remarks
and the many diﬀerent forms of it. A drawback with the mh algorithm and its pseudo-
marginal version is that the acceptance probability depends on the likelihood of the data. If
the number of observations is large, the estimation or computation of the likelihood can
be computationally prohibitive, which limits the feasibility of inference.
A large number of alterations to the mh algorithm have recently been proposed to mitigate
this problem. Two surveys of these recent eﬀorts are given by Bardenet et al. (2015) and
Angelino et al. (2016). One promising approach based on sub-sampling the data is proposed
by Quiroz et al. (2016), which makes use of the correlated pseudo-marginal mh algorithm
introduced in Paper D.
However, from the Bernstein-von-Mises theorem, we know that the posterior asymptoti-
cally concentrates to a Gaussian under some regularity conditions. It could therefore be
more fruitful to make use of this in models for which the number of parameters are much
smaller than the number of observations. We can then create a Laplace approximation of
the mode based on the output from some optimisation algorithm targeting the log-posterior.
Stochastic gradient methods have been proposed for solving this problem, where the stochas-
ticity comes from computing the gradients using a sub-sample of the observations. This
can be combined with mcmc approaches to estimate the posterior distribution as proposed
by Welling and Teh (2011) and Ahn et al. (2012) or variational inference as discussed by
Hoﬀman et al. (2012).
Probabilistic numerics
The gpo algorithm is an optimisation algorithm based on a surrogate function which often
is the gp predictive posterior. It turns out that this type of surrogate modelling is useful
for many other applications as well. This is the basis of the new ﬁeld called probabilistic
numerics, where similar approaches are used for solving diﬀerential equations, diﬀerentiation
and integration.
The main beneﬁts are that these methods make use of uncertainty and often require less
samples from the function of interest. The former means that the approximate value of an
integral is given by a probability distribution and not a point estimate. An example of the
latter is the gpo algorithm, which requires less evaluations of the objective function than
some other similar optimisation algorithms. More information about these methods are
available from the homepage: http://www.probabilistic-numerics.org/ as well as in
Hennig (2013), Briol et al. (2015), Osborne et al. (2012), Osborne (2010) and Boyle (2007).
Reproducible research
Finally, we would like to take the opportunity to discuss the promise and necessity of re-
producible research (Claerbout and Karrenbach, 1992). Before the advent of the computer,
science was typically divided into theoretical and experimental ﬁelds as discussed by Vande-
walle et al. (2009). In both set of ﬁelds, it is important to describe the method used to derive
a proof or the experimental set-up used to study a certain phenomenon. A third branch
of science based on computer experiments has developed rapidly during the last decades.
However, the requirements on documenting the algorithms, their settings and data are not
yet as strict as for the other two branches. This is a major problem since it often is diﬃcult

5.3
Source code and data
93
or almost impossible to reproduce the results in peer-reviewed papers based on computer
experiments. Hence, the fundamental principle of reproducibility in science is violated as
new results cannot easily be veriﬁed by independent colleagues in the research community.
Naturally, it is not possible to provide implementations of the proposed algorithm for all
diﬀerent programming languages. However, just making the code and the data available is
a big step forward in not only encouraging veriﬁcation but also for encouraging further
development of promising algorithms. Making the code available also makes it easier for
your readers to understand your work and to suggest improvements on it. In our view, it
should be as natural to provide the source code and data as a supplement to a paper as it is
to provide a detailed proof of a theorem. It is only then that reviewers and colleagues really
know what happens inside the magical box, which sometimes produce impressive plots and
tables. Hence, not making the source code available is problematic and not really science as
all details should be revealed.
There are also many other beneﬁts in making the source code freely available to other
researchers. One major beneﬁt is that this practice puts pressure on commenting and docu-
menting the code for your own sake. This is useful for decreasing the number of bugs and
mistakes. Moreover, iPython notebooks (Pérez and Granger, 2007) and knitR (Xie, 2014)
are excellent approaches to keep track of your own progress and write comments on how
e.g., speciﬁc parameters of the algorithms were selected. This is helpful when a revision
is due for a journal paper or when a new member joins the research group. Finally, if the
code is not ﬁled and documented, all the details of the algorithm are lost when e.g., a phd
student graduates and leaves the research group.
There is much more to read about reproducible research in e.g., Markowetz (2015), Fomel
and Claerbout (2009), Vandewalle et al. (2009) and Donoho (2010).
Source code and data
Source code written in Python and R for recreating the examples in Part I are available
from GitHub: https://github.com/compops/phd-thesis. Furthermore, source code
for recreating some of the numerical illustrations from the papers included in the thesis
are also available from http://code.johandahlin.com. See the README.md ﬁle in each
repository for dependencies, instructions and implementation details. The source code and
data are provided under various open source licenses with no guaranteed support and no
responsibility for their use and function. Note that some data have to be download from
other sites due to licensing issues.


Notation
Probability
a.s.
−→
Almost sure convergence.
d
−→
Convergence in distribution.
p
−→
Convergence in probability.
P,E,C,V
Probability, expectation, variance and covariance operators.
∼
Sampled from or distributed according to.
π[ϕ]
The expected value of ϕ under the distribution π.
Statistical distributions
δx′(dx)
Dirac distribution (measure) located at x = x′.
A(α, β, γ, η)
α-stable distribution
with stability α, skewness β, scale γ and location η.
B(p)
Bernoulli distribution with success probability p.
D(α)
Dirichlet distribution with concentration parameter α.
DP(α,G0)
Dirichlet process with concentration parameter α and base
measure G0.
N(µ, σ2)
Gaussian (normal) distribution with mean µ
and variance σ2
GP(µ, κ)
Gaussian process with mean function µ
and covariance function(kernel) κ
G(α, β)
Gamma distribution with rate α and shape β.
IG(α, β)
Inverse Gamma distribution with rate α and shape β.
M(n, p)
Multinomial distribution with n trials and probability p.
P(λ)
Poisson distribution with mean λ.
U(a, b)
Uniform distribution on the interval [a, b].
95

96
Notation
Operators and other symbols
0p
zero p-vector.
Id
d × d identity matrix.
≜
Deﬁnition.
diag(v)
Diagonal matrix with the vector v on the diagonal.
∇f (x)
Gradient of f (x).
∇2 f (x)
Hessian of f (x).
I
Indicator function.
det(A), |A|
Matrix determinant of A.
A−1, A⊤
Matrix inverse/transpose of A.
tr(A)
Matrix trace of A.
v2 = vv⊤
Outer product of the vector v.
an:m
Sequence {an, an+1, . . ., am−1, am}, for m > n.
sign(x)
Sign of x.
supp( f )
Support of the function f , {x : f (x) > 0}.
Statistical quantities
G(θ)
Gradient of log-posterior/target evaluated at θ.
I(θ)
Expected information matrix evaluated at θ.
H(θ)
Negative Hessian of log-posterior/target evaluated at θ.
J (θ)
Observed information matrix evaluated at θ.
Dθ
Parameter estimate.
p(θ|y)
Parameter posterior distribution.
p(θ)
Parameter prior distribution.
θ
Parameter vector, θ ∈Θ ⊆Rd.
S(θ)
Score function evaluated at θ.
Algorithmic quantities
a(i)
t
Ancestor of particle i at time t.
R(xk−1, xk)
Markov kernel.
Z
Normalisation constant.
x(i)
t
Particle i at time t.
qt(xt |x0:t−1)
Particle proposal kernel.
Wθ(xt, xt−1)
Particle weighting function.
q(θ)
Proposal distribution.
xk
State of the Markov chain at iteration k.
π(θ)
Target distribution.
γ(θ)
Unnormalised target distribution.
w(i)
t , H
w(i)
t
Un- and normalised weight of particle i at time t.

Notation
97
Abbreviations
a.s.
Almost surely (with probability 1).
abc
Approximate Bayesian computations.
acf
Autocorrelation function.
ar(p)
Autoregressive process of order p.
ard
Automatic relevance determination.
arx(p)
Autoregressive exogenous process of order p.
bo
Bayesian optimisation.
bpf
Bootstrap particle ﬁlter.
cdf
Cumulative distribution function.
clt
Central limit theorem.
dp( m)
Dirichlet process (mixture).
fapf
Fully-adapted particle ﬁlter.
ffbsm
Forward-ﬁltering backward-smoothing.
ffbsi
Forward-ﬁltering backward-simulation.
fl
Fixed-lag (particle smoother).
gp( o)
Gaussian process (optimisation).
gpu
Graphical processing unit.
hmm
Hidden Markov model.
iact
Integrated autocorrelation time.
iid
Independent and identically distributed.
( sn)is
(self-normalised) Importance sampling.
kde
Kernel density estimate/estimator.
lgss
Linear Gaussian state space.
( p)mcmc
(particle) Markov chain Monte Carlo.
( rj)mh
(reversible-jump) Metropolis-Hastings.
ml
Maximum likelihood.
mle
Maximum likelihood estimator.
mse
Mean square error.
pd
Positive deﬁnite.
pdf
Probability density function.
pmf
Probability mass function.
pf
Particle ﬁlter.
pmmh
Pseudo-marginal Metropolis-Hastings.
pmh
Particle Metropolis-Hastings.
pmh0
Marginal particle Metropolis-Hastings.
pmh1
pmh using ﬁrst-order information
pmh2
pmh using ﬁrst and second-order information
qpmh2
quasi-Newton Metropolis-Hastings

98
Notation
Abbreviations (cont.)
ps
Particle smoother.
rjmh
Reversible jump Metropolis-Hastings.
rls
Regularised least squares.
rts
Rauch-Tung-Stribel.
sbp
Stick-breaking process.
sis
Sequential importance sampling.
sir
Sequential importance sampling and resampling.
slln
Strong law of large numbers.
smc
Sequential Monte Carlo.
spsa
Simultaneous perturbation stochastic approximation.
ssm
State space model.

Bibliography
B. P. Abbott, R. Abbott, T. D. Abbott, and Others. The rate of binary black hole mergers
inferred from advanced LIGO observations surrounding GW150914. Pre-print, 2016.
arXiv:1602.03842v1.
M. Adolfson, S. Laséen, J. Lindé, and M. Villani. RAMSES – a new general equilibrium
model for monetary policy analysis. Sveriges Riksbank Economic Review, 2, 2007a.
M. Adolfson, S. Laséen, J. Lindé, and M. Villani. Bayesian estimation of an open economy
DSGE model with incomplete pass-through. Journal of International Economics, 72(2):
481–511, 2007b.
M. Adolfson, S. Laséen, L. Christiano, M. Trabandt, and K. Walentin. RAMSES II – model
description. Sveriges Riksbank Occasional Paper Series, 12, 2013.
S. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via dtochastic gradient
Fisher scoring. In Proceedings of the 29th International Conference on Machine Learning
(ICML), pages 1591–1598, Edinburgh, Scotland, July 2012.
J. H. Albert.
Bayesian estimation of normal ogive item response curves using Gibbs
sampling. Journal of Educational and Behavioral Statistics, 17(3):251–269, 1992.
B. D. O. Anderson and J. B. Moore. Optimal ﬁltering. Courier Publications, 2005.
C. Andrieu and G. O. Roberts. The pseudo-marginal approach for eﬃcient Monte Carlo
computations. The Annals of Statistics, 37(2):697–725, 2009.
C. Andrieu and J. Thoms. A tutorial on adaptive MCMC. Statistics and Computing, 18(4):
343–373, 2008.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342,
2010.
E. Angelino, M. J. Johnson, and A. P. Ryan. Patterns of scalable Bayesian inference. Pre-
print, 2016. arXiv:1602.05221v1.
C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparamet-
ric problems. The Annals of Statistics, 2(6):1152–1174, 1974.
99

100
Bibliography
B. H. Baltagi. Econometric analysis of panel data. John Wiley & Sons, 2008.
R. Bardenet, A. Doucet, and C. Holmes. On Markov chain Monte Carlo methods for tall
data. Pre-print, 2015. arXiv:1505.02827v1.
T. Bayes. An essay towards solving a problem in the doctrine of chances. Philosophical
Transactions of the Royal Society of London, 53:370–418, 1764.
A. L. Beam, S. K. Ghosh, and J. Doyle. Fast Hamiltonian Monte Carlo ssing GPU comput-
ing. Pre-print, 2014. arXiv:1402.4089v1.
M. A. Beaumont. Estimation of population growth or decline in genetically monitored
populations. Genetics, 164(3):1139–1160, 2003.
J. O. Berger. Statistical decision theory and Bayesian analysis. Springer Verlag, 1985.
A. Beskos, G. Roberts, A. Stuart, and J. Voss.
MCMC methods for diﬀusion bridges.
Stochastics and Dynamics, 8(03):319–350, 2008.
A. Beskos, D. Crisan, A. Jasra, K. Kamatani, and Y. Zhou. A stable particle ﬁlter in high-
dimensions. Pre-print, 2014. arXiv:1412.3501v1.
J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah. Julia: A fresh approach to numerical
computing. Pre-print, 2014. arXiv:1411.1607v4.
P. Billingsley.
Probability and measure.
Wiley series in probability and mathematical
statistics. John Wiley & Sons, 3 edition, 2012.
H. J. B. Birks. Numerical tools in palaeolimnology–progress, potentialities, and problems.
Journal of Paleolimnology, 20(4):307–332, 1998.
C. M. Bishop. Pattern recognition and machine learning. Springer Verlag, New York, USA,
2006.
D. Blackwell and J. B. MacQueen. Ferguson distributions via Pólya urn schemes. The
Annals of Statistics, 1(2):353–355, 1973.
D. M. Blei, A. Kucukelbir, and J. D. McAuliﬀe. Variational inference: A review for statisti-
cians. Pre-print, 2016. arXiv:1601.00670v1.
A. Bouchard-Côté, S. Sankararaman, and M. I. Jordan. Phylogenetic inference via sequential
Monte Carlo. Systematic biology, 2012.
P. Boyle. Gaussian processes for regression and optimisation. PhD thesis, Victoria University
of Wellington, 2007.
F-X. Briol, C. J. Oates, M. Girolami, M. A. Osborne, and D. Sejdinovic. Probabilistic
integration. Pre-print, 2015. arXiv:1510.00933v1.
A. Brockwell, P. Del Moral, and A. Doucet. Sequentially interacting Markov chain Monte
Carlo methods. The Annals of Statistics, 38(6):3387–3411, 2010.
P. J. Brockwell and R. A. Davis. Introduction to time series and forecasting. Springer Verlag,
2002.

Bibliography
101
M. Burda and M. Harding. Panel probit with ﬂexible correlated eﬀects: quantifying tech-
nology spillovers in the presence of latent heterogeneity. Journal of Applied Econometrics,
28(6):956–981, 2013.
W. S. Bush and J. H. Moore. Genome-wide association studies. PLoS Computational Biology,
8(12):e1002822, 2012.
O. Cappé, A. Guillin, J-M. Marin, and C. P. Robert. Population Monte Carlo. Journal of
Computational and Graphical Statistics, 13(4), 2004.
O. Cappé, E. Moulines, and T. Rydén. Inference in hidden Markov models. Springer Verlag,
2005.
C. M. Carvalho, M. S. Johannes, H. F. Lopes, and N. G. Polson. Particle learning and
smoothing. Statistical Science, 25(1):88–106, 2010.
G. Casella and R. L. Berger. Statistical Inference. Duxbury Press, 2 edition, 2001.
N. Chopin. Central limit theorem for sequential Monte Carlo methods and its application
to Bayesian inference. The Annals of Statistics, 32(6):2385–2411, 2004.
N. Chopin, P. E. Jacob, and O. Papaspiliopoulos. SMC2: an eﬃcient algorithm for sequen-
tial analysis of state space models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 75(3):397–426, 2013.
J. Claerbout and M. Karrenbach. Electronic documents give reproducible research a new
meaning. In Proceedings of the 62nd Annual International Meeting of the Society of Explo-
ration Geophysics, pages 601–604, New Orleans, USA, October 1992.
M. K. Condliﬀ, D. D. Lewis, D. Madigan, and C. Posse. Bayesian mixed-eﬀects models
for recommender systems. In Proceedings of ACM SIGIR’99 Workshop on Recommender
Systems, Berkeley, USA, August 1999.
J-M. Cornuet, J-M. Marin, A. Mira, and C. P. Robert.
Adaptive multiple importance
sampling. Pre-print, 2011. arXiv:0907.1254v5.
S. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White. MCMC methods for functions:
modifying old algorithms to make them faster. Statistical Science, 28(3):424–446, 2013.
N. Cressie. Statistics for spatial data. Wiley, 1993.
D. Crisan and A. Doucet. A survey of convergence results on particle ﬁltering methods for
practitioners. IEEE Transactions on Signal Processing, 50(3):736–746, 2002.
J. Dahlin. Sequential Monte Carlo for inference in nonlinear state space models. Licentiate’s
thesis no. 1652, Linköping University, May 2014.
J. Dahlin and F. Lindsten. Particle ﬁlter-based Gaussian process optimisation for parameter
inference. In Proceedings of the 19th IFAC World Congress, Cape Town, South Africa,
August 2014.
J. Dahlin and T. B. Schön. Getting started with particle Metropolis-Hastings for inference
in nonlinear models. Pre-print, 2015. arXiv:1511.01707v4.

102
Bibliography
J. Dahlin and P. Svenson. A method for community detection in uncertain networks.
In Proceedings of 2011 European Intelligence and Security Informatics Conference, Athens,
Greece, August 2011.
J. Dahlin and P. Svenson. Ensemble approaches for improving community detection meth-
ods. Pre-print, 2013. arXiv:1309.0242v1.
J. Dahlin, F. Johansson, L. Kaati, C. Mårtensson, and P. Svenson. A method for community
detection in uncertain networks. In Proceedings of International Symposium on Foundation
of Open Source Intelligence and Security Informatics 2012, Istanbul, Turkey, August 2012a.
J. Dahlin, F. Lindsten, T. B. Schön, and A. Wills. Hierarchical Bayesian ARX models
for robust inference. In Proceedings of the 16th IFAC Symposium on System Identiﬁcation
(SYSID), Brussels, Belgium, July 2012b.
J. Dahlin, F. Lindsten, and T. B. Schön.
Particle Metropolis Hastings using Langevin
dynamics. In Proceedings of the 38th International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Vancouver, Canada, May 2013a.
J. Dahlin, F. Lindsten, and T. B. Schön. Inference in Gaussian models with missing data
using equalisation maximisation. Pre-print, 2013b. arXiv:1308.4601v1.
J. Dahlin, F. Lindsten, and T. B. Schön. Second-order particle MCMC for Bayesian param-
eter inference. In Proceedings of the 19th IFAC World Congress, Cape Town, South Africa,
August 2014a.
J. Dahlin, T. B. Schön, and M. Villani. Approximate inference in state space models with
intractable likelihoods using Gaussian process optimisation. Technical Report LiTH-ISY-
R-3075, Department of Electrical Engineering, Linköping University, Linköping, Sweden,
April 2014b.
J. Dahlin, F. Lindsten, J. Kronander, and T. B.
Schön.
Accelerating pseudo-
marginal Metropolis-Hastings by correlating auxiliary variables.
Pre-print, 2015a.
arXiv:1512.05483v1.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using gradient and
Hessian information. Statistics and Computing, 25(1):81–92, 2015b.
J. Dahlin, F. Lindsten, and T. B. Schön. Quasi-Newton particle Metropolis-Hastings. In
Proceedings of the 17th IFAC Symposium on System Identiﬁcation (SYSID), pages 981–986,
Beijing, China, October 2015c.
J. Dahlin, M. Villani, and T. B. Schön. Eﬃcient approximate Bayesian inference for models
with intractable likelihoods. Pre-print, 2015d. arXiv:1506.06975v1.
J. Dahlin, R. Kohn, and T. B. Schön. Bayesian inference for mixed eﬀects models with
heterogeneity. Technical Report LiTH-ISY-R-3091, Department of Electrical Engineering,
Linköping University, Linköping, Sweden, March 2016.
A.C. Davison and D.V. Hinkley.
Bootstrap methods and their application.
Cambridge
University Press, 1997.

Bibliography
103
T. A. Dean and S. S. Singh. Asymptotic behaviour of approximate Bayesian estimators.
Pre-print, 2011. arXiv:1105.3655v1.
P. Debevec. Rendering synthetic objects into real scenes: bridging traditional and image-
based graphics with global illumination and high dynamic range photography. In Pro-
ceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques,
pages 189–198, Orlando, USA, jul 1998.
P. Del Moral. Feynman-Kac formulae - genealogical and interacting particle systems with
applications. Springer Verlag, 2004.
P. Del Moral. Mean ﬁeld simulation for Monte Carlo integration. CRC Press, 2013.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.
P. Del Moral, A. Doucet, and S. Singh. Forward smoothing using sequential Monte Carlo.
Pre-print, 2010. arXiv:1012.5390v1.
G. Deligiannidis, A. Doucet, and M. K. Pitt. The correlated pseudo-marginal method.
Pre-print, 2015. arXiv:1511.04992v2.
A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 39
(1):1–38, 1977.
P. Diaconis, S. Holmes, and R. Neal. Analysis of a nonreversible Markov chain sampler.
The Annals of Applied Probability, 10(3):685–1064, 2000.
D. L. Donoho. An invitation to reproducible computational research. Biostatistics, 11(3):
385–388, 2010.
R. Douc and O. Cappé.
Comparison of resampling schemes for particle ﬁltering.
In
Proceedings of the 4th International Symposium on Image and Signal Processing and Analysis
(ISPA), pages 64–69, Zagreb, Croatia, September 2005.
R. Douc, E. Moulines, and D. S Stoﬀer. Nonlinear time series: theory, methods and applications
with R examples. CRC Press, 2014.
A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years
later. In D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering.
Oxford University Press, 2011.
A. Doucet, S. Godsill, and C. Andrieu. On sequential Monte Carlo sampling methods for
Bayesian ﬁltering. Statistics and computing, 10(3):197–208, 2000.
A. Doucet, P. E. Jacob, and S. Rubenthaler. Derivative-free estimation of the score vector
and observed information matrix with application to state-space models. Pre-print, 2013.
arXiv:1304.5768v2.
A. Doucet, M. K. Pitt, G. Deligiannidis, and R. Kohn. Eﬃcient implementation of Markov
chain Monte Carlo when using an unbiased likelihood estimator. Biometrika, 102(2):
295–313, 2015.

104
Bibliography
S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics
letters B, 195(2):216–222, 1987.
J. Durbin and S. J. Koopman. Time series analysis by state space methods. Oxford University
Press, 2 edition, 2012.
R. Eckhardt. Stan Ulam, John von Neumann, and the Monte Carlo method. Los Alamos
Science, 15:131–136, 1987.
B. Efron. Bootstrap methods: another look at the jackknife. The Annals of Statistics, 7(1):
1–26, 1979.
P. Embrechts, C. Klüppelberg, and T. Mikosch. Modelling extremal events. Springer Verlag,
1997.
P. Fearnhead. Markov chain Monte Carlo, suﬃcient statistics, and particle ﬁlters. Journal
of Computational and Graphical Statistics, 11(4):848–862, 2002.
P. Fearnhead. Particle ﬁlters for mixture models with an unknown number of components.
Statistics and Computing, 14(1):11–21, 2004.
T. S. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of
Statistics, 1(2):209–230, 1973.
T. S. Ferguson.
Prior distributions on spaces of probability measures.
The Annals of
Statistics, 2(4):615–629, 1974.
J. Fernández-Villaverde and J. F. Rubio-Ramírez. Estimating macroeconomic models: A
likelihood approach. The Review of Economic Studies, 74(4):1059–1087, 2007.
A. Finke. On extended state-space constructions for Monte Carlo methods. PhD thesis, Uni-
versity of Warwick, 2015.
R. A. Fisher.
On the mathematical foundations of theoretical statistics.
Philosophical
Transactions of the Royal Society of London Series A, 222:309–368, 1922.
G. Fitzmaurice, M. Davidian, G. Verbeke, and G. Molenberghs. Longitudinal data analysis.
CRC Press, 2008.
S. Fomel and J. F. Claerbout. Reproducible research. Computing in Science & Engineering,
11(1):5–40, 2009.
E. B. Fox. Bayesian nonparametric learning of complex dynamical phenomena. PhD thesis,
Massachusetts Institute of Technology, 2009.
J-P. Fox. Bayesian item response modeling: Theory and applications. Springer Verlag, 2010.
A. Gelman.
Bayesian model-building by pure thought: some principles and examples.
Statistica Sinica, 6(1):215–232, 1996.
A. Gelman, X-L Meng, and H. Stern. Posterior predictive assessment of model ﬁtness via
realized discrepancies. Statistica sinica, 6(4):733–760, 1996.
A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. Bayesian
data analysis. Chapman & Hall/CRC, 3 edition, 2013.

Bibliography
105
S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:
721–741, 1984.
M. Gerber and N. Chopin. Sequential quasi Monte Carlo. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 77(3):509–579, 2015.
Z. Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature, 521(7553):
452–459, 2015.
A. Ghosh, A. Doucet, and W. Heidrich. Sequential sampling for dynamic environment map
illumination. In Proceedings of the 17th Eurographics conference on Rendering Techniques,
pages 115–126, Nicosia, Cyprus, June 2006.
M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73
(2):1–37, 2011.
P. Glasserman. Monte Carlo methods in ﬁnancial engineering. Springer Verlag, 2004.
S. J. Godsill, A. Doucet, and M. West. Monte Carlo smoothing for nonlinear time series.
Journal of the American Statistical Association, 99(465):156–168, March 2004.
N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-
Gaussian Bayesian state estimation. IEEE Proceedings of Radar and Signal Processing, 140
(2):107–113, 1993.
P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination. Biometrica, 82(4):711–732, 1995.
W. H. Greene. Econometric analysis. Prentice Hall, 2008.
M. U. Gutmann and J. Corander. Bayesian optimization for likelihood-free inference of
simulator-based statistical models. Pre-print, 2015. arXiv:1501.03291v1.
M. Hairer, A. M. Stuart, and S. J. Vollmer. Spectral gaps for a Metropolis-Hastings algorithm
in inﬁnite dimensions. The Annals of Applied Probability, 24(6):2455–2490, 2014.
D. I. Hastie, S. Liverani, and S. Richardson. Sampling from Dirichlet process mixture mod-
els with unknown concentration parameter: mixing issues in large data implementations.
Statistics and Computing, 25(5):1023–1037, 2015.
T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning. Springer
Verlag, 2009.
W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57(1):97–109, 1970.
P. Hennig. Fast probabilistic optimization from noisy gradients. In Proceedings of the 30th
International Conference on Machine Learning (ICML), Atlanta, USA, June 2013.
S. Henriksen, A. Wills, T. B. Schön, and B. Ninness. Parallel implementation of particle
MCMC methods on a GPU.
In Proceedings of the 16th IFAC Symposium on System
Identiﬁcation (SYSID), Brussels, Belgium, July 2012.

106
Bibliography
J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In Proceedings
of the 29th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), Bellevue, USA, July
2013.
N. L. Hjort, C. Holmes, P. Müller, and S. G. Walker. Bayesian nonparametrics. Cambridge
University Press, 2010.
A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67, 1970.
M. Hoﬀman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Pre-print,
2012. arXiv:1206.7051v3.
J. D. Hol, T. B. Schön, and F. Gustafsson. On resampling algorithms for particle ﬁlters.
In Proceedings of the Nonlinear Statistical Signal Processing Workshop, Cambridge, UK,
September 2006.
D. Hultqvist, J. Roll, F. Svensson, J. Dahlin, and T. B. Schön. Detection and positioning of
overtaking vehicles using 1D optical ﬂow. In Proceedings of the IEEE Intelligent Vehicles
(IV) Symposium, Dearborn, USA, June 2014.
H. Ishwaran and M. Zarepour. Dirichlet prior sieves in ﬁnite normal mixtures. Statistica
Sinica, 12(3):941–963, 2002.
P. E. Jacob and A. H Thiery. On nonnegative unbiased estimators. The Annals of Statistics,
43(2):769–784, 2015.
A. Jasra, S. S. Singh, J. S. Martin, and E. McCoy.
Filtering via approximate Bayesian
computation. Statistics and Computing, 22(6):1223–1237, 2012.
G. L. Jones. On the Markov chain central limit theorem. Probability surveys, 1:299–320,
2004.
T. Kailath, A. H. Sayed, and B. Hassibi. Linear estimation. Prentice Hall, 2000.
R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of Basic
Engineering, 82(1):35–45, 1960.
G. Kitagawa. Monte Carlo ﬁlter and smoother for non-Gaussian nonlinear state space
models. Journal of Computational and Graphical Statistics, 5(1):1–25, 1996.
G. Kitagawa and S. Sato. Monte Carlo smoothing and self-organising state-space model.
In A. Doucet, N. de Fretias, and N. Gordon, editors, Sequential Monte Carlo methods in
practice, pages 177–195. Springer Verlag, 2001.
M. Kok, J. Dahlin, , T. B. Schön, and A. Wills. Newton-based maximum likelihood esti-
mation in nonlinear state space models. In Proceedings of the 17th IFAC Symposium on
System Identiﬁcation (SYSID), pages 398–403, Beijing, China, October 2015.
D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT
press, 2009.

Bibliography
107
Y. Koren. The BellKor solution to the Netﬂix grand prize. Netﬂix prize documenta-
tion, 2009.
URL http://www.netflixprize.com/assets/GrandPrize2009_BPC_
BellKor.pdf.
J. Kronander and T. B. Schön. Robust auxiliary particle ﬁlters using multiple importance
sampling. In Proceedings of the 2014 IEEE Statistical Signal Processing Workshop (SSP),
Gold Coast, Australia, July 2014.
J. Kronander, J. Dahlin, D. Jönsson, M. Kok, T. B. Schön, and J. Unger. Real-time video
based lighting using GPU raytracing. In Proceedings of the 2014 European Signal Processing
Conference (EUSIPCO), Lisbon, Portugal, September 2014a.
J. Kronander, T. B. Schön, and J. Dahlin. Backward sequential Monte Carlo for marginal
smoothing. In Proceedings of the 2014 IEEE Statistical Signal Processing Workshop (SSP),
Gold Coast, Australia, July 2014b.
R. Langrock. Some applications of nonlinear and non-Gaussian state–space modelling by
means of hidden Markov models. Journal of Applied Statistics, 38(12):2955–2970, 2011.
P-S. Laplace.
Essai philosophique sur les probabilités.
Académie des Sciences, Oeuvres
complétes de Laplace, 7, 1886.
B. Larget and D. L. Simon. Markov chain Monte Carlo algorithms for the Bayesian analysis
of phylogenetic trees. Molecular Biology and Evolution, 16(6):750–759, 1999.
Q. V. Le, A. J. Smola, and S. Canu.
Heteroscedastic Gaussian process regression.
In
Proceedings of the 22nd International Conference on Machine Learning (ICML), pages 489–
496, Bonn, Germany, August 2005.
E. L. Lehmann and G. Casella. Theory of point estimation. Springer Verlag, 1998.
F. Lindsten and T. B. Schön. Backward simulation methods for Monte Carlo statistical
inference. In Foundations and Trends in Machine Learning, volume 6, pages 1–143, August
2013.
S. Livingstone and M. Girolami. Information-geometric Markov chain Monte Carlo meth-
ods using diﬀusions. Entropy, 16(6):3074–3102, 2014.
D. J. Lizotte. Practical Bayesian optimization. PhD thesis, University of Alberta, 2008.
L. Ljung. System identiﬁcation: theory for the user. Prentice Hall, 1999.
V. K. Mansinghka, D. Selsam, and Y. N. Perov.
Venture: a higher-order probabilistic
programming platform with programmable inference. Pre-print, 2014. arXiv:1404.0099.
J-M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder. Approximate Bayesian computational
methods. Statistics and Computing, 22(6):1167–1180, 2012.
F. Markowetz. Five selﬁsh reasons to work reproducibly. Genome biology, 16(1):1–4, 2015.
A. Marshall. The use of multi-stage sampling schemes in Monte Carlo simulations. In
M. Meyer, editor, Symposium on Monte Carlo Methods, pages 123–140. Wiley, 1956.

108
Bibliography
A. Martin, K. Quinn, and J. H. Park. MCMCpack: Markov chain Monte Carlo in R.
Journal of Statistical Software, 42(1):1–21, 2011.
A. D. Martin and K. M. Quinn. Dynamic ideal point estimation via Markov chain Monte
Carlo for the US Supreme Court, 1953–1999. Political Analysis, 10(2):134–153, 2002.
A. D. Martin and K. M. Quinn. Assessing preference change on the US Supreme Court.
Journal of Law, Economics, and Organization, 23(2):365–385, 2007.
G. Matheron. Principles of geostatistics. Economic Geology, 58(8):1246–1266, 1963.
M. Matsumoto and T. Nishimura. Mersenne twister: a 623-dimensionally equidistributed
uniform pseudo-random number generator. ACM Transactions on Modeling and Computer
Simulation (TOMACS), 8(1):3–30, 1998.
D. Q. Mayne. Model predictive control: Recent developments and future promise. Auto-
matica, 50(12):2967–2986, 2014.
P. McCullagh and J. A. Nelder. Generalized linear models. Chapman Hall/CRC, 1989.
G. J. McLachlan and T. Krishnan. The EM algorithm and extensions. Wiley-Interscience, 2
edition, 2008.
A. J. McNeil, R. Frey, and P. Embrechts. Quantitative risk management: concepts, techniques,
and tools. Princeton University Press, 2010.
E. Meeds and M. Welling. GPS-ABC: Gaussian process surrogate approximate Bayesian
computation. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), Quebec City, Canada, July 2014.
N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation
of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6):
1087–1092, 1953.
S. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. Cambridge University
Press, 2009.
A. Mira, R. Solgi, and D. Imparato. Zero variance Markov chain Monte Carlo for Bayesian
estimators. Statistics and Computing, 23(5):653–662, 2013.
J. Mockus, V. Tiesis, and A. Zilinskas. The application of Bayesian methods for seeking
the extremum. In L. C. W. Dixon and G. P. Szego, editors, Toward Global Optimization,
pages 117–129. North-Holland, 1978.
C. Monteleoni, G. A. Schmidt, S. Saroha, and E. Asplund. Tracking climate models. Statis-
tical Analysis and Data Mining, 4(4):372–392, 2011.
K. P. Murphy. Machine learning: a probabilistic perspective. The MIT Press, 2012.
I. Murray and M. M. Graham.
Pseudo-marginal slice sampling.
Pre-print, 2015.
arXiv:1510.02958v1.
L. M. Murray. Bayesian state-space modelling on high-performance hardware using LibBi.
Pre-print, 2013. arXiv:1306.3277.

Bibliography
109
L. M. Murray, A. Lee, and P. E. Jacob. Parallel resampling in the particle ﬁlter. Journal of
Computational and Graphical Statistics (accepted for publication), 2015.
C. A. Naesseth, F. Lindsten, and T. B. Schön. Sequential Monte Carlo for graphical models.
In Proceedings of the 2014 Conference on Neural Information Processing Systems (NIPS),
Montreal, Canada, December 2014.
C. A. Naesseth, F. Lindsten, and T. B. Schön. Nested sequential Monte Carlo methods.
In Proceedings of the 32nd International Conference on Machine Learning (ICML), Lille,
France, July 2015.
R. M. Neal. Slice sampling. The Annals of Statistics, 31(3):705–741, 2003.
R. M. Neal. MCMC using Hamiltonian dynamics. In S. Brooks, A. Gelman, G. Jones, and
X-L. Meng, editors, Handbook of Markov Chain Monte Carlo. Chapman & Hall/CRC
Press, 2010.
W. Neiswanger, C. Wang, and E Xing.
Asymptotically exact, embarrassingly parallel
MCMC. Pre-print, July 2014.
R. B. Nelsen. An introduction to copulas. Springer Verlag, 2007.
C. Nemeth, C. Sherlock, and P. Fearnhead. Particle Metropolis adjusted Langevin algo-
rithms. Pre-print, 2014. arXiv:1412.7299v1.
E. Neuwirth. RColorBrewer: ColorBrewer Palettes, 2014. URL https://CRAN.R-project.
org/package=RColorBrewer. R package version 1.1-2.
H. Niederreiter. Quasi-Monte Carlo methods. In Encyclopedia of Quantitative Finance.
John Wiley & Sons, 2010.
J. Nocedal and S. Wright. Numerical optimization. Springer Verlag, 2 edition, 2006.
J. Nolan. Stable distributions: models for heavy-tailed data. Birkhauser, 2003.
P. Orbanz. Construction of nonparametric Bayesian models from parametric Bayes equa-
tions. In Proceedings of the 2009 Conference on Neural Information Processing Systems
(NIPS), Vancouver, Canada, December 2009.
M. Osborne. Bayesian Gaussian processes for sequential prediction, optimisation and quadra-
ture. PhD thesis, University of Oxford, 2010.
M. A. Osborne, R. Garnett, S. J. Roberts, C. Hart, S. Aigrain, and N. Gibson. Bayesian
quadrature for ratios. In Proceedings of the 15th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), pages 832–840, La Palma, Canary Islands, Spain,
April 2012.
A. B. Owen. Monte Carlo theory, methods and examples. Book manuscript, 2013. URL
http://statweb.stanford.edu/~owen/mc/.
T. Papamarkou, A. Mira, and M. Girolami. Zero variance diﬀerential geometric Markov
chain Monte Carlo algorithms. Bayesian Analysis, 9(1):97–128, 2014.

110
Bibliography
F. Pérez and R. E. Granger. IPython: a system for interactive scientiﬁc computing. Com-
puting in Science and Engineering, 9(3):21–29, 2007.
G. W. Peters, G. R. Hosack, and K. R. Hayes. Ecological non-linear state space model selec-
tion via adaptive particle Markov chain Monte Carlo. Pre-print, 2010. arXiv:1005.2238v1.
M. Pharr and G. Humphreys. Physically based rendering: from theory to implementation.
Morgan Kaufmann, 2010.
A. W. Phillips. The relation between unemployment and the rate of change of money wage
rates in the United Kingdom, 1861-1957. Economica, 25(100):283–299, 1958.
M. K. Pitt, R. S. Silva, P. Giordani, and R. Kohn. On some properties of Markov chain
Monte Carlo simulation methods based on the particle ﬁlter. Journal of Econometrics, 171
(2):134–151, 2012.
G. Poyiadjis, A. Doucet, and S. S. Singh. Particle approximations of the score and ob-
served information matrix in state space models with application to parameter estimation.
Biometrika, 98(1):65–80, 2011.
M. Quiroz, M. Villani, and R. Kohn. Speeding up MCMC by eﬃcient data subsampling.
Pre-print, 2016. arXiv:1404.4178v3.
R Core Team. R: A language and environment for statistical computing. R Foundation for
Statistical Computing, Vienna, Austria, 2015. URL https://www.R-project.org/.
W. Raghupathi and V. Raghupathi. Big data analytics in healthcare: promise and potential.
Health Information Science and Systems, 2(1):3, 2014.
C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. MIT Press,
2006.
D. A. Rasmussen, O. Ratmann, and K. Koelle. Inference for nonlinear epidemiological
models using genealogies and time series. PLoS Computational Biology, 7(8):1–11, 2011.
P. Rebeschini and R. van Handel. Can local particle ﬁlters beat the curse of dimensionality?
The Annals of Applied Probability, 25(5):2809–2866, 2015.
C. P. Robert. The Bayesian choice. Springer Verlag, 2007.
C. P. Robert and G. Casella. Monte Carlo statistical methods. Springer Verlag, 2 edition,
2004.
C. P. Robert and G. Casella. Introducing Monte Carlo methods with R. Springer Verlag,
2009.
G. O. Roberts and O. Stramer. Langevin diﬀusions and Metropolis-Hastings algorithms.
Methodology and Computing in Applied Probability, 4(4):337–357, 2003.
G. O. Roberts and R. L. Tweedie. Exponential convergence of Langevin distributions and
their discrete approximations. Bernoulli, 2(4):341–363, 1996.

Bibliography
111
G. O. Roberts, A. Gelman, and W. R. Gilks. Weak convergence and optimal scaling of
random walk Metropolis algorithms. The Annals of Applied Probability, 7(1):110–120,
1997.
J. Rousseau and K. Mengersen. Asymptotic behaviour of the posterior distribution in
overﬁtted mixture models.
Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 73(5):689–710, 2011.
T. B. Schön, F. Lindsten, J. Dahlin, J. Wågberg, C. A. Naesseth, A. Svensson, and L. Dai.
Sequential Monte Carlo methods for system identiﬁcation. In Proceedings of the 17th IFAC
Symposium on System Identiﬁcation (SYSID), pages 775–786, Beijing, China, October 2015.
M. Segal and E. Weinstein. A new method for evaluating the log-likelihood gradient, the
Hessian, and the Fisher information matrix for linear dynamic systems. IEEE Transactions
on Information Theory, 35(3):682–687, 1989.
J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650,
1994.
A. Shah, A. G. Wilson, and Z. Ghahramani. Student-t processes as alternatives to Gaussian
processes. In Proceedings of the 17th International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), pages 832–840, Reykjavik, Iceland, April 2014.
C. Sherlock, A. H. Thiery, G. O. Roberts, and J. S. Rosenthal. On the eﬃcency of pseudo-
marginal random walk Metropolis algorithms. The Annals of Statistics, 43(1):238–275,
2015.
R. H. Shumway and D. S. Stoﬀer. Time series analysis and its applications. Springer Verlag,
3 edition, 2011.
J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learn-
ing algorithms. In Proceedings of the 2012 Conference on Neural Information Processing
Systems (NIPS), Lake Tahoe, USA, December 2012.
I. M. Sobol. On the distribution of points in a cube and the approximate evaluation of
integrals. Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 7(4):784–802, 1967.
H. J. Spaeth, L. Epstein, A. D. Martin, J. A. Segal, T. J. Ruget, and S. C. Benesh. Supreme
court database. Version 2016 Release 01, 2016. URL http://supremecourtdatabase.
org.
J. C. Spall. A stochastic approximation technique for generating maximum likelihood
parameter estimates. In Proceedings of the 6th American Control Conference (ACC), pages
1161–1167, Minneapolis, USA, June 1987.
J. C. Spall. Implementation of the simultaneous perturbation algorithm for stochastic
optimization. IEEE Transactions on Aerospace and Electronic Systems, 34(3):817–823, 1998.
D. J. Spiegelhalter. Incorporating Bayesian ideas into health-care evaluation. Statistical
Science, 19(1):156–174, 2004.
Stan Development Team. Stan: A C++ library for probability and sampling, version 2.8.0,
2015. URL http://mc-stan.org/.

112
Bibliography
D. H. Stern, R. Herbrich, and T. Graepel. Matchbox: large scale online Bayesian recom-
mendations. In Proceedings of the 18th international conference on World wide web, pages
111–120, Madrid, Spain, April 2009.
L. Stewart and P. McCarty, Jr. Use of Bayesian belief networks to fuse continuous and
discrete information for target recognition, tracking, and situation assessment. In Pro-
ceedings of SPIE Signal Processing, Sensor Fusion and Target Recognition, pages 177–185,
Orlando, USA, April 1992.
J. Stoer and R. Bulirsch. Introduction to numerical analysis. Springer Verlag, 2 edition, 1993.
G. Storvik. Particle ﬁlters for state-space models with the presence of unknown static
parameters. IEEE Transactions on Signal Processing, 50(2):281–289, 2002.
A. Svensson, J. Dahlin, and T. B. Schön. Marginalizing Gaussian process hyperparameters
using sequential Monte Carlo. In Proceedings of the 6th IEEE International Workshop on
Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), Cancun, Mexico,
December 2015.
R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 58(1):267–288, 1996.
L. Tierney. Markov chains for exploring posterior distributions. The Annals of Statistics, 22
(4):1701–1728, 1994.
M. K. Titsias and N. D. Lawrence. Bayesian quadrature for ratios. In Proceedings of the 13th
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 844–851,
Sardinia, Italy, May 2010.
R. S. Tsay. Analysis of ﬁnancial time series. John Wiley & Sons, 2 edition, 2005.
V. Turri, B. Besselink, and K. H. Johansson.
Cooperative look-ahead control for fuel-
eﬃcient and safe heavy-duty vehicle platooning. Pre-print, 2015. arXiv:1505.00447v1.
Y. Ulker, B. Gunsel, and T. A. Cemgil. Sequential Monte Carlo samplers for Dirichlet pro-
cess mixtures. In Proceedings of the 13th International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), pages 876–883, Sardinia, Italy, May 2010.
J. Unger, J. Kronander, P. Larsson, S. Gustavson, J. Löw, and A. Ynnerman. Spatially
varying image based lighting using HDR-video. Computers & Graphics, 37(7):923–934,
2013.
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. A graph/particle-based method for
experiment design in nonlinear systems. In Proceedings of the 19th IFAC World Congress,
Cape Town, South Africa, August 2014.
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. On robust input design for
nonlinear dynamical models. Automatica, 2016a. (provisionally accepted).
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. Particle-based Gaussian pro-
cess optimization for input design in nonlinear dynamical models.
Pre-print, 2016b.
arXiv:1603.05445v1.

Bibliography
113
A. W. Van der Vaart. Asymptotic statistics. Cambridge University Press, 2000.
P. Vandewalle, J. Kovačević, and M. Vetterli. Reproducible research in signal processing.
IEEE Signal Processing Magazine, 26(3):37–47, 2009.
E. Veach and L. J. Guibas. Optimally combining sampling techniques for Monte Carlo
rendering. In Proceedings of the 22nd Annual Conference on Computer Graphics, pages
419–428, Los Angeles, USA, August 1995.
J. Wågberg, F. Lindsten, and T. B. Schön. Bayesian nonparametric identiﬁcation of piecewise
aﬃne ARX systems. In Proceedings of the 17th IFAC Symposium on System Identiﬁcation
(SYSID), pages 709–714, Beijing, China, October 2015.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics.
In Proceedings of the 28th International Conference on Machine Learning (ICML), pages
681–688, Bellevue, USA, July 2011.
N. Whiteley. Stability properties of some particle ﬁlters. The Annals of Applied Probability,
23(6):2500–2537, 2013.
F. Wood, J. W. van de Meent, and V. Mansinghka. A new approach to probabilistic program-
ming inference. In Proceedings of the 17th International conference on Artiﬁcial Intelligence
and Statistics (AISTATS), pages 1024–1032, Reykjavik, Iceland, April 2014.
Y. Xie. knitr: a comprehensive tool for reproducible research in R. In V. Stodden, F. Leisch,
and R. D. Peng, editors, Implementing reproducible computational research. Chapman and
Hall/CRC, 2014.
E. P. Xing, M. I. Jordan, and R. Sharan. Bayesian haplotype inference via the Dirichlet
process. Journal of Computational Biology, 14(3):267–284, 2007.
H. Xue, F. Gu, and X. Hu. Data assimilation using sequential Monte Carlo methods in
wildﬁre spread simulation. ACM Transactions on Modeling and Computer Simulation
(TOMACS), 22(4):23, 2012.
J. Yang, N. A. Zaitlen, M. E. Goddard, P. M. Visscher, and A. L. Price. Advantages and
pitfalls in the application of mixed-model association methods. Nature genetics, 46(2):
100–106, 2014.
Z. Zhang, E. Ersoz, C-Q. Lai, R. J. Todhunter, H. K. Tiwari, M. A. Gore, P. J. Bradbury,
J. Yu, D. K. Arnett, J. M. Ordovas, and E.S Buckler. Mixed linear model approach
adapted for genome-wide association studies. Nature genetics, 42(4):355–360, 2010.
H. S. Zhou. Modiﬁed backward sampling smoothing with EM algorithm - application to
economics and ﬁnance. Pre-print, 2013. Unpublished report.
H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of
the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.


Part II
Papers


Paper A
Getting started with particle
Metropolis-Hastings for inference in
non-linear models
Authors:
J. Dahlin and T. B. Schön
Edited version of the paper:
J. Dahlin and T. B. Schön. Getting started with particle Metropolis-Hastings
for inference in nonlinear models. Pre-print, 2015. arXiv:1511.01707v4.


Getting started with particle Metropolis-Hastings
for inference in non-linear models
J. Dahlin⋆and T. B. Schön†
⋆Dept. of Electrical Engineering,
Linköping University,
SE–581 83 Linköping, Sweden.
johan.dahlin@liu.se
†Dept. of Information Technology,
Uppsala University,
se-751 05 Uppsala, Sweden.
thomas.schon@it.uu.se
Abstract
We provide a gentle introduction to the particle Metropolis-Hastings ( pmh) algorithm for parameter
inference in non-linear state space models (ssms) together with a software implementation in the
statistical programming language R. Throughout this tutorial, we develop an implementation of the
pmh algorithm (and the integrated particle ﬁlter), which is distributed as the package pmhtutorial
available from the cran repository. Moreover, we provide the reader with some intuition for how
the algorithm operates and discuss some solutions to numerical problems that might occur in practice.
To illustrate the use of pmh, we consider parameter inference in a linear Gaussian ssm with synthetic
data and a non-linear stochastic volatility model with real-world data. We conclude the tutorial by
discussing important possible improvements to the algorithm and we also list suitable references for
further study.
Keywords
Bayesian inference, state space models, particle ﬁltering, particle Markov chain Monte Carlo.
Data and source code in R, MATLAB and Python
https://github.com/compops/pmh-tutorial
Financial support from
The projects Probabilistic modeling of dynamical systems (Contract number: 621-2013-5524) and cadics,
a Linnaeus Center, both funded by the Swedish Research Council.
119

120
Paper A
Getting started with PMH for inference in non-linear models
Introductory overview
We are concerned with Bayesian parameter inference in non-linear and/or non-Gaussian
state space models (ssms). This is an important problem as ssms are ubiquitous in e.g.,
automatic control (Ljung, 1999), econometrics (Durbin and Koopman, 2012), ﬁnance (Tsay,
2005) and many other ﬁelds. An overview of some concrete application of state space
modelling is given by Langrock (2011). The major problem with Bayesian inference in
ssms is that it is an analytically intractable problem. That is, we cannot obtain closed-form
prior-posterior updates by using conjugate priors, which is sometimes possible e.g., the data
is independent and identically distributed. Instead, we make use of statistical simulation
techniques based on Monte Carlo to create an empirical approximation of the posterior
distribution. That is, a weighted sum of Dirac atoms that converge to the true posterior as
the number of atoms tends to inﬁnity.
In this tutorial, we make use of the particle Metropolis-Hastings (pmh; Andrieu et al.,
2010) algorithm to sample from the sought posterior distribution and create the empirical
approximation. A requirement for using the pmh algorithm is that we can evaluate the
posterior point-wise by an unbiased non-negative estimator. In this tutorial, we employ
a particle ﬁlter (Gordon et al., 1993; Doucet and Johansen, 2011) to create this unbiased
estimator. Note that particle ﬁltering is interesting on its own, which is discussed in the
aforementioned references.
The main aim and contribution of this tutorial is to give a gentle introduction (both in
terms of the methods and in terms of software) to the pmh algorithm and to the inherent
particle ﬁltering. We assume that the reader is familiar with traditional time series analysis
and Kalman ﬁltering at the level of Brockwell and Davis (2002). Some familiarity with
Monte Carlo approximations, importance sampling and Markov chain Monte Carlo at the
level of Ross (2012) is also beneﬁcial. The reader is referred to the aforementioned books
for the background material required for this tutorial.
We focus on developing implementable code for the algorithms in two diﬀerent models.
Furthermore, we provide the reader with some implementation tricks to avoid numeri-
cal problems that might occur in some models. We also discuss some properties of the
algorithms and try to give the reader some intuition of the ideas behind them. The ﬁnal
implementation is available as a R (R Core Team, 2015) package with the name pmhtutorial
distributed under the gpl-2 license via the cran repository. See Section 7.3 for alternative
implementations in MATLAB and Python.
We continue this section by introducing the ssm and the parameter and the state inference
problems that are solved using the pmh algorithm and the particle ﬁlter.
State space models
In Figure 1, we present a graphical model of the ssm with the latent states (upper), the
observations (lower) and no input1. We note that the latent state denoted by xt only
depends on the previous state xt−1 of the process as it is a Markov process (of ﬁrst order).
1It is straightforward to modify the algorithms presented for vector valued quantities and also to include a
known exogenous input ut .

1
Introductory overview
121
· · ·
xt−1
xt
xt+1
· · ·
· · ·
yt−1
yt
yt+1
· · ·
Figure 1. A cartoon of the structure of an ssm using a graphical model.
That is, all the information in the past states x0:t−1 is summarised in the most recent state
xt−1. Furthermore, the observations are conditionally independent as they only relate to
each other through the states.
More specially, we assume that the observations and the states are denoted by y1:T ≜{yt }T
t=1
and x0:T , respectively. Furthermore, we assume that the state and observations are real-
valued scalars, i.e., yt ∈Y ⊆R and xt ∈X ⊆R. Note that this assumption can be
straightforwardly relaxed so that states and observations are vector-valued real numbers.
The initial state is assumed to be distributed according to the density µθ(x0) parametrised
by some unknown static parameters θ ∈Θ ⊂Rp. The latent state and the observation pro-
cesses are assumed to be governed by the densities2 fθ(xt | xt−1) and gθ(yt | xt), respectively.
That is, the density describing the state processes gives the probability that the next state is
xt given the previous state xt−1. The same interpretation holds for the observation process.
With these assumptions in place, we can express a general non-linear ssm by
x0 ∼µθ(x0),
xt | xt−1 ∼fθ(xt | xt−1),
yt | xt ∼gθ(yt | xt),
(1)
where we make no notational distinction between the random variables and their realisa-
tions for brevity.
Bayesian inference
The main objective in the parameter inference problem is to obtain an estimate of the pa-
rameters θ given the measurements y1:T . In this tutorial, we adopt a Bayesian approach to
estimate θ by computing the parameter posterior distribution given by
πθ(θ) ≜p(θ |y1:T ) =
p(θ)p(y1:T |θ)

Θ
p(θ′)p(y1:T |θ′)dθ′
,
(2)
2In this tutorial, we assume that xt | xt−1 and yt | xt can be modelled as continuous random variables with a
density function. However, the algorithms that we introduce can be applied to deterministic states and discrete
states/observations as well.

122
Paper A
Getting started with PMH for inference in non-linear models
where p(θ) and p(y1:T |θ) = pθ(y1:T ) denote the parameter prior distribution and the like-
lihood of the data, respectively. The denominator is usually referred to as the marginal
likelihood or the model evidence.
A standard approach to estimate (2) is to employ Markov chain Monte Carlo (mcmc;
Robert and Casella, 2004) methods. This family of methods constructs a Markov chain
to sample from πθ(θ). An empirical approximation is then constructed of the parameter
posterior by samples from the Markov chain. In this tutorial, we make use of an mcmc
algorithm known as the particle Metropolis-Hastings (pmh; Andrieu et al., 2010) algorithm.
However, this algorithm requires that we can estimate pθ(y1:T ) point-wise but this is diﬃcult
as we do not know the state sequence. The problem appears when we would like to compute
the likelihood by the decomposition
pθ(y1:T ) = pθ(y1)
T
Y
t=2
pθ(yt |y1:t−1).
(3)
In this expression, the predictive likelihood can be computed by
pθ(yt |y1:t−1) =

gθ(yt | xt) fθ(xt | xt−1)πt−1(xt−1)dxtdxt−1,
(4)
where πt−1(xt−1) = pθ(xt−1 |y1:t−1) denotes the unknown marginal ﬁltering distribution.
Hence, we are required to solve a state inference problem to estimate the value of the state
xt to be able sample from πθ(θ) to form the empirical approximation.
For an ssm, we can compute the marginal ﬁltering distribution using the Bayesian ﬁltering
recursions given the parameters. As the name suggests, these are recursions that update
the estimate of the states as more observations arrive in a Bayesian prior-posterior update.
Speciﬁcally, we obtain the marginal ﬁltering distribution πt(xt) as the posterior distribution
of the current state given all the observations up until the current time step. However,
for most models of interest the recursions cannot be solved in closed form. Instead, we
approximate πt(xt) using a Monte Carlo method known as the particle ﬁlter (Doucet and
Johansen, 2011) or sequential importance sampling with resampling (sir).
Related software
There are a number of diﬀerent software packages related to the current tutorial. We focus
on some minimal working examples for some toy problems to explain the workings of the
algorithms. Other software projects are far more comprehensive and provide platforms for
solving the inference problem in more general models. Hence, we view pmhtutorial as
a gentle introduction to the pmh algorithm after which the reader can move on to more
general software to solve larger application speciﬁc problems.
The software LibBi (Murray, 2013) provides a platform for Bayesian inference in ssms using
both serial and parallel hardware. It consists of a C++ template library, a parser and a com-
piler. This allow the user to in a simple manner deﬁne new models and solve the parameter
inference problem using e.g., pmcmc and smc2 (Chopin et al., 2013). Furthermore, inter-
faces for R and MATLAB are currently under development. Another alternative with the
same set-up is Biips (Todeschini et al., 2014).

3
Overview of the PMH algorithm
123
Two interesting probabilistic programming languages for more multi-purpose statistical
inference are Venture (Mansinghka et al., 2014) and Anglican (Wood et al., 2014). Both
packages provides functionality for implementing particle ﬁltering and pmcmc for gen-
eral ssms. However, Venture is currently under the alpha stage of development and the
documentation is therefore currently a bit lacking.
There are also a number of additional software packages that implement particle ﬁltering
in ssms. The software pyParicleEst (Nordh and Berntorp, 2013) is written in Python and
provides functionality for state estimate using diﬀerent types of particle ﬁlters. It also in-
cludes parameter estimation using Maximum Likelihood via the Expectation Maximisation
(em; Dempster et al. (1977); McLachlan and Krishnan (2008)) algorithm. A C++ template
for particle ﬁltering and more general sequential algorithms based on the Feynman-Kac
formalism is provided by SMCTC (Johansen, 2009).
Overview of the PMH algorithm
As previously discussed, we approximate the intractable parameter posterior for a general
ssm (1) using an empirical approximation based on a Monte Carlo method known as
pmh. This algorithm generates samples from πθ(θ) by creating a Markov chain with some
necessary properties. More speciﬁcally, we require the stationary distribution of the Markov
chain to be the parameter posterior. This means that we can simulate samples from the
posterior by using the Markov chain when it has reached stationarity. We refer to the initial
transient phase before stationarity has been reached as the burn-in.
Constructing the Markov chain
The construction of the Markov chain consists of two steps. The ﬁrst step is to propose a
so-called candidate parameter θ′ from a proposal distribution q(θ′|θk−1) given the previous
state of the Markov chain denoted θk−1. The proposal distribution is determined by the user
with the requirement that it should at least have a support covering the target distribution.
The second step is to determine if we should change the state to θ′ or remain in the current
state θk−1. This is done to facilitate exploration of (in theory) the entire posterior distri-
bution. Also, this mechanism is the necessary condition for the Markov chain to actually
have the sought posterior as its stationary distribution. The so-called acceptance probability
is computed as
α θ′, θk−1
 = 1 ∧
πθ
 θ′
πθ
 θk−1
 q θk−1 |θ′
q θ′|θk−1
,
(5)
which determines the probability that we assign the candidate parameter as the next state
of the Markov chain, i.e., {θk ←θ′}. Here, we introduce the notation a ∧b ≜min{a, b}.
The intuition for the acceptance probability (5) (disregarding the inﬂuence of the proposal
q) is that we always accept a candidate parameter θ′ if πθ
 θ′ > πθ
 θk−1

. That is if θ′
increases the value of the target compared with the previous state θk−1. This results in a
mode seeking behaviour which is similar to an optimisation algorithm, where we would
like to estimate the maximum of the posterior distribution. However from (5), we also

124
Paper A
Getting started with PMH for inference in non-linear models
note that we can accept a small decrease in the posterior value to facilitate exploration of
the entire posterior. This is the main diﬀerence between pmh and that of an optimisation
algorithm, where we would like to explore the full posterior in the former and only the
mode in the latter.
The resulting K correlated samples θ1:K ≜{θk}K
k=1 can be used to construct a Monte
Carlo approximation of πθ(θ). In this case, we can write the empirical approximation of the
parameter posterior distribution as
DπK
θ (dθ) = 1
K
K
X
k=1
δθk(dθ),
(6)
which corresponds to a collection of Dirac atoms δθ′(dθ) located at θ = θ′ with equal
weights. In practice, we make use of histograms or kernel density estimators to visualise the
estimate of the parameter posterior obtained from (6).
In Bayesian parameter inference, we are often interested in computing the expectation of
a so-called test function, which is a well-behaved (integrable) function ϕ : Θ →R , which
maps the parameters to a value on the real line. The expectation with respect to parameter
posterior is given by
πθ[ϕ] ≜E
f
ϕ(θ)|y1:T
g
=

Θ
ϕ(θ)π(θ) dθ,
(7)
where ϕ(θ) = θ corresponds to computing the mean of the parameter posterior. Unfortu-
nately, we have that πθ[ϕ] is intractable as we do not have access to a closed-form expression
for πθ(θ). Instead, we can replace it with the empirical approximation in (6) to obtain
DπK
θ [ϕ] ≜

Θ
ϕ(θ)DπK (dθ) = 1
K
K
X
k=1

Θ
ϕ(θ)δθk(dθ) = 1
K
K
X
k=1
ϕ θk
,
(8)
which follows from the properties of the Dirac delta function (measure). This estimator
is well-behaved and it is possible to establish a law of large numbers ( lln) and a central
limit theorem ( clt), see Robert and Casella (2004) or Meyn and Tweedie (2009) for more
information. From the lln, we know that the estimator is consistent (and asymptotically
unbiased as K →∞).
Moreover from the clt, we know that the error is approximately Gaussian with a variance
that decreases with 1/K, which is the usual Monte Carlo rate. Note that the lln usually
assumes independent samples but a similar result known as the ergodic theorem gives a similar
result (under some assumptions) even when θ1:K are correlated. However, the asymptotic
variance is larger than if the samples would be uncorrelated.
Approximating the acceptance probability
The main problem when implementing the pmh algorithm for many ssms is that we
cannot compute the acceptance probability (5). This is the result of the intractability of the
posterior as the likelihood cannot be evaluated point-wise. Instead, we make use of a point

3
Overview of the PMH algorithm
125
estimate of the likelihood given by the particle ﬁlter. The resulting estimator is unbiased
and non-negative, which turns out to be the necessary condition for this approach to be
valid. That is, for the pmh algorithm to still generate samples from and generate a valid
empirical approximation of πθ(θ). This approach is known as an exact approximation, as
we approximate the likelihood but still obtain an exact algorithm. We return to discussing
this family of methods in Section 7.
To estimate the likelihood, we are required to estimate the states as outlined in the decompo-
sition of the likelihood in (3). These states can (in theory) be estimated using the Bayesian
ﬁltering recursions (Anderson and Moore, 2005) (for ﬁxed parameters θ) by
πt(xt) =
gθ(yt | xt)
pθ(yt |y1:t−1)

X
fθ(xt | xt−1)πt−1(xt−1) dxt−1,
for 0 < t ≤T ,
(9)
with π0(x0) = µθ(x0) as the initialisation. In theory, we can construct a sequence of ﬁltering
distributions by iteratively applying (9). However, we cannot carry out this procedure using
closed-form expressions for many models of interest.
Instead, we again make use of an empirical approximation of πt(xt) for each t. To generate
samples from the marginal ﬁltering distribution, we apply importance sampling sequentially
to estimate the state trajectories by approximating (9). The resulting algorithm is known as
the particle ﬁlter or sir. Using the samples generated by the particle ﬁlter, we can construct
the approximation by
DπN
t (dxt) ≜DpN
θ (dxt |y1:t) =
N
X
i=1
w(i)
t δx(i)
t (dxt),
(10)
where the particles x(i)
t
and their corresponding weights w(i)
t
constitutes the so-called particle
system generated during a run of the particle ﬁlter.
In Figure 2, we present a cartoon of (10) with the true marginal ﬁltering distribution (green)
and the Dirac point masses (orange) that aims to approximate it. The location of the point
masses corresponds to the location of the particle x(i)
t
and the height is given by the weight
of the particle w(i)
t . It is possible to prove that the empirical approximation converges to
the true distribution when N →∞under some regularity assumptions.
As for the parameter inference problem, we are often required to compute an expectation
of a well-behaved test function ϕ : X →R with respect to πt(xt) given by
πt[ϕ] ≜E
f
ϕ(xt)|y1:t
g
=

X
ϕ(xt)πt(xt) dxt,
where ϕ(xt) = xt corresponds to computing the mean of the marginal ﬁltering distribu-
tion. The computation of πt[ϕ] is intractable as we cannot compute πt in closed form
for most ssms. However, we can approximate this expectation by inserting the empirical

126
Paper A
Getting started with PMH for inference in non-linear models
approximation (10). This results in
DπN
t [ϕ] ≜

X
ϕ(xt)DπN
t (dxt) =
N
X
i=1
w(i)
t

X
ϕ(xt)δx(i)
t (dxt) =
N
X
i=1
w(i)
t ϕ

x(i)
t

,
(11)
for some 0 ≤t ≤T by the properties of the Dirac delta function. Under some assumptions,
the properties of DπN
t [ϕ] are similar as for the estimator in the pmh algorithm, see Crisan
and Doucet (2002) or Doucet and Johansen (2011) for more information. Hence, we have
that the estimator is consistent (and asymptotically unbiased when N →∞) and the error
is Gaussian with a variance that decreases as 1/N .
We conclude this section by noting that solving the parameter inference problem in an
ssm also requires us to solve the state inference problem. As a consequence, we begin the
subsequent section by presenting how to implement the particle ﬁlter for constructing (10).
We then proceed with implementing the pmh algorithm using the output from the particle
ﬁlter to approximate the acceptance probability (5).
We also discuss how to avoid some issues with numerical precision and to decrease the
computational complexity by constructing optimal proposal distributions. In Section 5, we
consider a real-world application of the pmh algorithm from ﬁnance and highlight some
practical problems and more sophisticated improvements in Section 6. Finally, we conclude
in Section 7 with an overview of further reading for extending the material covered here.
Estimating the parameters in a linear Gaussian SSM
In this section, we discuss how to estimate the parameter posterior πθ(θ) for a linear Gaus-
sian state space ( lgss) model. We consider this model as it is possible to solve the state
inference problem exactly using the Kalman ﬁlter. That is, we can solve (9) in closed-form
using the properties of the Gaussian distribution. Hence, we can make use of the Kalman
ﬁlter to validate and benchmark the particle ﬁlter to learn more about its properties.
The particular lgss model that we consider is given by
x0 ∼δx0(x),
xt | xt−1 ∼N

xt; φxt−1, σ2
v

,
yt | xt ∼N

yt; xt, σ2
e

,
(12)
where parameters are denoted by θ = {φ, σv, σe}. The parameter φ ∈(−1, 1) determines
the persistence of the state. The parameters σv, σe ∈R+ denote the standard deviations
of the state noise and the observation noise, respectively. Here, N(x; µ, σ2) denotes the
Gaussian distribution of a random variable x with mean µ and standard deviation σ > 0
and δx0(x) denotes a Dirac mass located at x = x0 ∈R. In Figure 3, we present T =
250 simulated data points from this model using θ = {0.95, 0.10, 1.00}. Note that, the
autocorrelation function ( acf) for y1:T falls oﬀslowly as the persistence parameter φ is
close to one and σv is rather small.

4
Estimating the parameters in a linear Gaussian SSM
127
0
2
4
6
8
10
0.00
0.05
0.10
0.15
0.20
0.25
0.30
x
density
0
2
4
6
8
10
0.00
0.05
0.10
0.15
0.20
0.25
0.30
x
density
Figure 2. A cartoon of the particle approximation of some distribution (green) given by four
(left) and nine (right) particles (orange), respectively.
0
50
100
150
200
250
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
time
latent state xt
0
50
100
150
200
250
-4
-2
0
2
4
time
observation yt
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
time
ACF of yt
Figure 3. Simulated data from the lgss model with latent state (left), observations (center) and
acf of the observations (right).

128
Paper A
Getting started with PMH for inference in non-linear models
Implementing the particle filter
To implement the pmh algorithm, we are required to also implement a particle ﬁlter to
estimate the likelihood in the parameter posterior (2). Here, we start out by presenting the
particle ﬁlter and then make use of it in the pmh algorithm in Section 4.3.
Both algorithms are implemented in R (R Core Team, 2015). For this end, we make use of
literate programming3 and the complete code is available in the function sm included in the
package pmhtutorial. The corresponding skeleton for the code is given by:
sm <- function(y, par, nPart, T, x0) {
< initialisation >
for (tt in 2:T) {
< resampleParticles >
< propagateParticles >
< weightParticles >
< estimateFilteredStateAndLogLikelihood >
}
< returnEstimates >
}
The function sm has inputs: y (vector with T observations), par (parameters {φ, σv, σe}),
nPart (no. particles), T (no. data points) and x0 (the initial state). Note that, the function
updates the particle system and compute the estimates iteratively over the index tt starting
at 2 and ending with T (corresponding to t = 1 to T −1 in (12)).
< initialisation > We allocate the variables p, u, w, xhatf and ll for the particles, their
corresponding unnormalised and normalised weights, the ﬁltered state estimates and the
log-likelihood estimate, respectively. For the lgss model (12), we have µθ(x0) = δ0(x0)
so all the particles are initially set to x0 = 0 and all weights to 1/N (as every particle is
identical). This operation is carried out by:
< initialisation > =
xhatf <- matrix(x0,
nrow=T,
ncol=1)
p
<- matrix(x0,
nrow=nPart, ncol=T+1)
v
<- matrix(1,
nrow=nPart, ncol=T+1)
w
<- matrix(1 / nPart, nrow=nPart, ncol=T+1)
ll
<- 0;
< resampleParticles > The intuition behind this step is that we focus the computational
eﬀort of the algorithm to the relevant part of the state space. That is, we focus on states that
are likely to have generated the obtained observation under the model. More speciﬁcally,
we compute a weight for each particle corresponding to the probability that the speciﬁc
particle generated the observation under the model. In the resampling, we then randomly
duplicate particles with large weights and discard particles with small weights. Note that,
the resampling step is unbiased in the sense that the expected proportions of the resampled
3In this approach, we start by outlining the skeleton of the code with some variables marked by < variable
>. In the following paragraphs, we assign code to each of these variables using a C++ like syntax. That is, the
assignment operator is denoted by = and += denotes the operation that we add the code after the current code
stored in the variable. For more information, see e.g., Section 1.1 in Pharr and Humphreys (2010).

4
Estimating the parameters in a linear Gaussian SSM
129
particles are given by the particle weights. This step is important as we otherwise would
end up with a single unique particle and a large variance in (11) after a number of iterations.
In our implementation, we make use of multinomial resampling, which is also known
as a weighted bootstrap with replacement. The output from this procedure are the so-
called ancestor indices a(i)
t
for each particle i, which can be interpreted as the parent index
of particle i at time t. For each i = 1, . . ., N , we sample the ancestor index from the
multinomial distribution with
P
f
a(i)
t
= j
g
= w(j)
t−1,
j = 1, . . ., N .
The resampling step is carried out by a call to the function sample by:
< resampleParticles > =
nIdx <- sample(nPart, nPart, replace = TRUE, prob = w[, tt-1])
where the resulting ancestor indices a(1:N )
t
are stored in nIdx.
< propagateParticles > In this step, we simulate the particle system one step forward
in time. This is done to generate a number of hypotheses of the current state of the system,
which we then can compare to the observation. The simulation step amounts to sampling
from some particle proposal distribution where we make use of the previous state x a(i)
t
t−1 and
the current measurement yt to propose new states x(i)
t
at time t by
x(i)
t | x a(i)
t
t−1 ∼pθ

x(i)
t | x a(i)
t
t−1, yt

.
(13)
In our implementation, we make use of optimal proposal for the lgss model given by
popt
θ

x(i)
t | x a(i)
t
t−1, yt

∝gθ

yt | xt

fθ

xt | x a(i)
t
t−1

∝N

x(i)
t ; σ2f
σ−2
e yt + σ−2
v φx a(i)
t
t−1
g
, σ2
,
(14)
with σ−2 = σ−2
v + σ−2
e
which minimises the variance of the incremental particle weights
at the current time step4. The expression follows from the model and the properties of the
Gaussian distribution. The propagation step is carried out by:
< propagateParticles > =
Delta
<- ( par[2]^(-2) + par[3]^(-2) )^(-1)
mup
<- par[3]^(-2) * y[tt] + par[2]^(-2) * par[1] * p[nIdx, tt-1]
p[, tt] <- Delta * mup + rnorm(nPart, 0, sqrt(Delta))
From (14), we have that the ratio between the noise variances determine the shape of the
proposal. Essentially, we have two diﬀerent extreme situations (a) σ2
e/σ2
v ≪1 and (b)
σ2
e/σ2
v ≫1. In the ﬁrst situation (a), the location of the proposal is essentially governed
by yt and the scale is mainly determined by σe. This corresponds to essentially simulating
particles from gθ(yt | xt). When σe is small this usually allows for running the particle
ﬁlter using only a small number of particles. In the second situation (b), we essentially
simulate from fθ(xt | xt−1) and do not take the observation into account. In summary, the
4However, it is unclear exactly how this inﬂuences the entire particle system, i.e., if this is the globally optimal
choice.

130
Paper A
Getting started with PMH for inference in non-linear models
performance and characteristics of the optimal proposal is therefore related to the noise
variances of the model and their relative sizes.
< weightParticles > In this step, we compute the weights required for the resampling
step. We make use of the optimal weighting function for the lgss model given by
v(i)
t
≜p(yt+1 | xt) =

gθ

yt+1 | xt+1

fθ

xt+1 | x(i)
t

dxt+1 = N

yt+1; φx(i)
t , σ2
v + σ2
e

,
which follows from the properties of the Gaussian distribution. Hence in this implementa-
tion, we make use of the new observation yt+1 between the propagation and the weighting
steps for the ﬁrst time. In many situations, the resulting weights are small and it is therefore
beneﬁcial to work with shifted log-weights to avoid problems with numerical precision.
This is done by applying the transformation
Hv(i)
t
= log v(i)
t
−vmax,
for i = 1, . . ., N,
where vmax denotes the largest element in log v(1:N )
t
. Then we normalise the weights (so
that they sum to one) by
w(i)
t
=
exp(−vmax) exp

log v(i)
t

exp(−vmax) PN
j=1 exp

log v(j)
t
 =
exp

Hv(i)
t

PN
j=1 exp

Hv(j)
t
 ,
(15)
where the shifts −vmax cancel and does not aﬀect the relative size of the weight. The com-
putation of the weights is carried out by:
< weightParticles > =
v[, tt] <- dnorm(y[tt+1], par[0] * p[, tt], sqrt(par[2]^2 + par[3]^2), log = TRUE)
vmax
<- max(v[, tt])
v[, tt] <- exp(v[, tt] - wmax)
w[, tt] <- v[, tt] / sum(v[, tt])
We remind the reader that we compare y[tt+1] and p[, tt] due to the convention for
indexing arrays in R, which corresponds to yt and x(1:N )
t−1
in the model, respectively. Note
also that the weights depends on the next observation and this is the reason for why the
for-loop runs over tt= 1, . . .,T −1.
< estimateFilteredStateAndLogLikelihood > In this step, we estimate the mean of
the ﬁltered state distribution and the log-likelihood at time t. The former can be expressed
as DπN
t [xt], i.e., by using ϕ(xt) = xt in (11). This results in the estimator5
DxN
t|t = 1
N
N
X
i=1
x(i)
t ,
(16)
5Note that this corresponds to that all weights w(i)
t
are equal with value 1/N . However, this is not the case as
we can see from (15). The reason for the slight reformulation of the general estimator (11) is that we make use of
the optimal propagation and the optimal weighting steps. This results in a speciﬁc implementation of the particle
ﬁlter known as the fully-adapted particle ﬁlter (fapf; Pitt and Shephard, 1999). In a fapf, we make use of separate
weights for resampling and for constructing the empirical approximation of πt(xt). However, we refrain from
using this more general formulation to keep the presentation simple.

4
Estimating the parameters in a linear Gaussian SSM
131
of the ﬁltered state at time t. For the latter, we make use of the logarithm of the decompo-
sition in (3) to obtain
log DpN (y1:T ) = log DpN (y1) +
TX
t=2
log DpN (yt |y1:t−1) =
T −1
X
t=1
("
log
N
X
i=1
v(i)
t
#
−log N
)
, (17)
where the second equality follows from making use of the particle system to approximate
the predictive likelihood (4). That is, making use of the empirical approximation as
DpN (yt |y1:t−1) =

X 2
gθ(yt | xt) fθ(xt | xt−1)DπN
t−1(dxt−1) dxt,
in a similar manner to (11), see Dahlin (2014) for more information. We carry out (16) and
(17) by:
< estimateFilteredStateAndLogLikelihood > =
xhatf[tt] <- mean(p[, tt])
ll
<- ll + vmax + log(sum(v[, tt])) - log(nPart)
< returnEstimates > The outputs from sm are xh (ﬁltered state estimates) and ll (esti-
mate of the log-likelihood). This operation is carried by:
< returnEstimates > =
output <- list(xh = xhatf, ll = ll)
Numerical illustration of state inference
In this section, we make use of the particle ﬁlter to estimate the ﬁltered state and to investi-
gate the properties of this estimate for ﬁnite N . The complete implementation and code is
available in the function example1_lgss. We simulate a single realisation from (12) with
T = 250 observations using the parameters θ = {φ, σv, σe} = {0.75, 1.0, 0.1}. We present
the resulting observations y1:T in upper part of Figure 4.
We generate data using the function generateData with the same inputs as for the particle
ﬁlter. Running this function returns x (the latent states x0:T ) and y (the observations y1:T ).
The particle ﬁlter can then be executed by a call to the function sm with the generated data
as an input. This entire operation is carried out by:
set.seed(10)
library("pmhtutorial")
d
<- generateData(par=c(0.75, 1.0, 0.10), T=250, x0=0.0)
outPF <- sm(d$y, par=c(0.75, 1.0, 0.10), nPart=100, T=250, x0=0.0)
In the middle of Figure 4, we present the diﬀerence between the optimal state estimate from
the Kalman ﬁlter and the estimate from the particle ﬁlter using N = 20 particles. Two
alternative measures of accuracy are the bias (absolute error) and the mean square error
( mse) of the state estimate. These are computed according to
Bias DxN
t|t
 = 1
T
TX
t=1
DxN
t|t −Dxt|t
,
MSE DxN
t|t
 = 1
T
TX
t=1
 DxN
t|t −Dxt|t
2,

132
Paper A
Getting started with PMH for inference in non-linear models
0
50
100
150
200
250
-6
-4
-2
0
2
4
6
time
observation
0
50
100
150
200
250
-0.10
-0.05
0.00
0.05
0.10
time
diﬀerence in state estimate
0
200
400
600
800
1000
-7
-6
-5
-4
-3
no. particles (N)
log-bias
0
200
400
600
800
1000
-12
-11
-10
-9
-8
-7
-6
no. particles (N)
log-MSE
Figure 4. Upper and middle: A simulated set of observations (green) from the lgss model and
the error in the latent state estimate (purple) using a particle ﬁlter with N = 20. Lower: the
estimated log-bias and log-mse for the particle ﬁlter when varying the number of particles N .

4
Estimating the parameters in a linear Gaussian SSM
133
N
10
20
50
100
200
500
1000
log-bias
-3.70
-4.01
-4.51
-4.78
-5.19
-5.68
-5.94
log-MSE
-6.84
-7.73
-8.65
-9.24
-9.93
-10.96
-11.58
Table 1. The logarithm of the bias and the mse of the ﬁltered states while varying N .
where Dxt|t denotes the optimal state estimate obtained by the Kalman ﬁlter. In Table 1 and
in the lower part of Figure 4, we present the logarithm of the bias and the mse for diﬀerent
values of N . We note that the bias and the mse decrease rapidly when increasing N . Hence,
we conclude that for this model N does not need to be large for the estimate to be accurate.
Implementing particle Metropolis-Hastings
We proceed by implementing pmh for sampling from πθ(θ) with θ = φ, which we refer to
as the target distribution of the Markov chain. We ﬁx the parameters {σv, σe} to their true
values and only aim to infer the posterior of φ given the data y1:T denoted by πθ(φ). The
complete source code is available in the function pmh and its skeleton is given by:
pmh <- function(y, initPar, par, nPart, T, x0, nIter, stepSize)
{
< initialisation >
for (kk in 2:nIter) {
< proposeParameters >
< computeAcceptanceProbability >
< acceptRejectStep >
}
output <- th
}
The function pmh has inputs: y (vector with T observations), initPar (φ0 the initial value
for φ), sigmav,sigmae (parameters {σv, σe}), nPart (no. particles), T (no. observations),
x0 (initial state), nIter (no. pmh iterations K ) and stepSize (step length in the pro-
posal). The output from pmh is θ1:K , i.e., the correlated samples approximately distributed
according to the parameter posterior πθ.
< initialisation > We allocate some variables to store the current state of the Markov
chain th, the proposed state thp, the current log-likelihood ll and the proposed likelihood
llp. Furthermore, we allocate the binary variable accept that assumes the value 1 if the
proposed parameter is accepted and 0 otherwise. Finally, we run the particle ﬁlter with the
parameters {φ0, σv, σe} to estimate the initial likelihood. The initialisation is done by:
< initialisation > =
th
<- matrix(0, nrow=nIter, ncol=1)
thp
<- matrix(0, nrow=nIter, ncol=1)
ll
<- matrix(0, nrow=nIter, ncol=1)
llp
<- matrix(0, nrow=nIter, ncol=1)
accept <- matrix(0, nrow=nIter, ncol=1)
th[1]
<- initPar
ll[1]
<- sm(y, c(th[1], par[2:3]), nPart, T, xo)$ll

134
Paper A
Getting started with PMH for inference in non-linear models
< proposeParameters > As discussed in Section 3, we are required to select a parameter
proposal distribution to implement the pmh algorithm. A standard choice is a Gaussian
random walk given by
q θ′|θk−1
 = N θ′; θk−1, ϵ 2,
(18)
where ϵ > 0 denotes the step length of the random walk, i.e., the standard deviation of the
increment. The proposal step is carried out by:
< proposeParameters > =
thp[kk] <- th[kk-1] + stepSize * rnorm(1)
< computeAcceptanceProbability > The acceptance probability (5) can be simpliﬁed
as (18) is symmetric, i.e.,
q θ′|θk−1
 = q θk−1 |θ′.
Finally, we have to make an assumption of the prior distribution of φ, which we select as a
(unnormalised) truncated Gaussian prior deﬁned by
T N [a,b](z; µ, σ2) = I[a < z < b] N(z; µ, σ2),
where I(s) denotes the indicator function that assumes the value one if s is true and zero
otherwise. Hence, T N [a,b](z; µ, σ2) denotes a distribution that is Gaussian with mean µ,
standard deviation σ in the interval z ∈[a, b] and zero otherwise.
For the lgss model, we make use of p(φ) = T N (−1,1)(φ; 0, 0.5) to ensure that the system
is stable (i.e., that the value of xt is bounded for every t ). This is the reason for why we
need to check that |φ′| < 1 before running the particle ﬁlter to avoid numerical problems.
We set the acceptance probability to zero if this stability requirement is not fulﬁlled. From
these choices of prior and proposal, we can rewrite (5) to obtain
α(θ′, θk−1) = 1 ∧log
" p(θ′)
p(θk−1)
#
+ log
" DpN
θ′ (y1:T )
DpN
θk−1(y1:T )
#
,
where we make use of the log-likelihood estimates to avoid numerical problems with small
values of the likelihood. The computation of the acceptance probability is carried out by:
< computeAcceptanceProbability > =
if (abs(thp[kk]) < 1.0) {
llp[kk]
<- sm(y, c(thp[kk], par[2:3]), nPart, T, x0)$ll
aprob_prior
<- dnorm(thp[kk], log = TRUE) - dnorm(th[kk-1], log = TRUE)
aprob_lldiff <- llp[kk] - ll[kk-1]
aprob
<- exp(aprob_prior + aprob_lldiff)
} else {
aprob
<- 0
}
< acceptRejectStep > Finally, we need to take a decision for accepting or rejecting the
proposed parameter. This is done by simulating a uniform random variable ω over [0, 1]
by the built-in R command runif. We accept θ′ if ω < α θ′, θk−1

by storing it and its
corresponding log-likelihood as the current state of the Markov chain. Otherwise, we keep

4
Estimating the parameters in a linear Gaussian SSM
135
the current values for the state and the log-likelihood from the previous iteration. The
accept/reject step is carried out in R by:
< acceptRejectStep > =
u <- runif(1)
if (u < aprob) {
th[kk]
<- thp[kk]
ll[kk]
<- llp[kk]
accept[kk] <- 1.0
} else {
th[kk]
<- th[kk-1]
ll[kk]
<- ll[kk-1]
accept[kk] <- 0.0
}
Numerical illustration of parameter inference
In this section, we make use of the pmh algorithm to estimate the parameter posterior
of φ given the data. The complete implementation and code is available in the function
example2_lgss. We consider the same data and settings for the particle ﬁlter as in Sec-
tion 4.2. Futhermore, we select the proposal step length ϵ = 0.10 and initialise the Markov
chain in θ0 = 0.5. The algorithm is executed for K = 5, 000 iterations and the ﬁrst
Kb = 1, 000 iterations are discarded as burn-in. That is, we only make use of the last 4, 000
samples to construct the empirical approximation of the parameter posterior distribution.
We can setup and run the algorithm by:
set.seed(10)
library("pmhtutorial")
d
<- generateData(par=c(0.75, 1.0, 0.10), T=250, x0=0.0)
res <- pmh(d$y, initPar=0.50, par, nPart=100, T=250, x0 = 0.0,
nIter=5000, stepSize=0.10)
In Figure 5, we present three runs of the pmh algorithm using diﬀerent step lengths ϵ = 0.01
(left), 0.10 (center) and 0.50 (right). The resulting posterior estimate (upper) are presented
as a histogram and a kernel density estimate (solid line). We also plot the state of the Markov
chain at each iteration (middle) and the resulting estimate of the acf (lower). The dotted
lines indicate the estimate of the posterior mean Dφ = 0.66 computed by:
mean(res[nBurnIn:nIter])
From the acf, we see that the choice of ϵ inﬂuences the correlation in the Markov chain
and thereby the variance in the estimates. The tuning of the proposal step length is therefore
important to obtain an eﬃcient exploration of the parameter posterior. We return to this
problem in Section 6.3.
We note that the parameter estimate diﬀers slightly from the true value 0.75 and that the
uncertainty is rather large in the estimate of the parameter posterior. This is due to the
relatively small sample size T (and a ﬁnite K ). From the asymptotic theory of the Bayesian
estimator, we know that the posterior mass tends to concentrate around the true parameter
as T (and K ) increase.

136
Paper A
Getting started with PMH for inference in non-linear models
φ
posterior estimate
0.50
0.60
0.70
0.80
0
2
4
6
8
10
12
φ
posterior estimate
0.50
0.60
0.70
0.80
0
2
4
6
8
10
12
φ
posterior estimate
0.50
0.60
0.70
0.80
0
2
4
6
8
10
12
1000
1200
1400
1600
1800
2000
0.50
0.55
0.60
0.65
0.70
0.75
0.80
iteration
φ
1000
1200
1400
1600
1800
2000
0.50
0.55
0.60
0.65
0.70
0.75
0.80
iteration
φ
1000
1200
1400
1600
1800
2000
0.50
0.55
0.60
0.65
0.70
0.75
0.80
iteration
φ
0
10
20
30
40
50
60
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
iteration
ACF
0
10
20
30
40
50
60
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
iteration
ACF
0
10
20
30
40
50
60
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
iteration
ACF
Figure 5. The estimate of πθ(φ) in the lgss model using the pmh algorithm using three diﬀerent
step lengths: ϵ = 0.01 (left), 0.10 (center) and 0.50 (right). Upper: the estimate of πθ presented
as a histogram and kernel density estimate (solid line). Middle: the state of the Markov chain at
1, 000 iterations after the burn-in. Lower: the estimated acf for the Markov chain. Dotted lines
in the upper and middle plots indicate the estimate of the posterior mean. The dotted lines in
the lower plot indicate the 95% conﬁdence intervals of the acf coeﬃcients.

5
Application example: volatility estimation OMXS30
137
Number of observations T
10
20
50
100
200
500
Estimated posterior mean
0.596
0.551
0.581
0.685
0.723
0.737
Estimated posterior variance
0.040
0.025
0.011
0.005
0.005
0.001
Table 2. The estimated posterior mean and variance when varying T .
We exemplify this in Table 2 by estimating the posterior mean and variance using the
same setup when T increases. This small study supports that the true parameter is indeed
recovered by the posterior mean estimate in the limit. However, the rate of this convergence
is determined by the model and therefore it is not possible to give any general guidelines
for how large T needs to be to achieve a certain accuracy.
Application example: volatility estimation OMXS30
We continue with a concrete application of the pmh algorithm to infer the parameters of
a stochastic volatility (sv; Hull and White, 1987) model. This is a non-linear ssm with
Gaussian noise and inference in this type of model is an important problem as the log-
volatility (the latent state in this model) is useful for risk management and to price various
ﬁnancial contracts. See e.g., Tsay (2005) and Hull (2009) for more information.
A particular parametrisation of the sv model is given by
x0 ∼N

x0; µ,
σ2
v
1 −φ2

,
(19a)
xt+1 | xt ∼N

xt+1; µ + φ(xt −µ), σ2
v

,
(19b)
yt | xt ∼N

yt; 0, exp(xt)

,
(19c)
where the parameters are denoted by θ = {µ, φ, σv}. Here, µ ∈R, φ ∈[−1, 1] and
σv ∈R+ denote the mean value, the persistence and standard deviation of the state process,
respectively. Note that this model is quite similar to the lgss model, but here the state xt
scales the variance of the observation noise. Hence, we have Gaussian observations with
zero mean and a state dependent standard deviation given by exp(xt/2).
In econometrics, volatility is another word for standard deviation and therefore we refer to
xt as the log-volatility. The measurements in this model yt are so-called log-returns,
yt = 100 log
" st
st−1
#
= 100 
log(st) −log(st−1),
where st denotes the price of some ﬁnancial asset (e.g., an index, stock or commodity) at
time t. Here, we consider {st }T
t=1 to be daily closing prices of the nasdaq omxs30 index,
i.e., a weighted average of the 30 most traded stocks at the Stockholm stock exchange. We
extract the data from Quandl6 for the period between January 2, 2012 and January 2, 2014.
The resulting log-returns are presented in the upper part of Figure 6. Note the varying
persistent volatility in the log-returns, i.e., periods of small and large variations. This is
6The data is available for download from: https://www.quandl.com/data/NASDAQOMX/OMXS30.

138
Paper A
Getting started with PMH for inference in non-linear models
known as the volatility clustering eﬀect and is one of the features of real-world data that sv
models aim to capture. From (19) and the example in Figure 3, we note that this can be
achieved when |φ| is close to one and when σv is small.
The objectives in this application are to estimate the parameters θ and to estimate the log-
volatility x0:T from the observed data y1:T . We can estimate both quantities using the pmh
algorithm as we obtain both samples from the posterior of the parameter and the state at
each iteration of the algorithm. To complete the sv model, we assume some priors for the
parameters based on domain knowledge of usual ranges of the parameters, i.e.,
p(µ) = N(µ; 0, 1),
p(φ) = T N [−1,1](φ; 0.95, 0.052),
p(σv) = G(σv; 2, 10).
Here, G(a, b) denotes a Gamma distribution with shape a and scale b, i.e., with expected
value a/b.
We need to adapt the code for the particle ﬁlter and for the pmh algorithm to this new
model. We outline the necessary modiﬁcations by replacing parts of the code in the skeleton
for the two algorithms. The resulting implementations and source codes are found in the
functions sm_sv and pmh_sv, respectively. In the particle ﬁlter, we need to modify all steps
except the resampling. In the initialisation, we need to simulate the initial particle system
from µθ(x0) by adding
< initialisation > +=
p[, 1]
<- rnorm(nPart, mu, par[3] / sqrt(1 - par[2]^2))
xhatf[, 1] <- mean(p[, 1])
To implement the particle ﬁlter for the sv model, we need to choose a (particle) proposal
distribution and the weighting function. However, we cannot ﬁnd the optimal choices as
for the lgss model, so instead the state dynamics is used as the proposal (13) given by
x(i)
t | x a(i)
t
t−1 ∼fθ

xt | x a(i)
t
t−1

= N

xt; µ + φ

x a(i)
t
t−1 −µ

, σ2
v

,
and the observation model as a weighing function by
W (i)
t
= gθ

yt | x(i)
t

= N

yt; 0, exp  x(i)
t

.
These two choices result in that we must change the estimator for xt|t = πt[xt] to
DxN
t|t =
N
X
i=1
w(i)
t x(i)
t ,
where the weights are normalised according to (15). We refer the interested reader to Doucet
and Johansen (2011) or to Dahlin (2014) for details. These three changes to the particle ﬁlter
are carried out by replacing:
< propagateParticles > =
p[, tt] <- par[1] + par[2] * (p[nIdx, tt-1] - par[1]) + par[3] * rnorm(nPart)
< weightParticles
> =
v[, tt]
<- dnorm(y[tt-1], 0, exp(0.5 * p[, tt]), log = TRUE)
vmax
<- max(v[, tt])

5
Application example: volatility estimation OMXS30
139
v[, tt]
<- exp(v[, tt] - wmax)
w[, tt]
<- v[,tt] / sum(v[,tt])
< estimateFilteredStateAndLogLikelihood > =
zhatf[tt] <- sum(w[, tt] * p[, tt])
ll
<- ll + wmax + log(sum(v[, tt])) - log(nPart)
We also need to generalise the pmh code to have more than one parameter. This is straight-
forward and we refer the reader to the source code for the necessary changes. The major
change is to turn th and thp into matrices instead of vectors and to modify the parameter
proposal. In this model, we make use of a multivariate Gaussian proposal centred around
the previous parameters θk−1 with covariance matrix Σ = diag(ϵ), where ϵ now denotes
a vector with three elements. The complete implementation and code is available in the
function example3_sv.
We can load the data and the settings as well as run the pmh algorithm by executing:
library("pmhtutorial")
y
<- as.numeric(read.table("omxs30data.csv")[,1 ])
res <- pmh(y, initPar=c(0, 0.9, 0.2), nPart=500, T=500, nIter=7500,
stepSize=diag(c(0.10, 0.01, 0.05)^2))
The estimate of the log-volatility is obtained by a marginalisation approach, see Andrieu
et al. (2010) for the details. In this approach, we average the state estimate over the parameter
posterior. This is done by modifying the code for the particle ﬁlter. After a run of the algo-
rithm, we sample a single trajectory by sampling a particle at time T with probability given
by w(1:N )
T
. We then follow the ancestor lineage back to t = 0 and extract the corresponding
path in the state space.
To implement this, we introduce the ancestor index variable explicitly in the code and
resample the ancestry during each iteration. This operation is carried out by changing the
code for the particle ﬁlter in the function sm_sv by adding:
< initialisation > +=
a <- matrix(0, nrow=nPart, ncol=T+1);
< resampleParticles
> +=
a[, 1:tt-1]
<- a[idx, 1:tt-1]
a[, tt]
<- idx
We can then sample the state trajectory by replacing:
< returnEstimates > =
nIdx
<- sample(nPart, 1, prob=w[, T])
xhatf
<- p[ cbind(a[nIdx,], 1:(T+1)) ]
output <- list(xh = xhatf, ll = ll)
in the function sm_sv.
The sampled state trajectory in xhatf is then treated in the same manner as the candi-
date parameter in the pmh algorithm. Hence, we can compute the posterior mean of the
parameters and the log-volatility and its corresponding standard deviation by:

140
Paper A
Getting started with PMH for inference in non-linear models
0
100
200
300
400
500
-4
-2
0
2
4
time
log-returns
0
100
200
300
400
500
-2
-1
0
1
2
time
log-volatility estimate
µ
posterior estimate
-1.0
-0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2500
3000
3500
4000
-1.0
-0.5
0.0
0.5
1.0
iteration
µ
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of µ
φ
posterior estimate
0.88
0.92
0.96
1.00
0
5
10
20
30
2500
3000
3500
4000
0.90
0.95
1.00
1.05
1.10
iteration
φ
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of φ
σv
posterior estimate
0.0
0.1
0.2
0.3
0.4
0
2
4
6
8
10
2500
3000
3500
4000
0.0
0.1
0.2
0.3
0.4
iteration
σv
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of σv
Figure 6. Upper: the daily log-returns (dark green) and estimated log-volatility (orange) with
95% conﬁdence intervals of the nasdaq omxs30 index for the period January 2, 2012 to
January 2, 2014. Lower: the posterior estimate (left), the trace of the Markov chain (middle)
and the corresponding acf (right) for µ (purple), φ (magenta) and σv (green) obtained from
pmh. Dotted and gray lines in the left and middle plots indicate the parameter posterior mean
and the parameter priors, respectively.

6
Improving the PMH algorithm
141
thhat
<- colMeans(res$thhat[ 2500:7500,])
thhatSD <- apply(res$thhat[2500:7500,], 2, sd)
xhat
<- colMeans(res$xhat[ 2500:7500,])
xhatSD
<- apply(res$xhat[2500:7500,], 2, sd)
The resulting posterior estimates, traces and acfs are presented in Figure 6. We see that
the chain and posterior are clearly concentrated around the posterior mean estimate Dθ =
{−0.10, 0.97, 0.15} with standard deviations {0.27, 0.02, 0.05}. This conﬁrms our belief that
the log-volatility is a slowly varying process as φ is close to one and σv is small.
We also compute the estimate of the log-volatility given this parameter and present it in the
second row of Figure 6. We note that the volatility is larger around t = 100 and t = 370,
which corresponds well to what is seen in the log-returns.
Improving the PMH algorithm
In this section, we outline some possible improvements and best practices for the pmh
algorithm applied to estimate the sv model in Section 5. The material in this section is
more advanced than in the previous and can be omitted during a ﬁrst reading. We discuss
initialisation, convergence diagnostics and how to improve the so-called mixing of the
constructed Markov chain. For the latter, we consider two diﬀerent approaches: tuning the
random walk proposal and reparameterising the model.
Initialisation
It is important to initialise the Markov chain in areas of high posterior probability mass
to obtain an eﬃcient algorithm. In theory, we can initialise the chain anywhere in the
parameter space and still obtain a convergent Markov chain. However, in practice we can
experience numerical diﬃculties when the parameter posterior assumes small values or is rel-
atively insensitive to changes in θ. Therefore it is advisable to try to obtain an approximate
estimate of the parameters and initialise closer to the posterior mode.
In the lgss model, we can make use of a quasi-Newton optimisation algorithm (Nocedal
and Wright, 2006) in combination with a Kalman ﬁlter to estimate the mode of the proposal
and extract a Hessian estimate of the log-posterior around that mode. This information can
be used directly to tailor a Gaussian random walk proposal. We initialise the pmh algorithm
at the mode and set Σ = −D
P−1, where D
P denotes the estimate of the Hessian obtained from
the quasi-Newton optimisation.
However, this is not possible for general ssms as we can only obtain noisy estimates of the
posterior distribution by the particle ﬁlter. Therefore, quasi-Newton algorithms can get
stuck in local minima or fail to converge. This is due to the fact that the gradient estimates
are noisy and line search algorithms often fail. Good approaches for initialisations of the
pmh algorithm for non-linear ssms is an open research question and we provide references
to some attempts to solve this in Section 7.

142
Paper A
Getting started with PMH for inference in non-linear models
Diagnosing convergence
It is in general diﬃcult to prove that the Markov chain has reached its stationary regime and
that the samples obtained are actually samples from πθ(θ). A simple solution is to initialise
the algorithm at diﬀerent points in the parameter space and compare the resulting posterior
estimates. If they are approximately the same (after discarding the burn-in), we conclude
that the Markov chains have reach the stationary phase.
Another alternative is to make use of the Kolmogorov-Smirnov (ks; Massey, 1951) test to
establish that the approximation of the posterior distribution does not change after the burn-
in. This is done by dividing the samples obtained from the pmh algorithm {θk}K
k=1 into
three partitions: the burn-in and two sets of equal number of samples from the stationary
phase. We thin the Markov chain to obtain uncorrelated samples by extracting every lth
sample from the two sets of samples from the stationary phase. We then conduct a standard
ks test to compare the corresponding posterior estimates. The Markov chain is in the
stationary regime, if the test does not reject the null hypothesis that the data is identically
distributed. There also exist other methods for diagnosing convergence, see Robert and
Casella (2009) and Gelman et al. (2013).
Improving mixing
Mixing is an important concept for the pmh algorithm and is associated with the variance
of the estimates obtained from the algorithm. We can see this from the central limit theorem
( clt) for the estimator DπK
θ [ϕ] in (8) given by
√
K

πθ[ϕ] −DπK
θ [ϕ]

d
−→N

0, Vπ[ϕ] · IACT θ1:K

,
K →∞,
when Vπ[ϕ] = πθ[(ϕ −πθ[ϕ])2] < ∞. Here, IACT θ1:K

denotes the integrated autocorrela-
tion time ( iact) of the Markov chain, which is computed as the area under the acf. Note
that the clt is established under some assumptions discussed by e.g., Meyn and Tweedie
(2009) and Robert and Casella (2004).
Intuitively, we can view the iact as the price that we have to pay in variance for using
correlated samples to estimate the expectation in (8). An interpretation of the iact is that
it represents the number of iterations of the pmh algorithm between two uncorrelated
samples. Hence, an iact of one indicates that the samples are completely uncorrelated
as is the case in an importance sampler. Minimising the iact is therefore an important
objective when implementing a computationally eﬃcient algorithm. We refer to that the
chain is mixing well or mixing bad depending on the iact.
Tuning the proposal
In the middle of Figure 6, we present the acf for the three parameters in the sv model. We
can make use of this information to compute the corresponding iacts by
IACT(θ1:K ) = 1 + 2
∞
X
τ=1
ρτ
 θ1:K
,

6
Improving the PMH algorithm
143
where ρτ = E[(θk −πθ[θ])(θk+τ −πθ[θ])]/πθ[(θ −πθ[θ])2 denotes the autocorrelation
coeﬃcient at lag τ for θ1:K . In practice, we cannot compute the iact exactly as we do not
know the autocorrelation coeﬃcients for all possible lags. Also, it is diﬃcult to estimate ρτ
when τ is large and K is rather small. A possible solution to this problem is to cut oﬀthe
acf estimate at some lag and only consider lags smaller than this limit L. In this tutorial,
we make use of L = 100 lags to estimate the iact by
E
IACT(θ1:K ) = 1 + 2
100
X
τ=1
DρK
τ
 θ1:K
,
where DρK
τ = corr(ϕ(θk), ϕ(θk+τ)) denotes the estimate of the lag-τ autocorrelation of ϕ.
When we apply this estimator for the sv model in Section 5, we obtain the iact {91, 50, 42}
for each of the three parameters.
To obtain a better proposal, we can tune the proposal by the unknown covariance of
the posterior distribution as discussed in connection with the initialisation problem. An
estimate of the so-called pre-conditioning matrix P can be computed using the sample from
a pilot run of the pmh algorithm by:
resTh
<- res$thhat[nBurnIn:nIter,]
estCov <- var(resTh)
If we make use of the trace from the run in Section 5, we obtain the covariance estimate
D
P = 10−4

726
10
−11
10
3
−7
−11
−7
24

.
Using this we can form a new improved proposal by
θ′ ∼N

θ′; θk−1, ϵ 2 D
P

,
(20)
where ϵ 2 = 0.8 is selected using trial-and-error (pilot runs) to obtain an acceptance rate of
about 35%.
In Figure 7, we present the marginal posteriors (in color) for each pair of parameters in (19).
These posteriors are estimated using a 2-dimensional kernel density estimate ( kde) and
indicated by contour lines at a number of posterior levels. We also compare the original and
the new proposal (20) distributions. Both proposals are centred around θk−1 = Dθ, i.e., the
estimated posterior mean. We note that the new proposal ﬁts the posterior better as there
shapes are more similar than for the original proposal. This improvement in the matching
of the proposal to the posterior is expected to improve the mixing of the Markov chain.
In Figure 8, we present the results of the same implementation as in the previous section
with the new proposal. The complete implementation and code is available in the function
example4_sv. The resulting iact estimates (computed in analogue with the above) are
{28, 22, 25}. That is, we obtain clear improvements in the mixing of the Markov chain for
µ and σv. The consequence is that we can cut the number of iterations (and therefore
computational cost) by 91/28 = 3.25 and still obtain the same variance in the estimates.

144
Paper A
Getting started with PMH for inference in non-linear models
µ
φ
-1.0
-0.5
0.0
0.5
1.0
0.90
0.95
1.00
1.05
µ
σv
-1.0
-0.5
0.0
0.5
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
φ
σv
0.90
0.95
1.00
1.05
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
µ
φ
-1.0
-0.5
0.0
0.5
1.0
0.90
0.95
1.00
1.05
µ
σv
-1.0
-0.5
0.0
0.5
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
φ
σv
0.90
0.95
1.00
1.05
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Figure 7. The estimated marginal posterior for µ and φ (purple), µ and σv (magenta) and φ and σv (green) obtained from pmh. The dark contours
(upper/lower) indicate the original proposal from Section 5 and the new (improved) proposal estimated from the pilot run, respectively. Both proposals
are centred at the posterior mean estimate.

6
Improving the PMH algorithm
145
0
100
200
300
400
500
-4
-2
0
2
4
time
log-returns
0
100
200
300
400
500
-2
-1
0
1
2
time
log-volatility estimate
µ
posterior estimate
-1.0
-0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2.0
2500
3000
3500
4000
-1.0
-0.5
0.0
0.5
1.0
iteration
µ
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of µ
φ
posterior estimate
0.88
0.92
0.96
1.00
0
5
10
20
30
2500
3000
3500
4000
0.90
0.95
1.00
1.05
1.10
iteration
φ
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of φ
σv
posterior estimate
0.0
0.1
0.2
0.3
0.4
0
2
4
6
8
10
2500
3000
3500
4000
0.0
0.1
0.2
0.3
0.4
iteration
σv
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of σv
Figure 8. Upper: the daily log-returns (dark green) and estimated log-volatility (orange) with
95% conﬁdence intervals of the nasdaq omxs30 index for the period January 2, 2012 and
January 2, 2014. Lower: the posterior estimate (left), the trace of the Markov chain (middle)
and the corresponding acf (right) for µ (purple), φ (magenta) and σv (green) obtained from
pmh. Dotted and gray lines in the left and middle plots indicate the parameter posterior mean
and the parameter priors, respectively.

146
Paper A
Getting started with PMH for inference in non-linear models
Reparametrisating the model
Another approach to improve mixing is to reparameterise the model to obtain uncon-
strained parameters, which can assume any value on the real line. In our model, a problem
is the parameter φ, which is constrained to the region |φ| < 1 to obtain a stable ssm. This
results in poor mixing if we propose many candidate parameters such that |φ′| > 1. Be-
cause any such candidate parameter leads to a reject decision in the pmh algorithm and
this increases the autocorrelation. Also, for the same reason it is beneﬁcial to constrain the
standard deviation σv of the process noise to be positive.
To mitigate the problem with constrained parameters, we consider a reparameterisation of
the model given by
φ = tanh  ψ,
σv = exp  ς,
(21)
such that ψ, ς ∈R are unconstrained parameters. Hence, we change the target of the pmh
algorithm to the parameter posterior of ϑ = {µ, ψ, ς}. To retain a valid algorithm that still
targets the original parameter posterior, we need to compensate for this transformation in
the acceptance probability. This is done by taking the Jacobians of (21) into account, which
are given by
∂
∂ψ tanh−1(ψ) =
1
1 −ψ2,
∂
∂ς log(ς) = ς−1.
(22)
The resulting acceptance probability is calculated according to
α(ϑ′, ϑk−1) = 1 ∧
p(θ′)
p(θk−1)
DpN
ϑ′ (y1:T | u′)
DpN
ϑk−1(y1:T | uk−1)

1 −(φ′)2
1 −φ2
k−1


σ′
v
σv,k−1

,
(23)
where we make use of the original parameters to compute the prior and to estimate the
likelihood. Hence, the proposal operates on ϑ to propose a new candidate parameter ϑ′.
We then use the transformation (22) to obtain θ′, which we make use of to estimate the
likelihood and to compute the acceptance probability (23). After a run of this implementa-
tion, we recover the samples from the original posterior by transforming the trace of the
Markov chain by (22).
To implement this, we need change the call to the particle ﬁlter and the computation of the
acceptance probability. The complete implementation and code is available in the function
example5_sv. The resulting mean estimate of the parameter posterior is {−0.12, 0.96, 0.17}
with standard deviation {0.28, 0.02, 0.05} and iact {18, 28, 26}.
Outlook and conclusions
We have introduced the pmh algorithm for Bayesian parameter inference in non-linear
ssms. The particle ﬁlter plays an important role in the pmh algorithm and provides an
non-negative unbiased estimator of the likelihood. Furthermore, we have applied the pmh
algorithm for inference in an lgss model and a sv model using both synthetic and real-
world data. We have identiﬁed that initialisation and tuning of the parameter proposal are
important problems for the user. The complete code developed in this tutorial is available in
the R-package pmhtutorial and can be seen as a compilation of minimal working examples

7
Outlook and conclusions
147
of how to implement the particle ﬁlter and the pmh algorithm. Hopefully, these code
snippets can be of use for the interested reader as a starting point to develop his/her own
implementations of the algorithms.
We devote this section to give some references for important improvements and further
studies of the pmh algorithm and the particle ﬁlter. A good start for learning more about
the particle ﬁlter is the surveys by Doucet and Johansen (2011) and Cappé et al. (2007).
They discuss a more general form of the particle ﬁlter and related algorithms for solving
the ﬁltering problem in ssms. The parameter inference problem is surveyed by Kantas
et al. (2015) and Schön et al. (2015). They discuss both Bayesian and maximum likelihood
inference methods in ssms based on the particle ﬁlter and related algorithms.
Improving the particle filter
In this tutorial, we introduced two diﬀerent types of particle ﬁlters: the bootstrap particle
ﬁlter (bpf) in Section 5 and fapf in Section 4.1. For small and relatively simple models,
the bpf performs well but often performs poorly (or is computationally costly) in many
real-world problems with noisy observations, non-linear models or a large dimension of
the state vector. Some common improvements are to: (i) consider better proposals and
weighting functions, (ii) use other resampling schemes and (iii) combine the particle and
Kalman ﬁlters.
A possible improvement to bpf is the fapf introduced by Pitt and Shephard (1999) dis-
cussed in Section 4.1. As previously stated, this algorithm makes use of the observations in
the resampling and propagation steps. In the cases when fapf can be implemented, it can
result in a signiﬁcant decrease in the mse of the estimates and therefore decrease the number
of particles required. However, we need to be able to sample from p(xt+1 | xt, yt+1) and eval-
uate p(yt+1 | xt) to be able to implement fapf. This is not possible in many cases although
Gaussian approximations can be used instead, see Doucet et al. (2001) and Pitt et al. (2012).
However, these methods rely on quasi-Newton optimisation that can be computationally
prohibitive if N is large.
Another approach is to use another particle ﬁlter to approximate the fully adapted proposal,
resulting in a nested construction where one particle ﬁlter is used within another particle
ﬁlter to construct a proposal distribution for that particle ﬁlter. The resulting construction
is referred to as nested sequential Monte Carlo ( smc), see Naesseth et al. (2015) for details.
The nested smc construction makes it possible to consider state spaces of signiﬁcantly
higher state dimension compared to what the bpf can handle.
It is also possible to make use of a mixture of proposals and weighting functions in the
particle ﬁlter as discussed by Kronander and Schön (2014). This type of ﬁlters are based
on multiple importance sampling, which is commonly used in e.g., computer graphics.
Alternative resampling schemes can also be useful in decreasing the mse, see Hol et al.
(2006) and Douc and Cappé (2005) for some comparisons.
The resampling step can also be modiﬁed to give smooth (or continuous) estimates of the
likelihood as presented by Malik and Pitt (2011) and Pitt et al. (2014). This results in that
standard optimisation approaches can be used to obtain the maximum a-posteriori ( map)

148
Paper A
Getting started with PMH for inference in non-linear models
estimate. However, ﬁxing the random numbers (which is required in this type of ﬁlters)
can introduce a bias in the parameter estimates.
Another possible improvement is the combination of Kalman and particle ﬁltering, which
is possible if the model is conditionally linear and Gaussian in some of its states. The idea
is then to make use of Kalman ﬁltering to estimate these states and particle ﬁltering for the
remaining states while keeping the linear ones ﬁxed to their Kalman estimates. These types
of models are common in engineering and Rao-Blackwellisation schemes like this can lead
to a substantial decrease in variance. For more information see e.g., Doucet et al. (2000),
Chen and Liu (2000) and Schön et al. (2005).
Finally, note that particle ﬁltering is an instance of smc methods (Del Moral et al., 2006).
smc is a large class of importance sampling based algorithms that can be applied for many
interesting problems in statistics. In principle, smc can be applied to any problem that can
be solved using mcmc methods. In particular, smc can be used in a sequential or online
setting, which makes it an attractive alternative or complement to mcmc.
Improving particle Metropolis-Hastings
As previously discussed, the pmh algorithm is a member of the family of exact approxima-
tion or pseudo-marginal algorithms introduced by Andrieu and Roberts (2009). Further-
more, this framework can be used to introduce particle mcmc ( pmcmc) algorithms as
presented by Andrieu et al. (2010). These papers are useful resources for the reader that is
interested in the proofs of the validity of the pmcmc algorithms and their properties.
The particle ﬁlter plays an important role in the pmh algorithm and greatly inﬂuences the
mixing of the Markov chain. If the log-likelihood estimates are noisy (too small N in the
particle ﬁlter), the chain tends to get stuck for several iterations and this leads to bad mixing.
This is the result of that sometimes pθ′(y1:T ) ≪DpN
θ′ (y1:T ), which is due to the noise in the
estimator. Thus, balancing N and K to obtain good mixing at a reasonable computational
cost is an important problem in the phm algorithm. A small N might result in that we
need to take K large and vice verse. This problem is investigated by Pitt et al. (2012) and
Doucet et al. (2015). A simple rule-of-thumb is to select N such that the standard deviation
of the log-likelihood estimates is around one.
Another aspect of the algorithm that inﬂuences the mixing is the choice of proposal. In
this tutorial, we considered a marginal proposal, which we usually refer to as pmh0. This
proposal is studied by Sherlock et al. (2015) and the authors oﬀer a rule-of-thumb for tuning
the step length of the proposal to take into account the (unknown) posterior covariance.
However, we typically encounter problems when the number of parameters p grows over
about 5 or when the chain is initialised far from the areas of high posterior probability. In
these cases, we often experience poor mixing of the Markov chain, which requires a large
number of iterations K to produce accurate estimates.
To mitigate this problem, it can be useful to take geometrical information about the poste-
rior into account. Girolami and Calderhead (2011) show how to make use of the gradient
and the Hessian of the log-posterior to guide the Markov chain to areas of high posterior
probability. In the paper, this is motivated by diﬀusion processes on Riemann manifolds.

7
Outlook and conclusions
149
However, a perhaps simpler analogy is found in optimisation, where we can make use of
noisy gradient ascent or Newton updates in the proposal. Gradient information is useful
to guide the Markov chain towards the area of interest during the burn-in and also to keep
it in this area after the burn-in phase. The Hessian information can be used to rescale the
parameter posterior to make it more isotropic, which greatly simpliﬁes sampling.
In Dahlin et al. (2015a), the authors show how to make use of this type of proposals in
the pmh algorithm. We refer to the proposal that makes use of only gradient information
as pmh1 (for ﬁrst-order pmh). The proposal that makes use of both gradient and Hessian
information is referred to as pmh2 (for second-order). The challenge here is to obtain good
estimates of the gradient and Hessian, which both are analytically intractable for a non-
linear ssm. Furthermore, the computation of these quantities usually requires the use of
a particle smoother with a high computational cost, which is prohibitive inside the pmh
algorithm.
To mitigate this problem, they propose to make use of the faster but more inaccurate ﬁxed-
lag particle smoother and instead regularise the Hessian estimate when it is non-positive
deﬁnite. In Dahlin et al. (2015b), the authors describe a quasi-Newton pmh2 proposal
(qpmh2) based on a noisy quasi-Newton update that does not require any Hessian infor-
mation but constructs a local approximation of the Hessian based on gradient information.
pmh1 and similar algorithms are theoretically studied by Nemeth et al. (2014), which oﬀers
a rule-of-thumb to tune the step lengths based on an estimate of the posterior covariance.
Another approach to decrease the variance of the estimates from pmh is to modify the test
function. Some preliminary work by Mira et al. (2013) and Papamarkou et al. (2014) make
use of this in the case when the log-likelihood is available in closed-form. In these papers,
the authors make use of a so-called zero-variance post-processing step, which removes some
of the variance in the estimates. It is straightforward to make use of the same approach for
pmh1 and pmh2 as it only requires information about the gradient of the log-posterior.
The required information can be collected during the run of the pmh algorithm and applied
analogously to Mira et al. (2013).
Finally, we return to the problem of initialising the pmh algorithm. The main problem
is that many optimisation methods encounter problems with noisy evaluations of the pos-
terior and its gradients. To mitigate this problem, Dahlin et al. (2015c) propose a method
based on Gaussian process optimisation (gpo; Boyle, 2007, Lizotte, 2008), where we opti-
mise a smooth surrogate of the posterior. The predictive distribution of a Gaussian process
is used as the surrogate, which is constructed using the noisy estimates of the posterior
distribution obtained from a particle ﬁlter. Another alternative approach is the simultane-
ous perturbation and stochastic approximation (spsa; Spall, 1987) algorithm, which can be
applied in combination with the particle ﬁlter to estimate the posterior mode. A drawback
with spsa compared with gpo is that it does not provide an estimate of the Hessian to
help tuning the proposal.
Additional software implementations
The code developed in this tutorial is available for R via the package pmhtutorial from
cran. However, similar code for MATLAB and Python is available from GitHub at: https:

150
Paper A
Getting started with PMH for inference in non-linear models
//github.com/compops/pmh-tutorial. Make sure to read the README.md ﬁle at GitHub
for more information and to ﬁx the dependencies required for running the code.
Acknowledgements
The authors would like to thank Christian Andersson Naesseth, Wilfried Bonou, Manon
Kok, Joel Kronander, Fredrik Lindsten, Andreas Svensson and Patricio Valuenzula for com-
ments and suggestions that greatly improved this tutorial.

Bibliography
151
Bibliography
B. D. O. Anderson and J. B. Moore. Optimal ﬁltering. Courier Publications, 2005.
C. Andrieu and G. O. Roberts. The pseudo-marginal approach for eﬃcient Monte Carlo
computations. The Annals of Statistics, 37(2):697–725, 2009.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342,
2010.
P. Boyle. Gaussian processes for regression and optimisation. PhD thesis, Victoria University
of Wellington, 2007.
P. J. Brockwell and R. A. Davis. Introduction to time series and forecasting. Springer Verlag,
2002.
O. Cappé, S. J Godsill, and E. Moulines. An overview of existing methods and recent
advances in sequential Monte Carlo. Proceedings of the IEEE, 95(5):899–924, 2007.
R. Chen and J. S. Liu. Mixture Kalman ﬁlters. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 62(3):493–508, 2000.
N. Chopin, P. E. Jacob, and O. Papaspiliopoulos. SMC2: an eﬃcient algorithm for sequen-
tial analysis of state space models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 75(3):397–426, 2013.
D. Crisan and A. Doucet. A survey of convergence results on particle ﬁltering methods for
practitioners. IEEE Transactions on Signal Processing, 50(3):736–746, 2002.
J. Dahlin. Sequential Monte Carlo for inference in nonlinear state space models. Licentiate’s
thesis no. 1652, Linköping University, May 2014.
J. Dahlin and T. B. Schön. Getting started with particle Metropolis-Hastings for inference
in nonlinear models. Pre-print, 2015. arXiv:1511.01707v4.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using gradient and
Hessian information. Statistics and Computing, 25(1):81–92, 2015a.
J. Dahlin, F. Lindsten, and T. B. Schön. Quasi-Newton particle Metropolis-Hastings. In
Proceedings of the 17th IFAC Symposium on System Identiﬁcation (SYSID), pages 981–986,
Beijing, China, October 2015b.
J. Dahlin, M. Villani, and T. B. Schön. Eﬃcient approximate Bayesian inference for models
with intractable likelihoods. Pre-print, 2015c. arXiv:1506.06975v1.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.
A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 39
(1):1–38, 1977.

152
Paper A
Getting started with PMH for inference in non-linear models
R. Douc and O. Cappé.
Comparison of resampling schemes for particle ﬁltering.
In
Proceedings of the 4th International Symposium on Image and Signal Processing and Analysis
(ISPA), pages 64–69, Zagreb, Croatia, September 2005.
A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years
later. In D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering.
Oxford University Press, 2011.
A. Doucet, N. de Freitas, K. Murphy, and S. Russell. Rao-Blackwellised particle ﬁltering
for dynamic Bayesian networks. In Proceedings of the Sixteenth Conference on Uncertainty
in Artiﬁcial Intelligence, pages 176–183, Stanford, USA, July 2000.
A. Doucet, N. de Freitas, and N. Gordon.
Sequential Monte Carlo methods in practice.
Springer Verlag, 2001.
A. Doucet, M. K. Pitt, G. Deligiannidis, and R. Kohn. Eﬃcient implementation of Markov
chain Monte Carlo when using an unbiased likelihood estimator. Biometrika, 102(2):
295–313, 2015.
J. Durbin and S. J. Koopman. Time series analysis by state space methods. Oxford University
Press, 2 edition, 2012.
A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. Bayesian
data analysis. Chapman & Hall/CRC, 3 edition, 2013.
M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73
(2):1–37, 2011.
N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-
Gaussian Bayesian state estimation. IEEE Proceedings of Radar and Signal Processing, 140
(2):107–113, 1993.
J. D. Hol, T. B. Schön, and F. Gustafsson. On resampling algorithms for particle ﬁlters.
In Proceedings of the Nonlinear Statistical Signal Processing Workshop, Cambridge, UK,
September 2006.
J. Hull. Options, futures, and other derivatives. Pearson, 7 edition, 2009.
J. Hull and A. White. The pricing of options on assets with stochastic volatilities. The
Journal of Finance, 42(2):281–300, 1987.
A. Johansen. SMCTC: Sequential Monte Carlo in C++. Journal of Statistical Software, 30
(1):1–41, 2009.
N. Kantas, A. Doucet, S.S. Singh, J.M. Maciejowski, and N. Chopin. On particle methods
for parameter estimation in general state-space models. Statistical Science, 30(3):328–351,
2015.
J. Kronander and T. B. Schön. Robust auxiliary particle ﬁlters using multiple importance
sampling. In Proceedings of the 2014 IEEE Statistical Signal Processing Workshop (SSP),
Gold Coast, Australia, July 2014.

Bibliography
153
R. Langrock. Some applications of nonlinear and non-Gaussian state–space modelling by
means of hidden Markov models. Journal of Applied Statistics, 38(12):2955–2970, 2011.
D. J. Lizotte. Practical Bayesian optimization. PhD thesis, University of Alberta, 2008.
L. Ljung. System identiﬁcation: theory for the user. Prentice Hall, 1999.
S. Malik and M. K. Pitt. Particle ﬁlters for continuous likelihood evaluation and maximisa-
tion. Journal of Econometrics, 165(2):190–209, 2011.
V. K. Mansinghka, D. Selsam, and Y. N. Perov.
Venture: a higher-order probabilistic
programming platform with programmable inference. Pre-print, 2014. arXiv:1404.0099.
F. J. Massey, Jr. The Kolmogorov-Smirnov test for goodness of ﬁt. Journal of the American
Statistical Association, 46(253):68–78, 1951.
G. J. McLachlan and T. Krishnan. The EM algorithm and extensions. Wiley-Interscience, 2
edition, 2008.
S. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. Cambridge University
Press, 2009.
A. Mira, R. Solgi, and D. Imparato. Zero variance Markov chain Monte Carlo for Bayesian
estimators. Statistics and Computing, 23(5):653–662, 2013.
L. M. Murray. Bayesian state-space modelling on high-performance hardware using LibBi.
Pre-print, 2013. arXiv:1306.3277.
C. A. Naesseth, F. Lindsten, and T. B. Schön. Nested sequential Monte Carlo methods.
In Proceedings of the 32nd International Conference on Machine Learning (ICML), Lille,
France, July 2015.
C. Nemeth, C. Sherlock, and P. Fearnhead. Particle Metropolis adjusted Langevin algo-
rithms. Pre-print, 2014. arXiv:1412.7299v1.
J. Nocedal and S. Wright. Numerical optimization. Springer Verlag, 2 edition, 2006.
J. Nordh and K. Berntorp. pyParticleEst - a Python framework for particle based estimation.
Technical Report LUTFD2/TFRT–7628–SE, Department of Automatic Control, Lund,
Sweden, March 2013.
T. Papamarkou, A. Mira, and M. Girolami. Zero variance diﬀerential geometric Markov
chain Monte Carlo algorithms. Bayesian Analysis, 9(1):97–128, 2014.
M. Pharr and G. Humphreys. Physically based rendering: from theory to implementation.
Morgan Kaufmann, 2010.
M. K. Pitt and N. Shephard. Filtering via simulation: auxiliary particle ﬁlters. Journal of
the American Statistical Association, 94(446):590–599, 1999.
M. K. Pitt, R. S. Silva, P. Giordani, and R. Kohn. On some properties of Markov chain
Monte Carlo simulation methods based on the particle ﬁlter. Journal of Econometrics, 171
(2):134–151, 2012.

154
Paper A
Getting started with PMH for inference in non-linear models
M. K. Pitt, S. Malik, and A. Doucet. Simulated likelihood inference for stochastic volatility
models using continuous particle ﬁltering. Journal of Econometrics, 66(3):527–552, 2014.
R Core Team. R: A language and environment for statistical computing. R Foundation for
Statistical Computing, Vienna, Austria, 2015. URL https://www.R-project.org/.
C. P. Robert and G. Casella. Monte Carlo statistical methods. Springer Verlag, 2 edition,
2004.
C. P. Robert and G. Casella. Introducing Monte Carlo methods with R. Springer Verlag,
2009.
S. M. Ross. Simulation. Academic Press, 5 edition, 2012.
T. Schön, F. Gustafsson, and P.-J. Nordlund. Marginalized particle ﬁlters for mixed lin-
ear/nonlinear state-space models. IEEE Transactions on Signal Processing, 53(7):2279–2289,
July 2005.
T. B. Schön, F. Lindsten, J. Dahlin, J. Wågberg, C. A. Naesseth, A. Svensson, and L. Dai.
Sequential Monte Carlo methods for system identiﬁcation. In Proceedings of the 17th IFAC
Symposium on System Identiﬁcation (SYSID), pages 775–786, Beijing, China, October 2015.
C. Sherlock, A. H. Thiery, G. O. Roberts, and J. S. Rosenthal. On the eﬃcency of pseudo-
marginal random walk Metropolis algorithms. The Annals of Statistics, 43(1):238–275,
2015.
J. C. Spall. A stochastic approximation technique for generating maximum likelihood
parameter estimates. In Proceedings of the 6th American Control Conference (ACC), pages
1161–1167, Minneapolis, USA, June 1987.
A. Todeschini, F. Caron, M. Fuentes, P. Legrand, and P. Del Moral. Biips: software for
Bayesian inference with interacting particle systems. Pre-print, 2014. arXiv:1412.3779.
R. S. Tsay. Analysis of ﬁnancial time series. John Wiley & Sons, 2 edition, 2005.
F. Wood, J. W. van de Meent, and V. Mansinghka. A new approach to probabilistic program-
ming inference. In Proceedings of the 17th International conference on Artiﬁcial Intelligence
and Statistics (AISTATS), pages 1024–1032, Reykjavik, Iceland, April 2014.

Paper B
Particle Metropolis-Hastings
using gradient and Hessian information
Authors:
J. Dahlin, F. Lindsten and T. B. Schön
This paper is published by Springer:
http://dx.doi.org/10.1007/s11222-014-9510-0.
Edited version of the paper:
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using
gradient and Hessian information. Statistics and Computing, 25(1):81–92, 2015b.
Parts of the theory presented in this paper have also been presented in:
J. Dahlin, F. Lindsten, and T. B. Schön.
Second-order particle MCMC for
Bayesian parameter inference. In Proceedings of the 19th IFAC World Congress,
Cape Town, South Africa, August 2014a.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis Hastings using
Langevin dynamics. In Proceedings of the 38th International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), Vancouver, Canada, May 2013a.


Particle Metropolis-Hastings
using gradient and Hessian information
J. Dahlin⋆, F. Lindsten† and T. B. Schön†
⋆Dept. of Electrical Engineering,
Linköping University,
SE–581 83 Linköping, Sweden.
johan.dahlin@liu.se
†Dept. of Information Technology,
Uppsala University,
SE-751 05 Uppsala, Sweden.
fredrik.lindsten@it.uu.se
thomas.schon@it.uu.se
Abstract
Particle Metropolis-Hastings ( pmh) allows for Bayesian parameter inference in non-linear state space
models by combining Markov chain Monte Carlo ( mcmc) and particle ﬁltering. The latter is used to
estimate the intractable likelihood. In its original formulation, pmh makes use of a marginal mcmc
proposal for the parameters, typically a Gaussian random walk. However, this can lead to a poor
exploration of the parameter space and an ineﬃcient use of the generated particles.
We propose a number of alternative versions of pmh that incorporate gradient and Hessian informa-
tion about the posterior into the proposal. This information is more or less obtained as a by-product
of the likelihood estimation. Indeed, we show how to estimate the required information using a
ﬁxed-lag particle smoother, with a computational cost growing linearly in the number of particles.
We conclude that the proposed methods can: (i) decrease the length of the burn-in phase, (ii) increase
the mixing of the Markov chain at the stationary phase, and (iii) make the proposal distribution scale
invariant which simpliﬁes tuning.
Keywords
Sequential Monte Carlo, Particle Markov chain Monte Carlo, Manifold mala, Fixed-lag particle
smoothing, Parameter Inference.
Data and source code in Python
https://github.com/compops/pmh-stco2015
Financial support from
The projects Learning of complex dynamical systems (Contract number: 637-2014-466), Probabilistic
modeling of dynamical systems (Contract number: 621-2013-5524) and cadics, a Linnaeus Center, all
funded by the Swedish Research Council.
157

158
Paper B
PMH using gradient and Hessian information
Introduction
We are interested in Bayesian parameter inference in non-linear state space models (ssms)
of the form
xt | xt−1 ∼fθ(xt | xt−1),
yt | xt ∼gθ(yt | xt),
(1)
where the latent states and the measurements are denoted by x0:T = x0:T ≜{xt }T
t=0 and
y1:T = y1:T , respectively. Here, fθ( · ) and gθ( · ) denote the transition and observation
kernels, respectively, parametrised by the unknown static parameter vector θ ∈Θ ⊂Rd.
The initial state is distributed according to some distribution µ(x0) which, for notational
simplicity, is assumed to be independent of θ.
The aim of Bayesian parameter inference is to compute the parameter posterior distribution
p(θ |y1:T ) = pθ(y1:T )p(θ)
p(y1:T )
,
(2)
where p(θ) denotes the prior of θ and pθ(y1:T ) denotes the likelihood, which for an ssm
can be expressed as
pθ(y1:T ) = pθ(y1)
T
Y
t=2
pθ(yt |y1:t−1).
(3)
The one-step ahead predictor pθ(yt |y1:t−1), and thus also the likelihood function, is in
general not analytically tractable. However, unbiased estimators of the likelihood can be
constructed using sequential Monte Carlo ( smc) (Doucet and Johansen, 2011; Del Moral,
2004) and these can be used as plug-in estimators. This is especially useful in the Metropolis-
Hastings ( mh) algorithm that can be used for estimating the parameter posterior in (2).
This combination of mh and smc is known as the particle Metropolis-Hastings (pmh;
Andrieu et al., 2010) algorithm. The mh acceptance probability depends on the intractable
likelihood, which in pmh is estimated using smc (see Section 2). Despite the apparent
approximation, this results in an algorithm that targets the correct posterior distribution
(Andrieu et al., 2010). The original pmh algorithm makes use of a marginal proposal for θ,
i.e., only the current parameter is used when proposing a new parameter. The theoretical
properties of the marginal pmh algorithm have been analysed in Andrieu and Vihola (2015);
Pitt et al. (2012); Doucet et al. (2015) and it has been applied for a number of interesting
applications in, e.g., economics, social network analysis and ecology (Flury and Shephard,
2011; Everitt, 2012; Golightly and Wilkinson, 2011).
In this paper, we show that information such as the gradient and the Hessian about the pos-
terior can be included in the construction of the pmh proposal. This idea is ﬁrst suggested
by Doucet et al. (2011) in the discussions following Girolami and Calderhead (2011). In two
previous proceedings, we have applied and extended this idea with gradient information
(Dahlin et al., 2013) and also using Hessian information (Dahlin et al., 2014). The present
article builds upon and extends this preliminary work. A pmh method using gradient infor-
mation similar to Dahlin et al. (2013) has recently been proposed by Nemeth and Fearnhead
(2014) and Nemeth et al. (2014).

2
Particle Metropolis-Hastings
159
In the context of mh sampling, it has been recognised that the gradient and Hessian can
be used to construct eﬃcient proposal distributions. In the Metropolis adjusted Langevin
algorithm (mala; Roberts and Stramer, 2003), a drift term is added to the proposal in
the direction of the gradient, which intuitively guides the Markov chain to regions of high
posterior probability. In the manifold mala (mmala; Girolami and Calderhead, 2011), the
Hessian (or some other appropriate metric tensor) is also included to scale the proposal to
take the curvature of the log-posterior into account.
Drawing parallels with the optimisation literature, mmala shares some properties with
Newton-type optimisation algorithms (where mala is more similar to a steepest ascent
method). In particular, scaling the proposal with the Hessian can considerably simplify the
tedious tuning of the method since it removes the need for running pilot runs, which are
commonly used to tune the covariance matrices of the random walk mh and the mala.
In our problem, i.e., for inference in a non-linear ssm (1), the gradient and Hessian can-
not be computed analytically. However, in analogue with the intractable likelihood, these
quantities can be estimated using smc algorithms, see e.g., Poyiadjis et al. (2011); Doucet
et al. (2013). This provides us with the tools necessary to construct pmh algorithms in the
ﬂavour of the mala and the mmala, resulting in the two methods proposed in this paper,
pmh1 and pmh2, respectively.
In particular, we make use of a ﬁxed-lag ( fl) particle smoother (Kitagawa and Sato, 2001)
to estimate the gradient and Hessian. The motivation for this is that this smoother only
makes use of the weighted particles computed by the particle ﬁlter. Consequently, we obtain
this information as a by-product of the likelihood computation in the pmh algorithm. This
results in only a small computational overhead for the proposed methods when compared
to the marginal method.
Finally, we provide numerical experiments to illustrate the beneﬁts of using the gradient and
Hessian and the accuracy of the fl smoother. We demonstrate some interesting properties
of the proposed algorithms, in particular that they enjoy (i) a shorter burn-in compared
with the marginal algorithm,(ii) a better mixing of the Markov chain in the stationary phase,
and (iii) a simpliﬁed tuning of the step length(s), especially when the target distribution is
non-isotropic, i.e., when the variances diﬀers between dimensions.
Particle Metropolis-Hastings
In this section, we review the pmh algorithm and show how the random variables used to
compute the likelihood estimator can be incorporated in the proposal construction. We
also outline how this can be used to construct the proposed pmh1 and pmh2 algorithms.
MH sampling with unbiased likelihoods
The mh algorithm (see, e.g., Robert and Casella (2004)) is a member of the mcmc family
for sampling from a target distribution π(θ) by simulating a carefully constructed Markov
chain on Θ. The chain is constructed in such a way that it admits the target as its unique
stationary distribution.

160
Paper B
PMH using gradient and Hessian information
The algorithm consists of two steps during iteration k: (i) a new parameter θ′ is sampled
from a proposal distribution q(θ′|θk−1) given the current state θk−1 and (ii) the current
parameter is changed to θ′ with probability α(θ′, θk−1), otherwise the chain remains at the
current state. The acceptance probability is given by
α(θ′, θk−1) = 1 ∧
π(θ′)
π(θk−1)
q(θk−1 |θ′)
q(θ′|θk−1),
(4)
where we use the notation a ∧b ≜min{a, b}.
In this paper, we have the parameter posterior distribution (2) as the target distribution, i.e.,
π(θ) = p(θ |y1:T ). This implies that the acceptance probability (4) will depend explicitly
on the intractable likelihood pθ(y1:T ), preventing direct application of the mh algorithm
to this problem. However, this diﬃculty can be circumvented by using a pseudo-marginal
approach (Beaumont, 2003; Andrieu and Roberts, 2009).
Assume that there exists an unbiased, non-negative estimator of the likelihood Dpθ(y1:T | u).
We introduce explicitly the random variable u ∈U used to construct this estimator, and
we let mθ(u) denote the probability density of u on U. The pseudo-marginal method is
then a standard mh algorithm operating in a non-standard extended space Θ × U, with the
extended target
π(θ, u |y1:T ) = Dpθ(y1:T | u)mθ(u)p(θ)
p(y1:T )
= Dpθ(y1:T | u)mθ(u)p(θ |y1:T )
pθ(y1:T )
,
and proposal distribution mθ′(u′)q(θ′|θk−1). Since the likelihood estimator is unbiased,
Eu|θ[Dpθ(y1:T | u)] = pθ(y1:T ), it follows that the extended target admits p(θ |y1:T ) as a
marginal. Hence, by simulating from the extended target π(θ, u |y1:T ) we also obtain sam-
ples from the original target distribution p(θ |y1:T ).
If the likelihood is estimated by using smc (see Section 3) we obtain the pmh algorithm.
The random variable u then corresponds to all the weighted particles generated by the
smc algorithm. However, these random variables carry useful information, not only about
the likelihood, but also about the geometry of the posterior distribution. We suggest to
incorporate this information into the proposal construction. With (θk−1, uk−1) being the
current state of the Markov chain we simulate θ′ ∼q( · |θk−1, uk−1) and u′ ∼mθ′( · ), using
some proposal q (see Section 2.2).
It follows that the mh acceptance probability for the extended target is given by
α(θ′, u′, θk−1, uk−1) = 1 ∧
Dpθ′(y1:T | u′)mθ′(u′)p(θ′)
Dpθk−1(y1:T | uk−1)mθk−1(uk−1)p(θk−1)
mθk−1(uk−1)q(θk−1 |θ′, u′)
mθ′(u′)q(θ′|θk−1, uk−1)
= 1 ∧
Dpθ′(y1:T | u′)p(θ′)
Dpθk−1(y1:T | uk−1)p(θk−1)
q(θk−1 |θ′, u′)
q(θ′|θk−1, uk−1).
(5)
Note that q(θ′|θk−1, uk−1) may depend on the auxiliary variable uk−1 in a (formally) ar-
bitrary way. In particular, in Section 3 we propose a construction making use of biased
estimates of the gradient and Hessian of the log-posterior. Nevertheless, expression (5) still

2
Particle Metropolis-Hastings
161
deﬁnes a correct mh acceptance probability for the extended target, ensuring the validity
of our approach. Note also that the aforementioned proposal construction opens up for a
wide range of adapted proposals, possibly diﬀerent from the ones considered here.
Constructing PMH1 and PMH2
We now turn to the construction of a proposal that makes use of the gradient and Hes-
sian of the log-posterior. Following Robert and Casella (2004), we do this by a Laplace
approximation of the parameter posterior around the current state θk−1. Hence, consider a
second-order Taylor expansion of log p(θ′|y1:T ) at θk−1:
log p(θ′|y1:T ) ≈log p(θk−1 |y1:T )
+ (θ′ −θk−1)⊤f
∇log p(θ |y1:T )
g
θ=θk−1
+ 1
2(θ′ −θk−1)⊤f
∇2 log p(θ |y1:T )
g
θ=θk−1(θ′ −θk−1).
Taking the exponential of both sides and completing the square, we obtain
p(θ′|y1:T ) ≈N

θ′; θk−1 + H−1
T (θk−1)GT (θk−1), H−1
T (θk−1)

,
where we have introduced the notation GT (θk−1) = ∇log p(θ |y1:T )|θ=θk−1 for the gradient
of the log-posterior and HT (θk−1) = −∇2 log p(θ |y1:T )|θ=θk−1 for the negative Hessian of
the log-posterior. Here, we assume for now that the negative Hessian is positive deﬁnite;
see Section 3.5 for further discussion on this matter.
As pointed out above, these quantities cannot be computed in closed form, but they can
be estimated from the random variable uk−1 (see Section 3). This suggests three diﬀerent
versions of the pmh algorithm, each resulting from a speciﬁc choice of the proposal:
q(θ′|θk−1) =

N (θk−1, Σ),
[pmh0]
N

θk−1 + 1
2ΣDG(θk−1), Σ

,
[pmh1]
N

θk−1 + 1
2Σ D
H−1(θ′)DG(θk−1), 1
2Σ D
H−1(θ′)

,
[pmh2]
(6)
where we omit the dependence on u for brevity. Here, Σ denotes a scaling matrix that
controls the step lengths of the proposal. For pmh0 and pmh1, Σ can be chosen as the
inverse of an estimate of the posterior covariance matrix. However, computing this estimate
typically requires costly and tedious trial runs. For pmh2, the curvature of the problem is
captured by the Hessian matrix, i.e., a single step length can by used which can signiﬁcantly
simplify the tuning. It is also possible to choose diﬀerent step lengths for the drift term and
for the covariance matrix of the proposal.
The ﬁnal pmh2 algorithm is presented in Algorithm 1. It makes use of Algorithm 2, de-
scribed in Section 3, to estimate the quantities needed for computing the proposal and the
acceptance probability. Clearly, pmh0 and pmh1 are special cases obtained by using the
corresponding proposal from (6) in the algorithm. Note that, while the algorithm make
explicit reference to the auxiliary variable u, it only depends on this variable through the
estimates Dpθk−1(y1:T ), DG(θk−1) and D
H(θk−1).

162
Paper B
PMH using gradient and Hessian information
Algorithm 1 Second-order particle Metropolis-Hastings
Inputs: Algorithm 2. K > 0 (no. mcmc steps), θ0 (initial parameters), ϵ (step length).
Output: θ = {θ1, . . ., θK } (samples from the posterior).
1: Run Algorithm 2 to obtain Dpθ0(y1:T ), DG(θ0) and D
H(θ0).
2: for k = 1 to K do
3:
Sample θ′ ∼q(θ′|θk−1, uk−1) by (6), DG(θk−1) and D
H(θk−1).
4:
Run Algorithm 2 to obtain Dpθ′(y1:T ), DG(θ′) and D
H(θ′).
5:
Sample ωk uniformly over [0, 1].
6:
if ωk < α(θ′, u′, θk−1, uk−1) given by (5) then
7:
θk ←θ′. {Accept the parameter}
8:
{Dpθk(y1:T ), DG(θk), D
H(θk)} ←{Dpθ′(y1:T ), DG(θ′), D
H(θ′)}.
9:
else
10:
θk ←θk−1. {Reject the parameter}
11:
{Dpθk(y1:T ), DG(θk), D
H(θk)} ←{Dpθk−1(y1:T ), DG(θk−1), D
H(θk−1)}.
12:
end if
13: end for
Properties of the PMH1 and PMH2 proposals
In the sequel, we use a single step size Σ = ϵ 2Id for all the parameters in the (standard)
proposal. This is done to illustrate the advantage of adding the Hessian information, which
rescales the step lengths according to the local curvature. Hence, it allows for taking larger
steps when the curvature is small and vice verse.
This property of pmh2 makes the algorithm scale-free in the same manner as a Newton
algorithm in optimisation (Nocedal and Wright, 2006, Chapter 3). That is, the proposal is
invariant to aﬃne transformations of the parameters. Note that, since the local information
is used, this is diﬀerent from scaling the proposal in pmh0 with the posterior covariance
matrix estimated from a pilot run, as this only takes the geometry at the mode of the
posterior into account.
Some analyses of the statistical properties are available for pmh0 (Sherlock et al., 2015),
mh using a random walk (Roberts et al., 1997) and mala (Roberts and Rosenthal, 1998).
It is known from these analyses that adding the gradient into the proposal can increase the
mixing of the Markov chain. Note that these results are obtained under somewhat strict
assumptions. Also, we know from numerical experiments (Girolami and Calderhead, 2011)
that there are further beneﬁts of also taking the local curvature into account.
Estimation of the likelihood, gradient, and Hessian
In this section, we show how to estimate the likelihood together with the gradient and
Hessian using smc methods.

3
Estimation of the likelihood, gradient, and Hessian
163
Auxiliary particle filter
An auxiliary particle ﬁlter ( apf) (Pitt and Shephard, 1999) can be used to approximate
the sequence of joint smoothing distributions (jsds) pθ(x1:t |y1:t) for t = 1 to T . The
apf makes use of a particle system consisting of N weighted particles {x(i)
1:t, w(i)
t }N
i=1 to
approximate the jsd at time t by
Dpθ( dx1:t |y1:t) ≜
N
X
i=1
w(i)
t
PN
k=1 w(k)
t
δx(i)
1:t (dx1:t).
(7)
Here, δz(dx1:t) denotes the Dirac measure placed at z. The particle system is propagated
from t −1 to t by ﬁrst sampling an ancestor index a(i)
t , with
P(a(i)
t
= j) = ν(j)
t−1

N
X
k=1
ν(k)
t−1

−1
,
i, j = 1, . . ., N,
(8)
where ν(i)
t−1 denotes the resampling weights. Given the ancestor index, a new particle is
sampled according to
x(i)
t
∼Rθ

xt | x a(i)
t
1:t−1, yt

,
i = 1, . . ., N .
(9)
Finally, we append the obtained sample to the trajectory by x(i)
1:t = {x a(i)
t
1:t−1, x(i)
t } and com-
pute a new importance weight by
w(i)
t
≜
w a(i)
t
t−1
ν a(i)
t
t−1
gθ

yt | x(i)
t

fθ

x(i)
t | x a(i)
t
t−1

Rθ

x(i)
t | x a(i)
t
1:t−1, yt

,
i = 1, . . ., N .
(10)
Hence, the empirical approximations of the smoothing distributions (7) can be computed
sequentially for t = 1 to T by repeating (8)–(10).
Note that the random variables u appearing in the extended target of the pmh algorithm
correspond to all the random variables generated by the apf, i.e., all the particles and
ancestor indices,
u =
(
x(i)
t , a(i)
t
)N
i=1, t = 1, . . ., T

.
In this article, we make use of two important special cases of the apf: the bootstrap particle
ﬁlter (bpf; Gordon et al., 1993) and the fully adapted particle ﬁlter (fapf; Pitt and Shephard,
1999). For the bpf, we select the proposal kernel Rθ(xt | x1:t−1, yt) = fθ(xt | xt−1) and the
auxiliary weights νt = wt = gθ(yt | xt). The fapf is obtained by Rθ(xt | x1:t−1, yt) =
pθ(xt |yt, xt−1) and νt = pθ(yt+1 | xt), resulting in the weights wt ≡1. Note, that the fapf
can only be used in models for which these quantities are available in closed-form.

164
Paper B
PMH using gradient and Hessian information
Estimation of the likelihood
The likelihood for the ssm in (1) can be estimated using (3) by inserting estimated one-step
predictors pθ(yt |y1:t−1) obtained from the apf. The resulting estimator is given by
Dpθ(y1:T | u) =
1
N T
N
X
i=1
w(i)
T

T −1
Y
t=1
N
X
i=1
ν(i)
t

.
(11)
It is known that this likelihood estimator is unbiased for any number of particles, see e.g.,
(Pitt et al., 2012) and Proposition 7.4.1 in (Del Moral, 2004). As discussed in Section 2.1, this
is exactly the property that is needed in order to obtain p(θ |y1:T ) as the unique stationary
distribution for the Markov chain generated by the pmh algorithm.
Consequently, pmh will target the correct distribution for any number of particles N ≥1.
However, the variance in the likelihood estimate is connected with the acceptance rate and
the mixing of the Markov chain. Therefore it is important to determine the number of
particles that balances a reasonable acceptance rate with a reasonable computational cost.
This problem is studied for pmh0 in Pitt et al. (2012) and Doucet et al. (2015).
Estimation of the gradient
As we shall see below, the gradient of the log-posterior can be estimated by solving a smooth-
ing problem. The apf can be used directly to address this problem, since the particles
{x(i)
1:T, w(i)
T }N
i=1 provide an approximation of the jsd at time T according to (7) (see also
Poyiadjis et al. (2011)). However, this method can give estimates with high variance due to
particle degeneracy.
Instead, we make use of the fl smoother (Kitagawa and Sato, 2001) which has the same
linear computational cost, but smaller problems with particle degeneracy than the apf.
Alternative algorithms for estimating this information are also available (Del Moral et al.,
2010; Poyiadjis et al., 2011).
The gradient of the parameter log-posterior is given by
GT (θ) = ∇log p(θ) + ∇log pθ(y1:T ),
(12)
where it is assumed that the gradient of the log-prior ∇log p(θ) can be calculated explicitly.
The gradient of the log-likelihood ∇log pθ(y1:T ) can, using Fisher’s identity (Cappé et al.,
2005), be expressed as
∇log pθ(y1:T ) = Eθ
∇log pθ(x0:T, y1:T )|y1:T
,
(13)
where for an ssm (1) we can write the gradient of the complete data log-likelihood as
∇log pθ(x0:T, y1:T ) =
TX
t=1
ξθ(xt, xt−1), where
(14)
ξθ(xt, xt−1) = ∇log fθ(xt | xt−1) + ∇log gθ(yt | xt).

3
Estimation of the likelihood, gradient, and Hessian
165
Combining (14) with Fisher’s identity (13) yields
∇log pθ(y1:T ) =
TX
t=1

ξθ(xt, xt−1)pθ(xt−1:t |y1:T ) dxt−1:t,
which depends on the (intractable) two-step smoothing distribution pθ(xt−1:t |y1:T ). To
approximate this quantity we use the fl smoother which relies on the assumption that
there is a decaying inﬂuence of future observations yt+∆:T on the state xt. This means that
pθ(xt−1:t |y1:T ) ≈pθ(xt−1:t |y1:κt ),
holds for some large enough κt = min{t + ∆,T }. Here, ∆denotes a pre-determined lag
decided by the user, which depends on the forgetting properties of the model.
By marginalisation of the empirical smoothing distribution Dpθ(x1:κt |y1:κt ) over x1:t−2 and
xt+1:κt , we obtain the approximation
Dp∆
θ (dxt−1:t |y1:T ) ≜
N
X
i=1
w(i)
κt δ˜x(i)
κt ,t−1:t (dxt−1:t).
(15)
Here, we use the notation ˜x(i)
κt,t to denote the ancestor at time t of particle x(i)
κt and
˜x(i)
κt,t−1:t = {˜x(i)
κt,t−1, ˜x(i)
κt,t }.
Inserting (14)–(15) into (13) provides an estimator of (12),
DG(θ | u) = ∇log p(θ) +
TX
t=1
N
X
i=1
w(i)
κt ξθ

˜x(i)
κt,t, ˜x(i)
κt,t−1

,
(16)
which is used in the proposal distributions in (6).
Estimation of the Hessian
The negative Hessian of the parameter log-posterior can be written as
HT (θ) = −∇2 log p(θ) −∇2 log pθ(y1:T ),
(17)
where it is assumed that the Hessian of the log-prior ∇2 log p(θ) can be calculated analyti-
cally. The negative Hessian of the log-likelihood, also known as the observed information
matrix, can using Louis’ identity (Cappé et al., 2005) be expressed as
−∇2 log pθ(y1:T ) = ∇log pθ(y1:T )2
−Eθ
f
∇2 log pθ(x0:T, y1:T )|y1:T
g
−Eθ
f
∇log pθ(x0:T, y1:T )2 |y1:T
g
.
(18)
Here, we have introduced the notation v2 = vv⊤for a vector v.
From this, we can construct an estimator of the negative Hessian in (17) using the estimator
of the gradient in (16), of the form
D
H(θ | u) = −∇2 log p(θ) + DG(θ | u)2 −D
H(1)(θ | u) −D
H(2)(θ | u),
(19)

166
Paper B
PMH using gradient and Hessian information
where we introduce the auxiliary quantities given by
H(1)(θ) = Eθ
∇2 log pθ(x0:T, y1:T )|y1:T
,
H(2)(θ) = Eθ
∇log pθ(x0:T, y1:T )2 |y1:T
 .
We obtain the estimator of the ﬁrst term analogously to (16) as
D
H(1)(θ | u) =
TX
t=1
N
X
i=1
w(i)
κt ζθ

˜x(i)
κt,t, ˜x(i)
κt,t−1

,
(20)
where we introduce the additive functional given by
ζθ(xt, xt−1) = ∇2 log fθ(xt | xt−1) + ∇2 log gθ(yt | xt).
The estimator of the second term needs a bit more work and we start by rewriting the last
term in (18) as
TX
t=1
TX
s=1
Eθ
ξθ(xt, xt−1)ξθ(xs, xs−1)⊤|y1:T
 =
TX
t=1

Eθ
ξθ(xt, xt−1)2 |y1:T

+
t−1
X
s=1
Eθ
f ξθ(xt, xt−1), ξθ(xs, xs−1)† |y1:T
g 
,
(21)
where we have introduced the operator (a, b)† = ab⊤+ ba⊤for brevity. Consider the last
term appearing in this expression, we can rewrite it as
t−1
X
s=1
Eθ
ξθ(xt, xt−1)ξθ(xs, xs−1)⊤|y1:T

= Eθ
"
ξθ(xt, xt−1)

t−1
X
s=1
Eθ
ξθ(xs, xs−1)| xt−1, y1:t−1

⊤
|                                          {z                                          }
≜αθ(xt−1)⊤
|y1:T
#
.
From this, we see that (21) can be written as an additive functional of the form
TX
t=1
Eθ
f
(ξθ(xt, xt−1))2 +  (ξθ(xt, xt−1), αθ(xt−1)† |y1:T
g
,
which can be estimated using the fl smoother as before. However, for this we need to
compute the quantities αθ(xt−1). One option is to make use of a type of ﬁxed-lag approxi-
mation for αθ(xt−1), by assuming that xs and xt are conditionally independent given y1:κt ,
whenever |s −t| > ∆. This approach has previously been used by Doucet et al. (2013).
Alternatively, we can use a ﬁlter approximation according to
Dαθ

x(i)
t

= Dαθ

x a(i)
t
t−1

+ ξθ

x(i)
t , x a(i)
t
t−1

,
(22)
for i = 1, . . ., N . Note that this approach suﬀers from the same particle degeneracy as
the apf. However, this only aﬀects a small number of terms and in our experience this

3
Estimation of the likelihood, gradient, and Hessian
167
approximation works suﬃciently well to give estimates with reasonable variance. The
resulting estimator using (21) is
D
H(2)(θ | u) =
TX
t=1
N
X
i=1
w(i)
κt ηθ

˜x(i)
κt,t, ˜x(i)
κt,t−1

, where
(23)
ηθ(xt, xt−1) = ξθ(xt, xt−1)2 +  ξθ(xt, xt−1), Dαθ(xt−1)†.
Hence, the Hessian can be estimated using (19) by inserting the estimators from (20), (22)
and (23).
Regularisation of the estimate of the Hessian
The pmh2 proposal (6) relies on the assumption that the observed information matrix is
positive deﬁnite ( pd). The estimator given in (19) does not always satisfy this, especially
when the Markov chain is located far from the posterior mode. Typically, the amount of
information is limited in such regions and this results in that the curvature is diﬃcult to
estimate. To cope with this issue, one alternative is to regularize the Hessian by adding
a diagonal matrix to shift the eigenvalues to be positive. The diagonal matrix can e.g., be
selected such that
∆D
H = max
(
0, −2λmin
  D
H)
Id,
(24)
where λmin( D
H) denotes the smallest eigenvalue of D
H(θ | u). In this article, we make use of
this method for handling non–pd estimates of the negative Hessian for the pmh2 algorithm.
This heuristic is common for Newton-type optimisation algorithms (Nocedal and Wright,
2006, Chapter 3.4).
Note, that there are other solutions available for ensuring positive deﬁniteness that only
shifts the negative eigenvalues, see (Nocedal and Wright, 2006, Chapter 3). We emphasise
that this type of regularization keeps the Markov chain invariant, i.e., still targets the correct
posterior distribution (recall Section 2.1).
Another alternative is to replace the estimate of the negative Hessian with the inverse
sample covariance matrix calculated using the trace of Markov chain when the estimate is
not pd. This can be seen as a hybrid between the pmh2 algorithm and a pre-conditioned
pmh1 algorithm. This resembles some other adaptive mh algorithms (Andrieu and Thoms,
2008) in which the same procedure is used to adapt the covariance matrix of a random walk
proposal. For this, we can make use of the last L iterations of the mh algorithm after that
the Markov chain has reached stationarity. During the burn-in phase, non–pd estimates can
be handled using a regularization approach or by rejecting the proposed parameter. In this
article, we refer to this method for handling non–pd estimates of the negative Hessian as
the hybrid pmh2 algorithm, where we use the latter alternative during the burn-in phase.
Note that this pre-conditioning can also be applied to the pmh0 and pmh1 algorithm, we
return to this in Section 4.4.

168
Paper B
PMH using gradient and Hessian information
Algorithm 2 Estimation of the likelihood, the gradient and the Hessian of the log-
posterior
Inputs: y1:T (data), R( · ) (propagation kernel), ν( · ) (weight function), N > 0 (no. parti-
cles), 0 < ∆≤T (lag).
Outputs: Dpθ(y) (est. of the likelihood), DG(θ) (est. of the gradient), D
H(θ) (est. of the negative
Hessian).
1: Initialise each particle x(i)
0 .
2: for t = 1 to T do
3:
Resample and propagate each particle using (9).
4:
Calculate the weights for each particle using (10).
5: end for
6: Compute Dpθ(y1:T ) by (11).
7: Compute DG(θ) and D
H(θ) by (16) and (19), respectively.
8: if D
H(θ) ≤0 then
9:
[standard] Regularize D
H(θ) by adding ∆D
H computed by (24)
10:
[hybrid] Replace D
H(θ) by the inverse covariance matrix computed using the L ﬁnal
samples of the Markov chain during the burn-in.
11: end if
Resulting SMC algorithm
In Algorithm 2, we present the complete procedure that combines the apf with the fl
smoother to compute the estimates needed for the pmh2 proposal (6). Note that the two
diﬀerent methods to handle non–pd estimates of the negative Hessian matrix results in the
standard and hybrid pmh2 algorithm, respectively.
We end this section by brieﬂy discussing the statistical properties of the estimates of the
gradient and Hessian obtained from the fl smoother. From Olsson et al. (2008), we know
that the fl smoother gives biased estimates of the gradient and Hessian for any number
of particles. Remember that this does not eﬀect the invariance of the Markov chain (recall
Section 2.1). The main advantage of the fl smoother over the apf (which gives a consis-
tent estimate) is that the former enjoys a smaller variance than the apf, i.e., we obtain a
favourable bias-variance trade-oﬀfor a certain choice of lag ∆. Note that a too small lag
gives a large bias in the estimate and a too large lag gives a large variance in the estimate.
Numerical illustrations
In this section, we provide illustrations of the properties of the fl smoother and the dif-
ferent proposed algorithms. The source code in Python and the data used for some of the
numerical illustrations are available for download from GitHub at: https://github.com/
compops/pmh-stco2015.

4
Numerical illustrations
169
Estimation of the log-likelihood and the gradient
We begin by illustrating the use of the fl smoother for estimating the log-likelihood and
the gradient. Here, we consider a linear Gaussian state space ( lgss) model given by
xt+1 | xt ∼N

xt+1; φxt, σ2
v

,
yt | xt ∼N

yt; xt, σ2
e

.
(25)
We generate two data realisations of length T = 100 using parameters θ(1) = {φ, σ2
v, σ2
e } =
{0.5, 1.0, 0.12} and θ(2) = {0.5, 1.0, 1.0} with a known initial zero state. We use the lag
∆= 5 and run the pfs with systematic resampling (Carpenter et al., 1999).
For this model, we can compute the true values of the log-likelihood and the gradient by
running an rts smoother (Rauch et al., 1965). In Figure 1, we present boxplots of the L1-
errors in the estimated log-likelihood and the gradient of the log-posterior with respect to
φ, evaluated at the true parameters. When σe = 0.1, we observe that the fapf has a large
advantage over the bpf for all choices of N . When σe = 1.0, we get smaller diﬀerence in
the error of the gradient estimates, but the log-likelihood estimates are still better for the
fapf. Similar results are also obtained for the gradient with respect to σv.
In Figure 2, we present the error in the gradient estimates with respect to φ using a varying
lag ∆and a varying number of particles N . The results are obtained by 1, 000 Monte Carlo
runs on a single data set generated from the previously discussed lgss model with T = 100.
We conclude again that fapf is preferable when available. The results are largely robust
to the lag, as long as this is chosen large enough when using the fapf. A lag of about 12
seems to be a good choice for this model when T = 100 and when using the fapf with
systematic resampling.
Burn-in and scale-invariance
Consider the problem of inferring {θ1, θ2} = {φ, σv} in the lgss model (25). We simulate
a single data set with parameters θ(1) (as deﬁned in the previous section) of length T = 250.
We use an uniform parameter prior over |φ| < 1, σv > 0 and initialise in θ0 = {0.1, 2}. We
use fapf with systematic resampling, N = 100 and ∆= 12. Here, we use the standard
version of Algorithm 2 to adjust the estimate of the Hessian in the cases when it is not pd,
resulting in the pmh2 algorithm.
We adjust the step lengths ϵ to give an acceptance rate during a pilot run of between
0.7 and 0.8 in the stationary phase. We obtain ϵ = {0.04, 0.065, 1.0} for PMH{0, 1, 2},
respectively. Note that a single step length is used for each proposal to simplify the tuning.
Of course, diﬀerent step lengths can be used for each parameter, and we could also use
diﬀerent step lengths during the burn-in and the stationary phase of the algorithm using the
approach discussed in Section 2.2. As previously mentioned, the pmh2 algorithm avoids
this (potentially diﬃcult and time-consuming) procedure, by taking the local geometric
information into account.
In the left column of Figure 3, we present the ﬁrst 50 iterations of the Markov chain from
the three diﬀerent algorithms. We note that the added information in the proposals of pmh1
and pmh2 aids the Markov chain in the burn-in phase. This results in that the Markov

170
Paper B
PMH using gradient and Hessian information
10
20
50
100
200
500
1000
2000
5000
-5
0
5
No. particles
log L1-error of log-likelihood
σe= 0.1
10
20
50
100
200
500
1000
2000
5000
-5
0
5
No. particles
log L1-error of log-likelihood
σe= 1.0
10
20
50
100
200
500
1000
2000
5000
-4
-2
0
2
No. particles
log L1-error of gradient wrt φ
σe= 0.1
10
20
50
100
200
500
1000
2000
5000
-4
-2
0
2
No. particles
log L1-error of gradient wrt φ
σe= 1.0
Figure 1. The log L1-error in the log-likelihood estimates and the estimates of the gradient with
respect to φ in the lgss model with σe = 0.1 (left) and σe = 1 (right). The bpf (green) and
fapf (orange) are evaluated by 1, 000 mc iterations using a ﬁxed data set with T = 100.

4
Numerical illustrations
171
2
4
8
16
32
64
-5
0
5
10
lag
log L1-error of gradient wrt φ
N=10, σe=0.1
2
4
8
16
32
64
-5
0
5
10
lag
log L1-error of gradient wrt φ
N=100, σe=0.1
2
4
8
16
32
64
-5
0
5
10
lag
log L1-error of gradient wrt φ
N=1000, σe=0.1
2
4
8
16
32
64
-5
0
5
10
lag
log L1-error of gradient wrt φ
N=10, σe=1.0
2
4
8
16
32
64
-5
0
5
10
lag
log L1-error of gradient wrt φ
N=100, σe=1.0
2
4
8
16
32
64
-5
0
5
10
lag
log L1-error of gradient wrt φ
N=1000, σe=1.0
Figure 2. The log L1-error in the estimates of the gradient with respect to φ in the lgss model
with σe = 0.1 (left) and σe = 1 (right). The bpf (green) and fapf (orange) are evaluated by
1, 000 Monte Carlo iterations using a ﬁxed data set with T = 100.

172
Paper B
PMH using gradient and Hessian information
θ 1 (φ)
θ 2 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
θ 1 (φ)
θ 2 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
θ 1 (φ)
θ 2 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
θ 3 (φ)
10 × θ 4 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
θ 3 (φ)
10 × θ 4 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
θ 3 (φ)
10 × θ 4 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
θ 3 (φ)
10 × θ 4 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
θ 3 (φ)
10 × θ 4 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
θ 3 (φ)
10 × θ 4 (σv)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Figure 3. The trace plots of the ﬁrst 50 steps using pmh0 (green), pmh1 (orange) and pmh2
(purple). The dotted lines show the true parameters of the lgss model. The gray contours show
the log-posterior.

4
Numerical illustrations
173
chains for the proposed algorithms reach the mode of the posterior quicker than the random
walk used in pmh0.
To illustrate the scale invariance of the pmh2 algorithm, we reparametrise the lgss model
by {θ3, θ4} = {φ, σv/10}. We keep the same settings as for the previous parametrisation
and rerun the algorithms. From this run we obtain the middle column in Figure 3. We
see clearly that the pmh1-algorithm does not perform well and gets stuck at the initial
parameter value. The reason is that the second component of the gradient is increased by
a factor 10 for the rescaled model. Since we still use the same step length, this will cause
the pmh1 algorithm to overshoot the region of high posterior probability when proposing
new values, and these will therefore never be accepted.
Finally, we recalibrate the three algorithms on the new parametrisation using the same
procedure as before and obtain the new step lengths {0.005, 0.0075, 1.0}. The resulting
Markov chains are presented in the right column of Figure 3. Despite the new step lengths,
pmh0 and pmh1 continue to struggle. The reason is that the step lengths are limited by
the small posterior variance in the θ4-parameter, resulting in a very slow progression in
the θ3-direction. Again, for pmh2, the added Hessian information is used to rescale the
proposal in each dimension resulting in a more eﬃcient exploration of the posterior than
for pmh0 and pmh1.
The mixing of the Markov chains at stationarity
We continue by investigating the mixing of the Markov chains at stationarity using an
estimate of the integrated autocorrelation time ( iact) given by
E
IACT(θ1:K ) = 1 + 2
Kl
X
k=1
Dρτ(θ1:K ),
(26)
where Dρτ(θ1:K ) denotes the empirical autocorrelation at lag τ of θ1:K (after the burn-in
has been discarded). A low value of the iact indicates that we obtain many uncorrelated
samples from the target distribution, implying that the chain is mixing well. Here, Kl is
determined as the ﬁrst index for which the empirical autocorrelation satisﬁes |DρKl (θ1:K )| <
2/
√
K, i.e., when the coeﬃcient is statistically insigniﬁcant.
We return to the lgss model in (25) with the original parameterisation {θ1, θ2} = {φ, σv}
using the same settings as before. A total of 25 data sets are generated using the parameters
θ(1) and the algorithms are initialised at the true parameter values to avoid a long burn-in
phase. The step sizes are determined using a series of pilot runs on the ﬁrst generated dataset
to minimise the total iact for each algorithm. This is done to make a fair comparison
between the diﬀerent algorithms at their near optimal performance. The resulting step sizes
are obtained as {0.08, 0.075, 1.50}.
Finally, we estimate the mixing in each of the 25 simulated data sets during K = 30, 000
mcmc iterations (discarding the ﬁrst 10, 000 iterations as burn-in). The results are presented
in Table 1, where the median and interquartile range (iqr; the distance between the 25%
and 75% quartiles) are presented for each pmh algorithm. Here, we present the results from
the standard version of Algorithm 2.

174
Paper B
PMH using gradient and Hessian information
Acc. rate
IACT(φ)
IACT(σv)
Median
Median
iqr
Median
iqr
pmh0
bpf(500)
0.02
257
146
265
371
bpf(1000)
0.06
83
129
79
118
bpf(2000)
0.15
29
23
15
24
fapf(50)
0.37
9
8
8
5
fapf(100)
0.38
9
6
7
4
fapf(200)
0.38
7
6
7
4
pmh1
bpf(500)
0.02
187
271
203
347
bpf(1000)
0.10
64
85
49
72
bpf(2000)
0.22
23
16
12
24
fapf(50)
0.58
3
2
3
1
fapf(100)
0.59
4
2
3
1
fapf(200)
0.58
3
1
3
1
pmh2
bpf(500)
0.03
170
211
164
190
bpf(1000)
0.10
59
73
65
80
bpf(2000)
0.24
13
10
19
17
fapf(50)
0.66
3
1
4
2
fapf(100)
0.66
3
1
5
2
fapf(200)
0.66
3
1
4
2
Table 1. Median and iqr for the acceptance rate and iact using diﬀerent smc algorithms. The
values are computed using 25 diﬀerent data sets from the lgss model.

4
Numerical illustrations
175
We see that the added information decreases the iact about 2 times for pmh1 and pmh2
compared with pmh0. We conclude that the extra information brought by the gradient
and the Hessian improves the mixing of the Markov chains in this model, which results
in a more eﬃcient exploration of the posterior. Note that, for this parametrisation of the
lgss model the posterior is quite isotropic (which can also be seen in the left column of
Figure 3). Hence, the conditions are in fact rather favourable for pmh0 and pmh1.
Parameter inference in a Poisson count model
In this section, we analyse the annual number of major earthquakes1 (over 7 on the Richter
scale) during the period from year 1900 to 2014. Following Langrock (2011), we model the
data using
xt+1 | xt ∼N

xt+1; φxt, σ2
,
yt | xt ∼P

yt; β exp(xt)

,
(27)
with parameters θ = {φ, σ, β} and uniform priors over |φ| < 1, σ > 0 and β > 0. Here,
P(λ) denotes a Poisson distribution with parameter λ.
We repeat the procedure from the previous subsection and obtain the step lengths using pilot
runs given by {0.06, 0.006, 0.85}. Here, we use K = 30, 000 mcmc iterations (discarding
the ﬁrst 10, 000 iterations as burn-in), the bpf with systematic resampling, ∆= 12, θ0 =
{0.5, 0.5, 18} and L = 2, 500. In this model, the estimate of the negative Hessian is often
non–pd (during about half of the iterations) and the choice of regularisation is therefore
important. To explore the properties of the regularisation, we apply both the standard and
hybrid version of the pmh2 algorithm discussed in Section 3.5. We compare these methods
to standard and pre-conditioned versions of the the pmh0 and pmh1 algorithms, using the
sample posterior covariance matrix calculated in the same manner as for the hybrid pmh2.
In Table 2, we present the resulting acceptance rates and iact values for each parameter
and algorithm. We note the large decrease in iact for β when using the Hessian infor-
mation, where the hybrid pmh2 seems to perform better than standard version for this
model. The improved mixing by using pmh2 is due to the scale invariance property, as the
parameter β is at least an order of magnitude larger than φ and σ (c.f. Figure 3). Note that
a reparameterisation or using separate step lengths for the parameters could possibly have
helped in improving the mixing in β for the standard versions of pmh0 and pmh1.
Using the standard and hybrid version of pmh2, decreases the overall computational cost
by a factor of about 100 for a speciﬁc number of eﬀective samples. The poor performance
of the pre-conditioned algorithms is probably due to that the sample posterior covariance
matrix does not fully capture the geometry of the posterior distribution.
In Figure 4, we present the trace and posterior estimates for β using the standard versions of
pmh0 and pmh1 as well as hybrid pmh2. The posterior estimates are obtained by pooling
the 10 parallel Markov chains after the burn-ins have been discarded. We see that the traces
behave rather diﬀerently with hybrid pmh2 exploring the space well compared with the
other methods.
1The data is obtained from the Earthquake Data Base System of the U.S. Geological Survey, which can be
accessed at http://earthquake.usgs.gov/earthquakes/eqarchives/.

176
Paper B
PMH using gradient and Hessian information
Version
smc alg.
Acc. rate
IACT(φ)
IACT(σ)
IACT(β)
Median
Median
iqr
Median
iqr
Median
iqr
pmh0
Standard
bpf(500)
0.26
497
712
16
3
2639
1163
Standard
bpf(1000)
0.30
89
150
15
3
2680
438
Pre-cond.
bpf(500)
0.43
35
17
16
1
107
105
Pre-cond.
bpf(1000)
0.45
38
28
16
2
129
131
pmh1
Standard
bpf(500)
0.76
665
442
277
162
2651
364
Standard
bpf(1000)
0.82
490
134
205
30
2875
1007
Pre-cond.
bpf(500)
0.62
266
187
9
3
1728
1638
Pre-cond.
bpf(1000)
0.70
98
209
9
3
1480
1732
pmh2
Standard
bpf(500)
0.24
91
17
53
14
222
37
Standard
bpf(1000)
0.28
60
14
47
17
139
59
Hybrid
bpf(500)
0.45
20
3
17
4
30
15
Hybrid
bpf(1000)
0.49
17
4
18
3
23
5
Table 2. Median and iqr for the acceptance rate and iact using diﬀerent number of particles. The values are computed using 10 runs on the Earthquake
count data model.

4
Numerical illustrations
177
0
200
400
600
800
1000
16.5
17.0
17.5
18.0
18.5
19.0
iteration
β
β
Density
12
14
16
18
20
22
24
0.00
0.05
0.10
0.15
0.20
0.25
0
200
400
600
800
1000
17.10
17.15
17.20
17.25
17.30
17.35
17.40
iteration
β
β
Density
15
16
17
18
19
20
0.0
0.2
0.4
0.6
0.8
0
200
400
600
800
1000
10
12
14
16
18
20
22
iteration
β
β
Density
5
10
15
20
25
0.00
0.05
0.10
0.15
0.20
Figure 4. Part of the trace (left) and posterior estimates (right) for the β parameter in the
earthquake count model using standard versions of pmh0 (green), pmh (orange) and hybrid
version of pmh2 (purple). Dotted lines indicate the posterior means.

178
Paper B
PMH using gradient and Hessian information
Using the parameter posterior estimate, we can compute point estimates for the parameters
of the model. The posterior mean for hybrid pmh2 is obtained as {0.88, 0.15, 16.58} with
standard deviations {0.07, 0.03, 2}. The parameter estimate is comparable to the estimate
{0.88, 0.15, 17.65} obtained by a maximum likelihood-based method using the same data
and model in Dahlin (2014, Example 4.9).
Robustness in the lag and step size
The pmh2 algorithm requires a number of parameters to be select by the user for each
parameter inference problem. It is therefore interesting to discuss the robustness of the
method with respect to these parameters. In the previous illustrations, we have seen that
the number of particles N is an important factor in determining the mixing.
Two other important parameters are the step length ϵ and the lag in the ﬂ-smoother ∆. To
illustrate the impact of these quantities on the iact, we return to the Earthquake model
in (27) using the standard pmh2 algorithm with the same settings but with K = 15, 000
(discarding the ﬁrst 5, 000 iterations as burn-in) and N = 1, 500. In Figure 5, we present the
iact for the three parameters in the model when varying ϵ and ∆, keeping everything else
ﬁxed. The standard pmh2 algorithm seems to be rather robust to both the choice of ∆and
ϵ after a certain threshold. Recall the discussion in Section 4.1 for the fl smoother. We
conclude that a suitable standard choice for the step length could be ϵ = 1, which can be
ﬁne tuned if the performance is not good enough. This recommendation is also common
in the literature concerning Newton-type algorithms.
Discussion and future work
Adding the gradient and Hessian information to the pmh proposal can have beneﬁcial
results including: (i) a shorter burn-in phase, (ii) a better mixing of the Markov chain,
and (iii) scale-invariance of the proposal which simpliﬁes tuning. The latter point is true
in particular for pmh2, since this method takes the local curvature of the posterior into
account, eﬀectively making the method invariant to aﬃne transformations.
It is common to distinguish between two phases of mcmc algorithms: the burn-in and
stationary phases. We have seen empirically that the proposed methods can improve upon
the original pmh0 during both of these phases but the best choices for the step lengths
can diﬀer between these two phases. Typically, a smaller step length is preferred during
burn-in and a larger during stationarity (the opposite holds for pmh0). The reason for this
is that during burn-in, the (natural) gradient information will heavily skew the proposal in a
direction of increasing posterior probability. That is, the methods tend to be aggressive and
propose large steps to make rapid progression toward regions of high posterior probability.
While this is intuitively appealing, the problem is that we require the Markov chains to be
reversible at all times. The reverse of these large steps can have very low probability which
prevents them from being accepted.
One interesting direction for future work is therefore to pursue adaptive algorithms (see e.g.,
Andrieu and Thoms (2008); Peters et al. (2010); Pitt et al. (2012)), to automatically tune the
step lengths during the diﬀerent phases of the algorithms. Another interesting possibility is

5
Discussion and future work
179
0.0
0.5
1.0
1.5
2.0
0
200
400
600
800
1000
ε
IACT
0
5
10
15
20
25
0
50
100
150
200
250
300
Δ
IACT
Figure 5. The iact for φ (green), σ (orange) and β (purple) for varying step sizes ϵ (upper)
and lag ∆(lower). The values are computed as the median of 10 runs using standard pmh2 with
the same data.

180
Paper B
PMH using gradient and Hessian information
to relax the reversibility requirement during burn-in; see (Diaconis et al., 2000) for a related
reference. This would cause the methods to behave like optimisation procedures during the
initial phase, but transition into samplers during the second phase.

Bibliography
181
Bibliography
C. Andrieu and G. O. Roberts. The pseudo-marginal approach for eﬃcient Monte Carlo
computations. The Annals of Statistics, 37(2):697–725, 2009.
C. Andrieu and J. Thoms. A tutorial on adaptive MCMC. Statistics and Computing, 18(4):
343–373, 2008.
C. Andrieu and M. Vihola. Convergence properties of pseudo-marginal Markov chain
Monte Carlo algorithms. Annals of Applied Probability, 25(2):1030–1077, 2015.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342,
2010.
M. A. Beaumont. Estimation of population growth or decline in genetically monitored
populations. Genetics, 164(3):1139–1160, 2003.
O. Cappé, E. Moulines, and T. Rydén. Inference in hidden Markov models. Springer Verlag,
2005.
J. Carpenter, P. Cliﬀord, and P. Fearnhead. Improved particle ﬁlter for nonlinear problems.
IEE Proceedings Radar, Sonar and Navigation, 146(1):2–7, 1999.
J. Dahlin. Sequential Monte Carlo for inference in nonlinear state space models. Licentiate’s
thesis no. 1652, Linköping University, May 2014.
J. Dahlin, F. Lindsten, and T. B. Schön.
Particle Metropolis Hastings using Langevin
dynamics. In Proceedings of the 38th International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Vancouver, Canada, May 2013.
J. Dahlin, F. Lindsten, and T. B. Schön. Second-order particle MCMC for Bayesian param-
eter inference. In Proceedings of the 19th IFAC World Congress, Cape Town, South Africa,
August 2014.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using gradient and
Hessian information. Statistics and Computing, 25(1):81–92, 2015.
P. Del Moral. Feynman-Kac formulae - genealogical and interacting particle systems with
applications. Springer Verlag, 2004.
P. Del Moral, A. Doucet, and S. Singh. Forward smoothing using sequential Monte Carlo.
Pre-print, 2010. arXiv:1012.5390v1.
P. Diaconis, S. Holmes, and R. Neal. Analysis of a nonreversible Markov chain sampler.
The Annals of Applied Probability, 10(3):685–1064, 2000.
A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years
later. In D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering.
Oxford University Press, 2011.
A. Doucet, P. Jacob, and A. M. Johansen. Discussion on Riemann manifold Langevin and
Hamiltonian Monte Carlo methods, 2011.

182
Paper B
PMH using gradient and Hessian information
A. Doucet, P. E. Jacob, and S. Rubenthaler. Derivative-free estimation of the score vector
and observed information matrix with application to state-space models. Pre-print, 2013.
arXiv:1304.5768v2.
A. Doucet, M. K. Pitt, G. Deligiannidis, and R. Kohn. Eﬃcient implementation of Markov
chain Monte Carlo when using an unbiased likelihood estimator. Biometrika, 102(2):
295–313, 2015.
R. G. Everitt. Bayesian parameter estimation for latent Markov random ﬁelds and social
networks. Journal of Computational and Graphical Statistics, 21(4):940–960, 2012.
T. Flury and N. Shephard. Bayesian inference based only on simulated likelihood: particle
ﬁlter analysis of dynamic economic models. Econometric Theory, 27(5):933–956, 2011.
M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73
(2):1–37, 2011.
A. Golightly and D. J. Wilkinson. Bayesian parameter inference for stochastic biochemical
network models using particle Markov chain Monte Carlo. Interface Focus, 1(6):807–820,
2011.
N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-
Gaussian Bayesian state estimation. IEEE Proceedings of Radar and Signal Processing, 140
(2):107–113, 1993.
G. Kitagawa and S. Sato. Monte Carlo smoothing and self-organising state-space model.
In A. Doucet, N. de Fretias, and N. Gordon, editors, Sequential Monte Carlo methods in
practice, pages 177–195. Springer Verlag, 2001.
R. Langrock. Some applications of nonlinear and non-Gaussian state–space modelling by
means of hidden Markov models. Journal of Applied Statistics, 38(12):2955–2970, 2011.
C. Nemeth and P. Fearnhead. Particle Metropolis adjusted Langevin algorithms for state-
space models. Pre-print, 2014. arXiv:1402.0694v1.
C. Nemeth, C. Sherlock, and P. Fearnhead. Particle Metropolis adjusted Langevin algo-
rithms. Pre-print, 2014. arXiv:1412.7299v1.
J. Nocedal and S. Wright. Numerical optimization. Springer Verlag, 2 edition, 2006.
J. Olsson, O. Cappé, R. Douc, and E. Moulines. Sequential Monte Carlo smoothing with
application to parameter estimation in nonlinear state space models. Bernoulli, 14(1):
155–179, 2008.
G. W. Peters, G. R. Hosack, and K. R. Hayes. Ecological non-linear state space model selec-
tion via adaptive particle Markov chain Monte Carlo. Pre-print, 2010. arXiv:1005.2238v1.
M. K. Pitt and N. Shephard. Filtering via simulation: auxiliary particle ﬁlters. Journal of
the American Statistical Association, 94(446):590–599, 1999.

Bibliography
183
M. K. Pitt, R. S. Silva, P. Giordani, and R. Kohn. On some properties of Markov chain
Monte Carlo simulation methods based on the particle ﬁlter. Journal of Econometrics, 171
(2):134–151, 2012.
G. Poyiadjis, A. Doucet, and S. S. Singh. Particle approximations of the score and ob-
served information matrix in state space models with application to parameter estimation.
Biometrika, 98(1):65–80, 2011.
H. E. Rauch, F. Tung, and C. T. Striebel. Maximum likelihood estimates of linear dynamic
systems. AIAA Journal, 3(8):1445–1450, August 1965.
C. P. Robert and G. Casella. Monte Carlo statistical methods. Springer Verlag, 2 edition,
2004.
G. O. Roberts and J. S. Rosenthal. Optimal scaling of discrete approximations to langevin
diﬀusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1):
255–268, 1998.
G. O. Roberts and O. Stramer. Langevin diﬀusions and Metropolis-Hastings algorithms.
Methodology and Computing in Applied Probability, 4(4):337–357, 2003.
G. O. Roberts, A. Gelman, and W. R. Gilks. Weak convergence and optimal scaling of
random walk Metropolis algorithms. The Annals of Applied Probability, 7(1):110–120,
1997.
C. Sherlock, A. H. Thiery, G. O. Roberts, and J. S. Rosenthal. On the eﬃcency of pseudo-
marginal random walk Metropolis algorithms. The Annals of Statistics, 43(1):238–275,
2015.


Paper C
Quasi-Newton particle
Metropolis-Hastings
Authors:
J. Dahlin, F. Lindsten and T. B. Schön
This paper is published by Elsevier:
http://dx.doi.org/10.1016/j.ifacol.2015.12.258.
Edited version of the paper:
J. Dahlin, F. Lindsten, and T. B. Schön. Quasi-Newton particle Metropolis-
Hastings. In Proceedings of the 17th IFAC Symposium on System Identiﬁcation
(SYSID), pages 981–986, Beijing, China, October 2015c.


Quasi-Newton particle Metropolis-Hastings
J. Dahlin⋆, F. Lindsten† and T. B. Schön†
⋆Dept. of Electrical Engineering,
Linköping University,
SE–581 83 Linköping, Sweden.
johan.dahlin@liu.se
†Dept. of Information Technology,
Uppsala University,
SE-751 05 Uppsala, Sweden.
fredrik.lindsten@it.uu.se
thomas.schon@it.uu.se
Abstract
Particle Metropolis-Hastings enables Bayesian parameter inference in general non-linear state space
models (ssms). However, in many implementations a random walk proposal is used and this can
result in poor mixing if not tuned correctly using tedious pilot runs. Therefore, we consider a new
proposal inspired by quasi-Newton algorithms that may achieve similar (or better) mixing with less
tuning. An advantage compared to other Hessian based proposals, is that it only requires estimates
of the gradient of the log-posterior. A possible application is parameter inference in the challenging
class of ssms with intractable likelihoods. We exemplify this application and the beneﬁts of the new
proposal by modelling log-returns of future contracts on coﬀee by a stochastic volatility model with
α-stable observations.
Keywords
Bayesian parameter inference, state space models, approximate Bayesian computations, particle Markov
chain Monte Carlo, α-stable distributions.
Data and source code in Python
https://github.com/compops/qpmh-sysid2015
Financial support from
The projects Learning of complex dynamical systems (Contract number: 637-2014-466), Probabilistic
modeling of dynamical systems (Contract number: 621-2013-5524) and cadics, a Linnaeus Center, all
funded by the Swedish Research Council.
187

188
Paper C
Quasi-Newton particle Metropolis-Hastings
Introduction
We are interested in Bayesian parameter inference in the non-linear state space model ( ssm)
possibly with an intractable likelihood. An ssm with latent states x0:T = {xt }T
t=0 and
observations y1:T is given by
xt+1 | xt ∼fθ(xt+1 | xt),
yt | xt ∼gθ(yt | xt),
(1)
with x0 ∼µθ(x0) and where θ ∈Θ ⊆Rp denotes the static unknown parameters. Here,
we assume that it is possible to simulate from the distributions µθ(x0), fθ(xt+1 | xt) and
gθ(yt | xt), even if the respective densities are unavailable.
The main object of interest in Bayesian inference is the parameter posterior distribution,
π(θ) = p(θ |y1:T ) ∝pθ(y1:T )p(θ),
(2)
which is often intractable and cannot be computed in closed form. The problem lies in
that the likelihood pθ(y1:T ) = p(y1:T |θ) cannot be exactly computed. However, it can
be estimated by computational statistical methods such as sequential Monte Carlo (smc;
Doucet and Johansen, 2011). The problem is further complicated when gθ(yt | xt) cannot
be evaluated point-wise, which prohibits direct application of smc. This could be the result
of that the density does not exist or that it is computationally prohibitive to evaluate. In
both cases, we say that the likelihood of the ssm (1) is intractable.
Recent eﬀorts to develop methods for inference in models with intractable likelihoods have
focused on approximate Bayesian computations (abc; Marin et al., 2012). The main idea in
abc is that data simulated from the model (using the correct parameters) should be similar
to the observed data. This idea can easily be incorporated into many existing inference
algorithms, see Dean et al. (2014).
An example of this is the abc version of particle Metropolis-Hastings (pmh-abc; Jasra,
2015; Bornn et al., 2014). In this algorithm, the intractable likelihood is replaced with an
estimate obtained by the abc version of smc (smc-abc; Jasra et al., 2012). However, the
random walk proposal often used in pmh-( abc) can result in problems with poor mixing,
which leads to high variance in the posterior estimates. The mixing can be improved by
pre-conditioning the proposal with a matrix P, which typically is chosen as the unknown
posterior covariance (Sherlock et al., 2015). However, estimating P can be challenging when
p is large or the posterior (2) is non-isotropic. This typically results in that the user needs
to run many tedious pilot runs when implementing pmh-( abc) for parameter inference
in a new ssm.
Our main contribution is to adapt a limited-memory bfgs algorithm (Nocedal and Wright,
2006) as a proposal in pmh-( abc). This is based on earlier work by Dahlin et al. (2015a)
and Zhang and Sutton (2011). In the former, we discuss how to make use of gradient ascent
and Newton-type proposals in pmh. The advantages of the new proposal are; (i) good
mixing when gradient estimates are accurate, (ii) no tedious pilot runs required and (iii)
only requires gradients to approximate the local Hessian. These advantages are important as
direct estimation of gradients using smc often is simpler than for Hessians. Note that this
new proposal is useful both with and without the abc approximation of the likelihood.

2
Particle Metropolis-Hastings
189
To demonstrate these beneﬁts we consider a linear Gaussian state space ( lgss) model,
where we compare the performance of our proposal with and without abc. Furthermore,
we consider using a stochastic volatility model with α-stable log-returns (Nolan, 2003)
to model future contracts on coﬀee. This model is common in the abc literature as the
likelihood is intractable, see Dahlin et al. (2015c), Jasra (2015) and Yıldırım et al. (2014).
Particle Metropolis-Hastings
A popular approach to estimate the parameter posterior (2) is to make use of statistical
simulation methods. pmh (Andrieu et al., 2010) is one such method and it operates by
constructing a Markov chain, which has the sought posterior as its stationary distribution.
As a result, we obtain samples from the posterior by simulating the Markov chain for a
large number of iterations after its has converged.
The Markov chain targeting (2) is constructed by an iterative procedure. During iteration
k, we propose a candidate parameter θ′ ∼q(θ′|θk−1, uk−1) and an auxiliary variable u′ ∼
mθ′(u′) as detailed in the following using proposals q and mθ′. The candidate is then
accepted, i.e., {θk, uk} ←{θ′, u′}, with the probability
α θ′, θk−1, u′, uk−1
 =
Dπ θ′| u′
Dπ θk−1 | uk−1
 q θk−1 |θ′, u′
q θ′|θk−1, uk−1
,
(3)
otherwise the parameter is rejected, i.e., {θk, uk} ←{θk−1, uk−1}. Here, we introduce the
notation Dπ(θ | u) = Dpθ(y1:T | u)p(θ) for some unbiased estimate of π(θ) constructed using u
and p(θ) denotes the parameter prior distribution.
pmh can be viewed as a Metropolis-Hastings algorithm in which the intractable likelihood
is replaced with an unbiased noisy estimate. It is possible to show that this so-called exact
approximation results in a valid algorithm as discussed by Andrieu and Roberts (2009).
Speciﬁcally, the Markov chain generated by pmh converges to the desired stationary dis-
tribution despite the fact that we are using an approximation of the likelihood. It is also
possible to show that u can be included into the proposal q, which is necessary for including
gradients and Hessians when proposing θ′ as discussed by Dahlin et al. (2015a).
In Section 4, we discuss how to construct the proposal mθ by running an smc algorithm.
In this case, the auxiliary variable u is the resulting generated particle system. We obtain
pmh-abc as presented in Algorithm 1, when smc-abc is used for Step 4. This is a com-
plete procedure for generating K correlated samples {θ1, . . ., θK } from (2). By the ergodic
theorem, we can estimate any posterior expectation of an integrable test function ϕ : Θ →R
(e.g., the posterior mean) by
E
f
ϕ(θ)|y1:T
g
≈DϕMH ≜
1
K −Kb
K
X
k=Kb
ϕ(θk),
(4)
which is a strongly consistent estimator if the Markov chain is ergodic (Meyn and Tweedie,
2009). Here, we discard the ﬁrst Kb samples known as the burn-in, i.e., before the chain
reaches stationarity. Under geometric mixing conditions, the error of the estimate obeys

190
Paper C
Quasi-Newton particle Metropolis-Hastings
Algorithm 1 Particle Metropolis-Hastings ( pmh)
Inputs: K > 0 (no. mcmc steps), θ0 (initial parameters)
and {q, mθ} (proposals).
Output: {θ1, . . ., θK } (approximate samples from the posterior).
1: Generate u0 ∼mθ0 and compute Dpθ0(y1:T | u0).
2: for k = 1 to K do
3:
Sample θ′ ∼q(θ′|θk−1, uk−1).
4:
Sample u′ ∼mθ′ using Algorithm 2.
5:
Compute Dpθ′(y1:T | u′) using (10).
6:
Sample ωk uniformly over [0, 1].
7:
if ωk ≤min{1, α(θ′, θk−1, u′, uk−1)} given by (3) then
8:
Accept θ′, i.e., {θk, uk} ←{θ′, u′}.
9:
else
10:
Reject θ′, i.e., {θk, uk} ←{θk−1, uk−1}.
11:
end if
12: end for
the central limit theorem (when (K −Kb) →∞) given by
p
K −Kb
f
DϕMH −Eϕ(θ)|y1:T
g
d
−→N

0, σ2
ϕ

,
(5)
where σ2
ϕ denotes the variance of the estimator. The variance is proportional to the in-
tegrated autocorrelation time ( iact), which describes the mixing of the Markov chain.
Hence, we can use iact in the illustrations presented in Section 5 to compare the mixing
between diﬀerent proposals.
Proposal for parameters
To complete Algorithm 1, we need to specify a proposal q from which we sample θ′. The
choice of proposal is important as it is one of the factors that inﬂuences the mixing of
resulting Markov chain. The general form of a Gaussian proposal discussed in Dahlin et al.
(2015a) is
q θ′′|θ′, u′ = N

θ′′; µ θ′, u′, Σ θ′, u′
,
(6)
where diﬀerent choices of the mean function µ(θ′, u′) and covariance function Σ(θ′, u′)
results in diﬀerent versions of pmh as presented in Table 1.
Zeroth and first-order proposals (PMH0/1)
pmh0 is referred to as a zero order (or marginal) proposal as it only makes use of the last
accepted parameter to propose the new parameter. Essentially, this proposal is a Gaussian
random walk scaled by a positive semi-deﬁnite ( psd) preconditioning matrix P−1. The
performance of pmh0 is highly dependent on P−1, which is tedious and diﬃcult to estimate
as it should be selected as the unknown posterior covariance, see Sherlock et al. (2015).

3
Proposal for parameters
191
Table 1. Diﬀerent proposals for the pmh algorithm.
Proposal
µ(θ′, u′)
Σ(θ′, u′)
pmh0
θ′
ϵ 2
0 P−1
pmh2
θ′ +
ϵ 2
1
2
P−1 DG(θ′| u′)
ϵ 2
1 P−1
pmh2
θ′ +
ϵ 2
2
2
 D
H(θ′| u′)−1 DG(θ′| u′)
ϵ 2
2
 D
H(θ′| u′)−1
Furthermore, it is known that gradient information can be useful to give the proposal a
mode-seeking behaviour. This can be beneﬁcial both initially to ﬁnd the mode and for
increasing mixing by keeping the Markov chain in areas with high posterior probability.
This information can be included by making use of noisy gradient ascent update, where
DG(θ′| u′) denotes the particle estimate of the gradient of the log-posterior given by G(θ′) =
∇log π(θ)|θ=θ′. Again, we scale the step size and the gradient by P−1 and this results in the
pmh1 proposal.
Second-order proposal (PMH2)
An alternative is to make use of a noisy Newton update as the proposal by replacing P with
D
H(θ′| u′), which denotes the particle estimate of the negative Hessian of the log-posterior
given by H(θ′) = −∇2 log π(θ)|θ=θ′. This results in the second-order pmh2 proposal dis-
cussed by Dahlin et al. (2015a), which relies on accurate estimates of the Hessian but these
often require many particles and therefore incur a high computational cost. This problem
is encountered in e.g., the abc approximation of the α-stable model in (9).
The new quasi-pmh2 (qpmh2) proposal circumvents this problem by constructing a local
approximation of the Hessian based on a quasi-Newton update, which only makes use
of gradient information. The update is inspired by the limited-memory bfgs algorithm
(Nocedal, 1980; Nocedal and Wright, 2006) given by
B−1
l+1(θ′) =  Ip −ρl sl g⊤
l
B−1
l
 Ip −ρl gl s⊤
l
 + ρl sl s⊤
l ,
(7)
with ρ−1
l
= g⊤
l sl, sl = θI (l) −θI (l−1) and gl = DG(θI (l) | uI (l))−DG(θI (l−1) | uI (l−1)). The update
is iterated over l ∈{1, 2, . . ., M −1} with I (l) = k −l, i.e., over the M −1 previous states
of the Markov chain. Hence, we refer to M as the memory length of the proposal. We
initialise the update with B−1
1
= ρ−1
1 (g⊤
1 g1)−1Ip and make use of a pmh0 proposal with
P = δ−1Ip for the ﬁrst M iterations, where δ > 0 is deﬁned by the user. The resulting
estimate of the negative Hessian is given by D
H(θ′| u′) = −BM (θ′). See Appendix B for more
details regarding the implementation of the qpmh2 proposal and the complete algorithm
in Algorithm 3.
An apparent problem with using a quasi-Newton approximation of the Hessian is that
the resulting proposal is no longer Markov (resulting in a non-standard mcmc). However,
as shown by Zhang and Sutton (2011), it is still possible to obtain a valid algorithm by
viewing the chain as an M -dimensional Markov chain. In eﬀect, this amounts to using the
sample at lag M as the basis for the proposal. Hence, we set {θ′, u′} = {θk−M, uk−M }

192
Paper C
Quasi-Newton particle Metropolis-Hastings
and ϵ2 = 1 in the PMH2 proposal in Table 1. Furthermore, in case of a rejection we set
{θk, uk} = {θk−M, uk−M }. We refer to Zhang and Sutton (2011) for further details.
The approximate Hessian D
H(θ′| u′) has to be psd to be a valid covariance matrix. This can
be problematic when the Markov chain is located in areas with low posterior probability
or sometimes due to noise in the gradient estimates. In our experience, this happens occa-
sionally in the stationary regime, i.e., after the burn-in phase. However, when it happens
D
H(θ′| u′) is corrected by the hybrid approach discussed by Dahlin et al. (2015a).
Proposal for auxiliary variables
To implement qpmh2, we require estimates of the likelihood and the gradient of the log-
posterior. These are obtained by running smc-abc which corresponds to simulating the
auxiliary variables u. In this section, we show how to estimate the likelihood and its gradient
by the ﬁxed-lag (fl; Kitagawa and Sato, 2001) smoother.
SMC-ABC algorithm
smc-abc (Jasra et al., 2012) relies on a reformulation of the non-linear ssm (1). We start
by perturbing the observations yt to obtain ˇy1:T by
ˇyt = ψ(yt) + ϵzt,
zt ∼ρϵ,
for t = 1, . . .,T ,
(8)
where ψ denotes a one-to-one transformation and ρϵ denotes a kernel, e.g., Gaussian or
uniform, with ϵ as the bandwidth or tolerance parameter. We continue with assuming that
there exists some random variables vt ∼νθ(vt | xt) such that we can generate a sample from
gθ(yt | xt) by the transformation yt = τθ(vt, xt). An example is the Box-Muller transforma-
tion to obtain a Gaussian random variable from two uniforms, see Appendix A.
To obtain the perturbed ssm, we introduce ˇx⊤
t = (x⊤
t , v⊤
t ) as the new state variable with
the dynamics
ˇxt+1 | ˇxt ∼Ξθ(ˇxt+1 | ˇxt) = νθ(vt+1 | xt+1) fθ(xt+1 | xt),
(9a)
and the likelihood is modelled by
ˇyt | ˇxt ∼ℎθ,ϵ(ˇyt | ˇxt) = ρϵ
 ˇyt −ψ(τθ(ˇxt)),
(9b)
which follows from the perturbation in (8). With this reformulation, we can construct
smc-abc as outlined in Algorithm 2, which is a standard smc algorithm applied to the
perturbed model. Note that, we do not require any evaluations of the intractable density
gθ(yt | xt). Instead, we only simulate from this distribution and compare the simulated and
observed (perturbed) data by ρϵ.
The accuracy of the abc approximation is determined by ϵ, where we recover the original
formulation in the limit when ϵ →0. In practice, this is not possible and we return to
study the impact of a non-zero ϵ in Section 5.1.

4
Proposal for auxiliary variables
193
Algorithm 2 Sequential Monte Carlo with approximate Bayesian computations
Inputs: ˇy1:T (perturbed data), the ssm (9), N ∈N (no. particles), ϵ > 0 (tolerance parame-
ter), ∆∈[0,T ) ⊂N (lag).
Outputs: Dpθ(ˇy1:T | u), DG(θ | u) (est. of likelihood and gradient).
Note: all operations are carried out over i, j = 1, . . ., N .
1: Sample ˇx(i)
0 ∼µθ(x0)νθ(v0 | x0) and set w(i)
0 = 1/N .
2: for t = 1 to T do
3:
Resample the particles by sampling a new ancestor index a(i)
t
from a multinomial
distribution with P a(i)
t
= j = w(j)
t−1.
4:
Propagate the particles by sampling ˇx(i)
t
∼Ξθ
 ˇx(i)
t
ˇx a(i)
t
t−1

and extending the trajectory
by ˇx(i)
0:t = ˇx a(i)
t
0:t−1, ˇx(i)
t
	
.
5:
Calculate the particle weights by H
w(i)
t
= ℎθ,ϵ
 ˇyt, ˇx(i)
t

which by normalisation (over
i) gives w(i)
t .
6: end for
7: Estimate Dpθ(ˇy1:T | u) by (10) and DG(θ | u) by (11).
Estimation of the likelihood
From Section 2, we require an unbiased estimate of the likelihood to compute the acceptance
probability (3). This can be achieved by using u generated by smc-abc. In this case, the
auxiliary variables u ≜{{ˇx(i)
0:t }N
i=1}T
t=0 are the particle system composed of all the particles
and their trajectories. The resulting likelihood estimator is given by
Dpθ(y1:T | u) =
T
Y
t=1

1
N
N
X
i=1
H
w(i)
t

,
(10)
where the unnormalised particle weights H
w(i)
t
are deterministic functions of u. This is an
unbiased and N -consistent estimator for the likelihood in the perturbed model. However,
the perturbation itself introduces some bias and additional variance compared with the
original unperturbed model (Dean et al., 2014). The former can result in biased parameter
estimates and the latter can result in poor mixing of the Markov chain. We return to study
the impact on the mixing numerically in Section 5.1.
Estimation of the gradient of the log-posterior
We also require estimates of the gradient of the log-posterior given u to implement the
proposals introduced in Table 1. In Dahlin et al. (2015a), this is accomplished by using the
fl smoother together with the Fisher identity. However, this requires accurate evaluations
of the gradient of log gθ(yt | xt) with respect to θ. As discussed by Yıldırım et al. (2014),
we can circumvent this problem by the reformulation of the ssm in (9) if the gradient of

194
Paper C
Quasi-Newton particle Metropolis-Hastings
τθ(ˇxt) can be evaluated. This results in the gradient estimate
DG(θ′| u′) = ∇log p(θ)θ=θ′ +
TX
t=1
N
X
i=1
w(i)
κt ξθ′

˜z(i)
κt,t, ˜z(i)
κt,t−1

,
(11)
ξθ′(ˇxt, ˇxt−1) ≜∇log Ξθ(ˇxt | ˇxt−1)
θ=θ′ + ∇log ℎθ(ˇyt | ˇxt)
θ=θ′,
where ˜z(i)
κt,t denotes the ancestor at time t of particle ˇx(i)
κt and ˜z(i)
κt,t−1:t = {˜z(i)
κt,t−1, ˜z(i)
κt,t }.
The estimator in (11) relies on the assumption that the ssm is mixing quickly, which
means that past states have a diminishing inﬂuence on future states and observations. More
speciﬁcally, we assume that pθ(xt |y1:T ) ≈pθ(xt |y1:κt ), with κt = min{t + ∆,T } and lag
∆∈[0,T ) ⊂N. Note that this estimator is biased, but this is compensated for by the accept-
reject step in Algorithm 1 and does not eﬀect the stationary distribution of the Markov
chain. See Dahlin et al. (2015a) for details.
Numerical illustrations
We evaluate qpmh2 by two illustrations with synthetic and real-world data. In the ﬁrst
model, we can evaluate gθ(yt | xt) exactly in closed-form, which is useful to compare stan-
dard pmh and pmh-abc. In the second model, the likelihood is intractable and therefore
only pmh-abc can be used. See Appendix A for implementation details.
Linear Gaussian SSM
Consider the following lgss model
xt+1 | xt ∼N

xt+1; µ + φ(xt −µ), σ2
v

,
yt | xt ∼N

yt; xt, 0.12
,
(12)
with θ = {µ, φ, σv} and µ ∈R, φ ∈(−1, 1) and σv ∈R+. A synthetic data set consisting of
a realisation with T = 250 observations is simulated from the model using the parameters
{0.2, 0.8, 1.0}. We begin by investigating the accuracy of Algorithm 2 for estimating the
log-likelihood and the gradients of the log-posterior with respect to θ. The error of these
estimates are computed by comparing with the true values obtain by a Kalman smoother.
In Figure 1, we present the log-L1 error of the log-likelihood and the gradients for diﬀerent
values of ϵ. The error in the gradient with respect to φ is not presented here, but is similar
to the gradients for µ. We see that the error in both the log-likelihood and the gradient
are minimized when ϵ ≈0.05 −0.10. Here, smc-abc achieve almost the same error as
standard smc. However, when ϵ grows larger smc-abc suﬀers from an increasing bias
resulting from a deteriorating approximation in (8). We conclude that this results in a
bias in the parameter estimates as the bias in the log-likelihood estimate propagates to the
parameter posterior estimate.
We now consider estimating the parameters in (12). In this model, we can compare standard
pmh with pmh-abc to study the eﬀects of using abc to approximate the log-likelihood.
For this comparison, we quantify the mixing of the Markov chain using the estimated iact

5
Numerical illustrations
195
0.01
0.1
0.2
0.3
-2
0
2
4
tolarance parameter ε
log L1-error in log-likelihood
0.01
0.1
0.2
0.3
-6
-5
-4
-3
-2
-1
tolarance parameter ε
log L1-error in score wrt µ
0.01
0.1
0.2
0.3
-3
-2
-1
0
1
2
3
tolarance parameter ε
log L1-error in score wrt σv
Figure 1. The log-L1-error in the log-likelihood (top) and gradient with respect to µ (lower left)
and σv (lower right) using smc-abc when varying ϵ. The shaded area and the dotted lines
indicate maximum/minimum error and the standard smc error, respectively. The plots are
created from the output of 100 Monte Carlo runs on a single synthetic data set.

196
Paper C
Quasi-Newton particle Metropolis-Hastings
min IACT
max IACT
Alg.
Acc.
Median
iqr
Median
iqr
L adapted
pmh0
0.28
12.13
1.53
13.71
1.23
pmh1
0.78
11.28
0.50
14.50
1.45
qpmh2
0.55
3.00
0.03
3.01
0.07
pmh0-abc
0.14
29.66
10.37
34.04
9.36
pmh1-abc
0.31
33.09
5.45
38.32
14.42
qpmh2-abc
0.45
3.00
0.02
3.03
0.08
L = 1, 000
pmh0
0.28
7.96
9.60
10.92
6.00
pmh1
0.78
9.47
4.39
10.60
8.19
qpmh2
0.55
5.40
3.01
8.98
6.90
pmh0-abc
0.14
12.91
8.86
35.68
3.22
pmh1-abc
0.31
27.34
23.63
35.84
30.31
qpmh2-abc
0.45
6.65
5.14
10.96
6.71
Table 2. The iact values for the lgss model (12).
given by
E
IACT(θKb:K ) = 1 + 2
L
X
l=1
Dρl(θKb:K ),
(13)
where Dρl(θKb:K ) denotes the empirical autocorrelation at lag l of θKb:K , and Kb is the burn-
in time. A small value of if indicates that we obtain many uncorrelated samples from the
target distribution. This implies that the chain is mixing well and that σ2
ϕ in (5) is rather
small. We make use of two diﬀerent L to compare the mixing: (i) the smallest L such that
|DρL(θKb:K )| < 2/
p
K −Kb (i.e., when statistical signiﬁcant is lost) and (ii) a ﬁxed number
of lags L = 1, 000.
In Table 2, we present the minimum and maximum iacts as the median and interquartile
range ( iqr) computed using 10 Monte Carlo runs over the same data set. We note the good
performance of the qpmh2 proposal which achieves similar (or better) mixing compared
to the pre-conditioned proposals. Remember that pmh0/1 are tuned to their optimal
performance using tedious pilot runs, see Sherlock et al. (2015) and Nemeth et al. (2014).
We present some additional diagnostic plots (trace, autocorrelation and posterior estimates)
for pmh0 and qpmh2 (without abc) in Appendix C.
In our experience, the performance of qpmh2 seems to be connected with the variance
of DG(θ′| u′). This is similar to the existing theoretical results for pmh1 (Nemeth et al.,
2014). For the lgss model, we can compute the gradient with a small variance and there-
fore qpmh2 performs well. However, this might not be the case for all models and the
performance of qpmh2 thus depends on both the model and which particle smoother is
applied to estimate the gradient. Finally, note that using abc results in a smaller acceptance

6
Conclusions
197
rate and worse mixing. This problem can probably be mitigated by adjusting ϵ and N as
discussed by Bornn et al. (2014) and Jasra (2015).
Modelling the volatility in coffee futures
Consider the problem of modelling the volatility of the log-returns of future contracts
on coﬀee using the T = 399 observations in Figure 2. A prominent feature in this type
of data is jumps (present around March, 2014). These are typically the result of sudden
changes in the market due to e.g., news arrivals and it has been proposed to make use of an
α-stable distribution to model these jumps, see Lombardi and Calzolari (2009) and Dahlin
et al. (2015c). Therefore, we consider a stochastic volatility model with symmetric α-stable
returns (αsv) given by
xt+1 | xt ∼N

xt+1; µ + φ(xt −µ), σ2
v

,
yt | xt ∼A

yt; α, exp(xt)

,
(14)
with θ = {µ, φ, σv, α}. Here, A(α, η) denotes a symmetric α-stable distribution with
stability parameter α ∈(0, 2) and scale parameter η ∈R+. As previously discussed, we cannot
evaluate gθ(yt | xt) for this model but it is possible to simulate from it. See Appendices A
and D for more details regarding the α-stable distribution and methods for simulating
random variables.
In Figure 3, we present the posterior estimates obtained from qpmh2, which corresponds
to the estimated parameter posterior mean Dθ = {0.214, 0.931, 0.268, 1.538}. This indicates
a slowly varying log-volatility with heavy-tailed log-returns as the Cauchy and Gaussian
distribution corresponds to α = 1 and α = 2, respectively. We also compare with the
posterior estimates from pmh0, which are quite similar in location and scale.
Finally, we present the smoothed estimate of the log-volatility using Dθ in Figure 2. The
estimate seems reasonable and tracks the periods with low volatility (around October, 2013)
and high volatility (around May, 2014).
Conclusions
We have demonstrated that qpmh2 exhibits similar or improved mixing when compared
with pre-conditioned proposals with/without the abc approximation. The main advantage
is that qpmh2 does not require extensive tuning of the step sizes in the proposal to achieve
good mixing, which can be a problem in practice for pmh0/1. The user only needs to
choose δ and M , which in our experience are simpler to tune. Finally, qpmh2 only requires
gradient information, which are usually simpler to obtain than directly estimate the Hessian
of the log-posterior.
In future work, it would be interesting to analyse the impact of the variance of the gradient
estimates on the mixing of the Markov chain in qpmh2. In our experience, qpmh2 per-
forms well when the variance is small or moderate. However when the variance increases,
the mixing can be worse than for pmh0. This motivates further theoretical study and devel-
opment of better particle smoothing techniques for gradient estimation to obtain gradient
estimates with lower variance.

198
Paper C
Quasi-Newton particle Metropolis-Hastings
-10
-5
0
5
10
time
daily log-returns
Jun 13
Aug 13
Oct 13
Dec 13
Feb 14
Apr 14
Jun 14
Aug 14
Oct 14
Dec 14
-1.0
-0.5
0.0
0.5
1.0
1.5
smoothed log-volatility
daily log-returns
density
-10
-5
0
5
10
0.00
0.05
0.10
0.15
0.20
0.25
-4
-2
0
2
4
-10
-5
0
5
10
theoretical Gaussian quantiles
sample quantiles
Figure 2. Upper: log-returns (green) and smoothed log-volatility (orange) of futures on coﬀee
between June 1, 2013 and December 31, 2014. The shaded area indicate the approximate 95%
credibility region for the log-returns estimated from the model. Lower left: kernel density
estimate (green) and a Gaussian approximation (magenta). Lower right: qq-plot comparing the
quantiles of the data with the best Gaussian approximation (magenta).

A
Implementation details
199
Another extension of this work is to consider models where p is considerable larger than
discussed in this paper. This would probably lead to an even greater increase in mixing when
using qpmh2 compared with pmh0. This eﬀect is theoretically and empirically examined
by Nemeth et al. (2014).
In this context, it would also be interesting to implement a quasi-Newton proposal in a
particle Hamiltonian Monte Carlo ( hmc) algorithm in the spirit of Zhang and Sutton
(2011). This as hmc algorithms are known to greatly increase mixing for some models
when the gradient and log-likelihood can be evaluated analytically. However, no particle
version of hmc has yet been proposed in the literature but the possibility is considered in
the discussions following Girolami and Calderhead (2011).
The source code and data for the lgss model as well as some supplementary material are
available from https://github.com/compops/qpmh2-sysid2015/.
Acknowledgements
The simulations were performed on resources provided by the Swedish National Infrastruc-
ture for Computing ( snic) at Linköping University, Sweden.
Appendix
Implementation details
In Algorithm 2 for the lgss model, we use a fully adapted smc algorithm with N = 50
and smc-abc with N = 2, 500, lag ∆= 12, ϵ = 0.10 and ρϵ as the Gaussian density with
standard deviation ϵ. For the αsv, we use the same settings expect for N = 5, 000. For
qpmh2, we use the memory length M = 100, δ = 1, 000 and nhyb = 2, 500 samples for the
hybrid method. We use K = 15, 000 iterations (discarding the ﬁrst Kb = 5, 000 as burn-in)
for all pmh algorithms and initialise in the maximum a posteriori ( map) estimate obtained
accordingly to Dahlin et al. (2015c).
The pre-conditioning matrix P is estimated by pilot runs using pmh0, with step sizes based
on the Hessian estimate obtained in the map estimation. The ﬁnal step sizes are given by
the rules of thumb by Sherlock et al. (2015) and Nemeth et al. (2014), i.e.,
ϵ 2
0 = 2.5622p−1,
ϵ 2
1 = 1.1252p−1/3.
Finally, we use the following prior densities
p(µ)
∼T N (0,1)(µ; 0, 0.22),
p(φ) ∼T N (−1,1)(φ; 0.9, 0.052),
p(σv) ∼G(σv; 0.2, 0.2),
p(α) ∼B(α/2; 6, 2),
where T N (a,b)( · ) denotes a truncated Gaussian distribution on [a, b], G(a, b) denotes the
Gamma distribution with mean a/b and B(a, b) denotes the Beta distribution.
For smc-abc, we require the transformation τ(ˇxt) to simulate random variables from the
two models. For the lgss model, we use the identity transformation ψ(x) = x and the

200
Paper C
Quasi-Newton particle Metropolis-Hastings
µ
posterior estimate
-0.6
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
φ
posterior estimate
0.80
0.85
0.90
0.95
1.00
0
5
10
15
σv
posterior estimate
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0
1
2
3
4
5
6
α
posterior estimate
1.2
1.4
1.6
1.8
2.0
0
1
2
3
4
5
6
Figure 3. Parameter posteriors for (14) for µ (green), φ (orange), σv (purple) and α (magenta)
obtained by pooling the output from 10 runs using qpmh2. The dashed lines indicate the
corresponding estimates from pmh0. Dotted lines and grey densities indicate the estimate of
the posterior means and the prior densities, respectively.

B
Implementation details for quasi-Newton proposal
201
Box-Muller transformation to simulate yt by
τθ(ˇxt) = xt + σe
q
−2 log vt,1 cos(2πvt,2),
where {vt,1, vt,2} ∼U[0, 1].
For the αsv model, we use ψ(x) = arctan(x) as proposed by Yıldırım et al. (2014) to make
the variance in the gradient estimate ﬁnite. We generate samples from A(α, exp(xt)) for the
stability parameter α , 1 by
τθ(ˇxt) = exp(xt/2)
sin(αvt,2)
[cos(vt,2)]1/α

cos (α −1)vt,2

vt,1

1−α
α
,
where {vt,1, vt,2} ∼{Exp(1), U(−π/2, π/2)}. The real-world data in the αsv model is
computed as yt = 100[log(st) −log(st−1)], where st denotes the price of a future contract
on coﬀee obtained from https://www.quandl.com/CHRIS/ICE_KC2.
Implementation details for quasi-Newton proposal
The quasi-Newton proposal adapts the Hessian estimate at iteration k using the previous M
states of the Markov chain. Hence, the current proposed parameter depends on the previous
M states and therefore this can be seen as a Markov chain of order M . Following Zhang
and Sutton (2011), we can analyse this chain as a ﬁrst-order Markov chain on an extended
M -fold product space ΘM . This results in that the stationary distribution,
π(θ1:M ) =
M
Y
i=1
π(θi),
where π(θk,i) is deﬁned as in (2). To proceed with the analysis, we introduce the notation
ψk = {θk, uk} and ψk,1:M\i for the vector ψk,1:M with ψk,i removed for brevity. We can
then update a component of ψk,1:M using some transition kernel Ti deﬁned by
Ti

ψi, ψ′
i
ψk,1:M\i

= δ

ψk,1:M\i, ψ′
k,1:M\i

B

ψi, ψ′
i
ψk,1:M\i

,
where B

ψi, ψ′
i
ψk,1:M\i

denotes the qpmh2 proposal adapted using the last M −1 samples
ψk,1:M\i, analogously to (7). This is similar to a Gibbs-type step in which we update a
component conditional on the remaining M −1 components. Here, this conditioning is
used to construct the local Hessian approximation using the information in the remaining
components. As this update leaves π(θk,i) invariant, it follows that
T  ψk,1:M, ψ′
k,1:M
 = T1 ◦T2 ◦· · · ◦TM
 ψk,1:M, ψ′
k,1:M
,
leaves π(θk,1:M ) invariant. Hence, we can update all components of the M -dimensional
Markov chain during each iteration k of the sampler.
To implement the algorithm, we instead interpret the proposal as depending on the last
M −1 states of the Markov chain. This results in a sliding window of samples, which we use
to adapt the proposal. This results in two minor diﬀerences from a standard pmh algorithm.

202
Paper C
Quasi-Newton particle Metropolis-Hastings
Algorithm 3 Quasi-Newton proposal
Inputs: {θk−M :k−1, uk−M :k−1} (last M states of the Markov chain), δ > 0 (initial Hessian).
Outputs: θ′ (proposed parameter).
1: Extract the M ⋆unique elements from {θk−M+1:k−1, uk−M+1:k−1} and sort them in as-
cending order (with respect to the log-likelihood) to obtain {θ⋆, u⋆}.
2: if M ⋆≥2 then
3:
Calculate sl and yl for l = 1, . . ., M −2 using the ordered pairs in {θ⋆, u⋆} and their
corresponding gradient estimates.
4:
Initialise the Hessian estimate B−1
1
= ρ−1
1 (y⊤
1 y1)−1Ip.
5:
for l = 1 to M ⋆−1 do
6:
Carry out the update (7) to obtain B−1
l+1.
7:
end for
8:
Set ΣBFGS(ψk−M+1:k−1) = −BM ⋆(θ′).
9: else
10:
Set ΣBFGS(ψk−M+1:k−1) = δIp.
11: end if
12: Sample from (15) to obtain θ′.
The ﬁrst is that we center the proposal around the position of the Markov chain at k −M ,
q(θ′|ψk−M+1:k−1) = N θ′; θk−M, ΣBFGS(ψk−M+1:k−1),
(15)
where ΣBFGS(ψk−M+1:k−1) = −BM (θ′) obtain by iterating (7). The second diﬀerence is that
we set {θk, uk} ←{θk−M, uk−M } is the candidate parameter θ′ is rejected. The complete
procedure for proposing from q(θ′
k |ψk−M+1:k−1) is presented in Algorithm 3.
Some possible extensions to this noisy quasi-Newton inspired update are discussed by
Schraudolph et al. (2007) and Zhang and Sutton (2011). These include how to rearrange
the update such that the Hessian estimate always is a psd matrix. In this paper, we instead
make use of the hybrid method discussed by Dahlin et al. (2015a) to handle these situations.
In Schraudolph et al. (2007), the authors also discuss possible alternations to handle noisy
gradients, which could be useful in some situations. In our experience, these alternations
do not always improve the quality of the Hessian estimate as the noisy is stochastic and not
the result of a growing data set as in the aforementioned paper.
Additional results
In this section, we present some additional plots for the lgss example in Section 5.1 using
pmh0 in Figure 4 and qpmh2 in Figure 5. Note that the mixing is better for qpmh2
as the acf decreases quicker as the lag increased compared with pmh0. However due to
the quasi-Newton proposal, we obtain a large correlation coeﬃcient at lag M . This is also
reﬂected in the trace plots, which exhibits a periodic behaviour. Hence, we conclude that
the mixing is increased in some sense when using qpmh2 compared with pmh0. The exact
magnitude of this improvement depends on the method to compute the iact values.

C
Additional results
203
9000
9200
9400
9600
9800
10000
-1.0
0.0
1.0
iteration
µ
0
20
40
60
80
100
120
0.0
0.4
0.8
lag
acf of µ
9000
9200
9400
9600
9800
10000
0.70
0.85
1.00
iteration
φ
0
20
40
60
80
100
120
0.0
0.4
0.8
lag
acf of φ
9000
9200
9400
9600
9800
10000
0.8
1.0
1.2
iteration
σv
0
20
40
60
80
100
120
0.0
0.4
0.8
lag
acf of σv
µ
posterior estimate
-1.0
-0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2.0
2.5
φ
posterior estimate
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0
5
10
15
σv
posterior estimate
0.8
0.9
1.0
1.1
1.2
1.3
0
2
4
6
8
10
Figure 4. Trace plots and acf estimates (left) for µ (green), φ (orange) and σv (purple) in the
lgss model using ppmh0-abc. The resulting posterior estimates (right) are also presented
as histograms and kernel density estimates. The dotted vertical lines in the estimates of the
posterior indicate its estimated mean. The grey lines indicate the parameter prior distributions.

204
Paper C
Quasi-Newton particle Metropolis-Hastings
9000
9200
9400
9600
9800
10000
-1.0
0.0
1.0
iteration
µ
0
20
40
60
80
100
120
0.0
0.4
0.8
lag
acf of µ
9000
9200
9400
9600
9800
10000
0.70
0.85
1.00
iteration
φ
0
20
40
60
80
100
120
0.0
0.4
0.8
lag
acf of φ
9000
9200
9400
9600
9800
10000
0.8
1.0
1.2
iteration
σv
0
20
40
60
80
100
120
0.0
0.4
0.8
lag
acf of σv
µ
posterior estimate
-1.0
-0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2.0
2.5
φ
posterior estimate
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0
5
10
15
σv
posterior estimate
0.8
0.9
1.0
1.1
1.2
1.3
0
2
4
6
8
10
Figure 5. Trace plots and acf estimates (left) for µ (green), φ (orange) and σv (purple) in the
lgss model using qpmh2-abc. The resulting posterior estimates (right) are also presented
as histograms and kernel density estimates. The dotted vertical lines in the estimates of the
posterior indicate its estimated mean. The grey lines indicate the parameter prior distributions.

D
α-stable distributions
205
α-stable distributions
This appendix summarises some important results regarding α-stable distributions. For a
more detailed presentation, see Nolan (2003), Peters et al. (2012) and references therein.
Deﬁnition 1 (α-stable distribution (Nolan, 2003)). An univariate α-stable distribution
denoted by A(α, β, c, µ) has the characteristic function
φ(t; α, β, c, µ) =

exp it µ −|ct|α 
1 −i β tan   πα
2

sign(t)	
if α , 1,
exp
(
it µ −|ct|α f
1 + 2i β
π sign(t) ln |t|
g)
if α = 1,
where α ∈[0, 2] denotes the stability parameter, β ∈[−1, 1] denotes the skewness parame-
ters, c ∈R+ denotes the scale parameter and µ ∈R denotes the location parameter.
The α-stable distribution is typically deﬁned through its characteristic function given in
Deﬁnition 1. Except for some special cases, we cannot recover the probability distribution
function (pdf) from φ(t; α, β, c, µ) as it cannot be computed analytically. These exceptions
are; (i) the Gaussian distribution N(µ, 2c2) is recovered when α = 2 for any β (as the
α-stable distribution is symmetric for this choice of α), (ii) the Cauchy distribution C(µ, c)
is recovered when α = 1 and β = 0, and (iii) the Lévy distribution L(η, c) is recovered
when α = 0.5 and β = 1.
For all other choices of α and β, we cannot recover the pdf from φ(t; α, β, c, µ) due to the
analytical intractability of the Fourier transform. However, we can often approximate the
pdf using numerical methods as discussed in Peters et al. (2012). In this work, we make use
of another approach based on abc approximations to circumvent the intractability of the
pdf. For this, we require to be able to sample from A(α, β, c, µ) for any parameters. An
procedure for this discussed by Chambers et al. (1976) is presented in Proposition 2.
Proposition 2 (Simulating α-stable variable (Chambers et al., 1976)). Assume that we
can simulate w ∼Exp(1) and u ∼U(−π/2, π/2). Then, we can obtain a sample from
A(α, β, 1, 0) by
¯y =

sin[α(u+Tα,β)]
(cos(αTα,β) cos(u))1/α

cos[αTα,β+(α−1)u]
w
 1−α
α
if α , 1,
2
π
  π
2 + βu
tan(u) −β log
π
2 w cos u
π
2 +βu

if α = 1,
(16)
where we have introduced the following notation
Tα,β = 1
α arctan

β tan
 πα
2

.
A sample from A(α, β, c, µ) is obtained by the transformation
y =

c ¯y + µ
if α , 1,
c ¯y +  µ + β 2
π c log c
if α = 1.

206
Paper C
Quasi-Newton particle Metropolis-Hastings
Bibliography
C. Andrieu and G. O. Roberts. The pseudo-marginal approach for eﬃcient Monte Carlo
computations. The Annals of Statistics, 37(2):697–725, 2009.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342,
2010.
L. Bornn, N. Pillai, A. Smith, and D. Woordward. A pseudo-marginal perspective on the
abc algorithm. Pre-print, 2014. arXiv:1404.6298v1.
J. M. Chambers, C. L. Mallows, and B. Stuck. A method for simulating stable random
variables. Journal of the American Statistical Association, 71(354):340–344, 1976.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using gradient and
Hessian information. Statistics and Computing, 25(1):81–92, 2015a.
J. Dahlin, F. Lindsten, and T. B. Schön. Quasi-Newton particle Metropolis-Hastings. In
Proceedings of the 17th IFAC Symposium on System Identiﬁcation (SYSID), pages 981–986,
Beijing, China, October 2015b.
J. Dahlin, M. Villani, and T. B. Schön. Eﬃcient approximate Bayesian inference for models
with intractable likelihoods. Pre-print, 2015c. arXiv:1506.06975v1.
T. A. Dean, S. S. Singh, A. Jasra, and G. W. Peters.
Parameter estimation for hidden
Markov models with intractable likelihoods. Scandinavian Journal of Statistics, 41(4):
970–987, 2014.
A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years
later. In D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering.
Oxford University Press, 2011.
M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73
(2):1–37, 2011.
A. Jasra. Approximate Bayesian computation for a class of time series models. International
Statistical Review, 83(3):405–435, 2015.
A. Jasra, S. S. Singh, J. S. Martin, and E. McCoy.
Filtering via approximate Bayesian
computation. Statistics and Computing, 22(6):1223–1237, 2012.
G. Kitagawa and S. Sato. Monte Carlo smoothing and self-organising state-space model.
In A. Doucet, N. de Fretias, and N. Gordon, editors, Sequential Monte Carlo methods in
practice, pages 177–195. Springer Verlag, 2001.
M. J. Lombardi and G. Calzolari. Indirect estimation of α-stable stochastic volatility models.
Computational Statistics and Data Analysis, 53(6):2298–2308, 2009.
J-M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder. Approximate Bayesian computational
methods. Statistics and Computing, 22(6):1167–1180, 2012.

Bibliography
207
S. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. Cambridge University
Press, 2009.
C. Nemeth, C. Sherlock, and P. Fearnhead. Particle Metropolis adjusted Langevin algo-
rithms. Pre-print, 2014. arXiv:1412.7299v1.
J. Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of Compu-
tation, 35(151):773–782, 1980.
J. Nocedal and S. Wright. Numerical optimization. Springer Verlag, 2 edition, 2006.
J. Nolan. Stable distributions: models for heavy-tailed data. Birkhauser, 2003.
G. W. Peters, S. A. Sisson, and Y. Fan. Likelihood-free Bayesian inference for α-stable
models. Comput. Stat. Data Anal., 56(11):3743–3756, November 2012.
N. Schraudolph, J. Yu, and S. Günter. A stochastic quasi-Newton method for online convex
optimization. In Proceedings of the 11th International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), San Juan, Puerto Rico, March 2007.
C. Sherlock, A. H. Thiery, G. O. Roberts, and J. S. Rosenthal. On the eﬃcency of pseudo-
marginal random walk Metropolis algorithms. The Annals of Statistics, 43(1):238–275,
2015.
S. Yıldırım, S. S. Singh, T. Dean, and A. Jasra. Parameter estimation in hidden Markov mod-
els with intractable likelihoods using sequential Monte Carlo. Journal of Computational
and Graphical Statistics, 24(3):846–865, 2014.
Y. Zhang and C. A. Sutton.
Quasi-Newton methods for Markov chain Monte Carlo.
In Proceedings of the 2011 Conference on Neural Information Processing Systems (NIPS),
Granada, Spain, December 2011.


Paper D
Accelerating pseudo-marginal
Metropolis-Hastings
using correlated likelihood estimators
Authors:
J. Dahlin, F. Lindsten, J. Kronander and T. B. Schön
Edited version of the paper:
J. Dahlin, F. Lindsten, J. Kronander, and T. B. Schön. Accelerating pseudo-
marginal Metropolis-Hastings by correlating auxiliary variables. Pre-print, 2015a.
arXiv:1512.05483v1.


Accelerating pseudo-marginal Metropolis-Hastings
using correlated likelihood estimators
J. Dahlin⋆, F. Lindsten†, J. Kronander‡ and T. B. Schön†
⋆Dept. of Electrical Engineering,
Linköping University,
SE–581 83 Linköping, Sweden.
johan.dahlin@liu.se
†Dept. of Information Technology,
Uppsala University,
SE-751 05 Uppsala, Sweden.
fredrik.lindsten@it.uu.se
thomas.schon@it.uu.se
‡Dept. of Science and Technology,
Linköping University,
SE-581 85 Linköping, Sweden.
joel.kronander@liu.se
Abstract
Pseudo-marginal Metropolis-Hastings (pmmh) is a powerful method for Bayesian inference in models
where the posterior distribution is analytical intractable or computationally costly to evaluate directly.
It operates by introducing additional auxiliary variables into the model and form an extended target
distribution, which then can be evaluated point-wise. In many cases, the standard Metropolis-Hastings
is then applied to sample from the extended target and the sought posterior can be obtained by
marginalisation. However, in some implementations this approach suﬀers from poor mixing as the
auxiliary variables are sampled from an independent proposal. We propose a modiﬁcation to the
pmmh algorithm in which a Crank-Nicolson ( cn) proposal is used instead. This results in that
we introduce a positive correlation in the auxiliary variables. We investigate how to tune the cn
proposal and its impact on the mixing of the resulting pmmh sampler. The conclusion is that the
proposed modiﬁcation can have a beneﬁcial eﬀect on both the mixing of the Markov chain and the
computational cost for each iteration of the pmmh algorithm.
Keywords
Bayesian inference, pseudo-marginal algorithms, auxiliary variables, Crank-Nicolson, particle ﬁltering,
importance sampling.
Data and source code in Python
https://github.com/compops/pmmh-correlated2015
Financial support from
The projects Learning of complex dynamical systems (Contract number: 637-2014-466), Probabilistic
modeling of dynamical systems (Contract number: 621-2013-5524) and cadics, a Linnaeus Center,
all funded by the Swedish Research Council. The Swedish Foundation for Strategic Research ( ssf)
through grant IIS11-0081.
211

212
Paper D
Accelerating pmMH using correlated likelihood estimators
Introduction
We are interested in approximating some probability distribution ¯π(θ, u) using simula-
tion methods based on Markov chain Monte Carlo (mcmc; Robert and Casella, 2004).
In Bayesian inference, ¯π(θ, u) could represent the posterior distribution of the parameters
θ ⊂Θ and some auxiliary variables u ⊂U, which e.g., can be latent states in the model or
missing data. Often, we are interested in the marginal posterior distribution of the parameters
given by
πθ(θ) =
p(θ)p(y |θ)

Θ
p(θ)p(y |θ)dθ
= p(θ)pθ(y)
p(y)
,
(1)
where pθ(y) ≜p(y |θ) denotes the likelihood function and p(θ) denotes the parameter prior
distribution. The denominator p(y) is usually referred to as the marginal likelihood or the
model evidence.
In principle, we can apply mcmc methods to sample directly from (1) using e.g., the
Metropolis-Hastings (mh; Metropolis et al., 1953; Hastings, 1970) algorithm. However,
in practice πθ(θ) can be analytically intractable or too costly from a computationally per-
spective to evaluate point-wise. By introducing the auxiliary variables u, we can sometimes
mitigate these problems and obtain an algorithm which can be of practical use.
This approach is discussed by Andrieu and Roberts (2009), where the pseudo-marginal mh
(pmmh) algorithm is introduced to sample from ¯π(θ, u) using a standard mh sampler. This
approach can be seen as an exact approximation of the ideal algorithm, where πθ(θ) can
be recovered by marginalisation. A concrete example is the particle mh (pmh; Andrieu
et al., 2010), where the auxiliary variables are all the random variables generated in a run
of a sequential Monte Carlo (smc; Del Moral et al., 2006) algorithm. In this setting, the
smc algorithm is used to provide an unbiased estimate of the likelihood function. This is
useful for inference in latent variables models such as state space models (ssms; Flury and
Shephard, 2011; Pitt et al., 2012), mixture models (Fearnhead and Meligkotsidou, 2015) and
generalised linear mixed models (Tran et al., 2014).
A typical problem in pmmh is that the resulting Markov chain can be highly autocorrelated,
which corresponds to a sticky chain and poor exploration of the posterior. This is the
result of that occasionally we obtain an estimate of the posterior which is much larger
than its true value. This problem can typically be mitigated by increasing the number of
auxiliary variables Nu, which increase the accuracy of the estimate of the target. However,
this increases the computational cost for each iteration of the algorithm, which can limit
practical use of the pmmh algorithm for many interesting but challenging models. The
trade-oﬀbetween increasing Nu and the number of iterations K in the pmmh algorithm is
studied by Pitt et al. (2012), Sherlock et al. (2015) and Doucet et al. (2015). They conclude
that Nu should be selected such that the standard deviation in the log-target estimate is
roughly between 1 and 2.
The main contribution of this paper is to introduce a positive correlation in the auxiliary
variables u between two consecutive iterations of the pmmh algorithm. Intuitively, this
allows us to decrease Nu and therefore the computational cost while keeping the mixing of

2
Introducing correlation into the auxiliary variables
213
the Markov chain constant. The correlation is introduced by changing the proposal for u
from the standard independent proposal to a random walk proposal in the space U.
The idea for this originates in the ﬁeld of computer graphics, where a similar approach is
used for rendering images by sampling light paths connecting the light sources in the scene
to the camera using mcmc methods. In Kelemen et al. (2002), the authors propose the
primary sample path space Metropolis light transport algorithm that operates directly on a
set of uniform variates used in a second step to construct a light ray through the scene, using
sequential importance sampling. Recently, Hachisuka et al. (2014) extended the original
algorithm to include multiple importance sampling (mis; Owen and Zhou, 2000; Veach
and Guibas, 1997; Kronander and Schön, 2014) to improve the eﬃciency further.
The idea of introducing correlation in u between iterations has previously been suggested
(independently) in the original particle mcmc ( pmcmc) paper by Andrieu et al. (2010)
and in the connected discussions by Lee and Holmes (2010). Furthermore, a similar idea
of introducing correlation in u is proposed by Andrieu et al. (2012). The main diﬀerence
in our contribution is the use of the Crank-Nicolson (sn; Beskos et al., 2008; Cotter et al.,
2013; Hairer et al., 2014) proposal to update u at every iteration of the algorithm. This in
contrast with the standard pmmh proposal for u, which samples new random variables
independently at each iteration of the algorithm. The cn proposal is a natural choice
as it is known to scale independently with the dimension of the space. Furthermore, we
provide the reader with both a theoretical and numerical analysis of how the mixing and the
computational cost is aﬀected and propose an adaptive algorithm based on the theoretical
results. Deligiannidis et al. (2015) also proposes (independently) to make use of the cn
proposal in this setting.
We consider two diﬀerent numerical examples to illustrate the properties of the proposed
modiﬁcations to the pmmh algorithm. In the ﬁrst example, we infer the parameters in iid
data from the Gaussian distribution to analyse how to tune the cn proposal and show how
it improves upon the idea proposed by Lee and Holmes (2010). In the second example, we
conduct inference of the log-volatility using a stochastic volatility ( sv) model with leverage
on real-world stock index data.
We continue this paper by introducing the proposed modiﬁcations in Section 2 and analyse
its theoretical properties, in a simpliﬁed setting, in Section 3. We end the paper by some
numerical illustrations of the proposed method in Section 4 from which we draw some
general conclusions in Section 5.
Introducing correlation into the auxiliary variables
We would like to sample from the parameter posterior πθ(θ) in (1). However, this is not
possible using a direct implementation of the mh algorithm as we cannot evaluate πθ(θ)
point-wise. Instead, we introduce the auxiliary variables u = (u1, . . ., uNu) as Nu indepen-
dent standard Gaussian random variables, i.e., ui ∼N(0, 1). We make use of the setup
used by Andrieu and Roberts (2009) and Doucet et al. (2015). Furthermore, we make no
notational distinction between a random variable and its realisation for brevity.

214
Paper D
Accelerating pmMH using correlated likelihood estimators
Let the potential function Φθ(u) : Θ × RNu →R be deﬁned such that
E
f
exp   −Φθ(u)g
= c πθ(θ),
∀θ ∈Θ,
and some constant c > 0. Hence, we can deﬁne the extended target distribution as
¯π(θ, u) =
exp   −Φθ(u)
c
N u; 0, INu
,
(2)
where N(u; 0, INu) denotes a Nu-dimensional multivariate standard Gaussian distribution.
Note that the parameter posterior is the marginal of ¯π(θ, u) as

RNu
¯π(θ, u) du = 1
c E
f
exp   −Φθ(u)g
= πθ(θ),
as required in the exact approximation scheme used in the pmmh algorithm. Hence, we
conclude that this is a valid extended target for sampling from the parameter posterior πθ(θ).
A concrete example of a suitable potential function follows from the deﬁnition in (1). In
this case, we have
Φθ(u) = −

log Dpθ(y; u) + log p(θ)

,
exp   −Φθ(u) = Dpθ(y; u)p(θ),
E
f
exp   −Φθ(u)g
= pθ(y)p(θ) = p(y)
|{z}
=c
πθ(θ),
where the constant c corresponds to the marginal likelihood and Dpθ(y; u) denotes the non-
negative and unbiased estimator of the likelihood constructed by the auxiliary variables.
(The typical application of the pmmh algorithm makes use of importance sampling or
particle ﬁltering to construct such an estimator.)
From (2), we have that the target is high-dimensional in u whenever Nu is large, and that
the extended target correspond to a change of measure from the Gaussian prior. In this
setting, we know that the cn proposal is a suitable choice as it is dimension independent
(Cotter et al., 2013). An intuition for this can be given by realising that the cn proposal is a
discretisation of the Ornstein-Uhlenbeck stochastic diﬀerential equation, see e.g., Kloeden
and Platen (1992). The main diﬀerence between using the cn proposal and a standard
random walk proposal is its autoregressive nature, which results in the mean-reverting
behaviour. As a consequence, the cn proposal has the standard Gaussian distribution as its
limiting distribution, which suits us well in this setting. Therefore, we assume a proposal
distribution for u and θ with the form
qθ,u
 {θ′, u′}|{θ, u} = qθ
 θ′|{θ, u}qu
 u′| u
= N θ′; µ(θ, u), Σ(θ, u) N u′;
q
1 −σ2uu, σ2
uINu
.
(3)
This corresponds to using a standard Gaussian random walk for θ, where µ(θ, u) and Σ(θ, u)
denote a mean and covariance function possibly depending on the auxiliary variables in the
previous step, respectively. Note that by introducing u into qθ, we can make use of gradient
and Hessian information as proposed by Dahlin et al. (2015b) to improve the performance
of the algorithm. The cn proposal for u is parametrised by the step length σu ∈(0, 1),

2
Introducing correlation into the auxiliary variables
215
Algorithm 1 Pseudo-marginal Metropolis-Hastings (PMMH)
Inputs: K > 0 (no.
textsc mcmc steps), θ0 (initial parameters), Φθ(u) (potential) and qθ,u (proposal).
Output: {{θ1, u1} . . ., {θK, uK }} (approximate samples from ¯π(θ, u)).
1: Generate u0 ∼p(u0) and compute Φθ0(u0).
2: for k = 1 to K do
3:
Sample {θ′, u′} using the proposal in (3).
4:
Compute exp(−Φθ′(u′)) and α({θ′, u′}, {θk−1, uk−1}) given by (4).
5:
Sample ωk uniformly over [0, 1].
6:
if ωk ≤α({θ′, u′}, {θk−1, uk−1}) then
7:
Accept {θ′, u′}, i.e., {θk, uk} ←{θ′, u′}.
8:
else
9:
Reject {θ′, u′}, i.e., {θk, uk} ←{θk−1, uk−1}.
10:
end if
11: end for
which is determined by the user. We return to discussing how to tune σu in Section 3. Note
that (3) is in contrast with the standard independent pmmh proposal, which we recover for
the choice σu = 1.
The corresponding pmmh algorithm for sampling from (2) follows directly from the choice
of proposal in (3). During iteration k of the algorithm, we ﬁrst sample from (3) to ob-
tain the candidate parameter {θ′, u′} given {θk−1, uk−1}. We then compute the acceptance
probability by
α {θ′, u′}, {θk−1, uk−1} = 1 ∧exp

Φθk−1(uk−1) −Φθ′(u′)
 qθ
 θk−1 |{θ′, u′}
qθ
 θ′|{θk−1, uk−1},
(4)
where we make use of the notation a ∧b ≜min{a, b}. This follows directly from the
properties of the cn proposal (Cotter et al., 2013). In the last step, we accept the candidate
parameter, i.e., set {θk, uk} ←{θ′, u′}, with the probability given by (4). Otherwise, we
reject the candidate parameter and set {θk, uk} ←{θk−1, uk−1}.
The resulting pmmh is presented in Algorithm 1. In Line 4, we need to evaluate the potential
function in {θ′, u′}. In this paper, we make use of importance sampling and particle ﬁltering
in Section 4 to construct Φθ(u). Note that Algorithm 1 only diﬀers from a standard pmmh
algorithm in Lines 3 and 4, where we add a cn proposal for u and evaluate the potential
function Φθ(u). Hence, implementing the new algorithm only requires minor changes to
the existing code.
In many models, we are required to make use of non-Gaussian random variables to generate
samples from the proposal distributions in e.g., importance sampling and particle ﬁltering
algorithms. Furthermore, we require uniform random variables for the resampling step in
the particle ﬁlter, see e.g., Doucet and Johansen (2011). These variables can be obtained by
using a inverse cumulative distribution function ( cdf) transformation also known as a
quantile transformation. Firstly, we compute a uniform random variable by u⋆= Φ(u),

216
Paper D
Accelerating pmMH using correlated likelihood estimators
where Φ(u) denotes the cdf of the standard Gaussian distribution. Secondly, we compute
¯u by a quantile transform of u⋆using the inverse cdf for the distribution that we would
like to simulate a random variable from. There exists other approaches e.g., accept-reject
sampling that also can be useful for simulating random variables when the cdf cannot be
inverted in closed-form, see e.g., Ross (2012) or Robert and Casella (2004).
Theoretical analysis
The aim of the theoretical analysis in this section is to give some guidance of how to tune
σu to obtain a reasonable performance in the pmmh algorithm. More speciﬁcally, we are
interested in determining the value of σu that maximises the mixing and to determine how
this translates into a suitable acceptance rate in the algorithm. We begin by setting up the
model for the analysis in Section 3.1, which depends on the multivariate random variable
u. We then reformulate the model to replace u with a univariate random variable z. This
enables use to make use of an analysis of a discretised model (Gelman et al., 1996; Yang
and Rodríguez, 2013) in Section 3.2 to tune the cn proposal to optimise the mixing in the
Markov chain. In Section 3.3, we report the results of this analysis. We return to the analysis
in Section 4, the optimal tuning of σu is investigated in some practical scenarios.
Setting up the model
To accomplish the aforementioned aim we will study the algorithm in a simpliﬁed setting.
Firstly, since we are primarily interested in the eﬀect of the correlation in the u-variables,
we will start out by making the simplifying assumption that we ﬁx θ in the extended target
¯π(θ, u). Hence, we would like to analyse sampling from
¯π(u) =
exp   −Φ(u)
c
N u; 0, INu
,
(5)
using the cn proposal qu(u′| u) in (3). We believe that this is a reasonable proxy for
the complete model (2) since in many cases only local moves are made in the θ variable.
Following Pitt et al. (2012) and Doucet et al. (2015) we also assume that Φ(u) (which
depend on Nu) follows a central limit theorem ( clt) and that it therefore can be accurately
approximated as being Gaussian distributed:
−Φ(u) ∼N *
,
−
σ2
Φ
2 , σ2
Φ+
-
,
when u ∼N u; 0, INu
.
(6)
for some σΦ > 0. This assumption follows from the properties of the log-likelihood
estimator based on (sequential) importance sampling; see Pitt et al., 2012; Doucet et al.,
2015 for further details. This case is of interested to us as we make use of this type of
estimator in Section 4 for computing an estimate of the parameter posterior distribution.
We can readily check that assumption (6) implies that
−Φ(u) ∼N *
,
σ2
Φ
2 , σ2
Φ+
-
,
when u ∼¯π(u),
(7)

3
Theoretical analysis
217
where ¯π is the Nu-dimensional distribution deﬁned in (5). Consequently, if we deﬁne the
random variable
z ≜σΦ
2 −1
σΦ
Φ(u)
it follows that the law of z under ¯π is given by
Hπ(z) = N(z; σΦ, 1).
Note that z is a one-dimensional variable, which means that we have reduced the high-
dimensional target (5) to one of its one-dimensional marginals. We will study the properties
of the cn proposal in this one-dimensional marginal space.
Indeed, assume that u ∼¯π(u) is distributed according to the target (5) (i.e., it can be viewed
as the state of the Markov chain at stationarity) and that u′| u ∼qu(u′| u) is simulated
from the cn proposal in (3). If can deﬁne z and z ′ in the same manner as above, i.e. as
transformations of u and u′, respectively. From a bivariate clt, we can then obtain a
bivariate Gaussian approximation for the vector (z, z ′)T, suggesting that the cn proposal
for u corresponds to the proposal
Hq(z ′| z) = N

z ′;
q
1 −σ2z, σ2
z

,
(8)
for z ∈R, for some σz > 0. Note that σz in general diﬀers from σu and that it depends
on the non-linear transformation Φ(u). However, it is clear that σu = 0 ⇒σz = 0 and
σu = 1 ⇒σz = 1 and, intuitively, σz is a monotone function of σu. We investigate the
dependence between these two variables numerically in Section 4.1.
Note that the resulting acceptance probability, for the reduced one-dimensional mh algo-
rithm with target Hπ(z) and proposal Hq(z ′| z), is given by
Hα(z, z ′) = 1 ∧exp(σΦz ′ −σΦz),
(9)
which is due to the symmetry of the proposal.
Analysis by discretisation of the state space
We follow Gelman et al. (1996) and Yang and Rodríguez (2013) and analyse the mixing of
the Markov chain using a state space discretisation approach. We know that the expectation
of any integrable test function ϕ : Z →R under the target distribution
Hπ[ϕ] = EHπ[ϕ(z)] =

Z
ϕ(z)Hπ(z) dz,
can be approximated by the ergodic theorem (Meyn and Tweedie, 2009; Tierney, 1994) as
DHπ[ϕ] = 1
K
K
X
k=1
ϕ zk
,
using the samples {z1, z2, . . ., zK } ≜z1:K obtained from the pmmh algorithm. Under the
assumption of geometric ergodicity, we have that the error of the estimate obeys a clt

218
Paper D
Accelerating pmMH using correlated likelihood estimators
(Meyn and Tweedie, 2009; Tierney, 1994) given by
√
K

Hπ[ϕ] −DHπ[ϕ]

d
−→N(0, ν),
where ν denotes the asymptotic variance,
ν = VHπ[ϕ] ·

1 + 2
∞
X
τ=1
Dρτ(z)

|               {z               }
≜IACT(z)
,
(10)
where VHπ[ϕ] and IACT(z) denote the variance of ϕ(z) over Hπ(z) and the integrated autocor-
relation time ( iact), respectively. Here, ρτ(z) = corr(ϕ(zk), ϕ(zk+τ)) denotes the lag-τ
autocorrelation of ϕ.
To optimise the mixing, we would like to determine σz such that 1/ν is as large as possible.
To estimate the asymptotic variance, we partition the interval (zmin, zmax) into L bins of
equal length ∆= (zmax −zmin)/L. Hence, the center point of bin l is given by zl =
zmin + (l −0.5)∆. We can then deﬁne the transition matrix P = {plm} following Yang and
Rodríguez (2013) by
plm =

Hq(zm | zl)Hα(zl, zm)∆,
for l, m ∈{1, . . ., K }, l , m
1 −P
k,l plk,
for l = m
,
(11)
where the proposal (8) and acceptance probability (9) enters into plm. Hence, we can
estimate the acceptance rate (the probability of a jump) by
Pjump =
L
X
l=1
πl(1 −pll),
(12)
where the stationary probability is calculated as
πl = N(zl; σΦ, 1).
Furthermore from the asymptotic results in Peskun (1973) and Kemeny and Snell (1976),
we can estimate ν by
Dν( f , π, P) = ϕ⊤ 2BZ −B −BAϕ,
(13)
where ϕ = [ϕ(z1), ϕ(z2), . . ., ϕ(zL)] and B = diag(π1, π2, . . ., πL). Here, we introduce the
fundamental matrix by Z = [I −(P −A)]−1 and the limiting matrix by A = {alm} with
alm = πm.
Tuning the CN proposal for the univariate case
We make use of a modiﬁed version of the C-code provided by Yang and Rodríguez (2013)
to implement the discretisation approach. We set zmin = −4, zmax = σΦ + 4 and L = 1, 000.
Here, we use ϕ(z) = z to optimise the mixing of the posterior mean. We vary σΦ in
the target and σz in the proposal in {0, 0.25, . . ., 3.5} and {0.05, 0.075, . . ., 1}, respectively.
For each σΦ, we ﬁnd the σz that maximises Pjump computed by (12) and 1/Dν( f , π, P)
calculated by (13). In Figure 1, we present the resulting σz is as a function of σΦ. We

3
Theoretical analysis
219
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
0.0
0.2
0.4
0.6
0.8
1.0
σΦ
σz for optimal ν
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
0.0
0.2
0.4
0.6
0.8
1.0
σΦ
accept rate for optimal ν
Figure 1. Optimal σu (left) and acceptance rate (right) for minimising the 1/Dν( f , π, P).
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
σu
correlation
Figure 2. The estimated correlation (dots) in Dpθ(y; u) for the Gaussian iid model (15) as a
function of σu and a linear regression (green). The linear approximation provides a reasonable
approximation for much of the central part of the interval.

220
Paper D
Accelerating pmMH using correlated likelihood estimators
note that for σΘ ∈[1.0, 1.8] as recommended by e.g., Doucet et al. (2015), we should have
σz ∈[0.95, 0.7] to obtain the optimal mixing and this corresponds to an acceptance rate of
around 75% to 85% in the Markov chain for u.
Note that these results hold for the case when u can be described by the univariate Gaussian
random variable z. We return to numerically investigate how to optimal value of σz in the
cn proposal for the univariate random variable z relates to the corresponding value σu
used in the cn proposal for the multivariate random variable u in Section 4.
Numerical illustrations
In this section, we investigate the numerical properties of the proposed alterations to the
pmmh algorithm. We are especially interested in how the mixing is aﬀected by making use
of the cn proposal instead of an independent proposal for u. For this end with compare
the mixing in two diﬀerent models: (i) a Gaussian iid model with synthetic data and (ii) a
non-linear state space model with real-world data. The implementation details are presented
in Appendix A, where we also describe how to estimate the value of the target distribution
(log-likelihood) for each model.
We quantify the mixing by estimating the iact using the empirical autocorrelation func-
tion. The estimate is computed by
DIF(θKb:K ) = 1 + 2
100
X
τ=1
Dρτ(θKb:K ),
(14)
where Dρτ(θKb:K ) denotes the empirical lag-τ autocorrelation of θKb:K , and Kb denotes the
burn-in time.
Gaussian IID model
Consider the independent and identically ( iid) distributed Gaussian model given by
x0 ∼δ0,
xt ∼N

xt; µ, σ2
v

,
yt | xt ∼N

yt; xt, σ2
e

,
(15)
with parameters θ = {µ, σv, σe} and where δx′ denotes the Dirac measure placed at
x = x′. We generate a realisation with T = 10 observations using the parameters θ⋆=
{0.5, 0.3, 0.1}.
We begin by investigating how the correlation of the log-likelihood estimated using im-
portance sampling (see Appendix A.1) depends on σu. In Figure 2, we present the cor-
relation between two consecutive estimates of the log-likelihood (keeping µ ﬁxed) when
σu ∈{0, 0.05, . . ., 1.0}. We note that the correlation is almost linear when σu > 0.2 and
can be described as corr(Dpθ(y; u), Dpθ(y; u′)) = 1 −σu. We also estimate the standard devia-
tion in the log-likelihood and obtain 3.5. Hence, we conclude that this implies from Figure 1
that the optimal correlation in the log-likelihood estimator is approximately 0.5.
We proceed by ﬁxing σv and σe to their true values and would like to infer the parameter
posterior of µ given the data using Algorithm 1. Here, the potential function corresponds to
the log-likelihood estimator for the importance sampler as discussed in Appendix A.1. The

4
Numerical illustrations
221
σu
α
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
50
60
70
80
90
100
110
120
130
Figure 3. A heatmap of the iact for the Gaussian iid model (15) when varying α and σu in
(16). The results presented are the median from 32 independent Monte Carlo runs.

222
Paper D
Accelerating pmMH using correlated likelihood estimators
aim is to compute the iact over a grid of σv and compare with the optimal theoretical
results from Section 3. We would also like to compare the proposed changes to pmmh
with the suggestions discussed by Lee and Holmes (2010). For this end, we consider a
generalisation of the cn proposal (3) in which we introduce so-called global moves. The
mixture of local and global moves is a popular approach in e.g., computer graphics (Kelemen
et al., 2002) to promote exploration of the entire target space. In our setup, we can introduce
such a mixture into qu(u′| uk−1) and obtain the mixture proposal given by
¯qθ,u
 {θ′, u}|{θ, u} = qθ
 θ′|{θ, u}f
α N u′; 0, INu
 + (1 −α) qu
 u′| uk−1
g
(16)
where we make use of the original proposals for θ and u from (3). Here, α ∈[0, 1] denotes
the probability of a global move for u, where we recover the proposal in (3) by α = 0 and
the proposal discussed by Lee and Holmes (2010) by σu = 0 and α , 0. We recover the
standard pmmh proposal for u when α = 1 and/or σu = 1.
In Figure 3, we present a heatmap of the median iact using (16) when varying α, σu ∈
{0, 0.025, . . ., 1.0}. The smallest iact values are obtain by using σu ∈[0.4, 0.6] with a
small (or zero) value of α. This range of σu corresponds well with the results in Figure 1
with σΦ = 3.5, which results in the optimal σz = 0.4. Furthermore, we note that using
σu = 0 results in worse performance than if σu > 0 independently of the value of α. Hence,
we conclude that there is some beneﬁt of using our proposed modiﬁcation compared with
the approach discussed by Lee and Holmes (2010) in this speciﬁc example. That is, using
local moves seem to be more beneﬁcial than global moves but there could be some merit to
consider a mixture proposal in any case. However, we set α = 0 to simplify the tuning and
make the conclusions more precise regarding the choice of σu.
Stochastic volatility model with leverage
Consider the problem of modelling the volatility in the daily closing prices of the nasdaq
omx30 index, i.e., a weighted average of the 30 most traded stocks at the Stockholm stock
exchange. We extract the T = 747 log-returns using Quandl1 for the period January 2,
2011 and January 2, 2014. To model the underlying volatility, we make use of a stochastic
volatility model with leverage given by
x0 ∼N
 
x0; µ,
σ2
v
 1 −φ22
!
,
(17a)
"xt+1
yt
# 
xt ∼N
 "xt+1
yt
#
;
"µ + φ(xt −µ)
0
#
,
"σ2
v
ρ
ρ
exp(xt)
#!
,
(17b)
with parameters θ = {µ, φ, σv, ρ}. In this model, we would like to infer the parameter
posterior of θ given the data using Algorithm 1. Here, the potential function corresponds
to the log-likelihood estimator using the particle ﬁlter as discussed in Appendix A.2. The
aim is to again compute the iact and compare with the theoretical results from Section 3.
In Figure 4, we present the median acceptance rate and iact for the four parameters in
the model when varying σu ∈{0.05, 0.10, . . ., 1.00}. The maximum iact (over the four
variables) is minimised when σu = 0.55. However, the variation in the iact is signiﬁcant
1The data is available for download from: https://www.quandl.com/data/NASDAQOMX/OMXS30.

4
Numerical illustrations
223
0.0
0.2
0.4
0.6
0.8
1.0
0.10
0.15
0.20
0.25
0.30
σu
acceptance rate
0.0
0.2
0.4
0.6
0.8
1.0
20
25
30
35
40
45
50
σu
IACT of µ
0.0
0.2
0.4
0.6
0.8
1.0
20
25
30
35
40
45
50
σu
IACT of φ
0.0
0.2
0.4
0.6
0.8
1.0
20
25
30
35
40
45
50
σu
IACT of σv
0.0
0.2
0.4
0.6
0.8
1.0
20
25
30
35
40
45
50
σu
IACT of ρ
Figure 4. The acceptance probability (upper) and the resulting iact (middle and lower) for µ
(orange), φ (purple), σv (magenta) and ρ (light green) when varying σu. The results presented
are the median (line) and ﬁrst and third quantiles (shaded area) from 32 independent Monte
Carlo runs.

224
Paper D
Accelerating pmMH using correlated likelihood estimators
time
log-returns
-8
-4
0
4
0
250
500
750
µ
posterior estimate
-1.0
-0.5
0.0
0.5
1.0
0.0
1.0
2.0
0
1000
2000
3000
4000
5000
-1.0
-0.5
0.0
0.5
1.0
iteration
µ
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of µ
φ
posterior estimate
0.90
0.92
0.94
0.96
0.98
1.00
0
10
20
30
40
50
0
1000
2000
3000
4000
5000
0.90
1.00
1.10
iteration
φ
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of φ
σv
posterior estimate
0.00
0.10
0.20
0.30
0
5
10
15
0
1000
2000
3000
4000
5000
0.0
0.1
0.2
0.3
0.4
iteration
σv
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of σv
ρ
posterior estimate
-1.0
-0.8
-0.6
-0.4
-0.2
0
1
2
3
4
5
6
0
1000
2000
3000
4000
5000
-1.0
-0.8
-0.6
-0.4
-0.2
iteration
ρ
0
20
40
60
80
100
-0.5
0.0
0.5
1.0
iteration
ACF of ρ
Figure 5. Upper: the closing log-returns for the nasdaq omxs30 index between January 2,
2011 and January 2, 2014. Lower: the parameter trace (left), autocorrelation function (center)
and resulting posterior estimate (right) for the sv model (17) for µ (orange), φ (purple), σv
(magenta) and ρ (light green). We make use of σu = 0.55 in the cn proposal (3) and the
histograms are computed using the output from 32 independent Monte Carlo runs.

5
Conclusions and future work
225
and a range between 0.4 and 0.8 seems as suitable choices for σu. The standard deviation
of the log-likelihood estimator (around the estimated posterior mean) is 1.2, which by
Section 3 would imply an optimal σz of around 0.9. This is clearly not an appropriate
choice for this model. This can be due to that the Gaussian assumption for Φθ(u) is not
fulﬁlled or that the standard deviation of the log-likelihood estimate varies with θ.
From Figure 4, we conclude that using σu < 1 results in a decrease of the iact comparing
with using an independent proposal for u. The improvement in the mixing is about 1.5
times. We present a speciﬁc case in Figure (5), where we ﬁx σu = 0.55. We conclude that
the chains are mixing well and the posterior estimates are reasonable with the estimated pos-
terior mean θ = {0.19, 0.98, 0.18, −0.70} with standard deviations {0.22, 0.01, 0.04, 0.09}.
Conclusions and future work
The numerical illustrations in Section 4 indicate that we can obtain improvements in the
mixing of the pmmh algorithm by introducing correlation in u. In this paper, we consider
using a cn proposal for introducing the correlation and it seems that selecting σu = 0.5
oﬀers good performance in the settings that we have considered. Hence, implementing the
proposed alterations to the pmmh algorithm only requires changing a small number of lines
of code and hopefully does not introduce any additional tuning parameters for the user. We
are currently working on an adaptive method to further alleviate the work of tuning σu
for the user. The main beneﬁt of our proposed modiﬁcations of the pmmh algorithm is
that we can decrease the number of particles Nu and obtain better mixing at the same time.
Hence, this results in a signiﬁcant decrease in the computational cost for Bayesian inference
based on the pseudo-marginal framework.
Some important additional future work is to extended the theoretical analysis in Section 3
to a more realistic setting, where the inﬂuence of θ also is taking into the account. Also
it would be interesting to make use of Hilbert curve resampling proposed by Gerber and
Chopin (2015) or some tree methods discussed by Lee (2008) in the particle ﬁlter. This
would allow for inference in state space models with a multivariate state process.
It is also possible to make use of u to construct estimates of the gradient and Hessian of
the log-posterior. This information can be included into the proposal for θ as discussed by
Dahlin et al. (2015b,c) to further improve the mixing in the Markov chain. The gradient
can also be used to reduce the variance in a post-processing step, see Mira et al. (2013).
The proposed alterations of the pmmh algorithm can also be useful in models where the
likelihood is intractable as discussed by e.g., Jasra (2015) and Dahlin et al. (2015c). In this
class of models, approximate Bayesian computations (abc; Marin et al., 2012 can be used to
approximate the log-likelihood using importance sampling and particle ﬁltering. However,
in practice these estimates suﬀer from a large variance with results in bad mixing for the
Markov chain. This approach can possible result in a large decrease in the computational
cost for using the pmmh algorithm for inference in models with intractable likelihoods.

226
Paper D
Accelerating pmMH using correlated likelihood estimators
Acknowledgements
The simulations were performed on resources provided by the Swedish National Infrastruc-
ture for Computing ( snic) at Linköping University.
Appendix
Implementation details
In this appendix, we outline the implementation details of the numerical illustrations pre-
sented in Section 4. The implementations for estimating the log-likelihood are based on
standard importance sampling and particle ﬁltering. Extensive treatments of these methods
are found in e.g., Doucet and Johansen (2011) and Robert and Casella (2004).
Gaussian IID model
We make use of an importance sampler with N = 10 to estimate the log-likelihood with the
prior as the importance distribution. This results in that the log-likelihood can be estimated
by the importance weights according to
log Dpθ(y; u) =
TX
t=1
log

N
X
i=1
w(i)
t

−T log N,
(18)
where the weights w(i)
t
are generated using the procedure outlined in Algorithm 2.
For pmmh, we use K = 10, 000 iterations (discarding the ﬁrst Kb = 1, 000 as burn-in) in
Algorithm 1 and initialise in the true parameter θ0 = 0.5. The proposal for θ is a standard
Gaussian random walk proposal (3) with µ(θ, u) = θ and Σ(θ, u) = 0.102. Finally, we make
use of the following prior
p(µ) ∼T N (0,1)(µ; 0, 1),
where T N (−1,1)(µ; 0, 1) denotes a Gaussian distribution truncated to the interval (−1, 1)
with mean 0 and scale 1.
Stochastic volatility model with leverage
For any ssm, we make use of a bootstrap particle ﬁlter (bpf; Doucet and Johansen, 2011)
to estimate the log-likelihood. An ssm with latent states x0:T = {xt }T
t=0 and observations
y1:T is given by
x0 ∼µθ(x0),
xt+1 | xt ∼fθ(xt+1 | xt),
yt | xt ∼gθ(yt | xt),
(19)
where θ ∈Θ ⊆Rp denotes the static unknown parameters. Here, we assume that it is
possible to simulate from the distributions µθ(x0) and fθ(xt+1 | xt) and evaluate gθ(yt | xt)
point-wise. A bpf to estimate Dpθ(y; u) is presented in Algorithm 3.
Hence, the log-likelihood is computed using the same expression (18) as for importance
sampling but the particles x(i)
t
are instead generated by sequential importance sampling

A
Implementation details
227
with resampling. Note that we are required to sort the particles after the propagation step,
see the discussion about smooth particle ﬁlter by Malik and Pitt (2011) for more details. We
make use of the probability transform to generate the uniform random variables required
for the systematic resampling step. Hence, u is a (Nu +1) · (T +1)-variate Gaussian random
variable, where u2:N +1,t is used directly in the propagation step and u1,t is used in the
resampling step after a transformation into a uniform random variable. Here, we make use
of Nu = 50 particles.
For pmmh, we use K = 10, 000 iterations (discarding the ﬁrst Kb = 1, 000 as burn-in) in
Algorithm 1 and initialise the parameters at θ0 = {0.23, 0.98, 0.18, −0.72} obtained using
the approach discussed by Dahlin et al. (2015d). The proposal for θ is a standard Gaussian
random walk proposal (3) with µ(θ, u) = θ. Here, we make use of the rules of thumb
by Sherlock et al. (2015) to select the covariance function. We extract an estimate of the
Hessian using the method by Dahlin et al. (2015d) and set
Σ(θ, u) = 2.5622
p
· 10−4 ·

384
3
−5
−16
3
1
−3
−2
−5
−3
12
3
−16
−2
3
65

.
Finally, we use the following prior distributions
p(µ)
∼N(µ; 0, 22),
p(φ) ∼T N (−1,1)(φ; 0.9, 0.052),
p(σv) ∼G(σv; 2.0, 0.05),
p(ρ) ∼N(ρ; −0.5, 0.22),
where G(a, b) denotes the Gamma distribution with mean a/b.

228
Paper D
Accelerating pmMH using correlated likelihood estimators
Algorithm 2 Likelihood estimation using importance sampling with fixed random numbers
Inputs: y (vector of T observations), Nu ∈N (no. samples) and u ∈U ( T · Nu standard
Gaussian random variables).
Outputs: Dpθ(y; u) (est. of the likelihood).
Note: all operations are carried out over i = 1, . . ., Nu.
1: for t = 1 to T do
2:
Simulate from the proposal by
x(i)
t
= µ + σ2
vu(i)
t ,
using random variables u1:Nu,t.
3:
Calculate the weights by
w(i)
t
= N(yt; x(i)
t , σ2
e ).
4: end for
5: Estimate Dpθ(y; u) by (18).
Algorithm 3 Likelihood estimation using particle filtering with fixed random numbers
Inputs: y (vector of T observations), an ssm (19), Nu ∈N (no. particles) and u ∈U (
(T + 1) · (Nu + 1) standard Gaussian random variables).
Outputs: Dpθ(y; u) (est. of the likelihood).
Note: all operations are carried out over i, j = 1, . . ., Nu.
1: Sample x(i)
0 ∼µθ(x0) using u2:(N +1),1.
2: Set W (i)
0
= N −1
u
as initial weights.
3: for t = 1 to T do
4:
Apply the inverse cdf approach to transform u1,t+1 into a uniform random number
¯u1,t+1.
5:
Apply systematic resampling with ¯u1,t+1 to sample the ancestor index a(i)
t
from a
multinomial distribution with
P a(i)
t
= j = W (j)
t−1.
6:
Propagate the particles by sampling
x(i)
t
∼fθ

x(i)
t
x a(i)
t
t−1

,
using random variables u2:(Nu+1),t+1.
7:
Extend the trajectory by x(i)
0:t =
(
x a(i)
t
0:t−1, x(i)
t
)
.
8:
Sort the particle trajectories according to the current state x(i)
t .
9:
Calculate the particle weights by w(i)
t
= gθ
 yt | x(i)
t

which by normalisation (over i)
gives W (i)
t .
10: end for
11: Estimate Dpθ(y; u) by (18).

Bibliography
229
Bibliography
C. Andrieu and G. O. Roberts. The pseudo-marginal approach for eﬃcient Monte Carlo
computations. The Annals of Statistics, 37(2):697–725, 2009.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342,
2010.
C. Andrieu, A. Doucet, and A. Lee. Discussion on constructing summary statistics for
approximate Bayesian computation. Journal of the Royal Statistical Society: Series B (Statis-
tical Methodology), 72(3):451–452, 2012.
A. Beskos, G. Roberts, A. Stuart, and J. Voss.
MCMC methods for diﬀusion bridges.
Stochastics and Dynamics, 8(03):319–350, 2008.
S. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White. MCMC methods for functions:
modifying old algorithms to make them faster. Statistical Science, 28(3):424–446, 2013.
J. Dahlin, F. Lindsten, J. Kronander, and T. B.
Schön.
Accelerating pseudo-
marginal Metropolis-Hastings by correlating auxiliary variables.
Pre-print, 2015a.
arXiv:1512.05483v1.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using gradient and
Hessian information. Statistics and Computing, 25(1):81–92, 2015b.
J. Dahlin, F. Lindsten, and T. B. Schön. Quasi-Newton particle Metropolis-Hastings. In
Proceedings of the 17th IFAC Symposium on System Identiﬁcation (SYSID), pages 981–986,
Beijing, China, October 2015c.
J. Dahlin, M. Villani, and T. B. Schön. Eﬃcient approximate Bayesian inference for models
with intractable likelihoods. Pre-print, 2015d. arXiv:1506.06975v1.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.
G. Deligiannidis, A. Doucet, and M. K. Pitt. The correlated pseudo-marginal method.
Pre-print, 2015. arXiv:1511.04992v2.
A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years
later. In D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering.
Oxford University Press, 2011.
A. Doucet, M. K. Pitt, G. Deligiannidis, and R. Kohn. Eﬃcient implementation of Markov
chain Monte Carlo when using an unbiased likelihood estimator. Biometrika, 102(2):
295–313, 2015.
P. Fearnhead and L. Meligkotsidou. Augmentation schemes for particle MCMC. Statistics
and Computing (accepted for publication), pages 1–14, 2015.
T. Flury and N. Shephard. Bayesian inference based only on simulated likelihood: particle
ﬁlter analysis of dynamic economic models. Econometric Theory, 27(5):933–956, 2011.

230
Paper D
Accelerating pmMH using correlated likelihood estimators
A. Gelman, G. Roberts, and W. Gilks. Eﬃcient Metropolis jumping rules. Bayesian statistics,
5:599–607, 1996.
M. Gerber and N. Chopin. Sequential quasi Monte Carlo. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 77(3):509–579, 2015.
T. Hachisuka, A. S. Kaplanyan, and C. Dachsbacher. Multiplexed Metropolis light transport.
ACM Transactions on Graphics (TOG), 33(4):100, 2014.
M. Hairer, A. M. Stuart, and S. J. Vollmer. Spectral gaps for a Metropolis-Hastings algorithm
in inﬁnite dimensions. The Annals of Applied Probability, 24(6):2455–2490, 2014.
W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57(1):97–109, 1970.
A. Jasra. Approximate Bayesian computation for a class of time series models. International
Statistical Review, 83(3):405–435, 2015.
C. Kelemen, L. Szirmay-Kalos, G. Antal, and F. Csonka. A simple and robust mutation
strategy for the Metropolis light transport algorithm. Computer Graphics Forum, 21(3):
531–540, 2002.
J. G. Kemeny and J. L. Snell. Finite Markov chains. Springer Verlag, 1976.
P. E. Kloeden and E. Platen. Numerical solution of stochastic diﬀerential equations, volume 23.
Springer Verlag, 4 edition, 1992.
J. Kronander and T. B. Schön. Robust auxiliary particle ﬁlters using multiple importance
sampling. In Proceedings of the 2014 IEEE Statistical Signal Processing Workshop (SSP),
Gold Coast, Australia, July 2014.
A. Lee. Towards smooth particle ﬁlters for likelihood estimation with multivariate latent
variables. Msc thesis, University of British Columbia, 2008.
A. Lee and C. Holmes. Discussion on particle Markov chain Monte Carlo methods. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):327–328, 2010.
S. Malik and M. K. Pitt. Particle ﬁlters for continuous likelihood evaluation and maximisa-
tion. Journal of Econometrics, 165(2):190–209, 2011.
J-M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder. Approximate Bayesian computational
methods. Statistics and Computing, 22(6):1167–1180, 2012.
N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation
of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6):
1087–1092, 1953.
S. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. Cambridge University
Press, 2009.
A. Mira, R. Solgi, and D. Imparato. Zero variance Markov chain Monte Carlo for Bayesian
estimators. Statistics and Computing, 23(5):653–662, 2013.

Bibliography
231
A. Owen and Y. Zhou. Safe and eﬀective importance sampling. Journal of the American
Statistical Association, 95(449):135–143, 2000.
P. H. Peskun. Optimum Monte-carlo sampling using Markov chains. Biometrika, 60(3):
607–612, 1973.
M. K. Pitt, R. S. Silva, P. Giordani, and R. Kohn. On some properties of Markov chain
Monte Carlo simulation methods based on the particle ﬁlter. Journal of Econometrics, 171
(2):134–151, 2012.
C. P. Robert and G. Casella. Monte Carlo statistical methods. Springer Verlag, 2 edition,
2004.
S. M. Ross. Simulation. Academic Press, 5 edition, 2012.
C. Sherlock, A. H. Thiery, G. O. Roberts, and J. S. Rosenthal. On the eﬃcency of pseudo-
marginal random walk Metropolis algorithms. The Annals of Statistics, 43(1):238–275,
2015.
L. Tierney. Markov chains for exploring posterior distributions. The Annals of Statistics, 22
(4):1701–1728, 1994.
M.-N. Tran, M. Scharth, M. K. Pitt, and R. Kohn. Importance sampling squared for Bayesian
inference in latent variable models. Pre-print, 2014. arXiv:1309.3339v3.
E. Veach and L. J. Guibas. Metropolis light transport. In Proceedings of the 24th annual
conference on Computer graphics and interactive techniques, pages 65–76, 1997.
Z. Yang and Carlos E Rodríguez. Searching for eﬃcient Markov chain Monte Carlo pro-
posal kernels. Proceedings of the National Academy of Sciences, 110(48):19307–19312, 2013.


Paper E
Approximate Bayesian inference
for models with
intractable likelihoods
Authors:
J. Dahlin, M. Villani and T. B. Schön
Edited version of the paper:
J. Dahlin, M. Villani, and T. B. Schön. Eﬃcient approximate Bayesian inference
for models with intractable likelihoods. Pre-print, 2015d. arXiv:1506.06975v1.


Approximate Bayesian inference
for models with
intractable likelihoods
J. Dahlin⋆, M. Villani† and T. B. Schön‡
⋆Dept. of Electrical Engineering,
Linköping University,
SE–581 83 Linköping, Sweden.
johan.dahlin@liu.se
†Dept. of Computer and Information
Science, Linköping University,
SE-581 83 Linköping, Sweden.
mattias.villani@liu.se
‡Dept. of Information Technology,
Uppsala University,
SE-751 05 Uppsala, Sweden.
thomas.schon@it.uu.se
Abstract
We consider the problem of approximate Bayesian parameter inference in non-linear state space
models with intractable likelihoods. Sequential Monte Carlo with approximate Bayesian computations
(smc-abc) is an approach to approximate the likelihood in this type of models. However, such
approximations can be noisy and computationally costly which hinders eﬃcient implementations
using standard methods based on optimisation and statistical simulation. We propose a novel method
based on the combination of Gaussian process optimisation ( gpo) and smc-abc to create a Laplace
approximation of the intractable posterior. The properties of the resulting gpo-abc method are
studied using stochastic volatility ( sv) models with both synthetic and real-world data. We conclude
that the algorithm enjoys: good accuracy comparable to particle Markov chain Monte Carlo with a
signiﬁcant reduction in computational cost and better robustness to noise in the estimates compared
with a gradient-based optimisation algorithm. Finally, we make use of gpo-abc to estimate the
Value-at-Risk for a portfolio using a copula model with sv models for the margins.
Keywords
Bayesian inference, approximate Bayesian computations, Gaussian process optimisation, sequential
Monte Carlo, α-stable distributions.
Data and source code in Python
https://github.com/compops/gpo-abc2015
Financial support from
The projects Probabilistic modeling of dynamical systems (Contract number: 621-2013-5524) and cadics,
a Linnaeus Center, both funded by the Swedish Research Council.
235

236
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Introduction
Dynamical modelling of time series data is an essential part of modern econometrics. A
popular model is the state space model ( ssm) used extensively in both macroeconomics
and ﬁnancial economics to capture e.g., the volatility of returns of stocks or other ﬁnancial
commodities. See McNeil et al. (2010), Tsay (2005) and Durbin and Koopman (2012) for a
multitude of other applications within econometrics. Modelling of volatility is an important
problem in ﬁnancial risk management, where marginal models often are combined using a
copula model to compute the Value-at-Risk (var) for a portfolio of assets.
The main problem is that Bayesian inference in non-linear ssms is computationally ex-
pensive, which limits practical application of this type of modelling. Therefore, we aim to
develop an eﬃcient Bayesian approach for parameter inference in non-linear ssms,
x0 ∼µθ(x0),
xt | xt−1 ∼fθ(xt | xt−1),
yt | xt ∼gθ(yt | xt),
(1)
where θ ∈Θ ⊆Rp denotes unknown static parameters. Here, xt ∈X ⊆Rdx and yt ∈
Y ⊆Rdy denotes the latent state and the observations at time t ∈N, respectively. For an
ssm, we say that the likelihood p(y1, y2, . . ., yT |θ) is intractable, when we cannot evaluate
gθ(yt | xt) point-wise. This can be the result of gθ(yt | xt) lacking an analytical closed-form
expression, is deﬁned recursively or is computationally prohibitive to evaluate. The main
object of interest in Bayesian inference is the parameter posterior distribution given by
p(θ |y1:T ) =
p(θ)pθ(y1:T )

Θ
p(θ′)pθ′(y1:T ) dθ′
,
(2)
where p(θ) denotes the parameter prior distribution and we denote the likelihood (with
some slight abuse of notation) as pθ(y1:T ) ≜p(y1, y2, . . ., yT |θ). It follows that the poste-
rior is intractable and cannot be evaluated as the likelihood is intractable.
A possible approach for inference in problems involving intractable likelihoods is provided
by approximate Bayesian computations (abc; Marin et al., 2012). This family of methods
makes use of a reformulation of the inference problem, where the sought after parameter
posterior (2) is perturbed by some noise. This results in that we can apply standard inference
algorithms to estimate the parameters of the perturbed model. The error between the
parameters of the perturbed model and the true model is in general diﬃcult to quantify but
this approach has been successfully applied for many challenging problems.
In our setting, we can make use of an abc version of sequential Monte Carlo (smc-abc;
Jasra et al., 2012) to estimate the likelihood of the perturbed ssm. However, these estimates
suﬀer from problems with a large variance and are computationally expensive. We would
therefore like to design an inference method that only requires a few estimates of the poste-
rior and can handle the noise in them. With these properties in mind, we propose to make
use of Gaussian process optimisation (gpo; Lizotte, 2008; Snoek et al., 2012) for extracting
a Laplace approximation of the posterior. This is an iterative method that operates by con-
structing a surrogate function that mimics the parameter posterior. The resulting surrogate is
smooth and computationally cheap to evaluate. Therefore, standard optimisation methods
can be applied to the surrogate for constructing the Laplace approximation.

1
Introduction
237
The main contribution of this paper is to introduce, develop and numerically study the
resulting gpo-abc algorithm, which is a combination of smc-abc and gpo. We note
in the passing that gpo-abc can be a competitive alternative for inference in any model
where the likelihood is computationally costly to evaluate/estimate. Here, we exemplify
this by combining gpo with abc for inference in intractable ssms. However, there are
other interesting areas where gpo can be a competitive alternative. An example of this is
our earlier work in Dahlin and Lindsten (2014), where we combine smc and gpo to solve
the maximum likelihood inference problem in ssms with tractable likelihoods. Another
example from biology is discussed by Gutmann and Corander (2015), where gpo is used
for parameter inference in predator-prey models from computational biology. A similar
application using abc is considered by Meeds and Welling (2014).
We illustrate the usefulness of gpo-abc by conducting parameter inference in two diﬀerent
stochastic volatility models using both synthetic and real-world data. In this ﬁrst model,
the log-returns are modelled as Gaussian and the likelihood is tractable. Therefore, we can
apply the combination of standard smc and gpo referred to as gpo-smc to approximate
the parameter posterior. This model is therefore useful for investigating the impact of the
perturbation in abc on the accuracy of the posterior approximation. In the second model,
we assume that the log-returns are distributed according to an α-stable distribution (Nolan,
2003; Lombardi and Calzolari, 2009) and this results in that the likelihood is intractable.
Therefore, we cannot make use of standard smc and are required to make use of gpo-abc
for the inference.
We also consider an application from ﬁnancial risk management, where we would like to
estimate the var for a portfolio of assets. Here, we demonstrate that gpo-smc and gpo-
abc can be used for inference of the parameters in the margins of a copula model. The
use of gpo results in a considerable speed-up compared with alternative methods. This
especially for the model with α-stable log-returns and this could encourage its use in this
and related applications.
In our illustrations, we compare the gpo-abc algorithm with some popular alternatives
that also only require samples of the log-posterior. We present results that indicate some in-
teresting advantages with the proposed method. In particular, we obtain: (i) good posterior
approximations using only a small number of posterior estimates, (ii) a smaller computa-
tional cost compared with alternatives, (iii) good robustness to the abc approximation
and noise in the estimates. Altogether, this indicates that the proposed method can be a
competitive alternative for approximate Bayesian inference in some models.
Alternative methods for solving the Bayesian parameter inference problem in ssms with
an intractable likelihood are usually based on statistical sampling or optimisation. An
example of the former is particle Metropolis-Hastings (pmh; Andrieu et al., 2010), where
Jasra (2015) and Dahlin et al. (2015b) discuss similar applications. Examples of the latter
include gradient ascent algorithms discussed by Yıldırım et al. (2014) and the simultaneous
perturbation and stochastic approximation (spsa; Spall, 1987) discussed by Ehrlich et al.
(2015). Some drawbacks with these approaches are that they in general require many samples
from the intractable posterior and are sensitive to noise. The latter problem can be mitigated
by increasing the computational cost but this can hinder practical parameter inference.

238
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Gradient ascent methods also require running an smc-based smoother, which typically is
more computationally costly than only estimating the posterior point-wise using smc-abc.
The paper continues with a discussion of the background and motivation for using gpo
together with an overview of the proposed gpo-abc algorithm. We then continue in
Section 3 with discussing the details of smc-abc, which we make use of to approximate the
intractable log-posterior. In Section 4, we discuss how to construct the surrogate function
that approximates the log-posterior inside gpo-abc. We also show how to make use of the
surrogate function to eﬃciently explore the parameter space by balancing exploration and
exploitation. In Sections 6 and 7, we provide some numerical illustrations of gpo-abc to
illustrate its advantages, discuss some conclusions from the experiments and highlight some
interesting areas for future developments of gpo-abc.
An intuitive overview of GPO-ABC
The aim of the proposed method is to construct a Laplace approximation (Bishop, 2006) of
the parameter posterior given in (2). This approximation is given by completing the square
of a second-order Taylor approximation of the log-posterior and can be expressed as
Dp(θ |y1:T ) = N

θ; DθMAP,
f
−∇2 log p(θ |y1:T )θ=DθMAP
|                         {z                         }
≜J (DθMAP)
g−1
,
(3)
where N(x; µ, Σ) denotes a Gaussian density with mean vector µ ∈R and covariance
matrix Σ > 0. Here, J (DθMAP) denotes the estimate of the Hessian of the log-posterior at
the map estimate given by
DθMAP = argmax
θ ∈Θ
log p(θ |y1:T ),
(4)
where y1:T denotes the recorded observations. This can be seen as a Gaussian approximation
around the mode of the posterior. However, it can also be motivated by the Bernstein-von
Mises theorem (Van der Vaart, 2000), which states that the posterior is asymptotically
Gaussian concentrated around the maximum likelihood estimate with the inverse expected
information matrix as its covariance. It then follows that the Laplace approximation tends
to the true posterior in the limit of inﬁnite number of observations. However, often only a
ﬁnite number of samples are required to obtain reasonable approximations, see e.g., Panov
and Spokoiny (2015).
We encounter two main problems when constructing the Laplace approximation of (2): (i)
the optimisation problem in (4) cannot be eﬃciently solved using standard methods and
(ii) J (DθMAP) is typically diﬃcult to estimate with good accuracy. The ﬁrst problem is a
result of that we can only obtain quite noisy estimates of the log-posterior with a large
computational cost. Consequently, it is diﬃcult to apply spsa and quasi-Newton methods
with numerical gradients, which typically requires many samples of the log-posterior. These
methods can also suﬀer from slow convergence when the variance of the estimates is large,
which further limits their computational feasibility.

2
An intuitive overview of GPO-ABC
239
Conversely, it is possible to estimate gradients of the log-posterior and make use of ﬁrst-
order optimisation methods such as gradient ascent algorithms. However, gradient estimates
are even more costly to obtain and often suﬀers from even larger variance than estimates
of the log-posterior. In analogue, we can use smoothing to estimate the Hessian of the
log-posterior but this approach suﬀers from the same problems as for gradient estimation.
Furthermore, Hessian estimates can often be negative deﬁnite, which is problematic as they
should be interpreted as an inverse covariance matrix. This is the main reason for the second
problem that we encounter when constructing the Laplace approximation.
The main contribution in this paper is that we instead propose to make use of gpo to cir-
cumvent these two diﬃculties by creating the Laplace approximation from the information
obtained by a surrogate function (also known as a response surface). The aim is to create
this surrogate function using samples from the log-posterior obtained by smc-abc. For this
approach to work, we require that the surrogate is smooth, computationally cheap to evalu-
ate and closely resembles the log-posterior around its mode. We can therefore make use of
standard optimisation methods and ﬁnite diﬀerences to construct a Laplace approximation
of the log-posterior by the use of the surrogate function.
gpo is a speciﬁc instance of Bayesian optimisation and operates by sequentially updating
the surrogate function using samples from the log-posterior. In this paper, we make use of
the predictive distribution of a Gaussian process ( gp) as the surrogate function. Hence,
we focus on brieﬂy introducing the gpo algorithm and refer the interested reader to Boyle
(2007), Lizotte (2008) and Brochu et al. (2010) for more detailed accounts. gpo is a useful
method for globally optimising objective functions that are costly to evaluate and possibly
quite noisy. This as we may encode certain prior knowledge regarding the log-posterior into
the gp prior, which can reduce the number of samples required to explore the log-posterior.
Note that the gpo algorithm can possibly be useful for parameter inference in other types
of models than the intractable likelihood models discussed in this paper.
The gpo algorithm is an iterative algorithm, which alternates between the following steps:
(i) compute an approximation of the log-posterior ξk = log Dp(θk |y1:T ) using the smc-
abc algorithm.
(ii) construct a surrogate function by computing the gp predictive posterior
given {θk,ξk} = {θj, ξj}k
j=1.
(iii) evaluate the acquisition rule to determine θk+1.
We have already brieﬂy discussed Steps (i) and (ii). However, Step (iii) requires some ad-
ditional explanation. The gp predictive posterior can be seen as an inﬁnite dimensional
Gaussian distribution, which is described by a mean function and a covariance function.
Hence, we have information about areas where the predictive mean and covariance are
large and this could be useful in determining the next parameter θk+1 to sample the log-
posterior in. This is known as the exploration and exploration problem, i.e., when we sample
parameters in areas where the covariance and mean are large, respectively. The heuristic that
balances these two phases is known as an acquisition function in the gpo literature and is
the second reason for why gpo is a competitive method for global optimisation. We return
to discussing the use of the gp and the acquisition function in more detail in Section 4.

240
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Note that (3) only provide us with an approximation of p(θ |y1:T ) in (2). Therefore, it could
be advantageous to consider gpo-abc as an initial step for parameter inference using the
pmh algorithm. The ﬁrst alternative is to use the Laplace approximation as an independent
proposal as discussed by Andrieu et al. (2010) and Pitt et al. (2012). The second alternative is
to use the information to pre-condition a Gaussian random walk as discussed by Girolami
and Calderhead (2011) and Sherlock et al. (2015). Hence, gpo-abc could possibly help
with the design of a proposal distribution and initialisation in pmh. Both problems are very
challenging in practice when the number of parameters in the model is large.
Estimating the log-posterior
In this section, we discuss how to estimate the log-posterior that is required for carrying
out Step (i) of gpo-abc. Remember that the main diﬃculty is that the log-likelihood is
intractable. For a general ssm, we can write the complete data log-likelihood as
log pθ(x0:T, y1:T ) = log µθ(x0) +
TX
t=1
f
log fθ(xt | xt−1) + log gθ(yt | xt)
g
,
which we can marginalise to obtain the log-likelihood according to
log pθ(y1:T ) =

log pθ(x0:T, y1:T ) dx0:T .
However, this integral is intractable in all models except in the linear Gaussian case or when
the state space is ﬁnite. Instead, we propose to use smc methods known as particle ﬁlters
to sequentially estimate the predictive log-likelihood log pθ(yt |y1:t−1) and make use of
log pθ(y1:T ) = log pθ(y1) +
TX
t=2
log pθ(yt |y1:t−1),
to estimate the log-likelihood. Note that the predictive likelihood can be computed using
pθ(yt |y1:t−1) =

pθ(yt, xt |y1:t−1) dxt =

gθ(yt | xt) fθ(xt | xt−1)pθ(xt−1 |y1:t−1) dxt−1:t.
(5)
A standard approach to estimate this quantity is to obtain an estimate of the marginal
ﬁltering distribution pθ(xt−1 |y1:t−1) as a collection of weighted Dirac measures
pN
θ (dxt−1 |y1:t−1) =
N
X
i=1
w(i)
t−1δx(i)
t−1(dxt−1),
(6)
where x(i)
t−1 and w(i)
t−1 denotes the particle i at time t −1 and its normalised weight, respec-
tively. Here, δx denotes the Dirac measure placed at x. The particle system {w(i)
t , x(i)
t }N
i=1
can be generated using the particle ﬁlter. However, the standard particle ﬁlter cannot be
directly used as we assume that gθ(yt | xt) cannot be evaluated point-wise.
Consequently, we cannot estimate the predictive likelihood using (5) and the output of the
particle ﬁlter. To circumvent this problem, we propose to make use of abc methods (Marin

3
Estimating the log-posterior
241
et al., 2012) to reformulate the model. This is done so that we only are required to simulate
from gθ(yt | xt) and not evaluate it. After this reformulation, we can apply a standard smc
algorithm to estimate the log-likelihood.
Particle filtering with approximate Bayesian computations
abc is based on the idea that we augment the posterior with an auxiliary variable ˇy1:T ,
which is the data that we simulate from gθ(yt | xt) for t = 1, . . .,T . The resulting posterior
can be expressed as
pϵ(θ, ˇy1:T |y1:T ) =
p(θ)pθ

ˇy1:T

ρϵ

ˇy1:T −y1:T


Θ
p(θ′)pθ′

ˇy1:T

ρϵ

ˇy1:T −y1:T

dθ′
.
For a small enough tolerance parameter ϵ > 0, we assume that we can marginalise the
augmented posterior with respect to ˇy1:T to obtain
pϵ(θ |y1:T ) =

pϵ(θ, ˇy1:T |y1:T ) dˇy1:T,
where pϵ(θ |y1:T ) denotes the posterior of some perturbed model. Here, we make use of
some density ρϵ with tolerance parameter ϵ (in non-parametric statistics, ϵ is usually re-
ferred to as the bandwidth of the kernel ρϵ ) to compute the distance between the simulated
observations ˇyt and the true observations yt. A common choice that we make use of in
this paper is the Gaussian density with standard deviation ϵ, i.e., ρϵ = N(0, ϵ 2). The fun-
damental assumption of abc is therefore that data ˇyt generated from gθ(yt | xt) should be
similar to the observed data yt if θ is selected properly.
To make use of this in the particle ﬁlter, we reformulate the model following the procedure
discussed by Jasra et al. (2012). First, the observed data y1:T is perturbed by noise generated
from the density ρϵ resulting in
ˇyt = ψ(yt) + zt,
zt ∼ρϵ,
(7)
where ψ denotes some suitable one-to-one transformation. For example, Yıldırım et al.
(2014) propose to make use of ψ(x) = arctan(x) to stabilise the variance of the likelihood
(and gradient estimate) of an ssm. We later adopt this transformation in Section 6.2, where
we consider a stochastic volatility model with α-stable log-returns.
Secondly, we assume that it is possible to simulate from the model using a transformation
of some random variables. This corresponds to that we can write ˇyt = τθ(vt, xt) for some
deterministic function τθ, where vt ∼νθ(vt | xt) denotes some random variable that can
be easily generated using standard methods. An example of this is simulation from the zero-
mean symmetric α-stable distribution A(α, γ) discussed in Section 6.2. First, we sample
v(1)
t
∼Exp(1) and v(2)
t
∼U(−π/2, π/2). Then, we can obtain a sample (when α , 1) by
ˇyt = γ
sin  αv(2)
t


cos(v(2)
t )1/α
"cos (α −1)v(2)
t

v(1)
t
# 1−α
α
,

242
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Algorithm 1 Sequential Monte Carlo with approximate Bayesian computations
Inputs: ˇy1:T (perturbed data), the reformulated ssm (8), N ∈N (no. particles), ρϵ (density)
with ϵ ∈R+ (tolerance parameter).
Outputs: log DpN
θ (ˇy1:T ) (est. of log-likelihood).
Note: all operations are carried out over i, j = 1, . . ., N .
1: Sample ˇx(i)
0 ∼µθ(x0)νθ(v0 | x0) and set w(i)
0 = 1/N .
2: for t = 1 to T do
3:
Resample the particles using systematic resampling with probabilities given by (9).
4:
Propagate the particles by (10) and extend the trajectory by ˇx(i)
0:t =
(
ˇx a(i)
t
0:t−1, ˇx(i)
t
)
.
5:
Calculate the particle weights by (11) which by normalisation (over i) gives w(i)
t .
6: end for
7: Compute log DpN
θ (ˇy⋆
1:T ) by (12).
see Nolan (2003) for more information on how to generate α-stable random numbers. By
making use of the perturbation in (7) and the simulation of ˇyt using vt, we can rewrite the
ssm (1) as
ˇxt | ˇxt−1 ∼Ξθ(ˇxt | ˇxt−1) = νθ(vt | xt) fθ(xt | xt−1),
(8a)
ˇyt | ˇxt ∼ℎθ,ϵ(ˇyt | ˇxt) = ρϵ

ˇyt; ψ(τθ(ˇxt))

,
(8b)
where ˇx⊤
t = (x⊤
t , v⊤
t ) denotes the new state vector. We can now apply a standard particle
ﬁlter for this new model, which does not require us to evaluate the gθ(yt | xt) point-wise
but only that we can simulate from it using τθ(vt, xt).
A particle ﬁlter (Doucet and Johansen, 2011) is an iterative algorithm with three steps that
allows us to generate a particle system to construct the empirical distribution in (6). Assume
that we have the particle system at time t −1 and would like to construct the particle system
at time t. To achieve this, we apply three operations: (i) resampling, (ii) propagation and
(iii) weighting.
In the ﬁrst step, we sample a so-called ancestor index a(i)
t
from a multinomial distribution
given by
P

a(i)
t
= j

= w(j)
t−1,
for i, j = 1, . . ., N .
(9)
The ancestor index a(i)
t
can be interpreted as the parent from which the particle x(i)
t
is a
descendent. This step is carried out to randomly multiply particles with a large weight and
discard particles with a small weight. While introducing some variance into the estimate,
this step is essential to focus our attention to the parts of the state space of interest. In this
paper, we make use of the implementation known as systematic resampling (Kitagawa, 1996),
which is usually recommended for many practical applications.
In the second step, we propagate the particle system to time t by simulating the system

3
Estimating the log-posterior
243
forward in time. This is done by sampling
ˇx(i)
t
∼Ξθ

ˇxt | ˇx a(i)
t
t−1

,
for i = 1, . . ., N .
(10)
Finally in the third step, we compute the (unnormalised) weight of each particle by
W (i)
t
= ℎθ,ϵ

ˇyt | ˇx(i)
t

,
for i = 1, . . ., N .
(11)
which after normalisation (over i) gives w(i)
t . The complete procedure is presented in
Algorithm 1 and has a computational cost in the order O( nt), where N denotes the
number of particles. This implementation is known as the bootstrap particle ﬁlter (bpf)
but there are other interesting alternatives. For example, the alive particle ﬁlter (Jasra, 2015)
is useful when the boxcar kernel is used, i.e., ρϵ(x) = U−ϵ/2,ϵ/2)(x), as the particle ﬁlter
otherwise collapses when all weights are zero. Another interesting alternative when using
the boxcar kernel is to adapt ϵ using the approach discussed by Del Moral et al. (2012).
The estimator and its statistical properties
An estimator for the log-likelihood of the perturbed model (8) is given by
log DpN
θ (y1:T ) =
TX
t=1
log

N
X
i=1
W (i)
t

−T log N,
(12)
where we use the unnormalised weights from the particle system generated by Algorithm 1.
It is known from Pitt et al. (2012) that the estimator (12) is biased for a ﬁnite number of
particles but it is still consistent and asymptotically normal. Furthermore, we have that the
error in the log-likelihood estimate fulﬁls a clt given by
√
N

log pθ(y1:T ) −log DpN
θ (y1:T ) + γ2(θ)
2N

d
−→N

0, γ2(θ)

,
N →∞,
(13)
for some unknown variance γ(θ). As a result, we have an expression for the bias of the
estimator given by −γ2(θ)/2N for a ﬁnite number of particles. Consequently, it is possible
to compensate for the bias as the variance of the estimator γ2(θ) can be estimated using
Monte Carlo simulations by repeated application of the particle ﬁlter on the same data.
Note that the log-likelihood estimator in (12) is asymptotically unbiased with respect to the
perturbed model (8), not the true model. Therefore, inference for Dθ using (12) corresponds
to inference in a misspeciﬁed model as discussed by Wilkinson (2013). This view is also
adopted by Dean and Singh (2011) and Dean et al. (2014), where it is concluded that this
results in a bias in the parameter estimates (compared with the unperturbed model) that
decrease proportional to O(ϵ 2) under some regularity assumptions.
However, the asymptotic normality of the log-likelihood estimator holds even with a non-
zero ϵ. Furthermore, Dean and Singh (2011) shows that we also retain Bernstein-von Mises
type theorem for the posterior estimate based on the estimator for a small enough ϵ. This is
an important fact to motivate the Laplace approximation as discussed in Section 2. In prac-
tice, it is diﬃcult to quantify the size of the bias and we return to a numerical investigation
of this in Section 6.1.

244
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Constructing the surrogate of the log-posterior
In this section, we discuss Steps (ii) and (iii) of gpo-abc, where we construct a surrogate
function to mimic the log-posterior. Good references for further information about general
gpo approaches are found in Boyle (2007), Lizotte (2008) and Brochu et al. (2010).
Gaussian process prior
gps (Rasmussen and Williams, 2006) are an instance of Bayesian non-parametric models
and can be seen as a generalisation of the multivariate Gaussian distribution to an inﬁnite
dimensional setting. As such, it is a collection of random variables, where each ﬁnite subset
is jointly distributed according to a Gaussian distribution. A realisation drawn from a gp
can therefore be seen as an inﬁnite vector of values, which can be seen as a function over
the real space Rp. This is why the gp can be viewed as a prior over functions on Rp. To
construct the surrogate function, we assume a priori that the log-posterior is distributed
according to a gp as
log p(θ |y1:T ) ∼GP m(θ), κ(θ, θ′).
(14)
As the gp is a Gaussian distribution of inﬁnite dimension, we cannot characterise it using
a mean vector and covariance matrix. Instead, we introduce a mean function m(θ) and a
covariance function κ(θ, θ′) deﬁned by
m(θ) = E
f
log p(θ |y1:T )
g
,
κ(θ, θ′) = E
f
log p(θ |y1:T ) −m(θ)
 
log p(θ′|y1:T ) −m(θ′)
g
.
Here, the mean function speciﬁes the average value of the process and the covariance func-
tion speciﬁes the correlation between (nearby) samples. Both functions are considered to
be prior choices to the algorithm and are used to encode the beliefs about the log-posterior
before we observed the samples from it. From the clt in (13), we know that the error in
the log-posterior is asymptotically Gaussian. Therefore, we assume that
ξk = log DpN (θk |y1:T ) = log p(θk |y1:T ) + σξzk,
zk ∼N(0, 1),
(15)
where σ2
ξ denotes some unknown variance estimated in a later stage of the algorithm.
Consequently, we have that both the prior (14) and the estimates of the log-posterior (15) are
distributed according to Gaussian distributions. Hence, the posterior resulting from Bayes’
theorem is a Gaussian distribution with some mean and covariance that can be calculated
using standard results. From this posterior, we can construct the predictive posterior for any
test point θ⋆∈Θ given by
log p(θ⋆|y1:T )|Dk ∼GP

µ(θ⋆|Dk), σ2(θ⋆|Dk) + σ2
ξ

,
(16)
where we have introduced the notation Dk = {θk,ξk} for the information available about
the parameter log-posterior at iteration k. Here, the posterior mean and covariance func-
tions are given by
µ(θ⋆|Dk) = m(θ⋆) + κ(θ⋆,θk)
f
κ(θk,θk) + σ2
ξIk×k
g−1(
ξk −m(θ⋆)
)
,
(17a)
σ2(θ⋆|Dk) = κ(θ⋆, θ⋆) −κ(θ⋆,θk)
f
κ(θk,θk) + σ2
ξIk×k
g−1κ(θk, θ⋆).
(17b)

4
Constructing the surrogate of the log-posterior
245
Hence, we can construct the surrogate function of the log-posterior by using (16) obtained
from the gp. The major cost in computing the predictive posterior is incurred by the
matrix inversion which has a computational cost of order O(K 3), where K denotes the
maximum number of iterations. However, in all our applications it is suﬃcient with K <
300, which results in a calculation that is easily handled by modern computers and is much
less demanding than running the particle ﬁlter. However, it could be of interest to use a
recursive and sparse formulation of the gp to decrease computational cost and memory
requirements. Especially, when p is large and therefore K needs to grow to properly explore
the parameter space. We return to discuss this issue is Section 7.
It is common that the log-posterior varies widely over the parameter space Θ, where it in
some areas can be almost ﬂat and fall oﬀquickly in others. This problem often arises when
the number of parameters p increases above four or ﬁve. It is therefore important to make
use of ard covariance functions (with diﬀerent length scales for each parameters) and to
normalise the log-posterior estimates before calculating the predictive posterior. In some
models, this is not enough and it could be necessary to consider non-stationary covariance
functions. However, we did not experience any major problems in the models considered
in this paper but it is always recommended to plot at the marginal predictive distributions
to check for potential problems.
Design of priors and estimating hyperparameters
In this paper, we assume a zero prior mean function m(θ) = 0 and the combination of a
bias and the Matérn 5/2 covariance functions with ard given by
κ(θ, θ′) =
p
X
r=1
"
κ0,r + σ2
κ

1 +
√
5(θr −θ′
r)
`r
+ 5(θr −θ′
r)2
3`2r

exp

−
√
5(θr −θ′
r)
`r
#
, (18)
where λ = {κ0,1:p, σ2
κ, `1:p} denotes the hyperparameters of the covariance function. Note,
that the bias term κ1:p and length scales `1:p are vector valued with p elements (one for each
parameter in θ). This type of prior is equivalent with using a constant mean function, i.e.,
m(θ) = c for some constant c ∈R, and the Matérn 5/2 function to describe the covariance
structure with a non-zero mean.
The choice of covariance function in (18) results in a prior for the log-posterior with some
non-zero mean and two continuous derivatives. These are reasonable assumptions as this
kind of smoothness is assumed in the Laplace approximation. Both stronger and weaker
smoothness properties can be assumed by replacing the Matérn 5/2 covariance function
with either a squared exponential or Matérn 3/2 covariance function, respectively. Another
interesting choice for the mean function is the log-prior, i.e., m(θ) = log p(θ). See Ras-
mussen and Williams (2006) for more information regarding the design of gp prior.
In gp models, it is common to infer the hyperparameters of the covariance function from
the data. There are a number of diﬀerent approaches to this based on both maximum
likelihood and Bayesian inference. Some common Bayesian approaches include Hamiltonian
Markov chain Monte Carlo (Saatçi et al., 2010; Neal, 2010) and smc (Gramacy and Polson,
2011; Svensson et al., 2015).

246
Paper E
Approximate Bayesian inference for models with intractable likelihoods
In this paper, we make use of a maximum likelihood approach and estimate the hyperpa-
rameters using empirical Bayes ( eb), i.e., by optimising the marginal likelihood with respect
to λ. The marginal likelihood can be computed in closed-form by marginalisation and can
be optimised by a standard gradient-based algorithm. This procedure is often carried out
for a number of random initialisations, so that the optimisation does not get stuck in some
local optima.
Acquisition function
In this section, we discuss how to select the next parameter to sample the parameter posterior
in, given the Gaussian process model from Step (ii). The aim is to construct a heuristic
that selects the next sampling point given the information available up until the current
iteration of the algorithm. Using the acquisition rule AQ(θ⋆|Dk), the next point in which
to sample the log-posterior is selected as
θk+1 = argmax
θ⋆∈ΘGPO
AQ(θ⋆|Dk),
where ΘGPO denotes a search space deﬁned by the user. Note that this optimisation is
relatively cheap to carry out as we only need to evaluate the gp predictive posterior for a
number of test points θ⋆and not sample the log-posterior in these points.
There are a number of diﬀerent acquisition rules in the literature and constructing new
alternatives is a current and active research ﬁeld. In this paper, we make use of the expected
improvement ( ei) (Jones, 2001) as it is generally recommended for gpo applications (Li-
zotte, 2008). To derive the ei rule, consider the predicted improvement deﬁned as
PI(θ⋆) = max
(
0, log p(θ⋆|y1:T ) −µmax −ζ
)
,
∀θ⋆∈ΘGPO,
(19)
where µmax denotes the maximum value of µ(θ) for the sampled points θ ∈θk. Here,
we introduce ζ as an user deﬁned parameter that controls the exploitation/exploration
behaviour as discussed by Lizotte (2008). Hence for ζ = 0, we have that PI(θ⋆) is the
diﬀerence between the posterior and the maximum value it assumes in the sampled points.
Therefore, it is positive for points where the log-posterior is larger than the current peak
and zero for all other points.
From (16), we know that the predictive posterior is Gaussian with mean µ(θ⋆|Dk) and
variance σ2(θ⋆|Dk). Using this, we can compute the expectation of (19) with respect to the
predictive posterior. From this calculation, we obtain the ei rule (Jones, 2001) as
EI(θ⋆|Dk) = E
f
PI(θ⋆)|Dk
g
=
∞

µmax+ζ
PI(θ⋆)
1
p
2πσ2(θ⋆|Dk)
exp

−1
2
 θ⋆−µ(θ⋆|Dk)
σ2(θ⋆|Dk)
2
dθ⋆,
= σ(θ⋆|Dk)
f
Z(θ⋆)Φ Z(θ⋆) + φ Z(θ⋆)g
, with
(20)
Z(θ) = σ−1(θ⋆|Dk)
f
µ(θ⋆|Dk) −µmax −ζ
g
,
where φ and Φ denotes the density and distribution function of the standard Gaussian
distribution, respectively. Here, we introduce the standardised variable From practical expe-

5
The GPO-ABC algorithm
247
rience of the authors, it is often useful to add some noise to θk+1 when making inference in
ssms. This is done to improve the exploration of the area around the peak, thus increasing
the accuracy of the obtained parameter estimates. This so-called jittering amount to adding
some noise to the estimate, i.e.,
θk+1 = argmax
θ⋆∈ΘGPO
EI(θ⋆|Dk) + zk,
zk ∼N(0, Σ),
(21)
where Σ denotes a covariance matrix determined by the user. Similar approaches are dis-
cussed by Bull (2011) and Gutmann and Corander (2015) to improve the convergence rate
of the algorithm and the exploration of the parameter space, respectively.
The optimisation in (21) is possibly non-convex and therefore diﬃcult to carry out in a
global setting. Two common approaches in gpo are: (i) using a few local gradient-based
algorithms initialised in randomly selected points in ΘGPO (Lizotte, 2008), and (ii) using
some global optimisation algorithm e.g., the gradient-free dividing rectangles (direct;
Jones et al., 1993) algorithm over ΘGPO. Note that we can determine a suitable search space
ΘGPO from the support of the prior distribution p(θ).
The GPO-ABC algorithm
In this section, we combine the developments in the two previous sections to put together
gpo-abc and discuss its properties. In Algorithm 2, we outline the complete procedure
that combines smc-abc from Algorithm 1 to approximate the log-posterior and gpo to
create a surrogate function that mimics the log-posterior.
Initialisation and convergence criteria
We initialise Algorithm 2 at Line 1 to ﬁnd some suitable hyperparameters for the gp prior.
This is important as gpo-abc may fail to converge to an accuracy parameter estimate if
it is not properly initialised. The hyperparameters are determined by eb using L samples
from the log-posterior obtain by some sampling scheme. There are at least three diﬀerent
alternative for this: (i) uniform sampling, (ii) quasi Monte Carlo sampling and (iii) Latin
hypercube sampling. See Glasserman (2004) for a discussion. All these approaches are
carried out over the set ΘGPO discussed in the previous section.
We then execute Algorithm 1 for each of the sampled parameters {θ⋆
1 , θ⋆
2 , . . ., θ⋆
L} to obtain
D⋆
L by the analogue of Line 4 in Algorithm 2. After the initialisation of the algorithm,
we can update the hyperparameters with Line 5 at every iteration or at some pre-deﬁned
interval. Estimating the hyperparameters is computationally costly and it is therefore rec-
ommended to reestimate them at some ﬁxed interval.
gpo-abc can be executed for some pre-deﬁned number of iterations K or using some other
suitable convergence criteria. An interesting alternative in the gpo literature is to run the
algorithm until the ei is smaller than some ∆EI > 0, i.e., until k satisﬁes EI(θk |D) < ∆EI.

248
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Algorithm 2 Gaussian process optimisation with sequential Monte Carlo using ap-
proximate Bayesian computations (gpo-abc)
Inputs: Algorithm 1, p(θ) (parameter prior), m(θ) (mean function), κ(θ, θ′) (covariance
function), θ1 (initial parameter), Σ (jittering covariance matrix) and ΘGPO (optimisation
bounds).
Output: DθMAP (est. of the parameter) and D
J (DθMAP) (est. of posterior covariance).
1: Initialise the gp prior by running eb on some initial sampled data D⋆
L.
2: Initialise the parameter estimate in θ1 and set k = 1.
3: while convergence criteria is not satisﬁed do
4:
Estimate log DpN (θk |y1:T ) = log DpN
θk (y1:T ) + log p(θk) using Algorithm 1 and set
Dk = {D⋆
L,θk,ξk}.
5:
(if required) Run eb to update the hyperparameters of the gp prior using Dk.
6:
Construct log p(θ⋆|y1:T )|Dk using (16) and (17).
7:
Compute µmax = argmaxθ ∈θk µ(θ |Dk).
8:
Compute θk+1 by (21) with (20) using e.g., direct over ΘGPO.
9:
Set k = k + 1.
10: end while
11: Compute the map estimate Dθ by optimising µ(θ |Dk−1) using direct over ΘGPO.
12: Extract the Hessian estimate J (DθMAP) using e.g., ﬁnite diﬀerences.
Extracting the Laplace approximation
After running Algorithm 2, we compute the predictive posterior mean function µ(θ⋆|D),
which hopefully is an accurate surrogate for the log-posterior. We can then proceed to
extract the map estimate DθMAP deﬁned by (4). As the surrogate is smooth and cheap to
evaluate, we can carry out the optimisation using standard methods, e.g., the direct
algorithm, gradient-based methods or a quasi-Newton algorithm. Hence, we circumvent
the two problems discussed in Section 2, i.e., that we can only get noisy point-wise estimates
of the log-posterior.
To ﬁnally construct the Laplace approximation of the posterior (3), we require an estimate
of the Hessian of the log-posterior at DθMAP denoted by J (DθMAP). This can be computed
by taking the Hessian with respect to θ⋆of the predictive mean function (17). Hence, com-
puting the Hessian of the mean function amounts to computing Hessians of the covariance
functions. This can be implemented eﬃciently for some covariance functions (e.g., the
squared exponential).
For other choices of covariance functions, we can apply some ﬁnite diﬀerence scheme
to approximate the Hessian numerically. We note that this estimate is obtained directly
as a by-product when using a quasi-Newton algorithm to solve (4). When we use the
direct algorithm, we can compute an estimate of the Hessian in a separate step using
ﬁnite diﬀerences with an adaptive scheme with Romberg extrapolation to obtain estimates
with a good accuracy. Another approach is to make use of a smc-abc smoother to estimate
the Hessian (Poyiadjis et al., 2011; Dahlin et al., 2015a). However, this is computationally
costly and can result in a negative-deﬁnite estimate of the posterior covariance.

6
Numerical illustrations and real-world applications
249
Convergence properties
As previously discussed, gpo( -abc) is an eﬃcient approach for global optimisation when
the objective function is costly to evaluate and possibly noisy. The convergence properties
typically depends on the choice of the gp prior and are discussed by Bull (2011) and Vazquez
and Bect (2010). They conclude that gpo using the ei rule samples the log-posterior densely
if it is continuous with respect to the gp prior. Also, gpo achieves an optimal convergence
rate of the order O (n log n)−5/p(log n)1/2
for the Matérn 5/2 covariance function, where
n and p denote the number of samples and parameters to infer, respectively.
Numerical illustrations and real-world applications
In this section, we provide three illustrations of the usefulness of the proposed algorithm.
In the ﬁrst illustration, we study the accuracy and convergence rate of the gpo-abc al-
gorithm as well as the impact of the tolerance parameter ϵ on the accuracy of the poste-
rior approximation. In the second illustration, we make use of gpo-abc for modelling
log-returns of future contracts on coﬀee. In the third illustration, we consider an appli-
cation from ﬁnancial risk management, where we would like to calculate the var for
a portfolio of oil futures. This problem serves as an illustration of a setting where the
gpo-abc algorithm can be a competitive alternative in an important practical setting. All
implementation details are collected in Appendix A and the source code is available from
https://github.com/compops/gpo-abc2015/.
Stochastic volatility model with Gaussian log-returns
Consider the stochastic volatility model with Gaussian log-returns ( gsv) given by
x0 ∼N

x0; µ, σ2
v/ 1 −φ2
,
(22a)
xt+1 ∼N

xt+1; µ + φ(xt −µ), σ2
v

,
(22b)
yt ∼N

yt; 0, exp(xt)

,
(22c)
with parameters θ = {µ, φ, σv}. Here, the latent log-volatility is assumed to follow a mean-
reverting random walk with mean µ ∈R, persistence φ ∈(−1, 1) and standard deviation
of the increments σv ∈R+. We generate a single synthetic data set from this model with
T = 500 observations, true parameters θ⋆= {0.20, 0.96, 0.15} and initial state x0 = 0.
For this model, we can make use of standard smc to estimate the log-likelihood as we can
evaluate gθ(yt | xt) using a closed-form expression. Therefore, we implement Algorithm 2
using standard smc to obtain gpo-smc introduced by Dahlin and Lindsten (2014). This
corresponds in some sense to the ideal gpo-abc implementation with tolerance parameter
ϵ ↘0. Furthermore, we make use of the quasi-Newton pmh2 (qpmh2) algorithm intro-
duced by Dahlin et al. (2015b) to estimate the parameter posteriors. This can be seen as the
gold standard to which the Laplace approximations obtained by gpo-smc and gpo-abc
care compare. This is done to evaluate their accuracy.
In the left part of Figure 1, we present the posterior estimates from gpo-smc (solid curve)
and qpmh2 (histogram). We begin by observing a good ﬁt of the Laplace approximation

250
Paper E
Approximate Bayesian inference for models with intractable likelihoods
µ
posterior estimate
-0.2
0.0
0.2
0.4
0.6
0
1
2
3
4
5
6
φ
posterior estimate
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0
2
4
6
8
10
12
σv
posterior estimate
0.0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
-0.2
0.0
0.2
0.4
0.6
0
1
2
3
4
5
6
µ
posterior estimate
0.1
0.2
0.3
0.4
0.5
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0
2
4
6
8
10
12
φ
posterior estimate
0.0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
σv
posterior estimate
Figure 1. Marginal parameter posteriors for the synthetic data in the svg model. Left: Solid
curves indicate the Laplace approximations of the posterior using gpo-smc for µ (green), φ
(orange) and σv (purple). The histograms represent the exact posteriors estimated using qpmh2
and the dark grey (left) curves indicate the prior distributions. Right: Laplace approximations
(shaded areas) from gpo-smc for the three parameters. The grey curves (right) indicate the
Laplace approximations obtained by gpo-abc using ﬁve diﬀerent values of the tolerance pa-
rameter ϵ in the abc approximation.

6
Numerical illustrations and real-world applications
251
from gpo-smc to the histogram approximation obtained by qpmh2. Both the location
and the spread of the posterior approximations are similar and this shows that gpo-smc
could be a more eﬃcient alternative to qpmh2. This as gpo-smc results in a speed-up of
about 40 −60 times compared with qpmh2.
We now continue with analysing the Laplace approximations obtained by gpo-abc. In
the right side of Figure 1, we present the posterior approximations obtained by varying
the tolerance parameter ϵ between 0.1 and 0.5 to study the bias and robustness of the
approximation. Darker shades of grey indicate a larger tolerance parameter. For ϵ = 0.10,
we see that the approximation is rather poor with a signiﬁcant bias and bad ﬁt to the spread.
However for the other choices of ϵ, the approximations from gpo-abc converges quickly
to be similar to gpo-smc (solid curve). From these results, we conclude that we have a
bias in the posterior approximation when ϵ is too small and the approximation tends to
grow wider as ϵ increases. Hence, the posterior approximation experiences a bias-variance
trade-oﬀeﬀect that is determined by the tolerance parameter ϵ.
We continue by comparing gpo-smc to spsa, where the latter is a gradient-free alternative
with good convergence properties in many applications (Spall, 1987). spsa operates by
constructing a ﬁnite-diﬀerence approximation of the gradient at each iteration after which
it takes a step in the gradient direction. We tune the algorithm using the procedure discussed
by Spall (1998) to achieve good performance. Note that spsa requires two log-posterior
estimates at each iteration compared with only one sample in the gpo-smc. Another
possible drawback with spsa is that it only provides the map estimate and no quantiﬁcation
of its uncertainty.
In Figure 2, we compare the map parameter estimates of two algorithms as a function of
the number of log-posterior estimates. The ﬁrst L = 100 samples of the log-posterior are
used to estimate the hyperparameters of the gp prior. After this initial phase, gpo-smc
converges quickly to reasonable values of the parameters. As the log-posterior estimates are
rather noisy, we require that spsa adapts slowly which results in a slow convergence of the
parameter estimates. Finally, we note that the three methods that we consider in this section
make use of varying number of estimates of the log-posterior to determine the parameter
estimates: gpo-smc (350), spsa (700) and qpmh2 (10, 000). However, the estimates are
comparable for gpo-smc and qpmh2, see the left part of Figure 1.
Stochastic volatility model with α-stable log-returns
In the upper part of Figure 3, we present the log-returns of future contracts on coﬀee
during the period between June, 2013 and December, 2014. The data seem to indicate the
presence of jumps around the ﬁrst half of 2014. This is common in ﬁnancial data and can
be modelled in a number of diﬀerent ways. Here, we follow Casarin (2004) by making
use of the α-stable distribution to model the jumps in the log-returns. The use of α-stable
distributions is motivated and exempliﬁed by Stoyanov et al. (2010), Rachev et al. (2005),
Khindanova et al. (2001), Lombardi and Calzolari (2009) and references therein.

252
Paper E
Approximate Bayesian inference for models with intractable likelihoods
0
100
200
300
400
500
600
700
no. log-posterior samples
µ^
0.20
0.30
0.40
0.50
0
100
200
300
400
500
600
700
0.80
0.85
0.90
0.95
no. log-posterior samples
φ^
0
100
200
300
400
500
600
700
no. log-posterior samples
σv^
0.20
0.30
0.40
0.50
Figure 2. The trace of the MAP estimate for µ (green), φ (orange) and σv (purple) from gpo-smc (solid) and spsa (solid-cicle) as a function of the
number of log-posterior samples. The ﬁrst L = 100 samples of gpo-smc are used to estimate the hyperparameters. Both algorithms are run for a total
of 700 log-posterior samples. Dotted lines indicate the posterior means from qpmh2.

6
Numerical illustrations and real-world applications
253
To model the log-returns, we consider the stochastic volatility model with α-stable log-
returns (αsv) given by
x0 ∼N

x0; µ, σ2
v/ 1 −φ2
,
(23a)
xt+1 ∼N

xt+1; µ + φ(xt −µ), σ2
v

,
(23b)
yt ∼A

yt; α, exp(xt)

,
(23c)
with parameters θ = {µ, φ, σv, α} and A(α, γ) denoting a zero-mean symmetric α-stable
distribution with stability parameter α ∈(0, 2) and scale γ ∈R+. The stability parameter
determines the tail behaviour of the distribution, see Nolan (2003) for a discussion of the α-
stable distribution and its properties. The likelihood is in general intractable for the α-stable
distribution but for speciﬁc choices of α we can recover three named distributions: Gaussian
(α = 2), Cauchy (α = 1) and Levy (α = 1/2). As a result, we can only in general make use
of qpmh2-abc and gpo-abc for this model, where both approaches are approximate as
they rely on the abc approximation of the model.
In the middle and lower part of Figure 3, we compare the posterior approximations obtained
with gpo-abc (solid curves) with 10 independent runs and qpmh2-abc (histograms). We
see that the mixing of qpmh2-abc is quite poor for this model as the histograms are peaky.
This is a common problem as Markov chains tends to get stuck if the log-posterior estimates
are noisy. However, the posterior estimates overlap and seems to give reasonable parameter
values for each run of gpo-abc. We conclude that gpo-abc can be a good alternative
to pmh-abc for inference in αsv models, as it enjoys similar accuracy but at a signiﬁcant
lower computational cost. Finally, we make use of the abc version of the forward-ﬁltering
backward simulator (ffbsi; Taghavi et al., 2013) smoother using rejection sampling and
early stopping to estimate the log-volatility. The resulting estimate (black) seems reasonable
when compared to the log-returns.
Modelling price dependencies between oil futures
In this section, we follow Charpentier (2015) and would like to construct a copula model
to capture the dependency structure between prices of oil future contracts. The data that
we consider is presented in Figure 4 and consists of weekly log-returns between January
10, 1997 and June 4, 2010 of Brent (produced in the North Sea), Dubai (produced in the
Persian Gulf) and Maya (produced in the Gulf of Mexico) oil. For this problem, we adopt
the commonly used two-stage approach to copula modelling where marginal models are
ﬁrst ﬁtted separately to each of the log-return series, and then combined using a copula to
model the dependency structure (Joe, 2005). We outline this procedure in Algorithm 3.
Lines 1 and 2 We make use of the same procedure as in Section 6.2 to estimate the parameters
of an αsv model to describe the log-returns for each type of oil. The resulting parameter es-
timates are presented in Table 1, which are similar with the exception of the mean-reverting
parameter µ. From the model, we expect that the log-volatility varies slowly over time
around diﬀerent mean values and that the log-returns are more heavy-tailed than a Gaussian
distribution. In the left part of Figure 4, we present the estimates of the log-volatility and
the resulting credibility regions which conﬁrms our expectations.

254
Paper E
Approximate Bayesian inference for models with intractable likelihoods
-10
-5
0
5
10
date
daily log-returns
Jun 13
Aug 13
Oct 13
Dec 13
Feb 14
Apr 14
Jun 14
Aug 14
Oct 14
Dec 14
-1
0
1
2
3
smoothed log-volatility
µ
posterior estimate
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
φ
posterior estimate
0.75
0.80
0.85
0.90
0.95
1.00
0
2
4
6
8
10
12
σv
posterior estimate
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
2
4
6
8
α
posterior estimate
1.0
1.2
1.4
1.6
1.8
2.0
0
1
2
3
4
5
Figure 3. Upper: log-returns (green) of coﬀee futures and the estimate of the log-volatility (dark
grey). The shaded area indicates the approximate 95% credibility region for the log-returns.
Middle and lower: marginal parameter posterior in the αsv model estimated by gpo-abc (solid
curves) using 10 independent runs and qpmh2-abc (histogram) for µ (green), φ (orange), σv
(purple) and α (magenta). Dark grey curves indicate the prior distributions.

6
Numerical illustrations and real-world applications
255
Algorithm 3 Copula modelling using αsv as the marginal models
Stage 1 (repeated for each type of oil, i.e., i = 1, 2, 3)
1: Estimate DθMAP,i for the αsv model (23) using gpo-abc in analogue with Section 6.2.
2: Estimate the log-volatility Dx1:T ,i using the ABC-FFBSi smoother and the parameter
estimate DθMAP,i. Compute the ﬁltered log-returns Det,i by
Det,i = exp

−1
2Dxt,i

yt,
for t = 1, . . .,T .
(24)
3: Estimate the cumulative distribution function ( cdf) denoted by D
Gi from the data.
Compute the probability integral transformation of the residuals by
Dut,i = D
Gi(Det,i),
for t = 1, . . .,T .
(25)
Stage 2
4: Infer the parameters of the copula function to model the dependency between
{Dut,1, Dut,2, Dut,3}T
t=1.
Line 3 The ﬁltered residuals given by (24) are independent and identically distributed as
A(Dαi, 0, 1, 0) according to the αsv model (23) if x1:T is known. Here, we assume that
this holds approximately, so that we can transform these residuals into the 3-dimensional
hypercube and make use of the copula model. This can be done by the probability integral
transform, i.e., Gθ,i(Dut,i), which unfortunately is intractable for the α-stable distribution.
A possible approach is instead to calculate the empirical cdf ( ecdf) from the data and
then compute the transformed residuals using this estimate.
A drawback of using the ecdf directly is that it suﬀers from poor accuracy in the tails due
to the small number of samples in these regions. An alternative is instead to compute a
semi-parametric estimate of the cdf. This is done by combining a parametric model known
as generalised Pareto distribution ( gpd) for each of the tails and the non-parametric ecdf
estimator for the main bulk of the data. The density of the gpd is given by
pη,µ,σ(Det,i) = σ−1 1 + ησ−1(Det,i −µ)1+η−1
,
where µ ∈R, σ ∈R+ and η ∈R denotes the parameters determining the location, scale
and shape of the distribution, respectively.
The combination is made by non-parametric regression using the Gaussian kernel to stitch
the three components of the estimated cdf together. The use of gpd for the tails is known
as the threshold excess method and is a tool from extreme value statistics. In this approach,
we model the 10% smallest and largest ﬁltered residuals using two gpd distributions (Em-
brechts et al., 1997). To estimate the parameters of the gpds, we make use of the log-
likelihood given by
` De⋆
1,i, . . .,De⋆
K,i |η, µ, σ = −K log σ − 1 + η−1 K
X
k=1
log  1 + ησ−1(De⋆
k,i −µ),
(26)
where De⋆
i denotes the upper/lower 10% of the ﬁltered residuals. For the parameter inference,
we assume uniform priors and compute the map estimate by optimizing (26) using a quasi-

256
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Parameters for marginal model
Parameters for gpd tail distributions
Asset
µ
φ
σv
α
η−
µ−
σ−
η+
µ+
σ+
αsv
Brent
0.56
0.99
0.40
1.67
0.22
0.01
0.58
0.10
0.01
0.54
(0.18)
(0.06)
(0.09)
(0.10)
(0.11)
(0.05)
(0.03)
(0.11)
(0.04)
(0.02)
Dubai
0.33
0.99
0.47
1.65
0.05
0.02
0.66
0.01
0.01
0.58
(0.26)
(0.02)
(0.11)
(0.14)
(0.09)
(0.05)
(0.02)
(0.09)
(0.05)
(0.01)
Maya
0.56
0.99
0.47
1.67
-0.04
0.02
0.68
0.17
0.02
0.56
(0.37)
(0.01)
(0.09)
(0.23)
(0.09)
(0.05)
(0.02)
(0.11)
(0.04)
(0.03)
GSV
Brent
1.78
0.99
0.13
(0.13)
(0.01)
( 0.02)
Dubai
1.63
0.99
0.15
(0.07)
(0.01)
(0.01)
Maya
1.86
0.99
0.14
(0.17)
(0.01)
(0.03)
Table 1. Left: Posterior means and standard deviations (in parentheses) from gpo-abc for the αsv model in (23) for diﬀerent oil assets. Right: map
estimates and posterior standard deviations (in parentheses) for the lower (-) and upper (+) tails of the distribution of the residuals modelled using gpds.

6
Numerical illustrations and real-world applications
257
1998
2000
2002
2004
2006
2008
2010
-20
-10
0
10
20
date
log-returns (brent)
0
1
2
3
4
5
6
7
smoothed log-volatility
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
brent
dubai
1998
2000
2002
2004
2006
2008
2010
-20
-10
0
10
20
date
log-returns (dubai)
0
1
2
3
4
5
6
7
smoothed log-volatility
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
brent
maya
1998
2000
2002
2004
2006
2008
2010
-20
-10
0
10
20
date
log-returns (maya)
0
1
2
3
4
5
6
7
smoothed log-volatility
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
dubai
maya
Figure 4. Left: weekly log-returns of Brent (green), Dubai (orange) and Maya (purple) oil be-
tween January 10, 1997 and June 4, 2010. The shaded area indicates the approximate 95% cred-
ibility region for the log-returns according to the αSV model. The dark grey curves indicate
estimates of the log-volatility using the αsv model and the abc-ffbsi smoother. Right: the
transformed ﬁltered residuals Dut,i for the diﬀerent types of oil.

258
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Newton method. The resulting parameter estimates are presented in the right side of Table 1,
where we note that the shape parameter of the lower tail diﬀers slightly between each type
of oil. The transformed residuals are calculated by using (25) where D
Gi is given by the
semiparametric model of the cdf. The resulting residuals are presented in the right part of
Figure 4, where we note a strong correlation between the diﬀerent types of oil.
Line 4 Finally, we combine the transformed residuals {Du1:T ,1, . . ., Du1:T ,d} using a copula
function to ﬁnd a model for the joint distribution. A copula function C (Nelsen, 2007;
McNeil et al., 2010) is a probability distribution on a d-dimensional hypercube [0, 1]d. By
Sklar’s theorem, we can write any multivariate distribution function using a copula of some
marginal distributions by
H

Du1:T ,1, . . ., Du1:T ,d

= C

H1(Du1:T ,1), . . ., Hd(Du1:T ,d)

,
where Hi(Du1:T ,i) denotes the cdf of the ith marginal distribution of H . A popular example
of C is the heavy-tailed Student t-copula, which is a common choice to model ﬁnancial
returns, see Rachev et al. (2005). This type of copula has the form
Cν,P = tν,P

t −1
ν (Du1:T ,1), . . ., t −1
ν (Du1:T ,d)

,
(27)
where tν,P denotes the multivariate t distribution with zero mean, scale matrix P and ν
degrees of freedom. Here, t −1
ν
denotes the quantile function of the univariate Student’s t
distribution with the same degrees of freedom. The corresponding log-likelihood function
is given by
`(Du1:T ,1), . . ., Du1:T ,d)| ν, P) =
TX
t=1
log gν,P

t −1
ν (Dut,1), . . ., t −1
ν (Dut,d)

−
TX
t=1
d
X
i=1
log gν

t −1
ν (ut,i)

,
(28)
where gν,P and gν denotes the density of the multivariate and univariate Student’s t distri-
butions, respectively. Here, we assume that the correlation matrix P can be written
P = ∆A(AA⊤)∆A,
with ∆A = diag(AA⊤)−1/2,
A =

1
0
0
ρ1
1
0
ρ2
ρ3
1

.
This is done to ensure that P is a valid correlation matrix, while keeping the individual
correlations unrestricted, i.e., ρi ∈[−1, 1] for i = 1, 2, 3.
We compute the map estimate of the parameters in the copula function {ν, ρ1, ρ2, ρ3}
using a quasi-Newton algorithm. The resulting parameter estimates are presented in Table 2,
where we also compare with using an arma-garch model as in Charpentier (2015) and
the gsv model for the margins. For the latter, we present the parameters estimated using
gpo-smc in Table 1. We conclude that the copula parameters are quite similar for the three
diﬀerent margins but the degrees of freedom are smaller for arma-garch. This could be
an indication of that gsv and αsv capture more of the variations in the log-returns.
Finally, we make use of the copula models and their margins to estimate the var for each

6
Numerical illustrations and real-world applications
259
Margins
Parameters for copula
ν
ρ1
ρ2
ρ3
arma-garch
4.29 (0.65)
-0.12 (0.03)
0.38 (0.02)
0.29 (0.02)
gsv
8.49 (1.77)
-0.08 (0.03)
0.34 (0.02)
0.28 (0.02)
αsv
9.38 (1.97)
-0.10 (0.03)
0.33 (0.02)
0.25 (0.02)
Table 2. map estimates and standard deviations (in parentheses) for the parameters in the Stu-
dent’s t copula for diﬀerent models of the margins.
Margins
Spearman correlations
Brent-Dubai
Brent-Maya
Dubai-Maya
arma-garch
0.73
0.85
0.76
gsv
0.76
0.84
0.79
αsv
0.75
0.82
0.77
Table 3. The estimated correlations between the diﬀerent types of oil in the Student’s t copula
for diﬀerent models of the margins.
1998
2000
2002
2004
2006
2008
2010
0
10
20
30
40
date
VaR0.99
Figure 5. Estimated values of VaR0.99(et) for an equally weighted portfolio of the three oils
future contracts using the gsv (magenta) and αsv (green) model with the Student’s t copula.

260
Paper E
Approximate Bayesian inference for models with intractable likelihoods
type of oil. The var of an asset at conﬁdence α ∈(0, 1) is deﬁned by
VaRα(et) = inf
(
−et ∈R : G(−et) ≥α
)
,
i.e., the smallest loss −et such that probability that the loss (the negative log-return) exceeds
−et is no larger then (1 −α). We adopt a Monte Carlo approach to estimate VaRα(et) by:
(i) simulating from the copula by the inverse of (27), (ii) making use of the inverse of the
ecdf in (25) to obtain simulated ﬁltered residuals, (iii) computing the resulting log-returns
by the estimated volatility and the inverse of (24). We repeat this procedure 100, 000 times
for each type of oil and then compute the empirical quantile corresponding to VaR0.99(et)
for an equally weighted portfolio. Clearly, we can make use of more advanced portfolio
weighting schemes in this approach as well. Note that we can make use of the true cdf for
the gsv and do not require computing the ecdf in this case.
In Figure 5, we present the resulting estimates of VaR0.99(et) resulting from the simulation
for the Student’s t copula with the gsv and αsv models for the margins, respectively.
We note that the estimates in the two cases are quite diﬀerent especially at the end of the
data series. It is diﬃcult to determine, which value of var is the more accurate and we
leave it to the reader to decide on which model to use. However, we conclude that this
application serves as an illustration that the proposed method can be useful in solving a
practical problem.
Conclusions
The illustrations provided in Section 6 indicate that gpo-abc is quite accurate and ex-
hibit a substantial decrease in the computational cost. Speciﬁcally, compared with qpmh2
and spsa we obtain similar posterior estimates requiring up to 60 and 3 times less com-
putational time, respectively. gpo-abc is also more robust to noise in the log-posterior
estimates, which typically results in that the qpmh2 algorithm gets stuck at times and that
the spsa algorithm converges slowly. We also note that gpo-abc provides estimates of
the parameter uncertainty encoded in the Laplace approximation, which cannot be directly
extracted from spsa. Furthermore, we remind the reader that gpo can be applied for other
inference problems, where the log-posterior estimates are noisy and computationally costly
to evaluate. Another possible application is to initialise and construct proposals in pmh,
which can limit the number of tedious pilot runs that are usually required when p is large.
Future work includes adopting a sparse presentation of the gp and make use of covariance
functions tailored to the speciﬁc application. Some ideas for sequential and sparse repre-
sentations of gps are discussed by Csató and Opper (2002); Bijl et al. (2015); Solin and
Särkkä (2014); Deisenroth and Ng (2015) and Huber (2014). An interesting approach for tai-
lored gp priors would be to make use of non-stationary covariance functions (Paciorek and
Schervish, 2004). This would capture the fact that the log-posterior often falls oﬀrapidly in
some parts of the parameter space and is almost ﬂat in other parts. More computationally ef-
ﬁcient alternatives to smc-abc are also an important area of future work. Some interesting
ideas in this area are discussed in Martin et al. (2014).

A
Implementation details
261
Acknowledgements
The simulations were performed on resources provided by the Swedish National Infras-
tructure for Computing ( snic) at Linköping University, Sweden. jd would like to thank
Fredrik Lindsten, Neil Lawrence and Carl-Henrik Ek for fruitful discussions. Thanks also
to Joerg M. Gablonsky, Abraham Lee, Per A. Brodtkorb and the GPy for making their
Python implementations available.
Appendix
Implementation details
GPO algorithm: We make use of the the GPy package (The GPy authors, 2014) for calculat-
ing the gp predictive posterior and the gp prior hyperparameters using eb. The covariance
presented in (18) is used together with a zero mean function as the gp prior for the log-
posterior estimates. We make use of ei as the acquisition rule in (21) with ζ = 0.01 and
Σ = 0.01Ip. We run the gpo algorithm for K = 250 iterations after the initialisation and
re-estimate the hyperparameters of the gp prior every 50th iteration.
We initialise the gpo algorithm using L = 50 samples obtained using Latin hypercube
sampling with the implementation written by Abraham Lee available at https://pypi.
python.org/pypi/pyDOE. The optimisation problems in Lines 8 and 11 in Algorithm 2
are solved using the direct implementation written by Joerg M. Gablonsky, available
from https://pypi.python.org/pypi/DIRECT/. Finally for Line 12, we make use of the
Python implementation by Per A. Brodtkorb available at https://pypi.python.org/
pypi/Numdifftools.
Section 6.1: We use N = 1, 000 particles in gpo-smc and N = 2, 000 particles with the
Gaussian density with tolerance level ϵ = 0.20 and ψ(x) = x in gpo-abc to produce the
results in Figure 1. The search space for the gpo algorithm ΘGPO is given by µ ∈(−1, 1),
φ ∈(0, 1) and σv ∈(0, 1). We use the following prior densities
p(µ) ∼T N (0,1)(µ; 0, 0.22),
p(φ) ∼T N (−1,1)(φ; 0.9, 0.052),
p(σv) ∼G(σv; 0.2, 0.2),
where T N (a,b)( · ) denotes a truncated Gaussian distribution on [a, b], G(a, b) denotes the
Gamma distribution with mean a/b.
For qpmh2, we make use of the ﬁxed-lag particle smoother with N = 2, 000 particles,
lag ∆= 12, memory length nmem = 20 and initialise the Hessian in 400Ip. We initialise
qpmh2 in the map estimate obtained by gpo-smc and run it for M = 30, 000 iterations
(discarding the ﬁrst 5, 000 as burn-in). We use nhyb = 2, 500 samples of the Markov chain
to construct the posterior estimate used in the hybrid approach. For spsa, we make use
of N = 1, 000 particles, a = 0.001, c = 0.30, A = 35, α = 0.602, γ = 0.101 and initalise
the algorithm in the map estimate from gpo-smc. See Spall (1998) for the details of the
algorithm and its implementation.

262
Paper E
Approximate Bayesian inference for models with intractable likelihoods
Section 6.2: The real-world data is computed as yt = 100[log(st) −log(st−1)] , where st
denotes the price of a future contract on coﬀee obtained from https://www.quandl.com/
CHRIS/ICE_KC2. We use N = 5, 000 particles with the Gaussian density with tolerance
level ϵ = 0.10 and ψ(x) = arctan(x) in gpo-abc. The search space for the gpo algorithm
ΘGPO is given by µ ∈(0, 1), φ ∈(0, 1), σv ∈(0, 0.7) and α ∈(1, 2). We use the same
priors as for gsv with the additional p(α) ∼B(α/2; 6, 2), where B(a, b) denotes the Beta
distribution. For qpmh2, we use the same settings as before but adjust nmem = 100 and
initialise the Hessian in 1000Ip.
Section 6.3: The real-world data is downloaded from http://freakonometrics.free.
fr/oil.xls and the same settings for standard smc as in Section 6.1 and smc-abc as in
Section 6.2. We make use of uniform priors for both the parameters of both the gpds to
model the tails of the cdf and for the copula parameters. The parameter estimates are
obtain as the map from a quasi-Newton solver.

Bibliography
263
Bibliography
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342,
2010.
H. Bijl, J-W. van Wingerden, T. B. Schön, and M. Verhaegen. Online sparse Gaussian
process regression using FITC and PITC approximations. In Proceedings of the 17th IFAC
Symposium on System Identiﬁcation (SYSID), pages 703–708, Beijing, China, October 2015.
C. M. Bishop. Pattern recognition and machine learning. Springer Verlag, New York, USA,
2006.
P. Boyle. Gaussian processes for regression and optimisation. PhD thesis, Victoria University
of Wellington, 2007.
E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement
learning. Pre-print, 2010. arXiv:1012.2599v1.
A. D. Bull.
Convergence rates of eﬃcient global optimization algorithms.
Journal of
Machine Learning Research, 12:2879–2904, 2011.
R. Casarin. Bayesian inference for generalised Markov switching stochastic volatility models,
2004. CEREMADE Journal Working Paper 0414.
A. Charpentier. Prévision avec des copules en ﬁnance. Technical report, May 2015. URL
https://hal.archives-ouvertes.fr/hal-01151233.
Lehel Csató and Manfred Opper. Sparse on-line Gaussian processes. Neural computation,
14(3):641–668, 2002.
J. Dahlin and F. Lindsten. Particle ﬁlter-based Gaussian process optimisation for parameter
inference. In Proceedings of the 19th IFAC World Congress, Cape Town, South Africa,
August 2014.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using gradient and
Hessian information. Statistics and Computing, 25(1):81–92, 2015a.
J. Dahlin, F. Lindsten, and T. B. Schön. Quasi-Newton particle Metropolis-Hastings. In
Proceedings of the 17th IFAC Symposium on System Identiﬁcation (SYSID), pages 981–986,
Beijing, China, October 2015b.
J. Dahlin, M. Villani, and T. B. Schön. Eﬃcient approximate Bayesian inference for models
with intractable likelihoods. Pre-print, 2015c. arXiv:1506.06975v1.
T. A. Dean and S. S. Singh. Asymptotic behaviour of approximate Bayesian estimators.
Pre-print, 2011. arXiv:1105.3655v1.
T. A. Dean, S. S. Singh, A. Jasra, and G. W. Peters.
Parameter estimation for hidden
Markov models with intractable likelihoods. Scandinavian Journal of Statistics, 41(4):
970–987, 2014.

264
Paper E
Approximate Bayesian inference for models with intractable likelihoods
M. P. Deisenroth and J. W. Ng.
Distributed Gaussian Processes.
Pre-print, 2015.
arXiv:1502.02843v1.
P. Del Moral, A. Doucet, and A. Jasra. An adaptive sequential Monte Carlo method for
approximate Bayesian computation. Statistics and Computing, 22(5):1009–1020, 2012.
A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years
later. In D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering.
Oxford University Press, 2011.
J. Durbin and S. J. Koopman. Time series analysis by state space methods. Oxford University
Press, 2 edition, 2012.
E. Ehrlich, A. Jasra, and N. Kantas. Gradient free parameter estimation for hidden Markov
models with intractable likelihoods. Methodology and Computing in Applied Probability,
17(2), 2015.
P. Embrechts, C. Klüppelberg, and T. Mikosch. Modelling extremal events. Springer Verlag,
1997.
M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73
(2):1–37, 2011.
P. Glasserman. Monte Carlo methods in ﬁnancial engineering. Springer Verlag, 2004.
R. B. Gramacy and N. G. Polson. Particle learning of Gaussian process models for sequential
design and optimization. Journal of Computational and Graphical Statistics, 20(1):102–118,
2011.
M. U. Gutmann and J. Corander. Bayesian optimization for likelihood-free inference of
simulator-based statistical models. Pre-print, 2015. arXiv:1501.03291v1.
M. F. Huber. Recursive Gaussian process: On-line regression and learning. Pattern Recogni-
tion Letters, 45:85–91, 2014.
A. Jasra. Approximate Bayesian computation for a class of time series models. International
Statistical Review, 83(3):405–435, 2015.
A. Jasra, S. S. Singh, J. S. Martin, and E. McCoy.
Filtering via approximate Bayesian
computation. Statistics and Computing, 22(6):1223–1237, 2012.
H. Joe. Asymptotic eﬃciency of the two-stage estimation method for copula-based models.
Journal of Multivariate Analysis, 94(2):401–419, 2005.
D. R. Jones. A taxonomy of global optimization methods based on response surfaces.
Journal of Global Optimization, 21(4):345–383, 2001.
D. R. Jones, C. D. Perttunen, and B. E. Stuckman. Lipschitzian optimization without the
Lipschitz constant. Journal of Optimization Theory and Applications, 79(1):157–181, 1993.
I. Khindanova, S. Rachev, and E. Schwartz. Stable modeling of value at risk. Mathematical
and Computer Modelling, 34(9):1223–1259, 2001.

Bibliography
265
G. Kitagawa. Monte Carlo ﬁlter and smoother for non-Gaussian nonlinear state space
models. Journal of Computational and Graphical Statistics, 5(1):1–25, 1996.
D. J. Lizotte. Practical Bayesian optimization. PhD thesis, University of Alberta, 2008.
M. J. Lombardi and G. Calzolari. Indirect estimation of α-stable stochastic volatility models.
Computational Statistics and Data Analysis, 53(6):2298–2308, 2009.
J-M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder. Approximate Bayesian computational
methods. Statistics and Computing, 22(6):1167–1180, 2012.
G. M. Martin, B. P. M. McCabe, W. Maneesoonthorn, and C. P. Robert. Approximate
Bayesian computation in state space models. Pre-print, 2014. arXiv:1409.8363v1.
A. J. McNeil, R. Frey, and P. Embrechts. Quantitative risk management: concepts, techniques,
and tools. Princeton University Press, 2010.
E. Meeds and M. Welling. GPS-ABC: Gaussian process surrogate approximate Bayesian
computation. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), Quebec City, Canada, July 2014.
R. M. Neal. MCMC using Hamiltonian dynamics. In S. Brooks, A. Gelman, G. Jones, and
X-L. Meng, editors, Handbook of Markov Chain Monte Carlo. Chapman & Hall/CRC
Press, 2010.
R. B. Nelsen. An introduction to copulas. Springer Verlag, 2007.
J. Nolan. Stable distributions: models for heavy-tailed data. Birkhauser, 2003.
C. J. Paciorek and M. J. Schervish. Nonstationary covariance functions for Gaussian process
regression. In Proceedings of the 2004 Conference on Neural Information Processing Systems
(NIPS), pages 273–280, Vancouver, Canada, December 2004.
M. Panov and V. Spokoiny. Finite sample Bernstein-von-Mises theorem for semiparametric
problems. Bayesian Analysis, 10(3):665–710, 2015.
M. K. Pitt, R. S. Silva, P. Giordani, and R. Kohn. On some properties of Markov chain
Monte Carlo simulation methods based on the particle ﬁlter. Journal of Econometrics, 171
(2):134–151, 2012.
G. Poyiadjis, A. Doucet, and S. S. Singh. Particle approximations of the score and ob-
served information matrix in state space models with application to parameter estimation.
Biometrika, 98(1):65–80, 2011.
S. T. Rachev, S. V. Stoyanov, A. Biglova, and F. J. Fabozzi. An empirical examination of
daily stock return distributions for US stocks. In Data Analysis and Decision Support,
pages 269–281. Springer Verlag, 2005.
C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. MIT Press,
2006.
Y. Saatçi, R. D. Turner, and C. E. Rasmussen. Gaussian process change point models. In
Proceedings of the 2010 Conference on Neural Information Processing Systems (NIPS), pages
927–934, Vancouver, Canada, December 2010.

266
Paper E
Approximate Bayesian inference for models with intractable likelihoods
C. Sherlock, A. H. Thiery, G. O. Roberts, and J. S. Rosenthal. On the eﬃcency of pseudo-
marginal random walk Metropolis algorithms. The Annals of Statistics, 43(1):238–275,
2015.
J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learn-
ing algorithms. In Proceedings of the 2012 Conference on Neural Information Processing
Systems (NIPS), Lake Tahoe, USA, December 2012.
A Solin and S. Särkkä. Hilbert space methods for reduced-rank Gaussian process regression.
Pre-print, 2014. arXiv:1401.5508v1.
J. C. Spall. A stochastic approximation technique for generating maximum likelihood
parameter estimates. In Proceedings of the 6th American Control Conference (ACC), pages
1161–1167, Minneapolis, USA, June 1987.
J. C. Spall. Implementation of the simultaneous perturbation algorithm for stochastic
optimization. IEEE Transactions on Aerospace and Electronic Systems, 34(3):817–823, 1998.
S. V. Stoyanov, B. Racheva-Iotova, S. T. Rachev, and F. J. Fabozzi. Stochastic models for risk
estimation in volatile markets: a survey. Annals of Operations Research, 176(1):293–309,
2010.
A. Svensson, J. Dahlin, and T. B. Schön. Marginalizing Gaussian process hyperparameters
using sequential Monte Carlo. In Proceedings of the 6th IEEE International Workshop on
Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), Cancun, Mexico,
December 2015.
E. Taghavi, F. Lindsten, L. Svensson, and T. B. Schön. Adaptive stopping for fast particle
smoothing. In Proceedings of the 38th International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Vancouver, Canada, May 2013.
The GPy authors. GPy: A Gaussian process framework in Python. http://github.com/
SheffieldML/GPy, 2014.
R. S. Tsay. Analysis of ﬁnancial time series. John Wiley & Sons, 2 edition, 2005.
A. W. Van der Vaart. Asymptotic statistics. Cambridge University Press, 2000.
E. Vazquez and J. Bect. Convergence properties of the expected improvement algorithm
with ﬁxed mean and covariance functions. Journal of Statistical Planning and inference,
140(11):3088–3095, 2010.
R. D. Wilkinson. Approximate Bayesian computation (ABC) gives exact results under the
assumption of model error. Statistical applications in genetics and molecular biology, 12(2):
129–141, 2013.
S. Yıldırım, S. S. Singh, T. Dean, and A. Jasra. Parameter estimation in hidden Markov mod-
els with intractable likelihoods using sequential Monte Carlo. Journal of Computational
and Graphical Statistics, 24(3):846–865, 2014.

Paper F
Hierarchical Bayesian approaches for
robust inference in ARX models
Authors:
J. Dahlin, F. Lindsten, T. B. Schön and A. Wills
This paper is published by Elsevier:
http://dx.doi.org/10.3182/20120711-3-BE-2027.00318.
Edited version of the paper:
J. Dahlin, F. Lindsten, T. B. Schön, and A. Wills. Hierarchical Bayesian ARX
models for robust inference. In Proceedings of the 16th IFAC Symposium on System
Identiﬁcation (SYSID), Brussels, Belgium, July 2012b.
Preliminary version:
Technical Report LiTH-ISY-R-3041, Dept. of Electrical Engineering, Linköping
University, SE-581 83 Linköping, Sweden.


Hierarchical Bayesian approaches for robust
inference in ARX models
J. Dahlin⋆, F. Lindsten†, T. B. Schön† and A. Wills‡
⋆Dept. of Electrical Engineering,
Linköping University,
SE–581 83 Linköping, Sweden.
johan.dahlin@liu.se
†Dept. of Information Technology,
Uppsala University,
SE-751 05 Uppsala, Sweden.
fredrik.lindsten@it.uu.se
thomas.schon@it.uu.se
‡School of EECS,
University of Newcastle,
Callaghan, NSE, Australia.
adrian.wills@newcastle.edu.au
Abstract
Gaussian innovations are the typical choice in most arx models but using other distributions such
as the Student’s t could be useful. We demonstrate that this choice of distribution for the innovations
provides an increased robustness to data anomalies, such as outliers and missing observations. We
consider these models in a Bayesian setting and perform inference using numerical procedures based
on Markov Chain Monte Carlo methods. These models include automatic order determination by
two alternative methods, based on a parametric model order and a sparseness prior, respectively. The
methods and the advantage of our choice of innovations are illustrated in three numerical studies
using both simulated data and real eeg data.
Keywords
ARX models, Robust estimation, Bayesian methods, Markov chain Monte Carlo.
Data and source code in MATLAB
https://github.com/compops/rjmcmc-sysid2012
Financial support from
The projects Learning of complex dynamical systems (Contract number: 637-2014-466), Probabilistic
modeling of dynamical systems (Contract number: 621-2013-5524) and cadics, a Linnaeus Center, all
funded by the Swedish Research Council.
269

270
Paper F
Hierarchical Bayesian approaches for robust inference in ARX models
Introduction
An autoregressive exogenous ( arx) model of orders n = {na, nb}, is given by
yt +
na
X
i=1
an
i yt−i =
nb
X
i=1
bn
i ut−i + et,
(1)
where an
i and bn
i are model coeﬃcients, ut is a known input signal and et is white excitation
noise, often assumed to be Gaussian and independent of the input signal. Then, for known
model orders n, the maximum likelihood estimate of the unknown arx coeﬃcients θn =
{an
1, . . ., an
na, bn
1 , . . ., bn
nb } is given by least squares ( ls). In practice, we are often faced
with the following problems:
1. The appropriate model order is unknown or no best model order may exist.
2. The observed data is non-Gaussian in nature, e.g., due to outliers.
In this work, we propose two hierarchical Bayesian arx models and algorithms to make
inference in these models, thereby addressing both of the practical issues mentioned above.
The proposed models diﬀers from (1) in two aspects: (i) the excitation noise is modelled as
Student’s t distributed, and (ii) a built-in form of automatic order selection is used.
The t distribution is more heavy-tailed than the Gaussian distribution, which means that
the proposed arx model can capture jumps in the internal state of the system (as an eﬀect of
occasional large innovations). Furthermore, we believe that this will result in an inference
method that is more robust to model errors and outliers in the observations, a property
which we illustrate in this work.
We propose two alternative methods to determine the system order n. Firstly, we let the
model order n be a parameter of the Bayesian arx model. The model order is inferred
alongside the other unknown parameters, resulting in a posterior distribution over model
orders. In the second model, we instead use a sparseness prior over the arx coeﬃcients,
known as automatic relevance determination ( ard) (MacKey, 1994; Neal, 1996).
Based on the models introduced above, the resulting identiﬁcation problem amounts to
ﬁnding the posterior distribution of the model parameters θn and the order n. This is
done using Markov Chain Monte Carlo Methods (see e.g., Robert and Casella (2004)),
where we are constructing a Markov Chain with the posterior distribution as its stationary
distribution. We can thus compute estimates under the posterior parameter distribution by
sampling from the constructed Markov Chain.
For the ﬁrst model, this is a challenging task as the model order is explicitly included in the
parameter vector. This is due to the fact that we are now dealing with a parameter space
of varying dimension, which thereby require the Markov Chain to do the same. This will
be solved using the reversible jump Metropolis-Hastings (rjmh) algorithm introduced by
Green (1995). The inference problem resulting from the use of an ard prior is in the other
hand solvable using standard Markov chain Monte Carlo ( mcmc) algorithms.
The use of rjmh to estimate the model order and the parameters of an ar model driven by
Gaussian noise, is fairly well studied, see e.g., (Troughton and Godsill, 1998; Godsill, 2001;
Brooks et al., 2003). The present work diﬀers from these contributions, mainly in the use

2
Hierarchical Bayesia ARX models
271
of Student t distributed innovations. Similar models are also considered by Christmas and
Everson (2011), who derive a variational Bayes algorithm for the inference problem. This
approach is not based on Monte Carlo sampling, but instead makes use of certain determin-
istic approximations to overcome the intractable integrals that appear in the expression for
the posterior distribution.
Hierarchical Bayesia ARX models
In this section, we present the two proposed hierarchical Bayesian arx models both using
Student’s t distributed excitation noise, as described in Section 2.1. The models diﬀer in
how the model orders are incorporated. The two alternatives are presented in Sections 2.2
and 2.3, respectively.
Student’s t distributed innovations
We model the excitation noise as Student’s t-distributed, with scale λ and ν degrees of
freedom ( dof)
et ∼St(0, λ, ν).
(2)
This can equivalently be seen as a latent variable model in which et is modelled as zero-mean
Gaussian with unknown variance (λzt)−1 and zt is a gamma distributed latent variable.
Hence, an equivalent model to (2) is given by
zt ∼G(ν/2, ν/2),
(3a)
et ∼N(0, (λzt)−1),
(3b)
where G(α, β) is the gamma distribution with shape α and inverse scale β and N(µ, σ2) is
the Gaussian distribution with mean µ and variance σ2.
Note that λ and ν are unknowns, we wish to infer these in the proposed Bayesian models.
As we do not know much about these parameters, vague (non-informative) gamma priors
are used as in Christmas and Everson (2011)
p(λ) = G(λ; αλ, βλ),
(4a)
p(ν) = G(ν; αν, βν),
(4b)
where α and β denote hyperparameters that we deﬁne below. Note that these are standard
choices resulting from the property of conjugate priors. This type of priors used in com-
bination with a suitable likelihood gives an analytical expression for the posterior, see e.g.
Bishop (2006) for other examples of conjugate priors.
Parametric model order
The ﬁrst automatic order determination alternative is to infer the order n along with the
model parameters. Assume that there exists some maximum order such that na, nb ≤nmax,
resulting in n2
max diﬀerent model hypotheses
Mn :
yt = (ϕn
t )⊤θn + et,
(5)

272
Paper F
Hierarchical Bayesian approaches for robust inference in ARX models
for n = {1, 1}, {1, 2}, . . ., {nmax, nmax}, where
ϕn
t = {−yt−1, . . ., −yt−na, ut−1, . . ., ut−nb }⊤,
(6)
denotes the known inputs and outputs, θn the model coeﬃcients, and et the excitation noise
that is assumed to be independent of the input signal. We use a uniform prior distribution
over these model hypotheses with order n as
p(n) =

1/n2
max
if na, nb ∈{1, . . ., nmax},
0
otherwise.
(7)
Furthermore, we model the coeﬃcients θn as random vectors, with prior distributions
p(θn |n, δ) = N(θn; 0, δ−1Ina+nb ),
(8)
with the same variance δ−1 for all orders n and where In denotes the n × n identity matrix.
Finally, we place the standard conjugate gamma prior on δ as
p(δ) = G(δ; αδ, βδ).
(9)
All put together, the collection of unknowns of the model is given by
η = {θn, n, δ, z1:t, λ, ν}.
(10)
The latent variables z1:T , as well as the coeﬃcients’ variance δ−1, can be seen as nuisance
parameters which are not really of interest, but they will simplify the inference.
Automatic relevance determination
An alternative approach for order determination is to use ard. Consider a high-order arx
model with ﬁxed orders n = {nmax, nmax}. Hence, we overparameterise the model and the
arx coeﬃcients θ will be a vector of ﬁxed dimension m = 2nmax. To avoid overﬁtting, we
place a sparseness prior, known as ard, on the arx coeﬃcients
p(θi |δi) = N(θi; 0, δ−1
i ),
(11)
with the conjugate distribution on the variance
p(δi) = G(δi; αδ, βδ),
(12)
for i = 1, . . ., m.
The diﬀerence between the ard prior and (8) is that in (11), each coeﬃcient is governed
by a diﬀerent variance, which is iid according to (12). If there is not enough evidence in
the data that the ith parameter should be non-zero, this prior will favor a large value for
δi which means that the ith parameter in eﬀect will be switched oﬀ. Hence, the ard prior
will encourage a sparse solution(MacKey, 1994; Neal, 1996) for further discussion. and the
collection of unknowns of the model is given by
η = {θ, δ1:m, z1:T, λ, ν},
(13)
where θ is the parameter vector of the over-parameterised model of order nmax.

3
Markov chain Monte Carlo
273
Markov chain Monte Carlo
Assume that we have observed a sequence of input/output pairs DT = {u1:T, y1:T }. We then
seek the posterior distribution of the model parameters, p(η | DT ), which is not available
in closed form. An mcmc sampler is therefore used to approximately sample from the
posterior distribution. The most fundamental mcmc sampler is known as the Metropolis-
Hastings ( mh) algorithm. In this method, we propose a new value for the state of the
Markov chain from some arbitrary chosen proposal kernel. The proposed value is then
accepted with a certain probability, otherwise the previous state of the chain is kept.
A special case of the mh algorithm is the Gibbs sampler. In this method, we loop over
the diﬀerent variables of our model, sampling each variable conditioned on the remaining
ones. By using these conditional posterior distributions as proposals, the mh acceptance
probability will be exactly one. Hence, the Gibbs sampler will always accept its proposed
values. As pointed out by Tierney (1994), it is possible to mix diﬀerent types of proposals.
This will be done in the sampling strategies employed in this work, where we use Gibbs
moves for some variables and random walk mh moves for other variables.
The rjmh sampler (Green, 1995) is a generalisation of mh, which allows for moves between
parameter spaces of diﬀerent dimensionality. This approach will be used in this work, for
the model presented in Section 2.2. The reason is that when the model order n is seen
as a parameter, the dimension of the vector θn will change between iterations. An rjmh
sampler can be seen as employing standard mh moves, but all variables that are aﬀected by
the changed dimensionality must either be accepted or rejected as a group. That is, in our
case, we propose new values for {n, θn} as a pair, and either accept or reject both of them
(see step (I-1a) below).
For the arx model with parametric model order, we employ an rjmh sampler using the
following sweep1,
(I-1) Order and arx coeﬃcients:
(a) Draw {θn⋆, n⋆}| zs+1:T, λ, δ, DT .
(b) Draw δ⋆|θn⋆, n⋆.
(I-2) Innovation parameters:
(a) Draw z⋆
s+1:T |θn⋆, n⋆, λ, ν, DT .
(b) Draw λ⋆|θn⋆, n⋆, z⋆
s+1:T, DT .
(c) Draw ν⋆| z⋆
s+1:T .
If we instead consider the arx model with an ard prior we use the following sweep,
denoted Gibbs,
(II-1)
arx coeﬃcients:
(a) Draw θ⋆| zs+1:T, λ, δ1:m, DT .
1The reason for why we condition on some variables from time s + 1 to T , instead of from time 1 to T , is to
deal with the unknown initial state of the system. This will be explained in more detail in Section 4.2.

274
Paper F
Hierarchical Bayesian approaches for robust inference in ARX models
(b) Draw δ⋆
1:m |θ⋆.
(II-2) Innovation parameters:
(a) Draw z⋆
s+1:T |θ⋆, λ, ν, DT .
(b) Draw λ⋆|θ⋆, z⋆
s+1:T, DT .
(c) Draw ν⋆| z⋆
s+1:T .
The diﬀerence between the two methods lies in steps (I-1) and (II-1), where the parameters re-
lated to the arx coeﬃcients are sampled. In steps (I-2) and (II-2), we sample the parameters
of the excitation noise distribution, which are essentially the same for both samplers.
Posteriors and proposal distributions
In this section, we present the posterior and proposal distributions for the model order and
other parameters used by the proposed mcmc methods.
Model order
Sampling the model order and the arx coeﬃcients in step (I-1a) is done via a reversible
jump mh step. We start by proposing a new model order n′, according to some chosen
proposal kernel q(n′|n). In this work, we follow Troughton and Godsill (1998) and use a
constrained random walk with discretised Laplace increments with scale parameter `, i.e.,
q(n′
a |n) ∝exp(−`|n′
a −na|),
if 1 ≤n′
a ≤nmax,
(14)
and analogously for nb. This proposal will favour small changes in the model order, but
allows for occasional large jumps.
Once we have sampled the proposed model order n′, we generate a set of arx coeﬃcients
from the posterior distribution
θn′ ∼p(θn′ |n′, zs+1:T, λ, δ, DT ) = N(θn′; µθn′, Σθn′).
(15)
The expressions for the mean and the covariance are provided in the subsequent section.
Since the proposed coeﬃcients θn′ are directly connected to the model order n′, we apply
an mh accept/reject decision to the pair {θn′, n′} with the acceptance probability given by
ρnn′ ≜1 ∧p(n′, θn′ | zs+1:T, λ, δ, DT )
p(n, θn | zs+1:T, λ, δ, DT )
q(n, θn |n′, θn′)
q(n′, θn′ |n, θn)
= 1 ∧p(n′| zs+1:T, λ, δ, DT )
p(n | zs+1:T, λ, δ, DT )
q(n |n′)
q(n′|n),
(16)
where a ∧b ≜min(a, b). Since
p(n | zs+1:T, λ, δ, DT ) ∝p(y1:T |n, zs+1:T, λ, δ, u1:T )p(n),
(17)

4
Posteriors and proposal distributions
275
where the prior over model orders is ﬂat according to (7), the acceptance probability can
be simpliﬁed to (Troughton and Godsill, 1998)
ρnn′ = 1 ∧
δ
n′
2 |Σθn′ |
1
2 exp   1
2 µ⊤
θn′Σ−1
θn′ µθn′
δ
n
2 |Σθn|
1
2 exp   1
2 µ⊤
θnΣ−1
θn µθn
q(n |n′)
q(n′|n).
Note by (21) that the acceptance probability does not depend on the actual value of θn′.
Hence, we do not have to carry out the sampling according to (15) unless the proposed
sample is accepted.
ARX coefficients
The arx coeﬃcients are sampled in step (I-1a) and step (II-1a) of the two proposed mcmc
samplers, respectively. In both cases, we sample from the posterior distribution over the
parameters; see (15). In this section, we adopt the notation used in the rjmh sampler, but
the sampling is completely analogous for the Gibbs sampler. A stacked version of the linear
regression model (5) is
ys+1:T = Φnθn + es+1:T,
(18)
where the regression matrix Φn is given by
Φn =

−ys
· · ·
−ys−na
us
· · ·
us−nb+1
...
...
...
...
...
...
−yT −1
· · ·
−yT −na
uT −1
· · ·
uT −nb

.
(19)
Here, we have take into account that the initial state of the system is not known, and only
use observations from time s + 1 to T in the vector of observations on the left hand side of
(18). For the rjmh sampler s = max(na, n′
a) and for the Gibbs sampler s = nmax.
Let ∆−1 be the covariance matrix for the parameter prior, according to (8) or to (11), i.e.,
∆−1 =

δIna+nb
for rjmh,
diag(δ1, . . ., δm)
for Gibbs.
(20)
Since we condition on the latent variables zs+1:T (and the variance parameter λ−1), the
noise term in (18) can be viewed as Gaussian according to (3b). It follows that the posterior
parameter distribution is Gaussian, as stated in (15), with mean and covariance given by
µθn = Σθn(Φn)⊤(λzs+1:T ◦ys+1:T ),
(21a)
Σθn =  (Φn)⊤diag(λzs+1, . . ., λzT )Φn + ∆−1 ,
(21b)
respectively. Here, ◦denotes elementwise multiplication.
ARX coefficients variance
We now derive the posterior distributions for the arx coeﬃcients variance(s), sampled in
steps (I-1b) and (II-1b) for the two models, respectively.

276
Paper F
Hierarchical Bayesian approaches for robust inference in ARX models
Consider ﬁrst the model described with parametric model order. The arx coeﬃcients vari-
ance δ−1 is a priori gamma distributed according to (9). The likelihood is given by (8) and an
analytical expression for the posterior distribution is easily found as the gamma distributed
is a conjugate prior. Thereby motivating the standard choice of a gamma distributed prior
for the inverse variance in a Gaussian distribution. It follows from standard results (see e.g.,
Bishop (2006, p. 100)) that
p(δ |θn, n) = G(δ; αpost
δ
, βpost
δ
),
(22)
with hyperparameters
αpost
δ
= αδ + na + nb
2
,
and
βpost
δ
= βδ + 1
2(θn)⊤θn.
(23)
Similarly, for the ard model, we get from the prior (12) and the likelihood (11), that the
posterior distributions for the arx coeﬃcients variances are given by
p(δi |θi) = G(δi; αpost
δi , βpost
δi ),
(24)
with hyperparameters
αpost
δi
= αδ + 1
2,
and
βpost
δi
= βδ + 1
2θ2
i ,
(25)
for i = 1, . . ., m.
Latent variance variables
Let us now turn to the parameters deﬁning the excitation noise distribution. We start with
the latent variance variables zs+1:T . These variables are sampled analogously in steps (I-2a)
and (II-2a). The latent variables are a priori gamma distributed according to (3a) and since
they are iid, we focus on one of them, say zt. Note that we here once again have chosen a
prior distribution conjugate to the likelihood.
The likelihood model for zt is given by (5), where the model order now is ﬁxed since we
condition on n (in the ard model, the order is always ﬁxed)
p(yt | zt, θn, n, λ, ν, ϕn
t ) = N(yt, (ϕn
t )⊤θn, (λzt)−1).
(26)
It follows that the posterior is given by
p(zt |θn, n, λ, ν, DT ) = G(zt; αpost
z
, βpost
zt ),
(27)
with the hyperparameters
αpost
z
= 1
ν + 1
2,
and
βpost
zt
= ν
2 + λ
2 ϵ 2
t .
(28)
Here, the prediction error ϵ t is given by
ϵ t = yt −(ϕn
t )⊤θn.
(29)
We can thus generate z⋆
s+1:T by sampling independently from (27) for t = s + 1, . . ., T .

5
Numerical illustrations
277
Innovation scale parameter
The innovation scale parameter λ is sampled in steps (I-2b) and (II-2b). This variable follows
a model that is very similar to zt. The diﬀerence is that, whereas the individual zt variables
are iid and only enter the likelihood model (5) for a single t each, we have the same λ for
all time instances. The posterior distribution of λ is thus given by
p(λ |θn, n, zs+1:T, DT ) = G(λ; αpost
λ
, βpost
λ
),
(30)
with
αpost
λ
= αλ + T −s
2
,
and
βpost
λ
= βλ + 1
2ϵ ⊤
s+1:T (zs+1:T ◦ϵ s+1:T ),
(31a)
where the prediction errors ϵ s+1:T are given by (29).
Innovation DOF
The dof ν, sampled in steps (I-2c) and (II-2c), is a priori gamma distributed according to
(4b). The likelihood for this variable is given by (3a). It follows that the posterior of ν is
given by
p(ν | zs+1:T ) ∝p(zs+1:T | ν)p(ν) =
T
Y
t=s+1
G(zt; ν/2, ν/2)G(ν; αν, βν).
(32)
Unfortunately, this does not correspond to any standard distribution. To circumvent this,
we apply an mh accept/reject step to sample the dof. Hence, we propose a value according
to some proposal kernel ν ′ ∼q(ν ′| ν). Here, the proposal is taken as a Gaussian random
walk, constrained to the positive real line. The proposed sample is accepted with probability
ρνν′ = 1 ∧p(ν ′| zs+1:T )
p(ν | zs+1:T )
q(ν | ν ′)
q(ν ′| ν),
(33)
which can be computed using (32).
Numerical illustrations
We now give some numerical results to illustrate the performance of the proposed methods.
First, we compare the average performance of the mcmc samplers with least squares ( ls)
in Section 5.1. These experiments are included mostly to build some conﬁdence in the
proposed method. We then illustrate how the proposed methods are aﬀected by outliers and
missing data in Section 5.2. As a ﬁnal example, in Section 5.3 we illustrate the performance
of the rjmh on real eeg data.
Average model performance
We evaluate the proposed methods by analysing the average identiﬁcation performance
for 25, 000 randomly generated arx systems. These systems are generated by sampling a
uniform number of poles and zeros (so that the resulting system is strictly proper) up to

278
Paper F
Hierarchical Bayesian approaches for robust inference in ARX models
Method
Mean
ci
ls
77.51
[77.21 77.81]
rjmh
78.24
[77.95 78.83]
Gibbs
77.73
[77.47 78.06]
Table 1. The average and 95% conﬁdence intervals (cis) for the model ﬁt (in percent) from
experiments with 25, 000 random arx models.
some maximum order, here taken as 30. The poles and zeros are generated uniformly over
a disc with radius 0.95.
For each system, we generate T = 450 observations2. The input signal ut is generated as
Gaussian white noise with standard deviation 0.1. The innovations are simulated from a
Student’s t distribution, et ∼St(0, 1, 2). The hyperparameters of the model are chosen as
αλ = βλ = αν = βν = αδ = βδ = 0.1.
The data is split into three parts with 150 observations each. The ﬁrst two parts are used
for model estimation, and the last part is used for testing the model. For the ls method,
we employ cross validation by ﬁrst estimating models for all possible combinations of
model orders na and nb, such that both are less than or equal to nmax = 30, on the ﬁrst
batch of data. We then pick the model corresponding to the best model ﬁt (Ljung, 1999,
p. 500). The full estimation data set (300 observations) is then used to re-estimate the model
parameters. For the mcmc methods, we use all the estimation data at once, since these
methods comprise automatic order determination and no explicit order selection is made.
The average model ﬁt for the test data, for the 25,000 arx systems is given in Table 1. We
note a slight statistically signiﬁcant improvement by using the rjmh method in comparison
with the standard ls technique. Also, the rjmh appear to perform better than the simpler
Gibbs method (for this model class). Therefore, we will focus primarily on the former
method in the remainder of the numerical illustrations.
In the upper part of Figure 1, the diﬀerences in model ﬁt between rjmh and ls for all 25,000
systems are shown. We note that there are no cases with large negative values, indicating
that the rjmh method performs at least as good as, or better than, ls for the vast majority
of these systems. We also note that there are a few cases in which ls is much worse that
rjmh. Hence, the average model ﬁt for ls is deteriorated by the fact that the method fails
completely from time to time. This is not the case for the proposed rjmh sampler (nor for
the Gibbs sampler), which suggests that the proposed method is more robust to variations
in the data.
It is interesting to review a typical case with a large diﬀerence in model ﬁt between the two
methods. Data from such a case is shown in the lower part of Figure 1. Here, we see a large
jump in the system state. The arx model with Student’s t distributed innovations can,
due to the heavy tails of the noise distribution, accommodate for the large output values
2When simulating the systems, we run the simulations for 900 time steps, out of which the ﬁrst 450 observations
are discarded, to remove the eﬀect of transients.

5
Numerical illustrations
279
0
5000
10000
15000
20000
25000
0.0
0.5
1.0
1.5
2.0
system
diﬀerence in model ﬁt
-0.02
-0.01
0.00
0.01
0.02
diﬀerence in model ﬁt
0
100
200
300
400
-150
-100
-50
0
time (t)
signal (y)
Figure 1. Upper: The diﬀerence in model ﬁt between the rjmh and ls methods. Middle: A box-
plot of the diﬀerence in model ﬁt with the outliers removed. Lower: One particular randomly
generated arx model with a large innovation outlier that aﬀects the system output.

280
Paper F
Hierarchical Bayesian approaches for robust inference in ARX models
better than the model with Gaussian noise. The model ﬁt for this system was 46.15% for
the rjmh method and 14.98% for the ls methods.
It is important to note that the use of the ls method is due to its simplicity. For the problem
under study the ls method is the maximum likelihood ( ml) solution to an arx model
with Gaussian noise and a given model order. The ml problem can of course also be posed
for the case where t-distributed noise is assumed. Another alternative would be to make
use of a prediction error method with a robust norm, such as the Huber or Vapnik norm.
A cross validation scheme could also be used to handle the automatic order determination
in this setting by an exhaustive search of the model set.
Robustness to outliers and missing data
We continue by evaluating the proposed models and inference algorithms in the presence
of missing data or outliers in the observations. The hypothesis is that, due to the use of
Student’s t innovations in the model, we should be more robust to such data anomalies
than an ls estimate (based on a Gaussian assumption).
In these experiments, the innovations used in the data generation are drawn from a Gaus-
sian distribution with unit variance. We then add outliers or missing observations to the
outputs of the systems (i.e., this can be interpreted as an eﬀect of sensor imperfections or
measurement noise). This is done by randomly selecting between 1–3 % of the observations
in the estimation data, which are modiﬁed as described below. In the ﬁrst set of experiments
we add outliers to the selected observations. The size of the outliers are sampled from a
uniform distribution U(−5y+, 5y+), with y+ = max |yt |. In the second set of experiment,
we instead replace the selected observations by zero-mean Gaussian noise with variance
0.01. This is to represent missing data due to sensor errors, resulting in values close to zero
compared with the actual observations.
For each scenario, we generate 1, 000 random arx systems and simulate T = 450 observa-
tions from each. We then apply the proposed mcmc samplers and ls with cross validation,
similarly to the previous sections, but with the modiﬁcations described above. Table 2 gives
the average results over the 1, 000 randomly generated models with added outliers and miss-
ing values, respectively. Here, we have not corrupted the test data by adding outliers or
missing observations, not to overshadow the results3.
The mean results show statistically certain diﬀerences between the ls approach and the
two proposed methods. We conclude that, in general the proposed mcmc based methods
are more robust to data anomalies such as missing observations or outliers.
In Figure 2, the predicted versus the corresponding observed data points are shown for the
rjmh method (green stars) and the ls approach (orange dots), for two of the data batches.
It is clearly visible that the ls method is unable to handle the problem with outliers, and
the predictions are systematically too small (in absolute value). ls performs better in the
situation with missing data, but the variance of the prediction errors is still clearly larger
than for the rjmh method.
3If an outlier is added to the test data, the model ﬁt can be extremely low even if there is a good ﬁt for all time
points apart from the one where the outlier occurs.

5
Numerical illustrations
281
-100
-50
0
50
100
-100
-50
0
50
100
observations
predictions
LS
-100
-50
0
50
100
-100
-50
0
50
100
observations
predictions
rjMH
-100
-50
0
50
100
-100
-50
0
50
100
observations
predictions
LS
-100
-50
0
50
100
-100
-50
0
50
100
observations
predictions
rjMH
Figure 2. Predictions versus observations for data with outliers (upper) and data with missing
observations (lower). The model ﬁt values for the outlier data example are 91.6% for the rjmh
(orange stars) and 40.2% for ls (green dots). The corresponding values for the missing data
example are 94.4% and 75.7%.

282
Paper F
Hierarchical Bayesian approaches for robust inference in ARX models
Outliers
Missing data
Method
Mean
ci
Mean
ci
ls
39.13
[37.86 40.41]
75.20
[74.00 76.40]
rjmh
70.54
[69.03 72.04]
80.18
[78.74 81.62]
Gibbs
72.46
[71.02 73.91]
81.57
[80.24 82.90]
Table 2. The mean and 95% cis for the model ﬁt (in percent) from 1, 000 systems with outliers
and missing data, respectively.
Real EGG data
We now present some results from real world eeg data, which often include large outliers
(and therefore deviates from normality). Therefore this data serves as a good example for
when the propose methods are useful in a practical setting. The deviations from normality
can be seen in Figure 3, by observing the signal and the qq-plot, i.e., a comparison between
distributions by plotting their quantiles against each other (Wilk and Gnanadesikan, 1968).
The rjmh method with Student’s t innovations is used to estimate an ar model for this data
set. The resulting estimated posterior density for the model order is shown in the lower left
part of Figure 3. Knowing this posterior, allows for e.g., weighting several diﬀerent models
together using the estimated density values.
In addition, we can also estimate the posterior density of the dof of the innovations. This
density is useful for quantifying deviations from normality, as the Gaussian distribution
is asymptotically recovered from the Student’s t distribution with inﬁnite dof. As the
maximum posterior value is attained at approximately 4.0 dof, this conﬁrm non-Gaussian
innovations. We have thereby illustrated the usefulness of the proposed methods, both for
parameter inference but also for estimating useful posterior densities not easily obtainable
in the ls framework.
Conclusions and Future work
We have considered hierarchical Bayesian arx model with Student’s t distributed innova-
tions. This was considered to be able to capture non-Gaussian elements in the data and to
increase robustness. Furthermore, both models contain a mechanism for automatic order
selection. To perform inference in these models, we also derived two mcmc samplers: a
rjmh sampler and a standard Gibbs sampler.
Three numerical examples have been presented, providing evidence that the proposed mod-
els provide increased robustness to data anomalies, such as outliers and missing data. We
have shown that the proposed methods perform on average as good as (Gibbs) or better
(rjmh) than ls with cross validation, when the true system is in the model class. Another
beneﬁt with the proposed methods is that they provide a type of information which is not
easily attainable using more standard techniques. As an example, this can be the posterior
distribution over the model orders, as illustrated in Figure 3.

6
Conclusions and Future work
283
0
50
100
150
200
250
300
350
2500
3000
3500
4000
4500
5000
5500
time (s)
signal (y)
339.3
339.4
339.5
339.6
4400
4450
4500
4550
4600
time (s)
signal (y)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
model order
probability
1
2
3
4
5
6
7
-4
-2
0
2
4
2000
3000
4000
5000
6000
standard Gaussian quantiles
sample quantiles
Figure 3. Upper: the eeg signal (green) collected on one speciﬁc channel and patient with the
one-step-ahead predictions (orange). Middle: the last 100 samples from the upper graph. Lower
left: The estimated posterior model order density from the rjmh method. Lower right: The
qq-plot for the data set. The model ﬁt for the results in this ﬁgure is 85.6%.

284
Paper F
Hierarchical Bayesian approaches for robust inference in ARX models
There are several interesting avenues for future research, and we view the present work as
a stepping stone for estimating more complex models. The next step is to generalize the
proposed methods to encompass e.g., oe and armax models. A more far reaching step is
to generalize the methods to non-linear systems, possibly by using Particle mcmc methods
(Andrieu et al., 2010). It is also interesting to further analyse the use of sparseness priors in
this setting.
Acknowledgements
The eeg data was kindly provided by Eline Borch Petersen and Thomas Lunner at Erik-
sholm Research Centre, Oticon A/S, Denmark.

Bibliography
285
Bibliography
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342,
2010.
C. M. Bishop. Pattern recognition and machine learning. Springer Verlag, New York, USA,
2006.
S. P. Brooks, P. Giudici, and G. O. Roberts. Eﬃcient construction of reversible jump
Markov chain Monte Carlo proposal distributions. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 65(1):3–55, February 2003.
J. Christmas and R. Everson. Robust autoregression: Student-t innovations using variational
Bayes. IEEE Transactions on Signal Processing, 59(1):48–57, 2011.
J. Dahlin, F. Lindsten, T. B. Schön, and A. Wills. Hierarchical Bayesian ARX models
for robust inference. In Proceedings of the 16th IFAC Symposium on System Identiﬁcation
(SYSID), Brussels, Belgium, July 2012.
S. Godsill. On the relationship between Markov chain Monte Carlo methods for model
uncertainty. Journal of Computational and Graphical Statistics, 10(2):230–248, 2001.
P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination. Biometrica, 82(4):711–732, 1995.
L. Ljung. System identiﬁcation: theory for the user. Prentice Hall, 1999.
D. J. C. MacKey. Bayesian non-linear modelling for the prediction competition. ASHRAE
Transactions, 100(2):1053–1062, 1994.
R. M. Neal. Bayesian learning for neural networks. Springer Verlag, 1996.
C. P. Robert and G. Casella. Monte Carlo statistical methods. Springer Verlag, 2 edition,
2004.
L. Tierney. Markov chains for exploring posterior distributions. The Annals of Statistics, 22
(4):1701–1728, 1994.
P. T. Troughton and S. J. Godsill.
A reversible jump sampler for autoregressive time
series. In Proceedings of the 23rd International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Seattle, USA, May 1998.
M. B. Wilk and R. Gnanadesikan. Probability plotting methods for the analysis of data.
Biometrika, 55(1):1–17, March 1968.


Paper G
Bayesian inference for
mixed effects models with heterogeneity
Authors:
J. Dahlin, R. Kohn and T. B. Schön
Preliminary version:
Technical Report LiTH-ISY-R-3091, Dept. of Electrical Engineering, Linköping
University, SE-581 83 Linköping, Sweden.


Bayesian inference for
mixed effects models with heterogeneity
J. Dahlin⋆, R. Kohn† and T. B. Schön‡
⋆Dept. of Electrical Engineering,
Linköping University,
SE–581 83 Linköping, Sweden.
johan.dahlin@isy.liu.se
†UNSW Business School,
University of New South Wales,
Kensington nsw 2033, Sydney, Australia.
r.kohn@unsw.edu.au
‡Dept. of Information Technology,
Uppsala University,
SE-751 05 Uppsala, Sweden.
thomas.schon@it.uu.se
Abstract
We are interested in Bayesian modelling of panel data using a mixed eﬀects model with heterogeneity in
the individual random eﬀects. We compare two diﬀerent approaches for modelling the heterogeneity
using a mixture of Gaussians. In the ﬁrst model, we assume an inﬁnite mixture model with a Dirichlet
process prior, which is a non-parametric Bayesian model. In the second model, we assume an over-
parametrised ﬁnite mixture model with a sparseness prior. Recent work indicates that the second
model can be seen as an approximation of the former. In this paper, we investigate this claim and
compare the estimates of the posteriors and the mixing obtained by Gibbs sampling in these two
models. The results from using both synthetic and real-world data supports the claim that the estimates
of the posterior from both models agree even when the data record is ﬁnite.
Keywords
Bayesian inference, mixed eﬀects model, panel/longitudinal data, Dirichlet process mixture, ﬁnite
mixture, sparseness prior.
Data and source code in R
https://github.com/compops/panel-dpm2016
Financial support from
The projects Probabilistic modeling of dynamical systems (Contract number: 621-2013-5524) and cadics,
a Linnaeus Center, both funded by the Swedish Research Council.
289

290
Paper G
Bayesian inference for mixed effects models with heterogeneity
Introduction
In many ﬁelds, we are interested in modelling the dependence of an observation yit of
individual i at time t given some regressors/covariates xit. This type of data is known
as panel data in economics (Baltagi, 2008) and longitudinal data in statistics (Verbeke and
Molenberghs, 2009). That is, when we obtain multiple observations T of a number of N
individuals over time. A common application is health surveys in which annual question-
naires are sent out to a group of individuals. The focus is then to isolate diﬀerent factors
that are correlated with disease or visits to the doctor. However, the importance of these
factors can vary between diﬀerent sub-groups in the population and this is referred to as
heterogeneity. For prediction purposes, it is therefore important to captures these variations
and be able to identify to which sub-group a speciﬁc individual belongs.
Another popular application is recommendation systems for online retailing or streaming
sites such as Amazon, Netﬂix and Spotify. The underlying algorithms vary between diﬀerent
sites and are often proprietary information unknown to the public. However, academic
work in recommendation system by Condliﬀet al. (1999) and Ansari et al. (2000) have made
used of panel data models. The common theme for both applications are that the number
of observations T for each individual is typically much smaller than the size N of the
population. It is therefore essential to pool information together from similar individuals
to construct a model from data.
For this end, we consider a linear mixed eﬀects model (Verbeke and Lesaﬀre, 1996) with
observation yit ∈R for individual i = 1, . . ., N at time t = 1, . . .,T . This model can be
expressed as
yit |α, βs
i, σ2
e, xit ∼N

yit, αxit + βs
i zit, σ2
e/ωi

,
(1)
where α ∈Rd denotes the ﬁxed eﬀects and βs
i = { βs
ij}p
j=1 ∈Rp denotes the random eﬀects
for individual i. Here, xit and zit denote the design matrices connected to the ﬁxed and
random eﬀects, respectively. Furthermore, σe > 0 denotes the standard deviation of noise
aﬀecting the observations and ωi > 0 denotes the individual scaling of the variance to allow
for variance heterogeneity. We denote a Gaussian distribution with mean µ and standard
deviation σ > 0 by N(µ, σ2).
In this paper, we assume that βs
i can be modelled as an inﬁnite mixture of Gaussians. This
means that the random eﬀects can for example be distributed according to a multi-modal
distribution. Each mode would then potentially correspond to a certain sub-group of the
population with similar behaviour. This information could be important in marketing,
economics, medicine and other applications as discussed by Allenby et al. (1998), Canova
(2004) and Lopes et al. (2003). A potential beneﬁt of having an inﬁnite mixture is that the
data determines the number of components to include in the model.
An inﬁnite mixture of Gaussians can be expressed by
βs
i ∼
∞
X
k=1
ηkN(βs
i ; βk,Qk),
s.t.
∞
X
k=1
ηk = 1,
(2)

2
Bayesian mixture modelling
291
for some weights ηk > 0, mean vector βk ∈Rp and covariance matrix Qk ∈Rp×p. We
proceed to model this mixture in a Bayesian setting in Section 2, where a Dirichlet process
(dp; Ferguson, 1973, 1974) prior is employed. This is a Bayesian non-parametric method,
which can act as a prior for probability distributions such as (2). However, the inference
often relies on Markov chain Monte Carlo (mcmc; Robert and Casella, 2004), which can
be challenging to implement and sometimes mixes poorly. The latter is discussed by Hastie
et al. (2015) and could be a problem when applying this kind of model for big data.
The main contribution of this paper is to compare two diﬀerent models for the heterogenity
described by (2). In the ﬁrst approach, we make use of a dp model for (2). In the second
approach, we truncate the inﬁnite mixture to obtain an overparametrised ﬁnite mixture
( fm) for which we make use of a sparseness prior. The latter model is often simpler to
implement and can also enjoy better mixing properties. Ishwaran and Zarepour (2002)
and Rousseau and Mengersen (2011) have analysed the properties of this approximation.
In short, the results are that the approximation is asymptotically consistent (in NT ) and
converges to the solution obtained when using a dp. The fm is over-parametrised but the
sparseness prior empties the superﬂuous components. Our aim is to compare these two
approaches for mixed eﬀects models in terms of the estimate of the posterior and the mixing
in the Markov chain constructed by mcmc algorithms.
The comparisons are made on both synthetic and real-world data. The results indicate that
the posterior estimates are similar in most cases and that the similarity increases as N and
T tend to inﬁnity. The mixing is compared using a Monte Carlo simulation with synthetic
data. The resulting mixing seems to be similar for the ﬁnite and inﬁnite mixture models.
Therefore, there is nothing lost or gained by using either model compared with the other.
More work is needed to see if this result holds for even larger sets of data and alternative
sampling schemes.
Bayesian mixture modelling
In this section, we discuss the details of the two approaches for modelling the mixture of
the random eﬀects (2). The ﬁrst approach uses the dp prior of the inﬁnite mixture of
Gaussians and the second approach approximates the inﬁnite mixture by a fm. We return
to the problem of sampling from the posterior of the parameters in the mixed eﬀects model
and the mixture for the random eﬀects in Section 3.
Infinite mixture model using a Dirichlet process
In the ﬁrst approach, we model the random eﬀects βs
i using the inﬁnite mixture model
in (2) with the dp (Ferguson, 1973, 1974) as a prior. The mixture model can be seen as a
hierarchical Bayesian model, which we develop step by step in this section.
The dp is an example of a Bayesian non-parametric model, where the number of parameters
grow with the number of observations and can be viewed as inﬁnite. A realisation G from
a dp is a random discrete probability distribution in the form of an empirical distribution.

292
Paper G
Bayesian inference for mixed effects models with heterogeneity
Hence, we can express G (Gelman et al., 2013) by
G =
∞
X
k=1
ηkδϑk,
(3)
where the weights {ηk}∞
k=1 and locations {ϑk}∞
k=1 are random variables. Here, δϑ′ denotes
a Dirac measure placed at ϑ′, where ϑ denotes the parameters of the mixture component.
Furthermore, we have that P∞
k=1 ηk = 1 with probability 1, which means that G can be
interpreted as a probability measure.
Let DP(η0,G0) denote a dp with the concentration parameter η0 > 0 and the base measure
G0. We say that G is distributed according to a dp if all of its marginal distributions
are Dirichlet distributed. Let D(η) denote the Dirichlet distribution with concentration
parameter η = {η1, . . ., ηR}. Hence, if G0 is a probability measure on the space (Ω, F), we
have that

G(A1),G(A2), . . .,G(AN )

∼D

η0G0(A1), η0G0(A2), . . ., η0G0(AN )

,
(4)
for any ﬁnite (measurable) partition A1:N of Ω. Note that the expected value of G is the
base measure and therefore G has the same support as G0. Moreover, G is discrete with
probability one even if the base measure is continuous, which is useful in mixture models
as discussed below.
Assume that we obtain some data generated from the model given by
G ∼DP(η0,G0),
ϑi |G ∼G,
i = 1, 2, . . . .
In many applications, we would like to compute the predictive distribution for some new
parameter ϑ⋆given the observations ϑ1:N . The predictive distribution can be computed as
a marginalisation given by
p(ϑ⋆|ϑ1:N ) =

G(ϑ⋆)p(G|ϑ1:N )dG,
which is possible to carry out in closed-form. The result is a so-called Pólya urn scheme
discussed by Blackwell and MacQueen (1973). This scheme can be expressed mathematically
ϑ⋆|ϑ1:N ∼
η0
η0 + N G0 +
1
η0 + N
N
X
i=1
niδϑi .
(5)
Here, ni denotes the number of parameters that are identical to ϑi, i.e.,
ni =
N
X
j=1
I [ϑj = ϑi],
where I [A] denotes the indicator function which assumes the value one if A is true and
zero otherwise.
From (5), we note that the dp is a discrete process with a non-zero probability of ties, i.e.,
that two or more realisations are identical to each other. From the Pólya urn scheme, we
either draw a new parameter from the base measure or an existing parameter from the Dirac
mixture. The probability of sampling a new parameter from the base measure is determined

2
Bayesian mixture modelling
293
by concentration parameter. We are more likely to sample from the base measure if η0 ≫
N , which means that the predictive posterior concentrates to the base measure. If the
concentration parameter is small, we often sample from the existing parameters, which
gives many ties and a strong clustering behaviour.
Hence, we can make use of this clustering eﬀect to reformulate (2) as a Dirichlet process
mixture (dpm; Antoniak, 1974) given by
G ∼DP(α,G0),
β,Q |G ∼G,
βs
i ∼N(β,Q),
(6)
where we choose ϑ = { β,Q} for this particular model. The presence of ties means that
several βs
i will be drawn from a Gaussian distribution with the same parameters. Hence,
this is an alternative presentation of (2).
We have now introduced the dpm and discussed the properties of the underlying dp as well
as how to compute the predictive posterior distribution. What remains is to discuss how to
simulate from a dpm and how to compute the prior-posterior update given some data. In
this paper, we make use of the stick-breaking by Sethuraman (1994) to simulate from the
mixture. The procedure iterates
βk,Qk ∼G0,
ηk = Vk
Y
l<k
(1 −Vl),
Vk ∼B(1, η0),
(7)
for k = 1, . . ., R, where R is an upper bound on the number of components in the mixture
selected by the user. Here, B(a, b) denotes a Beta distribution with shape a > 0 and scale
b > 0. Moreover, Q
l<k(1 −Vl) denotes the length remaining of a unit stick after breaking
oﬀk −1 pieces, where each Vk denotes the fraction of the remaining stick to break oﬀ.
Note that the truncation implied by the stick-breaking corresponds a mixture given by
βs
i |η1:R, β1:R,Q1:R ∼
R
X
r=1
ηrN(βs
i ; βr,Qr).
In many cases, we can choose R to be larger than the number of individuals N , so this is not
a problem when n is rather small. To see the connection between the sick-breaking and (5),
we note that the expected value of the Beta distribution sampled from in (7) is (1 + η0)−1.
Hence, the expected part of the stick to break oﬀtends to zero when η0 →∞and tends
to one when η0 →0. In the latter case, we usually obtain a few components, which is
in agreement with the previous discussion of the concentration parameter. We return to
computing the posterior of a dpm in Section 3 using Gibbs sampling.
Finite mixture model
The second approach that we consider is the fm, which can be recovered as a special case of
the dpm model (6) given by
βs
i |Si, β1:R,Q1:R ∼N(βSi,QSi),
Si |η ∼M(1, η1, . . ., ηR),
(8a)
βk,Qk
∼G0,
η ∼D(η0,1, . . ., η0,R),
(8b)
where the latent variable Si denotes which component that individual i belongs to. Here,
M(1, p) denotes the multinomial distribution with one try and event probabilities given by

294
Paper G
Bayesian inference for mixed effects models with heterogeneity
p. The parameters of the mixture components are still realisations from the base measure.
However, the dp is now a Dirichlet distribution, which follows from the property of the
dp discussed in connection with (4). The Dirichlet distribution determines the probability
of a certain cluster membership Si. Hence, we get a similar clustering eﬀect as for the dpm
if we select η0 to be small. This is essentially the same result as we had for the Pólya urn
scheme in (5).
An interesting property discussed by Ishwaran and Zarepour (2002) and Rousseau and
Mengersen (2011) is that the fm can approximate the dpm. This means that the sparsity
will empty unnecessary components and that the estimate of the posterior tend to the true
one. However, we need to be careful when setting the hyperparameters in the Dirichlet
prior distribution for this approximation to work. On choice advocated by Ishwaran and
Zarepour (2002) is to use the concentration parameter η0,1 = . . . = η0,R = η0/R with η0 =
1. Here, the number of components R can be selected as n if the amount of data is small.
Here, the small value of η0 results in that a few samples from the Dirichlet distribution
are allocated most of the probability mass. Therefore, we get a sparsity eﬀect as only a few
components are occupied a priori when η0 ≤1.
We discuss how to sample from the posterior of the fm (8) in Section 3
Sampling from the posterior
In this section, we make use of mcmc sampling to estimate the posterior of the param-
eters in the mixed eﬀects model (1) and the mixture modelling the random eﬀects (2)
given the data {y, x, z} = {{yit, xit, zit }n
i=1}T
t=1}. The parameter vector is given by θ =
{α, β1:R,Q1:R, βs
1:N,ω1:N, σ2
e, S1:N, η1:R}, which includes the parameters of the dpm (6)
and the fm (8). To decrease the notational burden, we have used the same notation for the
parameters in both mixture models.
We make use of conjugate priors to obtain closed-form conditional posteriors, which allows
for Gibbs sampling as discussed by Gelman et al. (2013), Frühwirth-Schnatter (2006) and
Neal (2000). Note that there are many other interesting alternatives to Gibbs sampling for
estimating the posterior of a dpm. Sequential Monte Carlo algorithms (Del Moral et al.,
2006) discussed by Fearnhead (2004) and Ulker et al. (2010) do not have problems with
mixing but can be challenging to implement.
Slice samplers are also an interesting alternative discussed by Walker (2007), which are easy
to implement and give moderate or good mixing. Finally, split-merge algorithms can give
good mixing as discussed by Jain and Neal (2004) and Bouchard-Côté et al. (2015) but can
be challenging to implement. For the fm, a simple Gibbs sampling approach often gives
good mixing and is easy to implement.
Prior distributions
To compute the posterior, we need to assign prior distributions for each of the elements in
θ. Here, we make use of the prior distributions from Frühwirth-Schnatter (2006, p. 265)

3
Sampling from the posterior
295
for all the parameters except for the mixture weights η1:R. For the ﬁxed eﬀects and the noise
variance with heterogeneity, we choose
α ∼N(α; a0, A0),
σ2
e ∼G−1(σ2
e ; c e
0,C e
0 ),
ωi ∼G
 ν
2, ν
2

,
(9)
for individuals i = 1, . . ., n. Here, we introduce the notation G(a, b) for the Gamma
distribution with shape a > 0 and rate b > 0, which means that the expected value is ab−1.
The inverse Gamma distribution is denoted by G−1(a, b) with the expected value b(a −1)−1.
Hence, we have the hyperparameters {a0, A0, c e
0,C e
0, ν} for the user to choose.
We also need to choose a number of priors for the mixture model (2) describing the distri-
bution of the random eﬀects. Here, we make use of the priors
βk ∼N(βk; b0, B0),
Q−1
k ∼W(Q−1
k ; cQ
0 ,C Q
0 ),
(10)
where W(n,V ) denotes the Wishart distribution with n > 0 degrees of freedom and scale
matrix V > 0, respectively. Hence, we have {b0, B0, cQ
0 ,C Q
0 } as additional hyperparameters
for the user to choose.
Furthermore, we assign a prior for the concentration parameter in the dpm given by
η0 ∼G(cη
0 ,C η
0 ),
(11)
for some hyperparameters cη
0 and C η
0 .
Gibbs sampling
To sample from the posterior, we make use of blocked Gibbs sampling algorithms. For the
dpm model, we make use of the algorithm proposed by Gelman et al. (2013, p. 552), which
is a truncated approximation using at most R clusters. The stick-breaking procedure is used
to sample from the dpm in this formulation.
For the fm, we make use of the Gibbs sampler proposed by Frühwirth-Schnatter (2006,
p. 368), which samples from the mixture using the conjugacy between the Drichlet prior
distribution and the multinomial data. The remaining steps of the Gibbs sampler are iden-
tical and the full procedure performed during one iteration of the sampler is presented in
Algorithm 1. Note that the diﬀerences between the two alternatives for modelling appears
in Steps 1(i) and 5.
For the fm, we sample the mixture weights from the posterior given by the conjugacy
property as previously discussed. This results in the sampling scheme
η′
1:R ∼D(η0/R + n1, η0/R + n2, . . ., η0/R + nR),
(12)
where nr denotes the number of individuals in component r. The details are discussed
by Gelman et al. (2013, p. 534). For the dpm model, we make use of the stick-breaking
construction in (7) to generate the weights η′
1:R for the truncated model, i.e.,
η′
r = Vr
Y
l<r
(1 −Vl),
Vr ∼B *.
,
1 + nr, η0 + N −
rX
j=1
nj+/
-
,
(13)

296
Paper G
Bayesian inference for mixed effects models with heterogeneity
Algorithm 1 Gibbs sampling for mixture models
Inputs: {{yit, xit, zit }N
i=1}T
t=1 (data), θ0 (hyperparameters).
Outputs: θ′ (approximate sample from the parameter posterior).
During one iteration of the Gibbs sampler:
1: Update parameters given the cluster allocation.
(i: fm) Sample η′
r |S1:N using (12) for r = 1, 2, . . ., R.
(i: dpm) Sample η′
r |S1:N using (13) for r = 1, 2, . . ., R.
(ii) Sample α⋆,′|y, x, z,Q1:R,ω1:N, S1:N using (14) with α⋆= {α, β1:R}.
(iii) Sample Q ′
r | βs
1:N, β′
1:R, S1:N using (15) for r = 1, 2, . . ., R.
(iv) Sample σ2,′
e |y, x, z, α′, βs
1:N,ω1:N using (16).
2: Sample allocations S ′
i |y, x, z, α′, βs,′
i , β′
1:R,Q ′
1:R,ω1:N using (17) for i = 1, 2, . . ., N .
3: Sample the random eﬀects βs,′
i |yi, xi, zi, α′, β′
S′
i,Q ′
S′
i,ωi using (18) for i = 1, 2, . . ., N .
4: Sample the variance heterogeneity ω′
i|y, x, z, α′, βs,′
i , σ2,′
e
using (19) for i = 1, 2, . . ., N .
5: [dpm] Sample the concentration parameter η′
0 using (20).
for r = 1, . . ., R and starting with a stick of unit length. The remaining parameters are
sampled from the conditional posteriors derived in the subsequent section.
Conditional posteriors
In this section, we outline the details for sampling from each of the conditional posteriors
in Algorithm 1. In Step 1(ii), we are required to sample from the conditional of α⋆≜
{α, β1, . . ., βR}, which are the ﬁxed eﬀects and the mean of each component in the mixture
(2). This essentially requires us to solve a regression problem where the regressors are given
by zi = (xi, ziDi1, . . ., ziDiK ) with Dik = 1 if Si = k and zero otherwise. Hence, we
rewrite the regression model to obtain
yi = ziα⋆+ Hei,
where Hei ∼N(0,Vi) with Vi = ziQSi(zi)⊤+ σ2
e/ωiIT . We can therefore compute the
posterior using a standard Bayesian linear regression with known covariance and Gaussian
prior for the regression coeﬃcients. The conditional posterior is given by
α⋆|y1:N,Q1:R, σ2
e,ω1:N, S1:N ∼Nd+K p(α⋆; a⋆
N, A⋆
N ),
(14)
where the mean and the covariance are given by
 A⋆
N
−1 =
N
X
i=1
(z⋆
i )⊤V −1
i
z⋆
i + (A⋆
0)−1,
a⋆
N = A⋆
N *
,
N
X
i=1
(z⋆
i )⊤V −1
i
yi + (A⋆
0)−1a⋆
0 +
-
.
In Step 1(iii), we sample the covariance of the mixture components. The posterior is given
by the conjugacy of the Wishart prior for the covariance matrix and a Gaussian likelihood.

3
Sampling from the posterior
297
We can compute the conditional posterior for r = 1, . . ., R by
Q−1
r | β1:R, βs
1:N, S1:N ∼W

Q−1
r ; cQ
r ,C Q
r

,
(15)
where the suﬃcient statistics (assuming independence between B0 and C Q
r ) are given by
cQ
r = cQ
0 + nr
2 ,
C Q
r = C Q
0 + 1
2
X
{i:Si=r }
(βs
i −βr)(βs
i −βr)⊤,
where again nr denotes the number of individuals in component r, i.e.,
nr =
N
X
i=1
I(Si = r).
In Step 1(iv), we sample the variance of the noise σ2
e , which we assume to have an inverse-
Gamma prior distribution. As the likelihood is Gaussian, we obtain the conjugate posterior
given by
σ2
e |y1:N, α, βs
1:N,ω1:N ∼G−1 
σ2
e ; c e
k,C e
k

,
(16)
where the suﬃcient statistics are given by
c e
k = c e
0 + NT
2 ,
C e
k = C e
0 + 1
2
N
X
i=1
ωi
 yi −αxi −βs
i zi
2.
Note that the data in this update is given by the scaled residuals for each of the individuals.
In Step 2, we update the allocation of each individual to a component by sampling Si for
i = 1, . . ., N . The latent variable Si is sampled from a multinomial distribution, where the
probabilities reﬂect the likelihood that individual i belongs to a certain component. That
is, we sample
Si |α, β1:R,Q1:R,ωi, σ2
e, yi ∼M(1, p1:R),
(17)
where the probabilities are given by
pr =
ηrN yi; αxi + βkzi,Wik

PR
l=1 ηlN yi; αxi + βl zi,Wil
,
Wil = ziQl(zi)⊤+ σ2
e
ωi
IT .
Note that the probabilities follow from the mixture model (2) directly. The intuition is
that we are more likely to select the component that best explains the data in terms of its
contribution to the likelihood.
In Step 3, we sample the random eﬀects for each individual from the component to which
the individual is assigned. The prior for each individual is given by p(βi) = N(βS′
i,QS′
i)
and the likelihood is Gaussian and given by
yi −αxi = zi βs
i + σe
√wi
ei,
where ei is a standard Gaussian random variable. Note that this again is a Bayesian linear
regression with a Gaussian prior for the regression coeﬃcient βs
i , where the observations

298
Paper G
Bayesian inference for mixed effects models with heterogeneity
are given by yi −αxi and the regressors are xi. In this case, the variance is known and the
conditional posterior for i = 1, . . ., N is
βs
i |yi, α, βSi,QSi,ωi ∼N(βs
i ; b s
i , Bs
i ),
(18)
where the suﬃcient statistics are given by
Bs
i =
 
Q−1
Si + (zi)⊤zi
ωi
σ2e
!−1
,
b s
i = Bs
i
 
Q−1
Si βSi + (zi)⊤(yi −αxi) ωi
σ2e
!
,
In Step 4, we sample the individual scaling of the noise of the observations. The derivation is
similar to for Step 1(iv), but here we have a Gamma distributed prior for ωi and a Gaussian
likelihood. The conditional posterior is given by
ωi |y1:N, α, βs
1:N, σ2
e ∼G

ωi; cω
k ,C ω
k

,
(19)
where the suﬃcient statistics are given by
cω
k = ν
2 + T
2 ,
C ω
k = ν
2 +
1
2σ2e
 yi −αxi −βs
i zi
2.
In Step 5, we sample the concentration parameter η0 for the dpm model using the Gamma
prior in (11). In this case, the conditional posterior (Gelman et al., 2013, p. 553) is given by
η′
0 ∼G *
,
cη
0 + N −1,C η
0 −
R−1
X
r=1
log(1 −Vr)+
-
,
(20)
where Vr is the fraction broken oﬀduring the stick-breaking in Step 1(i) of the procedure.
Numerical illustrations
In this section we present three numerical experiments to compare the diﬀerence of mod-
elling heterogeneity in the random eﬀects using the fm and dpm model. The aim is to
compare the posterior estimates and the mixing in the Markov chain created by the Gibbs
sampler. The latter is important as it determines the ineﬃciency of the sampling and the
asymptotic variance of the posterior estimates.
Mixture of Gaussians
We begin with a simple mixture of Gaussians model (Escobar and West, 1995) to validate our
implementation and present the setup that we use for the comparisons in this section. In this
model, we simplify the mixed eﬀects model (1) such that yi = βs
i , where the heterogeneity
of the random eﬀects is modelled by (2). We generate T = 1 observations for N = 100
individuals using R = 3 components and η1:3 = {0.3, 0.5, 0.2}, β1:3 = {−1, 0, 2} and
Q1:3 = {0.22, 1, 0.052}. We carry out the inference using Algorithm 1 with the settings
presented in Appendix A.
The results are presented in Figure 1. In the upper part, we present the estimates of the
posterior of βs
i using three diﬀerent models: (i) a fm with the true number of components

4
Numerical illustrations
299
-3
-2
-1
0
1
2
3
4
0.0
0.2
0.4
0.6
βi
s
density
no. occupied components
density
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
FMs
no. occupied components
density
0
5
10
15
20
0.0
0.1
0.2
0.3
0.4
DPM model
Figure 1. Upper: the posterior estimate from a ﬁnite mixture with 3 components (green), the
fm (orange) and dpm model (purple). The rug indicate the observed data. Lower: the number
of active clusters, where vertical dotted lines indicate the true number of clusters.

300
Paper G
Bayesian inference for mixed effects models with heterogeneity
R = 3, (ii) a fm with R = 20 components and (iii) a dpm model. We note that the three
models give almost the same estimate of the posterior, which indicates that the sparsity
eﬀect of the fm works satisfactory and empties the extra components. This can be seen
in the lower part of the same ﬁgure as less than 20 components are active at any time
in the fm. Furthermore, the dpm model use on average more components than the fm.
However, both models often make use of more components than in the model from which
we generated the data.
Mixed effects model with synthetic data
We now consider the complete mixed eﬀects model (1) with heterogenity in the individual
random eﬀects (2). We simulate 40 independent data sets using T = 100 observations in
each while varying the number of individuals N in {10, 20, 50, 100, 200, 500}. We make use
of a mixture for the random eﬀects with R = 3 components and parameters
η1:3 = {0.4, 0.3, 0.3},
β1:3 = {2, 3, −2},
Q1:3 = {1, 0.2, 0.2}.
Moreover, we make use of σ2
e = 1, α = 2 and ω1:N ∼|N(0, 1)| for the remaining parame-
ters of the model. We carry out the inference using Algorithm 1 with the settings presented
in Appendix A.
We proceed by comparing the diﬀerence in the posterior estimates of the random eﬀects.
This is done by computing the mean squared error ( mse) between the kernel density
estimates (kdes) of the sought posterior using the fm and dpm model. Hence for each
data set, we compute the kdes of the posterior estimates and compute the squared distance
between them. In Table 1, we present the results which indicate that the mse (after an
initial increase) tends to decrease when N grows. We therefore conclude that the posterior
estimates tend to become similar as the amount of data increases. Furthermore, we present
some graphical comparisons in Figure 2 for four Monte Carlo runs. We see that the posterior
estimates are similar most of the time, which is promising and validates the conclusion from
the mse comparison.
We compare the mixing in the two models using the integrated autocorrelation time ( iact)
also known as the ineﬃciency factor for the Gibbs sampler applied to the two models. The
iact is estimated by
E
IACT ϕ(θKb:K ) = 1 + 2
L
X
l=1
Dρl
 ϕ(θKb:K ),
(21)
where Kb denotes the number of iterations in the burn-in and K denotes the number of
iterations of the Gibbs sampler. Here Dρl(ϕ(θKb:K )) denotes the empirical autocorrelation
at lag l of some test function ϕ that depends on θ. Here, we make use of the log-likelihood
and the number of occupied components to compute the iact. The log-likelihood for the
mixed eﬀects model (1) is given by
ψ1(θ) =
N
X
i=1
TX
t=1
log N
 
yit; αxit −βs
i zit, σ2
e
ωi
!
.

4
Numerical illustrations
301
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
density
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
density
-4
-2
0
2
4
0.0
0.5
1.0
1.5
density
-4
-2
0
2
4
0.0
0.5
1.0
1.5
2.0
density
-4
-2
0
2
4
0.0
1.0
2.0
3.0
density
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
β
density
-4
-2
0
2
4
0.0
0.5
1.0
1.5
2.0
2.5
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
-4
-2
0
2
4
0.0
0.4
0.8
1.2
-4
-2
0
2
4
0.0
0.5
1.0
1.5
2.0
2.5
-4
-2
0
2
4
0.0
0.5
1.0
1.5
2.0
2.5
-4
-2
0
2
4
0
1
2
3
4
β
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
1.2
-4
-2
0
2
4
0.0
0.5
1.0
1.5
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
-4
-2
0
2
4
0.0
0.5
1.0
1.5
-4
-2
0
2
4
0
2
4
6
8
10
-4
-2
0
2
4
0
1
2
3
4
β
-4
-2
0
2
4
0.0
0.1
0.2
0.3
0.4
0.5
-4
-2
0
2
4
0.0
0.5
1.0
1.5
-4
-2
0
2
4
0.0
0.5
1.0
1.5
2.0
2.5
-4
-2
0
2
4
0.0
0.5
1.0
1.5
2.0
2.5
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
-4
-2
0
2
4
0
1
2
3
4
β
Figure 2. Estimates of p(βs
i |y) from a fm (orange) and dpm model (purple) for n
=
{10, 20, 50, 100, 200, 500} (rows) and 4 independent Monte Carlo runs (columns).

302
Paper G
Bayesian inference for mixed effects models with heterogeneity
N 10
N = 20
N = 50
N = 100
N = 200
N = 500
Log-MSE
-6.1
-5.5
-5.1
-3.7
-4.1
-4.9
E
IACT(ψ1)
fm
16 (27)
22 (48)
36 (75)
19 (61)
2 (5)
1 (1)
dpm
11 (25)
24 (34)
29 (45)
10 (46)
2 (4)
1 (1)
E
IACT(ψ2)
fm
4 (1)
7 (3)
8 (4)
8 (3)
7 (1)
11 (5)
dpm
12 (10)
12 (10)
13 (9)
11 (10)
8 (5)
8 (4)
Table 1. The logarithm of the mse of the estimates and the iact of the log-likelihood and the
number of occupied components obtained in the fm and in a dpm model. The iact values are
the median with the interquartile range in parenthesise from 40 Monte Carlo simulations.
The number of occupied clusters is given by
ψ2(θ) =
R
X
k=1
I(nr > 0),
where nr denotes the number of individuals in component r. A small value of the iact
for both quantities indicates that we obtain many uncorrelated samples from the sought
posterior. This implies that the chain is mixing well and that the asymptotic variance of
the parameter estimates is rather small. We choose L = max{3, ⌊0.5(M −Mb)⌋} and make
use of the IAT command in the R-package LaplacesDemon (Hall, 2012) to compute the
estimate of the iact.
Table 1 also presents the median iact with the interquartile range (the length between the
ﬁrst and third quartiles) from 40 Monte Carlo runs over diﬀerent data sets. We compare
the iact for the fm and the dpm model for both the log-likelihood and the number of
occupied components. We note that there are no signiﬁcant diﬀerences between the two
models. Also, the iact in the log-likelihood seems to decrease with increasing N and the
opposite holds for the mixing in the number of occupied components.
Mixed effects model with sleep deprivation data
Finally, we consider the data set presented by Belenky et al. (2003) of the reaction times in a
sleep deprivation trial of N = 18 individuals. We present the data in Figure 3 from the test
during T = 10 consecutive days during which the subjects are only allowed to sleep three
hours every night. Moreover, we present the linear regression for each individual indicated
by the green lines. It seems that there are two diﬀerent types of individuals in the data. In
the ﬁrst sub-group, the reaction time signiﬁcantly increases for each day of sleep deprivation,
e.g., individuals 337, 308 and 350. In the second sub-group, the lack of sleep does not seem
to have a large impact on the reaction times, e.g., individual 351, 335 and 309. Hence, we
can assume that the distribution of the random eﬀects is possibly multi-modal, where the
modes represent these sub-groups.

4
Numerical illustrations
303
0
2
4
6
8
10
100
300
500
reaction time
308
0
2
4
6
8
10
100
300
500
reaction time
309
0
2
4
6
8
10
100
300
500
reaction time
310
0
2
4
6
8
10
100
300
500
reaction time
330
0
2
4
6
8
10
100
300
500
reaction time
331
0
2
4
6
8
10
100
300
500
day
reaction time
332
0
2
4
6
8
10
100
300
500
333
0
2
4
6
8
10
100
300
500
334
0
2
4
6
8
10
100
300
500
335
0
2
4
6
8
10
100
300
500
337
0
2
4
6
8
10
100
300
500
349
0
2
4
6
8
10
100
300
500
day
350
0
2
4
6
8
10
100
300
500
351
0
2
4
6
8
10
100
300
500
352
0
2
4
6
8
10
100
300
500
369
0
2
4
6
8
10
100
300
500
370
0
2
4
6
8
10
100
300
500
371
0
2
4
6
8
10
100
300
500
day
372
Figure 3. The reaction time in milliseconds for N = 18 individuals during T = 10 consecutive
days with only three hours of sleep per night. The solid lines indicate the best linear ﬁt for each
individual and the dots indicate the data points.

304
Paper G
Bayesian inference for mixed effects models with heterogeneity
We make use of the model discussed by Bates et al. (2015) to try to capture this eﬀect. Let
{{yit }N
i=1}T
t=1 denote the reaction time in millisecond for individual i on day t. We assume
that the observations can be modelled by a mixed eﬀects model given by
yit |α, βs
i, σ2
e ∼N
 
yit; (α0 + βs
i0) + (α1 + βs
i1)(t −1), σ2
e
ωi
!
.
(22)
Furthermore, we assume that the individual random eﬀects can be modelled using a mixture
of Gaussians (2). The model parameters are θ = {α0, α1, β1:R,0, β1:R,1, σ2
e,ω1:N }. The
interpretation of this model is that the mean reaction time and change in reaction time for
every day of sleep deprivation are given by α0 and α1. The individual variations of these
two quantities are captured by βs
i0 and βs
i1, respectively.
Figure 4 presents the estimates of the posterior of the ﬁxed and random eﬀects using the
fm (orange) and the dpm model (purple). From the upper part, we see the estimates of the
mean reaction time at the ﬁrst day when the participants are well-rested together with the
average increase for each day. The estimates of the posterior means {Dα0, Dα1} are {237, 1.15}
and {237, 1.07} for the fm and the dpm model, respectively. This corresponds to that the
mean reaction time the ﬁrst day of the test is 237 milliseconds, which then increases by 1.15
or 1.07 milliseconds for every day of sleep deprivation.
From the lower part, we note that posterior estimate for βs
i0 is uni-modal with some skew-
ness to the right. This indicates that most individuals have the same or an increased reaction
time on the ﬁrst day compared with the corresponding ﬁxed eﬀect α0. However, from the
posterior regarding βs
i1, we note that there seems to be three diﬀerent sub-groups with:
a small decrease, small increase and large increase in reaction times for each day of sleep
deprivation. This validates some of our initial ﬁndings that some individuals have a small
(or even negative) change in reaction time when sleep deprived. Finally, the estimates of the
random eﬀects posterior means {L
βs i0, L
βs i1} are {9.50, 8.94} and {9.50, 9.09} for the fm
and the dpm model, respectively.
Conclusions
We have compared two diﬀerent models for describing heterogeneity of the random eﬀects
in a mixed eﬀects model. The numerical illustrations show that the two approaches give
similar estimates of the posteriors using both synthetic and real-world data. This provides
us with some promising indications that the results from Rousseau and Mengersen (2011)
holds for this type of models. However, more theoretical work is required to generalise the
convergence results to this type of model and priors.
Another important future work is to apply this approach for more realistic real-world mod-
els as discussed by e.g., Burda and Harding (2013). It would also be useful to conduct more
extensive simulations to compare the mixing in the Markov chain to see if the approximate
fm can help mitigating the problems with poor mixing as discussed by Hastie et al. (2015).
The source code and data for the numerical illustrations are available from https://
github.com/compops/panel-dpm2016/.

5
Conclusions
305
220
230
240
250
260
0.00
0.05
0.10
0.15
0.20
α0
density
-30
-20
-10
0
10
20
0.00
0.05
0.10
0.15
0.20
α1
density
-20
-10
0
10
20
30
0.00
0.02
0.04
0.06
0.08
0.10
βi0
s
density
-10
0
10
20
30
40
0.00
0.02
0.04
0.06
0.08
0.10
βi1
s
density
Figure 4. Posterior estimates of α0 (upper left), α (upper right), βs
0i (lower left) and βs
1i (lower
right) for the fm (orange) and the dpm model (purple).

306
Paper G
Bayesian inference for mixed effects models with heterogeneity
Acknowledgements
The simulations were performed on resources provided by the Swedish National Infrastruc-
ture for Computing ( snic) at Linköping University, Sweden.
Appendix
Implementation details
In all illustrations, we use K = 10, 000 iterations in the Gibbs sampler and discard the ﬁrst
Kb = 2, 500 iterations as burn-in.
Mixture of Gaussians
We use R = 20 components in the fm with η0,r = 1/R for r = 1, . . ., R as the concentration
parameter to promote sparsity as suggested by Ishwaran and Zarepour (2002). Furthermore,
we use η = 1 for the ﬁnite mixture with the true number of components R = 3 so that
all components are occupied. For the prior of η0 in the dpm model, we use cη
o = 1 and
C η
o = 0.5 as the hyperparameters. The remaining hyperparameters for the priors are
selected as b0 = 0, B0 = 0.22, cQ
0 = 1 and cQ
0 = 1.
Mixed effects model with synthetic data
We use R = 20 components in the fm with η0,r = 1/R for r = 1, . . ., R to promote
sparsity as in the previous example. For the prior of η0 in the dpm model, we use the
hyperparameters cη
0 = 0.01 and C η
0 = 0.01. We make use of the mean estimate from the
linear regressions of each individual to select a⋆
0 . Hence, the ﬁrst element corresponds to
the mean intercept and the remaining R elements correspond to the mean slope from the
N linear regression estimates. We make use of A⋆
0 = Id+K p as the prior covariance of α⋆.
Furthermore, we select cQ
0 = 10, C Q
0 = 1, c e
0 = 0, C e
0 = 0 and ν = 5.
Mixed effects model with sleep deprivation data
We use R = 20 components in the fm with η0,r = 1/R for r = 1, . . ., R to promote sparsity
as in the previous example. For the prior of η0 in the dpm model, we use the hyperparame-
ters cη
0 = 0.01 and C η
0 = 0.01. We make use of the same approach as in the previous illustra-
tion to choose a⋆
0 = {251.4, 10.5, 0, . . ., 0} and select A⋆
0 = diag{0.01, 0.005, 0.02, . . ., 0.02}.
Furthermore, we select cQ
0 = 10, C Q
0 = 0.05I2, c e
0 = 1, C e
0 = 2 and ν = 5.

Bibliography
307
Bibliography
G. M. Allenby, N. Arora, and J. L. Ginter. On the heterogeneity of demand. Journal of
Marketing Research, 35(3):384–389, 1998.
A. Ansari, S. Essegaier, and R. Kohli. Internet recommendation systems. Journal of Market-
ing research, 37(3):363–375, 2000.
C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparamet-
ric problems. The Annals of Statistics, 2(6):1152–1174, 1974.
B. H. Baltagi. Econometric analysis of panel data. John Wiley & Sons, 2008.
D. Bates, M. Mächler, B. Bolker, and S. Walker. Fitting linear mixed-eﬀects models using
lme4. Journal of Statistical Software, 67(1):1–48, 2015.
G. Belenky, N. J. Wesensten, D. R. Thorne, M. L. Thomas, H. C. Sing, D. P. Redmond,
M. B. Russo, and T. J. Balkin. Patterns of performance degradation and restoration
during sleep restriction and subsequent recovery: A sleep dose-response study. Journal of
sleep research, 12(1):1–12, 2003.
D. Blackwell and J. B. MacQueen. Ferguson distributions via Pólya urn schemes. The
Annals of Statistics, 1(2):353–355, 1973.
A. Bouchard-Côté, A. Doucet, and A. Roth.
Particle Gibbs split-merge sampling for
Bayesian inference in mixture models. Pre-print, 2015. arXiv:1508.02663v1.
M. Burda and M. Harding. Panel probit with ﬂexible correlated eﬀects: quantifying tech-
nology spillovers in the presence of latent heterogeneity. Journal of Applied Econometrics,
28(6):956–981, 2013.
F. Canova. Testing for convergence clubs in income per capita: a predictive density approach.
International Economic Review, 45(1):49–77, 2004.
M. K. Condliﬀ, D. D. Lewis, D. Madigan, and C. Posse. Bayesian mixed-eﬀects models
for recommender systems. In Proceedings of ACM SIGIR’99 Workshop on Recommender
Systems, Berkeley, USA, August 1999.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.
M. D. Escobar and M. West. Bayesian density estimation and inference using mixtures.
Journal of the American Statistical Association, 90(430):577–588, 1995.
P. Fearnhead. Particle ﬁlters for mixture models with an unknown number of components.
Statistics and Computing, 14(1):11–21, 2004.
T. S. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of
Statistics, 1(2):209–230, 1973.
T. S. Ferguson.
Prior distributions on spaces of probability measures.
The Annals of
Statistics, 2(4):615–629, 1974.
S. Frühwirth-Schnatter. Finite mixture and Markov switching models. Springer Verlag, 2006.

308
Paper G
Bayesian inference for mixed effects models with heterogeneity
A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. Bayesian
data analysis. Chapman & Hall/CRC, 3 edition, 2013.
B. Hall.
LaplacesDemon: software for Bayesian inference, 2012.
URL http://cran.
r-project.org/web/packages/LaplacesDemon/index.html.
R package version
12.05.07.
D. I. Hastie, S. Liverani, and S. Richardson. Sampling from Dirichlet process mixture mod-
els with unknown concentration parameter: mixing issues in large data implementations.
Statistics and Computing, 25(5):1023–1037, 2015.
H. Ishwaran and M. Zarepour. Dirichlet prior sieves in ﬁnite normal mixtures. Statistica
Sinica, 12(3):941–963, 2002.
S. Jain and R. M. Neal.
A split-merge Markov chain Monte Carlo procedure for the
Dirichlet process mixture model. Journal of Computational and Graphical Statistics, 13(1):
158–182, 2004.
H. F. Lopes, P. Müller, and G. L. Rosner. Bayesian meta-analysis for longitudinal data
models using multivariate mixture priors. Biometrics, 59(1):66–75, 2003.
R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9(2):249–265, 2000.
C. P. Robert and G. Casella. Monte Carlo statistical methods. Springer Verlag, 2 edition,
2004.
J. Rousseau and K. Mengersen. Asymptotic behaviour of the posterior distribution in
overﬁtted mixture models.
Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 73(5):689–710, 2011.
J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650,
1994.
Y. Ulker, B. Gunsel, and T. A. Cemgil. Sequential Monte Carlo samplers for Dirichlet pro-
cess mixtures. In Proceedings of the 13th International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), pages 876–883, Sardinia, Italy, May 2010.
F. Verbeke and E. Lesaﬀre. A linear mixed-eﬀects model with heterogeneity in the random-
eﬀects population. Journal of the American Statistical Association, 91(433):217–221, 1996.
G. Verbeke and G. Molenberghs. Linear mixed models for longitudinal data. Springer Verlag,
2009.
S. G. Walker.
Sampling the Dirichlet mixture model with slices.
Communications in
Statistics - Simulation and Computation, 36(1):45–54, 2007.

Paper H
Robust Input design
for non-linear dynamical models
Authors:
P. E. Valenzuela, J. Dahlin, C. R. Rojas and T. B. Schön
Edited version of the paper:
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. On robust input design
for nonlinear dynamical models. Automatica, 2016a. (provisionally accepted).
Parts of the theory presented in this paper have also been presented in:
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. A graph/particle-based
method for experiment design in nonlinear systems. In Proceedings of the 19th
IFAC World Congress, Cape Town, South Africa, August 2014.


Robust Input design
for non-linear dynamical models
P. E. Valenzuela⋆, J. Dahlin†, C. R. Rojas⋆and T. B. Schön‡
⋆Dept. of Automatic Control,
KTH, Royal Institute of Technology,
SE-100 44 Stockholm, Sweden.
{pva,crro}@kth.se
†Dept. of Electrical Engineering,
Linköping University,
SE–581 83 Linköping, Sweden.
johan.dahlin@liu.se
‡Dept. of Information Technology,
Uppsala University,
SE-751 05 Uppsala, Sweden.
thomas.schon@it.uu.se
Abstract
We present a method for robust input design for non-linear state-space models. The method optimizes
a scalar cost function of the Fisher information matrix over a set of marginal distributions of stationary
processes. By using elements from graph theory we characterize such a set. Since the true system is
unknown, the resulting optimization problem considers a measure of the uncertainty over the set of
model parameters. In addition, the required estimates of the information matrix are computed using
particle methods, and the resulting problem is convex in the decision variables. Numerical examples
illustrate the proposed technique by identifying models using the expectation maximization.
Keywords
System identiﬁcation, input design, particle ﬁlter, non-linear systems.
Financial support from
The projects Probabilistic modeling of dynamical systems (Contract number: 621-2013-5524) and cadics,
a Linnaeus Center, both funded by the Swedish Research Council.
311

312
Paper H
Robust Input design for non-linear dynamical models
Introduction
Input design is concerned with generating an input signal that maximizes the information
retrieved from an experiment, quantiﬁed in terms of a cost function related to the intended
model application. Some of the initial contributions are discussed in Cox (1958) and Good-
win and Payne (1977). Since then, many contributions to the subject have been presented;
see e.g. Fedorov (1972); Whittle (1973); Hildebrand and Gevers (2003); Gevers (2005) and
the references therein.
In the case of dynamical systems, the existing results on input design are mostly focused on
linear models. The assumption of a linear model structure can reduce the complexity of the
problem, leading to formulations that are convex in the decision variables Ljung (1999). In
this case, the convexity of the problem is achieved by designing the power spectrum of the
input signal.
Several approaches to input design for linear models have been proposed in the literature
involving, e.g., linear matrix inequalities (lmis; Jansson and Hjalmarsson, 2005; Lindqvist
and Hjalmarsson, 2000), Markov chains (Brighenti et al., 2009), and time domain techniques
(Suzuki and Sugie, 2007). With the exception of the methods by Jansson and Hjalmarsson
(2005) and Lindqvist and Hjalmarsson (2000) that rely on convexiﬁcation of the problem,
the previous formulations are non-convex, which illustrates the diﬃculty of solving the
input design problem.
In recent years, there has been an interest to extend the input design methods to non-
linear model structures. The main issue here is that the frequency domain methods cannot
be applied to solve the input design problem, which restricts the applicability of convex
formulations (Jansson and Hjalmarsson, 2005; Lindqvist and Hjalmarsson, 2000). The input
design problem for non-linear models using the knowledge of linear systems is discussed
in Hjalmarsson and Mårtensson (2007), where the main challenges of designing inputs
for non-linear systems are presented by employing a non-linear fir model structure. The
non-linear fir model structure is also considered in Larsson et al. (2010), where the input
design problem is solved over a set of marginal distributions of stationary processes.
An extension of the input design problem to structured non-linear models is presented
in Vincent et al. (2009) and Vincent et al. (2010). The model structures in Vincent et al.
(2009) and Vincent et al. (2010) assume the interconnection of linear models and static
non-linearities, and a realization of the optimal excitation is obtained by running a Markov
chain with a prescribed stationary distribution. The class of non-linear model structures
covered in input design is generalized in Forgione et al. (2014), where the input signal is
optimized over an alphabet with ﬁnite cardinality.
A multilevel excitation is also discussed in De Cock et al. (2013) for identiﬁcation of Wiener
models, where the input is optimized using the D-criterion. The restriction to a ﬁnite
alphabet is relaxed in Gopaluni et al. (2011), where the design of an input sequence for the
identiﬁcation of non-linear state-space models is discussed. In Gopaluni et al. (2011), the
input is constrained to the class of stationary arx processes.
A methodology to design inputs for identiﬁcation of non-linear output-error models is devel-
oped in Valenzuela et al. (2013) and Valenzuela et al. (2015), which is extended in Valenzuela

1
Introduction
313
et al. (2014a) to non-linear state space models. The technique proposed in Valenzuela et al.
(2013), Valenzuela et al. (2015) and Valenzuela et al. (2014a) relies on graph-theoretical tools
to describe a set of marginal distributions of stationary processes with ﬁnite alphabet.
The existing results on input design allow to optimize input signals when the system con-
tains non-linear functions, but the restrictions on the system dynamics and/or the input
structure are the main limitations of most of the previous contributions. Moreover, with
the exception of multilevel excitation (Forgione et al., 2014; Larsson et al., 2010), and sta-
tionary processes (Brighenti et al., 2009; Valenzuela et al., 2013, 2014a, 2015), most part
of the proposed methods cannot handle amplitude limitations on the input signal, which
could arise due to physical and/or safety reasons.
The input design methods previously mentioned assume that a prior estimate of the model
parameters is available for optimization. The requirement of such knowledge is a common
issue in input design and diﬀerent solutions to this diﬃculty have been proposed in the
literature (Welsh and Rojas, 2009). A ﬁrst approach is to consider an adaptive scheme,
where the input sequence is designed as more information is collected from the system
(Rojas et al., 2011; Gerencsér et al., 2009). The second option is to consider a robust input
design (Rojas et al., 2007, 2012), where the input signal is designed by optimizing a measure
of the uncertainty over the parameter space. In this article we address the uncertainty in
the model parameters following the second option.
The main contribution of this article is to present a robust input design method for the
identiﬁcation of non-linear state-space models ( ssm) with input constraints, which extends
the model structure considered in Valenzuela et al. (2013) and Valenzuela et al. (2015), and
the nominal input design presented in Valenzuela et al. (2014a). The optimal input signal is
considered to be a realization of a stationary process, which optimizes a scalar cost function
of the Fisher information matrix.
To pose a tractable problem, we restrict the optimization to a set of marginal distributions
of stationary processes with ﬁnite alphabet. This set is characterized by a ﬁnite set of linear
inequalities, and hence it can be described by a convex combination of its vertices. The
vertices are cumulative distribution functions that can be found by using de Bruijn graphs,
as discussed in Valenzuela et al. (2013) and Valenzuela et al. (2014a).
Since the vertices of the set are known, we can draw an input realization and compute an
estimate of the Fisher information matrix for each vertex using particle methods (Doucet
and Johansen, 2011; Del Moral et al., 2006). The estimates of the information matrices are
computed using the method introduced in Segal and Weinstein (1989), which only needs
one realization of the input-output data, and thus reducing the computational eﬀort when
estimating the Fisher information matrix compared to Valenzuela et al. (2014a). Moreover,
the resulting optimization problem is convex in the decision variables.
To make the input design robust against model uncertainty, the optimization problem
considers a measure of the uncertainty over the space of model parameters, which relaxes the
requirements on the knowledge of the system assumed in Valenzuela et al. (2013), Valenzuela
et al. (2015) and Valenzuela et al. (2014a). The method is illustrated through numerical
examples, where the designed input is employed to identify an ssm using the expectation-
maximization (em; Schön et al., 2011; McLachlan and Krishnan, 2008) algorithm.

314
Paper H
Robust Input design for non-linear dynamical models
The rest of this article is organized as follows. Section 2 states the problem and the main
challenges when designing inputs for identiﬁcation of non-linear ssm. Section 3 describes
the graph theoretical approach to input design. Section 4 discusses the estimation of the
Fisher information matrix using particle methods. A summary of the proposed robust input
design method is presented in Section 5. The input signal generation once the optimization
is solved is presented in Section 6. To illustrate the correctness and utility of the method,
two numerical examples are discussed in Section 7. Finally, Section 8 concludes this work
and presents future research directions.
Notation: Throughout this article, N denotes the set of natural numbers, Rp denotes the
set of p-dimensional vectors with real entries, Rp×r is the set of p × r matrices with real
entries, and R+ the set of positive real numbers. P, E, and Var{ · } stand for a probability
measure, the expected value, and the variance, respectively. Sometimes a subscript is added
to P and E to clarify the stochastic process considered by these operators. Finally, for a set
A with ﬁnite number of elements, #A denotes its cardinality.
Problem formulation
Consider a non-linear ssm that can be described for all t ≥1 by
xt | xt−1 ∼fθ(xt | xt−1, ut−1),
(1a)
yt | xt ∼gθ(yt | xt, ut),
(1b)
x0 ∼µθ(x0),
(1c)
where fθ, gθ, and µθ denote probability density functions (pdf) parameterized by θ ∈Θ ⊂
Rnθ (where Θ is an open set). Here, ut ∈Rnu denotes the input signal, xt ∈Rnx are the
(unobserved/latent) internal states, and yt ∈Rny are the measured outputs.
The objective in this article is to design an input signal u1:nseq := (u1, . . ., unseq), as a
realization of a stationary process, such that the non-linear ssm (1) can be identiﬁed with
maximum accuracy as deﬁned by a scalar function of the Fisher information matrix (Ljung,
1999). In the sequel, we will assume that there exists at least one parameter θ0 ∈Θ such that
the model (1) exactly describes the pdfs of the system, i.e., there is no undermodelling.
Given u1:nseq, the Fisher information matrix is deﬁned as
I
nseq
F
(θ0) := E
(
S(θ0)S⊤(θ0)| u1:nseq
)
,
(2)
where S(θ0) denotes the score function, i.e.,
S(θ0) := ∇θ `θ(y1:nseq)θ=θ0 .
(3)
Here, `θ(y1:nseq) denotes the log-likelihood function
`θ(y1:nseq) := log pθ(y1:nseq|u1:nseq) .
(4)
We note that the expected value in (2) is with respect to the stochastic processes in (1). Since
we consider u1:nseq to be a realization of a stationary process, here we are interested in the

2
Problem formulation
315
per-sample Fisher information matrix, deﬁned as
IF (θ0) :=
1
nseq
Eu
(
I
nseq
F
(θ0)
)
=
1
nseq
E S(θ0)S⊤(θ0)	 ,
(5)
where the expected value in (5) is over both the stochastic processes in (1), and the stochastic
vector u1:nseq.
We note that (5) depends on the cumulative distribution function (cdf) of u1:nseq, denoted
by Pu(u1:nseq). Therefore, the input design problem is to ﬁnd a cdf P opt
u (u1:nseq) which
maximizes a scalar function of (5). We deﬁne this scalar function as H : Rnθ×nθ × Θ →R,
where H is a matrix concave function (Boyd and Vandenberghe, 2004, p. 108). Diﬀerent
choices of H have been proposed in the literature, see e.g. Rojas et al. (2007); some examples
are H(A, θ) = log det(A), and H(A, θ) = −tr{A−1} for all invertible matrices A ∈Rnθ×nθ.
Since P opt
u (u1:nseq) has to be a marginal cdf of a stationary process, the optimization prob-
lem must be constrained to the set
P :=
(
Pu : Rnseq →R Pu(x) ≥0, ∀x ∈Rnseq;
Pu is monotone non-decreasing ;
lim
xi→∞
i={1, ..., nseq}
x=(x1, ..., xnseq)
Pu(x) = 1;

v ∈R
dPu(v, z) =

v ∈R
dPu(z, v),∀z ∈Rnseq−1
)
.
(6)
The last condition in (6) (with slight abuse of notation) guarantees that Pu ∈P is a marginal
cdf associated with a stationary process (Zaman, 1983). Indeed, in order to have Pu as a
valid marginal cdf of a stationary process, it must satisfy the shift-invariant property,
namely that the marginal cdf for the ﬁrst nseq −1 arguments in Pu must be equal to that
given by the last nseq −1 arguments in Pu, which is translated to the last condition in (6).
Hence, the set P considers cdfs whose marginal distributions are time invariant.
To simplify our problem, we will assume that ut can only adopt a ﬁnite number cseq of
values. We denote this set of values as C. With the previous assumption, we can deﬁne the
following set, which is derived from a proper subset of P:
PC :=
(
pu : Cnseq →R pu(x) ≥0, ∀x ∈Cnseq;
X
x∈Cnseq
pu(x) = 1;
X
v ∈C
pu(v, z) =
X
v ∈C
pu(z, v),∀z ∈Cnseq−1
)
.
(7)

316
Paper H
Robust Input design for non-linear dynamical models
The set introduced in (7) will constrain the probability mass function (pmf) pu to the set
of marginal pmfs associated with stationary processes.
So far we have been concerned with the formulation of the problem in terms of the sta-
tionary process describing u1:nseq. However, we still need to consider a remaining issue:
Equation (5) depends on the parameter θ0 describing the true system (1). Therefore, the
optimal input sequence u1:nseq depends on the parameter we want to estimate, which lim-
its the applicability of the previous formulation in practical situations. To overcome this
issue, we consider the function R : Θ →R that measures the uncertainty over Θ. In the
following, two deﬁnitions of R are considered: R = Eθ{ · } (where we assume that Θ is a
measurable space with known cdf Pθ), and R = minθ ∈Θ{ · }.
To summarize, the problem we are interested in solving can be written as
Problem 1. Design an optimal input signal u1:nseq ∈Cnseq as a realization from popt
u (u1:nseq), where
popt
u
:= arg max
pu ∈PC
R{H(IF (θ), θ)} ,
(8)
with H : Rnθ×nθ × Θ →R a matrix concave function, R : Θ →R, and IF (θ) ∈Rnθ×nθ deﬁned as
in (5).
■
Problem 1 is diﬃcult to solve. The main challenges are:
(A) The set Θ may be uncountably inﬁnite, which implies that the computation of
R{H(IF (θ), θ)} can be intractable.
(B) The set PC may be very large.
(C) The model structure (1) is very general, which implies that a closed form expression
for the Fisher information matrix (5) might not be available, with the exception of
special cases (e.g. when (1) is a linear Gaussian ssm).
To address issue (A), we consider a procedure that depends on R. For R = Eθ{ · }, we solve
a Monte-Carlo approximation of Problem 1 by sampling Ns points from the set Θ according
to the cdf Pθ, and replacing the expected value by its sample mean estimate. In the case
that R = minθ ∈Θ{ · }, we employ the scenario approach (Calaﬁore and Campi, 2005; Welsh
and Rojas, 2009). By sampling Ns points from the set Θ according to a given cdf Ps, we
can rewrite Problem 1 as an optimization problem over a ﬁnite number of points in Θ. In
this case we will obtain a sub-optimal solution to Problem 1 which, however, can be made
close to the optimal solution by increasing Ns; we refer to Calaﬁore and Campi (2005);
Campi and Garatti (2008) for more details.
Issue (B) will be addressed in the next section using a graph theoretical perspective, while
issue (C) will be addressed in Section 4 using particle methods to approximate the Fisher
information matrix.
Describing the set of stationary processes
The diﬃculties associated with issue (B) are:

3
Describing the set of stationary processes
317
(B.i) the pmf pu is of dimension nseq, where nseq can potentially be very large, and
(B.ii) we need to represent the elements in PC.
The points mentioned above make Problem 1 computationally intractable. To overcome
these diﬃculties, we will use the graph theoretical approach introduced in Valenzuela et al.
(2013), which is brieﬂy described in this section.
To solve issue (B.i), we restrict pu to the set of pmfs deﬁned over Cnm, where nm < nseq,
and corresponding to marginal pmfs of stationary processes. This assumption allows to
solve an approximation of Problem 1 in the sense that the diﬀerence between the optimal
cost for the solution considering pu(u1:nseq), and the optimal cost for pu(u1:nm) can be made
arbitrarily small by deﬁning nm suﬃciently close to nseq.
To address issue (B.ii), we notice that all the elements in PC can be represented as a con-
vex combination of its extreme points, since PC is described by a ﬁnite number of linear
inequalities (Rockafellar, 1970, Chapter 17). We will refer to VPC := {v1, . . ., vnV } as
the set of the extreme points of PC. To ﬁnd all the elements in VPC, we will make use of
graph theory as follows. Cnm is composed of (cseq)nm elements. Each element in Cnm can be
viewed as one node in a graph. In addition, the transitions (edges) between the elements in
Cnm are deﬁned by the possible values of uk+1 when we move from (uk−nm+1, . . ., uk) to
(uk−nm+2, . . ., uk+1), for all integers k > 0. The resulting graph is referred to as a de Bruijn
graph. Figure 1 illustrates this idea, when cseq = 2, nm = 2, and C = {0, 1}. From this ﬁgure
we can see that, if we are in node (0, 1) at time t, then we can only transit to node (1, 0) or
(1, 1) at time t + 1. In the following, GX = {X, E} will denote a de Bruijn graph with set
of nodes X and set of edges E.
To ﬁnd all the elements in VPC we rely on the concept of prime cycles. A prime cycle is an
elementary cycle1 whose set of nodes does not have a proper subset which is an elementary
cycle (Zaman, 1983, pp. 678). It is known that the prime cycles of a graph describe all the
elements in the set VPC (Zaman, 1983, Theorem 6). In other words, each prime cycle deﬁnes
one element vj ∈VPC. Furthermore, each vj corresponds to a uniform distribution whose
support is the set of elements of its prime cycle, for all j ∈{1, . . ., nV} (Zaman, 1983,
pp. 681). Therefore, the elements in VPC can be described by ﬁnding all the prime cycles
associated with de Bruijn graph GCnm .
It is known that all the prime cycles associated with GCnm can be derived from the ele-
mentary cycles associated with GCnm −1 (Zaman, 1983, Lemma 4), which can be found by
using existing algorithms2. To illustrate this, we consider the graph depicted in Figure 2.
One elementary cycle for this graph is given by (0, 1, 0). Using Zaman (1983, Lemma 4),
the elements of one prime cycle for the graph GC2 are obtained as a concatenation of the
elements in the elementary cycle (0, 1, 0). Hence, the prime cycle in GC2 associated with
this elementary cycle is ((0, 1), (1, 0), (0, 1)).
1An elementary cycle is a cycle where all nodes are diﬀerent, except for the ﬁrst and the last (Johnson, 1975, p.
77).
2For the examples in Section 7, we have used the algorithm presented in Johnson (1975, p. 79–80) complemented
with the one proposed in (Tarjan, 1972, pp. 157).

318
Paper H
Robust Input design for non-linear dynamical models
Figure 1. Example of graph derived from Cnm , with nm = 2, and C := {0, 1}.
Figure 2. Example of graph derived from Cnm , with nm = 1, and C := {0, 1}.

4
Estimation of the Fisher information matrix
319
Once we ﬁnd the prime cycles, it is possible to generate an input sequence u j
1:T from vj,
which will be referred to as the basis inputs. As an example, we use the graph in Figure 1. One
prime cycle for this graph is given by ((0, 1), (1, 0), (0, 1)). Therefore, the sequence u j
1:T is
given by taking the value of ut in each node, i.e., u j
1:T = {1, 0, 1, 0, . . ., ((−1)T −1 + 1)/2}.
Given u j
1:T , we can use it to obtain the corresponding information matrix for vj ∈VPC,
denoted by I(j)
F . However, in general the matrix I(j)
F
cannot be computed explicitly, as
mentioned in issue (C) in Section 2. To overcome this problem, we use particle methods to
approximate I(j)
F , as discussed in the next section.
Remark 1. For simplicity, we considered in this section a scalar input. However, the proposed ap-
proach can also be used when the model has multiple inputs. Indeed, for a multiple input formulation
the states in the de Bruijn graph are associated with the possible states of an nu × nm matrix. The
rest of the procedure for the multiple input case follows exactly the same lines as for the scalar case
described here.
■
Estimation of the Fisher information matrix
The proposed method relies upon accurate estimates of the Fisher information matrix,
which unfortunately is analytically intractable for general ssms (1). Instead, we make use of
statistical simulation methods known as particle smoothers to compute estimates. In this
section, we provide the reader with an overview of the approach. More extensive treatments
are found in Doucet and Johansen (2011) and Lindsten and Schön (2013).
Estimating the score function
From (2), we know that we can compute the Fisher information matrix by the use of the
score function. However, the score function is also intractable for a general ssm but it
can be estimated using particle smoothers. The key ingredient for this is the Fisher identity
(Cappé et al., 2007) that we present in Lemma 2. For the remainder of this section, we write
v := v1:T for any vector v1:T to ease the notation. In the Fisher identity, pθ(x, y|u) denotes
the complete data log-likelihood for (1) given by
log pθ(x, y|u) = log µθ(x0) +
TX
t=1
ξθ(xt−1:t),
(9)
ξθ(xt−1:t) := log fθ(xt | xt−1, ut−1) + log gθ(yt | xt, ut),
where we make use of the notation xt−1:t = {xt−1, xt }.
Lemma 2 (Fisher identity (Cappé et al., 2005)). Assume that the following hold:
(i) For any θ ∈Θ, pθ(y|u) is positive and ﬁnite.
(ii) For any (θ, θ′) ∈Θ × Θ,

 log pθ(x|y, u)pθ′(x|y, u) dx,

320
Paper H
Robust Input design for non-linear dynamical models
is ﬁnite.
(iii) `θ(y|u) is continuously diﬀerentiable on Θ.
(iv) For any θ′ ∈Θ, the function Lθ′ : Θ →R deﬁned as
Lθ′(θ) := −

log pθ(x|y, u)pθ′(x|y, u) dx,
is continuously diﬀerentiable on Θ. In addition, Lθ′(θ) is ﬁnite for any (θ, θ′) ∈Θ × Θ,
and
∇θ

log pθ(x|y, u)pθ′(x|y, u) dx
=

∇θ log pθ(x|y, u)pθ′(x|y, u) dx.
Then,
S(θ′) =

∇θ log pθ(x, y|u)
θ=θ′pθ′(x|y, u) dx .
(10)
Proof: We refer to Cappé et al. (2005) for a proof of this lemma.
□
Using (9) and (10), we arrive at the estimator
S(θ′) =
TX
t=1

∇θ ξθ(xt−1:t)|θ=θ′pθ′(xt−1:t |y, u) dxt−1:t
|                                                  {z                                                  }
:=St (θ′)
.
(11)
To make use of this estimator, we require the two-step smoothing distribution pθ(xt−1:t |y, u),
which is not analytically available for a general ssm. Instead, we approximate it using an
empirical distribution
Dpθ(dxt−1:t |y) :=
N
X
i=1
w(i)
T δx(i)
t−1:t (dxt−1:t),
(12)
where x(i)
t and w(i)
t
denote particle i and its normalized weight at time t. Here, {x(i)
t , w(i)
t }T
t=1
denotes the particle system generated by a particle ﬁlter and δx′ denotes the Dirac measure
located at x = x′.
In this paper, we propose to generate the particle system using the bootstrap particle ﬁlter
(bpf). We outline the procedure in Algorithm 1. However, the estimator in (12) based on
the bpf often suﬀers from poor accuracy. This is due to problems with particle degeneracy,
see e.g., Doucet and Johansen (2011).
To mitigate this problem, we make use of a particle smoother that introduces a backward
sweep after the forward run of the particle ﬁlter. Here, we use the forward-ﬁltering back-
wards simulator (ffbsi) with rejection sampling and early stopping as discussed by Douc
et al. (2011); Taghavi et al. (2013). There are many other interesting alternatives, we refer to
Lindsten and Schön (2013) for a recent survey.

4
Estimation of the Fisher information matrix
321
Algorithm 1 Bootstrap particle ﬁlter (bpf)
Inputs: An ssm (1), y1:T (observations), u1:T (inputs), N ∈N (number of particles).
Output: {x(i)
t , w(i)
t }N
i=1, t = 1, . . .,T .
All operations are carried out over i, j = 1, . . ., N .
1: Sample x(i)
0 ∼µθ(x0) and set w(i)
0 = 1/N .
2: for t = 1 to T do
3:
(Resampling) Sample a(i)
t
from a multinomial distribution with P

a(i)
t
= j

= w(j)
t−1.
4:
(Propagation) Sample x(i)
t
∼fθ

x(i)
t x a(i)
t
t−1, ut

.
5:
Set x(i)
0:t =
(
x a(i)
t
0:t−1, x(i)
t
)
.
6:
(Weighting) Calculate H
w(i)
t
= gθ

ytx(i)
t , ut

.
7:
Normalize H
w(i)
t
(over i) to obtain w(i)
t .
8: end for
The complete procedure for running the ffbsi is presented in Algorithm 2 and it makes use
of the bpf in Algorithm 1 to create the particle system. In Algorithm 2, Multi({p(i)}N
i=1)
denotes the multinomial distribution over N elements, with p(i) being the probability of
choosing the i-th element. In addition, we note that the parameter ρ required by Algo-
rithm 2 is chosen such that fθ(xt+1 | xt) ≤ρ for all t ∈{1, . . ., T }.
The computational complexity of ffbsi is of order O( nmt), where N and M denote
the number of ﬁlter and smoother particles, respectively. In general, N and M determine
the accuracy of the estimates and are highly dependent on the model and on T . However,
often we can have M < N and still obtain reasonable accuracy but the estimates are only
unbiased in the limit when N and M tend to inﬁnity. The theoretical properties of the
ffbsi are further analysed and discussed in Douc et al. (2011). We return to numerically
examine the properties of the smoother when N and M are ﬁnite in Section 7.1.
The resulting estimator
From (11), we note that S(θ′) can be written as a sum of conditional scores St(θ′). This can
be seen as a Martingale representation as each conditional score is conditionally independent
given the past. Using results presented in Segal and Weinstein (1989) and Meilijson (1989),
we can compute an estimate of the Fisher information matrix by
DIF (θ) = 1
T

TX
t=1
f D
St(θ)
g f D
St(θ)
g⊤−1
T
f D
S(θ)
g f D
S(θ)
g⊤
,
(13)
where
D
S(θ) :=
TX
t=1
D
St(θ) .
(14)

322
Paper H
Robust Input design for non-linear dynamical models
Algorithm 2 Fast forward-ﬁltering backward-smoothing with early stopping (fffbsi-es)
Inputs: Inputs to Algorithm 1, M ∈N (No. backward trajectories), Nlimit ∈N (Limit for
when to stop using rejection sampling), ρ > 0.
Output: DIF (θ) (estimate of the Fisher information matrix).
1: Run Algorithm 1 to obtain the particle system
(
x(i)
t , w(i)
t
)N
i=1 for t = 1, . . .,T .
2: Sample bT (j)	M
j=1 ∼Multi {w(i)
T }N
i=1

.
3: Set ˜x(j)
T = xbT (j)
T
for j = 1, . . ., N .
4: for t = T −1 to 1 do
5:
L ←1, . . ., M .
6:
{Rejection sampling until Nlimit trajectories remain.}
7:
while |L| ≥Nlimit do
8:
n ←Multi(L).
9:
δ ←∅.
10:
Sample I (k)	n
k=1 ∼Multi {w(i)
t }N
i=1

.
11:
Sample U(k)	n
k=1 ∼Uniform([0, 1]).
12:
for k = 1 to n do
13:
if U(k) ≤f  ˜xL(k)
t+1 | xI (k)
t
/ρ then
14:
bt(L(k)) ←I (k).
15:
δ ←δ ∪{L(k)}.
16:
end if
17:
end for
18:
L ←L \ δ.
19:
end while
20:
{Use standard ffbsi for the remaining trajectories.}
21:
for j ∈L do
22:
Compute ˜w(i,j)
t|T ∝w(i)
t f  ˜x(j)
t+1 | x(i)
t

for i = 1, . . ., N .
23:
Normalize the smoothing weights  ˜w(i,j)
t|T
	N
i=1.
24:
Draw bt(j) ∼Multi
 ˜w(i,j)
t|T
	N
i=1

.
25:
end for
26:
Set ˜x(j)
t:T =
(
xbt (j)
t
, ˜x(j)
t+1:T
)
for j = 1, . . ., M .
27:
Calculate the estimate of the score function at time t using (11) by
D
St(θ) = 1
M
M
X
j=1
ξθ

˜x(j)
t:t+1

.
28: end for
29: Compute DIF (θ) using (13).

4
Estimation of the Fisher information matrix
323
Here we make use of Algorithm 2 to compute DIF (θ). We can analyze the variance of this
estimator using the martingale diﬀerence property. Let D
S`(θ) and DIF ,`m(θ) denote the `-th
and (`, m) entry of D
S(θ) and DIF (θ), respectively. Then, for the estimator (13) we have
T 2 Var
(DIF ,`m(θ)
)
= (T −1)2
T 2
Var
( TX
t=1
D
St,`(θ)D
St,m(θ)
)
+ 2
T
 
E
( TX
t=1
D
St,`(θ)D
St,m(θ)
2)
−E
( TX
t=1
f D
St,`(θ)D
St,m(θ)
g2)!
+ 1
T 2
 
E
(f D
S`(θ)D
Sm(θ)
g2)
−E
( TX
t=1
D
St,`(θ)D
St,m(θ)
2)!
.
(15)
We note that the variance is bounded for a ﬁxed T , provided that the variances of each
element in PT
t=1
f D
St(θ)
g f D
St(θ)
g⊤and
f D
S(θ)
g f D
S(θ)
g⊤together with their second order
moments are bounded. The latter is satisﬁed by ffbsi under some regularity assumptions
as discussed in Douc et al. (2011). Hence, we conclude that this estimator is consistent
as T →∞when D
S(θ) is unbiased, which means that both N , and M should also tend
to inﬁnity. Clearly, this is not satisﬁed in any practical application and we investigate the
accuracy of the estimate in Section 7.1 for the ﬁnite sample case.
An alternative for estimating IF (θ) is to use the sample covariance matrix of the score
function as discussed in Valenzuela et al. (2014a). In this approach, we estimate the score
function D
S(k) using particle methods over k ∈{1, . . ., K } diﬀerent realizations of the ssms
based on a single realization of the input. Hence, we obtain the estimate by
DIF (θ) =
1
KT
K
X
k=1
f D
S(k)(θ)
g f D
S(k)(θ)
g⊤.
(16)
The variance of this estimator is given by
T 2 Var
(DIF ,`m(θ)
)
= 1
K Var

D
S(k)
` (θ)D
S(k)
m (θ)

,
(17)
for some k ∈{1, . . ., K }. This implies that the accuracy of (16) increases with the number
of realizations K, provided that the variance of each element in
f D
S(k)(θ)
g f D
S(k)(θ)
g⊤is
bounded, which again is satisﬁed by ffbsi. Therefore, we conclude that the variance of the
estimator (16) is bounded as T →∞if N and M also increases suﬃciently fast, and it goes
to zero as K →∞.
The main beneﬁt of using the estimator in (13) instead of (16) is a smaller computational
cost. In the former, we only require estimating the score function for a single realization of
the ssm, whereas we require K such estimates in the latter case. It is diﬃcult to establish
theoretically which of the two estimators has better accuracy. However, we have observed
numerically that the new estimator outperforms the latter in terms of both accuracy and
computational cost. A third alternative discussed by Poyiadjis et al. (2011) and Dahlin et al.
(2015) is to make use of the Louis identity to estimate the Fisher information matrix, which
often results in negative deﬁnite estimate, which is a problem in our application.

324
Paper H
Robust Input design for non-linear dynamical models
Final input design method
The proposed method to design input signals in Cnm for the identiﬁcation of non-linear
ssms is summarized in Algorithm 3. The method solves an approximation of Problem 1,
which can be written as
γ⋆= argmax
γ={αj }nV
j=1
D
R  H(Iapp
F (γ, θ), θ)
(18)
subject to
Iapp
F (γ, θi) =
nV
X
j=1
αj DI(j)
F (θi)
nV
X
j=1
αj = 1,
for i ∈{1, . . ., Ns} and j ∈{1, . . ., nV}. Here, αj ≥0 and D
R denotes the approximation
of the function R when the set Θ is approximated by {θi}Ns
i=1. The implementation of D
R
for the cost functions considered in this article follows the solution presented for issue (A)
in Section 2. Thus, for R = Eθ{ · } we have
D
R(H(Iapp
F (γ, θ), θ)) = 1
Ns
Ns
X
i=1
H(Iapp
F (γ, θi), θi),
(19)
and for R = minθ ∈Θ{ · } we obtain
D
R(H(Iapp
F (γ, θ), θ)) =
min
θ ∈{θi}Ns
i=1
H(Iapp
F (γ, θ), θ) .
(20)
Algorithm 3 computes the vector γ⋆= {α⋆
j }nV
j=1 which deﬁnes the optimal pmf popt
u (u1:nm)
as a convex combination of the measures associated with the elements in VPC, according to
popt
u
=
nV
X
j=1
α⋆
j vj .
(21)
Input signal generation
In Section 3 we discussed a parametrization of PC to obtain a computationally tractable
description of the pmfs in this set. However, a remaining issue must be addressed: given
a pmf in PC, say p, we want to obtain an input vector u1:nseq, where each ut is sampled
from p, for t ∈{1, . . ., nseq}. In this section we present a procedure to generate an input
sequence u1:nseq from a given pmf p(u1:nm) ∈PC, which has been introduced in Valenzuela
et al. (2015). To this end, we note that it is possible to associate GCnm with the discrete-time
Markov chain (Doob, 1953)
πk+1 = A πk ,
(24)
where A ∈RCnm ×Cnm is a transition probability matrix3, and πk ∈RCnm is the state vector.
In this case, there is a one-to-one correspondence between each entry of πk ∈RCnm and an
element of Cnm.
3Given a set of ﬁnite cardinality X, we denote by RX×X the matrices with real entries, with dimensions given
by the cardinality of X.

6
Input signal generation
325
Algorithm 3 New input design method
Inputs: C (input values), nm, T (number of input samples), P (probability measure over
Θ), and Ns (number of samples over Θ).
Output: γ⋆(optimal weighting of the basis inputs).
1: Sample {θi}Ns
i=1 from Θ according to P.
2: Compute all the elementary cycles of GCnm −1 using, e.g., Johnson (1975, p. 79–80),
Tarjan (1972, p. 157).
3: Compute all the prime cycles of GCnm from the elementary cycles of GCnm −1 as explained
in Section 3 (cf. Zaman (1983, Lemma 4)).
4: Generate the input signals u j
1:T from the prime cycles of GCnm , for each j
∈
{1, . . ., nV}.
5: for i = 1 to Ns do
6:
Execute Algorithm 2 based on θi and {u j
1:T }nV
j=1 to obtain {DI(j)
F (θi)}nV
j=1.
7: end for
8: Solve the optimization problem (18) to obtain γ⋆.
Algorithm 4 Design of a transition probability matrix
Inputs: A pmf p ∈PC, and nm.
Output: A transition probability matrix A with stationary distribution p.
1: For each r ∈Cnm, deﬁne
Nr := {l ∈Cnm : (l, r) ∈E} .
(22)
In other words, Nr is the set of ancestors of r.
2: For each r, l ∈Cnm, let
Arl =

P{r }
P
k∈Nr
P{k} ,
if l ∈Nr and
P
k∈Nr
P{k} , 0,
1
#Nr ,
if l ∈Nr and
P
k∈Nr
P{k} = 0,
0,
otherwise.
(23)

326
Paper H
Robust Input design for non-linear dynamical models
Based on this association, p(u1:nm) corresponds to Πs ∈RCnm , the stationary distribution
of a Markov chain of the form (24). Therefore, in order to generate an input sequence u1:nseq
from p(u1:nm), we can design a Markov chain having p(u1:nm) as its stationary distribution,
and simulate this Markov chain to generate u1:nseq from its samples.
If Arl ∈R denotes the (r, l)-entry in A, then a valid A for the Markov chain (24) must
satisfy the following conditions
Arl ≥0, for all r, l ∈Cnm ,
(25a)
X
r ∈Cnm
Arl = 1, for all l ∈Cnm ,
(25b)
Arl = 0, if (l, r) < E .
(25c)
Note that the indices of A are not numerical, but belong to Cnm. Algorithm 4 presents a
method to design a transition probability matrix A for GCnm with Πs as stationary distri-
bution. Here we will only make use of this result to generate u1:nseq with distribution given
by the optimal pmf popt
u (u1:nm). We refer to Valenzuela et al. (2015) for more details about
Algorithm 4, where its validity is established.
Numerical illustrations
In this section, we discuss numerical simulations to illustrate some aspects of the proposed
input design method in two ssms. In the ﬁrst illustration, we make use of a linear Gaussian
state space ( lgss) model to evaluate the accuracy of the estimator for the Fisher informa-
tion matrix and the impact of the design parameters in Algorithm 2. We also compare the
solution obtained for the input design problem to some standard input signals as a sanity
check of that the method works. In the second illustration, we consider a non-linear state
space model, which is more challenging from an input design perspective.
Accuracy of information matrix estimation
Consider an lgss model given by
xt+1 | xt ∼N xt+1; φxt + ut, σ2
v
,
yt | xt ∼N yt; xt, 0.12,
(26)
where the parameter vector is θ = {φ, σv} with the state persistence φ ∈(−1, 1) and the
standard deviation of the innovations σv ∈R+.
Kalman methods
In our application, we require accurate estimates of the Fisher information matrix. In
general, this accuracy depends on both the state inference problem and on the model. For
the lgss model, we can solve the former exactly using Kalman methods. Hence, we can
investigate the properties of the estimator in (13) without the inﬂuence of the variance
induced by the particle ﬁlter. More speciﬁcally, we are interested in the accuracy of the
estimator as T grows large as discussed in Section 4.2.

7
Numerical illustrations
327
-0.5
1.0
2.5
information matrix
0
5000
10000
15000
20000
25000
-0.06
0.02
no. observations
natural gradient
0
5000
10000
15000
20000
25000
Figure 3. Top: the information matrix element for φ (orange), σv (green) and cross-term be-
tween φ and σv (blue). Bottom: the natural gradient for diﬀerent number of observations T .
The plots show the results from 25 Monte Carlo simulations.
10
20
50
100
200
500
-12.4
-11.8
no. particles
log-error
10
20
30
40
50
60
70
80
90
100
-12.16
-12.22
no. backward paths
log-error
Figure 4. The logarithm of the error in Frobenius norm of the estimate of the information
matrix when varying N (top) and M (bottom). The plots show the results from 25 Monte
Carlo simulations and the shaded areas indicate the span of the errors.

328
Paper H
Robust Input design for non-linear dynamical models
To this end, we simulate 25 data sets with T = 25 · 103 observations using the parameter
θ0 = {0.5, 1.0} without any input (ut ≡0). We solve the state inference problem using the
Kalman smoother (Rauch et al., 1965) and estimate the information matrix using (13) while
varying the amount of observations. In Figure 3, we present the elements of the information
matrix and the resulting natural gradient (the gradient scaled by the inverse Hessian) when
varying T . We see that the estimator stabilizes after about T ≈15 · 103 observations. Hence,
we conclude that this is the number of observations required to obtain accurate estimates
of the Fisher information matrix for this model. Furthermore, we observe that the natural
gradients are almost zero at this value ofT , indicating that the maximum likelihood estimate
of parameter vector is indeed θ0.
Particle methods
For a general ssm, we cannot solve the state inference problem exactly and we need to
make use of approximations obtained by e.g., particle methods. Here, we are interested in
observing how the accuracy of the estimates of the Fisher information matrix depends on
the choice of the number of particles N and the number of backwards trajectories M in
Algorithm 2. We ﬁx the number of observations to T = 15 · 103 based on the results for
the Kalman smoother in the previous section. Furthermore, we use of the fully adapted
particle ﬁlter (fapf; Pitt and Shephard, 1999), which reduces the computational cost and
increases the accuracy of the state estimates.
In the uppper part of Figure 4, we present the log-error in the Frobenius norm of the
information matrix when varying N and M . In the ﬁrst case, we ﬁx M = ⌊N /4⌋(the
closest integer to N /4 from below) and vary N between 10 and 500 particles. For this
model, we conclude that 200 particles are enough to obtain good accuracy of the estimate.
Note that the increase of the log-error of the box plots in Figure 4 for N ≥50 is probably
due to the randomness of the Monte Carlo simulations.
In the second case, we ﬁx N = 200 and vary the number of backward paths in the set
M ∈{10, 20, . . ., 100}. We present the results in the lower part of Figure 4. Here, we note
that the accuracy of the estimates is quite robust to the choice of M and increasing the
number of backward trajectories above 10 seems to be a waste of computational resources.
Input design for the LGSS model
We now return to the problem of designing an input sequence u1:T for the lgss model
(26) such that the parameters θ = {φ, σv} can be estimated with a high precision. As
discussed before, we can make use of both Kalman methods and the fapf algorithm to
solve the state estimating problem and estimate the Fisher information matrix. Hence,
we make use of both methods together with the procedure outlined in Algorithm 3 to
compare the approximate particle implementation to the optimal Kalman implementation.
Furthermore, we make use of the same parameters and settings as in Section 7.1.
The robust input design problem considers the set of model parameters given by Θ =
{(φ, σv) : φ ∈[0.4, 0.6], σv ∈[0.8, 1.2]}. For Algorithm 3, we use nm = 2, and cseq =
3 values (-1, 0 and 1), resulting in 8 diﬀerent basis inputs. Also, we use H(IF (θ), θ) =

7
Numerical illustrations
329
Kalman ( km)
Particle ( pm)
Input
Dφ
L
σv
Dφ
L
σv
none
-9.04
-9.04
-2.84
-2.84
constant
-15.33
-13.47
-6.29
-5.41
uniform
-12.88
-13.44
-5.79
-5.43
mean case ( km)
-14.13
-13.46
-5.91
-5.42
worst case ( km)
-15.33
-13.45
-6.29
-5.41
mean case ( pm)
-15.37
-13.47
-6.29
-5.41
worst case ( pm)
-15.33
-13.47
-6.29
-5.41
Table 1. Log-mse of the parameter estimates in the lgss model obtained using em with diﬀerent
inputs. Bold face marks the best values.
Input
Log-mse
Bootstrapped 95% ci
Dγ
Dβ
Dγ
Dβ
none
-2.54
-3.48
0.011
0.067
constant
3.45
-2.76
0.704
0.020
uniform
-1.67
-4.65
0.167
0.038
mean case
-1.11
-4.99
0.187
0.025
worst case
-1.71
-5.14
0.157
0.032
Table 2. Left: log-mse of the parameter estimates in the nl model obtained using em with
diﬀerent inputs. Right: length of the bootstrapped 95% conﬁdence intervals for the estimated
parameters. Bold face marks the best values.

330
Paper H
Robust Input design for non-linear dynamical models
log det(IF (θ)) as the scalar cost function of the Fisher information matrix. By using Ns =
100 realizations uniformly sampled from the parameter space Θ, we can estimate the optimal
weighting of the basis inputs using both Kalman and particle methods.
Parameter inference
With the optimal input generated, we would like to evaluate the eﬀect of applying the input
to the system in terms of the uncertainty in the estimated parameters. Here, we make use
of the expectation maximization ( em) algorithm from Schön et al. (2011). We consider
N = 200 particles, M = 20 backward trajectories and 150 iterations of the em algorithm to
identify the parameters.
In Table 1, we present the logarithm of the mean squared error ( mse) of the parameter
estimates for diﬀerent inputs: none (ut ≡0), constant (ut ≡1), uniform random input
and the optimal inputs constructed using the proposed technique with the Kalman method
( km) and the particle method ( pm) when R = Eθ{ · } (mean case), and R = minθ ∈Θ{ · }
(worst case). From these results, we conclude that the input design technique using particle
methods attains similar log-MSE values than those obtained with a constant signal (ut = 1).
Hence, the input design approach works for the simple lgss model.
Input design for a non-linear model
As a second example, we consider a non-linear ( nl) ssm of the form
xt+1 | xt ∼N

xt+1;
1
γ + x2
t
+ ut, 0.12
,
(27a)
yt | xt ∼N

yt; βx2
t, 12
,
(27b)
where the parameters are θ = {γ, β}. We generate T = 103 observations from the model
with θ0 = {2, 0.8}.
For Algorithm 3, we use nm = 2, and cseq = 4 values (−1, −1/3, 1/3 and 1), resulting in 24
diﬀerent basis inputs. Otherwise, we apply the same procedure as before to determine the
optimal input and to carry out the parameter inference. Note that for this model, we cannot
make use of the Kalman smoother or the fapf as the model is not linear in the parameters
and state. Instead, we make use of the bpf using N = 2.5 · 103 particles and M = 100
backward trajectories in the ffbsi smoother. In addition, we consider the parameter set
described as Θ = {(β, γ) : γ ∈[1.6, 2.4], β ∈[0.6, 1]}. The cost functions R are computed
by approximating Θ using {θi}Ns
i=1, where Ns = 30 and each θi is sampled from a uniform
distribution over Θ.
In Figure 5, we present 40 realizations of the estimated parameter vector at each iteration
of the em algorithm for diﬀerent inputs. We note that the parameter β in the model
(27) is easier to estimate than the parameter γ. This can be explained by noticing that the
measurement equation in (27) is linear in β, which is not the case for γ in the state equation.
Moreover, the expression x2
t in the measurement equation implies that two diﬀerent values
for the state can equally represent a value for yt. Therefore, the parameter estimation
problem for the model (27) is diﬃcult to solve.

7
Numerical illustrations
331
0
200
400
600
800
1000
0
1
2
3
4
iteration
parameter estimate
none
0
200
400
600
800
1000
0
1
2
3
4
iteration
parameter estimate
uniform
0
200
400
600
800
1000
0
1
2
3
4
iteration
parameter estimate
mean case
0
200
400
600
800
1000
0
1
2
3
4
iteration
parameter estimate
worst case
Figure 5. The evolution of the estimated parameters in the nl model obtained from 40 runs
of the em algorithm with random initializations and diﬀerent inputs (none, uniform, mean
optimal input, and worst case optimal input). For each plot, the upper curves represent the
value of Dγ at each iteration, while the lower ones show the value of Dβ at each iteration. The
shaded area indicates the span between the maximum and minimum values.

332
Paper H
Robust Input design for non-linear dynamical models
An interesting result derived from Figure 5 is that the optimal input accelerates the conver-
gence of the model parameters when compared to a uniformly distributed input. Indeed,
for the optimal inputs the parameter estimates stabilize after approximately 500 iterations,
and the same is achieved after approximately 800 iterations for the uniform input.
Table 2 presents the log-mse of the parameter estimates using diﬀerent input signals. It
seems that applying a zero input would lead to more accurate results for the parameter γ.
However, from Figure 5 we see that the parameter γ is not correctly identiﬁed, and thus
the log-mse value is aﬀected by a non-zero bias. The same conclusion can be drawn for the
estimated parameters using a constant input. On the other hand, we note that a nonconstant
input helps to improve the accuracy of the estimated parameters. In particular, the designed
input for the worst case achieves better accuracy than the uniform excitation.
To conﬁrm this, Table 2 also shows the bootstrapped 95% conﬁdence interval ( ci) for
the estimated parameters, which is obtained after 1, 000 resamplings and using the adjusted
bootstrap percentile (Davison and Hinkley, 1997). We note that the length of the conﬁdence
interval for the worst case input is smaller than the one obtained with the uniform input.
The optimal input obtained in the mean case also reduces the length of the conﬁdence
interval for the parameter Dβ. Hence, the input design method can be employed to obtain
better parameter estimates in this challenging model.
Conclusions
In this article a robust input design method for the identiﬁcation of non-linear ssms is
presented. The problem considers the design of an input sequence as a realization of a
stationary process maximizing a scalar cost function of the Fisher information matrix.
Since the model parameters are unknown a priori, the optimization of the experiment
considers a measure of the uncertainty over the space of model parameters.
Numerical examples show that the method helps to improve the accuracy of the estimated
parameters when the em algorithm is employed to identify the model.
Future work on the subject will be focused on reducing the computational eﬀort required to
obtain a tractable parametrization of the optimization set, and extensions of the proposed
method to more general input classes.

Bibliography
333
Bibliography
S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, 2004.
C. Brighenti, B. Wahlberg, and C. R. Rojas. Input design using Markov chains for system
identiﬁcation. In Proceedings of the joint 48th Conference on Decision and Control and
28th Chinese Conference, pages 1557–1562, Shanghai, China, December 2009.
G. Calaﬁore and M.C. Campi. Uncertain convex programs: randomized solutions and
conﬁdence levels. Mathematical Programming, 102(1):25–46, 2005.
M. C. Campi and S. Garatti. The exact feasibility of randomized solutions of uncertain
convex programs. SIAM Journal on Optimization, 19(3):1211–1230, 2008.
O. Cappé, E. Moulines, and T. Rydén. Inference in hidden Markov models. Springer Verlag,
2005.
O. Cappé, S. J Godsill, and E. Moulines. An overview of existing methods and recent
advances in sequential Monte Carlo. Proceedings of the IEEE, 95(5):899–924, 2007.
D. R. Cox. Planning of experiments. New York: Wiley, 1958.
J. Dahlin, F. Lindsten, and T. B. Schön. Particle Metropolis-Hastings using gradient and
Hessian information. Statistics and Computing, 25(1):81–92, 2015.
A.C. Davison and D.V. Hinkley.
Bootstrap methods and their application.
Cambridge
University Press, 1997.
A. De Cock, M. Gevers, and J. Schoukens. A preliminary study on optimal input design for
nonlinear systems. In Proceedings of the 52nd Conference on Decision and Control (CDC),
pages 4931–4936, Florence, Italy, December 2013.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.
J. L. Doob. Stochastic processes. New York Wiley, 1953.
R. Douc, A. Garivier, E. Moulines, and J. Olsson. Sequential Monte Carlo smoothing for
general state space hidden Markov models. Annals of Applied Probability, 21(6):2109–2145,
2011.
A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years
later. In D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering.
Oxford University Press, 2011.
V. V. Fedorov. Theory of optimal experiments. Academic Press, 1972.
M. Forgione, X. Bombois, P. M. J. Van den Hof, and H. Hjalmarsson. Experiment design for
parameter estimation in nonlinear systems based on multilevel excitation. In Proceedings
of the 13th European Control Conference (ECC), Strasbourg, France, June 2014.
L. Gerencsér, H. Hjalmarsson, and J. Mårtensson. Identiﬁcation of ARX systems with
non−stationary inputs−asymptotic analysis with application to adaptive input design.
Automatica, 45(3):623–633, 2009.

334
Paper H
Robust Input design for non-linear dynamical models
M. Gevers. Identiﬁcation for control: from the early achievements to the revival of experi-
ment design. European Journal of Control, 11:1–18, 2005.
G. C. Goodwin and R. L. Payne. Dynamic System Identiﬁcation: Experiment Design and
Data Analysis. Academic Press, New York, 1977.
R. B. Gopaluni, T. B. Schön, and A. G. Wills. Input design for nonlinear stochastic dynamic
systems - A particle ﬁlter approach. In Proceedings of the 18th IFAC World Congress,
Milano, Italy, August 2011.
R. Hildebrand and M. Gevers. Identiﬁcation for control: Optimal input design with respect
to a worst-case ν-gap cost function. SIAM Journal of Control Optimization, 41(5):1586–
1608, 2003.
H. Hjalmarsson and J. Mårtensson. Optimal input design for identiﬁcation of non-linear
systems: Learning from the linear case. In Proceedings of the American Control Conference,
pages 1572–1576, New York, United States, 2007.
H. Jansson and H. Hjalmarsson. Input design via LMIs admitting frequency-wise model
speciﬁcations in conﬁdence regions. IEEE Transactions on Automatic Control, 50(10):
1534–1549, 2005.
D. B. Johnson. Finding all the elementary circuits of a directed graph. SIAM Journal on
Computing, 4(1):77–84, mar 1975.
C. Larsson, H. Hjalmarsson, and C. R. Rojas. On optimal input design for nonlinear FIR-
type systems. In Proceedings of the 49th IEEE Conference on Decision and Control (CDC),
pages 7220–7225, Atlanta, USA, December 2010.
K. Lindqvist and H. Hjalmarsson. Optimal input design using linear matrix inequalities.
In Proceedings of the IFAC Symposium on System Identiﬁcation, Santa Barbara, California,
USA, July 2000.
F. Lindsten and T. B. Schön. Backward simulation methods for Monte Carlo statistical
inference. In Foundations and Trends in Machine Learning, volume 6, pages 1–143, August
2013.
L. Ljung. System Identiﬁcation. Theory for the User, 2nd ed. Upper Saddle River, NJ: Prentice-
Hall, 1999.
G. McLachlan and T. Krishnan. The EM algorithm and extensions. John Wiley & Sons,
2008.
I. Meilijson. A fast improvement to the EM algorithm on its own terms. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 51(1):127–138, 1989.
M. K. Pitt and N. Shephard. Filtering via simulation: auxiliary particle ﬁlters. Journal of
the American Statistical Association, 94(446):590–599, 1999.
G. Poyiadjis, A. Doucet, and S. S. Singh. Particle approximations of the score and ob-
served information matrix in state space models with application to parameter estimation.
Biometrika, 98(1):65–80, 2011.

Bibliography
335
H. E. Rauch, F. Tung, and C. T. Striebel. Maximum likelihood estimates of linear dynamic
systems. AIAA Journal, 3(8):1445–1450, August 1965.
R. T. Rockafellar. Convex Analysis. Princeton University Press, 1970.
C. R. Rojas, J. S. Welsh, G. C. Goodwin, and A. Feuer. Robust optimal experiment design
for system identiﬁcation. Automatica, 43(6):993–1008, June 2007.
C. R. Rojas, H. Hjalmarsson, L. Gerencsér, and J. Mårtensson. An adaptive method for
consistent estimation of real-valued non-minimum phase zeros in stable LTI systems.
Automatica, 47(7):1388–1398, 2011.
C. R. Rojas, J. C. Aguero, J. S. Welsh, G. C. Goodwin, and A. Feuer. Robustness in
experiment design. IEEE Transactions on Automatic Control, 57(4):860–874, 2012.
T. B. Schön, A. Wills, and B. Ninness. System identiﬁcation of nonlinear state-space models.
Automatica, 47(1):39–49, January 2011.
M. Segal and E. Weinstein. A new method for evaluating the log-likelihood gradient, the
Hessian, and the Fisher information matrix for linear dynamic systems. IEEE Transactions
on Information Theory, 35(3):682–687, 1989.
H. Suzuki and T. Sugie. On input design for system identiﬁcation in time domain. In
Proceedings of the European Control Conference (ECC), Kos, Greece, July 2007.
E. Taghavi, F. Lindsten, L. Svensson, and T. B. Schön. Adaptive stopping for fast particle
smoothing. In Proceedings of the 38th International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Vancouver, Canada, May 2013.
R. Tarjan. Depth-First Search and Linear Graph Algorithms. SIAM Journal on Computing,
1(2):146–160, June 1972.
P. E. Valenzuela, C. R. Rojas, and H. Hjalmarsson. Optimal input design for dynamic
systems: a graph theory approach. In Proceedings of the IEEE Conference on Decision and
Control (CDC), pages 5740–5745, Florence, Italy, December 2013.
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. A graph/particle-based method for
experiment design in nonlinear systems. In Proceedings of the 19th IFAC World Congress,
Cape Town, South Africa, August 2014a.
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. A graph/particle-based method for
experiment design in nonlinear systems. In Proceedings of the 19th IFAC World Congress,
Cape Town, South Africa, August 2014b.
P. E. Valenzuela, C. R. Rojas, and H. Hjalmarsson. A graph theoretical approach to input
design for identiﬁcation of nonlinear dynamical models. Automatica, 51:233–242, 2015.
P. E. Valenzuela, J. Dahlin, C. R. Rojas, and T. B. Schön. On robust input design for
nonlinear dynamical models. Automatica, 2016. (provisionally accepted).
T. L. Vincent, C. Novara, K. Hsu, and K. Poolla. Input design for structured nonlinear
system identiﬁcation. In Proceedings of the 15th IFAC Symposium on System Identiﬁcation,
pages 174–179, Saint-Malo, France, July 2009.

336
Paper H
Robust Input design for non-linear dynamical models
T. L. Vincent, C. Novara, K. Hsu, and K. Poolla. Input design for structured nonlinear
system identiﬁcation. Automatica, 46(6):990–998, 2010.
J.S. Welsh and C.R. Rojas. A scenario based approach to robust experiment design. In
Proceedings of the 15th IFAC Symposium on System Identiﬁcation, Saint-Malo, France, July
2009.
P. Whittle. Some general points in the theory of optimal experiment design. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 1:123–130, 1973.
A. Zaman. Stationarity on ﬁnite strings and shift register sequences. The Annals of Proba-
bility, 11(3):678–684, August 1983.

PhD Dissertations
Division of Automatic Control
Linköping University
M. Millnert: Identiﬁcation and control of systems subject to abrupt changes. Thesis No. 82, 1982.
ISBN 91-7372-542-0.
A. J. M. van Overbeek: On-line structure selection for the identiﬁcation of multivariable systems.
Thesis No. 86, 1982. ISBN 91-7372-586-2.
B. Bengtsson: On some control problems for queues. Thesis No. 87, 1982. ISBN 91-7372-593-5.
S. Ljung: Fast algorithms for integral equations and least squares identiﬁcation problems. Thesis
No. 93, 1983. ISBN 91-7372-641-9.
H. Jonson: A Newton method for solving non-linear optimal control problems with general con-
straints. Thesis No. 104, 1983. ISBN 91-7372-718-0.
E. Trulsson: Adaptive control based on explicit criterion minimization. Thesis No. 106, 1983. ISBN 91-
7372-728-8.
K. Nordström: Uncertainty, robustness and sensitivity reduction in the design of single input control
systems. Thesis No. 162, 1987. ISBN 91-7870-170-8.
B. Wahlberg: On the identiﬁcation and approximation of linear systems. Thesis No. 163, 1987.
ISBN 91-7870-175-9.
S. Gunnarsson: Frequency domain aspects of modeling and control in adaptive systems. Thesis
No. 194, 1988. ISBN 91-7870-380-8.
A. Isaksson: On system identiﬁcation in one and two dimensions with signal processing applications.
Thesis No. 196, 1988. ISBN 91-7870-383-2.
M. Viberg: Subspace ﬁtting concepts in sensor array processing. Thesis No. 217, 1989. ISBN 91-7870-
529-0.
K. Forsman: Constructive commutative algebra in nonlinear control theory. Thesis No. 261, 1991.
ISBN 91-7870-827-3.
F. Gustafsson: Estimation of discrete parameters in linear systems. Thesis No. 271, 1992. ISBN 91-
7870-876-1.
P. Nagy: Tools for knowledge-based signal processing with applications to system identiﬁcation.
Thesis No. 280, 1992. ISBN 91-7870-962-8.
T. Svensson: Mathematical tools and software for analysis and design of nonlinear control systems.
Thesis No. 285, 1992. ISBN 91-7870-989-X.
S. Andersson: On dimension reduction in sensor array signal processing. Thesis No. 290, 1992.
ISBN 91-7871-015-4.
H. Hjalmarsson: Aspects on incomplete modeling in system identiﬁcation. Thesis No. 298, 1993.
ISBN 91-7871-070-7.
I. Klein: Automatic synthesis of sequential control schemes. Thesis No. 305, 1993. ISBN 91-7871-090-1.
J.-E. Strömberg: A mode switching modelling philosophy. Thesis No. 353, 1994. ISBN 91-7871-430-3.
K. Wang Chen: Transformation and symbolic calculations in ﬁltering and control. Thesis No. 361,
1994. ISBN 91-7871-467-2.
T. McKelvey: Identiﬁcation of state-space models from time and frequency data. Thesis No. 380, 1995.
ISBN 91-7871-531-8.
J. Sjöberg: Non-linear system identiﬁcation with neural networks. Thesis No. 381, 1995. ISBN 91-
7871-534-2.
R. Germundsson: Symbolic systems – theory, computation and applications. Thesis No. 389, 1995.
ISBN 91-7871-578-4.
P. Pucar: Modeling and segmentation using multiple models. Thesis No. 405, 1995. ISBN 91-7871-627-
6.

H. Fortell: Algebraic approaches to normal forms and zero dynamics. Thesis No. 407, 1995. ISBN 91-
7871-629-2.
A. Helmersson: Methods for robust gain scheduling. Thesis No. 406, 1995. ISBN 91-7871-628-4.
P. Lindskog: Methods, algorithms and tools for system identiﬁcation based on prior knowledge.
Thesis No. 436, 1996. ISBN 91-7871-424-8.
J. Gunnarsson: Symbolic methods and tools for discrete event dynamic systems. Thesis No. 477,
1997. ISBN 91-7871-917-8.
M. Jirstrand: Constructive methods for inequality constraints in control. Thesis No. 527, 1998.
ISBN 91-7219-187-2.
U. Forssell: Closed-loop identiﬁcation: Methods, theory, and applications. Thesis No. 566, 1999.
ISBN 91-7219-432-4.
A. Stenman: Model on demand: Algorithms, analysis and applications. Thesis No. 571, 1999. ISBN 91-
7219-450-2.
N. Bergman: Recursive Bayesian estimation: Navigation and tracking applications. Thesis No. 579,
1999. ISBN 91-7219-473-1.
K. Edström: Switched bond graphs: Simulation and analysis. Thesis No. 586, 1999. ISBN 91-7219-493-
6.
M. Larsson: Behavioral and structural model based approaches to discrete diagnosis. Thesis No. 608,
1999. ISBN 91-7219-615-5.
F. Gunnarsson: Power control in cellular radio systems: Analysis, design and estimation. Thesis
No. 623, 2000. ISBN 91-7219-689-0.
V. Einarsson: Model checking methods for mode switching systems. Thesis No. 652, 2000. ISBN 91-
7219-836-2.
M. Norrlöf: Iterative learning control: Analysis, design, and experiments. Thesis No. 653, 2000.
ISBN 91-7219-837-0.
F. Tjärnström: Variance expressions and model reduction in system identiﬁcation. Thesis No. 730,
2002. ISBN 91-7373-253-2.
J. Löfberg: Minimax approaches to robust model predictive control. Thesis No. 812, 2003. ISBN 91-
7373-622-8.
J. Roll: Local and piecewise aﬃne approaches to system identiﬁcation. Thesis No. 802, 2003. ISBN 91-
7373-608-2.
J. Elbornsson: Analysis, estimation and compensation of mismatch eﬀects in A/D converters. Thesis
No. 811, 2003. ISBN 91-7373-621-X.
O. Härkegård: Backstepping and control allocation with applications to ﬂight control. Thesis
No. 820, 2003. ISBN 91-7373-647-3.
R. Wallin: Optimization algorithms for system analysis and identiﬁcation. Thesis No. 919, 2004.
ISBN 91-85297-19-4.
D. Lindgren: Projection methods for classiﬁcation and identiﬁcation. Thesis No. 915, 2005. ISBN 91-
85297-06-2.
R. Karlsson: Particle Filtering for Positioning and Tracking Applications. Thesis No. 924, 2005.
ISBN 91-85297-34-8.
J. Jansson: Collision Avoidance Theory with Applications to Automotive Collision Mitigation. The-
sis No. 950, 2005. ISBN 91-85299-45-6.
E. Geijer Lundin: Uplink Load in CDMA Cellular Radio Systems. Thesis No. 977, 2005. ISBN 91-
85457-49-3.
M. Enqvist: Linear Models of Nonlinear Systems. Thesis No. 985, 2005. ISBN 91-85457-64-7.
T. B. Schön: Estimation of Nonlinear Dynamic Systems — Theory and Applications. Thesis No. 998,
2006. ISBN 91-85497-03-7.

I. Lind: Regressor and Structure Selection — Uses of ANOVA in System Identiﬁcation. Thesis
No. 1012, 2006. ISBN 91-85523-98-4.
J. Gillberg: Frequency Domain Identiﬁcation of Continuous-Time Systems Reconstruction and
Robustness. Thesis No. 1031, 2006. ISBN 91-85523-34-8.
M. Gerdin: Identiﬁcation and Estimation for Models Described by Diﬀerential-Algebraic Equations.
Thesis No. 1046, 2006. ISBN 91-85643-87-4.
C. Grönwall: Ground Object Recognition using Laser Radar Data – Geometric Fitting, Performance
Analysis, and Applications. Thesis No. 1055, 2006. ISBN 91-85643-53-X.
A. Eidehall: Tracking and threat assessment for automotive collision avoidance. Thesis No. 1066,
2007. ISBN 91-85643-10-6.
F. Eng: Non-Uniform Sampling in Statistical Signal Processing. Thesis No. 1082, 2007. ISBN 978-91-
85715-49-7.
E. Wernholt: Multivariable Frequency-Domain Identiﬁcation of Industrial Robots. Thesis No. 1138,
2007. ISBN 978-91-85895-72-4.
D. Axehill: Integer Quadratic Programming for Control and Communication. Thesis No. 1158, 2008.
ISBN 978-91-85523-03-0.
G. Hendeby: Performance and Implementation Aspects of Nonlinear Filtering. Thesis No. 1161, 2008.
ISBN 978-91-7393-979-9.
J. Sjöberg: Optimal Control and Model Reduction of Nonlinear DAE Models. Thesis No. 1166, 2008.
ISBN 978-91-7393-964-5.
D. Törnqvist: Estimation and Detection with Applications to Navigation. Thesis No. 1216, 2008.
ISBN 978-91-7393-785-6.
P-J. Nordlund: Eﬃcient Estimation and Detection Methods for Airborne Applications. Thesis
No. 1231, 2008. ISBN 978-91-7393-720-7.
H. Tidefelt: Diﬀerential-algebraic equations and matrix-valued singular perturbation. Thesis No. 1292,
2009. ISBN 978-91-7393-479-4.
H. Ohlsson: Regularization for Sparseness and Smoothness — Applications in System Identiﬁcation
and Signal Processing. Thesis No. 1351, 2010. ISBN 978-91-7393-287-5.
S. Moberg: Modeling and Control of Flexible Manipulators. Thesis No. 1349, 2010. ISBN 978-91-
7393-289-9.
J. Wallén: Estimation-based iterative learning control. Thesis No. 1358, 2011. ISBN 978-91-7393-255-4.
J. Hol: Sensor Fusion and Calibration of Inertial Sensors, Vision, Ultra-Wideband and GPS. Thesis
No. 1368, 2011. ISBN 978-91-7393-197-7.
D. Ankelhed: On the Design of Low Order H-inﬁnity Controllers. Thesis No. 1371, 2011. ISBN 978-
91-7393-157-1.
C. Lundquist: Sensor Fusion for Automotive Applications. Thesis No. 1409, 2011. ISBN 978-91-7393-
023-9.
P. Skoglar: Tracking and Planning for Surveillance Applications. Thesis No. 1432, 2012. ISBN 978-
91-7519-941-2.
K. Granström: Extended target tracking using PHD ﬁlters. Thesis No. 1476, 2012. ISBN 978-91-7519-
796-8.
C. Lyzell: Structural Reformulations in System Identiﬁcation. Thesis No. 1475, 2012. ISBN 978-91-
7519-800-2.
J. Callmer: Autonomous Localization in Unknown Environments. Thesis No. 1520, 2013. ISBN 978-
91-7519-620-6.
D. Petersson: A Nonlinear Optimization Approach to H2-Optimal Modeling and Control. Thesis
No. 1528, 2013. ISBN 978-91-7519-567-4.

Z. Sjanic: Navigation and Mapping for Aerial Vehicles Based on Inertial and Imaging Sensors. Thesis
No. 1533, 2013. ISBN 978-91-7519-553-7.
F. Lindsten: Particle Filters and Markov Chains for Learning of Dynamical Systems. Thesis No. 1530,
2013. ISBN 978-91-7519-559-9.
P. Axelsson: Sensor Fusion and Control Applied to Industrial Manipulators. Thesis No. 1585, 2014.
ISBN 978-91-7519-368-7.
A. Carvalho Bittencourt: Modeling and Diagnosis of Friction and Wear in Industrial Robots. Thesis
No. 1617, 2014. ISBN 978-91-7519-251-2.
M. Skoglund: Inertial Navigation and Mapping for Autonomous Vehicles. Thesis No. 1623, 2014.
ISBN 978-91-7519-233-8.
S. Khoshfetrat Pakazad: Divide and Conquer: Distributed Optimization and Robustness Analysis.
Thesis No. 1676, 2015. ISBN 978-91-7519-050-1.
T. Ardeshiri: Analytical Approximations for Bayesian Inference. Thesis No. 1710, 2015. ISBN 978-91-
7685-930-8.
N. Wahlström: Modeling of Magnetic Fields and Extended Objects for Localization Applications.
Thesis No. 1723, 2015. ISBN 978-91-7685-903-2.

FACULTY OF SCIENCE AND ENGINEERING
Linköping studies in science and technology. Dissertations. No. 1754
Department of Electrical Engineering, Linköping University.
Linköping University
SE-581 83 Linköping, Sweden
www.liu.se

