AN IMPARTIAL TRANSFORMER FOR STORY VISUALIZATION
Nikolaos Tsakas
Maria Lymperaiou
Giorgos Filandrianos
Giorgos Stamou
National Technical University of Athens
ABSTRACT
Story Visualization is an advanced task of computed vision
that targets sequential image synthesis, where the generated
samples need to be realistic, faithful to their conditioning
and sequentially consistent. Our work proposes a novel ar-
chitectural and training approach: the Impartial Transformer
achieves both text-relevant plausible scenes and sequential
consistency utilizing as few trainable parameters as possi-
ble. This enhancement is even able to handle synthesis of
’hard’ samples with occluded objects, achieving improved
evaluation metrics comparing to past approaches.
Index Terms— Story Visualization, GANs, Transformers
1. INTRODUCTION
The emergence of GANs [1] has inspired several advance-
ments in image synthesis, one of the most prominent being
conditional image synthesis with the usage of cGANs [2].
Text-conditioned image generation has been a popular vari-
ant of the conditional case, displaying a long line of archi-
tectural exploration. Those topics stimulated the novel task
of Story Visualization (SV), where a visual story needs to be
generated conditioned on text or other semantic information.
The images need not only to correspond to their conditioning,
but also to remain consistent within the sequence, which re-
quires a global understanding of the story context. The basic
idea involves a GAN-based variant with one generator G and
two discriminators. The ﬁrst discriminator (image discrimi-
nator Dim) focuses on text-image relevance, while the other
one (story discriminator Dst) ensures the overall sequential
coherence. The same task can be viewed as a sequence trans-
duction problem, a task widely explored with the usage of
recurrent neural networks (RNNs) and Transformers [3].
So far, SV has only received a few improvements, while
it faces scarcity of viable datasets and evaluation methods. To
this end, we propose a reﬁned transformer-based approach,
where a simple and lightweight adjustment called Impartial
transformer is enough to resolve problems present in our pre-
decessors. A transformer encoder jointly trained from G and
Dim is employed to create an input representation, yielding
a resource-friendly scenario comparing to using separate en-
coders for each generative component or adding a plethora of
modules [4, 5] to achieve advanced results
2. RELATED WORK
Generative Adversarial Networks (GANs) [1] are able to
synthesize high-quality images by initially receiving random
noise z ∼pz in the input of G and are trained to gradually
improve the synthesized sample from receiving feedback re-
garding sample quality from D. Conditional GANs (cGANS)
also receive a conditioning vector y among with z to guide
synthesis towards certain areas of the target distribution. Ear-
lier works in conditional synthesis where y is in textual form
attempt to fully synthesize the ﬁnal image in one step, re-
sulting in samples lacking in ﬁdelity [6]. The ﬁrst signiﬁcant
improvements emerged with the introduction of StackGAN
[7] and its variants [8] which gradually upsample images up
to the ﬁnal resolution. Further implementations target detail
reﬁnement [9, 10] and improvements of text-image relevance
[11]. Proceeding to the sequential case, StoryGAN [12] intro-
duced the SV task utilizing RNNs for conditional encoding,
as well as the two-discriminator GAN architecture that later
variants follow [13, 14]. Only recently transformer-based ap-
proaches for conditional encoding emerged [4, 5] indicating
a new direction of research obeying to recent trends [3].
3. METHOD
We propose an updated framework for the SV task based on
the emergence of transformer-based techniques for sequence
processing.
Primarily, we recommend the use of a trans-
former encoder [3] as a replacement for the RNN structure
of StoryGAN [12], focusing on its optimal training regime.
Fig. 1: The generator G network (T = 4 frames)
arXiv:2301.03563v1  [cs.CV]  9 Jan 2023

3.1. Generator
The input to the generator G is a sequence of symbols st,
embedded by an encoder into vector representations φt, t ∈
[1, T] where T corresponds to the length of all stories. Fig. 1
depicts the basic G architecture.
We recommend using a conditioning augmentation (CA)
module, similar to [7]: Instead of conditioning the GAN on
an embedding of the input φt, a random vector ˆc is sampled
from a Gaussian distribution N(µ(φt, Σ(φt))) with the mean
µ(φt) and the diagonal covariance matrix Σ(φt) being func-
tions of the input embeddings. The vector ˆc serves as the
conditioning variable. CA promotes continuity in the data
manifold, and can be also used to map the dimension of φt to
its appropriate size. Training the parameters of this stochas-
tic process becomes possible using the reparametrization trick
[15], where a sample from a Gaussian distribution with arbi-
trary mean µ and covariance matrix σ can be produced as:
ˆc = µ + z ∗σ, where z ∼N(0, 1). In addition, to ensure the
smoothness of the manifold, the KL divergence between the
learned Gaussian distribution and the standard one is added
to the loss function of G as a regularization term, therefore
avoiding overﬁtting caused by collapsing to a single point or
by a distribution that deviates from the standard Gaussian [7]:
LossKL = DKL(N(µ(ϕt), Σ(ϕt))∥N(0, I))
The Transformer inputs ˆct are ﬁrst added to positional en-
codings to properly inﬂuence transduction, and then context-
aware conditioning vectors ct are produced from the position
encoded inputs. The context-informed vectors ct are concate-
nated with Gaussian noise zt ∼pz, where pz is the random
input prior z ∼N(0, 1). This combined input is fed through a
fully connected (FC) layer, mapping each instance to dimen-
sion C × H × W, where H, W are the height and width of
the initial image channels to be upsampled, and C their chan-
nel number. This output mapping is rearranged in a tensor
It ∈RC×H×W and fed through a set of residual upsampling
blocks, similar to [16]. The purpose of a residual block [17]
is to learn a mapping F(x) = H(x) −x where H(x) is the
actual desired mapping in the underlying distribution. The ﬁ-
nal output is produced utilizing a skip connection such that
ˆH(x) = F(x) + x. In each upsampling block, the input im-
age features It are normalized via Batch Normalization [18]
and passed through a ReLU activation. Then, both spatial di-
mensions are doubled via nearest-neighbor upsampling, and
a convolutional ﬁlter is applied to transform image features,
while halving the channel dimension to mitigate computa-
tional complexity as the image planes get larger. The tensor
is again normalized and passed through a ReLU activation as
well as a ﬁnal convolutional ﬁlter. In order to match the spa-
tial input and output dimensions we perform a minimal trans-
form on the skip connection, using nearest-neighbor upsam-
pling and passing through a learned 1 × 1 convolutional ﬁlter.
After feature upsampling to the desired dimension H × W, a
Fig. 2: Image discriminator Dim (T = 4 frames)
ﬁnal 3 × 3 convolution layer is used to produce a 3-channel
image, followed by a tanh activation to remap pixel values
into [−1, 1]. We also use Spectral Normalization to further
stabilize the training process. The entire image sequence can
be generated in parallel, greatly improving training efﬁciency.
3.2. Image Discriminator
The image discriminator Dim (Fig. 2) is tasked to discern
between real and generated images individually. To that end,
Dim utilizes the input features φt of each individual sentence
corresponding to a story frame, the context, and the image
It itself to be evaluated. The context is important for Dim,
because each frame in a story depends on the rest to form
many of its details. Each image to be evaluated is passed
through a series of residual downsampling blocks. Image fea-
tures from each layer are ﬁrst passed through a Leaky ReLU,
then from a spectrally normalized convolutional layer, remap-
ping the C × H × W tensor to double the channels. Af-
ter another Leaky ReLU, a spectrally normalized strided con-
volution layer downsamples the image features. We prefer
this option over a pooling layer due to the inferences made
by Radford et.
al in [19].
All images are evaluated in a
batch to take advantage of the Transformer’s parallel process-
ing. Dropout in all Dim residual blocks is proven beneﬁcial,
to prevent overﬁtting and overt coupling of individual layer
units. To produce an output scalar, each vector of dimension
dmodel given by the encoder is spatially replicated to create a
dmodel ×H ×W tensor that is then concatenated with the im-
age features along the channel axis. These features are passed
through a residual block to jointly learn from image and text
features. A ﬁnal FC layer mapping features to a single scalar
leads to a sigmoid activation function, ultimately producing a
probability Dim(It) ∈[0, 1].
3.3. Story Discriminator
The story discriminator Dst (Fig. 3) enforces consistency
and meaningful progression along the image sequence I =
(I1, ..., IT ) by jointly learning a common feature space for
text and images. The image features are downsampled using
similar residual blocks as in Dim. All image features for the

Fig. 3: Story discriminator Dst (T = 4 frames)
same story are concatenated into a single storyboard vector.
On the text side, a FC layer maps all sentence embeddings
S = (φ1, ..., φT ) to vectors in this shared space, also concate-
nated into one big text feature vector. The two story-wide vec-
tors are then multiplied elementwise and the result is passed
through a FC layer to output a scalar similarity score Dst.
3.4. Training
Training requires minimizing Lim, Lst, LG:
Lim =
T
X
t=1
(E(it,ϕt)[logDim(it, ϕt, h0; ψI)]+
E(zt,ϕt)[log(1 −Dim(G(zt, ϕt; θ), ϕt, h0; ψI))]),
Lst = E(I,S)[logDst(I, S; ψS)]+
Eϵ,S[log(1 −Dst([G(zt, ϕt; θ)]T
t=1), S; ψS))],
LG = E(zt,ϕt)[log(Dim(G(zt, ϕt; θ), ϕt, h0; ψI))]+
Eϵ,S[log(Dst([G(zt, ϕt; θ)]T
t=1), S; ψS))] + LossKL
where zt ∼pz, and h0 serves as story embedding. The alter-
native formulation following [1] is employed for G to provide
sufﬁcient gradients. We also use the matching aware discrim-
inator criterion as in [20]. One-sided label smoothing is uti-
lized by setting positive labels to 0.9 instead of 1.0 to avoid
the pitfalls of regular label smoothing [21].
4. EXPERIMENTS
We present results on CLEVR-SV [22], focusing on cases
where objects may not be clearly separated or even occluded.
This issue, despite its signiﬁcance, was not addressed in prior
work. For all experiments, Adam optimizer [23] is used for
gradient descent with β1 = 0.5 and β2 = 0.999. After exten-
sive hyperparameter tuning we present results on the original
Transformer with dmodel = 512, Nheads = 8, Nlayers = 6.
4.1. Impartial Transformer Encoder
We explore the option of utilizing one Impartial transformer
encoder, whose parameters are updated jointly by G and
Dim. We hypothesize such an encoder would learn a task-
conducive representation for embedding sequences by simply
encoding necessary context without giving an advantage to
either adversary. We further attempted to train the encoder to
also receive gradients from the Dst, but found this addition to
be confusing the encoder, to the point of learning completely
mismatched representations of the context space.
4.2. Learning rate schemes
Motivated by the Two Time-scale Update Rule [24], we at-
tempt to ﬁnd an optimal learning rate scheme for the three
networks while maintaining a 1/1/1 update ratio for more ef-
ﬁcient training, thus proposing a Three Time-scale Update
Rule. After 20 epochs, the learning rates are halved based on
a typical scheduling scheme. We observe that when G learns
faster than the discriminators, the whole model suffers from
mode collapse: G easily fools both discriminators early on,
leading training to a stalemate since the discriminators can-
not produce any meaningful gradients to guide generation.
When maintaining a low learning rate for G, increasing the
Dim learning rate proves to lead G into creating images that
correspond better to the conditioning. G is faster in learning
the correct matching for color and shape between image and
description vector, as well as learning to produce more con-
crete shape features, at least for large objects. When increas-
ing the learning rate of Dst, we immediately observe greater
consistency across images. Lower learning rates also seem
to affect text-image matching, with G creating images with
wrong color, shape and size more frequently. We thus argue
that it is beneﬁcial for the two discriminators to learn about 4
times as fast as G. Speciﬁcally, we ﬁnd lrG = 0.0001, lrDim
= 0.0004, lrDst = 0.0004 to be optimal, as higher learning
rates proved to be too fast for convergence.
4.3. Warmup Scheduler
We experiment with decaying the learning rate by halving it
every 20 epochs. The original Transformer [3] recommends
a speciﬁc learning rate scheduling scheme to be used along
with the Adam optimizer: The learning rate should ﬁrst be
increased linearly for a number of warmup steps and then de-
creased proportionally to the inverse square root of the num-
ber of total steps, where one step is considered to be a sin-
gle batch of data passing through the network. We observe
that the scheduler fails to train the context encoder, result-
ing in mostly nonsensical representations. We presume this is
because the recommended optimizer only takes into account
dmodel and the number of warmup steps, forcing the learning
rate to generally remain much higher than what the learning
rates of the Adam optimizer in regular decay are, preventing
network from convergence.
4.4. Results
Visual results including ablations are presented in Fig 4, while
comparison over easy and hard examples are presented in Fig.
5. There is an obvious improvement over StoryGAN [12],
which fails to generate the proper sequence, and also lacks in

(a) Left: Ground truth (T=4). Middle: StoryGAN generated frames, low relevance and object quality. Right: Ours, baseline.
(b) Our results without attention. Left: Separate Transformer Encoder for G, Dim, Dst, low object relevance. Middle: Impartial
Encoder (G and Dim gradients). Right: Impartial encoder (all G, Dim, Dst gradients), mode collapse.
Fig. 4: Ablation studies of our framework indicate the power of the Impartial Transformer (G and Dim gradients).
Fig. 5: (a) 1st row ground truth, (b) 2nd row generated frames (ours-Impartial Transformer), (c) 3rd row generated frames
(storyGAN) of 3 stories with T=4. From left to right (every 4 images) difﬁculty of stories increases due to object occlusion.
ﬁdelity. The second row of Fig. 4 indicates the optimal usage
of the Impartial transformer. Even though our implementa-
tion presents satisfactory results when objects are placed in a
distance from each other (Fig 5, left), in cases when objects
are adjacent or overlap, there are some sacriﬁces to be made:
either semantics -especially shape and material- are not dis-
tinct enough (Fig 5, middle), or objects are ’swallowed’ by
their neighbors (Fig 5, right), which results in low quality se-
mantics. The results of human evaluation experiments over
preference are presented in Table 1. Results using automated
metrics are presented in Table 2. Our framework clearly out-
performs prior efforts [12, 4, 5] according to Clean-FID [25],
LPIPS [26] and SSIM. We mainly focus on LPIPS metric for
comparison that reﬂects human perception, where we achieve
16% improvement over prior approaches [12, 4, 5].
Table 1: Human Evaluation preference (averaged results),
Win% = % times our output stories were preferred over [12],
Lose% for vice-versa, Tie% when equally preferred.
Attribute
Win%
Loose%
Tie%
Visual Quality
25
20
55
Consistency
37
32
31
Relevance
32
30
38
Table 2: Average evaluation metrics.
Frame FID↓
Clean-
FID↓
LPIPS↓
SSIM↑
1st
32.94 ± 7.85
111.20
0.18 ± 0.06
0.81
2nd
37.41 ± 6.67
110.80
0.19 ± 0.05
0.73
3rd
47.41 ± 15.83
106.69
0.23 ± 0.05
0.68
4th
48.41 ± 3.84
133.15
0.25 ± 0.05
0.62
All
41.54 ± 8.55
115.46
0.21 ± 0.05
0.71
[12]
41.45 ± 6.25
123.40
0.25 ± 0.03
0.65
[5]
41.96 ± 9.66
124.97
0.25 ± 0.08
0.67
[4]
41.80 ± 8.81
122.62
0.25 ± 0.05
0.68
’All’ refers to global results of the Impartial Transformer and is compare
with the global results of [12], [5], [4]. Results from [5], [4] are obtained by
re-training on CLEVR-SV.
5. CONCLUSION
In this work, we developed a transformer-inspired framework
for story visualization, aiming to set a new baseline in litera-
ture by achieving improvements according to perceptual met-
rics. The usage of the Impartial Transformer demonstrated
promising directions for the evolution of generative models
in the same track, as few -if any- current implementations ex-
ploit a ’forking’ module jointly trained by two adversaries.
As future work we plan to explore the evaluation part of SV.

6. REFERENCES
[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio, “Generative adversarial
nets,” in NeurIPS, 2014.
[2] Augustus Odena,
Christopher Olah,
and Jonathon
Shlens,
“Conditional image synthesis with auxiliary
classiﬁer gans,” 2017.
[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin, “Attention is all you need,”
in NeurIPS, 2017.
[4] Adyasha Maharana, Darryl Hannan, and Mohit Bansal,
“Improving generation and evaluation of visual stories
via semantic consistency,” ArXiv, vol. abs/2105.10026,
2021.
[5] Adyasha Maharana and Mohit Bansal, “Integrating vi-
suospatial, linguistic, and commonsense structure into
story visualization,” ArXiv, vol. abs/2110.10834, 2021.
[6] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen
Logeswaran, Bernt Schiele, and Honglak Lee, “Gener-
ative adversarial text to image synthesis,” CoRR, vol.
abs/1605.05396, 2016.
[7] Han Zhang, Tao Xu, and Hongsheng Li,
“Stackgan:
Text to photo-realistic image synthesis with stacked gen-
erative adversarial networks,” in ICCV, 2017.
[8] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang,
Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas,
“Stackgan++: Realistic image synthesis with stacked
generative adversarial networks,” 2018.
[9] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han
Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He, “At-
tngan: Fine-grained text to image generation with atten-
tional generative adversarial networks,” in CVPR 2018.
[10] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang,
“DM-GAN: dynamic memory generative adversarial
networks for text-to-image synthesis,”
CoRR, vol.
abs/1904.01310, 2019.
[11] Hongchen Tan, Xiuping Liu, Xin Li, Yi Zhang, and Bao-
cai Yin, “Semantics-enhanced adversarial nets for text-
to-image synthesis,” in ICCV, 2019.
[12] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu,
Yu Cheng, Yuexin Wu, Lawrence Carin, David Edwin
Carlson, and Jianfeng Gao,
“Storygan: A sequential
conditional gan for story visualization,” CVPR, 2019.
[13] Gangyan Zeng, Zhaohui Li, and Yuan Zhang, “Pororo-
gan: An improved story visualization model on pororo-
sv dataset,” CSAI2019, 2019, ACM.
[14] Chunye Li, Liya Kong, and Zhiping Zhou, “Improved-
storygan for sequential images visualization,” Journal
of Visual Communication and Image Representation,
2020.
[15] Diederik P Kingma and Max Welling, “Auto-encoding
variational bayes,” 2014.
[16] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Au-
gustus Odena,
“Self-attention generative adversarial
networks,” 2018.
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
CVPR, 2016.
[18] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” 2015.
[19] Alec Radford, Luke Metz, and Soumith Chintala, “Un-
supervised representation learning with deep convolu-
tional generative adversarial networks,” 2016.
[20] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee, “Generative
adversarial text to image synthesis,” in ICML, 2016.
[21] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna,
“Rethinking
the inception architecture for computer vision,” CVPR,
2016.
[22] Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B.
Girshick,
“Clevr:
A diagnostic dataset for com-
positional language and elementary visual reasoning,”
CVPR, 2017.
[23] Diederik Kingma and Jimmy Ba, “Adam: A method for
stochastic optimization,” ICLR, 2014.
[24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter, “Gans trained
by a two time-scale update rule converge to a local nash
equilibrium,” 2017.
[25] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu, “On
aliased resizing and surprising subtleties in gan evalua-
tion,” in CVPR, 2022.
[26] Richard Zhang, Phillip Isola, Alexei A Efros, Eli
Shechtman, and Oliver Wang, “The unreasonable ef-
fectiveness of deep features as a perceptual metric,” in
CVPR, 2018.

