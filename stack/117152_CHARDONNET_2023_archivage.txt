THESE DE DOCTORAT
NNT : 2023UPASG005
Towards a Curry-Howard Correspondence
for Quantum Computation
Vers une Correspondance de Curry-Howard pour
le Calcul Quantique
Thèse de doctorat de l’université Paris-Saclay
École doctorale n◦580 : sciences et technologies de l’information et de
la communication (STIC)
Spécialité de doctorat: Informatique
Graduate School : Informatique et sciences du numérique, Référent :
Faculté des sciences d’Orsay
Thèse préparée dans l’unité de recherche Laboratoire Méthodes Formelles
(Université Paris-Saclay, CNRS, ENS Paris-Saclay), sous la direction de Pablo
Arrighi, le co-encadrement d’Alexis Saurin et le co-encadrement de Benoît
Valiron.
Thèse soutenue à Paris-Saclay, le 9 janvier 2023, par
Kostia CHARDONNET
Composition du jury
Membres du jury avec voix délibérative
Delia KESNER
Présidente
Professeure à l’Université Paris Cité
Emmanuel JEANDEL
Rapporteur
Professeur à l’Université de Lorraine
Laurent RÉGNIER
Rapporteur & Examinateur
Professeur à l’Université d’Aix-Marseille
Miriam BACKENS
Examinatrice
Maître de Conférence à l’Université de Birming-
ham
Pierre CLAIRAMBAULT
Examinatreur
Chargé de recherche CNRS à l’Université d’Aix-
Marseille
Claudia FAGGIAN
Examinatrice
Chargée de recherche CNRS à l’Université Paris
Cité

Titre: Vers une Correspondance de Curry-Howard pour le Calcul Quantique
Mots clés: Calcul Quantique, Logique linéaire, Langage de programmation, Géométrie de l’Interaction
Résumé:Dans cette thèse, nous nous intéressons
au développement d’une correspondance de Curry-
Howard pour l’informatique quantique, permettant
de représenter des types quantiques et le ﬂot de
contrôle quantique. Dans le modèle standard de
l’informatique quantique, un ordinateur classique
est lié à un coprocesseur quantique. L’ordinateur
classique peut alors envoyer des instructions pour
allouer, mettre à jour ou lire des registres quan-
tiques.
Les programmes exécutés par le copro-
cesseur sont représentés par un circuit quantique
:
une séquence d’instructions qui applique des
opérations unitaires aux registres quantiques. Bien
que le modèle soit universel, dans le sens où il peut
représenter n’importe quelle opérations unitaire, il
reste limité : il lui manque une représentation cor-
recte du ﬂot d’exécution non causal.
Normale-
ment, pour représenter le branchement, on peut
utiliser un système de type contenant un copro-
duit, permettant le choix entre deux exécutions
possibles, mais les circuits quantiques ne contien-
nent que des qubits et leurs tenseurs. D’autre part,
les types sont fortement liés à la logique à travers
la correspondance Curry-Howard qui stipule que
les types de programmes correspondent aux for-
mules et les programmes aux preuves, tandis que
l’évaluation du programme correspond à la simpliﬁ-
cation de la preuve correspondante. Bien que cette
correspondance ait été étendue à des cas multi-
ples en informatique classique, elle n’a pas encore
émergée dans l’informatique quantique.
Pour résoudre ces problèmes, nous suivons
deux approches diﬀérentes : la première, par le
développement d’un langage de programmation
linéaire et réversible, capturant un sous-ensemble
de l’informatique quantique, ainsi qu’une corre-
spondance Curry-Howard avec la logique µMALL.
Le langage existe en deux versions : l’une représen-
tant des fonctions réversibles et totales, tandis que
l’autre peut représenter des fonctions partielles.
Les deux versions sont accompagnées d’un résul-
tat d’expressivité : dans la première, nous pouvons
capturer l’ensemble des fonctions primitive récur-
sives, tandis que dans la deuxième, nous montrons
comment capturer n’importe quelle Machine de
Turing. La deuxième approche suit le développe-
ment d’une sémantique à base de jetons, inspirée
de la géométrie de l’interaction de Girard, pour
des langages graphique pour le calcul quantique.
Dans cette approche, une sémantique à base de je-
tons a été donnée pour le ZX-Calcul : un langage
graphique pour l’informatique quantique capable
de représenter n’importe quel opérateur linéaire.
Nous montrons comment cette nouvelle séman-
tique correspond à la sémantique dénotationnelle
standard.
Nous étendons ensuite le ZX-Calcul
avec un coproduit et un tenseur explicite dans le
développement du Many-Worlds Calcul. Ce nou-
veau langage est accompagné sémantique à base
de jetons et une théorie équationnelle. Nous mon-
trons comment le contrôle quantique peut être
représenté dans ce système.
Enﬁn, le langage
de programmation est modiﬁé dans le cas quan-
tique pur et nous utilisons le Many-Worlds Calculus
comme un modèle dénotationnel pour ce nouveau
langage de programmation.

Title: Towards a Curry-Howard Correspondence for Quantum Computation
Keywords: Quantum Computation, Linear Logic, Programming Languages, Geometry of Interaction
Abstract:
In this thesis, we are interested in the develop-
ment of a Curry-Howard correspondence for quan-
tum computing, allowing to represent quantum
types and quantum control-ﬂow. In the standard
model of Quantum Computation, a classical com-
puter is linked to a quantum coprocessor.
The
classical computer can then send instructions to
allocate, update, or read quantum registers. The
programs executed by the coprocessor are repre-
sented by a quantum circuit: a sequence of in-
structions that applies unitary operations to the
input quantum bits. While the model is universal,
in the sense that it can represent any unitary op-
erations, it stays limited: it lacks of proper repre-
sentation of non-causal execution ﬂow. Normally,
to represent branching, one can use a type sys-
tem featuring a coproduct, allowing for the choice
between two possible executions, but quantum cir-
cuits only feature qubits and tensors thereof. On
the other hand, types and strongly related to logic
via the Curry-Howard correspondence which states
that types of programs correspond to formulas and
programs to proofs, while the program evaluation
is matched with the proof simpliﬁcation.
While
this correspondence has been extended to multiple
setting in classical computer, it has yet to emerge
in quantum computing.
To address those problems we follow two dif-
ferent approaches: the ﬁrst one, through the de-
velopment of a linear and reversible programming
language, capturing a subset of quantum comput-
ing, along with a Curry-Howard correspondence
with the logic µMALL. The language comes in
two versions: one representing exactly complete,
reversible functions while the other one can repre-
sent partial functions. Both version comes with an
expressivity result: in the former, one can capture
the whole class of Primitive Recursive Functions,
while in the later any Turing Machine we show
how to capture any Turing Machine. The second
approach follows the development of token-based
semantics, inspired by Girard’s Geometry of Inter-
action, for graphical language for quantum compu-
tation. In this approach, a token-based semantics
was given for the ZX-Calculus: a graphical lan-
guage for quantum computation capable of rep-
resenting any linear operators. We show how the
token-based semantics matches the denotational
one. We extend the ZX-Calculus with a coprod-
uct and an explicit tensor in the development of
the Many-Worlds Calculus.
This new languages
comes with a token-based semantics and an equa-
tional theory. We show how quantum control can
be represented in this system.
Finally, the pro-
gramming language is modiﬁed to be able to real-
ize pure quantum computation and the graphical
language is then used as a denotational model for
it.
3


Contents
Introduction
9
Introduction (fr)
15
I
Background
21
1
Reversible & Quantum Computation
23
1.1
Reversible Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.1.1
Reversible Primitive Permutations
. . . . . . . . . . . . . . . . . . . . . . . . . .
24
1.1.2
Reversible Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.2
Quantum Computation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2
Graphical Language for Quantum Computation
31
2.1
The ZX-Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.1.1
Pure Operators
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
2.1.2
Standard Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.1.3
Properties and structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.1.4
ZX-diagrams for Mixed Processes . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3
Proof Theory & The Curry Howard Isomorphism
39
3.1
Logic & Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.1.1
The Simply-Typed λ-Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.1.2
Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.1.3
Curry-Howard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.2
Linear Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.2.1
Proof Nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2.2
Geometry of Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.3
Linear Logic & Quantum Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.4
Inﬁnitary Linear Logic : µMALL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.4.1
Background on µMALL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.4.2
Bouncing Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.4.3
Circular Representation of Derivations
. . . . . . . . . . . . . . . . . . . . . . . .
56
II
Contributions
57
4
A Curry-Howard Correspondence for
Linear, Reversible Computation
59
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
5

4.2
First-order Isos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.3
Computational Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.3.1
From RPP to Isos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.4
Proof Theoretical Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.4.1
Translating isos into µMALL . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.4.2
Pre-Proof Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.4.3
Proof Simulation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.5
Removing Exhaustivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.5.1
Encoding of Reversible Turing Machines . . . . . . . . . . . . . . . . . . . . . . .
91
4.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5
Geometry of Interaction for ZX Calculus
99
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5.2
Notions of Graph Theory in ZX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
5.3
A Token Machine for ZX-diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
5.3.1
Diﬀusion and Collision Rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
5.3.2
Strong Normalization and Conﬂuence . . . . . . . . . . . . . . . . . . . . . . . . .
105
5.3.3
Semantics and Structure of Normal Forms . . . . . . . . . . . . . . . . . . . . . .
111
5.3.4
Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
5.4
Extension to Mixed Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
5.4.1
Token Machine for Mixed Processes
. . . . . . . . . . . . . . . . . . . . . . . . .
118
5.5
Variations of the Token Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
5.5.1
Pulse Rewriting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
5.5.2
Synchronicity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
5.6
Sum Over Path Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
5.6.1
SOP Token Machine for Pure Operators . . . . . . . . . . . . . . . . . . . . . . .
122
5.6.2
SOP Token Machine for Mixed Processes
. . . . . . . . . . . . . . . . . . . . . .
127
5.7
Conclusion and Future Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
6
Many Worlds Calculus
131
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
6.2
The Many-Worlds Calculus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
6.2.1
A First Graphical Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
6.3
The Token Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
6.3.1
Tokens and Token States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
6.3.2
Token Machine’s Pulse Rewrite Strategy . . . . . . . . . . . . . . . . . . . . . . .
136
6.3.3
Token Machine’s Asynchronous Rewriting
. . . . . . . . . . . . . . . . . . . . . .
143
6.3.4
Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
6.4
Worlds labelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
6.5
The Denotational Semantics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
6.6
The Equational Theory
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
6.7
Induced Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
6.8
Comparison with Other Graphical Languages . . . . . . . . . . . . . . . . . . . . . . . . .
160
6

6.8.1
ZX-Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
6.8.2
Tensor-Sum Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
6.8.3
PBS-Calculus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
6.8.4
LOv-Calculus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
6.8.5
Path-Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
6.8.6
Linear Logic’s Proof Nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
6.9
Representing Computation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
6.9.1
Syntax of the Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
6.9.2
Encoding into the Many-Worlds . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
6.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
Conclusion
171
Bibliography
175
7


Introduction
Quantum Computation. In quantum computing, one has access to a new kind
of data: quantum bits (qubits), which consists in superposition of the classical
bits 0 and 1. The use of quantum bits has allowed for the development of new
algorithms enjoying an exponential speedup compared to their classical counter-
part. The most prominent examples being Grover’s algorithm, allowing to search
for an element in a list of n elements in O(
√
N) [Gro96] and its direct application
in Shor’s algorithm, allowing for decomposing a number into its prime factors
exponentially faster than known classical algorithms [Sho99]. These algorithms
are written using the Quantum Memory (QRAM) model. In this model, a classical
computer is linked to a quantum coprocessor. While the classical computer has
access to the full expressiveness of types systems and known programming methods,
the quantum coprocessor is only able to execute quantum circuits, the quantum
counterparts of boolean circuits, on some input qubits. A quantum circuit consists
in a sequence of unitary operations (called gates) which update the states of the
input qubits, ﬂowing from the input wires of the circuits, through the output wires,
traversing the gates as they move. Then, the result of the execution of the quantum
circuit is sent back to the classical computer after the measurement.
From a semantical perspective, the state of a quantum circuit consisting of
n quantum bits is a vector in a 2n-dimensional Hilbert space. A (pure) quantum
circuit is a linear, sequential description of elementary operations describing a linear,
unitary map on the state space.
On a formal aspect, quantum circuits have a very rigid structure, allowing for
little abstract reasoning on the execution of the circuit: most gates are matrices
and in order to prove any property, one has to realize matrices computation, al-
though some formalisms mitigate this diﬃculty [Amy18]. Graphical languages for
quantum computing has been introduced as a way to solve this problem. Coming
all the way from Feyman’s diagrams [FH65], graphical languages are commonly
used for representing quantum processes. Whether directly based on quantum cir-
cuits [Gre+13; Dal17; PRZ17; Cha+] or stemming from categorical analysis such
as the ZX-calculus [CK17; CD11], these formal languages are still tied to the quan-
tum coprocessor model in the sense that the only monoidal structure that can be
applied to quantum information is the (multiplicative) Kronecker product. Another
approach for abstract reasoning on quantum programs would be the development
of typed quantum programming languages. In this setting, types could help us
reason about properties of quantum programs. Types systems have been success-
fully used in classical computing to reason about programs, in particular through
the Curry-Howard Correspondence [Cur34] which states that types of programs
correspond to formulas and programs to proofs, while the program evaluation is
9

matched with the proof simpliﬁcation. This correspondence has been used to mirror
ﬁrst and second-order logics with dependent-type systems [BC13; Ler09], separa-
tion logics with memory-aware type systems [Rey02; Jun+17], resource-sensitive
logics with diﬀerential privacy [Gab+13], logics with monads with reasoning on
side eﬀects [Swa+16; Mai+19], classical logic [Gri89], in the development of rich
typed programming languages [Nor07; OSG08]. Such a correspondence has yet to
emerge in the quantum setting, even through some progress has been made [DD22;
DCM22; SVV18], albeit with limited type structures.
Quantum Control Flow. One peculiar feature of quantum computation is non-
causal execution paths. Indeed, the Janus-faced quantum computational paradigm
features two seemingly distinct notions of control structure. On the one hand,
a quantum program follows classical control: it is hosted on the conventional
computer governing the coprocessor, and can therefore only enjoy loops, tests
and other regular causally ordered sequences of operations. On the other hand,
the lab bench turns out to be more ﬂexible than the rigid coprocessor model,
permitting more elaborate purely quantum computational constructs than what
quantum circuits or ZX-calculus allow.
The archetypal example of a quantum computational behaviour hardly attain-
able within quantum circuits or ZX-calculus is the Quantum Switch [Chi+13].
Consider two quantum bits x and y and two unitary operations U and V acting
on y. The problem consists in generating the operation that performs UV on y if
x is in state |0⟩and V U if it is in state |1⟩.
QSwitch(x, U, V ) =
(
U
V
if x = |0⟩
V
U
if x = |1⟩
But, as x can be in superposition, in general the operation is then:
(α |0⟩+ β |1⟩) ⊗|y⟩7→α |0⟩⊗(UV |y⟩) + β |1⟩⊗(V U |y⟩).
It is a purely quantum test: not only can we have values in superposition (here,
x) but also execution orders. This is in sharp contrast with what happens within
the standard quantum coprocessor model. While this program has been shown to
be impossible to implement in a quantum circuit with only one instance of U and
V [Chi+13], it is nonetheless physically implementable [Abb+20].
While most quantum programming languages are still linked to the QRAM
model and features only classical tests and loops [Sin+22; Gre+13], computa-
tional models supporting superposition of execution orders have been studied in
the literature, such as proposing a suitable extension of quantum circuits [CDP08;
Por+17; VKB21; Wec+21].
These approaches typically aim at discussing the
notion of quantum channel from a quantum information theoretical standpoint.
Quantum Types.
Both the QRAM model and the ZX-Calculus only considered
tensors of qubits as the types of their input and output data. They do not natively
10

(A ⊗B) ⊕C
(A′ ⊗B′) ⊕C′
C
C′
⊕
⊕
f
g
h
A
B
A′
B′
⊗
⊗
Figure 1: Splits over coproduct and tensor
feature coproduct, which could be used for tests, nor richer types structures such
as inductive or coinductive types.
The presence of a coproduct could ease the representation of tests, allowing
for diﬀerent execution paths to be considered.
This is the approach followed
in the PBS-Calculus [CP20], albeit the PBS-Calculus does not feature a tensor:
their parallel composition corresponds to the superposition of positions of a single
qubit, while in other languages (such as the ZX-Calculus), the parallel composition
corresponds to a tensor of qubits.
Typed quantum programming languages featuring both a tensor and a coprod-
uct have been developed [SVV18; VRH22] albeit in these works, no relation with a
formal proof system has been established yet. Relating a pure quantum type sys-
tem with a logical system has been considered in [DD22] in the style of a natural
deduction logic through the use of a new connective ⊙allowing for the superposi-
tion of data, with a studied semantics for a fragment of the language, based on the
category of S-semimodule, for S a cancellative commutative semiring [DCM22].
However, their logic is limited to the intuitionistic fragment of linear logic without
tensors and the question of quantum control is not discussed.
Graphical languages featuring both a tensor and a coproduct are based on the
notion of tapes, or sheets [Dun09; Mel14]. The coproduct is able to separate a
tape in two, making the two part impossible to communicate with one another
until they are merged back together, while the tensor is simply the pair of a data,
as shown in Figure 1. However, these formalisms are not inherently quantum: the
splitting and merging of tapes have to be done in a well-bracketed manners making
it impossible to represent quantum superposition. From a semantics point of view,
pure quantum computation with coproduct and tensors have been studied [DCM22;
Cha+22], but it is not clear about inductive types and recursion would ﬁt in those
settings and is, so far, the question is left open.
Branching of execution paths can be used to determine whenever a program
should terminate or not.
The notion of recursion and inductive types in pure
quantum computing is yet again lacking: in order to consider recursion, one must
make sure that each step of the computation is indeed reversible and that the
recursive program is always well-deﬁned, as the whole operation has to be a unitary
operation. In [SVV18] the authors develop a pure quantum programming language
with the product and coproduct and the inductive type of lists and with strong
constraints ensuring termination and reversibility, which opens the door to further
11

generalization and the development of a proper logic.
Contributions. We try to answer the problem of ﬁnding a suitable computation
model for pure quantum computation with rich datatype, along with a Curry-
Howard correspondence. We oﬀer two approaches:
• The ﬁrst one, based on a pure quantum programming languages in the style
of Theseus [JS12; JS14; SVV18] where we present a linear and reversible
programming language with inductive types, together with a Curry-Howard
correspondence with the logic µMALL: linear logic with least and greatest
ﬁx points. While [SVV18] extend the language to the quantum case, with
the limited type of lists, we stay in the purely classical case. The extension
for generalized pure quantum inductive type is left as future work.
• And then using graphical languages. First, we deﬁne a new semantics for the
ZX-Calculus, based on a token-machine inspired from Girard’s Geometry of
Interaction, we show how this new semantics capture the usual denotational
semantics of the ZX-Calculus. Then, we develop a new graphical language in
the style of linear logic proof nets equipped with both a tensor, allowing for
handling multiple pieces of information together, and a coproduct, allowing
for branching depending on the given input.
We develop a token-based
semantics on this new language, and then a denotational semantics with an
equational theory that is sound and complete.
Plan of the thesis.
The thesis is organized as follows: Part I is focused on
the mathematical background needed for this thesis. It is split into two parts,
Chapter 1 introduce the quantum-theorical background needed, while Chapter 3
introduce the proof-theorical formalism of linear logic and µMALL. Then, Part II
focus on my personal contributions. In Chapter 4 we present the linear, reversible
language based on pattern-matching. The language features constraints forcing
any well-typed function (called iso) to be reversible, and hence an isomorphism.
We then show (i) that any primitive recursive function can be encoded as an iso of
the language, (ii) how any iso represent a proof-isomorphism in the logic µMALL,
(iii) that by relaxing the constraints on the iso to be able to consider partial maps,
the language is Turing Complete. This chapter is a ﬁrst step towards pure quantum
types and quantum control.
Chapter 5 is focused around deﬁning a new kind of semantics for the ZX-
Calculus : a graphical language for quantum computation.
The semantics is
inspired from the token-based geometry of interaction of linear logic, in which
tokens move around the graph, capturing the computational content of the graph.
We show (i) how the token machine, under some invariants, exactly captures the
denotational semantics of the ZX-Diagram, (ii) how it can be adapted to the ZX-
Calculus with mixed states, allowing to represent measurement, and (iii) how it
can be adapted to another kind of semantics based on sum-over-paths. The goal
12

of this chapter is to see how token-based semantics could be adapted to the case
of graphical languages. While some previous works have already be done in the
quantum case [Dal17], it was still on quantum circuits.
Finally, in Chapter 6 we present a new graphical language called the Many-
Worlds Calculus, featuring both products and coproducts. The language allows one
to encode all of ZX-Calculus with more primitive constructions and richer types.
We give (i) a token-based semantics in the style of Chapter 5, (ii) a denotational
semantics deduced from the token machine and (iii) a equational theory that we
prove sound and complete with respect to the denotational semantics. Finally, we
show how, by adapting a restriction of the language from Chapter 4 to the quantum
case, the Many-Worlds Calculus can serve as a model for such a language and as
a case study we show how to encode the Quantum Switch into the Many-Worlds
with only one occurrence of U and V .
13


Introduction (fr)
Informatique Quantique.
En informatique quantique, nous avons accès à un
nouveau type de données : les bits quantique (qubits), qui consistent en la su-
perposition des bits classiques 0 et 1. L’utilisation de bits quantiques a permis
le développement de nouveaux algorithmes bénéﬁciant d’une vitesse exponentielle
par rapport à leurs homologues classiques. Les exemples les plus marquants étant
l’algorithme de Grover, permettant de rechercher un élément dans une liste de
n éléments en O(
√
N) [Gro96] et son application directe dans l’algorithme de
Shor, permettant de décomposer un nombre en ses facteurs premiers exponen-
tiellement plus rapidement que les algorithmes classiques connus [Sho99].
Ces
algorithmes sont écrits en utilisant le modèle de mémoire quantique (QRAM).
Dans ce modèle, un ordinateur classique est relié à un coprocesseur quantique.
Alors que l’ordinateur classique a accès à l’expressivité complète des systèmes de
types et aux méthodes de programmation connues, le coprocesseur quantique est
seulement capable d’exécuter des circuits quantiques, l’homologue quantiques des
circuits booléens, sur certains qubits d’entrée. Un circuit quantique consiste en
une séquence d’opérations unitaires (appelées portes) qui mettent à jour les les
états des qubits d’entrée, lorsqu’ils traversent les portes quantiques. Le résultat de
l’exécution du circuit quantique est ensuite renvoyé à l’ordinateur classique après
avoir eﬀectué une mesure.
D’un point de vue sémantique, les états d’une mémoire quantique constituée
de n bits quantiques est un vecteur dans un espace de Hilbert de 2n dimensions.
Un circuit quantique est alors une description linéaire et séquentielle d’opérations
élémentaires décrivant une application linéaire, unitaire sur l’espace d’état.
D’un point de vue formel, les circuits quantiques ont une structure très rigide,
ne permettant que peu de raisonnement abstrait sur l’exécution du circuit : la
plupart des portes sont des matrices et pour prouver une quelconque propriété il
faut réaliser des calculs matriciels, bien que certains formalismes atténuent cette
diﬃculté [Amy18]. Les langages graphiques pour l’informatique quantique ont été
présentés comme un moyen de résoudre ce problème. Venant tout droit des di-
agrammes de Feyman [FH65], les langages graphiques sont couramment utilisés
pour représenter les processus quantiques. Qu’ils soient directement basés sur les
circuits [Gre+13; Dal17; PRZ17; Cha+] ou issus de l’étude catégorique comme
le ZX-calcul [CK17; CD11], néanmoins, ces langages formels sont toujours liés au
modèle de coprocesseur quantique dans le sens où la seule structure monoïdale
qui peut être appliquée aux qubits est le produit de Kronecker (multiplicatif). Une
autre approche pour le raisonnement abstrait sur les programmes quantiques serait
le développement de langages de programmation quantiques typés. Dans ce con-
texte, les types pourraient nous aider à raisonner sur les propriétés des programmes
15

quantiques. Les systèmes de types ont été utilisés avec succès en informatique clas-
sique pour raisonner sur les programmes, en particulier à travers la correspondance
de Curry-Howard [Cur34] qui stipule que les types de programmes correspondent
aux formules logique et les programmes aux preuves, tandis que l’évaluation du
programme correspond à la simpliﬁcation d’une preuve. Cette correspondance a
été utilisée pour reﬂéter les logiques du premier et du second ordre avec des types
dépendants [BC13; Ler09], des logiques de séparation avec des systèmes de type
pouvant gérer la gestion de la mémoire [Rey02; Jun+17], des logiques sensibles aux
ressources avec conﬁdentialité diﬀérentielle [Gab+13], des logiques avec monades
permettant le raisonnement sur les eﬀets de bord [Swa+16; Mai+19], la logique
classique [Gri89], dans le développement de langages de programmation avec des
systèmes de types riches [Nor07; OSG08]. Néanmoins, une telle correspondance
n’a pas encore émergée dans le cadre quantique, même si certains progrès ont été
réalisés [DD22; DCM22; SVV18], avec des structures de type limitées.
Quantum Control Flow.
En eﬀet, dans le modèle standard, les opérations sur la mémoire quantique
sont séquentielles et non-branchantes. Cela signiﬁe que dans le modèle standard,
seul l’ordinateur classique a accès aux boucles, tests, et autres structures de con-
trôle. Cependant, des expériences montrent que la notion de contrôle quantique est
réalisable et que le modèle coprocesseur est de ce fait trop rigide et des construc-
tions plus riche peut être considérée, ce que les circuits quantique ou le ZX-Calcul
n’autorisent pas.
L’exemple typique d’un comportement de calcul quantique diﬃcilement réalis-
able dans les circuits quantiques ou le ZX-calcul est le Quantum Switch [Chi+13].
Considérons deux bits quantiques x et y et deux opérations unitaires U et V
agissant sur y. Le problème consiste à générer l’opération qui exécute UV sur y
si x est dans l’état |0⟩et V U s’il est dans l’état |1⟩.
QSwitch(x, U, V ) =
(
U
V
if x = |0⟩
V
U
if x = |1⟩
Mais, comme x peut être en superposition, l’opération peut donc être décrite
par:
(α |0⟩+ β |1⟩) ⊗|y⟩7→α |0⟩⊗(UV |y⟩) + β |1⟩⊗(V U |y⟩).
Il s’agit d’un test purement quantique : non seulement on peut avoir des valeurs
en superposition (ici, x) mais aussi des ordres d’exécution. Ceci est en contraste
ﬂagrant avec ce qui se passe dans le modèle standard de coprocesseur quantique
où la seule source de branchement vient de l’ordinateur classique. Alors que ce
programme a été montré comme étant impossible d’implémenter dans le modèle
standard du coprocesseur avec une seule instance de U et de V [Chi+13], il est
néanmoins physiquement réalisable [Abb+20].
16

La plupart des langages de programmation quantique sont encore liés au mod-
èle QRAM et ne comportent que des tests et des boucles classiques [Sin+22;
Gre+13], des modèles de calcul supportant la superposition des ordres d’exécution
ont été étudiés dans la littérature, tels que ceux proposant une extension des
circuits [CDP08; Por+17; VKB21; Wec+21].
Ces approches visent générale-
ment à discuter de la notion de canal quantique d’un point de vue théorique de
l’information quantique.
Types Quantique. Le modèle QRAM et le ZX-Calcul ne permettent de représen-
ter que des tenseurs de qubits comme types de données d’entrée et de sortie. Ils ne
n’ont pas de coproduit natif, qui pourrait être utilisé pour les tests, ni de structures
de types plus riches comme les types inductifs ou coïnductifs.
La présence d’un coproduit pourrait faciliter la représentation des tests, per-
mettant de considérer diﬀérents chemins d’exécution. C’est l’approche suivie dans
le PBS-Calcul [CP20], bien que le PBS-Calcul ne comporte pas de tenseur : leur
composition parallèle correspond à la superposition des positions d’un seul qubit,
alors que dans d’autres langages (comme le ZX-Calcul), la composition parallèle
correspond à un tenseur de qubits.
Des langages de programmation quantique avec des types, comportant à la fois
un tenseur et un coproduit, ont été développés [SVV18; VRH22] bien que dans ces
travaux, aucune relation avec un système de preuve formel n’ait encore été établie.
La mise en relation d’un système de type quantique pur avec un système logique
a été considéré dans [DD22] dans le style d’une logique de déduction naturelle
à travers l’utilisation d’un nouveau connecteur ⊙permettant la superposition de
données, avec une sémantique étudiée pour un fragment de la langage, basée sur la
catégorie des S-semimodule, pour S un semiring commutatif annulatif [DCM22].
Cependant, leur logique est limitée au fragment intuitionniste de la logique linéaire
sans tenseurs et la question du contrôle quantique n’est pas discutée.
Les langages graphiques comportant à la fois un tenseur et un coproduit sont
basés sur la notion de bandes, ou de rubans [Dun09; Mel14]. Le coproduit est
capable de séparer une bande en deux, rendant les deux parties impossible de
communiquer l’une avec l’autre jusqu’à ce qu’elles soient fusionnées à nouveau.
Tandis que le tenseur est simplement la paire d’une donnée, comme le montre
dans Figure 2. Cependant, ces formalismes ne sont pas intrinsèquement quantiques
: le séparation et la fusion des bandes doivent être eﬀectués d’une manière bien
parenthésée rendant impossible la représentation de la superposition quantique.
D’un point de vue sémantique, le calcul quantique pure avec coproduit et tenseurs
ont été étudiés [DCM22; Cha+22], mais il n’est pas clair comment la notion
des types inductifs et la récursion s’adapterait dans ces contextes et est, jusqu’à
présent, la question est laissée ouverte.
Le branchement des chemins d’exécution peut être utilisé pour déterminer
quand un programme doit se terminer ou non. La notion de récursion et de types
inductifs dans l’informatique quantique pure est encore une fois absente : aﬁn de
17

(A ⊗B) ⊕C
(A′ ⊗B′) ⊕C′
C
C′
⊕
⊕
f
g
h
A
B
A′
B′
⊗
⊗
Figure 2: Splits over coproduct and tensor
réaliser de la récursion, il faut s’assurer que chaque étape du calcul est réversible
et que le programme récursif est toujours bien déﬁni, car l’opération entière doit
être une opération unitaire. Dans [SVV18], les auteurs développent un langage de
programmation quantique pur avec le produit et coproduit et le type inductif des
listes et avec de fortes contraintes assurant la la terminaison et la réversibilité, ce
qui ouvre la voie à une généralisation et le développement d’une logique propre.
Contributions.
Nous essayons de répondre au problème de la recherche d’un
modèle de calcul quantique pur avec un système de type de données riches, le tout
avec une correspondance Curry-Howard. Nous proposons deux approches :
• Le premier, basé sur un langage de programmation purement quantique
dans le style de Theseus [JS12; JS14; SVV18] où nous présentons un lan-
gage de programmation linéaire et réversible avec des types inductifs, ainsi
qu’une correspondance une correspondance de Curry-Howard avec la logique
µMALL : logique linéaire avec les plus petits et plus grands points ﬁxes. Là
où [SVV18] étend le langage au cas quantique, avec le type limité de listes,
nous restons dans le cas purement classique. L’extension pour type inductif
quantique pur est laissée comme travail futur.
• La seconde consiste à l’utilisation des sémantiques à jetons inspiré de la
Géométrie de l’Intéraction de Girard pour des langages graphiques. Nous
montrons dans un premier temps comment capturer la sémantique dénota-
tionnelle du ZX-Calcul à partir d’une sémantique à jeton. Nous développons
ensuite un nouveau langage graphique, inspiré des réseaux de preuves de la
logique linéaire, possédant à la fois un tenseur et un coproduit, permettant
de gérer à la fois la collections de plusieurs données, mais aussi le branche-
ment d’exécution. Le langage vient avec sa propre sémantique à jeton ainsi
qu’une théorie équationnelle.
Plan de la thèse.
La thèse est organisée comme en deux partie: la partie I
introduit les notions nécessaires à la compréhension de cette thèse. Le chapitre
1 introduit les notions de calcul réversible et quantique nécessaire, tandis que le
chapitre 2 introduit le ZX-Calcul : un langage de cordes pour le calcul quantique.
Enﬁn, le chapitre 3 introduit les notions de théorie de la démonstration nécessaire,
en particulier en introduisant le formalisme de µMALL: logique linéaire avec plus
petits et plus grands point ﬁxes.
18

La partie II est composée de trois chapitres sur mes contributions personnelles.
Dans le chapitre 4 nous introduisons un langage linéaire et réversible basé sur
le pattern-matching. Le langage est une extension du langage classique présenté
dans [SVV18]. Le système de type du langage enforce des contraintes qui nous
garantissent que n’importe quelle fonction bien typée (appellées isos) est reversible.
Nous montrons (i) comment n’importe quelle fonction primitive récursive peut être
encodée comme un iso du langage, (ii) comment n’importe quelle fonction bien
typée représente un isomorphisme de preuve dans la logique µMALL, et enﬁn (iii)
nous montrons comment en relâchant les contraintes du système de type pour
considérer des fonctions partielles, nous pouvons encoder les Machines de Turing
Réversibles. Les programmes linéaires et réversibles étant un sous-ensemble du
calcul quantique, ce chapitre est un premier pas vers des types purement quantique
et du contrôle quantique.
Le chapitre 5 s’intéresse à la déﬁnition d’une sémantique à jetons pour le ZX-
Calcul. La sémantique est inspirée des sémantiques à jetons de la Géométrie de
l’Intéraction de la logique linéaire, dans lesquels des jetons se déplacent dans le
réseaux de preuve, vu comme un graphe, tout en capturant le contenu calculatoire
de la preuve. Dans ce chapitre, nous montrons comment notre machine à jetons
(i) sous certains invariants, capture exactement la sémantique dénotationnelle du
ZX-Calcul, (ii) comment la machine à jetons peut être adaptée pour capturer
l’extension du ZX-Calcul avec mesure, et (iii) comment la modiﬁer pour capturer
un autre type de sémantique du ZX-Calcul bassé sur les sum-over-paths. Le but
du chapitre est de voir comment des sémantiques à base de jetons peuvent être
adaptées dans le cas des langages graphiques pour le calcul quantique, pour pouvoir
par la suite se rapprocher des réseaux de preuves de la logique linéaire. Des travaux
existent déjà sur les sémantiques à jetons pour le calcul quantique [Dal17], mais
restent dans le cadre des circuits quantiques.
Pour ﬁnir, dans le chapitre 6 nous présentons un nouveau langage graphique
nommé le Many-Worlds Calculus. A l’inverse du ZX-Calcul qui ne contient qu’un
tenseur, le Many-Worlds contient à la fois un tenseur et un coproduit. Il permet
d’encoder de manière naturelle l’ensemble du ZX-Calcul, mais possède en plus un
système de type plus riche, dû au coproduit, qui nous permet de réaliser des tests
quantique. Le chapitre est découpé en plusieurs parties, (i) nous donnons d’abord
une sémantique à jetons, adaptée de celle du chapitre précédent, (ii) nous donnons
une sémantique dénotationnelle qui a été détuite de la machine à jetons, (iii) nous
donnons une théorie équationnelle sur les diagrammes du Many-Worlds Calculus, et
nous montrons l’universalité du langage ainsi que sa complétude. Comme étude de
cas nous montrons comment encoder le Quantum Switch dans le langage. Enﬁn,
en (iv) nous montrons comment nous pouvons encoder une version quantique, sans
récursion, du langage d’isos du chapitre 4 dans le Many-Worldd.
19


Part I
Background
21


1 - Reversible & Quantum Computation
1.1 . Reversible Computation
The idea of reversible computation comes from Landauer and Bennett [Lan61;
Ben73] with the analysis of its expressivity, and the relationship between irre-
versible computing and dissipation of energy. Indeed, Laundauer’s principle states
that the erasure of information is linked to the dissipation of energy as heat [Lan61;
Bér+12]. This principle was enough to motivate the study of reversible processes.
In order to avoid erasure of information, reversible computation often makes use
of garbage or auxiliary wires: additional information kept in order to ensure both
reversibility and the non-erasure of information. In reversible computation, given
some process f, there always exists an inverse process f−1 such that their compo-
sition is equal to the identity: f ◦f−1 = Id = f−1 ◦f. In programming languages,
this is done by ensuring both forward and backward determinism. Forward de-
terminism is almost always ensured in programming language: it is about making
sure that, given some state of your system, there is a unique next state that it
can go to. Backward determinism on the other hand checks that given a state,
there is only one state that it comes from. Standard programming languages does
not ensure backward determinism. On a computational perspective, the Toﬀoli
gate [Tof80], is enough to realize universal classical computation: any boolean
function can be implemented by Toﬀoli gates.
The Toﬀoli gate is a reversible
3-input 3-output gate which ﬂips the 3rd input if and only if the ﬁrst two are at
1. This also means that when implementing a classical, reversible computation
with only Toﬀoli gates, additional information is necessarily kept. Nevertheless,
there exists a way to turn a non-reversible process into a reversible one, without
additional information at the end (but with auxiliary wires), albeit with an increase
in computational time [Ben73]. All of this led to an interest in reversible compu-
tation [Ben00; Ama+20], both with a low-level approach [Car12; Wil+16; SM13],
and from a high-level perspective [Lut86; YG07; YAG16; JS12; JS14; SVV18;
YAG12; TA15; JKT18]. Reversible programming lies on the latter side of the spec-
trum, and two main approaches have been followed. Embodied by Janus [Lut86;
YG07; Yok10; YAG16] and later R-CORE and R-WHILE [GKY19], the ﬁrst one
focuses on imperative languages whose control ﬂow is inherently reversible —the
main issue with this aspect being tests and loops. The other approach is con-
cerned with the design of functional languages with structured data and related
case-analysis, or pattern-matching [YAG12; TA15; JS14; SVV18; JKT18]. To en-
sure reversibility, strong constraints have to be established on the pattern-matching
in order to maintain reversibility.
In general, reversible computation captures partial injective maps [GKY19]
from inputs to outputs. Indeed, from a computational perspective reversibility is
23

understood as a time-local property: if each time-step of the execution of the
computation can soundly be reversed, there is no overall condition on the global
behaviour of the computation.
In particular, this does not say anything about
termination: a computation seen as a map from inputs to outputs might very well
be partial, as some inputs may trigger a (global) non-terminating behaviour.
The categorical analysis of partial injective maps has been thoroughly analysed
since 1979, ﬁrst by Kastl [Kas79], and then by Cockett and Lack [CL02; CL03;
CL07]. This led to the development of inverse categories: a category equipped
with an inverse operator in which all morphisms have partial inverses and are
therefore reversible. The main aspect of this line of research is that partiality can
have a purely algebraic description: one can introduce a restriction operator on
morphisms, associating to a morphism a partial identity on its domain.
This categorical framework has recently been put to use to develop the se-
mantics of speciﬁc reversible programming constructs and concrete reversible lan-
guages: analysis of recursion in the context of reversibility [AK16; Kaa19b; KV19],
formalization of reversible ﬂowchart languages [GK18; Kaa19a], analysis of side
eﬀects [HK15; HKK18], etc. Interestingly enough however, the adequacy of the
developed categorical constructs with reversible functional programming languages
has seldom been studied. For instance, if Kaarsgaard et al. [KAG17] mention The-
seus as a potential use-case, they do not discuss it in detail. So far, the semantics
of functional and applicative reversible languages has always been done in concrete
categories of partial isomorphisms [KV19; KR21; CLV21].
We present two models of reversible computation that will be useful at some
point in this thesis, the ﬁrst one is called RPP [PPR20] (Reversible Primitive
Permutations): a set of reversible functions that allows to represent any primitive
recursive function, and Reversible Turing Machine [AG11a; MY07].
1.1.1 . Reversible Primitive Permutations
RPP is a set of integer-valued functions of variable arity. We deﬁne it by arity
as follows: we note RPPk for the set of functions in RPP from Zk to Zk, it is
built inductively on k ∈N by:
• the successor (S), the predecessor (P), the identity (ID) and the sign-change
that are part of RPP1.
• The swap function (X) is part of RPP2.
• For any function f, g, h ∈RPPk and j ∈RPPl, we can build (i) the
sequential composition f; g ∈RPPk, (ii) the parallel composition f || j ∈
RPPk+l (iii) the iterator It[f] ∈RPPk+1 and (iv) the selection If[f, g, h] ∈
RPPk+1.
The behaviour of the functions is described under a circuit-like form, as in [PPR20],
where the left-hand-side variables of the diagram represent the input of the function
24


x S x + 1


x P
x −1


x Sign −x


x Id x


x X y
y
x

x1
...
xn
x

f; g


y1
...
yn
x
=


x1
... f
xn




y1
g ...
yn


x1
...
xn
x

It[f]


y1
...
yn
x





= (f; . . . ; f)
|
{z
}
| x |
(x1, . . . , xn)
x1
...
xn
x

If[f, g, h]


y1
...
yn
x





=



f (x1, . . . , xn)
if x > 0
g (x1, . . . , xn)
if x = 0
h (x1, . . . , xn)
if x < 0


x1
y1
...
...
xk f || g yk
x′
1
y′
1
...
...
x′
l
y′
l


=


x1
y1
... f
...
xk
yn




x′
1
y′
1
... g ...
xl
y′
l


Figure 1.1: Generators of RPP
and the right-hand-side is the output of the function. The functions are shown
in Figure 1.1.
Finally, the set of all functions that form RPP is taken as the union for all k
all of the RPPk:
RPP = ∪k∈N RPPk
Remark 1.1.1. In their paper [PPR20], the authors make use of two other con-
structors: generalized permutations over Zk and weakenings of functions, but
those can actually be deﬁned from the other constructors [PPR20, Section 3] so
that in the following section we do not give their encoding.
Then, if f ∈RPPk we can deﬁne an inverse f−1:
Deﬁnition 1.1.2 (Inversion). The inversion is deﬁned as follows:
Id−1 = Id
S−1 = P
P−1 = S
Sign−1 = Sign
X −1 = X
(g; f)−1 = f−1; g−1
(f || g)−1 = f−1 || g−1
(It[f])−1 = It[f−1]
(If[f, g, h])−1 = If[f−1, g−1, h−1]
Proposition 1.1.3 (Inversion deﬁnes an inverse [PPR20]). Given f ∈RPPk
then f; f−1 = Id = f−1; f.
25

Theorem 1.1.4 (Soundness & Completeness [PPR20]). RPP is PRF-Complete
and PRF-Sound: it can represent any Primitive Recursive Function and every func-
tion in RPP can be represented in PRF.
1.1.2 . Reversible Turing Machines
We start by introducing Reversible Turing Machine, following the formalism
from [MY07; Ben73].
Deﬁnition 1.1.5 (Turing Machine). We deﬁne the notion of a Turing Machine
(TM) T as a tuple (Q, Σ, δ, b, qs, qf) where Q is a ﬁnite set of states, Σ a ﬁnite set
of tape symbols, b ∈Σ, the blank symbol and δ ⊆∆= (Q × ((Σ × Σ) ∪{←, ↓
, →}) × Q) is a partial relation deﬁning the transition relation.
The states qs and qf are the starting and ﬁnal state. There must be no transi-
tion leading out of qf and no relation leading into qs.
Deﬁnition 1.1.6 (Conﬁguration). The conﬁguration of a TM is a tuple (q, (l, s, r)) ∈
Q × (Σ∗× Σ × Σ∗) where q is the internal state, l, r are the left and right parts of
the tape (as string) and s ∈Σ is the current symbol being scanned.
A TM T in conﬁguration C = (q, (l, s, r)) leads to conﬁguration C′ = (q′, (l′, s′, r′)),
written as T ⊢C ⇝C′ in a single computation step if there exists a transi-
tion (q, a, q′) ∈δ where a is either (s, s′), in which case l = l′ and r = r′ or
a ∈{←, ↓, →} in which case we have for the case a =←: l′ = l · s and for
r = x · r2 we have s′ = x and r′ = r2, similarly for the case a =→and for the
case a =↓we have l′ = l and r′ = r.
Deﬁnition 1.1.7 (Local forward/backward determinism). A TM T is local for-
ward deterministic if and only if for any distinct pair of triples (q1, a1, q′
1) and
(q2, a2, q′
2) in δ, if q1 = q2 then a1 = (s1, s′
1) and a2 = (s2, s′
2) and s1 ̸= s2.
A TM T is local backward deterministic if and only if for any distinct pair
of triples (q1, a1, q′
1) and (q2, a2, q′
2) in δ, if q′
1 = q′
2 then a1 = (s1, s′
1) and a2 =
(s2, s′
2) and s′
1 ̸= s′
2.
Deﬁnition 1.1.8 (Reversible Turing Machine). We say that a Turing Machine is
reversible if and only if it is locally forward and backward deterministic.
The transition relation works as follow:
If the transition is done by an element (q, (s, s′), q′) ∈δ, then it means that if we
are in state q and read s, we write s′ and go in state q′. If the transition is (q, d, q′)
it means that if we are in state q, we move by direction d and go into state q′.
Deﬁnition 1.1.9 (String Semantics). The semantic JTK of a TM is given by:
JTK = {(s, s′) ∈((Σ\{b})∗×(Σ\{b})∗) | T ⊢(qs, (ϵ, b, s)) ⇝∗(qf, (ϵ, b, s′))}
26

The computation is as follows: from starting state qs with input s, in a standard
conﬁguration (qs, (ϵ, b, s)) run the machine until it halts in a standard conﬁgura-
tion (qf, (ϵ, b, s′)) with output s′, or diverges.
We say that T computes function f if and only if JTK = f.
Theorem 1.1.10 (RTMs are injective [Ben73]). If T is a RTM, then JTK is an in-
jective function.
Lemma 1.1.11 (RTM Inversion [Ben73]). Given a RTM T = (Q, Σ, δ, b, qs, qf),
deﬁne T −1 = (Q, Σ, inv(δ), b, qf, qs) as the inverse Turing Machine, where inv(δ)
is deﬁned as:
• inv(q, (s, s′), s′) = (q′, (s′, s), q)
• inv(q, ←, q′) = (q′, →, q)
• inv(q, ↓, q′) = (q′, ↓, q)
• inv(q, →, q′) = (q′, ←, q)
T −1 compute the inverse function, i.e
q
T −1y
= JTK−1.
Finally, an important result is that any Turing Machine can be turned into a
Reversible Turing Machine while preserving the semantics:
Theorem 1.1.12 (Bennett’s method [Ben73]). Given a 1-tape Turing Machine
T, there exists a 3-tape reversible Turing Machine B(T), such that JB(T)K (x) =
(x, JTK (x)).
Notice that T and B(T) does not have the same exact the same semantics:
in the output of B(T) the initial input x is still present. As we stated earlier,
reversible computation always come at the cost of having to keep in memory
additional information to ensure reversibility.
Finally, for robustness, we will state that any k-tape RTM can be turned into
a 1-tape RTM [AG11b].
1.2 . Quantum Computation
Reversible computation makes an important subset of quantum computation
without measurement. In both setting, all operations are reversible. In the quantum
case, a reversible operation send basis vectors to basis vectors: it is a unitary
operation. Quantum computation also requires operations to be linear, which can
be considered in a linear, reversible computational model. The main diﬀerence
arise in the data they handle: quantum computing have access to a new kind of
data, quantum bits.
27

Figure 1.2: The QRAM Model
The QRAM Model
In general quantum computation, one has access to a co-
processor holding a “quantum” memory. This memory consists of “quantum” bits,
the basic unit of information in quantum computation, having a peculiar prop-
erty: their state cannot be duplicated, and the operations one can perform on
them are unitary, reversible operations. The coprocessor comes with an interface
to which one can send instructions to allocate, update or read quantum registers.
Quantum memories can be used to solve classical problems faster than with purely
conventional means. The most prominent examples being Grover’s algorithm, al-
lowing to search for an element in a list of n element in O(
√
N) [Gro96] and its
direct application in Shor’s algorithm, allowing for decomposing a number into its
prime factor exponentially faster than classical algorithm [Sho99]. Quantum pro-
gramming languages are nowadays pervasive [FBW18; VRH22] and several formal
approaches based on logical systems have been proposed to relate to this model
of computation [SV06; PRZ17; RS17]. However, all of these languages rely on a
purely classical control-ﬂow: quantum computation is reduced to describing a list
of instructions —a quantum circuit— to be sent to the coprocessor. In particular,
in this model, operations performed on the quantum memory only act on quan-
tum bits and tensors thereof, while the classical computer enjoys the manipulation
of any kind of data with the help of rich type systems. Reversible computation
can be seen as a subcase of quantum computation. While reversible computation
allows for the duplication and erasure of data, this is not the case in quantum
computation, but in both cases the operations are always reversible.
Dirac Notation
Dirac’s notation is a way to represent matrices and vectors
in a more compact way. In Dirac notation [NC02], vectors of the form |.⟩(called
“kets”) are considered as column vector, and therefore |0⟩= ( 1
0 ), |1⟩= ( 0
1 ), and
given α, β complex numbers, α |0⟩+ β |1⟩= ( α
β ). The tensor product of spaces
V and W whose bases are respectively {vi}i∈I and {wj}j∈J is the vector space
of basis {vi ⊗wj}i,j∈I×J, where vi ⊗wj is a formal object consisting of a pair of
vi and wj. We denote |x⟩⊗|y⟩as |xy⟩and |0m⟩to represent an m-fold tensor
of |0⟩. As a shortcut notation, we write |φ⟩for column vectors consisting of a
linear combination of kets. Dirac also introduced the notation “bra” ⟨x|, standing
for a row vector. So for instance, α ⟨0| + β ⟨1| is ( α β ). If |φ⟩= α |0⟩+ β |1⟩,
we then write ⟨φ| for the vector α ⟨0| + β ⟨1| (with (.) the complex conjugation).
The notation for tensors of bras is similar to the one for kets.
For instance,
⟨x| ⊗⟨y| = ⟨xy|.
Using this notation, the scalar product is transparently the
28

product of a row and a column vector:

φ ψ

, and matrices can be written as
sums of elements of the form |φ⟩⟨ψ|. For instance, the identity on C2 is ( 1 0
0 1 ) =
( 1 0
0 0 ) + ( 0 0
0 1 ) = ( 1
0 ) ( 1 0 ) + ( 0
1 ) ( 0 1 ) = |0⟩⟨0| + |1⟩⟨1|.
Quantum State
As quantum computation works with vectors and matrices,
we take the two column vectors |0⟩and |1⟩as the counterpart of the classical bits
0 and 1. Richer states of multiple bits are built using tensor products of states.
Remember that the tensor product between matrices is deﬁned as:
A ⊗B =




a00B
a01B
· · ·
a10B
...
...




and hence |00⟩= |0⟩⊗|0⟩=
 1
0
0
0

. Quantum state can be in superposition by
considering a linear combination of classical bits, written α |0⟩+ β |1⟩, and where
α, β ∈C. Quantum computation asks for the state to be normalized, meaning
that |α|2 + |β|2 = 1. In general, a quantum state can be written as
X
i∈I
αi |i⟩with
X
i∈I
|αi|2 = 1.
Not all quantum states can be written as a tensor of smaller states. Those
inseparable states are called entangled state. For example, the Bell State |00⟩+|11⟩
√
2
cannot be written as the tensors of two other states.
While |0⟩, |1⟩form a computational basis, we could also consider the basis
|+⟩, |−⟩deﬁned as |+⟩= |0⟩+|1⟩
√
2
and |−⟩= |0⟩−|1⟩
√
2 . One way to go from one
basis to the other is via the Hadamard gate.
Operations & Quantum Circuits
Quantum computation is restricted to a
particular kind of operations called unitaries: matrices whose conjugate transpose
U∗is also its inverse, U−1. Any unitary can be written as P
i,j∈B αi,j |i⟩⟨j| for B
an orthogonal basis of the state space under consideration.
A quantum circuit is the quantum counterpart of boolean circuits: a graphi-
cal language where wires represent qubits and gates represent unitary operations.
Qubits ﬂow from the left of the circuit towards the right, updating their state as
they traverse quantum gates. A quantum circuit is then a single unitary operation
which is made by the composition of the smaller gates inside it.
A standard universal set consists of the one-input, one-output Pauli-X, -Y and
-Z gates, the Hadamard gate and the Phase Shift P(φ) gate, represented in a
circuit by the diagram
#
with # ∈{X, Y, Z, H, P(φ)} and correspond-
ing respectively to the matrices ( 0 1
1 0 ) ,
  0 −i
i 0

,
  1 0
0 −1

,
1
√
2
  1 1
1 −1

,
  1
0
0 eiφ

. Along
29

|0⟩
H
|0⟩
Figure 1.3: Bell State’s quantum circuit
with the two-input two-output CNOT gate
 1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0

represented by
.
For example, the circuit in Figure 1.3 takes for input two qubits initialized at
|0⟩, apply the Hadamard gate on the ﬁrst qubit and then applies a CNOT in order
to produce the Bell State |00⟩+|11⟩
√
2
.
Measurement
One distinct feature of quantum computation is measurement:
the collapse of a superposition of states into a single one with some probability. For
instance, in the qubit case, applying a measurement on the state α |0⟩+ β |1⟩give
|0⟩(resp. |1⟩) with probability |α|2 (resp. |β|2). More generally, given a quantum
state P
i∈B αi |i⟩with B a orthogonal basis, a measurement on such a state gives
us the state |i⟩with probability |αi|2. The standard semantics for measurement is
to work with density matrices and completely positive maps (CPM).
In a quantum circuit the measurement can always be done at the end. This is
called the Deferred Measurement Principle [NC02]. Therefore, for this thesis we
will focus purely on the unitary part and ignore the measurement.
More information and details can be found in [NC02].
30

2 - Graphical Language for Quantum Com-
putation
The use of graphical representations for computation can be found in many
ﬁelds of computer science, the most basic example being the control ﬂow of a
program, where one can initialize a state of the machine, in which each variable
is given a value of its type, and look at the order of instructions that are being
executed by the program. A more formal one would be boolean circuits: graphical
representations of an assembly-like language in which the only connectives are
the usual logical ones (negation, conjunction and disjunction), read from left to
right in which the semantics is given by considering some tokens (the bits) that
ﬂow from the input of the diagram all the way to the output and whose values
change according to the gate they enter. Similarly, quantum circuits (quantum
counterparts of boolean circuits) work in the same way, with qubits instead of
bits ﬂowing through the circuit. Although successful, those formalisms are akin to
assembly-like languages which makes it hard to use and to reason about.
Diagrammatic languages for describing the mathematical behaviour of quan-
tum processes were already present in the 1940’s with Feynman’s diagrams but, in
this section, we are interested in a particular kind of graphical languages: string di-
agrams. String diagrams are a graphical formalism that allows for a 2-dimensional
representation of categorical syntax.
Coming all the way from Feynman dia-
grams [FH65], a graphical language is an alternative syntax to the usual matrix
representation of quantum computation. String diagrams for quantum computa-
tion emerged from the Categorical Quantum Mechanics program by Abramsky and
Coecke [AC04; AC09]. Their goal was to be able to study properties of quantum
processes in a more abstract way through the use of category theory. This led
to the development of the ZX-Calculus [CD11], a graphical language for quantum
computation. On a formal level, a graphical language is a PROP [Lac04], that
is, a symmetric, strict monoidal structure (C, ⊤, ⊠) whose objects are of the form
W ⊠· · · ⊠W.
The object W is a “wire”, and any object stands for a bunch
of wires. The monoidal structure formalizes how the bunching of wires behaves.
For the purpose of this thesis, we introduce the ZX-Calculus purely as a graphical
language. For further bibliography on the subject we refer to [Vil19; Sel10; JS91;
CD11; CK17].
2.1 . The ZX-Calculus
Although the pervasive model for quantum computation, quantum circuits,
only have an informal semantics: qubits travelling from the inputs wires to the
outputs wires, changing their state as they pass through the gates of the circuit.
31

A quantum circuit is understood as some sequential, low-level assembly language
where quantum gates are opaque black boxes.
In particular, quantum circuits
do not natively feature any formal operational semantics giving rise to abstract
reasoning or well-founded rewriting system, and did not, until recently [Clé+22a],
feature an equational theory, even though the current one is not very usable.
From a denotational perspective, quantum circuits are literal descriptions of
tensors and applications of linear operators.
These can be described with the
original matrix interpretation [NC02], or with the more recent sum-over-path se-
mantics [Amy18; Cha+] —this can be regarded as a wave-style semantics. In such
a semantics, the state of all of the quantum bits of the memory is mathematically
represented as a vector in a (ﬁnite dimensional) Hilbert space: the set of quantum
bits is a wave ﬂowing in the circuit, from the inputs to the output, while the com-
putation generated by the list of quantum gates is a linear map from the Hilbert
space of inputs to the Hilbert space of outputs.
In recent years, an alternative model of quantum computation with better
formal properties than quantum circuits have emerged: the ZX-Calculus [CD11].
Originally motivated by a categorical interpretation of quantum theory, the ZX-
Calculus is a graphical language that represents linear maps as special kinds of
graphs called diagrams. Unlike the quantum circuit framework, the ZX-Calculus
comes with a sound and complete [Vil19], well-deﬁned equational theory on a small
set of canonical generators making it possible to reason on quantum computation
by means of local graph rewriting.
The canonical semantics of a ZX diagram consists in a linear operator. This
operator can be represented as a matrix or through the more recent sum-over-path
semantics [Vil20]. But in both cases, these semantics give a purely functional,
wave-style interpretation to the diagram. Nonetheless, this graphical language —
and its equational theory— has been shown to be amenable to many extensions and
is being used in a wide spectrum of applications ranging from quantum circuit opti-
mization [Dun+20; Bac+20], veriﬁcation [Hil11; DL14; DG18] and representation
such as MBQC patterns [DP10] or error-correction [BH20; Bea+19].
The ZX-Calculus is a powerful graphical language for reasoning about quantum
computation introduced by Bob Coecke and Ross Duncan [CD11]. A term in this
language is a graph —called a string diagram— built from a core set of primitives.
In the standard interpretation of ZX-Calculus, a string diagram is interpreted as a
matrix. The language is equipped with an equational theory preserving the standard
interpretation.
2.1.1 . Pure Operators
The so-called pure ZX-diagrams are generated from a set of primitives, given
on the right: the Identity, Swap, Cup, Cap, Green-spider and H-gate:
(
,
,
,
,
α
n...
...
m
,
)
n,m∈N
α∈R
32

The real number α attached to the green spiders is called the angle, we gen-
erally write Zn
m(α) for a green-spider with n inputs, m outputs and angle α.
ZX-diagrams are read top-to-bottom: dangling top edges are the input edges and
dangling edges at the bottom are output edges. For instance, Swap has 2 input and
2 output edges, while Cup has 2 input edges and no output edges. ZX-primitives
can be composed either sequentially or in parallel:
D2 ◦D1 :=
...
...
...
D2
D1
D1 ⊗D2 :=
...
...
D1
...
...
D2
We write ZX for the set of all ZX-diagrams. Notice that when composing
diagrams with (_ ◦_), we “join” the outputs of the top diagram with the inputs
of the bottom diagram. This requires that the two sets of edges have the same
cardinality.
Convention 2.1.1. We deﬁne a second spider, red this time, by composition of
Green-spiders and H-gates, as shown on the right, similarly for the green-spider,
we write the red-spider as Xn
m(α).
α
...
:=
α
...
...
...
Convention 2.1.2. We write σ for a permutation of wires, i.e any diagram gen-
erated by
n
,
o
with sequential and parallel composition. The Cap and Cup
are written respectively as η and ϵ. We write Zn
k (α) (resp, Xn
k ) for the green-node
(resp, red-node) of n inputs, k outputs and parameter α and H for the H-gate.
Finally, by abuse of notation a green or red node with no explicit parameter holds
the angle 0:
...
...
0
...
...
:=
and
...
...
0
...
...
:=
.
Formally, the ZX-Calculus is a dagger compact category and is in particular
a PROP: objects are natural numbers, and morphisms are the diagrams [CD11].
For more information on the categorical background of the ZX-Calculus and other
graphical languages we refer to [Sel10; CK17].
2.1.2 . Standard Interpretation
We understand ZX-diagrams as linear operators through the standard interpre-
tation. Informally, wires are interpreted with the two-dimensional Hilbert space,
with orthonormal basis {|0⟩, |1⟩}. In the standard interpretation [CD11], a dia-
gram D is mapped to a ﬁnite dimensional Hilbert space of dimension some powers
of 2: JDK ∈Qubit := {C2n →C2m | n, m ∈N}.
If D has n inputs and m outputs, its interpretation is a map JDK : C2n →C2m
(by abuse of notation we shall use the notation JDK : n →m). It is deﬁned
inductively as follows.
33

u
wv
...
...
...
D2
D1
}
~ =
t ...
...
D2
|
◦
t ...
...
D1
|
t ...
...
D1
...
...
D2
|
=
t ...
...
D1
|
⊗
t ...
...
D2
|
r z
= idC2 = |0⟩⟨0| + |1⟩⟨1|
r
z
=
X
i,j∈{0,1}
|ji⟩⟨ij|
q
y
=
q
y† = |00⟩+ |11⟩
r
z
= |+⟩⟨0| + |−⟩⟨1|
t
α
n...
...
m
|
= |0m⟩⟨0n| + eiα |1m⟩⟨1n|
t
α
n...
...
m
|
= |+m⟩⟨+n| + eiα |−m⟩⟨−n|
Intuitively:
• The Hadamard gate is the standard one from quantum computation:
1
√
2
  1 1
1 −1

,
that sends the state |0⟩to |0⟩+|1⟩
√
2
and |1⟩to |0⟩−|1⟩
√
2 .
• The cap creates an entangled pair of |00⟩and |11⟩, while the cup is its dual:
asking both element of the |xy⟩to be the same, either 0 or 1.
• The green-spider can be seen as a map that checks that all its input are
the same, if its input is |0n⟩it returns |0m⟩, if they are all |1n⟩it returns
eiα |1m⟩and otherwise returns the 0 vector.
• The red-spider is the same but on the |+⟩, |−⟩basis.
Example 2.1.3. The states |0⟩and |1⟩can be encoded as:
and
π
A maybe more intuitive way to look at it is when considering a simpler green
and red-spider where the green-spider has one input and n outputs and the red-
spider have n inputs and one output. Then, the green spider can be seen as a copy
operation on classical data while the red spider can be seen as a XOR:
=
...
...
kπ
kπ
kπ
s.t.
...
knπ
k1π ...
=
P
j
kjπ
...
s.t.
for k, k1, . . . , kn ∈
{0, 1}
From those, we can recover the general green and red spider:
...
...
:=
...
...
...
α
α
...
...
:=
...
...
...
α
α
Example 2.1.4. The CNOT gate (up to some scalar) in the ZX-Calculus can be
deﬁned as:
u
v
}
~ =
s {
⊗
r
z
◦
s
{
⊗
s {
=
1
√
2




1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0



.
2.1.3 . Properties and structure
In this section, we list several deﬁnitions and known results that we shall use
in the remainder of the thesis. See e.g. [Vil19] for more information.
34

...
=
α+β
β
...
α
...
(S)
...
...
...
...
=
(Id1)
=
(Id2)
=
(CP)
=
(B)
π
α =
(π)
−α
π
π
α
...
=
α
...
...
...
(H)
Figure 2.1: Minimal equational theory of the ZX-Calculus
Universality
ZX-diagrams are universal [CD11] in the sense that for any linear
map f : n →m, there exists a diagram D of ZX such that JDK = f.
The price to pay for universality is that diﬀerent diagrams can possibly repre-
sent the same quantum operator, for instance we have that
t |
=
rz
. There
exists however a way to deal with this problem: an equational theory. We give in
Figure 2.1 a complete axiomatization of the standard ZX-Calculus. This equational
theory is not the only one that exists for the ZX-Calculus, also several equational
theories have been designed for various fragments of the language [Bac14; JPV18a;
HNW18; JPV18b; JPV19; Vil19].
One important aspect of the equational theory is that ZX-diagrams can be
seen as open graphs. Therefore, any graph isomorphism is a valid derivation in the
equational theories. For example:
π
2
=
π
2
Core axiomatization
Despite this variety, any ZX axiomatization builds upon
the core set of equations provided in Figure 2.2, meaning that edges really behave
as wires that can be bent, tangled and untangled. They also enforce the irrelevance
on the ordering of inputs and outputs for spiders. Most importantly, these rules
preserve the standard interpretation given in Section 2.1.2. These rules are some-
times referred to as “only connectivity matters”, and they preserve the semantics,
most of the time people consider diagrams modulo those rules. Those rules are
the one given by the underlying PROP structure.
Completeness
The ability to transform a diagram D1 into a diagram D2 using
the rules of some axiomatization zx (e.g. the core one presented in Figure 2.1) is
35

=
D =
... ...
...
...
D ...
...
... ...
=
=
α
...
...
=
α
...
...
σ...
σ′...
...
...
α
=
...
...
α
=
Figure 2.2: Connectivity rules. D represents any ZX-diagram, and σ, σ′
any permutation of wires.
denoted zx ⊢D1 = D2.
The axiomatization is said to be complete whenever any two diagrams repre-
senting the same operator can be turned into one another using this axiomatization.
Formally:
JD1K = JD2K ⇐⇒zx ⊢D1 = D2
The ﬁrst complete axiomatization of the ZX-Calculus was provided in [Vil19].
It is common in quantum computing to work with restrictions of quantum
mechanics. Such restrictions translate to restrictions to particular sets of diagrams
– e.g., the π
4 -fragment which consists of all ZX-diagrams where the angles are
multiples of
π
4 .
There exist axiomatizations that were proven to be complete
for the corresponding fragment (all of the aforementioned references tackle the
problem of completeness).
Input and output wires
An important result from quantum computation
that translates nicely is the ZX-Calculus is the following:
Theorem 2.1.5 (Choi-Jamiołkowski). There are isomorphisms between {D ∈
ZX | D : n →m} and {D ∈ZX | D : n −k →k + m} (when k ≤n).
To see how this can be true, simply add cups or caps to turn input edges to
output edges (or vice versa), and use the fact that we work modulo the rules of
Figure 2.2 as in:
...
...
...
When k = n, this isomorphism is referred to as the map/state duality. A
related but more obvious isomorphism between ZX-diagrams is obtained by per-
mutation of input wires (resp. output wires).
Example 2.1.6. The CNOT diagram composed with the state |10⟩and its reduction
in the ZX-Calculus:
π
π=
π
π
C=
π
π
S=
π
π
S=
π
π
and we have that
36

t
π
π
|
=
r
πz
⊗
r
πz
= |11⟩
2.1.4 . ZX-diagrams for Mixed Processes
While the ZX-Caclulus allows to represent pure quantum computation, it is
possible to consider its extension to mixed processes, allowing to represent mea-
surement by adding a unary generator
to the language [CP12; Car+19], that
intuitively enforces the state of the wire to be classical. We denote with ZX
the
set of diagrams obtained by adding the
generator.
Similar to what is done in quantum computation, the standard interpretation
J.K
for ZX
maps diagrams to CPMs. If D ∈ZX we deﬁne JDK
as ρ 7→
JDK† ◦ρ ◦JDK, and we set J
K
as ρ 7→Tr(ρ), where Tr(ρ) is the trace of ρ.
There is a canonical way to map a ZX -diagram to a ZX-diagram in a way
that preserves the semantics: the so-called CPM-construction [Sel07]. We deﬁne
the map (conveniently named) CPM as the map that preserves compositions (_ ◦
_) and (_ ⊗_) and such that:
CPM



...
...
...
D2
D1


= CPM
 ...
...
D2
!
◦CPM
 ...
...
D1
!
CPM
 ...
...
D1
...
...
D2
!
= CPM
 ...
...
D1
!
⊗CPM
 ...
...
D2
!
CPM
 
=
CPM


=
CPM
 
=
CPM
 
=
CPM (
) =
CPM
 
α
n...
...
m
!
= α ...
m
-α
n...
CPM
 
α
n...
...
m
!
= α ...
m
-α
n...
CPM


=
CPM(D) has to be understood as two copies of D where
is replaced by
and where every angle α is changed to −α in the second copy. Indeed, a CPM
operation from A →B can be seen as a unitary operation from A⊗A∗→B⊗B∗,
this is what is done in this transformation.
In the general ZX-Calculus, it has been shown that the axiomatization itself
could be extended to a complete one by adding only four axioms and is sound and
complete [Car+19].
Example 2.1.7. A ZX -diagram and its associated CPM construction.
α
7→
α
−α
37

=
=
=
α
=
Figure 2.3: Additional rules for
. Together with zx, they form the
equational theory zx .
38

3 - Proof Theory & The Curry Howard Iso-
morphism
3.1 . Logic & Computation
In this chapter, we give a short background on the typed lambda-calculus whose
notions we will use in Chapter 4, and logic, and see how both notions are related
by the Curry-Howard Correspondence. We also give a extension introduction to
the logic µMALL, an extension of linear logic with least and greatest ﬁxed point.
3.1.1 . The Simply-Typed λ-Calculus
Syntax
Among the many computational models that exists, the one that im-
pacted the most the theory and development of programming languages is without
a doubt the λ-calculus. Developed by Alonzo Church in the 1930s [Chu36; Chu41].
The λ-calculus is a model of higher-order computation that manipulates λ-terms
built upon the following syntax:
t, u ::= x | t u | λx.t
where x is a variable, taken from an inﬁnite set of variables V, the term (t u) is
the application of a term t (considered as a function) to another term u (the input
of said function) and λx.t is a function declaration (also called a λ-abstraction)
which binds the variable x inside t. Intuitively λx.t can be seen as an unnamed
function x →t. For instance λx.x can be seen as the identity function while λx.y
as a constant function that always returns y.
Scope & The α-equivalence
As said, λx.t acts as a binder that binds the
variable x inside t, and so we say that x is bound inside t. On the opposite, we
say that x is free inside t if it is not bound by an abstraction. More formally we
can deﬁne the set of free-variables inside a term t by induction on t as FV(x) = x,
FV(t t′) = FV(t)∪FV(t′), FV(λx.t) = FV(t)\{x}. A term with no free variable
is said to be closed (also called a combinator).
Contexts
A context is a term with a hole in it. If C is a context, then C[t] is
the result of ﬁlling the hole with t. For example, if we have the context C = λx.□
then C[t] = λx.t. Contexts can be used when we wish to focus on one position
in a term, they can be used in deﬁning particular evaluation order. For instance,
we can formulate the arbitrary context C ::= □| C M | M C | λx.C. Contexts are
used to deﬁne how a term valuate through the β-reduction as we will see shortly.
39

Types
Introduced by Church [Chu40], the Simply Typed λ-Calculus is an ex-
tension of the base λ-calculus in which terms are typed: annotated with some
types, according to some typing system. A type system is a set of deduction rules
allowing us to build typing derivations. Type systems usually use typing context:
a set of pairs of term-variable and a type deﬁned as ∆::= ∅| x : A, ∆where the
comma represents a union and A is a type. In the simply typed λ-calculus, the
types are deﬁned by the grammar o ::= o | o1 →o2 and a typing judgement is of
the form ∆⊢t : o, which can be understood as: under context ∆, the term t has
type o. The typing system is formally deﬁned by the rules:
x : o, ∆⊢x : o
x : o1, ∆⊢t : o2
∆⊢λx.t : o1 →o2
∆⊢t : o1 →o2 ∆⊢t′ : o1
∆⊢t t′ : o2
Then, one can decide to only consider well-typed terms: terms who have a
typing derivation deﬁned inductively by the rules given above.
Computation
Computation inside the λ-calculus works with β-reduction: when
an abstraction (λx.t) is applied to some argument u, the β-reduction will produce
the term t in which all occurrence of the free variable x inside t has been replaced
by u, noted t[x ←u]. In the way we described the β-reduction we may end up with
a problem called variable capture: consider (λx.y)[y ←x] = λy.y. We went from
the constant functions that always returns y to the identity function. To avoid
this problem, we consider the α-equivalence which put into relation terms with
diﬀerent bounded variables, for instance we can say that λx.x ≡α λz.z or that
(λx.t) ≡α (λy.t[x ←y]). It allows us to work up to renaming of bound variables.
In order to avoid conﬂicts between variables we will always work up to α-conversion
and use Barendregt’s convention [Bar84, p.26] which consists in keeping all bound
and free variables names distinct, even when this remains implicit. The β-reduction
is then deﬁned using the evaluation context as C[(λx.t)u] →β C[t[x ←u]]. The
λ-calculus, with β-reduction is Turing Complete.
An important property is that the β-reduction is conﬂuent: if a term t can
be reduced in two diﬀerent ways by the β-reduction into terms t1, t2, there always
exists another term t′ such that t1 and t2 reduce to t3. The typing system ensures
us that when a term reduces, it keeps the same type and that well-typed terms
always terminate: there is no inﬁnite sequence of reduction. For instance, the term
Ω= (λx.xx)(λx.xx) is ill-typed and its reduction does not terminate: Ω→β
xx[x ←(λx.xx)] = (λx.xx)(λx.xx)
Example 3.1.1. Take the function that takes two arguments x and y and returns
the second argument while erasing the ﬁrst one: λx.λy.y and applying it to any
two arguments t, t′, we get: ((λx.λy.y)t)t′ →β (λy.y[x ←t])t′ = (λy.y)t′ →β
y[y ←t′] = t′
40

Example 3.1.2 (Booleans values in the λ-calculus). We can deﬁne the two truth
values True (tt) and False (ff) as λx.λy.x and λx.λy.y and the Boolean opera-
tion AND as λx.λy.x y ff.
To give a few intuitions, consider AND tt ff: then
(λx.λy.x y ff) tt ff →β (λy.tt y ff) ff →β tt ff ff
= (λx.λy.x) ff ff →β (λy.ff) ff →β ff
and AND tt tt:
(λx.λy.x y ff) tt tt →β (λy.tt y ff) tt →β tt tt ff
= (λx.λy.x) tt ff →β (λy.tt) ff →β tt
Other Boolean operations such as the disjunction, negation, etc. can also be
deﬁned in the λ-calculus.
3.1.2 . Logic
Logic and Proof Theory is the study of mathematical proof and mathematical
reasoning. A logical system is made of formulas, allowing one to express logical
statements and of inference rules, allowing to reason, and prove, said statements.
In a logical system, one is interested in proof trees. A proof tree is a tree where the
leafs are formulas and the internal nodes are inference rules. The root of the tree is
the formula we want to prove and the leafs are formulas which are always considered
provable, called axioms. In this chapter, we present a fragment of the intuitionistic
sequent calculus LJ, that we will also call LJ by abuse of notation. Formulas are
builds upon connectives between atoms. Usual connectives feature the negation,
conjunction, disjunction and implication, respectively noted ¬, ∧, ∨, →. Hence,
given an inﬁnite set of atoms {p, q, . . . } the syntax of formulas is deﬁned as:
A, B ::= p | ¬A | A ∧B | A ∨B | A →B
Then, a proof of some formula A, under a set of assumptions A1, . . . , An is
noted as A1, . . . , An ⊢A. The left-hand side of the sequent (⊢) is called the
context and is deﬁned as ∆::= ∅| A, ∆where the comma stands for the union.
We only allow to have at most one formula on the right-hand-side of the sequent.
A sequent A1, . . . , An ⊢B should be read as A1 ∧· · · ∧An →B, meaning that
the formulas on the left side of the sequent are put in conjunction and should imply
the formula from the right hand side of the sequent.
The rules of the logic are given in Figure 3.1
Among the rules, the one of particular interest is the cut rule, it says that in
order to prove some formula A, one can ﬁrst start by proving B, and then use
the hypothesis B in order to prove A. It corresponds to the use of a lemma in
a mathematical proof. However, the use of the cut rule can lead to superﬂous
41

∆, A ⊢A ax
∆⊢B
∆, B ⊢A
∆⊢A
cut
∆, Ai ⊢C
∆, A1 ∧A2 ⊢C ∧i
L, i ∈{1, 2}
∆⊢A
∆⊢B
∆⊢A ∧B
∧R
∆, A ⊢C
∆, B ⊢C
∆, A ∨B ⊢C
∨L
∆⊢A
∆⊢A ∨B ∨1
R
∆⊢B
∆⊢A ∨B ∨2
R
∆⊢A
∆, B ⊢C
∆, A →B ⊢C
→L
∆, A ⊢B
∆⊢A →B →1
R
∆⊢B
∆⊢A →B →2
R
∆⊢A
∆, ¬A ⊢¬L
∆, A ⊢
∆⊢¬A ¬R
Figure 3.1: Rules of LJ
information. Consider a proof π of some formula A, one can then consider the
proof:
π
⊢A
A ⊢A ax
⊢A
cut
The use of the cut and axiom rules are superﬂous: they make a useless detour in
order to prove A. This led to the notion of proof simpliﬁcation, or cut-elimination
by Gentzen [Gen35]: if there exists a proof π of some ∆⊢A, then there exists a
proof π′ of ∆⊢A without cuts. This is an important result: it implies consistency,
which says that one cannot prove the empty statement.
For instance, a rule of the cut-elimination is:
π
⊢A
A ⊢A ax
⊢A
cut ⇝
π
⊢A
3.1.3 . Curry-Howard
Computation and logic are two faces of the same coin. For instance, consider a
proof s of A →B and a proof t of A. With the logical rule Modus Ponens one
can construct a proof of B: Figure 3.2 features a graphical presentation of the
corresponding proof. In the Curry-Howard correspondence [Cur34; How80] types
correspond to formulas and programs (terms) to proofs, while program evaluation
42

s....
A ⊢B
t....
⊢A
⊢B
cut
Figure 3.2: Modus Ponens
is mirrored with proof simpliﬁcation (the so-called cut-elimination).
The Curry-Howard correspondence formalizes the fact that the proof s of A →
B can be regarded as a function —parametrized by an argument of type A—
that produces a proof of B whenever it is fed with a proof of A. Therefore, the
computational interpretation of Modus Ponens corresponds to the application of an
argument (i.e. t) of type A to a function (i.e. s) of type A →B. When computing
the corresponding program, one substitutes the parameter of the function with t
and get a result of type B. On the logical side, this corresponds to substituting
every axiom introducing A in the proof s with the full proof t of A. This yields a
direct proof of B without any invocation of the “lemma” A →B.
Paving the way toward the veriﬁcation of critical software, the Curry-Howard
correspondence provides a versatile framework. It has been used to mirror ﬁrst
and second-order logics with dependent-type systems [BC13; Ler09], separation
logics with memory-aware type systems [Rey02; Jun+17], resource-sensitive log-
ics with diﬀerential privacy [Gab+13], logics with monads with reasoning on side
eﬀects [Swa+16; Mai+19], classical logic [Gri89], etc.
3.2 . Linear Logic
Linear Logic, introduced by Girard [Gir87] is a resource sensitive logic in which
formulas cannot be duplicated nor erased at will and which embed both classical
and intuitionistic logic. Linear Logic was discovered by the study of the semantics
of system F with coherent spaces, where the intuitionistic arrows A →B was
decomposed into !A ⊸B. In this decomposition, !A (of course A) tells you that
you can duplicate A as many times as you want and A ⊸B is a function that
uses its argument exactly once.
The syntax of the formulas of Linear Logic is given by:
A, B ::= 1 | ⊥| A ⊗B | A ` B
Multiplicative Fragment
000 | ⊤| A ⊕B | A & B
Additive Fragment
!A | ?A
Exponential Fragment
Linear Logic admits several fragments of interests among which:
43

• Multiplicative Linear Logic (MLL): consists of the syntax whose connectors
are ⊗, ` and its units 1 and ⊥.
• MALL (Multiplicative Linear Logic): is MLL with the additives connectives
⊕and & and their unit 000, ⊤.
• MELL (Multiplicative Exponentials Linear Logic): consist of MLL with the
exponential connectives !, ?.
Finally, the formulas come with an involution operation for negation, noted A⊥
deﬁned by:
1⊥= ⊥
(A ⊗B)⊥= A⊥` B⊥
000⊥= ⊤
(A ⊕B)⊥= A⊥& B⊥
⊢A⊥, A id
⊢∆, A
⊢Γ, A⊥
⊢∆, Γ
cut
⊢∆
⊢⊥, ∆⊥
⊢1 1
⊢A, B, ∆
⊢A ` B, ∆`
⊢A, ∆
⊢B, Γ
⊢A ⊗B, ∆, Γ
⊗
⊢A, ∆
⊢B, ∆
⊢A & B, ∆
&
⊢Ai, ∆
⊢A1 ⊕A2, ∆⊕i i ∈{1, 2}
∆
⊢∆, ⊥⊥
⊢A, ?∆
⊢!A, ?∆!
⊢A, ∆
⊢?A, ∆?d
⊢?A, ?A, ∆
⊢?A, ∆
?c
⊢∆
⊢?A, ∆?w
Figure 3.3: Rules of Linear Logic.
Remark 3.2.1. Usually, Linear Logic is presented in a two-sided way where se-
quents are of the form ∆⊢Γ and come with twice as many rules (for rules that
are applied on the right-hand-side or on the left-hand-side of the sequent). This
is closer to the formalism of LJ. One can go from the two-sided representation
to the one-sided representation by applying the negation on the context ∆, i.e.:
∆⊢Γ ⇝⊢Γ, ∆⊥.
For example, in the two-sided version of Linear Logic, the rules for the & be-
comes:
Γ ⊢A
Γ ⊢B
Γ ⊢A & B
&R
Γ, Ai ⊢C
Γ, A1 & A2 ⊢C &i
L, i ∈{1, 2}
which matches the rule ∧R and ∨i
L. This comparison can be done with the
other additive connectives of Linear Logic. For the multiplicative connectives, there
exists a multiplicative version of LJ that requires structural rules, that are matched
by the exponential connectives. More information can be found in [Lau11].
44

Finally, Linear Logic comes with cut-elimination rules (also called proof sim-
pliﬁcation) given in Figure 3.4 where a double bar means that we apply the same
rule multiple times. The cut-elimination also comes with some commutation rules
which allows to commute some inference rules below a cut. We do not give all the
rules, but for example we have:
⊢C, A, B, ∆
`
⊢C, A ` B, ∆
⊢C⊥, Γ cut
⊢A ` B, ∆, Γ
⇝
⊢C, A, B, ∆
⊢C⊥, Γ cut
⊢A, B, ∆, Γ
`
⊢A ` B, ∆, Γ
Linear Logic enjoys the cut-elimination theorem: given some proof
π
⊢∆with
cuts, there exists a proof
π′
⊢∆without cuts.
From a Curry-Howard point of view, the Multiplicative fragment of Linear Logic
(MLL) correspond to the linear λ-calculus while MELL correspond to the typed
λ-calculus [Gir87]. One can then translate a typing derivation x1 : A1, . . . , xn :
An ⊢t : B into the formula ?(A∗
1)⊥, . . . , ?(A∗
n)⊥, B∗, where A∗is deﬁned on the
base type as the identity and (A →B)∗= !A∗⊸B∗. Following this perspective,
linear logic is a good ﬁt for the study of λ-calculus and their extensions with a
well-studied semantics [Mel09].
3.2.1 . Proof Nets
In Linear Logic, some rules are said to be invertible: they do not change
the provability of the sequent. An instance of such a rule is the `: a ` rule
can always be applied before, or after, another rule. This cause of problem of
identifying multiple proofs of the same formula that are equivalent up to some rule
permutation. To solve this problem, Linear Logic comes with another syntax for
proofs: a graphical language called proof nets [Gir87]. Proofs nets are deﬁned in
two steps: ﬁrst, the general graphical language call proof structures and then a
validity criterion that restraint the proof structure to only those that correspond
to real proofs of linear logic. As with linear logic, there exists multiple kinds of
proof nets, depending on which fragment of linear logic we are interested in. Most
notably, the additive fragment as with the units have always created problems
in deﬁning a proper validity criterion that matches properly the cut-elimination
procedure of their sequent-calculus counterpart. For the multiplicative fragment,
the proof-structures are deﬁned as:
(
ax
A⊥
A,
cut
A
A⊥,
A
B
A⊗B
,
`
A
B
A`B
)
The constructions of the proof-structures correspond respectively to the axiom,
cut, tensor and par rule of MLL. Then, each derivation judgement from MLL can
be sent into a proof structure. As discussed, not every proof structure is a proof
45

ax
⊢A, A⊥
⊢A, Γ cut
⊢A, Γ
⇝
⊢A, Γ
⊢Γ, A
⊢∆, B ⊗
⊢Γ, ∆, A ⊗B
⊢Ξ, A⊥, B⊥
`
⊢Ξ, A⊥` B⊥
cut
⊢Γ, ∆, Ξ
⇝
⊢Γ, A
⊢∆, B
⊢Ξ, A⊥, B⊥
cut
⊢∆, Ξ, A⊥
cut
⊢Γ, ∆, Ξ
⊢Γ, A1
⊢Γ, A2 &
⊢Γ, A1 & A2
⊢∆, A⊥
k
⊕k
⊢∆, A⊥
1 ⊕A⊥
2 cut
⊢Γ, ∆
⇝
⊢Γ, Ak
⊢∆, A⊥
k cut
⊢Γ, ∆
for k ∈{0, 1}
⊢?Γ, A !
⊢?Γ, !A
⊢∆
w
⊢∆, ?A⊥
cut
⊢?Γ, ∆
⇝
⊢∆
w
⊢?Γ, ∆
⊢?Γ, A !
⊢?Γ, !A
⊢∆, A⊥
d
⊢∆, ?A⊥
cut
⊢?Γ, ∆
⇝
⊢?Γ, A
⊢∆, A⊥
cut
⊢?Γ, ∆
⊢?Γ, A !
⊢?Γ, !A
⊢∆, ?A⊥, ?A⊥
c
⊢∆, ?A⊥
cut
⊢?Γ, ∆
⇝
⊢?Γ, A !
⊢?Γ, !A
⊢?Γ, A !
⊢?Γ, !A
⊢∆, ?A⊥, ?A⊥
cut
⊢?Γ, ∆, ?A⊥
cut
⊢?Γ, ?Γ, ∆
c
⊢?Γ, ∆
⊢?Γ, A !
⊢?Γ, !A
⊢?∆, ?A⊥, B !
⊢?∆, ?A⊥, !B cut
⊢?Γ, ?∆, !B
⇝
⊢?Γ, A !
⊢?Γ, !A
⊢?∆, ?A⊥, B cut
⊢?Γ, ?∆, B !
⊢?Γ, ?∆, !B
1
⊢1
⊢Γ
⊥
⊢Γ, ⊥cut
⊢Γ
⇝
⊢Γ
Figure 3.4: Cut-elimination rules
46

net, for instance the following proof structure does not correspond to any proof of
linear logic:
ax
A
A⊥
A⊥⊗A
This is due to the fact that the tensor has two premisses and in this proof structure
it only has one. Therefore, proof nets come with a validity criterion. We do not go
into all the details as it is not necessary for this thesis, but intuitively the validity
condition considers a switching function S : ` →{l, r} which consist in removing
one of the two input wire of every ` node in the proof structure. The obtained
graph is called a switching graph and then, the validity criterion require that every
switching graph that can be obtained from a proof structure is a connected tree.
Similarly, the cut-elimination procedure can also be deﬁned inside proof nets:
`
A⊥`B⊥
A⊗B cut
A
B A⊥B⊥
cut
A
A⊥
cut
B
B⊥
→
ax
A
A⊥
cut
A
→
A
Finally, one can show that cut-elimination holds inside proof nets and that any
proof of linear logic gives a proof net, and that every proof net is the image of a
proof [Gir87; Lau11]. More details can be found in [Lau11].
MELL Proof Nets
MELL allows one to encode all of the simply typed λ-
calculus, duplication of a λ-term is represented using exponentials, which also
handle erasure. To make proof nets for MELL, one then needs a tool to represent
what can, and cannot, be duplicated or erased. For that, the most common tool
is exponential boxes. In MELL Proof Nets, proofs nets can be put inside boxes,
which indicate that they can be duplicate or erased under certain conditions. The
erasure is made through weakening and may create a situation where the proof
net is disconnected. This creates problems in deﬁning the validity condition. One
solution would be to consider MELL + Mix, another way is through the use of
jumps, connecting weakening to a correct proof net. Once again, more details can
be found in [Lau11].
MALL Proof Nets
On the additive side, correctness criterion becomes much
harder. The main criterion of MALL Proof Nets [HVG03] works with additive reso-
lutions: deleting one branch of each ⊕, &-node and through a notion of toggling of
&-node. A toggle-& creates some kind of dependency between himself and other
part of the proof-nets through jump-edges. One then needs to check a similar
criterion to the MLL criterion: toggled & must not be present in a cycle of the
proof-net with the added jump-edges.
47

3.2.2 . Geometry of Interaction
Proof Nets, in particular MLL ones, comes with another form of semantics
called the Geometry of Interaction (GoI) [Gir89b; Gir89a; Gir88; Gir95; Gir06;
Gir11; Gir13]. The Geometry of Interaction can be seen in two ways:
• As a wave-style semantics where the proof net is seen as a global operator;
• as a particle-style semantics (also called token-machine) where tokens travel
through the proof net, computing a global operator in a local manner.
Both view are related through the Execution Formula: a new, purely computa-
tional way to view the cut-elimination in an abstract way. In this thesis, we follow
the particle-style approach of GoI, following the notion of GoI has a token-based
automaton [DR99; AL95]. In this setting, the ﬂow of tokens inside a proof net,
seen as a graph, characterizes an invariant of the proof: its computational con-
tent. A token carry a state, which is being updated as the token goes up or down
through nodes of the proof net. In this setting, applying the execution formula n
times can be seen as moving the token n times through the proof net, and hence,
by iterating the execution formula, the token always ends up leaving the proof net.
Then, proof nets are seen as a global operation on the state space of tokens. For
the case of MLL, this corresponds simply to a permutation. Due to the lineary of
the logic and the fact that each operation is forward and backward deterministic
(each step corresponding to simply going down, or up, on the right, or left, side of
a node), each step of the token machine is reversible.
3.3 . Linear Logic & Quantum Computation
The fact that linear logic doesn’t allow for duplication or erasure of data is
very reminiscent of the constraints of quantum computation. Both linear logic and
quantum computation features a tensor, and the exponential can be used for typing
classical data, which can be duplicated and erased at will. Multiple typed languages
have been developed, along with their semantics in such a setting [PSV14; Gre+13;
SV+09; Lee+21]. More related to proof nets, major works in quantum compu-
tation has taken inspiration from MLL Proof Nets, most notably the Categorical
Quantum Theory [] and the ZX-Calculus [CD11] that grew out of it. However, the
connection is not perfect as in quantum computation the tensor is self-dual and
so the correctness condition of proof nets is no longer relevant, also there is no
reason for the obtained graph to be acyclic as the trace is a fundamental structure.
Finally, proof nets are oriented, which is no longer the case in the ZX-Calculus
as all objects are self-dual. More details can be found in [AD06; Dun06; Dun09;
Dun04]. Geometry of Interaction has also been applied in the context of quantum
computation, both in a categorical setting [HH16] or in token-based-GoI setting
such as in [Dal17; LZ15; DLF11], where the tokens are seen as qubits ﬂowing inside
a higher-order term, computing a quantum circuit. In this setting, tokens require
48

a notion of synchronization: a token arriving at an input of a gate is blocked until
all the inputs of the gates are populated by a token, at which point all the tokens
go through at once (while obviously changing the state).
3.4 . Inﬁnitary Linear Logic : µMALL
Functional programming languages often feature the ability to encode inductive
types and coinductive types as data structure. On a Curry-Howard point of view,
how inductive and coinductive types and reasoning are related to Linear Logic is
not directly clear.
From a proof theory point of view, inductive and coinductive reasoning have
been studied for a long time.
Most notably by the µ-calculus [DR79; Par69;
Koz83]: an extension of modal logic with ﬁxed point operator. Modal logic was
extended with a new formula of the form µX.A where A is a formula, along with
the dual operator νX.A. Already at this point, the µ and ν operator described
the least and greatest ﬁxed point operator. By the Curry-Howard correspondence,
those new logics helped in modelling possibly inﬁnite computation. Among those
logics and derivations, circular proofs [San02; FS13], inﬁnite proofs with ﬁnitely
many subtrees, are in particular interest to represent recursive programs. As we are
concerned with the particular case of Linear Logic, we look at the logic µMALL.
In his PhD, Baelde [Bae08] looked at adapting linear logic with formulas with
ﬁxed points and showed the cut-elimination and the equivalence of provability
with regards to higher order linear logic. Finally, Amina et al [BDS16; Dou17;
BDS16] looked more precisely at the inﬁnitary aspects of the derivation with a
proof-theorical approach, deﬁning validity criterions and their properties.
3.4.1 . Background on µMALL
The logic µMALL [Bae12; BDS16] is an extension of the additive and multi-
plicative fragment of linear logic [Gir87]. The syntax of linear logic, where formula
are denoted by ψ, φ, is extended with the formulas µX.ψ and its dual νX.ψ (where
X is a type variable occurring in ψ), which can be understood at the least and
greatest ﬁxed points of the operator X 7→ψ. These permits inductive and coin-
ductive statements. One can for instance deﬁne the type of natural numbers as
µX.1 ⊕X or of lists of type ψ as [ψ] = µX.1 ⊕(ψ ⊗X) or of streams of type ψ
as νX.ψ ⊗X. Note that our system only deals with closed formulas. The syntax
of formulas is
ψ, φ ::= α | α⊥| µX.ψ | νX.ψ | X
| ⊥| 1 | ψ `
φ | ψ ⊗φ | 000 | ⊤| ψ ⊕φ | ψ & φ.
Where α ∈A, an inﬁnite set of atoms and X, Y, · · · ∈V an inﬁnite set of
ﬁxed point variables.
Deﬁnition 3.4.1 (Negation). The negation of a formula, ψ⊥is the involution on
formulas satisfying α⊥⊥= α, (ψ`φ)⊥= (ψ⊥⊗φ⊥), (ψ⊕φ)⊥= ψ⊥&φ⊥, ⊥⊥=
1, ⊤⊥= 000, (νX.ψ)⊥= µX.ψ⊥, X⊥= X. Having X⊥= X is harmless since we
49

...
ν
⊢νX.X
...
µ
µX.X ⊢ψ cut
⊢ψ
Figure 3.5: Degenerated proof
only deal with closed formulas.
Following the notation of one-sided sequents, the rules for the µ and ν con-
nectives are:
⊢ψ[X ←µX.ψ], ∆
⊢µX.ψ, ∆
µ
⊢ψ[X ←νX.ψ], ∆
⊢νX.ψ, ∆
ν
The logic allows for the constructions of inﬁnite derivations, called pre-proofs.
Their name comes from the fact that, if we consider that any derivation is a proof,
then we can prove any statement ψ using the cut-rule, as shown in Figure 3.5.
This is why µMALL comes with a validity criterion, separating pre-proofs from
actual proofs. µMALL also comes with a boolean semantics [BDS16; Dou17] on
formulas, noted J−K, giving us information on whenever or not a formula can be
deemed provable. They proved that if a sequent ⊢Γ is provable then JFK = tt
for some formula F ∈Γ.
In order to distinguish between pre-proofs and actual proofs, µMALL comes
with validity criterion on derivations: mainly, whether or not each inﬁnite branch
can be justiﬁed by a form of coinductive reasoning. The criterion also ensures that
the cut-elimination procedure holds. The criterion is based on Girard’s Geometry
of Interaction where some data (namely a thread) move through the derivation,
following a subformula and collecting information (its weight).
Then, one can
analyse the collected information and determine whenever or not it is valid. More
formally, a thread [BDS16; Bae+22] is an inﬁnite sequence of tuples of formulas,
sequents and directions (either up or down) written (F; ⊢∆; d). Intuitively, these
threads follow some formula starting from the root of the derivation and start by
going up. The thread has the possibility to bounce on axioms and cuts and change
its direction, either going back-down on an axioms or back-up on a cut. A thread
will be called valid when it is non-stationary (does not follow a formula that is
never a principal formula of a rule), and when in the set of formulas appearing
inﬁnitely often, the minimum formula (according to the subformula ordering) is a
ν formula. For the multiplicative fragment, we say that a pre-proof is valid if for all
inﬁnite branches, there exists a valid thread, while for the additive part, we require
a notion of additive slices and persistent slices, and we ask that all persistent slices
are valid in the sence of the multiplicative fragment.
50

Multiple validity criterion exists, some covering more derivation than others.
One thing that has to be noticed is that, even though a valid proof is productive in
the sense of the cut-elimination, not all productive derivation are valid proofs. One
on hand [NST18] introduced a local condition for circular proofs validity. In their
system µMALL
↷
, every valid proof is a valid proof in µMALL, while the conserve is
not true: interleaving of ﬁxed-point and back-edge are not captured by their system.
The system of thread previously mentioned is developed in [BDS16] and extended
in [Bae+22] and called the bouncing-thread-validity. Compare to [NST18] the
criterion is a global one that looks at inﬁnite threads. This is the criterion that we
present here and that we will use in Chapter 4.
3.4.2 . Bouncing Validity
We consider sequents to work with sets of named formulas, also called for-
mula occurrences. The idea is that each formula is attached a unique address,
when a rule is applied to said formula, its subformulas will extend their addresses,
corresponding to their provenance by {l, r, i} (for left, right, inside).
Deﬁnition 3.4.2 (Addresses). Let Afresh be an inﬁnite set of atomic adddresses
and A⊥
fresh = {α⊥
| α ∈Afresh} and Σ = {l, r, i}. An address is a word of
the form α · x where α ∈Afresh ∪A⊥
fresh and x ∈Σ∗. Given two addresses α′, α
we say that α′ is a sub-address of α when α is a preﬁx of α′, noted a ⊑α′. Two
addresses are disjoint if they are incompatible with regard to ⊑.
We can now deﬁne the notion of formula occurrence F, G, . . . as a simple
pair (ψ, α) where ψ is a formula and α is an address, noted F = ψα. We say
that two formula occurrences are structurally equivalent, noted ψα ≡φβ when the
underlying formulas are the same: ψ = φ.
As we will work with formula occurrences, logical connectives need to be lifted
on occurrences:
Deﬁnition 3.4.3 (Logical Connectives with occurrences).
• For any # ∈{⊗, ⊕, `, &} if F = ψαl and G = φαr then F#G = (ψ#φ)α.
• For any # ∈{µ, ν} if F = ψαi then #X.F = (#X.ψ)α.
The derivation rules are shown in Figure 3.6. They deﬁne a relation ⊢∆on
a set of formula occurrences deﬁned co-inductively. For each rule the assumptions
are above the line while the conclusion is under. In the rules, the comma stands
for the disjoint union. Given a rule, we call the formula to which the rule is applied
to the active formula or principal formula. Given a sequent s in a pre-proof π, we
denote by premiss(s) the set of premisses of the rule of conclusion s in π. The
cut-elimination rules for µMALL are the same as the one presented in Figure 3.4
with the additional rule
51

F ≡G
⊢F ⊥G id
⊢∆, F
⊢Γ, F ⊥
⊢∆, Γ
cut
⊢∆
⊢⊥, ∆⊥
⊢1 1
⊢F, G, ∆
⊢F ` G, ∆`
⊢F, ∆
⊢G, Γ
⊢F ⊗G, ∆, Γ
⊗
⊢F, ∆
⊢G, ∆
⊢F & G, ∆
&
⊢Fi, ∆
⊢F1 ⊕F2, ∆⊕i
R i ∈{1, 2}
∆
⊢∆, ⊥⊥
⊢F[X ←µX.F], ∆
⊢µX.F, ∆
µ
⊢F[X ←νX.F], ∆
⊢νX.F, ∆
ν
Figure 3.6: Rules for µMALL.
⊢F ⊥[X ←νX.F ⊥], Γ
µ
⊢µX.F ⊥, Γ
⊢F[X ←νX.F], ∆
ν
⊢νX.F, ∆cut
⊢∆, Γ
⇝
⊢F ⊥[X ←νX.F ⊥], Γ
⊢F[X ←νX.F], ∆
cut
⊢∆, Γ
Derivations can be potentially non-well-founded trees: they are not necessarily
ﬁnite as we can for instance consider the formula µX.X and apply the rule µ an
inﬁnite number of times.
Example 3.4.4. Taking the natural number N = µX.1 ⊕X, one can deﬁne the
proof of 0 and of n as:
π0 =
⊢1 1
⊢1 ⊕N ⊕1
⊢µX.1 ⊕X µ
πn =
πn−1
⊢N
⊢1 ⊕N ⊕2
⊢µX.1 ⊕X µ
As mentioned earlier, not all derivation are indeed proofs.
To answer this
problem, µMALL comes with a validity criterion for derivations. This makes uses
of the notion of bouncing-threads: paths that travel along the inﬁnite derivation
and collect some information along the way. In order to formally deﬁne bouncing-
threads we ﬁrst need to introduce some notations: given an alphabet Σ, we denote
by Σ∞the set of inﬁnite words over Σ and Σω = Σ∗∪Σ∞. The letter ϱ will denote
ordinals in ω+1. Finally, we use a special concatenation: given u = (ui)i≤n≤ω and
v = (vi)i∈ϱ such that un = v0, we deﬁne u ⊙v as the concatenation of u and v
without the ﬁrst element of v, i;e : u·(vi)i∈ϱ\{0}. For instance aab⊙bac = aabac.
We begin with the deﬁnition of a pre-threads : the basic construction of a path
along an inﬁnite tree that follows a formula occurrence. Then, we only look at
some specials ones called threads, and ﬁnally deﬁne the notion of valid thread that
52

validates an inﬁnite derivation as being a proof. A pre-proof is a proof whenever
all of its inﬁnite branches are valid.
Deﬁnition 3.4.5 (Pre-thread). A pre-thread is a sequence (Fi, si, di)i∈ϱ of tuples
of a formula, a sequent and a direction d ∈{↑, ↓} such that for all i ∈ϱ and
i + 1 ∈ϱ one of the following holds:
• di = di+1 =↑, si+1 ∈premiss(si) and Fi+1 ⊑Fi
• di = di+1 =↓, si ∈premiss(si+1) and Fi ⊑Fi+1
• di =↓, di+1 =↑, si and si+1 are the two premisses of the same cut rule and
Fi = F ⊥
i+1
• di =↑, di+1 =↓and si = si+1 = {⊢Fi, Fi+1} is the conclusion of an axiom
rule.
As mentioned before, a pre-thread follows a subformula inside a derivation,
going up or down and bouncing back on axioms rules and cut rules.
We can
then deﬁne a notion of weight on a pre-thread: a potentially inﬁnite word over
{l, r, i, l, r, i, W, A, C} where l, r, i stands for being on the left, right or inside a
subformula while the pre-thread is going up, l, r, i is similar but while the pre-
thread is going down and W, A and C stands respectively for wait, axiom and cut:
if a pre-thread is going up on a formula that is not the principal formula, then the
weight becomes W, and when the pre-thread bounces back on an axiom (resp. a
cut), its weight becomes A (resp. C).
Deﬁnition 3.4.6 (Weight of a pre-thread). Let t be a pre-thread, the weight of t,
noted w(t) is a word over (wi)i∈ϱ{l, r, i, l, r, i, W, A, C}∞such that for every i ∈ϱ
one of the following holds:
• wi = x if Fi = ψα and Fi+1 = φαx for x ∈{l, r, i}
• wi = x if Fi = ψαx and Fi+1 = φα for x ∈{l, r, i}
• wi = A if di =↑and di+1 =↓(corresponding to an axiom rule)
• wi = C if di =↓and di+1 =↑(corresponding to a cut-rule).
• wi = W if Fi = Fi+1 (corresponding to the fact that a rule is not applied to
the formula followed by the pre-thread).
As mentioned earlier we are not interested in every single pre-thread, but only
those that follow a certain pattern (one that matches with the cut-elimination
procedure).
We deﬁne two set of words B and H inductively as follows:
B ::= C | BW∗AW∗B | xW∗BW∗x
53

H ::= ϵ | AW∗B
A ﬁnite pre-thread t is called a b-path if w(t) ∈B, and it is called a h-path if
w(t) ∈H.
We are now ready to deﬁne the notion of thread:
Deﬁnition 3.4.7 (Thread). A pre-thread t is a thread when it can be decomposed
as J
i∈ϱ+1(Hi ⊙Vi) where for all i ∈ϱ + 1:
• w(Vi) ∈{l, r, i, W}∞, and it is non-empty if i ̸= ϱ
• w(Hi) ∈H, and it is non-empty if i ̸= 0
The decomposition can be read as a thread initially going up, accumulating
some debt in the form of the alphabet {l, r, i} that will need to be repaid by their
opposite {l, r, i} when going down after meeting an axiom until it reaches a cut.
Those dual alphabets correspond to steps of the cut-elimination: making sure that
the correct formulas will at some point interact by a cut.
Such a decomposition is unique, and we call (Vi)i∈ϱ+1 the visible part of t and
(Hi)i∈ϱ+1 the hidden part. A thread is stationary when its visible part is a ﬁnite
sequence (of ﬁnite words) or when there exists k ∈ϱ+1 such that w(Vi) ∈{W}∞
for all k ≤i ∈ϱ + 1.
We can now deﬁne the validity criterion on threads:
Deﬁnition 3.4.8 (Valid threads). If we take the sequence of formulas followed by
a non-stationary thread on its visible part and skipping the steps corresponding to
W weights, we obtain an inﬁnite sequence of formulas where each formula is an
immediate subformula or an unfolding of the previous formula. The formulas ap-
pearing inﬁnitely often admit a minimum with regard to the subformula ordering,
such a formula is the minimal formula of the thread.
A non-stationary thread is valid if its minimal formula is a ν formula.
We then need to import this notion of valid threads to pre-proofs. For this we
split it into two cases: the ﬁrst one being for µMLL: the multiplicative fragment
of µMALL, and then for the whole of µMALL.
Deﬁnition 3.4.9 (Validity of µMLL). A pre-proof of µMLL is valid if every inﬁnite
branch is valid.
We say than an inﬁnite branch β is valid if there exists a valid thread starting
from one of its sequents, whose visible part is contained in this branch.
Example 3.4.10. The inﬁnite derivation
...
⊢µX.X µ
⊢µX.X µ is not valid as the only thread
t = (µX.X; ⊢µX.X; ↑)∞have for weight i∞which is stationary.
54

This notion is not suﬃcient for the whole of µMALL mainly due to the dupli-
cation of proofs that arise from the cut-elimination of the & connective (see Fig-
ure 3.4). We need to take into accounts slices, we consider two new rules :
⊢∆, F
⊢∆, F & G &1
⊢∆, G
⊢∆, F & G &2
Now, given a pre-proof π of µMALL we consider the set of its slices Sl(π), deﬁned
corecursively by:
Sl





π1
⊢∆, F1
π2
⊢∆, F2
⊢∆, F1 & F2
&




≜











π′
i
⊢∆, Fi
⊢∆, F1 & F2 &i | i ∈{1, 2}, π′
i ∈Sl(πi)











Equation Section 3.4.2 means that for each rule & in π, we consider the two proofs
where the rule & is replaced with &1 and &2, similarly to the switching of proof
nets.
Deﬁnition 3.4.11 (Persisting slice). Given some slice, a rule &i of principal for-
mula F1 & F2 is well-sliced if no b-path starting from F1 & F2 ends in a formula
F ⊥
1 ⊕F ⊥
2 that is the principal formula of a ⊕j rule with i ̸= j. A slice is persistent
if all its &i are well-sliced.
Remark 3.4.12. An additional rule is needed to account for what happens when
a πi and a ⊕interact the cut-elimination. But as we just want to deﬁne the validity
criterion, we do not detail the additional rule. The interested reader can always
refer to [Bae+22].
Deﬁnition 3.4.13 (Persistent slice validity). A persistent slice is valid if it is valid
in the sense of µMLL.
Deﬁnition 3.4.14 (µMALL validity). A pre-proof of µMALL is valid if all its per-
sistent slices are valid.
Example 3.4.15. Remember that N = µX.1 ⊕X. We can then deﬁne the suc-
cessor function as:
πsucc =
1
⊢1
⊕1
⊢1 ⊕N µ
⊢N
⊥
1 ⊢N
πsucc
N ⊢N
⊕2
N ⊢1 ⊕N µ
N ⊢N &
1 ⊕N ⊢N µ
N ⊢N
55

In this derivation, all the slices from &1 are ﬁnite derivation. The only inﬁnite
derivation becomes the one where all occurrences of & becomes &2. In this inﬁnite
branch we have two threads: the one on the left and the one on the right of the
sequent.
The left-thread tl have for weight (irWW)∞and the set of its formulas en-
countered inﬁnitely often are {νX.⊥& X, ⊥& νX.⊥& X}. The smallest formula
is a ν formula, validating the branch.
On the other hand, the right-thread has weight (WWir)∞and for smallest
formula a µ formula and hence is not valid. Since we only require one valid thread
per inﬁnite branch, the whole derivation is proof.
3.4.3 . Circular Representation of Derivations
Among the inﬁnite derivations that µMALL oﬀer we can look at the circular
ones: an inﬁnite derivation is circular if it has ﬁnitely many diﬀerent subtrees.
The circular derivation can therefore be represented in a more compact way with
the help of back-edges: arrows in the derivation that represent a repetition of
the derivation.
Derivations with back-edges are represented with the addition
of sequents marked with a back-edge label, noted ⊢f, and an additional rule,
⊢Σ be(f), which represents a back-edge pointing to the sequent ⊢f. We take the
convention that from the root of the derivation from to rule be(f) there must be
exactly one sequent annotated by f.
While a circular proof has multiple ﬁnite representations (depending on where
the back-edge is placed), they can all be mapped back to the same inﬁnite deriva-
tion via an inﬁnite unfolding of the back-edge and forgetting the back-edge labels:
Deﬁnition 3.4.16 (Unfolding). We deﬁne the unfolding of a circular derivation
P with a valuation v from back-edge labels to derivations by:
• U

P :
P1, . . . , Pn
⊢Σ
r, v

=
U(P1, v), . . . , U(Pn, v)
⊢Σ
r
• U(be(f), v) = v(f)
• U

P :
P1, . . . , Pn
⊢f Σ
r , v

=

π =
U(P1, v′), . . . , U(Pn, v′)
⊢Σ
r

with v′(g) =
π if g = f, else v(g).
Example 3.4.17. On the left, an inﬁnite derivation and on the right two circular
representation of the left-most derivation. Notice that the unfolding of the second
and third derivation are equal to the ﬁrst derivation.
...
µ
⊢µX.X µ
⊢µX.X
be(f)
⊢µX.X
µ
⊢f µX.X
be(f)
⊢µX.X µ
⊢µX.X
µ
⊢f µX.X
56

Part II
Contributions
57


4 - A Curry-Howard Correspondence for
Linear, Reversible Computation
Abstract
In this chapter, we present a linear and reversible programming lan-
guage with inductive types and recursion. The semantics of the lan-
guage is based on pattern-matching; we show how ensuring syntac-
tical exhaustivity and non-overlapping of clauses is enough to ensure
reversibility. The language allows to represent any Primitive Recursive
Function. We then give a Curry-Howard correspondence with the logic
µMALL, that is linear logic extended with least ﬁxed points allowing
inductive statements. The critical part of our work is to show how
primitive recursion yields circular proofs that satisfy µMALL validity
criterion and how the language simulates the cut-elimination proce-
dure of µMALL.
References: Results of this chapter have been accepted in the paper
Towards a Curry-Howard Correspondence for Linear, Reversible Com-
putation at CSL’23 and accepted as a Work in Progress at RC’20 [CSV20].
4.1 . Introduction
Reversible computation is a paradigm of computation which emerged as an
energy-preserving model of computation in which data is never erased [FT82] that
makes sure that, given some process f, there always exists an inverse process f−1
such that f ◦f−1 = Id = f−1 ◦f. Many aspects of reversible computation have
been considered, such as the development of reversible Turing Machines [AG11a;
MY07], reversible programming languages [JS14] and their semantics [CLV21;
KAG17; KR21]. However, the formal relationship between a logical system and
a computational model has not been developed yet.
While multiple reversible functional programming languages have been developed [YAG12;
TA15; JS14; SVV18; JKT18] featuring type systems, case-analysis and pattern-
matching, they most often lack a formal connection with logical systems. This
chapter aims at proposing a type system featuring inductive types for a purely
linear and reversible language, together with a Curry-Howard correspondence with
the logic µMALL. As reversible and linear computation make for a subset of quan-
tum computation, this work is a ﬁrst step towards understanding purely quantum
recursive types. We base our study on the approach presented in [SVV18]. In this
model, reversible computation is restricted to two main types: the tensor, written
A ⊗B, and its neutral element 1, and the co-product, written A ⊕B. The former
corresponds to the type of all pairs of elements of type A and elements of type
59

B, while the latter represents the disjoint union of all elements of type A and
elements of type B. For instance, a bit can be typed with 1 ⊕1, where 1 is a
type with only one element. The language in [SVV18] oﬀers the possibility to code
isos —reversible maps— with pattern-matching. An iso is for instance the swap
operation, typed with A⊗B ↔B ⊗A. However, if [SVV18] hints at an extension
towards pure quantum computation, the type system is not formally connected to
any logical system.
The problem of reversibility between ﬁnite types of same cardinality simply
requires to check that the function is injective. That is no longer the case when
we work with types of inﬁnite cardinality such as natural numbers.
The main contribution of this work is a Curry-Howard correspondence for a
purely reversible typed language in the style of [SVV18], with added generalized
inductive types and terminating recursion, enforced by the fact that recursive func-
tions must be structurally recursive: each recursive call must be applied to a
decreasing argument. We show how requiring exhaustivity and non-overlapping
of the clauses of the pattern-matching is enough to ensure reversibility and that
the obtained language can encode any Primitive Recursive function [RJ87]. For
the Curry-Howard part, we capitalize on the logic µMALL [BDS16; Bae12]: an
extension of the additive and multiplicative fragment of linear logic with least and
greatest ﬁxed points allowing inductive and coinductive statements. This logic
contains both a tensor and a co-product, and its strict linearity makes it a good ﬁt
for a reversible type system. In the literature, multiple proofs systems have been
considered for µMALL, some ﬁnitary proof system with explicit induction inferences
à la Park [Bae12] as well as non-well-founded proof systems which allow to build
inﬁnite derivations [BDS16; Bae+22]. In the former, the inference rule features an
invariant of the operator. While in the latter, no invariant is explicitly featured in
the proof system and one can consider inﬁnite derivation, but then the obtained
logic is inconsistent and require a validity criterion. The present chapter focuses
on the latter. In general, an inﬁnite derivation is called a pre-proof and is not nec-
essarily consistent. To solve this problem µMALL comes equipped with a validity
criterion, telling us when an inﬁnite derivation can be considered as a logical proof.
We show how the syntactical constraints of being structurally recursive imply the
validation of pre-proofs. In [CLV21], together with Louis Lemonnier and Benoît
Valiron we also provided a categorical semantics of a simpliﬁed version of the lan-
guage presented in this chapter. As it was the main work of Louis Lemonnier, we
do not present it here, but the interested reader can refer to [CLV21].
Organization of the chapter
The chapter is organized as follows: in
Section 4.2 we present the language, its syntax, typing rules and semantics and
show that any function that can be encoded in our language represents an iso-
morphism. In Section 4.3 we show that our language can encode any Primitive
Recursive Function [RJ87], this is shown by encoding the set of Recursive Primitive
60

Permutations [PPR20] functions. Then in Section 4.4, we develop on the Curry-
Howard Correspondence part: we show, given a well-typed term from our language,
how to translate it into a circular derivation of the logic µMALL and show that
the given derivation respects the validity condition and how our evaluation strategy
simulates the cut-elimination procedure of the logic. Finally, in Section 4.5 we look
at the expressiveness of the language if we remove the exhaustivity and termination
condition and show that we obtain Turing Completeness.
4.2 . First-order Isos
Our language is based on the one introduced by Sabry et al [SVV18] which
deﬁnes isomorphisms between various types, including the type of lists. We build on
the reversible part of the chapter by extending the language to support both a more
general rewriting system and more general inductive types. The language is deﬁned
by layers. Terms and types are presented in Table 4.1, while typing derivations,
inspired from µMALL, can be found in Tables 4.2 and 4.3. The language consists
of the following pieces.
Basic type.
They allow us to construct ﬁrst-order terms. The constructors injl
and injr represent the choice between either the left or right-hand side of a type of
the form A⊕B; the constructor ⟨, ⟩builds pairs of elements (with the corresponding
type constructor ⊗); fold represents inductive structure of the types µX.A. A
value can serve both as a result and as a pattern in the deﬁning clause of an
iso. We write x1, . . . , xn for ⟨x1, ⟨. . . , xn⟩⟩or −→x when n is non-ambiguous and
A1 ⊗· · · ⊗An for A1 ⊗(· · · ⊗An) and An for A ⊗· · · ⊗A
|
{z
}
n times
.
First-order isos.
An iso of type A ↔B acts on terms of base types. An iso is
a function of type A ↔B, deﬁned as a set of clauses of the form {v1 ↔e1 |
· · · | vn ↔en}. In the clauses, the tokens vi are open values and ei are expres-
sions. In order to apply an iso to a term, the iso must be of type A ↔B and
the term of type A. In the typing rules of isos, the ODA({v1, . . . , vn}) predicate
(adapted from [SVV18] and given in Table 4.4) stands for Orthogonal Decom-
position and syntactically enforces the exhaustivity and non-overlapping condi-
tions on a set of well-typed values v1, . . . , vn of type A. Exhaustivity for an iso
{v1 ↔e1 | · · · | vn ↔en} of type A ↔B means that the expressions on the
left (resp. on the right) of the clauses describe all possible values for the type A
(resp. the type B). Non-overlapping means that two expressions cannot match
the same value. For instance, the left and right injections injl v and injr v′ are
non-overlapping while a variable x is always exhaustive. The typing conditions
make sure that both the left-hand-side and right-hand-side of clauses satisfy this
condition. Its formal deﬁnition can be found in Table 4.3 where V al(e) is de-
ﬁned as V al(let p = ω p′ in e) = V al(e), and V al(v) = v otherwise. These
checks are crucial to make sure that our isos are indeed reversible. One can note
61

that the deﬁnition is sound, but not complete: not every set of exhaustive and
non-overlapping values satisﬁed the OD predicate, but it captures enough for the
expressivity results of Theorem 4.3.7. In the rule ODA⊗B, we deﬁne S1
v and S2
v
respectively as {w | ⟨v, w⟩∈S} and {w | ⟨w, v⟩∈S} and π1(S) and π2(S)
respectively as {v | ⟨v, w⟩∈S} and {w | ⟨v, w⟩∈S}. The construction ﬁx g.ω
represents the creation of a recursive function, rewritten as ω[g ←ﬁx g.ω] by
the operational semantics. Each recursive function needs to satisfy a structural
recursion criterion, formalized in Deﬁnition 4.2.2. It ensures that one of the in-
put arguments strictly decreases on each recursive call. Indeed, since isos can be
non-terminating (due to recursion), we need a criterion that implies termination
to ensure that we work with total functions. If ω is of type A ↔B, we can
build its inverse ω⊥: B ↔A and show that their composition is the identity.
We recall that in order to avoid conﬂicts between variables, we will always work
up to α-conversion and use Barendregt’s convention [Bar84, p.26] which consists
in keeping all bound and free variables names distinct, even when this remains
implicit.
Convention 4.2.1. We assume that the type constructors are righ-associative,
hence fold injl x is fold (injl (x)).
The type system has two types of judgments: one for terms (noted ∆; Ψ ⊢e
t : A) and one for isos (noted Ψ ⊢ω ω : A ↔B). In the typing rules, the contexts
∆are sets of pairs that consist of a term-variable and a base type, where each
variable can only occur once and Ψ is a singleton set of a pair of an iso-variable
and an iso-type association.
Deﬁnition 4.2.2 (Structurally Recursive). Given an iso ﬁx f.{v1 ↔e1 | · · · |
vn ↔en} of type A1 ⊗· · · ⊗Am ↔C, it is structurally recursive if there is
1 ≤j ≤m such that Aj = µX.B and for all i ∈{1, . . . , n} we have that vi is of
the form (v1
i , . . . , vm
i ) such that vj
i is either:
• A closed value, in which case ei does not contain the subterm f p;
• Open, in which case for all subterms of the form f p in ei we have p =
(x1, . . . , xm) and xj : µX.B is a strict subterm of vj
i .
Given a clause v ↔e, we call the value vj
i (resp. the variable xj) the decreasing
argument (resp. the focus) of the structurally recursive criterion.
Example 4.2.3. The iso ﬁx f.{x ↔let y = f x in y} is not structurally recursive
while the one from Example 4.2.14 is.
Remark 4.2.4. As we are focused on a very basic notion of structurally recur-
sive function, the typing rules of isos allow to have at most one iso-variable in the
context, meaning that we cannot have intertwined recursive calls.
62

(Base types)
A, B ::= 1 | A ⊕B | A ⊗B | µX.A
(Isos, ﬁrst-order)
α ::= A ↔B
(Values)
v ::= () | x | injl v | injr v | ⟨v1, v2⟩| fold v
(Pattern)
p ::= x | ⟨p1, p2⟩
(Expressions)
e ::= v | let p1 = ω p2 in e
(Isos)
ω ::= {v1 ↔e1 | · · · | vn ↔en} | ﬁx f.ω | f
(Terms)
t ::= () | x | injl t | injr t | ⟨t1, t2⟩|
fold t | ω t | let p = t1 in t2
Table 4.1: Terms and types
∅; Ψ ⊢e () : 1
x : A; Ψ ⊢e x : A
∆; Ψ ⊢e t : A
∆; Ψ ⊢e injl t : A ⊕B
∆; Ψ ⊢e t : B
∆; Ψ ⊢e injr t : A ⊕B
∆1; Ψ ⊢e t1 : A
∆2; Ψ ⊢e t2 : B
∆1, ∆2; Ψ ⊢e ⟨t1, t2⟩: A ⊗B
∆; Ψ ⊢e t : A[X ←µX.A]
∆; Ψ ⊢e fold t : µX.A
Ψ ⊢ω f : A ↔B
∆; Ψ ⊢e t : A
∆; Ψ ⊢e f t : B
⊢ω ω : A ↔B
∆; Ψ ⊢e t : A
∆; Ψ ⊢e ω t : B
∆1; Ψ ⊢e t1 : A1 ⊗· · · ⊗An
∆2, x1 : A1, . . . , xn : An; Ψ ⊢e t2 : B
∆1, ∆2; Ψ ⊢e let (x1, . . . , xn) = t1 in t2 : B
Table 4.2: Typing of terms and expressions
Example 4.2.5. We can deﬁne the iso of type: A ⊕(B ⊕C) ↔C ⊕(A ⊕B) as



injl a
↔injr injl a
injr injl b ↔injr injr b
injr injr c ↔injl c



Finally, our language is equipped with a rewriting system →on terms, de-
ﬁned in Deﬁnition 4.2.9, that follows a deterministic call-by-value strategy: each
argument of a function is fully evaluated before applying the substitution. This is
done through the use of an evaluation context C[], as for the β-reduction deﬁned
in Section 3.1.1. Due to the deterministic nature of the strategy, we directly obtain
the unicity of the normal form. The evaluation of an iso applied to a value relies
on a notion of pattern-matching: the argument is matched against the left-hand-
side of each clause until one of them matches (written σ[v] = v′), in which case
the pattern-matching, as deﬁned in Deﬁnition 4.5, returns a substitution σ that
63

∆1 ⊢e v1 : A
. . .
∆n ⊢e vn : A
ODA({v1, . . . , vn})
∆1; Ψ ⊢e e1 : B
. . .
∆n; Ψ ⊢e en : B
ODB({V al(e1), . . . , V al(en)})
Ψ ⊢ω {v1 ↔e1 | · · · | vn ↔en} : A ↔B.
f : α ⊢ω f : α
f : α ⊢ω ω : α
ﬁx f.ω is structurally recursive
Ψ ⊢ω ﬁx f.ω : α
Table 4.3: Typing of isos
ODA({x})
OD1({()})
ODA(S)
ODB(T)
ODA⊕B({injl v | v ∈S} ∪{injr v | v ∈T})
ODA[X←µX.A](S)
ODµX.A({fold v | v ∈S})
ODA(π1(S)) and ∀v ∈π1(S), ODB(S1
v)
or ODB(π2(S)) and ∀v ∈π2(S), ODA(S2
v)
ODA⊗B(S = {⟨v1, v′
1⟩, . . . , ⟨vn, v′
n⟩})
Table 4.4: Exhaustivity and Non-Overlapping
sends variables to values.
Because we ensure exhaustivity and non-overlapping
(Lemma 4.2.7), the pattern-matching can always occur on well-typed terms. The
support of a substitution σ is deﬁned as supp(σ) = {x | (x 7→v) ∈σ}.
Deﬁnition 4.2.6 (Substitution). Applying substitution σ on an expression t, writ-
ten σ(t), is deﬁned, as:
• σ(()) = (),
• σ(x) = v if {x 7→v} ⊆σ,
• σ(injr t) = injr σ(t),
• σ(injl t) = injl σ(t),
• σ(⟨t, t′⟩) = ⟨σ(t), σ(t′)⟩,
• σ(ω t) = ω σ(t),
• σ(let p = t1 in t2) = (let p = σ(t1) in σ(t2)).
Lemma 4.2.7 (ODA(A) ensures exhaustivity and non-overlapping.).
Let ODA(S)
then for all close value v of type A, there exists a unique v′ ∈S and a unique σ
such that σ[v′] = v.
Proof. By induction on ODA(S):
64

σ[e] = e′
σ[injl e] = injl e′
σ[e] = e′
σ[injr e] = injr e′
σ = {x 7→e}
σ[x] = e
σ[e] = e′
σ[fold e] = fold e′
σ2[e1] = e′
1
σ1[e2] = e′
2
supp(σ1) ∩supp(σ2) = ∅
σ = σ1 ∪σ2
σ[⟨e1, e2⟩] = ⟨e′
1, e′
2⟩
σ[()] = ()
Table 4.5: Pattern-matching
• ODA({x}) and OD1({()}) are direct.
• ODA⊕B({injl v | v ∈SA} ∪{injr v | v ∈SB}) let v = injl ev then by
induction hypothesis on SA there exists v1 ∈SA such that σ1[v1] = ev,
hence σ1[injl v1] = injl ev. Now assume there also exists v2 ∈SA such
that σ2[v2] = ev, by induction hypothesis we have that v1 = v2 hence
injl v1 = injl v2.
Similar case for the injr .
• Similar case for the fold .
• Assuming we are in the ﬁrst case of the disjunction in the premise of
ODA⊗B, the other case being similar:
Take some value ⟨ev, ev′⟩, by induction hypothesis we know that there
exists some vi ∈{v1, . . . , vn} such that σi[vi] = ev and therefore that
there also exists some v′
i ∈S1
vi such that σ′
i[v′
i] = v′.
Now assuming that there exists another pair ⟨v1, v2⟩∈S such that
σ[⟨v1, v2⟩] = ⟨ev, ev′⟩, by induction hypothesis we know that v1 = vi and
that therefore v2 ∈S1
vi = S1
v1 so v2 = v′
i.
Deﬁnition 4.2.8 (Evaluation Contexts). The evaluations contexts C are deﬁned
as:
C ::= [ ] | injl C | injr C | ω C | let p = C in t | ⟨C, v⟩| ⟨v, C⟩
Deﬁnition 4.2.9 (Evaluation relation →). The rewriting system →is deﬁned as:
t1 →t2
C[t1] →C[t2] Cong
σ[p] = v
let p = v in t →σ(t) LetE
(ﬁx f.ω) →ω[f ←(ﬁx f.ω)] IsoRec
σ[vi] = v′
{v1 ↔e1 | · · · | vn ↔en} v′ →σ(ei) IsoApp
As usual we note →∗for the reﬂexive transitive closure of →.
65

As mentioned above, from any iso ω : A ↔B we can build its inverse
ω⊥: B ↔A, the inverse operation is deﬁned inductively on ω and is given in
Deﬁnition 4.2.10.
Deﬁnition 4.2.10 (Inversion). Given an iso ω, we deﬁne its dual ω⊥as:
• f⊥= f
• (ﬁx f.ω)⊥= ﬁx f.ω⊥
• {(vi ↔ei)i∈I}⊥= {((vi ↔ei)⊥)i∈I}
and the inverse of a clause as:


v1 ↔let p1 = ω1 p′
1 in
· · ·
let pn = ωn p′
n in v′
1


⊥
:=


v′
1
↔
let p′
n = ω⊥
n pn in
· · ·
let p′
1 = ω⊥
1 p1 in v1

.
We can show that the inverse is well-typed and behaves as expected:
Lemma 4.2.11 (Inversion is well-typed). If Ψ ⊢ω ω : A ↔B, then Ψ ⊢ω ω⊥:
B ↔A.
Proof. w.l.o.g. consider ω = {v1 ↔e1 | · · · | vn ↔en}, we look at one clause
in particular and its dual:


v1 ↔let p1 = ω1 p′
1 in
· · ·
let pn = ωn p′
n in v′
1


⊥
:=


v′
1
↔
let p′
n = ω⊥
n pn in
· · ·
let p′
1 = ω⊥
1 p1 in v1

.
By typing we know that ∆⊢e v1 : A and ∆; Ψ ⊢e let p1 = ω1 p′
1 in . . . v′
1 :
B
∆can be split into ∆1, . . . , ∆n, ∆n+1 and for all 1 ≤i ≤n we get that the
typing judgment of the expression let pi = ωi p′
i in . . . generates the new
typing judgment Γi+1
i
, . . . , Γn+1
i
. For all i we get that ∆i
Si−1
j=1 Γi
j ⊢e ω p′
i, ﬁnally
v′
1 is typed by ∆n+1
Sn
i=1 Γn+1
i
.
When typing the dual clause, we start with contexts ∆n+1
Sn
i=1 Γn+1
i
. We
have from hypothesis that:
• Each ω⊥
i pi is typed by Sn
j=i+1 Γn
j , which is possible by our typing hypoth-
esis.
• Each p′
i generates the contexts ∆i, Si
j=1 Γi
j.
At the end we end up with ∆1, . . . , ∆n, ∆n+1 ⊢e v1 which is typable by our
hypothesis.
In order to show that our isos are indeed isomorphisms, we need the following
lemma:
66

Lemma 4.2.12 (Commutativity of substitution). Let σ1, σ2 and v, such that σ1 ∪
σ2 closes v and supp(σ1) ∩supp(σ2) = ∅then σ1(σ2(v)) = σ2(σ1(v))
Proof. Direct by induction on v as σ1 and σ2 have disjoint support: In the
case where v = x then either {x 7→v′} ∈σ1 or {x 7→v′} ∈σ2 and hence
σ1(σ2(x)) = v′ = σ2(σ1(x)). All the other case are by direct induction hypoth-
esis as the substitutions enter the subterms.
Theorem 4.2.13 (Isos are isomorphisms). For all well-typed isos ⊢ω ω : A ↔B,
and for all well-typed values ⊢e v : A, if (ω (ω⊥v)) →∗v′ then v = v′.
Proof. By induction hypothesis on the size of ω:
• Case where ω = {v1 ↔v1 | . . . | vn ↔v′
n} then ω⊥(ω v0), by non-
overlapping and exhaustivity there exists a vi such that σ[vi] = v0 and
hence the term reduces to ω⊥σ(v′
i). It is clear that σ[v′
i] = σ(vi) and
hence the terms reduces to σ(vi), but by the ﬁrst pattern-matching we
know that σ(vi) = v0, which concludes the case.
• Case where ω = {v1 ↔e1 | · · · | vn ↔en},
for simplicity of writing we write a single clause:


v1 ↔let p1 = ω1 p′
1 in
· · ·
let pn = ωn p′
n in v′
1


⊥
:=


v′
1
↔
let p′
n = ω⊥
n pn in
· · ·
let p′
1 = ω⊥
1 p1 in v1

.
Take some closed value ⊢e v0 : A such that σ[v1] = v0.
By linearity, we can decompose σ into σ1, . . . , σn, σn+1 such that, after
substitution we obtain
let p1 = ω1 σ1(p′
1) in . . . let pn = ωn σn(p′
n) in σn+1(v′
1)
Given By Lemma 4.2.19, each let construction will reduce, and by the
rewriting strategy we get:
let p1 = ω1 σ1(p′
1) in
. . .
let pn = ωn σn(p′
n) in
σn+1(v′
1)
→
let p1 = v1 in
. . .
let pn = ωn σn(p′
n) in
σn+1(v′
1)
→
let p2 = ω2 γ2
1(σ1(p′
1)) in
. . .
let pn = ωn γn
1 (σn(p′
n)) in
σn+1(v′
1)
The ﬁnal term reduces to γn
n(. . . (γn
1 (σn+1(v′
1))) . . . ) and creates a new
substitution γi, the term will hence reduce to γn(. . . (γ1(σn+1(v′
1))) . . . ).
Let δ = ∪iγi ∪σn+1
We are left to evaluate
67



v′
1
↔
let p′
n = ω−1
n
pn in
· · ·
let p′
1 = ω−1
1
p1 in v1

δ(v′
1)
We get δ[v′
1] = δ(v′
1).
We know that each γi closes only pi, we can therefore substitute the
term as:
let p′
n = ωn γn(pn) in . . .let p′
1 = ω1γ1(p1) in σn+1(v′
1)
By induction hypothesis, Each let clause will re-create the substitution
σi, we know this as the fact that the initial let construction, let pi =
ωi σi(p′
i) in . . . reduces to let pi = vi in . . . While the new one, let p′
i =
ω⊥γi(pi) in . . ., is, by deﬁnition of the substitution the same as let p′
i =
ω⊥vi in . . .
Then, since we know that vi is the result of ω σi(p′
i), we get by induction
hypothesis σ(p′
i) as the result of the evaluation.
Therefore, after rewriting we obtain: σn(. . . (σ1(σn+1(v′
1))) . . . ). By Lemma 4.2.12
we get σ1(. . . (σn(σn+1(v′
1))) . . . ) which is equal to v.
Example 4.2.14. We give the encoding of the isomorphism map(ω) and its in-
verse: for any given iso ⊢ω ω : A ↔B in our language, we can deﬁne map(ω) :
[A] ↔[B] where [A] = µX.1 ⊕(A ⊗X) is the type of lists of type A and [ ] is the
empty list (fold (injl ())) and h :: t is the list construction (fold (injr ⟨h, t⟩)).
We also give its dual map(ω)⊥below, as given by Deﬁnition 4.2.10.
map(ω) = ﬁx f.







[ ]
↔[ ]
h :: t ↔let h′ = ω h in
let t′ = f t in
h′ :: t′







: [A] ↔[B]
map(ω)⊥= ﬁx f.







[ ]
↔[ ]
h′ :: t′ ↔let t = f t′ in
let h = ω⊥h′ in
h :: t







: [B] ↔[A]
Remark 4.2.15. In Example 4.2.5 and Example 4.2.14, the left and right-hand
side of the ↔on each function respect both the criteria of exhaustivity —
every-value of each type is being covered by at least one expression— and non-
overlapping —no two expressions cover the same value. Both isos are therefore
bijections.
The language enjoys the standard properties of typed languages of progress
and subject reduction, but requires a substitution lemma on both terms and isos:
68

Lemma 4.2.16 (Substitution Lemma of variables).
• For all values v1, . . . , vn, types A1, . . . , An and context ∆1, . . . , ∆n such that
for i ∈{1, . . . , n}, ∆i ⊢e vi : Ai.
• For all term t, type B and context Γ and variables x1, . . . , xn ∈FV(t) such
that Γ, x1 : A1, . . . , xn : An ⊢e t : B
Let σ = {x1 7→v1, . . . , xn 7→vn} then Γ, ∆1, . . . , ∆n ⊢e σ(t) : B
Proof. By induction on t.
• Case x, then Γ = ∅and we have σ = {x 7→v} for some v of type B under
some context ∆, then we get ∆⊢e σ(x) : B which leads to ∆⊢e v : B
which is typable by our hypothesis.
• Case (), nothing to do.
• Case injl t′, by substitution we have σ(injl t′) = injl σ(t′) and by typ-
ing we get
Γ, ∆1, . . . , ∆n ⊢σ(t′)
Γ, ∆1, . . . , ∆n ⊢injl σ(t′) which is typable by induction hypothesis on
t′.
• Case injr t′, fold t′, ω t′ are similar.
• Case ⟨t1, t2⟩, by typing we get that we can split Γ into Γ1, Γ2 and the
variables x1, . . . , xn are split into two parts for typing both t1 or t2 de-
pending on whenever or not a variable occurs freely in t1 or t2, w.l.o.g.
say that x1, . . . , xl are free in t1 and xl+1, . . . , xn are free in t2 then we
get:
Γ1, x1 : A1, . . . , xl : Al ⊢e t1 : B1
Γ2, xl+1 : Al+1, . . . , xn : An ⊢e t2 : B2
Γ1, Γ2, x1 : A1, . . . , xl : Al, xl+1 : Al+1, . . . , xn : An ⊢e ⟨t1, t2⟩: B1 ⊗B2
By substitution we get that σ(⟨t1, t2⟩) = ⟨σ(t1), σ(t2)⟩so we get the fol-
lowing typing derivation which is completed by induction hypothesis on
the subterms:
Γ1, ∆1, . . . , ∆l ⊢e t1 : B1
Γ2, ∆l+1, . . . , ∆n ⊢e t2 : B2
Γ1, Γ2, ∆1, . . . , ∆l, ∆l+1, . . . , ∆n ⊢e ⟨t1, t2⟩: B1 ⊗B2
• Case let p = t1 in t2 Similar to the case of the tensor.
Lemma 4.2.17 (Substitution Lemma Of Isos). If :
• ∆; f : α ⊢e t : A
• g : β ⊢ω ω : α
69

Then ∆; g : β ⊢e t[f ←ω] : A.
If :
• f : α ⊢ω ω1 : β
• h : γ ⊢ω ω2 : α
Then h : γ ⊢ω ω1[f ←ω2] : β.
Proof. We prove those two propositions by mutual induction on t and ω1.
Terms, by induction on t.
• If t = x or t = () then there is nothing to do.
• If t = injl t′ or injr t′ or fold t′ or ⟨t1, t2⟩or let p = t1 in t2, then simi-
larly to the proof of Lemma 4.2.16 the substitution goes to the subterms
and we can apply the induction hypothesis.
• If t = ω′ t′. In that case, the substitution goes to both subterms: t[f ←
ω] = (ω′[f ←ω]) (t′[f ←ω]) and by induction hypothesis on t′ and by
the mutually recursive proof.
Isos, by induction on ω1.
• If ω1 = f, then we get h : γ ⊢ω f[f ←ω2] : β which is typable by
hypothesis.
• Ifω1 = g ̸= f is impossible by our typing hypothesis.
• If ω1 = ﬁx g.ω, then by typing f does not occur in ω1 so nothing hap-
pens.
• If ω1 = {v1 ↔e1 | · · · | vn ↔en}, then, by deﬁnition of the substitution
we have that
{v1 ↔e1 | · · · | vn ↔en}[f ←ω2]
= {v1 ↔e1[f ←ω2] | . . . | vn[f ←ω2] ↔en[f ←ω2]}
in which case we apply the substitution lemma of isos on terms.
From that we can directly deduce subject reduction and progress:
Lemma 4.2.18 (Subject Reduction). If ∆; Ψ ⊢e t : A and t →t′ then ∆; Ψ ⊢e
t′ : A.
Proof. By induction on t →t′ and direct by Lemma 4.2.16 and Lemma 4.2.17
Lemma 4.2.19 (Progress). If ⊢e t : A then, either t is a value, or t →t′.
Proof. Direct by induction on ⊢e t : A. The two possible reduction cases,
ω v and let p = v in t always reduce by typing, pattern-matching and by
Lemma 4.2.7.
70

4.3 . Computational Content
In this section, we study the computational content of our language. We show
that we can encode Recursive Primitive Permutations [PPR20] (RPP), which shows
us that we can encode at least all Primitive Recursive Functions [RJ87].
4.3.1 . From RPP to Isos
We start by deﬁning the type of strictly positive natural numbers, npos, as
npos = µX.1 ⊕X. We deﬁne n, the encoding of a positive natural number into
a value of type npos as 1 = fold injl () and n + 1 = fold injr n. Finally, we
deﬁne the type of integers as Z = 1⊕(npos ⊕npos) along with z the encoding of
any z ∈Z into a value of type Z deﬁned as: 0 = injl (), z = injr injl z for z
positive, and z = injr injr −z for z negative. Given some function f ∈RPPk,
we will build an iso isos(f) : Zk ↔Zk which simulates f. isos(f) is deﬁned by
the size of the proof that f is in RPPk.
Deﬁnition 4.3.1 (Encoding of the primitives).
• The Sign-change is
isos(Sign) =



injr injl x
↔
injr injr x
injr injr x
↔
injr injl x
injl ()
↔
injl ()


: Z ↔Z
• The identity is isos(Id) = {x ↔x} : Z ↔Z
• The Swap is isos(X) = {(x, y) ↔(y, x)} : Z2 ↔Z2
• The Successor is
isos(S) =







injl ()
↔
injr injl fold injl ()
injr injl x
↔
injr injl fold injr x
injr injr fold injl ()
↔
injl ()
injr injr fold injr x
↔
injr injr x







: Z ↔Z
• The Predecessor is the inverse of the Successor: isos(P) = isos(S)⊥.
Deﬁnition 4.3.2 (Encoding of Composition). Let f, g ∈RPPj, ωf = isos(f) and
ωg = isos(g) the isos encoding f and g, we build isos(f; g) of type Zj ↔Zj as:
isos(f; g) =



let (y1, . . . , yj) = ωf (x1, . . . , xj) in
(x1, . . . , xj)
↔
let (z1, . . . , zj) = ωg (y1, . . . , yj) in
(z1, . . . , zj)



71

Deﬁnition 4.3.3 (Encoding of Parallel Composition). Let f ∈RPPj and g ∈
RPPk, and ωf = isos(f) and ωg = isos(g), we deﬁne isos(f || g) of type Zj+k ↔
Zj+k as:.
isos(f || g) =



let (x′
1, . . . , x′
j) = ωf (x1, . . . , xj) in
(x1, . . . , xj, y1, . . . , yk)
↔
let (y′
1, . . . , y′
k) = ωg (y1, . . . , yk) in
(x′
1, . . . , x′
j, y′
1, . . . , y′
k)



Deﬁnition 4.3.4 (Encoding of Finite Iteration). Let f ∈RPPk, and ωf = isos(f),
we encode the ﬁnite iteration It[f] ∈RPPk+1 with the help of an auxiliary iso,
ωaux, of type Zk ⊗npos ↔Zk ⊗npos doing the ﬁnite iteration using npos, de-
ﬁned as:
ωaux = ﬁxg.















(−→x , fold injl ())
↔
let −→y = ωf −→x in
(−→y , fold injl ())
(−→x , fold injr n)
↔
let (−→y ) = ωf (−→x ) in
let (−→z , n′) = g (−→y , n) in
(−→z , fold injr n′)















We can now properly deﬁne isos(It[f]) of type Zk+1 ↔Zk+1 as:
isos(It[f]) =















(−→x , injl ())
↔
(−→x , injl ())
(−→x , injr injl z)
↔
let (−→y , z′) = ωaux(−→x , z) in
(−→y , injr injl z′)
(−→x , injr injr z)
↔
let (−→y , z′) = ωaux(−→x , z) in
(−→y , injr injr z′)















Deﬁnition 4.3.5 (Encoding of Selection). Let f, g, h ∈RPPk and ωf = isos(f), ωg =
isos(g), ωh = isos(h). We deﬁne isos(If[f, g, h]) of type Zk+1 ↔Zk+1 as:
isos(If[f, g, h]) =





(−→x , injr injl z)
↔
let −→
x′ = ωf(−→x ) in (−→
x′, injr injl z)
(−→x , injl ())
↔
let −→
x′ = ωg(−→x ) in (−→
x′, injl ())
(−→x , injr injr z)
↔
let −→
x′ = ωh(−→x ) in (−→
x′, injr injr z)





Theorem 4.3.6 (The encoding is well-typed). If f ∈RPPk, then ⊢ω isos(f) :
Zk ↔Zk.
Proof. By induction on f, for the two compositions, iteration, and selection
the variables −→x are all of type Z, while for ωaux the last argument is of type
npos. The predicate ODZ is always satisﬁed as the left columns of the n-fold
tensor are always variables and the right-most argument on each isos always
satisﬁes ODZ.
Theorem 4.3.7 (Simulation). Let f ∈RPPk and n1, . . . , nk elements of Z such
that f(n1, . . . , nk) = (m1, . . . , mk) then isos(f)(n1, . . . , nk) →∗(m1, . . . , mk)
72

Proof. By induction on f.
• Direct for the identity, swap and sign-change.
• For the Successor:
ω =







injl ()
↔
injr injl fold injl ()
injr injl x
↔
injr injl fold injr x
injr injr fold injl ()
↔
injl ()
injr injr fold injr x
↔
injr injr x







we do it by case analysis on the sole input n.
– n = 0 then 0 = injl () and ω injl () →injr (injl (fold (injl ()))) =
1
– n = −1 then −1 = injr (injr (fold (injl ()))), so the term re-
duces to injl () = 0
– Case n < −1, we have n = injr (injr (fold (injr n′))) with n′ =
n + 1, by pattern-matching we get injr injr x which is n′
– Case n > 1 is similar.
• The Predecessor is the dual of the Successor.
• Composition & Parallel composition: Direct by induction hypothesis
on ωf and ωg.: for the composition, ωf if ﬁrst applied on all the input
and then ωg on the result of ωf. For the parallel composition, ωf is ap-
plied on the ﬁrst j arguments and ωg on the argument j + 1 to k before
concatenating the results from both isos.
• Finite Iteration: It[f].
We need the following lemma: ωaux( ¯x1, . . . , ¯xn, z) →∗( ¯z1, . . . , ¯zn, z) where
z is a non-zero integer and (z1, . . . , zn) = f|z|(x1, . . . , xn) which can be
shown by induction on | z |: the case z = 1 and ¯z = fold injl () is
direct by induction hypothesis on ωf. Then if z = n+1 we get it directly
by induction hypothesis on both ωf and our lemma.
Then, for isos It[f] we do it by case analysis on the last argument: when
it is ¯0 then we simply return the result, if it is ¯z for z (no matter if stricly
positive or stricly negative) then we enter ωaux, and apply the previous
lemma.
• Conditional: If[f, g, h]. Direct by case analysis of the last value and by
induction hypothesis on ωf, ωg, ωh.
Remark 4.3.8. Notice that isos(f)⊥̸= isos(f−1), due to the fact that isos(f)⊥will
inverse the order of the let constructions, which will not be the case for isos(f−1).
They can nonetheless be considered equivalent up to a permutation of let con-
structions and renaming of variable.
73

4.4 . Proof Theoretical Content
In this section, we want to relate our language of isos to proofs in a suitable
logic. As mentioned earlier, an iso ⊢ω ω : A ↔B corresponds to both a computa-
tion sending a value of type A to a result of type B and a computation sending a
value of type B to a result of type A. Therefore, we want to be able to translate
an iso into a proof isomorphism: two proofs π and π⊥of respectively A ⊢B and
B ⊢A such that their composition reduces through the cut-elimination to the
identity either on A or on B depending on the way we make the cut between those
proofs.
Since we are working in a linear system with inductive types we will use the logic
µMALL: linear logic with least and greatest ﬁxed points, which allows us to reason
about inductive and coinductive statements. µMALL allows us to consider inﬁnite
derivation trees, which is required as our isos can contain recursive variables. As
not all derivations of µMALL are considered as proofs, we need to be careful in
the way we translate isos into derivations. We will also need to show that the
obtained derivation are indeed proofs, this is ensured by the structurally recursive
constraints.
4.4.1 . Translating isos into µMALL
We start by giving the translation from isos to pre-proofs, and then, in Theo-
rem 4.4.20 show that they are actually proofs, therefore obtaining a static corre-
spondence between programs and proofs. We then show in Theorem 4.4.29 that
our translation entails a dynamic correspondence between the evaluation procedure
of our language and the cut-elimination procedure of µMALL. This will implies
that the proofs we obtain are indeed isomorphisms. meaning that if we cut the
aforementioned proofs π and π⊥, performing the cut-elimination procedure would
give either the identity on A or the identity on B.
The derivations we obtain are circular, and we therefore translate the isos
directly into ﬁnite derivations with back-edges, written circ(ω) (deﬁned in Def-
inition 4.4.2).
As mentioned in Section 3.4, we can deﬁne another translation
into inﬁnite derivations as the composition of circ(−) with the unfolding: JωK =
U(circ(ω)). Intuitively, this corresponds to the inﬁnite unfolding of the isos-variable
f in an iso ﬁx f.ω.
Representing sequents.
As mentioned in Section 3.4, there exists multiple
validity criterion for the logic µMALL, for instance the one from [NST18] uses
lists to represent sequents with a threading function that tells us how the formulas
are distributed over the inferences rules. Due to the way sequents are represented
there is no need for formula occurrence, as in [Bae+22], and one simply works
with formulas.
The diﬀerence between the two criterias lies in the fact that the one from [NST18]
does not enjoy the cut-elimination procedure. Because we need to keep track of
74

which formula is associated to which variable from the typing context, the trans-
lation uses a slightly modiﬁed version of µMALL in which contexts are split in
two parts, written Υ; Θ, where Υ is a list of formulas occurrence and Θ is a set
of formula occurrences associated with a term-variable (written x : F). When
starting the translation of an iso of type A ↔B, we start in the context [Aα]; ∅
(for some address α) and end in some context []; Θ. The additional information
of the variable in Θ is here to make sure we know how to split the contexts ac-
cordingly when needed later during the translation, with respect to the way they
are split in the typing derivation (for instance, in the case of a tensor). We write
Θ = {F | x : F ∈Θ} and Θ = {x : A | x : Aα ∈Θ}. This modiﬁed system also
make uses of another rule, called the exchange rule which allows us to send the
ﬁrst formula from Υ to Θ and aﬀecting it a variable, deﬁned as:
Υ; x : F, Θ ⊢G
F :: Υ; Θ ⊢G ex(x)
Given a derivation ι in this system, we write TιU for the function that sends ι into
a derivation of µMALL where (i) we remove all occurrences of the exchange rule
(ii) the contexts []; Θ become Θ. In this system the cut-elimination holds: one
can simply ignore the lists, remove the exchange rule and the variable in the typing
context Θ in order to fall back to the system from [Bae+22].
Translation.
Given an iso ω : A ↔B and initial addresses α, β, its translation
into a derivation of µMALL of Aα ⊢Bβ is described with three separate phases:
Iso Phase. The ﬁrst phase consists in travelling through the syntactical deﬁnition of
an iso, keeping as information the last encountered iso-variable bounded by a ﬁx
and calling the negative phase when encountering an iso of the form {v1 ↔e1 |
· · · | vn ↔en} and attaching to the formulas A and B two distinct addresses α
and β and labelling the sequent with the name of the last encountered iso-variable.
Later on, during the translation, this phase will be recalled when encountering
another iso in one of the ei, and, if that iso corresponds to an iso-variable, we will
create a back-edge pointing towards the corresponding sequents.
Negative Phase. Starting from some context [Aα], Θ, the negative phase consists
in decomposing the formula A according to the way the values of type A on the
left-hand-side of ω are decomposed. The negative phase works as follows: we
consider a set of pairs made of a list of values and a typing judgment, written
(l, ξ) where each element of the set corresponds to one clause v ↔e of the given
iso and ξ is the typing judgment of e. The list of values corresponds to what is
left to be decomposed in the left-hand-side of the clause (for instance if v is a pair
⟨v1, v2⟩the list will have two elements to decompose). Each element of the list
Υ will correspond to exactly one value in the list l. If the term that needs to be
decomposed is a variable x, then we will apply the ex(x) rule, sending the formula
to the context Θ. The negative phase ends when the list Υ is empty. When it is
75

the case, we can start decomposing ξ and the positive phase starts. The negative
phase is deﬁned by case analysis on the ﬁrst elements of the lists of the set, which
are known by typing to have the same type constructor, and is given in Figure 4.1.
Positive phase. The translation of an expression is pretty straightforward: each
let and iso-application is represented by two cut rules: as usual in Curry-Howard
correspondence [SU06]. Keeping the variable-formula pair in the derivation is here
to help us know how to split accordingly the context Θ when needed, while Υ
is always empty and is therefore omitted. While the positive phase carries over
the information of the last-seen iso-variable, it is not noted explicitly as it is only
needed when calling the Iso Phase. The positive phase is given in Figure 4.2.
Remark 4.4.1. While µMALL is presented in a one-sided way, we write Σ ⊢Φ
for ⊢Σ⊥, Φ in order to stay closer to the formalism of the type system of isos and
label the left rules on Σ as the corresponding right rules on Σ⊥.
Deﬁnition 4.4.2. The translation circ(ω, S, α, β) = π takes a well-typed iso,
a singleton set S of an iso-variable corresponding to the last iso-variable seen
in the induction deﬁnition of ω and two fresh addresses α, β and produces a
circular derivation of the variant of µMALL described above with back-edges.
circ(ω, S, α, β) is deﬁned inductively on ω:
• circ(ﬁx f.ω, S, α, β) = circ(ω, {f}, α, β)
• circ(f, {f}, α, β) = Aα ⊢Bβ be(f)
• circ({(vi ↔e′
i)i∈I} : A ↔B, {f}, α, β) = T
Neg(([vi], ξi)i∈I)
Aα ⊢f Bβ
U where ξi
is the typing derivation of ei.
Example 4.4.3. The translation π = circ(ω, ∅, α, β) of the iso ω from Exam-
ple 4.2.5, with F = Aαl, G = Bαrl, H = Cαrr and F ′ = Aβrl, G′ = Bβrr, H′ =
Cβl is:
id
[]; a : F ⊢F ′
⊕1
[]; a : F ⊢F ′ ⊕G′
⊕2
[]; a : F ⊢H′ ⊕(F ′ ⊕G′)
ex(a)
[F]; ∅⊢H′ ⊕(F ′ ⊕G′)
id
[]; b : G ⊢G′
⊕2
[]; b : G ⊢F ′ ⊕G′
⊕2
[]; b : G ⊢H′ ⊕(F ′ ⊕G′)
ex(b)
[G]; ∅⊢H′ ⊕(F ′ ⊕G′)
id
[]; c : H ⊢H′
⊕1
[]; c : H ⊢H′ ⊕(F ′ ⊕G′)
ex(c)
[H]; ∅⊢H′ ⊕(F ′ ⊕G′)
`
[G ⊕H]; ∅⊢H′ ⊕(F ′ ⊕G′)
`
[F ⊕(G ⊕H)]; ∅⊢H′ ⊕(F ′ ⊕G′)
and its corresponding proof TπU in µMALL:
id
F ⊢F ′
⊕1
F ⊢F ′ ⊕G′
⊕2
F ⊢H′ ⊕(F ′ ⊕G′)
id
G ⊢G′
⊕2
G ⊢F ′ ⊕G′
⊕2
G ⊢H′ ⊕(F ′ ⊕G′)
id
H ⊢H′
⊕1
H ⊢H′ ⊕(F ′ ⊕G′)
&
G ⊕H ⊢H′ ⊕(F ′ ⊕G′)
&
F ⊕(G ⊕H) ⊢H′ ⊕(F ′ ⊕G′)
76

Example 4.4.4. Considering the iso swap of type A ⊗B ↔B ⊗A and its
µMALL proof
πS =
Aγl ⊢Aγ′r id
Bγr ⊢Bγ′l id
Aγl, Bγr ⊢(B ⊗A)γ′
⊗
(A ⊗B)γ ⊢(B ⊗A)γ′ `
, we give the proof πm(S)
corresponding to Example 4.2.14 where F = (A ⊗B)αirl and G = (B ⊗A)βirl,
then [F] and [G] are respectively of address α and β:
1
⊢1
⊕1
⊢1 ⊕(G ⊗[G])
µ
⊢[G]
⊥
1 ⊢[G]
id
F ⊢F
πS
F ⊢G cut
F ⊢G
id
[F] ⊢[F]
πm(S)
[F] ⊢[G]
cut
[F] ⊢[G]
id
G ⊢G
id
[G] ⊢[G]
⊗
G, [G] ⊢(G) ⊗[G]
⊕2
G, [G] ⊢1 ⊕(G ⊗[G])
µ
G, [G] ⊢[G]
G, [G] ⊢[G]
cut
G, [F] ⊢[G]
cut
F, [F] ⊢[G]
`
F ⊗[F] ⊢[G]
&
1 ⊕(F ⊗[F]) ⊢[G]
ν
[F] ⊢[G]
We painted in blue the pre-thread that follows the focus of the structurally recur-
sive criterion. During the negative phase which consists of the ν, &, `, ⊥rules the
pre-thread is going up, at each time going into the subformula corresponding to
the focus. Then, during the positive phase the pre-thread is not active during the
multiple cut rules until it reaches the id rule, where the pre-thread bounces and
starts going down before bouncing back up again in the cut rule, into the inﬁnite
branch, where the behaviour of the pre-thread will repeat itself.
4.4.2 . Pre-Proof Validity
Lemma 4.4.5. Given π = circ(ω), for each inﬁnite branch of π, only a single
iso-variable is visited inﬁnitely often.
Proof. Since we have at most one iso-variable, we never end up in the case
that between an annotated sequent ⊢f and a back-edge pointing to f we en-
counter another annotated sequent.
Among the terms that we translate, the translation of a value yields what we
call a Purely Positive Proof : a ﬁnite derivation whose only rules have for active
formula the sole formula on the right of the sequent. Any such derivation is trivially
a valid pre-proof.
77

Neg({(injl vj :: lj, ξj)j∈J} ∪{(injr vk :: lk, ξk)k∈K})
F1 ⊕F2 :: Υ; Θ ⊢G
=
Neg({(vj :: lj, ξj)j∈J})
F1 :: Υ; Θ ⊢G
Neg({(vk :: lk, ξk)k∈K})
F2 :: Υ; Θ ⊢G
F1 ⊕F2 :: Υ; Θ ⊢G
&
Neg({([], ξ)})
[]; Θ ⊢G
=
Pos(ξ)
[]; Θ ⊢G
Neg({(() :: l, ξ)})
1 :: Υ; Θ ⊢G
=
Neg({l, ξ})
Υ; Θ ⊢G
1 :: Υ; Θ ⊢G ⊤
Neg({(⟨v1
i , v2
i ⟩:: li, ξi)i∈I})
F1 ⊗F2 :: Υ; Θ ⊢G
=
Neg({(v1
i :: v2
i :: li, ξi)i∈I})
F1, F2 :: Υ; Θ ⊢G
F1 ⊗F2 :: Υ; Θ ⊢G `
Neg({(fold vi :: li, ξi)i∈I})
µX.F :: Υ; Θ ⊢G
=
Neg({(vi :: li, ξi)i∈I})
F[X ←µX.F] :: Υ; Θ ⊢G
µX.F :: Υ; Θ ⊢G
ν
Neg({(x :: l, ξ)})
F :: Υ; Θ ⊢G
=
Neg({l, ξ})
Υ; Θ, x : F ⊢G
F :: Υ; Θ ⊢G ex(x)
Figure 4.1: Negative Phase
Deﬁnition 4.4.6 (Purely Positive Proof). A Purely Positive Proof is a ﬁnite, cut-
free proof whose rules are only ⊕i, ⊗, µ, 1, id for i ∈{1, 2}.
Lemma 4.4.7 (Values are Purely Positive Proofs). Given x1 : A1, . . . , xn : An ⊢e
v : A then
JvK
[]; x1 : A1
α1, . . . , xn : An
αn ⊢Aα is a purely positive proof.
Proof. By induction on ∆⊢e v : A
• x : A ⊢e x : A then the derivation is []; F ⊢F id, which is a purely
positive proof;
• ⊢() : 1 then the derivation is []; ∅⊢1 1, which is a purely positive proof;
78

Pos

⊢e () : 1

= []; ∅⊢1α 1
Pos

x : A ⊢e x : A

= []; x : Aα ⊢Aβ id
Pos


ξ
Θ ⊢e t : A1
Θ ⊢e injl t : A1 ⊕A2

=
Pos(ξ)
Θ ⊢(A1)αl
[]; Θ ⊢(A1 ⊕A2)α ⊕1
Pos


ξ
Θ ⊢e t : A2
Θ ⊢e injr t : A1 ⊕A2

=
Pos(ξ)
[]; Θ ⊢(A2)αr
[]; Θ ⊢(A1 ⊕A2)α ⊕2
Pos


ξ1
Θ1 ⊢e t1
ξ2
Θ2 ⊢e t2 : A2
Θ1, Θ2 ⊢e ⟨t1, t2⟩: A1 ⊗A2

=
Pos(ξ1)
[]; Θ1 ⊢(A1)αl
Pos(ξ2)
[]; Θ2 ⊢(A2)αr
[]; Θ1, Θ2 ⊢(A1 ⊗A2)α
⊗
Pos


ξ
Θ ⊢e t : A[X ←µX.A]
Θ ⊢e fold t : µX.A

=
Pos(ξ)
[]; Θ ⊢(A[X ←µX.A])αi
[]; Θ ⊢(µX.A)α
µ
Pos


ξ1
Θ1 ⊢e t1 : A1 ⊗· · · ⊗An
ξ2
Θ2, x1 : A1, . . . , xn : An ⊢e t2 : B
Θ1, Θ2 ⊢e let (xi)i∈I = t1 in t2 : B


=
Pos(ξ1)
[]; Θ1 ⊢F1 ⊗· · · ⊗Fn
Neg(([(xi)i∈I], ξ2))
[F1 ⊗· · · ⊗Fn]; Θ2 ⊢Bα
[]; Θ1, Θ2 ⊢Bα
cut
Pos

Ψ ⊢ω ω : A ↔B
ξ
Θ ⊢e t : A
Θ; Ψ ⊢e ω t : B

=
Pos(ξ)
[]; Θ ⊢A
circ(ω, {f}, α, β)
[A]; ∅⊢Bβ
[]; Θ ⊢Bβ
cut
Figure 4.2: Positive Phase
• ∆1, ∆2 ⊢⟨v1, v2⟩: A ⊗B then we get
π1
∆1 ⊢A
π2
∆2 ⊢B
[]; ∆1, ∆2 ⊢A ⊗B ⊗and then
by induction hypothesis on π1 and π2;
• ∆⊢injl v : A ⊕B then the derivation is
π
∆⊢A
[]; ∆⊢A ⊕B ⊕1
then by
induction hypothesis on π;
• Similar for injr v and fold v.
We can then deﬁne the notion of bouncing-cut and their origin:
Deﬁnition 4.4.8 (Bouncing-Cut). A Bouncing-cut is a cut of the form:
79

π
Σ ⊢G
G ⊢F be(f)
Σ ⊢F
cut
Due to the syntactical restrictions of the language we get the following:
Property 4.4.9 (Origin of Bouncing-Cut). Given a well-typed iso, every occur-
rence of a rule be(f) in Tcirc(ω)U is a premise of a bouncing-cut.
In particular, when following a thread going up into a bouncing-cut, it will
always start from the left-hand-side of the sequent, before going back down on
the right-hand-side of the sequent.
It will also always bounce back up on the
bouncing-cut to reach the back-edge. Also note that when translation an iso, the
derivation π in the bouncing-cut is a purely positive proof.
To make sure we follow the correct formula we deﬁne a notion of term occur-
rence and show that it matches the addresses obtained from the negative phases:
Deﬁnition 4.4.10 (Term Occurrence). We note by Occ(v) the set of Occurrence
in the value v deﬁned inductively on v by:
• Occ(v) = {ϵ} if v = x or v = ()
• Occ(⟨v1, v2⟩) = {ϵ} ∪l · Occ(v1) ∪r · Occ(v2)
• Occ(injl v) = {ϵ} ∪l · Occ(v)
• Occ(injr v) = {ϵ} ∪r · Occ(v)
• Occ(fold v) = {ϵ} ∪i · Occ(v)
Where x · S = {xα | α ∈S} for x ∈{l, r, i}
Given α ∈Occ(v) we write v@α for the subterm of v at position α
We write ξ(v) = {α ∈Occ(v) | v@α = x} for the set of position of variables
in v and ξ(x, v) for the position of x in v.
Theorem 4.4.11. Given a sequence of sequents S0, . . . , Sn, with S0 = Aα ⊢Bβ
and Sn = Σ ⊢Bβ and the only rules applied are ⊤, &, `, ν.
There exists a unique value v and context ∆such that ∆⊢e v : A and such
that for all expressions e such that ∆⊢e e : B, for all isos ⊢ω ω : A ↔B such
that v ↔e is a clause of ω, consider π = Tcirc(ω, ∅, α, β)U then S0, . . . , Sn is a
branch of π and for all formulas Aα ∈Σ, there exists a unique variable x such
that ξ(x, v) is a suﬃx of α.
Proof. By induction on n.
• Case 0, then take ∆= x : A and v = x, obviously ∆⊢e x : A. We also
get that ω = {x ↔e} and Tcirc(ω)U =
TPos(e)U
Aα ⊢Bβ so the empty sequence
is a branch and ξ(v, v) = ϵ which is a suﬃx of α.
80

• Case n + 1. By induction hypothesis, the sequence S0, . . . , Sn with Sn
sequent of Σn ⊢Bβ gives us ∆n ⊢e vn : A. Deﬁne the values contexts as
V = [] | ⟨V, v⟩| ⟨v, V⟩| injl V | injr V | fold V.
Then, by case analysis on the rule of Sn+1.
– `: we can write Σn as A1
α1, . . . , (C1 ⊗C2)k
αk, . . . , An
αn ⊢Bβ and we
know that ∆n = x1 : A1, . . . , xk : C1 ⊗C2, . . . , xn : An then v can
be written as V[xk].
Build vn+1 = V[⟨y, z⟩] and ∆n+1 = ∆\{xk : C1 ⊗C2} ∪{y : C1, z :
C2}.
We get that ∆n+1 ⊢e vn+1, then for any iso ω such that V[xk] ↔e is
a clause, we can replace the clause by V[⟨y, z⟩] ↔e[x ←⟨y, z⟩] in
order to build ω′, and if S0, . . . , Sn was a branch in Tcirc(ω)U then
so is S0, . . . , Sn, Sn+1 in ω′.
We know that ξ(x, v) = γ is a suﬃx of αk, then after applying the
` rule we have that C1 has address αkl and C2 has address αkr.
Therefore, ξ(y, vn+1) = γ l and ξ(z, vn+1) = γ r which are respec-
tively suﬃxes of αkl and αkr.
– &. Assuming that Sn+1 goes to the left branch of the & rule.
We then have Σn = A1
α1, . . . , (C1 ⊕C2)k
αk, . . . , An
αn ⊢Bβ and ∆n =
x1 : A1, . . . , xk : C1 ⊕C2, . . . , xn : An with v = V[xk].
Consider vn+1 = V[injl y] and ∆n+1.
For any iso ω where vn ↔e was a clause, we can consider the
isos ω′ where the clause vn ↔e has been replaced by two clauses
V[injl y] and V[injr r] with e[x ←y] and e[x ←z]. S0, . . . , Sn, Sn+1
is obviously a branch in Tcirc(ω)U by deﬁnition of the negative
phase.
Also since ξ(x, v) = γ is a suﬃx of αk, after applying the & rule, on
the left branch we get C1 with address αkl. And ξ(y, vn+1) = γ l is
a suﬃx of αkl.
– The other side of the & is similar.
– The ν rule is similar.
– ⊤. In which case we have Σn = 1α, A1
α1 . . . , An
αn with ∆n = x :
1, x1 : A1, . . . , xn : An with vn = V[x], build vn+1 as V[()] and
∆n+1 = x1 : A1, . . . , xn : An. Then after the ⊤rule we get Γn+1 =
A1
α1, . . . , An
αn so the property holds by our induction hypothesis.
We can now show that our translation is well-deﬁned:
Lemma 4.4.12. Given a closed iso ⊢ω ω then circ(ω, ∅, α, β) for α, β fresh ad-
dresses, is well-deﬁned.
81

Proof. By induction on ⊢ω. The iso is of the form ﬁx f1. . . . , ﬁx fn.{v1 ↔e1 |
· · · | vn ↔en} and as only the last iso-variable is kept, we end up in the case
circ({(vi ↔e′
i)i∈I} : A ↔B, {fn}, α, β), where the root of the derivation take
place, annotated with fn.
The negative phase Neg(([vi], ξ′
i)i∈I) is well-deﬁned due to the predicate
ODA: by deﬁnition of ODA all left-hand-side values have the same type con-
structors, they also have the same type.
The positive phase is well-deﬁned as it consists in recreating the typing
judgment of each expression ei. By Theorem 4.4.11 we know that the typing
context of ei and the one obtained from the negative phase contain the same
variable associated to the same formula / formula occurrence.
Given an iso ω = ﬁx f.{v1 ↔e1 | · · · | vn ↔en}, we want to show that
for any inﬁnite branch there exists a valid thread that inhabits it. As given by
Lemma 4.4.5, an inﬁnite branch is uniquely deﬁned by a single iso-variable.
Given the value vj
i that is the decreasing argument for the structurally recursive
criterion, we want to build a pre-thread that follows the variable xj : µX.B in
vj
i : µX.B that is the focus of the criterion.
Since our sequents are diﬀerent from the one of µMALL during the negative
phase, we are forced to deﬁne properly the way the pre-thread is built. For the
positive phase, our sequents and the one of µMALL are the same, aside from
the fact that the context also contains variables, so for this part we can just
use Deﬁnition 3.4.5.
Deﬁnition 4.4.13 (Pre-Thread of the negative phase). Given a well-typed iso
ω = ﬁx f.{v1 ↔e1 | · · · | vn ↔en} : A ↔B and a clause vi ↔ei such that
f p is a subterm of ei and the variable xp is the focus of the primitive recursive
criterion, and considering π = circ(ω), we deﬁne PTn(xp, π) as the pre-thread
that follow the formula µX.A′ corresponding to xp up to the positive phase by
induction on Neg({([vi], ei)i∈I}). For simplicity, we simply omit the ﬁrst argument
of PTn.
• PTn(Neg({([], e)})) is impossible as we follow a variable;
• PTn(Neg({((() :: l, e))})) = PTn(Neg({(l, e)}));
• PTn(Neg({((y :: l, e))})) =
(
ϵ
if y = xp
PTn(Neg({(l, e)}))
otherwise;
• PTn(Neg({(injl vi :: li, ei)i∈I} ∪{(injl vj :: lj, ej)j∈J}))
=















(A ⊕C; A ⊕C, ∆⊢B) · (A; A, ∆⊢B) · PTn(Neg({(vi :: li, ei)i∈I}))
if xp ⊆vi
(C ⊕A; C ⊕A, ∆⊢B) · (A; A, ∆⊢B) · PTn(Neg({(vj :: lj, ej)j∈J}))
if xp ⊆vj
(µX.D; A1 ⊕A2, ∆⊢B) · (µX.D, Ak, ∆⊢B) · PTn(Neg({(vl :: ll, el)l∈L}))
for L ∈{I, J}, k ∈{1, 2} and if xp ⊂lL
82

• Case PTn(Neg({(v1
i , v2
i :: li, ei)i∈I}))
=







(A1 ⊗A2; A1 ⊗A2, ∆⊢B) · (Ak; A1, A2, ∆⊢B) · PTn(Neg({(v1
i , v2
i :: li, ei)i∈I}))
for k ∈{1, 2} if xp ⊂vk
(µX.D; A1 ⊗A2, ∆⊢B) · (µX.D; A1, A2, ∆⊢B) · PTn(Neg({(v1
i , v2
i :: li, ei)i∈I}))
if xp ⊆l
• PTn(Neg({(fold vi :: li, ei)i∈I}))
=
(
(µX.A; µX.A, ∆⊢B) · (A[X ←µX.A]; A[X ←µX.A], ∆⊢B)
if xp ⊂vi
(µX.D; µX.A, ∆⊢B) · (µX.D; A[X ←µX.A], ∆⊢B)
if xp ⊂li
Lemma 4.4.14 (Weight of the pre-thread for the negative phase). Given a well-
typed iso ω = ﬁx f.ω with a clause v ↔e, then w(PTn(Neg({([v], e)}))) is a word
over {l, r, i, W}.
Proof. By case analysis of Deﬁnition 4.4.13:
• If the variable xp is not a subterm of the ﬁrst value from the list l then
the thread has the form:(A; C, ∆⊢B, ↑) · (A; C′, ∆⊢B, ↑) and the
weight is W.
• If the variable xp is a subterm of the ﬁrst value of the list l then by di-
rect case analysis on the ﬁrst value: if the value is of form ⟨v1, v2⟩then
depending on whether xp is in v1 or v2 the weight will be l or r, similarly
for the injl and injr , while the fold will create weight i.
A similar analysis can be done for the positive phase:
Deﬁnition 4.4.15 (Pre-thread of the positive phase). Given a well-typed iso ω =
ﬁx f.{v1 ↔e1 | · · · | vn ↔en} : A ↔B and a clause vi ↔ei such that f p is a
subterm of ei and the variable xp is the focus of the primitive recursive criterion,
and considering π = circ(ω, S, Aα, Bβ), then we deﬁne PTp(xp, Pos(ei)) as the
pre-thread that follow the formula µX.A′ corresponding to xp until the sequent
Aα′ ⊢f Bβ′ is reached. For simplicity we also just write it as PTp(xp).
Lemma 4.4.16. Shape of the pre-thread of the positive phase Given a well-typed
iso ω such that the variable xp is the focus of the primitive recursive criterion, and
two successive element a, b from the pre-thread PT(xp) from the translation of ω,
then a · b are of either:
• (νX.F; s; ↑) · (νX.F; s′; ↑);
• ((νX.A)α; (νX.A)α ⊢(νX.A)β; ↑) · ((νX.A)β; (νX.A)α ⊢(νX.A)β; ↓);
• (F; s; ↓) · (F ′; s′; ↓);
• (F; s; ↓) · (F; s′; ↑)
83

Proof. By a straightforward case analysis on the positive phase:
• If a right-rule is applied while the pre-thread is on the left side of the
sequent, then we are in the ﬁrst case as the thread just goes up while
following the formula.
• The second case occurs when encountering an axiom rule on the vari-
able xp that we follow.
• The third case is when going down on a purely positive proof. As we
follow the sole formula on the right, the formula necessarily changes
and s, s′ are of the shape ∆⊢s, ∆′ ⊢s′.
• The last one is when encountering a cut, at the root of the purely posi-
tive proof that comes from the translation of a let.
We can now look at the weight of the positive phase:
Lemma 4.4.17 (Weight of the pre-Thread for the positive phase). Given a well-
typed iso such that the variable xp is the focus of the primitive recursive crite-
rion, the weight of the pre-thread on the positive phase PTp(xp) is of the shape
W∗A{l, r, W}∗C
Proof. By case analysis on Pos(e). As the thread only goes up by encounter-
ing cut-rules or right-rules, we get W∗, and the thread goes up all the way to
an axiom rule, corresponding to the formula xp : νX.F, which adds the A. Fi-
nally, the thread goes down on the purely positive proof, generating {l, r, W}∗
until reaching the cut-rule from the bouncing cut.
We can then consider the inﬁnite pre-thread as the concatenation of both the
pre-thread of the negative phase, and the one of the positive phase.
Lemma 4.4.18 (Form of the Pre-Thread). Given the pre-thread t following xp we
have that w(t) = p0(Σi≤npiW∗
i AqiC)ω with
• p0 is any preﬁx.
• pi ∈{l, r, i, W}∗
• qi ∈{l, r, W}∗
With, ∀i ≤n, qi ⊏pi and | pi |>| qi | without counting the W, where p ⊏q
is q is a preﬁx of p and with xp = xp if x ∈{l, r, i}, xp = xp if x ∈{l, r, i} and
Wp = p
84

Proof. pi is generated from Deﬁnition 4.4.13 while the rest up to the C (in-
cluded) is generated from Deﬁnition 4.4.15.
First, we show that | pi |>| qi | modulo the W.
Since pi is generated by the negative phase, we have that, modulo W,
pi = {r}∗l+{l, r, i}∗, this is due by deﬁnition of being primitive recursive and
because we are looking for the right variable. By deﬁnition of being primitive
recursive the input type of the iso is A1 ⊗· · · ⊗An, hence {r}∗l+ correspond
to the search for the correct Ai for the input type A1 ⊗· · · ⊗An while {l, r, i}∗
is the decomposition of the primitive recursive value, as described in Theo-
rem 4.4.11.
As qi corresponds to the Purely Positive Proof, we know that the Purely
Positive Proof is the encoding of a pattern p = ⟨x1, ⟨. . . , xn⟩⟩. Hence, qi can
be decomposed as {l
+r∗}
By the fact that the iso is primitive recursive we know that the variable in
p is a strict subterm of the primitive recursive value, hence | pi |>| qi |.
The fact that qi ⊏pi is direct as the Purely Positive Proof reconstructs the
type A1 ⊗· · · ⊗An without modifying the Ai while pi start by searching for
the corresponding type Ai, so it is only composed of {l, r}∗, which will be the
same as qi.
Theorem 4.4.19 (The Pre-thread generated is a thread). We want to ﬁnd a
decomposition of the pre-thread such that it can uniquely be decomposed into
J(Hi ⊙Vi) with
• w(Vi) ∈{l, r, i, W}∞and non-empty if i ̸= λ
• w(Hi) ∈H
Proof. We set H0 as the empty pre-thread. (so w(H0) = ϵ) We set V0 as the
maximal possible sequence such that w(V0) ∈{l, r, i, W}∗, i.e the sequence
that ends with (A; A ⊢A; ↑). Then, for all i ≥1 we set
• Hi starts at (A; A ⊢A; ↑) just before the axiom rule so that the ﬁrst
element of w(Hi) is A. Then Hi is composed of
– All of the pre-thread going down on the Purely Positive Proof after
the axiom, accumulating a word over {l, r, i, W}∗;
– Going back up into the cut-rule of the bouncing cut, making a C;
– Going up to compensate every x seen in the Purely Positive Proof
while going down. This is possible as shown in Lemma 4.4.18.
• Vi is the maximal possible sequence such that w(Vi) ∈{l, r, i, W}∗, i.e
the sequence that ends with (A; A ⊢A; ↑).
Theorem 4.4.20 (Validity of proofs). If ⊢ω ω : A ↔B and π = Tcirc(ω, ∅, α, β)U
then π satisﬁes µMALL validity criterion from Chapter 3.
85

Proof. By Theorem 4.4.19 we know that we have a thread of which we also
know by Theorem 4.4.19 that the visible part is not stationary.
Finally, by Lemma 4.4.18 and Theorem 4.4.19 we know that the visible part
will see inﬁnitely often the subformulas of the formula µX.B that is the focus
of the primitive recursive criterion. This is due to the diﬀerence in size in the
part of the thread from the negative and from the positive phase and the fact
that the positive phase does not encounter a µ formula when going down on
a purely positive proof.
By the constraints on the syntax of our isos, all the possible slices are nec-
essarily persistent.
Therefore, the smallest formula we will encounter is nu formula, validating
the thread.
4.4.3 . Proof Simulation
In order to show the relationship between the cut-elimination procedure of
µMALL and the rewriting system of our language we introduce a slightly modiﬁed
version of the rewriting system, that we call →eβ, using explicit substitution.
Convention 4.4.21. Given a substitution σ = {x1 7→v1, . . . , xn 7→vn} we will
use the shorthand let σ in t for let x1 = v1 in . . . let xn = vn in t.
Deﬁnition 4.4.22 (Explicit Substitution Rewriting System). →eβ is deﬁned by
the following rules:
let x = v in x →elet v
let ⟨x1, p⟩= ⟨t1, t2⟩in t →elet let x1 = t1 in let p = t2 in t
let x = v in ⟨t1, t2⟩→elet ⟨let x = v in t1, t2⟩
when x ∈FV (t1)
let x = v in ⟨t1, t2⟩→elet ⟨t1, let x = v in t2⟩
when x ∈FV (t2)
let x = v in injl t →elet injl let x = v in t
let x = v in injr t →elet injr let x = v in t
let x = v in fold t →elet fold let x = v in t
let x = v in ω t →elet ω let x = v in t
and the following rules:
t→eβ∪→elett′
C[t] →eβ C[t′] β −Cong
σ[p] = v
let p = v in t →eβ let σ in t β −LetE
(ﬁx f.ω) →eβ ω[f ←(ﬁx f.ω)] β −IsoRec
σ[vi] = v′
{v1 ↔e1 | · · · | vn ↔en} v′ →eβ let σ in ei
β −IsoApp
Remark 4.4.23. The rule β −LetE is superﬂuous as it can be inferred from the
decomposition of the rule of the decomposition of a let.
86

Each step of this rewriting system will correspond to exactly one step of cut-
elimination, while in the previous system, the rewriting that uses a substitution σ
correspond in fact to multiple steps of cut-elimination. →eβ makes this explicit.
But ﬁrst, we need to make sure that both systems do the same thing:
Lemma 4.4.24 (Specialisation of the substitution on pairs). Let σ be a substi-
tution that closes ∆⊢e ⟨t1, t2⟩, then there exists σ1, σ2, such that σ(⟨t1, t2⟩) =
⟨σ1(t1), σ2(t2)⟩Where σ = σ1 ∪σ2.
Proof. By the linearity of the typing system we know that FV (t1)∪FV (t2) = ∅,
so there always exists a decomposition of σ into σ1, σ2 deﬁned as σi = {(xi 7→
vi) | xi ∈FV (ti)} for i ∈{1, 2}.
Lemma 4.4.25 (Explicit substitution and substitution coincide). Let σ = {xi 7→
vi} be a substitution that closes t, then let σ in t →∗
elet σ(t).
Proof. By induction on t.
• x, then σ(x) = v and let x = v in x →elet v = σ(x).
• () then σ is empty and no substitution applies.
• ⟨t1, t2⟩, then by Lemma 4.4.24 σ(⟨t1, t2⟩) = ⟨σ1(t1), σ2(t2)⟩. By →elet,
each let construction will enter either t1 or t2, then by induction hy-
pothesis.
• let p = t1 in t2 is similar to the product case.
• injl t, injr t, fold t, ω t. All cases are treated in the same way: by deﬁ-
nition of →elet, each let will enter into the subterm t, as with the sub-
stitution σ, then by induction hypothesis.
Corollary 4.4.26. If t →t′ then t →∗
eβ t′.
Proof. By induction on →, the case of IsoRec is the same, while for the other
rules, just by applying either β−IsoApp or β−LetE and then by Lemma 4.4.25,
let σ in t →∗
elet σ(t) for any substitution σ that closes t. But σ(t) = t′, so
t →∗
eβ t′.
It is then possible to show a ﬁrst step of the simulation procedure: that →eβ
corresponds to one step of cut-elimination:
Lemma 4.4.27 (Simulation of the let-rules of →eβ).
Let Θ ⊢e t : G be a
well-typed closed term: if t →elet t′ then, given some fresh address β we get:
TPos(t)U
Θ ⊢Gβ
⇝TPos(t′)U
Θ ⊢Gβ
.
Proof. By case analysis on →elet, for simplicity of reading we omit the address
β.
87

• let x = v in x →elet v.
TPos(v)U
Θ ⊢G
G ⊢G id
Θ ⊢G
cut ⇝TPos(v)U
Θ ⊢G
• let ⟨x1, p⟩= ⟨t1, t2⟩in t →elet let x1 = t1 in let p = t2 in t
Then:
TPos(t1)U
Θ1 ⊢G
TPos(t2)U
Θ2 ⊢F
Θ1, Θ2 ⊢G ⊗F
⊗
TNeg({[p], t})U
Θ3, G, F ⊢H
Θ3, G ⊗F ⊢F `
Θ1, Θ2, Θ3 ⊢H
cut
⇝TPos(t1)U
Θ1 ⊢G
TPos(t2)U
Θ2 ⊢F
TNeg({[p], t})U
Θ3, G, F ⊢H
cut
Θ2, Θ3, G ⊢H
cut
Θ1, Θ2, Θ3 ⊢H
• let x = v in injl t →elet injl let x = v in t.
Then:
TPos(v)U
Θ1 ⊢F
TPos(t)U
F, Θ ⊢H
F, Θ2 ⊢H ⊕G ⊕1
R
Θ1, Θ2 ⊢H ⊕G
cut
⇝
TPos(v)U
Θ1 ⊢F
TPos(t)U
F, Θ2 ⊢H
Θ1, Θ2 ⊢H
cut
Θ1, Θ2 ⊢H ⊕G ⊕1
R
• The same goes for let x = v in injr t →elet injr let x = v in t and
let x = v in fold t →elet fold let x = v in t
• let x = v in ⟨t1, t2⟩→elet ⟨let x = v in t1, t2⟩when x ∈FV (t1)
Then:
TPos(v)U
Θ1 ⊢F
TPos(t1)U
F, Θ2 ⊢H
TPos(t2)U
Θ3 ⊢G
F, Θ2, Θ3 ⊢H ⊗G
⊗R
Θ1, Θ2, Θ3 ⊢H ⊗G
cut
⇝
TPos(v)U
Θ1 ⊢F
TPos(t1)U
F, Θ2 ⊢H
Θ1, Θ2 ⊢H
cut
TPos(t2)U
Θ3 ⊢G
Θ1, Θ2, Θ3 ⊢H ⊗G
⊗R
• Similar for the second rule on the pair.
88

• let x = v in ω t →elet ω (let x = v in t)
Then: TPos(v)U
Θ1 ⊢F
TPos(t)U
F, Θ2 ⊢G
Tcirc(ω)U
G ⊢H
cut
F, Θ2 ⊢H
cut
Θ1, Θ2 ⊢H
⇝
TPos(v)U
Θ1 ⊢F
TPos(t)U
F, Θ2 ⊢G
Θ1, Θ2 ⊢G
cut
Tcirc(ω)U
G ⊢H
Θ1, Θ2 ⊢H
cut
We then show that the pattern-matching is captured by the cut-elimination:
Lemma 4.4.28. Let Γ ⊢e v : A and ∆⊢e v′ : A such that σ[v] = v′ and
σ = {−→
xj 7→−→
wj} then for any list l = [v1, . . . , vn] where for all i ∈{0, . . . , n}
there exists Γi such that (Γi ⊢e vi) and such that ODA({v, v1, . . . , vn}) and for all
i ∈{0, . . . , n} and e1, . . . , en such that (Γi ⊢e ei : B) and such that such that
ODB({V al(e1), . . . , V al(en)}) and given Θ = {x : Aα | x : A ∈∆} and G = Bβ
we have:
π =
TPos(v′)U
Θ ⊢H
TNeg({((v :: l)i, ei)i∈I})U
H ⊢G cut
Θ ⊢G
⇝∗
TNeg({l, let −→
xi = −→
wi in e})U
Θ ⊢G
=
π′
Proof. By induction on ODA({v, v1, . . . , vn}):
• Case ODA({x}) we get σ[x] = v then π = π′.
• Case OD1({1}) then σ[()] = ()
π =
⊢1 1R
TPos(e)U
⊢G
1 ⊢G 1L
⊢G
cut which reduces to
TPos(e)U
⊢G
= π′ as σ is
empty.
• Case ODµX.A({fold vi}) such that σ[fold vj] = fold v′
Then π =
TPos(v′)U
Θ ⊢H[X ←µX.H]
µR
Θ ⊢µX.H
TNeg({[vi], ei})U
H[X ←µX.H] ⊢G
µL
µX.H ⊢G
Θ ⊢G
cut
will reduce to
TPos(v′)U
Θ ⊢H[X ←µX.H]
TNeg({([vi], ei)})U
H[X ←µX.H] ⊢G
Θ ⊢G
cut
then we can apply our induction hypothesis.
89

• Case ODA⊕B({injl vi} ∪{injr vk}) with σ[injl vj] = injl v′
Then the proof π =
TPos(v′)U
Θ ⊢H
⊕1
R
Θ ⊢H ⊕F
TNeg({[vi], ei})U
H ⊢G
TNeg({[vk], ek})U
F ⊢G ⊕L
H ⊕F ⊢G cut
Θ ⊢G
reduces to
TPos(v′)U
Θ ⊢H
TNeg({[vi], e})U
H ⊢G cut
Θ ⊢G
then we can apply our induction hypothesis.
• The case σ[injr vj] = injr v′ is similar to the previous case.
• Case ODA⊗B({⟨v1
i , v2
i ⟩}) with σ[⟨v1
j , v2
j ⟩] = ⟨v′
1, v′
2⟩
Then π =
TPos(v′
1)U
Θ1 ⊢H
TPos(v′
2)U
Θ2 ⊢F ⊗R
Θ1, Θ2 ⊢H ⊗F
TNeg({([⟨v1, v2,⟩i], ei)i∈I})U
H, F ⊢G
⊗L
H ⊗F ⊢G
cut
Θ1, Θ2 ⊢G
Which reduces to
TPos(v′
1)U
Θ1 ⊢H
TPos(v′
2)U
Θ2 ⊢F
TNeg({((v1 :: v2 :: l)i, ei)i∈I})U
H, F ⊢G
cut
Θ2, H ⊢G
cut
Θ1, Θ2 ⊢G
Because the negative phase on [v1, v2] only produces &, `, ⊤, ν rules,
we get that Neg({(v1 :: v2 :: l, e)}) = Neg({(v2 :: v1 :: l, e)}) by the
commutation of rules of Linear Logic. Therefore, we can get
TPos(v′
1)U
Θ1 ⊢H
TPos(v′
2)U
Θ2 ⊢F
TNeg({((v2 :: v1 :: l)i, ei)i∈I})U
H, F ⊢G
cut
Θ2, H ⊢G
cut
Θ1, Θ2 ⊢G
which, by induction hypothesis on v2, reduces to
TPos(v′
1)U
Θ1 ⊢H
TNeg({((v1 :: l)i, let xj = wj in ei)i∈I})U
H ⊢G cut
Θ1, Θ2 ⊢G
And then we can apply our second induction hypothesis on v1.
We can then conclude with the global simulation theorem as a direct implication
of the two previous lemmas:
90

Theorem 4.4.29 (Iso-substitution cut-elim). Let {v1 ↔e1 | · · · | vn ↔en} v →
σ(ei) when σ[vi] = v then TPos({v1 ↔e1 | · · · | vn ↔en} v)U ⇝∗TPos(let xj =
vj in ei)U ⇝∗TPos(σ(ei))U when σ = {xj 7→vj}
Proof. By Lemma 4.4.25, we know that the explicit substitution and the sub-
stitution coincide. Lemma 4.4.27 tells us that one step of →elet is simulated
by exactly one step of cut-elimination and ﬁnally Lemma 4.4.28 tells us that
we simulate properly the pattern-matching.
Corollary 4.4.30 (Simulation). Provided an iso ⊢ω ω : A ↔B and values ⊢e
v : A and ⊢e v′ : B, let π = TPos(ω v)U and π′ = TPos(v′)U, if ω v →∗v′ then
π ⇝∗π′.
Proof. Direct application of Theorem 4.4.29.
This leads to the following corollary:
Corollary 4.4.31 (Isomorphism of proofs.). Given a well-typed iso ⊢ω ω : A ↔
B and two well-typed close value v1 of type A and v2 of type B and the proofs
π : F1 ⊢G1, π⊥: G2 ⊢F1, φ : F3, ψ : G2 corresponding respectively to the
translation of ω, ω⊥, v1, v2 then:
φ
⊢F3
⇝
φ
⊢F3
π
F1 ⊢G1
⊢G1
cut
π⊥
G2 ⊢F2
⊢F2
cut
ψ
⊢G3
π⊥
F2 ⊢G2
⊢F2
cut
π
F1 ⊢G1
⊢G1
cut ⇝
ψ
⊢G3
Proof. As a direct implication of Theorem 4.2.13 and Corollary 4.4.30.
4.5 . Removing Exhaustivity
While the language presented thus far works with total functions, partial func-
tions are enough to consider reversible computation. In particular, a partial injective
function f can be made reversible by considering its inverse f−1 as being only de-
ﬁned on the codomain of f. In this section we work on partial isos by removing the
constraints of exhaustivity imposed by ODA. We preserve the orthogonality between
values, noted v⊥v′ and deﬁned in Table 4.6. Most of the result from Section 4.2
still holds except for progress, as the term ω v can be stuck and not reduce, if
no pattern of ω matches the value v. What we obtain though is a proof of Tur-
ing Completeness for the language, for that we show how any Reversible Turing
Machine [MY07] can be encoded as a well-typed, partial iso of the language.
4.5.1 . Encoding of Reversible Turing Machines
We want to be able to encode any RTM (Q, Σ, δ, b, qs, qf) into our language.
As mentioned in Deﬁnition 1.1.5, δ is a partial relation: hence the encoded isos
91

injl v⊥injr v′
injr v⊥injl v′
v⊥v′
fold v⊥fold v′
v⊥v′
injl v⊥injl v′
v⊥v′
injr v⊥injr v′
v1⊥v2
⟨v, v1⟩⊥⟨v′, v2⟩
v1⊥v2
⟨v1, v⟩⊥⟨v2, v′⟩
Table 4.6: Orthogonality Condition on Values
will also need to be partial, something not possible with the current typing rules
because of the predicate ODA. We modify the predicate ODA in order to ensure
only non-overlapping through a notion of orthogonality between values, noted
v⊥v′. Obtained isos will then represent partial injective functions. Orthogonality
is deﬁned similarly as in [SVV18] and its deﬁnition is given in Table 4.6. Another
constraint we need to lift is termination: a Turing Machine may not terminate,
while our isos always do.
We can show that this new deﬁnition of orthogonality satisﬁes the usual con-
dition:
Lemma 4.5.1. Given a ﬁnite set of well-typed value S of type A such that for all
v1 ̸= v2 ∈S, v1⊥v2, and a value v of type A, if there exists v1, v2 ∈S such that
σ1[v1] = v and σ2[v2] = v then v1 = v2.
Proof. By induction on σ1[v1] = v
• Case σ1[x] = v: There is no value v2 such that v2⊥x, hence S = {x} so
v2 = x.
• Case σ1[()] = (): Similarly.
• Case σ1[injl v′
1] = injl v′: By deﬁnition of the pattern-matching we
have σ1[v′
1] = v′. The only possible value for v2 is then injl v′
2. By IH we
get v′
1 = v′
2 hence v1 = v2.
• The case for injr v1 and fold v1 are similar.
• Case for σ[⟨v1
1, v2
1⟩] = ⟨v1, v2⟩: By deﬁnition of the pattern-matching
we have σ1[v1
1] = v1 and σ2[v2
1] = v2. By orthogonality, we have that
v2 = ⟨v1
2, v2
2⟩and therefore by IH v1
1 = v1
2 and v2
1 = v2
2 so v1 = v2.
We can also related the OD predicate with this orthogonality:
Lemma 4.5.2. Given a set of well-typed value S = {v1, . . . , vn} of type A, if
ODA(S) then, for all i ̸= j, vi⊥vj.
Proof. By induction on ODA(S).
92

• Direct for the case where S = {x} or {()}.
• Case where S = {injl v | v ∈S1} ∪{injr v | v ∈S2}, then we want to
show that for all i ̸= j, vi⊥vj for vi, vj ∈S. We have three cases:
– If vi, vj ∈S1 then by direction induction hypothesis on S1.
– Similar is vi, vj ∈S2
– If vi ∈S1 and vj ∈S2 (or conversely) then by construction we have
vi = injl v′
i and vj = injr v′
j and are therefore orthogonal.
• Case S = injl fold v | v ∈S′ is direct by induction hypothesis on S′.
• Case where S = {⟨v1, v′
1⟩, . . . , ⟨vn, v′
n⟩}, assuming the left side of the
premise is taken:
We want to show that for all i ̸= j, ⟨vi, v′
i⟩⊥⟨vj, v′
j⟩.
By induction hypothesis we know ODA(π1(S)) and therefore vi⊥v′
j. The
other case being similar.
Then the typing rules for isos become
∆1 ⊢e v1 : A
. . .
∆n ⊢e vn : A
∀i ̸= j, vi⊥vj
∆1; Ψ ⊢e e1 : B
. . .
∆n; Ψ ⊢e en : B
∀i ̸= j, V al(ei)⊥V al(ej)
Ψ ⊢ω {v1 ↔e1 | · · · | vn ↔en} : A ↔B.
f : A ↔B ⊢ω ω : A ↔B
Ψ ⊢ω ﬁx f.ω : A ↔B
In order to encode a Turing Machine T = (Q, Σ, δ, b, qs, qf) into an iso isos(T)
we ﬁrst need to deﬁne suitable types and a representation for each component of
T.
Deﬁnition 4.5.3 (Encoding of States and Tape Symbols). Given a ﬁnite set of
states Q = {q1, . . . , qn} and ﬁnite set of tape symbols Σ = {s1, . . . , sm} with
b ∈Σ, qs, qf ∈Q deﬁne the types QT and ΣT as respectively
n+1 times
z
}|
{
1 ⊕· · · ⊕1 and
m+1 times
z
}|
{
1 ⊕· · · ⊕1.
Then b, qs and qf are values of type QT and ΣT taken by convention as: b =
injl (), qs = injl (), qf = injr injl ().
We write qT (resp. sT ) for a value of type QT (resp. ΣT ) representing the state
q (resp. the letter s) of the RTM.
Since the String Semantics deﬁned on Deﬁnition 1.1.9 is only deﬁned on ter-
minating run that use a ﬁnite amount of tape, we can represent the tapes as a
Zipper: a pair of lists of type ΣT .
93

Deﬁnition 4.5.4 (Encoding of Conﬁgurations). Given a Turing Machine T let
Zipper = [ΣT ] ⊗[ΣT ]
Deﬁne the type of conﬁguration as CT = (QT ⊗Zipper) We represent the
conﬁguration C = (q, (l, a, r)) as isos(C) = (isos(q), (isos(a) :: isos(l), isos(r)))
where isos(l), isos(r) is the list of element of the encoding of the element on tape
where
• If the tape is empty (inﬁnite tape of empty symbol), isos(l) = [].
• If the tape is non-empty but has an inﬁnite suﬃx of empty symbol, i.e it can
be written as l = [s1, . . . , sn, ϵ, ϵ, . . . ] then isos(l) = [isos(s1), . . . , isos(sn)].
We can ﬁnally encode the transition relation δ where each element of δ give
rise to a clause made of values:
Deﬁnition 4.5.5 (Encoding of δ). We encode each rule of δ as a clause of an iso.
• Transition (q, →, q′) is encoded as the clause
(isos(q), (l1, x :: l2)) ↔(isos(q′), (x :: l1, l2)).
• Transition (q, ←, q′) is encoded as the clause
(isos(q), (x :: l1, l2)) ↔(isos(q′), (l1, x :: l2))
• Transition (q, ↓, q′) is encoded as the clause
(isos(q), (l1, l2)) ↔(isos(q′), (l1, l2))
• Transition (q, (s, s′), q′) is encoded as the clause
(isos(q), (sT :: l1, l2)) ↔(isos(q′), (s′T :: l1, l2))
Lemma 4.5.6. Given two diﬀerent states (resp. two letter) a1, a2 we get isos(a1)⊥isos(a2)
Proof. Direct as each state (resp. letter) is represented by a distinct value of
its type.
This lemma tells us that local / backward determinism will imply the orthog-
onality of the clauses of the iso.
Corollary 4.5.7 (isos(T) is well-typed). Given a RTM T, isos(T) is well-typed iso
of type CT ↔CT .
Lemma 4.5.8 (One Step Simulation). Given a RTM T, if T ⊢C ⇝C′ then
isos(T) isos(C) →isos(C′)
Proof. By analysis of the transition relation.
94

• If (q, (l, s, r)) and (q, ↓, q′) ∈δ then the conﬁguration becomes (q′, (l, s, r)).
In this case, by deﬁnition we have a clause (isos(q′), (l, r)) ↔(isos(q′), (l, r)).
The conﬁguration C will be in state isos(q′) and so will enter this clause,
the second clause is indeed the conﬁguration isos(C).
• If (q, (l, s, a :: r)) and (q, →, q′) ∈δ then the conﬁguration becomes
(q′, (l · s, a, r)).
• Similar if (q, ←, q′) ∈δ
• If (q, (l, s, r)) and (q, (s, s′), q′) ∈δ then the conﬁguration becomes (q′, (l, s′, r))
By translation, we have a rule of the form (isos(q′), (s :: l, r)) ↔(isos(q′), (s′ ::
l, r)) which directly send isos(C) to isos(C′).
So far, isos(T) simulates exactly one evaluation step of the Turing Machine, in
order to simulate a full run we need to deﬁne an iterator iso that will apply isos(T)
until the conﬁguration reached is in the ﬁnal state.
Deﬁnition 4.5.9 (Iterator Iso). Let It(ω) : A ↔A ⊗N be an iso parametrized
by another iso ω : A ↔A ⊗(1 ⊕1). Remember that 0 = fold injl () and
S n = fold injr n of type N = µX.1 ⊕X:
First deﬁne ωaux =
 (y, ⊤) ↔let (z, n) = g y in (z, S n)
(y, ⊥) ↔(y, S 0)

Let It(ω) : A ↔A ⊗N be an iso deﬁned as:
ﬁx g.



x ↔let y = ω x in
let z = ωaux y in
z



Assuming that ω is an iso of type A ↔A ⊗(1 ⊕1), then It(ω) will iterate ω
until it returns some value a, ⊥, while counting the number of time it called ω.
Lemma 4.5.10 (Semantics of It(ω)). Given ω : A ↔A⊗(1⊕1) and some value
⊢e v : A if It(ω) v →∗(v′, n) then ω . . . ω
| {z }
n+1 times
v →∗(v′, ⊥)
Proof. By induction on n.
• 0, then It(ω) v →let y = ω v in let z = ωaux y in z →∗let z =
ωaux (v′, ⊥) in z →(v′, 0), and hence by hypothesis ω v →(v′, ⊥).
• n+1: It(ω) v →let y = ω v in let z = ωaux y in z →let z = ωaux (v1, ⊤) in z =
let z =
 (y, ⊤) ↔let (z, n) = It(ω) y in (z, S n)
(y, ⊥) ↔(y, S 0)

(v1, ⊤) in z →
let y = let (z, n) = It v1 in z, S n in y
Then by we know that It v1 will reduce to v′, n and so by IH we get that
ω . . . ω
| {z }
n+1 times
v1 →∗(v′, ⊥), so we get ω . . . ω
| {z }
n+2 times
v →∗(v′, ⊥).
95

With this, we just need to adapt the encoding δ to the type CT ↔CT ⊗(1⊕1):
Deﬁnition 4.5.11 (Encoding of δ). We modify the encoding of δ to be of type
CT ↔CT ⊗(1 ⊕1) to be the same but if the translation leads into the ﬁnal state,
the second argument is sent to ⊥otherwise it is sent to ⊤. By abuse of notation,
we also call this encoding isos(T).
The new encoding does not change the orthogonality and hence the new iso is
well-typed:
Lemma 4.5.12 (isos(δ) is well-typed). Given a RTM T, then ⊢ω isos(δ) : CT ⊗
(1 ⊕1) ↔CT ⊗(1 ⊕1).
Proof. Same as Corollary 4.5.7.
Finally, we get that if a Turing Machine, from an initial conﬁguration C evalu-
ates into the ﬁnal conﬁguration C′ in n + 1 steps, then the Iterator iso on isos(δ)
with input the encoding of C reduces to the encoding of C′ with the encoding of
the number n.
Theorem 4.5.13 (Simulation of String Semantics). Let T be a RTM, if
T ⊢(qs, (ϵ, b, s)) ⇝n+1 (qf, (ϵ, b, s′))
then
It(isos(δ)) isos(qs, ([], b, s)) →∗(isos(qf, (ϵ, b, s′)), ¯n)
.
Proof. Direct by Lemma 4.5.8 and Lemma 4.5.10.
4.6 . Conclusion
Summary of the contribution.
We presented a linear, reversible lan-
guage with inductive types, extending the language from [SVV18]. We showed
how ensuring non-overlapping and exhaustivity is enough to ensure the reversibil-
ity of the isos. The language comes with both an expressivity result that shows
that any Primitive Recursive Functions can be encoded in this language as well
as an interpretation of programs into µMALL proofs. The latter result rests on
the fact that our isos are structurally recursive. We then removed the constraints
of exhaustivity: while still preserving a notion of reversibility on partial injective
functions, we showed how this version of the languages allows us to encode any
Reversible Turing Machine, hence attaining Turing Completeness.
96

Future works.
A ﬁrst extension to our work would be to relax the struc-
turally recursive condition to allow for more functions to be encoded. For instance,
the Cantor Pairing N ↔N ⊗N can be encoded as:
Example 4.6.1 (Cantor Pairing).
ω1 =







⟨S i, j⟩
↔injl (⟨i, S j⟩)
⟨0, S S j⟩↔injl (⟨S j, 0⟩)
⟨0, S 0⟩
↔injl (⟨0, 0⟩)
⟨0, 0⟩
↔injr ()







: N ⊗N ↔(N ⊗N) ⊕1
ω2 =
 tinjl (x) ↔let y = g x in S y
injr (x) ↔0

: (N ⊗N) ⊕1 ↔N
CantorPairing = ﬁx g.
 x ↔let y = ω1 x in
let z = ω2 y in z

: (N ⊗N) ↔N
While the iso has the expected operational semantics, it is not well-typed as it
is not structurally recursive. One would require accepting lexicographical order (or
more generally, any well-founded order) on recursive isos that act as a termination
proof. And then, see how such a criterion would be captured in terms of pre-proof
validity.
Along with this, allowing for coinductive statements and terms would
allow for a truly general reversible language. This is a focus of our forthcoming
research.
On a more denotational point of view, the works in collaboration with Louis
Lemonnier [CLV21] is currently being extended to the case of the language pre-
sented in this chapter, along to a categorical semantics, with Robin Kaarsgaard,
for the quantum version of the language from [SVV18] extended with inductive
types, as done in this chapter.
Finally, we want to consider quantum computation, by extending our language
with linear combinations of terms. We plan to study purely quantum recursive
types and generalized quantum loops: in [SVV18], lists are the only recursive type
which is captured, and recursion is terminating. The logic µMALL would help in
providing a ﬁner understanding of termination and non-termination.
97


5 - Geometry of Interaction for ZX Calculus
Abstract
Getting inspiration from the Geometry of Interaction as a Token Ma-
chine, we introduce a Token Machine for the ZX-Calculus and its
extension to mixed-processes. We show how the Token Machine cap-
tures the denotational semantics of the ZX-Calculus. We then discuss
variants of the token machine, in particular in the context of Sum-
Over-Paths semantics.
References: Results of this chapter have been published in the paper
Geometry of Interaction for ZX-Calculus at MFCS 2021 [CVV21].
5.1 . Introduction
As seen previously, the standard models of both quantum circuits and the ZX-
Calculus is based on linear operators in some Hilbert space, most often described
with a matrix interpretation. An alternative operational interpretation of quantum
circuits following a particle-style semantics has recently been investigated in the lit-
erature [Dal17]. In this model, quantum bits are intuitively seen as tokens ﬂowing
inside the wires of the circuit. Formally, a quantum circuit is interpreted as a token-
based automata, based on Geometry of Interaction (GoI) [Gir89b; Gir89a; Gir88;
Gir95; Gir06; Gir11; Gir13]. This framework is used in [Dal17] to formalize the
notion of qubits-as-tokens ﬂowing inside a higher-order term representing a quan-
tum computation—that is, computing a quantum circuit. However, in this work,
quantum gates are still regarded as black-boxes, and tokens are purely classical
objects requiring synchronicity: to ﬁre, a two-qubit gate needs its two arguments
to be ready.
As a summary, despite their ad-hoc construction, quantum circuits can be seen
from two perspectives: computation as a ﬂow of particles (i.e., tokens), and as a
wave passing through the gates, i.e. the standard vectorial state representation.
On the other hand, although ZX-Calculus is a well-established language, it still
misses such a particle-style perspective.
In this chapter, we aim at giving a novel insight on the computational content
of a ZX term in an asynchronous way, emphasizing the non-locality of the behaviour
of a ZX-computation.
Following the idea of using a token machine to exhibit the computational con-
tent of a proof-net or a quantum circuit, we present in this chapter a token machine
for the ZX-Calculus. To exemplify the versatility of the approach, we show how
to extend it to mixed processes [CP12; Car+19] and to the Sum-Over-Path se-
mantics, the development for both perspectives being very similar.
Those two
99

perspectives are related to the notion of particle-style and wave-style semantics,
similarly to what has been done for quantum circuits [Dal17]. To assess the va-
lidity of the semantics, we show how it links to the standard interpretation of
ZX-diagrams.
While the standard interpretation of ZX-diagrams proceeds with
conventional graph rewriting, the tokens ﬂowing inside the diagram do not modify
it, and the computation emerges from the ability of tokens to be in superposition.
This ability illustrates one fundamental diﬀerence between our approach and
the one in [Dal17]. The latter follows a classical control approach: if qubits can
be in superposition, each qubit inhabits a token sitting in one single position in
the circuit. For instance, on the circuit below, the state of the two tokens |••⟩is
√
2
2 (|00⟩+|10⟩). Although the two tokens can be regarded as being in superposition,
their position is not. In our system, tokens and positions can be superposed.
H
H
|0⟩
|0⟩
1
2
CNOT
The second fundamental diﬀerence lies in the asynchronicity of our token-
machine.
Unlike [Dal17], we rely on the canonical generators of ZX-diagrams:
tokens can travel through these nodes in an asynchronous manner. For instance,
in the above circuit the orange token must wait for the blue token before crossing
the CNOT gate. As illustrated Table 5.1, in our system one token can interact
with multi-wire nodes. Finally, as formalized in Theorem 5.3.25, a third diﬀerence
is that compared to [Dal17], the token-machine we present is non-oriented: in the
circuit above, tokens have to start on the left and ﬂow towards the right of the
circuit whereas our system is agnostic on where tokens initially “start”.
Organization of the chapter
The chapter is organized as follows: in
Section 5.3 we present the ﬁrst token machine, with the rewriting invariant needed
to obtain conﬂuence, termination and the relation with the standard interpreta-
tion of the ZX-Calculus, then in Section 5.4 we consider the extension of mixed-
processes and relate the new token machine with the previous one through the
use of the map CPM. Then we brieﬂy discuss two variations of the token machine
in Section 5.5. Finally, in Section 5.6 we present the Sum-Over-Paths semantics
token machine.
5.2 . Notions of Graph Theory in ZX
100

In order to work with our token machine, we need to attribute to each wire of
the ZX-Diagram a name, given as such:







e0,
e0
e1, e0
e1, e0
e1,
...
e1 en
e′
1 e′
m
...
α ,
e0
e1






n,m∈N
α∈R
ei,e′
i∈E
We shall be using the following labelling convention: wires (edges) are labelled
with ei, taken from an inﬁnite set of labels E. We take for granted that distinct
wires have distinct labels. We write E(D) for the set of edge labels in the diagram
D, and I(D) (resp. O(D)) for the list of input edges (resp. output edges) of D.
We need to have a special treatment of edge labels when composing two
diagrams D2 and D1:
For the sequential composition D2 ◦D1 we need to do a relabelling of the
input edges of the bottom diagram by the output labels of the top diagram, we
also require that E(D2 ◦D1) = E(D1) ∪E(D2)\I(D2), I(D2 ◦D1) = I(D1) and
O(D2 ◦D1) = O(D2).
While for the parallel composition we require that E(D2 ⊗D1) = E(D1) ∪
E(D2), I(D2 ⊗D1) = I(D2) ∪I(D1) and O(D2 ⊗D1) = O(D2) ∪O(D1).
Remind that we assume that E(D1) ∩E(D2) = ∅
Theorem 2.1.5 is essential: it allows us to transpose notions of graphs into
ZX-Calculus. It is for instance possible to deﬁne a notion of connectivity.
Deﬁnition 5.2.1 (Connected Components). Let D be a non-empty ZX-diagram.
Consider all the possible decompositions with D1, ..., Dn ∈ZX and σ, σ′ permu-
tations of wires:
D = ...
D1
σ′...
... σ
...
...
Dk
...
...
The largest such k is called the number of connected components
of D. It induces a decomposition. The induced D1, ..., Dn are
called the connected components of D. If D has only one con-
nected component, we say that D is connected.
We can also consider the notions of paths, distance and cycles of usual multi-
graphs. We denote Paths(e, e′) the set of paths from edge e to e′. The set of
paths (resp. cycles) of a diagram D is denoted by Paths(D) (resp. Cycles(D)).
For a path p, we denote |p| its length. We denote d(e, e′) the distance i.e. the
length of the shortest path between e and e′.
In the remainder of the thesis, we omit the edge labels when not necessary.
5.3 . A Token Machine for ZX-diagrams
Inspired by the Geometry of Interaction [Gir89b; Gir89a; Gir88; Gir95; Gir06;
Gir11; Gir13] and the associated notion of token machine [DR99; AL95] for proof
101

nets [Gir96], we deﬁne here a ﬁrst token machine on pure ZX-diagrams. A token
consists of an edge of the diagram, a direction (either going up, noted ↑, or down,
noted ↓) and a bit (state). The idea is that, starting from an input edge the token
will traverse the graph and duplicate itself when encountering an n-ary node (such
as the green and red node) into each of the input / output edges of the node.
Notice that it is not the case for token machines for proof-nets where the token
never duplicates itself. This duplication is necessary to make sure we capture the
whole linear map encoded by the ZX-diagram. Due to this duplication, two tokens
might collide together when they are on the same edge and going in diﬀerent
directions. The result of such a collision will depend on the states held by both
tokens. For a cup, cap or identity diagram, the token will simply traverse it. As for
the Hadamard node the token will traverse it and become a superposition of two
tokens with opposite states. Therefore, as tokens move through a diagram, some
may be added, multiplied together, or annihilated.
Deﬁnition 5.3.1 (Tokens and Token States). Let D be a ZX-diagram. A token in
D is a triplet (e, d, b) ∈E(D) × {↓, ↑} × {0, 1}. We shall omit the commas and
simply write (e d b). The set of tokens on D is written tk(D). A token state s is
then a multivariate polynomial over C, evaluated in tk(D). We deﬁne tkS(D) :=
C[tk(D)] the algebra of multivariate polynomials over tk(D).
In the token state t = P
i αi t1,i · · · tni,i, where the tk,i’s are tokens, the com-
ponents αi t1,i · · · tni,i are called the terms of t.
A monomial (e1 d1, b1) · · · (en dn, bn) encodes the state of n tokens in the
process of ﬂowing in the diagram D. A token state is understood as a superposition
—a linear combination— of multi-tokens ﬂowing in the diagram.
Convention 5.3.2. In token states, the sum (+) stands for the superposition while
the product stands for additional tokens within a given diagram. We follow the
usual convention of algebras of polynomials: for instance, if ti stands for some
token (ei di bi), then (t1 +t2)t3 = (t1t2)+(t1t3), that is, the superposition of t1,t2
ﬂowing in D and t1,t3 ﬂowing in D. Similarly, we consider token states modulo
commutativity of sum and product, so that for instance the monomial t1t2 is the
same as t2t1. Notice that 0 is an absorbing element for the product (0 × t = 0)
and that 1 is a neutral element for the same operation (1 × t = t).
Example 5.3.3. Given D =
...
e1 en
e′
1 e′
m
...
α then (e1 ↑0)(e2 ↑1) + (e′
3 ↓1) is a token state
on D.
5.3.1 . Diﬀusion and Collision Rules
The tokens in a ZX-diagram D are meant to move inside D. The set of rules
presented in this section describes an asynchronous evolution, meaning that given
102

e0
(e0 ↓x)(e0 ↑x) ⇝c 1
(e0 ↓x)(e0 ↑¬x) ⇝c 0
(Positive/Negative Collision)
e0
e1
(eb ↓x) ⇝d (e¬b ↑x)
(
-diﬀusion)
e0
e1
(eb ↑x) ⇝d (e¬b ↓x)
(
-diﬀusion)
(ek ↓x) ⇝d eiαx Y
i̸=k
(ei ↑x)
Y
j
(e′
j ↓x)
(e′
k ↑x) ⇝d eiαx Y
j̸=k
(e′
j ↓x)
Y
i
(ei ↑x)
(e0 ↓x) ⇝d (−1)x 1
√
2(e1 ↓x) + 1
√
2(e1 ↓¬x)
(e1 ↑x) ⇝d (−1)x 1
√
2(e0 ↑x) + 1
√
2(e0 ↑¬x)
...
e1 en
e′
1 e′
m
...
α
e0
e1
( -Diﬀusion)
(
...
...
-
Diﬀusion)
Table 5.1: Asynchronous token-state evolution, for all x, b ∈{0, 1}
a token state, we will rewrite only one token at a time. The synchronous setting
is discussed in Section 5.5.2.
Deﬁnition 5.3.4 (Asynchronous Evolution). Token states on a diagram D are
equipped with two transition systems:
• a collision system (⇝c), whose eﬀect is to annihilate tokens;
• a diﬀusion system (⇝d), deﬁning the ﬂow of tokens within D.
The two systems are deﬁned as follows. With X ∈{d, c} and 1 ≤j ≤ni, if ti,j are
tokens in tk(D), then using Convention 5.3.2,
X
i
αiti,1 · · · ti,j · · · ti,ni ⇝X
X
i
αiti,1 · · ·
 X
k
βkt′
k
!
· · · ti,ni
provided that ti,j ⇝X
P
k βkt′
k according to the rules of Table 5.1. In the table,
each rule corresponds to the interaction with the primitive diagram constructor on
the left-hand-side. Variables x and b span {0, 1}, and ¬ stands for the negation.
In the green-spider rules, eiαx stands for the complex number cos(αx)+i sin(αx)
and not an edge label.
Finally, as it is customary for rewrite systems, if (→) is a step in a transition
system, (→∗) stands for the reﬂexive, transitive closure of (→).
We do not give the rewriting rule for the red-spider since it can be recovered
by Convention 2.1.1, for reference one can check that the rules in Table 5.2 are
correct.
We aim at a transition system marrying both collision and diﬀusion steps.
However, for consistency of the system, the order in which we apply them is
important as illustrated by the following example.
103

α
a
(a ↓x) ⇝d
1
√
2
 1 + (−1)xeiα
α
a
b
c
(a ↓x) ⇝d
1
2
√
2
X
y,z
 1 + (−1)x+y+zeiα
(b ↓y)(c ↓z)
Table 5.2: Samples of asynchronous token-state evolution for red spi-
ders
Example 5.3.5. Consider the equality given by the ZX equational theories:
=
.
If we drop a token with bit 0 at the top, we hence expect to get a single token with
bit 0 at the bottom. We underline the token that is being rewritten at each step.
This is what we get when giving the priority to collisions:
a
d
b
c
(a ↓0) ⇝d (b ↓0)(c ↓0) ⇝d (d ↓0)(c ↑0)(c ↓0) ⇝c (d ↓0)
Notice that the collision (c ↑0)(c ↓0) rewrites to 1, and therefore the product
(d ↓0) × 1 = (d ↓0). If however we decide to ignore the priority of collisions, we
may end up with a non-terminating run, unable to converge to (d ↓0):
(a ↓0) ⇝d (b ↓0)(c ↓0) ⇝d (d ↓0)(c ↑0)(c ↓0) ⇝d (d ↓0)(a ↑0)(b ↓0)(c ↓0) ⇝d . . .
We therefore set a rewriting strategy as follows.
Deﬁnition 5.3.6 (Collision-Free). A token state s of tkS(D) is called collision-
free if for all s′ ∈tkS(D), we have s ̸⇝c s′.
Deﬁnition 5.3.7 (Token Machine Rewriting System). We deﬁne a transition sys-
tem ⇝as exactly one ⇝d rule followed by all possible ⇝c rules. In other words,
t ⇝u if and only if there exists t′ such that t ⇝d t′ ⇝∗
c u and u is collision-free.
In [Dal17], a token arriving at an input of a gate is blocked until all the inputs
of the gates are populated by a token, at which point all the tokens go through
at once (while obviously changing the state). The control is purely classical: it is
causal. In our approach, the state of the system is global and there is no explicit
notion of qubit. Instead, tokens collect the operations that are to be applied to
the input qubits.
104

5.3.2 . Strong Normalization and Conﬂuence
The token machine Rewrite System of Deﬁnition 5.3.7 ensures that the col-
lisions that can happen always happen. The system does not a priori forbid two
tokens on the same edge, provided that they have the same direction. However,
this is something we want to avoid as there is no good intuition behind it: we want
to link the token machine to the standard interpretation, which is not possible if
two tokens can appear on the same edge.
In this section we show that, under a notion of well-formedness characterizing
token’s uniqueness on each edge, the Token State Rewrite System (⇝) is strongly
normalizing and conﬂuent.
Deﬁnition 5.3.8 (Polarity of a Term in a Path). Let D be a ZX-diagram, and
p ∈Paths(D) be a path in D. Let t = (e, d, x) ∈tk(D). Then:
P(p, t) =





1
if e ∈p and e is d-oriented
−1
if e ∈p and e is ¬d-oriented
0
if e /∈p
We extend the deﬁnition to subterms α t1...tm of a token-state s:
P(p, 0) = P(p, 1) = 0,
P(p, α t1...tm) = P(p, t1) + ... + P(p, tm).
In the following, we shall simply refer to such subterms as “terms of s”.
Example 5.3.9. In the (piece of) diagram presented below, the blue directed line
p = (e0, e1, e2, e3, e4) is a path. The orientation of the edges in the path is rep-
resented by the arrow heads, and e3 for instance is ↓-oriented in p which implies
that we have P(p, (e3 ↑x)) = −1.
e0
e1
e2
e3
e4
Deﬁnition 5.3.10 (Well-formedness). Let D be a ZX-diagram, and s ∈tkS(D)
a token state on D. We say that s is well-formed if for every term t in s and every
path p ∈Paths(D) we have P(p, t) ∈{−1, 0, 1}.
Intuitively, this deﬁnition tells us that when we have multiple tokens on the
same path, they will all collide with each other until there is at most one left, going
in either direction.
Proposition 5.3.11 (Invariance of Well-Formedness). Well-formedness is pre-
served by (⇝): if s ⇝∗s′ and s is well-formed, then s′ is well-formed.
Proof. Let D be a ZX-diagram, and s be a well-formed token state on D. Con-
sider a rewrite s ⇝s′. We want to show that for all paths p in D, if P(p, t) ∈
{−1, 0, 1} for all terms t of s, then P(p, t′) ∈{−1, 0, 1} for all terms t′ in s′.
105

Let t be a term of s, and e0 be the edge where a rewriting occurs. If the
rewriting does not aﬀect t, then the well-formedness of t obviously holds. If
it does, and t ⇝c,d
P
q tq, we have to check two cases:
• Collision: let p ∈Paths(D). If no token remains in the term tq, then
P(p, tq) = 0. Otherwise:
– if e0 /∈p, then P(p, tq) = P(p, t) = 0;
– if e0 ∈p, then P(p, tq) = P(p, t) + 1 −1 because the two tokens
have alternating polarity.
• Diﬀusion: let p ∈Paths(D), and (e0, d, x) ⇝d
P
q λq
Q
i∈S(ei, di, xi,q)
(this captures all possible diﬀusion rules).
– if e0 /∈p and ∀i ∈S, ei /∈p, then P(p, tq) = P(p, t);
– if e0 ∈p and ∃k1, . . . , kn ∈S such that ∀i ∈{1, . . . , n}, eki ∈p then
we want to show that P(p, (e0, d, x)) = P
i∈{1,...,n} P(p, (eki, di, xi)).
For that, consider a subpath forming a cycle c between ki and kj,
both ki and kj will have a token on it as the result of the diﬀusion
rule of e0, then we can reason by case analysis on the orientation
of the path on ki and kj, and it can be shown that in everything
case we have P(c, (ki, di, xi)) + P(c, (kj, dj, xj)) = 0. Hence the
polarity on the whole path p is preserved;
– if e0 ∈p and ∀i, ei /∈p, then, either (i) p ends with e0 and e0 is
d-oriented in p, or (ii) p starts with e0 and e0 is ¬d-oriented in p. In
both cases, since that p\{e0} is still a path, we have P(p\{e0}, t) ∈
{−1, 0, 1} and since P(p, tq) = P(p \ {e0}, t), we deduce that tq is
still well-formed;
– if e0 /∈p but ∃k ∈S, ek ∈p, either ek is an endpoint of p, or
∃k′, ek′ ∈p. In the latter case, the tokens in ek and ek′ will have
alternating polarity in p, so ∀q, P(p, tq) = P(p, t) + 1 −1. In the
ﬁrst case, we can show in a way similar to the previous point, that
P(p, tq) = P(p \ {ek}, t) ∈{−1, 0, 1}.
Well-formedness prevents the unwanted scenario of having two tokens on the
same wire, and oriented in the same direction (e.g. (e0 ↓x)(e0 ↓y)). As shown in
the Proposition 5.3.12, this property is in fact stronger.
Proposition 5.3.12 (Full Characterization of Well-Formed Terms). Let D be a
ZX-diagram, and s ∈tkS(D) be ill-formed, i.e. there exists a term t in s, and
p ∈Paths(D) such that |P(p, t)| ≥2. Then we can rewrite s ⇝s′ such that a
term in s′ has a product of at least two tokens of the form (e0, d, _).
106

Proof. Let t be a term in s, and p = (e0, ..., en) such that P(p, t) ≥2. We can
show that we can rewrite t into a token state with term t′ = (ei, d, _)(ei, d, _)t′′.
We do so by induction on n = |p| −1.
If n = 0, we have a path constituted of one edge, such that |P(p, t)| ≥2.
Even after doing all possible collisions, we are left with |P(p, t)| tokens on e0,
and oriented accordingly.
For n + 1, we look at e0, build p′ := (e1, ..., en), and distinguish four cases.
• If there is no token on e0, we have P(p′, t) = P(p, t), so the result is true
by induction hypothesis on p′.
• If we have a product of at least two tokens going in the same direction,
the result is directly true.
• If we have exactly one token going in each direction, we apply the colli-
sion rules, and therefore we have P(p′, t) = P(p, t), so the result is true
by induction hypothesis on p′.
• Finally, if we have exactly one token (e0, d, _) on e0, either e0 is not d-
oriented, in which case P(p′, t) = P(p, t)+1, or e0 is d-oriented, in which
case the adequate diﬀusion rule on (e0, d, _) will rewrite t ⇝P
q tq with
P(p′, tq) = P(p, t).
Although well-formedness prevents products of tokens on the same wire, it does
not guarantee termination: for this we need to consider polarities along cycles.
Proposition 5.3.13 (Invariant on Cycles). Let D be a ZX-diagram, and c ∈Cycles(D)
a cycle. Let t1, . . . , tn be tokens, and s be a token state such that t1...tn ⇝∗s. Then
for every non-null term t in s we have P(c, t1...tn) = P(c, t).
Proof. The proof can be adapted from the previous one, by forgetting the
cases related to the endpoint of the paths, as well as the null terms (which
can arise from collisions). It can then be observed that the quantity P in this
simpliﬁed setting is more than bounded to {−1, 0, 1}, but preserved.
This proposition tells us that the polarity is preserved inside a cycle. By re-
quiring the polarity to be 0, we can show that the token machine terminates. This
property is deﬁned formally in the following.
Deﬁnition 5.3.14 (Cycle-Balanced Token State). Let D be a ZX-diagram, and
t a term in a token state on D. We say that t is cycle-balanced if for all cycles
c ∈Cycles(D) we have P(c, t) = 0. We say that a token state is cycle-balanced
if all its terms are cycle-balanced.
107

To show that being cycle-balanced implies termination, we need the following
intermediate lemma. This essentially captures the fact that a token in the diagram
comes from some other token that “travelled” in the diagram earlier on.
Lemma 5.3.15 (Rewinding). Let D be a ZX-diagram, and t be a term in a well-
formed token state on D, and such that t ⇝∗P
i λiti, with (en, d, x) ∈t1. If t is
cycle-balanced, then there exists a path p = (e0, ..., en) ∈Paths(D) such that en
is d-oriented in p, and P(p, t) = 1.
Proof. We reason by induction on the length k of the rewrite that leads from
t to P
i λiti.
If k = 0, we have (en, d, x) ∈t, so the path p := (en) is suﬃcient.
For the induction case, k = n+1, suppose t ⇝P
i λiti, and t1 ⇝n P
j λ ′
jt′
j
(hence t ⇝n+1 P
i̸=1 λiti + P
j λ ′
jt′
j), with (en, d, x) ∈t′
1. By induction hypoth-
esis, there is p = (e0, ..., en) such that P(p, t1) = 1. We now need to look at
the ﬁrst rewrite from t.
• if the rewrite concerns a generator not in p, then P(p, t) = P(p, t1) = 1;
• if the rewrite is a collision, then P(p, t) = P(p, t1) = 1;
• if the rewrite is (e, de, xe) ⇝P
q λq
Q
i(e′
i, di, xi,q)
– If e ∈p and e′
1 ∈p, then P(p, t) = P(p, t1) = 1.
– If e′
1 ∈p and e′
2 ∈p, then P(p, t) = P(p, t1) −1 + 1 = 1.
– The case e ∈p and ∀i, e′
i /∈p is impossible:
* if e is not de-oriented in p, it means e = e0, hence P((e1, ..., en), t) =
P(p, t) + 1 = 2 which is forbidden by well-formedness;
* if e is de-oriented in p, it means e = en, which would imply that
P(p, t1) = 0.
– If e /∈p and e′
1 ∈p and ∀i ̸= 1, e′
i /∈p, then P(e :: p, t) = P(p, t1) =
1, since well-formedness prevents the otherwise possible situa-
tion P(e :: p, t) = P(p, t1) + 1 = 2. However, e :: p may not be
a path any more. If c = (e, e0, ..., eℓ) forms a cycle, then, since
P(c, t) = 0, we can simply keep the path p′ := (eℓ+1, ..., en) with
P(p′, t) = 1.
We can now prove strong-normalization.
Theorem 5.3.16 (Termination of well-formed, cycle-balanced token state). Let
D be a ZX-diagram, and s ∈tkS(D) be well-formed. The token state s is strongly
normalizing if and only if it is cycle-balanced.
108

Proof. [⇒]: Suppose ∃c ∈Cycles(D) and t a term of s such that P(c, t) ̸= 0. By
well-formedness, P(c, t) ∈{−1, 1}. Any terminal term t′ has P(c, t′) = 0, so
by preservation of the quantity P(c, _), t (and henceforth s) cannot terminate.
[⇐]: We are going to show for the reciprocal that, if t is well-formed, and
if the constraint P(c, t) = 0 is veriﬁed for every cycle c, then any generator
in the diagram can be visited at most once. More precisely, we show that if
a generator is visited in a term t, then it cannot be visited any more in all the
terms derived from t. However, the same generator can be visited once for
each superposed term (e.g. once in t1 and once in t2 for the token state t1+t2).
Consider an edge e with token exiting generator g in the term t. Suppose, by
contradiction, that a token will visit g again in t′ (obtained from t), by edge en
with orientation d. By Lemma 5.3.15, there exists a path p = (e0, ..., en) such
that P(p, t) = 1 and en is d-oriented. Since e /∈p (we would not have a path
then), then p′ := (e0, ..., en, e) is a path (or possibly a cycle) such that P(p′, t) =
2. This is forbidden by well-formedness. Hence, every generator can be visited
at most once. As a consequence, the lexicographic order (#g, #tk) (where #g
is the number of non-visited generators in the diagram, and #tk the number
of tokens in the diagram) strictly reduces with each rewrite. This ﬁnishes the
proof of termination.
Intuitively, this means that tokens inside a cycle will cancel themselves out if
the token state is cycle-balanced. Since cycles are the only way to have a non-
terminating token machine, we are sure that our machine will always terminate.
Example 5.3.17. Going back to the diagram from Example 5.3.5:
a
d
b
c consider
the token state (b ↓0)(c ↑0) we get that the polarity in the cycle (b, c, b) is 2 and
hence the token state will not terminate, which is indeed the case as:
(b ↓0)(c ↑0) ⇝d (b ↓0)(b ↓0)(a ↑0) ⇝d (b ↓0)(c ↑0)(d ↓0)(a ↑0)
The tokens (b ↓0) and (c ↑0) will never collide, hence termination cannot be
ensured.
Proposition 5.3.18 (Local Conﬂuence). Let D be a ZX-diagram, and s ∈tkS(D)
be well-formed and collision-free. Then, for all s1, s2 ∈tkS(D) such that s1
⇝
s ⇝
s2, there exists s′ ∈tkS(D) such that s1 ⇝∗s′ ∗
⇝
s2.
Proof. We are going to reason on every possible pair of rewrite rules that can
be applied from a single token state s. Notice ﬁrst, that if the two rules are
applied on two diﬀerent terms of s, such that the rewriting of a term creates
a copy of the other, they obviously commute, so
s ⇝s2
⇝
⇝
s1 ⇝s′
.
109

In the case where s = αt + βt1 + s0 such that t1 ⇝s′ and t ⇝P
i λiti, we
have:
⇝
αt + βs′ + s0
⇝
P
i αλiti + βs′ + s0
s
⇝
⇝
(αλ1 + β)t1 + P
i̸=1 αλiti + s0 ⇝(αλ1 + β)s′ + P
i̸=1 αλiti + s0
Then, we can, in the following, focus on pairs of rules applied on the same
term.
The term we focus on is obviously collision-free, by hypothesis and by preser-
vation of collision-freeness by ⇝.
Suppose the two rewrites are applied on tokens at positions e and e′. We
may reason using the distance between the two edges.
• The case d(e, e′) = 0 would imply a collision, which is impossible by
collision-freeness;
• if d(e, e′) ≥3, the two rules still do not interfere, they commute (up to
collisions which do not change the result);
• if d(e, e′) = 2, there will be common collisions (i.e. collisions between
tokens created by each of the diﬀusions), however, the order of appli-
cation of the rules will not change the bits in the tokens we will apply a
collision on, so the result holds;
• if d(e, e′) = 1, then the two tokens have to point to the same generator.
If they didn’t, (e, e′) would form a path such that |P((e, e′), t)| = 2 which
is forbidden by well-formedness. We can then show the property for all
generators:
Case e0
e1.
(e0 ↓x)(e1 ↓x′) ⇝d (e1 ↑x)(e1 ↓x′)
⇝
d
⇝
c
(e0 ↓x)(e0 ↑x′) ⇝c

x x′
Case e0
e1: similar.
Case
...
e1 en
e′
1 e′
m
...
α .
eiαx Q
i̸=1(ei ↑x) Q
i(e′
i ↓x)(e′
1 ↑x′)
⇝
c
⇝
d

x x′
eiαx Q
i̸=1(ei ↑x) Q
i̸=1(e′
i ↓x)
(e1 ↓x)(e′
1 ↑x′)
| |
⇝
d

x x′
eiαx′ Q
i̸=1(ei ↑x′) Q
i̸=1(e′
i ↓x′)
eiαx′ Q
i(ei ↑x′) Q
i̸=1(e′
i ↓x)(e1 ↓x)
⇝
c
110

Case
e0
e1
.
1
√
2 ((−1)x(e1 ↓x)(e1 ↑x′) + (e1 ↓¬x)(e1 ↑x′))
⇝
2
c
⇝
d
1
√
2
 (−1)x 
x x′
+

¬x x′
(e0 ↓x)(e1 ↑x′)
| |
⇝
d
1
√
2

(−1)x′ 
x x′
+

x ¬x′
1
√
2

(−1)x′(e0 ↓x)(e0 ↑x′) + (e0 ↓x)(e0 ↑¬x′)

⇝
2
c
Using Newmann’s Lemma [New42] that states that any terminating and locally
conﬂuent rewriting system is conﬂuent, we obtain the conﬂuence of our rewriting
system:
Corollary 5.3.19 (Conﬂuence). Let D be a ZX-diagram. The rewrite system ⇝is
conﬂuent for well-formed and cycle-balanced token states.
Corollary 5.3.20 (Uniqueness of Normal Forms). Let us consider a ZX-diagram
D. A well-formed and cycle-balanced token state admits a unique normal form
under the rewrite system ⇝.
5.3.3 . Semantics and Structure of Normal Forms
In this section, we discuss the structure of normal forms, and relate the system
to the standard interpretation presented in Section 2.1.
Proposition 5.3.21 (Single-Token Input). Let D : n →m be a connected ZX-
diagram with I(D) = [ai]0<i≤n and O(D) = [bi]0<i≤m, 0 < k ≤n and x ∈
{0, 1}, such that:
JDK ◦(idk−1 ⊗|x⟩⊗idn−k) =
2m+n−1
X
q=1
λq |y1,q, ..., ym,q⟩⟨x1,q, ..., xk−1,q, xk+1,q, ..., xn,q|
Then:
(ak ↓x) ⇝∗
2m+n−1
X
q=1
λq
Y
i
(bi ↓yi,q)
Y
i̸=k
(ai ↑xi,q)
Proof. Let us ﬁrst notice that, using the map/state duality, we have
(ak ↓x) ⇝∗
2m+n−1
X
q=1
λq
Y
i
(bi ↓yi,q)
Y
i̸=k
(ai ↑xi,q)
in D iﬀwe have
(ak ↓x) ⇝∗
2m+n−1
X
q=1
λq
Y
i
(bi ↓yi,q)
Y
i̸=k
(a′
i ↓xi,q)
111

in D′ where
D′
...
:=
...
D
...
...
ak
ak
a′
1
a′
n
. Hence, we can, w.l.o.g. consider in the
following that n = 1. We also notice that, thanks to the conﬂuence of the
rewrite system, we can consider diagrams up to "topological deformations",
and hence ignore cups and caps.
We then proceed by induction on the number N of “non-wire generators”
(i.e. Z-spider, X-spiders and H-gates) of D, using the fact that the diagram is
connected:
If N = 0, then D = , where the result is obvious.
If N = 1, then D ∈


, ,
α
...
...
,
α
...
...


. The result in this base case is
then a straightforward veriﬁcation (self-loops in green and red nodes simply
give rise to collisions that are handled as expected).
For N + 1, there exists D′ with N non-wire generators such that
D ∈





D′
... , D′
...
...α
, D′
...
...α





(we should actually take into account the self-loops, but they do not change
the result). Let us look at the ﬁrst two cases, since the last one can be induced
by composition.
If D =
D′
...
a
a′
b1
bm
, then D′ is necessarily connected, by connectivity of D. Then:
(a ↓x) ⇝(−1)x
√
2 (a′ ↓x) + 1
√
2(a′ ↓¬x)
⇝∗(−1)x
√
2
2m
X
q=1
λq
m
Y
i=1
(bi ↓yi,q) + 1
√
2
2m
X
q=1
λ ′
q
m
Y
i=1
(bi ↓yi,q)
=
2m
X
q=1
λ ′
q + (−1)xλq
√
2
m
Y
i=1
(bi ↓yi,q)
whereby induction hypothesis
q
D′y
|x⟩=
2m
X
q=1
λq |y1,q, ..., ym,q⟩
and
q
D′y
|¬x⟩=
2m
X
q=1
λ ′
q |y1,q, ..., ym,q⟩
112

so:
JDK |x⟩=
q
D′ ◦H
y
|x⟩=
q
D′y
◦JHK |x⟩=
q
D′y
◦
(−1)x
√
2
|x⟩+ 1
√
2 |¬x⟩

= (−1)x
√
2
q
D′y
|x⟩+ 1
√
2
q
D′y
|¬x⟩=
2m
X
q=1
λ ′
q + (−1)xλq
√
2
|y1,q, ..., ym,q⟩
which is the expected result.
Now, if D =
D′
...
...α
, we can decompose D′ in its connected components:
D =
D′
...
...α
=
...
D1
σ...
...
...
Dk
...
...
α
a
a1,1
a1,n1
ak,nk
ak,1
b1
bm
bm1bm−mk
with Di connected. Then:
(a ↓x) ⇝eiαx Y
ℓ
Y
i
(aℓ,i ↓x)
⇝∗eiαx Y
ℓ


2mℓ+nℓ−1
X
q=1
λq,ℓ
Y
i̸=1
(aℓ,i ↓x)(aℓ,i ↑xℓ,i,q)
Y
i
(bℓ,i ↓yℓ,i,q)


⇝∗eiαx Y
ℓ


2mℓ+nℓ−1
X
q=1
λq,ℓδx,xℓ,i,q
Y
i
(bℓ,i ↓yℓ,i,q)


= eiαx Y
ℓ


2mℓ
X
q=1
λ ′
q,ℓ
Y
i
(bℓ,i ↓yℓ,i,q)


= eiαx
2m1
X
q1=1
...
2mk
X
qk=1
λ ′
q1,1...λ ′
qk,k
Y
i
(b1,i ↓y1,i,q1)...
Y
i
(bk,i ↓yk,i,qk)
=
2m
X
q=1
λ ′
q
Y
i
(bi ↓yi,q)
where the ﬁrst is the diﬀusion through a Z-spider, and the second set of rewrites
is the induction hypothesis applied to each connected component.
JDK |x⟩=
q
(D1 ⊗... ⊗Dk) ◦Z1
k(α)
y
|x⟩= (JD1K ⊗... ⊗JDkK) ◦
q
Z1
k(α)
y
|x⟩
113

= eiαx(JD1K ⊗... ⊗JDkK) ◦|x, ..., x⟩= eiαx JD1K |x, ..., x⟩⊗... ⊗JDkK |x, ..., x⟩
= eiαx


2m1+n1−1
X
q1
λq1,1 |y1,1,q1, ..., y1,m1,q1⟩

x1,2,q1, ..., x1,n1,q1 x, ..., x


⊗
... ⊗


2mk+nk−1
X
qk
λqk,k |yk,1,q1, ..., yk,m1,qk⟩

xk,2,qk, ..., xk,n1,qk x, ..., x



= eiαx


2m1+n1−1
X
q1
λq1,1
Y
i
δx,x1,i,q1 |y1,1,q1, ..., y1,m1,q1⟩

⊗
... ⊗


2mk+nk−1
X
qk
λqk,k
Y
i
δx,xk,i,qk |yk,1,q1, ..., yk,m1,qk⟩


= eiαx
 2m1
X
q1
λ ′
q1,1 |y1,1,q1, ..., y1,m1,q1⟩
!
⊗... ⊗
 2mk
X
qk
λ ′
qk,k |yk,1,q1, ..., yk,m1,qk⟩
!
=
2m
X
q=1
λ ′
q |y1,q, ..., ym,q⟩
where the third line is obtained by induction hypothesis, and all λ ′ match
the ones obtained from the rewrite of token states.
This proposition conveys the fact that dropping a single token in state x on wire
ak gives the same semantics as the one obtained from the standard interpretation
on the ZX-diagram, with wire ak connected to the state |x⟩.
Proposition 5.3.21 can be made more general.
However, we ﬁrst need the
following result on ZX-diagrams:
Lemma 5.3.22 (Universality of Connected ZX-Diagrams). Let f : C2n →C2m.
There exists a connected ZX-diagram Df : n →m such that JDfK = f.
Proof. There exist several methods to build a diagram Df such that JDfK = f,
using the universality of quantum circuits together with the map/state duality
[CD11], or using normal forms [JPV19]. The novelty here is that the diagram
should be connected. This problem can be fairly simply dealt with:
Suppose that we have such a Df that has several connected components.
We can turn it into an equivalent diagram that is connected. Let us consider
two disconnected components of Df. Each of these disconnected compo-
nents either has at least one wire, or is one of {
α,
α}. In either case, we
can use the rules of ZX ((Ig) or (H)) to force the existence of a green node. These
green nodes in each of the connected components can be “joined” together
like this:
α
β
...
...
=
α
β
...
...
114

It is hence possible to connect every diﬀerent connected components of a
diagram in a way that preserves the semantics.
Proposition 5.3.23 (Multi-Token Input). Let D be a connected ZX-diagram with
I(D) = [ai]1≤i≤n and O(D) = [bi]1≤i≤m; with n ≥1.
If:
JDK ◦


2n
X
q=1
λq |x1,q, ..., xn,q⟩

=
2m
X
q=1
λ ′
q |y1,q, ..., ym,q⟩
then:
2n
X
q=1
λq
n
Y
i=1
(ai ↓xi,q) ⇝∗
2m
X
q=1
λ ′
q
m
Y
i=1
(bi ↓yi,q)
Proof. Using Lemma 5.3.22, there exists a connected ZX-diagram D′ with I(D′) =
[a′] and such that JD′K |0⟩= P2n
q=1 λq |x1,q, ..., xn,q⟩. Consider now a derivation
from the token state (a′ ↓0) in D ◦D′:
...
...
D
a1
an
b1 bm
D′
a′
(a′ ↓0) ⇝∗P2n
q=1 λq
Qn
i=1(ai ↓xi,q)
and
(a′ ↓0) ⇝∗P2m
q=1 λ ′
q
Qm
i=1(bi ↓yi,q)
The ﬁrst run comes from Proposition 5.3.21 on D′ which is connected. The
second run results from Proposition 5.3.21 on D◦D′ which is also connected.
The proposition also gives us that:
JDK◦


2n
X
q=1
λq |x1,q, ..., xn,q⟩

= JDK◦
q
D′y
◦|0⟩=
q
D ◦D′y
◦|0⟩=
2m
X
q=1
λ ′
q |y1,q, ..., ym,q⟩
Finally, by conﬂuence in D◦D′, we get P2n
q=1 λq
Qn
i=1(ai ↓xi,q) ⇝∗P2m
q=1 λ ′
q
Qm
i=1(bi ↓
yi,q) in D.
Example 5.3.24 (CNOT). In the ZX-Calculus, the CNOT-gate (up to some scalar)
can be constructed as follows:
u
wwwv
a1
a2
b1
e1
e2
e3
e4
b2
}
~ =
1
√
2




1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0




On classical inputs, this gate applies the NOT-gate on the second bit if and only
if the ﬁrst bit is at 1. Therefore, if we apply the state |10⟩to it, we get
1
√
2 |11⟩.
We demonstrate how the token machine can be used to get this result. Follow-
ing Proposition 5.3.23, we start by initializing the Token Machine in the token state
(a1 ↓1)(a2 ↓0), matching the input state |10⟩.
We underline each step that is being rewritten, and take the liberty to some-
times do several rewrites in parallel at the same time.
(a1 ↓1)(a2 ↓0) ⇝d (b1 ↓1)(e1 ↓1)(a2 ↓0) ⇝d (b1 ↓1)(e1 ↓1) 1
√
2

(e3 ↓0) + (e3 ↓1)

115

⇝d
1
√
2(b1 ↓1)(e1 ↓1)

(e2 ↑0)(e4 ↓0) + (e2 ↑1)(e4 ↓1)

⇝d 1
2(b1 ↓1)

(e2 ↓0) −(e2 ↓1)

(e2 ↑0)(e4 ↓0) + (e2 ↑1)(e4 ↓1)

⇝2
c
1
2(b1 ↓1)

(e4 ↓0) +
 (e2 ↓0) −(e2 ↓1)

(e2 ↑1)(e4 ↓1)

⇝2
c
1
2(b1 ↓1)

(e4 ↓0) −(e4 ↓1)

⇝d
1
2
√
2(b1 ↓1)

(b2 ↓0) + (b2 ↓1) −(b2 ↓0) + (b2 ↓1)

=
1
√
2(b1 ↓1)(b2 ↓1)
The ﬁnal token state corresponds to
1
√
2 |11⟩, as described by Proposition 5.3.23.
Notice that during the run, all invariants presented before holds and that due to
conﬂuence we could have rewritten the tokens in any order and still obtain the
same result.
Proposition 5.3.23 is a direct generalization of Proposition 5.3.21. It shows we
can compute the output of a diagram provided a particular input state. We can
also recover the semantics of the whole operator by initializing the starting token
state in a particular conﬁguration.
Theorem 5.3.25 (Arbitrary Wire Initialisation). Let D be a connected ZX-diagram,
with I(D) = [ai]1≤i≤n, O(D) = [bi]1≤i≤m, and e ∈E(D) ̸= ∅such that (e ↓
x)(e ↑x) ⇝∗tx for x ∈{0, 1} with tx terminal (the rewriting terminates by Corol-
lary 5.3.20). Then:
JDK =
2m+n
X
q=1
λq |y1,q . . . ym,q⟩⟨x1,q . . . xn,q| =⇒t0+t1 =
2m+n
X
q=1
λq
Y
i
(bi ↓yi,q)
Y
i
(ai ↑
xi,q)
Proof. First, let us single out e in the diagram D =
D2
e
D1
...
ai
...
bi
...
. We can build a
second diagram by cutting e in half and seeing each piece of wire as an input
and an output:
D2
e0
D1
...
ai
...
bi
...
e1
:=
e0
D′
...
ai
...
bi
e1
. We can easily see that a rewriting of the
token states (e ↓0)(e ↑0) and (e ↓1)(e ↑1) in D corresponds step by step to
a rewriting of the token states (e0 ↓0)(e1 ↑0) and (e0 ↓1)(e1 ↑1) in D′. We
can then focus on D′, whose interpretation is taken to be
q
D′y
=
2m+n+2
X
q=1
λ ′
q
y′
1,q, ..., y′
m+1,q

x′
1,q, ..., x′
n+1,q

116

such that
(id⊗m ⊗⟨0|)◦
q
D′y
◦(id⊗n ⊗|0⟩)+(id⊗m ⊗⟨1|)◦
q
D′y
◦(id⊗n ⊗|1⟩) = JDK
from which we get:
JDK =
2m+n+2
X
q=1
λ ′
qδ0,y′
m+1,qδ0,x′
n+1,q
y′
1,q, ..., y′
m,q

x′
1,q, ..., x′
n,q

+
2m+n+2
X
q=1
λ ′
qδ1,y′
m+1,qδ1,x′
n+1,q
y′
1,q, ..., y′
m,q

x′
1,q, ..., x′
n,q

We now have to consider two cases:
• D′ is still connected: By Proposition 5.3.21, for x ∈{0, 1}:
(e0 ↓x)(e1 ↑x) ⇝∗
2m+n+2
X
q=1
λ ′
qδx,x′
n+1,q
Y
i
(ai ↑x′
i,q)
Y
i
(bi ↓y′
i,q)(e1 ↓y′
m+1,q)(e1 ↑x)
⇝
2m+n+2
X
q=1
λ ′
qδx,y′
m+1,qδx,x′
n+1,q
Y
i
(ai ↑x′
i,q)
Y
i
(bi ↓y′
i,q)
We hence have
(e0 ↓0)(e1 ↑0) ⇝∗t0 =
2m+n+2
X
q=1
λ ′
qδ0,y′
m+1,qδ0,x′
n+1,q
Y
i
(ai ↑x′
i,q)
Y
i
(bi ↓y′
i,q)
(e0 ↓1)(e1 ↑1) ⇝∗t1 =
2m+n+2
X
q=1
λ ′
qδ1,y′
m+1,qδ1,x′
n+1,q
Y
i
(ai ↑x′
i,q)
Y
i
(bi ↓y′
i,q)
so t0 + t1 corresponds to the interpretation of D.
• D′ is now disconnected: Since D was connected, the two connected
components of D were connected through e. Hence, D′ only has two
connected components, one connected to e0 and the other to e1. By
applying Proposition 5.3.21 to both connected components, we get the
desired result.
Example 5.3.26. Consider again the diagram from Example 5.3.24 and initialize
any wire e of the diagram in the state (e ↓0)(e ↑0) + (e ↓1)(e ↑1) and apply
the rewriting as in Theorem 5.3.25 we end up with the ﬁnal token state
1
√
2

(a1 ↑
0)(a2 ↑0)(b1 ↓0)(b2 ↓0)+(a1 ↑0)(a2 ↑1)(b1 ↓0)(b2 ↓1)+(a1 ↑1)(a2 ↑0)(b1 ↓
1)(b2 ↓1) + (a1 ↑1)(a2 ↑1)(b1 ↓1)(b2 ↓0)

which corresponds to the matrix
1
√
2




1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0



, that is the one obtained from the standard interpretation.
117

Notice that while technically there is a collision happening on the initial token
state given by Theorem 5.3.25, we do not apply it, intuitively this is saying that
while the tokens are on the same wire they have already crossed each others, so
they cannot collide.
5.3.4 . Discussions
At this point, it is legitimate to wonder about the beneﬁts of the token machine
over the standard interpretation for computing the semantics of a diagram. Let us
ﬁrst notice that when computing the semantics of a diagram à la Theorem 5.3.25,
we get in the token state one term per non-null entry in the associated matrix (the
one obtained by the standard interpretation).
We can already see that the token-based interpretation can be interesting if
the matrix is sparse, the textbook case being Zn
n whose standard interpretation
requires a 2n × 2n matrix, while the token-based interpretation only requires two
terms (each with 2n tokens).
Secondly, we can notice that we can "mimic" the standard interpretation with
the token machine. Consider a diagram decomposed as a product of slices (ten-
sor product of generators) for the standard interpretation. Then, for the token
machine, without going into technical details, we can follow the strategy that con-
sists in moving tokens through the diagram one slice at a time. This essentially
computes the matrix associated with each slice and its composition.
The point of the token machine, however, is that it is versatile enough to allow
for more original strategies, some of which may have a worst complexity, but also
some of which may have a better one.
5.4 . Extension to Mixed Processes
The token machine presented in Section 5.3 works for so-called pure quantum
processes i.e. with no interaction with the environment.
To demonstrate how
generic our approach is, we show how to adapt it to the natural extension of mixed
processes, presented in Section 2.1.4. In particular, this allows us to represent
quantum measurements.
With respect to what happens to edge labels, notice that every edge in D can
be mapped to 2 edges in CPM(D). We propose that label e induces label e in the
ﬁrst copy, and e in the second, e.g., for the identity diagram:
e0
7−→
e0
e0
5.4.1 . Token Machine for Mixed Processes
We now aim at adapting the token machine to ZX , the formalism for com-
pletely positive maps. To do so we give an additional state to each token to mimic
the evolution of two tokens on CPM(D).
Deﬁnition 5.4.1. Let D be a ZX-diagram. A
-token is a quadruplet (p, d, x, y) ∈
E(D) × {↓, ↑} × {0, 1} × {0, 1}. We denote the set of
-tokens on D by tk (D). A
118

-token-state is then a multivariate polynomial over C, evaluated in tk (D). We
denote the set of
-token-states on D by tkS (D)
In other words, the diﬀerence with the previous machine is that tokens here
have an additional state (e.g. y in (e ↓x, y)). The rewrite rules are given in Table
5.3.
e0
(e0 ↓x, y)(e0 ↑x′, y′) ⇝c δx,x′δy,y′
(Collision)
e0
e1
(eb ↓x, y) ⇝d (e¬b ↑x, y)
(
-diﬀusion)
e0
e1
(eb ↑x, y) ⇝d (e¬b ↓x, y)
(
-diﬀusion)
(ek ↓x, y) ⇝d eiα(x−y) Y
j̸=k
(ej ↑x, y)
Y
j
(e′
j ↓x, y)
(e′
k ↑x, y) ⇝d eiα(x−y) Y
j
(ej ↑x, y)
Y
j̸=k
(e′
j ↓x, y)
(e0 ↓x, y) ⇝d
1
2
X
z,z′∈{0,1}
(−1)xz+yz′(e1 ↓z, z′)
(e1 ↑x, y) ⇝d
1
2
X
z,z′∈{0,1}
(−1)xz+yz′(e0 ↑z, z′)
e0
(e0 ↓x, y) ⇝d δx,y
(Trace-Out)
...
e1 en
e′
1 e′
m
...
α
e0
e1
( -Diﬀusion)
(
...
...
-
Diﬀusion)
Table 5.3: The rewrite rules for ⇝, where δ is the Kronecker delta.
It is possible to link this formalism back to the pure token-states, using the
existing CPM construction for ZX-diagrams.
We extend this map by CPM : tkS (D) →tkS(CPM(D)), deﬁned as:
2m+n
X
q=1
λq
Y
j
(pj, dj, xj,q, yj,q) 7→
2m+n
X
q=1
λq
Y
j
(pj, dj, xj,q)(pj, dj, yj,q)
Since CPM(D) can be seen as two copies of D where
is replaced by
,
each token in D corresponds to two tokens in CPM(D), at the same spot but
in the two copies of D. The two states x and y represent the states of the two
corresponding tokens.
We can then show that this rewriting system is consistent:
Theorem 5.4.2. Let D be a ZX -diagram, and t1, t2 ∈tkS (D). Then whenever
t1 ⇝
t2 we have CPM(t1) ⇝{1,2} CPM(t2).
119

Proof. The proof is done by induction on ⇝:
• Collision: t = (e0 ↓x, y)(e0 ↑x′, y′) ⇝
δx,x′δy,y′.
We get CPM(t) = (e0 ↓x)(e0 ↓y)(e0 ↑x′)(e0 ↑y′) ⇝δx,x′δy,y′
• Cup (Cap being similar): t = (eb ↓x, y) ⇝
(e¬b ↑x, y).
We get CPM(t) = (eb ↓x)(eb ↓y) ⇝(e¬b ↑x)(e¬b ↑y).
• Zn
m(α): t = (ek ↓x, y) ⇝
eiα(x−y) Q
j̸=k(ej ↑x, y) Q
j(e′
j ↓x, y) = t′
then we get
CPM(t) = (ek ↓x)(ek ↓y)
⇝2 eiαx Y
j̸=k
(ej ↑x)
Y
j
(e′
j ↓x)ei(−α)y Y
j̸=k
(ej ↑y)
Y
j
(e′
j ↓y)
= CPM(t′)
• Hadamard: t = (e0 ↓x, y) ⇝
1
2
P
z,z′∈{0,1}(−1)xz+yz′(e1 ↓z, z′) = t′,
then
CPM(t) = (e0 ↓x)(e0 ↓y)
⇝2
d [(−1)x 1
√
2(e1 ↓x) + 1
√
2(e1 ↓¬x)][(−1)y 1
√
2(e1 ↓y) + 1
√
2(e1 ↓¬y)]
= 1
2((−1)x+y(e1 ↓x)(e1 ↓y) + (−1)x(e1 ↓x)(e1 ↓¬y)
+ (−1)y(e1 ↓¬x)(e1 ↓y) + (e1 ↓¬x)(e1 ↓¬y))
= CPM(t′)
• Ground: t = (e0 ↓x, y) ⇝
δx,y then CPM(t) = (e0 ↓x)(e0 ↓y).
Remember than in CPM(D) the Ground is translated as a Cup we get
one diﬀusion and one collision rule: CPM(t) ⇝(e0 ↑x)(e0 ↓y) ⇝
δx,y.
The notions of polarity, well-formedness and cycle-balancedness can be adapted,
and we get strong normalization (Theorem 5.3.16), conﬂuence (Corollary 5.3.19),
and uniqueness of normal forms (Corollary 5.3.20) for well-formed and cycle-
balanced token states.
5.5 . Variations of the Token Machine
5.5.1 . Pulse Rewriting
So far, the Token Machines that we presented required us to specify an initial
state in order to compute the semantics of a ZX-Diagram. It is nonetheless possible
to consider another version of the rewriting rules, called the pulse-semantics in
which a node of the diagram will pulse and emit tokens in all of its input and
120

output edges, directly reﬂecting its matrix semantics as in Theorem 5.3.25. In
order to properly compute the semantics of the whole diagram, each node needs
to pulse exactly once, in any order, and then one can simply apply the collisions. As
each generator is forced to pulse, there is no need for the diagram to be connected.
We may however have fringe cases, with connected components in the diagram that
have no generators that pulse. To remedy this, it suﬃces to recall that the identity
is nothing but
=
, and make this last generator pulse in these cases. Notice
that it is technically important to break the wire’s name e from the left-hand-side
in two e1e2 on each side of the green node, so that the tokens obtained from the
pulse are not removed by a collision. The rules for the Pulse Token Machine are
given in Table 5.4.
e0
(e0 ↓x)(e0 ↑x′) ⇝c δx,x′
(Collision)
e0
e1 ⇝p (e0 ↑0)(e1 ↑0) + (e0 ↑1)(e1 ↑1)
(
-diﬀusion)
e0
e1 ⇝p (e0 ↓0)(e1 ↓0) + (e0 ↓1)(e1 ↓1)
(
-diﬀusion)
...
e1 en
e′
1 e′
m
...
α ⇝p
Y
j
(ej ↑0)
Y
j
(e′
j ↓0) + eiα Y
j
(ej ↑1)
Y
j
(e′
j ↓1)
(x −y)
e0
e1
⇝p (e0 ↑0) 1
√
2((e1 ↓0) + (e1 ↓1))
+ (e0 ↑1) 1
√
2((e1 ↓0) −(e1 ↓1))
( -Diﬀusion)
(
...
...
-
Diﬀusion)
Table 5.4: The Pulse Rewriting System
The pulse token machine is pretty straightforward: following the idea from
Theorem 5.3.25 one can notice that the token state obtained by the pulse of a
generator is exactly the same as the standard interpretation of said generator.
The main results are obtained trivially: the pulse rewriting strategy enjoys
termination (as each generator pulses only once), conﬂuence (as pulse and collisions
do not interact), that we reach the expected normal form with no token inside the
diagram and only one token going up (resp. down) on any input (resp. output)
wire (after each pulse each input (resp. output) wire has exactly one token going
up (resp. down), for the internal ones, the collisions will necessarily happen).
It is actually even possible to recover the previous token machine through the
pulse one by pulsing a generator while having a token going through it and applying
the collisions, as exemplify in the following example:
121

Example 5.5.1. Considering
...
e1 en
e′
1 e′
m
...
α with the token state (ek ↓0) for k ∈{1, . . . , n},
after pulsing we get:
(ek ↓0)(
Y
j
(ej ↑0)
Y
j
(e′
j ↓0)+eiα Y
j
(ej ↑1)
Y
j
(ej ↓1)) ⇝c
Y
j̸=k
(ej ↑0)
Y
j
(e′
j ↓0)
which is equal to applying the diﬀusion rule of the initial token machine.
While not necessarily very useful for the case of the ZX-Calculus, the Pulse
Rewriting Strategy can allow us to more easily deﬁne both an asynchronous token
machine and a denotational semantics, as was originally done in the work presented
in Chapter 6.
5.5.2 . Synchronicity
Our token machine can be made synchronous: all tokens in a token state then
move at once. This implies adapting the rules to take into account all incoming
tokens for each generator. For instance, in the
h ...
... -Diﬀusion
i
-rule the product
Q
i(ei ↓xi) rewrites into δx1,...,xneiαx1 Q
i(e′
i ↓xi). This notion of synchronicity
is to be contrasted with [Dal17] where tokens have to wait for all other incoming
tokens to reach the gate before going through it.
5.6 . Sum Over Path Semantics
A serious drawback of the previous token machines is that the token state grows
exponentially quickly in the number of nodes in the diagram. A more compact
representation (linear in the size of the diagram as we will see in Prop. 5.6.7) can
be obtained by adapting the concept of sums-over-paths (SOP) [Amy18] to our
machine. This can be obtained naturally, as strong links between ZX-Calculus and
SOP morphisms were already shown to exist [LWK20; Vil20]. Intuitively, SOP
will allow us to manipulate token states in a symbolic way, where for instance
(e ↓0) + (e ↓1) will be represented by (e ↓y). While the development of the
sums-over-paths token machine was mostly done by Renaud Vilmart, we put it here
for comprehensiveness, with added proof of Theorem 5.4.2 and Theorem 5.6.9 that
were missing in the original paper.
5.6.1 . SOP Token Machine for Pure Operators
Deﬁnition 5.6.1. Let D be a ZX-diagram. A SOP-token is a triplet (p, d, B) be-
longing to E(D) × {↓, ↑} × F2[⃗y] where ⃗y := (yi)0≤i<n are n variables from a set
of variables V; and where F2 := Z/2Z. We denote the set of SOP-tokens on D
with variables ⃗y by tkSOP(D)[⃗y]. A SOP-token-state is a quadruplet:
(s, ⃗y, P, {tj}0≤j<p) ∈R × Vn × R[⃗y]/(1, {y2
i −yi}0≤i<n) × tkSOP(D)[⃗y]p
122

where R[⃗y]/(1, {y2
i −yi}0≤i<n) is the set of real-valued multivariate polynomials
(whose variables are ⃗y), modulo 1 and modulo (y2
i −yi) for all variables yi.
For any valuation of ⃗y, 2πP(⃗y) represents an angle, hence P is taken modulo
1. Since each yi is a boolean variable, we can consider y2
i −yi = 0. To better
reﬂect what this quadruplet represents, we usually write it as:
s
X
⃗y
e2iπP(⃗y)(p0, d0, B0(⃗y))...(pm−1, dm−1, Bm−1(⃗y))
We denote the set of SOP-token-states on D by tkSSOP(D).
Example 5.6.2. Let D =
e0
e1
, then
1
√
2
P
y0,y1
e2iπ y0y1
2 (e0 ↑y0)(e1 ↓y1) ∈tkSSOP(D).
We can link this formalism back to the previous one, by deﬁning a map that as-
sociates any SOP-token-state to a “usual” token-state. This map simply evaluates
the term by having all its variables span {0, 1}:
Deﬁnition 5.6.3. We deﬁne [.]tk : tkSSOP(D) →tkS(D) by:

s
X
⃗y
e2iπP(⃗y) Y
j
(pj, dj, Bj(⃗y))


tk
:= s
X
⃗y∈{0,1}n
e2iπP(⃗y) Y
j
(pj, dj, Bj(⃗y))
Notice that therefore, the gain in size of the token state will be purely on a
representation point of view. Computing the actual matrices encoded by the token
state will still require an exponential growth in the number of tokens.
Example 5.6.4.
"
1
√
2
X
y0,y1
e2iπ y0y1
2 (e0 ↑y0)(e1 ↓y1)
#tk
=
1
√
2

(e0 ↑0)(e1 ↓0) + (e0 ↑1)(e1 ↓0)
+(e0 ↑0)(e1 ↓1) −(e0 ↑1)(e1 ↓1)

Once again, the rule for a red node can be obtained from the previous rewrite
rules and the ZX-rule:
α
...
:=
α
...
...
...
. For reference, see Table 5.6.
We give the adapted set of rewrite rules for our SOP-token-machine in Ta-
ble 5.5. In the rewrite rules of our token machine, we have to map elements of
F2[⃗y] to elements of R[⃗y]/(1, {y2
i −yi}) for the Boolean polynomials to be sent to
the phase polynomial. The map c
(.) : F2[⃗y] →R[⃗y]/1(1, {y2
i −yi}) that does this
is deﬁned as:
\
B ⊕B′ = bB + c
B′ −2 d
BB′
d
BB′ = bBc
B′
byi = yi
b0 = 0
b1 = 1
The provided rewrite rules do not give the full picture, for simplicity.
If a
rule gives (e, d, b) ⇝sop s′ P
⃗y′ e2iπP ′ Q
j(e′
j, d′
j, b′
j), we have to apply it to a full
SOP-token-state as follows:
s
X
⃗y
e2iπP (e, d, b)
Y
j
(ej, dj, bj) ⇝ss′ X
⃗y,⃗y′
e2iπ(P+P ′) Y
j
(e′
j, d′
j, b′
j)
Y
j
(ej, dj, bj).
123

e0
(e0 ↓B)(e0 ↑B′) ⇝c
1
2
X
z
e2iπ z
2 ( \
B⊕B′)
(Collision)
e0
e1
(eb ↓B) ⇝d (e¬b ↑B)
(
-diﬀusion)
e0
e1
(eb ↑B) ⇝d (e¬b ↓B)
(
-diﬀusion)
(ek ↓B) ⇝d e2iπ( α
2π b
B) Y
j̸=k
(ej ↑B)
Y
j
(e′
j ↓B)
(e′
k ↑B) ⇝d e2iπ( α
2π b
B) Y
j
(ej ↑B)
Y
j̸=k
(e′
j ↓B)
(e0 ↓B) ⇝d
1
√
2
X
z
e2iπ( z
2 b
B)(e1 ↓z)
(e1 ↑B) ⇝d
1
√
2
X
z
e2iπ( z
2 b
B)(e0 ↑z)
...
e1 en
e′
1 e′
m
...
α
e0
e1
( -Diﬀusion)
(
...
...
-
Diﬀusion)
Table 5.5: Rewrite rules for ⇝sop.
α
a
(a ↓x) →1
√
2
X
y
e2iπ( xy
2 + α
2π y)
α
a
b
c
(a ↓x) →
1
2
√
2
X
yi
e2iπ( xy1
2 + α
2π y1+ y1y2
2
+ y1y3
2
)(b ↓y2)(c ↓y3)
Table 5.6: Samples of asynchronous token-state evolution for red spi-
ders with SOP Token Machine
Just as before, the rewrite system is deﬁned by ﬁrst applying a diﬀusion rule then
all possible collision rules.
This set of rules mimics the previous one for SOP-token-states, except that
it “synchronizes” rewrites on all the terms at once (but not on all tokens).
Example 5.6.5. Let us compare the behaviour of the token machine from Sec-
tion 5.3 to the SOP machine. We send a sum of tokens in states 0 and 1 down the
wire a in the diagram
a
b
c . In the former machine, this leads to
(a ↓0) + (a ↓1) ⇝(b ↓0)(c ↓0) + (a ↓1) ⇝(b ↓0)(c ↓0) + (b ↓1)(c ↓1)
while in the latter:
X
y
(a ↓y) ⇝sop
X
y
(b ↓y)(c ↓y).
In both cases the result is the same when interpreted as usual token states. We
124

notice that the ⇝sop token machine only takes one step compared to the standard
one, which leads to the following proposition:
Proposition 5.6.6. For any D ∈ZX and s, s′ ∈tkSSOP(D), whenever s ⇝sop
s′ we have [s]tk ⇝∗[s′]tk.
Proof. By induction on ⇝sop:
• Collision: s = (e0 ↓B)(e0 ↑B′) ⇝sop 1
2
P
t e2iπ t
2 ( \
B⊕B′) = s′
We get
[s]tk =
X
−
→
y ∈{0,1}n,−
→
z ∈{0,1}m
(e0 ↓B(−→y ))(e0 ↑B′(−→z ))
⇝
X
−
→
y ∈{0,1}n,−
→
z ∈{0,1}m
δB(−
→
y ),B′(−
→
z )
and
[s′]tk = 1
2
X
−
→
y ∈{0,1}n,−
→z ∈{0,1}m
X
t
e2iπ t
2 (
\
B(−
→
y )⊕B′(−
→
z ))
= 1
2
X
−
→
y ∈{0,1}n,−
→z ∈{0,1}m,t∈{0,1}
δB(−
→
y ),B′(−
→z )
= 1
2
X
−
→
y ∈{0,1}n,−
→z ∈{0,1}m
δB(−
→
y ),B′(−
→z )
X
t∈{0,1}
1
=
X
−
→
y ∈{0,1}n,−
→
z ∈{0,1}m
δB(−
→
y ),B′(−
→
z )
• Cup (Cap being similar): s = (eb ↓B) ⇝sop (e¬b ↑B) = s′ then we get
[s]tk = P
−
→
y ∈{0,1}n(eb ↓B(−→y )) ⇝P
−
→
y ∈{0,1}n(e¬b ↑B(−→y )) = [s′]tk
• Zn
m(α) : s = (ek ↓B) ⇝sop e2iπ( α
2π b
B) Q
j̸=k(ej ↑B) Q
j(e′
j ↓B) = s′
We get
[s]tk =
X
−
→
y ∈{0,1}n
(ek ↓B(−→y ))
⇝
X
−
→
y ∈{0,1}n
eiαB(−
→
y ) Y
j̸=k
(ej ↑B(−→y ))
Y
j
(e′
j ↓B(−→y ))
=
X
−
→
y ∈{0,1}n
e2iπ( α
2π \
B(−
→
y )) Y
j̸=k
(ej ↑B(−→y ))
Y
j
(e′
j ↓B(−→y )) = [s′]tk
125

• Hadamard: s = (e0 ↓B) ⇝sop
1
√
2
P
z e2iπ( z
2 b
B)(e1 ↓z) = s′
We have
[s]tk =
X
−
→
y ∈{0,1}n
(e0 ↓B(−→y ))
⇝
1
√
2
X
−
→
y ∈{0,1}n
(−1)B(−
→
y )(e1 ↓B(−→y )) + (e1 ↓¬B(−→y ))
Take [s′]tk and sum z over {B(−→y ), ¬B(−→y )} and remember that B2 = B.
then:
1
√
2
X
−
→
y ∈{0,1}n
e2iπ(
\
B(−
→
y )
2
\
B(−
→
y ))(e1 ↓B(−→y )) + e2iπ( ¬ \
B(−
→
y )
2
\
B(−
→
y ))(e1 ↓¬B(−→y ))
=
1
√
2
X
−
→
y ∈{0,1}n
e2iπ(
\
B(−
→
y )
2
)(e1 ↓B(−→y )) + (e1 ↓¬B(−→y ))
=
1
√
2
X
−
→
y ∈{0,1}n
(−1)
\
B(−
→
y )(e1 ↓B(−→y )) + (e1 ↓¬B(−→y )) = [s′]tk
We can show a result on the growth size of the token-state as it rewrites, which
was the motivation for the use of this formalism.
Proposition 5.6.7. Let D ∈ZX and s, s′ ∈tkSSOP(D) such that all Boolean
polynomials Bj in s are reduced to a single term of degree ≤1, and such that
s ⇝sop s′. Then, the size of s′ is bounded by: S(s′) ≤S(s) + ∆(D) where S
denotes the cumulative number of terms in the phase polynomial and the number
of tokens in the token-state, and where ∆(D) represents the maximum arity of
generators in D.
Proof. Let D ∈ZX and s ∈tkSSOP(D) such that its Bj ∈{0, 1, y}y∈V for
all j. Note that, at worse, all collisions do not change the size of the term (at
best reduce the size). Indeed, we turn two tokens into at most two terms in the
phase polynomial, since z
2(
\
Bj1 ⊕Bj2) = z
2(Bj1+Bj2−2Bj1Bj2) = z
2(Bj1+Bj2)
because we work modulo 1 in the phase polynomial.
Hence, since a rewrite step consists in a diﬀusion step followed by some
collision rule, showing the result only for diﬀusions is enough.
• Diﬀusions through Cups and Caps do not change the size.
• A diﬀusion through H adds a single term in the phase polynomial. How-
ever, since H is in the diagram, ∆(D) ≥2, so the proposition holds.
• A diﬀusion through a Green-spider with arity δ adds δ −2 tokens, and
a single term in the phase polynomial. However, δ ≤∆(D).
126

The requirement on Boolean polynomials may seem overly restrictive. However,
it is invariant under rewriting: starting with a token-state in this form ensures
polynomial growth.
Polarity can be deﬁned in this setting (and is even more natural, as we do not
need to consider each term individually) providing the notions of well-formedness
and cycle-balancedness. The main results from Section 5.3 are valid in this set-
ting. We recover strong normalization for well-formed, cycle-balanced token-states
(Theorem 5.3.16), Local Conﬂuence (Proposition 5.3.18) and their corollaries, such
as uniqueness of normal forms (Corollary 5.3.20).
Non-empty terminal token states can also be interpreted as SOP-morphisms.
Suppose an SOP-token state
S = s
X
⃗y′
e2iπP Y
i
(bi ↓Bi(⃗y′))
Y
i
(ai ↑Ai(⃗y′))
on a diagram D with I(D) = [ai]1≤i≤n and O(D) = [bi]1≤i≤m.
Then [S]SOP := s P
⃗y e2iπP(⃗y) |B0(⃗y), ...⟩⟨A0(⃗y), ...| is the SOP morphism as-
sociated to S. We have the following commutative diagram:
tkSSOP ↓
tkS ↓
SOP
Qubit
[.]tk
[.]SOP
J.K
J.K
Where tkSSOP ↓(resp.
tkS ↓) is the set of non-empty well-formed terminal
SOP-token states (resp. token states), and tkS ↓
J.K
→Qubit is the interpretation
obtained from Theorem 5.3.25.
5.6.2 . SOP Token Machine for Mixed Processes
We now aim at adapting the SOP token machine to ZX , the formalism for
completely positive maps.
Deﬁnition 5.6.8. Let D be a ZX-diagram. A SOP -token is a quadruplet (p, d, B, B′) ∈
E(D) × {↓, ↑} × F2[⃗y] × F2[⃗y] where ⃗y := (yi)0≤i<n are variables from a set
of variables V. We denote the set of SOP -tokens on D with variables ⃗y by
tkSOP(D)[⃗y]. Similar to what was done in Deﬁnition 5.6.1, a SOP -token-state
is a quadruplet
(s, ⃗y, P, {tj}0≤j<p) ∈R × Vn × R[⃗y]/(1, {y2
i −yi}0≤i<n) × tkSOP(D)[⃗y]p
To better reﬂect what this quadruplet represents, we usually write it as:
s
X
⃗y
e2iπP(⃗y)(p0, d0, B0(⃗y), B′
0(⃗y))...(pm−1, dm−1, Bm−1(⃗y), B′
m−1(⃗y))
We denote the set of SOP -token-states on D by tkSSOP(D).
In other words, the diﬀerence with the previous machine is that tokens here
have an additional Boolean function (e.g. y in (a ↓x, y)). The rewrite rules can
be found in Table 5.7.
127

e0
(e0 ↓B0, B1)
×
(e0 ↑B′
0, B′
1) ⇝c
1
4
X
z0,z1
e2iπ( z0
2
\
B0⊕B1+ z1
2
\
B′
0⊕B′
1)
(Collision)
e0
e1
(eb ↓B, B′) ⇝d (e¬b ↑B, B′)
(
-diﬀusion)
e0
e1
(eb ↑B, B′) ⇝d (e¬b ↓B, B′)
(
-diﬀusion)
(ek ↓B0, B1) ⇝d e2iπ α
2π (c
B0−c
B1)
Q
j̸=k(ej ↑B0, B1)
×Q
j(e′
j ↓B0, B1)
(e′
k ↑B0, B1) ⇝d e2iπ α
2π (c
B0−c
B1)
Q
j(ej ↑B0, B1)
×Q
j̸=k(e′
j ↓B0, B1)
(e0 ↓B, B′) ⇝d
1
2
X
z,z′
e2iπ( z
2 b
B+ z′
2 c
B′)(e1 ↓z, z′)
(e1 ↑B, B′) ⇝d
1
2
X
z,z′
e2iπ( z
2 b
B+ z′
2 c
B′)(e0 ↑z, z′)
e0
(e0 ↓B, B′) ⇝d
1
2
X
z
e2iπ z
2 ( \
B⊕B′)
(Trace-Out)
...
e1 en
e′
1 e′
m
...
α
e0
e1
( -Diﬀusion)
(
...
...
-
Diﬀusion)
Table 5.7: The rewrite rules for ⇝.
It is possible to link this formalism back to the mixed processes-free SOP-
token-states, using the existing CPM construction for ZX-diagrams. We extend
this map by CPM : tkSSOP(D) →tkSSOP(CPM(D)), deﬁned as:
s
X
⃗y
e2iπP(⃗y) Y
j
(pj, dj, Bj(⃗y), B′
j(⃗y)) 7→s
X
⃗y
e2iπP(⃗y) Y
j
(pj, dj, Bj(⃗y))(pj, dj, B′
j(⃗y)).
As described in Section 2.1, CPM(D) can be seen as two copies of D where
is replaced by a cup between the two copies. Each token in D corresponds to two
tokens in CPM(D), at the same spot but in the two copies of D. The two Boolean
polynomials B and B′ represent the Boolean polynomials of the two corresponding
tokens.
We can then show that this rewriting system is consistent:
Theorem 5.6.9. Let D be a ZX -diagram, and t1, t2 ∈tkSSOP(D). Then when-
ever t1 ⇝
t2 we have CPM(t1) ⇝{1,2}
sop CPM(t2).
Proof. Similar to proof of Theorem 5.4.2, done by induction on ⇝
• Collision: t1 = (e0 ↓B0, B1)(e0 ↑B′
0, B′
1) ⇝
1
4
P
z0,z1 e2iπ( z0
2
\
B0⊕B1+ z1
2
\
B′
0⊕B′
1) =
t2
128

Then CPM(t1) = (e0 ↓B0)(e0 ↑B′
0)(e0 ↓B1)(e0 ↑B′
1)
⇝2
sop (1
2
P
z0 e2iπ z0
2 ( \
B0⊕B′
0))(1
2
P
z1 e2iπ z1
2 ( \
B1⊕B′
1)) = CPM(t2)
• Cup (Cap being similar): t1 = (eb ↓B, B′) ⇝
(e¬b ↑B, B′) = t2 with
CPM(t1) = (e0 ↓B)(e0 ↓B′) ⇝2 = (e¬b ↑B)(e¬b ↑B′) CPM(t2)
• Zn
m(α): t1 = (ek ↓B0, B1) ⇝
e2iπ α
2π (c
B0−c
B1) Q
j̸=k(ej ↑B0, B1) Q
j(e′
j ↓
B0, B1) = t2
We get CPM(t1) = (ek ↓B0)(ek ↓B1)
⇝2
sop (e2iπ( α
2π c
B0) Q
j̸=k(ej ↑B0) Q
j(e′
j ↓B0))(e2iπ( −α
2π c
B1) Q
j̸=k(ej ↑B0) Q
j(e′
j ↓
B1))
= (e2iπ( α
2π c
B0)×e2iπ( −α
2π c
B1))(Q
j̸=k(ej ↑B0) Q
j(e′
j ↓B0) Q
j̸=k(ej ↑B0) Q
j(e′
j ↓
B1)) = CPM(t2)
• Hadamard: t1 = (e0 ↓B, B′) ⇝
1
2
P
z,z′ e2iπ( z
2 b
B+ z′
2 c
B′)(e1 ↓z, z′) = t2
with CPM(t1) = (e0 ↓B)(e0 ↓B′)
⇝2
sop ( 1
√
2
P
z e2iπ( z
2 b
B)(e1 ↓z))( 1
√
2
P
z′ e2iπ( z′
2 c
B′)(e1 ↓z′)) = CPM(t2)
• Ground: t1 = (e0 ↓B, B′) ⇝
1
2
P
z e2iπ z
2 ( \
B⊕B′) = t2
Do not forget that the ground is translated as a cup, so we end up with
two wires e0 and e0 on both side of the cup, so:
CPM(t1) = (e0 ↓B)(e0 ↓B′) then we just need to pass one of the two
token on the other side of the cup and apply a collision:
CPM(t1) ⇝sop (e0 ↑B)(e0 ↓B′) ⇝sop 1
2
P
z e2iπ z
2 ( \
B⊕B′) = CPM(t2).
In fact, the ⇝
rewriting rule will only be simulated by 2 rewriting rules
(⇝sop), except in the case of the Trace-out where (⇝sop) only needs to apply
one rule.
Again, the notions of polarity, well-formedness and cycle-balancedness can be
adapted, and again, we get strong (Theorem 5.3.16), conﬂuence (Corollary 5.3.19),
and uniqueness of normal forms (Corollary 5.3.20) for well-formed and cycle-
balanced token states.
5.7 . Conclusion and Future Work
In this chapter presented a novel particle-style semantics for ZX-Calculus.
Based on a token-machine automaton, it emphasizes the asynchronicity and non-
orientation of the computational content of a ZX-diagram. Compared to existing
token-based semantics of quantum computation such as [Dal17], our proposal fur-
thermore supports decentralized tokens where the position of a token can be in
superposition.
129

The Token Machine has been adapted to the case of mixed-processes, and we
give another version using the sum-over-paths semantics which allows the number
of tokens to stays bounded in size.
As quantum circuits can be mapped to ZX-diagrams, our token machines in-
duce a notion of asynchronicity for quantum circuits.
This contrasts with the
notion of token machine deﬁned in [Dal17] where some form of synchronicity is
enforced: in their works each token represents a qubit and therefore multiple to-
kens need to be synchronized in order to properly handle entanglement, while our
tokens do not represent qubits, but they collect the operation that will be applied
to the input qubits.
Our token machine gives us a new way to look at how a ZX-diagram computes
with a more local, operational approach, and in fact could be applied to any tensor
network.
This work is a ﬁrst step towards adding more expressive logical and computa-
tional constructions in the ZX-Calculus to get closer to the theory of proof nets,
such as considering biproducts (that we deﬁne in the next chapter) or even recur-
sion (that we leave as future works.)
130

6 - Many Worlds Calculus
Abstract
In this chapter, we explore the interaction between two monoidal struc-
tures: a multiplicative one, for the encoding of pairing, and an additive
one, for the encoding of choice. We propose a PROP to model com-
putation in this framework, where the choice is parametrized by an
algebraic side eﬀect: the model can support regular tests, probabilis-
tic and non-deterministic branching, as well as quantum branching,
i.e., superposition.
The graphical language comes equipped with a (i) a token-based se-
mantics (ii) a worlds labelling system, giving us information and how
and where some equation can take place and (iii) an equational the-
ory based on the worlds labelling.
We also show how a quantum
version of the language of isos from Chapter 4 can be encoded into
the Many-Worlds.
References: Results of this chapter is a draft under submission, in
collaboration with Marc de Visme, Benoît Valiron and Renaud Vilmart.
6.1 . Introduction
Two Canonical Monoidal Structures. The basic execution ﬂow of a computa-
tion is arguably based on three notions: sequences, tuples and branches. Sequences
form the building block of compositionality, tuples are what makes it possible to
consider multiple pieces of information together, while branches allow the behaviour
to change depending on the inputs or on the state of the system. In a graphical
language, described by a: PROP (C, ⊤, ⊠), the monoidal structure formalizes how
the bunching of wires behaves. This monoidal structure is very versatile. On one
hand, it can be considered in a multiplicative way, with A ⊠B seen as the pairing
of an element of type A and an element of type B. This approach is one followed
in the design of MLL proof-nets for instance [Dal17], or in the ZX-Calculus. On the
other hand, one can consider the monoidal structure in an additive way, with ⊠for
instance being a co- or a bi-product. Standard examples are the category FinRel
of ﬁnite sets and relations, forming an additive PROP with ⊠being the disjoint
union, or the category of ﬁnite dimensional vector spaces (or semi-modules) and
linear maps, with ⊠being the cartesian product. Graphical languages based on
linear optics such as the PBS-Calculus [CP20] make uses of an additive structure.
From a computational perspective, an additive monoidal structure can be regarded
as the possibility to choose a computational path upon the state of the input.
Depending on the underlying system, this choice can be regarded as deterministic
131

f
g
A ⊕B
A′ ⊕B′
A
B
B′
A′
⊕
⊕
(a) Split over coproduct
(A ⊗B) ⊕C
(A′ ⊗B′) ⊕C′
C
C′
⊕
⊕
f
g
h
A
B
A′
B′
⊗
⊗
(b) Splits over coproduct and ten-
sor
Figure 6.1: Examples of branchings
(if based on Set), non-deterministic (if based on Rel), probabilistic (if based on a
suitable semi-module), etc.
To be able to handle both pairing and branching in a PROP, we cannot uniquely
identify ⊠as being multiplicative and additive. We instead need to extend the
PROP with two additional monoidal structures, one for pairing (⊗) and one for
branching (⊕).
In this chapter, we focus on a framework where these two monoidal structures
are available. Graphical languages for such a setting usually rely on a notion of
sheet, or worlds, to handle general branching [Dun09; Mel14]. Figure 6.1a shows for
instance how to represent the construction of the morphism f⊕g : A⊕A′ →B⊕B′
out of f : A →A′ and g : B →B′. The symbol “⊕” stands for the “split”
of worlds. Such a graphical language therefore comes with two distinct “splits”:
one for the monoidal structure —leaving inside one speciﬁc world—, and one
for the coproduct —splitting worlds—.
They can be intertwined, as shown in
Figure 6.1b. Another approach followed by [CDH20] externalizes the two products
(tensor product and coproduct) into the structure of the diagrams themselves, at
the price of a less intuitive tensor product and a form of synchronization constraint.
However, in the state of the art this “splitting-world” understanding has only
been carried for deterministic or probabilistic branching [Dal17; Dun09; Sta15].
These existing approaches do not support more exotic branching, such as quantum
superposition.
Limitation of Current Approaches and Objective of the Chapter. Although
there is a ﬁner and ﬁner understanding of superposition of causal orders in the
literature, none of the existing PROPs can support both the quantum switch on
complex data built from tensors and coproducts. We claim in this chapter that
the same intuition underlying probabilistic branching can be followed for quantum
(and more general) branching. In the conventional case, 1⊕1 is a regular boolean:
either “left” (standing e.g., for True) or “right” (standing e.g., for False). In quan-
tum computation, the sum-type 1 ⊕1 can however be understood as a sum of
vector spaces, giving an alternative interpretation to 1 ⊕1: it can be regarded
as the type of a quantum bit, superposition of True and False. One should note
that this appealing standpoint should be taken cautiously: (Pure) quantum infor-
132

A
A
⊕
⊕
V
U
V
1 ⊕1
1 ⊕1
c
c
A
A
A
1
1
U
⊗
⊗
A ⊗(1 ⊕1)
A ⊗(1 ⊕1)
Figure 6.2: Quantum Switch with Worlds
mation imposes strong constraints on the structure of the data in superposition:
orthogonality and unit-norm must be preserved [AG05; SVV18].
The Quantum Switch can then be naturally understood in this framework.
Consider for instance Figure 6.2, read from left to right: as input, a pair of an
element of type A and a quantum bit. Based on the value of the qubit (True or
False), the wire A goes in the upper or the lower sheet, and is fed with U then V
or V then U. Then everything is merged back together.
Organization of the chapter
In this chapter, we introduce a new graph-
ical language for quantum computation, based on compact category with biproduct
[HV19]. This language allows us to express any process with both pairing and a gen-
eral notion of algebraic branching, encompassing deterministic, non-deterministic,
probabilistic and quantum branching. In Section 6.3 we develop ﬁrst a token-based
semantics as in Chapter 5. The development of the token machine is split in two.
First, in Section 6.3.2 we introduce a pulse token machine, following the intuition
from Section 5.5.1.
We show that the token machine is terminating and con-
ﬂuent. Then in Section 6.3.3 we develop the asynchronous token machine as a
special case of the pulse one. We show how to simulate a run of the pulse token
machine with the asynchronous one and show conﬂuence and termination. From
there, we develop the worlds labelling system in Section 6.4 and the equational
theory in Section 6.6. We show as an example how to encode some basic quantum
primitives into the language and then show how to encode the Quantum Switch
into. We then compare our language with other known graphical languages in Sec-
tion 6.8. We ﬁnish this chapter showing how a quantum variant of the language
from Chapter 4 can be interpreted as diagrams of the language.
6.2 . The Many-Worlds Calculus
Our calculus is parametrized by a commutative semiring (R, +, 0, ×, 1). It can
be instantiated by the complex numbers (C, +, 0, ×, 1) to represent pure quantum
computations, the non-negative real numbers (R≥0, +, 0, ×, 1) for probabilistic
computations, or the booleans ({⊥, ⊤}, ∨, ⊥, ∧, ⊤) for non-deterministic compu-
tations.
6.2.1 . A First Graphical Language
133

The generators of our language are described in Figure 6.3 and are respectively
the Identity, the Swap, the Cup, the Cap, the Plus, the Tensor, the Unit, the n-ary
Contraction for n ≥0, and the Scalar for s ranging over the commutative semiring
R. Mirrored versions of those generators are deﬁned as syntactic sugar through the
cup and cap, as shown for the mirrored Plus on the right-hand-side of Figure 6.3.
Diagrams are read top-to-bottom: the top-most wires are the input wires, and the
bottom-most wires are the output wires. Each wire carry a type, deﬁned by the
syntax A, B ::= 1 | A ⊕B | A ⊗B. As such, the Unit starts a wire of type 1, the
Plus combines two wires of type A and B into a wire of type A ⊕B, and similarly
the Tensor combines two wires of type A and B into a wire of type A ⊗B, as in
linear logic proof nets.
Diagrams are obtained from generators by composing them in parallel (written
□), or sequentially (written ◦). Sequential composition requires the type (and
number) of wires to match. The notation for □is not common for the parallel
composition, this is because wires that are graphically in parallel are not necessarily
“in tensor with one another". In fact, A □B can be understood semantically as
“either A ⊗B or A ⊕B".
D2 ◦D1 :=
...
...
...
D2
D1
D1 □D2 :=
...
...
D1
...
...
D2
Intuitively, the Many-Worlds calculus can be seen as a ﬂattened version of sheet
or tape-diagrams, for instance, the diagram from Figure 6.1a will be represented
in the Many-Worlds Calculus as:
f
g
A⊕B
A
B
A′
B′
A′⊕B′
While the diagram from Figure 6.1b would be similar, but with a tensor on the
left branch of the ⊕, splitting and merging the left wire. The plus and tensors
will be treated diﬀerently: intuitively, if a data enters a plus node, it will either
go on the left, or on the right, disabling the other branch, while for a tensor, the
data will simply split itself into the two diﬀerent branches, without disabling any.
If we replace the second plus by a tensor, we would obtain a diagram that is not
representable using sheet-diagrams but that we can write in our language. But
this diagram wouldn’t make much sense as a plus is supposed to split data that
cannot communicate while the tensor is a collection of data. In order to handle
theses cases, we will introduce a worlds labelling system in Section 6.4. We write
f :   □n(Ai) →  □m(Bj) for a diagram f with n input wires of type A1, . . . , An and
m output wires of type B1, . . . , Bm.
Remark 6.2.1. Instead of having the Cup and the Cap as generators and deﬁning
the mirrored version of each generator through them, one could proceed the other
134

A
A
A
B
B
A
A
A
A
A
A
B
A⊕B
A
B
A⊗B
1
n
· · ·
c
A
A
A
s
A
A
A
B
A⊕B
:=
A
A⊕B
B
Figure 6.3: Generators of our First Graphical Language (n ≥0, s ∈R)
way around by deﬁning the Cap as follows, and the Cup in a mirrored way:
:=
A
B
A⊗B
:=
A
B
A⊕B
1
:=
1
1
The Many-Worlds Calculus was developed as an extension of the works from Chap-
ter 5 in which a biproduct was added to the ZX-Calculus. While it wasn’t clear
how to give a denotational semantics to such a language, the development of the
token machine gave us a more intuitive look at how the generators of the languages
should behave, and from there the denotational semantics and then the worlds la-
belling was developed. However, in this thesis we only focus on the token machine,
equational theory and encoding of the programming language, as the categorical
deﬁnitions and denotational semantics along with its property was developed by
Marc de Visme and Renaud Vilmart.
6.3 . The Token Machine
The Token machine follows the principles of Chapter 5:
• We give a distinct name to each of its wires. By convention, we will use the
names e0, e1, e2, . . . for naming the wires of the diagram. We also consider
the two sides of a cup or cap as diﬀerent wires.
• The notion of path, cycle, length of a path are the same.
In Section 6.3.1 we introduce the base formalism of tokens and token states.
In Section 6.3.2 we present a ﬁrst token machine, based on a pulse rewriting
strategy, akin to the one discussed in Section 5.5.1. In Section 6.3.3 we introduce
the asynchronous token machine as a special case of the pulse one: a rewriting
of the asynchronous machine will correspond to a pulse of a generator where a
token is entering from one of the input or output wire. Finally, in Section 6.3.4
we discuss how some “incorrect” diagrams are interpreted by the token machine in
order to motivate the worlds system introduced in Section 6.4.
6.3.1 . Tokens and Token States
Each type can be assigned a set of basis elements, by analogy with vector
spaces and with the set of closed values of type A from the language of Chapter 4.
Deﬁnition 6.3.1 (Basis Element). Basis elements are deﬁned inductively as fol-
lows:
135

• ⋆is the unique basis element of type 1
For tA and tB basis elements of types A and B respectively:
• ⟨tA, tB⟩is a basis element of type A ⊗B
• injl tA and injr tB are basis elements of A ⊕B
We can deﬁne an inner product of basis elements t and t′ of a type A simply
by

t t′
=
(
1
if t = t′
0
if t ̸= t′
By analogy with vector spaces, the set of basis elements (or the set of closed value
from Chapter 4) of a type A is an orthonormal basis.
In addition to its basis elements, each type is provided with a speciﬁc element
called annihilator, denoted by •. Intuitively, these states are here to indicate parts
of the diagram that are being ignored during evaluation, e.g., one of the two if-
then-else branches during the evaluation of a classical program. Although it may
be quite mysterious for now, we allow this element to be used in inner products,
as follows:

• •

= 1 and

• t

=

t •

= 0 for any basis element t.
Deﬁnition 6.3.2 (Token). A token on diagram D is a 3-tuple made of:
• A name of a wire of D, corresponding to the position of the token
• A direction in {↑, ↓}
• A state, being either: • or a basis element of the type of the wire it rests on
Tokens only deﬁne part of the evaluation of a diagram. To completely capture
it, we need to consider collections of tokens that interact with each other.
Deﬁnition 6.3.3 (Token State). A token state on a diagram D is a C-valued mul-
tivariate polynomial evaluated in tokens on D, e.g., it is a C-weighted sum of prod-
ucts of tokens.
6.3.2 . Token Machine’s Pulse Rewrite Strategy
We start by developing a Pulse Rewrite Strategy for the Token Machine, in
which we show strong normalization and conﬂuence. Then, following the intuition
from Section 5.5.1, we develop an asynchronous token machine for the Many-
Worlds and show that both strategies are equivalents and that results from the
pulse rewriting strategy can be transposed to the asynchronous one.
The idea is that each generator “pulses” – it generates all necessary token states
to fully capture its dynamics – then tokens that end up colliding interact, hence
capturing the interaction between neighbouring generators.
Again, the token state’s evolution rules are put in two categories. The ﬁrst is
the set of collision rules, which thanks to the above deﬁnition of the inner product
can be summed up in one line:
136

e0 :: (e0 ↓x)(e0 ↑y) →c

x y

That is: tokens that collide on a wire perform the inner product of their respective
states, either reducing to 0, the absorbing element of products of tokens that
represents an error that cancel a product from the sum of product of the tokens
state, or to 1, the neutral element.
The other set of rules indicates how a generator “pulses”:
e1
e0
e2
A
B
A ⊗B
:: __ →p
X
tA∈BA
tB∈BB
(e0 ↑tA)(e1 ↑tB)(e2 ↓⟨tA, tB⟩)
+(e0 ↑•)(e1 ↑•)(e2 ↓•)
e1
e0
e2
A
B
A ⊗B
:: __ →p
X
tA∈BA
tB∈BB
(e0 ↓tA)(e1 ↓tB)(e2 ↑⟨tA, tB⟩)
+(e0 ↑•)(e1 ↓•)(e2 ↑•)
e1
e0
e2
A
B
A ⊕B
:: __ →p
X
tA∈BA
(e0 ↑tA)(e1 ↑•)(e2 ↓injl tA)
+
X
tB∈BB
(e0 ↑•)(e1 ↑tB)(e2 ↓injr tB)
+(e0 ↑•)(e1 ↑•)(e2 ↓•)
e1
e0
e2
A
B
A ⊕B
:: __ →p
X
tA∈BA
(e0 ↓tA)(e1 ↓•)(e2 ↑injl tA)
+
X
tB∈BB
(e0 ↓•)(e1 ↓tB)(e2 ↑injr tB)
+(e0 ↓•)(e1 ↓•)(e2 ↑•)
c
en
e1
e0
A
A
A
...
:: __ →p
n
X
i=1
X
tA∈BA
(e1 ↑•)...(ei ↑tA)...(en ↑•)(e0 ↓tA)
+(e1 ↑•)...(en ↑•)(e0 ↓•)
c
en
e1
e0
A
A
A
...
:: __ →p
n
X
i=1
X
tA∈BA
(e1 ↓•)...(ei ↓tA)...(en ↓•)(e0 ↑tA)
+(e1 ↓•)...(en ↓•)(e0 ↑•)
137

s
e0
e1
A
A
:: __ →p s ·
X
tA∈BA
(e0 ↑tA)(e1 ↓tA) + (e0 ↑•)(e1 ↓•)
e0
e1
A
A
:: __ →p
X
tA∈BA
(e0 ↓tA)(e1 ↓tA) + (e0 ↓•)(e1 ↓•)
e0
e1
A
A
:: __ →p
X
tA∈BA
(e0 ↑tA)(e1 ↑tA) + (e0 ↑•)(e1 ↑•)
e0
1
:: __ →p (e0 ↓⋆) + (e0 ↓•)
e0
1 :: __ →p (e0 ↑⋆) + (e0 ↑•)
Remark 6.3.4. The upside-down versions of each of these generators is simply
exchanging ↑and ↓.
Notice that we denote the collision rules by →c and the pulse rules by →p. We
are now ready to deﬁne the pulse strategy:
Deﬁnition 6.3.5 (Pulse on a generator). Given a diagram D with a generator g,
we write →p|{g} for the pulse rewriting that pulses the generator g.
Deﬁnition 6.3.6 (Restricted Pulse). Given a set S = {g1, . . . , gn} of the genera-
tors of a diagram D, we deﬁne the S-restricted pulse by making all generators of
S pulse exactly once: →all
p|S=→p|{g1} · · · →p|{gn}
Deﬁnition 6.3.7 (Maximum collisions). We denote by t →max
c
s the reduction
t →∗
c s such that s is collision free.
Deﬁnition 6.3.8 (All Pulsing). Given a diagram D with generators S We deﬁne
→all
p as →all
p|S, that is, all generators of the diagram pulse exactly once.
Deﬁnition 6.3.9 (Pulse Strategy). The pulse strategy is deﬁned as being: →all
p
; →max
c
, that is we make every generator pulse exactly once, then we apply collision
rules as long as we can.
138

Remark 6.3.10. Notice that there is an ambiguity here: as the ﬁrst pulse may
create several terms, what we mean is that the next pulse rewrite must be applied
on all of them, and so on. Notice also that this amounts to multiplying together
the terms obtained from each individual pulse rewrite. We could technically allow
more freedom on the order of application of the pulses, but that would defeat the
purpose of having a straightforward canonical rewrite strategy, which is the aim
here.
For example, if we have two generators g1, g2 and we ﬁrst pulse the generator
g1 and obtain the token state t1 + t2, then the generator g2 pulse for both t1 and
t2, obtaining t′
1 + t′
2.
Remark 6.3.11. As each generator is forced to pulse, there is no need for the
diagram to be connected. We may however have fringe cases, with connected
components in the diagram that have no generators that pulse. To remedy this,
via the equational theory deﬁned in Section 6.6 we get that the identity is nothing
but
=
1 , and make this last generator pulse in these cases. Notice that it is
technically important to break the wire’s name in two, so that the tokens obtained
from the pulse are not removed by a collision.
When looking at the rule of the Plus, we can notice that in each summation
where a token carrying some value of type A (resp. B) goes to the right (resp.
left), an annihilator is sent to the left (resp. right): this represents the fact that, if
you imagine a token in the state injl t (resp. injr t) entering the node through
the bottom, it can only go on the left branch (resp. right branch), and hence
the annihilator is here to disable the other branch. This is similar for the n-ary
contraction, except that an annihilator is sent to all the other branches.
Example 6.3.12 (Not Diagram). Take the type of boolean B = 1 ⊕1, then we
compute the pulse semantic of the NOT-diagram:
e0
1 ⊕1
1 ⊕1
e1
e2
e3
:: __ →2
p


(e0 ↑injl ⋆)(e1 ↓⋆)(e2 ↓•)
+(e0 ↑injr ⋆)(e1 ↓•)(e2 ↓⋆)
+(e0 ↑•)(e1 ↓•)(e2 ↓•)

·


(e2 ↑⋆)(e1 ↑•)(e3 ↓injl ⋆)
+(e2 ↑•)(e1 ↑⋆)(e3 ↓injr ⋆)
+(e2 ↑•)(e1 ↑•)(e3 ↓•)


→∗
c (e0 ↑injl ⋆)(e3 ↓injl ⋆)

• ⋆
 
⋆•

+ (e0 ↑injl ⋆)(e3 ↓injr ⋆)

⋆⋆
 
• •

+ (e0 ↑injl ⋆)(e3 ↓•)

• •
 
• ⋆

+ (e0 ↑injr ⋆)(e3 ↓injl ⋆)

• •
 
⋆⋆

139

+ (e0 ↑injr ⋆)(e3 ↓injr ⋆)

⋆•
 
• ⋆

+ (e0 ↑injr ⋆)(e3 ↓•)

• ⋆
 
• •

+ (e0 ↑•)(e3 ↓injl ⋆)

⋆•
 
• •

+ (e0 ↑•)(e3 ↓injr ⋆)

• ⋆
 
• •

+ (e0 ↑•)(e3 ↓•)

• •
 
• •

= (e0 ↑injl ⋆)(e3 ↓injr ⋆)
+ (e0 ↑injr ⋆)(e3 ↓injl ⋆)
+ (e0 ↑•)(e3 ↓•)
The three elements of the sum represent the three possible executions: The ﬁrst
element of the sum, (e0 ↑injl ⋆)(e3 ↓injr ⋆) represents the fact that given input
injl ⋆, the token machine will output the token state injr ⋆, the second element
of the sum does the opposite : those two elements represent sending swapping the
value of a boolean. The third represents the choice of not executing the diagram
at all.
In the Example 6.3.12, we took the liberty to do both pulses at once, and
distributing the terms obtained between the two token states obtained. Formally,
in a run, there is an order of application of the pulses. More than that: since a
ﬁrst rewrite may yield several terms, we can choose the order of application of
the remaining pulses independently for all the terms. To make things clearer, let
us denote a rewrite run as a directed graph where vertices represent terms, and
edges are labelled by a rewrite from a term to one of the terms it rewrites to (we
simply write g to represent →p|{g}). For instance, if a diagram has 3 generators
(gi, i ∈{1, 2, 3}), one possible run of the pulse part of the rewrite strategy may
be:
t
t1
t11
...
...
t111
...
...
tn
tn1
...
tn11
...
...
g1
g1
g2
g2
g2
g2
g2
g2
g3
g3
g3
g3
g3
g3
It is important to underline the fact that a run where all pulses have occurred is
one where every path from the root to a leaf goes through each generator exactly
once.
The pulse strategy is very well-behaved:
Proposition 6.3.13.
The following properties hold for the pulse strategy:
140

• The “pulsing” strategy terminates.
When starting with an empty token state:
• In the terminal token state, there is no token on internal wires.
• In every term of the terminal token state, there is exactly one ascending
(resp. descending) token per input (resp. output).
Proof.
• Termination. In each branch, every generator pulses exactly once, and
there is a ﬁnite number of collisions (actually one per wire connecting
two generators); and there is a ﬁnite number of these branches, as each
pulse creates a ﬁnite number of terms.
• Absence of tokens on internal wires. We have to notice that each gen-
erator pulses once in a sum of terms, each of which sports an outgoing
token per wire (after the rewrite step). Hence, after the pulse phase, in
each term, each internal wire has one descending token from the top
generator, and one ascending token from the bottom generator. There
will then be a collision per term per internal wire, ridding all internal
wires from tokens in the end.
• Tokens on boundary wires. The reasoning is the same as previously,
except that on an input wire, there is no top generator able to create
a descending token; and similarly for output wires. In each term, there
will hence be one remaining ascending token per input, and one de-
scending token per output wire.
The graph may not necessarily be a tree, as two terms that are colinear αt and
βt will be merged into a single vertex (α + β)t. However, as we will see in the
following, this does not happen for the pulse part of the rewrite.
It is easy to check the following lemmas about the pulse part of a rewrite,
which both stem from the fact that pulses are totally independent: a token on a
given wire and with a given direction can only come from a single generator (the
one connected to the wire and which it is pointing away from):
Lemma 6.3.14. Pulse rewrites (on diﬀerent generators) commute, i.e.:
...
...
...
g1
g1
g2
g2
g2
g2
≡
...
...
g2
g2
g1
g1
g1
g1
Proof. As we pulse all the generators before applying the collision, the com-
mutation of the order of the pulse is direct.
141

Lemma 6.3.15. The pulse part (and subparts) of a rewrite run is a directed rooted
tree.
Proof. Direct by deﬁnition of the pulse.
It is also easy to get the following results on the collision part of the rewrite
run:
Lemma 6.3.16. Collision rewrites commute.
Proof. As each wire have at most two tokens on it, two diﬀerent collisions
does not interact with each other and hence commute.
Lemma 6.3.17. The collision part of the pulse strategy consists in an in-forest (a
forest whose edge points to the roots).
Proof. As some collision may merge, it makes it an in-tree. All terms at the
end hence form an in-forest.
This can be used to show that all possible orderings of the application of the
pulses are equivalent:
Theorem 6.3.18. The pulse rewrite strategy is conﬂuent.
Proof. To show conﬂuence, we can show that, for a given diagram, there is a
generic run to which every other pulse run is equivalent. Provide an arbitrary
(total) ordering of the generators (g1 ≺... ≺gn), and similarly on the wires
(e1 ≺... ≺em). The generic canonical run pulses the generators always in the
selected order, and then does the same for the collisions.
Let us ﬁrst focus on the pulse part of a given run. We can show that using
commutation of the pulses, we can get it in the same form as the canonical
one: by induction, we can show that every subtree can be put in a form that
preserves the ordering. The base case is obvious. For the induction part,
consider a vertex t of the tree, and consider all its subtrees t1, ..., tk, each on
their own subalphabet of {g1, ..., gn}. It is easy to see that all the subalphabets
are the same, as every path goes through each generator exactly once, and all
edges from t to its children go through the same gi. By induction hypothesis,
all the subtrees can be put in a form that preserves the ordering of generators.
Let gk1 be the ﬁrst in this subalphabet, then either gi ≺gk1, in which case t is
already in the right form; or gk1 ≺gi, in which case we can commute gi and
142

gk1 and use the induction hypothesis again on the subtrees.
t
t1
tk
...
gi
gi
{gk1, ..., gkp}
t
t′
1
t′
k
...
gi
gi
gk1 ≺... ≺gkp
t
gi≺gk1≺...≺gkp
gi≺gk1
t
t′
1
t′
k
...
gk1
gk1
{gi, gk2, ..., gkp}
gk1≺gi
Ind.
Ind.
commutation
t
gk1≺...≺gi≺...≺gkp
Hence, each pulse part of any pulse rewrite yields the same terms. From
there, it is easy to see that collisions can be commuted when necessary to get
the canonical form, where again the ﬁnal terms do not change. This ﬁnishes
the proof of conﬂuence.
The pulse token machine will allow us to ﬁrst deﬁne the asynchronous token
machine, following the same intuition as the pulse token machine for the ZX-
Calculus described in Section 5.5.1.
6.3.3 . Token Machine’s Asynchronous Rewriting
We now introduce the set of rules for the Asynchronous Token Machine. When
a token ﬂows through the graph it will rewrite according to the set of local rules.
The rules are split in two: a single collision rule and a set of diﬀusion rules. The
collision rule is the same as before: e0 :: (e0 ↓x)(e0 ↑y) →c

x y

.
The diﬀusion rule tells us what to do when a token enters a node: tokens car-
rying an annihilator will simply travel through the graph, duplicating itself through
the other input / output wire of the node. Otherwise, the token will travel through
the node, updating the state it carries and eventually generating new tokens on
the other wires of the node.
The diﬀusion rules are given by:
e1
e0
e2
A
B
A ⊗B
::
(e0 ↓•) →d
(e1 ↑•)(e2 ↓•)
(e1 ↓•) →d
(e0 ↑•)(e2 ↓•)
(e2 ↑•) →d
(e0 ↑•)(e1 ↑•)
(e0 ↓tA) →d
X
tB∈BB
(e1 ↑tB)(e2 ↓⟨tA, tB⟩)
(e1 ↓tB) →d
X
tA∈BA
(e1 ↑tA)(e2 ↓⟨tA, tB⟩)
(e2 ↑⟨tA, tB⟩) →d
(e0 ↑tA)(e1 ↑tB)
143

e1
e0
e2
A
B
A ⊕B
::
(e0 ↓•) →d
X
tB∈BB
(e1 ↑tB)(e2 ↓injr tB) + (e1 ↑•)(e2 ↓•)
(e1 ↓•) →d
X
tA∈BA
(e0 ↑tA)(e2 ↓injl tA) + (e0 ↑•)(e2 ↓•)
(e2 ↑•) →d
(e0 ↑•)(e1 ↑•)
(e0 ↓tA) →d
(e1 ↑•)(e2 ↓injl tA)
(e1 ↓tB) →d
(e0 ↑•)(e2 ↓injr tB)
(e2 ↑injl tA) →d
(e0 ↑tA)(e1 ↑•)
(e2 ↑injr tB) →d
(e0 ↑•)(e1 ↑tB)
c
en
e1
e0
A
A
A
...
::
(ei ↓tA) →d
(e0 ↓tA)
Y
k∈{1,...,n}\{i}
(ek ↑•)
for i ∈{1, ..., n}
(ei ↓•) →d
X
k∈{1,...,n}\{i}
tA∈BA
(ek ↑tA)(e0 ↓tA)
Y
j∈{1,...,n}\{i,k}
(ej ↑•)
+(e0 ↓•)
Y
j∈{1,...,n}\{i}
(ej ↑•)
for i ∈{1, ..., n}
(e0 ↑•) →d
Y
j∈{1,...,n}
(ej ↑•)
(e0 ↑tA) →d
X
k∈{1,...,n}
(ek ↑tA)
Y
j∈{1,...,n}\{k}
(ej ↑•)
s
e0
e1
A
A
::
(e0 ↓•) →d
(e1 ↓•)
(e1 ↑•) →d
(e0 ↑•)
(e0 ↓tA) →d
s(e1 ↓tA)
(e1 ↑tA) →d
s(e0 ↑tA)
e0
e1
A
A
::
(e0 ↑•) →d
(e1 ↓•)
(e1 ↑•) →d
(e0 ↓•)
(e0 ↑tA) →d
(e1 ↓tA)
(e1 ↑tA) →d
(e0 ↓tA)
e0
e1
A
A
::
(e0 ↓•) →d
(e1 ↑•)
(e1 ↓•) →d
(e0 ↑•)
(e0 ↓tA) →d
(e1 ↑tA)
(e1 ↓tA) →d
(e0 ↑tA)
e0
1
:: (e0 ↑•) →d
1
(e0 ↑⋆) →d
1
The upside-down versions of each of these generators can easily be obtained by
simply exchanging ↑and ↓.
144

With this token machine, it is important that we give priority to collisions, as
to not allow token to cross each other without colliding, just as in Chapter 5. We
then take the same deﬁnition of collision-free token state (Deﬁnition 5.3.6) and of
rewriting system (Deﬁnition 5.3.7.)
Deﬁnition 6.3.19 (Asynchronous Token Machine Rewriting System). A rewrite
step (→) is deﬁned as a diﬀusion rule, followed by all possible collisions involving
tokens arising from this rewrite, until none apply, →:=→d; →max
c
Remark 6.3.20. Another possible deﬁnition of the rewriting system is to use the
Pulse Machine. Following intuition given in Section 5.5.1, a diﬀusion rule from the
asynchronous machine is equivalent to a pulse (on the same generator) followed
by a collision (on the wire that bore the initial token): →d=→p|g; →c. We will use
this extensively in the proofs.
Example 6.3.21. Taking back the NOT diagram from Example 6.3.12, we now use
the Asynchronous rewriting machine with initial state (e0 ↓injl ⋆), underlying the
term being rewritten:
e0
1 ⊕1
1 ⊕1
e1
e2
e3
:: (e0 ↓injl ⋆)
→d (e1 ↓⋆)(e2 ↓•)
→d (e3 ↓injr ⋆)(e2 ↑•)(e2 ↓•)
→c (e3 ↓injr ⋆)
When a token in the state injl −enters a plus node, it will go on the left output,
producing a annihilator on the right output as a way of indicating that this branch
cannot be taken. Instead, if we had rewritten the annihilator ﬁrst we would have
gotten:
(e1 ↓⋆)(e2 ↓•)
→d (e1 ↓⋆)(e1 ↑⋆)(e3 ↓injr ⋆) + (e1 ↓⋆)(e1 ↑•)(e3 ↓•)
→c (e3 ↓injr ⋆) + (e1 ↓⋆)(e1 ↑•)(e3 ↓•)
→c (e3 ↓injr ⋆)
Finally, if we took for initial token state (e0 ↑injl ⋆)(e0 ↓injl ⋆) + (e0 ↑
injr ⋆)(e0 ↓injr ⋆) + (e0 ↑•)(e0 ↓•) we would have gotten the same results
as the one from Example 6.3.12. This token state is reminiscent of the initial token
state from Theorem 5.3.25.
An important fact is the following:
145

Lemma 6.3.22. In the run of a well-formed cycle-balanced token state, collisions
and pulses can commute in the following sense:
...
g
g
...
≡
...
g
g
...
g
g
c1
cn
c1
cn
c1
cn
...
...
Proof. Let us consider generator g on which the pulse applies, and term t1
with collision c1 that happens before the pulse (one of the other collisions
may happen on the same wire). The collision necessarily happens between
two tokens on the same wire e1. Notice that because of well-formedness, e1
cannot be connected to g. Hence, the collision and the pulse are independent,
we have:
...
g
g
...
≡
c1
cn
c2
...
g
g
...
c1
cn
c2
c1
g
g
...
Continuing with all collisions gives the desired result.
Example 6.3.23. We run a simple example of the execution of the Token Machine.
Underlined terms are the one being rewritten.
c
c
c
c
α
λ
γ
β
e1
e7
e6
e8
e12
e5
e4
e3
e2
e11
e10
e9
e13
e14
1⊕1
1⊕1
:: (e1 ↓injl ⋆) →p (e2 ↓⋆)(e3 ↓•)
→p (e4 ↓⋆)(e3 ↓•)(e5 ↓•) + (e5 ↓⋆)(e3 ↓•)(e4 ↓•)
→2
p α(e8 ↓⋆)(e3 ↓•)(e5 ↓•) + β(e10 ↓⋆)(e3 ↓•)(e4 ↓•)
→2
p α(e12 ↓⋆)(e3 ↓•)(e5 ↓•)(e9 ↑•) + s
→2
p α(e12 ↓⋆)(e6 ↓•)(e7 ↓•)(e10 ↓•)(e9 ↑•) + s
→p α(e12 ↓⋆)(e9 ↓•)(e7 ↓•)(e10 ↓•)(e9 ↑•) + s
→c α(e12 ↓⋆)(e7 ↓•)(e10 ↓•) + s
→p α(e12 ↓⋆)(e11 ↓•)(e10 ↓•) + s
→p α(e14 ↓injl ⋆)(e13 ↑•)(e11 ↓•)(e10 ↓•) + s
→p α(e14 ↓injl ⋆)(e10 ↑•)(e11 ↑•)(e10 ↓•)(e11 ↓•) + s
→2
c α(e14 ↓injl ⋆) + s
→∗α(e14 ↓injl ⋆) + β(e14 ↓injr ⋆)
Where s = β(e13 ↓⋆)(e3 ↓•)(e4 ↓•)(e11 ↑•) and rewrites similarly as the other
operand of the +.
146

This diagram describes a linear operator from the spaces of input token of
type 1 ⊕1 to the spaces of output token of type 1 ⊕1, that sends (e0injl ⋆) to
α(e14 ↓injl ⋆) + β(e14 ↓injr ⋆) and (e1injr ⋆) to γ(e14 ↓injl ⋆) + λ(e14 ↓
injr ⋆). This map can therefore be seen as the
α
γ
β
λ

matrix.
This is very reminiscent of the rewrite strategy from Chapter 5, and indeed
several results can be transported to our framework. Not all initial conﬁgurations
make sense, or even terminate in the token machine’s semantics, so we start by
narrowing the set of allowed states, such as having two tokens on the same output
wire without colliding.
Deﬁnition 6.3.24 (Polarity of a Term in a Path). Let D be a diagram, and p ∈
Paths(D) be a path in D. Let t = (e, d, x) be a token on D. Then:
P(p, t) =





1
if e ∈p and e is d-oriented
−1
if e ∈p and e is ¬d-oriented
0
if e /∈p
We extend the deﬁnition to any term α t1...tm of a token-state s:
P(p, 0) = P(p, 1) = 0,
P(p, α t1...tm) = P(p, t1) + ... + P(p, tm).
Deﬁnition 6.3.25 (Well-formedness). Let D be a diagram, and s a token state on
D. We say that s is well-formed if for every term t in s and every path p ∈Paths(D)
we have P(p, t) ∈{−1, 0, 1}.
As before, well-formedness is invariant under any of the token machine rewriting
rule deﬁned above. It prevents “bad”, unwanted conﬁgurations, such as for instance
the possibility for two tokens in the same term to be on the same wire, facing the
same direction, but with diﬀerent states. It does not, however, prevent inﬁnite
runs. We can do so by requiring the cycle-balanceness:
Deﬁnition 6.3.26 (Cycle-Balanced Token State). Let D be a diagram, and t a
term in a token state on D. We say that t is cycle-balanced if for all cycles c ∈
Cycles(D) we have P(c, t) = 0. We say that a token state is cycle-balanced if all
its terms are cycle-balanced.
We recover Proposition 5.3.11, Proposition 5.3.12, Proposition 5.3.13, Lemma 5.3.15
and Theorem 5.3.16 in this new setting, the proofs are the same:
Proposition 6.3.27 (Invariance of Well-Formedness). Well-formedness is pre-
served by (⇝): if s ⇝∗s′ and s is well-formed, then s′ is well-formed.
Proposition 6.3.28 (Full Characterization of Well-Formed Terms). Let D be a
Many-Worlds diagram, and s ∈tkS(D) be ill-formed, i.e. there exists a term t in
s, and p ∈Paths(D) such that |P(p, t)| ≥2. Then we can rewrite s ⇝s′ such
that a term in s′ has a product of at least two tokens of the form (e0, d, _).
147

Proposition 6.3.29 (Invariant on Cycles). Let D be a Many-Worlds diagram, and
c ∈Cycles(D) a cycle. Let t1, . . . , tn be tokens, and s be a token state such that
t1...tn ⇝∗s. Then for every non-null term t in s we have P(c, t1...tn) = P(c, t).
Lemma 6.3.30 (Rewinding). Let D be a Many-Worlds diagram, and t be a term
in a well-formed token state on D, and such that t ⇝∗P
i λiti, with (en, d, x) ∈t1.
If t is cycle-balanced, then there exists a path p = (e0, ..., en) ∈Paths(D) such
that en is d-oriented in p, and P(p, t) = 1.
Theorem 6.3.31 (Termination of well-formed, cycle-balanced token state). Let
D be a Many-Worlds diagram, and s ∈tkS(D) be well-formed. The token state s
is strongly normalizing if and only if it is cycle-balanced.
Proposition 6.3.32 (Local Conﬂuence). Let D be a Many-Worlds diagram, and
s ∈tkS(D) be well-formed and collision-free. Then, for all s1, s2 ∈tkS(D) such
that s1
⇝
s ⇝s2, there exists s′ ∈tkS(D) such that s1 ⇝∗s′ ∗
⇝
s2.
Proof. We can directly adapt the proof from Proposition 5.3.18. When two
rewrites are applied to tokens at position e and e′, we once again reason on the
distance between the two tokens. All the initial cases are the same, so we only
need to look at when d(e, e′) = 1. We recall that the two tokens have to point
to the same generator (otherwise they would not respect well-formedness).
We then show the property for all generators and by case analysis on the state
carried by the tokens:
• e0
e1
A
A
, for any state t, t′ we have:
(e0 ↓t)(e1 ↓t′) →d (e1 ↑t)(e1 ↓t′)
↓d
↓c
(e0 ↓t)(e0 ↑t′) →c

t t′
• Cap is similar
•
e1
e0
e2
A
B
A ⊕B
: We reason by case hypothesis on the state of the tokens.
–
(e0 ↓t)(e2 ↑injl t′) →d (e1 ↑•)(e2 ↓injl t)(e2 ↑injl t′)
↓d
↓c
(e0 ↓t)(e0 ↑t′)(e1 ↑•) →c
(e1 ↑•)

t t′
–
(e0 ↓t)(e2 ↑injr t′) →d (e1 ↑•)(e2 ↓injl t)(e2 ↑injr t′)
↓d
↓c
(e0 ↓t)(e1 ↑•)(e2 ↑t′) →c
0
148

–
(e0 ↓•)(e2 ↑•)
→d (
X
tB∈BB
(e1 ↑tB)(e2 ↓injr tB) + (e1 ↑•)(e2 ↓•))(e2 ↑•)
↓d
↓c
(e0 ↓•)(e0 ↑•)(e1 ↑•) →c
(e1 ↑•)
–
(e0 ↓t)(e2 ↑•)
→d
(e1 ↑•)(e2 ↓injl t)(e2 ↑•)
↓d
↓c
(e0 ↓t)(e0 ↑•)(e1 ↑•) (e1 ↑•) →c
(e1 ↑•)
–
(e0 ↓t1)(e1 ↓t2)
→d
(e1 ↑•)(e2 ↓injl t1)(e1 ↓t2)
↓d
↓c
(e0 ↓t1)(e2 ↓injr t2)(e0 ↑•) (e1 ↑•) →c
0
– All the other cases being similar.
• The cases
e1
e0
e2
A
B
A ⊗B
and
c
en
e1
e0
A
A
A
...
are akin to the case of the
e1
e0
e2
A
B
A ⊕B
.
•
s
e0
e1
A
A
–
(e0 ↓•)(e1 ↑t) →d (e1 ↓•)(e1 ↑t)
↓d
↓c
s(e0 ↓•)(e0 ↑t) →c
0
–
(e0 ↓t1)(e1 ↑t2) →d s(e1 ↓t1)(e1 ↑t2)
↓d
↓c
s(e0 ↓t1)(e0 ↑t2) →c
s

t1 t2

– All the other cases being similar.
The upside-down version of each generator is similar.
Corollary 6.3.33 (Conﬂuence). Let s be a well-formed, cycle-balanced token state
on a diagram D. Then (→) terminates on s, and it is conﬂuent.
Finally, we can now relate the pulse token machine to the asynchronous one:
Theorem 6.3.34 (Relation to the Pulse Strategy). Let t be a well-formed cycle-
balanced term on a connected diagram D. A generator will be visited by tokens
during the rewrite if there exists a path p from a token of t leading to the generator
such that P(p, t) = 1. Call S the set of generators that will be visited.
Then, if t →max↓s then t →all
p|S; →max
c
↓s.
In other words, the result of the rewrite strategy on the term t is the same as
the pulse strategy on t restricted to S.
149

Proof. First, following Remark 6.3.20 we remind that a diﬀusion rule is equal
to a pulse followed by collision.
Hence →=→d; →max
c
=→p|g; →c; →max
c
We write c|E(g) for the application of the collisions on the edges of a given
generator g.
First, consider the following run of the asynchronous token machine:
t
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
p|{g}
p|{g}
c|E(g)
c|E(g)
∗
∗
p|{g′}
p|{g′}
p|{g′′}
p|{g′′}
Then, following Lemma 6.3.22 we can push the collisions rules at the end of
the rewriting sequence:
t
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
p|{g}
p|{g}
p|{g′}
p|{g′}
p|{g′′}
p|{g′′}
...
cmax
cmax
cmax
cmax
This form a run of the pulse token machine, which by the conﬂuence of the
pulse token machine (Theorem 6.3.18) and the fact pulse commutes (Lemma 6.3.14),
we can make sure that on each step, all terms pulse on the same generator
as in:
t
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
p|{g}
p|{g}
p|{g′}
p|{g′}
p|{g′}
p|{g′}
...
cmax
cmax
cmax
cmax
150

6.3.4 . Discussions
As discussed in Section 6.2.1, some diagrams does not make sense. We will
consider two diagrams, both of input type A⊕B, for some A, B and a run of each
diagram, using the asynchronous token machine. For both diagrams, the initial
token state consist of a single token entering through the sole input of type A⊕B
in the state injl t for some t an element of the basis of type A.
e1
e2
e0
:: (e0 ↓injl t)
→d (e1 ↓t)(e2 ↓•)
→d (e2 ↑t)(e2 ↓•)
→c 0
e0
e1
e2
e3
:: (e0 ↓injl t)
→d (e1 ↓t)(e2 ↓•)
→d (e1 ↓t)(e1 ↑•)(e3 ↓•)
→c 0
In both case, the end result is 0: this can be explained by the fact that in both
case, we try to make interact two wires that are in ⊕with one another: on the
ﬁrst diagram through a cup, and the second diagram through a tensor. Once two
wires have been split by a ⊕, they cannot interact freely with one another. This
requires us to be careful when we will consider the equational theory. In particular,
this motivates the introduction of the worlds labellings where each wire will be
aﬀected a set of worlds, making sure that we can distinguish between diagrams
that have a proper semantics and whose semantics will just be 0.
6.4 . Worlds labelling
We now want to deﬁne the worlds labelling and equational theory of the lan-
guage. We take inspiration from the Token Machine in order to deﬁne both. For
instance, take the diagram that splits and merges wires through tensors:
If a token carries the state ⟨t1, t2⟩of type A ⊗B, it will be split into the two
branches before being remerged again, so it corresponds to just doing nothing,
therefore we should have the following equation:
≡
and similarly for the plus. Now imagine two scalar one after another on a wire:
s
t
151

then, a token entering this diagram will collect both scalar in, creating their prod-
uct, therefore we should get the following equations:
≡
s
t
s × t
Now, consider the diagram of a plus where one of the input wire is bent
backward by a cap and put in parallel of the output wire of the plus, together with
another wire:
This diagrams cannot be represented in sheet diagrams as the two parallel wire
at the bottom left lives in “disjoint worlds” and hence cannot interact, therefore
those two wire are in a sort of superposition. It is also possible to consider that
the third wire is in all the worlds and hence in superposition with nobody. In sheet
diagrams, one cannot have next to each other, two sheets in superposition and one
not in superposition with any of those.
One thing that is not inherently clear is how to represent the fact that a part
of a diagram can be disabled by an annihilator. This will be done through the
worlds labelling. We call worlds labelling the attribution of multiple worlds to a
single wire. The worlds labelling should be thought of as a kind of validity criterion,
similarly to what is done in Proof Nets. As mentioned, a wire can be deactivated,
this was represented in the token machine with the use of the annihilator. When
a token enters a co-contraction-node, we take the sum of all the possible outputs
where the token can be, in product with annihilators on all the other branches,
similarly, the worlds labelling of the output of the co-contraction-node will have to
be all disjoint, while its input will be the disjoint union of all the possible worlds
labelling of the output. We then deﬁne the equational theory on our diagram.
In order to label wires of our diagram with sets of worlds w ⊆W from a given
world set W. For each world a ∈W, wires labelled by a set containing a are said
to be "enabled in a", and the others are said "disabled in a". This allows us to
correlate the enabling of wires. Before making this formal, we illustrate this notion
through the following example:
Example 6.4.1. The "controlled not" on quantum bits can be represented by the
Figure 6.4. The ﬁgure is split into two parts: the control part on the left-hand-side,
and the computational part on the right-hand-side. The idea is that the control
part, that uses ⊕, will behave as an if-then-else and will bind the world a to the
case where the control quantum bit is |0⟩, and the worlds b and c to the case where
the control quantum bit is |1⟩. Lastly, the world ⋆appears nowhere in the labels,
152

c
c
1 ⊕1 : {a, b, c}
1 : {a}
1 : {b, c}
1 ⊕1 : {a, b, c}
1 ⊕1 : {b, c}
1 : {a}
1 : {b}
1 : {c}
1 ⊕1 : {b, c}
1 ⊕1 : {a, b, c}
1 ⊕1 : {a, b, c}
Figure 6.4: Controlled Not with world set {a, b, c, ⋆}
and corresponds to "we do not evaluate this circuit at all"1. On the computational
part, we apply the identity within the world a, we negate |0⟩into |1⟩within b, and
we negate |1⟩into |0⟩within c. The "controlled not" above can then be seen as a
diagram DCNOT of A →A together with a labelling function ℓA which labels every
wire with a set of worlds.
We now give a formal deﬁnition of the concept of worlds:
Deﬁnition 6.4.2 (Worlds Labelling). Given a diagram D and a world set W, the
worlds labeling consist of a labelling function ℓD from the wires of D to the subsets
of W, satisfying the following constraints:
w
w
v
w
w
s
w
w
w
v
w⊔v
w
w
w
w
∀n ≥0,
c
w1
wn
w1 ⊔· · · ⊔wn
· · ·
where ⊔denotes disjoint set-theoretic union. The constraints for the mirrored
versions are similar. The sequential composition ◦and the parallel composition
□preserve the labels.
We write f :   □n(Ai, wi) →  □m(Bj, vj) for a diagram f with n input wires
of type A1, . . . , An with worlds labelling w1, . . . , wn and m output wires of type
B1, . . . , Bm with worlds labelling v1, . . . , vm.
6.5 . The Denotational Semantics
We give a few intuitions and how the denotational semantics of the Many-
Worlds Calculus works without going into the details and precise deﬁnition, as it
was mainly developed by Marc de Visme and Renaud Vilmart. More details can be
found in [Cha+22].
Intuitively, the denotational semantics is based on the semantics of the pulse
rewriting strategy and was developed in order to match it correctly, even though a
1While not strictly necessary, it is often practical to have a world absent from every
wire.
153

formal relation between the two, in the style of Theorem 5.3.25 is not yet estab-
lished.
The semantics comes in two forms: a ﬁrst one, noted JDKa which correspond
to the semantics of the diagram D on the world a, and another JDK which consist
of the sum of the semantics JDKa for each world a in the world set W. In some
way, JDKa can also be seen as the denotational equivalence of the asynchronous
rewriting while JDK as the denotational equivalence of the pulse rewriting.
The semantics is deﬁned on ﬁnite dimensional R-modules and, as for the ZX-
Calculus, the semantics of each generators is deﬁned as a matrix, in this section
we only show the semantics of the tensor and plus in order to give an intuition of
the semantics.
e1
e0
e2
A
B
A ⊗B
:: __ →p
X
tA∈BA
tB∈BB
(e0 ↑tA)(e1 ↑tB)(e2 ↓⟨tA, tB⟩)
+(e0 ↑•)(e1 ↑•)(e2 ↓•)
u
wwv
A : {a}
B : {a}
A⊗B : {a}
World set: {a, ⋆}}
~ =
A □B
A □•
• □B
• □•


A ⊗B
Id
0
0
0
•
0
0
0
1
e1
e0
e2
A
B
A ⊕B
:: __ →p
X
tA∈BA
(e0 ↑tA)(e1 ↑•)(e2 ↓injl tA)
+
X
tB∈BB
(e0 ↑•)(e1 ↑tB)(e2 ↓injr tB)
+(e0 ↑•)(e1 ↑•)(e2 ↓•)
u
wwv
A : {a}
B : {b}
A ⊕B : {a, b}
World set: {a, b, ⋆}}
~ =
A □B
A □•
• □B
• □•


A ⊕B
0
0
Id
0
0
Id
0
0
•
0
0
0
1
In each matrix, the last-line last-column correspond to the case where only
annihilators are pulsed: as the annihilators destroys everything, the values are ﬁll
to 0, with a 1 in the diagonal to indicate the fact that we do not evaluate this
diagram at all.
In the case of the tensor, the two lines A□• and •□B correspond to the fact
that, in the type of the diagram A□B, we only have the data of type A (resp. of
type B) available, in both case the matrix is set to 0, as it is impossible to have
only one of the two data in a tensor, and hence, in the case A□B, where both
data are available, we set the matrix to the identity, as it correspond to the tokens
just ﬂowing through the diagram.
The same intuition can be given for the plus: in the case A□B we get 0, as
both data cannot be avaiable, and then we let the token ﬂow either in A or in B
in the case A□• and •□B.
154

c
≡W
c
c
≡W
c
· · ·
· · ·
· · ·
· · ·
w1
wn
w1
wn
⊔nwi
⊔nwi
⊔nwi ⊔nwi
w1
wn
w1
wn
c
c
c
≡W
· · ·
· · ·
⊔nwi ⊔nvi
w1 ⊔v1wn ⊔vn w1 ⊔v1wn ⊔vn
⊔nwi ⊔nvi
c
s
s
s
c
· · ·
· · ·
⊔nwi
⊔nwi
w1
w1
wn
wn
≡W
c
c
c
≡W
· · ·
· · ·
v1
vk
w1
w1
wn
wn
v1
vk
c
c
c
· · ·
· · ·
≡W
w
w
w
w
w
w
w
w
s × t
s
t
w
w
w
w
w
w
≡W
≡W
w
w
w
w
≡W
w
w
w
w
≡W
⊔nwi
⊔nwi
⊔nwi
c
c
· · ·
⊔nwi
≡W
w
w
w
w
1
≡W
w
w
w
w
s
t
s × t
≡W
w
c
· · ·
w1
wℓ
wk
wn
⊔nwi
c
c
· · ·
· · ·
≡W
⊔nwi
c
w1
wℓ
wk
wn
· · ·
· · ·
· · ·
w ⊔v
c
w v
w ⊔v
c
v
w
≡W
Worlds annotations on wires
are ommited when uniquely
determined.
We assume
that:
w ∩v = ∅
wi ∩wj = ∅whenever i ̸= j
vi ∩vj = ∅whenever i ̸= j
w1
wn
wn∩vk
w1∩v1
≡W
w
v
w
v
v
w
w
v
Mirrored up-down versions
of those equations can be de-
duced from the compact clo-
sure.
Additional equations for the
⊕and the scalars are pro-
vided in the next Figure.
c
w
w
≡W
w
w
Figure 6.5: Equations with a Fixed World Set W
The denotation semantics have a more rigorous deﬁnition in [Cha+22] and
comes with an universality theorem (i.e. it can represent any matrices) and com-
pleteness theorem with regards to the equational theory.
6.6 . The Equational Theory
The equational theory is deﬁned in two steps:
1. A set of equations for a ﬁxed set of worlds W. We write ≡W for the induced
congruence, in other words the smallest equivalence relation satisfying those
equations and such that f ≡W f′
=⇒
∀g, h, l, g ◦(f □h) ◦k ≡W g ◦
(f′ □h) ◦k. We list those equations in Figure 6.5.
2. Five additional equations with side eﬀects on the set of worlds. We write
≡for the induced equivalence relation. We have: two equations allowing
the annihilation (or creation, when looking at them from right to left) of
worlds due to coproducts or scalars (ﬁrst row of Figure 6.6); Two equations
155

0
F
· · ·
· · ·
w
0
F\w
· · ·
· · ·
∅
≡
F
· · ·
· · ·
w
v
w′ v′
F\z
· · ·
· · ·
W
W
W\w
W\z
≡
Worlds sets
w ∩w′
v ∩v′
Worlds sets
w′
w′′
w′
w′′
s
t
c
c
W
W
(W\w) ⊔w′ ⊔w′′
Worlds sets
Worlds sets
(W\w) ⊔w′ ⊔w′′
s + t
F
· · ·
· · ·
w ≡
· · ·
· · ·
G
F
· · ·
· · ·
w ≡
· · ·
· · ·
G
Where:
w = {a1, . . . , an}
w′ = {a′
1, . . . , a′
n}
w′′ = {a′′
1, . . . , a′′
n}
G is F where every in-
stance of the world ai
has been replaced by
both a′
i and a′′
i .
Where:
z = (w\w′) ∪(w′\w)
∪(v\v′) ∪(v′\v)
F\w (resp. F\z) is F
where every world of
w (resp. z) has been
removed from the la-
bels.
Figure 6.6: Equations with Side-Eﬀects on World Sets
α |0⟩+ β |1⟩⇝α
β ∈CD(  □, 1 ⊕1)
 
1
√
2
1
√
2
1
√
2
−1
√
2
!
⇝
c
c
c
c
1/
√
2
−1/
√
2
1/
√
2
1/
√
2
∈
CD(1 ⊕1, 1 ⊕1)
Figure 6.7: A Quantum Bit and the Hadamard Unitary
allowing the splitting (or merging, when looking at them from right to left)
of worlds due to coproducts or scalars (second row of Figure 6.6).
Example 6.6.1. We consider R = C. We illustrate how our language can encode
some basic quantum primitive in it and show how they operate. In Figure 6.7 we
show the encoding of a quantum bit α |0⟩+ β |1⟩and the Hadamard unitary. In
particular, the Plus allows to "build" a new quantum bit from two scalars in parallel
or to "open" a quantum bit to recover its corresponding scalars, the left branch
corresponding to |0⟩and the right branch to |1⟩. The meaning of the Contraction
is better seen when applying Hadamard to a quantum bit as we show in Figure 6.8:
it allows us to duplicate and sum scalars. The rewriting sequence of Figure 6.8 is
made using the equational theory deﬁned in Section 6.6.
6.7 . Induced Equations
156

c
c
c
c
1/
√
2
−1/
√
2
1/
√
2
1/
√
2
α
β
→
c
c
c
c
1/
√
2
−1/
√
2
1/
√
2
1/
√
2
α
β
→
c
c
α
√
2
−β
√
2
α
√
2
β
√
2
→
α+β
√
2
α−β
√
2
Figure 6.8: Applying the Hadamard unitary to a quantum bit
c
c
c
c
≡W
w1w2w3
w1w2w3
w1 ⊔w2 ⊔w3
w1 ⊔w2 ⊔w3
c
c
≡W
w
w
w
w
c ≡W c
w1w2w1w2
c
c
c
c
c
c
c
c
c
c
c
c
≡W
≡W
≡W
w1w2w1w2
u v u v
∅∅
∅∅
≡W
c
c ≡W
∅
∅
∅
∅
≡W
s
s
s
≡W
≡W
c ≡W
c
c
c
c
c
≡W
c
c
c
≡W
c
c
c
≡W
c
c ≡W
≡W
s × t≡W
s
t
s
c
s
s
c
≡W
c
s
c
≡W
c ≡W
≡W
c
c
c
c
c
c
≡W
c
c
c ≡W
c
c
1
s
t
s × t
s
s
s
≡W
≡W
≡W
w
w
w
w
w
w
w
w
w
w
w
w
w
w
w
w
v
v v
v
w
w
w
w
w
w
w
w
w
w
w
w
w
u
u
w = u ⊔v
u u
v
v
u
u
v v
u u
∅∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
≡W
≡W
1 ≡W
c ≡W
≡W
c
c ≡W
c ≡W
w
w
w
w
w
v
v
w
w
w
u
u
∅
∅
1
w1w2
w1w2
w1w2
w1w2
w1w2w1w2
w = w1 ⊔w2
u = u1 ⊔u2
v = v1 ⊔v2
w1w2
w1w2
w1w2
w1w2
w1w2
u1 u2 v1 v2 u1 u2 v1 v2
w
w
w
w
w1w2
w1w2
w1w2
w
w
w
w w1w2
w w1w2
w w1w2
w w1w2
w
w
w
w
w
w
w
w
c
c
Figure 6.9: Alternative Presentation of the Equational Theory for a
Fixed World Set W
In Figure 6.5, we presented a set of equations reasonably small, by having
equations parametrized by the arity of the contraction, and by omitting a lot of
useful equations that can be deduced from them.
In Figure 6.9, we take the
opposite approach: we give equations using the contractions of arity zero and
two (which are suﬃcient to generate all the other contractions) and we provide
additional axioms that follow from Lemma 6.7.1 and Lemma 6.7.2.
Note that this is only an alternative presentation to the equational theory for
≡W , the axioms of Figure 6.6 are still required for ≡. We can also note the two
representations are equivalent by taking the contraction of arity n as a syntactic
sugar for multiple composition of the binary contraction.
157

Lemma 6.7.1. Whenever wi are disjoints sets of worlds, we have the following:
c
≡W
· · ·
· · ·
w1
wn
w1
w1
wn
wn
w1
wn
c
· · ·
· · ·
c
c
≡W
· · ·
w1
wn
w1
wn
c
c
· · ·
wn∩wn
w1∩w1
c
c
· · ·
w1
wn
w1
wn
c
c
· · ·
wn
w1
≡W
c
c
c
c
s
≡W
w
w
w w
s
w
w
w
w
≡W
w
w
w
w
1
≡W
w
w
w
w
s
1
s
s
≡W
w w
w
w
s
⊔nwi
≡W
s
c
c
w1
wn
≡W
s
s
w1
wn
· · ·
· · ·
Lemma 6.7.2. Whenever wi are disjoint sets of worlds, and that the vis are dis-
joint sets of worlds too, we have the following:
c
w1 w1
wnwn
· · ·
⊔nwi
≡W
c
c
· · ·
⊔nwi
w1 w1
wnwn
· · ·
c
w1 v1
wn vn
· · ·
⊔n(wi ⊔vi)
≡W
c
c
· · ·
⊔n(wi ⊔vi)
w1 v1
wn vn
· · ·
c
· · ·
⊔nwi
≡W
⊔nwi
Proof. We provide a proof for the third equation, the ﬁrst two are proven sim-
ilarly.
c
w1 v1
wn vn
· · ·
⊔n(wi ⊔vi)
≡W
c
c
· · ·
⊔n(wi ⊔vi)
w1 v1
wn vn
· · ·
c
c
· · ·
⊔n(wi ⊔vi)
w1 v1
wn vn
c
c
· · ·
w1 ⊔v1
wn ⊔vn
≡W
c
c
· · ·
⊔n(wi ⊔vi)
w1 v1
wn vn
c
· · ·
w1 ⊔v1
wn ⊔vn
c
c
≡W
Lemma 6.7.3 (Naturality of the Binary Contraction). For every f : □n(Ai :
158

wi) →□m(Bj : vj) with world set W and every u ⊆W, we have
f
c
c
· · ·
w1\u
wn\u
w1 ∩u
wn ∩u
v1
vm
≡W
c
c
w1\u
wn\u
w1 ∩u
wn ∩u
v1
vm
· · ·
· · ·
f\u
f ∩u
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
where f\u : □n(Ai : wi\u) →□m(Bj : vj\u) is equal to f where every worlds
label w has been replaced by w\u, and similarly for f ∩w.
Lemma 6.7.4 (Empty World). For every f : □n(Ai : ∅) →□m(Bj : ∅) with
world set W but such that every worlds label of f is ∅, we have
f
∅
∅
· · ·
· · ·
∅
∅
≡W
c
c
c
c
· · ·
· · ·
∅
∅
∅
∅
Proof. This is simply proven by replacing every wire by two contractions of
arity zero (sixth axiom of Figure 6.5 with n = 0), and then using the naturality
of the contraction of arity zero (last two lines of Figure 6.5 with n = 0) to
consume every generator.
Example 6.7.5 (The Quantum Switch). Similarly to the "controlled not" in Ex-
ample 6.4.1 where a quantum bit controls whether the identity or a negation is
applied to some data, one can consider the case where a quantum bit control
whether U ◦V or V ◦U is applied to some data (for U and V two quantum oper-
ators on the same type A).
On the rightmost part of Figure 6.10, one can see the most direct implementa-
tion of this "quantum switch", but this implementation uses two copies of U and
V . On the leftmost part of the ﬁgure, we show another implementation which
only relies on one copy of each, and both can be rewritten into another using the
equational theory.
In those diagrams, the world set is W = w ⊔v and we rely on violet, blue and
red wires to indicate respectively worlds labels w ⊔v, w and v. Each ﬁgure has a
control side which operates on a quantum bit (type 1 ⊕1) and binds the world w
to |0⟩and the world v to |1⟩; and a computational side which operates on some
data of an arbitrary type A, on which could be applied U and/or V .
The ﬁrst rewriting step relies on the two lemmas below, both of which being
deducible from the equational theory (see Section 6.7). The second rewriting step
is simply using the properties of a compact closed category.
159

w ⊔v
w
v
Legend
c
c
c
c
c
c
U
V
1 ⊕1 : w ⊔v
A : w ⊔v
1 ⊕1 : w ⊔v
A : w ⊔v
≡w⊔v
c
c
U\v
V \v
1 ⊕1 : w ⊔v
A : w ⊔v
1 ⊕1 : w ⊔v
A : w ⊔v
U\w
V \w
≡w⊔v
c
c
U\v
V \w
1 ⊕1 : w ⊔v
A : w ⊔v
1 ⊕1 : w ⊔v
A : w ⊔v
U\w
V \v
Figure 6.10: Rewriting the Quantum Switch
c
U
≡W
c
U\v
U\w
c
c
≡W
Remark 6.7.6. With Marc de Visme and Renaud Vilmart, a categorical deﬁnition
of the graphical language has been deﬁned in the setting of compact closed and
auto-dual coloured PROP. With it comes a denotational semantics that is sound
and complete with regard to the equational theory. Also, a normal form of the
diagrams have been developed, which allowed to show the completeness. As most
of this work was developed by Marc de Visme and Renaud Vilmart, and was not
necessary for this thesis, we did not put it here. The interested reader can refer
to [Cha+22].
6.8 . Comparison with Other Graphical Languages
The distinctive feature of the Many-Worlds Calculus is that it graphically puts
the tensor and the biproduct on an equal footing. By comparison, other graphical
language for quantum computing are inherently centred around either one of them.
The ZX-calculus [CD11] and cousin languages ZW- and ZH-Calculi [BK19; Had15],
as well as Duncan’s Tensor-Sum Logic [Dun09], use the tensor product as the
default monoid, while more recent language – particularly for linear optics [CP20;
FC22; Clé+22b] – use the biproduct. We have a closer look at each of them in
the following and show how – at least part of – each language can be encoded
naturally in the Many-Worlds Calculus. Most of them comes equipped with an
equational theory. By completeness of our language [Cha+22], all the equations
expressible in the fragments we consider can be derived in our framework.
6.8.1 . ZX-Calculus
The ﬁrst diﬀerence is the restrictions of the ZX-calculus to computations be-
tween qubits, in other words linear map from C2n 7→C2m, while our language can
160

encode any linear map from Cn 7→Cm. The Tensor generator allowing the decom-
position of C2n into instances of C2 was already present in the scalable extension
of the ZX-calculus [CHP19], but the main diﬀerence comes from the Plus (and the
Contraction).
⇝
eiα
1 : {b}
1 : {a}
1 ⊕1 : {a, b} 1 ⊕1 : {a, b}
1 ⊕1 : {a, b}
α
Additionally, every ZX-diagram can be encoded in our graphical language. The
identity, swap, cup and cap of the ZX calculus are encoded by the similar generators
over the type 1⊕1, the Hadamard gate is encoded as in Figure 6.7, and the green
spider is encoded as shown above. An encoding for the red spider can then be
deduced from those.
6.8.2 . Tensor-Sum Logic
The core diﬀerence between the Tensor-Sum Logic [Dun09] and ours is the
presence of the contraction in our graphical language. They instead rely on an en-
richment of their category by a sum, which they represent graphically with boxes.
We show on below how the morphism f + g would be encoded in both their and
our language. More generally, their boxes correspond to uses of our contraction
generator in a "well-bracketed" way. Another point of diﬀerence is their approach
to quantum computation, as we do not assign the same semantics to those su-
perpositions of morphisms. In their approach, the superposition is a classical con-
struction and corresponds to the measurement and the classical control ﬂow, while
in our approach the superposition is a quantum construction and corresponds to
the quantum control.
f
g
f
g
c
c
⇝
w
v
6.8.3 . PBS-Calculus
The PBS Calculus [CP20] allow one to represent coherent quantum control
by the use of polarizing beam splitters (pbs): whenever a qubit enter a pbs node,
depending on the polarity of the qubit it will either go through or be reﬂected. By
making implicit the target system, controlled by the optical system represented by
the diagram, the PBS-Calculus allows one to encode the Quantum Switch (depicted
below). The pbs generator is related to the ⊕of the Many-Worlds.
|
U
V
|
The ﬁrst main diﬀerence with our language is that, since the generators of the
PBS-Calculus represent physical components, any PBS-diagram is by construction
physical, while our approach is more atomic and decomposes physical components
161

into abstract smaller ones.
The second main diﬀerence lie in the trace: while
they can allow a particle to pass through a wire at most twice, in our system,
each wire can be used at most once: more formally, their trace is based on the
coproduct while ours is on the tensor product. If we are assured that each wire can
only be used once during the computation, any PBS-diagram can be translated
to the Many-Worlds calculus directly, with the transformation below, where we
distinguish the control system (the part of the diagram connected to
s) from the
target system (connected to c s) which is implicit in the PBS-Calculus.
|
⇝
c
c
c
c
6.8.4 . LOv-Calculus
In the PBS-Calculus, the qubit in the control system (the one explicitly repre-
sented) cannot be put in arbitrary superpositions of |0⟩and |1⟩during the com-
putation. To allow this feature, we may add some linear optical components to
the language’s generators and end up with the LOv-Calculus [Clé+22b]. In this
language, there is no trace and there is a unique photon travelling the circuit,
which relieves us of the previous constraint. There is also no need for an implicit
target system anymore. All wires at the interface between the generators are of
type 1⊕1, and parallel wires have disjoint sets of worlds. Each generator can then
be interpreted as follows:
c
c
c
c
cos(θ)
cos(θ)
i sin(θ)
i sin(θ)
⇝
θ
c
c
c
c
cos(θ)
cos(θ)
i sin(θ)
i sin(θ)
⇝
θ
|
⇝
eiϕ
⇝
ϕ
0
⇝
c
6.8.5 . Path-Calculus
The Path-Calculus is another recent graphical language for linear optical cir-
cuits [FC22]. Its generators correspond directly to a subset of the Many-Worlds’
with
⇝
c
,
⇝
c
and
⇝
r
r
; where each wire
has type 1 and where parallel wires are on disjoint sets of worlds. This language
is then used as the core for a more expressive language called QPath, which this
time cannot be directly encoded in our language, except when restricting the set
of generators (speciﬁcally to n = 1), in which case
⇝
|1⟩
.
6.8.6 . Linear Logic’s Proof Nets
In a similar spirit to what is done with linear logic proof nets, the Many-Worlds
Calculus feature a validity criterion in the form of the world labelling. While in
162

Linear Logic, a non-valid proof structure has no computational content, in our case
non-valid diagrams (i.e. diagrams whose only worlds label is ∅) can be reduced
under the equational theory to a normal form and is captured by the semantics. As
in the Many-Worlds Calculus, formulas are self-dual, the connection with MALL
validity criterion as in [HVG03] fall short.
6.9 . Representing Computation
As a motivational example, we may see how this Many-Worlds diagrams can
be used to represent computations expressed in the language of Chapter 4 that
explicitly uses the two compositions ⊗(through pairs), and ⊕(through pattern-
matching).
6.9.1 . Syntax of the Language
The language we present here is adapted from Chapter 4, with the addition of
linear combination of terms, as in [SVV18], but without abstraction nor recursion.
The syntax that is used in the language is given as follows, with scalars α ranging
over the commutative semiring R:
(Base types)
A, B ::= 1 | A ⊕B | A ⊗B
(Isos, ﬁrst-order)
α ::= A ↔B
(Values)
v ::= () | x | injl v | injr v | ⟨v1, v2⟩
(Patterns)
p ::= x | ⟨p1, p2⟩
(Expressions)
e ::= v | let p1 = ω p2 in e | e + e | αe
(Isos)
ω ::= {v1 ↔e1 | · · · | vn ↔en}
(Terms)
t ::= () | x | injl t | injr t | ⟨t1, t2⟩|
αt | t + t | ω t | let p1 = t in t
As before, the language features branching through the injl and injr con-
structors, linear combinations of terms and expressions, and as before, the isos
comes with the ODA predicates, used in the typing rule of isos, to ensure exhaus-
tivity and the non-overlapping character of the left-hand and right expressions
of the clauses, allowing in particular to deﬁne unitaries (in the complex setting).
Constraints on the linear combinations may also be used to enforce probabilistic
constraints (i.e., that states are normalized in the quantum setting).
Terms and values are considered modulo associativity and commutativity of
the addition, and modulo the equational theory of modules.
α · (e1 + e2) = α · e1 + α · e2
1 · e = e
α · e + β · e = (α + β) · e
α · (β · e) = (αβ) · e
0 · e1 + e2 = e2
163

We furthermore consider the value and term constructs ⟨−, −⟩, let −= −in −,
injl (−), injr (−) distributive over sum and scalar multiplication.
The typing judgements are the same as in Table 4.2 with the additional rules
from [SVV18]:
Ψ; ∆⊢e t1 : A
Ψ; ∆⊢e t2 : A
Ψ; ∆⊢e t1 + t2 : A
Ψ; ∆⊢e t : A
Ψ; ∆⊢e α t : A
For a more complete description of the language, we refer the reader to [SVV18].
In the following, we will use the shorthands ﬀ:= injl () and tt := injr ().
Example 6.9.1. In the case where R = C, one can encode the Hadamard gate
by:
(
ﬀ↔
1
√
2(ﬀ+ tt)
tt ↔
1
√
2(ﬀ−tt)
)
: 1 ⊕1 ↔1 ⊕1
In particular, when R = C, the isos are necessary unitaries [SVV18] and when
restricting the type system to only the type of booleans 1 ⊕1 it is possible to
encode any quantum circuits inside the language.
For the encoding, we will take again the explicit substitution alternative rewrit-
ing system from Chapter 4.
We recall Convention 4.4.21:
Convention 6.9.2. Given a substitution σ = {x1 7→v1, . . . , xn 7→vn} we will
use the shorthand let σ in t for let x1 = v1 in . . . let xn = vn in t.
The results can be adapted in a straightforward way to this new language:
Property 6.9.3. Given a substitution σ = {xi 7→vi} that closes a term t, then
let σ in t →∗
eβ σ(t).
Property 6.9.4. Given two well-typed terms t, t′ such that t →t′ then t →∗
eβ
t′.
6.9.2 . Encoding into the Many-Worlds
One can encode any term of the language into a Many-Worlds diagram. Given
some typing derivation ξ of a term x1 : A1, . . . , xn : An ⊢e t : B we write Lξ, WM
for the function that maps ξ to a diagram in the Many-Worlds Calculus with n in-
put wires of type A1, . . . , An and one output wire of type B and where W is the set
of worlds needed to encode ξ. For the typing derivation ξ of an iso ⊢ω ω : A ↔B,
Lξ, WM gives a diagram with one input wire of type A and one output wire of type
B.
In order to encode a term into the Many-Worlds it is important to know how
many worlds are needed. For that, we deﬁne W(t) as the set of worlds needed for
t.
164

Deﬁnition 6.9.5. Let W be an inﬁnite set of worlds, we deﬁne W(t), the set of
worlds needed for t by induction over t:
• W(1) = a ∈W such that a is fresh.
• W(injr t) = W(injl t) = W(t)
• W(⟨t1, t2⟩) = W(t1) ⊎W(t2)
• W(αt) = W(t)
• W(t1 + t2) = W(t1) ⊎W(t2)
• W(ω t) = W(ω) × W(t)
• W({v1 ↔e1 | · · · | vn ↔en}) = U
i W(ei)
• W(let p = t in t′) = W(t) × W(t′)
Lξ, W(t)M is deﬁned inductively over ⊢e and ⊢ω as shown in Figure 6.11. For the
encoding, we use the following syntactic sugar:
:=
...
...
. In the remaining
we will simply write LtM for the encoding of the well-typed term t.
One can show that the input worlds and output worlds of an expression are the
same:
Lemma 6.9.6. Given a well-typed term ξ : ∆⊢e t : B, then Lξ, W(e)M have the
same worlds on all of its inputs and all outputs.
Proof. By a straightforward induction on t:
• Case x: as it is just the identity wire the worlds cannot change.
• Case (): direct as there is no input wire.
• All the other cases by direct application of the induction hypothesis on
the subterms.
Example 6.9.7. We can represent the term
t :=
(
⟨tt, x⟩↔
let y = H x in
1
√
2⟨tt, y⟩+
1
√
2⟨ﬀ, y⟩
⟨ﬀ, x⟩↔
let y = Id x in
1
√
2⟨tt, y⟩−
1
√
2⟨ﬀ, y⟩
)
: (1⊕1)⊗2 ↔(1⊕1)⊗2
(already given in [SVV18]) as shown in Figure 6.12. The yellow box stands for the
Hadamard gate (which one can build following Example 6.9.1). Each line of this iso-
morphism corresponds to a column of the ﬁgure. Each column start by matching
the input as ⟨tt, x⟩or ⟨ﬀ, x⟩, then compute y from x, and then build the output by
following the syntax. The world set is computed by composing each of the blocks
165

⦅
x : A ⊢e x : A
⦆
= A ⦅
⊢e () : 1
⦆
=
1
⦅
ξ
∆⊢e t : A
∆⊢e αt : A
⦆
=
α
LξM
...
∆
⦅
ξ
∆⊢e t : A
∆⊢e injr t : A ⊕B
⦆
=
LξM
...
∆
c
⦅
ξ
∆⊢e t : B
∆⊢e injl t : A ⊕B
⦆
= LξM
...
∆
c
⦅
ξ1
∆1 ⊢e t1 : A
ξ2
∆2 ⊢e t2 : B
∆1, ∆2 ⊢e ⟨t1, t2⟩: A ⊗B
⦆
= Lξ1M
...
∆1
Lξ2M
...
∆2
⦅
ξ1
∆⊢e t1 : A
ξ2
∆⊢e t2 : A
∆⊢e t1 + t2 : A
⦆
=
c
c
...
∆
c
Lξ1M
...
Lξ2M
...
⦅
ξ1
∆1 ⊢e t1 : A1 ⊗· · · ⊗An
ξ2
x1 : A1, . . . , xn : An, ∆2 ⊢e t2 : B
∆1, ∆2 ⊢e let ⟨x1, . . . , xn⟩= t1 in t2 : B
⦆
=
Lξ1M
...
∆1
Lξ2M
...
∆2
...
⦅
ξi
∆i ⊢e vi : A
ξ′
i
∆i ⊢e ei : B
⊢e {v1 ↔e1 | · · · | vn ↔en} : A ↔B
⦆
=
A
c
Lξ′
1M
Lξ′
nM
c
Lξ1M†
LξnM†
...
B
...
...
⦅
ξ1
⊢ω ω : A ↔B
ξ2
∆⊢e t : A
∆⊢e ω t : B
⦆
= Lξ1M
Lξ2M
...
∆
Figure 6.11: Inductive translation of terms from Section 6.9 into Many-
Worlds diagrams.
of this term, using the world-agnostic composition of MW∀. It can be seen as a
subset of {a, b} × {c, c′} × {0, 1, 2, 3} where {a, b} corresponds to being on the
ﬁrst or second line of the matching, {c, c′} being on the left or right of the sum,
and {0, 1, 2, 3} being the world set of the Hadamard gate.
We can then show that the reduction of the language matches the equational
theory of the Many-Worlds Calculus. First, we recall the notion of orthogonality
between pure values and show that two values are orthogonal when they do not
match. Then, we show that when the diagrams of orthogonal values are composed
together, it reduces to the diagram with empty worlds.
166

c
c
c
c
c
c
c
c
c
c
c
c
1
√
2
1
√
2
1
√
2
−1
√
2
{b, b′}
{a0, a1, a2, a3, a′
0, a′
1, a′
2, a′
3}
(1 ⊕1) ⊗(1 ⊕1) : {a0, a1, a2, a3, a′
0, a′
1, a′
2, a′
3, b, b′}
{a0, a1, a2, a3}
{a′
0, a′
1, a′
2, a′
3}
{b}
{b′}
{b}
{b′}
{b, b′}
ﬀ
x
tt
x
y
tt
ﬀ
y
tt
ﬀ
y
y
{a0, a1, a2, a3}
{a′
0, a′
1, a′
2, a′
3}
{a0, a1, a2, a3, a′
0, a′
1, a′
2, a′
3}
(1 ⊕1) ⊗(1 ⊕1) : {a0, a1, a2, a3, a′
0, a′
1, a′
2, a′
3, b, b′}
⟨tt, x⟩↔
let y = H x in
1
√
2 ⟨tt, y⟩+
1
√
2 ⟨ﬀ, y⟩
⟨ﬀ, x⟩↔
let y = Id x in
1
√
2 ⟨tt, y⟩−
1
√
2 ⟨ﬀ, y⟩
H
Figure 6.12: Representation of the term t from Example 6.9.7.
Deﬁnition 6.9.8 (Orthogonality on pure values). Two values v1, v2 of the same
type are said to be orthogonal, noted v1⊥v2 if it can be inferred from the following
rules:
injl v⊥injr v′
v⊥v′
injl v⊥injl v′
v⊥v′
injr v⊥injr v′
v⊥v′
⟨v, v1⟩⊥⟨v′, v2⟩
v⊥v′
⟨v1, v⟩⊥⟨v2, v′⟩
Lemma 6.9.9. Given two well-typed values v1, v2 and a subsitution σ, if σ[v1] ̸= v2
then v1⊥v2
Proof. By a straightforward induction on the pattern-matching σ[v1] = v2, the
case where v1 = x or v1 = v2 = () cannot occur otherwise v1 and v2 would
match. For the other cases we can directly apply the induction hypothesis.
We can then show that orthogonal values, composed together cancel each
other out and destroy the worlds on their inputs and outputs:
Lemma 6.9.10. Given two well typed values ∆1 ⊢e v1 : A and ∆2 ⊢e v2 : A, if
167

v1⊥v2 then
W
· · ·
· · ·
Lv1M†
Lv2M
W ′
· · ·
· · ·
∅
∅
c
c
c
c
· · ·
· · ·
· · ·
· · ·
≡
∅
∅
w
w
w
w
where W ′ = W\w.
Proof. By induction on v1⊥v2, for simplicity we do not draw the context.
• Case injl v⊥injr v′:
c
c
Lv1M†
Lv2M
w
∅
∅
w
w
≡
Lv1M†
c
∅
∅
≡
c
c
c
Lv2M
∅
∅
• Case
v⊥v′
injl v⊥injl v′:
c
Lv2M
Lv1M†
w
∅
w
∅
w
≡
Lv1M†
c
w
∅
IH
=
c
Lv2M
∅
c
∅
c
c
c
∅
c
c
∅
c
∅
≡
• Case
v⊥v′
injr v⊥injr v′ is similar.
• Case
v⊥v′
⟨v, v1⟩⊥⟨v′, v2⟩:
Lv2M†
Lv′
1M
Lv1M†
w
w
w
w
w
≡
Lv1M† Lv2M†
w
w
IH
=
c
Lv′
1M
∅
c
∅
Lv′
2M
Lv′
2M
c
∅
c
∅
≡
Lv2M†
∅
Lv′
2M
c
∅
c
∅
• Case
v⊥v′
⟨v1, v⟩⊥⟨v2, v′⟩is similar.
Conversely, we can show that if two values matches under substitution σ, their
composition gives the diagram where σ has been decomposed in a succession of
let:
Lemma 6.9.11. Given two well-typed pure values ∆⊢e v and ∆⊢e v′ and any
expression e such that ∆⊢e e, given σ[v] = v′ such that σ = {(xi 7→vi)i∈I}, then
Lv′M
LvM†
LeM
...
· · ·
≡
Llet σ in eM
...
Proof. By induction on σ[v] = v′:
168

• Case σ[x] = v′ then σ = {x 7→v′} and we directly get the desired result:
Lv′M
LeM
• Case σ[()] = ():
LeM
≡
LeM
• Case
σ[v] = v′
σ[injl v] = injl v′:
Lv′M
c
c
LvM†
LeM
. . .
...
w
w
w
∅
∅
≡
c
c
LvM†
LeM
...
Lv′M
. . .
≡
LvM†
LeM
...
Lv′M
. . .
w
∅
w
and then by induction hypothesis on σ[v] = v′.
• Case σ[injr v] = injr v′ is similar.
• Case σ[⟨v1, v2⟩] = ⟨v′
1, v′
2⟩
Lv′
1M
Lv′
2M
Lv1M†
Lv2M†
...
...
LeM
...
...
≡
Lv′
1M
...
Lv′
2M
...
Lv1M†
Lv2M†
LeM
...
...
then by induction hypothesis on σ1[v1] = v′
1 and σ2[v2] = v′
2. This holds
due to the fact that the support of σ1 and σ2 are disjoint, as imposed by
the deﬁnition of σ[⟨v1, v2⟩] = ⟨v′
1, v′
2⟩.
As with µMALL, we consider the explicit substitution reduction deﬁned as
in Chapter 4. As with Proof Net and the lambda-calculus with explicit substitution,
two terms that reduce with explicit substitution give rise to exactly the same
diagram. The only special case is the one of the decomposition of a let term into
two:
Lemma 6.9.12. Given a well typed term
ξ1
∆⊢t : A such as t →elet t′ and
ξ2
∆⊢t′ : A
then the interpretation of the two terms are the same: Lξ1M = Lξ2M.
Proof. By induction on →elet:
• u1 = let ⟨x, p⟩= ⟨t1, t2⟩in t3 →elet let x = t1 in let p = t2 in t3 = u2
169

Then
Lt1M
Lt2M
...
...
...
...
Lt3M
∆3
∆1
∆2
...
...
Lt3M
∆3
Lt2M
...
∆2
Lt1M
...
∆1
≡
• All the other case are direct as the translation of both ξ1 and ξ2 gives
exactly the same diagrams.
Lemma 6.9.13 (Evaluation of an iso). Given ⊢ω {v1 ↔e1 | · · · | vn ↔en} :
A ↔B and ∆⊢e v : A and v′ : B such that ω v →σ(ej) with σ[vj] = v then
Lω vM = Lσ(ej)M.
Proof.
A
c
Lξ′
1M
Lξ′
nM
c
Lξ1M†
LξnM†
...
B
...
...
LvM
...∆
≡
c
Lξ′
1M
Lξ′
nM
Lξ1M†
LξnM†
...
B
...
...
LvM
...
LvM
...
∆
≡
c
c
...
LejM
LvjM†
...
LvM
...∆
≡
...∆
Llet σ in ejM
...∆
Lσ(ej)M
≡
Where:
• The ﬁrst equation is due to the naturality of the contraction.
• The second equality is due to Lemma 6.9.10 and by the ﬁrst axiom of Fig-
ure 6.5 for removing the left-over unary contraction and co-contraction
from Lemma 6.9.10.
• The third equation is due to Lemma 6.9.11.
• The fourth equation is due to Property 6.9.3.
.
Theorem 6.9.14 (Soundness). Given two well-typed terms t1, t2, if t1 →t2 then
their interpretation is the same: Lt1M = Lt2M.
Proof. By direct application of Lemma 6.9.13, Lemma 6.9.12 and Property 6.9.4
and Property 6.9.3.
We could also deﬁne an operational equivalence between terms and show ad-
equacy, but this is left as future work.
170

6.10 . Conclusion
We introduced a new graphical language with a token-based semantics, along
with a worlds labelling system and an equational theory.
This language is a ﬁrst step towards the uniﬁcation of languages based on
the tensor ⊗and those based on the biproduct ⊕.
This allows us to reason
about both systems in parallel, and superposition of executions, as shown by the
encoding of the Quantum Switch in Example 6.7.5 and the translation from term
of the language in Section 6.9.
Following this translation, a natural development of the Many-Worlds calculus
consists in accommodating recursion in the language. The question of a complete
equational theory for the language on mixed states (e.g. via the discard construction
[Car+19]) is also left open. With recursion, one would be able to encode the whole
language of [SVV18]. As mentioned in Chapter 4, the language of [SVV18] only
have the type of lists as recursive data-type and is very restrictive in the way
recursion is handled. Extending the language of Chapter 4 to the quantum case
would be more beﬁtting.
Finally, while our language allows for quantum control, it does not entirely
capture another language that aims at formalizing quantum control, namely the
PBS-Calculus [CP20].
How and in which context could we capture the PBS-
Calculus is left for future work.
171


Conclusion
Summary.
In this thesis, we studied the question of reversible and quantum
control along with pure quantum types and its Curry-Howard correspondence. We
gave a ﬁrst linear and reversible programming language with a typing system based
on the logic µMALL. The language is expressive enough to represent any primitive
recursive function and comes with its Curry-Howard correspondence with the logic
µMALL.
We then looked at token-based semantics for quantum graphical languages,
ﬁrst in the case of the ZX-Calculus, which gave us strong intuition on how to
adapt it for a new graphical language featuring both a pairing and branching: the
Many-Worlds Calculus. We showed how the Many-Worlds can be used to represent
pure quantum computation with quantum tests, by encoding both a pure quantum
programming language and the quantum switch.
Future Work.
The iso language and the Many-Worlds are strongly linked by
linear logic. While the iso-language features recursion and inductive types but no
quantum computation, the Many-Worlds-Calculus features quantum computation
but no recursion and inductive types.
A clear extension is then to extend the
language of Chapter 4 to the quantum version, as done in [SVV18], while on
the other hand the Many-Worlds could be extended in order to handle recursive
types and recursion.
Then, the Many-Worlds-Calculus would serve as a direct
denotational model of the language, as shown in Section 6.9. Then could come
the question of coinductive types. In [CDVP21], the authors explore a variant of
the ZX-Calculus with delayed traces, allowing to consider streams of qubits to
ﬂow in the diagram indeﬁnitely, while in [DS19], the authors explore proof nets
for µMALL. Those two approaches could be used for the extension of the Many-
Worlds-Calculus to the case of inductive and coinductive types.
The Many-Worlds Calculus could also serve as a denotational model for other
similar logic or programming language with eﬀects, as [DD22]. A collaboration is
currently planned on this subject.
As the Many-Worlds-Calculus is parametrized by a commutative semiring, it is
not, in essence, necessary quantum. It could therefore also be used to represent
any kind of algebraic branching, be it non-deterministic or probabilistic. On this
note, seeing how the Many-Worlds-Calculus adapt to probabilistic programming
and their denotational semantics could be interesting.
173


Bibliography
[Abb+20]
Alastair A Abbott et al. “Communication through coherent
control of quantum channels”. In: Quantum 4 (2020), p. 333.
[AC04]
Samson Abramsky and Bob Coecke. “A categorical seman-
tics of quantum protocols”. In: Proceedings of the 19th An-
nual IEEE Symposium on Logic in Computer Science, 2004. IEEE.
2004, pp. 415–425.
[AC09]
Samson Abramsky and Bob Coecke. “Categorical quantum
mechanics”. In: Handbook of quantum logic and quantum
structures 2 (2009), pp. 261–325.
[AD06]
Samson Abramsky and Ross Duncan. “A categorical quan-
tum logic”. In: Mathematical Structures in Computer Science
16.3 (2006), pp. 469–489.
[AG05]
Thorsten Altenkirch and Jonathan Grattage. “A Functional
Quantum Programming Language”. In: Proceedings of the
20th Symposium on Logic in Computer Science, LICS’05 (Chicago,
Illinois, US.). Ed. by Prakash Panangaden. IEEE. IEEE Com-
puter Society Press, 2005, pp. 249–258. doi: 10.1109/LICS.
2005.1.
[AG11a]
Holger Bock Axelsen and Robert Glück. “A simple and eﬃ-
cient universal reversible Turing machine”. In: International
Conference on Language and Automata Theory and Applica-
tions. Springer. 2011, pp. 117–128.
[AG11b]
Holger Bock Axelsen and Robert Glück. “What do reversible
programs compute?” In: International Conference on Foun-
dations of Software Science and Computational Structures. Springer.
2011, pp. 42–56.
[AK16]
Holger Bock Axelsen and Robin Kaarsgaard. “Join Inverse
Categories as Models of Reversible Recursion”. In: Proceed-
ings of the 19th International Conference on Foundations of
Software Science and Computation Structures (FOSSACS’16).
Ed. by Bart Jacobs and Christof Löding. Vol. 9634. Lecture
Notes in Computer Science. Eindhoven, The Netherlands:
Springer, 2016, pp. 73–90. doi: 10 . 1007 / 978 - 3 - 662 -
49630-5\_5.
175

[AL95]
Andrea Asperti and Cosimo Laneve. “Paths, computations
and labels in the λ-calculus”. In: Theoretical Computer Sci-
ence 142.2 (1995), pp. 277–297.
[Ama+20]
Bogdan Aman et al. “Foundations of Reversible Computa-
tion”. In: Reversible Computation: Extending Horizons of Com-
puting - Selected Results of the COST Action IC1405. Ed. by
Irek Ulidowski et al. Vol. 12070. Lecture Notes in Computer
Science. Springer, 2020, pp. 1–40. isbn: 978-3-030-47360-0.
doi: 10.1007/978-3-030-47361-7\_1.
[Amy18]
Matthew Amy. “Towards Large-scale Functional Veriﬁcation
of Universal Quantum Circuits”. In: Proceedings 15th Inter-
national Conference on Quantum Physics and Logic, QPL 2018,
Halifax, Canada, 3-7th June 2018. Ed. by Peter Selinger and
Giulio Chiribella. Vol. 287. EPTCS. 2018, pp. 1–21. url: https:
//doi.org/10.4204/EPTCS.287.1.
[Bac14]
Miriam Backens. “The ZX-Calculus is Complete for Stabilizer
Quantum Mechanics”. In: New Journal of Physics 16.9 (2014),
p. 093021. doi: 10.1088/1367-2630/16/9/093021.
[Bac+20]
Miriam Backens et al. There and back again: A circuit extrac-
tion tale. 2020. arXiv: 2003.01664 [quant-ph].
[Bae08]
David Baelde. “A Linear Approach to the Proof-Theory of
Least and Greatest Fixed Points”. Theses. École Polytech-
nique, 2008.
[Bae12]
David Baelde. “Least and greatest ﬁxed points in linear logic”.
In: ACM Transactions on Computational Logic (TOCL) 13.1 (2012),
pp. 1–44.
[Bae+22]
David Baelde et al. “Bouncing Threads for Circular and Non-
Wellfounded Proofs: Towards Compositionality with Circu-
lar Proofs”. In: Proceedings of the 37th Annual ACM/IEEE Sym-
posium on Logic in Computer Science. New York, NY, USA: As-
sociation for Computing Machinery, 2022. isbn: 9781450393515.
url: https://doi.org/10.1145/3531130.3533375.
[Bar84]
Henk Barendregt. “’The lambda calculus: its syntax and se-
mantics’”. In: Studies in logic and the foundations of Mathe-
matics (1984).
[BC13]
Yves Bertot and Pierre Castéran. Interactive theorem proving
and program development: Coq’Art: the calculus of inductive
constructions. Springer Science & Business Media, 2013.
176

[BDS16]
David Baelde, Amina Doumane, and Alexis Saurin. “Inﬁni-
tary Proof Theory: the Multiplicative Additive Case”. In: 25th
EACSL Annual Conference on Computer Science Logic (CSL 2016).
Ed. by Jean-Marc Talbot and Laurent Regnier. Vol. 62. Leib-
niz International Proceedings in Informatics (LIPIcs). Dagstuhl,
Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik,
2016, 42:1–42:17. isbn: 978-3-95977-022-4. url: http : / /
drops.dagstuhl.de/opus/volltexte/2016/6582.
[Bea+19]
Niel de Beaudrap et al. “Pauli Fusion: a computational model
to realise quantum transformations from ZX terms”. In: QPL’19
: International Conference on Quantum Physics and Logic. 12
pages + appendices. Los Angeles, United States, June 2019.
url: https://hal.archives-ouvertes.fr/hal-02413388.
[Ben00]
Charles H. Bennett. “Notes on the history of reversible com-
putation”. In: IBM Journal of Research and Development 44.1
(2000), pp. 270–278. doi: 10.1147/rd.441.0270.
[Ben73]
Charles H Bennett. “Logical reversibility of computation”.
In: IBM journal of Research and Development 17.6 (1973), pp. 525–
532.
[Bér+12]
Antoine Bérut et al. “Experimental veriﬁcation of Landauer’s
principle linking information and thermodynamics”. In: Na-
ture 483.7388 (2012), pp. 187–189.
[BH20]
Niel de Beaudrap and Dominic Horsman. “The ZX calculus
is a language for surface code lattice surgery”. In: Quantum
4 (Jan. 2020), p. 218. issn: 2521-327X. url: https://doi.
org/10.22331/q-2020-01-09-218.
[BK19]
Miriam Backens and Aleks Kissinger. “ZH: A Complete Graph-
ical Calculus for Quantum Computations Involving Classi-
cal Non-linearity”. In: Proceedings of the 15th International
Conference on Quantum Physics and Logic, Halifax, Canada,
3-7th June 2018. Ed. by Peter Selinger and Giulio Chiribella.
Vol. 287. Electronic Proceedings in Theoretical Computer
Science. 2019, pp. 23–42. url: https://doi.org/10.4204/
EPTCS.287.2.
[Car12]
Michael Kirkedal Carøe. “Design of Reversible Computing
Systems”. PhD thesis. University of Copenhagen, Denmark,
2012.
177

[Car+19]
Titouan Carette et al. “Completeness of Graphical Languages
for Mixed States Quantum Mechanics”. In: 46th International
Colloquium on Automata, Languages, and Programming (ICALP
2019). Ed. by Christel Baier et al. Vol. 132. Leibniz Inter-
national Proceedings in Informatics (LIPIcs). Dagstuhl, Ger-
many: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik,
2019, 108:1–108:15. isbn: 978-3-95977-109-2. url: http://
drops.dagstuhl.de/opus/volltexte/2019/10684.
[CD11]
Bob Coecke and Ross Duncan. “Interacting quantum ob-
servables: categorical algebra and diagrammatics”. In: New
Journal of Physics 13.4 (2011), p. 043016.
[CDH20]
Cole Comfort, Antonin Delpeuch, and Jules Hedges. Sheet
diagrams for bimonoidal categories. 2020. url: https://arxiv.
org/abs/2010.13361.
[CDP08]
G. Chiribella, G. M. D’Ariano, and P. Perinotti. “Transform-
ing Quantum Operations: Quantum Supermaps”. In: EPL
(Europhysics Letters) 83.3 (2008), p. 30004. doi: 10.1209/
0295-5075/83/30004.
[CDVP21]
Titouan Carette, Marc De Visme, and Simon Perdrix. “Graph-
ical language with delayed trace: Picturing quantum com-
puting with ﬁnite memory”. In: 2021 36th Annual ACM/IEEE
Symposium on Logic in Computer Science (LICS). IEEE. 2021,
pp. 1–13.
[Cha+]
Christophe Chareton et al. “A Deductive Veriﬁcation Frame-
work for Circuit-building Quantum Programs”. arXiv:2003.05841.
To appear in Proceedings of ESOP’21.
[Cha+22]
Kostia Chardonnet et al. “The Many-Worlds Calculus”. work-
ing paper or preprint. Aug. 2022. url: https://hal.archives-
ouvertes.fr/hal-03654190.
[Chi+13]
Giulio Chiribella et al. “Quantum computations without def-
inite causal structure”. In: Physical Review A 88.2 (2013), p. 022318.
[CHP19]
Titouan Carette, Dominic Horsman, and Simon Perdrix. “SZX-
Calculus: Scalable Graphical Quantum Reasoning”. In: 44th
International Symposium on Mathematical Foundations of Com-
puter Science (MFCS 2019). Ed. by Peter Rossmanith, Pinar
Heggernes, and Joost-Pieter Katoen. Vol. 138. Leibniz Inter-
national Proceedings in Informatics (LIPIcs). Dagstuhl, Ger-
many: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik,
2019, 55:1–55:15. isbn: 978-3-95977-117-7. url: http : / /
drops.dagstuhl.de/opus/volltexte/2019/10999.
178

[Chu36]
Alonzo Church. “An unsolvable problem of elementary num-
ber theory”. In: American journal of mathematics 58.2 (1936),
pp. 345–363.
[Chu40]
Alonzo Church. “A formulation of the simple theory of types”.
In: The journal of symbolic logic 5.2 (1940), pp. 56–68.
[Chu41]
Alonzo Church. The calculi of lambda-conversion. 6. Prince-
ton University Press, 1941.
[CK17]
Bob Coecke and Aleks Kissinger. Picturing Quantum Processes:
A First Course in Quantum Theory and Diagrammatic Reason-
ing. Cambridge University Press, 2017. isbn: 9781316219317.
doi: 10.1017/9781316219317.
[CL02]
J. Robin B. Cockett and Stephen Lack. “Restriction Categories
I: Categories of Partial Maps”. In: Theoretical Computer Sci-
ence 270.1 (2002), pp. 223–259. doi: 10.1016/S0304-3975(00)
00382-0.
[CL03]
J. Robin B. Cockett and Stephen Lack. “Restriction categories
II: partial map classiﬁcation”. In: Theoretical Computer Sci-
ence 294.1 (2003), pp. 61–102. doi: 10.1016/S0304-3975(01)
00245-6.
[CL07]
Robin Cockett and Stephen Lack. “Restriction Categories
III: Colimits, Partial Limits and Extensivity”. In: Mathemati-
cal Structures in Computer Science 17.4 (2007), pp. 775–817.
doi: 10.1017/S0960129507006056.
[Clé+22a]
Alexandre Clément et al. “A Complete Equational Theory
for Quantum Circuits”. In: arXiv preprint arXiv:2206.10577
(2022).
[Clé+22b]
Alexandre Clément et al. LOv-Calculus: A Graphical Language
for Linear Optical Quantum Circuits. 2022. url: https : / /
arxiv.org/abs/2204.11787.
[CLV21]
Kostia Chardonnet, Louis Lemonnier, and Benoît Valiron.
“Categorical Semantics of Reversible Pattern-Matching”. In:
Proceedings 37th Conference on Mathematical Foundations of
Programming Semantics, Hybrid: Salzburg, Austria and On-
line, 30th August - 2nd September, 2021. Ed. by Ana Sokolova.
Vol. 351. Electronic Proceedings in Theoretical Computer
Science. Open Publishing Association, 2021, pp. 18–33. doi:
10.4204/EPTCS.351.2.
179

[CP12]
Bob Coecke and Simon Perdrix. “Environment and Classical
Channels in Categorical Quantum Mechanics”. In: Logical
Methods in Computer Science Volume 8, Issue 4 (2012). url:
https://lmcs.episciences.org/719.
[CP20]
Alexandre Clément and Simon Perdrix. “PBS-calculus: A graph-
ical language for coherent control of quantum computa-
tions”. In: arXiv preprint arXiv:2002.09387 (2020).
[CSV20]
Kostia Chardonnet, Alexis Saurin, and Benoît Valiron. “To-
ward a Curry-Howard Equivalence for Linear, Reversible Com-
putation - Work-in-Progress”. In: Proceedings of the 12th In-
ternational Conference on Reversible Computation (RC 2020).
Ed. by Ivan Lanese and Mariusz Rawski. Vol. 12227. Lecture
Notes in Computer Science. Springer, 2020, pp. 144–152.
isbn: 978-3-030-52481-4. doi: 10.1007/978-3-030-52482-
1\_8.
[Cur34]
Haskell B Curry. “Functionality in combinatory logic”. In: Pro-
ceedings of the National Academy of Sciences 20.11 (1934),
pp. 584–590.
[CVV21]
Kostia Chardonnet, Benoît Valiron, and Renaud Vilmart. “Ge-
ometry of Interaction for ZX-Diagrams”. In: Proceedings of
the 46th International Symposium on Mathematical Founda-
tions of Computer Science, MFCS 2021 (Tallinn, Estonia). Ed.
by Filippo Bonchi and Simon J. Puglisi. Vol. 202. LIPIcs. Schloss
Dagstuhl - Leibniz-Zentrum fuer Informatik, 2021, 30:1–30:16.
isbn: 978-3-95977-201-3. doi: 10.4230/LIPIcs.MFCS.2021.
30.
[Dal17]
Dal Lago, Ugo and Faggian, Claudia and Valiron, Benoît and
Yoshimizu, Akira. “The Geometry of Parallelism: Classical,
Probabilistic, and Quantum Eﬀects”. In: Proceedings of the
44th ACM SIGPLAN Symposium on Principles of Programming
Languages. POPL 2017. New York, NY, USA: Association for
Computing Machinery, 2017, 833–845. isbn: 9781450346603.
url: https://doi.org/10.1145/3009837.3009859.
[DCM22]
Alejandro Díaz-Caro and Octavio Malherbe. Semimodules
and the (syntactically-)linear lambda calculus. 2022. doi: 10.
48550/ARXIV.2205.02142. url: https://arxiv.org/abs/
2205.02142.
[DD22]
Alejandro Díaz-Caro and Gilles Dowek. “Linear lambda-calculus
is linear”. In: CoRR abs/2201.11221 (2022). arXiv: 2201.11221.
url: https://arxiv.org/abs/2201.11221.
180

[DG18]
Ross Duncan and Liam Garvie. “Verifying the Smallest In-
teresting Colour Code with Quantomatic”. In: Proceedings
14th International Conference on Quantum Physics and Logic,
Nijmegen, The Netherlands, 3-7 July 2017. Ed. by Bob Coecke
and Aleks Kissinger. Vol. 266. Electronic Proceedings in The-
oretical Computer Science. 2018, pp. 147–163. url: https:
//doi.org/10.4204/EPTCS.266.10.
[DL14]
Ross Duncan and Maxime Lucas. “Verifying the Steane code
with Quantomatic”. In: Proceedings of the 10th International
Workshop on Quantum Physics and Logic, Castelldefels (Barcelona),
Spain, 17th to 19th July 2013. Ed. by Bob Coecke and Matty
Hoban. Vol. 171. Electronic Proceedings in Theoretical Com-
puter Science. Open Publishing Association, 2014, pp. 33–
49. doi: 10.4204/EPTCS.171.4.
[DLF11]
Ugo Dal Lago and Claudia Faggian. “On multiplicative lin-
ear logic, modality and quantum circuits”. In: QPL 95 (2011),
pp. 55–66.
[Dou17]
Amina Doumane. “On the inﬁnitary proof theory of logics
with ﬁxed points”. Theses. Université Sorbonne Paris Cité,
June 2017. url: https://hal.archives-ouvertes.fr/tel-
01676953.
[DP10]
Ross Duncan and Simon Perdrix. “Rewriting Measurement-
Based Quantum Computations with Generalised Flow”. In:
Lecture Notes in Computer Science 6199 (2010), pp. 285–296.
doi: 10.1007/978-3-642-14162-1_24.
[DR79]
Willem-Paul De Roever. “Recursive program schemes: se-
mantics and proof theory”. In: Journal of Symbolic Logic 44.4
(1979).
[DR99]
Vincent Danos and Laurent Regnier. “Reversible, irreversible
and optimal λ-machines”. In: Theoretical Computer Science
227.1-2 (1999), pp. 79–97.
[DS19]
Abhishek De and Alexis Saurin. “Inﬁnets: The parallel syn-
tax for non-wellfounded proof-theory”. In: TABLEAUX 2019 -
28th International Conference on Automated Reasoning with
Analytic Tableaux and Related Methods. TABLEAUX 2019 -
28th International Conference on Automated Reasoning with
Analytic Tableaux and Related Methods. London, United
Kingdom, Sept. 2019. url: https://hal.archives-ouvertes.
fr/hal-02337286.
181

[Dun04]
Ross Duncan. Believe it or not, Bell states are a model of mul-
tiplicative linear logic. Tech. rep. 2004.
[Dun06]
Ross Duncan. “Types for quantum computing”. In: (2006).
[Dun09]
Ross Duncan. “Generalized Proof-Nets for Compact Cate-
gories with Biproducts”. In: Semantic Techniques in Quan-
tum Computation. Ed. by Simon Gay and Ian Mackie. Cam-
bridge University Press, 2009. Chap. 3, pp. 70–134. isbn:
978-0-521-51374-6.
[Dun+20]
Ross Duncan et al. “Graph-theoretic Simpliﬁcation of Quan-
tum Circuits with the ZX-calculus”. In: Quantum 4 (2020),
p. 279.
[FBW18]
Mark Fingerhuth, Tomàš Babej, and Peter Wittek. “Open
source software in quantum computing”. In: PLOS ONE 13.12
(Dec. 2018), pp. 1–28. url: https://doi.org/10.1371/
journal.pone.0208561.
[FC22]
Giovanni de Felice and Bob Coecke. Quantum Linear Optics
via String Diagrams. 2022. url: https://arxiv.org/abs/
2204.12985.
[FH65]
Richard P. Feynman and A. R. Hibbs. Quantum Mechanics
and Path Integrals. McGraw-Hill Publishing Company, 1965.
isbn: 0-07-020650-3.
[FS13]
Jérôme Fortier and Luigi Santocanale. “Cuts for circular proofs:
semantics and cut-elimination”. In: Computer Science Logic
2013 (CSL 2013). Ed. by Simona Ronchi Della Rocca. Vol. 23.
Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl,
Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik,
2013, pp. 248–262. isbn: 978-3-939897-60-6. url: http://
drops.dagstuhl.de/opus/volltexte/2013/4201.
[FT82]
Edward Fredkin and Tommaso Toﬀoli. “Conservative logic”.
In: International Journal of theoretical physics 21.3 (1982), pp. 219–
253.
[Gab+13]
Marco Gaboardi et al. “Linear dependent types for diﬀeren-
tial privacy”. In: Proceedings of the 40th annual ACM SIGPLAN-
SIGACT symposium on Principles of programming languages.
2013, pp. 357–370.
[Gen35]
Gerhard Gentzen. “Untersuchungen über das logische schließen.
I.” In: Mathematische zeitschrift 35 (1935).
[Gir06]
Jean-Yves Girard. “Geometry of interaction IV: the feedback
equation”. In: Logic Colloquium. Vol. 3. 2006, pp. 76–117.
182

[Gir11]
Jean-Yves Girard. “Geometry of interaction V: logic in the
hyperﬁnite factor”. In: Theoretical Computer Science 412.20
(2011), pp. 1860–1883.
[Gir13]
Jean-Yves Girard. “Geometry of interaction VI: a blueprint
for transcendental syntax”. In: preprint (2013).
[Gir87]
Jean-Yves Girard. “Linear logic”. In: Theoretical computer sci-
ence 50.1 (1987), pp. 1–101.
[Gir88]
Jean-Yves Girard. “Geometry of interaction II: deadlock-free
algorithms”. In: International Conference on Computer Logic.
Springer. 1988, pp. 76–93.
[Gir89a]
Jean-Yves Girard. “Geometry of interaction I: interpretation
of System F”. In: Studies in Logic and the Foundations of Math-
ematics. Vol. 127. Elsevier, 1989, pp. 221–260.
[Gir89b]
Jean-Yves Girard. “Towards a geometry of interaction”. In:
Contemporary Mathematics 92.69-108 (1989), p. 6.
[Gir95]
Jean-Yves Girard. “Geometry of interaction III: accommo-
dating the additives”. In: London Mathematical Society Lec-
ture Note Series (1995), pp. 329–389.
[Gir96]
Jean-Yves Girard. “Proof-nets: the parallel syntax for proof-
theory”. In: Lecture Notes in Pure and Applied Mathematics
(1996), pp. 97–124.
[GK18]
Robert Glück and Robin Kaarsgaard. “A categorical founda-
tion for structured reversible ﬂowchart languages: Sound-
ness and adequacy”. In: Log. Methods Comput. Sci. 14.3 (2018).
doi: 10.23638/LMCS-14(3:16)2018.
[GKY19]
Robert Glück, Robin Kaarsgaard, and Tetsuo Yokoyama. “Re-
versible Programs Have Reversible Semantics”. In: Formal
Methods. FM 2019 International Workshops - Porto, Portugal,
October 7-11, 2019, Revised Selected Papers, Part II. Ed. by
Emil Sekerinski et al. Vol. 12233. Lecture Notes in Com-
puter Science. Springer, 2019, pp. 413–427. isbn: 978-3-030-
54996-1. doi: 10.1007/978-3-030-54997-8\_26.
[Gre+13]
Alexander S. Green et al. “Quipper: A Scalable Quantum
Programming Language”. In: Proceedings of the ACM SIG-
PLAN Conference on Programming Language Design and Im-
plementation, PLDI’13 (Seattle, WA, USA). Ed. by Hans-Juergen
Boehm and Cormac Flanagan. ACM, 2013, pp. 333–342. isbn:
978-1-4503-2014-6. doi: 10.1145/2491956.2462177.
183

[Gri89]
Timothy G Griﬃn. “A formulae-as-type notion of control”.
In: Proceedings of the 17th ACM SIGPLAN-SIGACT symposium
on Principles of programming languages. 1989, pp. 47–58.
[Gro96]
Lov K Grover. “A fast quantum mechanical algorithm for
database search”. In: Proceedings of the twenty-eighth an-
nual ACM symposium on Theory of computing. 1996, pp. 212–
219.
[Had15]
Amar Hadzihasanovic. “A Diagrammatic Axiomatisation for
Qubit Entanglement”. In: 2015 30th Annual ACM/IEEE Sympo-
sium on Logic in Computer Science. 2015, pp. 573–584. url:
http://dx.doi.org/10.1109/LICS.2015.59.
[HH16]
Ichiro Hasuo and Naohiko Hoshino. “Semantics of Higher-
Order Quantum Computation via Geometry of Interaction”.
In: CoRR abs/1605.05079 (2016). arXiv: 1605.05079. url: http:
//arxiv.org/abs/1605.05079.
[Hil11]
Anne Hillebrand. “Quantum Protocols involving Multipar-
ticle Entanglement and their Representations”. MA thesis.
University of Oxford, 2011. url: https://www.cs.ox.ac.
uk/people/bob.coecke/Anne.pdf.
[HK15]
Chris Heunen and Martti Karvonen. “Reversible Monadic
Computing”. In: Proceedings of the 31st Conference on the
Mathematical Foundations of Programming Semantics (MFPS
XXXI). Ed. by Dan Ghica. Vol. 319. Electronic Notes in Theo-
retical Computer Science. Nijmegen, The Netherlands, 2015,
pp. 217–237. doi: 10.1016/j.entcs.2015.12.014.
[HKK18]
Chris Heunen, Robin Kaarsgaard, and Martti Karvonen. “Re-
versible Eﬀects as Inverse Arrows”. In: Proceedings of the
34th Conference on the Mathematical Foundations of Program-
ming Semantics (MFPS XXXIV). Ed. by Sam Staton. Vol. 341.
Electronic Notes in Theoretical Computer Science. Dalhousie
University, Halifax, Canada: Elsevier, 2018, pp. 179–199. doi:
10.1016/j.entcs.2018.11.009.
[HNW18]
Amar Hadzihasanovic, Kang Feng Ng, and Quanlong Wang.
“Two Complete Axiomatisations of Pure-state Qubit Quan-
tum Computing”. In: Proceedings of the 33rd Annual ACM/IEEE
Symposium on Logic in Computer Science. LICS ’18. Oxford,
United Kingdom: ACM, 2018, pp. 502–511. isbn: 978-1-4503-
5583-4. url: http://doi.acm.org/10.1145/3209108.
3209128.
184

[How80]
William A Howard. “The formulae-as-types notion of con-
struction”. In: To HB Curry: essays on combinatory logic, lambda
calculus and formalism 44 (1980), pp. 479–490.
[HV19]
Chris Heunen and Jamie Vicary. Categories for Quantum The-
ory. Oxford University Press, 2019. url: https://doi.org/
10.1093%2Foso%2F9780198739623.001.0001.
[HVG03]
D. Hughes and R. Van Glabbeek. “Proof nets for unit-free
multiplicative-additive linear logic (extended abstract)”. In:
18th Annual IEEE Symposium of Logic in Computer Science,
2003. Proceedings. Ottawa, Ont., Canada: IEEE Comput. Soc,
2003. isbn: 978-0-7695-1884-8. doi: 10.1109/LICS.2003.
1210039. url: http://ieeexplore.ieee.org/document/
1210039/ (visited on 09/29/2022).
[JKT18]
Petur Andrias Højgaard Jacobsen, Robin Kaarsgaard, and
Michael Kirkedal Thomsen. “CoreFun : A Typed Functional
Reversible Core Language”. In: Reversible Computation - 10th
International Conference, RC 2018, Leicester, UK, September
12-14, 2018, Proceedings. Ed. by Jarkko Kari and Irek Ulid-
owski. Vol. 11106. Lecture Notes in Computer Science. Springer,
2018, pp. 304–321. doi: 10.1007/978-3-319-99498-7\_21.
[JPV18a]
Emmanuel Jeandel, Simon Perdrix, and Renaud Vilmart. “A
Complete Axiomatisation of the ZX-Calculus for Cliﬀord+T
Quantum Mechanics”. In: Proceedings of the 33rd Annual ACM/IEEE
Symposium on Logic in Computer Science. LICS ’18. Oxford,
United Kingdom: ACM, 2018, pp. 559–568. isbn: 978-1-4503-
5583-4. url: https://doi.acm.org/10.1145/3209108.
3209131.
[JPV18b]
Emmanuel Jeandel, Simon Perdrix, and Renaud Vilmart. “Di-
agrammatic Reasoning Beyond Cliﬀord+T Quantum Mechan-
ics”. In: Proceedings of the 33rd Annual ACM/IEEE Symposium
on Logic in Computer Science. LICS ’18. Oxford, United King-
dom: ACM, 2018, pp. 569–578. isbn: 978-1-4503-5583-4. url:
http://doi.acm.org/10.1145/3209108.3209139.
[JPV19]
Emmanuel Jeandel, Simon Perdrix, and Renaud Vilmart. “A
Generic Normal Form for ZX-Diagrams and Application to
the Rational Angle Completeness”. In: 2019 34th Annual ACM/IEEE
Symposium on Logic in Computer Science (LICS). 2019, pp. 1–
10. doi: 10.1109/LICS.2019.8785754.
185

[JS12]
Roshan P. James and Amr Sabry. “Information eﬀects”. In:
Proceedings of the 39th ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages (POPL’12). Ed. by John
Field and Michael Hicks. Philadelphia, Pennsylvania, USA:
ACM, 2012, pp. 73–84. isbn: 978-1-4503-1083-3. doi: 10 .
1145/2103656.2103667.
[JS14]
Rosham P. James and Amr Sabry. “Theseus: A High-Level
Language for Reversible Computing”. Draft, available at https:
//legacy.cs.indiana.edu/~sabry/papers/theseus.pdf.
2014.
[JS91]
André Joyal and Ross Street. “The geometry of tensor cal-
culus, I”. In: Advances in mathematics 88.1 (1991), pp. 55–
112.
[Jun+17]
Ralf Jung et al. “RustBelt: Securing the foundations of the
Rust programming language”. In: Proceedings of the ACM on
Programming Languages 2.POPL (2017), pp. 1–34.
[Kaa19a]
Robin Kaarsgaard. “Condition/Decision Duality and the In-
ternal Logic of Extensive Restriction Categories”. In: Pro-
ceedings of the 35th Conference on the Mathematical Founda-
tions of Programming Semantics (MFPS XXXV). Ed. by Barbara
König. Vol. 347. Electronic Notes in Theoretical Computer
Science. London, UK, 2019, pp. 179–202. doi: 10.1016/j.
entcs.2019.09.010.
[Kaa19b]
Robin Kaarsgaard. “Inversion, Iteration, and the Art of Dual
Wielding”. In: Proceedings of the 11th International Confer-
ence on Reversible Computation (RC 2019). Ed. by Michael
Kirkedal Thomsen and Mathias Soeken. Vol. 11497. Lec-
ture Notes in Computer Science. Lausanne, Switzerland:
Springer, 2019, pp. 34–50. isbn: 978-3-030-21499-9. doi: 10.
1007/978-3-030-21500-2\_3.
[KAG17]
Robin Kaarsgaard, Holger Bock Axelsen, and Robert Glück.
“Join inverse categories and reversible recursion”. In: Jour-
nal of Logical and Algebraic Methods in Programming 87 (2017),
pp. 33–50. issn: 2352-2208. doi: 10.1016/j.jlamp.2016.
08.003.
[Kas79]
J. Kastl. “Inverse Categories”. In: Algebraische Modelle, Kate-
gorien und Gruppoide. Studien zur Algebra und ihre Anwen-
dungen, Band 7. Berlin, Akademie-Verlag, 1979, pp. 51–60.
[Koz83]
Dexter Kozen. “Results on the propositional µ-calculus”. In:
Theoretical computer science 27.3 (1983), pp. 333–354.
186

[KR21]
Robin Kaarsgaard and Mathys Rennela. Join inverse rig cat-
egories for reversible functional programming, and beyond.
Draft, available at arXiv:2105.09929. 2021.
[KV19]
Robin Kaarsgaard and Niccolò Veltri. “En Garde! Unguarded
Iteration for Reversible Computation in the Delay Monad”.
In: Proceedings of the 13th International Conference on Math-
ematics of Program Construction (MPC 2019). Ed. by Graham
Hutton. Vol. 11825. Lecture Notes in Computer Science.
Porto, Portugal: Springer Verlag, 2019, pp. 366–384. isbn:
978-3-030-33635-6. doi: 10.1007/978-3-030-33636-3\
_13.
[Lac04]
Stephen Lack. “Composing PROPs”. In: Theory and Applica-
tions of Categories. Vol. 13. 9. 2004, pp. 147–163. url: http:
//www.tac.mta.ca/tac/volumes/13/9/13-09abs.html.
[Lan61]
Rolf Landauer. “Irreversibility and Heat Generation in the
Computing Process”. In: IBM Journal of Research and Devel-
opment. 5.3 (1961), pp. 183–191. doi: 10.1147/rd.53.0183.
[Lau11]
Olivier Laurent. “Théorie de la démonstration”. In: Cours du
Master MPRI (2011). url: https://perso.ens- lyon.fr/
olivier.laurent/thdem11.pdf.
[Lee+21]
Dongho Lee et al. “Concrete categorical model of a quan-
tum circuit description language with measurement”. In:
arXiv preprint arXiv:2110.02691 (2021).
[Ler09]
Xavier Leroy. “Formal veriﬁcation of a realistic compiler”.
In: Communications of the ACM 52.7 (2009), pp. 107–115.
[Lut86]
Christopher Lutz. “Janus: a time-reversible language”. Let-
ter to Rolf Landauer, posted online by Tetsuo Yokoyama
on http://www.tetsuo.jp/ref/janus.html. 1986.
[LWK20]
Louis Lemonnier, John van de Wetering, and Aleks Kissinger.
Hypergraph simpliﬁcation: Linking the path-sum approach to
the ZH-calculus. arXiv:2003.13564. 2020. arXiv: 2003.13564
[quant-ph].
[LZ15]
Ugo Dal Lago and Margherita Zorzi. “Wave-style token ma-
chines and quantum lambda calculi”. In: arXiv preprint arXiv:1502.04774
(2015).
[Mai+19]
Kenji Maillard et al. “The next 700 relational program log-
ics”. In: Proceedings of the ACM on Programming Languages
4.POPL (2019), pp. 1–33.
187

[Mel09]
Paul-André Mellies. “Categorical semantics of linear logic”.
In: Panoramas et syntheses 27 (2009), pp. 15–215.
[Mel14]
Paul-André Mellies. “Local states in string diagrams”. In: Rewrit-
ing and Typed Lambda Calculi. Springer, 2014, pp. 334–348.
[MY07]
Kenichi Morita and Yoshikazu Yamaguchi. “A universal re-
versible Turing machine”. In: International Conference on Ma-
chines, Computations, and Universality. Springer. 2007, pp. 90–
98.
[NC02]
Michael A. Nielsen and Isaac L. Chuang. Quantum Computa-
tion and Quantum Information. Cambridge University Press,
2002.
[New42]
Maxwell Herman Alexander Newman. “On theories with
a combinatorial deﬁnition of" equivalence"”. In: Annals of
mathematics (1942), pp. 223–243.
[Nor07]
Ulf Norell. “Towards a practical programming language based
on dependent type theory”. PhD thesis. SE-412 96 Göte-
borg, Sweden: Department of Computer Science and Engi-
neering, Chalmers University of Technology, 2007.
[NST18]
Rémi Nollet, Alexis Saurin, and Christine Tasson. “Local va-
lidity for circular proofs in linear logic with ﬁxed points”. In:
27th EACSL Annual Conference on Computer Science Logic (CSL
2018). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
2018.
[OSG08]
Bryan O’Sullivan, Don Stewart, and John Goerzen. “Real World
Haskell”. In: Available online http://book.realworldhaskell.org/
(2008).
[Par69]
David Park. “Fixpoint induction and proofs of program prop-
erties”. In: Machine intelligence 5 (1969).
[Por+17]
Christopher Portmann et al. “Causal Boxes: Quantum Information-
Processing Systems Closed Under Composition”. In: IEEE
Transactions on Information Theory 63.5 (2017), pp. 3277–
3305. doi: 10.1109/TIT.2017.2676805.
[PPR20]
Luca Paolini, Mauro Piccolo, and Luca Roversi. “A class of
recursive permutations which is primitive recursive com-
plete”. In: Theoretical Computer Science 813 (2020), pp. 218–
233.
188

[PRZ17]
Jennifer Paykin, Robert Rand, and Steve Zdancewic. “QWIRE:
a core language for quantum circuits”. In: Proceedings of
the 44th ACM SIGPLAN Symposium on Principles of Program-
ming Languages, POPL’17 (Paris, France). Ed. by Giuseppe
Castagna and Andrew D. Gordon. ACM, 2017, pp. 846–858.
isbn: 978-1-4503-4660-3. doi: 10.1145/3009837.3009894.
[PSV14]
Michele Pagani, Peter Selinger, and Benoît Valiron. “Apply-
ing quantitative semantics to higher-order quantum com-
puting”. In: Proceedings of the 41st ACM SIGPLAN-SIGACT Sym-
posium on Principles of Programming Languages, POPL’14 (San
Diego, California, USA). Ed. by Suresh Jagannathan and Pe-
ter Sewell. ACM, 2014, pp. 647–658. isbn: 978-1-4503-2544-
8. doi: 10.1145/2535838.2535879.
[Rey02]
John C Reynolds. “Separation logic: A logic for shared mu-
table data structures”. In: Proceedings 17th Annual IEEE Sym-
posium on Logic in Computer Science. IEEE. 2002, pp. 55–74.
[RJ87]
Hartley Rogers Jr. Theory of recursive functions and eﬀective
computability. MIT press, 1987.
[RS17]
Francisco Rios and Peter Selinger. “A categorical model for
a quantum circuit description language”. In: arXiv preprint
arXiv:1706.02630 (2017).
[San02]
Luigi Santocanale. “A calculus of circular proofs and its cat-
egorical semantics”. In: FOSSACS 2002. Vol. 2303. FOSSACS
2002, proceedings of the 5th International Conference on
Foundations of Software Science and Computation Struc-
tures. Grenoble, France: Springer, Apr. 2002, pp. 357–371.
url: https://hal.archives-ouvertes.fr/hal-01261170.
[Sel07]
Peter Selinger. “Dagger compact closed categories and com-
pletely positive maps”. In: Electronic Notes in Theoretical com-
puter science 170 (2007), pp. 139–163.
[Sel10]
Peter Selinger. “A survey of graphical languages for monoidal
categories”. In: New structures for physics. Springer, 2010,
pp. 289–355.
[Sho99]
Peter W Shor. “Polynomial-time algorithms for prime fac-
torization and discrete logarithms on a quantum computer”.
In: SIAM review 41.2 (1999), pp. 303–332.
[Sin+22]
Kartik Singhal et al. “Q# as a Quantum Algorithmic Lan-
guage”. In: arXiv preprint arXiv:2206.03532 (2022).
189

[SM13]
Mehdi Saeedi and Igor L. Markov. “Synthesis and Optimiza-
tion of Reversible Circuits - a Survey”. In: ACM Computing
Surveys 45.2 (2013), 21:1–21:34. doi: 10 . 1145 / 2431211 .
2431220.
[Sta15]
Sam Staton. “Algebraic Eﬀects, Linearity, and Quantum Pro-
gramming Languages”. In: Proceedings of the 42nd Annual
ACM SIGPLAN-SIGACT Symposium on Principles of Program-
ming Languages, POPL’15. Ed. by Sriram K. Rajamani and
David Walker. ACM, 2015, pp. 395–406. doi: 10.1145/2676726.
2676999.
[SU06]
Morten Heine Sørensen and Pawel Urzyczyn. Lectures on
the Curry-Howard isomorphism. Elsevier, 2006.
[SV06]
Peter Selinger and Benoît Valiron. “A lambda calculus for
quantum computation with classical control”. In: Mathemat-
ical Structures in Computer Science 16.3 (2006), pp. 527–552.
[SV+09]
Peter Selinger, Benoıt Valiron, et al. “Quantum lambda cal-
culus”. In: Semantic techniques in quantum computation (2009),
pp. 135–172.
[SVV18]
Amr Sabry, Benoit Valiron, and Juliana Kaizer Vizzotto. “From
Symmetric Pattern-Matching to Quantum Control”. In: Pro-
ceedings of the 21st International Conference on Foundations
of Software Science and Computation Structures (FOSSACS’18).
Ed. by Christel Baier and Ugo Dal Lago. Vol. 10803. Lecture
Notes in Computer Science. Thessaloniki, Greece: Springer,
2018, pp. 348–364. doi: 10.1007/978-3-319-89366-2\_19.
[Swa+16]
Nikhil Swamy et al. “Dependent types and multi-monadic
eﬀects in F”. In: Proceedings of the 43rd annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages.
2016, pp. 256–270.
[TA15]
Michael Kirkedal Thomsen and Holger Bock Axelsen. “In-
terpretation and programming of the reversible functional
language RFUN”. In: Proceedings of the 27th Symposium on
the Implementation and Application of Functional Program-
ming Languages, IFL 2015, Koblenz, Germany, September 14-
16, 2015. Ed. by Ralf Lämmel. ACM, 2015, 8:1–8:13. isbn:
978-1-4503-4273-5. doi: 10.1145/2897336.2897345.
[Tof80]
Tommaso Toﬀoli. “Reversible computing”. In: Automata, Lan-
guages and Programming. Ed. by Jaco de Bakker and Jan van
Leeuwen. Berlin, Heidelberg: Springer Berlin Heidelberg,
1980, pp. 632–644. isbn: 978-3-540-39346-7.
190

[Vil19]
Renaud Vilmart. “A Near-Minimal Axiomatisation of ZX-Calculus
for Pure Qubit Quantum Mechanics”. In: 2019 34th Annual
ACM/IEEE Symposium on Logic in Computer Science (LICS). 2019,
pp. 1–10. doi: 10.1109/LICS.2019.8785765.
[Vil19]
Renaud Vilmart. “ZX-Calculi for Quantum Computing and
their Completeness”. Theses. Université de Lorraine, Sept.
2019. url: https://hal.archives- ouvertes.fr/tel-
02395443.
[Vil20]
Renaud Vilmart. The Structure of Sum-Over-Paths, its Con-
sequences, and Completeness for Cliﬀord. arXiv:2003.05678.
Mar. 2020. arXiv: 2003.05678 [quant-ph].
[VKB21]
Augustin Vanrietvelde, Hlér Kristjánsson, and Jonathan Bar-
rett. “Routed quantum circuits”. In: Quantum 5 (2021), p. 503.
doi: 10.22331/q-2021-07-13-503.
[VRH22]
Finn Voichick, Robert Rand, and Michael Hicks. “Qunity: A
Uniﬁed Language for Quantum and Classical Computing”.
In: arXiv preprint arXiv:2204.12384 (2022).
[Wec+21]
Julian Wechs et al. “Quantum Circuits with Classical Versus
Quantum Control of Causal Order”. In: PRX Quantum 2 (3
2021), p. 030335. doi: 10.1103/PRXQuantum.2.030335.
[Wil+16]
Robert Wille et al. “SyReC: A hardware description language
for the speciﬁcation and synthesis of reversible circuits”. In:
Integration, the VLSI Journal 53 (2016), pp. 39–53. doi: 10.
1016/j.vlsi.2015.10.001.
[YAG12]
Tetsuo Yokoyama, Holger Bock Axelsen, and Robert Glück.
“Towards a Reversible Functional Language”. In: Revised Pa-
pers of the Third International Workshop on Reversible Com-
putation (RC’11). Ed. by Alexis De Vos and Robert Wille. Vol. 7165.
Lecture Notes in Computer Science. Gent, Belgium: Springer,
2012, pp. 14–29. doi: 10.1007/978-3-642-29517-1\_2.
[YAG16]
Tetsuo Yokoyama, Holger Bock Axelsen, and Robert Glück.
“Fundamentals of reversible ﬂowchart languages”. In: The-
oretical Computer Science 611 (2016), pp. 87–115. doi: 10.
1016/j.tcs.2015.07.046.
[YG07]
Tetsuo Yokoyama and Robert Glück. “A reversible program-
ming language and its invertible self-interpreter”. In: Pro-
ceedings of the 2007 ACM SIGPLAN Workshop on Partial Evalu-
ation and Semantics-based Program Manipulation, PEPM 2007,
Nice, France, January 15-16, 2007. Ed. by G. Ramalingam and
191

Eelco Visser. 2007, pp. 144–153. doi: 10.1145/1244381.
1244404.
[Yok10]
Tetsuo Yokoyama. “Reversible Computation and Reversible
Programming Languages”. In: Proceedings of the Workshop
on Reversible Computation (RC’09). Ed. by Irek Ulidowski. Vol. 253(6).
Electronic Notes in Theoretical Computer Science. York, UK:
Elsevier, 2010, pp. 71–81. doi: 10.1016/j.entcs.2010.02.
007.
192

