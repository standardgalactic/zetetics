Research on image stitching method based
on fuzzy inference
Jinbo Lu1 & Guanqun Huo1 & Jixiang Cheng1
Received: 6 March 2021 /Revised: 9 June 2021 /Accepted: 21 February 2022
# Springer Science+Business Media, LLC, part of Springer Nature 2022
Abstract
This paper concern the problem of ghosting caused by parallax and moving objects in
image stitching. Previous approaches have used local homography or optimal seam lines
to avoid ghosting. In this work, we propose an image stitching method based on fuzzy
inference. At first, our use of the contrast limited adaptive histogram equalization
(CLAHE) increase the matching points of the object surface in the low-contrast images.
Then, to reduce the number of mismatching points, we combine the orientation of feature
points to improve the zero-mean normalized cross-correlation (ZNCC) for filter matching
points. Furthermore, by viewing the distance weight and gray difference of pixels in
overlapping region as the first input and the second input of fuzzy inference respectively,
and regarding the output of the fuzzy inference as the weight of image fusion. We
generate high-quality stitching images. The experimental results show that our approach
can reduce the ghosting phenomenon and improve the quality of the stitching.
Keywords Image stitching . Fuzzy inference . ZNCC . Eliminate ghosting
1 Introduction
Image stitching has been widely used in 3-D reconstruction, panoramic roaming, remote
sensing [14, 24, 30], etc. It is an important research field of computer vision [15], which aims
to generate an image with a larger field of view through two or more images [17]. The
processing of image stitching can be divided into two vital steps: image alignment and image
fusion.
Image alignment mainly includes two types of methods based on region and feature. The
method based on region calculates the images similarity according to the pixel value [1, 5, 18,
https://doi.org/10.1007/s11042-022-12748-9
* Jinbo Lu
lujb@swpu.edu.cn
1
School of Electronics and Information Engineering, Southwest Petroleum University, Chengdu, CO
637001, China
Multimedia Tools and Applications (2022) 81:23991–24002
Published online: 19 March 2022
/

19]. However, the alignment effect of this method is poor when the images texture is complex
or geometric changes [22]. On the contrary, the feature-based method has better adaptability
[23]. As a result, feature-based method has become a hot topic in the field of image registration
and formed a lot of research results [4, 9, 13].
Feature extraction is the key of feature-based method. The methods of feature extraction are
as follows: Harris corner detector [10], scale invariant feature transform (SIFT) [16], speeded
up robust features (SURF) [2] and oriented fast and rotated brief algorithm (ORB) [20], etc.
Among these methods, SIFT algorithm has better accuracy and stability despite images
rotation and scale change [3], which makes SIFT algorithm and its improved algorithm widely
used in image registration [3, 28, 29]. One excellent work of alignment named AutoStitch [3]
was proposed to automatically stitch multiple images that uses SIFT algorithm extract feature
points for registration. The other representative work is as-projective-as-possible warping [29],
which also utilizes SIFT extract feature points to estimate the initial homography. Unfortu-
nately, even with SIFT algorithm, we cannot ensure that all matching points are correct. The
wrong matches are not beneficial to use random sample consensus (RANSAC) to estimate the
homography [11]. Thus, in order to improve the proportion of correct points, we use the
improved zero-mean normalized cross-correlation (ZNCC) algorithm to filter the matching
points.
Image fusion is another vital stage which helps to obtain high-quality image by reducing
noticeable seams and ghosting. Linear blending [21] and multi-band blending [26] scheme
ensures smooth transitions between images, but will produce ghosting caused by parallax and
moving objects [25]. In order to eliminate the ghosting, different searching methods of optimal
seam line are proposed in [6, 7, 12, 26]. These methods search for the best stitching line in the
overlapping area, and then maps the image to the corresponding side of the line. But these
methods are very sensitive to the illumination of the images, a slight change will cause
noticeable seam [8].
The above mentioned issues raise a strong motivation for reducing ghosting. Fuzzy
inference provide abundant fusion operators, and the input of fuzzy inference can incorporate
more overlapping pixel information as the reference for fusion weights. We argue that the
introduction of fuzzy inference may be able to solve such issues. For this purpose, inspired by
linear blending, an image fusion method based on fuzzy inference is proposed in this paper.
We first calculate the distance weight and gray difference of pixels in the overlapping area as
the input of fuzzy inference. And then use the output of fuzzy inference as the weight of image
fusion.
To sum up, the stitching experimental scheme is shown in Fig. 1, and the main works of
this paper are as follows:
1)
Before extracting feature points, we proposed to use contrast limited adaptive histo-
gram equalization (CLAHE) algorithm to enhance image contrast, which will make the
feature points on the low contrast image more evenly spread.
2)
We improved the ZNCC algorithm and apply it to reduce the wrong matching rate of
feature points.
3)
We proposed an image fusion method based on fuzzy inference.
The rest of this paper is organized as follows. In Section 2, the image preprocessing method,
the improvement of ZNCC algorithm and the fusion method based on fuzzy inference are
introduced. The experimental results are presented in Section 3. And the conclusions are
presented in Section 4.
23992
Multimedia Tools and Applications (2022) 81:23991–24002

2 Proposed method
Before image alignment, we found that using CLAHE algorithm to enhance the image
contrast can increase the spread of matching points on the object in the low-contrast
images. And for ordinary images, it will not have negative impact on feature extraction
and matching [27]. In addition, to achieve more precise matching points set, we extract
feature points and match them though SIFT algorithm firstly, then detect matching points
by our improved ZNCC algorithm. Finally, in the image fusion stage, we proposed a
method to calculate fusion weight by fuzzy inference. These methods are discussed as
follows.
2.1 Image preprocessing
Different from others, when we use the CLAHE algorithm, we first calculate the mapping
function of gray image. Then, the R, G and B layers of the image are processed by this
mapping function. Finally, we use SIFT algorithm to extract feature points.
Figure 2(a) shows a set of images with low-contrast. Applying CLAHE on the images in
Fig. 2(a) results in the images that can be found in Fig. 2(b). We can found that the details on
the trunk are clearer in Fig. 2(b). After enhance the contrast, we use the code provided by
David Lowe [16] to extract feature points, and match them with Euclidean Distance. The
results of feature point extraction and matching are shown in Fig. 3. From Fig. 3(a), we can see
that there are many obvious wrong match points, which marked by red circles. Compared with
Fig. 3(a), there are less obvious mismatches in Fig. 3(b), and the spread of matching points is
more uniform. Furthermore, the enlarged area of Fig. 3(b) has more matching points on the
trunk. This example proves that using CLAHE can increase the matching points on objects in
low-contrast images.
Image to be stitched
Image preprocessing
Feature point extraction 
 And rough matching
RANSAC algorithm 
estimation model
Image blending
Final stitched image
Use CLAHE algorithm 
to enhance image 
Improved ZNCC algorithm to 
screen feature points
Fuzzy inference calculate 
fusion weight 
Fig. 1 The process of image stitching experiment
23993
Multimedia Tools and Applications (2022) 81:23991–24002

2.2 Improved ZNCC algorithm
After enhance the contrast and obtain the SIFT matching points, we use RANSAC algorithm
to calculate the homography between images. RANSAC is a robust estimation procedure that
uses a minimal set of randomly sampled correspondences to estimate image transformation
parameters, and finds a solution that has the best consensus with the date [3]. Due to its
randomness, the number of wrong matching points will affect its estimation. Unfortunately, it
could be found that there are some mismatches in the matching points. In order to reduce the
proportion of mismatching points, we use a template matching method ZNCC to detect the
matching points. However, ZNCC cannot be used for images that rotate with each other. To
solve this problem, we combine the gradient orientation of feature points with ZNCC.
(a)
(b)
Fig. 2 Contrast enhancement using CLAHE. (a) The original image. (b) Result of enhance contrast
(a)
(b) 
Fig. 3 Result of feature point matching. (a) The matching results of Fig. 2(a). (b) The matching results of
Fig. 2(b)
23994
Multimedia Tools and Applications (2022) 81:23991–24002

Figure 4 shows the orientation of the sample template, where θ represent the direction of the
feature points obtained by SIFT. After rotation, the new coordinates can be obtained by
Eq. (1).
x
0
y
0
1
0
@
1
A ¼
cosθ  sinθ0
sinθcosθ0
001
2
4
3
5
x
y
1
0
@
1
A
ð1Þ
where (x', y') is the new coordinate.
When we obtain the new sample coordinates, the value of ZNCC can be calculate by the
following formula,
CZNCC ¼
P
i IAðxi; yiÞ  IA


IBðxi; yiÞ  IB


ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
i IAðxi; yiÞ  IA

2 P
i IBðxi; yiÞ  IB

2
q
ð2Þ
where IA and IB are the pixels in the sample template of match points; IA and IB are the average
gray value in the sample template of match points. The range of CZNCC is between [-1, 1] (1
means perfect match).
Figure 5 shows the change of correct matching rate of ten groups after using our improved
ZNCC algorithm. It can be seen from the figure that the correct matching rate is improved to
varying degrees after using the improved ZNCC algorithm. Through these matching point sets
Fig. 4 Rotate the sampling window
(a)
(b)
(c)
(d)
Fig. 5 Comparison of correct matching rate
23995
Multimedia Tools and Applications (2022) 81:23991–24002

with higher accuracy, we can get the transformation matrix with higher accuracy and get better
stitching results.
2.3 Our image fusion method
The traditional linear blending [21] has been able to fuse the images well. However, it only
considers the distance coefficient of pixels, which may produce some negative fusion results. In
order to get better fusion effect, we integrate distance coefficient and other parameters into the
input of fuzzy inference. And then use the output of fuzzy inference as the weight of fusion.
The basic block diagram of the fuzzy inference system we use is shown in Fig. 6. Its core is
a fuzzy rule base composed of IF-THEN rules. And the fuzzy inference engine determines
how to map the input fuzzy set to the output fuzzy set. The steps to calculate the fuzzy fusion
weight are as follows:
Step 1: The distance weight is selected as the Input1 of the fuzzy inference. The
distance weight can be calculated by Eq. (3), and divided it into five fuzzy subsets.
The Input2 of fuzzy inference is the gray difference of pixels, which divided into three
fuzzy subsets, and the output of fuzzy inference is also divided into five fuzzy subsets.
In this paper, we use μA1 , μA2 , and μB to express the membership functions
of Input1, Input2 and Output2. The membership functions are shown in Fig. 7.
d ¼ N  j
N
ð3Þ
where N is the width of overlap area (sum of image columns in overlap); 0 < j< N is the
number of columns in the overlap region (count from the left bound of the overlap
area).
Step 2: Design fuzzy rules. Each fuzzy rule is composed of IF-THEN statements like “if
the distance weight is small and the gray difference is large, than the output weight is
large”. All IF-THEN statements that constitute the fuzzy rule base can be described in
Table 1, which means “if Input1 is mf1 and Input2 is mf3, then Output is mf5”. In Table 1,
the front part of each fuzzy rule consists of two parts and is connected by “and”, so it
(a)
(b)
(c)
(d)
Fig. 6 The basic block diagram of fuzzy system with fuzzifier and defuzzifier
23996
Multimedia Tools and Applications (2022) 81:23991–24002

should be synthesized by the operators corresponding to the “and”. After using Mamdani
type implication operator to combine the front part and the latter part of the IF-THEN
rule, the membership function of the fuzzy relationship represented by the IF-THEN rule
is:
μAl
1Al
2!Bl xl
1; xl
2; yl


¼ min t μAl
1; μAl
2
h
i
; μBl
h
i
ð4Þ
where t is t-norm, which is the operator corresponding to the conjunction “and”. And the
t-norm is take the smallest operator. l is the number of rules in Table 1, and l 2 1; 15
½
,
because to form a complete fuzzy rule base, fifteen fuzzy rules are needed
when Input1 has five fuzzy subsets and Input2 has three fuzzy subsets.
Step 3: Let μRu lð Þ ¼ μAl
1Al
2!Bl, the output fuzzy set of each rule is B
0
l. When l ¼ 1; 2;
   ; 15, according to generalized modus ponens, its membership function is:
μB
0
l y
ð Þ ¼ supt μA
0
1A
0
2
h

x1; x2Þ; μRu lð Þ
ð5Þ
Step 4: Select the maximum operator to combine the output of each fuzzy rule, and the
membership function can be expressed as follow:
μB
0 y
ð Þ ¼ max μB
0
1


yÞ; μB
0
2ðyÞ;    μB
0
15ðyÞÞ
ð6Þ
Step 5: Use the defuzzifier center of gravity to map the output blur set to the clear point y*,
and use the result as the fusion weight of the reference image. y* can be calculated by,
(a)
(b)
(d)
(c)
Fig. 7 Membership functions of inputs and output
Table 1 Rule base of the proposed fuzzy fusion method
μB
μA1
mf 1
mf 2
mf 3
mf 4
mf 5
μA2
mf 1
mf 5
mf 4
mf 3
mf 2
mf 1
mf 2
mf 4
mf 3
mf 3
mf 3
mf 2
mf 3
mf 5
mf 4
mf 3
mf 2
mf 1
23997
Multimedia Tools and Applications (2022) 81:23991–24002

y* ¼
R
yμB
0 y
ð Þdy
R
μB
0 y
ð Þdy
ð7Þ
where
R
is the regular integral. y* is the center of the area covered by the membership
function of B'.
After obtain the fusion weight value through the above steps, we can get the stitch image by
the following formula,
f x; y
ð
Þ ¼
f A x; y
ð
Þ
x; y
ð
Þ 2 f A
FWAf A x; y
ð
Þ þ FWBf B x; y
ð
Þ
x; y
ð
Þ 2 f A \ f B
f B x; y
ð
Þ
x; y
ð
Þ 2 f B
8
<
:
 >
ð8Þ
where FWA and FWB are the weights of the overlapping area of fA and fB. FWA is directly
obtained by fuzzy inference, and
FWA þ FWB ¼ 1
ð9Þ
3 Experimental results and analysis
In this section, we verify the effectiveness of the proposed method from subjective evaluation
and objective evaluation, respectively. The compared methods include linear blending [21],
APAP [29] and ELA [13]. The image test sets are shown in Fig. 8, where Fig. 8(a, b) have
parallax, Fig. 8(c) has a moving people and Fig. 8(d) has a moving car.
3.1 Result comparisons
Figures 9 and 10 compare the stitching results of linear blending, APAP, ELA and our method
on images with parallax. Due to the existence of parallax, the linear blending and APAP have
obvious ghosting. ELA can reduce ghosting, but ghosting still exists. Our method alleviates
ghosting phenomenon while stitching images. Figures 11 and 12 compare the stitching results
of these three methods on images with moving object. From Figs. 11 and 12, we can see that
none of these three methods can align moving object, lead to undesirable virtualization. Our
92.73%
82.58%
92.00% 90.07%
57.78%
67.71%
73.51%
62.22%
98.05%
99.79%
97.06%
85.70%
95.24%
91.94%
74.07%
81.97%
77.66%
66.51%
99.50%
100.00%
50.00%
60.00%
70.00%
80.00%
90.00%
100.00%
0
1
2
3
4
5
6
7
8
9
10
Correct matching rate
Group label
Before using improved ZNCC
Aer using improved ZNCC
Fig. 8 Image dataset
23998
Multimedia Tools and Applications (2022) 81:23991–24002

method obtains the fusion weight through fuzzy inference, which makes the outline of moving
object clearer and achieves better stitching results. The above experiments show that our
method can deal with challenging examples more robust than other three methods, and the
quality of the stitching image is better, avoid ghosting phenomenon.
To evaluate the details and the clarity of the stitched images, we calculated the information
average gradient of the image. We also calculated the information entropy to evaluate the
amount of information contained in the stitched images. The test images are collected from
public available datasets [4, 9, 11, 13, 29] or captured by ourselves. The definition of each
indicator are as follows.
The definition of information entropy is:
EN ¼
X
N
i¼1
pi  log pi
ð Þ
ð10Þ
where i is the gray value of the pixel 0  i  255, pi is the probability of gray value i; N is the
number of gray levels; EN is the information entropy.
The average gradient is defined as:
G ¼
1
M  N
X
M
i¼1
X
N
j¼0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
@f
@X

2
þ
@f
@Y

2
2
v
u
u
t
ð11Þ
(a)
(b)
(c)
(d)
Fig. 9 Comparison of stitching quality of Fig. 8(a). (a) The results of the linear blending [21]. (b) The results of
APAP [29]. (c) The results of ELA [13]. (d) The results of our method
Fuzzifier
Fuzzy rule base
Defuzzifier
Fuzzy inference 
Engine 
Input 
Output
Fuzzy sets on
Input 
Fuzzy sets on
Output 
Fig. 10 Comparison of stitching quality of Fig. 8(b). (a) The results of the linear blending [21]. (b) The results of
APAP [29]. (c) The results of ELA [13]. (d) The results of our method
23999
Multimedia Tools and Applications (2022) 81:23991–24002

where M and N represent the length and width of the image; @f
@X and @f
@Y represents the gradient
of the image in the horizontal and vertical directions respectively.
Table 2 shows the computed scores of average gradient (G) and information entropy (EN)
between our method and other three methods in different scenarios. And in Table 2, the highest
scores of average gradient (G) and information entropy (EN) of each group of images are in
bold. For the upper twenty cases in Table 2, our method can get the highest evaluation score in
most cases. Only in a few groups of image cases, our method has a slightly lower score on one
index. But there is no ghosting or obvious information loss in visual. On the whole, the scores
Fig. 11 Comparison of stitching quality of Fig. 8(c). (a) The results of the linear blending [21]. (b) The results of
APAP [29]. (c) The results of ELA [13]. (d) The results of our method
D
E
F
G
Fig. 12 Comparison of stitching quality of Fig. 8(d). (a) The results of the linear blending [21]. (b) The results of
APAP [29]. (c) The results of ELA [13]. (d) The results of our method
24000
Multimedia Tools and Applications (2022) 81:23991–24002

are higher by using our proposed method in most cases, which means that the images stitched
by our method are more detailed, clearer and better in quality than linear blinding.
4 Conclusions
This paper has presented an effective image stitching method based on fuzzy inference. Firstly,
we proposed to use CLAHE algorithm to improve the contrast of the image. Without affecting
the original matching points, we increase the matching points in the area with poor contrast to
make the distribution of matching points more even. This is helpful to improve the applica-
bility of global homography to all pixels. Secondly, an improved ZNCC of feature refinement
is proposed to remove the outliers from the matching data. Thirdly, we provide a method to
calculate the weight of image fusion by fuzzy reasoning to improve the quality of image
mosaic. Finally, the stitching quality of our method are analyzed on different test images.
Acknowledgements This work was supported in part by the National Natural Science Foundation for Young
Scientists of China (61,603,319, 61,601,385).
References
1. Adwan S, Alsaleh I, Majed R (2016) A new approach for image stitching technique using Dynamic Time
Warping (DTW) algorithm towards scoliosis X-ray diagnosis. Measurement 84:32–46
2. Bay H, Tuytelaars T, Van Gool L (2006) Surf: Speeded up robust features. In: Proceedings 9th European
conference computer vision, pp 404–417
Table 2 Comparison of average gradient (G) and information entropy (EN) on different datasets
Dataset
G
EN
Linear blending
APAP
ELA
Ours
Linear blending
APAP
ELA
Ours
Figure 8(a)
4.1618
4.1486
6.0120
6.5822
5.7056
5.1728
7.4796
7.6189
Figure 8(b)
4.8851
4.8518
4.3696
5.8486
6.0134
6.0168
6.1593
6.9949
Figure 8(c)
9.3037
9.1125
9.2793
9.9460
7.4326
7.4458
7.4152
7.5031
Figure 8(d)
2.5632
2.6135
2.4615
2.6262
6.7996
6.7875
6.7935
6.8123
OpenCV sample
4.1572
4.0962
6.1765
6.0574
5.8802
5.8614
7.0949
7.1003
garden [4]
5.5872
5.5665
6.4923
7.0194
6.6651
6.6671
7.6425
7.6791
building [4]
3.9529
3.9366
3.3504
4.0606
6.1211
6.0888
6.1239
6.1434
street [4]
3.9334
3.8928
3.4696
4.1498
6.2929
6.2834
6.2553
6.3435
campus [4]
3.7984
3.8307
6.2361
6.7875
5.6007
5.6111
7.1968
7.5698
garden [29]
4.8584
4.7318
4.0414
5.1283
7.0184
7.0121
7.0536
7.0538
skyline [29]
3.8743
3.8838
4.3983
4.4256
6.6126
6.6158
7.0878
6.9491
temple [9]
3.2434
3.2423
3.2028
3.8624
6.2534
6.2646
6.4220
7.0139
carpark [9]
5.7265
5.7373
5.2421
6.0035
6.5462
6.5493
6.6403
6.6599
forest [9]
8.1523
7.9444
7.2025
7.8303
7.1329
7.1229
7.0435
7.3672
cabin [13]
8.3865
8.2838
7.5375
8.7062
6.8908
6.8679
6.8749
6.9077
wall [13]
6.0812
6.0252
4.8371
6.1578
5.8605
5.8656
5.8342
5.8473
lawn [13]
4.8767
4.8723
3.9938
5.0530
6.1670
6.6079
6.5366
6.6273
theater [13]
4.1907
4.1963
3.6156
4.3330
6.5975
6.5986
6.6045
6.6485
riverbank [13]
3.1036
3.0954
2.6345
3.1590
6.6603
6.6651
6.6469
6.6489
footpath [13]
3.7993
3.7757
3.1586
3.8436
6.3685
6.3585
6.2437
6.3731
24001
Multimedia Tools and Applications (2022) 81:23991–24002

3. Brown M, Lowe DG (2007) Automatic panoramic image stitching using invariant features. Int J Comput
Vis 74(1):59–73
4. Chang CH, Sato Y, Chuang YY (2014) Shape-preserving half-projective warps for image stitching. In:
Proceedings IEEE Conference on Computer Vision Pattern Recognition, pp 3254–3261
5. Dame A, Marchand E (2012) Second-order optimization of mutual information for real-time image
registration. IEEE Trans Image Process 21(9):4190–4203
6. Davis J (1998) Mosaics of scenes with moving objects. In: Proceedings IEEE Computer Society Conference
on Computer Vision and Pattern Recognition, pp 354–360
7. Duplaquet ML (1998) Building large image mosaics with invisible seam lines. In: Proceedings Visual
Information Processing VII, pp 369–377
8. Fang F, Wang T, Fang Y, Zhang G (2019) Fast color blending for seamless image stitching. IEEE Geosci
Remote Sens Lett 16(7):1115–1119
9. Gao J, Kim SJ, Brown MS (2011) Constructing image panoramas using dual-homography warping. In:
Proceedings of the 2011 IEEE conference on computer vision and pattern recognition, pp 49–56
10. Harris CG, Stephens M (1988) A combined corner and edge detector. In: Alvey vision conference, pp 147–151
11. Laraqui A, Baataoui A, Saaidi A, Jarrar A, Masrar M, Satori K (2016) Image mosaicing using voronoi
diagram. Multimed Tools Appl 76(6):8803–8829
12. Levin A, Zomet A, Peleg S, Weiss Y (2004) Seamless image stitching in the gradient domain. In: European
Conference on Computer Vision, pp 377–389
13. Li J, Wang Z, Lai S, Zhai Y, Zhang M (2018) Parallax-tolerant image stitching based on robust elastic
warping. IEEE Trans Multimed 20(7):1672–1687
14. Lin K, Liu S, Cheong LF, Zeng B (2016) Seamless video stitching from hand-held camera inputs. Comput
Graph Forum 35(2):479–487
15. Liu SB, Wang JH, Yuan RY et al (2020) Real-time and ultrahigh accuracy image synthesis algorithm for
full field of view imaging system. Sci Rep 10(1):1–12
16. Lowe DG (2004) Distinctive image features from scale-invariant key points. Int J Comput Vis 60(2):91–110
17. Luo X, Li Y, Yan J, Guan X (2020) Image stitching with positional relationship constraints of feature points
and lines. Pattern Recognit Lett 135:431–440
18. Pilchak AL, Shiveley AR, Shade PA, Tiley JS, Ballard DL (2012) Using cross-correlation for automated
stitching of two-dimensional multi-tile electron backscatter diffraction data. J Microsc 248(2):172–186
19. Rivaz H, Karimaghaloo Z, Collins DL (2014) Self-similarity weighted mutual information: a new nonrigid
image registration metric. Med Image Anal 18(2):343–358
20. Rublee E, Rabaud V, Konolige K, Bradski G (2011) ORB: An efficient alternative to SIFT or SURF. In:
IEEE international conference on computer vision, pp 2564–2571
21. Su H, Wang J, Li Y, Hong X, Li P (2014) An algorithm for stitching images with different contrast and
elimination of ghost. 2014 Seventh International Symposium on Computational Intelligence and Design,
Hangzhou, pp 104–107
22. Szeliski R (2006) Image alignment and stitching: A tutorial. Foundations and Trends® in Computer
Graphics and Vision 2(1):1-104
23. Vaidya OS, Gandhe ST (2018) The study of preprocessing and postprocessing techniques of image stitching.
In: International Conference On Advances in Communication and Computing Technology, pp 431–435
24. Vishwakarma A, Bhuyan MK (2020) Image mosaicking using improved auto-sorting algorithm and local
difference-based harris features. Multimed Tools Appl 79(1):1–18
25. Wang D, Liu H, Li K, Zhou W (2017) An image fusion algorithm based on trigonometric functions.
Infrared Technol 39(1):53–57
26. Xianyong F, Zhigeng P (2003) An improved algorithm for image mosaics. J Comput Aided Des Comput
Graph 11:1362–1365
27. Yadav G, Maheshwari S, Agarwal A (2014) Contrast limited adaptive histogram equalization based
enhancement for real time video system. In: International Conference On Advances in Communication
and Computing Technology, pp 2392–2397
28. Yi KM, Trulls E, Lepetit V, Fua P (2016) Lift: Learned invariant feature transform. In: European conference
on computer vision, pp 467–483
29. Zaragoza J, Chin T, Tran Q, Brown MS, Suter D (2014) As-projective-as-possible image stitching with
moving DLT. IEEE Trans Pattern Anal Mach Intell, pp 1285–1298
30. Zheng J, Wang Y, Wang H, Li B, Hu HM (2019) A novel projective-consistent plane based image stitching
method. IEEE Trans Multimed 21(10):2561–2575
Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps
and institutional affiliations.
24002
Multimedia Tools and Applications (2022) 81:23991–24002

