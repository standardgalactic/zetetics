This document is downloaded from DR‑NTU (https://dr.ntu.edu.sg)
Nanyang Technological University, Singapore.
Counterfactual explanations for machine learning
models on heterogeneous data
Wang, Yongjie
2023
Wang, Y. (2023). Counterfactual explanations for machine learning models on
heterogeneous data. Doctoral thesis, Nanyang Technological University, Singapore.
https://hdl.handle.net/10356/169968
https://hdl.handle.net/10356/169968
https://doi.org/10.32657/10356/169968
This work is licensed under a Creative Commons Attribution‑NonCommercial 4.0
International License (CC BY‑NC 4.0).
Downloaded on 13 Jul 2024 03:09:02 SGT

Counterfactual Explanations for Machine
Learning Models on Heterogeneous Data
Yongjie Wang
School of Computer Science and Engineering
A thesis submitted to the Nanyang Technological University
in partial fulﬁllment of the requirements for the degree of
Doctor of Philosophy
2023


Statement of Originality
I hereby certify that the work embodied in this thesis is the result
of original research, is free of plagiarised materials, and has not been
submitted for a higher degree to any other University or Institution.
15/12/2022
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
Date
Yongjie Wang


Supervisor Declaration Statement
I have reviewed the content and presentation style of this thesis and
declare it is free of plagiarism and of suﬃcient grammatical clarity
to be examined.
To the best of my knowledge, the research and
writing are those of the candidate except as acknowledged in the
Author Attribution Statement. I conﬁrm that the investigations were
conducted in accord with the ethics policies and integrity standards
of Nanyang Technological University and that the research data are
presented honestly and without prejudice.
05/01/23
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
Date
Prof. Chunyan Miao


Authorship Attribution Statement
This thesis contains material from 1 paper published in the following
peer-reviewed journal / from 3 papers published at conferences in
which I am listed as an author.
Chapter 3 is published as Yongjie Wang, Ke Wang, Cheng Long and Chun-
yan Miao, ”Summarizing User-Item Matrix By Group Utility Maximization,” 2021
IEEE International Conference on Data Mining (ICDM), 2021, pp. 1409-1414, doi:
10.1109/ICDM51629.2021.00180. and
Yongjie Wang, Ke Wang, Cheng Long, and Chunyan Miao. 2023. Summarizing
User-item Matrix By Group Utility Maximization. ACM Transactions on Knowl-
edge Discovery from Data, 17, 6, Article 86 (July 2023), 22 pages. https://doi.org/
10.1145/3578586.
The contributions of the co-authors are as follows:
• Prof. Wang proposed the initial idea and k-max algorithm.
• Prof. Long designed the greedy algorithm and proved the formal properties.
• I implemented both algorithms and conducted experiments.
• I prepared the draft. Prof. Wang, Prof. Long, and Prof. Miao revised the
draft.
Chapter 4 is published as Yongjie Wang, Qinxu Ding, Ke Wang, Yue Liu, Xingyu
Wu, Jinglong Wang, Yong Liu, and Chunyan Miao. 2021. The Skyline of Coun-
terfactual Explanations for Machine Learning Decision Models. In Proceedings
of the 30th ACM International Conference on Information & Knowledge Manage-
ment (CIKM ’21). Association for Computing Machinery, New York, NY, USA,
2030–2039. https://doi.org/10.1145/3459637.3482397.
The contributions of the co-authors are as follows:
• Prof. Wang Ke proposed the initial idea.
• Dr. Ding and I followed Prof. Wang’s idea, formulated the research problem,
and designed the detailed algorithm.
• I implemented our algorithm and veriﬁed it with Dr. Ding and Prof. Wang.
• I conducted all experiments and discussed the experiment results with Dr.
Ding, Prof. Wang, and Dr. Liu.
• I and Dr. Ding wrote the draft. Prof. Wang, Dr. Liu, and Prof. Miao
reviewed the draft together.
• Three Alibaba collaborators, Mrs. Liu, Mr. Wu, and Mr. Wang gave their
advice and checked our methods from industrial perspectives.

viii
Chapter 5 is published as Yongjie Wang, Hangwei Qian, and Chunyan Miao.
2022. DualCF: Eﬃcient Model Extraction Attack from Counterfactual Explana-
tions. In 2022 ACM Conference on Fairness, Accountability, and Transparency
(FAccT ’22). Association for Computing Machinery, New York, NY, USA, 1318–1329.
https://doi.org/10.1145/3531146.3533188.
The contributions of the co-authors are as follows:
• Prof. Miao pointed out the shortages of existing research and speciﬁed our
research aim.
• I designed the initial idea, detailed method and conducted experiments, and
discussed with Dr. Qian.
• I prepared the manuscript and Dr. Qian and Prof. Miao revised and polished
the paper.
15/12/2022
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
Date
Yongjie Wang

Acknowledgements
I would like to express my utmost gratitude to all those who have accompanied me
throughout my Ph.D. journey.
First and foremost, my heartfelt thanks go to my supervisor, Professor Chunyan
Miao, for her professional and insightful guidance throughout my four-year re-
search. She taught me to explore a new research ﬁeld, write scientiﬁc papers, and
most importantly, become an independent researcher. I would also like to express
my sincerest appreciation to Professor Ke Wang and Professor Cheng Long. Their
patience, motivation, enthusiasm, and knowledge have supported me through all
stages of my Ph.D. Without their help, I would not have been able to complete my
Ph.D. I would also like to thank my Ph.D. committee members for their thoughtful
comments and constructive suggestions.
Second, I would like to extend my gratitude to my fellow labmates at NTU: Hang-
wei Qian, Qinxu Ding, and Yong Liu for discussing and revising my papers dur-
ing many sleepless nights before the deadlines. I am also grateful to have had
lovely badminton and basketball buddies: Shengfei Lyu, Leung Jonathan Cyril,
Hao Chen, Yadan Zeng, Zeyu Jin, and Tong Zhang. All the fun we experienced on
campus in the last four years is unforgettable.
Third, I would like to express my deepest appreciation to my parents who have
always supported me selﬂessly and unconditionally. Life is not easy, and diﬃculties
are always present. My parents have always inspired me to bravely overcome all
the negatives, pressure, and challenges. I would like to express my greatest love
to my girlfriend and soulmate, Xu Guo, who has accompanied, encouraged, and
supported me all the time.
Lastly, I would like to thank all the hardships that I encountered in the past, as
they have made me stronger.
ix


Abstract
Counterfactual explanation aims to identify minimal and meaningful changes re-
quired in an input instance to produce a di↵erent prediction from a given model.
Counterfactual explanations can assist users in comprehending the model’s current
prediction, detecting model unfairness, and providing actionable recommendations
for users who receive an undesired prediction. Consequently, counterfactual expla-
nations have diverse applications in ﬁelds such as education, ﬁnance, marketing,
and healthcare.
The counterfactual explanation problem is formulated as a constrained optimiza-
tion problem, where the goal is to minimize the cost between the input and coun-
terfactual explanations subject to certain constraints. Existing research has mainly
focused on two areas: incorporating practical constraints and introducing various
solving methods. However, counterfactual explanations are still far from practi-
cal deployment. In this thesis, we improve this problem from the angles of trust,
actionability, and safety, thus making counterfactual explanations more deployable.
One goal of counterfactual explanations is to seek action suggestions from the
model.
However, commonly used models such as ensemble models and neural
networks are black boxes with poor trustworthiness. Explaining the model can
improve the trustworthiness of models. Yet, global explanations are too general to
apply to all instances, and examining all local explanations one by one is also a
burden. Therefore, we propose a group-level summarization method that ﬁnds k
groups, where each group is summarized by the distinct top-l important features
for a feature importance matrix. This approach provides a compact summary that
makes it easier to understand and inspect the model.
In real-life applications, it is diﬃcult to compare changes in heterogeneous features
with a scalar cost function. Moreover, existing methods do not support interactive
exploration for users. To address them, we propose a skyline method that treats
the change of each incomparable feature as an objective to minimize and ﬁnds a set
xi

xii
of non-dominant counterfactual explanations. Users can interactively reﬁne their
requirements from this non-dominated set. Our experiments demonstrate that our
method provides superior results compared to state-of-the-art methods
Model security and privacy are critical concerns for model owners who want to de-
ploy a counterfactual explanation service. However, these issues have not received
much attention in the literature. To address this gap, we propose an eﬃcient and
e↵ective attack method that can extract the pretrained model through counterfac-
tual explanations (CFs). Speciﬁcally, our method treats CFs as common queries to
ﬁnd counterfactual explanations of counterfactual explanations (CCFs) and then
trains a substitute model using pairs of CFs and CCFs. Experiments reveal that
our approach can obtain a substitute model with a higher agreement.
In summary, our research helps to bridge the research gap between the theoret-
ical understanding and practical use of counterfactual explanations and provides
valuable insights for researchers and practitioners in various domains.

Contents
Acknowledgements
ix
Abstract
xi
List of Figures
xvii
List of Tables
xxi
Symbols and Acronyms
xxiii
1
Introduction
1
1.1
Research Background . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Demands of Explainable AI (XAI)
. . . . . . . . . . . . . .
1
1.1.2
A Brief History of XAI . . . . . . . . . . . . . . . . . . . . .
2
1.1.3
“Explanability” vs “Interpretability” . . . . . . . . . . . . .
3
1.1.4
Pipeline of XAI . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.5
Taxonomies of XAI . . . . . . . . . . . . . . . . . . . . . . .
6
1.2
Counterfactual Explanations . . . . . . . . . . . . . . . . . . . . . .
7
1.2.1
Demands
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.2.2
Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.2.3
Landscapes
. . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.2.4
Implications . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.3
Research Challenges
. . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.4
Overview of Contributions . . . . . . . . . . . . . . . . . . . . . . .
14
1.5
Research Outlines . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2
Literature Review
19
2.1
Feature Attribution Explanations . . . . . . . . . . . . . . . . . . .
19
2.1.1
Perturbation-based methods . . . . . . . . . . . . . . . . . .
20
2.1.2
Gradient-based methods . . . . . . . . . . . . . . . . . . . .
22
2.1.3
Decomposition-based methods . . . . . . . . . . . . . . . . .
24
2.1.4
Approximation-based methods . . . . . . . . . . . . . . . . .
24
2.2
Counterfactual Explanations . . . . . . . . . . . . . . . . . . . . . .
25
2.2.1
Research Scope . . . . . . . . . . . . . . . . . . . . . . . . .
26
xiii

xiv
CONTENTS
2.2.2
Problem Formulation . . . . . . . . . . . . . . . . . . . . . .
28
2.2.3
Problem Solving
. . . . . . . . . . . . . . . . . . . . . . . .
34
2.3
Model Extraction Attacks and Defenses . . . . . . . . . . . . . . . .
37
2.3.1
Model Extraction Attacks . . . . . . . . . . . . . . . . . . .
37
2.3.1.1
Model Attribute Extraction . . . . . . . . . . . . .
38
2.3.1.2
Model Functionality Extraction . . . . . . . . . . .
38
2.3.2
Model Extraction Defenses . . . . . . . . . . . . . . . . . . .
39
2.3.2.1
Output Perturbations
. . . . . . . . . . . . . . . .
39
2.3.2.2
Query Monitoring
. . . . . . . . . . . . . . . . . .
40
2.4
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3
Group Summarization for Model Inspection
43
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.3
Group Utility Maximization . . . . . . . . . . . . . . . . . . . . . .
49
3.3.1
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.3.2
Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.4
Greedy Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.4.1
Approximation Factor
. . . . . . . . . . . . . . . . . . . . .
55
3.4.2
Implementation . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.5
k-max Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.6
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
3.6.1
Experimental Setup . . . . . . . . . . . . . . . . . . . . . . .
63
3.6.2
Group-level Model Inspection on Titanic . . . . . . . . . . .
65
3.6.3
Summarizing Preferences of Netﬂix Movies . . . . . . . . . .
67
3.6.4
Scalability on MovieLens . . . . . . . . . . . . . . . . . . . .
71
3.6.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.7
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4
The Skyline of Counterfactual Explanations
73
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.3
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
4.3.1
The Skyline Approach
. . . . . . . . . . . . . . . . . . . . .
79
4.3.2
Querying the Skyline . . . . . . . . . . . . . . . . . . . . . .
80
4.4
Skyline Counterfactual Algorithm . . . . . . . . . . . . . . . . . . .
81
4.4.1
The Search Space C
. . . . . . . . . . . . . . . . . . . . . .
81
4.4.2
SkylineCF . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.4.3
Complexity Analysis . . . . . . . . . . . . . . . . . . . . . .
85
4.5
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.5.1
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.5.2
Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . .
87
4.5.3
Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89

CONTENTS
xv
4.5.4
Quantitative Evaluation
. . . . . . . . . . . . . . . . . . . .
90
4.5.5
Use Case Evaluation . . . . . . . . . . . . . . . . . . . . . .
92
4.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
5
Model Extraction Attack from Counterfactual Explanations
95
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.3
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.4
Proposed Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.4.1
Research Motivation
. . . . . . . . . . . . . . . . . . . . . . 104
5.4.2
Decision Boundary Shift Issue . . . . . . . . . . . . . . . . . 105
5.4.3
Proposed Method: DualCF
. . . . . . . . . . . . . . . . . . 107
5.4.4
DualCF for A Linear Model . . . . . . . . . . . . . . . . . . 109
5.5
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.5.1
Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.5.2
Implementation Details . . . . . . . . . . . . . . . . . . . . . 111
5.5.3
Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . 112
5.5.4
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
5.5.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . 114
5.5.6
Ablation Studies
. . . . . . . . . . . . . . . . . . . . . . . . 115
5.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
6
Conclusion and Future Works
121
6.1
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.2
Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
List of Author’s Awards, Patents, and Publications
127
Bibliography
129


List of Figures
1.1
Number of publications in the past 7 years, searching with four key-
words “explainable artiﬁcial intelligence”, “explainable AI”, “inter-
pretable artiﬁcial intelligence” and “interpretability” on Dimensions
AI website.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Pipeline of XAI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Taxonomy of XAI and our research interest (in the orange back-
ground).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
An example of counterfactual explanations in loan applications.
. .
10
1.5
Counterfactual explanations bridge the model prediction and user
behaviors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.1
Heatmaps and saliency maps produced by feature attribution meth-
ods on images and texts. Important regions in heatmaps are high-
lighted with red colors. Positive/negative signiﬁcant words in the
saliency map below are masked in green/red respectively. . . . . . .
21
2.2
The di↵erences between back-propagation [1], deconvnet [2] and guided
back-propagation [3].
. . . . . . . . . . . . . . . . . . . . . . . . . .
22
3.1
Titanic dataset: AIU and running iterations for various k and l =
3.
The red dashed line represents the utility for top-l selection.
The coeﬃcients of variation of Random Init, KMA utility are within
74.92%, 2.75%.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
3.2
Netﬂix-Prize-17770: The average item utility (AIU), running time
(in ln scale), and running iterations for various k (x-axis) and l = 30.
The red dash line represents the utility of top-l selection. CoClust,
K-CPGC, Greedy and SGreedy-✏were omitted due to long running
time. The coeﬃcients of variation of Random Init, KMA utility are
within 6.96%, 1.21%.
. . . . . . . . . . . . . . . . . . . . . . . . .
69
3.3
Netﬂix-Prize-200: The average item utility (AIU), running time (in
ln scale), and running iterations for various k and l = 2. The red
dash line represents the utility of top-l selection. CoClust and K-
CPGC cannot run within our limited time and are not reported.
The coeﬃcients of variation of AIU of Random Init, SGreedy-0.5,
SGreedy-0.9 and KMA and are within 6.90%, 1.14%, 1.40%, 5.71%
respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
xvii

xviii
LIST OF FIGURES
3.4
Netﬂix-Prize-200: The average item utility (AIU), running time (in
ln scale), and running iterations for various k and l = 3. The red
dash line represents the utility of top-l selection. CoClust, K-CPGC,
Greedy and SGreedy-0.5 were omitted due to long running time.
The coeﬃcients of variation of AIU of Random Init, SGreedy-0.9
and KMA are within 4.95%, 1.22% and 3.86% respectively. . . . . .
69
3.5
MovieLens-1B-200: The average item utility (AIU), running time (in
ln scale) and running iterations for various k and l = 30. The red
dash line represents the utility of top-l selection. CoClust, K-CPGC,
Greedy and SGreedy-✏are omitted due to long running time. The
coeﬃcients of variation of AIU of Random Init, KMA are within
2.60%, 4.15%. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
4.1
The di↵erences between previous works and the proposed method.
Using 2 features “Education year” and “Salary” as an example, pos-
itive and negative space are separated by the classiﬁer f. With a
given cost function, existing methods often return a single explana-
tion as the red circle. Our method returns the skyline of counterfac-
tuals shown as the blue curve. The skyline of counterfactuals forms
the database and user can query the database interactively. . . . . .
75
4.2
Purple color represents the input point x. Red colors represent the
observed points o with the target prediction. Blue colors represent
the projection o0 of o onto the actionable subspace. Black arrows
from input to anchor points specify the search space and directions.
82
4.3
Each row represents the results on a dataset. Each column repre-
sents the results evaluated by a metric. The "/# represents that a
higher/lower score is better. . . . . . . . . . . . . . . . . . . . . . .
91
4.4
The use case of an input instance in UCI Adult Dataset. The shaded
features in the input instance are actionable features. The query pro-
cess starts with 58 counterfactual explanations in the skyline. With
two queries Q1 and Q2, the user reaches a small set of candidates
that can be looked at closely. Other contains the jobs of protective-
services and tech-support following [4]. . . . . . . . . . . . . . . . .
92
5.1
The workﬂow of model extraction attacks with counterfactual ex-
planations. Service providers train the cloud model on their private
data and deploy the counterfactual explanation service in the cloud.
Adversaries aim to construct a substitute model by leveraging the
information provided by APIs. Three types of model extraction at-
tacks are illustrated. Steal-ML [5] only uses the prediction output of
the query x. Model Extraction [6] considers both the query x and
CF c while our proposed DualCF takes CF c and CCF c0 into train-
ing. In Model Extraction Attacks module, “ ” denotes uploading
a query to the API and “!” denotes receiving outcomes from the
API. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96

LIST OF FIGURES
xix
5.2
The illustration of decision boundary shift issue. (a) and (b) demon-
strate that substitute model decision boundary shifts away from the
ground truth due to far-distant queries in existing method Model Ex-
traction [6]. (c) shows that Model Extraction has to use more queries
to mitigate this issue. Our method DualCF can achieve comparable
agreement with only one pair CF and CCF, as shown in (d), which
favorably illustrates the eﬃcacy of our method.
. . . . . . . . . . .
99
5.3
The probability density contour lines during training stage (at epoch
25, 50, 75, 100) of the cloud model trained on a synthetic dataset
for binary classiﬁcation task. The blue and purple dots are sampled
points from training set. The values on the contour lines represent
the probability calculated by the sigmoid function with range being
[0, 1]. We can see the model tends to assign high probabilities to
more samples during training. . . . . . . . . . . . . . . . . . . . . . 106
5.4
The toy examples on a binary synthetic dataset. We show 3 queries
and corresponding CFs and CCFs (orange dots, green triangles, and
red hexagons) in ﬁgure (a). Figure (b), (c) and (d) illustrate the
substitute model’s decision boundary produced by 3 attack meth-
ods. Steal-ML uses prediction outputs of queries, Model Extraction
applies both queries and CFs, and our proposed DualCF considers
CFs and CCFs. Our proposed DualCF achieves the highest agreement.109
5.5
Counterfactual explanations visualization on Syn-Linear and Syn-
Nonlinear datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.6
Experiment results on synthetic and real-life datasets. The upper
ﬁgures report the average agreement while the bottom ﬁgures report
the standard deviation of agreements of 100 runs for a ﬁxed size of
queries. "/# mean the higher/lower results are better. . . . . . . . . 115
5.7
Experiment comparisons on di↵erent model capacities in proposed
DualCF. “Remove Nodes”, “Add Nodes”, and “Add Layer” repre-
sent three variants of baseline architectures.
. . . . . . . . . . . . . 116
5.8
Experimental comparisons on di↵erent thresholds in counterfactual
explanation generation in our proposed DualCF. We select di↵erent
thresholds {0.6, 0.7, 0.8, 0.9} as the prediction conﬁdence to generate
di↵erent CFs and CCFs. . . . . . . . . . . . . . . . . . . . . . . . . 116
5.9
Experimental comparisons on di↵erent cost metrics in our proposed
DualCF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.10 Experiment results on imbalanced GMSC query set. . . . . . . . . . 118
5.11 Experiment results on with/without shu✏ing for DualCF.
. . . . . 118


List of Tables
3.1
The user-item matrix D contains the feature importance (i.e., IG)
values for the survival probability F(x) of six Titanic correctly pre-
dicted survivors x, where F is a black-box ML model. The IG value
of the feature xi for a survivor x represents the contribution of xi to
the prediction F(x) and xi with a larger IG value is more inﬂuential
for x’s prediction. With k = 2 and l = 2, GUM partitions the six
survivors into two groups, i.e., {U1, U2, U3} and {U4, U5, U6}, and
selects two most inﬂuential features for each group, marked by the
green and pink colors.
. . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2
Frequently used notations in Chapter 3.
. . . . . . . . . . . . . . .
49
3.3
The time and space complexities of di↵erent implementations of
Greedy algorithm. In the case of log( 1
✏) > k, the time complex-
ity of Greedy and SGreedy algorithms is the same since the sample
size cannot be greater than the full size |X| . # means the empiri-
cal running time is reduced compared to without the Lazy-forward
strategy, even if the theoretical time complexity remains the same. .
59
3.4
A toy movie rating dataset . . . . . . . . . . . . . . . . . . . . . . .
61
3.5
Statistics of processed datasets. . . . . . . . . . . . . . . . . . . . .
65
3.6
Titanic dataset: the groups produced with l = 2 and k = 3 by
Greedy and KMM. In this setting, both Greedy and KMM produce
the same results. For example, group 1 has 159 survivals and se-
lects Pclass and Sex as top-2 important features whose average item
utility are 1.28 and 1.75 respectively. The second row denotes the
average utility of an item on the whole population. The numbers in
the parentheses represent the sizes of survivals.
. . . . . . . . . . .
66
3.7
The Netﬂix-Prize-17770: X1, X2, X3, X4, X5 found by KMM with
l = 3 and k = 5. E.g., the ﬁrst group has size 172,843 and “Miss
Congeniality” is one of the top-3 movies in X1 and has 85,555 ratings
with the average rating of 3.66 in this group. . . . . . . . . . . . . .
68
3.8
The Netﬂix-Prize-17770: the top-15 movies found by traditional
ranking methods by popularity (the total number of ratings) or av-
erage rating. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.1
Frequently used notations in Chapter 4.
. . . . . . . . . . . . . . .
77
5.1
The model architectures, hyper-parameters used for training substi-
tute models on synthetic and real-life datasets. . . . . . . . . . . . . 112
xxi

xxii
LIST OF TABLES
5.2
The models used in model capacity studies.
. . . . . . . . . . . . . 117

Symbols and Acronyms
Symbols
x
an input instance
c
a counterfactual explanation
xi
the i-th component of a vector x
f
a pretrained and black-box model to explain
y
the desired prediction for a counterfactual explanation
g(f, · )
a counterfactual explanation method for an instance
el(f, · )
a feature importance explanation method for an instance
X
feature space
Y
prediction space
rf
the gradient of model
O( · )
order of magnitude or ergodic convergence rate (running average)
1( · )
indicator function
k · k
the 2-norm of a vector or matrix in Euclidean space
Rn
n-dimensional real coordinate space
R
real coordinate space
R+
the space of positive real numbers
⊙
the Hadamard (component-wise) product
⌦
the Kronecker product
h · , · i
the inner product of two vectors
1
all-ones column vector with proper dimension
◦
the composition of functions
≻h
dominate on a set of objectives h
xxiii

xxiv
SYMBOLS AND ACRONYMS
Acronyms
AI
Artiﬁcial Intelligence
API
Application Programming Interface
CF
CounterFactual explanation
CCF
Counterfactual explanation of CounterFactual explanation
DNN
Deep Neural Network
FI
Feature Importance
FISTA
Fast Iterative Shrinkage-Thresholding Algorithm
GD
Gradient Descent
GDPR
European General Data Protection Regulation
HCI
Human-Computer Interaction
ISTA
Iterative Shrinkage-Thresholding Algorithms
IG
Integrated Gradient
IP
Integer Programming
MAD
Median Absolute Deviation
MIP
Mixed Integer Programming
ML
Machine Learning
LIME
Local Interpretable Model-agnostic Explanations
LRP
Layer-wise Relevance Propagation
ML
Machine Learning
SAT
Boolean SATisﬁability problem
SGD
Stochastic Gradient Descent
SMT
Satisﬁability Modulo Theories
SHAP
SHapley Additive exPlanations
AE
AutoEncoder
VAE
Variational AutoEncoder
XAI
eXplainable Artiﬁcial Intelligence
i.i.d.
independent and identically distributed
a.s.
almost sure convergence of a random sequence

Chapter 1
Introduction
In this chapter, we provide an overview of the background, deﬁnitions, research
challenges and objectives, and contributions of this thesis. We begin with an intro-
duction to Explainable Artiﬁcial Intelligence (XAI), including practical demands
and historical developments. We also clarify the di↵erences between “explainabil-
ity” and “interpretability”, which are often used interchangeably to describe the
common ground in XAI. Next, we focus on counterfactual explanations, a subﬁeld
of XAI, due to their unique properties. While several methods have been proposed
to enhance counterfactual explanations, there are still several challenges that need
to be addressed. After that, we introduce three crucial challenges that our thesis
tries to tackle and highlight our contributions. Finally, we provide an overview of
the organization of this thesis.
1.1
Research Background
1.1.1
Demands of Explainable AI (XAI)
In recent years, the rapid advancements in machine learning models have led to
their superior performance in various areas, such as computer vision [7], medical
diagnosis [8], recommender systems [9], language translation [10], and even com-
plex strategic games like Go and Chess [11]. As a result, machine learning models
have become increasingly popular and are being widely adopted across many dis-
ciplines, leading to numerous breakthroughs.
The success of machine learning
1

2
1.1. Research Background
models can be attributed to the massive volume of data and millions of parameters
that we have never seen before. Fundamentally, machine learning models aim to
recognize the discriminative patterns by minimizing the empirical loss on training
datasets.
To enhance model performance, researchers design larger and deeper
models to improve the learning capacity. For example, VGG19 model [1] consists
of 175M training parameters, AlexNet [12] contains 60M parameters, and GPT-3
[13] has a whopping 175 billion parameters. Despite the impressive performance,
the large volume of model parameters in machine learning models makes it diﬃ-
cult for humans to reason about how a prediction is made, which is essential for
high-stakes applications. Furthermore, The generalization ability of these black-
box models is poor [14], and they can potentially make unreliable, unfair, and
even dangerous decisions for users. This lack of transparency and accountability in
the decision-making process can lead to distrust and skepticism towards machine
learning models, hindering their widespread adoption in critical applications.
As such, it is essential and urgent to explain the behavior of a black-box model to
avoid mistakes, identify biases, and ultimately increase trust before deployment.
Explainable AI (XAI) refers to the general methods and techniques in the ﬁeld of
artiﬁcial intelligence that enable the reasoning and understanding of the decision-
making process of machine learning models. From the user’s perspective, the ﬁnal
goal of XAI is to make these models more transparent and interpretable, allowing
humans to comprehend and trust the decisions made by them. Need to mention,
as there is no universally agreed-upon deﬁnition of XAI, it is generally accepted
that any practice or approach that improves the interpretability or transparency
of machine learning models can be considered a form of XAI.
1.1.2
A Brief History of XAI
Requirements of explanations motivate and steer abundant research from govern-
ments, industry, and academics. For example, European General Data Protection
Regulation(GDPR) [15] states that the individual will have “The right to the logic
involved in any automatic personal data processing” and “The right, ..., to obtain
an explanation of the decision reached after such assessment”; Equal Credit Op-
portunity Act in the US requires creditors to provide the reasons behind decisions
of credit deny for an applicant; US Defense Advanced Research Project Agency

Chapter 1. Introduction
3
(DARPA) [16] provides abundant funding to support the research on AI explana-
tions, incorporating with top universities worldwide; Forbes published an article
[17] named “understanding explainable AI”, which discussed the invisibility and
public concerns on AI systems and proposed some general methods to make the AI
models transparent. As a torchbearer in XAI research, Google [18] released a white
paper, which highlights the importance of AI explanations and introduces state-
of-the-art algorithms and the representative progress of Google. IBM launched
the AIX360 toolkit [19] to integrate a series of methods of interpretability and
explainability.
At the same time, XAI has attracted great interest from researchers. Many work-
shops in top conferences are hosted annually to share promising approaches and
discuss the prospects and challenges together. In particular, ACM Conference on
Fairness, Accountability, and Transparency (ACM FaccT) is hosted annually to fo-
cus on transparent machine learning, fairness/ethics in machine learning, learning
bias, etc. Figure 1.1 shows the remarkable surge of publication numbers in the last
7 years, searched from Dimensions AI 1 with keywords “explainable artiﬁcial intel-
ligence”, “explainable AI”, “interpretable artiﬁcial intelligence”, “interpretability”.
1.1.3
“Explanability” vs “Interpretability”
Before establishing the grounds of XAI, it is essential to clarify and unify several
terminologies. For example, the terms “Explainability” and “Interpretability” seem
closely related but stress di↵erent aspects of XAI. These di↵erences are subtle
but crucial to understand the nuances of XAI across multiple disciplines, such as
machine learning, cognitive science, and human-computer interaction.
• Explainability refers to an action/interface to provide post-hoc justiﬁca-
tions for the model’s behavior so that users can better understand the model
when the model is complex and black-box. For example, we have complex
deep models and do not understand which parts of features impact the pre-
diction. However, through explanation methods, like LIME [20], SHAP [21],
we understand the importance of each feature to the model prediction.
1https://app.dimensions.ai/analytics/publication/overview/timeline

4
1.1. Research Background
Figure 1.1: Number of publications in the past 7 years, searching with four
keywords “explainable artiﬁcial intelligence”, “explainable AI”, “interpretable
artiﬁcial intelligence” and “interpretability” on Dimensions AI website.
• Interpretability describes the model’s ability to explain its internal mech-
anism or provide an understandable decision-making process to users. In-
terpretability depicts a self-disclosure model that is explicitly comprehensive
and does not require extra interfaces to open it. For example, one builds a
model; and he can exactly understand the cause and e↵ect and ﬁgure out the
prediction process given a change of input. This model is called the highly
interpretable model.
• Transparency: the characteristics that the inner mechanism of a machine
learning model is visible and accessible to humans.
Explainability stresses that users should ﬁnd a mechanism to identify understand-
able information about a model while interpretability does not need such a mech-
anism and understandable information is intrinsically provided. Next, we clarify
interpretability and transparency.
Transparency implies that the model’s inner
workings are clear and comprehensible, but it does not necessarily mean that users
can easily interpret the model’s behavior. For example, a decision tree with more
than 1 million nodes is transparent but not interpretable. In contrast, interpretabil-
ity focuses on the ability of users to understand and explain the model’s behavior,

Chapter 1. Introduction
5
even if the inner workings are not fully transparent. However, the border between
interpretability and transparency is vague in most situations and these two terms
are often interchangeably used.
1.1.4
Pipeline of XAI
Data 
X
Black-Box Models
f(.)
Explanation 
Methods g()
Prediction 
f(x)
Explanations
Users
Global
Local
Figure 1.2: Pipeline of XAI.
To better understand the concept of XAI, we present the general pipeline of an
XAI system, as shown in Figure 1.2. First, a black-box model is trained on a
labeled dataset and produces a prediction for a test instance. However, due to the
opaque nature of the model, it can be diﬃcult for users to understand how the
model arrived at the prediction. An explanation method is then applied to the
black-box model, generating an understandable explanation that can help users
understand the model’s reasoning. This explanation method can be applied either
globally to the entire model or locally to a speciﬁc prediction. Additionally, some
explanation methods may make use of the dataset to generate explanations that
are more plausible or relevant to the speciﬁc instance.
It’s important to note that models that are inherently transparent do not require
the use of an explanation method, such as decision trees or linear models, which
may provide explanations by design.

6
1.1. Research Background
1.1.5
Taxonomies of XAI
As we stated previously, XAI is a general and abstract concept that contains various
works to make models understandable and trustworthy. Next, we introduce the
following criteria to archive the existing XAI approaches.
• Pre-hoc vs post-hoc. Pre-hoc XAI refers to the design of an interpretable
model where the model’s architecture and training process are intended to
be transparent and interpretable from the beginning.
In contrast, ”post-
hoc XAI” refers to the process of explaining a pre-trained black-box model
whose inner workings are opaque. The aim of post-hoc XAI is to provide an
explanation for the model’s decision-making process, without modifying the
model itself.
• Model-speciﬁc vs model-agnostic. Model-speciﬁc XAI refers to expla-
nation methods designed for a speciﬁc class of models, such as deep neural
networks or linear models. These methods leverage the unique properties
of the model to produce explanations. In contrast, model-agnostic XAI is
designed to work for any model, without considering the speciﬁc properties
of the model. Usually, model-agnostic XAI only requires the ability to make
predictions for a given input.
• Global vs local. Global XAI refers to the explanation of a model’s be-
havior at a global level, such as understanding its overall decision-making
process. In this regard, global XAI is not speciﬁc to any input instance. Lo-
cal XAI focuses on explaining the prediction of an input instance, typically
by highlighting the input features that the model relied on the most for that
prediction.
Following the survey paper [22], XAI can be categorized into two major ﬁelds:
pre-hoc explanation and post-hoc explanation. Pre-hoc explanation targets to de-
sign an interpretable model that humans can understand directly, such as decision
tree [23], decision rule [24], linear classiﬁers, generalized linear models [25], while
post-hoc explanation intends to explain a pretrained black-box model. More specif-
ically, post-hoc explanation can be divided into model inspection, model explana-
tion, concepts explanation, and outcome explanation. Model inspection examines
the relations between the model prediction and attributes. For example, partial

Chapter 1. Introduction
7
Explainable AI
Pre-hoc Explanation
Post-hoc Explanation
Model Inspection
Outcome Explanation
Feature Importance
Counterfactual Explanation
Model Explanation
Interpretable Models
Concepts Explanation
Figure 1.3: Taxonomy of XAI and our research interest (in the orange back-
ground).
dependence plot (PDP) [26] shows the marginal e↵ect of a model’s prediction on 1
or 2 features; individual conditional expectation (ICE) [27] shows the dependence
of the prediction on a feature for a given instance. Model explanation refers to
approximating the whole logic of the black-box model through an interpretable
and transparent model. Concepts explanation [28, 29] aims to extract meaning-
ful and high-level concepts that are used in model prediction. Outcome explana-
tion [20, 21, 30] mainly seeks to explain the outcome of black-box models on a
single instance. Model explanation aims to explain the model on the global level,
whereas the outcome explanation explains a single instance locally. There are two
mainstream ways to explain an instance: (1) feature attribution (e.g., LIME [20],
Integrated Gradient [30], Shap [21]) identiﬁes the importance of input features
to the current prediction. (2) Counterfactual explanation [31] o↵ers data points
that remain proximal to the input but lead to a di↵erent prediction. The research
taxonomy of XAI is shown in Figure 1.3.
1.2
Counterfactual Explanations
After providing a general overview of XAI, we narrow our focus to counterfactual
explanation, which is the main topic of this thesis. We begin by discussing the
motivations for using counterfactual explanations; then formally deﬁne the problem
mathematically; after that, we survey existing research on it; ﬁnally, we discuss

8
1.2. Counterfactual Explanations
implications and challenges of this problem and our corresponding solutions to
mitigate these challenges.
1.2.1
Demands
Counterfactual explanation [31] is a branch of outcome explanation that aims to
answer the question, “If certain changes (X) had not occurred, would the current
prediction (Y) still hold?” Instead of explaining why a given model makes a certain
prediction, as in LRP [32], LIME [20], and Integrated Gradient [30], counterfactual
explanations focus on explaining why a model made a speciﬁc prediction instead
of another possible prediction. The current prediction is the reality that stands
in contrast to a hypothetical alternative prediction. Therefore, these explanations
are called “counterfactual” because they explain why a model made a speciﬁc
prediction and not an alternative one.
According to a recent study [33], B. Miller drew a conclusion that when people ask
“why did event P happen”, they essentially ask “why did event P happen instead
of an absence of event Q”. P is the fact that did occur while Q is the foil that did
not occur. For example, one asks the question “why does Bob open the door?”,
but actually he wants to ask “why does Bob open the door rather than turn on
the air conditioning?”, “why does Bob open the door, not the window” or “why
does Bob open the door rather than leave it open”. Before he asks the question,
he already has an expectation di↵erent from the fact “open the door”. Such con-
trastive explanations are signiﬁcant to explain the black-box models. On the one
hand, these explanations identify what should users do to achieve an expected out-
come, which is quite crucial when users receive adverse predictions from black-box
models or are hurt by automatic decision systems. On the other hand, counterfac-
tual explanations are cognitively satisﬁable to both users and explainers because
such explanations ﬁll the cognition gap between the occurrence and counterfactual
alternatives. Therefore, we center on counterfactual explanations in this thesis due
to their distinct properties.
1.2.2
Deﬁnition
In the following, we give a formal deﬁnition of counterfactual explanations.

Chapter 1. Introduction
9
Deﬁnition 1.1 (Counterfactual Explanations). Given a data point x and a pre-
trained model f : X ✓Rd ! Y which maps an input instance x 2 X into a
prediction y 2 Y. X ✓Rd denotes the d-dimensional feature space and Y denotes
the prediction space. The output space Y is deﬁned as Y ✓R for regression tasks.
For classiﬁcation tasks, the output space Y is deﬁned as Y = {1, 2, ..., K}, where K
is the number of classes. A counterfactual explanation method [31] g : f ⇥X ! X
ﬁnds an improved example c that is close to x but leads to a di↵erent prediction
y,
arg min
c
cost(x, c)
(1.1)
s.t. f(c) = y
where cost( · , · ) : X ⇥X ! R+ is a distance or cost metric that quantiﬁes the
e↵orts required in order to change from an input x to its CFE c. In practice, the
commonly used cost function includes  L1/MAD [31, 34], total log-percentile shift
[35], and  L2 norm on latent space [36]. To optimize Eqn. (1.1), it can be further
transformed to the Lagrangian form [31], as shown below:
arg min
c
max
λ
λ`(f(c), y) + cost(x, c)
(1.2)
where `( · , · ) is a di↵erential function to measure the gap between f(c) and the
favorable prediction y, and λ is a positive trade-o↵factor. Maximization over λ is
achieved via searching c iteratively and increasing λ until a suﬃcient small loss is
found. By optimizing the above objective, a CFE method g(f, x) returns a set of
CFEs for an input x.
Essentially, a counterfactual explanation can be regarded as a synthetic instance
that approximates the input instance but belongs to a di↵erent prediction. As
minimal perturbations can guide users to achieve a di↵erent prediction, it has many
applications. The following two examples illustrate the use cases of counterfactual
explanations.
• Example 1 : Bob wants to apply for a loan for buying a house. The bank
deployed a binary classiﬁcation model to help make an automatic decision.
After submitting his personal information (education, salary, credit score,

10
1.2. Counterfactual Explanations
Figure 1.4: An example of counterfactual explanations in loan applications.
family number, work type, etc.) to the bank, the model denied his application
due to his low salary and credit score. According to regulations and laws,
the bank should provide explanations about the rejection. A counterfactual
explanation may look like “increasing the salary by 10, 000”. As shown in
Figure 1.4, a counterfactual explanation will guide Bob to get the loan from
the decision model if he takes this suggestion.
• Example 2: Alice is diagnosed with a high risk of hyperlipidemia by a ma-
chine learning model deployed in the hospital after providing her personal
status (height, calorie intake, weight, exercise, sleep hours, etc.). Alice wants
to reduce the risk. A counterfactual explanation gives such suggestions: “jog-
ging 2 kilometers every two days, reducing fat intake by 10%, and sleeping
one more hour”. Following the explanation, Alice will quickly recover with
the minimum cost in a short period.
Counterfactual explanations have potential applications across a variety of do-
mains. For instance, in healthcare, they can help identify how to alter an unhealthy
situation to a healthy one, thereby enabling better health outcomes and reducing
medical costs [37, 38]. In the ﬁeld of justice, counterfactual explanations can help

Chapter 1. Introduction
11
address fairness concerns related to machine learning-based decision-making [39].
In marketing, they can be used to improve customer retention and sales [40].
1.2.3
Landscapes
Counterfactual explanations are also studied under other terminologies, which can
date back to the 1970s. Before 2017, this problem was mainly studied under “in-
verse classiﬁcation” [41–43] but received less attention. After black-box models are
increasingly deployed for automatic decisions, multiple independent approaches are
proposed to provide this kind of explanation between 2017 and 2018, but with dif-
ferent names (“counterfactual explanation” [31], “recourse” [35], and “contrastive
explanation” [44, 45]). Fundamentally, these terms capture the same gist, only with
slight di↵erences. The major theme of recent research is to make counterfactual
explanations more actionable, interpretable, and acceptable, Following [31], much
research has gone into the following two aspects: modeling the practical require-
ments and adopting various solving methods. A more detailed description can be
found in the literature review chapter 2.
• Modeling practical requirements. As not all features (race, gender) can
be changed, [35] enforces the actionability by ﬁxing the immutable features.
[36, 46] state that counterfactual explanations should follow the data dis-
tribution to guarantee the plausibility. Instead of returning a single result,
[31, 47] suggest providing a diverse set to cover the wide requirements that
are hard to express clearly. [44, 48] enforce sparsity by adding L0 or L1 norm
terms to penalize the changes over many features. [46, 49–51] consider causal
relations between features when some features are a↵ected by other features.
• Adopting various solving methods. Once the constraints are added, the
next step concentrates on how to solve the constrained optimization prob-
lem. The solving method depends on the data type, model type, and level
of model access. For a linear model, integer programming [35] or mixed in-
teger programming [34] is applied when some feature values are integers or
categories. For a nonlinear model, gradient descent can be adopted [31] if
the model is di↵erentiable. Otherwise, heuristic search methods like growing
spheres [52], feature tweaking [53], FISTA[44, 48], Dijkstra’s algorithm [54]
can be used.

12
1.2. Counterfactual Explanations
Users
Data
Model
Counterfactual
Explanations
Figure 1.5: Counterfactual explanations bridge the model prediction and user
behaviors.
1.2.4
Implications
Counterfactual explanations have signiﬁcant impacts on both machine learning
research and practical applications. Here are some advantages of counterfactual
explanations: Firstly, they help in understanding the predictions of models [55] by
probing local behaviors and identifying the attributes that strongly determine the
desired decision. Secondly, counterfactual explanations can detect bias or prejudice
in black-box models by perturbing protected or immutable features such as gen-
der, race, religion, etc., which can achieve the desired prediction. Developers can
use this information to debug and ﬁx bias issues. Thirdly, counterfactual explana-
tions can provide suggestions to users receiving adverse predictions. The minimal
perturbations can guide users to achieve the desired prediction in the future by
adopting certain actions. Lastly, counterfactual explanations o↵er a potential way
to comply with regulations and laws for machine learning communities, such as
GDPR [15, 31].
Counterfactual explanations also fall within the research scope of Human-Centered
AI, by o↵ering useful solutions that match users’ needs and help them beneﬁt from
following these solutions.
Figure 1.5 illustrates the workﬂow of counterfactual
explanations in the life cycle of an AI service, showing how they can bridge the gap
between model predictions and user behaviors. Traditional AI models stop at model

Chapter 1. Introduction
13
prediction, whereas counterfactual explanations provide actionable suggestions for
the user to improve the current prediction. This creates a loop where users take
feasible actions towards a better target, the better data help reﬁne the model, and
a better model gives a more concise explanation in turn. In each loop, users are no
longer passive participants but the major beneﬁciaries of the system, leading to a
more Human-Centered AI approach.
1.3
Research Challenges
Although this problem has been extensively studied, several challenges remain to
be addressed, which motivate our research in subsequent chapters. Speciﬁcally,
these challenges include:
• Inspecting the black-box model f. Current research usually assumes
that the model f is frozen. Essentially, the model f serves as a tutor that
guides users to achieve the desired prediction.
The hidden assumption is
that the model is trustworthy and with high performance.
However, the
frequent models used are neural networks and ensemble models, whose in-
terpretability/trustability is poor. Therefore, we should inspect the model
comprehensively to prevent generating counterfactual explanations under a
ﬂawed model. Global explanations identify general explanations that are not
applicable to all users while local explanations identify important features
for an instance. Examining a large number of users is time-consuming. We
should inspect the black box in a more eﬃcient and e↵ective way.
• Specifying the realistic cost/distance function.
In real-life applica-
tions, we often face heterogeneous datasets where di↵erent types of features
are mixed. It is challenging to deﬁne a single scalar distance function for
measuring the cost required. There are three principal diﬃculties in deﬁning
the distance function. First, it is hard to trade o↵two incomparable features,
like “Education year” and “Salary”. Second, the cost of change often depends
on the current feature value. For example, improving the exam score from
50 to 60 is much easier than it is from 90 to 100. Third, feature correlation
and causality often exist. We cannot simply sum all feature changes up as

14
1.4. Overview of Contributions
the ﬁnal cost. We should design a deliberate function to mitigate the above
issues.
• Avoiding model extraction attack. Model security and privacy are sig-
niﬁcant for safety-critical applications. If hackers can easily replicate the pre-
trained model with a counterfactual explanation interface, they will tamper
with the data to bypass the AI system or directly use the substitute model
without expensive investigation. Currently, few works study this problem.
Some work [31, 56, 57] simply claims that counterfactual explanation allows
individuals to receive explanations without conveying the internal logic of
the algorithmic black box. A recent study [6] attempts to extract the model
by considering counterfactual explanations as training data. However, this
model security problem should be studied further with profound analysis or
strict proof.
1.4
Overview of Contributions
In this thesis, we aim to produce actionable, reliable counterfactual explanations
for black-box models by addressing the above challenges, so that such explanations
can be deployed safely.
First, we start with inspecting the model to enhance trust in black-box models,
which is the premise of good counterfactual explanations. If the important features
under the prediction align with domain knowledge and expert understanding, we
can believe this model with higher conﬁdence. Post-hoc explanation methods are
needed to mitigate distrust. However, existing post-hoc explanation methods either
provide global feature importance scores for the whole population or provide local
feature importance scores for a single instance. In addition, the important features
of some groups are not always consistent with the important features of the whole.
For example, in the Titanic wreck, most of the passengers survive because of the
ﬁrst-class cabin, but some people survive because of gender or age. The local feature
importance is ﬁne-grain, but reading it individually for large datasets is quite time-
consuming. We assume that there exist some groups where each group shares the
same important features while di↵erent groups have distinct important features.
Based on the assumption, we propose the group summarization algorithm which

Chapter 1. Introduction
15
ﬁnds k groups with each group being summarized by l most important features
from a large feature importance matrix in chapter 3. From the k ⇥l summary, we
can easily inspect the model at a compact group level.
Second, the existing counterfactual explanation methods su↵er from two major
limitations: (1) the distance function cost( · , · ) over heterogeneous features is hard
to deﬁne, because of the trade-o↵factors between the incomparable features and
the inﬂuence of the current feature value; (2) the user constraints are diﬃcult to
estimate precisely, especially when lacking the domain knowledge and alternative
solution available. To overcome these limitations, we formulate this problem by
ﬁnding non-dominated counterfactual explanations with minimum changes on each
actionable feature. Instead of returning a single or a few explanations like previous
methods, our method provides a set of non-dominated counterfactual explanations,
a.k.a. the skyline of counterfactual explanations. Initially, we only restrict the basic
requirements and ignore unclear user-related constraints, which leave the maximum
set of alternative explanations. With the skyline on hand, users have opportunities
to explore suitable results incrementally and interactively. Details of the proposed
method refer to chapter 4.
Third, we study the model security of counterfactual explanations.
The secu-
rity of counterfactual explanations has not received much attention. We observe
that counterfactual explanations may disclose the decision boundary of the model
that enhances the risk to be attacked. We propose an eﬃcient model extraction
algorithm to extract a high-ﬁdelity model via a counterfactual explanation inter-
face. Our method treats counterfactual explanation (CF) as an input to query the
existing method and obtains the counterfactual explanation of counterfactual ex-
planation (CCF). With pairs of CFs and CCFs as training samples for extraction,
we can replicate the black-box model eﬃciently. Our method raises awareness of
the model security issue of counterfactual explanations, which motivates further
work on countermeasures.
Research Connections. As shown in Eqn. (1.1), the counterfactual explanation
problem includes two crucial components: the model f in the constraint and the
cost function cost( · , · ) in the objective. The ﬁrst study proposes a group-level
summary of the feature importance matrix, which facilitates the inspection of the
model f in the constraint. In the second study, we address the diﬃculty of deﬁning
the cost( · , · ) function on heterogeneous data. Finally, our third study focuses on

16
1.5. Research Outlines
the security issue of the model f to motivate safety-aware counterfactual expla-
nations. Although these studies are not suﬃcient for the real-life deployment of
counterfactual explanations, they are critical in paving the way by addressing some
of the signiﬁcant problems.
1.5
Research Outlines
The thesis is organized as follows:
• In chapter 1, we provide an introduction to the research background, deﬁne
the problem, discuss the research challenges, and outline the contributions
of this thesis. Additionally, we provide a clearer outline of the publications
related to this research topic.
• In chapter 2, we present a detailed review of the existing literature on coun-
terfactual explanations, and summarize recent advancements in the ﬁeld,
which covers both problem formulation and solving. In addition, we also
cover feature importance explanations, which aim to explain the importance
of input features for a given prediction. We also explore research on model
security and privacy, as these topics are closely related to the development
and deployment of counterfactual explanations.
• In chapter 3, we introduce our research that was published by ICDM 2021
[58] and TKDD 2023 [59], which addresses the cost of examining local expla-
nations on a large number of users. Speciﬁcally, we propose a summarization
method that summarizes all local explanations on the group level. We pro-
pose two methods: the Greedy algorithm that provides an approximation
guarantee for a nonnegative matrix, and the k-max algorithm that runs eﬃ-
ciently for larger matrices. In addition, we also extend this method to more
general summarization tasks. Our approach could enhance trust in models
when we seek counterfactual explanations from black-box models.
• In chapter 4, we present our research that was published by CIKM 2021 [60],
to address the diﬃculty to deﬁne a scalar cost function over heterogeneous
data. We formulate the counterfactual explanation problem by minimizing

Chapter 1. Introduction
17
the change of each incomparable feature under the multiobjective optimiza-
tion framework and return the skyline of counterfactual explanations. The
skyline of counterfactual explanations can allow users to explore their pref-
erences and requirements incrementally and interactively.
• In chapter 5, we demonstrate our research that was published by FaCCT
2022 [61], to study the security and privacy of counterfactual explanations in
deployment. In this chapter, we present an eﬃcient model extraction attack
algorithm to replicate the AI model from counterfactual explanation APIs
with fewer queries than state-of-the-art methods. Our method points out
the decision boundary shift issue of existing research and then proposes an
algorithm named DualCF which considers pairs of CFs and CCFs for the
substitute model training to overcome that issue.
• In chapter 6, we summarize the main contributions and ﬁndings of this the-
sis.
Additionally, we highlight some important research challenges to be
addressed, which could present promising opportunities for future research
and development in this ﬁeld.


Chapter 2
Literature Review
In this chapter, we conduct a comprehensive survey of research on both feature
attribution explanations and counterfactual explanations, which are two critical
post-hoc explanation topics for providing local explanations of the prediction of an
input. Feature attribution explanations explain why a model makes the current
prediction by identifying important features of the prediction, while counterfactual
explanations explain how a model can make another prediction with minimal cost
by perturbing the input. We primarily focus on counterfactual explanations, start-
ing with problem formulation and then covering various approaches to problem-
solving. While most existing explainability approaches focus on di↵erent strategies
to open black-box models, they may inadvertently increase the risk of model at-
tacks. Hence, we also review previous works on the security and privacy of ML
models to establish the foundation for model attack and protection. Our survey
aims to provide readers with a comprehensive understanding of the state-of-the-art
research that is related to our thesis.
2.1
Feature Attribution Explanations
Feature attribution methods target to obtain attribution scores, denoted by el(f, x) 2
Rd, for a black-box pretrained model f and a given instance x 2 Rd. Not required
to understand the overall logic of black-box models, feature attribution methods
merely explain the prediction for an instance by measuring the importance of each
19

20
2.1. Feature Attribution Explanations
feature to its prediction. Often, a higher importance score indicates a stronger
contribution to the prediction. Formally, this problem is deﬁned as follows,
Deﬁnition 2.1 (Feature Attribution Explanations). Given a black-box model f :
X ✓Rd ! Y that maps an instance x 2 X to an output f(x) 2 Y, a feature
attribution method el : f ⇥X ! Rd aim to obtain importance scores el(f, x) 2 Rd
for the current prediction f(x) with regard to an instance x.
For example, for an input x = [x1, x2, ..., xd], a feature attribution method re-
turns a feature importance vector a = [a1, a2, ..., ad], where the ai measures the
contribution of feature xi to the prediction f(x). For image and text data, im-
portance vectors are usually visualized by heat maps or saliency maps, as shown
in Figure 2.1. According to computational di↵erences of feature importance vec-
tor, feature attribution methods can be divided into the following four categories:
perturbation-based methods, gradient-based methods, decomposition-based methods,
and approximation-based methods. Perturbation-based methods measure feature im-
portance by perturbing certain features ﬁrst and observing how the feature changes
inﬂuence the prediction; Gradient-based methods view gradients (variants of gra-
dients) of prediction with respect to an input as feature importance. The hidden
assumption is that gradient-based methods require the model to be di↵erential.
Decomposition-based methods identify the feature importance by a series of opera-
tions that rollback model prediction to input features. Such operations decompose
the output of each layer into di↵erent pieces on the input features; Approximation-
based methods use an interpretable model (e.g. a linear model and a decision tree)
to approximate the local behavior of an input instance. The coeﬃcients of linear
models or decision paths of decision trees are used to measure the importance of
input features to the ﬁnal prediction. In the following, we introduce more details
about them.
2.1.1
Perturbation-based methods
Perturbation-based methods deﬁne the feature importance by removing, masking,
altering a fraction of feature values, and observing the impact on prediction from a
forward pass. If a feature perturbation is associated with a large drop in prediction,
this feature has large importance and verse vice.

Chapter 2. Literature Review
21
y=sci.med (probability 0.996, score 5.826)
Figure 2.1:
Heatmaps and saliency maps produced by feature attribution
methods on images and texts. Important regions in heatmaps are highlighted
with red colors. Positive/negative signiﬁcant words in the saliency map below
are masked in green/red respectively.
Zeiler et al. [2] introduce an occlusion method by occluding di↵erent segments of an
input image and visualizing the change in the activation of subsequent layers. Let x
represent the input instance and x( · , j) represent the occluded input by replacing
feature j value with an uninformative value. For a black-box model f, the feature
importance score for feature j is proportional to the prediction di↵erence,
ej
l (f, x) _ f(x( · , j)) −f(x)
(2.1)
Similarly, Fong et al. [62] learn the maximally informative masks such that the
model’s prediction will drop sharply after deletion. Perturbation-based methods
are simple and straightforward to understand and leverage. It only requires the
prediction interface, regardless of model’s internal details and model types. How-
ever, perturbation-based methods are computationally expensive and ﬁnal results
are a↵ected by the selection of uninformative values to replace because there are
exponential choices of perturbations. Besides, the “uninformative” is unclear for
di↵erent scenarios and there is no hint for “uninformative” on tabular datasets.
For example, researchers often replace current feature values with 0, but 0 some-
times (e.g. deposit, temperature) can be meaningful. Another disadvantage is that
improper occlusion will bring unrealistic samples out of the data distribution, and
the model may give a wrong prediction to it.

22
2.1. Feature Attribution Explanations
Figure 2.2: The di↵erences between back-propagation [1], deconvnet [2] and
guided back-propagation [3].
2.1.2
Gradient-based methods
Gradient explains a model behavior upon an inﬁnitesimal perturbation, a.k.a., sen-
sitivity of the model around a point. Gradient-based explanations adopt gradients
(or variants of gradients) of prediction with respect to an input instance x to
explain the model.
Simonyan et al. [1] propose an image-speciﬁc class saliency visualization method
(back-propagation). Given an image I0, a predicted class c, and a classiﬁcation Con-
vNet with the class score Sc(I), it deﬁnes the feature importance by the gradient
of output with regard to input pixels in the following,
el(f, x) = @Sc(I)
@I
!!!!
I0
(2.2)
Zeiler et al. [2] visualize the saliency map (feature importance of image) through
the deconvolution network, named deconvnet. This method back-propagates an
activation map by successive layers (unpooling/rectiﬁcation/deconvotional) from
the output layer until it reaches the pixel space. Springenberg et al. [63] introduce
guided back-propagation, which modiﬁes the backward pass of the ReLU layer. The
di↵erences between deconvnet, back-propagation and guided back-propagation are
shown in Figure 2.2.
Deconvnet zeros out negative gradients from the bottom
layers during backward while back-propagation sets the gradient as 0 if the input of
ReLU is negative. Guided back-propagation reserves the intersection of propagated

Chapter 2. Literature Review
23
gradient of deconvnet and back-propagation.
Class activation maps (CAM) [3]
back-propagates the predicted class score to the last convolutional layer and then
upsamples to the input size. SmoothGrad [64] averages the gradient of samples in
the neighborhood of the input to mitigate the noisy gradient issue.
All above gradient-based methods have the saturation issue [65], where the gradi-
ents of important features are close to zero even though these features are important
to prediction. To avoid this issue, researchers propose integrated gradient methods
where the feature importance is computed by accumulating the gradient of a path
from a reference point (also named baseline point) to the input. The reference
points represent the missing interest of an object, e.g., all-zero vectors, random
noise, Gaussian noise, and all-one vectors. Sundararajan et al. [30] present the
integrated gradient algorithm as shown as below. Suppose a deep network repre-
sented by F : Rn ! [0, 1] and x is the input and b is the baseline. The integrated
gradient (IG) along i-th dimension is deﬁned by,
IGi(x) ::= (xi −bi) ⇥
Z 1
↵=0
@F(b + ↵⇥(x −b))
@xi
d↵
(2.3)
Here,
@F(x)
@xi
is the gradient of F(x) along the i-th feature.
Fundamentally, IG
computes the accumulated gradient along the straight line from b to x.
One
desired property is completeness that the importance of each feature adds up to
the F(x) −F(b).
Considering the baseline is hard to determine. BlurIG [66] modiﬁes the integral
path of IG by successively blurring the input image with a Gaussian ﬁlter. Formally,
a blurred image of the input after a 2D Gaussian kernel with a variance ↵can be
written as,
L(x, y, ↵) ::=
1
X
m=−1
1
X
n=−1
1
⇡↵e
−(x2+y2)
↵
z(x −m, y −n)
(2.4)
where z( · ) is the 2D input and x, y determine the location of an image. Then
BlurIG is deﬁned as the integral along the path of ↵from inﬁnity to zero,
BlurIG(x, y) ::=
Z 0
↵=1
@F(L(x, y, ↵))
@L(x, y, ↵)
@L(x, y, ↵)
@↵
d↵
(2.5)

24
2.1. Feature Attribution Explanations
Even though IG and BlurIG overcome the gradient saturation, the choice of refer-
ence points determines the ﬁnal explanation [65]. The reference points denote the
missingness of interest, but the “missingness” is not straightforward for semantic
recognition. More discussion about missingness should be explored.
2.1.3
Decomposition-based methods
Decomposition-based methods explain deep neural network models by propagat-
ing the prediction backwards layer by layer. In each layer, the back-propagation
operation should satisfy the deﬁned properties. For example, LRP [32] requires
that (1) the received information of each neuron from the higher layer should be
allocated to all connected neurons of the lower layers, as shown in Eq. (2.6). (2)
the amount of information redistributed backward should be proportional to the
information ﬂowed in the forward pass.
f(x) =
X
i2l
R(l)
i
(2.6)
where Ri denotes the i-th layer and the superscript l denotes the l-th neuron in this
layer. [67, 68] ﬁnd that the Valina LRP without considering numerical stability
equals the gradient ⇥input.
Similarly, DeepLIFT [69] and DeepTaylor [70] incorporate the concept of reference
points into decomposition. Decomposition-based methods are robust to architec-
tures of neural networks and easy to consider prior knowledge into decomposition.
However, these methods require all network parameters in advance, which does not
work when only the prediction interface is available.
2.1.4
Approximation-based methods
Even though black-box models are often nonlinear and complex, the decision
boundary can be linear locally. Grounding on this assumption, approximation-
based methods aim to learn an interpretable model from the samples in the neigh-
borhood of input. The learned model behaves similarly as the black-box model in
order to faithfully explain the black-box model.

Chapter 2. Literature Review
25
One of the most famous approximation-based methods is LIME [20]. LIME con-
siders both the local ﬁdelity and complexity of the interpretable model in the
following Equation 2.7, where L(f, g, ⇡x) measures how faithful the interpretable
model g approximates the black-box model f in the vicinity determined by ⇡x,
and ⌦measures the complexity of the interpretable model g and G is a class of
interpretable models. To learn the function g, LIME samples several instances in
the vicinity of x, where the closer samples are assigned with higher weights and the
far ones with lower weights. Then, LIME minimizes the following loss by gradient
descent.
✏(x) = argming2GL(f, g, ⇡x) + ⌦(g)
(2.7)
LORE (LOcal Rule-based Explanations) [71], is proposed to learn a series of rules
through a genetic algorithm. SHAP [21] does not explicitly learn the additive linear
model by minimizing the loss of samples while coeﬃcients of the linear model are
computed by the Shapley values. Approximation-based methods aim to train an
interpretable model with samples in the neighborhood of an input instance. The
greatest advantage is that it is model-agnostic, only requiring the prediction of a
black-box model. However, how to determine the sample points in the neighbor-
hood and assign the relative weights of all samples should be studied further.
2.2
Counterfactual Explanations
Counterfactual explanations, in contrast to feature attribution methods that an-
swer the question “Why does the model make this prediction?”, aim to answer
“Why does the model make this prediction instead of something else?” Counter-
factual explanations describe states that did not occur but could have occurred.
They can be seen as conditional statements where the antecedent is not true, and
the consequences describe what would have happened if the antecedent were true.
As the counterfactual explanation problem is also studied under other technologies,
we will ﬁrst review closely related research topics and distinguish between some
topics that may sound related but are actually di↵erent. Remember the Lagrange
form in Eqn. (1.2), where there terms, `(f(c), y), cost(x, c), and other constraints,

26
2.2. Counterfactual Explanations
together determine the ﬁnal formulation of the counterfactual explanation prob-
lem. Di↵erent choices of three terms are applicable to certain models and data,
and have di↵erent e↵ects.
After these three terms are determined, researchers
adopt di↵erent solving methods to ﬁnd satisﬁed counterfactual explanations. In
the following, we review common formulations of both terms and popular solving
methods accordingly.
2.2.1
Research Scope
Counterfactual explanations are also studied under other terminologies, which can
date back to the 1970s [72]. Before 2017, this problem was primarily studied un-
der “inverse classiﬁcation” but received less attention [41, 43]. After black-box
models are increasingly deployed for making automatic decisions in high-stakes
scenarios, multiple independent approaches are proposed for the counterfactual
explanation problem between 2017 and 2018, but with di↵erent names, “counter-
factual explanation”, “recourse”, and “contrastive explanation”. Fundamentally,
these terminologies capture the same gist only with slight di↵erences. As “coun-
terfactual explanations” appears more frequently, we unify existing terminologies
under the umbrella of this name in our thesis. Besides, we also identify related
ﬁelds that are di↵erent from counterfactual explanations.
• Inverse Classiﬁcation [41, 43, 52, 73] targets to ﬁnd the closest observa-
tion classiﬁed to a di↵erent class. This term describes the same destination
in counterfactual explanations. One thing that needs to be mentioned is that
Aggarwal et al. [42] use this term to describe a problem that aims to restore
missing values of an incompletely speciﬁed test dataset for the desired target.
That paper [42] aims to determine the missing feature variables to maximize
the accuracy of the desired class, di↵erent from minimizing the cost of per-
turbations in counterfactual explanations. Readers should be cautious of this
term in di↵erent problem contexts.
• Contrastive explanations [44, 45] aims to identify the minimal and neces-
sary absence of features, named by Pertinent Negative (PN), from an input
instance but with a di↵erent classiﬁcation.
Di↵erent from counterfactual
explanations, contrastive explanations require that the perturbed inputs af-
ter PN should resemble existing observations in the data distribution. This

Chapter 2. Literature Review
27
problem is the same with counterfactual explanations with the plausibility
constraint.
• Recourse [35] highlights the actionable recommendations that an individual
who receives unfavorable predictions can undertake for the desired predic-
tion. In this regard, recourse should take more practical requirements, like
immutable features, and causal relations between features, into account. Pro-
viding counterfactual explanations seems to be simpler than o↵ering recourse.
However, in certain applications, counterfactual explanations can also include
causality constraints [51, 56], and then counterfactual explanations are the
same as recourse.
• Nearest Unlike Neighbours (NUNs) [72, 74] are data points that reside
at or close to the decision boundaries but have di↵erent predictions from an
input. In this regard, NUNs can be seen as naive counterfactual explana-
tions. Di↵erently, NUNs are the observed data points in a dataset, whereas
counterfactual explanations can be synthetic data points. Correspondingly,
the search strategies are di↵erent. NUNs are often obtained from traditional
search processes and the optimal solution can be found. However, the search
for counterfactual explanations is more complex and ﬂexible.
The following research topics are seemingly related but with clear di↵erences from
counterfactual explanations. Next, we clarify them in detail.
• Adversarial learning [75] targets to fool a neural network via impercep-
tible perturbations of features by ﬂipping the current prediction. Although
counterfactual explanations also aim to ﬂip the prediction by changing some
features [56, 76], the motivation and goal are di↵erent from deceiving the
model, i.e., to ﬁnd changes that are plausible, explainable, actionable. For
a detailed comparison of the two problems, please refer to the survey paper
[76].
• Feature attribution [20, 32] aims to identify the most inﬂuential input
features for a particular prediction.
It addresses the question of why an
input x is classiﬁed as y. On the other hand, counterfactual explanations aim
to answer a di↵erent question: why an input x is classiﬁed as y instead of
another prediction y0. Although both problems aim to explain the prediction
of input, they deﬁne the problem from di↵erent perspectives.

28
2.2. Counterfactual Explanations
• Nearest Point Problem (NPP) [77] is stated to ﬁnd a closest point from
a set ⌦for a point x. NPP is a classic search problem that does not involve
a pre-trained model like in the counterfactual explanation problem.
2.2.2
Problem Formulation
As stated in Eq.(1.1), the formulation of the counterfactual explanation problem
consists of three components: (1) the cost measurement in the objective function;
(2) the loss function that gauges the di↵erence between the current prediction and
the desired prediction; (3) other constraints that reﬂect the practical desiderata,
which also is the major theme of recent research. All components are investigated to
adapt to di↵erent black-box models, data types, and to result in more actionable,
interpretable, and deployable counterfactual explanations.
In the following, we
detail the three components that are proposed in the existing research.
(1) Cost in the objective function. The cost measurement is used to evaluate
the e↵ort required from the input to its counterfactual explanations. To minimize
the e↵ort required in real-life scenarios, it is crucial to carefully design the cost
function. Here, we review commonly used cost functions in existing works.
•
`1 or `2 norm. A straightforward distance is `2 or `1 norm [41].
cost(x, c) = ||x −c||1 or ||x −c||2
(2.8)
• Weighted combination of `p norms. `0 norm measures the sparsity of fea-
ture changes, `1, `2 are elastic distance metrics, and `1 gauges the maximum
change over all features. In previous work [44, 48, 78], researchers propose a
weighted combination of them with the following format.
cost(x, c) =↵||x −c||0 + β||x −c||1
(2.9)
+ γ||x −c||2 + ⇣||x −c||1
where ↵, β, γ, ⇣are trade-o↵factors for balancing the above terms.
• `1/MAD. Several works [31, 34] have suggested leveraging the `1 distance,
weighted by the inverse median absolute deviation (MAD), in order to mini-
mize real-life e↵ort. This distance metric favors sparse results due to the `1

Chapter 2. Literature Review
29
norm, and dividing the MAD accounts for the varying scales in the feature
space while being robust to outliers.
cost(x, c) =
M
X
j=1
|xj −cj|
MADj
(2.10)
For a set of points X = {x1, x2, ..., xi, ...}, the median absolute deviation
(MAD) of j-th feature is deﬁned as follows,
MADj = median(|xj
i −median(Xj)|)
(2.11)
• `p norm on latent space. Pawelczyk et al. [36] propose using the `p norm
on latent space due to the diﬃculty to measure the closeness of heterogeneous
tabular data. Speciﬁcally, the distance is deﬁned as,
cost(x, c) = ||zx −zc||p
(2.12)
where zx and zc are low dimensional representations in latent space. In their
paper, they employ a variational autoencoder to map the raw data into the
latent space and generate counterfactual explanations adhering to the data
manifold.
• Mahalanobis’ Distance (MD) [79, 80]. Mahalanobis’ Distance (MD) is
a measure of distance between two points in a multivariate space. It takes
into account the correlation between variables as well as the scale of each
variable. Recent works [79, 80] have focused on using MD for counterfactual
explanations due to its ability to capture the inter-feature dependencies in
data, which is deﬁned as,
cost(x, c) = ||(x −c)TM(x −c)||
(2.13)
M is a positive semi-deﬁne matrix, which is the inverse matrix of the covari-
ance matrix ⌃of the dataset.
• Total log-percentile shift [35]. This function favors the “easy” change in
the target population. For example, improving the percentile of examination
score from 95 ! 100 is much harder than it is from 50 ! 55. This function
takes data density into consideration, where changing in the lower density

30
2.2. Counterfactual Explanations
will have a large cost. The cost function is deﬁned as,
cost(x, c) =
d
X
j=1
log( 1 −Qj(cj)
1 −Qj(xj))
(2.14)
where Qj(xj) is the empirical CDF of the j-th feature at x, and Qj(cj) is
the target CDF of the j-th feature at c. The function is log-transformed to
make it easier to optimize, and the use of CDFs makes it robust to outliers.
• Causal distance. Mahajan et al. [51] deﬁne the distance that preserves
causal relations between variables.
cost(x, c) =
X
u2U
costu(xu, cu) +
X
v2V
costv(xv, cv)
(2.15)
where U are exogenous variables and V are endogenous variables. Variables
in V will be a↵ected by their ancestor variables in U. For example, getting
a higher education degree will increase the monthly salary when submitting
both features “education” and “salary” for applying for a loan.
(2) Prediction Di↵erence Loss. This loss function quantiﬁes the disparity be-
tween the current prediction and the desired target. Typically, a search algorithm
begins by evaluating the predictions of initial data points. The di↵erence in predic-
tions indicates the distance between the initial points and the target, which guides
and enhances the search strategy. This approach facilitates the search for the next
point that is closer to the optimal counterfactual explanation in a more eﬃcient
and e↵ective manner. In the following sections, we review popular loss functions
that estimate prediction di↵erences.
• Zero-one loss [52]. If the prediction f(c) is not equal to the desired pre-
diction y0, this loss function returns 1, and 0 otherwise. As this loss is non-
di↵erentiable and discontinuous, it is not applicable to gradient-based solvers.
• Squared loss [31, 81].
This loss is typically used for regression models
or probabilistic classiﬁcation models. f(c) is the probability of the target
class or the continuous prediction of an explanation, and y0 is the probability
threshold of the target class or the desired prediction value. An advantage is

Chapter 2. Literature Review
31
that this loss is di↵erential.
`(f(c), y0) = (f(c) −y0)2
(2.16)
• Hinge loss [44, 47]. For a probabilistic classiﬁcation model, a counterfactual
explanation only requires that f(c) to be greater than a conﬁdence threshold.
However, the square loss penalties a higher probability above the threshold.
To solve this problem, [47] proposes the following hinge loss for a binary
classiﬁcation model.
`(f(c), y0) = max(0, 1 −z ⇤logit(f(c))
(2.17)
where z is −1 if the target class y0 is 0 and 1 when y0 = 1. logit(f(c)) is
the unscaled output from a model (before the softmax function). Similarly,
Dhurandhar et al.[44] presents a hinge-like loss function for the multiclass
classiﬁcation task,
`(f(c), y0) = max(f(y0|c) −max
i6=y0 f(i|c), −)
(2.18)
where f(y0|c) and f(i|c) are probabilities that belong to class y0 and i re-
spectively. This loss function favors that the probability of the target class
is greater than that of the second top class by . Otherwise, there will have
a nonzero loss in Eq.(2.18).
• Cross entropy loss [82–84]. This loss is widely used in training a neural
network model to measure the di↵erence between two distributions. Given a
counterfactual explanation for a classiﬁcation model, it is deﬁned as,
`(f(c), y0) = −log(f(y0|c))
(2.19)
Minimizing the cross entropy loss is equivalent to maximizing the probability
of the target class.
(3) Mathematical Constraints.
Here, we introduce optional constraints for
speciﬁc purposes. Rethink the previous example 1.2.2 in loan application. When
Bob wants to seek suggestions to get the loan approved, he may consider the fol-
lowing truth or questions: “family number” cannot be reduced, getting a higher

32
2.2. Counterfactual Explanations
education will probably improve his salary or work type, “credit score” seems easier
to change, and it is better not to change multiple features simultaneously. Such
desiderata, beyond the mentioned ones, have steered the recent development of
counterfactual explanations. The major theme of current research aims to formu-
late these desiderata by mathematical constraints and incorporate them into the
generation of counterfactual explanations to make them more reasonable, plausi-
ble, and acceptable. The followings are the popular mathematical constraints that
are proposed in recent studies.
• Actionability [35]. Considering certain features (e.g., race, gender, birth-
place) are inherently immutable, counterfactual explanations should avoid
changes on them. Therefore, the optimization problem is updated to take
the actionable space A into account.
arg min
c2A max
λ
λ`(f(c), y0) + cost(x, c)
(2.20)
• Sparsity. Sparser results are easier to execute and accept by users as changes
over too many features will overwhelm users’ understanding and downgrade
the explainability. Therefore, a counterfactual explanation should change a
smaller number of features. To achieve this, some works [44, 48] add `0 or
`1 norm to penalize the case where too many features are changed at a time.
The loss function with `1 norm is written as.
arg min
c
max
λ
λ`(f(c), y0) + cost(x, c) + γ||x −c||1
(2.21)
In addition, some researchers [47, 52] adopt post-processing to obtain sparser
results.
• Plausibility. Users do not trust unrealistic counterfactual explanations with
arbitrary combinations of feature values because these explanations never or
rarely exist in historical observations.
To enhance user trust, we should
consider plausibility, a.k.a. a counterfactual explanation that should follow
the data distribution and adhere to observed data points. Joshi et al. [46]
formulate the problem as the following objective function,
arg min
c|p(c)>γ
max
λ
λ`(f(c), y0) + cost(x, c)
(2.22)

Chapter 2. Literature Review
33
where p(c) tells the probability of the explanation c following the data dis-
tribution, and γ describes the threshold that a counterfactual explanation
should satisfy. Researchers use various methods to guarantee the plausibility
of counterfactual explanations. For example, Susanne et al. [85] use the close-
ness to the nearest observed data points; some works [36, 44] minimize the re-
construction loss using an autoencoder or variational autoencoder trained on
the training dataset; Kanamori et al. [79] uses an existing out-of-distribution
detector, called local outlier factor (LOF). Here, we distinguish actionability
from plausibility. Using the loan example in Example 1.2.2 again, Bob has
5 family members. Even though another user who has the same features
besides 3 family members, is granted the loan, such an explanation (reducing
the number of family members to 3) seems plausible but not actionable at
all to Bob.
• Diversity. A single counterfactual explanation may be restricted to satisfy
the user’s requirements. This could be resolved by providing multiple diverse
counterfactual explanations. Mothilal et al. [47] reformulate the problem by
deﬁning a new objective function as follows,
C = arg min
c1,...,ck
1
k
k
X
i=1
`(f(ci), y) + λ1
k
k
X
i=1
d(x, ci)
(2.23)
−λ2dpp(c1, . . . , ck)
where dpp() term describes the diversity of a counterfactual explanation set
based on a distance metric. Similarly, Russell [34] enforces diversity by adopt-
ing a rule to avoid previously produced counterfactual explanations. The
multiple initializations in the work [31] also have a diverse e↵ect. The side
e↵ect of diversity is that if we want a higher diversity, there could produce
sparser explanations.
• Connectedness [54, 86–88]. Connectedness means counterfactual explana-
tions are continuously connected to data points in high-density regions with
the target prediction. This provides another ground-truth justiﬁcation be-
sides plausibility. Counterfactual explanations are regarded as a consequence
of actions from the input that can be executed step by step. The continuity is
measured by ✏-chainability, that is, for a ﬁnite sequence of data {x1, x2, ...xN}

34
2.2. Counterfactual Explanations
in this path, x = x1, c = xN, then 8i < N, d(xi, xi+1) < ✏, and ✏controls
the granularity between two continuous points.
• Causality.
Causal relation can exist between features, that is, changing
antecedent features will a↵ect subsequent features. For example, increasing
your education will probably increase your monthly salary. Counterfactual
explanation generation should withhold the causal relationship between the
current input and its explanation. These works [46, 49–51] consider obtaining
reliable and actionable counterfactuals by preserving causal relations between
features.
• Validity [56]. Validity measures whether counterfactual explanations satisfy
the desired prediction, i.e., f(c) = y0.
• Similarity. A counterfactual explanation should be similar to the input x.
The similarity should be as larger as possible between counterfactual expla-
nations and the input under a similarity metric. In practice, the similarity is
often deﬁned by a cost function.
The above constraints are integrated for particular proposes to determine the ﬁnal
format of the optimization problem. Therefore, researchers should design corre-
sponding evaluation metrics to measure the satisfaction of the above properties.
2.2.3
Problem Solving
After the objective and constraints of counterfactual explanations are determined,
the next step focuses on searching for minimal solutions with proper optimization
solvers, which depend on data type, model type, and level of model access. There-
fore, we ﬁrst introduce how these factors a↵ect problem-solving, and then introduce
details of popular solving methods.
Counterfactual explanations are often proposed for heterogeneous tabular data
[31, 34, 35, 54], which consists of both continuous and categorical data. There are
also several works [44, 89] proposed for image or text data. Due to the discontinuity
of categorical features (including images and texts), gradient-based solvers should
be modiﬁed to adapt to discrete values accordingly.
In addition, the types of
models to be explained play a crucial role in problem-solving. The simplest model

Chapter 2. Literature Review
35
is the linear model and optimal solutions can be found eﬃciently. For tree-based
models (e.g., decision tree [23], random forest [90], boosted tree [91]), the prediction
function is not di↵erential. Therefore, researchers often investigate tree structures
to identify counterfactual explanations. For the popular neural network models,
gradient information can be obtained in a backward pass, which can facilitate the
search process.
The level of model access also decides which information can be used for a solver to
ﬁnd counterfactual explanations. Here, we present four levels of access to the black-
box model f. (1) model-free, no model is required. For example, CRUDS [92] is
a model-free method that directly generates counterfactual explanations from the
data distribution of the target class. (2) Prediction. The black-box models only
provide the prediction information.
As all models can provide prediction, this
branch of solving methods is model-agnostic. (3) Prediction and gradient. Some
CF methods need both the prediction and gradient w.r.t. input and solve it by
gradient descent or variants of it. In this regard, the model must be di↵erential.
(4) Complete model details should be provided, e.g., the structure of the decision
tree. CF methods will optimize the constrained problem with internal details of
models.
Next, we mainly include the following branches of solving methods:
• Gradient Descent (GD) In certain suiations, objectives and constraints
are di↵erential. The ﬁnal objective [31, 47, 51] is usually reformulated into
the Lagrangian function, as shown in the form below (2.24).
L(c, λ) = cost(x, c) + λ1 · `(f(c), y0) + λ2 · Constraints + · · ·
(2.24)
Then, gradient descent can be adopted to iteratively minimize the loss func-
tion L(c, λ). Usually, the optimized objective function and constraints are
nonconvex, no optimal counterfactual explanations can be found. Therefore,
the GD optimizer stops when it reaches a deﬁned maximum number of itera-
tions or the drop of loss is tiny. GD starts with a random point and updates
the point iteratively by the following rule, where ci denotes the updated
random point in i-th iteration.
ci+1 = ci + δ@L(ci, λ)
@ci
(2.25)

36
2.2. Counterfactual Explanations
Considering some objectives and constraints are not di↵erential, e.g., `1 term
for sparsity, FISTA [44, 45, 48] and ISTA [93] can be used.
• Genetic algorithm [41, 85, 94].
The genetic algorithm is motivated by
natural selection where the o↵spring is selected by certain criteria and re-
combined to produce the next generation.
Genetic algorithms can adapt
to the generation of counterfactual explanations. Selection criteria depend
on the objective to minimize and the constraints to satisfy. Recombination
(crossover) means combining parts of the features of two o↵springs to form a
new instance. In addition, mutation (noise) should be considered during the
crossover.
• Integer Programming (IP) or Mixed Integer Programming (MIP)
[35, 79, 95–97]. When we have a linear model and discrete variables, standard
solvers about IP and MLP can be used to ﬁnd a solution. The crucial points
are how to convert the constraints into linear functions.
• Growing Sphere [36, 52]. This solver starts with random points sampled
from the neighborhood of the input. The search radius of the neighborhood
(a.k.a. sphere) increases until a satisﬁed counterfactual explanation is found.
The input can be a real-life instance or a representation in hidden space.
• SAT, SMT solvers [78, 98]. The counterfactual explanation problem can
be formulated under the satisﬁability problem that can be solved by standard
SAT or SMT solvers. The cost function and constraints on black-box mod-
els are considered as conditions to satisfy. The above solvers will return a
satisﬁable counterfactual explanation that holds for all the above conditions
simultaneously.
In addition to the mentioned solvers, there are many other heuristic methods. For
example, Poyiadzi et al.
[54] leverage the Dijkstra’s algorithm to ﬁnd existing
data points that are ✏-connected to the input; Tsirtsis et al. [99] formulates the
counterfactual explanation problem by a monotone and submodular objective, and
solves it with a greedy algorithm. Rathi et al. [100] estimate the feature importance
by Shapley values and perturb the features where the Shapley values are negative
w.r.t.
the desired prediction.
Feature tweaking [53] is designed especially for
the decision tree model; Since the models are nonconvex generally, the proposed

Chapter 2. Literature Review
37
methods often get stuck in the local minima. As such, they often try di↵erent
initializations or di↵erent trials to choose the best one.
2.3
Model Extraction Attacks and Defenses
ML models have achieved impressive performance over various tasks. However,
training these models needs a large amount of investigation. Developers should
collect enormous labeled data and tune the ML models on powerful servers. The
spending on human resources also takes up a large portion. Due to the deﬁcient
computational capacity of clients, the commercial value of ML models, and private
user information, service providers deploy ML models on remote cloud servers and
open public APIs that allow users to access the ML models. For users, the APIs
are black-boxes that predict the input received. Even though data transmissions
are totally trusted, there still have great threats of various attacks. Attackers have
incentives to steal the cloud model for personal use [101–105] or conduct further
attacks (e.g., adversarial attack [106], model inversion attack [107] and membership
inference attack [108]). At the same time, service providers should protect ML
models from the mentioned attacks by a series of countermeasures [104, 109, 110].
In the following, I will introduce related methods of model extraction attack and
model defense.
2.3.1
Model Extraction Attacks
Model extraction attack intends to infer attributes of the cloud model or acquire
a functionally equivalent model as the cloud model from remote APIs. The most
frequent victim models are DNN models. In most situations, attackers can not
access the training data, and training details of the cloud model, and they aim to
replicate the cloud models based on inputs and predictions (including discrete pre-
dicted class, probability of top-1 class, and probability vectors). Considering every
query runs on a pay-per-query basis and service providers monitor the abnormal
request, attackers should extract the cloud model with as fewer queries as possible.
In the next, we investigate two branches of model extraction attacks according to
the di↵erences of information leak, named model attribute extraction and model
functionality extraction.

38
2.3. Model Extraction Attacks and Defenses
2.3.1.1
Model Attribute Extraction
Model attribute extraction aims to extract the attributes of ML model, which con-
sists of architectures [111], hyper-parameters [112] and decision boundaries [104].
Wang et al. [112] propose an attack that acquires hyper-parameters of cloud mod-
els. They assume training datasets, the ML algorithm, and the model’s parameters
(optional) can be obtained. This method grounds on the observation that the gra-
dient of the loss function w.r.t. model parameters should be close to 0 because
the model parameters are determined at the local minima of the loss function.
They construct linear equations by setting the gradient of the loss function at the
parameters to zero, and solve this equation with linear least square method.
Duddu et al. introduce a timing side channel attack, named StealNN [111] to infer
the number of layers of a network model. This method is based on the intuition that
the total execution time depends on the depth of the neural network. Speciﬁcally,
attackers feed the inputs to the cloud model and obtain the actual execution time.
After that, a regressor is trained to estimate the number of layers. A clear drawback
is that the hardware conﬁguration should keep the same, which can be expensive
for attackers.
2.3.1.2
Model Functionality Extraction
Model functionality extraction targets to create a substitute model that performs
as similarly (up to functionally equivalent) as the cloud model from the predic-
tions and (optional) conﬁdence values of cloud models. Some attack methods [5]
are proposed for speciﬁc model types. For example, Tramer et al. [5] adopt an
equation-solving method to extract logistic models and a path-ﬁnding algorithm
to extract the decision path for tree-based models. Most popular attacker methods
are model-agnostic where model types are unknown. For example, Papernot et
al. [106] propose an iterative attack method that contains three steps: (1) query
the cloud model with collected samples and obtain the corresponding labels; (2)
train the substitute model; (3) construct synthetic queries that are close to the
boundary of the substitute model with Jacobian-Based Data Augmentation. This
method deﬁnes a paradigm of model extraction method. Following it, some works
[104, 113] select close-to-boundary points, or Pal et al. [114] considers max-coverage

Chapter 2. Literature Review
39
points to query the model. Since the architectures of cloud models are unknown,
selecting the best architecture is insigniﬁcant. By default, attackers use compli-
cated models that could reproduce the functionality of simple models theoretically.
Meanwhile, the close-to-boundary points will reveal the boundary information of
the cloud model, and max-coverage points are far from each other and associated
with higher information entropy.
2.3.2
Model Extraction Defenses
Model extraction defenses are countermeasures that prevent model extraction at-
tacks. In detail, defenders want to reduce the e↵ectiveness and eﬃciency of attack
algorithms and retain the original prediction performance of cloud models. There
are two branches of defense methods: named output perturbations and queries mon-
itoring. Output perturbations perturb the model predictions by instilling elaborate
noise to misguide attackers. However, the model’s performance can be hurt due to
prediction perturbations. The trade-o↵between model utility and security should
be considered. Query monitoring tries to detect malicious queries and then block
such queries directly.
2.3.2.1
Output Perturbations
Attackers mainly leverage the predictions and conﬁdence scores of cloud models
to train the substitute model, so defenders can obfuscate the returned information
from cloud models to misguide attackers. For example, Lee et al. [115] add a layer
upon an existing cloud model by perturbing the probability vector to maximize
the training loss of the substitute model and reserve the accuracy of the cloud
model simultaneously. Similarly, BDPL (boundary di↵erentially private layer) [109]
adopts the di↵erential privacy method to obfuscate the prediction near the decision
boundary while keeping the original prediction far from the decision boundary.
One important point is that a higher obfuscation will result in lower accuracy.
Therefore, BDPL is customized by an obfuscation factor ✏to balance the utility
and privacy of models. The challenge is to determine the closeness to the decision
boundary of the cloud model.

40
2.4. Conclusion
Output perturbations are active defenses that directly consider defense into deploy-
ment regardless of the existence of attackers. Therefore, normal clients can also be
a↵ected by output perturbations.
2.3.2.2
Query Monitoring
Query monitoring aims to discover suspicious points that are probably used by
attack methods. Service providers should deﬁne criteria to measure the level of
warning. Once the score is above the threshold, then an alarm is triggered. As we
discussed in the model extraction attack, the close-to-boundary and max-coverage
points are useful for extraction. Therefore, model defense methods should be cau-
tious about these points.
For example, Forgotten Siblings [110] is proposed to
detect queries that are close-to-boundary. The threshold of closeness is determined
by the statistical distribution of training samples. For each query, a binary signal
indicates whether the current query is in the boundary region or not. If the exces-
sive queries (above a ratio) are close to the boundary, then the defender system will
alarm. Extraction Warning [116] is introduced to measure the coverage of queries.
The coverage can represent the knowledge that a user has about the model. Sim-
ilarly, PRADA [104] is proposed based on the observations: the query samples
used by attackers are speciﬁcally generated or selected and are expected to have
a di↵erent distribution as benign queries. In speciﬁc, the distances between two
random benign queries follow the normal distribution, whereas malicious queries
do not. Then they adopt existing metrics (e.g., Shapiro-Wilk test [117]) to perform
a normality test.
Query monitoring is a passive defense method that may detect the attack after the
model is stolen. Further actions should be taken to impair the leaky issue of cloud
models after detecting malicious queries.
2.4
Conclusion
Counterfactual explanations have drawn great attention in recent years. Di↵erent
from feature attribution methods, which identify important features to a model
prediction, counterfactual explanations ﬁnd the minimal perturbations of input fea-
tures leading to a di↵erent prediction. Speciﬁcally, existing methods mainly focus

Chapter 2. Literature Review
41
on formulating mathematical constrains from practical requirements and solving
the constrained optimization problem. Even though intensive research progress is
reported, some challenges still exist. In the remaining thesis, motivated by real-life
demand, we ﬁrst study the group-level summarization problem to inspect the model
in a compact way. After that, we reformulate the counterfactual explanations prob-
lem under the multiobjective optimization framework to avoid the improper scalar
cost function on heterogeneous data. Last, we study the model security and privacy
of counterfactual explanation, arousing the awareness of model protection.


Chapter 3
Group Summarization for Model
Inspection
The black-box model f in counterfactual explanations serves as a tutor that guides
users to achieve the desired prediction. Therefore, we should ﬁrst examine the
trustworthiness of f to ensure the reliability of the tutor, especially for neural net-
works and ensemble models. Existing post-hoc explanation methods either provide
global feature importance scores for the whole population or provide local feature
importance scores for a single instance. The global explanations are too coarse to
cover all individuals and examining all local explanations one by one for a large
dataset is time-consuming.
We observe that there exist some groups where each group shares the same im-
portant features while di↵erent groups have distinct important features. Based
on this observation, we propose the group summarization algorithm GUM, which
ﬁnds k groups with each group being summarized by l most important features
from a large feature importance matrix. From the k ⇥l summary, we can easily
inspect the model at a compact group level. Intensive experiments reveal that our
proposed algorithms also can solve broad summarization problems such as the one
that summarizes user preferences for rating datasets.
43

44
3.1. Introduction
3.1
Introduction
In the zettabyte era, massive volumes of data are produced every day, and it is
problematic to analyze, retrieve, and comprehend data at such scales. To mit-
igate this problem, data summarization techniques [118, 119] provide a concise,
compact, yet informative representation of original data. These techniques include
top-l selection [120], histograms [118], clustering [121], sampling [122, 123], matrix
decomposition [124], frequent itemset mining [125], principal component analysis
(PCA) [126] and wavelets [127].
In this chapter, we consider summarizing a utility matrix with rows representing
users and columns representing items. Each entry in the user-item utility matrix
represents the utility (or preference) associated with a (user,item) pair. The terms
“user” and “item” are only for convenience and can represent any two classes
of objects that interact in some way, such as author-to-paper citation, customer-
to-item purchases, user’s rating/vote on items or locations, user’s website clicks,
weighted edges in a social network, gene expression scores under conditions and
samples’ feature importance for model’s prediction. In many real-life applications,
a high utility is more interesting because it indicates a strong association, e.g.,
a high citation count, a high rating, a presence of purchase, a high level of gene
expression, importance of a feature, etc.
The main question studied in this work is: given a user-item utility matrix, how
to summarize the high utility interactions of the whole user pool using a small
summary. Take the MovieLens data [128] as an example where millions of users
give ratings to hundreds of thousands of movies.
Scanning through individual
user’s ratings does not give a high level overview on what movies users like due
to the large number of users and movies. Traditional summarization techniques
fail to address our summary that focuses on high utility. For example, the top-l
item selection [120] summarizes all users using the same l items though typically
di↵erent user groups may prefer di↵erent movies, therefore, the top-l items fail to
maximize the utility for di↵erent user groups; biclustering [129] groups users and
items according to the similarity of cells in a matrix, which fails to focus on high
utility.
With the above in mind, we deﬁne a new summarization problem, called Group
Utility Maximization (GUM): given a user-item matrix and integers k and l, we

Chapter 3. Group Summarization for Model Inspection
45
want to partition users into k groups and select l items for each group so that
the sum of utility of selected items over all groups is maximized. In other words,
we want to summarize all users through k groups with each group having its own
top l items. The selected items for each group summarize the preferences of the
users in the group. Since the data analyst has to read l items for each group,
k ⇥l represents the summary size and are usually small integers. Note that users
are strictly partitioned but items selected for di↵erent groups can be overlapped.
The objective is to group the users such that the sum of the utilities of selected
items over all groups is maximized. This objective is di↵erent from clustering that
is similarity-based, and di↵erent from max-sum submatrix that extracts certain
submatrices that do not always contain all users. See more discussion on related
works in Section 3.2.
Consider the MovieLens example again. Despite millions of users and hundreds
of thousands of movies and billions of ratings, GUM will ﬁnd a summary of size
k ⇥l, which summarizes all users in k groups with each group being summarized
by l top rated items. The case of k = 1 summarizes all users in one group by the
top-l items. In general, there may be k > 1 groups of users and each group has its
distinct preferred items. One application of GUM is summarizing the explanation
of a black-box model’s prediction, as shown in the following example.
Table 3.1: The user-item matrix D contains the feature importance (i.e., IG)
values for the survival probability F(x) of six Titanic correctly predicted sur-
vivors x, where F is a black-box ML model. The IG value of the feature xi
for a survivor x represents the contribution of xi to the prediction F(x) and xi
with a larger IG value is more inﬂuential for x’s prediction. With k = 2 and
l = 2, GUM partitions the six survivors into two groups, i.e., {U1, U2, U3} and
{U4, U5, U6}, and selects two most inﬂuential features for each group, marked by
the green and pink colors.
Pclass
Sex
Age
SibSp
Parch
Fare
C
Q
S
U1
1.30
1.49
-0.20
-0.12
-0.05
0.28
-0.24
0.01
0.14
U2
1.12
1.40
0.06
0.15
-0.04
0.31
0
-0.02
-0.10
U3
1.22
1.49
0.15
0.25
-0.14
0.27
-0.01
-0.02
-0.12
U4
-0.35
1.01
0.82
0.16
0.29
0
-0.02
0.01
-0.19
U5
-0.34
0.88
0.92
0.09
0.41
-0.04
-0.16
0.01
0.35
U6
0.49
1.34
0.72
-0.14
0.11
0.03
-0.03
-0.01
-0.16
Example 3.1. Consider a neural network F(x) 1 for predicting the surviving prob-
ability of Titanic survivors 2, where each survivor x has the features xi: Pclass
1We use F to denote a model and f to represent the utility function in Chapter 4.
2https://www.kaggle.com/c/titanic

46
3.1. Introduction
(cabin class), Sex, Age, Sibsp (number of siblings/spouses aboard), Parch (number
of parents/children aboard), Fare, and Port of embarkation denoted by C, Q, S.
[30] proposes the Integrated Gradient (IG) to measure the attribution (i.e., impor-
tance) of each feature xi to the prediction F(x). Table 3.1 shows the IG values
of six survivors as a user-item matrix D. A high IG value means that the feature
in the corresponding column is more important for F(x) for the survivor x in the
corresponding row. Suppose that the data analyst wants to get a quick grasp of
important features for F(x) for all survivors x, but this is not easy because impor-
tant features generally vary for di↵erent survivors, especially when there is a large
number of users and features.
Through the GUM problem, the data analyst can get a high level summary of
important features, as shown by the two colors in Table 3.1 where the survivors
are partitioned into k = 2 groups and each group is summarized by l = 2 most
inﬂuential features. The choices of these groups and selected items will maximize
the sum of the IG values over the 12 colored entries. The group in green has Pclass
and Sex as the two most inﬂuential factors for the survival probability, and the
group in pink has Sex and Age as the two most inﬂuential factors for the survival
probability. Importantly, the data analyst only needs to read 6⇥2 IG values to get a
high level overview about important features, regardless of the number of survivors
and features in the matrix. Such a group level summary provides a concise and
easy-to-understand explanation of important features for all survivors.
The main contributions of this chapter are:
• (Section 3.2) We review the related work and discuss the di↵erences between
our work and existing works.
• (Section 3.3) We deﬁne the GUM problem for a user-item utility matrix D
and show the NP-hardness, the monotonicity, and the submodularity of the
problem.
• (Section 3.4) We present a Greedy algorithm that ﬁnds one group at a time.
This algorithm provides the (1−1
e) approximation factor of optimal solutions
for a nonnegative matrix, which is the most common case.
• (Section 3.5) We present a heuristic k-max algorithm that iteratively reﬁnes
the k groups. In each iteration, it performs Assignment and Update steps to

Chapter 3. Group Summarization for Model Inspection
47
maximize the utility from k initial groups. Compared with the Greedy algo-
rithm, the k-max algorithm is more scalable for larger datasets since it does
not evaluate an exponential number of l-itemsets as the Greedy algorithm
does.
• (Section 3.6) We evaluate the usefulness and eﬃciency of the proposed sum-
marization methods on real life datasets. The study shows that 1) the two
proposed methods produce better utility than existing summarization tech-
niques, 2) our summary provides an easy-to-understand overview of the whole
dataset, and 3) the k-max algorithm achieves empirically comparable utility
but is more eﬃcient than the Greedy algorithm for large datasets.
3.2
Related Work
In this section, we discuss the di↵erences of our work from several related works,
namely, clustering, subgroup discovery, max-sum submatrix, preference learning,
and data summarization methods..
Clustering [130–132] is the task of grouping a set of objects such that objects in
the same group are more similar to each other than to those in di↵erent groups.
Subspace clustering [133] groups objects in subspace. Motivated by gene expression
analysis for ﬁnding submatrices of genes exhibiting similar behaviors, biclustering
[134], a.k.a. co-clustering [135], ﬁnds genes subsets (rows) with similar expression
values (columns). See the survey papers [129, 136, 137] for more details of clustering
methods. All these problems rely on a similarity measure for a pair of objects.
Our GUM does not use any similarity measure because the goal is to maximize the
utility sum of groups, not similarity of objects. In Example 3.1, features C and
Q (ports of embarkation) are more similar than Pclass and Sex for the ﬁrst group
but the latter is more interesting for explaining the prediction because higher IG
values mean more importance to the survival prediction. Due to this di↵erence in
the objective, existing clustering works cannot address our problem.
Subgroup discovery [138][139] is concerned with ﬁnding descriptive associations
(usually represented as rules) among attributes with respect to a target property
of interest. For example, if the success rate of an operation is 30% over all patients
and if the success rate for female patients under 30 and with drug A intake is

48
3.2. Related Work
90%, these algorithms will identify the rule “Gender = Female AND Age < 30
AND Drug = A -> Operation = SUCCESS”, which describes an unusually high
operation success rate for a subgroup. Exceptional model mining [140] allows more
than one target and looks for unusual target interaction. Exceptional preferences
mining [141] searches for subgroups with deviating preferences. For example, if
a subgroup ranks tekka-maki consistently in the top 3 while the majority in the
dataset ranks it in the last 3, this measure will ﬁnd the subgroup to be very
interesting. All these works ﬁnd interesting subsets of data with respect to some
pre-selected target properties (e.g., operation success rate and tekka-maki).
In
contrast, GUM summarizes the whole dataset by partitioning the dataset into
groups and maximizing the utility sum for groups.
Max-sum submatrix [142][143] targets highly expressed subsets of genes and of
samples by identifying a submatrix with the maximum sum. [135] ﬁnds k sub-
matrices by constraint programming and [144] adds the submatrix disjointness
constraint. While similar in maximizing the sum of entries, there are key di↵er-
ences between these works and ours. Motivated by the objective of summarization,
our groups cover all users and are nonoverlapping in rows but not in columns. In
contrast, the max-sum submatrix solution allows overlapping of both rows and
columns, and does not require to cover all rows. The max-sum submatrix prob-
lem assumes that the matrix contains both positive and negative entries. For a
nonnegative matrix, which is common for ratings, votes, and implicit feedback,
their problem will return the entire matrix as the solution due to the maximum
sum, which is useless for our summarization. The authors of [142][143] suggested
to subtract all entries by a constant to make the matrix contain both positive and
negative values, but did not give a clue on what constant should be used.
Preference learning [145] aims to build a global predictive model to predict the
order of preferences for new cases, and recommender system [146] seeks to predict
the rating that a user would give to an unseen item. Our GUM is designed for
a better understanding of preferences of existing users, instead of prediction for
future users.
Data summarization [147–149] targets to produce a compact summary of origi-
nal data by choosing the most informative and representative content, e.g., keyframes
[147], keywords [148] and image prototypes [149]. Due to di↵erent problem con-
texts and objectives, these summarization algorithms cannot address our GUM

Chapter 3. Group Summarization for Model Inspection
49
Table 3.2: Frequently used notations in Chapter 3.
Symbol
Description
D 2 RN⇥M
dataset of N rows over M columns 1, ..., M
ric
the value of item c of user ri in D
(k, l)
problem parameters: group number and itemset size
Dj
a subset of rows
{D1, ..., Dk}
a partitioning of D
Xj
l-itemset, i.e., a set of l items
{X1, ..., Xk}
a collection of l-itemsets
h( · )
the utility function over a collection of l-itemsets on a row
f( · )
the utility function over a collection of l-itemsets on D
problem that maximizes the utility of selected entries. To the best of our knowl-
edge, our work is the ﬁrst to summarize a dataset by k groups with each group
being summarized by top-l interesting items.
3.3
Group Utility Maximization
We now formally deﬁne the GUM problem and show the NP-hardness, the mono-
tonicity, and the submodularity of this problem. Some frequently used notations
are given in Table 3.2.
3.3.1
Deﬁnitions
In this section, we consider a user-item matrix D 2 RN⇥M, consisting of N rows for
users {r1, ..., rN} and M columns for items {1, ..., M}. ric denotes user ri’s score
on item c (e.g., vote, rating, etc.) and a larger score represents a higher utility or
preference. The only assumption is that the addition operation is meaningful for
entry values over rows and columns. For example, ric representing ratings in 5-star
scales is additive (because the sum returns the total number of stars), and ric in
the log scale is not additive. For a set X of l items, a.k.a. l-itemset, and a user
ri, we deﬁne g(ri, X) ⌘P
c2X ric as the total score of ri over the items in X. For
a collection of l-itemsets X = {X1, ..., Xk}, named summary, where Xi 6= Xj for

50
3.3. Group Utility Maximization
i 6= j, we deﬁne h(ri, X) ⌘maxXj2X g(ri, Xj) and deﬁne the utility of X as
f(X) ⌘
N
X
i=1
h(ri, X)
(3.1)
In other words, f(X) is the total utility over all users by assigning each user ri
to the Xj that maximizes g(ri, Xj). Therefore, X induces a partitioning on the
users. Let f(;) ⌘0. Note that h(ri, X) can also be deﬁned using min instead of
max operator. In the rest of this work, we consider only maximization because
minimization can be performed by negating all utility values ﬁrst.
Deﬁnition 3.1 (GUM Problem). Given a user-item matrix D, and numbers k and
l, ﬁnd a collection of distinct l-itemsets X⇤= {X1, ..., Xk} such that
X⇤= arg max
X
f(X)
(3.2)
We write f(X⇤) as f ⇤(k, l) or simply f ⇤if k and l are understood.
We say that a partitioning D1, ..., Dk of D is induced by X where Dj contains a
user ri if h(ri, X) = g(ri, Xj), that is, Dj contains all users ri such that Xj gives
the maximum g(ri, Xj), i.e., the maximum sum of ri’s scores on the items in Xj.
This gives rise to the following equivalent deﬁnition of GUM.
Deﬁnition 3.2 (GUM Problem (Alternative)). Given a user-item matrix D and
numbers k and l, ﬁnd distinct l-itemsets X = {X1, ..., Xk} such that for the parti-
tioning D1, ..., Dk induced by X,
k
X
j=1
X
ri2Dj
g(ri, Xj)
is maximized.
Intuitively, (D1, X1), ..., (Dk, Xk) is a group level summary where each Xj repre-
sents the l most preferred items (in terms of largest sum) over the users in the
group Dj. The group number k and description size l represent the summary size,
serving as the budget on the cost for reading the summaries. For example, if D
stores ratings/votes, the GUM solution summarizes what items the users would
like through a summary of size |X1| + · · · + |Xk| = k ⇥l. Note that this size is

Chapter 3. Group Summarization for Model Inspection
51
independent of the data size |D|. Similar to the cluster center that summarizes the
location of points in each cluster, the l-itemset Xj summarizes the preferred items
for each group Dj. The di↵erence is that, instead of being the mean of points, Xj
is the top-l items (in terms of score sum) in the group. It is possible that some Dj
is empty, which means that fewer than k groups are suﬃcient to achieve the same
utility as k groups.
Typically, l is a small number, say 1 to 5, as too many items would overwhelm
the human analyst. k is in the range [1, N]. k = N summarizes each user by
personalized l items, whereas k = 1 summarizes all users by the same set of l
items and corresponds to the standard top-l item selection over the whole data
set. A smaller k produces larger groups , thus, a larger variance of scores in such
groups. A larger k leads to more customized preferences for smaller groups but the
summary size increases. k can be speciﬁed by the analyst or determined similar to
determining the cluster number k for k-means clustering methods (i.e., gradually
increasing k until the increase of f ⇤slows down signiﬁcantly). Finally, note that
for the same l, f ⇤never decreases as k increases, and for the same k, the average
utility of selected items, i.e.,
f⇤
N⇥l, never increases as l increases (because lower
utility items are selected as l increases).
The maximization objective in Deﬁnition 3.1 is suﬃcient for modeling other re-
quirements in practice.
For example, for the 5-star rating scale, if both a low
rating close 1 and a high rating close to 5 are interesting, we need to summarize
users in terms of both high ratings and low ratings. In this case, we can shift the
5-star scale to the symmetric scale [−2, 1, 0, 1, 2] by subtracting the medium rating
3 from every known rating and consider D deﬁned by the absolute values of shifted
ratings in Deﬁnition 3.1. Then f represents the sum over the distances from the
medium rating 0 and the GUM problem will summarize the users into groups by
the items that have the most extreme ratings.
3.3.2
Properties
With
%M
l
&
possible l-itemsets, there are
%(M
l )
k
&
possible summaries {X1, ..., Xk}.
This number grows quickly as M, l, k grow. The exception cases are k = 1, which
coincides with the top-l item selection on the whole dataset, and k = N, which

52
3.3. Group Utility Maximization
coincides with the top-l item selection for each user. In general, even for the strict
case l = 1, GUM problem is NP-hard, as shown below.
Theorem 3.1. The GUM problem is NP-hard for l = 1.
Proof. Consider the following NP-hard maximum coverage problem [150]: Given a
number k and a collection of sets S = {S1, · · · , Sm}, ﬁnd a subset S0 ✓S of sets,
such that |S0| k and | [Sj2S0 Sj| is maximized.
We can construct an instance of GUM problem from the maximum coverage prob-
lem in polynomial time such that X is a solution to the GUM problem if and
only if S0 is a solution to the maximum coverage problem, where S0 is a subset
of S constructed from X. Therefore, if there is a polynomial time algorithm for
the GUM problem, we also have a polynomial time algorithm for the maximum
coverage problem.
We deﬁne the instance < N, M, D, k, l > for the GUM problem as follows. Let
N = | [Sj2S Sj|, M = |S|, k be same as in the maximum coverage problem, and
l = 1. D contains one row for each element in [Sj2SSj and one item (column) for
each Sj in S, such that the entry for row r and item c contains 1 if the corresponding
element for the row is in the Sj corresponding to the item c, otherwise 0. Note that
with l = 1, every l-itemset is a single item and there is an 1-to-1 correspondence
between a Sj in S and a l-itemset Xj.
Consider any collection of l-itemsets X = {X1, ..., Xk} for the above GUM problem.
For each row ri in D, h(ri, X) = maxXj2X g(ri, Xj). With l = 1, each Xj is a single
item and h(ri, X) = 1 if ri has 1 for some Xj in X, otherwise h(ri, X) = 0. Therefore,
f(X) (Equation (3.1)) is equal to the number of rows that have 1 for some Xj in X.
Deﬁne S0 to be the set of Sj’s corresponding to the Xj’s in X, f(X) = | [Sj2S0 Sj|.
Therefore, f(X) is maximized if and only if | [Sj2S0 Sj| is maximized, in other
words, X is a solution to the GUM problem if and only if S0 is a solution to the
maximum coverage problem. The only di↵erence is that S0 may contain redundant
Sj’s whose removal from S0 does not a↵ect [Sj2S0Sj; such Sj’s correspond to the
Xj’s in X that have empty groups.
Next, we show two properties of f that are useful for proving an approximation
bound of our solution.

Chapter 3. Group Summarization for Model Inspection
53
Theorem 3.2 (Monotonicity of f). For a nonnegative D (i.e., D 2 RN⇥M
≥0
),
• (i) if X = {X1, ..., Xk} and X0 = X [ {Xk+1}, then f(X0) ≥f(X), and
• (ii) f ⇤(k + 1, l) ≥f ⇤(k, l).
Proof. (i). Recall f(;) = 0. The nonnegativity of D implies g(ri, X1) ≥0 for any
l-itemset X1, so f({X1}) ≥f(;). Then (i) follows because having an additional
Xk+1 gives one more choice of Xj for the max function of computing h(ri, X), thus,
never decreases the value of f. (ii) If X = {X1, ..., Xk} is the solution of f ⇤(k, l)
and if X0 = X[{Xk+1}, then f ⇤(k +1, l) ≥f(X0) ≥f ⇤(k, l). The second inequality
follows from (i).
(ii) implies that f ⇤(k, l) is not worse than f ⇤(1, l), which is the utility of the stan-
dard top-l selection over the whole dataset.
We say that a function f : 2N ! R is submodular if f(A [ {u}) −f(A) ≥f(B [
{u}) −f(B), 8A ✓B ✓N, u 2 N\B, N is a ground set of elements. In other
words, f is submodular if the marginal gain of adding an element to a subset A
is no less than the marginal gain of adding this element to any superset of A. In
our context, the universe N is the set of all l-itemsets. Next, we show that f is
submodular for a general D.
Theorem 3.3 (Submodularity of f). For a general D, the function f (deﬁned by
Equation (3.1)) is submodular.
Proof. Let X be a set of l-itemsets and X0 be a subset of X, i.e., X0 ⇢X. In addition,
let X be an l-itemset that is not in X. Here, we consider each user separately. For
a arbitrary user ri, we show that the gain of adding X to X0 is at least that of
adding X to X (both wrt ri), i.e.,
h(ri, X0 [ {X}) −h(ri, X0) ≥h(ri, X [ {X}) −h(ri, X)
(3.3)
Then summing up on both sides over all users ri, we obtain
f(X0 [ {X}) −f(X0) ≥f(X [ {X}) −f(X)
(3.4)

54
3.3. Group Utility Maximization
which implies that function f is submodular. Therefore, what remains is to verify
Equation (3.3).
Let Xi be the l-itemset in X, for which the utility of ri is the largest, i.e., Xi =
arg maxX02X g(ri, X0). We consider two cases depending on whether Xi is in X0 or
not.
Case 1 (Xi 2 X0): In this case, only X0 need to be considered when computing
the gain of adding X to X, which is exactly the case when computing the gain of
adding X to X0. Therefore, we have
h(ri, X [ {X}) −h(ri, X) = h(ri, X0 [ {X}) −h(ri, X0)
(3.5)
which implies that Equation (3.3) holds in Case 1.
Case 2 (Xi 2 X\X0): This case implies
h(ri, X0) g(ri, Xi)
(3.6)
We consider two sub-cases based on ri’s utility on X.
Case 2.1 (g(ri, X) > g(ri, Xi)): In this sub-case, we have
h(ri, X [ {X}) = h(ri, X0 [ {X}) = g(ri, X)
(3.7)
h(ri, X) = g(ri, Xi) ≥h(ri, X0)
(3.8)
Equation (3.7) and (3.8) imply that Equation (3.3) holds in Case 2.1.
Case 2.2 (g(ri, X) g(ri, Xi)): In this sub-case, we have
h(ri, X [ {X}) −h(ri, X) = g(ri, Xi) −g(ri, Xi) = 0
(3.9)
h(ri, X0 [ {X}) −h(ri, X0) ≥0 (h is monotone)
(3.10)
Equation (3.9) and (3.10) imply that Equation (3.3) holds in Case 2.2.
In conclusion, Equation (3.3) holds for any user ri on both cases. Summing up
over all users, we can deduce that the function f is also submodular.
We can also explain the intuition of the submodularity using f({X1} [ {X}) −
f(X1) ≥f({X1, X2} [ {X}) −f({X1, X2}), where X is a l-itemset di↵erent from

Chapter 3. Group Summarization for Model Inspection
55
X1 and X2. Recall that f(X) assigns each row ri to the Xj in X that maximizes
g(ri, Xj). The left-hand-side of the inequality is the marginal gain of reassigning
some rows ri from X1 to X, and the right-hand-side is the marginal gain of reas-
signing some rows ri from {X1, X2} to X. Let g1 and g2 be the gains in these two
cases, respectively. There must be a nonnegative gain, denoted g0, for moving ri
from X1 to {X1, X2}, and g1 = g0 + g2. Therefore, g1 ≥g2.
In the next two sections, we present two algorithms for solving the GUM problem
for a matrix D.
3.4
Greedy Algorithm
In this section, we ﬁrst propose the Greedy algorithm that greedily ﬁnds a set of
l-itemsets successively. This algorithm achieves the (1 −1
e) approximation factor
of optimal solutions for a non-negative matrix as shown in Section 3.4.1. Next, we
introduce two strategies to speed up the Greedy algorithm in Section 3.4.2.
With the input parameters k, l, N, M, D, Greedy algorithm in Algorithm 1 starts
with an empty set X, adds one l-itemset Xj at a time that maximizes the marginal
gain of f, ∆f(Xj|X) ⌘f(X S{Xj}) −f(X), until k l-itemsets are added. After
ﬁnding X, the corresponding D1, ..., Dk can be induced by X in one additional pass
over the data as discussed in Section 3.3. In each iteration, the arg maxXj2X\X
operation requires evaluating the marginal gains for
%M
l
&
l-itemsets, and each l-
itemset takes O(N · l) time to check the utility for N users. For k iterations, the
overall time complexity is O(k ·
%M
l
&
· N · l). Note that computing arg maxXj2X\X
does not require materializing X. Instead, we can enumerate the l-itemsets using
l nested loops where each loop iterates over all items {1, · · · , M}. This method
enumerates only one l-itemset at a time, requiring the constant space O(1), plus
the space for the k l-itemsets of X and the space for the input matrix. Therefore,
the overall space complexity is O(N · M).
3.4.1
Approximation Factor
For a nonnegative D, the approximate solution by Greedy algorithm has its quality
guaranteed to be not far away from that of the optimal one. We present this result

56
3.4. Greedy Algorithm
in the following theorem.
Theorem 3.4. For a nonnegative D, Greedy algorithm (Algorithm 1) provides a
(1−1
e)-factor approximation for the GUM problem, where e is the natural logarith-
mic base.
Proof. The result holds since the f function is monotone and submodular (Theo-
rem 3.2 and Theorem 3.3) and according to [151], a simple Greedy algorithm would
provide a (1−1
e)-factor approximation for maximizing a monotone and submodular
function f with f(;) = 0 subject to a cardinality constraint, i.e., maxX f(X) s.t.
|X| k in our context.
Theorem 3.4 shows (1 −1
e)-factor approximation for a nonnegative matrix D. In
practice, a nonnegative D is common, such as rating/vote, citation count, presence
of purchase, etc. In the case of a general D containing both positive and negative
values, Greedy algorithm still works but we cannot claim the (1−1
e) approximation
factor.
While adding a positive constant to every entry can make the matrix
nonnegative, the (1−1
e) approximation will be for the the transformed nonnegative
matrix, not the original matrix. Replacing all negative scores with zeros also makes
the matrix nonnegative, but this has the e↵ect of ignoring the penalty of negative
scores, which changes the problem statement.
In theory, no normalization is required when we do not require the (1 −1
e) approx-
imation factor for the Greedy algorithm and there is no computational limitation.
However, normalization can be used to rescale the entry values to avoid data over-
ﬂow and negative values. We can still use the Greedy algorithm on the normalized
dataset, but the produced l-itemsets may be di↵erent and the (1−1
e) approximation
factor may not hold for the original matrix, which depends on the normalization
method used and the nonnegativity of the normalized dataset.
In general, the
Greedy algorithm is not a↵ected by the normalization divided by a constant (e.g.,
the maximum value).
3.4.2
Implementation
The costly step of Greedy algorithm is ﬁnding the l-itemset that has the greatest
marginal gain from all those l-itemsets that have not been selected at each iteration.

Chapter 3. Group Summarization for Model Inspection
57
Algorithm 1: Greedy algorithm
Input: A matrix D, group number k, itemset size l
Output: X
Notation: Let X as the collection of all l-itemsets;
initialize X as an empty set;
for i = 1; i k; i = i + 1 do
X  arg maxXj2X\X ∆f(Xj|X);
X  X S{X};
return X
Algorithm 2: Stochastic-Greedy algorithm
Input: A matrix D, group number k, itemset size l, sampling factor ✏.
Output: X
Notation: Let X as a collection of all l-itemsets;
initialize X as an empty set;
for i = 1; i k; i = i + 1 do
/* Randomly sampling from X\X
*/
x  min( |X|
k log( 1
✏), |X|);
R  randomly sample x distinct numbers from
{1, · · · , |X|} −{j | Xj 2 X};
X  arg maxXj|j2R ∆f(Xj|X);
X = X S{X};
return X;
For better eﬃciency, we adapt two existing strategies, namely Lazy-forward [152]
and Random-sampling [153], and discuss how to integrate them together eﬃciently.
Lazy-forward. The idea is to leverage the submodularity of f that the marginal
gain ∆f of Xj never increases by growing X. Based on this property, we can prune
the evaluations of ∆f for a Xj if its upper bound on ∆f is smaller than the greatest
∆f evaluated so far. In particular, we maintain an upper bound of the marginal
gain for each l-itemset Xj and check the l-itemsets in the decreasing order of their
upper bounds using a priority-queue, if its upper bound is larger than the greatest
marginal gain ∆f found so far, we evaluate the actual ∆f for the Xj, otherwise, we
terminate the current iteration and selects the l-itemset that gives the greatest ∆f
so far. Initially, the upper bound for a l-itemset is computed at the ﬁrst iteration
against the empty itemset and is updated in subsequent iterations whenever the
l-itemset’s marginal gain is evaluated. For all l-itemsets that are not evaluated,
the submodularity of f implies that their upper bounds remain unchanged because
growing X never increases ∆f of a l-itemset.

58
3.4. Greedy Algorithm
Lazy-forward only evaluates the l-itemsets whose upper bounds are larger than
the best-known marginal gain. Even though the theoretical time complexity is
the same as that of the straightforward implementation, the empirical running
time is reduced due to fewer evaluations of marginal gains. However, the Lazy-
forward implementation requires an extra priority-queue to store the upper bound
of marginal gain for all l-itemsets, with O(
%M
l
&
) space complexity.
Random-sampling. With Lazy-forward, Greedy algorithm needs to evaluate the
marginal gains of all
%M
l
&
l-itemsets at the ﬁrst iteration (before which all upper
bounds are initialized as inﬁnity), which is still time-consuming. To achieve better
time eﬃciency, we adapt the sampling procedure from [153] to reduce the number
of l-itemsets considered at each iteration. Speciﬁcally, at each iteration, we ran-
domly sample |X|
k log( 1
✏) l-itemsets from the set of all l-itemsets that have not been
selected, and proceed with the sampled l-itemsets only, where |X| is the number of
all l-itemsets and ✏is in (0, 1). According to [153], the approximation factor in The-
orem 3.4 becomes (1−1
e −✏). The Greedy algorithm with Random-sampling, called
Stochastic-Greedy algorithm or simply SGreedy, is given in Algorithm 2. The sam-
pling step is implemented by randomly sampling |X|
k log( 1
✏) indexes for l-itemsets,
i.e., R, and restricting the space for computing arg max to the l-itemsets with the
indexes in R. This can be implemented as l nested loops, as for the straightfor-
ward implementation, except that it considers only the jth generated l-itemsets
such that j is in R. In each iteration, this algorithm evaluates the marginal gain
for no more than |X|
k log( 1
✏) l-itemsets and the evaluation of marginal gain on each
l-itemset takes O(N · l) time. Therefore, for k iterations, the time complexity is
O(log( 1
✏) ·
%M
l
&
· N · l). With the total sample size being no more than |X|, SGreedy
algorithm’s time complexity is no more than that of the straightforward implemen-
tation. Similar to the straightforward implementation, this algorithm only takes
the space O(N · M) for storing the input matrix.
To integrate Random-sampling with Lazy-forward, we note that the sets of l-
itemsets at two adjacent iterations are sampled independently, and a straightfor-
ward integration requires building a di↵erent priority-queue structure at each iter-
ation, which introduces considerable overhead (as veriﬁed empirically). To avoid
this cost, we ﬁrst check the upper bound of each sampled l-itemset and only eval-
uate its ∆f if its upper bound is larger than the best-known marginal gain found.

Chapter 3. Group Summarization for Model Inspection
59
Table 3.3: The time and space complexities of di↵erent implementations of
Greedy algorithm. In the case of log( 1
✏) > k, the time complexity of Greedy
and SGreedy algorithms is the same since the sample size cannot be greater
than the full size |X| . # means the empirical running time is reduced compared
to without the Lazy-forward strategy, even if the theoretical time complexity
remains the same.
Algorithms
Acceleration strategies
Space Complexity
Time Complexity
Expirical
Running Time
Greedy algorithm
Straightforward
O(N · M)
O(k ·
%M
l
&
· N · l)
-
Lazy-forward
O(
%M
l
&
)
O(k ·
%M
l
&
· N · l)
#
SGreedy algorithm
Random-sampling
O(N · M)
O(log( 1
✏) ·
%M
l
&
· N · l)
-
Random-sampling +
Lazy-forward
O(
%M
l
&
)
O(log( 1
✏) ·
%M
l
&
· N · l)
#
Algorithm 3: k-max algorithm
Input: A matrix D, group number k, itemset size l, stop threshold ✓.
Output: (D1, X1), ..., (Dk, Xk)
initialize distinct l-itemsets X1, ..., Xk;
iter increase = +1;
cur utility = 0;
while iter increase > ✓do
/* Assignment Step:
induce D1, ..., Dk
*/
for i = 1; i N; i = i + 1 do
assign row ri in D to Dj such that g(ri, Xj) is maximum;
/* Update Step:
update X1, ..., Xk
*/
if some rows were assigned to a new group then
for j = 1; j k; j = j + 1 do
Xj  l-itemset X with largest P
ri2Dj g(ri, X);
iter increase = f(X) −cur utility;
cur utility = f(X);
return (D1, X1), ..., (Dk, Xk)
The worst case time complexity is the same as Random sample without Lazy-
forward, but the empirical running time is reduced due to fewer evaluations of
marginal gains. This algorithm needs O(
%M
l
&
) space to store the upper bounds of
all l-itemsets sampled in k iterations.
The time and space complexities of di↵erent implementations of Greedy algorithm
are summarized in Table 3.3.

60
3.5. k-max Algorithm
3.5
k-max Algorithm
The bottleneck of Greedy algorithms is identifying the best Xj at each step, which
is time-consuming when there are many items, even with the Lazy-forward and
Random-sampling techniques. In this section we present the second algorithm,
named k-max, to address this issue. This name comes from the iterative reﬁne-
ment of the k groups, (D1, X1), ..., (Dk, Xk). Given in Algorithm 3, k-max starts
with initializing k distinct l-itemsets, X1, ..., Xk. In each iteration, the algorithm
alternatively performs two steps:
• Assignment Step Given X1, ..., Xk, this step assigns each row ri to the
group Dj such that g(ri, Xj) is maximized over 1 j k. ri stays in its
current group Dj if g(ri, Xj) is already maximized.
• Update Step Given D1, ..., Dk, this step updates each Xj to the l-itemset
X such that P
ri2Dj g(ri, X) is maximized, in other words, Xj is the top-l
items ranked by the sum of scores of the users in Dj on the item.
In each iteration, each step improves X1, ..., Xk or D1, ..., Dk assuming that the
other is ﬁxed. This iterative process continues until the increase of utility is smaller
than the speciﬁed threshold ✓, in which case each row ri belongs to Dj that maxi-
mizes g(ri, Xj), that is, D1, ..., Dk is induced by X (i.e., h(ri, X) = g(ri, Xj)).
The following theorem shows that, after the ﬁrst iteration, f(X) is no less than the
utility of the standard top-l items f ⇤(1, l).
Theorem 3.5. For X = {X1, ..., Xk} produced at the end of the ﬁrst iteration,
f(X) ≥f ⇤(1, l).
Proof. f ⇤(1, l) is f({X}) for the top-l items X in the whole dataset. Let Dj be
produced by Assignment Step in the ﬁrst iteration and let Xj be produced by
Update Step in the ﬁrst iteration. Note that Xj is the set of top-l items in Dj,
thus, P
ri2Dj g(ri, Xj) ≥P
ri2Dj g(ri, X), and
X
j
X
ri2Dj
g(ri, Xj) ≥
X
j
X
ri2Dj
g(ri, X)
The left-hand-side is f(X) and the right-hand-side is f ⇤(1, l).

Chapter 3. Group Summarization for Model Inspection
61
Initialization of X1, ..., Xk. The simplest way of initializing X1, ..., Xk is randomly
selecting these Xj’s. Let Random Init denote this random initialization. Another
way is greedily selecting the Xj for 1 j k as the top-l items in the remaining
data, initially D. After selecting Xj, we remove the rows having Xj as its top-
l items before selecting Xj+1.
Let Smart Init denote this greedy initialization.
Unlike Random Init, Smart Init is deterministic. We will study the e↵ect of these
initializations experimentally.
Time complexity. Assignment Step takes k · l · N time because it takes k · l time
to ﬁnd Xj that maximizes g(ri, Xj) for a row ri. In Update Step, for each 1 j k,
the computation of Xj involves computing the top-l items in Dj, which takes time
O(|Dj| · M) (with |Dj| being the number of rows in Dj), therefore, Update Step
takes time O(N · M). Ignoring the small constants l and k, the algorithm with ⌧
iterations takes O(⌧· M · N) time, which is a linear in the input matrix size M · N.
We will empirically evaluate the eﬃciency of this algorithm.
Convergence analysis. The algorithm always converges because the change of
group membership is triggered by a positive increase of f and there is only a ﬁnite
number of such increases. For a general matrix D, the optimal utility is capped by
the sum of top-l columns selected for each row. Let ⇥denote this sum. Remember
that ✓is the threshold of increase for early stop. According to Theorem 3.5, after
the ﬁrst iteration, the increase of f is no more than ⇥−f ⇤(1, l). With each iteration
increasing f by at least ✓after the ﬁrst iteration, the maximum number of iterations
of k-max algorithm is no more than d(⇥−f ⇤(1, l))/✓e + 1. Typically, the increases
of f in early iterations are much greater than ✓, so the number of iterations needed
is much fewer than d(⇥−f ⇤(1, l))/✓e+1. We will evaluate empirically the number
of iterations in the experiment section.
Table 3.4: A toy movie rating dataset
m1
m2
m3
m4
m5
r1
2
0
3
3
2
r2
4
1
4
3
1
r3
2
4
4
0
1
r4
3
2
1
0
1
r5
2
4
4
3
1
r6
3
2
3
3
3

62
3.5. k-max Algorithm
Example 3.2. We illustrate the working of k-max algorithm using the toy movie
rating data in Table 3.4, and k = 3 and l = 2. The optimal solution has the utility
f = 40 given by the groups
(r4, m1m2), (r1r2r6, m1m3), (r3r5, m2m3)
Here m1m2 and r1r2r6 are the short-hand for {m1, m2} and {r1, r2, r6}, and same
for others. The standard top-l selection will ﬁnd m1m3 with the utility f = 35
because these items have the largest score sums 16 and 19 on the whole dataset,
respectively.
Let us run k-max algorithm with an random initialization X1 =
m2m4, X2 = m1m3, X3 = m4m5.
First iteration: For r2, the g function on X1, X2, X3 returns 4, 8, 4, respectively, so
r2 is assigned to D2. After assigning all rows, the utility f becomes 36 with the
groups
(r5, m2m4), (r1r2r3r4r6, m1m3), (;, m4m5)
In Update Step, X1, X2, X3 are updated to maximize the total rating for their
partitions of rows, i.e.X1 = m2m3, X2 = m1m3, X3 = m4m5,
Second iteration: Reassign each row to the group that maximizes its total score.
For example, r3 is reassigned to m2m3 because it has total score 6 in m1m3 but
has the total score 8 for m2m3. After Assignment Step, f = 39 given by
(r3r5, m2m3), (r1r2r4r6, m1m3), (;, m4m5)
After Update Step, all Xj’s remain unchanged.
Third iteration: no row changes its group and the increase is 0, so the algorithm
stops. The ﬁnal utility f = 39, which is close to the optimal utility f = 40.
It is easy to verify that with the initialization X1 = m1m2, X2 = m2m3, X3 =
m4m5, we will get the following partitions with the utility f = 40 after two itera-
tions,
(r2r4, m1m3), (r3r5, m2m3), (r1r6, m3m4)

Chapter 3. Group Summarization for Model Inspection
63
3.6
Evaluation
In this section, we conducted experiments to study the e↵ectiveness of the pro-
posed methods on three real life public datasets. Section 3.6.1 ﬁrst introduces the
experimental setup. Section 3.6.2 compares the solution of each algorithm with
the optimal solution, and these experiments are conducted on small datasets be-
cause obtaining the optimal solution is time consuming. Section 3.6.3 reports the
experiments on summarizing users’ preferences by comparing our summaries with
those of the top-l selection. Section 3.6.4 studies the scalability of algorithms on
large datasets. The codes are released on GitHub 3.
3.6.1
Experimental Setup
We consider the following setup to evaluate our proposed algorithms.
Evaluation metrics: We evaluate the average utility of the selected items, called
average item utility (AIU), mathematically deﬁned as f(X)
k⇥l where f(X) is deﬁned
in Equation (3.1). A larger AIU indicates a better summary of high utility items.
For algorithms with random factors (i.e., random initialization in k-max algorithm
and Random-sampling in SGreedy algorithm), we also report the coeﬃcient of
variation of AIU of multiple runs, i.e., σ
µ where σ and µ are standard deviation
and mean, respectively. We also evaluate the running time of proposed methods.
All experiments were implemented in Python on a Ubuntu server with Intel Xeon
Silver 4114 Processor at 2.2 GHz, 187GB of memory, with matrix operations from
the Numpy and Pandas libraries.
The proposed algorithms: The GUM problem has two parameters, k and l,
that deﬁne the summary size k ⇥l. In practice, this size is bounded by the human
e↵ort required to read the l items for each of the k groups. For this reason, k
and l cannot be too large, for example, k 2 [1, 10] and l 2 [1, 5].
We study
two proposed solutions to the GUM problem. The ﬁrst is the Greedy algorithm
presented in Section 3.4. Greedy denotes Greedy algorithm integrated with Lazy-
forward strategy and SGreedy-✏denotes Greedy algorithm integrated with Lazy-
forward and Random-sampling strategies (averaged over 10 runs). We set ✏to 0.5
and 0.9 to explore di↵erent trade-o↵s between eﬃciency and approximation quality.
3https://github.com/wangyongjie-ntu/GUM

64
3.6. Evaluation
The second algorithm is k-max algorithm introduced in Section 3.5. We consider
three options of running this algorithm. KMM reports the best AIU of running
k-max algorithm with 50 initializations by Random Init and reports the total time
of the 50 runs. KMA reports the average AIU and time of the 50 runs, which
gives an estimated performance of a single initialization. KMS denotes running
k-max algorithm with the greedy initialization Smart Init. In our experiment, we
set ✓to 0 for k-max algorithm because it can run eﬃciently.
We compare our algorithms with several baseline algorithms. These baselines do
not ﬁnd a solution for the GUM problem, i.e., k groups of users and l items for
each group, but they are the closest to our work.
Baselines: K-CPGC [143] denotes the max-sum submatrix algorithm, which
ﬁnds k max-sum submatrices iteratively.
CoClust
[135] denotes the spectral
co-clustering, which ﬁnds k(≥2) nonoverlapping clusters.
Both K-CPGC and
CoClust do not constrain each submatrix to l columns. To convert their solutions
for our GUM problem, if the column number of a submatrix is greater than l,
we only keep top-l columns; otherwise, we make up l columns by adding columns
that are outside of the submatrix and have the largest sum over the rows of this
submatrix. Note that the submatrices found by K-CPGC may not cover all rows.
We also consider Random Init (averaged over 50 runs) and Smart Init, i.e., the
initialization methods of k-max algorithms as described in Section 3.5. Comparison
with these initializations can tell the improvement by k-max algorithms. Finally,
we include the top-l selection over the whole data set (i.e., k = 1), which has the
utility of f ⇤(1, l). The comparison with this solution tells the improved utility due
to user partitioning. As discussed in Section 3.2, subgroup discovery and preference
mining are not applicable to our problem.
Datasets: We experimented on three real-life datasets. Titanic dataset is a
feature importance dataset of size 240 ⇥9 where each row denotes a correctly
predicted survivor and each entry represents the importance of the corresponding
feature value to the survival prediction. The detailed generation of Titanic dataset
is shown in Section 3.6.2. Netﬂix-Prize-Dataset [154] contains about 100 million
ratings in the scale from 1 to 5, given by 480, 189 users on 17, 770 movies. The
absence of rating is ﬁlled by the value 0. Since many movies have very few ratings
(i.e., the density of the full dataset is only 1.18%), we considered two datasets.

Chapter 3. Group Summarization for Model Inspection
65
Netﬂix-Prize-17770 denotes the full dataset containing all movies, and Netﬂix-
Prize-200 denotes the denser dataset with the density 25.66%, containing the 200
movies that have the most number of ratings. MovieLens-1B [128] is a synthetic
dataset consisting of 2,197,225 users and 855,776 items with binarized ratings, that
was expanded from the real-world rating dataset MovieLens-20M [155]. Due to the
extremely small density of original dataset (e.g., half of items have less than 0.02%
votes, the density of original dataset is 0.065%), we restricted to the 200 most
voted items, and the resulting data has N=2, 210, 078 rows and M=200 columns
with binarized ratings and density of 2.10%, denoting by MovieLens-1B-200.
The statistics of processed datasets are summarized in Table 3.5.
Table 3.5: Statistics of processed datasets.
Dataset
#Rows
#Cols
Density
Scales
Titanic dataset
240
9
100%
-
Netﬂix-Prize-17770
480,189
17,770
1.18%
1,2,3,4,5
Netﬂix-Prize-200
480,189
200
25.66%
1,2,3,4,5
MovieLens-1B-200
2,210,078
200
2.10%
0, 1
3.6.2
Group-level Model Inspection on Titanic
This experiment demonstrates an application of the GUM problem in explaining
the prediction of a black-box model. We consider the Titanic data, previously used
for Kaggle’s Titanic machine learning competition, which describes the survival
status (the class attribute) and other information of 891 passengers on Titanic,
342 survived and 549 died.
We removed the superﬁcial features passenger ID,
ticket number, cabin number, and name, and the remaining features are given in
Example 3.1. Our task is to explain the survival prediction made by a black-box
model. To this end, we trained a four-layer multilayer perceptron (MLP) model
F(x) using the dataset to predict the surviving probability of a passenger x. The
MLP model consists of 4 fully-connected layers of sizes (9, 20), (20, 20), (20, 10),
and (10, 2) with 2 output units and ReLU activation function, with the binary
cross entropy loss minimized by the SGD optimizer with learning rate 0.1. The
accuracy and F1-score (for the surviving class) on the data set are 84.06% and
77.17%, respectively.
Next, we created a user-item matrix D containing 240 predicted correctly survivors
of the full dataset, where there is a row r for each survivor x and there is a column

66
3.6. Evaluation
for each feature. The entry for a feature xi stores the Integrated Gradient (IG)
[30] of F(x) w.r.t. the feature xi, as explained in Example 3.1. The computation
of IG is based on a baseline point. We specify the baseline as the mean of all
dead passengers that are predicted as dead.
A solution to the GUM problem,
(D1, X1), · · · , (Dk, Xk), would provide a high level explanation for the prediction
of the survivors in D, where each Xj contains the l features that have largest sum
of IG for a group Dj.
Figure 3.1: Titanic dataset: AIU and running iterations for various k and l = 3.
The red dashed line represents the utility for top-l selection. The coeﬃcients of
variation of Random Init, KMA utility are within 74.92%, 2.75%.
Table 3.6: Titanic dataset: the groups produced with l = 2 and k = 3 by
Greedy and KMM. In this setting, both Greedy and KMM produce the same
results. For example, group 1 has 159 survivals and selects Pclass and Sex as top-
2 important features whose average item utility are 1.28 and 1.75 respectively.
The second row denotes the average utility of an item on the whole population.
The numbers in the parentheses represent the sizes of survivals.
Pclass
Sex
Age
SibSp
Parch
Fare
C
Q
S
Whole Population (240)
0.82
1.53
0.25
-0.01
0.06
-0.02
0.18
0.06
-0.06
Group 1 (159)
1.28
1.75
Group 2 (26)
1.80
0.98
Group 3 (55)
0.79
0.89
Figure 3.1 compares AIU and running iterations. “Optimal” represents the optimal
solution. The dashed line represents AIU of top-l items. First of all, Greedy and
KMM have almost the same utility as Optimal; KMS and KMA are slightly lower
than Optimal. While KMA is the result of a single initialization, it is about 2.60%
lower than KMM that is the best result of 50 initializations. Also, the coeﬃcient of
variation (CV) of KMA is within 2.75%, suggesting a small variance due to random
initialization. On the other hand, the single initialization o↵ers most eﬃciency for
dealing with large datasets, as shown in later experiments. Smart Init achieves

Chapter 3. Group Summarization for Model Inspection
67
competitive results for small k but performs poorly for larger k.
KMS always
improves on Smart Init. k-max algorithms consistently surpass the top-l selection,
which is the result for k = 1. This is consistent with Theorem 3.5. The right ﬁgure
shows the maximum number of iterations of the 50 runs for KMA, and the actual
number of iteration required of KMM and KMS over various k. We observe that
the largest iteration is 9, 6, 3 on Titanic dataset for KMA, KMM and KMS.
However, in Figure 3.1, CoClust and Random Init have the lowest utility, even
lower than that of top-l selection.
Due to nonoverlapping in columns, groups
produced by CoClust cannot select the top important features simultaneously, re-
sulting in the poor utility. K-CPGC selects 239 users from all 240 survivors in
the ﬁrst iteration. As we only keep the top-l columns if the column number of a
submatrix is greater than l, K-CPGC achieves the similar utility as top-l selection.
Table 3.6 shows the three groups produced by Greedy and KMM algorithms with
k = 3 and l = 2. As we can see, these features’ importance in their groups is usually
higher than on the whole dataset. Group 1 identiﬁes Pclass and Sex (i.e., female in
ﬁrst-class cabin) are top-2 important features for 159 survivors’ prediction; Group
2 selects Sex and Q (i.e., female and not embarked from Queenstown) are top-2
important features for 26 survivors’ prediction. 93.5% passengers embarked from
Queenstown are in third-class cabin; Sex and Age (i.e., young female) are top-
2 important features for 55 survivors’ prediction in Group 3.
This group level
summary provides an easier explanation on the prediction for the hundreds of
survivors, compared to reading the l most important features for each survivor
individually. On the other hand, top-l selection explains all survivors using the
same top-l features (i.e., Pclass and Sex), which fails to distinguish the di↵erences
for di↵erent groups.
3.6.3
Summarizing Preferences of Netﬂix Movies
The second experiment was conducted on Netﬂix-Prize-Dataset to summarize users’
preferences of movies. Our task is to answer “what kinds of movies people like (i.e.,
give a high rating)” using a compact summary with small k and l. We apply KMM
with k = 5 and l = 3 to Netﬂix-Prize-17770. Table 3.7 shows the k = 5 groups
generated and the l = 3 preferred movies for each group. “Pop” (Popularity) and
“Avg rating” are the number of ratings and the average rating of selected movies

68
3.6. Evaluation
Table 3.7: The Netﬂix-Prize-17770: X1, X2, X3, X4, X5 found by KMM with
l = 3 and k = 5. E.g., the ﬁrst group has size 172,843 and “Miss Congeniality”
is one of the top-3 movies in X1 and has 85,555 ratings with the average rating
of 3.66 in this group.
Group
ID
Movies
Genre
Pop
Avg
Rating
Group
Size
1
Miss Congeniality
Adventure|Comedy|Crime
85555
3.66
172843
The Patriot
Action|Thriller
77183
4.07
Independence Day
Action|Adventure|Sci-Fi|Thriller
78110
4.07
2
Pretty Woman
Comedy|Romance
67535
4.31
89646
Forrest Gump
Comedy|Drama|Romance|War
65743
4.58
The Green Mile
Crime|Drama
67825
4.53
3
Lord of the Rings: The Return of the King
Adventure|Fantasy
90008
4.79
101142
Lord of the Rings: The Fellowship of the Ring
Adventure|Fantasy
91358
4.77
Lord of the Rings: The Two Towers
Adventure|Fantasy
93855
4.76
4
The Royal Tenenbaums
Comedy|Drama
45443
3.93
60157
American Beauty
Comedy|Drama
42848
4.38
Pulp Fiction
Comedy|Crime|Drama|Thriller
44142
4.45
5
Pirates of the Caribbean: The Curse of ...
Action|Adventure|Comedy|Fantasy
33193
4.25
56401
The Day After Tomorrow
Action|Adventure|Drama|Sci-Fi|Thriller
38438
3.71
Man on Fire
Action|Crime|Drama|Mystery|Thriller
35539
4.21
Table 3.8: The Netﬂix-Prize-17770: the top-15 movies found by traditional
ranking methods by popularity (the total number of ratings) or average rating.
Top-15 Movies by Popularity
Top-15 Movies by Average Rating
Movies
Pop
Avg
Rat-
ing
Movies
Avg
Rat-
ing
Pop
Miss Congeniality
232944
3.36
Lord of the Rings: The Return of the King (Ex-
tended)
4.72
73335
Independence Day
216596
3.72
Lord of the Rings:
The Fellowship of the
Ring(Extended)
4.71
73422
The Patriot
200832
3.78
Lord of the Rings: The Two Towers(Extended)
4.70
74912
The Day After To-
morrow
196397
3.44
Lost: Season 1
4.67
7249
Pirates
of
the
Caribbean:
The
Curse of Black Pearl
193941
4.15
Battlestar Galactica: Season 1
4.64
1747
Pretty Woman
193295
3.91
Fullmetal Alchemist
4.61
1633
Forrest Gump
181508
4.30
Tenchi Muyo! Ryo Ohki
4.60
89
The Green Mile
181426
4.31
Trailer Park Boys: Season 3
4.60
75
Con Air
178068
3.45
Trailer Park Boys: Season 4
4.60
25
Twister
177556
3.41
The Shawshank Redemption: Special Edition
4.59
139660
Sweet
Home
Al-
abama
176539
3.54
Veronica Mars: Season 1
4.59
1238
Armageddon
171991
3.58
Ghost in the Shell: Stand Alone Complex: 2nd
Gig
4.59
220
The Rock
164792
3.77
The Simpsons: Season 6
4.58
8426
What Women Want
162597
3.43
Arrested Development: Season 2
4.58
6621
Bruce Almighty
160454
3.43
Inu-Yasha
4.55
1883

Chapter 3. Group Summarization for Model Inspection
69
Figure 3.2: Netﬂix-Prize-17770: The average item utility (AIU), running time
(in ln scale), and running iterations for various k (x-axis) and l = 30. The red
dash line represents the utility of top-l selection. CoClust, K-CPGC, Greedy and
SGreedy-✏were omitted due to long running time. The coeﬃcients of variation
of Random Init, KMA utility are within 6.96%, 1.21%.
Figure 3.3: Netﬂix-Prize-200: The average item utility (AIU), running time
(in ln scale), and running iterations for various k and l = 2. The red dash line
represents the utility of top-l selection. CoClust and K-CPGC cannot run within
our limited time and are not reported. The coeﬃcients of variation of AIU of
Random Init, SGreedy-0.5, SGreedy-0.9 and KMA and are within 6.90%, 1.14%,
1.40%, 5.71% respectively.
Figure 3.4: Netﬂix-Prize-200: The average item utility (AIU), running time
(in ln scale), and running iterations for various k and l = 3. The red dash line
represents the utility of top-l selection. CoClust, K-CPGC, Greedy and SGreedy-
0.5 were omitted due to long running time. The coeﬃcients of variation of AIU
of Random Init, SGreedy-0.9 and KMA are within 4.95%, 1.22% and 3.86%
respectively.

70
3.6. Evaluation
Figure 3.5: MovieLens-1B-200: The average item utility (AIU), running time
(in ln scale) and running iterations for various k and l = 30.
The red dash
line represents the utility of top-l selection.
CoClust, K-CPGC, Greedy and
SGreedy-✏are omitted due to long running time. The coeﬃcients of variation of
AIU of Random Init, KMA are within 2.60%, 4.15%.
in each group. For example, Group 3 summarizes the users who love the “Lord
of the Rings” series, which share the common theme of “adventure” and “fan-
tasy”. Groups 2 and 4 share similar interests in “comedy” and “drama” movies,
with Group 2 also loving “romance” movies. Groups 1 and 5 share the common
theme of “adventure” and “action” while group 5 also likes “comedy” and “drama”
movies. Such group-level preferences provide a concise summary of the entire user
population, with most movies selected having both a large number of ratings and
a large average rating in a group. In contrast, traditional ranking either by the
number of ratings or by the average rating will have either a large number of rat-
ings or a large average rating, but not both, and produces a single list of movies
for all users, shown in the Table 3.8.
Figure 3.2 compares AIU, running time and running iterations for various k (x-axis)
with l = 30 on Netﬂix-Prize-17770. Note that l is typically small because a large l
would overwhelm the analyst. On this large dataset, only k-max algorithms, Ran-
dom Init, and Smart Init can ﬁnish within our time limit (10 hours). In general,
KMM achieves slightly better utility and KMA and KMS have similar utilities and
surpass Smart Init and Random Init. Random Init has the worst utility on this
sparse dataset because most items have very few ratings. For running time, KMA
is a big winner. Smart Init takes a signiﬁcantly long time because of the intensive
invokes of top-rank algorithm to ﬁnd the next initialization greedily, which is the
computational bottleneck for the dataset with the larger number of movies. KMS
runs Smart Init in the initialization step, so is not faster than Smart Init. Overall,
KMA is a good trade-o↵between utility and eﬃciency on this large dataset. The

Chapter 3. Group Summarization for Model Inspection
71
right ﬁgure shows that the maximum iterations of 10 runs of KMA, actual itera-
tions of KMM and KMS over various k. We can see that KMA, KMM and KMS
stop within 19, 15 and 14 iterations.
Figures 3.3 and 3.4 report a similar study on the denser Netﬂix-Prize-200. K-CPGC
and CoClust were not included because they cannot be run eﬃciently. Moreover,
K-CPGC will return the entire matrix for a nonnegative matrix, which is not help-
ful for our purpose. Compared to the sparse Netﬂix-Prize-17770, all algorithms
have much higher utility than top-l selection on this denser dataset, even for Ran-
dom Init, because the utility maximization due to group partitioning beneﬁts more
from a denser matrix. In general, Greedy and SGreedy-✏have a better utility than
KMM, which has a better utility than KMS, which has a better utility than KMA.
Smart Init and Random Init have a lower utility with Random Init being the worst.
Greedy and SGreedy-0.5 run slow for a larger k and l, and are not included for
l = 3. For the running iterations, we can see KMA, KMM and KMS can converge
quickly within 9, 5 and 4 iterations respectively on Netﬂix-Prize-200 dataset. Con-
sidering both utility and eﬃciency, KMA, KMM and KMS are preferred as they
are less sensitive to larger k and l.
3.6.4
Scalability on MovieLens
The ﬁnal experiment was conducted on the larger dataset MovieLens-1B-200 which
has a larger number of rows than the above netﬂix prize datasets. The original
dataset MovieLens-1B [128] is a synthetic dataset consisting of 2,197,225 users
and 855,776 items with binarized ratings, that was expanded from the real-world
rating dataset MovieLens-20M [155]. Due to the extremely small density of original
dataset (e.g., half of items have less than 0.02% votes, the density of original dataset
is 0.065%), we restricted to the 200 most voted items, and the resulting data has
N=2, 210, 078 rows and M=200 columns with binarized ratings and density of 2.1%,
denoting by MovieLens-1B-200.
Figure 3.5 shows the average item utility, running time and running iterations on
the MovieLens-1B-200. Again, only k-max algorithms and Smart Init and Ran-
dom Init can run eﬃciently. CoClust, K-CPGC, Greedy and SGreedy-✏cannot
ﬁnish within a time limit (10 hours), therefore, are not reported here. Consis-
tent with previous experiments, KMA is most scalable and yet has a utility close

72
3.7. Conclusion
to KMM. Smart Init and Random Init have very poor utility. The right ﬁgure
demonstrates the maximum running iterations of KMA, the actual iterations of
KMM and KMS over various k are within 20, 18, 29.
Due to the lower density of this dataset, there are several di↵erent ﬁndings. First,
KMA, KMM, and KMS have a similar utility. This is because the small number
of 1’s in a row implies that there are few choices for the l-itemsets Xj, so di↵erent
algorithms tend to have similar utility.
Second, Smart Init has a signiﬁcantly
lower utility. In each round of Smart Init, Xj is selected as the top-l items in
the remaining data and those rows having Xj as their top-l items are removed.
However, few such rows were removed because few rows have Xj as their top-l
items due to the low rating density. Consequently, the remaining data still contains
most of rows in the next round, so the next Xj+1 will be similar to Xj and the
overall utility is similar to that of the top-l selection over the whole data.
3.6.5
Summary
While Greedy algorithm provides the approximation factor 1−1
e for a nonnegative
matrix, the k-max algorithms (i.e., KMM, KMA, and KMS) are more eﬃcient
for large datasets. The small gap between KMM and KMA, as well as the small
coeﬃcient of variation of KMA suggests that KMA is a good trade-o↵between
utility and eﬃciency. We recommend Greedy algorithms for small datasets and
parameters k and l, and KMM for larger datasets and parameters k and l, and
KMA for very large datasets and parameters k and l.
3.7
Conclusion
Summarizing a user-item matrix in terms of high utility items is of interest in many
real-world applications. Existing techniques (e.g., clustering, subgroup discovery)
fail to address this high utility requirement. We proposed a new summarization
technique, named group utility maximization, and prove that the optimal solution
is NP-hard. We proposed two algorithms, Greedy algorithm that adds one group at
a time and provides a theoretical approximation bound for a nonnegative matrix,
and the k-max algorithm that reﬁnes existing groups iteratively. Empirical studies
show that Greedy algorithm provides a good utility whereas k-max algorithm is a
good trade-o↵between utility and eﬃciency for dealing large datasets.

Chapter 4
The Skyline of Counterfactual
Explanations
The counterfactual explanation problem consists of three components: the pre-
trained model f, the cost function in the objective, and other constraints (includ-
ing user requirements/preferences). In Chapter 3, we propose a summarization
method to inspect the model f. However, previous works frame this problem as
a constrained cost minimization, where the cost is deﬁned as L1/L2 distance (or
variants) over multiple features to measure the change. In real-life applications,
features of di↵erent types are hardly comparable and it is diﬃcult to measure the
changes of heterogeneous features by a single cost function. Moreover, existing ap-
proaches do not support the interactive exploration of counterfactual explanations.
In this chapter, we propose to address the diﬃculty of deﬁning cost function over
heterogeneous features and favor user preferences.
In this chapter, we propose the skyline counterfactual explanations that deﬁne the
skyline of counterfactual explanations as all non-dominated changes. We solve this
problem as multiobjective optimization over actionable features. This approach
does not require any cost function over heterogeneous features. With the skyline,
the user can interactively and incrementally reﬁne their goals on the features and
magnitudes to be changed, especially when lacking prior knowledge to express their
needs precisely. Intensive experiment results on three real-life datasets demonstrate
73

74
4.1. Introduction
that the skyline method provides a friendly way for ﬁnding interesting counterfac-
tual explanations, and achieves superior results compared to the state-of-the-art
methods.
4.1
Introduction
Machine learning algorithms with millions of parameters are increasingly deployed
in real-life applications (e.g., ﬁnance, autopilot, security, medical) for automatic
decision-making. Despite the impressive performance, the large volume of trainable
parameters makes it diﬃcult to understand how a prediction is made by a machine
learning model, which is essential for high-stakes applications. Various machine
learning explanation methods [20–22, 30, 32] have been proposed to explain the
opaque behaviors of machine learning models. Counterfactual explanation
[31]
is a method for answering the question “What minimum changes are needed for
an input instance to ﬂip its bad prediction outcome by the model (e.g., a high
risk of disease), into a good one (e.g., a low risk of disease).” Therefore, it has
wide applications [156] in healthcare (altering an unhealthy situation to a healthy
one), ﬁnance (improving loan approval rate), education (improving school work),
marketing (customer retention or improving sales), etc.
Following an early counterfactual explanation work [31], much research goes into
adding various constraints (e.g., diversity[31, 47], reliability [36, 46], actionability
[35], sparsity [44], causality [46, 49, 50]), and searching strategies (e.g., gradient
descent [31, 46], FISTA[44, 48], integer program[35], mixed integer program [34]).
However, almost all methods [31, 44, 46–49, 52, 78, 79, 157] require a determinate
cost function d to measure the change of c from x in the feature space.
For
example, [31, 52] introduce the L2 distance on features scaled to the same range;
[44, 46, 47] adopt L1 or L2 distances weighted by the inverse median absolute
deviation (MAD) on heterogeneous features; [79] uses the Mahalanobis distance;
[48, 78] combine the mixture of above distance functions; [47, 158] allow users
to specify the relative feature weights and deﬁne a weighted distance. Instead of
minimizing the distance on the input space, [36] proposes a scalar objective on the
latent space via a variational autoencoder.
However, existing works su↵er from two major limitations:

Chapter 4. SkylineCF
75
Figure 4.1: The di↵erences between previous works and the proposed method.
Using 2 features “Education year” and “Salary” as an example, positive and
negative space are separated by the classiﬁer f. With a given cost function,
existing methods often return a single explanation as the red circle. Our method
returns the skyline of counterfactuals shown as the blue curve. The skyline of
counterfactuals forms the database and user can query the database interactively.
Diﬃculty of deﬁning a cost function d over heterogeneous features. It is
hard to compare changes of di↵erent features and deﬁne the cost of change by a
single cost function d. For example, it is unclear how to trade-o↵between 1-year
increase in “Education year” and 2 hour increase in “Work hour”. Similarly, it is
hard to compare changes between a numerical feature and a categorical feature.
In addition, the cost of change often depends on the current feature values of x
[156]. For example, improving the exam score from 50 to 60 is easier than from 90
to 100.
Lack of support for specifying constraints. It is hard for the user to spec-
ify the constraints precisely [31], especially when lacking domain knowledge and
alternatives available. For example, after knowing either a 1-year increase in “Ed-
ucation year” or a 2-hour increase in “Work hour” can ﬂip the bad prediction, the
user may change the initial constraint that no work hour is increased and prefer
a 2-hour increase in “Work hour”. This solution cannot be found if the user does
not know that it is available.

76
4.1. Introduction
We propose a skyline counterfactual explanation framework to mitigate the above
issues.
First, we formulate this problem as ﬁnding counterfactual explanations
with minimum changes under the multiobjective optimization (MOO) [159, 160].
Instead of ﬁnding a single or a few counterfactual explanations determined by
the minimum cost, our approach ﬁnds a set of all non-dominated counterfactual
explanations, called the skyline [161] or Pareto front [159], where a counterfactual
explanation c0 is dominated by another c if the change of c0 is no smaller than
that of c on every feature and is larger on at least one feature. Figure 4.1 shows
the skyline of counterfactual explanations. Importantly, the skyline computation
does not use any cost function d that must trade-o↵between di↵erent features.
For example, consider changes in (Education year, Salary): (+2, 0) is dominated
by (+1, 0), whereas (+1, 0) and (0, +5000) are non-dominated.
The skyline also provides a solution to the lack of prior knowledge in specifying
users’ constraints. Initially, the only constraints on the skyline are the basic require-
ments for non-dominated and valid counterfactuals, which leave the maximum set
of alternative counterfactual explanations for further exploration. With the skyline
being available, the user has the opportunity to reﬁne his criteria on counterfactual
explanations by ”querying” the skyline interactively and incrementally, where the
results of previous queries provide the context and prior knowledge for formulating
the next query. With a few queries and certain ranking criteria, the user quickly
identiﬁes several desired counterfactual explanations. To the best of our knowl-
edge, this is the ﬁrst work to use non-dominated counterfactual explanations to
deal with the diﬃculty of specifying the cost function and constraints.
The main contributions of our work are four folds:
• Section 4.2: We review related work and discuss the di↵erences of our work.
• Section 4.3: We formulate the counterfactual explanation problem as ﬁnding
the skyline of valid counterfactual explanations in a given feature space. This
skyline serves as the counterfactual database for exploratory discovery of
desired counterfactual explanations. We propose a query template for this
discovery process.
• Section 4.4: We propose a sample-directed method, named skyline counter-
factual algorithm (SkylineCF), to search for the skyline of counterfactual
explanations. The eﬃciency of this method is provided by considering the

Chapter 4. SkylineCF
77
search space deﬁned by straight lines from the input x to valid samples with
the target prediction y.
• Section 4.5: We evaluate the skyline approach using real life datasets by
common evaluation metrics, and we also showcase the power of querying the
skyline to discover interesting counterfactual explanations.
Notations. Frequently used notations are given in Table 4.1. An instance x can
be divided into actionable features xact and immutable features ximt, denoted as
x = (xact, ximt). Immutable features cannot be changed while actionable features
can be changed. Note that some actionable features are semi-actionable, which can
be changed only in one direction, such as age or education degree.
Table 4.1: Frequently used notations in Chapter 4.
Symbol
Description
x
A query instance to be explained
c
A counterfactual explanation of x
y
The desired target
f
The pretrained machine learning model
xj,cj
The feature j of x, c
ximt
The immutable features of x
xact
The actionable features of x
Fimt
The set of immutable features
Fact
The set of actionable features
4.2
Related Work
The counterfactual explanation was ﬁrst formulated as an optimization problem by
[31] with the objective function in Eq. (1.2). After that, most of the research focus
on adding various constraints and solving the constrained optimization problem.
However, most of the above works require a scalar cost function d(x, c) for measur-
ing the changes over multiple features in the input space. [36] optimizes the scalar
distance in the latent space, but the minimum explanation in latent space is not
equivalent to the minimum changes in the input space [162], and their experiments
reveal a higher degree of cost. Besides, minimizing the latent space distance can
alter the semi-actionable features in an impossible direction, e.g., reducing the age.

78
4.3. Overview
However, our skyline approach does not require a scalar cost function to measure
the changes over di↵erent features.
How to set the constraints for an appropriate counterfactual remains a challenge
[31]. This is because users often have trouble in expressing their preferences with-
out knowing what alternative solutions are available. Instead, the user often starts
with some imprecise estimates [163]. Two solutions in the literature are: (1) a user
tries several trails [31] using rough constraints, then tunes some trade-o↵factors
and stops until a ﬁnal explanation is found. This approach requires running an
algorithm multiple times, which is time-consuming; (2) directly o↵ering a diverse
set of counterfactuals [34, 47], but such counterfactuals do not necessarily meet the
user preferences. Our skyline provides all alternative solutions, i.e., valid counter-
factuals with minimum changes. With the skyline as the starting point, the user
will formulate her preferences interactively by specifying the next query based on
the examination of the results of previous queries to the skyline. The user does
not need to have a clear choice of preferences from the start.
4.3
Overview
We assume that the followings are given: a prediction model f, an input instance x
having undesirable prediction by f, and the desired target prediction y. The goal
is to identify promising counterfactual explanations c that convert the undesirable
prediction to the target y with the minimum cost measured by changes of features.
Previous works need to specify a scalar cost function over changes of multiple
features, which can be diﬃcult as discussed in Section 4.1.
To avoid specifying a scalar cost function over multiple features, our approach has
two steps. The ﬁrst step considers each actionable feature as an objective and
ﬁnds all explanations with minimum changes under multiobjective optimization
[159, 160]. The result is the set of “non-dominated” solutions over incomparable
objectives, a.k.a. skyline. In the second step, we propose a query interface to the
skyline to help the user locate the preferred counterfactuals interactively.

Chapter 4. SkylineCF
79
4.3.1
The Skyline Approach
For two solutions c and c0, c is said to dominate c0 on objectives h, denoted by
c ≻h c0, if c is no larger than c0 in each objective and c is smaller than c0 in at
least one objective. A solution is non-dominated if it is not dominated by any other
solution.
Consider the set of actionable features Fact. For a given input x and an explanation
c, the change of c on feature j is deﬁned by,
hj(c, x) = |cj −xj|,
j 2 Fact.
(4.1)
The changes of c with respect to x are given by h(c, x) = (h1(c, x),
. . . , hm(c, x)), where m = |Fact|. For a given search space C of counterfactuals
and an input point x, we deﬁne the x-skyline of C as the set of c in C such that
h(c, x) is not dominated by any h(c0, x) for c0 in C.
In the following deﬁnition, p denotes the probability of following the data distri-
bution, and r(c, x) returns 1 if the changes of c from x are in the valid directions,
otherwise, returns 0. For example, semi-actionable feature “working years” can
be altered in only one direction. Any c reducing the “working years” will have
r(c, x) = 0.
Deﬁnition 4.1. (Skyline counterfactual explanation problem) For a given search
space C and an input x, we want to ﬁnd the x-skyline of C, subject to the
constraints,
S = arg min
c2C
(h1(c, x), h2(c, x), . . . , h|Fact|(c, x))
(4.2)
s.t.
`(f(c), y) "
(4.3)
p(c) > γ
(4.4)
r(c, x) = 1
(4.5)
Eq. (6) ensures the closeness to the target prediction; Eq. (7) ensures that c follows
the data distribution; Eq. (8) ensures the change in the allowed directions. These
constraints ensure that S contains only valid changes. arg min( · ) returns the x-
skyline of all satisfying counterfactuals, i.e., the non-dominated counterfactuals on
the objectives h1, .., h|Fact|.

80
4.3. Overview
The loss function ` is a general loss covering both regression and classiﬁcation
models. For example, for a regression model f, `(f(c) −y) = (f(c) −y)2. For a
classiﬁcation model f, let fy(c) denote the probability of c predicted to have the
class y. There should be no penalty when fy(c) is greater than a speciﬁed threshold
[47], which can be speciﬁed using " = 0 and the following hinge-loss function,
`(f(c), y) = max(0, threshold −fy(c)).
(4.6)
4.3.2
Querying the Skyline
S contains all non-dominated counterfactual explanations satisfying the basic re-
quirements in Eqs. (6)-(8). The number of such counterfactuals can still be large,
and not all such counterfactual explanations are interesting to users. On the other
hand, users may not be always clear about their preferences from the start, espe-
cially before knowing what alternatives are available. In this context, the skyline
S provides exemplary alternatives for users to formulate their preferences. To this
end, we propose the following SQL-like interface to allow users to interactively
explore what alternatives are available in the skyline,
SELECT TOPhan integerihfeatures or *i
FROM S
WHERE hconstraintsi
ORDER BY hranking criteriai ASC | DESC
where the ASC and DESC specify the rank in ascending or descending order.
This query returns the top hintegeri counterfactuals in the skyline, ordered by
hranking criteriai, which satisfy hconstraintsi on the changes on actionable features.
hfeatures or *i speciﬁes the features returned.
For example, suppose that there are three actionable features: “Education Year”,
“Credit Points”, and “Salary”. Instead of having exact preferences on the changes
on these features, initially the user knows only that her salary can be increased
by at most $1000. With this in mind, she queries the skyline using the above
query with the constraint “Salary < 1000”. From the returned counterfactuals,

Chapter 4. SkylineCF
81
the user notes that some solutions have no changes on “Salary” but have increases
on “Credit Points”. Feeling that it is easier to increase credit points than increas-
ing salary, the user queries the skyline second time with the constraint “Salary = 0
AND Credit Point 100” and the ranking criteria on the sparsity of changes (de-
ﬁned in Eq. (4.7)), ordered by descending order. Suppose that the top 3 changes
returned are “Credit Points=100”, “Education Year = 2”, and “Credit Points=50
AND Education Year = 1”. Then, the user makes a ﬁnal decision from these alter-
natives based on her estimated diﬃculty of increasing credit points and increasing
education years.
In this example, the user does not have precise and clear preferences from the
start, and uses the query interface to interactively formulate her preferences by
recognizing what alternatives are available.
The basic idea is similar to query
reﬁnement for search engine query [163, 164], and is consistent with the cognition
theory that recognition is easier than description [165].
4.4
Skyline Counterfactual Algorithm
We now present our skyline counterfactual algorithm (SkylineCF) to solve the prob-
lem in Deﬁnition 1. The solution S depends on the search space C. On one hand,
C should be large enough to contain good solutions, on the other hand, it should
allow eﬃcient search. In the following, we ﬁrst propose several choices of C and
then present an algorithm for solving the problem in Deﬁnition 1 for a given C.
4.4.1
The Search Space C
A good counterfactual c should be close to the input x to have a small change,
whereas any observed point o with the target y being the true label and predicted
label in the data set is always a valid counterfactual for x. Therefore, to ﬁnd a
good counterfactual c, we can move from x towards o and sample the points along
the path at some step size, and eventually we will convert the current prediction
to the target prediction. A short travel in this move is along the straight line from
x to o. Based on this idea, C could be the set of sampled points on the straight

82
4.4. Skyline Counterfactual Algorithm
Figure 4.2: Purple color represents the input point x. Red colors represent the
observed points o with the target prediction. Blue colors represent the projection
o0 of o onto the actionable subspace. Black arrows from input to anchor points
specify the search space and directions.
lines from x to o. To support the straight line search, we assume that a categorical
feature j is encoded using one-hot encoding in Fact.
The question is how to choose the observed target points o. In general, more and
diverse observed points would lead to a larger search space, but also increase the
search time. If the data set is small, we can consider all observed target points o
with the target y being the true label and predicted label in the data set. For a
large data set, we can group observed target points into clusters and consider the
cluster centers, instead of individual observed points, as o. Note that a clustering
algorithm may require a distance function, but this step is external to our method
that is free of distance function. Other choices of o include several diverse observed
points that are far from each other, nearest target neighbours around x, or several
observed target points that serve as the prototypes of target. Let O denote the set
of chosen observed target points o. Our algorithm below takes O as input but is
independent of how O is chosen. Also, we continue referring to o as an observed
point although it could be a cluster center.

Chapter 4. SkylineCF
83
Function 4: AnchorSet(O, x)
Input: A set of observed target points O, an input point x.
Output: The set of non-dominated anchor points A.
A0 = { };
for o 2 O do
o0 = (oact, ximt) ;
// Project o onto the actionable subspace
if p(o0) ≥γ & `(f(o0), y) " & r(o0, x) = 1 then
A0 = A0 S{o0} ;
// Add valid counterfactuals o0 into A0
A = Skyline(A0, x) ;
// Prune dominated anchors
return A
The observed target point o may have di↵erent values from x on immutable fea-
tures, in which case the sampled points on the straight line from x to o, except
for x, will change the values on immutable features, which are not valid counter-
factual explanations. To avoid such changes, we replace o with another point o0
obtained by replacing the immutable features of o with those of x while keeping
all actionable features unchanged. Intuitively, o0 is the projection of o onto the
actionable subspace of x, as shown in Figure 4.2, where the actionable subspace is
determined by the immutable features of the input. To ensure o0 is a valid counter-
factual explanation, we require p(o0) > γ, `(f(o0), y) ", and r(o0, x) = 1. Only
valid counterfactual explanations o0 will serve as the search “destination”, a.k.a.
anchor points (blue points in Figure 4.2).
For the given collection of observed target points O, Function 4 computes the set
of anchor points, as discussed above. Considering some anchor points o0 domi-
nate other anchor points, we only keep the non-dominated points for eﬃciency by
applying the skyline operator Skyline( · ), implemented as in [161, 166–168], for
example.
Finally, we deﬁne our search space C as the set of sampled points on the straight
lines from the input x to the anchor points in the anchor set returned by Function
4. Importantly, as we shall see in Section 4.4.2, there is no need to materialize
the sampled points in C; instead, these points are implicitly represented by the
straight lines from the input x to the anchor points and the step size for sampling.

84
4.4. Skyline Counterfactual Algorithm
Algorithm 5: Skyline Counterfactual Algorithm (SkylineCF)
Input: A set of observed target points O, an input point x, and the number
of sampled points on each line s.
Output: A skyline of counterfactual explanations.
A  AnchorSet(O, x);
S0  LineSearch(A, x, s);
S  Skyline(S0, x);
return S
Function 6: LineSearch(A, x, s)
Input: Anchor set A, an input point x, and the number of sampled points on
each line s.
Output: A set of counterfactuals.
S0 = { } ;
// i-th point on the line.
for i  1, s do
t = s−i
s ⇤x + i
s ⇤o0;
if p(t) ≥γ and `(f(t), y) " then
S0 = S0 S{t} ;
// Add valid counterfactual.
Break ;
// Stop once the nearest one is found.
end
end
return S0;
4.4.2
SkylineCF
The main algorithm, SkylineCF, is given in Algorithm 5. The inputs contain a set of
observed target points O, an instance x, and the number of sampled points on each
line s. AnchorSet(O, x) ﬁnds the anchor set. LineSearch(A, x, s) searches over
the straight line from x to every anchor point until it ﬁnds the ﬁrst (i.e., nearest)
valid counterfactual. Note that checking r(t, x) = 1 in LineSearch(A, x, s) is not
needed because the anchor point o0 satisﬁes r(o0, x) = 1, which ensures that all
sampled points on the line also satisfy this condition.
On each line, s equally
spaced points are searched.
Exactly one counterfactual will be found for each
anchor point in A because the anchor point is a valid solution. Skyline(S0, x)
returns the x-skyline of the counterfactuals in S0.
The number of sampled points s serves as the trade-o↵between granularity of
search and eﬃciency of search.
A smaller s allows a more eﬃcient search but
has a higher chance of missing the nearest valid counterfactual. Assume there is
a neighborhood around o0 in which the condition p(t) ≥γ & `(f(t), y) " is

Chapter 4. SkylineCF
85
satisﬁed. So once we enter this neighborhood along the line search from x to o0,
this condition will remain satisﬁed for the remaining sampled points. Therefore,
even if we miss the nearest valid counterfactual on this line, the next searched point
is guaranteed to be a solution because it will satisfy the above condition, and this
solution is at most one search step away from the nearest solution. The same idea
also underlies the nearest neighbor classiﬁcation [169] where nearest neighbors are
used to estimate the class label for a query point.
Our method is orthogonal to the choices of the implementations for testing p(x) ≥γ
and for computing the skyline. As obtaining the prior probability distribution is
challenging, we use out-of-distribution (OOD) detectors, e.g., [170–172], to test
p(x) ≥γ. In our experiments, we choose the tree-based Isolation Forest [171] by
default because it avoids the use of distance function. The p( · ) can be replaced
arbitrarily as long as it is free of distance function. The skyline algorithms such
as [161, 166–168] can be used for the Skyline( · ) operator. We use Block-Nested-
Loops (BNL) algorithm [161] because of its good eﬃciency as discussed in [168].
4.4.3
Complexity Analysis
AnchorSet(O, x) of Function 4 iterates over each observed target point o in O
to deﬁne the corresponding anchor point o0.
Checking the condition in line 4
takes constant time, given the pre-trained functions f and p. Skyline(A0, x) in
line 8 when implemented by the BNL skyline algorithm [161] has the worst case
complexity of O(m ⇤|A0|2) and the average case complexity of O(m ⇤|A0|) [168],
where m is the number of objectives (i.e., the number of actionable features).
LineSearch(A, x, s) of Function 6 has the complexity of O(|A| ⇤s) because at
most s points on each line are searched and there is one line for each anchor point
in A. For Skyline(S0, x), from the discussion above, the worst case complexity is
O(m ⇤|S0|2) and the average case complexity is O(m ⇤|S0|).
To sum up, we note that |A0| |O| and |S0| |A| |O|. So the worst case
complexity of Algorithm 5 is O(m⇤|O|2 +|O|⇤s), and the average case complexity
is O(m ⇤|O| + |O| ⇤s).

86
4.5. Experiments
4.5
Experiments
In this section, we assess the counterfactual explanations generated by the proposed
SkylineCF algorithm on three public datasets, comparing the results with state-
of-the-art methods. After that, we present a case study to demonstrate how the
user narrows down the candidates using the query interface to explore the skyline
of counterfactual explanations interactively and incrementally.
4.5.1
Datasets
We consider the following datasets widely used in existing works [34, 36, 47] for
experimental evaluation.
UCI Adult Dataset [173]. This dataset contains 48, 842 records of the 1994 US
census database with 14 features (6 numerical, 1 ordinal, and 7 nominal features)
describing the demographic, educational, investment, and other information of a
person. The target variable indicates whether personal income is above $50, 000 or
not. A larger than $50, 000 income is viewed as the positive class, while the opposite
is the negative class. The yearly income of 37, 155 records is below $50, 000. We
ﬁll missing values with mean values for numerical features and mode values for
categorical features. A categorical feature is blocked into several coarse categories
as in [4], then encoded into one-hot vector. We treat “capital-loss”, “capital-gain”,
“occupation”, “hours-per-week”, and “workclass” as actionable features, and the
rest as immutable features.
Give Me Some Credit (GMSC)1. This dataset was used to predict the proba-
bility that someone will experience ﬁnancial distress in the next two years. It con-
tains ﬁnancial and demographic information of 150, 000 applicants where 139, 974
applicants are labeled as “good” (positive class) and 10, 026 applicants as “bad”
(negative class). All 10 features are numerical. We applied the same preprocessing
as 2 to ﬁll the missing values, remove outliers, delete irrelevant features, etc. As
the dataset is imbalanced, we select the ﬁrst 10, 026 “good” applicants and all the
10, 026 “bad” applicants to form the ﬁnal set. We treat “Age” and “NumberofDe-
pendents” as immutable features following [36].
1https://www.kaggle.com/c/GiveMeSomeCredit/overview
2https://www.kaggle.com/nicholasgah/eda-credit-scoring-top-100-on-leaderboard

Chapter 4. SkylineCF
87
HELOC Dataset3.
This dataset was used by the FICO explainable machine
learning challenge to predict whether a user will repay her HELOC account over
a two-year period. Each record has 23 numerical features of an anonymous ap-
plicant and the binary predicted target “RiskPerformance”, and there are 5,000
“good” records (positive class) and 5, 459 “bad” records (negative class).
We
treat “ExternalRiskEstimate”, “MSinceOldestTradeOpen” and “AverageMInFile”
as immutable features following [36].
For each dataset, we obtain the prediction model f and select the input instances
x as follows. First, we normalize all features by the min-max scaler into the range
[0, 1] and randomly split the records into the train/test sets at the ratio of 4 : 1
as in [47], and apply the 5-fold cross validation on the train set for tuning hyper-
parameters. Then, we train a 3-layer multilayer perception (MLP) model with two
hidden layer sizes of 20 and 10, using the Adam optimizer and 10−4 learning rate.
The test accuracy for the three datasets is 85.03%, 76.85%, 72.81%, respectively.
We use this MLP model as the black box f. As for the input instances x for
producing counterfactual explanations, for UCI Adult and GMSC, we consider the
ﬁrst 1, 000 negative samples in the test set that are also predicted as negative, and
for Heloc, all 808 negative samples in the test set that are predicted as negative.
We report the average evaluation score of the selected input instances x.
4.5.2
Evaluation Metrics
We shall evaluate our method through quantitative evaluation and use case eval-
uation. For the quantitative evaluation, we consider the following four evaluation
metrics. The use case evaluation is through the exploration of the skyline using
the query interface in Section 3.2.
Sparsity. The sparsity [47] is deﬁned as the percentage of actionable features not
changed,
Sparsity(x, c) =
1
|Fact|
X
j2Fact
1xj=cj.
(4.7)
c with a larger sparsity is preferred due to changes on fewer features.
3https://community.ﬁco.com/s/explainable-machine-learning-challenge

88
4.5. Experiments
Average Percentile Shift (APS). The change of numerical features is deﬁned
as the average percentile shift [35, 36]. Let Qj( · ) denote the percentile rank of a
value relative to all the values of feature j of the whole dataset. Fnum denotes the
set of actionable numerical features. The APS is deﬁned as follows,
APS(x, c) =
1
|Fnum|
X
j2Fnum
|Qj(xj) −Qj(cj)|.
(4.8)
Non-dominated Ratio (NR). Recall that a non-dominated counterfactual rep-
resents valid minimum changes that ﬂip the class in our search space, whereas for
each dominated counterfactual, there is a non-dominated counterfactual that has
smaller changes than the dominated counterfactual.
We deﬁne NR to measure
the ability of returning only non-dominated counterfactuals. Note that our sky-
line algorithm has the maximum NR of 1 because it returns only non-dominated
counterfactuals.
Rule-based Score (RBS). An important consideration for useful counterfactuals
is whether the change matches with prior knowledge on the relation between the
change and the target prediction. In particular, for some features, increasing (")
the feature value will increase (") or decrease (#) the target probability. We can
represent such prior knowledge by rules and use them to check the feasibility of
counterfactual explanations. A rule is generally written as,
feature "! target " or #
For a pre-selected set of rules R that models the prior knowledge, let R(x, c) denote
the set of rules in R whose feature on the left-hand-side has a non-zero change in c,
and let M(x, c) denote the set of rules in R(x, c) that are satisﬁed by the relation
between the target and the changes in c. RBS is deﬁned by,
RBS(x, c) = |M(x, c)|
|R(x, c)| .
(4.9)
For the UCI Adult data, we create 4 rules as prior knowledge:
• R1: “Capital-gain” " ! “income” ".
• R2: “Capital-loss” " ! “income” #.

Chapter 4. SkylineCF
89
• R3: “Occupation” " ! “income” ", where the values of “Occupation” are
ascendingly ordered by their average income:“Ser-vice” < “Admin” < “Blue-
Collar” < “Sales” < “Other” < “Military” < “Professional” < “White-
Collar”.
• R4: “Workclass” " ! “income” ", where the values of “Workclass” are as-
cendingly ordered by their average income: “Other/Unknown” < “Private”
< “Government” < “Self-Employed”.
R1 and R2 are common sense. R3 is a rule for the categorical feature “Occupation”
where the categories are ordered by their corresponding average income on the
whole dataset. Similarly, we create R4 for “Workclass” by ordering the categories
of “Workclass” using the average income.
For the GMSC data, “RevolvingUtilizationOfUnsecuredLines”, “NumberOfTime30-
59Days
PastDueNotWorse”, “NumberOfTimes90-DaysLate”, “NumberOfTime60-89DaysPastDueNot
Worse”, and “DebtRatio” are positively related to the ”Bad” prediction, whereas
“MonthlyIncome” are negatively related to the ”Bad” prediction. Therefore, we
have 6 rules for the GMSC data.
For the Heloc data, we use the 16 rules discussed in the link 4. These features are
monotonically decreasing or increasing with respect to the “Bad” probability.
4.5.3
Baselines
We compare the proposed SkylineCF algorithm with state-of-the-art methods. Our
method returns a skyline of variable size, we select top-k explanations from the
skyline ranked by the above four evaluation metrics respectively. If the size of
skyline is less than k, we report all counterfactuals of skyline. For a fair comparison,
we also consider k explanations found by the baseline methods.
The following
baselines are used in our experiments.
PlainCF [31]. This approach ﬁrstly frames the counterfactual explanation as the
objective in Eq. (1.1) and searches for the solution by gradient descent from a
random initialization. The L1 distance weighted by the inverse median absolute
4https://github.com/Trusted-AI/AIX360/blob/master/examples/tutorials/HELOC.ipynb

90
4.5. Experiments
deviation (MAD) is chosen as the cost function. We take k solutions obtained from
k random initializations.
DiCE [47].
This is one of the most popular methods released on GitHub for
counterfactual explanations. This method ﬁnds k diverse counterfactuals based
on the objective in Eq.
(2.23), with L1 distance weighted by MAD as the cost
function.
Growing Spheres (GS) [52]. This algorithm searches the counterfactuals from
random samples in a close sphere neighborhood of the input instance. The sphere
grows until a counterfactual is found. A post-processing step is adopted to enforce
the sparsity. Because of the random sampling in the neighborhood, we possibly
obtain di↵erent explanations in each run. We repeat this algorithm with random
samples until k counterfactuals are found.
C-CHVAE [36]. This algorithm maps the input x to the latent space using a
variational autoencoder and searches the closest counterfactual from a neighbor-
hood of the latent representation z like GS [52]. We also try several trails as GS
to ﬁnd k explanations.
For our SkylineCF algorithm, we set O as all true positive observations in the
training set for covering as much as good solutions. We set the number of sampled
points on each line s as 20, and the threshold in Eq. (4.6) as 0.7. The OOD thresh-
old γ of our method is determined automatically as discussed in the original paper
[171]. On the ﬁrst two datasets, we set r(x, c) = 1 (see Eq. (8)) for all counter-
factuals c for simplicity, and on the third dataset, we set r(x, c) such that the fea-
tures “NumTradesOpeninLast12M”, “MSinceMostRecentInqexcl7days” and “Nu-
mInqLast6M”, which represent user’s historical information, cannot be reduced.
4.5.4
Quantitative Evaluation
Figure 4.3 reports the results on the four quantitative evaluation metrics with the
x-axis representing the number of counterfactuals, k, and y-axis representing the
averaged metrics value of the k counterfactuals for the input instances considered.
Our SkylineCF achieves the highest RBS and NR, almost the lowest APS, and
competitive sparsity over all k values considered on the three datasets.

Chapter 4. SkylineCF
91
Figure 4.3: Each row represents the results on a dataset. Each column repre-
sents the results evaluated by a metric. The "/# represents that a higher/lower
score is better.
The lower APS means that SkylineCF achieves the desirable outcome with smaller
perturbations on continuous features. This result is reasonable as our algorithm
ﬁnds the ﬁrst (i.e., nearest) solution in the line search and ﬁlters dominated solu-
tions. For sparsity, SkylineCF falls only behind GS as expected because GS [52]
adopts the post-processing to enforce sparsity while SkylineCF does not optimize
sparsity directly. SkylineCF guarantees the highest NR because it returns only
non-dominated solutions.
In fact, the other methods do not enforce the “non-
dominated” requirement on returned solutions. GS’s NR drops quickly for a larger
k. This may result from its post-processing on sparsity, which tends to return
counterfactuals with changes on similar features, leading to dominated solutions.
Our method has a better balance on sparsity and NR.For RBS, we note that the
anchor point usually follows the relation speciﬁed by the predeﬁned rules, so do
the points on the straight line from the input instance to the anchor point.
The skyline of counterfactuals returned by SkylineCF provides a diverse set of
candidate solutions for user’s consideration. The user has the ﬂexibility to select
counterfactuals from such candidates to meet additional preferences. This aspect

92
4.5. Experiments
Figure 4.4: The use case of an input instance in UCI Adult Dataset. The
shaded features in the input instance are actionable features. The query process
starts with 58 counterfactual explanations in the skyline. With two queries Q1
and Q2, the user reaches a small set of candidates that can be looked at closely.
Other contains the jobs of protective-services and tech-support following [4].
is evaluated next.
4.5.5
Use Case Evaluation
In this section, we use the UCI Adult Dataset to illustrate how a user obtains
satisfactory counterfactual explanations by exploring the skyline interactively with
our query interface.
The user is represented by an input instance with yearly
income below $50, 000 and is also predicted by the model f as below $50, 000. The
user wants some recommendations of counterfactuals that can increase her income
to above $50, 000. With background knowledge, the user knows that the actionable
features are capital-gain, capital-loss, hours-per-week, workclass, and occupation.
She may also know that some of these features are easier to be changed than others,
for example, increasing hours-per-week might be hard but not impossible, so if there
are other options, she would prefer not increasing hours-per-week. Importantly,
while the user does have some preferences, such preferences may not be speciﬁed
precisely from the start without knowing the alternatives available. For this reason,
it would be very hard for existing methods, such as PlainCF, DiCE, GS and C-
CHVAE, to model user preferences through constrained optimization because they
do not o↵er all available alternatives to the user.

Chapter 4. SkylineCF
93
Now, let’s consider how the user could use our skyline-query interface to ﬁnd satis-
factory counterfactuals. The user’s input instance and the query process are shown
in Figure 4.4. The top table shows the feature values of the input instance. The
ﬁrst table on the second row shows the entire skyline of 58 counterfactual expla-
nations (showing actionable features only), all can increase her income to above
$50, 000. This skyline is produced from the 7, 592 observed target samples, which
are the true positive training samples (recall above $50, 000 is a positive class). As
we can see, the skyline operator has removed many non-dominated counterfactuals.
Since the size of the skyline is still large, the user applies her preferences to narrow
down the candidates using our query interface.
First query Q1: knowing that there are many alternatives available, the user applies
her ﬁrst preference of not increasing hours-per-week by issuing the ﬁrst query Q1
with the constraint “hours-per-week<=40”. 16 results are retrieved for this query.
Note that this preference was not speciﬁed an immutable feature because the user
is willing to increase hours-per-week if there is no other alternative. Before knowing
whether there is other alternative, she will leave hours-per-week as an actionable
feature.
Second query Q2: with 16 results retrieved, the user applies her next preference of
not increasing capital-gain and captial-loss too much, say no more than 5,000 and
0, respectively. 3 results are retrieved.
Final selection: at this time, since only 3 results are remaining, the user can
examine each closely. The user notices that the ﬁnal selection will be a trade-o↵
between an increase in capital-gain and a change of occupation. For example, the
ﬁrst result has a smaller increase in capital-gain but requires changing occupation
to White-collar. Since increasing capital-gain is an easier option, the user selects
the second and third results as the ﬁnal candidates.
Discussion. This example highlights several interesting properties of our approach
that stand out from existing works. First, the skyline contains all and only valid
counterfactuals that have minimal changes in our search space. Therefore, with the
skyline as the starting point, the user has access to all candidate solutions. Existing
works do not have this property. Second, the user does not need to have a clear
and precise notion of preferences from the start, instead, the user will interactively
formulate and prioritize her preferences “on-the-ﬂy” based on the result of previous

94
4.6. Conclusion
queries and impose such preferences through the next query. If many results are
returned by the previous query, the user could tighten up her preferences, and if no
or few results are returned, she could give up some preferences. Third, while the
user interactively formulates and explores her preferences, the SkylineCF is only
performed once. In contrast, existing works require rerunning the search algorithm
each time for reﬁning her preferences.
4.6
Conclusion
Previous work minimizes the changes of counterfactual explanations through a
scalar cost function, which is problematic because changes of di↵erent features are
incomparable. To address this issue, we formulated the minimum change problem
as ﬁnding the skyline of changes under the multi-objective optimization frame-
work. We proposed a novel solution to this problem to ﬁnd the skyline of changes.
To our knowledge, this is the ﬁrst work that ﬁnds the full set of valid counter-
factual explanations with minimum changes. This completeness provides the user
with a set of all possible candidates. We also presented a query interface to help
the user narrow down suitable candidates through interactively and incrementally
formulating her preferences based on previous query results. Our current work
has not considered causal relations between the target and features, and feature
correlations. Investigation into these issues will be our future work.

Chapter 5
Model Extraction Attack from
Counterfactual Explanations
Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS)
platforms to allow users to access counterfactual explanations (CFs) via APIs.
However, such information inevitably causes the cloud models to be more vulnera-
ble to extraction attacks which aim to steal the internal functionality of models in
the cloud. Due to the black-box nature of cloud models, however, a vast number of
queries are inevitably required by existing attack strategies before the substitute
model achieves high ﬁdelity. In this chapter, we propose a novel simple yet eﬃcient
querying strategy to greatly enhance the querying eﬃciency to steal a classiﬁcation
model. This is motivated by our observation that current querying strategies su↵er
from the decision boundary shift issue induced by taking far-distant queries and
close-to-boundary CFs into substitute model training. We then propose DualCF
strategy to circumvent the above issues, which is achieved by taking not only CF
but also counterfactual explanation of CF (CCF) as pairs of training samples for
the substitute model. Extensive and comprehensive experimental evaluations are
conducted on both synthetic and real-world datasets. The experimental results fa-
vorably illustrate that DualCF can produce a high-ﬁdelity model with fewer queries
eﬃciently and e↵ectively.
95

96
5.1. Introduction
Data
Train the
model
Cloud model
CF method
Counterfactual Explanation API
x
f(x)
Substitute model
Model Extraction Attacks
x
f(x), c & f(c) = y
Steal-ML
Model 
Extraction
x
f(x), c & f(c) = y
c
f(c), c' & f(c') = f(x)
DualCF 
(ours) 
Figure 5.1:
The workﬂow of model extraction attacks with counterfactual
explanations. Service providers train the cloud model on their private data and
deploy the counterfactual explanation service in the cloud. Adversaries aim to
construct a substitute model by leveraging the information provided by APIs.
Three types of model extraction attacks are illustrated. Steal-ML [5] only uses
the prediction output of the query x. Model Extraction [6] considers both the
query x and CF c while our proposed DualCF takes CF c and CCF c0 into
training. In Model Extraction Attacks module, “ ” denotes uploading a query
to the API and “!” denotes receiving outcomes from the API.
5.1
Introduction
Abundant machine learning models are deployed with automated decision-making
ability in various ﬁelds such as computer vision [7], medical diagnosis [8], recom-
mender systems [9], language translation [10], healthcare [174] and ﬁnance [175].
Due to the model/data privacy and computational capacity, the trained models
are usually deployed in the cloud via MLaaS platforms, with only public Applica-
tion Programming Interfaces (APIs) for remote access on a pay-per-query basis.
Inevitably, there exists a tension between public accessibility and model conﬁden-
tiality. On the one hand, the open APIs should be publicly accessible everywhere
and anytime. On the other hand, both the datasets and models are intellectual
property of the owners and should be kept private and conﬁdential since (1) model
training requires expensive costs on human power, data collection, and computa-
tion resources; (2) individual’s privacy rights should be protected from potential
reveals and attacks. Even so, the great commercial value of cloud model steers
adversaries to conduct model extraction attack, i.e., to steal the internal func-
tionality of cloud model to construct a substitute model without expensive cost,
which facilitates further data tampering to bypass monitoring [176] and stronger
attacks, e.g., adversarial attack [106], model inversion attack [107] and membership
inference attack [108].

Chapter 5. DualCF
97
Model extraction attack aims to obtain a functionally equivalent or near-equivalent
machine learning model, which achieves high agreement (up to 100%) with the
cloud model with as fewer queries as possible. We consider real-life scenarios for
model extraction attack where service providers (e.g., banks, health centers) deploy
machine learning services in the cloud and allow remote access via open APIs.
Users can query the API with an input to obtain the corresponding prediction.
We assume that (1) the training details, architectures, and parameters of the cloud
model are invisible to users; (2) users can obtain the data format, the number of
classes from the public proﬁle; and (3) users can collect a set of samples to query
multiple times. As the API is often on pay-per-query basis and the service provider
monitors the abnormal data traﬃc, fewer queries are preferred.
Among all these attacks, we focus on the model extraction attack with counter-
factual explanations (CF). Counterfactual explanations answer “what minimum
changes are needed for an input instance to alter the current prediction to a par-
ticular di↵erent one” [31]. Especially, for a given input and a pretrained model,
counterfactual explanation methods ﬁnd explanations that have minimal cost to
convert the current prediction to a di↵erent particular prediction, usually from an
undesirable prediction to a desirable one, subject to speciﬁed constraints. Note
that our proposed approach also uses counterfactual explanation methods to ﬂip
desired predictions to undesired ones. This is uncommon for general propose, but
such counterfactual explanation methods are capable of ﬂipping predictions of any
input. Counterfactual explanations from a desirable prediction to undesirable one
reveal the corresponding actions the subject should avoid to prevent the situation
from turning worse. As counterfactual explanations o↵er suggestions with mini-
mum cost to ﬂip the current prediction, therefore, they have broad applications
in healthcare (altering an unhealthy situation to a health one), ﬁnance (improving
the loan approval rate), school admission (obtaining a school o↵er), paper review
(minor revision for paper acceptance), and custom service recovery (regaining the
loyalty of unhappy customers). To better illustrate how counterfactual explana-
tions work, we take the loan application as an example: an applicant seeks a
mortgage from a loan-granting bank. The applicant submits his/her related in-
formation (including age, education, salary, credit score) to the bank. The bank
deploys a machine learning model with the binary classiﬁer and then denies this
loan application due to the submitted attributes of low salary and poor credit score.
Naturally, the individual seeks to know the reason behind the rejection and further

98
5.1. Introduction
to know the necessary changes required before the loan can be approved. The
decision-making system equipped with counterfactual explanations is able to pro-
vide constructive suggestions in a human-understandable way such as “increasing
the salary by 500 and increasing the credit score by 100” to the applicant, showing
the minimal improvement required before the application can be approved. Hence,
di↵erent from original cloud models merely making opaque algorithmic decisions,
cloud models equipped with CF provide additional information to better reﬂect
the underlying key factors that explain the outcome of cloud models. Both attacks
are illustrated in Fig. 5.1.
While much research [35, 44, 47, 49] studies how to improve the explanations to
meet user requirements, few works study the security and privacy issues of coun-
terfactual explanations, which are essential for safety-critical applications. Despite
the intuition that such additional information would naturally enhance the risk of
model leakage, up to now, researchers have not reached a consensus on the secu-
rity and privacy issues of model extraction attacks with CF. Some works [31, 57]
claim that CF allows individuals to receive explanations without conveying the
internal logic of the algorithmic black box since it only conveys a limited set of
dependencies on an input, while other researchers [6, 177] do not agree with the
above viewpoint. Recently, [6] experimentally veriﬁes that adversaries can extract
a high-ﬁdelity model with counterfactual explanations leaking the information of
decision boundary of cloud model. Nevertheless, due to the black-box property of
the cloud model, adversaries need to continuously query the API to collect suf-
ﬁcient information before the substitute model can mimic the cloud model with
a high agreement. With this in mind, we seek to propose a simple but eﬃcient
querying strategy, named DualCF, which greatly reduces the number of required
queries, and further lowers the overall cost.
Our querying strategy DualCF is motivated from the decision boundary shift issue
in existing Model Extraction method [6]. This issue refers to the decision bound-
ary of the substitute model shifting away from the decision boundary of the cloud
model when Model Extraction method takes CFs and queries into substitute model
training. As it is challenging and almost impossible to obtain the ground-truth de-
cision boundary and training data distribution of the cloud model in advance, the
random queries may be far from the decision boundary of cloud model. However,

Chapter 5. DualCF
99
Figure 5.2:
The illustration of decision boundary shift issue.
(a) and (b)
demonstrate that substitute model decision boundary shifts away from the
ground truth due to far-distant queries in existing method Model Extraction [6].
(c) shows that Model Extraction has to use more queries to mitigate this issue.
Our method DualCF can achieve comparable agreement with only one pair CF
and CCF, as shown in (d), which favorably illustrates the eﬃcacy of our method.
counterfactual explanations are close to the decision boundary regardless of what-
ever probability threshold of target class is selected for the stop condition. This is
because the cloud model tends to assign higher probabilities to all training samples
to minimize the training loss. As queries and counterfactual explanations have dif-
ferent predictions, substitute model tends to move towards the middle area between
them to separate them conﬁdently. Note that, however, the decision boundary of
cloud model is actually close to counterfactual explanations but far from queries.
We illustrate this issue in Fig. 5.2(a) in two-dimensional data space. As queries are
randomly selected at each time, the decision boundary of substitute model may
shift to di↵erent locations, as shown in Fig.
5.2(a) and Fig. 5.2(b). To mitigate
this issue, Model Extraction method has to require more queries at a higher attack
cost and hence lower eﬃciency, as shown in Fig. 5.2(c).
To reduce the number of queries resulting from the decision boundary shift issue,
we propose a simple yet eﬃcient method DualCF. The gist of our method is to ﬁnd
proper queries with similar distances to decision boundary of cloud model. Note
that explicitly selecting the close-to-boundary queries is quite diﬃcult without de-
cision boundary information of cloud model beforehand. To achieve so, our method
feeds the current counterfactual explanation (CF) of a query into the open API,
and obtains the counterfactual explanation of CF (named CCF for abbreviation).
The workﬂow di↵erences between our DualCF and existing methods (Steal-ML [5]
and Model Extraction [6]) are shown in Fig. 5.1. Note that CF and CCF are coun-
terfactual explanations of query and CF respectively, and they tend to locate in

100
5.1. Introduction
the region close to the decision boundary of cloud model but with di↵erent pre-
dictions. Lastly, we train the substitute model using pairs of CF and CCF. As
shown in Fig. 5.2(d), DualCF with only a pair of CF and CCF achieves compa-
rable agreements as Model Extraction. Due to the fact that CF and CCF have
the similar distances to the decision boundary of the cloud model and have oppo-
site predictions, the decision boundary shift issue in substitute model training is
mitigated. CF and CCF work similarly to support vectors in SVM [178] to help
infer the decision boundary. For any query, CF and CCF are always located in the
close-to-boundary region, a subset of full data space. Therefore, our method is less
sensitive to sampling procedure of queries from full data space.
Our proposed DualCF greatly reduces the queries theoretically and experimentally
for model extraction attack, which consolidates the viewpoint that counterfactual
explanations cause the model leakage. Our research triggers an alarm about the
privacy and security of counterfactual explanation service in the cloud and we
hope it motivates the corresponding protection strategy in the future. The main
contributions of this chapter are three-folds:
• We dive into the fundamental mechanism of counterfactual explanation gener-
ation, and bridge the model extraction attack and counterfactual explanation
theoretically.
• We observe that the bottleneck of existing attacks come from decision bound-
ary shift issue, which is caused by training on far-distant queries and CF
together. To enhance the eﬃciency, we propose a simple method DualCF
that leverages both the CF and the counterfactual explanation of CF (CCF)
into the substitute model training. As CF and CCF have the similar dis-
tances to the the decision boundary of cloud model, the boundary shift issue
is reduced.
• We conduct extensive experiments on synthetic and real-life datasets. Our
study shows that our proposed method can extract the high-ﬁdelity model
eﬃciently and e↵ectively compared with existing methods.

Chapter 5. DualCF
101
5.2
Related Work
Counterfactual Explanations. More complex models are deployed in the cloud
for automatic decisions. Due to the black box nature of cloud models, attempts
to explain the internal process for a prediction are required to enhance the model
trust and rectify negative decisions. Counterfactual explanations [31] provide a way
to understand the reasons on certain predictions and advice how to make small-
est changes to receive a desired prediction. The desired prediction contrasts with
the fact (current prediction), which is regarded “counterfactual”. Counterfactual
explanations mainly serve the following purposes: helping users understand why a
prediction is made; detecting the model bias for algorithmic fairness [179]; and pro-
viding suggestions to receive a desired result from current decision model [35, 180].
This problem is also studied under other research terminologies like recourse [35],
inverse classiﬁcation [52], and contrastive explanation [44]. Following [31], many
studies focus on how to model the practical and case-related requirements into
mathematical constraints and then solving problems with proper solvers.
Considering some features are immutable (e.g., race, gender), [35] introduces the ac-
tionability constraint by freezing the immutable features among improved instance
and original query. [46] requires the probability of counterfactual explanations to
follow the data distribution should be large enough to ensure plausible explana-
tions. Diverse explanations are possible to cover multiple choices for each query.
Based on this, [34, 47] incorporate diversity constraints on the generated set. As
sparser explanations are easier to be adopted for users, [44] enforces sparsity prop-
erty by adding L0 or L1 loss to penalize the changes over many entries of features.
In practice, changing a certain feature (e.g., education) may implicitly a↵ect other
features (e.g., salary). Hence, it is proper to consider relations [46, 49] or joint
e↵ect [181] between feature subsets into modeling. As some features are incom-
parable, [182] returns the skyline of non-dominated counterfactual explanations.
Once the objective and constraints are determined, it is crucial to develop proper
solvers to ﬁnd satisﬁed solutions. Typically, di↵erent solvers are required for dif-
ferent models and data properties. Besides, varying conﬁdential levels of cloud
models require di↵erent search process of solvers. For example, integer program-
ming [35] or mixed integer programming [34] solvers are used for linear models with
integer or categorical features. If a model is di↵erentiable, gradient descent can
be adopted [31]. Similarly, model-agnostic method growing spheres [52] searches

102
5.2. Related Work
the closest counterfactual explanation from the growing sphere around the query.
Dijkstra’s algorithm [54] is used for ﬁnding a feasible path from the query and the
closest counterfactual explanation. Feature tweaking [53] is designed to retrieve the
sub-path leading to target prediction from the current prediction path and select
the minimum perturbations for a decision tree model.
Model Extraction Attacks. Model extraction attack intends to train a substi-
tute model that approximates the cloud model well in terms of accuracy and ﬁ-
delity. The accuracy-oriented methods [101, 102] aim to create a substitute model
that has similar or better performance on a task as the cloud model, while the
ﬁdelity-oriented methods [103–105] target to reconstruct a high-ﬁdelity substitute
model that approximates the decision boundary of the cloud model as faithfully
as possible. In addition to the attacks based on counterfactual explanations, it is
also possible to conduct attack with prediction outputs, and gradients. Prediction
outputs are the most common outcomes from remote APIs, which consist of the
discrete predicted class, predicted class probability and probability vector. Ad-
versaries design various attack algorithms [5, 104, 106, 113, 114, 183] to extract a
high-ﬁdelity or high-accuracy model from remote predictions. The essential step
for these methods is using active learning [184] strategies to either generate infor-
mative synthetic points [5, 104, 106, 113] or select max-coverage points [114]. The
“informative points” represent the data points close to the decision boundary (i.e.,
adversarial examples [75]) of the cloud model, and the “max-coverage points” mean
the data points should be far distant between each other (i.e., core-set [185]). Gra-
dients help explain the model behavior upon an inﬁnitesimal perturbation [186],
a.k.a.
the sensitivity in the neighborhood.
As such, gradients can be used to
explain the feature importance for a prediction of a given input. [187] ﬁnds that
gradient-based feature importance methods can easily expose the cloud model to
adversaries. In particular, the gradient of a arbitrary instance is the model weights
for a linear model.
Despite the fact that counterfactual explanations do not disclose the cloud model in
its entirety, the security and privacy of CFs have largely been overlooked [179, 188].
Some works [31, 57] claim that counterfactual explanations cannot expose the inter-
nal algorithmic logic except a limited set of dependencies on a single instance. Even
though such limited information is trivial for extraction, gathering suﬃcient infor-
mation with more queries is prominent to conduct model extraction. [177] points

Chapter 5. DualCF
103
out that counterfactual explanations disclose more secrets of cloud model and can
enhance the attack eﬃciency. A recent study [6] ﬁrstly conducts the model ex-
traction attack on counterfactual explanations by viewing these close-to-boundary
explanations as additional training instances, but we observe that it su↵ers from
the decision boundary shift issue caused by far-distant queries especially when the
query size is small. Therefore, it requires higher attack cost to query more times.
Our proposed DualCF mitigates the boundary shift issue by introducing a novel
querying strategy, which also greatly reduces the querying cost.
5.3
Preliminaries
A cloud model f✓: X 2 Rd ! Y 2 R takes an arbitrary input query x 2 X as the
input, and predicts the output y 2 Y. In this chapter, we assume the cloud model
is a pretrained neural network model for classiﬁcation, parametrized by frozen
weights ✓. The counterfactual explanation method g : f✓⇥X ! X generates a
minimal perturbed instance c 2 X for the input instance x such that f(c) has the
desirable prediction. Formally, searching for the counterfactual explanation c can
be framed as the following mathematical formulation,
arg min
c
cost(x, c)
(5.1)
s.t. f(c) = y and f(c) 6= f(x)
where cost( · ) : X ⇥X ! R+ is a cost (distance) metric measuring the changes
between the input x and c, and y is the desirable target which is di↵erent from the
original prediction f(x). That is, we seek to ﬁnd counterfactual explanations that
belong to a target class y while still remains proximal to the original instance. In
addition to the constraints in Eq (5.1), more constraints such as sparsity, feasibility
and diversity can be added according to task-speciﬁc requirements [56], which are
left for future work.
In this chapter, we focus on the high-ﬁdelity extraction attack on counterfactual
explanations which aims to extract a functional equivalent or near-equivalent model
hφ that behaves very similarly to the model f✓. It can be formulated as the following
mathematical problem: for a set of queries D and a set of corresponding counter-
factual explanations, it ﬁnds a substitute model hφ that performs equivalently on

104
5.4. Proposed Approach
an evaluation set T .
max
X
xi2T
1f✓(xi)=hφ(xi)
(5.2)
s.t. hφ(x) = f✓(x), x 2 D
(5.3)
c = g(f✓, x), x 2 D
(5.4)
hφ(c) = f✓(c)
(5.5)
The existing method [6] feeds a query x into the API and then obtains the predic-
tion f(x) and counterfactual explanation c. The pairs of (x, f(x)) and (c, f(c))
are treated as the training set without discrimination to learn the substitute model
hφ.
5.4
Proposed Approach
5.4.1
Research Motivation
In this section, we observe that counterfactual explanations explicitly reveal not
only the decision boundary location of cloud model but also important features of
the cloud model. Such information is favorable for model extraction attack.
Here, we illustrate the information leakage through construction of counterfactual
explanations. Let {x1, x2, ..., xd} denote d-dimensional features of an instance x,
where each feature xi is associated with a feature importance value wi w.r.t. cloud
model’s prediction, e.g., wi is i-th coeﬃcient if f✓is a linear model. A larger wi
leads to a larger prediction change if we tweak the feature xi by the same amount.
For simplicity, we consider the scenario with 2 features (d = 2) and w1 > w2. In
Eq. (5.1), note that the prediction change ∆f = f(c)−f(x) from current prediction
f(x) to desired prediction f(c) should be strictly satisﬁed, and the explanations
with minimal cost (measured by some distance metrics) are preferred. We write
the objective on 2d example as,
arg min
c
||(c1 −x1, c2 −x2)||p, s.t. (c1 −x1, c2 −x2)(w1, w2)T = ∆f.

Chapter 5. DualCF
105
As w1 is larger in our assumption, smaller change (c1−x1) can satisfy the prediction
change in the constraint and then associates with a smaller cost in the objective.
Therefore, the ideal counterfactual explanation should change the feature x1 ﬁrst.
From the above analysis, we can conclude that counterfactual explanation methods
naturally give high priority to change the features that have larger importance to
the desirable prediction.
Essentially, model extraction attack seeks to infer the decision boundary of cloud
model from the parameter space. With the favorable information from counter-
factual explanations besides prediction output, the search process of substitute
model is accelerated. As counterfactual explanations and queries have di↵erent
predictions, there must exist a decision boundary to separate them. In addition,
the feature with minor change corresponds to the important features in the cloud
model. Therefore, we can infer the full or partial ranking of important features,
which also help enhance the attack eﬃciency. The closeness of counterfactual expla-
nations to decision boundary of cloud model should be also unearthed for eﬃcient
attacks.
5.4.2
Decision Boundary Shift Issue
The straightforward method takes counterfactual explanations and queries as train-
ing samples of the substitute model, as pointed in [6]. However, we observe that this
method su↵ers from the decision boundary shift issue, i.e., the substitute model’s
decision boundary shifts away from the ground truth. The issue is more severe
especially when query size is smaller. To relieve the issue, existing methods have
to adopt more queries, which result in higher querying cost. Here, we illustrate
where this issue comes from and how it a↵ects the attack eﬃciency.
In the beginning, adversaries know nothing about the decision boundary of cloud
model and therefore the queries to upload may be far from the decision boundary of
cloud model. However, counterfactual explanations produced by cloud model are
usually close to decision boundary, and in the other side of the decision boundary.
When we train the substitute model with counterfactual explanations and queries,
the decision boundary of the substitute model tends to move to the middle area of
the query and CF to separate them conﬁdently. This deviates from the fact that
decision boundary of the cloud model is close to counterfactual explanations and

106
5.4. Proposed Approach
Figure 5.3: The probability density contour lines during training stage (at
epoch 25, 50, 75, 100) of the cloud model trained on a synthetic dataset for
binary classiﬁcation task. The blue and purple dots are sampled points from
training set. The values on the contour lines represent the probability calculated
by the sigmoid function with range being [0, 1]. We can see the model tends to
assign high probabilities to more samples during training.
may be far from queries. As shown in Fig. 5.2(a) and (b), the substitute model’s
decision boundary is distorted by far-distant queries. Due to the sampling variance
in queries, the substitute model’s decision boundary may move to di↵erent regions.
This results in a unstable substitute model for an attack method. That is why
existing methods need to use more queries to achieve high ﬁdelity and algorithm
robustness.
For probabilistic models, the constraint f(c) = y in Eq. (5.1) becomes p(y|c) > ✏,
meaning that the counterfactual explanation method searches the closest explana-
tions above a given probability threshold ✏of target class. As shown in Fig. 5.3(a),
regions closer to decision boundary correspond to probability around 0.5, wherein
the model is uncertain about its predictions. Regions far away from decision bound-
ary, on the contrary, correspond to either smaller or larger values, and the values
indicate more conﬁdent predictions on class 0 and 1, respectively. In this case, could
a higher probability threshold ✏push counterfactual explanations away from the
decision boundary to reduce the issue? The answer is yes, but the fact is that even
if we set a higher probability threshold, counterfactual explanations are still close
to the decision boundary. We observe that this issue roots in the over-conﬁdent
predictions phenomenon in cloud models. That is, deep neural networks tend to
assign high probability to all training instances, resulting in that instances with
high probability can be close to the decision boundary, as shown in Fig. 5.3(d).
We discuss this phenomenon in the following.
For a general neural network model for classiﬁcation, the probability of predicted
label belonging to class k 2 [1, K] is computed as p(k) =
exp(zk)
PK
i=1 exp(zi) or p(k) = σ(zk),

Chapter 5. DualCF
107
where zi, zk represent the logits in the last layer of the neural network. The cross-
entropy loss for classiﬁcation is deﬁned as ` = −PK
k=1 q(k) log(p(k)), where q is
a one-hot vector for ground truth. Minimizing this loss function is equivalent to
maximize the likelihood of ground truth, in other words, zk >> zi for i 6= k is
desired if the ground truth class is k. As all training points are undi↵erentiated
during training, the model tends to assign higher probability on actual class entry
for each instance in order to minimize the loss function. This brings the over-
conﬁdent prediction problem: the probability increases quickly and ﬂattens if we
gradually move an instance away from the decision boundary. We illustrate this
with a binary classiﬁcation model on a synthetic dataset in Fig. 5.3. The ﬁgure
shows that more and more instances (i.e., instances in the left side of < 10%
contour and instances in the right side of > 90% contour) have high probability
(> 90% for class 1 or < 10% for class 0) and hence the probability density contour
lines become denser and denser during training. Therefore, simply adjusting the
probability threshold ✏cannot ease the issue since counterfactual explanations are
consistently close to the decision boundary due to the over-conﬁdent prediction
problem.
5.4.3
Proposed Method: DualCF
To overcome the decision boundary shift issue resulting from far-distant queries,
we propose our algorithm DualCF by considering both CF and the counterfactual
explanation of CF (CCF) as pairs of training samples for substitute model. As
discussed previously, the decision boundary of the substitute model is distorted by
taking far-distant queries and close-to-boundary counterfactual explanations into
training. Our intuition is that, if training samples with di↵erent classes have similar
distances to the decision boundary of cloud model, then this issue can be alleviated.
As counterfactual explanations locate in the close-to-boundary region, could we
search close-to-boundary queries directly? Actually, we do not know the decision
boundary of cloud model in the beginning. Without decision boundary information,
it is impossible to ﬁnd the close-to-boundary queries directly. However, we can treat
the counterfactual explanation of an arbitrary query as another query to the cloud
API and obtain a CCF with the same prediction of original query. Since both CF
and CCF are produced by the same counterfactual explanation API, CF and CCF
have the similar distances to the decision boundary of the cloud model naturally.

108
5.4. Proposed Approach
In addition, CCF and CF reside in opposite regions of the decision boundary due
to their di↵erent predictions of classes. As such, the decision boundary shift issue
is mitigated by adopting CF and CCF into substitute model training.
The proposed DualCF is listed in Algorithm 7. Our method ﬁrst uploads a query
x into the counterfactual explanation API and obtain an explanation c (line 5).
Secondly, our method treats current explanation c as another query and obtains
a CCF c0 (line 6). Similarly, we generate the pair of CF and CCF for each of N
queries and denote them as S. Finally, we train the substitute model hφ with S. It
needs to be emphasized that c and its corresponding c0 should be used in the same
batch in order to mitigate the decision boundary shift issue. Hence, our proposed
DualCF is a simple yet eﬃcient querying strategy. The major di↵erence between
DualCF and recent studies [5, 6] is how to construct the training set for hφ. Steal-
ML [5] collects (x, f(x)), Model Extraction [6] adopts (c, f(c)) and (x, f(x)) while
our proposed DualCF uses (c, f(c)) and (c0, f(c0)). Note that the initial query x
is not used as training set in our DualCF. We also introduce a variant of our
proposed method, denoted as DualCFX, which takes x as training data as well,
i.e., (c, f(c)), (c0, f(c0)), and (x, f(x)).
Function 7: DualCF
Input: Queries {xi}N
i=1, cloud model f✓, counterfactual explanation API g( · ).
Output: Substitute model hφ
Initialize empty training set for substitute model S = {}. for i< N do
c = g(f✓, x);
c0 = g(f✓, c);
S = S S{(c, f(c)), (c0, f(c0))};
cur iter = 0;
while cur iter < max iter do
Minimize the training loss of hφ on dataset S;
cur iter++ ;
return hφ;
We visualize the di↵erences between existing attack methods and DualCF in Fig. 5.4
on a binary dataset. Two classes (shown as blue and purple dots) are separated
by the model f✓. Three orange dots are three sampled queries, red hexogons and
green triangles are CFs and CCFs respectively. The queries, corresponding CFs and
CCFs are numbered by 0, 1, 2. Steal-ML [5] with prediction output only achieves
lowest agreement. Model Extraction adds counterfactual explanations into training
and achieves higher agreement. We also notice the decision boundary are pulled

Chapter 5. DualCF
109
Figure 5.4:
The toy examples on a binary synthetic dataset.
We show 3
queries and corresponding CFs and CCFs (orange dots, green triangles, and red
hexagons) in ﬁgure (a). Figure (b), (c) and (d) illustrate the substitute model’s
decision boundary produced by 3 attack methods.
Steal-ML uses prediction
outputs of queries, Model Extraction applies both queries and CFs, and our
proposed DualCF considers CFs and CCFs. Our proposed DualCF achieves the
highest agreement.
away from the ground truth by far-distant queries in Fig. 5.4(c). With CFs and
CCFs as training pairs, our method obtains a substitute model with the highest
agreement.
From the above analysis, we can see the advantages of DualCF. CF c and CCF
c0 are both close to decision boundary and have di↵erent predictions, not only
reducing the boundary shift issue but also bringing a tighter space for inferring
the decision boundary of cloud model. The CF c and CCF c work as the support
vectors in SVM algorithm [178]. Secondly, the number of each class is balanced
favoring the learning process. Thirdly, our method is less sensitive to the sampling
procedure of the queries because CF and CCF locate in the denser region close to
the decision boundary.
5.4.4
DualCF for A Linear Model
Here, we illustrate how our DualCF extracts a functionally equivalent model from
only a pair of CF and CCF for a linear model. Suppose we have a binary linear
model f✓= σ(h✓, xi + b) , where the decision boundary is determined by the
parameter ✓and σ is the sigmoid function.
For a query x, its counterfactual
explanation c can be optimally found by searching along the direction of gradient
(f 0
✓(x) = f✓(x) ⇤(1 −f✓(x)) ⇤✓), stopping when it reaches a certain probability
✏belonging to the target class. Using the API, we obtain the CF and CCF of a
query.

110
5.5. Experiments
Lemma 5.1. For a binary linear model f✓, we can extract a substitute model hφ
with 100% agreement, from a pair of CF c and CCF c0 given an input x.
Proof. As the model is linear, the CF and CCF can be optimally found along or
against the gradient direction. We draw a straight line through the input point
x which is perpendicular to the decision boundary. The intersection point of the
straight line and decision boundary is denoted as x0. Then, the CF and CCF can
be written as
c = x0 + a✓
(5.6)
c0 = x0 −a✓
(5.7)
respectively, where a depends on the probability threshold for a counterfactual
explanation. As we have c and c0 on hand, we can obtain a point x0 = c+c0
2
on
the decision boundary. Besides, from any two of x, c and c0, we can obtain the
slope ✓. With a slope and a point in the line, we can obtain the decision boundary
without any training. The lemma holds.
Although extending the lemma to a nonlinear model remains unsolved, this linear
scenario veriﬁes that our proposed method can replicate the cloud model eﬃciently.
In the experiment section, we evaluate the proposed DualCF in more general situ-
ations where the cloud model is nonlinear and complex.
5.5
Experiments
In this section, we conduct extensive experiments to evaluate the proposed Du-
alCF and its variant DualCFX on both synthetic and real-life datasets to compare
with state-of-the-art methods.
5.5.1
Baselines
We employ the following state-of-the-art methods to verify the performance of our
method.

Chapter 5. DualCF
111
• Steal-ML [5]. This method ﬁrst labels the queries with the cloud model f✓,
and then trains the model hφ on the labeled dataset. We randomly select
queries for this method.
• Steal-ML (CoreSet) [114]. As random selection may choose redundant or
similar samples that do not bring more useful information for extraction, we
consider the second baseline that leverages the CoreSet algorithm in [105] to
select the most distant samples to query the cloud model.
• Model Extraction [6]. This method is proposed to extract a substitute
model for counterfactual explanation methods. It utilizes the prediction of
queries and counterfactual explanations from the cloud model to train the
substitute model.
5.5.2
Implementation Details
We ﬁrst train a Multilayer Perceptron (MLP) model f✓as the cloud model for
each dataset.
For the counterfactual explanation method, we use the existing
algorithm [31] implemented in DiCE [189] (without diverse requirements) for the
cloud model. The default objective function d( · ) is the L2 distance metric on
normalized features.
Figure 5.5 visualizes counterfactual explanations of sampled
Figure 5.5: Counterfactual explanations visualization on Syn-Linear and Syn-
Nonlinear datasets.
queries on two synthetic datasets. From the ﬁgure, we can see all counterfactual

112
5.5. Experiments
explanations locate in the close-to-boundary regions. The CCFs and queries have
the same prediction from the cloud model. CCFs and CFs have similar distances
to the decision boundary of the cloud model and have opposite predictions.
After that, we train another MLP model hφ to approximate cloud model f✓. For
simplicity, the cloud model and the substitute model have the same architecture,
but we train both models with di↵erent random initializations and di↵erent data.
In the following experiments, we also study the inﬂuence of substitute models with
di↵erent capacities from the cloud model. Adam optimizer is used for minimizing
the binary cross-entropy loss of the cloud model and substitute model. We stop the
substitute model training until it reaches a maximum epoch. For a fair comparison,
all substitute models for all baselines and proposed methods on each dataset have
the same training settings and architectures.
As shown in Table 5.1, the substitute models are MLP. We adopt the Adam op-
timizer to minimize the binary cross entropy loss for the substitute model. The
learning rate and training epochs are tuned by the curve of training loss.
Table 5.1: The model architectures, hyper-parameters used for training sub-
stitute models on synthetic and real-life datasets.
Datasets
Syn-Linear
Syn-Nonliear
GMSC
Heloc-10
Boston-Housing
Model
Linear(2, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(2, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(12, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 1), Sigmoid
Optimizer
Adam
Adam
Adam
Adam
Adam
Loss
Binary cross entropy
(BCE)
BCE
BCE
BCE
BCE
Lr
0.005
0.005
0.01
0.01
0.005
Batch Size
32
32
32
32
32
Epoch
200
500
200
200
200
5.5.3
Evaluation Metrics
As we target to build a high-ﬁdelity model hφ that behaves as similar to f✓as
possible, we ﬁrstly deﬁne the agreement which measures the prediction di↵erence
between the f✓and hφ on a evaluation set of size n,
Agreement = 1
n
n
X
i=1
1f✓(xi)=hφ(xi).
(5.8)
A higher agreement is better for substitute models on the same queries. To show
how the query size inﬂuences the algorithm behavior, we gradually select an increas-
ing number of queries and report the agreement on them. A higher agreement curve

Chapter 5. DualCF
113
is better. To reduce the experiment variance due to random sampling of queries
and random initialization in model training, we compute the average agreement
over 100 runs for a ﬁxed query size. We also compare the standard deviation
(std) of 100 runs to measure the algorithmic stability. The smaller standard devi-
ation means that the attack method is more stable for the sampling procedure and
model initialization. If two model extraction methods have the same agreement,
then a lower standard deviation is preferred.
5.5.4
Datasets
We consider the following 5 datasets, which are widely used in research on coun-
terfactual explanations for evaluation.
• Synthetic Dataset. We generate two 2-dimensional datasets where all data
points are uniformly sampled from a close interval [0, 6]. We draw a straight
line on the ﬁrst dataset and an S-curve on the second dataset, to separate
data points of each dataset into two parts and assign each part with a label.
The ﬁrst dataset is named Syn-Linear and the second dataset is named
Syn-Nonlinear respectively.
• Give Me Some Credit (GMSC) [190]. This dataset is collected for pre-
dicting whether someone will experience ﬁnancial distress in the next two
years by his/her ﬁnancial and demographic information (10 numerical fea-
tures). The full dataset contains 150, 000 applicants where 139, 974 appli-
cants are labeled as “good” and 10, 026 applicants are labeled as “bad”. We
select the ﬁrst 10, 026 “good” records and all “bad” records to form the ﬁnal
dataset for balance. We follow the pre-processing procedure in [191] to ﬁll in
the missing values, remove outliers and delete irrelevant features, etc.
• Heloc Dataset [192]. It is used for predicting whether an individual will
repay the Heloc account in two years by 23 numerical features describing
the personal information. The target variable ”RiskPerformance” is binary,
5000 “good” records indicating no default, 5459 “bad” records indicating the
opposite. We keep the top-10 important features based on the analysis [193]
of the IBM team, which is the champion of FICO Heloc challenge. We adopt
the same pre-processing in [193] to remove abnormal values.

114
5.5. Experiments
• Boston Housing Dataset [194]. The dataset has 506 records, where each
record has 12 features for predicting the house price in Boston. We follow
the pre-processing of the alibi tutorial [195], which ﬁrstly transforms the
continuous labels into binary classes based on whether the house price is
above the median or not, and then removes three categorical features.
We split these datasets into 3 disjointed sets: training, query and evaluation sets at
the ratio of 50%, 25%, 25%. We describe them in detail as follows: (1) training set
(50%) is used for training the cloud model f✓; (2) query set (25%) is used to feed
into counterfactual explanation API g( · ) for obtaining counterfactual explanations,
and we train the substitute model hφ on them. Note that we merely upload a subset
of queries to counterfactual explanation API in our experiments; (3) evaluation
set (25%) is for evaluating how faithful the substitute model hφ is. As we target
to build a high-ﬁdelity model, we use the predicted label instead of the true label
on the evaluation set. A validation set for tuning the cloud model f is omitted
because we assume the cloud model is given and frozen. Therefore, we directly use
the architecture and hyper-parameters from the public to train the cloud model f✓.
For the cloud model, we normalize the features by standard normalization where
the mean value and variance are estimated on the training set. For the substitute
model, we compute the mean value and variance on the full query set for standard
normalization.
5.5.5
Experimental Results
We report competitive experimental results on synthetic and real-life datasets in
Fig. 5.6.
The x-axis represents the size of queries in each run.
The y-axis of
the upper ﬁgures represents the average agreement of 100 runs while the y-axis
of the bottom ﬁgures describes the standard deviation of agreements of 100 runs.
From them, we can see (1) Extraction methods on counterfactual explanations
achieve better results than those merely on prediction output. This is consistent
with our previous analysis where counterfactual explanations leak the boundary
information of the cloud model which can help the model extraction; (2) DualCF
and DualCFX achieve the best agreement on ﬁve datasets when the query size is
small; (3) DualCFX will improve the DualCF slightly because it takes the queries
into training. More training data will result in better performance. Therefore, we

Chapter 5. DualCF
115
Figure 5.6: Experiment results on synthetic and real-life datasets. The upper
ﬁgures report the average agreement while the bottom ﬁgures report the standard
deviation of agreements of 100 runs for a ﬁxed size of queries. "/# mean the
higher/lower results are better.
suggest adopting the DualCFX as the default method; (4) The standard deviation
of agreements of our proposed methods is smaller than baseline methods in general,
which means our methods are less sensitive to the sampling procedure; (5) When
the query size increases, the gaps between all algorithms decrease because too many
predictions from the cloud model are enough to train a good substitute model.
5.5.6
Ablation Studies
Next, we perform several ablation studies to investigate the e↵ects of some exper-
iment factors on extraction performance.
Model Capacity. We ﬁrst study how the capacity of the substitute model a↵ects
the extraction results. The baseline substitute model is an MLP classiﬁer, and we
consider three variants of the baseline architecture: removing 50% nodes of the
last layer, adding 50% nodes to the last layer, and adding one layer. We detail the
baseline architectures and their variants on each dataset in Table 5.2. We ﬁrst train
the baseline architectures and their three variants on the same experiment settings.
“Remove Nodes” and “Add Nodes” represent that we remove 50% nodes of the
last layer and add 50% nodes on the last layer. “Add Layer” means we add one
layer. After that, we train these models on the same query set and counterfactual
explanations and then report the agreement in Fig. 5.7. We can see the di↵erences

116
5.5. Experiments
Figure 5.7: Experiment comparisons on di↵erent model capacities in proposed
DualCF. “Remove Nodes”, “Add Nodes”, and “Add Layer” represent three vari-
ants of baseline architectures.
Figure 5.8: Experimental comparisons on di↵erent thresholds in counterfactual
explanation generation in our proposed DualCF. We select di↵erent thresholds
{0.6, 0.7, 0.8, 0.9} as the prediction conﬁdence to generate di↵erent CFs and
CCFs.
Figure 5.9: Experimental comparisons on di↵erent cost metrics in our proposed
DualCF.
between substitute models with di↵erent capacities are inconsiderable, implying
that the model capacity does not play a key role in the extraction attack. A similar
conclusion can be found in [6, 106]. As the architectures of the cloud model are
often unknown to adversaries, selecting the best model is challenging. Adversaries
can select a substitute model based on prior knowledge, e.g., the state-of-the-art
models for the same or similar task.
Threshold. Counterfactual explanation methods search an explanation above a
probability threshold of target class. Here we empirically investigate the inﬂuence

Chapter 5. DualCF
117
Table 5.2: The models used in model capacity studies.
Datasets
Syn-Linear
Syn-Nonliear
GMSC
Heloc-10
Boston-Housing
Base
Model
Linear(2, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(2, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(12, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 1), Sigmoid
Remove Nodes
Linear(2, 5), ReLU,
Linear(5, 1), Sigmoid
Linear(2, 20), ReLU,
Linear(20, 5), ReLU,
Linear(5, 1), Sigmoid
Linear(12, 20), ReLU,
Linear(20, 5), ReLU,
Linear(5, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 5), ReLU,
Linear(5, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 5), ReLU,
Linear(5, 1), Sigmoid
Add Nodes
Linear(2, 15), ReLU,
Linear(15, 1), Sigmoid
Linear(2, 20), ReLU,
Linear(20, 15), ReLU,
Linear(15, 1), Sigmoid
Linear(12, 20), ReLU,
Linear(20, 15), ReLU,
Linear(15, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 15), ReLU,
Linear(15, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 15), ReLU,
Linear(15, 1), Sigmoid
Add Layer
Linear(2, 10), ReLU,
Linear(10, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(2, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(12, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 10), ReLU,
Linear(10, 1), Sigmoid
Linear(10, 20), ReLU,
Linear(20, 10), ReLU,
Linear(10, 10), ReLU,
Linear(10, 1), Sigmoid
of di↵erent thresholds. We set the threshold from 0.6 to 0.9 by 0.1 and gener-
ate di↵erent counterfactual explanations for each threshold with the same method.
Then, we train a substitute model with DualCF on CFs and CCFs for each thresh-
old respectively. The experiment results are shown in Fig. 5.8. The agreement
di↵erences between di↵erent thresholds are slight on the ﬁve datasets. This is con-
sistent to our discussion in Section 5.4.2. In the training stage, the cloud model
tends to assign high probability to the training points to minimize the training loss.
With a higher probability threshold, CF and CCF are still close to the decision
boundary of cloud model.
The Cost Metric cost( · ). A proper distance metric is essential for ﬁnding mean-
ingful proximal instances. Here, we investigate the eﬃcacy of the following three
commonly used distance metrics, i.e.,
• `2: Pd
i=1(xi −ci)2.
• `1: Pd
i=1 |xi −ci|.
• `1/MAD: Pd
i=1
|xi−ci|
MADi with MADi is the median absolute deviation of feature
i over the training set.
As shown in Fig. 5.9, our proposed model is consistently robust to the choice of
distance metric on synthetic datasets. On the rest datasets, however, such robust-
ness does not stand when the number of queries is small, i.e., `2 distance metric
outperforms `1 and `1/MAD when the queries are limited. We conjecture that it
is due to the sparsity nature of `1-based metrics, where such sparse explanations
do not ﬁt the high-dimensional input space.
Imbalanced Dataset. This experiment was conducted to illustrate that perfor-
mance of attack methods based on counterfactual explanations is more stable on

118
5.6. Conclusion
Figure 5.10:
Experiment results
on imbalanced GMSC query set.
Figure 5.11:
Experiment results
on with/without shu✏ing for Du-
alCF.
the imbalanced dataset. In this experiment, we choose an imbalanced GMSC query
dataset where the “Good” applicants are 5 times more than “Bad” applicants. The
evaluation set is kept unchanged. This is the general case in some applications like
fraud detection, and healthcare. We report experimental results on the imbalanced
query set in Fig. 5.10. Compared with the results on balanced datasets in column
3 in Fig. 5.6, we can see the variance of attack methods based on the cloud model’s
prediction are larger. However, attack methods for counterfactual explanations are
more stable due to the balanced classes during the substitute model training.
Shu✏ing CFs and CCFs. We claim that CF and its CCF should be in the
same batch during training. Fig. 5.11 reports the experiment di↵erence between
with/without shu✏ing CFs and CCFs. Experiment results reveal that training
with pairs of CFs and CCFs in the same batch achieves better results. The shu✏e
operation will destroy the purpose of our method that we should train on pairs
of instances with similar distance to distance boundary of cloud model and with
di↵erent predictions.
5.6
Conclusion
Counterfactual explanation method searches the minimum perturbations for an in-
put to achieve a particular di↵erent prediction, which has broad applications. How-
ever, the boundary information leaked from counterfactual explanations is prone to
model extraction attacks. In this chapter, we propose a simple yet eﬃcient method

Chapter 5. DualCF
119
DualCF that mitigates the decision boundary shift issue in existing methods. Ex-
tensive experiments demonstrate that our method can achieve a high-ﬁdelity model
with much fewer queries. Our work raises awareness of the privacy and security
issues of counterfactual explanations, and further motivates the countermeasures
(e.g., monitoring malicious queries, adding superﬂuous features on model training
and counterfactual explanations for misleading adversaries, restricting the one-way
generation of counterfactual explanations.) to protect the cloud model. In the
future, we will work on more general attacks and defenses on multi-class classiﬁca-
tion models and regression models, and explore the inﬂuence of di↵erent practical
constraints on the security of counterfactual explanations.


Chapter 6
Conclusion and Future Works
In this chapter, the main contributions and ﬁndings of this thesis are summarized.
In addition, open challenges are introduced to steer and motivate future research.
6.1
Conclusions
In this thesis, we aim to produce more trustworthy, actionable, and safe counter-
factual explanations on heterogeneous datasets. In detail, we propose three works
to achieve the above goals: 1) inspect the black-box model at the group level via a
summarization technique, 2) address the cost deﬁnition over heterogeneous data,
and 3) study the security issues of counterfactual explanations.
Group summarization for model inspection.
Current research explains a
black-box model at either the instance level or the global level. Global-level expla-
nations return the same important features [90, 196, 197] or concepts [29, 198, 199]
that contribute to the ﬁnal prediction. However, such global-level explanations are
too coarse to ﬁt all input instances. Local-level explanations explain the prediction
of input by attributing the prediction to input features. Nevertheless, not all fea-
tures are equally important to the prediction, and reading local explanations of all
individuals can be time-consuming. In our research, we summarize local explana-
tions of all individuals by k groups, and each group is described by the distinct top
l important features. With our method, analysts only need to read k ⇥l compact
summary to understand the important features used by the model for each group.
121

122
6.1. Conclusions
In addition, we experimentally observe that our summarization method can apply
to other applications, e.g., ﬁnding groups’ interestingness from a user-item rating
matrix.
The skyline of counterfactual explanations. For a heterogeneous dataset,
the cost to measure the feature changes is hard to deﬁne because di↵erent features
are incomparable, e.g., “education” and “salary”. Users cannot simply add the
changes of di↵erent features to form the scalar cost function, which is a common
practice in existing methods.
On the other aspect, the cost of feature change
depends on the current feature value. For example, improving the exam score from
50 to 60 is much easier than that from 90 to 100. Our method treats the change
of each incomparable feature as an objective to minimize and returns the skyline
of counterfactual explanations, a.k.a. a set of counterfactual explanations where
each explanation in this set does not perform better than the others on all metrics.
With non-dominated explanations in the skyline, users can reﬁne and explore their
preferences interactively and incrementally.
Security of counterfactual explanations. Despite the fact that counterfactual
explanations do not expose the black-box model intentionally, the security and
privacy issues of counterfactual explanations are severe and have been largely ig-
nored [179, 188]. Researchers have not reached an agreement on the model leaky
issue of counterfactual explanations. These works [31, 57] state that counterfac-
tual explanations merely expose limited dependencies for an input, whereas the
internal algorithmic logic is still invisible. We admit the fact that such little infor-
mation is minor for model extraction attacks. However, gathering suﬃcient infor-
mation from queries can accelerate the extraction signiﬁcantly. Sokol et al. [177]
declare that counterfactual explanations disclose the sensitive information (close-
to-boundary points) of cloud models that facilitate the attack methods. Aivodji et
al. [6] experimentally prove that model extraction attacks can beneﬁt by consider-
ing counterfactual explanations as additional training instances. We observe that
this method [6] su↵ers from the decision boundary shift issue when it combines
far-distance queries and close-to-boundary explanations during substitute model
training. The phenomenon becomes severe when the query size is small. There-
fore, it requires more queries at a higher attack cost. Our proposed method views
the counterfactual explanations (CFs) as normal queries and obtains the counter-
factual explanations of counterfactual explanations (CCFs). As CFs and CCFs are

Chapter 6. Conclusions
123
all close-to-boundary points, the boundary shift issue can be mitigated and further
reduce the querying cost.
6.2
Future Works
Counterfactual explanations have been studied extensively from di↵erent perspec-
tives, including machine learning, cognitive science, human-computer interaction
(HCI), and psychology. Despite the progress made in this ﬁeld, there is still no
consensus on the conditions that counterfactual explanations should meet, and
how to evaluate them.
Researchers should rethink what should counterfactual
explanations be from di↵erent lenses. In addition to the fundamental challenge,
algorithmic eﬃciency and robustness also need to be investigated. Moving forward,
we will discuss some promising directions for future research.
Fair evaluation. Counterfactual explanations are a rapidly growing ﬁeld of re-
search, and evaluating di↵erent methods is a crucial and urgent aspect of this
research. The major concern is to design fair, standardized, and subjective evalu-
ation metrics.
As counterfactual explanation aims to explain the model and provide actionable
suggestions to users, the golden standard of comparison is the practical user study.
However, this study can be costly, less feasible, and less reproducible. In most re-
cent studies, researchers have developed mathematical evaluation metrics for spe-
ciﬁc properties, such as diversity, sparsity, cost, and plausibility. Meanwhile, it is
still unclear whether these metrics accurately reﬂect the desiderata that we want
to have. Several metrics used for evaluating counterfactual explanations are of-
ten contradictory and incomparable, such as cost and plausibility. A lower cost
indicates that the counterfactual explanation is close to the input instance, which
could lead to being out of the data distribution [200] and may not be plausible.
Therefore, there is a need to strike a balance between di↵erent metrics and con-
sider their trade-o↵s in order to design a comprehensive evaluation framework for
counterfactual explanations.
Standardization is also important because there are currently no publicly accepted
datasets, like ImageNet [201], for evaluating counterfactual explanations. Most

124
6.2. Future Works
existing methods train black-box models from scratch with di↵erent data prepro-
cessing. This makes it diﬃcult to compare di↵erent methods, and the e↵ectiveness
of evaluation is discounted. Therefore, e↵orts should be made to develop standard-
ized datasets and models to enable fair comparisons.
Additionally, subjectivity is another concern because counterfactual explanations
are intended to be understandable to individuals with di↵erent backgrounds and
knowledge levels. How to take the user’s understanding into account is also critical
rather than simply using sparsity to measure the simpleness.
Therefore, future research should explore methods for fair and comprehensive eval-
uation of counterfactual explanations.
Robustness. In counterfactual explanation research, however, in practice, pre-
trained models are frequently updated due to shifts in data distribution and al-
gorithm updates. This can render formal counterfactual explanations ine↵ective,
posing a signiﬁcant challenge to their validity. Recent works [200, 202, 203] attempt
to address this issue by generating counterfactual explanations with consistent pre-
dictions. Pawelczyk et al. [200] argue that adhering to a data manifold can improve
robustness, while Upadhyay et al. incorporate adversarial training to learn more
robust models. Black et al. [203] propose a Stable Neighbor Search (SNS) that ﬁnds
explanations with lower Lipschitz continuity and higher conﬁdence for consistency.
Even so, existing methods can return di↵erent counterfactual explanations for two
users with the same or similar feature values, which may cause concerns about
unfairness. For example, two ﬁnancially similar individuals are rejected when they
apply for a loan. Yet, CFEs for two people are quite di↵erent-one needs to update
the salary slightly while the other is required to get a higher education degree and
a better job. Therefore, returning consistent counterfactual explanations will be a
promising direction for future work.
We observe this problem may result from counterfactual multiplicity, i.e., the phe-
nomenon where there are multiple possible counterfactual explanations for a given
prediction or decision. In other words, there can be multiple ways to change the
input features or variables to obtain a desired outcome, a.k.a. Rashomon e↵ect
[204] of counterfactual explanations. This phenomenon is inherently caused by the
nonoptimality of the counterfactual explanation problem due to the non-convex
model and complex constraints. Counterfactual explanation methods could select

Chapter 6. Conclusions
125
di↵erent solutions at di↵erent trials. Therefore, a potential method is to incor-
porate domain knowledge to restrict and stabilize the search space, e.g., reference
intervals in healthcare, and prototypes of the target population.
Human computer interaction (HCI) and Counterfactual explanations.
Need to stress, the goals of counterfactual explanations are to facilitate users
to understand the model and ﬂip the current prediction.
From the user’s per-
spective, HCI and counterfactual explanations have great overlap, including user-
centered design, usability, and interactive visualization.
Therefore, counterfac-
tual explanations should incorporate some HCI techniques to help create visual
explanations, collect users’ feedback, and gather users’ customization and prefer-
ences [158, 205, 206]. For example, it should allow users to customize and express
their preferences/requirements, e.g., features to change, change scopes, and relative
weights, interactively with some HCI tools; some visualization techniques can help
users better understand the decision-making process and rationales of counterfac-
tual explanations [207]; if a current counterfactual explanation is not satisﬁable,
it should provide a way to help rectify or reﬁne it without much cost. With the
integration of HCI, current methods can produce more interpretable, acceptable
explanations for end users. However, designing an HCI interface should consider
the user’s background, knowledge, and abilities. Another challenge is the consid-
eration of social science, cognition science, and psychology when we combine HCI
and counterfactual explanations.
Handling missing values. Missing values occur when some features of an input
instance are unknown and unavailable, for example, users are reluctant to share
some sensitive information, and some features are damaged in data preprocessing.
Missing values have two obvious shortcomings. First, the performance of black-
box models might drop signiﬁcantly, leading to inaccurate predictions. Second, the
existing cost function cannot work on instances with missing values.
A straightforward approach is to impute missing values with other available infor-
mation, e.g., average values, most frequent values, and median values. However,
directly substituting missing values with other observed values may introduce un-
realistic instances. In a recent study, Nazabal et al. [208] propose HI-VAE that is
suitable to ﬁt incomplete heterogeneous data. Hi-VAE could be a promising way
to estimate probability density on incomplete data and generate realistic explana-
tions. Another approach could be to incorporate techniques in the recommender

126
6.2. Future Works
system [209], which help predict the missing values from personal interactions and
behaviors of similar users.
The problem of generating counterfactual explanations is a rapidly emerging and
novel research area that has garnered increasing interest. In addition to the chal-
lenges we have already discussed, there are other important issues that need to be
addressed, such as improving the eﬃciency of algorithms and ensuring the security
of development. All these challenges should be investigated and addressed before
real-life deployment.

List of Author’s Awards, Patents,
and Publications
Journal Articles
• Yongjie Wang, Ke Wang, Cheng Long, and Chunyan Miao. 2023. Summa-
rizing User-item Matrix By Group Utility Maximization. ACM Transactions
on Knowledge Discovery from Data, 17, 6, Article 86 (July 2023), 22 pages.
https://doi.org/10.1145/3578586
Conference Proceedings
• Yongjie Wang, Hangwei Qian, Yongjie Liu, Wei Guo, Chunyan Miao. Flexi-
ble and Robust Counterfactual Explanations with Minimal Satisﬁable Pertur-
bations. 32nd ACM International Conference on Information and Knowledge
Management (CIKM’ 2023, accepted)
• Yongjie Wang, Hangwei Qian, and Chunyan Miao. 2022. DualCF: Eﬃ-
cient Model Extraction Attack from Counterfactual Explanations. In 2022
ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22).
Association for Computing Machinery, New York, NY, USA, 1318–1329.
https://doi.org/10.1145/3531146.3533188.
• Yongjie Wang, Qinxu Ding, Ke Wang, Yue Liu, Xingyu Wu, Jinglong
Wang, Yong Liu, and Chunyan Miao. 2021. The Skyline of Counterfactual
Explanations for Machine Learning Decision Models. In Proceedings of the
30th ACM International Conference on Information & Knowledge Manage-
ment (CIKM ’21). Association for Computing Machinery, New York, NY,
USA, 2030–2039. https://doi.org/10.1145/3459637.3482397.
127

128
List of Author’s Awards, Patents, and Publications
• Yongjie Wang, Ke Wang, Cheng Long and Chunyan Miao, ”Summariz-
ing User-Item Matrix By Group Utility Maximization,” 2021 IEEE Inter-
national Conference on Data Mining (ICDM), 2021, pp.
1409-1414, doi:
10.1109/ICDM51629.2021.00180.

Bibliography
[1] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. xvii,
2, 22
[2] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolu-
tional networks. In European conference on computer vision, pages 818–833.
Springer, 2014. xvii, 21, 22
[3] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Tor-
ralba. Learning deep features for discriminative localization. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages
2921–2929, 2016. xvii, 22, 23
[4] Haojun Zhu. Predicting earning potential using the adult dataset, December
2016. URL https://rpubs.com/H_Zhu/235617. xviii, 86, 92
[5] Florian Tram`er, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ris-
tenpart.
Stealing machine learning models via prediction apis.
In 25th
{USENIX} Security Symposium ({USENIX} Security 16), pages 601–618,
2016. xviii, 38, 96, 99, 102, 108, 111
[6] Ulrich A¨ıvodji, Alexandre Bolot, and S´ebastien Gambs. Model extraction
from counterfactual explanations. arXiv preprint arXiv:2009.01884, 2020.
xviii, xix, 14, 96, 98, 99, 103, 104, 105, 108, 111, 116, 122
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual
learning for image recognition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016. 1, 96
[8] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Fig-
urnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
ˇZ´ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction
with alphafold. Nature, 596(7873):583–589, 2021. 1, 96
[9] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang.
Recommender system application developments: a survey. Decision Support
Systems, 74:12–32, 2015. 1, 96
129

130
BIBLIOGRAPHY
[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you
need. In Advances in neural information processing systems, pages 5998–
6008, 2017. 1, 96
[11] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
Bolton, et al. Mastering the game of go without human knowledge. nature,
550(7676):354–359, 2017. 1
[12] Alex Krizhevsky, Ilya Sutskever, and Geo↵rey E Hinton.
Imagenet clas-
siﬁcation with deep convolutional neural networks. In Advances in neural
information processing systems, pages 1097–1105, 2012. 2
[13] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Ka-
plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, et al. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165, 2020. 2
[14] Xu Guo and Han Yu. On the domain adaptation and generalization of pre-
trained language models: A survey. arXiv preprint arXiv:2211.03154, 2022.
2
[15] Protection Regulation. Regulation (eu) 2016/679 of the european parliament
and of the council. REGULATION (EU), 679:2016, 2016. 2, 12
[16] David Gunning. Explainable artiﬁcial intelligence (xai). Defense Advanced
Research Projects Agency (DARPA), nd Web, 2, 2017. 3
[17] Ron
Schmelzer.
Understanding
explainable
ai,
july
2019.
URL
https://www.forbes.com/sites/cognitiveworld/2019/07/23/
understanding-explainable-ai. 3
[18] Google.
Ai explainability whitepaper, 2020.
URL https://storage.
googleapis.com/cloud-ai-whitepapers/AI%20Explainability%
20Whitepaper.pdf. 3
[19] Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael
Hind, Samuel C. Ho↵man, Stephanie Houde, Q. Vera Liao, Ronny Luss,
Aleksandra Mojsilovi´c, Sami Mourad, Pablo Pedemonte, Ramya Raghaven-
dra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder
Singh, Kush R. Varshney, Dennis Wei, and Yunfeng Zhang. One explanation
does not ﬁt all: A toolkit and taxonomy of ai explainability techniques, sept
2019. URL https://arxiv.org/abs/1909.03012. 3
[20] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust
you?”. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM, aug 2016. doi: 10.1145/
2939672.2939778. 3, 7, 8, 25, 27, 74

BIBLIOGRAPHY
131
[21] Scott Lundberg and Su-In Lee. A uniﬁed approach to interpreting model
predictions. arXiv preprint arXiv:1705.07874, 2017. 3, 7, 25
[22] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca
Giannotti, and Dino Pedreschi. A survey of methods for explaining black
box models. ACM computing surveys (CSUR), 51(5):1–42, 2018. 6, 74
[23] J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81–106,
1986. 6, 35
[24] Robert C Holte. Very simple classiﬁcation rules perform well on most com-
monly used datasets. Machine learning, 11(1):63–90, 1993. 6
[25] Dennis Wei, Sanjeeb Dash, Tian Gao, and Oktay Gunluk. Generalized linear
rule models. In International Conference on Machine Learning, pages 6687–
6696. PMLR, 2019. 6
[26] Jerome H Friedman. Greedy function approximation: a gradient boosting
machine. Annals of statistics, pages 1189–1232, 2001. 7
[27] Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. Peeking
inside the black box: Visualizing statistical learning with plots of individual
conditional expectation. Journal of Computational and Graphical Statistics,
24(1):44–65, 2015. 7
[28] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler,
Fernanda Viegas, et al. Interpretability beyond feature attribution: Quan-
titative testing with concept activation vectors (tcav). In International con-
ference on machine learning, pages 2668–2677. PMLR, 2018. 7
[29] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pﬁster, and
Pradeep Ravikumar. On completeness-aware concept-based explanations in
deep neural networks. Advances in Neural Information Processing Systems,
33:20554–20565, 2020. 7, 121
[30] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution
for deep networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 3319–3328. JMLR. org, 2017. 7, 8, 23,
46, 66, 74
[31] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual expla-
nations without opening the black box: Automated decisions and the gdpr.
Harv. JL & Tech., 31:841, 2017. 7, 8, 9, 11, 12, 14, 28, 30, 33, 34, 35, 74, 75,
77, 78, 89, 97, 98, 101, 102, 111, 122
[32] Sebastian Bach, Alexander Binder, Gr´egoire Montavon, Frederick Klauschen,
Klaus-Robert M¨uller, and Wojciech Samek. On pixel-wise explanations for
non-linear classiﬁer decisions by layer-wise relevance propagation. PloS one,
10(7), 2015. 8, 24, 27, 74

132
BIBLIOGRAPHY
[33] Tim Miller. Explanation in artiﬁcial intelligence: Insights from the social
sciences. Artiﬁcial intelligence, 267:1–38, 2019. 8
[34] Chris Russell. Eﬃcient search for diverse coherent explanations. In Proceed-
ings of the Conference on Fairness, Accountability, and Transparency, pages
20–28, 2019. 9, 11, 28, 33, 34, 74, 78, 86, 101
[35] Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear
classiﬁcation. In Proceedings of the Conference on Fairness, Accountability,
and Transparency, pages 10–19, 2019. 9, 11, 27, 29, 32, 34, 36, 74, 88, 98,
101
[36] Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-
agnostic counterfactual explanations for tabular data. In Proceedings of The
Web Conference 2020, pages 3126–3132, 2020. 9, 11, 29, 33, 36, 74, 77, 86,
87, 88, 90
[37] Edmon Begoli, Tanmoy Bhattacharya, and Dimitri Kusnezov.
The need
for uncertainty quantiﬁcation in machine-assisted medical decision making.
Nature Machine Intelligence, 1(1):20–23, 2019. 10
[38] Thomas Grote and Philipp Berens. On the ethics of algorithmic decision-
making in healthcare. Journal of medical ethics, 46(3):205–211, 2020. 10
[39] Julia Angwin, Je↵Larson, Surya Mattu, and Lauren Kirchner. Machine bias.
In Ethics of Data and Analytics, pages 254–264. Auerbach Publications, 2016.
11
[40] Wenjie Wang, Fuli Feng, Xiangnan He, Hanwang Zhang, and Tat-Seng Chua.
Clicks can be cheating: Counterfactual recommendation for mitigating click-
bait issue.
In Proceedings of the 44th International ACM SIGIR Confer-
ence on Research and Development in Information Retrieval, SIGIR ’21,
page 1288–1297, New York, NY, USA, 2021. Association for Computing
Machinery.
ISBN 9781450380379.
doi: 10.1145/3404835.3462962.
URL
https://doi.org/10.1145/3404835.3462962. 11
[41] Michael V Mannino and Murlidhar V Koushik. The cost-minimizing inverse
classiﬁcation problem: a genetic algorithm approach. Decision Support Sys-
tems, 29(3):283–300, 2000. 11, 26, 28, 36
[42] Charu C Aggarwal, Chen Chen, and Jiawei Han. The inverse classiﬁcation
problem. Journal of Computer Science and Technology, 25(3):458–468, 2010.
26
[43] Parag C Pendharkar. A potential use of data envelopment analysis for the
inverse classiﬁcation problem. Omega, 30(3):243–248, 2002. 11, 26
[44] Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting,
Karthikeyan Shanmugam, and Payel Das. Explanations based on the missing:
Towards contrastive explanations with pertinent negatives. In Advances in

BIBLIOGRAPHY
133
Neural Information Processing Systems, pages 592–603, 2018. 11, 26, 28, 31,
32, 33, 34, 36, 74, 98, 101
[45] Amit Dhurandhar, Tejaswini Pedapati, Avinash Balakrishnan, Pin-Yu Chen,
Karthikeyan Shanmugam, and Ruchir Puri. Model agnostic contrastive ex-
planations for structured data. arXiv preprint arXiv:1906.00117, 2019. 11,
26, 36
[46] Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and
Joydeep Ghosh. Towards realistic individual recourse and actionable explana-
tions in black-box decision making systems. arXiv preprint arXiv:1907.09615,
2019. 11, 32, 34, 74, 101
[47] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining ma-
chine learning classiﬁers through diverse counterfactual explanations.
In
Proceedings of the 2020 Conference on Fairness, Accountability, and Trans-
parency, pages 607–617, 2020. 11, 31, 32, 33, 35, 74, 78, 80, 86, 87, 90, 98,
101
[48] Arnaud Van Looveren and Janis Klaise. Interpretable counterfactual expla-
nations guided by prototypes. arXiv preprint arXiv:1907.02584, 2019. 11,
28, 32, 36, 74
[49] Amir-Hossein Karimi, Julius von K¨ugelgen, Bernhard Sch¨olkopf, and Isabel
Valera. Algorithmic recourse under imperfect causal knowledge: a proba-
bilistic approach. Advances in Neural Information Processing Systems, 33,
2020. 11, 34, 74, 98, 101
[50] Amir-Hossein Karimi, Bernhard Sch¨olkopf, and Isabel Valera. Algorithmic
recourse: from counterfactual explanations to interventions. arXiv preprint
arXiv:2002.06278, 2020. 74
[51] Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving causal con-
straints in counterfactual explanations for machine learning classiﬁers. arXiv
preprint arXiv:1912.03277, 2019. 11, 27, 30, 34, 35
[52] Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard,
and Marcin Detyniecki.
Inverse classiﬁcation for comparison-based inter-
pretability in machine learning. arXiv preprint arXiv:1712.08443, 2017. 11,
26, 30, 32, 36, 74, 90, 91, 101
[53] Gabriele Tolomei, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. In-
terpretable predictions of tree-based ensembles via actionable feature tweak-
ing. In Proceedings of the 23rd ACM SIGKDD international conference on
knowledge discovery and data mining, pages 465–474, 2017. 11, 36, 102
[54] Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and
Peter Flach. Face: feasible and actionable counterfactual explanations. In
Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages
344–350, 2020. 11, 33, 34, 36, 102

134
BIBLIOGRAPHY
[55] Jakob Schoe↵er, Niklas Kuehl, and Yvette Machowski. “there is not enough
information”:
On the e↵ects of explanations on perceptions of informa-
tional fairness and trustworthiness in automated decision-making. In 2022
ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22,
page 1616–1628, New York, NY, USA, 2022. Association for Computing
Machinery.
ISBN 9781450393522.
doi: 10.1145/3531146.3533218.
URL
https://doi.org/10.1145/3531146.3533218. 12
[56] Sahil Verma, John Dickerson, and Keegan Hines. Counterfactual explana-
tions for machine learning: A review. arXiv preprint arXiv:2010.10596, 2020.
14, 27, 34, 103
[57] Masoud Hashemi and Ali Fathi. Permuteattack: Counterfactual explanation
of machine learning credit scorecards. arXiv preprint arXiv:2008.10138, 2020.
14, 98, 102, 122
[58] Yongjie Wang, Ke Wang, Cheng Long, and Chunyan Miao. Summarizing
user-item matrix by group utility maximization. In 2021 IEEE International
Conference on Data Mining (ICDM), pages 1409–1414, 2021. doi: 10.1109/
ICDM51629.2021.00180. 16
[59] Yongjie Wang, Ke Wang, Cheng Long, and Chunyan Miao. Summarizing
user-item matrix by group utility maximization. ACM Trans. Knowl. Discov.
Data, 17(6), mar 2023. ISSN 1556-4681. doi: 10.1145/3578586. URL https:
//doi.org/10.1145/3578586. 16
[60] Yongjie Wang, Qinxu Ding, Ke Wang, Yue Liu, Xingyu Wu, Jinglong Wang,
Yong Liu, and Chunyan Miao. The skyline of counterfactual explanations
for machine learning decision models. In Proceedings of the 30th ACM In-
ternational Conference on Information & Knowledge Management, CIKM
’21, page 2030–2039, New York, NY, USA, 2021. Association for Comput-
ing Machinery. ISBN 9781450384469. doi: 10.1145/3459637.3482397. URL
https://doi.org/10.1145/3459637.3482397. 16
[61] Yongjie Wang, Hangwei Qian, and Chunyan Miao.
Dualcf:
Eﬃcient
model extraction attack from counterfactual explanations. In Proceedings
of the 2022 ACM Conference on Fairness, Accountability, and Transparency,
FAccT ’22, page 1318–1329, New York, NY, USA, 2022. Association for Com-
puting Machinery.
ISBN 9781450393522.
doi: 10.1145/3531146.3533188.
URL https://doi.org/10.1145/3531146.3533188. 17
[62] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes
by meaningful perturbation. In Proceedings of the IEEE international con-
ference on computer vision, pages 3429–3437, 2017. 21
[63] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin
Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint
arXiv:1412.6806, 2014. 22

BIBLIOGRAPHY
135
[64] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna
Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations
from deep networks via gradient-based localization. In Proceedings of the
IEEE international conference on computer vision, pages 618–626, 2017. 23
[65] Pascal Sturmfels, Scott Lundberg, and Su-In Lee. Visualizing the impact
of feature attribution baselines. Distill, 2020. doi: 10.23915/distill.00022.
https://distill.pub/2020/attribution-baselines. 23, 24
[66] Shawn Xu, Subashini Venugopalan, and Mukund Sundararajan. Attribution
in scale and space. arXiv preprint arXiv:2004.03383, 2020. 23
[67] Pieter-Jan Kindermans, Kristof Sch¨utt, Klaus-Robert M¨uller, and Sven
D¨ahne.
Investigating the inﬂuence of noise and distractors on the inter-
pretation of neural networks. arXiv preprint arXiv:1611.07270, 2016. 24
[68] Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kun-
daje. Not just a black box: Learning important features through propagating
activation di↵erences. arXiv preprint arXiv:1605.01713, 2016. 24
[69] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning im-
portant features through propagating activation di↵erences. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pages
3145–3153. JMLR. org, 2017. 24
[70] Gr´egoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech
Samek, and Klaus-Robert M¨uller. Explaining nonlinear classiﬁcation deci-
sions with deep taylor decomposition. Pattern recognition, 65:211–222, 2017.
24
[71] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi,
Franco Turini, and Fosca Giannotti. Local rule-based explanations of black
box decision systems. arXiv preprint arXiv:1805.10820, 2018. 25
[72] Ivan Tomek.
Two modiﬁcations of cnn.
IEEE Transactions on Systems,
Man, and Cybernetics, SMC-6(11):769–772, 1976. doi: 10.1109/TSMC.1976.
4309452. 26, 27
[73] Michael T Lash, Qihang Lin, Nick Street, Jennifer G Robinson, and Je↵rey
Ohlmann. Generalized inverse classiﬁcation. In Proceedings of the 2017 SIAM
International Conference on Data Mining, pages 162–170. SIAM, 2017. 26
[74] Belur V Dasarathy. Nearest neighbor (nn) norms: Nn pattern classiﬁcation
techniques. IEEE Computer Society Tutorial, 1991. 27
[75] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 27,
102

136
BIBLIOGRAPHY
[76] Timo Freiesleben.
Counterfactual explanations & adversarial examples–
common grounds, essential di↵erences, and potential transfers. arXiv preprint
arXiv:2009.05487, 2020. 27
[77] Khaled Saleh Al-Sultan.
Nearest point problems: Theory and algorithms.
University of Michigan, 1990. 28
[78] Amir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-
agnostic counterfactual explanations for consequential decisions. In Inter-
national Conference on Artiﬁcial Intelligence and Statistics, pages 895–905.
PMLR, 2020. 28, 36, 74
[79] Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, and Hiroki Arimura.
Dace: Distribution-aware counterfactual explanation by mixed-integer linear
optimization. In Proceedings of the Twenty-Ninth International Joint Con-
ference on Artiﬁcial Intelligence, IJCAI-20, Christian Bessiere (Ed.). In-
ternational Joint Conferences on Artiﬁcial Intelligence Organization, pages
2855–2862, 2020. 29, 33, 36, 74
[80] Ana Lucic, Harrie Oosterhuis, Hinda Haned, and Maarten de Rijke. Focus:
Flexible optimizable counterfactual explanations for tree ensembles. In Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 36, pages
5313–5322, 2022. 29
[81] Brent Mittelstadt, Chris Russell, and Sandra Wachter. Explaining explana-
tions in ai. In Proceedings of the Conference on Fairness, Accountability, and
Transparency, FAT* ’19, page 279–288, New York, NY, USA, 2019. Associa-
tion for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.
3287574. URL https://doi.org/10.1145/3287560.3287574. 30
[82] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly,
Yunhan Jia, Joydeep Ghosh, Ruchir Puri, Jos´e MF Moura, and Peter Eck-
ersley. Explainable machine learning in deployment. In Proceedings of the
2020 conference on fairness, accountability, and transparency, pages 648–657,
2020. 31
[83] Shusen Liu, Bhavya Kailkhura, Donald Loveland, and Yong Han. Generative
counterfactual introspection for explainable deep learning.
In 2019 IEEE
Global Conference on Signal and Information Processing (GlobalSIP), pages
1–5. IEEE, 2019.
[84] Fan Yang, Sahan Suresh Alva, Jiahao Chen, and Xia Hu. Model-based coun-
terfactual synthesizer for interpretation.
In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1964–
1974, 2021. 31
[85] Susanne Dandl, Christoph Molnar, Martin Binder, and Bernd Bischl. Multi-
objective counterfactual explanations.
arXiv preprint arXiv:2004.11165,
2020. 33, 36

BIBLIOGRAPHY
137
[86] Mark T Keane and Barry Smyth. Good counterfactuals and where to ﬁnd
them: A case-based technique for generating counterfactuals for explainable
ai (xai). In International Conference on Case-Based Reasoning, pages 163–
178. Springer, 2020. 33
[87] Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, and Marcin De-
tyniecki.
Issues with post-hoc counterfactual explanations: a discussion.
arXiv preprint arXiv:1906.04774, 2019.
[88] Eoin Delaney, Derek Greene, and Mark T Keane. Instance-based counterfac-
tual explanations for time series classiﬁcation. In International Conference
on Case-Based Reasoning, pages 32–47. Springer, 2021. 33
[89] George Tolkachev, Stephen Mell, Stephan Zdancewic, and Osbert Bastani.
Counterfactual explanations for natural language interfaces. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 113–118, Dublin, Ireland, May 2022. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.14.
URL https://aclanthology.org/2022.acl-short.14. 34
[90] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001. 35, 121
[91] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system.
In Proceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining, pages 785–794, 2016. 35
[92] Michael Downs, Jonathan L Chu, Yaniv Yacoby, Finale Doshi-Velez, and
Weiwei Pan. Cruds: Counterfactual recourse using disentangled subspaces.
35
[93] Ronny Luss, Pin Yu Chen, Amit Dhurandhar, Prasanna Sattigeri, Yunfeng
Zhang, Karthikeyan Shanmugam, and Chun Chen Tu. Leveraging Latent
Features for Local Explanations. Proceedings of the ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining, pages 1139–
1149, 2021. ISSN 2331-8422. doi: 10.1145/3447548.3467265. 36
[94] Shubham Sharma, Jette Henderson, and Joydeep Ghosh. CERTIFAI: Coun-
terfactual explanations for robustness, transparency, interpretability, and
fairness of artiﬁcial intelligence models, 2019. 36
[95] Axel Parmentier and Thibaut Vidal. Optimal Counterfactual Explanations in
Tree Ensembles. In ICML, 2021. URL http://arxiv.org/abs/2106.06631.
36
[96] Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike, Kento Ue-
mura, and Hiroki Arimura. Ordered counterfactual explanation by mixed-
integer linear optimization. arXiv preprint arXiv:2012.11782, 2020.

138
BIBLIOGRAPHY
[97] Miguel ´A Carreira-Perpi˜n´an and Suryabhan Singh Hada.
Counterfactual
explanations for oblique decision trees: Exact, eﬃcient algorithms. In Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages
6903–6911, 2021. 36
[98] Guy Blanc, Caleb Koch, Jane Lange, and Li-Yang Tan. A query-optimal al-
gorithm for ﬁnding counterfactuals. In International Conference on Machine
Learning, pages 2075–2090. PMLR, 2022. 36
[99] Stratis Tsirtsis and Manuel Gomez Rodriguez. Decisions, counterfactual ex-
planations and strategic behavior. Advances in Neural Information Process-
ing Systems, 33:16749–16760, 2020. 36
[100] Shubham Rathi. Generating counterfactual and contrastive explanations us-
ing shap. arXiv preprint arXiv:1906.09293, 2019. 36
[101] Kalpesh Krishna, Gaurav Singh Tomar, Ankur P Parikh, Nicolas Papernot,
and Mohit Iyyer. Thieves on sesame street! model extraction of bert-based
apis. In International Conference on Learning Representations, 2019. 37, 102
[102] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knocko↵nets: Steal-
ing functionality of black-box models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages 4954–4963, 2019.
102
[103] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and
Nicolas Papernot. High accuracy and high ﬁdelity extraction of neural net-
works. In 29th {USENIX} Security Symposium ({USENIX} Security 20),
pages 1345–1362, 2020. 102
[104] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: pro-
tecting against dnn model stealing attacks. In 2019 IEEE European Sympo-
sium on Security and Privacy (EuroS&P), pages 512–527. IEEE, 2019. 37,
38, 40, 102
[105] Xueluan Gong, Yanjiao Chen, Wenbin Yang, Guanghao Mei, and Qian Wang.
Inversenet: Augmenting model extraction attacks with training data inver-
sion.
In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International
Joint Conference on Artiﬁcial Intelligence, IJCAI-21, pages 2439–2447. In-
ternational Joint Conferences on Artiﬁcial Intelligence Organization, 8 2021.
doi:
10.24963/ijcai.2021/336.
URL https://doi.org/10.24963/ijcai.
2021/336. Main Track. 37, 102, 111
[106] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia conference on computer
and communications security, pages 506–519, 2017. 37, 38, 96, 102, 116

BIBLIOGRAPHY
139
[107] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model inversion
attacks that exploit conﬁdence information and basic countermeasures. In
Proceedings of the 22nd ACM SIGSAC conference on computer and commu-
nications security, pages 1322–1333, 2015. 37, 96
[108] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Mem-
bership inference attacks against machine learning models. In 2017 IEEE
Symposium on Security and Privacy (SP), pages 3–18. IEEE, 2017. 37, 96
[109] Huadi Zheng, Qingqing Ye, Haibo Hu, Chengfang Fang, and Jie Shi. Bdpl:
A boundary di↵erentially private layer against machine learning model ex-
traction attacks. In European Symposium on Research in Computer Security,
pages 66–83. Springer, 2019. 37, 39
[110] Erwin Quiring, Daniel Arp, and Konrad Rieck. Forgotten siblings: Unifying
attacks on machine learning and digital watermarking. In 2018 IEEE Euro-
pean Symposium on Security and Privacy (EuroS&P), pages 488–502. IEEE,
2018. 37, 40
[111] Vasisht Duddu, Debasis Samanta, D Vijay Rao, and Valentina E Balas.
Stealing neural networks via timing side channels.
arXiv preprint
arXiv:1812.11720, 2018. 38
[112] Binghui Wang and Neil Zhenqiang Gong. Stealing hyperparameters in ma-
chine learning. In 2018 IEEE symposium on security and privacy (SP), pages
36–52. IEEE, 2018. 38
[113] Honggang Yu, Kaichen Yang, Teng Zhang, Yun-Yun Tsai, Tsung-Yi Ho,
and Yier Jin. Cloudleak: Large-scale deep learning models stealing through
adversarial examples. In NDSS, 2020. 38, 102
[114] Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish Shevade,
and Vinod Ganapathy. Activethief: Model extraction using active learning
and unannotated public data. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 34, pages 865–872, 2020. 38, 102, 111
[115] Taesung Lee, Benjamin Edwards, Ian Molloy, and Dong Su.
Defending
against neural network model stealing attacks using deceptive perturbations.
In 2019 IEEE Security and Privacy Workshops (SPW), pages 43–49, 2019.
doi: 10.1109/SPW.2019.00020. 39
[116] Manish Kesarwani, Bhaskar Mukhoty, Vijay Arya, and Sameep Mehta.
Model extraction warning in mlaas paradigm. In Proceedings of the 34th An-
nual Computer Security Applications Conference, ACSAC ’18, page 371–380,
New York, NY, USA, 2018. Association for Computing Machinery. ISBN
9781450365697. doi: 10.1145/3274694.3274740. URL https://doi.org/10.
1145/3274694.3274740. 40
[117] Samuel Sanford Shapiro and Martin B Wilk. An analysis of variance test for
normality (complete samples). Biometrika, 52(3/4):591–611, 1965. 40

140
BIBLIOGRAPHY
[118] ZR Hesabi, Zahir Tari, A Goscinski, Adil Fahad, Ibrahim Khalil, and Carlos
Queiroz. Data summarization techniques for big data—a survey. In Handbook
on Data Centers, pages 1109–1152. Springer, 2015. 44
[119] Mohiuddin Ahmed. Data summarization: a survey. Knowledge and Infor-
mation Systems, 58(2):249–273, 2019. 44
[120] Ihab F Ilyas, George Beskales, and Mohamed A Soliman. A survey of top-k
query processing techniques in relational database systems. ACM Computing
Surveys (CSUR), 40(4):1–58, 2008. 44
[121] Dan Pelleg and Andrew W. Moore. Accelerating exact k-means algorithms
with geometric reasoning. In KDD ’99, 1999. 44
[122] William G Ruesink. Introduction to sampling theory. In Sampling methods
in soybean entomology, pages 61–78. Springer, 1980. 44
[123] Graham Cormode and Donatella Firmani.
A unifying framework for l0-
sampling algorithms. Distrib. Parallel Databases, 32(3):315–335, sep 2014.
ISSN 0926-8782. doi: 10.1007/s10619-013-7131-9. URL https://doi.org/
10.1007/s10619-013-7131-9. 44
[124] Sikder Tahsin Al-Amin and Carlos Ordonez. Eﬃcient machine learning on
data science languages with parallel data summarization. Data & Knowledge
Engineering, 136:101930, 2021. 44
[125] Christian Borgelt. Frequent item set mining. Wiley interdisciplinary reviews:
data mining and knowledge discovery, 2(6):437–456, 2012. 44
[126] Carlos Ordonez, Naveen Mohanam, and Carlos Garcia-Alvarado. Pca for
large data sets with parallel data summarization. Distrib. Parallel Databases,
32(3):377–403, sep 2014. ISSN 0926-8782. doi: 10.1007/s10619-013-7134-6.
URL https://doi.org/10.1007/s10619-013-7134-6. 44
[127] Eric J Stollnitz, Tony D DeRose, Anthony D DeRose, and David H Salesin.
Wavelets for computer graphics: theory and applications. Morgan Kaufmann,
1996. 44
[128] Francois Belletti, Karthik Lakshmanan, Walid Krichene, Yi-Fan Chen, and
John Anderson. Scalable realistic recommendation datasets through fractal
expansions. arXiv preprint arXiv:1901.08910, 2019. 44, 65, 71
[129] Sara C Madeira and Arlindo L Oliveira. Biclustering algorithms for biological
data analysis: a survey. IEEE/ACM transactions on computational biology
and bioinformatics, 1(1):24–45, 2004. 44, 47
[130] Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. Introduction to data
mining. Pearson Education India, 2016. 47

BIBLIOGRAPHY
141
[131] Bidyut Kr. Patra and Sukumar Nandi.
E↵ective data summarization for
hierarchical clustering in large datasets. Knowl. Inf. Syst., 42(1):1–20, jan
2015. ISSN 0219-1377. doi: 10.1007/s10115-013-0709-8. URL https://doi.
org/10.1007/s10115-013-0709-8.
[132] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful
seeding. Technical report, Stanford, 2006. 47
[133] Lance Parsons, Ehtesham Haque, and Huan Liu. Subspace clustering for
high dimensional data: a review. Acm Sigkdd Explorations Newsletter, 6(1):
90–105, 2004. 47
[134] Church GM Cheng Y. Biclustering of expression data. In Proc Int Conf
Intell Syst Mol Biol., 2000. 47
[135] Inderjit S Dhillon. Co-clustering documents and words using bipartite spec-
tral graph partitioning. In Proceedings of the seventh ACM SIGKDD inter-
national conference on Knowledge discovery and data mining, pages 269–274,
2001. 47, 48, 64
[136] S. C. Madeira and A. L. Oliveira. Biclustering algorithms for biological data
analysis: a survey. IEEE/ACM Transactions on Computational Biology and
Bioinformatics, 1(1):24–45, 2004. 47
[137] Kelvin Sim, Vivekanand Gopalkrishnan, Arthur Zimek, and Gao Cong. A
survey on enhanced subspace clustering. Data mining and knowledge discov-
ery, 26(2):332–397, 2013. 47
[138] Franciso Herrera, Crist´obal Jos´e Carmona, Pedro Gonz´alez, and Mar´ıa Jos´e
Del Jesus. An overview on subgroup discovery: foundations and applications.
Knowledge and information systems, 29(3):495–525, 2011. 47
[139] Stefan Wrobel. An algorithm for multi-relational discovery of subgroups. In
European Symposium on Principles of Data Mining and Knowledge Discov-
ery, pages 78–87. Springer, 1997. 47
[140] Dennis Leman, Ad Feelders, and Arno Knobbe. Exceptional model mining.
In Joint European conference on machine learning and knowledge discovery
in databases, pages 1–16. Springer, 2008. 48
[141] Cl´audio Rebelo de S´a, Wouter Duivesteijn, Carlos Soares, and Arno Knobbe.
Exceptional preferences mining. In International Conference on Discovery
Science, pages 3–18. Springer, 2016. 48
[142] Vincent Branders, Pierre Schaus, and Pierre Dupont.
Combinatorial op-
timization algorithms to mine a sub-matrix of maximal sum. In Interna-
tional Workshop on New Frontiers in Mining Complex Patterns, pages 65–79.
Springer, 2017. 48

142
BIBLIOGRAPHY
[143] Branders, Vincent and Schaus, Pierre and Dupont, Pierre. Identifying gene-
speciﬁc subgroups: an alternative to biclustering. BMC Bioinformatics, 20
(1):1–13, 2019. 48, 64
[144] Vincent Branders, Guillaume Derval, Pierre Schaus, and Pierre Dupont. Min-
ing a maximum weighted set of disjoint submatrices. In International Con-
ference on Discovery Science, pages 18–28. Springer, 2019. 48
[145] Cl´audio Rebelo de S´a, Wouter Duivesteijn, Carlos Soares, and Arno J.
Knobbe. Exceptional preferences mining. In DS, 2016. 48
[146] Marco De Gemmis, Leo Iaquinta, Pasquale Lops, Cataldo Musto, Fedelu-
cio Narducci, and Giovanni Semeraro. Preference learning in recommender
systems. Preference Learning, 41:41–55, 2009. 48
[147] Chong-Wah Ngo and Feng Wang. Video summarization., 2009. 48
[148] Han Xu, Eric Martin, and Ashesh Mahidadia.
Extractive summarisation
based on keyword proﬁle and language model. In Proceedings of the 2015
conference of the North American chapter of the association for computa-
tional linguistics: Human language technologies, pages 123–132, 2015. 48
[149] Jacob Bien and Robert Tibshirani.
Prototype selection for interpretable
classiﬁcation. The Annals of Applied Statistics, 5(4):2403–2424, 2011. 48
[150] Reuven Cohen and Liran Katzir. The generalized maximum coverage prob-
lem. Information Processing Letters, 108(1):15–22, 2008. 52
[151] George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis
of approximations for maximizing submodular set functions—i. Mathematical
programming, 14(1):265–294, 1978. 56
[152] Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne
VanBriesen, and Natalie Glance. Cost-e↵ective outbreak detection in net-
works. In Proceedings of the 13th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 420–429, 2007. 57
[153] Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan
Vondr´ak, and Andreas Krause. Lazier than lazy greedy. In Twenty-Ninth
AAAI Conference on Artiﬁcial Intelligence, 2015. 57, 58
[154] James Bennett, Stan Lanning, et al. The netﬂix prize. In Proceedings of
KDD cup and workshop, volume 2007, page 35. Citeseer, 2007. 64
[155] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History
and context. Acm transactions on interactive intelligent systems (tiis), 5(4):
1–19, 2015. 65, 71
[156] Amir-Hossein Karimi, Gilles Barthe, Bernhard Sch¨olkopf, and Isabel Valera.
A survey of algorithmic recourse: deﬁnitions, formulations, solutions, and
prospects. arXiv preprint arXiv:2010.04050, 2020. 74, 75

BIBLIOGRAPHY
143
[157] Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard,
and Marcin Detyniecki. The dangers of post-hoc interpretability: Unjustiﬁed
counterfactual explanations. arXiv preprint arXiv:1907.09294, 2019. 74
[158] Furui Cheng, Yao Ming, and Huamin Qu.
Dece: Decision explorer with
counterfactual explanations for machine learning models. IEEE Transactions
on Visualization and Computer Graphics, 2020. 74, 125
[159] R Timothy Marler and Jasbir S Arora. Survey of multi-objective optimization
methods for engineering. Structural and multidisciplinary optimization, 26
(6):369–395, 2004. 76, 78
[160] Michael TM Emmerich and Andr´e H Deutz. A tutorial on multiobjective
optimization: fundamentals and evolutionary methods. Natural computing,
17(3):585–609, 2018. 76, 78
[161] Stephan Borzsony, Donald Kossmann, and Konrad Stocker.
The skyline
operator. In Proceedings 17th international conference on data engineering,
pages 421–430. IEEE, 2001. 76, 83, 85
[162] Xingyu Chen, Chunyu Wang, Xuguang Lan, Nanning Zheng, and Wenjun
Zeng. Neighborhood geometric structure-preserving variational autoencoder
for smooth and bounded data sources. IEEE Transactions on Neural Net-
works and Learning Systems, 2021. 77
[163] Tuukka Ruotsalo, Giulio Jacucci, Petri Myllym¨aki, and Samuel Kaski. In-
teractive intent modeling: Information discovery beyond search. Communi-
cations of the ACM, 58(1):86–92, 2014. 78, 81
[164] Gary Marchionini. Exploratory search: from ﬁnding to understanding. Com-
munications of the ACM, 49(4):41–46, 2006. 81
[165] John Robert Anderson. Learning and memory: An integrated approach. John
Wiley & Sons Inc, 2000. 81
[166] Chee-Yong Chan, HV Jagadish, Kian-Lee Tan, Anthony KH Tung, and Zhen-
jie Zhang. On high dimensional skylines. In International Conference on
Extending Database Technology, pages 478–495. Springer, 2006. 83, 85
[167] Chee-Yong Chan, HV Jagadish, Kian-Lee Tan, Anthony KH Tung, and Zhen-
jie Zhang. Finding k-dominant skylines in high dimensional space. In Proceed-
ings of the 2006 ACM SIGMOD international conference on Management of
data, pages 503–514, 2006.
[168] Parke Godfrey, Ryan Shipley, and Jarek Gryz. Algorithms and analyses for
maximal vector computation. The VLDB Journal, 16(1):5–28, 2007. 83, 85
[169] Thomas Cover and Peter Hart. Nearest neighbor pattern classiﬁcation. IEEE
transactions on information theory, 13(1):21–27, 1967. 85

144
BIBLIOGRAPHY
[170] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J¨org Sander.
Lof: identifying density-based local outliers. In Proceedings of the 2000 ACM
SIGMOD international conference on Management of data, pages 93–104,
2000. 85
[171] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.
Isolation forest.
In
2008 Eighth IEEE International Conference on Data Mining, pages 413–422.
IEEE, 2008. 85, 90
[172] Junshui Ma and Simon Perkins. Time-series novelty detection using one-
class support vector machines.
In Proceedings of the International Joint
Conference on Neural Networks, 2003., volume 3, pages 1741–1745. IEEE,
2003. 85
[173] Ron Kohavi. Scaling up the accuracy of naive-bayes classiﬁers: A decision-
tree hybrid. In Kdd, volume 96, pages 202–207, 1996. 86
[174] Hangwei Qian, Sinno Jialin Pan, Bingshui Da, and Chunyan Miao. A novel
distribution-embedded neural network for sensor-based activity recognition.
In IJCAI, pages 5614–5620, 2019. 96
[175] Luyang Chen, Markus Pelger, and Jason Zhu. Deep learning in asset pricing.
Available at SSRN 3350138, 2020. 96
[176] Weilin Xu, Yanjun Qi, and David Evans. Automatically evading classiﬁers.
In Proceedings of the 2016 network and distributed systems symposium, vol-
ume 10, 2016. 96
[177] Kacper Sokol and Peter A Flach. Counterfactual explanations of machine
learning predictions: opportunities and challenges for ai safety. In SafeAI@
AAAI, 2019. 98, 102, 122
[178] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine
learning, 20(3):273–297, 1995. 100, 109
[179] Atoosa Kasirzadeh and Andrew Smart. The use and misuse of counterfactuals
in ethical machine learning. In Madeleine Clare Elish, William Isaac, and
Richard S. Zemel, editors, FAccT ’21: 2021 ACM Conference on Fairness,
Accountability, and Transparency, Virtual Event / Toronto, Canada, March
3-10, 2021, pages 228–236. ACM, 2021. doi: 10.1145/3442188.3445886. URL
https://doi.org/10.1145/3442188.3445886. 101, 102, 122
[180] Amir-Hossein Karimi, Bernhard Sch¨olkopf, and Isabel Valera. Algorithmic re-
course: from counterfactual explanations to interventions. In Madeleine Clare
Elish, William Isaac, and Richard S. Zemel, editors, FAccT ’21:
2021
ACM Conference on Fairness, Accountability, and Transparency, Virtual
Event / Toronto, Canada, March 3-10, 2021, pages 353–362. ACM, 2021.
doi: 10.1145/3442188.3445899. URL https://doi.org/10.1145/3442188.
3445899. 101

BIBLIOGRAPHY
145
[181] Neel Patel, Martin Strobel, and Yair Zick. High dimensional model explana-
tions: An axiomatic approach. In Madeleine Clare Elish, William Isaac, and
Richard S. Zemel, editors, FAccT ’21: 2021 ACM Conference on Fairness,
Accountability, and Transparency, Virtual Event / Toronto, Canada, March
3-10, 2021, pages 401–411. ACM, 2021. doi: 10.1145/3442188.3445903. URL
https://doi.org/10.1145/3442188.3445903. 101
[182] Yongjie Wang, Qinxu Ding, Ke Wang, Yue Liu, Xingyu Wu, Jinglong Wang,
Yong Liu, and Chunyan Miao. The skyline of counterfactual explanations for
machine learning decision models. In Proceedings of the 30th ACM Interna-
tional Conference on Information & Knowledge Management, pages 2030–
2039, 2021. 101
[183] Xueluan Gong, Qian Wang, Yanjiao Chen, Wang Yang, and Xinchang Jiang.
Model extraction attacks and defenses on cloud-based machine learning mod-
els. IEEE Communications Magazine, 58(12):83–89, 2020. 102
[184] David Cohn, Les Atlas, and Richard Ladner. Improving generalization with
active learning. Machine learning, 15(2):201–221, 1994. 102
[185] Ozan Sener and Silvio Savarese.
Active learning for convolutional neural
networks: A core-set approach.
In International Conference on Learning
Representations, 2018. 102
[186] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside con-
volutional networks: Visualising image classiﬁcation models and saliency
maps. arXiv preprint arXiv:1312.6034, 2013. 102
[187] Smitha Milli, Ludwig Schmidt, Anca D Dragan, and Moritz Hardt. Model
reconstruction from model explanations. In Proceedings of the Conference on
Fairness, Accountability, and Transparency, pages 1–9, 2019. 102
[188] Solon Barocas, Andrew D. Selbst, and Manish Raghavan. The hidden as-
sumptions behind counterfactual explanations and principal reasons.
In
Mireille Hildebrandt, Carlos Castillo, L. Elisa Celis, Salvatore Ruggieri, Lin-
net Taylor, and Gabriela Zanﬁr-Fortuna, editors, FAT* ’20: Conference on
Fairness, Accountability, and Transparency, Barcelona, Spain, January 27-
30, 2020, pages 80–89. ACM, 2020. doi: 10.1145/3351095.3372830. URL
https://doi.org/10.1145/3351095.3372830. 102, 122
[189] Chenhao Tan Ramaravind K. Mothilal, Amit Sharma. Diverse counterfactual
explanations (dice) for ml, 2020. URL https://github.com/interpretml/
DiCE. 111
[190] Give
me
some
credit
dataset.
URL
https://www.kaggle.com/c/
GiveMeSomeCredit/overview. 113
[191] Nicholas.
Eda
-
credit
scoring,
top
100
on
leader-
board.
URL
https://www.kaggle.com/nicholasgah/
eda-credit-scoring-top-100-on-leaderboard. 113

146
BIBLIOGRAPHY
[192] FICO Community. Explainable machine learning challenge. URL https://
community.fico.com/s/explainable-machine-learning-challenge. 113
[193] IBM AI Explainability 360. Credit approval. URL https://nbviewer.org/
github/IBM/AIX360/blob/master/examples/tutorials/HELOC.ipynb.
113
[194] David Harrison Jr and Daniel L Rubinfeld. Hedonic housing prices and the
demand for clean air. Journal of environmental economics and management,
5(1):81–102, 1978. 114
[195] SeldonIO alibi.
Counterfactuals guided by prototypes on boston hous-
ing dataset. URL https://docs.seldon.io/projects/alibi/en/latest/
examples/cfproto_housing.html. 114
[196] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. All models are wrong,
but many are useful: Learning a variable’s importance by studying an entire
class of prediction models simultaneously. J. Mach. Learn. Res., 20(177):
1–81, 2019. 121
[197] Pengfei Wei, Zhenzhou Lu, and Jingwen Song. Variable importance analysis:
a comprehensive review. Reliability Engineering & System Safety, 142:399–
432, 2015. 121
[198] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler,
Fernanda Viegas, and Rory Sayres. Interpretability beyond feature attribu-
tion: Quantitative Testing with Concept Activation Vectors (TCAV). 35th
International Conference on Machine Learning, ICML 2018, 6:4186–4195,
2018. 121
[199] Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening for interpretable
image recognition. Nature Machine Intelligence, 2(12):772–782, 2020. 121
[200] Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. On counterfac-
tual explanations under predictive multiplicity. In Conference on Uncertainty
in Artiﬁcial Intelligence, pages 809–818. PMLR, 2020. 123, 124
[201] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Ima-
genet: A large-scale hierarchical image database. In 2009 IEEE Conference
on Computer Vision and Pattern Recognition, pages 248–255, 2009.
doi:
10.1109/CVPR.2009.5206848. 123
[202] Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju. Towards robust
and reliable algorithmic recourse. Advances in Neural Information Processing
Systems, 34:16926–16937, 2021. 124
[203] Emily Black, Zifan Wang, and Matt Fredrikson. Consistent counterfactuals
for deep models. In International Conference on Learning Representations,
2022. URL https://openreview.net/forum?id=St6eyiTEHnG. 124

BIBLIOGRAPHY
147
[204] Andr´e Artelt and Barbara Hammer. On the computation of counterfactual
explanations - A survey. CoRR, abs/1911.07749, 2019. URL http://arxiv.
org/abs/1911.07749. 124
[205] James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg,
Fernanda Vi´egas, and Jimbo Wilson. The what-if tool: Interactive probing
of machine learning models. IEEE transactions on visualization and computer
graphics, 26(1):56–65, 2019. 125
[206] Oscar Gomez, Ste↵en Holter, Jun Yuan, and Enrico Bertini. Vice: visual
counterfactual explanations for machine learning models. In Proceedings of
the 25th International Conference on Intelligent User Interfaces, pages 531–
535, 2020. 125
[207] Tongshuang Wu, Marco Tulio Ribeiro, Je↵rey Heer, and Daniel Weld.
Polyjuice: Generating counterfactuals for explaining, evaluating, and im-
proving models. In Proceedings of the 59th Annual Meeting of the Associ-
ation for Computational Linguistics and the 11th International Joint Con-
ference on Natural Language Processing (Volume 1: Long Papers), pages
6707–6723, Online, August 2021. Association for Computational Linguis-
tics.
doi: 10.18653/v1/2021.acl-long.523.
URL https://aclanthology.
org/2021.acl-long.523. 125
[208] Alfredo Nazabal, Pablo M Olmos, Zoubin Ghahramani, and Isabel Valera.
Handling incomplete heterogeneous data using vaes. Pattern Recognition,
107:107501, 2020. 125
[209] Xiaoyuan Su and Taghi M Khoshgoftaar. A survey of collaborative ﬁltering
techniques. Advances in artiﬁcial intelligence, 2009, 2009. 126

