Diffusion Geometry
Iolo Jones
Durham University
May 2024
Abstract
We introduce diffusion geometry as a new framework for geometric and topological data analysis. Dif-
fusion geometry uses the Bakry-Emery Γ-calculus of Markov diffusion operators to define objects from
Riemannian geometry on a wide range of probability spaces. We construct statistical estimators for these
objects from a sample of data, and so introduce a whole family of new methods for geometric data analysis
and computational geometry. This includes vector fields and differential forms on the data, and many of
the important operators in exterior calculus. Unlike existing methods like persistent homology and local
principal component analysis, diffusion geometry is explicitly related to Riemannian geometry, and is signif-
icantly more robust to noise, significantly faster to compute, provides a richer topological description (like
the cup product on cohomology), and is naturally vectorised for statistics and machine learning. We find
that diffusion geometry outperforms multiparameter persistent homology as a biomarker for real and simu-
lated tumour histology data and can robustly measure the manifold hypothesis by detecting singularities in
manifold-like data.
Contents
1
Introduction
1
2
Markov Diffusion Operators and Markov Triples
3
3
Theory of Diffusion Geometry
6
4
Computing Diffusion Geometry from Data
20
5
Computational Geometry and Topology, Machine Learning, and Statistics
23
6
Computational Complexity
29
7
Comparison with Related Work
30
8
Conclusions
31
9
Appendix A: Proofs
32
10 Appendix B: Implementation Details
43
1
Introduction
Many important problems in science and engineering are closely related to the study of shape. This is especially
true in physical sciences like biology, medicine, materials science, and geology, where the role or function of
tissues, organs, materials, and crystals can be largely determined by their shape.
The recent proliferation
of better measures of this shape, such as more sensitive and accurate imagery, CT and MRI scanning, and
microscopy, has led to a huge increase in the quality and quantity of geometric data. The data have shape, and
the shape has meaning. How can we measure it?
This work introduces diffusion geometry as a new framework for geometric and topological data analysis. The
main idea is to take constructions from the mathematical theory of Riemannian geometry (where notions like
connectedness, length, perimeter, angles, and curvature are defined and well-understood) and define them on
the probability space from which our data are sampled. We justify this with Figure 1.
1
arXiv:2405.10858v1  [math.MG]  17 May 2024

Figure 1: Why probability spaces?
In the top left of Figure 1 is a manifold, M, whose shape is described by Riemannian geometry. Bottom left
is a finite sample of points drawn from M, in which we can clearly see much of the same geometry as M. It
stands to reason that we should be able to estimate these features statistically, given such a sample. Top right
is a probability density function, q, taking large values near M, and perhaps represents a ‘manifold with noise’.
It is also clear that most of what can be said about the geometry of M can be said for the density q. Bottom
right is a sample drawn from q, where again we recognise the same ‘geometry’ as q, and hope to use this sample
to construct estimators for it.
These observations lead to two questions:
1. How much of the Riemannian geometry of a manifold is a special case of the ‘Riemannian geometry’ of a
probability space?
2. How can the geometry of manifolds and probability spaces be estimated from finite samples of data?
We propose diffusion geometry as a natural extension of Riemannian geometry to certain probability spaces.
We show how to estimate it from data as a general framework for geometric and topological machine learning
and statistics, as well as computational geometry and topology. We now briefly highlight the main aspects of
diffusion geometry (see Section 7 for a full comparison with related work).
1.1
Why define a new theory?
Applied geometry and topology are often motivated by the idea that ‘data have shape’. However, data come
from probability spaces. In this paper, we will argue that really it is the underlying probability space that has
shape. We will use Markov diffusion operators, which generalise the heat flow on a manifold, to develop a theory
of Riemannian geometry on a broad class of probability spaces.
Crucially, this theory will lead to a natural statistical framework for estimating this geometry from data. All the
objects we define in the theory have corresponding algorithms that follow straight from the definitions. While
our theory does generalise a variety of existing approaches to metric and measure-theoretic geometry, the main
motivation is that it leads directly to computational tools for data.
1.2
Machine learning and statistics
There are centuries of work in differential and Riemannian geometry that can be exploited for geometric machine
learning and statistics. We can compute objects from this theory to create shape descriptors for the data, known
as feature vectors in machine learning. These encode a rich geometric representation (measuring things like
2

curvature, connectedness, and the spectrum of the Hodge Laplacian), which can then be used for supervised
and unsupervised learning. As we will explore, these feature vectors are robust to noise and outliers, fast to
compute, and explainable, so that the output of the final model can be interpreted and used to learn more
about the data.
A fundamental observation of geometric deep learning [15] is that geometric machine learning models should be
invariant under various group actions, such as translations and rotations of the input data. Our feature vectors,
and hence our models, will be invariant under translation, rotation, reflection, and, if required, also rescaling.
However, group invariance alone is not enough. Geometry is also preserved by adding noise and outliers, and,
perhaps, changing the sampling density of the data.
Models should therefore be both group invariant and
statistically invariant. The diffusion geometry features are highly robust to noise, and, if required, can be made
density-invariant as well.
We test diffusion geometry for supervised and unsupervised learning on tumour histology data and find that it
outperforms existing methods.
1.3
Computational and numerical geometry and topology
When the data are drawn from a manifold, diffusion geometry agrees with the usual theory of Riemannian
geometry. As such, we can use it as a computational tool for investigating the properties of manifolds, and
using optimisation to test examples and conjectures in Riemannian geometry. This means we can view diffusion
geometry as a novel tool for numerical geometry that, uniquely, only requires a sample of points from the
manifold, which need not be uniform, and no triangulation.
Many applications of geometry and topology to data assume the manifold hypothesis: that the data are drawn
from a manifold.
We can also use diffusion geometry to test this hypothesis and develop tools for finding
singularities in otherwise manifold-like data.
1.4
How to read this paper
The theory of diffusion geometry is developed in Sections
(2) where we show that Markov diffusion operators give a suitable generalisation of the Laplacian operator
on a manifold,
(3) where we define an analogue of Riemannian geometry using this alternative Laplacian, and
(4) where we show how to estimate this geometry from data.
If you are mainly interested in using diffusion geometry for data analysis then read Sections
(5) where we test diffusion geometry as a tool for computational geometry and topology, machine learning,
and statistics,
(6) where we test the computational complexity of diffusion geometry, and
(7) where we review existing methods and compare them with diffusion geometry.
2
Markov Diffusion Operators and Markov Triples
Much of the Riemannian geometry of a manifold can be described in terms of its Laplacian operator ∆, via the
‘carr´e du champ’ identity
1
2
 f∆h + h∆f −∆(fh)

= g(df, dh),
(1)
which gives a formula for the Riemannian metric of 1-forms df and dh in terms of ∆. The main idea behind
diffusion geometry is to replace the manifold Laplacian ∆with some other operator L on a more general space
so that (1) can become a definition for Riemannian geometry on that space. In this section, we will propose
the use of Markov diffusion operators as an appropriate generalisation of the Laplacian.
3

2.1
Examples of Markov diffusion operators
The theory of Markov diffusion operators is very well developed, and we will only state the essential properties
here (refer to [9] for a comprehensive reference). As a motivating example, we consider the heat diffusion in the
Euclidean space Rn. The Gaussian kernel
pt(x, y) =
1
(4πt)n/2 exp

−∥x −y∥2
4t

(2)
can be used to define the heat diffusion operator Pt as
Ptf(x) =
Z
pt(x, y)f(y)dy.
Setting u(t, x) = Ptf(x) then solves the heat equation
∂u
∂t = ∆u
u(0, x) = f(x).
So if f is an initial distribution of heat in space, then Ptf is the heat distribution after t time, and ∆measures
the rate of diffusion. The operator Pt satisfies
lim
t→0
Ptf −f
t
= −∆f,
(3)
in L2(Rn), so we can interpret the Laplacian ∆as the derivative of the diffusion operator Pt at t = 0. In the
language of Markov diffusion operators, we say that (Pt)t≥0 is a Markov semigroup and ∆is its infinitesimal
generator. By ‘semigroup’ we mean that Pt is an operator acting on a function space, and Pt ◦Ps = Pt+s, and
P0 = Id. A semigroup has an infinitesimal generator defined as the limit in (3). There are many important
examples of semigroups.
Example 2.1. If M is a Riemannian manifold, then the Laplacian ∆defines a heat equation as above, and
the associated heat diffusion operator Pt is a semigroup. The infinitesimal generator is the Laplacian. This is
the central example that motivates the rest of this work.
Example 2.2. If M is a Riemannian manifold with a density q(x), known as a weighted manifold, then the
heat kernel can be used to define a weighted heat diffusion operator
Ptf(x) =
Z
pt(x, y)f(y)q(y)dy.
The operator Pt is also a semigroup, and its infinitesimal generator is given by
∆q = ∆−∆q
q .
Weighted manifolds will also be an important source of motivation for this work, and are well explained in [25].
Example 2.3. If (Xt)t≥0 is a Markov process on a measurable space E, then
Ptf(x) = E(f(Xt) : X0 = x)
is a semigroup acting on some space of measurable functions. The two above examples are a special case of this
one, corresponding to Xt being a standard or weighted Brownian motion. If E is finite (or countable) then Pt
is called a Markov chain.
These examples are all specifically Markov semigroups: see Sections 1 and 2 of [9] for a formal definition and
many other examples. The important point here is that the Laplacian on a manifold is a special example of an
infinitesimal generator of a Markov semigroup.
2.2
Markov triples
Markov semigroups are often analysed indirectly through their associated carr´e du champ operators, which are
inspired by the carr´e du champ identity (1).
4

Definition 2.4. If L is the infinitesimal generator of a Markov semigroup defined on a function space A, the
bilinear map A × A →A given by
Γ(f, h) = 1
2
 fLh + hLf −L(fh)

is called the carr´e du champ operator.
So if L = ∆on a manifold, then Γ(f, h) = g(df, dh). In the next section, we will use this property of the
carr´e du champ Γ to generalise manifold geometry: it will act in place of the metric where none exists. The
general setup we require is defined as follows (where a ‘good’ measurable space is subject to loose but technical
conditions explained in [9], Section 1.1).
Definition 2.5. A Markov triple (E, µ, Γ) consists of
1. a ‘good’ measurable space (E, F) (where E is a set with σ-algebra F) with a σ-finite measure µ,
2. an algebra of real-valued bounded functions A that is dense in all the Lp(M) spaces, and
3. a symmetric bilinear map Γ : A × A →A (the carr´e du champ operator),
such that
(a) Γ(f, f) ≥0 for all f ∈A,
(b) the fundamental identity
Z
E
Γ(h, f 2)dµ + 2
Z
E
hΓ(f, f)dµ = 2
Z
E
Γ(fh, f)dµ
holds for all f, h ∈A,
(c) there exists an increasing sequence of functions in A converging µ-almost everywhere to the constant
function 1.
We are only interested in probability spaces, so always normalise µ(E) = 1. The following canonical example
justifies the rest of this work.
Example 2.6. If M is a compact manifold with Riemannian metric g and induced Riemannian volume form
dx, and Γ(f, h) = g(df, dh) then (M, dx, Γ) is a Markov triple with A = C∞(M). When we say that a Markov
triple ‘is a manifold’, this is what we mean.
This is a special example of a very general class of Markov triples arising from Markov semigroups.
Example 2.7. Let (Xt)t≥0 be a Markov process on a measurable space E, where the Markov semigroup
Ptf(x) = E(f(Xt) : X0 = x)
acts on a space A of measurable functions satisfying the conditions in Definition 2.5. Suppose that µ is an
invariant measure for Pt, meaning that
R
E Pt(f)dµ =
R
E fdµ for all f ∈L1(µ), and let Γ be the carr´e du champ
of Pt. Then (E, µ, Γ) is a Markov triple. The previous example is a special case of this one, corresponding to
Xt being a Brownian motion on a manifold.
Manifolds
Markov triples
M
E
dx
µ
C∞
0 (M)
A
heat diffusion operator
Markov semigroup
∆
infinitessimal generator L
g(df, dh)
Γ(f, h)
This class includes many classical examples (see [9] chapter 2). The important idea is that Markov triples
(corresponding to Markov semigroups) generalise Riemannian manifolds (corresponding to heat diffusion oper-
ators). We can use them to define geometry on a broad class of measure spaces. For example, a probability
density function on Rn (such as in Figure 1) defines a weighted heat diffusion (Example 2.2) that defines the
appropriate geometry. It also includes the crucial case where E is finite, so Pt is a Markov chain.
5

Example 2.8. Let X be a finite set with σ-algebra given by all its subsets, and A be the space of real-valued
functions on X (so if |X| = n, we can identify A ∼= Rn with component-wise multiplication). Let Pt be a Markov
chain on X (i.e. a stochastic matrix), with stationary distribution µ and carr´e du champ Γ. Then (X, µ, Γ) is
a Markov triple (see [9] Section 1.9.1). To be explicit, if L is the generator of Pt and i ∈E, we get the formula
Γ(f, h)(xi) = −
n
X
j=1
L(i, j)
 fi −fj
 hi −hj

.
Suppose M = (E, µ, Γ) is a Markov triple arising from a Markov semigroup Pt, and X ⊂E is a finite sample
of data. As explained in Section 4, we can construct a finite Markov triple for X that approximates M by
constructing a Markov chain on X that approximates Pt.
We will consider the special class of Markov triples called diffusion Markov triples, where the carr´e du champ
satisfies an analogue of the Leibniz rule (when the Markov triple is a manifold, the diffusion property is equivalent
to the Leibniz rule for d).
Definition 2.9. Let (E, µ, Γ) be a Markov triple with function algebra A. Suppose A is closed under composi-
tion with smooth functions Rn →R that vanish at 0, so if f1, ..., fn ∈A and ϕ : Rn →R then ϕ(f1, ..., fn) ∈A.
The carr´e du champ operator Γ has the diffusion property if
Γ(ϕ(f1, ..., fn), h) =
n
X
i=1
∂iϕ(f1, ..., fn)Γ(fi, h)
for all f1, ..., fn, h ∈A and smooth ϕ : Rn →R vanishing at 0. The Markov triple is then called a diffusion
Markov triple.
3
Theory of Diffusion Geometry
We can now construct a theory of Riemannian geometry using only the properties of diffusion Markov triples,
which we will term ‘diffusion geometry’. The approach is essentially algebraic: we may not have the usual notions
of tangent and cotangent bundles, sections, or connections, but we can instead use the various algebraic identities
between them (like the carr´e du champ) as alternative definitions. This section is expository, and all proofs are
deferred to Appendix A (section 9). We illustrate the definitions and concepts with examples of Markov triples
corresponding to density functions on R2, which we approximate by finite data (this computation is explained
in Section 4). The Python code for all the examples is available at github.com/Iolo-Jones/DiffusionGeometry.
In this section, we will cover
(3.1) differential forms and the exterior algebra,
(3.2) first-order calculus: vector fields and their action on functions,
(3.3) second-order calculus: the Hessian (second derivative), covariant derivative, and Lie bracket,
(3.4) the exterior derivative and codifferential on forms,
(3.5) differential algebra: differential operators, de Rham cohomology, and the Hodge Laplacian, and
(3.6) third-order calculus: the second covariant derivative and curvature.
3.1
Differential forms
We start by defining differential forms on a diffusion Markov triple M = (E, µ, Γ), with function algebra A.
While we do not have a cotangent bundle on Markov triples, we observe that, on a manifold M,
Ω1(M) = span{fdh : f, h ∈C∞(M)}.
Using this, along with the fact that we expect Γ(f, h) = g(df, dh), we make the following definition.
Definition 3.1. Define a (possibly degenerate) inner product ⟨·, ·⟩on the tensor product A ⊗A by
⟨f ⊗h, f ′ ⊗h′⟩:=
Z
ff ′Γ(h, h′)dµ.
6

Let D1 = {ω ∈A ⊗A : ⟨ω, ω⟩= 0} be the vector subspace on which ⟨·, ·⟩is degenerate. We define the space
of differential 1-forms as the quotient space
Ω1(M) := A ⊗A
D1
with the inner product ⟨·, ·⟩and induced norm ∥· ∥, and where the elements f ⊗h are denoted fdh.
Although this definition of forms is purely formal, we hope that, by imposing the right inner product structure
on it, we can recover enough of the geometry. In other words, we want to construct ‘manifold-free’ Riemannian
geometry using only the metric. We take a similar approach for the spaces of higher order k-forms. On a
manifold M, their metric is given by
g(α1 ∧· · · ∧αk, β1 ∧· · · ∧βk) = det(g(αi, βj)).
which suggests the following definition.
Definition 3.2. Define a (possibly degenerate) inner product ⟨·, ·⟩on A ⊗Vk A by
⟨f ⊗(h1 ∧· · · ∧hk), f ′ ⊗(h′
1 ∧· · · ∧h′
k)⟩:=
Z
ff ′ det(Γ(hi, h′
j))dµ.
Let Dk = {ω ∈A ⊗Vk A : ⟨ω, ω⟩= 0} be the vector subspace on which ⟨·, ·⟩is degenerate. We define the
space of differential k-forms as the quotient space
Ωk(M) := A ⊗Vk A
Dk
with the inner product ⟨·, ·⟩and induced norm ∥· ∥, and where the elements f ⊗(h1 ∧· · · ∧hk) are denoted
fdh1 ∧· · · ∧dhk.
Notice that Ωk(M) is an A-module, and that Ω0(M) = A with the inner product from L2(M). We can define
operators between these spaces of forms by defining them on the space A ⊗Vk A. To check that they are well
defined, we need to check that they are zero on forms of zero norm, and so descend to the quotient Ωk(M).
We visualise forms in Figure 2 using their dual vector fields (explained in Remark 3.12), as well as the metric
which we now define.
Definition 3.3. The metric is the bilinear function g : Ωk(M) × Ωk(M) →A given by
g(fdh1 ∧· · · ∧dhk, f ′dh′
1 ∧· · · ∧dh′
k) := ff ′ det(Γ(hi, h′
j)).
Note that g(α, α′) ∈A, because the tensor product A ⊗Vk A comprises finite linear combinations of the
irreducible elements on which g is defined. We then see that
⟨α, β⟩=
Z
g(α, β)dµ < ∞
for all α, β ∈Ωk(M), as functions in A have finite integral, so the inner product is finite. We now verify that
the metric satisfies all the properties we would like, so the notion of differential forms introduced above is well
defined.
Proposition 3.4. The metric g on Ωk(M) is symmetric, bilinear, and positive semi-definite. In particular, it
satisfies the Cauchy-Schwarz inequality pointwise, and the inner product ⟨·, ·⟩and metric g on Ωk(M) are well
defined.
We abbreviate 1dh1 ∧· · · ∧dhk to dh1 ∧· · · ∧dhk. Using these definitions for differential forms, we can collect
other objects from Riemannian geometry for our theory (see Figure 3).
Definition 3.5. The wedge product is the bilinear map ∧: Ωk(M) × Ωl(M) →Ωk+l(M) defined by
(fdh1 ∧· · · ∧dhk, f ′dh′
1 ∧· · · ∧dh′
l) 7→ff ′dh1 ∧· · · ∧dhk ∧dh′
1 ∧· · · ∧dh′
l.
The wedge product induces an isometric isomorphism Ωk(M) ∼= Vk Ω1(M) as a tensor product of A-modules.
7

(a) α
(b) g(α, α)
(c) β
(d) g(α, β)
(e) γ
(f) g(γ, γ)
Figure 2: Forms and the metric. The 1-forms α and β are represented
by their dual vector fields (a, c) (see Remark 3.12), and the 2-form γ
is (non-uniquely) represented by its dual bi-vector field (e). g(α, α) and
g(γ, γ) measure the supports of α and γ (b, f), and g(α, β) measures the
alignment of α and β (d). Negative values are blue, and positive values
are madder rose.
Proposition 3.6. If α ∈Ωk(M) and β ∈Ωl(M) then g(α ∧β, α ∧β) ≤g(α, α)g(β, β). In particular, the wedge
product is well defined on Ωk(M) × Ωl(M), and is a bounded linear operator in each argument.
See Figures 3 and 10 for examples of the wedge product. We also define the exterior derivative, but for now
only on functions1.
Definition 3.7. The exterior derivative is the linear map d0 : A →Ω1(M) defined by
f 7→df,
1The derivative of k-forms is given in subsection 3.4.
8

i.e. f maps to the equivalence class of 1 ⊗f ∈A ⊗A.
We will usually suppress the subscript in d0 and just write d where there is no ambiguity. We immediately see
that these definitions agree with the notation, for example
fdh1 ∧dh2 = f ∧
 d0(h1) ∧d0(h2)

.
(a) α
(b) f
(c) fα
(d) df
Figure 3: Wedge product and exterior derivative. We can multiply
a 1-form α (a) by a function f (b). The product fα (c) restricts α to the
support of f (the left ‘component’ of the data), and the negative values
of f (coloured blue) reverse the orientation of α. df, visualised as its dual
vector field ∇f (d), shows the direction of steepest increase of f. See
Figure 10 for a wedge product of two 1-forms.
The fact that M is a diffusion Markov triple means the Leibniz rule holds for d, as an example of the chain
rule.
Proposition 3.8 (Calculus rules for d0). The following properties hold for d = d0:
• Chain rule: if f1, ..., fk ∈A and ϕ : Rk →R is smooth then
d
 ϕ(f1, ..., fk)

=
k
X
i=1
∂iϕ(f1, ..., fk)dfi
• Leibniz rule: if f, g ∈A then d(fg) = fd(g) + gd(f).
3.2
First-order calculus: vector fields and duality
We now construct the first-order calculus in diffusion geometry, including vector fields and their duality with
1-forms. We first consider the larger set of derivations, which define a sort of directional derivative of functions.
9

Definition 3.9. A derivation is a linear map X : A →A which satisfies X(fh) = fX(h) + hX(f) (which we
call the Leibniz rule).
Contraction with the metric g gives a natural pairing between forms and derivations. If α ∈Ω1(M) is a 1-form,
then α♯(f) := g(α, df) defines a derivation (where the Leibniz rule for α♯is inherited from d). We define the
vector fields as the derivations in the image of this map (see Figure 4).
Definition 3.10. A vector field is a derivation X that satisfies X = α♯for some α ∈Ω1(M). X is then
called the dual vector field of α. We denote the space of vector fields by X(M). Each vector field X has
a dual one form X♭defined by g(X♭, df) = X(f) for all f ∈A. We extend the metric to vector fields as
g(X, Y ) = g(X♭, Y ♭).
This definition of X♭specifies it uniquely because the terms fdg span Ω1(M), and we can evaluate ⟨X♭, fdh⟩=
R
fg(X♭, dh)dµ. It is clear to see that the map α 7→α♯is an isometric isomorphism between Ω1(M) and X(M),
with inverse X 7→X♭(both called musical isomorphisms). We can use this duality to define the usual action
of 1-forms on vector fields.
Definition 3.11. A 1-form α defines an A-linear map X(M) →A by α(X) = g(α, X♭).
(a) X
(b) f
(c) X(f)
(d) h
(e) X(h)
Figure 4: Vector fields act on functions. The vector field X (a) acts
on f and h (b, d) by differentiation in the clockwise direction (c, e).
Remark 3.12. We can visualise vector fields by their action on the coordinate functions of data. In all the
examples in this section, the data are drawn from probability distributions on R2, and so have two coordinate
functions x and y. We can calculate the ‘direction’ of a vector field X at a point p, with position
 x(p), y(p)

,
as the vector
 X(x)(p), X(y)(p)

. We visualise 1-forms by their dual vector fields, i.e. (fdh)♯(x) = fΓ(h, x).
We can also define the gradient operator by duality.
Definition 3.13. The gradient ∇: A →X(M) is the dual operator to the exterior derivative d0, defined by
∇(f) = (df)♯.
Given this definition of gradient, we see that we could offer precisely the same definition for X(M) as of Ω1(M),
10

except with d exchanged for ∇(hence the musical isomorphism). The duality of forms and vector fields lets us
define the interior product, which satisfies the usual properties.
Definition 3.14 (Interior product). If X ∈X(M), we define the interior product iX : Ωk(M) →Ωk−1(M)
inductively by
1. if α ∈Ω0(M), iX(α) := 0
2. if α ∈Ω1(M), iX(α) := α(X)
3. if α ∈Ωk(M), iX(α ∧β) := iX(α) ∧β + (−1)kα ∧iX(β).
Proposition 3.15. iX(α) a well defined bilinear operator in X and α, and satisfies iY iX = −iXiY .
We are focusing, for simplicity, on a function space A which satisfies several nice properties. The next section
will require more delicate functional analysis, so we define the following complete spaces of forms and vector
fields.
Definition 3.16. We define L2Ω1(M) and L2X(M) to be the completions of Ω1(M) and X(M) respectively.
3.3
Second-order calculus: Hessian, covariant derivative, and Lie bracket
We can now introduce the second-order objects from Riemannian geometry, and start by introducing the
necessary tensor products of forms and vector fields.
Definition 3.17. We define the space of (0,2)-tensors
Ω1(M)⊗2 := Ω1(M) ⊗A Ω1(M),
so elements of Ω1(M)⊗2 are finite sums P
i hidai ⊗dbi. We extend the metric to these tensors by
g(h1da1 ⊗db1, h2da2 ⊗db2) := h1h2g(a1, a2)g(b1, b2).
Equivalently, g(α1 ⊗β1, α2 ⊗β2) = g(α1, α2)g(β1, β2). The pointwise norm
p
g(A, A) is known as the (point-
wise) Hilbert-Schmidt norm of the tensor and is denoted |A|HS. This metric induces a norm by integration,
and the complete space of (0,2)-tensors, denoted L2(Ω1(M)⊗2), is the completion of Ω1(M)⊗2.
Definition 3.18. We define the space of (2,0)-tensors
X(M)⊗2 := X(M) ⊗A X(M),
so elements of X(M)⊗2 are finite sums P
i hi∇ai ⊗∇bi. We extend the metric analogously to the (0,2)-tensors,
and define the corresponding norm. The complete space of (2,0)-tensors, denoted L2(X(M)⊗2), is the
completion of X(M)⊗2. We obtain the usual musical isomorphisms ♯and ♭by
(fdh1 ⊗dh2)♯= f∇h1 ⊗∇h2
(f∇h1 ⊗∇h2)♭= fdh1 ⊗dh2,
which are isometric isomorphisms Ω1(M)⊗2 ∼= X(M)⊗2 and L2(Ω1(M)⊗2) ∼= L2(X(M)⊗2). There is an action
of (0, 2)-tensors on (2, 0)-tensors: if A ∈Ω1(M)⊗2 or A ∈L2(Ω1(M)⊗2) and ξ ∈X(M)⊗2 then
A(ξ) := g(A, ξ♭).
If A ∈Ω1(M)⊗2 or A ∈L2(Ω1(M)⊗2), we also define
A(X, Y ) := A(X ⊗Y )
for X, Y ∈X(M).
3.3.1
Hessian
We can use this tensor technology to define the second-order geometric objects, starting with the Hessian. For
functions f : Rn →R, the Hessian is the matrix
 ∂2f
∂xi∂xj

, and so generalises the second derivative of a function
(see Figure 5). On manifolds, we get the same expression in geodesic normal coordinates. The formula we use
here as a definition is derived from standard properties of the manifold Hessian in Appendix A, Proposition 9.1.
11

Definition 3.19. We define the Sobolev space W 2,2(M) ⊆A as the set of f ∈A for which there exists an
A ∈L2(Ω1(M)⊗2) satisfying
A(∇a, ∇b) = 1
2
 Γ(a, Γ(f, b)) + Γ(b, Γ(f, a)) −Γ(f, Γ(a, b))

.
for all a, b ∈A. This tensor, called the Hessian, is uniquely determined and is denoted H(f).
(a) f
(b) X
(c) Y
(d) g(df, df) = Γ(f, f)
(e) H(f)(X, X)
(f) H(f)(X, Y )
Figure 5: The Hessian (second derivative of functions). A function
f has a maximum at the top right of the data and a minimum at the top
left (a), which correspond to points where its derivative df is zero (d).
Given a vector field X, which is supported near these local extrema (b),
we can compute the Hessian H(f)(X, X) to classify the extrema relative
to the direction of X (e). The Hessian is negative at the maximum and
positive at the minimum. If we use a different vector field Y (c) as the
second argument, the Hessian changes sign in the areas where X and Y
are unaligned (f).
It is clear to see that the Hessian is a trilinear operator in f, a, and b, and is symmetric in a and b. The
formula above dually defines H(f) as an operator X(M)⊗2 →A for any f ∈A, but this is not necessarily
a bounded operator. If it were bounded, the Reisz representation theorem would guarantee the existence of a
H(f) ∈L2(Ω1(M)⊗2). Instead, we have to make do with W 2,2(M), which may be a proper subset of A. In fact,
in the most general setting, we cannot guarantee which, if any, functions from A are in W 2,2(M). Although, in
the special case of RCD spaces2, the following result follows from a theorem of Gigli.
Theorem 3.20 (Follows from 6.2.22. in [24]). If M is an RCD(K, ∞) space then A = W 2,2(M).
The Sobolev space W 2,2(M) contains the functions whose Hessian exists as an L2(Ω1(M)⊗2) tensor. In the
following, we would like to work with Hessians that are actually in Ω1(M)⊗2, and so define the following space
of ‘twice differentiable’ functions.
2RCD spaces place a lower bound on the ‘Ricci curvature’ of a metric measure space in a synthetic sense.
A space being
RCD(K, ∞) is equivalent to the Ricci curvature being bounded below by K, and imposes strong regularity properties on the heat
flow.
12

Definition 3.21. We define the ∞-Sobolev space
W 2,∞(M) := {f ∈W 2,2 : H(f) ∈Ω1(M)⊗2}.
In other words, W 2,∞(M) contains the functions whose Hessian exists and can be written as a finite sum
H(f) =
X
i
hidai ⊗dbi,
unlike the other functions in W 2,2(M) whose Hessian may be a limit of such sums. The other second-order
differential objects can be built from the Hessian, and so, by considering just the functions from W 2,∞(M), we
can guarantee that these other objects are well defined. Specifically, we can now define a notion of ‘differentiable’
vector fields.
Definition 3.22. We define the space of differentiable vector fields
X1(M) :=
nX
fi∇hi ∈X(M) : hi ∈W 2,∞(M) for all i
o
.
These vector fields are ‘differentiable’ because the functions hi have already been differentiated once by ∇, and
so we also need their second derivative H(hi) to be a well behaved tensor. We can derive the usual calculus
rules for H.
Proposition 3.23. The Hessian satisfies the (second order) Leibniz rule
H(fh) = fH(h) + hH(f) + df ⊗dh + dh ⊗df
for all f, h ∈W 2,2(M). In particular, W 2,2(M) and W 2,∞(M) are closed under multiplication (and so are
subalgebras of A).
Proposition 3.24. The Hessian satisfies the product rule for the carr´e du champ
d
 Γ(f1, f2)

= H(f1)(∇f2, ·) + H(f2)(∇f1, ·)
for all f1, f2 ∈W 2,2(M).
3.3.2
Covariant derivative
We now define the covariant derivative, which is usually defined as a (1, 1) tensor field. Here, we instead define
it as a (0, 2) field to make use of the space X1(M). The formula we use comes from the fact that, on a manifold,
∇Y (f∇h) = df(Y )∇h + f∇Y (∇h)
and g(∇Y (∇h), Z) = H(h)(Y, Z) by definition, so
g(∇Y (f∇h), Z) = df(Y )dh(Z) + fH(h)(Y, Z).
Definition 3.25. If X = P fi∇hi ∈X1(M), we define ∇X ∈Ω1(M)⊗2 by
∇X :=
X  dfi ⊗dhi + fiH(hi)

,
which we call the covariant derivative of X.
Proposition 3.26. The covariant derivative ∇is a well defined map X1(M) →Ω1(M)⊗2.
We saw earlier that H(f) is well defined as an operator X(M)⊗2 →A for all f ∈A, even if that operator is not a
tensor in L2(Ω1(M)⊗2). By extension, the formula for ∇X above defines a ‘weak’ derivative ∇X : X(M)⊗2 →A
by
∇X(Y, Z) =
X  Y (fi)Z(hi) + fiH(hi)(Y, Z)

for all X = P fi∇hi ∈X(M), although we cannot guarantee that this operator defines a form in L2(Ω1(M)⊗2)
unless X ∈X1(M). We can now define the (1, 1) tensor field version of the covariant derivative (see Figure 6
(d)).
13

Definition 3.27. If X = P fi∇hi ∈X1(M), we define ∇Y X ∈X(M) by
∇Y X :=
X  Y (fi)∇hi + fi[H(hi)(Y, ·)]♯
for all Y ∈X(M). If f ∈A, we also define ∇Y (f) = Y (f).
Notice that g(∇Y X, Z) = ∇X(Y, Z) for all Z ∈X(M), so X 7→∇Y X is well defined by Proposition 3.26. As
with the Hessian and ∇, we can define a more general ‘weak’ derivative ∇Y X as a derivation A →A by
∇Y X(f) = g(∇Y X, ∇f) = ∇X(Y, ∇f)
for all X ∈X(M), although this only defines a vector field ∇Y X ∈X(M) under stronger assumptions. We
check that this definition for ∇satisfies the standard properties.
Proposition 3.28. The covariant derivative ∇is an affine connection, meaning ∇X(Y, Z) is A-linear in Y
and Z, linear in X, and satisfies the Leibniz rule
∇(fX) = df ⊗X♭+ f∇X.
In particular, ∇Y X is A-linear in Y , and ∇Y (fX) = Y (f)X + f∇Y X.
Proposition 3.29. The covariant derivative ∇is compatible with the metric, meaning
∇Xg(Y, Z) = g(∇XY, Z) + g(Y, ∇XZ).
3.3.3
Lie bracket
The final property needed for the covariant derivative to be a Levi-Civita connection is that it is also torsion-free,
a property involving the Lie bracket (see Figure 6 (c)). Here, we will take this property as a definition.
Definition 3.30. If X, Y ∈X1(M) then we define their Lie bracket [X, Y ] ∈X(M) by
[X, Y ] := ∇XY −∇Y X.
The Lie bracket is usually defined as the commutator of X and Y as operators XY −Y X, although we cannot
always guarantee3 that XY −Y X ∈X(M). However, on X1(M) where the Lie bracket is a well-defined vector
field, it does agree with the commutator.
Proposition 3.31. We have [X, Y ] = XY −Y X for all X, Y ∈X1(M).
Notice, again, that we can define a ‘weak’ Lie bracket as a derivation [X, Y ] : A →A by f 7→X(Y (f))−Y (X(f)),
although this does not necessarily define a vector field without stronger assumptions. We can also define the
action of a form on the Lie bracket in this weak sense, by (in the example of 1-forms),
fdh([X, Y ]) = f
 X(Y (h)) −Y (X(h))

.
(4)
This ‘weak’ notion of action will be crucial in Proposition 3.38 below. We find that the usual product rule holds
for the Lie bracket.
Proposition 3.32. We have [X, fY ] = X(f)Y + f[X, Y ] and [fX, Y ] = −Y (f)X + f[X, Y ].
The above properties of ∇and the Lie bracket lead, by a standard argument, to the Koszul formula
g(∇XY, Z) = ∇Y (X, Z) = 1
2

X(g(Y, Z)) + Y (g(Z, X)) −Z(g(X, Y ))
+ g([X, Y ], Z) −g([Y, Z], X) + g([Z, X], Y )

,
which shows that ∇is the unique affine linear connection (in the sense of Proposition 3.28) which is torsion-free
(satisfies Definition 3.30) and preserves the metric (Proposition 3.29), and so is the Levi-Civita connection.
We can use the Lie bracket to derive the following formula to evaluate the Hessian.
Proposition 3.33. The Hessian satisfies
H(f)(X, Y ) = 1
2

g(X, [Y, ∇f]) + g(Y, [X, ∇f]) + Γ(f, g(X, Y ))

.
3The commutator XY −Y X is always a derivation, but that is not enough to be a vector field.
14

(a) X
(b) Y
(c) [X, Y ]
(d) ∇XY
Figure 6: Lie bracket and covariant derivative. Given two vector
fields X and Y (a, b), both [X, Y ] and ∇XY define a notion of ‘differen-
tiating Y with respect to X’ (c, d). They measure the ‘acceleration’ or
change in relative velocity of Y in the direction of X, and produce similar
pictures. Following the cycle of X clockwise, the flow of Y on the left
and right sides is constant relative to X, and so [X, Y ] and ∇XY are zero
here. But at the top and bottom Y changes orientation (relative to X):
it accelerates at the bottom of the cycle and decelerates over the top, and
[X, Y ] and ∇XY increase in the direction of this force.
3.3.4
Covariant derivative of forms
We can use these conditions, as well as duality, to extend the definition of ∇X to other tensors. We first define
the following notions of ‘differentiable’ forms analogous to X1(M).
Definition 3.34. We define the space of differentiable k-forms
Ωk,1(M) :=
nX
fidhi
1 ∧· · · ∧dhi
k ∈Ωk(M) : hi
j ∈W 2,∞(M) for all i, j
o
.
So Ω1,1(M) is exactly the ♭-dual of X1(M). We can define the covariant derivative on 1-forms by this duality.
Definition 3.35. If α ∈Ω1,1(M), then define ∇Xα :=
 ∇X(α♯)
♭= ∇(α♯)(X, ·).
We can derive the usual formula for the action of ∇Xα on a vector field Y , using the fact that ∇X preserves
the metric, as
∇Xα(Y ) = g(∇X(α♯), Y )
= ∇Xg(α♯, Y ) −g(α♯, ∇XY )
= X(α(Y )) −α(∇XY ).
This action formula defines a ‘weak’ notion of ∇Xα as an operator X(M) →A for any α ∈Ω1(M), but ∇Xα
is not necessarily a 1-form in Ω1(M) unless α ∈Ω1,1(M). We can further extend ∇X to tensor products in a
15

way that satisfies the Leibniz rule, so in general
∇X(ξ ⊗ζ) = ∇X(ξ) ⊗ζ + ξ ⊗∇X(ζ).
In particular, we obtain the following definition.
Definition 3.36. ∇X is defined inductively on α, β ∈Ωk,1(M) by
∇X(α ∧β) := ∇X(α) ∧β + α ∧∇X(β).
It is straightforward to check that ∇Xα is still linear in α and A-linear in X, and satisfies the Leibniz rule in α.
3.4
Exterior derivative and codifferential
We can now introduce the exterior derivative on higher-order forms. The derivative is a fundamental object in
geometry, although we have waited this long to define it to employ the Lie bracket in the following proofs.
Definition 3.37. The exterior derivative is the linear map dk : Ωk(M) →Ωk+1(M) defined by
fdh1 ∧· · · ∧dhk 7→df ∧dh1 ∧· · · ∧dhk.
Proposition 3.38. If X0, ..., Xk ∈X(M) and α ∈Ωk(M) then dkα satisfies
dkα(X0, ..., Xk) =
k
X
i=0
(−1)iXi(α(..., ˆ
Xi, ...)) +
X
i<j
(−1)i+jα([Xi, Xj], ..., ˆ
Xi, ..., ˆ
Xj, ...),
where [Xi, Xj] is defined in the weak sense4. In particular, if ∥α∥= 0 then ∥dkα∥= 0, so dk is a well defined
map.
As with d0, we will usually drop the subscript. We immediately see that d2 = 0, because
d2(fdh1 ∧· · · ∧dhk) = d(1) ∧df ∧dh1 ∧· · · ∧dhk
and d(1) = 0. We also find that the exterior derivative satisfies the Leibniz rule.
Proposition 3.39. If α ∈Ωk(M) and β ∈Ωl(M) then
d(α ∧β) = dα ∧β + (−1)kα ∧dβ.
We can also introduce the dual operator to d, known as the codifferential ∂, which we define as follows.
Definition 3.40. The codifferential is the linear map ∂k : Ωk,1(M) →Ωk−1(M) defined by
fdh1 ∧· · · ∧dhk 7→
k
X
i=1
(−1)i Γ(f, hi) −fL(hi)

dh1 ∧· · · ∧ˆ
dhi ∧· · · ∧dhk
+
X
i<j
(−1)i+jf[∇hi, ∇hj]♭∧dh1 ∧· · · ∧ˆ
dhi ∧· · · ∧ˆ
dhj ∧· · · ∧dhk.
Proposition 3.41. The codifferential satisfies ⟨∂kα, β⟩= ⟨α, dk−1β⟩for all β, and so, in particular, is a well
defined map.
We took Ωk,1(M) as the domain for ∂k because we needed [∇hi, ∇hj]♭to be a well defined form. We can, if
needed, extend this domain by the adjoint relationship with dk−1.
Definition 3.42. The domain of the codifferential Ωk
∂(M) is defined as the set of all α ∈Ωk(M) for which
there exists some η ∈Ωk−1(M) satisfying
⟨η, β⟩= ⟨α, dk−1β⟩
for all β ∈Ωk−1(M). This η is then uniquely determined and is called ∂kα. We equip Ωk
∂(M) with the Sobolev
inner product
⟨α, β⟩∂= ⟨α, β⟩+ ⟨∂α, ∂β⟩
and let L2Ωk
∂(M) be the completion of Ωk
∂(M) with respect to ∥· ∥∂.
4See equation 4.
16

We can, as usual, also define a ‘weak’ codifferential ∂kα : X(M)k−1 →A for all α ∈Ωk(M), using the weak Lie
bracket
[∇hi, ∇hj]♭(X) = Γ(hi, X(hj)) −Γ(hj, X(hi)).
3.5
Differential operators, cohomology and the Hodge Laplacian
We can use the standard language of differential operators from commutative algebra to describe operators in
diffusion geometry.
Definition 3.43. A linear operator X : A →A is
• a first-order differential operator if its carr´e du champ
ΓX(f, h) = 1
2
 X(fh) −fX(h) −hX(f)

vanishes (i.e. it is a derivation), and
• a nth-order differential operator if its carr´e du champ is a differential operator of order (n −1) in
both f and h.
Vector fields are first-order differential operators. Notice that the Markov generator L is a second-order op-
erator precisely when the carr´e du champ Γ satisfies the diffusion property (Definition 2.9).
The Hessian
f 7→H(f)(X, Y ) is also a second-order operator because we can show
ΓH(f, h)(X, Y ) = 1
2
 X(f)Y (h) + X(h)Y (f)

,
for all X and Y , which satisfies the Leibniz rule in both f and h. We can also define differential algebra for the
Markov triple M.
Definition 3.44. We call Ω•(M) = L
k Ωk(M) the algebra of differential forms. The facts that d2 = 0
and d satisfies the Leibniz rule (Proposition 3.39) mean that (Ω•(M), ∧, d) is a commutative differential graded
algebra. We can interpret Ω•(M) as a cochain complex
Ω0(M)
Ω1(M)
· · ·
Ωk−1(M)
Ωk(M)
Ωk+1(M)
· · ·
d
d
d
d
d
d
and define the de Rham cohomology groups
Hk(M) := ker(dk)
im(dk−1).
The wedge product ∧descends to a well defined cup product on Hk(M) by the Leibniz rule, which we
denote ∪, and makes (H•(M), ∪) into a commutative graded algebra. The algebra structure makes (H•(M), ∪)
a strictly stronger piece of data than H•(M) alone: see Figure 10 for an example of two spaces that share the
same homology but have different cohomology, so can be distinguished by the cup product.
A fundamental link between the geometry and topology of a manifold is established by the Hodge theorem,
which uses the Hodge Laplacian. Before defining this, we need to introduce one more space of forms.
Definition 3.45. We define the space of coefficient-differentiable k-forms
Ωk,1
+ (M) =
nX
fidhi
1 ∧· · · ∧dhi
k ∈Ωk(M) : fi, hi
j ∈W 2,∞(M) for all i, j
o
.
Recall that forms in Ωk,1(M) do not also require the coefficient functions fi to be in W 2,∞(M). This additional
assumption in Ωk,1
+ (M) means we can define the following.
Definition 3.46. The Hodge Laplacian is the linear map ∆k : Ωk,1
+ (M) →Ωk(M) defined by
∆k := dk−1∂k + ∂k+1dk.
This definition makes sense for α ∈Ωk,1
+ (M), because Ωk,1
+ (M) ⊆Ωk,1(M), so ∂α is well defined, and dα ∈
Ωk+1,1(M), so ∂dα is also well defined. As with the codifferential, the domain of ∆k may actually be slightly
larger, and so we make the following definition.
17

Definition 3.47. The domain of the Hodge Laplacian Ωk
∆(M) is defined as the set of all α ∈Ωk
∂(M) for
which there exists some η ∈Ωk−1(M) satisfying
⟨η, β⟩= ⟨dα, dβ⟩+ ⟨d∂α, β⟩
for all β ∈Ωk−1(M). This η is then uniquely determined and is called ∆kα. We equip Ωk
∆(M) with the Sobolev
inner product
⟨α, β⟩∆= ⟨α, β⟩+ ⟨dα, dβ⟩+ ⟨∂α, ∂β⟩
and let L2Ωk
∆(M) be the ∥· ∥∆-completion of Ωk
∆(M).
For clarity, the hierarchy of spaces we have now defined is
Ωk,1
+ (M) ⊆Ωk
∆(M) ⊆Ωk
∂(M) ⊆Ωk(M).
The Hodge theorem states that, if M is a manifold (or weighted manifold), then Hk(M) ∼= ker ∆k. It may
be the case that Hk(M) ∼= ker ∆k on more general Markov triples than manifolds, but we do not address
that question here (see [23] for a discussion of Hodge theory on L2 spaces of forms on RCD(k, ∞) spaces).
Nonetheless, this result will motivate our practical approach to measuring the ‘shape’ of a dataset with the
spectrum of the Hodge Laplacian (see Figure 7).
(a) 0.09
(b) 0.44
(c) 0.60
(d) 0.67
(e) 1.63
(f) 2.33
Figure 7: The spectrum of the 1-Hodge Laplacian.
Eigenforms
are labelled with their eigenvalues (a - f). The first eigenvalue is very
close to zero (0.09) and, up to the influence of noise and artefacts of
the discretisation, represents the large ‘hole’ in the data as an element
of its H1 cohomology. The eigenform’s dual vector field (a) represents a
rotating flow around that hole. The second eigenvalue is bigger (0.44),
and measures a much less prominent hole at the top as indicated by its
corresponding eigenform (b).
3.6
Third order calculus: second covariant derivative and curvature
We are also interested in the curvature of functions, tensors, and spaces, and this analysis requires the iterated
covariant derivative. To simplify the exposition, and to work in further generality, we consider the covariant
18

derivative ∇Xξ where ξ ∈F and F may stand for A, X1(M), or Ωk,1(M) (or other tensors). We can then
regard ∇•ξ : X(M) →F as an F-valued 1-form, i.e. an element of Ω1(M) ⊗F. To avoid confusion with the
gradient ∇: A →X(M), we will always write ∇• to mean the covariant derivative in this sense. So
∇• : F →Ω1(M) ⊗F.
We have already introduced two examples of this ∇•:
1. F = X1(M), where ∇• is the (1,1) tensor of Definition 3.27, and
2. F = A, where ∇Xf = X(f) = df(X), so ∇•f = df and ∇• = d : A →Ω1(M).
Just like the extension of ∇• from vector fields to 1-forms, we will extend ∇• to Ω1(M) ⊗F by preserving the
Leibniz rule, so we expect
∇•(α ⊗ξ) = ∇•(α) ⊗ξ + α ⊗∇•(ξ).
For example, for this to hold would mean that the second covariant derivative ∇2
X,Y must satisfy
∇X(∇Y ξ) = ∇∇XY ξ + ∇2
X,Y ξ,
although this requires vector fields with even more regularity than X1(M).
Definition 3.48. We define the space of twice-differentiable vector fields
X2(M) := {X ∈X1(M) : ∇X ∈Ω1(M) ⊗Ω1,1(M)},
or, equivalently, X ∈X2(M) if ∇Y X ∈X1(M) for all Y ∈X(M).
These vector fields are ‘twice-differentiable’ because we can take their covariant derivative twice. If Z ∈X2(M)
then ∇XZ ∈X1(M) for all X ∈X(M), so ∇Y ∇XZ ∈X(M) is well defined for all Y ∈X(M). We also define
the corresponding spaces of twice-differentiable k-forms.
Definition 3.49. We define the space of twice-differentiable k-forms as
Ωk,2(M) := {α ∈Ωk,1(M) : ∇Y α ∈Ωk,1(M) for all Y ∈X(M)}.
In particular, Ω1,2(M) = X2(M)♭.
As before, we can take the covariant derivative twice: if α ∈Ωk,2(M), then ∇Xα ∈Ωk,1(M) and ∇Y ∇Xα ∈
Ωk(M). We can now define the second covariant derivative on A, X2(M), and Ωk,2(M).
Definition 3.50. If X ∈X(M), Y ∈X1(M), and F = A, X2(M), or Ωk,2(M), the second covariant
derivative on F is given by ∇2
X,Y := ∇X∇Y −∇∇XY . So if
1. F = A, then ∇2
X,Y : A →A,
2. F = X2(M), then ∇2
X,Y : X2(M) →X(M), and
3. F = Ωk,2(M), then ∇2
X,Y : Ωk,2(M) →Ωk(M).
Note that we need Y ∈X1(M) to ensure that ∇XY ∈X(M). Also, we can see that the regularity issues that
led us to define X1(M) and X2(M) do not arise for functions in A, because
∇2
X,Y (f) = XY (f) −∇XY (f)
is defined for all f ∈A. In the proof of Proposition 3.31 we show that that
XY (f) = ∇XY (f) + H(f)(X, Y ),
so ∇2
X,Y (f) = H(f)(X, Y ). Although we only technically proved Proposition 3.31 in the case that X, Y ∈
X1(M), the same argument applies to the ‘weak’ formulations of operators, with which we are working here.
So in fact ∇2
X,Y (f) = H(f)(X, Y ) for all X, Y ∈X(M).
Finally, we can use the covariant derivative to introduce the Riemann curvature operator.
19

Definition 3.51. The Riemann curvature operator is the map R : X1(M) × X1(M) × X2(M) →X(M)
given by
R(X, Y )Z := ∇X∇Y Z −∇Y ∇XZ −∇[X,Y ]Z.
We needed X, Y ∈X1(M) to ensure [X, Y ] ∈X(M), and Z ∈X2(M) to ensure ∇X∇Y Z ∈X(M).
The
torsion-free property of ∇X means that R(X, Y ) = ∇2
X,Y −∇2
Y,X : X2(M) →X(M).
4
Computing Diffusion Geometry from Data
We can separate the question of computing diffusion geometry into two parts: how to compute the infinitesimal
generator (i.e. the Laplacian), and how to compute the rest of the geometry given that operator.
4.1
Computing the infinitesimal generator
Estimating the Laplacian of a manifold from a point cloud of samples from it is a very well-developed topic. It
has been a central question in manifold learning because the eigenfunctions of the Laplacian are good candidates
for coordinate functions for dimensionality reduction. The approach we favour here, known as diffusion maps
[19], exploits the relationship between the Laplacian and the heat kernel laid out in Section 2 and particularly
equation 3. They show that, if M is a manifold isometrically embedded in Rd, and pt(x, y) is the heat kernel
in the ambient space (equation 2), then pt approximates the heat kernel in M. So if
Ptf(x) =
Z
M
pt(x, y)f(y)dy,
then
lim
t→0
f −Ptf
t
= ∆f
pointwise (where ∆is the Laplacian on M). Given a finite sample (xi)i≤n from M, then by the law of large
numbers we can approximate
Ptf(xi) ≈1
n
n
X
j=1
pt(xi, xj)f(xj),
like a Monte Carlo integration. So, if the function f is represented by a vector of length n (encoding its value
on the n points), then multiplication by the heat kernel matrix (pt(xi, xj))ij represents the action of the heat
diffusion operator. Diagonalising this matrix then yields estimators for the eigenfunctions of the Laplacian.
When M has a density q (i.e. it is a weighted manifold), the authors of [19] show that the heat diffusion
operator
Ptf(x) =
Z
M
pt(x, y)f(y)q(y)dy,
is generated by
lim
t→0
f −Ptf
t
= ∆(fq)
q
−∆(q)
q
f
(5)
which is equivalent to the weighted Laplacian in Example 2.2. This is relevant when the data are sampled
non-uniformly from M, with probability mass function q, because
E
 1
n
X
j
pt(xi, xj)f(xj)

=
Z
M
pt(xi, y)f(y)q(y)dy,
so the heat kernel matrix will instead yield eigenfunctions of this q-weighted operator (5). They also introduce
a simple 1-parameter renormalisation of the heat kernel, which controls the role of this density q. We may use
this in practice, but refer to [19] for the details.
In the following, we assume that our data are drawn from some smooth distribution on Rd, or on an embedded
submanifold of Rd, and so the diffusion maps algorithm will estimate the infinitesimal generator (5) (where the
Laplacian ∆in (5) is either that of Rd or the submanifold). However, there are many other pertinent approaches,
such as the combinatorial Laplacian on a graph or simplicial complex, or the generator of some correlation matrix
between the data. Working with the graph Laplacian would lead to an alternative construction of graph theory,
which we will explore in future work.
20

4.2
Computing diffusion geometry with the generator
Now suppose we have an infinitesimal generator L, which acts on an n-dimensional function algebra A. We will
usually suppose that A ∼= Rn represents functions on n points or vertices, and so the basis vector ei represents
a function being 1 on the ith vertex and 0 on the others. A natural way to represent forms is suggested by the
definition, which we recall (for 1-forms) to be
Ω1(M) = A ⊗A
D1
.
We should
1. choose a basis {fi : i ∈I} for the function algebra A, so {fi ⊗fj : i, j ∈I} gives a basis for A ⊗A,
2. compute the inner product (Gram) matrix G for {fi ⊗fj : i, j ∈I}, and
3. quotient A ⊗A by its kernel (i.e. project onto an orthonormal basis of eigenvectors for G with positive
eigenvalue).
The formulae given in Section 3 can then be used to construct all the other objects in the theory, given the
operator L. For example, we can compute the carr´e du champ
Γ(fi, fj) = 1
2
 fiLfj + fjLfi −L(fifj)

and metric
g(fidfj, fkdfl) = fifkΓ(fj, fl)
in the fi ⊗fj basis for A ⊗A, and then project into the orthonormal basis for Ω1(M). We take the analogous
approach for higher-order forms. All the operators defined above are then matrices and tensors and can be
processed using highly optimised software for multilinear algebra.
4.2.1
Well-posed Galerkin scheme
The above approach is equivalent (up to an orthogonal change of basis) to the following procedure in L2(Ω1(M)):
1. choose a basis {fi : i ∈I} for A, so the span of {bij = fi ⊗fj : i, j ∈I} projects onto a dense subset of
L2(Ω1(M)), and
2. apply the Gram-Schmidt orthonormalisation to the bij, removing linearly dependent elements, to obtain
an orthonormal basis {ck : k ∈N} for L2(Ω1(M)).
In other words, we have
lim
K→∞

K
X
k=1
⟨α, ck⟩ck −α
 = 0
for all α ∈Ω1(M). This means that the representations of forms and operators in the finite truncated basis
{ck : k ≤K} are guaranteed to converge to the correct objects as K →∞. The same approach applies to all
the Ωk(M).
If we want to represent operators on forms with higher regularity than Ωk(M), such as the Hodge Laplacian ∆,
we need to work in the appropriate Sobolev spaces, like Ωk
∆(M). In these cases, we can apply the same method
as above but pick an orthonormal basis for the Sobolev inner product instead, which in Ωk
∆(M) is
⟨α, β⟩∆= ⟨α, β⟩+ ⟨dα, dβ⟩+ ⟨∂α, ∂β⟩.
We can then solve the eigenproblem for ∆in Ωk
∆(M) using the weak formulation: rather than solve ∆α = λα
directly, we solve
⟨dα, dβ⟩+ ⟨∂α, ∂β⟩+ ϵ⟨α, β⟩∆= λ⟨α, β⟩
(6)
for all β, where ϵ is a small regularisation parameter. The bilinear form on the left of this equation is then
coercive, because
⟨dα, dα⟩+ ⟨∂α, ∂α⟩+ ϵ⟨α, α⟩∆≥ϵ∥α∥2
∆
and bounded, because
⟨dα, dα⟩+ ⟨∂α, ∂α⟩+ ϵ⟨α, α⟩∆= (1 + ϵ)∥α∥2
∆−∥α∥2
≤(1 + ϵ)∥α∥2
∆
21

and the form defines an inner product so Cauchy-Schwarz implies
⟨dα, dβ⟩+ ⟨∂α, ∂β⟩+ ϵ⟨α, β⟩∆≤
 ⟨dα, dα⟩+ ⟨∂α, ∂α⟩+ ϵ⟨α, α⟩∆
 1
2  ⟨dβ, dβ⟩+ ⟨∂β, ∂β⟩+ ϵ⟨β, β⟩∆
 1
2
≤(1 + ϵ)∥α∥∆∥β∥∆.
These two properties mean that the Galerkin equation 6 is well-posed and has a unique solution in L2(Ωk
∆(M)).
The finite truncated basis {ck : k ≤K} is orthonormal, so the solution we obtain for each K will converge to
the true solution as K →∞. This formulation is fully symmetric, so is more efficient to compute, and there
are nice formulas for the terms ⟨dα, dβ⟩(see Section 10.10).
We can solve equations involving other operators, like the Hessian or codifferential, by defining the corresponding
Sobolev inner products on their domains, picking an orthonormal basis for that inner product, and solving the
equations in the truncated basis.
4.2.2
Choosing the basis
The steps above give a general computational framework for diffusion geometry, although working with higher-
order forms and tensors may become computationally expensive as the dimension of the function space, n,
increases. Usually (and in the diffusion maps setting) n is the number of data, so this computational complexity
will eventually become prohibitive. We can, instead, project the functions into a smaller space and reduce the
cost.
If L is an n × n matrix, we can diagonalise L and use the span of its first n0 eigenfunctions as a compressed
representation of A. This is equivalent to the band-limited Fourier space of functions when L is the Laplacian
on a manifold. This space is a natural choice for compression, because, among all sets of n0 functions in A, the
first n0 eigenfunctions of L have the lowest Dirichlet energy as defined by the energy functional
E(f) :=
Z
fL(f)dµ =
Z
Γ(f, f)dµ.
Functions with low Dirichlet energy are smoother and less oscillatory, so we can capture the finer details of
functions by increasing n0.
We apply the same principle to representing higher-order forms. For example, the 2-forms are spanned by the
terms ϕidϕj ∧dϕk, so if we use all n functions ϕi then the 2-forms are n3-dimensional. Instead, we limit i ≤n1
and j, k ≤n2 for some n1, n2 ≤n. These approximate 2-forms are then (at most) n1n2
2-dimensional (and in
general the k-forms are n1nk
2-dimensional, which lets us explicitly trade off computational complexity against
precision. We test the consequences of this in Section 6.
Working with these eigenfunctions also leads to some simplification in the formulae. Let ϕi be the ith eigen-
function of L, with eigenvalue λi, and define
cijk =
Z
ϕiϕjϕkdµ,
so that ϕiϕj = P
k cijkϕk. Then, for example, we can simplify
Γ(ϕi, ϕj) = 1
2
X
k
(λi + λj −λk)cijkϕk.
Formulae for computing all the other objects in Section 3 follow straight from their definitions, and are deferred
to Appendix B (section 10). The Python code is available at github.com/Iolo-Jones/DiffusionGeometry.
This approach to representing and computing forms as tensor products of eigenfunctions was developed for
Riemannian manifolds in [12] as ‘spectral exterior calculus’ (SEC). The authors used the fact that simplifications
like the above exist to derive formulae for computing the Hodge Laplacian on 1-forms. In the special case of
Riemannian manifolds, the computation of diffusion geometry can be seen as an extension of the SEC programme
to other objects in Riemannian geometry, as well as higher-order forms.
These Laplacian eigenfunctions are, in the above sense, the most natural choice for representing objects in
diffusion geometry but computing them has O(n3) complexity (from diagonalising an n × n matrix), which
may still be prohibitively expensive in some cases. We could choose other bases to allow faster approximate
computation of diffusion geometry, but this will be explored in future work.
22

5
Computational Geometry and Topology, Machine Learning, and
Statistics
Diffusion geometry provides a very general framework for explainable geometric machine learning and statistics,
where we can interpret all the objects defined above as representing geometric features of the data. It also
provides a broad framework for computational geometry and topology, where the data are assumed to lie on a
manifold and we want to compute various properties.
In this section, we give some simple demonstrations of diffusion geometry as a tool for
1. computational geometry, as a measure for identifying intersections and tangent vectors in manifold-like
data,
2. computational topology, for computing the cohomology of manifolds,
3. unsupervised learning, as a biomarker for the infiltration of immune cells into a tumour, and
4. supervised learning, as a feature vector for classifying different types of immune cells in tumours by their
spatial distribution.
5.1
Computational geometry and topology
The assumption that data are drawn from a manifold is called the ‘manifold hypothesis’. In this setting, diffusion
geometry agrees with the usual Riemannian geometry so can be used to estimate properties of that manifold
from a finite sample. When the data are drawn from a space that looks almost everywhere like a manifold, we
can still compute the same objects and use these to test the manifold hypothesis.
(a) Two circles and a curve intersecting
(b) Two circles and a curve intersecting (noisy)
(c) Two spheres and a circle intersecting
(d) Two spheres and a circle intersecting (noisy)
Figure 8: Testing the manifold hypothesis in 2D (a, b) and 3D
(c, d). The first eigenvalue of the metric measures the degeneracy of the
tangent space. It is large (grey) where the data looks like a manifold, and
dips lower (rose) at the singularities where the tangent space degenerates.
This measure is very robust to noise (b, d).
23

5.1.1
Geometry of manifolds and cell complexes
On a Riemannian manifold M of dimension d, each point has a tangent space TxM, and the Riemannian metric
is an inner product TxM × TxM →R. The tangent space TxM is d-dimensional, and so the metric has exactly
d positive eigenvalues, whose eigenvectors form an orthonormal basis for TxM. Given a sample of data from
M we can compute the metric at x (i.e. a square symmetric matrix at each point x) and find these eigenvalues.
The kth largest eigenvalue is, therefore, a proxy statistic for ‘at least k-dimensionality at x’.
Suppose now that the data are drawn from a space that is not quite a manifold (such as the intersection of
manifolds or a cell complex). The tangent space is well defined everywhere that the space looks like a manifold,
but degenerates at the points where the manifold hypothesis fails. We can measure this local failure with the
largest eigenvalue of the metric, which should be positive where the manifold hypothesis holds, but dip down
towards zero at the degenerate points. We evaluate this in Figure 8. The results are comparable to existing
approaches for singularity detection (such as [29, 34, 39]), but are significantly more robust to noise and outliers.
Where the manifold hypothesis holds, i.e. the largest eigenvalue of the metric is positive, we can also expect
its eigenvectors to span the tangent space. We test this in Figure 9. Unlike the commonly used local principal
component analysis [28], this approach is very robust to noise and outliers, and may yet perform better with
further smoothing processes, which we will explore in future work.
(a)
(b)
(c)
(d)
Figure 9: Fitting tangent lines to data. The first eigenvector of the
metric represents the tangent space on a one-dimensional manifold (a).
Where the manifold hypothesis fails (b), the tangent vectors try to align
at the intersections. This process is very robust to noise (c, d).
5.1.2
Topology
Homology and cohomology are important topological properties of a space, which can be used to tell spaces
apart when they are topologically distinct. The dimensions of the homology (or cohomology) groups Hi are
24

called the Betti numbers βi of the space, and measure the number of topological features in each dimension.
For example, β0 is the number of connected components, β1 is the number of holes or loops in the space, and β2
is the number of enclosed voids. As discussed in Section 3.5, we can compute the cohomology of a manifold by
finding differential forms in the kernel of the Hodge Laplacian, which are known as harmonic forms (by analogy
with harmonic functions in the kernel of the Laplacian). Unlike homology, cohomology also has a product
structure (i.e. it is a ring), so we can multiply elements of cohomology together. This makes cohomology a
strictly finer invariant than homology: two spaces may share the same Betti numbers (so their homology and
cohomology groups are the same size), but the products of elements in the cohomology groups may differ. They
are indistinguishable by homology, but cohomology can tell them apart.
(a) α1, ∥α1∥= 1
(b) β1, ∥β1∥= 1
(c) α2, ∥α2∥= 1
(d) β2, ∥β2∥= 1
(e) α1 ∧α2, ∥α1 ∧α2∥= 0.112
(f) β1 ∧β2, ∥β1 ∧β2∥= 0.003
Figure 10: The wedge product on cohomology. A torus (left column)
and a sphere with two adjoined circles (right column) share the same
Betti numbers (one component, two holes, one void) and so have the
same homology. However, cohomology has a product that can tell the two
spaces apart. Both spaces have two harmonic 1-forms (a, c) and (b, d),
which measure the holes. However, their product is nonzero on the torus
(e) but zero on the sphere-and-circles (f).
25

We can use diffusion geometry as a computational tool for cohomology by finding eigenforms of the Hodge
Laplacian, whose zero (or approximately zero) eigenvalues correspond to harmonic forms. We can then compute
their wedge products (the product on cohomology) to obtain more topological information about the space.
One of the most popular tools for computational geometry and topology is persistent homology ([31, 22]), which
computes the homology (or cohomology) of data across a range of scales and represents it in a persistence
diagram. However, even though persistent cohomology does have a product, it cannot be straightforwardly
represented in the diagram5. By contrast, the wedge product of harmonic forms can be easily described in
diffusion geometry. We test this in Figure 10 on two spaces that have the same Betti numbers, but distinct
cohomology.
5.2
Feature vectors for geometric machine learning and statistics
Machine learning and statistics usually require data in the form of feature vectors that encode the necessary or
interesting information. Given complex geometric data, such as a point cloud, it is a fundamental problem to
produce feature vectors that give a rich geometric description of the data with a large amount of information,
are robust to noise and outliers, are fast to compute, and are explainable, so that the output of the final model
can be interpreted and used to learn more about the data.
A fundamental observation of geometric deep learning [15] is that geometric machine learning models must be
invariant under various group actions, such as translations and rotations of the input data. Diffusion maps,
and hence diffusion geometry, works with the pairwise distances of the data (through the heat kernel), and so is
naturally invariant under all isometries: translation, rotation, and reflection. It can also be made scale-invariant
through an appropriate choice of the heat kernel bandwidth.
However, group invariance alone is not enough.
Geometry is also preserved by adding noise and outliers,
and, perhaps, changing the sampling density of the data. Models should therefore be both group invariant
and statistically invariant.
The heat kernel is highly robust to noise, as we will explore, and the density-
renormalisation of diffusion maps means that diffusion geometry models can be made density-invariant as well.
We can use diffusion geometry to produce feature vectors by
1. computing eigenfunctions ϕi and eigenforms αi of the Laplacian and Hodge Laplacian (which encode rich
geometric and topological information about the space: see Subsection 3.5 and Figure 7),
2. applying different combinations of the geometric operators defined in Section 3 to produce more functions
and forms, e.g. dϕ2, H(ϕ1)(∇α1α2, α3),
3. reducing these functions and forms to single numbers by taking inner products, e.g. ⟨α3, dϕ2⟩, and
4. stacking these numbers into a long vector.
These vectors are explicitly computing (generalisations of) well-understood objects from Riemannian geometry,
so give a rich geometric description and are explainable. In the computational framework described in Section
4 (and tested later in Section 6), they are also fast to compute and inherit very strong noise-robustness from
the heat kernel used in the diffusion maps algorithm. We can then use these vectors for unsupervised learning
(dimensionality reduction or clustering) and supervised learning (as features for regression).
5.2.1
Unsupervised learning: biomarkers for tumour dynamics
We consider the problem of finding biomarkers for processes in the tumour microenvironment to demonstrate
diffusion geometry’s effectiveness for unsupervised representation learning.
Figure 11 shows agent-based model (ABM) data from [38], which simulates the infiltration of immune cells into
the centre of a tumour. Cells start outside the tumour and end up in the middle. The problem the authors of
[38] consider is to find a statistic (called a biomarker in this context) that measures the stage of this process and
lets us track the infiltration of cells over time. The ABMs are initialised with different chemotactic gradients
χ which change the rate of infiltration: when χ = 0 the immune cells move slowly and when χ = 10 they are
roughly twice as fast. A good biomarker will measure the stage of the process while clearly separating the
different chemotaxis parameters. Real data from biological and medical imaging are frequently corrupted with
noise and outliers, and this is modelled by misclassifying some of the background tumour cells as immune cells.
The biomarker must still perform well even under the addition of this kind of noise.
5The product of points in the persistence diagram is, in general, a linear combination of other points.
26

Figure 11: Agent-based model simulation of immune cells (blue)
infiltrating a tumour (yellow). The cells start on the outside edge
(first row) and end in the centre (third row).
We consider data with
different levels of background noise (columns), where increasing numbers
of tumour cells are misclassified as immune cells.
As a benchmark, the authors of [38] consider biomarkers derived from persistent homology. They compute
Vietoris-Rips persistence diagrams for each collection of immune cells at each point in time with Ripser [11].
There are several standard statistics from these diagrams, and [38] uses the longest persistent bar in H1, which
measures the radius of the largest ‘hole’ in the data. We additionally consider the ‘total persistence’ in H1 (the
sum of all the H1 bars), which measures the overall amount of ‘holes’ in the data6.
The results are shown in the first two rows of Figure 12, where each biomarker is plotted against time. Each
ABM simulation is run five times, and we plot the means and standard deviation bars for each chemotaxis
parameter and at each noise level. The longest bar in H1 (first row) shows a clear sigmoid transition curve
which tracks the transition of cells from the outside to the inside of the tumour, although there is no significant
separation between classes, particularly from time 60 onwards. Conversely, total H1 persistence gives better
separation in the latter half, but is generally much harder to interpret. Both persistent homology biomarkers
are significantly distorted by even a small amount of noise and do not yield meaningful results for data with a
noise/signal level over 50%.
We compare these with a diffusion geometry biomarker. The feature vectors described at the start of Section
5.2 encode the geometry of a point cloud in a very high-dimensional space, and we compute them for each
simulation at each point in time. We can reduce these high-dimensional vectors to a single dimension with
principal component analysis (PCA) for use as a biomarker.
The results are shown in the third row of Figure 12. The diffusion geometry biomarker produces a very clear
sigmoid curve as the cells transition from the outside to the inside of the tumour, with very clear chemotaxis
6Trying other standard vectorisations of the persistence diagrams did not yield significantly different pictures than these two.
27

parameter separation. The different curves are also the same shape, except for an elongation that increases
with the chemotaxis parameter, suggesting the correct intuition that the underlying process is the same but
happening at different speeds.
As such, the diffusion geometry biomarker is strong enough to hint at the
underlying biology without prior knowledge of the mechanism. Crucially, it is also extremely robust to noise:
even with 100% noise it outperforms persistent homology with 0% noise in all the above senses.
Figure 12: Biomarkers for immune cell infiltration. Three different
biomarkers (rows) are plotted over time for the four different noise levels
(columns). The colours denote the different chemotaxis parameters. The
persistent homology biomarkers (first two rows) are less descriptive than
diffusion geometry (third row), with less chemotaxis class separation, and
are significantly less robust to noise.
PCA assigns a weight to each of the features in the feature vector, and we can sparsify this weight vector by
setting all but the largest few entries to zero. This produces a PCA feature that depends on only a few geometric
features, which we can then interpret. For example, we find that one of the features here is approximately equal
to
∥dϕ1∥2 + ∥dϕ2∥2 +
h
λ0
1 −e−λ0
1 −e−10λ0
1 −e−50λ0
1
i
+ λ0
2 +
h
λ1
1 −e−λ1
1 −e−10λ1
1
i
where λ0
i is the ith eigenvalue of the Laplacian with eigenfunction ϕi and λ1
i is the ith eigenvalue of the Hodge
Laplacian on 1-forms.
If f is a function then its Dirichlet energy ∥df∥2 measures how variable it is.
The
eigenfunctions ϕ1 and ϕ2 always increase along the length of the data (see, for example, Figure 4 (b)), and
so for ∥dϕ1∥2 + ∥dϕ2∥2 to be small means that the data is smaller and more compact. We can use intuition
from spectral geometry and Hodge theory (e.g. Figure 7) to interpret the Laplacian eigenvalues λ0
1 and λ0
2 as
measuring ‘connectedness’, and the Hodge Laplacian eigenvalue λ1
1 as measuring the prominence of the ‘hole’ in
the middle of the data7. As such, we can interpret the decrease in this diffusion geometry feature as measuring
the data becoming increasingly connected and compact, and decreasingly hollow.
7We include some exponential eigenvalues e−tλ0
i and e−tλ1
i as additional data. These are the eigenvalues of the corresponding
diffusion operators exp(−tL) and exp(−t∆1).
28

5.2.2
Supervised learning: classifying immune cell types
We also test diffusion geometry feature vectors as a representation for supervised learning. In the same paper
[38], the authors consider real histology images of slices of head and neck tumours. They use a semiautomated
procedure [16] to identify the locations of three different types of immune cells: CD8, CD68, and FoxP3, and use
persistent homology methods to classify the different cell types based on their spatial distribution. Automatic
cell identification is always vulnerable to the sort of misclassification noise described above, and so the authors
use multiparameter persistent homology (MPH) landscapes [37] to mitigate the poor robustness of 1-parameter
persistent homology to noise and outliers.
The MPH landscapes are used as feature vectors for linear discriminant analysis. Conversely, we use the diffusion
geometry features described above with logistic regression as a classifier and obtain similar or better results
across all categories.
MPH landscapes
Diffusion geometry
CD8 vs FoxP3
74.7%
88.2 ± 5.9%
CD8 vs CD68
65.3%
62.4 ± 8.2%
FoxP3 vs CD68
86.3%
90.1 ± 5.4%
5.2.3
Other types of machine learning problem
Diffusion geometry gives a unified framework for ‘strongly typed’ machine learning problems on geometric data.
The features we compute are naturally graded into data in the form of
(-1) numbers, for summaries of the whole point cloud,
(0) functions, for segmentation-type problems,
(1) vector fields/ 1-forms, for dynamical system and time series problems,
(k) k-forms for other analysis.
Many of the operators defined in Section 3, such as the differential and codifferential, and interior and wedge
products, give the tools for moving things up and down this hierarchy. This is important because different
geometric machine learning problems are also located in this hierarchy: point cloud classification requires
summary features, segmentation requires functional features, and dynamical systems are vector fields and
require vector field and 1-form features. In the feature vectors described above we just consider the first of these
types, computing inner products between all the functions and forms to collapse all the data to the bottom
level. However, given a higher-order problem, we can select higher-order features and use them instead, as we
will explore in future work.
6
Computational Complexity
The computational framework outlined in Section 4 gives us explicit control of the computational complexity
(subsection 4.2.2). The only unavoidable cost is diagonalising the diffusion maps Laplacian, which has O(n3)
complexity (where n is the number of data). After that, the functions and higher-order k-forms are represented
by spaces of dimension n0 and n1nk
2 respectively, where n0, n1, n2 ≤n. Crucially, we can vary n1 and n2 to
explicitly trade off computational complexity against precision.
To test the consequences of this, we compare the computation of H1 Vietoris-Rips persistent homology (PH)
with diagonalising the Hodge Laplacian on 1-forms (the equivalent problem in diffusion geometry). Computing
H1 persistent homology has O(n6) complexity, and diagonalising the Hodge Laplacian, with fixed n0 = 35,
n1 = 10, n2 = 4, has just O(n3) complexity. We compute PH with Ripser [11], which is very highly optimised.
We compute diffusion geometry with a naive implementation in NumPy [27], which is not yet optimised. Data
are sampled from a torus in R3 (the same as in Figure 10), and the computation is on a standard 2020 M1
Macbook Pro. We plot the mean and standard deviation bars from 20 runs on a logarithmic scale in Figure
13. Computing eigenforms of the 1-Hodge Laplacian in diffusion geometry for 12,000 points takes 6.8 seconds,
while the equivalent computation of PH in Ripser would take over two hours.
29

Figure 13: Comutational complexity of diffusion geometry and
persistent homology. Different numbers of data are sampled from a
torus. We compute H1 persistent homology with Ripser and diagonalise
the Hodge Laplacian on 1-forms (the equivalent problem in diffusion ge-
ometry).
We test each size 20 times and plot the mean and standard
deviation bars on a logarithmic scale. Computing diffusion geometry for
12,000 points takes 6.8 seconds, while the equivalent computation of PH
would take over two hours.
7
Comparison with Related Work
7.1
Theory
The Bakry-Emery Γ-calculus discussed here was originally developed in [8] to measure the subtle relationship
between the geometry and the probability of diffusion operators. Dirichlet forms like Γ and the Γ2 operator
(which we do not discuss here) give explicit control of curvature and dimension and have a powerful regularising
effect on diffusion, leading to a family of functional inequalities ([9, 4]). In a similar spirit, the existing work on
Γ-calculus explores the ‘geometry’ of Markov diffusion operators as a tool for understanding their probabilistic
aspects (e.g. [35, 5]). The ‘nonsmooth differential geometry’ programme of Gigli, Ambrosio, Savar´e and others
([2, 23, 24, 1, 3]) is spiritually similar to this and explicitly constructs gradients on metric measure spaces using
Sobolev weak derivatives. The approach to ‘Hodge theory on metric spaces’ in [10] gives another definition of
exterior calculus on metric spaces, but is more topological and is akin to simplicial cohomology.
In this work, we have used the Γ-calculus to define geometry in the most general setting, and our theory agrees
with Riemannian geometry, nonsmooth differential geometry, and the constructions in Γ-calculus, where they
apply. Crucially, by working exclusively with Γ-calculus we obtain a natural computational model for the whole
theory.
7.2
Computation
Our estimation of the Laplacian from data uses diffusion maps [19], one of many such methods in the richly
developed field of manifold learning, and we may explore others in the future.
There are several existing approaches for computing objects from differential geometry.
Many assume the
manifold hypothesis (that the data are drawn from a manifold), and so can use methods like local principal
component analysis (PCA) [28] that depend heavily on that assumption. This allows the computation of, for
30

example, the Hessian [21], the connection Laplacian [33], and curvature [40], but these methods break down
when local PCA does (when the manifold hypothesis fails or there is too much noise). ‘Finite-element exterior
calculus’ uses an explicit discretisation of a manifold and gives an overarching theory for computation ([7, 6])
in this setting. A significant influence on this work was Berry and Giannakis’ ‘spectral exterior calculus’ [12]
which derived formulae for the Hodge Laplacian of 1-forms on a manifold in terms of the eigenfunctions of the
Laplacian. We use the same eigenfunction expressions as a computational framework for diffusion geometry.
There are fewer computational geometry frameworks that do not assume the manifold hypothesis, and they
generally use simplicial complexes instead, with simplicial cochains playing the role of differential forms. An
overarching theory is given by ‘discrete exterior calculus’ [20], and includes many of the objects we define
here.
However, explicitly constructing a simplicial complex from a sample of data (such as by connecting
nearest neighbours) is problematic, and generally leads to poor noise-robustness. This is partially addressed by
persistent homology (see below) but at the expense of much of the geometry.
Diffusion geometry computes objects from Riemannian geometry, so is a rich geometric measure of the data,
but without assuming the manifold hypothesis, so is naturally adapted to the sorts of data encountered in real
problems.
7.3
Data analysis
Perhaps the most popular tool for topological data analysis is persistent homology (PH) ([31, 22, 41]), which
tracks the changes in homology on a filtered simplicial complex. The most popular filtrations use a varying
radius around each data point to connect them up, and so PH measures the change in homology across scales.
However, these radius filtrations are not robust to noise (e.g. Figure 12, [36]), and are not statistical estimators
of well defined geometric properties of the underlying probability space8. An alternative to radius filtration is
the sublevel set filtration of the underlying probability density function, which can be estimated with a kernel
([14]). These are generally harder to compute ([32]) and are less widely applied because they do not measure
scale. Radius and density can be combined in a bifiltration, which results in multiparameter persistent homology
(MPH) [18]. However, computing MPH is very computationally expensive ([30, 17]) and it has no complete
‘discrete invariants’ (like a barcode for 1-parameter PH), and so is hard to quantify statistically ([26, 13]).
Diffusion geometry is highly robust to noise (Figures 12, 8, and 9) and fast to compute (Figure 13).
8
Conclusions
This work introduces diffusion geometry as a framework for geometric and topological data analysis.
We constructed a theory of Riemannian geometry on measure spaces using the Bakry-Emery Γ-calculus, which
leads to a natural model for computation. We compute the diffusion geometry of point cloud data with the
diffusion maps Laplacian, leading to computationally inexpensive and highly robust estimators for the geometry
of the underlying probability distribution.
We view diffusion geometry as a statistical tool for geometric and topological data analysis, and computational
geometry and topology.
We find that it outperforms existing methods in a handful of real and synthetic
examples. Diffusion geometry has broad potential for future development.
Acknowledgements
I am extremely grateful to my supervisor Jeff Giansiracusa for pointing me in the direction that eventually
became this project, and his continual support throughout. I would also like to thank my supervisor Yue Ren,
along with Fernando Galaz Garcia, Andrew Krause, David Lanners, Jerry Swan, Kelly Maggs, and Thea Stevens
for their generous comments, suggestions, and advice, and Josh Bull for his permission to reproduce the ABM
simulations in Section 5. This work was carried out as part of the Centre for TDA, supported by EPSRC grant
EP/R018472/1.
8In the large-data limit, PH of radius filtrations converges to the PH of the support of the distribution, which is density-
independent.
31

9
Appendix A: Proofs
We now justify the definitions offered in Section 3, and prove the results stated there. In the following, we will
make use of the following standard properties of Markov diffusion triples, which we reference from [9]:
1. Positivity (1.4.2): Γ(f, f) ≥0, which implies
2. Cauchy-Schwarz inequality (1.4.3): Γ(f, h)2 ≤Γ(f, f)Γ(h, h).
3. General diffusion property (3.1.2): for all f1, ..., fk ∈A and a smooth function ϕ : Rk →R which vanishes
at 0, ϕ(f1, ..., fk) ∈A and
Γ(ϕ(f1, ..., fk), h) =
k
X
i=1
∂iϕ(f1, ..., fk)Γ(fi, h)
for all h ∈A.
4. Gradient bound (3.1.5): for each f ∈A there is a finite constant C(f) such that

Z
Γ(f, h)dµ
 ≤C(f)∥h∥2
for all g ∈A.
The general diffusion property implies the standard one by setting ϕ(f1, f2) = f1f2.
The gradient bound
condition means that Γ uniquely specifies an infinitesimal generator L by the integration-by-parts formula
Z
hL(f)dµ =
Z
Γ(f, h)dµ.
9.1
Differential forms
We first justify the definition of differential forms: the inner product is positive semi-definite on A ⊗Vk A and
so descends to a positive definite inner product when we take the quotient by its kernel.
Proposition 3.4. The metric g on Ωk(M) is symmetric, bilinear, and positive semi-definite. In particular, it
satisfies the Cauchy-Schwarz inequality pointwise, and the inner product ⟨·, ·⟩and metric g on Ωk(M) are well
defined.
Proof. It is straightforward to check that g is symmetric and bilinear. To show that it is also positive semi-
definite, we will take some α ∈A ⊗Vk A and evaluate g(α, α)(x) at some particular point x. Since elements of
the tensor product A ⊗Vk A are finite linear combinations of the irreducible elements, we can write
α =
N
X
i=1
f i
0df i
1 ∧· · · ∧df i
k
for some finite N depending on α. To avoid any infinite sums and the question of convergence, we define Aα to
be the (at most) N(k + 1)-dimensional subspace of A spanned by
{f i
j : i = 1, ..., N, j = 0, ..., k}.
The carre du champ Γ(·, ·)(x) evaluated at x defines a symmetric, positive semi-definite, bilinear form on Aα,
so let us take a basis for the kernel of Γ(·, ·)(x), and extend it to a basis for the rest of Aα. We can then apply
the Gram-Schmidt orthonormalisation to the positive definite basis elements to produce a basis ei for Aα where
Γ(ei, ej)(x) =
(
0
Γ(ei, ei)(x) = 0 or Γ(ej, ej)(x) = 0
δij
otherwise.
The Γ(ei, ej)(x) = 0 case follows from the Cauchy-Schwarz inequality for Γ. So, after a change of basis, we can
express α as
α =
N(k+1)
X
i=1
hidei
1 ∧· · · ∧dei
k
32

where each ei
l is an element of the basis we just constructed, and hi is a function in Aα. Using the fact that
df i
1 ∧· · · ∧df i
k is an element of the exterior algebra Vk A, we can ensure that the ordering of basis elements ei
l
by l respects the order of the basis. By keeping hi as an arbitrary function, and not an element of the basis, we
can also assume without loss of generality that
(ei
1, ..., ei
k) ̸= (ej
1, ..., ej
k)
for all i ̸= j (since otherwise we can combine their coefficient functions hi). We can now write
g(α, α)(x) =
X
i,j
hi(x)hj(x) det
 Γ(ei
l, ej
m)(x)

l,m

.
To simplify the sum, we first remove the terms which contain a basis function in the kernel of Γ(·, ·)(x). If the
index set for i is I = {1, ..., N(k + 1)}, we can split up I = I0 ⊔I+ where
I0 = {i ∈I : Γ(ei
l, ei
l)(x) = 0 for some l = 1, ..., k}
are the indices of those forms containing a function in the kernel of Γ(·, ·)(x) and
I+ = {i ∈I : Γ(ei
l, ei
l)(x) > 0 for all l = 1, ..., k}
are the indices of those forms entirely comprising functions on which Γ(·, ·)(x) is positive. If i ∈I0 (or j ∈I0),
then
Γ(ei
l, ej
m)(x)2 ≤Γ(ei
l, ei
l)(x)Γ(ej
m, ej
m)(x) = 0
for some l (or m) and all m (or l), and so the matrix
 Γ(ei
l, ej
m)(x)

l,m must contain a row (or column) of zeros,
and so has zero determinant. This means that
det
 Γ(ei
l, ej
m)(x)

l,m

=
(
0
i ∈I0 or j ∈I0
δij
i, j ∈I+
and so we can simplify
g(α, α)(x) =
X
i∈I+
hi(x)2 ≥0.
This holds for all x, so g(α, α) ≥0 for all α ∈A ⊗Vk A, which implies the Cauchy-Schwarz inequality for g.
It then follows that ⟨·, ·⟩=
R
g(·, ·)dµ is a symmetric, positive semi-definite, bilinear form on A ⊗Vk A, and so
descends to a positive definite inner product on Ωk(M). We also see that g descends to a well defined map on
the quotient, since ∥α∥= 0 if and only if g(α, α) = 0 almost everywhere, and so
g(α, β)2 ≤g(α, α)g(β, β) = 0
for all β ∈Ωk(M).
■
9.2
Wedge product and exterior derivative
We verify that the wedge product is well defined and that the usual calculus rules hold for d0.
Proposition 3.6. If α ∈Ωk(M) and β ∈Ωl(M) then g(α ∧β, α ∧β) ≤g(α, α)g(β, β). In particular, the wedge
product is well defined on Ωk(M) × Ωl(M), and is a bounded linear operator in each argument.
Proof. We will work with the same machinery as in the proof of Proposition 3.4. This time we consider two
forms α and β, so let A = Aα ∪Aβ be the finite subspace of A spanned by the functions that feature in the
expansion of α and β. As before, let {ei} be a basis for A which is orthonormal on the positive definite subspace
of Γ(·, ·)(x). We can then expand
α =
X
i
f idei
1 ∧· · · ∧dei
k
and
β =
X
j
hjdej
1 ∧· · · ∧dej
l
where each ei
p and ej
p is an element of the basis, the ordering of basis elements ei
p and ej
p by p respects the order
of the basis, and
(ei
1, ..., ei
k) ̸= (ei′
1 , ..., ei′
k )
(ej
1, ..., ej
l ) ̸= (ej′
1 , ..., ej′
l )
33

for all i ̸= i′ and j ̸= j′. We can then evaluate g(α ∧β, α ∧β)(x) as
X
i,i′,j,j′
f i(x)f i′(x)hj(x)hj′(x) det
 Γ((ei ∧ej)p, (ei′ ∧ej′)q)(x)

p,q

,
where by (ei ∧ej)p we mean the pth element of (ei
1, ..., ei
k, ej
1, ..., ej
l ). As before, we simplify this sum by removing
the zero terms, which can now appear in two ways. If the index sets for i and j are I and J, we decompose
I = I0 ⊔I+ and J = J0 ⊔J+ just like in Proposition 3.4. Now suppose that i, j, i′, j′ ∈I+, and neither ei and
ej nor ei′ and ej′ share a common basis function. Let π and π′ be the permutations of ei ∧ej and ei′ ∧ej′ into
their basis orders, which are unique since the functions are all distinct. Then
det
 Γ((ei ∧ej)p, (ei′ ∧ej′)q)(x)

p,q

= δii′δjj′(−1)sign(π)(−1)sign(π′)
= δii′δjj′ (−1)sign(π)2
= δii′δjj′
since, if i = i′ and j = j′, then π = π′. If either ei and ej or ei′ and ej′ do share a common function, suppose
without loss of generality that it is shared by both ei and ej. Let π be a permutation of ei ∧ej into its basis
order. The repeated function means there exists a transposition ˜π that fixes π(ei ∧ej), so
det
 Γ((ei ∧ej)p, (ei′ ∧ej′)q)(x)

p,q

= (−1)sign(π) det
 Γ((π(ei ∧ej))p, (ei′ ∧ej′)q)(x)

p,q

= (−1)sign(˜π)(−1)sign(π) det
 Γ((˜π(π(ei ∧ej)))p, (ei′ ∧ej′)q)(x)

p,q

= −(−1)sign(π) det
 Γ((π(ei ∧ej))p, (ei′ ∧ej′)q)(x)

p,q

= −det
 Γ((ei ∧ej)p, (ei′ ∧ej′)q)(x)

p,q

and hence
det
 Γ((ei ∧ej)p, (ei′ ∧ej′)q)(x)

p,q

= 0.
Let S0 be the subset of indices (i, j) ∈I+ × J+ such that ei and ej share a common basis function, and let
S+ = (I+ × J+) \ S0. Then, using the same argument as in Proposition 3.4 for the indices in I0 and J0, we can
conclude that
det
 Γ((ei ∧ej)p, (ei′ ∧ej′)q)(x)

p,q

=





0
i ∈I0 or j ∈I0 or i′ ∈I0 or j′ ∈I0
0
(i, j) ∈S0 or (i′, j′) ∈S0
δii′δjj′
(i, j) ∈S+ and (i′, j′) ∈S+
and simplify
g(α ∧β, α ∧β)(x) =
X
(i,j)∈S+
 f i(x)hj(x)
2
≤
X
i∈I+,j∈J+
 f i(x)hj(x)
2
=
X
i∈I+
f i(x)2 X
j∈J+
hj(x)2
= g(α, α)(x)g(β, β)(x).
Notice that, as on a manifold, there is equality at x if and only if S0 = ∅, i.e. the spans of the 1-forms comprising
α and β are orthogonal. We then have, for fixed β,
Z
g(α ∧β, α ∧β)dµ ≤
Z
g(α, α)g(β, β)dµ ≤sup(g(β, β))
Z
g(α, α)dµ,
so ∥α ∧β∥≤
p
sup(g(β, β))∥α∥, and likewise for β. In particular, if ∥α∥= 0 or ∥β∥= 0 then ∥α ∧β∥= 0, so
the wedge product is well defined on Ωk(M) × Ωl(M), and is a bounded linear operator in each argument.
■
Proposition 3.8 (Calculus rules for d0). The following properties hold for d = d0:
34

• Chain rule: if f1, ..., fk ∈A and ϕ : Rk →R is smooth then
d
 ϕ(f1, ..., fk)

=
k
X
i=1
∂iϕ(f1, ..., fk)dfi
• Leibniz rule: if f, g ∈A then d(fg) = fd(g) + gd(f).
Proof. The general diffusion property implies that
⟨dϕ(f1, ..., fk), h′dh⟩=
Z
h′Γ(ϕ(f1, ..., fk), h)dµ
=
k
X
i=1
Z
∂iϕ(f1, ..., fk)h′Γ(fi, h)dµ
=
k
X
i=1
⟨∂iϕ(f1, ..., fk)dfi, h′dh⟩.
The chain rule then follows from the non-degeneracy of the inner product and the fact that terms h′dh span
Ω1(M). The Leibniz rule follows by setting ϕ(f1, f2) = f1f2.
■
9.3
First-order calculus: vector fields and duality
The interior product is also well defined.
Proposition 3.15. iX(α) a well defined bilinear operator in X and α, and satisfies iY iX = −iXiY .
Proof. It is clear that condition (3) in the definition is compatible with (1) and (2), and that iα(β) is linear in
α and β for k = 0, 1. Now take β = f0df1 ∧· · · ∧dfk ∈Ωk(M). By definition, we must have
iα(β) = iα(f0df1 ∧· · · ∧dfk−1) ∧dfk −(−1)kg(α, dfk)f0df1 ∧· · · ∧dfk−1.
Since f0df1 ∧· · · ∧dfk−1 ∈Ωk−1(M), this expression gives an inductive definition for iα(β), which we extend
linearly to all β ∈Ωk(M). Linearity in α also follows by the same induction on k.
Notice that the antisymmetry property holds vacuously for k = 0, 1, since iγiα(β) = iαiγ(β) = 0, but in general
iγiα(β) = iγiα(f0df1 ∧· · · ∧dfk−1) ∧dfk
+ (−1)kg(γ, dfk)iα(f0df1 ∧· · · ∧dfk−1)
−(−1)kg(α, dfk)iγ
 f0df1 ∧· · · ∧dfk−1

.
We can inductively assume that iαiγ is antisymmetric on Ωk−1(M), and obtain
iαiγ(β) = −iγiα(f0df1 ∧· · · ∧dfk−1) ∧dfk
+ (−1)kg(α, dfk)iγ
 f0df1 ∧· · · ∧dfk−1

−(−1)kg(γ, dfk)iα(f0df1 ∧· · · ∧dfk−1)
= −iγiα(β)
which proves the result.
■
9.4
Second-order calculus: Hessian, covariant derivative, and Lie bracket
To motivate the definition of the Hessian (Definition 3.19), we derive the corresponding formula for the Hessian
on a manifold. We will use two standard facts from the Riemannian geometry of manifolds: that the Hessian
is given by
H(f)(X, Y ) = g(∇X(∇f), Y )
(7)
and the Levi-Civita connection satisfies the Koszul formula
g(∇XY, Z) = ∇Y (X, Z) = 1
2

X(g(Y, Z)) + Y (g(Z, X)) −Z(g(X, Y ))
+ g([X, Y ], Z) −g([Y, Z], X) + g([Z, X], Y )

.
(8)
To avoid confusion we will stick with the carr´e du champ notation, so here Γ(f, h) = g(∇f, ∇h) = ∇f(h).
35

Proposition 9.1. The Hessian on a manifold satisfies
H(f)(∇a, ∇b) = 1
2
 Γ(a, Γ(f, b)) + Γ(b, Γ(f, a)) −Γ(f, Γ(a, b))

.
Proof. We directly invoke (7) and (8) to compute
H(f)(∇a, ∇b) = g(∇∇a(∇f), ∇b)
= 1
2

∇a(g(∇f, ∇b)) + ∇f(g(∇b, ∇a)) −∇b(g(∇a, ∇f))
+ g([∇a, ∇f], ∇b) −g([∇f, ∇b], ∇a) + g([∇b, ∇a], ∇f)

and note that
g([∇a, ∇f], ∇b) = [∇a, ∇f](b)
= ∇a(∇f(b)) −∇f(∇a(b))
= Γ(a, Γ(f, b)) −Γ(f, Γ(a, b))
so obtain
H(f)(∇a, ∇b) = 1
2

Γ(a, Γ(f, b)) + Γ(f, Γ(a, b)) −Γ(b, Γ(f, a))
+ Γ(a, Γ(f, b)) −Γ(f, Γ(a, b)) −Γ(f, Γ(b, a))
+ Γ(b, Γ(f, a)) + Γ(b, Γ(f, a)) −Γ(a, Γ(f, b))

= 1
2
 Γ(a, Γ(f, b)) + Γ(b, Γ(f, a)) −Γ(f, Γ(a, b))

for all f, a, b.
■
With the definition for the Hessian offered here, we can derive the usual calculus rules.
Proposition 3.23. The Hessian satisfies the (second order) Leibniz rule
H(fh) = fH(h) + hH(f) + df ⊗dh + dh ⊗df
for all f, h ∈W 2,2(M). In particular, W 2,2(M) and W 2,∞(M) are closed under multiplication (and so are
subalgebras of A).
Proof. We can compute
H(fh)(∇a, ∇a) = Γ(a, Γ(fh, a)) −1
2Γ(fh, Γ(a, a))
= Γ(a, fΓ(h, a) + hΓ(f, a)) −1
2fΓ(h, Γ(a, a)) −1
2hΓ(f, Γ(a, a))
= fΓ(a, Γ(h, a)) + Γ(f, a)Γ(h, a) + hΓ(a, Γ(f, a)) + Γ(f, a)Γ(h, a)
−1
2fΓ(h, Γ(a, a)) −1
2hΓ(f, Γ(a, a))
= f

Γ(a, Γ(h, a)) −1
2Γ(h, Γ(a, a))

+ h

Γ(a, Γ(f, a)) −1
2Γ(f, Γ(a, a))

+ 2Γ(f, a)Γ(h, a)
=
h
fH(h) + hH(f) + 2(df ⊗dh)
i
(∇a, ∇a)
from which the result follows by polarisation and linearity.
■
Proposition 3.24. The Hessian satisfies the product rule for the carr´e du champ
d
 Γ(f1, f2)

= H(f1)(∇f2, ·) + H(f2)(∇f1, ·)
for all f1, f2 ∈W 2,2(M).
36

Proof. We can compute
H(f1)(∇f2, ∇h) + H(f2)(∇f1, ∇h) = 1
2

Γ(f2, Γ(f1, h)) + Γ(h, Γ(f1, f2)) −Γ(f1, Γ(f2, h))
Γ(f1, Γ(f2, h)) + Γ(h, Γ(f1, f2)) −Γ(f2, Γ(f1, h))

= Γ(h, Γ(f1, f2))
= d
 Γ(f1, f2)

(∇h)
and extend ∇h to an arbitrary X ∈X(M) by linearity.
■
We can derive the following formula for evaluating the Hessian.
Proposition 3.33. The Hessian satisfies
H(f)(X, Y ) = 1
2

g(X, [Y, ∇f]) + g(Y, [X, ∇f]) + Γ(f, g(X, Y ))

.
Proof. Suppose first that X = a0∇a1 and Y = b0∇b1. We evalutate
g(X, [Y, ∇f]) = g(a0∇a1, [b0∇b1, ∇f])
= a0[b0∇b1, ∇f](a1)
= a0
 b0[∇b1, ∇f] −Γ(f, b0)∇b1

(a1)
= a0b0Γ
 b1, Γ(f, a1)

−a0b0Γ
 f, Γ(a1, b1)

−a0Γ(f, b0)Γ(a1, b1)
and
Γ
 f, a0b0Γ(a1, b1)

= a0b0Γ
 f, Γ(a1, b1)

+ a0Γ(f, b0)Γ(a1, b1) + b0Γ(f, a0)Γ(a1, b1)
so
1
2

g(X, [Y, ∇f]) + g(Y, [X, ∇f]) + Γ(f, g(X, Y ))

= 1
2
h
a0b0Γ
 b1, Γ(f, a1)

−a0b0Γ
 f, Γ(a1, b1)

−a0Γ(f, b0)Γ(a1, b1)
+ a0b0Γ
 a1, Γ(f, b1)

−a0b0Γ
 f, Γ(a1, b1)

−b0Γ(f, a0)Γ(a1, b1)
+ a0b0Γ
 f, Γ(a1, b1)

+ a0Γ(f, b0)Γ(a1, b1) + b0Γ(f, a0)Γ(a1, b1)
i
= 1
2a0b0

Γ
 a1, Γ(f, b1)

+ Γ
 b1, Γ(f, a1)

−Γ
 f, Γ(a1, b1)

= a0b0H(f)(∇a1, ∇b1)
= H(f)(X, Y )
from which the result follows by linearity in X and Y .
■
We also check that the covariant derivative is well defined and satisfies the standard properties of the Levi-Civita
connection.
Proposition 3.26. The covariant derivative ∇is a well defined map X1(M) →Ω1(M)⊗2.
Proof. First notice that ∇X is indeed in Ω1(M)⊗2 because hi ∈W 2,∞(M) for all i, and so H(hi) ∈Ω1(M)⊗2.
We need to show that, if ∥X∥= 0, then ∥∇X∥= 0. We can compute
⟨∇X, da ⊗db⟩=
X Z h
Γ(fi, a)Γ(hi, b) + fiH(hi)(∇a, ∇b)
i
dµ,
and apply Proposition 3.24 to see that
H(hi)(∇a, ∇b) = Γ(Γ(hi, a), b) −H(a)(∇hi, ∇b),
37

so
⟨∇X, f ′da ⊗db⟩=
X Z
f ′h
Γ(fi, a)Γ(hi, b) + fiΓ(Γ(hi, a), b) −fiH(a)(∇hi, ∇b)
i
dµ
=
X Z
f ′h
Γ(fiΓ(hi, a), b) −fiH(a)(∇hi, ∇b)
i
dµ
=
Z
f ′h
Γ(g(X, ∇a), b) −H(a)(X, ∇b)
i
dµ.
If ∥X∥= 0 then g(X, X) = 0 almost everywhere, so g(X, ∇a) = 0 by Cauchy-Schwarz, and likewise
H(a)(X, ∇b)2 = g(H(a), X♭⊗db)2
≤g(H(a), H(a))g(X♭⊗db, X♭⊗db)
= g(H(a), H(a))g(X, X)Γ(b, b)
= 0
so ⟨∇X, f ′da⊗db⟩= 0. The terms f ′da⊗db span Ω1(M)⊗2, so we must have ∥∇X∥= 0, and the map X 7→∇X
is well defined.
■
Proposition 3.28. The covariant derivative ∇is an affine connection, meaning ∇X(Y, Z) is A-linear in Y
and Z, linear in X, and satisfies the Leibniz rule
∇(fX) = df ⊗X♭+ f∇X.
In particular, ∇Y X is A-linear in Y , and ∇Y (fX) = Y (f)X + f∇Y X.
Proof. The linearity claims follow directly from the definition. To prove the Leibniz rule, in the case X = f ′∇h,
we have
∇(fX) = d(ff ′) ⊗dh + ff ′H(hi)
=
 fd(f ′) + f ′d(f)

⊗dh + ff ′H(hi)
= df ⊗X♭+ f∇X,
from which the result follows by linearity.
■
Proposition 3.29. The covariant derivative ∇is compatible with the metric, meaning
∇Xg(Y, Z) = g(∇XY, Z) + g(Y, ∇XZ).
Proof. We need to check that X(g(Y, Z)) = ∇Y (X, Z) + ∇Z(X, Y ). If we set Y = f1∇h1 and Z = f2∇h2, we
can evaluate
X
 g(Y, Z)

= X
 f1f2Γ(h1, h2)

= X(f1f2)Γ(h1, h2) + f1f2X
 Γ(h1, h2)

= X(f1)f2Γ(h1, h2) + f1f2H(h2)(X, ∇h2)
+ X(f2)f1Γ(h1, h2) + f1f2H(h1)(X, ∇h1)
= X(f1)Z(h1) + f1H(h2)(X, Z)
+ X(f2)Y (h2) + f2H(h1)(X, Y )
= ∇Y (X, Z) + ∇Z(X, Y )
using the Leibniz rule for X twice and the product rule for Γ (Proposition 3.24). The general case follows by
linearity in Y and Z.
■
We also check that the Lie bracket is indeed the commutator of operators [X, Y ] = XY −Y X on X1(M), and
satisfies the Leibniz rule.
Proposition 3.31. We have [X, Y ] = XY −Y X for all X, Y ∈X1(M).
38

Proof. We first consider Y = f∇h and compute
X(Y (a)) = X(fΓ(h, a))
= X(f)Γ(h, a) + fX(Γ(h, a))
= X(f)Γ(h, a) + f
 H(h)(∇a, X) + H(a)(∇h, X)

= X(f)Γ(h, a) + fH(h)(∇a, X) + H(a)(Y, X)
using the product rule for Γ (Proposition 3.24). Now notice that
∇XY (a) = g(∇XY, ∇a)
= ∇Y (X, ∇a)
= X(f)Γ(h, a) + fH(h)(∇a, X)
so
X(Y (a)) = ∇XY (a) + H(a)(X, Y ),
which extends to arbitrary Y by linearity. We then see that
X(Y (a)) −Y (X(a)) = [X, Y ](a)
by symmetry of H(a).
■
Proposition 3.32. We have [X, fY ] = X(f)Y + f[X, Y ] and [fX, Y ] = −Y (f)X + f[X, Y ].
Proof. We directly compute
[X, fY ] = ∇X(fY ) −∇fY X
= ∇(fY )(X, ·)♯−∇(X)(fY, ·)♯
= (df ⊗Y + f∇Y )(X, ·)♯−f∇(X)(Y, ·)♯
= X(f)Y + f∇(Y )(X, ·)♯−f∇(X)(Y, ·)♯
= X(f)Y + f[X, Y ]
and likewise for [fX, Y ].
■
9.5
Exterior derivative and codifferential
We verify that the exterior derivative and codifferential are well defined on higher-order forms, and that the
differential satisfies the Leibniz rule.
Proposition 3.38. If X0, ..., Xk ∈X(M) and α ∈Ωk(M) then dkα satisfies
dkα(X0, ..., Xk) =
k
X
i=0
(−1)iXi(α(..., ˆ
Xi, ...)) +
X
i<j
(−1)i+jα([Xi, Xj], ..., ˆ
Xi, ..., ˆ
Xj, ...),
where [Xi, Xj] is defined in the weak sense9. In particular, if ∥α∥= 0 then ∥dkα∥= 0, so dk is a well defined
map.
Proof. Let Tα : X(M)k+1 →A be given by
Tα(X0, ..., Xk) =
k
X
i=0
(−1)iXi(α(..., ˆ
Xi, ...)) +
X
i<j
(−1)i+jα([Xi, Xj], ..., ˆ
Xi, ..., ˆ
Xj, ...),
so want to show that Tα = dα. We first show that, if β = dh1 ∧· · · ∧dhk, then Tβ = 0. Let X0, ..., Xk ∈X(M),
and notice that β(X1, ..., Xk) = det
 Xi(hj)

so that
β(..., ˆ
Xi, ...) =
X
σ∈Sk
sign(σ)
Y
l̸=i
Xl(hσ(ai(l)))
9See equation 4.
39

where
ai(l) =
(
l + 1
l < i
l
l > i
for l = 0, ..., i −1, i + 1, ..., k. We can compute
Xi(β(..., ˆ
Xi, ...)) = Xi
  X
σ∈Sk
sign(σ)
Y
l̸=i
Xl(hσ(ai(l)))

=
X
σ∈Sk
sign(σ)
X
j̸=i
XiXj(hσ(ai(j)))
Y
l̸=i,j
Xl(hσ(ai(l)))

,
with the Leibniz rule, and so
k
X
i=0
(−1)iXi(β(..., ˆ
Xi, ...)) =
X
i̸=j
X
σ∈Sk
sign(σ)(−1)iXiXj(hσ(ai(j)))
Y
l̸=i,j
Xl(hσ(ai(l)))

.
On the other hand, we can see that
β([Xi, Xj], ..., ˆ
Xi, ..., ˆ
Xj, ...) =
X
σ∈Sk
sign(σ)[Xi, Xj](hσ(1))
Y
l̸=i,j
Xl(hσ(bij(l)))
where now
bij(l) =





l + 2
l < min(i, j)
l + 1
min(i, j) < l < max(i, j)
l
max(i, j) < l
for l ̸= i, j. We then find
X
i<j
(−1)i+jβ([Xi, Xj], ..., ˆ
Xi, ..., ˆ
Xj, ...) =
X
i<j
(−1)i+j X
σ∈Sk
sign(σ)[Xi, Xj](hσ(1))
Y
l̸=i,j
Xl(hσ(bij(l)))
=
X
i<j
(−1)i+j X
σ∈Sk
sign(σ)XiXj(hσ(1))
Y
l̸=i,j
Xl(hσ(bij(l)))
−
X
i<j
(−1)i+j X
σ∈Sk
sign(σ)XjXi(hσ(1))
Y
l̸=i,j
Xl(hσ(bij(l)))
=
X
i<j
(−1)i+j X
σ∈Sk
sign(σ)XiXj(hσ(1))
Y
l̸=i,j
Xl(hσ(bij(l)))
−
X
i>j
(−1)i+j X
σ∈Sk
sign(σ)XiXj(hσ(1))
Y
l̸=i,j
Xl(hσ(bij(l))).
We now want to reindex the sum over Sk, and do so separately for the cases i < j and i > j. If i < j, we define
τ ij ∈Sk as
τ ij = (j, 1, 2, ..., j −1, j + 1, ..., k)
which has sign (−1)j. If i > j, we define τ ij ∈Sk as
τ ij = (j + 1, 1, 2, ..., j, j + 2, ..., k)
which has sign (−1)j+1. An elementary but tedious calculation shows that, in both cases, τ ij(1) = ai(j) and
τ ij(bij(l)) = ai(l). τ ij has a transitive right action on Sk, so we can replace σ with στ ij in the summand, and
obtain
X
i<j
(−1)i+jβ([Xi, Xj], ..., ˆ
Xi, ..., ˆ
Xj, ...) =
X
i<j
(−1)i+j X
σ∈Sk
sign(στ ij)XiXj(hστ ij(1))
Y
l̸=i,j
Xl(hστ ij(bij(l)))
−
X
i>j
(−1)i+j X
σ∈Sk
sign(στ ij)XiXj(hστ ij(1))
Y
l̸=i,j
Xl(hστ ij(bij(l)))
=
X
i<j
(−1)i+j X
σ∈Sk
sign(σ)(−1)j+1XiXj(hσ(ai(j)))
Y
l̸=i,j
Xl(hσ(ai(l)))
−
X
i>j
(−1)i+j X
σ∈Sk
sign(σ)(−1)jXiXj(hσ(ai(j)))
Y
l̸=i,j
Xl(hσ(ai(l)))
= −
X
i̸=j
X
σ∈Sk
sign(σ)(−1)iXiXj(hσ(ai(j)))
Y
l̸=i,j
Xl(hσ(ai(l)))
= −
k
X
i=0
(−1)iXi(β(..., ˆ
Xi, ...))
40

so Tβ = 0.
We now let α = fβ where β = dh1 ∧· · · ∧dhk, and find
Tα(X0, ..., Xk) =
k
X
i=0
(−1)iXi(fβ(..., ˆ
Xi, ...)) +
X
i<j
(−1)i+jfβ([Xi, Xj], ..., ˆ
Xi, ..., ˆ
Xj, ...)
=
k
X
i=0
(−1)iXi(f)β(..., ˆ
Xi, ...) + fTβ(X0, ..., Xk)
= df ∧dh1 ∧· · · ∧dhk(X0, ..., Xk),
where the last step follows by expanding the determinant along the first column. By linearity we see that
Tα(X0, ..., Xk) = dα(X0, ..., Xk) for all α ∈Ωk(M).
Now suppose that ∥α∥= 0, so g(α, α) = 0 almost everywhere. Then
α(X0, ..., Xk)2 = g(α, X♭
0 ∧· · · ∧X♭
k)2
≤g(α, α)g(X♭
0 ∧· · · ∧X♭
k, X♭
0 ∧· · · ∧X♭
k)
= 0
almost everywhere. Tα(X0, ..., Xk) = 0 where α(X0, ..., Xk) = 0 so
⟨dα, fdh0 ∧· · · ∧dhk⟩=
Z
fdα(∇h0, ..., ∇hk)dµ
=
Z
fTα(∇h0, ..., ∇hk)dµ
= 0.
It follows by linearity that ⟨dα, β⟩= 0 for all β ∈Ωk+1(M), so ∥dα∥= 0. This means that the map A⊗Vk(A) →
A ⊗Vk+1(A) given by
fdh1 ∧· · · ∧dhk 7→1 ⊗df ∧dh1 ∧· · · ∧dhk
descends to the quotient (by zero-norm tensors), and so α 7→dα is a well defined map Ωk(M) →Ωk+1(M).
■
Proposition 3.39. If α ∈Ωk(M) and β ∈Ωl(M) then
d(α ∧β) = dα ∧β + (−1)kα ∧dβ.
Proof. We dealt with the case where f, h ∈Ω0(M) = A in Proposition 3.8. If we now take α = f0df1 ∧···∧dfk ∈
Ωk(M) and β = h0dh1 ∧· · · ∧dhl ∈Ωl(M), we can calculate
d(α ∧β) = d(f0h0) ∧df1 ∧· · · ∧dfk ∧dh1 ∧· · · ∧dhl
=
 h0df0 + f0dh0

∧df1 ∧· · · ∧dfk ∧dh1 ∧· · · ∧dhl
= h0df0 ∧df1 ∧· · · ∧dfk ∧dh1 ∧· · · ∧dhl
+ f0dh0 ∧df1 ∧· · · ∧dfk ∧dh1 ∧· · · ∧dhl
=
 df0 ∧df1 ∧· · · ∧dfk

∧
 h0dh1 ∧· · · ∧dhl

+ (−1)k f0df1 ∧· · · ∧dfk

∧
 dh0 ∧dh1 ∧· · · ∧dhl

= dα ∧β + (−1)kα ∧dβ.
The case for general α ∈Ωk(µ) and β ∈Ωl(µ) follows by linearity of ∧and d.
■
Proposition 3.41. The codifferential satisfies ⟨∂kα, β⟩= ⟨α, dk−1β⟩for all β, and so, in particular, is a well
defined map.
Proof. For brevity, we will write
ˆ
dhi = dh1 ∧· · · ∧ˆ
dhi ∧· · · ∧dhk
and
ˆ
dhi ∧ˆ
dhj = dh1 ∧· · · ∧ˆ
dhi ∧· · · ∧ˆ
dhj ∧· · · ∧dhk.
41

We consider a general form aβ ∈Ωk−1(M), where a ∈A and β = dh′
1 ∧· · · ∧dh′
k−1. We find that
⟨
 Γ(f, hi) −fL(hi)
 ˆ
dhi, aβ⟩=
Z h
Γ(f, hi)g( ˆ
dhi, aβ) −fL(hi)g( ˆ
dhi, aβ)
i
dµ
=
Z h
Γ(f, hi)ag( ˆ
dhi, β) −Γ
 hi, fag( ˆ
dhi, β)
i
dµ
= −
Z
fΓ
 hi, ag( ˆ
dhi, β)

dµ
= −
Z
faΓ
 hi, g( ˆ
dhi, β)

dµ −
Z
fΓ(hi, a)g( ˆ
dhi, β)dµ.
Notice that
−
Z
f
h
k
X
i=1
(−1)iΓ(hi, a)g( ˆ
dhi, β)
i
dµ =
Z
fg(dh1 ∧· · · ∧dhk, da ∧β)dµ
= ⟨fdh1 ∧· · · ∧dhk, d(aβ)⟩
by expanding the first column of the determinant, so
⟨∂(fdh1 ∧· · · ∧dhk), aβ⟩=
k
X
i=1
(−1)i⟨
 Γ(f, hi) −fL(hi)
 ˆ
dhi, aβ⟩
+
X
i<j
(−1)i+j⟨f[∇hi, ∇hj]♭∧ˆ
dhi ∧ˆ
dhj, aβ⟩
= ⟨fdh1 ∧· · · ∧dhk, d(aβ)⟩
−
Z
fa
h
k
X
i=1
(−1)iΓ
 hi, g( ˆ
dhi, β)

−
X
i<j
(−1)i+jg
 [∇hi, ∇hj]♭∧ˆ
dhi ∧ˆ
dhj, β
i
dµ
We can apply Proposition 3.38 to find that
k
X
i=1
(−1)iΓ
 hi, g( ˆ
dhi, β)

−
X
i<j
(−1)i+jg
 [∇hi, ∇hj]♭∧ˆ
dhi ∧ˆ
dhj, β

=
k
X
i=1
(−1)i∇hi(β(..., ˆ
∇hi, ...)) −
X
i<j
(−1)i+jβ([∇hi, ∇hj], ..., ˆ
∇hi, ...,
ˆ
∇hj, ...)
= −dβ(∇h1, ..., ∇hk) = 0
because β is exact (hence closed), which proves the claim. Now if ∥α∥= 0 then
⟨∂α, β⟩= ⟨α, dβ⟩= 0
for all β ∈Ωk−1(M), giving ∥∂α∥= 0. So the map A ⊗Vk(A) →A ⊗Vk−1(A) given by α 7→∂α descends to
the quotient (by zero-norm tensors), meaning α 7→∂α is a well defined map Ωk(M) →Ωk−1(M).
■
42

10
Appendix B: Implementation Details
We implement the diffusion maps Laplacian exactly as in [19]. All the basic objects in diffusion geometry can
be expressed with tensor operations on the eigenvalues and eigenfunctions of ∆. These are easily calculated
with the Einstein summation functions in Numpy, we make the following derivations in that form.
All the forms here are represented in the eigenfunction frame discussed in Section 4, so functions are in the
basis ϕi, and 1-forms in the frame ϕidϕj etc. Where needed, we will project forms into an orthonormal basis for
the positive definite subspace of the inner product. However, it is more convenient here to work in the original
eigenfunction frame, and so we will always include forms back in the frame after working in the orthonormal
basis.
10.1
Structure constants for the multiplicative algebra
cijk = ⟨ϕiϕj, ϕk⟩= 1
n
n
X
s=1
ϕi(s)ϕj(s)ϕk(s)D(s)
Notice that, since ϕ0 is constant, we have
cij0 = ϕ0⟨ϕi, ϕj⟩= ϕ0δij.
We will let ci1...ik denote the corresponding k-fold products.
10.2
Carr´e du champ
Γijs = ⟨Γ(ϕi, ϕj), ϕs⟩= 1
2(λi + λj −λs)cijs
10.3
Metric on k-forms
To illustrate the general formula for gk, we derive it here for k = 2. Let us write the 2-forms
αI = ϕi0dϕi1 ∧dϕi2
αJ = ϕj0dϕj1 ∧dϕj2
where I = (i0, i1, i2) and J = (j0, j1, j2). Then, using Einstein summation notation,
g(αI, αJ) = ϕi0ϕj0 det
Γ(ϕi1, ϕj1)
Γ(ϕi1, ϕj2)
Γ(ϕi2, ϕj1)
Γ(ϕi2, ϕj2)

= ϕi0ϕj0 det
Γi1j1sϕs
Γi1j2tϕt
Γi2j1sϕs
Γi2j2tϕt

= ϕi0ϕj0ϕsϕt det
Γi1j1s
Γi1j2t
Γi2j1s
Γi2j2t

= ci0j0stℓdet
Γi1j1s
Γi1j2t
Γi2j1s
Γi2j2t

ϕℓ
= ci0j0ucstuℓdet
Γi1j1s
Γi1j2t
Γi2j1s
Γi2j2t

ϕℓ
where we are allowed to reuse the dummy indices s, t because two entries in the same column are not multiplied
in the determinant. The general formula is derived analogously. Let I = (i0, ..., ik) and J = (j0, ..., jk), and
take dummy indices s1, ..., sk. Then
g(αI, αJ) = ci0j0s1···skℓdet
 (Γinjmsm)n,m

ϕℓ.
In the special case k = 1, we find the formula
g(ϕidϕj, ϕkdϕl) = cikstΓjlsϕt,
and when k = 0 we retrieve g(ϕi, ϕj) = ϕiϕj = cijtϕt.
43

10.4
Inner product on k-forms (Gram matrix)
To illustrate the general formula for Gk, we derive it here for k = 2. Let us write the 2-forms
αI = ϕi0dϕi1 ∧dϕi2
αJ = ϕj0dϕj1 ∧dϕj2
where I = (i0, i1, i2) and J = (j0, j1, j2). Then, using Einstein summation notation,
⟨αI, αJ⟩=
Z
ϕi0ϕj0 det
Γ(ϕi1, ϕj1)
Γ(ϕi1, ϕj2)
Γ(ϕi2, ϕj1)
Γ(ϕi2, ϕj2)

dµ
=
Z
ϕi0ϕj0 det
Γi1j1sϕs
Γi1j2tϕt
Γi2j1sϕs
Γi2j2tϕt

dµ
=
 Z
ϕi0ϕj0ϕsϕtdµ

det
Γi1j1s
Γi1j2t
Γi2j1s
Γi2j2t

= ci0j0st det
Γi1j1s
Γi1j2t
Γi2j1s
Γi2j2t

where we are allowed to reuse the dummy indices s, t because two entries in the same column are not multiplied
in the determinant. Recall here that
Γijs = 1
2(λi + λj −λs)cijs.
The general formula is derived analogously. Let I = (i0, ..., ik) and J = (j0, ..., jk), and take dummy indices
s1, ..., sk. Then
⟨αI, αJ⟩= ci0j0s1···sk det
 (Γinjmsm)n,m

.
In the special case k = 1, we obtain the formula
G1
ijkl = ⟨ϕidϕj, ϕkdϕl⟩= ciksΓjls,
and when k = 0 we retrieve ⟨ϕi, ϕj⟩= cij = δij.
10.5
Pseudo-inverse of the Gram matrix.
A common computational problem here is the recovery of a form v from its ‘weak’ representation Gv. For
example, we might have some expression for ⟨w, v⟩= wT Gv for all w and would like to know v. The Gram
matrix we compute above is generally degenerate, and so is not invertible: the solution is to take the Moore-
Penrose pseudo-inverse of G, where we
1. project v onto the positive definite subspace of G,
2. invert G on this subspace and compute G−1v, and
3. include G−1v in the total space.
So while we cannot truly invert G in the frame, we can invert it up to a form of zero norm.
We can obtain this representation by diagonalising G, and restricting to the eigenspaces with positive eigenvalue
(in practice we take eigenvalues above some variable threshold, to control numerical instability). We denote the
pseudoinverse of G by G+ and can express it as
G+ = P T (PGP T )−1P
where P is the orthonormal projection matrix onto the positive definite subspace of G. P was obtained by
diagonalising G, so the matrix (PGP T ) is diagonal and easily inverted. We can verify that, if w = P T v is in
the positive definite subspace of G, then
G+Gw = P T (PGP T )−1PGP T v = w
and so we can recover w from Gw with G+. Likewise
GG+w = GP T (PGP T )−1PP T v
= P T PGP T (PGP T )−1v
= w.
using the facts that PP T = Id and G = P T DP = P T PP T DP = P T PG.
44

10.6
Dual vector fields
⟨ϕk, (ϕidϕj)♯ϕl⟩=
Z
g(ϕidϕj, dϕl)ϕkdµ = G1
ijkl
So, if v is a 1-form, G1v gives the coefficients of v♯as a linear operator. By reshaping G1v into a matrix we
can compute its action on functions. In the following, we will write G1v to denote this matrix and write ◦to
denote matrix multiplication (i.e. composition of vector fields).
10.7
Dual 1-forms
By duality with the above, if X is a vector field represented by Xij = ⟨ϕi, X(ϕj)⟩, then X = G1X♭, and so we
can recover X♭= (G1)+X.
10.8
Wedge product
Let αI ∈Ωk(M) and αJ ∈Ωl(M) be given by
αI = ϕi0dϕi1 ∧· · · ∧dϕik
αJ = ϕj0dϕj1 ∧· · · ∧dϕjl
where I = (i0, ..., ik) and J = (j0, j1, j2). Then, using Einstein summation notation,
αI ∧αJ = (ϕi0ϕj0)dϕi1 ∧· · · ∧dϕik ∧dϕj1 ∧· · · ∧dϕjl
= ci0j0sϕsdϕi1 ∧· · · ∧dϕik ∧dϕj1 ∧· · · ∧dϕjl.
10.9
Exterior derivative
We can represent the exterior derivative weakly in the matrix
( ˜dk)ij = ⟨dk(vj), wi⟩
and recover the strong formulation as the matrix
dk = (Gk)+( ˜dk).
To illustrate the general formula, we derive it here for k = 1. Let us write the 2 and 1-forms
αI = ϕi0dϕi1 ∧dϕi2
αJ = ϕj0dϕj1
where I = (i0, i1, i2) and J = (j0, j1). Then, using Einstein summation notation,
⟨αI, dαJ⟩=
Z
ϕi0 det
Γ(ϕi1, ϕj0)
Γ(ϕi1, ϕj1)
Γ(ϕi2, ϕj0)
Γ(ϕi2, ϕj1)

dµ
=
Z
ϕi0 det
Γi1j0sϕs
Γi1j1tϕt
Γi2j0sϕs
Γi2j1tϕt

dµ
=
 Z
ϕi0ϕsϕtdµ

det
Γi1j0s
Γi1j1t
Γi2j0s
Γi2j1t

= ci0st det
Γi1j0s
Γi1j1t
Γi2j0s
Γi2j1t

where we are allowed to reuse the dummy indices s, t because two entries in the same column are not multiplied
in the determinant. The general formula is derived analogously. Let I = (i0, ..., ik+1) and J = (j0, ..., jk), and
take dummy indices s1, ..., sk. Then
⟨αI, dαJ⟩= ci0s1···sk det
 (Γin+1jmsm)n,m

.
In the special case k = 0, we obtain the formula
⟨ϕi0dϕi1, dϕj⟩= Γi1ji0.
We can also recover the weak and strong forms of d∗
k from ˜dk since
( ˜dk)ij = ⟨vj, d∗
k(wi)⟩= ( ˜d∗
k)ji
and so ˜d∗
k = ˜dT
k and d∗
k = (Gk−1)+ ˜dT
k .
45

10.10
Hodge Laplacian
We will represent the Hodge Laplacian ∆k weakly as
( ˜
∆k)ij = ⟨wi, ∆k(vj)⟩= ⟨dk(wi), dk(vj)⟩+ ⟨wi, dk−1d∗
k−1(vj)⟩.
The ‘down’ term ⟨wi, dk−1d∗
k−1(vj)⟩is the weak formulation of the operator dk−1d∗
k−1, which we can evaluate
using ˜dk−1 as
Gkdk−1d∗
k−1 = ˜dk−1(Gk−1)+ ˜dT
k−1.
Although a similar process would work for the ‘up’ term, this would involve the pseudo-inverse of Gk+1, which
is more computationally complex. We can instead use the ‘kernel trick’ and directly evaluate ⟨dk(wi), dk(vj)⟩
using the metric to get
⟨dk(wi), dk(vj)⟩=
Z
g(dk(vj), dk(vj))dµ.
We can derive an explicit formula for this ‘up’ energy in terms of the eigenfunction frame, which we illustrate
here with the case k = 2. Let us write the 2-forms
αI = ϕi0dϕi1 ∧dϕi2
αJ = ϕj0dϕj1 ∧dϕj2
where I = (i0, i1, i2) and J = (j0, j1, j2). Then, using Einstein summation notation,
⟨dαI, dαJ⟩=
Z
det


Γ(ϕi0, ϕj0)
Γ(ϕi0, ϕj1)
Γ(ϕi0, ϕj2)
Γ(ϕi1, ϕj0)
Γ(ϕi1, ϕj1)
Γ(ϕi1, ϕj2)
Γ(ϕi2, ϕj0)
Γ(ϕi2, ϕj1)
Γ(ϕi2, ϕj2)

dµ
=
Z
det


Γi0j0sϕs
Γi0j1tϕt
Γi0j2uϕu
Γi1j0sϕs
Γi1j1tϕt
Γi1j2uϕu
Γi2j0sϕs
Γi2j1tϕt
Γi2j2uϕu

dµ
=
 Z
ϕsϕtϕudµ

det


Γi0j0s
Γi0j1t
Γi0j2u
Γi1j0s
Γi1j1t
Γi1j2u
Γi2j0s
Γi2j1t
Γi2j2u


= cstu det


Γi0j0s
Γi0j1t
Γi0j2u
Γi1j0s
Γi1j1t
Γi1j2u
Γi2j0s
Γi2j1t
Γi2j2u


where we are allowed to reuse the dummy indices s, t, u because two entries in the same column are not multiplied
in the determinant. Recall here that
Γijs = 1
2(λi + λj −λs)cijs.
The general formula is derived analogously. Let I = (i0, ..., ik) and J = (j0, ..., jk), and take dummy indices
S = (s0, ..., sk). Then
⟨dαI, dαJ⟩= cS det
 (Γinjmsm)n,m

.
In the special case k = 1, we have cpq = δpq and so
⟨dϕi0 ∧dϕi1, dϕj0 ∧dϕj1⟩= det
Γi0j0s
Γi0j1s
Γi1j0s
Γi1j1s

In the very special case k = 0, we have
cs =
Z
ϕsdµ = 1
ϕ0
Z
ϕ0ϕsdµ = 1
ϕ0
δs0
and so
⟨dϕi, dϕj⟩= 1
ϕ0
Γij0.
Now notice that
cij0 =
Z
ϕiϕjϕ0dµ = ϕ0δij
so
Γij0 = 1
2(λi + λj −λ0)cij0 = ϕ0λiδij
giving ⟨dϕi, dϕj⟩= λiδij. In particular, we recover that the Laplacian ∆is represented by a diagonal matrix
with entries λi.
46

10.11
Hodge Decomposition
On a manifold, a 1-form α ∈Ω1(M) can be decomposed uniquely as
α = h + df + d∗β
where h ∈ker ∆, f ∈A, and β ∈Ω2(M). In our case, the Hodge Laplacian on 1-forms does not actually have a
kernel, due to approximation error, and so the ‘harmonic’ bit h shows up in the d∗β part. Although we have not
proved this result for general diffusion Markov triples, the Hodge decomposition theorem can be shown (fairly
straightforwardly) for finite-dimensional spaces, with which we are working here.
To compute the ‘gradient’ part df, we can recover the function f as
f = ∆+d∗α,
where ∆+ is the pseudo-inverse of ∆. In our eigenfunction basis, this is given by
(∆+f)i =
(
0
λi = 0
fi/λi
λi > 0
We can therefore write the projection onto the ‘gradient-only’ part of Ω1(M) as d∆+d∗.
10.12
Lie bracket
[v, w] =
 v♯◦w♯−w♯◦v♯♭
= (G1)+ G1v ◦G1w −G1w ◦G1v

10.13
Covariant derivative
We use the Koszul formula
⟨∇XY, Z⟩= 1
2
 1
ϕ0
⟨X(g(Y, Z)), ϕ0⟩+ 1
ϕ0
⟨Y (g(Z, X)), ϕ0⟩−1
ϕ0
⟨Z(g(X, Y )), ϕ0⟩
+ ⟨[X, Y ], Z⟩−⟨[Y, Z], X⟩+ g⟨[Z, X], Y ⟩

.
References
[1]
Luigi Ambrosio. Calculus, heat flow and curvature-dimension bounds in metric measure spaces. In Pro-
ceedings of the International Congress of Mathematicians: Rio de Janeiro 2018, pages 301–340. World
Scientific, 2018.
[2]
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar´e. Calculus and heat flow in metric measure spaces and
applications to spaces with ricci bounds from below. Inventiones mathematicae, 195(2):289–391, 2014.
[3]
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar´e. Metric measure spaces with riemannian ricci curvature
bounded from below. 2014.
[4]
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar´e.
Bakry–´emery curvature-dimension condition and
riemannian ricci curvature bounds. 2015.
[5]
Anton Arnold, Peter Markowich, Giuseppe Toscani, and Andreas Unterreiter. On convex sobolev inequal-
ities and the rate of convergence to equilibrium for fokker-planck type equations. 2001.
[6]
Douglas Arnold, Richard Falk, and Ragnar Winther. Finite element exterior calculus: from hodge theory
to numerical stability. Bulletin of the American mathematical society, 47(2):281–354, 2010.
[7]
Douglas N Arnold, Richard S Falk, and Ragnar Winther. Finite element exterior calculus, homological
techniques, and applications. Acta numerica, 15:1–155, 2006.
[8]
Dominique Bakry and M ´Emery. S´eminaire de probabilit´es xix 1983/84, 1985.
[9]
Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov diffusion operators,
volume 103. Springer, 2014.
47

[10] Laurent Bartholdi, Thomas Schick, Nat Smale, and Steve Smale. Hodge theory on metric spaces. Founda-
tions of Computational Mathematics, 12:1–48, 2012.
[11] Ulrich Bauer. Ripser: efficient computation of vietoris–rips persistence barcodes. Journal of Applied and
Computational Topology, 5(3):391–423, 2021.
[12] Tyrus Berry and Dimitrios Giannakis. Spectral exterior calculus. Communications on Pure and Applied
Mathematics, 73(4):689–770, 2020.
[13] Andrew J Blumberg and Michael Lesnick. Stability of 2-parameter persistent homology. Foundations of
Computational Mathematics, pages 1–43, 2022.
[14] Omer Bobrowski, Sayan Mukherjee, and Jonathan E Taylor. Topological consistency via kernel estimation.
2017.
[15] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning: Grids,
groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.
[16] Joshua A Bull, Philip S Macklin, Tom Quaiser, Franziska Braun, Sarah L Waters, Chris W Pugh, and
Helen M Byrne. Combining multiple spatial statistics enhances the description of immune cell localisation
within tumours. Scientific reports, 10(1):18624, 2020.
[17] Gunnar Carlsson, Gurjeet Singh, and Afra Zomorodian. Computing multidimensional persistence. In Algo-
rithms and Computation: 20th International Symposium, ISAAC 2009, Honolulu, Hawaii, USA, December
16-18, 2009. Proceedings 20, pages 730–739. Springer, 2009.
[18] Gunnar Carlsson and Afra Zomorodian. The theory of multidimensional persistence. In Proceedings of the
twenty-third annual symposium on Computational geometry, pages 184–193, 2007.
[19] Ronald R. Coifman and St´ephane Lafon. Diffusion maps. Applied and Computational Harmonic Analysis,
21(1):5–30, 2006. Special Issue: Diffusion Maps and Wavelets.
[20] Mathieu Desbrun, Anil N Hirani, Melvin Leok, and Jerrold E Marsden. Discrete exterior calculus. arXiv
preprint math/0508341, 2005.
[21] David L Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-
dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591–5596, 2003.
[22] Edelsbrunner, Letscher, and Zomorodian. Topological persistence and simplification. Discrete & computa-
tional geometry, 28:511–533, 2002.
[23] Nicola Gigli. Nonsmooth differential geometry–an approach tailored for spaces with Ricci curvature bounded
from below, volume 251. American Mathematical Society, 2018.
[24] Nicola Gigli, Enrico Pasqualetto, et al. Lectures on nonsmooth differential geometry. Springer, 2020.
[25] Alexander Grigor’yan. Heat kernels on weighted manifolds and applications. Cont. Math, 398(2006):93–191,
2006.
[26] Heather A Harrington, Nina Otter, Hal Schenck, and Ulrike Tillmann. Stratifying multiparameter persistent
homology. SIAM Journal on Applied Algebra and Geometry, 3(3):439–471, 2019.
[27] Charles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf Gommers, Pauli Virtanen, David
Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Pi-
cus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern´andez del R´ıo,
Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser,
Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature,
585(7825):357–362, September 2020.
[28] Nandakishore Kambhatla and Todd K Leen. Dimension reduction by local principal component analysis.
Neural computation, 9(7):1493–1516, 1997.
[29] Uzu Lim, Harald Oberhauser, and Vidit Nanda. Hades: Fast singularity detection with local measure
comparison. arXiv preprint arXiv:2311.04171, 2023.
48

[30] Nina Otter, Mason A Porter, Ulrike Tillmann, Peter Grindrod, and Heather A Harrington. A roadmap for
the computation of persistent homology. EPJ Data Science, 6:1–38, 2017.
[31] Vanessa Robins. Towards computing homology from finite approximations. Topology proceedings, 24(1):503–
532, 1999.
[32] Jaehyeok Shin, Jisu Kim, Alessandro Rinaldo, and Larry Wasserman. Confidence sets for persistent ho-
mology of the kde filtration.
[33] Amit Singer and H-T Wu. Vector diffusion maps and the connection laplacian. Communications on pure
and applied mathematics, 65(8):1067–1144, 2012.
[34] Bernadette J Stolz, Jared Tanner, Heather A Harrington, and Vidit Nanda. Geometric anomaly detection
in data. Proceedings of the national academy of sciences, 117(33):19664–19669, 2020.
[35] Karl-Theodor Sturm. Ricci tensor for diffusion operators and curvature-dimension inequalities under con-
formal transformations and time changes. Journal of Functional Analysis, 275(4):793–829, 2018.
[36] Renata Turkeˇs, Jannes Nys, Tim Verdonck, and Steven Latr´e. Noise robustness of persistent homology on
greyscale images, across filtrations and signatures. Plos one, 16(9):e0257215, 2021.
[37] Oliver Vipond. Multiparameter persistence landscapes. Journal of Machine Learning Research, 21(61):1–
38, 2020.
[38] Oliver Vipond, Joshua A. Bull, Philip S. Macklin, Ulrike Tillmann, Christopher W. Pugh, Helen M. Byrne,
and Heather A. Harrington. Multiparameter persistent homology landscapes identify immune cell spatial
patterns in tumors. Proceedings of the National Academy of Sciences, 118(41):e2102166118, 2021.
[39] Julius Von Rohrscheidt and Bastian Rieck. Topological singularity detection at multiple scales. In Inter-
national Conference on Machine Learning, pages 35175–35197. PMLR, 2023.
[40] Yong-Liang Yang, Yu-Kun Lai, Shi-Min Hu, Helmut Pottmann, et al. Robust principal curvatures on
multiple scales. In Symposium on Geometry processing, pages 223–226, 2006.
[41] Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. In Proceedings of the twentieth
annual symposium on Computational geometry, pages 347–356, 2004.
49

