
Regularization,  
Optimization, Kernels, and 
Support Vector Machines

Chapman & Hall/CRC 
Machine Learning & Pattern Recognition Series
SERIES EDITORS
Ralf Herbrich
Amazon Development Center
Berlin, Germany
Thore Graepel
Microsoft Research Ltd.
Cambridge, UK
AIMS AND SCOPE
This series reflects the latest advances and applications in machine learning and pattern recog-
nition through the publication of a broad range of reference works, textbooks, and handbooks. 
The inclusion of concrete examples, applications, and methods is highly encouraged. The scope 
of the series includes, but is not limited to, titles in the areas of machine learning, pattern rec-
ognition, computational intelligence, robotics, computational/statistical learning theory, natural 
language processing, computer vision, game AI, game theory, neural networks, computational 
neuroscience, and other relevant topics, such as machine learning applied to bioinformatics or 
cognitive science, which might be proposed by potential contributors.
PUBLISHED TITLES
BAYESIAN PROGRAMMING 
Pierre Bessière, Emmanuel Mazer, Juan-Manuel Ahuactzin, and Kamel Mekhnacha
UTILITY-BASED LEARNING FROM DATA
Craig Friedman and Sven Sandow
HANDBOOK OF NATURAL LANGUAGE PROCESSING, SECOND EDITION
Nitin Indurkhya and Fred J. Damerau
COST-SENSITIVE MACHINE LEARNING
Balaji Krishnapuram, Shipeng Yu, and Bharat Rao
COMPUTATIONAL TRUST MODELS AND MACHINE LEARNING 
Xin Liu, Anwitaman Datta, and Ee-Peng Lim
MULTILINEAR SUBSPACE LEARNING: DIMENSIONALITY REDUCTION OF  
MULTIDIMENSIONAL DATA 
Haiping Lu, Konstantinos N. Plataniotis, and Anastasios N. Venetsanopoulos
MACHINE LEARNING: An Algorithmic Perspective, Second Edition
Stephen Marsland
A FIRST COURSE IN MACHINE LEARNING
Simon Rogers and Mark Girolami
MULTI-LABEL DIMENSIONALITY REDUCTION 
Liang Sun, Shuiwang Ji, and Jieping Ye
REGULARIZATION, OPTIMIZATION, KERNELS, AND SUPPORT VECTOR MACHINES 
Johan A. K. Suykens, Marco Signoretto, and Andreas Argyriou
ENSEMBLE METHODS: FOUNDATIONS AND ALGORITHMS 
Zhi-Hua Zhou

Edited by
Johan A. K. Suykens
KU LEUVEN, BELGIUM  
Marco Signoretto 
KU LEUVEN, BELGIUM 
Andreas Argyriou
ECOLE CENTRALE PARIS, FRANCE
Chapman & Hall/CRC 
Machine Learning & Pattern Recognition Series
Regularization,  
Optimization, Kernels, and 
Support Vector Machines

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2015 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Version Date: 20140929
International Standard Book Number-13: 978-1-4822-4140-2 (eBook - PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable 
efforts have been made to publish reliable data and information, but the author and publisher cannot 
assume responsibility for the validity of all materials or the consequences of their use. The authors and 
publishers have attempted to trace the copyright holders of all material reproduced in this publication 
and apologize to copyright holders if permission to publish in this form has not been obtained. If any 
copyright material has not been acknowledged please write and let us know so we may rectify in any 
future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or 
hereafter invented, including photocopying, microfilming, and recording, or in any information stor-
age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copy-
right.com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 
Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that pro-
vides licenses and registration for a variety of users. For organizations that have been granted a photo-
copy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are 
used only for identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Contents
Preface
ix
Contributors
xv
1
An Equivalence between the Lasso and Support Vector
Machines
1
Martin Jaggi
2
Regularized Dictionary Learning
27
Annalisa Barla, Saverio Salzo, and Alessandro Verri
3
Hybrid Conditional Gradient-Smoothing Algorithms with
Applications to Sparse and Low Rank Regularization
53
Andreas Argyriou, Marco Signoretto, and Johan A.K. Suykens
4
Nonconvex Proximal Splitting with Computational Errors
83
Suvrit Sra
5
Learning Constrained Task Similarities in Graph-Regularized
Multi-Task Learning
103
Rémi Flamary, Alain Rakotomamonjy, and Gilles Gasso
6
The Graph-Guided Group Lasso for Genome-Wide
Association Studies
131
Zi Wang and Giovanni Montana
7
On the Convergence Rate of Stochastic Gradient Descent
for Strongly Convex Functions
159
Cheng Tang and Claire Monteleoni
8
Detecting Ineﬀective Features for Nonparametric
Regression
177
Kris De Brabanter, Paola Gloria Ferrario, and László Györﬁ
v

vi
Contents
9
Quadratic Basis Pursuit
195
Henrik Ohlsson, Allen Y. Yang, Roy Dong, Michel Verhaegen, and
S. Shankar Sastry
10 Robust Compressive Sensing
217
Esa Ollila, Hyon-Jung Kim, and Visa Koivunen
11 Regularized Robust Portfolio Estimation
237
Theodoros Evgeniou, Massimiliano Pontil, Diomidis Spinellis,
and Nick Nassuphis
12 The Why and How of Nonnegative Matrix Factorization
257
Nicolas Gillis
13 Rank Constrained Optimization Problems in Computer
Vision
293
Ivan Markovsky
14 Low-Rank Tensor Denoising and Recovery via Convex
Optimization
313
Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, and Hisashi Kashima
15 Learning Sets and Subspaces
337
Alessandro Rudi, Guillermo D. Canas, Ernesto De Vito, and
Lorenzo Rosasco
16 Output Kernel Learning Methods
359
Francesco Dinuzzo, Cheng Soon Ong, and Kenji Fukumizu
17 Kernel Based Identiﬁcation of Systems with Multiple
Outputs Using Nuclear Norm Regularization
371
Tillmann Falck, Bart De Moor, and Johan A.K. Suykens
18 Kernel Methods for Image Denoising
401
Pantelis Bouboulis and Sergios Theodoridis
19 Single-Source Domain Adaptation with Target and
Conditional Shift
427
Kun Zhang, Bernhard Schölkopf, Krikamol Muandet, Zhikun Wang,
Zhi-Hua Zhou, and Claudio Persello
20 Multi-Layer Support Vector Machines
457
Marco A. Wiering and Lambert R.B. Schomaker

Contents
vii
21 Online Regression with Kernels
477
Steven Van Vaerenbergh and Ignacio Santamaría
Index
503

This page intentionally left blank
This page intentionally left blank

Preface
Scope
Obtaining reliable models from given data is becoming increasingly im-
portant in a wide range of diﬀerent applications ﬁelds including the pre-
diction of energy consumption, complex networks, environmental modelling,
biomedicine, bioinformatics, ﬁnance, process modelling, image and signal pro-
cessing, brain-computer interfaces, and others. In data-driven modelling ap-
proaches one has witnessed considerable progress in the understanding of es-
timating ﬂexible nonlinear models, learning and generalization aspects, opti-
mization methods, and structured modelling. One area of high impact both
in theory and applications is kernel methods and support vector machines.
Optimization problems, learning, and representations of models are key ingre-
dients in these methods. On the other hand, considerable progress has also
been made on regularization of parametric models, including methods for com-
pressed sensing and sparsity, where convex optimization plays an important
role.
At the international workshop ROKS 2013 Leuven,1 July 8–10, 2013, re-
searchers from diverse ﬁelds were meeting on the theory and applications of
regularization, optimization, kernels, and support vector machines. At this
occasion the present book has been edited as a follow-up to this event, with a
variety of invited contributions from presenters and scientiﬁc committee mem-
bers. It is a collection of recent progress and advanced contributions on these
topics, addressing methods including:
• Regularization: ridge regression, Lasso, group Lasso, graph-guided
group lasso, (weighted) total variation, ℓ1-ℓ2 mixed norms, elastic net,
fused Lasso, ℓp, ℓ0 regularization, sparse coding, hierarchical sparse
coding, nuclear norm, trace norm, K-SVD, basis pursuit denoising,
quadratic basis pursuit denoising, sparse portfolio optimization, low-
rank approximation, overlapped Schatten 1-norm, latent Schatten 1-
norm, tensor completion, tensor denoising, composite penalties, sparse
multicomposite problems, sparse PCA, structured sparsity;
• Optimization:
forward-backward
splitting,
alternating
directions
method of multipliers (ADMM), alternating proximal algorithms, gra-
dient projection, stochastic generalized gradient, bilevel optimization,
1http://www.esat.kuleuven.be/stadius/ROKS2013
ix

x
Preface
non-convex proximal splitting, block coordinate descent algorithm, par-
allel coordinate descent algorithm, stochastic gradient descent, semidef-
inite programming, robust iterative hard thresholding, alternating non-
negative least squares, hierarchical alternating least squares, conditional
gradient algorithm, Frank-Wolfe algorithm, hybrid conditional gradient-
smoothing algorithm;
• Kernels and support vector machines: support estimation, kernel
PCA, subspace learning, reproducing kernel Hilbert spaces, output ker-
nel learning, multi-task learning, support vector machines, least squares
support vector machines, Lagrange duality, primal and dual represen-
tations, feature map, Mercer kernel, Nyström approximation, Fenchel
duality, kernel ridge regression, kernel mean embedding, causality, im-
portance reweighting, domain adaptation, multilayer support vector ma-
chine, Gaussian process regression, kernel recursive least-squares, dictio-
nary learning, quantized kernel LMS;
with application examples in genome-wide data understanding and associ-
ation studies, brain-computer interfaces, optical character recognition, sub-
wavelength imaging, robust portfolio estimation, collaborative ﬁltering, phar-
macology, system identiﬁcation, image denoising, remote sensing image clas-
siﬁcation, wireless communications, and robotic radiosurgery.
The book has intentionally not been divided into diﬀerent parts because
there exist many aspects in common between the main subjects (see Figure).
Regularization
Optimization
Kernels
Support vector machines

Preface
xi
In fact it is our hope that the interested reader may discover new unexpected
links and synergies. Therefore we rather aim at presenting the chapters with
a natural ﬂow of themes going from regularization, optimization, towards ker-
nels and support vector machines.
Chapter-by-Chapter Overview
In Chapter 1, Martin Jaggi studies the relation between support vector
machine classiﬁers and the Lasso technique as used in regression. An equiva-
lence between the two methods is pointed out, together with its consequences
for sparsity and regularization paths. For a given Lasso instance it is explained
how an equivalent SVM instance can be constructed. Also vice versa, for a
given SVM instance, how an equivalent Lasso instance can be constructed. A
kernelized version of the Lasso is also discussed.
In Chapter 2, Annalisa Barla, Saverio Salzo, and Alessandro Verri present
an alternating proximal algorithm suitable for a general dictionary learning
framework with composite convex penalization terms, with stronger conver-
gence properties than the simpler alternating minimization scheme. An anal-
ysis of the problem of computing the proximity operators is made for general
sums of composite functions. The proposed algorithm is applied with mixed
norms in the context of genome-wide data understanding, with identifying la-
tent features (atoms) in array-based comparative genomic hybridization data,
to reveal a genotype-phenotype relationship.
In Chapter 3, Andreas Argyriou, Marco Signoretto, and Johan Suykens
present hybrid conditional gradient-smoothing algorithms with applications
to sparse and low rank regularization for solving composite convex optimiza-
tion problems. The method extends conditional gradient methods for prob-
lems with multiple nonsmooth terms. Examples and comparisons are given on
regularization of matrices with combined ℓ1 and trace norm penalties and a
convex relaxation of sparse PCA.
In Chapter 4, Suvrit Sra presents a framework for solving a broad class
of nonconvex composite objective (regularized) problems, called Nips, which
handles nonsmooth components by proximity operators. It includes forward-
backward splitting with convex costs, incremental forward-backward splitting
(convex), gradient projection (both convex and nonconvex), and the proximal-
point algorithm. Both batch and incremental versions are discussed. The
method is illustrated for sparsity regularized low-rank matrix factorization
and compared with stochastic generalized gradient.
In Chapter 5, Rémi Flamary, Alain Rakotomamonjy, and Gilles Gasso
address the problem of learning constrained task relatedness in a frame-
work of graph-regularized multi-task learning. These similarities are learnt
by optimizing a proxy on the generalization errors of all tasks. A bilevel opti-
mization approach is taken with optimization of the generalization error and
the task parameters. The global optimization is solved by means of a non-
convex proximal splitting algorithm. The interpretability of the task similarity

xii
Preface
matrices is illustrated in applications on a brain-computer interface problem
of recognizing the presence of an event-related potential, and optical character
recognition.
Motivated by genome-wide association studies, in Chapter 6 Zi Wang and
Giovanni Montana propose a penalized linear regression model, the graph-
guided group Lasso in two versions, which can select functionally related genes
and inﬂuential SNPs within these genes that explain the variability in the
trait. The method uses graph and grouping structure on hierarchical biological
variants to drive variable selection at multiple levels. Theoretical properties
of the smoothing eﬀect in the proposed models are presented.
In Chapter 7, Cheng Tang and Claire Monteleoni survey recent work on
the non-asymptotic analysis of stochastic gradient descent on strongly convex
functions. They discuss how the degree of strong convexity and the degree of
smoothness of a function inﬂuence the convergence rate of stochastic gradient
descent.
In Chapter 8, Kris De Brabanter, Paola Gloria Ferrario, and László Györﬁ
investigate the hypothesis that some components of the covariate (feature) vec-
tor are ineﬀective. Consequences of the results for binary pattern recognition
are discussed.
In Chapter 9, Henrik Ohlsson, Allen Yang, Roy Dong, Michel Verhae-
gen, and Shankar Sastry extend classical compressive sensing, which assumes
a linear relation between samples and the unknowns, to nonlinear models.
The case of quadratic relations or second order Taylor expansions is studied.
The extension is based on lifting and convex relaxations, where the ﬁnal for-
mulation takes the form of a semideﬁnite program. The proposed method of
quadratic basis pursuit inherits properties of basis pursuit and classical com-
pressive sensing. Moreover, conditions for perfect recovery are derived. An
application to subwavelength diﬀractive imaging is presented.
In Chapter 10, Esa Ollila, Hyon-Jung Kim, and Visa Koivunen provide
an overview of recent robust approaches to compressive sensing. Huber itera-
tive hard thresholding avoids the computation of the preliminary estimate of
scale which is needed in the generalized iterative hard thresholding method.
Good signal reconstruction performance is shown under various noise dis-
tributions and signal to noise ratios. Improved performance is observed in
comparison with normalized and Lorentzian iterative hard thresholding un-
der non-Gaussian noise.
In Chapter 11, Theodoros Evgeniou, Massimiliano Pontil, Diomidis Spinel-
lis, and Nick Nassuphis propose an approach to estimate large autocorre-
lation portfolios. It employs regularization methods derived from a robust
optimization formulation. An iterative optimization learning algorithm that
estimates sparse portfolios is discussed. The potential of the method is il-
lustrated on time series of daily S&P 500 stock returns. An extension and
algorithm for the more general case of canonical correlation analysis is also
discussed.

Preface
xiii
Motivated by applications of facial feature extraction in image processing,
and topic recovery and document classiﬁcation in text mining and hyperspec-
tral unmixing, in Chapter 12 Nicolas Gillis explains about diﬀerent nonnega-
tive matrix factorization methods. Besides standard NMF algorithms, a recent
subclass of NMF problems, referred to as near-separable NMF, which can be
solved eﬃciently in the presence of noise.
In Chapter 13, Ivan Markovsky emphasizes the role of structured low-rank
approximation problems in data modeling problems. Examples are given in
computer vision related to multidimensional scaling, conic section ﬁtting, fun-
damental matrix estimation, and contour alignment. The contour alignment
problem is reduced to the orthogonal Procrustes problem, which is a low-rank
approximation problem with an additional orthogonality constraint.
In Chapter 14, Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, and Hisashi
Kashima review convex optimization based algorithms for tensor decomposi-
tion. Notions of rank and multilinear rank are explained and related convex
relaxation schemes using trace norm regularization, together with statistical
guarantees. ADMM optimization algorithms for overlapped Schatten 1-norm
and latent Schatten 1-norm regularization, are discussed. Experimental exam-
ples are shown for tensor denoising and tensor completion.
In Chapter 15, Alessandro Rudi, Guillermo Canas, Ernesto De Vito, and
Lorenzo Rosasco study set learning by analyzing its relations with subspace
learning. The latter consists of estimating the smallest linear subspace con-
taining the distribution. They show that the set learning problem can be cast
as a subspace learning problem in the associated feature space. In the sta-
tistical analysis, novel and sharper sample complexity upper bounds and the
consistency of set learning have been established.
In Chapter 16, Francesco Dinuzzo, Cheng Soon Ong, and Kenji Fukumizu
overview a family of regularization techniques called output kernel learning
for learning a multi-task kernel. In these problems the multi-task kernel can
be decomposed as the product of a kernel on the inputs and a kernel on the
task indices. The output kernel learning methods are illustrated on examples
in collaborative ﬁltering, structure discovery in multiclass classiﬁcation, and
pharmacological problems.
In Chapter 17, Tillmann Falck, Bart De Moor, and Johan Suykens propose
a nuclear norm based regularization scheme for kernel-based identiﬁcation
of systems with multiple outputs using nuclear norm regularization. Based
upon least squares support vector machines, the model estimation scheme is
formulated as a constrained optimization problem, resulting in a Lagrange
dual representation of the model, which employs a positive deﬁnite kernel.
In Chapter 18, Pantelis Bouboulis and Sergios Theodoridis discuss kernel
methods for image denoising, based on the kernel ridge regression scheme
and the theory of sparse representations. In the ﬁrst method a rich basis
of functions to model image edges is employed and a regularized ℓ1 error is
taken to account for outliers. In a second method the outliers are explicitly

xiv
Preface
modelled using a sparse vector and the ℓ0 norm of this vector is minimized.
A comparison is made with wavelet methods.
In Chapter 19, Kun Zhang, Bernhard Schölkopf, Krikamol Muandet,
Zhikun Wang, Zhi-Hua Zhou, and Claudio Persello consider domain adap-
tation, where both the distribution of the covariate and the conditional distri-
bution of the target given the covariate, change across domains. Target shift,
conditional shift, and generalized target shift are studied. Kernel mean em-
bedding of conditional and marginal distributions is proposed to handle the
diﬀerent situations. Applications to remote sensing image classiﬁcation are
given.
In Chapter 20, Marco Wiering and Lambert Schomaker present multi-layer
support vector machines. Comparisons with deep neural network architecture
and multiple-kernel learning algorithms are made. Gradient ascent algorithms
for training are discussed. Examples are given on classiﬁcation, regression, and
dimensionality reduction problems.
Motivated by signal processing applications in Chapter 21, Steven Van
Vaerenbergh and Ignacio Santamaría present on-line regression with kernels.
Methods of on-line kernel ridge regression, on-line dictionary learning, kernel
recursive least-squares regression, on-line Gaussian processes, and stochastic
gradient descent with kernels are discussed.
Acknowledgments
We thank the many chapter authors and co-authors for their enthusiasm
and interest in contributing to this edited book.
This edited book has been published within the framework of the European
Research Council2 (ERC) Advanced Grant project3 290923 A-DATADRIVE-
B. We also gratefully acknowledge support from KU Leuven, FWO, IUAP
DYSCO, CoE OPTEC EF/05/006, GOA MANET, iMinds Medical Informa-
tion Technologies, and the European Union Seventh Framework Programme
(FP7 2007-2013) under grant agreement No. 246556.
Johan A.K. Suykens, KU Leuven
Marco Signoretto, KU Leuven
Andreas Argyriou, Ecole Centrale Paris
2http://erc.europa.eu/
3http://www.esat.kuleuven.be/stadius/ADB/

Contributors
Andreas Argyriou
École Centrale Paris
Center for Visual Computing
Annalisa Barla
DIBRIS
Università degli Studi di Genova
Pantelis Bouboulis
Department of Informatics and
Telecommunications
University of Athens
Guillermo D. Canas
Massachusetts Institute of
Technology
Kris De Brabanter
Department of Statistics
Department of Computer Science
Iowa State University
Bart De Moor
KU Leuven
ESAT-STADIUS
IMinds Medical IT
Ernesto De Vito
DIMA
Università degli Studi di Genova
Francesco Dinuzzo
IBM Research
Dublin, Ireland
Roy Dong
Department of Electrical Engineering
and Computer Sciences
University of California, Berkeley
Theodoros Evgeniou
Decision Sciences and Technology
Management INSEAD
Fontainebleau
Tillmann Falck
KU Leuven
ESAT-STADIUS
Paola Gloria Ferrario
Institut für Medizinische Biometrie
und Statistik
Universität zu Lübeck
Rémi Flamary
Laboratoire Lagrange
Observatoire de la Côte d’Azur
Université de Nice Sophia-Antipolis
Kenji Fukumizu
The Institute of Statistical
Mathematics
Tachikawa, Tokyo
Gilles Gasso
LITIS
INSA de Rouen
Nicolas Gillis
Department of Mathematics and
Operational Research
Faculté Polytechnique
Université de Mons
László Györﬁ
Department of Computer Science
and Information Theory
Budapest University of Technology
and Economics
xv

xvi
Contributors
Kohei Hayashi
National Institute of Informatics
Japan
Martin Jaggi
ETH Zürich
Hisashi Kashima
University of Tokyo
Japan
Hyon-Jung Kim
Aalto University
Finland
Visa Koivunen
Aalto University
Finland
Ivan Markovsky
Department ELEC
Vrije Universiteit Brussel
Giovanni Montana
King’s College London
Claire Monteleoni
George Washington University
Krikamol Muandet
Max Planck Institute for Intelligent
Systems
Tübingen
Nick Nassuphis
31 St. Martin’s Lane
London
Henrik Ohlsson
Department of Electrical Engineering
and Computer Sciences
University of California, Berkeley
Esa Ollila
Aalto University
Finland
Cheng Soon Ong
NICTA
Canberra, Australia
Claudio Persello
Max Planck Institute for Intelligent
Systems, Tübingen
University of Trento
Massimiliano Pontil
Department of Computer Science
University College London
Alain Rakotomamonjy
LITIS
Université de Rouen
Lorenzo Rosasco
DIBRIS, Università degli Studi di
Genova
LCSL, Massachusetts Institute of
Technology
Istituto Italiano di Tecnologia
Alessandro Rudi
DIBRIS, Università degli Studi di
Genova
LCSL, Massachusetts Institute of
Technology
Istituto Italiano di Tecnologia
Saverio Salzo
DIMA
Università degli Studi di Genova
Ignacio Santamaría
Department of Communications
Engineering
University of Cantabria
S. Shankar Sastry
Department of Electrical Engineering
and Computer Sciences
University of California, Berkeley

Contributors
xvii
Bernhard Schölkopf
Max Planck Institute for Intelligent
Systems
Tübingen
Lambert R.B. Schomaker
Institute of Artiﬁcial Intelligence and
Cognitive Engineering
University of Groningen
Marco Signoretto
KU Leuven
ESAT-STADIUS
Diomidis Spinellis
Department of Management Science
and Technology
Athens University of Economics and
Business
Suvrit Sra
Max Planck Institute Intelligent
Systems, Tübingen
Carnegie Mellon University,
Pittsburgh
Johan A.K. Suykens
KU Leuven
ESAT-STADIUS
Taiji Suzuki
Tokyo Institute of Technology
Japan
Cheng Tang
George Washington University
Sergios Theodoridis
Department of Informatics and
Telecommunications
University of Athens
Ryota Tomioka
Toyota Technological Institute at
Chicago
Steven Van Vaerenbergh
Department of Communications
Engineering
University of Cantabria
Michel Verhaegen
Delft Center for Systems and Control
Delft University
Alessandro Verri
DIBRIS
Università degli Studi di Genova
Allen Y. Yang
Department of Electrical Engineering
and Computer Sciences
University of California, Berkeley
Zhikun Wang
Max Planck Institute for Intelligent
Systems
Tübingen
Zi Wang
Imperial College London
Marco A. Wiering
Institute of Artiﬁcial Intelligence and
Cognitive Engineering
University of Groningen
Kun Zhang
Max Planck Institute for Intelligent
Systems
Tübingen
Zhi-Hua Zhou
National Key Laboratory for Novel
Software Technology
Nanjing University

This page intentionally left blank
This page intentionally left blank

Chapter 1
An Equivalence between the Lasso
and Support Vector Machines
Martin Jaggi∗
ETH Zürich
1.1
Introduction ......................................................
2
1.2
Linear Classiﬁers and Support Vector Machines ................
5
1.2.1
Support Vector Machines ................................
6
1.2.2
Soft-Margin SVMs .......................................
8
1.3
The Equivalence ..................................................
9
1.3.1
Warm-Up: Equivalence between SVM and
Non-Negative Lasso ......................................
9
1.3.2
(Lasso ⪯SVM): Given a Lasso Instance, Constructing
an Equivalent SVM Instance ............................
10
1.3.3
(SVM ⪯Lasso): Given an SVM Instance, Constructing
an Equivalent Lasso Instance ............................
11
1.4
Implications and Remarks .......................................
16
1.4.1
Sublinear Time Algorithms for Lasso and SVMs .......
16
1.4.2
A Kernelized Lasso ......................................
16
1.4.3
The Pattern of Support Vectors, in the View of Lasso
Sparsity ..................................................
18
1.4.4
Screening Rules for Support Vector Machines ..........
19
1.4.5
Regularization Paths and Homotopy Methods ..........
19
1.5
Conclusions .......................................................
21
1.6
Appendix: Some Soft-Margin SVM Variants That Are
Equivalent to (1.1) ...............................................
22
Bibliography ......................................................
23
Overview
We investigate the relation of two fundamental tools in machine learn-
ing and signal processing, which are the support vector machine (SVM) for
classiﬁcation, and the Lasso technique used in regression. We show that the
resulting optimization problems are equivalent, in the following sense. Given
∗jaggi@inf.ethz.ch
1

2
Regularization, Optimization, Kernels, and Support Vector Machines
any instance of an ℓ2-loss soft-margin (or hard-margin) SVM, we construct a
Lasso instance having the same optimal solutions, and vice versa.
As a consequence, many existing optimization algorithms for both SVMs
and Lasso can also be applied to the respective other problem instances. Also,
the equivalence allows for many known theoretical insights for SVM and Lasso
to be translated between the two settings. One such implication gives a simple
kernelized version of the Lasso, analogous to the kernels used in the SVM
setting. Another consequence is that the sparsity of a Lasso solution is equal to
the number of support vectors for the corresponding SVM instance, and that
one can use screening rules to prune the set of support vectors. Furthermore,
we can relate sublinear time algorithms for the two problems, and give a new
such algorithm variant for the Lasso. We also study the regularization paths
for both methods.
1.1
Introduction
Linear classiﬁers and kernel methods, and in particular the support vector
machine (SVM) [9], are among the most popular standard tools for classiﬁca-
tion. On the other hand, ℓ1-regularized least squares regression, i.e., the Lasso
estimator [41], is one of the most widely used tools for robust regression and
sparse estimation.
Along with the many successful practical applications of SVMs and the
Lasso in various ﬁelds, there is a vast amount of existing literature1 on the
two methods themselves, considering both theory and also algorithms for each
of the two. However, the two research topics developed largely independently
and were not much set into context with each other so far.
In this chapter, we attempt to better relate the two problems, with two
main goals in mind. On the algorithmic side, we show that many of the existing
algorithms for each of the two problems can be set into comparison, and can
be directly applied to the other respective problem. As a particular example
of this idea, we can also apply the recent sublinear time SVM algorithm by [8]
to any Lasso problem, resulting in a new alternative sublinear time algorithm
variant for the Lasso.
On the other hand, we can relate and transfer existing theoretical results
between the literature for SVMs and the Lasso. In this spirit, a ﬁrst example
is the idea of the kernel trick. Originally employed for SVMs, this powerful
concept has allowed for lifting most insights from the linear classiﬁer setting
also to the more general setting of non-linear classiﬁers, by using an implicit
higher dimensional space. Here, by using our equivalence, we propose a simple
1As of January 2014, Google Scholar returned 300, 000 publications containing the term
Support Vector Machine, and over 20, 000 for Lasso regression.

An Equivalence between the Lasso and Support Vector Machines
3
kernelized variant of the Lasso, being equivalent to the well-researched use of
kernels in the SVM setting.
As another example, we can also transfer some insights in the other di-
rection, from the Lasso to SVMs. The important datapoints, i.e., those that
deﬁne the solution to the SVM classiﬁer, are called the support vectors. Hav-
ing a small set of support vectors is crucial for the practical performance of
SVMs. Using our equivalence, we see that the set of support vectors for a given
SVM instance is exactly the same as the sparsity pattern of the corresponding
Lasso solution.
Screening rules are a way of pre-processing the input data for a Lasso
problem, in order to identify inactive variables. We show that screening rules
can also be applied to SVMs, in order to eliminate potential support vectors
beforehand, and thereby speeding up the training process by reducing the
problem size.
Finally, we study the complexity of the solution paths of Lasso and SVMs,
as the regularization parameter changes. We discuss path algorithms that ap-
ply to both problems, and also translate a result on the Lasso path complexity
to show that a single SVM instance, depending on the scaling of the data, can
have very diﬀerent patterns of support vectors.
Support Vector Machines. In this chapter, we focus on large margin
linear classiﬁers, and more precisely on those SVM variants whose dual opti-
mization problem is of the form
min
x∈△∥Ax∥2 .
(1.1)
Here the matrix A ∈Rd×n contains all n datapoints as its columns, and △
is the unit simplex in Rn, i.e., the set of probability vectors, that is the non-
negative vectors whose entries sum up to one. The formulation (1.1) includes
the commonly used soft-margin SVM with ℓ2-loss. It includes both the one or
two classes variants, both with or without using a kernel, and both using a
(regularized) oﬀset term (allowing hyperplanes not passing through the origin)
or no oﬀset. We will explain these variants and the large margin interpretation
of this optimization problem in more detail in Section 1.2.
Lasso. On the other hand, the Lasso [41] is given by the quadratic program
min
x∈♦∥Ax −b∥2 .
(1.2)
It is also known as the constrained variant of ℓ1-regularized least squares
regression. Here the right hand side b is a ﬁxed vector b ∈Rd, and ♦is the
ℓ1-unit-ball in Rn. Note that the 1-norm ∥.∥1 is the sum of the absolute values
of the entries of a vector. Sometimes in practice, one would like to have the
constraint ∥x∥1 ≤r for some value r > 0, instead of the simple unit-norm
case ∥x∥1 ≤1. However, in that case it is enough to simply re-scale the input

4
Regularization, Optimization, Kernels, and Support Vector Machines
matrix A by a factor of r, in order to obtain our above formulation (1.2) for
any general Lasso problem.
In applications of the Lasso, it is important to distinguish two alternative
interpretations of the data matrix A, which deﬁnes the problem instance (1.2):
on one hand, in the setting of sparse regression, the matrix A is usually called
the dictionary matrix, with its columns A:j being the dictionary elements,
and the goal being to approximate the single vector b by a combination of
few dictionary vectors. On the other hand, if the Lasso problem is interpreted
as feature-selection, then each row Ai: of the matrix A is interpreted as an
input vector, and for each of those, the Lasso is approximating the response
bi to input row Ai:. The book [3] gives a recent overview of Lasso-type meth-
ods and their broad applications. While the penalized variant of the Lasso
(meaning that the term ∥x∥1 is added to the objective instead of imposed
as a constraint) is also popular in applications, here we focus on the original
constrained variant.
The Equivalence. We will prove that the two problems (1.1) and (1.2)
are indeed equivalent, in the following sense. For any Lasso instance given
by (A, b), we construct an equivalent (hard-margin) SVM instance, having the
same optimal solution. This will be a simple reduction preserving all objective
values. On the other hand, the task of ﬁnding an equivalent Lasso instance
for a given SVM appears to be a harder problem. Here we show that there
always exists such an equivalent Lasso instance, and furthermore, if we are
given a weakly separating vector for the SVM (formal deﬁnition to follow soon
below), then we can explicitly construct the equivalent Lasso instance. This
reduction also applies to the ℓ2-loss soft-margin SVM, where we show that a
weakly separating vector is trivial to obtain, making the reduction eﬃcient.
The reduction does not require that the SVM input data is separable.
Our shown equivalence is on the level of the (SVM or Lasso) training
formulations; we are not making any claims on the performance of the two
diﬀerent kinds of methods on unseen test data.
On the way to this goal, we will also explain the relation to the “non-
negative” Lasso variant when the variable vector x is required to lie in the
simplex, i.e.,
min
x∈△∥Ax −b∥2 .
(1.3)
It turns out the equivalence of the optimization problems (1.1) and (1.3) is
straightforward to see. Our main contribution is to explain the relation of
these two optimization problems to the original Lasso problem (1.2), and to
study some of the implications of the equivalence.
Related Work. The early work of [18] has already signiﬁcantly deepened
the joint understanding of kernel methods and the sparse coding setting of
the Lasso. Despite its title, [18] is not addressing SVM classiﬁers, but in fact
the ε-insensitive loss variant of support vector regression (SVR), which the

An Equivalence between the Lasso and Support Vector Machines
5
author proves to be equivalent to a Lasso problem where ε then becomes the
ℓ1-regularization. Unfortunately, this reduction does not apply anymore when
ε = 0, which is the case of interest for standard hinge-loss SVR, and also for
SVMs in the classiﬁcation setting, which are the focus of our work here.
Another reduction has been known if the SVR insensitivity parameter ε is
chosen close enough to one. In that case, [32] has shown that the SVR problem
can be reduced to SVM classiﬁcation with standard hinge-loss. Unfortunately,
this reduction does not apply to Lasso regression.
In a diﬀerent line of research, [28] have studied the relation of a dual
variant of the Lasso to the primal of the so called potential SVM originally
proposed by [23, 24], which is not a classiﬁer but a specialized method of
feature selection.
In the application paper [15] in the area of computational biology, the au-
thors already suggested making use of the “easier” direction of our reduction,
reducing the Lasso to a very particular SVM instance. The idea is to employ
the standard trick of using two non-negative vectors to represent a point in
the ℓ1-ball [1, 7]. Alternatively, this can also be interpreted as considering an
SVM deﬁned by all Lasso dictionary vectors together with their negatives (2n
many points). We formalize this interpretation more precisely in Section 1.3.2.
The work of [15] does not address the SVM regularization parameter.
Notation. The following three sets of points will be central for our inves-
tigations. We denote the unit simplex, and the ﬁlled simplex, as well as the
ℓ1-unit-ball in Rn as follows.
△
:=
{x ∈Rn | x ≥0, P
i xi = 1} ,
▲
:=
{x ∈Rn | x ≥0, P
i xi ≤1} ,
♦
:=
{x ∈Rn | ∥x∥1 ≤1} .
The 1-norm ∥.∥1 is the sum of the absolute values of the entries of a vector.
The standard Euclidean norm is written as ∥.∥.
For a given matrix A ∈Rd×n, we write Ai ∈Rd, i ∈[1..n] for its columns.
We use the notation AS := {Ax | x ∈S} for subsets S ⊆Rd and matrices A.
The convex hull of a set S is written as conv(S). By 0 and 1 we denote the
all-zero and all-ones vectors in Rn, and In is the n × n identity matrix. We
write (A|B) for the horizontal concatenation of two matrices A, B.
1.2
Linear Classiﬁers and Support Vector Machines
Linear classiﬁers have become the standard workhorse for many machine
learning problems. Suppose we are given n datapoints Xi ∈Rd, together with
their binary labels yi ∈{±1}, for i ∈[1..n].

6
Regularization, Optimization, Kernels, and Support Vector Machines
As we illustrate in Figure 1.1, a linear classiﬁer is a hyperplane that par-
titions the space Rd into two parts, such that hopefully each point with a
positive label will be on one side of the plane, and the points of negative la-
bel will be on the other site. Writing the classiﬁer as the normal vector w of
that hyperplane, we can formally write this separation as XT
i w > 0 for those
points i with yi
= +1, and XT
i w < 0 if yi
= −1, assuming we consider
hyperplanes that pass through the origin.
Xj
yjXj
yiXi
w
Xi
Xj
w
FIGURE 1.1: A linear classiﬁer: illustration of the separation of two point
classes, for a normal vector w. Here we highlight two points i, j of diﬀerent
labels, with yi = +1 and yj = −1.
1.2.1
Support Vector Machines
The popular concept of support vector machines (SVM) is precisely this
linear classiﬁer idea as we have introduced above, with one important addi-
tion: instead of being satisﬁed with any arbitrary one among all separating
hyperplanes, we want to ﬁnd the hyperplane that separates the two point
classes by the best possible margin.
The margin is deﬁned as the smallest distance to the hyperplane among
all the datapoints. Maximizing this margin over all possible linear classiﬁers w
can be simply formalized as the following optimization problem:
max
w∈Rd min
i
yiXT
i
w
∥w∥,
i.e., maximizing the smallest projection onto the direction w, among all dat-
apoints.
The SVM optimization problem (1.1) that we deﬁned in the introduction
is very closely related to this form of optimization problem, as follows. If we
take the Lagrange dual of problem (1.1), we obtain a problem of exactly this
margin maximization type, namely
max
x∈△min
i
AT
i
Ax
∥Ax∥.
(1.4)
Here we think of the columns of A being the SVM datapoints with their signs

An Equivalence between the Lasso and Support Vector Machines
7
Ai = yiXi. For an overview of Lagrange duality, we refer the reader to [2,
Section 5], or see for example [12, Appendix A] for the SVM case here. An
alternative and slightly easier way to obtain this dual problem is to compute
the simple “linearization” dual function as in [22, 25], which avoids the notion
of any dual variables. When starting from the non-squared version of the SVM
problem (1.1), this also gives the same formulation (1.4).
No matter if we use problem formulation (1.1) or (1.4), any feasible weight
vector x readily gives us a candidate classiﬁer w = Ax, represented as a convex
combination of the datapoints, because the weight vectors x ∈△⊂Rn lie in
the simplex. The datapoints corresponding to non-zero entries in x are called
the support vectors.
Several other SVM variants of practical interest also have the property
that their dual optimization problem is of the form (1.1), as we will discuss
in the next Subsection 1.3.2.
Kernelization. A crucial and widely used observation is that both opti-
mization problems (1.1) and (1.4) are formulated purely in terms of the inner
products between pairs of datapoints Ai := yiXi, meaning that they can di-
rectly be optimized in the kernel case [9], provided that we only have access to
the entries of the matrix AT A ∈Rn×n, but not the explicit features A ∈Rd×n.
The matrix K = AT A is called the kernel matrix in the literature. Using this
notation, the SVM dual optimization problem (1.1) becomes
min
x∈△xT Kx .
Separation, and Approximate Solutions. It is natural to measure the
quality of an approximate solution x to the SVM problem as the attained
margin, which is precisely the attained value in the above problem (1.4).
Deﬁnition 1.1. A vector w ∈Rd is called σ-weakly separating for the SVM
instances (1.1) or (1.4), respectively, for a parameter σ ≥0, if it holds that
AT
i
w
∥w∥≥σ
∀i ,
meaning that w attains a margin of separation of σ.
The margin value σ in this deﬁnition, or also the objective in (1.4), can
be interpreted as a useful certiﬁcate for the attained optimization quality as
follows. If we take some x (the separating vector now being interpreted as
w = Ax), then the duality gap is given by the diﬀerence of the margin value
from the corresponding objective ∥Ax∥in problem (1.1). This gap function
is a certiﬁcate for the approximation quality (since the unknown optimum
must lie within the gap), which makes it a very useful stopping criterion for
optimizers; see, e.g., [22, 12, 8, 25].
The simple perceptron algorithm [34] is known to return a σ-weakly sepa-
rating solution to the SVM after O(1/ε2) iterations, for ε := σ∗−σ being the
additive error, if σ∗is the optimal solution to (1.1) and (1.4).

8
Regularization, Optimization, Kernels, and Support Vector Machines
1.2.2
Soft-Margin SVMs
For the successful practical application of SVMs, the soft-margin concept of
tolerating outliers is of central importance. Here we recall that the soft-margin
SVM variants using ℓ2-loss, with regularized oﬀset or no oﬀset, both in the
one-class and the two-class case, can also be formulated in the form (1.1).
This fact is known in the SVM literature [37, 26, 43], and can be formalized
as follows.
The two-class soft-margin SVM with squared loss (ℓ2-loss), without oﬀset
term, is given by the primal optimization problem
min
¯
w∈Rd, ρ∈R,
ξ∈Rn
1
2 ∥¯w∥2 −ρ + C
2
P
i ξ2
i
s.t.
yi · ¯wT Xi ≥ρ −ξi ∀i ∈[1..n] .
(1.5)
For each datapoint, we have introduced a slack-variable ξi which is penalized
in the objective function in the case that the point does violate the margin.
Here C > 0 is the regularization parameter, steering the tradeoﬀbetween large
margin and punishing outliers. In the end, ρ/∥¯w∥will be the attained margin
of separation. Note that in the classical SVM formulation, the margin parame-
ter ρ is usually ﬁxed to one instead, while ρ is explicitly used in the equivalent
ν-SVM formulation known in the literature; see, e.g., [37]. The equivalence of
the soft-margin SVM dual problem to the optimization problem (1.1) is stated
in the following Lemma:
Lemma 1.1. The dual of the soft-margin SVM (1.5) is an instance of the
classiﬁer formulation (1.1), that is, minx∈△∥Ax∥2 , with
A :=

Z
1
√
C In

∈R(d+n)×n
where the data matrix Z ∈Rd×n consists of the n columns Zi := yiXi.
Proof. Given in Appendix 1.6 for completeness, using standard Lagrange du-
ality.
Not all SVM variants have our desired structure of the dual problem. For
example, the hinge-loss (or ℓ1-loss) SVM refers to the setting where the outliers
are penalized according to their margin-violation ξi, not the squared values ξ2
i
as we use here. Changing the loss function in the primal optimization problem
also aﬀects the dual problem, so that the dual of the ℓ1-loss SVM is not of
the form (1.1). However in practice, it is known that both SVM variants with
ℓ1- or ℓ2-loss do perform similarly well for most applications [27, 6].
Obtaining a Weakly Separating Vector for the ℓ2-Loss Soft-
Margin SVM. By the above lemma, we observe that a weakly separating
vector is trivial to obtain for the ℓ2-loss SVM. This holds without any as-
sumptions on the original input data (Xi, yi). We set w :=
 0
1
√n 1

∈Rd+n to

An Equivalence between the Lasso and Support Vector Machines
9
the all-one vector only on the second block of coordinates, rescaled to unit
length. Clearly, this direction w attains a separation margin of AT
i
w
∥w∥=
  yiXi
1
√
C ei
T  0
1
√n 1

=
1
√
nC > 0 for all points i in Deﬁnition 1.1.
Incorporating an Oﬀset Term. Our above SVM formulation also al-
lows the use of an oﬀset (or bias) variable b ∈R to obtain a classiﬁer that does
not necessarily pass through the origin. Formally, the separation constraints
then become yi · (wT Xi + b) ≥ρ −ξi ∀i ∈[1..n]. There is a well-known trick
to eﬃciently emulate such an oﬀset parameter while still using our formula-
tion (1.5), by simply increasing the dimensionality of Xi and w by one, and
adding a ﬁxed value of one as the last coordinate to each of the datapoints Xi;
see, e.g., [26, 43]. As a side-eﬀect, the oﬀset b2 is then also regularized in the
new term ∥w∥2. Nevertheless, if desired, the eﬀect of this additional regular-
ization can be made arbitrarily weak by re-scaling the ﬁxed additional feature
value from one to a larger value.
One-Class SVMs. All mentioned properties in this section also hold for
the case of one-class SVMs, by setting all labels yi to one, resulting in the same
form of optimization problems (1.1) and (1.4). One class SVMs are popular,
for example, for anomaly or novelty detection applications.
1.3
The Equivalence
1.3.1
Warm-Up: Equivalence between SVM and Non-Negative
Lasso
Before we investigate the “real” Lasso problem (1.2) in the next two sub-
sections, we will warm-up by considering the non-negative variant (1.3). It is
a simple observation that the non-negative Lasso (1.3) is directly equivalent
to the dual SVM problem (1.1) by a translation:
Equivalence by Translation. Given a non-negative Lasso instance (1.3),
we can translate each column vector of the matrix A by the vector −b. Doing
so, we precisely obtain an SVM instance (1.1), with the data matrix being
˜A := A −b1T ∈Rd×n .
Here we have crucially used the simplex domain, ensuring that b1T x = b for
any x ∈△. To summarize, for those two optimization problems, the described
translation precisely preserves all the objective values of all feasible points for
both problems (1.3) and (1.1), that is, for all x ∈△. This is why we say that
the problems are equivalent.

10
Regularization, Optimization, Kernels, and Support Vector Machines
The reduction in the other direction — i.e., reducing an SVM instance (1.1)
to a non-negative Lasso instance (1.3) — is made trivial by choosing b := 0.
Now to relate the SVM to the “real” Lasso, the same translation idea is of
crucial importance. We explain the two reductions in the following subsections.
1.3.2
(Lasso ⪯SVM): Given a Lasso Instance, Constructing
an Equivalent SVM Instance
(This reduction is signiﬁcantly easier than the other direction.)
Parameterizing the ℓ1-Ball as a Convex Hull. One of the main prop-
erties of polytopes — if not the main one — is that every polytope can be
represented as the convex hull of its vertices [45]. When expressing an arbi-
trary point in the polytope as a convex combination of some vertices, this
leads to the standard concept of barycentric coordinates.
In order to represent the ℓ1-ball ♦by a simplex △, this becomes partic-
ularly simple. The ℓ1-ball ♦is the convex hull of its 2n vertices, which are
{±ei | i ∈[1..n]}, illustrating why ♦is also called the cross-polytope.
The barycentric representation of the ℓ1-ball therefore amounts to the
simple trick of using two non-negative variables to represent each real variable,
which is standard, for example, when writing linear programs in standard
form. For ℓ1 problems such as the Lasso, this representation was known very
early [1, 7]. Formally, any n-vector x⋄∈♦can be written as
x⋄= (In |−In)x△for x△∈△⊂R2n .
Here x△is a 2n-vector, and we have used the notation (A|B) for the horizontal
concatenation of two matrices A, B.
Note that the barycentric representation is not a bijection in general, as
there can be several x△∈△⊂R2n representing the same point x⋄∈Rn.
The Equivalent SVM Instance. Given a Lasso instance of the
form (1.2), that is, minx∈♦∥Ax −b∥2, we can directly parameterize the ℓ1-ball
by the 2n-dimensional simplex as described above. By writing (In |−In)x△for
any x ∈♦, the objective function becomes ∥(A |−A)x△−b∥2. This means we
have obtained the equivalent non-negative regression problem of the form (1.3)
over the domain x△∈△, which, by our above remark on translations, is equiv-
alent to the SVM formulation (1.1), i.e.,
min
x△∈△
 ˜Ax△
2 ,
where the data matrix is given by
˜A := (A |−A) −b1T ∈Rd×2n .

An Equivalence between the Lasso and Support Vector Machines
11
The additive rank-one term b1T for 1 ∈R2n again just means that the vector b
is subtracted from each original column of A and −A, resulting in a translation
of the problem. So we have obtained an equivalent SVM instance consisting
of 2n points in Rd.
Note that this equivalence not only means that the optimal solutions of
the Lasso and the SVM coincide, but indeed gives us the correspondence of all
feasible points, preserving the objective values: for any points solution x ∈Rn
to the Lasso, we have a feasible SVM point x△∈△⊂R2n of the same
objective value, and vice versa.
1.3.3
(SVM ⪯Lasso): Given an SVM Instance, Constructing
an Equivalent Lasso Instance
This reduction is harder to accomplish than the other direction we ex-
plained before. Given an instance of an SVM problem (1.1), we suppose that
we have a (possibly non-optimal) σ-weakly separating vector w ∈Rd available,
for some (small) value σ > 0. Given w, we will demonstrate in the following
how to construct an equivalent Lasso instance (1.2).
Perhaps surprisingly, such a weakly separating vector w is trivial to obtain
for the ℓ2-loss soft-margin SVM, as we have observed in Section 1.2.2 (even
if the SVM input data is not separable). Also for hard-margin SVM variants,
ﬁnding such a weakly separating vector for a small σ is still signiﬁcantly easier
than the ﬁnal goal of obtaining a near-perfect (σ∗−ε)-separation for a small
precision ε. It corresponds to running an SVM solver (such as the perceptron
algorithm) for only a constant number of iterations. In contrast, obtaining a
better ε-accurate solution by the same algorithm would require O(1/ε2) iter-
ations, as mentioned in Section 1.2.
The Equivalent Lasso Instance. Formally, we deﬁne the Lasso in-
stance ( ˜A,˜b) as the translated SVM datapoints
˜A :=

Ai + ˜b
 i ∈[1..n]
	
together with the right hand side
˜b := −w
∥w∥· D2
σ
.
Here D > 0 is a strict upper bound on the length of the original SVM
datapoints, i.e., ∥Ai∥< D ∀i.
By deﬁnition of ˜A, the resulting new Lasso objective function is
 ˜Ax −˜b
 =
(A + ˜b1T )x −˜b
 =
Ax + (1T x −1)˜b
 .
(1.6)
Therefore, this objective coincides with the original SVM objective (1.1), for
any x ∈△(meaning that 1T x = 1). However, this does not necessarily hold

12
Regularization, Optimization, Kernels, and Support Vector Machines
for the larger part of the Lasso domain when x ∈♦\ △. In the following
discussion and the main Theorem 1.1, we will prove that all those candidates
x ∈♦\△can be discarded from the Lasso problem, as they do not contribute
to any optimal solutions.
As a side-remark, we note that the quantity D
σ that determines the mag-
nitude of our translation is a known parameter in the SVM literature. [4, 37]
have shown that the VC-dimension of an SVM, a measure of “diﬃculty” for
the classiﬁer, is always lower than D2
σ2 . Note that by the deﬁnition of separa-
tion, σ ≤D always holds.
Geometric Intuition. Geometrically, the Lasso problem (1.2) is to com-
pute the smallest Euclidean distance of the set A♦to the point b ∈Rd. On
the other hand the SVM problem — after translating by b — is to mini-
mize the distance of the smaller set A△⊂A♦to the point b. Here we have
used the notation AS := {Ax | x ∈S} for subsets S ⊆Rd and linear maps
A (it is easy to check that linear maps do preserve convexity of sets, so that
conv(AS) = A conv(S)).
Intuitively, the main idea of our reduction is to mirror our SVM points Ai
at the origin, so that both the points and their mirrored copies — and there-
fore the entire larger polytope A♦— do end up lying “behind” the separating
SVM margin. The hope is that the resulting Lasso instance will have all its
optimal solutions be non-negative, and lying in the simplex. Surprisingly, this
can be done, and we will show that all SVM solutions are preserved (and no
new solutions are introduced) when the feasible set △is extended to ♦. In
the following we will formalize this precisely, and demonstrate how to trans-
late along our known weakly separating vector w so that the resulting Lasso
problem will have the same solution as the original SVM.
Properties of the Constructed Lasso Instance. The following the-
orem shows that for our constructed Lasso instance, all interesting feasible
points are contained in the simplex △. By our previous observation (1.6), we
already know that all those candidates are feasible for both the Lasso (1.2)
and the SVM (1.1), and obtain the same objective values in both problems.
In other words, we have a one-to-one correspondence between all feasible
points for the SVM (1.1) on one hand, and the subset △⊂♦of feasible
points of our constructed Lasso instance (1.2), preserving all objective values.
Furthermore, we have that in this Lasso instance, all points in ♦\ △are
strictly worse than the ones in △. Therefore, we have also shown that all
optimal solutions must coincide.
Theorem 1.1. For any candidate solution x⋄∈♦to the Lasso problem (1.2)
deﬁned by ( ˜A,˜b), there is a feasible vector x△∈△in the simplex, of the same
or better Lasso objective value γ.
Furthermore, this x△∈△attains the same objective value γ in the original
SVM problem (1.1).

An Equivalence between the Lasso and Support Vector Machines
13
On the other hand, every x△∈△is of course also feasible for the Lasso,
and attains the same objective value there, again by (1.6).
Proof. The proof follows directly from the two main facts given in Proposi-
tions 1.1 and 1.2 below, which state that “ﬂipping negative signs improves
the objective”, and that “scaling up improves for non-negative vectors”, re-
spectively. We will see below why these two facts hold, which is precisely by
the choice of the translation ˜b along a weakly separating vector w, in order to
deﬁne our Lasso instance.
We assume that the given x⋄does not already lie in the simplex. Now
by applying Propositions 1.1 and 1.2, we obtain x△∈△, of a strictly better
objective value γ for problem (1.3). By the observation (1.6) about the Lasso
objective, we know that the original SVM objective value attained by this x△
is equal to γ.
Proposition 1.1 (Flipping negative signs improves the objective). Consider
the Lasso problem (1.2) deﬁned by ( ˜A,˜b), and assume that x⋄∈♦has some
negative entries.
Then there is a strictly better solution x▲∈▲having only non-negative
entries.
Proof. We are given x⋄̸= 0, having at least one negative coordinate. Deﬁne
x▲̸= 0 as the vector you get by ﬂipping all the negative coordinates in x⋄.
We deﬁne δ ∈▲to be the diﬀerence vector corresponding to this ﬂipping,
i.e., δi := −(x⋄)i if (x⋄)i < 0, and δi := 0 otherwise, so that x▲:= x⋄+ 2δ
gives x▲∈▲. We want to show that with respect to the quadratic objective
function, x▲is strictly better than x⋄. We do this by showing that the following
diﬀerence in the objective values is strictly negative:
 ˜Ax▲−˜b
2 −
 ˜Ax⋄−˜b
2
=
∥c + d∥2 −∥c∥2
=
cT c + 2cT d + dT d −cT c = (2c + d)T d
=
4( ˜Ax⋄−˜b + ˜Aδ)T ˜Aδ
=
4( ˜A(x⋄+ δ) −˜b)T ˜Aδ
where in the above calculations we have used that ˜Ax▲= ˜Ax⋄+ 2 ˜Aδ, and we
substituted c := ˜Ax⋄−˜b and d := 2 ˜Aδ. Interestingly, x⋄+ δ ∈▲, since this
addition just sets all previously negative coordinates to zero.
The proof then follows from Lemma 1.3 below.
Proposition 1.2 (Scaling up improves for non-negative vectors). Consider
the Lasso problem (1.2) deﬁned by ( ˜A,˜b), and assume that x▲∈▲has ∥x▲∥1 <
1.
Then we obtain a strictly better solution x△∈△by linearly scaling x▲.
Proof. The proof follows along similar lines as the above proposition. We are
given x▲̸= 0 with ∥x▲∥1 < 1. Deﬁne x△as the vector we get by scaling up

14
Regularization, Optimization, Kernels, and Support Vector Machines
x△:= λx▲by λ > 1 such that ∥x△∥1 = 1. We want to show that with respect
to the quadratic objective function, x△is strictly better than x▲. As in the
previous proof, we again do this by showing that the following diﬀerence in
the objective values is strictly negative:
 ˜Ax△−˜b
2 −
 ˜Ax▲−˜b
2
=
∥c + d∥2 −∥c∥2
=
cT c + 2cT d + dT d −cT c = (2c + d)T d
=
λ′(2 ˜Ax▲−2b + λ′ ˜Ax▲)T ˜Ax▲
=
2λ′( ˜A
 1 + λ′
2

x▲−˜b)T ˜Ax▲
where in the above calculations we have used that ˜Ax△= λ ˜Ax▲for λ > 1,
and we substituted c := ˜Ax▲−˜b and d := ˜Ax△−˜Ax▲= (λ −1) ˜Ax▲=: λ′ ˜Ax▲
for λ′ := λ −1 > 0. Note that x△:= (1 + λ′)x▲∈△so
 1 + λ′
2

x▲∈▲.
The proof then follows from Lemma 1.3 below.
Deﬁnition 1.2. For a given axis vector w ∈Rd, the cone with axis
w, angle α ∈(0, π
2 ) with tip at the origin is deﬁned as cone(w, α) :=

x ∈Rd  ∡(x, w) ≤α
	
, or equivalently
xT w
∥x∥∥w∥≥cos α. By
◦
cone (w, α) we
denote the interior of the convex set cone(w, α), including the tip 0.
σ
D
◦
cone
!
w, ⇡
2 −↵
"
◦
cone
!
w, ↵
"
↵
w
A
FIGURE 1.2: Illustration of the separation idea from Lemma 1.2, showing
the cone of vectors that are still weakly separating for the set of points A.
Here we used the angle α := arccos( σ
D).
Lemma 1.2 (Separation). Let w be some σ-weakly separating vector for the
SVM (1.1) for σ > 0. Then
i) A▲⊆
◦
cone(w, arccos( σ
D))
ii) Any vector in cone(w, arcsin( σ
D)) is still σ′-weakly separating for A for
some σ′ > 0.

An Equivalence between the Lasso and Support Vector Machines
15
Proof. i) Deﬁnition 1.1 of weakly separating, and using that ∥Ai∥< D.
ii) For any unit length vector v ∈cone(w, arcsin( σ
D)), every other vector
having a zero or negative inner product with this v must have angle at least
π
2 −arcsin( σ
D) = arccos( σ
D) with the cone axis w. However, by using i), we
have A△⊆
◦
cone(w, arccos( σ
D)), so every column vector of A must have strictly
positive inner product with v, or in other words v is σ′-weakly separating for
A (for some σ′ > 0). See also the illustration in Figure 1.2.
w
˜b
˜A
−˜A
FIGURE 1.3: Illustration of Lemma 1.3. Recall that the translated points are
deﬁned by ˜A :=

Ai + ˜b
 i ∈[1..n]
	
, where the translation is ˜b := −w
∥w∥· D2
σ .
Lemma 1.3. Let w be some σ-weakly separating vector for the SVM for
σ > 0. Then we claim that the translation by the vector ˜b := −w
∥w∥· D2
σ
has
the following properties. For any pair of vectors x, δ ∈▲, δ ̸= 0, we have that
( ˜Ax −˜b)T (−˜Aδ) > 0.
Proof. (See also Figure 1.3). By deﬁnition of the translation ˜b, we have that
the entire Euclidean ball of radius D around the point −˜b — and there-
fore also the point set −˜A▲and in particular v := −˜Aδ — is contained in
cone(w, arcsin( σ
D)). Therefore by Lemma 1.2 ii), v is separating for A, and by
translation v also separates ˜A from ˜b. This establishes the result ( ˜Ax−˜b)T v > 0
for any x ∈△.
To extend this to the case x ∈▲, we observe that by deﬁnition of ˜b, the
point 0−˜b also has strictly positive inner product with v. Therefore the entire
convex hull of ˜A△∪0 and thus the set ˜A▲has the desired property.

16
Regularization, Optimization, Kernels, and Support Vector Machines
1.4
Implications and Remarks
In the following, we will explain a handful of implications of the shown
equivalence, by relating both algorithms as well as known theoretical results
for the Lasso or the SVM to the respective other method.
1.4.1
Sublinear Time Algorithms for Lasso and SVMs
The recent breakthrough SVM algorithm of [8, 21] in time O(ε−2(n +
d) log n) returns an ε-accurate solution to problem (1.1). Here ε-accurate
means (σ∗−ε)-weakly separating. The running time of the algorithm is re-
markable since for large data, it is signiﬁcantly smaller than even the size of
the input matrix, being d · n. Therefore, the algorithm does not read the full
input matrix ˜A. More precisely, [8, Corollary III.2] proves that the algorithm
provides (with high probability) a solution p∗∈△of additive error at most ε
to
min
p∈△max
w∈Rd,
∥w∥≤1
wT ˜Ap .
This is a reformulation of minp∈△pT ˜AT ˜Ap , which is exactly our SVM prob-
lem (1.1), since for given p, the inner maximum is attained when w = ˜Ap.
Therefore, using our simple trick from Section 1.3.2 of reducing any Lasso
instance (1.2) to an SVM (1.1) (with its matrix ˜A having twice the number
of columns as A), we directly obtain a sublinear time algorithm for the Lasso.
Note that since the algorithm of [8, 21] only accesses the matrix ˜A by sim-
ple entry-wise queries, it is not necessary to explicitly compute and store ˜A
(which is a preprocessing that would need linear time and storage). Instead,
every entry ˜Aij that is queried by the algorithm can be provided on the ﬂy, by
returning the corresponding (signed) entry of the Lasso matrix A, minus bi.
It will be interesting to compare this alternative algorithm to the recent
more specialized sublinear time Lasso solvers in the line of work of [5, 20],
which are only allowed to access a constant fraction of the entries (or features)
of each row of A. If we use our proposed reduction here instead, the resulting
algorithm from [8] has more freedom: it can (randomly) pick arbitrary entries
of A, without necessarily accessing an equal number of entries from each row.
On the other hand, it is an open research question if a sublinear SVM
algorithm exists that only accesses a constant small fraction of each datapoint,
or of each feature of the input data.
1.4.2
A Kernelized Lasso
Traditional kernel regression techniques [38, 36, 35] try to learn a real-
valued function f from the space Rd of the datapoints, such that the resulting

An Equivalence between the Lasso and Support Vector Machines
17
real value for each datapoint approximates some observed value. The regres-
sion model is chosen as a linear combination of the (kernel) inner products
with few existing landmark datapoints (the support vectors).
Here, as we discuss a kernelization of the Lasso that is in complete analogy
to the classical kernel trick for SVMs, our goal is diﬀerent. We are not trying to
approximate n many individual real values (one for each datapoint, or row of
A), but instead we are searching for a linear combination of our points in the
kernel space, such that the resulting combination is close to the lifted point b,
measured in the kernel space norm. Formally, suppose our kernel space H is
given by an inner product κ(y, z) = ⟨Φ(y), Φ(z)⟩for some implicit mapping
Φ : Rd →H. Then we deﬁne our kernelized variant of the Lasso as
min
x∈♦

X
i
Φ(Ai)xi −Φ(b)

2
H
.
(1.7)
Nicely, analogous to the SVM case, here this objective function also is deter-
mined purely in terms of the pairwise (kernel) inner products κ(·, ·).
An alternative way to see this is to observe that our simple “mirror-and-
translate” trick from Section 1.3.2 also works the very same way in any kernel
space H. Here, the equivalent SVM instance is given by the 2n new points
{±Φ(Ai) −Φ(b) | i ∈[1..n]} ⊂H. The crucial observation is that the (kernel)
inner product of any two such points is
⟨siΦ(Ai) −Φ(b), sjΦ(Aj) −Φ(b)⟩
=
sisjκ(Ai, Aj) −siκ(Ai, b) −sjκ(Aj, b) + κ(b, b) .
Here si, sj ∈±1 are the signs corresponding to each point. Therefore we have
completely determined the resulting 2n × 2n kernel matrix K that deﬁnes
the kernelized SVM (1.1), namely minx∈△xT Kx, which solves our equivalent
Lasso problem (1.7) in the kernel space H.
Discussion. While traditional kernel regression corresponds to a lifting of
the rows of the Lasso matrix A into the kernel space, our approach (1.7) by
contrast is lifting the columns of A (and the r.h.s. b). We note that it seems
indeed counter-intuitive to make the regression “more diﬃcult” by artiﬁcially
increasing the dimension of b. Using, e.g., a polynomial kernel, this means
that we also want the higher moments of b to be well approximated by our
estimated x. On the other hand, increasing the dimension of b naturally cor-
responds to adding more data rows (or measurements) to a classical Lasso
instance (1.2).
In light of the success of the kernel idea for the classiﬁcation case with its
existing well-developed theory, we think it will be interesting to relate these
results to the above proposed kernelized version of the Lasso, and to study
how diﬀerent kernels will aﬀect the solution x for applications of the Lasso.
Using a diﬀerent connection to SVMs, the early work of [18] has studied
a similar kernelization of the penalized version of the Lasso; see also [11].
For applications in image retrieval, [40] has recently applied a similar Lasso
kernelization idea.

18
Regularization, Optimization, Kernels, and Support Vector Machines
1.4.3
The Pattern of Support Vectors, in the View of Lasso
Sparsity
Using our construction of the equivalent Lasso instance for a given SVM,
we can translate sparsity results for the Lasso to understand the pattern of
support vectors of SVMs.
The motivation here is that a small number of support vectors is crucial
for the eﬃcient application of SVMs in practice, in particular in the kernelized
case, because the cost to evaluate the resulting classiﬁer is directly propor-
tional to the number of support vectors. Furthermore, the support vectors are
the most informative points for the classiﬁcation task, while the non-support
vectors could be safely discarded from the problem.
Using Sparse Recovery Results. There has been a vast amount of liter-
ature studying the sparsity of solutions to the Lasso and related ℓ1-regularized
methods, in particular the study of the sparsity of x when A and b are from
distributions with certain properties. For example, in the setting known as
sparse recovery, the goal is to approximately recover a sparse solution x using
a Lasso instance A, b (consisting of only a small number of rows). Here b is
interpreted as a noisy or corrupted linear measurement Aˆx, and the unknown
original ˆx is sparse. Classical recovery results then show that under weak as-
sumptions on A, b and the sparsity of ˆx, the optimal Lasso solution x must
be identical to the (unknown) sparse ˆx; see, e.g., [7, 33].
Now our construction of the equivalent Lasso instance for a given SVM
allows us to translate such sparsity results to the pattern of SVM support
vectors. More precisely, any result that characterizes the Lasso sparsity for
some distribution of matrices A and suitable b, will also characterize the pat-
terns of support vectors for the equivalent SVM instance (and in particular
the number of support vectors). This assumes that a Lasso sparsity result is
applicable for the type of translation b that we have used to construct the
equivalent Lasso instance. However, this is not hopeless. For example, exis-
tence of a weakly separating vector that is a sparse convex combination of the
SVM datapoints is suﬃcient, since this results in a translation b that satisﬁes
b ∝Aˆx for a sparse weight vector ˆx. It remains to investigate which distri-
butions and corresponding sparsity results are of most practical interest from
the SVM perspective, in order to guarantee a small number of support vectors.
Lasso Sparsity in the View of SVMs. In another way, sparsity has
also been studied for SVMs in the literature, for example in the work of [39],
which analyzes the asymptotic regime n →∞. Using the simpler one of our
reductions, the same results also hold for the Lasso, when the number of
variables n grows.

An Equivalence between the Lasso and Support Vector Machines
19
1.4.4
Screening Rules for Support Vector Machines
For the Lasso, screening rules have been developed recently. This approach
consists of a pre-processing of the data A, in order to immediately discard
those predictors Ai that can be guaranteed to be inactive for the optimal
solution. Provable guarantees for such rules were ﬁrst obtained by [14], and
also studied in the later work [44], or the heuristic paper [42].
Translated to the SVM setting by our reduction, any such existing Lasso
screening rule can be used to permanently discard input points before the
SVM optimization is started. The screening rule then guarantees that any
discarded point will not be a support vector, so the resulting optimal clas-
siﬁer remains unchanged. We are not aware of screening rules in the SVM
literature so far, with the exception of the more recent paper of [30].
While the two previous subsections have mainly made use of the more
complicated direction of our reduction (SVM ⪯Lasso) from Section 1.3.3, we
can also gain some insights into the pattern of support vectors of SVMs by
using the other (simpler) direction of reduction, as we will do next.
1.4.5
Regularization Paths and Homotopy Methods
For most machine learning methods — including SVMs and Lasso — one
of the main hurdles in practice is the selection of the right free parameters. For
SVMs and Lasso, the main question boils down to how to select the best value
for the regularization parameter, which determines the trade-oﬀbetween the
best ﬁt of the model, and the model complexity. For the SVM, this is the
soft-margin parameter C, while in the Lasso, the regularization parameter is
the value r for the required bound ∥x∥1 ≤r.
Since naive grid-search for the best parameter is error-prone and comes
without guarantees between the grid-values, algorithms that follow the solu-
tion path — as the regularization parameter changes — have been developed
for both SVMs [19] and Lasso [31, 10], and have become popular in particular
on the Lasso side.
In light of our joint investigation of Lasso and SVMs here, we can gain the
following insights on path methods for the two problems:
General Solution Path Algorithms. We have observed that both the
primal Lasso (1.2) and the dual ℓ2-SVM (1.1) are in fact convex optimiza-
tion problems over the simplex. This enables us to apply the same solution
path methods to both problems. More precisely, for problems of the form
minx∈△f(x, t), general path following methods are available that can main-
tain an approximation guarantee along the entire path in the parameter t,
as shown in [16] and more recently strengthened by [17]. These methods do
apply for objective functions f that are convex in x and continuous in the

20
Regularization, Optimization, Kernels, and Support Vector Machines
parameter t (which in our case is r for the Lasso and C for the SVM).
Path Complexity. The exact solution path for the Lasso is known to
be piecewise linear. However, the number of pieces, i.e., the complexity of the
path, is not easy to determine. The recent work of [29] has constructed a Lasso
instance A ∈Rd×n, b ∈Rn, such that the complexity of the solution path (as
the parameter r increases) is exponential in n. This is inspired by a similar
result by [13] which holds for the ℓ1-SVM.
Making use of the easier one of the reductions we have shown above, we ask
if similar complexity worst-case results could also be obtained for the ℓ2-SVM.
For every constraint value r > 0 for the Lasso problem, we have seen that the
corresponding equivalent SVM instance (1.1) as constructed in Section 1.3.2
is minx∈△
 ˜A(r)x
2, with
˜A(r) := r(A |−A) −b1T ∈Rd×2n.
(1.8)
Therefore, we have obtained a hard-margin SVM, with the datapoints
moving in the space Rd as the Lasso regularization parameter r changes. The
movement is a simple linear rescaling by r, relative to the reference point
b ∈Rd. The result of [29] shows that essentially all sparsity patterns do
occur in the Lasso solution as r changes2, i.e., that the number of patterns
is exponential in n. For each pattern, we know that the SVM solution x⋄=
(In |−In)x△is identical to the Lasso solution, and in particular also has the
same sparsity pattern. Therefore, we also have the same (exponential) number
of diﬀerent sparsity patterns in the simplex parameterization x△for the SVM
(one could even choose more in those cases where the mapping is not unique).
To summarize, we have shown that a simple rescaling of the SVM data can
have a very drastic eﬀect, in that every pattern of support vectors can poten-
tially occur as this scaling changes. While our construction is still a worst-case
result, note that the operation of rescaling is not unrealistic in practice, as it
2The result of [29] applies to the penalized formulation of the Lasso, that is the solution
path of minx∈Rn ∥Ax −b∥2 + λ ∥x∥1 as the parameter λ ∈R+ varies. However, here we are
interested in the constrained Lasso formulation, that is minx, ∥x∥1≤r ∥Ax −b∥2 where the
regularization parameter is r ∈R+.
Luckily, the penalized and the constrained problem formulations are known to be equiv-
alent by standard convex optimization theory, and have the same regularization paths, in
the following sense. For every choice of λ ∈R+, there is a value for the constraint pa-
rameter r such that the solutions to the two problems are identical (choose for example
r(λ) := x∗
(λ)

1 for some optimal x∗
(λ), then the same vector is obviously also optimal for
the constrained problem). This mapping from λ to r(λ) is monotone. As the regulariza-
tion parameter λ is weakened (i.e., decreased), the corresponding constraint value r(λ) only
grows larger.
The mapping in the other direction is similar: for every r, we know there is a value λ(r)
(in fact this is the Lagrange multiplier of the constraint ∥x∥1 ≤r), such that the penalized
formulation has the same solution, and the mapping from r to λ(r) is monotone as well.
Having both connections, it is clear that the two kinds of regularization paths must be
the same, and that the support patterns that occur in the solutions — as we go along the
path — are also appearing in the very same order as we track the respective other path.

An Equivalence between the Lasso and Support Vector Machines
21
is similar to popular data preprocessing by re-normalizing the data, e.g., for
zero mean and variance one.
However, note that the constructed instance (1.8) is a hard-margin SVM,
with the datapoints moving as the parameter r changes. It does not directly
correspond to a soft-margin SVM, because the movement of points is diﬀerent
from changing the regularization parameter C in an ℓ2-SVM. As we have seen
in Lemma 1.1, changing C in the SVM formulation (1.5) has the eﬀect that
the datapoints ˜A(C) in the dual problem minx∈△
 ˜A(C)x
2 move as follows
(after re-scaling the entire problem by the constant factor C):
˜A(C) :=
√
CX
In

∈R(d+n)×n.
In conclusion, the reduction technique here does unfortunately not yet
directly translate the regularization path of the Lasso to a regularization path
for an ℓ2-SVM. Still, we have gained some more insight as to how “badly”
the set of SVM support vectors can change when the SVM data is simply
re-scaled. We hope that the correspondence will be a ﬁrst step to better relate
the two kinds of regularization paths in the future. Similar methods could also
extend to the case when other types of parameters are varied, such as, e.g., a
kernel parameter.
1.5
Conclusions
We have investigated the relation between the Lasso and SVMs, and con-
structed equivalent instances of the respective other problem. While obtaining
an equivalent SVM instance for a given Lasso is straightforward, the other
direction is slightly more involved in terms of proof, but still simple to imple-
ment, in particular, e.g., for ℓ2-loss SVMs.
The two reductions allow us to better relate and compare many existing
algorithms for both problems. Also, it can be used to translate a lot of the
known theory for each method to the respective other method. In the future,
we hope that the understanding of both types of methods can be further
deepened by using this correspondence.

22
Regularization, Optimization, Kernels, and Support Vector Machines
1.6
Appendix: Some Soft-Margin SVM Variants That
Are Equivalent to (1.1)
We include the derivation of the dual formulation to the ℓ2-loss soft-margin
SVM (1.5) for n datapoints Xi ∈Rd, together with their binary class labels
yi ∈{±1}, for i ∈[1..n], as deﬁned above in Section 1.2.2.
The equivalence to (1.1) directly extends to the one- and two-class case,
without or with (regularized) oﬀset term, and as well for the hard-margin
SVM. These equivalent formulations have been known in the SVM literature;
see, e.g., [37, 26, 43, 12], and the references therein.
Lemma 1.4. The dual of the soft-margin SVM (1.5) is an instance of the
classiﬁer formulation (1.1), that is minx∈△∥Ax∥2 , with
A :=

Z
1
√
C In

∈R(d+n)×n
where the data matrix Z ∈Rd×n consists of the n columns Zi := yiXi.
Proof. The Lagrangian [2, Section 5] of the soft-margin SVM formulation (1.5)
with its n constraints can be written as
L(w, ρ, ξ, α) :=
1
2 ∥w∥2 −ρ + C
2
P
i ξ2
i
+ P
i αi
 −wT Zi + ρ −ξi

.
Here we introduced a non-negative Lagrange multiplier αi ≥0 for each of
the n constraints. Diﬀerentiating L with respect to the primal variables, we
obtain the KKT optimality conditions
0
!=
∂
∂w
= w −P
i αiZi
0
!=
∂
∂ρ
= 1 −P
i αi
0
!=
∂
∂ξ
= Cξ −α .
When plugged into the Lagrange dual problem maxα minw,ρ,ξ L(w, ρ, ξ, α) ,
these give us the equivalent formulation (sometimes called the Wolfe-Dual)
maxα
1
2αT ZT Zα −ρ + C
2
1
C2 αT α
−αT ZT Zα + ρ −1
C αT α
= −1
2αT ZT Zα −
1
2C αT α .
In other words, the dual is
minα
αT  ZT Z + 1
C In

α
s.t.
α ≥0
αT 1 = 1 .

An Equivalence between the Lasso and Support Vector Machines
23
This is directly an instance of our ﬁrst SVM formulation (1.1) used in the
introduction, if we use the extended matrix
A :=

Z
1
√
C In

∈R(d+n)×n .
Note that the optimal primal solution w can directly be obtained from any
dual optimal α by using the optimality condition w = Aα.
Bibliography
[1] P Bloomﬁeld and W L Steiger. Least absolute deviations: theory, applica-
tions, and algorithms. Progress in probability and statistics. Birkhäuser,
1983.
[2] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cam-
bridge University Press, 2004.
[3] Peter Bühlmann and Sara van de Geer. Statistics for High-Dimensional
Data - Methods, Theory and Applications. Springer Series in Statistics
0172-7397. Springer, Berlin, Heidelberg, 2011.
[4] Christopher Burges. A Tutorial on Support Vector Machines for Pattern
Recognition. Data Mining and Knowledge Discovery, 2(2):121–167, 1998.
[5] Nicolò Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. Eﬃcient
Learning with Partially Observed Attributes. The Journal of Machine
Learning Research, 12:2857–2878, 2011.
[6] Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. Coordinate Descent
Method for Large-scale L2-loss Linear Support Vector Machines. JMLR,
9:1369–1398, 2008.
[7] Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic
Decomposition by Basis Pursuit. SIAM Journal on Scientiﬁc Computing,
20(1):33, 1998.
[8] Kenneth L Clarkson, Elad Hazan, and David P Woodruﬀ.
Sublinear
Optimization for Machine Learning. FOCS 2010 - 51st Annual IEEE
Symposium on Foundations of Computer Science, 2010.
[9] Corinna Cortes and Vladimir Vapnik. Support-Vector Networks. Machine
Learning, 20(3):273–297, 1995.

24
Regularization, Optimization, Kernels, and Support Vector Machines
[10] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani.
Least angle regression. Annals of Statistics, 32(2):407–499, 2004.
[11] Theodoros Evgeniou, Massimiliano Pontil, and Tomaso Poggio. Regular-
ization Networks and Support Vector Machines. Advances in Computa-
tional Mathematics, 13(1):1–50, 2000.
[12] Bernd Gärtner and Martin Jaggi. Coresets for polytope distance. SCG
’09: Proceedings of the 25th annual symposium on computational geome-
try, 2009.
[13] Bernd Gärtner, Martin Jaggi, and Clément Maria. An Exponential Lower
Bound on the Complexity of Regularization Paths. Journal of Computa-
tional Geometry, 3(1):168–195, 2012.
[14] Laurent El Ghaoui, Vivian Viallon, and Tarek Rabbani. Safe Feature
Elimination for the LASSO and Sparse Supervised Learning Problems.
arXiv.org, 2010.
[15] Debashis Ghosh and Arul M Chinnaiyan. Classiﬁcation and Selection of
Biomarkers in Genomic Data Using LASSO. Journal of Biomedicine and
Biotechnology, 2005(2):147–154, 2005.
[16] Joachim Giesen, Martin Jaggi, and Sören Laue. Approximating parame-
terized convex optimization problems. ACM Transactions on Algorithms,
9(10):1–17, 2012.
[17] Joachim Giesen, Jens Müller, Soeren Laue, and Sascha Swiercy.
Ap-
proximating Concavely Parameterized Optimization Problems. In NIPS,
2012.
[18] Federico Girosi.
An Equivalence Between Sparse Approximation and
Support Vector Machines. Neural Computation, 10(6):1455–1480, 1998.
[19] Trevor Hastie, Saharon Rosset, Robert Tibshirani, and Ji Zhu. The Entire
Regularization Path for the Support Vector Machine. The Journal of
Machine Learning Research, 5:1391–1415, 2004.
[20] Elad Hazan and Tomer Koren. Linear Regression with Limited Observa-
tion. In ICML, 2012.
[21] Elad Hazan, Tomer Koren, and Nathan Srebro. Beating SGD: Learning
SVMs in Sublinear Time. In NIPS, 2011.
[22] Donald W Hearn. The gap function of a convex program. Operations
Research Letters, 1(2):67–71, 1982.
[23] Sepp Hochreiter and Klaus Obermayer. Gene Selection for Microarray
Data. In Bernhard Schölkopf, Jean-Philippe Vert, and Koji Tsuda, ed-
itors, Kernel Methods in Computational Biology, page 319. MIT Press,
2004.

An Equivalence between the Lasso and Support Vector Machines
25
[24] Sepp Hochreiter and Klaus Obermayer.
Support Vector Machines for
Dyadic Data. Neural Computation, 18(6):1472–1510, 2006.
[25] Martin Jaggi.
Revisiting Frank-Wolfe: Projection-Free Sparse Convex
Optimization. In ICML, 2013.
[26] S Sathiya Keerthi, Shirish K Shevade, Chiranjib Bhattacharyya, and
K R K Murthy.
A fast iterative nearest point algorithm for support
vector machine classiﬁer design. IEEE Transactions on Neural Networks,
11(1):124–136, 2000.
[27] Yuh-Jye Lee and Olvi L Mangasarian. RSVM: Reduced Support Vector
Machines. In SDM 2001 - Proceedings of the ﬁrst SIAM international
conference on data mining, Philadelphia, 2001.
[28] Fan Li, Yiming Yang, and Eric P Xing. From Lasso regression to Feature
vector machine. In NIPS, 2005.
[29] Julien Mairal and Bin Yu. Complexity Analysis of the Lasso Regulariza-
tion Path. In ICML, 2012.
[30] Kohei Ogawa, Yoshiki Suzuki, and Ichiro Takeuchi. Safe Screening of
Non-Support Vectors in Pathwise SVM Computation. In ICML, pages
1382–1390, 2013.
[31] Michael R Osborne, Brett Presnell, and Berwin A Turlach. A new ap-
proach to variable selection in least squares problems. IMA Journal of
Numerical Analysis, 20(3):389–403, 2000.
[32] Massimiliano Pontil, Ryan Rifkin, and Theodoros Evgeniou. From Re-
gression to Classiﬁcation in Support Vector Machines. Technical Report
A.I. Memo No. 1649, MIT, 1998.
[33] Ely Porat and Martin J Strauss. Sublinear time, measurement-optimal,
sparse recovery for all. In SODA ’12: Proceedings of the Twenty-Third
Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM, 2012.
[34] Frank Rosenblatt. The Perceptron: A Probabilistic Model for Information
Storage and Organization in the Brain. Psychological Review, 65(6):386–
408, 1958.
[35] Volker Roth. The Generalized LASSO. IEEE Transactions on Neural
Networks, 15(1):16–28, 2004.
[36] Craig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge Re-
gression Learning Algorithm in Dual Variables. In ICML. Morgan Kauf-
mann Publishers Inc, 1998.
[37] Bernhard Schölkopf and Alex J Smola.
Learning with kernels.
sup-
port vector machines, regularization, optimization, and beyond. The MIT
Press, 2002.

26
Regularization, Optimization, Kernels, and Support Vector Machines
[38] Alex J Smola and Bernhard Schölkopf.
A tutorial on support vector
regression. Statistics and Computing, 14:199–222, 2004.
[39] Ingo Steinwart. Sparseness of Support Vector Machines—Some Asymp-
totically Sharp Bounds. In NIPS, 2003.
[40] Jayaraman J Thiagarajan, Karthikeyan Natesan Ramamurthy, and An-
dreas Spanias.
Local Sparse Coding for Image Classiﬁcation and Re-
trieval. Technical report, ASU, 2012.
[41] Robert Tibshirani. Regression Shrinkage and Selection via the Lasso.
Journal of the Royal Statistical Society. Series B (Methodological), pages
267–288, 1996.
[42] Robert Tibshirani, Jacob Bien, Jerome Friedman, Trevor Hastie, Noah
Simon, Jonathan Taylor, and Ryan J. Tibshirani. Strong rules for dis-
carding predictors in lasso-type problems. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 74(2):245–266, 2011.
[43] Ivor W Tsang, James T Kwok, and Pak-Ming Cheung. Core Vector Ma-
chines: Fast SVM Training on Very Large Data Sets. Journal of Machine
Learning Research, 6:363–392, 2005.
[44] Jie Wang, Binbin Lin, Pinghua Gong, Peter Wonka, and Jieping Ye. Lasso
Screening Rules via Dual Polytope Projection. In NIPS, 2013.
[45] Günter M Ziegler. Lectures on Polytopes, volume 152 of Graduate Texts
in Mathematics. Springer Verlag, 1995.

Chapter 2
Regularized Dictionary Learning
Annalisa Barla
DIBRIS, Università degli Studi di Genova
Saverio Salzo
DIMA, Università degli Studi di Genova
Alessandro Verri
DIBRIS, Università degli Studi di Genova
2.1
Introduction ......................................................
27
2.2
Basic Notation and Deﬁnitions ..................................
29
2.3
The Problem of Dictionary Learning ............................
30
2.4
The Algorithm ...................................................
31
2.4.1
An Alternating Proximal Algorithm ....................
32
2.4.2
The Proximity Operator of Composite Penalties .......
33
2.5
Application to Computational Biology ..........................
36
2.5.1
A Model for aCGH Data Latent Feature Detection ....
37
2.5.2
Experiment on Synthetic Data ..........................
40
2.5.3
Experiment on Simulated Data .........................
41
2.5.4
Clustering of Breast Cancer Data .......................
43
2.6
Conclusion ........................................................
46
Bibliography ......................................................
48
2.1
Introduction
In dictionary learning, given a set of signals belonging to a certain class, one
wishes to extract relevant information by identifying the generating causes,
that is, recovering the elementary signals (atoms) that eﬃciently represent the
data. Generally this goal is achieved by imposing some kind of sparseness con-
straint on the coeﬃcients of the representation. Moreover, one typically puts
priors on the atoms themselves. Ultimately, this gives rise to an optimization
problem whose objective function is composed by a data ﬁt term, which ac-
counts for the goodness of the representation, and several penalization terms
and constraints on the coeﬃcients and atoms that explain the prior knowledge
27

28
Regularization, Optimization, Kernels, and Support Vector Machines
at hand. Since the pioneering work [25], diﬀerent instances of dictionary learn-
ing problems have been proposed. This encompasses sparse coding [25, 35],
ℓp-sparse coding [16], hierarchical sparse coding [13], elastic-net based dictio-
nary learning [19], and fused-lasso based dictionary learning [34, 24], among
others.
Theoretical studies on dictionary learning that ﬁt the problem into a statis-
tical learning framework and justify the above-mentioned optimization prob-
lem are given in [23, 36, 12, 10]. In fact, in the previous framework, we min-
imize an empirical average over the training data, whereas the idealized task
would be to optimize an expected cost function over the underlying (and un-
known) distribution that generated the data. We point out that in [10] this
study is pursed with general coeﬃcient penalties and dictionary constraints,
covering all the types of dictionary learning problems mentioned above.
Concerning the algorithmic aspects, several studies have appeared in lit-
erature dealing with diﬀerent dictionary learning problems [25, 1, 18, 13, 24].
In all these works the optimization task is solved by a procedure that alter-
natively minimizes over the coeﬃcients and atoms separately. The diﬀerent
contributions, in fact, rely on the way the partial minimization subproblems
are tackled.
Dictionary learning has been found eﬀective on a variety of applied prob-
lems, such as image denoising [20], audio processing [11, 9], and image classi-
ﬁcation [29], to name a few. We refer to the aforementioned papers and the
references therein for a complete overview of the state-of-the-art on applica-
tions of dictionary learning, which is beyond the scope of our work.
In this chapter we focus on the algorithmic and applied aspects of dictio-
nary learning. Based on the work in [2], we present an alternating proximal
algorithm suitable for a general dictionary learning framework with composite
convex penalization terms. Such an algorithm is attractive for it has stronger
convergence properties with respect to the simpler alternating minimization
scheme, as established in [2] and recalled in Section 2.4.1. Here, we couple
that algorithm with an eﬃcient dual algorithm for the computation of the re-
lated proximity operators, which keeps the overall procedure still eﬀective. An
analysis of the problem of computing the proximity operators for general sums
of composite functions is provided. In this respect, we extend related results
given in [38] which also allows us to consider hard constraints. Finally, we note
that, to the best of our knowledge, in the context of dictionary learning, the
alternating proximal algorithm represents a novelty.
Next, we give an application of the proposed algorithm in the context
of genome-wide data understanding, in the setting of [24]. This application
was already presented in [22, 21]. The problem consists of identifying latent
features (atoms) in array-based comparative genomic hybridization (aCGH)
data, which may reveal a genotype-phenotype relationship. Thanks to the
generality of the proposed algorithm, we revise the model proposed by [24],
modifying and adding penalties in order to treat the whole genomic signal
and to select more representative atoms. More precisely: (a) we constrain the

Regularized Dictionary Learning
29
coeﬃcients to be positive, so that the complexity of the matrix of coeﬃcients
is reduced and the resulting atoms become more informative; (b) we employ a
weighted total variation penalty for the atoms, in order to treat the signal of
the genome as a whole, still guaranteeing independency among chromosomes;
(c) we force a structured sparsity on the matrix of coeﬃcients and atoms by
using ℓ1–ℓ2 mixed norms, which better suits the actual purposes of dictionary
learning — this contrasts with the majority of similar models that instead use
a simple ℓ1 norm inducing just a global sparsity on the matrices.
We set up two experiments with properly designed synthetic and simulated
data mimicking the properties of aCGH data. The purpose is to assess the
eﬀectiveness of the proposed model in identifying the relevant latent features.
Then, we consider one further experiment based on real breast cancer aCGH
data, with the aim of evaluating the improvements in a clustering scenario. We
always compare the results with the ones obtained by the model and algorithm
proposed in [24].
The remainder of the chapter has the following structure. In the next
section we begin by giving some basic mathematical deﬁnitions. Section 2.3
formalizes the problem of dictionary learning. In Section 2.4 we present the
algorithm. In Section 2.5 we discuss the application to a relevant problem of
computational biology, providing three experiments on synthetic, simulated
and real data.
2.2
Basic Notation and Deﬁnitions
Let us ﬁrst introduce some basic notation (for a detailed account of convex
analysis, see [4]). Hereafter, H is a real Hilbert space and we denote with ⟨·, ·⟩H
and ∥·∥H its scalar product and associated norm. The extended real line is
denoted by R. Let C ⊆H, then the indicator function of C is
δC : H →R,
δC(x) =
(
0
if x ∈C,
+∞
if x /∈C,
(2.1)
and its support function is
σC : H →R,
σC(u) = sup
x∈X
⟨x, u⟩H .
Let f : H →R be an extended real valued function. The domain of f is
the set dom f =

x ∈H
 f(x) < +∞
	
. The function f is proper if for
every x ∈H f(x) > −∞and dom f ̸= ∅and is coercive if f(x) →+∞as
∥x∥H →+∞. The function f is said to be positively homogeneous if for every
λ ≥0 and x ∈H, f(λx) = λf(x). The (Fenchel) conjugate of f is the function
f ∗: H →R, such that for every u ∈H, f ∗(u) = supx∈H(⟨x, u⟩H−f(x)). If f is

30
Regularization, Optimization, Kernels, and Support Vector Machines
proper and lower semicontinuous, the proximity operator of f is the (nonlinear)
operator proxf : H →H which maps every x ∈H to the unique minimizer of
the function f + ∥· −x∥2
H /2. Clearly proxδC = PC, the projection operator
onto C. We recall the Moreau decomposition formula
x = proxγf(x) + γ proxf ∗/γ(x/γ) ,
(2.2)
valid for every x ∈H and γ ∈R. Finally one can show that if g is proper convex
and lower semicontinuous, then f ∗∗= f, hence the Moreau decomposition
formula (2.2) also writes x = proxγf ∗(x) + γ proxf/γ(x/γ).
Throughout the chapter, vectors will be denoted by bold small letters,
whereas matrices will be denoted by bold capital letters, and their entries by
the corresponding small plain letters. The space of real matrices of dimensions
N by M is denoted RN×M. The Frobenius norm is ∥A∥F =
qP
i,j |ai,j|2.
Moreover, using a MATLAB-like notation, if A ∈RN×M, for every (n, m) ∈
{1, . . . , N} × {1, . . . , M}, we will set A(:, m) = (an′,m)1≤n′≤N and A(n, :) =
(an,m′)1≤m′≤M.
2.3
The Problem of Dictionary Learning
We are given S ∈N samples (ys)1≤s≤S, with ys ∈RL. Then, one seeks J
atoms (bj)1≤j≤J, bj ∈RL, which possibly give complete representation of all
samples, in the sense that
ys ≊
J
X
j=1
θjsbj
∀s = 1, . . . , S
(2.3)
for some vectors of coeﬃcients θs = (θjs)i≤j≤J ∈RJ.
Usually, some kind of sparseness on the coeﬃcients is enforced as well as
constraints on the atoms, and the quality of the representation is assessed by
some ℓq norm with 1 ≤q ≤+∞. Hereafter, for the sake of brevity, we deﬁne
the matrices Y =
y1
y2
· · ·
yS

∈RL×S, B =
b1
b2
· · ·
bJ

∈
RL×J, and Θ =
θ1
θ2
· · ·
θS

∈RJ×S, which gather the data, atoms
and coeﬃcients, respectively.
Then, dictionary learning leads to the following minimization problem
min
B,Θ
S
X
s=1
 ∥ys −Bθs∥q
q + h(θs)

s.t. B ∈B, Θ ∈RJ×S ,
(2.4)
where h : RJ →R is a penalization function promoting sparsity in the coeﬃ-
cients and B ⊆RL×J is a constraint set for the matrix of atoms.

Regularized Dictionary Learning
31
In the literature, diﬀerent instances of h and B have been considered. We
list some important examples:
Sparse coding [25, 18]. h(θ) = τ ∥θ∥1, and B =

B | (∀j) ∥bj∥2 = 1
	
in
[25] and B =

B | (∀j) ∥bj∥2 ≤c
	
in [18].
ℓp sparsity [16]. h(θ) = ∥θ∥p, with 0 < p ≤1, and B =

B | ∥B∥F = 1
	
or
B =

B | (∀j) ∥bj∥2
2 = 1/J
	
.
Hierarchical Sparse Coding [13]. h
=
P
C∈C
θ|C

2, where θ|C
=
(θs)s∈C and C is a tree-structured set of indices in {1, . . . , S}, and the
dictionary constraint is B =

B | (∀j) µ ∥bj∥1+(1−µ) ∥bj∥2
2 ≤1
	
, with
µ ∈[0, 1].
K-SVD [1]. h = δ{θ | ∥θ∥0≤τ}, where ∥·∥0 is the semi-norm that counts the
non-zero entries of a vector, and B =

B | (∀j) ∥bj∥2 = 1
	
.
Elastic-net [19]. h(θ) = τ ∥θ∥1 + η ∥θ∥2
2 and B =

B | (∀j) ∥bj∥2 ≤1
	
.
We note that in the sparse coding example above one can take for h the
hard constraint variant, i.e., h = δ{θ | ∥θ∥1≤τ}. Similarly, in the K-SVD exam-
ple, one can consider the soft constraint variant h(θ) = τ ∥θ∥0.
2.4
The Algorithm
We consider a general dictionary learning framework whose objective func-
tion is as follows
ϕ(Θ, B) = 1
2 ∥Y −BΘ∥2
F + h(Θ) + g(B),
(2.5)
where g: RJ×S →R and h: RJ×S →R are extended real valued convex
functions. This model assumes convex regularization terms, hence it cannot
cover some dictionary learning problems described in Section 2.3. On the other
hand, we also add a penalization term g for the atoms, allowing more ﬂexibility
in setting prior knowledge in the dictionary. Clearly, we recover model (2.4)
by choosing g as the indicator function of the set B. In this work we assume
the penalties g and h have the following form:
h(Θ) =
M1
X
m=1
φm(Qm(Θ)),
g(B) =
M2
X
m=1
ψm(Tm(B)),
(2.6)
where φm and ψm are proper extended real valued lower semicontinuous and
convex functions, and Qm and Tm are linear operators acting on spaces of

32
Regularization, Optimization, Kernels, and Support Vector Machines
matrices of appropriate dimensions. We emphasize that problem (2.5), even
with h and g convex, is nonconvex and in general nonsmooth, too. Moreover,
we remark that in the data ﬁt term of (2.5), for simplicity, we considered
the Frobenius (Euclidean) norm, but the proposed algorithm can be easily
adapted to handle the ℓq norms with 1 ≤q ≤+∞.
2.4.1
An Alternating Proximal Algorithm
Very often, in the context of dictionary learning, the minimization problem
(2.5) has been solved by employing an alternating minimization algorithm
(AMA) [25, 1, 18, 13, 35, 24], which is nothing but a two-block Gauss-Seidel
method. In fact, within this algorithmic scheme, depending on the application
at hand, the contributions of the papers mainly focus on the diﬀerent strategies
to solve the partial minimization subproblems.
Here we propose an alternating proximal algorithm of the following form
ηn, ζn ∈[ρ1, ρ2]
Θn+1 = proxηnϕ(·,Bn)(Θn)
Bn+1 = proxζnϕ(Θn+1,·)(Bn) .
(2.7)
where 0 < ρ1 ≤ρ2. The formulas for Θn+1 and Bn+1 are written down
explicitly as follows
Θn+1 = argmin
Θ
n
ϕ(Θ, Bn) +
1
2ηn
∥Θ −Θn∥2
F
o
Bn+1 = argmin
B
n
ϕ(Θn+1, B) +
1
2ζn
∥B −Bn∥2
F
o
.
(2.8)
Thus, algorithm (2.7) actually consists of an alternating regularized minimiza-
tion procedure.
In [2] a deep analysis of algorithm (2.7) is presented and the following
result can be worked out (see Lemma 3.1 and Theorem 3.2 in [2]).
Theorem 2.1. Suppose that both the functions g and h, in (2.5), are coer-
cive and satisfy the Kurdyka-Łojasiewicz property. If (Θ0, B0) ∈RJ×S ×
RL×J, and ηn, ζn and (Θn, Bn)n∈N are deﬁned according to (2.7), then
(ϕ(Θn, Bn))n∈N is decreasing and (Θn, Bn)n∈N converges to a critical point
of ϕ.
In Theorem 2.1, the coercivity property serves to ensure the boundedness
of the sequence (Θn, Bn)n∈N and, in this context, it is standard and always
satisﬁed. The Kurdyka-Łojasiewicz property is a kind of metric regularity
property for nonsmooth functions and has been deeply studied in [7] and
applied to minimization problems in [2, 3]. We brieﬂy recall here the deﬁnition.
Let f : RN →]−∞, +∞] be a proper and lower semicontinuous function. Then,

Regularized Dictionary Learning
33
f satisﬁes the Kurdyka-Łojasiewicz property, if for every critical point ¯x of f,
it holds
∀u ∈∂(ϕ ◦(f −f(¯x)))(x)
∥u∥≥1
for a suitable continuously diﬀerentiable concave and strictly increasing func-
tion ϕ : [0, η[→R+, and for every x ∈f −1(]0, η[) suﬃciently close to ¯x. This
requires the function f to be sharp up to a reparameterization of its values.
The signiﬁcance of Theorem 2.1 stands on the fact that the class of func-
tions satisfying the Kurdyka-Łojasiewicz property is large enough to encom-
pass semi-algebraic and tame functions and ultimately most of the penaliza-
tion functions used in statistical learning and inverse problems, as for instance
power of norms, and indicator functions of norm balls.
This result shows that the alternating proximal algorithm (2.7) has
stronger convergence properties with respect to AMA. Indeed, in general,
AMA provides convergence to stationary points only for a subsequence of
(Θn, Bn)n∈N [6].
Algorithm (2.7) requires the computation of the proximity operator of the
following partial functions
ϕ(·, Bn)(Θ) = 1
2 ∥Y −BΘ∥2
F +
M1
X
m=1
φm(Qm(Θ))
(2.9)
ϕ(Θn+1, ·)(B) = 1
2 ∥Y −BΘ∥2
F +
M2
X
m=1
ψm(Tm(B)) .
(2.10)
The partial functions (2.9)-(2.10) have the form of a sum of composite penal-
ties and for that reason, in general, there is no closed form expression available
for their proximity operators. Therefore, a further algorithm is needed that
takes into account the particular structure of the functions. Moreover, hope-
fully the algorithm should be fast enough to keep the alternating proximal
algorithm still eﬀective.
2.4.2
The Proximity Operator of Composite Penalties
In view of the discussion presented in the previous section, it is desirable
to eﬃciently compute proximity operators for penalties that are sums of com-
posite functions. In this section we answer that issue in a general setting,
presenting an algorithm based on a dual approach.
Let f : H →R be a function of the following form
f(x) =
M
X
m=1
ωm(Amx) ,
(∀x ∈H)
(2.11)
where, for every m = 1, 2, . . . , M, Am : H →Gm are bounded linear operators
between Hilbert spaces and ωm : Gm →R are proper convex and lower semi-
continuous functions. Our purpose is to show how to compute the proximity

34
Regularization, Optimization, Kernels, and Support Vector Machines
operator proxλf : H →H for λ > 0, in terms of the mappings Am and the
proximity operators of ωm.
First of all, we note that f is actually of the form
f(x) = ω(Ax) ,
(2.12)
for a suitable operator A : H →G and ω : G →R. Indeed, it is suﬃcient to
consider the direct sum of the Hilbert spaces (Gi)1≤m≤M
G :=
M
M
m=1
Gm
⟨u, v⟩G :=
M
X
m=1
⟨um, vm⟩Gm ,
and deﬁne the linear operator A : H →G, Ax = (Amx)1≤m≤M and the
function
ω : G →R ,
ω(v) =
M
X
m=1
ωm(vm).
Therefore, computing proxλf(y) aims to solve the following minimization
problem
min
x∈H ω(Ax) + 1
2λ ∥x −y∥2
H := Φλ(x) .
(2.13)
Its dual problem (in the sense of Fenchel-Rockafellar duality [4]) is
min
v∈G
1
2λ ∥y −λA∗v∥2
H + ω∗(v) −1
2λ ∥y∥2
H := Ψλ(v) .
(2.14)
We note that in (2.14) the adjoint operator A∗: G →H and the Fenchel
conjugate ω∗: G →R are both decomposable, meaning that for every v =
(vm)1≤m≤M ∈G, it holds that
A∗v =
M
X
i=m
A∗
mvm ,
ω∗(v) =
M
X
i=m
ω∗
m(vm) .
(2.15)
From the separability of ω∗, stated in (2.15), it follows that for γ > 0
proxγω∗(v) = argmin
u∈G
n
ω∗(u) + 1
2γ ∥u −v∥2
G
o
=
 proxγω∗
m(vm)

1≤m≤M ,
(2.16)
and the proximity operator of γω∗can be decomposed component-wise, too.
Due to the aforementioned decomposability property, the dual problem
(2.14) is possibly easier to solve. Thus, instead of solving the primal problem
(2.13), we tackle the dual problem (2.14).

Regularized Dictionary Learning
35
The primal and dual problem are connected by the so-called duality gap
function G : H × G →[0, +∞]. It is deﬁned for every (x, v) ∈H × G as
G(x, v) = Φλ(x) + Ψλ(v)
(2.17)
=
M
X
m=1
ωm(Amx) + ω∗
m(vm) + 1
λ⟨x −y, x⟩H + 1
2λ
 ∥xv∥2
H −∥x∥2
H

,
where (the primal variable) xv = y −λA∗v = y −λ PM
m=1 A∗
mvm ∈H. The
following proposition links minimizing sequences for the dual problem with
those of the primal problem in a general setting and it is at the basis of the
proposed dual method. This result extends Theorem 5.1 in [38], in the sense
that here it is not required that ω has full domain, so hard constraints are
allowed.
Proposition 2.1. Let ω: G →R be proper, convex, and lower semicontin-
uous. Assume that the domain of ω is closed and that the restriction of ω
to its domain is continuous1. Moreover, suppose ω is continuous in Ax0 for
some x02. Let (vk)k∈N be such that Ψλ(vk) →inf Ψλ. If we deﬁne the primal
variables as
xk = PA−1(dom ω)(y −λA∗vk) ,
(2.18)
where PA−1(dom ω) stands for the projection operator onto the closed convex
set A−1(dom ω), then
Φλ(xk) −inf Φλ ≤G(xk, vk) →0.
(2.19)
Proof. Set ˆx = proxλf(y), which is the solution of the primal problem (2.13),
let ˆv be a solution of the dual problem (2.14), and set zk = y −λA∗vk. One
can prove (see Theorem 6.1 in [38]) that
(∀k ∈N)
1
2λ ∥zk −ˆx∥2
H ≤Ψλ(vk) −Ψλ(ˆv) .
Clearly dom Φλ
=
A−1(dom ω) and since Pdom Φλ is continuous, and
Ψλ(vk) →inf Ψλ by hypothesis, we have
xk = Pdom Φλ(zk) →Pdom Φλ(ˆx) = ˆx
(as k →+∞)
and hence Axk →Aˆx, Axk, Aˆx ∈dom ω. Now from the continuity of ω|dom ω
it follows that Φλ(xk) →Φλ(ˆx) = inf Φλ. Since Φ(ˆx) = −Ψ(ˆv), we have
G(xk, vk) = Φλ(xk) + Ψλ(vk)
= Φλ(xk) −Φλ(ˆx)
|
{z
}
≥0
+ Ψλ(vk) −Ψλ(ˆv)
|
{z
}
≥0
→0 .
(2.20)
and the statement follows.
1The continuity is for the relative topology of dom ω. This is equivalent to require that
for every sequence (vk)k∈N, vk ∈dom ω and v ∈dom ω, vk →v =⇒ω(vk) →ω(v).
2This hypothesis is needed just to ensure inf Φλ+inf Ψλ = 0. However, weaker conditions
can be employed, as 0 ∈sri(R(A) −dom ω), where sri stands for strong relative interior [4].

36
Regularization, Optimization, Kernels, and Support Vector Machines
We remark that Proposition 2.1 also gives a stopping criterion for the
algorithm. Indeed from (2.19) it follows that one can control the objective
function values of the primal problem by controlling the values of the duality
gap — which are explicitly computable by (2.17).
Thus, we are justiﬁed in solving the dual problem (2.14) by whatever
algorithm just provides a minimizing sequence. We underline that, for the
dual problem, no convergence on the minimizers is required, but convergence
in value is suﬃcient. Taking advantage of the component-wise decomposition
(2.15)-(2.16), we present a generalization to the sum of composite functions of
the algorithm given in [38, Section 5.1], which corresponds to applying FISTA
[5] to the dual problem (2.14). Set u0 = v0 = 0, t0 = 1 and for every k ∈N
deﬁne
xtmp = y −λPM
m=1A∗
muk,m
0 < γk ≤(λ ∥A∥2)−1
for m = 1, . . . , M
vk+1,m = proxγkω∗
m
 uk,m + γkAmxtmp

xk+1 = PA−1(dom ω)
 y −λPM
m=1A∗
mvk+1,m

tk+1 =
 1 +
q
1 + 4t2
k

/2
for m = 1, . . . , M
uk+1,m = vk+1,m + tk−1
tk+1 (vk+1,m −vk,m).
(2.21)
Remark 2.1. In the case that ωm is positively homogeneous, it holds that
ω∗
m = δSm the indicator function of Sm = ∂ωm(0) and dom ωm = Gm. Hence
proxγkω∗
m = PSm and vk+1,m is computed by
vn+1,m = PSm
 uk,m + γkAmzk

.
(2.22)
If ωm = δSm for a closed convex set Sm ⊆Gm, then ω∗
m = σSm the support
function of Sm and using the Moreau decomposition formula proxγkω∗
m(y) =
y −PγkSmy, the vectors vk+1,m in (2.21) can be computed by the formula
vk+1,m = (I −PγkSm)
 uk,m + γkAmzk

.
(2.23)
Note that in this case the restriction of ωm to dom ωm is continuous.
2.5
Application to Computational Biology
In this section we provide an application of the proposed algorithm to a sig-
niﬁcant problem of computational biology, the identiﬁcation of latent features

Regularized Dictionary Learning
37
in array-based comparative genomic hybridization data (aCGH). We present
an enhancement of the model proposed in [24] as well as three diﬀerent exper-
iments to test our approach. The ﬁrst two consider synthetic and simulated
aCGH data. The third experiment analyzes real breast cancer aCGH data in
the context of clustering.
2.5.1
A Model for aCGH Data Latent Feature Detection
Array-based comparative genomic hybridization is a modern whole-genome
measuring technique that evaluates the occurrence of copy number variations
(CNVs) across the genome and extends the original CGH technology [14].
CNVs are alterations of the DNA that result in the cell having an abnormal
number of copies of one or more sections of the DNA and ultimately may
indicate an oncogene or a tumor suppressor gene. A signal measured with
an aCGH technology is made of a piecewise constant component plus some
composite noise.
The typical analysis on such data is segmentation, which is the automatic
detection of loci where copy number alterations (ampliﬁcations or deletions)
occur. Beyond that, it is crucial to understand how these alterations co-occur.
This turns to identifying shared patterns (latent features) in the data, which
may reveal a genotype-phenotype relationship.
Many methods have been proposed for the extraction of CNVs based on
diﬀerent principles like ﬁltering (or smoothing), breakpoint detection, and
calling, taking into account one sample at a time [17]. Some interesting recent
results exploit the possibility of adopting regularization methods for a joint
segmentation of many aCGH proﬁles. The works proposed by [24, 33, 37]
follow this stream, and are based on total variation (TV ) or fused lasso signal
approximation.
Thanks to the generality of the framework proposed in Section 2.4, we
present E-FLLat3 (enhanced fused Lasso latent feature model), a reﬁnement
of FLLat (fused Lasso latent feature model)[24], a dictionary learning model
that was originally proposed for aCGH data segmentation and latent features
identiﬁcation. We improve several modelling aspects that, in the end, provide
a straightforward way to reveal the elementary patterns in the signal, increas-
ing the interpretability of the results. In particular, we choose more complex
penalty terms that better suit the prior knowledge on the problem.
We choose the same application context of FLLat, that is aCGH data seg-
mentation and extraction of latent features (atoms). The E-FLLat model is
based on the minimization of a functional combining several penalties, prop-
erly designed to treat the whole genomic signal and to select more represen-
tative atoms. We ﬁrst recall FLLat and then describe E-FLLat.
3In [22], we call it CGHDL.

38
Regularization, Optimization, Kernels, and Support Vector Machines
The FLLat model is written as follows:
min
θs,bj
S
X
s=1
ys −
J
X
j=1
θjsbj

2
+ λ
J
X
j=1
∥bj∥1 + µ
J
X
j=1
TV (bj)
s.t.
S
X
s=1
θ2
j,s ≤1
∀j = 1, . . . , J
(FLLat)
where λ, µ > 0 are regularization parameters. The problem can be put in
matrix form as in (2.5), where
g(B) = λ
J
X
j=1
∥B(:, j)∥1 + µ
J
X
j=1
TV (B(:, j)),
h(Θ) =
J
X
j=1
δB1(Θ(j, :)),
(2.24)
and δB1 is the indicator function of the euclidean unit ball B1 of RS.
These penalties are chosen in order to model the prior knowledge on the
atoms, which are expected to be simple, in the sense that they resemble step
functions. This is achieved by employing the sum of the fused lasso penalties
PJ
j=1(λ ∥bj∥1 + µ TV (bj)). The ℓ1 penalization term forces each atom bj to
be sparse and the total variation term TV (bj) = PL−1
l=1 |bl+1,j −bl,j| induces
small variations in the atoms. The hard constraints on the coeﬃcients θj · are
imposed for consistency and identiﬁability of the model. Indeed, multiplying a
particular feature bj by a constant, and dividing the corresponding coeﬃcients
by the same constant, leaves the ﬁt unchanged, but reduces the penalty.
E-FLLat is an enhancement of FLLat, driven by the following optimization
problem depending on the three regularization parameters λ, µ, τ > 0:
min
θs,bj
S
X
s=1
ys −
J
X
i=1
θjsbj

2
+ λ
J
X
j=1
∥bj∥2
1 + µ
J
X
j=1
TVw(bj) + τ
S
X
s=1
∥θs∥2
1
s.t. 0 ≤θjs ≤θmax,
∀j = 1, . . . , J
∀s = 1, . . . , S
(E-FLLat)
where
TVw(bj) =
L−1
X
l=1
wl|bl+1,j −bl,j| ,
w = (wl)1≤l≤L−1 ∈RL−1
is the weighted total variation. E-FLLat may be turned into problem (2.5) by
choosing g and h as follows:

Regularized Dictionary Learning
39
g(B) = λ
J
X
j=1
∥B(:, j)∥2
1 + µ
J
X
j=1
TVw(B(:, j)),
h(Θ) = δ∆J×S(Θ) + τ
S
X
s=1
∥Θ(:, s)∥2
1 ,
(2.25)
and δ∆J×S is the indicator function of the box set ∆J×S = [0, θmax]J×S. In
the experiments we will always set θmax = 1. This choice forces the atoms to
be of the same amplitude as the original data.
This model improves FLLat, in several aspects. First it employs the pe-
nalization terms
λ
J
X
i=1
∥B(:, j)∥2
1 ,
S
X
s=1
∥Θ(:, s)∥2
1 ,
which are sum of squares of ℓ1 norms (i.e., mixed norms). This possibly forces
a structured sparsity only along the columns of the matrix of atoms B and of
coeﬃcients Θ. This choice is more faithful to the actual purposes of dictionary
learning and contrasts with the majority of similar models [24] that instead
use a global ℓ1 norm inducing just a scattered sparsity on the matrices.
Secondly in E-FLLat, the total variation term is indeed a generalized total
variation due to the presence of the weights w. This modiﬁcation is intro-
duced in order to relax at some points the constraint of small jumps on the
atoms. When dealing with aCGH data, this allows us to treat the signal of the
whole genome, giving the capability of identifying concomitant alterations on
diﬀerent chromosomes, still guaranteeing their independency. This is achieved
by setting the weights wl equal to zero at the chromosomes’ borders, and one
elsewhere. We remark that taking into account the geometry of the platform
and the actual distance between probes on the chromosomes may lead to a
better strategy to set the weights. Similarly, the weights corresponding to the
probes lying in the centromere region may as well be set to zero. This is in
line with the work [8]. Conversely, in FLLat, the analysis is supposed to be
performed on each chromosome separately (see [24] Section 2.1). This leads to
selecting a diﬀerent set of atoms for each chromosome preventing identifying
concomitant alterations on diﬀerent chromosomes.
Finally, we constrain the coeﬃcients to be positive. This reduces the com-
plexity of the matrix of coeﬃcients Θ and forces the matrix of atoms B to be
more informative: e.g., for deletions and ampliﬁcations occurring in diﬀerent
samples but on the same locus on the chromosome, diﬀerent atoms may be
selected. Ultimately the interpretability of the results is improved.
We solve the optimization problem connected to the E-FLLat model by
means of the algorithm studied in Section 2.4. This amounts to performing
the proximal alternating algorithm (2.7) as outer loop and computing the
proximity operators proxηnϕ(·,Bn) and proxζnϕ(Θn+1,·) with (2.21) as inner
loop. This resulting nested algorithm will be named, from now on, E-FLLatPA.

40
Regularization, Optimization, Kernels, and Support Vector Machines
We ﬁnally remark that the penalization functions g and h in (2.25) satisfy the
Kurdyka-Łojasiewicz property, for they are sums of functions satisfying the
Kurdyka-Łojasiewicz property. See Section 4.3 in [2] and Example 5.3 in [3].
2.5.2
Experiment on Synthetic Data
The purpose of this experiment was to evaluate the performance of E-
FLLat both in estimating the true signals, and in identifying latent features
in the data. We considered here synthetic data, in order to have full control
of the relevant alterations. We generated three datasets that were used to test
the denoising performance, following analogous experiments in [24]. Next, we
built a fourth dataset with a properly designed pattern of alterations and we
checked whether E-FLLat made a correct identiﬁcation.
Data generation The model of the signal follows [24] and the additive
noise model follows [26]. The signal is deﬁned as:
yls = µls + ϵls,
µls =
Ms
X
m=1
cmsI{lms≤l≤lms+kms},
ϵls ∼N(0, σ2),
(2.26)
where l = 1, . . . , L, s = 1, . . . , S, µls is the mean, and σ is the standard devia-
tion of the noise ϵls. The mean signal µs = (µls)1≤l≤L is a step function where
Ms is the number of segments (regions of CNVs) generated for sample s and
cms, lms and kms are the height, starting position, and length, respectively,
for each segment. We chose Ms ∈{1, 2, 3, 4, 5}, cms ∈{±1, ±2, ±3, ±4, ±5},
lms ∈{1, . . . , L −100}, and kms ∈{5, 10, 20, 50, 100}, L = 1000, S = 20.
According to this general schema, we generated four types of datasets:
Dataset 1: The samples are generated in order to reduce the chance of sharing
CNV regions. Therefore, the choice of the values Ms, cms, lms, and kms is
done separately for each sample. This schema follows [24, Section 4.1, Dataset
1].
Dataset 2: Following [24, Section 4.1, Dataset 2], the samples were designed to
have common segments of CNVs. Each shared segment appears in the samples
according to a ﬁxed proportion randomly picked between (0.25, 0.75). Start-
ing points and lengths were shared among the selected samples, whereas the
amplitudes cms still might vary within samples. The unshared segments were
built as in Dataset 1 for a maximum of 5 segments per sample.
Dataset 3: The atoms βj were generated as the µls’s in (2.26), with cms, lms
and kms chosen as above. The coeﬃcients θjs were randomly sampled in [0, 1]
and the signal was built as Y = BΘ + ε, where ε is an additive Gaussian
noise.
Dataset 4: This dataset was explicitly designed to mimic a real signal com-
posed of diﬀerent chromosomes. We built three classes of samples. One third
of the samples had mean signal as in the upper panel of Figure 2.1, one third
had mean signal as in the lower panel of Figure 2.1, and the remaining third
was built as in Dataset 1.

Regularized Dictionary Learning
41
FIGURE 2.1: Mean signals of the two patterns used for Dataset 4 generation.
Parameter selection The choice of the parameters (J, λ, µ, τ) was done
by minimizing the Bayesian information criterion (BIC) [30]. The BIC miti-
gates the problem of overﬁtting by introducing a penalty term for the com-
plexity of the model. In our case the BIC is written as:
(SL) · log
∥Y −BΘ∥2
F
SL

+ k(B) log(SL)
(2.27)
and k(B) was computed as the number of jumps in B and ultimately depends
on the parameters (J, λ, µ, τ). Note that, diﬀerently from [24], we also used
BIC to select the number of atoms J from {5, 10, 15, 20}.
Results Figure 2.2 shows the performances of the two approaches. Fol-
lowing [24], ROC curves were built by evaluating the correct detection of
alterations based on the denoised signal: the results are comparable. Perfor-
mances on the raw noisy signal are also plotted for reference. Figure 2.3 shows
a plot of the solutions obtained by the two approaches on Dataset 4. The al-
gorithm implementing (FLLat) achieves good results in denoising, selecting
J = 10 atoms, but fails in detecting the underlying patterns of Figure 2.1. The
selected atoms represent single alterations. Conversely, our approach (right
panel in Figure 2.3) selects 5 atoms that clearly comprise the two patterns.
Summarizing, these results show that the proposed approach is very eﬀective
in identifying the underlying data generating features.
2.5.3
Experiment on Simulated Data
This experiment is in the same vein of that described in Section 2.5.2 on
Dataset 4, but performed on simulated data, mimicking a real aCGH signal.
Data generation The dataset mimics a real aCGH signal composed of
diﬀerent chromosomes. We simulated a signal aﬀected by spatial bias [15] using
chip geometry information and by a wave eﬀect modeled through a sinusoidal
function [27]. As chip design, we used the Agilent 44k aCGH platform. To deal
with a simpler model, we restricted the dataset to 4 out of 23 chromosomes,
choosing those with a relatively small number of probes (i.e., chromosomes 13,

42
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 2.2: ROC curves for varying levels of noise (columns) and diﬀerent
dataset type (rows). The left column shows σ = 1.0, while the right column
shows σ = 2.0. Red lines refer to the performances on the noisy Y, dashed
and solid lines refer to FLLat and E-FLLat performances, respectively.
15, 18, and 21). Overall, we considered 80 samples composed of 3750 probes.
We built three classes of samples. One third of the samples (G1) followed a
pattern with a loss on chromosome 13, a gain on chromosome 15, and a gain
on the longer arm of chromosome 18. The alterations occur randomly with a
probability of 80% each. Similarly, one third of the samples (G2) followed a
pattern with a gain on chromosome 13 and a narrow loss on the shorter arm

Regularized Dictionary Learning
43
FIGURE 2.3: Dataset 4 analyzed by FLLat (left panel) and E-FLLat (right
panel). Each panel shows four subplots: top left plot represents the noisy data
matrix, top right plot shows the atom matrix with atoms as columns, bottom
left subplot is the true data matrix, and bottom right is the estimated signal.
of chromosome 21. Moreover, in groups G1 and G2, alterations (either gain
or loss) on the chromosomes not involved in the patterns can occur randomly
with 20% probability. Finally, group G3 had random alterations (either gain
or loss) on all chromosomes with low probability (10%).
Parameter selection Keeping in mind that we had three main groups of
data, we chose J = 5. The parameters (λ, µ, τ) were chosen according to BIC
(2.27).
Results Figure 2.4 reports the atoms and coeﬃcients computed by E-
FLLatPA on the simulated dataset. One can recognize that the ﬁrst two atoms
capture the pattern of group G1, while the third corresponds to group G2. The
fourth atom represents one of the deletions that have been introduced ran-
domly as noise. These results conﬁrm the capability of E-FLLatPA in ﬁnding
shared patterns in the data.
2.5.4
Clustering of Breast Cancer Data
Here we test the performance of E-FLLat on a clustering problem where a
benchmark is available. The clustering procedure takes as input the coeﬃcient
matrix or the reconstructed signal matrix computed by E-FLLat and FLLat.
We show that E-FLLat allows for a better clustering in terms of intra-group
coherence. We performed two diﬀerent experiments: a single chromosome anal-
ysis for chromosomes 17 and 8, and a whole genome analysis.
Data We considered the aCGH dataset from [28], already used by [24]
to test FLLat on real data. The dataset consisted of 44 samples of advanced
primary breast cancer. Each signal measured the CNV of 6691 human genes.

44
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 2.4: Proﬁles of the atoms and coeﬃcient matrix identiﬁed by E-
FLLatPA for the simulated dataset. The atoms are sorted according to their
frequency of occurrence.
The samples were assigned to 3 classes according to tumor grading: 5 samples
were assigned to grade 1(G1), 21 to grade 2(G2), 17 to grade 3(G3), and 1
unassigned.
Parameter Selection The number of atoms J was set diﬀerently for the
considered scenarios. Concerning the analysis based on a single chromosome,
for comparison purposes we follow [24] and set J = 5 for chromosome 17 and
J = 6 for chromosome 8. Regarding the whole genome analysis, we consider
three diﬀerent J ∈{10, 18, 24}, which correspond to the number of princi-
pal components of Y able to explain respectively the 50%, 70%, and 80% of
the variance. For E-FLLat, the rest of the parameters (µ, λ, τ) were set by
searching the best triple in {0.01, 0.1, 1.0, 10, 100} × {0.01, 0.1, 1.0, 10, 100} ×
{0.1, 1.0, 10} according to BIC. Regarding FLLat, the grid of parameters was
deﬁned by some heuristics implemented into the given R package.
Experiment Design and Protocol In the ﬁrst experiment we compared
E-FLLat and FLLat focusing on chromosomes 8 (241 mapped genes) and
17 (382 mapped genes), identiﬁed by [28] as chromosomes with biologically
relevant CNVs. Clustering was performed on Yc, the original raw noisy data
matrix restricted to the chromosome c ∈{8, 17}, on coeﬃcients matrices Θc
EF
and Θc
F , and on the denoised samples matrices ˆYc
EF and ˆYc
F .
In a second experiment, we compared FLLat and E-FLLat considering the
analysis of aCGH data of the whole genome (6691 probes). Clustering was
performed on the original raw noisy data matrix Y, on the coeﬃcients matrix
ΘEF , and the denoised samples matrix ˆYEF . We remark that this context is
not completely appropriate for FLLat, for it was designed to treat chromo-
somes one at a time — indeed the unweighted total variation included into its
model would unfairly penalize variations on borders of the chromosomes.

Regularized Dictionary Learning
45
For clustering, we adopted a hierarchical agglomerative algorithm, using
the city block or Manhattan distance between points d(a, b) = P
i |ai −bi| and
the single linkage criterion d(A, B) = min{d(a, b) : a ∈A, b ∈B} [31]. The
cluster A is linked with the cluster B if the distance d(A, B) is the minimum
with respect to all the other clusters B′.
Moreover, to evaluate the coherence of the obtained dendrogram with re-
spect to the groups G1, G2, and G3, we measured the cophenetic distance
among the samples within each group [32]. For each pair of observations (a, b),
the cophenetic distance is the distance between the two clusters that were
merged to assign the two points in a single new cluster. The average of the
cophenetic distances within each clinical group provides an objective measure
of how the resulting dendrogram describes the diﬀerences between observa-
tions, using the clinical grades as ground truth.
Note that, by design, the values contained in the coeﬃcients matrix pro-
duced by FLLat and E-FLLat could have a diﬀerent range of values (in E-
FLLat the values are positive and bounded). In order to calculate comparable
distance metrics, before clustering and cophenetic distances evaluation, each
estimated coeﬃcients matrix Θ was normalized by is maximum absolute value.
The same preprocessing was also applied on the original aCGH signals and
on the estimated ones ˆY = BΘ.
Results Analysis restricted to chromosome 17. In Figure 2.6 (left) we show
the means of the cophenetic distances calculated for each group of samples
(the unannotated one was not considered) restricted to the chromosome 17.
It is clear that the clustering on the coeﬃcients matrix produced by E-FLLat
places the samples belonging to homogeneous clinical groups (G1, G2, and
G3) closer in the dendrogram. Moreover, also the denoised data matrix ˆY17
EF
shows better discriminative performances with respect to ˆY17
F . This may be
due to the capability of our model to better detect the main altered patterns in
the signals, despite a possibly higher reconstruction error [22]. Such property
ultimately induces a more eﬀective clustering. In Table 2.1 (top) we report a
summary of the averaged cophenetic distances, also including the clustering
on raw signals.
Analysis restricted to chromosome 8. The analysis on chromosome 8 gives
similar results. Figure 2.6 (right) shows the means of the cophenetic distances
calculated for each group of samples, and Table 2.1 (bottom) shows the cor-
responding averaged cophenetic distances.
Whole genome analysis. We present the results obtained with J = 10.
The resulting atoms (see Figure 2.5) describe co-occurrent alterations along
diﬀerent chromosomes but are still fairly simple for a visual interpretation
by the domain experts. For diﬀerent Js we did not note relevant diﬀerences
in terms of ﬁt and clustering. It is important to note that the four most
used atoms of the dictionary extracted by E-FLLat detect the main genomic
alterations on chromosomes 8 and 17 as well as a co-occurrence of deletions
on chromosome 3 and 5. In [28] all these alterations were already indicated as

46
Regularization, Optimization, Kernels, and Support Vector Machines
TABLE 2.1: Average cophenetic distances after clustering for the analysis
restricted to chromosome 17.
G1
G2
G3
Θ17
EF
0.008 ± 0.004
0.079 ± 0.112
0.111 ± 0.124
ˆY17
EF
0.022 ± 0.019
0.476 ± 0.720
0.687 ± 0.795
Θ17
F
0.178 ± 0.044
0.265 ± 0.173
0.517 ± 0.446
ˆY17
F
1.737 ± 0.484
2.945 ± 2.074
5.212 ± 3.851
Y17
19.284 ± 2.374
19.589 ± 3.961
23.941 ± 5.870
G1
G2
G3
Θ8
EF
0.016 ± 0.007
0.054 ± 0.024
0.147 ± 0.142
ˆY8
EF
0.222 ± 0.095
0.842 ± 0.410
1.720 ± 1.135
Θ8
F
0.301 ± 0.095
0.469 ± 0.236
0.951 ± 0.657
ˆY8
F
3.135 ± 1.090
4.962 ± 2.605
9.547 ± 6.638
Y8
12.363 ± 1.165
15.484 ± 4.124
20.150 ± 6.200
being very common but the relation between chromosomes 3 and 5 was not
indicated as co-occurrence and needs further biological validation.
TABLE 2.2: Average cophenetic distances after clustering for the analysis
extended to all chromosomes with J = 10.
G1
G2
G3
ΘEF
0.738 ± 0.541
0.290 ± 0.213
0.463 ± 0.406
ˆYEF
7.988 ± 4.663
4.191 ± 2.795
5.512 ± 3.632
Y
305.76 ± 39.85
290.26 ± 38.04
302.86 ± 34.76
2.6
Conclusion
We presented an algorithm for dictionary learning, which is based on the
alternating proximal algorithm studied by [2] coupled with a fast dual algo-
rithm for the computation of the related proximity operators. This algorithm
is suitable for a general dictionary learning model composed of a data ﬁt term
that accounts for the goodness of the representation, and several convex pe-
nalization terms on the coeﬃcients and atoms, explaining the prior knowledge
at hand. As recently proved by [2], an alternating proximal scheme ensures
better convergence properties than the simpler alternating minimization. We
also show the performance of the proposed algorithm on synthetic, simulated,
and real case scenarios.

Regularized Dictionary Learning
47
FIGURE 2.5: Proﬁles of the ﬁrst four most used atoms for sample recon-
struction (sum of the row of Θ) extracted by E-FLLat on all chromosomes.
The atom #1 maps a general pattern of alterations, and it is responsible for
a high proportion of signal reconstruction. E-FLLat found the alterations on
chromosomes 8 and 17, and also detected co-occurring alterations on chromo-
somes 3 and 5. Vertical lines indicate chromosomes’ boundaries.

48
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 2.6: Average cophenetic distances for the groups G1, G2, and G3
on chromosome 17 (left) and chromosome 8 (right). E-FLLat always has better
clustering results (also see Table 2.1). Moreover, it is also interesting to note
that clustering the denoised samples by E-FLLat and FLLat, the former has
better results, suggesting also a higher quality of the dictionary atoms used
to reconstruct the samples.
Bibliography
[1] M. Aharon, M. Elad, and A. Bruckstein.
K-SVD: An Algorithm for
Designing Overcomplete Dictionaries for Sparse Representation. IEEE
Transactions on Signal Processing, 54(11):4311–4322, 2006.
[2] H. Attouch, J. Bolte, P. Redont, and A. Soubeyran. Proximal Alternat-
ing Minimization and Projection Methods for Nonconvex Problems: An
Approach Based on the Kurdyka-Lojasiewicz Inequality. Mathematics of
Operational Research, 35(2):438–457, 2010.
[3] H. Attouch, J. Bolte, and B. F. Svaiter. Convergence of descent meth-
ods for semi-algebraic and tame problems: proximal algorithms, forward-
backward splitting, and regularized Gauss-Seidel methods. Math. Pro-
gram., 137(1-2, Ser. A):91–129, 2013.
[4] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Op-
erator Theory in Hilbert Spaces. CMS Books in Mathematics/Ouvrages
de Mathématiques de la SMC. Springer, New York, 2011.
[5] A. Beck and M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algo-
rithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences,
2(1):183–202, 2009.
[6] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 2nd edition,
1999.

Regularized Dictionary Learning
49
FIGURE 2.7: Average cophenetic distances for the groups G1, G2, and G3
on all chromosomes and J = 10 (also see Table 2.2).
[7] J. Bolte, A. Daniilidis, and A. Lewis. The Lojasiewicz inequality for non-
smooth subanalytic functions with applications to subgradient dynamical
systems. SIAM J. Optim., 17(4):1205–1223 (electronic), 2006.
[8] S. J. Diskin, T. Eck, J.Greshock, Y. P. Mosse, T. Naylor, C. J. Stoeckert,
B. L. Weber, J. M. Maris, and G. R. Grant. Stac: A method for testing the
signiﬁcance of DNA copy number aberrations across multiple array-CGH
experiments. Genome Res, 16(9):1149–58, Sep 2006.
[9] C. Févotte, N. Bertin, and J-L. Durrieu. Nonnegative Matrix Factor-
ization with the Itakura-Saito Divergence: With Application to Music
Analysis. Neural Computation, 21(3):793–830, 2008.
[10] R. Gribonval, R. Jenatton, F. Bach, M. Kleinsteuber, and M. Seibert.
Sample Complexity of Dictionary Learning and Other Matrix Factoriza-
tions. ArXiv e-prints, 2013.
[11] R. Grosse, R. Raina, H. Kwong, and A. Ng. Shift-invariance sparse coding
for audio classiﬁcation. Proceedings of the Twenty-Third Annual Confer-
ence on Uncertainty in Artiﬁcial Intelligence (UAI-07), pages 149–158,
2007.
[12] R. Jenatton, R. Gribonval, and F. Bach. Local stability and robustness of
sparse dictionary learning in the presence of noise. ArXiv e-prints, 2012.

50
Regularization, Optimization, Kernels, and Support Vector Machines
[13] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
Proximal Methods
for Hierarchical Sparse Coding. Journal of Machine Learning Research,
12:2297–2334, 2011.
[14] A. Kallioniemi, O.-P. Kallioniemi, D. Sudar, D. Rutovitz, J.W. Gray,
F. Waldman, and D. Pinkel.
Comparative genomic hybridization for
molecular cytogenetic analysis of solid tumors. Science (New York, N.Y.),
258(5083):818–21, 1992.
[15] M. Khojasteh, W.L. Lam, R.K. Ward, and C. MacAulay.
A stepwise
framework for the normalization of array CGH data. BMC bioinformat-
ics, 6:274, 2005.
[16] K. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T.-W. Lee, and
T. J. Sejnowski. Dictionary Learning Algorithms for Sparse Representa-
tion. Neural Comput., 15(2):349–396, 2003.
[17] W. R Lai, M. D Johnson, R. Kucherlapati, and P. J. Park. Comparative
analysis of algorithms for identifying ampliﬁcations and deletions in array
CGH data. Bioinformatics (Oxford, England), 21(19):3763–70, 2005.
[18] H. Lee, A. Battle, R. Raina, and A.Y. Ng. Eﬃcient sparse coding al-
gorithms. Advances in Neural Information Processing Systems (NIPS),
2007.
[19] J. Mairal, F. Bach, and J. Ponce.
Task-Driven Dictionary Learn-
ing. Pattern Analysis and Machine Intelligence, IEEE Transactions on,
34(4):791–804, 2012.
[20] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local
sparse models for image restoration. In Computer Vision, 2009 IEEE
12th International Conference on, pages 2272–2279, 2009.
[21] S. Masecchia, A. Barla, S. Salzo, and A. Verri. Dictionary learning im-
proves subtyping of breast cancer aCGH data. In Engineering in Medicine
and Biology Society (EMBC), 2013 35th Annual International Conference
of the IEEE, pages 604–607, 2013.
[22] S. Masecchia, S. Salzo, A. Barla, and A. Verri. A dictionary learning
based method for aCGH segmentation. Proc. of ESANN, 2013.
[23] A. Maurer and M. Pontil.
K-dimensional coding schemes in Hilbert
spaces. IEEE Trans. Inform. Theory, 56(11):5839–5846, 2010.
[24] G. Nowak, T. Hastie, J.R. Pollack, and R. Tibshirani.
A fused lasso
latent feature model for analyzing multi-sample aCGH data. Biostatistics,
12(4):776–791, 2011.

Regularized Dictionary Learning
51
[25] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete
basis set: A strategy employed by V1? Vision Research, 37(23):3311–
3325, 1997.
[26] A. B. Olshen, E. S. Venkatraman, R. Lucito, and M. Wigler. Circular
binary segmentation for the analysis of array-based DNA copy number
data. Biostatistics, 5(4):557–572, 2004.
[27] R. Pique-Regi, A. Ortega, and S. Asgharzadeh. Joint estimation of copy
number variation and reference intensities on multiple DNA arrays using
GADA. Bioinformatics, 25(10):1223–1230, 2009.
[28] J. R. Pollack, T. Sørlie, C. M. Perou, C.A. Rees, S. S. Jeﬀrey, P. E. Lon-
ning, R. Tibshirani, D. Botstein, A. L. Børresen-Dale, and P. O. Brown.
Microarray analysis reveals a major direct role of DNA copy number alter-
ation in the transcriptional program of human breast tumors. Proceedings
of the National Academy of Sciences, 99(20):12963–12968, 2002.
[29] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught Learn-
ing: Transfer Learning from Unlabeled Data. In Proceedings of the 24th
International Conference on Machine Learning, ICML ’07, pages 759–
766, 2007.
[30] G. Schwarz. Estimating the Dimension of a Model. The Annals of Statis-
tics, 6(2):461–464, 1978.
[31] R. Sibson. SLINK: An optimally eﬃcient algorithm for the single-link
cluster method. The Computer Journal, 16(1):30–34, 1973.
[32] R.R. Sokal and F.J. Rohlf. The comparison of dendrograms by objective
methods. Taxon, 11(2):33–40, 1962.
[33] Z. Tian, H. Zhang, and R. Kuang.
Sparse Group Selection on Fused
Lasso Components for Identifying Group-speciﬁc DNA Copy Number
Variations. In Proc. of IEEE International Conference on Data Mining
(ICDM), pages 665–674, 2012.
[34] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Spar-
sity and smoothness via the fused lasso. Journal of the Royal Statistical
Society - Series B: Statistical Methodology, 67(1):91–108, 2005.
[35] I. Tosic and P. Frossard. Dictionary learning. Signal Processing Magazine,
IEEE, 28(2):27–38, 2011.
[36] D. Vainsencher, S. Mannor, and A. M. Bruckstein. The sample complexity
of dictionary learning. J. Mach. Learn. Res., 12:3259–3281, 2011.
[37] J.-P. Vert and K. Bleakley.
Fast detection of multiple change-points
shared by many signals using group LARS. Advances in Neural Informa-
tion Processing Systems 23, 1:1–9, 2010.

52
Regularization, Optimization, Kernels, and Support Vector Machines
[38] S. Villa, S. Salzo, L. Baldassarre, and A. Verri.
Accelerated and in-
exact forward-backward algorithms.
SIAM Journal on Optimization,
23(3):1607–1633, 2013.

Chapter 3
Hybrid Conditional
Gradient-Smoothing Algorithms
with Applications to Sparse and Low
Rank Regularization
Andreas Argyriou∗
École Centrale Paris, Center for Visual Computing
Marco Signoretto∗
KU Leuven, ESAT-STADIUS
Johan A.K. Suykens∗
KU Leuven, ESAT-STADIUS
3.1
Introduction ......................................................
54
3.2
Preliminaries from Convex Analysis .............................
55
3.3
Generalized Conditional Gradient Algorithm ....................
57
3.3.1
Convergence Rate .......................................
60
3.3.2
Connections to Mirror Descent and Gradient Descent ..
60
3.4
Hybrid Conditional Gradient-Smoothing Algorithm ............
61
3.4.1
Description of the Hybrid Algorithm ....................
61
3.4.2
Convergence Rate .......................................
64
3.4.3
Minimization of Lipschitz Continuous Functions .......
68
3.4.4
Implementation Details ..................................
69
3.5
Applications ......................................................
69
3.6
Simulations .......................................................
72
3.6.1
Simultaneous Sparse and Low Rank Regularization ....
72
3.6.2
Sparse PCA ..............................................
75
3.7
Conclusion ........................................................
78
Acknowledgments ................................................
78
Bibliography ......................................................
78
∗andreas.argyriou@ecp.fr, marco.signoretto@esat.kuleuven.be, johan.suykens@esat.ku-
leuven.be
53

54
Regularization, Optimization, Kernels, and Support Vector Machines
3.1
Introduction
Conditional gradient methods are old and well-studied optimization algo-
rithms. Their origin dates at least to the 1950s and the Frank-Wolfe algorithm
for quadratic programming [18], but they apply to much more general opti-
mization problems. General formulations of conditional gradient algorithms
have been studied in the past and various convergence properties of these
algorithms have been proven. Moreover, such algorithms have found appli-
cation in many ﬁelds, such as optimal control, statistics, signal processing,
computational geometry and machine learning. Currently, interest in condi-
tional gradient methods is undergoing a revival because of their computational
advantages when applied to certain large scale optimization problems. Such ex-
amples are regularization problems involving sparsity or low rank constraints,
which appear in many widely used methods in machine learning.
Inspired by such algorithms, in this chapter we study a ﬁrst-order method
for solving certain convex optimization problems. We focus on problems of
the form
min {f(x) + g(Ax) + ω(x) : x ∈H}
(3.1)
over a real Hilbert space H. We assume that f is a convex function with Hölder
continuous gradient, g a Lipschitz continuous convex function, A a bounded
linear operator, and ω a convex function deﬁned over a bounded domain. We
also assume that the computational operations available are the gradient of
f, the proximity operator of g, and a subgradient of the convex conjugate ω∗.1
A particularly common type of problems covered by (3.1) is
min {f(x) + g(Ax) : x ∈C} ,
(3.2)
where C is a bounded, closed, convex subset of H. Common among such exam-
ples are regularization problems with one or more penalties in the objective
(as the term g ◦A) and one penalty as a constraint described by C.
Before presenting the algorithm, we review in Section 3.3 a generic condi-
tional gradient algorithm that has been well studied in the past. This standard
algorithm can be used for solving problems of the form (3.1) whenever g = 0.
However, the conditional gradient algorithm cannot handle problems with a
nonzero term g, because it would require computation of a subgradient of a
composite convex conjugate function, namely a subgradient of (g ◦A + ω)∗.
In many cases of interest, there is no simple rule for such subgradients and
the computation itself requires an iterative algorithm.
Thus, in Section 3.4 we discuss an alternative approach that combines
ideas from both conditional gradient algorithms and smoothing proximal al-
gorithms, such as Nesterov smoothing. We call the resulting algorithm a hybrid
1For the precise assumptions required, see Assumptions 3.2, 3.3.

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 55
conditional gradient-smoothing algorithm, in short, HCGS. This approach in-
volves smoothing the g term, that is, approximating g with a function whose
gradient is Lipschitz continuous. Besides this modiﬁcation, HCGS is similar to
the conditional gradient algorithm. We show that, for suitable choices of the
smoothing parameter, the estimates of the objective in HCGS converge to the
minimum of (3.1). Moreover, the convergence rate is of the order of O
  1
ε2

iterations for attaining an accuracy of ε in terms of the objective. We do not
claim originality, however, since similar theoretical results have appeared in
recent work by Lan [34].
Our main focus is on highlighting applications of the hybrid approach on
certain applications of interest. To demonstrate the applicability of HCGS
to regularization problems from machine learning, we present simulations of
matrix problems with simultaneous sparsity and low rank penalizations. Ex-
amples of such applications are graph denoising, link prediction in social net-
works, covariance estimation, and sparse PCA. Each of these problems in-
volves two penalties, an elementwise ℓ1 norm to promote sparsity, and a trace
norm to promote low rank. Standard algorithms may not be practical in high
dimensional problems of this type. As mentioned above, standard conditional
gradient methods require a subgradient computation of a complicated func-
tion, whereas proximal algorithms or subgradient-based algorithms require
an expensive singular value decomposition per iteration. In contrast, HCGS
requires only computation of dominant singular vectors, which is more prac-
tical by means of the power iteration. Thus, even though HCGS exhibits a
slower asymptotic rate of convergence than conditional gradient algorithms,
Nesterov’s method or the forward-backward algorithm, it scales much better
to large matrices than these methods.
3.2
Preliminaries from Convex Analysis
Throughout the chapter, H is a real Hilbert space endowed with norm
∥· ∥and inner product ⟨·, ·⟩. As is standard in convex analysis, we consider
extended value functions f : H →(−∞, +∞], which can take the value +∞.
With this notation, constraints can be written as indicator functions that take
the zero value inside the feasible set and +∞outside.
Deﬁnition 3.1. The domain of a function f : H →(−∞, +∞] is deﬁned as
the set dom f = {x ∈H : f(x) < +∞}.
Deﬁnition 3.2. The function f : H →(−∞, +∞] is called proper if dom f ̸=
∅.
Deﬁnition 3.3. The set of proper lower semicontinuous convex functions
from H to (−∞, +∞] is denoted by Γ0(H).

56
Regularization, Optimization, Kernels, and Support Vector Machines
Deﬁnition 3.4. Let f : H →[−∞, +∞]. The convex conjugate of f is the
function f ∗: H →[−∞, +∞] deﬁned as
f ∗(x) = sup{⟨u, x⟩−f(u) : u ∈H}
(3.3)
for every x ∈H.
Theorem 3.1. (Fenchel-Moreau) [4, Thm. 13.32]. Every function f ∈Γ0(H)
is biconjugate,
f ∗∗= f .
(3.4)
Moreover, f ∗∈Γ0(H).
Deﬁnition 3.5. Let f : H →(−∞, +∞] be proper. A subgradient of f at
x ∈H is a vector u ∈H satisfying
⟨u, y −x⟩+ f(x) ≤f(y)
(3.5)
for every y ∈H. The set of subgradients of f at x is called the subdiﬀerential
of f at x and is denoted as ∂f(x).
Proposition 3.1. Let f : H →(−∞, +∞] be proper and x ∈H. Then
∂f(x) ̸= ∅implies x ∈dom f.
Theorem 3.2. [4, Thm. 16.23]. Let f ∈Γ0(H), x ∈H, u ∈H. Then
u ∈∂f(x) ⇐⇒x ∈∂f ∗(u) ⇐⇒f(x) + f ∗(u) = ⟨x, u⟩,
(3.6)
(∂f)−1 = ∂f ∗.
(3.7)
Theorem 3.3. [4, Thm. 16.37]. Let f ∈Γ0(H), K a Hilbert space, g ∈Γ0(K)
and A : H →K a bounded linear operator. If dom g = K then
∂(f + g ◦A) = ∂f + A∗◦∂g ◦A .
Theorem 3.4. [4, Thm. 16.2]. Let f : H →(−∞, +∞] be proper. Then
argmin f = {x ∈H : 0 ∈∂f(x)} .
Deﬁnition 3.6. The function f ∈Γ0(H) is called (p, L)-smooth, where L >
0, p ∈(0, 1], if f is Fréchet diﬀerentiable on H with a Hölder continuous
gradient,
∥∇f(x) −∇f(y)∥≤L ∥x −y∥p
∀x, y ∈H.

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 57
The case p = 1 corresponds to functions with Lipschitz continuous gradient,
which appear frequently in optimization. The following lemma is sometimes
called the descent lemma [4, Cor. 18.14].
Lemma 3.1. If the function f ∈Γ0(H) is (p, L)-smooth then
f(x) ≤f(y) + ⟨x −y, ∇f(y)⟩+
L
p + 1∥x −y∥p+1
∀x, y ∈H.
(3.8)
Theorem 3.5. Let ω ∈Γ0(H). Then dom ω is contained in the ball of radius
ρ ∈R+, if and only if the convex conjugate ω∗is ρ-Lipschitz continuous on
H.
Proof. See [47], or [48, Cor. 13.3.3] for a ﬁnite-dimensional version.
Corollary 3.1. If the function ω ∈Γ0(H) has bounded domain then ∂ω∗is
nonempty everywhere on H.
Proof. Follows from Theorem 3.5 and [4, Prop. 16.17].
3.3
Generalized Conditional Gradient Algorithm
In this section, we brieﬂy review the conditional gradient algorithm in
one of its many formulations. We focus on convex optimization problems of
a general type and discuss how a generalized conditional gradient algorithm
applies to such problems. We should note that this algorithm is not the most
generic formulation that has been studied; see, for example [34, 13], but it
covers a broad variety of optimization problems in machine learning.
Speciﬁcally, we consider the optimization problem
min {f(x) + ω(x) : x ∈H}
(3.9)
where we make the following assumptions.
Assumption 3.1.
• f, ω ∈Γ0(H)
• f is (1, L)-smooth
• dom ω is bounded, that is, there exists ρ ∈R++ such that ∥x∥≤ρ, ∀x ∈
dom ω.

58
Regularization, Optimization, Kernels, and Support Vector Machines
Algorithm 1 Generalized conditional gradient algorithm.
Input x1 ∈dom ω
for k = 1, 2, . . . do
yk ←an element of ∂ω∗(−∇f(xk))
(I)
xk+1 ←(1 −αk)xk + αkyk
(II)
end for
Remark 3.1. Under Assumption 3.1, the problem (3.9) admits a minimizer.
The reason is that, since dom ω is bounded,
lim
∥x∥→+∞
f(x)+ω(x)
∥x∥
= +∞(super-
coercivity).
The Fenchel dual problem associated with (3.9) is
max {−f ∗(−z) −ω∗(z) : z ∈H} .
(3.10)
Due to Fenchel’s duality theorem and the fact that dom f = H, the duality
gap equals zero and the maximum in (3.10) is attained [4, Thm. 15.23].
Algorithm 1 has been used frequently in the past for solving problems
of the type (3.9). It is a generalization of algorithms such as the Frank-
Wolfe algorithm for quadratic programming [18, 21] and conditional gradi-
ent algorithms [32, 14, 15]. Algorithm 1 applies to the general setting of
convex optimization problems of the form (3.9) which satisfy Assumption
3.1. In such general forms, the algorithm has been known and studied for
a long time in control theory and several of its convergence properties have
been obtained [31, 32, 15, 14, 16]. More recently, interest in the family of
conditional gradient algorithms has been revived, especially in theoretical
computer science, machine learning, computational geometry and elsewhere
[24, 23, 29, 3, 20, 54, 56, 27, 10, 22, 33, 19]. Some of these algorithms have ap-
peared independently in various ﬁelds, such as statistics and signal processing,
under diﬀerent names and various guises. For example, it has been observed
that conditional gradient methods are related to boosting, greedy methods for
sparse problems [10, 51], and to orthogonal matching pursuit [28, 27]. Some
very recent papers [3, 53] show an equivalence to the optimization method of
mirror descent, which we discuss brieﬂy in Section 3.3.2.
One reason for the popularity and the revival of interest in conditional
gradient methods has been their applicability to large scale problems. This
advantage is evident, for example, in comparison to proximal methods, see
[11] and references therein, and especially in optimization problems involving
matrices. Conditional gradient methods generally trade oﬀa slower conver-
gence rate (number of iterations) for lower complexity of each iteration step.
The accelerated proximal gradient methods [43] beneﬁt from the “optimal”
O
q
1
ε

rate (where ε is the accuracy with respect to the optimization ob-
jective), whereas conditional gradient methods exhibit a slower O
  1
ε

rate.
On the other side, each step in the proximal methods requires computation

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 59
of the proximity operator [37, 4] (see Section 3.4), which in some cases can be
particularly costly. For example, the proximity operator of the trace norm of
a matrix X ∈Rd×n,
∥X∥tr =
min{d,n}
X
i=1
σi(X) ,
(3.11)
where σi(X) denote the singular values of X, requires computation of a com-
plete singular value decomposition. In contrast, a conditional gradient method
need only compute a dominant pair of left-right singular vectors, and such a
computation scales better to large matrices [29].
In general, as Algorithm 1 indicates, conditional gradient methods require
computation of dual subgradients. Often, this is a much less expensive oper-
ation than projection or the proximity operation. In other cases, proximity
operations may not be feasible in a ﬁnite number of steps, whereas dual sub-
gradients are easy to compute. An obvious such case is ℓp or Schatten-ℓp
regularization; see [28]. Other cases of interest occur when ω is the conjugate
of a max-function. Then the dual subgradient could be fast to compute while
the proximity operation may be complex.
Finally, another advantage of conditional gradient methods is that they
build their estimate of the solution incrementally. This implies that, in earlier
iterations, time and space costs are low and the algorithm may be stopped once
an estimate of the desired parsimony is obtained (this could be, for example,
a vector of certain sparsity or a matrix of certain rank). Proximal methods, in
contrast, do not necessarily obtain the desired parsimony until later iterations
(and even then it is not “exact”).
Formulation (3.9) covers many optimization problems studied so far in the
conditional gradients literature and provides a concise description of varia-
tional problems amenable to the standard conditional gradient algorithm. In
Section 3.4 we extend the applicability to problems with multiple penalties,
by combining conditional gradients and smoothing techniques.
We remark that formulation (3.9) is valid in a generalized Hilbert space
setting, so that it can be applied to inﬁnite dimensional problems. This is par-
ticularly useful for kernel methods in machine learning, for example, kernelized
support vector machines or structured SVMs [52] and nuclear or Schatten-ℓp
regularization of operators [1].
To motivate Algorithm 1, consider the convex optimization problem (3.9).
By Theorems 3.3 and 3.4, ˆx ∈H is a minimizer of (3.9) if and only if
0 ∈∇f(ˆx) + ∂ω(ˆx)
(3.12)
or, equivalently,
−∇f(ˆx) ∈∂ω(ˆx) ⇐⇒ˆx ∈∂ω∗(−∇f(ˆx)) ,
(3.13)
where we have used Theorem 3.2. Thus, step (I) in Algorithm 1 reﬂects the
ﬁxed point equation (3.13). However, ∂ω∗(−∇f(xk)) is not a singleton in

60
Regularization, Optimization, Kernels, and Support Vector Machines
general and some elements of this set may be far from the minimizers of the
problem. Hence step (II), which weighs the new estimate with past ones, is
necessary. With any aﬃne weighting like that of step (II), the ﬁxed point
equation (3.13) still holds.
Remark 3.2. Algorithm 1 is well deﬁned, since the subdiﬀerential at step (I)
is always nonempty, due to Corollary 3.1 and Assumption 3.1.
Finally, let us note that several variants of Algorithm 1 are possible, in the
spirit of the extensive literature on conditional gradient methods. For example,
there are various techniques (like line search) for the choice of coeﬃcients αk,
more of the past iterates may be used in (II) and so on.
3.3.1
Convergence Rate
Theorem 3.6. If, for every k ∈N, αk ∈[0, 1], then xk ∈dom ω and
f(xk+1) + ω(xk+1) −f(x) −ω(x) ≤(1 −αk)
 f(xk) + ω(xk) −f(x) −ω(x)

+ 2α2
kLρ2
for every x ∈dom ω, k ∈N.
Theorem 3.6 implies an O
  1
k

convergence rate with respect to the objec-
tive values f(xk) + ω(xk) −f(ˆx) −ω(ˆx), where ˆx is a minimizer of (3.9). This
rate can be attained, for example, with the choice αk =
2
k+1.
Corollary 3.2. If αk =
2
k+1, for every k ∈N, then
f(xk+1) + ω(xk+1) −f(x) −ω(x) ≤8Lρ2
k + 1
(3.14)
for every x ∈dom ω, k ∈N.
See [14, 15, 29, 10, 3] and references therein for these and related results,
as well as for bounds involving the duality gap estimates. It is also known
that the lower bound for conditional gradient and similar algorithms is of the
same order [9, 28, 34].
3.3.2
Connections to Mirror Descent and Gradient Descent
It has been observed recently [3, 53] that the conditional gradient algorithm
is equivalent to a mirror descent algorithm in the dual. The basic mirror
descent algorithm [5, 38, 30] may be written as the iteration
xk+1 ←an element of xk −tk∂ϕ(∇ψ∗(xk)) ,
(3.15)
where tk > 0 are step sizes, ψ is strongly convex on a closed convex set C and
ϕ is convex and Lipschitz continuous on C. Setting ω = (ϕ ◦(−I))∗, where

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 61
I denotes the identity operator, and f = ψ∗, algorithm (3.15) rewrites as a
variant of Algorithm 1 (in which the update is not a convex combination).
The set C can be viewed as the domain of ω.
Consequently, when (3.9) is a proximity computation (that is, when
f =
1
2β ∥· ∥2, β > 0) the conditional gradient algorithm 1 is equivalent to
a subgradient descent in the dual. In such cases ∇f =
1
β I and Algorithm 1
becomes
xk+1 ∈(1 −αk)xk + αk∂ω∗
−1
β xk

.
(3.16)
Letting h =

ω∗◦

−1
β I
∗
, by the chain rule (Theorem 3.3) this iteration
is equivalent to
xk+1 ∈xk −αk(I + β ∂h∗)(xk) .
(3.17)
In particular, when ω is µ-strongly convex (and hence ω∗is (1, 1
µ)-smooth) and
αk ≤
µβ
1+µβ for every k ∈N, the above iteration is equivalent to a proximal
point algorithm [11, 17, 49] because of [4, Thm. 18.15]. Note that not all cases
of subgradient descent are covered, since ω should have bounded domain,
implying that the dual objective function should be a quadratic perturbation
of a Lipschitz continuous function.
3.4
Hybrid Conditional Gradient-Smoothing Algorithm
We now introduce Algorithm 2, an extension of conditional gradient meth-
ods to optimization problems on bounded domains that contain smooth and
Lipschitz continuous terms.
3.4.1
Description of the Hybrid Algorithm
Formally, we consider the class of optimization problems of the form
min {f(x) + g(Ax) + ω(x) : x ∈H}
(3.18)
where we make the following assumptions:
Assumption 3.2.
• f, ω ∈Γ0(H)
• g ∈Γ0(K), K is a Hilbert space
• A : H →K is a bounded linear operator
• f is (p, Lf)-smooth

62
Regularization, Optimization, Kernels, and Support Vector Machines
• g is Lg-Lipschitz continuous on K
• dom ω is bounded, that is, there exists ρ ∈R++ such that ∥x∥≤ρ, ∀x ∈
dom ω
Remark 3.3. Under Assumption 3.2, problem (3.18) admits a minimizer. As
in Remark 3.1, the reason is growth of the objective function at inﬁnity (the
objective equals +∞outside the feasible set, which is bounded).
In order for the algorithm to be practical, we require that
Assumption 3.3.
• the gradient of f is simple to compute at every x ∈H,
• a subgradient of ω∗is simple to compute at every x ∈H,
• the proximity operator of βg is simple to compute for every β > 0, x ∈H.
The proximity operator was introduced by Moreau [37] as the (unique) mini-
mizer
proxg(x) = argmin
1
2∥x −u∥2 + g(u) : u ∈H

.
(3.19)
For a review of the numerous applications of proximity operators to optimiza-
tion, see, for example, [12, 11] and references therein.
The following are some examples of optimization problems that belong to
the general class (3.18).
Example 3.1. Regularization with two norm penalties:
min

f(x) + λ ∥x∥a : ∥x∥b ≤B, x ∈Rd	
(3.20)
where f is (p, Lf)-smooth, λ > 0 and ∥· ∥a, ∥· ∥b can be any norms on Rd.
Example 3.2. Regularization with a linear composite penalty and a norm:
min

f(x) + λ ∥Ax∥a : ∥x∥b ≤B, x ∈Rd	
(3.21)
where f is (p, Lf)-smooth, λ > 0, ∥·∥a, ∥·∥b are norms on Rδ, Rd, respectively,
and A ∈Rδ×d.
Example 3.3. Regularization with multiple linear composite penalties and a
norm:
min
(
f(x) +
n
X
i=1
λi ∥Aix∥ai : ∥x∥b ≤B, x ∈Rd
)
(3.22)
where f is (p, Lf)-smooth and, for all i ∈{1, . . . , n}, λi > 0, ∥· ∥ai, ∥· ∥b are
norms on Rδi, Rd, respectively, and Ai ∈Rδi×d. Such problems can be seen
as special cases of Example 3.2 by applying the classical direct sum technique,
δ = Pn
i=1 δi, A =
 A1
...
An
!
, ∥(vi)n
i=1∥a = Pn
i=1 λi ∥vi∥ai.

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 63
Algorithm 2 Hybrid conditional gradient-smoothing algorithm.
Input x1 ∈dom ω
for k = 1, 2, . . . do
zk ←−∇f(xk) −
1
βk A∗Axk +
1
βk A∗proxβkg(Axk)
(I)
yk ←an element of ∂ω∗(zk)
(II)
xk+1 ←(1 −αk)xk + αkyk
(III)
end for
We propose to solve problems like the above with Algorithm 2.2 We call it
a hybrid conditional gradient-smoothing algorithm (HCGS in short), because
it involves a smoothing of function g with parameter βk. For any β > 0, the
β-smoothing of g is the Moreau envelope of g, that is, the function gβ deﬁned
as
gβ(x) := min
 1
2β ∥x −u∥2 + g(u) : u ∈H

∀x ∈H .
(3.23)
The function gβ is a smooth approximation to g (in fact, the best possible
approximation of 1
β smoothness), as summarized in the following lemmas from
the literature.
Lemma 3.2 (Proposition 12.29 in [4]). Let g ∈Γ0(H), β > 0. Then gβ is
(1, 1
β )-smooth and its gradient can be obtained from the proximity operator of
g as: ∇gβ(x) = 1
β
 x −proxβg(x)

.
Lemma 3.3. Let g ∈Γ0(H) be Lg-Lipschitz continuous and β > 0. Then
• gβ ≤g ≤gβ + 1
2βL2
g
• if β ≥β′ > 0, then gβ ≤gβ′ ≤gβ + 1
2(β −β′)L2
g.
Proof. Deﬁne Ψx(u) =
1
2β ∥x −u∥2 + g(u). We have gβ(x) = minu Ψx(u) ≤
Ψx(x) = g(x), and this proves the left hand side of the ﬁrst property. For the
other side of the inequality, we have
Ψx(u) = 1
2β ∥x −u∥2 + g(u) −g(x) + g(x) ≥1
2β ∥x −u∥2 + g(x) −Lg∥x −u∥
where we have used the Lipschitz property of g. This implies that
gβ(x) = min
u Ψx(u) ≥g(x) + inf
u
 1
2β ∥x −u∥2 −Lg∥x −u∥

= g(x) −1
2βL2
g .
The second property follows from the ﬁrst one and (gβ′)β−β′ = gβ (Proposition
12.22 in [4]).
2Algorithm 2 is well-deﬁned (see Remark 3.2).

64
Regularization, Optimization, Kernels, and Support Vector Machines
Thus, the smoothing parameter β controls the tradeoﬀbetween the smooth-
ness and the quality of approximation.
At each iteration, the hybrid Algorithm 2 computes the gradient of the
smoothed part f + gβk ◦A, where βk is the adaptive smoothing parameter. By
Lemma 3.2 and the chain rule, its gradient equals
∇(f + gβk ◦A)(x) = ∇f(x) + 1
βk
A∗ Ax −proxβkg(Ax)

.
The function f is (p, Lf)-smooth and the function gβk ◦A is (1, 1
βk ∥A∥2)-
smooth. By selecting βk that approaches 0 as k increases, we ensure that gβk
approaches g.
Algorithm 2 can be viewed as an extension of conditional gradient al-
gorithms and of Algorithm 1. But besides conditional gradient methods, the
algorithm also exploits ideas from proximal algorithms obtained by smoothing
Lipschitz terms in the objective. These methods are primarily due to Nesterov
and have been successfully applied to many problems [42, 41, 44, 25, 7]. The
smoothing we apply here is a type of Moreau envelope as in the variational
problem (3.23), which is connected to Nesterov smoothing; see, for example,
[44, 7].
However, unlike Nesterov’s smoothing and other proximal methods, in our
method we choose not to smooth function ω, or apply any other proximity-
like operation to it. We do this because computation of the proximity operator
of ω is not available in the settings that we consider here.3 For example, if
ω expresses a trace norm constraint, the proximity computation requires a
full singular value decomposition, which does not scale well with the size of
the matrix. In contrast, the dual subgradient requires only computation of a
single pair of dominant singular vectors and this is feasible even for very large
matrices using the power method or Lanczos algorithms.
3.4.2
Convergence Rate
A bound on the convergence rate of the objective function can be obtained
by ﬁrst bounding the convergence of the smoothed objective in a recursive
way. The required number of iterations is a function of p and ε, where p is
the smoothing exponent of f and ε is the accuracy in terms of the objective
function. Regarding the proof technique, we should note that the HCGS Al-
gorithm 2 and the proof of its convergence properties are mostly related to
conditional gradient methods. On the other side, the proof technique does not
share similarities with proximal methods such as ISTA or FISTA [6, 39, 40].
Theorem 3.7. Suppose that, for every k ∈N, αk ∈[0, 1] and βk ≥βk+1 > 0.
3Note that Nesterov’s smoothing would require ω to be Lipschitz continuous, and hence
it does not apply directly to the case of bounded dom ω but to a similar regularization
problem with ω as a penalty in the objective.

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 65
Let F = f + g ◦A, Fk = f + gβk ◦A. Then xk ∈dom ω, for every k ∈N, and
Fk+1(xk+1)+ω(xk+1)−F(x)−ω(x) ≤(1−αk)
 Fk(xk)+ω(xk)−F(x)−ω(x)

+ (2ρ)p+1Lf
p + 1
αp+1
k
+ 2∥A∥2ρ2 α2
k
βk
+ 1
2(βk −βk+1)L2
g
(3.24)
for every x ∈dom ω, k ∈N.
Proof. For every k ∈N, we apply the descent Lemma 3.1 twice to obtain that
f(xk+1) ≤f(xk) + ⟨∇f(xk), xk+1 −xk⟩+
Lf
p + 1∥xk+1 −xk∥p+1
= f(xk) + αk⟨∇f(xk), yk −xk⟩+ αp+1
k
Lf
p + 1 ∥yk −xk∥p+1
(3.25)
and
gβk(Axk+1) ≤gβk(Axk) + ⟨∇(gβk ◦A)(xk), xk+1 −xk⟩+
1
2βk
∥A∥2∥xk+1 −xk∥2
= gβk(Axk) + αk⟨∇(gβk ◦A)(xk), yk −xk⟩+ α2
k
2βk
∥A∥2∥yk −xk∥2 .
(3.26)
Applying Lemma 3.3 to gβk(Axk+1) yields
gβk+1(Axk+1) ≤gβk(Axk) + αk⟨∇(gβk ◦A)(xk), yk −xk⟩+ α2
k
2βk
∥A∥2∥yk −xk∥2
+ 1
2(βk −βk+1)L2
g .
Adding (3.25) and (3.26) we obtain that
Fk+1(xk+1) ≤Fk(xk) + αk⟨∇Fk(xk), yk −xk⟩+ αp+1
k
Lf
p + 1 ∥yk −xk∥p+1
+ α2
k
2βk
∥A∥2∥yk −xk∥2 + 1
2(βk −βk+1)L2
g .
(3.27)
By Theorem 3.2 and Proposition 3.1, yk ∈dom ω for every k ∈N. Since
αk ∈[0, 1], applying an induction argument yields that xk ∈dom ω, for every
k ∈N. Thus, the values of the objective generated by the algorithm are ﬁnite.
From the construction of yk in steps (I), (II) and Theorem 3.2, we obtain that,
for every x ∈dom ω,
⟨yk, −∇Fk(xk)⟩−ω(yk) ≥⟨x, −∇Fk(xk)⟩−ω(x)
and hence that
⟨yk −xk, −∇Fk(xk)⟩−ω(yk) ≥⟨x −xk, −∇Fk(xk)⟩−ω(x)
≥Fk(xk) −Fk(x) −ω(x) .

66
Regularization, Optimization, Kernels, and Support Vector Machines
Applying Lemma 3.3 to gβk(Ax) yields
⟨yk −xk, −∇Fk(xk)⟩−ω(yk) ≥Fk(xk) −F(x) −ω(x)
and, therefore,
αk⟨yk −xk, −∇Fk(xk)⟩−αkω(yk) ≥αkFk(xk) −αk(F(x) + ω(x)) .
(3.28)
Adding (3.27) and (3.28), we obtain that
Fk+1(xk+1) + αkFk(xk) −αk(F(x) + ω(x)) ≤Fk(xk) −αkω(yk)
+ αp+1
k
Lf
p + 1 ∥yk −xk∥p+1 + α2
k
2βk
∥A∥2∥yk −xk∥2 + 1
2(βk −βk+1)L2
g
or that
Fk+1(xk+1)+ω(xk+1)−F(x)−ω(x) ≤(1−αk)
 Fk(xk)−F(x)−ω(x)

+ω(xk+1)
−αkω(yk) + αp+1
k
Lf
p + 1 ∥yk −xk∥p+1 + α2
k
2βk
∥A∥2∥yk −xk∥2 + 1
2(βk −βk+1)L2
g
≤(1 −αk)
 Fk(xk) + ω(xk) −F(x) −ω(x)

+ αp+1
k
Lf
p + 1 ∥yk −xk∥p+1
+ α2
k
2βk
∥A∥2∥yk −xk∥2 + 1
2(βk −βk+1)L2
g ,
where the last step uses the convexity of ω and (III). Since xk, yk ∈dom ω, for
every k ∈N, it follows that ∥yk∥, ∥xk∥≤ρ and hence that (3.24) holds.
Corollary 3.3. Suppose that, α1 = 1, αk ∈[0, 1] and βk ≥βk+1 > 0, for
every k ∈N. Let Pj = Qk
i=j+1(1 −αi), for every j ∈{1, . . . , k}. Then
f(xk+1)+g(Axk+1)+ω(xk+1)−f(x)−g(Ax)−ω(x) ≤(2ρ)p+1Lf
p + 1
k
X
j=1
Pjαp+1
j
+ 2∥A∥2ρ2
k
X
j=1
Pj
α2
j
βj
+ 1
2L2
g
k
X
j=1
Pj(βj −βj+1) + 1
2βk+1L2
g
for every x ∈dom ω, k ∈N.
Proof. Let Dk = f(xk) + gβk(Axk) + ω(xk) −f(x) −g(Ax) −ω(x). Applying
Theorem 3.7, we obtain
Dj+1 ≤(1 −αj)Dj + (2ρ)p+1Lf
p + 1
αp+1
j
+ 2∥A∥2ρ2 α2
j
βj
+ 1
2(βj −βj+1)L2
g (3.29)

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 67
for every j ∈{1, . . . , k}. Multiplying by Pj and adding up, we obtain
Dk+1 ≤(1 −α1)P1D1 + (2ρ)p+1Lf
p + 1
k
X
j=1
Pjαp+1
j
+ 2∥A∥2ρ2
k
X
j=1
Pj
α2
j
βj
+ 1
2L2
g
k
X
j=1
Pj(βj −βj+1)
= (2ρ)p+1Lf
p + 1
k
X
j=1
Pjαp+1
j
+ 2∥A∥2ρ2
k
X
j=1
Pj
α2
j
βj
+ 1
2L2
g
k
X
j=1
Pj(βj −βj+1)
Applying Lemma 3.3 to gβk+1(Axk+1) the assertion follows.
Corollary 3.4. If αk =
2
k+1, β > 0 and βk =
β
√
k, for every k ∈N, then
f(xk+1) + g(Axk+1) + ω(xk+1) −f(x) −g(Ax) −ω(x) ≤
(4ρ)p+1Lf
(p + 1)(k + 1)p + 8ρ2∥A∥2
β
√
k + 1 + 1
2L2
gβ
√
k + 2
k
+
L2
gβ
2
√
k + 1
for every x ∈dom ω, k ∈N.
Proof. It follows easily from Corollary 3.3 and the computation Pj = j(j+1)
k(k+1).
We notice that when p ≥1
2 the asymptotic rate does not depend on p and
translates to O
  1
ε2

iterations, if ε is the precision in terms of the objective
function. This rate of convergence is an order of magnitude slower than the
rate for the standard conditional gradient algorithm (Corollary 3.2). Thus,
the extended ﬂexibility of handling multiple additional penalties (function g)
and the Moreau smoothing incur a cost in terms of iterations. In other words,
the class of optimization problems to which the hybrid algorithm applies is
signiﬁcantly larger than that of the standard algorithm 1 and a deterioration
in the rate of convergence is inevitable. When 0 < p <
1
2, the bound is
dominated by the term involving p and the number of iterations required
grows as O

ε−1
p

.
If there is no g ◦A term (A = 0, g = 0) then the algorithm becomes the
standard conditional gradient and the corollary reduces to known bounds for
standard conditional gradient methods. The number of iterations grows as
O

ε−1
p

, which ranges from O
  1
ε

(for p = 1) to impractical when f is too
“close” to a Lipschitz continuous function (p ≃0).
The rate in Corollary 3.4 is also slower than the O
  1
ε

rates obtained with
smoothing methods, such as [44, 42, 25, 7]. However, smoothing methods
require a more powerful computational oracle (the proximity operator of ω
instead of the dual subgradient) and hence may be inapplicable in problems

68
Regularization, Optimization, Kernels, and Support Vector Machines
like those involving very large matrices, because computation of proxω may
not scale well. Another O
  1
ε2

alternative is subgradient methods, but these
may be inapplicable too for similar reasons. For example, the subgradient of
the trace norm as either a penalty term or a constraint requires a full singular
value decomposition.
In addition, like other conditional gradient methods or greedy methods
and matching pursuits, the HCGS algorithm 2 builds a parsimonious solution
in additive fashion rather than starting from a complex solution and then
simplifying it. This feature may be desirable in itself whenever a parsimonious
solution is sought. For example, in many cases it is more important to obtain
a sparse or low rank estimate of the solution rather than a more accurate one
with many small nonzero components or singular values. In machine learning
problems, especially, this is frequently the case since the optimization objective
is just an approximation of the ideal measure of expected risk [51]. Another
advantage of such algorithmic schemes is computational. In sparse estimation
problems regularized with an ℓ1 constraint, the data matrix or the dictionary
may be huge and hence computation of ∇f(x) may be feasible only for sparse
vectors x (when f is a quadratic function). Moreover, such a computation
can be done eﬃciently since the gradient from the previous iteration can be
reused, due to update (III).
3.4.3
Minimization of Lipschitz Continuous Functions
A special case of particular interest occurs when f = 0, that is, when there
is no smooth part. Then HCGS solves the optimization problem
min {g(Ax) + ω(x) : x ∈H}
(3.30)
under Assumption 3.2 as before. Namely, the objective function consists of
a Lipschitz term g and a generic term ω deﬁned on a bounded domain. For
example, such a problem is the minimization of a Lipschitz continuous function
over a bounded domain. More generally, g ◦A may incorporate a sum of
multiple Lipschitz continuous penalties.
The HCGS algorithm speciﬁed to problem (3.30) is the same as Algorithm
2 with ∇f(xk) removed. In this way, the computational model of conditional
gradient methods extends from minimization of smooth functions to mini-
mization of Lipschitz continuous functions. Moreover, the convergence rate
deteriorates from O
  1
ε

for smooth functions to O
  1
ε2

for Lipschitz func-
tions, which is not surprising, since the latter are in general more diﬃcult
to optimize than the former. This fact has been shown recently by Lan for
several conditional gradient algorithms [34]. Lan has also shown that these
rates coincide with the lower complexity bounds for a family of algorithms
involving ∂ω∗oracles. The above fact is also intriguing in view of the analogy
to the results known about Nesterov’s proximal methods [40]. Those methods,

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 69
under a more powerful computational oracle for ω, exhibit an O

1
√ε

rate
when g is smooth versus an O
  1
ε

rate when g is Lipschitz continuous.
3.4.4
Implementation Details
It is worth noting that the HCGS algorithm does not require knowledge
of the Lipschitz constants Lf, Lg and can be implemented with an arbitrary
choice of β. An alternative is to optimize the bound in Corollary 3.4 with
respect to β, which gives an optimal choice of
2
√
2ρ∥A∥
Lg
, asymptotically. If
the desired accuracy ε can be speciﬁed in advance, then the optimal β will
also depend on ε. Computing such a β value is possible only if the Lipschitz
constant and bound of the optimization problem are available, but for regu-
larization problems these constants can be computed from the regularization
parameters.
For p ≥1
2, these two constants, ρ and Lg, have the largest inﬂuence in
the convergence rate, since they appear in the O

1
√
k

terms that dominate
the bound. The constant ρ cannot be changed, since it is a property of the
feasibility domain. However, Lg can be reduced by rescaling the objective
function and hence it can become independent of the dimensionality of the
problem.
Some care may be needed to tackle numerical issues arising from very small
values of βk as k becomes large. These issues aﬀect only step (I), whereas the
computation of yk in step (II) always remains inside the ρ-ball, since ω∗is
ρ-Lipschitz continuous. Moreover, for large k, the past estimates dominate
the update (III) and hence the eﬀect of any numerical issues diminishes as k
grows.
3.5
Applications
We now instantiate the HCGS Algorithm 2 to some special cases that
appear in applications and we present the corresponding algorithms. These
examples are only a sample and do not cover the whole range of possible
applications. First, consider the problem of learning a sparse and low rank
matrix by regularization with the ℓ1 norm and a trace norm constraint [46],
min{f(X) + λ∥X∥1 : ∥X∥tr ≤B, X ∈Rn×n} ,
(3.31)
where ∥· ∥1 denotes the elementwise ℓ1 norm of a matrix and ∥· ∥tr the
trace norm (or nuclear norm). The strongly smooth function f expresses an
error term (where the dependence on the data is absorbed in f) and may
arise by using, for example, the square loss or the logistic loss. This setting

70
Regularization, Optimization, Kernels, and Support Vector Machines
Algorithm 3 Hybrid algorithm for sparse-low rank problems.
Input X1 ∈Rn×n such that ∥X1∥tr ≤B
for k = 1, 2, . . . do
Zk ←−∇f(Xk) −
1
βk Xk +
1
βk S(Xk; βkλ)
(uk, vk) ←a left and right pair of singular vectors of Zk corresponding
to the largest singular value
Yk ←B ukv⊤
k
Xk+1 ←(1 −αk)Xk + αkYk
end for
has been proposed for applications such as graph denoising or prediction of
links on a social network. The resulting algorithm (Algorithm 3) depends on
the proximity operator of the ℓ1 norm, also known as the soft thresholding
operator,
S(X; γ) = sgn(X) ⊙(|X| −γ)+ ,
(3.32)
where sgn, ⊙, | · | denote elementwise sign, multiplication and absolute value
on matrices and (·)+ the positive part elementwise.
Note that the same algorithm can be used for solving a variation of (3.31)
that restricts the optimization to the space of symmetric matrices. This may
occur, for example, when learning the adjacency matrix of an undirected
graph. One should ensure, however, that the initial matrix X1 is symmet-
ric.
A problem that shares some similarities with the previous one is the convex
relaxation of sparse PCA proposed in [2],
max{⟨C, X⟩−λ∥X∥1 : tr(X) = 1, X ⪰0, X ∈Rn×n} .
(3.33)
Solving this optimization problem can be used for ﬁnding a dominant sparse
eigenvector of C, which is a prescribed n × n symmetric matrix. The problem
falls under the framework (3.18) with f being a linear function and ω the indi-
cator function of the (bounded) spectrahedron {X ∈Rn×n : tr(X) = 1, X ⪰
0}. Computation of a dual subgradient amounts to computing a solution of
the problem
max{⟨Y, Z⟩: tr(Y ) = 1, Y ⪰0, Y ∈Rn×n}
(3.34)
for a given symmetric matrix Z ∈Rn×n. It is easy to see that this computation
requires a dominant eigenvector of Z. This results in Algorithm 4.
A related problem is to restrict the sparse-low rank optimization (3.31) to
the cone of positive semideﬁnite matrices. This problem has been proposed
for estimating a covariance matrix in [46]. Since the trace norm is equal to the
trace on the positive semideﬁnite cone, the algorithm is similar to Algorithm
4. The only diﬀerences are the initialization, a general smooth function f, and
a factor of B in the update of Yk.

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 71
Algorithm 4 Hybrid algorithm for sparse PCA relaxation.
Input X1 ∈Rn×n such that tr(X1) = 1, X1 ⪰0
for k = 1, 2, . . . do
Zk ←C −
1
βk Xk +
1
βk S(Xk; βkλ)
uk ←a dominant eigenvector of Zk
Yk ←uku⊤
k
Xk+1 ←(1 −αk)Xk + αkYk
end for
A third example of an optimization problem that falls under our framework
is a regularization problem with ℓ1 and additional penalties,
min
1
2⟨x, Qx⟩+ ⟨c, x⟩+ g(Ax) : ∥x∥1 ≤B, x ∈Rd

.
(3.35)
Here Q ∈Rd×d is a prescribed positive semideﬁnite matrix, c ∈Rd a pre-
scribed vector, and g, A satisfy Assumption 3.2. For example, (3.35) could
arise from an estimation or learning problem, the quadratic part correspond-
ing to the data ﬁt term. The ℓ1 constraint is used to favor sparse solutions.
The penalty terms g◦A may involve multiple norms whose proximity operator
is simple to compute, such as the group Lasso norm [57], total variation norms
[50], etc.
The hybrid method, specialized to such problems, is shown in Algorithm
5. In general, several other algorithms may be used for solving problems like
(3.35) (smoothing, Douglas-Rachford, subgradient methods, etc.), but here
we are interested in cases with very large dimensionality d. In such cases,
computation of the gradient at an arbitrary vector is O(d2) and very costly. On
the other side, in the HCGS algorithm, xk+1 is (k + 1)-sparse and computing
the new gradient Qxk+1 can be done eﬃciently by keeping Qxk in memory
and computing Qyk, which is proportional to the j-th column of Q. The latter
requires only O(d) operations, or O(dm) if Q is the square of an m × d data
matrix. Thus, HCGS can be applied to such problems at a smaller cost, by
starting with an initial cardinality-one vector and stopping before k becomes
too large.
There is also an interesting interpretation of Algorithm 5 as an extension of
matching pursuits [36, 55] to problems with multiple penalties. Assuming that
Q is the square of a matrix of dictionary elements (or more generally a Gram
matrix of elements from a Hilbert space), then the algorithm shares similari-
ties with orthogonal matching pursuit (OMP). Indeed, such a connection has
already been observed for the standard conjugate gradient (which corresponds
to absence of the g ◦A term) [28, 27], the main diﬀerence from OMP being in
the update of xk+1. Similarly, the HCGS algorithm 5 could be phrased as an
extension of OMP that imposes additional penalties g ◦A, besides sparsity,
on the coeﬃcients of the atoms. For example, g ◦A could involve structured

72
Regularization, Optimization, Kernels, and Support Vector Machines
Algorithm 5 Hybrid algorithm for sparse multicomposite problems (3.35).
Input x1 = Bei for some i ∈{1, . . . , d}
for k = 1, 2, . . . do
zk ←−Qxk −c −
1
βk A∗Axk +
1
βk A∗proxβkg(Axk)
yk ←B sgn((zk)j)ej, where j ∈argmaxd
i=1 |(zk)j|
xk+1 ←(1 −αk)xk + αkyk
end for
sparsity penalties (such as penalties for group, hierarchical or graph sparsity)
and then HCGS would yield a scalable alternative to structured variants of
OMP [26] or proximal methods for structured sparsity [35].
3.6
Simulations
3.6.1
Simultaneous Sparse and Low Rank Regularization
In this section we focus on testing Algorithm 3 (HCGS) on the estimation
of simultaneously sparse and low rank matrices4. Our aim is to compare the
procedure with the proximal algorithms proposed, for the same task, in [46].
The experiments illustrate the fact that HCGS scales better than the SVD-
based alternatives.
We considered the task of recovering a matrix from a subset of its entries.
To this end in each simulation we generated two N × 5 random matrices with
entries drawn from the uniform distribution. 90% of the entries corresponding
to a subset of uniformly distributed indices were then set to zero. The resulting
matrices, denoted by U and V , were then used to obtain a sparse and low
rank matrix UV ⊤. This matrix was corrupted by zero-mean Gaussian noise
with variance σ2 = 10−4 to obtain the observation matrix Y . A fraction
f ∈{0.05, 0.4} of entries of Y were used for recovery; see Figure 3.1 for an
illustration.
We compared HCGS with the two algorithms proposed in [46], namely
generalized forward-backward (GFB) [45] and incremental proximal descent
(IPD) [8]. Both these algorithms solve a convex matrix recovery problem that
aims at ﬁnding a matrix that is simultaneously low rank and sparse. This
problem is:
min

J(X) := 1
2p∥Ω(Y −X)∥2
F + λ1∥X∥1 + λ2∥X∥tr : X ∈RN×N

(3.36)
4Code is available at http://cvn.ecp.fr/personnel/andreas/code/index.html

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 73
(a)
(b)
(c)
0
5
10
15
20
0
1
2
3
4
5
6
7
8
9
 
 
estimated
generating
observation
(d)
FIGURE 3.1: An illustration of the synthetic problem for N = 200. The
generating matrix, simultaneously sparse and low-rank (a), a mask with the
observed entries, in white; 40% of the total number of entries are observed
(b), the matrix estimated by HCGS (c), the leading singular values for the
generating/observation/estimated matrix (d).

74
Regularization, Optimization, Kernels, and Support Vector Machines
(a)
(b)
FIGURE 3.2: Comparison of objective values for N = 400, (a) as a function
of the iteration count, and (b) as a function of time.
FIGURE 3.3: Required time for convergence as a function of N (40% ob-
served entries), for the sparse-low rank experiment.

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 75
where ∥· ∥F is the Frobenius norm, p is the number of observed entries, and
Ω: RN×N →RN×N is the sampling operator deﬁned entry-wise by Ω(X)ij =
Xij if the entry indexed by (i, j) is observed, Ω(X)ij = 0 otherwise. In our
experiments we set λ1 =
1
N2 , λ2 = 10−3
N2
and used GFB and IPD to obtain
optimal estimates ˆXGFB and ˆXIPD, respectively. We then set τ := ∥ˆXGFB∥tr
and used HCGS to solve the constrained formulation equivalent to (3.36),
namely:
min
 1
2p∥Ω(Y −X)∥2
F + λ1∥X∥1 : ∥X∥tr ≤τ, X ∈RN×N

.
(3.37)
The comparisons were performed on an Intel Xeon with 8 cores and 24GB of
memory.
Figure 3.2 shows the evolution of objective values for N = 400. Note that
for the sake of comparison we have reported the objective of the optimization
problem (3.36) even though HCGS actually solves the equivalent problem in
(3.37). The same applies to the attained objective function value Jk∗in Table
3.1. In this table we have also compared the diﬀerent algorithms in terms of
CPU time5, relative change in the objective function, and number of iterations
upon termination. In all the cases we terminated the algorithms at iteration
k∗when the relative change rk∗in the objective value:
rk∗=

fk∗−fk∗−1
fk∗−1

(3.38)
was less that 10−7. Note that f in (3.38) refers to the objective function
actually minimized by each algorithm; this is not necessarily the objective
function J in (3.36). Figure 3.3 shows the time complexity as a function of
N. Finally, in Table 3.2 we have reported the average time per iteration as a
function of N.
From these ﬁgures and tables, we see that the running time of HCGS scales
as O(N 2) with the matrix size, whereas both GFB and IPD scale as O(N 3).
3.6.2
Sparse PCA
The second set of simulations assesses the computational eﬃciency of
HCGS on the convex relaxation of sparse PCA (3.33). Similar to [2], we gen-
erated random matrices C as follows. For each size n, we drew an n×n matrix
U with uniformly distributed elements in [0, 1]. Then we generated a vector
v ∈Rn from the uniform distribution and set a random 90% of its components
to zero. We then set
C = UU
⊤+ 10vv
⊤.
5In Figure 3.2b the CPU times also include the evaluation of the objective value in (3.36),
which requires computing the singular values of the estimate. In the case of HCGS, this is
required only for the sake of comparison with GFB and IPD. In contrast, for Table 3.1 the
objective value of (3.36) is computed only upon termination.

76
Regularization, Optimization, Kernels, and Support Vector Machines
TABLE 3.1: Comparison of diﬀerent algorithms for convex matrix recovery.
5% observed entries
N
Jk∗(×10−4)
rk∗(×10−8)
time (s)
k∗
50
HCGS
6.77
8.35
2.38
1605
GFB
6.87
5.15
0.09
45
IPD
6.76
9.07
0.09
50
100
HCGS
4.41
7.52
3.60
1462
GFB
4.45
9.67
2.57
385
IPD
4.40
9.80
2.5
388
200
HCGS
4.16
2.42
66
3308
GFB
4.17
9.95
113
4136
IPD
4.16
9.83
123
4645
400
HCGS
2.98
3.35
176
4555
GFB
2.98
9.99
2333
15241
IPD
2.98
9.99
2389
15665
600
HCGS
2.46
7.15
158
2797
GFB
2.45
9.99
13157
36049
IPD
2.45
9.99
13080
36408
800
HCGS
2.20
5.47
478
5197
GFB
2.20
9.99
40779
61299
IPD
2.20
9.99
41263
61529
40% observed entries
N
Jk∗(×10−4)
rk∗(×10−8)
time (s)
k∗
50
HCGS
18.51
8.85
0.62
427
GFB
18.66
8.75
1.82
751
IPD
18.52
9.96
2.17
1004
100
HCGS
12.09
4.96
2.25
835
GFB
12.15
9.84
12.1
1681
IPD
12.10
6.50
11.8
1697
200
HCGS
7.65
9.64
21
1410
GFB
7.66
9.98
134
3521
IPD
7.65
7.66
112
3033
400
HCGS
4.39
2.54
40
1281
GFB
4.39
9.96
1559
7379
IPD
4.39
9.46
1580
7515
600
HCGS
3.41
7.65
43
760
GFB
3.41
9.98
5664
10793
IPD
3.41
9.99
5446
10841
800
HCGS
2.68
1.37
148
1378
GFB
2.68
9.98
14897
15615
IPD
2.68
9.99
11242
15647
1600
HCGS
1.62
9.91
929
2931
GFB
1.62
9.99
119724
36573
IPD
1.62
9.99
118223
36583

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 77
TABLE 3.2: Average time (in seconds) per iteration as a function of N (40%
observed entries), for the sparse-low rank experiment.
N
200
400
600
800
1600
HCGS
0.0155
0.032
0.058
0.107
0.317
GFB
0.038
0.211
0.524
0.954
3.273
IPD
0.037
0.210
0.502
0.718
3.231
FIGURE 3.4: Computational time (in seconds) versus matrix size for the
sparse PCA experiment.
We solved (3.33) for λ = 1 with HCGS (Algorithm 4) and the Nesterov
smoothing method of [2], which optimizes the dual problem of (3.33). We
implemented both algorithms in Matlab and used a cluster with 24 cores and
suﬃcient memory. For HCGS we used the power method with a tolerance
of 10−6, for computing dominant eigenvectors. We also rescaled the objec-
tive function by n in order to keep Lg small enough (see Section 3.4.4). For
Nesterov smoothing we used µ =
10−6
2 log n.
In Figure 3.4, we plot the computational times required to attain relative
change of 10−5 in the objective. We note that the objective functions are
diﬀerent, since HCGS optimizes (3.33) whereas the method of [2] optimizes
the dual problem. In fact, we have veriﬁed that the duality gap estimates are
consistently larger for the latter and hence the running times for Nesterov
smoothing are optimistic. We observe that the running time scales roughly as
O(n2) for HCGS whereas Nesterov smoothing scales worse than O(n3).

78
Regularization, Optimization, Kernels, and Support Vector Machines
3.7
Conclusion
We have studied the hybrid conditional gradient-smoothing algorithm
(HCGS) for solving composite convex optimization problems that contain
several terms over a bounded set. Examples of these include regularization
problems with several norms as penalties and a norm constraint. HCGS ex-
tends conditional gradient methods to cases with multiple nonsmooth terms,
in which standard conditional gradient methods may be diﬃcult to apply.
The HCGS algorithm borrows techniques from smoothing proximal meth-
ods and requires ﬁrst-order computations (subgradients and proximity opera-
tions). Moreover, it exhibits convergence in terms of the objective values at an
O
  1
ε2

rate of iterations. Unlike proximal methods, HCGS beneﬁts from the
advantages of conditional gradient methods, which render it more eﬃcient on
certain large scale optimization problems. We have demonstrated these advan-
tages with simulations on two matrix optimization problems: regularization of
matrices with combined ℓ1 and trace norm penalties; and a convex relaxation
of sparse PCA.
Acknowledgments
The research leading to the above results has received funding from the Eu-
ropean Union Seventh Framework Programme (FP7 2007-2013) under grant
agreement No. 246556. Research was also supported by: Research Council
KUL: GOA/10/09 MaNet, PFV/10/002 (OPTEC), several PhD/postdoc and
fellow grants; Flemish Government:IOF: IOF/KP/SCORES4CHEM, FWO:
PhD/postdoc grants, projects: G.0377.12 (Structured systems), G.083014N
(Block term decompositions), G.088114N (Tensor based data similarity),
IWT: PhD Grants, projects: SBO POM, EUROSTARS SMART, iMinds 2013,
Belgian Federal Science Policy Oﬃce: IUAP P7/19 (DYSCO, Dynamical sys-
tems, control and optimization, 2012-2017), EU: FP7-SADCO (MC ITN-
264735), ERC AdG A-DATADRIVE-B (290923), COST: Action ICO806: In-
telliCIS.
Bibliography
[1] J. Abernethy, F. Bach, T. Evgeniou, and J-P. Vert. A new approach to
collaborative ﬁltering: Operator estimation with spectral regularization.
Journal of Machine Learning Research, 10:803–826, 2009.

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 79
[2] A. d’ Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet.
A direct formulation for sparse PCA using semideﬁnite programming.
SIAM Review, pages 434–448, 2007.
[3] F. Bach. Duality between subgradient and conditional gradient methods.
arXiv preprint arXiv:1211.6302, 2012.
[4] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone
Operator Theory in Hilbert Spaces. CMS Books in Mathematics. Springer,
2011.
[5] A. Beck and M. Teboulle. Mirror descent and nonlinear projected sub-
gradient methods for convex optimization. Operations Research Letters,
31(3):167–175, 2003.
[6] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algo-
rithm for linear inverse problems. SIAM Journal of Imaging Sciences,
2(1):183–202, 2009.
[7] A. Beck and M. Teboulle.
Smoothing and ﬁrst order methods: a
uniﬁed framework.
SIAM Journal on Optimization, 22(2):557–580,
2012.
[8] D. P. Bertsekas. Incremental gradient, subgradient, and proximal meth-
ods for convex optimization: A survey. In S. Sra, S. Nowozin, and S. J.
Wright, editors, Optimization for Machine Learning, pages 85–119. MIT
Press, 2011.
[9] M. D. Canon and C. D. Cullum. A tight upper bound on the rate of con-
vergence of Frank-Wolfe algorithm. SIAM Journal on Control, 6(4):509–
516, 1968.
[10] K. L. Clarkson. Coresets, sparse greedy approximation, and the Frank-
Wolfe algorithm.
ACM Transactions on Algorithms, 6(4):63:1–63:30,
2010.
[11] P. L. Combettes and J. C. Pesquet. Proximal splitting methods in signal
processing. In Fixed-Point Algorithms for Inverse Problems in Science
and Engineering, pages 185–212. Springer, 2011.
[12] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-
backward splitting. Multiscale Modeling and Simulation, 4(4):1168–1200,
2006.
[13] V. F. Dem’yanov and A. B. Pevnyi. Some estimates in minimax problems.
Cybernetics and Systems Analysis, 8(1):116–123, 1972.
[14] V. F. Dem’yanov and A. M. Rubinov. Approximate methods in optimiza-
tion problems, volume 32. Elsevier, 1970.

80
Regularization, Optimization, Kernels, and Support Vector Machines
[15] J. C. Dunn. Rates of convergence for conditional gradient algorithms
near singular and nonsingular extremals. SIAM Journal on Control and
Optimization, 17(2):187–211, 1979.
[16] J. C. Dunn. Convergence rates for conditional gradient sequences gener-
ated by implicit step length rules. SIAM Journal on Control and Opti-
mization, 18(5):473–487, 1980.
[17] J. Eckstein and D. P. Bertsekas.
On the Douglas-Rachford splitting
method and the proximal point algorithm for maximal monotone op-
erators. Mathematical Programming, 55(1-3):293–318, 1992.
[18] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval
research logistics quarterly, 3(1-2):95–110, 1956.
[19] R. M. Freund and P. Grigas. New analysis and results for the conditional
gradient method. Technical report, Massachusetts Institute of Technol-
ogy, Operations Research Center, 2013.
[20] D. Garber and E. Hazan. Approximating semideﬁnite programs in sub-
linear time. In Advances in Neural Information Processing Systems 24,
pages 1080–1088, 2011.
[21] E. G. Gilbert. An iterative procedure for computing the minimum of a
quadratic form on a convex set. SIAM Journal on Control, 4(1):61–80,
1966.
[22] Z. Harchaoui, A. Juditsky, and A. Nemirovski. Conditional gradient al-
gorithms for machine learning. In NIPS Workshop on Optimization for
Machine Learning, 2012.
[23] E. Hazan. Sparse approximate solutions to semideﬁnite programs. In
Proceedings of the 8th Latin American conference on Theoretical infor-
matics, LATIN’08, pages 306–316, 2008.
[24] E. Hazan and S. Kale. Projection-free online learning. In Proceedings of
the 29th International Conference on Machine Learning, 2012.
[25] N. He, A. Juditsky, and A. Nemirovski. Mirror prox algorithm for multi-
term composite minimization and alternating directions. arXiv preprint
arXiv:1311.1098, 2013.
[26] J. Huang, T. Zhang, and D. Metaxas. Learning with structured sparsity.
In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 417–424. ACM, 2009.
[27] M. Jaggi. Convex optimization without projection steps. Arxiv preprint
arXiv:1108.1170, 2011.

Hybrid Algorithms with Applications to Sparse and Low Rank Regularization 81
[28] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimiza-
tion. In Proceedings of the 30th International Conference on Machine
Learning, pages 427–435, 2013.
[29] M. Jaggi and M. Sulovsk`y. A simple algorithm for nuclear norm regu-
larized problems. In Proceedings of the 27th International Conference on
Machine Learning, pages 471–478, 2010.
[30] A. Juditsky and A. Nemirovski. First order methods for nonsmooth con-
vex large-scale optimization, I: general purpose methods. In Optimization
for Machine Learning, pages 121–148. MIT Press, 2010.
[31] H. J. Kelley. Method of gradients. Academic Press, 2:1578–1580, 1962.
[32] V. Kumar. A control averaging technique for solving a class of singular
optimal control problems. International Journal of Control, 23(3):361–
380, 1976.
[33] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher.
Block-
coordinate Frank-Wolfe optimization for structural SVMs. arXiv preprint
arXiv:1207.4747, 2012.
[34] G. Lan. The complexity of large-scale convex programming under a linear
optimization oracle. arXiv preprint arXiv:1309.5550, 2013.
[35] J. Mairal, R. Jenatton, F. R. Bach, and G. R. Obozinski. Network ﬂow
algorithms for structured sparsity. In Advances in Neural Information
Processing Systems, pages 1558–1566, 2010.
[36] S. G. Mallat and Z. Zhang. Matching pursuits with time-frequency dic-
tionaries.
IEEE Transactions on Signal Processing, 41(12):3397–3415,
1993.
[37] J.J. Moreau. Proximité et dualité dans un espace hilbertien. Bulletin de
la Société Mathématique de France, 93(2):273–299, 1965.
[38] A. S. Nemirovsky and D. B. Yudin.
Problem complexity and method
eﬃciency in optimization. Wiley, 1983.
[39] Y. Nesterov. A method of solving a convex programming problem with
convergence rate O(1/k2). Soviet Mathematics Doklady, 27(2):372–376,
1983.
[40] Y. Nesterov. Introductory lectures on convex optimization: A basic course.
Springer, 2004.
[41] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization.
SIAM J. on Optimization, 16(1):235–249, May 2005.
[42] Y. Nesterov. Smooth minimization of non-smooth functions. Mathemat-
ical Programming, 103(1):127–152, 2005.

82
Regularization, Optimization, Kernels, and Support Vector Machines
[43] Y. Nesterov. Gradient methods for minimizing composite objective func-
tion. CORE Discussion Papers, 2007.
[44] F. Orabona, A. Argyriou, and N. Srebro. PRISMA: PRoximal Iterative
SMoothing Algorithm. arXiv preprint arXiv:1206.2372, 2012.
[45] H. Raguet, J. Fadili, and G. Peyré.
A generalized forward-backward
splitting. SIAM Journal on Imaging Sciences, 6(3):1199–1226, 2013.
[46] E. Richard, P.-A. Savalle, and N. Vayatis. Estimation of simultaneously
sparse and low rank matrices. In Proceedings of the 29th International
Conference on Machine Learning, pages 1351–1358, 2012.
[47] R. T. Rockafellar. Level sets and continuity of conjugate convex functions.
Transactions of the American Mathematical Society, 123:46–63, 1966.
[48] R. T. Rockafellar. Convex Analysis. Princeton University Press, 1970.
[49] R. T. Rockafellar. Monotone operators and the proximal point algorithm.
SIAM Journal on Control and Optimization, 14(5):877–898, 1976.
[50] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based
noise removal algorithms. Physica D: Nonlinear Phenomena, 60(1):259–
268, 1992.
[51] S. Shalev-Shwartz, N. Srebro, and T. Zhang. Trading accuracy for spar-
sity in optimization problems with sparsity constraints. SIAM Journal
on Optimization, 20(6):2807–2832, 2010.
[52] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis.
Cambridge University Press, 2004.
[53] J. Steinhardt and J. Huggins. A greedy framework for ﬁrst-order opti-
mization. Preprint.
[54] A. Tewari, P. K. Ravikumar, and I. S. Dhillon. Greedy algorithms for
structurally constrained high dimensional problems. In Advances in Neu-
ral Information Processing Systems 24, pages 882–890, 2011.
[55] J. A. Tropp and A. C. Gilbert. Signal recovery from random measure-
ments via orthogonal matching pursuit. IEEE Transactions on Informa-
tion Theory, 53(12):4655–4666, 2007.
[56] Y. Ying and P. Li. Distance metric learning with eigenvalue optimization.
Journal of Machine Learning Research, 13:1–26, 2012.
[57] M. Yuan and Y. Lin. Model selection and estimation in regression with
grouped variables. Journal of the Royal Statistical Society, Series B (Sta-
tistical Methodology), 68(1):49–67, 2006.

Chapter 4
Nonconvex Proximal Splitting with
Computational Errors
Suvrit Sra
Max Planck Institute for Intelligent Systems and Carnegie Mellon University
4.1
Introduction ......................................................
83
4.2
The Nips Framework ............................................
85
4.2.1
Convergence Analysis ....................................
87
4.3
Scaling Up: Incremental Proximal Splitting .....................
91
4.3.1
Convergence .............................................
92
4.4
Application to Matrix Factorization .............................
95
4.5
Other Applications ...............................................
98
4.6
Discussion ........................................................
98
Bibliography ......................................................
99
4.1
Introduction
In this chapter we study large-scale nonconvex optimization problems with
composite objective functions that are composed of a diﬀerentiable possibly
nonconvex cost and a nonsmooth but convex regularizer. More precisely, we
consider optimization problems of the form
minimize
Φ(x) := f(x) + r(x),
s.t.
x ∈X,
(4.1)
where X ⊂Rn is a compact convex set, f : Rn →R is a diﬀerentiable cost
function, and r : Rn →R is a closed convex function. Further, we assume that
the gradient ∇f is Lipschitz continuous on X (denoted f ∈C1
L(X)), i.e.,
∃L > 0
s.t.
∥∇f(x) −∇f(y)∥≤L ∥x −y∥
for all
x, y ∈X.
(4.2)
Throughout this chapter, ∥·∥denotes the standard Euclidean norm.
Problem (4.1) generalizes the more thoroughly studied class of composite
convex optimization problems [30], a class that has witnessed huge interest
in machine learning, signal processing, statistics, and other related areas. We
refer the interested reader to [21, 2, 3, 37] for several convex examples and
83

84
Regularization, Optimization, Kernels, and Support Vector Machines
recent references. A thread common to existing algorithms for solving com-
posite problems is the remarkably fruitful idea of proximal-splitting [9]. Here,
nonsmoothness is handled via proximity operators [29], which allows one to
treat the nonsmooth objective f + r essentially as a smooth one.
But leveraging proximal-splitting methods is considerably harder for non-
convex problems, especially without compromising scalability. Numerous im-
portant problems have appealing nonconvex formulations: matrix factoriza-
tion [25, 27], blind deconvolution [24], dictionary learning and sparse recon-
struction [23, 27], and neural networks [4, 28, 19], to name a few. Regularized
optimization within these problems requires handling nonconvex composite
objectives, which motivates the material of this chapter.
The focus of this chapter is on a new proximal splitting framework called
Nonconvex Inexact Proximal Splitting, hereafter Nips. The Nips framework
is inexact because it allows for computational errors, a feature that helps it
scale to large-data problems. In contrast to typical incremental methods [5]
and to most stochastic gradient methods [16, 18] that assume vanishing errors,
Nips allows the computational errors to be nonvanishing.
Nips
inherits
this
capability
from
the
remarkable
framework
of
Solodov [33]. But Nips not only builds on [33], it strictly generalizes it: Unlike
[33], Nips allows r ̸= 0 in (4.1). To our knowledge, Nips is the ﬁrst nonconvex
proximal splitting method that has both batch and incremental incarnations;
this claim remains true, even if we were to exclude the nonvanishing error
capability.1 We mention some more related work below.
Among batch nonconvex splitting methods an early paper is [14]. Another
batch method can be found in the pioneering paper on composite minimiza-
tion by Nesterov [30], who solves (4.1) via a splitting-like algorithm. Both [14]
and [30] rely on monotonic descent (using line-search or otherwise) to en-
sure convergence. Very recently, [1] introduced a powerful class of “descent-
methods” based on Kurdyka-Łojasiewicz theory. In general, the insistence on
descent, while theoretically convenient, makes it hard to extend these methods
to incremental, stochastic, or online variants. The general proximal framework
of [40] avoids strict monotonic descent at each step by using a non-monotonic
line-search.
There are some incremental and stochastic methods that apply to (4.1),
namely the generalized gradient algorithm of [35] and the stochastic general-
ized gradient methods of [12, 13], (and the very recent work of [17, 18]). All
these approaches are analogous to subgradient methods, and thus face simi-
lar practical diﬃculties (except [18]). For example, it is well-recognized that
subgradient methods fail to exploit composite objectives [30, 11]. Moreover,
they exhibit the eﬀect of the regularizer only in the limit, which conﬂicts with
early termination heuristics frequently used in practice. If, say, the nonsmooth
part of the objective is ∥x∥1, then with subgradient-style methods sparse
1Though very recently, in [18] scalable nonconvex stochastic methods were proposed for
smooth nonconvex problems; and even more recently those ideas were extended to cover
nonsmooth and accelerated methods [17], though still in the vanishing error framework.

Nonconvex Proximal Splitting with Computational Errors
85
solutions are obtained only in the limit and intermediate iterates may be
dense. Thus, like the convex case it may be of substantial practical advantage
to use proximal splitting even for (4.1).
4.2
The Nips Framework
We rewrite (4.1) as an unconstrained problem by introducing the function
g(x) := r(x) + δ(x|X),
where δ(x|X) is the indicator function for set X. Our problem then becomes:
minimize
Φ(x) := f(x) + g(x)
x ∈Rn.
(4.3)
Since we solve (4.3) via a proximal method, we begin with the deﬁnition below.
Deﬁnition 4.1 (Proximity operator). Let g : Rn →R be lower semicontinu-
ous (lsc) and convex. The proximity operator for g, indexed by η > 0, is the
nonlinear map [31, Def. 1.22]:
proxg,η :
y 7→argmin
x∈Rn
 g(x) + 1
2η ∥x −y∥2
.
(4.4)
Using the operator (4.4), the classic forward-backward splitting (FBS) [8]
iteration (for suitable ηk and convex f) is written as
xk+1 = proxg,ηk(xk −ηk∇f(xk)),
k = 0, 1, . . . .
(4.5)
The Nips framework described in this chapter is motivated by the sim-
ple form of iteration (4.5). In particular, for this iteration Nips introduces
two powerful generalizations: (i) it permits a nonconvex f; and (ii) it allows
computational errors. More precisely, Nips performs the iteration
xk+1 = proxg,ηk(xk −ηk∇f(xk) + ηke(xk)).
(4.6)
The error vector e(xk) in (4.6) is the interesting part. It denotes potential error
made at step k in the computation of the gradient ∇f(xk). It is important to
observe that the net error is ηke(xk), so that the error is scaled by the stepsize.
This scaling is made to suggest that the limiting value of the stepsize is what
ultimately determines the eﬀective error, and thereby governs convergence.
Remark 4.1. We warn the reader against a potential pitfall of the notation
for error in (4.6). That iteration does not mean that Nips adds an error vector
e(xk) when iterating, but rather that it iterates
xk+1 = proxg,ηk(xk −ηkgk),
where gk is an erroneous computation of the gradient, which is explicitly de-
picted in (4.6) as gk = ∇f(xk) −e(xk).

86
Regularization, Optimization, Kernels, and Support Vector Machines
But notice that we do not impose the following condition
lim
k→∞
e(xk)
 →0
(4.7)
on the error vectors, which is typically imposed by stochastic-gradient meth-
ods [5]. Since we do not require the errors to vanish in the limit, to make Nips
well-deﬁned we must nevertheless somehow control them. Thus, we impose a
mild restriction on the errors: we assume that there is a ﬁxed value ¯η, so that
for all stepsizes η smaller than ¯η the gradient errors satisfy
η ∥e(x)∥≤¯ϵ,
for some ﬁxed ¯ϵ ≥0,
and ∀x ∈X.
(4.8)
Clearly, condition (4.8) is weaker than the usual requirement (4.7).
Remark 4.2. We can consider errors in the proximity operator too, i.e.,
the proxg,η computation may also be inexact (for convex optimization inexact
proximity operators have been studied for a long time; two recent references
are [39, 32]). With inexact proximity operations iteration (4.6) becomes
xk+1 = proxg,ηk(xk −ηk∇f(xk) + ηke(xk)) + ηkp(xk),
where ηkp(xk) is the error in proximity operator. The dependency on ηk high-
lights that the error should eventually shrink in a manner similar to (4.8).
This error can be easily incorporated into our analysis below, though at the
expense of heavier notation. To avoid clutter we omit details and leave them
as an exercise for the interested reader.
Since errors in the gradient computation need not disappear, we cannot
ensure exact stationary points; but we can nevertheless hope to ensure inexact
stationary points. Let us make this more precise. A point x∗∈Rn is stationary
for (4.3), if and only if it satisﬁes the inclusion
0 ∈∂CΦ(x∗) = ∇f(x∗) + ∂g(x∗),
(4.9)
where ∂CΦ(x∗) is the Clarke subdiﬀerential [7] at x∗. The optimality condi-
tion (4.9) may be recast as the ﬁxed-point equation
x∗= proxg,η(x∗−η∇f(x∗)),
for η > 0,
(4.10)
which helps characterize approximate stationarity. Deﬁne the prox-residual
ρ(x) := x −proxg,1(x −∇f(x));
(4.11)
then, for stationary x∗the residual norm ∥ρ(x∗)∥vanishes. At a point x,
let the total perturbation be given by ϵ(x) ≥0. We deﬁne a point ¯x to be
ϵ-stationary if the residual norm satisﬁes the condition
∥ρ(¯x)∥≤ϵ(¯x).
(4.12)
Since we cannot measure convergence to an accuracy better than the amount
of prevalent noise, we require ϵ(x) ≥η ∥e(x)∥. By letting η become small
enough, we may hope to come arbitrarily close to a stationary point.

Nonconvex Proximal Splitting with Computational Errors
87
4.2.1
Convergence Analysis
In this section, we outline a simple convergence analysis for the Nips it-
eration (4.6). Our analysis is structured upon the powerful framework of [33].
But our problem class of composite objectives is more general than the diﬀer-
entiable problems considered in [33] since we allow nonsmooth objective func-
tions. Our analysis leads to the ﬁrst nonconvex proximal splitting algorithm
that allows noisy gradients; also we obtain the ﬁrst nonconvex incremental
proximal splitting algorithm regardless of whether the noise vanishes or not.
For our analysis we make the following standing assumption.
Assumption. The stepsizes ηk satisfy the bounds
c ≤lim infk ηk,
lim supk ηk ≤min{1, 2/L −c},
0 < c < 1/L.
(4.13)
We start our analysis by recalling two well-known facts.
Lemma 4.1 (Descent). Let f be such that ∇f satisﬁes (4.2). Then
|f(x) −f(y) −⟨∇f(y), x −y⟩| ≤L
2 ∥x −y∥2 ,
∀x, y ∈X.
(4.14)
Proof. Since f ∈C1
L, by Taylor’s theorem for zt = y + t(x −y) we have
|f(x) −f(y) −⟨∇f(y), x −y⟩| = |
R 1
0 ⟨∇f(zt) −∇f(y), x −y⟩dt|,
≤
R 1
0 ∥∇f(zt) −∇f(y)∥· ∥x −y∥dt ≤L
R 1
0 t∥x −y∥2
2dt = L
2 ∥x −y∥2
2.
We used the triangle-inequality, Cauchy-Schwarz, and that f ∈C1
L above.
Lemma 4.2. The operator proxg,η is nonexpansive, that is,
proxg,η x −proxg,η y
 ≤∥x −y∥,
∀x, y ∈Rn.
(4.15)
Proof. For brevity we drop the subscripted η. After renaming variables, from
optimality conditions for the problem (4.4), it follows that x −proxg x ∈
η∂g(proxg x). A similar characterization holds for y. Thus, x −proxg x and
y −proxg y are subgradients of g at proxg x and proxg y, respectively. Thus,
g(proxg x) ≥g(proxg y) + ⟨y −proxg y, proxg x −proxg y⟩
g(proxg y) ≥g(proxg x) + ⟨x −proxg x, proxg y −proxg x⟩.
Adding the two inequalities we obtain ﬁrm nonexpansivity
proxg x −proxg y
2 ≤⟨proxg x −proxg y, x −y⟩,
from which via Cauchy-Schwarz, we easily obtain (4.15).
Next, we prove a crucial monotonicity property of proximity operators.

88
Regularization, Optimization, Kernels, and Support Vector Machines
Lemma 4.3. Deﬁne Pη ≡proxg,η; let y, z ∈Rn, and η > 0; deﬁne the
functions
p(η) :=
η−1 ∥Pη(y −ηz) −y∥,
(4.16)
q(η) :=
∥Pη(y −ηz) −y∥.
(4.17)
Then, p(η) is a decreasing function and q(η) is an increasing function of η.
Proof. Our proof relies on well-known results about Moreau envelopes [31, 8].
Consider thus the “deﬂected” proximal objective
mg(x, η; y, z) := ⟨z, x −y⟩+ 1
2η−1 ∥x −y∥2 + g(x),
(4.18)
to which we associate its Moreau-envelope
Eg(η) := inf
x∈X mg(x, η; y, z).
(4.19)
Since mg is strongly convex in x, and X is compact, the inﬁmum in (4.19)
is attained at a unique point, which is precisely P g
η (y −ηz). Thus, Eg(η) is a
diﬀerentiable function of η, and in particular
∂Eg(η)
∂η
= −1
2η−2 ∥Pη(y −ηz) −y∥2 = −1
2p(η)2.
Observe that mg is jointly convex in (x, η); it follows that Eg is convex too.
Thus, its derivative ∂Eg/∂η is increasing, whereby p(η) is decreasing. Similarly,
ˆEg(γ) := Eg(1/γ) is concave in γ (it is a pointwise inﬁmum of linear functions).
Thus, its derivative
∂ˆEg(γ)
∂γ
= 1
2
P1/γ(x −γ−1y) −x
2 = q(1/γ),
is a decreasing function of γ. Writing η = 1/γ completes our claim.
Remark 4.3. The monotonicity results (4.16) and (4.17) subsume the mono-
tonicity results for projection operators derived in [15, Lemma 1].
We now proceed to analyze how the objective function value changes after
one step of the Nips iteration (4.6). Speciﬁcally, we seek to derive an inequality
of the form (4.20) (where Φ is as in (4.3)):
Φ(xk) −Φ(xk+1) ≥h(xk).
(4.20)
Our strategy is to bound the potential function h(x) in terms of prox-residual
∥ρ(x)∥and the error level ϵ(x). It is important to note that the potential h(x)
may be negative, because we do not insist on monotonic descent.
To reduce clutter, let us introduce brief notation: u ≡xk+1, x ≡xk, and
η ≡ηk; therewith the main Nips update (4.6) may be rewritten as
u = proxg,η(x −η∇f(x) + ηe(x)).
(4.21)
We are now ready to state the following “descent” theorem.

Nonconvex Proximal Splitting with Computational Errors
89
Theorem 4.1. Let u, x, η be as in (4.21); assume ϵ(x) ≥η ∥e(x)∥. Then,
Φ(x) −Φ(u)
≥
2−Lη
2η
∥u −x∥2 −1
ηϵ(x) ∥u −x∥.
(4.22)
Proof. Let mg be as in (4.18); consider its directional derivative dmg with
respect to x in direction w; at x = u it satisﬁes the optimality condition
dmg(u, η; y, z)(w) = ⟨z + η−1(u −y) + s, w⟩≥0,
s ∈∂g(u).
(4.23)
In (4.23), substitute z = ∇f(x) −e(x), y = x, and w = x −u to obtain
⟨∇f(x) −e(x), u −x⟩≤⟨η−1(u −x) + s, x −u⟩.
(4.24)
From Lemma 4.1 we know that Φ(u) ≤f(x) + ⟨∇f(x), u −x⟩+ L
2 ∥u −x∥2 +
g(u); now add and subtract e(x) to this and combine with (4.24) to obtain
Φ(u) ≤f(x) + ⟨∇f(x) −e(x), u −x⟩+ L
2 ∥u −x∥2 + g(u) + ⟨e(x), u −x⟩
≤f(x) + ⟨η−1(u −x) + s, x −u⟩+ L
2 ∥u −x∥2 + g(u) + ⟨e(x), u −x⟩
= f(x) + g(u) + ⟨s, x −u⟩+
  L
2 −1
η

∥u −x∥2 + ⟨e(x), u −x⟩
≤f(x) + g(x) −2−Lη
2η
∥u −x∥2 + ⟨e(x), u −x⟩
≤Φ(x) −2−Lη
2η
∥u −x∥2 + ∥e(x)∥∥u −x∥,
≤Φ(x) −2−Lη
2η
∥u −x∥2 + 1
ηϵ(x) ∥u −x∥.
The third inequality follows from convexity of g, the fourth one from Cauchy-
Schwarz, and the last one from the deﬁnition of ϵ(x).
To further analyze (4.22), we derive two-sided bounds on ∥x −u∥below.
Lemma 4.4. Let x, u, and η be as in Theorem 4.1, and c as in (4.13). Then,
c ∥ρ(x)∥−ϵ(x) ≤∥x −u∥≤∥ρ(x)∥+ ϵ(x).
(4.25)
Proof. Lemma 4.3 implies that for η > 0 we have the crucial bounds
1 ≤η =⇒q(1) ≤q(η),
and
1 ≥η =⇒p(1) ≤p(η).
(4.26)
Let y ←x, z ←∇f(x). Note that q(1) = ∥ρ(x)∥≤∥Pη(x −η∇f(x)) −x∥if
η ≥1, while if η ≤1, we get ηp(1) ≤∥Pη(x −η∇f(x)) −x∥. Compactly, we
may therefore write
min{1, η} ∥ρ(x)∥≤∥Pη(x −η∇f(x)) −x∥.
Using the triangle inequality and nonexpansivity of prox we see that
min{1, η} ∥ρ(x)∥
≤
∥Pη(x −η∇f(x)) −x∥
≤
∥x −u∥+ ∥u −Pη(x −η∇f(x))∥
≤
∥x −u∥+ η ∥e(x)∥
≤
∥x −u∥+ ϵ(x).

90
Regularization, Optimization, Kernels, and Support Vector Machines
As c ≤lim infk ηk, for large enough k it holds that ∥x −u∥≥c ∥ρ(x)∥−ϵ(x).
An upper-bound on ∥x −u∥may be obtained as follows:
∥x −u∥≤∥x −Pη(x −η∇f(x))∥+ ∥Pη(x −η∇f(x)) −u∥
≤max{1, η} ∥ρ(x)∥+ η ∥e(x)∥
≤
∥ρ(x)∥+ ϵ(x),
where we again used Lemma 4.3 and nonexpansivity.
Theorem 4.1 and Lemma 4.4 have done the hard work; they imply the
following corollary, which is a key component of the convergence framework
of [33] that we ultimately will also invoke.
Corollary 4.1. Let x, u, η, and c be as above and k suﬃciently large so that
c and η ≡ηk satisfy (4.13). Then, Φ(x) −Φ(u) ≥h(x) holds for
h(x) :=
L2c3
2(2−2Lc) ∥ρ(x)∥2−
  L2c2
2−cL + 1
c

∥ρ(x)∥ϵ(x)−
  1
c −
L2c
2(2−cL)

ϵ(x)2. (4.27)
Proof. We prove (4.27) by showing that h(x) can be chosen as
h(x) := a1 ∥ρ(x)∥2 −a2 ∥ρ(x)∥ϵ(x) −a3ϵ(x)2,
(4.28)
where the constants a1, a2, and a3 satisfy
a1 =
L2c3
2(2−2Lc),
a2 = L2c2
2−cL + 1
c,
a3 = 1
c −
L2c
2(2−cL).
(4.29)
Note that by construction the scalars a1, a2, a3 > 0. For suﬃciently large k,
condition (4.13) implies that
c < η < 2
L −c
=⇒
1
η >
L
2−Lc,
1
η < 1
c,
and
2−Lη > Lc, (4.30)
which in turn shows that
2−Lη
2η
> (2−Lη)L
2(2−Lc) >
L2c
2(2−Lc)
and
−1
η > −1
c.
We can plug this into (4.22) to obtain
Φ(x) −Φ(u) ≥
L2c
2(2−Lc) ∥x −u∥2 −1
cϵ(x) ∥x −u∥.
Apply to this the two-sided bounds (4.25), so that we get
Φ(x) −Φ(u) ≥
L2c
2(2−Lc)
 c ∥ρ(x)∥−ϵ(x)
2 −1
cϵ(x) (∥ρ(x)∥+ ϵ(x))
=
L2c3
2(2−Lc) ∥ρ(x)∥2 −
  L2c2
2−Lc + 1
c) ∥ρ(x)∥ϵ(x) −
  1
c −
L2c
2(2−Lc)

ϵ(x)2 =: h(x).
All that remains to show is that the said coeﬃcients of h(x) are positive.
Since 2 −Lc > 0 and c > 0, positivity of a1 and a2 is immediate. Inequality
a3 = 1
c −
L2c
2(2−Lc) > 0, holds as long as 0 < c <
√
5−1
L
, which is obviously true
since c < 1/L by assumption (4.13). Thus, a1, a2, a3 > 0.

Nonconvex Proximal Splitting with Computational Errors
91
Theorem 4.2 (Convergence). Let f ∈C1
L(X) such that infX f > −∞and
g be lsc, convex on X. Let {xk} ⊂X be a sequence generated by (4.6), and
let condition (4.8) hold. Then, there exists a limit point x∗of the sequence
{xk}, and a constant K > 0, such that ∥ρ(x∗)∥≤Kϵ(x∗). Moreover, if the
sequence {f(xk)} converges, then for every limit point x∗of {xk} it holds that
∥ρ(x∗)∥≤Kϵ(x∗).
Proof. Theorem 4.1, Lemma 4.4, and Corollary 4.1 have shown that the net
change in objective from one step to the next is lower bounded by a quadratic
function with suitable positive coeﬃcients, which makes the analysis technique
of the diﬀerentiable case treated by [33, Thm. 2.1] applicable to the setting
(the exact nature of the quadratic bound derived above is crucial to the proof,
which essentially shows that for a large enough iteration count, this bound
must be positive, which ensures progress); we omit the details for brevity.
Theorem 4.2 says that we can obtain an approximate stationary point for
which the norm of the residual is bounded by a linear function of the error
level. The statement of the theorem is written in a conditional form, because
nonvanishing errors e(x) prevent us from making a stronger statement. In
particular, once the iterates enter a region where the residual norm falls below
the error threshold, the behavior of {xk} may be arbitrary. This, however, is
a small price to pay for having the added ﬂexibility of nonvanishing errors.
Under the stronger assumption of vanishing errors (and suitable stepsizes),
we can also ensure exact stationarity.
4.3
Scaling Up: Incremental Proximal Splitting
We now apply Nips to a large-scale setting. Here, the objective function
f(x) is assumed to be decomposable, that is
f(x) :=
XT
t=1 ft(x),
(4.31)
where ft : Rn →R is in C1
Lt(X) (set L ≥Lt for all t), and we solve
min
f(x) + g(x),
x ∈X,
(4.32)
where g and X are as before (4.3).
It has long been known that for decomposable objectives it can be ad-
vantageous to replace the full gradient ∇f(x) by an incremental gradient
∇fr(t)(x), where r(t) is some suitably chosen index. Nonconvex incremen-
tal methods have been extensively analyzed in the setting of backpropagation
algorithms [5, 33], which correspond to g(x) ≡0 in (4.32). For g(x) ̸= 0,

92
Regularization, Optimization, Kernels, and Support Vector Machines
the stochastic generalized gradient methods of [13] or the perturbed gener-
alized methods of [35] apply. As previously mentioned, these approaches fail
to exploit the composite structure of the objective function, which can be a
disadvantage already in the convex case [11].
In contrast, we exploit the composite structure of (4.31), and propose the
following incremental nonconvex proximal-splitting method:
xk+1 = M
 xk −ηk
XT
t=1 ∇ft(zt)

z1 = xk,
zt+1 = O(zt −η∇ft(zt)),
t = 1, . . . , T −1.
(4.33)
Here, O and M are appropriate nonexpansive maps, choosing which we get
diﬀerent algorithms. For example, when X = Rn, g(x) ≡0, and M = O =
Id, then (4.33) reduces to the problem class considered in [34]. If X is a
closed convex set, g(x) ≡0, M = ΠX , and O = Id, then (4.33) reduces to
a method that is essentially implicit in [34]. Note, however, that in this case,
the constraints are enforced only once every major iteration; the intermediate
iterates (zt) may be infeasible.
Depending on the application, we may implement either of the four variants
of (4.33) in Table 4.1. Which of these one prefers depends on the complexity
of the constraint set X and on the cost of applying P g
η . In the ﬁrst two exam-
ples X is not bounded, which complicates the convergence analysis; the third
variant is also of practical importance, but the fourth variant allows a more
instructive analysis, so we only discuss that one.
TABLE 4.1: Diﬀerent variants of incremental Nips (4.33). ‘P’ indicates penal-
ized, ‘U’ indicates ‘unconstrained’, while ‘C’ refers to a constrained problem; ‘CCvx’
signiﬁes ‘Compact convex’.
X
g
M
O
Penalty
Proximity operator calls
Rn
̸≡0
proxg
Id
P,U
once every major (k) iteration
Rn
̸≡0
proxg
proxg
P,U
once every minor (k, t) iteration
CCvx
h(x) + δ(x|X)
proxg
Id
P,C
once every major (k) iteration
CCvx
h(x) + δ(x|X)
proxg
proxg
P,C
once every minor (k, t) iteration
4.3.1
Convergence
Our analysis is inspired by [34] with the obvious diﬀerence that we are deal-
ing with a nonsmooth problem. First, as is usual with incremental methods,
we also rewrite (4.33) in a form that matches the main iteration (4.6)
xk+1 = M
 xk −ηk
XT
t=1 ∇ft(zt)

=
M
 xk −ηk∇F(xk) + ηke(xk)

.
The error term at a general x is then given by e(x) := PT
t=1
 ft(x) −ft(zt)

.
Since we wish to reduce incremental Nips to a setting where the analysis of

Nonconvex Proximal Splitting with Computational Errors
93
the batch method applies, we must ensure that the norm of the error term
is bounded. Lemma 4.7 proves such a bound; but ﬁrst we need to prove two
auxiliary results.
Lemma 4.5 (Bounded increment). Let zt+1 be computed by (4.33). Then,
if
O = Id, then
zt+1 −zt = η
∇ft(zt)

(4.34)
if
O = ΠX , then
zt+1 −zt ≤η
∇ft(zt)

(4.35)
if
O = proxη
g,
st ∈∂g(zt), then
zt+1 −zt ≤2η
∇ft(zt) + st .
(4.36)
Proof. Relation (4.34) is obvious, and (4.35) follows immediately from nonex-
pansivity of projections. To prove (4.36), notice that deﬁnition (4.4) implies
the inequality
1
2
zt+1 −zt + η∇ft(zt)
2 + ηg(zt+1) ≤1
2
η∇ft(zt)
2 + ηg(zt),
1
2
zt+1 −zt2 ≤η⟨∇ft(zt), zt −zt+1⟩+ η(g(zt) −g(zt+1)).
Since g is convex, g(zt+1) ≥g(zt) + ⟨st, zt+1 −zt⟩for st ∈∂g(zt). Moreover,
1
2
zt+1 −zt2 ≤η⟨st, zt −zt+1⟩+ η⟨∇ft(zt), zt −zt+1⟩
≤η
st + ∇ft(zt)
 zt −zt+1
=⇒
zt+1 −zt ≤2η
∇ft(zt) + st .
Lemma 4.6 (Incrementality error). Let x ≡xk, and deﬁne
ϵt :=
∇ft(zt) −∇ft(x)
 ,
t = 1, . . . , T.
(4.37)
Then, for each t ≥2, the following bound on the error holds:
ϵt ≤2ηL
Xt−1
j=1(1 + 2ηL)t−1−j ∇fj(x) + sj ,
t = 2, . . . , T.
(4.38)
Proof. The proof extends the diﬀerentiable case treated in [34]. We proceed
by induction. The base case is t = 2, for which we have
ϵ2 =
∇f2(z2) −∇f2(x)
 ≤L
z2 −x
 = L
z2 −z1
(4.36)
≤
2ηL
∇f1(x) + s1 .
Assume inductively that (4.38) holds for t ≤r < T, and consider t = r + 1.
Then,
ϵr+1
=
∇fr+1(zr+1) −∇fr+1(x)
 ≤L
zr+1 −x

=
L

Xr
j=1(zj+1 −zj)
 ≤L
Xr
j=1
zj+1 −zj
Lemma 4.5
≤
2ηL
Xr
j=1
∇fj(zj) + sj .
(4.39)

94
Regularization, Optimization, Kernels, and Support Vector Machines
To complete the induction, ﬁrst observe that ∥∇ft(zt) + st∥≤∥∇ft(x) + st∥+
ϵt, so that on invoking the induction hypothesis we obtain for t = 2, . . . , r,
∇ft(zt)
 ≤∥∇ft(x)∥+ 2ηL
Xt−1
j=1(1 + 2ηL)t−1−j ∇fj(x) + sj .
(4.40)
Combining inequality (4.40) with (4.39) we further obtain
ϵr+1 ≤2ηL
Xr
j=1
∇fj(x) + sj + 2ηL
Xj−1
l=1 (1 + Lη)j−1−l ∇fl(x) + sl

.
Writing βj ≡
∇fj(x) + sj a simple manipulation of the above inequality
yields
ϵr+1 ≤
2ηLβr +
Xr−1
l=1

2ηL + 4η2L2 Xr
j=l+1(1 + 2ηL)j−l−1
βl
=
2ηLβr +
Xr−1
l=1

2ηL + 4η2L2 Xr−l−1
j=0
(1 + 2ηL)j)

βl
=
2ηLβr +
Xr−1
l=1 2ηL(1 + 2ηL)r−lβl
=
2ηL
Xr
l=1(1 + 2ηL)r−lβl.
Now we are ready to bound the error, which is done by Lemma 4.7 below.
Lemma 4.7 (Bounded error). If for all x ∈X, ∥∇ft(x)∥≤M and ∥∂g(x)∥≤
G, then ∥e(x)∥≤K for some constant K > 0.
Proof. If zt+1 is computed by (4.33), O = proxg, and st ∈∂g(zt), then
zt+1 −zt
≤
2η
∇ft(zt) + st .
(4.41)
Using (4.41) we can bound the error incurred upon using zt instead of xk.
Speciﬁcally, if x ≡xk, and
ϵt :=
∇ft(zt) −∇ft(x)
 ,
t = 1, . . . , T,
(4.42)
then Lemma 4.6 shows the following bound
ϵt ≤2ηL
Xt−1
j=1(1 + 2ηL)t−1−j ∇fj(x) + sj ,
t = 2, . . . , T.
(4.43)
Since ϵ1 = 0, we have
∥e(x)∥≤
XT
t=2 ϵt
(4.43)
≤
2ηL
XT
t=2
Xt−1
j=1(1 + 2ηL)t−1−jβj
= 2ηL
XT −1
t=1 βt
XT −t−1
j=0
(1 + 2ηL)j

=
XT −1
t=1 βt
 (1 + 2ηL)T −t −1

≤
XT −1
t=1 (1 + 2ηL)T −tβt
≤(1 + 2ηL)T −1 XT −1
t=1
∇ft(x) + st
≤C1(T −1)(M + G) =: K.

Nonconvex Proximal Splitting with Computational Errors
95
Thanks to the error bounds established above, convergence of incremental
Nips follows immediately from Theorem 4.2; we omit details for brevity.
4.4
Application to Matrix Factorization
The main contribution of our paper is the new Nips framework, and a
speciﬁc application is not one of the prime aims of this paper. We do, how-
ever, provide an illustrative application of Nips to a challenging nonconvex
problem: sparsity regularized low-rank matrix factorization
min
X,A≥0
1
2∥Y −XA∥2
F + ψ0(X) +
XT
t=1 ψt(at),
(4.44)
where Y ∈Rm×T , X ∈Rm×K and A ∈RK×T , with a1, . . . , aT as its columns.
Problem (4.44) generalizes the well-known nonnegative matrix factorization
(NMF) problem of [25] by permitting arbitrary Y (not necessarily nonnega-
tive), and adding regularizers on X and A. A related class of problems was
studied in [27], but with a crucial diﬀerence: the formulation in [27] does not
allow nonsmooth regularizers on X. The class of problems studied in [27] is
in fact a subset of those covered by Nips. On a more theoretical note, [27]
considered stochastic-gradient like methods whose analysis requires computa-
tional errors and stepsizes to vanish, whereas our method is deterministic and
allows nonvanishing stepsizes and errors.
Following [27] we also rewrite (4.44) in a form more amenable to Nips. We
eliminate A and consider the nonnegatively constrained optimization problem
minX
Φ(X) :=
XT
t=1 ft(X) + g(X),
where
g(X) := ψ0(X) + δ(X|≥0),
(4.45)
and where each ft(X) for 1 ≤t ≤T is deﬁned as
ft(X) := mina
1
2 ∥yt −Xa∥2 + gt(a),
(4.46)
where gt(a) := ψt(a) + δ(a|≥0). For simplicity, assume that (4.46) at-
tains its unique2 minimum, say a∗, then ft(X) is diﬀerentiable and we have
∇Xft(X) = (Xa∗−yt)(a∗)T . Thus, we can instantiate (4.33), and all we need
is a subroutine for solving (4.46).3
We present empirical results on the following two variants of (4.45): (i)
pure unpenalized NMF (ψt ≡0 for 0 ≤t ≤T) as a baseline; and (ii) spar-
sity penalized NMF where ψ0(X) ≡λ∥X∥1 and ψt(at) ≡γ∥at∥1. Note that
without the nonnegativity constraints, (4.45) is similar to sparse-PCA.
2If not, then at the expense of more notation, we can add a strictly convex perturbation
to ensure uniqueness; this error can be absorbed into the overall computational error.
3In practice, we use mini-batches for all the algorithms.

96
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 4.1: Running times of Nips (Matlab) versus SPAMS (C++) for NMF
on RAND, CBCL, and YALE datasets. Initial objective values and tiny runtimes have
been suppressed for clarity.
We use the following datasets and parameters:
(i) RAND: 4000 × 4000 dense random (uniform [0, 1]); rank-32 factorization;
(λ, γ) = (10−5, 10);
(ii) CBCL: CBCL database [38]; 361 × 2429; rank-49 factorization;
(iii) YALE: Yale B Database [26]; 32256 × 2414 matrix; rank-32 factorization;
(iv) WEB: Web graph from Google; sparse 714545 × 739454 (empty rows and
columns removed) matrix; ID: 2301 in the sparse matrix collection [10]);
rank-4 factorization; (λ = γ = 10−6).
On the NMF baseline (Fig. 4.1), we compare Nips against the well opti-
mized state-of-the-art C++ toolbox SPAMS (version 2.3) [27]. We compare
against SPAMS only on dense matrices, as its NMF code seems to be optimized
for this case. Obviously, the comparison is not fair: unlike SPAMS, Nips and
its subroutines are all implemented in Matlab, and they run equally easily
on large sparse matrices. Nevertheless, Nips proves to be quite competitive:
Fig. 4.1 shows that our Matlab implementation runs only slightly slower
than SPAMS. We expect a well-tuned C++ implementation of Nips to run
at least 4–10 times faster than the Matlab version—the dashed line in the
plots visualizes what such a mere 3X-speedup to Nips might mean.
Figure 4.2 shows numerical results comparing the stochastic generalized
gradient (SGGD) algorithm of [13] against Nips, when started at the same
point. As is well-known, SGGD requires careful stepsize tuning; so we searched
over a range of stepsizes, and have reported the best results. Nips too re-
quires some stepsize tuning, but to a much lesser extent than SGGD. As
predicted, the solutions returned by Nips have objective function values lower
than SGGD, and have greater sparsity.

Nonconvex Proximal Splitting with Computational Errors
97
FIGURE 4.2: Sparse NMF: Nips versus SGGD. The bar plots show the sparsity
(higher is better) of the factors X and A. Left plots for RAND dataset; right plots
for WEB. SGGD yields slightly worse objective function values and signiﬁcantly less
sparse solutions than Nips.

98
Regularization, Optimization, Kernels, and Support Vector Machines
4.5
Other Applications
We mention below a few other applications where we have used the Nips
framework successfully. While it lies outside the scope of this chapter to cover
the details of these applications, we refer the reader to research articles that
include the requisite details.
• Online multiframe blind deconvolution [20]. In this application, a slightly
modiﬁed version of Nips is used for processing a stream of blurry im-
ages in an incremental fashion. The end goal is to obtain a sharp re-
construction of a single underlying image. The optimization problem is
nonconvex because given observations y1, . . . , yT , which are assumed to
satisfy the linear model yi ≈Aix, we need to recover both Ai and x.
• Generalized dictionary learning for positive deﬁnite tensors [36]. In this
problem, we seek a dictionary whose “atoms” can be sparsely combined
to reconstruct a set of matrices. The key diﬀerence from ordinary dictio-
nary learning [23] is that the observations are positive deﬁnite matrices,
so the dictionary atoms must be positive deﬁnite, too. The problem ﬁts
in the Nips framework (as for NMF, subproblems relied on a nonnega-
tive least-squares solver [22] and a nonsmooth convex solver [21]).
• Denoising signals with spiky (sparse) noise [6]. This application formu-
lates the task of removing spiky noise from signals by formulating it
as a nonconvex problem with sparsity regularization, and was hence a
suitable candidate for Nips.
4.6
Discussion
This chapter discussed a general optimization framework called Nips that
can solve a broad class of nonconvex composite objective (regularized) prob-
lems. Our analysis is inspired by [33], and we extend the results of [33] to admit
problems that are strictly more general by handling nonsmooth components
via proximity operators. Nips permits nonvanishing perturbations, which is a
useful practical feature. We exploited the perturbation analysis to derive both
batch and incremental versions of Nips. Finally, experiments with medium to
large matrices showed that Nips is competitive with state-of-the-art methods;
Nips was also seen to outperform the stochastic generalized gradient method.
We conclude by mentioning that Nips includes numerous algorithms and
problem settings as special cases. Example are: forward-backward splitting
with convex costs, incremental forward-backward splitting (convex), gradient

Nonconvex Proximal Splitting with Computational Errors
99
projection (both convex and nonconvex), the proximal-point algorithm, and
so on. Thus, it will be valuable to investigate if some of the theoretical results
for these methods can be carried over to Nips.
The most important theoretical question worth pursuing at this point is
a less pessimistic convergence analysis for the scalable incremental version of
Nips than implied by Lemma 4.7.
Bibliography
[1] H. Attouch, J. Bolte, and B. Svaiter.
Convergence of descent meth-
ods for semi-algebraic and tame problems: proximal algorithms, forward-
backward splitting, and regularized Gauss-Seidel methods. Mathematical
Programming, 137(1-2):91–129, 2013.
[2] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Convex optimization
with sparsity-inducing norms. In S. Sra, S. Nowozin, and S. J. Wright,
editors, Optimization for Machine Learning. MIT Press, 2011.
[3] A. Beck and M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algo-
rithm for Linear Inverse Problems. SIAM J. Imgaging Sciences, 2(1):183–
202, 2009.
[4] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, second edi-
tion, 1999.
[5] D. P. Bertsekas. Incremental Gradient, Subgradient, and Proximal Meth-
ods for Convex Optimization: A Survey. In S. Sra, S. Nowozin, and S. J.
Wright, editors, Optimization for Machine Learning. MIT Press, 2011.
[6] A. Cherian, S. Sra, and N. Papanikolopoulos. Denoising sparse noise via
online dictionary learning. In IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), May 2011.
[7] F. H. Clarke. Optimization and nonsmooth analysis. John Wiley & Sons,
Inc., 1983.
[8] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-
backward splitting. Multiscale Modeling and Simulation, 4(4):1168–1200,
2005.
[9] Patrick L. Combettes and Jean-Christophe Pesquet. Proximal splitting
methods in signal processing. In Fixed-Point Algorithms for Inverse Prob-
lems in Science and Engineering, pages 185–212. Springer, 2011.

100
Regularization, Optimization, Kernels, and Support Vector Machines
[10] Timothy A. Davis and Yifan Hu. The University of Florida sparse ma-
trix collection. ACM Transactions on Mathematical Software (TOMS),
38(1):1, 2011.
[11] J. Duchi and Y. Singer.
Online and Batch Learning using Forward-
Backward Splitting.
J. Mach. Learning Res. (JMLR), 10: 2873–2898,
Dec 2009.
[12] Y. M. Ermoliev and V. I. Norkin. Stochastic generalized gradient method
with application to insurance risk management. Technical Report IR-97-
021, IIASA, Austria, Apr. 1997.
[13] Y. M. Ermoliev and V. I. Norkin. Stochastic generalized gradient method
for nonconvex nonsmooth stochastic optimization. Cybernetics and Sys-
tems Analysis, 34:196–215, 1998.
[14] M. Fukushima and H. Mine.
A generalized proximal point algorithm
for certain non-convex minimization problems. Int. J. Systems Science,
12(8):989–1000, 1981.
[15] E. M. Gafni and D. P. Bertsekas. Two-metric projection methods for
constrained optimization. SIAM Journal on Control and Optimization,
22(6):936–964, 1984.
[16] A. A. Gaivoronski. Convergence properties of backpropagation for neural
nets via theory of stochastic gradient methods. Part 1.
Optimization
Methods and Software, 4(2):117–134, 1994.
[17] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for
nonconvex nonlinear and stochastic programming.
arXiv:1310.3787,
2013.
[18] Saeed Ghadimi and Guanghui Lan. Stochastic First- and Zeroth-order
Methods for Nonconvex Stochastic Programming. arXiv:1309.5549, 2013.
[19] S. Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall
PTR, 1st edition, 1994.
[20] M. Hirsch, S. Harmeling, S. Sra, and B. Schölkopf. Online multi-frame
blind deconvolution with super-resolution and saturation correction. As-
tronomy & Astrophysics (AA), Feb. 2011. 11 pages.
[21] D. Kim, S. Sra, and I. S. Dhillon. A scalable trust-region algorithm with
application to mixed-norm regression. In International Conference on
Machine Learning (ICML), 2010.
[22] D. Kim, S. Sra, and I. S. Dhillon. A non-monotonic method for large-scale
non-negative least squares. Optimization Methods and Software (OMS),
Dec. 2011. 28 pages.

Nonconvex Proximal Splitting with Computational Errors
101
[23] K. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T.-W. Lee, and
T. J. Sejnowski. Dictionary learning algorithms for sparse representation.
Neural Computation, 15:349–396, 2003.
[24] D. Kundur and D. Hatzinakos. Blind image deconvolution. IEEE Signal
Processing Magazine, 13(3):43–64, May 1996.
[25] D. D. Lee and H. S. Seung. Algorithms for nonnegative matrix factor-
ization. In Advances in Neural Information Processing Systems (NIPS),
pages 556–562, 2000.
[26] K.C. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face
recognition under variable lighting. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(5):684–698, 2005.
[27] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online Learning for Matrix
Factorization and Sparse Coding. Journal of Machine Learning Research
(JMLR), 11:10–60, 2010.
[28] O. L. Mangasarian.
Mathematical Programming in Neural Networks.
Informs J. Computing, 5(4):349–360, 1993.
[29] J. J. Moreau. Proximité et dualité dans un espace hilbertien. Bull. Soc.
Math. France, 93:273–299, 1965.
[30] Yu. Nesterov. Gradient methods for minimizing composite objective func-
tion. Technical Report 2007/76, Université catholique de Louvain, Center
for Operations Research and Econometrics (CORE), September 2007.
[31] R. T. Rockafellar and R. J.-B. Wets. Variational analysis. Springer, 1998.
[32] M. Schmidt, N. Le Roux, and F. Bach. Convergence rates of inexact
proximal-gradient methods for convex optimization. In Advances in Neu-
ral Information Processing Systems (NIPS), 2011.
[33] M. V. Solodov. Convergence analysis of perturbed feasible descent meth-
ods. Journal Optimization Theory and Applications, 93(2):337–353, 1997.
[34] M. V. Solodov. Incremental gradient algorithms with stepsizes bounded
away from zero. Computational Optimization and Applications, 11:23–35,
1998.
[35] M. V. Solodov and S. K. Zavriev. Error stability properties of generalized
gradient-type algorithms. Journal Optimization Theory and Applications,
98(3):663–680, 1998.
[36] S. Sra and A. Cherian. Generalized Dictionary Learning for Symmet-
ric Positive Deﬁnite Matrices with Application to Nearest Neighbor Re-
trieval. In European Conf. Machine Learning (ECML), Sept. 2011.

102
Regularization, Optimization, Kernels, and Support Vector Machines
[37] S. Sra, S. Nowozin, and S. J. Wright, editors. Optimization for Machine
Learning. MIT Press, 2011.
[38] K.-K. Sung.
Learning and Example Selection for Object and Pattern
Recognition. PhD thesis, MIT, Artiﬁcial Intelligence Laboratory and Cen-
ter for Biological and Computational Learning, Cambridge, MA, 1996.
[39] Silvia Villa, Saverio Salzo, Luca Baldassarre, and Alessandro Verri. Ac-
celerated and inexact forward-backward algorithms. SIAM Journal on
Optimization, 23(3):1607–1633, 2013.
[40] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo. Sparse reconstruc-
tion by separable approximation. IEEE Trans. Sig. Proc., 57(7):2479–
2493, 2009.

Chapter 5
Learning Constrained Task
Similarities in Graph-Regularized
Multi-Task Learning
Rémi Flamary
Laboratoire Lagrange, Observatoire de la Côte d’Azur, Université de Nice
Sophia-Antipolis
Alain Rakotomamonjy
LITIS, Université de Rouen
Gilles Gasso
LITIS, INSA de Rouen
5.1
Introduction ......................................................
104
5.2
Similarity Based Multi-Task Learning ...........................
106
5.2.1
Multi-Task Learning Framework ........................
106
5.2.2
Similarity-Based Regularization .........................
107
5.2.3
Solving the Multi-Task Learning Problem ..............
108
5.3
Non-Convex Proximal Algorithm for Learning Similarities .....
110
5.3.1
Bilevel Optimization Framework ........................
110
5.3.2
Gradient Computation ..................................
112
5.3.3
Constraints on P and λt
................................
113
5.3.4
Computational Complexity ..............................
114
5.4
Numerical Experiments ..........................................
115
5.4.1
Toy Problems ............................................
116
5.4.2
Real-World Datasets .....................................
119
5.4.2.1
Experimental Set-Up .......................
119
5.4.2.2
School Dataset .............................
120
5.4.2.3
Brain Computer Interface Dataset
........
121
5.4.2.4
OCR Dataset ...............................
123
5.5
Conclusion ........................................................
125
Acknowledgments ................................................
125
Appendix .........................................................
126
Bibliography ......................................................
127
103

104
Regularization, Optimization, Kernels, and Support Vector Machines
This chapter addresses the problem of learning constrained task related-
ness in a graph-regularized multi-task learning framework. In such a context,
the weighted adjacency matrix of a graph encodes the knowledge on task
similarities and each entry of this matrix can be interpreted as a hyperpa-
rameter of the learning problem. This task relation matrix is learned via a
bilevel optimization procedure where the outer level optimizes a proxy of the
generalization errors over all tasks with respect to the similarity matrix and
the inner level estimates the parameters of the tasks knowing this similarity
matrix. Constraints on task similarities are also taken into account in this
optimization framework and they allow the task similarity matrix to be more
interpretable, for instance, by imposing a sparse similarity matrix. Since the
global problem is non-convex, we propose a non-convex proximal algorithm
that provably converges to a stationary point of the problem. Empirical ev-
idence illustrates the approach is competitive compared to existing methods
that also learn task relation and exhibits an enhanced interpretability of the
learned task similarity matrix.
5.1
Introduction
Multi-task learning (MTL) has gained a lot of attention over the past
years. Given a set of diﬀerent but related tasks, the underlying idea is to jointly
learn these tasks by exploiting their relationships. Such a procedure has proved
useful (in terms of generalization ability) when only a few samples are available
for the tasks [5, 1, 26, 21]. One of the most important problems in multi-task
learning is the assessment of task relatedness. Existing approaches seek task
relationship either in input space, feature space [1, 27], or output space [11].
They consider relations between tasks through dedicated regularizations or
based on Bayesian priors.
To be more concrete, when jointly learning the tasks, regularization ap-
proaches assume that the tasks share a common low-dimensional latent sub-
space or their parameters are close to the average parameter vector [10]. The
links between tasks can also be enforced by imposing a joint sparsity pattern
[22, 1] across tasks. However, such a similarity is globally imposed for all tasks
and may hinder good generalization performance mainly when unrelated tasks
are forced to borrow similar characteristics. Hence, more elaborate approaches
such as pairwise similar tasks [10], clustering of tasks [10, 15, 16], or hierarchi-
cally structured tasks [28, 33] were proposed in order to cope with this issue.
For instance, Widmer et al. [33] supposed the knowledge of a hierarchical tree
modeling task dependency while Xing et al. [28] used agglomerative clustering
to ﬁnd the tree, which is further applied to learn task parameters. Contrarily,
clustering approaches due to [15], [16], [19], or [35] attempt to discover groups
of similar tasks along with the estimation of their parameters. In the same

Learning Constrained Task Similarities in Graph-Regularized MTL
105
vein, Zhang et al. [34] characterized task relatedness via a covariance matrix.
Their formulation encompasses various existing methods including pairwise
similarity constraints or cluster constraints. Solving for the covariance matrix
turns out to be a problem of learning distance metric between tasks.
Most methods for learning task similarities provide good empirical perfor-
mance, but few of them aim at enhancing the interpretability of the learned
task relations. For instance, Zhang et al. [34] learn a dense task covariance
matrix that depicts relations between tasks. Since this matrix is dense, it is
thus diﬃcult to interpret which task relations are the most relevant for the
learning problem. In this work, we look at learning interpretable task sim-
ilarities in multi-task learning problems. We focus on a popular multi-task
framework denoted as a graph-based regularized framework [10]. As formally
deﬁned in the sequel, in this framework, task relations are represented as a
graph whose nodes are the tasks, and the weighted edges encode some knowl-
edge over similarities between tasks. This framework has been shown to be of
practical interest [9, 32] and beneﬁts from very eﬃcient algorithms for solving
the related multi-task optimization problems [32].
Our objective and proposal in this chapter is to learn the adjacency ma-
trix of the task relations graph, jointly with the task decision function pa-
rameters, while making the graph as interpretable as possible. Hence, we may
accept some slight loss in generalization performance if the gain in graph in-
terpretability is important. This interpretability of the adjacency matrix is
achieved by incorporating in the global learning problem some speciﬁc con-
straints over the graph parameters. The constraints that we consider in the
sequel are usually sparsity-inducing penalties that are enforcing the tasks to
become unrelated.
Our main contribution hereafter is to propose a novel procedure for learn-
ing similarities between tasks in graph-based multi-task learning. As detailed
in the sequel, since in this framework the relation between a pair of tasks
can be interpreted as a hyper-parameter of the global multi-task model, we
address the problem as a hyper-parameter optimization issue ([2, 6]). Typ-
ically, when few hyper-parameters have to be optimized, a cross-validation
procedure, aiming at optimizing an estimation of the generalization error, is
employed in conjunction with a grid-search over the hyper-parameter values
[14, 4]. Since in our framework, the number of hyper-parameters (typically
O(T 2) parameters, for T tasks) to optimize make this approach intractable,
the method we advocate consists of a bilevel approach: at the outer level, a
generalization criterion over all tasks is employed to measure the goodness of
task relatedness parameters and this criterion is thus optimized with respect to
these parameters under some sparsity-inducing constraints. The inner level is
devoted to the optimization of task parameters for a ﬁxed task relation graph.
Due to the non-convexity of the generalization errors with respect to task sim-
ilarity parameters, the overall problem is non-convex. Fortunately, the inner
problem we design is convex and, depending on the loss functions considered,
it may admit a closed-form solution. We solve this bilevel problem through a

106
Regularization, Optimization, Kernels, and Support Vector Machines
non-convex proximal approach with guaranteed convergence properties. The
ﬂexibility of the approach allows the use of a broad range of generalization
error proxies that can be adapted to the MTL problem at hand. It also al-
lows easy incorporation of constraints over the task relations such as sparsity,
link, or cannot-link constraints in the matrix similarity learning process. As
a consequence, the learned task-similarity matrix is sparse and provides im-
proved interpretability of connections between tasks. The experimental results
of synthetic and real-world problems clearly support this evidence.
The rest of the chapter is organized as follows: Section 5.2 describes the
graph-based multi-task learning setting we are interested in and states how
the task parameters are obtained once the task similarity matrix is ﬁxed. The
learning of this matrix is explained in Section 5.3 where we formulate the
bilevel optimization problem and the non-convex proximal algorithm used to
solve it. Finally, empirical comparisons illustrate the compelling performance
of the approach. In particular, these experiments show the ability of our al-
gorithm to unravel the underlying structure (groups or manifold) of the tasks
and emphasize the interpretability of the results.
5.2
Similarity Based Multi-Task Learning
Before describing how task relatedness is learned, we ﬁrst present the gen-
eral multi-task learning framework we are dealing with, as well as the regu-
larizer we have considered for inducing transfer between tasks.
5.2.1
Multi-Task Learning Framework
Assume we are given T learning tasks to be learned from T diﬀerent
datasets (xi,1, yi,1)n1
i=1, · · · , (xi,T , yi,T )nT
i=1, where any xi,· ∈Rd and yi,· ∈R,
and nt denotes the t-th dataset size. In the sequel, we will represent the train-
ing examples {xi,t}nt
i=1 in a matrix form as Xt ∈Rnt×d and the corresponding
labels gathered in vector yt ∈Rnt. For a given task t, we are looking for a
linear prediction function ft(x) of the form
ft(x) = w⊤
t x + bt
(5.1)
with wt ∈Rd and bt ∈R being the linear function parameters. Basically,
ft(x) depicts the presumable dependencies between a given example x and its
associated label y.
Multi-task methods aim at learning all T decision functions in a simulta-
neous way while imposing some constraints that induce relatedness between
tasks. Hence, most multi-task learning problems can be cast as the following

Learning Constrained Task Similarities in Graph-Regularized MTL
107
optimization setup:
min
{wt},{bt}
X
t,i
L(ft(xi,t), yi,t) + Ω(w1, · · · , wT )
(5.2)
where L(ft(x), y) is a loss function measuring discrepancy between the actual
and predicted output related to an example x, and Ωa regularizer inducing
task relatedness thus involving all vectors {wt}.
5.2.2
Similarity-Based Regularization
One typical issue in multi-task learning is the choice of a regularization
term Ωthat eﬃciently helps in improving the generalization performances
of the prediction functions. Indeed, most of the existing MTL regularization
terms are based on a strong prior knowledge about the task relatedness. We
can for instance mention regularizers that enforce similarity of task parameters
{wt} to the average parameter vector
1
T
P
t wt [9, 10], that make classiﬁers
belong to a low dimensional linear subspace [1]. Other regularizers induce
the classiﬁers to be agglomerated into clusters [15], or compel tasks to share
a common subset of discriminative kernels [23] or even impose tasks to be
similar according to pre-deﬁned task networks [17].
Because it encompasses several forms of the above-mentioned regularizers,
we focus on the graph-based regularization term proposed by Evgeniou et al.
[10] that induces pairwise similarity between tasks. This regularizer is deﬁned
as
Ω({wt}T
t=1, {λt}, P)=
X
t
λt∥wt∥2
2+
X
t,s
ρt,s∥wt −ws∥2
2
(5.3)
where λt ∈R+ \ {0} and P ∈(R+)T ×T , a matrix of general term ρt,s (i.e.,
P = [ρt,s]T
t,s=1), are the regularization hyper-parameters. The ﬁrst term of this
regularizer corresponds to the classical ℓ2-norm regularization (ridge) while
the second one promotes a pairwise task similarity imposed by the ρt,s pa-
rameters. From the graph point of view, the matrix P is the weighted graph
adjacency matrix and it reﬂects the relationship between tasks, as obviously,
a large ρt,s value enforces tasks s and t to be similar while if ρt,s = 0 then
these tasks will likely be unrelated (in the ℓ2-norm sense). We have imposed
P(t, t) = 0 ∀t as these diagonal terms of the matrix have no impact on the cost
function. Furthermore, in order to reduce the number of hyper-parameters in
the model and because it intuitively makes sense, we also have considered P
to be a symmetric matrix. Graph-regularized multi-task learning problems are
denoted as such because the regularizer given in Equation (5.3) can also be
interpreted as the following. Indeed, by deﬁning matrix W = [w1 · · · wT ], one
can notice that Equation (5.3) equivalently writes
Ω(W, {λt}, P) = trace
 WΓW⊤
(5.4)
where Γ = Λ + L and Λ is a diagonal matrix with entries λt and L is the

108
Regularization, Optimization, Kernels, and Support Vector Machines
Laplacian of the graph where the vertices are corresponding to the tasks.
Assuming the edges are parameters ρt,s, the Laplacian matrix writes L =
D + P where D is a diagonal matrix with elements D(t, t) = PT
s=1 ρt,s.
Our main contribution is to propose a framework for learning this matrix
P of task relatedness and to make this matrix as interpretable as possible
by imposing some constraints on its entries. Because the matrix P can also
be considered as a matrix of hyper-parameters, we introduce here a problem
where these hyper-parameters are learned with respect to a proxy of the gen-
eralization error. This contribution is of importance for obtaining prediction
functions with good generalization capabilities as well as a task similarity ma-
trix that is interpretable. Indeed, using our novel formulation of the problem,
it becomes easy to impose single or group sparsity-inducing constraints over
entries of P.
Before providing the details of how these task relations are learned, we
show in the next paragraph how Problem (5.2) can be solved for ﬁxed λt and
matrix P.
5.2.3
Solving the Graph-Regularized Multi-Task Learning
Problem
We focus now on solving problem (5.2) with the regularization term deﬁned
as in Equation (5.3). For the sake of clarity, we restrict ourselves to the squared
loss function, denoted as L(f(x), y) = 1
2(f(x) −y)2, although our algorithm
can be applied to other loss functions such as the hinge loss. We discuss this
point in the sequel.
Using a quadratic loss function and matrix notation, and based on the
regularization given in Equation (5.3), Problem (5.2) reads
min
{wt},{bt}
J({wt}, {bt}, P, {λt})
(5.5)
where the objective function is
J(·) = 1
2
X
t
∥yt −Xtwt −bt1It∥2
2 +
X
t
λt∥wt∥2
2 +
X
t,s
ρt,s∥wt −ws∥2
2 (5.6)
with 1It ∈Rnt being a vector of ones. Note that for λt > 0, ∀t and ρt,s ≥
0, ∀t, s, this problem is strictly convex and admits a unique solution. For ﬁxed
parameters {λt}T
t=1 and P, a closed-form solution of this problem can be
obtained by solving the linear system related to the normal equations. The
gradient of J(·) with respect to the prediction function parameters of task k
is given by:
∇wkJ = Qkwk + bkX⊤
k 1Ik −4
X
t
ρt,kwt −ck
(5.7)
where I is the identity matrix, Qk = X⊤
k Xk + (2λk + 4pk) I ∈Rd×d,

Learning Constrained Task Similarities in Graph-Regularized MTL
109
ck
= X⊤
k yk ∈Rd and ﬁnally pk = P
t ρt,k. Similarly, the gradient of J(·)
with respect to the bias term bk takes the form
∇bkJ = 1I⊤
k Xkwk + nkbk −1I⊤
k yk.
(5.8)
From the gradients (5.7) and (5.8) and the resulting optimality conditions,
solution of Problem (5.5) is obtained by solving the system:
Aβ = c
(5.9)
where β =

˜w⊤
1
· · ·
˜w⊤
T
⊤∈R(d+1)·T is the vector containing all the
prediction function parameters with ˜wk =

w⊤
k
bk
⊤∈Rd+1, c =

˜c⊤
1 · · · ˜c⊤
T
⊤∈R(d+1)·T with ˜ck =
h
c⊤
k 1I⊤
k yk
i⊤
, and A ∈R(d+1)·T ×(d+1)·T
is a matrix of the form:
A =


˜Q1
−4ρ1,2˜I
. . .
−4ρ1,T˜I
−4ρ2,1˜I
˜Q2
. . .
−4ρ2,T˜I
...
...
...
...
−4ρT,1˜I
−4ρT,2˜I
. . .
˜QT


(5.10)
involving the matrices
˜Qk =

Qk
X⊤
k 1I
1I⊤Xk
nk

, and ˜I =
 I
0
0
0

.
Notice that for λt > 0 ∀t, the matrix A is full-rank regardless of the matrices
{Xt} and thus the linear system (5.9) has a unique solution that provides the
optimal parameters of all prediction functions {ft}T
t=1. Note that depending
on the number of tasks and the dimensionality of the training examples this
matrix A can be pretty large. In such a case, it can be beneﬁcial to take ad-
vantage of the sparse structure of A for the linear system (5.9) resolution. For
instance, a Gauss-Seidel procedure, which consists of optimizing alternatingly
over the parameters of a given task, can be considered.
For optimizing the task similarity matrix P, the linear system Aβ = c
will be of paramount importance since it deﬁnes an implicit function that
relates the optimal task parameters {wt, bt}T
t=1 to the entries of P. Indeed,
the bilevel approach we apply to determine P (see Equations (5.12) and (5.13))
in the next section requires the gradient of the estimated generalization error
measure in function of P. Owing to this equation we will be able to compute
this gradient via the explicit expression of the gradient of β with respect to P.
Consequently, while we have stated that other loss functions can be considered
in our approach, the solution of the graph-regularizer multi-task learning has
to satisfy a linear system of the form Aβ = c. For instance, the hinge loss
function satisﬁes this property if the learned problem is solved in the dual
[18, 31, 9].

110
Regularization, Optimization, Kernels, and Support Vector Machines
5.3
Non-Convex Proximal Algorithm for Learning
Similarities
The multi-task approach with ﬁxed hyperparameters described in the
above section is interesting in itself. However, it may be limited by the large
number of regularization parameters to be chosen, namely all the {λt}T
t=1 and
the matrix of task similarities P. When there exists a strong prior knowledge
concerning task similarities, the P matrix might be pre-deﬁned beforehand.
When no prior information is available, P can be learned from training data
as done by Zhang et al. [34]. In addition, when one’s objective is also to gain
some insights over the structure of the tasks and how they are related, then
constraints on task similarities have to be imposed. In what follows, we de-
scribe our algorithm for learning the matrix P as well as the regularization
parameters {λt} in the context of graph-regularized multi-task learning.
5.3.1
Bilevel Optimization Framework
We learn the matrix task similarity P as well as the regularization param-
eters {λt}T
t=1 by considering them as hyper-parameters and by minimizing an
estimate of a generalization error denoted as E(·). This estimate E is naturally
a function of all decision function parameters β. For addressing this problem,
we consider a bilevel optimization problem similar to the one of Bennett et
al. [3]: the outer level of the problem consists of minimizing E with respect
to P and all {λt}T
t=1 and the inner level aims at learning all decision function
parameters β.
While several choices of E can be considered, for the sake of clarity, we
have set E(·) to be a validation error of the form :
E(β⋆) =
T
X
t=1
X
i
Lv(˜yi,t, ˜x⊤
i,tw⋆
t + b⋆
t )
(5.11)
where β⋆is a vector including the optimal decision function parameters w⋆
t
and b⋆
t for all tasks t = 1, . . . , T. The sets {˜xi,t, ˜yi,t}T
t=1 refer to some valida-
tion examples and Lv(·, ·) is a twice diﬀerentiable loss function that measures
the discrepancy between the real and predicted output associated to an in-
put example. Two kinds of loss have been considered in this work, one more
adapted to regression tasks Lv(y, ˆy) = (y −ˆy)2 and another one more suited
to classiﬁcation tasks, which is a non-convex sigmoid function that smoothly
approximates the 0 −1 loss function Lv(y, ˆy) =
1
1+eκyˆy with κ > 0. Note that,
at the expense of introducing some cumbersome notations, it is straightfor-
ward to modify Equation (5.11), so that generalization error estimate is a
leave-one-out error or a k-fold cross-validation error.

Learning Constrained Task Similarities in Graph-Regularized MTL
111
Now that E(·) has been formally deﬁned, we are in a position to state the
bilevel optimization we are interested in. Indeed, since the vector β depends on
the matrix P and the hyper-parameters λt, the bilevel optimization problem
can be expressed as:
min
θ
E(β∗(θ)) + Ωθ(θ)
(5.12)
with β∗(θ) = argmin
β
J(θ, β)
(5.13)
with θ =
h
λ1, . . . , λT , {ρi,j}T
i=1,j>i
i⊤
, a vector of size D = T + T (T −1)
2
(consid-
ering symmetry of P and P(t, t) = 0, ∀t), J(·) the objective function deﬁned
in Equation (5.6), and Ωθ being a regularizer over the parameters θ. Typ-
ically, Ωθ is related to the projection onto some convex and closed subset
Θ of RD that deﬁnes some constraints over the vector θ. The bilevel Prob-
lem (5.12) has a particular structure in that the inner Problem (5.13), which
is actually Problem (5.5), is strictly convex for λt > 0, a condition that is
guaranteed by some speciﬁc choice of Θ. This strict convexity is of primary
importance since it allows us to compute the unique β∗for a given θ. In gen-
eral cases, this Problem (5.12) is non-convex, but its structure suggests that
a non-convex proximal splitting can be of interest. Indeed, since E(·) is sup-
posed to be twice diﬀerentiable and Ωθ a non-smooth function, a non-convex
forward-backward splitting algorithm, such as the one proposed by [29], can
be successfully lifted to our purpose, especially if the proximal operator of Ωθ
can be simply computed. For a proper convex function, this proximal operator
is deﬁned as [8]:
PΩθ(ˆθ) = arg min
θ
1
2∥θ −ˆθ∥2
2 + Ωθ(θ).
Hence, if Ωθ is deﬁned as the indicator over a convex set Θ, the proximal
operator boils down to be a projection onto the set Θ.
According to Sra [29], the non-convex proximal algorithm we use for solv-
ing (5.12) is based on the following simple iterative scheme:
θk+1 = PΩθ

θk −ηk∇θE(β∗(θk))

(5.14)
where PΩθ is the proximal operator of Ωθ, ηk a step size that should satisfy
ηk ≤1
L, L is the Lipschitz constant of ∇θE, and β∗(θk) denotes the vector of
all optimal decision function parameters given the ﬁxed matrix P, and {λt}
as deﬁned by θk.
Convergence of this iterative scheme to a stationary point of Problem
(5.12) can be formally stated according to the following proposition:
Proposition 5.1. For compact sets Θ that guarantee λt > 0, ∀t and for any
loss functions Lv that are continuous and twice diﬀerentiable on Θ, E(β(θ)) is
continuous and gradient Lipschitz on Θ. Hence, the sequence of {θ(k)} obtained
using the iteration given by Equation (5.14) converges towards a stationary
point of Problem (5.12).

112
Regularization, Optimization, Kernels, and Support Vector Machines
Proof. (Sketch) The proof proceeds by showing that E(·) is continuous and its
gradient is Lipschitz and then by directly applying Theorem 2 in Sra [29]. For
showing smoothness of E(·) with respect to θ, we compute
∂2E
∂θk∂θs and show
that each of these components of the Hessian is continuous with respect to θ.
Once continuity has been shown, we use arguments on compactness of Θ to
prove that absolute value of all these components is bounded. This implies that
the Frobenius norm of the Hessian is bounded and so is the largest eigenvalue
of the Hessian. Hence, we can state that E(·) is indeed gradient Lipschitz and
this concludes the proof. Details about continuity and diﬀerentiability of E(·)
as well as Hessian entries computation are given in the appendix.
5.3.2
Gradient Computation
Like all gradient proximal splitting algorithms, our approach needs the
gradient of the objective function E(·). The next paragraphs explain how it
can be eﬃciently computed.
At ﬁrst, we apply the chain rule of diﬀerentiation [7] in order to obtain the
gradient of E(·) with respect to θ. This leads to the general expression of the
partial derivatives of E(·):
∂E(β(θ))
∂θk
=
(d+1)×T
X
s=1
∂E(β(θ))
∂βs(θ)
∂βs(θ)
∂θk
= ∇βE(β)⊤˙βk
(5.15)
with ˙βk a vector containing the partial derivatives { ∂βs
∂θk }. These latter partial
derivatives can be obtained through the implicit function deﬁned by Equation
(5.9) relating the optimal values of β to the parameters θ. Diﬀerentiating (5.9)
with respect to θk leads to
A ˙βk + ˙Akβ = ˙ck,
with ˙Ak and ˙ck being, respectively, the matrix and vector of component-wise
derivative of A and c with respect to θk (detailed expression of the matrix
˙Ak is given in the appendix). By rearranging the equation and taking into
account the fact that ˙ck = 0, we have
˙βk = −A−1( ˙Akβ)
(5.16)
Note that for small-size problems, computing this gradient can be relatively
cheap since the inverse matrix A−1 may be obtained as a by-product of the
resolution of Problem (5.9). However, if A−1 has not been pre-computed,
obtaining the complete gradient of E(·) requires solving D =
T 2+T
2
linear
systems of size (d + 1) · T and this can rapidly become intractable. In order
to render the problem tractable, a simple trick proposed in Keerthi et al. [18]
can also be used here. Indeed, by plugging Equation (5.16) back into (5.15),

Learning Constrained Task Similarities in Graph-Regularized MTL
113
∂E
∂θk can be reformulated as:
∂E(β(θ))
∂θk
= ∇βE(β)⊤A−1(−˙Akβ)
= d⊤(−˙Akβ)
(5.17)
with d being the solution of the linear system:
A⊤d = ∇β, E(β)
(5.18)
which does not depend on the variable θk used for diﬀerentiation. Hence,
according to this formulation of the partial derivative ∂E(β(θ))
∂θk
, only a single
linear system has to be solved for computing the full gradient of E(·) with
respect to θ.
5.3.3
Constraints on P and λt
Let us now discuss the choice of the regularizer Ωθ. Typically, Ωθ is deﬁned
as the indicator function over a set Θ, formally
Ωθ(θ) = IΘ(θ) =
 0
if θ ∈Θ
∞
otherwise
where the set Θ deﬁnes some constraints we want to impose on the matrix
similarity P and the hyper-parameters {λt}T
t=1.
This set Θ can be deﬁned as the intersection of several constraints and it
typically translates some prior knowledge we have over the task relatedness.
If no knowledge on task similarities are given, the simplest set one may choose
is
Θ =
 ρt,s
: 0 ≤ρt,s ≤M, ∀t, s
λt
: mλ ≤λt ≤M, ∀t
(5.19)
with 0 < mλ and M being some lower and upper bounds. The small quantity
mλ ensures that a minimal smoothness constraint is enforced on all task pa-
rameters {wt}T
t=1. The set deﬁned by (5.19) imposes on the λt to be strictly
positive as required for convergence of the algorithm and it lets the algo-
rithm ﬁx all task similarity parameters. While rather simple, this choice al-
ready proves to provide good multi-task performance as well as excellent in-
terpretability of the task similarity since it induces sparsity of the matrix P
because negative correlations of pairwise tasks are ignored by our regular-
izer deﬁned in Equation (5.3). Ignoring these negative correlations can surely
induce a lack of information transfer between tasks and thus may induce
slight loss of generalization performance. However, this is inherently due to
the graph-based regularization framework and cannot be alleviated by our
learning algorithm.
If some tasks are known to be respectively unrelated, strongly related,
and with unknown relatedness, the following set Θ can be considered

114
Regularization, Optimization, Kernels, and Support Vector Machines
instead:
Θ =







ρt,s
: ρt,s = 0, for non-similar tasks
ρt,s
: mρ ≤ρt,s ≤M, for must-be-similar tasks
ρt,s
: 0 ≤ρt,s ≤M, for all other pairwise tasks
λt
: mλ ≤λ ≤M, ∀t.
(5.20)
Note that in this set, we have lower-bounded some task similarities with mρ≫0
for tasks that are known to be related since this will indeed force the param-
eters of these related tasks to be close.
In order to enhance interpretability, sparsity of matrix P can be further in-
creased by considering in the regularization term an ℓ1 regularizer in addition
to the projection on the set Θ. In such a case, we may have
ΩΘ−ℓ1(θ) = λθ
T (T −1)
2
X
k=1
|θk| + IΘ(θ)
(5.21)
where IΘ(θ) stands for the indicator function of the set Θ. From simple alge-
bras, one can show that for the above-given convex sets Θ, the proximal oper-
ator of ΩΘ−ℓ1 consists of a component-wise application of a soft-thresholding
operator S(θ) = sign(θ)(|θ| −λθ)+ followed by a projection on the set Θ with
the function (z)+ = max(0, z).
According to the iterative scheme, it is easy to consider other kinds of
constraints on the matrix task similarities as long as their proximal operators
are simple to obtain. For instance, we could have dropped the positivity con-
straints on ρt,s and instead impose positive deﬁniteness constraint on the task
covariance matrix Γ deﬁned in Equation 5.4. In this context, the proximal
operator on the set of positive deﬁnite matrices would have been in play. We
could also have combined sparsity constraints on components of P in addition
to the positive deﬁniteness of Γ, resulting in a more involved but computable
proximal operator. However, these constraints would considerably increase the
computational burden of the overall optimization scheme as the related prox-
imals involve spectral decomposition of Γ and we have not considered them
in this work.
5.3.4
Computational Complexity
The global algorithm for learning task similarities and regularization pa-
rameters is presented in Algorithm 6. It is diﬃcult to evaluate the number
of iterations needed before convergence. We can note, however, that for each
iteration of the algorithm, the main computational bulk resides in solving the
MTL problem for ﬁxed task-similarity matrix P. In our case, this consists of
solving (5.9) and the linear system needed for obtaining d. A plain implemen-
tation of these two linear systems would lead to a global complexity of the
order of O
 (d + 1)3T 3
. Nonetheless, we believe that the speciﬁc structure of

Learning Constrained Task Similarities in Graph-Regularized MTL
115
Algorithm 6 Non-convex Proximal Splitting for Learning Task Similarities
1: k ←0
2: initialize θ0 ∈Θ
3: choose step size η ≤1
L (or do backtracking)
4: repeat
5:
% steps for computing ∇θE
6:
compute β(θk) by solving (5.9)
7:
compute ∇βE
8:
d ←solution of A⊤d = ∇βE
9:
for all k do
10:
compute ˙Ak
11:
∂E
∂θk ←−d⊤˙Akβ
12:
end for
13:
% proximal step
14:
θk+1 ←PΘ(θk −η∇θE)
15: until convergence criterion is met
A can be exploited for achieving better complexity, or that speciﬁc eﬃcient
algorithms for graph-based regularized multi-task learning can be developed.
Such an algorithm already exists for hinge loss function and it can be adapted
to the square loss. From another perspective, block iterative methods such as
Gauss-Seidel methods can be implemented for solving the linear system (5.9).
But this implementation study is left to future works.
5.4
Numerical Experiments
The approach we propose for learning the task similarity matrix P as
well as the model hyper-parameters λt has been tested on several numerical
problems including toy examples and three real-world problems.
Besides reporting regression and classiﬁcation performance, we also pro-
vide results on the interpretability of the task relations learned by our algo-
rithm. Indeed, the similarity matrix P can be understood as an adjacency
matrix of the task relation graph. Hence, it can be nicely plotted and its
sparsity pattern analyzed.
Note that for all experiments we have considered, the loss function of
the inner level is the quadratic loss function, thus J(·) is exactly the one
given in Equation (5.6). While one may argue that such a loss function is
inadequate for classiﬁcation problems,
Rifkin et al. [25] and Suykens and
co-authors [30, 13, 12], however, stated that it is still competitive in many of
these problems.

116
Regularization, Optimization, Kernels, and Support Vector Machines
5.4.1
Toy Problems
The toy problems we consider here aim at only proving that our algorithm
can learn the intrinsic structure of the tasks and how they are related. We
show that even for the simple constraints Θ (5.19) we have imposed on θ, our
approach is able to learn diﬀerent task-relation structures such as clusters of
tasks or tasks living in a non-linear manifold. The problem is built as follows:
given a vector ¯w⊤= [1, 2], a rotation of angle γt is applied to ¯w so as to obtain
the actual linear model parameters ¯wt for task t. Examples {xi,t} are drawn
from a two-dimensional zero-mean and unit variance normal distribution and
the corresponding yi,t are obtained according to the equation
yi,t = x⊤
i,t ¯wt + ϵi,t
where ϵi,t ∼N(0, 0.5) is some additive noise added to the output.
Two speciﬁc synthetic problems illustrate our points. In the ﬁrst one, tasks
are structured in two clusters by randomly applying a rotation of γt = 0 or
γt = π/2. Hence, our algorithm should be able to recover this clustered struc-
ture. For the other example, we apply a rotation whose angle γt is uniformly
drawn from [0, π]. Hence, tasks are supposed to be similar only to few neigh-
bors and the adjacency matrix should reﬂect these local similarities of tasks.
For each of these problems, 40 tasks were built and 20 examples for the learn-
ing set and 20 examples for the validation set are randomly drawn. We have
reported an example of the qualitative results obtained using ΩΘ as the in-
dicator of the set given in Equation (5.19) as well as ΩΘ−ℓ1 with mλ = 0.1,
M = 1000, and λθ = 0.05. We also report the obtained results while applying
the approach of Zhang et al. [34], named hereafter metric-MTL and based on
the estimation of a dense task covariance matrix.
Figure 5.1 provides an example of results that can be obtained by our
approach as well as the competitor’s on the clustered tasks toy problem. We
note that our algorithm using ΩΘ is able to nicely infer the tasks relation since
only 23% of the adjacency matrix P entries are non-zero and among those
coeﬃcients, 98% corresponds to links between tasks from the same cluster.
When a sparsity-inducing regularizer is further added to the constraints, the
adjacency matrix is more sparse and links between tasks from diﬀerent clusters
disappear.
For the manifold-based tasks problem, results are depicted in Figure 5.2.
Again, we can clearly see that our approach using both types of constraints
is able to learn the underlying task structure. Indeed, we can note that, when
using only ΩΘ, the ratio of non-zero coeﬃcients in the adjacency matrix is
about 14% which shows that pairwise relationships were found. In addition,
as desired, links between tasks mainly exist between neighbor tasks, thus
providing evidence that the manifold structure of the task has been recovered.
Using ΩΘ−ℓ1 as a regularizer gives a similar result although with a more
aggressive sparsity pattern.

Learning Constrained Task Similarities in Graph-Regularized MTL
117
−0.4
−0.2
0
0.2
0.4
0.6
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
wt(1)
wt(2)
Vectors wt and the adjacency matrix P. Sparsity rate of P :0.23
 
 
P matrix
wt for γ=0
wt for γ = π/2
−0.4
−0.2
0
0.2
0.4
0.6
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
wt(1)
wt(2)
Vectors wt and the adjacency matrix P. Sparsity rate of P :0.10
 
 
P matrix
wt for γ=0
wt for γ = π/2
−0.4
−0.2
0
0.2
0.4
0.6
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
wt(1)
wt(2)
Vectors wt and the adjacency matrix P. Sparsity rate of P :1.00
 
 
P matrix
wt for γ=0
wt for γ = π/2
FIGURE 5.1: Learned weight vectors {wt} and adjacency graph P for the
clustered tasks toy problem. The graphs plot these 2D vectors learned for
each task and the adjacency graph inferred by our approach is materialized as
lines between the vectors. The task parameters are theoretically split in two
clusters with either γt = 0 or γt = π/2. Results obtained by our algorithm
using (top) ΩΘ, (middle) ΩΘ−ℓ1, and (bottom) metric-MTL (method due to
Zhang et al. [34]). For the latter method, the links depict non-zero entries in
the inverse covariance task matrix. Sparsity rate refers to the proportion of
non-zero entries of the learned similarity matrix.

118
Regularization, Optimization, Kernels, and Support Vector Machines
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
wt(1)
wt(2)
Vectors wt and the adjacency matrix P. Sparsity rate of P :0.14
 
 
P matrix
wt
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
wt(1)
wt(2)
Vectors wt and the adjacency matrix P. Sparsity rate of P :0.08
 
 
P matrix
wt
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
wt(1)
wt(2)
Vectors wt and the adjacency matrix P. Sparsity rate of P :1.00
 
 
P matrix
wt
FIGURE 5.2: Learned weight vectors {wt} and adjacency graph P for the
manifold-based tasks toy problem. The graphs plot these 2D vectors learned
for each task and the adjacency graph inferred by our approach is materialized
as lines between the vectors. The task parameters theoretically lie on a mani-
fold. Results obtained by our algorithm using (top) ΩΘ, (middle) ΩΘ−ℓ1, and
(bottom) metric-MTL. For the latter method, the links depict non-zero entries
in the inverse covariance task matrix. The sparsity rate is the proportion of
non-zero entries of the similarity matrix.

Learning Constrained Task Similarities in Graph-Regularized MTL
119
These two examples illustrate that our approach is able to learn complex
relationships between tasks such as non-linear manifold and that the proposed
constraints induce sparsity in the similarity matrix and thus enhance inter-
pretability of the task relations. In comparison, looking at the rightmost plots
of Figures 5.1 and 5.2, we can see that metric-MTL is also able to recover
the complex geometrical relationship between tasks but these relationships
are completely hidden by the dense structure of the learned similarity matrix.
Note that for this problem involving 40 tasks and a total number of 1600
training/validation examples, the optimization is of the order of 10 seconds
on a recent Intel processor with non-optimized MATLAB source code. Our
approach can then estimate an optimal P matrix in a reasonable amount of
time. This could not have been performed using a classical cross-validation
procedure on the corresponding 580 ρt,s parameters.
5.4.2
Real-World Datasets
The approach we proposed has also been experimented on several real-
world datasets. The results we have achieved are presented hereafter detailing
the experimental set-up.
5.4.2.1
Experimental Set-Up
Several multi-task learning algorithms have been compared in terms of
performance as well as in terms of interpretability of the learned task rela-
tionships if the latter is applicable.
The baseline approach, denoted as “ridge indep.”, is an ensemble of ridge
regression problems trained independently on each task. MTL approaches that
learn task relations have also been considered. This includes the metric-MTL
of Zhang et al. [34] and clustered multi-task learning (cluster-MTL in the
remainder), an approach proposed by Jacob et al. [15] where task similarities
are also learned through the inference of the underlying metric between tasks.
For a fair comparison between our approach, named “CoGraph-MTL” (for
Constrained Graph-regularized MTL) and the other methods, we selected
competitor hyper-parameters by maximizing their performance on the val-
idation set. Note that our bilevel approach also uses the validation set for
selecting hyper-parameters but they are optimized through our non-convex
proximal method in the outer level.
Depending on the datasets, a squared function Eℓ2(·, ·) or a sigmoid func-
tion Esig(·, ·) with κ = 1 is used as the outer loss function Lv(·, ·). In addition,
for all problems, unless speciﬁed, we have used ΩΘ as deﬁned in Equation
(5.19) with mλ = 1 and M = 1000 as well as ΩΘ−ℓ1 (see Equation 5.21) with
λθ = 0.05 for constraining our graph-regularized MTL method to be sparser.
For each dataset, 10 random splits have been generated and averaged per-
formance measure, mean square error or area under the ROC curve (AUC),
on the test set was reported. We also performed a signed rank Wilcoxon test

120
Regularization, Optimization, Kernels, and Support Vector Machines
tasks
tasks
Cluster−MTL
 
 
10
20
30
40
50
5
10
15
20
25
30
35
40
45
50
2
2.1
2.2
2.3
2.4
2.5
tasks
tasks
Metric−MTL
 
 
10
20
30
40
50
5
10
15
20
25
30
35
40
45
50
−1
0
1
2
3
4
5
x 10
5
tasks
tasks
CoGraph−MTL
 
 
10
20
30
40
50
5
10
15
20
25
30
35
40
45
50
1
2
3
4
5
6
7
8
9
10
tasks
tasks
CoGraph−MTL with l_1 penalty
 
 
10
20
30
40
50
5
10
15
20
25
30
35
40
45
50
1
2
3
4
5
6
7
8
9
10
FIGURE 5.3: Example of task similarity matrices on the school dataset:
(top-left) cluster-MTL; (top-right) metric-MTL inverse covariance task ma-
trix; (bottom-left) our CoGraph-MTL with ΩΘ; (bottom-right) our CoGraph-
MTL with ΩΘ−ℓ1.
to evaluate the statistical diﬀerence in performance between the two variants
of our method and the best performing competitor.
5.4.2.2
School Dataset
We have also tested our approach on the well known school dataset that is
available online and consists of predicting the examination score of students
from diﬀerent schools in London. This problem can be addressed as a multi-
task problem since diﬀerences between schools have to be taken into account,
for instance, by learning a prediction function per school. We refer the reader
to Argyriou et al. [1] for a more complete description of the data and focus
instead on the feature extraction we used. In their work, they have shown
that the tasks might share a common linear subspace. Hence, we took this
knowledge into account and performed a PCA on the whole dataset and kept
the 10 principal components out of 27 features. We learned the prediction
function of the 50 tasks having the largest number of examples. For each task,
we have an average number of ≈170 samples and we randomly selected 50

Learning Constrained Task Similarities in Graph-Regularized MTL
121
TABLE 5.1: Mean square error on the school dataset. p-value of a Wilcoxon
signrank test with respect to the performance of the best competitor as well
as the sparsity of the resulting task relation matrix are also reported.
Method
MSE
p-value
Sparsity (%)
Ridge Indep
118.27±2.97
-
-
Cluster-MTL
110.78±2.62
-
100.0
Metric-MTL
108.27±2.51
-
100.0
CoGraph-MTL
107.31±2.24
0.002
56.6
CoGraph-MTL ΩΘ−l1
107.32±2.24
0.002
55.8
of them for the training set, 50 for the validation set, and the remaining ones
for the test set.
Mean square error (MSE) obtained for all the described methods are re-
ported in Table 5.1, as well as the sparsity of resulting task similarity matrices.
We can note that the two variants of our approach achieve the lowest predic-
tion error and they are statistically better than the competitor according to
a Wilcoxon sign-rank test.
Learned similarity matrices are depicted in Figure 5.3. We can remark that
the matrices retrieved by the cluster-MTL and metric-MTL are rather dense
and present a very similar structure: large diagonal terms and nearly constant
oﬀdiagonal components. According to these matrices, we may conclude that
examination scores of one given school are related to those of all other schools.
The matrices learned by our method also have a similar structure but their
entries are more sparse. Hence, we can understand that a given school is
related only to few other ones. A deeper understanding of these similarities
may then be carried out if some more information, like geographical or social
aspects, was available about all the schools.
5.4.2.3
Brain Computer Interface Dataset
In this brain computer interface (BCI) problem, our objective is to rec-
ognize the presence of an event-related potential (ERP) in a recorded signal
during the use of a virtual keyboard. The dataset has been recorded by the
Neuroimaging Laboratory of Universidad Autonoma Metropolitana (UAM,
Mexico) [20] on 30 subjects performing P300 spelling tasks on a 6×6 virtual
keyboard. We consider each subject as a task and learn all classiﬁers simul-
taneously. For each subject, we have approximately 4000 single trial samples
that have been pre-processed according to the following steps: ﬁrst a low-pass
ﬁltering is applied to the 10 channel signals followed by a decimation; we
kept a 1s time window (6 temporal samples) following the stimulus as fea-
tures leading to trials containing 60 features [24]. Finally for each data split,
we randomly selected for each task 100 trials for the training set, 100 trials
for the validation set, and the remaining samples for the test set. Note that

122
Regularization, Optimization, Kernels, and Support Vector Machines
tasks
tasks
Cluster−MTL
 
 
5
10
15
20
25
30
5
10
15
20
25
30
3.34
3.35
3.36
3.37
3.38
3.39
3.4
3.41
3.42
3.43
tasks
tasks
Metric−MTL
 
 
5
10
15
20
25
30
5
10
15
20
25
30
−1
−0.5
0
0.5
1
x 10
5
tasks
tasks
CoGraph−MTL
 
 
5
10
15
20
25
30
5
10
15
20
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
tasks
tasks
CoGraph−MTL with l1 penalty
 
 
5
10
15
20
25
30
5
10
15
20
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FIGURE 5.4: Example of task similarity matrices on the BCI dataset: (top-
left) cluster-MTL; (top-right) metric-MTL inverse covariance task matrix;
(bottom-left) our CoGraph-MTL with ΩΘ; (bottom-right) our CoGraph-MTL
with ΩΘ−ℓ1.
for P300 classiﬁcation, since the datasets are highly imbalanced, we decided
to use the area under the ROC curve as a measure of performance. For this
classiﬁcation task, we have used a sigmoid function as the outer loss function.
We note in Table 5.2 that all multi-task learning approaches provided
statistically similar performance measures and they all perform far better than
a method that learns each task independently from all others. Interestingly, the
two variants of our method output similarity matrices that are considerably
sparse. Hence, for each subject, instead of considering that all other subjects
were similar to that one, they were able to retrieve a few others that provide
suﬃcient information for transfer learning. Figure 5.4 provides some examples
of learned task relation matrix for the diﬀerent algorithms. We can see there
how our method is able to extract relevant and interpretable information from
the data. Indeed, the task relation matrices retrieved by our two variants are
very sparse. It is thus possible to ﬁnd which BCI users are related.

Learning Constrained Task Similarities in Graph-Regularized MTL
123
TABLE 5.2: Area under the ROC curve (AUC) on the BCI dataset. p-
value of a Wilcoxon signrank test with respect to the performance of the best
competitor as well as the sparsity of the resulting task relation matrix are also
reported.
Method
AUC
p-value
Sparsity (%)
Ridge Indep
0.65±0.01
-
-
Cluster-MTL
0.78±0.00
-
100.0
Metric-MTL
0.78±0.01
-
100.0
CoGraph-MTL
0.78±0.00
0.625
6.9
CoGraph-MTL ΩΘ−l1
0.77±0.00
0.232
6.4
TABLE 5.3: Area under the ROC curve (AUC) on the OCR dataset. p-
value of a Wilcoxon signrank test with respect to the performance of the best
competitor as well as the sparsity of the resulting task relation matrix are also
reported.
Method
AUC
p-value
Sparsity
Ridge Indep
0.94±0.01
-
-
Cluster-MTL
0.96±0.01
-
100.0
Metric-MTL
0.98±0.01
-
100.0
CoGraph-MTL
0.96±0.01
0.002
11.1
CoGraph-MTL ΩΘ−l1
0.95±0.01
0.002
9.8
CoGraph-MTL ΩΘG
0.97±0.01
0.375
51.7
5.4.2.4
OCR Dataset
Finally we evaluate our approach on an OCR classiﬁcation problem. We
used the same OCR dataset as in the works of [22]. Here, the aim is to learn
a binary classiﬁer for each writer so as to take into account writer variability.
The main diﬃculty in this dataset is that we have only a few examples but
still want to learn robust classiﬁers.
We focus here on two binary classiﬁcation problems : “e” vs. “c” and
“a” vs. “g” for 20 diﬀerent writers. We want to learn simultaneously these
40 classiﬁcation tasks. The data is a raw bitmap of size 8 × 16 that was pre-
processed as follows. We performed PCA on the raw data and kept 20 principal
components. We randomly select 8 samples for the training set, 4 samples for
the validation set, and the remaining 8 for the test set. Moreover, in order to
prove how easy it is to add prior information in our approach, we integrated
group knowledge in the learning problem by imposing
speciﬁc constraints
on θ, denoted as ΩΘG. Indeed, we force a link between tasks from the same
binary classiﬁcation problem by imposing 0.01 ≤ρt,k ≤1000 (which is rather
a weak constraint).

124
Regularization, Optimization, Kernels, and Support Vector Machines
tasks
tasks
Cluster−MTL
 
 
5
10
15
20
25
30
35
40
5
10
15
20
25
30
35
40
−20
−10
0
10
20
30
tasks
tasks
Metric−MTL
 
 
5
10
15
20
25
30
35
40
5
10
15
20
25
30
35
40
−2
−1
0
1
2
3
4
5
6
7
x 10
4
tasks
tasks
CoGraph−MTL
 
 
5
10
15
20
25
30
35
40
5
10
15
20
25
30
35
40
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
tasks
tasks
CoGraph−MTL with l1 penalty
 
 
5
10
15
20
25
30
35
40
5
10
15
20
25
30
35
40
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
tasks
tasks
CoGraph−MTL with must−link constraints
 
 
5
10
15
20
25
30
35
40
5
10
15
20
25
30
35
40
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
FIGURE 5.5: Examples of learned task similarity matrices on the OCR
dataset: (top-left) cluster-MTL; (top-right) metric-MTL inverse covariance
task matrix; (middle-left) our CoGraph-MTL with ΩΘ; (middle-right) our
CoGraph-MTL with ΩΘ−ℓ1; (bottom) our CoGraph-MTL with ΩΘG.

Learning Constrained Task Similarities in Graph-Regularized MTL
125
Performance of the diﬀerent methods can be seen in Table 5.3. We remark
that our method using ΩΘG achieves equivalent performance with metric-MTL
whereas the two other variants of our method are slightly but signiﬁcantly
worse than the best performing competitor. Indeed, adding prior knowledge
helps in learning robust classiﬁers.
We should however emphasize that the metric-MTL approach is not able
to retrieve the correct relation between tasks as shown in Figure 5.5. Indeed,
the learned similarity matrix tells us that for metric-MTL all the tasks are
related. Cluster-MTL is able to infer that they form two speciﬁc clusters of
tasks in the problem. Our methods are also capable of detecting these clusters
and in addition yield sparse similarity matrices as very few relations between
tasks from two diﬀerent clusters were uncovered. Note that even though some
must-link constraints have been imposed when using ΩΘG as a regularizer,
some hyper-parameters have still been optimized by our algorithm.
5.5
Conclusion
We have proposed a novel framework for learning task similarities in multi-
task learning problems. Unlike other previous works on this topic, we learn
these similarities so as to optimize a proxy on the generalization errors of
all tasks. For this purpose, we introduce a bilevel optimization problem that
involves both the minimization of the generalization error and the optimiza-
tion of the task parameters. The global optimization is solved by means of a
non-convex proximal splitting algorithm, which is simple to implement and
can easily be extended to situations where diﬀerent constraints on task simi-
larities have to be imposed.
Experimental results on toy problems clearly show that the method we
propose can help in learning complex structures of task similarities. On real-
world problems, our method clearly achieves performances similar to other
multi-task learning algorithms that learn tasks similarities while providing
task similarity matrices that are far more interpretable.
Acknowledgments
This work has been partly supported by the French ANR Agency Grant
ANR-11-JS02-0010 Lemon.

126
Regularization, Optimization, Kernels, and Support Vector Machines
Appendix
Continuity and diﬀerentiability of E with respect to θk
First, note that β∗is implicitly deﬁned by the linear system Aβ = c.
Hence, we have
β = A−1c
Existence and unicity of β is guaranteed by invertibility of A since we have
imposed that λt > 0, ∀t. Hence, since A and c are both continuous and dif-
ferentiable with respect to θk, so are A−1 and each component of β. Using
similar arguments, we can show that each component of ˙βk = −A−1( ˙Akβ) is
continuous.
Continuity and diﬀerentiability of E(·) is easily obtained since E is built
from diﬀerentiability-preserving operations over diﬀerentiable functions.
Hessian Computation. In order to prove the gradient Lipschitz property
of E, we show that the Hessian of the function E(β(θ)) is bounded. The
Hessian is a matrix of general term:
∂E(β(θ))
∂θk∂θs
=
∂
∂θs
 d⊤(˙ck −˙Akβ)

(5.22)
=
−
∂d⊤
∂θs
( ˙Akβ) + d⊤˙Ak
∂β
∂θs

(5.23)
where we used the fact that c does not depend on θ so ˙ck = 0 and ˙Ak is
a constant matrix whose components are equal to 0 when diﬀerentiated. By
using the deﬁnition of d in Equation 5.18, it is easy to see that
∂d
∂θs
= (A⊤)−1
"
∂
∂θs
∇βE(β) −
∂A
∂θs
⊤
d
#
(5.24)
Now, we can note that all components of the Hessian general term (5.23)
are continuous with respect to θ. For instance, d is continuous since it is the
product of a continuous matrix A−1 and continuous vector ∇βE (by hypoth-
esis on the loss function). Similar arguments can be employed for showing
continuity of all other terms.
Expression of ˙Ak. The expression of this gradient takes diﬀerent forms
according to the type of hyper-parameter. Using Deﬁnition (5.10), we get the

Learning Constrained Task Similarities in Graph-Regularized MTL
127
expression
˙Ak =


0
. . .
0
. . .
0
. . .
0
...
. . .
...
. . .
...
. . .
...
0
. . .
4˜I
−4˜I
0
...
. . .
0
. . .
0
. . .
0
0
. . .
−4˜I
4˜I
. . .
0
...
. . .
0
. . .
0
. . .
...
0
. . .
0
. . .
0
. . .
0


,
for
θk = ρi,j, with j > i
and
˙Ak =


0
. . .
0
. . .
0
...
. . .
...
. . .
...
0
. . .
2˜I
0
...
...
. . .
0
. . .
...
0
. . .
0
. . .
0


,
for
θk = λt, with t = 1, . . . , T
Bibliography
[1] A. Argyriou, T. Evgeniou, and M. Pontil.
Convex multi-task feature
learning. Machine Learning, 73(3):243–272, 2008.
[2] Y. Bengio.
Gradient-based optimization of hyperparameters.
Neural
Computation, 12:1889–1900, 2000.
[3] K.P. Bennett, J. Hu, X. Ji, G. Kunapuli, and J.S. Pang. Model selec-
tion via bilevel optimization. In Neural Networks, International Joint
Conference on, pages 1922–1929, 2006.
[4] J. Bergstra and Y. Bengio. Random search for hyper-parameters opti-
mization. Journal of Machine Learning Research, 13:281–305, 2012.
[5] R. Caruana. Multi-task learning. Machine Learning, 28:41–75, 1997.
[6] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukerjhee. Choosing mul-
tiple parameters for SVM. Machine Learning, 46(1-3):131–159, 2002.
[7] B. Colson, P. Marcotte, and G. Savard. An overview of bilevel optimiza-
tion. Annals of operations research, 153(1):235–256, 2007.

128
Regularization, Optimization, Kernels, and Support Vector Machines
[8] P.L. Combettes and J.C. Pesquet. Proximal splitting methods in signal
processing. Fixed-Point Algorithms for Inverse Problems in Science and
Engineering, pages 185–212, 2011.
[9] T. Evgeniou and M. Pontil. Regularized multi-task learning. In Proceed-
ings of the Tenth Conference on Knowledge Discovery and Data Mining,
2004.
[10] Theodoros Evgeniou, Charles A. Micchelli, and Massimiliano Pontil.
Learning multiple tasks with kernel methods. Journal of Machine Learn-
ing Research, 6:615–637, 2005.
[11] S. Feldman, B. A. Frigyk, M. R. Gupta, L. Cazzanti, and P. Sadowski.
Multi-task output space regularization. arXiv, 2011.
[12] T. Van Gestel, J.A.K. Suykens, B. Baesens, S. Viaene, J. Vanthienen,
G. Dedene, B. De Moor, and J. Vandewalle. Benchmarking least squares
support vector machine classiﬁers. Machine Learning, 54(1):5–32, 2004.
[13] Tony Van Gestel, Johan A. K. Suykens, Gert R. G. Lanckriet, Annemie
Lambrechts, Bart De Moor, and Joos Vandewalle. Bayesian framework
for least-squares support vector machine classiﬁers, Gaussian processes,
and kernel Fisher discriminant analysis. Neural Computation, 14(5):1115–
1147, 2002.
[14] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical
Learning. Springer-Verlag, 2001.
[15] L. Jacob, F. Bach, and J.-P. Vert. Clustered multi-task learning: A con-
vex formulation. In Advances in Neural Information Processing Systems,
NIPS, 2008.
[16] Zhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom
to share in multi-task feature learning. In Proceedings of the 28th ICML,
pages 521–528. ACM, June 2011.
[17] T. Kato, H. Kashima, M. Sugiyama, and K. Asai. Multi-task learning
via conic programming. In Advances in Neural Information Processing
Systems, 2008.
[18] S. Sathiya Keerthi, Vikas Sindhwani, and Olivier Chapelle. An eﬃcient
method for gradient-based adaptation of hyperparameters in svm models.
In Advances in Neural Information Processing Systems 19, pages 673–680.
MIT Press, 2007.
[19] A. Kumar and H. Daumé III. Learning task grouping and overlap in
multi-task learning. In Proceedings of the International Conference on
Machine Learning, 2012.

Learning Constrained Task Similarities in Graph-Regularized MTL
129
[20] Claudia Ledesma-Ramirez, Erik Bojorges Valdez, Oscar Yáñez Suarez,
Carolina Saavedra, Laurent Bougrain, and Gerardo Gabriel Gentiletti.
An Open-Access P300 Speller Database. In Fourth International Brain-
Computer Interface Meeting, 2010.
[21] A. Maurer, M. Pontil, and B. Romera-Paredes. Sparse coding for multi-
task and transfer learning. In Proceedings of the International Conference
on Machine Learning, 2013.
[22] Guillaume Obozinski, Ben Taskar, and Michael I. Jordan. Joint covariate
selection and joint subspace selection for multiple classiﬁcation problems.
Statistics and Computing, 20:231–252, April 2010.
[23] A. Rakotomamonjy, R. Flamary, G. Gasso, and S. Canu. lp-lq penalty
for sparse linear and sparse multiple kernel multi-task learning, IEEE
Transactions on Neural Networks, 22(8):1307–1320, 2011.
[24] A. Rakotomamonjy and V. Guigue. BCI competition III: Dataset II -
ensemble of SVMs for BCI P300 speller. IEEE Trans. Biomedical Engi-
neering, 55(3):1147–1154, 2008.
[25] R. Rifkin, G. Yeo, and T. Poggio. Regularized least squares classiﬁcation.
In Advances in Learning Theory : Methods, Model and Applications, pages
131–153. IOS Press, 2003.
[26] B. Romera-Paredes, A. Argyriou, N. Bianchi-Berthouze, and M. Pontil.
Exploiting unrelated tasks in multi-task learning. In JMLR Proceeding
track, volume 22, pages 951–959, 2012.
[27] B. Romera-Paredes, M. Hane Aung, N. Bianchi-Berthouze, and M. Pon-
til. Multilinear multitask learning. In Proceedings of the International
Conference on Machine Learning, 2013.
[28] Kim Seyoung and Eric P. Xing. Tree-guided group lasso for multi-task
regression with structured sparsity. In ICML, pages 543–550, 2010.
[29] S. Sra. Nonconvex proximal splitting: batch and incremental algorithms.
In Advances in Neural Information Processing Systems (NIPS), 2012.
[30] J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and
J. Vandewalle. Least squares support vector machines. World Scientiﬁc,
2002.
[31] S. V. N. Vishwanathan, A. J. Smola, and M. Murty. SimpleSVM. In
International Conference on Machine Learning, 2003.
[32] C. Widmer, M. Kloft, N. Goernitz, and G. Raetsch. Eﬃcient training of
graph-regularizer multi-task svm. In Proceedings of the European Con-
ference on Machine Learning (ECML), 2012.

130
Regularization, Optimization, Kernels, and Support Vector Machines
[33] Christian Widmer, Nora C. Toussain, Yasemin Altun, and Rätsch Gun-
nar. Inferring latent task structure for multitask learning by multiple
kernel learning. BMC Bioinformatics, 11(Suppl 8):S5, 2010.
[34] Y. Zhang and D.Y. Yeung. A convex formulation for learning task re-
lationships in multiple task learning. In Proceedings of Uncertainty and
Artiﬁcial Intelligence, 2010.
[35] L. Zhong and J. Kwok.
Convex multitask learning with ﬂexible task
clusters. In Proceedings of the 29th International Conference on Machine
Learning (ICML), 2012, 2012.

Chapter 6
The Graph-Guided Group Lasso for
Genome-Wide Association Studies
Zi Wang
Imperial College London
Giovanni Montana
King’s College London
6.1
Introduction ......................................................
131
6.2
Method ...........................................................
134
6.2.1
The Graph-Guided Group Lasso (GGGL) ..............
134
6.2.2
Properties ................................................
137
6.2.2.1
GGGL-1: Smoothing Eﬀect ................
137
6.2.2.2
GGGL-2: Smoothing Eﬀect ................
140
6.3
Estimation Algorithms ...........................................
142
6.3.1
Serial Block Coordinate Descent Algorithm ............
142
6.3.1.1
Serial Algorithm for GGGL-1 ..............
142
6.3.1.2
Serial Algorithm for GGGL-2 ..............
145
6.3.2
Parallel Coordinate Descent Algorithm .................
145
6.4
Simulation Studies ...............................................
149
6.5
Conclusion ........................................................
152
Bibliography ......................................................
153
6.1
Introduction
Genetic variation in human DNA sequences can cause alterations in an
individual’s traits, which may be indicators of disease status or quantitative
measures of disease risk or physical properties. The associations between ge-
netic variations and human traits have traditionally been investigated through
inheritance studies in families [9]. While this has proved useful for single
gene disorders, the associations identiﬁed with “complex” diseases involving
multiple genetic determinants are hard to reproduce [1]. In the last decade,
the number of genome-wide association studies (GWAs) has increased im-
mensely [14, 10, 7, 19]. GWAs search common genetic variants across the
human genome in unrelated individuals that are associated with a trait. They
131

132
Regularization, Optimization, Kernels, and Support Vector Machines
have been widely applied and have succeeded in identifying reproducible as-
sociations with thousands of human diseases and traits [18]. In this work we
assume the genetic variants are single-nucleotide polymorphisms (SNPs), and
we develop a statistical method to identify SNPs associated to a univariate
trait. We refer to the SNPs/genes having a non-random association with the
trait as “causal SNPs/genes”, which are unknown and to be inferred.
Conventional GWAs involve a case-control study design, in which the sub-
jects are partitioned into case and control groups, and hypothesis tests (e.g.,
based on χ2 statistic) are carried out for each SNP to examine if the frequen-
cies of the genotypes (commonly denoted as aa, Aa, and AA) are signiﬁcantly
altered between the case and control groups [5, 6]. Some limitations of this
approach include: substantial estimation biases and spurious associations due
to violation of the assumptions in case-control design [28]; insuﬃcient sam-
ple size in comparison to the number of variables, typically sample sizes vary
between a few hundred and a few thousand, whereas the number of SNPs to
be investigated is about 1 million and may go beyond 10 million [6, 24]; and
multiple testing issues [28].
In this work, we let the “trait of interest” be a continuous random variable,
and formulate the variable selection problem in a linear regression context. We
deﬁne the SNPs as predictors and the trait as a univariate response, thereby
the strength of associations can be quantiﬁed by the coeﬃcients from the
linear regression model. When the number of subjects is smaller than the
number of SNPs, the coeﬃcients can be estimated by minimizing the empirical
loss function (the squared loss). However, in GWAs, the number of SNPs
greatly exceeds the number of subjects, which raises computational issues
in coeﬃcients estimation. Many regularized methods have been proposed to
tackle this problem, which involve adding a penalty term of the coeﬃcient
vector to the objective function. Taking the ℓ1 norm of the coeﬃcient vector
as the penalty term, amongst others, some entries in the estimated coeﬃcients
are set to zero and a set of important SNPs can be automatically selected that
correspond to the non-zero entries [38, 3, 15, 33, 27]. We refer to such models
as penalized linear regression models.
There appears to be two trends in the recent development of the penalized
linear regression models for GWAs. The ﬁrst is developing fast computation
algorithms that can be applied to “big data” problems [32, 31], where the
sample size is as large as 200, 000 or beyond [11]; the second trend is devel-
oping new penalties to impose biologically informed sparsity patterns. The
motivation is that the variability in disease related traits often arises from the
joint contribution from multiple loci within a gene or the joint action of genes
within a pathway [22, 34]. Although some genetic variants may only facilitate
a moderate or weak eﬀect on the trait individually, their joint eﬀect could
be inﬂuential. Unfortunately, the set of SNPs extracted by purely data-driven
methods often account for only a small proportion of the genetic variants
associated with the trait, which may provide very limited insight into the bi-
ological mechanisms underpinning the complex disease [23]. One promising

The Graph-Guided Group Lasso for Genome-Wide Association Studies 133
approach involves incorporating prior biological information on the functional
relations between the genetic variants to guide the selection of causal predic-
tors. The hope is that the penalty function acting on this information will
encourage the selection of co-functioning genetic variants, hence producing a
set of important predictors explanatory of the variability in the trait as well
as biologically plausible [41, 39, 36, 35].
Structured knowledge on co-functioning SNPs and genes can be organized
in many ways. One of these is partitioning the candidate SNPs into non-
overlapping groups according to the gene-SNP mapping, where each group
corresponds to a set of SNPs that constitute a gene or a pathway. Using the
group lasso penalty [42], the model can identify a set of important genes or
pathways that explain the variability in the disease related trait [43, 40]. Using
the sparse group lasso penalty, the most inﬂuential SNPs within the important
genes or pathways can be extracted, which may give further insight of the
biological process [13]. When groups overlap, for instance when an SNP is
mapped to more than one gene, the set of selected SNPs belong to the union
of some important genes [17, 36, 34].
Another way to organize the structured knowledge of the co-functioning
genes is to present the pairwise relations in a network. Typically, the nodes
represent genes (or proteins), and two nodes are connected by an edge if
the associated genes belong to the same genetic pathway or share related
functions [37]. In previous GWAs, such networks were often used after the set
of important genes had been selected and to examine the roles of these genes in
the biological process [20, 30]. Nonetheless, the reverse engineering that uses
the gene network as prior information to guide the selection of interacting
genes has also been exploited. The selected genes often correspond to densely
connected regions in the given network and this has been shown to improve
the accuracy of variable selection [21, 2, 29].
In this work we consider the case where heterogeneous prior information
is available for GWAs: SNPs are grouped into genes, and genes are organized
into a weighted gene network encoding the functional relatedness between all
pairs of genes. We believe that integrating prior knowledge from the two lev-
els will lead to improved variable selection accuracy of genes and SNPs while
facilitating biologically informed models. We propose a penalized regression
model, the graph-guided group lasso (GGGL), which selects functionally re-
lated genes and SNPs within these genes that are associated with a quantita-
tive trait. To the best of our knowledge, this is the ﬁrst model in GWAs that
uses graph and grouping structure on hierarchical biological variants to drive
variable selection at multiple levels.
The rest of this chapter is organized as follows: In Section 6.2 we present
two versions of the GGGL models, which diﬀer on the integration of structured
information at heterogeneous levels, and we show their theoretical properties;
in Section 6.3 we present a serial and a parallel computation algorithm for
each version of GGGL; we study the power of variable and group selection

134
Regularization, Optimization, Kernels, and Support Vector Machines
of the GGGL models using simulated data in Section 6.4; a brief conclusion
summarizing our contribution is given in Section 6.5.
6.2
Method
6.2.1
The Graph-Guided Group Lasso (GGGL)
Let X be the n × p design matrix containing n independent samples for
which p SNPs have been observed, and y be the n-dimensional vector con-
taining the univariate quantitative trait. We normalize the columns of X to
have zero sum and unit length and center y, such that the predictors are scaled
and the intercept term can be dropped from the linear regression model which
seeks coeﬃcient vector β to minimize the squared loss: ∥y −Xβ∥2
2. In a pe-
nalized linear regression model, the estimated coeﬃcients ˆβ is obtained by
minimizing:
1
2∥y −Xβ∥2
2 + P(β)
where P(β) in (6.1) is a function of β, called “the penalty function”. For
example, P(β) = ∥β∥1 is the “lasso penalty” [38].
Suppose that the SNPs are grouped into mutually exclusive genes R =
{R1, R2, ...}, in which each element RI is a set of SNP indexes. The size of
gene RI is denoted by |RI|. We let XI denote the n × |RI| sub-matrix of
X, where the columns correspond to SNPs in RI, and let Xi denote the ith
column of X. Let G = G(V, E) be the gene network with vertex set V corre-
sponding to the |R| genes in R. We shall use the terms “network” and “graph”
interchangeably in the following. For convenience, when referring to the node
in G corresponding to the gene RI, we shall write I for short. The weight of
the edge connecting I and J is denoted by wIJ, which can be either binary or
continuous. In the case it is continuous, the magnitude measures the strength
of the relatedness between two genes: a larger magnitude indicates the two
genes are more likely to involve in the same biological process. For simplicity,
we assume that all weights are non-negative. The network G may consist of
several disjoint subgraphs, whose vertex set and edge set are subsets of V (G)
and E(G), respectively. Each of these disjoint subgraphs is called a “compo-
nent”. For the GGGL, regression coeﬃcients are obtained by minimizing the
least square loss plus a composite penalty term:
1
2∥y −Xβ∥2
2 + P1(β) + P2(β) + P3(β)
(6.1)
where:
P1(β) = λ1
|R|
X
K=1
p
|RK|∥βK∥2,
P2(β) = λ2∥β∥1
(6.2)

The Graph-Guided Group Lasso for Genome-Wide Association Studies 135
FIGURE 6.1: Illustration of the sparsity patterns allowed by the GGGL
model: SNPs are grouped into genes, and the pairwise relations between the
genes are represented by a network. The model selects genes that explain
the variance observed in the response variable and the most inﬂuential SNPs
within these genes. The network structure is accounted such that the model
encourages the selection of genes that are connected in the network.
P3(β) = 1
2µ
X
i∈RI,j∈RJ,I∼J
wIJ(βi −βj)2
(6.3)
the constants λ1, λ2, and µ are non-negative regularization parameters, and
I ∼J if and only if there is an edge between nodes I and J in G. In P1,
βK denotes the sub-vector extracted from the RK
th entries of β, while the ℓ2
norm measures the magnitude of the eﬀect on y from each gene. The sum of
ℓ2 norms is referred as an ℓ1/ℓ2 norm, which induces sparsity at genes’ level
[42]. The number of selected genes is controlled via λ1: as it increases, less
genes are included in the model. The scaling factor
p
|RK| is added to correct
the bias towards selecting genes consisting of a large number of SNPs. The ℓ1
norm penalty P2 is added to enable the model to identify the most inﬂuential
SNPs within the selected genes [13]. When we are only interested in selecting
SNP-sets rather than SNPs, we let λ2 = 0 and no sparsity within the genes
is imposed. The sparsity pattern imposed by P1 + P2 in (6.1) typically looks
like:
ˆβ = ([0.2, 0, 0], [0, 0, 0, 0], [0, 0.1], ...)
where coeﬃcients in “[ ]” indicate SNPs belonging to the same gene. Applying
P1 and P2 alone, the solution to (6.1) is solely determined by data matrices X
and y, ignoring the rich structured information between the interacting genes.
This prior knowledge is accounted for by P3 in (6.3), which is a standard

136
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 6.2: Insight of the graph penalty in GGGL-1: If I ∼J, then penalty
P3 in (6.3) is equivalent to reformulating G by a composite bipartite graph
with vertex sets I and J. All edge weights in this bipartite graph are set to
wIJ.
Laplacian penalty penalizing the squared diﬀerence between all related SNPs.
Two SNPs i and j are “related” if the genes they belong to are connected
in the given network G. The eﬀect of the standard Laplacian penalty is to
smooth the coeﬃcient estimates ˆβi and ˆβj towards a value between the two,
known as “coeﬃcient smoothing”. In particular, if ˆβi > 0 and ˆβj = 0 when
the composite penalty involves P1 and P2 only, introducing P3 will drive ˆβj
towards a nonzero value, and consequently encourages the selection of both
genes that the two SNPs belong to [21, 4]. The strength of this coeﬃcient
smoothing is controlled by µ: When µ is large, we have a stronger belief that
the given network truly describes the underlying biological process and that
the selected genes should be densely clustered in G. When µ is suﬃciently
large, coeﬃcients associated to SNPs from interacting genes tend to obtain
the same value, in which case the coeﬃcients reﬂect the averaged eﬀect from
the multiple components in G.
Note that through P3, we make use of structured information at the genes
level to guide the selection of genes and SNPs, which involves integrating infor-
mation at heterogeneous levels. The choice of P3 deﬁned in (6.3) is equivalent
to a re-formulation of G from the genes level to the SNPs level. This is done
by constructing a complete bipartite graph K(I, J) on the vertex set I ∪J
whenever there is an edge between genes RI and RJ in G, as illustrated in Fig-
ure 6.2. A side eﬀect of this is that coeﬃcients of SNPs belonging to the same
gene are also smoothed, thus the estimated coeﬃcients of causal SNPs and
non-causal SNPs tend to have similar values, which may increase the chances
of selecting non-causal SNPs within the selected genes.
We also propose a second version of the GGGL. This version, called GGGL-
2, addresses this problem arisen when imposing sparsity within the selected
genes. In GGGL-2, P3 is replaced by:
P3(β) = 1
2µ
X
I∼J
wIJ(¯βI −¯βJ)2
(6.4)
where ¯βI denotes the average coeﬃcient of variables in RI. We further require

The Graph-Guided Group Lasso for Genome-Wide Association Studies 137
the coeﬃcients to be non-negative, so that the averaged coeﬃcients can repre-
sent the average eﬀect of the grouped variables. Intuitively, if the group RJ is
selected and ¯ˆβJ ̸= 0, then for its neighbouring group RI, ¯ˆβI will be smoothed
towards a nonzero value and in this way the model encourages the selection
of the group RI. This coeﬃcient smoothing on averaged coeﬃcients will be
formally presented in the next subsection. In addition, we show the choice of
P3 in (6.4) does not smooth the coeﬃcients of the SNPs belonging to the same
gene, which is the primary diﬀerence between the two versions of the GGGL
model.
6.2.2
Properties
We shall present properties regarding the smoothing eﬀect of the two versions
of GGGL, diﬀered by the choice of P3 in (6.3) and (6.4), respectively. By
smoothing eﬀect we refer to the shrinkage of the diﬀerence between a pair
of coeﬃcient estimates toward the diﬀerence between the weighted average of
their respective neighbors, as the regularization parameter µ increases [16].
This further indicates the genes interacting with a large common set of genes
have similar magnitude of coeﬃcients, and are encouraged to be selected to-
gether. Since the group lasso penalty P1 + P2 induces no smoothing eﬀect, it
is only necessary to consider P3 alone.
6.2.2.1
GGGL-1: Smoothing Eﬀect
In this subsection we show GGGL-1 imposes a smoothing eﬀect on ˆβi and
ˆβj as long as the associated genes of ith and the jth SNPs belong to the same
component in G. Thus the coeﬃcients are smoothed both within and between
interacting SNP-sets.
Proposition 6.1. Given data matrices X and y, where X is column-wise
normalized and y is centered, let i ∈RI and j ∈RJ and assume RI and RJ
belong to the same component in the given network G. For ﬁxed µ, let ˆβ be
the vector that minimizes:
L1(β) := ∥y −Xβ∥2
2 + µ
X
k,l:k∈RK,l∈RL,K∼L
wKL(βk −βl)2
Deﬁne the following:
ρij = XT
i Xj
CI =
X
K∼I
wIK|RK|
ΓI =
P
k∈RK,K∼I wIK ˆβk
CI
Dµ(i, j) = |(ˆβi −ˆβj) −(ΓI −ΓJ)|
(6.5)

138
Regularization, Optimization, Kernels, and Support Vector Machines
Then:
Dµ(i, j) ≤∥y∥2
µ
p
2(1 −ρij)
CI
+

1
CI
−1
CJ


.
(6.6)
To prove (6.6), we need the following lemmas:
Lemma 6.1. Let a, b, c, d ∈R, and b, d ̸= 0, the following inequality holds:

a
b −c
d
 ≤

a −c
b
 + |c| ·

1
b −1
d
 .
Proof.

a
b −c
d

=

ad −bc + cd −cd
bd

=

ad −cd
bd
+ cd −bc
bd

≤

a −c
b
 + |c| ·

1
b −1
d
 .
Lemma 6.2. Let X be an n × p matrix where each column is normalized
to have zero mean and unit length. Let y be an n-dimensional vector that is
centered. Let ˆr = y−X ˆβ and assuming ∥ˆr∥2 ≤∥y∥2, the following inequalities
hold:
|(XT
i −XT
j )ˆr| ≤
q
2(1 −ρij)∥y∥2
|XT
i ˆr| ≤∥y∥2
The proofs were given in [44].
Now we give the proof for Proposition 6.1:
Proof. Suppose i ∈RI, solving ∂L1
∂βi = 0 gives:
−2XT
i ˆr + 2µ ·

X
k: k∈RK,
K∼I
wIK

|
{z
}
CI
·ˆβi −2µ ·

X
k: k∈RK,
K∼I
wIK ˆβk

|
{z
}
ΓI·CI
= 0
where ˆr = y −X ˆβ. Rearranging to give:
ˆβi = XT
i ˆr
µCI
+ ΓI
Similarly for j ∈RJ, we have:
ˆβj = XT
j ˆr
µCJ
+ ΓJ

The Graph-Guided Group Lasso for Genome-Wide Association Studies 139
Thus:
|(ˆβi −ˆβj) −(ΓI −ΓJ)| =

XT
i ˆr
µCI
−XT
j ˆr
µCJ
 .
Applying Lemma 1 and 2, we can obtain an upper bound for the right hand
side of (6.6):

XT
i ˆr
µCI
−XT
j ˆr
µCJ

≤

(XT
i −XT
j )ˆr
µCI
 + |XT
j ˆr| ·

1
µCI
−
1
µCJ

by Lemma 1
≤
p
2(1 −ρij)∥y∥2
µCI
+ ∥y∥2
µ
·

1
CI
−1
CJ

by Lemma 2.
Remark 6.1. Proposition 6.1 does not require I ∼J, and it also holds when
RI = RJ, namely i and j belong to the same gene.
Note CI depends on the topology of G and gene size only, therefore CI
and CJ are constants. ΓI is the weighted average estimated coeﬃcients of the
SNPs whose associated genes are connected with gene RI in G. The quantity
Dµ(i, j) measures the deviation between the diﬀerence of ˆβi and ˆβj and the
diﬀerence of the weighted average estimated coeﬃcients of their corresponding
neighbors. In other words, it measures the discrepancy between the centered
coeﬃcients at the ith and jth predictor. When we have a strong belief that
interacting genes and the corresponding SNPs have similar functions, we would
expect smoother coeﬃcients of the interacting genes and smaller Dµ(i, j) as
µ increases. Letting µ tend to inﬁnity in (6.6), we indeed have D∞(i, j) →0,
thus proved the smoothing eﬀect from GGGL-1.
In the case where i and j belong to the same gene such that RI = RJ, we
can deduce the following:
Corollary 6.1. Under the same setting of Proposition 6.1 and assuming i
and j belong to the same gene RI, deﬁne the partial residual:
ˆrij = y −X ˆβ + Xi ˆβi + Xj ˆβj
(6.7)
the estimated coeﬃcients ˆβ satisfy:
|ˆβi −ˆβj| = (XT
i −XT
j )ˆrij
1 −ρij + µCI
.
(6.8)
Moreover, the left hand side of (6.8) is bounded above by:
p
2(1 −ρij)∥y∥2
µCI
.
(6.9)
From (6.9), it can be observed when µ increases, the coeﬃcient estimates
of all SNPs in the same gene are pushed towards the same value, possibly
making it more diﬃcult to identify the causal SNPs within the selected gene.

140
Regularization, Optimization, Kernels, and Support Vector Machines
6.2.2.2
GGGL-2: Smoothing Eﬀect
In this subsection we show GGGL-2 imposes a smoothing eﬀect on the
average coeﬃcients ¯βI and ¯βJ provided RI and RJ belong to the same com-
ponent in G. However, it does not impose any smoothing eﬀects on the coef-
ﬁcients of SNPs within the same gene.
Proposition 6.2. Given data matrices X and y, where X is column-wise
normalized and y is centered, let i ∈RI and j ∈RJ and assume RI and
RJ belong to the same component in the given network G. Denote the vertex
degree of RI in G by dI, and let ¯βI =
1
|RI|
P
i∈RI βi. For ﬁxed µ, let ˆβ be the
vector that minimizes:
L2(β) := ∥y −Xβ∥2
2 + µ
X
K∼L
wKL(¯βK −¯βL)2 .
(6.10)
Deﬁne the following:
ΘI =
X
K∼I
wIK
dI
¯ˆβK ,
and
Dµ(I, J) = |(¯ˆβI −¯ˆβJ) −(ΘI −ΘJ)|
(6.11)
Then:
Dµ(I, J) ≤∥y∥2
µ
2|RI|
dI
+

|RI|
dI
−|RJ|
dJ


.
(6.12)
Proof. Suppose i ∈RI, solving ∂L2
∂βi = 0 gives:
−2XT
i ˆr + 2µ
X
I∼K
wIK
1
|RI|(¯βI −¯βJ) = 0 .
Rewriting the summation of the diﬀerences as the diﬀerence of two sums:
−XT
i ˆr +
µ
|RI| ·
 X
K∼I
wIK

|
{z
}
dI
·¯βI −
µ
|RI| ·
 X
K∼I
wIK ¯βK

|
{z
}
ΘI·dI
= 0
Rearrange the above equation to give:
¯βI = XT
i ˆr
µdI
|RI|
+ ΘI
and similarly for j ∈RJ we have:
¯βJ = XT
j ˆr
µdJ
|RJ|
+ ΘJ .

The Graph-Guided Group Lasso for Genome-Wide Association Studies 141
Hence, we can derive an upper bound for the left hand side of (6.12):
|(¯βI −¯βJ) −(ΘI −ΘJ)|
=

XT
i ˆr
µdI
|RI|
−XT
j ˆr
µdJ
|RJ|

≤
|(XT
i −XT
j )ˆr|
µdI
|RI|
+ |XT
j ˆr| ·

|RI|
µdI
−|RJ|
µdJ

by Lemma 1
≤
p
2(1 −ρij)∥y∥2 · |RI|
µdI
+ ∥y∥2
µ
·

|RI|
dI
−|RJ|
dJ

by Lemma 2
=
∥y∥2
µ
p
2(1 −ρij) · |RI|
dI
+

|RI|
dI
−|RJ|
dJ


≤
∥y∥2
µ
2|RI|
dI
+

|RI|
dI
−|RJ|
dJ


.
Remark 6.2. Proposition 6.2 does not require I ∼J, and it also holds when
RI = RJ, namely i and j belong to the same gene.
Note ΘI is a weighted mean of the coeﬃcients of the SNPs that are func-
tionally related to the gene RI, and Dµ(I, J) measures the strength of the
smoothing eﬀect on the averaged coeﬃcient estimates for the genes. The up-
per bound obtained in (6.12) is determined by the data, the network topology,
and the gene size, for ﬁxed µ. Letting µ →∞, we have D∞(I, J) →0, thus
having proved the smoothing eﬀect from GGGL-2. In the case where i and j
belong to the same gene such that RI = RJ, the following corollary can be
deduced following the same proof line as Proposition 6.2.
Corollary 6.2. Under the same setting of Proposition 6.2 and assuming i
and j belong to the same gene, recalling the partial residual deﬁned in (6.7),
the estimated coeﬃcients ˆβ satisfy:
|ˆβi −ˆβj| = |(XT
i −XT
j )ˆrij|
1 −ρij
.
(6.13)
The right hand side of (6.13) does not involve µ, which means |ˆβi −ˆβj|
is data-driven, via a trade-oﬀbetween the loss function and graph penalty in
(6.10). We conclude that the graph penalty does not enforce the coeﬃcient
estimates of SNPs within the same gene to take the same value.

142
Regularization, Optimization, Kernels, and Support Vector Machines
6.3
Estimation Algorithms
In this section we present model estimation algorithms for the two versions
of the GGGL model, based on block coordinate descent methods. The ﬁrst
is a serial version that updates the coeﬃcients one block (gene) at a time.
The second is a parallel algorithm that updates a subset of the blocks (i.e.,
multiple genes) in parallel in each step. This enables the program to be run
independently on multiple processors (e.g., graphics processing units), so that
the amount of computation required by each processor is reduced and thus
speeds up the computation. The parallel algorithm is suitable for “big data”
in which both n and p are large.
6.3.1
Serial Block Coordinate Descent Algorithm
Note both versions of the GGGL models seek to minimise (6.1) in which
sparsity penalties are deﬁned in (6.2) and graph penalties are deﬁned in (6.3)
and (6.4), respectively. Note when µ = 0 and thus P3 = 0, the objective
function reduces to that of the sparse group lasso, which can be eﬃciently
solved using a block coordinate descent algorithm [13]. We thus attempt to
re-formulate (6.1) into a sparse group lasso problem and apply the existing
eﬃcient algorithm to solve it.
6.3.1.1
Serial Algorithm for GGGL-1
By deﬁnition, the summation part in (6.3) can be rewritten as:
X
i∈RI,j∈RJ,I∼J
wIJ(βi −βj)2 =
X
i≤j
wij(βi −βj)2
(6.14)
where wij is deﬁned as:
wij =

0
if
i and j belongs to the same gene
wIJ
if
i ∈RI, j ∈RJ ̸= RI
(6.15)
Let L be a p × p matrix whose (i, j)th entry is:
(L)ij =

di
if
i = j ∈RI
−wij
if
i ̸= j
(6.16)
where di = Pp
j=1 wij. Using L, the right hand side of (6.14) can be re-
formulated into: [4]
X
i≤j
wij(βi −βj)2 = βT Lβ .
Up to this point, we have:
∥y −Xβ∥2
2 + µ
X
i∈RI,j∈RJ,I∼J
wIJ(βi −βj)2 = ∥y −Xβ∥2
2 + µβT Lβ . (6.17)

The Graph-Guided Group Lasso for Genome-Wide Association Studies 143
By construction, L is positive semi-deﬁnite, therefore we can ﬁnd p×p matrix
U such that: L = UU T , using for instance, Cholesky decomposition. We then
construct the (n + p) × 1 matrix y∗and the (n + p) × p matrix X∗by:
y∗=
 yn×1
0p×1

,
X∗=

X
√µU T

.
(6.18)
Consequently, (6.17) can be re-formulated into: ∥y∗−X∗β∥2
2, as in [8, 21, 44].
The objective function (6.1) with penalties (6.2) (6.3) can be re-written as:
∥y∗−X∗β∥2
2 + 2λ1
|R|
X
K=1
p
|RK|∥βK∥2 + 2λ2∥β∥1
(6.19)
where βK is the |RK| × 1 matrix containing the entries corresponding to vari-
ables in RK only. Note (6.19) is exactly the objective function for the sparse
group lasso [13] on data matrices (X∗, y∗), which can be eﬃciently solved
by the block coordinate descent algorithm. The full procedure is given in
Algorithm 7. We remark that the one-dimensional optimization problem cor-
responding to steps 12 to 15 in Algorithm 7 can be dealt by other methods,
for example, the successive parabolic interpolation, as done in [13].

144
Regularization, Optimization, Kernels, and Support Vector Machines
Algorithm 7 GGGL Version 1
Input: data X, y; parameter λ1, λ2, µ; partition of predictors R, which con-
sists of |R| groups; weight matrix on groups wIJ; starting value ˜β; ϵ
Output: column vector ˆβ
1:
Deﬁne Sλ(z) := σ(z) · max{|z| −λ, 0}, where σ(z) = 1 if z > 0,
−1 if z < 0, 0 if z = 0.
2:
Compute p × p weight matrix wij according to (6.15).
3:
Compute L from (6.16).
4:
Use Cholesky decomposition to compute U such that L = UU T .
5:
Compute X∗and y∗deﬁned by (6.18).
6:
β0 ←˜β
7:
for I in 1:|R| do
8:
rI ←y∗−P
K̸=I X∗
K ˜βK
9:
if ∥Sλ2(X∗
I rI)∥2 ≤λ1|RI|
then
10:
ˆβI = 0
11:
else
12:
for i in RI do
13:
ri ←rI −P
j:j∈RI/{i} X∗
j ˜βj
14:
ˆβi ←(1 −
λ1√
|RI|
qP
j:j∈RI (Sλ2(X∗T
j rj))2 ) · Sλ2(X∗T
i ri)
X∗T
i X∗
i
15:
end for
16:
if ∥ˆβI −˜βI∥2 ≤ϵ
then
17:
Return: ˆβI
18:
else
19:
˜βI ←ˆβI; go back to 12
20:
end if
21:
end if
22:
end for
23:
if ∥ˆβ −β0∥2 ≤ϵ
then
24:
Return: ˆβ
25:
else
26:
β0 ←ˆβ; go back to 7
27:
end if

The Graph-Guided Group Lasso for Genome-Wide Association Studies 145
6.3.1.2
Serial Algorithm for GGGL-2
Considering the graph penalty (6.4) in GGGL-2, we notice:
wIJ(¯βI −¯βJ)2
=
wIJ
 1
|RI|
X
i:i∈RI
βi −
1
|RJ|
X
j:j∈RJ
βj
2
by deﬁnition
=
wIJ
|RI|2
X
i:i∈RI
β2
i + wIJ
|RJ|2
X
j:j∈RJ
β2
j
+ wIJ
|RI|2
X
i:i∈RI
X
s:s∈RI\{i}
βiβs
+ wIJ
|RJ|2
X
j:j∈RJ
X
t:t∈RJ\{j}
βjβt
−
2 · wIJ
|RI| · |RJ|
X
i:i∈RI
X
j:j∈RJ
βiβj
(6.20)
after expanding the brackets.
From (6.20) it can be deduced that for i ∈RI, the terms involving βi in
P
I∼J wIJ(¯βI −¯βJ)2 are:
 X
J:J∼I
wIJ
|RI|2

β2
i + 2
 X
J:J∼I
wIJ
|RI|2

X
s:s∈RI\{i}
βiβs
−2
X
J:J∼I

wIJ
|RI| · |RJ|
 X
j:j∈RJ
βiβj

.
It is then straightforward to deduce that there exists a p × p matrix L such
that P
I∼J wIJ(¯βRI −¯βRJ)2 = βT Lβ, where L is deﬁned as:
(L)ij =
( P
{K:K∼I}
wIK
|RI|2
if
i ∈RI, j ∈RI
−
wIJ
|RI|·|RJ|
if
i ∈RI, j ∈RJ
(6.21)
Then solution to GGGL-2 can be computed according to Algorithm 7, re-
placing steps 2 and 3 by (6.21). The non-negative coeﬃcients constraint in
GGGL-2 is tackled by further requiring ˆβi in step 14 to be non-negative.
Thus if a negative estimate is obtained, the corresponding entry in ˆβ is set to
zero.
6.3.2
Parallel Coordinate Descent Algorithm
In this subsection we describe the implementation of a parallel algorithm
that can be used to solve (6.19). The algorithm presented below is an applica-
tion of the general framework proposed in [32], which considers the problem
of minimizing:
F(β) := f(β) + Ω(β)
(6.22)

146
Regularization, Optimization, Kernels, and Support Vector Machines
where β ∈Rp, f(x) denotes the loss function, and Ω(β) denotes the com-
posite penalty function. If there exists a partition Sm
i=1 Ui of the variables
{β1, β2, ..., βp} such that F(β) can be written as the sum of m ≥2 functions,
each depending on a unique set Ui of the variables, we say F(β) is “separable”,
and the minimization problem (6.22) can be decomposed into m independent
optimization problems, which can be solved in parallel. Therefore if there are ρ
(2 ≤ρ ≤m) processors available for the computation, it is expected to get a ρ
times speed-up by parallelizing the computation. However, sometimes, e.g. in
(6.19), only the composite penalty term is “separable”, making it non-trivial
to parallelize.
In addition to the usual requirement of smoothness and convexity, the
parallel coordinate descent method (PCDM) presented in [32] relaxes the sep-
arability condition on f(β) to f being a (block) partially separable function.
By coordinate descent method (CDM) we refer to the strategy of cyclically
updating the coordinates of β one coordinate (or one block of coordinates)
at each iteration [12]. CDMs are fast to compute at each iteration but usu-
ally take more iterations to converge than competing methods, e.g., gradient
methods [26]; however, their performance on big data optimisation is generally
more eﬃcient [32]. An outline of this PCDM is given in Algorithm 8.
Algorithm 8 Parallel Coordinate Descent Method
Input: data X, y; parameter λ1, λ2; partition of predictors R, which consists
of |R| blocks; starting value ˜β; ϵ; number of blocks to be updated in each step
ρ (which is not less than the number of processors available).
Output: column vector ˆβ
1:
Compute constants ωRI for each block RI and the constant B.
2:
Choose initial estimate ˆβ(0).
3:
k ←0
4:
Randomly generate a set of blocks from R: k1, k2, ....
5:
In parallel do: ˆβ(k+1)
RkI
←φ(ˆβ(k), kI), for I = 1, 2, ....
6:
Collect estimates from the processors to obtain ˆβ(k+1).
7:
Set k ←k + 1 and go back to 4 until a stopping criterion is met.
In Algorithm 8 step 4, the blocks to be updated in each iteration are
randomly generated from a uniform sampling scheme, so that the probability
of any block being updated in step k is constant and independent of previous
updates. In step 5, the blockwise update of ˆβ in step (k + 1) is obtained from
a function φ, which corresponds to another optimization problem depending
on the full estimate from step k. For notation simplicity, denote φ(ˆβ(k), kI) by
φI, which is a column vector with |RkI| entries. Without loss of generality we
assume a processor only updates one block of variables in each iteration, so

The Graph-Guided Group Lasso for Genome-Wide Association Studies 147
that the index I corresponds to RI in (6.19). Then φI minimizes the function:
< ∇If, Φ > +BωI
2
∥ΦI∥2
2 + Ω(β(k)
I
+ ΦI)
(6.23)
with respect to ΦI, where <, > denotes the inner product, ∇denotes the
gradient operation, and B and ωi are constants to be determined.
The consequence of the conditions on f and Ω, the uniform sampling
scheme, and the computation in (6.23) with pre-determined constants B and
ωI is that we obtain a monotonically decreasing sequence in terms of the
expected value of F(ˆβ) after each iteration. Speciﬁcally:
E[F(ˆβ(k+1))|ˆβ(k)] ≤E[F(ˆβ(k))] .
(6.24)
Since this monotonically decreasing sequence is bounded below (certainly by
0), the algorithm is expected to converge.
We now re-state the assumptions necessary to validate this PCDM algorithm,
as originally presented in [32], and apply it to (6.19). These include:
Partial separability of f: Let Sm
i=1 Ui be a partition of the variables
{β1, β2, ..., βp}, then f is partially separable of degree δ if it can be rewrit-
ten as:
f(β) =
X
J∈J
fJ(β)
(6.25)
where J is a subset of {U1, U2, ..., Um} and each J is non-empty (possibly
overlaps with others), |J| ≤δ for all J ∈J . fJ are diﬀerentiable convex
functions that depend on the blocks of β in J only.
Note the algorithm requires no knowledge on the exact decomposition of
(6.25); it is only necessary to compute δ, which can be taken as the maximum
number of non-zero elements in the rows of X∗, when f(β) = ∥y∗−X∗β∥2
2 as
in (6.19).
Smoothness of f: This requires ∇if to satisfy the Lipschitz condition with
Lipschitz constants Li for i = 1, 2, ..., m.
Separability and convexity of Ω: Let Sm
i=1 Ui be a partition of the vari-
ables {β1, β2, ..., βp}, Ω(β) can be written as:
Ω(β) =
m
X
i=1
Ωi(β[Ui])
(6.26)
where β[Ui] depends on the entries of β in Ui only, and all Ωi are convex and
closed.
It then can be easily veriﬁed that these conditions are satisﬁed in (6.19),
where the partition of variables can be readily obtained as {U1, U2, ..., Um} =

148
Regularization, Optimization, Kernels, and Support Vector Machines
R. Now, to apply PCDM, note:
f
=
1
2∥y −Xβ∥2
2
and Ω(β)
=
λ1
P
I
p
|RI|∥βI∥2 +λ2∥β∥1. Substituting into (6.23) we obtain the optimiza-
tion problem on φI for block RI: (for notation simplicity we drop * on X and
y)
−(y−Xβ)T XIφI + BωI
2
∥φI∥2
2 +λ1
p
|RI|·∥βI +φI∥2 +λ2∥βI +φI∥1 . (6.27)
Let i ∈RI, diﬀerentiating (6.27) with respect to the ith coordinate of φI,
denoted by φi, and setting to zero, we see φi must satisfy:
−(y −Xβ)T Xi + BωIφi + λ1
p
|RI| · si + λ2ti = 0
(6.28)
where:
si
 =
βi+φi
∥βI+φI∥2
if
βI + φI ̸= 0
∈[−1, 1]
if
βI + φI = 0
(6.29)
ti
 = σ(βi + φi)
if
βi + φi ̸= 0
∈[−1, 1]
if
βi + φi = 0
(6.30)
where σ is the sign function.
Note (6.28), (6.29), and (6.30) are the sub-gradient equations on βi +φi of
a sparse group lasso model as in [13], which can be solved by a block CDM.
Speciﬁcally, set ˆφI = −βI if:
∥Sλ2[(y −Xβ)T XI + BωIβI]∥2 ≤λ1
p
|RI|
(6.31)
where Sλ(z) := σ(z) · max{|z| −λ, 0}. Otherwise, for i ∈RI, set ˆφi = −βi if:
|(y −Xβ)T Xi + BωIβi| ≤λ2 .
(6.32)
If neither (6.31) or (6.32) holds, let γi = βi + φi and Equations (6.28), (6.29),
and (6.30) merge to:
(y −Xβ)T Xi + BωIβi = BωIγi + λ1
p
|RI| ·
γi
∥γI∥2
+ λ2σ(γi)
which can be solved by:
ˆγi =
1
BωI
(1 −
λ1
p
|RI|
∥Sλ2[(y −Xβ)T XI + BωIβI]∥2
)
× Sλ2[(y −Xβ)T Xi + BωIβi] .
(6.33)
To summarize, step 5 in Algorithm 8 can be computed by Algorithm 9.
The constants B and ωI are determined by the data and the sampling
scheme involved in step 4 of Algorithm 8. In [32], several sampling methods
are described and studied. In our implementation, we use “nice sampling”,
which randomly selects ρ groups from R and each group is to be selected with
equal probability. In this case we have: B = min{δ, ρ} and ωI is the Lipschitz
constant associated with RI, which is taken as ∥XT
I XI∥F.

The Graph-Guided Group Lasso for Genome-Wide Association Studies 149
Algorithm 9 PCDM Step 5
Input: The group to be updated RkI, denoted by RI for short; data
X, y (superscript * removed for simplicity); parameter λ1, λ2; estimated
coeﬃcients before step k: β(k), denoted by β for convenience, ϵ
Output:
column
vector
ˆβ(k+1)
RkI
,
denoted
by
ˆβI
for
convenience.
1:
if (6.31) holds then
2:
ˆβI = (0, 0, ..., 0)T
3:
else
4:
for i ∈Ri do
5:
if (6.32) holds then
6:
ˆβi = 0
7:
else
8:
ˆβi = ˆγi as in (6.33).
9:
end if
10:
end for
11:
Repeat 4-10 until ∥ˆγI −ˆβI∥2 ≤ϵ
12:
end if
6.4
Simulation Studies
In this section we present simulation studies to assess the power of de-
tecting true signal-carrying predictor groups (e.g., genes) and predictors (e.g.,
SNPs). We compare the performance of the GGGL models with models that
do not account for the prior information between the predictor groups, as
encoded by the network. We carry out two sets of simulations: in (I) we com-
pare the GGGL-1 with the group lasso [42] on group selection; and in (II)
we compare the GGGL-2 with the sparse group lasso [13] on both group
and predictor selection. Throughout the simulation studies we ﬁx n = 200,
p = 1000, and |R| = 100, which have equal sizes of 10. In set (I), we con-
sider ﬁve cases of simulations, corresponding to signal-to-noise ratios (SNRs)
at 0.15, 0.125, 0.10, 0.075, 0.05; in set (II), due to the extensive computation it
involves, we perform one case of simulation in which SNR is set at 0.075. Here
the SNR is deﬁned as the ratio of the mean of the response variable to the
standard deviation of the noise. For each case of the simulations, we generate
500 data sets each consisting of X, y, R, and G. We generate the n×p matrix
X assuming the predictors follow independent standard normal distributions.
The indexes of the signal-carrying variables are ﬁxed and known in each set
of the simulations so that the performance of each model can be evaluated by
comparing the number of correctly identiﬁed predictors or predictor groups
while controlling for the number of falsely detected ones. Speciﬁcally, we let
signal-carrying predictors fall into 20 groups, and deﬁne coeﬃcient vector v

150
Regularization, Optimization, Kernels, and Support Vector Machines
in which the non-zero entries, which correspond to the signal-carrying predic-
tors, are generated from a uniform distribution in the interval (0.1, 1). In (I),
we assume 70% of the variables in the signal-carrying groups have non-zero
entries in v, while in (II) we assume a smaller proportion at 50%. We deﬁne
the response vector: y = Xv + η · e, where e is the vector of random errors
generated from independent standard normal distributions, and η is a posi-
tive real number set to achieve the desired SNRs, computed as the ratio of
the mean of Xv to the standard deviation of (η · e). Finally, we normalize the
columns of X to have zero mean and unit Euclidean norm and center y as
required by the models.
As for the network generation, we randomly partition the signal carry-
ing groups into clusters of ten, and likewise partition the non signal-carrying
groups, resulting in ten clusters each of size ten. Assuming that the proba-
bility for a pair of nodes being directly connected is independent of all other
pairs, we generate the network G using a set of three probability parameters:
pC is the probability of connection between groups belonging to the same
cluster; pCC is the probability of connection between signal-carrying or non
signal-carrying groups belonging to diﬀerent clusters; and pSN is the proba-
bility of connection between a signal-carrying group and a non signal-carrying
group. We generate networks such that signal-carrying groups are relatively
densely connected whereas there are very few links between these groups and
the non signal-carrying groups. Speciﬁcally, we set pC = 0.4, pCC = 0.15, and
pSN = 0.03.
In (I) we evaluate the performance of the GGGL-1 and use the group lasso
[42] as a benchmark, which essentially drops P2 and P3 in (6.2) and (6.3)
from (6.1). We assess the performance of the two models by group selection.
When ﬁtting the GGGL-1, we ﬁx µ = 100, λ2 = 0, and tune λ1 such that
the model selects 25 groups on average and all variables in these groups have
non-zero coeﬃcients. For group lasso, we tune λ1 such that the same number
of groups are selected. Note the sparsity level does not aﬀect the relative per-
formance of the models as long as the same sparsity level is imposed so that
the results are comparable. We compute the empirical selection probabilities
for all groups within each case of the simulations. By varying the threshold
from 0 to 1 and deﬁning the important groups as those with selection prob-
abilities greater than the threshold, we can construct the receiver operating
characteristic (ROC) curve on the |R| groups. In a ROC curve, the proportion
of signal-carriers classiﬁed as important groups (true positive rate; TPR) is
plotted against the proportion of non signal-carriers classiﬁed as important
groups (false positive rate; FPR). The area under curve (AUC) can be inter-
preted as the probability that a randomly chosen signal-carrying group has
larger selection frequency than a randomly chosen group carrying no signal,
which will be used as the evaluation criterion of model comparison. We plot
the AUC against SNR in Figure 6.3. We observe when signal is strong, both
models perform equally well; yet as the SNR continues to decrease, the loss
in power of the group lasso is at a faster rate compared with GGGL-1. When

The Graph-Guided Group Lasso for Genome-Wide Association Studies 151
FIGURE 6.3: Plot of area under ROC curve (AUC) against signal-to-noise
ratio (SNR), where variable selection is carried out at groups (genes) level. As
the SNR continues to fall, using the prior knowledge encoded by the network
to guide group selection, GGGL-1 (red line) gains additional power compared
to the group lasso (dotted blue).
FIGURE 6.4: ROC curves plotting true positive rate (TPR) against false
positive rate (FPR). The left column refers to selection of groups (genes) and
the right column refers to selection of predictors (SNPs). Using the network,
GGGL-2 (red line) retains the power of group selection yet improves on pre-
dictor selection, compared with the sparse group lasso (dotted blue).
SNR is low, the plot shows a clear advantage of incorporating the structured
prior knowledge on the predictor groups.
In (II) we compare the GGGL-2 with the sparse group lasso [13], whose
penalty terms consist of (6.2), where we impose sparsity on and within the
groups. We ﬁx µ = 10 for GGGL-2 and tune λ1, λ2 in both models such that
on average 25 groups are selected and half of the variables in these groups
have non-zero coeﬃcients. The ROC curves for group selection and predictor
selection are presented in Figure 6.4. We observe the two models are equally
competitive in selecting the predictor groups. However, GGGL-2 enhances the

152
Regularization, Optimization, Kernels, and Support Vector Machines
power of selecting the signal-carrying predictors via the smoothing of average
coeﬃcients it exercises on the predictor groups. The results in (I) and (II)
demonstrate that incorporating graphical prior knowledge on the predictor
groups enables the GGGL models to gain additional power compared to the
(sparse) group lasso models that ignore such information, in particular when
the SNR is low.
6.5
Conclusion
The work presented in this chapter is motivated by GWAs and the need
to use structured prior information to guide the selection of SNPs and genes
associated with a quantitative trait. The prior information is available at two
levels: SNPs grouped into genes via gene-SNPs mapping, and a network en-
coding the functional relatedness between the genes. We have proposed a
penalized linear regression model, the graph-guided group lasso in two ver-
sions, which can select functionally related genes and inﬂuential SNPs within
these genes that explain the variability in the trait.
We have studied some theoretical properties regarding the smoothing eﬀect
of the GGGL models, and derived an upper bound for Dµ(i, j) and Dµ(I, J),
deﬁned in (6.5) and (6.11), respectively, which measure the strength of the
smoothing eﬀect. Speciﬁcally, Proposition 6.1 shows GGGL-1 smooths the co-
eﬃcients of the SNPs if they belong to the same gene or functionally related
genes; and Proposition 6.2 shows GGGL-2 smooths the average coeﬃcients of
the SNPs belonging to functionally related genes. The primary diﬀerence is,
as stated in Corollaries 6.1 and 6.2, that GGGL-2 does not smooth the coef-
ﬁcients of SNPs within the same gene while GGGL-1 does. From a biological
perspective, GGGL-1 assumes all or almost all of the SNPs in a causal gene
contribute to the variability in the trait and hence encourage the model to
select all of them, whereas GGGL-2 assumes only a subset of the SNPs in a
causal gene do so and hence tries to avoid selecting a SNP simply because
it belongs to a causal gene. In practice we use GGGL-1 when the primary
interest is identifying the causal genes, and we use GGGL-2 when we are also
interested in identifying the causal SNPs within the selected genes.
We have presented two estimation algorithms for the GGGL models, one
serial algorithm and one parallel algorithm. The latter, adopted from the
theoretical framework in [32], has been implemented in CUDA and can be
run on graphic processing units (GPUs).
Using simulated data, we have demonstrated that by incorporating the
prior information encoded by the network, GGGL-1 gains additional power
in identifying the signal-carrying predictor groups compared with the group
lasso [42]. In addition, when imposing sparsity within the selected predictor

The Graph-Guided Group Lasso for Genome-Wide Association Studies 153
groups, GGGL-2 improves the power of selecting the signal-carrying predictors
compared with the sparse group lasso [13].
The models presented here were motivated by GWAs, in which the predic-
tors were assumed to be SNPs and the predictor groups were genes. However,
application of the GGGL models need not be restricted to GWAs data. One of
the possible diﬀerent applications is in the context of medical imaging data,
where predictors are voxels and predictor groups correspond to regions of
interest (ROI). A network on the ROIs may encode the functional correla-
tions between diﬀerent regions, as presented in [25]. Application of the GGGL
models to real GWAs and imaging data will constitute the future work.
Symbol Description
AUC
area under curve
CDM
coordinate descent method
FPR
false positive rate
GGGL
graph-guided group lasso
GWAs
genome-wide association
studies
PCDM
parallel coordinate descent
method
ROC
receiver operating
characteristic
ROI
region of interest
SNPs
single-nucleotide
polymorphisms
SNR
signal-to-noise ratio
TPR
true positive rate
Bibliography
[1] Janine Altmüller, Lyle Palmer, Guido Fischer, Hagen Scherb, and
Matthias Wjst.
Genomewide scans of complex human diseases: True
linkage is hard to ﬁnd. Am J Hum Genet, 69(5):936–950, 2001.
[2] Chloé-Agathe Azencott, Dominik Grimm, Mahito Sugiyama, Yoshinobu
Kawahara, and Karsten Borgwardt. Eﬃcient network-guided multi-locus
association mapping with graph cuts. Bioinformatics, 29:i171–i179, 2013.
[3] Lin-S. Chen, Carolyn Hutter, John Potter, Yan Liu, Ross Prentice, Ulrike
Peters, and Li Hsu. Insights into colon cancer etiology via a regularized
approach to gene set analysis of gwas data. The American Journal of
Human Genetics, 86(6):860–871, 2010.
[4] F.R.K. Chung. Spectral graph theory. CBMS regional conference series
92. Amer. Math. Soc., Providence, RI. MR1421568., 1997.

154
Regularization, Optimization, Kernels, and Support Vector Machines
[5] Geraldine Clarke, Carl Anderson, Fredrik Pettersson, Lon Cardon, An-
drew Morris, and Krina Zondervan. Basic statistical analysis in genetic
case-control studies. Nat Protoc., 6(2):121–133, 2011.
[6] The Wellcome Trust Case Control Consortium. Genome-wide association
study of 14,000 cases of seven common diseases and 3,000 shared controls.
Nature, 447(7415):661–678, 2009.
[7] Jing
Cui,
Eli
Stahl,
Saedis
Saevarsdottir,
Corinne
Miceli,
and
Dorothee Diogo et al. Genome-wide association study and gene expression
analysis identiﬁes cd84 as a predictor of response to etanercept therapy
in rheumatoid arthritis. PLOS Genetics, 9(3):1–11, 2013.
[8] Z.J. Daye and X.J. Jeng. Shrinkage and model selection with correlated
variables via weighted fusion. Computational Statistics and Data Analy-
sis, 53(4):1284–1298, 2009.
[9] D.F. Easton, D.T. Bishop, D. Ford, and G.P. Crockford. Genetic linkage
analysis in familial breast and ovarian cancer: results from 214 families.
The Breast Cancer Linkage Consortium. Am J Hum Genet., 52(4):678–
710, 1993.
[10] Stephan Ripke et al. Genome-wide association study identiﬁes ﬁve new
schizophrenia loci. Nature Genetics, 43:969–976, 2011.
[11] The International Consortium for Blood Pressure Genome-Wide Associa-
tion Studies. Genetic variants in novel pathways inﬂuence blood pressure
and cardiovascular disease risk. Nature, 478(7367):103–109, 2011.
[12] Jerome Friedman, Trevor Hastie, Holger Höﬂing, and Robert Tibshirani.
Pathwise coordinate optimization. Ann. Appl. Stat., 2(1):302–332, 2007.
[13] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. A note on the
group lasso and a sparse group lasso. arXiv:1001.0736, 2010.
[14] Jonathan Haines, Michael Hauser, Silke Schmidt, William Scott, Lana
Olson, Paul Gallins, Kylee Spencer, ShuYing Kwan, Maher Noureddine,
John Gilbert, Nathalie Schnetz-Boutaud, Anita Agarwal, Eric Postel, and
Margaret Pericak-Vance. Complement factor h variant increases the risk
of age-related macular degeneration. Science, 308:419–421, 2005.
[15] Gabriel Hoﬀman, Benjamin Logsdon, and Jason Mezey. Puma: A uniﬁed
framework for penalized multiple regression analysis of gwas data. PLOS
Computational Biology, 9:(1), 2013.
[16] Jian Huang, Shuangge Ma, Hongzhe Li, and CunHui Zhang. The sparse
Laplacian shrinkage estimator for high-dimensional regression. The An-
nals of Statistics, 39(4):2021–2046, 2011.

The Graph-Guided Group Lasso for Genome-Wide Association Studies 155
[17] Laurent Jacob, Guillaume Obozinski, and Jean-Phillippe Vert. Group
lasso with overlap and graph lasso. ICML ’09 Proceedings of the 26th
Annual International Conference on Machine Learning, pages 433–440,
2009.
[18] Andrew Johnson and Christopher O’Donnell. An open access database
of genome-wide association results. BMC Med Genet., 10:(6), 2009.
[19] CheeSeng Ku, EnYun Loy, Yudi Pawitan, and Kee Seng Chia. The pursuit
of genome-wide association studies: where are we now? J. Hum. Genet.,
55(4):195–206, 2010.
[20] Younghee Lee, Haiquan Li, Jianrong Li, Ellen Rebman, Ikbel Achour,
Kelly Regan, Eric Gamazon, James Chen, Xinan Yang, Nancy Cox, and
Yves Lussier. Network models of genome-wide association studies uncover
the topological centrality of protein interactions in complex diseases. J
Am Med Inform Assoc, 20:6:619–629, 2013.
[21] Caiyan Li and Hongzhe Li. Network-constrained regularization and vari-
able selection for analysis of genomic data. Bioinformatics, 24(9):1175–
1182, 2008.
[22] Li Luo, Gang Peng, Yun Zhu, Hua Dong, Christopher Amos, and Momiao
Xiong. Genome-wide gene and pathway analysis. European Journal of
Human Genetics, 18:1045–1053, 2010.
[23] Teri Manolio, Francis Collins, Nancy Cox, David Goldstein, Lucia Hin-
dorﬀet al. Finding the missing heritability of complex diseases. Nature,
461:747–753, 2009.
[24] dbSNP. National Center for Biotechnology Information.
http://www.ncbi.nlm.nih.gov/SNP/, 2014.
[25] Steven Nelson, Alexander Cohen, Jonathan Power, Gagan Wig, Fran-
cis Miezin, Mark Wheeler, Katerina Velanova, David Donaldson, Jeﬀrey
Phillips, Bradley Schlaggar, and Steven Petersen. A parcellation scheme
for human left lateral parietal cortex. Neuron, 67(1):156–170, 2010.
[26] Yurii Nesterov. Eﬃciency of coordinate descent methods on huge-scale
optimization problems. SIAM Journal on Optimization, 22(2):341–362,
2012.
[27] Fabian Ojeda, Marco Signoretto, Raf Van de Plas, Etienne Waelkens,
Bart De Moor, and Johan A.K. Suykens. Semi-supervised learning of
sparse linear models in mass spectral imaging.
Pattern Recognition
in Bioinformatics, 5th IAPR International Conference, PRIB 2010 Ni-
jmegen, The Netherlands, IV:325–334, 2010.

156
Regularization, Optimization, Kernels, and Support Vector Machines
[28] Thomas Pearson and Teri Manolio.
How to interpret a genome-
wide association study. Journal of the American Medical Association,
299(11):1335–1344, 2008.
[29] Yu Qian, Søren Besenbacher, Thomas Mailund, and Mikkel Schierup.
Identifying disease associated genes by network propagation. BMC Sys-
tems Biology, 8(Suppl 1):S6, 2014.
[30] Towﬁque Raj, Joshua Shulman, Brendan Keenan, Lori Chibnik, Denis
Evans, David Bennett, Barbara Stranger, and Philip De Jager. Alzheimer
disease susceptibility loci: Evidence for a protein network under natu-
ral selection. The American Journal of Human Genetics, 90(4):720–726,
2012.
[31] Peter Richtárik and Martin Takáč.
Distributed coordinate descent
method for learning with big data. arXiv:1310.2059v1, 2013.
[32] Peter Richtárik and Martin Takáč. Parallel coordinate descent methods
for big data optimization. arXiv:1212.0873v2, 2013.
[33] Marco Signoretto, Anneleen Daemen, Carlo Savorgnan, and Johan
A.K. Suykens. Variable selection and grouping with multiple graph priors.
in Proc. of the 2nd Neural Information Processing Systems (NIPS) Work-
shop on Optimization for Machine Learning, Whister, Canada, 2009.
[34] Matt Silver, Peng Chen, Ruoying Li, Ching-Yu Cheng, Tien-Yin Wong,
E-Shyong Tai, Yik-Ying Teo, and Giovanni Montana. Pathways-driven
sparse regression identiﬁes pathways and genes associated with high-
density lipoprotein cholesterol in two Asian cohorts.
PLoS Genetics,
9(11):1–28, 2013.
[35] Matt Silver, Eva Janousova, Xue Hua, Paul Thompson, and Giovanni
Montana. Identiﬁcation of gene pathways implicated in Alzheimer’s dis-
ease using longitudinal imaging phenotypes with sparse regression. Neu-
roImage, 63(3):1681–1694, 2012.
[36] Matt Silver and Giovanni Montana. Fast identiﬁcation of biological path-
ways associated with a quantitative trait using group lasso with overlaps.
Statistical Applications in Genetics and Molecular Biology, 11(1):article
7, pp 1–43, 2012.
[37] Damian Szklarczyk, Andrea Franceschini, Michael Kuhn, Milan Si-
monovic, Alexander Roth, Pablo Minguez, Tobias Doerks, Manuel Stark,
Jean Muller, Peer Bork, Lars Jensen, and Christian von Mering. The
string database in 2011: functional interaction networks of proteins, glob-
ally integrated and scored. Nucleic Acids Res., 39(Database issue):D561–
D568, 2011.

The Graph-Guided Group Lasso for Genome-Wide Association Studies 157
[38] Robert Tibshirani.
Regression shrinkage and selection via the lasso.
J.R.Statist., Soc.B, 58:267–288, 1996.
[39] Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith
Knight. Sparsity and smoothness via the fused lasso. Journal of the Royal
Statistical Society: Series B, 67(1):91–108, 2005.
[40] Hua Wang, Feiping Nie, Heng Huang, Sungeun Kim, Kwangsik Nho,
Shannon Risacher, Andrew Saykin, and Li Shen. Identifying quantitative
trait loci via group-sparse multitask regression and feature selection: an
imaging genetics study of the adni cohort. Bioinformatics, 28(2):229–237,
2012.
[41] Kai Wang, Mingyao Li, and Hakon Hakonarson.
Analysing biological
pathways in genome-wide association studies. Nature Reviews Genetics,
11:843–854, 2010.
[42] Ming Yuan and Yi Lin. Model selection and estimation in regression with
grouped variables. J.R.Statist., Soc.B, 68(1):49–67, 2006.
[43] Hua Zhou, Mary Sehl, Janet Sinsheimer, and Kenneth Lange. Association
screening of common and rare genetic variants by penalized regression.
Bioinformatics, 26(19):2375–2382, 2010.
[44] Hui Zou and Trevor Hastie. Regularization and variable selection via the
elastic net. J.R.Statist., Series B, 67(2):301–320, 2005.

This page intentionally left blank
This page intentionally left blank

Chapter 7
On the Convergence Rate of
Stochastic Gradient Descent for
Strongly Convex Functions
Cheng Tang
George Washington University
Claire Monteleoni
George Washington University
7.1
Introduction ......................................................
160
7.1.1
Background ..............................................
160
7.1.2
The First-Order Oracle Model for Stochastic
Optimization .............................................
161
7.1.3
When F is Strongly Convex and Non-Smooth ..........
162
7.1.4
Contributions ............................................
163
7.2
Related Work ....................................................
163
7.2.1
Strong Convexity and the Lower Bound of Stochastic
Optimization .............................................
164
7.2.2
Variance Reduction Methods and Optimal Algorithms .
165
7.3
Preliminaries .....................................................
166
7.4
Our Approach ....................................................
167
7.4.1
Subclassiﬁcation of Strongly Convex Functions in Two
Ways .....................................................
167
7.4.2
Subclassify Strongly Convex Functions by (ν, Lν)-Weak
Smoothness ..............................................
168
7.4.3
Subclassify Strongly Convex Functions by
(µ, λµ)-Strong Convexity ................................
168
7.5
Main Results .....................................................
169
7.5.1
Set Relations between Sµ and Hν
......................
169
7.5.2
The Gap between Strong Convexity and Weak
Smoothness ..............................................
171
7.5.3
Convergence Rate and the Quantitative Relation
between µ and ν .........................................
172
7.5.4
A Technical Lemma .....................................
172
7.6
Conclusion and Future Work ....................................
173
Bibliography ......................................................
174
159

160
Regularization, Optimization, Kernels, and Support Vector Machines
7.1
Introduction
In this chapter we survey recent works on the non-asymptotic analysis of
stochastic gradient descent (SGD) on strongly convex functions. We discuss
how both strong convexity and smoothness of a strongly convex function, de-
noted by F, may aﬀect the rate of error-convergence in Section 7.2. This leads
us to characterize F by these two properties by presenting a subclassiﬁcation
scheme of strongly convex functions in Section 7.4.1.
7.1.1
Background
Consider a standard binary classiﬁcation problem: We have a sequence
of i.i.d. pairs of random variables, (Y1, X1), . . . , (Yn, Xn), sampled from some
unknown distribution p, where ∀i = 1, . . . , n, Yi = 0 or 1 is the label of Xi ∈X,
where X is the sample space. Additionally, suppose our hypothesis space is
composed of linear classiﬁers w ∈W ⊂Rd and we are given a loss function
l(w, (Y, X)) to measure the discrepancy between the classiﬁer’s prediction and
the true label.
Based on this information, how can we determine the optimal classiﬁer
in the hypothesis space? According to statistical learning theory, the optimal
classiﬁer is the one that minimizes the true risk Ep[l(w, (Y, X))]. Empirical
Risk Minimization (ERM) is a commonly used principle to estimate the op-
timal classiﬁer using a ﬁnite data sample. It chooses the best classiﬁer as the
one that minimizes the empirical risk or regularized empirical risk:
Qn[l(w, (Y, X))] = 1
n
n
X
i=1
l(w, (Yi, Xi)) + R(w)
(7.1)
where R is a regularizer, which penalizes the complexity of w.
When Qn[l(w, (Y, X))] is convex in w ∈W, a gradient descent algorithm
will ﬁnd the optimum of this function. For simpliﬁcation, we temporarily
assume Qn[l(w, (Y, X))] is diﬀerentiable. Then, to run gradient descent, we
need to compute the gradient ∇Qn[l(w, (y, x))]. Given the functional form of
Qn[l(w, (y, x))], we see that to ﬁnd the gradient of the empirical risk, we need
to evaluate the loss on every sample in the data set. For large data sets, this
imposes a lot of computation.
We study the performance of an eﬃcient version of gradient descent,
Stochastic Gradient Descent (SGD). Instead of computing the gradient based
on the entire data, SGD estimates ∇Ep[l(w, (y, x))] by only evaluating the loss,
l(w, (Y, X)), based on one random sample, (Y, X), of the data. With diﬀerent
learning paradigms, there are two ways to generate the random sample.
The ﬁrst corresponds to our binary classiﬁcation model: if we have a batch
i.i.d. sample from p, then we can sub-sample the pair (Y, X) from the batch

Convergence Rate of Stochastic Gradient Descent
161
uniformly at random. Alternatively, suppose we are in an online setting and
we receive samples according to the true distribution p, then (Y, X) is a ran-
dom sample from p. Under either model, (Y, X) is distributed according to p
(see [14]).
According to the gradient sampling scheme deﬁned by SGD, it is easy to
see that ˆ∇SGD[l(w, (Y, X))] is an unbiased estimator of the gradient of the
empirical risk in Equation (7.1):
E[ ˆ∇SGD[l(w, (Y, X))]] = ∇Qn[l(w, (Y, X))]
(7.2)
where the expectation on the left-hand side of the equation is with respect to
the uniform random sampling of the batch data when estimating the gradient.
Then we may ask how does SGD compare with the deterministic gradient
descent in optimizing the empirical risk Qn[l(w, (Y, X)), assuming it is convex
in w.
In SGD, data are processed in an online fashion and the computational
eﬀort of evaluating the approximated gradient is independent of the data
size. Despite the signiﬁcant reduction in computation and memory, SGD has
been shown to converge in expectation to the optimum of a convex function.
This advantage contributes to the popularity of SGD for large-scale learning
(see [2]).
Note that the setting we consider here is subsumed by and easier than the
more general online convex programming setting introduced by [15], where the
instant loss function lt(w, (Yt, Xt)) upon receiving (Yt, Xt) can be adversarial.
7.1.2
The First-Order Oracle Model for Stochastic
Optimization
The optimization mechanism with SGD described earlier is a special case
of the following oracle model (see [6]). Given a convex hypothesis space W and
a convex function F (not necessarily strongly convex), we run a gradient-based
algorithm A (not necessarily SGD) on W to ﬁnd an optimizer of F. At the t-th
iteration, A chooses wt ∈W. Then an oracle provides unbiased estimates of
the function value and the gradient of F at wt. In general, the function F need
not be diﬀerentiable. However, since F is convex, a subdiﬀerential exists at any
point of the convex domain. In that case, the oracle will provide an unbiased
estimate of a subgradient of F at wt. That is, ˆg(wt) is provided by the oracle
such that E[ˆg(wt)] = g(wt) ∈∂f(wt), where ∂f(wt) is the subdiﬀerential of F
at wt (see [11] for more details). Then we update the hypothesis to get wt+1
using the function and the gradient information.
We focus on studying SGD under the ﬁrst-order oracle model, where the
optimization procedure can be formally stated as:
1. At T = 1, choose w1 ∈W arbitrarily.
2. At T = t, choose learning rate ηt and use the update,
wt+1 = ΠW (wt −ηtˆg(wt))
(7.3)

162
Regularization, Optimization, Kernels, and Support Vector Machines
where Π is the orthogonal projection operator:
ΠW (y) = arg min
w∈W ∥w −y∥L2 .
The reason for projection is that wt −ηtˆg(wt) might fall outside of the hy-
pothesis space W. Therefore, a projection step is used to guarantee that wt+1
stays in W. We are mainly interested in how the expected optimization error
E[F(wt) −F(wopt)] depends on T, the number of oracle queries.
7.1.3
When F is Strongly Convex and Non-Smooth
Diﬀerent assumptions about F can be made in accordance with diﬀerent
problem formulations. Convexity is usually assumed for any global conver-
gence analysis. In addition, strong convexity of F and smoothness around
the optimum wopt are two of the most common assumptions. Formally, F is
λ-strongly convex, if ∀w, w′ ∈W, and any subgradient g of F at w, [9]
F(w′) ≥F(w) + ⟨g(w), w′ −w⟩+ λ
2 ∥w′ −w∥2 .
(7.4)
An important consequence of the above deﬁnition of strong convexity is, lo-
cally, with wopt as the reference point,
F(w) −F(wopt) ≥λ
2 ∥w −wopt∥2
(7.5)
and
∥g(w)∥≥λ ∥w −wopt∥.
(7.6)
On the other hand, let wopt be the optimum of F, then F is µ-smooth with
respect to wopt if ∀w ∈W
F(w) −F(wopt) ≤µ
2 ∥w −wopt∥2
(7.7)
which implies
∥g(w)∥≤µ ∥w −wopt∥.
(7.8)
In view of Equation (7.6) and Equation (7.8), strong convexity and smoothness
conditions are nothing but lower and upper bounds on ∥g(w)∥with respect
to ∥w −wopt∥, the distance of the current hypothesis to the optimum.
Recall the binary classiﬁcation problem described at the beginning. We
may ﬁnd a classiﬁer using ERM, which can be cast as an optimization problem
where F = Qn[l(w, (Y, X))] = 1
n
Pn
i=1 l(w, (Yi, Xi)) + R(w). Diﬀerent losses
and regularizers can be chosen with diﬀerent learning methods. Under linear
SVM, the problem is instantiated as
min
w
1
n
n
X
i=1
l(w, (Yi, Xi)) + λ
2 ∥w∥2
(7.9)

Convergence Rate of Stochastic Gradient Descent
163
where l(w, (Yi, Xi)) = max{0, 1 −Yi⟨w, Xi⟩}, i.e., the hinge loss. Under this
formulation, [12] derived Pegasos, a primal solver of linear SVM geared to-
wards large-scale classiﬁcation problems. Note that in Equation (7.9), the
objective F is strongly convex and non-smooth.
7.1.4
Contributions
Driven by discussions in Section 7.1.2 and Section 7.1.3, we study the
general SGD-oracle model, where F is strongly convex. Our assumptions
follow the analyses of [4] and [9]. Our contributions can be summarized as
follows:
1. We provide a survey on the current analysis of the convergence rate
of gradient-based algorithms on strongly convex functions under the
ﬁrst-order oracle model: We summarize the major techniques used in
the analysis of the lower bound of the problem, and we compare vari-
ance reduction methods used in developing faster algorithms under this
framework (Section 7.2).
2. We demonstrate how both the degree of strong convexity and smooth-
ness aﬀect the convergence rate of E[F(wt) −F(wopt)] and derive a
subclassiﬁcation scheme to characterize F (Section 7.4.1).
3. We illustrate how the new scheme could yield a better analysis of con-
vergence, and we proposed an open problem on whether it is easier to
prove the optimal convergence rate of SGD on a “nice” subset of strongly
convex functions (Section 7.5.3).
Note that the subclassiﬁcation of F is only for the analysis of SGD. The
algorithm itself does not have information about F, other than that given by
the oracle.
7.2
Related Work
To motivate our approach, we review existing analyses of stochastic convex
optimization under the oracle model introduced above. In particular, we ﬁrst
discuss how strong convexity is tied to information gain in proving lower
bounds. Then we review the optimal algorithms and discuss how variance
reduction methods are used to achieve fast convergence.

164
Regularization, Optimization, Kernels, and Support Vector Machines
7.2.1
Strong Convexity and the Lower Bound of Stochastic
Optimization
Pertaining to lower bounds for stochastic convex optimization, [1] initiated
an information-theoretic approach. Their key observation is that convex func-
tions of the same shape can be parametrized by their optima. Thus, function
optimization is tied to the problem of statistical parametric estimation. Based
on the dimension of the problem, they construct a core subclass of functions,
parametrized by their optima. They show that in order to achieve low opti-
mization error on any of the functions in this subclass, an algorithm needs
to estimate which function is being used. This amounts to gaining enough in-
formation about the true parameter from the oracle during the optimization
procedure. Then they construct a Bernoulli oracle, which outputs an unbiased
estimator of a subgradient, while not revealing much information about the
true parameter at any iteration. Thus, they show that in order to achieve the
desired accuracy of optimization, the algorithm needs to query the oracle a
suﬃcient amount of times. Speciﬁcally, they show that the error of any algo-
rithm for any convex F is Ω( 1
√
T ), where T is the number of oracle calls. For
strongly convex functions, the lower bound is Ω( 1
T ).
The work of [8] analyzes the problem by constructing a Markov chain
feedback system and uses a white-noise Gaussian oracle instead. Utilizing
the Markov chain property, they also use information theoretic inequalities
to achieve the same lower bounds. In addition, they point out the diﬀerence
between an algorithm achieving a desired accuracy arbitrarily and an algo-
rithm whose expected accuracy decreases to zero asymptotically. The latter
class, which contains SGD, is called “any-time” algorithms. For any-time al-
gorithms, they show that decrease in errors ∥F(wt) −F(wopt)∥and ∥g(wt)∥
entails decrease in information gain. This law of diminishing return makes it
clear why strong convexity is beneﬁcial to optimization from an information-
theoretic perspective; it provides a lower bound on information gain through
a lower bound on the subgradients in Equation (7.6). In other words, at each
query, a strongly convex function is bound to give more information about
the true parameter through its large subgradients.
The relation between information gain and strong convexity is further
quantiﬁed and generalized in the work of [10]. There, they subclassify the
degree to which a function is strongly convex and derive speciﬁc lower bounds
based on the degree: they show that for a Lipschitz continuous function F
such that ∀w ∈W, F(w) −F(wopt) ≥
λ
2 ∥w −wopt∥k, where k > 1 (i.e., k
is the exponent of the Hölder condition), the error of the T-th iteration of
SGD on F(wT ) −F(wopt) is Ω(
1
T
k
2k−2 ). When k = 2, F is strongly convex as
deﬁned in Equation (7.5). When 1 < k < 2, F is highly strongly convex, and
should have faster convergence rate. They also developed an algorithm based
on [4], which attains the lower bound for any k > 1. However, their adapted
algorithm uses the knowledge of the strong convexity parameter k, which is
usually not assumed to be given.

Convergence Rate of Stochastic Gradient Descent
165
Does the previous work suggest strong convexity is always beneﬁcial? Per-
haps in view of lower bound, since it means an algorithm could gain more
information at each query of the oracle by the strong convexity assumption
on F. However, if we restrict ourselves to the simple update rule of SGD as
in Equation (7.3), then we may ﬁnd a possible disadvantage of strong con-
vexity: large subgradients might inﬂuence the stability of the predictors wt’s
produced by SGD. We will motivate this intuition in the next section and
further discuss it in Section 7.5.3.
7.2.2
Variance Reduction Methods and Optimal Algorithms
The convergence rate of SGD is inherently limited by the randomness of
the estimated subgradients. The oracle model only assumes that the variance
of the estimated gradient, var[ˆg(wt)], is bounded. As a result, if one simply
updates the algorithm based on the given gradient, wt will diverge from wopt
asymptotically. Therefore, in order to achieve asymptotic convergence of wt to
wopt, the term ηt is added in the SGD update rule in Equation (7.3) to ensure
var[ηtˆg(wt)] →0 as t →∞[7]. Since there are two terms in ηtˆg(wt), two
natural variance reduction methods are derived in previous work: choosing ηt
as a decreasing function of t, or reducing the variance of the oracle estimate
ˆg(wt).
Letting ηt decrease reduces variance by forcing it to go to zero asymp-
totically. However, by choosing a small ηt, the speed of convergence will be
limited due to an update step of size ηt ∥ˆg(wt)∥, which is less than what the
gradient suggests. Therefore, a careful choice of ηt is needed. Unfortunately,
the optimal ηt is very sensitive to other factors, as we shall see soon, making
it a diﬃcult task.
On the other hand, reducing var[ˆg(wt)] directly does not have the disad-
vantage of limiting the convergence speed. [5] develops a subgradient modiﬁ-
cation scheme that achieves exponentially fast error decay in expectation. The
idea is that the variance of their improved estimate goes to zero asymptotically.
Thus, they can choose a constant η, meaning the speed of convergence does
not have to be tempered by ηt. However, the analysis of these methods requires
additional assumptions on the form of F, and the storage and re-computation
of gradients impose more computational demands, which deviates from the
original goal of using SGD. Moreover, it violates the assumptions made in the
ﬁrst-order oracle model underlying our setting.
Besides the two obvious methods of variance reduction, predictor aver-
aging is also used; after obtaining a sequence of predictors w1, . . . , wt from
SGD, we take ˜wt = Pt
i=t−Nt wi as our ﬁnal prediction at each iteration. By
predictor averaging, var[wt −wopt] can be decreased to var[ ˜wt −wopt], thus
indirectly reducing var[F(wt)−F(wopt)], the variance of the error in function
value. However, since the previous iterates w1, . . . , wt−1 are likely to be farther
away from wopt, making Nt too large is also suboptimal. Thus, the method of
predictor averaging faces a similar problem as that of choosing ηt.

166
Regularization, Optimization, Kernels, and Support Vector Machines
To reach optimal convergence, one might have to interweave the choice
of ηt and Nt. The problem was studied in earlier stochastic approximation
literature, where the optimization objective and the algorithm, called Robbins-
Monro procedure, resembles SGD under the ﬁrst-order oracle model (see [7]
and references therein). There, they show ηt = 1
t is optimal for the stochastic
procedure in terms of estimator eﬃciency when Nt = 0, ∀t, i.e., no averaging
is taken. Interestingly, they also show that when ηt =
1
tγ , where 1
2 < γ < 1,
averaging the predictors is the optimal choice instead.
The two known optimal algorithms for optimizing general strongly convex
functions both extend from simple SGD (as described earlier) and incorpo-
rate the choice of ηt and Nt. The ﬁrst optimal algorithm for a general strongly
convex function under the ﬁrst-order oracle model is given by [4]). Their al-
gorithm chooses a sequence of geometrically decreasing ηt, accompanied by a
geometrically increasing Nt. Then [9] subsequently develops a simpler opti-
mal algorithm of a similar ﬂavor; with ηt = 1
t , and ﬁxing the total number of
iterations, T, it takes NT = αT number of averages. The optimality of their
algorithms suggests that more stabilization is needed around the optimum
for the optimization of a general strongly convex function. Not surprisingly,
the optimal degree of stabilization is also related to the functional property.
The work in [4] shows that if NT = T, then the convergence rate is Ω( log T
T
),
suggesting that too much stabilization is suboptimal for strongly convex func-
tions.
The discussion above seems to lead us to conclude that the variance of
SGD-based prediction is only determined by ηt, Nt, and var[ˆg(wt)]. How-
ever, the degree to which local smoothness of F around wopt, or equiva-
lently, an upper bound assumption on ∥g(wt)∥with respect to ∥wt −wopt∥
as in Equation (7.7) and Equation (7.8), aﬀects the convergence rate was also
brought to attention recently by [9] and [13]. It can be shown that if F is
strongly convex and locally smooth around wopt, then SGD with ηt = 1
t and
Nt = 0, ∀t ≥1 attains the optimal convergence rate. The analysis of conver-
gence provided in [9] gives a simple reason for why smoothness is beneﬁcial to
SGD: running SGD on a strongly convex function F has the guarantee that
∥wt −wopt∥2 = O( 1
t ). Thus, by the smoothness assumption in 7.7, one di-
rectly obtains F(wt) −F(wopt) = O( 1
t ). Intuitively, under a certain Lipschitz
assumption of F, F can be viewed as a stabilizing transformation of random
variable wt. We will explain this eﬀect further in Section 7.5.3.
7.3
Preliminaries
For the rest of the chapter, we assume that

Convergence Rate of Stochastic Gradient Descent
167
• The unknown objective function F to be optimized is λ-strongly convex
whose (unique) optimum is denoted by wopt.
• The estimated subgradient provided by the oracle at time t satisﬁes
E[ ˆgt] = g(wt) ∈∂F(wt) and that ∃G > 0 such that E[∥ˆgt∥2] ≤G. That
is, ˆg(·) is an unbiased estimator of a subgradient of F(·), and its second
moment is bounded.
• In the SGD algorithm, we set the learning rate ηt as a linear function
of 1
t . That is, ηt =
c
λt, where c is a positive constant and λ is the strong
convexity parameter.
Based on the oracle model framework introduced earlier and the above as-
sumptions, we are interested in whether the expected error of the last iterate
of SGD, that is, E[ϵt] := E[F(wt)−F(wopt)], achieves the optimal convergence
rate, Ω( 1
t ).
7.4
Our Approach
The observation that strong convexity and smoothness of function F might
both aﬀect the convergence rate of SGD leads us to characterize F by these
two properties. The beneﬁts and motivations of our subclassiﬁcation scheme
include the following:
1. Since both the degree of strong convexity and the degree of local smooth-
ness aﬀect the performance of SGD, we may examine diﬀerent subclasses
of F and gain further insight into how the property of F aﬀects the con-
vergence rate.
2. Our proposed subclassiﬁcation scheme provides us with a tool for a
careful analysis of the choice of learning rate ηt and the use of predictor
averaging, by restricting our focus on a subclass of convex functions
(this can already be observed in the analysis of [10]), which might lead
to faster algorithms.
3. Our proposed subclassiﬁcation scheme uniﬁes diﬀerent assumptions on
F, including those used in the past work.
7.4.1
Subclassiﬁcation of Strongly Convex Functions in Two
Ways
Comparing the deﬁnitions of strong convexity and smoothness in Equa-
tion (7.4) and Equation (7.7), we see that since the inequality signs are re-
versed, it seems that gaining one advantage means losing the other. However,

168
Regularization, Optimization, Kernels, and Support Vector Machines
these deﬁnitions only capture functions at the two extremes. Can we balance
their advantage and disadvantage when the functions vary from one extreme
to the other? We utilize ﬁner characterizations to bridge the gap between
strong convexity and smoothness.
7.4.2
Subclassify Strongly Convex Functions by (ν, Lν)-Weak
Smoothness
We ﬁrst relax the notion of smoothness and deﬁne weak smoothness using
the Hölder condition, which allows us to consider polynomials of continuous
degree.
Deﬁnition 7.1 ((ν, Lν)-Weak smoothness). Let ν ∈[0, 1] and let F be a
strictly convex function. Then, we say F is (ν, Lν)-weakly smooth, if ∃Lν > 0
such that for all w ∈W, any subgradient g(w) ∈∂F(w) and any g(wopt) ∈
∂F(wopt) satisfy the following inequality,
∥g(w) −g(wopt)∥≤Lν ∥w −wopt∥ν
where the norm is the Euclidean norm.
Since F is a strictly convex function, ∃g(wopt) = 0, the inequality above
implies
∥g(w)∥≤Lν ∥w −wopt∥ν
This leads to the following inequality (see, for example, [3])
F(w) −F(wopt) ≤
Lν
1 + ν ∥w −wopt∥1+ν
for all w ∈W.
Naturally, we deﬁne the set of Hν functions as below:
Deﬁnition 7.2 (the Hν class). F ∈Hν, if F is (ν, Lν)-weakly smooth.
7.4.3
Subclassify Strongly Convex Functions by (µ, λµ)-Strong
Convexity
In the following deﬁnition, (µ, λµ)-strong convexity is deﬁned almost sym-
metrically, except for the extra restriction on the boundedness of the subgra-
dients, which is automatically guaranteed for the weakly-smooth functions.
Deﬁnition 7.3 ((µ, λµ)-strong convexity). Let µ ∈[0, 1] and F be a strictly
convex function, and ∃K > 0 s.t. ∀w ∈W, and ∀g(w) ∈∂F(w), ∥g(w)∥≤
K. Then, we say F is (µ, λµ)-strongly convex, if ∃λµ > 0 such that for all
w ∈W, any subgradient g(w) ∈∂F(w) and any g(wopt) ∈∂F(wopt) satisfy
the following inequality,
∥g(w) −g(wopt)∥≥λµ ∥w −wopt∥µ .

Convergence Rate of Stochastic Gradient Descent
169
Similar to the weakly-smooth case, this leads us to the following inequalities,
∥g(w)∥≥λµ ∥w −wopt∥µ
F(w) −F(wopt) ≥
λµ
1 + µ ∥w −wopt∥1+µ
for all w ∈W.
We subsequently deﬁne the Sµ functions.
Deﬁnition 7.4 (the Sµ class). F ∈Sµ, if F is (µ, λµ)-strongly convex.
Remark
1. By deﬁnition, the class of all strongly convex functions with
bounded subgradients coincides with S1.
2. It is easy to check that if ν1 ≥ν2 ≥0, then F ∈Hν1
=⇒F ∈Hν2.
That is, the sets form a nested family: H0 ⊇H1 ⊇H2, . . . Symmetri-
cally, we have S0 ⊆S1 ⊆S2, . . .
7.5
Main Results
In this section, we ﬁrst illustrate the set relations captured by the strong
convexity parameter µ and the weak smoothness parameter ν. Then we discuss
why we use the two parameters and how their relation aﬀects the convergence
rate of SGD.
7.5.1
Set Relations between Sµ and Hν
Now we prove the set relations between Sµ and Hν and discuss why we
need two diﬀerent ways to subclassify strongly convex functions. The set re-
lations are illustrated in Figure 7.1.
Lemma 7.1. S1 ⊆H0 \ {∪ν>1,ν∈QHν}.
Proof. That is, we need to show if F ∈S1, then F ∈H0 and F ̸∈Hν for any
ν > 1, ν ∈Q.
To prove F ∈H0, suppose otherwise. Then, ∀L > 0, ∃w ∈W s.t. F(w) −
F(wopt) > L ∥w −wopt∥. Then we can choose L∗> K and w ∈W s.t. F(w)−
F(wopt) > L∗∥w −wopt∥. Then, by deﬁnition, ∃a subgradient g(w) with
∥g(w)∥> L∗> K. On the other hand, we also have ∥g(w)∥≤K,
=⇒
contradiction. To prove F ̸∈Hν, suppose otherwise. Then by deﬁnition of
(ν, Lν)-weak-smoothness, ∃Lν > 0 s.t. F(w) −F(wopt) ≤
Lν
1+ν ∥w −wopt∥1+ν.
On the other hand, by strong convexity, we also have ∃λ > 0 s.t. F(w) −
F(wopt) ≥λ
2 ∥w −wopt∥2. Since ν > 1, this leads to a contradiction.

170
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 7.1: Set relations between Sµ and Hν. The entire ellipse is the
set of strongly convex functions with bounded subgradients; previous results
showed optimal convergence on the intersection of the H1 and S1.
Remark: We use the restriction ν ∈Q to avoid the problem of uncount-
ability of real numbers, when taking the inﬁnite unions of sets Hν.
This lemma shows that the set H0 \ {∪ν>1,ν∈QHν} contains all strongly
convex functions. Note that we use a proof by contradiction to show that
F ∈H0, instead of simply using the boundedness of subgradients for strongly
convex functions in Deﬁnition 7.3 because the given bound K need not be the
same as the strong-smoothness coeﬃcient L0 deﬁned in Deﬁnition 7.1, which
depends on the true property of F. In particular, it is possible that K > L0.
That is, K may not exactly characterize the strong-smoothness property of
the unknown function F, but only an upper bound.
Lemma 7.2. S0 ⊆

S1 ∩{H0 \ ∪ν>0,ν∈QHν}
	
Proof. By the deﬁnition of Sµ, S0 ⊆S1. Also, since H0 includes all functions
with bounded subgradients, S0 ⊆H0. Finally, suppose S0 ∩{∪ν>0,ν∈QHν} ̸=
∅. Then ∃F ∈S0∩{∪ν>0,ν∈QHν}. That is, ∃λ0, Lν such that Lν ∥w −wopt∥ν ≥
∥g(w)∥≥λ0 ∥w −wopt∥0 = λ0 , where ν > 0, for all w ∈W. Taking the
limit as w →wopt, we have 0 ≥λ0 > 0
=⇒
contradiction. Thus, S0 ∩
{∪ν>0,ν∈QHν} = ∅. Therefore, S0 ⊆S1 ∩{H0 \ ∪ν>0,ν∈QHν}.
An example: As a simple example of F ∈S0, consider the function
y = |x|, where x ∈R. It satisﬁes g(w) = 1, ∀w > 0 and g(w) = −1, ∀w < 0.
Thus, ∥g(w)∥= 1, ∀w ̸= 0, which is the optimum. Thus, by Deﬁnition 7.3,
F is a (0, 1)-strongly convex function, and F ∈S0.

Convergence Rate of Stochastic Gradient Descent
171
In the subsequent section, we discuss the need for employing both (µ, λµ)-
strong convexity and (ν, Lν)-weak smoothness to subclassify the strongly con-
vex functions and demonstrate how the quantitative relation between strong
smoothness parameter ν and strong convexity parameter µ characterizes the
diﬃculty of optimization.
7.5.2
The Gap between Strong Convexity and Weak
Smoothness
Deﬁnitions 7.1 and 7.3 imply that for a function to be strongly convex
(or weakly smooth) at wopt, it must be strongly convex (or weakly smooth)
at wopt from every direction. Thus, for any α ∈[0, 1], it is possible that a
function F is in Sα and also in Hβ \ {∪ν>β,ν∈QHν} for any 0 ≤β ≤α;
for example, there is a function F that is (0, λ0)-strongly convex from one
direction at wopt, and thus failing to be in {∪ν>β,ν∈QHν} for any β ∈[0, 1],
i.e., F ∈H0 \ {∪ν>0,ν∈QHν}. On the other hand, F can simultaneously be
(1, L1)-weakly smooth from the opposite direction at wopt, failing to be in
{∪µ<β,µ∈QSµ} for any β ∈[0, 1], hence F is also in S1 \{∪µ<1,µ∈QSµ}. In this
case, F ∈

H0 \ {∪ν>0,ν∈QHν}
	
∩

S1 \ {∪µ<1,µ∈QSµ}
	
, implying that the
tightest upper and lower bound on ∥g(wt)∥derived from the (µ, λµ)-strong
convexity and (ν, Lν)-weak smoothness conditions are that ∥g(wt)∥= O(1)
and ∥g(wt)∥= Ω(∥w −wopt∥). These are the functions where there is no α
such that F ∈{Hα \ {∪ν>α,ν∈QHν}} ∩{Sα \ {∪µ<α,µ∈QSµ}}.
This observation justiﬁes our two diﬀerent ways of subclassifying the
strongly convex functions. Combined, they provide a reﬁned way of de-
scribing the function around its optimum. This is achieved by locating the
function at the intersection of the two sets described by strong convexity
and weak smoothness. Speciﬁcally, given F, we can ﬁnd β ≤α such that
F ∈

Hβ \ {∪ν>β,ν∈QHν}
	
∩{Sα \ {∪µ<α,µ∈QSµ}}. It is easy to see that
0 ≤α −β ≤1, and the greater this discrepancy is, the larger the gap between
the upper and lower bound of ∥g(wt)∥is. Here we present a lemma that de-
scribes the subclass of functions that we would not have been able to cover
without using parameter ν.
Lemma 7.3. Let X =

S1 −S1 ∩{∪ν>0,ν∈QHν}
	
△S0, then
X ⊆

H0 \ {∪ν>0,ν∈QHν} −S0	
.
Proof. Let ν ∈Q and ν ∈[0, 1]. S1 =

S1 ∩{∪ν>0Hν}
	
∪

S1 ∩{∪ν>0Hν}c	
.
By Lemma 7.1, S1 ⊆H0, so S1 ∩{∪ν>0Hν}c ⊆H0 ∩{∪ν>0Hν}c =
H0 \ {∪ν>0Hν}, since ∪ν>0Hν ⊆H0. Therefore, S1 ⊆

S1 ∩{∪ν>0Hν}
	
∪

H0 \ {∪ν>0Hν}
	
. Since S0 ⊂S1, and by Lemma 7.2, S0 ⊆H0 \ {∪ν>0Hν},
we have X =

S1 −S1 ∩{∪ν>0Hν}
	
△S0 =

S1 −S1 ∩{∪ν>0Hν}
	
\ S0 ⊆

H0 \ {∪ν>0Hν}
	
\ S0	
. Thus X ⊆

H0 \ {∪ν>0Hν}
	
−S0.
Note that the symbols “−” and “\” are used interchangeably in the proof
above to represent set exclusion.

172
Regularization, Optimization, Kernels, and Support Vector Machines
7.5.3
Convergence Rate and the Quantitative Relation
between µ and ν
Here we demonstrate how the relation between the weak smoothness and
strong convexity parameters pertain to the analysis of convergence of SGD.
Suppose we have a (ν, Lν)- weakly smooth and (µ, , λµ)-strongly convex func-
tion F such that 1 ≥µ ≥ν ≥0. Note that F ∈S1, hence it is strongly convex
in the usual sense. According to Equation (7.3) and strong convexity, it is not
hard to derive that (see Lemma 7.4)
E ∥wt+1 −wopt∥2 ≤E ∥wt −wopt∥2 −2ηtE[F(wt) −F(wopt)] + η2
t G
where G is an upper bound on E ∥ˆg(wt)∥2, as assumed in Section 7.3. Then,
according to our assumptions on F,
E[F(wt) −F(wopt)] ≥
λµ
1 + µE ∥wt −wopt∥1+µ .
Therefore,
E ∥wt+1 −wopt∥2
(7.10)
≤
E ∥wt −wopt∥2 −2ηt
λµ
1 + µE ∥wt −wopt∥1+µ + η2
t G.
(7.11)
A
standard
analysis
uses
this
recurrence
relation
to
upper
bound
E ∥wT +1 −wopt∥2 by induction for all t = 1, . . . , T. Hence, Inequality (7.10)
suggests that ∥wt −wopt∥converges to zero faster if µ is smaller, i.e., if F is
more strongly-convex around wopt.
On the other hand, the weak-smoothness assumption implies
F(wt) −F(wopt) ≤
Lν
1 + ν ∥wt −wopt∥1+ν .
(7.12)
Inequality (7.12) implies that for a ﬁxed quantity ∥wt −wopt∥, F(wt)−F(wopt)
is smaller when ν is larger, i.e., if F is more smooth around wopt, demonstrat-
ing the stabilizing eﬀect of a smooth function F, as introduced at the end of
Section 7.2.2.
Combining Inequalities (7.10) and (7.12), we see that when µ −ν is small,
the convergence of F(wt)−F(wopt) should be faster. This eﬀect is illustrated in
the already known optimal convergence analysis of SGD on strongly convex
and smooth functions, in which case ν = µ = 1. We conjecture that the
optimal convergence can be achieved for any functions with ν = µ.
7.5.4
A Technical Lemma
Here, we present a technical lemma that derives some standard inequalities
(see [9], for example) by the SGD updating rule and strong convexity.

Convergence Rate of Stochastic Gradient Descent
173
Lemma 7.4.
1. Suppose the conditions in Section 7.3 hold, then ∀w ∈W,
E ∥wt+1 −w∥2 ≤(1 −ληt)E ∥wt −w∥2 −2ηtE[F(wt) −F(w)] + η2
t E ∥ˆgt∥2
2. Further assume that ∃d0 > 0 s.t. ∀wt ̸= wopt, F(wt) −F(wopt) ≥
d0 ∥wt −wopt∥. Consider SGD with step sizes ηt =
c
λt, with c ≥2. Then
for any T > 1, it holds that
E ∥wt+1 −wopt∥2 ≤(1 −ληt)E ∥wt −wopt∥2 −2ηtd0E ∥wt −wopt∥+ η2
t G
Proof. Since by the SGD update rule and by strong convexity, we have
E ∥wt+1 −w∥2 ≤E ∥wt −ηt ˆgt −w∥2
= E ∥wt −w∥2 −2ηtE⟨ˆgt, wt −w⟩+ η2
t E ∥ˆgt∥2
≤E ∥wt −w∥2 −2ηtE[(F(wt) −F(w) + λ
2 ∥wt −w∥2)] + η2
t E ∥ˆgt∥2
≤(1 −ληt)E ∥wt −w∥2 −2ηtE[F(wt) −F(w)] + η2
t E ∥ˆgt∥2 (7.13)
Since Equation (7.13) holds for any w ∈W, we substitute w by wopt, and get
E ∥wt+1 −wopt∥2
≤(1 −ληt)E ∥wt −wopt∥2 −2ηtE[F(wt) −F(wopt)] + η2
t E ∥ˆgt∥2
Since by the our assumptions, we have
F(wt) −F(wopt) ≥d0 ∥wt −wopt∥
Combining the inequalities, we get
E ∥wt+1 −wopt∥2 ≤(1 −ληt)E ∥wt −wopt∥2 −2ηtd0E ∥wt −wopt∥+ η2
t G.
7.6
Conclusion and Future Work
In conclusion, we demonstrate how the degree of strong convexity and the
degree of smoothness of a function inﬂuence the convergence rate of SGD. We
introduce a subclassiﬁcation scheme of strongly convex functions. Then we
discuss how this new scheme could help us in the convergence analysis. For
future work, we intend to analyze the convergence rate of SGD on diﬀerent
subclasses. As discussed in Section 7.5.2, we believe when µ −ν is small,
the optimal convergence is more likely to be achieved. Another direction is

174
Regularization, Optimization, Kernels, and Support Vector Machines
to use the scheme to unify existing assumptions that capture the properties
of commonly used objective functions (for example, Assumption 4.1 in [14]).
We also want to further explore how the learning rate ηt, number of averaging
terms Nt, oracle induced variance, and functional property together determine
the convergence rate of SGD.
Bibliography
[1] Alekh Agarwal, Peter L. Bartlett, Pradeep D. Ravikumar, and Martin J.
Wainwright. Information-theoretic lower bounds on the oracle complexity
of convex optimization. In NIPS, pages 1–9, 2009.
[2] Léon Bottou. Large-scale machine learning with stochastic gradient de-
scent. In Yves Lechevallier and Gilbert Saporta, editors, Proceedings of
the 19th International Conference on Computational Statistics (COMP-
STAT’2010), pages 177–187, Paris, France, August 2010. Springer.
[3] Olivier Devolder, François Glineur, and Yurii Nesterov. First-order meth-
ods of smooth convex optimization with inexact oracle. CORE Discussion
Papers, 2011.
[4] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an
optimal algorithm for stochastic strongly-convex optimization. Journal
of Machine Learning Research - Proceedings Track, 19:421–436, 2011.
[5] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent
using predictive variance reduction. In NIPS, pages 315–323, 2013.
[6] A.S. Nemirovsky and D.B. Yudin. Problem complexity and problem eﬃ-
ciency in optimization. Wiley-Interscience, 1983.
[7] B.T. Polyak and A.B. Juditsky. Acceleration of stochastic approximation
by averaging. SIAM Journal on Control and Optimization, 30(4):838–855,
1992.
[8] Maxim Raginsky and Alexander Rakhlin. Information-based complexity,
feedback and dynamics in convex programming. IEEE Transactions on
Information Theory, 57(10):7036–7056, 2011.
[9] Alexander Rakhlin and Ohad Shamir. Making gradient descent optimal
for strongly convex stochastic optimization. In Proceedings of the 29th
International Conference on Machine Learning, ICML 2012, Edinburgh,
Scotland, UK, June 26–July 1, 2012. Omnipress, 2012.

Convergence Rate of Stochastic Gradient Descent
175
[10] Aaditya Ramdas and Aarti Singh.
Optimal rates for stochastic con-
vex optimization under Tsybakov noise condition. In Proceedings of the
30th International Conference on Machine Learning, ICML 2013, At-
lanta, GA, USA, 16–21 June 2013, volume 28 of JMLR Proceedings.
JMLR.org, 2013.
[11] Tyrrell Rockafellar. Convex Analysis. Princeton University Press, Prince-
ton, NJ, USA, 1970.
[12] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal
estimated sub-gradient solver for svm. In Proceedings of the 24th Interna-
tional Conference on Machine Learning, ICML ’07, pages 807–814, New
York, NY, USA, 2007. ACM.
[13] Ohad Shamir. Open problem: Is averaging needed for strongly convex
stochastic gradient descent? Open Problems presented at COLT, 2012.
[14] Tong Zhang. Solving large scale linear prediction problems using stochas-
tic gradient descent algorithms.
In ICML 2004: Proceedings of the
Twenty-First International Conference on Machine Learning. Omnipress,
pages 919–926, 2004.
[15] Martin Zinkevich. Online convex programming and generalized inﬁnites-
imal gradient ascent. In ICML, pages 928–936, 2003.

This page intentionally left blank
This page intentionally left blank

Chapter 8
Detecting Ineﬀective Features for
Nonparametric Regression
Kris De Brabanter
Department of Statistics and Department of Computer Science, Iowa State
University
Paola Gloria Ferrario
Institut für Medizinische Biometrie und Statistik, Universität zu Lübeck
László Györﬁ
Department of Computer Science and Information Theory, Budapest Univer-
sity of Technology and Economics
8.1
Estimate the Minimum Mean Squared Error ....................
178
8.2
Tests for the Regression Problem ................................
181
8.3
A New Test
......................................................
185
8.4
Estimation of the Bayes Error Probability ......................
188
8.5
Tests for the Classiﬁcation Problem .............................
190
8.6
Conclusions .......................................................
191
Acknowledgments ................................................
192
Bibliography ......................................................
192
In the nonparametric regression setting, we investigate the hypothesis that
some components of the covariate (feature) vector are ineﬀective. Identifying
them would enable us to reduce the dimension of the feature vector, without
increasing the minimum mean squared error.
Since the proof of the asymptotical normality of the related test statistic is
fairly complicated, we present here some attempts, heuristics, and simulations
about its behavior. Finally, we show some consequences of the results for
binary pattern recognition.
This chapter is organized as follows: After some preliminary deﬁnitions and
a comparison with available results, we introduce an estimator of the minimum
mean squared error. In Section 8.2 we try to test the hypothesis that the
minimum mean squared error does not increase by leaving out an ineﬀective
component, say, the k-one, and we make a conjecture about the asymptotic
behavior of the related test statistic. In Section 8.3 we are constrained to take
177

178
Regularization, Optimization, Kernels, and Support Vector Machines
a modiﬁcation of it, by splitting the sample, in order to avoid some diﬃculties
and remain capable of testing. Here we support our conjecture by Monte
Carlo experiment. Later, in Section 8.4 we introduce another setting, where
the response variable is binary valued and in Section 8.5 we extend the test
setting and the results of the previous sections to this classiﬁcation problem.
At the end we conclude with a brief summary of the remaining open questions
and we indicate where some additional works remain still to be done.
8.1
Estimate the Minimum Mean Squared Error
Let the label Y be a real valued random variable and let the feature vector
X = (X1, . . . , Xd) be a d-dimensional random vector. The regression function
m is deﬁned by
m(x) = E{Y | X = x}.
The minimum mean squared error, also called variance of the residual Y −
m(X), is denoted by
L∗:= E{(Y −m(X))2} = min
f
E{(Y −f(X))2}.
The regression function m and the minimum mean squared error L∗cannot
be calculated when the distribution of (X, Y ) is unknown. Assume, however,
that we observe data
Dn = {(X1, Y1), . . . , (Xn, Yn)}
consisting of independent and identically distributed copies of (X, Y ). Dn
can be used to produce an estimate of L∗. For nonparametric estimates of
the minimum mean squared error L∗= E{(Y −m(X))2} see, e.g., Dudoit
and van der Laan [8], Kohler [13], Liitiäinen et al. [15], [16], Liitiäinen et
al. [17], Müller and Stadtmüller [19], Neumann [21], Pelckmans et al. [23],
Stadtmüller and Tsybakov [24], and the literature cited there (see also [9,
11, 22, 25] for related work). Devroye et al. [7] proved that without any tail
and smoothness condition, L∗cannot be estimated with guaranteed rate of
convergence. Müller, Schick, and Wefelmeyer [20] estimated L∗as the variance
of an independent measurement error Z in the model
Y = m(X) + Z
(8.1)
such that E{Z} = 0, and X and Z are independent. Sometimes this is called
the additive noise model or the homoscedastic regression model. Devroye et
al. [7] introduced an estimate of the minimum mean squared error L∗by the
modiﬁed nearest neighbor cross-validation estimate
bLn = 1
2n
n
X
i=1
(Yi −Yj(i))2,

Detecting Ineﬀective Features for Nonparametric Regression
179
where Yj(i) is the label of the modiﬁed ﬁrst nearest neighbor of Xi among
X1, . . . , Xi−1, Xi+1, . . . , Xn. We adopt the same deﬁnition of the ﬁrst nearest
neighbor, but slightly changing the notation in Xn,i,1, where
{n, i, 1} := arg min
1≤j≤n, j̸=i
ρ(Xi, Xj),
(8.2)
and ρ is a metric (typically the Euclidean one) in Rd.
The k-th nearest neighbor of Xi among X1, . . . , Xi−1, Xi+1, . . . , Xn is deﬁned
as Xn,i,k via generalization of deﬁnition (8.2):
{n, i, k} :=
arg min
1≤j≤n, j̸=i, j /∈{{n,i,1},...,{n,i,k−1}}
ρ(Xi, Xj),
(8.3)
by removing the preceding neighbors. If ties (ambiguities in the assignment
of the neighbors) occur, a simple possibility to break them is given by taking
the minimal index. On the other hand, when X has a density, the case of ties
among nearest neighbor distances occurs with probability zero, and this one
will be our setting.
If Y and X are bounded, and m is Lipschitz continuous
|m(x) −m(z)| ≤C∥x −z∥,
(8.4)
then for d ≥3, Devroye et al. [7] proved that
E{|bLn −L∗|} ≤c1n−1/2 + c2n−2/d.
(8.5)
Liitiäinen et al. [15] introduced another estimate of the minimum mean
squared error L∗by the ﬁrst and second nearest neighbor cross-validation
Ln = 1
n
n
X
i=1
(Yi −Yn,i,1)(Yi −Yn,i,2),
where Yn,i,1 and Yn,i,2 are the labels of the ﬁrst and second nearest neighbors
of Xi among X1, . . . , Xi−1, Xi+1, . . . , Xn, resp.
If Y and X are bounded and m is Lipschitz continuous, then for d ≥2
and for Ln, they proved the rate of convergence (8.5). Ferrario and Walk [10]
proved that, for bounded Y ,
Ln →L∗
almost surely (a.s.). Moreover, under the condition E{Y 2} < ∞
1
n
n
X
i=1
Li →L∗
a.s., which is a universal consistency result. Devroye et al. [5] derived a new
and simple estimator of L∗, considering the deﬁnition
L∗= E{(Y −m(X))2} = E{Y 2} −E{m(X)2}.

180
Regularization, Optimization, Kernels, and Support Vector Machines
Obviously, E{Y 2} can be estimated by 1
n
Pn
i=1 Y 2
i , while we estimate the term
E{m(X)2} by 1
n
Pn
i=1 YiYn,i,1. Therefore, L∗can be estimated by
˜Ln := 1
n
n
X
i=1
Y 2
i −1
n
n
X
i=1
YiYn,i,1.
(8.6)
They proved the strong universal consistency, and under some regularity con-
dition showed the rate (8.5). In the next section we consider the testing of
ineﬀective features, where the following limit distribution result would be use-
ful:
Conjecture 8.1.
Zn :=
1
√n
n
X
i=1
(YiYn,i,1 −E{YiYn,i,1})
is asymptotically normal with mean zero and a ﬁnite variance σ2.
The main diﬃculty here is that Zn is an average of dependent random
variables. However, this dependence has a special property, called exchange-
ability. A triangular array Vn,i, n = 1, 2, . . . , i = 1, . . . , n is called (row-wise)
exchangeable, if for any ﬁxed n and for any permutation ρ(1), . . . , ρ(n) of
1, . . . , n, the distributions of
(Vn,1, . . . , Vn,n)
and
(Vn,ρ(1), . . . , Vn,ρ(n))
are equal. There is a classical central limit theorem for exchangeable arrays:
Theorem 8.1. (Blum et al. [2], Weber [27].) Let {Vn,i} be a triangular array
of exchangeable random variables with zero mean and variance σ2. Assume
that
(i)
lim
n→∞nE{Vn,1Vn,2} = 0,
(ii)
lim
n→∞E{V 2
n,1V 2
n,2} = σ4,
(iii)
E{|Vn,1|3} = o(√n).
Then
Pn
i=1 Vn,i
√n
is asymptotically normal with mean zero and variance σ2.

Detecting Ineﬀective Features for Nonparametric Regression
181
In order to prove the asymptotic normality of Zn we may apply Theorem
8.1 for
Vn,i = YiYn,i,1 −E{YiYn,i,1}.
From the conditions of Theorem 8.1, (i) is the most crucial. Equivalently, it
requires that
Var(Vn,1) = Var(Pn
i=1 Vn,i)
n
+ o(1).
The veriﬁcation of the condition (ii) in Theorem 8.1 seems to be easier, while
(iii) is satisﬁed if Y is bounded. According to our simulations (i) is not true
such that instead of (i) we have that
nE{Vn,1Vn,2} →c,
where c ̸= 0 is a constant. This modiﬁed condition is equivalent to
Var(Pn
i=1 Vn,i)
n
→σ2 + c.
Thus, one has to generalize Theorem 1 as follows:
Conjecture 8.2. Let {Vn,i} be a triangular array of exchangeable random
variables with zero mean and variance σ2. Assume that
(i)
lim
n→∞nE{Vn,1Vn,2} = c,
(ii)
lim
n→∞E{V 2
n,1V 2
n,2} = σ4,
(iii)
E{|Vn,1|3} = o(√n).
Then
Pn
i=1 Vn,i
√n
is asymptotically normal with mean zero and variance σ2 + c.
8.2
Tests for the Regression Problem
It is of great importance to be able to estimate the minimum mean squared
error L∗accurately, even before one of the regression estimates is applied: in
a standard nonparametric regression design process, one considers a ﬁnite

182
Regularization, Optimization, Kernels, and Support Vector Machines
number of real-valued features X(I) = (Xk, k ∈I), and evaluates whether
these suﬃce to explain Y . In case they suﬃce for the given explanatory task,
an estimation method can be applied on the basis of the features already
under consideration; if not, more or diﬀerent features must be considered.
The quality of a collection of features X(I) is measured by the minimum
mean squared error
L∗(I) := E{(Y −E{Y |X(I)})2}
that can be achieved using the features as explanatory variables. L∗(I) de-
pends upon the unknown distribution of (Y, X(I)). The ﬁrst phase of any
regression estimation process therefore heavily relies on estimates of L∗(even
before a regression estimate is picked). Another way of dimension reduc-
tion would be to detect the ineﬀective components of the feature vector. Let
X(−k) = (X1, . . . , Xk−1, Xk+1, . . . , Xd) be the d−1 dimensional feature vector
such that we leave out the k-th component from X. Then the corresponding
minimum error is
L∗(−k) := E{(Y −E{Y |X(−k)})2}.
The aim of this section is to test the hypothesis
Hk : L∗(−k) = L∗,
which means that by leaving out the k-th component the minimum mean
squared error does not increase. The hypothesis Hk means that
E{E{Y | X}2} = E{E{Y | X(−k)}2}.
(8.7)
Because of the equality
E{E{Y | X}2} −E{E{Y | X(−k)}2} = E{(E{Y | X} −E{Y | X(−k)})2}
Hk happens if and only if
m(X) = E{Y | X} = E{Y | X(−k)} =: m(−k)(X(−k))
a.s.
Koepke and Bilenko [12] investigated this testing problem such that the hy-
pothesis Hk is veriﬁed from the hypothesis that Y, X(−k) and Xk are indepen-
dent. However, under this independence condition we have
E{Y | X} = E{Y | X(−k), Xk} = E{Y | X(−k)}.
Unfortunately, this independence condition is too much for our purposes, since
a component can be ineﬀective even in the case when Y, X(−k) and Xk are
dependent.
Next we introduce a new test. Using the data
D(−k)
n
= {(X(−k)
1
, Y1), . . . , (X(−k)
n
, Yn)},

Detecting Ineﬀective Features for Nonparametric Regression
183
one can estimate L∗(−k) by
˜L(−k)
n
:= 1
n
n
X
i=1
Y 2
i −1
n
n
X
i=1
YiY (−k)
n,i,1 ,
so the corresponding test statistic is
˜L(−k)
n
−˜Ln = 1
n
n
X
i=1
Yi(Yn,i,1 −Y (−k)
n,i,1 ).
We can accept the hypothesis Hk if
˜L(−k)
n
−˜Ln
(8.8)
is ”close” to zero. De Brabanter and Györﬁ[3] observed that this test statistic
is small even in the case when the hypothesis Hk is not true, since with large
probability the ﬁrst nearest neighbors of Xi and X(−k)
i
are the same, hence
Yn,i,1 −Y (−k)
n,i,1 = 0. It implies that according to the asymptotic distribution of
√n(˜L(−k)
n
−˜Ln) the zero is an atom and consequently asymptotic normality
is not possible. Their simulations showed that P(Yn,i,1 = Y (−k)
n,i,1 ) is decreasing
as n increases and increasing as d increases. In the simulations where they
considered the function
Y =
d
X
i=1
ciXi + Z,
(8.9)
X is uniform on [0, 1]d and Z ∼N(0, 1). First, they investigated how this
probability depends on the sample size for ﬁxed d = 5. Figure 8.1 shows the
percentage of Yn,i,1 = Y (−1)
n,i,1 as a function of the sample size n. Figure 8.2
shows the percentage of Yn,i,1 = Y (−1)
n,i,1 as a function of the dimensionality d
with sample size n = 2000.
Thus, De Brabanter and Györﬁ[3] modiﬁed the test statistic such that
( ˆXn,i,1, ˆX(−k)
n,i,1)
=
(
(Xn,i,1, X(−k)
n,i,1)
if
(Xn,i,1)(−k) ̸= X(−k)
n,i,1,
Ii(Xn,i,2, X(−k)
n,i,1) + (1 −Ii)(Xn,i,1, X(−k)
n,i,2)
otherwise
with
Ii =

0
with probability 1/2,
1
with probability 1/2,
and correspondingly
( ˆYn,i,1, ˆY (−k)
n,i,1 )
=
(
(Yn,i,1, Y (−k)
n,i,1 )
if
(Xn,i,1)(−k) ̸= X(−k)
n,i,1,
Ii(Yn,i,2, Y (−k)
n,i,1 ) + (1 −Ii)(Yn,i,1, Y (−k)
n,i,2 )
otherwise

184
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 8.1: Percentage of Yn,i,1 = Y (−k)
n,i,1 as a function of the sample size
n with d = 5.
FIGURE 8.2: Percentage of Yn,i,1 = Y (−k)
n,i,1 as a function of the dimension-
ality d with sample size n = 2000.
yielding the test statistic
ˆL(−k)
n
−ˆLn = 1
n
n
X
i=1
Yi( ˆYn,i,1 −ˆY (−k)
n,i,1 ).
De Brabanter and Györﬁ[3] performed some additional simulations, based on
which they conjectured that, under Hk,
√n(ˆL(−k)
n
−ˆLn)
(8.10)

Detecting Ineﬀective Features for Nonparametric Regression
185
is asymptotically normal with mean zero and ﬁnite variance. Such a result
would be a surprise, since in this case the smoothness of the regression function
m and the dimension d do not count. Under the alternative hypothesis Hc
k,
one can prove that
lim
n→∞
1
n
n
X
i=1
Yi( ˆYn,i,1 −ˆY (−k)
n,i,1 ) = E{(m(X) −m(−k)(X(−k)))2} > 0 a.s.
8.3
A New Test
The estimate ˆLn is fairly complicated such that the proof of (8.10) seems
to be intractable.
Therefore we introduce another test based on the estimate (8.6). In the
previous section, we illustrated that the test statistic (8.8) has an atom
at zero. This diﬃculty can be avoided if we apply data splitting. Split
therefore the data D2n = {(X1, Y1), ..., (X2n, Y2n)} into two sub-samples
Dn = {(X1, Y1), ..., (Xn, Yn)} and D′
n = {(X′
1, Y ′
1), ..., (X′
n, Y ′
n)} where
(X′
i, Y ′
i ) = (Xn+i, Yn+i),
i = 1, . . . , n. One can estimate L∗(−k) by
˜L′(−k)
n
:= 1
n
n
X
i=1
Y 2
i −1
n
n
X
i=1
Y ′
i Y ′(−k)
n,i,1 .
(8.11)
The diﬀerence between the estimates (8.11) and (8.6)
Tn := 1
n
n
X
i=1
YiYn,i,1 −1
n
n
X
i=1
Y ′
i Y ′(−k)
n,i,1 .
(8.12)
can be a test statistic, which avoids the problem of an atom at zero, since the
two averages in (8.12) are independent.
If Conjecture 8.1 holds, then under Hk
√n(Tn −E{Tn})
is asymptotically normal with mean zero and variance 2σ2. Moreover, us-
ing the fact that m(X) = m(−k)(X(−k)), we guess that under the Lipschitz

186
Regularization, Optimization, Kernels, and Support Vector Machines
continuity of m
√nE{Tn} =
1
√n
n
X
i=1
E{YiYn,i,1 −Y ′
i Y ′(−k)
n,i,1 }
= √nE{m(X1)m(Xn,1,1) −m(X′
1)m(−k)(X′(−k)
n,1,1 )}
= √nE{m(X1)(m(Xn,1,1) −m(−k)(X(−k)
n,1,1))}
≈0.
Because of independence,
Var(Y1Yn,1,1 −Y ′
1Y ′(−k)
n,1,1 ) = Var(Y1Yn,1,1) + Var(Y ′
1Y ′(−k)
n,1,1 ).
Introduce the notation
M2(x) = E{Y 2 | X = x}.
Then
Var(Y1Yn,1,1) = E{Y 2
1 Y 2
n,1,1} −E{m(X1)m(Xn,1,1)}2
= E{M2(X1)M2(Xn,1,1)} −E{m(X1)m(Xn,1,1)}2
→E{M2(X1)2} −E{m(X1)2}2,
as n →∞. Similarly,
Var(Y ′
1Y ′(−k)
n,1,1 ) →E{M (−k)
2
(X(−k)
1
)2} −E{m(−k)(X(−k)
1
)2}2,
where
M (−k)
2
(x(−k)) = E{Y 2 | X(−k) = x(−k)}.
If n-times the covariances of the terms in Tn are asymptotically zero, then
under Hk
Var(√nTn) ≈E{M2(X1)2} + E{M (−k)
2
(X(−k)
1
)2} −2E{m(X1)2}2
= 2E{M2(X1)2} −2E{m(X1)2}2
=: 2σ2.
This asymptotic variance can be estimated by
σ2
n := 2
n
n
X
i=1
Y 2
i Y 2
n,i,1 −2
 
1
n
n
X
i=1
YiYn,i,1
!2
.
For an 0 < α < 1, an asymptotically α-level test accepts the hypothesis
Hk if
T ′
n :=
√nTn
σn
≤Φ−1(1 −α),

Detecting Ineﬀective Features for Nonparametric Regression
187
where Φ denotes the standard normal distribution function.
In order to verify the asymptotic normality of the test statistic T ′
n we con-
ducted the following Monte Carlo experiment. Consider the regression function
(8.9) with c2 = 0, c1 = c3 = c4 = 1, X is uniform on [0, 1]4 and Z ∼N(0, 1).
The sample size is set to n = 1000.
The result of the simulation is represented in Figure 8.3. The density his-
togram shows the test statistic T ′
n based on 10,000 repetitions. We also ﬁtted
the best normal density (using maximum likelihood estimation) together with
the standard normal density. It shows that the distribution of T ′
n is approx-
imately normal with mean zero, but the variance is larger than 1. In fact
σ2
n = 5.06 and the asymptotic variance of √nTn is σ′
n
2 = 6.07.
Thus, the condition (i) of Theorem 8.1 is not satisﬁed, and therefore there
is a need to prove its extension (Conjecture 8.2).
FIGURE 8.3: Test statistic T ′
n with best normal ﬁt and standard normal
distributed ﬁt.
Under the alternative hypothesis Hc
k, the strong universal consistency result
of Devroye et al. [5] implies that
lim
n→∞Tn
=
E{m(X)2} −E{m(−k)(X(−k))2}
=
E{(m(X) −m(−k)(X(−k)))2} > 0
a.s.
The approximate calculations above and the simulation results lead to the
following conjecture:
Conjecture 8.3. If X and Y are bounded, X has a density, and m is Lipschitz
continuous, then √nTn is asymptotically normal with mean zero and variance
2 (σ′)2 such that σ < σ′.
We considered the following nonlinear function with 5 uniformly dis-
tributed inputs on [0, 1]5 with n = 4000: Y = sin(2πX1) cos(πX2) + ε, with

188
Regularization, Optimization, Kernels, and Support Vector Machines
ε ∼N(0, 0.12). The simulation results (based on 1000 runs) are summarized
in Table 8.1. The signiﬁcance level is set to 0.05.
TABLE 8.1: Appearance frequency of the variables.
X1
X2
X3
X4
X5
982
988
42
44
46
We performed some simulations testing whether the k-th component is
ineﬀective (k = 1, . . . , 5). Table 8.1 conﬁrms our conjectures on asymptotic
normality. We have to emphasize that the rate of convergence of the distribu-
tion of normality depends on d and on the actual regression function.
8.4
Estimation of the Bayes Error Probability
Again, let the feature vector X = (X1, . . . , Xd) be a d-dimensional random
vector such that its distribution is denoted by µ, and let the label Y be
binary valued. If g is an arbitrary decision function then its error probability
is denoted by
R(g) = P{g(X) ̸= Y }.
Put
m(x) = P{Y = 1 | X = x} = E{Y | X = x}.
The Bayes decision g∗minimizes the error probability:
g∗(x) =
 1
if m(x) > 1/2
0
otherwise,
and
R∗= P{g∗(X) ̸= Y } = E{min[m(X), 1 −m(X)]}
denotes its error probability. Put
D(x) = 2m(x) −1
then the Bayes decision has the following equivalent form:
g∗(x) =
 1
if D(x) > 0
0
otherwise.
In the standard model of pattern recognition, we are given training labelled
samples, which are independent and identical copies of (X, Y ): (X1, Y1), . . .
, (Xn, Yn). Based on these labeled samples, one can estimate the regression

Detecting Ineﬀective Features for Nonparametric Regression
189
function D by Dn, and the corresponding plug-in classiﬁcation rule is deﬁned
by
gn(x) =

1
if Dn(x) > 0
0
otherwise.
Then
E{R(gn)} −R∗≤E{|D(X) −Dn(X)|}
(cf. Theorem 2.2 in Devroye, Györﬁ, Lugosi [6]), therefore we may get an
upper bound on the rate of convergence of E{R(gn)} −R∗via the L1 rate of
convergence of the corresponding regression estimation. However, according
to Section 6.7 in Devroye, Györﬁ, Lugosi [6], the classiﬁcation is easier than
L1 regression function estimation, since the rate of convergence of the error
probability depends on the behavior of the function D in the neighborhood of
the decision boundary
B = {x; D(x) = 0}.
This phenomenon has been discovered by Mammen and Tsybakov [18], Tsy-
bakov [26], who formulated the so called margin condition:
• The strong margin condition. Assume that for all 1 ≥t > 0,
P {|D(X)| ≤t} ≤ctα,
(8.13)
where α > 0.
Kohler and Krzyzak [14] introduced the weak margin condition:
• The weak margin condition. Assume that for all 1 ≥t > 0,
E

I{|D(X)|≤t}|D(X)|
	
≤ct1+α.
(8.14)
where α > 0 and I denotes the indicator function.
Obviously, the strong margin condition implies the weak margin condition:
E

I{|D(X)|≤t}|D(X)|
	
≤E

I{|D(X)|≤t}t
	
= tP {|D(X)| ≤t} ≤ct · tα.
The reverse is not true, for example, let X be uniformly distributed on [−1, 1]
and put
D(x) = (x −t0)+ −(−t0 −x)+,
where 0 < t0 < 1 and x+ denotes the positive part of x. Then on the one
hand, for 0 < t < 1 −t0,
P {|D(X)| ≤t} = 2P

0 ≤X, (X −t0)+ ≤t
	
= t + t0,
which does not tend to 0 as t →0, so the strong margin condition is not
satisﬁed. On the other hand,
E

I{|D(X)|≤t}|D(X)|
	
= 2E

I{0≤X, (X−t0)+≤t}(X −t0)+|
	
= t2/2,

190
Regularization, Optimization, Kernels, and Support Vector Machines
therefore the weak margin condition is satisﬁed with α = 1 and c = 1/2.
Audibert and Tsybakov [1] proved that if the plug-in classiﬁer g has been
derived from the regression estimate ˜D and the strong margin condition is
satisﬁed, then
R(g) −R∗≤
Z
( ˜D(x) −D(x))2µ(dx)
 1+α
2+α
.
(8.15)
It is easy to see that (8.15) holds even under weak margin conditions. If D
is Lipschitz continuous and X is bounded then there are regression estimates
such that
E
Z
(Dn(x) −D(x))2µ(dx)

≤c2
2n−
2
d+2 ,
therefore for the corresponding plug in rule gn, (8.15) implies that
E{R(gn)} −R∗≤

c2
2n−
2
d+2
 1+α
2+α =

c2
2n−1+α
d+2

2
2+α .
Kohler and Krzyzak [14] proved that for the standard local majority classiﬁ-
cation rules (partitioning, kernel, nearest neighbor) we get that the order of
the upper bound can be smaller:
n−1+α
d+2 .
(8.16)
For the time being, there is a fast estimate of the Bayes error probability
R∗. One can estimate R∗using data splitting as follows: For the data D2n =
{(X1, Y1), ..., (X2n, Y2n)} let gn be a pattern recognition rule based on the ﬁrst
n observations Dn = {(X1, Y1), ..., (Xn, Yn)} (training data). An estimate of
R∗can be obtained by the relative frequency of errors for gn on the remaining
samples (test data). More precisely, we consider the estimate
¯R2n = 1
n
2n
X
i=n+1
I{gn(Xi)̸=Yi}.
Obviously, the rate of convergence of ¯R2n to R∗depends on the quality of
the classiﬁcation rule gn. If D is Lipschitz continuous and X is bounded then
(8.16) implies that
E{| ¯R2n −R∗|} ≤E{| ¯R2n −R(gn)|} + E{R(gn)} −R∗≤n−1/2 + c3n−1+α
d+2 .
How to estimate R∗with faster rate of convergence is an open problem.
8.5
Tests for the Classiﬁcation Problem
For pattern recognition, the dimension reduction is to detect the ineﬀective
components of the feature vector. As before, let X(−k) be the d−1 dimensional

Detecting Ineﬀective Features for Nonparametric Regression
191
feature vector such that we leave out the k-th component from X. Then the
corresponding Bayes error probability is
R∗(−k) := E{min[E{Y | X(−k)}, 1 −E{Y | X(−k)}]}.
Our aim is to test the hypothesis
Hk : R∗(−k) = R∗,
which means that leaving out the k-th component the Bayes error probability
does not increase. The hypothesis Hk means that
E{min[E{Y | X(−k)}, 1 −E{Y | X(−k)}]}
=
E{min[E{Y | X}, 1 −E{Y | X}]}.
Obviously, if
m(X) = E{Y | X} = E{Y | X(−k)} =: m(−k)(X(−k))
(8.17)
a.s. then Hk is satisﬁed. The function min is not strictly concave, therefore
the reverse is not true, i.e., Hk does not imply (8.17). We have no solution
for this testing problem. Instead we modify the hypothesis Hk such that the
Bayes error probability is replaced by the asymptotic error probability of the
ﬁrst nearest neighbor classiﬁcation rule:
RNN = E{E{Y | X}(1 −E{Y | X})}.
(cf. Cover, Hart [4]). Because of
RNN = E{Y } −E{E{Y | X}2},
the modiﬁed hypothesis is deﬁned by
E{m(X)2} = E{m(−k)(X(−k))2}.
(8.18)
The hypotheses (8.7) and (8.18) are the same, therefore all results of subsec-
tions 2, 3, 4 can be applied for the more restrictive hypothesis (8.18) such that
here Y is binary valued.
8.6
Conclusions
In this chapter we introduced diﬀerent statistics, in order to identify if
some components of the independent vector X are ineﬀective. Since we could
only simulate the asymptotic behavior of the test statistic, the complete chain
of proof remains an open topic for future works. It would also be interesting
to ﬁnd other strategies to avoid the splitting of the sample.

192
Regularization, Optimization, Kernels, and Support Vector Machines
Acknowledgments
This work was partially supported by the European Union and the Euro-
pean Social Fund through project FuturICT.hu (grant no.: TAMOP-4.2.2.C-11
/1 /KONV-2012-0013).
Bibliography
[1] J-Y. Audibert and A. B. Tsybakov. Fast learning rates for plug-in classi-
ﬁers. Annals of Statistics, 35:608–633, 2007.
[2] J. R. Blum, H. Chernoﬀ, M. Rosenblatt, and H. Teicher. Central limit
theorems for interexchangeable processes. Canadian J. of Mathematics,
10:222–2229, 1958.
[3] K. De Brabanter and L. Györﬁ. Feature Selection via Detecting In-
eﬀective Features. In International Workshop on Advances in Reg-
ularization, Optimization, Kernel Methods and Support Vector Ma-
chines: Theory and Applications. July 8–10, 2013, Leuven, Belgium.
ftp://inmmic.org/sista/kdebaban/13-21.pdf
[4] T. M. Cover and P. E. Hart. Nearest neighbor pattern classiﬁcation. IEEE
Transactions on Information Theory, IT-13:21–27, 1967.
[5] L. Devroye, P. G. Ferrario, L. Györﬁ, and H. Walk. Strong universal
consistent estimate of the minimum mean squared error. In Empirical
Inference - Festschrift in Honor of Vladimir N. Vapnik, ed. by Bernhard
Schölkopf, Zhiyuan Luo, and Vladimir Vovk, Springer, Heidelberg.
[6] L. Devroye, L. Györﬁ, and G. Lugosi. A Probabilistic Theory of Pattern
Recognition. Springer–Verlag, New York, 1996.
[7] L. Devroye, D. Schäfer, L. Györﬁ, and H. Walk. The estimation problem
of minimum mean squared error. Statistics and Decisions, 21:15–28, 2003.
[8] S. Dudoit and M.J. van der Laan. Asymptotics of cross-validated risk
estimation in estimator selection and performance assessment. Statistical
Methodology, 2:131–154, 2005.
[9] B. Efron and C. Stein. The jackknife estimate of variance. Annals of
Statistics, 9:586–596, 1981.

Detecting Ineﬀective Features for Nonparametric Regression
193
[10] P.G. Ferrario and H. Walk. Nonparametric partitioning estimation of
residual and local variance based on ﬁrst and second nearest neighbors.
Journal of Nonparametric Statistics, 24:1019–1039, 2012.
[11] L. Györﬁ, M. Kohler, A. Krzyżak, and H. Walk. A Distribution-Free The-
ory of Nonparametric Regression. Springer, New York, 2002.
[12] H. Koepke and M. Bilenko. Fast prediction of new feature utility.
http://arxiv.org/ftp/arxiv/papers/1206/1206.4680.pdf
[13] M. Kohler. Nonparametric regression with additional measurement errors
in the dependent variable. Journal of Statistical Planning and Inference,
136:3339–3361, 2006.
[14] M. Kohler and A. Krżyzak. On the rate of convergence of local averaging
plug-in classiﬁcation rules under a margin condition. IEEE Transactions
on Information Theory, 53:1735–1742, 2007.
[15] E. Liitiäinen, F. Corona, and A. Lendasse. On nonparametric residual
variance estimation. Neural Processing Letters, 28:155–167, 2009.
[16] E. Liitiäinen, F. Corona, and A. Lendasse. Residual variance estima-
tion using a nearest neighbor statistic. Journal of Multivariate Analysis,
101:811–823, 2010.
[17] E. Liitiäinen, M. Verleysen, F. Corona, and A. Lendasse. Residual vari-
ance estimation in machine learning. Neurocomputing, 72:3692–3703,
2009.
[18] E. Mammen and A. B. Tsybakov. Smooth discrimination analysis. Annals
of Statistics, 27:1808–1829, 1999.
[19] H.-G. Müller and U. Stadtmüller. Estimation of heteroscedasticity in re-
gression analysis. Annals of Statistics, 15:610–625, 1987.
[20] U. Müller, A. Schick, and W. Wefelmeyer. Estimating the error variance
in nonparametric regression by a covariate-matched U-statistic. Statis-
tics, 37:179–188, 2003.
[21] M.-H. Neumann. Fully data-driven nonparametric variance estimators.
Statistics, 25:189–212, 1994.
[22] N. Neumeyer and I. Van Keilegom. Estimating the error distribution in
nonparametric multiple regression with applications to model testing.
Journal of Multivariate Analysis, 101:1067–1078, 2010.
[23] K. Pelckmans, J. De Brabanter, J.A.K. Suykens, and B. De Moor. The
diﬀerogram: Non-parametric noise variance estimation and its use for
model selection. Neurocomputing, 69:100–122, 2005.

194
Regularization, Optimization, Kernels, and Support Vector Machines
[24] U. Stadtmüller and A. Tsybakov. Nonparametric recursive variance esti-
mation. Statistics, 27:55–63, 1995.
[25] C.J. Stone. Consistent nonparametric regression. Annals of Statistics,
5:595–645, 1977.
[26] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning.
Annals of Statistics, 32:135–166, 2004.
[27] N. C. Weber. A martingale approach to central limit theorems for ex-
changeable random variables. Journal of Applied Probability, 17:662–673,
1980.

Chapter 9
Quadratic Basis Pursuit
Henrik Ohlsson
Department of Electrical Engineering and Computer Sciences, University of
California, Berkeley
Allen Y. Yang
Department of Electrical Engineering and Computer Sciences, University of
California, Berkeley
Roy Dong
Department of Electrical Engineering and Computer Sciences, University of
California, Berkeley
Michel Verhaegen
Delft Center for Systems and Control, Delft University
S. Shankar Sastry
Department of Electrical Engineering and Computer Sciences, University of
California, Berkeley
9.1
Introduction ......................................................
196
9.1.1
Literature Review .......................................
197
9.1.2
Notation .................................................
198
9.2
Quadratic Basis Pursuit .........................................
199
9.2.1
Convex Relaxation via Lifting ...........................
199
9.2.2
Theoretical Analysis .....................................
200
9.3
Numerical Algorithms ............................................
202
9.4
Experiments ......................................................
205
9.4.1
Nonlinear Compressive Sensing in Real Domain ........
206
9.4.2
The Shepp-Logan Phantom .............................
208
9.4.3
Subwavelength Imaging
.................................
209
9.5
Conclusion ........................................................
210
Acknowledgments ................................................
211
Bibliography ......................................................
212
195

196
Regularization, Optimization, Kernels, and Support Vector Machines
9.1
Introduction
Consider the problem of ﬁnding the sparsest signal x satisfying a system
of linear equations:
min
x∈ℜn
∥x∥0
subj. to
yi = bT
i x,
yi ∈ℜ, bi ∈ℜn, i = 1, . . . , N.
(9.1)
This problem is known to be combinatorial and NP-hard [45]. A number of
approaches to approximate its solution have been proposed. One of the most
well known approaches is to relax the zero norm and replace it with the ℓ1-
norm:
min
x∈ℜn ∥x∥1
subj. to
yi = bT
i x,
i = 1, . . . , N.
(9.2)
This approach is often referred to as basis pursuit (BP) [19].
The ability to recover the optimal solution to (9.1) is essential in the theory
of compressive sensing (CS) [16, 24]. A tremendous amount of work has been
dedicated to solving and analyzing the solution of (9.1) and (9.2) in the last
decade. Today CS is regarded as a powerful tool in signal processing and
widely used in many applications. For a detailed review of the literature, the
reader is referred to several recent publications such as [10, 27].
It has recently been shown that CS can be extended to nonlinear models.
More speciﬁcally, the relatively new topic of nonlinear compressive sensing
(NLCS) deals with a more general problem of ﬁnding the sparsest signal x to
a nonlinear set of equations:
min
x∈ℜn
∥x∥0
subj. to
yi = fi(x),
yi ∈ℜ, i = 1, . . . , N,
(9.3)
where each fi : ℜn →ℜis a continuously diﬀerentiable function. Compared
to CS, the literature on NLCS is still very limited. The interested reader is
referred to [3, 7] and references therein.
In this chapter, we will restrict our attention from general nonlinear sys-
tems. Instead, we focus on nonlinearities that depends quadratically on the
unknown x. More speciﬁcally, we consider the following problem formulated
in the complex domain:
min
x∈Cn
∥x∥0
subj. to
yi = ai + bH
i x + xHci + xHQix,
i = 1, . . . , N,
(9.4)
where ai ∈C, bi, ci ∈Cn, yi ∈C, Qi ∈Cn×n, i = 1, . . . , N, and H denotes
the conjugate transpose. In a sense, being able to solve (9.4) would make

Quadratic Basis Pursuit
197
it possible to apply the principles of CS to a second-order Taylor expansion
of the nonlinear relationship in (9.3), while traditional CS mainly considers
its linear approximation or ﬁrst-order Taylor expansion. In particular, in the
most simple case, when a second order Taylor expansion is taken around
zero (i.e., a Maclaurin expansion), let ai = fi(0), bi = ci = ∇T
xfi(0)/2 and
Qi = ∇2
xfi(0)/2, i = 1, . . . , N, with ∇x and ∇2
x denoting the gradient and
Hessian with respect to x. In this case, Q is a Hermitian matrix. Nevertheless,
we note that our derivations in the chapter do not depend on the matrix Q
to be symmetric in the real domain or Hermitian in the complex domain.
In another motivating example, we consider the well-known phase retrieval
problem in x-ray crystallography; see for instance [36, 33, 31, 29, 41, 2]. The
underlying principle of x-ray crystallography is that the information about
the crystal structure can be obtained from its diﬀraction pattern by hitting
the crystal by an x-ray beam. Due to physical limitations, typically only the
intensity of the diﬀraction pattern can be measured but not its phase. This
leads to a nonlinear relation
yi = |aH
i x|2 = xHaiaH
i x,
i = 1, . . . , N,
(9.5)
between the measurements y1, . . . , yN ∈ℜand the structural information
contained in x ∈Cn. The complex vectors a1, . . . , aN ∈Cn are known. The
mathematical problem of recovering x from y1, . . . , yN, and a1, . . . , aN is re-
ferred to as the phase retrieval (PR) problem. The traditional phase retrieval
problem is known to be combinatorial [17].
If x is sparse under an appropriate basis in (9.5) and the measurements
are under-sampled, the problem is referred to as compressive phase retrieval
(CPR) in [44, 49] or quadratic compressed sensing (QCS) in [53]. Compressive
phase retrieval is of relevance to several important imaging applications, such
as diﬀraction imaging [11], astronomy [21, 30], optics [57], x-ray tomography
[23], microscopy [43, 1, 55], and quantum mechanics [20], to mention a few.
As we will later show, our solution as a convex relaxation of (9.4), called
quadratic basis pursuit (QBP), can be readily applied to solve this type of
problem, namely, let ai = bi = ci = 0, Qi = aiaH
i , i = 1, . . . , N.
9.1.1
Literature Review
Overall, the literature on nonlinear sparse problems and NLCS is very
limited. One of the ﬁrst papers discussing these topics is [8]. The authors pre-
sented a greedy gradient based algorithm for estimating the sparsest solution
to a general nonlinear equation system. A greedy approach was also proposed
in [38] for the estimation of sparse solutions of nonlinear equation systems.
The work of [3] proposed several iterative hard-thresholding and sparse sim-
plex pursuit algorithms. As the algorithms are nonconvex greedy solutions, the
analysis of the theoretical convergence only concerns their local behavior. In
[7] and [26], the authors also considered a generalization of the restricted isom-

198
Regularization, Optimization, Kernels, and Support Vector Machines
etry property (RIP) to support the use of similar iterative hard-thresholding
algorithms and orthogonal least squares for solving general NLCS problems.
The work presented in this chapter is inspired by several recent papers on
CS applied to the phase retrieval problem [44, 42, 18, 53, 49, 50, 55, 35, 51, 52].
In particular, the generalization of compressive sensing to phase retrieval was
ﬁrst proposed in [44]. In [53], the problem was also referred to as QCS. These
methods typically do not consider a general quadratic constraint as in (9.4)
but a pure quadratic form (i.e., ai = bi = ci = 0, i = 1, . . . , N, in (9.4)).
In terms of the numerical algorithms that solve the CPR problem, most of
the existing methods are greedy algorithms, where a solution to the underlying
non-convex problem is sought by a sequence of local decisions [44, 42, 53, 50,
55, 52]. In particular, the QCS algorithm in [53] used a lifting technique similar
to that in [54, 40, 46, 32] and iterative rank minimization resulting in a series
of semideﬁnite programs (SDPs) that would converge to a local optimum.
The ﬁrst work that applied the lifting technique to the PR and CPR
problems was presented in [18]. Extensions of similar techniques were also
studied in [39, 35]. The methods presented in our previous publications [48, 49]
were also based on the lifting technique. It is important to note that the
algorithms proposed in [18, 48, 49] are non-greedy global solutions, which are
diﬀerent from the previous local solutions [44, 53]. The technique presented
in this chapter was inspired by the solutions to phase retrieval via low-rank
approximation in [18, 17, 13]. Given an oversampled phase retrieval problem, a
lifting technique was used to relax the nonconvex problem with an SDP. The
authors of [17, 13] also derived an upper-bound for the sampling rate that
guarantees exact recovery in the noise-free case and stable recovery in the
noisy case. Nevertheless, the work in [17, 13] only addressed the oversampled
phase retrieval problem but not CPR or NLCS. The only similarities between
our work and theirs are the lifting technique and convex relaxation. This lifting
technique has also been used in other topics to convert nonconvex quadratic
problems to SDPs; see for instance [56, 35]. The work presented in [18] and
our previous contributions [48, 49] only discussed the CPR problem.
Finally, in [51], a message passing algorithm similar to that in CS was
proposed to solve the compressive phase retrieval problem. The work in [28, 47]
further considered stability and uniqueness in real phase retrieval problems.
CPR has also been shown useful in practice and we refer the interested reader
to [44, 55] for two very nice contributions. We ﬁnd the work presented in
[55] especially fascinating where the authors show how CPR can be used to
facilitate sub-wavelength imaging in microscopy.
9.1.2
Notation
In this chapter, we will use boldface to denote vectors and matrices and
normal font for scalars. Xi,j is used to denote the (i, j)th element, Xi,: the
ith row, and X:,j the jth column of a matrix X, respectively. We will use
the notation Xi1:i2,j1:j2 to denote a submatrix constructed from rows i1 to

Quadratic Basis Pursuit
199
i2 and columns j1 to j2 of X. Given two matrices X and Y , we use the
following fact that their product in the trace function commutes, namely,
Tr(XY ) = Tr(Y X), under the assumption that the dimensions match. ∥· ∥0
counts the number of nonzero elements in a vector or matrix; similarly, ∥· ∥1
denotes the element-wise ℓ1-norm of a vector or matrix, i.e., the sum of the
magnitudes of the elements; whereas ∥· ∥represents the ℓ2-norm for vectors
and the Frobenius norm for matrices. For a quadratic matrix X, X ⪰0 is
used to denote that X is positive semi-deﬁnite.
9.2
Quadratic Basis Pursuit
9.2.1
Convex Relaxation via Lifting
As optimizing the ℓ0-norm function in (9.4) is known to be a combinatorial
problem, in this section, we ﬁrst introduce a convex relaxation of (9.4).
It is easy to see that the general quadratic constraint of (9.4) can be
rewritten as the quadratic form:
yi =

1
xH 
ai
bH
i
ci
Qi
 1
x

∈C,
i = 1, . . . , N.
(9.6)
Since each yi is a scalar, we further have
yi = Tr

1
xH 
ai
bH
i
ci
Qi
 1
x

(9.7)
= Tr

ai
bH
i
ci
Qi
 1
x
 
1
xH
.
(9.8)
Deﬁne Φi =

ai
bH
i
ci
Qi

and X =
1
x
 
1
xH
, both matrices of dimensions
(n + 1) × (n + 1). The operation that constructs X from the vector
1
x

is
known as the lifting operator [54, 40, 46, 32]. By deﬁnition, X is a Hermitian
matrix, and it satisﬁes the constraints that X1,1 = 1 and rank(X) = 1. Hence,
(9.4) can be rewritten as
minX
∥X∥0
subj. to
yi = Tr(ΦiX),
i = 1, . . . , N,
rank(X) = 1, X1,1 = 1, X ⪰0.
(9.9)
When the optimal solution X∗is found, the unknown x can be obtained by
the rank-1 decomposition of X∗via singular value decomposition (SVD).
The above problem is still non-convex and combinatorial. Therefore, solv-
ing it for any moderate size of n is impractical. Inspired by recent literature on

200
Regularization, Optimization, Kernels, and Support Vector Machines
matrix completion [15, 18, 17, 13] and sparse PCA [22], we relax the problem
into the following convex non-smooth semideﬁnite program (SDP):
minX
Tr(X) + λ∥X∥1
subj. to
yi = Tr(ΦiX),
i = 1, . . . , N,
X1,1 = 1, X ⪰0,
(9.10)
where λ ≥0 is a design parameter. In particular, the trace of X is a convex
surrogate of the low-rank condition and ∥X∥1 is the well-known convex sur-
rogate for ∥X∥0 in (9.9). We refer to the approach as quadratic basis pursuit
(QBP).
One can further consider a noisy counterpart of the QBP problem, where
some deviation between the measurements and the estimates is allowed.
More speciﬁcally, we propose the following quadratic basis pursuit denoising
(QBPD) problem:
minX
Tr(X) + λ∥X∥1
subj. to
PN
i
 yi −Tr(ΦiX)
2 ≤ε,
X1,1 = 1, X ⪰0,
(9.11)
for some ε > 0.
9.2.2
Theoretical Analysis
In this section, we highlight some theoretical results derived for QBP. The
analysis follows that of CS, and is inspired by derivations given in [17, 16,
18, 24, 12, 4, 10]. For further analysis on special cases of QBP and its noisy
counterpart QBPD, please refer to [49].
First, it is convenient to introduce a linear operator B:
B : X ∈Cn×n 7→{Tr(ΦiX)}1≤i≤N ∈CN.
(9.12)
We consider a generalization of the restricted isometry property (RIP) of the
linear operator B.
Deﬁnition 9.1 (RIP). A linear operator B(·) as deﬁned in (9.12) is (ϵ, k)-
RIP if

∥B(X)∥2
∥X∥2
−1
 < ϵ
(9.13)
for all ∥X∥0 ≤k and X ̸= 0.
We can now state the following theorem:
Theorem 9.1 (Recoverability/Uniqueness). Let ¯x ∈Cn be a solution
to (9.4). If X∗∈C(n+1)×(n+1) satisﬁes y = B(X∗), X∗⪰0, rank(X∗) =
1, X∗
1,1 = 1 and if B(·) is a (ϵ, 2∥X∗∥0)-RIP linear operator with ϵ < 1 then
X∗and ¯x are unique and X∗
2:n+1,1 = ¯x.

Quadratic Basis Pursuit
201
Proof. Assume the contrary, i.e., X∗
2:n+1,1 ̸= ¯x and hence that X∗̸=
1
¯x
 
1
¯xH
.
It
is
clear
that

1
¯x
 
1
¯xH
0
≤
∥X∗∥0
and
hence

1
¯x
 
1
¯xH
−X∗

0
≤2∥X∗∥0. Since

1
¯x
 
1
¯xH
−X∗

0
≤2∥X∗∥0, we
can apply the RIP inequality (9.13) to
1
¯x
 
1
¯xH
−X∗. If we use that y =
B(X∗) = B

1
¯x
 
1
¯xH
and hence B

1
¯x
 
1
¯xH
−X∗

= 0, we are
led to the contradiction 1 < ϵ. We therefore conclude that X∗=
1
¯x
 
1
¯xH
,
X∗
2:n+1,1 = ¯x, and that X∗and ¯x are unique.
We can also give a bound on the sparsity of ¯x:
Theorem 9.2 (Bound on ∥¯x∥0 from above). Let ¯x be the sparsest solution
(or one of the solutions if the sparsest solution is not unique) to (9.4) and let
˜
X be a solution of QBP (9.10). If ˜
X has rank 1 then ∥˜
X2:n+1,1∥0 ≥∥¯x∥0.
Proof. Let ˜
X be a rank-1 solution of QBP (9.10). By contradiction, assume
∥˜
X2:n+1,1∥0 < ∥¯x∥0. Since ˜
X2:n+1,1 satisﬁes the constraints of (9.4), it is a
feasible solution of (9.4). As assumed, ˜
X2:n+1,1 also gives a lower objective
value than ¯x in (9.4). This is a contradiction since ¯x was assumed to be a
solution of (9.4). Hence we must have that ∥˜
X2:n+1,1∥0 ≥∥¯x∥0.
The following result now holds trivially:
Corollary 9.1 (Guaranteed recovery using RIP). Let ¯x be the sparsest
solution to (9.4). The solution of QBP ˜
X is equal to

1
¯x
 
1
¯xH
if it has
rank 1 and B(·) is (ϵ, 2∥˜
X∥0)-RIP with ϵ < 1.
Proof. This follows trivially from Theorem 9.1 by realizing that ˜
X satisfy all
properties of X∗.
Given the RIP analysis, it may be that the linear operator B(·) does not
satisfy the RIP property deﬁned in Deﬁnition 9.1 with a small enough ϵ, as
pointed out in [17]. In these cases, RIP-1 may be considered:
Deﬁnition 9.2 (RIP-1). A linear operator B(·) is (ϵ, k)-RIP-1 if

∥B(X)∥1
∥X∥1
−1
 < ϵ
(9.14)
for all matrices X ̸= 0 and ∥X∥0 ≤k.
Theorems 9.1–9.2 and Corollary 9.1 all hold with RIP replaced by RIP-
1 and will not be restated in detail here. Instead, we summarize the most
important property in the following theorem:

202
Regularization, Optimization, Kernels, and Support Vector Machines
Theorem 9.3 (Upper bound and recoverability using RIP-1). Let ¯x
be the sparsest solution to (9.4). The solution of QBP (9.10), ˜
X, is equal to
1
¯x
 
1
¯xH
if it has rank 1 and B(·) is (ϵ, 2∥˜
X∥0)-RIP-1 with ϵ < 1.
Proof. The proof follows trivially from the proof of Theorem 9.1.
The RIP-type argument may be diﬃcult to check for a given matrix and
is more useful for claiming results for classes of matrices/linear operators. For
instance, it has been shown that random Gaussian matrices satisfy the RIP
with high probability. However, given realization of a random Gaussian matrix,
it is indeed diﬃcult to check if it actually satisﬁes the RIP. Two alternative
arguments are the spark condition [19] and the mutual coherence [25, 14]. The
spark condition usually gives tighter bounds but is known to be diﬃcult to
compute as well. On the other hand, mutual coherence may give less tight
bounds, but is more tractable. We will focus on mutual coherence, which is
deﬁned as:
Deﬁnition 9.3 (Mutual coherence). For a matrix A, deﬁne the mutual
coherence as
µ(A) =
max
1≤i,j≤n,i̸=j
|AH
:,iA:,j|
∥A:,i∥∥A:,j∥.
(9.15)
Let B be the matrix satisfying y = BXs = B(X) with Xs being the
vectorized version of X. We are now ready to state the following theorem:
Theorem 9.4 (Recovery using mutual coherence). Let ¯x be the sparsest
solution to (9.4). The solution of QBP (9.10), ˜
X, is equal to
1
¯x
 
1
¯xH
if
it has rank 1 and ∥˜
X∥0 < 0.5(1 + 1/µ(B)).
Proof. It follows from [25] [10, Thm. 5] that if
∥˜
X∥0 < 1
2

1 +
1
µ(B)

(9.16)
then ˜
X is the sparsest solution to y = B(X). Since
1
¯x
 
1
¯xH
is by def-
inition the sparsest rank 1 solution to y = B(X), it follows that
˜
X =

1
¯x
 
1
¯xH
.
9.3
Numerical Algorithms
In addition to the above analysis of guaranteed recovery properties, a criti-
cal issue for practitioners is the eﬃciency of numerical solvers that can handle

Quadratic Basis Pursuit
203
moderate-sized SDP problems. Several numerical solvers used in CS may be
applied to solve nonsmooth SDPs, which include interior-point methods, e.g.,
those used in CVX [34], gradient projection methods [5], and augmented La-
grangian methods (ALM) [5]. However, interior-point methods are known to
scale badly to moderate-sized convex problems in general. Gradient projection
methods also fail to meaningfully accelerate QBP due to the complexity of the
projection operator. Alternatively, nonsmooth SDPs can be solved by ALM.
However, the augmented primal and dual objective functions are still SDPs,
which are equally expensive to solve in each iteration. There also exist a fam-
ily of iterative approaches, often referred to as outer approximation methods,
that successively approximate the solution of an SDP by solving a sequence of
linear programs (see [37]). These methods approximate the positive semidef-
inite cone by a set of linear constraints and reﬁne the approximation in each
iteration by adding a new set of linear constraints. However, we have experi-
enced slow convergence using these types of methods. In summary, QBP as
a nonsmooth SDP is categorically more expensive to solve compared to the
linear programs in CS, and the task exceeds the capability of many popular
sparse optimization techniques.
In this chapter, we propose an eﬀective solver to the nonsmooth SDP
underlying QBP via the alternating directions method of multipliers (ADMM,
see for instance [9] and [6, Sec. 3.4]) technique. The motivation to use ADMM
is two-fold:
1. It scales well to large data sets.
2. It is known for its fast convergence.
There are also a number of strong convergence results that further motivate
the choice [9].
To set the stage for ADMM, let n denote the dimension of x, and let N
denote the number of measurements. Then, rewrite (9.10) to the equivalent
SDP
minX1,X2,Z
f1(X1) + f2(X2) + g(Z),
subj. to
X1 −Z = 0,
X2 −Z = 0,
(9.17)
where X1 = XH
1 ∈C(n+1)×(n+1), X2 = XH
2 ∈C(n+1)×(n+1), Z = ZH ∈
C(n+1)×(n+1), and
f1(X) ≜





Tr(X)
if yi = Tr(ΦiX), i = 1, . . . , N
and X1,1 = 1
∞
otherwise
f2(X) ≜
(
0
if X ⪰0
∞
otherwise
g(Z) ≜λ∥Z∥1.
Deﬁne two matrices Y1 and Y2 as the Lagrange multipliers of the two

204
Regularization, Optimization, Kernels, and Support Vector Machines
equality constraints in (9.17), respectively. Then the update rules of ADMM
lead to the following:
Xl+1
i
=
arg minX=XH fi(X) + Tr(Y l
i (X −Zl))
+
ρ
2∥X −Zl∥2,
Zl+1
=
arg minZ=ZH g(Z) + P2
i=1 Tr(Y l
i Z)
+
ρ
2∥Xl+1
i
−Z∥2,
Y l+1
i
=
Y l
i + ρ(Xl+1
i
−Zl+1),
(9.18)
for i = 1, 2, where ρ ≥0 is a parameter that enforces consensus between X1,
X2, and Z. Each of these steps has a tractable calculation. After some simple
manipulations, we have:
Xl+1
1
= argminX=XH
∥X −(Zl −I+Y l
1
ρ
)∥,
subj. to
yi = Tr(ΦiX),
i = 1, . . . , N,
X1,1 = 1.
(9.19)
Let ˜B : C(n+1)×(n+1) →CN+1 be the augmented linear operator such that
˜B(X) =
B(X)
X1,1

, where B is the linear operator deﬁned by (9.12). Assuming
that a feasible solution exists, and deﬁning Π ˜
B as the orthogonal projection
onto the convex set given by the linear constraints, i.e.,
y
1

= ˜B(X), the
solution is: Xl+1
1
= Π ˜
B(Zl−I+Y l
1
ρ
). This matrix-valued problem can be solved
by converting the linear constraint on Hermitian matrices into an equivalent
constraint on real-valued vectors.
Next,
Xl+1
2
= argmin
X⪰0
X −

Zl −Y l
2
ρ
 = ΠP SD

Zl −Y l
2
ρ

,
(9.20)
where ΠP SD denotes the orthogonal projection onto the positive-semideﬁnite
cone, which can easily be obtained via eigenvalue decomposition.
Finally, let X
l+1 = 1
2
P2
i=1 Xl+1
i
and similarly Y
l. Then, the Z update
rule can be written:
Zl+1 =
argminZ=ZT λ∥Z∥1 + ρ∥Z −(X
l+1 + Y
l
ρ )∥2
=
soft(X
l+1 + Y
l
ρ , λ
2ρ)
(9.21)
where soft(·) in the complex domain is deﬁned with respect to a positive real
scalar q as:
soft(x, q) =
(
0
if |x| ≤q,
|x|−q
|x| x
otherwise.
(9.22)
Note that if the ﬁrst argument is a complex value, the soft operator is deﬁned

Quadratic Basis Pursuit
205
in terms of the magnitude rather than the sign and if it is a matrix, the soft
operator acts element-wise.
Setting l = 1, Xl
1 = Xl
2 = Zl = I, where I denotes the identity matrix,
and ρl = 1, setting l = 0, the Hermitian matrices Xl+1
i
, Zl+1
i
, Y l+1
i
can
now be iteratively computed using the ADMM iterations (9.18). The stopping
criterion of the algorithm is given by (see for instance [9]):
∥rl∥≤nϵabs + ϵrel max(∥X
l∥, ∥Zl∥),
(9.23)
∥sl∥≤nϵabs + ϵrel∥Y
l∥,
(9.24)
where ϵabs, ϵrel are algorithm parameters set to 10−3 and rl and sl are the
primal and dual residuals, respectively, as:
rl =

Xl
1 −Zl
Xl
2 −Zl
,
(9.25)
sl = −ρ

Zl −Zl−1
Zl −Zl−1
.
(9.26)
We also update ρ according to the rule discussed in [9]:
ρl+1
=





τincrρl
if ∥rl∥> µ∥sl∥,
ρl/τdecr
if ∥sl∥> µ∥rl∥,
ρl
otherwise,
(9.27)
where τincr, τdecr, and µ are algorithm parameters. Values commonly used are
µ = 10 and τincr = τdecr = 2.
In terms of the computational complexity of the ADMM algorithm, its
inner loop calculates the updates of Xi, Z, and Yi, i = 1, 2. It is easy to see
that its complexity is dominated by (9.19) and (9.20), which is bounded by
O(n2N 2 + n3), while the calculation of Z and Yi is linear with respect to the
number of their elements.
9.4
Experiments
In this section, we provide comprehensive experiments to validate the ef-
ﬁcacy of the QBP algorithms in solving several representative nonlinear CS
problems that depend quadratically on the unknown. We compare their per-
formance primarily with two existing algorithms. As we mentioned in Section
9.1, if an underdetermined nonlinear system is approximated up to the ﬁrst
order, the classical sparse solver in CS is basis pursuit. In NLCS literature,
several greedy algorithms have been proposed for nonlinear systems. In this
section, we choose to compare them with the iterative hard thresholding (IHT)
algorithm in [3] in Section 9.4.1 and another greedy algorithm demonstrated

206
Regularization, Optimization, Kernels, and Support Vector Machines
in [55] in Section 9.4.3. Besides the comparisons shown here, we have also
compared them to a number of CPR algorithms [44, 52]. Not surprisingly,
they performed badly on the general quadratic problems since they do not
account for the linear term.
9.4.1
Nonlinear Compressive Sensing in Real Domain
In this experiment, we illustrate the concept of nonlinear compressive sens-
ing. Assume that there is a cost associated with sampling and that we would
like to recover z0 ∈ℜm, related to our samples yi ∈ℜ, i = 1, . . . , N, via
yi = fi(z0),
i = 1, . . . , N,
(9.28)
using as few samples as possible. Also, assume that there is a sparsifying basis
D ∈ℜm×n, possibly overcomplete, such that
z0 = Dx0,
with x0 sparse.
(9.29)
Hence, we have
yi = fi(Dx0),
i = 1, . . . , N,
(9.30)
with x0 a sparse vector. If we approximate the nonlinear equation system
(9.30) using a second order Maclaurin expansion we end up with a set of
quadratic equations,
yi = fi(0) + ∇fi(0)Dx0 + xT
0 DT ∇2fi(0)
2
Dx0, i = 1, . . . , N.
(9.31)
Hence, we can use QBP to recover x0 given {fi(x), yi}N
i=1 and D.
In particular, let D = I, n = m = 20, N = 25, fi(x) = ai + bT
i x +
xTQix, i = 1, . . . , N, and generate {yi}N
i=1 by sampling {ai, bi, Qi}N
i=1 from a
unitary Gaussian distribution. Let x0 be a binary vector with three elements
diﬀerent than zero. Given {yi, ai, bi, Qi}N
i=1, the task is now to recover x0.
The results of this simulation are shown in Figure 9.1.
First, as the noiseless measurements are generated by a quadratic system
of equations, it is not surprising that QBP perfectly recovers the sparse signal
x0 when λ = 50. One may wonder whether in the 20-D ambient space, the
solution x0 is unique. To show that the solution is not unique, we let λ = 0
and again apply QBP. As shown in Figure 9.1 (c), the solution is dense and
it also satisﬁes the quadratic constraints. Therefore, we have veriﬁed that the
system is underdetermined and there exist multiple solutions.
Second, in Figure 9.1 (d), we approximate (9.31) only up to the ﬁrst order
and set Qi = 0, i = 1, . . . , N. The approximation enables us to employ the
classical basis pursuit algorithm in CS to seek the best 3-sparse estimate x.
As expected, the approximation is not accurate enough, and the estimate is
far from the ground truth.
Third, we implement the iterative hard thresholding (IHT) algorithm in

Quadratic Basis Pursuit
207
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
1.2
 
 
true x
(a) Ground truth.
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
1.2
 
 
QBP (λ=50)
(b) QBP with λ = 50.
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
1.2
 
 
QBP (λ=0)
(c) QBP with λ = 0.
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
1.2
1.4
 
 
BP
(d) Basis pursuit.
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
1.2
 
 
IHT
(e) Iterative hard thresholding.
FIGURE 9.1: Estimated 20-D sparse signals measured in a simulated
quadratic system of equations. The QBP solution perfectly recovers the
ground truth with λ = 50, while the remaining algorithms fail to recover
the correct sparse coeﬃcients.

208
Regularization, Optimization, Kernels, and Support Vector Machines
[3], and the correct number of nonzero coeﬃcients in x0 is also provided to
the algorithm. Its estimate is given in Figure 9.1 (e). As IHT is a greedy
algorithm, its performance is aﬀected by the initialization. In Figure 9.1 (e),
the initial value is set by x = 0, and the estimate is incorrect.
Finally, we note that the advantage of using general CS theory is that
fewer samples are needed to recover a source signal from its observations.
This remains true for NLCS presented in this chapter. However, as (9.28) and
(9.31) are nonlinear equation systems, typically N ≫m measurements are
required for recovering a unique solution. In the same simulation shown in
Figure 9.1, one could ignore the sparsity constraint (i.e., by letting λ = 0 in
Figure 9.1 (c)), and it would require N ′ = 40 observations for QBP to recover
the unique solution, which is exactly the ground-truth signal.
Clearly, Figure 9.1 is only able to illustrate one set of simulation results.
To more systematically demonstrate the accuracy of the four algorithms in
probability, a Monte Carlo simulation is performed that repeats the above sim-
ulation but with diﬀerent randomly generated x0 and {ai, bi, Qi}. Table 9.1
shows the rates of successful recovery. We can see QBP achieves the high-
est success rate, which is followed by IHT. BP and the dense QBP solution
basically fail to return enough good results. λ = 50 was used in all trials.
TABLE 9.1: The percentage of correctly recovering x0 in 100 trials.
Method
QBP (λ = 50)
QBP (λ = 0)
BP
IHT
Success rate
79%
5%
3%
54%
9.4.2
The Shepp-Logan Phantom
In this experiment, we consider recovery of images from random samples.
More speciﬁcally, we formulate an example of the CPR problem in the QBP
framework using the Shepp-Logan phantom. Our goal is to show that using
the QBPD algorithm provides approximate solutions that are visually close
to the ground-truth images.
Consider the ground-truth image in Figure 9.2. This 30 × 30 Shepp-Logan
phantom has a 2D Fourier transform with 100 nonzero complex coeﬃcients.
We generate N linear combinations of pixels, and then measure the square of
the measurements. This relationship can be written as:
y = |Ax|2 = {xHaiaH
i x}1≤i≤N,
(9.32)
where A = RF is the concatenation of a random matrix R and the Fourier
basis F, and the image Fx is represented as a stacked vector in the 900-D
complex domain. The CPR problem minimizes the following objective func-
tion:
min
x ∥x∥1
subj. to
y = |Ax|2 ∈ℜN.
(9.33)

Quadratic Basis Pursuit
209
Previously, an SDP solution to the nonsparse phase retrieval problem was
proposed in [17], which is called PhaseLift. In a sense, PhaseLift can be viewed
as a special case of the QBP solution in (9.10) where λ = 0, namely, the
sparsity constraint is not enforced. In Figure 9.2 (b), the recovered result
using PhaseLift is shown with N = 2400.
To compare visually the performance of the QBP solution when the spar-
sity constraint is properly enforced, two recovered results are shown in Figure
9.2 (c) and (d) with N = 2400 and 1500, respectively. Note that the number
of measurements with respect to the sparsity in x is too low for both QBP
and PhaseLift to perfectly recover x. Therefore, in this case, we employ the
noisy version of the algorithm QBPD to recover the image. We can clearly
see from the illustrations that QBPD provides a much better approximation
and outperforms PhaseLift visually even though it uses considerably fewer
measurements.
5
10
15
20
25
30
5
10
15
20
25
30
(a) Ground truth
5
10
15
20
25
30
5
10
15
20
25
30
(b) PhaseLift with N = 2400
5
10
15
20
25
30
5
10
15
20
25
30
(c) QBPD with N = 2400
5
10
15
20
25
30
5
10
15
20
25
30
(d) QBPD with N = 1500
FIGURE 9.2: Recovery of a Shepp-Logan image by PhaseLift and QBPD.
9.4.3
Subwavelength Imaging
In this example, we present an example in sub-wavelength coherent diﬀrac-
tive imaging. The experiment and the data collection were conducted by [55].
Let yi, i = 1, . . . , N, be intensity samples of a 2D diﬀraction pattern. The

210
Regularization, Optimization, Kernels, and Support Vector Machines
diﬀraction pattern is the result of a 532 nm laser beam passing through an
arrangement of holes made on an opaque piece of glass. The task is to decide
the location of the holes out of a number of possible locations.
It can be shown that the relation between the intensity measurements and
the arrangements of holes is of the following type:
yi = |aH
i x|2,
i = 1, . . . , N,
(9.34)
where yi ∈ℜ, i = 1, . . . , N, are intensity measurements, ai ∈Cn, i =
1, . . . , N, are known complex vectors, and x ∈ℜn is the sought entity, each
element giving the likelihood of a hole at a given location.
We use QBPD with ε = 0.0012 and λ = 100. 89 measurements were
selected by taking every 200th intensity measurement from the dataset of
[55]. The quantity x is from the setup of the experiment known to be real and
ai = bi = ci = 0. We hence have
yi = xTQix = |aH
i x|2,
i = 1, . . . , N,
(9.35)
with Qi = aiaH
i ∈Cn×n, i = 1, . . . , N, and x ∈ℜn.
The resulting estimate is given to the left in Figure 9.3. The result deviates
from the ground truth and the result presented in [55] (shown in Figure 9.3
right), and it actually ﬁnds a more sparse pattern. It is interesting to note
that both estimates are, however, within the noise level estimated in [55]:
1
N
N
X
i
(yi −|aH
i x|2)2 ≤1.8 × 10−6.
(9.36)
Therefore, under the same noise assumptions, the two solutions are equally
likely to lead to the same observations y. However, knowing that there is a
solution within the noise level that is indeed sparser than the ground-truth
pattern, it should not be the optimal solution to have recovered the ground
truth, since there exists a sparser solution.
9.5
Conclusion
Classical compressive sensing assumes a linear relation between samples
and the unknowns. The ability to more accurately characterize nonlinear mod-
els has the potential to improve the results in both existing compressive sens-
ing applications and those where a linear approximation does not suﬃce, e.g.,
phase retrieval.
This chapter presents an extension of classical compressive sensing to
quadratic relations or second order Taylor expansions of the nonlinearity relat-
ing measurements and the unknowns. The novel extension is based on lifting

Quadratic Basis Pursuit
211
20
40
60
80
100
120
140
160
180
20
40
60
80
100
120
140
160
180
200
20
40
60
80
100
120
140
160
180
20
40
60
80
100
120
140
160
180
200
FIGURE 9.3: The estimated sparse vector x. The crosses mark possible
positions for holes, while the dots represent the recovered nonzero coeﬃcients.
Left: Recovered pattern by QBPD. Note that this estimate is sparser than the
ground truth but within the estimated noise level. Right: Recovered pattern
by the compressive phase retrieval method used in [55].
and convex relaxations and the ﬁnal formulation takes the form of a SDP. The
proposed method, quadratic basis pursuit, inherits properties of basis pursuit
and classical compressive sensing, and conditions for perfect recovery, etc., are
derived. We also give an eﬃcient numerical implementation.
Acknowledgments
The authors would like to acknowledge useful discussions and inputs from
Yonina C. Eldar, Mordechai Segev, Laura Waller, Filipe Maia, Stefano March-
esini, and Michael Lustig. We also want to acknowledge the authors of [55] for
kindly sharing their data with us.
Ohlsson is partially supported by the Swedish Research Council in the Lin-
naeus center CADICS, the European Research Council under the advanced
grant LEARN, contract 267381, by a postdoctoral grant from the Sweden-
America Foundation, donated by ASEA’s Fellowship Fund, and by a postdoc-
toral grant from the Swedish Research Council. Yang is supported in part by
ARO 63092-MA-II and DARPA FA8650-11-1-7153. Dong is supported by an
NSF Graduate Research Fellowship under grant DGE 1106400 TRUST (Team
for Research in Ubiquitous Secure Technology) which receives support from
NSF (award number CCF-0424422).

212
Regularization, Optimization, Kernels, and Support Vector Machines
Bibliography
[1] J. Antonello, M. Verhaegen, R. Fraanje, T. van Werkhoven, H. C. Ger-
ritsen, and C. U. Keller. Semideﬁnite programming for model-based sen-
sorless adaptive optics. J. Opt. Soc. Am. A, 29(11):2428–2438, November
2012.
[2] R. Balan, P. Casazza, and D. Edidin. On signal reconstruction without
phase. Applied and Computational Harmonic Analysis, 20:345–356, 2006.
[3] A. Beck and Y. C. Eldar. Sparsity constrained nonlinear optimization:
Optimality conditions and algorithms. Technical Report arXiv:1203.4580,
2012.
[4] R. Berinde, A. Gilbert, P. Indyk, H. Karloﬀ, and M. Strauss. Combining
geometry and combinatorics: A uniﬁed approach to sparse signal recovery.
In Communication, Control, and Computing, 2008 46th Annual Allerton
Conference on, pages 798–805, September 2008.
[5] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999.
[6] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computa-
tion: Numerical Methods. Athena Scientiﬁc, 1997.
[7] T. Blumensath.
Compressed sensing with nonlinear observations
and
related
nonlinear
optimization
problems.
Technical
Report
arXiv:1205.1650, 2012.
[8] T. Blumensath and M. E. Davies. Gradient pursuit for non-linear sparse
signal modelling. In European Signal Processing Conference, Lausanne,
Switzerland, April 2008.
[9] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed op-
timization and statistical learning via the alternating direction method of
multipliers. Foundations and Trends in Machine Learning, 3(1), January
2011.
[10] A. Bruckstein, D. Donoho, and M. Elad. From sparse solutions of systems
of equations to sparse modeling of signals and images. SIAM Review,
51(1):34–81, 2009.
[11] O. Bunk, A. Diaz, F. Pfeiﬀer, C. David, B. Schmitt, D. K. Satapathy, and
J. F. van der Veen. Diﬀractive imaging for periodic samples: retrieving
one-dimensional concentration proﬁles across microﬂuidic channels. Acta
Crystallographica Section A, 63(4):306–314, July 2007.

Quadratic Basis Pursuit
213
[12] E. Candès.
The restricted isometry property and its implications for
compressed sensing. Comptes Rendus Mathematique, 346(9–10):589–592,
2008.
[13] E. Candès, Y. C. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval
via matrix completion. Technical Report arXiv:1109.0573, Stanford Uni-
versity, September 2011.
[14] E. Candès, X. Li, Y. Ma, and J. Wright. Robust Principal Component
Analysis? Journal of the ACM, 58(3), 2011.
[15] E. Candès and B. Recht. Exact matrix completion via convex optimiza-
tion. CoRR, abs/0805.4471, 2008.
[16] E. Candès, J. Romberg, and T. Tao. Robust uncertainty principles: Ex-
act signal reconstruction from highly incomplete frequency information.
IEEE Transactions on Information Theory, 52:489–509, February 2006.
[17] E. Candès, T. Strohmer, and V. Voroninski. PhaseLift: Exact and stable
signal recovery from magnitude measurements via convex programming.
Technical Report arXiv:1109.4499, Stanford University, September 2011.
[18] A. Chai, M. Moscoso, and G. Papanicolaou.
Array imaging using
intensity-only measurements. Technical report, Stanford University, 2010.
[19] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis
pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.
[20] J.V. Corbett. The Pauli problem, state reconstruction and quantum-real
numbers. Reports on Mathematical Physics, 57(1):53–68, 2006.
[21] J.C. Dainty and J.R. Fienup. Phase retrieval and image reconstruction
for astronomy. In editor H. Stark, editor, Image Recovery: Theory and
Application. Academic Press, New York, 1987.
[22] A. d’Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet. A direct
formulation for Sparse PCA using semideﬁnite programming. SIAM Re-
view, 49(3):434–448, 2007.
[23] M. Dierolf, A. Menzel, P. Thibault, P. Schneider, C. M. Kewish, R. Wepf,
O. Bunk, and F. Pfeiﬀer. Ptychographic x-ray computed tomography at
the nanoscale. Nature, 467:436–439, 2010.
[24] D. Donoho.
Compressed sensing.
IEEE Transactions on Information
Theory, 52(4):1289–1306, April 2006.
[25] D. Donoho and M. Elad.
Optimally sparse representation in general
(nonorthogonal) dictionaries via ℓ1-minimization. PNAS, 100(5):2197–
2202, March 2003.

214
Regularization, Optimization, Kernels, and Support Vector Machines
[26] M. Ehler, M. Fornasier, and J. Sigl. Quasi-Linear Compressed Sensing.
ArXiv e-prints, November 2013.
[27] Y. C. Eldar and G. Kutyniok. Compresed Sensing: Theory and Applica-
tions. Cambridge University Press, 2012.
[28] Y. C. Eldar and S. Mendelson. Phase Retrieval: Stability and Recovery
Guarantees. ArXiv e-prints, November 2012.
[29] J. Fienup.
Phase retrieval algorithms: a comparison.
Applied Optics,
21(15):2758–2769, 1982.
[30] J. R. Fienup, J. C. Marron, T. J. Schulz, and J. H. Seldin.
Hubble
space telescope characterized by using phase-retrieval algorithms. Applied
Optics, 32(10):1747–1767, Apr 1993.
[31] R. Gerchberg and W. Saxton. A practical algorithm for the determination
of phase from image and diﬀraction plane pictures. Optik, 35:237–246,
1972.
[32] M. X. Goemans and D. P. Williamson. Improved approximation algo-
rithms for maximum cut and satisﬁability problems using semideﬁnite
programming. J. ACM, 42(6):1115–1145, November 1995.
[33] R. Gonsalves. Phase retrieval from modulus data. Journal of Optical
Society of America, 66(9):961–964, 1976.
[34] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex
programming, version 1.21. http://cvxr.com/cvx, August 2010.
[35] K. Jaganathan, S. Oymak, and B. Hassibi. Recovery of Sparse 1-D Signals
from the Magnitudes of their Fourier Transform. ArXiv e-prints, June
2012.
[36] D. Kohler and L. Mandel. Source reconstruction from the modulus of the
correlation function: a practical approach to the phase problem of optical
coherence theory. Journal of the Optical Society of America, 63(2):126–
134, 1973.
[37] H. Konno, J. Gotoh, T. Uno, and A. Yuki. A cutting plane algorithm
for semi-deﬁnite programming problems with applications to failure dis-
criminant analysis. Journal of Computational and Applied Mathematics,
146(1):141–154, 2002.
[38] L. Li and B. Jafarpour. An iteratively reweighted algorithm for sparse
reconstruction of subsurface ﬂow properties from nonlinear dynamic data.
CoRR, abs/0911.2270, 2009.
[39] X. Li and V. Voroninski. Sparse Signal Recovery from Quadratic Mea-
surements via Convex Programming. ArXiv e-prints, September 2012.

Quadratic Basis Pursuit
215
[40] L. Lovász and A. Schrijver. Cones of matrices and set-functions and 0-1
optimization. SIAM Journal on Optimization, 1:166–190, 1991.
[41] S. Marchesini. Phase retrieval and saddle-point optimization. Journal of
the Optical Society of America A, 24(10):3289–3296, 2007.
[42] S. Marchesini. Ab Initio Undersampled Phase Retrieval. Microscopy and
Microanalysis, 15, July 2009.
[43] J. Miao, T. Ishikawa, Q. Shen, and T. Earnest. Extending x-ray crystal-
lography to allow the imaging of noncrystalline materials, cells, and single
protein complexes. Annual Review of Physical Chemistry, 59(1):387–410,
2008.
[44] M. Moravec, J. Romberg, and R. Baraniuk. Compressive phase retrieval.
In SPIE International Symposium on Optical Science and Technology,
2007.
[45] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM
Journal on Computing, 24(2):227–234, 1995.
[46] Y. Nesterov. Semideﬁnite relaxation and nonconvex quadratic optimiza-
tion. Optimization Methods & Software, 9:141–160, 1998.
[47] H. Ohlsson and Y. C. Eldar. On conditions for uniqueness in sparse phase
retrieval. CoRR, abs/1308.5447, 2013.
[48] H. Ohlsson, A. Yang, R. Dong, and S. S. Sastry. CPRL — an extension
of compressive sensing to the phase retrieval problem. In P. Bartlett,
F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors,
Advances in Neural Information Processing Systems 25, pages 1376–1384.
2012.
[49] H. Ohlsson, A. Y. Yang, R. Dong, and S. Sastry. Compressive Phase Re-
trieval From Squared Output Measurements Via Semideﬁnite Program-
ming. Technical Report arXiv:1111.6323, University of California, Berke-
ley, November 2011.
[50] E. Osherovich, Y. Shechtman, A. Szameit, P. Sidorenko, E. Bullkich,
S. Gazit, S. Shoham, E.B. Kley, M. Zibulevsky, I. Yavneh, Y.C. Eldar,
O. Cohen, and M. Segev. Sparsity-based single-shot subwavelength coher-
ent diﬀractive imaging. In 2012 Conference on Lasers and Electro-Optics
(CLEO), San Jose, CA, USA, May 2012.
[51] P. Schniter and S. Rangan. Compressive phase retrieval via generalized
approximate message passing. In Proceedings of Allerton Conference on
Communication, Control, and Computing, Monticello, IL, USA, October
2012.

216
Regularization, Optimization, Kernels, and Support Vector Machines
[52] Y. Shechtman, A. Beck, and Y. C. Eldar.
GESPAR: Eﬃcient Phase
Retrieval of Sparse Signals. ArXiv e-prints, January 2013.
[53] Y. Shechtman, Y. C. Eldar, A. Szameit, and M. Segev. Sparsity based
sub-wavelength imaging with partially incoherent light via quadratic
compressed sensing. Opt. Express, 19(16):14807–14822, Aug 2011.
[54] N.Z. Shor. Quadratic optimization problems. Soviet Journal of Computer
and Systems Sciences, 25:1–11, 1987.
[55] A. Szameit, Y. Shechtman, E. Osherovich, E. Bullkich, P. Sidorenko,
H. Dana, S. Steiner, E. B. Kley, S. Gazit, T. Cohen-Hyams, S. Shoham,
M. Zibulevsky, I. Yavneh, Y. C. Eldar, O. Cohen, and M. Segev. Sparsity-
based single-shot subwavelength coherent diﬀractive imaging. Nature Ma-
terials, 11(5):455–459, May 2012.
[56] I. Waldspurger, A. d’Aspremont, and S. Mallat. Phase Recovery, MaxCut
and Complex Semideﬁnite Programming. ArXiv e-prints, June 2012.
[57] A. Walther. The question of phase retrieval in optics. Optica Acta, 10:41–
49, 1963.

Chapter 10
Robust Compressive Sensing
Esa Ollila
Aalto University
Hyon-Jung Kim
Aalto University
Visa Koivunen
Aalto University
10.1
Introduction ......................................................
217
10.2
Robust Regression and Robust Loss Functions ..................
219
10.3
Iterative Hard Thresholding (IHT) ..............................
221
10.4
Robust IHT Based on Preliminary Estimate of Scale ...........
223
10.5
Robust IHT Based on Joint Estimation of Signal and Scale ....
225
10.6
Simulation Studies ...............................................
229
10.7
Conclusions .......................................................
233
Bibliography ......................................................
235
10.1
Introduction
The compressed sensing (CS) or sparse signal reconstruction/approximation
(SSR) is a signal processing technique that exploits the fact that acquired data
can have a sparse representation in some basis. It allows for solving under-
determined systems of equations. The problem can be formulated as follows
[8, 7, 5]. Let y = (y1, . . . , yM)⊤denote the observed data vector (measure-
ments) modelled as
y = Φx + ε
(10.1)
i.e., yi = φ⊤
i x + εi,
i = 1, . . . , M
(10.2)
where Φ =
 φ1
· · ·
φM
⊤is M × N measurement matrix with more col-
umn vectors than row vectors φi (i.e., N > M), x = (x1, . . . , xN)⊤is the
unobserved signal vector, and ε = (ε1, . . . , εM)⊤is the (unobserved) random
noise vector. It is assumed that the signal vector x is K-sparse (i.e., it has K
217

218
Regularization, Optimization, Kernels, and Support Vector Machines
non-zero elements) or is compressible (i.e., it has a representation whose en-
tries decay rapidly when sorted in order of decreasing magnitude). Note that
compressible signals are well approximated by K-sparse signals and typically
in many applications K ≪N. Let us denote the signal support (i.e., the loca-
tions of non-zero elements) as Γ = supp(x) = {j : xj ̸= 0}. The objective is
then to reconstruct or approximate the signal vector x by K-sparse represen-
tation knowing only the acquired vector y, the measurement matrix Φ, and
the sparsity K.
One approach for ﬁnding a K-sparse estimate of x is by solving the fol-
lowing optimization problem
ˆx⋆= arg min
x ∥y −Φx∥2
2
subject to ∥x∥0 ≤K
(10.3)
where ∥· ∥0 denotes the ℓ0 pseudo-norm, ∥x∥0 = #{j
:
xj ̸= 0}, and
∥· ∥p for p ≥1 denotes the usual ℓp-norm, ∥x∥p =
  PN
i=1 |xi|p1/p. This
optimization problem is known to be NP-hard and hence suboptimal strategies
have been under active research; see [8] for a review. These methods, such
as Compressive Sampling Matching Pursuit (CoSaMP) [14] or Iterative Hard
Thresholding (IHT) [2, 3] are guaranteed to perform very well provided that
suitable conditions (e.g., restricted isometry property on Φ and non impulsive
noise conditions) are met. It is important to notice that the derived recovery
bounds depend linearly on ∥ε∥2 and thus the methods are not guaranteed
to provide accurate reconstruction/approximation under heavy-tailed (spiky)
non-Gaussian noise. An alternative approach to enforce sparsity of the solution
is to deploy ℓ1 constraint on the signal, i.e., ∥x∥1 ≤δ, as is done in the
celebrated LASSO method [18].
Despite the vast interest in CS/SSR during the past decade, sparse and
robust signal reconstruction methods, i.e., methods that can provide accurate
sparse reconstruction even in heavy-tailed non-Gaussian noise conditions or in
the face of outliers (gross errors), have been considered in the literature only
recently. In [6], the authors robustify the iterative hard thresholding (IHT)
approach for K-sparse signal reconstruction by replacing the ℓ2 norm on the
residuals (reconstruction error) by the Lorentzian pseudo-norm; this leads to
a slightly modiﬁed algorithm referred to as Lorentzian IHT (LIHT) hereafter.
In [16], the authors use the ℓ0-regularized least absolute deviation (LAD) re-
gression model and propose an approximate algorithm that utilizes weighted
median regression. This method is computationally demanding and also in-
volves tuning parameters whose selection is not an easy task. In our earlier
work in [17], robust versions of the CoSaMP, IHT and orthogonal matching
pursuit (OMP) [19] algorithms were proposed. These methods were based on
replacing the non-robust ℓ2-norm of the residuals by robust loss functions (i.e.,
which downweights large residuals) that are commonly used in robust statis-
tics [11, 13], leading to modiﬁcations of the respective algorithms. See also
our earlier work in [12] in which we proposed robust and sparse ℓ1-regularized
estimation of tensor decompositions and devised an alternating LS-type algo-
rithm for their computations.

Robust Compressive Sensing
219
In this chapter, we propose a new and novel IHT method based on joint
estimation of the unknown parameters of the system model, namely the K-
sparse or compressible signal x and the scale parameter σ of the error dis-
tribution. The chapter is organized as follows. First, Section 10.2 provides a
review of robust M-estimation approach to regression with particular empha-
sis put on diﬀerent robust loss and objective functions. We shall then adopt
these approaches for obtaining (constrained) sparse and robust estimates of x
in the CS system model using the IHT technique. Section 10.3 describes the
IHT method [2, 3] and Section 10.4 its robustiﬁcation proposed in our earlier
paper [17] that is based on preliminary estimate of the scale. This method,
referred to as generalized IHT, utilizes robust objective functions arising from
M-estimation of regression reviewed in Section 10.2. The disadvantage of this
method is that it requires a preliminary (auxiliary) robust estimate of the
scale parameter σ of the error distribution. Section 10.4 provides the main
contribution of this chapter, describing in detail the new robust IHT method
that is based on joint estimation of signal x and scale σ. This method is also
described in a recent unpublished manuscript [15]. Section 10.4 provides ex-
tensive simulation studies illustrating the eﬀectiveness and usefulness of the
proposed method in reconstructing a K-sparse signal in various noise condi-
tions and SNR settings.
Notations: For a vector a ∈Rm, a matrix A ∈Rn×m, and an index
set Γ = (γ1, . . . , γp), ai denotes the ith component of a, ai denotes the ith
column vector of A, and aΓ denotes the p-vector of a with elements aγi selected
according to the support set Γ. Similarly AΓ = (aγ1 · · · aγp) is an n×p matrix
whose columns are selected from the columns of A according to the index set
Γ. Notation =d is read “has the same distributions as”.
10.2
Robust Regression and Robust Loss Functions
We assume that the noise terms εi are independent and identically dis-
tributed (i.i.d.) random variables from a continuous symmetric distribution
and let σ > 0 denote the scale parameter of the error distribution. The den-
sity of εi is then f(e) = (1/σ)f0(e/σ), where f0(·) denotes the standard form
of the density, e.g., f0(e) = (1/
√
2π) exp(−1
2e2) in case of normal (Gaussian)
error distribution. Let us denote residuals for a given (candidate) signal vector
x as
ei ≡ei(x) = yi −φ⊤
i x
and write e ≡e(x) = (e1, . . . , eM)⊤= y −Φx for the vector residuals. Let us
ﬁrst point out that if M > N and sparse approximation of x is not looked for
(unconstrained problem), then (10.1) is just a conventional regression model.
In this section we review robust regression approach (namely, M-estimation)

220
Regularization, Optimization, Kernels, and Support Vector Machines
with particular emphasis put on diﬀerent robust loss and objective functions
and their properties. See [11, 13] for a more detailed overview of M-estimation
of regression parameters.
Recall that the least squares (LS-) estimator of regression minimizes the
sum of squares of the residuals. A common approach to obtaining a robust
estimator of regression parameters is then to replace the LS (ℓ2-)loss function
ρ(e) = 1
2e2 with a robust loss function that downweights large residuals, thus
reducing the inﬂuence (eﬀect, impact) of gross errors in the obtained solution.
Suppose for a moment that we have a preliminary estimate of the scale pa-
rameter σ or that the scale parameter is known; in both cases, we denote this
(estimated or known) value of σ by ˆσ. Then the objective function approach
for obtaining a robust M-estimator ˆx of x is to solve the optimization problem
ˆx = arg min
x
M
X
i=1
ρ
yi −φ⊤
i x
ˆσ

(10.4)
where ρ is continuous, even function and increasing for e ≥0. An M-
estimating equation approach to robust regression is to ﬁnd ˆx that solves
M
X
i=1
ψ
yi −φ⊤
i x
ˆσ

φi = 0
(10.5)
where ψ is continuous and odd function (ψ(−e) = −ψ(e)). Naturally when
ψ = ρ′, a stationary point of the objective function in (10.4) is a solution to
(10.5). Moreover, if ρ is a convex function, then solving (10.4) is equivalent to
solving (10.5) and vice versa, i.e., they are equivalent characterizations of the
problem. If the distribution of the errors is known, then ρ(e) = −log f0(e)+c is
the optimal loss function, where f0(e) denotes the standard form of the error
density. In this case, i.e., in the terminology of maximum likelihood (ML)-
estimation, ψ-function is called as score function; we adopt this terminology
and refer to ψ function as the score function. Below we review two commonly
used robust loss functions, namely Huber’s and Cauchy loss functions. We
note that in both cases there also exists a noise distribution for which these
functions are also optimal loss functions.
Huber’s loss function combines ℓ2 and ℓ1 loss functions. Recall that
the optimal loss function when errors follow a Gaussian distribution, i.e.,
εi ∼N(0, σ2), is the ℓ2 loss function ρ(r) =
1
2e2 whereas the optimal loss
function for Laplacian (double exponential) errors, εi ∼Lap(0, σ), is the ℓ1-
loss function ρ(e) = |e|. The corresponding ψ = ρ′ functions are ψ(e) = e
and ψ(e) = sign(e) (the latter being “pseudo-derivative” since |e| is not dif-
ferentiable at e = 0). Huber’s loss function is then deﬁned as
ρH(e) =
(
1
2e2,
for |e| ≤c
c|e| −1
2c2,
for |e| > c
(10.6)
where c is a user-deﬁned tuning constant that aﬀects robustness and eﬃciency

Robust Compressive Sensing
221
of the method. For example, the following choices of constant c,
c1 = 1.345
and
c2 = 0.732
(10.7)
yield 95 and 85 percent (asymptotic) relative eﬃciency compared to LS-
estimator of regression in the case of Gaussian errors. These constants will
also be used in the sequel in the simulations for the Huber IHT method intro-
duced in Section 10.5. Huber’s loss function is also an optimal loss function in
the case that the errors follow the so-called (Huber’s) least favourable distribu-
tion, i.e., a symmetric unimodal distribution with smallest Fisher Information
within a “neighborhood” of the normal distribution, which can be charac-
terized as being Gaussian in the middle and double exponential in the tails.
The corresponding score function ψ = ρ′ is a winsorizing (clipping, trimming)
function
ψH(e) = max[−c, min(c, e)] =
(
e,
for |e| ≤c
c sign(e),
for e > c.
Thus the smaller the c, the more severe is the trimming (clipping) of the
residuals. Huber’s ρ and ψ-functions are depicted in Figure 10.1.
Cauchy loss function is the optimal loss function when the errors follow
the Cauchy distribution, εi ∼Cau(0, σ); it is given by
ρC(e) = 1
2 log(1 + e2).
(10.8)
Note that Cauchy loss function is diﬀerentiable but non-convex function.
Hence solving the optimization problem (10.4) is a non-convex optimization
problem, and ﬁnding a global solution remains an open problem. The corre-
sponding ψ = ρ′ function is
ψC(e) =
e
1 + e2 .
The fact the Cauchy loss function is “more robust” than Huber’s loss, i.e.,
downweighting the residuals more rigidly, is attested by the fact that ψ is
redescending to zero. Hence the “scores” of very large residuals can be zero or
close to zero and hence will not have much eﬀect on the solution.
10.3
Iterative Hard Thresholding (IHT)
Iterative hard thresholding (IHT) has been proposed and extensively stud-
ied for sparse signal recovery by Blumensath and Davies [1, 2, 3]. Let the initial
value of iteration be x0 = 0. Then the IHT algorithm iterates
xn+1 = HK(xn + µΦ⊤en),
(10.9)

222
Regularization, Optimization, Kernels, and Support Vector Machines
(a) Loss functions ρ(e)
(b) Score functions ψ(e)
FIGURE 10.1: Huber’s and Cauchy ρ and ψ functions. Unlike the Huber’s
loss function, the Cauchy loss function is non-convex. Cauchy loss function
on the other hand can handle very spiky or large residuals. This can be seen
from the Cauchy score function, which redescends to zero.
where en = y−Φxn denote the residual (error) vector at nth iteration, µ > 0
denotes a stepsize, and HK(·) denotes the hard thresholding operator that sets
all but the largest (in magnitude) K elements of its vector-valued argument to

Robust Compressive Sensing
223
zero. In [1] it was shown that the iterations converge to a local minimum of the
optimization problem (10.3) given that ∥Φ∥2 ≤1 and Φ has full rank. Later
in [2], the authors derived performance guarantees of the algorithm based on
the restricted isometry property (RIP) [4] of matrix Φ and showed that the
algorithm will reach a neighborhood of the best K-term approximation. Since
the recovery bound [2, c.f., Theorems 4 & 5] is linearly dependent on the ℓ2-
norm of the noise vector ∥ε∥2, it is clear that the IHT method will not be
robust or provide accurate reconstruction in heavy-tailed noise (in such cases
∥e2∥will be very large). Later in [2], the authors proposed an improvement of
the method that computes an optimal stepsize update (in terms of reduction
in squared approximation error) at each iteration. As its beneﬁts, the resulting
normalized IHT algorithm has a faster convergence rate. Another important
advantage is that the requirement ∥Φ∥2 ≤1 is avoided.
Stepsize selection. Let Γn denote the support set of xn at nth iteration.
The stepsize is updated as
µn =
g⊤
ΓngΓn
g⊤
ΓnΦ⊤
ΓnΦΓngΓn ,
(10.10)
where g = Φ⊤en is the gradient update at nth iteration. If Γn+1 ̸= Γn, then
this stepsize might not be optimal and does not guarantee convergence. For
guaranteed convergence, one requires that
µn ≤(1 −c)
∥xn+1 −xn∥2
2
∥Φ(xn+1 −xn)∥2
2
= ωn
for some c > 0. Then, if Γn+1 ̸= Γn, one calculates ωn and if µn > ωn, one sets
µn ←µn/((1−c)κ) for some κ > 1/(1−c). Then a new proposal is calculated
using (10.9) and the value of ωn is updated. The process terminates when
µn < ωn in which case the latest proposal is accepted and one continues with
the next iteration.
10.4
Robust IHT Based on Preliminary Estimate of
Scale
We now describe the robust IHT method proposed in our earlier work
[17]. Let ˆσ denote a preliminary estimate of the scale parameter of the error
distribution. Instead of using the LS-loss on the residuals, we look for the
minimum of the problem
ˆσ2
M
X
i=1
ρ
yi −φ⊤
i x
ˆσ

subject to
∥x∥0 ≤K
(10.11)

224
Regularization, Optimization, Kernels, and Support Vector Machines
where the multiplying factor σ2 is used so that the optimization problem
(10.11) coincides with (10.3) when ρ is the LS-loss function ρ(e) = 1
2e2. As
earlier, let us denote the residual vector by e = y −Φx and let J(x) =
ˆσ2 PM
i=1 ρ
  yi−φ⊤
i x
ˆσ

denote the objective function in (10.11). Now the gradient
descent update is
g = −∇J(x) = ˆσ
M
X
i=!
ψ
ei
ˆσ

φi = Φ⊤eψ
where ψ = ρ′ and
eψ = ψ
 e
ˆσ

ˆσ
(10.12)
denotes the vector of pseudo-residuals. Above the notation is such that for
a vector e, ψ(e) = (ψ(ei), . . . , ψ(eM))⊤, i.e., a co-ordinatewise application
of the function ψ on the elements of vector e. As an example, when the LS
loss function ρ(e) =
1
2e2 is used, then eψ = e. On the other hand, when
Huber’s loss function is used, eψ is simply a vector of Winsorized (clipped,
trimmed) residuals. Again let the initial value of iteration be x0 = 0. Then
the generalized IHT algorithm iterates
xn+1 = HK(xn + µΦ⊤en
ψ),
(10.13)
where µ is the stepsize and en
ψ = ψ(en/ˆσ)ˆσ denotes the pseudo-residual vec-
tor at nth iteration. Finally, we note that one can use a similar idea as in
normalized IHT to ﬁnd an approximate value of the optimal stepsize at each
iteration. The stepsize calculation of the robust IHT will be discussed in detail
in Section 10.5.
Computation of the preliminary scale parameter ˆσ. If robust loss
functions are used then a preliminary scale estimate ˆσ is needed. Even if one
chooses an optimal loss function for the noise distribution (e.g., Huber loss
function when the errors follow the least favorable distribution), the perfor-
mance of the method deteriorates if the scale parameter is badly estimated.
For example, over-estimation of ˆσ can result in the case that pseudo-residuals
in (10.12) are not Winsorized (clipped) at all, or, Winsorized too much if ˆσ
is severely under-estimated. Obtaining a robust estimate of scale ˆσ is a very
diﬃcult problem. A ﬁrst step is to ﬁnd a robust initial K-sparse estimate
x0 of x after which ˆσ can be computed by some robust scale statistic (e.g.,
median absolute deviation (MAD)) of the obtained residuals e0 = y −Φx0.
A robust initial estimate x0 can be, e.g., the robust OMP estimate proposed
in [17]. Naturally, it can be questioned if this makes sense, as the resulting
robust IHT estimate may not provide a better (more robust or accurate) es-
timate than the initial OMP estimator x0. To avoid computation of an initial
estimate x0, one approach is to recompute ˆσ at each iteration by MAD of
the current residuals as in [17]. The problem with this approach is that the
obtained estimate (given that the iterations converged) is not a solution to

Robust Compressive Sensing
225
an optimization problem (10.13) and hence any performance guarantees are
diﬃcult to provide. To overcome the above problems, we shall propose in Sec-
tion 10.5 a sound approach that estimates x and σ simultaneously. Moreover,
the proposed IHT algorithm converges to a local minimum of the proposed
optimization problem.
Lorentzian IHT (LIHT) proposed in [6] is essentially a special case
of the generalized IHT method described above based on Cauchy loss func-
tion (10.8). In this case, the objective function in (10.11) (if one discards the
multiplier term (ˆσ/2)) can be given as
M
X
i=1
log(1 + e2
i /ˆσ2),
which authors denote as ∥e∥LL and refer to as Lorentzian (pseudo-)norm. Note
that ∥· ∥LL is pseudo-norm as it does not verify axioms of norms such as the
triangle-inequality. The authors propose to estimate ˆσ (denoted as γ in their
paper) by
ˆσ = (y(0.875) −y(0.125))
(10.14)
where y(a) denotes ath empirical quantile of the measurements y1, . . . , yM.
Thus ˆσ is the range between .875th and .125th sample quantiles of the mea-
surements. This choice has no guarantees of estimating the scale parameter of
the error distribution and thus the performance of the LIHT method with this
choice of ˆσ is varying heavily with the signal-to-noise ratio. This is evident in
the simulations in Section 10.6. Therein, it is observed that for all noise distri-
butions and all considered SNR ranges, the LIHT method is performing much
worse than the Huber IHT method proposed in Section 10.5, which estimates
the signal and scale simultaneously.
10.5
Robust IHT Based on Joint Estimation of Signal
and Scale
The main problem with the generalized IHT method described in the pre-
vious section was in obtaining accurate and robust preliminary scale estimate
ˆσ. To circumvent the above problem, we propose to estimate x and σ simul-
taneously. To do this elegantly, we propose to minimize
Q(x, σ) = σ
M
X
i=1
ρ
yi −φ⊤
i x
σ

+ (M −K)ασ
(10.15)
subject to
∥x∥0 ≤K,

226
Regularization, Optimization, Kernels, and Support Vector Machines
where ρ is a convex loss function, ρ(0) = 0, which should verify
lim
|x|→∞
ρ(x)
|x| = c ≤∞
and α > 0 is a scaling factor chosen so that the solution ˆσ is Fisher-consistent
for σ when the errors follow the normal distribution, i.e., εi ∼N(0, σ2). This
is achieved by setting α = E[χ(u)], where u ∼N(0, 1) and
χ(e) = ψ(e)e −ρ(e).
(10.16)
Note also that a multiplier (M −K) is used in the second term of (10.15)
instead of M in order to reduce the bias of the obtained scale estimate ˆσ at
small sample lengths. The objective function Q in (10.15) was proposed for
joint estimation of location and scale and regression and scale parameters by
Huber (1964, 1973) in [9, 10] and is often referred to as “Huber’s proposal 2”.
Note that Q(x, σ) is a convex function of (x, σ) [11, p. 177–178] which enables
to derive a simple convergence proof of an iterative algorithm to compute the
solution (ˆx, ˆσ).
Let us now choose Huber’s loss function ρH(e) in Equation (10.6) as our
choice of ρ function. In this case χ in Equation (10.16) becomes χH(e) =
1
2ψ2
H(e) and the scaling factor α = β/2 can be computed as
β = 2{c2(1 −FG(c)) + FG(c) −1/2 −c fG(c)},
(10.17)
where FG and fG denote the cumulative distribution function (c.d.f.) and
the probability density function (p.d.f.) of N(0, 1) distribution, respectively,
and c is the trimming threshold of Huber’s loss function that controls the
trade-oﬀbetween robustness and eﬃciency of the method. Thus the choice of
c determines β ≡β(c). For example, for the choices in (10.7), the respective
β constants are β1 = 0.7102 and β2 = 0.3378. The algorithm for ﬁnding the
solution to (10.15) then proceeds as follows:

Robust Compressive Sensing
227
Huber IHT algorithm
Initialization Let x0 = 0 and σ0 = 1. For a given trimming threshold c,
compute the scaling factor β = β(c) in (10.17).
0. Compute the initial signal support Γ0 = supp
 HK(Φ⊤yψ)

, where yψ =
ψH(y).
For n = 0, 1, . . . , iterate the steps
1. Compute the residuals en = y −Φxn
2. Update the value of the scale:
(σn+1)2 =
(σn)2
(M −K)β
M
X
i=1
ψ2
H
en
i
σn

3. Compute the pseudo-residuals and the gradient update
en
ψ = ψH
 en
σn+1

σn+1
g = Φ⊤en
ψ
4. Compute the stepsize µn using (10.20) if n = 0 and (10.22) otherwise
5. Update the value of the signal vector and the signal support
xn+1 = HK(xn + µng)
Γn+1 = supp(xn+1)
6. Approve the updates (xn+1, Γn) or recompute them (discussed later).
7. Terminate iteration if
∥xn+1 −xn∥2
∥xn∥2
< δ,
where δ is a predetermined tolerance/accuracy level (e.g., δ = 1.0−6).
Relation to IHT algorithm. Let us consider the special that we let the
trimming threshold c be arbitrarily large (c →∞). In this case, the Huber’s
loss function coincides with LS loss function and hence ψH(e) = e. Also note
that when c →∞, then the scaling factor β = β(c) in (10.17) converges to
1 (β →1). It is now easy to verify that in this special case, the Huber IHT
algorithm above coincides with the IHT algorithm [2, 3]. In this case Step 2

228
Regularization, Optimization, Kernels, and Support Vector Machines
can be discarded as it does not have any eﬀect on Step 3 because en
ψ = en.
Furthermore, now V0 = I in (10.20) and Wn = I in (10.22), and hence Step
4 reduces to
4. Compute the stepsize µn using (10.10)
which equals with the optimal stepsize update used in the normalized IHT
algorithm [3]. Step 6 of the normalized IHT is computed as follows
6. if Γn+1 ̸= Γn and µn > ωn, then recompute new values for xn+1 and
Γn+1 using the procedure described in Section 10.3.
Computing the stepsize µn in Step 4. Recall the following notations:
at nth iteration, Γn denotes the current signal support, en denotes the resid-
ual vector computed in Step 1, σn+1 denotes the updated value of the scale
computed in Step 2, and g denotes the gradient update g = Φ⊤en
ψ computed
in Step 3. Assuming that we have identiﬁed the correct signal support, an
optimal step size can be found in gradient ascent direction xΓn + µngΓn by
solving
µn
opt = arg min
µ
M
X
i=1
ρ
yi −[φi]⊤
Γn(xn
Γn + µgΓn)
σn+1

.
(10.18)
Since closed-form solution can not be derived for µn
opt above, we aim at ﬁnd-
ing a good approximation in closed-form. By writing v(e) = ρ(e)/e2, we can
express the optimization problem above in an equivalent form
µn
opt = arg min
µ
M
X
i=1
vn
i (µ)
 yi −[φi]⊤
Γn(xn
Γn + µgΓn)
2
(10.19)
where the “weights”, deﬁned as
vn
i (µ) = v
yi −[φi]⊤
Γn(xn
Γn + µgΓn)
σn+1

,
depend on µ. If we replace vn
i (µ) in the above optimization problem by their
approximation vn
i = vn
i (0), then we can calculate stepsize (i.e., an approxima-
tion of µn
opt) in closed-form by elementary calculus. Hence, when the iteration
starts at n = 0, we calculate the stepsize µ0 in Step 4 as
µ0 =
(e0)⊤V0ΦΓ0gΓ0
g⊤
Γ0Φ⊤
Γ0V0ΦΓ0gΓ0 ,
(10.20)
where V0 = diag(v0
1, . . . , v0
M). When iteration proceeds (for n = 1, 2, . . . ,) the
current support Γn and the signal update xn are more accurate estimates of
the true signal support Γ and the K-sparse signal x. Hence, when n ≥1, we
ﬁnd an approximation of the optimal stepsize µn
opt by solving
µn = arg min
µ
n
X
i=1
wn
i

yi −[φi]⊤
Γn(xn
Γn + µgΓn)
2
(10.21)

Robust Compressive Sensing
229
where the “weights” wn
i are deﬁned as
wn
i = w
yi −[φi]⊤
Γnxn
Γn
σn+1

,
and w(·) is the weight function, deﬁned as wH(e) = ψH(e)/e. The solution to
(10.21) can be given in closed form as
µn =
g⊤
ΓngΓn
g⊤
ΓnΦ⊤
ΓnWnΦΓngΓn ,
(10.22)
where Wn = diag(wn
1 , . . . , wn
M). The idea of (10.21) is based on the prop-
erty that if Γn correctly identiﬁes the signal support, then the signal ele-
ments can be found solving the M-estimation objective function Q(xΓn, σn+1)
(where σn+1 is considered ﬁxed) in the regression model y = ΦΓnxΓn + ε.
Recall that a solution to an M-estimate of regression can be found by
an iteratively reweighted least squares (IRLS) algorithm, which iteratively
solves Pn
i=1 wi(yi −[φi]⊤
Γnx)2 with weights dependent on the current residu-
als wi = w(ei/σn+1). These iterations are guaranteed to decrease the objective
function.
Approving or recomputing the updates (xn+1, Γn+1) in Step 6. We
accept the updates if the value of the new objective function is smaller than
the old objective function, i.e., Q(xn+1, σn+1) < Q(xn, σn), otherwise we set
µn ←µn/2 and step back to Step 5 and recompute new updates.
10.6
Simulation Studies
Next, extensive simulation studies are used to illustrate the validity and
usefulness of the proposed methods in a variety of noise environments and
SNR levels. The considered methods and their acronyms in the simulations
are
• IHT corresponds to the normalized IHT method, implemented as de-
scribed in [3] and in Section 10.3.
• LIHT corresponds to the Lorentzian IHT method, implemented as de-
scribed in [6]. See also Section 10.4 for discussions about this approach.
• Hub IHT (ci), i ∈{1, 2}, corresponds to the Huber IHT method (esti-
mating x and σ jointly) using the trimming threshold c1 (or c2) in (10.7).
The method is implemented as described in detail in Section 10.5.
Description of the setup and performance measures. For all exper-
iments, the elements of the measurement matrix Φ are drawn from N(0, 1)

230
Regularization, Optimization, Kernels, and Support Vector Machines
distribution after which the columns are normalized to have unit norm. Fur-
thermore, the K nonzero coeﬃcients of x are set to have equal amplitude
σs = |xi| = 10 for all i ∈Γ, equiprobable signs (i.e,. ±1 = sign(xi) with equal
probability 1
2), and the signal support set Γ = supp(x) is randomly chosen
from {1, . . . , N} without replacement for each trial. In all of our experiments,
the noise random vector ε consists of i.i.d. elements εi from a continuous sym-
metric distribution Fε with p.d.f. fε(e) = (1/σ)f0(e/σ), where σ > 0 denotes
the scale parameter and f0(e) the standard form of the p.d.f. Then the signal
to noise ratio (SNR) can be deﬁned
SNR(σ) = 20 log10
σs
σ
and thus depends on the used scale parameter σ of the error distribu-
tion. When the underlying noise distribution is not particularly heavy-tailed
in nature, a conventional scale parameter is the standard deviation (SD)
σSD =
p
E[|ε|2] (e.g., for Gaussian noise distribution). When SD is employed
as the scale parameter, SNR(σSD) is denoted shortly as SNR. In some cases,
the mean absolute deviation (MeAD) σMeAD = E[|ε|] is the most natural scale
parameter of the distribution. This is the case for the Laplace (double expo-
nential) distribution. However for noise distribution families with possibly in-
ﬁnite variance such as Student’s tν-distribution for degrees of freedom (d.o.f.)
ν ≤2, we choose the median absolute deviation (MAD) σMAD = Med(|εi|) as
the scale parameter.
As performance measures of sparse signal recovery, we use both the (ob-
served) mean squared error
MSE(ˆx) = 1
Q
Q
X
q=1
∥ˆx[q] −x[q]∥2
2
and the (observed) probability of exact recovery
PER ≜1
Q
Q
X
q=1
I(ˆΓ[q] = Γ[q])
where I(·) denotes the indicator function, ˆx[q] and ˆΓ[q] = supp(ˆx[q]) denote
the estimate of the K-sparse signal x[q] and the signal support Γ[q] for the
qth Monte Carlo (MC) trial, respectively. In all simulation settings described
below, the number of MC trials is Q = 2000, the length of the signal is
M = 512, the number of measurements is N = 256 and the sparsity level is
K = 8.
Experiment I: Gaussian noise. The noise distribution Fε is Gaus-
sian N(0, σ2) distribution with conventional scale parameter σ = σSD. Fig-
ure 10.2(a) gives the signal reconstruction performance as a function of SNR.
As expected, the IHT has the best performance, but Huber IHT with c1 suﬀers
a negligible 0.2 dB performance loss compared to IHT whereas Huber IHT

Robust Compressive Sensing
231
with c2 has 0.65 dB performance loss. All three methods had a full PER rate
(= 1) for all SNR. We note that the Lorentzian IHT was also included in the
study but the method experienced convergence problems and hence was left
out. These problems may be due to the choice of the preliminary scale esti-
mate ˆσ in (10.14), which seems appropriate only for heavy-tailed distributions
at low SNR regimes.
Experiment II: Laplacian noise. The noise distribution Fε is Laplace
distribution Lap(0, σ2) with conventional scale parameter σ = σMeAD. Recall
that the Laplace distribution is a heavy-tailed distribution with kurtosis equal
to 3. Figure 10.2(b) gives the signal reconstruction performance as a function
of SNR(σMeAD). As expected, the Huber IHT with c2 has the best performance,
next comes Huber IHT with c1, whereas LIHT has only slightly better perfor-
mance compared to IHT in the high SNR regime [30, 40] dB. The performance
loss of IHT as compared to Huber IHT with c2 is 1.9 dB on average in SNR
[22, 40], but jumps to 2.5 dB at SNR 20 dB. Huber IHT with c1 has 0.59 dB
performance loss compared to Huber IHT with c2. Note also that LIHT has
the worst performance at low SNR regime. In terms of PER rates, Huber IHT
methods had again the best performance and they attained full PER (= 1)
rate at all SNR levels considered. The PER rates of LIHT decayed to 0.99,
0.93, and 0.7 ad SNR = 24, 22, and 20 dB, respectively. The PER rate of IHT
was full until SNR 20 dB at which it decayed to 0.97.
Experiment III: tν-distributed noise. The noise distribution Fε is sym-
metric Student’s t-distribution, tν(0, σ), with scale parameter σ = σMAD. Re-
call that t-distribution with ν > 0 degrees of freedom (d.o.f.) is heavy-tailed
distribution with ν = 1 corresponding to Cauchy distribution and ν →∞
corresponding to Gaussian distribution. Figure 10.3 depicts signal recovery
performance in terms of MSE as a function of d.o.f. ν and for three diﬀer-
ent SNR(σMAD) levels. Note that the right hand side (RHS) plots provide a
zoom of the lower right corner of the LHS ﬁgure in the ν > 2 regime (i.e, for
less heavy-tailed distributions). First we wish to point out that the proposed
Huber’s IHT methods are outperforming the competing methods in all cases.
Note especially that at high SNR 40dB in Figure 10.3(a), the Huber IHT
method with c2 tuning constant is able to retain a steady MSE around -6.5
dB for all ν = 1, . . . , 5. The Huber IHT method with c1 is (as expected) less
robust with slightly worse performance; namely, its MSE is increasing mildly
as the distributions get heavier tailed (i.e., as ν decreases). IHT is already
performing poorly at ν = 5 and its performance deteriorates at a rapid rate
with decreasing ν. Note that the performance decay of LIHT is much milder
than that of IHT, yet it also has a rapid decay when it is compared to the
Huber IHT method. The PER rates given in Table 10.1 illustrate the remark-
able performance of Huber’s IHT methods, which are able to maintain full
recovery rates even at Cauchy distribution (ν = 1) for SNR 30 and 40 dB.
At low SNR 20 dB, only the Huber’s IHT methods are able to maintain good
PER rates whereas the other methods, IHT or LIHT, provide estimates that
are completely corrupted.

232
Regularization, Optimization, Kernels, and Support Vector Machines
TABLE 10.1: Probability of exact recovery (PER) rates for diﬀerent meth-
ods under tν(0, σMAD) distributed noise at varying SNR(σMAD) and d.o.f. ν.
System parameters were (M, N, K) = (512, 256, 8) and results are averages
over 2000 trials As can be seen, the Huber IHT methods are able to maintain
full recovery rates even at Cauchy distribution (ν = 1) for SNR 30 and 40 dB.
At low SNR 20 dB, the performance of Huber IHT methods are remarkably
better than those of IHT or LIHT, which are breaking down completely.
Degrees of freedom ν
SNR(σMAD)
Method
1
1.25
1.5
1.75
2
3
4
5
40 dB
IHT
.51
.86
.95
.99
0.99
1.0
1.0
1.0
LIHT
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
Hub IHT (c1)
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
Hub IHT (c2)
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
30 dB
IHT
.06
.33
.66
.85
0.93
1.0
1.0
1.0
LIHT
0.98
1.0
1.0
1.0
1.0
1.0
1.0
1.0
Hub IHT (c1)
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
Hub IHT (c2)
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
20 dB
IHT
0
0
0
.01
.04
.31
.57
.72
LIHT
.09
.10
.13
.13
.17
.22
.24
.24
Hub IHT (c1)
.46
.61
.70
.77
.81
.90
.92
.93
Hub IHT (c2)
.61
.66
.72
.73
.74
.80
.82
.82

Robust Compressive Sensing
233
(a) Gaussian noise, εi ∼N(0, σ2
SD)
(b) Laplacian noise, εi ∼Lap(0, σMeAD) noise
FIGURE 10.2: Average MSE as a function of SNR(σ) in reconstructing a
K-sparse signal using IHT, LIHT, and Huber IHT methods under N(0, σ2
SD)
and Lap(0, σMeAD) distributed noise. The RHS plot provides a zoom of the
lower right corner of the LHS plot in high SNR regime [30, 40] dB. System
parameters were (M, N, K) = (256, 512, 8) and number of trials is Q = 2000.
10.7
Conclusions
In this chapter, we have provided an overview of recent robust approaches
to compressive sensing. The Huber IHT method proposed here (and in the
submitted conference paper [15]) avoids the computation of the preliminary
estimate of scale that is needed in the generalized IHT method [17] and in the
Lorentzian IHT [6]. The Huber IHT method provides excellent signal recon-
struction performance under various noise distributions and SNR levels. Under
non-Gaussian noise (e.g., Laplace and Cauchy) it outperforms the normalized
IHT [3] and the Lorentzian IHT by orders of magnitude, yet it has a negligible
performance loss compared to the normalized IHT under the nominal Gaus-

234
Regularization, Optimization, Kernels, and Support Vector Machines
(a) SNR(σMAD) = 40 dB
(b) SNR(σMAD) = 30 dB
(c) SNR(σMAD) = 20 dB
FIGURE 10.3: Average MSE in reconstructing a K-sparse signal using IHT,
LIHT, and the Huber IHT using c1 or c2 under tν(0, σMAD) distributed noise
as a function of d.o.f. ν. Figures (a)-(c) illustrate the results at SNR(σMAD)
= 40, 30, 20 dB, respectively. Note that the RHS plot provides a zoom of the
lower right corner of the LHS ﬁgure in the ν > 2 regime (i.e, for less heavy-
tailed distributions). System parameters were (M, N, K) = (256, 512, 8) and
the number of trials is Q = 2000.

Robust Compressive Sensing
235
sian noise. A plan for future work is to derive exact theoretical performance
guarantees of the proposed Huber IHT method, which will be reported in a
separate paper.
Bibliography
[1] T. Blumensath and M. E. Davies. Iterative thresholding for sparse ap-
proximations. Journal of Fourier Analysis and Applications, 14(5–6):629–
654, 2008.
[2] T. Blumensath and M. E. Davies.
Iterative hard thresholding for
compressed sensing.
Applied and Computational Harmonic Analysis,
27(3):265–274, 2009.
[3] T. Blumensath and M. E. Davies. Normalized iterative hard thresholding:
guaranteed stability and performance. IEEE Journal of Selected Topics
in Signal Processing, 4(2):298–309, 2010.
[4] E. Candés, J. Romberg, and T. Tao. Robust uncertainty principles: Ex-
act signal reconstruction from highly incomplete frequency information.
IEEE Trans. Inform. Theory, 52(4):1289–1306, 2006.
[5] E. J. Candes and M. B Wakin. An introduction to compressive sampling.
IEEE Signal Proc. Mag., 25(2):21–30, 2008.
[6] R. E. Carrillo and K. E. Barner. Lorentzian based iterative hard thresh-
olding for compressed sensing. In Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP’11), pages 3664–3667, Prague,
Czech Republic, May 22–27, 2011.
[7] D. Donoho.
Compressive sensing.
IEEE Trans. Inform. Theory,
52(2):5406–5425, 2006.
[8] M. Elad. Sparse and redundant representations. Springer, New York,
2010.
[9] P. J. Huber. Robust estimation of a location parameter. Ann. Math.
Statist., 35:73–101, 1964.
[10] P. J. Huber.
Robust regression: Asymptotics, conjectures and Monte
Carlo. Ann. Statist., 1:799–821, 1973.
[11] Peter J. Huber. Robust Statistics. Wiley, New York, 1981.

236
Regularization, Optimization, Kernels, and Support Vector Machines
[12] H.-J. Kim, E. Ollila, and V. Koivunen. Robust and sparse estimation of
tensor decompositions. In Proc. IEEE Global Conference on Signal and
Information Processing (GlobalSIP’13), pages 1–5, Austin, Texas, USA,
December 3–5, 2013.
[13] R. A. Maronna, R. D. Martin, and V. J. Yohai. Robust Statistics: Theory
and Methods. Wiley, New York, 2006.
[14] D. Needell and J. A. Tropp. CoSaMP: Iterative signal recovery from in-
complete and inaccurate samples. Applied and Computational Harmonic
Analysis, 26(3):301–321, 2009.
[15] E. Ollila, H.-J. Kim, and V. Koivunen. Robust iterative hard thresh-
olding for compressed sensing. In Proc. 6th International Symposium on
Communications, Control, and Signal Processing (ISCCSP’14), Athens,
Greece, May 21–24, 2014.
[16] J. L. Paredes and G. R. Arce. Compressive sensing signal reconstruction
by weighted median regression estimates. IEEE Trans. Signal Processing,
59(6):2585–2601, 2011.
[17] S. A. Razavi, E. Ollila, and V. Koivunen. Robust greedy algorithms for
compressed sensing. In Proc. 20th European Signal Processing Conference
(EUSIPCO’12), pages 969–973, Bucharest, Romania, 2012.
[18] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal
Stat. Soc., Ser. B, 58:267–288, 1996.
[19] J. A. Tropp and A. C. Gilbert. Signal recovery from random measure-
ments via orthogonal matching pursuit. IEEE Trans. Inform. Theory,
53(12):4655–4666, 2007.

Chapter 11
Regularized Robust Portfolio
Estimation
Theodoros Evgeniou
Decision Sciences and Technology Management INSEAD
Massimiliano Pontil
Department of Computer Science, University College London
Diomidis Spinellis
Department of Management Science and Technology, Athens University of
Economics and Business
Nick Nassuphis
31 St. Martin’s Lane, London
11.1
Introduction ......................................................
238
11.2
Finding Robust Autocorrelation Portfolios ......................
239
11.2.1
Financial Time Series: Notation and Deﬁnitions ........
239
11.2.2
Regularization Problem .................................
239
11.2.3
Interpretation of the Case ϵ →∞.......................
241
11.2.4
Connection to Slow Feature Analysis ...................
242
11.3
Optimization Method ............................................
242
11.4
Robust Canonical Correlation Analysis ..........................
244
11.5
Experiments ......................................................
246
11.5.1
Synthetic Data ...........................................
247
11.5.2
S&P 500 Stock Data .....................................
247
11.6
Conclusion and Future Research .................................
251
11.7
Appendix: Robust cca Algorithm ...............................
252
Bibliography ......................................................
254
237

238
Regularization, Optimization, Kernels, and Support Vector Machines
11.1
Introduction
Given a vector-valued time series, we study the problem of learning the
weights of a linear combination of the series’ components (e.g., a portfolio),
which has large autocorrelation, and discuss the extension to the problem of
learning two combinations, which have large cross-correlation. Both problems
have been studied from diﬀerent perspectives in various areas, ranging from
computational neuroscience [27], to computer vision [20, 14], to information
retrieval [15], among others. In this chapter, we address these problems from
the point of view of robust optimization (see, e.g., [5, 8] and references therein)
and regularization, and highlight their application to the context of ﬁnancial
time series analysis; see, e.g., [25].
The autocorrelation (or cross-correlation) function is a quantity diﬃcult to
measure, as it depends on the lag-1 autocovariance matrix of the time series,
which is typically unstable. To mitigate this problem, we propose a robust
optimization approach that leads to regularized versions of the autocorrela-
tion (or cross-correlation) function. We describe various forms of regularizers
derived from diﬀerent constraints on the uncertainty region of the lag-1 au-
tocovariance matrix, which in particular induce ℓ2 or ℓ1 regularized portfolio
estimation methods. We present an optimization algorithm to solve the ℓ1
regularization problem, which is inspired by recent work on sparse principal
component analysis [16, 17], also linking this work to the broader literature
on sparsity regularization. We then apply the proposed methods to estimate
high lag-1 autocorrelation portfolios for ﬁnancial time series. On the way, we
link speciﬁc instances of our method to portfolio creation strategies previously
considered in the ﬁnance literature. Finally, we extend the proposed approach
to the setting of canonical correlation analysis [2, 13], an older statistical tech-
nique, which has seen revived interest in machine learning and statistics; see,
e.g., [15, 28].
In summary, the key contributions of this chapter are both methodolog-
ical, namely developing novel regularization methodologies and optimization
based estimation algorithms, as well as theoretical, namely establishing a link
between robust optimization and regularization in the context of portfolio es-
timation. Although the methods are more broadly applicable, we study them
in the context of portfolio creation for ﬁnancial time series. This type of data
is among the most challenging ones to develop predictive models for. Another
important contribution of this chapter is a demonstration of the proposed
approach application potential on ﬁnancial time series. Although developing
portfolio estimation methods for such series that can be used in practice (e.g.,
for trading [23]) is beyond the scope of this chapter, we discuss potential future
research that can lead to such methods building on the approach we develop.
Over the past few years there has been a rising interest within the ﬁnancial

Regularized Robust Portfolio Estimation
239
industry in employing machine learning techniques [1]; this work also builds
in that direction.
This chapter is organized as follows. In Section 11.2, we introduce the port-
folio learning problem. In Section 11.3, we address the issue of implementing
the learning method numerically. In Section 11.4, we extend our regularization
approach to the setting of canonical correlation analysis. Finally, in Section
11.5 we report experiments with the proposed methods in the context of ﬁ-
nancial time series.
11.2
Finding Robust Autocorrelation Portfolios
We start with the relation between a robust optimization formulation of
the maximally autocorrelated portfolio estimation problem and regularization.
As the experimental focus of the chapter is on ﬁnancial time series, we ﬁrst
introduce some notation from that context.
11.2.1
Financial Time Series: Notation and Deﬁnitions
Let r1, . . . , rT ∈Rn be the realization of a vector-valued time series over
T consecutive time frames (e.g., days). In the experiments, rt represents the
vector of log-returns on day t of n assets (e.g., stocks). A common goal in
practice is to learn a weight vector x ∈Rn that maximizes some investment
performance, such as the cumulative return or Sharpe ratio. The former quan-
tity is deﬁned as the sum of the daily returns, that is, PT
t=2 f(x⊤rt−1) x⊤rt,
where the function f can, for example, be sign(·) or −sign(·), depending on
whether the portfolio follows a momentum or mean reversion strategy; we re-
fer to [7] for background. The Sharpe ratio is deﬁned as the ratio between the
average daily returns and the standard deviation of the daily returns. Both
quantities are diﬃcult to optimize in x, i.e., they are not diﬀerentiable because
of the sign function in the numerator. Therefore in this chapter we use as a
surrogate function the lag-1 autocorrelation of the portfolio. Intuitively, a pos-
itive (respectively negative) autocorrelated portfolio will favor a momentum
(respectively mean reversion) strategy: we buy (respectively sell) the portfolio
on day t if it had a positive (respectively negative) return on the previous day.
11.2.2
Regularization Problem
The vector x gives rise to the scalar time series pt := x⊤rt, t = 1, . . . , T,
called the portfolio series [11]. Our goal is to ﬁnd a portfolio that has maximal
lag-1 autocorrelation, which is deﬁned as the correlation between pt−1 and pt.

240
Regularization, Optimization, Kernels, and Support Vector Machines
If the series is stationary1 this quantity simpliﬁes to
ν(x) = x⊤Θx
x⊤Γx
(11.1)
where Θ and Γ are the lag-1 autocovariance and the covariance of the time
series, respectively. The latter matrix is assumed to be invertible2. In order to
emphasize the fact that the autocorrelation ν depends on matrix Θ, we will
sometimes use the notation ν(·|Θ). We then solve the problem
max
x∈Rn ν(x).
(11.2)
This is a generalized eigenvalue problem [12], whose solution is given by x =
Γ−1
2 u, where u is the leading eigenvector of the matrix Γ−1
2 ΘΓ−1
2 .
In practice, matrices Θ and Γ are estimated from historical data, the most
common estimates being ˆΘ = 1
T
PT
t=2 rtr⊤
t−1 and ˆΓ = 1
T
PT
t=1 rtr⊤
t (see, e.g.,
[25], where for simplicity we assumed that the series has mean zero. Often,
these estimates are inaccurate and it therefore can be useful to introduce
robust versions of Problem (11.2), in which we suppose that the matrix Θ
and/or Γ is known to belong to some uncertainty set. For simplicity here
we only address the robustness of Θ, which is typically the main problem of
concern since the covariance is in practice often more stable than the lag-1
autocovariance [7, 25]. In particular, we prescribe an uncertainty set A and
consider the problem
max
x∈Rn min
Θ∈A ν(x|Θ).
(11.3)
That is, we maximize the worst autocorrelation obtained when varying matrix
Θ in the set A.
A natural choice for the uncertainty set is a ball centered at the empirical
estimate ˆΘ, namely we choose
A = {Θ : ∥Θ −ˆΘ∥p ≤ϵ},
where ∥· ∥p is the elementwise ℓp norm of the matrix (or vector) elements,
p ∈[1, ∞]. The associated dual norm is ∥· ∥q, where q ∈[1, ∞] is deﬁned by
the equation 1/p + 1/q = 1.
Lemma 11.1. It holds that
min
Θ∈A{x
⊤Θy : Θ ∈Rn×m} = x
⊤ˆΘy −ϵ∥x∥q∥y∥q.
1Relaxing this assumption within our framework is left for future research.
2If this assumption is violated, adding a small positive value to the diagonal elements of
Γ will ensure that it is invertible. Invertibility was not an issue in the numerical experiments
presented in Section 11.5 below.

Regularized Robust Portfolio Estimation
241
Proof. We write Θ = ˆΘ + ∆for some ∥∆∥p ≤ϵ. Using Hölder’s inequality, we
obtain that
x
⊤Θy = x
⊤ˆΘy + x
⊤∆y ≥x
⊤ˆΘy −∥∆∥p∥xy
⊤∥q.
This inequality is tight for ∆= ϵδγ⊤, where δ ∈Rn satisﬁes ∥δ∥p = 1 and
δ⊤x = ∥x∥q, and γ ∈Rm satisﬁes ∥γ∥p = 1 and γ⊤y = ∥y∥q. The result
follows by noting that ∥xy⊤∥q = ∥x∥q∥y∥q.
Using this lemma with x = y, we see that min{x⊤Θx : Θ ∈A} = x⊤ˆΘx −
ϵ∥x∥2
q. Consequently, Problem (11.3) becomes
max
x∈Rn
(
x⊤ˆΘx −ϵ∥x∥2
q
x⊤Γx
)
.
(11.4)
Parameter ϵ is related to the size of the uncertainty set A, and, as we adopt
the regularization framework below, for simplicity we call it hereafter the
regularization parameter. In this chapter we consider the cases q = 2 and
q = 1. In the ﬁrst case, Problem (11.4) is still a generalized eigenvalue problem
of the form (11.1) with matrix Θ replaced by ˆΘ −ϵI. In the second case,
the problem becomes a nonlinear one, for which we present an optimization
method in the next section.
The above robust analysis can be applied in a similar way to the problem of
ﬁnding the most negative autocorrelated portfolio, namely min{ν(x|Θ) : x ∈
Rn}. This merely requires replacing Θ by −Θ in Problem (11.4). Moreover,
the above analysis can be extended to take into account uncertainty in both
matrices Θ and Γ, with not much additional diﬃculty. Speciﬁcally, if we choose
the set
A = {Θ : ∥ˆΘ −Θ∥p ≤ϵ} × {Γ : ∥ˆΓ −Γ∥p ≤λ}
then we obtain a robust optimization problem similar to (11.4) but with the
denominator replaced by x⊤Γx + λ∥x∥2
q. As we have already noted, in ﬁnan-
cial time series Θ is much more unstable than Γ and, so, we do not explore
robustness with respect to both Θ and Γ further in this chapter. We also refer
to [26] for related ideas in the case of ℓ2.
11.2.3
Interpretation of the Case ϵ →∞
We note that, in the case of ﬁnancial data, when ϵ →∞the solutions
of Problem (11.4) are related to portfolios studied before in the ﬁnance liter-
ature. Speciﬁcally, for q = 2 we recover the leading eigenvector of Γ, which
for ﬁnancial time series is close to the “market portfolio” [3]. If q = 1, and
the series components all have the same unit variance, then the solution of
Problem (11.4) is (up to a nonzero multiplicative constant) given by x = ej,
where j is the most positively autocorrelated series component [19], that is
Θjj = maxn
i=1 Θii. To see this, note that Problem (11.4) is equivalent to

242
Regularization, Optimization, Kernels, and Support Vector Machines
max{x⊤Θx −ϵ∥x∥2
1 : x⊤Γx = 1}. If ϵ is large enough there is an advantage
in choosing x such that ∥x∥1 is as small as possible provided that x⊤Γx = 1.
Since we assumed that the series all have the same unit variance, we have that
1
=
x
⊤Γx =
n
X
i=1
x2
i +
X
i̸=j
Γijxixj
=
∥x∥2
1 +
X
i̸=j
|xixj| (Γijsign (xixj) −1) .
Since |Γij| < 1 if i ̸= j (otherwise Γ would not be strictly positive deﬁnite) we
conclude that
∥x∥2
1 = 1 +
X
i̸=j
|xixj| (1 −Γijsign (xixj)) ≥1
and ∥x∥1 = 1 if and only if x ∈{e1, . . . , en}.
11.2.4
Connection to Slow Feature Analysis
We end this section by noting a connection between Problem (11.2) and
a method of unsupervised learning. Using the identity 2(x⊤rt−1)(x⊤rt) =
(x⊤rt−1)2 + (x⊤rt)2 −(x⊤(rt −rt−1))2, we can rewrite twice the numerator in
(11.1) as x⊤Γt−1x + x⊤Γtx −x⊤V x, where Γt = Ertr⊤
t and Vt = Eδtδ⊤
t is the
covariance of the “velocity” process δt = rt −rt−1. If the process is stationary,
so Γt and Vt are time invariant, then ν(x) := 1 −1
2ψ(x), where
ψ(x) := x⊤V x
x⊤Γx .
Thus, maximizing ν is the same as minimizing ψ. The latter optimization
problem is very similar to the method of slow feature analysis [20, 22, 27],
an unsupervised learning technique that was originally designed to extract
invariant representations from time varying visual signals [6]. The robust ver-
sion (11.4) of Problem (11.2) could likewise be interpreted as a robust version
of slow feature analysis, which could lead to interesting applications to that
context.
11.3
Optimization Method
In this section, we address the issue of implementing the learning method
(11.4) in the case q = 1.3 We begin by rewriting Problem (11.4) as that of
3Similar observations apply to the general case q ∈(1, ∞].

Regularized Robust Portfolio Estimation
243
Algorithm 10 ℓ1-Regularized Autocorrelation
Choose a starting point x0 ∈Rn and tolerance parameter tol.
for k = 0, 1, . . . do
Let xk+1 = argmin{φ(x|xk) : x ∈Rn}
If |η(xk+1) −η(xk)| ≤tol terminate.
end for
minimizing the function
η(x) = x⊤Nx −x⊤Px + ϵ∥x∥2
1
x⊤Γx
where P and N are symmetric positive deﬁnite matrices such that P −N =
S := (ˆΘ + ˆΘ⊤)/2. These matrices can be obtained via the eigenvalue decom-
position of S as P = (S)+ and N = P −S, where (·)+ is a spectral function
that acts on the eigenvalues λ ∈R as (λ)+ = max(λ, 0).
Fix a point x0 and let φ(·|x0) be the function, deﬁned, for every x ∈Rn,
as
φ(x|x0) = x
⊤Nx −(x0)
⊤Px0 −2(x −x0)
⊤Px0
+ϵ∥x∥2
1 −η(x0)γ(x|x0)
(11.5)
where
γ(x|x0) =



(x0)⊤Γx0 + 2(x −x0)Γx0
if η(x0) > 0,
x⊤Γx
otherwise.
We then consider the convex optimization problem
min
x φ(x|x0).
(11.6)
The following lemma provides a rationale behind this problem.
Lemma 11.2. If φ(x|x0) < 0 then η(x) < η(x0).
Proof. This result follows from the inequality x⊤Nx −x⊤Px + ϵ∥x∥2
1 −
η(x0)x⊤Γx ≤φ(x|x0).
This observation leads to the descent Algorithm 10, which is inspired by
a method outlined in [16] for sparse principal component analysis.
Algorithm 10 iteratively solves a sequence of problems of the form (11.6).
Lemma 11.2 guarantees that the algorithm produces a sequence of points
{xk : k ∈N} such that the corresponding sequence of function values {η(xk) :
k ∈N} is strictly monotonically decreasing or the algorithm terminates. In
practice, we terminate the algorithm when |η(xk+1)−η(xk)| is less than some
tolerance parameter, e.g., 10−4.

244
Regularization, Optimization, Kernels, and Support Vector Machines
It remains to show how to solve Problem (11.6). This problem is of the
form min ∥Ax −b∥2
2 + ϵ∥x∥2
1, for an appropriate choice of the n × n matrix
A and vector b ∈Rn. Hence, it is equivalent to the Lasso method and can
be solved up to numerical precision by proximal gradient methods; see, e.g.,
[4, 24, 9, 10]. In our numerical experiments below we have found that we do
not need to solve (11.6) exactly. It is enough to ﬁnd a point x that strictly
decreases the objective. The simplest choice for the update rule is to set
xk+1 = proxr∥·∥2
1

I + 1
αk
 S + η(xk)Γ

xk

(11.7)
where αk = |||N||| if η(xk) > 0 and αk = |||N −η(xk)Γ||| otherwise, with |||·|||
the spectral norm. This corresponds to a single step of the proximal gradient
method [24]. The function proxr∥·∥2
1 is the proximity operator of the function
r∥· ∥2
1, where r :=
ϵ
2αk and it is deﬁned, for every z ∈Rn, as
proxr∥·∥2
1(z) = argmin
x∈Rn
1
2∥x −z∥2
2 + r∥x∥2
1

.
(11.8)
To solve problem (11.8), we use the following identity [21, Lemma 26]
∥x∥2
1 = inf
( n
X
i=1
x2
i
λi
: λ > 0,
n
X
i=1
λi = 1
)
where λ ∈Rn denotes the vector (λ1, . . . , λn) and λ > 0 means that all
components of λ must be greater than zero. Replacing the above expression
in the right hand side of (11.8), ﬁxing λ and minimizing over x we obtain the
solution
xi(λ) =
λizi
2r + λi
.
(11.9)
Using this equation, we obtain the problem
min
( n
X
i=1
rz2
i
2r + λi
: λ ≥0,
n
X
i=1
λi = 1
)
.
One veriﬁes that the minimizing λ is given by
λi = (ρ|zi| −2r)+
where the positive parameter ρ is found by binary search in order to ensure
that ∥λ∥1 = 1. Finally, we replace the obtained value of λ in the right hand
side of (11.9) to obtain the solution of (11.8).
11.4
Robust Canonical Correlation Analysis
In this section, we sketch an extension of the proposed approach to
the problem of maximizing the correlation between the one-dimensional

Regularized Robust Portfolio Estimation
245
Algorithm 11 Robust cca
Choose a starting point z0 ∈Rn and tolerance parameter tol.
for k = 0, 1, . . . do
Let (ˆx, ˆy) = ˆz where ˆz = argmin{φ(z|zk) : z ∈Rn}
Set zk+1 = (xk+1, yk+1) with xk+1 = ˆx/
√
ˆx⊤Γˆx, yk+1 = ˆy/√ˆy⊤Σˆy.
If |η(zk+1) −η(zk)| ≤tol terminate.
end for
FIGURE 11.1: Synthetic Data: Solution path (left), Test Autocorrelation
(center), and Test Cumulative Return (right), as a function of the regulariza-
tion parameter ϵ, for the ℓ1-Method.
projections of the time series rt and a second m-dimensional series, denoted
by st. Speciﬁcally, for every pair of vectors x ∈Rn and y ∈Rm, we deﬁne the
cross-correlation function
ρ(x, y) =
x⊤Θy
√
x⊤Γx√y⊤Σy
(11.10)
where Γ and Σ are estimates of the covariance of rt and st respectively (both
assumed to be invertible) and the n × m matrix Θ is an estimate of the cross-
covariance of rt and st. This problem is valuable when additional time series st
(e.g., commodities, other markets, series formed by technical indicators, etc.),
which co-vary with the main stock series rt are at hand. Notice also that if
st = rt−1 and x = y we recover problem (11.1).
Our goal is to solve the problem
max
x,y ρ(x, y) = max
x,y
2x⊤Θy
x⊤Γx + y⊤Σy
(11.11)
where the equality follows by the arithmetic-geometric mean inequality and
the fact that the solutions are invariant by rescaling; see, e.g., [2, 15]. The
right problem is a generalized eigenvalue problem of the form
max{z
⊤Az : z ∈Rn+m, z
⊤Bz = 1},

246
Regularization, Optimization, Kernels, and Support Vector Machines
where
z =
 x
y

,
A =

0
Θ
Θ⊤
0

,
B =
 Γ
0
0
Σ

.
We can now have an extension of Problem (11.3) to the case of canonical
correlation analysis (cca) where again we consider robustness with respect to
perturbations of matrix Θ, that is, we consider the problem
max
x,y min
Θ∈A ρ(x, y|Θ)
(11.12)
for A = {Θ : ∥Θ −ˆΘ∥p ≤ϵ}. Using Lemma 11.1 to compute the inner
minimum in (11.12), we obtain the problem
max
x,y
x⊤ˆΘy −ϵ∥x∥q∥y∥q
√
x⊤Γx√y⊤Σy
.
(11.13)
Two choices of interest are q = 2 and q = 1. In the latter case we obtain again
an interpretation of the problem for large values of ϵ, which is equivalent to
selecting the pair of most positively correlated series, studied in [19]. This
analysis requires that Γ and Σ have all their diagonal elements equal to one.
Indeed, if ϵ is suﬃciently large the second term in the numerator of (11.13)
dominates, hence we want this term to be as small as possible. Thus, it must
be the case that x ∈{e1, . . . , en} and y ∈{e1, . . . , em}. We conclude that the
solution of (11.13) is given by x = ej, y = ek such that ˆΘjk = max{ˆΘiℓ: i =
1, . . . , n, ℓ= 1, . . . , m}. A similar reasoning applies for the most negatively
autocorrelated series pair. This is obtained by replacing Θ by −Θ in the above
analysis.
Algorithm 11 presents a method to ﬁnd a solution of Problem (11.13).
The algorithm, which is similar in spirit to Algorithm 10, starts from a vector
z0 and iteratively decreases the objective by solving the convex optimization
problem
φ
 z|zk
= z
⊤ N + |η
 zk
|B

z −
 z −zk⊤ 2Pzk + ϵu + 1{η(zk)>0}v

(11.14)
where P = (A)+, N = P −A, u ∈∂∥zk∥2
2,q, and v ∈∂
 ∥Γ
1
2 xk∥2 +∥Σ
1
2 yk∥2
2.
Similarly to Lemma 11.2 one can show that function φ(·|zk) has the property
that if φ(z|zk) < 0 then η(z) < η(zk). Furthermore this function can be
optimized by means of proximal gradient methods; see, e.g., [4, 24]. A detailed
description of the algorithm in presented in the appendix.
11.5
Experiments
In this section, we present experiments with the proposed methods on a
synthetic and a real dataset. As the main goal of this chapter is to study the

Regularized Robust Portfolio Estimation
247
use of robust optimization through the development of regularization methods
for time series prediction, a key aim of the experiments is to explore whether
robustness — and the corresponding regularization — has the potential to
improve test performance when the data is highly noisy, such as in the case of
ﬁnancial data. Under these circumstances, one would expect that as we “add
robustness” by increasing the regularization parameter ϵ, the test performance
of the methods improves and, potentially, drops after some point.
11.5.1
Synthetic Data
We used synthetic data to test the ℓ1 method above. We generated 30
synthetic time series of length 200. Among these, three were governed by a
stationary AR(1) process [25], whereas the remaining 27 series were white
noise (normally distributed with zero mean and unit variance). The AR(1)
time series were centered and normalized in order to have the same mean and
variance as the remaining 27 time series. We trained Algorithm 10 using this
dataset for 100 values of the regularization parameter ϵ. The left plot in Figure
11.1 shows the weights of the portfolio found by Algorithm 10 as a function of
ϵ. It can be seen that for the portfolios of sparsity less than or equal to 3, the
AR(1) time series were heavily favored by our method. While the noise time
series had non-zero weights for small values of ϵ, these weights were usually
smaller than the weights of the AR(1) time series for all values of ϵ, and, as ϵ
increased, they tended to be suppressed to zero earlier than the weights of the
AR(1) time series. The next two plots in Figure 11.1 show the performance
of the portfolios on 800 consecutive test data points, again as a function of ϵ.
Both test performances are maximized at approximately the same value of ϵ
in this case.
11.5.2
S&P 500 Stock Data
Next, we tested the methods using a dataset that is known to be notori-
ously challenging: we focused on the problem of constructing portfolios x of
n stocks using daily adjusted close prices of stocks in the S&P 500 index.4
We used data for the past 10 years, from January 1, 2003 until April 12,
2013 (the date of the ﬁnal data construction). This corresponds to a total of
2586 daily (close to close) returns. We tested the methods by constructing
portfolios using only stocks from speciﬁc sectors, for a number of diﬀerent
sectors. This was done both in order to perform multiple experiments and
because companies in the same sector are known to “co-move” [3], making
the proposed methods more applicable. We consider four large sectors deﬁned
4The data (downloaded from Yahoo!) and the R code used to run the experiments are
available from the authors upon request.

248
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 11.2: S&P 500 stock data: validation ν, validation cumulative
return, and test cumulative return over 1336 days for the ℓ2-Method (left)
and ℓ1-Method (right) as functions of the regularization parameter ϵ.

Regularized Robust Portfolio Estimation
249
TABLE 11.1: Comparison of methods for the S&P 500 Stock Data: for each
of the four sectors we note with Market the average of the stocks in that sec-
tor, with Market ac the selection between momentum and mean reversion
of the market, with Max ac the non-regularized maximum autocorrelation
solution (ϵ = 0), with Selected L1/L2
the regularization methods with
the selected regularization parameter ϵ using the validation data, and with
Best L1/L2 the best ϵ using the test data (hence with hindsight). Cumu-
lative returns during 1336 test days are reported, with annual Sharpe ratio
(values above 0.85 are statistically signiﬁcant, see text) in parentheses. Best
performance for each sector, without considering the “best” cases, is indicated
in bold.
Method
Healthcare
Financials
Energy
Technology
Market
37.5% (0.47)
2.0% (0.01)
7.8% (0.04)
28.1% (0.19)
Market AC
57.2% (0.47)
353.3% (1.60)
66.25% (0.31)
22.44% (0.15)
Max AC
53.6% (1.52)
115.7% (1.80)
-3.6% (-0.12)
5.1% (0.16)
Selected L2
85.8% (0.77)
300.8 % (1.36)
34.4% (0.17)
52.7% (0.34)
Best L2
107.3% (2.13)
346.3% (1.57)
67.7% (0.30)
88.56% (0.71)
Selected L1
94.0%
(1.02)
439.6% (1.43)
161.1% (1.03)
-24.7% (-0.47)
Best L1
145.7% (1.04)
446.9% (1.48)
162.9% (0.88)
281.3% (0.83)
based on a standard industry classiﬁcation:5 energy, ﬁnancial, healthcare, and
technology (other sectors led to similar conclusions). These consist of, respec-
tively, n = 30, 30, 24, and 35 stocks. For each sector we use the ﬁrst 1000
days for training, the next 250 days for validation, and the remaining 1336
days for testing.6 The values of the regularization parameter ϵ considered were
{0, 0.1k, 1010} for k between −6 and 6 at increments of 0.005 (hence a total of
2403 values) for the ℓ2 method, and for k between 0.05 and 5 at increments of
0.05 (hence a total of 100 values) for the ℓ1 method, which led to “complete
U-curves” below for both methods. We used fewer values for the ℓ1 method as
it is a computationally more costly one. We report the following performance
metrics:
• Cumulative return: the sum of the 1336 daily returns of the constructed
portfolios. It corresponds to the cumulative returns one would get if one
had invested using the method over the period’s last 1336 days, investing
“one unit” every day.
5Based on the industry classiﬁcation at http://www.nasdaq.com/screening/industries.
aspx, sorted by market capitalization, and using only those companies with market capital-
ization larger than $10 billion in April 2013.
6Although the conclusions are similar for other data splits, the problem (discussed below)
that the selected parameter ϵ changes across windows (for some of which the performances
of the diﬀerent portfolios tested are similar) makes the analysis of the “average across
windows” eﬀects of ϵ on performance (ﬁgures below) noisy. To better present this eﬀect we
only report the results for one data split.

250
Regularization, Optimization, Kernels, and Support Vector Machines
• Yearly Sharpe ratio: the ratio of the average daily return over the stan-
dard deviation of the returns in the 1336 days. We scale it by a factor
of
√
250 to get an annual Sharpe ratio as typically reported in practice,
e.g., [25]. Note that, as we have 1336 test data, any Sharpe ratio larger
than
1.96
√
1336/250 = 0.85 is statistically signiﬁcant.
We report the performances of the two methods (ℓ1 and ℓ2) using both the
regularization parameter ϵ selected using the cumulative returns in the vali-
dation data, and the optimal regularization parameter based on the test data
(hence with hindsight). The latter indicates potential space for improvement,
as discussed below. We also report the performance of three benchmarks:
(a) Market: the buy-and-hold of the average of the stocks during the same
test window. This is typically used as a benchmark in practice and beat-
ing the market, which is a key challenge, means doing better than this
benchmark.
(b) Market AC: the portfolio x with equal weights (i.e., 1/n) for all stocks.
This corresponds to doing mean-reversion or momentum of the market
of the stocks considered.
(c) Max AC: the maximally autocorrelated portfolio, corresponding to the
non-regularization based solution, which is obtained for ϵ = 0.
For comparison in all cases we normalize the solution x so that it has ℓ1 norm
1 (hence in all cases “we invest 1 unit every day”).
Figure 11.2 shows the validation (minus) autocorrelation, the validation
cumulative return, and the test cumulative return as a function of the regu-
larization parameter ϵ, both for the case of the ℓ2 (top row) and ℓ1 (bottom
row) methods. The plots shown are for mean reversion, hence most negatively
autocorrelated portfolio, for the healthcare sector; similar conclusions can be
drawn from plots for the other sectors and for momentum/most positively
autocorrelated portfolio. The ﬁgure illustrates the main experimental ﬁnding:
using the proposed robust optimization approach improves performance and,
more interestingly, the observed inverted U-curve indicates that the proposed
methodologies capture “structure” even in the highly unpredictable S&P 500
daily stock returns time series.
Using the validation data, we choose between momentum (maximum posi-
tive autocorrelation) and mean reversion (minimum negative autocorrelation),
and select the regularization parameter for the proposed methods. We report
the performances in Table 11.1. For each case we report the cumulative re-
turn and the Sharpe ratio (in parentheses) in the test data. From the values
in Table 1 we can make the following observations.
1. For all sectors, the proposed approach leads to portfolios that outper-
form the sector’s Market. Given that in practice outperforming the
market, particularly for stocks in the S&P 500 index, is considered chal-
lenging, the results indicate the potential of the proposed approach.

Regularized Robust Portfolio Estimation
251
2. For all sectors, regularization improves performance relative to the case
ϵ = 0 (Max AC).
3. For all sectors the best ϵ with hindsight is, as expected, (much) better
than the performance of the selected ϵ. This indicates that there can
be further improvements if a better method to select ϵ for this data is
developed. Note that Figure 11.2 also illustrates this challenge of select-
ing ϵ for the speciﬁc ﬁnancial data: the best performing regularization
parameter ϵ for the validation data may diﬀer from that in the test data.
Although the “inverse U-curves” are observed for diﬀerent time windows
indicating that the proposed methods capture structure in this data, se-
lecting the regularization parameter in a “rolling window” setup can be
a challenge in practice as the “U-curve” may shift across time windows
(e.g., this structure may be non-stationary).
4. The largest performance improvement is for the ﬁnancial sector. This
indicates that it may be the case that the proposed methods work better
for certain groups of time series/stocks.7 Future work can improve our
understanding of the characteristics of such groups of time series and/or
lead to other methods for diﬀerent types of groups of time series.
11.6
Conclusion and Future Research
We proposed an approach to estimate large autocorrelation portfolios us-
ing regularization methods derived from a robust optimization formulation.
We developed two regularization methods as special cases of the proposed
general approach. For one of these methods we developed an iterative opti-
mization learning algorithm that estimates sparse portfolios. We then tested
the methods using notoriously noisy ﬁnancial time series data. The experi-
ments indicate the potential of the proposed approach to uncover structure in
time series of daily S&P 500 stock returns. The results also indicate that the
proposed method can lead to portfolios that outperform “the market” for this
data. Finally, we discussed an extension and a novel algorithm for the more
general case of cca.
A number of future research directions can further improve the proposed
approach. One of the key questions is the selection of the regularization param-
eter for (non-stationary, among others) time series such as the stock data we
explored. Another question may be to build on the proposed methods in order
to better identify subsets of time series for which the approach performs best.
7Note however that short selling ﬁnancial companies may require higher transaction
costs, but as noted we do not consider such costs in this chapter.

252
Regularization, Optimization, Kernels, and Support Vector Machines
Yet another direction for research is to further develop the proposed cca ap-
proach and test it using potentially diverse “predictors”, e.g., for the ﬁnancial
time series explored. Finally, potentially novel regularization methodologies
for time series analysis can be developed based on the robust optimization
approach used in this chapter.
11.7
Appendix: Robust cca Algorithm
We describe the main steps behind Algorithm 11, which solves the ro-
bust cca problem. If f is a convex function we denote by Linf,u(z|z0) =
f(z0) + u⊤(z −z0) a linear approximation of f at z0, for some u ∈∂f(z0).
We sometimes omit u and write Linf(z|z0) to denote a generic linear approx-
imation. This approximation can be visualized as a linear lower bound for f,
which touches f at z0.
We ﬁrst rewrite Problem (11.13) as that of minimizing the quantity
η(x, y) = −x⊤ˆΘy + ϵ∥x∥q∥y∥q
√
x⊤Γx√y⊤Σy
(11.15)
over x ∈Rn and y ∈Rm. Recall the notation
z =
 x
y

,
A =

0
ˆΘ
ˆΘ⊤
0

,
B =
 Γ
0
0
Σ

and note that −2x⊤ˆΘy = z⊤Az. Using the equality 2∥x∥q∥y∥q = (∥x∥q +
∥y∥q)2−∥x∥2
q−∥y∥2
q and the formula A = P −N, for P = (A)+ and N = P −A,
twice the numerator in (11.15) has the DC decomposition (diﬀerence of convex
functions)
R1(z) −R2(z) =

z
⊤Nz + ϵ∥z∥2
1,q

−

z
⊤Pz + ϵ∥z∥2
2,q

where we deﬁned the mixed norms ∥z∥1,q = ∥x∥q + ∥y∥q and ∥z∥2,q =
q
∥x∥2q + ∥y∥2q. Using this formula, problem (11.13) can be rewritten as
min
z
(
z⊤Nz −z⊤Pz + ϵ∥z∥2
1,q −ϵ∥z∥2
2,q
2
√
x⊤Γx√y⊤Σy
)
.
(11.16)
For every vector z0 ∈Rn+m we shall construct a function φ(·|z0) that has
the property that if φ(z|z0) < 0 then η(z) < η(z0). We distinguish between
two cases.
Case 1: η(z0) ≤0. We replace the denominator in (11.16) by the quadratic
form z⊤Bz, which provides a simpliﬁcation of the problem. Indeed, if the

Regularized Robust Portfolio Estimation
253
objective is negative by the arithmetic-geometric mean inequality, we obtain
that
η(z) ≤h(z) := R(z)
z⊤Bz
and equality holds if and only if x⊤Γx = y⊤Σy.
Fix z0 ∈Rn+m and let φ(·|z0) be the convex function deﬁned, for every
z ∈Rn+m, as
φ(z|z0) = R1(z) −LinR2(z, z0) −η(z0)z
⊤Bz.
(11.17)
Note that φ(z0|z0) = 0. If z is a point such that φ(z|z0) < 0, we conclude that
h(z) < h(z0). Indeed,
0 > φ(z|z0) ≥R1(z) −R2(z) −h(z0)z
⊤Bz.
Furthermore, if we rescale z so that x⊤Γx = y⊤Σy, then η(z) = h(z).
Case 2: η(z0) > 0. In this case we need to work with the original denomina-
tor,
√
x⊤Γx√y⊤Σy. We rewrite twice this quantity as the DC decomposition
S1(z) −S2(z) =

∥Γ
1
2 x∥2 + ∥Σ
1
2 y∥2
2
−x
⊤Γx −y
⊤Σy.
Now, we choose
φ(z|z0) = R1(z) −LinR2(z, z0) −η(z0)
 LinS1(z|z0) −S2(z)

.
(11.18)
This case is slightly more diﬃcult to handle since we also need to compute a
subgradient of the function S1. Combining [18, Chapter VI, Theorems 4.2.1
and 4.3.1], we obtain that
∂

∥Γ
1
2 x∥2 + ∥Σ
1
2 y∥2
2
=
2(∥Γ
1
2 x∥2 + ∥Σ
1
2 y∥2)
n
(Γ
1
2 α, Σ
1
2 β) : α ∈∂∥Γ
1
2 x∥2, β ∈∂∥Σ
1
2 y∥2
o
.
We are now ready to summarize the formula for φ(z|z0). Combining equa-
tions (11.17) and (11.18) we have
φ(z|z0) = z
⊤(N + |η(z0)|B)z −(z −z0)
⊤(2Pz0 + ϵu + 1{η(z0)>0}v)
where u ∈∂∥z0∥2
2,q and v ∈∂S1(z0).
Algorithm 11 solves a sequence of convex optimization problems of the
form
min{φ(z|zk) : z ∈Rn+m}.
In practice it is suﬃcient to solve this problem approximately, ﬁnding a point
z such that φ(z|zk) < 0. Similarly to Lemma 11.2 it is easily seen that if
φ(z|zk) < 0 then η(z) < η(z0). The simplest updating rule is provided by one
step of the proximal gradient method [4, 24]
zk+1 = proxr∥·∥2
1,q

zk −
1
|||N + |ηk|B |||

(A + |ηk|B)zk −ϵu + (ηk)+v
2


254
Regularization, Optimization, Kernels, and Support Vector Machines
where, recall, |||·||| denotes the spectral norm of a matrix, r =
ϵ
2|||N+|η(z0)|B|||,
and we have deﬁned ηk = η(zk).
Next, we discuss how to compute a subgradient of ∥z∥2
2,q and the proximity
operator when q ∈{1, 2}. For this purpose, we recall that if ∥· ∥is a norm,
then by [18, S4.3] the subdiﬀerential of ∥· ∥2 at z is equal to 2∥z∥times the
subdiﬀerential of ∥·∥at z, that is ∂∥z∥2 = 2∥z∥∂∥z∥. In particular, we obtain
that
∂∥z∥2
2,1 = {2(∥x∥1α, ∥y∥1β) : α ∈∂∥x∥1, β ∈∂∥y∥1}.
On the other hand, if q = 2 then ∥z∥2
2,2 is just the square ℓ2 norm of z and its
gradient is equal to 2z.
It remains to obtain the formula for the proximity operator deﬁned above.
The case q = 1 is conceptually identical to the derivation of the proximity
operator presented at the end of Section 11.3, with the understanding that n
is replaced by n + m and x by the vector z. The case q = 2 is derived along
the same lines and we only sketch the main points here. We obtain, for every
z = (x, y) ∈Rn+m, that
proxr∥z∥2
1,2(z) =

λx
2r + λ,
τ y
2r + τ

where λ = (ρ∥x∥2 −2r)+, τ = (ρ∥y∥2 −2r)+ and the positive parameter ρ is
determined by binary search in order to ensure that λ + τ = 1.
Bibliography
[1] A. d’Aspremont. Identifying small mean-reverting portfolios. Quantita-
tive Finance, 11(3):351–364, 2011.
[2] T.M. Anderson. An Introduction to Multivariate Statistical Analysis.
John Wiley & Sons, second edition, 1984.
[3] M. Avellaneda and J.H. Lee. Statistical arbitrage in the U.S. equities
market. Quantitative Finance, 10:761-782, 2010.
[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algo-
rithm for linear inverse problems. SIAM Journal of Imaging Sciences, 2
(1):183–202, 2009.
[5] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization,
Princeton University Press, 2009.
[6] M. Borga. Learning Multidimensional Signal Processing. Link¨ping Studies
in Science and Technology. Dissertations No. 531, Linköping University,
Sweden, 1998.

Regularized Robust Portfolio Estimation
255
[7] J.Y. Campbell, A.W.C. Lo, and A.C. MacKinlay. The Econometrics of
Financial Markets. Princeton University Press, 1997.
[8] G. Cornuejols and R. Tütüncü. Optimization Methods in Finance. Cam-
bridge University Press, 2011.
[9] K.-C. Chang. Variational methods for non-diﬀerentiable functionals and
their applications to partial diﬀerential equations. Journal of Mathemat-
ical Analysis and Applications, 80:102-129, 1981.
[10] F.H. Clarke. Optimization and Nonsmooth Analysis. Wiley, 1983.
[11] V. DeMiguel and F.J. Nogales. Portfolio selection with robust estimation.
Operations Research, 57(3):560–577, 2009.
[12] G.H. Golub and C.F. Van Loan. Matrix Computations. John Hopkins
University Press, 1996.
[13] H. Hotelling. Relations between two sets of variates. Biometrika, 28:321–
377, 1936.
[14] T.-K. Kim, J. Kittler, and R. Cipolla. Discriminative learning and recog-
nition of image set classes using canonical correlations. IEEE Trans. Pat-
tern Analysis and Machine Intelligence, 29(6):1005–1018, 2007.
[15] D.R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation
analysis: An overview with application to learning methods. Neural Com-
putation, 16(12):2639–2664, 2004.
[16] M. Hein and T. Buehler. An inverse power method for nonlinear eigen-
problems with applications in 1-spectral clustering and sparse PCA. Ad-
vances in Neural Information Processing Systems 23, pages 847–855,
2010.
[17] M. Hein and S. Setzer, Beyond spectral clustering - tight relaxations of
balanced graph cuts. Advances in Neural Information Processing Systems
24, pages 2366–2374, 2011.
[18] J.-B. Hiriart-Urruty and C. Lemaréchal. Convex Analysis and Minimiza-
tion Algorithms, Part I. Springer, 1996.
[19] A.W. Lo and A.C. MacKinlay. When are contrarian proﬁts due to stock
market overreaction? Review of Financial studies, 1990.
[20] A. Maurer. Unsupervised slow subspace-learning from stationary pro-
cesses. Theoretical Computer Science, 405(3):237–255, 2008.
[21] C.A. Micchelli and M. Pontil. Learning the kernel function via regular-
ization. J. Machine Learning Research, 6:1099-1125, 2005.

256
Regularization, Optimization, Kernels, and Support Vector Machines
[22] H.Q. Minh and L. Wiskott. Multivariate slow feature analysis and decor-
relation ﬁltering for blind source separation. IEEE Transactions on Image
Processing, 22(7):2737–2750, 2013.
[23] J. Moody and M. Saﬀell. Learning to trade via direct reinforcement.
Neural Networks, IEEE Transactions on, 12(4):875–889, 2001.
[24] Y. Nesterov. Gradient methods for minimizing composite objective func-
tions. ECORE Discussion Paper, 2007/96, 2007.
[25] R.S. Tsay. Analysis of Financial Time Series. John Wiley & Sons, 2002.
[26] P. Xanthopoulos, M.R. Guarracino, and P.M. Pardalos. Robust gener-
alized eigenvalue classiﬁer with ellipsoidal uncertainty. Annals of Opera-
tions Research, 2013.
[27] L. Wiskott and T.J. Sejnowski. Slow feature analysis: unsupervised learn-
ing of invariances. Neural Computation, 14:715–770, 2002.
[28] D.M. Witten, R. Tibshirani, and T. Hastie. A penalized matrix decom-
position, with applications to sparse principal components and canonical
correlation analysis. Biostatistics, 10(3):515–534, 2009.

Chapter 12
The Why and How of Nonnegative
Matrix Factorization
Nicolas Gillis∗
Department of Mathematics and Operational Research, Faculté Polytechnique,
Université de Mons
12.1
Summary .........................................................
258
12.2
Introduction ......................................................
258
12.3
The Why — NMF Generates Sparse and Meaningful Features .
259
12.3.1
Image Processing — Facial Feature Extraction .........
260
12.3.2
Text Mining — Topic Recovery and Document
Classiﬁcation .............................................
261
12.3.3
Hyperspectral Unmixing — Identify Endmembers and
Classify Pixels ...........................................
262
12.4
The How — Some Algorithms ...................................
263
12.4.1
Standard NMF Algorithms ..............................
265
12.4.1.1
First-Order Optimality Conditions ........
266
12.4.1.2
Multiplicative Updates .....................
266
12.4.1.3
Alternating Least Squares .................
268
12.4.1.4
Alternating Nonnegative Least Squares ...
268
12.4.1.5
Hierarchical Alternating Least Squares ....
269
12.4.1.6
Comparison ................................
270
12.4.1.7
Stopping Criterion .........................
270
12.4.1.8
Initialization ...............................
272
12.4.2
Near-Separable NMF ....................................
273
12.4.2.1
Self-Dictionary and Sparse Regression
Framework
................................
275
12.4.2.2
Geometric Algorithms .....................
276
12.5
Connections with Problems in Mathematics and Computer
Science ...........................................................
279
12.6
Conclusion ........................................................
281
Acknowledgments ................................................
282
Bibliography ......................................................
282
∗nicolas.gillis@umons.ac.be
257

258
Regularization, Optimization, Kernels, and Support Vector Machines
12.1
Summary
Nonnegative matrix factorization (NMF) has become a widely used tool for
the analysis of high-dimensional data as it automatically extracts sparse and
meaningful features from a set of nonnegative data vectors. We ﬁrst illustrate
this property of NMF on three applications, in image processing, text mining,
and hyperspectral imaging — this is the why. Then we address the problem
of solving NMF, which is NP-hard in general. We review some standard NMF
algorithms, and also present a recent subclass of NMF problems, referred to
as near-separable NMF, that can be solved eﬃciently (that is, in polynomial
time), even in the presence of noise — this is the how. Finally, we brieﬂy
describe some problems in mathematics and computer science closely related
to NMF via the nonnegative rank.
12.2
Introduction
Linear dimensionality reduction (LDR) techniques are a key tool in data
analysis, and are widely used: for example, for compression, visualization,
feature selection, and noise ﬁltering. Given a set of data points xj ∈Rp for
1 ≤j ≤n and a dimension r < min(p, n), LDR amounts to computing a set
of r basis elements wk ∈Rp for 1 ≤k ≤r such that the linear space spanned
by the wk’s approximates the data points as closely as possible, that is, such
that we have for all j
xj ≈
r
X
k=1
wkhj(k),
for some weights hj ∈Rr.
(12.1)
In other words, the p-dimensional data points are represented in an r-
dimensional linear subspace spanned by the basis elements wk’s and whose
coordinates are given by the vectors hj’s. LDR is equivalent to low-rank ma-
trix approximation: in fact, constructing
• the matrix X ∈Rp×n such that each column is a data point, that is,
X(:, j) = xj for 1 ≤j ≤n,
• the matrix W ∈Rp×r such that each column is a basis element, that is,
W(:, k) = wk for 1 ≤k ≤r, and
• the matrix H ∈Rr×n such that each column of H gives the coordinates
of a data point X(:, j) in the basis W, that is, H(:, j) = hj for 1 ≤j ≤n,

The Why and How of Nonnegative Matrix Factorization
259
the above LDR model (12.1) is equivalent to X ≈WH, that is, to approximate
the data matrix X with a low-rank matrix WH.
A ﬁrst key aspect of LDR is the choice of the measure to assess the qual-
ity of the approximation. It should be chosen depending on the noise model.
The most widely used measure is the Frobenius norm of the error, that is,
||X −WH||2
F = P
i,j(X −WH)2
ij. The reason for the popularity of the Frobe-
nius norm is two-fold. First, it implicitly assumes the noise N present in the
matrix X = WH + N to be Gaussian, which is reasonable in many practical
situations (see also the introduction of Section 12.4). Second, an optimal ap-
proximation can be computed eﬃciently through the truncated singular value
decomposition (SVD); see [57] and the references therein. Note that the SVD
is equivalent to principal component analysis (PCA) after mean centering of
the data points (that is, after shifting all data points so that their mean is on
the origin).
A second key aspect of LDR is the assumption on the structure of the
factors W and H. The truncated SVD and PCA do not make any assumption
on W and H. For example, assuming independence of the columns of W leads
to independent component analysis (ICA) [29], or assuming sparsity of W
(and/or H) leads to sparse low-rank matrix decompositions, such as sparse
PCA [32]. Nonnegative matrix factorization (NMF) is an LDR where both
the basis elements wk’s and the weights hj’s are assumed to be component-
wise nonnegative. Hence NMF aims at decomposing a given nonnegative data
matrix X as X ≈WH where W ≥0 and H ≥0 (meaning that W and H are
component-wise nonnegative). NMF was ﬁrst introduced in 1994 by Paatero
and Tapper [97] and gathered more and more interest after an article by Lee
and Seung [79] in 1999.
In this paper, we explain why NMF has been so popular in diﬀerent data
mining applications, and how one can compute NMF’s. The aim of this paper is
not to give a comprehensive overview of all NMF applications and algorithms
(and we apologize for not mentioning many relevant contributions) but rather
to serve as an introduction to NMF, describing three applications and several
standard algorithms.
12.3
The Why — NMF Generates Sparse and
Meaningful Features
The reason why NMF has become so popular is because of its ability to au-
tomatically extract sparse and easily interpretable factors. In this section, we
illustrate this property of NMF through three applications, in image process-
ing, text mining, and hyperspectral imaging. Other applications include air
emission control [97], computational biology [34], blind source separation [22],

260
Regularization, Optimization, Kernels, and Support Vector Machines
single-channel source separation [82], clustering [35], music analysis [42], col-
laborative ﬁltering [92], and community detection [106].
12.3.1
Image Processing — Facial Feature Extraction
Let each column of the data matrix X ∈Rp×n
+
be a vectorized gray-
level image of a face, with the (i, j)th entry of matrix X being the intensity
of the ith pixel in the jth face. NMF generates two factors (W, H) so that
each image X(:, j) is approximated using a linear combination of the columns
of W; see Equation (12.1), and Figure 12.1 for an illustration. Since W is
FIGURE 12.1: Decomposition of the CBCL face database, MIT Center for
Biological and Computation Learning (2429 gray-level 19-by-19 pixels images)
using r = 49 as in [79].
nonnegative, the columns of W can be interpreted as images (that is, vectors
of pixel intensities), which we refer to as the basis images. As the weights
in the linear combinations are nonnegative (H ≥0), these basis images can
only be summed up to reconstruct each original image. Moreover, the large
number of images in the data set must be reconstructed approximately with
only a few basis images (in fact, r is in general much smaller than n), hence
the latter should be localized features (hence sparse) found simultaneously in
several images. In the case of facial images, the basis images are features such
as eyes, noses, mustaches, and lips (see Figure 12.1) while the columns of H
indicate which feature is present in which image (see also [79, 61]).
A potential application of NMF is in face recognition. It has for example
been observed that NMF is more robust to occlusion than PCA (which gener-
ates dense factors): in fact, if a new occluded face (e.g., with sunglasses) has
to be mapped into the NMF basis, the non-occluded parts (e.g., the mustache
or the lips) can still be well approximated [61].

The Why and How of Nonnegative Matrix Factorization
261
12.3.2
Text Mining — Topic Recovery and Document
Classiﬁcation
Let each column of the nonnegative data matrix X correspond to a doc-
ument and each row to a word. The (i, j)th entry of the matrix X could for
example be equal to the number of times the ith word appears in the jth
document in which case each column of X is the vector of word counts of
a document; in practice, more sophisticated constructions are used, e.g., the
term frequency-inverse document frequency (tf-idf). This is the so-called bag-
of-words model: each document is associated with a set of words with diﬀerent
weights, while the ordering of the words in the documents is not taken into
account (see, e.g., the survey [10] for a discussion). Note that such a matrix X
is in general rather sparse as most documents only use a small subset of the
dictionary. Given such a matrix X and a factorization rank r, NMF generates
two factors (W, H) such that, for all 1 ≤j ≤n, we have
X(:, j)
| {z }
jth document
≈
r
X
k=1
W(:, k)
| {z }
kth topic
H(k, j)
| {z }
importance of kth topic
in jth document
,
(12.2)
where W ≥0 and H ≥0. This decomposition can be interpreted as follows
(see, also, e.g., [79, 101, 3]):
• Because W is nonnegative, each column of W can be interpreted as a
document, that is, as a bag of words.
• Because the weights in the linear combinations are nonnegative (H ≥0),
one can only take the union of the sets of words deﬁned by the columns
of W to reconstruct all the original documents.
• Moreover, because the number of documents in the data set is much
larger than the number of basis elements (that is, the number of columns
of W), the latter should be a set of words found simultaneously in sev-
eral documents. Hence the basis elements can be interpreted as topics,
that is, sets of words found simultaneously in diﬀerent documents, while
the weights in the linear combinations (that is, the matrix H) assign
the documents to the diﬀerent topics, that is, identify which document
discusses which topic.
Therefore, given a set of documents, NMF identiﬁes topics and simultane-
ously classiﬁes the documents among these diﬀerent topics. Note that NMF
is closely related to existing topic models, in particular probabilistic latent
semantic analysis and indexing (PLSA and PLSI) [45, 37].

262
Regularization, Optimization, Kernels, and Support Vector Machines
12.3.3
Hyperspectral Unmixing — Identify Endmembers and
Classify Pixels
Let the columns of the nonnegative data matrix X be the spectral sig-
natures of the pixels in a scene being imaged. The spectral signature of a
pixel is the fraction of incident light being reﬂected by that pixel at diﬀerent
wavelengths, and is therefore nonnegative. For a hyperspectral image, there
are usually between 100 and 200 wavelength-indexed bands, observed in much
broader spectrum than the visible light. This allows for more accurate analysis
of the scene under study.
Given a hyperspectral image (see Figure 12.2 for an illustration), the goal
FIGURE 12.2: Decomposition of the urban hyperspectral image from http:
//www.agc.army.mil/, constituted mainly of six endmembers (r = 6). Each
column of the matrix W is the spectral signature of an endmember, while each
row of the matrix H is the abundance map of the corresponding endmember,
that is, it contains the abundance of all pixels for that endmember. (Note
that to obtain this decomposition, we used a sparse prior on the matrix H;
see Section 12.4.)
of blind hyperspectral unmixing (blind HU) is two-fold:
1. Identify the constitutive materials present in the image; for example, it
could be grass, roads, or metallic surfaces. These are referred to as the
endmembers.
2. Classify the pixels, that is, identify which pixel contains which endmem-
ber and in which proportion. (In fact, pixels are in general mixture of
several endmembers, due for example to low spatial resolution or mixed
materials.)

The Why and How of Nonnegative Matrix Factorization
263
The simplest and most popular model used to address this problem is the
linear mixing model. It assumes that the spectral signature of a pixel results
from the linear combination of the spectral signature of the endmembers it
contains. The weights in the linear combination correspond to the abundances
of these endmembers in that pixel. For example, if a pixel contains 30% of
grass and 70% of road surface, then, under the linear mixing model, its spectral
signature will be 0.3 times the spectral signature of the grass plus 0.7 times
the spectral signature of the road surface. This is exactly the NMF model:
the spectral signatures of the endmembers are the basis elements, that is, the
columns of W, while the abundances of the endmembers in each pixel are the
weights, that is, the columns of H. Note that the factorization rank r corre-
sponds to the number of endmembers in the hyperspectral image. Figure 12.2
illustrates such a decomposition.
Therefore, given a hyperspectral image, NMF is able to compute the spec-
tral signatures of the endmembers and simultaneously the abundance of each
endmember in each pixel. We refer the reader to [8, 90] for recent surveys on
blind HU techniques.
12.4
The How — Some Algorithms
We have seen in the previous section that NMF is a useful LDR technique
for nonnegative data. The question is now: can we compute such factoriza-
tions? In this paper, we focus on the following optimization problem
min
W ∈Rp×r,H∈Rr×n ||X −WH||2
F
such that
W ≥0 and H ≥0.
(12.3)
Hence we implicitly assume Gaussian noise on the data; see Introduction.
Although this NMF model is arguably the most popular, it is not always
reasonable to assume Gaussian noise for nonnegative data, especially for sparse
matrices such as document data sets; see the discussion in [23]. In fact, many
other objective functions are used in practice, e.g., the (generalized) Kullback-
Leibler divergence for text mining [23], the Itakura-Saito distance for music
analysis [42], the ℓ1 norm to improve robustness against outliers [71], and the
earth mover’s distance for computer vision tasks [100]. Other NMF models
are motivated by statistical considerations; we refer the reader to the recent
survey [102].
There are many issues when using NMF in practice. In particular,
• NMF is NP-hard. Unfortunately, as opposed to the unconstrained
problem that can be solved eﬃciently using the SVD, NMF is NP-hard
in general [105]. Hence, in practice, most algorithms are applications
of standard nonlinear optimization methods and may only be guaran-
teed to converge to stationary points; see Section 12.4.1. However, these

264
Regularization, Optimization, Kernels, and Support Vector Machines
heuristics have been proved to be successful in many applications. More
recently, Arora et al. [4] described a subclass of nonnegative matrices
for which NMF can be solved eﬃciently. These are the near-separable
matrices that will be addressed in Section 12.4.2. Note that Arora et
al. [4] also described an algorithmic approach for exact NMF1 requiring
O
 (pn)2rr2
operations (later improved to O
 (pn)r2
by Moitra [94])
hence polynomial in the dimensions p and n for r ﬁxed. Although r
is usually small in practice, this approach cannot be used to solve real-
world problems because of its high computational cost (in contrast, most
heuristic NMF algorithms run in O(pnr) operations; see Section 12.4.1).
• NMF is ill-posed. Given an NMF (W, H) of X, there usually exist
equivalent NMF’s (W ′, H′) with W ′H′ = WH. In particular, any matrix
Q satisfying WQ ≥0 and Q−1H ≥0 generates such an equivalent
factorization. The matrix Q can always be chosen as the permutation of
a diagonal matrix with positive diagonal elements (that is, as a monomial
matrix) and this amounts to the scaling and permutation of the rank-
one factors W(:, k)H(k, :) for 1 ≤k ≤r; this is not an issue in practice.
The issue is when there exist non-monomial matrices Q satisfying the
above conditions. In that case, such equivalent factorizations generate
diﬀerent interpretations: for example, in text mining, they would lead
to diﬀerent topics and classiﬁcations; see the discussion in [48]. Here is
a simple example


0
1
1
1
1
0
1
1
1
1
0
1

=


0
1
1
1
0
1
1
1
0




1
0
0
0.5
0
1
0
0.5
0
0
1
0.5


=


1
0
0
0
1
0
0
0
1




0
1
1
1
1
0
1
1
1
1
0
1

.
We refer the reader to [66] and the references therein for recent results
on non-uniqueness of NMF.
In practice, this issue is tackled using other priors on the factors W and
H and adding proper regularization terms in the objective function.
The most popular prior is sparsity, which can be tackled with projec-
tions [64] or with ℓ1-norm penalty [72, 48]. For example, in blind HU
(Section 12.3.3), the abundance maps (that is, the rows of matrix H) are
usually very sparse (most pixels contain only a few endmembers) and ap-
plying plain NMF (12.3) usually gives poor results for these data sets.
Other priors for blind HU include piece-wise smoothness of the spec-
tral signatures or spatial coherence (neighboring pixels are more likely
to contain the same materials), which are usually tackled with TV-like
1Exact NMF refers to the NMF problem where an exact factorization is sought: X = WH
with W ≥0 and H ≥0.

The Why and How of Nonnegative Matrix Factorization
265
regularizations (that is, ℓ1 norm of the diﬀerence between neighboring
values to preserve the edges in the image); see, e.g., [68, 67], and the
references therein. Note that the design and algorithmic implementa-
tion of reﬁned NMF models for various applications is a very active area
of research, e.g., graph regularized NMF [16], orthogonal NMF [24],
tri-NMF [38, 85], semi and convex NMF [36], projective NMF [110],
minimum volume NMF [93], and hierarchical NMF [86], to cite only a
few.
• Choice of r. The choice of the factorization rank r, that is, the prob-
lem of order model selection, is usually rather tricky. Several popular
approaches are: trial and error (that is, try diﬀerent values of r and pick
the one performing best for the application at hand), estimation using
the SVD (that is, look at the decay of the singular values of the input
data matrix), and the use of experts’ insights (e.g., in blind HU, experts
might have a good guess for the number of endmembers present in a
scene); see also [7, 104, 70] and the references therein.
In this section, we focus on the ﬁrst issue. In Section 12.4.1, we present
several standard algorithms for the general problem (12.3). In Section 12.4.2,
we describe the near-separable NMF problem and several recent algorithms.
12.4.1
Standard NMF Algorithms
Almost all NMF algorithms designed for (12.3) use a two-block coordinate
descent scheme (exact or inexact; see below), that is, they optimize alterna-
tively over one of the two factors, W or H, while keeping the other ﬁxed.
The reason is that the subproblem in one factor is convex. More precisely, it
is a nonnegative least squares problem (NNLS): for example, for H ﬁxed, we
have to solve minW ≥0 ||X −WH||2
F . Note that this problem has a particular
structure as it can be decomposed into p independent NNLS in r variables
since
||X −WH||2
F =
p
X
i=1
||Xi: −Wi:H||2
2
=
p
X
i=1
Wi:
 HHT 
W T
i: −2Wi:
 HXT
i:

+ ||Xi:||2
2.
(12.4)
Many algorithms exist to solve the NNLS problem, and NMF algorithms based
on two-block coordinate descent diﬀer by which NNLS algorithm is used; see
also, e.g., the discussion in [74]. It is interesting to notice that the problem is
symmetric in W and H since ||X −WH||2
F = ||XT −HT W T ||2
F . Therefore, we
can focus on the update of only one factor and, in fact, most NMF algorithms
use the same update for W and H, and therefore adhere to the framework
described in Algorithm 12.

266
Regularization, Optimization, Kernels, and Support Vector Machines
Algorithm 12 Two-Block Coordinate Descent–Framework of Most NMF Al-
gorithms
Input: Input nonnegative matrix X ∈Rp×n
+
and factorization rank r.
Output: (W, H) ≥0: A rank-r NMF of X ≈WH.
1: Generate some initial matrices W (0)
≥0 and H(0)
≥0; see Sec-
tion 12.4.1.8.
2: for t = 1, 2, . . . † do
3:
W (t) = update
 X, H(t−1), W (t−1)
.
4:
H(t)T = update

XT , W (t)T , H(t−1)T 
.
5: end for
†See Section 12.4.1.7 for stopping criteria.
The update in steps 3 and 4 of Algorithm 12 usually guarantees the ob-
jective function to decrease. In this section, we describe the most widely used
updates, that is, we describe several standard and widely used NMF algo-
rithms, and compare them in Section 12.4.1.6. But ﬁrst we address an impor-
tant tool to designing NMF algorithms: the optimality conditions. To simplify
notations, we will drop the iteration index t.
12.4.1.1
First-Order Optimality Conditions
Given X, let us denote F(W, H) = 1
2||X −WH||2
F . The ﬁrst-order opti-
mality conditions for (12.3) are
W ≥0,
∇W F = WHHT −XHT ≥0,
W ◦∇W F = 0, (12.5)
H ≥0,
∇HF = W T WH −W T X ≥0,
H ◦∇HF = 0,
where ◦is the component-wise product of two matrices. Any (W, H) satisfying
these conditions is a stationary point of (12.3).
It is interesting to observe that these conditions give a more formal ex-
planation of why NMF naturally generates sparse solutions [51]: in fact, any
stationary point of (12.3) is expected to have zero entries because of the con-
ditions W ◦∇W F = 0 and H ◦∇HF = 0, that is, the conditions that for all
i, k either Wik is equal to zero or the partial derivative of F with respect to
Wik is, and similarly for H.
12.4.1.2
Multiplicative Updates
Given X, W, and H, the multiplicative updates (MU) modify W as follows
W ←W ◦

XHT 
[WHHT ]
(12.6)
where [ ]
[ ] denotes the component-wise division between two matrices. The MU
were ﬁrst developed in [33] for solving NNLS problems, and later rediscovered

The Why and How of Nonnegative Matrix Factorization
267
and used for NMF in [80]. The MU are based on the majorization-minimization
framework. In fact, (12.6) is the global minimizer of a quadratic function
majorizing F, that is, a function that is larger than F everywhere and is
equal to F at the current iterate [33, 80]. (Note that the majorizing function
is separable, that is, its Hessian is diagonal which explains why its global
minimizer over the nonnegative orthant can be written in closed form, which is
not possible for F.) Hence minimizing that function guarantees F to decrease
and therefore leads to an algorithm for which F monotonically decreases. The
MU can also be interpreted as a rescaled gradient method: in fact,
W ◦

XHT 
[WHHT ] = W −
[W]
[WHHT ] ◦∇W F.
(12.7)
Another more intuitive interpretation is as follows: we have that

XHT 
ik
[WHHT ]ik
≥1
⇐⇒
(∇W F)ik ≤0.
Therefore, in order to satisfy (12.5), for each entry of W, the MU either
(i) increase it if its partial derivative is negative, (ii) decrease it if its partial
derivative is positive, or (iii) leave it unchanged if its partial derivative is equal
to zero.
If an entry of W is equal to zero, the MU cannot modify it, hence it
may occur that an entry of W is equal to zero while its partial derivative is
negative, which would not satisfy (12.5). Therefore, the MU are not guaranteed
to converge to a stationary point2. There are several ways to ﬁx this issue, e.g.,
rewriting the MU as a rescaled gradient descent method, see Equation (12.7):
only entries in the same row interact, and modifying the step length [87],
or using a small positive lower bound for the entries of W and H [52, 103];
see also [5]. A simpler and nice way to guarantee convergence of the MU to
a stationary point is proposed in [23]: use the original updates (12.6) while
reinitializing zero entries of W to a small positive constant when their partial
derivatives become negative.
The MU became extremely popular mainly because (i) they are simple to
implement3, (ii) they scale well and are applicable to sparse matrices4, and
(iii) they were proposed in the paper of Lee and Seung [79], which launched the
research on NMF. However, the MU converge relatively slowly; see, e.g., [62]
for a theoretical analysis, and Section 12.4.1.6 for some numerical experiments.
Note that the original MU only update W once before updating H. They can
be signiﬁcantly accelerated using a more eﬀective alternation strategy [52]: the
2If the initial matrices are chosen positive, some entries can ﬁrst converge to zero while
their partial derivative eventually becomes negative or zero (when strict complementarity
is not met), which is numerically unstable; see [52] for some numerical experiments.
3For example, in Matlab: W = W.*(X*H’)./(W*(H*H’)).
4When computing the denominator WHHT in the MU, it is crucial to compute HHT
ﬁrst in order to have the lowest computational cost, and make the MU scalable for sparse
matrices; see, e.g., footnote 3.

268
Regularization, Optimization, Kernels, and Support Vector Machines
idea is to update W several times before updating H because the products
HHT and XHT do not need to be recomputed.
12.4.1.3
Alternating Least Squares
The alternating least squares method (ALS) ﬁrst computes the optimal
solution of the unconstrained least squares problem minW ||X −WH||F and
then projects the solution onto the nonnegative orthant:
W ←max

argminZ∈Rp×r||X −ZH||F , 0

,
where the max is taken component-wise. The method has the advantage to
be relatively cheap, and easy to implement5. ALS usually does not converge:
the objective function of (12.3) might oscillate under the ALS updates (es-
pecially for dense input matrices X; see Section 12.4.1.6). It is interesting to
notice that, because of the projection, the solution generated by ALS is not
scaled properly. In fact, the error can be reduced (sometimes drastically) by
multiplying the current solution WH by the constant
α∗= argminα≥0||X −αWH||F =
⟨X, WH⟩
⟨WH, WH⟩=
⟨XHT , W⟩
⟨W T W, HHT ⟩.
(12.8)
Although it is in general not recommended to use ALS because of the con-
vergence issues, ALS can be rather powerful for initialization purposes (that
is, perform a few steps of ALS and then switch to another NMF algorithm),
especially for sparse matrices [28].
12.4.1.4
Alternating Nonnegative Least Squares
Alternating nonnegative least squares (ANLS) is a class of methods where
the subproblems in W and H are solved exactly, that is, the update for W is
given by
W ←argminW ≥0||X −WH||F .
Many methods can be used to solve the NNLS argminW ≥0||X −WH||F , and
dedicated active-set methods have been shown to perform very well in prac-
tice6; see [72, 73, 75]. Other methods are based, for example, on projected
gradients [88], Quasi-Newton [26], or fast gradient methods [60]. ANLS is
guaranteed to converge to a stationary point [59]. Since each iteration of
ANLS computes an optimal solution of the NNLS subproblem, each itera-
tion of ANLS decreases the error the most among NMF algorithms following
the framework described in Algorithm 12. However, each iteration is compu-
tationally more expensive, and more diﬃcult to implement.
Note that, because usually the initial guess WH is a poor approximation
5For example, in Matlab: W = max(0,(X*H’)/(H*H’)).
6In particular, the Matlab function lsqnonneg implements an active-set method
from [78].

The Why and How of Nonnegative Matrix Factorization
269
of X, it does not make much sense to solve the NNLS subproblems exactly
at the ﬁrst steps of Algorithm 12, and therefore it might be proﬁtable to use
ANLS rather in a reﬁnement step of a cheaper NMF algorithm (such as the
MU or ALS).
12.4.1.5
Hierarchical Alternating Least Squares
Hierarchical alternating least squares (HALS) solves the NNLS subproblem
using an exact coordinate descent method, updating one column of W at a
time. The optimal solutions of the corresponding subproblems can be written
in closed form. In fact, the entries of a column of W do not interact (see
Equation (12.4)). Hence the corresponding problem can be decoupled into p
quadratic problems with a single nonnegative variable. HALS updates W as
follows. For ℓ= 1, 2, . . . , r:
W(:, ℓ) ←argminW (:,ℓ)≥0
X −
X
k̸=ℓ
W(:, k)H(k, :) −W(:, ℓ)H(ℓ, :)

F
←max
 
0,
XH(ℓ, :)T −P
k̸=ℓW(:, k)
 H(k, :)H(ℓ, :)T 
||H(ℓ, :)||2
2
!
.
HALS has been rediscovered several times, originally in [27] (see also [25]),
then as the rank-one residue iteration (RRI) in [63], as FastNMF in [84], and
also in [89]. Actually, HALS was ﬁrst described in Rasmus Bro’s thesis [14,
pp. 161–170] (although it was not investigated thoroughly):
. . . to solve for a column vector w of W it is only necessary to
solve the unconstrained problem and subsequently set negative
values to zero. Though the algorithm for imposing non-negativity
is thus simple and may be advantageous in some situations, it is
not pursued here. Since it optimizes a smaller subset of parameters
than the other approaches it may be unstable in diﬃcult situations.
HALS was observed to converge much faster than the MU (see [47, p. 131] for
a theoretical explanation, and Section 12.4.1.6 for a comparison) while having
almost the same computational cost; see [52] for a detailed account of the
ﬂops needed per iteration. Moreover, HALS is, under some mild assumptions,
guaranteed to converge to a stationary point; see the discussion in [52]. Note
that one should be particularly careful when initializing HALS, otherwise the
algorithm could set some columns of W to zero initially (e.g., if WH is badly
scaled with WH ≫X), hence it is recommended to initially scale (W, H)
according to (12.8); see the discussion in [47, p. 72].
In the original HALS, each column of W is updated only once before
updating H. However, as for the MU, it can be sped up by updating W
several times before updating H [52], or selecting the entries of W to update
following a Gauss-Southwell-type rule [65]. HALS can also be generalized to
other cost functions using Taylor expansion [83].

270
Regularization, Optimization, Kernels, and Support Vector Machines
12.4.1.6
Comparison
Figure 12.3 displays the evolution of the objective function of (12.3) for
the algorithms described in the previous section: on the top, the dense CBCL
data set, and, on the bottom, the sparse Classic document data set.
As anticipated in the description of the diﬀerent algorithms in the previous
sections, we observe that:
• The MU converge rather slowly.
• ALS oscillates for the dense matrix (CBCL data set) and performs quite
poorly while, for the sparse matrix (Classic data set), it converges ini-
tially very fast but then stabilizes and cannot compute a solution with
small objective function value.
• ANLS performs rather well for the dense matrix and is the second best
after HALS. However, it performs rather poorly for the sparse matrix.
• HALS performs the best as it generates the best solutions within the
allotted time.
For other comparisons of NMF algorithms and more numerical experiments,
we refer the reader to the book [28], the theses [63, 47], the survey [6], and
the references therein.
Further research on NMF includes the design of more eﬃcient algorithms,
in particular for regularized problems; see, e.g., [98] for a recent example of
imposing sparsity in a more robust and stable way. We conclude this sec-
tion with some comments about stopping criteria and initializations of NMF
algorithms.
12.4.1.7
Stopping Criterion
There are several approaches for the stopping criterion of NMF algorithms,
as in usual non-linear optimization schemes, e.g., based on the evolution of
the objective function, on the optimality conditions (12.5), or on the diﬀerence
between consecutive iterates. These criteria are typically combined with either
a maximum number of iterations or a time limit to ensure termination; see,
e.g., the discussion in [47]. In this section, we would like to point out an
issue that is sometimes overlooked in the literature when using the optimality
conditions to assess the convergence of NMF algorithms. A criterion based on
the optimality conditions is, for example, C(W, H) = CW (W)+CH(H) where
CW (W) = || min(W, 0)||F
|
{z
}
W ≥0
+ || min(∇W F, 0)||F
|
{z
}
∇W F ≥0
+ ||W ◦∇W F||F
|
{z
}
W ◦∇W F =0
,
(12.9)
and similarly CH(H) for H, so that C(W, H) = 0 ⇐⇒(W, H) is a station-
ary point of (12.3). There are several problems to using C(W, H) (and other

The Why and How of Nonnegative Matrix Factorization
271
FIGURE 12.3: Comparison of MU, ALS, ANLS, and HALS on the CBCL
facial images with r = 49, same data set as in Figure 12.1 (top), and
the Classic document data set with m = 7094, n = 41681, and r = 20;
see, e.g., [113] (bottom). The ﬁgure displays the average error using the
same ten initial matrices W and H for all algorithms, randomly gener-
ated with the rand function of MATLAB. All tests were performed using
MATLAB on a laptop Intel CORE i5-3210M CPU @2.5GHz 2.5GHz 6Go
RAM. Note that, for ALS, we display the error after scaling; see Equa-
tion (12.8). For MU and HALS, we used the implementation from https:
//sites.google.com/site/nicolasgillis/, for ANLS from http://www.
cc.gatech.edu/~hpark/nmfsoftware.php, and ALS was implemented fol-
lowing footnote 5.

272
Regularization, Optimization, Kernels, and Support Vector Machines
similar variants) as a stopping criterion and for comparing the convergence of
diﬀerent algorithms:
• It is sensitive to scaling. For α > 0 and α ̸= 1, we will have in general
that
CW (W) + CH(H) = C(W, H) ̸= C(αW, α−1H),
since the ﬁrst two terms in (12.9) are sensitive to scaling. For example,
if one solves the subproblem in W exactly and obtains CW (W) = 0 (this
will be the case for ANLS; see Section 12.4.1.4), then ∇HF can be made
arbitrarily small by multiplying W by a small constant and dividing H
by the same constant (while, if H ≥0, it will not inﬂuence the ﬁrst
term, which is equal to zero). This issue can be handled with proper
normalization, e.g., imposing ||W(:, k)||2 = ||H(k, :)||2 for all k; see [63].
• The value of C(W, H) after the update of W can be very diﬀerent from
the value after an update of H (in particular, if the scaling is bad or if
|m−n| ≫0). Therefore, one should be very careful when using this type
of criterion to compare ANLS-type methods with other algorithms such
as the MU or HALS as the evolution of C(W, H) can be misleading (in
fact, an algorithm that monotonically decreases the objective function,
such as the MU or HALS, is not guaranteed to monotonically decrease
C(W, H).) A potential ﬁx would be to scale the columns of W and the
rows of H so that CW (W) after the update of H and CH(H) after the
update of W have the same order of magnitude.
12.4.1.8
Initialization
A simple way to initialize W and H is to generate them randomly (e.g.,
generating all entries uniformly at random in the interval [0,1]). Several more
sophisticated initialization strategies have been developed in order to have
better initial estimates in the hope to (i) obtain a good factorization with
fewer iterations, and (ii) converge to a better stationary point. However, most
initialization strategies come with no theoretical guarantee (e.g., a bound on
the distance of the initial point to optimality), which can be explained in part
by the complexity of the problem (in fact, NMF is NP-hard in general; see the
introduction of this section). This could be an interesting direction for further
research. We list some initialization strategies here; they are based on:
• Clustering techniques. Use the centroids computed with some cluster-
ing method, e.g., with k-means or spherical k-means, to initialize the
columns of W, and initialize H as a proper scaling of the cluster indi-
cator matrix (that is, Hkj ̸= 0
⇐⇒
X(:, j) belongs to the kth clus-
ter) [107, 109]; see also [18] and the references therein for some recent
results.

The Why and How of Nonnegative Matrix Factorization
273
• The SVD. Let Pr
k=1 ukvT
k be the best rank-r approximation of X (which
can be computed, e.g., using the SVD; see Introduction). Each rank-
one factor ukvT
k might contain positive and negative entries (except for
the ﬁrst one, by the Perron-Frobenius theorem7). However, denoting
[x]+ = max(x, 0), we have
ukvT
k = [uk]+[vT
k ]+ + [−uk]+[−vT
k ]+ −[−uk]+[vT
k ]+ −[uk]+[−vT
k ]+,
and the ﬁrst two rank-one factors in this decomposition are nonnega-
tive. Boutsidis et al. [11] proposed to replace each rank-one factor in
Pr
k=1 ukvT
k with either [uk]+[vT
k ]+ or [−uk]+[−vT
k ]+, selecting the one
with larger norm and scaling it properly.
• Column subset selection. It is possible to initialize the columns of W
using data points, that is, initialize W = X(:, K) for some set K with
cardinality r; see [21, 112] and Section 12.4.2.
In practice, one may use several initializations, and keep the best solution
obtained; see, e.g., the discussion in [28].
12.4.2
Near-Separable NMF
A matrix X is r-separable if there exists an index set K of cardinality r
such that
X = X(:, K)H
for some H ≥0.
In other words, there exists a subset of r columns of X that generates a convex
cone containing all columns of X. Hence, given a separable matrix, the goal
of separable NMF is to identify the subset of columns K that allows us to
reconstruct all columns of X (in fact, given X(:, K), H can be computed by
solving a convex optimization program; see Section 12.4.1). The separability
assumption makes sense in several applications: for example,
• In text mining (see Section 12.3.2), separability of the word-by-document
matrix requires that for each topic, there exists a document only on that
topic. Note that we can also assume separability of the transpose of X
(that is, of the document-by-word matrix), i.e., for each topic there exists
one word used only by that topic (referred to as an ‘anchor’ word). In
fact, the latter is considered a more reasonable assumption in practice;
see [77, 3, 39] and also the thesis [46] for more details.
• In hyperspectral unmixing (see Section 12.3.3), separability of the
wavelength-by-pixel matrix requires that for each endmember there ex-
ists a pixel containing only that endmember. This is the so-called pure-
pixel assumption, and makes sense for relatively high spatial resolution
hyperspectral images; see [8, 90] and the references therein.
7Actually, the ﬁrst factor could contain negative entries if the input matrix is reducible
and its ﬁrst two singular values are equal to one another; see, e.g., [47, p. 16].

274
Regularization, Optimization, Kernels, and Support Vector Machines
Separability has also been used successfully in blind source separation [95, 22],
video summarization and image classiﬁcation [40], and foreground-background
separation in computer vision [76]. Note that for facial feature extraction de-
scribed in Section 12.3.1, separability does not make sense since we cannot
expect features to be present in the data set.
It is important to point out that separable NMF is closely related to several
problems, including
• Column subset selection, which is a long-standing problem in numerical
linear algebra (see [12] and the references therein).
• Pure-pixel search in hyperspectral unmixing, which has been addressed
long before NMF was introduced; see [90] for a historical note.
• The problem of identifying a few important data points in a data set
(see [40] and the references therein).
• Convex NMF [36], and the CUR decomposition [91].
Therefore, it is diﬃcult to pinpoint the roots of separable NMF, and a compre-
hensive overview of all methods related to separable NMF is out of the scope
of this paper. However, to the best of our knowledge, it is only very recently
that provably eﬃcient algorithms for separable NMF have been proposed. This
new direction of research was launched by a paper by Arora et al. [4] which
shows that NMF of separable matrices can be computed eﬃciently (that is, in
polynomial time), even in the presence of noise (the error can be bounded in
terms of the noise level; see below). We focus in this section on these provably
eﬃcient algorithms for separable NMF.
In the noiseless case, separable NMF reduces to identifying the extreme
rays of the cone spanned by the columns of X. If the columns of the in-
put matrix X are normalized so that their entries sum to one, that is,
X(:, j) ←||X(:, j)||−1
1 X(:, j) for all j (and discarding the zero columns of X),
then the problem reduces to identifying the vertices of the convex hull of the
columns of X. In fact, since the entries of each column of X sum to one and
X = X(:, K)H, the entries of each column of H must also sum to one: as X
and H are nonnegative, we have for all j
1 = ||X(:, j)||1 = ||X(:, K)H(:, j)||1
=
X
k
||X(:, K(k))||1H(k, j) =
X
k
H(k, j) = ||H(:, j)||1.
Therefore, the columns of X are convex combinations (that is, linear combi-
nations with nonnegative weights summing to one) of the columns of X(:, K).
In the presence of noise, the problem is referred to as near-separable NMF,
and can be formulated as follows:

The Why and How of Nonnegative Matrix Factorization
275
(Near-Separable NMF) Given a noisy r-separable matrix
˜X =
X + N with X = W[Ir, H′]Π where W and H′ are nonnegative
matrices, Π is a permutation matrix, and N is the noise, ﬁnd a
set K of r indices such that ˜X(:, K) ≈W.
In the following, we describe some algorithms for near-separable NMF; they
are classiﬁed in two categories: algorithms based on self-dictionary and sparse
regression (Section 12.4.2.1) and geometric algorithms (Section 12.4.2.2).
12.4.2.1
Self-Dictionary and Sparse Regression Framework
In the noiseless case, separable NMF can be formulated as follows
min
Y ∈Rn×n ||Y ||row,0
such that
X = XY and Y ≥0,
(12.10)
where ||Y ||row,0 is the number of non-zero rows of Y . In fact, if all the entries of
a row of Y are equal to zero, then the corresponding column of X is not needed
to reconstruct the other columns of X. Therefore, minimizing the number
of rows of Y diﬀerent from zero is equivalent to minimizing the number of
columns of X used to reconstruct all the other columns of X, which solves
the separable NMF problem. In particular, given an optimal solution Y ∗of
(12.10) and denoting K = {i|Y ∗(i, :) ̸= 0}, we have X = WY ∗(K, :) where
W = X(:, K).
In the presence of noise, the constraints X = XY are usually reformulated
as ||X −XY || ≤δ for some δ > 0 or put as a penalty λ||X −XY || in the
objective function for some penalty parameter λ > 0. In [40, 41], ||Y ||row,0 is
replaced using ℓ1-norm type relaxation:
||Y ||q,1 =
X
j
||Y (i, :)||q,
where q > 1 so that ||Y ||q,1 is convex and (12.10) becomes a convex optimiza-
tion problem. Note that this idea is closely related to compressive sensing
where ℓ1-norm relaxation is used to ﬁnd the sparsest solution to an underde-
termined linear system. This relaxation is exact given that the matrix involved
in the linear system satisﬁes some incoherence properties. In separable NMF,
the columns and rows of matrix X are usually highly correlated, hence it is
not clear how to extend the results from the compressive sensing literature to
this separable NMF model; see, e.g., the discussion in [90].
A potential problem in using convex relaxations of (12.10) is that it cannot
distinguish duplicates of the columns of W. In fact, if a column of W is present
twice in the data matrix X, the corresponding rows of Y can both be non-
zero, hence both columns of W can potentially be extracted (this is because
of the convexity and the symmetry of the objective function)–in [40], k-means
is used as a pre-processing in order to remove duplicates. Moreover, although
this model was successfully used to solve real-world problems, no robustness
results have been developed so far: so it is not clear how this model behaves

276
Regularization, Optimization, Kernels, and Support Vector Machines
in the presence of noise (only asymptotic results were proved, that is, when
the noise level goes to zero and when no duplicates are present [40]).
A rather diﬀerent approach to enforce row sparsity was suggested in [9],
and later improved in [49, 54]. Row sparsity of Y is enforced by: (i) minimizing
a weighted sum of the diagonal entries of Y hence enforcing diag(Y ) to be
sparse (in fact, this is nothing but a weighted ℓ1 norm since Y is nonnegative),
and (ii) imposing all entries in a row of Y to be smaller than the corresponding
diagonal entry of Y (we assume here that the columns of X are normalized).
The second condition implies that if diag(Y ) is sparse then Y is row sparse.
The corresponding near-separable NMF model is:
min
Y ∈Rn×n pT diag(Y ) such that ||X −XY ||1 ≤δ and 0 ≤Yij ≤Yii ≤1,
(12.11)
for some positive vector p ∈Rn with distinct entries (this breaks the symmetry
so that the model can distinguish duplicates). This model has been shown to
be robust: deﬁning the parameter8 α as
α(W) = min
1≤j≤r min
x∈Rr−1
+
||W(:, j) −W(:, Jj)x||1, where Jj = {1, 2, . . . , r}\{j},
and for a near-separable matrix
˜X = W[Ir, H′]Π + N (see above) with
ϵ = maxj ||N(:, j)||1 ≤O

α2(W )
r

, the model (12.11) can be used to iden-
tify the columns of W with ℓ1 error proportional to O

rϵ
α(W )

, that is, the
identiﬁed index set K satisﬁes maxj mink∈K || ˜X(:, k)−W(:, j)||1 ≤O

rϵ
α(W )

;
see [54, Th.7] for more details.
Finally, a drawback of the approaches based on self-dictionary and sparse
regression is that they are computationally expensive as they require us to
tackle optimization problems with n2 variables.
12.4.2.2
Geometric Algorithms
Another class of near-separable algorithms are based on geometric insights
and in particular on the fact that the columns of W are the vertices of the
convex hull of the normalized columns of X. Many geometric algorithms can
be found in the remote sensing literature (they are referred to as endmember
extraction algorithms or pure-pixel search algorithms); see [90] for a historical
note and [8] for a comprehensive survey. Because of the large body of litera-
ture, we do not aim at surveying all algorithms but rather focus on a single
algorithm that is particularly simple while being rather eﬀective in practice:
the successive projection algorithm (SPA). Moreover, the ideas behind SPA
8The larger the parameter α is, the less sensitive the data to noise. For example, it can
be easily checked that ϵ = maxj ||N(:, j)||1 < α
2 is a necessary condition to being able to
distinguish the columns of W [49].

The Why and How of Nonnegative Matrix Factorization
277
are at the heart of many geometric-based near-separable NMF algorithms (see
below).
SPA looks for the vertices of the convex hull of the columns of the input
data matrix X and works as follows: at each step, it selects the column of
X with maximum ℓ2 norm and then updates X by projecting each column
onto the orthogonal complement of the extracted column; see Algorithm 13.
SPA is extremely fast as it can be implemented in 2pnr + O(pr2) operations,
using the formula ||(I −uuT )v||2
2 = ||v||2
2 −(uT v)2, for any u, v ∈Rm with
||u||2 = 1 [55]. Moreover, if r is unknown, it can be estimated using the norm
of the residual R.
Algorithm 13 Successive Projection Algorithm [2]
Input: Near-separable matrix ˜X = W[Ir, H′]Π + N where W is full rank,
H′ ≥0, the entries of each column of H′ sum to at most one, Π is a
permutation, and N is the noise, and the number r of columns of W.
Output: Set of r indices K such that ˜X(:, K) ≈W (up to permutation).
1: Let R = ˜X, K = {}.
2: for k = 1 : r do
3:
p = argmaxj||R:j||2.
4:
R =

I −
R:pRT
:p
||R:p||2
2

R.
5:
K = K ∪{p}.
6: end for
Let us prove the correctness of SPA in the noiseless case using induction,
and assuming W is full rank (this is a necessary and suﬃcient condition) and
assuming the entries of each column of H′ sum to at most one (this can be
achieved through normalization; see above). At the ﬁrst step, SPA identiﬁes
a column of W because the ℓ2 norm can only be maximized at a vertex of the
convex hull of a set of points; see Figure 12.4 for an illustration. In fact, for
all 1 ≤j ≤n,
||X(:, j)||2 = ||WH(:, j)||2 ≤
r
X
k=1
H(k, j)||W(:, k)||2 ≤max
1≤k≤r ||W(:, k)||2.
The ﬁrst inequality follows from the triangle inequality, and the second since
H(k, j) ≥0 and P
k H(k, j) ≤1. Moreover, by strong convexity of the ℓ2
norm and the full rank assumption on W, the ﬁrst inequality is strict unless
H(:, k) is a column of the identity matrix, that is, unless X(:, j) = W(:, k) for
some k. For the induction step, assume without loss of generality that SPA
has extracted the ﬁrst ℓcolumns of W, and let Wℓ= W(:, 1:ℓ) and P ⊥
Wℓbe
the projection onto the orthogonal complement of the columns of Wℓso that

278
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 12.4: Illustration of SPA.
P ⊥
WℓWℓ= 0. We have, for all 1 ≤j ≤n,
||P ⊥
WℓX(:, j)||2 = ||P ⊥
WℓWH(:, j)||2 ≤
r
X
k=1
H(k, j)||P ⊥
WℓW(:, k)||2
≤
max
ℓ+1≤k≤r ||P ⊥
WℓW(:, k)||2,
where P ⊥
WℓW(:, k) ̸= 0 for ℓ+ 1 ≤k ≤r since W is full rank. Hence, using the
same reasoning as above, SPA will identify a column of W not extracted yet,
which concludes the proof.
Moreover, SPA is robust to noise: given a near-separable matrix ˜X =
W[Ir, H′]Π + N with W full rank, H′ nonnegative with ||H′(:, j)||1 ≤1 ∀j,
and ϵ = maxj ||N(:, j)||2 ≤O

σmin(W )
√rκ2(W )

, SPA identiﬁes the columns of W up
to ℓ2 error proportional to O
 ϵ κ2(W)

, where κ(W) = σmax(W )
σmin(W ) [55, Th.3].
These bounds can be improved using post-processing (see below), which re-
duces the error to O (ϵ κ(W)) [3], or preconditioning, which signiﬁcantly in-
creases the upper bound on the noise level, to ϵ ≤O

σmin(W )
r√r

, and reduces
the error to O (ϵ κ(W)) [56].
It is interesting to note that SPA has been developed and used for rather
diﬀerent purposes in various ﬁelds:
• Numerical linear algebra. SPA is closely related to the modiﬁed Gram-
Schmidt algorithm with column pivoting, used for example to solve lin-
ear least squares problems [15].
• Chemistry (and in particular spectroscopy). SPA is used for variable
selection in spectroscopic multicomponent analysis; in fact, the name
SPA comes from [2].
• Hyperspectral imaging. SPA is closely related to several endmember ex-
traction algorithms; in particular N-FINDR [108] and its variants, the
automatic target generation process (ATGP) [99], and the successive vol-
ume maximization algorithm (SVMAX) [21]; see the discussion in [90]

The Why and How of Nonnegative Matrix Factorization
279
for more details. The motivation behind all these approaches is to iden-
tify an index set K that maximizes the volume of the convex hull of the
columns of X(:, K). Note that most endmember extraction algorithms
use an LDR (such as the SVD) as a pre-processing step for noise ﬁltering,
and SPA can be combined with an LDR to improve performance.
• Text mining. Arora et al. [3] proposed FastAnchorWords whose diﬀer-
ences with SPA are that (i) the projection is made onto the aﬃne hull
of the columns extracted so far (instead of the linear span), and (ii) the
index set extracted is reﬁned using the following post-processing step:
let K be the extracted index set by SPA, for each k ∈K:
– Compute the projection R of X into the orthogonal complement of
X(:, K\{k}).
– Replace k with the index corresponding to the column of R with
maximum ℓ2 norm.
• Theoretical computer science. SPA was proved to be a good heuristic to
identify a subset of columns of a given matrix whose convex hull has
maximum volume [19, 20] (in the sense that no polynomial-time algo-
rithm can achieve better performance up to some logarithmic factors).
• Sparse regression with self-dictionary. SPA is closely related to orthog-
onal matching pursuit and can be interpreted as a greedy method to
solve the sparse regression problem with self-dictionary (12.10); see [44]
and the references therein.
Moreover, there exist many geometric algorithms that are variants of SPA,
e.g., vertex component analysis (VCA) using linear functions instead of the
ℓ2-norm [96], ℓp-norm based pure pixel algorithm (TRI-P) using p-norms [1],
FastSepNMF using strongly convex functions [55], the successive nonnega-
tive projection algorithm (SNPA) [50], and the fast conical hull algorithm
(XRAY) [77] using nonnegativity constraints for the projection step.
Further research on near-separable NMF includes the design of faster
and/or provably more robust algorithms.
12.5
Connections with Problems in Mathematics and
Computer Science
In this section, we brieﬂy mention several connections between NMF and
problems outside data mining and machine learning. The minimum r such
that an exact NMF of a nonnegative matrix X exists is the nonnegative rank

280
Regularization, Optimization, Kernels, and Support Vector Machines
of X, denoted rank+(X). More precisely, given X ∈Rp×n
+
, rank+(X) is the
minimum r such that there exist W ∈Rp×r
+
and H ∈Rr×n
+
with X = WH.
The nonnegative rank has tight connections with several problems in mathe-
matics and computer science:
• Graph Theory. Let G(X) = (V1 ∪V2, E) be the bipartite graph induced
by X (that is, (i, j) ∈E ⇐⇒Xij ̸= 0). The minimum biclique cover
bc(G(X)) of G(X) is the minimum number of complete bipartite sub-
graphs needed to cover G(X). It can be checked easily that for any
(W, H) ≥0 such that X = WH = Pr
k=1 W:kHk:, we have
G(X) = ∪r
k=1G(W:kHk:),
where G(W:kHk:) are complete bipartite subgraphs, hence bc(G(W:kHk:)) =
1 ∀k. Therefore,
bc(G(X)) ≤rank+(X).
This lower bound on the nonnegative rank is referred to as the rectangle
covering bound [43].
• Extended Formulations. Given a polytope P, an extended formulation
(or lift, or extension) is a higher dimensional polyhedron Q and a linear
projection π such that π(Q) = P. When the polytope P has exponen-
tially many facets, ﬁnding extended formulations of polynomial size is of
great importance since it allows us to solve linear programs (LP) over P
in polynomial time. It turns out that the minimum number of facets of
an extended formulation Q of a polytope P is equal to the nonnegative
rank of its slack matrix [111], deﬁned as X(i, j) = aT
i vj −bi where vj
is the jth vertex of P and {x ∈Rn | aT
i x −bi ≥0} its ith facet with
ai ∈Rn and bi ∈R, that is, X is a facet-by-vertex matrix and X(i, j)
is the slack of the jth vertex with respect to ith facet; see the sur-
veys [30, 69] and the references therein. These ideas can be generalized
to approximate extended formulations, directly related to approximate
factorizations (hence NMF) [13, 58].
• Probability. Let X(k) ∈{1, . . . , p} and Y (k) ∈{1, . . . , n} be two inde-
pendent variables for each 1 ≤k ≤r, and P (k) be the joint distribution
with
P (k)
ij
= P

X(k) = i, Y (k) = j

= P

X(k) = i

P

Y (k) = j

.
Each distribution P (k) corresponds to a nonnegative rank-one matrix.
Let us deﬁne the mixture P of these k independent models as follows:
– Choose the distribution P (k) with probability αk (Pr
k=1 αk = 1).
– Draw X and Y from the distribution P (k).

The Why and How of Nonnegative Matrix Factorization
281
We have that P = Pr
k=1 αkP (k) is the sum of r rank-one nonnegative
matrices. In practice, only P is observed and computing its nonnegative
rank and a corresponding factorization amounts to explaining the dis-
tribution P with as few independent variables as possible; see [17] and
the references therein.
• Communication Complexity. In its simplest variant, communication
complexity addresses the following problem: Alice and Bob have to com-
pute the following function
f : {0, 1}m × {0, 1}n →{0, 1} : (x, y) →f(x, y).
Alice only knows x and Bob y, and the aim is to minimize the number
of bits exchanged between Alice and Bob to compute f exactly. Non-
deterministic communication complexity (NCC) is a variant where Bob
and Alice ﬁrst receive a message before starting their communication;
see [81, Ch.3] and the references therein for more details. The communi-
cation matrix X ∈{0, 1}2n×2m is equal to the function f for all possible
combinations of inputs. Yannakakis [111] showed that the NCC for com-
puting f is upper bounded by the logarithm of the nonnegative rank of
the communication matrix (this result is closely related to the rectan-
gle covering bound described above: in fact, ⌈log(bc(G(X))⌉equals the
NCC of f).
• Computational Geometry. Computing the nonnegative rank is closely
related to the problem of ﬁnding a polytope with minimum number of
vertices nested between two given polytopes [53]. This is a well-known
problem is computational geometry, referred to as the nested polytopes
problem; see [31] and the references therein.
12.6
Conclusion
NMF is an easily interpretable linear dimensionality reduction technique
for nonnegative data. It is a rather versatile technique with many applica-
tions, and brings together a broad range of researchers. In the context of “Big
Data” science, which is becoming an increasingly important topic, we believe
NMF has a bright future; see Figure 12.5 for an illustration of the number of
publications related to NMF since the publication of the Lee and Seung paper
[79].

282
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 12.5: Number of search results for papers containing either “non-
negative matrix factorization” or “non-negative matrix factorization” on
Google Scholar and Scopus (as of December 12, 2013).
Acknowledgments
The author would like to thank Rafal Zdunek, Wing-Kin Ma, Marc Pirlot,
and the editors of this book for insightful comments that helped improve the
chapter.
Bibliography
[1] A. Ambikapathi, T.-H. Chan, C.-Y. Chi, and K. Keizer. Hyperspectral
data geometry based estimation of number of endmembers using p-norm
based pure pixel identiﬁcation. IEEE Trans. on Geoscience and Remote
Sensing, 51(5):2753–2769, 2013.
[2] U.M.C. Araújo, B.T.C. Saldanha, R.K.H. Galvão, T. Yoneyama, H.C.
Chame, and V. Visani. The successive projections algorithm for variable
selection in spectroscopic multicomponent analysis. Chemometrics and
Intelligent Laboratory Systems, 57(2):65–73, 2001.
[3] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu,
and M. Zhu. A practical algorithm for topic modeling with provable
guarantees. In Int. Conf. on Machine Learning (ICML ’13), volume 28,
pages 280–288. 2013.

The Why and How of Nonnegative Matrix Factorization
283
[4] S. Arora, R. Ge, R. Kannan, and A. Moitra. Computing a nonnegative
matrix factorization – provably. In Proc. of the 44th Symp. on Theory
of Computing (STOC ’12), pages 145–162, 2012.
[5] R. Badeau, N. Bertin, and E. Vincent. Stability analysis of multiplicative
update algorithms and application to nonnegative matrix factorization.
IEEE Trans. on Neural Networks, 21(12):1869–1881, 2010.
[6] M.W. Berry, M. Browne, A. Langville, V.P. Pauca, and R.J. Plemmons.
Algorithms and Applications for Approximate Nonnegative Matrix Fac-
torization. Computational Statistics & Data Analysis, 52:155–173, 2007.
[7] J.M. Bioucas-Dias and J.M.P. Nascimento. Estimation of signal sub-
space on hyperspectral data. In Remote Sensing, page 59820L. Interna-
tional Society for Optics and Photonics, 2005.
[8] J.M. Bioucas-Dias, A. Plaza, N. Dobigeon, M. Parente, Q. Du, P. Gader,
and J. Chanussot. Hyperspectral unmixing overview: Geometrical, sta-
tistical, and sparse regression-based approaches. IEEE J. of Selected
Topics in Applied Earth Observations and Remote Sensing, 5(2):354–
379, 2012.
[9] V. Bittorf, B. Recht, E. Ré, and J.A. Tropp.
Factoring nonnegative
matrices with linear programs. In Advances in Neural Information Pro-
cessing Systems (NIPS ’12), pages 1223–1231, 2012.
[10] D.M. Blei. Probabilistic topic models. Communications of the ACM,
55(4):77–84, 2012.
[11] C. Boutsidis and E. Gallopoulos. SVD based initialization: A head start
for nonnegative matrix factorization.
Pattern Recognition, 41:1350–
1362, 2008.
[12] C. Boutsidis, M.W. Mahoney, and P. Drineas. An improved approxima-
tion algorithm for the column subset selection problem. In Proc. of the
20th Annual ACM-SIAM Symp. on Discrete Algorithms (SODA ’09),
pages 968–977, 2009.
[13] G. Braun, S. Fiorini, S. Pokutta, and D. Steurer. Approximation limits
of linear programs (beyond hierarchies). In Proc. of the 53rd Annual
IEEE Symp. on Foundations of Computer Science (FOCS’ 12), pages
480–489, 2012.
[14] R. Bro. Multi-Way Analysis in the Food Industry: Models, Algorithms,
and Applications. PhD thesis, University of Copenhagen, 1998. http:
//curis.ku.dk/ws/files/13035961/Rasmus_Bro.pdf.
[15] P. Businger and G.H. Golub. Linear least squares solutions by house-
holder transformations. Numerische Mathematik, 7:269–276, 1965.

284
Regularization, Optimization, Kernels, and Support Vector Machines
[16] D. Cai, X. He, J. Han, and T.S. Huang. Graph regularized nonnegative
matrix factorization for data representation. IEEE Trans. on Pattern
Analysis and Machine Intelligence, 33(8):1548–1560, 2011.
[17] E. Carlini and F. Rapallo. Probability matrices, non-negative rank, and
parameterization of mixture models. Linear Algebra and its Applica-
tions, 433:424–432, 2010.
[18] G. Casalino, N. Del Buono, and C. Mencar.
Subtractive clustering
for seeding non-negative matrix factorizations. Information Sciences,
(257):369–387, 2013.
[19] A. Çivril and M. Magdon-Ismail. On selecting a maximum volume sub-
matrix of a matrix and related problems. Theoretical Computer Science,
410(47-49):4801–4811, 2009.
[20] A. Çivril and M. Magdon-Ismail. Exponential inapproximability of se-
lecting a maximum volume sub-matrix. Algorithmica, 65(1):159–176,
2013.
[21] T.-H. Chan, W.-K. Ma, A. Ambikapathi, and C.-Y. Chi. A simplex
volume maximization framework for hyperspectral endmember extrac-
tion. IEEE Trans. on Geoscience and Remote Sensing, 49(11):4177–
4193, 2011.
[22] T.-H. Chan, W.-K. Ma, C.-Y. Chi, and Y. Wang. A convex analysis
framework for blind separation of non-negative sources. IEEE Trans.
on Signal Processing, 56(10):5120–5134, 2008.
[23] E.C. Chi and T.G. Kolda. On tensors, sparsity, and nonnegative fac-
torizations. SIAM J. on Matrix Analysis and Applications, 33(4):1272–
1299, 2012.
[24] S. Choi. Algorithms for orthogonal nonnegative matrix factorization.
In Proc. of the Int. Joint Conf. on Neural Networks, pages 1828–1832,
2008.
[25] A. Cichocki and A.H. Phan. Fast local algorithms for large scale Nonneg-
ative Matrix and Tensor Factorizations. IEICE Trans. on Fundamentals
of Electronics, Vol. E92-A No.3:708–721, 2009.
[26] A. Cichocki, R. Zdunek, and S.-I. Amari. Non-negative Matrix Factor-
ization with Quasi-Newton Optimization. In Lecture Notes in Artiﬁcial
Intelligence, Springer, volume 4029, pages 870–879, 2006.
[27] A. Cichocki, R. Zdunek, and S.-I. Amari. Hierarchical ALS Algorithms
for Nonnegative Matrix and 3D Tensor Factorization. In Lecture Notes
in Computer Science, Vol. 4666, Springer, pages 169–176, 2007.

The Why and How of Nonnegative Matrix Factorization
285
[28] A. Cichocki, R. Zdunek, A.H. Phan, and S.-I. Amari. Nonnegative Ma-
trix and Tensor Factorizations: Applications to Exploratory Multi-way
Data Analysis and Blind Source Separation. Wiley-Blackwell, 2009.
[29] P. Comon. Independent component analysis: A new concept? Signal
Processing, 36:287–314, 1994.
[30] M. Conforti, G. Cornuéjols, and G. Zambelli. Extended formulations in
combinatorial optimization. 4OR: A Quarterly Journal of Operations
Research, 10(1):1–48, 2010.
[31] G. Das and D.A. Joseph. The Complexity of Minimum Convex Nested
Polyhedra. In Proc. of the 2nd Canadian Conf. on Computational Ge-
ometry, pages 296–301, 1990.
[32] A. d’Aspremont, L. El Ghaoui, M.I. Jordan, and G.R.G. Lanckriet. A
Direct Formulation for Sparse PCA Using Semideﬁnite Programming.
SIAM Review, 49(3):434–448, 2007.
[33] M.E. Daube-Witherspoon and G. Muehllehner. An iterative image space
reconstruction algorithm suitable for volume ECT.
IEEE Trans. on
Medical Imaging, 5:61–66, 1986.
[34] K. Devarajan. Nonnegative Matrix Factorization: An Analytical and In-
terpretive Tool in Computational Biology. PLoS Computational Biology,
4(7):e1000029, 2008.
[35] C. Ding, X. He, and H.D. Simon. On the Equivalence of Nonnegative
Matrix Factorization and Spectral Clustering. In SIAM Int. Conf. Data
Mining (SDM’05), pages 606–610, 2005.
[36] C. Ding, T. Li, and M.I. Jordan. Convex and semi-nonnegative matrix
factorizations. IEEE Trans. on Pattern Analysis and Machine Intelli-
gence, 32(1):45–55, 2010.
[37] C. Ding, T. Li, and W. Peng. On the equivalence between non-negative
matrix factorization and probabilistic latent semantic indexing. Com-
putational Statistics & Data Analysis, 52(8):3913–3927, 2008.
[38] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix
t-factorizations for clustering. In Proc. of the 12th ACM SIGKDD Int.
Conf. on Knowledge Discovery and Data Mining, pages 126–135, 2006.
[39] W. Ding, M.H. Rohban, P. Ishwar, and V. Saligrama. Topic discov-
ery through data dependent and random projections. In Int. Conf. on
Machine Learning (ICML ’13), volume 28, pages 471–479. 2013.
[40] E. Elhamifar, G. Sapiro, and R. Vidal.
See all by looking at a few:
Sparse modeling for ﬁnding representative objects. In IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR ’12), 2012.

286
Regularization, Optimization, Kernels, and Support Vector Machines
[41] E. Esser, M. Moller, S. Osher, G. Sapiro, and J. Xin. A convex model
for nonnegative matrix factorization and dimensionality reduction on
physical space.
IEEE Trans. on Image Processing, 21(7):3239–3252,
2012.
[42] C. Févotte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix factor-
ization with the Itakura-Saito divergence: With application to music
analysis. Neural Computation, 21(3):793–830, 2009.
[43] S. Fiorini, V. Kaibel, K. Pashkovich, and D.O. Theis. Combinatorial
bounds on nonnegative rank and extended formulations. Discrete Math-
ematics, 313(1):67–83, 2013.
[44] X. Fu, W.-K. Ma, T.-H. Chan, J.M. Bioucas-Dias, and M.-D. Iordache.
Greedy algorithms for pure pixels identiﬁcation in hyperspectral un-
mixing: A multiple-measurement vector viewpoint.
In Proc. of 21st
European Signal Processing Conf. (EUSIPCO ’13), 2013.
[45] E. Gaussier and C. Goutte. Relation between PLSA and NMF and impli-
cations. In Proc. of the 28th Annual Int. ACM SIGIR Conf. on Research
and Development in Information Retrieval, pages 601–602, 2005.
[46] R. Ge.
Provable Algorithms for Machine Learning Problems.
PhD
thesis, Princeton University, 2013.
http://dataspace.princeton.
edu/jspui/bitstream/88435/dsp019k41zd62n/1/Ge_princeton_
0181D_10819.pdf.
[47] N. Gillis.
Nonnegative Matrix Factorization: Complexity, Algorithms
and Applications. PhD thesis, Université catholique de Louvain, 2011.
https://sites.google.com/site/nicolasgillis/.
[48] N. Gillis.
Sparse and unique nonnegative matrix factorization
through data preprocessing.
Journal of Machine Learning Research,
13(Nov):3349–3386, 2012.
[49] N. Gillis. Robustness analysis of Hottopixx, a linear programming model
for factoring nonnegative matrices. SIAM J. on Matrix Analysis and
Applications, 34(3):1189–1212, 2013.
[50] N. Gillis. Successive nonnegative projection algorithm for robust non-
negative blind source separation. arXiv:1310.7529, 2013.
[51] N. Gillis and F. Glineur. Using underapproximations for sparse nonneg-
ative matrix factorization. Pattern Recognition, 43(4):1676–1687, 2010.
[52] N. Gillis and F. Glineur. Accelerated multiplicative updates and hier-
archical ALS algorithms for nonnegative matrix factorization. Neural
Computation, 24(4):1085–1105, 2012.

The Why and How of Nonnegative Matrix Factorization
287
[53] N. Gillis and F. Glineur. On the geometric interpretation of the non-
negative rank. Linear Algebra and its Applications, 437(11):2685–2712,
2012.
[54] N. Gillis and R. Luce. Robust near-separable nonnegative matrix fac-
torization using linear optimization. Journal of Machine Learning Re-
search, 2014. to appear.
[55] N. Gillis and S.A. Vavasis.
Fast and robust recursive algorithms for
separable nonnegative matrix factorization. IEEE Trans. Pattern Anal.
Mach. Intell., 2013. doi:10.1109/TPAMI.2013.226.
[56] N. Gillis and S.A. Vavasis. Semideﬁnite programming based precondi-
tioning for more robust near-separable nonnegative matrix factorization.
arXiv:1310.2273, 2013.
[57] G.H. Golub and C.F. Van Loan. Matrix Computation, 3rd Edition. The
Johns Hopkins University Press Baltimore, 1996.
[58] J. Gouveia, P.A. Parrilo, and R.R. Thomas. Approximate cone factor-
izations and lifts of polytopes. arXiv:1308.2162, 2013.
[59] L. Grippo and M. Sciandrone. On the convergence of the block nonlinear
Gauss-Seidel method under convex constraints.
Operations Research
Letters, 26:127–136, 2000.
[60] N. Guan, D. Tao, Z. Luo, and B. Yuan. NeNMF: an optimal gradient
method for nonnegative matrix factorization. IEEE Trans. on Signal
Processing, 60(6):2882–2898, 2012.
[61] D. Guillamet and J. Vitrià. Non-negative matrix factorization for face
recognition. In Lecture Notes in Artiﬁcial Intelligence, pages 336–344.
Springer, 2002.
[62] J. Han, L. Han, M. Neumann, and U. Prasad. On the rate of convergence
of the image space reconstruction algorithm. Operators and Matrices,
3(1):41–58, 2009.
[63] N.-D. Ho. Nonnegative Matrix Factorization - Algorithms and Applica-
tions. PhD thesis, Université Catholique de Louvain, 2008.
[64] P.O. Hoyer.
Nonnegative matrix factorization with sparseness con-
straints. Journal of Machine Learning Research, 5:1457–1469, 2004.
[65] C.-J. Hsieh and I.S. Dhillon. Fast coordinate descent methods with vari-
able selection for non-negative matrix factorization. In Proc. of the 17th
ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining,
pages 1064–1072, 2011.

288
Regularization, Optimization, Kernels, and Support Vector Machines
[66] K. Huang, N.D. Sidiropoulos, and A. Swami. Non-negative matrix fac-
torization revisited: Uniqueness and algorithm for symmetric decompo-
sition. IEEE Trans. on Signal Processing, 62(1):211–224, 2014.
[67] M.-D. Iordache, J.M. Bioucas-Dias, and A. Plaza. Total variation spa-
tial regularization for sparse hyperspectral unmixing. IEEE Trans. on
Geoscience and Remote Sensing, 50(11):4484–4502, 2012.
[68] S. Jia and Y. Qian. Constrained nonnegative matrix factorization for
hyperspectral unmixing. IEEE Trans. on Geoscience and Remote Sens-
ing, 47(1):161–173, 2009.
[69] V. Kaibel. Extended Formulations in Combinatorial Optimization. Op-
tima, 85:2–7, 2011.
[70] B. Kanagal and V. Sindhwani. Rank selection in low-rank matrix ap-
proximations. In Advances in Neural Information Processing Systems
(NIPS ’10), 2010.
[71] Q. Ke and T. Kanade. Robust L1 norm factorization in the presence of
outliers and missing data by alternative convex programming. In IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR ’05), pages
739–746, 2005.
[72] H. Kim and H. Park.
Sparse non-negative matrix factorizations via
alternating non-negativity-constrained least squares for microarray data
analysis. Bioinformatics, 23(12):1495–1502, 2007.
[73] H. Kim and H. Park.
Non-negative Matrix Factorization Based on
Alternating Non-negativity Constrained Least Squares and Active Set
Method. SIAM J. on Matrix Analysis and Applications, 30(2):713–730,
2008.
[74] J. Kim, Y. He, and H. Park. Algorithms for nonnegative matrix and
tensor factorizations: A uniﬁed view based on block coordinate descent
framework. Journal of Global Optimization, 2013. doi:10.1007/s10898-
013-0035-4.
[75] J. Kim and H. Park. Fast nonnegative matrix factorization: An active-
set-like method and comparisons. SIAM J. on Scientiﬁc Computing,
33(6):3261–3281, 2011.
[76] A. Kumar and V. Sindhwani. Near-separable non-negative matrix fac-
torization with ℓ1- and Bregman loss functions. arXiv:1312.7167, 2013.
[77] A. Kumar, V. Sindhwani, and P. Kambadur. Fast conical hull algorithms
for near-separable non-negative matrix factorization. In Int. Conf. on
Machine Learning (ICML ’13), volume 28, pages 231–239. 2013.

The Why and How of Nonnegative Matrix Factorization
289
[78] C.L. Lawson and R.J. Hanson. Solving Least Squares Problems. Pren-
tice-Hall, 1974.
[79] D.D. Lee and H.S. Seung. Learning the Parts of Objects by Nonnegative
Matrix Factorization. Nature, 401:788–791, 1999.
[80] D.D. Lee and H.S. Seung. Algorithms for Non-negative Matrix Factor-
ization. In Advances in Neural Information Processing (NIPS ’01), 13,
2001.
[81] T. Lee and A. Shraibman. Lower bounds in communication complexity.
Now Publishers Inc., 2009.
[82] A. Lefèvre. Dictionary learning methods for single-channel source sep-
arations. PhD thesis, Ecole Normale Supérieure de Cachan, 2012.
[83] L. Li, G. Lebanon, and H. Park. Fast Bregman divergence NMF using
Taylor expansion and coordinate descent. In Proc. of the 18th ACM
SIGKDD Int. Conf. on Knowledge Discovery and Data Mining, pages
307–315, 2012.
[84] L. Li and Y.-J. Zhang. FastNMF: highly eﬃcient monotonic ﬁxed-point
nonnegative matrix factorization algorithm with good applicability. J.
Electron. Imaging, Vol. 18(033004), 2009.
[85] T. Li, Y. Zhang, and V. Sindhwani.
A non-negative matrix tri-
factorization approach to sentiment classiﬁcation with lexical prior
knowledge. In Association of Computational Lingustics, pages 244–252,
2009.
[86] Y. Li, D.M. Sima, S. Van Cauter, A.R. Croitor Sava, U. Himmelreich,
Y. Pi, and S. Van Huﬀel. Hierarchical non-negative matrix factoriza-
tion (hNMF): a tissue pattern diﬀerentiation method for glioblastoma
multiforme diagnosis using MRSI. NMR in Biomedicine, 26(3):307–319,
2013.
[87] C.-J. Lin. On the convergence of multiplicative update algorithms for
nonnegative matrix factorization.
IEEE Trans. on Neural Networks,
18(6):1589–1596, 2007.
[88] C.-J. Lin. Projected Gradient Methods for Nonnegative Matrix Factor-
ization. Neural Computation, 19:2756–2779, 2007.
[89] J. Liu, J. Liu, P. Wonka, and J. Ye. Sparse non-negative tensor fac-
torization using columnwise coordinate descent. Pattern Recognition,
45(1):649–656, 2012.
[90] W.-K. Ma, J.M. Bioucas-Dias, T.-H. Chan, N. Gillis, P. Gader, A. Plaza,
A. Ambikapathi, and C.-Y. Chi. A Signal Processing Perspective on
Hyperspectral Unmixing. IEEE Signal Processing Magazine, 31(1):67–
81, 2014.

290
Regularization, Optimization, Kernels, and Support Vector Machines
[91] M.W. Mahoney and P. Drineas. CUR matrix decompositions for im-
proved data analysis. Proc. of the National Academy of Sciences, 106(3):
697–702, 2009.
[92] P. Melville and V. Sindhwani. Recommender systems. Encyclopedia of
machine learning, 1:829–838, 2010.
[93] L. Miao and H. Qi. Endmember extraction from highly mixed data using
minimum volume constrained nonnegative matrix factorization. IEEE
Trans. on Geoscience and Remote Sensing, 45(3):765–777, 2007.
[94] A. Moitra.
An almost optimal algorithm for computing nonnegative
rank. In Proc. of the 24th Annual ACM-SIAM Symp. on Discrete Algo-
rithms (SODA ’13), pages 1454–1464, 2013.
[95] W. Naanaa and J.-M. Nuzillard. Blind source separation of positive and
partially correlated data. Signal Processing, 85(9):1711–1722, 2005.
[96] J.M.P. Nascimento and J.M. Bioucas-Dias. Vertex component analysis:
a fast algorithm to unmix hyperspectral data. IEEE Trans. on Geo-
science and Remote Sensing, 43(4):898–910, 2005.
[97] P. Paatero and U. Tapper. Positive matrix factorization: a non-negative
factor model with optimal utilization of error estimates of data values.
Environmetrics, 5:111–126, 1994.
[98] J. Rapin, J. Bobin, A. Larue, and J.-L. Starck. Sparse and non-negative
BSS for noisy data. IEEE Trans. on Signal Processing, 61(22):5620–
5632, 2013.
[99] H. Ren and C.-I. Chang. Automatic spectral target recognition in hy-
perspectral imagery. IEEE Trans. on Aerospace and Electronic Systems,
39(4):1232–1249, 2003.
[100] R. Sandler and M. Lindenbaum. Nonnegative matrix factorization with
earth mover’s distance metric. In IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR ’09), pages 1873–1880, 2009.
[101] F. Shahnaz, M.W. Berry, V.P. Pauca, and R.J. Plemmons. Document
clustering using nonnegative matrix factorization. Information Process-
ing and Management, 42:373–386, 2006.
[102] P. Smaragdis, C. Févotte, G.J. Mysore, N. Mohammadiha, and M. Hoﬀ-
man. A Uniﬁed View of Static and Dynamic Source Separation Using
Non-Negative Factorizations. IEEE Signal Processing Magazine, 2014.
[103] N. Takahashi and R. Hibi. Global convergence of modiﬁed multiplicative
updates for nonnegative matrix factorization. Computational Optimiza-
tion and Applications, 2013. doi:10.1007/s10589-013-9593-0.

The Why and How of Nonnegative Matrix Factorization
291
[104] V.Y.F. Tan and C. Févotte. Automatic relevance determination in non-
negative matrix factorization. In Signal Processing with Adaptive Sparse
Structured Representations (SPARS ’09), 2009.
[105] S.A. Vavasis. On the complexity of nonnegative matrix factorization.
SIAM J. on Optimization, 20(3):1364–1377, 2009.
[106] F. Wang, T. Li, X. Wang, S. Zhu, and C. Ding. Community discov-
ery using nonnegative matrix factorization. Data Min. Knowl. Disc.,
22(3):493–521, 2011.
[107] S. Wild, J. Curry, and A. Dougherty. Improving non-negative matrix
factorizations through structured initialization.
Pattern Recognition,
37(11):2217–2232, 2004.
[108] M. Winter. N-FINDR: an algorithm for fast autonomous spectral end-
member determination in hyperspectral data. In Proc. SPIE Conf. on
Imaging Spectrometry V, pages 266–275, 1999.
[109] Y. Xue, C.S. Tong, Y. Chen, and W.-S. Chen. Clustering-based initial-
ization for non-negative matrix factorization. Applied Mathematics and
Computation, 205(2):525–536, 2008.
[110] Z. Yang and E. Oja. Linear and nonlinear projective nonnegative matrix
factorization. IEEE Trans. on Neural Networks, 21(5):734–749, 2010.
[111] M. Yannakakis. Expressing Combinatorial Optimization Problems by
Linear Programs. Journal of Computer and System Sciences, 43(3):441–
466, 1991.
[112] R. Zdunek. Initialization of nonnegative matrix factorization with ver-
tices of convex polytope. In Artiﬁcial Intelligence and Soft Comput-
ing, volume 7267 of Lecture Notes in Computer Science, pages 448–455.
Springer Berlin Heidelberg, 2012.
[113] S. Zhong and J. Ghosh. Generative model-based document clustering: a
comparative study. Knowledge and Information Systems, 8(3):374–384,
2005.

This page intentionally left blank
This page intentionally left blank

Chapter 13
Rank Constrained Optimization
Problems in Computer Vision
Ivan Markovsky
Department ELEC, Vrije Universiteit Brussel
13.1
Introduction ......................................................
293
13.2
Multidimensional Scaling ........................................
294
13.3
Conic Section Fitting ............................................
295
13.4
Fundamental Matrix Estimation .................................
297
13.5
Least Squares Contour Alignment ...............................
298
13.5.1
Alignment by Reﬂection, Scaling, and Translation .....
300
13.5.2
Alignment by Rigid Transformation ....................
302
13.5.3
Invariance Properties and a Distance Measure ..........
304
13.5.4
Contour Alignment as an Orthogonal Procrustes
Problem ..................................................
305
13.6
Conclusions .......................................................
308
Acknowledgments ................................................
309
Appendix: Position Estimation from Exact and Complete
Distances .........................................................
309
Bibliography ......................................................
310
13.1
Introduction
The claim that
“Behind every data modeling problem there is a (hidden) low rank
approximation problem”
[13]
is demonstrated in this book chapter via four problems in computer vision:
• multidimensional scaling,
• conic section ﬁtting,
• fundamental matrix estimation, and
• least squares contour alignment.
293

294
Regularization, Optimization, Kernels, and Support Vector Machines
A matrix constructed from exact data is rank deﬁcient. The corresponding
data ﬁtting problem in the case of noisy data is a rank constraint optimization
problem. In general, rank constrained optimization is a hard nonconvex prob-
lem, for which application speciﬁc heuristics are proposed. In this chapter, I
do not describe solution methods for rank constrained optimization but refer
the reader to the literature.
Our main contribution is the analytic solution of the contour alignment
problem presented in Section 13.5.1. This problem is also nonconvex in the
original problem variables; however, a nonlinear change of variables renders
the problem convex in the transformed variables. The link to low-rank ap-
proximation (the theme of the chapter) is presented in Section 13.5.4, where
the problem is shown to be equivalent to the orthogonal Procrustes problem,
which is a constrained low-rank approximation problem [12].
13.2
Multidimensional Scaling
Consider N points { x1, . . . , xN } in an n-dimensional real space and let
dij be the squared Euclidean distances between xi and xj. The matrix D =
dij

∈RN×N of the pair-wise squared distances is symmetric, element-wise
nonnegative, and has zero diagonal elements. Moreover, since
dij := (xi −xj)⊤(xi −xj) = x⊤
i xi −2x⊤
i xj + x⊤
j xj,
D has the following structure
D =


x⊤
1 x1
...
x⊤
NxN


1
· · ·
1
−2


x⊤
1
...
x⊤
N


x1
· · ·
xN

+


1
...
1



x⊤
1 x1
· · ·
x⊤
NxN

,
or
D = diag(X⊤X)1⊤
N −2X⊤X + 1N diag⊤(X⊤X) =: S(X),
(13.1)
where
X :=
x1
· · ·
xN

and
1N =
1
· · ·
1⊤∈RN.
In particular, from (13.1) it can be seen that D is rank deﬁcient:
rank(D) ≤n + 2.
(13.2)
The image of the function S : X 7→D is referred to as the set of element-
wise-squared-distance matrices. The inverse of S is a set valued function
S−1(D) := { X | (13.1) holds }.

Rank Constrained Optimization Problems in Computer Vision
295
If D is a distance matrix of a set of points X, S−1(D) consists of all rigid
transformations (translation, rotation, and reﬂection) of X. In other words
the nonuniqueness in ﬁnding X, given D, is up to a rigid transformation.
Theorem 13.1. Let D be a distance matrix and let ¯X be a particular solution
of the Equation (13.1). Then
S−1(D) = { R ¯X + c1⊤
N | c ∈Rn and R ∈Rn×n, such that RR⊤= I }.
The considered problem is deﬁned informally as follows:
Given noisy and incomplete information about the pair-wise squared-
distances dij among the points { x1, . . . , xN } and the dimension n of
the ambient space, ﬁnd estimates of the points { x1, . . . , xN }, up to a
rigid transformation.
With exact data, the problem can be posed and solved as a rank revealing
factorization problem (see the Appendix). With noisy measurements, however,
the matrix D is generically full rank. In this case, the relative (up to rigid
transformation) point locations can be estimated by approximating D by a
rank-(n + 2) matrix ˆD. In order to be a valid distance matrix, however, ˆD
must have the structure ˆD = S( ˆX), for some ˆX =
ˆx1
· · ·
ˆxN

, i.e., the
estimation problem is a bilinearly structured low-rank approximation problem:
minimize over ˆD ∈RN×N and ˆX ∈Rn×N D −ˆD

F subject to ˆD = S( ˆX),
where ∥·∥F is the Frobenius norm. Note that the rank constraint (13.2) is
automatically satisﬁed by the structure constraint (13.1).
For comprehensive treatment of applications and solution methods for mul-
tidimensional scaling, the reader is refered to the books [4, 2].
13.3
Conic Section Fitting
A conic section is a static quadratic model. In this section, I show that the
conic section ﬁtting problem can be formulated as a low-rank approximation
of an extended data matrix. The mapping from the original data to the ex-
tended data is called in the machine learning literature the feature map. In the
application at hand, the feature map is naturally deﬁned by the conic model,
i.e., it is a quadratic function.
Let
{ d1, . . . , dN } ⊂R2,
where
dj =
xj
yj

,

296
Regularization, Optimization, Kernels, and Support Vector Machines
be the given data. A conic section is a set deﬁned by a second order equation
B(A, b, c) := { d ∈R2 | d⊤Ad + b⊤d + c = 0 }.
(13.3)
Here A is a 2 × 2 symmetric matrix, b is a 2 × 1 vector, and c is a scalar. A,
b, and c are the parameters of the conic section. In order to avoid a trivial
solution B = R2, it is assumed that at least one of the parameters A, b, or c is
nonzero. The representation (13.3) is an implicit representation of the conic
section, because it imposes a relation (implicit function) on the elements x
and y of d. In special cases, it is possible to use explicit representations deﬁned
by a function from x to y or from y to x; however, this approach is restrictive
as it does not cover all conic sections (e.g., an ellipse cannot be represented
by a map from one variable to the other).
Deﬁning the parameter vector
θ :=
a11
2a12
b1
a22
b2
c
,
and the extended data vector
dext :=
x2
xy
x
y2
y
1⊤,
(13.4)
we have that
d ∈B(θ) = B(A, b, c)
⇐⇒
θdext = 0.
(The map d 7→dext, deﬁned by (13.4), is the feature map for the conic section
model.) Consequently, all data points d1, . . . , dN are ﬁtted by the model if
θ
dext,1
· · ·
dext,N

|
{z
}
Dext
= 0
⇐⇒
rank(Dext) ≤5.
(13.5)
Indeed, for θ ̸= 0, the left-hand-side of the equivalence states that Dext has a
nontrivial left kernel. Since Dext has 6 rows (see (13.4)), its rank is at most 5.
The mapping D 7→Dext is denoted by S.
In the presence of noise, generically, rank(Dext) > 5. Then, the aim is to
approximate the data points d1, . . . , dN by nearby points ˆd1, . . . , ˆdN
that lie exactly on a conic section.
Minimizing the sum of squares of the orthogonal distances from the data
points to their approximations leads to the structured low-rank approximation
problem
minimize
over ˆD ∈R2×N
D −ˆD

F
subject to
rank
 S( ˆD)

≤5,
where
D :=
d1
· · ·
dN

,
ˆD :=
 ˆd1
· · ·
ˆdN

are the data matrix and the approximating matrix, respectively.

Rank Constrained Optimization Problems in Computer Vision
297
In the computer vision literature, see, e.g., the tutorial paper [23], conic
section ﬁtting by orthogonal projections is called geometric ﬁtting. As shown
above, the corresponding computational problem is a quadratically structured
low-rank approximation problem. The problem is intuitively appealing; how-
ever, it is nonconvex and, moreover, leads to an inconsistent estimator. This
has motivated work on easier to compute methods [1, 6, 8, 5, 10, 14, 19] that
also reduce or even eliminate the bias.
13.4
Fundamental Matrix Estimation
In two-dimensional motion analysis [11] a scene is captured by two cameras
at ﬁxed locations (stereo vision) and N matching pairs of points
{ u1, . . . , uN } ⊂R2
and
{ v1, . . . , vN } ⊂R2
(13.6)
are located in the resulting images. The corresponding points u and v in the
two images satisfy what is called an epipolar constraint

v⊤
1

F

u
1

= 0,
for some F ∈R3×3, with rank(F) = 2.
(13.7)
The 3 × 3 matrix F ̸= 0, called the fundamental matrix, characterizes the
relative position and orientation of the cameras and does not depend on the
selected pairs of points. Estimation of F from data is a necessary calibration
step in many computer vision methods.
The epipolar constraint (13.7) is linear in F. Indeed, deﬁning
dext :=
uxvx
uxvy
ux
uyvx
uyvy
uy
vx
vy
1⊤∈R9,
(13.8)
where u = [ ux
uy ] and v = [ vx
vy ], (13.7) can be written as
vec⊤(F)dext = 0.
Note that, as in the application for conic section ﬁtting, the original data
(u, v) is mapped to an extended data vector dext via a nonlinear function (a
feature map). In this case, however, the function is bilinear.
Taking into account the epipolar constraints for all data points, we obtain
the matrix equation
vec⊤(F)
dext,1
· · ·
dext,N

|
{z
}
Dext
= 0.
(13.9)

298
Regularization, Optimization, Kernels, and Support Vector Machines
The rank constraint imposed on F implies that F is a nonzero matrix. There-
fore, by (13.9) Dext has a nontrivial left kernel and since Dext is 9 × N
rank(Dext) ≤8.
It can be concluded that for N ≥8 data points, Dext is not full row rank.
Moreover, if the left kernel is one dimensional, the fundamental matrix F can
be reconstructed up to a scaling factor from the data.
In the case of noisy data,
the aim is to perturb as little as possible the data (13.6), so that the
perturbed data satisﬁes exactly the epipolar constraints for some ˆF
with rank( ˆF) = 2.
The resulting estimation problem is a bilinearly structured low-rank approxi-
mation with an additional rank constraint. This problem deﬁnes a maximum-
likelihood estimator for the true parameter value. As in the conic section ﬁt-
ting problem, the maximum-likelihood estimator is a nonconvex optimization
problem and is inconsistent in the measurement errors or errors-in-variables
setup. These facts motivated the development of methods that are convex and
unbiased; see [21, 3, 9, 14] and the references therein. Closely related to the
estimation of the fundamental matrix problem in two-view computer vision is
the shape from motion problem [20].
13.5
Least Squares Contour Alignment
Let Rθ be the operator in R2 that rotates its argument by θ rad (positive
angle corresponding to anticlockwise rotation) and let R′
θ be the operator that
reﬂects its argument about a line, passing through the origin, at θ/2 rad with
respect to the ﬁrst basis vector (see Figure 13.1).
It can be shown that Rθ and R′
θ have matrix representations
Rθ(p) =
cos θ
−sin θ
sin θ
cos θ

p = Rθp
and
R′
θ(p) =

cos θ
sin θ
sin θ
−cos θ

p = R′
θp.
In [15], the authors considered transformation by rotation, scaling, and
translation, i.e.,
Aa,θ,s(p) = sRθ(p) + a,
(13.10)

Rank Constrained Optimization Problems in Computer Vision
299
Rπ/4(p)
R′
0(p)
π
4
p
reﬂection line
FIGURE 13.1: Rotation Rθ1 and reﬂection R′
θ2 of a point p.
where s > 0 is the scaling factor and a ∈R2 is the translation parame-
ter. The problem of determining the parameters θ, s, and a of a transforma-
tion Aa,θ,s(p) that best, in a least squares sense, matches one set of points
p(1), . . . , p(N) to another set of points q(1), . . . , q(N) can be used to align two
explicitly represented contours, speciﬁed by corresponding points. Although
this alignment problem is a nonlinear least squares problem in the parameters
θ, s, and a, it is shown in [15] that the change of variables
b =
b1
b2

= s
cos θ
sin θ

,
θ
s

=
sin−1 (b2/ ∥b∥)
∥b∥

(13.11)
results in an equivalent linear least squares problem in the parameters a and b.
This fact allowed eﬃcient solution of image registration problems (see, e.g.,
[17, 16]) with a large number of corresponding points. The invariance to rigid
transformation appears also in learning with linear functionals on reproducing
kernel Hilbert space; see [22].
It is well known, however, that dilation and rigid transformation involves
reﬂection, in addition to rotation, scaling, and translation. Therefore, a prob-
lem occurs regarding how to optimally align in a least squares sense the set
of points
{ p(1), . . . , p(N) }
and
{ q(1), . . . , q(N) }
under reﬂection, rotation, scaling, and translation, i.e., transformation of the
type
Aa,θ1,θ2,s(p) = sRθ1
 R′
θ2(p)

+ a.
(13.12)
In order to solve the problem of alignment by dilation and rigid transfor-
mation, ﬁrst consider alignment by reﬂection, scaling, and translation, i.e.,
transformation of the type
A′
a,θ,s(p) = sR′
θ(p) + a.
(13.13)

300
Regularization, Optimization, Kernels, and Support Vector Machines
The solution of this latter problem, given in Section 13.5.1, also uses the change
of variables (13.11) to convert the original nonlinear least squares problem to
a linear one. The derivation given in Section 13.5.1, however, is diﬀerent from
the derivation in [15] and reveals a link between the alignment problems by
rotation and reﬂection.
The solution to the general least squares alignment problem by rigid trans-
formation is given in Section 13.5.2. Since a transformation (13.12) is either
rotation, scaling, and translation, or reﬂection, scaling, and translation, the
alignment problem (13.12) reduces to solving problems (13.10) and (13.13)
separately, and choosing the solution that corresponds to the better ﬁt.
In Section 13.5.4, I show that least squares alignment by rotation and
reﬂection is equivalent to the orthogonal Procrustes problem [7, p. 601]. An
extension of the orthogonal Procrustes problem to alignment by (13.12), pre-
sented in [18], gives an alternative solution method for contour alignment by
dilation and rigid transformation. An advantage of the approach based on
the orthogonal Procrustes problem is that the solution is applicable to data in
higher dimensional space; however, the method requires singular value decom-
position of a matrix computed from the data, which may be computationally
more expensive than solving an ordinary linear least squares problem.
13.5.1
Alignment by Reﬂection, Scaling, and Translation
Let C1 and C2 be the matrices of the points p(1), . . . , p(N) stacked next to
each other, and q(1), . . . , q(N), respectively, i.e.,
C1 :=

p(1)
· · ·
p(N)
and
C2 :=

q(1)
· · ·
q(N)
,
and let ∥·∥F be the Frobenius norm, deﬁned as
∥C1∥F :=
v
u
u
t
N
X
i=1
p(i)2
2.
The problem considered in this section is least squares alignment by
reﬂection:
minimize
C1 −A′
a,θ,s(C2)

F
over
a ∈R2, s > 0, θ ∈[−π, π).
(13.14)
Similarly to the alignment by rotation problem
minimize
∥C1 −Aa,θ,s(C2)∥F
over
a ∈R2, s > 0, θ ∈[−π, π),
(13.15)
(13.14) is a nonlinear least squares problem in the parameters θ, s, and a. The
change of variables (13.11), however, also transforms problem (13.14) into a
linear least squares problem.

Rank Constrained Optimization Problems in Computer Vision
301
Theorem 13.2 (Alignment by reﬂection, scaling, and translation). Prob-
lem (13.14) is equivalent to the linear least squares problem
minimize
over a, b ∈R2
vec(C1) −

(C⊤
2 ⊗I2)E
1N ⊗I2
 b
a

2
(13.16)
where vec(·) is the column-wise matrix vectorization operator, ⊗is the Kro-
necker product,
1N :=


1
...
1

∈RN, E :=


1
0
0
1
0
1
−1
0

, and I2 :=

1
0
0
1

.
(13.17)
The one-to-one relation between the parameters θ, s and b1, b2 is given
by (13.11).
Proof. Note that
A′
a,θ,s(C2) = sR′
θC2 −a1⊤
N.
Using the identity,
vec(AXB) = (B⊤⊗A) vec(X),
we rewrite the cost function of (13.14) as
C1−I2
s cos θ
s sin θ
s sin θ
−s cos θ

C2−a

F
=
 vec(C1)−(C⊤
2 ⊗I2)


s cos θ
s sin θ
s sin θ
−s cos θ

−a

2
=
 vec(C1) −(C⊤
2 ⊗I2)


1
0
0
1
0
1
−1
0


s cos θ
s sin θ

−a

2
.
Problem (13.14) and the relation (13.11) follows by setting
b1 := s cos θ
and
b2 := s sin θ.
Note 13.1 (Alignment by rotation, scaling, and translation). The above solu-
tion of Problem (13.14) can be modiﬁed easily for the corresponding alignment
problem with rotation (13.15), giving an alternative shorter proof to Theorem 1
in [15]. Indeed, the only necessary modiﬁcation is to replace the matrix E
in (13.17) by
E =


1
0
0
1
0
−1
1
0

.

302
Regularization, Optimization, Kernels, and Support Vector Machines
Example 13.1. As an illustration of the presented alignment procedure, con-
sider the contours shown in Figure 13.2. The optimal alignment by rotation,
scaling, and translation is shown in Figure 13.3, right, and the optimal align-
ment by reﬂection, scaling, and translation is shown in Figure 13.3, left.
−2
−1
0
1
2
3
−1
0
1
2
3
C1
p(1)
p(2)
p(3)
p(4)
p(5)
p(6)
p(7)
p(8)
C2
q(1)
q(2)
q(3)
q(4)
q(5)
q(6)
q(7)
q(8)
FIGURE 13.2: Example of contour alignment problem (13.14): given con-
tours C1 and C2 with corresponding points p(i) ↔q(i), ﬁnd a transformation
A′
a,θ,s that minimizes the distance between C1 and the transformed contour
A′
a,θ,s(C2).
13.5.2
Alignment by Rigid Transformation
The problem considered in this section is:
minimize
∥C1 −Aa,θ1,θ2,s(C2)∥F
over
a ∈R2, s > 0, θ1, θ2 ∈[−π, π)
(13.18)

Rank Constrained Optimization Problems in Computer Vision
303
−2
−1
0
1
2
3
−1
0
1
2
3
−2
−1
0
1
2
3
−1
0
1
2
3
FIGURE 13.3: Left: optimal alignment of C2 to C1 and C1 to C2 by A′
a,θ,s
(reﬂection), Right: optimal alignment of C2 to C1 and C1 to C2 by Aa,θ,s (ro-
tation).
The following fact allows us to reduce Problem (13.18) to the already studied
Problems (13.14) and (13.15).
Proposition 13.1. A transformation by rotation and reﬂection, Rθ1
 R′
θ2(p)

,
is equivalent to a transformation by an orthogonal matrix Qp. Moreover,
Qp = Rθ(p),
if det(Q) = 1,
and
Qp = R′
θ(p),
if det(Q) = −1,
where, in either case,
θ = cos−1(q11).
(13.19)
Proof. The matrix Rθ1R′
θ2 is orthogonal, because Rθ1 and R′
θ2 are orthogonal
matrices and the product of orthogonal matrices is an orthogonal matrix.
Next, I show that an orthogonal matrix Q is either a rotation matrix Rθ, for
some θ ∈[−π, π), or a reﬂection matrix R′
θ, for some θ ∈[−π, π).
Since Q is orthogonal
q11
q12
q21
q22
 q11
q21
q12
q22

=
1
0
0
1

.
Without loss of generality we can choose
q11 = cos θ
and
q12 = sin θ.
Then, there are two possibilities for q21 and q22
q21 = cos(θ + π/2)
and
q22 = sin(θ + π/2)

304
Regularization, Optimization, Kernels, and Support Vector Machines
or
q21 = cos(θ −π/2)
and
q22 = sin(θ −π/2).
In the ﬁrst case, Q is a rotation matrix and in the second case Q is a reﬂection
matrix. Therefore,
Q = Rθ
or
Q = R′
θ,
where
θ = cos−1(q11).
It is easy to check that
det(Rθ) = 1
and
det(R′
θ) = −1,
for any θ.
The result of Proposition 13.1 shows that Problem (13.18) can be solved
by the following procedure.
1. Solve the alignment problem by reﬂection (13.14).
2. Solve the alignment problem by rotation (13.15).
3. Select the solution of the problem that gives smaller cost function value.
Since Problems (13.14) and (13.15) are already solved in Section 13.5.1, we
have a complete solution to (13.18).
13.5.3
Invariance Properties and a Distance Measure
It turns out that the minimum value of (13.18)
dist′(C1, C2) :=
min
a∈R2, s>0, θ1,θ2∈[−π,π) ∥C1 −Aa,θ1,θ2,s(C2)∥F
is not a proper distance measure. (A counter example is given in Exam-
ple 13.3.) Proposition 4 (invariance) and Theorem 7 (distance measure) were
stated and proved in [15] for alignment by (13.10). However, they hold for the
more general problem of alignment by dilation and rigid transformation.
Proposition 13.2. If the contours C1 and C2, deﬁned by the sets of corre-
sponding points { p(i) } and { p(i) }, are centered (i.e., C11N = C21N = 0),
dist′(C1, C2) is invariant to a rigid transformation, i.e.,
dist′(C1, C2) = dist′  Rθ(C1), Rθ(C2)

= dist′  R′
θ(C1), R′
θ(C2)

, for any θ ∈[−π, π).
(13.20)
If, in addition, C1 and C2 are normalized by ∥C1∥F = ∥C2∥F = 1,
dist′(C1, C2) = dist′(C2, C1).
(13.21)

Rank Constrained Optimization Problems in Computer Vision
305
Example 13.2. Consider again the contours from Example 13.1. The points
p(i) and q(i) are preprocessed, so that the resulting contours, say C1,c and C2,c,
are centered. As a numerical veriﬁcation of (13.20), we have
dist′(C1,c, C2,c) = dist′  R0.3(C1,c), R0.3(C2,c)

= dist′  R′
0.3(C1,c), R′
0.3(C2,c)

= 0.40640.
Let, in addition, the points p(i) and q(i) be preprocessed, so that the resulting
contours, say C1,cn and C2,cn, are centered and normalized. As a numerical
veriﬁcation of (13.21), we have
dist′(C1,cn, C2,cn) = dist′(C2,cn, C1,cn) = 0.11271.
As in the case of the transformation (13.10), treated in [15, Section III],
the following deﬁnition gives a distance measure.
Deﬁnition 13.1 (2-norm distance between contours modulo rigid transfor-
mation).
dist(C1, C2) :=
1
C1 −1
N C11N1⊤
N

F
×
min
a∈R2, s>0, θ1,θ2∈[−π,π) ∥C1 −Aa,θ1,θ2,s(C2)∥F .
(13.22)
Theorem 13.3. The distance measure dist(C1, C2) is symmetric and invari-
ant to dilation and a rigid transformation, i.e.,
dist(C1, C2) = dist(C2, C1) = dist
 Aa,θ1,θ2,s(C1), Aa,θ1,θ2,s(C2)

,
for all a ∈R2, θ1, θ2 ∈[−π, π), and s > 0.
(13.23)
Example 13.3. For the contours in Example 13.1, we have
dist′(C1, C2) = 0.40640
and
dist′(C2, C1) = 0.20748,
while
dist(C1, C2) = dist(C2, C1) = 0.11271.
13.5.4
Contour Alignment as an Orthogonal Procrustes
Problem
As a consequence of Proposition 13.1, we have that
Problem (13.18) is equivalent to
minimize
∥C1 −sQC2 −a∥F
subject to
Q⊤Q = I2
over
a ∈R2, s > 0, Q ∈R2×2.
(13.24)

306
Regularization, Optimization, Kernels, and Support Vector Machines
In turn, Problem (13.24) is related to the orthogonal Procrustes problem in
numerical linear algebra.
Problem 13.1 (Orthogonal Procrustes problem). Given q × N real matrices
C1 and C2,
minimize
over Q
∥C1 −QC2∥F
subject to
Q⊤Q = Iq.
The classical solution of the orthogonal Procrustes problem is given by
Q = UV ⊤,
where UΣV ⊤is the singular value decomposition (SVD) of C⊤
1 C2, see [7,
p. 601].
The orthogonal Procrustes problem does not involve scaling and transla-
tion. The extension of the problem to alignment by dilation and rigid transfor-
mation is done in [18]. The resulting procedure is summarized in Algorithm 14.
It presents an alternative solution approach for solving Problem (13.18). Com-
pared to the solution proposed in Section 13.5.4, Algorithm 14 has the advan-
tage of being applicable to data of any dimension (C1, C2 ∈Rq×N, for any
natural number q), i.e., the solution based on the orthogonal Procrustes prob-
lem is applicable to contours in spaces of dimension higher than 2.
The solution based on the orthogonal Procrustes problem, however, uses
the singular value decomposition, while the solution proposed in Section 13.5.4
involves two ordinary least least squares problems. Therefore, an advantage
of the proposed solution is its conceptual simplicity. In particular, exploiting
the Kronecker structure of the coeﬃcients matrix in (13.16) one can derive
an eﬃcient algorithm for alignment of contours speciﬁed by a large number
of corresponding points. Furthermore, in the case of sequential but not neces-
sarily corresponding points (see [15, Section IV]), N alignment problems are
solved, which makes the computational eﬃciency an important factor.

Rank Constrained Optimization Problems in Computer Vision
307
Algorithm 14 Algorithm for least-squares contour alignment, based on the
orthogonal Procrustes problem.
Input: Contours with corresponding points, speciﬁed by matrices C1 and C2.
1: Centering of the contours:
Ci,c := Ci −a(i)1⊤
N,
where a(i) := 1
N Ci1N.
2: Alignment of the centered data by orthogonal transformation:
Q := UV ⊤,
where UΣV ⊤is the SVD of C⊤
2,cC1,c.
3: Computation of the scaling parameter:
s := trace(QC2C⊤
1 )
∥C2,c∥2
F
4: Rigid transformation of C2 to ﬁt C1:
ˆC1 := sQ(C2 −a(2)1⊤
N) + a(1)1⊤
N.
Output: Rigid transformation parameters:
• a(1) −sQa(2) — translation,
• Q — orthogonal transformation, and
• s — scaling.

308
Regularization, Optimization, Kernels, and Support Vector Machines
13.6
Conclusions
This chapter illustrated the claim that every data modeling problem is
related to a (structured) low-rank approximation problem for a matrix ob-
tained from the data via a nonlinear transformation (feature map) by four
speciﬁc examples in computer vision: multidimensional scaling, conic section
ﬁtting, fundamental matrix estimation, and contour alignment. In multidi-
mensional scaling, the data is the squared distances between a set of points
and the structure of the low-rank approximation problem is given by (13.1).
This structure automatically makes the constructed matrix rank deﬁcient, so
that the low-rank approximation problem has no additional rank constraint.
In the conic section ﬁtting problem, the feature map is a quadratic function
and, in the fundamental matrix estimation problem, the feature map is a bi-
linear function. Finally the contour alignment problem was reduced to the
orthogonal Procrustes problem, which is a low-rank approximation problem
with an additional orthogonality constraint. A summary of the application is
given in Table 13.1.
TABLE 13.1: Summary of applications, matrix structures, and rank con-
straints.
application
data
data matrix
structure
rank =
multidim.
distances dij
dij

(13.1)
dim(x) + 2
scaling
pair-wise
conic section
points di
(13.4), (13.5)
quadratic
5
ﬁtting
fundamental
corresponding
(13.8), (13.9)
bilinear
8
matrix
points uj, vj
estimation
contour
corresponding
 C1
C2

unstructured
2
alignment
points C1, C2

Rank Constrained Optimization Problems in Computer Vision
309
Acknowledgments
Funding from the European Research Council under the European Union’s
Seventh Framework Programme (FP7/2007-2013)/ERC Grant agreement
number 258581, “Structured low-rank approximation: Theory, algorithms, and
applications”, is gratefully acknowledged.
Appendix: Position Estimation from Exact and Complete
Distances
Consider the change of variables
S := X⊤X.
(A.1)
The inverse transformation S 7→X is a set valued function with nonunique-
ness described by the orthogonal transformation X 7→RX (i.e., rotation or
reﬂection of the set of points X). A particular solution of Equation (A.1),
for given symmetric matrix X of rank at most n, can be computed by the
eigenvalue decomposition of X. Let
S = V ΛV ⊤=
V1
V2
 
Λ1
0
 V1
V2
⊤,
where the diagonal elements of Λ1 are all positive, be the eigenvalue decom-
position of X. Then
p
Λ1V ⊤
1 = RX,
for some orthogonal matrix R.
Equation (13.1) is linear in S. We have,
vec(D) = (1N ⊗E + E ⊗1N −2I) vec(S) =: L vec(S).
(A.2)
Furthermore, taking into account the symmetry of D and S, (A.2) becomes
vecs(D) = Ls vecs(S).
(A.3)
The matrix Ls is of size Ns ×Ns, where Ns := N(N +1)/2, and is a submatrix
of L ∈RN×N.
The system of linear equations (A.3) has Ns equations and Ns unknowns.
The matrix Ls, however, is rank deﬁcient
rank(Ls) = Ns −N,

310
Regularization, Optimization, Kernels, and Support Vector Machines
so that a solution is nonunique. (Assuming that D is a distance matrix, an
exact solution of (A.3) exists.) We are aiming at a solution S of (A.3) of
rank at most n, ﬁnding such a solution in the aﬃne set of solutions is a hard
problem.
A simple transformation avoids the nonuniqueness issue. The translated
set of points
¯X := X −x11⊤
N =

0
¯x2
· · ·
¯xN

has the same distance matrix as X, i.e., S( ¯X) = D. The change of vari-
ables (A.1) then results in a matrix
¯S := ¯X⊤¯X =

01×1
0N−1×1
01×N−1
∗

,
so that
vecs( ¯S) =
0N×1
∗

.
From (A.3), we have
vecs(D) = Ls
0N×1
¯s

=: Ls(:, N + 1 :)¯s.
(A.4)
The submatrix Ls(:, N + 1 :) of Ls is full column rank, which implies that ¯s
is the unique solution of (A.4).
Bibliography
[1] F. L. Bookstein.
Fitting conic sections to scattered data.
Computer
Graphics Image Proc., 9:59–71, 1979.
[2] I. Borg and P. Groenen. Modern Multidimensional Scaling: Theory and
Applications. Springer, 2005.
[3] S. Chaudhuri and S. Chatterjee.
Recursive estimation of motion pa-
rameters.
Computer Vision and Image Understanding, 64(3):434–442,
November 1996.
[4] T. Cox and M. Cox. Multidimensional Scaling, Second Edition. CRC
Press, 2000.
[5] A. Fitzgibbon, M. Pilu, and R. Fisher.
Direct least-squares ﬁtting of
ellipses. IEEE Trans. Pattern Anal. Machine Intelligence, 21(5):476–480,
1999.

Rank Constrained Optimization Problems in Computer Vision
311
[6] W. Gander, G. Golub, and R. Strebel. Fitting of circles and ellipses:
Least squares solution. BIT, 34:558–578, 1994.
[7] G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins Uni-
versity Press, third edition, 1996.
[8] K. Kanatani. Statistical bias of conic ﬁtting and renormalization. IEEE
Trans. Pattern Anal. Machine Intelligence, 16(3):320–326, 1994.
[9] A. Kukush, I. Markovsky, and S. Van Huﬀel. Consistent fundamental
matrix estimation in a quadratic measurement error model arising in
motion analysis. Comput. Statist. Data Anal., 41(1):3–18, 2002.
[10] A. Kukush, I. Markovsky, and S. Van Huﬀel. Consistent estimation in
an implicit quadratic measurement error model. Comput. Statist. Data
Anal., 47(1):123–147, 2004.
[11] Y. Ma, S. Soatto, J. Kosecká, and S. Sastry. An Invitation to 3-D Vision,
volume 26 of Interdisciplinary Applied Mathematics. Springer, 2004.
[12] I. Markovsky. Structured low-rank approximation and its applications.
Automatica, 44(4):891–909, 2008.
[13] I. Markovsky.
Low Rank Approximation: Algorithms, Implementation,
Applications. Communications and Control Engineering. Springer, 2012.
[14] I. Markovsky, A. Kukush, and S. Van Huﬀel. Consistent least squares
ﬁtting of ellipsoids. Numerische Mathematik, 98(1):177–194, 2004.
[15] I. Markovsky and S. Mahmoodi. Least-squares contour alignment. IEEE
Signal Proc. Letters, 16(1):41–44, 2009.
[16] J. Marques. A fuzzy algorithm for curve and surface alignment. Pattern
Recognition Letters, 19:797–803, 1998.
[17] J. Marques and A. Abrantes. Shape alignment—optimal initial point and
pose estimation. Pattern Recognition Letters, 18:49–53, 1997.
[18] P. Schönemann and R. Carroll.
Fitting one matrix to another under
choice of a central dilation and a rigid motion. Psychometrika, 35(2):245–
255, 1970.
[19] S. Shklyar, A. Kukush, I. Markovsky, and S. Van Huﬀel. On the conic
section ﬁtting problem.
Journal of Multivariate Analysis, 98:588–624,
2007.
[20] C. Tomasi and T. Kanade. Shape and motion from image streams: A
factorization method. Proc. Natl. Adadem. Sci. USA, 90:9795–9802, 1993.

312
Regularization, Optimization, Kernels, and Support Vector Machines
[21] P. Torr and D. Murray.
The development and comparison of robust
methods for estimating the fundamental matrix. Int. J. Computer Vision,
24(3):271–300, 1997.
[22] X. Zhang, W. S. Lee, and Y. W. Teh.
Learning with invariance via
linear functionals on reproducing kernel Hilbert space. In Proc. of the
Neural Information Processing Systems (NIPS) conference, pages 2031–
2039, 2013.
[23] Z. Zhang. Parameter estimation techniques: A tutorial with application
to conic ﬁtting. Image Vision Comp. J., 15(1):59–76, 1997.

Chapter 14
Low-Rank Tensor Denoising and
Recovery via Convex Optimization
Ryota Tomioka
Toyota Technological Institute, Chicago
Taiji Suzuki
Tokyo Institute of Technology
Kohei Hayashi
National Institute of Informatics
Hisashi Kashima
University of Tokyo
14.1
Introduction ......................................................
314
14.2
Ranks and Norms ................................................
315
14.2.1
Rank and Multilinear Rank .............................
315
14.2.2
Convex Relaxations ......................................
316
14.3
Statistical Guarantees ............................................
317
14.3.1
Denoising Bounds
.......................................
318
14.3.2
Tensor Recovery Guarantee .............................
320
14.4
Optimization .....................................................
320
14.4.1
ADMM for the Overlapped Schatten 1-Norm
Regularization ...........................................
321
14.4.2
ADMM for Latent Schatten 1-Norm Regularization ....
322
14.5
Experiments ......................................................
323
14.5.1
Tensor Denoising ........................................
323
14.5.2
Tensor Completion ......................................
325
14.6
Extensions and Related Work ...................................
326
14.6.1
Balanced Unfolding ......................................
326
14.6.2
Tensor Nuclear Norm ....................................
328
14.6.3
Interpretation of the Result .............................
328
14.6.4
Related Work ............................................
329
14.7
Future Directions ................................................
331
Acknowledgments ................................................
331
Bibliography ......................................................
332
313

314
Regularization, Optimization, Kernels, and Support Vector Machines
14.1
Introduction
Low-rank decomposition of tensors (multi-way arrays) naturally arises in
many application areas, including signal processing, neuroimaging, bioinfor-
matics, recommender systems and other relational data analysis [29, 35, 20].
This chapter reviews convex-optimization-based algorithms for tensor de-
composition. There are several reasons to look into convex optimization for
tensor decomposition. First, it allows us to prove worst-case-performance guar-
antees. Although we might be able to give a performance guarantee for an
estimator based on a non-convex optimization (see, e.g., [43]), the practical
relevance of the bound would be limited if we cannot obtain the optimum
eﬃciently. Second, the convex methods allow us to side-step the tensor rank
selection problem; in practice misspeciﬁcation of tensor rank can signiﬁcantly
deteriorate the performance, whereas choosing a continuous regularization pa-
rameter can be considered an easier task. Third, it allows us to use various
eﬃcient techniques developed in the mathematical programming communi-
ties, such as proximity operation, alternating direction method of multipliers
(ADMM), and duality gap monitoring, which enable us to apply these algo-
rithms to a variety of settings reliably. The norms we propose can be used for
both denoising of a fully observed noisy tensor and reconstruction of a low-
rank tensor from incomplete measurements. Of course there are limitations
to what we can achieve with convex optimization, which we will discuss in
Section 14.6. Nevertheless we hope that the methods we discuss here serve
to connect tensor decomposition with statistics and (convex) optimization,
which have been largely disconnected until recently, and contribute to the
better understanding of the hardness and challenges of this area.
This chapter is structured as follows: in the next section, we introduce
diﬀerent notions of tensor ranks and present two norms that induce low-rank
tensors, namely the overlapped Schatten 1-norm and latent Schatten 1-norm.
In Section 14.3, we present denoising and recovery bounds for the two norms.
The proofs of the theorems can be found in original papers [54, 53]. In Sec-
tion 14.4, we propose optimization algorithms for the two norms based on
primal and dual ADMM, respectively. Although ADMM has become a stan-
dard practice these days, our implementation allows us to deal with the noisy
case and the exact case in the same framework (no need for continuation).
We also discuss the choice of the penalty parameter η. Section 14.5 consists
of some simple demonstrations of the implication of the theorems. Full quan-
titative evaluation of the bounds can be found in original papers [54, 53]. We
discuss various extensions and related work in Section 14.6. We conclude this
chapter with possible future directions.

Low-Rank Tensor Denoising and Recovery via Convex Optimization
315
14.2
Ranks and Norms
Let W ∈Rn1×n2×···×nK be a K-way tensor. We denote the total number
of entries in W by N = QK
k=1 nk.
14.2.1
Rank and Multilinear Rank
A tensor W is rank one if it can be expressed as an outer product of K
vectors as
W = u(1) ◦u(2) ◦· · · ◦u(K),
which can be written element-wise as follows:
Wi1i2···iK = u(1)
i1 u(2)
i2 · · · u(K)
iK ,
(1 ≤ik ≤nk, k = 1, . . . , K).
It is easy to verify that a tensor is rank one.
The rank of a tensor W is the smallest number r such that W can be
expressed as the sum of r rank-one tensors as follows:
W =
r
X
j=1
u(1)
j
◦u(2)
j
◦· · · ◦u(K)
j
.
(14.1)
The above decomposition is known as the canonical polyadic (CP) decompo-
sition [22]. It is known that ﬁnding the rank r or computing the best rank r
approximation (even for r = 1) is an NP hard problem [23, 21].
The multilinear rank of W is the K tuple (r1, . . . , rK) such that rk is the
dimension of the space spanned by the mode-k ﬁbers [13, 29]; here mode-
k ﬁbers are the nk dimensional vectors obtained by ﬁxing all but the kth
index. If W admits decomposition (14.1), rk is at most r, in which case the
multilinear rank of W is at most (r, . . . , r).
In contrast to the rank, the multilinear rank (r1, . . . , rK) can be com-
puted eﬃciently. To this end, it is convenient to deﬁne the mode-k unfolding
operation. The mode-k unfolding W(k) is a nk × N/nk matrix obtained by
concatenating the mode-k ﬁbers along columns. Then rk is the matrix rank
of the mode-k unfolding W(k).
The notion of multilinear rank is connected to another decomposition
known as the Tucker decomposition [56] or the higher-order SVD [13, 14]
W = C ×1 U1 ×2 U2 · · · ×K UK,
(14.2)
where ×k denotes the mode-k product [29].
Computation of Decompositions (14.1) and (14.2) from large noisy tensor
with possibly missing entries is a challenging task. Alternating least squares

316
Regularization, Optimization, Kernels, and Support Vector Machines
(ALS) [11] and higher-order orthogonal iteration (HOOI) [14] are well known
and many extensions of them are proposed [29]. However, they typically come
with no theoretical guarantee either about global optimality of the obtained
solution or the statistical performance of the estimator. Kannan and Vempala
(see Chapter 8) [27] proposed a sampling based algorithm with a performance
bound, which requires knowledge of the Frobenius norms of the slices.
14.2.2
Convex Relaxations
Recently, motivated by the success of the Schatten 1-norm (also known as
the trace norm and nuclear norm) for the recovery of low-rank matrices [16,
50, 42, 10, 43, 38], several authors have proposed norms that induce low-rank
tensors.
These approaches solve convex problems of the following form:
minimize
W
L(W) + λ
W

⋆,
(14.3)
where L : Rn1×···×nK →R is a convex loss function that measures how well
W ﬁts the data,
W

⋆is a norm (we discuss in detail below), and λ > 0 is a
regularization parameter.
For example, let’s assume that the measurements y = (yi)M
i=1 are generated
as
yi = ⟨Xi, W∗⟩+ ϵi,
(14.4)
where ⟨X, W⟩denotes the inner product between two tensors viewed as vec-
tors in RN; more precisely, ⟨X, W⟩= P
i1,...,iK Xi1...iKWi1...iK. Then the loss
function can be deﬁned as the sum of squared residuals
L(W) = 1
2∥y −X(W)∥2
2,
where X(W) := (⟨Xi, W⟩)M
i=1.
The minimization problem (14.3) minimizes the loss function also keep-
ing the norm small. The diﬀerence from the conventional optimization based
approaches for tensor decomposition is that instead of constraining the (mul-
tilinear) rank of the decomposition, it only constrains the complexity of the
solution measured by a particular norm.
In the case of matrices, it is well known that the Schatten 1-norm
∥W ∥S1 =
r
X
j=1
σj(W ),
where σj(W ) is the jth singular value of W and r is the rank of W promotes
the solution of (14.3) to be low-rank; see, e.g., [15]. Intuitively, this can be
understood analogous to the sparsity inducing property of the ℓ1 norm; it
promotes the spectrum of W to be sparse, i.e., a spectral version of lasso [51].

Low-Rank Tensor Denoising and Recovery via Convex Optimization
317
It is known that the Schatten 1-norm of a rank r matrix W can be related
to its Frobenius norm as follows [50]:
∥W ∥S1 ≤√r∥W ∥F .
Thus a low-rank matrix has a small Schatten 1-norm relative to its Frobenius
norm.
The following norm has been proposed by several authors [47, 18, 33, 52]:
W

S1/1 =
K
X
k=1
∥W(k)∥S1.
(14.5)
We call the norm (14.5) overlapped Schatten 1-norm. Intuitively, it penalizes
the Schatten 1-norms of the K unfoldings, and minimizing the norm promotes
W to have low-multilinear rank. In fact, it is easy to show (see [54]) the
inequality
W

S1/1 ≤
K
X
k=1
√rk
W

F ,
(14.6)
where
W

F is the Frobenius norm
W

F =
p
⟨W, W⟩. Thus, tensors that
have low multilinear rank (on average) have low overlapped Schatten 1-norm
relative to the Frobenius norm.
Another norm proposed in [52, 53] is the latent Schatten 1-norm
W

S1/1 =
inf
(W(1)+···+W(K))=W
K
X
k=1
∥W (k)
(k) ∥S1.
(14.7)
Here the norm is deﬁned as the inﬁmum over all tuple of K tensors that sums
to the original tensor W. It is also easy to relate the latent Schatten 1-norm
to the multilinear rank of W. In [53], it was shown that
W

S1/1 ≤min
k
√rk
W

F .
(14.8)
Note that the sum in inequality (14.6) is replaced by the minimum in inequal-
ity (14.8). Therefore, the latent Schatten 1-norm is small when the minimum
mode-k rank of W is small.
14.3
Statistical Guarantees
In this section we present statistical performance guarantee for the esti-
mators deﬁned by the overlapped and latent Schatten 1-norms.

318
Regularization, Optimization, Kernels, and Support Vector Machines
14.3.1
Denoising Bounds
The ﬁrst two theorems concern the denoising performance of the two
norms.
Suppose that the observation Y ∈Rn1×···×nK is obtained as follows:
Y = W∗+ E,
where W∗is the true low-rank tensor with multilinear rank (r1, . . . , rK) and
E ∈Rn1×···×nK is the noise tensor whose entries are independently identically
distributed zero-mean Gaussian random variables with variance σ2.
Deﬁne the estimator ˆ
W by
ˆ
W = argmin
W
1
2
Y −W
2
F + λ
W

S1/1

,
(14.9)
where λ > 0 is a regularization parameter.
Then we have the following denoising performance guarantee.
Theorem 14.1 (Denoising via the overlapped Schatten 1-norm [54]). There
are universal constants ci > 0 (i = 0, 1) such that any minimizer of (14.9)
with λ = c0 σ
K
PK
k=1(
p
N/nk + √nk) satisﬁes the following bound
1
N
 ˆ
W −W∗2
F ≤c1σ2
 
1
K
K
X
k=1
(
p
1/nk +
p
nk/N)
!2  
1
K
K
X
k=1
√rk
!2
.
with probability at least 1 −exp(−( 1
K
PK
k=1(
p
N/nk + √nk))2).
In particular, if nk = n, the above bound implies the following:
1
N
 ˆ
W −W∗2
F ≤Op

σ2 ∥r∥1/2
n

,
(14.10)
where ∥r∥1/2 := ( 1
K
PK
k=1
√rk)2.
In order to state a bound for the latent Schatten 1-norm, we need addi-
tional assumptions. Suppose the following observation model
Y = W∗+ E =
K
X
k=1
W∗(k) + E,
where W∗= PK
k=1 W∗(k) is the true tensor composed of factors W∗(k) that
each are low-rank in the corresponding mode, i.e., rank(W ∗(k)
(k) ) = ¯rk. Note
that generally ¯rk is diﬀerent from the mode-k rank of W∗denoted by rk.
The entries of the noise tensor E are distributed according to the Gaussian
distribution N(0, σ2) as above. In addition, we assume that the spectral norm
of a factor W∗(k) is bounded when unfolded at a diﬀerent mode as follows:
∥W∗(k)
(k′) ∥S∞≤α
K
p
N/nk′
(k ̸= k′).
(14.11)

Low-Rank Tensor Denoising and Recovery via Convex Optimization
319
In other words, we assume that the spectral norm of the kth factor unfolded at
the k′th mode is comparable to that of a random matrix for k′ ̸= k; note that
the spectral norm of a random m × n matrix whose entries are independently
distributed centered random variables with ﬁnite fourth moment scales as
O(√m + √n) [57]. This means that we want the kth factor W(k) to look only
low-rank in the kth mode as the spectral norm of a low-rank matrix would be
larger than a random full rank matrix.
Now let’s consider the estimator
ˆ
W = argmin
W
 
1
2
Y −W
2
F + λ
W

S1/1
s.t. W =
K
X
k=1
W(k), ∥W (k)
(k′)∥S∞≤α
K
p
N/nk′,
∀k ̸= k′
!
.
(14.12)
The following theorem states the denoising performance of the latent
Schatten 1-norm.
Theorem 14.2 (Denoising via the latent Schatten 1-norm [53]). There are
universal constants ci > 0 (i = 0, 1) such that any solution of the minimization
problem (14.12) with regularization constant λ = c0σ maxk(
p
N/nk + √nk)
satisﬁes
1
N
K
X
k=1
 ˆ
W(k) −W∗(k)2
F ≤c1σ2

max
k (1/√nk +
p
nk/N)
2
K
X
k=1
¯rk, (14.13)
with probability at least 1 −K exp(−(maxk(
p
N/nk + √nk))2). Moreover, the
total error ˆ
W −W∗can be bounded as follows:
1
N
 ˆ
W −W∗2
F ≤c1σ2

max
k (1/√nk +
p
nk/N)
2
min
k rk,
(14.14)
with the same probability as above.
In particular, if nk = n, the above bound implies the following:
1
N
 ˆ
W −W∗2
F ≤Op

σ2 mink rk
n

.
(14.15)
Comparing Inequalities (14.10) and (14.15), we can see that the bound for
the latent approach scales by the minimum mode-k rank, whereas that for the
overlap approach scales by the average (square-root) of the mode-k ranks; see
[53] for more details.

320
Regularization, Optimization, Kernels, and Support Vector Machines
14.3.2
Tensor Recovery Guarantee
The next theorem concerns the problem of recovering a low-rank tensor
from a small number of linear measurements. Suppose that the observations
y = (yi)M
i=1 are obtained as in (14.4) with ϵi ∼N(0, σ2). In addition, we
assume that the entries of the observation operator X are drawn independently
and identically from standard Gaussian distribution.
Now consider the estimator
ˆ
W = argmin
W
 1
2M ∥y −X(W)∥2
2 + λM
W

S1/1

.
(14.16)
The following theorem gives a bound for tensor reconstruction from a small
number of noisy measurements.
Theorem 14.3 (Tensor recovery with the overlapped Schatten 1-norm [54]).
There are universal constants ci > 0 (i = 0, 1, 2, 3, 4) such that for a
sample size M ≥c1( 1
K
PK
k=1(
p
N/nk + √nk))2( 1
K
PK
k=1
√rk)2, any solu-
tion ˆ
W of the minimization problem (14.16) with the regularization constant
λM = c0σ
  1
K
PK
k=1(
p
N/nk + √nk)

/
√
M satisﬁes the following bound:
 ˆ
W −W∗2
F ≤c2
σ2 
1
K
PK
k=1(√nk +
p
N/nk)
2
( 1
K
PK
k=1
√rk)2
M
,
with probability at least 1 −c3e−c4M −exp(−( 1
K
PK
k=1(√nk +
p
N/nk))2).
In particular, if nk = n the above bound implies the following:
 ˆ
W −W∗2
F ≤Op
 
σ2∥r∥1/2nK−1
M
!
,
(14.17)
where ∥r∥1/2 := ( 1
K
PK
k=1
√rk)2.
The above theorem tells us that the number of samples that we need scales
as O(∥r∥1/2nK−1). This is rather disappointing because it is only better by a
factor ∥r∥1/2/n compared to not assuming any low-rank-ness of the underlying
truth. This motivates some of the extensions we discuss in Section 14.6.
14.4
Optimization
In this section, we discuss optimization algorithms for overlapped Schatten
1-norm (14.5) and latent Schatten 1-norm (14.7) based on the alternating
direction method of multipliers (ADMM) [17].
ADMM is a general technique that can be used whenever splitting makes
the problem easier to solve; see [8, 55].

Low-Rank Tensor Denoising and Recovery via Convex Optimization
321
14.4.1
ADMM for the Overlapped Schatten 1-Norm
Regularization
We reformulate the overlapped Schatten 1-norm based tensor recovery
problem as follows:
minimize
W,Z1,...,ZK
1
2λ ∥y −Xw∥2
2 +
K
X
k=1
∥Zk∥S1,
(14.18)
subject to
Pkw = zk
(k = 1, . . . , K).
(14.19)
Here Zk ∈Rnk×N/nk (k = 1, . . . , K) are auxiliary variables and zk is the
vectorization of Zk. We also denote the vectorization of W by w and Xw =
X(W). Pk denotes the mode-k unfolding operation; i.e., vec(W(k)) = Pkw.
Note that the regularization parameter λ is in the denominator of the loss
term. Although dividing the objective by λ does not change the minimizer,
it keeps the regularization term from becoming negligible in the limit λ →0;
this is useful for dealing with the noiseless case as we explain below.
The augmented Lagrangian function for optimization problem (14.18) can
be deﬁned as
L(w, (zk)K
k=1, (αk)K
k=1) = 1
2λ∥y −Xw∥2
2 +
K
X
k=1
∥Zk∥S1
+ η
K
X
k=1

αk
⊤(zk −Pkw) + 1
2∥zk −Pkw∥2
2

,
where αk is the Lagrange multiplier vector corresponding to the equality
constraint zk = Pkw.
The basic idea of ADMM is to minimize the augmented Lagrangian func-
tion with respect to w and (zk) while maximizing it with respect to (αk).
Following a standard derivation (see [8, 55]), we obtain the following itera-
tions (see [52] for the derivation):





wt+1 =
 X⊤X + ληKI
−1 
X⊤y + λη PK
k=1 Pk⊤(zt
k + αt
k)

,
zt+1
k
= prox1/η
 Pkwt+1 −αt
k

(k = 1, . . . , K),
αt+1
k
= αt
k + (zt+1
k
−Pkwt+1)
(k = 1, . . . , K).
Here prox1/η is the proximity operator with respect to Schatten 1-norm and
is deﬁned as follows:
proxθ(z) = vec
 U max(S −θ, 0)V ⊤
,
(14.20)
where Z = USV ⊤is the singular-value decomposition (SVD) of Z, z is the
vectorization of Z, and θ ≥0 is a nonnegative parameter.
The ﬁrst step can be carried out eﬃciently, for example, by precomputing

322
Regularization, Optimization, Kernels, and Support Vector Machines
the Cholesky factorization of (X⊤X +ληKI) or linearization (see [60]). Note
that assuming M ≤N and rank(X) = M, we can express the limit of the
ﬁrst step as λ →0 as follows:
wt+1 = X+y + (I −X+X) 1
K
K
X
k=1
Pk
⊤(zt
k + αt
k),
where X+ := X⊤(XX⊤)−1 is the pseudo inverse of X. Taking the limit
λ →0 corresponds to solving the noise-free problem
minimize
W
K
X
k=1
∥W(k)∥S1
subject to
y = X(W).
Putting 1/λ in front of the loss term allows us to deal with the two problems
in the same framework.
In particular, in the case of tensor completion, X is a zero-or-one ma-
trix that has one non-zero entry in every row corresponding to the observed
position. In this case, the update can be further simpliﬁed as follows:
wt+1
i
=
(
(X⊤y)i
(if position i is observed),
( 1
K
PK
k=1 Pk⊤(zt
k + αt
k))i
(otherwise).
Although careful tuning of the parameter η is not essential for the conver-
gence of the above algorithm, in practice the speed of convergence can be quite
diﬀerent. Here we suggest the following heuristic choice. Consider scaling the
truth W∗and the noise ϵ by a constant c as W′∗= cW∗and ϵ′ = cϵ. Using
λ′ = cλ, we get the original solution multiplied by the same constant. Now we
require that the process of optimization should also be essentially the same.
To this end, we need to scale η inversely as 1/c so that all the terms appearing
in the augmented Lagrangian function scales linearly against c. Therefore, we
choose η as η = η0/std(y) where η0 is a constant and std(y) is the standard
deviation of y.
As a stopping criterion we use the primal-dual gap; see [52] for details.
14.4.2
ADMM for Latent Schatten 1-Norm Regularization
In this section, we present the ADMM for solving the dual of the latent
Schatten 1-norm regularized least squares regression problem:
minimize
W
1
2λ∥y −X(
XK
k=1 W(k))∥2
2 +
K
X
k=1
∥W (k)
(k) ∥S1.
(14.21)
The dual problem can be written as follows:
minimize
α,Z1,...,ZK
λ
2 ∥α∥2
2 −α⊤y +
K
X
k=1
δS∞(Zk) ,
subject to
zk = PkX⊤α
(k = 1, . . . , K),
(14.22)

Low-Rank Tensor Denoising and Recovery via Convex Optimization
323
where δS∞is the indicator function of the unit spectral norm ball, i.e.,
δS∞(Z) =
(
0
(if ∥Z∥S∞≤1),
+∞
(otherwise).
The augmented Lagrangian function can be written as follows:
Lη (α, (Zk), (Wk)) =λ
2 ∥α∥2
2 −α⊤y +
K
X
k=1
δS∞(Zk)
+
K
X
k=1

wk
⊤(PkX⊤α −zk) + η
2∥PkX⊤α −zk∥2
2

,
where Wk (k = 1, . . . , K) is the Lagrange multiplier vector corresponding
to the equality constraint (14.22) and equals the mode-k unfolding of primal
variable W(k) at the optimality.
The iterations can be derived as follows (see [52] for details):





wt+1
k
= proxη
 wt
k + ηPkX⊤αt
,
zt+1
k
= (wt
k + ηPkX⊤αt −wt+1
k
)/η,
αt+1 =
1
λ+ηK

y + ηX PK
k=1 Pk⊤(zt+1
k
−wt+1
k
/η)

,
where proxη is the proximity operator (14.20).
We can see that the algorithm updates the dual variables ((zk) and α)
and the primal variables (wk) alternately. In particular, the update equation
for the primal variables (wk) is a popular proximal-gradient-type update. In
fact, PkX⊤αt converges to the gradient of the loss term at the optimality.
Note that setting λ = 0 gives the correct update equations for the noiseless
case λ →0 in (14.21).
Consideration on the scale invariance of the algorithm similar to that in
the previous subsection suggests that we should scale η linearly as the scale
of y; thus we set η = η0std(y).
14.5
Experiments
14.5.1
Tensor Denoising
We generated synthetic tensor denoising problems as follows. First each
entry of the core tensor C was sampled independently from standard normal
distribution. Then orthogonal factors drawn from the uniform (Haar) measure
were multiplied to each of its modes to obtain the true tensor W∗. Then the
observed tensor Y was obtained by adding zero-mean Gaussian noise with
standard deviation σ = 0.1 to each entry.

324
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 14.1: Estimation of a low-rank 50×50×20 tensor of rank r × r × 3
from noisy measurements. The noise standard deviation was σ = 0.1. The
estimation errors of overlapped and latent approaches are plotted against the
rank r of the ﬁrst two modes. The solid lines show the error at the ﬁxed
regularization constant λ, which was 0.89 for the overlapped approach and
3.79 for the latent approach. The dashed lines show the minimum error over
candidates of the regularization constant λ from 0.1 to 100. In the inset, the
errors of the two approaches are plotted against the regularization constant
λ for rank r = 40 (marked with vertical gray dashed line in the outset). The
two values (0.89 and 3.79) are marked with vertical dashed lines.
The two approaches (overlap and latent Schatten 1-norms) were applied
with diﬀerent values of the regularization parameter λ ranging from 0.01 to
100. The incoherence parameter α for the latent Schatten 1-norm was set to
a suﬃciently large constant value so that it had no eﬀect on the solution.
Figure 14.1 shows the result of applying the two approaches to tensors
of multilinear rank (r, r, 3) for diﬀerent r. This experiment was speciﬁcally
designed to highlight the dependency of the denoising performance of the two
methods. The error of the overlapped Schatten 1-norm increases as r increases
although the rank of the third mode is constant; this is because the right-hand
side of (14.10) depends on the average (square-root) of multilinear ranks. On
the other hand, the error of the latent Schatten 1-norm stays almost constant;
this is because the minimum multilinear rank 3 is constant; see Theorem 14.2.
Of course, this is just one well-constructed example, and we refer the readers
to [53] for more results that quantitatively validate Theorem 14.2.

Low-Rank Tensor Denoising and Recovery via Convex Optimization
325
14.5.2
Tensor Completion
A synthetic tensor completion problem was generated as follows. The true
tensor W∗was generated the same way as in the previous subsection. Then
we randomly split the entries into training and testing. No observational noise
was added.
We trained overlapped and latent Schatten 1-norms using the optimization
algorithms discussed in the previous section. The operator X was deﬁned as
X(W) = (Wisjsks)M
s=1,
where (is, js, ks)M
s=1 is the set of indices corresponding to the observed posi-
tions. Since there is no observational noise, we took the limit λ →0 in the
update equations.
The result for 50×50×20 tensor of multilinear rank (7,8,9) is shown in Fig-
ure 14.2. As baselines we included an expectation-maximization-based Tucker
decomposition algorithm in [4] with the correct rank (exact) and 20% higher
rank (large). We also included matrix completion algorithm that treated a
tensor as a matrix by unfolding the tensor at a prespeciﬁed mode. This
method was implemented by instantiating only one of the auxiliary variables
Z1, . . . , ZK in the ADMM for overlapped Schatten 1-norm presented in Sec-
tion 14.4.1.
The result shows that ﬁrst, treating tensor as a matrix yields a rather
disappointing result, especially when we choose mode 3. This is because the
dimensions are not balanced, which is often the case in practice, and unluckily
the mode with the smallest dimension (mode 3) has the highest rank. On the
other hand, the overlapped Schatten 1-norm can recover this tensor reliably
from about 35% of the entries without any assumption about the low-rank-ness
of the modes.
Second, the reconstruction is exact (up to optimization tolerance) above
the suﬃcient sampling density (35%). This can be predicted from Theorem
14.3 in the following way: ﬁrst note that the condition for the sample size
M does not depend on the noise variance σ2; second, the right-hand side of
the bound is proportional to the noise variance σ2. Therefore, if we take the
limit σ2 →0, the theorem predicts zero error whenever the condition for the
sample size M is satisﬁed. We would need a lower bound to make this claim
more precise, which may be obtained by following the work of [2].
Compared to the overlapped approach, the latent approach recovers the
true tensor exactly only around 70% observation. Although we don’t have a
theory for tensor recovery via the latent approach, it seems to suggest that the
number of samples that we need scales faster than the minimum multilinear
rank, which appeared in the right-hand side of the denoising bound (14.14).

326
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 14.2: Comparison of tensor completion performance of overlapped
and latent Schatten 1-norm regularization. As baselines, Tucker decomposi-
tion with the correct rank (exact) and 20% higher rank (large), and convex
optimization based matrix completion (as a matrix) that focuses on a pre-
speciﬁed mode are included. The size of the tensor is 50 × 50 × 20 and the
true multilinear rank is (7, 8, 9). The generalization error is plotted against
the fraction of observed elements (M/N) of the underlying low-rank tensor.
Also the tolerance of optimization (10−3) is shown.
14.6
Extensions and Related Work
14.6.1
Balanced Unfolding
For a balanced-sized K-way tensor (i.e., nk = n), CP decomposition (14.1)
or Tucker decomposition (14.2) has only linearly many parameters in n. Thus
we would expect that a reasonable estimator would decrease the error as
O(n/M). However, the scaling we see in inequality (14.17) is O(nK−1/M),
which is far larger than what we expect.
Looking at the way the bound is derived, we notice (we thank Nam H.
Nguyen for pointing this out) that the unbalancedness of the unfolding is the
cause. More speciﬁcally, the term √nk +
p
N/nk is the spectral norm of a
random nk × N/nk matrix with independent centered entries with bounded
fourth moment [57]. Thus, we can ask what happens if we unfold the tensor
evenly.
Let W(i1,i2,...,ik;j1,j2,...,jl) denote the Qk
a=1 nia × Ql
b=1 njb matrix obtained
by concatenating the Qk
a=1 nia dimensional slices of W speciﬁed by indices in
[nj1] × · · · × [njl] along columns. For example, W(1;2,3,4) is the same as W(1)
in the original notation deﬁned in Section 14.2. We say that an unfolding is

Low-Rank Tensor Denoising and Recovery via Convex Optimization
327
FIGURE 14.3: The number of samples necessary to recover a n × n × n × n
tensor of multilinear rank (2, 2, 2, 2). The number of samples at the phase
transition Mc was deﬁned as the number of samples at which the empirical
probability of obtaining error smaller than 0.01 exceeded 1/2.
balanced if the number of rows and columns are the same, e.g., W(1,2;3,4) when
nk = n.
Figure 14.3 shows the number of samples at the phase transition Mc
against n for the completion of 4th order balanced-sized tensors. We com-
pared the original overlapped Schatten 1-norm (14.5) against the following
norm based on three balanced unfoldings,
W

balanced = ∥W(1,2;3,4)∥S1 + ∥W(1,3;2,4)∥S1 + ∥W(1,4;2,3)∥S1.
See Mu et al. [37] for a related approach, though they only considered one of
the three possible balanced unfoldings.
The threshold Mc was deﬁned as the number of samples at which the
probability that the reconstruction error
 ˆ
W −W∗
F was smaller than 0.01
exceeded 1/2. The dashed line corresponds to the original overlapped Schatten
1-norm (one mode against the rest). The dash-dotted line corresponds to the
overlapped Schatten 1-norm based on balanced unfoldings.
We can see that the empirical scaling of the balanced version is n2.08,
whereas that of the ordinary version is n2.93. Both of them were close to the
theoretically predicted scaling n2 and n3, respectively.
However, computationally this approach is more challenging. The major
computational cost for optimization is that of SVD. Since SVD scales as
O(m2n + m3) for an m × n matrix with m ≤n, the more balanced, the
more challenging the computation becomes. Note that the comparison here is

328
Regularization, Optimization, Kernels, and Support Vector Machines
made assuming that both approaches use the same ADMM-based optimiza-
tion algorithm (see Section 14.4.1). Thus there might be another optimization
algorithm (see, e.g., Jaggi [25]) that works better in the balanced case.
Recently Mu et al. [37] derived a lower-bound for the overlapped Schatten
1-norm based on the framework developed by Amelunxen et al. [2]. The lower-
bound indeed shows that rnK−1 samples is unavoidable for the vanilla version
of the overlapped Schatten 1-norm. Motivated by the lower bound, they pro-
posed a balanced version (without overlap), which they call the square norm.
14.6.2
Tensor Nuclear Norm
Chandrasekaran et al. [12] discuss a norm for tensors within the framework
of atomic norms. Let A be an atomic set that consists of rank one tensors of
unit Frobenius norm:
A = {u1 ◦u2 ◦· · · ◦uK : ∥uk∥= 1
(k = 1, . . . , K)}.
The nuclear norm for tensor is deﬁned as follows:
W

nuc = inf
X
a∈A
ca
s.t.
W =
X
a∈A
cau(a)
1
◦· · · ◦u(a)
K ,
where with a slight abuse of notation, we use a ∈A as an index for an element
in the atomic set.
It can be shown that for a tensor that admits an orthogonal CP decompo-
sition [28] with R terms (Decomposition (14.1) with orthogonality constraints
between the components), the nuclear norm can be related to the Frobenius
norm as follows:
W

nuc ≤
√
R
W

F .
Moreover, the tensor spectral norm
X

op = max
a∈A X ×1 u(a)
1
×2 u(a)
2
· · · ×K u(a)
K ,
which is dual to the nuclear norm, is known to be of order O(√n) for a random
Gaussian tensor; see [40]. Thus it is natural to hope that we can prove that the
nuclear norm would achieve an optimal O(Rn) convex relaxation for tensors.
However, computationally, the tensor nuclear norm seems to be intractable for
K ≥3. Although it is convex, it involves inﬁnitely many variables. There is no
analogue of linear matrix inequality or semideﬁnite programming for matrices
that can be used here to the best of our knowledge.
14.6.3
Interpretation of the Result
That we can bound the error in Frobenius norm as we have presented in
Section 14.3 does not mean that our method is useful in practice. In fact,

Low-Rank Tensor Denoising and Recovery via Convex Optimization
329
tensor decomposition methods are often used to uncover latent factors and
gain insight about the data.
Here we present how such an insight can be gained from the solutions of
the two algorithms we presented in Section 14.4.
For the overlapped approach, the factor matrices U1, . . . , UK correspond-
ing to the Tucker decomposition (14.2) can be obtained by computing the left
singular vectors of the auxiliary matrices Z1, . . . , ZK. The mode-k rank rk
is determined automatically by the proximity operator (14.20); importantly,
the rank at an optimum does not depend on the choice of η, though the rank
during optimization may depend on η. Once the factors are obtained, the core
can be obtained as follows:
C = W ×1 U1
⊤×2 U2
⊤· · · ×K UK
⊤.
To get the stronger CP decomposition (14.1), one can perform any oﬀ-the-shelf
CP decomposition algorithm on the core C. This post-processing step is easier
than applying CP decomposition directly to the original large tensor with
noise and missing entries. In other words, this two-step approach allows us to
separate the tasks of generalization and interpretation; see [52] for details.
It is less easy to interpret the solution for the latent approach because
in general the sum W = PK
k=1 W(k) is not low-rank even when each W(k) is.
However, in practice we found that the solution is often singleton, i.e., only one
non-zero component W(k). This corresponds to the intuition that the latent
Schatten 1-norm focuses on the low-rank-ness of the mode with the minimum
mode-k rank and does not care about the other modes. The fact that the
solution is only low-rank in one mode is still disappointing. This could be
solved by including more terms in the latent approach, which can be partially
low-rank (there are 2K possibilities to penalize the sum of the Schatten 1-
norms of some of the modes) or balanced unfolding. If the resulting solution
is a singleton, then the model automatically chose which mode should be
low-rank.
14.6.4
Related Work
Liu et al. [33, 34] proposed the overlapped approach in the context of
image and video imputation. They used a penalty method to deal with the
equality constraints in (14.19). Li et al. [32] extended Liu et al.’s work to
sparse+low-rank decomposition of tensors (also known as sparse PCA) and
applied to background/shadow removal and face recognition. Li et al. also
used a penalty method for the optimization.
Signoretto et al. [47, 49, 48, 46] proposed and extended the overlapped
Schatten 1-norm in the context of kernel-based learning, i.e., learning higher-
order operators over Hilbert spaces. The use of kernel allows us to incorporate
smoothness (or side-information) when we see an entry in a K-way tensor as
a representation of a relation among objects from K diﬀerent domains; see

330
Regularization, Optimization, Kernels, and Support Vector Machines
also [1]. They also proposed an optimization algorithm that supports gen-
eral diﬀerentiable loss function L based on an (accelerated) proximal gradient
method [39, 6]; the algorithm employs ADMM to compute the proximal op-
erator corresponding to the overlapped Schatten 1-norm.
Gandy et al. [18] proposed ADMM and Douglas-Rachford splitting algo-
rithm for the overlapped approach.
Yang et al. [58] proposed a fast optimization algorithm for the overlapped
approach based on a ﬁxed-point iteration combined with continuation.
Goldfarb and Qin [19] studied low-rank+sparse tensor decomposition
based on the overlapped and latent Schatten 1-norms. They also proposed
adaptive weighting of the terms appearing in the overlapped Schatten 1-
norm (14.5) and reported that the adaptive version outperformed other meth-
ods in many cases. They also studied the relationship between the normalized
rank ∥n−1∥1/2∥r∥1/2 (the quantity that appears in the condition for M in
Theorem 14.3), the necessary sampling density, and the allowable fraction of
corrupted entries.
Zhang et al. [61] extended Li et al.’s work [32] on sparse+low-rank decom-
position in several interesting ways. They have incorporated transformations
that align each image in order to make the spatial low-rank assumption (on
the ﬁrst two modes) as valid as possible (see also [36] for related work), while
keeping the sequence of images smooth by also penalizing the Schatten 1-norm
for the mode corresponding to the temporal dimension.
On the theoretical side, Nickel and Tresp [41] presented a generalization
bound for low multilinear rank tensor in the context of relational data analysis
by counting the number of possible sign patterns that low multilinear rank
tensors can attain. Although the theory does not lead to a model selection
criterion as we cannot provably compute the low-multilinear-rank decomposi-
tion at a given rank, it would still be fruitful to study a convex relaxation for
the set of low-rank sign tensors; see, e.g., [50].
Romera-Paredes and Pontil [45] proposed a convex relaxation of mode-k
rank with respect to the Frobenius norm ball and showed that it is tighter
than the overlapped Schatten 1-norm at some points. Although the resulting
regularizer is not a continuous function and thus challenging to compute, they
proposed a subgradient-based optimization algorithm.
Jiang et al. [26] studied the best rank-one approximation of super symmet-
ric even order tensors. They noticed that for super symmetric (meaning that
the tensor is invariant to arbitrary permutation of indices) even order tensors,
being rank-one is equivalent to a balanced unfolding (see Section 14.6.1) be-
ing rank-one (as a matrix). Then they solved the best rank-one approximation
problem with the Schatten 1-norm regularization (which promotes rank-one
solution) under linear equality constraints that ensured that the solution was
super-symmetric. Empirically the solution was found to be rank one in most
of the cases. They also showed many non-symmetric problems can be reduced
to the symmetric case.

Low-Rank Tensor Denoising and Recovery via Convex Optimization
331
Krishnamurthy and Singh [30] proposed an adaptive sampling algorithm
for tensor completion and showed that it succeeds with high probability with
O(n) samples. The number of samples required in their adaptive setting also
depends on the true rank r and the coherence parameter µ0. They also showed
a lower bound that scales as O(rK−1n) under the incoherence assumption.
Application of the overlapped approach includes language models [24],
hyper-spectral imaging [49], and multi-task learning [46, 44], besides image
reconstruction discussed in [33, 34, 32, 61].
14.7
Future Directions
Compared to the overlapped Schatten 1-norm, the behavior of the latent
Schatten 1-norm is still unclear in some parts. First, although we have argued
that empirically the solution of the optimization problem is often a singleton
(only one non-zero component), this needs a better explanation. Second, al-
though we believe that the incoherence assumption is necessary to prove the
stronger inequality (14.13), it may not be necessary to obtain the weaker one
(14.14).
Given that the sample complexities of both the overlapped and latent
Schatten 1-norms are far from optimal, it would be extremely interesting to
explore the statistics-computation trade-oﬀbetween what we can provably
achieve and how computationally expensive it would be. Balanced unfold-
ing [37], tensor nuclear norm [12], and the new convex relaxation [45] discussed
in the previous section are candidates to be evaluated and analyzed. It would
also be interesting to study recent work on decomposition of tensors arising
from higher order moments of latent variable models [3] in this context.
Finally, nonnegativity [9, 31] and positive semideﬁniteness [59] are con-
straints that are useful to impose on the factors in practice. Generalization
of the results for separable nonnegative matrix factorization [5, 7] to tensors
would be an interesting direction.
Acknowledgments
The authors would like to thank Franz J. Király, Nati Srebro, Nam H.
Nguyen, and Gilles Blanchard for discussions and the editors of this book for
comments. This work was partially supported by MEXT Kakenhi 25730013,
and JST, CREST.

332
Regularization, Optimization, Kernels, and Support Vector Machines
Bibliography
[1] J. Abernethy, F. Bach, T. Evgeniou, and J.P. Vert. A new approach to
collaborative ﬁltering: Operator estimation with spectral regularization.
J. Mach. Learn. Res., 10:803–826, 2009.
[2] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the
edge: A geometric theory of phase transitions in convex optimization.
Technical report, arXiv:1303.6672, 2013.
[3] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor
decompositions for learning latent variable models.
Technical report,
arXiv:1210.7559, 2012.
[4] C. A. Andersson and R. Bro.
The n-way toolbox for MATLAB.
Chemometr. Intell. Lab., 52(1):1–4, 2000.
http://www.models.life.ku.dk/source/nwaytoolbox/.
[5] S. Arora, R. Ge, R. Kannan, and A. Moitra. Computing a nonnegative
matrix factorization–provably. In Proceedings of the 44th symposium on
Theory of Computing, pages 145–162, 2012.
[6] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algo-
rithm for linear inverse problems. SIAM J. Imaging Sci., 2(1):183–202,
2009.
[7] V. Bittorf, B. Recht, C. Ré, and J. A. Tropp.
Factoring nonnegative
matrices with linear programs. In Adv. Neural. Inf. Process. Syst. 25,
pages 1223–1231. 2012.
[8] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed
optimization and statistical learning via the alternating direction method
of multipliers. Foundations and Trends R⃝in Machine Learning, 3(1):1–
122, 2011.
[9] R. Bro and S. De Jong. A fast non-negativity-constrained least squares
algorithm. J. Chemometr., 11(5):393–401, 1997.
[10] E. J. Candès and T. Tao. The power of convex relaxation: Near-optimal
matrix completion. IEEE T. Inform. Theory, 56(5):2053–2080, 2010.
[11] J.D. Carroll and J.J. Chang. Analysis of individual diﬀerences in mul-
tidimensional scaling via an n-way generalization of “Eckart-Young” de-
composition. Psychometrika, 35(3):283–319, 1970.
[12] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The con-
vex geometry of linear inverse problems. Foundations of Computational
Mathematics, 12(6):805–849, 2012.

Low-Rank Tensor Denoising and Recovery via Convex Optimization
333
[13] L. De Lathauwer, B. De Moor, and J. Vandewalle. A multilinear singular
value decomposition. SIAM J. Matrix Anal. Appl., 21(4):1253–1278, 2000.
[14] L. De Lathauwer, B. De Moor, and J. Vandewalle. On the best rank-1
and rank-(R1, R2, . . . , RN) approximation of higher-order tensors. SIAM
J. Matrix Anal. Appl., 21(4):1324–1342, 2000.
[15] M. Fazel. Matrix rank minimization with applications. PhD thesis, Stan-
ford University, 2002.
[16] M. Fazel, H. Hindi, and S. P. Boyd. A Rank Minimization Heuristic with
Application to Minimum Order System Approximation. In Proc. of the
American Control Conference, 2001.
[17] D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear
variational problems via ﬁnite element approximation. Comput. Math.
Appl., 2(1):17–40, 1976.
[18] S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank
tensor recovery via convex optimization. Inverse Problems, 27:025010,
2011.
[19] D. Goldfarb and Z. Qin. Robust low-rank tensor recovery: Models and
algorithms. Technical report, arXiv:1311.6182, 2013.
[20] R. A. Harshman.
Models for analysis of asymmetrical relationships
among n objects or stimuli. In First Joint Meeting of the Psychometric
Society and the Society for Mathematical Psychology, McMaster Univer-
sity, Hamilton, Ontario, 1978.
[21] C. J. Hillar and L.-H. Lim. Most tensor problems are np-hard. Journal
of the ACM, 60(6):45, 2013.
[22] F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of
products. J. Math. Phys., 6(1):164–189, 1927.
[23] J. Håstad. Tensor rank is NP-complete. Journal of Algorithms, 11(4):644–
654, 1990.
[24] B. Hutchinson, M. Ostendorf, and M. Fazel. Low rank language models
for small training sets. IEEE Signal Proc. Let., 18(9):489–492, 2011.
[25] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimiza-
tion. In Proceedings of the 30th International Conference on Machine
Learning, pages 427–435, 2013.
[26] B. Jiang, S. Ma, and S. Zhang. Tensor principal component analysis via
convex optimization. Technical report, arXiv:1212.2702, 2012.
[27] R. Kannan and S. Vempala. Spectral algorithms. Foundations and Trends
in Theoretical Computer Science, 4(3–4):157–288, 2008.

334
Regularization, Optimization, Kernels, and Support Vector Machines
[28] T. G. Kolda. Orthogonal tensor decompositions. SIAM J. Matrix Anal.
Appl., 23(1):243–255, 2001.
[29] T. G. Kolda and B. W. Bader. Tensor decompositions and applications.
SIAM Review, 51(3):455–500, 2009.
[30] A. Krishnamurthy and A. Singh. Low-rank matrix and tensor comple-
tion via adaptive sampling. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Adv. Neural. Inf. Pro-
cess. Syst. 26, pages 836–844. 2013.
[31] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative
matrix factorization. Nature, 401(6755):788–791, 1999.
[32] Y. Li, J. Yan, Y. Zhou, and J. Yang. Optimum subspace learning and
error correction for tensors. In Computer Vision–ECCV 2010, pages 790–
803. Springer, 2010.
[33] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for esti-
mating missing values in visual data. In Proc. ICCV, 2009.
[34] J. Liu, J. Ye, P. Musialski, and P. Wonka. Tensor completion for estimat-
ing missing values in visual data. IEEE T. Pattern. Anal., 35(1):208–220,
2013.
[35] M. Mørup. Applications of tensor (multiway array) factorizations and
decompositions in data mining. Wiley Interdisciplinary Rev.: Data Min.
Knowl. Dicov., 1(1):24–40, 2011.
[36] M. Mørup, L. K. Hansen, S. M. Arnfred, L.-H. Lim, and K. H. Madsen.
Shift-invariant multilinear decomposition of neuroimaging data.
Neu-
roimage, 42(4):1439–1450, 2008.
[37] C. Mu, B. Huang, J. Wright, and D. Goldfarb.
Square deal: Lower
bounds and improved relaxations for tensor recovery.
arXiv preprint
arXiv:1307.5870, 2013.
[38] S. Negahban and M.J. Wainwright. Estimation of (near) low-rank ma-
trices with noise and high-dimensional scaling. Ann. Statist., 39(2):673–
1333, 2011.
[39] Y. Nesterov. Gradient methods for minimizing composite objective func-
tion.
Technical Report 2007/76, Center for Operations Research and
Econometrics (CORE), Catholic University of Louvain, 2007.
[40] N. H. Nguyen, P. Drineas, and T. D. Tran.
Tensor sparsiﬁcation via
a bound on the spectral norm of random tensors.
Technical report,
arXiv:1005.4732, 2010.

Low-Rank Tensor Denoising and Recovery via Convex Optimization
335
[41] M. Nickel and V. Tresp.
An analysis of tensor models for learning
on structured data. In Machine Learning and Knowledge Discovery in
Databases, pages 272–287. Springer, 2013.
[42] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solu-
tions of linear matrix equations via nuclear norm minimization. SIAM
Review, 52(3):471–501, 2010.
[43] A. Rohde and A. B. Tsybakov. Estimation of high-dimensional low-rank
matrices. Ann. Statist., 39(2):887–930, 2011.
[44] B. Romera-Paredes, H. Aung, N. Bianchi-Berthouze, and M. Pontil. Mul-
tilinear multitask learning. In Proceedings of the 30th International Con-
ference on Machine Learning, pages 1444–1452, 2013.
[45] B. Romera-Paredes and M. Pontil. A new convex relaxation for tensor
completion. In Adv. Neural. Inf. Process. Syst. 26, pages 2967–2975, 2013.
[46] M. Signoretto, L. De Lathauwer, and J. A. K. Suykens. Learning tensors
in reproducing kernel hilbert spaces with multilinear spectral penalties.
Technical report, arXiv:1310.4977, 2013.
[47] M. Signoretto, L. De Lathauwer, and J. A. K. Suykens. Nuclear norms
for tensors and their use for convex multilinear estimation. Technical
Report 10-186, ESAT-SISTA, KU Leuven, 2010.
[48] M. Signoretto, Q. T. Dinh, L. De Lathauwer, and J. A. K. Suykens.
Learning with tensors: a framework based on convex optimization and
spectral regularization. Mach. Learn., 94(3):303–351, 2014.
[49] M. Signoretto, R. Van de Plas, B. De Moor, and J. A. K. Suykens. Tensor
versus matrix completion: a comparison with application to spectral data.
IEEE Signal Proc. Let., 18(7):403–406, 2011.
[50] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proc.
of the 18th Annual Conference on Learning Theory (COLT), pages 545–
560. Springer, 2005.
[51] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy.
Stat. Soc. B, 58(1):267–288, 1996.
[52] R. Tomioka, K. Hayashi, and H. Kashima. Estimation of low-rank tensors
via convex optimization. Technical report, arXiv:1010.0789, 2011.
[53] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured
schatten norm regularization. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Adv. Neural. Inf. Process.
Syst. 26, pages 1331–1339. 2013.

336
Regularization, Optimization, Kernels, and Support Vector Machines
[54] R. Tomioka, T. Suzuki, K. Hayashi, and H. Kashima. Statistical per-
formance of convex tensor decomposition. In Adv. Neural. Inf. Process.
Syst. 24, pages 972–980. 2011.
[55] R. Tomioka, T. Suzuki, and M. Sugiyama. Augmented lagrangian meth-
ods for learning, selecting, and combining features. In Suvrit Sra, Sebas-
tian Nowozin, and Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press, 2011.
[56] L. R. Tucker. Some mathematical notes on three-mode factor analysis.
Psychometrika, 31(3):279–311, 1966.
[57] R. Vershynin. Introduction to the non-asymptotic analysis of random
matrices. Technical report, arXiv:1011.3027, 2010.
[58] L. Yang, Z Huang, and X. Shi. A ﬁxed point iterative method for low
n-rank tensor pursuit. IEEE T. Signal Proces., 61(11):2952–2962, 2013.
[59] K. Yoshii, R. Tomioka, D. Mochihashi, and M. Goto. Inﬁnite positive
semideﬁnite tensor factorization for source separation of mixture signals.
In Proceedings of the 30th International Conference on Machine Learning,
pages 576–584, 2013.
[60] X. Zhang, M. Burger, and S. Osher. A uniﬁed primal-dual algorithm
framework based on bregman iteration. J. Sci. Comput., 46(1):20–46,
2010.
[61] X. Zhang, D. Wang, Z. Zhou, and Y. Ma. Simultaneous rectiﬁcation and
alignment via robust recovery of low-rank tensors. In Adv. Neural. Inf.
Process. Syst. 26, pages 1637–1645, 2013.

Chapter 15
Learning Sets and Subspaces
Alessandro Rudi∗
DIBRIS, Università degli Studi di Genova and LCSL, Massachusetts Institute
of Technology, and Istituto Italiano di Tecnologia
Guillermo D. Canas∗
Massachusetts Institute of Technology
Ernesto De Vito∗
DIMA, Università degli Studi di Genova
Lorenzo Rosasco∗
DIBRIS, Università degli Studi di Genova and LCSL, Massachusetts Institute
of Technology, and Istituto Italiano di Tecnologia
15.1
Unsupervised Statistical Learning ...............................
338
15.2
Subspace Learning ...............................................
339
15.2.1
Problem Deﬁnition and Notation .......................
340
15.2.2
Subspace Estimators .....................................
340
15.2.3
Performance Criteria ....................................
340
15.2.4
Summary of Results .....................................
341
15.2.5
Kernel PCA and Embedding Methods ..................
343
15.2.6
Comparison with Previous Results in the Literature ...
344
15.3
Set Learning ......................................................
345
15.3.1
Set Learning via Subspace Learning ....................
345
15.3.2
Consistency Results .....................................
347
15.4
Numerical Experiments ..........................................
349
15.5
Sketch of the Proofs ..............................................
351
15.6
Conclusions .......................................................
352
Acknowledgments ................................................
352
Bibliography ......................................................
353
We consider here the classic problem of support estimation, or learning a set
from random samples, and propose a natural but novel approach to address it.
We do this by investigating its connection with a seemingly distinct problem,
namely subspace learning.
∗alessandro.rudi@unige.it, guilledc@mit.edu, devito@dima.unige.it, lrosasco@mit.edu
337

338
Regularization, Optimization, Kernels, and Support Vector Machines
The problem of learning the smallest set containing the data distribution is
often called support estimation and it is a fundamental problem in statistics
and machine learning. As discussed in the following, its applications range
from surface estimation, to novelty detection, to name a few. In the following
we discuss how a suitable family of positive deﬁnite kernels, called separating
kernels, allows us to relate the problem of learning a set to the problem of
learning an appropriate linear subspace of a Hilbert space. More precisely, we
reduce the set learning problem to that of learning the smallest subspace that
contains the support of the distribution after a kernel (feature) embedding.
This connection between learning sets and learning subspaces allows us on
the one hand to design natural spectral estimators for this problem, and on
the other hand to use analytic and probabilistic tools to derive generalization
guarantees for them.
Besides establishing this novel connection, the goal of this work is to intro-
duce novel sharp sample complexity estimates for subspace and set learning.
The theoretical results are illustrated and complemented through some nu-
merical experiments.
The chapter is structured as follows. We begin by brieﬂy discussing some
concepts from the statistical analysis of unsupervised learning algorithms (Sec-
tion 15.1). We then develop our analysis of the subspace learning problem, and
discuss set learning in Section 15.3. Finally, we conclude in Section 15.4 with
some numerical results.
15.1
Unsupervised Statistical Learning
The present work can be more broadly framed in the context of unsu-
pervised learning, a term typically used to describe the general problem of
extracting patterns from data [23, 19]. Here, the term pattern refers to some
geometric property of the data distribution. Speciﬁcally, in the sequel we will
be interested in recovering the following: 1) the smallest (closed) set containing
the data distribution, and 2) the smallest subspace spanned by the data dis-
tribution. As we will discuss, these two problems are indeed tightly connected.
After formally describing this connection, our focus will be on introducing a
class of spectral estimators for this problem, and deriving sharp generalization
error estimates for them.
Given a probability space (X, ρ) from which data Xn are drawn identi-
cally and independently, we let S be a set endowed with a (pseudo) metric d.
We view S as the collection of possible patterns/structures in the data distri-
bution (for instance the set of possible supports of a distribution). In many
circumstances, the true distribution ρ identiﬁes an element Sρ in the space of
structures (for instance the true support), and the goal of an (unsupervised)
learning algorithm is to estimate an approximation ˆSn given the data. For

Learning Sets and Subspaces
339
example, in the context of set learning, S may be deﬁned as the collection
of all closed subsets of X endowed with the Hausdorﬀdistance, and Sρ to
be the support of ρ. In the context of subspace learning, S is the collection
of all linear subspaces of X, with some suitable pseudo-metric such as the
reconstruction criterion in (15.3) and Sρ is the smallest subspace spanned by
points drawn from ρ.
Since Sρ is estimated from random samples, we characterize the learning
error of an algorithm through non asymptotic bounds of the form
P
h
d( ˆSn, Sρ) ≤Rρ (δ, n)
i
≥1 −δ
(15.1)
for 0 < δ ≤1, where the learning error Rρ(δ, n) typically depends on n and
δ, but also on ρ. Once a bound of the form of (15.1) with an asymptotically
vanishing learning error R is obtained, almost sure convergence of d( ˆSn, Sρ) →
0 as n →∞follows from the Borel-Cantelli Lemma [29].
15.2
Subspace Learning
Subspace learning is the problem of ﬁnding the smallest linear space sup-
porting data drawn from an unknown distribution. It is a classical problem
in machine learning and statistics and is at the core of a number of spectral
methods for data analysis, most notably PCA [26], but also multidimensional
scaling (MDS) [8, 59]. While traditional methods, such as PCA and MDS, per-
form subspace learning in the original data space, more recent manifold learn-
ing methods, such as isomap [51], Hessian eigenmaps [18], maximum-variance
unfolding [57, 58, 50], locally-linear embedding [39, 42], and Laplacian eigen-
maps [2] (but also kernel PCA [44]), begin by embedding the data in a feature
space, in which subspace estimation is carried out. As pointed out in [22, 4, 3],
all the algorithms in this family have a common structure. They embed the
data in a suitable Hilbert space F, and compute a linear subspace that best
approximates the embedded data. The local coordinates in this subspace then
become the new representation space.
The analysis in this paper applies to learning subspaces both in the data
and in a feature space. In the following, we introduce a general formulation
of the subspace learning problem and derive novel learning error estimates.
Our results rely on natural assumptions on the spectral properties of the
covariance operator associated to the data distribution, and hold for a wide
class of metrics between subspaces. As a special case, we discuss sharp error
estimates for the reconstruction properties of PCA. Key to our analysis is an
operator theoretic approach that has broad applicability to the analysis of
spectral learning methods.

340
Regularization, Optimization, Kernels, and Support Vector Machines
15.2.1
Problem Deﬁnition and Notation
Given a measure ρ with support M in the unit ball of a separable Hilbert
space F, we consider in this work the problem of estimating, from n i.i.d.
samples Xn = {xi}1≤i≤n, the smallest linear subspace Sρ := span(M) that
contains M. In the framework introduced in Section 15.1, the above problem
corresponds to a choice of input space F, and the space of candidate structures
is the collection of all linear subspaces of F. The target of the learning problem
is Sρ, the smallest linear subspace that contains the support of ρ. As described
in Section 15.1, the quality of an estimate ˆSn of Sρ, for a given metric (or error
criterion) d, is characterized in terms of probabilistic bounds of the form of
Equation (15.1).
In the following the metric projection operator onto a subspace S is de-
noted by PS, where P 2
S = P ∗
S = PS (every P is idempotent and self-adjoint).
We denote by ∥· ∥F the norm induced by the dot product < ·, · >F in F, and
by ∥A∥p :=
pp
Tr(|A|p) the p-Schatten, or p-class norm of a linear bounded
operator A [37, p. 84].
15.2.2
Subspace Estimators
Spectral estimators can be naturally derived from the characterization of
Sρ in terms of the covariance operator C associated to ρ. Indeed, if C :=
Ex∼ρx ⊗x is the (uncentered) covariance operator associated to ρ, it is easy
to show that Sρ = Ran C. Similarly, given the empirical covariance Cn :=
1
n
Pn
i=1 x ⊗x, we deﬁne the empirical subspace estimate,
ˆSn := span(Xn) = Ran Cn,
where the closure is not needed because ˆSn is ﬁnite-dimensional. We also deﬁne
the k-truncated (kernel) PCA subspace estimate ˆSk
n := Ran Ck
n, where Ck
n is
obtained from Cn by keeping only its k top eigenvalues; see also Section 15.2.5.
Note that, since the PCA estimate ˆSk
n is spanned by the top k eigenvectors of
Cn, then clearly ˆSk
n ⊆ˆSk′
n for k < k′, and therefore { ˆSk
n}n
k=1 is a nested family
of subspaces (all of which are contained in Sρ). As discussed in Section 15.2.5,
since kernel-PCA reduces to regular PCA in a feature space [44] (and can be
computed with knowledge of the kernel alone), the following discussion applies
equally to kernel-PCA estimates.
15.2.3
Performance Criteria
We deﬁne the pseudo-metric
dα,p(U, V ) := ∥(PU −PV )Cα∥p
(15.2)
between subspaces U, V , which is a metric over the collection of subspaces
contained in Sρ, for 0 ≤α ≤1
2 and 1 ≤p ≤∞. Note that dα,p depends on

Learning Sets and Subspaces
341
ρ through C but this dependence is omitted in the notation. A number of
important performance criteria can be recovered as particular cases of dα,p.
In particular, the so-called reconstruction error [46, 7],
dR(Sρ, ˆS) := Ex∼ρ∥PSρ(x) −P ˆS(x)∥2
F
(15.3)
is dR(Sρ, ·) = d1/2,2(Sρ, ·)2. Note that dR is a natural criterion because a k-
truncated PCA estimate minimizes a suitable error dR over all subspaces of
dimension k. Clearly, dR(Sρ, ˆS) vanishes whenever ˆS contains Sρ and, be-
cause the family { ˆSk
n}n
k=1 of PCA estimates is nested, then dR(Sρ, ˆSk
n) is
non-increasing with k. As shown in [32], a number of unsupervised learn-
ing algorithms, including (kernel) PCA, k-means, k-ﬂats, sparse coding, and
non-negative matrix factorization, can be written as a minimization of dR over
an algorithm-speciﬁc class of sets (e.g., over the set of linear subspaces of a
ﬁxed dimension in the case of PCA).
15.2.4
Summary of Results
Our main technical contribution is a bound of the form of Equation (15.1),
for the k-truncated PCA estimate ˆSk
n (with the empirical estimate ˆSn := ˆSn
n
being a particular case), whose proof is postponed to Section 15.5.
We begin by bounding the distance dα,p between Sρ and the k-truncated
PCA estimate ˆSk
n, given a known covariance C.
Theorem 15.1. Let {xi}1≤i≤n be drawn i.i.d. according to a probability mea-
sure ρ supported on the unit ball of a separable Hilbert space F, with covariance
C. Assuming n > 3, 0 < δ < 1, 0 ≤α ≤1
2, 1 ≤p ≤∞, the following holds
for each k ∈{1, . . . , n}:
P
h
dα,p(Sρ, ˆSk
n) ≤3tα
k
Cα(C + tkI)−α p
i
≥1 −δ
(15.4)
where tk = max{σk, 9
n log n
δ }, and σk is the k-th top eigenvalue of C.
We say that C has eigenvalue decay rate of order r if there are constants
q, Q > 0 such that qj−r ≤σj ≤Qj−r, where σj are the (decreasingly ordered)
eigenvalues of C, and r > 1. From Equation (15.2) it is clear that, in order
for the subspace learning problem to be well-deﬁned, it must be ∥Cα∥p < ∞,
or alternatively: αp > 1/r. Note that this condition is always met for p = ∞,
and also holds in the reconstruction error case (α = 1/2, p = 2), for any decay
rate r > 1.
Knowledge of an eigenvalue decay rate can be incorporated into Theo-
rem 15.1 to obtain explicit learning rates, as follows.
Theorem 15.2 (Polynomial eigenvalue decay). Let C have eigenvalue de-
cay rate of order r. Under the assumptions of Theorem 15.1, it holds, with

342
Regularization, Optimization, Kernels, and Support Vector Machines
probability 1 −δ:
dα,p(Sρ, ˆSk
n) ≤
(
Q′k−rα+ 1
p
if k < k∗
n
(polynomial decay)
Q′k∗
n
−rα+ 1
p
if k ≥k∗
n
(plateau)
(15.5)
where it is k∗
n =

qn
9 log(n/δ)
1/r
, and
Q′ = 3
 
Q
1
r Γ
 αp −1
r

Γ
 1 + 1
r

Γ
  1
r

! 1
p
.
(15.6)
The above theorem guarantees a decay of dα,p with increasing k, at a rate
of k−rα+1/p, up to k = k∗
n, after which the bound remains constant. The
estimated plateau threshold k∗is thus the value of truncation past which the
upper bound does not improve. Note that, as described in Section 15.4, this
error decay and plateau behavior is observed in practice.
The proofs of Theorems 15.1 and 15.2 rely on recent non-commutative
Bernstein-type inequalities on operators [5, 52], and a novel analytical decom-
position. Note that classical Bernstein inequalities in Hilbert spaces (e.g.,[34])
could also be used instead of [52]. While this approach would simplify the
analysis, it produces looser bounds, as described in Section 15.5.
If we consider an algorithm that produces, for each set of n samples, an
estimate ˆSk
n with k ≥k∗
n then, by plugging the deﬁnition of k∗
n into Equa-
tion 15.5, we obtain an upper bound on dα,p as a function of n.
Corollary 15.1. Let C have eigenvalue decay rate of order r, and Q′, k∗
n be
as in Theorem 15.2. Let ˆS∗
n be a truncated subspace estimate ˆSk
n with k ≥k∗
n.
It is, with probability 1 −δ,
dα,p(Sρ, ˆS∗
n) ≤Q′
9 (log n −log δ)
qn
α−1
rp
Remark 15.1. Note that, by setting k = n, the above corollary also provides
guarantees on the rate of convergence of the empirical estimate ˆSn = span(Xn)
to Sρ, of order
dα,p(Sρ, ˆSn) = O
 log n −log δ
n
α−1
rp !
.
Corollary 15.2 and Remark 15.1 are valid for all n such that k∗
n ≤n (or
equivalently such that nr−1(log n −log δ) ≥q/9). Note that, because ρ is
supported on the unit ball, its covariance has eigenvalues no greater than one,
and therefore it must be q < 1. It thus suﬃces to require that n > 3 to ensure
the condition k∗
n ≤n is to hold.

Learning Sets and Subspaces
343
15.2.5
Kernel PCA and Embedding Methods
One of the main applications of subspace learning is to perform dimension-
ality reduction. In particular, one may ﬁnd nested subspaces of dimensions
1 ≤k ≤n that minimize the distances from the original to the projected
samples. This procedure is known as the Karhunen-Loève, PCA, or Hotelling
transform [26], and has been generalized to reproducing-kernel Hilbert spaces
(RKHS) [44].
In particular, the above procedure amounts to computing an eigen-
decomposition of the empirical covariance
Cn =
n
X
i=1
σiui ⊗ui,
where the k-th subspace estimate is ˆSk
n := Ran Ck
n = span{ui : 1 ≤i ≤k}.
In the case of kernel PCA, the samples {xi}1≤i≤n belong to some RKHS F,
and we can think of them as the embedding xi := φ(zi) of some original
data (z1, . . . , zn) ∈Zn, where, e.g., Z = RD. The measure ρ can be seen as
the measure induced by the embedding and the original data distribution.
Interestingly, in practice we may only have indirect information about φ in
the form of a kernel function K : Z × Z →R: a symmetric, positive deﬁnite
function satisfying K(z, w) = ⟨φ(z), φ(w)⟩F [48] (for technical reasons, we also
assume K to be continuous). Recall that every such K has a unique associated
RKHS, and vice versa [48, p. 120–121], whereas, given K, the embedding φ is
only unique up to an inner product-preserving transformation. The following
reproducing property f(x) = ⟨f, K(z, ·)⟩F holds for all z ∈Z, f ∈F.
If the embedding is deﬁned through a kernel K, it easy to see that the k-
truncated kernel PCA can be computed considering the n by n kernel matrix
Kn, where (Kn)i,j = K(xi, xj) [44]. It is easy to see that the k-truncated kernel
PCA subspace ˆSk
n minimizes the empirical reconstruction error dR( ˆSn, ˆS),
among all subspaces ˆS of dimension k. Indeed, it is
dR( ˆSn, ˆS) = Ex∼ˆρ∥x −P ˆS(x)∥2
F = Ex∼ˆρ

(I −P ˆS)x, (I −P ˆS)x

F
= Ex∼ˆρ

I −P ˆS, x ⊗x

HS =

I −P ˆS, Cn

HS ,
(15.7)
where ⟨·, ·⟩
HS is the Hilbert-Schmidt inner product. From this, it clearly fol-
lows that the k-dimensional subspace minimizing Equation 15.7 (maximizing

P ˆS, Cn

) is spanned by the k top eigenvectors of Cn. Since we are interested
in the expected error dR(Sρ, ˆSk
n) of the kernel PCA estimate (rather than the
empirical error dR( ˆSn, ˆS)), we may obtain a learning rate for Equation 15.7 by
specializing Theorem 15.2 to the reconstruction error, for all k (Theorem 15.2),
and for k ≥k∗with a suitable choice of k∗(Corollary 15.2). In particular,
recalling that dR(Sρ, ·) = dα,p(Sρ, ·)2 with α = 1/2 and p = 2, and choosing
a value of k ≥k∗
n that minimizes the bound of Theorem 15.2, we obtain the
following result.

344
Regularization, Optimization, Kernels, and Support Vector Machines
Corollary 15.2 (Performance of PCA/Reconstruction error). Let C have
eigenvalue decay rate of order r, and ˆS∗
n be as in Corollary 15.1. Then it
holds, with probability 1 −δ,
dR(Sρ, ˆS∗
n) = O
 log n −log δ
n
1−1/r !
.
15.2.6
Comparison with Previous Results in the Literature
Figure 15.1 shows a comparison of our learning rates with existing rates in
the literature [7, 46]. The plot shows the polynomial decay rate c of the high
probability bound dR(Sρ, ˆSk
n) = O(n−c), as a function of the eigenvalue decay
rate r of the covariance C, computed at the best value k∗
n (which minimizes
the bound).
FIGURE 15.1: Known upper bounds for the polynomial decay rate c (for
the best choice of k), for the expected distance from a random sample to the
empirical k-truncated kernel-PCA estimate, as a function of the covariance
eigenvalue decay rate (higher is better). Our bound consistently outperforms
previous ones [46]. The top (dashed) line [7], has signiﬁcantly stronger as-
sumptions, and is only included for completeness.
The learning rate exponent c, under a polynomial eigenvalue decay as-
sumption of the data covariance C, is c =
s(r−1)
r−s+sr for [7] and c =
r−1
2r−1 for
[46], where s is related to the fourth moment. Note that, among the two that
operate under the same assumptions, our bound is the best by a wide mar-
gin. The top, best performing, dashed line [7] is obtained for the best possible
fourth-order moment constraint s = 2r, and is therefore not a fair comparison.
However, it is worth noting that our bounds perform almost as well as the

Learning Sets and Subspaces
345
most restrictive one, even when we do not include any fourth-order moment
constraints.
Choice of truncation parameter k. Since, as pointed out in Section 15.2.2,
the subspace estimates ˆSk
n are nested for increasing k (i.e., ˆSk
n ⊆ˆSk′
n
for
k < k′), the distance dα,p(Sρ, ˆSk
n), and in particular the reconstruction error
dR(Sρ, ˆSk
n), is a non-increasing function of k. As discussed [7], this suggests
that there is no bias-variance trade-oﬀin the choice of k. Indeed, the fact that
the estimates ˆSk
n become increasingly close to Sρ as k increases indicates that,
when minimizing dα,p(Sρ, ˆSk
n), the best choice is simply k = n.
Interestingly, however, both in practice (Section 15.4), and in theory (Sec-
tion 15.2.4), we observe that a typical behavior for the subspace learning
problem in high dimensions (e.g., kernel PCA) is that there is a certain value
of k = k∗
n, past which performance plateaus. For problems such as spectral
embedding methods [51, 18, 58], in which a degree of dimensionality reduc-
tion is desirable, producing an estimate ˆSk
n where k is close to the plateau
threshold may be a natural parameter choice: it leads to an estimate of the
lowest dimension (k = k∗
n), whose distance to the true Sρ is almost as low as
the best-performing one (k = n).
15.3
Set Learning
The problem of set, or support estimation has received a great deal of
attention in the statistics community since the 1960s [36, 21], and since then
a number of practical approaches have been proposed to address it [17, 27,
20, 11, 53, 43, 13, 35, 49, 55, 45, 6, 12]. Support estimation is often considered
in machine learning in situations in which it is diﬃcult to gather negative
examples (as often happens in biological and biomedical problems) or when the
negative class is not well deﬁned (as in object detection problems in computer
vision), as is the case in one class estimation [43], and novelty and anomaly
detection [31, 9].
In this section, we describe an approach that is based on reducing the set
learning problem to that of learning a subspace. The results in this section
largely draw from [40, 14, 41].
15.3.1
Set Learning via Subspace Learning
We begin by recalling how support estimation can be reduced to sub-
space learning, and discuss how our results specialize to this setting. From
an algorithmic perspective, the approach we discuss is closely related to
the one in [25] and has been successfully applied in several practical do-
mains [38, 28, 54, 24, 30, 10, 47].

346
Regularization, Optimization, Kernels, and Support Vector Machines
Central to the connection between set and subspace learning is the notion
of separating kernel and separating feature map, which was introduced in [15].
Let K be a reproducing kernel on some space Z, and (φ, F) an associated
feature map and feature space pair (see Section 15.2.5). For simplicity, we
assume that ∥φ(z)∥F = 1 for all z ∈Z. This assumption is without loss
of generality because a kernel with non-zeros in its diagonal can always be
normalized. Given a non-empty set C ⊆Z, let FC = span{φ(z) | z ∈C} be
the closure of all ﬁnite linear combinations of points in the range φ(C) of C.
The distance from any given point φ(z), with z ∈Z, to the linear subspace
FC is
dFC(φ(z)) := inf
f∈FC ∥φ(z) −f∥F .
The following, key deﬁnition is equivalent to the separating property in
Deﬁnition 1 of [56].
Deﬁnition 15.1. We say that a feature map φ (and hence the corresponding
kernel) separates a set C ⊂Z if for all z ∈Z it holds:
dFC(φ(z)) = 0
iﬀ
z ∈C.
An example of separating kernel for Rd is the exponential kernel K(x, x′) =
e−∥x−x′∥. The proof of this fact, see [14], crucially depends on the fact that
for each compact subset of Rd the associated reproducing kernel Hilbert space
contains functions that are zero on the set and non-zero outside. Interest-
ingly, the Gaussian kernel is not separating, because the associated Hilbert
space contains only analytic functions, and the only function that is zero on
a compact subset (with non-empty interior) is the zero function.
The separating property has a clear geometric interpretation in the feature
space: the set φ(Sρ) is the intersection of the closed subspace FSρ (the smallest
linear subspace containing φ(Sρ)), and φ(Z) (see Figure 15.2).
Using the notion of separating kernel, the support Sρ can be characterized
in terms of the subspace Fρ = span φ(Sρ) ⊆F. More precisely, it can be
shown (see the next subsection) that, if the feature map φ separates Sρ, then
it is
Sρ = {z ∈Z | dFρ(φ(z)) = 0}.
The above discussion naturally leads to an empirical estimate ˆSn = {z ∈
Z | dFn(φ(z)) ≤τ} of Sρ, where ˆFn = span φ(Zn), and τ > 0. Given a training
set z1, . . . , zn, the estimator ˆSn is therefore the set of points z ∈Z whose as-
sociated distance from φ(z) to the linear space spanned by {φ(z1), . . . , φ(zn)}
is suﬃciently small, according to some tolerance τ. Any point with distance
greater than τ will be considered to be outside of the support by this estimator.
With the above choice of estimator, it can be shown that almost sure
convergence limn→∞dH(Sρ, ˆSn) = 0 in the Hausdorﬀdistance [1] is related to
the convergence of ˆFn to Fρ [15]. More precisely, if the eigenfunctions of the
covariance operator C = Ez∼ρ [φ(z) ⊗φ(z)] are uniformly bounded, then it

Learning Sets and Subspaces
347
Ψ(X)
Ψ(x)
FIGURE 15.2: The input space Z and the support Sρ are mapped into the
feature space F by the feature map φ. Letting Fρ := FSρ be the smallest
linear subspace containing φ(Sρ) then, if the kernel is separable, the image of
the support φ(Sρ) is given by the intersection between φ(Z) and Fρ. By the
separating property, a point z belongs to the support if and only the distance
between φ(z) and Fρ is zero.
suﬃces for Hausdorﬀconvergence to bound from above d r−1
2r ,∞(where r > 1
is the eigenvalue decay rate of C) as shown in Section 15.3.2.
15.3.2
Consistency Results
Before proving the consistency of the set estimator ˆSn we show an improved
learning rate for the associated subspace ˆFn. In particular we study the linear
subspace ˆF∗
n that is the one spanned by the ﬁrst k components of the empirical
covariance matrix ˆCn, where k ≥k∗
n (see Subsection 15.2.5 and Theorem 15.2).
Note that ˆF∗
n = ˆFn when k = n. The following result specializes Corollary 15.1
to this setting.
Corollary 15.3 (Performance of KPCA with the set learning metric). If
0 ≤α ≤1
2, then it holds, with probability 1 −δ,
dα,∞(Fρ, ˆF∗
n) = O
log n −log δ
n
α
where the constant in the Landau symbol does not depend on δ.
Letting α = r−1
2r above yields a high probability bound of order O

n−r−1
2r

(up to logarithmic factors), which is considerably sharper than the bound
O

n−
r−1
2(3r−1)

found in [16] (Theorem 7). Note that these are upper bounds for

348
Regularization, Optimization, Kernels, and Support Vector Machines
the best possible choice of k (which minimizes the bound). While the optima
of both bounds vanish with n →∞, their behavior is qualitatively diﬀerent.
In particular, the bound of [16] is U-shaped, and diverges for k = n, while
ours is L-shaped (no trade-oﬀ), and thus also convergent for k = n. Therefore,
when compared with [16], our results suggest that no regularization is required
from a statistical point of view though, as discussed in the following, it may
be required for numerical stability. With the above tools at hand, we are now
in a position to prove the consistency of ˆSn.
Theorem 15.3 (Consistency of Set Learning). Let the input space Z be
metrizable, K be a kernel on Z with the separating property [15], let the di-
mension k of the empirical subspace ˆF∗
n, be k∗
n ≤k ≤n, and the threshold
parameter τ = max1≤i≤n d ˆ
Fk
n(φ(zi)), then
ˆS∗
n =
n
z ∈Z
 d ˆ
F∗
n(φ(z)) ≤τ
o
(15.8)
is a universally consistent unsupervised learning algorithm.
Proof. By Theorem 6 of [56] and our Corollary 15.3, the estimator ˆS∗
n satisﬁes
the universal consistency conditions given in Section 15.1, under the given
hypotheses.
FIGURE 15.3: The experimental behavior of the distance dα,∞( ˆSk, Sρ) be-
tween the empirical and the actual support subspaces, with respect to the
regularization parameter. The setting is the one of Section 15.4. Here the ac-
tual subspace is analytically computed, while the empirical one is computed on
a dataset with n = 1000 and 32bit ﬂoating point precision. Note the numerical
instability as k tends to 1000.

Learning Sets and Subspaces
349
We note that the above result is an example of how kernel embedding
techniques can be used to provably estimate geometric invariants of the the
original data distribution. Note that the considered estimator achieves this
without having to explicitly solve a pre-image problem [33].
We end this section noting that, while, as proven in Corollary 15.3, reg-
ularization is not needed from a statistical perspective, it can play a role in
ensuring numerical stability in practice. Indeed, in order to ﬁnd ˆS, we com-
pute d ˆ
Fn(φ(z)) with z ∈Z. Using the reproducing property of K, it can
be shown that, for z ∈Z, it is d ˆ
Fk
n(φ(z)) = K(z, z) −
D
tz, ( ˆKk
n)†tz
E
where
(tz)i = K(z, zi), ˆKn is the Gram matrix ( ˆKn)ij = K(zi, zj), ˆKk
n is the rank-k
approximation of ˆKn, and ( ˆKk
n)† is the pseudo-inverse of ˆKk
n. The computa-
tion of ˆS therefore requires a matrix inversion, which is prone to instability
for high condition numbers. Figure 15.3 shows the behavior of the error that
results from replacing ˆFn by its k-truncated approximation ˆFk
n. For large val-
ues of k, the small eigenvalues of ˆFn are used in the inversion, leading to
numerical instability.
15.4
Numerical Experiments
In order to validate our analysis empirically, we consider the following ex-
periment. Let ρ be a uniform one-dimensional distribution in the unit interval.
We embed ρ into a reproducing-kernel Hilbert space F using the exponential
of the ℓ1 distance (k(u, v) = exp{−∥u −v∥1}) as kernel. Given n samples
drawn from ρ, we compute its empirical covariance in F (whose spectrum is
plotted in Figure 15.4 (top)), and truncate its eigen-decomposition to obtain
a subspace estimate ˆFk
n, as described in Section 15.2.2.
Figure 15.4 (bottom) is a box plot of reconstruction error dR(Fρ, ˆFk
n) as-
sociated with the k-truncated kernel-PCA estimate ˆFk
n (the expected distance
in F of samples to ˆFk
n), with n = 1000 and varying k. While dR is computed
analytically in this example, and Fρ is ﬁxed, the estimate ˆFk
n is a random
variable, and hence the variability in the graph. Notice from the ﬁgure that,
as pointed out in [7] and discussed in Section 15.2.6, the reconstruction error
dR(Fρ, ˆFk
n) is always a non-increasing function of k, due to the fact that the
kernel-PCA estimates are nested: ˆFk
n ⊂ˆFk′
n for k < k′ (see Section 15.2.2).
The graph is highly concentrated around a curve with a steep initial decay,
until reaching some suﬃciently high k, past which the reconstruction (pseudo)
distance becomes stable, and does not vanish. In our experiments, this behav-
ior is typical for the reconstruction distance and high-dimensional problems.
Due to the simple form of this example, we are able to compute analytically
the spectrum of the true covariance C. In this case, the eigenvalues of C decay
as 2γ/((kπ)2 + γ2), with k ∈N, and therefore they have a polynomial decay

350
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 15.4: The spectrum of the empirical covariance (top), and the
expected distance from a random sample to the empirical k-truncated kernel-
PCA subspace estimate (bottom), as a function of k (n = 1000, 1000 trials
shown in a boxplot). Our predicted plateau threshold k∗
n (Theorem 15.2) is a
good estimate of the value k past which the distance stabilizes.
rate r = 2 (see Section 15.2.4). Given the known spectrum decay rate, we
can estimate the plateau threshold k = k∗
n in the bound of Theorem 15.2,
which can be seen to be a good approximation of the observed start of a
plateau in dR(Fρ, ˆFk
n) (Figure 15.4, bottom). Notice that our bound for this
case (Corollary 15.2) similarly predicts a steep error decay until the threshold
k = k∗
n, and a plateau afterwards.

Learning Sets and Subspaces
351
15.5
Sketch of the Proofs
For the sake of completeness we sketch the main step in the proof of
our main theoretical result, Theorem 15.1, with some details omitted in the
interests of conciseness.
For each λ > 0, we denote by rλ(x) := 1{x > λ} the step function with
a cut-oﬀat λ. Given an empirical covariance operator Cn, we will consider
the truncated version rλ(Cn) where, in this notation, rλ is applied to the
eigenvalues of Cn, that is, rλ(Cn) has the same eigen-structure as Cn, but its
eigenvalues that are less than or equal to λ are clamped to zero.
In order to prove the bound of Equation (15.4), we begin by proving a more
general upper bound of dα,p(Sρ, ˆSk
n), which is split into a random (A), and a
deterministic part (B, C). The bound holds for all values of a free parameter
t > 0, which is then constrained and optimized in order to ﬁnd the (close to)
tightest version of the bound.
Lemma 15.1. Let t > 0, 0 ≤α ≤1/2, and λ = σk(C) be the k-th top
eigenvalue of C, it is,
dα,p(Sρ, ˆSk
n)
≤
∥(C + tI)
1
2 (Cn + tI)−1
2 ∥2α
∞
|
{z
}
A
·
(15.9)
· {3/2(λ + t)}α
|
{z
}
B
· ∥Cα(C + tI)−α∥p
|
{z
}
C
.
(15.10)
Note that the right-hand side of Equation (15.9) is the product of three
terms, the left of which (A) involves the empirical covariance operator Cn,
which is a random variable, and the right two (B, C) are entirely deterministic.
While the term B has already been reduced to the known quantities t, α, λ,
the remaining terms are bounded next. We bound the random term A in the
next Lemma, whose proof makes use of recent concentration results [52].
Lemma 15.2 (Term A). Let 0 ≤α ≤1/2, for each
9
n log n
δ ≤t ≤∥C∥∞,
with probability 1 −δ it is
(2/3)α ≤∥(C + tI)
1
2 (Cn + tI)−1
2 ∥2α
∞≤2α
Lemma 15.3 (Term C). Let C be a symmetric, bounded, positive semideﬁnite
linear operator on F. If σk(C) ≤f(k) for k ∈N, where f is a decreasing
function, then, for all t > 0 and α ≥0, it holds that
Cα(C + tI)−α p ≤
inf
0≤u≤1 guαt−uα
(15.11)
where guα =
 f(1)uαp +
R ∞
1
f(x)uαpdx
1/p. Furthermore, if f(k) = gk−1/γ,
with 0 < γ < 1 and αp > γ, then it holds that
Cα(C + tI)−α p ≤Qt−γ/p
(15.12)

352
Regularization, Optimization, Kernels, and Support Vector Machines
where Q = (gγΓ(αp −γ)Γ(1 + γ)/Γ(γ))1/p.
The combination of Lemmas 15.1 and 15.2 leads to the main Theorem 15.1,
which is a probabilistic bound, holding for every k ∈{1, . . . , n}, with a deter-
ministic term ∥Cα(C + tI)−α∥p that depends on knowledge of the covariance
C. In cases in which some knowledge of the decay rate of C is available,
Lemma 15.3 can be applied to obtain Theorem 15.2 and Corollary 15.1. Fi-
nally, Corollary 15.2 is simply a particular case for the reconstruction error
dR(Sρ, ·) = dα,p(Sρ, ·)2, with α = 1/2, p = 2.
As noted in Section 15.2.4, looser bounds would be obtained if classical
Bernstein inequalities in Hilbert spaces [34] were used instead. In particular,
Lemma 15.2 would result in a range for t of qn−r/(r+1) ≤t ≤∥C∥∞, implying
k∗= O(n1/(r+1)) rather than O(n1/r), and thus Theorem 15.2 would become
(for k ≥k∗) dα,p(Sρ, Sk
n) = O(n−αr/(r+1)+1/(p(r+1))) (compared with the
sharper O(n−α+1/rp) of Theorem 15.2). For instance, for p = 2, α = 1/2,
and a decay rate r = 2 (as in the example of Section 15.4), it would be
d1/2,2(Sρ, Sn) = O(n−1/4) using Theorem 15.2, and d1/2,2(Sρ, Sn) = O(n−1/6)
using classical Bernstein inequalities.
15.6
Conclusions
The problem of set learning consists of estimating the smallest subset of
the input space containing the data distribution. In this chapter the problem
has been investigated by analyzing its relations with subspace learning, that
consists of estimating the smallest linear subspace containing the distribution.
In particular we showed that, given a suitable feature map, the set learning
problem can be cast as a subspace learning problem in the associated feature
space. In order to analyze the theoretical properties of the ﬁrst problem, the
statistical analysis for the second has been developed obtaining novel and
sharper sample complexity upper bounds. Finally, by exploiting such results,
the consistency of set learning has been established. The chapter is concluded
by numerical examples that show the eﬀectiveness of our analysis.
Acknowledgments
L. R. acknowledges the ﬁnancial support of the Italian Ministry of Educa-
tion University and Research FIRB project RBFR12M3AC.

Learning Sets and Subspaces
353
Bibliography
[1] G. Beer. Topologies on Closed and Closed Convex Sets. Springer, 1993.
[2] M. Belkin and P. Niyogi.
Laplacian eigenmaps for dimensionality re-
duction and data representation. Neural computation, 15(6):1373–1396,
2003.
[3] Y. Bengio, O. Delalleau, N.L. Roux, J.F. Paiement, P. Vincent, and
M. Ouimet. Learning eigenfunctions links spectral embedding and kernel
pca. Neural Computation, 16(10):2197–2219, 2004.
[4] Y. Bengio, J.-F. Paiement, P. Vincent, O. Delalleau, N. Le Roux, and M.
Ouimet. Out-of-sample extensions for lle, isomap, mds, eigenmaps, and
spectral clustering. Advances in neural information processing systems,
16:177–184, 2004.
[5] S. Bernstein. The Theory of Probabilities. Gastehizdat Publishing House,
Moscow, 1946.
[6] G. Biau, B. Cadre, D. Mason, and B. Pelletier. Asymptotic normality in
density support estimation. Electron. J. Probab., 14:no. 91, 2617–2635,
2009.
[7] G. Blanchard, O. Bousquet, and L. Zwald. Statistical properties of kernel
principal component analysis. Machine Learning, 66(2):259–294, 2007.
[8] I. Borg and P.J.F. Groenen. Modern multidimensional scaling: Theory
and applications. Springer, 2005.
[9] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey.
ACM Computing Surveys (CSUR), 41(3):15, 2009.
[10] P. Cheng, W. Li, and P. Ogunbona. Kernel pca of hog features for pos-
ture detection.
In Image and Vision Computing New Zealand, 2009.
IVCNZ’09. 24th International Conference, pages 415–420. IEEE, 2009.
[11] A. Cuevas and R. Fraiman. A plug-in approach to support estimation.
Ann. Statist., 25(6):2300–2312, 1997.
[12] A. Cuevas and R. Fraiman.
Set estimation.
In New perspectives in
stochastic geometry, pages 374–397. Oxford Univ. Press, Oxford, 2010.
[13] A. Cuevas and A. Rodríguez-Casal. Set estimation: an overview and some
recent developments. In Recent advances and trends in nonparametric
statistics, pages 251–264. Elsevier, Amsterdam, 2003.

354
Regularization, Optimization, Kernels, and Support Vector Machines
[14] E. De Vito, L. Rosasco, and A. Toigo. A universally consistent spec-
tral estimator for the support of a distribution. Applied Computational
Harmonic Analysis, 2014. In press, DOI 10.1016/j.acha.2013.11.003.
[15] E. De Vito, L. Rosasco, and A. Toigo. Spectral regularization for support
estimation. Advances in Neural Information Processing Systems, NIPS
Foundation, pages 1–9, 2010.
[16] E. De Vito, L. Rosasco, and A. Toigo. Learning sets with separating
kernels. arXiv:1204.3573, 2012.
[17] L. Devroye and G.L. Wise. Detection of abnormal behavior via nonpara-
metric estimation of the support. SIAM J. Appl. Math., 38(3):480–488,
1980.
[18] D.L. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embed-
ding techniques for high-dimensional data. Proceedings of the National
Academy of Sciences, 100(10):5591–5596, 2003.
[19] R. Duda, P.E. Hart, and D.G. Stork. Pattern classiﬁcation. John Wiley
& Sons, 2012.
[20] L. Dümbgen and G. Walther. Rates of convergence for random approxi-
mations of convex sets. Adv. in Appl. Probab., 28(2):384–393, 1996.
[21] J. Geﬀroy. Sur un probleme d’estimation géométrique. Publ. Inst. Statist.
Univ. Paris, 13:191–210, 1964.
[22] J. Ham, D.D. Lee, S. Mika, and B. Schölkopf.
A kernel view of the
dimensionality reduction of manifolds. In Proceedings of the Twenty-First
International Conference on Machine Learning, page 47. ACM, 2004.
[23] T. Hastie, R. Tibshirani, J. Friedman, and J. Franklin. The elements of
statistical learning: data mining, inference and prediction. The Mathe-
matical Intelligencer, 27(2):83–85, 2005.
[24] F. He, J.H. Yang, M. Li, and J.W. Xu. Research on nonlinear process
monitoring and fault diagnosis based on kernel principal component anal-
ysis. Key Engineering Materials, 413:583–590, 2009.
[25] H. Hoﬀmann.
Kernel pca for novelty detection.
Pattern Recognition,
40(3):863–874, 2007.
[26] I. Jolliﬀe. Principal component analysis. Wiley Online Library, 2005.
[27] A.P. Korostelëv and A.B. Tsybakov.
Minimax theory of image recon-
struction. Springer-Verlag, New York, 1993.
[28] H.-J. Lee, S. Cho, and M.-S. Shin. Supporting diagnosis of attention-
deﬁcit hyperactive disorder with novelty detection. Artiﬁcial intelligence
in medicine, 42(3):199–212, 2008.

Learning Sets and Subspaces
355
[29] M. Loève. Probability theory, volume 45. Springer, 1963.
[30] M.L. Maestri, M.C. Cassanello, and G.I. Horowitz. Kernel pca perfor-
mance in processes with multiple operation modes. Chemical Product
and Process Modeling, 4(5), 2009.
[31] M. Markou and S. Singh. Novelty detection: a review–part 1: statistical
approaches. Signal Processing, 83(12):2481–2497, 2003.
[32] A. Maurer and M. Pontil.
K–dimensional coding schemes in hilbert
spaces. IEEE Transactions on Information Theory, 56(11):5839–5846,
2010.
[33] S. Mika, B. Schölkopf, A.J. Smola, K.-R. Müller, M. Scholz, and G.
Rätsch. Kernel pca and de-noising in feature spaces. In NIPS, volume 11,
pages 536–542, 1998.
[34] I. Pinelis. Optimum bounds for the distributions of martingales in banach
spaces. The Annals of Probability, pages 1679–1706, 1994.
[35] M. Reitzner. Random polytopes and the Efron-Stein jackknife inequality.
Ann. Probab., 31(4):2136–2166, 2003.
[36] A. Rényi and R. Sulanke. Über die konvexe hülle von n zufällig gewählten
punkten. Probability Theory and Related Fields, 2(1):75–84, 1963.
[37] J.R. Retherford. Hilbert Space: Compact Operators and the Trace Theo-
rem. London Mathematical Society Student Texts. Cambridge University
Press, 1993.
[38] B. Ristic, B. La Scala, M. Morelande, and N. Gordon. Statistical analysis
of motion patterns in ais data: Anomaly detection and motion prediction.
In Information Fusion, 2008 11th International Conference on, pages 1–7.
IEEE, 2008.
[39] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally
linear embedding. Science, 290(5500):2323–2326, 2000.
[40] A. Rudi, G.D. Canas, and L. Rosasco. On the sample complexity of sub-
space learning. In Advances in Neural Information Processing Systems,
pages 2067–2075, 2013.
[41] A. Rudi, F. Odone, and E. De Vito.
Geometrical and computational
aspects of spectral support estimation for novelty detection.
Pattern
Recognition Letters, 36:107–116, 2014.
[42] L.K. Saul and S.T. Roweis. Think globally, ﬁt locally: unsupervised learn-
ing of low dimensional manifolds. The Journal of Machine Learning Re-
search, 4:119–155, 2003.

356
Regularization, Optimization, Kernels, and Support Vector Machines
[43] B. Schölkopf, J. Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.
Estimating the support of a high-dimensional distribution. Neural Com-
put., 13(7):1443–1471, 2001.
[44] B. Schölkopf, A. Smola, and K.R. Müller. Kernel principal component
analysis. Artiﬁcial Neural Networks–ICANN’97, pages 583–588, 1997.
[45] C.D. Scott and R.D. Nowak. Learning minimum volume sets. J. Mach.
Learn. Res., 7:665–704, 2006.
[46] J. Shawe-Taylor, C.K. Williams, N. Cristianini, and J. Kandola. On the
eigenspectrum of the gram matrix and the generalization error of kernel-
pca. Information Theory, IEEE Transactions on, 51(7), 2005.
[47] B. Sofman, J.A. Bagnell, and A. Stentz. Anytime online novelty detection
for vehicle safeguarding. In Robotics and Automation (ICRA), 2010 IEEE
International Conference on, pages 1247–1254. IEEE, 2010.
[48] I. Steinwart and A. Christmann. Support vector machines. Information
science and statistics. Springer-Verlag. New York, 2008.
[49] I. Steinwart, D. Hush, and C. Scovel.
A classiﬁcation framework for
anomaly detection. J. Mach. Learn. Res., 6:211–232 (electronic), 2005.
[50] J. Sun, S. Boyd, L. Xiao, and P. Diaconis. The fastest mixing Markov
process on a graph and a connection to a maximum variance unfolding
problem. SIAM review, 48(4):681–699, 2006.
[51] J.B. Tenenbaum, V. De Silva, and J.C. Langford.
A global geo-
metric framework for nonlinear dimensionality reduction.
Science,
290(5500):2319–2323, 2000.
[52] J.A. Tropp. User-friendly tools for random matrices: An introduction.
2012.
[53] A.B. Tsybakov. On nonparametric estimation of density level sets. Ann.
Statist., 25(3):948–969, 1997.
[54] F.J. Valero-Cuevas, H. Hoﬀmann, M.U. Kurse, J.J. Kutch, and E.A.
Theodorou. Computational models for neuromuscular function. Biomed-
ical Engineering, IEEE Reviews in, 2:110–135, 2009.
[55] R. Vert and J.-P. Vert. Consistency and convergence rates of one-class
svms and related algorithms.
Journal of Machine Learning Research,
7:817–854, 2006.
[56] E. De Vito, L. Rosasco, and A. Toigo. Learning sets with separating
kernels. Applied and Computational Harmonic Analysis, 2013.

Learning Sets and Subspaces
357
[57] K.Q. Weinberger and L.K. Saul. Unsupervised learning of image man-
ifolds by semideﬁnite programming.
In Computer Vision and Pattern
Recognition, 2004. CVPR 2004, volume 2, pages II–988. IEEE, 2004.
[58] K.Q. Weinberger and L.K. Saul. Unsupervised learning of image mani-
folds by semideﬁnite programming. International Journal of Computer
Vision, 70(1):77–90, 2006.
[59] C.K.I. Williams. On a connection between kernel pca and metric multi-
dimensional scaling. Machine Learning, 46(1):11–19, 2002.

This page intentionally left blank
This page intentionally left blank

Chapter 16
Output Kernel Learning Methods
Francesco Dinuzzo∗
IBM Research
Cheng Soon Ong∗
NICTA
Kenji Fukumizu∗
The Institute of Statistical Mathematics
16.1
Learning Multi-Task Kernels ....................................
360
16.1.1
Multiple Kernel Learning ................................
361
16.1.2
Output Kernel Learning .................................
362
16.1.2.1
Frobenius Norm Output Kernel Learning .
362
16.1.2.2
Low-Rank Output Kernel Learning .......
363
16.1.2.3
Sparse Output Kernel Learning ...........
364
16.2
Applications ......................................................
364
16.2.1
Collaborative Filtering and Preference Estimation .....
364
16.2.2
Structure Discovery in Multiclass Classiﬁcation ........
365
16.2.3
Pharmacological Problems ..............................
366
16.3
Concluding Remarks and Future Directions .....................
368
Bibliography ......................................................
368
Simultaneously solving multiple related estimation tasks is a problem known
as multi-task learning in the machine learning literature. A rather ﬂexible
approach to multi-task learning consists of solving a regularization problem
where a positive semideﬁnite multi-task kernel is used to model joint relation-
ships between both inputs and tasks. Specifying an appropriate multi-task
kernel in advance is not always possible, therefore it is often desirable to esti-
mate one from the data. In this chapter, we overview a family of regularization
techniques called output kernel learning (OKL), for learning a multi-task ker-
nel that can be decomposed as the product of a kernel on the inputs and one
on the task indices. The kernel on the task indices is optimized simultane-
ously with the predictive function by solving a joint two-level regularization
problem.
∗francesd@ie.ibm.com, cheng-soon.ong@nicta.com.au, fukumizu@ism.ac.jp
359

360
Regularization, Optimization, Kernels, and Support Vector Machines
16.1
Learning Multi-Task Kernels
Supervised multi-task learning consists of estimating multiple functions
fj : Xj →Y from multiple datasets of input-output pairs
(xij, yij) ∈Xj × Y,
j = 1, . . . , m,
i = 1, . . . , ℓj,
where m is the number of tasks and ℓj is the number of data pairs for the j-th
task. In general, the input sets Xj and the output set Y can be arbitrary non-
empty sets. If the input sets Xj are the same for all the tasks, i.e., Xj = X, and
the power set Ym can be given a vector space structure, one can equivalently
think in terms of learning a single vector-valued function f : X →Ym from
a dataset of pairs with incomplete output data. The key point in multi-task
learning is to exploit relationships between the diﬀerent components fj in
order to improve performance with respect to solving each supervised learning
problem independently.
For a broad class of multi-task (or multi-output) learning problems, a suit-
able positive semideﬁnite multi-task kernel can be used to specify the joint re-
lationships between inputs and tasks [5]. The most general way to address this
problem is to specify a similarity function of the form K((x1, i), (x2, j)) deﬁned
for every pair of input data (x1, x2) and every pair of task indices (i, j). In the
context of a kernel-based regularization method, choosing a multi-task kernel
amounts to designing a suitable reproducing kernel Hilbert space (RKHS) of
vector-valued functions, over which the function f whose components are the
diﬀerent tasks fj is searched. See [13] for details about the theory of RKHS
of vector valued-functions.
Predictive performances of kernel-based regularization methods are highly
inﬂuenced by the choice of the kernel function. Such inﬂuence is especially
evident in the case of multi-task learning where, in addition to specifying input
similarities, it is crucial to correctly model inter-task relationships. Designing
the kernel allows us to incorporate domain knowledge by properly constraining
the function class over which the solution is searched. Unfortunately, in many
problems the available knowledge is not suﬃcient to uniquely determine a good
kernel in advance, making it highly desirable to have data-driven automatic
selection tools. This need has motivated a fruitful research stream that has
led to the development of a variety of techniques for learning the kernel.
There is considerable ﬂexibility in choosing the similarity function K, the
only constraint being positive semideﬁniteness of the resulting kernel. How-
ever, such ﬂexibility may also be a problem in practice, since choosing a good
multi-task kernel for a given problem may be diﬃcult. A very common way
to simplify such modeling is to utilize a multiplicative decomposition of the
form
K((x1, i), (x2, j)) = KX(x1, x2)KY (i, j),
where the input kernel KX is decoupled from the output kernel KY . The same

Output Kernel Learning Methods
361
structure can be equivalently represented in terms of a matrix-valued kernel
H(x1, x2) = KX(x1, x2) · L,
(16.1)
where L is a positive semideﬁnite matrix with entries Lij = KY (i, j). Since
specifying the kernel function KY is completely equivalent to specifying the
matrix L, we will use the term output kernel to denote both of them, with a
slight abuse of terminology.
Even after imposing such a simpliﬁed model, specifying the inter-task sim-
ilarities in advance is typically impractical. Indeed, it is often the case that
multiple learning tasks are known to be related, but no precise information
about the structure or the intensity of such relationships is available. Simply
ﬁxing L to the identity, which amounts to sharing no information between
the tasks, is clearly suboptimal in most of the cases. On the other hand,
wrongly specifying the entries may lead to a severe performance degradation.
It is therefore clear that, whenever the task relationships are subject to un-
certainty, learning them from the data is the only meaningful way to proceed.
16.1.1
Multiple Kernel Learning
The most studied approach to automatic kernel selection, known as multi-
ple kernel learning (MKL), consists of learning a conic combination of N basis
kernels of the form
K =
N
X
k=1
dkKk,
dk ≥0,
k = 1, . . . , N.
Appealing properties of MKL methods include the ability to perform selection
of a subset of kernels via sparsity, and tractability of the associated optimiza-
tion problem, typically (re)formulated as a convex program. Although most
of the works on MKL focus on learning similarity measured between inputs,
clearly the approach can also be used to learn a multi-task kernel of the form
K((x1, i), (x2, j)) =
N
X
k=1
dkKk
X(x1, x2)Kk
Y (i, j),
which includes the possibility of optimizing the matrix L in (16.1) as a conic
combination of basis matrices, by simply choosing the input kernels Kk
X to
be equal. In principle, proper complexity control allows us to combine an
arbitrarily large, even inﬁnite [1], number of kernels. However, computational
and memory constraints force the user to specify a relatively small dictionary
of basis kernels to be combined, which again calls for a certain amount of
domain knowledge. Examples of works that employ an MKL approach to
address multi-output or multi-task learning problems include [17, 11, 16].

362
Regularization, Optimization, Kernels, and Support Vector Machines
16.1.2
Output Kernel Learning
A more direct approach to learning inter-task similarities from the data
consists in searching the output kernel KY over the whole cone of positive
semideﬁnite kernels, by optimizing a suitable objective functional. Equiva-
lently, the corresponding matrix L can be searched over the cone of positive
semideﬁnite matrices.
This can be accomplished by solving a two-level regularization problem of
the form
min
L∈S+ min
f∈HL


m
X
j=1
ℓj
X
i=1
V (yij, fj(xij)) + λ
 ∥f∥2
HL + Ω(L)


,
(16.2)
where (xij, yij) are input-output data pairs for the j-th task, V is a suitable
loss function, HL is the RKHS of vector-valued functions associated with the
reproducing kernel (16.1), Ωis a suitable matrix regularizer, and S+ is the cone
of symmetric and positive semideﬁnite matrices. The regularization parameter
λ > 0 should be properly selected in order to achieve a good trade-oﬀbetween
approximation of the training data and regularization. This can be achieved
by hold-out validation, cross-validation, or other methods. We call such an
approach output kernel learning (OKL). By virtue of a suitable representer
theorem [13], the inner regularization problem in (16.2) can be shown to admit
solutions of the form
ˆfk(x) =
m
X
j=1
Lkj


ℓj
X
i=1
cijKX(xij, x)

,
(16.3)
under mild hypothesis on V .
From the expression (16.3), we can clearly see that when L equals the iden-
tity, the external sum decouples and each optimal function ˆfk only depends
on the corresponding dataset (independent single task-learning). On the other
hand, when all the entries of the matrix L are equal, all the functions ˆfk are
the same (pooled single task-learning). Finally, whenever L diﬀers from the
identity, the datasets from multiple tasks get mixed together and contribute
to the estimates of other tasks.
16.1.2.1
Frobenius Norm Output Kernel Learning
A ﬁrst OKL technique was introduced in [4] for the case where V is a
square loss function, Ωis the squared Frobenius norm, and the input data xij
is the same for all the output components fj, leading to a problem of the form
min
L∈S+ min
f∈HL
 ℓ
X
i=1
(yi −fj(xi))2 + λ
 ∥f∥2
HL + ∥L∥2
F

!
.
(16.4)
Such special structure of the objective functional allows us to develop an ef-
fective block coordinate descent strategy where each step involves the solution

Output Kernel Learning Methods
363
of a Sylvester linear matrix equation. A simple and eﬀective computational
scheme to solve (16.4) is described in [4]. Regularizing with the squared Frobe-
nius norm ensures that the sub-problem with respect to L is well-posed. How-
ever, one may want to encourage diﬀerent types of structures for the output
kernel matrix, depending on the application.
16.1.2.2
Low-Rank Output Kernel Learning
When the output kernel is low-rank, the estimated vector-valued function
maps into a low-dimensional subspace. Encouraging such low-rank structure is
of interest in several problems. Along this line, [3, 2] introduce low-rank OKL,
a method to discover relevant low dimensional subspaces of the output space
by learning a low-rank kernel matrix. This method corresponds to regulariz-
ing the output kernel with a combination of the trace and a rank indicator
function, namely
Ω(L) = tr(L) + I(rank(L) ≤p).
For p = m, the hard-rank constraint disappears and Ωreduces to the trace,
which still encourages low-rank solutions. Setting p < m gives up convexity
of the regularizer but, on the other hand, allows to set a hard bound on the
rank of the output kernel, which can be useful for both computational and
interpretative reasons. The optimization problem associated with low-rank
OKL is the following:
min
L∈S+ min
f∈HL


m
X
j=1
ℓj
X
i=1
(yij −fj(xij))2 + λ
 ∥f∥2
HL + tr(L)


, s.t. rank(L) ≤p.
(16.5)
The optimal output kernel matrix can be factorized as L = BBT , where the
horizontal dimension of B is equal to the rank parameter p. Problem (16.5)
exhibits several interesting properties and interpretations. Just as sparse MKL
with a square loss can be seen as a nonlinear generalization of (grouped)
Lasso, low-rank OKL is a natural kernel-based generalization of reduced-rank
regression, a popular multivariate technique in statistics [9]. When p = m
and the input kernel is linear, low-rank OKL reduces to multiple least squares
regression with nuclear norm regularization. Connections with reduced-rank
regression and nuclear norm regularization are analyzed in [3].
For problems where the inputs xij are the same for all the tasks, opti-
mization for low-rank OKL can be performed by means of a rather eﬀective
procedure that iteratively computes eigendecompositions; see Algorithm 1 in
[3]. Importantly, the size of the involved matrices such as B, the low rank
factor of L, can be controlled by selecting the parameter p. However, more
general multi-task learning problems where each task is sampled in correspon-
dence with diﬀerent inputs require completely diﬀerent methods. It turns out
that an eﬀective strategy to approach the problem consists of iteratively ap-
plying inexact preconditioned conjugate gradient (PCG) solvers to suitable

364
Regularization, Optimization, Kernels, and Support Vector Machines
linear operator equations that arise from the optimality conditions. Such lin-
ear operator equations are derived and analyzed in [2].
16.1.2.3
Sparse Output Kernel Learning
In many multitask learning problems it is known that some of the tasks
might be related while some others are independent, but it is unknown in
advance which of the tasks are related. In such cases, it may make sense to
try and encourage sparsity in the output kernel by means of suitable reg-
ularization. For instance, by choosing an entry-wise ℓ1 norm regularization
Ω(L) = ∥L∥1, one obtains the problem
min
L∈S+ min
f∈HL


m
X
j=1
ℓj
X
i=1
(yij −fj(xij))2 + λ
 ∥f∥2
HL + ∥L∥1


.
Encouraging a sparse output kernel may allow us to automatically discover
clusters of related tasks. However, some of the tasks may already be known in
advance to be unrelated. Such information can be encoded by also enforcing
a hard constraint on the entries of the output kernel, for instance by means of
the regularizer Ω(L) = ∥L∥1 + I(PS(L) = 0), where I is a indicator function,
and PS selects a subset S of the non-diagonal entries of L and projects them
into a vector, yielding the additional constraint
Lij = 0,
∀(i, j) ∈S.
The subproblem with respect to L is a convex nondiﬀerentiable problem, also
when hard sparsity constraints are present. Eﬀective solvers for sparse output
kernel learning problems are currently under investigation.
16.2
Applications
Multi-task learning problems where it is important to estimate the rela-
tionships between tasks are ubiquitous. In this section, we provide examples
of such problems where OKL techniques have been applied successfully.
16.2.1
Collaborative Filtering and Preference Estimation
Estimating preferences of several users for a set of items is a typical in-
stance of multi-task learning problem where each task is the preference func-
tion of one of the users, and exploiting similarities between the tasks matters.
Preference estimation is a key problem addressed by collaborative ﬁltering
systems and recommender systems, which ﬁnd wide applicability on the Web.

Output Kernel Learning Methods
365
In the context of collaborative ﬁltering, techniques such as low-rank matrix
approximation are considered state of the art. In the following, we present
some results from a study based on the MovieLens datasets (see Table 16.1),
three popular collaborative ﬁltering benchmarks containing collections of rat-
ings in the range {1, . . . , 5} assigned by several users to a set of movies; for
more details, see [2]. The study shows that, by exploiting additional informa-
tion about the inputs (movies), OKL techniques are superior to plain low-rank
matrix approximation.
TABLE 16.1: MovieLens datasets: total number of users, movies, and rat-
ings.
Dataset
Users
Movies
Ratings
MovieLens100K
943
1682
105
MovieLens1M
6040
3706
106
MovieLens10M
69878
10677
107
The results reported in Table 16.2 correspond to a setup where a random
test set is extracted, containing about the 50% of the ratings for each user, see
also [15, 10]. Results under diﬀerent test settings are also available, see [2]. The
25% of the remaining training data are used as a validation set to tune the reg-
ularization parameter. Performance is evaluated according to the root mean
squared error (RMSE) on the test set. Regularized matrix factorization (RMF)
corresponds to choosing the input kernel equal to KX(x1, x2) = δK(x1, x2),
where δK denotes the Kronecker delta (non-zero only when the two argu-
ments are equal), so that no information other than the movie Id is exploited
to express the similarity between the movies. The pooled and independent
baselines correspond to choosing Lij = 1 and Lij = δK(i, j), respectively. The
last method employed is low-rank OKL with rank parameter p = 5 ﬁxed a
priori for all three datasets, and input kernel designed as
K(x1, x2) = δK(xid
1 , xid
2 ) + exp (−dH(xg
1, xg
2)) ,
by taking into account movie Ids xid
1 , xid
2 and meta-data about genre catego-
rization of the movies xg
1, xg
2 available in all three datasets.
16.2.2
Structure Discovery in Multiclass Classiﬁcation
Multi-class classiﬁcation problems can also be seen as particular instances
of multi-task learning where each real-valued task function fj corresponds to
a score for a given class. The training labels can be converted into sparse real
vectors of length equal to the number of classes, with only one component
diﬀerent from zero. Employing an OKL method in this context allows not
only training a multi-class classiﬁer, but also learning the similarities between
the classes.

366
Regularization, Optimization, Kernels, and Support Vector Machines
TABLE 16.2: MovieLens datasets: test RMSE for low-rank OKL, RMF,
pooled and independent single-task learning.
Dataset
RMF
Pooled
Independent
OKL
MovieLens100K
1.0300
1.0209
1.0445
0.9557
MovieLens1M
0.9023
0.9811
1.0297
0.8945
MovieLens10M
0.8627
0.9441
0.9721
0.8501
FIGURE 16.1: Caltech 256: learned similarities between classes. Only a
subset of the classes is shown.
As an example, Figure 16.1 shows a visualization of the entries of output
kernel matrix obtained by applying low-rank OKL to the popular Caltech 256
dataset [6, 7], containing images of several diﬀerent categories of objects, in-
cluding buildings, animals, tools, etc. By using 30 training examples for each
class, the obtained classiﬁcation accuracy on the test set (0.44) is close to
state-of-the art results. At the same time, the graph obtained by threshold-
ing the entries of the learned output kernel matrix with low absolute value,
reveals clusters of classes that are meaningful and agree with common sense.
Output kernel learning methods have also been applied in [8] to solve object
recognition problems.
16.2.3
Pharmacological Problems
Multi-task learning problems are common in pharmacology, where data
from multiple subjects are available. Due to the scarcity of data for each
subject, it is often crucial to combine the information from diﬀerent datasets
in order to obtain a good estimation performance. Such combination needs
to take into account the similarities between the subjects, while allowing for
enough ﬂexibility to estimate personalized models for each of them. Output

Output Kernel Learning Methods
367
10
12
14
16
18
20
Pooled
Independent
Matrix Factorization
OKL
RMSE
RMSE on test data (100 splits)
FIGURE 16.2: Experiment on pharmacokinetic data [2]. Root mean squared
error averaged over 100 random splits for the 27 subject proﬁles in correspon-
dence with diﬀerent methods.
kernel learning methods have been successfully applied to pharmacological
problems in [2], where two diﬀerent problems are analyzed. Both problems
can be seen as multi-task regression problems or matrix completion problems
with side information.
The ﬁrst problem consists of ﬁlling a matrix of drug concentration mea-
surements for 27 subjects in correspondence with 8 diﬀerent time instants after
the drug administration, by having access to only 3 measurements per sub-
ject. Standard low-rank matrix completion techniques are not able to solve
this problem satisfactorily, since they ignore the available knowledge about
the temporal shape of the concentration curves. On the other hand, an OKL
method allows us to easily incorporate such knowledge by designing a suit-
able input kernel that takes into account temporal correlation, as done in [2].
Figure 16.2 reports boxplots over the 27 subjects of the root mean squared er-
ror, averaged over 100 random selections of the three training measurements,
showing a clear advantage of the OKL methodology with respect to both
pooled and independent baselines, as well as a low-rank matrix completion
technique that does not use side information.
The second problem analyzed in [2] has to do with completing a matrix
of hamilton depression rating scale (HAMD) scores for 494 subjects in corre-
spondence with 7 subsequent weeks, for which only a subset of 2855 entries
is available [12]. Performance is evaluated by keeping 1012 properly selected
entries for test purposes. In order to automatically select the regularization
parameter λ, a further splitting of the remaining data is performed to ob-
tain a validation set containing about 30% of the examples. Such splitting is

368
Regularization, Optimization, Kernels, and Support Vector Machines
performed randomly and repeated 50 times. By employing a low-rank OKL
approach with a simple linear spline input kernel, one can observe signiﬁ-
cantly better results (Table 16.3) with respect to low-rank matrix completion
and standard baselines; see [2] for further details.
TABLE 16.3: Drug eﬃcacy assessment experiment [2]: best average RMSE
on test data (and their standard deviation over 50 splits)
Pooled
Independent
RMF
OKL
6.86 (0.02)
6.72(0.16)
6.66(0.4)
5.37(0.2)
16.3
Concluding Remarks and Future Directions
Learning output kernels via regularization is an eﬀective way to solve
multi-task learning problems where the relationships between the tasks are
uncertain or unknown. The OKL framework that we have discussed in this
chapter is rather general and can be further developed in various directions.
There are several practically meaningful constraints that could be imposed on
the output kernel: sparsity patterns, hierarchies, groupings, etc. Eﬀective op-
timization techniques for more general (non-quadratic) loss functions are still
lacking and the use of a variety of matrix penalties for the output kernel ma-
trix is yet to be explored. Extensions to semi-supervised and online problems
are needed in order to broaden applicability of these techniques. Finally, some
hybrid methods that combine learning of, possibly multiple, input and output
kernels have been recently investigated [14] and are currently still under active
investigation.
Bibliography
[1] A. Argyriou, C. A. Micchelli, and M. Pontil. Learning convex combi-
nations of continuously parameterized basic kernels. In Peter Auer and
Ron Meir, editors, Learning Theory, volume 3559 of Lecture Notes in
Computer Science, pages 338–352. Springer Berlin/Heidelberg, 2005.
[2] F. Dinuzzo. Learning output kernels for multi-task problems. Neurocom-
puting, 118:119–126, 2013.

Output Kernel Learning Methods
369
[3] F. Dinuzzo and K. Fukumizu. Learning low-rank output kernels. Journal
of Machine Learning Research - Proceedings Track, 20:181–196, 2011.
[4] F. Dinuzzo, C. S. Ong, P. Gehler, and G. Pillonetto. Learning output
kernels with block coordinate descent. In Proceedings of the 28th An-
nual International Conference on Machine Learning, Bellevue, WA, USA,
2011.
[5] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks
with kernel methods. Journal of Machine Learning Research, 6:615–637,
2005.
[6] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models
from few training examples: An incremental Bayesian approach tested on
101 object categories. In CVPR, page 178, 2004.
[7] G. Griﬃn, A. Holub, and P. Perona. Caltech-256 object category dataset.
Technical Report 7694, Caltech, 2007.
[8] Z. Guo and Z. J. Wang.
Cross-domain object recognition by output
kernel learning. In Multimedia Signal Processing (MMSP), 2012 IEEE
14th International Workshop on, pages 372–377. IEEE, 2012.
[9] A. J. Izenman. Reduced-rank regression for the multivariate linear model.
Journal of Multivariate Analysis, 5(2):248–264, 1975.
[10] M. Jaggi and M. Sulovský. A simple algorithm for nuclear norm regu-
larized problems. In J. Fürnkranz and T. Joachims, editors, Proceedings
of the 27th International Conference on Machine Learning (ICML-10),
pages 471–478, Haifa, Israel, June 2010. Omnipress.
[11] H. Kadri, A. Rakotomamonjy, F. Bach, and P. Preux. Multiple operator-
valued kernel learning. In NIPS, pages 2438–2446, 2012.
[12] E. Merlo-Pich and R. Gomeni. Model-based approach and signal detec-
tion theory to evaluate the performance of recruitment centers in clinical
trials with antidepressant drugs. Clinical Pharmacology and Therapeu-
tics, 84:378–384, September 2008.
[13] C. A. Micchelli and M. Pontil. On learning vector-valued functions. Neu-
ral Computation, 17:177–204, 2005.
[14] V. Sindhwani, H.Q. Minh, and A. C. Lozano.
Scalable matrix-valued
kernel learning for high-dimensional nonlinear multivariate regression and
granger causality. In UAI, 2013.
[15] K. Toh and S. Yun.
An accelerated proximal gradient algorithm for
nuclear norm regularized least squares problems. Optimization Online,
2009.

370
Regularization, Optimization, Kernels, and Support Vector Machines
[16] C. Widmer, N. C. Toussaint, Y. Altun, and G. Rätsch. Inferring latent
task structure for multitask learning by multiple kernel learning. BMC
Bioinformatics, 11(Suppl 8):S5, 2010.
[17] A. Zien and C. S. Ong. Multiclass multiple kernel learning. In Proceedings
of the 24th International Conference on Machine Learning, pages 1191–
1198, 2007.

Chapter 17
Kernel-Based Identiﬁcation of
Systems with Multiple Outputs
Using Nuclear Norm Regularization
Tillmann Falck
KU Leuven, ESAT-STADIUS
Bart De Moor
KU Leuven, ESAT-STADIUS and IMinds Medical IT
Johan A.K. Suykens
KU Leuven, ESAT-STADIUS
17.1
Introduction ......................................................
372
17.1.1
Notation .................................................
373
17.2
Problem Description and Motivation ............................
373
17.2.1
Formal Problem Description ............................
374
17.2.2
Traditional vs. Proposed Handling of Multiple Outputs
375
17.2.3
Least Squares Support Vector Machines — A
Primal-Dual Kernel-Based Model .......................
376
17.2.4
Advantages of Primal-Dual Approach ...................
378
17.2.5
Nuclear Norm ............................................
379
17.3
Primal Model for Multiple Outputs .............................
380
17.3.1
Model Formulation ......................................
380
17.3.2
Characterization of Proposed Multiple Output Model ..
381
17.4
Dual Formulation of the Model ..................................
382
17.4.1
Dual Optimization Problem .............................
382
17.4.2
Predictive Model
........................................
385
17.5
Extension to Variable Input Vectors .............................
387
17.6
Numerical Example ..............................................
391
17.6.1
Agreement of Primal and Dual Solutions ...............
392
17.6.2
Cross-Validation Performance ...........................
394
17.6.3
Predictive Performance ..................................
395
17.7
Conclusions .......................................................
396
Acknowledgments ................................................
396
Bibliography ......................................................
396
371

372
Regularization, Optimization, Kernels, and Support Vector Machines
17.1
Introduction
System identiﬁcation is a ﬁeld that deals with modeling physical systems
in terms of their input-output relationship. The goal is to estimate a model
for the system from measurements of past inputs and outputs. Linear sys-
tem identiﬁcation [18] is a well developed ﬁeld that is able to handle diverse
system structures in eﬃcient ways. In many modeling tasks it is straightfor-
ward to incorporate information from multiple input sources; however it is
usually much more diﬃcult to exploit the presence of multiple outputs. In
an engineering context the notion of state-space descriptions popularized by
Kalman [14] provides an intuitive method to characterize systems with multi-
ple inputs and outputs. In a linear setting these can also be readily estimated
from data, using, e.g., subspace identiﬁcation techniques [29].
The class of nonlinear systems is much broader than that of linear systems.
Hence, for the estimation of nonlinear systems a lot of system structures are
still an active domain for research. Most established techniques for nonlin-
ear system identiﬁcation [26] formulate a nonlinear regression problem that
is then solved by a variety of techniques, like wavelets, neural networks, or
more recently, support vector machines and kernel-based methods. A com-
mon limitation for nonlinear regression techniques is that it is diﬃcult to
handle multiple outputs, and this is particularly true for most kernel-based
estimation schemes. A frequently applied workaround is to estimate indepen-
dent models for each individual output. This chapter introduces an advanced
estimation scheme that is able to exploit dependencies between output vari-
ables. The technique is based on least-squares support vector machines [28]
and a convex regularization term based on the nuclear norm [11].
Regularization schemes based on the nuclear norm have been successfully
applied in diﬀerent domains, such as multi-task learning [2], matrix comple-
tion [5, 21], tensor completion [25], and system identiﬁcation [17, 16].
Kernel-based methods are a very popular choice for nonlinear regression
due to their excellent modeling power. Support vector machines and in par-
ticular least-squares support vector machines are an attractive choice for sev-
eral reasons. Firstly their foundation on convex optimization provides eﬃcient
means for their numerical solutions. Also the serious problem of local minima
as occurring in many competing techniques is less pronounced as the main
problems are convex and as such the global optimum can be attained. Sec-
ondly, in line with other kernel-based estimation techniques, support vector
techniques are not as prone to the curse of dimensionality as other nonlinear
estimation techniques. Thirdly, the optimization problem formulation makes
it straightforward to introduce additional structure on the model and thereby
incorporate prior information [27].
This chapter has two key contributions. On the one hand a new kernel-
based model for nonlinear systems with multiple related outputs is proposed.

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
373
On the other hand a new primal-dual derivation of a kernel-based model with
nuclear norm regularization is presented.
The structure is as follows. The ﬁrst section motivates the problem setting
and introduces the necessary background on the employed techniques. The
section concludes with stating the model formulation for a nonlinear system
with multiple related outputs. The next section characterizes some properties
of the proposed model, in particular, uniqueness and the range of suitable
regularization parameters. Section 17.4 derives the dual model. In a ﬁrst step
a kernel-based optimization problem is found and in a second step its solution
is used to obtain a predictive equation. After establishing the primal as well
as the dual formulation of the proposed model, some of its properties are
illustrated using a numerical example. Finally the conclusions are given in the
last section.
17.1.1
Notation
All vectors are column vectors and written as bold lowercase letters. Matri-
ces are also bold but denoted by capital letters. Transposition is marked with
a superscript T. Subscripts are used to denote elements of a set or components
of a vector. It should be obvious from the context which one is applicable.
Furthermore a vector of all ones in dimension N is denoted by 1N and
similarly a vector of all zeros by 0N. The identity matrix of size N is written
as IN.
The eigenvalues of a square matrix X are denoted by 0 ≤λ1(X) ≤
· · · ≤λmax(X). In the same way singular values of matrix X are denoted by
0 ≤σ1(X) ≤· · · ≤σmax(X). Several matrix norms are used, the Frobenius
norm ∥X∥F =
qPN
n=1
PM
m=1 X2nm =
qPmin{N,M}
i=1
σi(X)2 for X ∈RN×M
and the matrix two-norm or operator or spectral norm ∥X∥2 = σmax(X). The
notation ∥X∥∗refers to the nuclear or trace norm and is deﬁned in Subsec-
tion 17.2.5.
17.2
Problem Description and Motivation
Most real-world phenomena are nonlinear. Also, most real-world problems
have more than one quantity of interest. These two statements, however, are
not yet suﬃcient to motivate the central assumption exploited in this chap-
ter. This assumption is that in many cases one has target variables that are
somehow related.
Consider a power distribution network as depicted in Figure 17.1. Several
consumers are connected to the electricity grid via power stations. An impor-
tant task in the control of power grids to ensure their stability is the ability to

374
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 17.1: Power distribution network with diﬀerent consumers con-
nected to it. Shown are clusters of industrial customers, residential areas, and
sections dominated by oﬃces. (Illustration is based on public domain image
by the United States Department of Energy.)
precisely forecast the load of the network [6]. One could approach this by us-
ing an independent model for each substation. This however does not need to
be optimal. As shown in [1] on the example of the Belgian power distribution
network there are only a small number of distinctive proﬁles. Some of these
can be easily understood. There will be a behavior that will correspond to
residential consumers, with peaks in the morning and the evening. Then there
are power stations dominated by businesses that operate on a more or less 9-
to-5 schedule. A distinctive third group are large industry consumers working
in shifts 24/7. Within a ﬁxed category of similar customers the behavior will
be similar as it is driven by similar people in a similar environment. As the
example is limited to Belgium, the weather conditions, holiday seasons, and
habits will likely by similar all around. Therefore it is assumed that under-
standing the load at one power station will already reveal a lot of information
relevant to power stations with similar customer proﬁles. The loads at diﬀerent
substations are related.
As a second motivational example consider sensors of an industrial process.
There likely are large arrays of diﬀerent sensors, such as temperature, pressure
or ﬂow rates, throughout the production environment. The process itself is
however most likely governed by a complex but connected set of equations.
Therefore all the measurements obtained from various parts of the process are
related through the process itself.
17.2.1
Formal Problem Description
The problem addressed in this chapter is to estimate a mathematical
model for the system depicted in Figure 17.2. Given input-output data

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
375
S
ut
y(1)
t
y(M)
t
FIGURE 17.2: Nonlinear system with single input and multiple outputs.
S1
ut
y(1)
t
SM
ut
y(M)
t
FIGURE 17.3: Nonlinear system with single input and multiple outputs
split into individual systems for every output.
D = {(ut, yt)}N
t=1 with yt ∈RM, estimate a model of the form
ˆyt = ˆf(xt)
(17.1)
where xt ∈RD and ˆf : RD →RM. In nonlinear system identiﬁcation the
vector xt is usually made up of past inputs and outputs to capture the memory
of the system, i.e., xt = [yT
t−1, . . . , yT
t−P , ut, . . . , ut−Q]
T where P and Q are
the number of considered lags for past outputs and past inputs, respectively.
To allow for a compact notation, a new data set D′ can be deﬁned in terms
of xt and yt as D′ = {(xt, yt)}N
t=R where R = max(P, Q) + 1. The size of this
set is N ′ = N −R + 1.
17.2.2
Traditional vs. Proposed Handling of Multiple Outputs
The most common approach for dealing with multiple outputs is to split
the system apart into individual systems with a single output each, as de-
picted in Figure 17.3. This allows to reuse all methods that are available for
systems with a single output. For a system with an output vector given by
yt = [y(1)
t
, . . . , y(M)
t
]
T this would, for example, correspond to M estimation
problems of the form
min
fm
η∥fm∥+
N
X
t=R
l(y(m)
t
, fm(xt))
(17.2)
where m = 1, . . . , M, and η > 0 is a regularization parameter. The choice of
the norm on fm and the loss function l depend on the estimation technique
and will be deﬁned later. Besides being straightforward, this approach has the
advantage that it oﬀers the most ﬂexibility for modeling an individual output
variable. In this setting it is simple to employ diﬀerent function bases for
diﬀerent outputs or to carry out the estimation using diﬀerent loss functions.
This might, for example, be advantageous in case the noise is very diﬀerent
across the outputs.

376
Regularization, Optimization, Kernels, and Support Vector Machines
However, this approach also clearly has some drawbacks. As the estimation
problems are completely decoupled, the information contained in one target
variable cannot be used to improve the estimation of another. Therefore this
chapter advocates leaving the system in the form shown in Figure 17.2. The
corresponding estimation problem
min
f
η∥f∥+
N
X
t=R
l(yt, f(xt))
(17.3)
is more complex, but at least in theory allows the use of all available infor-
mation. For kernel-based models in a functional setting, models of this form
have been described by [3]. In the following an alternative formulation in
primal-dual framework will be derived.
17.2.3
Least Squares Support Vector Machines — A Primal-
Dual Kernel-Based Model
A particular kind of kernel-based models are least-squares support vector
machines (LS-SVMs). Like most nonlinear regression techniques, they address
problems like those formulated in (17.2). Their name stems from the choice of
the loss function l, which is the least squares loss l(x, y) = (x −y)2. The regu-
larization term is as in all support vector machines the squared ℓ2-norm of the
function fm. In support vector techniques the model f is usually formulated
in the so-called primal description as
ˆy = ˆf(xt) = wT ϕ(xt) + b.
(17.4)
Here w ∈Rnh and b ∈R are the model parameters, while ϕ : RD →Rnh
is the feature map. The feature space is often high dimensional. In kernel-
based techniques the feature map ϕ is most often not deﬁned explicitly, but
rather implicitly via a positive deﬁnite kernel function K. Based on Mercer’s
condition [19, 23] every positive deﬁnite function K(x, y) can be expressed
as an inner product K(x, y) = ϕ(x)T ϕ(y). Therefore, by choosing a positive
deﬁnite kernel K and reformulating the problem in terms of inner products of
ϕ one can implicitly work in very high dimensional spaces without having to
formulate an explicit parametrization of the basis. A popular choice for the
kernel function is the Gaussian RBF kernel KRBF(x, y) = exp(−∥x −y∥2
2/σ2)
with positive bandwidth σ > 0. The RBF kernel is associated with an inﬁnite
dimensional feature map.
The primal estimation problem for LS-SVM regression is
min
w,b,et
1
2wT w + 1
2γ
N
X
t=R
e2
t
subject to
yt = wT ϕ(xt) + b + et,
t = R, . . . , N.
(17.5)
In the case of high dimensional or implicitly deﬁned feature spaces it is not

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
377
practical to work with this primal formulation. The kernel-based Lagrange
dual formulation represents the solution in terms of the kernel function and
changes the number of optimization variables from nh + 1 to N ′ + 1. The
derivation of the kernel-based model follows a scheme with ﬁve steps. As the
proposed model formulation will require substantial eﬀort to carry out some
of these steps, the regular derivation will be brieﬂy reproduced here for later
reference.
Lemma 17.1 (Dual LS-SVM model [28]). The dual model for (17.4) is given
by
ˆy = ˆf(z) =
N
X
t=R
αtK(xt, z) + b.
(17.6)
The dual variables α = [αR, . . . , αN]T ∈RN′ and b are given as the solution
of the linear system
Ω+ γ−1IN′
1N ′
1T
N′
0
 α
b

=
y
0

.
(17.7)
The kernel matrix Ω
∈
RN ′×N′
is deﬁned element-wise as Ωij
=
K(xR+i−1, xR+j−1) = ϕ(xR+i−1)T ϕ(xR+j−1) for i, j = 1, . . . , N ′. The vec-
tor y = [yR, . . . , yN]T ∈RN′ stacks all target values.
Proof.
1. Writing down the Lagrangian of the primal problem
L(w, b, et, αt) = 1
2wT w + 1
2γ
N
X
t=R
e2
t −
N
X
t=P
αt(wT ϕ(xt) + b + et −yt).
(17.8)
2. Taking the derivatives of the Lagrangian with respect to the primal and
dual variables, and
3. formulating the Karush-Kuhn-Tucker (KKT) conditions for optimality.
∂L
∂w = 0nh :
w =
N
X
t=R
αtϕ(xt),
(17.9a)
∂L
∂b = 0 :
N
X
t=R
αt = 0,
(17.9b)
∂L
∂et
= 0 :
et = γ−1αt,
t = R, . . . , N,
(17.9c)
∂L
∂αt
= 0 :
yt = wT ϕ(xt) + b + et,
t = R, . . . , N.
(17.9d)

378
Regularization, Optimization, Kernels, and Support Vector Machines
4. Writing down the dual optimization problem.
The ﬁrst line of the dual optimization problem (17.7) follows directly
from substituting (17.9a) and (17.9c) into (17.9d) and applying the ker-
nel trick, i.e. ϕ(x)T ϕ(y) = K(x, y). The second line is a reformulation
of (17.9b) in vector notation.
5. Substitution of the dual solution into the primal model to obtain the
dual model.
Substituting (17.9a) into (17.4) and applying the kernel trick once more
yields the dual model (17.6).
17.2.4
Advantages of Primal-Dual Approach
LS-SVMs have several advantages. For instance, the basic core models
almost solely rely on linear algebra and as such the mathematical tools are
well understood by most. Also the solution is straightforward in any number
of computing languages and can be achieved in just a handful of lines of code
necessary to set up a linear system.
An advantage on a more fundamental modeling level, is the primal-dual
formulation. The primal problem description (17.4) is parametric in nature
and allows direct interpretation. Even more importantly, it allows us to for-
mulate prior information on the modeled system in a straightforward fash-
ion. This additional information can then be incorporated into the estimation
problem as additional constraints. The strength of this approach is that it
is on the one hand simple and on the other hand it is often possible to at-
tach a straightforward physical interpretation to the introduced constraints.
When deriving the dual model formulation, the information incorporated via
constraints is embedded into the model itself. In some cases all information
can be captured in an equivalent kernel function while in others the dual
model obtains a special structure. For large scale problems, the primal-dual
approach also oﬀers a powerful approach to compute approximate solutions.
Using a subsample of the full data set, one can estimate a ﬁnite dimensional
approximation of the feature map. This approximation can then be utilized
in the primal to solve the parametric problem using all available data. This is
done in ﬁxed-size kernel methods for dealing with large scale data [28].
Examples for prior information that can be incorporated into the prior
model are symmetry wT ϕ(x) = ±wT ϕ(−x) or boundedness wT ϕ(x) ≥y0. A
more thorough discussion on the primal-dual nature of LS-SVM-based models
is given in [27, 7]. It is also not restricted to regression problems but discusses
other applications of the support vector formalism as well. A more complex
example from system identiﬁcation is estimation of structured systems, like
Hammerstein and Wiener-Hammerstein systems [12, 9]. In these one exploits

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
379
a partitioning of the system in linear dynamical parts and a concentrated
static nonlinearity.
17.2.5
Nuclear Norm
The trade-oﬀbetween model ﬁt and model complexity is governed by the
regularization term, in the case of SVMs wT w. Therefore by choosing an-
other regularization term one can potentially introduce new and additional
structure into the estimation problem. In convex optimization in general and
especially in compressed sensing many new regularization schemes and their
advantages have been studied. A particular one of those is the nuclear norm,
a regularization term for matrices.
The nuclear norm or trace norm is the convex envelope of the rank func-
tion. As such it can be used as a surrogate to obtain convex approximations
of otherwise NP-hard problems. For a rectangular matrix X ∈RN×M it is
deﬁned as
∥X∥∗=
min(M,N)
X
n=1
σn(X)
(17.10)
where σn(X) is the n-th singular value of the matrix X. The interpretation
as convex envelope of the rank function holds for all ∥X∥2 ≤1. Relating
the nuclear norm to vector norms, it can be seen as a generalization of the
ℓ1-norm. In fact it is computed as the ℓ1-norm of the singular values of a
matrix. In similarity to its vector-cousin, it promotes sparsity. Whereas the ℓ1-
norm favors solutions with sparse vectors, the nuclear norm promotes solutions
having a low rank. Conditions for sparse solutions are studied in [22].
To solve nuclear norm problems in polynomial time using general purpose
convex optimization solvers the problem can be formulated as a semideﬁnite
programming (SDP) problem. ∥X∥∗can be computed as [10, 13].
max
X,U=U T ,V =V T
1
2 tr(U) + 1
2 tr(V )
subject to
 U
X
XT
V

⪰0.
(17.11)
The number of variables in this SDP embedding is large. Therefore only small
scale problems can be solved using general purpose solvers. A brief overview
of ongoing research on solvers is given in [22, Section 5]. Some examples are
interior point solvers that exploit special problem structure [17] and ﬁrst order
methods based on subgradients [20, 15].

380
Regularization, Optimization, Kernels, and Support Vector Machines
17.3
Primal Model for Multiple Outputs
17.3.1
Model Formulation
The LS-SVM model (17.5) can and has been used to identify systems
with multiple outputs using the traditional decomposition formulation (17.2).
Going to the joint estimation (17.3) is discussed next. The simplest ex-
tension from the regularization wT w for one model to the joint regular-
ization is taking the sum over all models PM
m=1 wT
mwm = ∥W ∥2
F where
W = [w1, . . . , wM] [28]. This approach has only limited appeal as it is purely
of a cosmetic nature as the estimation problems are in essence still decoupled.
An interesting property, expected at least in some multi-output systems, is
that each output can be described as the combination of a small number of
principal behaviors. In case of a linear model and linear combinations of prin-
cipal models, this translates into y(m)
t
= PL
l=1 µmlwT
l xt. Here y(m)
t
denotes
the m-th output of yt and µml the weighting coeﬃcient for the contribution
of the l-th principal model to the m-th output. This problem can be rewritten
as yt = W T xt subject to rank(W ) ≤L.
This relation is non-convex, but in the previous section a convex relaxation
to this problem was already introduced. Hence, an approximate representation
of the prior belief that the joint model should be a combination of a small
number of principal models is given by a nuclear norm regularization. In this
brief motivation, all arguments were presented in terms of linear models and
linear combinations. The strength of kernel-based models and the primal-dual
approach is that this transparently maps to nonlinear models. The proposed
model formulation for a model with multiple related outputs is then
min
W ,b,et
η∥W ∥∗+ 1
2
N
X
t=R
eT
t T et
subject to
yt = W T ϕ(xt) + b + et,
t = R, . . . , N.
(17.12)
This description contains a few technical modiﬁcations with respect to the
basic LS-SVM model (17.5), besides the transition to vector valued outputs
and a nuclear norm-based regularization term. In the following derivations it
turns out to be easier to work with a norm instead of a squared norm. Also it
is convenient to have the regularization constant η next to the regularization
term. A slight generalization to allow some of the ﬂexibility of a decoupled
formulation is the introduction of the positive deﬁnite weighting matrix T ∈
RM×M. It can be used to model correlation among the residuals of the diﬀerent
outputs and provides the means to have diﬀerent regularization constants
acting on each output. Note that the problem is still convex and is only subject
to equality constraints. Therefore Slater’s condition is trivially satisﬁed [4,
pages 226–227] and as such strong duality holds for the problem. Thus the
duality gap is zero.

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
381
17.3.2
Characterization of Proposed Multiple Output Model
In contrast to LS-SVMs and many other kernel-based estimation tech-
niques, the regularization term in the case of (17.12) is not quadratic. This
has several implications. The desired eﬀect is that solutions with linearly de-
pendent model parameters in the primal domain wm are favored. Furthermore,
the primal is deﬁned in a typically high-dimensional feature space, therefore
even a linear dependence of the parameters can represent complex interactions.
On the technical side a property proved easily for a quadratic penalty term,
uniqueness of the solution, is much less evident for the proposed scheme. Due
to the similarity of the nuclear norm to ℓ1-regularization in case of vectors,
critical values for the regularization parameters exist from which the solution
will remain constant. This section as well as the following two are based on [8,
Chapter 6].
The following lemma establishes uniqueness of the solution.
Lemma 17.2. For η > 0 the solution of (17.12) is unique in W , b and et.
Proof. Considering the residuals et = yt −W T ϕ(xt) −b, a suﬃcient condi-
tion for their uniqueness is the uniqueness of W and b. By eliminating the
residuals et from (17.12), the problem can be written in unconstrained form
as minW ,b J (W , b) with
J (W , b) = 1
2∥(ΦT W + 1NbT −Y T )T
1
2 ∥
2
F + η∥W ∥∗.
(17.13)
Then the lemma holds if the solution to this problem is unique.
First the solution for b will be derived. Note that b is not subject to
regularization. Therefore, from 0M = ∂J (W , b)/∂b, it follows that b =
1
N (Y −W T Φ)1N. Substitution of this relation into J yields
˜
J (W ) = 1
2∥(eΦT W −eY T )T
1
2 ∥
2
F + η∥W ∥∗.
Here, eY T = P ⊥
1 Y T and eΦT = P ⊥
1 ΦT with P ⊥
1 = IN −1
N 1N1T
N.
The uniqueness ot W in ˜
J is shown in the proof of Lemma 6.1 in [8, page
101]. The general idea of the proof is to split the optimization variable W
into a part in the range of eΦ and a part in the null space of eΦT . For the
contribution in the range it is shown that the problem is strongly convex and
thus unique. The part in null space is assumed to be nonzero and then it is
shown that this leads to a contradiction, establishing the full proof.
The regularization parameter η needs to be chosen carefully to obtain a
model with good generalization performance. In analogy to ℓ1-regularization
one can determine a critical value for η above which the solution remains
constant.

382
Regularization, Optimization, Kernels, and Support Vector Machines
Lemma 17.3. For η ≥η0 = σmax(ΦP ⊥
1 Y T T ) the solution of (17.12) is
given by
W0 = 0
and
b0 = 1
N ′ Y 1N′.
(17.14)
Here, P ⊥
1 =
1
N′ 1N′1T
N ′ denotes the projector onto the null space of 1N′.
This result is proven as Lemma 6.2 in [8, page 102]. The proof relies on
the necessary condition for optimality, that the subdiﬀerential of (17.13) has
to contain the element (0, 0).
17.4
Dual Formulation of the Model
The model formulated in (17.12) is parametric and requires the selection
of appropriate basis functions. However, one advantage of kernel-based mod-
eling is that the choice of basis functions is simpliﬁed by reducing it to the
choice of a kernel function. The kernel function often induces very large sets
of basis functions, but the inherent regularization is an eﬀective methodology
to counter overﬁtting eﬀects.
For LS-SVMs a kernel-based dual model was derived in Subsection 17.2.3.
In analogy to that the derivation of a nonparametric model for (17.12) will
be outlined in this section. The derivation is based on convex optimization
and, as suggested in Subsection 17.2.3, the most challenging parts are stating
the KKT conditions by diﬀerentiating the Lagrangian (Step 2 in proof of
Lemma 17.1) and the formulation of a predictive model in terms of the dual
solution (Step 5 in proof of Lemma 17.1).
Remark 17.1. Some of the advantages of kernel-based modeling could also
be exploited without explicitly deriving a dual-based model. The Nyström ap-
proximation [32] is able to compute a ﬁnite dimensional approximation of the
feature map ϕ on a data set D′. This results in a set of basis functions that
approximately span the space induced by the kernel. The approximation is tai-
lored to the distribution of the given data sample. A thorough description on
how the Nyström approximation can be used to solve the primal estimation
problem is given in [28] for ﬁxed-size LS-SVMs. The same methodology could
be employed to solve (17.12).
17.4.1
Dual Optimization Problem
The kernel-based dual optimization problem corresponding to (17.12) is
given by the following lemma.

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
383
Lemma 17.4. The solution to (17.12) is equivalent to the solution of its
Lagrange dual
max
A∈RM×N
tr(AT Y ) −1
2 tr(AT T −1A)
subject to
A1N ′ = 0M,
∥GAT ∥2 ≤η
(17.15)
with Y
= [yR, . . . , yN] ∈RM×N′. The matrix G is deﬁned as a matrix
square root, such that GT G = Ω. The elements of the kernel matrix Ω
can be computed using the kernel trick Ωij = ϕ(xR+i−1)T ϕ(xR+j−1) =
K(xR+i−1, xR+j−1) for i, j = 1, . . . , N ′.
Here only the outline of the proof is given. For the full details please refer
to [8, page 104 (Lemma 6.3)]. The key ingredient is to exploit the deﬁnition
of the dual norm. This idea has also been applied in the context of robust
support vector regression using an ℓ2 regularization term in [24]. This allows
us to restate the nondiﬀerentiable term ∥W ∥∗as a linear function subject
to additional constraints. The reformulated Lagrangian can then be used to
compute a KKT system similar to the one derived in Subsection 17.2.3. An
alternative approach would be to use conic duality; this eventually gives rise
to the same dual problem stated above. A similarly structured problem has
been derived in [8, pages 211 (Lemma 10.1)] using conic duality.
Proof.
1a. Write down the Lagrangian for (17.12)
L(wm, bm, et, αt) = η∥[w1, . . . , wM]∥∗+ 1
2
N
X
t=R
eT
t T et
−
N
X
t=R
αT
t (yt −[w1, . . . , wM]T ϕ(xt) −b −et).
(17.16)
1b. Replace ∥[w1, . . . , wM]∥∗by maxcm
PM
m=1 cT
mwm subject to ∥[c1, . . . ,
cM]∥2 ≤1.
2a. The reformulated Lagrangian contains the maximization over a vari-
able. Due to this it is still not diﬀerentiable. Therefore intro-
duce a modiﬁed Lagrangian
˜LC
such that L(wm, bm, et, αt)
=
max∥C∥2≤1 ˜LC(wm, bm, et, αt). The dual function is deﬁned as g(αt) =
infαt L(wm, bm, et, αt) = infαt max∥C∥2≤1 ˜LC(wm, bm, et, αt). Using
the saddle point property [4, Exercise 5.25] one can reverse the or-
der of the inf and the max. For the saddle point property to hold
ξ(wm, bm, et, αt) = ˜LC(wm, bm, et, αt) has to be closed and convex for
every C in ∥C∥2 ≤1 and χ(C) = −˜LC(wm, bm, et, αt) has to be closed
and convex for every (wm, bm, et, αt). ξ is a convex quadratic function

384
Regularization, Optimization, Kernels, and Support Vector Machines
while χ is linear, hence the conditions are satisﬁed and the order of
minimization and maximization can be reversed.
2b. Compute the derivatives of the modiﬁed Lagrangian ˜LC in all variables.
3. Formulate the KKT system:
∂˜LC
∂wm
= 0nh :
cm =
N
X
t=R
α(m)
t
ϕ(xt),
m = 1, . . . , M,
(17.17a)
∂˜LC
∂bm
= 0 :
N
X
t=R
α(m)
t
= 0,
m = 1, . . . , M,
(17.17b)
∂˜LC
∂et
= 0M :
et = T −1αt,
t = R, . . . , N,
(17.17c)
∥[c1, . . . , cM]∥2 ≤1.
(17.17d)
The last condition (17.17d) does not belong to the KKT system itself,
but rather to the outer maximization problem. Note that the dual prob-
lem is deﬁned as the maximization over the dual function. To allow for
a simpler presentation, the maximization over C can be attributed to
this ﬁnal maximization step.
4. In contrast to the LS-SVM derivation in Subsection 17.2.3, (17.17a) does
not provide an expansion for wm. Nevertheless, the expression can be
substituted into (17.17d). Considering the square of this condition and
applying the kernel trick, the inequality constraint in (17.15) can be de-
rived. Substitution of all conditions into the Lagrangian and carrying out
some simpliﬁcations yields the objective function of the dual problem.
Lemmas 17.2 and 17.3 established uniqueness for the primal problem and
derived an upper bound for the regularization constant η. Similar results can
be obtained for the dual. The corresponding relations can be found in Corol-
laries 6.4 and 6.5 in [8, pages 105-106]. The critical value for η in terms of
the kernel is η0 = σmax(GP ⊥
1 Y T T ). For values larger than η0 the solution
of (17.15) is A = T Y P ⊥
1 .
Remark 17.2. Instead of relying on optimization theory and Lagrangian du-
ality, SVM solutions can alternatively be derived using function estimation in
reproducing kernel Hilbert spaces (RKHSs) [31] by proving a representer the-
orem [30]. For unitarily invariant matrix norms, of which the nuclear norm
is a special case, this has been done in [3].
The derivation outlined here has the advantage of being constructive and
that additional constraints can be integrated straightforwardly. Also it yields a
diﬀerent optimization problem.

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
385
17.4.2
Predictive Model
Lemma 17.4 provides the optimal solution for (17.12) even if the feature
map ϕ is not known explicitly. The predictive model (17.4) relies on the pri-
mal variables wm, however. For problems with Tikhonov type regularization
like (17.5), establishing a link between the primal and dual variables is imme-
diately given by the KKT conditions, in this example (17.9a). This relation
can directly be substituted into the primal model to obtain a predictive equa-
tion in terms of the dual variables; see Table 17.1. However, establishing an
equivalent relation for the solutions of (17.12) is much more involved.
The link is given by studying the form of the dual norm ∥X∥∗=
max∥Z∥2≤1 tr(XT Z). Theorem 6.6 in [8, p. 107] characterizes the set of ma-
trices X that satisfy tr(XT Z0) = ξ0 and ∥X∥∗= ξ0 for given Z0 and ξ0. The
elements of the resulting set are of the form U1HV T
1
where H is positive
semi-deﬁnite with tr(H) = ξ0 and U1, V1 contain the singular vectors corre-
sponding to the largest singular value of Z0. Thus the primal variables in the
deﬁnition of the dual norm are not uniquely deﬁned for a given dual matrix.
Therefore, more information has to be used to recover the primal variables
and derive a predictive model.
The approach followed in [8] is to substitute the set of solutions given above
into the primal problem (17.12) and solve for H. This is relatively straight-
forward once the value of ξ0 is recovered. Determining ξ0 can be achieved by
establishing that strong duality holds for the considered problem and thus the
duality gap between the solutions of (17.12) and (17.15) is zero. Integrating
all this information yields expansions of the primal variables in terms of the
dual solution.
Theorem 17.1. The optimal values for W and b in (17.12) in terms of the
dual optimal solution A are given by
W = ΦAT Q
and
b = 1
N (Y −QAΩ)1N
(17.18)
with
Q = η−2Pη(Y AT −T −1AAT )Pη
(17.19)
where Pη = VηV T
η . The matrix Vη contains the eigenvectors of AΩAT cor-
responding to the eigenvalue η2.
The derivation of this result is given in Section 6.5 of [8, pages 108–110].
Having found explicit parametrizations for the primal variables, formulating
a predictive equation is straightforward. It directly follows from substitution
of W and b given in (17.18) into the primal model by = ˆf(z) = W T ϕ(z) + b
and the application of the kernel trick.
Corollary 17.1. With the deﬁnitions from Theorem 17.1 the predictive model
for a new point z, in terms of the dual variables, is given by
by = ˆf(z) =
N
X
t=R
eαtK(xt, z) + b.
(17.20)

386
Regularization, Optimization, Kernels, and Support Vector Machines
TABLE 17.1: Overview of parametric/primal and kernel-based/dual estima-
tion problems and the corresponding models.
parametric model
kernel-based model
basis functions
choose ϕ
choose kernel K
model estimation
solve (17.12) for W and b
solve (17.15) for A
obtaining model
representation
prespeciﬁed
obtain Q and b from
Theorem 17.1
generating
predictions
ˆf(z) = W T ϕ(z) + b
ˆf(z) =
N
P
t=R
eαtK(xt, z)+b
The variables eαt form the matrix e
A = [eα1, . . . , eαN′], which is computed as
e
A = QA.
In the following the essential steps of the proposed method are summarized
to give a concise algorithm. It allows us to estimate a model from given data
and to use this model to generate predictions at an unknown point z.
Algorithm 15
Given a kernel function K(x, y), data {(xt, yt)}N
t=R, and a regularization con-
stant, proceed as follows:
1. Compute kernel matrix Ωij = K(xR+i−1, xR+j−1) for i, j = 1, . . . , N ′.
2. Compute a matrix square root G such that Ω= GT G.
3. Solve dual problem (17.15) to obtain A.
4. Compute the compact eigenvalue decomposition of AΩAT and form the
matrix Vη from the eigenvectors corresponding to the largest eigenvalue
(η2).
5. Evaluate (17.19) to obtain mixing matrix Q.
6. Generate predictions at a new point z by evaluating the model given
by (17.20).

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
387
17.5
Extension to Variable Input Vectors
The last two sections considered a model of the form (17.4) for which the
input vector xt was the same for every output y(m)
t
. This section generalizes
this model to situations in which the input vector is diﬀerent for each output.
This is a simpliﬁed version of the model discussed in Subsection 6.6.1 of [8,
page 112], which additionally considers the possibility that the amount of data
gathered for each output could be diﬀerent.
The generalization to diﬀerent input vectors is reﬂected in the modiﬁed
primal estimation problem
min
W ,b,e(m)
η∥W ∥∗+ 1
2
M
X
m=1
tme(m)T e(m)
subject to
y(m) = ΦT
mwm + bm1N ′ + e(m),
m = 1, . . . , M,
(17.21)
where y(m) = [y(m)
R
, . . . , y(m)
N ]
T ∈RN′, Φm = [ϕ(x(m)
R ), . . . , ϕ(x(m)
N )] ∈
Rnh×N′, e(m) = [e(m)
R , . . . , e(m)
N ]
T ∈RN′ and wm is the m-th column of W .
Remark 17.3. Note that the equality constraint in (17.21) is transposed with
respect to the one in (17.12). It is also written in terms of the target variables
(M constraints) instead of the samples (N ′ constraints). This is done to stay
close to the notation of the corresponding section of [8] in which it is necessary
to account for possible diﬀerent dimensionalities of the considered quantities.
The dual optimization problem for (17.21) is given in the following lemma,
thus generalizing Lemma 17.4.
Lemma 17.5. The solution to (17.21) is equivalent to the solution of its
Lagrange dual
max
α(m)
M
X
m=1
α(m)T y(m) −1
2
M
X
m=1
1
tm
α(m)T α(m)
subject to
1T
N′α(m) = 0,
m = 1, . . . , M,


α(1)T Ω11α(1)
· · ·
α(1)T Ω1Mα(M)
...
...
...
α(M)T ΩM1α(1)
· · ·
α(M)T ΩMMα(M)

⪯η2IM
(17.22)
with α(m) ∈RN′. The N ′×N ′ matrices Ωmn are given by ΦT
mΦn. They can be
computed elementwise as (Ωmn)ij = K(x(m)
R+i−1, x(n)
R+j−1) for i, j = 1, . . . , N ′,
and m, n = 1, . . . , M.

388
Regularization, Optimization, Kernels, and Support Vector Machines
The derivation of this dual formulation is almost a carbon copy of the
proof of Lemma 17.4. Therefore only the diﬀerences will be highlighted.
Proof.
1′. Write down the Lagrangian for (17.21)
L(wm, bm, e(m), α(m)) = η∥[w1, . . . , wM]∥∗+ 1
2
M
X
m=1
tme(m)T e(m)
−
M
X
m=1
α(m)T (y(m) −ΦT
mwm −bm1N ′ −e(m)).
(17.23)
3′. Formulate the KKT system.
∂L
∂wm
= 0nh :
cm = Φmα(m),
m = 1, . . . , M,
(17.24a)
∂L
∂bm
= 0 :
1T
N ′α(m) = 0,
m = 1, . . . , M,
(17.24b)
∂L
∂e(m) = 0N ′ :
e(m) = t−1
m α(m),
m = 1, . . . , M,
(17.24c)
∥[c1, . . . , cM]∥2 ≤1.
(17.24d)
Lemma 17.6. The quadratic matrix inequality in (17.22) can be reformulated
as the following collection of smaller linear matrix inequalities (LMIs).
"
IN′
Gmα(m)
α(m)
m
T GT
m
rm
#
⪰0,
m = 1, . . . , M,
(17.25a)


I2N ′
Fmn

α(m)
α(n)

h
α(m)
m
T
α(n)
m
T i
F T
mn
rm + 2smn + rn

⪰0,
(17.25b)
m = 1, . . . , M, n = m + 1, . . . , M


r1
s12
· · ·
s1M
s12
r2
· · ·
s2M
...
...
...
...
s1M
s2M
· · ·
rM

⪯η2IM.
(17.25c)
Here Gm and Fmn are matrix factorizations such that Ωmm = GT
mGm and
 Ωmm Ωmn
Ωnm Ωnn

= F T
mnFmn for m = 1, . . . , M and n = m + 1, . . . , M.

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
389
Proof. The proof is a repeated application of the Schur complement. Con-
sider α(m)T Ωmmα(m) ≤rm and the factorization for Ωmm given above;
then (17.25a) is a direct consequence of the Schur complement. Note that
α(m)T Ωmnα(n) = α(n)T Ωnmα(m). Then (17.25b) is, by its Schur comple-
ment, equivalent to rm + 2smn + rn ≥α(m)T Ωmmα(m) + α(n)T Ωnnα(n) +
2α(m)T Ωmnα(n) ≥0. At the solution the inequalities will be tight, there-
fore (17.25) is a reformulation of the inequality in (17.22) as LMI.
For the problems in (17.21) and (17.22) the relation between primal and
dual variables is as follows.
Corollary 17.2. Given the optimal dual solution α1, . . . , αM, the primal
optimal variables w1, . . . , wM and b1, . . . , bM can be represented as
wm =
M
X
n=1
QnmΦnαn
(17.26)
and
bm = 1
N ′
 
1T
N′y(m) −
M
X
n=1
Qnm1T
N′Ωmnαn
!
,
(17.27)
for m = 1, . . . , M and with (Q)mn = Qmn and Q = VηHηV T
η .
The matrix Vη contains the eigenvectors of e
AT eΩe
A corresponding to the
largest eigenvalue η2. Furthermore the matrix Hη is given as the solution to
the auxiliary feasibility problem
ﬁnd
Hη
subject to
Hη ⪰0, tr(Hη) = ξ
y(m) = [Ωm,1α1, . . . , Ωm,MαM]VηHηV T
η ϵm + bm1N ′ + t−1
m αm,
m = 1, . . . , M,
where the ϵi’s form the standard basis for RM and ξ = η−2 PM
m=1 αT
my(m)
−PM
m=1 t−1
m αT
mαm

.
The predictive model then follows immediately.
Corollary 17.3. With the deﬁnitions from Corollary 17.2 the predictive
model for a new point z(m), in terms of the dual variables, is given by
by(m) = ˆfm(z(m)) =
M
X
n=1
Qnmkn(z(m))
T αn + bm,
(17.28)

390
Regularization, Optimization, Kernels, and Support Vector Machines
M1
M2
...
MM
z(m)
ˆy(m)
Q1m
+
Q2m
+
QMm
+
FIGURE 17.4: Conceptual visualization of the predictive Equation (17.28)
for m-th output ˆy(m) given the corresponding input vector z(m). The in-
dividual models M1, . . . , MM are the same for each output. The weights
Q1m, . . . , QMm specify how each of the models contributes to the predic-
tion of an individual output. Each submodel Mn performs the mapping
z →kn(z)T αn.
with kn(ζ) = [K(ζ, x(n)
R ), . . . , K(ζ, x(n)
N )]
T for m = 1, . . . , M. The predictive
model is applicable to all outputs by(m) with m = 1, . . . , M. The matrix Q is
given by the solution of the feasibility problem stated in Corollary 17.2.
The proofs for the results in this section can be found in [8, pp. 113–115]
and in general are straightforward extensions of those in the previous section.
Remark 17.4 (Structure of predictive equation). Note that the predictive
Equation (17.28) is coupled by the matrix Q. The prediction for the m-th
output is generated by feeding the corresponding input z(m) into each of the
M submodels. Then the coeﬃcients Qmn determine the contribution of each
of these submodels to the ﬁnal prediction. This structure is also visualized in
Figure 17.4.
This is in contrast to the traditional modeling approach using a Frobenius
norm regularization scheme. In such a formulation the predictive equations are
decoupled from each other and only one model contributes to the prediction of
each output. In the setting here this would correspond to the weighting matrix
Q being the identity matrix IM.
Remark 17.5 (Numerical complexity). The extended model (17.21) presented
in this section is a generalization of the basic model (17.12) discussed before.
Hence, the question arises, how much more expensive is solving the more gen-
eral, and consequently more powerful, problem.
The relevant dimensionalities are summarized in Table 17.2. It can be seen
that the number of unknowns is independent of the chosen formulation. The
main diﬀerence is in the dimensionality of the data. As a diﬀerent input vector
is considered for each output this increase is inevitable. In the primal the
dimensionality of the data matrix Φ grows linearly with the number of outputs

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
391
TABLE 17.2: Dimensionalities of primal and dual model formulations. nh
denotes the dimension of the feature space (number of basis functions), N ′ is
the amount of data available for model estimation, and M is the number of
model outputs.
Identical inputs
Diﬀerent input for
each output
Primal
Feature map Φ
nh × N ′
M · (nh × N ′)
Unknown W
nh × M
nh × M
Dual
Kernel matrix Ω
N ′ × N ′
(M · N ′) × (M · N ′)
Unknowns αmt
N ′ × M
N ′ × M
M. The dependency in the dual goes with M 2 as it compares each input sample
to all the others.
Due to this signiﬁcant increase of data to be processed, one should carefully
consider whether the system can be modeled with a single consolidated input
instead of individual ones.
17.6
Numerical Example
To illustrate several properties of the proposed model a numerical example
is analyzed. In order to keep the analysis as simple as possible, an artiﬁcial
problem is constructed. The algorithms are implemented using MATLAB and
CVX, which severely limits the problem sizes that can be solved with moderate
computational resources. Therefore the number of data is limited to 50 samples
for training of the model, 100 samples to select hyper-parameters using cross
validation, and, ﬁnally, 150 samples to evaluate the model performance on
an independent test set. The model structure is chosen to closely match the
assumptions embedded in the model,
Y = W T
0 Φ + noise.
(17.29)
Instead of generating nonlinear data and projecting it to a higher dimensional
space, for the sake of simplicity, data for the evaluated feature map Φ is
generated directly. Data is drawn from a normal distribution to obtain Φ ∈
R50×50. The core assumption of the model is that the true parameter matrix
is low rank. Letting W0 = P3
i=1 girT
i
= GRT with G ∈R50×3 and R ∈

392
Regularization, Optimization, Kernels, and Support Vector Machines
R20×3, corresponds to a model with 20 outputs yet only three independent
“behaviors”.
To evaluate the predictive power of the proposed model it is compared to
several other models. The considered models are as follows.
MIMO The proposed nuclear norm regularized model as given in Subsec-
tion 17.2.5.
RR The notable diﬀerence of the proposed model to classical approaches is
the choice of the regularization term. The most common regularization
would be a squared 2-norm type, corresponding to ridge regression. In
this case the estimation problems for the diﬀerent components are inde-
pendent of each other. Each one can be seen as an individual LS-SVM
model as in Subsection 17.2.3. Using this factored form has the advan-
tage that diﬀerent hyper-parameters can be chosen for every output.
While in theory at least the regularization constant could also be cho-
sen independently for each output in the proposed model, this would
only be computationally tractable for very small numbers of outputs.
OLS To obtain an intuition on the upper and lower bounds for the predictive
performance, two very simple models are considered. An upper bound
is given by the model obtained using ordinary least squares, i.e., c
W0 =
Φ†Y .
OLS + oracle A lower bound is also generated based on ordinary least
squares. However, in this case the true structure of the problem in the
form of R is assumed given. The objective is therefore reduced to esti-
mating G, which in this example reduces the number of unknowns to
be determined by a factor of 60.
Model performance is measured using the root mean squared error (RMSE),
for a single output deﬁned as
RMSEm =
v
u
u
t 1
N ′
N
X
t=R
(y(m)
t
−ˆfm(xt))
2.
The total RMSE is then given by
RMSE =
v
u
u
t 1
M
M
X
m=1
RMSE2
m.
17.6.1
Agreement of Primal and Dual Solutions
To support that the derived dual model is equivalent to the proposed
primal form, the problem is solved in both forms. This is easily possible as the

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
393
10−3
10−2
10−1
100
101
−4
−2
0
·10−7
regularization η
relative duality gap
FIGURE 17.5: Relative duality gap (p∗−d∗)/p∗between optimal value p∗
of (17.12) and the optimal value d∗of (17.15) for toy example.
10−3
10−2
10−1
100
101
0
1
2
·10−3
regularization η
relative accuracy
FIGURE 17.6: Relative accuracy of parameter estimate c
W for toy example.
For the primal problem in (17.12) the value Wprimal is a direct result of the
optimization problem. For the dual (17.15) the optimal parameter Wdual is
recovered using the equation from Theorem 17.1. The reported quantity is
∥Wprimal −Wdual∥F /∥Wprimal∥F .

394
Regularization, Optimization, Kernels, and Support Vector Machines
10−3
10−2
10−1
100
101
1
2
3
regularization η
total RMSE
MIMO
RR
OLS
OLS + Oracle
FIGURE 17.7: Model performance evaluated on a validation set for diﬀer-
ent values of the regularization parameter η. The OLS-based models are not
subject to regularization and are thus constant. For RR the optimal regular-
ization constant for each output is computed individually and only the best
performance is indicated.
considered example has an explicitly given feature map. To check agreement of
both model representations, the recovered parameter vector c
W as well as the
duality gap between both estimates are compared. From Figures 17.5 and 17.6
it can be seen that both solutions are in agreement up to numerical precision.
This is expected as it has been shown that strong duality holds and therefore
the duality gap has to be zero.
17.6.2
Cross-Validation Performance
Selecting good values for the model hyper-parameters is essential for most
data-driven estimation schemes. A popular approach is to select the hyper-
parameters based on cross-validation performance. This technique is also vi-
able for the proposed model structure, as can be seen from Figure 17.7. The
total RMSE on the validation set shows a distinct minimum. Also the pro-
posed method MIMO yields a much lower error than the compared techniques
RR and OLS, which have no information on the problem structure. Full knowl-
edge of the structure, however, results in an even superior performance, as can
be seen from the value of OLS + Oracle.

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
395
0
10
20
0
1
2
output m
RMSEm on test set
ridge regression (RR)
0
10
20
0
1
2
output m
nuclear norm (MIMO)
FIGURE 17.8: Performance of the models RR and MIMO on an independent
test set. The accuracy of the predictions is shown for every output.
17.6.3
Predictive Performance
Figure 17.8 shows the predictive performance for RR and MIMO. It can
be seen that for almost all outputs the prediction performance of the pro-
posed model is improved with respect to the baseline. Table 17.3 shows the
total RMSE for the diﬀerent parts of the data. The results illustrate that in
this example the proposed approach MIMO improves signiﬁcantly over the
traditional technique RR. It can also be seen that selecting the regulariza-
tion parameter using cross validation works well as there is no evidence of
overﬁtting.
TABLE 17.3: Predictive performance for multivariate toy dataset. All given
quantities are RMSE values with respect to time and the output, for diﬀerent
partitions of the data.
training
validation
test
OLS
0
0.8633
0.8459
OLS + Oracle
0.0920
0.2885
0.2981
RR
0.0175
0.7644
0.7867
MIMO
0.0184
0.4716
0.4974

396
Regularization, Optimization, Kernels, and Support Vector Machines
17.7
Conclusions
This chapter introduced a novel identiﬁcation scheme for nonlinear sys-
tems with multiple outputs. It is able to exploit relations between output
variables. The beneﬁts in predictive performance are illustrated on a small
toy example. The model is derived in a primal-dual setting, which is new for
models with nuclear norm regularization. Due to the primal-dual approach,
it is straightforward to incorporate additional information in the estimation
problem. The derivation for primal-dual models presented here can also be
applied to other models with non-quadratic regularization terms. New algo-
rithms and increasing computational power will allow the application of the
presented technique on larger real world datasets in the future.
Acknowledgments
Research supported by: Research Council KUL: GOA/10/09 MaNet,
PFV/10/002 (OPTEC), several PhD/postdoc and fellow grants; Flemish
Government: IOF: IOF/KP/SCORES4CHEM, FWO: PhD/postdoc grants,
projects: G.0377.12 (structured systems), G.083014N (block term decomposi-
tions), G.088114N (tensor-based data similarity), IWT: PhD Grants, projects:
SBO POM, EUROSTARS SMART, iMinds 2013, Belgian Federal Science
Policy Oﬃce: IUAP P7/19 (DYSCO, Dynamical systems, control and opti-
mization, 2012-2017), EU: FP7-SADCO (MC ITN- 264735), ERC AdG A-
DATADRIVE-B (290923), COST: Action ICO806: IntelliCIS. Johan A.K.
Suykens is a professor and Bart De Moor is a full professor at KU Leuven.
Bibliography
[1] Carlos
Alzate,
Marcelo
Espinoza,
Bart
De
Moor,
and
Johan
A.K. Suykens. Identifying customer proﬁles in power load time series
using spectral clustering. In Proceedings of the 19th International Con-
ference on Artiﬁcial Neural Networks, ICANN ’09, pages 315–324, Berlin,
Heidelberg, 2009. Springer-Verlag.
[2] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano A. Pontil.
Convex multi-task feature learning. Machine Learning, 73(3):243–272,
January 2008.

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
397
[3] Andreas Argyriou, Charles A. Micchelli, and Massimiliano A. Pontil.
When Is There a Representer Theorem? Vector Versus Matrix Regu-
larizers. Journal of Machine Learning Research, 10:2507–2529, 2009.
[4] Stephen P. Boyd and Lieven Vandenberghe. Convex Optimization. Cam-
bridge University Press, 2004.
[5] Emmanuel J. Candès and Yaniv Plan. Matrix completion with noise.
Proceedings of the IEEE, 98(6):925–936, 2010.
[6] Marcelo Espinoza. Structured Kernel Based Modeling and its Application
to Electric Load Forecasting. PhD thesis, Katholieke Universiteit Leuven,
Belgium, June 2006.
[7] Marcelo Espinoza, Johan A. K. Suykens, Ronnie Belmans, and Bart De
Moor. Electric Load Forecasting – Using kernel based modeling for non-
linear system identiﬁcation. IEEE Control Systems Magazine, 27:43–57,
October 2007.
[8] Tillmann Falck. Nonlinear system identiﬁcation using structured kernel
based models. PhD thesis, Katholieke Universiteit Leuven, Belgium, April
2013.
[9] Tillmann Falck, Philippe Dreesen, Kris De Brabanter, Kristiaan Pelck-
mans, Bart De Moor, and Johan A. K. Suykens. Least-Squares Support
Vector Machines for the Identiﬁcation of Wiener-Hammerstein Systems.
Control Engineering Practice, 20(11):1165–1174, November 2012.
[10] Maryam Fazel. Matrix Rank Minimization with Applications. PhD thesis,
Stanford, 2002.
[11] Maryam Fazel, Haitham A. Hindi, and Stephen P. Boyd. A rank min-
imization heuristic with application to minimum order system approxi-
mation. In Proceedings of the American Control Conference, Arlington,
VA, USA, 2001.
[12] Ivan Goethals, Kristiaan Pelckmans, Johan A. K. Suykens, and Bart
De Moor. Subspace identiﬁcation of Hammerstein systems using least
squares support vector machines. IEEE Transactions on Automatic Con-
trol, 50:1509–1519, October 2005.
[13] Martin Jaggi and Marek Sulovský. A simple algorithm for nuclear norm
regularized problems. In Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pages 471–478, Haifa, Israel, June 2010.
[14] Rudolph E. Kalman.
Contributions to the theory of optimal control.
Boletín de la Sociedad Matemática Mexicana, 5:102–119, 1960.

398
Regularization, Optimization, Kernels, and Support Vector Machines
[15] Yong-Jin Liu, Defeng Sun, and Kim-Chuan Toh. An implementable prox-
imal point algorithmic framework for nuclear norm minimization. Math-
ematical programming, 133(1-2):399–436, 2012.
[16] Zhang Liu, Anders Hansson, and Lieven Vandenberghe. Nuclear norm
system identiﬁcation with missing inputs and outputs. Systems & Control
Letters, 62(8):605–612, 2013.
[17] Zhang Liu and Lieven Vandenberghe. Interior-Point Method for Nuclear
Norm Approximation with Application to System Identiﬁcation. SIAM
Journal on Matrix Analysis and Applications, 31(3):1235–1256, January
2009.
[18] Lennart Ljung. System identiﬁcation: Theory for the User. Prentice Hall
PTR, Upper Saddle River, NJ, USA, 2nd edition, 1999.
[19] James Mercer. Functions of positive and negative type, and their con-
nection with the theory of integral equations. Philosophical Transactions
of the Royal Society of London. Series A, Containing Papers of a Math-
ematical or Physical Character, 209:415–446, 1909.
[20] Ting Kei Pong, Paul Tseng, Shuiwang Ji, and Jieping Ye. Trace norm reg-
ularization: Reformulations, algorithms, and multi-task learning. SIAM
Journal on Optimization, 20(6):3465–3489, 2010.
[21] Benjamin Recht. A simpler approach to matrix completion. Journal of
Machine Learning Research, 12:3413–3430, 2011.
[22] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo.
Guaranteed
Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm
Minimization. SIAM Review, 52(3):471–501, January 2010.
[23] Bernhard Schölkopf and Alexander J. Smola. Learning with Kernels. MIT
Press, Cambridge, Mass., 2002.
[24] Pannagadatta K. Shivaswamy, Chiranjib Bhattacharyya, and Alexan-
der J. Smola. Second Order Cone Programming Approaches for Handling
Missing and Uncertain Data.
Journal of Machine Learning Research,
7:1283–1314, 2006.
[25] Marco Signoretto, Quoc Tran Dinh, Lieven De Lathauwer, and Johan
A. K. Suykens.
Learning with tensors: a framework based on convex
optimization and spectral regularization. Machine Learning, 94(3):303–
351, 2014.
[26] Jonas Sjoberg, Qinghua Zhang, Lennart Ljung, Albert Benveniste,
Bernard Delyon, Pierre-Yves Glorennec, Hakan Hjalmarsson, and Ana-
toli Juditsky. Nonlinear black-box modeling in system identiﬁcation: a
uniﬁed overview. Automatica, 31:1691–1724, December 1995.

Kernel-Based Identiﬁcation Using Nuclear Norm Regularization
399
[27] Johan A. K. Suykens, Carlos Alzate, and Kristiaan Pelckmans. Primal
and dual model representations in kernel-based learning. Statistics Sur-
veys, 4:148–183, August 2010.
[28] Johan A.K. Suykens, Tony Van Gestel, Jos De Brabanter, Bart De Moor,
and Joos Vandewalle. Least Squares Support Vector Machines. World
Scientiﬁc, 2002.
[29] Peter Van Overschee and Bart De Moor.
Subspace Identiﬁcation for
Linear Systems, Theory, Implementation, Applications. Kluwer Academic
Publishers, 1996.
[30] Grace Wahba. Spline Models for Observational Data. SIAM, 1990.
[31] Grace Wahba. Support Vector Machines, Reproducing Kernel Hilbert
Spaces and the Randomized GACV. In Bernhard Schölkopf, Christoper
J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 69–88. MIT Press, Cambridge, MA,
May 1998.
[32] Christopher K. I. Williams and Matthias Seeger.
Using the Nyström
Method to Speed Up Kernel Machines. In T. Leen, T. Dietterich, and
V. Tresp, editors, Neural Information Processing Systems 13, pages 682–
688. MIT Press, 2001.

This page intentionally left blank
This page intentionally left blank

Chapter 18
Kernel Methods for Image Denoising
Pantelis Bouboulis
Department of Informatics and Telecommunications, University of Athens
Sergios Theodoridis
Department of Informatics and Telecommunications, University of Athens
18.1
Introduction ......................................................
401
18.2
Preliminaries .....................................................
402
18.2.1
Reproducing Kernel Hilbert Spaces .....................
402
18.2.2
The Denoising Problem .................................
404
18.3
Kernel Ridge Regression Modeling ..............................
406
18.3.1
KRR-L2 ..................................................
407
18.3.2
KRR-L1 ..................................................
409
18.4
Semi-Parametric Modeling .......................................
411
18.5
A Robust Approach ..............................................
415
18.6
Conclusions .......................................................
422
Bibliography ......................................................
422
18.1
Introduction
One of the most commonly encountered problems in the area of image pro-
cessing is that of noise removal. Any image taken by conventional ﬁlm cameras
or digital cameras can pick up noise from a variety of sources. This noise usu-
ally has a negative aesthetic eﬀect on the human eye. Moreover, subsequent
uses of the digitized image such as in applications of computer vision, classiﬁ-
cation, recognition, etc., require the noise to be removed in order to maximize
the performance of the respective system. Hence, a large number of tech-
niques have been proposed to address this problem, ranging from the typical
low pass ﬁlters that convolve the original image with a predeﬁned mask [11], to
methods involving fractals, diﬀerential equations, and approximation theory.
Among the most popular methodologies are the denoising methods based on
wavelet theory (e.g., [19, 21, 26, 8, 9, 6]), where the coeﬃcients of the image’s
wavelet expansion are manipulated so that the noise is removed, while the im-
portant features of the image are preserved. Other popular techniques include
401

402
Regularization, Optimization, Kernels, and Support Vector Machines
the denoising methods based on the theory of partial diﬀerential equations
[27], methods for impulse detection ([1, 2, 22]), non-linear approaches that
employ kernel regression and/or local expansion approximation techniques
([31]), and sparse approximation techniques ([10, 5, 15]. In most cases, the
denoising techniques are developed assuming a particular noise model (Gaus-
sian, impulse, etc.); thus, they are unable to eﬀectively treat more complex
models, which are often met in practical applications. In this chapter, we are
interested in image denoising techniques that are based on the theory of repro-
ducing kernel Hilbert spaces (RKHS). In general, these methods attempt to
approximate the noise-free image as a linear combination of positive deﬁnite
kernel functions, while all other components are considered as noise and hence
they are discarded. The straightforward approach is to consider a kernel ridge
regression rationale, minimizing the square error between the original image
and the noise-free estimation, while, at the same time, constraining the norm
of the expansion’s coeﬃcients. However, it turns out that although this sim-
ple method can remove noise components, it also discards ﬁne details of the
image. Hence, we will discuss more advanced techniques that exploit notions
and ideas not only from functional analysis, but also from machine learning
and the recently developed theory of sparse representations. Observe that this
rationale makes no assumption for the noise distribution; hence it can be used
to remove any type of noise.
In a relatively similar context, kernels have also been employed by other
denoising methods that are not considered here. For example, in [14] and [35]
a support vector regression approach is considered for the Gaussian noise case,
while in [12] the kernel principal components of an image are extracted and
this expansion is truncated to produce the denoising eﬀect.
18.2
Preliminaries
Throughout this chapter, we use boldface letters for vectors and normal
letters for scalars. Prior to delving into the main part of this chapter, i.e.,
the problem of denoising into RKHS, we devote some space to introducing
the main notions and theories that are employed. We begin with a short
discussion on the main properties of RKHS.
18.2.1
Reproducing Kernel Hilbert Spaces
In a nutshell, RKHS [29, 25] are inner product spaces of functions on X,
in which pointwise evaluation is a continuous (and hence bounded) linear
evaluation functional. These spaces have been proved to be a very powerful
tool in the context of statistics, complex and harmonic analysis, quantum
mechanics, and, in particular, in machine learning applications [25, 28, 32]. It

Kernel Methods for Image Denoising
403
has been established that each such space is associated with a positive deﬁnite
kernel, κ(x, y), i.e., a symmetric function of two variables of X that satisﬁes
N
X
n,m=1
anamκ(xn, xm) ≥0,
for all numbers an, am and points xn, xm ∈X, where n, m = 1, 2, . . . , N. A
Hilbert space, H, is called an RKHS if there exists a positive deﬁnite kernel
κ with the following properties:
• For every x ∈H, the function κ(·, x) belongs to H.
• κ has the so-called reproducing property, i.e., f(x) = ⟨f, κ(·, x)⟩H, for all
f ∈H, x ∈X.
A large variety of kernels can be found in the respective literature [33, 25, 7].
However, in this chapter, we focus our attention on one of the most widely
used kernels, the Gaussian radial basis function:
κσ(x, y) = exp

−∥x −y∥2
σ2

,
due to its valuable characteristics (smoothness, universal approximation).
One of the most important properties of RKHS is that, although the space
might be inﬁnite-dimensional, the solution of any regularized risk minimiza-
tion problem admits a ﬁnite representation. In fact, the optimal solution lies
in the span of the kernels centered on the training points. This is ensured by
the celebrated Representer Theorem.
Theorem 18.1 (Representer Theorem [13]). Denote by Ω: [0, ∞) →R a
strictly monotonic increasing function, by X a set and by ℓ: (X ×R2)N →R∪
{∞} an arbitrary loss function. Then each minimizer f ∈H of the regularized
risk functional
ℓ((x1, z1, f(x1)), . . . , (xN, zN, f(xN))
+ Ω(∥f∥H)
(18.1)
admits a representation of the form
f(x) =
N
X
n=1
αnκ(xn, x).
(18.2)
This theorem can be generalized to include the case where f has two
components, one lying in H and the other in the span of a set of predeﬁned
independent functions, as follows:
Theorem 18.2 (Semi-Parametric Representer Theorem [25]). Suppose that,
in addition to the assumptions of the previous theorem, we are given
Ω2: [0, ∞) →R, another strictly monotonic increasing function, and a set

404
Regularization, Optimization, Kernels, and Support Vector Machines
of M real-valued functions {ψk}M
k=1 : X →R, with the property that the
N × M matrix (ψp(xn))n,p has rank M. Then any ˜f := f + ψ, with f ∈H
and ψ ∈H = span{ψk}, where ∥· ∥is a norm deﬁned in H, minimizing the
regularized risk functional
ℓ((x1, z1, f(x1)), . . . , (xN, zN, f(xN))
+ Ω(∥f∥H)
+ Ω2 (∥ψ∥)
(18.3)
admits a representation of the form
˜f(x) =
N
X
n=1
αnκ(xn, x) +
M
X
k=1
βkψk(x).
(18.4)
In the denoising tasks that are dealt with in this chapter, ℓtakes the form
ℓ((x1, z1, f(x1)), . . . , (xN, zN, f(xN)) =
N
X
n=1
L (zn −f(xn)) ,
where zn are the actual values of the (possibly corrupted) signal at xn and
f(xn) are the reconstructed (noise free) values. The function L is either the
ℓ2, or the ℓ1 norm. The regularization term Ω(f), on the other hand, takes
the form Ω(f) = 1/2∥f∥2
H, λ > 0. Recall that the RKHS considered here
are associated with the Gaussian kernel. This implies an inﬁnite dimensional
space ([25]). Moreover, in the cases where X = Rm, m > 0, it can be shown
that the induced norm of any function in this RKHS is given by
∥f∥H =
Z
X
X
n
σ2n
n!2n (Onf(x))2dx,
(18.5)
where O2n = ∆n and O2n+1 = ∇∆n, ∆being the Laplacian and ∇the
gradient operator (see [34]). The implication of this is that the regularization
term “penalizes” the derivatives of the minimizer, resulting in a very smooth
solution of the respective regularized risk minimization problem. In fact, this
penalization has a stronger eﬀect than the total variation scheme, which is
often used in wavelet-based denoising (see for example [9, 6, 24, 16, 23]).
Indeed, while the total variation penalizes only the ﬁrst order derivatives,
the term ∥f∥2
H penalizes derivatives of any order, resulting in very smooth
estimates.
18.2.2
The Denoising Problem
In this chapter, we model the noisy image as
ˆf(x, y) = f(x, y) + η(x, y),
(18.6)
for x, y ∈[0, 1], where f is the noise free image and η the additive noise [11].
Given the noisy image ˆf, the objective of any denoising method is to obtain an

Kernel Methods for Image Denoising
405
FIGURE 18.1: A square N × N region.
estimate of the original image f. Usually, this task is carried out by exploiting
some a priori knowledge concerning the noise distribution. For example, most
wavelet-based methods assume the noise to be Gaussian. This is a reason-
able assumption according to the central limit theorem, which states that the
sum of diﬀerent noise sources tends to approach a Gaussian distribution. In
fact, some methods go as far as to require the actual variance of the noise
distribution [8]. In contrast, the methods that will be presented in the follow-
ing sections make no assumption regarding the underlying noise model. They
use, however, some user-deﬁned parameters that depend on the amount of the
additive noise.
In the following, we will assume that the original “noisy” image is divided
into smaller N × N square regions of interest (ROIs), as it is illustrated in
Figure 18.1. Instead of applying the denoising process to the entire image, we
will process each ROI sequentially. This is done for two reasons. Firstly, the
time needed to solve the optimization tasks considered in the next sections
increases polynomially with N. Secondly, working in each ROI separately
enables us to change the parameters of the model in an adaptive manner, to
account for the diﬀerent levels of details in each ROI (Figure 18.2). Moreover,
we will assume that each ROI represents the points on the surface of a function,
ˆf, of two variables deﬁned on [0, 1] × [0, 1]. The pixel values of the digitized
image are represented as zn,m = ˆf(xn, ym) ∈[0, 255], where xn = n/(N −1),
ym = m/(N −1) for n, m = 0, 1, ..., N −1. To simplify the notation, we will
often rearrange the columns of each ROI so that it becomes a vector z ∈RN2,
as Figure 18.3 illustrates.
As this paper deals with kernel-based regression methods to address the
denoising problem, we should also discuss another important issue. It is known
that the accuracy of ﬁt of these methods drops near the borders of the data
set. There are several techniques to address this problem. For example, one
could employ overlapping ROIs and ﬁnally compute the mean value over all

406
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 18.2: The two ROIs shown above diﬀer in the level of details. The
ﬁrst one (A) belongs to a smooth area of the picture, while the second (B)
contains edges.
FIGURE 18.3: Rearranging the columns of a ROI so that it becomes a
vector.
estimations, or discard the estimation of the pixels near the borders, etc.
Another relative strategy is discussed in detail in Section 18.6.
18.3
Kernel Ridge Regression Modeling
This section provides two simple (non-adaptive) formulations of the denois-
ing problem in RKHS. Motivated by these simple methods, we will present
some more advanced techniques in the following sections.

Kernel Methods for Image Denoising
407
(a)
(b)
(c)
FIGURE 18.4: (a) The Lena image corrupted by 20 dB Gaussian noise
(PSNR = 25.67 dB). (b) The reconstructed Lena image derived by the KRR-
L2 method with σ = 0.5, λ = 4, N = 6 (PSNR = 31.84 dB). (c) The re-
constructed Lena image derived by the BM3D wavelet based method [8] with
σ = 15 (PSNR = 34.6 dB).
18.3.1
KRR-L2
A straightforward approach to the image denoising problem is to assume
that the original noise free image, f, can be closely approximated by its pro-
jection to an RKHS induced by the Gaussian kernel. Hence, one may consider
the so called kernel ridge regression (KRR) minimization task and solve:
min
g∈H,c∈RC(g, c) =
N−1
X
n,m=0
(g(xn, ym) + c −zn,m)2 + λ∥g∥2
H + λ · c2,
(18.7)
where f = g + c. This is motivated by the universal approximation properties
of the Gaussian kernel and by the fact that the RKHS associated to that ker-
nel is comprised of smooth functions. Observe the bias factor, c, that has been
explicitly used in (18.7). This is a common strategy in support vector regres-
sion (SVR) and kernel based regression tasks to improve their performance
[29, 30, 25]. It turns out that the bias is important in order to counteract
the eﬀect of the regularizer, which aﬀects the leveling of the solution. Recall
that the semi-parametric representer theorem (Theorem 18.2) ensures that
the solution of (18.7) takes the form g∗= PN−1
n,m=0 anκ(·, (xn, ym)). Hence,
the reconstructed denoised image is given by
f =
N−1
X
n,m=0
anκ(·, (xn, ym)) + c.
(18.8)
To take the respective “noise free” pixels, one simply computes f(xn, ym), for
all n, m = 0, 1, ..., N −1.

408
Regularization, Optimization, Kernels, and Support Vector Machines
(a)
(b)
(c)
FIGURE 18.5: (a) The Lena image corrupted by 20 dB Gaussian noise
and 5% salt and pepper noise (PSNR = 17.78 dB). (b) The reconstructed
Lena image derived by the KRR-L2 method with parameters σ = 0.5, λ = 4,
N = 12 (PSNR = 26.89 dB). (c) The reconstructed Lena image derived by
the KRR-L2 method with parameters σ = 0.5, λ = 50, N = 12 (PSNR =
27.52 dB).
To solve (18.7), we recast it using matrix notation and replace g with its
kernel expansion:
C(a, c) =
(K
1)
a
c

2
+ λaT KT Ka + λ · c2.
This is a strictly convex function with a unique minimum given by
a∗
c∗

=
 AT A + λB
−1 AT z,
(18.9)
where A = (K
c) and B =

KT K
0
0
1

. Besides the size of the ROIs, i.e., N,
the task in (18.7) has two more user deﬁned parameters: the kernel width, σ,
and the regularization parameter, λ. Both can be used to control the denoising
process. The larger the λ is, more inﬂuence is given to ∥g∥2
H, resulting to a
smoother picture. Similarly, large values of σ imply a smoother RKHS, also
resulting in smoother estimates. Figures 18.4 and 18.5 demonstrate the KRR-
L2 denoising process described by (18.7) in two simple cases. In the ﬁrst case,
the Lena image is corrupted by 20 dB Gaussian noise. Figure 18.4 shows
that KRR-L2 can remove this type of noise relatively well (although not close
enough to the state-of-the-art wavelet based methods). In the second case,
the Lena image is corrupted by 20 dB Gaussian noise and 5% salt and pepper
noise. In this situation, the KRR-L2 rationale fails to eﬀectively remove the
outliers for all values of λ and σ (Figure 18.5). This is due to the square error
loss function (L2) employed by (18.7). For small values of λ and σ, the cost
function forces f to ﬁt the outlier data (i.e., the black and white pixels) as
well as possible, resulting in small dark and light artifacts (Figure 18.5b). This

Kernel Methods for Image Denoising
409
(a)
(b)
(c)
FIGURE 18.6: (a) The Lena image corrupted by 20 dB Gaussian noise and
5% salt and pepper noise (PSNR = 17.78 dB). (b) The reconstructed Lena
image derived by the KRR-L1 method with parameters σ = 0.5, λ = 0.01,
N = 5 (PSNR = 30.13 dB). (c) The reconstructed Lena image derived by the
BM3D [8] method with σ = 45 (PSNR = 29.7 dB)
eﬀect is reduced as λ and/or σ are increased. However, in the latter case the
resulting image is blurry (Figure 18.5c).
The KRR-L2 denoising algorithm can be summarized in the following three
steps:
• For each image pixel (i, j):
• Form the N × N ROI, z (so that the (i, j) pixel is at the top left corner
of the ROI).
• Solve Problem (18.7) for that particular ROI.
• Move to the next pixel.
Note that each pixel is assigned to N 2 diﬀerent values (since it belongs to
each one of the N 2 regions of its neighboring pixels). The actual value that
we assign to each pixel at the end of the algorithm is the mean of these values.
18.3.2
KRR-L1
To enhance the performance of the KRR-L2 denoising process in the case
of impulse noise, one might consider replacing the L2 (square) loss of (18.7)
by an L1 type loss function. This results in the KRR-L1 method that can be
cast as
min
g∈H,c∈RC(g, c) =
N−1
X
n,m=0
|g(xn, ym) + c −zn,m| + λ∥g∥2
H.
(18.10)
Observe that in the KRR-L1 method the bias is not penalized, in contrast
to (18.7), where the penalization ensures the invertibility of the respective

410
Regularization, Optimization, Kernels, and Support Vector Machines
matrix. The task deﬁned in (18.10) has the advantage that it relaxes the need
to closely ﬁt the noisy data, a property that can be advantageous in the case
of impulse noise. Unfortunately, although its cost function is also a strictly
convex function, the solution cannot be given in a closed form as in the case
of the KRR-L2 formulation. Moreover, note that (18.10) is not diﬀerentiable.
Here, we mobilize the celebrated Polyak’s projected subgradient method (see
[20]). Polyak’s Algorithm solves for the optimal value of x = argmin{C(x)}
iteratively and it can be summarized in the following recursion:
xk+1 = xk −γk ·
∇C(xk)
∥∇C(xk)∥
where γk is an arbitrary sequence such that P∞
k=1 γk = ∞, P∞
k=1 γ2
k ∈R and
∇C(xk) is any subgradient of the cost function C at xk. Hence, to implement
the algorithm in the case of (18.10), we need to compute any of the subgradi-
ents C(g, c). Taking into account the reproducing property, after some algebra
we can deduce that a suitable choice is:
∇C(g, c) =

∇gC(g, c)
∇cC(g, c)

=
=






N−1
X
n,m=0
sign (g(xn, ym) + c −zn,m) · κ (·, (xn, ym)) + λ · g
N−1
X
n,m=0
sign (g(xn, ym) + c −zn,m)






.
(18.11)
The KRR-L1 denoising algorithm can be summarized in the following three
steps:
• For each image pixel (i, j):
• Form the N × N “pixel centered” ROI, z.
• Solve Problem (18.10) for that particular ROI using Polyak’s method.
(Here N is an odd number).
• Move to the next pixel.
Similar to the KRR-L2 case, at the end of the algorithm the value of each
image pixel is set as the mean of the N 2 values obtained by each ROI that
contains it. Figure 18.6 compares the KRR-L1 algorithm with the wavelet
based method of [8]. It is shown that although the wavelet method provides a
smoother picture (which is probably more eye pleasing), the KRR-L1 method
does a better job of preserving the edges and the details of the picture. KRR-
L1’s main disadvantage is that it cannot eﬀectively remove the “smaller” Gaus-
sian noise resulting in obscure artifacts in the smooth areas of the picture. Al-
though this eﬀect can be reduced by increasing the regularization parameter
λ of (18.10), it has the negative eﬀect of producing a blurry image.

Kernel Methods for Image Denoising
411
(a)
(b)
(c)
FIGURE 18.7: (a) Some of the functions that are used in the semi-
parametric modeling. (a) Erf(4x + 4y), (b) Erf(3x), (c) Erf(−(4x + 4y)2).
18.4
Semi-Parametric Modeling
Motivated by the KRR-L1 formulation, in this section we describe a more
advanced denoising method, by adopting the semi-parametric modeling (The-
orem 18.2), as the means to remedy the smoothing eﬀects associated with the
problem formulation of the previous section. To this end, we consider a set of
real valued two dimensional functions {ψk, k = 1, . . . , K}, suitably selected to
model edges. Although there can be numerous choices for this purpose, here
we consider three types of functions: a) bivariate polynomials of order 1, b)
functions of the form Erf(a · x + b · y + c), where Erf is the error function, i.e.,
Erf(x) =
2
√π
Z x
0
e−t2dt,
(which can approximate ridges - see Figure 18.7(a), (b)) and c) functions of
the form Exp(−(a · x + b · y + c)2) (see Figure 18.7(c)) for several suitable
predeﬁned choices of a, b and c.
The regularized risk minimization problem is now reformulated as follows:
minimize
g∈H, β∈RK,h∈R4 C(g, h, β) =
1
N 2
N−1
X
n=0
N−1
X
m=0
g(xn, ym) + h0 + h1xn + h2ym + h3xnym
(18.12)
+
K
X
k=1
βkψk(xn, ym) −zn,m
 +
λ
2N 2 ∥f∥2
H +
µ
2K
K
X
k=1
β2
k
+
µ1
2
3
X
l=1
h2
l ,
where β = (β1, . . . , βK), h = (h0, h1, h2, h3). In this case, we assume that
f∗belongs to the space H + Ψ + P, where Ψ = span{ψk, k = 1, . . . , K}
and P is the space of the bivariate polynomials of order 1. In other words,
we recast Problem (18.10), to account for some extra parameters, i.e., βk,

412
Regularization, Optimization, Kernels, and Support Vector Machines
k = 1, . . . , K, hi, i = 0, . . . , 3 (that contribute to the preservation of the
ﬁne details of the image), which are also regularized. In all the examples
presented below K = 32. This approach (learning edge models from a rich set
of basis functions) bears some similarities to the modeling taken by the K-SVD
algorithm [10]. According to Theorem 18.2, f∗will have a ﬁnite representation
of the form:
f∗(x, y) =
N−1
X
n=0
M−1
X
m=0
αn,mκ((xn, ym), (x, y))
+
K
X
k=1
βkψk(x, y) + h0 + h1x + h2y + h3xy.
(18.13)
To solve (18.12) the Polyak’s projected subgradient method is employed
once more. After some algebra we can deduce that the required subgradients
are:
∇C(g, h, β) =(∇Cg(g, h, β), ∇Ch0(f, h, β), ..., ∇Ch3(g, h, β),
∇Cβ1(g, h, β), . . . , ∇CβK(g, h, β))T ,
(18.14)
where
∇Cg(g, h, β) = 1
N 2
 N−1
X
n=0
N−1
X
m=0
sign (en,m(g, h, β)) ·
· κ ((xn, ym), (·, ·)) + λ · f

,
(18.15)
∇Ch0(g, h, β) = 1
N 2
 N−1
X
n=0
N−1
X
m=0
sign (en,m(g, h, β))
!
,
(18.16)
∇Ch1(g, h, β) = 1
N 2
 N−1
X
n=0
N−1
X
m=0
sign (en,m(g, h, β)) · xn
!
+ µ1 · h1,
(18.17)
∇Ch2(g, h, β) = 1
N 2
 N−1
X
n=0
N−1
X
m=0
sign (en,m(g, h, β)) · ym
!
+ µ1 · h2,
(18.18)
∇Ch3(g, h, β) = 1
N 2
 N−1
X
n=0
N−1
X
m=0
sign (en,m(g, h, β)) · xnym
!
+ µ1 · h3,
(18.19)

Kernel Methods for Image Denoising
413
and
∇Cβk(g, h, β) = 1
N 2
 N−1
X
n=0
N−1
X
m=0
sign (en,m(g, h, β)) ·
· ψk(xn, ym)

+ µ
K · βk,
(18.20)
for k = 1, . . . , K, where the en,m(g, h, β) term is given by:
en,m(g, h, β) =g(xn, ym) + h0 + h1xn + h2ym+
h3xnym +
K
X
k=1
βkψk(xn, ym) −zn,m.
(a)
(b)
(c)
(d)
FIGURE 18.8: (a) The Lena image corrupted by 20 dB Gaussian noise and
5% salt and pepper noise (PSNR = 17.78 dB). (b) The reconstructed Lena
image derived by the semi-parametric method with N = 5 (PSNR = 32.34
dB). (c) The Lena image corrupted by 20 dB Gaussian noise and 10% salt and
pepper noise (PSNR = 15.10 dB). (d) The reconstructed Lena image derived
by the semi-parametric method with N = 5 (PSNR = 31.69 dB).

414
Regularization, Optimization, Kernels, and Support Vector Machines
The regularization parameters µ, µ1 play an important role in the edge-
preservation properties of the algorithm. The proposed algorithm adjusts µ
and µ1 in an adaptive manner, so that they take small values in ROIs that con-
tain a lot of edges and large values in ROIs of smoother areas (the user deﬁned
regularization parameter λ is kept ﬁxed). The reason for this approach is that
small values for the regularization parameters, µ and µ1, enhance the contri-
bution of the semi-parametric part, which is desirable around edges, while in
smoother regions, this eﬀect needs to be suppressed. As the algorithm moves
from one ROI to the next, it solves Problem (18.12) using suitably selected
regularization parameters. More speciﬁcally, we consider six diﬀerent types of
ROIs, depending on how large the respective mean gradient of their pixels
is. Once the type of region has been decided, we assign values to µ and µ1
accordingly. This is accomplished using the values of the vectors µ and µ1 (6
elements each). The elements of those vectors, i.e., µi and µ1,i, i = 1, 2 . . . , 6,
contain the regularization values associated with any region of type i. For ex-
ample, if the algorithm decides that the current ROI belongs to a very smooth
area, it associates that ROI to the number 6 and sets µ = µ6 and µ1 = µ1,6. In
eﬀect, the proposed method gives higher weight to kernel smoothing in smooth
regions and performs sparse modeling for edge regions. A more thorough anal-
ysis of this algorithm with all the implementation details can be found in [4]
and the respective code can be found in bouboulis.mysch.gr/kernels.html. A
simpler version of the algorithm can be summarized in the following steps:
1. Input: The noisy image, the size of the ROIs, N, and the regularization
parameters λ, µ, µ1.
(a) Compute the mean gradient of all pixels in each ROI.
(b) Assign each ROI to a speciﬁc type, say i, according to the mean
gradient of its pixels.
(c) Set µ = µi and µ1 = µ1,i.
(d) Solve Problem 18.12 using the regularization parameters λ, µ, µ1
using the Polyak’s projected subgradient method.
(e) Move to the next ROI.
2. Output: The denoised image.
As is shown in Figure 18.8, the semi-parametric denoising method based
on the task given in (18.12) and Polyak’s subgradient method demonstrates
signiﬁcantly superior behavior compared to the state-of-the-art wavelet based
denoising methods (Figure 18.6), when both Gaussian and impulse noise are
present. The main drawback of this approach is an increased need for com-
putational resources. In fact, the algorithm (implemented in C) may require
more than 5 minutes to remove the noise from a typical 512 × 512 grayscale
picture. Moreover, in the case of images corrupted by Gaussian noise only,
wavelet-based methods are more eﬃcient (Figure 18.9).

Kernel Methods for Image Denoising
415
(a)
(b)
(c)
FIGURE 18.9: (a) The Lena image corrupted by 20 dB Gaussian noise
(PSNR = 25.67 dB). (b) The reconstructed Lena image derived by the semi-
parametric method (PSNR = 32.66 dB). (c) The reconstructed Lena image
derived by the BM3D [8] method with σ = 15 (PSNR = 34.6 dB).
18.5
A Robust Approach
Most image denoising methods (including the ones presented in the previ-
ous sections) adopt Equation (18.6) as a means to model the relation between
the noisy and the original image and minimize for diﬀerent kinds of empirical
loss, without explicitly taking into account the outliers. In this context, these
techniques are suitable for removing white Gaussian noise, as any other type
of noise that follows a heavy tailed distribution (e.g., Gamma distribution,
impulses, etc.), usually causes the model to overﬁt (especially when the model
parameters are chosen so that the ﬁne details of the image are preserved). On
the other hand, if the priority is to enhance the smoothing eﬀect and remove
the complete set of outliers, then the ﬁne details will probably be lost. To
resolve these issues, this section considers a more robust approach. The out-
liers are explicitly modeled as a sparse vector, u, and the popular orthogonal
matching pursuit (OMP) algorithm is employed to identify its support. Hence,
the relation between the noisy and the original image is modeled by
zi = f(xi) + ηi + ui,
(18.21)
where zi, i = 1, . . . , N 2 are the pixel values of the N ×N digitized image (rear-
ranged to become a vector, as Section 18.2 outlines), f(x) = PN 2
j=1 ajκ(·, xj)+
c, is the nonlinear function representing the noisy-free image, η is an unob-
servable bounded noise sequence, and u is a sparse vector representing the
possible outliers. Note that the image can be corrupted by any type of noise
(e.g., Gaussian) plus some outliers. In this case, we consider that η is the
noise contribution within some bounds, while beyond those bounds the noise

416
Regularization, Optimization, Kernels, and Support Vector Machines
(a)
(b)
(c)
FIGURE 18.10: (a) The Lena image has been corrupted by 20 dB Gaussian
noise and 5% salt and pepper noise (PSNR = 17.78 dB) and then reconstructed
using sparse modeling (KROMP). Model parameters were set to λ = 1, σ =
0.3, and ϵ = 45 (PSNR = 32.14 dB). (b) The Lena image has been corrupted
by 20 dB Gaussian noise and 10% salt and pepper noise (PSNR = 15.10 dB)
and then reconstructed via KROMP. Model parameters: λ = 1, σ = 0.35, and
ϵ = 45 (PSNR = 31.3 dB). (c) The Lena image has been corrupted by 20
dB Gaussian noise and then reconstructed using KROMP. Model parameters
were set to λ = 1, σ = 0.3, and ϵ = 45 (PSNR = 32.68 dB).
components are considered to be outliers. In this context, our objective is to
solve
minimize
a,u∈RN2c∈R
∥u∥0
subject to
N2
X
i=1

zi −
N 2
X
j=1
ajκ(xi, xj) −c −uj


2
+ λ(c2 + ∥a∥2) ≤ϵ,
(18.22)
where λ > 0 is the regularization parameter. Observe that the regularization
of the constraint in (18.22) does not employ the standard RKHS norm (i.e.,
∥f∥2
H = aT Ka), which excels in terms of generalization performance. Instead,
as our primary goal is to reduce the square error (and we do not care about
the generalization performance), we choose to directly penalize the elements
of a. Adopting matrix notation, (18.22) can be shown to take the form:
minimize
a,u∈RN2c∈R
∥u∥0
subject to
∥z −Aw∥2 + λwT Bw ≤ϵ,
(18.23)
where
A =
K
1
In

, w =


α
c
u

, B =


IN 2
0
ON2
0T
1
0T
ON 2
0
ON2

,

Kernel Methods for Image Denoising
417
Algorithm 16: Kernel Regularized OMP (KROMP)
Input: K, y, λ, ϵ
Initialization: k := 0
Sac = {1, 2, ..., n + 1}, Sinac = {n + 2, ..., 2n + 1}
Aac = [K 1], Ainac = In = [e1 · · · en]
Solve: w(0) := argminw||Aacw −z||2
2 + λ||α||2
2 + λc2
Initial Residual: r(0) = Aacw(0) −z
while ||r(k)||2 > ϵ do
k := k + 1
Find: jk := argmaxj∈Sinac|r(k−1)
j
|
Update Support:
Sac = Sac ∪{jk}, Sinac = Sinac −{jk}
Aac = [Aac ejk]
Update Current solution:
z(k) := argminz||Aacw −z||2
2 + λ||α||2
2 + λc2
Update Residual: r(k) = Aacw(k) −z
end while
Output: w = (α, c, u)T after k iterations
while IN 2 denotes the unitary matrix, 1 and 0 the vectors of ones and zeros,
respectively, and ON 2 the all zero square matrix. Problem 18.23 aims at ﬁnding
the sparsest vector of outliers, u, so that the mean square error between the
actual noisy data and the estimated noise-free data plus the outliers remains
below a certain tolerance, ϵ, while, at the same time, the solution remains
smooth. The latter is ensured by the regularization part of (18.23) as in KRR-
L2. The solution of (18.23), determines the noise-free image, i.e., yest = Ka+
c1.
To solve (18.23), we employ a two-step iterative approach. Firstly, A is
initialized as A(0) = [K
1]. The ﬁrst step adopts the OMP rationale to
compute the most correlated column, jk, of In with respect to the latest
residual, i.e., r(k) = Aw(k) −z, and then A is augmented to include the
ejk column. The second step attempts to solve minw J(w). It takes a few
lines of elementary algebra to conclude that J(w) attains a unique minimum,
which can be updated iteratively using Cholesky decomposition. Similar ap-
proaches, which are based on the OMP rationale to identify the support of
sparse vectors, have already been used in denoising tasks [5]. The procedure
proposed here is called Kernel Regularized OMP (KROMP) and it is sum-
marized in Algorithm 16. More details regarding KROMP, including conver-
gence properties, recovery of the support, and the solution of minw J(w) using
Cholesky decomposition (which is not shown here) can be found in [17, 3, 18].
In the following, we give only the major results, omitting the respective
proofs.

418
Regularization, Optimization, Kernels, and Support Vector Machines
Theorem 18.3. The norm of the residual vector, r(k) = Aacw(k)−z, in Algo-
rithm 16 is strictly decreasing. Moreover, the algorithm will always converge,
i.e., ∥r(k)∥2 will eventually drop below ϵ.
Theorem 18.4. Assuming that z admits a representation of the form z =
Ka0 + c01 + u0 + η, where ∥u∥0 = S and ∥η∥2 ≤ϵ, then Algorithm 16
recovers the exact sparsity pattern of u, after N steps, if
ϵ ≤

min
k=1,2,...,N 2{uk, uk ̸= 0}

2
−∥f0 −f (0)∥2,
with f0 =
 K
1
·
a0
c0

and f (0) =
 K
1
·
a(0)
c(0)

.
Theorem 18.5. Assuming that z admits a representation as in Theorem 18.4,
then the approximation error of Algorithm 16, after S steps, is bounded by
∥w(N) −w0∥≤|λ|∥(a0
c0)∥+
√
N
 1 + ∥(K
1)∥2

ϵ
λmin (ΩT Ω+ λI0)
,
where λmin
 ΩT Ω+ λI0

is the minimum eigenvalue of the matrix ΩT Ω+
λI0, where
Ω=
 K
1
I

,
I0 =
0
0
0
I

.
Similar to the case of KRR-L2, the user-selected parameter λ, controls the
quality of the reconstruction regarding the bounded noise. The parameter ϵ
on the other hand, controls the recovery of the outliers’ pattern. Small values
of ϵ lead to the recovery of all noise samples (even those originating from a
Gaussian source) as impulses, ﬁlling up the vector u, which will no longer be
sparse. Similarly, if λ is very small, f will closely ﬁt the noisy data including
some possible outliers (overﬁtting). In contrast, if ϵ is very large, only a handful
of outliers (possibly none) will be detected. If λ is large, then f will be smooth
resulting in a blurry picture.
Another important issue that needs to be addressed is that the accuracy
of ﬁt of any kernel-based regression technique drops near the borders of the
data set. Hence we need to take special care of these points. The proposed
denoising algorithmic scheme takes this important fact into consideration,
removing these points from the reconstructed image. The procedure splits
the noise image into small square ROIs, with dimensions N1 × N1. Then, it
applies the KROMP algorithm (see Algorithm 16) sequentially to each ROI.
However, only the centered N2 × N2 pixels are used to reconstruct the noise
free image. Hence, in order to avoid gaps in the ﬁnal reconstructed image, the
ROIs contain overlapping sections (Figure 18.11).
It is evident that this procedure requires three user deﬁned parameters: (a)
the regularization parameter, λ, (b) the parameter ϵ that controls the outlier

Kernel Methods for Image Denoising
419
FIGURE 18.11: Although each ROI contains N1 × N1 pixels (dark gray),
only N2 × N2 of them are used for the reconstruction of the image. This is
to account for the reduced accuracy of the reconstruction at the pixels lying
close to the borders of the respective ROI. Hence, each ROI has overlapping
sections with its neighbors.
selection of the OMP mechanism, and (c) the Gaussian kernel parameter σ.
The size of the ROIs can also be adjusted by the user, but in the following
we will consider that N1 = 12 and N2 = 8. As λ regulates the smoothness of
the estimation, it is controlled in an adaptive manner, so that smaller values
of λ are used in areas of the picture that contain edges. Hence, while the user
deﬁnes a single value for λ, say λ0, the denoising scheme computes the mean
gradient magnitude of each ROI and classiﬁes them into three categories.
The ROIs of the ﬁrst class, i.e., those with the largest values of gradient
magnitude, solve (18.23) using λ = λ0, while for the other two classes the
algorithm employs λ = 5 ∗λ0 and λ = 15 ∗λ0, respectively. Such choices
came out after extensive experimentation, and the algorithm is not sensitive
to them. The denoising scheme can be summarized in the following steps:
• Compute the mean gradient magnitude of each N2 × N2 ROI.
• Classify them into three categories.
• For each N2 × N2 ROI, say ˆR:
1. Extend the ROI to size N1 × N1. Let ˆRe be the extended ROI.
2. Apply the KROMP algorithm to the extended ROI using the re-
spective λ. Let Re be the output of the algorithm.
3. Reconstruct the noise-free ROI, R, taking only the centered N2×N2
points of Re.
Compared to the semi-parametric case (Section 18.4), the present denois-
ing approach oﬀers comparable performance (see Figures 18.9 and 18.10),
while at the same time it admits signiﬁcantly reduced complexity. While a
typical semi-parametric denoising task requires several minutes to complete,

420
Regularization, Optimization, Kernels, and Support Vector Machines
(a)
(b)
(c)
(d)
FIGURE 18.12: (a) The Lena image corrupted by 20 dB Gaussian noise and
5% impulses generated by a uniform distribution in the interval [−300, 300]
(PSNR = 19.23 dB). (b) The reconstructed Lena image derived by BM3D [8]
with σ = 40 (PSNR = 30.6 dB). (c) The reconstructed Lena image derived
by the semi-parametric model with N = 5 (PSNR = 32.39 dB). (d) The
reconstructed Lena image derived by KROMP with σ = 0.3, λ = 1, and
ϵ = 40 (PSNR = 32.14 dB).
the denoising approach oﬀered by KROMP takes only a few seconds. This is
due to slow convergence of the subgradient technique adopted in the semi-
parametric scenario. Table 18.1 compares the performance of the denoising
method based on KROMP with the wavelet based denoising method BM3D
presented in [8]. It is shown that KROMP oﬀers superior results when the
larger portion of the noise is due to outliers. In cases where the Gaussian
noise is dominant, the BM3D oﬀers comparable or even better performance
(Table 18.1).

Kernel Methods for Image Denoising
421
TABLE 18.1: Comparing the wavelet-based BM3D method of [8] with KROMP. In the case of the impulse noise the
term “sp” stands for “salt and pepper” noise, while the term “u” stands for uniform impulses in the intraval [−300, 300].
Image
Noise
PSNR
Denoising Method
BM3D
KROMP
Gaussian
Impulse
PSNR
parameters
PSNR
parameters
Lena
20 dB
5% sp
17.78 dB
29.70 dB
σ = 45
32.14 dB
σ = 0.3, λ = 1, ϵ = 45
Lena
20 dB
10% sp
15.10 dB
28.30 dB
σ = 45
31.31 dB
σ = 0.35, λ = 1, ϵ = 45
Lena
20 dB
5% u
19.23 dB
30.60 dB
σ = 45
32.14 dB
σ = 0.3, λ = 1, ϵ = 45
Lena
20 dB
10% u
16.80 dB
29.43 dB
σ = 45
31.55 dB
σ = 0.35, λ = 1, ϵ = 45
Lena
15 dB
5% sp
16.47 dB
29.30 dB
σ = 45
29.71 dB
σ = 0.4, λ = 2, ϵ = 70
Lena
15 dB
10% sp
14.42 dB
27.90 dB
σ = 45
29.11 dB
σ = 0.4, λ = 2, ϵ = 70
Lena
15 dB
5% u
17.63 dB
30.31 dB
σ = 40
29.80 dB
σ = 0.35, λ = 1.5, ϵ = 70
Lena
15 dB
10% u
15.84 dB
29.02 dB
σ = 45
29.18 dB
σ = 0.4, λ = 2, ϵ = 70
Boat
20 dB
5% sp
17.52 dB
27.82 dB
σ = 45
29.74 dB
σ = 0.3, λ = 1, ϵ = 45
Boat
20 dB
10% sp
14.92 dB
26.59 dB
σ = 45
29.10 dB
σ = 0.3, λ = 1, ϵ = 45
Boat
20 dB
5% u
19.08 dB
28.81 dB
σ = 40
29.79 dB
σ = 0.3, λ = 1, ϵ = 45
Boat
20 dB
10% u
16.65 dB
27.76 dB
σ = 40
29.39 dB
σ = 0.3, λ = 1, ϵ = 45
Boat
15 dB
5% sp
16.16 dB
27.41 dB
σ = 45
27.60 dB
σ = 0.35, λ = 1.5, ϵ = 70
Boat
15 dB
10% sp
14.16 dB
26.21 dB
σ = 45
27.08 dB
σ = 0.35, λ = 1.5, ϵ = 70
Boat
15 dB
5% u
17.25 dB
28.48 dB
σ = 35
27.68 dB
σ = 0.35, λ = 1.5, ϵ = 70
Boat
15 dB
10% u
15.57 dB
27.25 dB
σ = 45
27.12 dB
σ = 0.35, λ = 1.5, ϵ = 70

422
Regularization, Optimization, Kernels, and Support Vector Machines
18.6
Conclusions
Motivated by the Kernel Ridge Regression scheme and the theory of sparse
representations, we presented two methods suitable for image denoising tasks.
Both techniques employ the popular Gaussian kernel and change their param-
eters adaptively in each image region according to the amount of ﬁne details
contained. Besides the RKHS modeling (based on the semi-parametric repre-
senter theorem), the ﬁrst method employs a rich basis of functions to model
image edges and minimizes a regularized L1 error function to account for out-
liers. In contrast, the second method models the outliers explicitly using a
sparse vector, u, and minimizes the L0 norm of u, over the set where the
L2 error function remains small. Each outlier is selected iteratively using an
orthogonal matching pursuit rationale and the procedure is terminated when
the L2 error drops below a predeﬁned threshold. Both methods excel, com-
pared to the most up-to-date wavelet methods, when the underlying noise has
strong heavy tailed components (e.g., impulses), as it is demonstrated through
experiments. Although the ﬁrst method oﬀers slightly better performance, it
has signiﬁcantly increased complexity, due to slow convergence (a common
disadvantage of Polyak’s method). On the other hand, the second method
oﬀers comparable complexity with the latest wavelet based methods.
Bibliography
[1] E. Abreu, M. Lightstone, S. Mitra, and K. Arakawa.
A new eﬃcient
approach for the removal of impulse noise from highly corrupted images.
IEEE Trans. Im. Proc., 5:1012–1025, 1996.
[2] E. Besdok. Impulsive noise suppression from images by using anﬁs inter-
polant and lillietest. EURASIP J. Appl. Si. Pr., 16:2423–2433, 2004.
[3] P. Bouboulis, G. Papageorgiou, and S. Theodoridis. Robust image de-
noising in rkhs via orthogonal matching pursuit. In ICASSP, 2014 to
appear.
[4] P. Bouboulis, K. Slavakis, and S. Theodoridis.
Adaptive kernel-based
image denoising employing semi-parametric regularization. IEEE Trans-
actions on Image Processing, 19(6):1465–1479, 2010.
[5] Alfred M. Bruckstein, David L. Donoho, and Michael Elad. From sparse
solutions of systems of equations to sparse modeling of signals and images.
SIAM review, 51(1):34–81, 2009.

Kernel Methods for Image Denoising
423
[6] P. L. Combettes and J.-C. Pesquet. Image restoration subject to a total
variation constraint. IEEE Trans. Im. Proc., 13(9):1213–1222, 2004.
[7] N. Cristianini and J. Shawe-Taylor. An introduction to support vector
machines and other kernel-based learning methods. Cambridge University
Press, 2000.
[8] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by
sparse 3d transform-domain collaborative ﬁltering. IEEE Transactions
on Image Processing, 16(8):2080–2095, 2007.
[9] S. Durand and J. Froment.
Reconstruction of wavelet coeﬃcients us-
ing total variation minimization. SIAM J. Sci. Comput., 24:1754–1767,
2003.
[10] M. Elad and M. Aharon. Image denoising via sparse and redundant repre-
sentations over learned dictionaries. IEEE Tran. Im. Proc., 15(12):3736–
3745, 2006.
[11] R. C. Gonzalez and R.E. Woods. Digital Image Processing. Prentice Hall,
2002.
[12] K. Kim, M. O. Franz, and B. Scholkopf. Iterative kernel principal com-
ponent analysis for image modeling. IEEE Trans. Pattern Anal. Mach.
Intell., 27(9):1351–1366, 2005.
[13] G. S. Kimeldorf and G. Wahba. Some results on Tchebycheﬃan spline
functions. J. Math. Anal. Applic., 33:82–95, 1971.
[14] Dalong Li.
Support vector regression based image denoising.
Image
Vision Comput., 27:623–627, 2009.
[15] J. Liu, X.-C. Tai, H. Huang, and Z. Huan. A weighted dictionary learning
model for denoising images corrupted by mixed noise. Image Processing,
IEEE Transactions on, 22(3):1108–1120, 2013.
[16] S. Osher and L. I. Rudin. Feature-oriented image enhancement using
shock ﬁlters. SIAM J. Numer. Anal., 27:919–940, 1990.
[17] G. Papageorgiou, P. Bouboulis, and S. Theodoridis. Robust kernel-based
regression using orthogonal matching pursuit.
In MLSP, September
2013.
[18] G. Papageorgiou, P. Bouboulis, and S. Theodoridis. Robust kernel-based
regression using orthogonal matching pursuit with applications to image
denoising. to appear.
[19] M. Petrou and C. Petrou. Image Processing: The Fundamentals,. Wiley,
2nd edition, 2010.

424
Regularization, Optimization, Kernels, and Support Vector Machines
[20] B. T. Polyak. Introduction to Optimization. New York: Optimization
Software, 1987.
[21] J. Portilla, V. Strela, M. Wainwright, and E. P. Simoncelli. Image de-
noising using scale mixtures of gaussians in the wavelet domain. IEEE
Transactions on Image Processing, 12(11):1338–1351, 2003.
[22] R. Garnett, T. Huegerich, and C. Chui. A universal noise removal al-
gorithm with an impulse detector. IEEE Trans. Im. Proc., 14(11):1747–
1754, 2005.
[23] L. I. Rudin and S. Osher. Total variation based image restoration with
free local constraints. Proc. IEEE Int. Conf. Image Processing, 1:31–35,
1994.
[24] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based
noise removal algorithms. Physica D., 60:259–268, 1992.
[25] B. Scholkopf and A.J. Smola.
Learning with Kernels: Support Vector
Machines, Regularization, Optimization and Beyond. MIT Press, 2002.
[26] L. Sendur and I.W. Selesnick. Bivariate shrinkage functions for wavelet-
based denoising exploiting interscale dependency. IEEE Trans. Signal
Process., 50(11):2744–2756, 2002.
[27] K. Seongjai. PDE-based image restoration: A hybrid model and color
image denoising. IEEE Trans. Im. Proc., 15(5):1163–1170, 2006.
[28] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis.
Cambridge University Press, 2004.
[29] K. Slavakis, P. Bouboulis, and S. Theodoridis. Online learning in Repro-
ducing Kernel Hilbert Spaces. In Rama Chellappa and Sergios Theodor-
idis, editors, Academic Press Library in Signal Processing, volume 1, Sig-
nal Processing Theory and Machine Learning, Chapter 17, pages 883–987.
Academic Press, 2014.
[30] J.A.K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. Van-
dewalle. Least Squares Support Vector Machines. World Scientiﬁc, Sin-
gapore, 2002.
[31] H. Takeda, S. Farsiu, and P. Milanfar. Kernel regression for image pro-
cessing and reconstruction. IEEE Tran. Im. Proc., 16(2):349–366, 2007.
[32] S. Theodoridis and K. Koutroumbas. Pattern Recognition. Academic
Press, 4th edition, Nov. 2008.
[33] V.N. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag,
1999.

Kernel Methods for Image Denoising
425
[34] Alan L. Yuille and Norberto M. Grzywacz. A mathematical analysis of
the motion coherence theory. International Journal of Computer Vision,
3(2):155–175, 1989.
[35] S. Zhang and Y. Chen. Image denoising based on wavelet support vector
machine. IICCIAS 2006.

This page intentionally left blank
This page intentionally left blank

Chapter 19
Single-Source Domain Adaptation
with Target and Conditional Shift
Kun Zhang
Max Planck Institute for Intelligent Systems
Bernhard Schölkopf
Max Planck Institute for Intelligent Systems
Krikamol Muandet
Max Planck Institute for Intelligent Systems
Zhikun Wang
Max Planck Institute for Intelligent Systems
Zhi-Hua Zhou
National Key Laboratory for Novel Software Technology, Nanjing University
Claudio Persello
Max Planck Institute for Intelligent Systems and University of Trento
19.1
Introduction ......................................................
428
19.2
Distribution Shift Correction ....................................
430
19.2.1
Importance Reweighting .................................
430
19.2.2
Sample Transformation and Reweighting ...............
431
19.2.3
Classiﬁcation and Regression Machines .................
431
19.3
Correction for Target Shift .......................................
432
19.3.1
Assumptions .............................................
433
19.3.2
A Kernel Mean Matching Approach ....................
433
19.4
Location-Scale Conditional Shift .................................
436
19.4.1
Assumptions and Identiﬁability .........................
437
19.4.2
A Kernel Approach ......................................
438
19.5
Location-Scale Generalized Target Shift .........................
440
19.6
Determination of Hyperparameters ..............................
441
19.7
Simulations .......................................................
442
19.8
Experiments on Pseudo Real-World Data .......................
444
19.9
Experiments on Real-World Data ...............................
446
19.9.1
Regression under TarS ...................................
446
427

428
Regularization, Optimization, Kernels, and Support Vector Machines
19.9.2
Remote Sensing Image Classiﬁcation ....................
446
19.10
Conclusion and Discussions ......................................
450
Acknowledgments ................................................
453
Bibliography ......................................................
453
19.1
Introduction
The goal of supervised learning is to infer a function f from a training set
Dtr = {(xtr
1 , ytr
1 ), ..., (xtr
m, ytr
m)} ⊆X × Y, where X and Y denote the domains
of predictors X and target Y , respectively. The estimated f is expected to
generalize well on the test set Dte = {(xte
1 , yte
1 ), ..., (xte
n , yte
n )} ⊆X × Y, where
yte
i
are unknown. Traditionally, the training set and test set are assumed
to follow the same distribution. However, in many real world problems, the
training data and test data have diﬀerent distributions, i.e., P tr
XY ̸= P te
XY ,1 and
the goal is to ﬁnd a learning machine that performs well on the test domain.
This problem is known as domain adaptation in machine learning.
If the data distribution changes arbitrarily, training data would be of no
use to make predictions on the test domain. To perform domain adaptation
successfully, relevant knowledge in the training (or source) domain should be
transferred to the test (or target) domain. For instance, the situation where
P tr
XY and P te
XY only diﬀer in the marginal distribution of the covariate (i.e.,
P tr
X ̸= P te
X , while P tr
Y |X = P te
Y |X) is termed covariate shift [25, 33, 10] or
sample selection bias [37], and has been well studied. For surveys on domain
adaptation for classiﬁcation, see, e.g., [13, 17, 1].
In particular, we address the situation where both the marginal distribu-
tion PX and the conditional distribution PY |X may change across the domains.
Clearly, we need to make certain assumptions for the training domain to be
adaptable to the test domain. We ﬁrst consider the case where PX|Y is the
same on both domains. As a consequence of Bayes’ rule, the changes in PX
and PY |X are caused by the change in PY , the marginal distribution of the
target variable. We term this situation target shift (TarS) which is frequently
encountered in practice; for instance, it is known as choice-based or endoge-
nous stratiﬁed sampling [15] in econometrics, and is sometimes called prior
probability shift [31].
We further discuss the situation where PY remains the same, while PX|Y
changes, as termed conditional shift (ConS). Estimation of P te
X|Y under ConS
is in general ill-posed; we consider a rather practical yet identiﬁable case where
PX|Y changes under location-scale (LS) transformations on X. We show how
to transform the training points to mimic the distribution of test data and
facilitate learning on the test domain. Finally, the situation in which both PY
and PX|Y change across domains is termed generalized target shift (GeTarS);
1We use P to denote the probability density or mass function.

Single-Source Domain Adaptation with Target and Conditional Shift
429
we focus on LS-GeTarS, i.e., GeTarS with PX|Y changes under LS transfor-
mations, and propose practical methods to estimate both changes, making
domain adaptation possible.
Recently there have been major advances in discovering causal information
from purely observational data [29, 18, 24, 9, 38, 39, 16, 11, 40]. In particular,
it has been shown that causal information can be derived from changes in
data distributions [34]; on the other hand, knowledge of the data generating
process, or causal knowledge, would imply how the data distribution changes
across domains and helps in domain adaptation. It has been demonstrated
that a number of learning tasks, especially semi-supervised learning, can be
understood from the causal point of view [22]. The problems studied here,
TarS, ConS, and GeTarS, have clear causal interpretations. Throughout this
chapter, we assume that Y is a cause of X.2 If we further know that X depends
on the domain (or selection variable) only via Y , we have the TarS situation:
the marginal distribution of the cause, PY , describes the process that generates
Y in the domain, and PX|Y describes the data generating mechanism for X
from the cause Y , which is independent of the domain. According to [35], the
invariance of PX|Y with respect to the change in PY is one of the features
of the causal system Y →X. Consider the clinical diagnosis as an example.
The disease is naturally considered as the cause of symptoms; moreover, the
marginal distribution of the disease could change across diﬀerent regions, but
the conditional distribution of the symptoms given the disease is expected to
be invariant. Furthermore, if both Y and the domain are causes of X while
Y is independent of the domain, we have the ConS situation. More generally,
the situation where Y is a cause of X and both PY and PX|Y depend on the
domain corresponds to GeTarS.
In the classiﬁcation scenario, target shift was referred to the class imbal-
ance problem by [12]. To solve it, sometimes it is assumed that P te
Y is known
a priori [14], or that some knowledge about the change in PY is known [36].
However, this is usually not the case in practice. In [2], the authors proposed
to estimate P te
Y with an expectation-maximization (EM) algorithm. Unfortu-
nately, this approach has to estimate P tr
X|Y , which is a diﬃcult task if the
dimensionality of X is high; moreover, it does not apply to regression prob-
lems. In fact, lack of information on P te
Y causes the main diﬃculty in domain
adaptation under TarS.
In this chapter we provide practical approaches for domain adaptation
under TarS, LS-ConS, and LS-GeTarS, by sample importance reweighting or
sample transformation.3 The approach for TarS also applies to regression.
Kernel embedding of both conditional and marginal distributions provides a
2This is usually the case, especially for classiﬁcation: In many cases features were gen-
erated from classes, while the latter are hidden and to be estimated. For instance, in hand-
writing digit recognition, the written digit image was generated from the digit one intended
to write, which is to be estimated from the image.
3A preliminary version was presented at the 30th International Conference on Machine
Learning [41].

430
Regularization, Optimization, Kernels, and Support Vector Machines
convenient tool to estimate the importance weights and the sample transfor-
mations. With it, we are able to avoid estimating any distribution explicitly,
and the proposed approaches apply to high-dimensional problems without any
diﬃculty. We note that kernel distribution embedding has been used to correct
for covariate shift in [10, 6], but the studied problems are inherently diﬀerent:
they used the kernel mean matching to estimate the ratio P te
X /P tr
X , avoiding
estimating P te
X and P tr
X explicitly from data; in our problems we are interested
in how P te
Y is diﬀerent from P tr
Y (for TarS and GeTarS) or how P tr
X|Y changes
to P te
X|Y (for ConS and GeTarS), but there are no data points available to
estimate P te
Y or P te
X|Y , making the problems much more diﬃcult to solve.
19.2
Distribution Shift Correction
In this section, we outline two diﬀerent frameworks for distribution shift
correction, namely, importance reweighting and sample transformation, and
brieﬂy introduce the classiﬁcation and regression machines used in this chap-
ter.
19.2.1
Importance Reweighting
We aim to ﬁnd the function f(x) that minimizes the expected loss on test
data. Assume the support of P te
XY is contained by that of P tr
XY . The expected
loss is
R[P te, θ, L(x, y; θ)] = E(X,Y )∼P te[L(x, y; θ)]
=
Z
P tr
XY · P te
XY
P tr
XY
· L(x, y, θ)dxdy
=E(X,Y )∼P tr [β∗(y) · γ∗(x, y) · L(x, y; θ)] ,
(19.1)
where θ denotes the parameters in the loss function L(x, y; θ), β∗(y) ≜P te
Y /P tr
Y
and γ∗(x, y) ≜P te
X|Y /P tr
X|Y . Here we factorize PXY as PY PX|Y instead of
PXPY |X because it provides a more convenient way to handle the change in
PXY , according to our assumptions given later. In practice, we minimize the
empirical loss:
bR[P te, θ, L(x, y; θ)] = 1
m
m
X
i=1
β∗(ytr
i )γ∗(xtr
i , ytr
i )L(xtr
i , ytr
i ; θ),
(19.2)
to ﬁnd the supervised learning machine that is expected to work well on test
data, if β∗(ytr
i )γ∗(xtr
i , ytr
i ) are given.

Single-Source Domain Adaptation with Target and Conditional Shift
431
Note that although (19.2) converges almost surely to (19.1), it could have
a very high or even inﬁnite variance if in some regions P tr
XY is very small while
P te
XY is large—in this situation, a small number of data points in such regions
would have high weights, so that the learning machine is very sensitive to the
locations of such points, i.e., it has high random errors [20]. Readers who are
interested in how to reduce the variance of the empirical expected loss and
the corresponding learning machine may refer to, e.g., [25].
19.2.2
Sample Transformation and Reweighting
Sample reweighting only applies when the support of P te
XY is contained in
that of P tr
XY ; even under this condition, it is usually very diﬃcult to estimate
γ∗(x, y) without prior knowledge on how PX|Y changes. Therefore, in the case
where both PY and PX|Y change, the application of the sample reweighting
scheme is rather limited. Instead, if we can ﬁnd the transformation from P tr
X|Y
to P te
X|Y , i.e., ﬁnd the transformation T such that the conditional distribution
of Xnew = T (Xtr, Y tr) satisﬁes P new
X|Y = P te
X|Y , we can calculate the expected
loss on the test domain:
R[P te, θ, L(x, y; θ)] = E(X,Y )∼P te[L(x, y; θ)]
=
Z
P tr
Y · β∗(y) · P te
X|Y · L(x, y; θ)dxdy
=
Z
P tr
Y · β∗(y) · P new
X|Y · L(x, y; θ)dxdy,
= E(X,Y )∼P new
XY [β∗(y) · L(x, y; θ)] .
Note that Y tr is an argument of the transformation T , i.e., T might be diﬀer-
ent at diﬀerent Y values. This empirical loss can be calculated on the trans-
formed training points (xnew, ytr) with weights β∗:
bR[P te, θ, L(x, y; θ)] = 1
m
m
X
i=1
β∗(ytr
i )L(xnew
i
, ytr
i ; θ).
(19.3)
19.2.3
Classiﬁcation and Regression Machines
We consider both classiﬁcation and regression problems. For the former
problem, we adopt the support vector machine (SVM), and for the latter we
use the kernel ridge regression (KRR). The standard formulation of both SVM
and KRR can be straightforwardly modiﬁed to incorporate the importance
weights according to (19.2) and (19.3). All parameters in the learning machines
(e.g., the kernel width and regularization parameter) are selected by cross-
validation.
Reweighted support vector classiﬁcation: Support vector classi-
ﬁers can be extended to incorporate non-uniform importance weights of the

432
Regularization, Optimization, Kernels, and Support Vector Machines
training instances. Associated to each training instance is the importance
weight β∗(yi)γ∗(xi, yi), which can be incorporated into (19.2) via the follow-
ing minimization problem:
minimize
θ,b,ξ
1
2∥θ∥2 + C
n
X
i=1
β∗(yi)γ∗(xi, yi)ξi
(19.4a)
subject to
yi(⟨θ, φ(xi)⟩+ b) ≥1 −ξi, ξi ≥0
(19.4b)
where φ(x) is a feature map from X to a feature space F. The dual of the
above problem is
minimize
α
1
2
n
X
i=1
n
X
j=1
αiαjk(xi, xj) −
n
X
i=1
αi
(19.5a)
subject to
0 ≤αi ≤β∗(yi)γ∗(xi, yi)C,
(19.5b)
n
X
i=1
αiyi = 0
(19.5c)
Here k(x, x′) ≜⟨φ(x), φ(x′)⟩F denotes the inner product between the fea-
ture maps. We have modiﬁed the LIBSVM implementation4 for reweighted
instances.
Reweighted kernel ridge regression (KRR): The original kernel ridge
regression [21] represents the vector of ﬁtted target values as f = Kc, where
K is the kernel matrix of xtr, and ﬁnd the estimate of c by minimizing
(ytr −Kc)⊺(ytr −Kc)+λxc⊺Kc. The estimate is ˆc = (K +λxI)−1ytr and con-
sequently, the ﬁtted target values are ˆf = Kˆc = K(K + λcI)−1ytr. Similarly,
the reweighted kernel ridge regression minimizes (ytr −Kc)⊺· diag{β∗(ytr) ⊙
γ∗(xtr, ytr)}·(ytr −Kc)+λxc⊺Kc, where ⊙denotes the Hadamard (or entry-
wise) product. This gives ˆc = [K +λxdiag−1{β∗(ytr)⊙γ∗(xtr, ytr)}]−1ytr and
hence, the ﬁtted values are f = K[K+λx·diag−1{β∗(ytr)⊙γ∗(xtr, ytr)}]−1ytr.
19.3
Correction for Target Shift
Unfortunately, unlike the covariate shift, the weights β∗(yi)γ∗(xi, yi) can-
not be directly estimated because P te
Y and P te
X|Y are unknown on the test data.
Below we ﬁrst consider the situation where P te
X|Y = P tr
X|Y , i.e., γ∗(x, y) ≡1,
and propose a practical method to estimate β∗(ytr) as well as P te
Y based on
kernel embedding of conditional and marginal distributions. In subsequent
sections, we further extend the results to the case where not only does PY
change, but PX|Y is also allowed to change.
4http://www.csie.ntu.edu.tw/~cjlin/libsvm/ .

Single-Source Domain Adaptation with Target and Conditional Shift
433
19.3.1
Assumptions
We ﬁrst consider Target Shift (TarS):
ATarS
1
: P te
X|Y = P tr
X|Y and P te
Y ̸= P tr
Y .
domain
Y
X
FIGURE 19.1: A causal model for TarS.
That is, the diﬀerence between
P tr
XY and P te
XY is caused by a shift
in target distribution PY . Fig-
ure 19.1 shows a causal interpre-
tation of TarS. For classiﬁcation
problems, it is possible to estimate P te
Y
in an iterative way by maximizing
the likelihood on xte, for instance, with the EM algorithm [2]; however, such
approaches involve estimation of P tr
X|Y explicitly, which is diﬃcult for high-
dimensional problems. They are also not practical for regression.
We make the following assumptions on P te
Y and P tr
X|Y :
ATarS
2
: The support of P te
Y is contained in the support of P tr
Y (i.e., roughly
speaking, the training set is richer than the test set).
ATarS
3
: There exists only one possible distribution of Y that, together with
P tr
X|Y , leads to P te
X .
Imagine that we can draw a biased sample from the training data; here the
selection variable depends only on Y , i.e., it is independent of X given Y .
Denote by P new(·) the distribution on this sample. Note that P new
X|Y = P tr
X|Y =
P te
X|Y . Thus, we can make P new
X
identical to P te
X by adjusting P new
Y
.
Let β(y) be the ratio of the P new
Y
to P tr
Y , i.e., P new
Y
= β(y) · P tr
Y . To
make P new
X
identical to P te
X , we can adjust β(y) to minimize D(P te
X , P new
X
) =
D
 P te
X ,
R
P tr
Y β(y)P tr
X|Y dy

, where D measures the diﬀerence between two dis-
tributions; it can be the mean square error or the Kullback-Leibler distance.
To solve this problem, we have to estimate P tr
X|Y and P tr
X from the training
set, and moreover, the integral makes optimization very diﬃcult.
19.3.2
A Kernel Mean Matching Approach
Instead, we solve this problem by making use of the kernel mean embedding
of the marginal and conditional distributions; see Table 19.1 for the notation
we use. The kernel mean embedding of PX [26, 5] is a point in the reproducing
kernel Hilbert space (RKHS) given by µ[PX] = EX∼PX[ψ(X)], and its empir-
ical estimate is ˆµ[PX] =
1
m
Pm
i=1 ψ(xi). The embedding of the conditional
distribution has been studied by [28, 27]. The embedding of PX|Y can be con-
sidered as an operator mapping from G to F, deﬁned as U[PX|Y ] = CXY C−1
Y Y ,
where CXY and CY Y denote the (uncentered) cross-covariance and covariance
operators, respectively [3]. Furthermore, we have
µ[PX] = U[PX|Y ]µ[PY ].
We make the following assumption on the kernel k for X and l for Y :

434
Regularization, Optimization, Kernels, and Support Vector Machines
TABLE 19.1: Notation used in this chapter.
random variable
X
Y
domain
X
Y
observation
x
y
data matrix
x
y
kernel
k(x, x′)
l(y, y′)
kernel matrix on training set
K
L
feature map
ψ(x)
φ(y)
feature matrix on training set
Ψ
Φ
RKHS
F
G
ATarS
4
: Product kernel kl on X × Y is characteristic.
For characteristic kernels, the kernel mean map µ from the space of the dis-
tribution to the RKHS is injective, meaning that all information of the dis-
tribution is preserved [4, 30]. In this chapter we use the Gaussian kernel, i.e.,
k(xi, xj) = exp
 −||xi−xj||2
2σ2

, where σ is the kernel width. Note that under as-
sumptions ATarS
3
and ATarS
4
, for the embedding U[P tr
X|Y ], which is a mapping
from G to F, the pre-image of µ[P te
X ] is unique.
The kernel mean embedding of P new
Y
is
µ[P new
Y
] = EY ∼P new
Y
[φ(Y )] = EY ∼P tr
Y [β(y)φ(Y )].
(19.6)
The embedding of P new
X
is then given by µ[P new
X
] = U[P tr
X|Y ]µ[P new
Y
]. Conse-
quently, in the population version, we can ﬁnd β(y) by minimizing the maxi-
mum mean discrepancy (MMD):
µ[P new
X
] −µ[P te
X ]
 =
U[P tr
X|Y ]µ[P new
Y
] −µ[P te
X ]

=
U[P tr
X|Y ]EY ∼P tr
Y [β(y)φ(y)] −µ[P te
X ]
,
(19.7)
subject to β(y) ≥0 and EP tr
Y [β(y)] = 1, which guarantees that P new
Y
=
β(y)P tr
Y is a valid distribution.
Theorem 19.1. Under assumptions ATarS
2
, ATarS
3
, and ATarS
4
, the minimiza-
tion problem (19.7) is convex in β. Further suppose ATarS
1
holds. Then the
solution to (19.7) is β(y) = P te
Y (y)
P tr
Y (y).
Proof. In (19.7), U[P tr
X|Y ] is a linear operator, EY ∼P tr
Y [β(y)φ(y)] is linear in β.
Further note that the constraints are convex. We can see that the optimization
problem (19.7) is convex in β.
According to assumption ATarS
1
, we have µ[P te
X ] = U[P tr
X|Y ]µ[P te
Y ], and the
function in (19.7) reduces to

U[P tr
X|Y ] ·

EY ∼P tr
Y [β(y)φ(y)] −µ[P te
X ]
	
.

Single-Source Domain Adaptation with Target and Conditional Shift
435
It achieves zero, which is clearly a minimum, when EY ∼P tr
Y [β(y)φ(y)] = µ[P te
Y ].
It is equivalent to β(y)P tr
Y (y) = P te
Y (y), since the kernel l is characteristic.
Moreover, combining assumptions ATarS
1
and ATarS
4
implies that there is no
other solution of β(y) to (19.7).
In practice we have to use an empirical version. The empirical estimate
of UX|Y is ˆUX|Y = Ψ(L + λI)−1Φ⊺. Recall that m and n are the sizes of the
training and test sets. Denote by 1n the vector of 1’s of length n, and by Kc
the “cross” kernel matrix between xte and xtr , i.e., Kc
ij = k(xte
i , xtr
j ). Let
β stand for β(ytr) and βi for β(ytr
i ). The empirical version of the square of
(19.7) is

 ˆUX|Y · 1
m
m
X
i=1
βiφ(ytr
i ) −1
n
n
X
i=1
ψ(xte
i )


2
=

 1
m
ˆUX|Y φ(ytr)β −1
nψ(xte)1n


2
=
1
m2 β⊺φ⊺(ytr) ˆU⊺
X|Y ˆUX|Y φ(ytr)β −
2
mn1⊺
nψ⊺(xte) ˆUX|Y φ(ytr)β + const
=
1
m2 β⊺L(L + λI)−1K(L + λI)−1Lβ −
2
mn1⊺
nKc(L + λI)−1Lβ + const
=
1
m2 β⊺ΩKΩT
| {z }
≜A
β −
2
mn 1⊺
nKcΩT
|
{z
}
≜M
β + const,
(19.8)
where we use short-hand notation Ω≜L(L+λI)−1. As shown by [10, Lemma
3], if βi ∈[0, Bβ], i.e., Bβ is the upper bound of β, given that βi has ﬁnite mean
and non-zero variance, the sample mean
1
m
Pm
i=1 βi converges in distribution
to a Gaussian variable with mean EP tr
Y [β(y)] and standard deviation bounded
by
Bβ
2√m. As EP tr
Y [β(y)] = 1, we have the following constrained quadratic
programming (QP) problem:
minimize
β
1
2β⊺Aβ −m
n Mβ,
s.t.
βi ∈[0, Bβ] and

m
X
i=1
βi −m
 ≤mϵ,
where a good choice of ϵ is O

B
2√m

.
Note that β estimated this way is not necessarily a function of y: diﬀerent
data points in the training set with the same y value could correspond to
diﬀerent β values. We also found that the β values estimated by solving the
above optimization problem usually change dramatically along with y. We
can improve the estimation quality of β by making use of reparameterization.
First consider the case where Y is discrete. Let C be the cardinality of Y and
denote by v1, ..., vC its possible values. We can deﬁne a matrix R(d) where

436
Regularization, Optimization, Kernels, and Support Vector Machines
R(d)
iq is 1 if yi = vq and is zero everywhere else. β can then be reparameterized
as
β = R(d)α,
(19.9)
where the C-dimensional vector α is the new parameter.
We then consider the case where Y is continuous. Usually both distribu-
tions P tr
Y
and P te
Y
are smooth, and so is β(y). Therefore, we would like to
enforce the smoothness of β(y) with respect to y. Let R(c) ≜Lβ(Lβ +λβI)−1,
where Lβ is a kernel matrix of y with the Gaussian kernel and λβ is the
regularization parameter.5 Inspired by KRR [21], we parameterize β(ytr) as
β = R(c)α,
(19.10)
with new parameter α.6One can consider β as a smoothed version of α.
Finally, we ﬁnd α (and β) in both cases by solving:
minimize
α
1
2α⊺
R⊺AR

α −m
n

MR

α,
s.t.
0 ≤

Rα

i ≤Bβ and
1⊺
mRα −m
 ≤mϵ,
(19.11)
where R stands for R(d) or R(c), depending on whether Y is discrete or con-
tinuous. In all our experiments, we set Bβ = 10 and ϵ =
Bβ
4√m. We then set β∗
in (19.2) to the estimated β and γ∗(xi, yi) ≡1. Minimizing (19.2) produces
the classiﬁer or regression model after correction for TarS.
19.4
Location-Scale Conditional Shift
domain
Y
X
FIGURE
19.2: A causal model for
ConS.
In practice P tr
X|Y
and P te
X|Y
might diﬀer to some extent. It is
certainly not possible to transfer
useful knowledge from the train-
ing domain to the test domain
if PX|Y changes arbitrarily. How-
ever, under certain assumptions
on the change in PX|Y , one could estimate P te
X|Y without knowing Y on test
data. In this section we assume that PX|Y changes across domains and that
5Note that although Lβ and L are both kernel matrices of y, they have diﬀerent purposes
and might have diﬀerent hyperparameters, so we use diﬀerent notations.
6When m is large, say, bigger than 1000, we parameterize β as a linear combination of
the nonlinear principal components of y with the weight vector α. The nonlinear principal
components were extracted by kernel principal component analysis (kPCA) [23]. In this
way the dimensionality of α is much less than m and the optimization procedure is very
eﬃcient.

Single-Source Domain Adaptation with Target and Conditional Shift
437
P tr
Y = P te
Y . We term this situation Conditional Shift (ConS); Figure 19.2 gives
its causal interpretation. This situation might be less realistic in practice and
will not be considered in our experiments; however, it serves as a foundation
of a more general situation, GeTarS, which will be studied in Section 19.5.
When considering ConS and GeTarS, we focus on classiﬁcation problems.
19.4.1
Assumptions and Identiﬁability
x2
x1
y = −1
y = 1
FIGURE 19.3: An illustration
of LS-ConS where Y is binary and
X is two-dimensional. Lines are
contours of PX|Y (x|y = −1) and
PX|Y (x|y = 1). Solid and dashed
lines represent the contours on the
training and test domains, respec-
tively.
In some situations, we can formulate
how the conditional distribution changes.
For instance, for the same image, fea-
tures such as intensities and colors are in-
ﬂuenced by illumination, viewing angles,
etc., which might change across domains.
Modeling such a change enables distribu-
tion matching between the training do-
main and test domain, and consequently
improves the performance on the test do-
main. Here we use the approach of trans-
forming training data to reproduce the co-
variate distribution on the test domain;
see Section 19.2. Since we can model the
transformation from P tr
X|Y to P te
X|Y , we do
not need the condition that the support of
P te
X|Y is contained in that of P tr
X|Y , making
the approach more practical.
We assume that the shape of the dis-
tribution of each feature Xi, as well as the dependence structure between fea-
tures, is preserved across the domains. More precisely, we assume that given
any y value, P te
Xi|Y and P tr
Xi|Y only diﬀers in the location and scale:
AConS: There exists w(Y tr) = diag[w1(Y tr), ..., wd(Y tr)] and b(Y tr) =
[b1(Y tr), ..., bd(Y tr)]⊺, where d is the dimensionality of X, such that
the conditional distribution of Xnew ≜w(Y tr)Xtr + b(Y tr) given Y tr
is the same as that of Xte given Y te.
We term this situation location-scale ConS (LS-ConS). In matrix form, the
transformed training points
xnew ≜xtr ⊙W + B,
(19.12)
where
the
ith
columns
of
W
and
B
are
[w1(yi), ..., wd(yi)]⊺
and
[b1(yi), ..., bd(yi)]⊺, respectively, are expected to have the same distribution
as the test data. Figure 19.3 illustrates how the contours of PX|Y change
across domains under LS-ConS.
Let wq and bq be the transformation to be applied to the data points with

438
Regularization, Optimization, Kernels, and Support Vector Machines
y = vq, q = 1, ..., C. The following theorem states that P new
X|Y is identiﬁable
under some conditions on P tr
X|Y (x|y = vq).
Theorem 19.2. Let P (wq,bq)
X|Y
(x|y = vq) be the LS transformed version of
P tr
X|Y (x|y = vq) with parameters (wq, bq) and P te
Y
= P tr
Y . Suppose AConS
holds, i.e., ∀q, ∃(w∗
q, b∗
q) such that P
(w∗
q,b∗
q)
X|Y
(x|y = vq) = P te
X|Y (x|y = vq).
Further assume
AConS
2
: Set

cq1P (wq,bq)
X|Y
(x|y = vq) + cq2P
(w′
q,b′
q)
X|Y
(x|y = cq) ; q = 1, ..., C
	
is linearly independent ∀cq1, cq2 (c2
q1 + c2
q2 ̸= 0), wq, w′
q (||wq||2
F +
||w′
q||2
F ̸= 0), and bq, b′
q.
If ∃(wq, bq) such that P te
X = P
q P tr
Y (yq)P (wq,bq)
X|Y
(x|y = vq), then we have ∀
q, P (wq,bq)
X|Y
(x|y = vq) = P te
X|Y (x|y = vq).
Proof. This theorem is a special case of Theorem 19.3, which shall be pre-
sented in Section 19.5: In Theorem 19.3, setting P new
Y
= P tr
Y = P te
Y gives this
theorem.
A necessary condition for AConS
2
is that P tr
X|Y (x|y = vq), q = 1, ..., C,
are linearly independent after any LS transformations. Roughly speaking, the
higher d, the less likely for this assumption to be violated.
19.4.2
A Kernel Approach
As in Section 19.3.2, we parameterize W and B as W = RG and B = RH,
where G and H are the parameters to be estimated, and R is R(c) or R(d),
depending on whether Y is discrete or continuous. In this way W and B
are guaranteed to be functions of y, and the number of parameters is greatly
reduced.
Noting the relationship between Xnew and Xtr, and using the substitution
rule, we have
U[P new
X|Y ] = CXnewY C−1
Y Y
= E(Xnew,Y )∼P new
XY [ψ(Xnew) ⊗φ⊺(Y )] E−1
Y ∼P tr
Y [φ(Y ) ⊗φ⊺(Y )]
= E(Xtr,Y )∼P tr
XY [ψ(Xnew) ⊗φ⊺(Y )] · E−1
Y ∼P tr
Y [φ(Y ) ⊗φ⊺(Y )].
The empirical estimate of U[P new
X|Y ] is consequently
ˆU[P new
X|Y ] = 1
mψ(xnew) · φ⊺(ytr) ·
h 1
mφ(ytr)φ⊺(ytr) + ˜λI
i−1
=˜Ψ(L + λI)−1Φ⊺,
(19.13)
where ˜Ψ = ψ(xnew).

Single-Source Domain Adaptation with Target and Conditional Shift
439
Let ˜K be the kernel matrix corresponding to the feature matrix ˜Ψ, i.e.,
˜Ki,j = k(xnew
i
, xnew
j
), and ˜Kc the cross kernel matrix between xte and xnew,
i.e., ˜Kc
ij = k(xte
i , xnew
j
). We aim to minimize
µ[P new
X
] −µ[P te
X ]
2, whose
empirical version is
JConS ≜

ˆµ[P new
X
] −ˆµ[P te
X ]


2
=

 ˆU[P new
X|Y ]ˆµ[P tr
Y ] −ˆµ[P te
X ]


2
=
1
m2 1⊺
mφ⊺(ytr) ˆU⊺[P new
X|Y ] ˆU[P new
X|Y ]φ(ytr)1m
−
2
mn1⊺
nψ⊺(xte) ˆU[P new
X|Y ]φ(ytr)1m + const
=
1
m2 1⊺
mΩ˜KΩT 1m −
2
mn1⊺
n ˜KcΩT 1m + const.
(19.14)
We then estimate W (or G) together with B (or H) by minimizing JConS.
The gradient of JConS with respect to ˜K and ˜Kc is
∂JConS
∂˜K
=
1
m2 (L + λI)−1L1m · 1⊺
mL(L + λI)−1,
and
∂JConS
∂˜Kc
= −2
mn1n1⊺
mL(L + λI)−1.
Using the chain rule, we further have the gradient of JConS with respect to
the entries of G and H:
∂JConS
∂Gpq
= Tr
h∂JConS
∂˜K
⊺
· (Dpq ⊙˜K)
i
−Tr
h∂JConS
∂˜Kc
⊺
· (Epq ⊙˜Kc)
i
,
∂JConS
∂Hpq
= Tr
h∂JConS
∂˜K
⊺
· ( ˜Dpq ⊙˜K)
i
−Tr
h∂JConS
∂˜Kc
⊺
· (˜Epq ⊙˜Kc)
i
,
where
[Dpq]ij = −1
l2 (xnew
jq
−xnew
iq
)(xtr
jqRjp −xtr
iqRip),
[Epq]ij = −1
l2 xtr
jqRjp(xnew
jq
−xte
iq),
[ ˜Dpq]ij = −1
l2 (xnew
jq
−xnew
iq
)(Rjp −Rip),
[˜Epq]ij = −1
l2 Rjp(xnew
jq
−xte
iq).
In practice we also regularize (19.14) to prefer the change in PX|Y to be
as little as possible, i.e., to make entries of W close to one and those of B
close to zero. This is particularly useful in case assumption AConS
2
is violated;
we then prefer the slightest change in the conditional, among all possibilities.
The regularization term is
Jreg = λLS
m · ||W −1m1⊺
d||2
F + λLS
m · ||B||2
F .
(19.15)

440
Regularization, Optimization, Kernels, and Support Vector Machines
whose gradient with respect to G and H is ∂Jreg
∂G
= 2λLS
m R⊺(W −1m1⊺
d) and
∂Jreg
∂H
= 2λLS
m R⊺B. In our experiments we ﬁx λLS to 0.001.
We use the scaled conjugate gradient (SCG) to minimize JConS +Jreg. Af-
ter estimating W and B, we transform xtr to xnew according to (19.12), and
(xnew, ytr) would have the same distribution as the test data, under assump-
tion AConS. Consequently, the classiﬁer or regressor trained on (xnew, ytr) is
expected to generalize well to the test domain.
19.5
Location-Scale Generalized Target Shift
domain
Y
X
FIGURE 19.4: Causal model for GeTarS.
We then consider a more
general situation where both PY
and PX|Y change, called Gen-
eralized Target Shift (GeTarS).
Figure 19.4 gives the causal
model underlying the GeTarS
situation.
In this setting, we assume
that P te
Y ̸= P tr
Y and that assumption AConS holds, i.e., we consider LS-GeTarS,
and aim to estimate the importance weights β∗(yi) ≜P te
Y (yi)
P tr
Y (yi) and the matrices
W and B in (19.12). They would transform the training data to mimic the dis-
tribution of the test data, and the learning machine learned on the reweighted
transformed data is expected to work well on the test data. Parameters can
be estimated by reweighting and transforming the training data to reproduce
P te
X , i.e., by minimizing ||µ[P new
X
] −µ[P te
X ]||, where P new
X
=
R
P new
Y
P new
X|Y dy,
P new
Y
= βP tr
Y , and P new
X|Y (x|y = vq) = P (wq,bq)
X|Y
(x|y = vq). The following
theorem provides the identiﬁability of pnew
Y
and P new
X|Y .
Theorem 19.3. Suppose AConS holds. Under assumption AConS
2
, if there
exist (wq, bq) such that P te
X = PC
q=1 P new
Y
(y = vq)P (wq,bq)
X|Y
(x|y = vq), then
we have P new
Y
= P te
Y , and ∀q, P (wq,bq)
X|Y
(x|y = vq) = P te
X|Y (x|y = vq).
Proof. Combining assumption AConS, i.e., P te
X = P
q P te
Y (vq)P
(w∗
q,b∗
q)
X|Y
(x|y =
vq), and the condition in Theorem 19.3, we have
X
q
P te
Y (yq)P
(w∗
q,b∗
q)
X|Y
(x|y = vq) =
X
q
P new
Y
(yq)P (wq,bq)
X|Y
(x|y = vq)
⇒
X
i

P te
Y (vq)P
(w∗
q,b∗
q)
X|Y
(x|yq) −P new
Y
(yq)P (wq,bq)
X|Y
(x|y = vq)

= 0.

Single-Source Domain Adaptation with Target and Conditional Shift
441
Because of assumption AConS
2
, we know that ∀q,
P te
Y (y = vq)P
(w∗
q,b∗
q)
X|Y
(x|y = vq) −P new
Y
(yq)P (wq,bq)
X|Y
(x|y = vq) = 0.
Taking the integral of the above equation gives P new
Y
(vq) = P te
Y (vq). This
further implies P (wq,bq)
X|Y
(x|yq) = P
(w∗
q,b∗
q)
X|Y
(x|yq) = P te
X|Y (x|y = vq).
Combining (19.6) and (19.13), we can ﬁnd the empirical version of
||µ[P new
X
] −µ[P te
X ]||2:
J =

ˆµ[P new
X
] −ˆµ[P te
X ]


2
=

 ˆU[P new
X|Y ]ˆµ[P te
Y ] −ˆµ[P te
X ]


2
=

 1
m
ˆU[P new
X|Y ]φ(ytr)β −1
nψ(xte)1n


2
= 1
m2 β⊺φ⊺(ytr) ˆU⊺[P new
X|Y ] ˆU[P new
X|Y ]φ(ytr)β −
2
mn1⊺
nψ⊺(xte) ˆU[P new
X|Y ]φ(ytr)β
+ const
= 1
m2 β⊺Ω˜KΩT β −
2
mn1⊺
n ˜KcΩT β.
(19.16)
When minimizing J, we would also like the diﬀerence between P te
X|Y and
P tr
X|Y , as measured by Jreg given in (19.15), to be as little as possible. Com-
bining both constraints, we estimate the involved parameters β, W, and B
by minimizing
JGeT arS = J + λLSJreg.
(19.17)
Finally, for parameter estimation, we iteratively alternate between the QP to
minimize (19.16) w.r.t β and the SCG optimization procedure with respect
to {W, B}. Algorithm 17 summarizes this procedure. For details of the two
optimization sub-procedures, see Sections 19.3 and 19.4, respectively. After
estimating the parameters, we train the learning machine by minimizing the
weighted loss (19.3) on (xnew, ytr).
The MATLAB source code for correcting TarS and LS-GeTarS is available
at http://people.tuebingen.mpg.de/kzhang/Code-TarS.zip.
19.6
Determination of Hyperparameters
As discussed in Section 19.2, all hyperparameters in the subsequent learn-
ing machines reweighted SVM and KRR are selected by importance weighted
cross-validation [32]. In addition, there are three types of hyperparameters.
One is the kernel width of X to construct the kernel matrix K. In our experi-
ments we normalize all variables in X to unit variance, and use some empirical

442
Regularization, Optimization, Kernels, and Support Vector Machines
Algorithm 17 Estimating weights β∗, W, and B under LS-GeTarS
Input: training data (xtr, ytr) and test data xte
Output: weights β and xnew corresponding to the training data points
β ←1m, W ←1m1⊺
d, B ←0
repeat
ﬁx W and B and estimate β by minimizing (19.17) with QP, under
the constraint on β given in Section 19.3;
ﬁx β and estimate W and B by minimizing (19.17) with SCG;
until convergence
β∗←β, xnew = xtr ⊙W + B.
values for those kernel widths: they are set to 0.8
√
d if the sample size m ≤200,
to 0.3
√
d if m > 1200, or to 0.5
√
d otherwise, where d is the dimensionality of
X. This simple setting always works well in all our experiments; for a more
principled strategy, one might refer to [7].
The second type of hyperparameters are involved in the parameterization
of β for regression under TarS (the kernel width for Lβ and regularization
parameter λβ) and λLS for LS-GeTarS in (19.17). We set these parameters
by cross-validation. (On some large data sets we simply set λLS to 0.0001 to
save computational load.) Although the objective functions (Equation 19.8
for TarS, and Equation 19.16 for LS-GeTarS) are the sum of squared errors,
the corresponding problems are considered unsupervised, or in particular, as
density estimation problems, rather than supervised. We treat P new
X
as the
distribution given by the model, and xte as the corresponding observed data
points. They are diﬀerent from the classical density estimation problem in
that here we use the MMD between P new
X
and P te
X as the loss function. We
divide xte into ﬁve equal size subsamples, use four of them to estimate β
or W and B, and the remaining one for testing. Finally we ﬁnd the values
of these hyperparameters that give the smallest cross-validated loss, which
is (19.8) for regression under TarS or (19.16) for LS-GeTarS. The last type
of hyperparameters, including hyperparameters in L and the regularization
parameter λ, are learned by extending Gaussian process regression to the
multi-output case [40].
19.7
Simulations
We use simulations to study the performance of the proposed approach for
TarS and LS-GeTarS in four scenarios. Their settings are
(a) a nonlinear regression problem under TarS, where X = Y +3 tanh(Y )+

Single-Source Domain Adaptation with Target and Conditional Shift
443
E, E
∼
N(0, 1.52); Y tr
∼
N(0, 22), and Y te
∼
0.8N(1, 1) +
0.2N(0.2, 0.52),
(b) a classiﬁcation problem under TarS, where
X|Y =−1 ∼N

0,

0.21
0.09
0.09
0.21
 
,
X|Y =1 ∼N
  1
1

,

0.31
−0.06
−0.06
0.31
 
,
P tr
Y (y = −1) = 0.6, and P te
Y (y = −1) = 0.2,
(c) a classiﬁcation problem approximately following LS-GeTarS, where
Xtr|Y tr=−1 ∼N

0,
 0.24
0.22
0.22
0.24
 
,
Xtr|Y tr=1 ∼N
  1
1

,

0.16
−0.03
−0.03
0.16
 
,
Xte|Y te=−1 ∼N
  0.5
0

,
 0.12
0.11
0.11
0.12
 
,
Xte|Y te=1 ∼N
 
2
1.3

,

0.27
−0.04
−0.04
0.27
 
,
P tr
Y (y = −1) = 0.6, and P te
Y (y = −1) = 0.3, and
(d) a classiﬁcation problem under non-LS-GeTarS with slight change in the
conditional, where
Xtr|Y tr=−1 ∼N

0,
 0.16
0
0
0.16
 
,
Xtr|Y tr=1 ∼N
  0.9
0.9

,
 0.23
0
0
0.23
 
,
Xte|Y te=−1 ∼N
  −0.1
0

,

0.10
−0.03
−0.03
0.10
 
,
Xte|Y te=1 ∼N
  0.9
0.8

,
 0.11
0.05
0.05
0.11
 
,
P tr
Y (y = −1) = 0.6, and P te
Y (y = −1) = 0.2.
See Figure 19.5 (left) for the training and test points generated according to
the four settings in one random replication. The training and test sets consist
of 500 and 400 data points, respectively.
We compare our approaches to correction for TarS (Section 19.3) and
for LS-GeTarS (Section 19.5) with the baseline (unweighted) least squares
KRR or SVM, the importance weighting approach to correction for covariate
shift (CovS) proposed in [10, 6], as well as two “oracle” approaches: one uses

444
Regularization, Optimization, Kernels, and Support Vector Machines
the theoretical values of β∗(y) = P te
Y /P te
Y , and the other trains the learning
machine directly on the test set. Note that the result learned on the test set
certainly has the best performance, but in practice it cannot be applied; it
is given to show the limit of the performance that any domain-adaptation
approach can achieve. Since in the considered classiﬁcation problems X is
low-dimensional, it is possible to apply the EM algorithm proposed by [2] to
estimate P te
Y , so it is also included for comparison. We repeated the simulations
100 times.
Figure 19.5 (right) shows the boxplot of the performances of all approaches,
measured by the mean square error (MSE) or classiﬁcation error on the test
set; for illustrative purposes, the left panels show the data points generated in
one replication as well as the regression lines or decision boundaries learned
by selected approaches. Under TarS, (a, b), and non-LS-GeTarS with slightly
changing conditionals, (d), compared to the baseline unweighted method,
clearly our approaches for TarS and LS-GeTarS improve the performance sig-
niﬁcantly. For regression under TarS, the estimated β values are very close
to the theoretical ones, as seen from the lower-right corner of Figure 19.5 (a,
left). EM achieves a similar performance as TarS, since PX|Y can be mod-
eled well in this simple case. In (c) the conditional PX|Y changes signiﬁcantly,
such that none of the approaches correcting for CovS or TarS helps, but since
the change approximately follows LS-GeTarS, our approach for LS-GeTarS
greatly improves the classiﬁcation performance. Compared to the unweighted
method, the important reweighting approach for CovS slightly improves the
performance in settings (b) and (d), and make it worse in (a) and (c).
19.8
Experiments on Pseudo Real-World Data
Table 19.2 reports the results on pseudo real-world data sets. In these ex-
periments, we split each data set into training set and test set. The percentage
of training samples ranges from 60% to 80%. Then, we perform the biased sam-
pling on the training data to obtain the shifted training set. Letting P(s = 1|y)
be the probability of sample x being selected given that its true output value is
y, we consider the following two biased sampling schemes for selecting training
data: (1) Weighted Label uses P(s = 1|y) = exp(a + by)/(1 + exp(a + by))
denoted by label(a,b), and (2) PCA. In this case, we generate biased sam-
pling schemes over the features. Firstly, a kernel PCA is performed on the
data. We select the ﬁrst principal component and the corresponding projec-
tion values. The biased sampling scheme is then a normal distribution with
mean m + ( ¯m −m)/a and variance ( ¯m −m)/b where m and ¯m are the min-
imum value of the projection and the mean of the projection, respectively.
We denote this sampling scheme by PCA(a,b,σ), where σ is the bandwidth of
the Gaussian RBF kernel. In summary, the LS-GeTarS outperforms unweight,

Single-Source Domain Adaptation with Target and Conditional Shift
445
−5
0
5
10
−2
0
2
4
6
x
y
 
 
training
test
predicted by TarS
by unweighted KRR
fitted on (oracle) test data
−2
0
2
4
6
8
0
1
2
 
 
Ptr(Y)
Pte(Y)
theoretial
estimated
β
y
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
MSE on test data
Unweighted
CovS
TarS
With oracle beta
Oracle test data
(a) Regression under TarS
−2
−1
0
1
2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
x1
x2
 
 
class 1 (training)
class 2 (training)
class 1 (test)
class 2(test)
boundary (train. data) 
boundary by TarS
Ptr(class 1)=0.6
Pte(class 1)=0.2
2
4
6
8
10
12
14
16
18
20
Classification error (%)
Unweighted
CovS
EM
TarS
LS−GeTarS
With oracle beta
Oracle test data
(b) Classiﬁcation under TarS
−2
−1
0
1
2
3
4
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
3
x1
x2
 
 
class 1 (training)
class 2 (training)
class 1 (test)
class 2(test)
boundary (train. data) 
boundary by GeTarS
Ptr(class 1)=0.6
Pte(class 1)=0.3
0
5
10
15
20
25
30
35
40
45
Unweighted
CovS
EM
TarS
LS−GeTarS
With oracle beta
Oracle test data
Classification error (%)
(c) Classiﬁcation under LS-GeTarS
−2
−1
0
1
2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
x1
x2
 
 
class 1 (training)
class 2 (training)
class 1 (test)
class 2(test)
boundary (train. data) 
boundary by TarS
Ptr(class 1)=0.6
Pte(class 1)=0.2
0
2
4
6
8
10
12
14
Unweighted
CovS
EM
TarS
LS−GeTarS
With oracle beta
Oracle test data
Classification error (%)
(d) Classiﬁcation under non-location-scale GeTarS
FIGURE 19.5: Four simulation settings together with the performances of
diﬀerent approaches. Left panels show the data points together with the de-
cision boundaries (or regression lines) obtained by selected approaches in one
replication, and right panels give the boxplot of the performances of diﬀerent
approaches for 100 random replications. (a) For a regression problem with X
depending on Y nonlinearly. (b) For a classiﬁcation problem under TarS. (c)
For a classiﬁcation problem under shape-preserving GeTarS. (d) For a classi-
ﬁcation problem under GeTarS but the shape of the conditional distribution
changes. Note that y-values of the test data were not given in the training
phase, and they are plotted for illustrative purposes.

446
Regularization, Optimization, Kernels, and Support Vector Machines
CovS, and TarS on 5 out of 6 data sets for a classiﬁcation problem. The TarS
outperforms all other approaches on one of these data sets. For regression
problem, TarS outperforms the unweight and CovS on 7 out of 12 data sets.
19.9
Experiments on Real-World Data
We evaluate the performance of the proposed approaches for regression
and classiﬁcation on real data. We ﬁrst consider prediction of nonstationary
processes, and then tackle the remote sensing image classiﬁcation problem,
with images acquired on diﬀerent areas.
19.9.1
Regression under TarS
We ﬁrst applied our approach for prediction on suitable data selected from
the cause-eﬀect pairs.7 We selected Data Set No. 68, since 1) the data are
nonstationary time series, 2) there is a strong dependence between the two
variables so that one can be predicted non-trivially by the other, and 3) the
variables are believed to have a direct causal relation, so that the invariance
of the conditional distribution of one variable (eﬀect) given the other (cause)
is likely to hold approximately. Figure 19.6 (top) shows the time series as well
as the joint distribution. Here X and Y stand for the number of bytes sent
by a computer at the tth minute and the number of open http connections at
the same time, respectively. It is natural to have the causal relation Y →X,
and we aim to predict Y from X without making use of temporal dependence
in the data. One subsample was always used for training, because on it Y
has large values. The remaining data were divided into four subsets, and each
time one of them was used for testing and the others included for training.
Figure 19.6 (bottom) shows the estimated β∗values on the four test sets;
they match P te
Y well. Table 19.3 gives the MSE on the four test sets produced
by diﬀerent approaches. Note that to achieve robustness of the prediction
result, we incorporated an exponent q for β∗as the importance weights, as
in correction for CovS with importance re-weighting [25]. q = 1 (i.e., the
proposed standard approach) and q = 0.5 were used. From Table 19.3 one can
see TarS gives the best results on all four test sets.
19.9.2
Remote Sensing Image Classiﬁcation
Hyperspectral remote sensing images are characterized by a dense sampling
of the spectral signature of diﬀerent land-cover types. We used a benchmark
data set in the literature that consists of data acquired by the Hyperion sensor
7http://webdav.tuebingen.mpg.de/cause-effect/

Single-Source Domain Adaptation with Target and Conditional Shift
447
TABLE 19.2: The results of diﬀerent distribution shift correction schemes. The results are averaged over 10 trials for
regression problems (marked *) and 30 trials for classiﬁcation problems. We report the normalized mean squared error
(NMSE) for regression problem and test error for classiﬁcation problem.
Data Set
Sampling Scheme
NMSE/test error ± std. error
Unweight
CovS
TarS
LS-GeTarS
1. Abalone∗
label(1,10)
0.4447±0.0223
0.4497±0.0125
0.4430±0.0208
–
2. CA Housing∗
PCA(10,5,0.1)
0.4075±0.0298
0.3944±0.0346
0.4565±0.0422
–
3. Delta Alilerons (1)∗
label(1,10)
0.3120±0.0040
0.3408±0.0278
0.3451±0.0280
–
4. Ailerons∗
PCA(1e3,4,0.1)
0.1360±0.0350
0.1486±0.0264
0.1329±0.0174
–
5. haberman (1)
label(0.2,0.8)
0.2699±0.0304
0.2699±0.0315
0.2676±0.0287
0.2619±0.0352
6. Bank8FM∗
PCA(3,6,0.1)
0.0477±0.0014
0.0590±0.0117
0.0452±0.0070
–
7. Bank32nh∗
PCA(3,6,0.01)
0.5210±0.0318
0.5171±0.0131
0.5483±0.0455
–
8. cpu-act∗
PCA(4,2,1e-12)
0.2026±0.0382
0.2042±0.0316
0.2000±0.0474
–
9. cpu-small∗
PCA(4,2,1e-12)
0.1314±0.0347
0.2009±0.0849
0.0769±0.0100
–
10. Delta Ailerons(2)∗
PCA(1e3,4,0.1)
0.4496±0.0236
0.3373±0.0596
0.3258±0.0274
–
11. Boston House∗
PCA(2,4,1e-4)
0.5128±0.1269
0.4966±0.0970
0.5342±0.0777
–
12. kin8nm∗
PCA(8,5,0.1)
0.5382±0.0425
0.5266±0.1248
0.6079±0.0976
–
13. puma8nh∗
PCA(4,4,0.1)
0.6093±0.0629
0.5894±0.0361
0.5595±0.0297
–
14. haberman(2)
PCA(2,2,0.01)
0.2736±0.0374
0.2725±0.0422
0.2724±0.0367
0.2579±0.0241
15. Breast Cancer
label(0.3,0.7)
0.2699±0.0304
0.3196±0.1468
0.2670±0.0319
0.2609±0.0510
16. India Diabetes
label(0.3,0.7)
0.2742±0.0268
0.2797±0.0354
0.2846±0.0364
0.2700±0.0599
17. Ionosphere
label(0.3,0.7)
0.0865±0.0294
0.1079±0.0563
0.0846±0.0559
0.0938±0.0294
18. German Credit
label(0.2,0.8)
0.3000±0.0284
0.2802±0.0354
0.2846±0.0364
0.2596±0.0368

448
Regularization, Optimization, Kernels, and Support Vector Machines
200
400
600
800
1000
1200
 
 
X
Y
Always for training
due to large values
One for testing; 
the others included for training
−4
−2
0
2
4
−2
−1
0
1
2
3
4
X
Y
−2
0
2
4
0
2
4
6
y
β
−2
0
2
4
0
2
4
6
y
β
−2
0
2
4
0
2
4
6
y
β
−2
0
2
4
0
2
4
6
y
β
FIGURE 19.6: Prediction results on Pair 68 of the cause-eﬀect pairs. Top:
time series data of X and Y (left, shifted apart for clarity) and the joint
distribution (right). Bottom: estimated β∗values on the four test sets.
TABLE 19.3: Prediction performance (MSE) on test sets.
Test set
Unweight.
CovS
CovS (q = 0.5)
TarS
TarS (q=0.5)
1
0.3789
0.3844
0.3802
0.3310
0.3229
2
0.0969
0.1126
0.1071
0.0937
0.0887
3
0.0578
0.0673
0.0659
0.0466
0.0489
4
0.2054
0.2126
0.2136
0.2008
0.1630

Single-Source Domain Adaptation with Target and Conditional Shift
449
of the Earth Observing 1 (EO-1) satellite in an area of the Okavango Delta,
Botswana, with 145 features; for details of this data set, see [8]. The labeled
reference samples were collected on two diﬀerent and spatially disjoint areas
(Area 1 and Area 2), thus representing possible spatial variabilities of the
spectral signatures of classes. The samples taken on each area were partitioned
into a training set TR and a test set TS by random sampling.
The numbers of labeled reference samples for each set and class are re-
ported in Table 19.4. TR1, TS1, TR2, and TS2 have sample sizes 1242, 1252,
2621, and 627, respectively. One would expect that not only the prior prob-
abilities of the classes Y , but also the conditional distribution of X given Y
would change across them, due to physical factors related to ground (e.g., dif-
ferent soil moisture or composition), vegetation, and atmospheric conditions.
Our target is to do domain adaptation from TR1 to TS2 and from TR2 to
TS1, as in [19].
TABLE 19.4: Number of training (TR1 and TR2) and test (TS1 and TS2)
patterns acquired in the two spatially disjoint areas for the experiment on
remote sensing image classiﬁcation.
Class
Number of patterns
Area 1
Area 2
TR1
TS1
TR2
TS2
Water
69
57
213
57
Hippo grass
81
81
83
18
Floodplain grasses1
83
75
199
52
Floodplain grasses2
74
91
169
46
Reeds1
80
88
219
50
Riparian
102
109
221
48
Firescar2
93
83
215
44
Island interior
77
77
166
37
Acacia woodlands
84
67
253
61
Acacia shrublands
101
89
202
46
Acacia grasslands
184
174
243
62
Short mopane
68
85
154
27
Mixed mopane
105
128
203
65
Exposed soil
41
48
81
14
Total
1242
1252
2621
627
After estimating the weights and/or the transformed training points (with
λLS = 10−4), we applied the multi-class classiﬁer with an RBF kernel, pro-
vided by LIBSVM, on the weighted or transformed data. Each time, the kernel
size and parameter C were chosen by ﬁve-fold cross-validation over the sets
{25/2, 23/2, 21/2, 2−1/2, 2−3/2, 2−5/2}·
√
d and {26, 28, 210, 212, 214, 216, 218}, re-

450
Regularization, Optimization, Kernels, and Support Vector Machines
spectively. (We found that the selected values always belonged to the interior
of the sets.)
Table 19.5 shows the overall classiﬁcation error (i.e., the fraction of mis-
classiﬁed points) obtained by diﬀerent approaches for each domain adapta-
tion problem. We can see that in this experiment, correction for target shift
does not signiﬁcantly improve the performance; in fact, the β values for most
classes are rather close to one. However, correction for conditional shift with
LS-GeTarS reduces the overall classiﬁcation error from 20.73% to 11.96% for
domain adaptation from TR1 to TS2, and from 25.32% to 14.54% for that
from TR2 to TS1. Covariate shift helps slightly for TR2 →TS1, probably
because our classiﬁer is rather simple in that all dimensions have the same
kernel size.
TABLE 19.5: The misclassiﬁcation rate on remote sensing data set under
diﬀerent distribution shift correction schemes.
Problem
Unweight
CovS
TarS
LS-GeTarS
TR1 →TS2
20.73%
20.73%
20.41%
11.96%
TR2 →TS1
26.36%
25.32%
26.28%
14.54%
Correction for conditional shift with LS-GeTarS reduces the overall clas-
siﬁcation error. In addition to the overall classiﬁcation error, we also report
the number of correctly classiﬁed points from each class; see Figure 19.7.
One can see that for both domain adaptation problems, LS-GeTarS improves
the classiﬁcation accuracy on classes 11, 9, and 3. It also leads to signiﬁcant
improvement on class 13 for the problem TR1 →TS2, and on class 2 for
TR1 →TS2. Note that this is a multi-class classiﬁcation problem and we aim
to improve the overall classiﬁcation accuracy; to achieve that, the accuracy
of some particular classes, such as classes 10 and 6, may become worse. Fig-
ure 19.8 plots some of the estimated scale transformation coeﬃcients w(ytr)
and location transformations b(ytr) that are signiﬁcant (i.e., w(ytr) is signif-
icantly diﬀerent from one, and b(ytr) diﬀerent from zero). Roughly speaking,
the transformation learned for the domain adaptation problem TR2 →TS1
is the inverse of that for the problem TR1 →TS1.
19.10
Conclusion and Discussions
We have considered domain adaptation where both the distribution of the
covariate and the conditional distribution of the target given the covariate
change across domains. From the causal point of view, we assume the target
causes the covariate, such that the change in the data distribution can be
modeled easily. In particular, we studied three situations, target shift, con-

Single-Source Domain Adaptation with Target and Conditional Shift
451
Class 1      2
3
4
5
6
7
8
9
10
11
12
13
14
0
10
20
30
40
50
60
# corrected classified points
 
 
unweighted
CovS
TarS
LS−GeTarS
(a) For domain adaptation from TR1 to TS2
Class 1      2
3
4
5
6
7
8
9
10
11
12
13
14
0
20
40
60
80
100
120
# corrected classified points
 
 
unweighted
CovS
TarS
LS−GeTarS
(a) For domain adaptation from TR2 to TS1
FIGURE 19.7: The number of correctly classiﬁed data points for each class
and each approach. (a) TR1 as training set and TS2 as test set. (b) TR2 as
training set and TS1 as test set.

452
Regularization, Optimization, Kernels, and Support Vector Machines
0
50
100
0.8
0.9
1
1.1
1.2
1.3
Feature sequential number
Entries of w(ytr)
 
class 3
class 8
class 11
class 12
(a) Estimated scale transformation coeﬃcient
for selected classes for domain adaptation
TR1 →TS2.
0
50
100
−0.2
−0.1
0
0.1
0.2
0.3
Feature sequential number
Entries of b(ytr)
 
 
class 3
class 8
class 11
class 12
(b) Estimated location transformation for
selected classes for domain adaptation
TR1 →TS2.
0
50
100
0.8
0.9
1
1.1
1.2
1.3
Feature sequential number
Entries of w(ytr)
 
 
class 3
class 8
class 11
class 12
(c) Estimated scale transformation coeﬃcient
for selected classes for domain adaptation
TR2 →TS1.
0
50
100
−0.2
−0.1
0
0.1
0.2
0.3
Feature sequential number
Entries of b(ytr)
 
 
class 3
class 8
class 11
class 12
(d) Estimated location transformation for
selected classes for domain adaptation
TR2 →TS1.
FIGURE 19.8: Estimated scale transformation coeﬃcient w(ytr) and loca-
tion transformation b(ytr) for selected classes by correction for LS-GeTarS.
(a, b) For domain adaptation from TR1 to TS2. (c, d) For domain adaptation
from TR2 to TS1.

Single-Source Domain Adaptation with Target and Conditional Shift
453
ditional shift, and generalized target shift, which combines the above two
situations. We presented practical approaches to handling them based on the
kernel mean embedding of conditional and marginal distributions. Simula-
tions were conducted to verify our theoretical claims. Experimental results
on diverse real-world problems, including remote sensing image classiﬁcation,
showed that (generalized) target shift often happens in domain adaptation and
that the proposed approaches could substantially improve the classiﬁcation or
regression performance accordingly.
Acknowledgments
The authors would like to thank D. Janzing for helpful discussions and
M. Crawford for kindly providing the hyperspectral data set. Z.-H. Zhou was
supported by the National Science Foundation of China (61333014) and the
National Key Basic Research Program of China (2014CB340501). C. Persello
has been supported by the Autonomous Province of Trento and the European
Community in the framework of the project “Trentino - PCOFUND-GA-2008-
226070 (call 3 - post-doc 2010 Outgoing)”.
Bibliography
[1] J. Candela, M. Sugiyama, A. Schwaighofer, and N. Lawrence, editors.
Dataset Shift in Machine Learning. MIT Press, 2009.
[2] Y. S. Chan and H. T Ng. Word sense disambiguation with distribution
estimation. In Proceedings of the 19th International Joint Conference on
Artiﬁcial Intelligence, pages 1010–1015, Scotland, 2005.
[3] K. Fukumizu, F. R. Bach, M. I. Jordan, and C. Williams. Dimensionality
reduction for supervised learning with reproducing kernel Hilbert spaces.
Journal of Machine Learning Research, 5:73–99, 2004.
[4] K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf. Kernel measures of
conditional dependence. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing Systems 20, pages
489–496, Cambridge, MA, 2008. MIT Press.
[5] A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf, and A. Smola. A
kernel method for the two-sample-problem. In NIPS 19, pages 513–520,
Cambridge, MA, 2007. MIT Press.

454
Regularization, Optimization, Kernels, and Support Vector Machines
[6] A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and
B. Schölkopf. Covariate shift and local learning by distribution match-
ing.
In J. Quiñonero-Candela, M. Sugiyama, A. Schwaighofer, and
N. Lawrence, editors, Dataset shift in machine learning, pages 131–160.
MIT Press, Cambridge, MA, 2008.
[7] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakr-
ishnan, M. Pontil, and K. Fukumizu. Optimal kernel choice for large-scale
two-sample tests. In NIPS 25. MIT Press, 2012.
[8] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh. Investigation of the
random forest framework for classiﬁcation of hyperspectral data. IEEE
Trans. Geosci. Remote Sens., 43(3):492–501, 2005.
[9] P.O. Hoyer, D. Janzing, J. Mooji, J. Peters, and B. Schölkopf. Nonlin-
ear causal discovery with additive noise models. In Advances in Neural
Information Processing Systems 21, Vancouver, B.C., Canada, 2009.
[10] J. Huang, A. Smola, A. Gretton, K. Borgwardt, and B. Schölkopf. Cor-
recting sample selection bias by unlabeled data. In NIPS 19, pages 601–
608, 2007.
[11] D. Janzing, J. Mooij, K. Zhang, J. Lemeire, J. Zscheischler, P. Daniu-
vsis, B. Steudel, and B. Schölkopf. Information-geometric approach to
inferring causal directions. Artiﬁcial Intelligence, pages 1–31, 2012.
[12] N. Japkowicz and S. Stephen. The class imbalance problem: A systematic
study. Intelligent Data Analysis, 6:429–450, 2002.
[13] J. Jiang. A literature survey on domain adaptation of statistical classi-
ﬁers, 2008. http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey.
[14] Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classiﬁcation
in nonstandard situations. Machine Learning, 46:191–202, 2002.
[15] C. Manski and S. Lerman. The estimation of choice probabilities from
choice-based samples. Econometrica, 45:1977–1988, 1977.
[16] J. Mooij, O. Stegle, D. Janzing, K. Zhang, and B. Schölkopf. Probabilistic
latent variable models for distinguishing between cause and eﬀect. In
Advances in Neural Information Processing Systems 23 (NIPS 2010),
Curran, NY, USA, 2010.
[17] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions
on Knowledge and Data Engineering, 22:1345–1359, 2010.
[18] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge Uni-
versity Press, Cambridge, 2000.

Single-Source Domain Adaptation with Target and Conditional Shift
455
[19] C. Persello. Interactive domain adaptation for the classiﬁcation of re-
mote sensing images using active learning. IEEE Geoscience and Remote
Sensing Letters, 10:736–740, 2013.
[20] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer
Press, New York, 2nd edition, 2004.
[21] C. Saunders, A. Gammerman, and V. Vovk. Ridge regression learning
algorithm in dual variables. In 15th International Conference on Machine
Learning, pages 515–521, Madison, WI, 1998.
[22] B. Schölkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. Mooij.
On causal and anticausal learning. In Proc. 29th International Conference
on Machine Learning (ICML 2012), Edinburgh, Scotland, 2012.
[23] B. Schölkopf, A. Smola, and K. Muller. Nonlinear component analysis as
a kernel eigenvalue problem. Neural Computation, 10:1299–1319, 1998.
[24] S. Shimizu, P.O. Hoyer, A. Hyvärinen, and A.J. Kerminen. A linear non-
Gaussian acyclic model for causal discovery. Journal of Machine Learning
Research, 7:2003–2030, 2006.
[25] H. Shimodaira. Improving predictive inference under covariate shift by
weighting the log-likelihood function. Journal of Statistical Planning and
Inference, 90:227–244, 2000.
[26] A. Smola, A. Gretton, L. Song, and B. Schölkopf. A Hilbert space embed-
ding for distributions. In Proceedings of the 18th International Conference
on Algorithmic Learning Theory, pages 13–31. Springer-Verlag, 2007.
[27] L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola. Hilbert space
embeddings of hidden Markov models. In Proceedings of the 26th Inter-
national Conference on Machine Learning, Haifa, Israel, 2010.
[28] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert space embeddings
of conditional distributions with applications to dynamical systems. In
International Conference on Machine Learning (ICML 2009), June 2009.
[29] P. Spirtes, C. Glymour, and R. Scheines.
Causation, Prediction, and
Search. MIT Press, Cambridge, MA, 2nd edition, 2001.
[30] B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. Universality, charac-
teristic kernels and RKHS embedding of measures. Journal of Machine
Learning Research, 12:2389–2410, 2011.
[31] A. Storkey.
When training and test sets are diﬀerent: Characterizing
learning transfer.
In J. Candela, M. Sugiyama, A. Schwaighofer, and
N. Lawrence, editors, Dataset Shift in Machine Learning, pages 3–28.
MIT Press, 2009.

456
Regularization, Optimization, Kernels, and Support Vector Machines
[32] M. Sugiyama, M. Krauledat, and K. R. Müller. Covariate shift adaptation
by importance weighted cross validation. Journal of Machine Learning
Research, 8:985–1005, December 2007.
[33] M. Sugiyama, T. Suzuki, S. Nakajima, H. Kashima, P. von Bünau, and
M. Kawanabe. Direct importance estimation for covariate shift adap-
tation. Annals of the Institute of Statistical Mathematics, 60:699–746,
2008.
[34] J. Tian and J. Pearl. Causal discovery from changes: a bayesian approach.
In Proceedings of the 17th Conference on Uncertainty in Artiﬁcial Intel-
ligence (UAI2001), pages 512–521, 2001.
[35] J. Woodward.
Making things happen: A theory of causal explanation.
Oxford University Press, New York, 2003.
[36] Y. Yu and Z. H. Zhou. A framework for modeling positive class expansion
with single snapshot. In PAKDD 2008, 2008.
[37] B. Zadrozny. Learning and evaluating classiﬁers under sample selection
bias. In 21st International Conference on Machine Learning, pages 114–
121, Banﬀ, Canada, 2004.
[38] K. Zhang and A. Hyvärinen. Acyclic causality discovery with additive
noise: An information-theoretical perspective. In Proc. European Con-
ference on Machine Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML PKDD) 2009, Bled, Slovenia, 2009.
[39] K. Zhang and A. Hyvärinen. On the identiﬁability of the post-nonlinear
causal model. In Proceedings of the 25th Conference on Uncertainty in
Artiﬁcial Intelligence, Montreal, Canada, 2009.
[40] K. Zhang, J. Peters, D. Janzing, and B. Schölkopf. Kernel-based con-
ditional independence test and application in causal discovery. In Pro-
ceedings of the 27th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI 2011), Barcelona, Spain, 2011.
[41] K. Zhang, B. Schölkopf, K. Muandet, and Z. Wang. Domain adaptation
under target and conditional shift. In Proceedings of the 30th Interna-
tional Conference on Machine Learning, JMLR: W&CP Vol. 28, 2013.

Chapter 20
Multi-Layer Support Vector
Machines
Marco A. Wiering
Institute of Artiﬁcial Intelligence and Cognitive Engineering, University of
Groningen
Lambert R.B. Schomaker
Institute of Artiﬁcial Intelligence and Cognitive Engineering, University of
Groningen
20.1
Introduction ......................................................
457
20.2
Multi-Layer Support Vector Machines for Regression Problems
459
20.3
Multi-Layer Support Vector Machines for Classiﬁcation
Problems .........................................................
463
20.4
Multi-Layer Support Vector Machines for Dimensionality
Reduction ........................................................
464
20.5
Experiments and Results ........................................
465
20.5.1
Experiments on Regression Problems ...................
465
20.5.2
Experiments on Classiﬁcation Problems ................
467
20.5.3
Experiments on Dimensionality Reduction Problems ...
468
20.5.4
Experimental Analysis of the Multi-Layer SVM ........
469
20.6
Discussion and Future Work .....................................
471
Acknowledgments ................................................
472
Bibliography ......................................................
472
20.1
Introduction
Support vector machines (SVMs) [24, 8, 20, 22] and other learning algo-
rithms based on kernels have been shown to obtain very good results on many
diﬀerent classiﬁcation and regression datasets. SVMs have the advantage of
generalizing very well, but the standard SVM is limited in several ways. First,
the SVM uses a single layer of support vector coeﬃcients and is therefore a
shallow model. Deep architectures [17, 14, 13, 4, 25, 6] have been shown to be
very promising alternatives to these shallow models. Second, the results of the
SVM rely heavily on the selected kernel function, but most kernel functions
457

458
Regularization, Optimization, Kernels, and Support Vector Machines
have limited ﬂexibility in the sense they they are not trainable on a dataset.
Therefore, it is a natural step to go from the standard single-layer SVM to
the multi-layer SVM (ML-SVM). Just like the invention of the backpropa-
gation algorithm [26, 19] allowed to construct multi-layer perceptrons from
perceptrons, this chapter describes techniques for constructing and training
multi-layer SVMs consisting only of SVMs.
There is a lot of related work in multiple kernel learning (MKL) [16, 3,
21, 18, 31, 10]. In these approaches, some combination functions of a set of
ﬁxed kernels are adapted to the dataset. As has been shown by a number
of experiments, linear combinations of base kernels do not often help to get
signiﬁcantly better performance levels. Therefore, in [7] the authors describe
the use of non-linear (polynomial) combinations of kernels and their results
show that this technique is more eﬀective. An even more recent trend in MKL
is the use of multi-layer MKL. In [9], a general framework for two-layer kernel
machines is described, but unlike in the current study, no experimental results
were reported in which both layers used non-linear kernels. In [32], multi-layer
MKL is described where mixture coeﬃcients of diﬀerent kernels are stored in
an exponential function kernel. These coeﬃcients in the second layer of the
two-layer MKL algorithm are trained using a min-max objective function. In
[5] a new type of kernel is described, which is useful for mimicking a deep
learning architecture. The neural support vector machine (NSVM) [28] is also
related to the multi-layer SVM. The NSVM is a novel algorithm that uses
neural networks to extract features that are given to a support vector machine
for giving the ﬁnal output of the architecture. Finally, the current chapter
extends the ideas in [27] by describing a classiﬁcation and autoencoder method
using multi-layer support vector machines.
Contributions. We describe a simple method for constructing and train-
ing multi-layer SVMs. The hidden-layer SVMs in the architecture learn to
extract relevant features or latent variables from the inputs and the output-
layer SVMs learn to approximate the target function using the extracted fea-
tures from the hidden-layer SVMs. We can easily make the association with
multi-layer perceptrons (MLPs) by letting a complete SVM replace each in-
dividual neuron. However, in contrast to the MLP, the ML-SVM algorithm
is trained using a min-max objective function: the hidden-layer SVMs are
trained to minimize the dual-objective function of the output-layer SVMs and
the output-layer SVMs are trained to maximize their dual-objective functions.
This min-max optimization problem is a result of going from the primal objec-
tive to the dual objective. Therefore, the learning dynamics of the ML-SVM
are entirely diﬀerent compared to the MLP in which all model parameters
are trained to minimize the same error function. When compared to other
multi-layer MKL approaches, the ML-SVM does not make use of any com-
bination weights, but trains support vector coeﬃcients and the biases of all
SVMs in the architecture. Our experimental results show that the ML-SVM
signiﬁcantly outperforms state-of-the-art machine learning techniques on re-
gression, classiﬁcation and dimensionality reduction problems.

Multi-Layer Support Vector Machines
459
We have organized the rest of this chapter as follows. Section 20.2 de-
scribes the ML-SVM algorithm for regression problems. In Section 20.3, the
ML-SVM algorithm is introduced for classiﬁcation problems. In Section 20.4,
the autoencoding ML-SVM is described. In Section 20.5, experimental results
on 10 regression datasets, 8 classiﬁcation datasets, and a dimensionality re-
duction problem are presented. Finally, Section 20.6 discusses the ﬁndings and
describes future work.
20.2
Multi-Layer Support Vector Machines for
Regression Problems
We will ﬁrst describe the multi-layer SVM for regression problems. We use
a regression dataset: {(x1, y1), . . . , (xℓ, yℓ)}, where xi are input vectors and yi
are the scalar target outputs. The architecture of a two-layer SVM is shown
in Figure 20.1.
[x]1
/
f(x)
[x]2
/
S1
...
S2
M
g(x)
/
[x]D−1 /
S3
[x]D
/
FIGURE 20.1: Architecture of a two-layer SVM. In this example, the hidden
layer consists of three SVMs Sa.
The two-layer architecture contains an input layer of D inputs. Then, there
are a total of d SVMs Sa, each one learning to extract one latent variable
f(x|θ)a from an input pattern x. Here θ denotes the trainable parameters
in the hidden-layer SVMs (which are the support vector coeﬃcients and the
biases). Finally, there is the main support vector machine M that learns to
approximate the target function using the extracted feature vector as input.
For computing the hidden-layer representation f(x|θ) of input vector x, we

460
Regularization, Optimization, Kernels, and Support Vector Machines
use:
f(x|θ)a =
ℓ
X
i=1
(α∗
i (a) −αi(a))K1(xi, x) + ba,
(20.1)
which is iteratively used by each SVM Sa to compute the element f(x|θ)a.
In this equation, α∗
i (a) and αi(a) are support vector coeﬃcients for SVM Sa,
ba is its bias, and K1(·, ·) is a kernel function for the hidden-layer SVMs.
For computing the output of the whole ML-SVM, the main SVM maps the
extracted hidden-layer representation to an output:
g(f(x|θ)) =
ℓ
X
i=1
(α∗
i −αi)K2(f(xi|θ), f(x|θ)) + b.
(20.2)
Here, K2(·, ·) is the kernel function in the output layer of the multi-layer SVM.
The primal objective for a linear regression SVM M can be written as:
min
w,θ,ξ,ξ∗,b J(w, θ, ξ, ξ∗, b) = 1
2∥w∥2 + C
ℓ
X
i=1
(ξi + ξ∗
i )
(20.3)
subject to constraints:
yi −w · f(xi|θ) −b ≤ε + ξi ; w · f(xi|θ) + b −yi ≤ε + ξ∗
i
(20.4)
and ξi, ξ∗
i ≥0. Here C is a metaparameter, ϵ is an error tolerance value used
in the Hinge (ϵ-insensitive) loss function, and ξi and ξ∗
i are slack variables
that tolerate errors larger than ϵ, but which should be minimized. The dual-
objective function for the regression problem for the main SVM M is:
min
θ
max
α,α∗J(θ, α, α∗) = −ε
ℓ
X
i=1
(α∗
i + αi) +
ℓ
X
i=1
(α∗
i −αi)yi
−1
2
ℓ
X
i,j=1
(α∗
i −αi)(α∗
j −αj)K2(f(xi|θ), f(xj|θ))
(20.5)
subject to: 0 ≤α∗
i , αi ≤C and
Pℓ
i=1(αi −α∗
i ) = 0. The second constraint
in generally known as the bias constraint.
Our learning algorithm adjusts the SVM coeﬃcients of all SVMs through
the min-max formulation of the dual-objective function J(·) of the main SVM.
Note that the min-max optimization problem is a result of going from the
primal objective to the dual objective. In the primal objective, it is a joint
minimization with respect to θ and the α coeﬃcients. However, by dualizing
the primal objective of the main SVM, it is turned into a min-max problem.
We have implemented a simple gradient ascent algorithm to train the
SVMs. The method adapts all SVM coeﬃcients α∗
i and αi toward a (local)

Multi-Layer Support Vector Machines
461
maximum of J(·), where λ is the learning rate. The resulting gradient ascent
learning rule for αi is:
αi ←αi + λ(−ϵ −yi +
ℓ
X
j=1
(α∗
j −αj)K2(f(xi|θ), f(xj|θ))).
(20.6)
The resulting gradient ascent learning rule for α∗
i is:
α∗
i ←α∗
i + λ(−ϵ + yi −
ℓ
X
j=1
(α∗
j −αj)K2(f(xi|θ), f(xj|θ)))
(20.7)
The support vector coeﬃcients are set to 0 if they become less than 0, and set
to C if they become larger than C. We also added a penalty term to respect the
bias constraint, so actually the gradient ascent algorithm trains the support
vector coeﬃcients to maximize the objective J′(·) = J(·)−c1 ·(P
i(αi −α∗
i ))2,
with c1 some metaparameter. Although this simple strategy works well, this
ad-hoc optimization strategy could also be replaced by a gradient projection
method for which convergence properties are better understood.
In the experiments we will make use of radial basis function (RBF) kernels
in both layers of a two-layer SVM. Preliminary results with other often used
kernels were somewhat worse. For the main SVM and hidden-layer SVMs the
RBF kernel is deﬁned respectively by:
K2(f(xi|θ), f(x|θ))
=
exp(−
d
X
a=1
(f(xi|θ)a −f(x|θ)a)2
σ2
)
(20.8)
K1(xi, x)
=
exp(−
D
X
a=1
(xa
i −xa)2
σ1
)
(20.9)
where σ2 and σ1 determine the widths of the RBF kernels in the output
and hidden layers. The ML-SVM constructs a new dataset for each hidden-
layer SVM Sa with a backpropagation-like technique for making examples:
(xi, f(xi|θ)a −µ · ∂J(·)/∂f(xi|θ)a), where µ is some metaparameter, and
∂J(·)/∂f(xi|θ)a for the RBF kernel is given by:
∂J(·)
∂f(xi|θ)a
= (α∗
i −αi)
ℓ
X
j=1
(α∗
j −αj)f(xi|θ)a −f(xj|θ)a
σ2
· K2(f(xi|θ), f(xj|θ)).
(20.10)
We constrain the target values for hidden-layer features between -1 and 1, so if
some target output is larger than 1 for a feature we simply set the target value
to 1. To allow the hidden-layer SVMs to extract diﬀerent features, symmetry
breaking is necessary. For this, we could randomly initialize the trainable pa-
rameters in each hidden-layer SVM. However, we discovered that a better way
to initialize the hidden-layer SVMs is to let them train on diﬀerent perturbed

462
Regularization, Optimization, Kernels, and Support Vector Machines
versions of the target outputs. Therefore we initially construct a dataset (xi,
yi + γa
i ), with γa
i some random value ∈[−γ, γ] for the hidden-layer SVM
Sa, where γ is another metaparameter. In this way, the ML-SVM resembles
a stacking ensemble approach [30], but due to the further training with the
min-max optimization process, these approaches are still very diﬀerent. The
complete algorithm is given in Algorithm 18.
Algorithm 18 The multi-layer SVM algorithm
Initialize output SVM
Initialize hidden-layer SVMs
Compute kernel matrix for hidden-layer SVMs
Train hidden-layer SVMs on perturbed dataset
repeat
Compute kernel matrix for output-layer SVM
Train output-layer SVM
Use backpropagation to create training sets for hidden-layer SVMs
Train hidden-layer SVMs
until maximum number of epochs is reached
In the algorithm alternated training of the main SVM and hidden-layer
SVMs a number of epochs are executed. An epoch here is deﬁned as training
the main SVM and the hidden-layer SVM a single time on their respective
datasets with our gradient ascent technique that uses a small learning rate and
a ﬁxed number of iterations. The bias values of all SVMs are set by averaging
over the errors on all examples.
Theoretical insight. Due to the min-max optimization problem and the
two layers with non-linear kernel functions, the ML-SVM loses the property
that the optimization problem is convex. However, similar to multiple-kernel
learning, training the output-layer SVM given the outputs of the hidden
layer remains a convex learning problem. Furthermore, the datasets gener-
ated with the backpropagation technique explained above are like normal
training datasets. Since training an SVM on a dataset is a convex learning
problem, these newly created datasets are also convex learning problems for
the hidden-layer SVMs. By using the pre-training of hidden-layer SVMs on
perturbed versions of the target outputs, the learning problem of the output-
layer SVM becomes much simpler. In fact, this resembles a stacking ensemble
approach [30], but unlike any other ensemble approach, the ML-SVM is fur-
ther optimized using the min-max optimization process. This is interesting,
because it is diﬀerent from other approaches in which the same error function
is minimized by all model parameters. Still, it could also be seen as a disad-
vantage, because min-max learning is not yet well understood in the machine
learning community.

Multi-Layer Support Vector Machines
463
20.3
Multi-Layer Support Vector Machines for
Classiﬁcation Problems
In the multi-layer SVM classiﬁer, the architecture contains multiple sup-
port vector classiﬁers in the output layer. To deal with multiple classes, we
use a binary one vs. all classiﬁer Mc for each class c. We do this even with 2
classes for convenience. We use a classiﬁcation dataset for each classiﬁer Mc:
{(x1, yc
1), . . . , (xℓ, yc
ℓ)}, where xi are input vectors and yc
i ∈{−1, 1} are the
target outputs that denote if the example xi belongs to class c or not. All
classiﬁers Mc share the same hidden-layer of regression SVMs. Mc determines
its output on an example x as follows:
gc(f(x|θ)) =
ℓ
X
i=1
yc
i αc
iK2(f(xi|θ), f(x|θ)) + bc.
(20.11)
Here f(xi|θ) is computed with the hidden-layer SVMs as before. The values
αc
i are the support vector coeﬃcients for classiﬁer Mc. The value bc is its
bias. After computing all output values for all classiﬁers, the class with the
highest output is assumed to be the correct class label (with ties being broken
randomly). The primal objective for a linear support vector classiﬁer Mc can
be written as:
min
wc,ξ,b,θ Jc(wc, ξ, b, θ) = 1
2||wc||2 + C
ℓ
X
i=1
ξi
(20.12)
subject to: yc
i (wc·f(xi|θ)+bc) ≥1−ξi, and ξi ≥0. Here C is a metaparameter
and ξi are slack variables that tolerate errors, but which should be minimized.
The dual-objective function for the classiﬁcation problem for classiﬁer Mc is:
min
θ
max
αc Jc(θ, αc) =
ℓ
X
i=1
αc
i −1
2
ℓ
X
i,j=1
αc
iαc
jyc
i yc
jK2(f(xi|θ), f(xj|θ))
(20.13)
subject to: 0 ≤αc
i ≤C,
and
Pℓ
i=1 αc
iyc
i = 0. Whenever the ML-SVM is
presented a training pattern xi, each classiﬁer in the multi-layer SVM uses
gradient ascent to adapt its αc
i values towards a local maximum of Jc(·) by:
αc
i ←αc
i + λ(1 −
ℓ
X
j=1
αc
jyc
jyc
i K2(f(xi|θ), f(xj|θ)))
(20.14)
where λ is a metaparameter controlling the learning rate of the values αc
i.
As before, the support vector coeﬃcients are kept between 0 and C. Be-
cause we use a gradient ascent update rule, we use an additional penalty

464
Regularization, Optimization, Kernels, and Support Vector Machines
term c1(Pℓ
j=1 αc
jyc
j)2 with metaparameter c1 so that the bias constraint is
respected.
As in the regression ML-SVM, the classiﬁcation ML-SVM constructs a
new dataset for each hidden-layer SVM Sa with a backpropagation-like tech-
nique for making examples. However, in this case the aim of the hidden-layer
SVMs is to minimize the sum of objectives P
c Jc(·). Therefore, the algorithm
constructs a new dataset using: (xi, f(xi|θ)a −µ P
c ∂Jc(·)/∂f(xi|θ)a), where
µ is some metaparameter, and ∂Jc(·)/∂f(xi|θ)a for the RBF kernel is:
∂Jc(·)
∂f(xi|θ)a
= αc
iyc
i
ℓ
X
j=1
αc
jyc
j
f(xi|θ)a −f(xj|θ)a
σ2
· K2(f(xi|θ), f(xj|θ)) (20.15)
The target outputs for hidden-layer features are again kept between -1 and 1.
The datasets for hidden-layer SVMs are made so that the sum of the dual-
objective functions of the output SVMs is minimized. All SVMs are trained
with the gradient ascent algorithm on their constructed datasets. Note that
the hidden-layer SVMs are still regression SVMs, since they need to output
continuous values. For the ML-SVM classiﬁer, we use a diﬀerent initialization
procedure for the hidden-layer SVMs. Suppose there are d hidden-layer SVMs
and a total of ctot classes. The ﬁrst hidden-layer SVM is ﬁrst pre-trained on
inputs and perturbed target outputs for class 0, the second on the perturbed
target outputs for class 1, and the kth hidden-layer SVM is pre-trained on the
perturbed target outputs for class k modulo ctot. The bias values are com-
puted in a similar way as in the regression ML-SVM, but for the output
SVMs only examples with non-bound support vector coeﬃcients (which are
not 0 or C) are used.
20.4
Multi-Layer Support Vector Machines for
Dimensionality Reduction
The architecture of the ML-SVM autoencoder diﬀers from the single-
output regression ML-SVM in two respects: (1) The output layer consists
of D nodes, the same number of nodes the input layer has. (2) It utilizes a
total of D support vector regression machines Mc, which each take the entire
hidden-layer output as input and determine the value of one of the outputs.
The forward propagation of a pattern x of dimension D determines the
representation in the hidden layer. The hidden layer is then used as input for
each support vector machine Mc that determines its output with:
gc(f(x|θ)) =
ℓ
X
i=1
(αc∗
i −αc
i)K2(f(xi|θ), f(x|θ)) + bc.
(20.16)

Multi-Layer Support Vector Machines
465
Again we make use of RBF kernels in both layers. The aim of the ML-SVM
autoencoder is to reconstruct the inputs in the output layer using a bottleneck
of hidden-layer SVMs, where the number of hidden-layer SVMs is in general
much smaller than the number of inputs. The ML-SVM autoencoder tries
to ﬁnd the SVM coeﬃcients θ such that the hidden-layer representation f(·)
is most useful for accurately reconstructing the inputs, and thereby codes
the features most relevant to the input distribution. This is similar to neural
network autoencoders [23, 12]. Currently popular deep architectures [14, 4, 25]
stack these autoencoders one by one, which is also possible for the ML-SVM
autoencoder.
The dual objective of each support vector machine Mc is:
min
θ
max
αc∗,αc Jc(θ, αc(∗)
i
) = −ε
ℓ
X
i=1
(αc∗
i + αc
i) +
ℓ
X
i=1
(αc∗
i −αc
i)yc
i
−1
2
ℓ
X
i,j=1
(αc∗
i −αc
i)(αc∗
j −αc
j)K2(f(xi|θ), f(x|θ))
(20.17)
subject to: 0 ≤αc
i, αc∗
i
≤C, and
Pℓ
i=1(αc∗
i −αc
i) = 0. The minimization of
this equation with respect to θ is a bit diﬀerent from the single-node ML-SVM.
Since all SVMs share the same hidden layer, we cannot just minimize J(·) for
every SVM separately. It is actually this shared nature of the hidden layer
that enables the ML-SVM to perform autoencoding. Therefore the algorithm
creates new datasets for the hidden-layer SVMs by backpropagating the sum
of the derivatives of all dual objectives Jc(·). Thus, the ML-SVM autoencoder
uses: (x, f(x|θ)a −µ PD
c=1
∂Jc(·)
∂f(x|θ)a ) to create new datasets for the hidden-layer
SVMs.
20.5
Experiments and Results
We ﬁrst performed experiments on regression and classiﬁcation problems to
compare the multi-layer SVM (we used two layers) to the standard SVM and
also to a multi-layer perceptron. Furthermore, we performed experiments with
an image dataset where it was the goal to obtain the smallest reconstruction
error with a limited number of hidden components.
20.5.1
Experiments on Regression Problems
We experimented with 10 regression datasets to compare the multi-layer
SVM to an SVM, both using RBF kernels. We note that both methods are
trained with the simple gradient ascent learning rule, adapted to also consider

466
Regularization, Optimization, Kernels, and Support Vector Machines
the penalty for obeying the bias constraint, although standard algorithms for
the SVM could also be used. The ﬁrst 8 datasets are described in [11] and
the other 2 datasets are taken from the UCI repository [1]. The number of
examples per dataset ranges from 43 to 1049, and the number of input features
is between 2 and 13. The datasets are split into 90% training data and 10%
test data. For optimizing the metaparameters we have used particle swarm
optimization (PSO) [15]. There are in total around 15 metaparameters for the
ML-SVM such as the learning rates for the two layers, the values for the error
tolerance ϵ, the values for C, the number of gradient ascent iterations in the
gradient ascent algorithm, the values for respecting the bias constraint c1, the
RBF kernel widths σ1 and σ2, the number of hidden-layer SVMs, the value for
the perturbation value γ used for pre-training the hidden-layer SVMs, and the
maximal number of epochs. PSO saved us from laborious manual tuning of
these metaparameters. We made an eﬀective implementation of PSO that also
makes use of the UCB bandit algorithm [2] to eliminate unpromising sets of
metaparameters. We always performed 100,000 single training-runs to obtain
the best metaparameters that took at most 2 days on a 32-CPU machine
on the largest dataset. For the gradient ascent SVM algorithm we also used
100,000 evaluations with PSO to ﬁnd the best metaparameters, although our
implementation of the gradient ascent SVM has 7 metaparameters, which
makes it easier to ﬁnd the best ones. Finally, we used 1000 or 4000 new cross
validation runs with the best found metaparameters to compute the mean
squared error and its standard error of the diﬀerent methods for each dataset.
TABLE 20.1: The mean squared errors and standard errors of the gradient
ascent SVM, the two-layer SVM, and results published in [11] for an MLP on
10 regression datasets. N/A means not available.
Dataset
Gradient ascent SVM ML-SVM
MLP
Baseball
0.02413 ± 0.00011
0.02294 ± 0.00010
0.02825
Boston Housing
0.006838 ± 0.000095
0.006381 ± 0.000091 0.007809
Concrete Strength 0.00706 ± 0.00007
0.00621 ± 0.00005
0.00837
Diabetes
0.02719 ± 0.00026
0.02327 ± 0.00022
0.04008
Electrical Length
0.006382 ± 0.000066
0.006411 ± 0.000070
0.006417
Machine-CPU
0.00805 ± 0.00018
0.00638 ± 0.00012
0.00800
Mortgage
0.000080 ± 0.000001
0.000080 ± 0.000001
0.000144
Stock
0.000862 ± 0.000006
0.000757 ± 0.000005 0.002406
Auto-MPG
6.852 ± 0.091
6.715 ± 0.092
N/A
Housing
8.71 ± 0.14
9.30 ± 0.15
N/A
In Table 20.1 we show the results of the standard SVM trained with gra-
dient ascent and the results of the two-layer SVM. The table also shows the
results for a multi-layer perceptron (MLP) reported in [11] on the ﬁrst 8
datasets. The MLP used sigmoidal hidden units and was trained with back-

Multi-Layer Support Vector Machines
467
propagation. We note that Graczyk et al. [11] only performed 10-fold cross
validation and did not report any standard errors.
The results show that the two-layer SVM signiﬁcantly outperforms the
other methods on 6 datasets (p < 0.001) and only performs worse than the
standard SVM on the Housing dataset from the UCI repository. The average
gain over all datasets is 6.5% error reduction. The standard errors are very
small because we performed 1000 or 4000 times cross validation. We did this
because we observed that with less cross validation runs the results were less
trustworthy due to their stochastic nature caused by the randomized splits
into diﬀerent test sets. We also note that the results of the gradient ascent
SVM are a bit better than the results obtained with an SVM in [11]. We think
that the PSO method is more capable in optimizing the metaparameters than
the grid search employed in [11]. Finally, we want to remark that the results
of the MLP are worse than those of the two other approaches.
20.5.2
Experiments on Classiﬁcation Problems
We compare the multi-layer classiﬁcation SVM to the standard SVM and
a multi-layer perceptron trained with backpropagation with one hidden layer
with sigmoid activation functions. Early stopping was implemented in the
MLP by optimizing the number of training epochs. For the comparison we
use 8 datasets from the UCI repository. In these experiments we have used
SVMLight as standard SVM and optimized the metaparameters (σ and C)
with grid search (also with around 100,000 evaluations). We also optimized
the metaparameters (number of hidden units, learning rate, number of epochs)
for the multi-layer perceptron. The metaparameters for the multi-layer SVM
are again optimized with PSO.
TABLE 20.2: The accuracies and standard errors on the 8 UCI classiﬁcation
datasets. Shown are the results of an MLP, a support vector machine (SVM),
and the two-layer SVM.
Dataset
MLP
SVM
ML-SVM
Hepatitis
84.3 ± 0.3
81.9 ± 0.3
85.1 ± 0.1
Breast Cancer W.
97.0 ± 0.1
96.9 ± 0.1
97.0 ± 0.1
Ionosphere
91.1 ± 0.1
94.0 ± 0.1
95.5 ± 0.1
Ecoli
87.6 ± 0.2
87.0 ± 0.2
87.3 ± 0.2
Glass
64.5 ± 0.4
70.1 ± 0.3
74.0 ± 0.3
Pima Indians
77.4 ± 0.1
77.1 ± 0.1
77.2 ± 0.2
Votes
96.6 ± 0.1
96.5 ± 0.1
96.8 ± 0.1
Iris
97.8 ± 0.1
96.5 ± 0.2
98.4 ± 0.1
Average
87.0
87.5
88.9
We report the results on the 8 datasets with average accuracies and stan-
dard errors. We use 90% of the data for training data and 10% for test data.

468
Regularization, Optimization, Kernels, and Support Vector Machines
We have performed 1000 new random cross validation experiments per method
with the best found metaparameters (and 4000 times for Iris and Hepatitis,
since these are smaller datasets). The results are shown in Table 20.2. The
multi-layer SVM signiﬁcantly (p < 0.05) outperforms the other methods on 4
out of 8 classiﬁcation datasets. On the other problems the multi-layer SVM
performs equally well as the other methods. We also performed experiments
with the gradient ascent SVM on these datasets, but its results are very simi-
lar to those obtained with SVMLight, so we do not show them here. On some
datasets such as Breast Cancer Wisconsin and Votes, all methods perform
equally well. On some other datasets, the multi-layer SVM reduces the error
of the SVM a lot. For example, the error on Iris is 1.6% for the multi-layer SVM
compared to 3.5% for the standard SVM. The MLP obtained 2.2% error on
this dataset. Finally, we also optimized and tested a stacking ensemble SVM
method, which uses an SVM to directly map the outputs of the pretrained
hidden-layer SVMs to the desired output without further min-max optimiza-
tion. This approach obtained 2.3% error on Iris and is therefore signiﬁcantly
outperformed by the multi-layer SVM.
20.5.3
Experiments on Dimensionality Reduction Problems
The used dataset in the dimensionality reduction experiment contains a
total of 1300 instances of gray-scaled images of the left eyes manually cropped
from pictures in the “labeled faces in the wild” dataset. The images, shown in
Figure 20.2, are normalized and have a resolution of 20 by 20 pixels, and thus
have 400 values per image. The aim of this experiment is to see how well the
autoencoder ML-SVM performs compared to some state-of-the-art methods.
The goal of the used dimensionality reduction algorithms is to accurately
encode the input data using fewer dimensions than the number of inputs. A
well known, but suboptimal technique for doing this is the use of principal
component analysis.
FIGURE 20.2: Examples of some of the cropped gray-scaled images of left
eyes that are used in the dimensionality reduction experiment.
We compared the ML-SVM to principal component analysis (PCA) and a
neural network autoencoding method. We used a state-of-the-art neural net-
work autoencoding method, named a denoising autoencoder [25], for which we
optimized the metaparameters. The autoencoders were trained using stochas-
tic gradient descent with a decreasing learning rate. In each epoch, all samples
in the training set were presented to the network in a random order. To im-

Multi-Layer Support Vector Machines
469
prove generalization performance of the standard neural network autoencoder
[23], in the denoising autoencoder each input sample is augmented with Gaus-
sian noise, while the target stayed unaltered. We also added l1 regularization
on the hidden layer of the network to increase sparsity. These additions im-
proved the performance of this non-linear autoencoder.
We also compared the ML-SVM to principal component analysis using a
multi-variate partial-least squares (PLS) regression model with standardized
inputs and outputs [29]. It can easily be shown that the standard PLS algo-
rithm in autoencoder mode is actually equivalent to a principal component
projection (with symmetric weights in the layer from the latent variable bot-
tleneck layer to the output layer). The attractiveness of applying the PLS
autoencoder in this case is the elegant and eﬃcient implementation of the
standard PLS algorithm to compute the principal components.
For these experiments, random cross validation is used to divide the data
in a training set containing two thirds (867 examples) of the dataset, and a
test set containing one third. The methods are compared by measuring the re-
construction error for diﬀerent numbers of (non-linear) principal components:
we used 10, 20, and 50 dimensions to encode the eye images. The root mean
square error of 10 runs and standard errors are computed for the comparison.
TABLE 20.3: The RMSE and standard errors for diﬀerent numbers of prin-
cipal components for principal component analysis, a denoising autoencoder
(DAE), and a multi-layer support vector machine (ML-SVM)
#dim
PCA
DAE
ML-SVM
10
0.1242 ± 0.0004
0.1211 ± 0.0002
0.1202 ± 0.0003
20
0.0903 ± 0.0003
0.0890 ± 0.0002
0.0875 ± 0.0003
50
0.0519 ± 0.0002
0.0537 ± 0.0001
0.0513 ± 0.0002
The results of these experiments can be found in Table 20.3. These re-
sults show a signiﬁcantly better (p<0.05) performance for autoencoding with
the use of a multi-layer support vector machine compared to the denoising
autoencoder and PCA. As known from the literature, the diﬀerence to PCA
decreases when more principal components are used.
20.5.4
Experimental Analysis of the Multi-Layer SVM
We also studied why the multi-layer SVM outperforms the SVM in many
cases. For this we will examine the Iris dataset again, but in more detail. For
this dataset the multi-layer SVM and the MLP perform much better than
the standard SVM with an RBF kernel (see Table 20.2). We performed the
experiments again with the previous best found metaparameters, but set the
C-values to 3.0 for all methods so that the dual-objectives of diﬀerent methods

470
Regularization, Optimization, Kernels, and Support Vector Machines
can be easily compared. This did not signiﬁcantly change the performances.
Furthermore, we set the number of epochs to 14.
Figure 20.3 shows the evolution of the training and test errors for three
methods. The reported errors are averaged over 1000 simulations. We compare
the standard SVM trained with gradient ascent, the multi-layer SVM, and a
multi-layer SVM in which the hidden-layer SVMs were not pre-trained on
perturbed class labels, but completely randomly initialized. In the case of the
standard SVM, the epochs refer to the number of repetitions of the gradient
ascent algorithm. For the multi-layer SVMs, the epoch counter is increased
after only training the output SVMs or after only training the hidden-layer
SVMs. The training times on a single training set are less than one second
for the Iris dataset for all methods. In epoch 0, the output layer SVMs were
initialized with constant positive support vector coeﬃcients and by PSO op-
timized kernel widths. Therefore, they immediately work quite well since the
SVM and the ML-SVM behave like a k-nearest neighbor or locally weighted
learning method in this case.
FIGURE 20.3: (A) Training error results on the Iris dataset. (B) Test error
results on the Iris dataset.
The results show that the standard SVM obtains a low training error
quicker than the other methods, but that its test error is higher (its best
test error is 3.7%). The best test error is obtained by the (pre-trained) multi-
layer SVM after 13 epochs, when it obtains a test error of 1.9%. The error
of the multi-layer SVM that is not pre-trained starts much higher than with
the other methods, but this method is still able to obtain a test error of
2.8% and signiﬁcantly outperforms the standard SVM. The standard SVM
obtains its best performance after a single training epoch with the gradient
ascent algorithm during which 10 training iterations of the support vector
coeﬃcients were executed. Figures 20.3(A) and 20.3(B) show that for the
multi-layer SVMs the test errors are very close to the training errors, except
for the beginning. This behavior is due to the strong regularization power of
the output-layer SVMs. Even with many hidden-layer SVMs, generalization

Multi-Layer Support Vector Machines
471
performance can be made excellent by setting the regularization parameter C
to a small value.
We also plotted the evolution of the average values of the dual-objective
function that correspond to the evolution of the training and test errors shown
before. Again this plot shows averages of 1000 simulations. Figure 20.4 shows
that the gradient ascent SVM monotonically increases the dual-objective func-
tion (between epochs 1 and 14, the dual-objective value increases from 33 to
70). As can be seen in Figure 20.3(B), this does not lead to always improving
test errors. This may have to do with not exactly fulﬁlling the bias constraint.
However, when PSO is used it optimizes the number of epochs to overcome
this problem (it found the best value of 1 for the number of epochs). The multi-
layer SVMs alternate between minimizing and maximizing the dual-objective
function. The min-max optimization process is quite complex, because multi-
ple metaparameters inﬂuence the learning updates. Therefore, the dual objec-
tive does not just increase, then decrease in the next epoch, etc. Instead, the
dual-objective function increases for some epochs, then decreases, etc., without
any signs of convergence. The three ﬁgures show that the dual-objective should
be minimized to obtain the lowest test errors. However, standard SVMs can
only maximize the dual-objective function. Therefore, the ﬂexibility of the hid-
den layer in the ML-SVM is especially fruitful to minimize the dual-objective
function and thereby obtain lower test errors.
FIGURE 20.4: The evolution of the dual-objective value on the Iris dataset.
20.6
Discussion and Future Work
The multi-layer SVM consists of a hidden layer of SVMs and an output
layer of SVMs that learn to approximate the target function using the out-
puts of the hidden-layer SVMs. The results show that the ML-SVM can out-
perform other state-of-the-art machine learning algorithms. By going from a

472
Regularization, Optimization, Kernels, and Support Vector Machines
single SVM to the multi-layer SVM, we have made the SVM a deeper architec-
ture. Compared to other deep neural network architectures, the ML-SVM has
the advantage that due to the strong regularization power of the output-layer
SVMs, the system does not easily overﬁt the data. Therefore, the ML-SVM
could potentially perform very well with very large input vectors and few
training examples. On the other hand, training an SVM with many exam-
ples is more computationally demanding than training a deep neural network
architecture.
There are several advantages of the ML-SVM algorithm. First, the method
is very ﬂexible in adapting the kernel functions compared to other multiple-
kernel learning algorithms. Second, the algorithm is straightforward to im-
plement by using the gradient ascent algorithm and the backpropagation
technique. Finally, the training method uses a min-max optimization process,
which is interesting and not (yet) applicable to neural network training.
There remains future work to be done in order to increase the power of
the ML-SVM. First of all, the current implementation uses many metaparam-
eters. Instead of using PSO to optimize the metaparameters, many diﬀerent
real-coded optimization algorithms can be employed. Second, the ML-SVM
becomes very large for large datasets and then needs a lot of training time. To
deal with large datasets, we want to explore stochastic gradient ascent tech-
niques instead of the batch gradient ascent method we used in this chapter.
We can also include more diversity in the hidden-layer SVMs, for example
by letting them use diﬀerent subsets of inputs, diﬀerent examples, or diﬀerent
kernels. Finally, we want to develop a more rigorous theory to explain why the
ML-SVM performs so well and test the ML-SVM on challenging handwriting
and image recognition datasets.
Acknowledgments
Thanks to Mark Embrechts, Adrian Millea, Arnold Meijster, Aleke Nolte,
Egbert van der Wal, Marten Schutten, Rick van der Mark, Michiel van der
Ree, and Marijn Stollenga for helping with the experiments.
Bibliography
[1] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.
[2] P. Auer, N. Cesa-Bianchi, and P. Fischer.
Finite-time analysis of the

Multi-Layer Support Vector Machines
473
multiarmed bandit problem. Machine Learning, 47(2-3):235–256, May
2002.
[3] F.R. Bach, G.R.G. Lanckriet, and M.I. Jordan. Multiple kernel learning,
conic duality, and the SMO algorithm. In Proceedings of the Twenty-First
International Conference on Machine Learning, ICML ’04, pages 6–15,
2004.
[4] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise
training of deep networks. In Advances in Neural Information Processing
Systems, volume 13, pages 153–160, 2007.
[5] Y. Cho and L. K. Saul. Kernel methods for deep learning. Advances in
Neural Information Processing Systems, 22:342–350, 2009.
[6] D.C. Ciresan, U. Meier, L.M. Gambardella, and J. Schmidhuber. Deep
big simple neural nets excel on handwritten digit recognition. Neural
Computation, 22(12):3207–3220, 2010.
[7] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning non-linear combi-
nations of kernels. Advances in Neural Information Processing Systems,
22:396–404, 2009.
[8] N. Cristianini and J. Shawe-Taylor. Support Vector Machines and Other
Kernel-Based Learning Methods. Cambridge University Press, 2000.
[9] F. Dinuzzo. Kernel machines with two layers and multiple kernel learning.
CoRR, 2010.
[10] M. Gönen and E. Alpaydin. Multiple kernel learning algorithms. Journal
of Machine Learning Research, pages 2211–2268, July 2011.
[11] M. Graczyk, T. Lasota, Z. Telec, and B. Trawinski. Nonparametric statis-
tical analysis of machine learning algorithms for regression problems. In
Knowledge-Based and Intelligent Information and Engineering Systems,
pages 111–120. 2010.
[12] G.E. Hinton.
Training product of experts by minimizing constrastive
divergence. Neural Computation, 14:1771–1800, 2002.
[13] G.E. Hinton, S. Osindero, and Y.W. Teh. A fast learning algorithm for
deep belief nets. Neural Computation, pages 1527–1554, 2006.
[14] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of
data with neural networks. Science, 313:504–507, 2006.
[15] J. Kennedy and R. Eberhart. Particle swarm optimization. In Proceedings
of the IEEE International Conference on Neural Networks, volume 4,
pages 1942–1948, 1995.

474
Regularization, Optimization, Kernels, and Support Vector Machines
[16] G.R.G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M.I. Jor-
dan. Learning the kernel matrix with semideﬁnite programming. Journal
of Machine Learnine Research, 5:27–72, 2004.
[17] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hub-
bard, and L. D. Jackel.
Back-propagation applied to handwritten zip
code recognition. Neural Computation, 1(4):541–551, 1989.
[18] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. More eﬃciency
in multiple kernel learning. In Proceedings of the 24th international con-
ference on machine learning, pages 775–782, 2007.
[19] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal
representations by error propagation. In Parallel Distributed Processing,
volume 1, pages 318–362. MIT Press, 1986.
[20] B. Schölkopf and A. Smola. Learning with Kernels: Support Vector Ma-
chines, Regularization, Optimization, and Beyond. MIT Press, 2002.
[21] S. Sonnenburg, G. Rätsch, and C. Schäfer. A general and eﬃcient multiple
kernel learning algorithm. In Advances in Neural Information Processing
Systems 18, pages 1273–1280, Cambridge, MA, 2006.
[22] J.A.K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. Van-
dewalle. Least Squares Support Vector Machines. World Scientiﬁc Pub,
2002.
[23] M. Turk and A. Pentland. Eigenfaces for recognition. Cognitive Neuro-
science, 3(1):71–86, 1991.
[24] V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag,
1995.
[25] P. Vincent, H. Larochelle, Y. Bengio, and P-A. Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings
of the 25th International Conference on Machine Learning, pages 1096–
1103, 2008.
[26] P. J. Werbos. Advanced forecasting methods for global crisis warning and
models of intelligence. In General Systems, volume XXII, pages 25–38,
1977.
[27] M.A. Wiering, M. Schutten, A. Millea, A. Meijster, and L.R.B.
Schomaker. Deep support vector machines for regression problems. In
Proceedings of the International Workshop on Advances in Regulariza-
tion, Optimization, Kernel Methods, and Support Vector Machines: The-
ory and Applications, 2013.

Multi-Layer Support Vector Machines
475
[28] M.A. Wiering, M.H. van der Ree, M.J. Embrechts, M.F. Stollenga,
A. Meijster, A. Nolte, and L.R.B. Schomaker. The neural support vec-
tor machine. In Proceedings of the 25th Benelux Artiﬁcial Intelligence
Conference (BNAIC), 2013.
[29] S. Wold, M. Sjöström, and L. Eriksson.
PLS-regression: a basic tool
of chemometrics.
Chemometrics and Intelligent Laboratory Systems,
58:109–130, 2001.
[30] D.H. Wolpert. Stacked generalization. Neural Networks, 5:241–259, 1992.
[31] Z. Xu, R. Jin, I. King, and M.R. Lyu. An extended level method for
eﬃcient multiple kernel learning.
In Advances in Neural Information
Processing Systems 20, pages 1825–1832. Curran Associates, Inc., 2008.
[32] J. Zhuang, I.W. Tsang, and S.C.H. Hoi. Two-layer multiple kernel learn-
ing. In AISTATS, pages 909–917, 2011.

This page intentionally left blank
This page intentionally left blank

Chapter 21
Online Regression with Kernels
Steven Van Vaerenbergh
Department of Communications Engineering, University of Cantabria
Ignacio Santamaría
Department of Communications Engineering, University of Cantabria
21.1
Basic Principles of Kernel Machines .............................
478
21.1.1
Kernel Methods ..........................................
478
21.1.2
Kernel Ridge Regression .................................
479
21.2
Framework for Online Kernel Methods ..........................
480
21.2.1
Online Dictionary Learning .............................
481
21.3
Kernel Recursive Least-Squares Regression Methods ...........
483
21.3.1
Recursive Updates of the Least-Squares Solution .......
484
21.3.2
Approximate Linear Dependency KRLS Algorithm ....
485
21.3.3
Sliding-Window KRLS Algorithm .......................
485
21.3.4
Bayesian Interpretation ..................................
486
21.3.5
KRLS Tracker Algorithm ................................
488
21.4
Stochastic Gradient Descent with Kernels .......................
490
21.4.1
Naive Online Regularized Risk Minimization Algorithm
491
21.4.2
Quantized KLMS ........................................
492
21.4.3
Kernel Normalized LMS .................................
492
21.5
Performance Comparisons .......................................
493
21.5.1
Online Regression on the KIN40K Data Set ............
493
21.5.2
Cost-versus-Performance Trade-Oﬀs ....................
495
21.5.3
Empirical Convergence Analysis ........................
496
21.5.4
Experiments with Real-World Data .....................
497
21.6
Further Reading ..................................................
498
Bibliography ......................................................
499
Online machine learning algorithms are designed to learn from one data in-
stance at a time. They are typically used in real-time scenarios, such as predic-
tion or tracking problems, where data arrive sequentially and instant decisions
must be made. The real-time nature of these settings implies that shortly after
the decision is made, the true label will be made available, which allows the
learning algorithm to adjust its solution before a new datum is received.
477

478
Regularization, Optimization, Kernels, and Support Vector Machines
Online kernel methods extend the nonlinear learning capabilities of stan-
dard batch kernel methods to online environments. Especially important for
these techniques is that they maintain their computational load moderate dur-
ing each iteration, in order to perform fast updates in real time. Ideally, they
should not only be able to learn in a stationary environment but also in non-
stationary settings, where they must forget outdated information and adapt
their solution to respond to changes in time. Online kernel methods also ﬁnd
use in batch scenarios where the amount of data is too high to ﬁt in the ma-
chine’s memory, and one or several passes over the data are to be performed.
In this chapter we focus on the problem of online regression. We will give
an overview of the most important kernel-based methods for this problem,
which have been developed over the span of the last decade. We start by
formulating the online solution to the kernel ridge regression problem, and
we point out diﬀerent strategies to overcome the bottlenecks associated with
using kernels in online methods. The discussed techniques are often referred
to as kernel adaptive ﬁltering algorithms, due to their close relationship with
classical adaptive ﬁlters from the signal processing literature. After reviewing
the most relevant algorithms in this area, we introduce an evaluation frame-
work that allows us to compare their performance. We ﬁnish the discussion
with a brief overview of the recent and future research directions.
21.1
Basic Principles of Kernel Machines
Kernel-based methods have had considerable success in a wide range of
areas over the past decade, since they allow us to reformulate many nonlinear
problems as convex optimization problems. In this section, we brieﬂy outline
the basic principles behind kernel methods, and we introduce the main ideas
behind constructing online kernel-based algorithms.
21.1.1
Kernel Methods
Kernel methods rely on a nonlinear transformation of the input data x
into a high-dimensional reproducing kernel Hilbert space (RKHS), in which
it is more likely that the transformed data φ(x) are linearly separable. In this
space, denoted the feature space, inner products can be calculated by using a
positive deﬁnite kernel function κ(·, ·) satisfying Mercer’s condition:
κ(x, x′) = ⟨φ(x), φ(x′)⟩,
(21.1)
where x and x′ represent two diﬀerent data points. This duality between
positive deﬁnite kernels and feature spaces allows us to transform any inner-
product based linear algorithm to an alternative, nonlinear algorithm by re-
placing the inner products with kernels [1, 15]. The solution, obtained as a

Online Regression with Kernels
479
linear functional in the feature space, then corresponds to the solution of a
nonlinear problem in the input space.
Thanks to the Representer Theorem [14], a large class of optimization
problems in RKHS have solutions that can be expressed as kernel expansions
in terms of the training data only. Speciﬁcally, kernel-based learning aims at
ﬁnding a nonlinear relationship f : X →R that can be expressed as the kernel
expansion
f(x) =
N
X
n=1
α(n)κ(xn, x).
(21.2)
In this relationship, N is the number of training data, and α(n) ∈R are
denoted the expansion coeﬃcients. The training data xn used in this expansion
are sometimes referred to as bases.
Kernel methods are non-parametric techniques, since they do not assume
any speciﬁc model. Indeed, their solution is expressed as the functional rep-
resentation (21.2) that relies explicitly on the training data.
As an introductory example algorithm, we will now describe the batch
approach to the standard kernel regression problem, known as kernel ridge
regression. Its formulation will lie at the core of the online algorithms we will
review later in this chapter.
21.1.2
Kernel Ridge Regression
Assume we are given N input data points xn, n = 1, . . . , N, in a D-
dimensional space, and the corresponding outputs yn. In order to adopt a
matrix-based formulation, we stack the input data into an N × D matrix X
and the output data into the vector y. In linear regression, the regularized
least-squares problem consists of seeking a vector w ∈RD×1 that solves
min
w ∥y −Xw∥2 + c∥w∥2,
(21.3)
where c is a positive Tikhonov regularization constant. The solution to this
problem is given by
ˆw = (X⊤X + cI)−1X⊤y.
(21.4)
In the absence of regularization, i.e., c = 0, the solution is only well deﬁned
when X has full column rank, and thus (X⊤X)−1 exists.
In order to obtain the kernel-based version of Equation (21.3), we ﬁrst
transform the data and the solution into the feature space,
min
φ(w) ∥y −φ(X)φ(w)∥2 + c∥φ(w)∥2.
(21.5)
Here, the shorthand notation φ(X) refers to the data matrix that contains the
transformed data, stacked as rows, φ(X) = [φ(x1), . . . , φ(xN)]⊤. The Repre-
senter Theorem [14] states that the solution φ(w) of this problem can be

480
Regularization, Optimization, Kernels, and Support Vector Machines
expressed as a linear combination of the training data, in particular
φ(w) =
N
X
n=1
α(n)φ(xn) = φ(X)⊤α,
(21.6)
where α = [α(1), . . . , α(N)]⊤. After substituting Equation (21.6) into Equa-
tion (21.5), and deﬁning the matrix K = φ(X)φ(X)⊤, we obtain
min
α ∥y −Kα∥2 + cα⊤Kα.
(21.7)
The matrix K is denoted as the kernel matrix, and its elements represent the
inner products in the feature space, calculated as kernels kij = κ(xi, xj).
Equation (21.7) represents the kernel ridge regression problem [12], and
its solution is given by
ˆα = (K + cI)−1y.
(21.8)
21.2
Framework for Online Kernel Methods
Online learning methods update their solution iteratively. In the standard
online learning framework, each iteration consists of several steps, as outlined
in Algorithm 19. During the n-th iteration, the algorithm ﬁrst receives an
input datum, xn. Then, it calculates the estimated output ˆyn corresponding
to this datum. The learning setup is typically supervised, in that the true
outcome yn is made available next, which enables the algorithm to calculate
the loss L(·) incurred on the data pair (xn, yn), and, ﬁnally, to update its
solution. Initialization is typically performed by setting the involved weights
to zero.
Algorithm 19 Protocol for online, supervised learning.
Initialize variables as empty or zero.
for n = 1, 2, . . . do
Observe input xn.
Predict output ˆyn.
Observe true output yn.
Update solution based on L(en), with en = yn −ˆyn.
end for
A typical setup for online system identiﬁcation with a kernel-based method
is depicted in Figure 21.1. It represents an unknown nonlinear system, whose
input data xn and response yn (including additive noise rn) can be measured
at diﬀerent time steps, and an adaptive kernel-based algorithm, which is used
to identify the system’s response.

Online Regression with Kernels
481
xn
Nonlinear system
+
f(xn)
rn
+
X →H
αn
κ(xn, ·)
−
ˆyn
+
yn
en
Adaptive algorithm
en
αn
FIGURE 21.1: Kernel-based adaptive system identiﬁcation. (Figure adapted
from [8].)
Online algorithms should be capable of operating during extended peri-
ods of time, processing large amounts of data. Kernel methods rely on the
functional representation (21.2), which grows as the amount of observations
increases. A naive implementation of an online kernel method will therefore
require growing computational resources during operation, leading to perfor-
mance issues once the available memory is insuﬃcient to store the training
data or once the computations for one update take more time than the interval
between incoming data [6].
The standard approach to overcoming this issue is to limit the number of
terms in Equation (21.2) to a representative subset of observations, called a
dictionary. In the sequel, we will review several dictionary-learning methods
that can be carried out online. They will serve as building blocks for online
kernel methods.
21.2.1
Online Dictionary Learning
The dictionary learning process aims to identify the bases in the expansion
(21.2) that can be discarded without incurring a signiﬁcant performance loss.
After discarding these bases, a new, approximate expansion is obtained as
ˆf(x) =
m
X
i=1
α(i)κ(ui, x),
(21.9)
where m < N. The dictionary D = {u1, . . . , um} consists of several data
vectors ui which are selected from the input data xn. They should be chosen
carefully so that they represent the entire input data set suﬃciently well.
In the online setting, the classical way of constructing a dictionary is by
growing, i.e., by starting from an empty dictionary and gradually adding those
bases that fulﬁll a certain criterion. If the dictionary is not allowed to grow

482
Regularization, Optimization, Kernels, and Support Vector Machines
beyond a speciﬁed maximum size, it may be necessary to discard bases at
some point. This process is referred to as pruning. In Table 21.1 we have
listed several criteria to grow dictionaries, in the top half, and several criteria
to prune them, in the lower half. We will review these criteria here brieﬂy, and
revisit them in detail when discussing the learning algorithms that use them.
TABLE 21.1: Standard criteria for deciding whether to include new data
when growing a dictionary, and which data to prune.
criterion
type
complexity
all
growing
—
coherence
growing
O(m)
ALD
growing
O(m2)
oldest
pruning
—
least weight
pruning
O(m)
least a posteriori SE
pruning
O(m2)
A simple criterion to check whether the newly arriving datum is suﬃciently
informative is called the coherence criterion [11]. Given the dictionary D at
some iteration and the newly arriving datum x, this criterion deﬁnes the
coherence of the datum as the quantity
µ = max
ui∈D κ(ui, x).
(21.10)
In essence, the coherence criterion checks the similarity, as measured by the
kernel function, between the new datum and the most similar dictionary point.
Only if the coherence is below a certain predeﬁned threshold, µ ≤µ0, the
datum is inserted into the dictionary. The higher the threshold µ0 is chosen,
the more data will be accepted in the dictionary. It is a simple and eﬀective
criterion that has low computational complexity: it only requires calculating
m kernel functions.
A more sophisticated dictionary growth criterion was introduced in [5].
Whenever a new datum is observed, this criterion measures how well the
datum can be approximated in the feature space as a linear combination of
the dictionary bases in that space. It does so by checking if the following
approximate linear dependence (ALD) condition holds:
δ := min
a

m
X
i=1
a(i)φ(ui) −φ(x)

2
≤ν,
(21.11)
where ν is a precision threshold. The lower ν is chosen, the more data will be
accepted into the dictionary. In contrast to the coherence criterion, which only
compares the new datum to one dictionary basis at a time, the ALD criterion
looks for the best combination of all bases. This search corresponds to a least-
squares problem, which, if solved iteratively, requires quadratic complexity in

Online Regression with Kernels
483
terms of M, per iteration. In comparison to the coherence criterion, ALD is
computationally more complex. In return, it is able to construct more compact
dictionaries that represent the same information with fewer bases.
In practice, it is often necessary to specify a maximum dictionary size M,
or budget, that may not be exceeded. In order to avoid exceeding the budget,
one could simply stop allowing any inclusions in the dictionary once the budget
is reached, hence locking the dictionary. Nevertheless, it is not unimaginable
that after reaching the budget some new datum may still be observed that
is more informative than a currently stored dictionary basis. In this case, the
quality of the algorithm’s solution will improve by pruning the said dictionary
basis and by adding the new, more informative datum.
At this point it is interesting to remark that there exists a conceptual
diﬀerence between growing and pruning strategies: While growth criteria are
concerned with determining whether or not to include a new datum, pruning
criteria deal with determining which datum to discard.
In time-varying environments, it may be useful to simply discard the oldest
bases, as these were observed when the underlying model was possibly most
diﬀerent from the current model. This strategy is at the core of sliding-window
algorithms, which, in every iteration, accept the new datum and discard the
oldest basis, thereby maintaining a dictionary of ﬁxed size [22].
A diﬀerent pruning strategy is obtained by observing that the solution
takes the form of the functional representation (21.9). In this kernel expan-
sion, each dictionary element ui has an associated weight α(i). Hence, a low-
complexity pruning strategy simply consists of discarding the dictionary ele-
ment that has the least weight |α(i)| associated to it, as proposed in [16].
A more sophisticated pruning strategy was introduced in [3] and [4]. It
consists of selecting the element that causes the least squared error (SE) to
the solution after being pruned. The relevance of a dictionary basis, according
to this criterion, can be calculated as
|α(i)|
[K−1]ii
,
(21.12)
where [K−1]ii is the i-th element on the diagonal of the inverse kernel matrix,
calculated for the dictionary bases. We will refer to this criterion as the least
a posteriori SE criterion. Similarly to the ALD criterion, K−1 and α can be
updated iteratively, yielding a complexity per iteration of O(m2).
21.3
Kernel Recursive Least-Squares Regression
Methods
We now formulate the recursive update for the kernel ridge regression
solution (21.8), known as kernel recursive least-squares (KRLS). First, we

484
Regularization, Optimization, Kernels, and Support Vector Machines
describe the evergrowing approach, in which all training data appear in the
solution. By introducing dictionary strategies into this technique, we then
obtain several diﬀerent practical algorithms that limit their solution’s growth.
21.3.1
Recursive Updates of the Least-Squares Solution
Assume an online scenario in which n −1 data have been processed at the
n −1-th iteration. The regression solution from Equation (21.8) reads
αn−1 = ˙K
−1
n−1yn−1,
(21.13)
where we denote the regularized kernel matrix with a dot, ˙K = K+cI, in order
to simplify the notation. In the next iteration, n, a new data pair (xn, yn) is
received and we wish to update the solution (21.13) recursively. Following the
online protocol from Algorithm 19, we ﬁrst calculate the predicted output
ˆyn = k⊤
n αn−1,
(21.14)
in which kn = [κ(xn, x1), . . . , κ(xn, xn−1)]⊤, and we obtain the a priori error
for this datum, en = yn −ˆyn. The updated kernel matrix is
˙Kn =
 ˙Kn−1
kn
k⊤
n
knn + c

,
(21.15)
with knn = κ(xn, xn). By introducing the variables
an = ˙K
−1
n−1kn,
(21.16)
and
γn = knn + c −k⊤
n an,
(21.17)
the new inverse kernel matrix is calculated as
˙K
−1
n
= 1
γn

γn ˙K
−1
n−1 + ana⊤
n
−an
−an
1

.
(21.18)
Finally, the updated solution αn is obtained as
αn =
αn−1 −anen/γn
en/γn

.
(21.19)
Equations (21.18) and (21.19) are eﬃcient updates that allow us to obtain
the new solution in O(n2) time and memory, based on the previous solu-
tion. Directly applying Equation (21.13) at iteration n would require O(n3)
cost, so the recursive procedure is preferred in online scenarios. For a detailed
derivation of this evergrowing formulation we refer to [5, 18].

Online Regression with Kernels
485
21.3.2
Approximate Linear Dependency KRLS Algorithm
The KRLS algorithm from [5] uses the recursive solution we described
previously, and it introduces the ALD criterion in order to reduce the growth
of the functional representation. While the name KRLS was coined for the
algorithm proposed in [5], this algorithm is only one of the many possible
implementations of the KRLS principle. We will therefore refer to it as ALD-
KRLS. An outline of this algorithm is given in Algorithm 20.
At every iteration, ALD-KRLS decides whether or not to increase its or-
der, based on its dictionary growth criterion. If the criterion is fulﬁlled, it
performs a full update, consisting of an order increase of the dictionary and
the algorithm variables. If the criterion is not fulﬁlled, the dictionary is main-
tained, but instead of simply discarding the data pair (xn, yn) altogether, the
solution coeﬃcients are updated (though not expanded) with the information
contained in this datum. We denote this type of update as a reduced update.
Algorithm 20 KRLS algorithm with reduced growth.
Initialize variables as empty or zero.
for n = 1, 2, . . . do
Observe input xn.
Predict output: Equation (21.14)
Observe true output yn.
if dictionary growth criterion is fulﬁlled then
Expand dictionary: Dn = Dn−1 ∪{xn}
Update inverse kernel matrix: Equation (21.18)
Update expansion coeﬃcients: Equation (21.19)
else
Maintain dictionary: Dn = Dn−1
Perform reduced update of expansion coeﬃcients.
end if
end for
ALD-KRLS does not include regularization, c = 0. In this case, the co-
eﬃcients an = [an(1), . . . , an(m)]⊤that minimize the ALD condition (21.11)
are given by Equation (21.16), and the norm of the best linear combination
is given by δn = γn. In order to perform a reduced update, ALD-KRLS keeps
track of a projection matrix that is used to project the information of redun-
dant bases onto the current solution, before discarding them. For detail, refer
to [5].
21.3.3
Sliding-Window KRLS Algorithm
ALD-KRLS assumes a stationary model, and, therefore, it is not suitable
as a tracking algorithm. In addition, there does not seem to be an obvious
extension that allows for tracking, mainly because it summarizes past infor-

486
Regularization, Optimization, Kernels, and Support Vector Machines
mation into a compact formulation that allows for little manipulation. This
is a somewhat surprising fact, taking into account that it is derived from the
linear recursive least-squares (RLS) algorithm, which is easily extendible into
several tracking formulations, for instance by considering a forgetting factor.
In order to address this issue, a sliding-window KRLS (SW-KRLS) algo-
rithm was proposed in [22]. Instead of summarizing previous data, it simply
stores a window of the last M data as its dictionary. In each step it adds
the new datum and discards the oldest datum, leading to a sliding-window
approach. In order to expand the inverse kernel matrix with a new datum it
uses Equation (21.18). To discard the oldest datum it relies on the following
relationship. By breaking up the kernel matrix and its inverse as
˙Kn−1 =

a
bT
b
D

,
˙K
−1
n−1 =

e
fT
f
G

,
(21.20)
the inverse of the reduced kernel matrix is found as
D−1 = G −ffT /e.
(21.21)
Finally, the coeﬃcients αn are obtained through Equation (21.13), in which
the vector yn now contains only the M last outputs. This vector is stored
along with the dictionary.
SW-KRLS is a conceptually very simple algorithm that obtains reasonable
performance in a wide range of scenarios, including non-stationary environ-
ments. Nevertheless, its performance is limited by the quality of the bases in
its dictionary, over which it has no control. In particular, it has no means to
avoid redundancy in its dictionary or to maintain older bases that are relevant
to its kernel expansion. In order to improve this performance, a ﬁxed-budget
KRLS (FB-KRLS) algorithm was proposed in [21]. Instead of discarding the
oldest data point in each iteration, it discards the data point that causes
the least error upon being discarded, using the least a posteriori SE pruning
criterion from Table 21.1. In stationary scenarios, this extension obtains sig-
niﬁcantly better results. In non-stationary cases, however, it does not oﬀer any
advantage, and a diﬀerent approach is required, as we discuss in the sequel.
21.3.4
Bayesian Interpretation
The standard KRLS equations, as described above, can also be derived
from a Bayesian perspective. As we will see, the obtained solution is equivalent
to the KRLS update, though the Bayesian approach does not only provide the
mean value for the predicted solution but also its entire posterior distribution.
The full derivation, which is based on the framework of Gaussian Processes
(GPs) [10], can be found in [18, 9].
A Bayesian setting requires a model that describes the observations, and
priors on the parameters of this model. The GP regression model assumes that
the outputs can be modeled as some noiseless latent function of the inputs

Online Regression with Kernels
487
plus an independent noise component
y = f(x) + r,
(21.22)
and then sets a zero mean1 GP prior on f(x) and a Gaussian prior on r:
f(x) ∼GP(0, κ(x, x′)),
r ∼N(0, σ2),
(21.23)
where σ2 is a hyperparameter that speciﬁes the noise power. The notation
GP(m(x), κ(x, x′)) refers to a GP distribution over functions in terms of a
mean function m(x) (zero in this case) and a covariance function κ(x, x′),
equivalent to a kernel function. The covariance function speciﬁes the a priori
relationship between values f(x) and f(x′) in terms of their respective loca-
tions, and it is parameterized by a small set of hyperparameters, grouped in
vector θ.
By deﬁnition, the marginal distribution of a GP at a ﬁnite set of points
is a joint Gaussian distribution, with its mean and covariance being speciﬁed
by the homonymous functions evaluated at those points. Thus, the joint dis-
tribution of outputs y = [y1, . . . , yN]⊤and the corresponding latent vector
f = [f(x1), . . . , f(xN)]⊤is

y
f

∼N

0,
K + σ2I
K
K
K

.
(21.24)
By conditioning on the observed outputs y, the posterior over the latent vector
can be inferred
p(f|y) = N(f|K(K + σ2I)−1y, K −K(K + σ2I)−1K)
= N(f|µ, Σ).
(21.25)
Assuming this posterior is obtained for the data up till time instant n−1, the
predictive distribution of a new output yn at location xn is computed as
p(yn|xn, yn−1) = N(yn|µGP,n, σ2
GP,n)
(21.26a)
µGP,n = k⊤
n (Kn−1 + σ2I)−1yn−1
(21.26b)
σ2
GP,n = σ2 + knn −k⊤
n (Kn−1 + σ2I)−1kn.
(21.26c)
The mode of the predictive distribution, given by µGP,n in Equation (21.26b),
coincides with the solution of KRLS, given by Equation (21.18), showing that
the regularization in KRLS can be interpreted as a noise power σ2. Further-
more, the variance of the predictive distribution, given by σ2
GP,n in Equa-
tion (21.26c), coincides with Equation (21.17), which is used by the dictionary
criterion for ALD-KRLS.
1It is customary to subtract the sample mean from the data {yn}N
n=1, and then to assume
a zero-mean model.

488
Regularization, Optimization, Kernels, and Support Vector Machines
Using Equations (21.26), a recursive update of the complete GP can be
found as
p(f n|Xn, yn) = N(f n|µn, Σn)
(21.27a)
µn =
µn−1
ˆyn

+ en
ˆσ2yn
 hn
ˆσ2
fn

(21.27b)
Σn =
Σn−1
hn
h⊤
n
ˆσ2
fn

−
1
ˆσ2yn
 hn
ˆσ2
fn
  hn
ˆσ2
fn
⊤
,
(21.27c)
where hn = Σn−1K−1
n−1kn, and ˆσ2
fn and ˆσ2
yn are the predictive variances of
the latent function and the new output, respectively, calculated at the new
input. Details can be found in [18].
The update Equations (21.27) are formulated in terms of the predictive
mean and covariance, µn and Σn, which allows us to interpret them directly
in terms of the underlying GP. They can be reformulated in terms of αn and
a corresponding matrix, as shown in [3]. Speciﬁcally, the relationship between
µn and αn is (see [18])
αn = K−1
n µn
(21.28a)
= ˙K
−1
n yn.
(21.28b)
Interestingly, while standard KRLS obtains αn based on the noisy observa-
tions yn through Equation (21.28b), the GP-based formulation shows that
αn can be obtained equivalently using the values of the noiseless function
evaluated at the inputs µn, through Equation (21.28a).
The advantage of using a full GP model is that not only does it allow us
to update the predictive mean, as does KRLS, but it keeps track of the entire
predictive distribution of the solution [9]. This allows, for instance, establishing
conﬁdence intervals when predicting new outputs. And, more importantly in
adaptive contexts, it allows us to explicitly handle the uncertainty about all
learned data. In the sequel, we will review a recursive algorithm that exploits
this knowledge to perform tracking.
21.3.5
KRLS Tracker Algorithm
In [18], a KRLS tracker (KRLS-T) algorithm was devised that explicitly
handles uncertainty about the data, based on the above discussed probabilis-
tic Bayesian framework. While in stationary environments it operates identi-
cally to the earlier proposed Sparse Online GP algorithm (SOGP) from [3],
it includes a forgetting mechanism that enables it to handle non-stationary
scenarios as well.
In non-stationary scenarios, adaptive online algorithms should be capable
of tracking the changes of the observed model. This is possible by weighting
past data less heavily than more recent data. A quite radical example of

Online Regression with Kernels
489
forgetting is provided by the SW-KRLS algorithm, which either assigns full
validity to the data (those in its window), or discards them entirely.
KRLS-T includes a framework that permits several forms of forgetting.
We focus on the forgetting strategy called “back to the prior” (B2P), in which
the mean and covariance are replaced through
µ ←
√
λµ
(21.29a)
Σ ←λΣ + (1 −λ)K.
(21.29b)
As shown in [18], this particular form of forgetting corresponds to blending
the informative posterior with a “noise” distribution that uses the same color
as the prior. In other words, forgetting occurs by taking a step back towards
the prior knowledge. Since the prior has zero mean, the mean is simply scaled
by the square root of the forgetting factor λ. The covariance, which represents
the posterior uncertainty on the data, is pulled towards the covariance of the
prior. Interestingly, a regularized version of RLS (known as extended RLS)
can be obtained by using a linear kernel with the B2P forgetting procedure.
Standard RLS can be obtained by using a diﬀerent forgetting rule (see [18]).
Equations (21.29) may seem like an ad-hoc step to enable forgetting. How-
ever, it can be shown that the whole learning procedure — including the
mentioned controlled forgetting step — corresponds exactly to a principled
non-stationary scheme within the GP framework, as described in [20]. It is
suﬃcient to consider an augmented input space that includes the time stamp
t of each sample and deﬁne a spatio-temporal covariance function:
κst([t x⊤]⊤, [t′ x′⊤]⊤) = κt(t, t′)κs(x, x′),
(21.30)
where κs(x, x′) is the already-known spatial covariance function and κt(t, t′)
is a temporal covariance function giving more weight to samples that are
closer in time. Inference on this augmented model eﬀectively accounts for
non-stationarity in f(·) and recent samples have more impact in predictions
for the current time instant. It is fairly simple to include this augmented
model in the online learning process described in the previous section. When
the temporal covariance is set to kt(t, t′) = λ
|t−t′|
2
, λ ∈(0, 1], inference in the
augmented spatio-temporal GP model is exactly equivalent to using (21.29)
after each update (21.27).
This equivalence has interesting consequences. Most importantly, it implies
that the optimal hyperparameters for the recursive problem can be determined
by performing standard GP hyperparameter estimation techniques, such as
Type-II maximum likelihood estimation, on the equivalent spatio-temporal
batch problem. This is an important accomplishment in kernel adaptive ﬁlter-
ing theory, as it allows us to determine the hyperparameters in a principled
manner, including kernel parameters, the noise level, and the forgetting factor
λ. See [20] for further details.
The KRLS-T algorithm is summarized in Algorithm 21. Its ﬁrst step in
each iteration consists of applying a forgetting strategy, which takes away

490
Regularization, Optimization, Kernels, and Support Vector Machines
Algorithm 21 KRLS Tracker (KRLS-T) algorithm.
Initialize variables as empty or zero.
for n = 1, 2, . . . do
Forget: replace µn−1 and Σn−1 through Equations 21.29
Observe input xn.
Calculate predictive mean: ˆyn = knK−1
n−1µn−1
Calculate predictive variance ˆσ2
yn.
Observe true output yn.
Compute µn, Σn, K−1
n .
Add basis xn to the dictionary.
if number of bases in the dictionary > M then
Determine the least relevant basis, um.
Remove basis um from µn, Σn, K−1
n
Remove basis um from the dictionary.
end if
end for
some of the weight of older information. KRLS-T accepts every datum into
its dictionary (as long as this does not render K−1
n
rank-deﬁcient), and at
the end of each iteration it prunes the least relevant basis, after projecting
its information onto the remaining bases. For pruning, it uses the least a
posteriori SE (see Table 21.1). Additional details can be found in [18].
21.4
Stochastic Gradient Descent with Kernels
Up till this point we have reviewed several online algorithms that recur-
sively estimate the batch solution to the kernel ridge regression problem. These
algorithms have quadratic complexity in terms of the number of data that they
store in their dictionary, O(m2), which may be excessive in certain scenarios.
It is possible to obtain algorithms with lower complexity, typically O(m),
by performing approximations to the optimal recursive updates, as we will
describe here.
The starting point is, again, the kernel ridge regression problem, which we
repeat for convenience:
min
φ(w) J = ∥y −φ(X)φ(w)∥2 + c∥φ(w)∥2.
Earlier, we dealt with techniques that focus on the batch solution to this
problem. The same solution can be obtained through an iterative procedure,
called the steepest-descent method [13]. It consists in iteratively applying the

Online Regression with Kernels
491
rule
φ(w) ←φ(w) −η
2
∂J
∂φ(w),
(21.31)
where η represents a learning rate. After replacing the derivative in Equa-
tion (21.31) by its instantaneous estimate, and, omitting regularization mo-
mentarily, we obtain a low-cost online algorithm with the following stochastic
gradient descent update rule
φ(wn) = φ(wn−1) + η
 ynφ(xn) −φ(xn)φ(wn−1)⊤φ(xn)

= φ(wn−1) + ηenφ(xn).
By relying on the representer theorem [14], φ(wn) and φ(wn−1) are expressed
as linear combinations of the transformed data, yielding
n
X
i=1
αn(i)φ(xi) =
n−1
X
i=1
αn−1(i)φ(xi) + ηenφ(xn),
(21.32)
where αn(i) denotes the i-th element of the vector αn. If regularization is not
omitted, the update rule reads
n
X
i=1
αn(i)φ(xi) = (1 −ηc)
n−1
X
i=1
αn−1(i)φ(xi) + ηenφ(xn).
(21.33)
The update (21.32) is the core equation used to derive kernel least mean square
(KLMS) algorithms. In essence, these algorithms are kernelized versions of the
classical least-mean-squares (LMS) algorithm [13]. Similar to the previously
discussed online kernel algorithms, KLMS algorithms usually also build an
online dictionary, but since their core update is of linear complexity in term
of the number of points in the dictionary, O(M), their dictionary update
should not exceed this complexity.
KLMS algorithms possess the interesting property that their learning rule
also provides them with a tracking mechanism, at no additional cost. KRLS
algorithms, on the other hand, require their standard design to be speciﬁcally
extended in order to obtain this property, as we discussed. In what follows
we will discuss the mechanics of the three most popular KLMS algorithms,
highlighting their similarities and diﬀerences.
Several forms exist to obtain the new weights αn in such a way that Equa-
tion (21.32) holds. The simplest form to obtain the weights at step n consists
of maintaining the previous weights, i.e., αn(i) = αn−1(i), for i = 1, . . . , n −1
and adding a new weight αn(n) that accounts for the term ηenφ(xn). This is
the update mechanism behind NORMA [6] and Q-KLMS [2].
21.4.1
Naive Online Regularized Risk Minimization
Algorithm
Naive online regularized risk minimization algorithm (NORMA) is a family
of stochastic-gradient online kernel-based algorithms [6]. It includes regular-

492
Regularization, Optimization, Kernels, and Support Vector Machines
ization and thus uses Equation (21.33) as it basic update. We will focus on its
standard form for regression with a squared loss function. By concentrating
all the novelty in the new coeﬃcients αn, the update for NORMA at time
step n reads
αn =
(1 −ηc)αn−1
ηen

.
(21.34)
Note that the coeﬃcients shrink as n grows. Therefore, after a certain amount
of iterations, the oldest coeﬃcient can be discarded without aﬀecting the solu-
tion’s quality. Hence, a sliding-window dictionary mechanism is obtained that
prevents the functional representation from growing too large during online
operation.
21.4.2
Quantized KLMS
A second algorithm that concentrates all the novelty in one coeﬃcient is
quantized KLMS (Q-KLMS) [2]. The main characteristic of Q-KLMS is that
it slows down its growth by constructing a dictionary through a quantization
process. For each new datum, Q-KLMS uses the coherence criterion from [11]
to check whether or not to add the datum to the dictionary. Q-KLMS was
proposed based on a speciﬁc version of the coherence criterion that uses the
Euclidean distance, though any kernel could be used in its criterion. Note
that the calculation of the coherence criterion has linear cost in terms of the
current dictionary size, M, making it especially useful for KLMS algorithms.
If the coherence condition is not fulﬁlled, µ ≤µ0, Q-KLMS adds the new
datum to its dictionary and updates the coeﬃcients as follows
αn =
αn−1
ηen

.
(21.35)
If the coherence condition is fulﬁlled, µ > µ0, Q-KLMS only updates the
coeﬃcient of the dictionary element that is closest to the new datum. If we
denote the index of the closest dictionary element as j, the update rule for
this case reads
αn(j) =
αn−1(j) + ηen

,
(21.36)
Note that Q-KLMS does not include regularization. As shown in [7], it is a
member of a class of KLMS algorithms that possess a self-regularizing prop-
erty.
21.4.3
Kernel Normalized LMS
Instead of concentrating all the novelty in one coeﬃcient, a diﬀerent ap-
proach is followed in [11]. Speciﬁcally, the new coeﬃcient vector αn is obtained
by projecting the previous vector, αn−1, onto the line deﬁned by k⊤
n α−yn = 0.

Online Regression with Kernels
493
As shown in [11], this yields the following normalized KLMS update
αn =

αn−1
0

+
ηen
ϵ + ∥kn∥2

kn
knn

,
(21.37)
when the dictionary is to be expanded, where ϵ is a regularization constant.
This algorithm, denoted kernel normalized LMS (KNLMS) [11], uses the online
sparsiﬁcation based on the coherence criterion to determine whether or not
to include new data. In particular, when the new datum does not meet the
coherence condition, KNLMS updates its coeﬃcients without increasing the
order, following the rule
αn = αn−1 +
ηen
ϵ + ∥kn∥2 kn.
(21.38)
21.5
Performance Comparisons
The regression performance of online kernel methods has been studied in
several ways. The standard manner to compare the performance is to analyze
their learning curves, which depict their regression error over time. This, how-
ever, requires us to choose the parameters for each algorithm that are optimal
in some sense. We will analyze some learning curves ﬁrst, on a standard data
set, and then we will show how a more global comparison can be obtained
that encompasses multiple parameter conﬁgurations for each algorithm. Un-
less stated otherwise, we will use a Gaussian kernel of the form
κ(x, x′) = σ2
0 exp

−||x −x′||2
2ℓ2

,
(21.39)
in which σ2
0 is the signal power and ℓis the length scale.
The results from this section can be reproduced by the code included in
an open-source toolbox, available at http://sourceforge.net/projects/
kafbox/, which we have developed speciﬁcally for this purpose. It contains
implementations of the most popular kernel adaptive ﬁltering algorithms.
21.5.1
Online Regression on the KIN40K Data Set
In the ﬁrst experiment we train the online algorithms to perform regression
of the KIN40K data set2. This data set is obtained from the forward kinematics
of an 8-link all-revolute robot arm, and it represents a stationary regression
problem. It contains 40000 examples, each consisting of an 8-dimensional input
2Available at http://www.cs.toronto.edu/~delve/data/datasets.html.

494
Regularization, Optimization, Kernels, and Support Vector Machines
vector and a scalar output. We randomly select 10000 data points for training
and use the remaining 30000 points for testing the regression.
For all algorithms we use an anisotropic Gaussian kernel in which the
hyperparameters were determined oﬄine by standard GP regression. In par-
ticular, the noise-to-signal ratio was σ2
n/σ2
0 = 0.0021. Each algorithm performs
a single run over the data. The performance is measured as the normalized
mean-square error (NMSE) on the test data set at diﬀerent points throughout
the training run.
The results are displayed in Fig. 21.2. The algorithm-speciﬁc parameters
were set as follows: ALD-KRLS uses ν = 0.1, KRLS-T uses M = 500, SW-
KRLS uses M = 500, Q-KLMS uses η = 0.5 and ϵu = 1, and NORMA uses
η = 0.5 and M = 1500. Note that apart from the method mentioned in [20]
and the typical cross-validation, parameters for these algorithms are typically
determined by heuristics.
FIGURE 21.2: Learning curves on the KIN40K data set, with speciﬁc pa-
rameters per algorithm.
While Figure 21.2 shows some interesting results, one may wonder if it is
possible to improve the performance of a speciﬁc algorithm by tweaking its
parameters. Indeed, algorithms that use a budget parameter M to determine
their maximum dictionary size will typically obtain lower NMSE values if the
budget is raised. Nevertheless, this will increase their computational complex-
ity. A similar phenomenon is observed for all algorithms. Algorithms that use
a threshold to determine their budget, such as ALD-KRLS and Q-KLMS, ob-
tain a better steady-state NMSE at the cost of higher complexity. There is
thus a trade-oﬀbetween the cost of an algorithm, in terms of computation
and memory required, and its performance, in terms of the error it obtains
and how fast it converges to its steady-state. In the following, we will analyze
these trade-oﬀs instead of the learning curves, as they may provide us with a
more global picture of the performance of the algorithms.

Online Regression with Kernels
495
21.5.2
Cost-versus-Performance Trade-Oﬀs
The computational cost of an algorithm is often measured as the CPU
time. Nevertheless, this measure depends on the machine and the particular
implementation of the algorithm. For a fairer comparison, we use a count of
the ﬂoating point operations (FLOPS) instead, which are measured explicitly
by the toolbox used for the experiments. We also measure the used memory,
in terms of the number of bytes necessary to store the variables. In the results,
we report the maximum FLOPS and maximum bytes per iteration, as these
are the values that impose limits on the hardware.
FIGURE 21.3: Maximum FLOPS used per iteration (left) and maximum
bytes used per iteration (right), in the online regression experiment on the
KIN40K data set, for diﬀerent parameter conﬁgurations and diﬀerent algo-
rithms. Parameter values are represented in Table 21.2 and the black dots
represent the ﬁrst conﬁguration, for each algorithm.
Figure 21.3 illustrates the trade-oﬀs obtained by each algorithm. The
markers represent the results obtained for diﬀerent sets of algorithm parame-
ters. Each algorithm has one budget-related parameter, which also determines
the computational and memory complexities. We ﬁx every other parameter
to a value close to its optimum and vary the budget parameter over a wide
range. The full parameter values are displayed in Table 21.2. The steady-state
NMSE is measured as the average NMSE over the last 1000 iterations.
The best-performing algorithm conﬁgurations are located to the left in
both plots of Figure 21.3, corresponding to low steady-state errors, and to the
bottom, corresponding to low algorithmic complexities. The black dots show
the ﬁrst conﬁguration of each algorithm, and typically they also represent an
initial parameter setting beyond which it is diﬃcult to move, for instance due
to numerical limits.
By leaving the NMSE results out of Figure 21.3, we obtain the plot of
Figure 21.4. It shows that for all algorithms there is an approximately linear
relationship between the number of bytes stored in memory and the number

496
Regularization, Optimization, Kernels, and Support Vector Machines
TABLE 21.2: Parameters used in the KIN40K online regression.
Method
Fixed parameter
Varying parameters
NORMA
η = 0.5
τ = 100, 200, 500, 1000, 1500, 2000
Q-KLMS
η = 0.5
ϵu = 1, 1.2, 1.5, 2, 3, 5, 10, 12, 15
SW-KRLS
—
M = 10, 20, 50, 100, 200, 300, 400, 500
ALD-KRLS
—
ν = .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99
KRLS-T
λ = 1
M = 10, 20, 50, 100, 200, 300, 400, 500
of FLOPS required per iteration. Notice the logarithmic scales used. Algo-
rithms that lie below the diagonal are more eﬃcient with computation, i.e.,
when using the same amount of memory they require less computations, for
a given NMSE. Algorithms that lie above the diagonal are more eﬃcient with
memory, i.e., when performing the same amount of computation they require
less memory, for a given NMSE.
FIGURE 21.4: FLOPS per iteration versus bytes stored, in the KIN40K
experiment.
21.5.3
Empirical Convergence Analysis
Apart from the steady-state error, an important measure for online and
adaptive algorithms is the speed at which they converge to this error, called
the convergence rate. Some theoretical convergence analyses have been carried
out on speciﬁc algorithms, for instance the KNLMS algorithm in [8]. Here, in
line with the previous experiments, we will perform an empirical convergence
analysis that compares several algorithms.

Online Regression with Kernels
497
FIGURE 21.5: Trade-oﬀbetween steady-state error and convergence rate
in the KIN40K experiment.
In order to estimate the convergence rate, we measure the number of it-
erations it takes an algorithm to get within 1 dB of its steady-state error.
The trade-oﬀbetween this measure and the steady-state NMSE is shown in
Figure 21.5. Again, results that are most to the left or to the bottom of the
plot represent the most interesting algorithm conﬁgurations, as they reach the
lowest steady-state error or have the fastest convergence, respectively.
21.5.4
Experiments with Real-World Data
Figure 21.6 shows the convergence results obtained on two real-world data
sets. The ﬁrst data set is obtained by measuring the response of a wireless
communication channel. The online algorithm requires us to learn the nonlin-
ear channel response and track its changes in time. The second data set is a
recording of a patient’s body surface during robotic radiosurgery. The online
algorithm is used to predict the position of several markers on the body as
the patient moves, so that the robot can use this prediction to compensate for
its mechanical delay in positioning itself. More detailed descriptions of both
experiments can be found in [19]. The parameters used in both experiments
can be found in Tables 21.3 and 21.4, respectively.
In the ﬁrst case, as the solution is to be implemented for real-time op-
eration on a compact, low-power device, the maximum amount of FLOPS
is typically ﬁxed and the algorithm that obtains the lowest error under this
condition is chosen for implementation. In the second case there is usually
no such restriction on complexity, as large surgical robots can be equipped
with suﬃcient resources. Instead, the main requirement is to maintain the
prediction error as low as possible.

498
Regularization, Optimization, Kernels, and Support Vector Machines
FIGURE 21.6: Results obtained for identifying the wireless communication
channel (left) in the identiﬁcation problem and for predicting a patient’s po-
sition in the prediction problem.
TABLE 21.3: Parameters used in the online channel identiﬁcation.
Method
Fixed
Varying parameters
NORMA
η = 0.5
τ = 5, 10, 20, 50, 100, 200, 310, 400, 500, 700, 1000
Q-KLMS
η = 0.6
ϵu = 0.1, 1, 2, 3, 4, 5, 6, 7, 8
SW-KRLS
—
M = 5, 10, 15, 20, 30, 50, 70, 100, 150, 200, 300
KRLS-T
λ = 0.995
M = 2, 5, 10, 15, 20, 30, 50, 100, 200
TABLE 21.4: Parameters used in the motion prediction.
Method
Fixed
Varying parameters
NORMA
η = 0.99
τ = 3, 4, 5, 10, 15, 20, 30, 40, 60
Q-KLMS
η = 0.99
ϵu = .2, .5, 1, 2, 2.5, 2.75, 4, 6, 7
ALD-KRLS
—
ν = .0001, .001, .003, .01, .02, .05, .1, .3, .5
KRLS-T
λ = 0.999
M = 3, 4, 5, 7, 10, 20, 50, 70, 100
21.6
Further Reading
We have given an overview of one decade of research in the ﬁeld of online
regression with kernels. The standard approach followed in this ﬁeld, which
consists of constructing kernel-based versions of classical adaptive ﬁltering

Online Regression with Kernels
499
algorithms such as LMS and RLS, has produced several eﬃcient state-of-the-
art algorithms. Some other, related classes of algorithms that we have not
discussed here are kernel aﬃne projection algorithms (KAPA) [11, 7], which
occupy the middle ground between KLMS and KRLS, and projection-based
subgradient methods [17].
Several new directions are also being explored to improve the learning
capabilities of these algorithms. A major focus of new algorithms is on the
automatic learning of hyperparameters. Some algorithms pursue this by per-
forming stochastic natural gradient descent in an online manner, in order to
approximately maximize the marginal likelihood [10]. Other algorithms fol-
low approaches inspired by the recent advances in neural networks, and they
focus on online multi-kernel learning, where the parameter learning consists
of assigning weights to several diﬀerent kernels that are applied in parallel
[23]. Many approaches in this direction seek sparse solutions by performing
L1-norm based learning.
Bibliography
[1] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the
American mathematical society, 68(3):337–404, 1950.
[2] Badong Chen, Songlin Zhao, Pingping Zhu, and José C. Príncipe. Quan-
tized kernel least mean square algorithm. IEEE Transactions on Neural
Networks and Learning Systems, 23(1):22–32, January 2012.
[3] Lehel Csató and Manfred Opper. Sparse online Gaussian processes. Neu-
ral Computation, 14(3):641–668, 2002.
[4] Bas J. De Kruif and Theo J. A. De Vries. Pruning error minimization
in least squares support vector machines. IEEE Transactions on Neural
Networks, 14(3):696–702, 2003.
[5] Yaakov Engel, Shie Mannor, and Ron Meir. The kernel recursive least
squares algorithm. IEEE Transactions on Signal Processing, 52(8):2275–
2285, August 2004.
[6] Jyrki Kivinen, Alexander J. Smola, and Robert C. Williamson.
On-
line learning with kernels.
IEEE Transactions on Signal Processing,
52(8):2165–2176, August 2004.
[7] Weifeng Liu, José C. Príncipe, and Simon Haykin. Kernel Adaptive Fil-
tering: A Comprehensive Introduction. Wiley, 2010.
[8] Wemerson D. Parreira, Jose Carlos M. Bermudez, Cédric Richard, and

500
Regularization, Optimization, Kernels, and Support Vector Machines
Jean-Yves Tourneret. Stochastic behavior analysis of the Gaussian kernel
least-mean-square algorithm. IEEE Transactions on Signal Processing,
60(5):2208–2222, 2012.
[9] Fernando Pérez-Cruz, Steven Van Vaerenbergh, Juan José Murillo-
Fuentes, Miguel Lázaro-Gredilla, and Ignacio Santamaría. Gaussian pro-
cesses for nonlinear signal processing: An overview of recent advances.
IEEE Signal Processing Magazine, 30:40–50, July 2013.
[10] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Pro-
cesses for Machine Learning. MIT Press, 2006.
[11] Cédric Richard, José Carlos M. Bermudez, and Paul Honeine. Online
prediction of time series data with kernels. IEEE Transactions on Signal
Processing, 57(3):1058–1067, March 2009.
[12] Craig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge re-
gression learning algorithm in dual variables. In Proceedings of the 15th
International Conference on Machine Learning (ICML), pages 515–521,
Madison, WI, USA, July 1998.
[13] Ali H. Sayed. Fundamentals of Adaptive Filtering. Wiley-IEEE Press,
2003.
[14] Bernhard Schölkopf, Ralf Herbrich, and Alexander J. Smola.
A gen-
eralized representer theorem. In Computational learning theory, pages
416–426. Springer, 2001.
[15] Bernhard Schölkopf and Alexander J. Smola. Learning with Kernels. The
MIT Press, Cambridge, MA, USA, 2002.
[16] Johan A.K. Suykens, Jos De Brabanter, Lukas Lukas, and Joos Vande-
walle. Weighted least squares support vector machines: robustness and
sparse approximation. Neurocomputing, 48(1):85–105, 2002.
[17] Sergios Theodoridis, Konstantinos Slavakis, and Isao Yamada. Adaptive
learning in a world of projections. IEEE Signal Processing Magazine,
28(1):97 –123, January 2011.
[18] Steven Van Vaerenbergh, Miguel Lázaro-Gredilla, and Ignacio Santa-
maría.
Kernel recursive least-squares tracker for time-varying regres-
sion.
IEEE Transactions on Neural Networks and Learning Systems,
23(8):1313–1326, August 2012.
[19] Steven Van Vaerenbergh and Ignacio Santamaría. A comparative study
of kernel adaptive ﬁltering algorithms. In 2013 IEEE Digital Signal Pro-
cessing (DSP) Workshop and IEEE Signal Processing Education (SPE),
2013.

Online Regression with Kernels
501
[20] Steven Van Vaerenbergh, Ignacio Santamaría, and Miguel Lázaro-
Gredilla.
Estimation of the forgetting factor in kernel recursive least
squares. In 2012 IEEE International Workshop on Machine Learning for
Signal Processing (MLSP), September 2012.
[21] Steven Van Vaerenbergh, Ignacio Santamaría, Weifeng Liu, and José C.
Príncipe.
Fixed-budget kernel recursive least-squares.
In 2010 IEEE
Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Dallas,
USA, April 2010.
[22] Steven Van Vaerenbergh, Javier Vía, and Ignacio Santamaría. A sliding-
window kernel RLS algorithm and its application to nonlinear channel
identiﬁcation. In 2006 IEEE Int. Conf. on Acoustics, Speech, and Signal
Processing (ICASSP), pages 789–792, Toulouse, France, May 2006.
[23] Masahiro Yukawa. Multikernel adaptive ﬁltering. IEEE Transactions on
Signal Processing, 60(9):4672–4682, 2012.

This page intentionally left blank
This page intentionally left blank


