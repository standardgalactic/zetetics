ALANN: An event driven control mechanism for a non-
axiomatic reasoning system (NARS) 
Tony Lofthouse1 
1 Reasoning Systems Ltd, Andover, UK 
Tony_lofthouse@btinternet.com 
Abstract. Adaptive Logic and Neural network (ALANN): A neuro-symbolic ap-
proach to, event driven, attentional control of a NARS system. A spiking neural 
network (SNN) model is used as the control mechanism in conjunction with the 
Non-Axiomatic Logic (NAL). An inference engine is used to create and adjust 
links and associated link strengths and provide activation spreading under con-
textual control. 
Keywords: Non-axiomatic logic, NARS, control, attention, neuro-symbolic 
1 
Introduction 
The field of AGI research has often taken inspiration from the human brain, in par-
ticular, neural networks. The most plausible biological approach, it could be argued, is 
the spiking neural network (SNN) [5]. Despite the benefits of low energy requirements 
and implicit timing [1], SNNs have failed to match the achievements of the more typical 
artificial neural networks (ANN). This is, in part, due to a lack of an effective training 
regime for SNNs. Whilst ANNs have back propagation and gradient descent, there is 
no equally effective method for SNNs. The most fruitful area of research has been in 
Spike Timing Dependent Plasticity (STDP) [4]. There has been some recent progress 
in this area [10], but this does not yet match the results of back propagation and ANNs 
in general [10]. This paper presents an approach to combine the benefits of SNNs with 
a Non-Axiomatic Logic (NAL) [8] and a Non-Axiomatic Reasoning System philoso-
phy (NARS). We will show that a symbolic logic (NAL) can assume the role of ‚Äòtrain-
ing‚Äô a SNN via an inference mechanism and form the framework of an attentional con-
trol mechanism for a reasoning system whilst operating under the Assumption of Insuf-
ficient Knowledge and Resources (AIKR). 
1.1 
Non-Axiomatic Reasoning System (NARS) 
NARS was created as a concrete implementation of an adaptive reasoning system but 
now with multiple systems using the underlying principles it has become a methodol-
ogy and underlying philosophy for developing AGI systems. ALANN is a concrete 
implementation of a NARS and adheres to the underlying requirements of; a unified 
principle of cognition which adapts to its environment, Finite ‚Äì works with insufficient 

2 
knowledge and resource (fixed), Real-time: responds in real time to new information 
and is Open to input from any domain (expressible in the input language).  
NAL is a syllogistic logic and generally requires two premises to have a common 
term in order to apply an inference rule (there are exceptions to this such as temporal 
or structural inference) [8]. In principle a premise in NAL is a <term copula term> 
where term is a symbol or a premise and copula is a link, with a truth value, between 
terms. For the purpose of this discussion we will limit the range of NAL copulas and 
operators, but in NAL the number of copulas and operators is much larger [8]. In this 
paper we will assume a term is a symbol composed of letters and digits and a copula is 
an inheritance relation ‚Üí [8] 
Truth values in NAL are evidence based (w+, w-), where w+ represents positive evi-
dence and w- represents negative evidence, or alternatively as confidence c and fre-
quency f tuple, where ùëì=
w+
w++w‚àí and ùëê=
ùë§++ùë§‚àí
ùëò+ùë§++ùë§‚àí, where k is a global personality 
parameter that indicates a global evidential horizon [8]. 
Inference under NAL is carried out by applying inference rules that, typically, match 
a pair of premises with a common term. For example, the deduction rule would be ap-
plied to the following premises: <a ‚Üí b>, <b ‚Üí c> |- <a ‚Üí c> <ded>, where the 
premise <a ‚Üí c> is derived from the two premises, which share a common term, b, 
where <ded> is a truth function that takes two truth values and returns a new derived 
truth value; ded : TV * TV -> TV. NAL has a range of truth functions to cover the 
semantically meaningful premise patterns, such as deduction, induction, abduction, and 
exemplification. There are additional truth functions that we will not cover here, for 
further details see [8]. 
Furthermore, two premises can be revised when they have matching content (assum-
ing no overlapping evidence) using the revision truth function [8]. In this way evidence 
is accumulated over time. 
NARS is a generative model and when provided with experience in the form of Nar-
sese beliefs (premises), will generate derived beliefs. These derived beliefs form a be-
lief network, interconnected by copulas. This network can be conceptualized as a node 
link graph.  
Given that a NARS system must work with ‚ÄòFinite‚Äô resource and knowledge it is 
necessary to limit the number of beliefs that the system retains at any one time. Addi-
tionally, only certain beliefs are relevant at a specific moment in time. The primary role 
of the control mechanism is to identify the contextually relevant subset of the belief 
network and remove the least useful beliefs when resource limits are reached. The con-
trol mechanism needs to respond in real-time. 
1.2 
Spiking Neural Networks 
Spiking neural networks (SNN) are artificial neural networks that attempt to replicate 
the properties of biological neurons. A network is composed of nodes and links where 
nodes accumulate ‚Äòattention‚Äô and fire when a threshold is reached; links propagate the 
action potential through the network. Links have a weight which modifies the action 
potential. The network ‚Äòlearns‚Äô by adjusting the link weights (typically using Spike 

3 
Timing Dependent Plasticity - STDP). Unlike ANNs, nodes do not necessarily fire each 
cycle but rather when sufficient activation has accumulated. Additionally, timing is an 
inherent property of SNNs. 
The neuron model employed is effectively, accumulate action potential, which decays 
over time. If an activation threshold is exceeded, the neuron ‚Äòfires‚Äô propagating action 
potential to its ‚Äòsynapses‚Äô, with spikes, to connected nodes. A neuron is reset with its 
resting potential after firing and has a refractory period during which it cannot ‚Äòfire‚Äô. 
Action potential is transported though the system by spikes. Spike strength is modulated 
by the synaptic strength. In this paper, we will assume the leaky integrator approach [4] 
to the neural model. although there are other approaches [4]. 
The SNN property of activation spreading, via contextual relevance, is precisely 
what is required for an AGI control mechanism. 
2 
Approach 
The systems experience is defined as a stream of events E, which occur at a specific 
time and each event is weighted with an attention value (AV), semantics and some 
additional meta data (STAMP). An attention value is defined as [STI LTI] where STI 
is short term importance in the range (0, 1] and LTI is long term importance in the range 
(0 1). The attached semantics refer to the Narsese Term (premise) that the event repre-
sents, (for example <cat ‚Üí animal>), as explained in the NARS overview. The seman-
tic component may also contain a truth value (TV) containing (f, c). 
The comparison to spiking neural networks is necessary, but not sufficient, to ex-
plain the way attention works in ALANN. In a traditional spiking network, a spike has 
strength and timing. In ALANN, a spike is also loaded with semantics, effectively smart 
spikes (similar to neuro-symbols) [7]. There are three primary components to the 
ALANN network model; nodes, links and events. 
In ALANN, nodes have the properties of; leaky integrator approach to accumulate 
and lose action potential, activation threshold, action potential, resting potential and 
refractory period. Links between nodes have weight and semantics. Events have tem-
poral significance, attention level and semantics. 
Currently, ALANN belief-links are adjusted via an inference process (although 
STDP will be used to adjust the weights of the planned predictive-links - see future 
research). Evidence gathering is the basis of link weight adjustment in ALANN, where 
the revision truth function is used to accumulate evidence. 
 
rev: w+ = w1+ + w2+ 
 
w- = w1- + w2- 
 
Link strength is defined as the expectation value of the related belief truth, where 
expectation value is defined as:  
 
exp(f, c) = c(f - 0.5) + 0.5. 
 

4 
The node attention decay function is defined as: 
e‚àíŒª‚àá 
Where ùû¥ is a scaling factor (typically 0.1) and determines the rate of decay and ùû© is 
the temporal distance between events (in milliseconds). 
Node attention is accumulated as follows:  
 
attentionnode‚Äô = _or(attentionnode, eventsti) 
where: 
 
_or(x, y) = 1.0f - (1.0f - x) * (1.0f - y) 
Belief-links, which exist between nodes, always have a related TV. 
An events attention value is attenuated by belief-link strength (truth expectation of 
the related semantic term). The reduced event attention is then used to modify the node 
action potential. Events that have matching semantic content are revised with the 
matching belief-link (assuming no overlapping evidence). Events that do not have a 
matching belief-link generate a new link between the source node and the event desti-
nation node. Event destinations are determined by sending the event to each of the sub-
terms contained within the semantic component. 
At a high level the system can be considered as a feedback loop with input from 
external experience and inference results feeding back into the loop ‚Äì effectively a cy-
clic system. One of the challenges with a cyclic system, where the system cycle contains 
a feedback loop with a gain function (from inference), is avoiding combinatorial explo-
sion. Two aspects of the design enable the system to function without the expected 
combinatorial explosion from the feedback loop. Firstly, the node refractory period 
constrains the rate at which any one node can be cycled and contribute to the inference 
cycle, and secondly, an attention buffer is inserted into the event flow. The attention 
buffer is essentially a fixed length priority queue ordered by STI. In practice, the system 
is divided into multiple streams of activity with each stream having its own attention 
buffer which can be considered a form of winner-takes-all. The attention buffer also 
acts as a form of inhibition, where events are effectively competing for attention and 
the higher attention values inhibit the lower. Interestingly, the node latency period alone 
was able to eliminate the combinatorial explosion from the inference cycle, but the 
system performed better overall with the introduction of the attention buffer. Addition-
ally, the node latency period inhibits short term cyclic inference where a node is repeat-
edly activated by its own output. 
The system can be provided with input tasks such as ‚Äòfind the answer to a question‚Äô. 
Given that knowledge is represented as a belief network, finding an answer to a ques-
tion is a form of graph search. Tasks are potentially satisfied, by both forward and 
backward inference. Forward inference generates new knowledge and backward infer-
ence generates derived questions (tasks) that may help to satisfy the input task. Finding 
the balance between exploit and explore is critical to the effectiveness of the system. 
The approach taken is to partition search into shallow and deep search, where shallow 
determines the degree of exploration and deep the degree of exploitation. Essentially, 

5 
when a derived task is satisfied (to a degree), it is exploited via deep search otherwise 
shallow search is the default. An important consideration is that an AGI system must 
be both finite and real-time therefore graph search, whether deep or shallow, is incre-
mental as it must remain within the resource constraints provided and respond to new 
input at any time. To enforce this requirement the event attention (LTI) is used to spec-
ify deep or shallow search. Increasing the events attention value results in deeper search 
and reducing leads to shallow search. 
Tasks, in ALANN, are represented by events (with a task type attribute), an impli-
cation of this is that tasks are not stored in the belief network but are transient. Tasks 
cycle around the system whilst they retain enough attention to compete with the other 
current events. Once the attention value drops too low the task event is forgotten. Due 
to this property, there is no guarantee that a specific task will be satisfied. It is dependent 
on the relative attention of the system at a specific moment. The attention of an event 
can increase or decrease based on the contextual relevance of the related sub terms 
within the semantic component.  
3 
Comparison 
Three approaches are shown for comparison purposes: OpenCog, Neuro-symbolic prin-
ciple and OpenNARS. 
OpenCog initially used an activation spreading approach but then moved to ECAN. [2]. 
The approach used in ALANN is conceptually similar to the early activation spreading 
utilized by OpenCog. A key difference is that the semantics of the link types are man-
aged differently: OpenCog explicitly defines link types between nodes whereas 
ALANN links contain link types within the semantics of the contained term. A further 
difference is that within OpenCog activation spreading was a separate process whereas 
in ALANN it is a step wise incremental unified process. 
Neuro-symbolic principle [7]: Whilst there are some similarities: event driven and 
neuro-symbols, a key difference is the use of a logic with a formal grammar that defines 
the semantics and subsequently ‚Äòmeaning‚Äô of both premises and the connecting belief-
links. Furthermore, using a logic, NAL, as a basis for training the network weights, is 
a considerably different approach. A further difference is that no initial network con-
figuration is required with the ALANN approach. It can boot-strap directly from expe-
rience and the generative NARS/NAL principles [9]. Finally, learning is unsupervised 
and does not require training and target data. 
OpenNARS: link and bag approach with probabilistic selection mechanism. [3]. One 
of the significant differences between ALANN and OpenNARS is that ALANN is fully 
event driven. By this we mean that unlike in OpenNARS where concepts, task, beliefs 
and links are deterministically selected from ‚Äòbags‚Äô, ALANN has no selection mecha-
nism. Nodes and links in ALANN are activated purely by the flow of events within the 
system. The event flow is a form of activation spreading but rather than a cascade of 
activation throughout the system, activation spreading occurs one step at a time in con-
junction with inference and the overall system cycle.  

6 
4 
Discussion 
Many, if not all, of the challenges of using ANN and DL for AGI [6] can be overcome 
with a neuro-symbolic approach and an inference engine. Transparency comes for free 
with a symbolic approach. The use of a reasoning engine to generate a network and its 
related link weights allows one shot learning and boot strapping, with no initial training 
period, directly from the systems experience. Given that NAL is an evidence-based 
logic, beliefs are derived from countable evidence and its conclusions can be fully au-
dited by a competent human. NARS‚Äô are built upon a definition of intelligence that 
requires adaption to an environment under AIKR, therefore adaption to change is an 
intrinsic property of such a system. The generated belief network responds in real time 
to changes in the environment by adjusting link weights and forgetting less relevant 
nodes and links via a reasoning process.  
Grammar and semantics constrain both the extent and search direction of the gener-
ated space (belief network). There are a limited, but still very large, number of mean-
ingful combinations of premises. In this way the belief network is ‚Äògrown‚Äô into the 
possible search space by following the semantically meaningful pathways, under atten-
tional control. The degree to which a search space can be explored is a factor of the 
environmental complexity and available resources. Any moderately complex environ-
ment will have a search space larger than can be explored in full. In NARS, attention 
and experience is the guide to explore and exploit the belief network. NARS is a so-
called generative model as it builds its belief network in real time from experience. 
The symbolic nature of knowledge representation allows for existing prior 
knowledge to be incorporated into the belief network with no training required. Finally, 
using the framework of a reasoning engine for inference, means that the system can use 
a single principle to support both open ended inference and link weight training. 
5 
Future research 
The current scope of research on the ALANN control model is based on NAL 1-6. 
These NAL ‚Äòlayers‚Äô are related to declarative knowledge and do not include temporal 
or procedural components. 
The next steps in the research plan are three-fold: 
‚Ä¢ Incorporate ‚Äòpredictive-links (for temporal beliefs) with a variant of STDP to adjust 
link weights. Predictive-links gain an additional property: link delay (similar to syn-
aptic delay). 
‚Ä¢ Remove the need for separate truth values, by rationalizing eternal and event truth 
into a single principle. 
‚Ä¢ Add goal nodes to the belief network with supporting satisfying-links 
In OpenNARS there is an anticipation process which is used to confirm or deny the 
correctness of predictive beliefs. The use of STDP to adjust link weights has the addi-
tional advantage that in can remove the need for the anticipation process. STDP is a 
form of evidence gathering in a similar way to the revision truth function. The addition 

7 
of link delay enables precise timing in contextual mapping. OpenNARS also maintains 
multiple versions of truth for each node (concept), in particular, the need to separate 
eternal truth and event truth. By making nodes stateless from the perspective of truth 
value, the idea is to contextually generate truth value based on the link structure. Fi-
nally, adding goal nodes, which can be considered symmetrical to belief nodes, along 
with satisfying -links to support procedural reasoning. 
Once the above steps are completed, implement the remaining NAL levels (7/8/9) to 
fully support episodic and procedural beliefs and events. 
6 
Conclusion 
Using a symbolic reasoning engine as the framework for an AGI serves the dual pur-
pose of providing an open-ended inference capability and a unified approach to adapt-
ing link weights in an unsupervised spiking neural network. The event driven design 
coupled with the SNN properties are shown to be an effective alternative to the more 
typical ‚ÄòBag‚Äô based approach to memory model and control.  
A software implementation of the principles presented in this paper is available at 
the OpenNARS GitHub repository:  https://github.com/opennars/ALANN2018. 
References 
1. Ammar A., et al: A comparative study on spiking neural network encoding schema Cluster 
Computing (2019).  
2. Ikle M., et al: Economic Attention Networks: Associative Memory and Resource Allocation 
for General Intelligence. Proceedings of the Second Conference on Artificial General Intel-
ligence, Atlantis Press. (2009) 
3. Hammer P., Lofthouse T., Wang P.: The OpenNARS Implementation of the Non-Axiomatic 
Reasoning System, Springer, Proceedings International Conference on Artificial General 
Intelligence (2016) 
4. Izhekevich E., Dynamical System in Neuroscience, MIT Press (2007) 
5. Maass, W., Networks of spiking neurons: The third generation of neural network models". 
Neural Networks. 10 (9): 1659‚Äì1671 (1997) 
6. Marcus G.: Deep Learning: A Critical Appraisal, arXiv:801.00631 (2018) 
7. Velik R., Bruckner D: Neuro-Symbolic Networks: Introduction to a New Information Pro-
cessing Principle, Proceedings of the IEEE international conference on industrial Informat-
ics (2008)  
8. Wang P.: Non-Axiomatic Logic: A Model of Intelligent Reasoning, World Scientific, Singa-
pore (2013) 
9. Wang P.: Rigid Flexibility - The Logic of Intelligence, Springer (2006) 
10. Yujie W., et al, Direct Training for Spiking Neural Networks: Faster, Larger, Better, 
arXiv:1809.05793v2 (2018) 
 

