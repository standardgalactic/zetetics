ARTICLE
Deep learning enhanced Rydberg multifrequency
microwave recognition
Zong-Kai Liu
1,2, Li-Hua Zhang1,2, Bang Liu1,2, Zheng-Yuan Zhang1,2, Guang-Can Guo1,2,
Dong-Sheng Ding
1,2✉& Bao-Sen Shi
1,2✉
Recognition of multifrequency microwave (MW) electric ﬁelds is challenging because of the
complex interference of multifrequency ﬁelds in practical applications. Rydberg atom-based
measurements for multifrequency MW electric ﬁelds is promising in MW radar and MW
communications. However, Rydberg atoms are sensitive not only to the MW signal but also
to noise from atomic collisions and the environment, meaning that solution of the governing
Lindblad master equation of light-atom interactions is complicated by the inclusion of noise
and high-order terms. Here, we solve these problems by combining Rydberg atoms with deep
learning model, demonstrating that this model uses the sensitivity of the Rydberg atoms
while also reducing the impact of noise without solving the master equation. As a proof-of-
principle demonstration, the deep learning enhanced Rydberg receiver allows direct decoding
of the frequency-division multiplexed signal. This type of sensing technology is expected to
beneﬁt Rydberg-based MW ﬁelds sensing and communication.
https://doi.org/10.1038/s41467-022-29686-7
OPEN
1 Key Laboratory of Quantum Information, University of Science and Technology of China, Hefei, Anhui 230026, China. 2 Synergetic Innovation Center of
Quantum Information and Quantum Physics, University of Science and Technology of China, Hefei, Anhui 230026, China. ✉email: dds@ustc.edu.cn;
drshi@ustc.edu.cn
NATURE COMMUNICATIONS |   (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications
1
1234567890():,;

T
he strong interaction between Rydberg atoms and micro-
wave (MW) ﬁelds that results from their high polarizability
means that the Rydberg atom is a candidate medium for
MW ﬁelds measurement, e.g., using electromagnetically induced
absorption1, electromagnetically induced transparency (EIT)2,3
and the Autler–Townes effect3–6. The amplitudes7–10, phases10,11
and frequencies9,10 of MW ﬁelds could then be measured with
high sensitivity. Based on this measurement sensitivity for MW
ﬁelds, the Rydberg atom has been used in communications7,8,12,13
and radar14 as an atom-based radio receiver. In the commu-
nications ﬁeld, the Rydberg atom replaces the traditional antenna
with superior performance aspects that include sub-wavelength
size, high sensitivity, system international (SI) traceability to
Planck’s constant, high dynamic range, self-calibration and an
operating
range
that
spans
from
MHz
to
THz
frequencies7,9,10,15,16. One application is analogue communica-
tions, e.g., real-time recording and reconstruction of audio
signals13. Another application is digital communications, e.g.,
phase-shift keying and quadrature amplitude modulation7,8,12.
The channel capacity of MW-based communications is limited by
the standard quantum limited phase uncertainty7. Furthermore, a
continuously tunable radio-frequency carrier has been realized
based on Rydberg atoms17, thus paving the way for concurrent
multichannel communications. Detection and decoding of mul-
tifrequency MW ﬁelds are highly important in communications
for acceleration of information transmission and improved
bandwidth
efﬁciency.
Additionally,
MW
ﬁelds
recognition
enables simultaneous detection of multiple targets with different
velocities from the multifrequency spectrum induced by the
Doppler effect. However, because of the sensitivity of Rydberg
atoms, the noise is superimposed on the message, meaning that
the message cannot be recovered efﬁciently. Additionally, it is
difﬁcult to generalize and scale the band-pass ﬁlters to enable
demultiplexing of multifrequency signals with more carriers16.
To solve these problems, we use a deep learning model for its
accurate signal prediction capability and its outstanding ability to
recognize complex information from noisy data without use of
complex circuits. The deep learning model updates the weights
via backpropagation and then extracts features from massive data
without human intervention or prior knowledge of physics and
the experimental system. Because of these advantages, physicists
have constructed complex neural networks to complete numerous
tasks, including far-ﬁeld subwavelength acoustic imaging18, value
estimation
of
a
stochastic
magnetic
ﬁeld19,
vortex
light
recognition20,21, demultiplexing of an orbital angular momentum
beam22,23 and automatic control of experiments24–29.
Here, we demonstrate a deep learning enhanced Rydberg
receiver for frequency-division multiplexed digital communica-
tion. In our experiment, the Rydberg atoms act as a sensitive
antenna and a mixer to receive multifrequency MW signals and
extract information9,11,12. The modulated signal frequency is
reduced from several gigahertz to several kilohertz via the inter-
action between the Rydberg atoms and the MWs, thus allowing
the information to be extracted using simple apparatus. These
interference signals are then fed into a well-trained deep learning
model to retrieve the messages. The deep learning model extracts
the multifrequency MW signal phases, even without knowing
anything about the Lindblad master equation, which describes the
interactions between atoms and light beams in an open system
theoretically. The solution of the master equation is often com-
plex because the higher-order terms and the noises from the
environment and from among the atoms are taken into con-
sideration. However, the deep learning model is robust to the
noise because of its generalization ability, which takes advantage
of the sensitivity of the Rydberg atoms while also reducing the
impact of the noise that results from this sensitivity. Our deep
learning model is scalable, allowing it to recognize the informa-
tion carried by more than 20 MW bins. Additionally, when the
training is complete, the deep learning model extracts the phases
more rapidly than via direct solution of the master equation.
Results
Setup. We adapt a two-photon Rydberg-EIT scheme to excite
atoms from a ground state to a Rydberg state. A probe ﬁeld drives
the atomic transition j5S1=2; F ¼ 2i ! j5P1=2; F0 ¼ 3i and a
coupling light couples the transition j5P1=2; F0 ¼ 3i ! j51D3=2i
in rubidium 85, as shown in Fig. 1a. Multifrequency MW ﬁelds
drive a radio-frequency (RF) transition between the two different
Rydberg states j51D3=2i and j50F5=2i. The energy difference
between these states is 17.62 GHz. The multifrequency MW ﬁelds
consist of multiple MW bins (more than three bins) with fre-
quency differences of several kilohertz from the resonance fre-
quency. The amplitudes, frequencies, and phases of the multiple
MW bins can be adjusted individually (further details are pro-
vided in the “Methods” section). The detunings of the probe,
coupling and MW ﬁelds are Δp, Δc and Δs, respectively. The Rabi
frequencies of the probe, coupling and MW ﬁelds are Ωp, Ωc and
Ωs, respectively. The experimental setup is depicted in Fig. 1b. We
use MW ﬁelds to drive the Rydberg states constantly, producing
modulated EIT spectra, i.e., the probe transmission spectra, as
shown in the inset of Fig. 1b. The phases of the MW ﬁelds cor-
relate with the modulated EIT spectra and can be recovered from
these spectra with the aid of deep learning. Speciﬁcally, the probe
transmission spectra are fed into a well-trained deep learning
model that consists of a one-dimensional convolution layer (1D
CNN), a bi-directional long–short-term memory layer (Bi-LSTM)
and a dense layer to extract the phases of the MW ﬁelds.
Figure 1c–e shows these components of the neural network
(further details are presented in the “Methods” section). Finally,
the bin phases are recovered and the data are read out.
Frequency-division multiplexed signal encoding and receiving.
In the experiments, we use a four-bin frequency-division multi-
plexing (FDM) MW signal for demonstration, where one of the
four MW bins is used as the reference bin. The relative phase
differences between the reference bin and the other bins are
modulated by the message signal. Speciﬁcally, for the four-bin
MW signal,
E ¼ A1 cos½ðω0 þ ω1Þt þ φ1 þ A2 cos½ðω0 þ ω2Þt þ φ2
þ A3 cos½ðω0 þ ω3Þt þ φ3 þ A4 cos½ðω0 þ ω4Þt þ φ4;
where ω0 is the resonant frequency, ω1,2,3 are the relative fre-
quencies, the carrier frequencies are 2π(ω0 + ω1) = 17.62 GHz −
3 kHz,
2π(ω0 + ω2)
=
17.62 GHz −1 kHz,
2π(ω0 + ω3) =
17.62 GHz + 1 kHz and 2π(ω0 + ω4) = 17.62 GHz + 3 kHz, the
frequency difference between two frequency-adjacent bins is
Δf = 2 kHz and the message signal is φ1,2,3 = 0 or π, standing for 3
bits (0 or 1), and the reference phase is φ4 = 0 (which remains
unchanged). The phase list φ1; φ2; φ3; φ4


is a bit string for time
t0. By varying the phase of φ1,2,3 with time, we then obtain the
FDM signal for binary phase-shift keying (2PSK). Additionally,
the amplitudes of the four bins are 0.1A 4 = A1,2,3 to solve the
problem that results from the nonlinearity of the atom, where the
probe transmission spectra of two different bit strings, e.g.
(0, 0, π, 0) and (0, π, 0, 0), are the same (further details are pre-
sented in the “Methods” section). By increasing the frequency
difference Δf, we can obtain higher information transmission
rates. For four bins with Δf = 2 kHz, the information transmission
rate is nb × Δf = (4 −1) × 2 × 103 bps = 6 kbps, where nb is the
number of bits. In the experiments, disturbances originate from
ARTICLE
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
2
NATURE COMMUNICATIONS |  (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications

the environment and atomic collisions. Because of the sensitivity
of Rydberg atoms to MW ﬁelds, the resulting noise submerges our
signal. To use the sensitivity of the Rydberg atoms and simulta-
neously minimize the effects of noise, the deep learning model is
used to extract the relative phases φ1; φ2; φ3


.
Deep learning. To improve the robustness and speed of our
receiver, we use a deep learning model to decode the probe
transmission signal. The complete encoding and decoding process
is illustrated in Fig. 2a. The Rydberg antenna receives the FDM-
2PSK signal and down-converts this signal into the probe
transmission spectrum. The information is then retrieved from
the spectrum using the deep learning model. The precondition is
that the different bit strings correspond to distinct probe spectra;
this is resolved by setting 0.1A4 = A1,2,3, as discussed earlier.
Then, we combine the 1D CNN layer, the Bi-LSTM layer and the
dense layer to form the deep learning model (see the “Methods”
section for further details)30,31. One of the reasons for using the
1D CNN layer and the Bi-LSTM layer is that the data sequences
are long, which means that prediction of the phases φ ¼
φ1; φ2; φ3; 0


from the spectrum is a regression task and
requires a long-term memory for our model. Another reason is to
combine the convolution layer’s speed with the sequential sen-
sitivity of the Bi-LSTM layer32. The input sequence is ﬁrst
processed by the 1D CNN to extract the features, meaning that a
long sequence is converted into a shorter sequence with higher-
order features. This process is visualized to show how the deep
learning model treats the transmission spectrum; more details are
presented in the Supplementary Materials. The shorter sequence
is then fed into the Bi-LSTM layer and resized by the dense
layer to match the label size (see the “Methods” section for
further
details).
Speciﬁcally,
the
probe
spectrum
T ¼
T0; Tτ; T2τ;    ; Tiτ    ; TNτ


and the corresponding phases
φ ¼ φ1; φ2; φ3; φ4 ¼ 0


are collected to form the data set,
where Ti⋅τ is the ith data point of a probe spectrum and the fourth
bit φ4 = 0 is the reference bit. Both the spectra and the phases are
1D vectors with dimensions of N + 1 and 4, respectively. These
independent, identically distributed data {{T}, {φ}} are fed into
our model as a data set. By shufﬂing this data set and splitting it
into three sets, i.e., a training set, a validation set and a test set, we
train our model on the training set (feeding both the waveforms
and labels {{T}, {φ}}), validate, and test our model on the vali-
dation and test sets, respectively (by feeding waveforms without
labels and comparing the predictions with ground truth labels).
The validation set is used to determine whether there is either
overﬁtting or underﬁtting during training. Finally, the perfor-
mance (i.e., accuracy) of the model is estimated by predicting the
test set.
The performance of our deep learning model is affected by the
training epochs and the training and validation set sizes. The
training curves on different training sets and validation sets are
shown in Fig. 2b, c. Initially, our model performs well on the
training set only, implying overﬁtting. The curves then converge
(dashed line) and our model performs well on both the training
set and the validation set. The sudden jump in the loss curve in
Fig. 2c is caused by the change in the learning rate (further details
are presented in the “Methods” section). Use of more training and
validation data causes the curves to converge more quickly. The
deep learning model performs well after these few-sample
training. In Fig. 2d, we show a confusion matrix for prediction
of a uniformly distributed test set, which demonstrates accuracy
of 99.38%.
The “noise” shown in Fig. 2a refers to two kinds of noises. One
comes from atoms and the external environment (systematic
noise). The other comes from the noise added on purpose
(additional noise). The systematic noise cannot be adjusted
quantitatively and is discussed with its noise spectrum in
the Supplementary Materials. Because the noise on the data set
is independent and is distributed identically (i.i.d.), i.e., the entire
data set is shufﬂed before being split into the training and test
sets, the systematic noise pattern is almost the same in both the
training set and the test set. The deep learning model has already
Reference
Probe
Coupling
Horn
x
y
z
51D3/2
5P1/2
5S1/2
Probe laser
     ~795 nm
Coupling laser
       ~474 nm
Δp
Ωp
Ωc   
(a)
(b)
(c)
(d)
MW electric field
        ~ 17.62 GHz
0   1   0...
0   0   1...
1   0   1...
0   0   0...
time
ti ti+1ti+2
Ref.
Probe transmission
Δs
Δc   
Ωs
Neural network
50F5/2
ω1+ω2+ω3+...
Input data
Output
Kernal
Convolution
(e)
Fig. 1 Illustration of the setup. a Overview of experimental energy diagram. Probe and coupling laser beams excite the atoms at ground state j5S1=2i to the
Rydberg state j51D3=2i. Multifrequency microwave (MW) electric ﬁelds couple the Rydberg states j51D3=2i and j50F5=2i. b Schematic of Rydberg atom-
based antenna and mixer interacting with multifrequency signals. A 795 nm laser beam is split into two beams, which then propagate in parallel through a
heated Rb cell (length: 10 cm, temperature: 44.6 ∘C, atomic density: 9.0 × 1010 cm−3)46. One is the probe beam, which counterpropagates with the
coupling laser beam exciting atoms to Rydberg states to reduce Doppler broadening. The other is the reference beam, which does not counterpropagate
with the coupling laser beam. The beams are detected using a differencing photodetector (DD) to obtain the probe transmission spectrum (inset).
Multifrequency MW ﬁelds transmitted by a horn are applied to the atoms, with a radiated direction that is perpendicular to the laser beam propagation
direction. The multifrequency MW ﬁelds are modulated using a phase signal such that the phase differences between the reference bin and the other bins
carry the messages. The probe transmission spectrum is fed into a well-trained neural network to retrieve the variations of the phases with time.
c–e Schematics of the neural network. The network consists of c a one-dimensional convolution layer, d a bi-directional long–short-term memory layer and
e a dense layer; for further details about these layers, see the “Methods” section.
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
ARTICLE
NATURE COMMUNICATIONS |   (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications
3

learned the systematic noise pattern during the training process,
which is one of the major advantages of use of deep learning
against systematic noise. However, there is a case where the noise
is not i.i.d. (i.e., the case where a speciﬁc noise occurs during
testing only). This problem can be solved by online learning and
addition of prior knowledge as new features into the data, e.g.,
data for the temperature, the weather, and other factors33. Here
for simplicity, we talk about the i.i.d. case only and add the white
noise with a mean μ and a standard deviation σ. We ignore the 1/f
noise in this case because it decays rapidly in the low frequency
range and the signal with which it would interfere is located
within the 2–200 kHz range. The additional noise is added both
on the training set and the test set of the deep learning model in
Fig. 2e, which demonstrates the performance of the deep learning
model when used on a data set with biased or unbalanced noise.
The results below the red line show the performance of the model
after training on a weaker-noise training set when predicting
based on a stronger-noise test set, i.e., generalization for a
stronger noise case. These results indicate that the deep learning
model has the generalization ability required to adapt to stronger
noise. In the area above the red line, there is more noise in the
training set than in the test set. Theoretically, a small amount of
additional noise in the training set will increase the robustness of
the deep learning model. However, when the noise increases, it
affects the accuracy, which decays rapidly. Next, the well-trained
model is used to reconstruct the QR code. In Fig. 3a–c, the results
and the corresponding confusion matrices with their epochs are
shown. First, the information is encoded into a QR code. After
the code is transmitted, received and decoded using the Rydberg
atoms and the deep learning model, the information is then
reconstructed successfully using the 35-epoch training model in
Fig. 3c, but is not reconstructed in parts (a) and (b). The accuracy
is deﬁned by the number of correctly predicted bit strings divided
by the total number of bit strings (147 bit strings). After 35
epochs, the accuracy reaches 99.32% and the message is
reconstructed successfully from the QR code received.
Comparison between deep learning method and the master
equation. In our case, the master equation that we employed is
the commonly used one without considering the noise spectrum.
The accuracies of the deep learning model and the master
equation ﬁtting on noisy data are different. Figure 4 shows the
accuracies obtained by the two methods. The deep learning model
is trained on a training set without additional noise, and tested on
a test set with additional white noise whose standard deviation is
σ (the transmission spectra with noise are given in Supplementary
Materials). Here for simplicity, the data set is composed of the
transmission of four MW bins only (one of them is reference bin)
and the frequency difference between the adjoin bins is
Δf = 2kHz. On the other hand, the result of the master equation is
given based on the same test set as that of the deep learning
model. The deep learning method outperforms the ﬁtting of the
master equation on the noisy data set.
Apart from the robutness to the noise, when the transmission
rate is increased by increasing the number of MW bins or the
frequency difference Δf, the deep learning model performs well,
while it is difﬁcult to retrieve the messages with high accuracy
using the master equation. Speciﬁcally, to increase the bandwidth
efﬁciency and the transmission rate, the number of MW bins
used to carry the messages must be increased, but the information
is still recognizable because of the scalability of the deep learning
model. For 20 MW bins, the number of bits is (20 −1) with one
reference bit, giving a 20  1
ð
Þ ´ 2 kbps ¼ 38 kbps transmission
rate. The number of combinations of these bits is 219, which
increases exponentially as the number of MW bins increases.
0  1  0...
0  0  1...
1  0  1...
0  0  0...
time
ti ti+1ti+2
Ref.
“USTC”
“USTC”
1D  CNN
Bi-LSTM
Dense
Rydberg atoms
QR encoding
QR decoding
FDM-2PSK coding
Deep learning
Channel
Noise
Predicted labels
Target labels
000 001 010 011 100 101 110 111
0.0
20.0
000
001
010
011
100
101
110
111
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
19
20
20
20
20
20
20
20
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(d)
Accuracy = 99.38 %
Validation
Train
Converge
Epoch = 140   
0
200
25
125
50
150
75
100
175
0.20
0.25
0.05
0.10
0.15
Epoch
Loss
Training set standard deviation
Test set standard deviation
0.0
0.0
0.5
0.5
1.5
1.5
1.0
1.0
2.0
2.0
2.5
2.5
3.0
0.2
1.0
3.0
(a)
(b)
(c)
(e) 
Validation
Converge
Epoch = 30
0.00
0.05
0.10
0.15
0.20
0
10
20
30
40
50
60
70
Train
Loss
Epoch
Fig. 2 Flow chat of recognizing the multifrequency MWs and results. a Process of encoding and decoding frequency-division multiplexed binary phase-
shift keying (FDM-2PSK) signal with Rydberg atoms and the deep learning model. b, c Loss curve evolution with epochs for the training set (blue) and the
validation set (orange) during training with different training data and validation data. The training data sizes are b 393 and c 1194. The validation data sizes
are b 131 and c 398; more details about the data set split are presented in the “Methods” section. The loss curves for training and validation converge at
b 140 and c 30 epochs, where an epoch is a time unit during which the model iterates once over the complete data set; see “Methods” section. d Confusion
matrix for a test set (the number of the testing set is 160 and the labels are uniformly distributed) after training in case (c). The accuracy reaches 99.38%
after a 70-epoch training period. e Deep learning model accuracy on the noisy test set after training on the noisy training set. The x- and y-axes represent
the standard deviations of the additional white noise added to the test set and the training set, respectively. The colorbar represents the accuracy of the
model on the noisy test set. The results were obtained by averaging ﬁve sets of predictions. The diagonal (red line) indicates the accuracy of the model on a
test set in which the noise distribution is the same as that of the training set; more details about the noise are shown in Supplementary Materials.
ARTICLE
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
4
NATURE COMMUNICATIONS |  (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications

Here, for demonstration purposes, only the ﬁrst 3 bits of the total
of 19 bits carry the messages and the other bits, including the
reference, are set to be 0. To show how well our model performs,
we train, validate and test the model on this new data set without
varying the other parameters, with the exception of the training
epochs of our model. The loss curves for training and validation
are shown in Fig. 5a. A confusion matrix for epoch 78 is shown in
Fig. 5b. The model performs well on this new test set, which was
sampled uniformly from eight categories with an accuracy of
100%. Another method that can be used to increase the
information transmission rate involves increasing the frequency
difference. In our case, the frequency difference is increased
from
Δf = 2 kHz
to
Δf = 200 kHz.
The
transmission
rate
increases correspondingly, from (4 −1) × 2 kbps = 6 kbps to
(4 −1) × 200 kbps = 0.6 Mbps. To detect the high-speed signal,
the DD bandwidth is increased, which inevitably leads to
increased noise. After the model is trained on this new data set,
the training and validation loss curves are as shown in Fig. 5c. A
confusion matrix for epoch 83 is shown in Fig. 5d. Increasing the
number of training epochs allows the model to perform well on
this new data set, with an accuracy of 98.83% on a uniformly
sampled test set.
To compare the performances of the deep learning model and
the master equation, we ﬁtted the probe spectra for 20 bins with a
frequency difference Δf = 2 kHz and four bins with a frequency
difference Δf = 200 kHz by solving the master equation without
considering the higher-order terms and the effects of noise. In
each case, 160 probe spectra were ﬁtted that were sampled
uniformly from every category. The prediction results are shown
in Fig. 5(e) and (f). The prediction accuracy of the master
equation is lower than that of the deep learning model. In our
case, the impact of increasing the number of bins is greater than
increasing the DD bandwidth for high-speed signals on the ﬁtting
accuracy. The prediction accuracy for a 20-bin carrier with
frequency difference Δf = 2 kHz is 20.63%, which is like to the
accuracy of guessing, i.e., 1/8. This implies that there is a
disadvantage that comes from the ﬁtting method itself, i.e., it can
easily become trapped by local minima. Some type of prior
knowledge is required to overcome this disadvantage, e.g.,
provision of the initial values of the phases before ﬁtting. In
contrast, the deep learning model is data driven and does not
require any prior knowledge. The local minima problem of deep
learning can be overcome using some well-known techniques,
including learning rate scheduling and design of a more effective
optimizer32. Additionally, the accuracy difference for the 200-
000 001 010 011 100 101 110 111
000
001
010
011
100
101
110
111
0
25
Target labels
0
0
0
0
0
0
0
0
0
6
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
21
10
6
2
17
20
16
20
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
27
0
30
(a)
(b)
(c)
Predicted labels
000 001 010 011 100 101 110 111
000
001
010
011
100
101
110
111
Predicted labels
Target labels
0
0
0
0
0
0
0
0
0
33
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
21
10
8
17
21
16
20
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
30
000 001 010 011 100 101 110 111
000
001
010
011
100
101
110
111
Predicted labels
Target labels
33
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
21
10
8
17
21
16
15
1
4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
USTC
USTC
USTC
USTC
USTC
USTC
Input QR code Output QR code
Confusion matrix
Epoch
4
35
3
51.02%
76.87%
99.32%
Accuracy
Predicted labels
Fig. 3 Reconstruction of the QR code after different epochs of training of the deep learning model. After a 3 epochs, b 4 epochs, and c 35 epochs of
training, the accuracies of our deep learning model are 51.02%, 76.87%, 99.32%, respectively, as determined from the confusion matrices (last column).
The y-axis is the ground truth and the x-axis represents the predictions of the deep learning model. The colour bar and the numbers indicate the counts. If
the prediction is correct, then the corresponding diagonal term in the confusion matrix increases by 1. If incorrect, then the nondiagonal element
increases by 1.
Test set standard deviation
Deep learning
Master equation
Test set accuracy
5
0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
Fig. 4 Prediction accuracy of deep learning model and the master
equation on the noisy test set. The noise is white noise with mean μ = 0
and a standard deviation σ. The noise is added quantitatively using different
σ values. Before and after addition of the noise, the data are scaled between
0 and 1 using their maximum and minimum. The deep learning model is
trained in a training set without additional noise. In addition, we do not
involve noise spectrum when solving the master equation. Each point is
obtained after averaging of ﬁve predictions on the noisy data.
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
ARTICLE
NATURE COMMUNICATIONS |   (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications
5

kHz-difference MW bins between the deep learning model and
the master equation means that the deep learning model is more
robust to noise. Furthermore, the prediction time for the master
equation is 25 s per spectrum, while the time for the deep learning
model is 1.6 ms per spectrum. The master equation is solved by
“FindFit” function in Mathematica 11.1 with both “Accuracy-
Goal” and “PrecisionGoal” default, while the deep learning code
is written in Python 3.7.6. These codes are run on the same
computer with NVIDIA GTX 1650 and Intel®CoreTM i7-9750H.
Another method to decode the signal is available that uses an
in-phase and quadrature (I–Q) demodulator or a lock-in
ampliﬁer7,12. However, the carrier frequency must be given when
decoding the signal in this case. Additionally, for multiple MW
bins, numerous bandpass ﬁlters are required. The deep learning
method is thus much more convenient.
Discussion
We report a work on Rydberg receiver enhanced via deep
learning to detect multifrequency MW ﬁelds. The results show
that the deep learning enhanced Rydberg mixer receives and
decodes multifrequency MW ﬁelds efﬁciently; these ﬁelds are
often difﬁcult to decode using theoretical methods. Using
the deep learning model, the Rydberg receiver is robust to noise
induced by the environment and atomic collisions and is immune
to the distortion that results from the limited bandwidths of
the Rydberg atoms (from dipole-dipole interactions and the EIT
pumping rate, as studied in ref.
7) for high-speed signals
(Δf = 200 kHz). In addition to increasing the transmission speed
of the signals, further increments in the information transmission
rate are achieved by using more bins, which is feasible because of
the scalability of our model. Besides the transmission rate, this
deep learning enhanced Rydberg system promises for use in
studies of the channel capacity limitations. Because spectra that
are difﬁcult for humans to recognize as a result of noise and
distortion are distinguishable when using the deep learning
model, Rydberg systems enhanced by deep learning could take
steps toward the realization of the capacity limit proposed in the
literature ref. 34. To obtain high performance (i.e. high signal-to-
noise ratio, information transmission rate, channel capacity and
accuracy), the training epochs and training set must be extended
and enlarged.
In summary, we have demonstrated the advantages of receiving
and decoding multifrequency signals using a deep learning
enhanced Rydberg receiver. In a multifrequency signal receiver,
rather than using multiple band-pass ﬁlters, lock-in ampliﬁer7,12
and other complex circuits, signals can be decoded using the
extremely sensitive Rydberg atoms and the deep learning model
at high speed and with high accuracy without solving the Lind-
blad master equation. One of the advantages of use of the Ryd-
berg atom is that the accuracy of the Rydberg atom approaches
the photon shot noise limit35. In principle, the accuracy of the
Rydberg atom is higher than that of the classical antenna.
According to recent work based on the atomic superheterodyne
method, ultrahigh sensitivity can be obtained10. However, in this
proof-of-principle demonstration, there is considerable room for
the optimization required to reach that limit (e.g., stabilization of
the laser, narrowing the laser linewidth, and temperature stabi-
lization). The sensitivity of the Rydberg atoms is a double-edged
sword because it also involves noise. The deep learning model
restricts this side effect while taking full advantage of the Rydberg
atoms’ sensitivity to the signal. Using the automatic feature
Train
Validation
Loss
0
12
21
14
13
13
14
18
18
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
20
40
60
80
100
120
0.00
0.05
0.10
0.15
0.20
0.25
Loss
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
20
21
15 
21
18
0
0
0
0
0
0
0
0
0
0
22
29
0
0
0
0
0
0
0
0
23
0
1
1
0
0
0
0
0
0
0
0
20
40
60
80
100
120
0.00
0.05
0.10
0.15
0.20
(a)
(b)
(f)
(e)
(d)
(c)
0
0
0
1
0
0
0
1
0
1
1
0
0
0
1
110
1
1
1
1
0
1
25
0
20
0
Train
Validation
Epoch = 83
Epoch = 78
000 001 010 011 100 101 110 111
0
18
0
18
15
14
11
4
12
14
8
0
0
2
0
0
0
1
0
0
0
0
0
0
0
0
7
0
4
0
1
0
0
0
0
1
0
0
4
7
5
7
6
2
0
1
0
7
0
0
1
1
0
0
2
0
2
0
1
0
1
0
0
0
0
1
Accuracy=20.63%
Accuracy=100%
Epoch
Epoch
Accuracy=98.83%
Accuracy=60.00%
Predicted labels
Predicted labels
Predicted labels
Predicted labels
Target labels
Target labels
Target labels
Target labels
Deep learning
Master equation
20 bins
Resolution
0
0
0
1
0
0
0
1
0
1
1
0
0
0
1
110
1
1
1
1
0
1
0
0
0
1
0
0
0
1
0
1
1
0
0
0
1
110
1
1
1
1
0
1
0
0
0
1
0
0
0
1
0
1
1
0
0
0
1
110
1
1
1
1
0
1
000 001 010 011 100 101 110 111
000 001 010 011 100 101 110 111
000 001 010 011 100 101 110 111
0
18
0
18
2
10
0
2
0
0
1
0
0
0
0
1
1
0
0
0
1
5
0
0
1
0
0
0
0
0
1
0
0
0
4
0
0
1
13
16
6
12
7
17
8
1
1
0
7
1
0
1
6
0
1
9
0
0
3
1
0
0
0
0
1
1
Fig. 5 Loss curves and confusion maps of the deep learning method and the master equation ﬁtting method for MWs with 20 bins and frequency
difference Δf. a Loss vs. epoch curves for training and validation of a 20-bin carrier with frequency difference Δf = 2 kHz. b Confusion matrix for
epoch = 78 on the test data set. Only the ﬁrst 3 bits of the 20 bits are selected to carry the messages and all other bits are 0. The accuracy is 100% for 123
testing spectra. c Loss vs. epoch curves for training and validation of the four MW bins (one bin is the reference) with frequency difference Δf = 200 kHz.
d Confusion matrix for epoch = 83 on the test data set. The accuracy is 98.83% for 171 testing spectra. e Predicted solution of the master equation without
consideration of higher-order terms and noise on a 20-bin carrier with frequency difference Δf = 2 kHz. The number of spectra is 160, where the spectra
were sampled uniformly from eight categories. The accuracy is 20.63%. f Predicted solution of the master equation without consideration of higher-order
terms and noise on four-bin carrier with frequency difference Δf = 200 kHz. The number of spectra is 160, which were sampled uniformly from eight
categories. The accuracy is 60.00%. In (b) and (e), only the ﬁrst 3 bits of a total of 20 bits (including a reference) are labelled.
ARTICLE
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
6
NATURE COMMUNICATIONS |  (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications

extraction processes of the neural networks, the spectra are
classiﬁed in a supervised manner. If the features (e.g. mean value,
variance, frequency spectrum) are extracted manually, the spectra
are then clustered by unsupervised learning methods such as
t-distributed stochastic neighbour embedding (t-SNE) or the
density-based
spatial
clustering
of
applications
with
noise
(DBSCAN) method31, without training on the training set. Our
work will be useful in ﬁelds including high-precision signal
measurement and atomic sensors. Additionally, this decoding
ability can be generalized further to decode other signals that are
encoded by different encoding protocols, e.g., frequency division
multiplexing amplitude shift keying (FDM-ASK), frequency
division multiplexing quadrature amplitude modulation (FDM-
QAM), and IEEE 802.11ac WLAN standard signals for a 5 GHz
carrier. The frequency of carrier to be decoded covers from sev-
eral hertz to terahertz since for Rydberg atoms to receive MW
with different wavelengths, the only part of the system that needs
to be tuned is the frequency of the laser, while in classical
receivers, the wavelength of the received MW is limited by the
size of the antenna36–39. In addition to communications, our
receiver can be used to detect multiple targets from multi-
frequency signals caused by the Doppler effect.
Methods
Generation and calibration of MW ﬁelds. The MW ﬁelds used in our experi-
ments were synthesized by the signal generator (1465F-V from Ceyear) and a
frequency horn. Each bin in the multifrequency MW ﬁeld is tunable in terms of
frequency, amplitude and phase. The RF source operates in the range from DC to
40 GHz. The frequency horn is located close to the Rb cell. We used an antenna
and a spectrum analyser (4024F from Ceyear) to receive the MW ﬁelds and then
calibrated the amplitudes of the MW ﬁelds at the centre of the Rb cell.
The probe transmission spectrum in the time domain when Δp = 0, Δc = 0 and
Δs = 0 reﬂects the interference among the multifrequency MW bins, which results
from the beat frequencies of the bins that occur through the interaction between
the atoms and light. The Rydberg atoms receive the MW bins by acting as an
antenna and a mixer9,11,12. After reception by the atoms, the frequency spectrum of
the probe transmission shows that we can obtain the frequency differential signal
from the probe transmission spectrum. This represents an application of our atoms
to reduce the modulated signal frequency (from terahertz to kilohertz magnitude),
which allows the signal to be received and decoded using simple apparatus. In our
experiment, more than 20 frequency bins can be added to the atoms, for which the
dynamic range is greater than 30 dBm. The amplitudes, phases and frequencies of
these bins can be tuned individually. When the bandwidth is increased to detect an
increasing frequency difference Δf signal, more noise is involved, but this noise is
suppressed by the deep learning model. In other words, the signal can be
recognized using the deep learning model when the information transmission rate
is increased by raising the frequency difference Δf. These bins are used to send
FDM-PSK signals in the “FDM signal encoding and receiving ” section of the
main text.
Master equation. The Lindblad master equation is given as follows:
dρ=dt ¼ i H; ρ


=_ þ L=_, where ρ is the density matrix of the atomic ensemble
and H = ∑kH[ρ(k)] is the atom–light interaction Hamiltonian when summed over
all the single-atom Hamiltonians using the rotating wave approximation. This
Hamiltonian has the following matrix form:
H ¼ _
0

Ωp
2
0
0

Ωp
2
Δp
 Ωc
2
0
0
 Ωc
2
Δc þ Δp
 ΩsðtÞ
2
0
0
 ΩsðtÞ
2
Δc þ Δp þ Δs
0
B
B
B
B
B
@
1
C
C
C
C
C
A
;
ð1Þ
where for the MW signal E ¼ A1 cos½ ω0 þ ω1


t þ φ1 þ A2 cos½ ω0 þ ω2


tþ
φ2 þ A3 cos½ ω0 þ ω3


t þ φ3 þ A4 cos½ ω0 þ ω4


t þ φ4, we have the Rabi fre-
quency ΩsðtÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E2
1 þ E2
2
p
, where E1 ¼ A1 sin½ω1t þ φ1 þ A2 sin½ω2t þ φ2 þ
A3 sin½ω3t þ φ3 þ A4 sin½ω4t þ φ4 and E2 ¼ A1 cos½ω1t þ φ1 þ A2 cos½ω2tþ
φ2 þ A3 cos½ω3t þ φ3 þ A4 cos½ω4t þ φ4. The Rabi frequency can be derived as
follows:
E ¼ ∑
4
i¼1 Ai cos
ω0 þ ωi


t þ φi


¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E2
1 þ E2
2
q
cos ω0t þ arctan ∑4
i¼1 Ai sin ωit þ φi


∑4
i¼1 Ai cos ωit þ φi


 
!
;
ð2Þ
where the second term (which resonates with the energy levels of the Rydberg
atoms) induces the normal EIT spectrum and the ﬁrst term modulates that spec-
trum. In the interaction between the atoms and the MW ﬁelds, the atoms act as a
mixer such that the output signal frequency (ω1, ω2, ω3) is less than the input signal
frequency (ω0 + ω1, ω0 + ω2, ω0 + ω3). The modulation signal’s nonlinearity is
reduced by setting the reference and increasing its amplitude as shown in Eq. (3),
which is a precondition for recognition of these phases via deep learning.
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E2
1 þ E2
2
q
 A4
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 2 ∑
3
i¼1
Ai
A4
cos
ω4  ωi


t þ φ4  φi




s
 A4 þ ∑
3
i¼1 Ai cos
ω4  ωi


t þ φ4  φi




;
ð3Þ
where the condition for the approximations on the second line and the third line is
A4 ≫A1,2,3.
The Lindblad superoperator L = ∑kL[ρ(k)] is composed of single-atom
superoperators, where L[ρ(k)] represents the Lindbladian and has the following
form: L½ρðkÞ
_
¼  1
2 ∑m Cy
mCmρ þ ρCy
mCm


þ ∑mCmρCy
m where C1 ¼
ﬃﬃﬃﬃﬃ
Γe
p
g
		 
eh j,
C2 ¼
ﬃﬃﬃﬃ
Γr
p
ej i rh j and C3 ¼
ﬃﬃﬃﬃ
Γs
p
rj i sh j are collapse operators that stand for the decays
from state ej i to state g
		 
, from state rj i to state ej i and from state sj i to state rj i
with rates Γe, Γr and Γs, respectively. Because we are only concerned with the steady
state here, i.e. t →∞, the Lindblad master equation can be solved using dρ/dt = 0.
The complex susceptibility of the EIT medium has the form χ(v) = (∣μge∣2/ϵ0ℏ)ρeg,
where ρeg is the element of density matrix solved using the master equation. The
spectrum of the EIT medium can be obtained from the susceptibility using
T  eIm½χ:
Deep learning layers. Our deep learning model consists of a 1D CNN layer, a Bi-
LSTM layer and a dense layer. The mathematical sketches for these layers are given
as follows.
The 1D CNN layer is illustrated in Fig. 1c. The input signal convolutes the
kernel in the following form:
f  g ¼ ∑
N1
m¼0 f mgðnmÞ:
ð4Þ
where f represents the input data, g is the convolution kernel, m is the input data
index and n is the kernel index. The 1D CNN extracts the higher-order features
Fig. 6 Building blocks for LSTM layer and dense layer. The cells of a
long–short-term memory (LSTM) layer and dense layer are presented in
(a) and (b), respectively, where σ represents the sigmoid layer and tanh
represents the tanh layer.
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
ARTICLE
NATURE COMMUNICATIONS |   (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications
7

from the input data to reduce the lengths of the sequences fed into the Bi-LSTM
layer. Before ﬂowing into the Bi-LSTM layer, the data pass through the batch
normalization layer, the ReLU activation layer and the max-pooling layer, in that
sequence. For a mini-batch B ¼
x1m


, the output from the batch normalization
layer is yi = BNγ,β(xi) and the learning parameters are γ and β40. The update rules
for the batch normalization layer are:
μB  1
m ∑
m
i¼1 xi;
ð5Þ
σ2
B  1
m ∑
m
i¼1 xi  μB

2;
ð6Þ
^xi  xi  μB
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2
B þ ϵ
p
;
ð7Þ
yi  γ^xi þ β  BNγ;βðxiÞ;
ð8Þ
where Eqs. (5) and (6) evaluate the mean and the variance of the mini-batch,
respectively; the data are normalized using the mean and the variance in Eq. (7)
and the results are then scaled and shifted in Eq. (8). The training is accelerated
using the batch normalization layer and the overﬁtting is also weakened by this
layer. The output then passes through the ReLU activation layer. The activation
function of this layer is f ReLUðxÞ ¼ maxðx; 0Þ. The vanishing gradient problem is
diminished by this activation function. Next, the inputs are downsampled in a
max-pooling layer30.
The LSTM layer and an LSTM cell are shown schematically in Figs. 1d and 6a,
respectively. The equations for the LSTM are shown as Eqs. (9)–(14)32,41. At a time
t, the input xt and two internal states Ct−1and ht−1 are fed into the LSTM cell. The
ﬁrst thing to be decided by the LSTM cell is whether or not to forget in Eq. (9),
which outputs a number between 0 and 1 that represents retaining or forgetting.
Next, an input gate (Eq. (10)) decides which values are to be updated from a vector
of new candidate values created using Eq. (11). The new value is then added to the
cell state and the old value is forgotten in Eq. (12). Finally, the cell decides what to
output using Eqs. (13) and (14).
f t ¼ σ Wf  ht1; xt


þ bf


;
ð9Þ
it ¼ σ Wi  ht1; xt


þ bi


;
ð10Þ
~Ct ¼ tanh WC  ht1; xt


þ bC


;
ð11Þ
Ct ¼ f t ´ Ct1 þ it ´ ~Ct;
ð12Þ
ot ¼ σ Wo ht1; xt


þ bo


;
ð13Þ
ht ¼ ot ´ tanh Ct


;
ð14Þ
where σ(x) = 1/(1 + e−x) is the sigmoid function. The sigmoid and tanh functions
are applied in an element-wise manner. The LSTM is followed by a time-reversed
LSTM to constitute a Bi-LSTM layer that improves the memory for long sequences.
The dense layer and a neuron are drawn in Figs. 1e and 6b, respectively, and the
corresponding equations are
a ¼ w  x þ b;
ð15Þ
y ¼ gðaÞ;
ð16Þ
where w is the vector of weights, b is the bias, x represents the input data, g(a) = 1/
(1 + e−a) is the sigmoid activation function used to limit the output values to
between 0 and 1, and y is the output. The dense layer resizes the shape of the data
obtained from the Bi-LSTM to match the size of the label.
The training consists of both forward and backward propagation. A batch of
probe spectra propagates through the 1D CNN layer, the Bi-LSTM layer, and dense
layer during the forward training process. The differentiable loss function is then
calculated. In our case, the differentiable loss function is the mean squared error
(MSE) between the predictions and the ground truth, which is used widely in the
regression task32. The equation for the MSE is
LMSE ¼
1
m  n ∑
n
j¼1 ∑
m
i¼1 φi;j  f ðTi;jÞ

2
;
ð17Þ
where m is the number of data points in one spectrum, n is the mini-batch size, φi
is the ground truth and f(Ti) is the model prediction. In backpropagation, the
trainable weights of each layer are updated based on the learning rate and the
derivative of the MSE loss function with respect to the weights to minimize the loss
LMSE, such that
W  W  η ∂LMSE
∂W ;
ð18Þ
where η is the learning rate and W is the trainable weight for each layer. The
weights of each layer are then updated according to the RMSprop optimizer42.
The network is implemented using the Keras 2.3.1 framework on Python 3.6.11
(ref. 30). All weights are initialized with the Keras default. The hyper-parameters of
the deep learning model (including the convolution kernel length, the number of
hidden variables and the learning rate) are tuned using Optuna43.
Deep learning pipeline. To obtain better ﬁtting results, the data are scaled based on
their maximum and minimum values, i.e., T0 ¼ ðTi  minðTÞÞ=ðmaxðTÞ  minðTÞÞ.
The labels are encoded in dense vectors with four elements rather than in one-shot
encoding vectors to save space32. Each of these elements is either 0 or 1, representing
the relative phase 0 or π of each bin, respectively.
A one-dimensional convolution layer (1D CNN), a bidirectional long–short-
term memory layer (Bi-LSTM) and a dense layer are used in our deep learning
model. The deep learning model structure is shown in Fig. 7. The data size for the
input layer is given in the form (batch size, length of probe spectrum, number of
features). The batch size is 64 in our case. Because the duration of the spectrum
ranges from t = 0 to t = 0.999 ms with a time difference of τ = 1 μs, the spectrum
length is 1000. For a 1D input, the number of features is 1. Therefore, the data size
for the input layer is (64, 1000, 1).
During training of this model, fourfold cross-validation is used to save the
amount of training data.The data set is split as shown in Fig. 8. First, the data set is
split into two parts. The ﬁrst is the test set (red), which remains untouched during
training. The second (purple) is used to train the model. In the cross-validation
process, the rest data set (purple) is copied four times and is divided equally into
four parts each. One of these parts is the validation data set (green) and the others
are used as training sets (blue). Four models are trained on the different training
sets and validation sets. Then the best model is chosen according to the validation
set and is tested on the test set. After splitting, the training set, the validation set,
and the test set all remain unchanged. In every epoch, each model iterates the
training set only once. There is no new set being taken; instead, the same training
set is iterated once each epoch.
Conv1D
input:
output:
(64, 1000, 1)
(64, 541, 20)
BatchNormalization
input:
output:
(64, 541, 20)
(64, 541, 20)
Activation: ReLU
input:
output:
(64, 541, 20)
(64, 541, 20)
MaxPooling1D
input:
output:
(64, 541, 20)
(64, 180, 20)
Bidirectional(LSTM)
input:
output:
(64, 180, 20)
(64, 32)
BatchNormalization
input:
output:
(64, 32)
(64, 32)
Dense
input:
output:
(64, 32)
(64, 4)
four frequency bins MW fields
labels: 0100...
Visualized
Fig. 7 Structure of our deep learning model and size of the data. The
ReLU activation function is f(x) = max(0, x). After the data output from the
max-pooling layer, the visualizations are performed; see Supplementary
Materials for more details.
ARTICLE
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
8
NATURE COMMUNICATIONS |  (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications

The computational graph is cleared before each training sequence to prevent
leakage of the validation data. Gaussian noise (where the mean is 0 and the
standard deviation is 0.5) is added to the training data to increase the robustness of
the proposed model. In addition, the learning rate is adjusted during training to
jump out of the local minimum, which results in the jump in Fig. 2c in the main
text. The initial learning rate is 0.001. If the loss (mean-square error) of the
validation set does not decrease over 10 epochs, the learning rate is multiplied by
0.1. The RMSprop optimizer is used to update the weight of each layer during
training42.
The bidirectional LSTM layer can be replaced with the well-known self-
attention layer to improve the memory of our proposed model further44. However,
this would require more training time and increased GPU memory. The current
model has been able to meet our requirements to date.
Data availability
The data are available in Github45 (https://github.com/ZongkaiLiu/Deep-learning-
enhanced-Rydberg-multifrequency-microwave-recognition). The deep learning results
are presented in the Jupyter notebook. And the master equation results are presented in
the Mathematica notebooks.
Code availability
The codes are provided in Github45 (https://github.com/ZongkaiLiu/Deep-learning-
enhanced-Rydberg-multifrequency-microwave-recognition).
Received: 6 May 2021; Accepted: 22 March 2022;
References
1.
Liao, K.-Y. et al. Microwave electrometry via electromagnetically induced
absorption in cold Rydberg atoms. Phys. Rev. A 101, 053432 (2020).
2.
Fleischhauer, M., Imamoglu, A. & Marangos, J. P. Electromagnetically induced
transparency: optics in coherent media. Rev. Mod. Phys. 77, 633–673 (2005).
3.
Holloway, C. L. et al. Electric ﬁeld metrology for SI traceability: systematic
measurement uncertainties in electromagnetically induced transparency in
atomic vapor. J. Appl. Phys. 121, 233106 (2017).
4.
Sedlacek, J. A. et al. Microwave electrometry with Rydberg atoms in a vapour
cell using bright atomic resonances. Nat. Phys. 8, 819–824 (2012).
5.
Autler, S. H. & Townes, C. H. Stark effect in rapidly varying ﬁelds. Phys. Rev.
100, 703–722 (1955).
6.
Abi-Salloum, T. Y. Electromagnetically induced transparency and Autler-
Townes splitting: two similar but distinct phenomena in two categories of
three-level atomic systems. Phys. Rev. A 81, 053836 (2010).
7.
Meyer, D. H., Cox, K. C., Fatemi, F. K. & Kunz, P. D. Digital communication
with Rydberg atoms and amplitude-modulated microwave ﬁelds. Appl. Phys.
Lett. 112, 211108 (2018).
8.
Jiao, Y. et al. Atom-based receiver for amplitude-modulated baseband signals in
high-frequency radio communication. Appl. Phys. Express 12, 126002 (2019).
9.
Gordon, J. A., Simons, M. T., Haddab, A. H. & Holloway, C. L. Weak electric-
ﬁeld detection with sub-1 Hz resolution at radio frequencies using a Rydberg
atom-based mixer. AIP Adv. 9, 045030 (2019).
10. Jing, M. et al. Atomic superheterodyne receiver based on microwave-dressed
Rydberg spectroscopy. Nat. Phys. 16, 911–915 (2020).
11. Simons, M. T., Haddab, A. H., Gordon, J. A. & Holloway, C. L. A Rydberg
atom-based mixer: measuring the phase of a radio frequency wave. Appl. Phys.
Lett. 114, 114101 (2019).
12. Holloway, C. L., Simons, M. T., Gordon, J. A. & Novotny, D. Detecting and
receiving phase-modulated signals with a Rydberg atom-based receiver. IEEE
Antennas Wirel. Propag. Lett. 18, 1853–1857 (2019).
13. Holloway, C. L., Simons, M. T., Haddab, A. H., Williams, C. J. & Holloway, M.
W. A real-time guitar recording using Rydberg atoms and electromagnetically
induced transparency: quantum physics meets music. AIP Adv. 9, 065110
(2019).
14. Robinson, A. K., Prajapati, N., Senic, D., Simons, M. T. & Holloway, C. L.
Determining the angle-of-arrival of a radio-frequency source with a Rydberg
atom-based sensor. Appl. Phys. Lett. 118, 114001 (2021).
15. Bason, M. G. et al. Enhanced electric ﬁeld sensitivity of RF-dressed Rydberg
dark states. N. J. Phys. 12, 065015 (2010).
16. Zou, H. et al. Atomic receiver by utilizing multiple radio-frequency coupling
at Rydberg states of rubidium. Appl. Sci. 10, 1346 (2020).
17. Song, Z. et al. Rydberg-atom-based digital communication using a
continuously tunable radio-frequency carrier. Opt. Express 27, 8848–8857
(2019).
18. Orazbayev, B. & Fleury, R. Far-ﬁeld subwavelength acoustic imaging by deep
learning. Phys. Rev. X 10, 031029 (2020).
19. Khanahmadi, M. & Mølmer, K. Time-dependent atomic magnetometry with a
recurrent neural network. Phys. Rev. A 103, 032406 (2021).
20. Giordani, T. et al. Machine learning-based classiﬁcation of vector vortex
beams. Phys. Rev. Lett. 124, 160401 (2020).
21. Liu, Z., Yan, S., Liu, H. & Chen, X. Superhigh-resolution recognition of optical
vortex modes assisted by a deep-learning method. Phys. Rev. Lett. 123, 183902
(2019).
22. Doster, T. & Watnik, A. T. Machine learning approach to OAM beam
demultiplexing via convolutional neural networks. Appl. Opt. 56, 3386–3396
(2017).
23. da Silva, B. P., Marques, B. A. D., Rodrigues, R. B., Ribeiro, P. H. S. & Khoury,
A. Z. Machine-learning recognition of light orbital-angular-momentum
superpositions. Phys. Rev. A 103, 063704 (2021).
24. Wigley, P. B. et al. Fast machine-learning online optimization of ultra-cold-
atom experiments. Sci. Rep. 6, https://doi.org/10.1038/srep25890 (2016).
25. Tranter, A. D. et al. Multiparameter optimisation of a magneto-optical trap
using deep learning. Nat. Commun. 9, 4360 (2018).
26. Mukherjee, R., Xie, H. & Mintert, F. Bayesian optimal control of Greenberger-
Horne-Zeilinger states in Rydberg lattices. Phys. Rev. Lett. 125, 203603 (2020).
27. Mills, K., Ronagh, P. & Tamblyn, I. Finding the ground state of spin
hamiltonians with reinforcement learning. Nat. Mach. Intell. 2, 509–517
(2020).
28. Wang, Z. T., Ashida, Y. & Ueda, M. Deep reinforcement learning control of
quantum cartpoles. Phys. Rev. Lett. 125, 100401 (2020).
29. Bukov, M. et al. Reinforcement learning in different phases of quantum
control. Phys. Rev. X 8, 031086 (2018).
30. Chollet, F. et al. Keras. https://github.com/fchollet/keras (2015).
31. Pedregosa, F. et al. Scikit-learn: machine learning in Python. J. Mach. Learning
Res. 12, 2825–2830 (2011).
32. Chollet, F. Deep Learning with Python (Manning Publications, 2017).
33. Sahoo, D., Pham, Q., Lu, J. & Hoi, S. C. H. Online deep learning: learning deep
neural networks on the ﬂy. In IJCAI'18: Proceedings of the 27th International
Joint Conference on Artiﬁcial Intelligence (2018).
34. Cox, K. C., Meyer, D. H., Fatemi, F. K. & Kunz, P. D. Quantum-limited
atomic receiver in the electrically small regime. Phys. Rev. Lett. 121, 110502
(2018).
35. Kumar, S., Fan, H., Kübler, H., Jahangiri, A. J. & Shaffer, J. P. Rydberg-atom
based radio-frequency electrometry using frequency modulation spectroscopy
in room temperature vapor cells. Opt. Express 25, 8625–8637 (2017).
36. Meyer, D. H., Kunz, P. D. & Cox, K. C. Waveguide-coupled Rydberg spectrum
analyzer from 0 to 20 ghz. Phys. Rev. Appl. 15, 014053 (2021).
37. Meyer, D. H., Castillo, Z. A., Cox, K. C. & Kunz, P. D. Assessment of Rydberg
atoms for wideband electric ﬁeld sensing. J. Phys. B Atom. Mol. Opt. Phys. 53,
034001 (2020).
38. Wade, C. G. et al. A terahertz-driven non-equilibrium phase transition in a
room temperature atomic vapour. Nat. Commun. 9, 3567 (2018).
39. Holloway, C. L. et al. Broadband Rydberg atom-based electric-ﬁeld probe for
SI-traceable, self-calibrated measurements. IEEE Trans. Antennas Propagation
62, 6169–6182 (2014).
40. Ioffe, S. & Szegedy, C. Batch normalization: accelerating deep network training
by reducing internal covariate shift. In Proceedings of the 32nd International
Conference on Machine Learning (eds. Bach, F. & Blei, D.), Vol. 37 of
Proceedings of Machine Learning Research (PMLR), Lille, France, 448–456
(2015).
Validation set
Training set
Training and Validation set
Test set
Fig. 8 Data partition during training, validating, and testing. First, the
data are split into two sets. The ﬁrst is the test set. The remaining data set
is copied four times and is then split into four sets, each with different parts
that act as training sets and validation sets for the training of four deep
learning models.
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
ARTICLE
NATURE COMMUNICATIONS |   (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications
9

41. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9,
1735–1780 (1997).
42. Geoffrey, H., Nitish, S. & Kevin, S. Lecture 6a overview of mini-batch gradient
descent. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.
pdf (2012).
43. Akiba, T., Sano, S., Yanase, T., Ohta, T. & Koyama, M. Optuna: a next-
generation hyperparameter optimization framework. In Proceedings of the
25rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (2019).
44. Vaswani, A. et al. Attention is all you need. Advances in Neural Information
Processing Systems, (eds Guyon, I. et al.) Vol. 30 (Curran Associates, Inc., 2017).
45. Liu, Z.-K. Deep learning enhanced Rydberg multifrequency microwave
recognition. https://doi.org/10.5281/zenodo.6202552 (2022).
46. Šibalić, N., Pritchard, J. D., Adams, C. S. & Weatherill, K. J. Arc: an open-
source library for calculating properties of alkali Rydberg atoms. Comput.
Phys. Commun. 220, 319–331 (2017).
Acknowledgements
Z.-K.L. gratefully acknowledges the instructive discussion about deep learning with Yue
Chen at the National Engineering Laboratory for Speech and Language Information
Processing and the enlightenment of leraning machine learning during studying with Dr.
Lei Gong at Department of Optics and Optical Engineering at USTC. D.-S.D.
acknowledges funding from the National Key R&D Program of China (Grant No.
2017YFA0304800), the National Natural Science Foundation of China (Grant Nos.
U20A20218, 61525504, 61435011), the Anhui Initiative in Quantum Information
Technologies (Grant No. AHY020200), the Youth Innovation Promotion Association of
the Chinese Academy of Sciences (Grant No. 2018490), and the major science and
technology projects in Anhui Province. B.-S.S. acknowledges funding from the National
Natural Science Foundation of China (Grant No. 11934013). We thank David MacDo-
nald, MSc, from Liwen Bianji, Edanz Editing China (www.liwenbianji.cn/ac), for editing
the English text of a draft of this manuscript.
Author contributions
D.-S.D. conceived the idea for the study. Z.-K.L. conducted the physical experiments and
designed the deep learning model and communication protocols. Z.-K.L. derived the
theoretical formula. Z.-K.L. analysed the data with assistance from L.-H.Z., B.L., and Z.-
Y.Z. The manuscript was written by Z.-K.L. The research were supervised by D.-S.D., B.-
S.S. and G.-C.G. All authors contributed to discussions regarding the results and analysis
contained in the manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary information The online version contains supplementary material
available at https://doi.org/10.1038/s41467-022-29686-7.
Correspondence and requests for materials should be addressed to Dong-Sheng Ding or
Bao-Sen Shi.
Peer review information Nature Communications thanks the anonymous reviewer(s) for
their contribution to the peer review of this work. Peer reviewer reports are available.
Reprints and permission information is available at http://www.nature.com/reprints
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly from
the copyright holder. To view a copy of this license, visit http://creativecommons.org/
licenses/by/4.0/.
© The Author(s) 2022
ARTICLE
NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29686-7
10
NATURE COMMUNICATIONS |  (2022) 13:1997 | https://doi.org/10.1038/s41467-022-29686-7 | www.nature.com/naturecommunications

