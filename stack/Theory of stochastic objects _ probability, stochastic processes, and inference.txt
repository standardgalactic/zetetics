
Theory of  
Stochastic Objects
Probability, Stochastic Processes 
and Inference

CHAPMAN & HALL/CRC  
Texts in Statistical Science Series
Series Editors
Joseph K. Blitzstein, Harvard University, USA
Julian J. Faraway, University of Bath, UK
Martin Tanner, Northwestern University, USA
Jim Zidek, University of British Columbia, Canada
Statistical Theory: A Concise Introduction 
F. Abramovich and Y. Ritov 
Practical Multivariate Analysis, Fifth Edition
A. Afifi, S. May, and V.A. Clark
Practical Statistics for Medical Research
D.G. Altman
Interpreting Data: A First Course  
in Statistics
A.J.B. Anderson
Introduction to Probability with R 
K. Baclawski
Linear Algebra and Matrix Analysis for Statistics
S. Banerjee and A. Roy
Modern Data Science with R
B. S. Baumer, D. T. Kaplan, and N. J. Horton
Mathematical Statistics: Basic Ideas and Selected 
Topics, Volume I, Second Edition
P. J. Bickel and K. A. Doksum
Mathematical Statistics: Basic Ideas and Selected 
Topics, Volume II
P. J. Bickel and K. A. Doksum
Analysis of Categorical Data with R
C. R. Bilder and T. M. Loughin
Statistical Methods for SPC and TQM
D. Bissell
Introduction to Probability
J. K. Blitzstein and J. Hwang
Bayesian Methods for Data Analysis, Third Edition 
B.P. Carlin and T.A. Louis
Statistics in Research and Development,  
Second Edition
R. Caulcutt
The Analysis of Time Series: An Introduction,  
Sixth Edition 
C. Chatfield 
Introduction to Multivariate Analysis  
C. Chatfield and A.J. Collins 
Problem Solving: A Statistician’s Guide,  
Second Edition 
C. Chatfield
Statistics for Technology: A Course in Applied 
Statistics, Third Edition
C. Chatfield
Analysis of Variance, Design, and Regression:  
Linear Modeling for Unbalanced Data,  
Second Edition 
R. Christensen
Bayesian Ideas and Data Analysis: An Introduction 
for Scientists and Statisticians 
R. Christensen, W. Johnson, A. Branscum,  
and T.E. Hanson
Modelling Binary Data, Second Edition
D. Collett
Modelling Survival Data in Medical Research, Third 
Edition
D. Collett
Introduction to Statistical Methods for  
Clinical Trials 
T.D. Cook and D.L. DeMets 
Applied Statistics: Principles and Examples 
D.R. Cox and E.J. Snell
Multivariate Survival Analysis and Competing Risks
M. Crowder
Statistical Analysis of Reliability Data
M.J. Crowder, A.C. Kimber, T.J. Sweeting,  
and R.L. Smith
An Introduction to Generalized Linear Models, 
Third Edition
A.J. Dobson and A.G. Barnett
Nonlinear Time Series: Theory, Methods, and 
Applications with R Examples
R. Douc, E. Moulines, and D.S. Stoffer
Introduction to Optimization Methods and Their 
Applications in Statistics 
B.S. Everitt
Extending the Linear Model with R: Generalized 
Linear, Mixed Effects and Nonparametric Regression 
Models, Second Edition
J.J. Faraway
Linear Models with R, Second Edition
J.J. Faraway
A Course in Large Sample Theory
T.S. Ferguson
Multivariate Statistics: A Practical Approach
B. Flury and H. Riedwyl

Readings in Decision Analysis 
S. French
Discrete Data Analysis with R: Visualization and 
Modeling Techniques for Categorical and Count 
Data  
M. Friendly and D. Meyer
Markov Chain Monte Carlo: Stochastic Simulation 
for Bayesian Inference, Second Edition
D. Gamerman and H.F. Lopes
Bayesian Data Analysis,  Third Edition 
A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, 
A. Vehtari, and D.B. Rubin
Multivariate Analysis of  Variance and Repeated 
Measures: A Practical Approach for Behavioural 
Scientists
D.J. Hand and C.C. Taylor
Practical Longitudinal Data Analysis
D.J. Hand and M. Crowder
Linear Models and the Relevant Distributions and 
Matrix Algebra 
D.A. Harville 
Logistic Regression Models 
J.M. Hilbe 
Richly Parameterized Linear Models: Additive, Time 
Series, and Spatial Models Using Random Effects 
J.S. Hodges 
Statistics for Epidemiology 
N.P. Jewell
Stochastic Processes: An Introduction, Third Edition
P.W. Jones and P. Smith 
The Theory of Linear Models
B. Jørgensen
Pragmatics of Uncertainty
J.B. Kadane
Principles of Uncertainty
J.B. Kadane
Graphics for Statistics and Data Analysis with R
K.J. Keen
Mathematical Statistics 
K. Knight 
Introduction to Functional Data Analysis 
P. Kokoszka and M. Reimherr 
Introduction to Multivariate Analysis: Linear and 
Nonlinear Modeling
S. Konishi
Nonparametric Methods in Statistics with SAS 
Applications
O. Korosteleva
Modeling and Analysis of Stochastic Systems,  
Third Edition
V.G. Kulkarni
Exercises and Solutions in Biostatistical Theory
L.L. Kupper, B.H. Neelon, and S.M. O’Brien
Exercises and Solutions in Statistical Theory
L.L. Kupper, B.H. Neelon, and S.M. O’Brien
Design and Analysis of Experiments with R
J. Lawson
Design and Analysis of Experiments with SAS
J. Lawson
A Course in Categorical Data Analysis 
T. Leonard 
Statistics for Accountants
S. Letchford
Introduction to the Theory of Statistical Inference 
H. Liero and S. Zwanzig
Statistical Theory, Fourth Edition
B.W. Lindgren 
Stationary Stochastic Processes: Theory and 
Applications 
G. Lindgren
Statistics for Finance 
E. Lindström, H. Madsen, and J. N. Nielsen
The BUGS Book: A Practical Introduction to 
Bayesian Analysis  
D. Lunn, C. Jackson, N. Best, A. Thomas, and  
D. Spiegelhalter
Introduction to General and Generalized  
Linear Models
H. Madsen and P. Thyregod
Time Series Analysis
H. Madsen
Pólya Urn Models
H. Mahmoud
Randomization, Bootstrap and Monte Carlo 
Methods in Biology, Third Edition 
B.F.J. Manly
Statistical Regression and Classification:  
From Linear Models to Machine Learning 
N. Matloff
Introduction to Randomized Controlled Clinical 
Trials, Second Edition 
J.N.S. Matthews
Statistical Rethinking: A Bayesian Course with 
Examples in R and Stan 
R. McElreath
Statistical Methods in Agriculture and Experimental 
Biology,  Second Edition
R. Mead, R.N. Curnow, and A.M. Hasted
Statistics in Engineering: A Practical Approach
A.V. Metcalfe
Theory of Stochastic Objects: Probability, Stochastic 
Processes and Inference
A.C. Micheas
Statistical Inference: An Integrated Approach, 
Second Edition
H. S. Migon, D. Gamerman, and F. Louzada

Beyond ANOVA: Basics of Applied Statistics 
R.G. Miller, Jr.
A Primer on Linear Models
J.F. Monahan
Stochastic Processes: From Applications to Theory
P.D Moral and S. Penev
Applied Stochastic Modelling, Second Edition 
B.J.T. Morgan
Elements of Simulation 
B.J.T. Morgan
Probability: Methods and Measurement
A. O’Hagan
Introduction to Statistical Limit Theory
A.M. Polansky
Applied Bayesian Forecasting and Time Series 
Analysis
A. Pole, M. West, and J. Harrison
Statistics in Research and Development,  
Time Series: Modeling, Computation, and Inference
R. Prado and M. West
Essentials of Probability Theory for Statisticians 
M.A. Proschan and P.A. Shaw
Introduction to Statistical Process Control 
P. Qiu 
Sampling Methodologies with Applications 
P.S.R.S. Rao
A First Course in Linear Model Theory
N. Ravishanker and D.K. Dey
Essential Statistics, Fourth Edition 
D.A.G. Rees
Stochastic Modeling and Mathematical Statistics:  
A Text for Statisticians and Quantitative Scientists
F.J. Samaniego 
Statistical Methods for Spatial Data Analysis
O. Schabenberger and C.A. Gotway
Bayesian Networks: With Examples in R
M. Scutari and J.-B. Denis
Large Sample Methods in Statistics
P.K. Sen and J. da Motta Singer
Introduction to Statistical Methods for Financial 
Models
T. A. Severini
Spatio-Temporal Methods in Environmental 
Epidemiology
G. Shaddick and J.V. Zidek
Decision Analysis: A Bayesian Approach
J.Q. Smith
Analysis of Failure and Survival Data
P. J. Smith
Applied Statistics: Handbook of GENSTAT 
Analyses 
E.J. Snell and H. Simpson 
Applied Nonparametric Statistical Methods,  
Fourth Edition 
P. Sprent and N.C. Smeeton
Data Driven Statistical Methods 
P. Sprent 
Generalized Linear Mixed Models:  
Modern Concepts, Methods and Applications
W. W. Stroup
Survival Analysis Using S: Analysis of  
Time-to-Event Data
M. Tableman and J.S. Kim
Applied Categorical and Count Data Analysis  
W. Tang, H. He, and X.M. Tu 
Elementary Applications of Probability Theory, 
Second Edition 
H.C. Tuckwell
Introduction to Statistical Inference and Its 
Applications with R 
M.W. Trosset 
Understanding Advanced Statistical Methods
P.H. Westfall and K.S.S. Henning
Statistical Process Control: Theory and Practice, 
Third Edition 
G.B. Wetherill and D.W. Brown
Generalized Additive Models: An Introduction  
with R, Second Edition
S. Wood
Epidemiology: Study Design and  
Data Analysis, Third Edition
M. Woodward
Practical Data Analysis for Designed Experiments
B.S. Yandell

Texts in Statistical Science
Athanasios Christou Micheas 
Department of Statistics, University of Missouri, USA
Theory of  
Stochastic Objects
Probability, Stochastic Processes  
and Inference

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2018 by Taylor & Francis Group, LLC 
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed on acid-free paper
Version Date: 20171219
International Standard Book Number-13:  978-1-4665-1520-8 (Hardback)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been 
made to publish reliable data and information, but the author and publisher cannot assume responsibility for the 
validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the  
copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to 
publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let 
us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or 
utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including  
photocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission 
from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com  
(http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 
 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For 
organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for 
identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com 
and the CRC Press Web site at
http://www.crcpress.com
Library of Congress Cataloging-in-Publication Data
Names: Micheas, Athanasios Christou, author.
Title: Theory of stochastic objects : probability, stochastic processes and 
inference / by Athanasios Christou Micheas.
Description: Boca Raton, Florida : CRC Press, [2018] | Includes 
bibliographical references and index.
Identifiers: LCCN 2017043053| ISBN 9781466515208 (hardback) | ISBN 
9781315156705 (e-book)
Subjects: LCSH: Point processes. | Stochastic processes.
Classification: LCC QA274.42 .M53 2018 | DDC 519.2/3--dc23
LC record available at https://lccn.loc.gov/2017043053

To my family


Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xv
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xxi
List of Tables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xxiii
List of Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . .
xxv
List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xxvii
List of Distribution Notations . . . . . . . . . . . . . . . . . . . . . .
xxix
1 Rudimentary Models and Simulation Methods . . . . . . . . . . .
1
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2 Rudimentary Probability . . . . . . . . . . . . . . . . . . . . . .
2
1.2.1 Probability Distributions . . . . . . . . . . . . . . . . . . .
4
1.2.2 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.3 Mixtures of Distributions
. . . . . . . . . . . . . . . . . .
9
1.2.4 Transformations of Random Vectors . . . . . . . . . . . . .
11
1.3 The Bayesian Approach
. . . . . . . . . . . . . . . . . . . . . .
11
1.3.1 Conjugacy . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.3.2 General Prior Selection Methods and Properties
. . . . . .
14
1.3.3 Hierarchical Bayesian Models . . . . . . . . . . . . . . . .
15
1.4 Simulation Methods
. . . . . . . . . . . . . . . . . . . . . . . .
17
1.4.1 The Inverse Transform Algorithm . . . . . . . . . . . . . .
17
1.4.2 The Acceptance-Rejection Algorithm . . . . . . . . . . . .
18
1.4.3 The Composition Method for Generating Mixtures . . . . .
20
1.4.4 Generating Multivariate Normal and Related Distributions .
20
1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2 Statistical Inference . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.2 Decision Theory
. . . . . . . . . . . . . . . . . . . . . . . . . .
27
ix

x
CONTENTS
2.3 Point Estimation
. . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.3.1 Classical Methods . . . . . . . . . . . . . . . . . . . . . .
30
2.3.2 Bayesian Approach . . . . . . . . . . . . . . . . . . . . . .
39
2.3.3 Evaluating Point Estimators Using Decision Theory
. . . .
40
2.3.4 Convergence Concepts and Asymptotic Behavior . . . . . .
42
2.4 Interval Estimation . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.4.1 Conﬁdence Intervals . . . . . . . . . . . . . . . . . . . . .
45
2.4.2 Highest Posterior Density Credible Sets . . . . . . . . . . .
46
2.4.3 Decision Theoretic . . . . . . . . . . . . . . . . . . . . . .
48
2.5 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.5.1 Classic Methods . . . . . . . . . . . . . . . . . . . . . . .
49
2.5.2 Bayesian Testing Procedures . . . . . . . . . . . . . . . . .
55
2.5.3 Decision Theoretic . . . . . . . . . . . . . . . . . . . . . .
58
2.5.4 Classical and Bayesian p-values . . . . . . . . . . . . . . .
60
2.5.5 Reconciling the Bayesian and Classical Paradigms . . . . .
65
2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
2.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
3 Measure and Integration Theory . . . . . . . . . . . . . . . . . . .
77
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.2 Deterministic Set Theory . . . . . . . . . . . . . . . . . . . . . .
77
3.3 Topological Spaces and σ-ﬁelds . . . . . . . . . . . . . . . . . .
79
3.4 Product Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
3.5 Measurable Spaces and Mappings . . . . . . . . . . . . . . . . .
86
3.6 Measure Theory and Measure Spaces
. . . . . . . . . . . . . . .
91
3.6.1 Signed Measures and Decomposition Theorems
. . . . . .
97
3.6.2 Carath´eodory Measurability and Extension Theorem . . . .
100
3.6.3 Construction of the Lebesgue Measure
. . . . . . . . . . .
103
3.7 Deﬁning Integrals with Respect to Measures . . . . . . . . . . . .
106
3.7.1 Change of Variable and Integration over Sets . . . . . . . .
113
3.7.2 Lebesgue, Riemann and Riemann-Stieltjes Integrals
. . . .
115
3.7.3 Radon-Nikodym Theorem . . . . . . . . . . . . . . . . . .
118
3.7.4 Product Measure and Fubini Theorem . . . . . . . . . . . .
121
3.7.5 Lp-spaces . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
3.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
4 Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
4.2 Probability Measures and Probability Spaces
. . . . . . . . . . .
139
4.2.1 Extension of Probability Measure . . . . . . . . . . . . . .
140
4.2.2 Deﬁning Random Objects . . . . . . . . . . . . . . . . . .
143
4.2.3 Distribution Functions and Densities
. . . . . . . . . . . .
148

CONTENTS
xi
4.2.4 Independence . . . . . . . . . . . . . . . . . . . . . . . . .
153
4.2.5 Calculating Probabilities for Limits of Events . . . . . . . .
158
4.2.6 Expectation of a Random Object . . . . . . . . . . . . . . .
159
4.2.7 Characteristic Functions . . . . . . . . . . . . . . . . . . .
162
4.3 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . .
165
4.3.1 Conditioning on the Value of a Random Variable . . . . . .
165
4.3.2 Conditional Probability and Expectation Given a σ-ﬁeld . .
170
4.3.3 Conditional Independence Given a σ-ﬁeld
. . . . . . . . .
177
4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
4.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5 Convergence of Random Objects . . . . . . . . . . . . . . . . . .
183
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
5.2 Existence of Independent Sequences of Random Variables . . . .
183
5.3 Limiting Behavior of Sequences of Random Variables
. . . . . .
184
5.3.1 Slutsky and Cram´er Theorems . . . . . . . . . . . . . . . .
184
5.3.2 Consistency of the MLE . . . . . . . . . . . . . . . . . . .
186
5.4 Limiting Behavior of Probability Measures
. . . . . . . . . . . .
187
5.4.1 Integrating Probability Measures to the Limit . . . . . . . .
187
5.4.2 Compactness of the Space of Distribution Functions . . . .
191
5.4.3 Weak Convergence via Non-Central Moments
. . . . . . .
193
5.5 Random Series . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
5.5.1 Convolutions . . . . . . . . . . . . . . . . . . . . . . . . .
194
5.5.2 Fourier Inversion and the Continuity Theorem
. . . . . . .
196
5.5.3 Limiting Behavior of Partial Sums . . . . . . . . . . . . . .
199
5.5.4 Central Limit Theorems . . . . . . . . . . . . . . . . . . .
200
5.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
5.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
6 Random Sequences . . . . . . . . . . . . . . . . . . . . . . . . . .
207
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
6.2 Deﬁnitions and Properties
. . . . . . . . . . . . . . . . . . . . .
207
6.3 Martingales . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
6.3.1 Filtrations and Stopping Times
. . . . . . . . . . . . . . .
215
6.3.2 Convergence Theorems
. . . . . . . . . . . . . . . . . . .
216
6.3.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . .
223
6.3.4 Wald Identities and Random Walks . . . . . . . . . . . . .
225
6.4 Renewal Sequences . . . . . . . . . . . . . . . . . . . . . . . . .
226
6.5 Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
6.5.1 Deﬁnitions and Properties . . . . . . . . . . . . . . . . . .
233
6.5.2 Discrete State Space . . . . . . . . . . . . . . . . . . . . .
236
6.5.3 The Martingale Problem . . . . . . . . . . . . . . . . . . .
240
6.5.4 Visits to Fixed States: General State Space
. . . . . . . . .
241

xii
CONTENTS
6.5.5 Visits to Fixed States: Discrete State Space . . . . . . . . .
243
6.5.6 State Classiﬁcation . . . . . . . . . . . . . . . . . . . . . .
248
6.5.7 Limiting Behavior . . . . . . . . . . . . . . . . . . . . . .
255
6.6 Stationary Sequences and Ergodicity . . . . . . . . . . . . . . . .
258
6.7 Applications to Markov Chain Monte Carlo Methods . . . . . . .
262
6.7.1 Metropolis-Hastings Algorithm . . . . . . . . . . . . . . .
263
6.7.2 Gibbs Sampling
. . . . . . . . . . . . . . . . . . . . . . .
264
6.7.3 Reversible Jump Markov Chain Monte Carlo . . . . . . . .
265
6.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
268
6.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269
7 Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . .
275
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
7.2 The Poisson Process
. . . . . . . . . . . . . . . . . . . . . . . .
277
7.3 General Stochastic Processes . . . . . . . . . . . . . . . . . . . .
279
7.3.1 Continuous Time Filtrations and Stopping Times . . . . . .
281
7.3.2 Continuous Time Martingales . . . . . . . . . . . . . . . .
283
7.3.3 Kolmogorov Existence Theorem . . . . . . . . . . . . . . .
285
7.4 Markov Processes . . . . . . . . . . . . . . . . . . . . . . . . . .
289
7.4.1 C`adl`ag Space . . . . . . . . . . . . . . . . . . . . . . . . .
289
7.4.2 Inﬁnitesimal Generators . . . . . . . . . . . . . . . . . . .
294
7.4.3 The Martingale Problem . . . . . . . . . . . . . . . . . . .
296
7.4.4 Construction via Subordinated Markov Chains . . . . . . .
297
7.4.5 Discrete State Space . . . . . . . . . . . . . . . . . . . . .
298
7.4.5.1 Sample Paths and State Classiﬁcation . . . . . . . .
301
7.4.5.2 Construction via Jump Times . . . . . . . . . . . .
303
7.4.5.3 Inﬁnitesimal Generator and Transition Function . .
305
7.4.5.4 Limiting Behavior . . . . . . . . . . . . . . . . . .
307
7.4.5.5 Birth-Death Processes . . . . . . . . . . . . . . . .
309
7.4.6 Brownian Motion . . . . . . . . . . . . . . . . . . . . . . .
312
7.4.7 Construction of the Wiener Measure . . . . . . . . . . . . .
314
7.5 Building on Martingales and Brownian Motion . . . . . . . . . .
317
7.5.1 Stochastic Integrals . . . . . . . . . . . . . . . . . . . . . .
317
7.5.2 Stochastic Diﬀerential Equations
. . . . . . . . . . . . . .
324
7.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
7.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332
A Additional Topics and Complements . . . . . . . . . . . . . . . .
337
A.1 Mappings in Rp . . . . . . . . . . . . . . . . . . . . . . . . . . .
337
A.2 Topological, Measurable and Metric Spaces . . . . . . . . . . . .
338
A.3 Baire Functions and Spaces . . . . . . . . . . . . . . . . . . . . .
342
A.4 Fisher Information
. . . . . . . . . . . . . . . . . . . . . . . . .
343
A.5 Multivariate Analysis Topics . . . . . . . . . . . . . . . . . . . .
344

CONTENTS
xiii
A.6 State Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . .
345
A.7 MATLAB R⃝Code Function Calls
. . . . . . . . . . . . . . . . .
346
A.8 Commonly Used Distributions and Their Densities . . . . . . . .
347
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361


Preface
Random variables and random vectors have been well deﬁned and studied for
over a century. Subsequently, in the history of statistical science, researchers began
considering collections of points together, which gave birth to point process theory
and more recently, to random set theory. This was mainly motivated due to advances
in technology and the types of data that experimenters began investigating, which
in turn led to the creation and investigation of advanced statistical methods able to
handle such data.
In this book we take the reader on a journey through some of the most essential
topics in mathematics and statistics, constantly building on previous concepts, mak-
ing the transition from elementary statistical inference to the advanced probabilistic
treatment more natural and concrete. Our central focus is deﬁning and exploring the
concept of a random quantity or object in diﬀerent contexts, where depending on
the data under consideration, “random objects” are described using random vari-
ables, vectors or matrices, stochastic processes, integrals and diﬀerential equations,
or point processes and random sets.
This view of random objects has not been adequately investigated and pre-
sented in mathematics and statistics textbooks that are out there since they have
mostly concentrated on speciﬁc parts of the aforementioned concepts. This is one
of the reasons why I undertake the task of writing a textbook that would present
the knowledge in a concrete way, through examples and exercises, which is sorely
needed in understanding statistical inference, probability theory and stochastic pro-
cesses. This approach will help the instructor of these topics to engage the students
through problem sets and present the theory and applications involved in a way that
they will appreciate.
Since this monumental task cannot be accomplished in a single textbook, the
theoretical and modeling topics considered have been organized in two texts; this
text is concerned with rudimentary to advanced theoretical aspects of random ob-
jects based on random variables, including statistical inference, probability theory
and stochastic processes. The modeling of these objects and their applications to
real life data is presented in the text Theory and Modeling of Stochastic Objects:
Point Processes and Random Sets (forthcoming, hereafter referred to as TMSO-
PPRS). The latter stochastic objects are a natural extension of random variables
xv

xvi
PREFACE
and vectors and we can think of the TMSO-PPRS text as a natural continuation of
the theory presented herein.
In particular, we present a comprehensive account of topics in statistics in a
way that can be a natural extension of a more traditional graduate course in prob-
ability theory. This is especially true for Chapters 1 and 2, which is a feature that
has been lacking from available texts in probability theory. Another distinguishing
feature of this text is that we have included an amazing amount of material. More
precisely, one would need to use at least one book on real analysis, one in measure
and/or probability theory, one in stochastic processes, and at least one on statistics
to capture just the expository material that has gone into this text.
Being a teacher and mentor to undergraduate and graduate students, I have seen
their attempts to comprehend new material from rudimentary to advanced mathe-
matical and statistical concepts. I have also witnessed their struggles with essential
topics in statistics, such as deﬁning a probability space for a random variable, which
is one of the most important constructs in statistics. This book attempts to introduce
these concepts in a novel way making it more accessible to students and researchers
through examples. This approach is lacking in most textbooks/monographs that one
can use to teach students.
Instructors and researchers in academia often ﬁnd themselves complementing
material from several books in order to provide a spherical overview of the topics
of a class. This book is the result of my eﬀorts over the years to provide comprehen-
sive and compact accounts of topics I had to teach to undergraduate and graduate
students.
Therefore, the book is targeted toward students at the master’s and Ph.D. levels,
as well as academicians in the mathematics and statistics disciplines. Although the
concepts will be built from the master’s level up, the book addresses advanced read-
ers in the later chapters. When used as a textbook, prior knowledge of probability
or measure theory is welcomed but not necessary.
In particular, Chapters 1 and 2 can be used for several courses on statistical
inference with minor additions for any proofs the instructor chooses to further il-
lustrate. In these chapters we summarize over a century and a half of development
in mathematical statistics. Depending on the level of the course, the instructor can
select speciﬁc exercises to supplement the text, in order to provide a better under-
standing and more depth into the concepts under consideration. For example, using
selectively the material and exercises from Chapters 1, 2, 4 and 5, I have taught
several sequences on statistical inference at the University of Missouri (MU), in-
cluding Stat7750/60 and 4750/60 (statistical inference course at the undergraduate
and master’s level), Stat8710/20 (intermediate statistical inference course at the
Ph.D. level) and Stat9710/20 (advanced inference for Ph.D. students).
At the master’s level, it is recommended that the instructor omits advanced top-
ics from Chapter 2, including most of the decision-theoretic topics and the corre-

PREFACE
xvii
sponding proofs of the relevant results. Basic theorems and their proofs, such as
the Bayes or the factorization theorem, should be presented to the students in de-
tail. The proofs of such results are included as exercises, and the instructor can use
the solution manual in order to choose what they deem appropriate to illustrate to
the students.
For example, when teaching a statistical inference course for Ph.D. students, all
concepts presented in Chapters 1 and 2 should be introduced, as well as topics on
asymptotics from Chapter 5. However, certain proofs might be beyond the level of
an intermediate statistical inference course for Ph.D. students. For example, when
it comes to introducing evaluation of point estimators, we may omit the explicit
proof of all parts of the important remark 2.12 and simply present the material, or
the compactness results in Chapter 5, and focus only on the central limit theorems
or Slutsky and Cram´er theorems.
For an advanced course on statistical inference at the Ph.D. level, one would
omit most of the rudimentary results of Chapter 1, and focus on topics from Chap-
ter 2 (inference), Chapter 4 (e.g., characteristic functions), and Chapter 5 (asymp-
totics), including all the important proofs of the theorems and remarks presented in
the text. Once again, the instructor can ﬁnd the solution manual invaluable in this
case, since it will allow them to select the topics they want to present along with
concrete proofs.
Chapters 3-5 can be used to introduce measure theoretic probability to mathe-
matics and statistics graduate students. Some of the proofs should be skipped since
it would take more than one semester to go through all the material. More precisely,
over the past decade when I taught the advanced probability theory course Stat9810
at MU, I had to omit most of the measure theoretic proofs and be quite selective in
the material for a one-semester course. For example, important theorems and their
proofs, like Fubini, Kolmogorov 0-1 Law, Radon-Nikodym or Kolmogorov Three
Series, should be illustrated to the students in detail.
In contrast, one may skip the proofs of the theoretical development of the
Carathodory extension theorem, or omit the proofs of the decomposition theorems
(Chapter 3) and the compactness theorems of Chapter 5. Of course, most of the
important results in measure and probability theory and their proofs are still there
for the inquisitive student and researcher who needs to go deeper. These chapters
are fairly comprehensive and self-contained, which is important for Ph.D. students
that have not had an advanced real analysis course.
Chapter 6 is a fairly comprehensive account of stochastic processes in discrete
time and in particular Markov chains. This material has been used to teach an intro-
ductory course on stochastic processes to both undergraduate and master’s students
(Stat4850/7850), as well as Ph.D.-level students in one semester (Stat 9820, a con-
tinuation of Stat9810). Note that most of the development and exposition of discrete
Markov chains and processes does not require heavy measure theory as presented

xviii
PREFACE
in Chapters 6 and 7, therefore making it accessible to a wide variety of students, in-
cluding undergraduates. A good working knowledge of matrix algebra is required
in this case, which is a requirement for the undergraduate and graduate students
when they take this course. In particular, the instructor simply needs to explain in a
rudimentary way “transition probability measures,” e.g., replace it with the notion
of transition probabilities and matrices, and then the material can be presented to
the students in a non-measure theoretic way.
The material in Chapter 7 has been used to teach stochastic processes in con-
tinuous time to Ph.D. (Stat 9820) and advanced master’s level students, including
topics from Chapter 6, as mentioned above. The instructor can supplement materi-
als from other chapters as they see ﬁt in order to build the mathematical foundations
of the concepts presented as needed. For example, in the beginning of the class we
may conduct a mini review of probability theory and Markov chains before jumping
into continuous time stochastic processes.
As you begin reading, several features that help with the learning process should
immediately draw your attention; each chapter begins with basic illustrations and
ends with a more advanced treatment of the topic at hand. We are exploring and
reconciling, when feasible, both the frequentist and Bayesian approaches to the
topics considered. In addition, recent developments in statistics are presented or
referenced in the text and summary of each chapter.
Proofs for most of the theorems, lemmas and remarks presented in each chapter
are given in the text or are requested as exercises, with the exception of the rudi-
mentary Chapters 1 and 2, where the proofs are requested as exercises only. Proofs
and additional information on the topics discussed can be found in the books or
journal papers referenced at the summary section of each chapter. Of course, the
interested reader can ﬁnd proofs to selected exercises in the supplementary online
material for the book (see website below).
The theorems and results presented in the text can range from easy to compli-
cated, and therefore, we usually follow them with an illustrative remark or example
to explain the new concept. To further help in our understanding of the material
and for quick reference, various topics and complements from mathematics and
statistics are included in an appendix.
The MATLAB R⃝code used for the examples presented along with solutions
to exercises and other material, such as errata, can be found at the book website
https://www.crcpress.com/9781466515208.
There are many people that have contributed, in their own way, to the creation
of this book. I am grateful to the faculty members of the Department of Statistics at
the University of Missouri, USA, for their constructive interactions and discussions
over the years. In particular, special thanks go to my friends and colleagues Christo-
pher Wikle, Scott Holan, Stamatis Dostoglou and Joe Cavanaugh (University of

PREFACE
xix
Iowa), and my friend and mentor Konstantinos Zografos from the Department of
Mathematics, University of Ioannina, Greece. Lastly, my academic advisor, Distin-
guished Professor of Statistics Dipak Dey, Department of Statistics, University of
Connecticut, USA, has been an inspiration to me over the years.
I am grateful to Professors Stamatis Dostoglou, Department of Mathematics,
University of Missouri, USA, Georg Lindgren, Department of Mathematical Statis-
tics, Centre for Mathematical Sciences, Lund, Sweden, and an anonymous re-
viewer, for their invaluable comments and suggestions regarding earlier versions
of the manuscript. Special thanks go to my friend and colleague Distinguished Pro-
fessor Noel Cressie, School of Mathematics and Applied Statistics, University of
Wollongong, Australia, for his support and encouragement over the years as I was
working on the manuscript, as well as for his advice regarding all aspects of the
book, including its title.
Additional thanks go to the hundreds of students for their undivided attention
while they had to take classes from me on these topics and have helped me better
myself through the teaching process. In particular, special thanks goes to all my
graduate students, especially to Jiaxun Chen and Alex Oard. I am also grateful to
Rob Calver, Michele Dimont, Becky Condit and the friends at Chapman-Hall/CRC
for their patience while the manuscript was composed and for their help with the
copy edit process.
Above all, my appreciation and love to my family, my daughters Vaso, Evi and
Christina, my wife Lada and my father Christos, for their unconditional love and
understanding.
I apologize in advance for any typos or errors in the text and I would be grateful
for any comments, suggestions or corrections the kind reader would like to bring to
my attention.
Sakis Micheas
December 2017


List of Figures
4.1 Displaying random objects: (a) random functions, (b)-(d) random
counting measures showing regularity, randomness and clustering
of the points, (e) random discs, and (f) Gaussian random ﬁeld. . .
149
7.1 Brownian motion realizations: (a) univariate, (b) bivariate. . . . .
314
xxi


List of Tables
2.1 Schematic for any hypothesis testing problem along with the
occurrence of the Type I and II errors. . . . . . . . . . . . . . . .
50
2.2 Simulations of Monte Carlo goodness-of-ﬁt tests. We used
L = 100000 predictive samples for n = 10, 50, 100 observed sample
points and the data is simulated from three models Uni f (0, 1),
Gamma(10, 10), and N(−10, 1). We choose λ = 1 for the
hyperparameter, and ppred is provided for four statistics
T1(X) = X(1), T2(X) = X(n), T3(X) = X, and T4(X) = S 2. Based on
these results T1 emerges as the best test statistic in order to assess
the entertained model.
. . . . . . . . . . . . . . . . . . . . . . .
64
4.1 Characteristic functions of commonly used continuous
distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
4.2 Characteristic functions of commonly used discrete distributions.
166
xxiii


List of Abbreviations
a.s.
Almost Surely
BCT
Bounded Convergence Theorem
Be
Beta function
C`adl`ag
Continue `a Droite, Limites `a Gauche (RCLL)
cdf
Cumulative Distribution Function
cf
Characteristic Function
CI
Conﬁdence Interval
CLT
Central Limit Theorem
CR-LB
Cramer-Rao Lower Bound
DCT
Dominated Convergence Theorem
ev.
Eventually
HBM
Hierarchical Bayesian Model
HPD
Highest Posterior Density
iid
Independent and Identically Distributed
i.o.
Inﬁnitely Often
LHS
Left-Hand Side
MAP
Maximum a Posteriori
MCMC
Markov Chain Monte Carlo
MCT
Monotone Convergence Theorem
MLE
Maximum Likelihood Estimator
MG
Martingale
mgf
Moment Generating Function
PEL
Posterior Expected Loss
pgf
Probability Generating Function
xxv

xxvi
LIST OF ABBREVIATIONS
PP
Point Process
RHS
Right-Hand Side
RCLL
Right Continuous Left Limits
SLLN
Strong Law of Large Numbers
sMG
Sub-Martingale
SMG
Super-Martingale
TMSO-PPRS
Theory and Modeling of Stochastic Objects: Point Processes
and Random Sets
UMVUE
Uniformly Minimum Variance Unbiased Estimator
WLLN
Weak Law of Large Numbers
wlog
Without Loss of Generality
w.p.
With Probability

List of Symbols
Q ≪µ
Q is absolutely continuous with respect to µ
B(X)
Borel sets of the space X
DΨ
[0,+∞)
C`adl`ag space: Ψ-valued functions deﬁned on
[0, +∞) with RCLL
M or M(µ∗)
Carath´eodory measurable sets
Cn
x =
n!
(n−x)!x!
Combination: the number of ways we can select x
objects out of n objects
Cn
x1,x2,...,xk =
n!
x1!x2!...xk!
Multinomial Coeﬃcient,
kP
i=1 xi = n,
xi = 0, 1, . . ., n, i = 1, 2, . . ., k
CR
[0,+∞)
Continuous R-valued functions deﬁned on [0, +∞)
↓
Decreasing Sequence Converging to
X
d= Y
X and Y have the same distribution
X ∼fX, FX, QX
X has density fX or cdf FX or distribution QX
x! = 1 ∗2 ∗. . . ∗(x −1) ∗x
Factorial of an integer x
F
Closed subsets of Rp
∀
For every (or for all)
∃
There exist(s)
⇒
Implies
⇔
If and only if
↑
Increasing Sequence Converging to
I(x ∈A) = IA(x)
Indicator function of the set A, 1 if x ∈A, 0 if x < A
µp
Lebesgue measure on Rp
ω1 ∨ω2
max{ω1, ω2}
ω1 ∧ω2
min{ω1, ω2}
xxvii

xxviii
LIST OF SYMBOLS
an = o(bn)
an
bn →0 as n →∞
1:1
One-to-one (function)
hdQ
dµ
i
Radon-Nikodym derivative of Q with respect to µ
R, Rp
Real numbers in 1 and p dimensions
R = R ∪{−∞} ∪{+∞}
Extended real line
R+, R+
0
{x ∈R : x > 0}, {x ∈R : x ≥0}
Q ⊥µ
Q and µ are mutually singular measures
[P], [µ]
With respect to measure P or µ
Z
Integers, {. . . , −2, −1, 0, 1, 2, . . .}
Z = Z ∪{−∞} ∪{+∞}
Extended integers
Z+, Z+
0, Z+
{1, 2, . . .}, {0, 1, 2, . . .}, {0, 1, 2, . . ., +∞}

List of Distribution Notations
Beta
Beta distribution
Binom
Binomial distribution
Cauchy
Cauchy distribution
Dirichlet
Dirichlet distribution
DUni f
Discrete Uniform distribution
Exp
Exponential distribution
F
F distribution
Gamma
Gamma distribution
Geo
Geometric distribution
HyperGeo
Hyper Geometric distribution
InvGamma
Inverse Gamma distribution
W−1
p
Inverse Wishart distribution
N
Univariate Normal distribution
Multi
Multinomial distribution
Np
Multivariate Normal distribution
NB
Negative Binomial distribution
Poisson
Poisson distribution
tn
Student’s t distribution
Uni f
Uniform distribution
Wp
Wishart distribution
χ2
n
Chi-square distribution
xxix


Chapter 1
Rudimentary Models and Simulation
Methods
1.1
Introduction
Statistical modeling involves building mathematical equations (models) that try
to describe a natural phenomenon. The model used typically depends on some un-
known parameters and then statistical inference attempts to infer about the values
of these parameters in order to create the model that would best describe the phe-
nomenon. We think of the phenomenon as an experiment conducted and what we
observe as a possible outcome (or realization). The collection of all outcomes is
called the sample space, denoted by a set Ω, and any subset of Ωis called an event
of the experiment. An event of Ωthat cannot be further decomposed in terms of
other events is called a simple event.
A phenomenon characteristic is some deterministic function X(ω) that depends
on ω ∈Ωand hence it can potentially take diﬀerent values for diﬀerent outcomes
ω. We call this characteristic a random quantity or object. Notice that this non-
deterministic view of the world incorporates deterministic phenomena as special
cases. The experiment can be anything, from ﬂipping a coin to observing radar re-
ﬂectivities of a severe storm system as it propagates over an area. The characteristic
of interest in these cases could be whether we observe tails in the toss of the coin
or the amount of rainfall in the area in the next ten minutes.
If we conduct an experiment many times while keeping all parameters the same,
then the observed values of the characteristic of interest becomes our random sam-
ple often referred to as the data. The main goal in statistical inference is to utilize
random sample values in order to infer about the true value of the parameters used
in the model. In general, the models we ﬁt are used to accomplish one or more of
the following:
1) Fit the data well, so that if the experiment is conducted in the future, the
model proposed describes the values of characteristics well.
1

2
RUDIMENTARY MODELS AND SIMULATION METHODS
2) Capture the evolution of a characteristic (a process). This is the case where
the experiment characteristics of interest evolve over time.
3) Allow us to adequately predict (forecast) the characteristic in future observa-
tion times.
Depending on the type of data we observe we require models for random vari-
ables, vectors and matrices, random integrals and derivatives, random point pro-
cesses, or even random sets and ﬁelds. We use the term random object to refer to
each of these random quantities and our goal is to study each of these types of
random objects.
Example 1.1 (Coin toss) To formulate these ideas, consider an experiment where
we ﬂip a coin, with the possible outcomes described using the simple events
{Heads} or {Tails} and the sample space is the set Ω0 = {Heads, Tails}. A char-
acteristic of interest in this experiment has to be based on these simple events; for
example, we might be interested in the outcome of the toss. To obtain a random
sample, we repeat the experiment and record the outcome each time, e.g., ωi ∈Ω0,
i = 1, 2, . . ., n, denotes the outcome of the ith toss. Hence we can represent the ran-
dom sample as a vector ω0 = (ω1, ω2, . . . , ωn) of 0s and 1s. Now if we consider
the experiment where we toss the coin n times, then its sample space Ω1 consists
of all n-dimensional sequences of 0s and 1s and thus a random sample of size
n from Ω0 is a single possible outcome of this experiment. Therefore, the sam-
ple space for the n-toss experiment can be described using the Cartesian product
Ω1 = Ω0 × · · · × Ω0 =
n×
i=1Ω0.
1.2
Rudimentary Probability
Probability is the essential concept required to help us build from rudimentary
to advanced statistical models. Let 2Ωbe the collection of all events (subsets) of Ω.
Deﬁnition 1.1 Probability
Probability is a set function P : 2Ω→[0, 1] that satisﬁes
(i) P(Ω) = 1,
(ii) P(A) ≥0, ∀A ∈2Ω, and
(iii) for any sequence of disjoint events {Ai}+∞
i=1, we have
P
 +∞
S
i=1 Ai
!
=
+∞
P
i=1 P(Ai).
The collection 2Ωcan be so large that it makes it hard to assign probabilities
to all of its elements (events). Therefore, when it comes to deﬁning probability
(measures) we consider a smaller collection of sets than 2Ω, called σ-ﬁelds (see
deﬁnition 3.2).

RUDIMENTARY PROBABILITY
3
Remark 1.1 (Probability arithmetic) Using the properties of deﬁnition 1.1, we
can build a basic arithmetic over events. Let A, B ∈2Ω.
1. P(A ∪B) = P(A) + P(B) −P(A ∩B).
2. P(A) + P(Ac) = 1, where Ac = Ω∖A, the complement of the set A.
3. If A ⊆B then P(A) ≤P(B).
4. P(∅) = 0, where ∅the empty set.
The following basic deﬁnitions will be used throughout the rest of the book.
Deﬁnition 1.2 Rudimentary deﬁnitions
Let P be deﬁned as in deﬁnition 1.1.
1. Almost surely If P(A) = 1, then we say that A holds almost surely (a.s.) or
almost everywhere (a.e.) or with probability (w.p.) 1.
2. Independence Events A and B are called independent if and only if P(A ∩B) =
P(A)P(B).
3. Conditional probability The conditional probability of A given B, denoted by
P(A|B), is deﬁned by P(A|B) = P(A ∩B)/P(B), provided that P(B) > 0. Note that
if A and B are independent then P(A|B) = P(A).
From a Bayesian viewpoint conditional probability is important since it leads
to Bayes’ theorem.
Theorem 1.1 (Bayes’ theorem) For any events A, B ∈2Ω, with P(B) > 0, we
have P(A|B) = P(B|A)P(A)/P(B).
Next we collect the deﬁnitions of the most basic random objects.
Deﬁnition 1.3 Random variable or vector
A random variable or vector X is a set function from some sample space Ωinto
some space X. We refer to X as the set of values of X.
Although X is always deﬁned as a function X(ω) of simple events ω ∈Ω, as
we will see later we typically ignore this underlying structure and work directly
with the values of the random variable. Note that for random vectors the space X
is a subset of the p-dimensional real line Rp. For now we collect the rudimentary
deﬁnitions required in order to deﬁne basic statistical models.

4
RUDIMENTARY MODELS AND SIMULATION METHODS
1.2.1
Probability Distributions
In order to introduce randomness, we need to assign probabilities to the range
of values entertained by a random object, namely, deﬁne P(X = x), ∀x ∈X. We
consider ﬁrst aggregated probabilities over values of X.
Deﬁnition 1.4 Cumulative distribution function
The cumulative distribution function (cdf) of a random variable X is deﬁned as the
function FX(x) = P(X ≤x) = P({ω ∈Ω: X(ω) ≤x}), x ∈X.
A ﬁrst distinction between random variables is now possible based on the latter
deﬁnition.
Deﬁnition 1.5 Discrete or continuous random variable
A random variable X is called continuous if its cdf FX(x) is a continuous function,
otherwise, it is called discrete.
The following theorem provides the necessary and suﬃcient conditions for a
function to be a cdf.
Theorem 1.2 (Cdf requirements) The function FX(x) is a cdf if and only if the
following conditions hold:
(i) FX(−∞) = lim
x→−∞FX(x) = 0 and FX(+∞) = lim
x→+∞FX(x) = 1, and
(ii) FX(x) is non-decreasing, and
(ii) FX(x) is right-continuous, i.e., lim
ε→0FX(x + ε) = FX(x), ∀x ∈X.
The generalization of the latter deﬁnition to random vectors is immediate.
Remark 1.2 (Inclusion probability) The cdf FX(x) can be viewed as a set function,
since it provides the probability of the event {ω : X(ω) ∈(−∞, x]}. For random
objects being vectors of random variables, namely, X = (X1, X2, . . . , Xp), we deﬁne
the joint cdf similarly by
FX(x) = P(X1 ≤x1, X2 ≤x2, . . . , Xp ≤xp) = P (X ∈A(x)) ,
x = (x1, x2, . . . , xp), xi ∈Xi, i = 1, 2, . . ., p, where A(x) = {y ∈Rp : yi ≤xi, i =
1, 2, . . ., p} and hence we can think of FX(x) as the inclusion probability of the
singleton X in the set A(x). Consequently, if X is any type of random object deﬁned
on some space X, we can unify the deﬁnition of the cdf by deﬁning FX(A) = P(X ⊆
A), for all A ⊆X. For example, if X is a random closed set, a set valued random
object taking values in X = F, the collection of all closed subsets of Rp, then the cdf
of X is deﬁned as the containment functional FX(A) = P(X ⊂A), for all A ∈X. For
more details on this construction see Molchanov (2005, p. 22). We discuss these
types of random objects in the TMSO-PPRS text.

RUDIMENTARY PROBABILITY
5
We now turn to deﬁning “point probabilities” of random variables. Naturally,
we begin with the discrete case.
Deﬁnition 1.6 Probability mass function
The function fX(x) = P(X = x) = P({ω ∈Ω: X(ω) = x}), x ∈X, is called the
probability mass function (pmf) or density of a discrete random object X.
The pmf satisﬁes: (i) fX(x) ≥0, ∀x ∈X, and (ii) P
x∈X fX(x) = 1. Note that for X
discrete FX(x) = P
t≤x f (t), whereas, for X continuous FX(x) is a continuous function.
The deﬁnition of the density in the continuous case needs to be modiﬁed since
P(X = x) = 0. Indeed, using remark 1.1.3 we see that {X = x} ⊆{x −ε < X ≤x},
∀ε > 0, which leads to
P(X = x) ≤P(x −ε < X ≤x) = FX(x) −FX(x −ε),
and therefore
0 ≤P(X = x) ≤lim
ε↓0 [FX(x) −FX(x −ε)] = 0,
by the continuity of FX.
Deﬁnition 1.7 Probability density function
The function fX(x), x ∈X, is called the probability density function (pdf) or den-
sity of a continuous random object X, if it satisﬁes FX(x) =
xR
−∞
fX(t)dt, ∀x ∈X.
The pdf is such that: (i) fX(x) ≥0, ∀x ∈X, and (ii)
R
X
fX(x)dx = 1. The support
of X is the subset S ⊂X for which fX(x) > 0, x ∈S. In both the discrete and
continuous case fX(x) is called the probability distribution or density of the random
object X. The rigorous deﬁnition of the density of a random object will be obtained
via the Radon-Nikodym theorem (see theorem 3.20).
The deﬁnition of independence allows us to write the joint distribution for a
random sample from some model fX(x). We collect the rudimentary deﬁnition for
the bivariate case.
Deﬁnition 1.8 Independence of random variables
Let f (x, y) be the joint density of the random vector (X, Y) and fX(x) and fY(y) the
(marginal) densities of X and Y, respectively. We say that X and Y are independent
if and only if f (x, y) = fX(x)fY(y), for all x ∈X and y ∈Y.
Next, we collect some consequences of the deﬁnitions we have seen thus far,
including the deﬁnition and properties of conditional densities.

6
RUDIMENTARY MODELS AND SIMULATION METHODS
Remark 1.3 (Conditional densities) For any random variables X and Y and event
A we have the following:
1. The conditional cdf of a random variable X given an event A is deﬁned by
F(x|A) = P(X ≤x|A) = P({X ≤x} ∩A)/P(A),
x ∈X, provided that P(A) > 0.
2. Given the joint density f (x, y) of X and Y we deﬁne the conditional density of
X given Y = y by f (x|y) = f (x, y)/ fY(y), x ∈X, when fY(y) > 0, and f (x|y) = 0,
otherwise. If X and Y are independent then f (x|y) = f (x).
3. If X is continuous then by deﬁnition and the fundamental theorem of calculus,
we have f (x) = dF(x)
dx
and f (x|A) = dF(x|A)
dx
. Moreover, for continuous X and Y the
bivariate fundamental theorem of calculus yields f (x, y) =
∂2
∂x∂yF(x, y), at continuity
points of f (x, y).
4. It can be shown that P(A) =
R
X
P(A|X = x)f (x)dx, where P(A|X = x)f (x) =
P(A ∩{X = x}).
When we observe a random sample from some distribution, i.e., n independent
and identically distributed (iid) observations (or realizations) of the random object
X, say, x1, . . . , xn, then owing to the independence property of the random sam-
ple we can easily obtain the joint distribution of the data and utilize it to obtain
estimates of the parameters of the distribution of X. This joint distribution is an
essential element of the classical approach in statistics.
Deﬁnition 1.9 Likelihood function
The likelihood function for a parameter θ of a random sample of size n, x1, . . . , xn,
from some distribution f (x|θ) of a random object X is the joint distribution of the
sample deﬁned as
L(θ|x1, . . . , xn) =
nQ
i=1 f (xi|θ).
An appeal to Bayes’ Theorem for the events {ω ∈Ω: X(ω) = x} and {ω ∈
Ω: Y(ω) = y} yields an important quantity for Bayesian statistics, the posterior
distribution of Y|X.
Deﬁnition 1.10 Posterior distribution
The posterior (or a posteriori) distribution of a random object Y given the random
object X with prior (or a priori) distribution f (x) is deﬁned by
π(y|x) = f (x|y)f (y)/ f (x),
for all x ∈X such that f (x) > 0.

RUDIMENTARY PROBABILITY
7
1.2.2
Expectation
Every probability distribution has features that reveal important characteristics
of a random object, including central tendency and variability. In order to quantify
these features we require the concept of expectation.
Deﬁnition 1.11 Expectation
Let X be a random variable with density f (x), x ∈X and let g be a real-
valued function with domain X. The expected value (or mean) of g(X), denoted
by E(g(X)), is given by
E(g(X)) = P
x∈X g(x)f (x),
if X is discrete and
E(g(X)) =
R
X
g(x)f (x)dx,
if X is continuous. When f (x) is replaced by the conditional distribution of X given
Y = y, for some random variable Y, then the deﬁnition above corresponds to the
conditional expectation of g(X) given Y = y, denoted by E(g(X)|Y = y).
The extension of the deﬁnition of expectation to random vectors and random
matrices is straightforward as we see below.
Remark 1.4 (Expectation deﬁnition extensions) Suppose that the random ob-
ject X is a random vector in Rp. When the function g(.) = [g1(.), . . . , gq(.)]T
is Rq−valued, the deﬁnition of the mean is straightforward, namely, E(g(X)) =
[Eg1(X), . . . , Egq(X)]T. If X = [(Xij)], is a random matrix then we can easily ex-
tend the original deﬁnition above by writing EX = [(EXij)]. Unfortunately, this is
not the case when it comes to more complicated random objects, like random sets.
There are several deﬁnitions that have been proposed over the years that coincide
under certain conditions, but there is no unique way of deﬁning the expectation for
random sets. We study these random objects in more detail in the TMSO-PPRS text.
Several important characteristics about a random vector can be described using
moments and functions of moments.
Deﬁnition 1.12 Moments and functions of moments
Let X and Y be random vectors in Rp. The kth-order non-central moment is deﬁned
by µk = E(XXTX . . . XTX
|            {z            }
k−times
) and the variance-covariance matrix of X is deﬁned as
Var(X) = Cov(X, X) = E(X −µ)(X −µ)T = µ2 −µµT, where µ = µ1 is the mean
vector of X. The covariance between the two random vectors X and Y is deﬁned
as Cov(X, Y) = E(X −µx)(Y −µy)T, where µx = EX and µy = EY. The Pearson

8
RUDIMENTARY MODELS AND SIMULATION METHODS
correlation between two random variables X and Y is deﬁned as
Corr(X, Y) =
Cov(X, Y)
√Var(X)Var(Y)
.
Example 1.2 (Exponential family) Consider the q-parameter exponential family
in its canonical form with pdf given by
f (x|θ) = h(x) exp{θTT(x) −A(θ)},
(1.1)
where θ = [θ1, . . . , θq]T
∈Θ ⊆Rq, the parameters of the model, T(x) =
[T1(x), . . . , Tq(x)]T and h(x) ≥0, with x ∈Rp. First note that
R
Rp
h(x) exp{θTT(x)}dx = exp{A(θ)}
since f (x|θ) is a density so that diﬀerentiating both sides with respect to θ yields
R
Rp
T(x)h(x) exp{θTT(x)}dx = exp{A(θ)}∇A(θ)
(1.2)
and hence
ET(x) =∇A(θ),
(1.3)
where ∇denotes the gradient, i.e., ∇A(θ) =
h
∂
∂θ1 A(θ), . . . ,
∂
∂θq A(θ)
iT . Now taking
the gradient on both sides of (1.2) we obtain
R
Rp
T(x)T(x)Th(x) exp{θTT(x)}dx = exp{A(θ)}∇A(θ)∇A(θ)T + exp{A(θ)}∇2A(θ)
and hence
E(T(x)T(x)T) = ∇A(θ)∇A(θ)T + ∇2A(θ),
(1.4)
where ∇2A(θ) =
h ∂2
∂θiθ j A(θ)
i
the Hessian matrix. Using (1.3) and (1.4) we obtain
Cov(T(x), T(x)) = E(T(x)T(x)T)−ET(x)ET(x)T = ∇2A(θ).
This family is called minimal when neither of the vectors T and θ satisfy a linear
constraint, i.e., there do not exist constants a such that aTT = 0 and constants b
such that bTθ = 0. In addition, if the parameter space contains a q-dimensional
rectangle then the family is said to be of full rank.
Expectations of certain functions of a random object can be very useful in ob-
taining features of the corresponding density, as we see in the following deﬁnitions.
Deﬁnition 1.13 Moment generating functions
Let X be a random vector in Rp. The moment generating function (mgf) mX(t) of
X is deﬁned as mX(t) = EeXT t. The characteristic function (cf) of X is deﬁned as
ϕX(t) = EeiXT t, where i is the imaginary unit.
The following remark shows us how we can obtain moments from the cf of a
random vector.
Remark 1.5 (Obtaining moments from the cf) Note that the cf exists for all val-
ues of t but that is not the case for the mgf in general. Both quantities provide

RUDIMENTARY PROBABILITY
9
non-central moments for random vectors upon taking the gradient repeatedly with
respect to the vector t and setting t to the zero vector. In particular, we can show
that E(XXTX . . . XXT) = i−k∇kϕX(t)|t=0, where ∇k f (x) = ∇∇. . . ∇
|   {z   }
k−times
f (x).
For discrete random variables we have the following.
Deﬁnition 1.14 Probability generating function
Let X be a discrete random variable in X. The probability generating function
(pgf) of X is deﬁned as ρX(a) = E(aX) = P
x∈X P(X = x)ax, for |a| ≤1. Note that
ρX(a) = mX(ln a).
1.2.3
Mixtures of Distributions
Mixture models are useful in modeling the distribution of a random object when
there is indication that there are sub-populations within the main population.
Deﬁnition 1.15 Mixture
Let Y be a random variable with density p(y) and values in Y. The p-mixture
distribution of a random object X is the density deﬁned as
f (x) = P
y∈Y
p(y)f (x|y),
if Y is discrete and
f (x) =
R
Y
p(y)f (x|y)dy,
if Y continuous, where f (x|y) is the conditional distribution of x|y. We say that
f (x|y) is the mixture component for the given y and p(y) are the component proba-
bilities.
Example 1.3 (Mixture of exponential family components) Consider exponen-
tial family components (recall example 1.2) of the form
f j(x|θ j) = h(x) exp{θT
j T(x) −A(θ j)},
where θ j
=
[θ j1, . . . , θ jq]T
∈
Θ j, the parameters of the model, T(x)
=
[T1(x), . . . , Tq(x)]T and h(x) ≥0. Let p = [p1, . . . , pm]T ∈Sm = {p : p j ≥0 and
mP
j=1 p j = 1}, where Sm is the unit simplex and assume that we treat p as the compo-
nent probabilities. We deﬁne the m-component mixture of q-parameter exponential
family components as follows:
f (x|θ) =
mP
j=1 p j f j(x|θ j) =
mP
j=1 p jh(x) exp{θT
j T(x) −A(θ j)},
where θ = [θ1:m, p] ∈Θ =
m×
j=1Θ j × Sm, all the parameters of the model and θ1:m =
{θ j}m
j=1.

10
RUDIMENTARY MODELS AND SIMULATION METHODS
Example 1.4 (Data augmentation for mixture models)
Suppose that p
=
[p1, . . . , pm] ∈Sm, the component probabilities of a mixture model with bivariate
normal components, where Sm is the unit simplex and let
ϕ j(x|µ j, Σj) = |2πΣj|−1/2 exp{−.5(x −µj)TΣ−1
j (x −µ j)},
(1.5)
the density of a N2(µ j, Σj), j = 1, 2, . . ., m. For a datum x the mixture pdf is given
by
f (x|θ) =
mP
j=1 p jϕ j(x|µ j, Σj) =
mP
j=1 p j|2πΣj|−1/2 exp{(x −µ j)TΣ−1
j (x −µj)},
(1.6)
with θ denoting all the parameters of the model, p, µ j and Σj, j = 1, 2, . . . , m. Now
assume that x1, . . . , xn is a random sample from f (x|θ). The likelihood of the sample
is given by
L(θ|x1, . . . , xn) =
nQ
i=1
mP
j=1 p j|2πΣj|−1/2 exp{(xi −µ j)TΣ−1
j (xi −µ j)},
which is an intractable form, that is, both classical and Bayesian methods of esti-
mation for θ cannot handle the product of the sum, in order to provide closed form
solutions for estimators of θ.
However, any mixture model can be thought of as a missing or incomplete
data model as follows; for each datum xi we deﬁne the latent variable vector
zi = (zi1, . . . , zim) (known as the data augmentation or membership indicator vector),
which indicates to which component xi belongs; that is, zij = 1, if the ith observation
comes from the jth component, 0 otherwise, with
mP
j=1 zij = 1, for each i = 1, 2, . . ., n,
with the vectors zi assumed to be independent.
Let Mz = {z = (z1, . . . , zm) : z j ∈{0, 1} and
mP
j=1 z j = 1}, the space of all values
of zi, i = 1, 2, . . ., n. Further assume that P(zij = 1|θ) = p j, so that the joint distribu-
tion of the vector zi is multinomial on one trial, that is f (zi|θ) =pzi1
1 . . . pzim
m , zij = 0
or 1,
mP
j=1 zij = 1 and f (xi|zij = 1, θ) = ϕ j(xi|µj, Σj), for i = 1, 2, . . ., n, j = 1, . . . , m.
Then the completed data (x1, z1), . . . , (xn, zn) has a joint distribution (augmented
likelihood) given by
f (x1:n, z1:n|θ) =
nQ
i=1
mQ
j=1 p
zij
j

ϕj(xi|µj, Σj)
zij .
(1.7)
Notice that upon summing the joint over the zi we obtain the original form of the
mixture model. Indeed
f (xi|θ)
=
P
zi
f (xi, zi|θ) = P
zi∈Mz
mQ
j=1 p
zij
j

ϕj(xi|µ j, Σj)
zij
=
p1ϕ1(xi|µ1, Σ1) + · · · + pmϕm(xi|µm, Σm).
The mixture model with
mP
j=1 zij = 1 yields what is known as the additive mixture
model with data augmentation. There are situations where we might want to allow

THE BAYESIAN APPROACH
11
mP
j=1 zij ≥1 and in this case the mixture model is known as a multiplicative mixture
model with data augmentation.
1.2.4
Transformations of Random Vectors
Suppose that X ∼f (x|θ) and let Y = g(X), where X ∈X and Y ∈Y =
g(X), are p-dimensional, with Yj = g j(X j), j = 1, 2, . . ., p. We are interested in the
distribution of the random vector Y. First note that
fY(y) = P(Y = y) = P({ω : g(X(ω)) = y}),
which provides a generic approach to transformations of random vectors. There are
three basic approaches that follow below.
The cdf approach
This general approach considers the deﬁnition of the joint cdf of Y given by
FY(y) = P(Y1 ≤y1, . . . , Yp ≤yp) = P(g1(X1) ≤y1, . . . , gp(Xp) ≤yp)
which is a probability involving the random vector X.
The pdf approach
Let J(y) =
∂x
∂y
 =

h ∂xj
∂yi
i =

∂g−1
j (yj)
∂yi
, the Jacobian of the transformation
X 7−→Y. Then the joint of the random vector Y is given by
fY(y) = fX(g−1
1 (Y1), . . . , g−1
p (Yp))|J|, y ∈Y,
with |J| the absolute value of the Jacobian.
The mgf or cf approach
This method computes the mgf or cf of the transformed variables, namely,
mY(t) = E

eYT t
= E

eg(X)T t
and ϕY(t) = E

eiYT t
= E

eig(X)Tt
. A direct ap-
plication of this approach yields the distribution of sums of independent random
variables Xi, i = 1, 2, . . . , n. Indeed, letting Y = X1 + · · · + Xn, with Xi having cf
ϕXi(t) and utilizing the independence assumption we obtain
ϕY(ti) = E

eitY
= E

eit(X1+···+Xn)
= E

eitX1
. . . E

eitXn
=
nQ
i=1 ϕXi(ti).
1.3
The Bayesian Approach
Now we discuss some of the basic elements required to perform a Bayesian
analysis based on the posterior distribution. Prior choice is one of the most crucial
tasks in any Bayesian analysis and we begin with the concept of a conjugate prior.
1.3.1
Conjugacy
Conjugacy is the most commonly used approach when selecting a prior distri-
bution for a model parameter.

12
RUDIMENTARY MODELS AND SIMULATION METHODS
Deﬁnition 1.16 Conjugacy
A family of prior distributions Π is said to be a conjugate family for a class of
density functions D, if π(θ|x) is in the class Π for all f ∈D and π ∈Π.
Several classic examples employing conjugacy are presented next.
Example 1.5 (Conjugacy for exponential family) Suppose that a random vec-
tor X is distributed according to the exponential family (recall example 1.2) and θ
is assumed to have prior distribution
π(θ|ξ, a) = exp{ξTθ −aA(θ) −k(ξ, a)}, θ ∈Θ,
with
exp{k(ξ, a)} =
R
Θ
exp{ξTθ −aA(θ)}dθ.
We show that this is the conjugate prior for the exponential family. The posterior
distribution can be written as
π(θ|x, ξ, a) = exp{ξ + T(x)T θ −(a + 1)A(θ) −B(x, ξ, a)},
where
exp{B(x, ξ, a)}
=
R
Θ
exp{ξ + T(x)T θ −(a + 1)A(θ)}dθ
=
exp{k(ξ + T(x), a + 1)},
so that
π(θ|x, ξ, a) = exp
naξ + T(x)T θ −(a + 1)A(θ) −k (ξ + T(x), a + 1)
o
,
and hence the posterior is of the same type as the prior distribution.
Example 1.6 (Conjugate priors for multivariate normal models)
Let x1:n =
{x1, . . . , xn} be observations from a Np(µ, Σ), µ ∈Rp, Σ > 0, n > p and consider the
prior µ|ξ, A ∼Np(ξ, A). Assume that all other parameters are ﬁxed. The posterior
distribution of µ given the data x1:n is obtained from the full posterior distribution
by keeping the terms that involve µ as follows
π(µ|x1:n, Σ, ξ, A) ∝exp
−1
2
−2
 
Σ−1 nP
i=1 xi + A−1ξ
!T
µ + µT(Σ−1+A−1)µ

,
and letting V = (Σ−1+A−1)−1 > 0 and λ = V
 
Σ−1 nP
i=1 xi + A−1ξ
!
we obtain
π(µ|x1:n, Σ, ξ, A) ∝exp
(
−1
2(µ −λ)TV−1(µ −λ)
)
,
and hence µ|x1:n, Σ, ξ, A ∼Np(λ, V), which makes the multivariate normal the con-
jugate prior when we estimate a normal mean µ.
Now turn to the estimation of Σ keeping all other parameters ﬁxed and assume
that Σ|Ψ, m ∼W−1
p (Ψ, m), Ψ > 0, an inverted Wishart distribution with density
π(Σ|Ψ, m) =
1
2mp/2Γp(m/2)|Ψ|m/2|Σ|−(m+p+1)/2 exp
(
−1
2tr(ΨΣ−1)
)
,

THE BAYESIAN APPROACH
13
where
Γp (m/2) = πp(p−1)/4
pQ
i=1 Γ (m/2 −(i −1)/2) ,
(1.8)
the multivariate gamma function. The posterior distribution is obtained from the
full posterior distribution by keeping the terms involving Σ as follows
π(Σ|Ψ, m, µ, x1:n) ∝|Σ|−((m+n)+p+1)/2 exp
(
−1
2tr

(H + Ψ) Σ−1)
,
and hence Σ|Ψ, m, µ, x1:n ∼W−1
p (H + Ψ, m + n) , where H =
nP
i=1(xi −µ)(xi −µ)T, so
that the inverted Wishart is the conjugate prior for the covariance matrix Σ.
When µ and Σ are both unknown we consider the joint prior on µ and Σ via a
conditioning argument (letting A = K−1Σ), as the product of a Np(ξ, K−1Σ) density
with an W−1
p (Ψ, m) density, for some constant K, namely,
π(µ, Σ|ξ, Ψ, m, K)
=

2π
n Σ

−1/2
exp
−1
2(µ −ξ)T
 1
K Σ
!−1
(µ −ξ)

(1.9)
|Ψ|m/2|Σ|−(m+p+1)/2 exp
n
−1
2tr(ΨΣ−1)
o
2mp/2Γp(m/2)
.
Letting C =
nP
i=1(xi −x)(xi −x)T, the full posterior distribution of µ, Σ|ξ, Ψ, m, K, x1:n
can be written as
π(µ, Σ|ξ, Ψ, m, K, x1:n)
∝
|Σ|−((n−1+m)+p+1)/2 exp{−1
2tr

[Ψ + C] Σ−1
−1
2n(x −µ)TΣ−1(x −µ) −K
2 (µ −ξ)TΣ−1(µ −ξ)}.
Consider the part of the exponent that involves µ and write the quadratic forms
involved as
n(x −µ)TΣ−1(x −µ) + K(µ −ξ)TΣ−1(µ −ξ) =
(n + K)

µTΣ−1µ −2µTΣ−1 
n
n + K x +
K
n + K ξ

+ nxTΣ−1x + KξTΣ−1ξ,
and letting ν =
n
n+Kx +
K
n+Kξ, with ννT =
1
n+K(n2xxT + K2ξξT + 2nKxξT), we can
write
n(x −µ)TΣ−1(x −µ) + K(µ −ξ)TΣ−1(µ −ξ) =
(n + K)(µ −ν)TΣ−1(µ −ν) + tr
h
KξξT + nxxT −ννT
Σ−1i
,
so that the posterior distribution is given by
π(µ, Σ|ξ, Ψ, m, K, x1:n) ∝|Σ|−((n−1+m)+p+1)/2 exp{−1
2tr

(Ψ + C) Σ−1
−1
2(n + K)(µ −ν)TΣ−1(µ −ν) −1
2tr

KξξT + nxxT −ννT
Σ−1
}.
Clearly, we can separate a term that involves µ as a function of Σ, along with a term

14
RUDIMENTARY MODELS AND SIMULATION METHODS
that involves Σ only, as follows
π(µ, Σ|ξ, Ψ, m, x1:n) ∝

1
n + K Σ

−1/2
exp
(
−1
2(n + K)(µ −ν)TΣ−1(µ −ν)
)
|Σ|−((n+m)+p+1)/2 exp
(
−1
2tr

BΣ−1)
,
with B = Ψ + C + nK
n+K(x −ξ)(x −ξ)T, since
KξξT + nxxT −ννT =
nK
n + K (x −ξ)(x −ξ)T,
and hence π(µ, Σ|ξ, Ψ, m, K, x1:n) is a Np(ν, (n + K)−1 Σ) density times an W−1
p (B, n+
m) density so that the joint prior considered is indeed the conjugate prior.
Although conjugacy provides mathematical convenience, there are situations
where it leads to priors that are inappropriate from an application point of view, or
they cannot be used to include strong prior information. Such information is usually
provided by experts in the ﬁeld of the application (see remark 1.6.3). Therefore,
we discuss general approaches to prior selection along with some desired prior
properties next.
1.3.2
General Prior Selection Methods and Properties
Several approaches to prior choice are summarized next.
Remark 1.6 (Prior choice) The following approaches can be employed in prior
selection.
1. Prior robustness When minor changes in the prior distribution do not change
the posterior analysis substantially, then the prior chosen is called “robust.” A sim-
ple way to assess robustness with respect to prior choice is as follows; if a prior
distribution π(θ|λ), depends on a parameter λ, we repeat the analysis for diﬀerent
values of λ and record the changes in the features of the posterior distribution of θ,
such as changes in the posterior mean, mode, variance and so forth. Minor changes
indicate robustness to the prior chosen.
2. Subjective priors From a mathematical point of view, conjugacy is a desirable
property for a prior, since it makes it easy to compute the posterior distribution.
Another mathematical property that is of particular interest is that of exchangeabil-
ity, meaning that the prior probability does not change under any permutation of
the elements of the parameter vector θ. Both types of priors are called “subjective”
priors since we impose conditions on the prior for mathematical convenience.
3. Informative vs non-informative priors A prior is called informative if there
exists strong information about the parameter before we conduct the experiment.
Such a priori information is typically the result of previous analyses of the problem
or expert input by non-statisticians. For example, foresters know oﬀhand the gen-
eral dimensions of diﬀerent species of trees and statisticians can incorporate that

THE BAYESIAN APPROACH
15
information in the prior distribution.
On the other hand, if there is no information available then we need non-
informative priors and historically, Laplace was the ﬁrst to use noninformative
techniques to model a prior distribution. The most widely used method is that of
Jeﬀreys’ prior deﬁned as π(θ) ∝[det(IF
X(θ))]1/2 and is possibly improper (see Ap-
pendix deﬁnition A.6 for the deﬁnition of the Fisher information matrix IF
X(θ)).
Note that whenever we use the data to help us create a prior distribution the result-
ing prior is called an Empirical Bayes prior and the corresponding analysis is called
Empirical Bayes.
4. Objective Bayes Over the past few decades there has been an attempt by lead-
ing Bayesians to create “objective” Bayesian procedures, and in particular, regard-
ing prior choice. An objective Bayesian procedure may be regarded as a default
method which can be applied in cases where prior information is sparse or not well
understood or diﬀers among experimenters.
5. Prior selection methods Most methods of prior selection involve obtaining the
extrema of a quantity that depends on the model distribution f (x|θ) and the en-
tertained prior π(θ), that often belongs to a family of distributions. For example,
the Maximum Likelihood approach to prior selection suggests that we obtain the
marginal m(x|λ) =
R
Θ
f (x|θ)π(θ|λ)dθ, for π ∈Γλ = {π : π(θ|λ), λ ∈Λ} and then
maximize m(x|λ) with respect to λ. The resulting prior π(θ|bλ) provides the member
of the family Γλ that maximizes the probability of observing the datum x, where
bλ = arg max m(x|λ).
6. Classes of priors There are many useful classes of priors that can be considered
when conducting a Bayesian analysis. The ε−contamination class is the collection
of priors Γε = {π : π(θ) = (1 −ε)π0(θ) + επc(θ), πc ∈C}, where 0 < ε < 1 and C
is a class of possible contaminations. Other important classes include ΓS = {π : π
is symmetric about θo}, ΓUS = {π : π is unimodal and symmetric about θo} and
Γ+
U = {π : π is unimodal at θo with positive support}.
1.3.3
Hierarchical Bayesian Models
The general structure of Hierarchical Bayesian Models (HBMs) can be de-
scribed as follows:
Stage 1. Data Model: At the highest level we observe the data from some
distribution [data|process, data parameters]; that is, we assume that the observed
data is a result of some known process at known data model parameters.
Stage 2. Process Model: The process that governs the observed data is it-
self dependent on some parameters; that is, it is modeled using some distribution
[process|process parameters].
Stage 3. Parameter Model: The parameters of the data and process

16
RUDIMENTARY MODELS AND SIMULATION METHODS
models are modeled according to some prior distribution [data and process
parameters|hyperparameters].
Stages 2 and 3 can be thought of as the prior stage in the absence of a process. A
natural way to address the eﬀect of a priori information to the posterior distribution
is by considering hierarchical priors, an essential component of HBMs, so that the
eﬀect of the prior distribution on the posterior distribution diminishes the more
stages we include. Indeed, we can think of any problem in statistics in terms of
stages as the following hierarchical model:
X|θ
∼
f (x|θ), x ∈X (Stage 1, model or observation stage)
θ|λ1
∼
π1(θ|λ1), θ ∈Θ (Stage 2, prior stage)
λ1|λ2
∼
π12(λ1|λ2), λ1 ∈Λ1 (Stage 3, hyperprior stage)
λ2|λ3
∼
π23(λ2|λ3), λ2 ∈Λ2 (Stage 4, hyperprior stage)
. . .
Classical statistics recognizes stage 1 as the only valid model upon which we should
base all our inference, while stages 2, 3, 4, and so forth represent the idea behind
hierarchical priors. There are many posterior distributions that we can obtain in
this case, but it should be noted that by construction, the variables at the kth stage
aﬀect and are aﬀected only by variables in the k −1, k and k + 1 stages and are
conditionally independent of variables in any other stages.
Let us restrict to 3 stages for the sake of presentation, assuming that λ2 is ﬁxed.
The full posterior distribution of all parameters given the data is obtained by apply-
ing Bayes’ theorem and utilizing the distributions of each stage, as follows
π(θ, λ1|x) = π(θ, λ1, x)
m(x)
= f (x|θ, λ1)π(θ, λ1)
m(x)
= f (x|θ)π(θ|λ1)π12(λ1|λ2)
m(x)
,
since X is independent of λ1 given θ, where
m(x) =
R
Λ1
R
Θ
f (x|θ)π1(θ|λ1)π12(λ1|λ2)dθdλ1,
the normalizing constant. The full conditional distributions in this case are deﬁned
as the distributions of the parameter θ or hyperparameter λ1, given any other vari-
ables, namely,
π(θ|λ1, x) = f (x|θ)π1(θ|λ1)
m1(x|λ1)
,
and
π(λ1|θ, x) = π(λ1|θ) = π1(θ|λ1)π12(λ1|λ2)
m2(θ)
,
with m1(x|λ1) =
R
Θ
f (x|θ)π1(θ|λ1)dθ and m2(θ) =
R
Λ1
π1(θ|λ1)π12(λ1|λ2)dλ1. Other
posterior distributions of interest include
π(θ|x)
=
R
Λ1
π(θ|λ1, x)π3(λ1|x)dλ1, and
π3(λ1|x)
=
m1(x|λ1)π12(λ1|λ2)
m(x)
,

SIMULATION METHODS
17
with m(x) =
R
Λ1
R
Θ
f (x|θ)π1(θ|λ1)π12(λ1|λ2)dθdλ1 =
R
Λ1
m1(x|λ1)π12(λ1|λ2)dλ1. Note
that a necessary condition for allowing these operations above is that all these den-
sities exist and are nonzero. A classic example of an HBM is presented next.
Example 1.7 (Multivariate normal HBM)
Assume that the ﬁrst (data model)
stage is a multivariate normal model, X|µ, Σ ∼Np(µ, Σ), with µ ∈Rp, Σ > 0,
both unknown and let x1:n = {x1, . . . , xn}, a random sample. For the second stage
consider the joint conjugate prior of µ and Σ as deﬁned in equation (1.9), which
depends on (hyper) parameters ξ, Ψ, m and K. We treat m and K as ﬁxed and hence
require (hyper) priors for ξ and Ψ at the third stage of the hierarchy. Consider a
non-informative, improper, joint prior for ξ and Ψ given by π(ξ, Ψ) ∝1. Therefore,
we have the following hierarchical scheme for the multivariate normal HBM:
X|µ, Σ
∼
Np(µ, Σ), x ∈Rp (Stage 1)
µ|ξ, K, Σ
∼
Np(ξ, 1
K Σ), µ ∈Rp (Stage 2)
(1.10)
Σ|Ψ, m
∼
W−1
p (Ψ, m), Σ > 0 (Stage 2)
ξ, Ψ
∼
π(ξ, Ψ) ∝1, ξ ∈Rp, Ψ > 0 (Stage 3).
In the previous example, we obtained one of the important posterior distributions in
the hierarchy, namely, µ, Σ|ξ, Ψ, m, K, x1:n. Using the second and third stage priors
we obtain the posterior distribution ξ, Ψ|µ, Σ, K, m, by keeping only the terms that
involve ξ and Ψ, as
π(ξ, Ψ|µ, Σ, K, m) ∝π(ξ|µ, Σ, K)π(Ψ|Σ, m)
where we clearly recognize a Np(µ, 1
KΣ) density for ξ|µ, Σ, K and a Wishart
Wp(Σ, m + p + 1) density for Ψ|Σ, m, with density
π(Ψ|Σ, m + p + 1) =
|Σ|−(m+p+1)/2|Ψ|((m+p+1)−p−1)/2 exp
n
−1
2tr(Σ−1Ψ)
o
2(m+p+1)p/2Γp((m + p + 1)/2)
,
for Ψ > 0 and 0 otherwise, where Γp ((m + p + 1)/2) is the multivariate gamma
function. Note that for a random matrix W ∼Wp(Σ, n) with n ≤p, the density
does not exist and we have the so-called singular Wishart distribution, which is
still deﬁned as the distribution of W since its cf exists (ϕW(Θ) = E
h
eitr(WΘ)i
=
Ip −2iΘΣ

−n/2, with Θ a p × p matrix), even if the density does not. Finally, we no-
tice that even though the priors for ξ and Ψ are improper, the posterior distributions
are proper and hence useful for Bayesian analysis.
1.4
Simulation Methods
We brieﬂy discuss some of the most commonly used simulation approaches.
1.4.1
The Inverse Transform Algorithm
If the random variable X has cdf FX(x) then it is easy to see that U = FX(X) ∼
Uni f (0, 1) is a uniform random variable with density fU(u) = I(0 ≤u ≤1), and

18
RUDIMENTARY MODELS AND SIMULATION METHODS
therefore we can think of U as a random variable (random probability). Since x =
F−1
X (u), for a given U = u, the resulting x is nothing but a generated value from a
random variable having cdf FX(x).
Algorithm 1.1 (Inverse transform) The algorithm is as follows:
Step 1: Find the inverse of the cdf FX(x).
Step 2: Generate U ∼Uni f (0, 1) and set X = F−1
X (U) as a generated value from
fX(x).
Example 1.8 (Exponential distribution) Assume that X|θ ∼Exp(θ), with den-
sity fX(x) = θe−θx, x > 0, θ > 0. The inverse transform method is very eﬃcient in
this case, since FX(x|θ) = 1−e−θx, which leads to F−1
X (x) = −θ−1 ln(1−x). Hence, in
order to generate X ∼Exp(θ), generate U ∼Uni f (0, 1) and set X = −θ−1 ln(1−U).
Since −ln(1 −U)
d= −ln U(∼Exp(1)), we can set X = −θ−1 ln U.
Example 1.9 (Gamma distribution) Consider simulating from X ∼Gamma(a,
b), a, b > 0, with density fX(x|a, b) = xa−1e−x/b
Γ(a)ba , x > 0. The cdf FX(x|a, b) cannot
be inverted in this case. For a an integer, there exist iid Yi ∼Exp(1/b) such that
X
d=
aP
i=1 Yi, so that from the previous example we can write Yi = −b ln Ui, where
the Ui are iid Uni f (0, 1) and hence the Yi are iid. Therefore, in order to generate
X ∼Gamma(a, b), we generate independent U1, . . . , Ua as Uni f (0, 1) and then
simply set X = −b
aP
i=1 ln Ui. When a > 0, a real number, we turn to another method,
known as the rejection method.
1.4.2
The Acceptance-Rejection Algorithm
Consider the continuous case for the sake of presentation. Assume that we can
generate a random variable Y with pdf g (the source) and we are interested in gen-
erating values from another random variable X with pdf f (the target distribution),
where X and Y have the same support. The idea behind the Acceptance-Rejection
(or simply Rejection) method is to generate a variable Y ∼g and then accept (or
reject) it as a simulated value from f with probability proportional to f (Y)/g(Y).
Algorithm 1.2 (Rejection) Let c denote a constant such that f (Y)/g(Y) ≤c, for
all y such that g(y) > 0, e.g., c = max
y
{ f (Y)/g(Y)} .
Step 1: Generate Y ∼g and U ∼Uni f (0, 1).
Step 2: If U < f (Y)/(cg(Y)), set X = Y, the generated value from f, otherwise
return to step 1.

SIMULATION METHODS
19
Remark 1.7 (Acceptance and convergence) Let us answer some important ques-
tions regarding this algorithm.
1. At each iteration the probability that we accept the value generated is 1/c (com-
mon for all iterations).
2. The number of iterations until the ﬁrst accepted value is a geometric random
variable with probability of success 1/c. See appendix remark A.11.3 for the def-
inition and properties of the geometric random variable. As a result, the average
number of iterations until we generate a valid value from f is c.
Example 1.10 (Gamma distribution)
Assume that X ∼Gamma(a, b), where
a, b ∈R+, with pdf f (x|a, b) (target pdf). The case of an integer a was illustrated
in the previous example. For the source pdf g(x|µ) we consider an exponential with
mean equal to the mean of the Gamma, i.e., Y ∼Exp(1/ab), where µ = E(Y) = ab,
the mean of the Gamma(a, b) random variable. As a result
h(x) = f (x|a, b)/g(x|µ) = axa−1e−x(a−1)/(ab)
ba−1Γ(a)
,
and therefore the maximum is attained at x0 = ab, so that
c = max
x
{ f (x|a, b)/g(x|µ)} = f (x0|a, b)/g(x0|µ) = aae−a+1/Γ(a).
As a result the algorithm becomes:
Step 1: Generate Y ∼Exp(1/(ab)) and U ∼Uni f (0, 1).
Step 2: If U < ya−1e−y(a−1)/(ab)ea−1/

aa−1ba−1
, set X = Y as the generated value,
otherwise return to step 1.
Example 1.11 (Normal distribution) Assume now that X ∼N(0, 1), so that the
target pdf is given by
f (x) = (2π)−1/2e−x2/2, x ∈R.
First we consider |X| which has pdf
f (x) = 2(2π)−1/2e−x2/2, x > 0,
and we will use an exponential Y ∼Exp(1), as the source distribution g(x) to
initially generate from |X|. We have
f (x)/g(x) =
p
2/πex−x2/2,
and therefore the maximum occurs at x = 1, i.e.,
c = max
x
{ f (x)/g(x)} = f (1)/g(1) =
p
2e/π.
Thus, in order to generate from the absolute value of the standard normal |X| and
since
f (x)/(cg(x)) = exp
n
−(x −1)2/2
o
,
the algorithm becomes:
Step 1: Generate Y ∼Exp(1) and U ∼Uni f (0, 1).
Step 2: If U < exp
n
−(y −1)2 /2
o
, set X = Y, the generated value from |X|, other-
wise return to step 1.

20
RUDIMENTARY MODELS AND SIMULATION METHODS
Step 3: Now in order to obtain a generated value from X ∼N(0, 1), we simply take
X = Y or X = −Y, with equal probability, i.e., generate U1 ∼Uni f (0, 1) and if
U1 ≤.5, set X = −Y, otherwise set X = Y.
Since −ln U ∼Exp(1) and U < exp
n
−(y −1)2 /2
o
if and only if −ln U >
(y −1)2 /2, we may rewrite the steps as:
Step 1: Generate independent Y1, Y2 ∼Exp(1).
Step 2: If Y2 > (Y1 −1)2 /2, then set Y = Y2 −(Y1 −1)2 /2 and proceed to step 3,
otherwise go to step 1.
Step 3: Generate U ∼Uni f (0, 1) and set X = (2I(U ≤.5) −1)Y1, as the generated
value from N(0, 1).
This approach is much faster since we do not have to exponentiate in the condi-
tion in step 2. Moreover, note that the random variables X and Y generated this
way are independent with X ∼N(0, 1) and Y ∼Exp(1). Finally, to generate from
X ∼N(µ, σ2), we generate Z ∼N(0, 1) and take X = µ + σZ.
1.4.3
The Composition Method for Generating Mixtures
This approach is used when the target distribution is a mixture of well-known
distributions. Assume that the target distribution is written as
f (x|θ1, . . . , θm) =
mP
i=1 pi f (x|θi),
where pi are the mixture probabilities with
mP
i=1 pi = 1 and f (x|θi) is the ith mixture
component.
Algorithm 1.3 (Composite method) The general algorithm follows.
Step 1: Generate a value U ∼Uni f (0, 1).
Step 2: If
iP
j=1 p j−1 ≤U <
iP
j=1 p j, i = 1, 2, . . . , m, with p0 = 0, then generate
Xi ∼f (x|θi) and set X = Xi as the realization for the target distribution.
1.4.4
Generating Multivariate Normal and Related Distributions
Consider generating random vectors from a Np(µ, Σ), µ ∈Rp, Σ > 0. When
Σ is singular, i.e., it has a determinant of zero, the density does not exist but the
distribution is still deﬁned as the singular p-variate normal via its cf.
Since we can always generate p independent normal random variables, we de-
ﬁne the normal random vector Z with coordinates that consist of the random sample
Z1, . . . , Zp ∼N(0, 1), i.e., Z = [Z1, . . . , Zp]T ∼Np(0, Ip). To generate from a gen-
eral multivariate normal Np(µ,Σ), we can use the Choleski decomposition of Σ,
namely, there exists an upper triangular matrix C such that Σ = CTC. Finally, we
can easily see that X = µ+CTZ is distributed according to a Np(µ,CT IpC = Σ), the
target distribution.

SUMMARY
21
Assuming that X ∼Np(µ, Ip), we deﬁne χ2 = XTX ∼χ2
p(µTµ), the non-central
χ2 with p degrees of freedom and non-centrality parameter µTµ. Moreover, it can
be shown that the pdf of χ can be written as a Poisson(λ = µTµ) mixture of central
χ2 random variables with p + 2k degrees of freedom, for k = 0, 1, . . ., and E(χ2) =
p +λ. For the forms of the Poisson pmf and χ2 pdf see Appendix A.11.7 and A.8.3.
Now assume that X1, . . . , Xn ∼Np(0, Σ) and deﬁne the random matrix Z =
[X1, . . . , Xn] (with vec(Z) ∼Nnp(0,In⊗Σ), where ⊗denotes the Kronecker product),
n > p, otherwise it is singular. Then if we deﬁne the random matrix W = ZZT =
nP
i=1 XiXT, we have that W ∼Wp(Σ, n), the Wishart distribution of p dimensions,
n degrees of freedom and parameter matrix Σ. Note that E(W) = nΣ. In order to
simulate from an inverted Wishart random matrix A ∼W−1
p (Ψ, n), we simulate
W ∼Wp(Ψ−1, n) and set A = W−1, where E(A) = (n −p −1)−1Ψ−1.
1.5
Summary
Our rudimentary, non-measure theoretic treatment of probability and distribu-
tion theory provides the basics upon which we can start building a general frame-
work for modeling random objects and provide statistical inference. We will discuss
these topics in the next chapter, including point and interval estimation, as well as
hypothesis testing, along with a plethora of texts that have appeared on these topics
(see the summary of Chapter 2).
Compared to books on theoretical topics from statistics, texts on simulation
methods are much fewer and more recent. With the advancement in technology,
especially in the 80s and 90s, investigators were able to utilize the existing theoret-
ical framework and illustrate it using advanced computers. Consequently, methods
such as Monte Carlo, Markov Chain Monte Carlo (MCMC), Metropolis-Hasting,
Gibbs sampling, Importance Sampling, Reversible Jump MCMC and Birth-Death
MCMC have helped experimenters illustrate classical and Bayesian models in real-
life settings. Recent texts involving general simulation methods include Robert and
Casella (2004, 2010) and Thomopoulos (2013). There are many more books on
simulation methods for particular areas, ranging from point process theory and
stochastic geometry to statistical physics; we will refer to these texts later on ap-
propriately. Next we discuss some additional results on the topics presented in this
chapter.
Bayesian considerations
Hierarchical Bayesian Models (HBMs) were ﬁrst introduced in Berliner (1996)
and further studied in Wikle et al. (1998). Detailed reviews on Bayesian robustness
with respect to choice of prior can be found in Berger (1984, 1985, 1994) and
Robert (2007). In terms of prior selection, the investigation of conjugate priors for
exponential families (see example 1.5) can be found in Diaconis and Ylvisaker

22
RUDIMENTARY MODELS AND SIMULATION METHODS
(1979). A uniﬁed framework for the simultaneous investigation of loss and prior
robustness was presented in Micheas (2006).
Objective Bayes is the contemporary name for Bayesian analysis carried out in
the Laplace-Jeﬀreys manner (Jeﬀreys, 1946, 1961). There is an extensive literature
on the construction of objective (or reference) priors; see Kass and Wasserman
(1996), Bernardo and Ramon (1998), Press (2003) and Berger et al. (2009).
Mixture models
Mixture models have their roots in a seminal work by Pearson (1894). In this
article, Pearson was one of the ﬁrst to incorporate the use of mixture models as
well as note some of the issues involved, in particular, estimation and identiﬁability.
Monographs concerning the subject include Everitt and Hand (1981), Titterington
et al. (1985), Lindsay (1995), McLachlan and Peel (2000), Fr¨uhwirth-Schnatter
(2006) and Mengersen et al. (2011).
Mixture models were discarded as possible modeling tools since they intro-
duced major diﬃculties in estimation of their parameters, due to the intractable
form of the likelihood; namely, for a random sample the joint distribution of the
data involves the product of a sum (e.g., example 1.4). Moreover, the problems of
identiﬁability and label switching (Jasra et al., 2005) have caused many sceptics
to question the validity of the latest simulation techniques for mixture parameters.
Authors have suggested various approximations over the years, but the work by
Dempster et al. (1977) on the introduction of data augmentation along with an
EM (Expectation-Maximization) algorithm for estimation has provided an appeal-
ing solution to the problem. We discuss the EM algorithm in the context of point
estimation in Chapter 2, remark 2.8.
In particular from a Bayesian point of view, the Gibbs samplers by Tanner and
Wong (1987) and Diebolt and Robert (1994) were major steps toward building a
hierarchical Bayesian framework to help with estimation of the parameters of a
ﬁnite mixture with a ﬁxed number of components. Sampling methods in the vari-
able number of components case include Reversible Jump MCMC (RJMCMC)
and Birth Death MCMC (BDMCMC, see Section 6.7.3). For the development, ap-
plication, evaluation and algorithms for problems involving such methods we re-
fer to Green (1995), Richardson and Green (1997), Stephens (2000), Capp´e et al.
(2003), Robert and Casella (2004), Jasra et al. (2005), Dellaportas and Papageor-
giou (2006), Micheas et al. (2012) and Micheas (2014). The development in these
papers is with respect to additive mixture models. For the deﬁnition and applica-
tions of multiplicative mixture models, see Heller and Ghahramani (2007), Fu and
Banerjee (2008), and Dey and Micheas (2014).
Non-parametric approaches have been very successful as well, using for in-
stance Dirichlet process (DP) models or inﬁnite mixtures. See for example Sethu-
raman (1994), Escobar and West (1995), Wolpert and Ickstadt (1998), Scricciolo
(2006), Kottas and Sanso (2007) and Ji et al. (2009).

SUMMARY
23
Multivariate analysis
The study of random vectors and matrices (arranged collections of univariate
random variables) is aptly termed multivariate analysis, with the multivariate nor-
mal distribution and its related distributions playing a prominent role. Standard
texts illustrating simulation, distribution theory, theoretical foundations and appli-
cations of multivariate analysis techniques include Johnson (1987), Fang and Zhang
(1990), Anderson (2003), Muirhead (2005) and Johnson and Wichern (2007).
Multivariate analysis techniques do not involve only statistical inference, but
also general multivariate analysis methods such as classiﬁcation or clustering of
observations, tests of independence, principal component analysis, canonical cor-
relation analysis, factor analysis and specialized distribution theory, e.g., distribu-
tions of eigenvalues and eigenvectors of random matrices. See Appendix A.5 for
additional topics.
Simulation methods
In the last three decades there has been an explosion in the statistics literature re-
garding computation and simulation methods. As technology advanced, researchers
were able to visualize what theory was suggesting about Markov chains (see Chap-
ter 6), Markov random ﬁelds (Besag, 1974) and, in general, Bayesian computa-
tion and simulation of random vectors. For instance, calculation of the normalizing
constant in closed form has always been a diﬃcult (if not impossible) problem for
Bayesians before computers came along.
After the mid 80s, when computing resources were advancing, the illustration of
Simulated Annealing and Gibbs Sampling to image restoration, by Geman and Ge-
man (1984), opened up a new area in Bayesian statistics and computational statis-
tics in general. Other pioneering papers in advancing general statistical simulation
methods include Tanner and Wong (1989), Gelfand and Smith (1990), Diebolt and
Robert (1994), Geyer and Møller (1994), Green (1995) and Stephens (2000). We
have collected the basic ideas and methods behind simulating the simplest of ran-
dom objects, a random variable or vector. Advanced simulation methods will be
investigated in later chapters once we have collected the theory involved.
Statistical paradoxes
In general, a paradox is an argument that produces an inconsistency within logic
or common sense. Some paradoxes have revealed errors in deﬁnitions previously
assumed to be rigorous and have caused axioms of mathematics and statistics to
be re-examined. In statistics, there are paradoxes with frequentist or Bayesian rea-
soning that have sparked great discussions, with reasonable arguments from both
sides. Discussions on statistical paradoxes with examples can be found in Robert
(2007).

24
RUDIMENTARY MODELS AND SIMULATION METHODS
1.6
Exercises
Probability, distribution theory and expectation
Exercise 1.1 Prove all statements of remark 1.1.
Exercise 1.2 (Total probability) For any partition {Bi} of the sample space Ωand
event A ∈2Ω, show that P(A) = P
i P(A|Bi)P(Bi).
Exercise 1.3 Prove Bayes’ theorem.
Exercise 1.4 Prove theorem 1.2.
Exercise 1.5
Assume that X1, X2
iid∼Exp(1/θ). Show that Y = X1 + X2 and X1/Y
are independent.
Exercise 1.6
Let U ∼Uni f (0, 1) and Y|U = u ∼Binom(n, p = u), where the
pmf of the binomial is given in appendix remark A.11.1. Find the (unconditional)
distribution of Y.
Exercise 1.7
Show that FX(x) + FY(y) −1 ≤FX,Y(x, y) ≤
p
FX(x)FY(y), for two
random variables X and Y with joint cdf FX,Y and marginal cdfs FX and FY.
Exercise 1.8
Assume that X1, . . . , Xn is a random sample from some model
with density f (x) and cdf F(x). The ordered values of the sample, denoted by
X(1) ≤· · · ≤X(n), are known as the order statistics. Show that (i) fX(1)(x) =
n[1 −F(x)]n−1 f (x), and (ii) fX(n)(x) = n[F(x)]n−1 f (x).
Exercise 1.9 Consider two iid random variables X and Y. Find P(X < Y).
Exercise 1.10 Let Xi
iid∼N(0, 1), i = 1, 2, 3, 4. Show that Y = X1X2 −X3X4 follows
the Laplace distribution with pdf f (y) = 1
2e−|y|, y ∈R.
Exercise 1.11 Suppose that X1, X2
iid∼Exp(θ). Find the distribution of W = X1−X2.
Exercise 1.12 Consider Xi
iid∼f (x|θ1, θ2) = 1/h(θ1, θ2), θ1 ≤x ≤θ2, i = 1, 2, . . ., n,
where θ1 < θ2 are real numbers. Find the distribution of Q =  X(n) −X(1)
 /h(θ1, θ2).
Exercise 1.13 Assume that X1, . . . , Xn is a random sample from some model with
density f (x) and cdf F(x) and let X(1) ≤· · · ≤X(n) denote the order statistics. Show
that the joint distribution of X(r) and X(s) is given by
fx(r),x(s)(u, v) = Cn
r−1,s−r−1,n−s f (u)f (v) [F(u)]r−1 [F(v) −F(u)]s−r−1 [1 −F(v)]n−s ,
u < v, 1 ≤r < s ≤n.
Exercise 1.14
Assume that Xi
iid∼f (x|θ1, θ2) = g(x)/h(θ1, θ2), θ1 ≤x ≤θ2, i =
1, 2, . . ., n, where θ1 < θ2 are real numbers. Show that Qrs = h(X(r), X(s))/h(θ1, θ2) ∼
Beta(s−r, n−s+r +1), 1 ≤r < s ≤n, where the density of a Beta random variable
is given in Appendix A.12.1.
Exercise 1.15 Let Xi
iid∼f (x|θ), i = 1, 2, . . ., n. Show that E (X1/ (X1 + · · · + Xn)) =
1/n.
Exercise 1.16
If X is a random variable with mgf mX(t), show that P(X > a) ≤
inf{mX−a(t), t > 0}, for all a ∈R.

EXERCISES
25
Exercise 1.17 If X ∼N(0, 1) then show that P(X > a) ≤exp{−a2/2}, for all a > 0.
Exercise 1.18 Assume that X follows a (singular) normal distribution with mean
0 and covariance matrix Σ =

4
2
2
1
.
(i) Prove that Σ is of rank 1.
(ii) Find a so that X = aY and Y has a nonsingular univariate normal distribution
and give the density of Y.
Exercise 1.19
Consider X ∼Np(b + Γz,σ2Ip), where b = (β, β, . . . , β)T, z =
(z1, z2, . . . , zp)T, Γ = diag(γ), with β, γ and z constants with
pP
i=1 zi = 0. Find the joint
distribution of X = 1
p
pP
i=1 Xi and U =
pP
i=1 ziXi/
 pP
j=1 z2
j
!
, where
pP
j=1 z2
j > 0.
Exercise 1.20 (Chebyshev inequality) Let X be a random variable with E(X) =
µ. Then for any ε > 0, we have P(|X −µ| ≥ε) ≤Var(X)/ε2.
Exercise 1.21 (Markov inequality) If P(X ≥0) = 1 and P(X = 0) < 1 then
for any r > 0, P(X ≥r) ≤E(X)/r, with equality if and only if P(X = r) = p =
1 −P(X = 0), 0 < p ≤1.
Exercise 1.22 (Jensen inequality) For any random variable X, if g(x) is a convex
function, then Eg(X) ≥g(EX), with equality if and only if for every line a+bx that
is tangent to g(x) at x = E(X), we have P(g(X) = a + bX) = 1.
Simulation and computation: use your favorite language to code the functions
Exercise 1.23
Simulate the toss of a 12-sided perfect die 10000 times and com-
pute the frequency of each side. What values do you expect to see for the relative
frequencies?
Exercise 1.24
Write a function that, given an integer number N, would generate
values from the discrete uniform pmf that takes values N-1, N and N+1. Generate
100 values from this distribution and compute their average.
Exercise 1.25 Simulate 1000 values from a discrete random variable with: P(X =
1) = .3, P(X = 2) = .2, P(X = 3) = .35, P(X = 4) = .15. Produce a histogram of
the generated sample.
Exercise 1.26
Consider a Poisson random variable with pmf p(x|λ), truncated at
a value k > 0 (in the function you build, make sure we can specify both λ and
k as arguments). Call the truncated Poisson random variable X. How would you
simulate 1000 values from this X? Note that the pmf of X is nothing but p(x|λ, k) =
p(x|λ)/P(X ≤k), x = 0, 1, ..., k.
Exercise 1.27
A pair of fair dice is to be continually rolled until all the possible
outcomes 2,3,...,12, have occurred at least once. Write a function that would simu-
late values from this discrete random variable that stops once we get all outcomes
to appear at least once and displays the frequencies of each outcome in a plot.

26
RUDIMENTARY MODELS AND SIMULATION METHODS
Exercise 1.28
Give a method for generating a random variable having density
f (x) = exp(2x)I(x ≤0) + exp(−2x)I(x > 0). Simulate 10000 values from f (x) and
produce a pdf plot of these values (not a histogram).
Exercise 1.29
Simulate 1000 values from the continuous random variable with
pdf f (x) = 30(x2 −2x3 + x4), 0 ≤x ≤1. Also produce the pdf plot over these values
(not a histogram, i.e., order the simulated values x1, . . . , x1000, and plot xi vs f (xi)
using solid lines to connect the points).
Exercise 1.30 Generate 10000 values from a central t-distribution with 50 degrees
of freedom and compute their mean.
Exercise 1.31 Write a function that would generate and plot a histogram of 10000
values from a Pareto distribution with pdf f (x|θ) = aθa/xa+1, x ≥θ, scale parameter
θ > 0 and known shape parameter α > 0. Run your function for θ = 10 and a = 1.
Exercise 1.32
Write a function that simulates 2000 realizations from a random
variable having cdf F(x) = (1/3)(1 −exp(−2x) + 2x)I(0 < x < 1) +(1/3)(3 −
exp(−2x))I(x > 1) and produce their pdf plot (not a histogram). Repeat for F(x) =
1−exp(−axb), a, b, x > 0 (use a = .1, b = 2 when you run the function and produce
the pdf plot).
Exercise 1.33 Give three methods for generating from a continuous random vari-
able with cdf F(x) = xn, 0 ≤x ≤1, where n is a ﬁxed positive integer. Discuss the
eﬃciency of the algorithms.
Exercise 1.34 Prove the two statements of remark 1.7.
Exercise 1.35
Devise and implement a rejection sampler in order to generate
values from a mixture of two beta distributions: 0.7 ∗Beta(4, 2)+0.3 ∗Beta(1.5, 3).
Use your rejection method to simulate 1000 observations from this mixture, plot
the results in a histogram and add the true density to the histogram.
Exercise 1.36 Assume that X follows a mixture of univariate normal distributions,
i.e.,
f (x|p1, p2, µ1, µ2, σ2
1, σ2
2) = p1φ(x|µ1, σ2
1) + p2φ(x|µ2, σ2
2),
where φ(x|µ, σ2) denotes the N(µ, σ2) density. Write a function that would take,
as arguments, the mixture parameters θ = [p1, p2, µ1, µ2, σ2
1, σ2
2] and r, an integer
representing the number of realizations requested. The function should perform the
following tasks:
(i) Check if p1 + p2 = 1 and report an error and exit if not satisﬁed.
(ii) Plot the mixture density (not a histogram).
(iii) Simulate r realizations from the mixture and return them as an r × 1 vector.
(iv) Run your function with r = 1000 and for the following mixtures:
(a) θ = [.5, .5, 0, 4, 1, 2], (b) θ = [.2, .8, 1, 5, 1, 1], (c) θ = [.1, .9, 3, 4, .3, .3]

Chapter 2
Statistical Inference
2.1
Introduction
Suppose that we entertain a model f (x|θ) for a random object X taking values
in X, where θ ∈Θ ⊂Rk are the parameters of the model (typically unknown) and
assume that we have a random sample x1, . . . , xn, of size n from this model, namely
we have random variables Xi
iid∼f (x|θ) and Xi(ωi) = xi, i = 1, 2, . . ., n, for some
outcome ωi ∈Ω. Let X = (X1, . . . , Xn) be the random vector and x = (x1, . . . , xn)
the corresponding data vector.
Since any function T(x) of the data is an estimator for a parameter θ, there is
an uncountable number of estimators we can use and hence we need to somehow
reduce the collection of functions to those that are useful. This can be accomplished
in a classical or Bayesian framework, either by considering methods that give us
forms of these functions or by requiring certain properties to be satisﬁed by a func-
tion of the data.
In addition, we need to evaluate estimators and ﬁnd the best, in some sense,
among them. The area of statistics that attempts to unify methods of evaluating
estimators is called decision theory and its basic elements will be collected ﬁrst.
2.2
Decision Theory
When we choose an estimator T(x) to estimate the true value of a parameter
θ we are making a decision so that a = T(x) ∈A is called a decision rule or an
action (the term action is typically used in the absence of data), where A is the
action space, which consists of all possible actions. Decision theory provides us
with the necessary tools in order to obtain decision rules, as well as evaluate their
eﬀectiveness.
Remark 2.1 (Decision theory components) Any decision theoretic problem con-
sists of the following elements.
1. Loss function The primary component is that of the loss function, denoted by
L(θ, a), which quantiﬁes the error or loss incurred by making a speciﬁc decision
27

28
STATISTICAL INFERENCE
and is deﬁned as a function of the action a and the parameter θ. Standard choices
include the square error or quadratic loss (SEL) L(θ, a) = (a −θ)T(a −θ) and the
weighted square error loss (WSEL) Lw(θ, a) = (a −θ)TD(a −θ), for some matrix
D, that may depend on θ and is typically positive deﬁnite.
Other useful loss functions, in the univariate case in particular, include the
absolute value loss L(θ, a) = |a−θ| or the linear exponential (LINEX) loss Lb(θ, a) =
exp{b(a −θ)} −b(a −θ) −1, b , 0. A rich class of loss functions can be obtained
by considering the ϕ-divergence Dϕ(θ, a) between an entertained model f (x|a) and
the true model f (x|θ), deﬁned by
Dϕ(θ, a) =
R
X
ϕ (f (x|a)/ f (x|θ)) f (x|θ)dx,
(2.1)
with ϕ a real, continuous convex function on [0, +∞) that satisﬁes the condi-
tions 0ϕ(0/0) = 0 and 0ϕ(t/0) = t lim
u→+∞ϕ(u)/u, t > 0. For more details on
these conditions see Vajda (1989), Cressie and Pardo (2000) and Stummer and
Vajda (2010). Additional assumptions, such as ϕ(1) = 0, can be introduced in
order to have Dϕ(θ, a) take its minimum value at 0, since in general we have
ϕ(1) ≤Dϕ(θ, a) ≤ϕ(0) + lim
u→+∞ϕ(u)/u, with strict inequality if ϕ′′(u) > 0 (see
for example Lemma 2.1 of Micheas and Zografos, 2006).
2. Frequentist risk The next component that incorporates the classical paradigm
in a decision theoretic framework is the risk function. The frequentist risk for a
decision rule T is deﬁned by
R(θ, T) = Eθ [L(θ, T(X))] ,
and is the function of θ that quantiﬁes the penalty incurred (on the average), when
using T(X) to estimate θ. Clearly, a decision rule is good if its risk is small. For
example, in the univariate case note that under SEL the risk function becomes the
Mean Square Error (MSE) deﬁned by
MS E = R(θ, T) = E [T(X) −θ]2 = Var(T(X)) + [BIAS ]2 ,
(2.2)
where BIAS = E(T(X)) −θ.
3. Bayes risk The Bayesian paradigm can be incorporated in a decision theoretic
framework as follows; the Bayes risk of a decision rule T with respect to a prior
distribution π on Θ is deﬁned by
r(π, T) = Eπ[R(θ, T)],
(2.3)
and provides the average (with respect to the prior π) expected loss incurred when
estimating θ using T. The dependence of this measure on the prior π renders it quite
subjective and experimenters often conduct robustness studies with respect to the
choice of π.
Based on the frequentist and Bayes risks, we can built several principles to help
us evaluate estimators.
Remark 2.2 (Decision theory principles) The following principles can be used in
order to evaluate estimators.

DECISION THEORY
29
1. Admissibility principle A decision rule T is called R-better than a rule T∗if
R(θ, T) ≤R(θ, T∗), ∀θ ∈Θ, with strict inequality for at least one θ. More generally,
a decision rule T is admissible if it is R-better than any other rule T∗. If there is a
rule T∗that is R-better than T, then T is called inadmissible. Finding the admissible
estimator can be a daunting task since we need to compare functions across all θ in
the parameter space.
2. Bayes risk principle A decision rule T1 is preferred to a rule T2 if r(π, T1) <
r(π, T2), with the Bayes risk function r(π, .) given by (2.3). If Tπ minimizes r(π, T),
then Tπ is called the Bayes rule. When the prior π is improper, Tπ is called the
generalized Bayes rule.
A decision rule T ∗
n (that may depend on n) is called extended Bayes if it is a
Bayes rule in the limit, meaning that there exists a sequence of priors {πn} such
that lim
n→∞r(πn, T ∗
n) < lim
n→∞r(πn, T), for any other decision rule T.
3. Minimax principle A decision rule T1 is preferred to a rule T2 if sup
θ∈Θ
R(θ, T1) <
sup
θ∈Θ
R(θ, T2), that is, T1 has smaller maximum (over all θ) average loss than T2. A
decision rule T∗is called minimax if T∗is such that
sup
θ∈Θ
R(θ, T∗) = inf
a∈Asup
θ∈Θ
R(θ, a),
i.e., T∗minimizes maximum risk for all a ∈A.
Instead of working with the Bayes risk to ﬁnd the Bayes rule, it is often much
easier to work with the posterior distribution as we see next.
Remark 2.3 (Bayesian expected loss and the Bayes rule) If π∗(θ) is the believed
density of θ at the time of decision making then the Bayesian Expected Loss of an
action a is deﬁned as
ρ(π∗, a) = Eπ∗[L(θ, a)].
If we choose π∗to be the posterior density π(θ|x) with respect to some prior π(θ),
once the data has been observed, then ρ(π∗, a) is called the Posterior Expected Loss
(PEL) of the action a. If a∗minimizes ρ(π∗, a), then a∗is called the Bayes action.
As a consequence, instead of minimizing the Bayes risk with respect to T, we make
the problem easier by minimizing the PEL. Indeed, given a prior π and assuming x
and θ are continuous for simplicity, we write the Bayes Risk as
r(π, T) = Em(x) h
Eπ(θ|x) (L(θ, T(x)))
i
,
with
π(θ|x) = f (x|θ)π(θ)/m(x),
(2.4)
the posterior distribution of θ|x, where
m(x) =
R
Θ
f (x|θ)π(θ)dθ,
(2.5)
the normalizing constant (the marginal or unconditional distribution of x). Thus,

30
STATISTICAL INFERENCE
minimizing the Bayes risk is equivalent to minimizing the PEL, which is typically
easier to accomplish in closed form as well as numerically.
2.3
Point Estimation
Now we consider investigating point estimators, i.e., the rule T(x) we use to es-
timate θ is a singleton in X. We present both the classical and Bayesian approaches,
starting with the former.
2.3.1
Classical Methods
First we collect the classical methods of point estimation and their properties.
Based on a point estimator bθ = bθ(x) of a parameter θ, classical statistics suggests
that the best possible model from which these observations arise is simply f (x|bθ).
Consequently, in order to generate new values for this random object X we need to
simulate from f (x|bθ). The following remark summarizes two of the classic methods
in the literature.
Remark 2.4 (Estimation methods) The following methods can be used in a clas-
sical setting in order to obtain forms of estimators bθ.
1. Method of moments The ﬁrst method is dating back at least to Karl Pearson in
the late 1800’s and is perhaps the ﬁrst method of ﬁnding point estimators; it equates
the sample with the theoretical non-central means µ j, thus creating a system of
equations involving the data and the parameters. Recall that the sample kth non-
central moment is deﬁned by mk =
1
n
nP
i=1 xk
i , k = 1, 2, . . ., based on the random
sample Xi(ωi) = xi, i = 1, 2, . . ., n, from L(θ|x) = f (x|θ), where θ = (θ1, . . . , θk) is
the parameter vector. The method of moments estimator eθ = eθ(x) of a parameter
vector θ is deﬁned as the solution to the system of equations m j ⋍µj, j = 1, 2, . . ., k.
2. Maximum likelihood estimator The next method provides estimators by at-
tempting to maximize the probability of observing the acquired sample and is
based on the likelihood principle, namely, if x and y are two samples with L(θ|x) =
k(x, y)L(θ|y), for all θ ∈Θ, then any conclusion about θ drawn from x and y should
be identical. As a result, we deﬁne the maximum likelihood estimator (MLE)
bθ = bθ(x) of a parameter vector θ as the value θ ∈Θ that maximizes the likeli-
hood function, that is, bθ = arg max L(θ|x).
Next we summarize some of the most commonly sought-after properties of
estimators.
Remark 2.5 (Properties of estimators) We discuss some basic properties of esti-
mators in a classical framework.
1. Suﬃciency and minimal suﬃciency A statistic T(X) is a suﬃcient statistic for
a parameter vector θ (or the family of distributions P = { f (x|θ) : θ ∈Θ}) if the
conditional distribution of the sample X given T(X) does not depend on θ. Conse-

POINT ESTIMATION
31
quently, the suﬃciency principle suggests that any inference about θ should depend
on X only through T(X). Note that a suﬃcient statistic provides reduction of the
data without any loss of information about θ. Finding suﬃcient statistics is easy
using the factorization theorem (Fisher-Neyman): T(X) is a suﬃcient statistic for θ
if and only if there exist functions g(t|θ) and h(x) such that
f (x|θ) = h(x)g(T(x)|θ).
A statistic T(X) is called the minimal suﬃcient statistic for a parameter vector
θ if for any other suﬃcient statistic T ∗(X) we have T ∗(X) = g(T(X)), for some func-
tion g. Suﬃcient and minimal suﬃcient statistics are not unique; e.g., any monotone
function of a suﬃcient statistic is also suﬃcient and any one-to-one function of a
minimal suﬃcient statistic is also minimal. It can be shown that T(X) is a mini-
mal suﬃcient statistic if it satisﬁes the following: for two samples x and y the ratio
f (x|θ)/ f (y|θ) does not depend on θ if and only if T(x) = T(y).
2. Ancillary estimator A statistic T(X) is called ancillary for a parameter vector
θ if its pdf does not depend on θ. Clearly, an ancillary statistic does not contain any
information about θ so that intuitively a minimal suﬃcient statistic, which summa-
rizes all information about the parameter, is independent of any ancillary statistic.
However, when we combine ancillarity with other statistics with certain properties,
we obtain valuable information that can be used to conduct inference about θ.
3. Unbiased estimator An estimator T(X) of g(θ) is called unbiased if and only if
E(T(X)) = g(θ).
(2.6)
If such T exists, then g(θ) is called U-estimable. There are many functions T(X) that
can be unbiased, although solving equation (2.6) can be diﬃcult. Instead, we obtain
an estimator using one of the aforementioned methods and then try to transform it
into an unbiased estimator.
4. Complete statistic A statistic T(X) is called complete for θ (or the family of
distributions P = { f (x|θ) : θ ∈Θ}), if for any function h we have
Eθ[h(T(X))] = 0 =⇒Pθ(h(T(X)) = 0) = 1,
(2.7)
that is, the only unbiased estimator of zero that is a function of T is the function
zero a.e. When h is restricted to bounded functions, then T is called boundedly
complete. It can be shown that if T(X) is boundedly complete, suﬃcient and ﬁnite
dimensional then T is minimal suﬃcient.
5. Pivotal quantity A function Q(X, θ) of the data X and the parameter θ is called
a pivotal quantity if the distribution of Q(X, θ) does not depend on θ. Good pivotal
quantities are those that can be inverted, in the sense that we can solve the equation
Q(X, θ) = q with respect to θ, for a given value q. Pivotal quantities always exist;
indeed, if Xi
iid∼F(x|θ), i = 1, 2, . . ., n, where F is the cdf, then it can be shown that
Q(X, θ) = −
nP
i=1 ln F(Xi|θ) ∼Gamma(n, 1) ≡1
2χ2
2n,

32
STATISTICAL INFERENCE
independent of θ and Q(X, θ) is pivotal. Unfortunately, nothing guarantees that
Q(X, θ) will be invertible. For example, if Xi
iid∼Exp(θ) ≡Gamma(a = 1, b =
1/θ), i = 1, 2, . . ., n, with density f (x|θ) = θe−θx, x, θ > 0, then Q(X, θ) =
−
nP
i=1 ln

1 −e−θxi
, which cannot be inverted.
6. UMVUE An estimator T(X) is called the Uniformly Minimum Variance Un-
biased Estimator (UMVUE) for a function of the parameter g(θ), if T(X) has the
smallest variance among all unbiased estimators of g(θ).
We collect some additional results on MLEs and their properties below.
Remark 2.6 (Obtaining MLEs) Some comments are in order about the MLE.
1. Since the likelihood function L(θ|x) and the log-likelihood l(θ|x) = ln L(θ|x) =
nP
i=1 ln f (xi|θ) achieve their maximum at the same point θ, it is often more convenient
to maximize l(θ|x). The likelihood equation (also known as an estimating equation)
we need to solve is either ∇L(θ|x) = 0 or ∇l(θ|x) = 0. Once we obtain bθ we need
to show that the Hessian matrix ∇2L(θ|x)|θ=bθ or ∇2l(θ|x)|θ=bθ is negative deﬁnite, in
order to ensure that this valuebθ is where we achieve the maximum.
2. If bθ is the MLE of θ then for any invertible function g(θ) the MLE of g(θ) is
g(bθ).
3. The MLE of θ could be unique or not exist or there could be an inﬁnite number
of MLEs for θ.
4. If the MLE is unique then it is always a function of the suﬃcient statistic.
In many cases the MLE cannot be computed analytically (in closed form) since
we cannot solve the likelihood equation. In this case, methods such as the Netwon-
Raphson can provide a numerical solution. We collect the method in the following
remark.
Remark 2.7 (Newton-Raphson method) The Newton-Raphson method utilizes a
simple idea to help us obtain the root of a univariate (or multivariate) equation
f (x) = 0, with respect to x. The underlying idea is to approximate the graph of
the function f (x) by tangent lines. More precisely, the Newton-Raphson formula
consists geometrically of extending the tangent line on the graph of f (x) at a current
point xk, until it crosses zero, then setting the next guess xk+1 to the value where the
zero-crossing occurs. The method is as follows:
Suppose that we take the Taylor expansion of f (x) at the (starting) point x0.
Then we can write
f (x) = f (x0) + f ′(x0)(x −x0) + 1
2 f ′′(x0)(x −x0)2 + . . . ,
and keeping only the ﬁrst two terms we obtain
f (x) ≃f (x0) + f ′(x0)(x −x0).

POINT ESTIMATION
33
Therefore, setting f (x) = 0 to ﬁnd the next approximation x1 of the zero of f (x) we
ﬁnd:
x1 = x0 −f (x0)
f ′(x0).
Clearly, f ′(x0) must not be 0 for this to be deﬁned, i.e., we do not want to obtain ex-
trema of f (x) at this x0. Thus, letting x = x0, some starting value (that approximates
a root of the equation), we use the following recursive formula
xk+1 = xk −f (xk)
f ′(xk),
k = 0, 1, . . ., to obtain an xk closer and closer to the value of a root of f (x). Note
that the tangent line to the graph of f (x) at (x0, f (x0)) is given by the equation
f (x0) −f (x) = f ′(x0)(x0 −x).
Eventually we expect to hit a point xk that makes f (xk) = 0. The problem with
this method is that we cannot allow f ′(xk) to become 0. In other words, the method
will not work in an area about an extrema of f (x). In order to use this method to
compute the MLE numerically, we typically take the function f to be ∇l(θ|x).
The Newton-Raphson method is easily generalized to the multivariate case.
Indeed, assume that x = [x1, . . . , xp]T ∈Rp and consider solving the system of
equations f(x) = 0, where f(x) = [f1(x), . . . , fp(x)]T. Deﬁne the Jacobian matrix by
J(x) =

∂f1(x)
∂x1
· · ·
∂f1(x)
∂xp
...
...
...
∂fp(x)
∂x1
· · ·
∂fp(x)
∂xp

and assume that J−1(x) exists. Then an approximation
to the root of f is given by the recursive formula
xn+1 = xn −J−1(xn)f(xn),
for n = 0, 1, . . ., where x0 is an initial guess for the root.
An alternative approach to ﬁnding MLEs is via an appeal to the EM algorithm,
which was introduced by Dempster et al. (1977) in order to estimate missing data
models, e.g., recall example 1.4 where the membership indicators represent infor-
mation that is missing (latent, censored or unknown variables). The following re-
mark presents the general method and how it can be used in order to obtain MLEs.
Remark 2.8 (EM algorithm) Consider an experiment where the model distribu-
tion contains missing or incomplete information. Such models can be represented
in terms of what is known as demarginalization; that is, assume that the entertained
model f (x, z|θ) contains a latent variable z, with the marginal distribution (likeli-
hood) given by
g(x|θ) =
R
Z
f (x, z|θ)dz.
(2.8)
Although we have a random sample from the (marginal) distribution g(x|θ) we are
really interested in the data-augmented density f (x, z|θ), i.e., we know the marginal
but we want to make statements about θ based on the joint distribution f (x, z|θ) (de-
marginalization), including ﬁnding the MLE of θ. Note that working with f (x, z|θ)

34
STATISTICAL INFERENCE
is typically easier.
In particular, suppose that Xi
iid∼g(x|θ), i = 1, 2, . . ., n, and we want to com-
pute the MLE of θ, based on the likelihood L(θ|x) =
nQ
i=1 g(xi|θ) = g(x|θ), which is
hard to maximize. The idea behind the EM algorithm is to overcome the diﬃculty
of maximizing L(θ|x), by exploiting the form of (2.8) and solving a sequence of
easier maximization problems whose limit is the MLE of L(θ|x). The EM can be
thought of as the predecessor of the data augmentation MCMC algorithm where
we replace simulation with maximization.
Now assume that we augment the data x with the latent variables z, with the
completed data distribution of (x, z) being f (x, z|θ), so that the conditional distribu-
tion of the latent variables z given the data x and the parameter θ is given by
π(z|θ, x) = f (x, z|θ)
g(x|θ) .
Therefore, solving for the marginal yields
g(x|θ) = f (x, z|θ)
π(z|θ, x),
(2.9)
and we can use the latter equation can be used to connect the complete-data likeli-
hood L(θ|x, z) = f (x, z|θ), with the observed-data likelihood L(θ|x). Indeed, for any
θ0, (2.9) can be written as
l(θ|x) = ln L(θ|x) = Eπ(z|θ0,x) [ln L(θ|x, Z)] −Eπ(z|θ0,x) [ln π(Z|θ, x)] ,
and since the term Eπ(z|θ0,x) [ln π(Z|θ, x)] is maximized at θ = θ0, in order to max-
imize l(θ|x), we only need consider the term Eπ(z|θ0,x) [ln L(θ|x, Z)] . We collect the
EM algorithm next. See Robert and Casella (2004) for details on convergence is-
sues, as well as modiﬁcations and improvements of the EM algorithm.
Algorithm 2.1 (Expectation-Maximization) Consider the setup of the previous
remark. Assume that at iteration k the MLE of θ is bθ
(k). The following iterations
are conducted until a ﬁxedbθ
(k+1) is obtained.
Step 1 (E-step): Compute the expectation Q

θ|bθ
(k), x

= Eπ(z|θ(k),x) [ln L(θ|x, z)] .
Step 2 (M-step): Maximize Q

θ|bθ
(k), x

with respect to θ and obtain bθ
(k+1) =
arg max
θ Q

θ|bθ
(k), x

.
Example 2.1 (Exponential family)
Recall the setup of example 1.2. The log-
likelihood (ignoring the constant terms) is given by
l(θ|x) = θTT(x) −A(θ) =
qP
j=1 θ jT j(x) −A(θ).
(2.10)
Then assuming that we have augmented the data x using some unobserved z, so

POINT ESTIMATION
35
that (x, z) has log-likelihood of the form (2.10), we have
Q

θ|bθ
(k), x

= Eπ(z|θ(k),x) [ln L(θ|x, z)] =
qP
j=1 θ jT (k)
j (x) −A(θ),
where T (k)
j (x) = Eπ(z|θ(k),x) h
T j(x, z)
i
. Now maximize Q

θ|bθ
(k), x

with respect to θ to
obtain the complete-data MLE by solving the equations
0 = ∂
∂θ j
Q

θ|bθ
(k), x

= T (k)
j (x) −∂
∂θ j
A(θ)
j = 1, 2, . . ., q, and therefore, the maxima θ(k+1) are given by solving the equations
Eπ(z|θ(k),x) h
T j(x, Z)
i
= T (k)
j (x),
j = 1, 2, . . ., q, i.e., the observed equals the expected.
The following remark summarizes four main results we can use in order to
obtain UMVUEs.
Remark 2.9 (Obtaining UMVUEs) Choosing the SEL as the loss function, if T(X)
is unbiased then from equation (2.2) we have R(θ, T) = Var(T(X)) and T(X) is the
UMVUE if it minimizes the risk function of T. Note that if the UMVUE exists
then it is unique. We discuss four basic theorems for ﬁnding the UMVUE. Let
X ∼f (x|θ), θ ∈Θ.
1. Let T(X) be an estimator such that E(T 2(X)) < +∞. Let also U(X) denote any
unbiased estimator of zero, such that E(U2(X)) < +∞. Then T(X) is the UMVUE
for its mean E(T(X)) = g(θ) if and only if Eθ(T(X)U(X)) = 0, for all estimators U
and θ ∈Θ.
2. Rao-Blackwell Let T(X) be suﬃcient for θ. Let δ(X) be an unbiased estima-
tor of g(θ) and assume that the loss function L(θ, a) is a strictly convex func-
tion of a. If δ has ﬁnite expectation and risk R(θ, δ) < ∞, then the estimator
U(T) = E[δ(X)|T], that depends on the data only through T, has smaller risk than
δ, i.e., R(θ, U) < R(θ, δ) unless δ(X) = U(T) w.p. 1.
The Rao-Blackwell theorem provides R-better estimators U by conditioning
an unbiased estimator δ on the suﬃcient statistic T. Assume further that T is com-
plete. Then, under SEL conditioning an unbiased estimator δ of g(θ) on T, we obtain
a UMVUE estimator U of g(θ), since E(U) = E(E[δ(X)|T]) = E(δ(X)) = g(θ).
3. Lehmann-Scheﬀ´e Let T(X) be complete-suﬃcient for θ. Then every U-
estimable function g(θ) has one and only one unbiased estimator that is a function
of T.
4. Cramer-Rao lower bound Assume that Xi
iid∼f (x|θ), θ ∈Θ ⊂Rk and that the
regularity conditions (see Appendix A.4) hold. Let T(X) be an estimator such that
Eθ(T) = g(θ) and E(T 2) < +∞. Then Eθ
 ∂ln f(x|θ)
∂θi

= 0 and
Var(T) ≥aT[IF
x (θ)]−1a,
(2.11)
where aT
= [a1, . . . , ak], ai =
∂g(θ)
∂θi , i = 1, 2, . . ., k and IF
x (θ) is the Fisher

36
STATISTICAL INFERENCE
information. This lower bound is known as the Cram´er-Rao lower bound (CR-
LB). The CR-LB does not guarantee the existence or tell us how to compute the
UMVUE.
Before discussing speciﬁc examples, we obtain some ﬁnal remarks about point
estimators and their properties.
Remark 2.10 (Point estimators and their properties) The following are straight-
forward to prove.
1. Exponential families Let X be distributed according to a full rank q-parameter
exponential family (example 1.2). Then the statistic T is minimal suﬃcient and
complete for θ.
2. Basu If T is complete-suﬃcient for the family of distributions P = { f (x|θ) :
θ ∈Θ}, then any ancillary statistic V is independent of T. A direct consequence of
Basu’s theorem is that an ancillary statistic cannot be complete.
3. Generalizing the CR-LB When estimating g(θ)T = [g1(θ), . . . , gk(θ)], θ ∈Θ ⊂
Rp, we can generalize the CR-LB using
CR-LB = D(θ)[IF
x (θ)]−1D(θ)T,
where D(θ) =
h∂gj(θ)
∂θi
i
. Consequently, we can look for an unbiased estimator T =
[T1, . . . , Tk]T of g(θ), such that Var(T)=CR-LB and then T becomes the UMVUE.
4. Attaining the CR-LB Consider the univariate case for simplicity. The CR-LB
is achieved if and only if
∂
∂θ ln
nQ
i=1 f (xi|θ) = k(θ, n) U(x) −g(θ) ,
and in this case U(X) is the UMVUE of g(θ).
Next we present several examples illustrating the aforementioned methods of
point estimation.
Example 2.2 (Uniform distribution)
Suppose that Xi
iid∼
Uni f (0, θ), i
=
1, 2, . . ., n, with pdf fx(x) =
1
θI[0,θ](x) and cdf Fx(x) =
x
θI[0,θ](x), θ > 0, so that
the likelihood is given by
L(θ|x) = f (x|θ) = θ−n nQ
i=1 I[0,θ](xi) = θ−nI[T(x),+∞)(θ),
where T(X) = X(n) = max
i Xi, the largest order statistic. The distribution of T is
given by
fX(n)(t) = ntn−1θ−nI[0,θ](t).
(2.12)
Clearly, since L(θ|x) is a decreasing function of θ, the maximum is attained at bθ =
T(x). In contrast, using the method of moments we equate the sample with the

POINT ESTIMATION
37
theoretical mean to obtain the estimatoreθ = 2
n
nP
i=1 Xi. Now we write the joint as
f (x|θ) = θ−nI[x(n),+∞)(θ) = h(x)g(T(x)|θ),
with h(x) = 1 and g(T(x)|θ) = θ−nI[x(n),+∞)(θ) and using the factorization theorem
the suﬃcient statistic is the same as the MLE, namely, T(X) = X(n). In order to ﬁnd
the minimal suﬃcient statistic, we consider the ratio
r = f (x|θ)
f (y|θ) = I[x(n),+∞)(θ)
I[y(n),+∞)(θ),
which is constant in θ if and only if x(n) = y(n), so that X(n) is also minimal suﬃ-
cient. Using equation (2.12), we apply the pdf transformation approach to obtain
the distribution of Q = Q(X, θ) = X(n)/θ = T/θ. The Jacobian of the transformation
T 7−→Q is given by dT
dQ = θ and hence
fQ(q|θ) = θ fT(θq) = θn(qθ)n−1θ−nI[0,θ](qθ) = nqn−1I[0,1](q),
which is a Beta(n, 1) distribution that does not depend on θ and hence Q is a pivotal
quantity. Clearly, E  X(n)/θ = E(Q) = n/(n + 1), which leads to an unbiased esti-
mator (n + 1)X(n)/n for θ. Now consider the LHS of equation (2.7) for an arbitrary
function h, namely, Eθ[h(T(X))] = 0, which implies that
θR
0
h(t)tn−1dt = 0,
and upon diﬀerentiating with respect to θ both sides of the latter equation we obtain
h(θ)θn−1 = 0, θ > 0, which is satisﬁed only if h(t) = 0, for all t > 0. Thus T = X(n)
is also complete. Now consider all unbiased estimators of θ based on the complete-
suﬃcient statistic T, that is, Eθ[h(T)] = θ, so that
θR
0
h(t)tn−1dt = θn+1/n.
Diﬀerentiating with respect to θ on both sides we obtain
h(θ)θn−1 = (n + 1)θn/n,
and hence all unbiased estimators based on T must satisfy h(T) = (n+1)T/n, so that
U = (n + 1)T/n is the unique unbiased estimator that is also complete-suﬃcient,
i.e., U is the UMVUE.
Example 2.3 (A statistic that is not complete) Consider a random sample Xi ∼
Uni f (θ −1/2, θ + 1/2), i
=
1, 2, . . . , n, θ
∈
R. Here T
=
(X(1), X(n)) is
minimal suﬃcient but T is not complete since X(n) −X(1) is ancillary with
Eθ
 X(n) −X(1) −(n −1)/(n + 1) = 0, ∀θ ∈R and hence we have a function of
T that is an unbiased estimator of zero, but it is not the function zero.
Example 2.4 (Poisson distribution) Let Xi
iid∼Poisson(θ), i = 1, 2, . . ., n, θ > 0
and T(X) =
nP
i=1 Xi, so that
f (x|θ) = θT(x)e−nθ/
nQ
i=1 xi! = exp{T(x) ln θ −nθ}/
nQ
i=1 xi!,

38
STATISTICAL INFERENCE
which can be written in canonical form (letting λ = ln θ), as
f (x|λ) = exp{λT(x) −neλ}/
nQ
i=1 xi!,
and hence we have a full rank one-parameter exponential family (example 1.2)
with A(λ) = neλ. Consequently, T is complete-suﬃcient for λ (and hence for θ,
since the deﬁnitions of suﬃciency and completeness are unaﬀected by one-to-one
transformations on the parameter), with E(T) = dA(λ)
dλ
= neλ = nθ, which makes
T/n the UMVUE for θ. Now note that P(X1 = 0) = e−θ and letting δ(X) = 1, if
X1 = 0 and 0, if X1 , 0, then E(δ) = 1P(X1 = 0) + 0P(X1 , 0) = e−θ, so that δ is an
unbiased estimator of e−θ, but it is not the UMVUE since it is not a function of the
complete-suﬃcient statistic. Using Rao-Blackwell we know that U(t) = E(δ|T = t)
becomes the UMVUE. Since
nP
i=1 Xi ∼Poisson(nθ) and
nP
i=2 Xi ∼Poisson((n −1)θ),
we obtain after some algebra
U(t) = E(δ|T = t) = ((n −1)/n)t ,
and hence U(T) = ((n −1)/n)T is the UMVUE for e−θ.
Example 2.5 (Exponential family) Consider a full rank one-parameter exponen-
tial family (recall example 1.2). We can write
ln f (x|θ) = ln h(x) + θT(x) −A(θ),
and for a random sample xi, i = 1, 2, . . ., n, from f (x|θ), we obtain
∂
∂θ ln
nQ
i=1 f (xi|θ) = n
 1
n
nP
i=1 T(xi) −dA(θ)
dθ
!
.
From remark 2.10, with k(θ, n) = n, we have that U(X) = 1
n
nP
i=1 T(Xi) is the UMVUE
of g(θ) = dA(θ)
dθ .
Example 2.6 (Multivariate normal) Let x1:n = {x1, . . . , xn} be a random sample
from a Np(µ, Σ), µ ∈Rp, Σ > 0, n > p, with joint density
f (x1:n|µ, Σ) = |2πΣ|−n/2 exp
(
−1
2
nP
i=1(xi −µ)TΣ−1(xi −µ)
)
,
(2.13)
and considering the exponent we can write
−1
2
nP
i=1(xi −µ)TΣ−1(xi −µ)
=
−1
2tr
 
Σ−1
" nP
i=1 xixT
i
#!
+(Σ−1µ)T
nP
i=1 xi + n
4(Σ−1µ)T
 
−1
2Σ−1
!−1
Σ−1µ.
Let θ = (θ1, θ2) =

Σ−1µ, −1
2Σ−1
and write (2.13) as
f (x1:n|µ, Σ)
=
exp{θT
1 T1 + tr(θ2T2) + (n/4)θT
1 θ2θ1
−(np/2) ln(2π) + (n/2) ln (| −2θ2|)},
which is a member of the exponential family with h(x) = 1, A(θ) = (np/2) ln(2π)

POINT ESTIMATION
39
−(n/2) ln (| −2θ2|) −θT
1 θ−1
2 θ1/4 and T = [T1, T2] =
" nP
i=1 xi,
nP
i=1 xixT
i
#
is the complete-
suﬃcient statistic. Now take the exponent once more and add and subtract x =
1
n
nP
i=1 xi, to obtain
nP
i=1(xi −µ)TΣ−1(xi −µ) =
nP
i=1(xi −x)TΣ−1(xi −x) + n(x −µ)TΣ−1(x −µ),
(2.14)
since
nP
i=1(xi−x) = 0. Based on equation (2.14), we can clearly see that the likelihood
L(µ, Σ|x1:n) = f (x1:n|µ, Σ) is maximized for bµ = X. Now using the mgf approach
we can easily show that x ∼Np(µ, 1
nΣ), so that E(X) = µ and hence X is the
UMVUE for µ. Moreover, letting y = √nΣ−1/2(x −µ) ∼Np(0, Ip), we have that
n(x −µ)TΣ−1(x −µ) = yTy ∼χ2
p, a χ2 random variable with p degrees of freedom,
where Σ−1 = Σ−1/2Σ−1/2 and Ip is the p × p identity matrix. Turning to estimation
of Σ, in order to maximize the log-likelihood function with respect to Σ we write
ln L(bµ, Σ|x1:n) = −(np/2) ln(2π) −(n/2) ln |Σ| −(1/2)tr(Σ−1A),
where A =
nP
i=1(Xi −X)(Xi −X)T =
nP
i=1 XiXT −nXX
T, a random matrix that is a
function of the complete-suﬃcient statistic and considering the maximum of the
function f (Σ) = −nlog|Σ| −tr(Σ−1A), Σ > 0, we can show that it is achieved at
bΣ = 1
nA, the MLE of Σ. Noting that Var(X) = E(XXT) −E(X)E(X)T, we can write
E(A) =
nP
i=1 EXiXT −nEXX
T = (n −1)Σ,
and hence S =
1
n−1A is the UMVUE for Σ, which is diﬀerent than the MLE. In
addition, it can be shown that A ∼Wp(Σ, n −1), with density
π(A|Σ, m) =
|Σ|−(n−1)/2|A|((n−1)−p−1)/2 exp{−1
2tr(Σ−1A)}
2(n−1)p/2Γp((n −1)/2)
,
for A > 0 and 0 otherwise, where Γp ((n −1)/2) is the multivariate gamma function.
2.3.2
Bayesian Approach
Now we discuss Bayesian methods of point estimation.
Remark 2.11 (Bayesian estimation considerations) All methods created in a
Bayesian framework are based on the posterior distribution π(θ|x).
1. Suﬃcient statistic When calculating the posterior distribution it is convenient
to condition on the suﬃcient statistic T instead of the whole sample, that is, using
the factorization theorem, we can write
π(θ|T(x)) = π(θ|x) ∝g(T(x)|θ)h(x)π(θ) ∝g(T(x)|θ)π(θ).
(2.15)
2. Posterior propriety Posterior propriety is important since if the posterior
π(θ|x) is not a proper distribution (it does not integrate to one), we cannot apply
a Bayesian analysis. When the prior π(θ) is proper then the posterior will always be
a proper distribution since the marginal m(x) exists and is ﬁnite.

40
STATISTICAL INFERENCE
3. Generalized MLE The generalized MLE of θ is the largest mode of the poste-
rior distribution, also known as the Maximum a Posteriori (MAP) estimator. Note
that when we do not have any information about the parameter before the exper-
iment, then we could, in theory, consider a ﬂat, non-informative prior π(θ) ∝1,
which is possibly improper, i.e., a priori we place the same weight on all possible
values of the parameter. Then, since π(θ|x) ∝f (x|θ), the generalized MLE becomes
the classical MLE. Note that any central tendency measure of the posterior distri-
bution can be thought of as a point estimator of θ, including the mean and median
of π(θ|x).
Example 2.7 (Bayes rule) Suppose that X|θ ∼f (x|θ), θ ∈Θ and we entertain
a prior distribution π(θ), so that the posterior distribution is π(θ|x) ∝f (x|θ)π(θ).
Depending on the choice of loss function L(θ, a) the Bayes rule becomes a feature
of the posterior distribution. Indeed, under SEL L(θ, a) = (θ−a)2, the PEL becomes
PEL = Eπ(θ|x) (L(θ, a)) = Eπ(θ|x)(θ2|x) −2aEπ(θ|x)(θ|x) + a2,
with dPEL
da
= −2Eπ(θ|x)(θ|x) + 2a and d2PEL
da2
= 2 > 0, for all actions a, so that the PEL
is minimized at the Bayes rule T(X) = Eπ(θ|x)(θ|X), the posterior mean. Similarly,
under absolute loss L(θ, a) = |a −θ|, we can show that the Bayes rule is the median
of the posterior distribution.
2.3.3
Evaluating Point Estimators Using Decision Theory
We turn to the evaluation of the estimators we have seen thus far and how they
relate to each other. An important class of estimators are those with constant risk.
Deﬁnition 2.1 Equalizer rule
A rule T ∗is called an equalizer if it has constant frequentist risk, i.e., R(θ, T ∗) = c,
∀θ ∈Θ, for some constant c.
The following remark summarizes some of the classic results that can help us
evaluate estimators.
Remark 2.12 (Evaluating estimators) We summarize the important results that
relate admissible, Bayes and minimax rules. The proofs of many of these theorems
are simpliﬁed under an exponential family model. We can show the following.
1. Every admissible rule will be either Bayes or Extended Bayes.
2. An admissible rule with constant risk is minimax.
3. A unique minimax rule is admissible.
4. Any unique Bayes rule is admissible.
5. Unbiased estimators T(X) of g(θ) cannot be Bayes rules under SEL, unless
T(X) = g(θ) w.p. 1.

POINT ESTIMATION
41
6. Minimax rules are not necessarily unique or have constant risk or need not be
admissible or exist.
7. An equalizer rule which is Bayes or Extended Bayes is minimax.
Example 2.8 (Decision theoretic estimators of a normal mean)
Suppose that
X|θ ∼N(θ, 1), θ ∈R, choose the SEL L(θ, a) = (θ −a)2 as the loss function of
the problem and consider decision rules of the form Tc(X) = cX. Then the frequen-
tist risk is obtained as
R(θ, Tc) = Ex|θ(L(θ, a)) = Ex|θ((θ −cX)2) = c2 + (1 −c)2θ2.
Since for c > 1, R(θ, T1) = 1 < c2 + (1 −c)2θ2 = R(θ, Tc), the rules Tc are inadmis-
sible for c > 1. Now suppose that θ ∼N(0, τ2), a prior distribution on θ. Then the
Bayes risk with respect to this prior is given by
r(π, Tc) = Eπ[R(θ, Tc)] = Eπ[c2 + (1 −c)2θ2] = c2 + (1 −c)2τ2,
which is minimized as a function of c at c0 =
τ2
1+τ2. Thus Tc0(X) =
τ2
1+τ2 X, is the
unique Bayes rule with respect to this prior and hence (by remark 2.12.4) admissi-
ble. In fact, one can prove that Tc is admissible for 0 ≤c ≤1. By remark 2.12.2,
the rule T1(X) = X, is admissible with constant risk and hence minimax.
We present an important result below that can be used to connect admissibility
and Bayes rules, namely Blyth’s lemma. First, deﬁne the complete class of decision
rules.
Deﬁnition 2.2 Complete class of decision rules
A class C of rules is said to be complete if for every rule that is not a member of
C, there exists a rule from C, that is R-better.
Lemma 2.1 (Blyth) Consider a nonempty open set Θ ⊂Rp. Assume that the
decision rules with continuous risk form a complete class and that for a continuous
risk rule T0, there exists a sequence of {πn} of (possibly improper) prior distributions
such that
(i) r(πn, T0) is ﬁnite for all n;
(ii) for every nonempty open set C ⊂Θ, there exists K > 0 and N such that for
every n ≥N, πn(C) ≥K; and
(iii) lim
n→+∞[r(πn, T0) −r(πn, T ∗)] = 0, where T ∗is the Bayes rule with respect to πn.
Then the decision rule T0 is admissible.
There are two foundational elements from a decision theoretic point of view,
that involve the concepts of utility and randomized decision rules. The utility func-
tion is deﬁned as the opposite of the loss function and as such, it enjoys a diﬀerent
interpretation, in the sense that it allows us to talk about rewards in choosing a cer-
tain action, instead of just the loss incurred. Moreover, a randomized decision rule
T ∗(x, .) is, for each x, a probability distribution on the action space A, with the in-

42
STATISTICAL INFERENCE
terpretation that if x is observed, T ∗(x, A) is the probability that an action in A ⊂A,
will be chosen. In no-data problems, a randomized decision rule, also called a ran-
domized action, is again a probability distribution on A. An immediate application
of this concept can be seen in randomized test procedures, for hypothesis testing
problems in a decision theoretic framework (see Section 2.5.3).
Another important topic of decision theory involves the so-called shrinkage es-
timators. James and Stein (1961) deﬁned an estimator for the multivariate normal
mean problem, that is R-better than T0(x) = x under quadratic loss for p ≥3, by
T JS (x) =

1 −(p −2) /xTx

x.
(2.16)
This estimator has a strange behavior as x gets near 0. The term

1 −(p −2) /xTx

becomes negative and even goes to −∞as xTx goes to zero, but
T JS (x) remains R-better than T0(x). Further improvements (in terms of risk) based
on modiﬁcations of T JS have appeared in the literature (Robert, 2007, p. 98), in-
cluding the general form of the admissible estimator for p ≥3. All such estimators
are called shrinkage estimators since they tend to shrink x toward 0.
2.3.4
Convergence Concepts and Asymptotic Behavior
Point estimators are obtained based on a random sample X1, . . . , Xn, of size n,
from some distribution f (x|θ). As n increases, the sample approaches the whole
population and the estimator should approach the true value of the parameter. In
theory, we could increase the sample size to inﬁnity, but in reality we deal with
ﬁnite samples. Investigating the case n →+∞provides, at the very least, useful
approximations for estimators obtained based on ﬁnite samples. We discuss the
basic deﬁnitions of convergence for a sequence of random variables X1, . . . , Xn, . . . ,
along with the Laws of Large Numbers below. We revisit these concepts under a
more rigorous framework in the later chapters.
Deﬁnition 2.3 Convergence types
Let {Xn}+∞
n=1 be a sequence of random variables.
1. Convergence in probability We say that Xn converges in probability to a ran-
dom variable X, if for every ε > 0,
lim
n→∞P(|Xn −X| < ε) = 1.
We write Xn
p→X.
2. Almost sure convergence We say that Xn converges almost surely (a.s.) to a
random variable X, if
P(lim
n→∞Xn = X) = 1.
We write Xn
a.s.
→X or say that Xn converges to X w.p. 1 or a.s.

POINT ESTIMATION
43
3. Weak convergence If Xn has cdf Fn(x), then we say that Xn converges in distri-
bution (or weakly or in law) to a random variable X, with cdf F(x), if
lim
n→∞Fn(x) = F(x),
for all continuity points x of F(x). We write Xn
w→X (or Xn
d→X or Xn
L→X). It
is sometimes convenient to denote weak convergence in terms of the cdfs involved
by writing Fn
w→F.
Note that it can be shown that Xn
a.s.
→X ⇒Xn
p→X ⇒Xn
w→X (see exercise
5.4). Now we introduce an additional desirable property for a point estimator from
a classical perspective, that of consistency.
Deﬁnition 2.4 Estimator consistency
An estimator Tn(X) of a parameter θ, based on a random sample X = (X1, . . . , Xn),
is called weakly consistent for θ, when Tn(X)
p→θ and strongly consistent when
Tn(X)
a.s.
→θ.
Consistency is a desired property for MLE estimators and as we will see in
Chapter 5, it is not diﬃcult to establish. Next we collect a rudimentary theorem that
can be used to show consistency of an estimator, including the MLE.
Theorem 2.1 (Consistency) Consider an estimator Tn(X) of a parameter θ, based
on a random sample X = (X1, . . . , Xn), and assume that the following conditions
hold:
(i) E [Tn(X)] →θ, as n →∞, and
(ii) Var(Tn(X)) →0, as n →∞.
Then Tn(X) is consistent for θ.
The following theorems allow us to study the asymptotic behavior of averages
of random variables, as well as prove under which conditions sample averages are
consistent estimators of the corresponding population averages. A straightforward
application of Chebyshev’s inequality gives the following.
Theorem 2.2 (Weak law of large numbers (WLLN)) Let X1, . . . , Xn be a ran-
dom sample from some model with E(Xi) = µ and Var(Xi) = σ2 < ∞. Deﬁne
Xn = 1
n
nP
i=1 Xi. Then Xn
p→µ.
Note that there are more general versions of the WLLN where the assumption
σ2 < ∞is not required. In particular, requiring integrability is enough (see deﬁni-

44
STATISTICAL INFERENCE
tion 3.16), however, the rudimentary version presented above is applicable in most
practical situations.
A stronger analog of the WLLN is given next (under the same assumptions
as in the WLLN theorem) and the proofs of both laws are requested as exercises.
These laws illustrate the asymptotic behavior of the sample average as the sample
size increases and provide additional justiﬁcation for using the sample average in
estimating the population mean.
Theorem 2.3 (Strong law of large numbers (SLLN)) Let X1, . . . , Xn, be a ran-
dom sample with E(Xi) = µ and Var(Xi) = σ2 < ∞. Deﬁne Xn = 1
n
nP
i=1 Xi. Then
Xn
a.s.
→µ.
A great consequence of the SLLN is the Monte Carlo approximation.
Remark 2.13 (Monte Carlo) Assume that X1, . . . , Xn are iid f (x|θ), x ∈X, with
E(g(Xi)) = λ, for all i = 1, 2, . . ., n. Then the SLLN tells us that w.p. 1 we have
1
n
nP
i=1 g(Xi) →λ, as n →+∞.
The SLLN states that if we generate a large number of values from f (x|θ), then in
order to approximate λ all we have to do is take the average of the generated values
evaluated through g(x). Note that the values g(x1), . . . , g(xn) can be thought of as
realizations from the random variable g(X). The Monte Carlo integration method
consists of using this idea to evaluate the integral E(g(X)), i.e.,
λ = E(g(X)) =
R
X
g(x)f (x|θ)dx ≃1
n
nP
i=1 g(xi),
where X1, . . . , Xn is a random sample from f (x|θ). Clearly, the larger the n the better
the approximation. Choosing g(X) = I(X ∈A) and noting that E(g(X)) = P(X ∈A),
allows us to approximate probabilities, that is,
P(X ∈A) ≃1
n
nP
i=1 I(xi ∈A).
2.4
Interval Estimation
Point estimators T(x) provide a single value in an attempt to guess the parameter
θ. We turn now to a natural extension of point estimation by considering a range of
values to help us estimate the parameter. From a classical point of view, an interval
estimator of a parameter must be a set with bounds that are aﬀected by the observed
data. This leads to the following deﬁnition.
Deﬁnition 2.5 Interval estimator
Let Xi|θ
iid∼f (x|θ), x ∈X, i = 1, 2, . . . , n, θ ∈Θ ⊆R. An interval estimator or

INTERVAL ESTIMATION
45
conﬁdence interval (CI) of the parameter θ is deﬁned as the random interval C(X) =
[L(X), U(X)], for some real-valued statistics L and U, such that L(x) ≤U(x), for all
x. Moreover, if
Px|θ(θ ∈[L(X), U(X)]) = 1 −α,
(2.17)
then 100(1 −α)% is called the conﬁdence or coverage probability of the interval.
The conﬁdence coeﬃcient of C(X) is deﬁned as the smallest coverage probability
inf
θ∈ΘPx|θ(θ ∈C(X)).
The following remark provides some insight and interpretation of conﬁdence
intervals.
Remark 2.14 (Conﬁdence interval interpretation) Equation (2.17) involves a
probability that is computed based on the model distribution f (x|θ) and assumes
an interpretation in terms of a relative frequency, that is, if we were to conduct the
experiment 100 times, then we would expect to ﬁnd the true value of the parameter
100(1−α)% of the times within the CI. Classical methods are often called frequen-
tist methods due to interpretations such as the latter.
Further note that this probability statement is computed based on the distri-
bution of X|θ and requires all X ∈Xn, in order to compute it, namely, unobserved
data. Thus, requiring the statement L(X) ≤θ ≤U(X) to be satisﬁed w.p. 1 −α, for
ﬁxed and unknown θ, is similar to requiring from the statistician to guess realiza-
tions (data) x of the random vector X that satisfy L(x) ≤θ ≤U(x), for the given
θ. This logic is counterintuitive since the purpose of any estimation procedure in
statistics is to use observed data x to estimate the unknown parameter θ and not the
opposite.
Bayesians, on the other hand, utilize the observed data to update the prior and
calculate the posterior distribution in order to obtain an interval estimator for the
parameter.
Deﬁnition 2.6 Credible set
Let Xi|θ
iid∼f (x|θ), x ∈X, i = 1, 2, . . ., n, θ ∈Θ ⊆Rp and let π(θ) be a prior
distribution on θ. Then if the posterior π(θ|x) exists, a 100(1 −α)% credible set
(CS) for θ ∈Θ, is the subset A of Θ with
P(θ ∈A|x) = 1 −α.
(2.18)
Clearly, there are many sets that could satisfy equations (2.17) and (2.18) and
hence we need to evaluate these interval estimators in terms of their properties.
2.4.1
Conﬁdence Intervals
We investigate ﬁrst the construction of conﬁdence intervals from a classical
point of view by utilizing pivotal quantities. Let Q = Q(X, θ) ∈R be pivotal for a

46
STATISTICAL INFERENCE
real parameter θ ∈Θ ⊆R and consider the probability
P(q1 ≤Q ≤q2) = 1 −α,
(2.19)
for some constants q1 and q2, q1 < q2, which can be written in terms of the cdf of
Q, FQ(q) = P(Q ≤q), as
FQ(q2) −FQ(q1−) = 1 −α,
where q1−means we approach q1 from the left (q1−= limε→0(q1 −ε)). Note
that if q1 ≤Q(X, θ) ≤q2 if and only if L(X,q1, q2) ≤g(θ) ≤U(X,q1, q2), then
[L(X,q1, q2), U(X,q1, q2)] is a 100(1 −α)% CI for g(θ). We summarize some of the
classic properties of conﬁdence intervals next.
Remark 2.15 (Conﬁdence interval properties) The following are the desirable
properties for classical conﬁdence intervals and are obtained for diﬀerent choices
of q1 and q2.
1. Equal tail If q1 and q2 are such that FQ(q1) = 1 −FQ(q2) = α
2 then the resulting
CI is called equal tail.
2. Minimum width Let q∗
1 and q∗
2 be such that they satisfy equation (2.19) and
minimize the length of the interval [L(X,q1, q2), U(X,q1, q2)]. Then the interval
[L(X,q1, q2), U(X,q1, q2)] is called the Minimum Width CI among all conﬁdence
intervals of conﬁdence 100(1 −α)%.
3. Intervals of the form [L(X,q1, q2), U(X,q1, q2)] are called two-sided, while inter-
vals of the form [L(X,q1), +∞) or (−∞, U(X,q2)], are one-sided and can be obtained
in a similar fashion.
A second method is based on the suﬃcient statistic. Let T be the suﬃcient
statistic with distribution fT|θ(t|θ) and consider a conﬁdence interval [V1, V2], such
that h1(V1) = T and h2(V2) = T, h1(θ) < h2(θ), with p1 = PT|θ(T < h1(θ)) and
p2 = PT|θ(T > h2(θ)), with 0 < p1, p2 < 1 and 0 < p1 + p2 < 1. Then [V1, V2] has
coverage 100(1 −p1 −p2)%, since P(h1(θ) ≤T ≤h2(θ)) = 1 −p1 −p2.
2.4.2
Highest Posterior Density Credible Sets
A special class of credible sets is deﬁned below.
Deﬁnition 2.7 HPD credible set
Let Xi|θ
iid∼f (x|θ), x ∈X, i = 1, 2, . . . , n, θ ∈Θ ⊆Rp and let π(θ) be a prior distri-
bution on θ. Then if the posterior π(θ|x) exists, the 100(1 −α)% Highest Posterior
Density (HPD) credible set for θ ∈Θ, is the subset Ck(x) of Θ of the form
Ck(x) = {θ ∈Θ : π(θ|x) ≥k(α)},
(2.20)
where k(α) is the largest constant such that
P(θ ∈Ck(x)|x) ≥1 −α,
(2.21)
with equality when θ is continuous. It should be noted that for unimodal π(θ|x), it

INTERVAL ESTIMATION
47
can proven that the 100(1 −α)% HPD set is also the minimum width set for the
given α.
Example 2.9 (HPD credible set for normal mean)
Let Xi|θ
iid∼N(θ, σ2), with
σ2 known and consider the prior θ ∼N(µ, τ2), with µ, τ2 known. It is easy to see
that the posterior of θ|x is N

ξ(x), η2(x)

, with
ξ(x) =
nτ2
nτ2 + σ2 x +
σ2
nτ2 + σ2µ,
and η2(x) = τ2σ2/(nτ2 + σ2). We ﬁnd the 100(1 −α)% HPD credible set for θ. We
need to ﬁnd a set Ck(x) = {θ ∈Θ : π(θ|x) ≥k(α)}, with P(θ ∈Ck(x)|x) = 1 −α. We
can show that π(θ|x) ≥k(α) if and only if |θ −ξ(x)| ≤k1, where
1 −α = P(θ ∈Ck(x)|x) = P (|Z| ≤k1/η(x)|Z = (θ −ξ(x))/η(x) ∼N(0, 1)) ,
and hence
P (Z > k1/η(x)) = α/2,
so that k1 = η(x)zα/2, where zα/2 is the α/2 percentile of a N(0, 1) distribution.
Hence the 100(1 −α)% HPD is of the form ξ(x) −η(x)zα/2, ξ(x) + η(x)zα/2
 .
Example 2.10 (HPD using Monte Carlo) Let X|θ ∼Exp(θ), θ > 0 and consider
the prior θ ∼Exp(λ), with λ known. We can easily show that θ|x ∼Gamma(2, 1/(λ
+x)), so that π(θ|x) = k if and only if θe−(λ+x)θ(λ + x)2 = k, which leads to the
equation
ln θ −(λ + x)θ = ln

k/(λ + x)2
,
(2.22)
which cannot be solved analytically with respect to θ. Hence, in order to ﬁnd the
largest k that satisﬁes P(π(θ|x) ≥k|x) = 1 −α, we turn to Monte Carlo simula-
tion. Following remark 2.13, we approximate the posterior probability of the event
Ck(x) = {θ > 0 : π(θ|x) ≥k}, for a given k, using
P(π(θ|x) ≥k|x) = Eθ|X[I(θ ∈Ck(x))] ≃1
L
LP
i=1 I(θi ∈Ck(x)),
where L is a large integer and θ1, . . . , θL are realizations from a Gamma(2, 1/(λ+
x)). We then select the k∗that gives the largest probability below 1 −α (for contin-
uous θ there is only one such k∗). Finally, the HPD set is deﬁned based on values
θ > 0, that are the roots of equation (2.22) for k = k∗. In our case, since the posterior
is unimodal, there will be two roots θ∗
1 and θ∗
2 and the 100(1 −α)% HPD will be
[θ∗
1, θ∗
2].
Example 2.11 (Credible ellipsoid) Let X|θ ∼Np(θ, Σ), with θ ∼Np(µ, R), with
Σ > 0, µ, and R > 0, are ﬁxed. It is straightforward to see that θ|X ∼Np(v, Ψ),
with v = nΨΣ−1x + ΨR−1µ and Ψ = (nΣ−1 + R−1)−1. Now π(θ|x) ≥k if and only
if q = (θ −v)TΨ−1(θ −v) ≤k1, where q|x ∼χ2
p, so that P(q ≤k1|x) = P(π(θ|x) ≥
k|x) = 1−α, which leads to P(q ≥k1|x) = α and hence k1 = χ2
p;α. Thus a 100(1−α)%
HPD credible set for θ is {θ ∈Rp : (θ −v)TΨ−1(θ −v) ≤χ2
p;α}, which is an ellipsoid
in Rp.

48
STATISTICAL INFERENCE
2.4.3
Decision Theoretic
Classical or Bayesian methods can be easily incorporated in a decision theoretic
framework by choosing an appropriate loss function and treating possible actions
as intervals. Let I(θ, C) denote the indicator function taking value 1, if θ ∈C and 0,
if θ < C, where C = C(x) ⊆R and deﬁne the loss function
L(θ, C) = b ∥C∥−I(θ, C),
with b > 0, some constant and ∥C∥= length(C). The frequentist risk for an interval
C(x) is obtained as
R(θ, C) = bEx|θ
θ [∥C(x)∥] −Px|θ
θ (θ ∈C(x)),
which depends on the length of C, ∥C(x)∥, as well as the coverage probability
Px|θ
θ (θ ∈C(x)). Obtaining rules C(x), with diﬀerent properties, such as minimum
width or equal tail of conﬁdence 100(1−α)%, reduces to minimizing the frequentist
risk under diﬀerent conditions.
Example 2.12 (Decision theoretic interval estimation)
Let Xi|θ
iid∼N(θ, σ2),
with σ2 known and consider interval rules of the form Cr = Cr(x) = [x−rσ, x+rσ],
r ≥0, with length lr = ∥C(x)∥= 2rσ and coverage probability
Px|θ
θ (θ ∈Cr(x)) = Px|θ
θ

−√nr ≤√n(X −θ)/σ ≤√nr

= 2P(Z ≤√nr) −1,
where Z ∼N(0, 1). Then the frequentist risk becomes
R(θ, Cr) = b2rσ −2P(Z ≤√nr) + 1,
which is independent of θ, so that Cr, r ≥0, are equalizer rules. Choosing r0 =
zα/2/ √n, where P(Z > zα/2) = α/2, the coverage probability becomes
Px|θ
θ (θ ∈Cr0(x)) = 2P(Z ≤zα/2) −1 = 1 −α,
and the corresponding risk assumes the form
R(θ, Cr0) = 2bσzα/2/ √n + 1 −α.
In general we want to minimize risk with respect to the decision Cr in order to
obtain the admissible rule within the class of rules Cr, i.e., we need to ﬁnd
r = arg min
r≥0 R(θ, Cr).
Let Φ denote the cdf of Z. The derivative of the risk function with respect to r ≥0,
is given by
dR(θ, Cr)
dr
= 2bσ −2 √nΦ′( √nr) = 2bσ −2 √n(2π)−1/2e−nr2/2,
and setting it to zero, we obtain
−nr2/2 = ln

bσ
p
2π/n

.
(2.23)
Now if b > √n/(2π)/σ, then ln

bσ √2π/n

is deﬁned and is positive and con-
sequently dR(θ,Cr)
dr
≥0. Thus arg min
r≥0 R(θ, Cr) = 0 and C0(X) = {X}, a degenerate
random set (singleton). Now if b < √n/(2π)/σ, then ln

bσ √2π/n

< 0 and hence
equation (2.23) has a solution at r = ±
q
−(2/n) ln

bσ √2π/n

. Since r ≥0, the

HYPOTHESIS TESTING
49
only valid solution is
r0 = arg min
r≥0 R(θ, Cr) =
q
−(2/n) ln

bσ
p
2π/n

,
(2.24)
for b < √n/(2π)/σ. Note that when b =
1
σ
√n(2π)−1/2e−z2
α/2/2 < √n/(2π)/σ, the
solution to equation (2.23) is r0 = zα/2/ √n and yields the usual 100(1 −α)%
minimum width CI. Now consider the prior distribution θ ∼π(θ) ∝1 and since
X|θ ∼N(θ, σ2/n), where X is the suﬃcient statistic, the posterior distribution of
θ|X is N(X, σ2/n), or √n

θ −X

/σ|X ∼N(0, 1). Then the PEL is given by
PEL = Eθ|X[L(θ, Cr)] = 2rbσ −P(x −rσ ≤θ ≤x + rσ) = 2rbσ + 1 −2Φ(r √n),
and hence the PEL is exactly the same as the frequentist risk, which is minimized
at r0 given by equation (2.24). Thus, Cr0 is the unique Bayes rule and from remark
2.12.4 is admissible and using remark 2.12.7 it is also minimax.
2.5
Hypothesis Testing
The last statistical inference method we will discuss is hypothesis testing.
2.5.1
Classic Methods
The classical approach involves using a random sample x from a model f (x|θ),
in order to create a statistical test. The major components of a statistical test are
summarized below.
Remark 2.16 (Components of a statistical test) We collect all the components of
a statistical test below.
1. Hypotheses A null hypothesis H0 : θ ∈Θ0 ⊆Rp and an alternative hypoth-
esis Ha : θ ∈Θa. Note that when Θa = Rp ∖Θ0, the alternative hypothesis is
complementary.
2. Test statistic A statistic T(X) is called the test statistic for this hypothesis. The
distribution of T(X) under H0 and Ha is required in order to deﬁne other important
components of the test.
3. Rejection region The test we create is based on the idea that we divide the
support S of T(X) in two areas C and S ∖C and we reject the null hypothesis if
T(X) ∈C or do not reject if T(X) ∈S ∖C = Cc. The region C is aptly named the
rejection (or critical) region (RR) for this test.
4. Type I and II errors In order to evaluate a test procedure and select the best, in
some sense, we need the help of the so-called Type I and II errors. The schematic
of Table 2.1 provides the deﬁnition of these errors and when they occur for any
hypothesis testing problem. Therefore, the probability of the type I error is given
by
α = P(Type I Error) = P(Reject H0|H0 is true),

50
STATISTICAL INFERENCE
Statistician’s
Decision
Accept H0
Reject H0
State of
H0 describes the true state
No Error
Type I Error
Nature
Ha describes the true state
Type II Error
No Error
Table 2.1: Schematic for any hypothesis testing problem along with the occurrence
of the Type I and II errors.
and similarly, we quantify the probability of a type II error using
β = P(Type II Error) = P(Accept H0|H0 is false).
If H0 or Ha are simple hypotheses, e.g., Θ0 = {θ0} or Θa = {θa} (a single value for
the parameter), then α and β take only one value. In this case, α is called the level of
signiﬁcance of the test. If H0 is a composite hypothesis, i.e., Θ0 consists of a range
of values not just a singleton, then the level of signiﬁcance of the test is deﬁned as
α = sup
f∈H0
P(Reject H0| f ),
where f ∈H0 means that f is the distribution of T(X) under the null hypothesis.
Clearly, a testing procedure will prove useful if both α and β are small. If we enlarge
the RR C to a new region C∗(C ⊂C∗), then
α∗= P(T(X) ∈C∗|H0 true) ≥P(T(X) ∈C|H0 true) = α,
but on the contrary
β∗= P(T(X) < C∗|Ha true) ≤P(T(X) < C|Ha true) = β,
and thus as α increases, β gets smaller and when α decreases, β gets larger. This
means that we cannot minimize α and β at the same time. The standard approach is
to ﬁx α and then ﬁnd the RR C that minimizes β.
5. Test power The power of a test is deﬁned by
ρ = 1 −β = 1 −P(Accept H0|H0 is false) = P(Reject H0|H0 is false),
and hence we can write the power of a test as
ρ = P(Reject H0|Ha is true).
Clearly, the more powerful a test is the better it is.
We turn now to the mathematical foundations that incorporate these ideas in a
classical framework.
Deﬁnition 2.8 Randomized test function
A Randomized Test Function ϕ for testing H0 : θ ∈Θ ⊆Rp vs Ha : θ ∈Θa, is a
function ϕ : Rn →[0, 1], with
ϕ(x) = P(Reject H0|X = x),

HYPOTHESIS TESTING
51
where x is a random sample of size n, i.e., given that we observe X = x, we ﬂip a
coin and we reject H0 w.p. ϕ(x).
The function ϕ(x) allows us to describe all the necessary components of a statis-
tical test. Clearly, ϕ(X) is a random variable, leading to a randomized test function.
If ϕ is such that ϕ(x) = 1, if x ∈C, or 0, if x < C, then ϕ is called non-randomized.
In order to obtain the probabilities of the type I and II errors, we deﬁne βϕ(θ) =
PX|θ
θ (Reject H0), so that
βϕ(θ) = EX|θ
θ [ϕ(X)].
Now if θ ∈Θ0 then βϕ(θ) gives the probability of the type I error, while if θ ∈Θa
then βϕ(θ) is the power of the test function ϕ at θ ∈Θa.
The magnitude of the test function ϕ is deﬁned as α = sup
θ∈Θ0
βϕ(θ), which is
nothing but the level of signiﬁcance. Once we ﬁx α, we look for ϕ(x) such that
βϕ(θ) ≥βϕ∗(θ), for any other test function ϕ∗, such that sup
θ∈Θ0
βϕ∗(θ) ≤α.
Note that when we consider a hypothesis H : θ = θ0 we are assuming a speciﬁc
model f0 = f (x|θ0) for the data. Hence it is useful in some cases to write hypotheses
in terms of the underlying models implied, namely, H : f = f0. We also write
f ∈H meaning that f denotes a model under the hypothesis H. Test functions can
be classiﬁed in two main groups as we see next.
Remark 2.17 (Properties of test functions) There are two main properties for test
functions.
1. Most powerful A test function ϕ such that
sup
θ∈Θ0
βϕ(θ) = αϕ,
(2.25)
and
βϕ(θ) ≥βϕ∗(θ),
(2.26)
θ ∈Θa, for any other test function ϕ, with αϕ∗≤αϕ, is called Uniformly Most
Powerful (UMP) amongst all test functions of level αϕ.
If H0 is simple, then a test function ϕ that satisﬁes equations (2.25) and (2.26),
is called simply the Most Powerful (MP) test function. Note that the word uniformly
above is understood as “with respect to all θ ∈Θ0.” The nonexistence of UMP tests
in some cases has led to the deﬁnition of unbiased tests.
2. Unbiased Assume that Xi|θ
iid∼f (x|θ), θ ∈Θ ⊆Rp, p ≥1. Then for testing
H0 : θ ∈Θ0 vs Ha : θ ∈Θa = Θc
0, at level α, a test function ϕ(x) is called unbiased
if Eθ[ϕ(X)] ≤α, θ ∈Θ0 and Eθ[ϕ(X)] ≥α, θ ∈Θc
0.
Although not the most desirable property, unbiasedness guarantees two
things; ﬁrst, the probability of the type I error is at most α and second, the power
of the test is at least α. A test function is UMP unbiased if it is UMP within the
collection of all unbiased test functions. A UMP test function of level α is always

52
STATISTICAL INFERENCE
UMP unbiased since letting ϕ∗(x) = α, we have βϕ(θ) ≥βϕ∗(θ) = α, θ ∈Θc
0, since ϕ
is UMP.
The following remark summarizes the classic methods of ﬁnding test functions.
Remark 2.18 (Finding test functions) There are many methods of ﬁnding test
functions that lead to statistical tests with diﬀerent properties and we collect the
most important below.
1. Neyman-Pearson fundamental lemma For testing H0 : θ = θ0 vs Ha : θ = θ1
at a level α (0 < α < 1) and based on X, consider the test function
ϕNP(x) =

1
L(θ1|x) > kL(θ0|x)
γ
L(θ1|x) = kL(θ0|x)
0
L(θ1|x) < kL(θ0|x)
,
(2.27)
where L(θ|x), the likelihood function and γ and k are unique constants such that
Eθ0[ϕNP(X)] = α, or
PX|θ0(L(θ1|x) > kL(θ0|x)) + γPX|θ0(L(θ1|x) = kL(θ0|x)) = α.
(2.28)
Then ϕNP(x) is Most Powerful (MP) amongst all other test functions of level at
most α.
Neyman-Peason states that in order to test a simple H0 against a simple Ha
we should use a rejection region of level α and the most powerful among them is
C(x) = {x : L(θ0|x)/L(θ1|x) ≤k},
with k given by equation (2.28). Note that T(X) = L(θ0|x)/L(θ1|x), is a random
variable and hence C(X) = {X : T(X) ≤k} is a random set.
2. GLRT When we cannot apply the Neyman-Peason lemma and we need a test
function, we use the method of the Generalized Likelihood Ratio Test (GLRT).
Let L(θ|x) =
nQ
i=1 f (xi|θ), the likelihood function based on a sample X1, . . . , Xn from
f (x|θ), θ ∈Θ ⊆Rp. In order to test H0 : θ ∈Θ0 vs Ha : θ ∈Θa, we deﬁne the
likelihood ratio
λ(x) =
max
θ∈Θ0 L(θ|x)
max
θ∈Θ L(θ|x),
with 0 ≤λ ≤1, since Θ0 ⊆Θ. Now if H0 is true, then θ0 ∈Θ0, which means that
max
θ∈Θ0 L(θ|x) would be very close to max
θ∈Θ L(θ|x) and hence we should reject H0 for
small values of λ, that is, we have the rejection region
C = {x : λ(x) ≤k},
where P(λ(X) ≤k|θ0) = α, if Θ0 = {θ0} (simple H0) or P(λ(X) ≤k|θ0) ≤α,
∀θ ∈Θ0, with equality for at least one θ ∈Θ0, when H0 is composite. Clearly, the
test depends on knowing the distribution of the test statistic λ(x) under H0.
If Θ0 ⊆Rr ⊆Θ ⊆Rp, then it can be shown that for large values of n,
−2 ln λ(X) ∼χ2
p−r, under H0. Then C = {x : −2 ln λ(x) ≥−2 ln k} and a large

HYPOTHESIS TESTING
53
sample GLRT is based on
ϕGLRT(x) =

1
−2 ln λ(X) ≥χ2
p−r;α
0
−2 ln λ(X) < χ2
p−r;α
,
where χ2
p−r;α is the α percentile of the chi-square distribution with p −r degrees of
freedom.
3. MLR Consider the model f (x|θ), x ∈RP, k ≥1, θ ∈Θ ⊆R and let G =
{ f (x|θ) : θ ∈Θ}. The family of densities G is said to have the property of Monotone
Likelihood Ratio (MLR), with respect to a function V : RP →R, if (i) the support
{x ∈RP : f (x|θ) > 0} is independent of θ, (ii) for any two θ, θ1 ∈Θ, θ , θ1, f (x|θ) ,
f (x|θ1) a.e. and (iii) for all θ, θ1 ∈Θ, θ < θ1, the likelihood ratio f (x|θ1)/ f (x|θ) =
gθ,θ1(V(x)), where g is a strictly monotone function of V.
Now assume that the model f (x|θ), x ∈X, has the MLR property with respect
to V(x). Then for gθ,θ1(V(x)) increasing in V(x), where θ < θ1, the UMP test for
H0 : θ ≤θ0 vs Ha : θ > θ0, is given by
ϕMLR(x) =

1
V(x) > k
γ
V(x) = k
0
V(x) < k
,
(2.29)
where γ and k are constants such that Eθ0[ϕMLR(X)] = α, or
PX|θ0(V(x) > k) + γPX|θ0(V(x) = k) = α.
When gθ,θ1(V(x)) is decreasing in V(x), where θ < θ1, the inequalities in equation
(2.29) are reversed. To test H0 : θ ≥θ0 vs Ha : θ < θ0, the UMP test function
ϕMLR(x) for increasing MLR in V(x) is given by equation (2.29), with inequalities
reversed for decreasing MLR in V(x).
Example 2.13 (MLR for exponential family) Consider the exponential family
f (x|θ) = c(θ)eT(x)Q(θ)h(x),
where θ ∈Θ ⊆R, x ∈X, X does not depend on θ and Q is a strictly monotone
function. For a random sample X1, . . . , Xn ∼f (x|θ), the joint distribution is given
by
f (x|θ) = c0(θ)eQ(θ)V(x)h∗(x),
where c0(θ) = cn(θ), h∗(x) =
nQ
i=1 h(xi) and V(x) =
nP
i=1 T(xi). Then G = { f (x|θ) : θ ∈
Θ} has the MLR property with respect to V and
gθ,θ1(V(x)) = c0(θ1)
c0(θ) exp {[Q(θ1) −Q(θ)]V(x)} , θ < θ1.
The UMP test function for testing H0 : θ ≤θ1 or θ ≥θ2 vs Ha : θ1 < θ < θ2, is
given by
ϕ(x) =

1
c1 < V(x) < c2
γi
V(x) = ci, i = 1, 2
0
V(x) < c1 or V(x) > c2
,

54
STATISTICAL INFERENCE
where γi and ci, i = 1, 2, are constants such that Eθ1[ϕ(X)] = Eθ2[ϕ(X)] = α.
Moreover, the UMP test exists for H0 : θ ≤θ0 vs Ha : θ > θ0 and H0 : θ ≥θ0 vs
Ha : θ < θ0, but it does not exist for H0 : θ1 ≤θ ≤θ2 vs Ha : θ < θ1 or θ > θ2 and
H0 : θ = θ0 vs Ha : θ , θ0. Using equation (2.29) we can obtain the forms of the
test functions by replacing the wording about the monotonicity of gθ,θ1(V(x)) with
the corresponding wording about monotonicity of Q(θ).
The following results tell us how to use test functions created for simple vs sim-
ple hypotheses, in order to assess simple vs composite or composite vs composite
hypotheses.
Remark 2.19 (MP and UMP tests) The following results relate MP and UMP test
functions for simple and composite hypotheses.
1. Let H0 be simple, Ha a composite alternative, and ϕ(x) the MP test function of
level α for H0 against a simple alternative H1 : f = fa, fa ∈Ha. Assume that ϕ(x)
is the same for all fa ∈Ha. Then ϕ(x) is the UMP test of level α for H0 vs the
composite Ha.
2. Let H0 and Ha be composite hypotheses. Assume that there exists f0 ∈H0 and
ϕ(x), such that ϕ(x) is the UMP test function of level α for H0 against Ha. If ϕ is of
level α for all f0 ∈H0, then ϕ is also UMP for the composite H0 vs Ha.
Example 2.14 (UMP and GLRT tests for a uniform distribution)
Let Xi|θ
iid∼
Uni f (0, θ), θ > 0, i = 1, 2, . . ., n, with 0 ≤x(1) ≤xi ≤x(n) ≤θ, so that the joint
distribution can be written as
f (x|θ) = θ−nI[x(n),+∞)(θ).
We are interested ﬁrst in the UMP test of level α for H0 : θ = θ0 vs Ha : θ , θ0.
Since f (x|θ) has a domain that depends on θ, we have to use the Neyman-Pearson
lemma to obtain the UMP. We have
L(θ0|x)
=
θ−n
0 I[x(n),+∞)(θ0), and
L(θa|x)
=
θ−n
a I[x(n),+∞)(θa), θa , θ0,
and the UMP rejection region is based on
L(θ0|x) ≤kL(θa|x),
which leads to
θn
aI[x(n),+∞)(θ0) ≤kθn
0I[x(n),+∞)(θa).
(2.30)
If X(n) > θ0 then L(θ0|x) = 0 ≤kL(θa|x), for all k and θa , θ0 and in this case
we should reject H0 since equation (2.30) is satisﬁed always. If X(n) < C < θ0,
then equation (2.30) becomes θn
a ≤kθn
0 and hence we reject H0, for k ≥θn
a/θn
0 and
any θa , θ0. Notice that when k < θn
a/θn
0 we cannot reject H0. We need to ﬁnd the
constant C that would yield the level α rejection region. We have
α
=
Pθ0(Reject H0) = Pθ0(X(n) < C or X(n) > θ0)
=
Pθ0(X(n) < C or X(n) > θ0) = FX(n)(C) + 1 −FX(n)(θ0),

HYPOTHESIS TESTING
55
and using equation (2.12), we have FX(n)(x|θ0) = xn/θn
0, 0 ≤x ≤θ0, under H0 : θ =
θ0, which leads to α = Cn/θn
0 and hence C = θ0α1/n, so that L(θ0|X) ≤kL(θa|X),
for some k, if and only if X ∈C = {X : X(n) ≤θ0α1/n or X(n) > θ0} and the
rejection region does not depend on θa. Thus, in view of remark 2.19.2, the UMP
test function of level α for testing H0 : θ = θ0 vs Ha : θ , θ0, is given by
ϕ(x) =

1
X(n) ≤θ0α1/n or X(n) > θ0
0
θ0α1/n < X(n) ≤θ0
.
Note that this test is also UMPU. Now we discuss the GLRT for this problem. We
have θ ∈Θ = (0, +∞), with H0 : θ ∈Θ0 = {θ0} vs Ha : θ ∈Θa = Θ ∖Θ0, with
bθ = X(n), the MLE of θ ∈Θ. Then the likelihood ratio for X(n) ≤θ0, is given by
λ(x) =
max
θ∈Θ0 L(θ|x)
max
θ∈Θ L(θ|x) = L(θ0|x)
L(bθ|x)
=  x(n)/θ0
n I[x(n),+∞)(θ0),
and considering λ(X) ≤k, for some k, leads to X(n) ≤k1 = θ0k1/n, while for
X(n) > θ0, λ(x) = 0 ≤k, for all k > 0. Then the rejection region is of the form
C = {x : x(n) ≤c or x(n) > θ0},
and hence the GLRT is the same as the UMP and UMPU test.
2.5.2
Bayesian Testing Procedures
When we test H0 : θ ∈Θ0 vs H1 : θ ∈Θ1, we are entertaining two diﬀerent
models for θ, namely, Mi = { f (x|θ) : θ ∈Θi}, i = 1, 2. Bayesian testing procedures
are based on posterior probabilities or the Bayes factor, which involves computing
the relative odds between the two models, before and after we conduct the experi-
ment.
Deﬁnition 2.9 Bayes factor
Let a0 = P(Θ0|x) and a1 = P(Θ1|x), denote the posterior (a posteriori) probabili-
ties for model Θ0 and Θ1, respectively, and let π0 = P(Θ0), π1 = P(Θ1), the prior (a
priori) probabilities of the two models. Then a0/a1 is called the posterior odds of
H0 to H1 and π0/π1 the prior odds. Moreover, the quantity
B = posterior odds
prior odds
= a0/a1
π0/π1
= a0π1
a1π0
,
(2.31)
is called the Bayes factor in favor of Θ0.
Some comments are in order about the usage and basic interpretations of Bayes
factors.
Remark 2.20 (Using the Bayes factor) If a0/a1 = 2 then H0 is two times as likely
to be true as H1 is, after the experiment or when π0/π1 = 2, we believe a priori H0
to be two times as likely to be true as H1, before the experiment. Consequently, the
Bayes factor assumes the interpretation as the odds for H0 to H1 that are given by

56
STATISTICAL INFERENCE
the data and if B > 1, H0 is more likely, if B < 1, H1 is more likely, while if B = 1,
we cannot favor either H0 nor H1.
1. Suppose that Θ0 = {θ0} and Θ1 = {θ1}. Then we can easily show that
a0
=
π0 f (x|θ0)
π0 f (x|θ0) + π1 f (x|θ1), and
a1
=
π1 f (x|θ0)
π0 f (x|θ0) + π1 f (x|θ1),
so that
a0
a1
= π0 f (x|θ0)
π1 f (x|θ1),
and the Bayes factor becomes
B = a0π1
a1π0
= f (x|θ0)
f (x|θ1),
the usual likelihood ratio.
2. Consider testing H0 : θ ∈Θ0 vs H1 : θ ∈Θ1 and assume a priori
π(θ) =

π0g0(θ), θ ∈Θ0
π1g1(θ), θ ∈Θ1
,
(2.32)
with π0+π1 = 1, 0 ≤π0, π1 ≤1 and g0, g1, proper densities describing how the prior
mass is spread out over the two hypotheses. As a result, the a priori probabilities
of the two models are given by
R
Θ0
π(θ)dθ
=
π0
R
Θ0
g0(θ)dθ = π0, and
R
Θ1
π(θ)dθ
=
π1
R
Θ1
g1(θ)dθ = π1.
which leads to the Bayes factor
B =
R
Θ0
f (x|θ)g0(θ)dθ
R
Θ1
f (x|θ)g1(θ)dθ
,
(2.33)
which is a weighted ratio of the likelihoods of Θ0 and Θ1.
Example 2.15 (Bayes factor for point null hypothesis testing)
Consider test-
ing H0 : θ = θ0 vs H1 : θ , θ0 and take the prior of equation (2.32), with g0(θ) = 1.
Then the marginal distribution is given by
m(x) =
R
Θ
f (x|θ)π(θ)dθ = f (x|θ0)π0 + (1 −π0)m1(x),
with the marginal for θ , θ0 given by
m1(x) =
R
{θ,θ0}
f (x|θ)g1(θ)dθ,

HYPOTHESIS TESTING
57
and hence
a0
=
P(θ = θ0|x) = f (x|θ0)π0
m(x)
=
f (x|θ0)π0
f (x|θ0)π0 + (1 −π0)m1(x)
=
"
1 + 1 −π0
π0
m1(x)
f (x|θ0)
#−1
,
and a1 = 1 −a0. Thus the posterior odds is given by
a0
a1
=
π(θ0|x)
1 −π(θ0|x) =
π0
1 −π0
f (x|θ0)
m1(x) ,
and the Bayes factor is given by
B = f (x|θ0)/m1(x).
An important application of the Bayes factor is in Bayesian model selection.
Remark 2.21 (Bayesian model selection) The Bayes factor can be used for model
selection. Indeed, consider M possible models for x|θ, denoted by Mi = { f (x|θ) :
θ ∈Θi}, i = 1, 2, . . ., M, where {Θi}M
i=1 is a partition of the parameter space Θ. Now
consider a mixture prior distribution with M components, given by
π(θ|λ) =
MP
i=1 πigi(θ|λi)I(θ ∈Θi),
with
MP
i=1 πi = 1, 0 ≤πi ≤1, I(θ ∈Θi) the indicator of the set Θi, gi a proper
prior density over Θi and λ = [λ1, . . . , λM]′ hyperparameters, so that the a priori
probability of model Mi is given by
P(θ ∈Θ j|λ) =
R
Θj
π(θ|λ)dθ = πj
R
Θj
g j(θ|λ j)dθ = πj,
j = 1, 2, . . ., M, since Θi ∩Θ j = ∅, i , j. The a posteriori probability of model Mi
is computed as
ai
=
P(θ ∈Θi|x, λ) =
R
Θi
f (x|θ)π(θ|λ)
m(x)
dθ
(2.34)
=
πi
R
Θi
f (x|θ)gi(θ|λi)dθ
m(x)
= mi(x|λi)
m(x|λ) ,
where mi(x|λi) = πi
R
Θi
f (x|θ)gi(θ|λi)dθ, the marginal of X with respect to model Mi,
i = 1, 2, . . ., M and m(x|λ) =
R
Θ
f (x|θ)π(θ|λ)dθ, the marginal distribution of X. In
order to compare models Mi and M j we compute the Bayes factor
Bi, j = ai/a j
πi/πj
=
R
Θi
f (x|θ)gi(θ|λi)dθ
R
Θj
f (x|θ)g j(θ|λ j)dθ
= mi(x|λi)
m j(x|λ j),
(2.35)
for i, j = 1, 2, . . ., M, i , j and select model Mk using the following steps:
1. Compute all Bayes factors Bij based on equation (2.35), i, j = 1, 2, . . . , M, i , j

58
STATISTICAL INFERENCE
and create the matrix B = [(Bij)], with Bii = 0, i = 1, 2, . . . , M and Bij =
1
Bji.
Each row of this matrix compares Mi with every other model.
2. Start with the ﬁrst row, i = 1 and ﬁnd j′ = arg
max
j=1,2,...,MBi, j.
3. If Bi, j′ > 1, we choose model Mi as the most likely among all models and we
are done. Otherwise, consider the next row i = 2, 3, . . ., and repeat the process
of step 2.
Since the marginal distribution is typically intractable, we often turn to Monte
Carlo in order to compute the Bayes factor, as we see next.
Remark 2.22 (Monte Carlo Bayes factor) The Bayes factor of equation (2.33)
can be very hard, if not impossible, to compute in closed form. We turn to Monte
Carlo simulation instead and approximate the marginal distributions mi(x), using
mi(x) =
R
Θi
f (x|θ)gi(θ)dθ ≃1
Li
LiP
j=1 f (x|θ(i)
j ),
for large Li, where θ(i)
1 , . . . , θ(i)
Li, realizations from gi(θ), i = 0, 1, and the Bayes factor
is approximated using
Bik ≃
1
Li
LiP
j=1 f (x|θ(i)
j )
1
Lk
LkP
j=1 f (x|θ(k)
j )
,
(2.36)
i, k = 0, 1, which is easy to compute and requires realizations of the corresponding
prior distributions g0(θ) and g1(θ). See remark 6.22 (p. 266) for a general model
choice example.
We end this section with the idea behind predictive inference.
Remark 2.23 (Posterior predictive inference) Suppose that we want to predict
a random variable Z with density g(z|θ) (θ unknown) based on data X = x, with
X ∼f (x|θ), e.g., x is the data in a regression problem and Z the future response
variable we need to predict in the regression setup. In order to accomplish this, we
compute the predictive density of Z|X = x, when the prior distribution for θ is π(θ),
as follows
p(z|x) =
R
Θ
g(z|θ)π(θ|x)dθ,
(2.37)
and all predictive inference is based on this density p(z|x). Since we use the poste-
rior distribution π(θ|x) in equation (2.37), the distribution is known also as posterior
predictive, while if we use the prior π(θ) it is called prior predictive.
2.5.3
Decision Theoretic
We can unify both classical and Bayesian hypothesis testing methods by consid-
ering an appropriate decision theory problem for a speciﬁc choice of loss function.
Remark 2.24 (Decision theoretic hypothesis testing) Let Xi|θ
iid∼f (x|θ), θ ∈Θ ⊆

HYPOTHESIS TESTING
59
Rp, i = 1, 2, . . ., n, and deﬁne the loss incurred in choosing the test function δ, to
test H0 : θ ∈Θ0 vs H1 : θ ∈Θc
0, as
L(θ, δ) =

0,
if θ ∈Θ0 and δ = 0
or θ ∈Θc
0 and δ = 1
L1, if θ ∈Θ0 and δ = 1
L2, if θ ∈Θc
0 and δ = 0
,
(2.38)
for decision rules (test functions) of the form
δ(x) =

1, x ∈C
0, x < C
,
(2.39)
with C the rejection region and L1, L2 constants that indicate how much we lose
for wrong decisions. To further appreciate equation (2.38) recall the schematic of
Table 2.1. Turning to the calculation of the risk function under this loss we write
R(θ, δ) = Ex|θ[L(θ, δ)] = L(θ, 1)Pθ(X ∈C) + L(θ, 0)Pθ(X < C),
and thus
R(θ, δ) =

L1Pθ(X ∈C),
θ ∈Θ0,
L2Pθ(X ∈Cc),
θ ∈Θc
0.
1. Admissible test function To ﬁnd the admissible test we need to ﬁnd δ that min-
imizes R(θ, δ) which is equivalent to minimizing Pθ(X ∈C) with respect to θ ∈Θ0
and Pθ(X ∈Cc) with respect to θ ∈Θc
0. The latter is equivalent to maximizing
Pθ(X ∈C) with respect to θ ∈Θc
0, i.e., the power of the decision rule.
2. Minimax and Bayes test functions In the case of point null vs point alterna-
tive, we have Θ = {θ0, θ1}, Θ0 = {θ0}, Θc
0 = {θ1}, with Pθ0(X ∈C) = α and
Pθ1(X ∈C) = β, the probabilities of the type I and II errors and the risk function
assumes the form
R(θ, δ) =

L1α, θ = θ0,
L2β, θ = θ1.
Then in order to ﬁnd the minimax test function we need to ﬁnd the rule δ that
minimizes maximum risk, i.e., δ is such that
max{R(θ0, δ), R(θ1, δ)} ≤max{R(θ0, δ∗), R(θ1, δ∗)},
for any other rule δ∗. Now consider the prior distribution
π(θ) =

p0, θ = θ0,
p1, θ = θ1,
(2.40)
with p0 + p1 = 1, 0 ≤p0, p1 ≤1. Then in order to obtain the Bayes test function we
need to minimize the Bayes risk with respect to δ, namely, minimize the function
r(π, δ) = R(θ0, δ)p0 + R(θ1, δ)p1.
More details on decision theoretic hypothesis testing can be found in Berger
(1984) and Lehmann (1986). We summarize some of the ideas of this theory in the
two theorems that follow.

60
STATISTICAL INFERENCE
Theorem 2.4 (Minimax test function) Let Xi|θ
iid∼f (x|θ), θ ∈Θ = {θ0, θ1},
i = 1, 2, . . ., n and assume we wish to test H0 : θ = θ0 vs Ha : θ = θ1. Suppose
that there exists a constant k such that the region
C = {x ∈Rn : f (x|θ1) > k f (x|θ0)},
(2.41)
satisﬁes
L1Pθ0(X ∈C) = L2Pθ1(X ∈Cc).
Then the test function of equation (2.39) is minimax and the level of the test is
α = Pθ0(X ∈C).
Theorem 2.5 (Bayes test function) Let Xi|θ
iid∼f (x|θ), θ ∈Θ = {θ0, θ1}, i =
1, 2, . . ., n, let π denote the prior given in equation (2.40) and C the critical region
as deﬁned in equation (2.41) with k = p0L1/(p1L0). Then the test function of
equation (2.39) is the Bayes test function and the level of the test is α = Pθ0(X ∈
C).
It is important to note that the decision theoretic procedures of ﬁnding test func-
tions satisfy the following result.
Remark 2.25 (Bayes, minimax and MP test functions) From both theorems and
in view of the Neyman-Pearson lemma we can see that the minimax and the Bayes
test functions are also the MP test functions.
Example 2.16 (Bayes test function for normal mean)
Assume that Xi|θ
iid∼
N(θ, 1), θ ∈Θ = {θ0, θ1}, i = 1, 2, . . ., n and write the joint distribution as
f (x|θ) = (2π)−n/2 exp
(
−1
2
nP
i=1(xi −x)2
)
exp
n
−n(x −θ)2/2
o
,
so that f (x|θ1) > cf (x|θ0), leads to x > c0, with c0 = (θ1+θ0)/2+(ln c) / (n(θ1 −θ0)) .
Then the minimax test function is δ(X) = I(c0,+∞)(X), with c0 such that L1Pθ0(X >
c0) = L2Pθ1(X ≤c0), with X ∼N(θ, 1/n). The probability of the type I error is
given by
α = Pθ0(X > c0) = 1 −Φ( √n(c0 −θ0)),
while the power is computed as
Pθ1(X > c0) = 1 −Φ( √n(c0 −θ1)).
Now letting θ have the prior of equation (2.40), it is easy to see that the Bayes test
function is δ(X) = I(c0,+∞)(X), with
c0 = (θ1 + θ0)/2 + ln (p0L1/(p1L0)) / (n(θ1 −θ0)) .
2.5.4
Classical and Bayesian p-values
For the sake of presentation, we consider the hypotheses Ho : θ ≤θo vs H1 :
θ > θo, based on a test statistic X, with X ∼f (x|θ), continuous. Based on a datum

HYPOTHESIS TESTING
61
X = x0, the p-value is deﬁned as
p(θ0, x0) = P f(x|θ)(X ≥x0|θ = θ0) =
∞R
xo
f (x|θ0)dx,
which is a tail area probability, with respect to the model f (x|θ0).
Bayesians, on the other hand, argue utilizing the classical p-value p(θ, x0) and
calculate the probability of the event A(x0) = {X ≥xo}, based on various distribu-
tions that depend on a prior distribution π(θ). More precisely, the prior predictive
p-value is deﬁned as
ρ1 = Eπ(θ)[p(θ, x0)] = Pm(x)(X ≥xo),
so that ρ1 is the probability of the event A(x0), with respect to the marginal distri-
bution m(x).
The posterior predictive p-value is deﬁned as
ρ2 = Eπ(θ|xo)[p(θ, x0)|X = xo] = Pm(x|x0)(X ≥xo),
so that ρ2 is the probability of the event A(x0), with respect to the predictive
marginal distribution m(x|x0). The two measures of Bayesian surprise presented
do not come without criticism with the most crucial being that improper priors can-
not be used in most cases. There are other alternatives, including the conditional
predictive p-value and the partial posterior predictive p-value (see remark 2.27).
Example 2.17 (Monte Carlo p-values) Computing the p-value for a statistical
test can be easily accomplished via Monte Carlo. Letting T denote the test statistic
for the testing procedure and assuming the alternative hypothesis is H1 : θ > θo, the
classical p-value can be approximated by
p = P f(x|θ)(T(X) ≥t0|θ = θ0) ≃1
L
LP
j=1 I(T(xj) ≥t0),
where t0 = T(x), the value of the test statistic based on observed data X = x0 and
x1, . . . , xL, are the realizations from f (x|θ0). This procedure is called a Monte Carlo
test and can be extended to more general problems.
Next we discuss the ideas behind Monte Carlo tests.
Remark 2.26 (Monte Carlo tests) Assume that we replace g(z|θ) by f (x|θ) in re-
mark 2.23, i.e., the predictive distribution is given by
p(x(p)|x(s)) =
R
Θ
f (x(p)|θ)π(θ|x(s))dθ,
(2.42)
where x(s) denotes observed data from f (x|θ) and x(p) future values we need to pre-
dict from the model f (x|θ). Using Monte Carlo we can approximate the predictive
distribution p(x(p)|x(s)) using
p(x(p)|x(s)) ≃1
L
LP
i=1 f (x(p)|θi),
based on a sample θ1, . . . , θL, from π(θ|x(s)) ∝f (x(s)|θ)π(θ), where π(θ) is a prior
distribution for θ and L is a large positive integer. Setting L to 1, p(x(p)|x(s)) ≃

62
STATISTICAL INFERENCE
f (x(p)|θ1), is obviously a bad approximation to the predictive distribution. However,
generating a value x(p)
i
from the model f (x|θi), i = 1, 2, . . ., L, can be thought of as
an approximate predictive sample of size L and hence any predictive inferential
method can be based on these samples.
We discuss next the creation of a Monte Carlo test (as in example 2.17) for
goodness-of-ﬁt (gof) based on this idea. In order to assess the null hypothesis H0 :
the model f (x|θ), for some θ, describes the observed sample x(s) = {x(s)
1 , . . . , x(s)
n } of
size n well, against Ha : the model f (x|θ), is not appropriate for x(s), for any θ, we
choose a test statistic T(x) and compute the predictive p-value
ppred = Pp(x(p)|x(s))(T(X(p)) ≥T(x(s))) ≃1
L
LP
i=1 I(T(x(p)
i ) ≥T(x(s))),
(2.43)
based on predictive samples x(p)
i
= {x(p)
i1 , . . . , x(p)
in }, i = 1, 2, . . ., L, simulated from
p(x(p)
i |x(s)) = f (x|θi) =
nQ
j=1 f (xj|θi), where θi, i = 1, 2, . . ., L, are realizations from
the posterior distribution based on the sample x(s), given by
π(θ|x(s)) ∝
" nQ
j=1 f (x(s)
j |θ)
#
π(θ).
It is important to note that by construction, this test is assessing all the important
assumptions including choice of model f (x|θ) and choice of prior π(θ), as well as
the independent and identically distributed assumption of the observed data x(s).
The choice of the test statistic is crucial in this procedure and some statistics will
not be able to assess gof.
We can assess the usefulness of a test statistic by simulating the ideal situation
and computing ppred, based on ideal samples (namely, samples from f (x|θ) that will
be treated as the observed data). If a test statistic yields small values for ppred, say
smaller than 0.1, they should be discarded, since in the ideal situation they cannot
identify that f (x|θ) is the true model. Obviously, increasing n and L yields a better
approximation to the true value of the predictive p-value.
Standard test statistics can be considered as candidates, e.g., sample means,
variances or order statistics. Note that for any context in statistics we can construct
Monte Carlo tests in a similar way.
The discussion above leads to the creation of the following general Monte Carlo
test.
Algorithm 2.2 (Monte Carlo adequacy test) The general algorithm is as fol-
lows.
Step 1: Given a sample x(s) = {x(s)
1 , . . . , x(s)
n } from f (x|θ), simulate θi|x(s) ∼
π(θ|x(s)), i = 1, 2, . . ., L, for large L.
Step 2: For each i = 1, 2, . . . , L, simulate x(p)
i
∼f (x|θi), the predictive sample.
Step 3: Choose a statistic T(x) and calculate the p-value ppred of equation (2.43)
and assess gof based on ppred; if ppred ⋍0, we have strong evidence for reject-

HYPOTHESIS TESTING
63
ing H0 and the model is not appropriate for modeling the observed data. When
ppred > 0.1 we have a good indication that the entertained model ﬁts the data
well.
Example 2.18 (Monte Carlo adequacy test)
Suppose that we entertain a
Uni f (0, θ), f (x|θ) = I[0,θ](x), model for a random variable X, where θ > 0, un-
known and consider a subjective prior θ ∼Exp(λ), with λ > 0, known, which leads
to the posterior distribution π(θ|x(s)) ∝e−λθI[x(s)
(n),+∞)(θ), where x(s) = {x(s)
1 , . . . , x(s)
n }
denotes the observed data, with x(s)
(n) = max
i=1,2,..,nx(s)
i , so that θ −x(s)
(n)|x(s) ∼Exp(λ). In
order to build the Monte Carlo model adequacy test, consider the test statistic T(X)
and simulate a predictive sample x(p)
i
∼f (x|θi) =
nQ
j=1 f (xj|θi), based on simulated
θi ∼π(θ|x(s)), i = 1, 2, . . . , L. The Monte Carlo predictive p-value is given by
ppred = Pp(x(p)|x(s))(T(X(p)) ≥T(x(s))) ≃1
L
LP
i=1 I(T(x(p)) ≥T(x(s))).
(2.44)
Simulation results are given in Table 2.2 (see MATLAB R⃝procedure MCMode-
lAdequacyex1 and Appendix A.7). We use L = 100000 predictive samples for
n = 10, 50, 100, observed sample points. The observed data is simulated from
three models Uni f (0, 1), Gamma(10, 10) and N(−10, 1). The predictive ppred are
computed for four candidate test statistics: T1(X) = X(1), T2(X) = X(n), T3(X) = X
and T4(X) = S 2, the sample variance.
We would expect a good test statistic to indicate gof only for the ﬁrst model,
Uni f (0, 1), that is, large predictive ppred. Based on these results, T1 emerges as the
best test statistic to assess the entertained model, since it supports the true model
for the data in all cases, while it rejects the data from the alternative models. Note
that the choice of λ = 1 can potentially aﬀect the results since it directly impacts
an aspect of the whole procedure, the prior distribution.
The notion of p-value in the context of hypothesis testing has been widely used
but seriously criticized for a long time in the literature. We collect some of these
criticisms next.
Remark 2.27 (Criticisms about p-values) The major criticism regarding the p-
value is whether it provides adequate evidence against a null hypothesis or a model.
Hwang et al. (1992) present a brief summary of the controversy about the classical
p-value.
Criticism about the p-value has come mainly from the Bayesian viewpoint
since the calculation of a p-value almost always involves averaging over sample
values that have not occurred, which is a violation of the likelihood principle. There
is a vast literature and discussion papers on this topic, including Cox (1977), Shafer
(1982), Berger and Delampady (1987), Casella and Berger (1987), Meng (1994),
Gelman et al. (1996), De La Horra and Rodrıguez-Bernal (1997, 1999, 2000, 2001),
Bayarri and Berger (1999), Micheas and Dey (2003, 2007), De la Horra (2005) and

64
STATISTICAL INFERENCE
Data Model
The MC test
n = 10
n = 50
n = 100
should indicate:
ppred for Ti
ppred for Ti
ppred for Ti
Uni f (0, 1)
gof
0.4067
0.9231
0.8895
0.8847
0.5649
0.9816
0.9313
0.9464
0.7348
0.9905
0.9647
0.9880
Gamma(10, 10)
not adequate ﬁt
0.006
0.0514
0.1848
0.9845
0
0.2386
0.0004
1.0000
0
0.3922
0.0001
1.0000
Normal(−10, 1)
not adequate ﬁt
0.0003
0.00038
0.00038
0.00001
0.00049
0.00049
0.00049
0.00001
0.00038
0.00038
0.00038
0.00001
Table 2.2: Simulations of Monte Carlo goodness-of-ﬁt tests. We used L = 100000
predictive samples for n = 10, 50, 100 observed sample points and the data is sim-
ulated from three models Uni f (0, 1), Gamma(10, 10), and N(−10, 1). We choose
λ = 1 for the hyperparameter, and ppred is provided for four statistics T1(X) = X(1),
T2(X) = X(n), T3(X) = X, and T4(X) = S 2. Based on these results T1 emerges as the
best test statistic in order to assess the entertained model.
a host of others.
In the case of point null hypothesis, the use of a p-value is highly criticized
from a Bayesian point of view, whereas for a one-sided hypothesis testing scenario,
the use of a p-value is argued to be sensible (see Casella and Berger, 1987). Several
leading Bayesians (e.g., Box, 1980, and Rubin, 1984) have argued that the p-value
approach is equivalent to the calculation of a tail-area probability of a statistic,
which is useful even for checking model adequacy. The tail area probability used
by Box (1980) is called the prior predictive p-value.
On the contrary, Rubin (1984) deﬁned the posterior predictive p-value by
integrating the p-value with respect to the posterior distribution. Bayarri and Berger
(1999) propose two alternatives, the conditional predictive p-value and the partial
posterior predictive p-value which can be argued to be more acceptable from a
Bayesian (or conditional) reasoning.

SUMMARY
65
2.5.5
Reconciling the Bayesian and Classical Paradigms
We have already seen some cases where the classical and Bayesian approaches
in statistical inference coincide, e.g., remark 2.11.3, while in other cases they can-
not be reconciled, e.g., remark 2.12.5. Obviously, a statistical procedure would be
preferred if it is the common solution of a classical and a Bayesian approach, since
it would enjoy interpretations from both perspectives. We discuss how one can rec-
oncile the classical and Bayesian paradigms in the context of hypothesis testing.
Classical p-values provide a basis for rejection of a hypothesis or a model and
as such they should not be discarded. In fact, we can show that for many classes
of prior distributions, the inﬁmum of the posterior probability of a composite null
hypothesis, or the prior and posterior predictive p-values, are equal to the classi-
cal p-value, for very general classes of distributions. In a spirit of reconciliation,
Micheas and Dey (2003, 2007) investigated Bayesian evidence, like the prior and
posterior predictive p-values, against the classical p-value for the one-sided loca-
tion and scale parameter hypothesis testing problem, while De la Horra (2005),
examined the two-sided case. The following example (Casella and Berger, 1987),
motivates this discussion.
Example 2.19 (Reconciling Bayesian and frequentist evidence)
Assume that
X|θ ∼N(θ, σ2) and θ ∼π(θ) ∝1, so that θ|X ∼N(x, σ2) and consider testing
Ho : θ ≤θo vs H1 : θ > θo. The posterior probability of the null hypothesis is
simply
a0 = P(θ ≤θo|x) = Φ ((θ0 −x)/σ) ,
while the classical p-value against H0 is
p = P(X ≥x|θ = θ0) = 1 −Φ ((x −θ0)/σ) = a0,
so that the classical and Bayesian approaches lead to the same result.
2.6
Summary
Our review of statistical inference in this chapter was naturally separated into
three main areas, including point and interval estimation and hypothesis testing.
There are a great number of texts that have been published on these core subjects
in statistics, with the treatment of each subject accomplished in various depths of
mathematical rigor. A theoretical treatment of the classical perspective in proba-
bility theory, distribution theory and statistical inference can be found in the clas-
sic texts by Feller (1971), Lehmann (1986) and Lehmann and Casella (1998). An
excellent treatment of the classical and Bayesian approaches, that should be ac-
cessible to readers with a rudimentary mathematical background, can be found in
Casella and Berger (2002). Versions of all the results presented, along with their
proofs, can be found in these texts. For a general presentation of the Bayesian per-
spective, the reader can turn to the books by Berger (1985), Bernardo and Smith

66
STATISTICAL INFERENCE
(2000), Press (2003), Gelman et al. (2004) and Robert (2007). Standard texts on
asymptotics include Ferguson (1996) and Lehmann (2004).
Decision theory
The proof of each of the parts of the important remark 2.12, can be found in
Lehmann and Casella (1998). Additional theorems and lemmas that provide nec-
essary and/or suﬃcient conditions for admissibility, minimaxity or the Bayes and
the extended Bayes properties, can be found in Berger (1985), Anderson (2003)
and Robert (2007). Robustness with respect to choice of loss function has been
studied in Zellner and Geisel (1968), Brown (1975), Varian (1975), Kadane and
Chuang (1978), Ramsay and Novick (1980), Zellner (1986), Makov (1994), Dey
et al. (1998), Martin et al. (1998), Dey and Micheas (2000), Rios Insua and Rug-
geri (2000), Micheas and Dey (2004), Micheas (2006), Jozani et al. (2012) and the
references therein. A proof of Blyth’s lemma can be found in Berger (1985, p. 547).
Non-parametric statistical inference
All the methods presented in this chapter involve the deﬁnition of a para-
metric model f (x|θ) and hence all methods produced are called parametric. Non-
parametric methods do not make an assumption about the form of the underlying
model for the data. Many of the procedures we have discussed have non-parametric
analogs, but they involve general characteristics of the underlying distribution in the
absence of parameters, such as quantiles or moments.
In general, suppose we are given a random sample X1, . . . , Xn, from a dis-
tribution F ∈F , where F is some abstract family of distributions, e.g., distri-
butions that have ﬁrst moments or are continuous and so on. We are interested
in estimating a functional g(F) of F, for example, g(F) = E(X) =
R
xdF(x), or
g(F) = P(X ≤a) = F(a), for given a. Most of the deﬁnitions we have seen in
the parametric setting can be considered in a non-parametric setting as well, for
example, we can ﬁnd the UMVUE of a functional. Non-parametric statistical infer-
ence methods can be found in the texts by Lehmann (1986), Lehmann and Casella
(1998), Robert (2007) and Wasserman (2009).
2.7
Exercises
Point and interval estimation
Exercise 2.1 Prove Basu’s theorem (see remark 2.10.2).
Exercise 2.2 Let Xi
iid∼Uni f (0, θ), i = 1, 2, . . ., n. Show that X(1)/X(n) and X(n) are
independent. Show that the same holds for
nP
i=1 Xi/X(n) and X(n).
Exercise 2.3 Suppose that Xi
iid∼N(θ1, θ2), θ1 ∈R, θ2 > 0, i = 1, 2, . . ., n.
(i) Find the MLE bθ of θ = (θ1, θ2)T.
(ii) Find the unbiased estimator of g(θ) = (θ1, θ2
1 + θ2)T.
(iii) Find the Fisher Information matrix IF
X(θ) and its inverse [IF
X(θ)]−1.

EXERCISES
67
(iv) Find the variance of the estimator in part (ii). Is this estimator UMVUE and
why?
(v) Find the UMVUE of θ based on the CR-LB.
Exercise 2.4
Assume that (X, Y) follows a bivariate normal distribution, with
E(X) = µ = E(Y), Var(X) = σ2
X and Var(Y) = σ2
Y. Obtain conditions so that
IF
X(µ) > IF
X|Y(µ), where IF
X|Y(µ) is computed based on the conditional distribution of
X given Y.
Exercise 2.5
Assume that (X1, X2, X3) ∼Multi(n, p1, p2, p3), a trinomial random
vector, where p1 = θ, p2 = θ(1 −θ) and 0 < θ < 1. Obtain the MLE of θ.
Note that p(x1, x2, x3) = Cn
x1,x2,x3 px1
1 px2
2 px3
3 , 0 < pi < 1, p1+p2+p3 = 1, x1+x2+x3 =
n.
Exercise 2.6 Prove the factorization theorem (see remark 2.5.1).
Exercise 2.7
Assume that X1, X2, . . . , Xn is a random sample from f (x|θ), θ ∈Θ.
Give examples of the following:
(i) The MLEbθ of θ is unique.
(ii) The MLEbθ of θ does not exist.
(iii) An uncountable number of MLEsbθ of θ exist.
(iv) The MLEbθ of θ is not a function of only the complete-suﬃcient statistic.
(v) A member of the exponential family of distributions where the suﬃcient statistic
T lives in Rp, the parameter θ lives in Rq and p > q.
Exercise 2.8 Prove the statement of remark 2.10.1.
Exercise 2.9 Suppose that Xi
iid∼Exp(a, b), i = 1, 2, . . ., n, a ∈R, b > 0, (location-
scale exponential) with f (x|a, b) = 1
be−(x−a)/b, x ≥a.
(i) Find the MLE of θ = (a, b)T.
(ii) Find the MLE of g(θ) = E(X).
(iii) Calculate E(ba) and E(bb), wherebθ = (ba,bb)T.
Exercise 2.10
Prove the statement of remark 2.10.4 as part of the proof of the
CR-LB.
Exercise 2.11 Assuming that Xi
iid∼N(µ, σ2), σ > 0, µ ∈R, i = 1, 2, . . ., n, ﬁnd the
UMVUE for g1(µ, σ2) = µσ2, g2(µ, σ2) = µ/σ2 and g3(µ, σ2) = µ2/σ2.
Exercise 2.12 Let Xi
iid∼Uni f (θ, θ + 1), θ ∈R, i = 1, 2, . . ., n.
(i) Show that T(X) = [X(1), X(n)]T is not complete.
(ii) Find the MLE of θ. Is it unique? If not, ﬁnd the class of MLEs for θ. Are these
estimators a function of the suﬃcient statistic?
Exercise 2.13 Show that if T1 and T2 are UMVUE for g(θ) then T1 = T2.
Exercise 2.14
Assume that X1, X2, . . . , Xn is a random sample from f (x|θ) =
g(x)/h(θ), a ≤x ≤θ, where a is known and g(.) and h(.) are positive, real functions.
(i) Find the MLEbθ of θ.
(ii) Find the distribution of Q = h(bθ)/h(θ).
(iii) Find the UMVUE for θ.

68
STATISTICAL INFERENCE
(iv) Repeat the problem for the model f (x|θ) = g(x)/h(θ), θ ≤x ≤b, where b is
known.
(v) Can you generalize these results for the family of distributions f (x|θ) =
g(x)/h(θ), a(θ) ≤x ≤b(θ), for some functions a(.) and b(.)?
Exercise 2.15
Assume that X1, X2, . . . , Xn is a random sample from f (x|θ) =
g(x)/h(θ), a(θ) ≤x ≤b(θ), for some functions a(.) and b(.). If bθ is the MLE for
θ, show that the 100(1 −α)% CI of minimum length for h(θ), based on the pivotal
quantity Q = h(bθ)/h(θ), is given by [h(bθ), α−1
nh(bθ)].
Exercise 2.16 Assume that a pivotal quantity Q = Q(X, θ) has a decreasing distri-
bution fQ(q), q > 0. Show that the q1 and q2 in the minimum width 100(1 −α)% CI
for θ are given by q1 = 0 and
q2R
0
fQ(q)dq = 1 −α.
Exercise 2.17 Prove the statement of remark 2.9.1.
Exercise 2.18
Let X1, X2, . . . , Xn be a random sample from f (x|θ1, θ2)
=
g(x)/h(θ1, θ2), θ1 ≤x ≤θ2, where g(.) and h(.) are positive, real functions.
(i) Show that T(X) = (X(1), X(n))T, is the complete-suﬃcient statistic for θ =
(θ1, θ2)T.
(ii) Find the UMVUE for h(θ1, θ2).
Exercise 2.19 Prove the Rao-Blackwell theorem (see remark 2.9.2).
Exercise 2.20 Suppose that X1, X2, . . . , Xn is a random sample from a Poisson(θ),
with density f (x|θ) = e−θθx/x!, x = 0, 1, . . . , θ > 0.
(i) If n = 2, show that X1 + 2X2 is not suﬃcient for θ.
(ii) Based on the whole sample, ﬁnd the UMVUE of θ2.
(iii) Based on the whole sample, ﬁnd the general form of an unbiased estimator
U(X) of e−bθ, where b is a known constant.
Exercise 2.21 Prove the Lehmann-Scheﬀ´e theorem (see remark 2.9.3).
Exercise 2.22
Assume that X1, X2, . . . , Xn is a random sample from f (x|θ) =
c(θ)eT(x)θh(x), x ∈A, free of θ and θ > 0.
(i) Find the MLE of g(θ) = c′(θ)/c(θ), where c′(.) denotes a derivative with respect
to θ.
(ii) Find an unbiased estimator of θ based on the sample.
(iii) Show that the Fisher information is given by IF
X(θ) = −d2 ln c(θ)
dθ2
.
Exercise 2.23 Prove the CR-LB inequality (see remark 2.9.4).
Exercise 2.24
Consider two independent samples X1, X2, . . . , Xm and Y1, . . . , Yn
according to Exp(a, b) and Exp(a1, b1), respectively.
(i) If a, b, a1 and b1 are all unknown, ﬁnd the suﬃcient and complete statistic.
(ii) Find the UMVUE estimators of a1 −a and b1/b.
Exercise 2.25
Let X1, X2, . . . , Xn be a random sample from
f (x|θ)
=
a(θ)b(x)I(0,θ](x), θ > 0, b(x) , 0 and a(θ) , 0, ∀θ > 0.
(i) Find the Complete-Suﬃcient statistic for θ.

EXERCISES
69
(ii) Show that the UMVUE of 1/a(θ) is T(X) = n+1
n B(X(n)), where B(x) =
xR
0
b(t)dt,
0 ≤x ≤θ.
(iii) Show that Q = a(θ)B(X(n)) is pivotal for θ and identify its distribution.
(iv) Obtain the 100(1 −α)% minimum width CIs for a(θ). (Hint: Use the general
method based on a function of the suﬃcient statistic.)
Exercise 2.26
Assume that X1, X2, . . . , Xn, is a random sample from f (x|θ) =
c(θ)h(x)exθ, x ∈A ⊂R and A is free of θ ∈R.
(i) Find the mgf of Xi.
(ii) Find the complete and suﬃcient statistic.
(ii) Find the MLEbθ of θ. What assumptions do you need to impose on c(θ) and h(x)
in order forbθ to exist and be unique?
Exercise 2.27
Assuming that X1, X2, . . . , Xn is a random sample from N(θ, θ),
θ > 0, show that X is not suﬃcient for θ and obtain the suﬃcient statistic for θ.
Exercise 2.28
Let X1, X2, . . . , Xn be a random sample from p(x|θ) = c(θ)θx/h(x),
x = 0, 1, 2, . . ., θ ∈Θ (Power series family of distributions), for some non-negative
functions c(.) and h(.).
(i) Find the complete-suﬃcient statistic for θ.
(ii) Obtain the MLE of θ and discuss assumptions on c(θ) and h(x) for its existence
and uniqueness.
(iii) Find E(X).
Exercise 2.29
Suppose that X1, X2, . . . , Xn is a random sample from f (x|θ, σ) =
σe−(x−θ)σ, x ≥θ, σ > 0 and θ ∈R.
(i) Obtain the UMVUE for θ, with σ known.
(ii) Show that Q = X(1) −θ, is pivotal for θ, when σ is known.
(iii) Assuming θ is ﬁxed, obtain the UMVUE of 1/σ using the Cram´er-Rao In-
equality.
(iii) Assuming σ is ﬁxed, obtain the 100(1 −α)% minimum width and equal tail
CIs and compare them. For what value of σ will they be equal?
Exercise 2.30 Assuming that Xi
iid∼Exp(θ), θ > 0, i = 1, . . . , n, ﬁnd the UMVUE
of θr, r > 0.
Exercise 2.31 Let Xi
iid∼N(θ, a), a > 0, θ ∈R, with a known, i = 1, . . . , n.
(i) Obtain the UMVUE of θ2.
(ii) Show that the CR-LB is not attained in this case.
Exercise 2.32
If X1, . . . , Xn
iid∼B(1, p), ﬁnd the complete-suﬃcient statistic T for
p and the UMVUE δ(T) of pm, m ≤n.
Exercise 2.33
Suppose that Xi
iid∼Uni f (θ, a), a ∈R, θ ≤a, with a known, i =
1, . . . , n. Find Q = Q(X,θ), a pivotal quantity for θ and obtain the 100(1−α)% equal
tail and minimum width CIs based on Q and compare them. Solve the problem for
Xi
iid∼Uni f (a, θ), a ∈R, a ≤θ, with a known, i = 1, . . . , n.

70
STATISTICAL INFERENCE
Exercise 2.34 Let X1, X2, . . . , Xn be a random sample from N(0, θ2), θ > 0.
(i) Find a pivotal quantity for θ that is a function of
nP
i=1 Xi.
(ii) Find the 100(1 −α)% equal tail CI for θ.
Exercise 2.35 Let X be a random variable with f (x|θ) = θ(1 + x)−(1+θ), x > 0 and
θ > 0.
(i) Obtain a pivotal quantity for θ.
(ii) Obtain the 100(1 −α)% equal tail CI for θ based on the pivotal quantity from
part (i).
(iii) Obtain the 100(1 −α)% minimum width CI for θ based on the pivotal quantity
from part (i) and compare the result with the equal tail CI from part (ii).
Exercise 2.36 Let X be a random variable with f (x|θ) = |x −θ|, θ −1 ≤x ≤θ + 1,
θ ∈R.
(i) Show that Q = X −θ is a pivotal quantity for θ.
(ii) Obtain the 100(1 −α)% equal tail CI for θ based on the pivotal quantity from
part (i).
(iii) Obtain the 100(1 −α)% minimum width CI for θ based on the pivotal quantity
from part (i) and compare the result with the equal tail CI from part (ii).
Exercise 2.37 Assume that X1, X2, . . . , Xn is a random sample from the distribution
f (x|θ) = g(x)/θ2, 0 ≤x ≤θ, with θ > 0.
(i) Find a pivotal quantity for θ2.
(ii) Obtain the 100(1 −α)% minimum width CI for θ2.
Exercise 2.38
Consider X ∼f (x|θ) =
1
2θe−1
θ |x|, x ∈R, θ > 0, a Laplace density.
First show that Q =
X
θ is pivotal for θ and then ﬁnd the equal tail and minimum
width CIs if they exist. Otherwise, ﬁnd a CI for θ and comment on the behavior of
this distribution.
Exercise 2.39
Suppose that Xi
iid∼f (x|θ), θ ∈Θ, x ∈X ⊆R, i = 1, 2, . . . , n, with
θ ∼π(θ), a priori and that the resulting posterior distribution π(θ|x) is unimodal.
Show that the 100(1 −α)% HPD credible set is also the minimum width credible
set for the given α.
Exercise 2.40 Consider Xi
iid∼Uni f (0, θ), θ > 0, i = 1, 2, . . ., n and assume a priori
θ ∼π(θ|k, x0) = kxk
0/θk+1, θ ≥x0, with x0, k, known.
(i) Find the 100(1 −α)% HPD credible set.
(ii) Show that Q = X(n)/θ is pivotal.
(iii) Compute the 100(1 −α)% minimum width CI based on Q and compare it with
the HPD set from part (i).
Exercise 2.41
Assume that X1, X2, . . . , Xn is a random sample from N(θ, 1) and
assume we entertain a N(µ, 1) prior for θ, with µ ∈R, known.
(i) Derive the posterior distribution of θ|x, where x = (x1, . . . , xn)T is the observed
sample.
(ii) Find the general form of the 100(1 −α)% HPD credible set for θ.

EXERCISES
71
Exercise 2.42
Let f (x) be a unimodal pdf and assume that the interval [L, U] is
such that: a)
UR
L
f (x)dx = 1 −α, b) f (L) = f (U) > 0, and c) L ≤x∗≤U, where x∗
is the mode of f (x). Show that [L, U] is the shortest among all intervals that satisfy
a).
Hypothesis testing
Exercise 2.43
Assume that X ∼HyperGeo(N = 6, n = 3, p) (see Appendix
A.11.4 for the pmf of the hypergeometric random variable). Find the MP test func-
tion for testing H0 : p = 1
3 vs Ha : p = 2
3, and apply it for α = 0.05 and X = 1.
Exercise 2.44 Suppose that Xi
iid∼Exp(1
θ), θ > 0, i = 1, 2, . . ., n.
(i) Find the UMP test for H0 : θ ≥θ0 vs Ha : θ < θ0 of level α.
(ii) Find the smallest value of the sample size n in order for the power of the test in
(i) to be at least 95%, when θ0 = 1000 and α = 0.05 and the alternative has value
θ1 = 500.
Exercise 2.45 Prove the Neyman-Pearson fundamental lemma (remark 2.18.1).
Exercise 2.46
Suppose that (Xi, Yi), i = 1, 2, . . . , n, is a random sample from
f (x, y|λ, µ) = λµe−λx−µy, x, y > 0, λ, µ > 0.
(i) Test the hypothesis H0 : λ ≥kµ vs Ha : λ < kµ at level α, where k is some
known constant.
(ii) What are the properties of the test you found in (i)? (UMP or UMPU or GLRT?)
Exercise 2.47 Let Xi
iid∼f (x|θ), x ∈X, i = 1, 2, . . ., n, with θ ∈Θ = {θ0, θ1}. Deﬁne
the Hellinger divergence between f (x|θ0) and f (x|θ1) by
ρ = ρ(θ0, θ1) =
R
X
p
f (x|θ0)f (x|θ1)dx.
(2.45)
(i) Show that 0 ≤ρ ≤1 with ρ = 1 if and only if f (x|θ0) = f (x|θ1).
(ii) Let ϕ(x) denote the test function of the GLRT for testing H0 : θ = θ0 vs Ha :
θ = θ1, deﬁned as
ϕ(x) =

1
λ(x) = L(θ1|x)
L(θ0|x) > c
0
λ(x) < c
,
where λ(x) =
L(θ1|x)
L(θ0|x), L(θ|x) is the likelihood function and let α and β denote the
probabilities of the type I and II errors. Show that
√cα + 1√cβ ≤ρn.
(iii) Find ρ under the models: Exp(λ), N(µ, 1) and Poisson(λ) (replace the integral
sign in 2.45 with the summation sign for the discrete model).
Exercise 2.48 Prove the statement of remark 2.19.1.
Exercise 2.49 Suppose that Xi
iid∼Exp( 1
θ1), θ1 > 0, i = 1, 2, . . ., m, independent of
Yj
iid∼Exp( 1
θ2), θ2 > 0, j = 1, 2, . . ., n.
(i) Obtain the GLRT test function for H0 : θ1 = θ2 vs Ha : θ1 , θ2 of level α, and

72
STATISTICAL INFERENCE
show that it is a function X
Y .
(ii) Find the distribution of X
Y .
Exercise 2.50
Let X1, X2, . . . , Xn be a random sample from p(x|θ) = c(θ)θx/h(x),
x = 0, 1, 2, . . ., where θ ∈Θ.
(i) Use the Neyman-Pearson lemma to derive the form of the randomized level α
UMP test function for testing H0 : θ = θ0 vs Ha : θ > θ0.
(ii) Under what conditions has this family the MLR property? Obtain the UMP test
for H0 : θ = θ0 vs Ha : θ > θ0 (using MLR theory) and compare with that in part
(i).
Exercise 2.51
Consider X1, X2, . . . , Xn a random sample from
f (x|θ)
=
c(θ)eT(x)θh(x), x ∈A, free of θ and θ > 0.
(i) For what assumptions does f have the MLR property?
(ii) Find the UMP test function for Ho : θ ≤θo vs Ha : θ > θo and a form for its
power.
Exercise 2.52 Suppose that X1, X2, . . . , Xn is a random sample from N(µ, σ2).
(i) Find the UMPU test function for Ho : µ = µo vs Ha : µ , µo, σ2 unknown.
(ii) Derive the UMP test function for Ho : σ2 = σ2
o vs Ha : σ2 < σ2
o, µ unknown.
(iii) What is the power of the test in part (ii)?
Exercise 2.53 Prove the statement of remark 2.19.2.
Exercise 2.54
Let Xi
iid∼Uni f (θ, a), a ∈R, θ ≤a, with a known, i = 1, . . . , n.
Derive the GLRT for H0 : θ = θ0 vs Ha : θ , θ0. Solve the problem for Xi
iid∼
Uni f (a, θ), a ∈R, a ≤θ, with a known, i = 1, . . . , n.
Exercise 2.55
Consider the linear model Y = Xβ + ε, ε ∼Nn(0, σ2In),
rank(Xn×p) = p, where β =

β1
q×1
β2
r×1

, p = q + r. Obtain the GLRT for the hypothesis
Ho : A
s×qβ1 = B
s×rβ2 vs Ha : A
s×qβ1 , B
s×rβ2, for some matrices A and B.
Exercise 2.56
Suppose that X1, X2, . . . , Xn
iid∼N(µx, nm) and Y1, Y2, . . . , Ym
iid∼
N(µy, nm), two independent samples. Derive the form of the power, of the level
α test for Ho : µx = µy vs Ha : µx = µy + 1, that rejects Ho when X −Y ≥c, where
c is some constant to be found.
Exercise 2.57 Assume that X1, X2, . . . , Xn is a random sample from an exponential
distribution with location parameter θ, i.e., f (x|θ) = e−(x−θ), x ≥θ ∈R. Consider
testing Ho : θ ≤θo vs Ha : θ > θo. Find the UMP test function of level α and the
classical p-value.
Exercise 2.58 Suppose that Xi
iid∼Gamma(a, θ), i = 1, 2, . . ., n, where θ > 0 is the
scale parameter and α > 0 is a known shape parameter. Show that this gamma has
the MLR property and obtain the UMP test function of level α for testing Ho : θ ≥
θo vs Ha : θ < θo. In addition, ﬁnd the classical p-value.

EXERCISES
73
Exercise 2.59 Consider a random sample X1, X2, . . . , Xn from a Pareto distribution
with pdf f (x|θ) = aθa/xa+1, x ≥θ, scale parameter θ > 0 and known shape parame-
ter α > 0. For testing Ho : θ ≤θo vs Ha : θ > θo, ﬁnd the classical p-value, the prior
and the posterior predictive p-value assuming a priori that θ ∼Exp(λ), with λ > 0,
known. Compare the three measures of evidence against H0.
Exercise 2.60
Let Xi
iid∼Uni f (−θ, θ), θ > 0, i = 1, 2, . . ., n and consider a prior
for θ ∼Gamma(a, b), with a > 2, b > 0, known. Find the Bayes factor for testing
Ho : 0 < θ ≤θo vs Ha : θ > θo.
Exercise 2.61 Consider testing a point null H0 : θ = θ0 vs H0 : θ > θ0, based on a
random sample X1, . . . , Xn ∼f (x|θ), θ > 0, using a prior from the ε−contamination
class Γε = {π : π(θ) = (1−ε)δθ0(θ)+επa,b(θ)}, where δθ0(θ) = I(θ = θ0), a point mass
distribution at θ0 (Dirac distribution) and πa,b is a Gamma(a, b), density, a, b > 0.
Find the general form of the Bayes factor, the prior and the posterior predictive
p-values.
Decision theoretic
Exercise 2.62
Suppose that Xi
iid∼Exp(θ), i = 1, 2, . . ., n. Consider the loss
function L(θ, δ) = θ−2(θ −δ)2 (WSEL) and deﬁne the class of decision rules
δk = δk(X) = 1/Xk, Xk = 1
k
kP
i=1 Xi, k = 1, 2, . . ., n.
(i) Obtain the (frequentist) risk in estimating θ using the above δk and under L(θ, δ),
for any k = 1, 2, . . ., n.
(ii) Find the admissible estimator of θ in the class of decisions δk, k = 1, 2, . . ., n.
(iii) Consider the prior distribution on θ given by π(θ) = λe−λθ, θ > 0 and λ a ﬁxed
positive constant. Obtain the Bayes rule for θ with respect to this prior.
(iv) Will the two estimators you found in parts (ii) and (iii) ever be equal?
Exercise 2.63 Prove theorem 2.4.
Exercise 2.64
Suppose that X1, X2, . . . , Xn is a random sample from Poisson(λ),
where λ ∼Gamma(a, b), with a, b > 0, known. Find the Bayes rule for λ under the
weighted square error loss function L(λ, δ) = λ2(λ −δ)2.
Exercise 2.65 Suppose that T(X) is unbiased for g(θ), with Var(T(X)) > 0. Show
that T(X) is not Bayes under SEL for any prior π.
Exercise 2.66
Assume that X1, X2, . . . , Xn are independent where Xi ∼f (x|θi).
Show that, for i = 1, 2, . . . , n, if δπi
i (Xi) is the Bayes rule for estimating θi using
loss Li(θi, ai) and prior πi(θi), then δπ(X) = (δπ1
1 (Xn), . . . , δπn
n (Xn)) is a Bayes rule for
estimating θ = (θ1, . . . , θn) using loss
nP
i=1 Li(θi, ai) and prior π(θ) =
nQ
i=1 πi(θi).
Exercise 2.67 Prove theorem 2.5.
Exercise 2.68
Suppose that X = (X1, . . . , Xp)′ ∈X ⊆Rp is distributed according
to the exponential family: f (x|θ) = c(θ)eθ′xh(x), with h(x) > 0, on X. The parameter
θ ∈Θ = {θ :
R
X
h(x)eθ′xdx < ∞}, is to be estimated under quadratic loss L(θ, δ) =

74
STATISTICAL INFERENCE
(θ −δ)′W(θ −δ), for some weight matrix W.
(i) Show that the (generalized) Bayes rule with respect to a (possibly improper)
prior π, is given by
δπ(x) = E(θ|x) =
1
m(x|π)
R
Θ
θ f (x|θ)π(θ)dθ = ∇ln m(x|π) −∇ln h(x),
where m(x|π) =
R
Θ
f (x|θ)π(θ)dθ, the marginal with respect to π. What assumptions
are needed to help you prove this?
(ii) Suppose that X ∼Np(θ, Σ), with Σ > 0, known. Find the form of δπ(x) in part
(i).
Exercise 2.69
Consider X ∼N(θ, 1) and we wish to estimate θ under SEL. Use
Blyth’s lemma to show that δ(x) = x is admissible.
Exercise 2.70
Assume that Xi
iid∼N(θi, 1), i = 1, 2, . . ., p, p > 2 and deﬁne an
estimator δc = (δ1c, . . . , δpc)′ of θ = (θ1, . . . , θp), by δic =

1 −c(p −2)/S 2
Xi,
where S 2 =
pP
i=1 X2
i .
(i) Show that the risk of δc(x) under the average SEL function L(θ, d) = 1
p
pP
i=1(di−θi)2
is R(θ, δc) = 1 −(p−2)2
p
E

2c −c2
/S 2
.
(ii) Show that the estimator δc(X) dominates δ0(X) = X (for c = 0), provided that
0 < c < 2 and r ≥3.
(iii) Show that the James-Stein estimator δ1(X) = δJS (X) (for c = 1), dominates all
estimators δc, with c , 1.
Exercise 2.71
Let δ be the Bayes rule (UMVUE, minimax, admissible) for g(θ)
under SEL. Show that aδ + b is the Bayes rule (respectively, UMVUE, minimax,
admissible) for ag(θ) + b, where a, b are real constants.
Exercise 2.72 Prove all parts of remark 2.12.
Exercise 2.73 Assume that X ∼N(θ, 1) and we wish to estimate θ under absolute
value loss, L(θ, a) = |θ −a|. Show that δ(x) = x is admissible.
Exercise 2.74 Show that a Bayes estimator is always a function of the complete-
suﬃcient statistic.
Exercise 2.75 Assume that Xi
iid∼Np(µ, Ip), i = 1, 2, . . ., n and consider estimation
of µ under quadratic loss: L(µ, δ) = (µ −δ)T(µ −δ) = ∥µ −δ∥2 .
(i) Find the risk function for δ(X) = X = 1
n
nP
i=1 Xi.
(ii) Show that X is not admissible, p ≥3.
(iii) Suppose that µ ∼Np(0, Ip). Find the Bayes rule for µ and compare its frequen-
tist risk with that in part (i).
Exercise 2.76 Prove Blyth’s lemma.
Asymptotics

EXERCISES
75
Exercise 2.77 Prove theorem 2.1.
Exercise 2.78 Assuming that X ∼Binom(n, p), show that Tn = (X+1)/(n+2)
p→p.
Exercise 2.79
Suppose that {Xn} is an iid sequence of random variables with
P(Xn = k/n) = k/n, k = 1, 2, . . . , n. Show that Xn
w→X ∼Uni f (0, 1).
Exercise 2.80 Assume that Xi
iid∼Uni f (0, θ), θ > 0, i = 1, 2, . . ., n. Show that the
MLE of θ is a consistent estimator.
Exercise 2.81 Prove theorem 2.2.
Exercise 2.82 Prove the usual Central limit theorem (CLT), namely, let X1, X2 . . . ,
be iid random vectors with mean µ and ﬁnite covariance matrix, Σ, and prove that
Yn = √n(Xn −µ)
w→Np(0, Σ), where Xn = 1
n
nP
i=1 Xi. Hint: Use Taylor’s theorem
(appendix remark A.1.8) on the cf of Yn.
Exercise 2.83 Prove theorem 2.3.
Simulation and computation: use your favorite language to code the functions
Exercise 2.84
Let U
∼
Uni f (0, 1). Use simulation to approximate: a)
Cov(U,
√
1 −U2), b) Cov(U2,
√
1 −U2), c) Cov(U, eU).
Exercise 2.85
Use the Monte Carlo approach to approximate the integrals:
a)
+∞
R
−∞
exp(−x2)dx, b)
+∞
R
0
x(1 + x2)−2dx, c)
2R
−2
exp(x + x2)dx, d)
+∞
R
0
e−x/2dx, e)
0R
−∞
1R
−∞
e−x2−y2dxdy, and f)
10R
1
x
Γ(x)dx.
Exercise 2.86 Write functions that would:
(i) generate a random sample from N(µ, σ2),
(ii) compute the MLE values of the parameters and then the values of the likelihood
and the log-likelihood.
(iii) Run your code for the models: N(0, 1), N(10, 9) and comment on the results.
Exercise 2.87 Let Xi
iid∼Exp(θ), i = 1, 2, . . ., n. Write a function that would:
(i) take as argument data X1, . . . , Xn and compute and print the MLE of θ (in the
console). Run your function for n = 20 realizations from an Exp(10) distribution.
(ii) take as argument data X1, . . . , Xn and constants a, b > 0, and compute and
display the MAP of θ under a Gamma(a, b) prior. Run your function for n = 10
realizations from an Exp(5) distribution and a = 2, b = 3.
Exercise 2.88 Now consider Xi
iid∼Gamma(a, b), i = 1, 2, . . ., n, a, b > 0 unknown.
Compute the MLEs and then the values of the likelihood and the log-likelihood.
Use the Newton-Raphson method if you cannot get the MLEs in closed form (recall
remark 2.7). Run the code for the models Gamma(a = 2, b = 1) and Gamma(a =
10, b = 10), by ﬁrst simulating a sample of size n = 100 and then pass the generated
values to your function.
Exercise 2.89
Write a function that would approximate and plot the prior and

76
STATISTICAL INFERENCE
posterior predictive distribution (recall remark 2.26 and use equation (2.42)) for a
model X ∼N(µ, σ2), with σ2 known, and prior µ ∼N(0, 1). Produce the plots for
σ2 = 1, 2, 3.
Exercise 2.90 Using the setup of exercise 2.59, write a function that would com-
pute and return the classical p-value, the prior and the posterior predictive p-value,
as well as return the power of the test at some θ1. Run the function for n = 50 real-
izations from a Pareto model with a = 5, θ = 10, and assume that θo = 9, θ1 = 9.5,
with the prior having hyperparameter λ = 0.1. Comment on the results.
Exercise 2.91
Write a function that computes the HPD credible set of example
2.10. Generate a sample of size n = 100 from an Exp(θ = 5), let λ = 1, and plot the
posterior along with two vertical lines at the values of θ that give the 100(1 −α)%
HPD and a horizontal line at the k∗level. Use α = 0.1, 0.05, and 0.01 when you run
the function (make α an argument of the function).
Exercise 2.92 Using the setup of exercise 2.60, write a function that computes the
classical p-value, the prior and posterior predictive p-values and the Bayes factor.
Generate and pass to your function n = 100 realizations from a Uni f (−10, 10),
θ = 10, π0 = π1 = 1
2, and consider a prior with a = 10, b = 1. Use θ0 = 8 and ﬁnd
the power of the test at θ1 = 9. Comment on your ﬁndings and draw conclusions
with respect to the hypothesis at hand.
Exercise 2.93
Let Xi
iid∼Binom(m, p), i = 1, 2, . . ., n, and consider a prior p ∼
Beta(a, b). Write a function that would take as arguments the data X1, . . . , Xn the
hyper-parameters a, b > 0 and would perform the following tasks:
(i) Plot the posterior distribution in solid line and the prior distribution in a dashed
line on the same plot.
(ii) Obtain the 100(1 −α)% CS for p given some α.
(iii) Obtain the 100(1 −α)% HPD CS for p given some α.
(iv) Obtain the Bayes factor for testing H0 : p = 1
2 vs H1 : p , 1
2 (assume p ∼
Beta(a, b) for p , 1
2).
(v) Run your function for the following cases and report on your ﬁndings: (a) n = 20
generated data from Binom(5, .45), a = 2, b = 2, (b) n = 10 generated data from
Binom(10, .95), a = 1, b = 1, and (c) n = 100 generated data from Binom(20, .5),
a = 2, b = 1.
Exercise 2.94
Consider an iid sample X1, . . . , Xn from a N(0, 3), and write code
that would help you conduct a Monte Carlo goodness-of-ﬁt test based on two
test statistics, the sample mean and variance (see remark 2.26 and example 2.18).
Run your code for data generated from four models: N(0, 1), N(0, 3), N(0, 4) and
N(0, 9) and report on your ﬁndings (use n = 30).

Chapter 3
Measure and Integration Theory
3.1
Introduction
Measure theory provides tools that allow us to eﬃciently quantify or measure
sets of points, where by point here we could mean anything, from real vectors,
to matrices and functions, to points that are collections of points themselves. A
measure on a set provides a systematic way to assign a number to each suitable
subset of that set, intuitively interpreted as its size, and consequently, we can think
of a measure as a generalization of the concepts of length, area and volume.
The exposition of this chapter is mathematically demanding, requiring knowl-
edge of topology, real analysis, advanced calculus and integration theory in order
to fully comprehend the underlying structures of the spaces, σ-ﬁelds, functions and
integrals involved. Consequently, as we study material from this chapter, it would
help us to have some classic texts from mathematics nearby (such as Rubin, 1984,
Royden, 1989, Dudley, 2002 and Vestrup, 2003).
3.2
Deterministic Set Theory
We begin with some results on deterministic sets. We collect basics of deter-
ministic mappings in Appendix A.1.
Remark 3.1 (Deterministic set theory basics) Let X, Y be some spaces and con-
sider subsets A ⊆X and B ⊆X. Denote by ∅the empty set (set with no points from
X) and Ac = {x ∈X : x < A}, the complement of A.
1. The Cartesian product of A and B is deﬁned as A × B = {(x, y) : x ∈A, y ∈
B}. We easily generalize to the n-fold product by deﬁning Xn = X × · · · × X =
{(x1, . . . , xn) : xi ∈X, i = 1, 2, . . .n}.
2. The symmetric diﬀerence between the sets A and B is deﬁned by A △B =
(A ∖B) ∪(B ∖A), where A ∖B = {x ∈X : x ∈A and x < B} = A ∩Bc.
3. Let C be a collection of sets from X. Then we deﬁne
∩
A∈CA = ∩{A : A ∈C} = {x ∈X : i f A ∈C =⇒x ∈A},
77

78
MEASURE AND INTEGRATION THEORY
and
∪
A∈CA = ∪{A : A ∈C} = {x ∈X : ∃A such that A ∈C and x ∈A},
and consequently, we can show that

∪
A∈CA
c
= ∩
A∈CAc and

∩
A∈CA
c
= ∪
A∈CAc.
4. If Ai ⊆X, i = 1, 2, . . ., then we deﬁne
+∞
S
i=1 Ai = {x ∈X : ∃i such that x ∈Ai} and
+∞
T
i=1 Ai = {x ∈X : x ∈Ai, ∀i}.
5. A nonempty collection P of subsets of a nonempty space X is called a π−system
over X if and only if A, B ∈P =⇒A∩B ∈P, that is, P is closed under intersections.
Note that this implies that P is closed under ﬁnite intersections.
6. A nonempty collection L of subsets of a nonempty space X is called a λ−system
over X if and only if: (i) X ∈L, (ii) if A ∈L =⇒Ac ∈L, (iii) for every disjoint
sequence {An}+∞
n=1, of L-sets, we have
+∞
S
i=1 Ai ∈L, that is, L is closed under countable
disjoint unions.
7. We denote by 2X the collection of all subsets of the space X.
8. Note that the empty set ∅need not be a member of a collection of sets, even if
by deﬁnition ∅⊆X.
9. Assume that A1, A2, . . . , is a sequence of sets from X and deﬁne the limit inferior
and superior by
limAn = lim sup
n
An =
+∞
T
n=1
+∞
S
k=n Ak
and
limAn = lim inf
n An =
+∞
S
n=1
+∞
T
n=k Ak.
Then lim
n→∞An exists when
lim
n→∞An = limAn = limAn.
We attach the additional interpretation to limAn by saying An inﬁnitely often (i.o.)
and to limAn by saying An eventually (ev.).
10. Set-theoretic operations on events are easily represented using indicator func-
tions. In particular, we have the following: IAc = 1 −IA, IA∖B = IA(1 −IB),
IA∩B = IA ∧IB = IAIB, IA∪B = IA ∨IB = IA + IB −IAIB, I∩nAn = inf
n IAn = Q
n IAn,
I∪nAn = sup
n
IAn and IA△B = IA + IB −2IAIB. In terms of indicator functions we
may rewrite the limit deﬁnition of a sequence of sets as lim
n→∞An = A if and only if
IA = lim
n→∞IAn.

TOPOLOGICAL SPACES AND σ-FIELDS
79
A ﬁrst useful collection of subsets from a space X is deﬁned next.
Deﬁnition 3.1 Field
A collection A of subsets of X is called a ﬁeld (or algebra) of sets or simply a ﬁeld,
if (i) ∅∈A, (or X ∈A), (ii) Ac ∈A, ∀A ∈A, and (iii) A ∪B ∈A, ∀A, B ∈A.
Some comments are in order regarding this important deﬁnition.
Remark 3.2 (Field requirements) Requirement (i) guarantees that A is nonempty.
Some texts do not require it since it is implied by (ii) and (iii), when there is at least
one set in the collection A. Indeed, if A ∈A, X = A ∪Ac ∈A, so that X ∈A and
∅c = X ∈A. Noting that (Ac ∪Bc)c = A ∩B, for any A, B ∈A, we see that A is a
π−system.
Next we collect an existence theorem regarding a minimal ﬁeld based on a
collection of subsets of a space X.
Theorem 3.1 (Generated ﬁeld) Given a collection C of subsets of a space X,
there exists a smallest ﬁeld A such that C ⊆A, i.e., if there exists a ﬁeld B such
that C ⊆B, then A ⊆B.
Proof. Take F as the family of all ﬁelds that contain C and deﬁne A = ∩{B :
B ∈F }.
3.3
Topological Spaces and σ-ﬁelds
The collections of subsets from a space X that will allow us to eﬃciently assign
numbers to their elements are known as σ-ﬁelds.
Deﬁnition 3.2 σ-ﬁeld
A collection A of subsets of a space X is called a σ-ﬁeld if
(i) ∅∈A (or X ∈A),
(ii) Ac ∈A, ∀A ∈A, and
(iii) every countable collection of sets from A, is in A, i.e., ∀Ai ∈A, i = 1, 2, . . .,
we have
+∞
S
i=1 Ai ∈A.
A σ-ﬁeld A0 that satisﬁes A0 ⊆A, where A is a σ-ﬁeld, is called a sub-σ-ﬁeld.
As we will see later in this chapter, the σ-ﬁelds we end up using in our deﬁni-
tions are typically generated σ-ﬁelds based on a smaller collection of easy-to-use
sets. The following is analogous to the concept of a generated ﬁeld and is given as
a deﬁnition since there always exists a σ-ﬁeld containing any collection of sets C,
the σ-ﬁeld 2X.

80
MEASURE AND INTEGRATION THEORY
Deﬁnition 3.3 Generated σ-ﬁeld
The smallest σ-ﬁeld A containing a collection of sets C is called the generated
σ-ﬁeld from C and is denoted by A = σ(C). We may also write A = T
C⊆B{B : B a
σ −f ield}.
Let us collect some comments and important results on σ-ﬁelds.
Remark 3.3 (Properties of σ-ﬁelds) We note the following.
1. The generated σ-ﬁeld for a collection of sets C exists always, since there exists
a σ-ﬁeld that contains all sets of X, namely, 2X, the largest σ-ﬁeld of subsets of X.
2. Sub-σ-ﬁelds play a very important role in probability theory since we can attach
to them the interpretation of partial information. See for example Section 6.3.1 and
the idea behind ﬁltrations.
3. We can easily show that C ⊆σ(C), for any collection C of subsets of X. By
deﬁnition, C = σ(C) when C is a σ-ﬁeld.
4. If C1 and C2 are collections of subsets of X then C1 ⊆C2 =⇒σ(C1) ⊆σ(C2).
To see this, note that
σ(C1) = T
C1⊆B
{B : B a σ-ﬁeld} ⊆T
C2⊆B
{B : B a σ-ﬁeld} = σ(C2),
since in the second intersection we intersect over a smaller number of σ-ﬁelds
(since C1 ⊆C2) and thus we obtain larger collections of sets.
5. If C1 ⊆σ(C2) and C2 ⊆σ(C1) then σ(C1) = σ(C2). This provides us with an
elegant way of showing that two σ-ﬁelds are equal. To prove this take C1 ⊆σ(C2)
and use part 4 of the remark to obtain σ(C1) ⊆σ(σ(C2)) = σ(C2), with the latter
equality the result of part 3 of the remark. Similarly, the assumption C2 ⊆σ(C1)
leads to σ(C2) ⊆σ(C1), which establishes the other direction, so that σ(C1) =
σ(C2).
6. Let C = {Gk : k ∈K} be a collection of sub-σ-ﬁelds of a σ-ﬁeld G. The
intersection of all elements of C is easily shown to be a σ-ﬁeld with T
k∈K Gk = {G :
G ∈Gi, for all i ∈K}, whereas the union, S
k∈K Gk = {G : G ∈Gi, for some i ∈K},
is not necessarily a σ-ﬁeld. We write σ (G1, G2, . . . ) for σ
 S
k∈K Gk
!
, the generated
σ-ﬁeld based on all members of the collection C.
7. Dynkin’s π −λ theorem. Many uniqueness arguments can be proven based on
Dynkin’s π−λ theorem: if P is a π-system and L is a λ-system, then P ⊆L implies
σ(P) ⊆L.
8. Let f : X →Y, some map and C a collection of subsets of Y and deﬁne the

TOPOLOGICAL SPACES AND σ-FIELDS
81
inverse map of the collection C by f −1(C) = {E ⊂X : E = f −1(C) for some C ∈C}.
Then f −1(σ(C)) is a σ-ﬁeld on X and σ(f −1(C)) = f −1(σ(C)).
Example 3.1 (Union of σ-ﬁelds) Consider the space X = {a, b, c} and let C =
{{a}, {b}, {c}}, the collection of all singletons of X. Then 2X = σ(C) = {∅, X,
{a}, {b}, {c}, {a, b}, {a, c}, {b, c}}, is the largest σ-ﬁeld of subsets of X. Now let
A1 = σ({a}) = {∅, X, {a}, {b, c}} and A2 = σ({b}) = {∅, X, {b}, {a, c}}, two sub-
σ-ﬁelds of 2X. Then A1 ∪A2 = {∅, X, {a}, {b}, {b, c}, {a, c}}, with {a} ∈A1 ∪A2,
{b} ∈A1 ∪A2, but {a} ∪{b} = {a, b} < A1 ∪A2, so that A1 ∪A2 is not a σ-ﬁeld.
Note that A1 ∩A2 = {∅, X}, which is a trivial σ-ﬁeld.
A topological space is another important collection of subsets of a space X and
it helps us deﬁne what we mean by open sets. In particular, the celebrated Borel
σ-ﬁeld B(X) of the space X is the σ-ﬁeld generated by the open sets of X, i.e.,
B(X) = σ(O(X)). Appendix A.2 presents details on topological spaces and the
Borel σ-ﬁeld.
Deﬁnition 3.4 Borel σ-ﬁeld
Let O = O(X), denote the collection of open sets of a space X, so that (X, O) is a
topological space. The Borel σ-ﬁeld B of a space X is deﬁned by B = σ(O), that
is, B is the generated σ-ﬁeld from the collection of open sets. We write B = B(X)
and members of B are called Borel sets.
See appendix remark A.3 for additional results on Borel σ-ﬁelds. Next we col-
lect one of the most important generated Borel σ-ﬁelds, that over R.
Theorem 3.2 (Generating the Borel σ-ﬁeld in R) The Borel σ-ﬁeld in R,
B(R), can be generated by the collection of all open intervals of R, i.e., B(R) =
σ({(a, b) : ∀a < b ∈R}).
Proof. Recall that B = B(R) = σ(O), where O denotes the collection of all
open sets of R. Note that the collection of open intervals satisﬁes I = {(a, b) : ∀a <
b ∈R} ⊆O, so that remark 3.3.4 yields σ(I) ⊆σ(O) = B. For the other direction
it is enough to point out that each O ∈O, can be written using remark A.3.4 as
O =
+∞
S
n=1(an, bn) ∈σ(I), so that σ(O) ⊆σ(I) and hence B = σ(O) = σ(I).
Some consequences of this important theorem are given next.
Remark 3.4 (Generating the Borel σ-ﬁeld) This important result can be extended
to many other collections of intervals of R. In particular, we can show that
B(R)
=
σ({[a, b]}) = σ({(a, b]}) = σ({[a, b)}) = σ({(a, b)})
=
σ({(−∞, b)}) = σ({[a, +∞)}) = σ({(a, +∞)})
=
σ({all closed subsets of R}) = σ    j2−n, (j + 1)2−n , j, n, integers	 ,

82
MEASURE AND INTEGRATION THEORY
and more importantly, B(R) = σ({(−∞, b]}), that is, the Borel σ-ﬁeld is gen-
erated by sets we would use to deﬁne the cdf of a random variable X, with
FX(x) = P(X ≤x). Important results like theorem 3.2 and their extensions, allow us
to deﬁne probability measures by assigning probabilities to these sets only (a much
smaller class of sets compared to B(R)) and based on them, obtain probabilities for
any Borel set.
Example 3.2 (Cylinder sets) Assume that we equip Rp with the Euclidean met-
ric
ρ(x, y) =
s
pP
i=1(xi −yi)2,
for x = (x1, . . . , xp)T, y = (y1, . . . , yp)T, vectors in Rp. This leads to the deﬁnition of
open balls (see Appendix remark A.5), which leads to the description of O(Rp), the
open sets in Rp, which in turn induces a topology in Rp, so that we can deﬁne the
Borel sets B(Rp) = σ(O). Hence, we have an idea about how to create the open sets
in Rp. But how do we deﬁne the open sets in R∞? Let us create a topology without
the use of a metric. Let i1 < i2 < · · · < in, be arbitrary integers, n ≥1 and consider
open sets Oi1, . . . , Oin, in R. We construct special sets of elements from R∞given
by
Cn
=
i1−1
×
i=1 R × Oi1 × R × · · · × R × Oi2 × · · · × OiN × R × R × . . .
=
{(x1, x2, . . . ) : xi1 ∈Oi1, xi2 ∈Oi2, . . . , xin ∈Oin},
known as cylinder open sets. Then any set O ∈O(R∞), can be represented as
a union of cylinder open sets and the generated σ-ﬁeld yields the Borel σ-ﬁeld
B(R∞) = σ(O(R∞)). Cylinder open sets provide the necessary ingredient of a prod-
uct topology and we discuss the general framework next.
3.4
Product Spaces
The space X considered in the last section has elements of any type, including
real numbers, vectors, matrices, functions and so forth and can be used to describe
the values of a random variable of an experiment. For example, when we collect
a random sample X1, . . . , Xp ∼N(0, 1), in order to describe the probability dis-
tribution of the random vector X = (X1, . . . , Xp)T ∼Np(0, Ip) and investigate its
properties, we need to consider the p-fold Cartesian product of R, leading to the
product space Rp = R × · · · × R.
Since we have studied the case of R (one dimension), we would like to extend
results from R to the space Rp. Example 3.2 shows that this generalization is not
always obvious, especially when it comes to σ-ﬁelds like the Borel σ-ﬁeld. If we
consider a random sample X1, . . . , Xn, from Np(0, Ip), the joint distribution of the
random vector Xn = (XT
1 , . . . , XT
n )T ∼Nnp(0, Inp), must be studied in the space
Rnp = Rp × · · · × Rp = (R × · · · × R) × · · · × (R × · · · × R). Sending n to +∞, results

PRODUCT SPACES
83
in an inﬁnite dimensional random vector X∞= (XT
1 , . . . , XT
n , . . . )T, which lives in
the space R∞and needs to be handled appropriately.
The general deﬁnition of the product space and topology is given next.
Deﬁnition 3.5 Product space and topology
Let J be an arbitrary index set and assume that (X j, O j), j ∈J, is a topological
space. The product space is deﬁned as the Cartesian product
X = ×
j∈J X j = {x = (x1, x2, . . . ) ∈X : xj ∈X j, j ∈J}.
The product topology O = O(X) on X consists of the collection of all (arbitrary)
unions of members from the collection of sets
N = { ×
j∈JO j : O j ∈Oj and O j = X j, for all but ﬁnitely many j ∈J},
that is, O = {O ⊆X : O = S
i∈I
Ni, Ni ∈N, i ∈I}. Then the pair (X, O) is a
topological product space. We use the generic term “open rectangles” or “cylinder
open” to describe the sets of N.
We collect some comments and classic results on product spaces below.
Remark 3.5 (Product space and topology) The construction above is not unique.
It should be noted that this deﬁnition appears in some texts as a lemma that needs
to be proven.
1. Tychonoﬀ’s theorem A product of compact spaces is a compact space.
2. Since R is compact, R
∞=
+∞
×
j=1R, is compact, that is, the set of all sequences in R
is compact.
3. The phrase “and O j = X j, for all but ﬁnitely many j ∈J” is included in the
deﬁnition to stress the fact that we do not select as an open set O j the space X j, for
all j. See for example the cylinder sets of example 3.2.
4. For any product space X =
p×
j=1 X j, we deﬁne the ith projection function as
the map Zi : X →Xi, that yields the ith coordinate, namely, Zi(x) = X(i),
X = (X(1), . . . , X(p)) ∈X. We can think of X as a random vector and Zi(X) as
a random variable and consider that by construction X is known if and only if all
the projection maps are known. This relationship allows us, for example, to imme-
diately generalize results for random variables to random vectors by studying the
projections that are obviously easier to handle.
5. Writing X = ×
j∈J X j, leaves no room for questioning the deﬁnition of X as a
Cartesian product. However, writing D = ×
j∈J Oj has only notational usefulness and
should not be misunderstood as the product topology O(X). Indeed, if O j ∈Oj, then

84
MEASURE AND INTEGRATION THEORY
we can write O = ×
j∈JO j ∈D, if we were to follow the Cartesian product deﬁnition.
However, O as deﬁned is only one of the possible sets in N, so that D ⊆O(X),
with O(X) being a much larger collection of sets. To see this, consider X = R2,
with O(R2) = {O = S Ni : Ni ∈N}, where N = {N : N = O1 × O2, O1, O2 ∈O(R)},
contains the open rectangles of R2 and O(R) the usual Euclidean topology of R.
Let N1 = (0, 1) × (0, 2), N2 = (1, 2) × (0, 3) ∈O(R2), so that N1 ∪N2 ∈O(R2),
where N1 ∪N2 = {(x, y) ∈R2 : x ∈(0, 1) ∪(1, 2), y ∈(0, 2) ∪(0, 3)}. Thus we have
Ni ∈O(R) × O(R) = O(R)2 = {O1 × O2 : O1, O2 ∈O(R)}, i = 1, 2, but N1 ∪N2
does not belong in O(R)2 and hence O(R)2 is not a topology. This happens because
nS
i=1(Oi1 × · · · × Oip) ⊂
,
nS
i=1 Oi1 × · · · ×
nS
i=1 Oip, in general. To unify the notation and
avoid the confusion with the Cartesian product, we will write
×
j∈JX j,
N
j∈J
Oj
for
the topological product space (X, O).
6. Suppose that X = Rp. The bounded rectangles are deﬁned as
R = {x = (x1, . . . , xp) ∈Rp : ai < xi ≤bi, i = 1, 2, . . ., p},
and play a similar role in Rp, as the intervals {(a, b)} play in R (see theorem 3.2),
that is, they generate the Borel σ-ﬁeld in Rp. When ai, bi are rationals, we say
that R is a rational rectangle. Now if O is an open set in Rp and y ∈O, then there
is a rational rectangle Ay such that y ∈Ay ⊂O. But then O = S
y∈O Ay and since
there are only countably many rational rectangles, the latter is a countable union.
Consequently, the open sets of Rp are members of the generated σ-ﬁeld from these
rational rectangles, i.e., O ⊆σ({R}) and hence B(Rp) = σ(O) ⊆σ({R}). The other
direction is trivial, since R ∈O(Rp), so that {R} ⊆O and hence σ({R}) ⊆σ(O).
7. When J = {1, 2, . . .}, the inﬁnite product space X =
+∞
×
j=1X j, can be thought of
as the space of all inﬁnite sequences, namely, X = {(x1, x2, . . . ) : xi ∈Xi, for all
i = 1, 2, . . .}. An alternative deﬁnition of cylinder sets in this case can be given as
follows: choose a set B(n) ⊂
+∞
×
j=1X j and deﬁne the cylinder set Bn ⊂X, with base B(n)
to be Bn = {x ∈X : (x1, x2, . . . , xn) ∈B(n)}. If B(n) = A1 × A2 × · · · × An, Ai ⊂Xi,
then the base is called a rectangle and need not be open.
8. An alternative deﬁnition of the inﬁnite product σ-ﬁeld
+∞
N
n=1
An (see deﬁnition
3.6), can be given in terms of the generated σ-ﬁeld from the collection of all mea-
surable cylinders {Bn}, where a cylinder is called measurable if its base B(n) ∈An,
is measurable. The resulting product σ-ﬁelds are the same, no matter what collec-
tion of sets we choose to generate it, be it measurable rectangles or measurable
cylinders.
9. When working with product spaces it is useful to deﬁne the cross section of a

PRODUCT SPACES
85
set. If A is a subset of X = X1 × X2 and x a point of X1, we deﬁne the x cross
section Ax by Ax = {y ∈X2 : (x, y) ∈A} and similarly for the y cross section of A,
we set Ay = {x ∈X1 : (x, y) ∈A}. The following are straightforward to prove: (i)
IAx(y) = IA(x, y), (ii) (Ac)x = (Ax)c, and (iii)
 S
i Ai
!
x
= S
i (Ai)x, for any collection
{Ai}.
Products of larger classes of sets than topologies, i.e., σ-ﬁelds, are considered
next.
Deﬁnition 3.6 Product σ-ﬁeld
Suppose that A j, j ∈J, are σ-ﬁelds with J some index set. A measurable rect-
angle is a member of R∗= ×
j∈JA j = {A : A = ×
j∈JA j, A j ∈A j}. The product σ-ﬁeld
A is deﬁned as the generated σ-ﬁeld from the collection of measurable rectangles,
namely, A = σ
 
×
j∈JA j
!
and we write A =
N
j∈J
A j.
Next we collect two important results that utilize measurable rectangles.
Remark 3.6 (Rectangles and the Borel product σ-ﬁeld) Some comments are in
order.
1. Let U denote the collection of all ﬁnite disjoint unions of measurable rectan-
gles, that is, if U ∈U then U =
nS
i=1 Ri, where Ri ∈R∗. Then it can be shown that U
is a ﬁeld on X1 × · · · × Xn that generates A, that is, A = σ(U).
2. The Borel product σ-ﬁeld of Rp is given by B(Rp) =
p
N
j=1
B(R) = σ
 
p×
j=1B(R)
!
and it will be denoted by Bp = B(Rp). Note that the Borel product space in R2 is not
the Cartesian product of the Borel σ-ﬁeld on R with itself, that is, B(R) × B(R) ⊂
,
B(R2).
Example 3.3 (Borel product σ-ﬁeld) We show that Bp =
p
N
j=1
B1 = σ
 
p×
j=1B1
!
,
without the use of deﬁnition 3.6. Let E1 = {A ⊆Rp : A is a bounded rectangle}
and let E2 = {A =
p×
j=1A j : A j ∈B1} =
p×
j=1B1. For the easy direction, we note that
E1 ⊆E2, so that Bp = σ(E1) ⊆σ(E2) =
p
N
j=1
B1, in view of remark 3.3.5. For the
other direction, take a set B = A1 × · · · × Ap ∈E2, with Ai ∈B1, i = 1, 2, . . ., p and
write it as B =
nT
i=1Ci, where Ci =
i−1×
j=1R × Ai ×
p×
j=i+1R, so that Ci are cylinder sets (not
necessarily open). We need to show that
{Ci : Ai ∈B1} ⊆Bp,
(3.1)

86
MEASURE AND INTEGRATION THEORY
i = 1, 2, . . . , n, which would imply that B =
nT
i=1 Ci ∈Bk, so that E2 ⊆Bp leads to
the desired other direction σ(E2) ⊆Bp. To prove equation (3.1), consider a typical
element x = (x1, xi, x2) of Rp, where x1 ∈Ri−1, xi ∈R, x2 ∈Rp−i−1 and deﬁne
the projection mapping f : Rp →R, by f ((x1, xi, x2)) = xi, which is continuous.
Therefore, the inverse image of an open set in R, through f , is an open set in Rp,
so that f −1(O) ⊆T, with O ∈O(R) and T ∈O(Rp). Consequently, we can write
f −1(B1) = f −1(σ(O(R))) = σ(f −1(O(R))) ⊆σ(O(Rp)) = Bp,
(3.2)
using remark 3.3.8 for the second equality. But for any Ai ∈B1, we have
Ci =
i−1×
j=1R × Ai ×
p×
j=i+1R = {x ∈Rp : f (x) ∈Ai} = f −1(A1) ∈f −1(B1),
for each i = 1, 2, . . ., n and using equation (3.2), we establish equation (3.1).
3.5
Measurable Spaces and Mappings
The whole purpose of building topologies, ﬁelds, σ-ﬁelds and generated σ-
ﬁelds is to create collections of sets that are easy to describe, derive their properties
and use them in building a mathematical theory. The next deﬁnition is an important
component of the measure theory mosaic.
Deﬁnition 3.7 Measurable space
A measurable space is a pair (Ω, A), where Ωis a nonempty space and A is a
σ-ﬁeld of subsets of Ω. Sets in A are called measurable sets or A-measurable or
A-sets.
Example 3.4 (Coin ﬂip spaces) Let us revisit example 1.1.
1. Recall that the sample space for a single ﬂip of the coin is the set Ω0 = {ω0, ω1},
where the simple events ω0 = {Heads}, ω1 = {Tails}, are recoded as ω0 = 0 and
ω1 = 1, so that Ω0 = {0, 1}. Note that 2Ω0 = {∅, Ω0, {0}, {1}} is the largest σ-ﬁeld
of the coin ﬂip space and consequently, (Ω0, 2Ω0) can be thought of as the largest
measurable space. Further note that 2Ω0 = σ({0}) = σ({1}), so that we can generate
the largest σ-ﬁeld in the space based on the measurable set {0} or the set {1}. What is
the topology of Ω0? We need to describe the collection of open subsets of Ω0. From
appendix deﬁnition A.1, we can easily see that both collections T ∗
1 = {∅, Ω0, {0}}
and T ∗
2 = {∅, Ω0, {1}}, induce a topology on Ω0, so that the simple events {0} and
{1} are both open and closed sets in Ω0. Hence, the Borel σ-ﬁeld of Ω0 in this case
is B(Ω0) = 2Ω0.
2. Now consider the experiment of n successive ﬂips of a coin, with sample space
Ωn = {ω = (ω1, . . . , ωn) ∈Ωn
0 : ωi ∈{0, 1}, i = 1, 2, . . ., n} = {ω1, . . . , ω2n},
consisting of 2n simple events that are n-dimensional sequences of 0s and 1s, i.e.,
ω1 = (0, . . . , 0, 0), ω2 = (0, . . . , 0, 1), ω3 = (0, . . . , 1, 0), . . ., ω2n = (1, . . . , 1, 1).

MEASURABLE SPACES AND MAPPINGS
87
Clearly, we can think of Ωn as a product space Ωn =
n×
j=1Ω0 and the largest σ-ﬁeld
2Ωn consists of all subsets of Ωn. The topology Tn on Ωn is the one induced by the
product topology
nN
j=1
T ∗
1 or
nN
j=1
T ∗
2 and the corresponding Borel σ-ﬁeld B(Ωn) of
Ωn is obtained by generating the σ-ﬁeld over the collection of open sets Tn or the
Borel product space, namely,
B(Ωn) = σ(Tn) = σ
 
n×
j=1B(Ω0)
!
= 2Ωn.
The ﬁrst equality of the latter holds by the deﬁnition of a Borel σ-ﬁeld, while the
second equality holds from deﬁnition 3.6. To show that B(Ωn) = 2Ωn, we observe
ﬁrst that Tn ⊆2Ωn. For the other direction, consider a set A ∈2Ω2, where A is either
∅(=
n×
j=1∅), or Ωn, or A = {ωi1, . . . , ωip}, if card(A) = p ≤2n = card(Ωn). In the ﬁrst
two trivial cases, A ∈B(Ωn). For the last case, take a typical point ω ∈A, where
ω = (ω1, . . . , ωn), with ωi = 0 or 1, so that ωi ∈B(Ω0), for all i, which implies
ω ∈
n×
j=1 B(Ω0), for all ω ∈A and therefore A ∈B(Ωn), which establishes the other
direction.
3. We turn now to the experiment of inﬁnite coin ﬂips, with sample space Ω∞=
{ω = (ω1, ω2, . . . ) ∈Ω∞
0 : ωi ∈{0, 1}, i = 1, 2, . . .}, consisting of ∞-dimensional
sequences of 0s and 1s. Following the product space deﬁnition, Ω∞=
+∞
×
j=1Ω0 and the
largest σ-ﬁeld 2Ω∞consists of all subsets of Ω∞, while the topology T∞on Ω∞is the
one induced by the product topology
∞
N
j=1
T ∗
1 or
∞
N
j=1
T ∗
2 , with the corresponding Borel
σ-ﬁeld being B(Ω∞) = σ(T∞) = σ
 
∞×
j=1B(Ω0)
!
, by deﬁnitions 3.4 and 3.6. Using
similar arguments as in part 2 of this example we can show that B(Ω∞) = 2Ω∞.
Example 3.5 (Exponential spaces) Consider a bounded subset X ⊂Rp and de-
ﬁne Xn =
n×
i=1X, n = 1, 2, . . ., with X0 = ∅. If n , k, then Xk and Xn are disjoint,
where the extra dimensions are ﬁlled in with copies of X. We deﬁne the exponen-
tial space of X by Xe =
∞S
n=0 Xn. Every set B ∈Xe can be expressed uniquely as the
union of disjoint sets, that is, B =
∞S
n=0 B(n), where B(n) = B ∩Xn. Let A(n) be the
generated σ-ﬁeld in Xn from all Borel measurable rectangles B1 × B2 × · · · × Bn,
with Bi ∈B(X), i = 1, 2, . . ., n and deﬁne Ae as the class of all sets
∞S
n=0 B(n) ∈Xe,
such that B(n) ∈A(n). Then Ae is the smallest σ-ﬁeld of sets in Xe generated by sets
B(n) ∈A(n), n = 0, 1, . . . . The measurable space (Xe, Ae) is called an exponential
space (Carter and Prenter, 1972) and can be used in deﬁning an important random
object, namely, a point process.

88
MEASURE AND INTEGRATION THEORY
Example 3.6 (Hit-or-miss topology)
Let F denote the collection of all closed
subsets of Rp. In order to describe random objects that are random closed sets
(RACS), we need to equip F with a σ-ﬁeld, preferably B(F), so that we can build a
measurable space (F, B(F)). This can be accomplished in several ways, depending
on the topology we introduce in F. In particular, just as open intervals can be used
to build a topology in R and generate B(R), we can use a similar approach for the
space F. First deﬁne O and K to be the collection of open and compact subsets of
Rp, respectively. Then for any A ⊆Rp, O1, . . . , On ∈O and K ∈K, deﬁne
FA = {F ∈F : F ∩A , ∅},
the subsets of F that hit A,
FA = {F ∈F : F ∩A = ∅},
the subsets of F that miss A,
FK
O1,...,On = FK ∩FO1 ∩· · · ∩FOn,
the subsets of F that miss K but hit all the open sets O1, . . . , On and set
Υ =
n
FK
O1,...,On : K ∈K, O1, . . . , On ∈O, n ≥1
o
.
Note that by deﬁnition F∅= F, F∅= ∅and FO = F∅
O, ∀O ∈O. The collection
of sets Υ can be shown to be the base of a topology T (see Appendix A.2) on F,
known as the hit-or-miss topology. It can be shown that the corresponding topologi-
cal space (F, T ) is compact, Hausdorﬀand separable and therefore F is metrizable.
Moreover, the Borel σ-ﬁeld B(F) = σ(T ) on F can be generated by the collec-
tions
nn
FO, ∀O ∈O}, {FK, ∀K ∈K
oo
and the collections {FK, ∀K ∈K} , which form
a sub-base of the topology. In particular, B(F) = σ({FK, ∀K ∈K}) is known as the
Eﬀros σ-ﬁeld, while B(F) = σ(
n
FO, ∀O ∈O}, {FK, ∀K ∈K
o
) is known as the Fell
topology. The two constructions may lead to diﬀerent topologies once we depart
from a well-behaved space like Rp. More details on such constructions can be found
in Molchanov (2005) and Nguyen (2006).
We collect another important component of measure theory, that of a measur-
able mapping. Any random object is essentially described by a mapping that is
measurable.
Deﬁnition 3.8 Measurable map
Let (Ω, A) and (X, G) be two measurable spaces and deﬁne a mapping X : Ω→X.
Then X is called measurable A|G if X−1(G) ∈A, for all G ∈G, where X−1(G) =
{ω ∈Ω: X(ω) ∈G}. We may write X : (Ω, A) →(X, G) or simply say that X is
A-measurable if the other symbols are clear from the context.
Let us collect some comments and consequences of this important deﬁnition.
Remark 3.7 (Measurability) Based on deﬁnition 3.8 we can show the following.
1. In probability theory Ωplays the role of the sample space and A contains all
the events of the experiment. When X = R, we typically take as the target σ-ﬁeld

MEASURABLE SPACES AND MAPPINGS
89
the Borel σ-ﬁeld G = B(R) and X is called a random variable. For X = Rp-valued
maps, we take G = B(Rp) and X is called a random vector and if X = R∞, we take
G = B(R∞) and X is called a random sequence of real elements.
2. If X : Ω→X is measurable A|G and Y : X →Υ is measurable G|H, then
Y ◦X : Ω→Υ is measurable A|H.
3. Assume that X : Ω→X. If X−1(H) ∈A, for each H ∈H where G = σ(H),
then X is measurable A|G.
4. If X : (Ω, A) →(X, B(X)) then X is called a Borel mapping.
5. If f : (Rk, Bk) →(Rp, Bp), with f = (f1, . . . , fp), then f is called a Borel
function or simply measurable (when a measurable space is not speciﬁed). It can
be shown that a vector function f : Rk →Rp is measurable if and only if each fi is
measurable.
6. If the Borel function f : Rk →Rp is continuous then it is measurable.
7. If f and g are Borel functions then so are the functions f ± g, f ± c, cf, | f | and
f g, where c is some constant. If f : R →R is increasing then it is measurable.
8. If f1, f2, . . . , are A-measurable functions then so are the functions inf
n fn, sup
n
fn,
lim sup
n
fn = limfn = inf
n sup
k≥n
fk,
lim inf
n
fn = limfn = sup
n
inf
k≥n fk,
sup
ω
{ f1(ω), . . . , fn(ω)} and inf
ω { f1(ω), . . . , fn(ω)}. If f = lim
n fn exists for all ω, i.e.,
f = limfn = limfn, then it is A-measurable. The set {ω : limfn(ω) = limfn(ω)} is
A-measurable. If f is A-measurable then {ω : lim
n fn(ω) = f (ω)} is A-measurable.
9. The indicator function IA(ω) = I(ω ∈A) =

1
if ω ∈A
0
if ω < A , is A-measurable if
the set A is A-measurable.
10. A map X : Ω→A ⊆R, is called a set function, which should not be confused
with a set-valued map X : Ω→C, where C is a collection of subsets of a space
X, so that X(ω) = c, with ω ∈Ωand c ∈C, is a subset of X for the given ω. For
example, let X : R+ →C, where C = {b(0, r) ⊂Rp : r > 0}, the collection of all
open balls in Rp centered at the origin of radius r. Then X is a set-valued map with
X(r) = c = b(0, r), that takes a positive real number and returns an open ball with
the speciﬁed real number as a radius. In order to introduce measurability of such
maps one has to turn to the hit-or-miss topology and measurable space (example
3.6). Measurable set-valued maps are explored in the TMSO-PPRS text.

90
MEASURE AND INTEGRATION THEORY
Many of the results of the previous remark can be shown using the following
theorem (Royden, 1989, p. 66) and deﬁnition.
Theorem 3.3 (Measurable R-valued functions) Let f : E →R, where E is an
A-measurable set. The following statements are equivalent;
(i) ∀a ∈R the set {x : f (x) > a} is A-measurable.
(ii) ∀a ∈R the set {x : f (x) ≥a} is A-measurable.
(iii) ∀a ∈R the set {x : f (x) < a} is A-measurable.
(iv) ∀a ∈R the set {x : f (x) ≤a} is A-measurable.
These statements imply:
(v) If a ∈R the set {x : f (x) = a} is A-measurable.
Deﬁnition 3.9 Measurable function
An extended real-valued function f is said to be G-measurable if its domain is
G-measurable and it satisﬁes one of (i)-(iv) from theorem 3.3.
A very important class of measurable functions are the so-called simple func-
tions.
Deﬁnition 3.10 Simple function
Let {Ai}n
i=1 be a ﬁnite partition of Ωand {yi}n
i=1, real, distinct nonzero numbers. A
step function f : Ω→R is deﬁned by
f (x) =
nP
i=1 yiIAi(x), x ∈Ω,
and it is called a (canonical) simple function or A−measurable simple function, if
every set Ai is A-measurable, where Ai = {x ∈Ω: f (x) = yi}, i = 1, 2, . . ., n. When
the {yi} are not distinct or {Ai} are not a partition of Ω, then f (x) is still called a
simple function but it is in a non-canonical form.
The usefulness of sequences of simple functions is illustrated in the following
theorem.
Theorem 3.4 (Monotone limit of simple functions) If f is R-valued and A-
measurable, there exists a sequence { fn} of simple A-measurable functions such
that
0 ≤fn(ω) ↑f (ω), if f (ω) ≥0,
and
0 ≥fn(ω) ↓f (ω), if f (ω) ≤0,

MEASURE THEORY AND MEASURE SPACES
91
as n →∞, where fn ↑f means that fn increases toward f from below and fn ↓f
means that fn decreases toward f from above.
Proof. Deﬁne the sequence of simple functions with the desired properties by
fn(ω) =

−n
−∞≤f (ω) ≤−n,
−(k −1)2−n
−k2−n < f (ω) ≤−(k −1)2−n,
(k −1)2−n
(k −1)2−n ≤f (ω) < k2−n,
n
n ≤f (ω) ≤∞,
(3.3)
where 1 ≤k ≤n2n, n = 1, 2, . . . We note that fn as deﬁned covers the possibilities
f (ω) = ∞and f (ω) = −∞.
Example 3.7 (Function measurability) Using remark 3.7 we can show measur-
ability for a wide variety of functions.
1. Let Ω= [0, 1]2, B(Ω) be the Borel sets of Ωand consider the function
X(ω1, ω2) = (ω1 ∧ω2, ω1 ∨ω2), where ω1 ∧ω2 = min{ω1, ω2} and ω1 ∨
ω2 = max{ω1, ω2}. Both coordinates of X are continuous functions so that X is
B(Ω)|B(Ω) measurable using remark 3.7.
2. Recall example 3.4 and deﬁne the mapping f : Ω∞
0 →Ωn
0 from the measurable
space (Ω∞
0 , B(Ω∞
0 )) into the measurable space (Ωn
0, B(Ωn
0)) by f ((ω1, ω2, . . . )) =
(ω1, ω2, . . . , ωn). Then using remark 3.7 f is B(Ω∞
0 )|B(Ωn
0) measurable.
3.6
Measure Theory and Measure Spaces
Now that we have a good idea about measurable sets, spaces and functions, we
are ready to deﬁne a general set function that will allow us to build general measure
theory.
Deﬁnition 3.11 Measure
A set function µ on a ﬁeld A of subsets of a space Ωis called a measure if it
satisﬁes: (i) µ(∅) = 0, (ii) µ(A) ∈[0, +∞], ∀A ∈A, (iii) if A1, A2, . . . , is a disjoint
sequence of A-sets and if
∞S
n=1 An ∈A, then
µ
 ∞S
n=1 An
!
=
∞P
n=1 µ(An).
If A is a σ-ﬁeld, then the pair (Ω, A) is a measurable space and the triple (Ω, A, µ)
is called a measure space.
The following remark summarizes some of the most important deﬁnitions and
properties in general measure theory.
Remark 3.8 (Properties of measures) We collect some consequences of this deﬁ-
nition and some standard properties of measures.

92
MEASURE AND INTEGRATION THEORY
1. The measure µ is called ﬁnite if µ(Ω) < +∞, inﬁnite when µ(Ω) = +∞and a
probability measure if µ(Ω) = 1. Deﬁnition 1.1 is essentially deﬁnition 3.11 when
µ(Ω) = 1 and remark 1.1 contains immediate consequences of the deﬁnition.
2. σ-ﬁnite measure If Ω= ∪An for some ﬁnite or countable sequence {An} of
A-sets satisfying µ(An) < +∞then µ is called σ-ﬁnite. The {An} do not have to
be disjoint but we can turn them into a disjoint sequence {Bn} by setting B1 = A1,
Bn = An ∖
 n−1
S
i=1 Ai
!
, n ≥2 and thus we can assume wlog that the {An} are a partition
of Ω. A ﬁnite measure is by deﬁnition σ-ﬁnite, although a σ-ﬁnite measure may be
ﬁnite or inﬁnite. If Ωis not a ﬁnite or countable union of A-sets, then no measure
can be σ-ﬁnite on A. It can be shown that if µ is σ-ﬁnite on a ﬁeld A then A cannot
contain an uncountable disjoint collection of sets of positive µ-measure.
3. Complete measure space A measure space is said to be complete if A contains
all subsets of sets of measure zero, that is, if B ∈A with µ(B) = 0 and A ⊂B imply
A ∈A.
4. Finite additivity Condition (iii) is known as countable additivity and it implies
ﬁnite additivity: µ
 nS
i=1 Ai
!
=
nP
i=1 µ(Ai), for
nS
i=1 Ai ∈A, {Ai} disjoint sets from the ﬁeld
A.
5. Almost everywhere A property is said to hold almost everywhere with respect
to µ and we write a.e. [µ], if the set of points where it fails to hold is a set of measure
zero, i.e., if the property is expressed in terms of a collection of ω ∈Ωforming the
set A, then A a.e. [µ] if and only if µ(Ac) = 0. If the measure µ is understood by the
context, we simply say that the property holds a.e. If µ(Ac) = 0 for some A ∈A,
then A is a support of µ and µ is concentrated on A. For a ﬁnite measure µ, A is a
support if and only if µ(A) = µ(Ω).
6. Singularity Measures µ and ν on (Ω, A) are called mutually singular (denoted
by µ ⊥ν), if there are disjoint sets A, B ∈A such that Ω= A∪B and µ(A) = ν(B) =
0.
7. Absolute continuity A measure ν is said to be absolutely continuous with re-
spect to the measure µ if ν(A) = 0 for each set A for which µ(A) = 0. We write
ν ≪µ. If ν ≪µ and a property holds a.e. [µ], then it holds a.e. [ν].
8. Semiﬁnite measure A measure is called semiﬁnite if each measurable set of
inﬁnite measure contains measurable sets of arbitrarily large ﬁnite measure. This
is a wider class of measures since every σ-ﬁnite measure is semiﬁnite, while the
measure that assigns 0 to countable subsets of an uncountable set A and ∞to the
uncountable sets is not semiﬁnite.
9. Transformations Let (Ω1, A1) and (Ω2, A2) be measurable spaces and suppose
that T : Ω1 →Ω2 is measurable. If µ is a measure on A1 then deﬁne a set function

MEASURE THEORY AND MEASURE SPACES
93
µT −1 on A2 by µT −1(A2) = µ(T −1(A2)), ∀A2 ∈A2. We can show the following: (i)
µT −1 is a measure, (ii) if µ is ﬁnite then so is µT −1, and (iii) if µ is a probability
measure then so is µT −1.
10. Convergence almost everywhere We say that a sequence { fn} of measurable
functions converges almost everywhere on a set E to a function f if µ({ω ∈E ⊂
Ω: fn(ω) →f (ω), as n →∞}) = 1.
11. Atom If (Ω, A, µ) is a measure space, then a set A ∈A is called an atom of
µ if and only if 0 < µ(A) < +∞and for every subset B ⊂A with B ∈A, either
µ(B) = 0 or µ(B) = µ(A). A measure without atoms is called nonatomic.
12. Radon measure A measure µ on a Borel σ-ﬁeld B is called a Radon measure
if µ(C) < ∞for every compact set C ∈B.
13. Locally ﬁnite measure The measure µ on (Ω, A) is called locally ﬁnite if
µ(A) < ∞, for all bounded sets A ∈A.
14. Lebesgue decomposition Let (Ω, A, µ) be a σ-ﬁnite measure space and ν a σ-
ﬁnite measure deﬁned on A. Then we can ﬁnd a measure ν0 singular with respect
to µ and a measure ν1 absolutely continuous with respect to µ such that ν = ν0 + ν1.
The measures ν0 and ν1 are unique. This theorem connects singularity and absolute
continuity. The proof mirrors that of the Radon-Nikodym theorem (theorem 3.20)
and it will be discussed there.
Example 3.8 (Counterexample for non-additive measure) There are set func-
tions that are not additive on 2Ω. Indeed, let ω1 , ω2 in Ωand set µ(A) = 1, if A
contains both ω1 and ω2 and µ(A) = 0, otherwise. Then µ is not additive on 2Ω.
Example 3.9 (Discrete measure) A measure µ on (Ω, A) is discrete if there exist
ﬁnitely or countably many points {ωi} in Ωand masses mi ≥0, such that µ(A) =
P
ωi∈A mi, for all A ∈A. The measure µ is inﬁnite, ﬁnite or a probability measure if
P
i mi = +∞, < +∞, or converges to 1, respectively.
Example 3.10 (Counting measure) Let (Ω, A, µ) be a measure space where for
A ∈A ﬁnite µ is deﬁned by
µ(A) = card(A) = |A| = # of points in A,
and if A is not ﬁnite assume µ(A) = +∞. The measure µ is called the counting mea-
sure and is (σ-)ﬁnite if and only if Ωis (countable) ﬁnite. Let us verify deﬁnition
3.11. Clearly, µ(∅) = 0 and µ(A) ∈[0, +∞], for all A ∈A. Letting {An} disjoint
A-sets we have
µ
 nS
i=1 Ai
!
=

nS
i=1 Ai
 =
nP
i=1 |Ai| =
nP
i=1 µ(Ai).
Note that the counting measure on an uncountable set, e.g., [0, 1] or R, is an exam-
ple of a measure that is not σ-ﬁnite.

94
MEASURE AND INTEGRATION THEORY
We discuss and prove ﬁve important properties of measures below.
Theorem 3.5 (Properties of measures) Let µ be a measure on a ﬁeld A.
(i) Monotonicity: µ is monotone, that is, A ⊂B =⇒µ(A) ≤µ(B).
(ii) Continuity from below: If An, n = 1, 2, . . . and A are A-sets with An ↑A then
µ(An) ↑µ(A).
(iii) Continuity from above: If An, n = 1, 2, . . . and A are A-sets with µ(A1) < ∞
and An ↓A then µ(An) ↓µ(A).
(iv) Countable subadditivity (Boole’s Inequality): For a sequence of A-sets {An}
with
∞S
n=1 An ∈A we have
µ
 ∞S
n=1 An
!
≤
∞P
n=1 µ(An).
(v) Inclusion-exclusion Formula: For {Ai}n
i=1 any A-sets with µ(Ai) < ∞we have
µ
 nS
i=1 Ai
!
=
nP
k=1
P (
(−1)k−1µ
 kT
j=1 Aij
!
: 1 ≤i1 < · · · < ik ≤n
)
(3.4)
=
P
i µ(Ai) −P
i<j µ(Ai ∩A j) + · · · + (−1)n+1µ(A1 ∩· · · ∩An).
Proof. (i) For sets A, B ∈A, with A ⊂B and B ∖A ∈A, using ﬁnite additivity
we can write: µ(B) = µ(A ∪(B ∖A)) = µ(A) + µ(B ∖A), so that 0 ≤µ(B ∖A) =
µ(B) −µ(A).
(ii) Consider An, A ∈A, with An ↑A. Since An ⊆An+1 we see that µ(An) ≤
µ(An+1). Moreover, we see that
+∞
S
k=1 Ak ⊂
+∞
S
k=2 Ak ⊂
+∞
S
k=3 Ak ⊂... and remark 3.1.9 yields
+∞
T
n=1
+∞
S
k=n Ak =
+∞
S
n=1 An, so that A = lim
n→∞An = limAn =
+∞
S
n=1 An. Let Bn = An+1 ∖An and
note that Bi ∩B j = ∅, i , j and A =
+∞
S
n=1 An = A1 ∪
+∞
S
n=1 Bn. Applying countable
additivity to the sequence {Bn}+∞
n=0, with B0 = A1, we obtain
µ
 +∞
S
n=0 Bn
!
=
µ(A1) +
+∞
P
n=1 µ(Bn) = µ(A1) + lim
n→∞
nP
k=1
µ(Ak+1) −µ(Ak)
=
µ(A1) + lim
n→∞
(µ(A2) −µ(A1)) + ... + (µ(An+1) −µ(An))
=
µ(A1) + lim
n→∞µ(An+1) −µ(A1),
which leads to the desired lim
n→∞µ(An) = µ (A) .
(iii) Now let An, A ∈A, with An ⊇An+1, such that An ↓A, where A = lim
n→∞An =
limAn =
+∞
S
n=1
+∞
T
n=k Ak =
+∞
T
n=1 An. Setting Bn = A1 ∖An, we note that Bn ⊆Bn+1, with
Bn ↑B = A1 ∖A =
+∞
S
n=1(A1 ∖An) and from part (ii) we obtain µ(Bn) ↑µ(B), which

MEASURE THEORY AND MEASURE SPACES
95
leads to lim
n→∞µ(A1 ∖An) = µ(A1 ∖A), so that µ(A1) −lim
n→∞µ(An) = µ(A1) −µ(A) and
the desired result is established, since µ(A1) < ∞allows us to write the measure of
the set diﬀerence as the diﬀerence of the measures.
(iv) Suppose that {An}+∞
n=1 are A-sets with
∞S
n=1 An ∈A and let B1 = A1,
Bn = An ∖
"n−1
S
i=1 Ai
#
= An ∩
"n−1
T
i=1 Ac
i
#
= An ∩Ac
1 ∩Ac
2 ∩... ∩Ac
n−1,
for n > 1, a possibly improper set diﬀerence since we might be subtracting a larger
set, in which case Bn = ∅. Clearly, Bn ⊂An so that by monotonicity µ(Bn) ≤µ(An),
for all n. In addition, note that the {Bn} are disjoint with
∞S
n=1 Bn
=
A1 ∪
∞S
n=2
 
An ∩
"n−1
T
i=1 Ac
i
#!
= A1 ∪
 ∞S
n=2 An ∩
∞S
n=2
"n−1
T
i=1 Ac
i
#!
=
∞S
n=1 Ak ∩
 
A1 ∪
∞S
n=2
"n−1
T
i=1 Ac
i
#!
,
since
A1 ∪
∞S
n=2
"n−1
T
i=1 Ac
i
#
=
A1 ∪
∞S
n=2
" n−1
S
i=1 Ai
!c#
= A1 ∪
" ∞T
n=2
 n−1
S
i=1 Ai
!#c
=
A1 ∪[A1 ∩(A1 ∪A2) ∩(A1 ∪A2 ∪A3) ∩...]c
=
A1 ∪Ac
1 = Ω,
and appealing to countable additivity we obtain
µ
 ∞S
n=1 An
!
= µ
 ∞S
n=1 Bn
!
=
∞P
n=1 µ(Bn) ≤
∞P
n=1 µ(An),
as claimed.
(v) We use induction on n. For n = 1, equation (3.4) becomes µ (A1) = µ (A1) and
it holds trivially. For n = 2 we ﬁrst write
A1 ∪A2 = [A1 ∖(A1 ∩A2)] ∪[A2 ∖(A1 ∩A2)] ∪(A1 ∩A2),
a disjoint union and then apply ﬁnite additivity to obtain
µ (A1 ∪A2)
=
µ (A1 ∖(A1 ∩A2)) + µ (A2 ∖(A1 ∩A2)) + µ (A1 ∩A2)
=
µ(A1) −µ(A1 ∩A2) + µ(A2) −µ(A1 ∩A2) + µ (A1 ∩A2)
=
µ(A1) + µ(A2) −µ(A1 ∩A2),
thus establishing the formula for n = 2. Now assume equation (3.4) holds for n. We
show that it holds for n + 1. Using the result for n = 2 we can write
µ
 n+1
S
i=1 Ai
!
=
µ
 nS
i=1 Ai ∪An+1
!
=
µ
 nS
i=1 Ai
!
+ µ (An+1) −µ
 nS
i=1 Ai ∩An+1
!
=
µ
 nS
i=1 Ai
!
+ µ (An+1) −µ
 nS
i=1
(Ai ∩An+1)
!
,

96
MEASURE AND INTEGRATION THEORY
and using the formula for n we have
µ
 n+1
S
i=1 Ai
!
=
nP
k=1
P (
(−1)k−1µ
 kT
j=1 Aij
!
: 1 ≤i1 < ... < ik ≤n
)
+ µ (An+1)
−
nP
k=1
P (
(−1)k−1µ
 kT
j=1 Aij ∩An+1
!
: 1 ≤i1 < ... < ik ≤n
)
=
n+1
P
k=1
P (
(−1)k−1µ
 kT
j=1 Aij
!
: 1 ≤i1 < ... < ik ≤n + 1
)
,
with the last equality established after some tedious (not hard) algebra once we
expand the terms in the ﬁrst equality and simplify.
The uniqueness-of-measure theorem that follows, is the result that allows us to
uniquely determine measures on σ-ﬁelds when the measures are deﬁned and agree
in smaller collections of sets, i.e., a π-system. This result is particularly useful in
probability theory, since it helps us to uniquely identify probability distributions of
random variables.
Theorem 3.6 (Uniqueness of measure) Let µ1 and µ2 denote measures deﬁned
on σ(P), where P is a π-system. If µ1 is σ-ﬁnite with respect to P and if µ1 and
µ2 agree on P then µ2 is σ-ﬁnite and µ1 and µ2 agree on σ(P).
Proof. Since µ1 is σ-ﬁnite on P we may write Ω=
∞S
n=1 Bn, for some sequence
{Bn}∞
n=1 of P-sets, with µ1(Bn) < +∞, for all n = 1, 2, .... Since µ2 = µ1 on P we
have µ2(Bn) < +∞, for all n and hence µ2 is σ-ﬁnite on P.
Now choose an arbitrary A ∈σ(P). We need to show that µ1(A) = µ2(A).
First note that using continuity from below we can write
µi(A)
=
µi
 ∞S
k=1(Bk ∩A)
!
= µi
 
lim
n→∞
nS
k=1(Bk ∩A)
!
=
lim
n→∞µi
 nS
k=1(Bk ∩A)
!
,
for i = 1, 2 and therefore we need to show that
lim
n→∞µ1
 nS
k=1(Bk ∩A)
!
= lim
n→∞µ2
 nS
k=1(Bk ∩A)
!
,
which in turn will hold if we show that
µ1
 nS
k=1(Bk ∩A)
!
= µ2
 nS
k=1(Bk ∩A)
!
.
Use the inclusion-exclusion formula to write
µi
 nS
k=1(Bk ∩A)
!
=
nP
k=1
P (
(−1)k−1µi
 kT
j=1 Bij ∩A
!
: 1 ≤i1 < ... < ik ≤n
)
,
for all n, so that we need to show that
µ1
 kT
j=1 Bij ∩A
!
= µ2
 kT
j=1 Bij ∩A
!
,
(3.5)

MEASURE THEORY AND MEASURE SPACES
97
for k = 1, 2, ..., n, 1 ≤i1 < ... < ik ≤n and all n = 1, 2, ... For ﬁxed k, n and
i j, we have that
kT
j=1 Bij ∈P, since P is a π-system and Bij ∈P, j = 1, 2, .., k. By
monotonicity and the fact that µ1 and µ2 agree on P, we have that µ1
 kT
j=1 Bij
!
=
µ2
 kT
j=1 Bij
!
< +∞. Deﬁne the class of sets in σ(P)
LB = {A ∈σ(P) : µ1(B ∩A) = µ2(B ∩A)},
for some B ∈P. Then LB is a λ-system on Ωand since P ⊆LB, Dynkin’s π −λ
theorem implies σ(P) ⊆LB. Since A ∈σ(P) and σ(P) ⊆LBi1∩...∩Bik, we have
A ∈LBi1∩...∩Bik, so that equation (3.5) holds and the desired result is established.
Example 3.11 (Counterexample for uniqueness of measure)
Letting P = ∅
we have that P is a π-system and σ(P) = {∅, Ω}. Then any ﬁnite measures have to
agree on P but need not agree on σ(P).
Example 3.12 (Probability measure via the cdf) Let (Ω, A) = (R, B1) and con-
sider the collection of sets P = {(−∞, x], x ∈R}. Then P is a π-system and two
ﬁnite measures that agree on P also agree on B1. The consequence of uniqueness
of measure in this case illustrates that the cdf uniquely determines the probability
distribution of a random variable, since deﬁning the cdf F(x) = P((−∞, x]) based
on a probability measure P uniquely deﬁnes P on all Borel sets in R. This is also
the result of the Lebesgue-Stieltjes theorem: there exists a bijection between cdfs
F on R and probability measures P on B(R) via F(x) = P((−∞, x]).
3.6.1
Signed Measures and Decomposition Theorems
Decomposition theorems in measure theory provide results that connect diﬀer-
ent types of measures and have great applications in all manner of proofs. We study
below three of those measure decomposition theorems.
The ﬁrst two theorems involve the concept of a signed measure. If we have two
measures µ1 and µ2 on the same measurable space (Ω, A) then owing to condition
(ii) of deﬁnition 3.11, µ1 and µ2 are called nonnegative signed measures. We can
easily see that µ3 = µ1 + µ2 is a measure deﬁned on (Ω, A). But what happens if
set µ3 = µ1 −µ2? Clearly, µ3 will not be nonnegative always and it is possible that
it will be undeﬁned if for some A ∈A we have µ1(A) = µ2(A) = ∞. To avoid this
we could ask for at least one of the measures to be ﬁnite. The next deﬁnition takes
into account this discussion by replacing condition (ii) of deﬁnition 3.11 and taking
care of the consequences in condition (iii).
Deﬁnition 3.12 Signed measure
A signed measure µ on a measurable space (Ω, A), where A is a σ-ﬁeld, is

98
MEASURE AND INTEGRATION THEORY
an extended real-valued set function deﬁned for sets of A that has the following
properties: (i) µ(∅) = 0, (ii) µ assumes at most one of the values +∞or −∞, and
(iii) if A1, A2, ... is a disjoint sequence of A-sets then
µ
 ∞S
i=1 Ai
!
=
∞P
n=1 µ(An),
with equality understood to mean that the series on the right converges absolutely
if µ
 ∞S
i=1 Ai
!
is ﬁnite and that it properly diverges otherwise.
If we replace condition (ii) of deﬁnition 3.11 with (ii)′ µ(A) ∈[−∞, 0], ∀A ∈A,
then µ is called a signed nonpositive measure. Clearly, a measure is a special case
of a signed measure, however a signed measure is not in general a measure. We say
that a set A is a positive set with respect to a signed measure µ if A is measurable
and for every measurable subset of E of A we have µ(E) ≥0. Every measurable
subset of a positive set is again positive and the restriction of µ to a positive set
yields a measure.
Similarly, a set B is called a negative set if it is measurable and every measurable
subset of B has nonpositive µ measure. A set that is both positive and negative with
respect to µ has zero measure. We need to be careful in this case. A null set with
respect to a signed measure is not only a null set in the sense of remark 3.12,
i.e., has measure zero, but with the above deﬁnitions, it contains all its subsets by
construction.
While every null set must have measure zero in the case of signed measures, a
set of measure zero may be a union of two sets whose measures are not zero but
cancel each other. Clearly, for the nonnegatively signed measure in remark 3.12 we
do not have this situation and measure space completion is required.
We collect ﬁrst some useful lemmas and request their proofs as exercises.
Lemma 3.1 Every measurable subset of a positive set is itself positive. The union
of a countable collection of positive sets is positive.
Lemma 3.2 Let E be a measurable set such that 0 < µ(E) < ∞. Then there is a
positive set A contained in E with µ(A) > 0.
The following decomposition theorem allows us to work with a speciﬁc parti-
tion of the space.
Theorem 3.7 (Hahn decomposition) Let µ be a signed measure on the measur-
able space (Ω, A). Then there is a positive set A+ and a negative set A−such
that Ω= A+ ∪A−and A+ ∩A−= ∅. The collection {A+, A−} is called a Hahn
decomposition of Ωwith respect to µ and is not unique.
Proof. Without loss of generality, assume that µ does not assign the value +∞

MEASURE THEORY AND MEASURE SPACES
99
to any A-set. Let a be the supremum of µ(A) over all sets A that are positive with
respect to µ. Since the empty set is a positive set, a ≥0. Let {An} be a sequence of
positive sets such that a = lim
n→∞µ(An) and set A+ =
∞S
n=1 An. By lemma 3.1, the set A+
is itself a positive set and hence a ≥µ(A+). But A+ ∖An ⊂A+ yields µ(A+ ∖An) ≥0,
so that
µ(A+) = µ(An) + µ(A+ ∖An) ≥µ(An),
which implies that µ(A+) ≥a and hence µ(A+) = a, with a < ∞.
Now let A−= (A+)c and suppose that E is a positive subset of A−. Then E and
A+ are disjoint and A+ ∪E is a positive set. Therefore
a ≥µ(A+ ∪E) = µ(A+) + µ(E) = a + µ(E),
so that µ(E) = 0, since 0 ≤a < ∞. Thus A−does not contain any positive subsets
of positive measure and therefore, no subset of positive measure by lemma 3.2.
Therefore, A−is a negative set.
The second decomposition theorem presented next, allows decomposition of a
measure into two mutually singular signed measures.
Theorem 3.8 (Jordan decomposition) Let µ be a signed measure on the mea-
surable space (Ω, A). Then there are two mutually singular measures µ+ and µ−
on (Ω, A) such that µ = µ+ −µ−. Moreover, µ+ and µ−are unique.
Proof. Let {A+, A−} be a Hahn decomposition of Ωwith respect to µ and deﬁne
two ﬁnite measures µ+ and µ−for any A ∈A by
µ+(A)
=
µ(A ∩A+), and
µ−(A)
=
−µ(A ∩A−),
so that µ = µ+ −µ−and µ+ and µ−are mutually singular, since they have disjoint
supports. To show uniqueness, note that if E ⊂A then µ(E) ≤µ+(E) ≤µ+(A),
with equality if E = A ∩A+. Therefore, µ+(A) = sup
E⊂A
µ(E) and following similar
arguments µ−(A) = −inf
E⊂Aµ(E). The unique measures µ+ and µ−thus created are
called the upper and lower variations of µ and the measure |µ| deﬁned by |µ| =
µ+ + µ−is called the total variation.
The following lemma is useful in proving subsequent results.
Lemma 3.3 If µ and v are ﬁnite measures (i.e., signed nonnegative) that are not
mutually singular then there exists a set A and ε > 0 such that µ(A) > 0 and
εµ(E) ≤v(E), for all E ⊂A.
Proof. Let {A+
n, A−
n} be a Hahn decomposition of Ωwith respect to the set
function u = v −1
nµ and set B =
∞S
n=1 A+
n, so that Bc =
∞T
n=1 A−
n. Since Bc ⊂A−
n we
must have u(Bc) ≤0 or v(Bc) ≤1
nµ(Bc), for arbitrary n and hence v(Bc) = 0. This

100
MEASURE AND INTEGRATION THEORY
means that Bc supports v and since µ ⊥v, Bc cannot support µ, i.e., µ(Bc) = 0.
Consequently µ(B) > 0 and therefore µ(A+
n) > 0 for some n. Set A = A+
n and ε = 1
n
to obtain the claim.
The results of this section can be used to prove the Radon-Nikodym and the
Lebesgue Decomposition theorems, as well as the all-important Carath´eodory ex-
tension theorem as we discuss next.
3.6.2
Carath´eodory Measurability and Extension Theorem
It is much easier to deﬁne a measure on a π-system or a ﬁeld and then extend
it somehow to assign values to the generated σ-ﬁeld, since the former is a much
smaller and easier to describe collection of sets. This extension can be accom-
plished in a unique way using outer measures and the general construction is due
to the Greek mathematician Konstantinos Carath´eodory (1873-1950).
Deﬁnition 3.13 Outer measure
An outer measure is a set function µ∗deﬁned for all subsets of a space Ωthat
satisﬁes: (i) µ(A) ∈[0, +∞], ∀A ⊂Ω, (ii) µ∗(∅) = 0, (iii) µ∗is monotone, that is,
A ⊂B =⇒µ∗(A) ≤µ∗(B), and (iv) µ∗is countably subadditive, that is
µ∗
 ∞S
i=1 Ai
!
≤
∞P
n=1 µ∗(An).
Note that condition (i) is forcing the outer measure to be non-negative (signed)
and is not required in general. While the outer measure, as deﬁned, has the advan-
tage that it is deﬁned for all subsets of the space Ω, it is not countably additive. It
becomes countably additive once we suitably reduce the family of sets on which it
is deﬁned. Carath´eodory’s general deﬁnition of a measurable set with respect to µ∗
provides us with a collection of such well-behaved sets.
Deﬁnition 3.14 Carath´eodory measurability
A set E ⊂Ωis measurable with respect to µ∗if for every set A ⊂Ωwe have
µ∗(A) = µ∗(A ∩E) + µ∗(A ∩Ec).
We denote the collection of all Carath´eodory measurable sets by M or M(µ∗).
We note the following.
Remark 3.9 (Showing measurability) Since µ∗is subadditive, to show measura-
bility we only need to show the other direction
µ∗(A) ≥µ∗(A ∩E) + µ∗(A ∩Ec),
where A = (A ∩E) ∪(A ∩Ec). The inequality is trivially true when µ∗(A) = ∞and
so we need only establish it for sets A with µ∗(A) < ∞.

MEASURE THEORY AND MEASURE SPACES
101
The following theorem establishes some important properties of the class M of
µ∗-measurable sets, as well as the measure µ∗.
Theorem 3.9 (Properties of the class M) The class M of µ∗-measurable sets is
a σ-ﬁeld. If µ is the restriction of µ∗to M, then µ is a measure on M.
Proof. Clearly, ∅∈M, and the symmetry of deﬁnition 3.14, shows that
Ec ∈M, whenever E ∈M. Now consider two M-sets, E1 and E2. From the
measurability of E2, we have
µ∗(A) = µ∗(A ∩E2) + µ∗(A ∩Ec
2),
and applying the measurability condition for E1, we obtain
µ∗(A) = µ∗(A ∩E2) + µ∗(A ∩Ec
2 ∩E1) + µ∗(A ∩Ec
2 ∩Ec
1),
for any set A ⊂Ω. Since
A ∩(E1 ∪E2) = (A ∩E2) ∪(A ∩E1 ∩Ec
2),
using subadditivity we can write
µ∗(A ∩(E1 ∪E2)) ≤µ∗(A ∩E2) + µ∗(A ∩E1 ∩Ec
2),
which leads to
µ∗(A) ≥µ∗(A ∩(E1 ∪E2)) + µ∗(A ∩Ec
1 ∩Ec
2),
and hence E1 ∪E2 is measurable, since (E1 ∪E2)c = Ec
1 ∩Ec
2. Now using induction,
the union of any ﬁnite number of M-sets is measurable, and thus M is a ﬁeld. To
show that M is a σ-ﬁeld, let E =
∞S
i=1 Ei, where {Ei} is a disjoint sequence of M-sets.
We need to show that E is an M-set. Clearly, if Gn =
nS
i=1 Ei, then Gn ∈M, and since
Gn ⊂E ⇒Ec ⊂Gc
n, using monotonicity we can write
µ∗(A) = µ∗(A ∩Gn) + µ∗(A ∩Gc
n) ≥µ∗(A ∩Gn) + µ∗(A ∩Ec).
Now Gn ∩En = En and Gn ∩Ec
n = Gn−1, and by the measurability of En, we have
µ∗(A ∩Gn) = µ∗(A ∩En) + µ∗(A ∩Gn−1).
By induction
µ∗(A ∩Gn) =
nP
i=1 µ∗(A ∩Ei),
so that
µ∗(A) ≥
nP
i=1 µ∗(A ∩Ei) + µ∗(A ∩Ec).
Letting n →∞, noting that A ∩E ⊂
∞S
i=1(A ∩Ei), and using subadditivity, we can
write
µ∗(A)
≥
∞P
i=1 µ∗(A ∩Ei) + µ∗(A ∩Ec)
≥
µ∗
 ∞S
i=1(A ∩Ei)
!
+ µ∗(A ∩Ec),

102
MEASURE AND INTEGRATION THEORY
and hence
µ∗(A) ≥µ∗(A ∩E) + µ∗(A ∩Ec).
The latter shows that E is measurable, so that M is a σ-ﬁeld.
We next demonstrate the ﬁnite additivity of µ. Let E1 and E2 be disjoint mea-
surable sets. Then the measurability of E2 implies that
µ(E1 ∪E2)
=
µ∗(E1 ∪E2)
=
µ∗([E1 ∪E2] ∩E2) + µ∗([E1 ∪E2] ∩Ec
2)
=
µ∗(E2) + µ∗(E1),
and ﬁnite additivity follows by induction. If E is the disjoint union of the measur-
able sets {En}, then
µ(E) ≥µ
 nS
i=1 Ei
!
=
nP
i=1 µ(Ei),
so that
µ(E) ≥
∞P
i=1 µ(Ei).
By the subadditivity of µ∗we have
µ(E) ≤
∞P
i=1 µ(Ei),
and thus µ is countably additive. Since µ(E) ≥0 for all E ∈M, and µ(∅) = µ∗(∅) =
0, we conclude that µ is a measure on M.
Next we discuss the idea behind the extension of measure problem.
Remark 3.10 (Extension of measure) Let us formally state the extension problem:
given a measure µ on a ﬁeld A of subsets of a space Ω, we wish to extend it to a
measure on a σ-ﬁeld M containing A. To accomplish this, we construct an outer
measure µ∗and show that the measure µ (the restriction of µ∗to M), induced by µ∗,
is the desired extension deﬁned on σ(A) that agrees with µ on sets from the ﬁeld
A. This extension can be accomplished in a unique way. In particular, for any set
A ⊂Ω, we deﬁne the set function
µ∗(A) = inf
{Ai}
∞P
i=1 µ(Ai),
(3.6)
where {Ai} ranges over all sequences of A-sets such that A ⊂
∞S
i=1 Ai. We call µ∗
the outer measure induced by µ. Carath´eodory (1918) was the ﬁrst to deﬁne outer
measures. The proofs of the following results concerning µ∗can be found in Royden
(1989, pp. 292-295).
1. If A ∈A and {Ai} is any sequence of A-sets such that A ⊂
∞S
i=1 Ai, then µ(A) ≤
∞P
i=1 µ(Ai).
2. If A ∈A then µ∗(A) = µ(A).

MEASURE THEORY AND MEASURE SPACES
103
3. The set function µ∗is an outer measure.
4. If A ∈A then A is measurable with respect to µ∗, i.e., A ⊆M.
5. Let µ be a measure on a ﬁeld A and µ∗the outer measure induced by µ and E
any set. Then for ε > 0 there is a set A ∈A with E ⊂A and µ∗(A) ≤µ∗(E) + ε.
There is also a set B ∈Aσδ with E ⊂B and µ∗(E) = µ∗(B), where
Aσδ =
( ∞T
i=1 Ai : Ai ∈Aσ
)
,
and
Aσ =
( ∞S
i=1 Ai : Ai ∈A
)
.
6. Let µ be a σ-ﬁnite measure on a ﬁeld A and µ∗be the outer measure generated
by µ. A set E is µ∗-measurable if and only if E is the proper diﬀerence A ∖B of a
set A ∈Aσδ and a set B with µ∗(B) = 0. Each set B with µ∗(B) = 0 is contained in
a set C ∈Aσδ, with µ∗(C) = 0.
7. The following relationship is true in general: A ⊆σ(A) ⊆M(µ∗) ⊆2Ω.
The results of the previous remark are summarized in Carath´eodory’s theorem
(Royden, 1989, p. 295).
Theorem 3.10 (Carath´eodory) Let µ be a measure on a ﬁeld A and µ∗the outer
measure induced by µ. Then the restriction µ of µ∗to the µ∗-measurable sets is
an extension of µ to a σ-ﬁeld containing A. If µ is ﬁnite or σ-ﬁnite then so is µ.
If µ is σ-ﬁnite, then µ is the only measure on the smallest σ-ﬁeld containing A,
i.e., on σ(A), which is an extension of µ.
3.6.3
Construction of the Lebesgue Measure
The construction of the Lebesgue measure that follows is a special case of the
extension of measure by Carath´eodory. We illustrate the extension approach by
deﬁning the Lebesgue measure over Ω= R. A natural way of measuring an open
interval I = (a, b) in R is its length, that is, l(I) = |I| = b −a and as we have seen,
the Borel σ-ﬁeld B1 on R is generated by the collection I = {(a, b) : a < b}. We
would like to extend the idea of length for open intervals to help us describe the
“length” of any Borel set. First, deﬁne the collection of all ﬁnite disjoint unions of
open intervals of R by
A = {A : A =
nS
i=1 Ii, Ii = (ai, bi), ai < bi, i = 1, 2, . . ., n},
with I ⊆A. We view the empty set ∅as an element of I of length 0, while
R = (−∞, +∞) is a member of I with length ∞. Using these conventions, it is easy
to see that A is a ﬁeld.

104
MEASURE AND INTEGRATION THEORY
Now we can deﬁne the Lebesgue measure on the ﬁeld A by
λ(A) =
nP
i=1 l(Ii) =
nP
i=1(bi −ai).
Note that even if A assumes two diﬀerent representations A =
n1S
i=1 I1i =
n2S
i=1 I2i, λ(A)
remains unchanged.
The next step is to extend the deﬁnition of length to all Borel sets, namely,
B1 = σ(I). For any set A ⊆R, deﬁne EA to be the collection of collections of open
intervals of R that cover A, that is
EA = {{Ii} : A ⊂S
j∈JIj, Ii = (a j, b j), a j < b j, for all j}.
Finally, we deﬁne the outer measure µ induced by l on the class EA, ∀A ⊆R by
µ(A) = inf
A⊂S
j∈J
Ij
P
j∈J l(Ij).
(3.7)
This outer measure µ is called the Lebesgue outer measure, or simply the Lebesgue
measure in R. It is deﬁned for all subsets of R and it agrees with l on A.
Example 3.13 (Lebesgue measure in (0, 1]) Consider Ω= (0, 1] and let B0 de-
note the collection of ﬁnite disjoint unions of intervals (a, b] ⊆Ω, 0 < a < b ≤1,
augmented by the empty set. Then B0 is a ﬁeld but not a σ-ﬁeld, since it does not
contain, for instance, the singletons {x}
 
= T
n (x −1/n, x]
!
. The Lebesgue measure
µ as deﬁned (restricted on B0) is a measure on B0, with l(Ω) = 1 and it is the
Lebesgue measure on B(Ω) = σ(B0), the Borel σ-ﬁeld in Ω= (0, 1]. Since inter-
vals in (0, 1] form a π-system generating B(Ω), λ is the only probability measure
on B(Ω) that assigns to each interval its length as its measure.
Example 3.14 (p-dimensional Lebesgue measure)
Let x = (x1, . . . , xp) ∈Rp
and deﬁne the collection of bounded rectangles R = {R(an, bn), an, bn ∈Rp, ani <
bni, i = 1, 2, . . . , p}, augmented by the empty set, with R(an, bn) = {x : ani < xi ≤
bni, i = 1, 2, . . . , p}. Then the σ-ﬁeld Bp of Borel sets of Rp is generated by R ∪{∅}
and the k-dimensional Lebesgue measure µp is deﬁned on Bp as the extension of
the ordinary volume λp in Rp, where
λp(R(an, bn)) =
pQ
i=1(bni −ani),
and
µp(B) =
inf
B⊂S
n
R(an,bn)
P
n λp(R(an, bn)),
for any B ∈Bp ⊆M(µp) ⊆2Rp. The measure µp deﬁned on M(µp) will be denoted
by µp and is also called the p-dimensional Lebesgue measure. The Carath´eodory
sets M(µp) of Rp are called (p-dimensional) Lebesgue measurable sets or simply
Lebesgue sets. Any function is called Lebesgue measurable if and only if sets of
the form of theorem 3.3 are Lebesgue measurable sets.

MEASURE THEORY AND MEASURE SPACES
105
Some properties of the Lebesgue measure are collected below.
Remark 3.11 (Lebesgue measure) Clearly, the p-dimensional ordinary volume λp
is translation invariant, i.e., for any bounded rectangle R(a, b) and point x ∈Rp,
R(a, b) ⊕x = R(a + x, b + x), so that
λp(R(a, b)) =
pQ
i=1(bi −ai) =
pQ
i=1(bi + xi −(ai + xi)) = λp(R(a + x, b + x)).
1. Translation invariance This property can be extended to the Lebesgue mea-
sure deﬁned on Bp (or even M(µp)), that is, for every B ∈Bp and x ∈Rp we
have µp(B ⊕x) = µp(B). In fact, it can be proven that every translation-invariant
Radon measure v on (Rp, Bp) must be a multiple of the Lebesgue measure, i.e.,
v(B) = cµp(B), for some constant c.
2. Lebesgue in lower dimensional space If A ∈Bp−1 then A has measure 0 with
respect to the Lebesgue measure µp deﬁned on Bp.
One of the consequences of the Carath´eodory extension theorem is discussed
next.
Remark 3.12 (Measure space completion) Recall that a (signed nonnegative)
measure space (Ω, A, µ), µ(A) ≥0, A ∈A, is said to be complete if B ∈A, with
µ(B) = 0 and A ⊂B, imply A ∈A. Denote by Null(µ) = {A ∈A : µ(A) = 0}, all
the measurable sets of measure zero with respect to µ, called null sets, and Null(µ)
is called the null space.
Measure space completion refers to the process by which a measure space
becomes complete, namely, it contains all subsets of null sets. Hence the original
σ-ﬁeld A needs to be augmented with the subsets of the null sets, otherwise we
have the logical paradox of a set having zero measure, while its subset does not.
It can be shown (see for example Vestrup, 2003, pp. 88-96) that the
Carath´eodory extension and uniqueness theorem for a measure µ on a ﬁeld A,
makes the measure space (Ω, M(µ∗), µ∗
M(µ∗)) the completion of the measure space
(Ω, σ(A), µ∗
σ(A)), where µ∗
σ(A) is the restriction of µ∗on σ(A) and µ∗
M(µ∗) is the re-
striction of µ∗on M(µ∗).
To satisfy our curiosity that a measure space is not complete by deﬁnition,
we can show that the Lebesgue measure µ1 restricted to the Borel σ-ﬁeld induces
a measure space (R, B(R), µ1) that is not complete, whereas, (R, M(µ1), µ1) is a
complete measure space.
Example 3.15 (Nonmeasurable set) A set will be nonmeasurable in [0, 1) (with
respect to the Lebesgue measure on B([0, 1))) if it lives in the collection of sets
between B([0, 1)) and 2[0,1), i.e., outside the Borel sets of [0, 1). Let us construct
such a Lebesgue nonmeasurable set. First, if x, y ∈[0, 1), deﬁne the sum modulo 1
of x and y, denoted by x ∔y, by x ∔y = x + y, if x + y < 1 and x ∔y = x + y −1, if
x + y ≥1.
It can be shown that µ1 is translation invariant with respect to modulo 1, that

106
MEASURE AND INTEGRATION THEORY
is, if E ⊂[0, 1) is a measurable set, then for all y ∈[0, 1) the set E ∔y is measurable
and µ1(E ∔y) = µ1(E). Now if x −y is a rational number, we say that x and y are
equivalent and write x ∼y. This is an equivalence relation and hence partitions
[0, 1) into equivalence classes, that is, classes such that any two elements of one
class diﬀer by a rational number, while any two elements of diﬀerent classes diﬀer
by an irrational number.
By the Axiom of Choice, there is a set A which contains exactly one element
from each equivalence class. Let {ri}∞
i=0 denote all the rational numbers in [0, 1) with
r0 = 0 and deﬁne Ai = A∔ri, so that A0 = A. Let x ∈Ai∩A j. Then x = ai∔ri = a j∔ri,
with ai, a j ∈A. But ai −a j = r j −ri is a rational number and hence ai ∼a j. Since
A has only one element from each equivalence class, we must have i = j, which
implies that if i , j, Ai ∩A j = ∅, that is, {Ai} is a pairwise disjoint collection of
sets.
On the other hand, each real number x ∈[0, 1) is in some equivalence class
and so is equivalent to an element in A. But if x diﬀers from an element in A by the
rational number ri, then x ∈Ai. Thus S
i Ai = [0, 1). Since each Ai is a translation
modulo 1 of A, each Ai will be measurable if A is and will have the same measure.
This implies that
µ1([0, 1)) =
∞P
i=1 µ1(Ai) =
∞P
i=1 µ1(A),
and the right side is either zero or inﬁnite, depending on whether µ1(A) is zero or
positive. But since µ1([0, 1)) = 1, this leads to a contradiction, and consequently,
A cannot be measurable. This construction is due to Vitali and shows that there are
sets in [0, 1) that are not Borel sets.
3.7
Deﬁning Integrals with Respect to Measures
The last major component of a measure theoretic framework is the concept of
the integral with respect to a measure. The modern notion of the integral is due to
Professeur Henri Lebesgue (1902). We deﬁne and study the integral
R
f dµ =
R
Ω
f (ω)dµ(ω) =
R
Ω
f (ω)µ(dω),
where f : Ω→X is some measurable function in a measure space (Ω, A, µ), where
all the integrals above are equivalent forms and denote the same thing. We need to
make some conventions before we proceed. We agree that 0 ∗∞= ∞∗0 = 0,
x ∗∞= ∞∗x = ∞, x > 0, x −∞= −∞, ∞−x = ∞, x ∈R, ∞∗∞= ∞and we
say that ∞−∞is undeﬁned. Moreover, we agree that inf ∅= ∞. Although we can
deﬁne the integral for any mapping, we consider measurable extended real-valued
functions in this exposition.
Deﬁnition 3.15 Integral
Let (Ω, A, µ) be a measure space and {Ai}n
i=1 a partition of Ω.

DEFINING INTEGRALS WITH RESPECT TO MEASURES
107
1. Simple function Let h : Ω→R be a simple function h(ω) =
nP
i=1 hiIAi(ω), ω ∈
Ω, with {hi}n
i=1 ∈R and Ai = {ω ∈Ω: h(ω) = hi} is A-measurable. The integral of
h with respect to µ is deﬁned by
IR
h =
R
hdµ =
nP
i=1 hiµ(Ai).
2. R
+
0-valued function Let f : Ω→R
+
0 be a measurable, non-negative, extended
real-valued function. We deﬁne the integral of f with respect to µ by
IR
+
f
=
R
f dµ = sup
h≤f
R
hdµ = sup
{Ai}n
i=1
nP
i=1

inf
ω∈Ai f (ω)

µ(Ai),
where the ﬁrst supremum is taken over all non-negative measurable simple func-
tions h such that 0 ≤h ≤f, while the second is taken over all partitions {Ai}n
i=1 of
Ω.
3. R-valued function The integral of an R-valued function f with respect to µ is
deﬁned in terms of f + and f −, the positive and negative parts of f (see Appendix
A.1) by
IR
f =
R
f dµ =
R
f +dµ −
R
f −dµ,
provided that the two integrals on the right are not inﬁnity at the same time, in
which case
R
f dµ is undeﬁned. If f vanishes outside a measurable set A ⊂Ω, we
deﬁne
R
A
f dµ =
R
f IAdµ =
R
f +IAdµ −
R
f −IAdµ.
Some discussion is in order about when we say that an integral exists.
Remark 3.13 (Lebesgue integral and integral existence issues) If the measure µ
is the Lebesgue measure, then the integral is called the Lebesgue integral. By def-
inition 3.11, µ is a nonnegative signed measure, i.e., µ(Ai) ≥0, with yi = inf
ω∈Ai f (ω)
and therefore the signs of the integrals IR
h and IR
+
0
f
depend on the values yi. When µ
is a ﬁnite or σ-ﬁnite measure then the integral is ﬁnite. If f is bounded in R then
the integral is ﬁnite, but if f is, say, R-valued then it is possible that some yi = ∞
or −∞, in which case we say that the integral IR
+
0
f
exists but is inﬁnite.
In the case where yiµ(Ai) = ∞and yjµ(A j) = −∞, for some i , j, we say
that the integral does not exist or that it is undeﬁned, since ∞−∞is undeﬁned.
Finally, note that the value of IR
h is independent of the representation of h we
use. Indeed, if h(x) = P
j yjIBj(x) is a non-canonical representation of h, we have
B j = {x : h(x) = yj} = S
i:xi=yj
Ai, so that yjµ(B j) = P
i:xi=yj
xiµ(Ai), by the additivity of µ
and hence
P
j yjµ(B j) = P
j
P
i:xi=yj
xiµ(Ai) =
nP
i=1 xiµ(Ai) =
R
hdµ.

108
MEASURE AND INTEGRATION THEORY
Example 3.16 (Integrating with respect to counting measure) Let Ω= {1, 2,
. . . , n} and take µ to be the counting measure assigning measure 1 to each singleton,
so that (Ω, 2Ω, µ) is a measure space. Consider the function f (ω) = ω, ∀ω ∈Ωand
let {Ai}n
i=1 be the partition of Ωinto its singletons, where Ai = {i}, with µ(Ai) = 1,
i = 1, 2, . . ., n. Clearly, f is simple since it can be written as f (ω) =
nP
i=1 iIAi(ω) and
its integral is therefore given by
R
f dµ =
nP
i=1 iµ(Ai) =
nP
i=1 i = n(n + 1)/2.
The following theorem illustrates how we can prove some basic results for in-
tegrals of simple functions.
Theorem 3.11 Let h =
nP
i=1 xiIAi and g =
mP
j=1 yjIBj be two real-valued simple
functions in the measure space (Ω, A, µ) and a and b real constants. Then we can
show that
(i)
R
(ah + bg)dµ = a
R
hdµ + b
R
gdµ,
(ii) if g ≤h a.e. [µ] then
R
gdµ ≤
R
hdµ, and
(iii) if h is bounded then inf
h≤ϕ
R
ϕdµ = sup
ψ≤h
R
ψdµ, where ϕ and ψ range over all
simple functions.
Proof. We prove only the ﬁrst two parts here. The last part is requested as an
exercise.
(i) First note that the collection of intersections {Ek} = {Ai ∩B j} form a ﬁnite
disjoint collection of N = nm measurable sets since {Ai} and {B j} are the sets in the
canonical representations of h and g, respectively, so that we may write h =
NP
k=1 akIEk
and g =
NP
k=1 bkIEk. Therefore in view of remark 3.13 we can write
ah + bg =
NP
k=1(aak + bbk)IEk =
R
(ah + bg) dµ,
and the claim is established.
(ii) To prove this part we note that
R
hdµ −
R
gdµ =
R
(h −g)dµ ≥0,
since the integral of a simple function, which is greater than or equal to zero a.e., is
nonnegative by the deﬁnition of the integral.
Clearly, owing to the sequential form of deﬁnition 3.15, we can obtain results
ﬁrst for IR
h , which is easier to do, and then extend them to the R
+
0 and R cases. When
this extension is not straightforward we require additional tools (see remark 3.15).
First, we summarize some of these results for R
+
0-valued integrands below.

DEFINING INTEGRALS WITH RESPECT TO MEASURES
109
Remark 3.14 (Properties of IR
+
0
f ) Assume that f, g are R
+
0-valued in what follows.
1. If f = 0 a.e. [µ] then
R
f dµ = 0.
2. Monotonicity If 0 ≤f (ω) ≤g(ω), ∀ω ∈Ω, then
R
f dµ ≤
R
gdµ. Moreover, if
0 ≤f ≤g a.e. [µ] then
R
f dµ ≤
R
gdµ.
3. Linearity If a, b ∈R+
0 then
R
(a f + bg)dµ = a
R
f dµ + b
R
gdµ.
4. If f = g a.e. [µ] then
R
f dµ =
R
gdµ.
5. If µ({ω : f (ω) > 0}) > 0 then
R
f dµ > 0.
6. If
R
f dµ < ∞then f < ∞a.e. [µ].
7. Assume that µ(A) = 0. Then
R
f dµ =
R
A
f dµ by deﬁnition, since µ vanishes
outside A.
8. If f is bounded on a measurable set E ⊂R with µ1(E) < ∞, then sup
h≤f
R
E
hdµ1 =
inf
f≤ϕ
R
E
ϕdµ1, for all simple functions h and ϕ if and only if f is measurable.
The additional tool we need in order to generalize results from simple functions
to any R
+
0-valued function and then to any R-valued function is the following.
Theorem 3.12 (Monotone convergence theorem (MCT)) Let { fn} be a se-
quence of nonnegative measurable functions such that 0 ≤
fn(ω) ↑
f (ω),
∀ω ∈Ω. Then 0 ≤
R
fndµ ↑
R
f dµ.
Proof. Since fn ≤f, using remark 3.14.2 we have that the sequence an =
R
fndµ is nondecreasing and bounded above by
R
f dµ = sup
{Ai}k
i=1
kP
i=1

inf
ω∈Ai f (ω)

µ(Ai).
Now if an = ∞for any positive integer n then the claim is trivially true.
Therefore, assuming that an < ∞, ∀n = 1, 2, ..., to complete the proof it
suﬃces to show that
lim
n an = lim
n
R
fndµ ≥
kP
i=1 xiµ(Ai),
(3.8)
for any partition {Ai}k
i=1 of Ω, with xi = inf
ω∈Ai f (ω). Fix an ε > 0, with ε < xi and set
Ain = {ω ∈Ai : fn(ω) > xi −ε}, i = 1, 2, ..., k. Since fn ↑f we have that Ain ↑Ai,
as n →∞, with 0 < µ(Ain) < µ(Ai) < ∞. Now decomposing Ωinto A1n, ..., Akn and
 kS
i=1 Ain
!c
we note that
R
fndµ ≥
kP
i=1(xi −ε)µ(Ain) →
kP
i=1(xi −ε)µ(Ai) =
kP
i=1 xiµ(Ai) −ε
kP
i=1 µ(Ai),

110
MEASURE AND INTEGRATION THEORY
for arbitrary ε. Sending ε →0 we have that equation (3.8) holds.
Example 3.17 (Series of real numbers and MCT) Consider a countable space
Ω= {1, 2, . . ., ∞} and take µ to be the counting measure so that (Ω, 2Ω, µ) is a
measure space. Now any R
+ function deﬁned on Ωcan be thought of as a sequence
xm ≥0, m = 1, 2, . . .. Consider the sequence of functions x(n)
m
= xm, if m ≤n
and 0, if m > n and let {Ai}n+1
i=1 be a partition of Ω, with Ai = {i}, i = 1, 2, . . ., n
and An+1 = {n + 1, n + 2, . . . }. Then we can write x(n)
m = xmI[1,n](m) =
nP
k=1 xkIAk(m),
m, n = 1, 2, . . ., a sum of simple functions f (k)
m = IAk(m), so that its integral is given
by
R
x(n)
m dµ =
nP
k=1 xk
R
f (k)
m dµ =
nP
k=1 xkµ(Ak) =
nP
k=1 xk.
Now applying the MCT on the sequence {x(n)
m }, with 0 ≤x(n)
m ↑
∞P
k=1 xkIAk(m) yields
nP
k=1 xk ↑
∞P
k=1 xk, where the limit
∞P
k=1 xk may be ﬁnite or inﬁnite.
Deﬁnition 3.16 Integrability
Let f : Ω→R be deﬁned on a measure space (Ω, A, µ). We say that f is
integrable with respect to µ if
R
| f |dµ =
R
f +dµ +
R
f −dµ < +∞.
Example 3.18 (Integrability counterexample)
Integrability not only implies
that the integral is deﬁned but that it is also ﬁnite. In particular, we must have
R
f +dµ < +∞and
R
f −dµ < +∞. Consider the setup of the previous example
but now assume that xm is R-valued. The function xm = (−1)m+1/m, m = 1, 2, . . . ,
is not integrable by deﬁnition 3.16 since
∞P
m=1 x+
m =
∞P
m=1 x−
m = +∞, i.e., the inte-
gral is undeﬁned but the alternative harmonic series
∞P
m=1(−1)m+1/m, does converge
since lim
n
nP
m=1(−1)m+1/m = log 2. This illustrates why integrability of f requires both
R
f +dµ and
R
f −dµ to be ﬁnite.
Next we discuss how we can use the MCT and integration to the limit in order
to prove statements about a nonnegative measurable function.
Remark 3.15 (Using the MCT) Note that from theorem 3.4 any nonnegative func-
tion f can be written as a limit of nonnegative increasing simple functions, the
requirement of the MCT. Consequently, the MCT allows us to pass the limit under
the integral sign. Many claims in integration and probability theory can be proven
this way. The steps are summarized as follows.

DEFINING INTEGRALS WITH RESPECT TO MEASURES
111
Step 1: Show that the claim holds for the simple function ϕ = IA.
Step 2: By linearity of IR
h , we have the claim for any nonnegative simple
function, including sequences of simple functions like those of theorem 3.4.
Step 3: The claim holds for any nonnegative measurable function since there
is a sequence of simple functions { fn} such that 0 ≤fn(ω) ↑f (ω) and the MCT
allows us to prove the claim by passing the limit under the integral sign.
Example 3.19 (Using the MCT for proofs)
We illustrate the approach of re-
mark 3.15.
1. Consider the linearity property (remark 3.14.3). The claim holds trivially for
nonnegative simple functions (steps 1 and 2). By theorem 3.4, there exist nonnega-
tive sequences of simple functions { fn} and {gn} such that 0 ≤fn ↑f and 0 ≤gn ↑g
and therefore 0 ≤a fn + bgn ↑a f + bg, so that linearity of the integral for simple
functions yields
a
R
fndµ + b
R
gndµ =
R
(a fn + bgn)dµ,
and the MCT allows us to pass the limit under the integral sign and obtain the claim.
2. If 0 ≤fn ↑f a.e. [µ] then 0 ≤
R
fndµ ↑
R
f dµ. To see this consider 0 ≤fn ↑f
on the set A = {ω ∈Ω: 0 ≤fn(ω) ↑f (ω)}, with µ(Ac) = 0, so that 0 ≤fnIA ↑f IA
holds everywhere. Then the MCT gives
R
fndµ =
R
fnIAdµ ↑
R
f IAdµ =
R
f dµ.
We are now ready to collect integral properties for extended real-valued inte-
grands.
Remark 3.16 (Properties of IR
f ) For what follows, assume that all functions are
R-valued and measurable in (Ω, A, µ) where a, b, c ∈R.
1. Linearity We have
R
(a f + bg)dµ = a
R
f dµ + b
R
gdµ, provided that the ex-
pression on the right is meaningful. All integrals are deﬁned when f and g are
integrable.
2. If f = c a.e. [µ] then
R
f dµ = cµ(Ω).
3. If f = g a.e. [µ] then either the integrals of f and g with respect to µ are both
deﬁned and are equal or neither is deﬁned.
4. Monotonicity If f ≤g a.e. [µ] and
R
f dµ is deﬁned and is diﬀerent from −∞
or
R
gdµ is deﬁned and is diﬀerent from ∞then both integrals are deﬁned and
R
f dµ ≤
R
gdµ.
5. If
R
f dµ =
R
gdµ < +∞and f ≤g a.e. [µ] then f = g a.e. [µ].
6. If
R
f dµ is deﬁned then

R
f dµ
 <
R
| f |dµ.
7. If
R
f dµ is undeﬁned then
R
| f |dµ = +∞.

112
MEASURE AND INTEGRATION THEORY
8.
R
| f + g|dµ ≤
R
| f |dµ +
R
|g|dµ.
9. If fn ≥0 then
R +∞
P
n=1 fndµ =
+∞
P
n=1
R
fndµ.
10. If
+∞
P
n=1 fn converges a.e. [µ] and

nP
k=1 fk
 ≤g a.e. [µ], where g is integrable, then
+∞
P
n=1 fn and the fn are integrable and
R +∞
P
n=1 fndµ =
+∞
P
n=1
R
fndµ.
11. If
+∞
P
n=1
R
| fn|dµ < ∞then
+∞
P
n=1 fn converges absolutely a.e. [µ] and is integrable
and
R +∞
P
n=1 fndµ =
+∞
P
n=1
R
fndµ.
12. If | f | ≤|g| a.e. [µ] and g is integrable then f is integrable as well.
13. If µ(Ω) < ∞, then a bounded function f is integrable.
The following theorems can be proven easily using the MCT and provide three
more ways of passing the limit under the integral sign.
Theorem 3.13 (Fatou’s lemma) Let { fn} be a sequence of nonnegative measur-
able functions. Then
R
limfndµ ≤lim
R
fndµ.
Proof. Set gn = inf
n≥k fk and note that 0 ≤gn ↑g = limfn so that the fact that
gn ≤fn and the MCT gives R
fndµ ≥
R
gndµ ↑
R
gdµ,
as claimed.
Theorem 3.14 (Lebesgue’s dominated convergence theorem (DCT)) Let { fn},
f be R-valued and g ≥0, all measurable functions in (Ω, A, µ). If | fn| ≤g a.e.
[µ], where g is integrable and if fn →f a.e. [µ], then f and fn are integrable and
R
fndµ →
R
f dµ.
Proof. Apply Fatou’s Lemma on the sequences of functions g −fn and g + fn.
Theorem 3.15 (Bounded convergence theorem (BCT)) Suppose that µ(Ω) <
∞and that the fn are uniformly bounded, i.e., there exists M > 0 such that

DEFINING INTEGRALS WITH RESPECT TO MEASURES
113
| fn(ω)| ≤M < ∞, ∀ω ∈Ωand n = 1, 2, . . .. Then fn →f a.e. [µ] implies
R
fndµ →
R
f dµ.
Proof. This is a special case of theorem 3.14.
The last approach we will discuss that allows us to pass the limit under the
integral sign is based on the property of Uniform Integrability. Suppose that f is
integrable. Then | f |I[|f|≥a] goes to 0 a.e. [µ] as a →∞and is dominated by | f | so
that
lim
a→∞
R
|f|≥a
| f |dµ = 0.
(3.9)
Deﬁnition 3.17 Uniform integrability
A sequence { fn} is uniformly integrable if (3.9) holds uniformly in n, that is,
lim
a→∞sup
n
R
|fn|≥a
| fn|dµ = 0.
We collect some results involving uniformly integrable sequences of functions
below.
Remark 3.17 (Uniform integrability) For what follows assume that µ(Ω) < ∞
and fn →f a.e. [µ].
1. If { fn} are uniformly integrable then f is integrable and
R
fndµ →
R
f dµ.
(3.10)
2. If f and { fn} are nonnegative and integrable then equation (3.10) implies that
{ fn} are uniformly integrable.
3. If f and { fn} are integrable then the following are equivalent (i) { fn} are uni-
formly integrable, (b)
R
| f −fn|dµ →0, and (c)
R
| fn|dµ →
R
| f |dµ.
We end this section with a note on treating the integral as an operator.
Remark 3.18 (Integral operator) We can deﬁne an integral operator on the col-
lection of all R-valued functions by T(f ) =
R
f dµ, which is a functional that
takes as arguments functions and yields their integral with respect to µ. Since
T(a f + bg) = aT(f ) + bT(g), T(f ) is called a linear operator.
3.7.1
Change of Variable and Integration over Sets
An important consequence of transformation of measure in integration theory
is change of variable, which can be used to give tractable forms for integrals that
are otherwise hard to compute. Suppose that (Ω1, A1, µ) is a measure space and let
(Ω2, A2) be a measurable space. Consider a mapping T : Ω1 →Ω2 that is A1|A2-
measurable and deﬁne the induced measure µT −1 on A2 (see remark 3.8.9), by
µT −1(A2) = µ(T −1(A2)), ∀A2 ∈A2.

114
MEASURE AND INTEGRATION THEORY
Theorem 3.16 (Change of variable: general measure) If f : Ω2 →R+ is
A2|B(R+)-measurable then f ◦T : Ω1 →R+ is A1|B(R+)-measurable and
R
Ω1
f (T(ω))µ(dω) =
R
Ω2
f (t)µT −1(dt).
(3.11)
If f : Ω2 →R is A2|B(R)-measurable then f ◦T : Ω1 →R is A1|B(R)-
measurable and it is integrable [µT −1] if and only if f ◦T is integrable [µ], in
which case (3.11) and R
T −1(A2)
f (T(ω))µ(dω) =
R
A2
f (t)µT −1(dt),
(3.12)
for all A2 ∈A2, hold, in the sense that if the left side is deﬁned then the other is
deﬁned and they are the same. When f is R+-valued (3.11) always holds.
Proof. The measurability arguments follow by remark 3.7.2. For the trivial
simple function f = IA2, A2 ∈A2, we have f ◦T = IT −1(A2) and (3.11) reduces to
equation (3.11). By linearity of the integral, (3.11) holds for all nonnegative simple
functions and if fn are simple functions such that 0 ≤fn ↑f, then 0 ≤fn ◦T ↑f ◦T
and (3.11) follows by the MCT.
Applying (3.11) to | f | establishes the assertion about integrability and for
integrable f, (3.11) follows by decomposition into positive and negative parts. Fi-
nally, replacing f by f IA2 in (3.11) reduces to (3.12).
Example 3.20 (Change of variable) Let (Ω2, A2) = (R, B1) and let T = ϕ, a
B1-measurable, real-valued function. If f (x) = x then (3.11) becomes
R
Ω1
ϕ(ω)µ(dω) =
R
R
tµϕ−1(dt).
For a simple function ϕ = P
i xiIAi we have that µϕ−1 has mass µ(Ai) at xi so that the
latter integral reduces to P
i xiµ(Ai).
We summarize some results on integration over sets in the next theorem
(Billingsley, 2012, p. 226).
Theorem 3.17 (Integration over sets) We can show the following.
(i) If A1, A2, . . . , are disjoint with B =
+∞
S
n=1 An and if f is either nonnegative or
integrable then
R
B
f dµ =
+∞
P
n=1
R
An
f dµ.
(ii) If f and g are nonnegative and
R
A
f dµ =
R
A
gdµ for all A ∈A and if µ is
σ-ﬁnite then f = g a.e. [µ].

DEFINING INTEGRALS WITH RESPECT TO MEASURES
115
(iii) If f and g are integrable and
R
A
f dµ =
R
A
gdµ for all A ∈A and if µ is σ-ﬁnite
then f = g a.e. [µ].
(iv) If f and g are integrable and
R
A
f dµ =
R
A
gdµ for all A ∈P, where P is a
π-system generating A and Ωis a ﬁnite or countable union of P-sets, then f = g
a.e. [µ].
3.7.2
Lebesgue, Riemann and Riemann-Stieltjes Integrals
Recall that for any function f : [a, b] →R, the Riemann integral R −
bR
a
f (x)dx
is deﬁned over an arbitrary ﬁnite partition {Ai = (ξi−1, ξi)}n
i=1, with a = ξ0 < ξ1 <
· · · < ξn = b, as the common value of the upper and lower Riemann integrals, that
is
R −
bR
a
f (x)dx = sup
{ξi}n
i=0
nP
i=1
"
inf
ξi−1<x<ξi f (x)
#
(ξi −ξi−1).
(3.13)
On the other hand, the Lebesgue integral of f denoted by L −
bR
a
f (x)dx (or
R
f I[a,b]dµ1 =
bR
a
f dµ1) is deﬁned using equation (3.7) by
R
f I[a,b]dµ1 = sup
{Ai}n
i=1
nP
i=1

inf
x∈Ai f (x)

µ1(Ai)
(3.14)
where the Lebesgue measure in R for the open interval Ai is simply µ1(Ai) = ξi −
ξi−1, i = 1, 2, . . ., n. Obviously, equations (3.13) and (3.14) appear to arrive at the
same result, although the two integral deﬁnitions are diﬀerent. Does this happen
always? The answer is no, as we see in the next classic example.
Example 3.21 (Riemann integral counterexample)
Let f (x) = 0, if x irra-
tional, or 1, if x rational, be the indicator function of the set of rationals in (0, 1].
Then the Lebesgue integral L −
1R
0
f (x)dx is 0 because f
= 0 a.e. [µ1]. But
for an arbitrary partition {Ai = (ξi−1, ξi)} of (0, 1], Mi =
sup
ξi−1<x<ξi
f (x) = 1 and
mi =
inf
ξi−1<x<ξi f (x) = 0 for all i, so that the upper and lower Riemann sums are
given by
S =
nP
i=1(ξi −ξi−1)1 = b −a,
and
s =
nP
i=1(ξi −ξi−1)0 = 0,

116
MEASURE AND INTEGRATION THEORY
and hence R −
bR
a
f (x)dx = inf
{ξi}n
i=0
S = b −a, while R −
bR
a
f (x)dx = sup
{ξi}n
i=0
s = 0, so that
f is not Riemann integrable.
Next we discuss when the Lebesgue and Riemann integrals coincide.
Remark 3.19 (Lebesgue and Riemann integrals) Clearly, the Lebesgue integral
(when deﬁned) can be thought of as a generalization of the Riemann integral. For
what follows in the rest of the book, Lebesgue integrals will be computed using
their Riemann versions, whenever possible, since Riemann integrals are easier to
compute. Let f : Ω→[a, b], Ω⊂Rp, be a bounded, measurable function in the
space (Ω, Bp, µp). The following results clarify when the two integrals coincide.
1. If f is Riemann integrable on [a, b] then it is measurable and R −
bR
a
f (x)dx =
L −
bR
a
f (x)dx.
2. The function f is Riemann integrable if and only if f is continuous on Ωa.e.
[µp].
3. If f is Riemann integrable then R −
R
Ω
f (x)dx = L −
R
Ω
f dµp.
Example 3.22 (Riemann does not imply Lebesgue integrability)
In order to
appreciate that Riemann integrability does not imply Lebesgue integrability always,
consider integrating f (x) = sin x
x , x ∈(0, +∞). It can be shown (see example 3.23)
that R −
+∞
R
0
f (x)dx = lim
t→∞
tR
0
sin x
x dx = π/2, but the Lebesgue integral is undeﬁned
since f (x) is not Lebesgue integrable over (0, +∞) because the integrals of f + and
f −are both +∞.
The Riemann-Stieltjes (RS) integral RS −
bR
a
f (x)dg(x) or simply
bR
a
f (x)dg(x) is
deﬁned using similar arguments as in the case of the Riemann integral. First recall
the ε −δ deﬁnition of the Riemann integral: a bounded function f : [a, b] →R is
Riemann integrable with R −
bR
a
f (x)dx = r if and only if ∀ε > 0, ∃δ > 0, such that
for any ﬁnite partition {Ai = (ξi−1, ξi)}n
i=1, with a = ξ0 < ξ1 < · · · < ξn = b and
ξi −ξi−1 < δ we have
r −
nP
i=1 f (xi)(ξi −ξi−1)
 < ε, for some xi ∈Ai.
Similarly, a bounded function f : [a, b] →R is RS integrable with respect to
any bounded function g : [a, b] →R, if ∀ε > 0, ∃δ > 0, such that for any ﬁnite

DEFINING INTEGRALS WITH RESPECT TO MEASURES
117
partition {Ai = (ξi−1, ξi)}n
i=1, with a = ξ0 < ξ1 < · · · < ξn = b and g(ξi) −g(ξi−1) < δ
we have
r −
nP
i=1 f (xi)(g(ξi) −g(ξi−1))
 < ε, for some xi ∈Ai.
Note that under certain conditions, such as Riemann integrability of f and f
being RS integrable with respect to g over all bounded intervals, we can extend
these deﬁnitions of the Riemann and RS integrals to any subset A ⊆R. Moreover,
properties such as linearity, monotonicity and theorems like the MCT as easily seen
for the RS integral using the deﬁnition and similar arguments like the ones we used
for the construction of the general integral in the previous sections.
The following theorems are straightforward to prove using the deﬁnitions of the
Riemann and RS integrals (e.g., see Fristedt and Gray, 1997, pp. 705, 707).
Theorem 3.18 (Riemann-Stieltjes and Riemann integrals) Let g be a function
with a continuous ﬁrst derivative on an interval [a, b] and f a bounded R-valued
function on [a, b]. Then f g′ is Riemann integrable on [a, b] if and only if f is RS
integrable with respect to g on [a, b] in which case we have
RS −
bR
a
f (x)dg(x) = R −
bR
a
f (x)g′(x)dx.
(3.15)
Theorem 3.19 (Integration by parts) Suppose that a function f is RS integrable
with respect to a function g on an interval [a, b]. Then g is RS integrable with
respect to f on [a, b] and
bR
a
f (x)dg(x) = f (b)g(b) −f (a)g(a) −
bR
a
g(x)d f (x).
(3.16)
A direct consequence of the latter theorem is the following fundamental result.
Remark 3.20 (Fundamental theorem of calculus) Take f (x) = 1 and g(x) =
H(x) with H′(x) = h(x) in equation (3.16) to obtain the Fundamental Theorem of
Calculus
bR
a
h(x)dx =
bR
a
dH(x) = H(b) −H(a),
since
bR
a
H(x)d1 = 0 by deﬁnition and RS −
bR
a
dH(x) = R −
bR
a
h(x)dx, by equation
(3.15).
We end this section by connecting the RS integral with the Lebesgue inte-
gral on the Borel σ-ﬁeld in R. First recall that B(R) = σ({(−∞, x]}) and set
F(x) = µ1((−∞, x]), where µ1 is the Lebesgue measure deﬁned on the measure
space (R, B1, µ1). Clearly, F(x) is nonnegative and nondecreasing and we notice

118
MEASURE AND INTEGRATION THEORY
that for a < b we have
R
R
I(a,b](x)dF(x)
=
bR
a
dF(x) = F(b) −F(a) = µ1((−∞, b]) −µ1((−∞, a])
=
µ1((−∞, b] ∖(−∞, a]) = µ1((a, b]) =
R
R
I(a,b](x)µ1(dx) =
bR
a
dµ1,
so that the RS and Lebesgue integrals coincide when integrating the trivial simple
function I(a,b]. Omitting all the details involved, we can show that by linearity the
two integrals coincide on any nonnegative simple function ϕ =
nP
i=1 aiIAi, i.e.,
R
R
nP
i=1 aiIAidF(x) =
R
R
nP
i=1 aiIAiµ1(dx)
so that the MCT applied on both sides and for both types of integrals yields that for
any nonnegative, measurable function f the two integrals coincide. Therefore, for
any R-valued measurable function f = f + −f −we have
R
R
f (x)dF(x) =
R
R
f (x)µ1(dx),
(3.17)
provided that all integrals involved are deﬁned. This construction can be easily
extended to Rp by considering all bounded rectangles and the Lebesgue measure
µp on Bp. It can be shown that (3.17) holds for any Radon measure, not just the
Lebesgue measure.
3.7.3
Radon-Nikodym Theorem
The theorem that allows us to deﬁne and work with densities is the Radon-
Nikodym theorem. First note that for any measure µ and f ≥0 measurable function
deﬁned on a measure space (Ω, A, µ), we can deﬁne a new measure ν(A) =
R
A
f dµ,
A ∈A, which will be ﬁnite if and only if f is integrable. Since the integral over a
set of µ−measure zero is zero, we have that ν ≪µ. The converse of this result plays
an important role in probability theory. In particular, the next theorem shows that
every absolutely continuous measure ν is obtained in this fashion from µ, subject
to σ-ﬁniteness restrictions.
Theorem 3.20 (Radon-Nikodym) Let (Ω, A, µ) be a σ-ﬁnite measure space and
let ν be a σ-ﬁnite measure deﬁned on A, which is absolutely continuous with
respect to µ. Then there exists a nonnegative A|B(R+
0)-measurable function f
such that for each set A ∈A we have
ν(A) =
R
A
f dµ.
(3.18)
The function f is unique in the sense that if g is any measurable function with
this property then g = f a.e. [µ]. The function f is called the Radon-Nikodym
(R-N) derivative with respect to µ and is denoted by f =
h dν
dµ
i
.

DEFINING INTEGRALS WITH RESPECT TO MEASURES
119
Proof (Finite case). Consider ﬁrst the ﬁnite case for the measures ν and µ and
assume that ν ≪µ. Let G be the collection of nonnegative functions g such that
R
A
gdµ ≤ν(A), for all A ∈A and let a = sup
g∈G
R
gdµ ≤v(Ω). Choose g∗
n ∈G such
that
R
g∗
ndµ > a −1
n and set fn = max{g∗
1, ..., g∗
n} with lim
n fn = f (more precisely
fn ↑f ). We claim that this f has the desired property, i.e., f =
h dν
dµ
i
, so that equality
in
R
A
gdµ ≤ν(A) is achieved for g = f . First we show that f ∈G.
Take any g1, g2 ∈G and write
R
A
max{g1, g2}dµ
=
R
A∩{g1≥g2}
max{g1, g2}dµ +
R
A∩{g1<g2}
max{g1, g2}dµ
≤
v(A ∩{g1 ≥g2}) + v(A ∩{g1 < g2}) = v(A),
so that G is closed under the formation of ﬁnite maxima. Now take any gn ∈G such
that gn ↑g and use the MCT to obtain
R
A
gdµ = lim
n
R
A
gndµ ≤v(A),
which implies that G is closed under nondecreasing limits. Consequently, since
f = lim
n max{g∗
1, ..., g∗
n}, we have f ∈G and since
R
f dµ = lim
n
R
fndµ ≥lim
n
R
g∗
ndµ > lim
n
 
a −1
n
!
≥a,
the integral
R
f dµ is maximal, i.e.,
R
f dµ = a.
Let v0(A) =
R
A
f dµ and note that v1(A) = v(A)−v0(A) ≥0, for all A ∈A, since
f ∈G. Furthermore, v0 is absolutely continuous with respect to µ by deﬁnition.
Writing
v(A) = v0(A) + v1(A),
(3.19)
we see that v0 and v1 are ﬁnite and nonnegative measures. If we show that v1(A) = 0,
for all A ∈A, then v0(A) =
R
A
f dµ = ν(A), for all A ∈A, as claimed. Suppose that
v1 is not singular with respect to µ. From lemma 3.3, there is a set A0 and ε > 0
such that µ(A0) > 0 and εµ(E) ≤v1(E), for all E ⊂A0. Then for all E we can write
R
E
(f + εIA0)dµ
=
R
E
f dµ + εµ(E ∩A0) ≤
R
E
f dµ + v1(E ∩A0)
=
R
E∩A0
f dµ + v1(E ∩A0) +
R
E∩Ac
0
f dµ
≤
v(E ∩A0) +
R
E∩Ac
0
f dµ ≤v(E ∩A0) + v(E ∩Ac
0) = v(E),
so that f + εIA0 ∈G. Since
R
(f + εIA0)dµ = a + εµ(A0) > a, f is not maximal. This
contradiction arises since we assumed v1 is not singular with µ and so we must have
v1 ⊥µ, which means that there is some set S ⊂Ωsuch that v1(S ) = µ(S c) = 0.
But since v ≪µ, µ(S c) = 0 =⇒v(S c) = 0 and hence v1(S c) = v(S c) −
v0(S c) ≤v(S c) = 0, so that v1(S ) + v1(S c) = 0, or v1(Ω) = 0. Hence v1(A) = 0 for

120
MEASURE AND INTEGRATION THEORY
all A ∈A and (3.19) yields the claim. Note that the absolute continuity v ≪µ was
not used in the construction of v0 and v1, thus showing that v can be decomposed
always into an absolutely continuous part and a singular part with respect to µ.
Therefore, we can think of the presentation up to the last paragraph as the proof for
the ﬁnite measure case of the Lebesgue decomposition theorem (remark 3.8.14).
Proof (σ-ﬁnite case). To prove the σ-ﬁnite case, recall remark 3.8.2 and
the alternative deﬁnition of a σ-ﬁnite measure. Since µ and v are σ-ﬁnite measures,
there are partitions {Ai} and {B j} of Ω, such that µ(Ai) < ∞and v(B j) < ∞. Note that
the collection of sets C = {C : C = Ai ∩B j} form a partition of Ω. Consequently,
we can write the σ-ﬁnite measures µ and v, as µ =
+∞
P
i=1 µi and v =
+∞
P
i=1 vi, where
µi(A) = µ(A ∩Ci) < ∞and vi(A) = v(A ∩Ci) < ∞, ∀A ∈A, i = 1, 2, ..., are
sequences of ﬁnite, mutually singular measures on (Ω, A), i.e., µi ⊥µj, i , j and
vi ⊥vj, i , j. Since µi and vi are ﬁnite, the Lebesgue decomposition theorem for
the ﬁnite case we proved above allows the decomposition of vi as: vi = vi0 + vi1,
where vi0 ≪µi and vi1 ⊥µi. Then for all A ∈A we can write
v(A)
=
+∞
P
i=1 vi(A) =
+∞
P
i=1[vi0(A) + vi1(A)] =
+∞
P
i=1 vi0(A) +
+∞
P
i=1 vi1(A)
=
v0(A) + v1(A),
where v0 =
+∞
P
i=1 vi0 and v1 =
+∞
P
i=1 vi1 and it is straightforward to show that v0 and v1 are
σ-ﬁnite measures.
Now if A ∈A with µi(A) = 0, then vi0(A) = 0, for all i and hence v0 ≪µi.
Similarly, since vi1 ⊥µi, there exists A ∈A such that µi(Ac) = vi1(A) = 0, so that
v1 ⊥µi, for all i.
Moreover, if A ∈A with µ(A) = 0, then since µi is ﬁnite and nonnegative we
must have µi(A) = 0, for all i and therefore v0(A) = 0, which implies that v0 ≪µ.
Consider now v1 ⊥µi, for all i, so that there exists A ∈A with µi(Ac) = v1(A) = 0.
But µ(Ac) =
+∞
P
i=1 µi(Ac) = 0∞= 0 and hence v1 ⊥µ. The proof up to this point
establishes the Lebesgue decomposition theorem in the general case.
Now assume that v ≪µ, that is, for all A ∈A, µ(A) = 0 =⇒v(A) = 0.
But this implies that vi ≪µi, since µ(A) = 0 =⇒µi(A) = 0, for all i and µi(A) =
µ(A ∩Ci) = 0 =⇒v(A ∩Ci) = 0 =⇒vi(A) = 0. From the ﬁnite measure case of the
Radon-Nikodym theorem, there exist nonnegative A|B(R+
0)-measurable functions
{ fi}, such that fi =
h dvi
dµi
i
, i = 1, 2, ..., or vi(A) =
R
A
fidµi, A ∈A. Set f =
+∞
P
i=1 fiIAi, so
that f = fi on Ai. Clearly, f is nonnegative, ﬁnite valued and A|B(R+
0)-measurable.
Now since v =
+∞
P
i=1 vi we can write
v(A) =
+∞
P
i=1 vi(A) =
+∞
P
i=1
R
A
fidµi =
+∞
P
i=1
R
A
fiIAidµ =
R
A
+∞
P
i=1 fiIAidµ =
R
A
f dµ,

DEFINING INTEGRALS WITH RESPECT TO MEASURES
121
where we used remark 3.16.9 to pass the sum under the integral sign and thus the
claim is established.
The R-N derivative plays the role of the density of a probability measure and
the following general properties can be useful in establishing a variety of results for
a probability measure.
Remark 3.21 (R-N derivative properties) Let µ, λ and v be σ-ﬁnite measures on
the same measurable space (Ω, A).
1. If v ≪µ and f is a nonnegative, measurable function then
R
f dv =
R
f
"dv
dµ
#
dµ.
2. If v ≪µ and λ ≪µ then"d(v + λ)
dµ
#
=
"dv
dµ
#
+
"dλ
dµ
#
.
3. If v ≪µ ≪λ then
"dv
dλ
#
=
"dv
dµ
# "dµ
dλ
#
.
4. If v ≪µ and µ ≪v then
"dv
dµ
#
=
"dµ
dv
#−1
.
3.7.4
Product Measure and Fubini Theorem
Extending integration theory from one to higher dimensions can be accom-
plished by deﬁning product measures. The tool that we can use to compute inte-
grals over products of σ-ﬁnite measure spaces is Professore Guido Fubini’s famous
theorem which allows us to calculate double integrals as iterated integrals.
Recall that if (Ω1, A1) and (Ω2, A2) are two measurable spaces then the product
measurable space is deﬁned by (Ω, A), where Ω= Ω1 × Ω2, A = A1
N
A2 and
A is generated by the collection of all measurable rectangles A = A1 × A2 ⊂Ω, for
A1 ∈A1, A2 ∈A2. Suppose that we equip the measurable spaces with two σ-ﬁnite
measures µ1, µ2, so that (Ω1, A1, µ1) and (Ω2, A2, µ2) are σ-ﬁnite measure spaces.
To motivate the construction of the product measure, let us use a probabilis-
tic point of view where we assume that µ1, µ2 are probability measures, so that
µ1(Ω1) = µ2(Ω1) = 1. In this case, we would like the product measure we create on
(Ω, A) to coincide with µ1 and µ2 for measurable rectangles of the form A1 × Ω2
and Ω1 × A2. An obvious candidate is the set function µ = µ1 × µ2 deﬁned by
µ(A1 × A2) = µ1(A1)µ2(A2),
(3.20)
and we investigate its general properties next. Note that in Rp we have to work with
the collection of all measurable rectangles since Bp is generated by these sets.

122
MEASURE AND INTEGRATION THEORY
We begin our discussion with some useful results that will lead to the general
deﬁnition of the product measure and corresponding product space. Recall remark
3.5.9 and the deﬁnition of the section of the set. In a similar fashion, we can deﬁne
the ω1 section of an A-measurable function f by f (ω1, .), for every ω1 ∈Ω1. Based
on these deﬁnitions we can prove the following.
Lemma 3.4 Let (Ω1, A1, µ1) and (Ω2, A2, µ2) be σ-ﬁnite measure spaces and con-
sider the product measurable space (Ω, A) with Ω= Ω1 × Ω2 and A = A1
N
A2
and let µ = µ1 × µ2 as deﬁned by equation (3.20).
(i) If A is A-measurable then for each ﬁxed ω1 ∈Ω1, the ω1 section of A is A2-
measurable, i.e., Aω1 ∈A2 and similarly, for each ω2 ∈Ω2 we have Aω2 ∈A1.
(ii) If f is measurable A|B1 then for each ﬁxed ω1 ∈Ω1, the ω1 section of f,
f (ω1, .), is measurable A2|B1 and for each ﬁxed ω2 ∈Ω2 we have that the ω2 sec-
tion of f , f (., ω2), is measurable A1|B1.
(iii) Let {Ai1 × Ai2} be a countable, disjoint collection of measurable rectangles
whose union is a measurable rectangle A1 × A2 = S
i (Ai1 × Ai2). Then µ(A1 × A2) =
P
i µ(Ai1 × Ai2), so that µ is a measure on the measurable rectangles R∗= A1 × A2.
(iv) For any A ∈A, the function gA(ω1) = µ2(Aω1), ∀ω1 ∈Ω1, is measurable A1|B1
and the function hA(ω2) = µ1(Aω2), ∀ω2 ∈Ω2, is measurable A2|B1. Moreover,
R
Ω1
gAdµ1 =
R
Ω2
hAdµ2.
(3.21)
Proof. (i) Fix ω1 ∈Ω1 and consider the mapping Tω1 : Ω2 →Ω1 × Ω2,
deﬁned by Tω1(ω2) = (ω1, ω2). If A = A1 × A2 ∈A is a measurable rectangle,
then T −1
ω1 (A) = {ω2 ∈Ω2 : (ω1, ω2) ∈A} = Aω1 is equal to A2 if ω1 ∈A1 or ∅
if ω1 < A1 and in either case T −1
ω1 (A) ∈A2. Consequently, since the measurable
rectangles generate A, remark 3.7.3 yields that Tω1 is measurable A2|A1 × A2 and
hence T −1
ω1 (A) = Aω1 ∈A2. By symmetry we have Aω2 ∈A1.
(ii) By remark 3.7.2 if f is measurable A|B1 then f ◦Tω1 is measurable A2|B1 so
that f (ω1, .) = (f ◦Tω1)(.) is measurable A2. Similarly, by the symmetry of these
statements, we have that f (., ω2) is measurable A1, for each ﬁxed ω2 ∈Ω2.
(iii) Clearly, µ(∅) = 0 and µ(A) ≥0, for all A ∈R∗. Fix a point ω1 ∈A1. Then for
each ω2 ∈A2, the point (ω1, ω2) belongs to exactly one rectangle Ai1 × Ai2. Thus A2
is the disjoint union of those Ai2 such that ω1 is in the corresponding Ai1. Hence
P
i µ2(Ai2)IAi1(ω1) = µ2(A2)IA1(ω1),
since µ2 is countably additive. Therefore
P
i
R
µ2(Ai2)IAi1(ω1)dµ1 =
R
µ2(A2)IA1(ω1)dµ1,
or
P
i µ2(Ai2)µ1(Ai1) = µ2(A2)µ1(A1),
as claimed.
(iv) From part (i) for any A-set A we have Aω2 ∈A1 and Aω1 ∈A2 so that the

DEFINING INTEGRALS WITH RESPECT TO MEASURES
123
functions µ1(Aω2) and µ2(Aω1), for ﬁxed (ω1, ω2) ∈Ω, are well deﬁned. Due to
symmetry we only consider the measurability statements about gA(ω1), for ﬁxed
ω1 ∈Ω1. Consider the collection L of sets A ∈A, such that the function gA(ω1) =
µ2(Aω1), for ﬁxed ω1 ∈Ω1, is measurable A1|B1 and equation (3.21) holds. Clearly,
L ⊆A. We need to show the other direction.
First recall that the collection of measurable rectangles R∗= A1 × A2 =
{A = A1 × A2 ⊂Ω: A1 ∈A1 and A2 ∈A2} form a π-system and generate A, i.e.,
A = σ(A1 × A2) by deﬁnition 3.6. We show that R∗⊆L. Indeed, write
(A1 × A2)ω1 =

A2, ω1 ∈Ω1
∅, ω1 < Ω1
,
so that gA(ω1) = µ2(Aω1) = IA1(ω1)µ2(A2). Since A1 ∈A1, the function IA1(ω1) is
A1|B1-measurable and therefore gA(ω1) is A1|B1-measurable (since µ2(A2) ∈B1).
In addition
R
Ω1
IA1(ω1)µ2(A2)dµ1(ω1) = µ1(A1)µ2(A2) =
R
Ω2
IA2(ω2)µ1(A1)dµ2(ω2),
(3.22)
so that equation (3.21) is established for measurable rectangles.
Next we show that L is a λ-system. Clearly, Ω∈L since Ωis a measurable
rectangle. Suppose that A = A1 × A2 ∈L so that gA(ω1) is A1|B1-measurable and
therefore for all F ∈B1 we have g−1
A (F) ∈A1. We can write
g−1
Ac(F) = {ω1 ∈Ω1 : µ2((Ac)ω1) ∈F} = {ω1 ∈Ω1 : µ2( Aω1
c) ∈F}.
If µ2(Ω2) < ∞then
g−1
Ac(F)
=
{ω1 ∈Ω1 : µ2(Ω2) −µ2(Aω1) ∈F}
=
{ω1 ∈Ω1 : µ2(Aω1) ∈−F + µ2(Ω2)} = g−1
A (G),
where G = −F + µ2(Ω2) = {r ∈R : r = −ω1 + µ2(Ω2), ω1 ∈F} ∈B1, is well
deﬁned, so that g−1
A (G) ∈A1, i.e., gAc(ω1) is A1|B1-measurable and hence Ac ∈L.
When µ2(Ω2) = +∞, we have
g−1
Ac(F) = {ω1 ∈Ω1 : ∞∈F} =

Ω1, ∞∈F
∅, ∞< F
,
and in both cases g−1
Ac(F) ∈A1, so that gAc(ω1) is A1|B1-measurable and similarly
hAc(ω1) is A2|B1-measurable. To show that Ac ∈L it remains to show equation
(3.21). Since A ∈L we have
R
Ω1
µ2(Aω1)dµ1(ω1) =
R
Ω2
µ1(Aω2)dµ2(ω2),
so that
R
Ω1
gAcdµ1
=
R
Ω1
µ2((Ac)ω1)dµ1(ω1) =
R
Ω1
µ2( Aω1
c)dµ1(ω1)
=
R
Ω1
[µ2(Ω2) −µ2(Aω1)]dµ1(ω1)
=
µ2(Ω2)
R
Ω1
dµ1(ω1) −
R
Ω1
µ2(Aω1)dµ1(ω1)
=
µ2(Ω2)µ1(Ω1) −µ(A),

124
MEASURE AND INTEGRATION THEORY
and similarly
R
Ω2
hAcdµ2 = µ1(Ω1)µ2(Ω2) −µ(A).
This shows that Ac ∈L.
Now consider a disjoint sequence {Bi} of L-sets, with B = S
i Bi. Then the
mappings µ2((Bi)ω1) are A1|B1-measurable and therefore
P
i µ2((Bi)ω1) = µ2
 S
i (Bi)ω1
!
= µ2
 Bω1
 = gB (ω1)
is A1|B1-measurable and using similar arguments hB (ω1) is A2|B1-measurable.
Moreover R
Ω1
gBdµ1
=
R
Ω1
µ2(Bω1)dµ1(ω1) =
R
Ω1
P
i µ2((Bi)ω1)dµ1(ω1)
=
P
i
R
Ω1
µ2((Bi)ω1)dµ1(ω1) = P
i
R
Ω2
µ1((Bi)ω2)dµ2(ω2)
=
R
Ω2
P
i µ1((Bi)ω2)dµ2(ω2) =
R
Ω2
hBdµ2.
The last part shows that L is closed under countable, disjoint unions and hence L
is a λ-system. Consequently, appealing to Dynkin’s π −λ theorem, we have that
L = A and the claims are established.
We now have enough motivation to introduce a well-deﬁned product measure
µ. The theorem that follows the deﬁnition demonstrates some key properties of µ.
Deﬁnition 3.18 Product measure space
Suppose that (Ω1, A1, µ1) and (Ω2, A2, µ2) are σ-ﬁnite measure spaces. The prod-
uct measure space is deﬁned as the triple (Ω, A, µ), where Ω= Ω1 × Ω2, A =
A1
N
A2 and µ is deﬁned by
µ(A) = (µ1 × µ2)(A) =
R
Ω1
µ2(Aω1)dµ1(ω1) =
R
Ω2
µ1(Aω2)dµ2(ω2),
where A ∈A.
Theorem 3.21 (Product measure properties) Consider the setup of the previ-
ous deﬁnition. Then we can show
(i) µ is a σ-ﬁnite measure on A.
(ii) µ(A) = µ1(A1)µ2(A2), for all A1 ∈A1 and A2 ∈A2.
(iii) The measure µ is the only measure on A such that µ(A1×A2) = µ1(A1)µ2(A2)
on the measurable rectangles.
(iv) If µ1 and µ2 are probability measures then so is µ.
Proof. (i) Clearly, µ is nonnegative and µ(∅) = 0 by deﬁnition. Let {En}+∞
n=1 be
a disjoint sequence of A-sets so that {(En)ω1}+∞
n=1 is a disjoint sequence of A2-sets.

DEFINING INTEGRALS WITH RESPECT TO MEASURES
125
Since µ2 is a measure, using remark 3.14.9 we can swap the sum under the integral
sign and write
µ
 ∞S
n=1 En
!
=
R
Ω1
µ2
  ∞S
n=1 En
!
ω1
!
dµ1(ω1) =
R
Ω1
µ2
 ∞S
n=1
(En)ω1
!
dµ1(ω1)
=
R
Ω1
+∞
P
n=1 µ2

(En)ω1

dµ1(ω1) =
+∞
P
n=1
R
Ω1
µ2

(En)ω1

dµ1(ω1)
=
+∞
P
n=1 µ (En) ,
so that µ is a measure on A. Next we treat σ-ﬁniteness. Assume that {An} is a
disjoint partition of Ω1, with An ∈A1 and µ1(An) < ∞and {Bn} a disjoint partition
of Ω2, with Bn ∈A2 and µ2(Bn) < ∞. As a consequence, Ω1 × Ω2 =
+∞
S
n=1(An × Bn),
so that
µ(An × Bn)
=
R
Ω1
µ2
 (An × Bn)ω1
 dµ1(ω1) =
R
Ω1
µ2(Bn)IAn (ω1) dµ1(ω1)
=
µ1(An)µ2(Bn) < ∞,
for all n, thus showing that µ is a σ-ﬁnite measure.
(ii) This was proven in equation (3.22).
(iii) Let v be another measure on A such that v(A1×A2) = µ1(A1)µ2(A2) on the mea-
surable rectangles R∗. Since R∗is a π-system generating A, any measures agreeing
on R∗must agree on A (uniqueness of measure, theorem 3.6) and hence v = µ on
A.
(iv) Since Ω= Ω1 × Ω2 is a measurable rectangle, we have µ(Ω) = µ1(Ω1)µ2(Ω2) =
1.
Let f be an A|B
∗-measurable function and consider the integral of f with re-
spect to the product measure µ = µ1 × µ2 denoted by
R
f dµ =
R
Ω1×Ω2
f d(µ1 × µ2) =
R
Ω1×Ω2
f (ω1, ω2)d(µ1 × µ2)(ω1, ω2).
(3.23)
Extending the product space deﬁnition to higher dimensions is straightforward,
but in the inﬁnite product case we need to exercise caution as we see below.
Remark 3.22 (Inﬁnite product measure) The inﬁnite product measure space is
deﬁned on the inﬁnite product measurable space
 
Ω=
+∞
×
n=1Ωn, A =
+∞
N
n=1
An
!
, where
the product measure is the unique measure on (Ω, A) such that
µ(A) =
+∞
×
n=1µn

(A) =
+∞
Q
n=1 µn(An),
for all measurable rectangles A =
+∞
×
n=1An, with An ∈An. The inﬁnite product mea-
sure µ deﬁned for all A-sets is obtained through the extension of measure theorem
by ﬁrst deﬁning µ on the measurable cylinders and uniqueness is justiﬁed since the
measurable cylinders generate A.

126
MEASURE AND INTEGRATION THEORY
There are several results that allow us to calculate (3.23) using iterated integrals,
depending on if: a) f is nonnegative or b) f is integrable with respect to µ. Both
results (and their versions assuming complete measure spaces) can be referred to as
Fubini theorems, although the theorem that treats case a) is also known as Tonelli’s
theorem.
Theorem 3.22 (Tonelli) Consider the product measure as in deﬁnition 3.18 and
assume that f : Ω→[0, ∞] is an A|B
+-measurable function, with B
+ =
B([0, ∞]).
(i) For all ω1 ∈Ω1 the mapping fω1(ω2) = f (ω1, ω2) is A2|B
+-measurable and
for all ω2 ∈Ω2 the mapping fω2(ω1) = f (ω1, ω2) is A1|B
+-measurable.
(ii) The mapping
R
Ω2
f (ω1, ω2)dµ2(ω2) is A1|B
+-measurable and nonnegative and
the function
R
Ω1
f (ω1, ω2)dµ1(ω1) is A2|B
+-measurable and nonnegative.
(iii) We can write the double integral of f with respect to µ as an iterated integral,
that is,
R
Ω1

R
Ω2
f dµ2
dµ1 =
R
Ω1×Ω2
f d(µ1 × µ2) =
R
Ω2

R
Ω1
f dµ1
dµ2.
(3.24)
Proof. Due to symmetry it suﬃces to show the ﬁrst parts of all statements (i)-
(iii).
First note that the theorem holds for f = IA with A ∈A since
R
Ω2
f dµ2 =
R
Ω2
IAdµ2 = µ2(Aω1),
and using lemma 3.4, parts (ii) and (iv) and deﬁnition 3.18, parts (i) and (ii) hold
and
R
f dµ =
R
IAdµ = µ(A) =
R
Ω1
µ2(Aω1)dµ1(ω1) =
R
Ω1

R
Ω2
IAdµ2
dµ1,
and (3.24) is established.
Now consider any nonnegative simple function f =
nP
i=1 ciICi, 0 ≤c1, ..., cn <
∞and {Ci} disjoint A-sets that form a partition of Ω. Clearly, the mapping
f (ω1, ω2) =
nP
i=1 ciICi(ω1, ω2), for ﬁxed ω1 ∈Ω1, is a linear combination of A2|B
+-
measurable functions gω1(ω2) = ICi(ω1, ω2) and hence (i) holds. Moreover, linearity
of the integral gives
R
Ω2
f dµ2
=
R
Ω2
nP
i=1 ciICi(ω1, ω2)dµ2(ω2) =
nP
i=1 ci
R
Ω2
ICi(ω1, ω2)dµ2(ω2)
=
nP
i=1 ciµ2((Ci)ω1),

DEFINING INTEGRALS WITH RESPECT TO MEASURES
127
with µ2((Ci)ω1) an A1|B
+-measurable and nonnegative function, for each ω1 ∈Ω1,
so that (ii) is established. To see equation (3.24) we write
R
f dµ
=
R
nP
i=1 ciICi(ω1, ω2)dµ =
nP
i=1 ci
R
ICi(ω1, ω2)dµ =
nP
i=1 ciµ(Ci)
=
nP
i=1 ci
R
Ω1
µ2((Ci)ω1)dµ1(ω1) =
R
Ω1
nP
i=1 ciµ2((Ci)ω1)dµ1(ω1)
=
R
Ω1
nP
i=1 ciµ2((Ci)ω1)dµ1(ω1) =
R
Ω1

R
Ω2
f dµ2
dµ1.
Next consider any f ≥0, an A|B
+-measurable function. From theorem
3.4 we can approximate f via nonnegative, nondecreasing simple functions {ϕn},
namely, 0 ≤ϕn(ω1, ω2) ↑f (ω1, ω2) and therefore the ω1-sections of the functions
ϕn and ϕ maintain this property, that is, 0 ≤ϕn(ω1, .) ↑f (ω1, .), for ﬁxed ω1 ∈Ω1.
Using the MCT we can pass the limit under the integral sign whenever needed.
To see (i), the mapping f (ω1, ω2) = lim
n→∞ϕn(ω1, ω2), for ﬁxed ω1 ∈Ω1, is A2|B
+-
measurable as the limit of A2|B
+-measurable functions gω1(ω2) = ϕn(ω1, ω2). In
addition
R
Ω2
f dµ2 = lim
n→∞
R
Ω2
ϕndµ2,
is A1|B
+-measurable as the limit of A1|B
+-measurable functions
R
Ω2
ϕn(ω1, ω2)
dµ2(ω2), for each ω1 ∈Ω1 and part (ii) is established. For the last part we apply the
MCT twice to obtain
R
f dµ
=
lim
n→∞
R
ϕndµ = lim
n→∞
R
Ω1

R
Ω2
ϕndµ2
dµ1
=
R
Ω1
lim
n→∞
R
Ω2
ϕndµ2
dµ1 =
R
Ω1

R
Ω2
f dµ2
dµ1.
Next we collect and proof the classic Fubini theorem.
Theorem 3.23 (Fubini) Consider the product measure as in deﬁnition 3.18 and
assume that f : Ω→R is an A|B-measurable function that is integrable with
respect to µ.
(i) For almost all ω1, the function fω1(ω2) = f (ω1, ω2) is an integrable function
with respect to µ2, and for almost all ω2, the function fω2(ω1) = f (ω1, ω2) is
µ1-integrable.
(ii) The function
R
Ω2
f (ω1, ω2)dµ2(ω2) deﬁned for almost all ω1 ∈Ω1 is µ1-
integrable and the function
R
Ω1
f (x, y)dµ1(x) deﬁned for almost all ω2 ∈Ω2 is

128
MEASURE AND INTEGRATION THEORY
µ2-integrable.
(iii) We can write the double integral of f with respect to µ as an iterated integral,
that is,
R
Ω1

R
Ω2
f dµ2
dµ1 =
R
f dµ =
R
Ω2

R
Ω1
f dµ1
dµ2.
Proof. The proof mirrors the Tonelli theorem proof, with the only major dif-
ference being how we can pass limit under the integral sign for a.s. measurable
functions and so we appeal to the approach of example 3.19.2, the a.s. version of
the MCT. To bring in Tonelli’s result, note that
R
f dµ =
R
f +dµ −
R
f −dµ < ∞,
due to integrability and recall that
R
f dµ =
R
gdµ if f = g a.e. [µ].
We consider generalizations of Fubini and Tonelli below.
Remark 3.23 (Fubini in higher dimensional spaces) Versions of Fubini
and
Tonelli are
easily obtained
for
higher dimensional product
spaces,
that is, we can easily iterate n-dimensional integrals in a measure space
 
Ω=
n×
i=1Ωi, A =
nN
i=1
Ai, µ =
n×
i=1µi
!
by
R
f dµ =
R
Ω1
R
Ω2
. . .
R
Ωn
f dµ1 . . . dµ2dµn,
and we can change the order of integration as we see ﬁt. When µ is the Lebesgue
measure µn on (Rn, Bn), then the integral
R
f dµn is the n-dimensional Lebesgue
integral. The extension to the inﬁnite dimensional case is immediate and depends
on how well deﬁned the inﬁnite product measure is.
The next two theorems are concerned with the Lebesgue integral and change of
variable (Billingsley, 2012, p. 239). These are special cases of change of variable
for a general measure and these theorems were illustrated in Section 3.7.1.
Theorem 3.24 (Change of variable: Lebesgue integral) Let ϕ : I1 →I2 be a
strictly increasing diﬀerentiable function, where I1, I2 intervals and let µ1 be the
Lebesgue measure in (R, B1). If f : I2 →R is a measurable function then
R
I2
f dµ1 =
R
I1
(f ◦ϕ)ϕ′dµ1.
We extend this result to the multivariate case below.
Theorem 3.25 (Multivariate change of variable: Lebesgue integral) Let ϕ :
O →Rp be an invertible, continuously diﬀerentiable function, deﬁned on an
open set O ⊂Rp. Assume that B ∈Bp such that B ⊂O and let A = ϕ−1(B). If

DEFINING INTEGRALS WITH RESPECT TO MEASURES
129
f : Rp →R is a measurable function then
R
B
f dµp =
R
A
(f ◦ϕ)|J|dµp.
(3.25)
where J =

∂x
∂y
 =

h∂xj
∂yi
i =

∂ϕ−1
j (yj)
∂yi
 is the Jacobian determinant of the trans-
formation y = ϕ(x), with ϕ = (ϕ1, . . . , ϕp) and µp is the p-dimensional Lebesgue
measure.
Example 3.23 (Fubini theorem and change of variable)
We show that lim
t→∞
tR
0
(sin x) x−1dx = π/2. First note that
tR
0
e−ux sin xdx = (1 + u2)−1 1 −e−ut(u sin t + cos t) ,
as follows from diﬀerentiation with respect to t and since
tR
0

+∞
R
0
e−ux sin x
 du
dx =
tR
0
| sin x|x−1dx ≤t < ∞,
Fubini’s theorem applies to the integration of e−ux sin x over (0, t) × R+ so that we
can write
tR
0
(sin x) x−1dx =
+∞
R
0
(1 + u2)−1du −
+∞
R
0
e−ut(1 + u2)−1(u sin t + cos t)du.
Now note that
+∞
R
0
(1 + u2)−1du = [tan u]+∞
0
= π/2,
and making the transformation v = ut, we have
+∞
R
0
e−ut(1 + u2)−1(u sin t + cos t)du =
+∞
R
0

1 + v2/t2−1 e−v 
vt−1 sin t + cos t

t−1dv,
which goes to 0 as t →∞and the claim is established.
Example 3.24 (Fubini theorem and polar coordinate space) We compute the
integral I =
+∞
R
−∞
e−x2dµ1(x), where µ1 is the Lebesgue measure on (R, B1), using a
double integral. Consider the polar coordinate space (ρ, θ) ∈U = {(ρ, θ) : ρ >
0, 0 < θ < 2π} and deﬁne the transformation T(ρ, θ) = (ρ cos θ, ρ sin θ) : U →R2.
The Jacobian of the transformation is given by J =
∇2T(ρ, θ)
 = ρ, with T(U) = R2
and using equation (3.25) we can write
R
R2
f (x, y)dµ2(x, y) =
R
U
f (ρ cos θ, ρ sin θ)ρdµ2(ρ, θ),
for any measurable function f . Choose f (x, y) = e−x2−y2, a measurable function, so
that
I2 =
R
U
ρe−ρ2dµ2(ρ, θ),

130
MEASURE AND INTEGRATION THEORY
and using the Fubini theorem we have
I2 = 2π
+∞
R
0
ρe−ρ2dµ1(ρ),
where the function g(ρ) = ρe−ρ2, ρ > 0, is bounded and continuous. From remark
3.19, the Lebesgue integral is the same as the Riemann integral, so that using the
fundamental theorem of calculus we may write
I2 = 2π
+∞
R
0
ρe−ρ2dρ = 2π
+∞
R
0
d

−e−ρ2/2

= 2π
h
−e−ρ2/2
i+∞
0
= π,
and consequently
R
R
e−x2dµ1(x) = √π.
3.7.5
Lp-spaces
The following deﬁnition extends the idea of integrability and allows us to build
well-behaved spaces of functions.
Deﬁnition 3.19 Lp-space
If (Ω, A, µ) is a complete measure space, we denote by Lp(Ω, A, µ) (or sim-
ply Lp when the measure space is implied by the context) the space of all measur-
able functions f on Ωfor which
R
| f |pdµ < +∞(i.e., | f |p is integrable) and deﬁne
an equivalence class on Lp by: f ∼g ⇐⇒f = g a.e. [µ], for f, g ∈Lp.
Several results on Lp-spaces are summarized next.
Remark 3.24 (Lp-spaces results) In order to metrize Lp-spaces, for diﬀerent p ≥
1, we deﬁne the Lp-norm by
∥f ∥p =
R
| f |pdµ
1/p ,
for all p ≥1 and if p = +∞, we deﬁne the essential supremum norm by
∥f ∥∞= ess sup | f | = inf
a>0{a : µ({ω ∈Ω: | f (ω)| > a}) = 0},
and L∞consists of f for which ∥f ∥∞< +∞. The following are straightforward to
show.
1. The space Lp is a linear (or vector) space, i.e., if a, b ∈R and f, g ∈Lp, then
a f + bg ∈Lp.
2. H¨older’s inequality The indices p and q, 1 < p, q < +∞, are called conjugate
if 1/p + 1/q = 1. Note that this holds trivially for p = 1 and q = +∞. Suppose that
p and q are conjugate indices and let f ∈Lp and g ∈Lq. Then f g is integrable and

R
f gdµ
 ≤
R
| f g|dµ ≤∥f ∥p ∥g∥q .
3. Minkowski’s inequality Let f, g ∈Lp (1 ≤p ≤+∞). Then f + g ∈Lp and
∥f + g∥p ≤∥f ∥p + ∥g∥p .

DEFINING INTEGRALS WITH RESPECT TO MEASURES
131
4. ∥f ∥p is a norm in Lp so that Lp is a normed vector space.
5. The functional d(f, g) = ∥f −g∥p , for f, g ∈Lp, 1 ≤p ≤∞, deﬁnes a metric
on Lp.
In order to introduce convergence in Lp-spaces we require the following deﬁ-
nition.
Deﬁnition 3.20 Convergence in normed vector spaces
A sequence of functions { fn} in a normed vector space X is said to converge to f
if ∀ε > 0, ∃N > 0 : ∀n > N =⇒∥fn −f ∥< ε, where ∥.∥is the norm of the space X.
We write fn →f or lim
n→∞fn = f.
We are now ready to collect convergence concepts in Lp-spaces.
Remark 3.25 (Convergence in Lp-spaces) We collect some basic deﬁnitions and
results involving convergence in Lp-spaces below.
1. Lp-convergence In Lp-spaces, fn
Lp
→f ⇐⇒∀ε > 0, ∃N > 0 : ∀n > N =⇒
∥fn −f ∥p < ε, i.e., ∥fn −f ∥p →0, as n →+∞. This is often referred to as con-
vergence in the mean of order p. In probabilistic terms, a random sequence of R-
valued random variables X = (X1, X2, . . . ) deﬁned on a probability space (Ω, A, P)
converges in Lp to a random variable X if
R
|Xn −X|pdP →0,
for 0 < p < +∞, denoted by Xn
Lp
→X.
2. It is important to note that the latter deﬁnition is not pointwise convergence, i.e.,
lim
n→∞fn(x) = f (x), for all x, since ∥f ∥p operates on functions f, not their arguments
x (x is integrated out).
3. Cauchy sequence of functions A sequence of functions in a normed vector
space of function X is Cauchy if and only if ∀ε > 0, ∃N > 0 : ∀n, m > N =⇒
∥fn −fm∥< ε.
4. Complete normed space A normed space is complete if every Cauchy se-
quence of elements from the space converges to an element of the space. For func-
tions, X is complete if and only if for every Cauchy sequence of functions { fn} ∈X,
there exists f ∈X : fn →f.
5. Banach space A complete normed vector space is called a Banach space.
6. Riesz-Fischer theorem The Lp-spaces, 1 ≤p ≤+∞, are complete, and conse-
quently, Banach spaces.
Additional properties of Lp-spaces can be studied using linear functionals. We
collect the general deﬁnition ﬁrst.

132
MEASURE AND INTEGRATION THEORY
Deﬁnition 3.21 Bounded linear functionals
A linear functional on a normed space X is a mapping F : X →R such that
F(a f + bg) = aF(f ) + bF(g), for all a, b ∈R and f, g ∈X. The functional F is
bounded if there exists M > 0 such that |F(f )| ≤M ∥f ∥, ∀f ∈X. We deﬁne the
norm of the functional by
∥F∥= sup
f,0
|F(f )|
∥f ∥.
Next we present an important result about linear functionals in Lp-spaces.
Remark 3.26 (Bounded linear functionals in Lp-spaces) Given a complete mea-
sure space (Ω, A, µ) and any g ∈Lq we deﬁne a functional on Lp by F(f ) =
R
f gdµ, for all f ∈Lp. Then we can show that F is a bounded, linear functional
with ∥F∥= ∥g∥q . The converse of this is known as the Riesz representation theo-
rem. Namely, let F be any bounded, linear functional on Lp, 1 ≤p < +∞. Then
there exists g ∈Lq such that F(f ) =
R
f gdµ and moreover ∥F∥= ∥g∥q , where p
and q are conjugate.
A special case of Lp-spaces is for p = 2, and we collect some results for this
case below.
Remark 3.27 (L2-spaces) In what follows, consider the space L2. First note that
p = 2 is its own conjugate.
1. If f, g ∈L2 then (f, g) =
R
f gdµ deﬁnes an inner product on L2. Then using
H¨older’s inequality we can write
(f, g) =
R
f gdµ ≤
R
f 2dµ
1/2 R
g2dµ
1/2 = ∥f ∥2 ∥g∥2 ,
i.e., the Cauchy-Schwarz inequality.
2. The L2-space equipped with the norm ∥f ∥2 = (f, g) is complete.
3. If (f, g) = 0 then f and g are called orthogonal, denoted by f ⊥g. If f1, . . . , fn
are pairwise orthogonal then

nP
i=1 fi

2
=
 nP
i=1 fi,
nP
j=1 f j
!
=
nP
i=1
nP
j=1(fi, f j) =
nP
i=1 ∥fi∥2 ,
which is simply the Pythagorean theorem for functions.
3.8
Summary
In this chapter, we have presented a rigorous treatment of measure and integra-
tion theory. There is a plethora of texts on measure theory that the reader can turn
to for additional results and exposition with Halmos (1950) being the standard ref-
erence for decades. Recent texts include Kallenberg (1986, 2002), Dudley (2002),
Durrett (2010), Cinlar (2010), Billingsley (2012) and Klenke (2014). We brieﬂy

EXERCISES
133
discuss some key ideas from this chapter and give some complementary concepts
and references.
Set systems, measurability and measures
More details on set systems, the construction and cardinality of σ-ﬁelds, in
particular Borel σ-ﬁelds, can be found in Chapter 1 of Vestrup (2003). Historical
accounts on the development of set theory with the pioneering work of Georg Can-
tor and ´Emile Borel at the end of the 19th century can be found in Dudley (2002, p
22). Details on the Axiom of Choice and its equivalent forms along with references
to their proofs can be found there as well.
We have collected an example of a set that is not Borel measurable with re-
spect to the Lebesgue measure on [0, 1), in example 3.15. To answer the question
of whether or not all subsets of R are measurable with respect to the Lebesgue mea-
sure µ1, Dudley (2002, Section 3.4) presents some results on the Lebesgue measur-
able and nonmeasurable sets along with proofs, in particular about the cardinality
of Lebesgue measurable sets. References on the development of the concepts of
measurability and general measures can be found in Dudley (2002, pp 111 and
112).
Integration theory
An excellent exposition of Fubini’s theorem and its versions can be found in
Chapter 10 of Vestrup (2003), while Dudley (2002, pp 148, 149 and 246) provides
the historical account of the development of Lebesgue’s integral, the MCT, the
DCT, the Fubini and Tonelli theorems and Lebesgue’s fundamental theorem of cal-
culus. Most of the development was initially considered over bounded intervals. It
seems natural to start by showing that an integral statement holds for simple func-
tions, move to nonnegative functions and then ﬁnally real-valued functions, using
linearity of integral and an appropriate limit theorem, like the MCT, to integrate to
the limit. Much of the presentation of integration theory in this chapter was inﬂu-
enced by Royden (1989), Fristedt and Gray (1997) and Billingsley (2012).
3.9
Exercises
Set theory, σ-ﬁelds and measurability
Exercise 3.1 Show that for all A, B, C ⊆Ωwe have
(i) A △B = Ω⇐⇒A = Bc,
(ii) A △B = ∅⇐⇒A = B and
(iii) (A △B) ∩C = (A ∩C) △(B ∩C).
Exercise 3.2 Prove Dynkin’s π −λ theorem.
Exercise 3.3 Show in detail that the statement of remark 3.1.3 holds.
Exercise 3.4 Let f : X →Y be some map and C be a collection of subsets of Y
and deﬁne the inverse map of the collection C by f −1(C) = {E ⊂X : E = f −1(C) for
some C ∈C}. Show that f −1(σ(C)) is a σ-ﬁeld on X with σ(f −1(C)) = f −1(σ(C)).

134
MEASURE AND INTEGRATION THEORY
Exercise 3.5
Let A1 and A2 be two σ-ﬁelds of subsets of Ω. Give an example
(other than example 3.1) illustrating that A1 ∪A2 is not a σ-ﬁeld. What can be said
about A1 ∪A2 if A1 and A2 are ﬁelds?
Exercise 3.6 Prove the statements of remark 3.3.8.
Exercise 3.7 Let Ω= {ω1, ω2, ω3, ω4, ω5}. Find the σ-ﬁelds σ({ω1, ω2}), σ(Ω) and
σ({ω5}).
Exercise 3.8 Let f : X →Y be a one-to-one function from X onto Y. Show that
for all B ⊆Y we have f [f −1(B)] = B.
Exercise 3.9 Prove statements (i)-(iii) of remark 3.5.9.
Exercise 3.10
Prove that the Borel σ-ﬁeld B1 on the real line R is not generated
by the following:
(i) any ﬁnite collection of subsets of R,
(ii) the collection of real singletons, and
(iii) the collection of all ﬁnite subsets of R.
Exercise 3.11 Prove the statement of remark 3.6.1.
Exercise 3.12 Show that the sets {x ∈R∞: sup
n
xn < a} and {x ∈R∞:
+∞
P
n=1 xn < a}
are B∞-measurable, for all a ∈R.
Exercise 3.13 Prove all statements of theorem 3.3.
Exercise 3.14
Suppose that X : Ω→R is A|B-measurable. Show that if A =
{∅, Ω} then X must be a constant.
Exercise 3.15 Prove statements 2, 3 and 5, of remark 3.7.
Exercise 3.16 Prove Tychonoﬀ’s theorem: a product of compact spaces is a com-
pact space.
Exercise 3.17
Show that the smallest σ-ﬁeld A on Ωsuch that a function X :
Ω→R is A|B-measurable is X−1(B).
Exercise 3.18 Show that statements 7-9 of remark 3.7 hold.
Exercise 3.19
Let { fn}+∞
n=1 be a sequence of Lebesgue measurable functions with
common domain E. Show that the functions
kP
n=1 fn(x) and
kQ
n=1 fn(x) are also measur-
able for any k.
Exercise 3.20
A real-valued function f deﬁned on R is upper semicontinuous at
x if for each ε > 0 there is a δ > 0 such that |x −y| < δ implies that f (y) < f (x) + ε.
Show that if f is everywhere upper semicontinuous then is it measurable.
Exercise 3.21 Assume that f : R →R is diﬀerentiable. Show that f and d f(x)
dx are
measurable.
Measure theory
Exercise 3.22 Let µ be a ﬁnitely additive, real-valued function on a ﬁeld A. Show

EXERCISES
135
that µ is countably additive if and only if µ is “continuous at ∅,” that is, µ(An) →0,
when An ↓∅and An ∈A.
Exercise 3.23 Prove statements (i)-(iii) or remark 3.8.9.
Exercise 3.24 Show that a measure µ on (Ω, A) is σ-ﬁnite if and only if µ can be
written as a countable sum of pairwise mutually singular ﬁnite measures on (Ω, A).
Exercise 3.25 Prove lemma 3.1.
Exercise 3.26
Suppose that (Rp, Bp, µp) is the Lebesgue measure space in Rp.
Show that (i) A ⊕x ∈Bp and (ii) µp(A ⊕x) = µp(A), for all Borel sets A and points
x ∈Rp (i.e., the Lebesgue measure is translation invariant).
Exercise 3.27 Show that every translation-invariant Radon measure v on (Rp, Bp)
must be a multiple of the Lebesgue measure, i.e., v(B) = cµp(B), for some constant
c.
Exercise 3.28 Prove lemma 3.2.
Exercise 3.29 (Measure regularity) Suppose that µ is a measure on Bk such that
µ(A) < ∞if A is bounded. Show the following.
(i) For A ∈Bk and ε > 0, there exists a closed set C and an open set O such that
C ⊂A ⊂O and µ(O ∖C) < ε. This property is known as measure regularity.
(ii) If µ(A) < ∞, then µ(A) =
sup
K⊆A, K compact
µ(K).
Exercise 3.30 Let µ∗denote the Lebesgue outer measure. Show that
(i) if µ∗(A) = 0 then A is µ∗-measurable,
(ii) the interval (x, +∞) is µ∗-measurable, for any x ∈R, and
(iii) every Borel set is µ∗-measurable.
Exercise 3.31
Show that if A ∈Bp−1 then A has measure 0 with respect to the
Lebesgue measure µp deﬁned on Bp.
Exercise 3.32 Let µ∗denote the Lebesgue outer measure. Show that
(i) if A is countable then µ∗(A) = 0, and
(ii) if µ∗(A) = 0 then µ∗(A ∪B) = µ∗(B), for any set B ∈M(µ∗).
Exercise 3.33 Prove statements 1-6 of remark 3.10.
Exercise 3.34 Let A ⊆R be a (Lebesgue) µ1-measurable set of ﬁnite measure and
fn a sequence of measurable functions deﬁned on A. Let f be a real-valued function
such that for each x ∈A we have fn(x) →f (x) as n →∞. Then given ε > 0 and
δ > 0, there exists a measurable set B ⊂A with µ1(B) < δ and an integer n0 > 0
such that for all x < B and all n > n0 we have | fn(x) −f (x)| < ε.
Exercise 3.35 Give a detailed proof of the Carath´eodory extension theorem 3.10,
based on the results of remark 3.10.
Exercise 3.36 Let (Ω, A, µ) be a measure space. Show that for any sequence {An}
of A-sets we have
µ(limAn) ≤limµ(An) ≤limµ(An).
Exercise 3.37 (Convergence in measure) A sequence of functions fn of measur-

136
MEASURE AND INTEGRATION THEORY
able functions converges to f in the (Lebesgue) measure µ if for a given ε > 0 there
exists n0 > 0 such that for all n > n0 we have
µ ({x : | fn(x) −f (x)| ≥ε}) < ε.
We write fn
µ1→f. Show that
(i) if fn →f a.e. then fn
µ1→f, and
(ii) if fn
µ1→f then there exists a subsequence fkn of fn with fkn
µ1→f .
Exercise 3.38 Let (Ω, A, µ) be a measure space.
(i) Show that µ(A △B) = 0 =⇒µ(A) = µ(B), provided that A, B ∈A.
(ii) If µ is complete then show that A ∈A and B ⊂Ωwith µ(A △B) = 0 together
imply that B ∈A.
Exercise 3.39
Give an example of a measure space (Ω, A, µ) such that for
A, A1, A2, · · · ∈A with An ↓A and µ(An) = +∞we have A = ∅. What are the
implications of this with regard to the continuity of measure theorem?
Exercise 3.40
Let Ω= {1, 2, 3, . . .} and A = 2Ω. Deﬁne µ(A) = P
k∈A 2−k, if A
is a ﬁnite A-set and let µ(A) = +∞otherwise. Is µ ﬁnitely additive? Countably
additive?
Exercise 3.41
Let A = {A ⊆Ω: A is ﬁnite or Ac is ﬁnite}. Deﬁne µ on A by
µ(A) = 0 if A is ﬁnite and µ(A) = 1 if Ac is ﬁnite. Show that
(i) µ fails to be well-deﬁned when Ωis ﬁnite,
(ii) if Ωis inﬁnite, then µ(A) ≥0, ∀A ∈A, µ(∅) = 0 and for all disjoint A1, . . . , An ∈
A, such that
nS
i=1 Ai ∈A, we have µ
 nS
i=1 Ai
!
=
nP
i=1 µ(Ai) and
(iii) when Ωis uncountable, µ is a measure. Is µ a σ−ﬁnite measure?
Exercise 3.42 Let Q denote the rationals and µ1 the Lebesgue measure on R. Show
that µ1(Q) = 0.
Exercise 3.43
Give an example of an open and unbounded set in R with a ﬁnite,
strictly positive Lebesgue measure.
Exercise 3.44
Assume that f : R+ →R+ is increasing, g is Lebesgue integrable
and let µ1 denote the Lebesgue measure on R. Show that µ1({x : |g(x)| ≥c}) ≤
1
f(c)
R
f (|g(x)|)dµ1(x), for any c ≥0.
Integration theory
Exercise 3.45
Assume that µ1(A) = 0. Show that for any Lebesgue integrable
function f we have
R
A
f dµ1 = 0.
Exercise 3.46 Prove part (iii) of theorem 3.11.
Exercise 3.47
Using the deﬁnition of the Lebesgue integral with respect to the

EXERCISES
137
Lebesgue measure µ1 in (R, M), ﬁnd the integrals: (i)
1R
0
xµ1(dx), (ii)
1R
−1
x2µ1(dx),
(iii)
+∞
R
1
x−3µ1(dx), (iv)
+∞
R
−∞
e−x2µ1(dx), and (v)
+∞
R
0
e−xµ1(dx).
Exercise 3.48
Prove the Lebesgue decomposition theorem by assuming that the
Radon-Nikodym theorem holds.
Exercise 3.49 (Euler’s gamma function) Show that the function f (x) = xa−1e−x,
x > 0, a > 0, is Lebesgue integrable in (0, +∞).
Exercise 3.50 Prove all parts of remark 3.16.
Exercise 3.51
Let (Ω, A, µ) be a measure space and g a nonnegative measurable
function on Ω. Deﬁne v(E) =
R
E
gdµ, for all E ⊆Ω.
(i) Show that v is a measure on (Ω, A).
(ii) If f ≥0 is a simple function on (Ω, A) then show that
R
f dv =
R
f gdµ.
Exercise 3.52
Prove all the statements of remark 3.14. Assume that the corre-
sponding results for simple functions hold, where applicable.
Exercise 3.53
Show that for a Lebesgue measurable function f, if one of the
integrals
R
F
f IEdµ1 and
R
E∩F
f dµ1 is ﬁnite then so is the other and both integrals are
equal.
Exercise 3.54 Prove all the statements of remark 3.17.
Exercise 3.55 Show that the functions sin

x2
and cos

x2
are Riemann integrable
in (0, +∞) (and ﬁnd their value) but they are not Lebesgue integrable.
Exercise 3.56 Prove all parts of theorem 3.17.
Exercise 3.57
Let µ1 denote the Lebesgue measure on [0, 1] and show that
1R
0
x−xµ1(dx) =
+∞
P
n=1 n−n.
Exercise 3.58 Prove theorem 3.24.
Exercise 3.59 Let (Ω, A, µ) be a measure space and let f be an R
+-valued measur-
able function deﬁned on Ω. Let B = {x ∈Ω: f (x) > 0} and deﬁne v(A) = µ(A ∩B),
∀A ∈A. Show that
(i)
R
f dµ =
R
f dv, and
(ii) if
R
f dµ < ∞, then (Ω, A, v) is a σ−ﬁnite measure space.
Exercise 3.60
Suppose that fn are integrable and sup
n
R
fndµ < ∞. Show that if
fn ↑f then f is integrable and
R
fndµ ↑
R
f dµ.
Exercise 3.61 Prove all statements of remark 3.21.
Exercise 3.62 Prove the Riemann-Lebesgue Theorem: If f is integrable on R, then
lim
n→∞
R
R
f (x) cos(nx)dx = 0.

138
MEASURE AND INTEGRATION THEORY
Exercise 3.63 Prove theorem 3.18.
Exercise 3.64 Show all statements of remark 3.19.
Exercise 3.65 Prove the MCT for the Riemann-Stieltjes integral.
Exercise 3.66 Prove theorem 3.19.
Lp spaces
Exercise 3.67
Consider a function f ∈Lp(Ω, A, µ1), 1 ≤p < ∞. Then there
exists an δ > 0 and a bounded Lebesgue measurable function f0 with | f0| ≤δ and
for any ε > 0 we have ∥f −f0∥p < ε.
Exercise 3.68 Let (Ω, A, µ) be a measure space with µ(Ω) < +∞and 0 < p < q ≤
∞.
(i) Show that Lq(Ω, A, µ1) ⊆Lp(Ω, A, µ1).
(ii) If f ∈Lq(Ω, A, µ1) then we have ∥f ∥p ≤∥f ∥q µ(Ω)
1
p−1
q.
Exercise 3.69 Prove all the statements of remark 3.24.
Exercise 3.70
Show that the L2-space equipped with the norm ∥f ∥2 = (f, g) is
complete.
Exercise 3.71
Let f, g ∈L∞= L∞(Ω, A, µ) (i.e., A|B(R) measurable functions
with ess sup f < ∞). Show that
(i) | f (ω)| ≤ess sup f a.e. µ., and
(ii) L∞is a complete metric space.
Exercise 3.72 (Riesz representation theorem) Let F be any bounded linear func-
tional on Lp, 1 ≤p < +∞. Then there exists g ∈Lq such that F(f ) =
R
f gdµ and
moreover ∥F∥= ∥g∥q , where p and q are conjugate.
Exercise 3.73 Prove the Riesz-Fischer theorem.
Exercise 3.74
Assume that f, fn ∈Lp(Ω, A, µ1), 1 ≤p < ∞, such that
+∞
P
n=1 ∥fn −f ∥p < ∞. Show that lim
n→∞fn(x) = f (x) a.e.
Exercise 3.75 Show that if f, g ∈L2 then (f, g) =
R
f gdµ deﬁnes an inner product
on L2. Furthermore, prove that the L2-space equipped with the norm ∥f ∥2 = (f, g)
is complete.

Chapter 4
Probability Theory
4.1
Introduction
In probability theory, events are sets with a non-negative length that is at most
one, since we assume that the sample space Ωis the certain event and should be
assigned the largest length, i.e., 1 (probability 100%). In order to develop measure
theoretic probability, we use the theoretical development of the previous chapter in
order to describe sets that correspond to events and then assign numbers to them in
the interval [0, 1] (probabilities).
4.2
Probability Measures and Probability Spaces
A historical account with references about the development of probability the-
ory can be found in Dudley (2002, pp. 273-278), including Andrei Nikolaevich
Kolmogorov’s pioneering work in 1933, which ﬁrst made the deﬁnition of a prob-
ability measure and space widely known. Since probability measures are special
cases of general measures, the results of the previous chapter are inherited. We will
collect however deﬁnitions and results tailored to probability theory when needed.
Deﬁnition 4.1 Probability measure and space
Consider a measurable space (Ω, A), where A is a ﬁeld. A probability measure
on (Ω, A) is a set function P that satisﬁes: (i) P(∅) = 0, and P(Ω) = 1, (ii) P(A) ∈
[0, 1], ∀A ∈A, and (iii) if A1, A2, . . . , is a disjoint sequence of A-sets and if
∞S
n=1 An ∈
A, then
P
 ∞S
n=1 An
!
=
∞P
n=1 P(An).
(4.1)
If A is a σ-ﬁeld, the triple (Ω, A, P) is called a probability measure space or simply
a probability space.
In probability theory, the paradoxical case P(Ω) < 1 occurs in various situations
(in particular, when P is the limit of probability measures Pn) and in this case
(Ω, A, P) is called a defective probability space.
139

140
PROBABILITY THEORY
Example 4.1 (Probability for coin ﬂips) Recall the setup of example 3.4.
1. The sample space for a single ﬂip of the coin is the set Ω0 = {ω0, ω1}, ωi ∈{0, 1},
i = 1, 2, so that 2Ω0 = {∅, Ω0, {0}, {1}}. We deﬁne a discrete probability measure P1
on the discrete measurable space (Ω0, 2Ω0) by assigning mass to the simple events
{0} and {1}. For a fair coin we must have P1({0}) = 1/2 = P1({1}) and consequently
all simple events are equally likely. Therefore (Ω0, 2Ω0, P1) is a discrete probability
space.
2. Extend the experiment to n successive ﬂips of a fair coin and let the sample
space be denoted by Ωn = {ω = (ω1, . . . , ωn) ∈Ωn
0 : ωi ∈{0, 1}, i = 1, 2, . . ., n} =
{ω1, . . . , ω2n}, consisting of 2n simple events. We deﬁne the set function Pn on the
measurable space (Ωn, 2Ωn) by
Pn(A) = |A|/2n,
for all A ∈2Ωn, where |A| = card(A). From example 3.10 µ(A) = |A| is the count-
ing measure and since Pn(Ωn) = |Ωn|/2n = 1, the triple (Ωn, 2Ωn, Pn) is a discrete
probability space. Suppose that k ∈{0, 1, . . ., n}. In order for
nP
i=1 ωi = k (i.e., the
experiment yields k Tails), it is necessary and suﬃcient that exactly k of the ωis be
1 and hence there are Cn
k simple events with this property. Clearly, the event A ={k
Tails in n-ﬂips}=
(
ω ∈Ωn :
nP
i=1 ωi = k
)
, is an event from 2Ωn and therefore
Pn (A) = Cn
k2−n,
for k = 0, 1, . . ., n. Hence Pn gives us a way of describing the probability of all sets
in 2Ωn.
3. Now turn to the inﬁnite coin ﬂip experiment with sample space Ω∞= {ω =
(ω1, ω2, . . . ) ∈Ω∞
0 : ωi ∈{0, 1}, i = 1, 2, . . .}. Since Ω∞has inﬁnitely many mem-
bers we cannot use Pn(A) to deﬁne probabilities in this case. However, we do know
what we would like a probability measure P on (Ω∞, 2Ω∞) to be equal to for certain
subsets of Ω∞. In particular, following remark 3.5.8, consider the collection E of
all k-dimensional measurable cylinders consisting of sets of the form
Ek = {ω = (ω1, ω2, . . . ) ∈Ω∞
0 : (ω1, ω2, . . . , ωk) ∈Ak},
where Ak is a 2Ωk-set and deﬁne a set function P on the measurable cylinders Ek by
P(Ek) =
P
(ω1,ω2,...,ωk)∈Ak
2−k.
Recall that E generates 2Ω∞(= B(Ω∞)), so that P has a unique extension to the Borel
sets of Ω∞by the extension of measure theorem deﬁned by
P(E) = inf
E⊂∪Ak
P
(ω1,ω2,...,ωk)∈Ak
2−k.
4.2.1
Extension of Probability Measure
Let us restate and treat the extension problem speciﬁcally for probability mea-
sures. Suppose that we have deﬁned a probability measure P0 on a ﬁeld A0 of

PROBABILITY MEASURES AND PROBABILITY SPACES
141
subsets of a space Ωand set A = σ(A0). We are interested in deﬁning a unique
probability measure P so that (Ω, A, P) is a probability space and P agrees with
P0 on A0. The problem can be solved using the Carath´eodory extension theorem
(Section 3.6.2) by deﬁning for any subset A of Ωthe outer measure
P∗(A) = inf
A⊂∪Ak
P
k P0(Ak),
where {Ak} are A0-sets so that the desired probability measure P is the restriction
of P∗to the Carath´eodory measurable sets M(P∗) = {A ⊂Ω: P∗(E) = P∗(E ∩
A) + P∗(E ∩Ac), for all E ⊂Ω}. This proposed P∗is approximating the probability
of A from the outside (or from above) since P∗is computed over all covers of A
consisting of A0-sets.
In order to motivate M(P∗) from a probabilistic point of view, recall the com-
plement law. The problem is that for {Ak} ∈A0 we do not always have ∪Ak ∈A0
and hence P∗(A) = 1−P∗(Ac) may be violated. Deﬁne the inner probability measure
by P∗(A) = 1 −P∗(Ac), ∀A ⊂Ω. Clearly, we could extend P0 for sets A ∈A0 such
that P∗(A) = P∗(A), in order to satisfy the complement law P∗(A) + P∗(Ac) = 1.
This condition is weaker than that required by M(P∗)-sets, since it is a special case
for E = Ω, i.e., 1 = P∗(Ω) = P∗(Ω∩A) + P∗(Ω∩Ac) = P∗(A) + P∗(Ac).
Now we consider an alternative extension method based on the Sierpi´nski class
of sets. The interested reader can ﬁnd more details on this construction in Fristedt
and Gray (1997, Section 7.3).
Deﬁnition 4.2 Sierpi´nski class
A Sierpi´nski class S is a collection of subsets of a space Ωthat is closed under
limits of nondecreasing sequences of sets and proper set diﬀerences, i.e., if A1 ⊆
A2 ⊆. . . , with An ∈S, then lim
n→∞An = S
n An ∈S and if A, B ∈S, then A ⊆B implies
B ∖A ∈S. We say that S is an S-class.
Next we consider when an S-class is a σ-ﬁeld.
Remark 4.1 (σ-ﬁeld and S-class) Clearly, any σ-ﬁeld is an S-class. The converse
is also true; let S be an S-class and suppose that (i) Ω∈S, and (ii) S is closed
under pairwise intersections. Then we can easily verify that S is a σ-ﬁeld.
The next two theorems illustrate the usefulness of the Sierpi´nski class in deﬁn-
ing the probability measure extension.
Theorem 4.1 (Sierpi´nski class theorem) Let E be a collection of subsets of a
space Ωand suppose that E is closed under pairwise intersections and Ω∈E.
Then the smallest Sierpi´nski class of subsets of Ωthat contains E equals σ(E).
Proof. An obvious candidate is S∗= T
E⊆S
S, where S is any S-class containing

142
PROBABILITY THEORY
the collection E and this intersection is nonempty since 2Ωis clearly an S-class. In
view of remark 4.1, it suﬃces to show that S∗is an S-class. Suppose that An ∈S,
with An ⊆An+1, n = 1, 2, . . ., so that An ∈S, ∀S, which implies that lim
n→∞An = A ∈
S, ∀S and therefore A ∈S∗. Now take A, B ∈S∗with A ⊆B so that A, B ∈S, ∀S,
which implies B ∖A ∈S, ∀S and thus B ∖A ∈S∗. Noting that E ⊆S∗and S∗is
the smallest σ-ﬁeld completes the proof.
The following is not hard to show and its proof is requested as an exercise.
Theorem 4.2 (Uniqueness of probability measure) Let P and Q be probability
measures on the measurable space (Ω, σ(E)), where E is a collection of sets
closed under pairwise intersections. If P(A) = Q(A), ∀A ∈E, then P coincides
with Q on σ(E).
Using these results, we can deﬁne an extension using the following steps. Let E
be a ﬁeld of subsets of a space Ωand R a nonnegative countably additive function
deﬁned on E such that R(Ω) = 1. Deﬁne E1 to be the collection of subsets from
Ωthat are limits of subsets of E, which can be shown to be a ﬁeld as well and set
R1(A) = lim
n→∞R(An), for all A = lim
n→∞An ∈E1, An ∈E. Clearly, R1(A) = R(A), if A ∈E
and therefore R1 is an extension of R to E1, which can be shown to have the same
properties as R. We can repeat this process and create E2 = {A : A = lim
n→∞An, An ∈
E1} and R2(A) = lim
n→∞R1(An) and similarly E3, R3, or E4, R4 and so forth.
Note that every time we extend, it is possible that we enlarge the ﬁeld with
more sets, i.e., En ⊆En+1 and σ(E) is generally strictly larger than all the ﬁelds En.
However, we do not need to go past the second level since it can be shown that E2
is such that if B ∈σ(E) then there exist A, C ∈E2 such that A ⊆B ⊆C and R2(A) =
R2(C). Thus if a set B ∈σ(E) has any elements outside of E then those elements
form a set of measure zero with respect to R2 so that nothing new appears after the
extension to E2. The extension is deﬁned as the probability measure P(B) = R2(A),
for all B ∈σ(E), such that A ⊆B ⊆C, for A, C ∈E2 with R2(C ∖A) = 0. The
following result summarizes the extension theorem and will be collected without
proof (Fristedt and Gray, 1997, p. 94).
Theorem 4.3 (Extension of probability measure) Let E be a ﬁeld of subsets
of a space Ωand R a nonnegative countably additive function deﬁned on E such
that R(Ω) = 1. Then there exists a unique probability measure P deﬁned on σ(E)
such that P(A) = R(A), for all A ∈E.

PROBABILITY MEASURES AND PROBABILITY SPACES
143
4.2.2
Deﬁning Random Objects
We are now ready to give the general deﬁnition of a random object. The two
important concepts required are measurability (of a mapping and a space) and prob-
ability spaces.
Deﬁnition 4.3 Random object
Consider two measurable spaces (Ω, A) and (X, G). A measurable object X
from (Ω, A) to (X, G) is a measurable map X : Ω→X, i.e., X−1(G) ∈A, ∀G ∈G.
If we attach a probability measure to (Ω, A) so that (Ω, A, P) is a probability space
then X is called a random object from (Ω, A, P) to (X, G).
The formal deﬁnition of the distribution of a random object is given next. Note
that knowing the distribution of the random object allows us to ignore the under-
lying structure of the probability space (Ω, A, P) and work directly in the target
space (X, G). As we will see in the next section, the distribution of the random ob-
ject can be used to provide the cdf and the density function for a random object,
which in turn makes deﬁning and calculating important quantities about X, such as
expectation, much easier.
Deﬁnition 4.4 Distribution
The probability measure P (or the random object X) induces a probability measure
Q on (X, G) by
Q(G) = (P ◦X−1)(G) = P(X−1(G)) = PX−1(G).
(4.2)
Consequently, (X, G, Q) is a probability space and X−1 is a random object from
(X, G) to (Ω, A). The probability measure Q is called the distribution of the random
object X.
Two natural questions arise at this point; which class of A-sets should we
choose that will allow us to eﬃciently allocate probabilities to the values of the
random object X, and moreover, which class of functions X : Ω→X is acceptable
as a random object? In order to answer both questions, one can turn to Baire theory.
In general, Baire sets form the smallest σ-ﬁeld with respect to which X is measur-
able by deﬁnition (see Appendix Section A.3 and deﬁnitions A.4 and A.5). Instead,
we have studied Borel sets and functions in the previous chapter and we deﬁne
most of our random objects to be Borel measurable functions or maps, since the
concepts of Baire and Borel sets, as well as functions, are equivalent for X = Rp.
More generally, it can be shown that the Baire and Borel σ-ﬁelds coincide in
any locally compact separable metric space (X, ρ), provided that we use the topol-
ogy induced by ρ to generate the Borel σ-ﬁeld (see for example Dudley, 2002,
theorem 7.1.1 or Royden, 1989, Chapter 13). The following deﬁnition provides the
smallest class of such well-behaved sets for a random object X.

144
PROBABILITY THEORY
Deﬁnition 4.5 Generated σ-ﬁeld of a random object
The σ-ﬁeld σ(X) generated by a random object X is the smallest σ-ﬁeld with
respect to which X is measurable.
The ﬁrst part of the following theorem is of extreme importance since it gives us
the explicit form of the sets in σ(X). Note that the steps in its proof (e.g., showing
that inverse images form a σ-ﬁeld) hold always (see for example Kallenberg, 2002,
p. 3).
Theorem 4.4 (Explicit form of the generated σ-ﬁeld) Let X be a random object
from a probability space (Ω, A, P) to a measurable space (X, G).
(i) The σ-ﬁeld σ(X) consists of the sets of the form
{ω ∈Ω: X(ω) ∈G} = X−1(G),
(4.3)
for G ∈G.
(ii) A random variable Y : Ω→R is measurable σ(X)|B1 if and only if Y = f (X),
for some G|B1 measurable map f : X →R.
Proof. (i) Let S denote the class of sets of the form (4.3). We prove ﬁrst
that S is a σ-ﬁeld. Since X is a random object from (Ω, A, P) to (X, G), we
must have X−1(G) ∈A, for all G ∈G. Clearly, Ω= X−1(X) ∈S and if
S = X−1(G) ∈S, for some G ∈G, then S c =
h
X−1(G)
ic = X−1(Gc) ∈S,
since Gc ∈G. Moreover, if S i = X−1(Gi) ∈S, for Gi ∈G, i = 1, 2, . . ., then
S
i S i = S
i X−1(Gi) = X−1(S
i Gi) ∈S, since S
i Gi ∈G. Consequently, S is a σ-ﬁeld.
But X is S-measurable by deﬁnition of X as a random object and hence σ(X) ⊂S.
Moreover, since X is σ(X)-measurable by deﬁnition 4.5, S ⊂σ(X) and part (i) is
established.
(ii) By remark 3.7.2, Y = f ◦X is σ(X)|B1 measurable.
For the other direction, assume ﬁrst that Y : Ω→R is a measurable
σ(X)|B1 simple random variable, i.e., Y =
mP
i=1 yiIAi, where Ai = {ω : Y(ω) = yi},
i = 1, 2, . . ., m, are disjoint. Clearly, Ai is a σ(X) measurable set and hence by part
(i), Ai must be of the form Ai = X−1(Gi) = {ω : X(ω) ∈Gi}, for some Gi ∈G,
i = 1, 2, . . ., m, that need not be disjoint. Deﬁne the G|B1 measurable simple func-
tion f =
mP
i=1 yiIGi. Since the Ai are disjoint, no X(ω) can lie in more than one Gi and
hence f (X(ω)) = Y(ω).
Now consider any random variable Y : Ω→R that is measurable σ(X)|B1
and use theorem 3.4 to approximate Y(= Y+ −Y−), using a sequence of sim-
ple random variables Yn, that is, Yn(ω) →Y(ω) for all ω. Now for each sim-
ple function Yn there exists, as we saw above, a σ(X)|B1 measurable function
fn such that Yn(ω) = fn(X(ω)), for all ω. By remark 3.7.8 the set L of x in X

PROBABILITY MEASURES AND PROBABILITY SPACES
145
for which { fn(x)} converges is an G-set. Deﬁne f (x) = lim
n→∞fn(x) for x ∈L and
f (x) = 0, for x ∈Lc = X ∖L. Since f (x) = lim
n→∞fn(x)IL(x) and fn(x)IL(x) is
G|B1 measurable we have that the limit function f is also G|B1 measurable. Now
for each ω ∈Ωwe have Y(ω) = lim
n→∞fn(X(ω)) so that X(ω) ∈L and moreover
Y(ω) = lim
n→∞fn(X(ω)) = f (X(ω)).
The following remark gives the explicit forms result for random vectors.
Remark 4.2 (Borel measurable functions) Let X = (X1, . . . , Xp) be a random
vector, i.e., X is a map from a probability space (Ω, A, P) to the Borel measurable
space (Rp, Bp). The following are straightforward to show.
1. The σ-ﬁeld σ(X) = σ(X1, X2, . . . , Xp) consists exactly of the sets of the form
X−1(B), for all B ∈Bp.
2. A random variable Y is σ(X)|B1-measurable if and only if there exists a mea-
surable map f : Rp →R such that Y(ω) = f (X1(ω), . . . , Xp(ω)), for all ω ∈Ω.
Example 4.2 (Random object examples)
We consider some ﬁrst examples of
random objects. The source space (Ω, A, P) over which X takes its argument can
be the same in all cases. What makes the random objects diﬀerent is the (target)
measurable space (X, G). See Appendix Section A.7 for details on the code used to
generate these objects and produce their plots.
1. Uniform random variable Assume that Ω= [0, 1], A = B([0, 1]) and P = µ1,
the Lebesgue measure on [0, 1], so that (Ω, A, P) is a probability space and deﬁne
a function X : [0, 1] →[0, 1], by X(ω) = ω, so that (X, G) = ([0, 1], B([0, 1])).
Clearly, X is B([0, 1])|B([0, 1]) measurable and therefore X is a random variable
that assigns length as the probability of an event, i.e., for any Borel set A we have
P(X ∈A) = P({ω : X(ω) = ω ∈A}) = P(A) = µ1(A).
This random object is the uniform random variable on [0, 1].
2. Function-valued random variables Let CR
[0,1] denote the space of continuous
real-valued functions deﬁned on the interval [0, 1]. We metrize this space using the
metric d(f, g) = max
x∈R | f (x) −g(x)|, for all f, g ∈CR
[0,1] and consequently, we can talk
about open sets with elements that are functions of CR
[0,1], which leads to a topology
on CR
[0,1] and therefore allows the deﬁnition of the Borel subsets of CR
[0,1], denoted
by BC = B

CR
[0,1]

.
Now a measurable functional from (Ω, A, P) to

CR
[0,1], BC

can be thought of
as a “random function,” which is nothing but a measurable, CR
[0,1]-valued random
variable. For a speciﬁc example, suppose that (Ω, A, P) is the usual inﬁnite coin
probability space (example 4.1.3) and deﬁne for each positive integer k the random
variable X(k) = (X(k)
t
: t ∈[0, 1]) from (Ω, A, P) to

CR
[0,1], BC

, by specifying the

146
PROBABILITY THEORY
values X(k)
t (ω), ω = (ω1, ω2, . . . ) ∈Ω, of the continuous function t 7−→X(k)
t (ω), at
each t ∈[0, 1]. We ﬁrst specify these values for t equal to a multiple of 1/k and
then the value of X(k)
t (ω), ∀t ∈[0, 1] is determined by linear interpolation. More
precisely, for j = 0, 1, . . ., k, let
X(k)
j/k(ω) = k−1/2
jP
i=1(2ωi −1),
(4.4)
and extend via linear interpolation to obtain X(k)
t , ∀t ∈[0, 1]. Note that for any
subset B of CR
[0,1], the set

X(k)−1 (B) = {ω : X(k)(ω) ∈B} consists of a ﬁnite union
of cylinder sets {ω : (ω1, . . . , ωk) = (ε1, . . . , εk), εi ∈{0, 1}}, which are certainly
A-measurable, so that X(k) is a random function.
Recall that ωi ∈{0, 1}, with {0} = {Heads} and {1} = {Tails}, so that the
sum in (4.4) equals the diﬀerence between the number of tails and the number of
heads after j tosses. These types of random functions will be used as the building
block of a stochastic process known as Brownian motion and we will revisit them
in Chapter 7.
Now let Q(k) denote the induced probability measure on (CR
[0,1], BC) by X(k), so
that (CR
[0,1], BC, Q(k)) is a probability space, where the sample space and σ-ﬁeld are
the same for all k but the probability measures are diﬀerent. Consider calculating
the probability that we observe random functions X(k) with the property that X(k)
1
=
0, i.e., in k tosses, we have half heads and half tails. Using example 4.1.2, we have
Q(k)(X(k)
1 = 0) = Q(k)({g ∈CR
[0,1] : g(1) = 0}) =

Ck
k/22−k
if k is even
0
otherwise .
Figure 4.1 (a) presents the random functions of equation (4.4) for k = 2, 5 and 7
and a realization ω = (1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, . . .).
3. Random counting measure Consider a probability space (Ω, A, P) and let N f
denote the collection of all locally ﬁnite counting measures on X ⊂Rp, where if
ϕ ∈N f and B ∈B(X), then ϕ(B) denotes the number of points in the set B. In order
to deﬁne a random counting measure we need to create a measurable space based
on a σ-ﬁeld of subsets of N f. Therefore, consider N f, the generated σ-ﬁeld from
the collection of sets of the form {ϕ ∈N f : ϕ(B) = n}, for all B ∈B(X) and all
n = 0, 1, 2, . . ., so that (N f, N f) is a measurable space. Then a mapping N from
(Ω, A, P) to (N f, N f) can be thought of as a random object N(ω)(.) = ϕ(.), that
yields counting measures ϕ for diﬀerent ω ∈Ωand hence it is a random counting
measure. The induced probability measure ΠN(Y) = P(N ∈Y) = P(N−1(Y)), can
be used to compute probabilities over sets Y ∈N, where each element of Y is a
counting measure.
In order to appreciate the usefulness of this construction, consider a count-
able set of spatial locations S
= {x1, x2, . . . }, xi ∈X. Knowing ϕ(B) for all
B ∈B(X) is equivalent to knowing the spatial locations of all points in S . In-
deed, ϕ(B) =
+∞
P
i=1 I(xi ∈B), ∀B ∈B(X), so that knowing the locations implies that

PROBABILITY MEASURES AND PROBABILITY SPACES
147
we know the counting measure ϕ. Conversely, x is a point from S if ϕ({x}) > 0
and the point is distinct if ϕ({x}) = 1. Now if ϕ is a random counting measure,
i.e., ϕ = N(ω), for some ω ∈Ω, then this important relationship between ϕ and
S deﬁnes a random collection of points (a countable random set) N, known as
a point process. Alternatively, an equivalent deﬁnition for the random point pro-
cess N can be given through the exponential space of example 3.5, by letting
Xn = {ϕ : ϕ(X) = n}. Figure 4.1 (b)-(d) presents three realizations of random
counting measures illustrating the types of point processes N one can encounter,
namely, regular (b), random (c) and clustered (d). We study such models through-
out the TMSO-PPRS text.
4. Random disc Consider a random variable R : (Ω, A, P) →(R+, B(R+)) and let
C denote the collection of all open discs centered at the origin, i.e., C = {b(0, r) ⊂
R2 : r > 0}. Clearly, there is a one-to-one relationship between R+ and C, that is, for
each positive real number r there is a single open disc b(0, r) and conversely, each
open disc corresponds to a single positive real number. Now if we want to deﬁne
a random disc (recall example 3.6 and remark 3.7.10), we need to equip C with a
σ-ﬁeld G and then deﬁne a measurable, disc-valued map X : (Ω, A, P) →(C, G)
by
X(ω) = b(0, R(ω)),
(4.5)
i.e., for a given ω ∈Ω, X(ω) is the open disc centered at the origin of radius
R(ω) and X−1(G) ∈A, ∀G ∈G. Obviously, G = 2C is a valid choice but the
resulting σ-ﬁeld is too large and we will have a hard time deﬁning the distribution
Q(G) = P(X−1(G)) of X, ∀G ∈G. As we see in the TMSO-PPRS text, in order
to fully describe the distribution Q of X it suﬃces to use the hit-or-miss topology
G = B(C) = σ({FK, ∀K ∈K}) and then compute the hitting function
TX(K) = P(X ∩K , ∅), ∀K ∈K,
which is not a probability measure in general, but under certain conditions it can
uniquely identify the distribution of X. Figure 4.1 (e) presents ﬁve realizations of a
random disc centered at the origin with radii drawn from a Gamma(10, 1).
5. Gaussian random ﬁeld Assume that G = {G(x) ∈R : x ∈Rp} is a Gaus-
sian random ﬁeld (GRF), that is, G is such that the ﬁnite-dimensional distributions
P(G(x1) ∈B1, . . . , G(xn) ∈Bn), for any Borel sets B1, . . . , Bn ∈B(R) and n ≥1,
are multivariate normal. Therefore, a GRF is completely characterized by its mean
function
µ(x) = E(G(x)),
x ∈Rp and its covariance function
C(x1, x2) = E (G(x1) −µ(x1))(G(x2) −µ(x2)) ,
x1, x2 ∈Rp. In particular, G(x) ∼N(µ(x), C(x, x)). A GRF is stationary and
isotropic if µ(x) = µ, i.e., µ(x) does not depend on the location, and C(x1, x2) is
a function only of the distance r = ∥x1 −x2∥of the points x1 and x2. In this case we

148
PROBABILITY THEORY
use the notation C(r) = C(∥x1 −x2∥) = C(x1 −x2), so that σ2 = C(0) is the variance
of the GRF G. Note that for any x ∈Rp we can write C(r) = C(∥x −0∥) = C(x),
where 0 is the origin and r = ∥x∥. The most commonly used class of covariance
functions is the Mat´ern class (Mat´ern, 1986) given by
Cv,θ,σ2(r) = σ2 21−v
Γ(v)
 √
2vr
θ
v
Bv
 √
2vr
θ

,
(4.6)
for v, θ > 0, where Bv(.) denotes the modiﬁed Bessel function of the second kind, θ
is a scale parameter and v a smoothness parameter such that the sample functions
are m times diﬀerentiable if and only if m < v. For example, for v = 0.5, we have
C0.5,θ,σ2(r) = σ2e−r/θ. In Figure 4.1 (f), we display a realization of the GRF with
µ = 0 and covariance function C0.5,10,1(r) = e−r/10. Random ﬁelds play a prominent
role in spatial and spatiotemporal statistics (e.g., Cressie and Wikle, 2011) and will
be utilized in the TMSO-PPRS text.
4.2.3
Distribution Functions and Densities
The distribution theory of Chapter 1 is based on rudimentary quantities such
as densities and cdfs, which are related to probability measures on the measurable
space (Rp, Bp). We collect the basic deﬁnitions and properties below.
Deﬁnition 4.6 Distribution function
A real-valued function F deﬁned on R is called a (cumulative) distribution func-
tion for R, or simply a distribution function, if it is nondecreasing and right-
continuous and satisﬁes F(−∞) = lim
x→−∞F(x) = 0 and F(+∞) = lim
x→+∞F(x) = 1.
The following shows us how to obtain the cdf given a probability measure.
Theorem 4.5 (Cdf via a probability measure) Let P be a probability measure
on (R, B1). Then the function F(x) = P((−∞, x]) is a distribution function.
Proof. Since P is monotone then x ≤y =⇒(−∞, x] ⊆(−∞, y] =⇒
P((−∞, x]) ≤P((−∞, y]) and hence F is nondecreasing. To show right-continuity,
suppose that xn ↓x, with xn, x ∈R. Since (−∞, x1] ⊇(−∞, x2] ⊇. . . , by continuity
of measure from above we have
lim
n→+∞P((−∞, xn]) = P
 +∞
T
n=1(−∞, xn]
!
= P ((−∞, x]) ,
and hence F is right-continuous. Using similar arguments for xn ↓−∞, we have
F(−∞) = 0. Finally, letting xn ↑+∞and using continuity from below we have
P ((−∞, xn]) →P
 +∞
S
n=1(−∞, xn]
!
= P(R) = 1.

PROBABILITY MEASURES AND PROBABILITY SPACES
149
Figure 4.1: Displaying random objects: (a) random functions, (b)-(d) random
counting measures showing regularity, randomness and clustering of the points,
(e) random discs, and (f) Gaussian random ﬁeld.

150
PROBABILITY THEORY
The converse of this is also true and its proof is straightforward based on
uniqueness of measure (theorem 3.6).
Theorem 4.6 (Probability measure via cdf) Let F be a distribution func-
tion. Then there exists a unique probability measure P on (R, B1) such that
P((−∞, x]) = F(x). Suppose that X is the random variable with distribution
function FX(x) = P(X ≤x) and write X ∼FX. The random variable X can
be constructed as follows: let Ω= (0, 1) and deﬁne
X(ω) = inf{x : FX(x) ≥ω}, 0 ≤ω ≤1.
We collect below some basic properties and results on cdfs. Additional proper-
ties were collected in the rudimentary Section 1.2.1.
Remark 4.3 (Properties of distribution functions) It is straightforward to see the
following.
1. If P is discrete and noting that F(x−) = P(X < x) = P(X ≤x −1) then we can
write
P(X = x) = F(x) −F(x−) = P(X ≤x) −P(X ≤x −1) = ∆P(X ≤x)/∆x,
where ∆f (x) = f (x) −f (x −1), the ﬁrst-order diﬀerence operator on a function f .
Furthermore, F has at most countably many discontinuities.
2. The generalization to the probability space (Rp, Bp, P) is easily accomplished
by deﬁning
F(x1, x2, . . . , xp) = P({ω : Xi(ω) ≤xi, ∀i = 1, 2, . . ., p}).
3. The distribution function F of a random object X can be deﬁned in the general
case as the containment functional FX(A) = P(X ⊂A), for all A ∈F (recall remark
1.2).
4. The support of F is deﬁned as the set S = {x ∈R : F(x+ε)−F(x−ε) > 0, ∀ε >
0} and the elements of S are called the points of increase of F. Furthermore, S is a
closed set.
5. The distribution function F is said to be absolutely continuous if it is determined
by a Borel measurable function f, f ≥0 and such that F(x) =
xR
−∞
f (t)dt. The
distribution function F is said to be singular if it is continuous and its corresponding
measure is singular with respect to the Lebesgue measure.
6. We can write F = a1F1 + a2F2 + a3F3, where a1 + a2 + a3 = 1, ai ≥0 and F1,
F2 and F3, are discrete, absolutely continuous and singular, respectively.
7. The distribution function F is called defective if the corresponding probability
measure P is defective.

PROBABILITY MEASURES AND PROBABILITY SPACES
151
Although the distribution function provides much of our rudimentary distribu-
tion theory results, we can further simplify calculations by deﬁning any important
quantity of interest in terms of the density of the random object X.
Deﬁnition 4.7 Density
Consider a random object X : (Ω, A, P) →(Rp, Bp) with induced probability
measure Q(B) = P(X−1(B)), B ∈Bp and let µ be a σ-ﬁnite measure deﬁned on
(Rp, Bp). A density for X (or for Q) with respect to µ is a nonnegative, Bp|B(R+
0)-
measurable function f such that for each set B ∈Bp we have
Q(B) = P(X−1(B)) = P(X ∈B) =
R
B
f (x)dµ(x).
(4.7)
The following remark summarizes some important results on densities.
Remark 4.4 (Densities) Some consequences of this important deﬁnition follow.
1. If B = Rp and µ = µp then
1 = Q(Rp) = P(X ∈Rp) =
R
Rp
f (x)dµp(x),
and consequently f (x) represents how the probability or mass is distributed over
the domain Rp with the total volume under the surface y = f (x) being 1.
2. Any Lebesgue integrable, Bp|B(R+
0)-measurable function f that satisﬁes
R
Rp
f (x)dµk(x) = 1, can be thought of as the density of a random vector X deﬁned on
some probability space (Ω, A, P) and such that Q(B) = P(X ∈B) =
R
B
f (x)dµp(x).
We abbreviate this by writing X ∼f (x) (or X ∼Q). The density does not contain
any information about the measurable space (Ω, A) (see part 6 below).
3. If f = g a.e. [µ] and f is a density for P then so is g.
4. Note that if Q has the representation (4.7) then if for some B ∈Bp we have
µ(B) = 0 then Q(B) = 0 and hence Q ≪µ. The converse is obtained using the
Radon-Nikodym theorem, that is, in order for a random object X to have a density
it suﬃces to have Q ≪µ, for some σ-ﬁnite measure µ. Consequently, the density
is simply the Radon-Nikodym derivative f =
hdQ
dµ
i
. In theory, any measure µ could
be chosen but in order to simplify calculations of the integrals we typically choose
the Lebesgue measure for continuous random variables and the counting measure
for discrete random variables. Moreover, in view of remark 3.21.1 we can write
R
B
gdQ =
R
B
g f dµ,
for any measurable function g.
5. Recall Liebnitz’s rule for diﬀerentiation of integrals (the Riemann integral ver-

152
PROBABILITY THEORY
sion):
∂
∂θ

b(θ)
R
a(θ)
f (x, θ)dx
= f (b(θ), θ)db(θ)
dθ
−f (a(θ), θ)da(θ)
dθ
+
b(θ)
R
a(θ)
" ∂
∂θ f (x, θ)
#
dx.
Then assuming that X is a continuous random variable, applying the rule to FX(x) =
xR
−∞
fX(t)dt, we obtain fX(x) = dF(x)
dx .
6. A density f contains all the information about the probabilistic behavior of the
random variable X but it contains no detailed information about the underlying
probability space (Ω, A, P) over which X is deﬁned, or about the interaction of X
with other random variables deﬁned on the space. To see this, recall example 4.1.2
and the discrete probability space (Ωn, 2Ωn, Pn) and consider the inﬁnite coin ﬂip
space Ω∞. We can deﬁne the random variable X(ω) as the number of tails in n-ﬂips
by choosing any n distinct ﬂips from the inﬁnite coin ﬂip space (yielding the same
sample space Ωn for any choice) and all such variables have the same density f (the
Radon-Nikodym derivative of the induced probability measure Qn = Pn ◦X−1, with
respect to the counting measure), given by
f (k) = P(X(ω) = k) = Cn
k2−n,
where k = 0, 1, 2, . . ., n. The random variable can be deﬁned by X(ω) =
nP
i=1 ωi or
X(ω) =
n+r−1
P
i=r ωi, for any r = 2, . . . and its density f (k) loses sight of this underlying
structure.
The following theorem provides the rigorous deﬁnition of the marginal densi-
ties.
Theorem 4.7 (Marginal densities) Let X = (X1, X2) be a random vector deﬁned
on (Ω, A, P) that takes values in the product measure space (X, G, µ), where
X = X1 × X2, G = G1
N
G2, µ = µ1 × µ2 and µ1, µ2 are σ-ﬁnite measures.
Suppose that the distribution of X has (joint) density f with respect to µ. Then
the distributions of X1 and X2 have (marginal) densities
f1(x1) =
R
X2
f (x1, x2)µ2(dx2),
and
f2(x2) =
R
X1
f (x1, x2)µ1(dx1),
with respect to µ1 and µ2, respectively.
Proof. Let A be any G1-set. Then we can write
P({ω : X1(ω) ∈A}) =
R
A×X2
f (x1, x2)dµ(x1, x2) =
R
A

R
X2
f (x1, x2)dµ2(x2)
dµ1(x1),

PROBABILITY MEASURES AND PROBABILITY SPACES
153
by the Fubini theorem and therefore
R
X2
f (x1, x2)dµ2(x2) = f1(x1). Similarly for the
density of X2.
4.2.4
Independence
We begin exploring the concept of independence with the general deﬁnitions of
independence between σ-ﬁelds and random objects below.
Deﬁnition 4.8 Independence
Consider a probability space (Ω, A, P) and let Ak, k ∈K, be sub-σ-ﬁelds of A.
1. If K is ﬁnite then we say that {Ak : k ∈K} is (stochastically) independent if and
only if
P
 T
k∈K Ak
!
= Q
k∈K P (Ak) ,
(4.8)
for all Ak ∈Ak.
2. If K is an inﬁnite set, then {Ak : k ∈K} is independent if and only if for every
ﬁnite subset J ⊂K, {A j : j ∈J} are independent.
3. Let X1, X2, . . . , be random objects from (Ω, A, P) to (X, G).
(a) A ﬁnite collection X1, . . . , Xn is said to be independent if and only
if {σ(Xi), i = 1, . . . , n} are independent, where σ(Xi) = {X−1
i (G) : G ∈G},
i = 1, 2, . . ., n.
(b) An inﬁnite collection X1, X2, . . . , is said to be independent if and only if
{σ(Xi), i = 1, 2, . . .} are independent.
4. Let A1, A2, . . . , be A-sets.
(a) We say that A1, . . . , An are independent events if and only if {σ(Ai) : i =
1, 2, . . ., n} are independent, where σ(Ai) = {∅, Ω, Ai, Ac
i }, i = 1, 2, . . ., n.
(b) The events of the inﬁnite sequence A1, A2, . . . , are independent if and only
if {σ(Ai) : i = 1, 2, . . .} are independent.
A ﬁrst existence theorem for a random sequence is given next under the as-
sumption of independence.
Theorem 4.8 (Existence of a sequence under independence) Let (Ωj, A j, P j)
be probability spaces, j = 1, 2, . . .. Then there exists a sequence of independent
random objects (X1, X2, . . . ) such that X j has distribution P j, for all j.
Proof. Let Ω∞=
∞×
i=1Ωj, A∞=
∞
N
j=1
A j and P =
∞×
i=1P j. For j ≥1, let X j(ω) = ω j,
where ω = (ω1, ω2, . . . ), that is, deﬁne the projection map X j : (Ω∞, A∞, P) →
(Ωj, A j). If B j ∈A j then X−1
j (B j) = Ω1×· · ·×Ωj−1×B j×Ωj+1×· · · ∈A∞=⇒X j is an

154
PROBABILITY THEORY
A∞|A j measurable function, i.e., X j is a random object. Moreover, the distribution
of X j is given by
Q j(X j)
=
P(X j ∈B j) = P(X−1
j (B j))
=
P(Ω1 × · · · × Ωj−1 × B j × Ωj+1 × . . . ) = P j(B j),
and hence Q j = P j.
To show independence of X1, . . . , Xn, . . . , we have to show that σ(X1), . . . ,
σ(Xn), . . . , are independent. Let A ji ∈X−1
ji (B ji), where B ji ∈A ji, for some indices
ji, i = 1, 2, . . ., n. Then we can write
P
 nT
i=1 A ji
!
=
P
 nT
i=1{ω ∈Ω: X ji ∈B ji}
!
=
P
 nT
i=1

Ω1 × · · · × Ωj−1 × B ji × Ωj+1 × . . .
!
=
nQ
i=1 P

Ω1 × · · · × Ωj−1 × B ji × Ωj+1 × . . .

=
nQ
i=1 P ji

A ji

,
and thus X1, X2, . . . , are independent.
Next we give a characterization of independence via the marginal distributions.
Theorem 4.9 (Characterization of independence) Let X1, . . . , Xn be X-valued,
A|G-measurable random objects deﬁned on a probability space (Ω, A, P). Then
X1, . . . , Xn are independent if and only if for all B1, . . . , Bn ∈G we have
P(X1 ∈B1, . . . , Xn ∈Bn) =
nQ
i=1 P(Xi ∈Bi).
(4.9)
Proof. By deﬁnition X1, . . . , Xn are independent if and only if σ(X1), . . . , σ(Xn)
are independent, which holds if and only if for any Ai ∈σ(Xi), i = 1, 2, . . .n, we
have
P
 nT
i=1 Ai
!
=
nQ
i=1 P(Ai).
(4.10)
Now Ai ∈σ(Xi) ⇐⇒Ai = X−1
i (Bi), for some Bi ∈G, which implies that P(Ai) =
P(Xi ∈Bi). Therefore equation (4.10) is equivalent to the desired equation (4.9) and
the result is established.
The following are straightforward to prove based on the latter two theorems and
deﬁnition.
Remark 4.5 (Independence) Let {Xn}+∞
n=1 be a sequence of independent random
objects.
1. Let {i1, i2, . . . } and { j1, j2, . . . } be disjoint sets of integers. Then the σ-ﬁelds
σ({Xi1, Xi2, . . . }) and σ({X j1, X j2, . . . }) are independent.

PROBABILITY MEASURES AND PROBABILITY SPACES
155
2. Let J1, J2, . . . , be disjoint sets of integers. Then the σ-ﬁelds Gk = σ({X j : j ∈
Jk}), k = 1, 2, . . ., are independent.
3. The σ-ﬁelds σ({X1, X2, . . . , Xn}) and σ({Xn+1, Xn+2, . . . }) are independent.
4. If ϕ1, ϕ2, . . . , are Ψ-valued,
m
N
j=1
G|Y−measurable functions deﬁned on Xm, then
Z1 = ϕ1(X1, X2, . . . , Xm), Z2 = ϕ2(Xm+1, Xm+2, . . . , X2m), . . . , are independent.
5. If X = (X1, . . . , Xn) has (joint) distribution Q and Xi have (marginal) distribu-
tions Qi, i = 1, 2, . . ., n, then equation (4.9) holds if and only if
Q(B1 × · · · × Bn) =
nQ
i=1 Q(Bi).
Now we illustrate the importance of the cdf in identifying independence.
Theorem 4.10 (Independence via cdfs) Let X1, X2, . . . , be R−valued random
variables, deﬁned on a probability space (Ω, A, P). Then the coordinates of the
random vector X = (X1, . . . , Xn) are independent if and only if
FX1,...,Xn(x1, . . . , xn) = FX1(x1) . . . FXn(xn),
(4.11)
∀n ≥1 and (x1, . . . , xn) ∈Rn, where FX1,...,Xn, FX1, . . . , FXn denote distribution
functions.
Proof. (=⇒) If X1, . . . , Xn are independent then equation (4.11) holds using
Bi = (−∞, xi] in theorem 4.9.
(⇐=) For the other direction we need to show that for all B1, . . . , Bn ∈B1 and
for all n = 1, 2, . . ., we have
P(X1 ∈B1, . . . , Xn ∈Bn) =
nQ
i=1 P(Xi ∈Bi).
Fix x2, . . . , xn and deﬁne two σ-additive measures
Q1(B) = P(X1 ∈B, X2 ≤x2, . . . , Xn ≤xn),
and
Q′
1(B) = P(X1 ∈B)P(X2 ≤x2, . . . , Xn ≤xn),
and note that on sets of the form (−∞, x], Q1 and Q′
1 agree. Therefore, by extension
and uniqueness of measure, Q1 = Q′
1 on B1 = σ({(−∞, x]}). Repeat the process for
a ﬁxed B1 ∈B1 and ﬁxed x3, . . . , xn and deﬁne
Q2(B) = P(X1 ∈B1, X2 ∈B, X3 ≤x3, . . . , Xn ≤xn),
and
Q′
2(B) = P(X1 ∈B)P(X2 ∈B)P(X3 ≤x3, . . . , Xn ≤xn),
and since Q2 and Q′
2 agree for B = (−∞, x], they agree on B1. Continue this way to
establish the desired equation.
An immediate consequence of the latter theorem is the following.

156
PROBABILITY THEORY
Remark 4.6 (Independence via densities) If the random variables X1, . . . , Xn with
distributions Q1, . . . , Qn, have (marginal) densities fi =
hdQi
dµi
i
with respect to µi,
then equation (4.11) is equivalent to
fX1,...,Xn(x1, . . . , xn) = fX1(x1) . . . fXn(xn),
(4.12)
where fX1,...,Xn is the (joint) density of the random vector (X1, . . . , Xn).
Example 4.3 (Random objects via densities) The following examples illustrate
some random objects and their densities.
1. Uniform random point Let X = (X1, X2) be a random point in the unit square
[0, 1]2 that arises by generating the coordinates X1 and X2 as independent and uni-
formly distributed random variables on [0, 1]. The joint density is therefore given
by
f (x1, x2) = I[0,1](x1)I[0,1](x2).
For any subset A ⊆[0, 1]2 the probability that X falls in A is computed using the
Radon-Nikodym theorem by
P(X ∈A) =
R
A
f (x1, x2)dx1dx2 =
R
A
dx1dx2 = |A|,
the area of A and therefore probabilities in this context are measures of area. In
general, let A ⊂R2, with area |A| > 0. Let X be a random point uniformly distributed
in A, that is,
f (x1, x2) = |A|−1IA(x1, x2),
so that for B ⊂A, we have
P(X ∈B) = |B|/|A|.
2. Uniform random lines Any straight line in R2 is uniquely identiﬁed by its
orientation θ and signed distance ρ from the origin, namely, Lθ,ρ = {(x, y) :
x cos θ + y sin θ = ρ}, with θ ∈[0, π) and ρ ∈R. Let X be a random straight
line through the unit disc b(0, 1), which is generated by taking θ and ρ to be inde-
pendent and uniform over [0, π) and [−1, 1], respectively. Therefore, the density of
the random vector (θ, ρ) is given by
f (θ, ρ) = (2π)−1I[0,π)(θ)I[−1,1](ρ).
If C = {(x, y) : (x −c1)2 + (y −c2)2 = r2} is a circle of radius r centered at (c1, c2)
contained in b(0, 1), then the probability that X intersects C is
P(X ∩C , ∅) =
πR
0
1R
−1
I(Lθ,ρ ∩C , ∅)(2π)−1dρdθ = r,
which is proportional to the circumference of C, regardless of the location of C
within b(0, 1).
The following theorem can be particularly useful in proving independence be-
tween collections of events.

PROBABILITY MEASURES AND PROBABILITY SPACES
157
Theorem 4.11 (Showing independence) If G and L are independent classes of
events and L is closed under intersections, then G and σ(L) are independent.
Proof. For any B ∈G let
L∗= {A : A ∈σ(L) and P(A ∩B) = P(A)P(B)}.
Notice that 1) Ω∈L, 2) if A1, A2 ∈L∗and A2 ⊂A1 =⇒A1 ∖A2 ∈L∗and 3) if
A = lim
n→+∞An, with An ⊂An+1, n ≥1 and An ∈L∗=⇒A ∈L∗, since An∩B ⊂An+1∩B
and
P(A ∩B) = lim
n→+∞P(An ∩B) = lim
n→+∞P(An)P(B) = P(A)P(B).
Thus L∗is a Sierpi´nski class with L ⊂L∗and by the Sierpi´nski class theorem
σ(L) = L∗. As a consequence, G and σ(L) are independent.
The following deﬁnition and 0-1 law helps us assess when a sequence of iid
random variables converges a.s. More details on convergence of random sequences
and their partial sums can be found in Chapter 5.
Deﬁnition 4.9 Tail σ-ﬁeld
The tail σ-ﬁeld of a sequence of random variables {Xn : n ≥1} on (Ω, A, P) is
deﬁned by
+∞
T
n=1 σ({X j : j ≥n}). A subset of the tail σ-ﬁeld is called a tail event.
Example 4.4 (Tail events) Tail σ-ﬁelds and events appear very often in asymp-
totics.
1. Let {Xn}+∞
n=1 be a sequence of independent random variables. The set {ω ∈Ω:
+∞
P
n=1 Xn(ω) converges} is a tail event, and, as we will see in the next theorem, this
event has probability 0 or 1, which makes sense intuitively since a series either
converges or it does not converge.
2. Let {An} be independent. Then Xn(ω) = IAn(ω) are independent random vari-
ables and
+∞
S
j=n A j ∈σ({X j : j ≥m}), for n ≥m ≥1, which implies that the event
lim sup An = (An i.o.) =
+∞
T
n=m
+∞
S
j=n A j ∈σ({Xm, Xm+1, . . . }), is a tail event. Similarly, the
event lim inf An = (An ev.) is a tail event.
The celebrated Kolmogorov 0-1 Law is proven next.
Theorem 4.12 (Kolmogorov 0-1 Law) Tail events of a sequence {Xn : n ≥1} of
independent random variables have probability 0 or 1.

158
PROBABILITY THEORY
Proof. For n ≥1, σ({Xi : 1 ≤i ≤n}) and σ({X j : j > n}) are independent and
therefore σ({Xi : 1 ≤i ≤n}) is independent of D =
+∞
T
n=0 σ({Xi : j > n}), ∀n ≥1.
This implies that A =
+∞
S
n=1 σ({Xi : 1 ≤i ≤n}) is independent of D. We can easily
see that A is a ﬁeld and hence is closed under intersection. By theorem 4.11 σ(A)
is independent of D. But D ⊂σ({Xn : n ≥1}) = σ(A), which implies that the tail
σ-ﬁeld D is independent of itself and consequently ∀B ∈D, P(B∩B) = P2(B) =⇒
P(B) = 0 or 1.
Next we discuss a few classic methods of computing the probabilities of such
limiting events.
4.2.5
Calculating Probabilities for Limits of Events
Consider a probability space (Ω, A, P). In order to compute the probability of
an event A ∈A, it is often useful to express A in terms of other, easier to compute
events (for example using remark 3.1.9). Applying Fatou’s lemma (theorem 3.13)
on the functions fn = IAn, with µ = P, so that
R
f dµ = P(An) and noting that
(limAn)c = limAc
n, we have
P(limAn) ≤limP(An) ≤limP(An) ≤P(limAn).
Consequently, the continuity of probability measure from above and below reduces
to the continuity of probability measure.
Theorem 4.13 (Continuity of probability measure) Let {An} be a sequence of
events in a probability space (Ω, A, P). If A = lim
n→∞An then P(A) = lim
n→∞P(An).
For what follows we require an extended version of the rudimentary deﬁnition
of independence between events.
Deﬁnition 4.10 Independence of events
Let A and B be two events in the probability space (Ω, A, P). Then A and B are
independent if and only if P(A ∩B) = P(A)P(B). The events are called negatively
correlated if and only if P(A ∩B) < P(A)P(B) and positively correlated if and
only if P(A ∩B) > P(A)P(B). If the events are independent then they are called
uncorrelated.
Note that for random variables, i.e., A = {ω : X(ω) = x} and B = {ω : Y(ω) =
y}, zero correlation does not necessarily imply independence (recall deﬁnition 1.12
of Pearson’s correlation and see exercise 4.26). The following lemmas can be used
to calculate the probability of limit superior. The proof is straightforward and is
requested as an exercise.

PROBABILITY MEASURES AND PROBABILITY SPACES
159
Lemma 4.1 (Calculating probabilities in the limit) Let {An} be a sequence of
events in a probability space (Ω, A, P) and let A = limAn = (An i.o.).
1. Borel If
+∞
P
n=1 P(An) < +∞, then P(A) = 0.
2. Kochen-Stone If
+∞
P
n=1 P(An) = +∞, then
P(A) ≥lim sup
n→∞
" nP
k=1 P(Ak)
#2
nP
k=1
nP
m=1 P(Ak ∩Am)
.
3. Borel-Cantelli Assume that events Ai and A j are either negatively correlated or
uncorrelated. Then
+∞
P
n=1 P(An) = +∞implies P(A) = 1.
An immediate consequence of Borel’s lemma is the following.
Remark 4.7 (a.s. convergence) Borel’s lemma is useful in showing almost sure
convergence since Xn
a.s.
→X is equivalent to P(|Xn −X| > ε i.o.) = 0, for all ε > 0.
Example 4.5 (Inﬁnite coin ﬂips) Consider the inﬁnite coin ﬂip example 4.1.3,
and let pn = P(Heads), 0 ≤pn ≤1, n ≥1 and An = {Heads occurs at the nth ﬂip}
so that pn = P(An). Now if
+∞
P
n=1 pn < +∞then P(An i.o.) = 0, i.e., P(eventually
Tails) = 1 and if
+∞
P
n=1 pn = +∞then P(An i.o.) = 1. Moreover, if
+∞
P
n=1(1 −pn) < +∞
then P(eventually Heads) = 1 and if
+∞
P
n=1(1 −pn) = +∞then P(Ac
n i.o.) = 1.
4.2.6
Expectation of a Random Object
Expectation provides a useful tool to help us study random objects and their dis-
tribution. We can think of expectation as the “average” value of the random object
in the sense that we weigh the values of the random object by the corresponding
probabilities we assign to those values and then aggregate these weighted values.
The deﬁnition of expectation for standard random objects is given next.
Deﬁnition 4.11 Expectation
Let X be a random object deﬁned on the probability space (Ω, A, P).
1. Random variable If X takes values in the measurable space (R, B1), then the
expected value of the random variable X is the integral of the function X with

160
PROBABILITY THEORY
respect to P, that is
E(X) =
R
XdP =
R
Ω
X(ω)dP(ω).
(4.13)
2. Random vector If X = [X1, . . . , Xp]T is a random vector in the measurable
space (Rp, Bp) then the expected value of the random vector X is the vector of
expectations, that is,
E(X) = [EX1, . . . , EXp]T.
(4.14)
3. Random matrix If X = [(Xij)] is a random matrix in the measurable space
(M, 2M), where M is the collection of all m × n matrices, then the expected value
of the random matrix X is the matrix of expectations deﬁned by
E(X) = [(EXij)].
(4.15)
We collect some consequences of this important deﬁnition next.
Remark 4.8 (Expectation) In some cases we will use the notation EP(X) in order
to emphasize the probability measure with respect to which the integral is com-
puted. Since expectation of a random variable is by deﬁnition an integral of some
measurable function, it inherits all the properties we have seen from general integra-
tion theory, such as linearity and monotonicity. Note that the deﬁnition in equation
(4.13) does not require X to have a density.
1. Owing to properties of integrals for measurable functions, E(X) = E(X+) −
E(X−) is deﬁned if at least one of E(X+) or E(X−) is ﬁnite.
2. Let X = IA, where A is a measurable set. Then E(X) = P(X ∈A).
3. Consider a random object X : (Ω, A, P) →(X, G) and a measurable function
ϕ : (X, G) →(R, B(R)), so that ϕ ◦X is a random variable from (Ω, A, P) into
(R, B(R)). Using change of variable (with X playing the role of T, see theorem
3.16) we have that
EP(ϕ ◦X) =
R
Ω
ϕ(X(ω))dP(ω) =
R
R
ϕ(t)dQ(t) = EQ(ϕ),
(4.16)
where Q = PX−1 is the distribution of X.
4. The deﬁnitions and results of Section 1.2.2 are easily obtained using deﬁnition
4.11. In particular, the deﬁnitions of moments and mgfs will be utilized in this
chapter.
5. If X has distribution Q = PX−1 and a density f =
hdQ
dµ
i
with respect to a measure
µ, i.e., Q ≪µ, then we can write equation (4.16) as
EP(ϕ ◦X) =
R
R
ϕ(t)f (t)dµ(t),
(4.17)
which is typically easier to compute, especially in the case where µ is a Lebesgue
or the counting measure.

PROBABILITY MEASURES AND PROBABILITY SPACES
161
6. Let X : (Ω, A, P) →(R, B1) be a random variable with distribution function F
and let ϕ be an R-valued function that is Riemann-Stieltjes integrable with respect
to F on every bounded interval. Then we can show that
EP(ϕ ◦X) =
+∞
R
−∞
ϕ(x)dF(x),
(4.18)
in the sense that if one side is deﬁned then so is the other and the two are equal.
7. If a random object X takes values in a measurable space (X, G) then we should
deﬁne expectation in such a way as to have EX live in X. This is easily accom-
plished for the random objects in deﬁnition 4.11. For a random collection of points
(a point process) X in Rp, the expected value must be a collection of points. Sim-
ilarly, for a random set X in Rp, the expected value must be a subset of Rp. We
discuss these two cases in more detail in the TMSO-PPRS text.
The following is a version of the Fubini theorem in a probabilistic setting. In
particular, this result shows that under independence, two random variables are
uncorrelated. The converse is not true (see exercise 4.26).
Theorem 4.14 (Independent implies uncorrelated) Let X and Y be indepen-
dent, R-valued random variables with ﬁnite expectation. Then
E(XY) = E(X)E(Y),
or equivalently Cov(X, Y) = E(XY) −E(X)E(Y) = 0.
Proof. The independence of X and Y is equivalent to the distribution of (X, Y)
being the product measure Q1 × Q2, where Q1 and Q2 are the distributions of X and
Y. Appealing to the Fubini theorem we have
E|XY| =
R
X1
R
X2
|x||y|Q2(dy)Q1(dx) =
R
X1
|x|E(|Y|)Q2(dy) = E(|X|)E(|Y|) < +∞,
so that XY is integrable and hence we can use the Fubini theorem to obtain
E(XY) =
R
X1
R
X2
xyQ2(dy)Q1(dx) =
R
X1
xE(Y)Q2(dy) = E(X)E(Y).
The following result is particularly useful and allows us to swap the expectation
and product operators.
Theorem 4.15 (Expectation and independence) Let X1, . . . , Xn be independent
random variables taking values in the measurable spaces (Xi, Gi) and let fi be
Gi|B1-measurable with E(| fi(Xi)|) < +∞, i = 1, 2, . . ., n. Then
E
 nQ
i=1 fi(Xi)
!
=
nQ
i=1 E (fi(Xi)) .
(4.19)

162
PROBABILITY THEORY
Proof. For all A ∈G1 let f1(x) = IA(x) and consider the class L of R-valued
functions f2(y), for which equation (4.19) holds (for n = 2), i.e., E(IA(X1)f2(X2)) =
P(X1 ∈A)E(f2(X2)). Clearly, L contains indicator functions f2(X2) = IB(X2), for all
B ∈G2 and by linearity of expectation L contains f2(x2) =
mP
i=1 biIBi(x2), any simple
function. Using the MCT, L contains any measurable f2 ≥0 and for any R-valued
f2, since E(| f2(X2)|) < +∞, f2 is integrable and hence
E(IA(X1)f2(X2))
=
E(IA(X1)f +
2 (X2)) −E(IA(X1)f −
2 (X2))
=
P(X1 ∈A)E(f +
2 (X2)) −P(X1 ∈A)E(f −
2 (X2))
=
P(X1 ∈A)E(f +
2 (X2) −f −
2 (X2))
=
P(X1 ∈A)E(f2(X2)),
so that L contains any R-valued function. Now ﬁx an R-valued f2 and deﬁne L′
to be the class of R-valued measurable functions f1 for which E(f1(X1)f2(X2)) =
E(f1(X1))E(f2(X2)). Then L′ contains f1(x) = IA(x), A ∈G1 =⇒
f1(x1) =
mP
i=1 aiIAi(x1) ∈L′ for any simple function and using the MCT we extend to any
f1 ≥0. Following similar arguments as in the last display we can show that L′
contains all R-valued functions, thus establishing the result for n = 2. The general-
ization for any n is straightforward using induction.
4.2.7
Characteristic Functions
Let X : (Ω, A, P) →(R, B1), be a random variable and recall deﬁnition 1.13 of
the cf
ϕX(t) = EeitX = E [cos(tX)] + iE [sin(tX)] ,
for all t ∈R. Here we used DeMoivre’s formula for n = 1, that is,
einy = (cos y + i sin y)n = cos(ny) + i sin(ny),
n = 0, 1, . . . and y ∈R. It is sometimes convenient to say that ϕX(t) is the cf
corresponding to the probability measure P or the distribution Q = PX−1 instead of
the random variable X. Some basic properties are presented below.
Remark 4.9 (Properties of characteristic functions) If X has distribution func-
tion FX and density fX, then we can write
ϕX(t) =
R
R
eitxP(dx) =
R
R
eitxdFX(x) =
R
R
eitx fX(x)dx,
where the second integral is a RS-integral and the third a standard Riemann integral.
The cf ϕX(t) is also called the Fourier transform of fX(x).
1. Clearly, ϕX(0) = 1 and ϕaX+b(t) = eibtϕX(at) by deﬁnition. Moreover, if X is
integer valued then ϕX(t + 2π) = ϕX(t).
2. |ϕX(t)| = [ϕX(t)]∗ϕX(t) ≤1, where the absolute value for a complex number
z = x + iy is deﬁned as |z| = z∗z = x2 + y2, the length of z, with z∗= x −iy the
conjugate of z. If z is a vector of complex numbers then z∗is the transpose of the
conjugates of the coordinates of z, called the conjugate transpose of z.

PROBABILITY MEASURES AND PROBABILITY SPACES
163
3. ϕX is uniformly continuous on R with respect to the absolute value norm, that
is, ∀ε > 0, there exists a δ > 0 (that depends only on ε) such that for all x, y ∈R,
with |x −y| < δ =⇒|ϕX(x) −ϕX(y)| < ε.
4. The cf can generate moments since ϕ(k)
X (0) =

dkϕX(t)
dtk

t=0 = ikE(Xk), k =
0, 1, 2, . . .. If EXk exist for any k, it is easy to see that ϕX(t) has a Taylor expan-
sion about t = 0 given by
ϕX(t) =
+∞
P
k=0(it)kE(Xk)/k!,
and hence ϕX(t) is uniquely determined if and only if the non-central moments
E(Xk) are ﬁnite for all k = 0, 1, 2, . . .. If
+∞
P
k=0 |t|kE(|X|k)/k! = E(e|tX|) < ∞,
then the cf has a Taylor expansion if the mgf mX(t) = E

etX
of X is deﬁned for all
t ∈R.
5. If X has a density then ϕX(t) →0, as |t| →∞.
6. For all n = 1, 2, . . . and all complex n-tuples (z1, . . . , zn) and real n-tuples
(v1, . . . , vn) we have
nP
k=1
nP
j=1 ϕX(vk −vj)z jz∗
k ≥0,
that is, the cf is a positive deﬁnite C-valued function on R, where C is the complex
plane.
7. Bochner’s theorem A continuous function ϕ is the cf of a random variable if
and only if it is positive deﬁnite and ϕ(0) = 1.
Example 4.6 (Normal characteristic function)
Assume that X ∼N(0, 1) and
consider the cf for all t ∈R given by
ϕX(t) = (2π)−1/2
+∞
R
−∞
eitxe−x2/2dx.
Using Liebnitz’s rule we diﬀerentiate the latter with respect to t and use integration
by parts to obtain dϕX(t)
dt
= −tϕX(t) and thus ϕX(t) is the unique solution of a ﬁrst-
order, homogeneous diﬀerential equation under the condition ϕX(0) = 1. It is easy
to see that the solution is given by ϕX(t) = e−t2
2 , t ∈R. Since Y = σX+µ ∼N(µ, σ2),
remark 4.9.1 yields
ϕY(t) = eiµtϕX(σt) = eiµt−σ2t2/2, t ∈R.
Uniqueness arguments for cfs can be established using the following.
Lemma 4.2 (Parseval relation) Let P and Q be two probability measures on R
and denote their cfs by ϕP and ϕQ, respectively. Then
R
ϕQ(x −v)P(dx) =
R
e−ivyϕP(y)Q(dy),
(4.20)

164
PROBABILITY THEORY
Table 4.1: Characteristic functions of commonly used continuous distributions.
Distribution
Density
Support
Parameter
Space
Characteristic
Function
Normal
1
√
2πσe−
1
2σ2 (x−µ)2
R
(µ, σ) ∈R × R+
eiµt−σ2t2
2
Gamma
xa−1e−x
b
Γ(a)ba
R+
(a, b) ∈R+ × R+
(1 −ibt)−a
Uniform
1
b−a
[a, b]
(a, b) ∈R2, a < b
eibt−eiat
(b−a)it = i eiat−eibt
(b−a)t
Double
Exponential
1
2e−|x|
R
−
1
1+t2
Cauchy
1
π(1+x2)
R
−
e−|t|
Triangular
1 −|x|
(−1, 1)
−
2 1−cos t
t2
for all v ∈R.
Proof. The function f (x, y) = ei(x−v)y is bounded and continuous and hence
measurable so that the integral I =
R
f (x, y)d(P × Q)(x, y) exists. Using the Fubini
theorem we can write
I =
Z
ei(x−v)yd(P × Q)(x, y) =
Z "Z
ei(x−v)ydP(x)
#
dQ(y) =
Z
e−ivyϕP(y)dQ(y),
and on the other hand
I =
Z "Z
ei(x−v)ydQ(y)
#
dP(x) =
Z
ϕQ(x −v)dP(x),
and the result is established.
Based on the Parseval relation we collect the following.
Remark 4.10 (Special Parseval relation) Consider the special case of the Parseval
relation for Q corresponding to a N(0, σ2) random variable. In this case, equation
(4.20) reduces to
+∞
R
−∞
e−σ2(x−v)2/2P(dx) =

2πσ2−1/2 +∞
R
−∞
e−ivye−y2/(2σ2)ϕP(y)dy.
(4.21)
If P has distribution function FP, then we can write
+∞
R
−∞
e−σ2(x−v)2/2Fp(dx) =

2πσ2−1/2 +∞
R
−∞
e−ivye−y2/(2σ2)ϕP(y)dy.
We will refer to these equations as the special Parseval relation applied to {P, ϕP}
or {FP, ϕP}. Equation (4.21) uniquely determines P for a given ϕP, as shown in the
next theorem.
Now we are ready to discuss the uniqueness of probability measures via unique-
ness in characteristic functions.

CONDITIONAL PROBABILITY
165
Theorem 4.16 (Uniqueness via the characteristic function) If P and Q are
probability measures on R with the same cf ϕ then P = Q.
Proof. Since P and Q have the same characteristic function ϕ, the latter remark
implies
+∞
R
−∞
e−σ2(x−v)2
2
P(dx) =
+∞
R
−∞
e−σ2(x−v)2
2
Q(dx),
for v ∈R and σ > 0. Multiply both sides with
σ
√
2π, integrate over (−∞, a) with
respect to the Lebesgue measure and apply the Fubini theorem to obtain
+∞
R
−∞
" aR
−∞
σ
√
2π
e−σ2(x−v)2
2
dv
#
P(dx) =
+∞
R
−∞
" aR
−∞
σ
√
2π
e−σ2(x−v)2
2
dv
#
Q(dx),
so that we integrate a normal density with mean x and variance
1
σ2, that is,
aR
−∞
σ
√
2π
e−σ2(x−v)2
2
dv = Fv (a) ,
where Fv is the distribution function of a N

x, 1
σ2

random variable. Letting σ →
∞, we have Fv(a) →1, if x < a, 0 if x > a and Fv(a) →1
2, if x = a. Then the BCT
yields
P((−∞, a)) + 1
2P({a}) = Q((−∞, a)) + 1
2Q({a}),
∀a ∈R. Since there can be at most countably many values such that either P({a}) or
Q({a}) is nonzero, we conclude that there is a dense set of real numbers a such that
P((−∞, a)) = Q((−∞, a)) and consequently an appeal to the uniqueness of measure
theorem establishes the result.
4.3
Conditional Probability
The conditional probability and expectation given the (ﬁxed) value of a random
variable X(ω) = x is not a random variable since the ω is ﬁxed. We would like
to see what happens if we replace the event G = {ω : X(ω) = x}, with a larger
class of sets, for example a σ-ﬁeld G. Therefore, when we write P(A|X) or E(Y|X)
(without setting the value X(ω) = x for a speciﬁc ω) we are referring to a random
probability P(A|σ(X)) or a random expectation E(Y|σ(X)), where A is some event
and Y is some random variable. Consequently, we need to deﬁne probability and
expectation conditionally with respect to a σ-ﬁeld. First we introduce conditioning
given the value of a random variable via rigorous arguments.
4.3.1
Conditioning on the Value of a Random Variable
For what follows, let (Ω, A, P) denote a probability space and deﬁne X :
(Ω, A, P) →(X, G) a random variable with distribution QX. We use the Radon-
Nikodym theorem to give a general deﬁnition of conditional probability and moti-
vate the deﬁnition using the following theorem. The material in this section will be

166
PROBABILITY THEORY
Table 4.2: Characteristic functions of commonly used discrete distributions.
Distribution
Density
Support
Parameter
Space
Cf
Binomial
Cn
xpxqn−x
{0, 1, . . ., n}
n ∈Z+
p ∈[0, 1]
q = 1 −p
(peit + q)n
Poisson
λxe−λ
x!
{0, 1, . . .}
λ ∈R+
e−λ(1−eit)
Geometric
qpx
Z+
p ∈[0, 1]
q = 1 −p
1−p
1−peit
Negative
Binomial
Cx−1
r−1 prqx−r
{r, r + 1, . . . }
r ∈Z+
p ∈[0, 1]
q = 1 −p

q
1−peit
r
Dirac (Delta)
1
{0}
−
1
better appreciated if we have in the back of our minds equations (4.13), (4.16) and
(4.17), that illustrate how one can write expectations either as an integral [P] or the
distribution QX = PX−1 or the density fX (when the density exists).
Theorem 4.17 (Conditional probability via the Radon-Nikodym theorem)
Let A ∈A be a ﬁxed set. There exists a unique (in the a.e. [QX] sense), real-
valued, Borel measurable function g deﬁned on (X, G) such that
P((X ∈B) ∩A) =
R
B
g(x)dQX(x),
(4.22)
for all B ∈G.
Proof. Let λ(B) = P((X ∈B) ∩A), B ∈G and note the λ is a σ-ﬁnite measure
on G with λ ≪QX. Therefore, it follows from the Radon-Nikodym theorem that
there exists a function g satisfying the statements of the theorem.
The latter theorem motivates the following.
Deﬁnition 4.12 Conditional probability given X = x
The conditional probability of a set A ∈A given that X = x (denoted by A|X = x)
is deﬁned as the function
g(x) = P(A|X = x),

CONDITIONAL PROBABILITY
167
for all A ∈A where g is the Radon-Nikodym derivative of the latter theorem, that
is, for all B ∈G we have
P((X ∈B) ∩A) =
R
B
P(A|X = x)dQX(x).
(4.23)
We write P(A|X) to denote the random variable g(X) so that P(A|X) is thought of
as a random probability. If A = (Y ∈G) for some random variable Y : (Ω, A, P) →
(Y, G), G ∈G, then g(x) = P(Y ∈G|X = x) is called the conditional probability
distribution of Y given X = x and is denoted by QY(.|X = x).
Example 4.7 (Conditional probability) We should think of this deﬁnition as if
we are working backward, in the sense that we want to ﬁnd or guess some g(x) that
satisﬁes equation (4.22). To appreciate this further take (Ω, A, P) to be a discrete
probability space. In particular, assume that X takes values in X = {x1, x2, . . . } and
set G = 2X, with P(X = xi) = pi > 0, i = 1, 2, . . . and
+∞
P
i=1 pi = 1. In this case QX has
a density fX(x) = P(X = x), x ∈X, with respect to the counting measure.
1. We claim that for any A ∈A we have that
g(xi) = P(A|X = xi) = P(A ∩{ω : X(ω) = xi})
P(X = xi)
,
(4.24)
is the form of the conditional probability. We take a B ∈G and show that this g is
the desired one by showing that equation (4.23) is satisﬁed. Indeed, we can write
R
B
g(x)dQX(x)
=
R
X
g(x)IB(x)dQX(x) =
+∞
P
i=1 g(xi)IB(xi)P(X = xi)
=
P
xi∈B
P(A ∩{ω : X(ω) = xi})
P(X = xi)
P(X = xi)
=
P
xi∈B P(A ∩{ω : X(ω) = xi}) = P(A ∩(X ∈B)),
and therefore the function g(xi) as deﬁned above is the conditional probability
P(A|X = xi).
2. Note that the deﬁnition we have given agrees with the rudimentary P(A|B) =
P(A ∩B)/P(B), for P(B) > 0. To see this, set X(ω) = IB(ω) in equation (4.24).
3. Now take A = {ω : Y(ω) = y} for some discrete random variable Y, let B = X
and write (4.23) as
P((X ∈X) ∩(Y = y)) = P
x∈X P(Y = y|X = x)fX(x),
which reduces to the marginal density of the random vector (X, Y) with respect to
X, i.e.,
P(Y = y) = P
x∈X P(Y = y, X = x),
where g(x) = P(Y = y|X = x) = P(X = x, Y = y)/P(X = x).
Example 4.8 (Regular conditional probability) Let (Ωi, Ai), i = 1, 2, be mea-

168
PROBABILITY THEORY
surable spaces and deﬁne Ω= Ω1 × Ω2 and A = A1 ⊗A2. Let X(ω1, ω2) = ω1 and
Y(ω1, ω2) = ω2 be the projection functions. Assume that we are given QX a prob-
ability measure on (Ω1, A1) and P(x, B), x ∈Ω1, B ∈A2, a probability measure
in (Ω2, A2), for any x ∈Ω1. Further assume that P(., B) is a measurable function
on (Ω1, A1). Applying the deﬁnition of the product measure space (deﬁnition 3.18)
along with the Fubini theorem, there exists a unique measure P on (Ω, A) such that
P(X ∈A, Y ∈B) =
R
A
P(x, B)dQX(x),
so that P(x, B) = P(B|X = x). The function P(x, B) is called a version of a regular
conditional probability and any two versions are equal except on a set of probability
0. This follows from our deﬁnition by setting Ω= Ω1 × Ω2, X = Ω, A = A1 ⊗A2
and G = A. Note that in order to develop conditional probability for random objects
that are vectors we typically take X = Rp and G = Bp.
We motivate the deﬁnition of conditional expectation using the following theo-
rem.
Theorem 4.18 (Conditional expectation via the Radon-Nikodym theorem)
Let Y be a random variable on (Ω, A, P) and let X : (Ω, A, P) →(X, G) be a
random object with distribution QX = PX−1. If E(Y) exists then there exists a
unique Borel measurable function g : (X, G) →(R, B1), such that for all B ∈G
we have
R
{X∈B}
YdP =
R
B
g(x)dQX(x).
(4.25)
Proof. The proof is the same as that of theorem 4.12 once we set λ(B) =
R
{X∈B}
YdP, B ∈G.
The deﬁnition of conditional expectation is now naturally introduced.
Deﬁnition 4.13 Conditional expectation given X = x
The conditional expectation of a random variable Y given X = x (or {ω : X(ω) =
x}), denoted by E(Y|X = x), is the function of x deﬁned as the Radon-Nikodym
derivative g(x) given in the latter theorem, namely, it is the unique Borel measurable
function deﬁned on (X, G) such that for all B ∈G we have
R
{X∈B}
YdP =
R
B
E(Y|X = x)dQX(x).
(4.26)
We write E(Y|X) to denote the random variable g(X) so that E(Y|X) is thought of
as a random expectation.
An immediate consequence of the latter is the following.
Corollary 4.1 In the notation of theorem 4.18 we can show that

CONDITIONAL PROBABILITY
169
(i) EP(Y) = EQX[E(Y|X)], and
(ii) E(IA|X = x) = P(A|X = x) a.e. [QX], for any A ∈A.
Proof. (i) Let B = X so that {X ∈X} = {ω : X(ω) ∈X} = Ω, in theorem 4.18.
(ii) Let Y = IA in the deﬁnition of E(Y|X = x) to obtain
P((X ∈B) ∩A) =
R
B
E(IA|X = x)dQX(x),
for all B ∈G, which is exactly the deﬁnition of the conditional probability P(A|X =
x).
Example 4.9 (Finite mixtures) The mixtures of Section 1.2.3 are special cases
of the theory in this section. In particular, if Q1, . . . , Qn are candidate distributions
for the random variable Y : (Ω, A, P) →(Y, G) and if pi = P(X = xi), 0 ≤pi ≤1,
i = 1, 2, . . ., n, with
nP
i=1 pi = 1, is the density of the discrete random variable X ∈
X = {x1, . . . , xn} with distribution QX, then
Q(G) = P(Y ∈G) =
nP
i=1 piQi(G),
for G ∈G deﬁnes a discrete mixture of the distributions Q1, . . . , Qn, where each
Qi can be thought of as a conditional distribution. Indeed, equation (4.22) can be
employed with A = (Y ∈G), B = X, QX({xi}) = pi and Qi(G) = P(Y ∈G|X = xi) to
help us write
P(Y ∈G) =
R
X
P(Y ∈G|X = x)dQX(x) =
nP
i=1 Qi(G)pi = Q(G).
Next we collect some results on conditional densities.
Remark 4.11 (Conditional expectation) From deﬁnition 4.12 the conditional
probability distribution Q(.|X = x) of Y given X = x can be used to deﬁne the
conditional expectation of Y|X = x as a function of x by
E(Y|X = x) =
R
Y
yQ(dy|X = x),
where Y is the domain of Y. To see that this is equivalent to (4.26) integrate both
sides above with respect to the (marginal) distribution QX of X and over a set B to
obtain
R
B
E(Y|X = x)dQX(x) =
R
B

R
Y
yQ(dy|X = x)
dQX(x).
(4.27)
Since by deﬁnition 4.13 the left side of (4.27) is
R
B
E(Y|X = x)dQX(x) =
R
{X∈B}
YdP,
the right side of (4.27) becomes
R
{X∈B}
YdP =
R
B

R
Y
yQ(dy|X = x)
dQX(x),
and thus g(x) =
R
Y
yQ(dy|X = x) satisﬁes (4.26) as desired.

170
PROBABILITY THEORY
4.3.2
Conditional Probability and Expectation Given a σ-ﬁeld
Now we collect the deﬁnition of conditional probability given a σ-ﬁeld.
Deﬁnition 4.14 Conditional probability given a σ-ﬁeld
Let A be an event in a probability space (Ω, A, P) and let H be a sub-σ-ﬁeld of A.
The conditional probability of A given H, denoted by P(A|H), is a random variable
that satisﬁes:
(i) P(A|H) is measurable in H and integrable, and
(ii) for all H ∈H we have
R
H
P(A|H)dP = P(A ∩H).
(4.28)
There are many random variables P(A|H) that satisfy the requirements of the
latter deﬁnition, but any two of them are equal a.e. [P]. This follows from the
Radon-Nikodym theorem since P(H) = 0 implies v(H) =
R
H
P(A|H)dP = 0, that
is, v ≪P and P(A|H) is simply the Radon-Nikodym derivative. An alternative, yet
equivalent deﬁnition that does not require the existence of a density can be given in
terms of expectations.
Remark 4.12 (Alternative deﬁnition) If Y = P(A|H) a.s., then condition (ii)
above can be written as
EP(YIH) = EP(IAIH),
(4.29)
for all H ∈H, where the expectations are taken with respect to P, so that the
integrals are computed over the space Ω. The expectation on the left hand side
is ﬁnite when Y is integrable [P]. Hence we may rewrite the conditions of the
deﬁnition as:
(i) Y = P(A|H) is measurable in H, and
(ii) EP(YIH) = EP(IAIH), ∀H ∈H,
with condition (ii) now requiring the integrability condition of Y in order for the
two sides to be ﬁnite (since EP(IAIH) = P(A ∩H) ≤1) and equal. We will adapt
these versions of the conditions for what follows.
Example 4.10 (Special cases for H) Consider the setup of the latter deﬁnition.
1. Suppose that A ∈H (this is always the case when H = A). Then the random
variable X(ω) = IA(ω), satisﬁes conditions (i) and (ii), so that P(A|H) = IA a.s.,
that is, A ∈H is equivalent to knowing in advance whether or not A has occurred.
2. If H = {∅, Ω} then every function that is H-measurable must be a constant
since
R
∅
P(A|H)dP = P(∅∩A) = P(∅) = 0,

CONDITIONAL PROBABILITY
171
and
R
Ω
P(A|H)dP = P(Ω∩A) = P(A),
that is, P(A|H)(ω) = P(A), for all ω ∈Ωand hence we do not learn anything from
H about the probability of the event A.
3. An event A is independent of the σ-ﬁeld H if and only if P(A ∩H) =
P(A)P(H), ∀H ∈H, or equivalently
P(A ∩H) =
R
H
P(A)dP,
and therefore A is independent of H if and only if P(A|H)(ω) = P(A) a.s.
4. Suppose that B ∈A and take H = σ(B) = {∅, Ω, B, Bc}, with 0 < P(B) < 1.
We claim that
Y(ω) = P(A|H)(ω) = P(A ∩B)
P(B)
IB(ω) + P(A ∩Bc)
P(Bc)
IBc(ω) a.s.
First notice that Y(ω) as deﬁned is σ(B)-measurable. To check the second condition
we only have to consider the four members of σ(B). Clearly, E(YI∅) = 0 = P(A∩∅)
and
E(YIΩ) = E(YIB) + E(YIBc) = P(A ∩B) + P(A ∩Bc) = P(A ∩Ω).
Moreover
E(YIB) = P(A ∩B)
P(B)
E(IBIB) = P(A ∩B),
and similarly
E(YIBc) = P(A ∩Bc)
P(Bc)
E(IBcIBc) = P(A ∩Bc).
This example shows how P(A|H) in deﬁnition 4.14 becomes a generalization of
deﬁnition 4.12 (recall example 4.7.2).
5. If A and B are events in (Ω, A, P) with A ∈H then we claim that P(A ∩B|H) =
P(B|H)IA a.s. To see this, let us show that Y = P(B|H)IA satisﬁes conditions (i)
and (ii). Since P(B|H) is H-measurable by deﬁnition and A ∈H we have that Y is
H-measurable. We need to show that E(YIH) = P((A ∩B) ∩H), for all H ∈H, or
equivalently that
E(P(B|H)IA∩H) = P(B ∩(A ∩H)).
The latter holds by the deﬁnition of P(B|H) and since A ∩H ∈H.
The conditional probability of an event A given a random variable X is deﬁned
as the random variable P(A|σ(X))(ω), where σ(X) = {X−1(B) : B ∈B1}, but will be
denoted by P(A|X) for brevity. If the σ-ﬁeld is generated by many random variables
X1, X2, . . . , Xn then we write P(A|X1, . . . , Xn) instead of P(A|σ(X1, . . . , Xn)). If we
have a set of random variables {Xt : t ∈T}, for some index set T, then we will write
P(A|Xt : t ∈T) instead of P(A|σ(Xt : t ∈T)), where σ(Xt : t ∈T) = σ({X−1
t (B) :
B ∈B1, t ∈T}).

172
PROBABILITY THEORY
Properties and results for unconditional probabilities carry over to the condi-
tional case as we see next. Since we are given the forms of the conditional proba-
bilities, all we need to do to prove these results is verify conditions (i) and (ii). This
is straightforward in all cases and will be left as an exercise.
Theorem 4.19 (Properties of conditional probability given a σ-ﬁeld) Let
(Ω, A, P) be a probability space. Suppose that A, B, Ai, i = 1, 2, . . ., are A-
sets and let H be a sub-σ-ﬁeld of A. We can show the following:
(i) P(∅|H) = 0 a.s., P(Ω|H) = 1 a.s. and 0 ≤P(A|H) ≤1 a.s.
(ii) If {An} are disjoint then P
 S
n An|H
!
= P
n P (An|H) a.s.
(iii) If A ⊆B then P (B ∖A|H) = P (B|H)−P (A|H) a.s. and P (A|H) ≤P (B|H)
a.s.
(iv) If An ↑A then P(An|H) ↑P(A|H) a.s. and if An ↓A then P(An|H) ↓P(A|H)
a.s.
(v) The conditional inclusion-exclusion formula P
 nS
i=1 Ai|H
!
=
nP
i=1 P (Ai|H) −
P
i<j P

Ai ∩A j|H

+ . . . a.s., holds.
(vi) If P(A) = 1 then P(A|H) = 1 a.s. and P(A) = 0 implies that P(A|H) = 0 a.s.
Now we deﬁne the distribution of a random object given a σ-ﬁeld.
Deﬁnition 4.15 Conditional probability distribution given a σ-ﬁeld
Let X : (Ω, A, P) →(X, G), be a random object, H a sub-σ-ﬁeld of A and
consider Q the collection of all probability measures on (X, G). The conditional
probability distribution of X given H (or X|H) is a function of two arguments,
denoted by Z(ω, G) = QX(G|H)(ω), such that (i) Z(ω, .) is one of the probability
measures from the collection Q, for each ω ∈Ωand (ii) Z(., G) is (a version of)
the conditional probability of X−1(G) given H, that is, Z(., G) = P(X ∈G|H)(.) =
P(X−1(G)|H)(.), for each G ∈G. To simplify the notation, we will write QX(.|H)
for the conditional distribution of X|H.
Some comments about this deﬁnition are given next.
Remark 4.13 (Conditional distribution) We note the following.
1. Note that if we let H = σ(Y), for some random object Y, then QX(.|Y)(.) can be
thought of as the conditional distribution of X given Y.
2. From property (i), Z(ω, .) = QX(.|H)(ω) is a function that takes outcomes from
Ωand gives us an element of Q (that is a probability measure) and therefore if we
equip Q with a σ-ﬁeld then we can talk about a random probability distribution. In
fact, QX(.|H)(ω) deﬁnes a random distribution as we vary ω ∈Ω, but it does not

CONDITIONAL PROBABILITY
173
map to any probability measure in Q, just those with the additional property that
they are versions of the conditional probability P(X−1(G)|H), for each G ∈G.
Motivated by the discussion above, the deﬁnition of a random distribution is
provided next.
Deﬁnition 4.16 Random distribution
Let (X, G) be a measurable space and let Q denote the collection of all probability
measures on (X, G). The measurable space of probability measures on (X, G) is
denoted by (Q, V), where V is the smallest σ-ﬁeld such that for each G ∈G the
function U : Q →[0, 1] deﬁned by U(Q) = Q(G), Q ∈Q, is V-measurable. A
random distribution D on (X, G) is a measurable function from some probability
space (Ω, A, P) to (Q, V), that is, D−1(V) ∈A, ∀V ∈V.
Conditional probability calculations are signiﬁcantly simpliﬁed using a density.
The deﬁnition is similar to the unconditional case.
Deﬁnition 4.17 Conditional density given a σ-ﬁeld
Let X : (Ω, A, P) →(X, G) be a random vector (X ⊆Rp), H a sub-σ-ﬁeld of A
and let QX(.|H) be the conditional distribution of X|H. A nonnegative measurable
function q on (Ω× X, A ⊗G) is called a conditional density for X with respect to
µ, given H, if
P(X−1(G)|H)(ω) =
R
G
q(ω, x)µ(dx),
for all ω ∈Ωand G ∈G.
The deﬁnition holds for q that are the Radon-Nikodym derivatives of the condi-
tional distribution of X|H, QX(.|H), with respect to the measure µ, that is, it suﬃces
to have QX(.|H) be absolutely continuous with respect to µ. Since q is a function of
ω we may call it a random density.
The existence and uniqueness (in the a.s. sense) of conditional distributions can
be established in a rigorous framework regardless of the probability space on which
they are deﬁned and regardless of the conditioning σ-ﬁeld. See for example Fristedt
and Gray (1997, Section 21.4) or Billingsley (2012, theorem 33.3).
Next we collect conditional densities based on two random variables.
Remark 4.14 (Conditional density) Let Xi be (Xi, Gi)-valued random variables,
i = 1, 2, deﬁned on a common probability space. Assume that the distribution of the
random vector (X1, X2) has a density f with respect to a σ-ﬁnite product measure

174
PROBABILITY THEORY
µ = µ1 × µ2 on G1 ⊗G2 and let g(x1) =
R
X2
f (x1, x2)µ2(dx2). Then the function
h(ω, x2) =

f (X1(ω), x2)/g(X1(ω))
if g(X1(ω)) > 0
R
X1
f (x1, x2)µ1(dx1)
if g(X1(ω)) = 0
is a conditional density of X2 with respect to µ2 given σ(X1).
Before we tackle conditional expectation, let us discuss why we require the two
conditions in deﬁnition 4.14 of conditional probability.
Remark 4.15 (Insight on the requirements of conditional probability) We have
seen that if An ∈A, n = 1, 2, . . . , are disjoint events then
P
 S
n An|H
!
= P
n P (An|H) a.s.
(4.30)
The question is: can we choose a version of the conditional probability so that
P(.|H)(ω) is a measure on (Ω, A) for almost all ω ∈Ω? The problem is that for any
A ∈A we can choose a version of the conditional probability P(A|H)(ω) but for
ﬁxed ω ∈Ω, P(.|H)(ω) might not be countably additive on (Ω, A). The reason is
that (4.30) will hold except for some ω ∈N(A1, A2, . . . ), a null set [P] that depends
on the sequence {An}. Thus, as we vary the sequence {An}, the set of ωs where (4.30)
might fail is
M = ∪{N(A1, A2, . . . ) : An ∈A, disjoint},
and M can be an uncountable union of sets of measure 0 that can be of positive
measure or even not in A. The conditions take care of this situation. See Billingsley
(2012, p. 471) for a counterexample that shows that conditional probabilities may
not end up giving measures.
Next we collect the deﬁnition of conditional expectation.
Deﬁnition 4.18 Conditional expectation given a σ-ﬁeld
Suppose that X : (Ω, A, P) →(X, G) is an integrable random variable and let H
be a sub-σ-ﬁeld of A. Then there exists a random variable Y = E(X|H) a.s. called
the conditional expectation of X given H, with the properties (i) Y is H-measurable
and integrable and (ii) Y is such that
R
H
YdP =
R
H
XdP,
for all H ∈H.
Some consequences of this important deﬁnition are presented next.
Remark 4.16 (Conditional expectation) The existence of Y is easily established
in a similar fashion as in theorem 4.18, that is, Y is essentially a Radon-Nikodym
derivative. Any such random variable Y = E(X|H) will be called a version of the
conditional expectation of X|H. We note the following.

CONDITIONAL PROBABILITY
175
1. Condition (ii) may be rewritten in terms of expectations [P], that is ∀H ∈H,
we have
EP(YIH) = EP(XIH),
(4.31)
and will be adapted from now on in proofs. We can remove the integrability re-
quirement for X in the deﬁnition and Y in condition (i) provided that EP(YIH) and
EP(XIH) are deﬁned (in which case they will be equal). Consequently, in proofs we
only need to verify that a candidate random variable Y is H-measurable and that
(4.31) holds in order to have Y = E(X|H) a.s.
2. Conditional expectation can be deﬁned equivalently in terms of the conditional
probability distribution QX(.|H) or its density q(ω, x) (when it exists) as the random
variable
E(X|H)(ω) =
R
X
xQX(dx|H)(ω) a.s.,
or
E(X|H)(ω) =
R
X
xq(ω, x)µ(dx) a.s.
3. The conditional variance of X given H is naturally deﬁned by
Var(X|H) = EQX(.|H) h
(X −E(X|H))2i
a.s.
provided that E(X2|H) < +∞. We can easily show that Var(X|H) = E(X2|H) −
[E(X|H)]2 a.s.
The proofs of the following statements are requested as exercises. To prove the
conditional versions of the original theorems like the MCT and DCT one needs to
work with an arbitrarily ﬁxed ω and use the unconditional versions of the theorems.
Theorem 4.20 (Properties of conditional expectation given a σ-ﬁeld) Let
X, Y, Xn : (Ω, A, P) →(X, G) be integrable random variables and let H, H1,
H2, be sub-σ-ﬁelds of A. Then we can show the following.
(i) If X = c a.s. for some constant c then E(X|H) = c a.s.
(ii) E(E(X|H)) = E(X) a.s.
(iii) Var(X) = E(Var(X|H)) + Var(E(X|H)) a.s.
(iv) Linearity: E(aX + bY|H) = aE(X|H) + bE(Y|H) a.s. for any constants
a, b ∈R.
(v) Monotonicity: If X ≤Y a.s. then E(X|H) ≤E(Y|H) a.s.
(vi) |E(X|H)| ≤E(|X||H) a.s.
(vii) Conditional DCT: If Xn
a.s.
→X, |Xn| ≤Y and Y is integrable then E(Xn|H)
a.s.
→
E(X|H).
(viii) Conditional MCT: If 0 ≤X1 ≤X2 ≤. . . and Xn
a.s.
→X then 0 ≤E(Xn|H)
a.s.
→
E(X|H).
(ix) Conditional Jensen inequality: Let g : I →R be a convex function. Then
E(g ◦X|H) ≥g(E(X|H)) a.s.

176
PROBABILITY THEORY
(x) Conditional Chebyshev inequality: For any c > 0 and almost all ω ∈Ωfor
which E(X|H)(ω) < +∞, we have
P (|X −E(X|H)| ≥c|H) (ω) ≤Var(X|H)(ω)/c2 a.s.
(xi) Successive conditioning: If H1 ⊂H2 ⊂A then
E(E(X|H1)|H2) = E(X|H1) a.s.,
and
E(E(X|H2)|H1) = E(X|H1) a.s.,
that is, in successive conditioning we always end up conditioning with respect to
the smallest sub-σ-ﬁeld.
(xii) Suppose that Y is H-measurable and let g : X × X →R. Then
E(g ◦(X, Y)|H)(ω) =
R
X
g(x, Y(ω))QX(dx|H)(ω),
for almost all ω ∈Ω.
Example 4.11 (Conditional expectation given a σ-ﬁeld) For what follows as-
sume that X, Y : (Ω, A, P) →(X, G) are integrable random variables and let H be
a sub-σ-ﬁeld of A.
1. Assume that X is H-measurable. We claim that E(X|H) = X a.s. Indeed, let the
candidate be the random variable Z = X so that Z is obviously H-measurable and
the condition E(ZIH) = E(XIH), ∀H ∈H, is trivially satisﬁed.
2. Assume that X is H-measurable and assume that XY is integrable. Then we
claim that E(XY|H) = XE(Y|H) a.s. Let Z = XE(Y|H) denote the candidate ran-
dom variable and note that since X is H-measurable and E(Y|H) is H-measurable
by deﬁnition, then Z is H-measurable. We need to show that for all H ∈H, we
have
E(ZIH) = E((XY)IH),
or that
EP(XE(Y|H)IH) = EP(XYIH).
(4.32)
By the deﬁnition of E(Y|H) we have
EP(E(Y|H)IH) = EP(YIH).
(4.33)
Consider ﬁrst the case where X = IA, for some set A that is H-measurable. Thus
(4.32) is written as
EP(IAE(Y|H)IH) = EP(IAYIH),
which reduces to (4.33) (by replacing H with H ∩A ∈H) and consequently the
indicator satisﬁes the claim. From linearity of expectation, (4.32) is satisﬁed for any
simple random variable X. For any nonnegative X there exists (by theorem 3.4) an
increasing sequence of simple functions Xn such that |Xn| ≤|X| and Xn ↑X. Since

CONDITIONAL PROBABILITY
177
|XnY| ≤|XY| and XY is integrable, the conditional DCT implies that E(XnY|H)
a.s.
→
E(XY|H) a.s. But E(XnY|H) = XnE(Y|H) a.s. since Xn is a simple random variable
and therefore
E(XY|H) = lim
n→+∞E(XnY|H) = lim
n→+∞XnE(Y|H) = XE(Y|H) a.s.
If X is R-valued we take X = X+ −X−so that linearity of conditional expectation
yields the general case of the claim.
4.3.3
Conditional Independence Given a σ-ﬁeld
The deﬁnition of conditional independence is given ﬁrst, followed by several
results that illustrate the usefulness of conditioning. In particular, there are cases
where conditioning on an appropriate σ-ﬁeld can turn dependence into indepen-
dence.
Deﬁnition 4.19 Conditional independence given a σ-ﬁeld
Suppose that (Ω, A, P) is a probability space and let A1, A2, H be sub-σ-ﬁelds
of A. The σ-ﬁelds A1 and A2 are said to be conditionally independent given H if
P(A1 ∩A2|H) = P(A1|H)P(A2|H) a.s.,
for all A1 ∈A1 and A2 ∈A2.
We prove a basic theorem on conditional independence that can be used to prove
subsequent results.
Theorem 4.21 (Conditional independence of sub-σ-ﬁelds) Consider a proba-
bility space (Ω, A, P) and let H, A1, A2, be sub-σ-ﬁelds of A. Suppose that
A2 ⊆H. Then A1 and A2 are conditionally independent given H.
Proof. If A1 ∈A1 and A2 ∈A2 ⊆H, then P(A1 ∩A2|H) = P(A1|H)IA2 =
P(A1|H)P(A2|H) a.s. The ﬁrst equality follows from example 4.11.2 since P(A1 ∩
A2|H) = E(IA1∩A2|H) = E(IA1IA2|H) and A2 ⊆H implies IA2 is H-measurable.
The second equality holds from example 4.10.1.
Several results regarding conditional independence are presented below.
Remark 4.17 (Conditional independence) In what follows let (Ω, A, P) be a
probability space and let H, V, H1, H2, be sub-σ-ﬁelds of A.
1. Conditional Borel-Cantelli lemma Let {An}+∞
n=1 be a sequence of events condi-
tionally independent given H. Then P(limAn|H) = 1 a.s., if
+∞
P
n=1 P(An|H) = +∞a.s.
and 0 a.s., if
+∞
P
n=1 P(An|H) < +∞a.s. and moreover
P(limAn|H) = P
 (
ω ∈Ω:
+∞
P
n=1 P(An|H)(ω) = +∞
)!
.

178
PROBABILITY THEORY
2. If H1 and H2 are conditionally independent given H then H1 and σ(H2, H) are
conditionally independent given H.
3. Assume that H1 ⊆H, H2 ⊆V and suppose that H and V are independent.
Then H and V are conditionally independent given σ(H1, H2).
4. Suppose that H ⊆H1. Then H1 and H2 are conditionally independent given H
if and only if ∀A ∈H2 we have P(A|H) = P(A|H1) a.s.
5. Let H1 and H2 be independent with H ⊆H1. Then H1 and σ(H, H2) are
conditionally independent given H.
Example 4.12 (Random walk)
Consider a sequence of iid random variables
{Xn}+∞
n=1 on a probability space (Ω, A, P) and common distribution Q. Deﬁne the
sequence of partial sums S n =
nP
i=1 Xi, n = 1, 2, . . . and set S 0 = 0. The random
sequence S = {S n}+∞
n=0 is called a random walk with steps {Xn}+∞
n=1 and step distribu-
tion Q. We claim that for any n the random vector (S 0, S 1, . . . , S n) and the random
sequence (S n, S n+1, . . . ) are conditionally independent given σ(S n), that is, the past
(before time n) and the future (after time n) are conditionally independent given
the present (time n). To see that this is the case, consider remark 4.17.5 and set
H = σ(S n), H1 = σ(X1, X2, . . . , Xn) and H2 = σ(Xn+1, Xn+2, . . . ). Since H ⊆H1
the last remark yields that H1 and σ(H, H2) are conditionally independent given
H. But since (S 0, S 1, . . . , S n) is H1-measurable and (S n, S n+1, . . . ) is σ(H, H2)-
measurable the claim is established. This conditional property is essentially the
idea behind the Markov property and will be treated extensively in Chapters 6 and
7 along with random walks and their applications.
4.4
Summary
In this chapter we have presented a rigorous treatment of probability theory
from a mathematical perspective. Since this is an area of statistics that has great
overlap with mathematics and has been developed for over a century, there is a
plethora of texts on probability and measure theory that the reader can turn to
for additional results and exposition. A good summary of the bibliography (books
and textbooks) regarding probability theory topics can be found in Fristedt and
Gray (1997, Appendix F). The greatest inﬂuence on probability theory is perhaps
the classic text by Kolmogorov (1933, 1956). We will only mention here recent
texts that would be the standard for researchers in statistics or mathematicians
with an appetite for probability theory. Such excellent texts include Feller (1968,
1971), R´enyi (1970), Chung (1974), Fristedt and Gray (1997), Kallenberg (1986,
2002), Dudley (2002), Durrett (2010), Cinlar (2010), Billingsley (2012) and Klenke
(2014).

EXERCISES
179
4.5
Exercises
Probability measures and random variables
Exercise 4.1 Give an example of a probability measure P on a ﬁeld A0 of subsets
of a space Ω.
Exercise 4.2 Prove theorem 4.2.
Exercise 4.3
Let P and Q be two probability measures on the measurable space
(Ω, A). Deﬁne u(P, Q) = dQ/(dP + dQ) and assume that P and Q are absolutely
continuous with respect to the Lebesgue measure. Show that P⊥Q if and only if
R
Ωu(P, Q)dP = 0.
Exercise 4.4 Prove all the statements of lemma 4.1.
Exercise 4.5 Deﬁne the set function P : Ω→[0, 1] as P(A) = card(A)/3n, for all
A ⊆A = 2Ω, where Ω= {(ω1, . . . , ωn) : ωi ∈{0, 1, 2}, i = 1, 2, . . ., n}.
(i) Show that A is a σ-ﬁeld.
(ii) Show that P is a discrete probability measure.
(iii) Find the probability of the events An = {(ω1, . . . , ωn) :
nP
i=1 ωi = 2n}, n = 1, 2, . . .
Exercise 4.6
Assume that P is a ﬁnitely additive probability measure on a ﬁeld
A and assume that for A1, A2, · · · ∈A, if An ↓∅then P(An) ↓0. Show that P is
countably additive.
Exercise 4.7 Show that X : (Ω, A, P) →(R, B1) is a random variable if and only
if for each x ∈R we have {ω : X(ω) ≤x} ∈A.
Exercise 4.8
Let P and Q be two probability measures on the measurable space
(Ω, A) and µ a σ-ﬁnite measure on the same measurable space, such that P ≪µ
and Q ≪µ. Let p =
hdP
dµ
i
and q =
hdQ
dµ
i
denote the Radon-Nikodym derivatives and
deﬁne the Kullback-Leibler divergence by
IKL(P, Q) =
R
X
ln
 dP
dQ
!
dP =
R
X
ln
 p(x)
q(x)
!
p(x)µ(dx).
Show that IKL(P, Q) ≥0 with equality if and only if P = Q.
Exercise 4.9 (Bonferroni’s inequality) Let A1, . . . , An be events in a probability
space (Ω, A, P). Show that
P
 nS
i=1 Ai
!
≥
nP
i=1 P(Ai) −
nP
i, j=1,i<j P(Ai ∩A j).
Exercise 4.10
(Kounias’ inequality) Show that for any events A1, . . . , An in a
probability space (Ω, A, P) we have
P
 nS
i=1 Ai
!
≤min
j
( nP
i=1 P(Ai) −
nP
i=1,i,j P(Ai ∩A j)
)
.
Expectation
Exercise 4.11 Assume that X ∼Cauchy(0, 1). Show that E(Xn) does not exist for
any n ≥1.

180
PROBABILITY THEORY
Exercise 4.12
Consider X ∼Gamma(a, b) and ﬁnd E(Xc) along with conditions
on c under which this expectation exists.
Exercise 4.13 Let X ≥0 be some random variable with density f . Show that
E(Xc) =
+∞
R
0
cxc−1P(X > x)dx,
for all c ≥1 for which the expectation is ﬁnite.
Exercise 4.14 Give an example of a random variable X with E

1
X

,
1
E(X) and an
example of a random variable Y with E
 1
Y

=
1
E(Y).
Exercise 4.15 Let F be the distribution function of an R+-valued random variable
X and let ϕ : R →R be monotonic and left-continuous. Then the expectation of
ϕ ◦X exists as a member of R and
E(ϕ ◦X) = ϕ(0) +
+∞
R
0
[1 −F(x)]dϕ(x).
Exercise 4.16 Show that remark 4.8.6 holds.
Exercise 4.17
Let {Xn}+∞
n=1 and {Yn}+∞
n=1 be sequences of random variables. Show
that
Cov
 nP
i=1 Xi,
mP
j=1 Yj
!
=
nP
i=1
mP
j=1 Cov(Xi, Yj),
and therefore
Var
 nP
i=1 Xi
!
=
nP
i=1
nP
j=1Cov(Xi, X j).
Exercise 4.18
Assume that X ∼N(0, 1). Find the expected value of the random
variable |X|2n+1, n = 0, 1, 2, . . .
Exercise 4.19 Give an example of a real-valued random variable with the property
that all non-central moments are ﬁnite but the mgf is deﬁned only at 0.
Independence
Exercise 4.20 Show that any two events A and B of positive probability cannot be
disjoint and independent simultaneously.
Exercise 4.21 Use the Fubini theorem to show that for X and Y independent real-
valued random variables on (Ω, A, P) we have
(i) P(X ∈B −y) is a B1-measurable function of y for all B ∈B1 and
(ii) P(X + Y ∈B) =
R
Ω
P(X ∈B −y)P(dy).
Exercise 4.22 Prove all statements of remark 4.5.
Exercise 4.23 Let A1 and A2 be independent σ-ﬁelds on (Ω, A, P). Show that if
a set A is both in A1 and A2 then P(A) = 0 or 1.
Exercise 4.24
Show that for any independent random variables X and Y deﬁned
on the same space, Var(X + Y) = Var(X) + Var(Y).
Exercise 4.25 Let X and Y be two random variables. Show that the integrability of

EXERCISES
181
X+Y does not imply that of X and Y separately. Show that it does imply integrability
if X and Y are independent.
Exercise 4.26 Give an example of two dependent random variables with Pearson
correlation equal to 0.
Exercise 4.27
Consider two random variables X and Y deﬁned on the same
measurable space (Ω, A). Give an example of two probability measures P1 and
P2 where X is independent of Y in (Ω, A, P1) but they are not independent in
(Ω, A, P2).
Exercise 4.28 (Mutual information) Let X and Y be two discrete random vari-
ables with joint pmf fX,Y and marginal pmfs fX and fY. Show that
(i) E(ln fX(x)) ≥ln E(fY(x)), and
(ii) the mutual information deﬁned by
I = E
 
ln
 fX,Y(X, Y)
fX(X)fY(Y)
!!
,
is such that I ≥0 with equality if and only X and Y are independent.
Characteristic functions
Exercise 4.29
Show that the cf of a Np(0, Σ) distributed random vector is
exp
n
−1
2tTΣt
o
.
Exercise 4.30
Let X, Y be independent N(0, 1) random variables. Find the cf of
the random vector (X −Y, X + Y).
Exercise 4.31 Let X have a Laplace density f (x|λ) = λ
2e−λ|x|, x ∈R, λ > 0. Show
that the cf of X is ϕX(t) =
λ2
λ2+t2 .
Exercise 4.32 Assume that z1, . . . , zn, w1, . . . , wn are complex numbers of modulus
|.| at most 1. Show that

nQ
i=1 zi −
nQ
i=1 wi
 ≤
nP
i=1 |zi −wi|.
(4.34)
Exercise 4.33 Show that for any x ∈R we have
eitx −
 
1 + itx −1
2t2x2
! ≤min{|tx|2, |tx|3},
and as a result for a random variable X with cf ϕX(t), E(X) = 0 and σ2 = E(X2) the
following holds ϕX(t) −
 
1 −1
2t2σ2
! ≤E
h
min{|tX|2, |tX|3}
i
< ∞.
(4.35)
Exercise 4.34 If ϕX(t) is a cf then show that ec(ϕX(t)−1) is a cf, for c ≥0.
Conditional probability, expectation and independence
For this portion of the exercises assume the following: let (Ω, A, P) a probabil-
ity space and assume that A, Ai, i = 1, 2, . . . , are A-sets and let H be a sub-σ-ﬁeld
of A.
Exercise 4.35 Prove all the statements of theorem 4.19.

182
PROBABILITY THEORY
Exercise 4.36 Show that if X is R-valued then
E(X|H) = E(X+|H) −E(X−|H) a.s.,
assuming that the equality holds a.s. on the set where either side is deﬁned.
Exercise 4.37 Prove all the statements of theorem 4.20.
Exercise 4.38 For a nonnegative random variable X we can show that
E(X|H) =
+∞
R
0
P(X > t|H)dt a.s.
Exercise 4.39 Consider the random vector (X1, X2, X3) with joint density
P(X1 = x1, X2 = x2, , X3 = x3) = p3qx3−3,
where 0 < p < 1, p + q = 1, x1 = 1, . . . , x2 −1, x2 = 2, . . . , x3 −1, x3 = 3, 4, . . .
and show that
(i) E(X3|X1, X2) = X2 + 1/p a.s. and
(ii) E(aX3|X1, X2) = paX2+1/(1 −qa) a.s., for 0 ≤a ≤1.
Exercise 4.40
For any integers n, m
≥
1 and discrete random variables
Y, X1, X2, . . . . we have
E[E(Y|X1, . . . , Xn+m)|X1, . . . , Xn] = E(Y|X1, . . . , Xn) a.s.
Exercise 4.41 Consider the random vector X ∼N2(0, I2). Find the density of the
random variable X1|X2 > 0 and E(X1|X2 > 0).
Exercise 4.42 Assume that X ∼Exp(θ). Show that E(X|X > t) = t + 1
θ.
Exercise 4.43
Assume that

Xp×1
Yq×1
∼Np+q
µ =

µx
µy
, Σ =

Σ11
Σ12
Σ21
Σ22

.
Show that X|Y = y ∼Np

µx + Σ12Σ−1
22(y −µy

, Σ11 −Σ12Σ−1
22Σ21).
Exercise 4.44
Let X and Y have a bivariate normal density with zero means,
variances σ2
X, σ2
Y and correlation ρ. Show that
(i) E(X|X + Y = w) =
σ2
X+ρσXσY
σ2
X+2ρσXσY+σ2
Y w, and
(ii) Var(X|X + Y = w) =
σ2
Xσ2
Y(1−ρ2)
σ2
X+2ρσXσY+σ2
Y .

Chapter 5
Convergence of Random Objects
5.1
Introduction
A sequence of random objects {Xn}+∞
n=1 (also denoted by X = (X1, X2, . . . ) or
{Xn, n ≥1}) is simply a collection of random objects Xn often deﬁned on the same
probability space (Ω, A, P) and taking values in the same space (X, G) with cor-
responding distributions {Qn}+∞
n=1, under independence, or simply Q, if X is an iid
sequence, i.e., Xn ∼Q, for all n = 1, 2, . . .. Such sequences are called random or
stochastic sequences and will be investigated extensively in Chapter 6 under the
assumption that the distribution of Xn+1 observed at time n + 1 given the present
value Xn is independent of all past values.
The iid case has been discussed extensively in previous chapters. In particular,
the rudimentary deﬁnitions and results of section 2.3.4 are easily incorporated in
the theory developed in Chapter 4 for any standard random object (such as random
variables, vectors and matrices) and will be taken as given for random objects that
live in (Rp, Bp). In addition, showing a.s. convergence can be accomplished using
Kolmogorov’s 0-1 law (theorem 4.12) and the Borel-Cantelli lemmas (lemma 4.1).
In contrast, extending the weak and strong laws of large numbers is not a trivial
task for more complicated random objects, like point processes and random sets.
However, there exist versions of these important theorems and typically for special
cases; for example, a SLLN can be obtained for random compact and convex sets
using the concept of the support function. We study these methods in the TMSO-
PPRS text.
In this chapter we study the asymptotic behavior of a sequence of real-valued
random variables and its partial sums, as well as the limiting behavior of the se-
quence of distributions {Qn}+∞
n=1.
5.2
Existence of Independent Sequences of Random Variables
The main assumption up to this point has been that {Xn}+∞
n=1 are independent
and therefore we can deﬁne what we mean by an independent sequence of random
objects (or a random sample of objects). A natural question that arises is whether
183

184
CONVERGENCE OF RANDOM OBJECTS
or not such a sequence exists for a given sequence of probability measures and we
address this ﬁrst in the following existence theorem about independent sequences of
random variables. Note that this result is a special case of Kolmogorov’s existence
theorem (see section 7.3.3) or theorem 4.8 when the probability space is the same.
Theorem 5.1 (Existence of iid sequence) If {Qn}+∞
n=1 is a sequence of probability
measures on (R, B1), then there exists an independent sequence {Xn}+∞
n=1 of ran-
dom variables on some probability space (Ω, A, P) such that Xn has distribution
Qn.
Requiring the same probability space or target space is not necessary but it is
an assumption made frequently since it is painstaking (in some cases impossible)
to study the behavior of the sequence without it.
5.3
Limiting Behavior of Sequences of Random Variables
Consider a sequence of random vectors {Xn}+∞
n=1, and recall the convergence con-
cepts introduced in Chapter 2. Having acquired the rigorous probabilistic frame-
work of Chapter 4, we are now able to discuss convergence of random sequences in
probability, in distribution and a.s. convergence, in a rigorous framework. We begin
by presenting two of the most important results in asymptotic theory; the Slutsky
and Cram´er theorems, and their extensions. Moreover, we will show under which
conditions we have consistency for an MLE estimator.
5.3.1
Slutsky and Cram´er Theorems
The Slutsky theorems are the standard tool we use to show convergence in prob-
ability and in law for random sequences and their transformations. The proof is
requested as an exercise.
Theorem 5.2 (Slutsky) Let C(f) denote the set of continuity points of f : Rd →
Rk.
(i) If Xn ∈Rd and Xn
w→X and if f is such that P(X ∈C(f)) = 1, then f(Xn)
w→
f(X).
(ii) If Xn
w→X and (Xn −Yn)
p→0, then Yn
w→X.
(iii) If Xn ∈Rd and Yn ∈Rk, Xn
w→X and Yn
w→c, some constant, then

Xn
Yn

w→

X
c
.
(iv) If Xn ∈Rd and Xn
p→X and if f is such that P(X ∈C(f)) = 1, then
f(Xn)
p→f(X).

LIMITING BEHAVIOR OF SEQUENCES OF RANDOM VARIABLES
185
(v) If Xn
p→X and (Xn −Yn)
p→0, then Yn
p→X.
(vi) If Xn ∈Rd and Yn ∈Rk, Xn
p→X and Yn
p→Y, then

Xn
Yn

p→

X
Y
.
Note that we can replace the mode of convergence
p→above with
a.s.
→and the
results of Slutsky theorem still hold.
Example 5.1 (Asymptotic normality of the t-test statistic)
Assume that Xi
iid∼
N(µ, σ2), i = 1, 2, . . . , n, µ ∈R, σ > 0. From the WLLN and exercise 5.4, we have
Xn =
1
n
nP
i=1 Xi
w→µ, and the usual CLT (exercise 2.82) yields √n

Xn −µ

/σ
w→
N(0, 1). Moreover, the WLLN on the sequence {X2
i } yields 1
n
nP
i=1 X2
i
w→E

X2
.
Now from part (iv) of theorem 5.2, X
2
n
w→µ2, so that parts (iii) and (iv) yield
S 2
n = 1
n
nP
i=1 X2
i −X
2
n
w→E

X2
−µ2 = σ2. Finally, another appeal to parts (iii) and
(iv) gives √n

Xn −µ

/S n
w→N(0, 1), and therefore the asymptotic distribution of
the t-test statistic is
tn−1 =
√
n −1

Xn −µ

/S n
w→N(0, 1),
provided that S n , 0.
A great consequence of the Slutsky theorems involves the asymptotic normality
of functions of the sample moments.
Theorem 5.3 (Cram´er) Let g be a mapping g : Rd →Rk such that ∇g is contin-
uous in a neighborhood of µ ∈Rd. If Xn is a sequence of d-dimensional random
vectors such that √n(Xn −µ)
w→X, then √n(g(Xn) −g(µ))
w→(∇g(µ))X. In
particular, if √n(Xn −µ)
w→Nd(0, Σ), with Σ the covariance matrix, then
√n(g(Xn) −g(µ))
w→Nd(0, (∇g(µ))Σ(∇g(µ))T).
The latter result (under normality) is known as the Delta method.
Example 5.2 (Delta method) Assume that Xi
iid∼N(µ, σ2), i = 1, 2, . . ., n, so that
the usual CLT yields √n

Xn −µ

w→N(0, σ2), where Xn = 1
n
nP
i=1 Xi. Consider the
asymptotic distribution of 1/Xn. Let g(x) = 1/x, and note that g′(x) = −1/x2, and
therefore, g′(µ) = −1/µ2. An appeal to Cram´er’s theorem gives
√n
 1
Xn
−1
µ
!
w→N
 
0, σ2
µ4
!
,
provided that µ , 0.

186
CONVERGENCE OF RANDOM OBJECTS
5.3.2
Consistency of the MLE
Now we turn to investigating MLE consistency. The following theorem sum-
marizes all the requirements in order to prove consistency of an MLE.
Theorem 5.4 (MLE consistency) Assume that Xi
iid∼f (x|θ), i = 1, 2, . . . , n.
Let θ0 denote the true parameter value of θ ∈Θ. Assume that the following
conditions hold:
(C1) Θ is compact,
(C2) f (x|θ) is upper semicontinuous in θ for all x,
(C3) there exists a function K(x) such that Eθ0|K(X)| < ∞and
ln f (x|θ)
f (x|θ0) ≤K(x),
for all θ and x,
(C4) for all θ ∈Θ and suﬃciently small ε > 0, sup
|θ′−θ|<ε
f (x|θ′) is continuous in x,
and
(C5) f (x|θ) = f (x|θ0) ⇒θ = θ0.
Then any sequence of MLEs bθn is strongly consistent for θ.
We present an example where the MLE is not consistent.
Example 5.3 (The MLE is not always consistent) This example is due to Ney-
man and Scott (1948). Assume that Xij ∼N(µi, σ2), i = 1, 2, . . ., n, j = 1, 2, . . . , d,
are samples of size d from n independent normal populations, with common un-
known variance and diﬀerent unknown means. Let x denote all the samples, so that
the likelihood function is given by
L(θ|x)
=
nQ
i=1
dQ
j=1(2πσ2)−1/2 exp
(
−1
2σ2(xij −µi)2
)
=
(2πσ2)−nd/2 exp
(
−1
2σ2
nP
i=1
dP
j=1(xij −µi)2
)
,
and therefore, log-likelihood becomes
l(θ|x) = ln L(θ|x) = −nd
2 ln(2π) −nd
2 ln(σ2) −
1
2σ2
nP
i=1
dP
j=1(xij −µi)2.
It is straightforward to see that the MLE of µi is given by bµi =
1
d
dP
j=1 xij, i =
1, 2, . . ., n, whereas the MLE of σ2 is given by bσ2 =
1
nd
nP
i=1
dP
j=1(xij −bµi)2. Note that
bσ2 is not an unbiased estimator of σ2, i.e., letting S 2
i =
1
d
dP
j=1(xij −bµi)2, we have
E(S 2
i ) = d−1
d σ2. Since for all i = 1, 2, . . ., n, . . . , the sample variance S 2
i follows the

LIMITING BEHAVIOR OF PROBABILITY MEASURES
187
same law, the arithmetic mean bσ2 = 1
n
nP
i=1 S 2
i , tends a.s. (via an appeal to the SLLN
on the sequence {S 2
i }n
i=1) to the common expectation d−1
d σ2, and therefore the MLE
bσ2 is not consistent as n →∞and d is kept ﬁxed.
5.4
Limiting Behavior of Probability Measures
Important theorems such as the MCT and Lebesgue DCT allow us to pass the
limit, as n →∞, under the integral sign when we integrate sequences of functions
(random variables) Xn with respect to a probability measure P that does not depend
on n. More precisely, we have obtained results to help us write
E(Xn) =
R
XndP →E(X) =
R
XdP,
(5.1)
when Xn converges (in some sense, e.g., a.s. convergence) to X, as n →∞. Nat-
urally, one wonders if there are similar results when n is aﬀecting the probability
measure over which we compute the integral and not the function we integrate, that
is,
R
f dPn →
R
f dP,
(5.2)
for some function f when Pn converges (in some sense, e.g., when Pn(B) →P(B),
∀B) to P, as n →∞. Note that since a measure Pn is uniquely determined by its
corresponding distribution function Fn and in view of equation (3.17) we simplify
the problem and consider the distribution function Fn instead of the probability
measure Pn, that is, we investigate the limiting behavior of the integral
R
f dFn =
R
f dPn.
5.4.1
Integrating Probability Measures to the Limit
We begin by presenting some classic results involving equation (5.1) and weak
convergence followed by an investigation of equation (5.2).
Theorem 5.5 (Helly-Bray) Assume that Xn
w→X or equivalently that Fn
w→F
where Fn and F are the distribution functions of Xn and X, respectively, and
let C(F) = {x : F is continuous at x} denote the set of continuity points of F.
Then E(g(Xn)) →E(g(X)), as n →∞, for all continuous functions g that vanish
outside [a, b].
Proof. We treat the univariate case. Since g is a continuous function vanishing
outside [a, b], then g is uniformly continuous (Appendix Section A.5.3), that is,
∀ε > 0, ∃δ > 0 : |x −y| < δ =⇒|g(x) −g(y)| < ε. Choose a = x0 < x1 < x2 <
· · · < xk = b, with xi ∈C(F) and some δ (e.g., b −a) such that |xi −xi−1| < δ
so that |g(x) −g(xi)| ≤ε, for all x ∈(xi−1, xi], i = 1, 2, . . ., k. Therefore we can
write |g(x) −g∗(x)| ≤ε, for all x ∈[a, b], where g∗(x) =
kP
i=1 g(xi)I(xi−1,xi](x) and since

188
CONVERGENCE OF RANDOM OBJECTS
Fn
w→F it follows that
E(g∗(Xn))
=
kP
i=1 g(xi)E[I(xi−1,xi](Xn)] =
kP
i=1 g(xi)[Fn(xi) −Fn(xi−1)]
→
kP
i=1 g(xi)[F(xi) −F(xi−1)] =
kP
i=1 g(xi)E[I(xi−1,xi](X)] = E(g∗(X)),
as n →∞. Moreover we have
|E(g(Xn)) −E(g(X))|
=
|E g(Xn) −E g∗(Xn) + E g∗(Xn) −
E g∗(X) + E g∗(X) −E g(X) |
≤
E g(Xn) −E g∗(Xn) +
E g∗(Xn) −E g∗(X) +
E g∗(X) −E g(X)
≤
E g(Xn) −g∗(Xn) +
E g∗(Xn) −g∗(X) +
E g∗(X) −g(X)
≤
E |g(Xn) −g∗(Xn)| + E |g∗(Xn) −g∗(X)| +
E |g∗(X) −g(X)|
≤
2ε + E |g∗(Xn) −g∗(X)| →2ε.
Since this holds for all ε > 0 we must have E(g(Xn)) →E(g(X)).
The converse of the latter theorem is also true and there is a more general ver-
sion as we see below.
Remark 5.1 (Helly-Bray) The following conditions are equivalent:
(i) Xn
w→X,
(ii) E(g(Xn)) →E(g(X)), as n →∞, for all continuous functions g that vanish
outside a compact set,
(iii) E(g(Xn)) →E(g(X)), as n →∞, for all continuous bounded functions g, and
(iv) E(g(Xn)) →E(g(X)), as n →∞, for all bounded measurable functions g such
that P(X ∈C(g)) = 1.
All of the directions (i) =⇒(ii) or (iii) or (iv) are known as the Helly-Bray
theorem. Furthermore, note that the dimension of Xn and X has no eﬀect on the
statements, although the proofs would need to be slightly modiﬁed (see for example
Ferguson, 1996, theorem 3).
Example 5.4 (Counterexamples to Helly-Bray theorem) Let us discuss the ne-
cessity of some of these conditions through counterexamples.
1. Let g(x) = x and take Xn = n, w.p. 1
n and Xn = 0, w.p. n−1
n . Then Xn
w→X = 0 but
E(g(Xn)) = n 1
n = 1 ↛E(g(0)) = 0. Therefore one cannot remove the boundedness
requirement for g in (iii) and (iv).
2. Let g(x) = I(x > 0), x ≥0 and assume that P(Xn = 1
n) = 1. Then Xn
w→X = 0
but E(g(Xn)) = 1 ↛E(g(0)) = 0 and thus the continuity requirement from (i) and
(ii) cannot be removed. Similarly, in (iv) it is required that P(X ∈C(g)) = 1.

LIMITING BEHAVIOR OF PROBABILITY MEASURES
189
We deﬁne weak convergence in the space of probability measures via conver-
gence in the space of distribution functions.
Deﬁnition 5.1 Weak convergence of probability measures
Let Fn, F be the cdfs corresponding to the probability measures Pn and P. We say
that the sequence of probability measures Pn converges weakly to P if and only if
Fn
w→F and write Pn
w→P (recall deﬁnition 2.3.3).
The following theorem connects weak and a.s. convergence.
Theorem 5.6 (Skorohod representation) Suppose that Qn and Q are probability
measures on (R, B1) and that Qn
w→Q. Then there exist random variables Xn and
X on a common probability space (Ω, A, P) such that Xn has distribution Qn, X
has distribution Q and Xn
a.s.
→X as n →∞.
Proof. The construction is related to that of theorem 4.6. Take Ω= (0, 1),
A = B((0, 1)), the Borel sets of (0, 1) and let P = µ1, the Lebesgue measure on
(0, 1) (i.e., the uniform distribution). Consider the distribution functions Fn and F
corresponding to Qn and Q and set Xn(ω) = inf{x : Fn(x) ≥ω} and X(ω) = inf{x :
F(x) ≥ω}. We notice that Fn(x) ≥ω if and only if Xn(ω) ≤x, owing to the fact
that F is nondecreasing and {x : Fn(x) ≥ω} = [Xn(ω), +∞); the inﬁmum in the
deﬁnition is there in order to handle the case where F is the same for many x.
Therefore we can write
P(Xn ≤x) = P({ω : Xn(ω) ≤x}) = P({ω : ω ≤Fn(x)}) = Fn(x),
so that Xn has distribution function Fn and therefore distribution Qn. Similarly, X
has distribution Q and it remains to show that Xn
a.s.
→X.
Let ε > 0 and ﬁx ω ∈(0, 1). Select an x such that X(ω) −ε < x < X(ω)
and Q({x}) = 0 which leads to F(x) < ω. Now Fn(x) →F(x), as n →∞, implies
that for n large enough, Fn(x) < ω and hence X(ω) −ε < x < Xn(ω). Letting
n →∞and ε ↓0 we conclude that lim infn Xn(ω) ≥X(ω), for any ω ∈(0, 1). Next
choose ω′ satisfying 0 < ω < ω′ < 1 and ε > 0. Select again a point y such that
X(ω′) < y < X(ω′)+ε, with Q({y}) = 0 and note that ω < ω′ ≤F(X(ω′)) ≤F(y), so
that for n large enough, ω ≤Fn(y) and hence Xn(ω) ≤y < X(ω′)+ε. Letting n →∞
and ε ↓0 we have that lim supn Xn(ω) ≤X(ω′), for ω, ω′ with 0 < ω < ω′ < 1.
Let ω′ ↓ω with ω a continuity point of X so that X(ω′) ↓X(ω). Consequently, we
have the other direction, namely, lim supn Xn(ω) ≤X(ω) and hence Xn(ω) →X(ω),
as n →∞, provided that ω ∈(0, 1) is a continuity point of X.
Since X is nondecreasing on (0, 1) the set D ⊆(0, 1) of discontinuities of X
is countable so that P(D) = 0. Finally, we may write
P(Xn →X, as n →∞) = P({ω : Xn(ω) →X(ω), as n →∞}) = 1,
and the claim is established.

190
CONVERGENCE OF RANDOM OBJECTS
Next we discuss a generalization of the Skorohod’s theorem to Polish spaces.
Remark 5.2 (Generalization to Polish spaces) The original version of Skorohod’s
theorem (Skorohod, 1956) is more general and can be applied to any complete,
separable metric space (i.e., a Polish space), not just the real line R. In addition,
deﬁnition 5.1 can be reformulated as follows: let (X, ρ) be a Polish space and deﬁne
{Pn}+∞
n=1 as a sequence of probability measures on (X, B(X)) and let P be another
probability measure on the same space. Then Pn
w→P if and only if
lim
n→∞
R
X
g(x)Pn(dx) →
R
X
g(x)P(dx),
(5.3)
for all bounded, continuous real-valued functions f on X. Obviously, X = R is a
Polish space so that this general deﬁnition contains deﬁnition 5.1 as a special case,
provided that (5.3) holds.
The next theorem shows that (5.3) holds over intervals [a, b] ⊂R.
Theorem 5.7 (Portmanteau) If {Fn}+∞
n=1 is a sequence of distribution functions
with Fn
w→F and a, b ∈C(F), then for every real-valued, uniformly continuous
function g on [a, b] we have
lim
n→∞
bR
a
gdFn →
bR
a
gdF.
Proof. For ε > 0 choose by uniform continuity a δ > 0 so that | f (x)−g(y)| < ε,
for |x −y| < δ, x, y ∈[a, b]. Select xi ∈C(F), i = 2, . . . , k, such that a = x1 < x2 <
· · · < xk+1 = b and max
1≤i≤k|xi+1 −xi| < δ. Then we can write
Hn =
bR
a
gdFn −
bR
a
gdF =
kP
i=1

xi+1
R
xi
g(x)dFn(x) −
xi+1
R
xi
g(x)dF(x)
,
and adding and subtracting
xi+1
R
xi
g(xi)dFn(x) +
xi+1
R
xi
g(xi)dF(x), above we have
Hn
=
kP
i=1

xi+1
R
xi
g(x)dFn(x) −
xi+1
R
xi
g(xi)dFn(x)

+
kP
i=1

xi+1
R
xi
g(xi)dFn(x) −
xi+1
R
xi
g(xi)dF(x)

+
kP
i=1

xi+1
R
xi
g(xi)dF(x) −
xi+1
R
xi
g(x)dF(x)
,

LIMITING BEHAVIOR OF PROBABILITY MEASURES
191
which leads to
Hn
=
kP
i=1

xi+1
R
xi
[g(x) −g(xi)]dFn(x)
+ g(xi)[Fn(xi+1) −Fn(xi) −F(xi+1) + F(xi)]
+
kP
i=1

xi+1
R
xi
[g(xi) −g(x)]dF(x)
.
Moreover, we can bound |Hn| from above by
|Hn|
≤
kP
i=1

xi+1
R
xi
|g(x) −g(xi)| dFn(x)
+ g(xi) |Fn(xi+1) −Fn(xi) −F(xi+1) + F(xi)|
+
kP
i=1

xi+1
R
xi
|g(xi) −g(x)| dF(x)
.
First we notice that |g(x) −g(xi)| < ε so that we can write
|Hn|
≤
ε
kP
i=1 {Fn(xi+1) −Fn(xi)} + g(xi) |Fn(xi+1) −Fn(xi) −F(xi+1) + F(xi)|
+ε
kP
i=1 {F(xi+1) −F(xi)} ,
and expanding the two telescoping sums yields
|Hn|
≤
ε[Fn(xk+1) −Fn(x1) + F(xk+1) −F(x1)]
+g(xi) |Fn(xi+1) −Fn(xi) −F(xi+1) + F(xi)| ,
with both summands converging to 0 as n →∞, since Fn(xi) →F(xi), for xi ∈
C(F). Thus |Hn| →0, as n →∞and the result is established.
There are many versions of the Portmanteau theorem that involve diﬀerent prop-
erties of the function g. For more details see Feller (1971), Fristedt and Gray (1997),
and Billingsley (2012).
5.4.2
Compactness of the Space of Distribution Functions
We collect some essential results on establishing a compactness property for
the space of distribution functions. Versions of these theorems and their proofs can
be found in Billingsley (2012, p. 359). The ﬁrst theorem can be used to prove many
of the results that follow in this section.
Theorem 5.8 (Helly selection) For every sequence {Fn}+∞
n=1 of distribution func-
tions there exists a subsequence {Fnk}+∞
k=1 and a nondecreasing, right continuous
function F such that lim
k→∞Fnk(x) = F(x) at continuity points of F.
Note that even though the {Fn}+∞
n=1 in the Helly selection theorem are cdfs, their
limit need not be a cdf in general. For example, if Fn has a unit jump at n, then
the limit is F(x) = 0, which is not a valid distribution function. In terms of the
corresponding probability measures {Pn}+∞
n=1 of {Fn}+∞
n=1, the problem is described by

192
CONVERGENCE OF RANDOM OBJECTS
a Pn that has unit mass at n and the mass of Pn “escapes to inﬁnity” as n →+∞. To
avoid such cases we introduce the concept of tightness of probability measures.
Deﬁnition 5.2 Tightness
A sequence of probability measures {Qn}+∞
n=1 on (R, B1) is said to be uniformly
tight if for every ε > 0 there exists a ﬁnite interval (a, b] such that Qn((a, b]) >
1 −ε, for all n. In terms of the corresponding distribution functions Fn and F,
the condition is that for all ε > 0 there exist x and y such that Fn(x) < ε and
Fn(y) > 1 −ε, for all n.
The following theorem connects tightness and weak convergence of subse-
quences of probability measures.
Theorem 5.9 (Tightness and weak convergence) Tightness is a necessary and
suﬃcient condition that for every subsequence of probability measures {Qnk}+∞
k=1
there exists a sub-subsequence {Qnkj}+∞
j=1 and a probability measure Q such that
Qnkj
w→Q, as j →∞.
Based on the last two theorems, we can prove the following result that connects
weak convergence and the classic approach to deﬁning compactness in standard
Polish spaces like Rp, i.e., a space is compact if every convergent sequence of
elements of a space has a convergent subsequence in the space. In our case we
work in the space of all probability measures.
Theorem 5.10 (Weak convergence and subsequences) A sequence of proba-
bility measures {Qn}+∞
n=1 converges weakly to a probability measure Q if and only
if every subsequence {Qnk}+∞
k=1 has a subsequence {Qnkj}+∞
j=1 converging weakly to
Q.
We end this section by collecting some basic results involving weak conver-
gence and their proofs are requested as exercises.
Remark 5.3 (Weak convergence results) Let Fn and F be distribution functions
in R and let Qn and Q denote the corresponding distributions.
1. Fn
w→F if and only if there is a dense subset D of R such that for every x ∈D,
Fn(x) →F(x), as n →∞.
2. If G is a distribution function in R with Fn
w→G and if Fn
w→F then F = G.
3. Suppose that Qn, Q have densities fn and f with respect to a common σ-ﬁnite
measure µ. If fn →f a.e. [µ] then Qn
w→Q.

LIMITING BEHAVIOR OF PROBABILITY MEASURES
193
4. If {Qn}+∞
n=1 is a tight sequence of probability measures on a Polish space and if
each subsequence converges weakly to Q then Qn
w→Q.
5. Tightness in Polish spaces Tightness can be deﬁned for Polish spaces as fol-
lows. A distribution Q on a Polish space X is tight if ∀ε > 0, there exists a compact
subset K ⊂X such that Q(Kc) < ε. A family of distributions Q is uniformly tight if
∀ε > 0, there exists a compact subset K ⊂X such that Qn(Kc) < ε, for all Q ∈Q.
6. Relatively sequential compact A family Q of distributions deﬁned on a Polish
space is relatively sequentially compact if every sequence {Qn}+∞
n=1, with Qn ∈Q,
has a convergent subsequence.
7. Let {Qn}+∞
n=1 be a relatively sequentially compact sequence of distributions de-
ﬁned on a Polish space and assume that every convergent subsequence has the same
limiting probability measure Q. Then Qn
w→Q.
8. Prohorov theorem A family of probability measures deﬁned on a Polish space
is relatively sequentially compact if and only if it is uniformly tight.
5.4.3
Weak Convergence via Non-Central Moments
Next we connect convergence in non-central moments with weak convergence.
First we discuss when a distribution function F is uniquely determined by its mo-
ments, since knowing all non-central moments ak =
R
R
xkdF(x), k = 1, 2, . . ., of a
random variable with cdf F, does not necessarily imply that F is uniquely deter-
mined by {ak}+∞
k=1.
Necessary and suﬃcient conditions for F to be uniquely determined by {ak}+∞
k=1,
can be found in Lin (2017), and the references therein. However, some of these
conditions are not easy to verify in general. For example, Carleman’s condition
requires
+∞
P
k=1
 1
a2k
 1
2k = +∞, whereas, Cram´er’s condition requires that the mgf exists
and is ﬁnite, i.e., E(etX) < ∞, for all t ∈[−c, c], for some constant c > 0 (see Lin,
2017, p. 4). For what follows assume that F is uniquely determined by its moments.
The usefulness of convergence in all moments is illustrated in the next theorem.
Showing weak convergence by proving convergence of moments is referred to as
the “method of moments,” which should not be confused with the method of ﬁnding
point estimators, which we saw in Chapter 2, remark 2.4.1. For a proof see Chung
(1974, p. 103).
Theorem 5.11 (Fr´echet-Shohat) If {Fn}+∞
n=1 is a sequence of distribution func-
tions whose moments are ﬁnite and such that qn,k =
R
R
xkdFn(x) →ak < +∞, as

194
CONVERGENCE OF RANDOM OBJECTS
n →∞, k = 1, 2, . . ., where {ak}+∞
k=1 are the non-central moments of a uniquely
determined distribution function F, then Fn
w→F.
5.5
Random Series
Random series appear almost exclusively in statistics, in particular, in the form
of point estimators of parameters (e.g., sample averages). Therefore, it is important
to consider the distribution of a partial sum (or the average) and study its behavior
as the sample size increases.
5.5.1
Convolutions
Distributions of partial sums are known as convolutions. We begin with the
general deﬁnition under independence.
Deﬁnition 5.3 Convolution
Let X1, X2, . . . , Xn be independent random objects taking values on the same
space with distributions Q1, Q2, . . . , Qn, respectively. Then the distribution of the
random object S n = X1 + X2 + · · · + Xn is called a convolution and is denoted by
Q1 ∗Q2 ∗· · · ∗Qn. If Qi = Q, i = 1, 2, . . ., n, the distribution of S n is called the
n-fold convolution of Q with itself, denoted by Q∗n.
Example 5.5 (Random convolutions)
Let X1, X2, . . . , be independent random
variables with a common distribution Q. Let N be a random variable independent
of the Xi and taking values in Z+
0 = {0, 1, . . .} with positive probabilities pi, i ∈Z+
0.
We are interested in the random variable S N = X1 + X2 + · · · + XN. The conditional
distribution of S N given that N = n is Q∗n and hence the distribution of S N is given
by the inﬁnite mixture distribution
+∞
P
n=0 pnQ∗n.
The following remark presents some essential results that connect probability
distributions, convolutions and weak convergence.
Remark 5.4 (Convolution results) The addition symbol “+” in the deﬁnition of
S n needs to be deﬁned appropriately and depends on the target space of the random
objects. For example, in Rp it is standard vector addition but in the space K of
compact subsets of Rp we use Minkowski addition “⊕” so that
S n = X1 ⊕X2 ⊕· · · ⊕Xn = {x1 + x2 + · · · + xn : xi ∈Xi, ∀i},
where the random objects Xi are random sets in K. We collect some remarks on
convolutions below.
1. Consider the case of two independent random vectors X1 and X2 deﬁned on
(Ω, A, P) with distributions Q1, Q2 and taking values in (R j, B j) and (Rk, Bk), re-
spectively, so that the random vector (X1, X2) takes values in (R j+k, B j+k) and has

RANDOM SERIES
195
distribution Q1 × Q2. Using the Fubini theorem we can write
(Q1 × Q2)(B) =
R
Rj
Q2(Bx1)Q1(dx1),
for all Borel sets B in B j+k, with Bx1 = {x2 : (x1, x2) ∈B} ∈Rk, the x1-section of B,
for all x1 ∈R j. Replacing B with (A × Rk) ∩B in the latter equation for A ∈B j and
B ∈B j+k, we have
(Q1 × Q2)((A × Rk) ∩B) =
R
A
Q2(Bx1)Q1(dx1),
and since
Q2(Bx1) = P({ω : X2(ω) ∈Bx1}) = P({ω : (x1, X2(ω)) ∈B}) = P((x1, X2) ∈B),
(5.4)
we establish that
P((X1, X2) ∈B) =
R
Rj
P((x1, X2) ∈B)Q1(dx1),
and
P(X1 ∈A, (X1, X2) ∈B) =
R
A
P((x1, X2) ∈B)Q1(dx1).
(5.5)
Now consider the case of two random variables X1 and X2 (j = k = 1) and let
B = {(x1, x2) : x1 + x2 ∈H}, H ∈B1. Using equations (5.4) and (5.5) we can write
the distribution of X1 + X2 as
P(X1 + X2 ∈H) =
R
R
P(X2 ∈H −x)Q1(dx) =
R
R
Q2(H −x)Q1(dx),
where H −x = {h −x : h ∈H}. Therefore the convolution of Q1 and Q2 is the
measure Q1 ∗Q2 deﬁned by
(Q1 ∗Q2)(H) =
R
R
Q2(H −x)Q1(dx).
(5.6)
2. Convolution can be deﬁned based on distribution functions and densities. In-
deed, if F1 and F2 are the cdfs corresponding to Q1 and Q2, the distribution func-
tion corresponding to Q1 ∗Q2 is denoted by F1 ∗F2 and letting H = (−∞, y] in (5.6)
we can write
(F1 ∗F2)(y) =
R
R
F2(y −x)dF1(x).
Moreover, if X1 and X2 are continuous and have densities f1 and f2, respectively,
with respect to the Lebesgue measure µ1, then the convolution Q1 ∗Q2 has density
denoted by f1 ∗f2 and deﬁned by
(f1 ∗f2)(y) =
R
R
f2(y −x)f1(x)dµ1(x).
3. If Q, R and Qn and Rn, n = 1, 2, . . ., are probability measures on R with Qn
w→Q
and Rn
w→R then Qn ∗Rn
w→Q ∗R.
4. The distribution Q in R is called stable if for each n and iid random variables X,
Xi ∼Q, i = 1, 2, . . ., n, there exist constants cn > 0 and bn, such that the distribution

196
CONVERGENCE OF RANDOM OBJECTS
of the sum S n =
nP
i=1 Xi ∼Q∗n is the same as that of the random variable anX +bn, for
all n and Q is not degenerate (concentrated in one point). If bn = 0 then Q is called
strictly stable.
5. If U = Q∗n, for some distribution U and positive integer n, then Q is called
an nth convolution root of U and we write Q = P∗1
n. Distributions that have an
nth convolution root for every n = 1, 2, . . ., are said to be inﬁnitely divisible. For
example, the normal and Poisson distributions are inﬁnitely divisible.
The characteristic function plays a prominent role in identifying the distribution
of a convolution as shown in the following theorem.
Theorem 5.12 (Convolution distributions via characteristic functions) Let
X1, X2, . . . , Xn be independent random variables taking values on the same
space (R, B1) with characteristic functions ϕ1, ϕ2, . . . , ϕn and distributions
Q1, Q2, . . . , Qn, respectively. Then the distribution (convolution) Q1∗Q2∗· · ·∗Qn
of the random variable S n =
nP
i=1 Xi has cf β =
nQ
i=1 ϕi. Consequently, if Qi = Q and
ϕi = ϕ, i = 1, 2, . . ., n, the cf of the n-fold convolution Q∗n is given by ϕn.
Proof. Using theorem 4.15 and the deﬁnition of the characteristic function we
can write
β(t)
=
E

eitS n
= E
 
exp
(
it
nP
i=1 Xi
)!
= E
 nQ
i=1 exp {itXi}
!
=
nQ
i=1 E  exp {itXi} =
nQ
i=1 ϕi(t).
5.5.2
Fourier Inversion and the Continuity Theorem
The importance of characteristic functions is further exempliﬁed in the follow-
ing theorems and remarks, where we connect characteristic functions with densi-
ties, weak convergence and convolutions.
Theorem 5.13 (Fourier inversion) Assume that the random variable X
:
(Ω, A, P) →(R, B1) has cf ϕ, with |ϕ| Lebesgue integrable. Then the distribution
Q = PX−1 has a bounded continuous density f given by
f (x) = (2π)−1
+∞
R
−∞
e−itxϕ(t)dt.
(5.7)
In the discrete case, with X taking distinct values xj, j ≥1, we have
f (xj) = lim
T→∞
1
2T
TR
T
e−itxjϕ(t)dt, j ≥1.
(5.8)

RANDOM SERIES
197
Proof. We prove the continuous case only and request the discrete case as an
exercise. In view of remark 4.10 and the form of the convolution in remark 5.4.2,
the left side of the Parseval relation
+∞
R
−∞
σ
√
2π
e−σ2(x−v)2
2
f (x)dx = 1
2π
+∞
R
−∞
e−ivye−y2
2σ2 ϕ(y)dy,
is simply the convolution Nσ ∗Q, of a normal random variable Zσ ∼N

0, 1
σ2

with density fσ and distribution Nσ and the random variable X with distribution
Q = PX−1, where X has some density f corresponding to Q. Letting σ →∞in the
latter equation we see on the LHS that Nσ∗Q tends to Q, or in terms of the densities,
fσ∗f approaches f (using BCT to pass the limit under the integral sign), where this
f is the bounded continuous function deﬁned in equation (5.7) (since e−y2
2σ2 →1, as
σ →∞, in the RHS of the latter equation and |ϕ| the Lebesgue integrable). To see
that this f is indeed the density of Q, we write
(Nσ ∗Q)(B) =
R
B
(fσ ∗f )(y)dy →
R
B
f (y)dy,
with (Nσ ∗Q)(B) →Q(B), as σ →∞, so that Q(B) =
R
B
f (y)dy, for any B ∈B1 and
the result is established.
Now we can connect weak convergence with pointwise convergence of charac-
teristic functions. The following theorem is also known as the Cram´er-Wold device.
Theorem 5.14 (Continuity theorem) A sequence of probability distributions
{Qn}+∞
n=1 on R converges weakly to a distribution function Q if and only if the se-
quence {ϕn}+∞
n=1 of their characteristic functions converge pointwise to a complex-
valued function ϕ which is continuous at 0 and ϕ is the cf of Q.
Proof. (=⇒) This direction follows immediately from the Helly-Bray theorem
(remark 5.1), since gt(X) = eitX = cos(tX) + i sin(tX) is bounded and continuous
with respect to X so that Qn
w→Q implies that E(gt(Xn)) = ϕn(t) →E(gt(X)) = ϕ(t),
as n →∞, for all t ∈R.
(⇐=) Assume that ϕn(t) →ϕ(t), as n →∞, for all t ∈R, where ϕ is continuous at 0.
First note that the limit Q = lim
n→∞Qn exists. Indeed, using the Helly selection theorem
on the corresponding distribution functions F and Fn of Q and Qn, there exists a
subsequence {Fnk}+∞
k=1 and a nondecreasing, right continuous function F such that
lim
k→∞Fnk(x) = F(x) at continuity points of F. Now apply the special Parseval relation
on {Fnk, ϕnk} to obtain
+∞
R
−∞
e−σ2(x−v)2
2
Fnk(dx) =
1
√
2πσ2
+∞
R
−∞
e−ivye−y2
2σ2 ϕnk(y)dy,

198
CONVERGENCE OF RANDOM OBJECTS
and letting k →∞we have
+∞
R
−∞
e−σ2(x−v)2
2
F(dx) =
1
√
2πσ2
+∞
R
−∞
e−ivye−y2
2σ2 ϕ(y)dy.
Since for given ϕ the latter equation determines F uniquely, the limit F is the same
for all convergent subsequences {Fnk}+∞
k=1.
Now we may apply the special Parseval relation to {Qn, ϕn} so that
+∞
R
−∞
e−σ2(x−v)2
2
Qn(dx) =
1
√
2πσ2
+∞
R
−∞
e−ivye−y2
2σ2 ϕn(y)dy,
and sending n to inﬁnity
+∞
R
−∞
e−σ2(x−v)2
2
Q(dx) =
+∞
R
−∞
e−ivyϕ(y)
1
√
2πσ2e−y2
2σ2 dy,
we have that ϕ is the characteristic function of Q. Moreover, the RHS of the lat-
ter equation is the expectation of the bounded function e−ivyϕ(y) with respect to a
N(0, σ2) random variable. Sending σ to 0 this distribution is concentrated near the
origin and so the right side tends to ϕ(0) whenever ϕ is continuous at the origin
and since ϕn(0) = 1 we have ϕ(0) = 1. As σ →0 the LHS tends to Q(R) so that
Q(R) = 1 and Q is a valid probability measure.
Next we collect some consequences of the inversion and continuity theorems.
Remark 5.5 (Inversion and continuity for densities and cdfs) An immediate con-
sequence of the continuity theorem is that ϕ is continuous everywhere and the con-
vergence ϕn →ϕ is uniform in every ﬁnite interval. Based on the inversion theorem
we can prove the following.
1. Continuity for densities Let ϕn and ϕ be integrable such that
+∞
R
−∞
|ϕn(t) −ϕ(t)|dt →0,
as n →+∞. By the inversion theorem the corresponding distributions Qn and Q
have densities fn and f, respectively. Using the inversion formula we have that
| fn(x) −f (x)| ≤(2π)−1
+∞
R
−∞
|ϕn(t) −ϕ(t)|dt,
and therefore fn →f uniformly.
2. Inversion for cdfs Let F be the distribution function of the cf ϕ, where |ϕ(t)| is
integrable. Then for any h > 0
F(x + h) −F(x)
h
= (2π)−1
+∞
R
−∞
e−itx −e−it(x+h)
ith
ϕ(t)dt,
with e−itx−e−it(x+h)
ith
→eitx and F(x+h)−F(x)
h
→F′(x) = f (x), as h →0, where f is the
density of F.

RANDOM SERIES
199
5.5.3
Limiting Behavior of Partial Sums
Now we address the question of existence of a limit for the partial sum S n =
nP
i=1 Xi, as n →∞, where the {Xn}+∞
n=1 are independent, i.e., we investigate convergence
of the random series
+∞
P
n=1 Xn. Recall that from the SLLN (and the WLLN) we know
that under mild conditions in the iid case, S n/n converges a.s. (and in probability) to
E(X1) (see section 2.3.4). Next we obtain necessary and suﬃcient conditions on the
{Xn}+∞
n=1 and their distributions {Qn}+∞
n=1 in order for S n to converge. First we collect
the following theorem, requested as an exercise.
Theorem 5.15 (Kolmogorov’s maximal inequality) Assume that X1, X2, . . . ,Xn
are independent random variables with mean 0 and ﬁnite variances. Then for any
ε > 0 we have
P(max
1≤k≤n |S k| ≥ε) ≤Var(S n)
ε2
.
A classic convergence criterion is collected next.
Remark 5.6 (Cauchy criterion) When we need to assess convergence and we do
not have a candidate for the limit we can use a stochastic version of the determin-
istic Cauchy criterion for convergence of real series. More precisely, the sequence
{Xn}+∞
n=1 converges a.s. if and only if
lim
n,m→+∞|Xn −Xm| = 0 a.s. In order to use this
criterion in practice we can show that if
lim inf
n→+∞lim
m→+∞P( sup
1≤k≤m
|Xn+k −Xn| > ε) = 0,
(5.9)
for any ε > 0, then {Xn}+∞
n=1 converges a.s.
Using Kolmogorov’s inequality we can prove the following.
Theorem 5.16 (a.s. convergence for random series) Assume that X1, X2, . . . ,Xn
are independent and have zero mean. If
+∞
P
n=1 Var(Xn) converges then
+∞
P
n=1 Xn con-
verges a.s.
Proof. Using Kolmogorov’s inequality on the sequence {Xn+m}+∞
n=1, for m ≥1,
we have
P(max
1≤k≤m |S n+k −S n| > ε) ≤1
ε2
n+m
P
k=n+1 Var(Xk),
for all ε > 0. Now assume that
+∞
P
n=1 Var(Xn) < ∞. Then the RHS above goes to 0 as
we let m →∞ﬁrst and then as we let n →∞. As a result, condition (5.9) of the
Cauchy criterion is satisﬁed and therefore S n converges a.s.

200
CONVERGENCE OF RANDOM OBJECTS
The last result we require before we prove our main result is the following
(requested as an exercise).
Theorem 5.17 (Convergence of the series of variances) Suppose that {Xn}+∞
n=1 is
a bounded sequence of independent random variables. If
+∞
P
n=1(Xn −an) converges
a.s. for some real sequence an then
+∞
P
n=1 Var(Xn) converges.
From example 4.4.1 and Kolmogorov’s 0-1 law we know that S n converges or
diverges a.s. and the corresponding event has probability 0 or 1. The hard part here
is to identify the conditions under which we get the probability to be either 0 or 1
and Kolmogorov’s three-series theorem tells us exactly when this happens.
Theorem 5.18 (Kolmogorov three series) Assume that {Xn}+∞
n=1 is independent
and consider the series
+∞
P
n=1 P(|Xn| > c),
+∞
P
n=1 E [XnI(|Xn| ≤c)] , and
+∞
P
n=1 Var [XnI(|Xn| ≤c)] .
(5.10)
Then S n converges a.s. if and only if all the three series converge, ∀c > 0.
Proof. (=⇒) Assume that S n converges a.s. and ﬁx a c > 0. Since Xn →0
a.s., we have that
+∞
P
n=1 XnI(|Xn| ≤c) converges a.s. and using the Borel-Cantelli
lemma 4.1.3 we have that the ﬁrst series converges. Theorem 5.17 implies
that the third series converges, which in turn, using theorem 5.16 implies that
+∞
P
n=1 [XnI(|Xn| ≤c) −E(XnI(|Xn| ≤c))] converges a.s. As a result, the second series
converges a.s. since
+∞
P
n=1 XnI(|Xn| ≤c) converges a.s.
(⇐=) Now suppose that the series (5.10) converge. From theorem 5.16 we have that
+∞
P
n=1 [XnI(|Xn| ≤c) −E(XnI(|Xn| ≤c))] converges a.s. and since
+∞
P
n=1 E(XnI(|Xn| ≤c))
converges so does
+∞
P
n=1 XnI(|Xn| ≤c). But
+∞
P
n=1 P(|Xn| > c) < ∞and the Borel-Cantelli
lemma 4.1.1 imply that P(Xn , XnI(|Xn| ≤c) i.o.) = 0 and therefore we must have
that S n converges a.s.
5.5.4
Central Limit Theorems
Now we turn to investigating the asymptotic distribution of transformations of
partial sums, i.e., random variables of the form an(S n −bn). The usual Central Limit
Theorem (CLT, exercise 2.82) is quite restrictive since it requires independent and
identically distributed {Xn}+∞
n=1 with the same mean µ = E(Xn) and ﬁnite variance
σ2 = Var(Xn) < ∞. In particular, we have an(S n −bn)
w→Z ∼N(0, 1), with an =

RANDOM SERIES
201
1/
√
nσ2 and bn = nµ. There are several versions of CLTs where these requirements
are relaxed and we discuss the most well-known theorems, i.e., the Lindeberg and
Lyapounov theorems, in this section. An example of a CLT for iid sequences with
inﬁnite variance can be found in Durrett (2010, p 131). First we discuss triangular
arrays of random variables.
Remark 5.7 (Triangular arrays) Consider random variables Xn,m, m
=
1, 2, . . ., kn, n = 1, 2, . . . , and wlog assume that E(Xn,m) = 0 and let σ2
n,m =
E(X2
n,m) < ∞. For each n we assume that Xn,1, . . . , Xn,kn are independent. A col-
lection of random variables with these properties is known as a triangular array.
Further deﬁne
S n = Xn,1 + · · · + Xn,kn,
and let
σ2
n = σ2
n,1 + · · · + σ2
n,kn = Var(S n).
Note that there is no loss of generality if we assume σ2
n = 1 since we can replace
Xn,m by Xn,m/σn. The following conditions are required in the CLTs below.
1. Lindeberg condition For all ε > 0 we assume that
lim
n→+∞
knP
m=1
1
σ2
n
E
h
|Xn,m|2I(|Xn,m| ≥εσn)
i
= 0.
(5.11)
2. Lyapounov’s condition Suppose that |Xn,m|2+δ are integrable, for some δ > 0
and assume that
lim
n→+∞
knP
m=1
1
σ2+δ
n
E
h
|Xn,m|2+δi
= 0.
(5.12)
Note that Lindeberg’s condition is a special case of Lyapounov’s since the sum in
(5.11) is bounded from above by the sum in (5.12).
Now we relax the identically distributed condition of the usual CLT.
Theorem 5.19 (Lindeberg’s CLT) Consider the setup of remark 5.7 and assume
that condition (5.11) holds. Then S n/σn
w→Z ∼N(0, 1).
Proof. Without loss of generality assume that σn = 1. We will show that the
characteristic function of S n converges to the characteristic function of a N(0, 1).
Let ϕn,m(t) denote the characteristic function of Xn,m, so that owing to independence
for a given n, the characteristic function of S n is given by
ϕn(t) =
knQ
m=1 ϕn,m(t).
First using equation (4.35) we note that
ϕn,m(t) −
 
1 −1
2t2σ2
n,m
! ≤E
h
min{|tXn,m|2, |tXn,m|3}
i
< ∞,

202
CONVERGENCE OF RANDOM OBJECTS
and for ε > 0 the RHS can be bounded from above by
R
{ω:|Xn,m(ω)|<ε}
|tXn,m|3dP +
R
{ω:|Xn,m(ω)|≥ε}
|tXn,m|2dP ≤ε|t|3σ2
n,m + t2
R
{ω:|Xn,m(ω)|≥ε}
X2
n,mdP.
Since σ2
n =
knP
m=1 σ2
n,m = 1, using Lindeberg’s condition and the fact that ε > 0 is
arbitrary we obtain
knP
m=1
ϕn,m(t) −
 
1 −1
2t2σ2
n,m
!
≤
knP
m=1
h
ε|t|3σ2
n,m + t2E
h
|Xn,m|2I(|Xn,m| ≥ε)
ii
≤
ε|t|3 + t2 knP
m=1 E
h
|Xn,m|2I(|Xn,m| ≥ε)
i
→0,
for each ﬁxed t. It remains to show that
ϕn(t)
=
knQ
m=1
 
1 −1
2t2σ2
n,m
!
+ o(1)
(5.13)
=
knQ
m=1 e−1
2 t2σ2
n,m + o(1) = exp
(
−1
2t2 knP
m=1 σ2
n,m
)
+ o(1),
(5.14)
or
ϕn(t) = e−1
2 t2 + o(1).
For any ε > 0
σ2
n,m =
R
|Xn,m|2dP ≤ε2 +
R
{ω:|Xn,m(ω)|≥ε}
|Xn,m|2dP,
so that the Lindeberg condition gives
max
1≤m≤knσ2
n,k →0,
(5.15)
as n →∞. For large n, |1 −1
2t2σ2
n,m| < 1 so that using equation (4.34) we can write

knQ
m=1 ϕn,m(t) −
knQ
m=1
 
1 −1
2t2σ2
n,m
! ≤
knP
m=1
ϕn,m(t) −
 
1 −1
2t2σ2
n,m
! = o(1),
which establishes the asymptotic equation (5.13). In addition, using equation (4.34)
again we can write

knQ
m=1 e−1
2 t2σ2
n,m −
knQ
m=1
 
1 −1
2t2σ2
n,m
!
≤
knP
m=1
e−1
2 t2σ2
n,m −1 −
 
−1
2t2σ2
n,m
!
≤
t4et2
knP
m=1 σ4
n,m,
with the last inequality established since for any complex number z we have
|ez −1 −z| ≤|z|2
∞P
m=2
|z|m−2
k!
≤|z|2e|z|.
Finally, using (5.15) we establish the second asymptotic equality of (5.14).
In view of remark 5.7.2 we have the following as a result of theorem 5.19.

SUMMARY
203
Theorem 5.20 (Lyapounov’s CLT) Consider the setup of remark 5.7 and as-
sume that condition (5.12) holds. Then S n/σn
w→Z ∼N(0, 1).
We end this section by discussing how one can relax the independence assump-
tion of the random sequence and still obtain a CLT.
Deﬁnition 5.4 m-dependence and stationarity
A sequence of random variables X = {Xn}+∞
n=1 is said to be m-dependent if, for ev-
ery integer k ≥1, the sets of random variables {X1, . . . , Xk} and {Xm+k+1, Xm+k+2, . . . }
are independent. The random sequence X is called stationary if for any positive in-
tegers k and l, the joint distribution of (Xk, . . . , Xk+l) does not depend on k.
Clearly, for m = 0, a 0-dependent sequence becomes independent. Stationarity
of random sequences will be studied extensively in Chapter 6. The following result
provides a CLT for m-dependent and stationary sequences. For a proof see Ferguson
(1996, p. 70).
Theorem 5.21 (CLT under m-dependence and stationarity) Consider a se-
quence of random variables X = {Xn}+∞
n=1, and assume that X is m-dependent and
stationary, with Var(Xn) < +∞. Then
√n (S n/n −µ)
w→Z ∼N(0, σ2),
where µ = E(Xn), and σ2 = σ00 + 2σ01 + · · · + 2σ0m, with σ0i = Cov(Xn, Xn+i).
5.6
Summary
In this chapter we discussed an important aspect of classical probability theory,
asymptotics. Although we typically never see the whole population, our models
and the estimators of their parameters are based always on a sample of size n from
the whole population. Therefore if our statistical procedure is to be useful, sending
n to +∞should provide us with a good understanding of the true underlying model
that realizes the data we observe, e.g., a sequence of probability measures we build
based on a sample of size n should approach the true probability measure.
The theoretical results of this chapter also tell us that as n goes to +∞we have
statistical procedures (Central Limit Theorems) that can assess when partial sums
converge in distribution to a standard normal. Moreover, we have discussed how
one may develop theory to show a compactness property of the general space of
distribution functions. Recent texts on topics from asymptotics include Ferguson
(1996), Fristedt and Gray (1997), Kallenberg (2002), Dudley (2002), Lehmann
(2004), Durrett (2010), C¸ inlar (2010), Billingsley (2012) and Klenke (2014).

204
CONVERGENCE OF RANDOM OBJECTS
5.7
Exercises
Limiting behavior of iid random sequences
Exercise 5.1 Show that if Xn
w→c, some constant, then Xn
p→c.
Exercise 5.2 Prove theorem 5.2.
Exercise 5.3 Prove that Xn
a.s.
→X if and only if for all ε > 0, P(∥Xk −X∥< ε, for
all k ≥n) →1, as n →+∞.
Exercise 5.4 Show that Xn
a.s.
→X ⇒Xn
p→X ⇒Xn
w→X.
Exercise 5.5 Prove theorem 5.3.
Exercise 5.6
We say that Xn converges to X in the rth mean, r > 0 and write
Xn
r→X, if E ∥Xn −X∥r →0, as n →+∞. Show that if Xn
r→X, for some r > 0,
then Xn
p→X.
Exercise 5.7 If Xn
a.s.
→X and |Xn|r ≤Z for some r > 0 and random variable Z with
EZ < ∞, then Xn
r→X.
Exercise 5.8 If Xn
a.s.
→X, Xn ≥0 and EXn →EX < ∞, then Xn
r→X, for r = 1.
Exercise 5.9 Show that Xn
p→X if and only if for every subsequence {nk} there is
a sub-subsequence {nkj} such that Xnkj
a.s.
→X, as j →∞.
Exercise 5.10 Prove the Fr´echet-Shohat theorem.
Exercise 5.11 Prove theorem 5.1.
Exercise 5.12 Let {Xn}+∞
n=1 be a sequence of iid random variables with Xn ∼U(0, 1),
n ≥1. Prove that for all a > 1, lim
n→∞
1
naXn
= 0 a.s.
Exercise 5.13 Prove statements 1-4, 7 and 8 of remark 5.3.
Exercise 5.14 Let {Xn}+∞
n=1 be a sequence of extended real-valued random variables
deﬁned on a probability space (Ω, A, P). Assume that Xn
a.s.
→X and E|Xn| < ∞
for all n. Show that {Xn}+∞
n=1 is uniformly integrable if and only if lim
n→+∞E(|Xn|) =
E(|X|) < ∞.
Exercise 5.15 Prove the strong consistency of the MLE, i.e., theorem 5.4.
Exercise 5.16 (Asymptotic normality of the MLE) Let X1, . . . , Xn be iid with
density f (x|θ) [µ], θ = [θ1, . . . , θk] ∈Θ, and θ0 denote the true value of the param-
eter and deﬁne the log-likelihood by
ln(θ) = ∇θ ln
nQ
i=1 f (xi|θ) =
nP
i=1 ∇θ ln f (xi|θ),
(5.16)
where ∇θ denotes the gradient with respect to θ. Assume that
(C1) Θ is an open subset of Rk,
(C2) all second partial derivatives
∂2
∂θi∂θ j f (x|θ), i, j = 1, 2, . . ., k, exist and are con-
tinuous in x, and may be passed under the integral sign in
R
f (x|θ)dµ(x),
(C3) there exists a function K(x) such that Eθ0[K(x)] < ∞and each component of

EXERCISES
205
the Hessian ∇2 ln f (x|θ) =
h
∂2
∂θi∂θ j ln f (x|θ)
i
is bounded in absolute value by K(x)
uniformly in some area of θ0,
(C4) the Fisher information matrix IF
x (θ0) = −E
h
∇2 ln f (x|θ0)
i
is positive deﬁnite,
and
(C5) f (x|θ) = f (x|θ0) a.e. [µ] ⇒θ = θ0.
Then there exists a strong consistent sequencebθn of roots of the likelihood equation
(5.16) such that
√n
bθn −θ0

w→Nk

0,
h
IF
x (θ0)
i−1
.
Exercise 5.17
Let X1, . . . , Xn a random sample from a distribution with cdf
F(x). Deﬁne the Empirical distribution function or Empirical cdf, as Fn(x, ω) =
1
n
nP
i=1 I(−∞,x](Xi(ω)).
(i) Show that Fn(x, .)
a.s.
→F(x), for a ﬁxed x.
(ii) (Glivenko-Cantelli) Let Dn(ω) = sup
x∈R
|Fn(x, ω) −F(x)|. Then Fn converges to F
uniformly a.s., i.e., Dn
a.s.
→0.
Exercise 5.18 Prove theorem 5.21.
Exercise 5.19 (Bernstein-von Mises theorem) Let X1, . . . , Xn be an iid sample
from f (x|θ), and consider a continuous prior π(θ) > 0, for all θ ∈Θ ⊂Rk. Let
x = (x1, . . . , xn) denote the data, Ln(θ|x) the likelihood function, and bθn the MLE
under the assumptions of Cram´er’s theorem. Furthermore, denote the posterior dis-
tribution by πn(θ|x) =
Ln(θ|x)π(θ)
mn(x)
, let ϑ = √n(θ −bθn), and denote the density of a
Nk

0,
h
IF
x (θ)
i−1
by φ(θ). If
R
Θ
Ln(bθn + ϑ/ √n|x)
Ln(bθn|x)
π(bθn + ϑ/ √n)dϑ
a.s.
→
R
Θ
exp
(
−1
2ϑTIF
x (θ0)ϑ
)
dϑπ(θ0),
show that
R
Θ
|πn(ϑ|x) −φ(ϑ)| dϑ
a.s.
→0,
that is, the posterior distribution of √n(θ −bθn)|x approaches a normal density
Nk

0,
h
IF
x (θ)
i−1
, as n →∞, and this limiting distribution does not depend on
the selection of the prior π(θ).
Exercise 5.20 Show that the Bayes eθn rule under quadratic loss is asymptotically
normal, i.e., show that
√n
eθn −θ0

w→Nk

0,
h
IF
x (θ0)
i−1
,
(5.17)
where θ0 ∈Θ ⊂Rk, the true value of the parameter. Estimators that satisfy (5.17)
for any θ0 ∈Θ, are called asymptotically eﬃcient.
Convolutions and characteristic functions
Exercise 5.21
Let X ∼Binom(n, p). First show that ϕX(t) = (q + peit)n and then

206
CONVERGENCE OF RANDOM OBJECTS
use the inversion theorem to show that f (x) = Cn
xpxqn−x, x = 0, 1, . . ., n, q = 1 −p,
0 ≤p ≤1.
Exercise 5.22 Prove equation (5.8).
Exercise 5.23 Show that if Q, R, Qn and Rn, n = 1, 2, . . ., are probability measures
on R with Qn
w→Q and Rn
w→R then Qn ∗Rn
w→Q ∗R.
Exercise 5.24
Assume that X ∼Poisson(λt) and Y ∼Poisson(µt), λ, µ, t > 0.
Find the characteristic function of X + Y and identify its distribution.
Exercise 5.25 Consider a N(0, 1) random variable X and let Q be its distribution.
Find Q∗2.
Random Series
Exercise 5.26 Prove theorem 5.15.
Exercise 5.27 (SLLN) Suppose that {Xn}+∞
n=1 is independent and
+∞
P
n=1 Var(Xn/bn)
converges for some strictly positive, increasing, real sequence {bn} with bn →∞,
as n →∞. Then (S n −E(S n))/bn →0, a.s., where S n =
nP
i=1 Xi. Further show that
the result holds for convergence in L2 (recall remark 3.25.1).
Exercise 5.28
Prove the consequence of the Cauchy criterion, equation (5.9) of
remark 5.6.
Exercise 5.29 Suppose that {Xn}+∞
n=1 is a bounded sequence of independent random
variables. If
+∞
P
n=1(Xn −an) converges a.s. for some real sequence an then
+∞
P
n=1 Var(Xn)
converges.
Simulation and computation technique
Exercise 5.30 (Importance sampling) Consider a random variable X with density
fX and support X. Suppose that we want to estimate the expectation
E(g(X)) =
R
X
g(x)fX(x)dx,
and it is diﬃcult to sample from the density fX (so that Monte Carlo integration is
not an option) or g(X) has very large variance. Instead of sampling from the random
variable X ∼fX, consider a random variable Y ∼fY, with the same support X as the
random variable X and assume that we can easily generate values from the density
fY, say, Y1, . . . , Yn
iid∼fY. The density fY is known as the importance density. Deﬁne
In = 1
n
nP
i=1 g(Yi) fX(Yi)
fY(Yi),
and show that
(i) EX(g(X)) = EY(In) = EY h
g(Y) fX(Y)
fY(Y)
i
,
(ii) Var(In) = 1
n

EY 
g(Y) fX(Y)
fY(Y)
2 −
h
EX(g(X))
i2
, and
(iii) In
a.s.
→EX(g(X)) as n →∞.

Chapter 6
Random Sequences
6.1
Introduction
Every object around us evolves over time and the underlying mechanics of the
changes observed on the object are known as an evolution process. Therefore, in
order to capture the evolution or growth process of an object we need to deﬁne
and investigate a collection of random objects, say X = (Xt : t ∈T), where T is
called the time parameter set. Time t can be treated as discrete or continuous and
the values of Xt can be discrete or continuous, which leads to four possible cases.
Although we will see examples for all cases, we present our theoretical devel-
opment depending on the nature of T. In particular, if T is countable then the col-
lection X is called a discrete parameter stochastic process (or a random sequence),
otherwise it is called a continuous time parameter stochastic process (or simply a
stochastic process). The former is the subject of this chapter and the latter case is
treated in the next chapter. For what follows, we write n ≥0 for n ∈{0, 1, 2, . . .}
and n > 0 for n ∈{1, 2, . . .}.
6.2
Deﬁnitions and Properties
We begin with the general deﬁnition of a random sequence.
Deﬁnition 6.1 Random sequence
A random sequence with state space Ψ is a collection of random objects X =
{Xn : n ≥0}, with Xn taking values in Ψ and deﬁned on the same probability space
(Ω, A, P).
We introduce several random sequences in the following.
Example 6.1 (Random sequences) We discuss some speciﬁc examples of ran-
dom sequences next.
1. Bernoulli process Suppose that the sample space is of the form Ω
=
{(ω1, ω2, . . . ) : ωi is a Success or Failure} (recall the inﬁnite coin-ﬂip space) and
207

208
RANDOM SEQUENCES
let p = P(S ), q = 1 −p = P(F), 0 ≤p, q ≤1. For each ω ∈Ωand n ≥1, deﬁne
Xn(ω) = 0 or 1, if ωn = F or ωn = S, so that P(Xn = 1) = p and P(Xn = 0) = q.
Assuming that the Xn are independent we can think of the Xn as the number of
successes (0 or 1) at the nth trial. Then the collection X = {Xn : n ≥1} is a dis-
crete time-parameter stochastic process known as the Bernoulli process with suc-
cess probability p and state space Ψ = {0, 1}.
2. Binomial process Assume that X = {Xn : n ≥1} is a Bernoulli process with
probability of success p. Deﬁne Nn(ω) = 0, if n = 0 and Nn(ω) = X1(ω)+· · ·+Xn(ω),
n ≥1, for all ω ∈Ω. Then Nn is the number of successes in the ﬁrst n-trials and
the random sequence N = {Nn : n ≥0}, with N0 = 0, is known as the Binomial
process. It is easy to see that for all n, k ≥0, we have
Nn+m −Nm|N0, N1, N2, . . . , Nm
d= Nn+m −Nm
d= Nn ∼Binom(n, p).
3. Times of successes Let X be a Bernoulli process with probability of success p.
For ﬁxed ω, a given realization of X is X1(ω), X2(ω), . . . , a 0-1 sequence. Denote by
T1(ω), T2(ω), . . . , the indices corresponding to 1s. Then Tk denotes the trial number
where the kth success occurs. Clearly, Ψ = {1, 2, . . .} and T = {Tk : k ≥1} is
a discrete time-parameter stochastic process. It is possible that T1(ω) = +∞(no
success occurs) in which case T1(ω) = T2(ω) = · · · = +∞. There is a fundamental
relationship between the process T and the number of successes Nn. In particular,
we can show that {ω : Tk(ω) ≤n} = {ω : Nn(ω) ≥k}. Now if Tk(ω) = n, then there
are exactly (k −1)-successes in the ﬁrst n−1 trials and a success at the nth trial. The
converse is also true. As a result, for all k, m ≥1 we can show that
Tk+m −Tm|T1, T2, . . . , Tm
d= Tk+m −Tm
d= Tk ∼NB(k, p),
a negative binomial random variable.
4. Reliability analysis A component in a large system has a lifetime whose dis-
tribution is geometric with p(m) = pqm−1, m = 1, 2, . . . and suppose that when the
component fails it is replaced by an identical one. Let T1, T2, . . . , denote the times
of failure. Then Uk = Tk −Tk−1 denotes the lifetime of the kth item replaced with
Uk
iid∼Geo(p), for all k. Consequently, we can think of T = {Tk : k ≥1} as the times
of “successes” in a Bernoulli process. Now suppose that past records indicate that
the ﬁrst 3 failures occurred at times 3, 12 and 14 and we wish to estimate the time
of the 5th failure, namely, T5. We have
E(T5|T1, T2, T3)
=
E(T5|T3) = E(T3 + (T5 −T3)|T3)
=
T3 + E(T5 −T3|T3) = T3 + E(T5 −T3)
=
T3 + E(T2) = T3 + 2/p,
and therefore E(T5|T1 = 3, T2 = 12, T3 = 14) = 14 + 2/p.
The following properties are satisﬁed by many random sequences including the
ones of the previous example.

DEFINITIONS AND PROPERTIES
209
Deﬁnition 6.2 Stationarity and independent increments
A random sequence X = {Xn : n ≥0} is said to have stationary increments if the
distribution of the increment Xn+k −Xn does not depend on n. The random sequence
X is said to have independent increments if for any k ≥1 and n0 = 0 < n1 < n2 <
· · · < nk the increments Xn1 −Xn0, Xn2 −Xn1, . . . , Xnk −Xnk−1 are independent.
We discuss some classic examples in order to appreciate the latter deﬁnition.
Example 6.2 (Random walks) A random sequence deﬁned through sums of iid
random variables is a random walk. In particular, let Xn ∼Q, n > 0, deﬁned on
(Ω, A, P) and taking values in Ψ for some distribution Q. Deﬁne S n(ω) = 0, if
n = 0 and S n(ω) = X1(ω)+· · · + Xn(ω), n > 0, for all ω ∈Ω. Then S = {S n : n ≥0}
is a discrete time parameter stochastic process with state space Ψ.
1. First note that S n+m −S n = Xn+1 + · · · + Xn+m
d= X1 + · · · + Xm = S m since the Xn
are iid and hence S has stationary increments. Moreover, any increments are sums
of iid random variables so that S has independent increments and therefore
S n+m −S n|S 1, . . . , S n
d= S n+m −S n
d= S m.
2. Simple symmetric random walks A simple random walk S in Zp is a random
walk with state space Zp or Rp whose step distribution Q is supported by the set
S = {±ei, i = 1, 2, . . ., p}, where ei, i = 1, 2, . . ., p, the standard basis vectors in
Rp and Q(S) = 1, Q(C) < 1, for C ⊂S. It is also called a “nearest neighbor”
random walk because successive states S n(ω) and S n+1(ω) are always a unit apart.
For p = 1, e1 = ±1 and we have a simple random walk on Z with steps of size 1
in either the positive or negative direction. If Q({−1}) = Q({1}) = 1/2 the random
walk is called symmetric.
Example 6.3 (Returns to 0) Consider a simple random walk S on Z, with S 0 =
0 and step distribution Q with p = Q({1}) = 1 −Q({−1}), 0 < p < 1. Denote the
sequence of steps by X = (X1, X2, . . . ) and assume that the simple random walk S is
adapted to a ﬁltration F = (F1, F2, . . . ). We are interested in the ﬁrst return to time
{0}, i.e., in the random variable T1(ω) = inf{n > 0 : S n(ω) = 0}, where inf ∅= +∞.
Note that {ω : T1(ω) ≤n} =
nS
m=1{ω : S n(ω) = 0} ∈Fn, since {ω : S n(ω) = 0} ∈Fn
and therefore T1 is a stopping time. Similarly, for j > 1 we deﬁne the jth return
time to 0 of S recursively as
T j(ω) = inf
n
n > T j−1(ω) : S n(ω) = 0
o
.
It is straightforward to see that T j is a stopping time.
The following results give some insight on the distribution of T j for random
walks. The proofs are requested as exercises.

210
RANDOM SEQUENCES
Theorem 6.1 (Return times) Let T j denote the jth return time of some random
walk to 0, j = 1, 2, . . . and let W denote the distribution of T1. Then the distri-
bution of the sequence (0, T1, T2, . . . ) is that of a random walk in Z
+ with step
distribution W.
Corollary 6.1 (Number of visits) Let Q denote the step distribution of the ran-
dom walk S and deﬁne the random variable
V(ω) = inf
n
j : T j(ω) = ∞
o
= # {n ≥0 : S n(ω) = 0} ,
denoting the number of visits of the random walk to state 0. Then we can show that
if
+∞
P
n=0 Q∗n({0}) = +∞, then V = +∞and if
+∞
P
n=0 Q∗n({0}) < +∞, the distribution of V
is geometric with
P({ω : V(ω) = 1}) = P({ω : T1(ω) = +∞}) =
 +∞
P
n=0 Q∗n({0})
!−1
.
A ﬁrst classiﬁcation of random walks is given next.
Deﬁnition 6.3 Random walk classiﬁcation
For a simple random walk S in Z deﬁne An = {ω : S n(ω) = 0}. Then S is called
recurrent if P(lim sup An) = 1 and transient otherwise.
Since ω ∈lim sup An ⇔ω ∈An i.o., a recurrent random walk visits state 0 again
and again while a transient random walk will visit 0 a last time (even just once since
S 0 = 0) never to return. The corollary can then be rephrased as: P(lim sup An) = 1,
if
+∞
P
n=0 P(An) = +∞, or 0, if
+∞
P
n=0 P(An) < +∞(recall the Borel-Cantelli lemmas 4.1).
6.3
Martingales
Martingales play an important role in mathematics and statistics, and can be
used as a tool to deﬁne more complicated random objects, including stochastic
integrals. They owe much of their development to gambling modeling and we will
explore some of these applications later in this section. We begin with the deﬁnition
of a ﬁltration and an adapted sequence of random variables to the ﬁltration.
Deﬁnition 6.4 Filtration and adapted sequence
Let X = (X1, X2, . . . ) be a sequence of (R, B1)-valued random variables deﬁned
on a probability space (Ω, A, P) and let F = (F1, F2, . . . ) be a sequence of sub-σ-
ﬁelds of A. When the collection of sub-σ-ﬁelds F satisﬁes Fn ⊆Fn+1, we say that
F is a ﬁltration or that the Fn form a ﬁltration in A. If Xn is Fn-measurable, for all
n, then we say that the sequence X is adapted to the ﬁltration F .

MARTINGALES
211
The formal deﬁnition of a martingale is given next.
Deﬁnition 6.5 Martingale (MG)
Let X = (X1, X2, . . . ) be a sequence of (R, B1)-valued random variables deﬁned
on a probability space (Ω, A, P) and let F = (F1, F2, . . . ) be a sequence of sub-σ-
ﬁelds of A. The sequence {(Xn, Fn) : n = 1, 2, . . .} is a martingale if the following
conditions hold:
(i) Fn ⊆Fn+1,
(ii) Xn is Fn-measurable,
(iii) E|Xn| < +∞, and
(iv)
E(Xn+1|Fn) = Xn a.s.
(6.1)
We write (X, F ) or {Xn, Fn} to abbreviate the notation for a MG and we say that X
is a MG relative to the ﬁltration F .
For what follows in this section we assume the setup of this deﬁnition.
Remark 6.1 (Martingale properties and deﬁnitions) Condition (iii) is simply in-
tegrability for the random variable Xn, whereas condition (iv) expresses that given
the information about the experiment up to time n, which is contained in Fn, we
do not expect the average value of the random variable to change at the next time
period n + 1. We collect some essential deﬁnitions and results on MGs below.
1. Sub-MG If we replace condition (iv) with E(Xn+1|Fn) ≥Xn a.s., then (X, F ) is
called a sub-martingale (sMG). Using equation (4.31) we can show that
E((Xn+1 −Xn)IF) ≥0, ∀F ∈Fn.
2. Super-MG If condition (iv) becomes E(Xn+1|Fn) ≤Xn a.s., then (X, F ) is called
a super-martingale (SMG). Similarly as above, (X, F ) is a SMG if and only if
E((Xn+1 −Xn)IF) ≤0, ∀F ∈Fn. There is nothing super about a SMG. In fact,
in terms of gambling, SMG leads to unfavorable games for the player. See the
gambling examples that follow in this section.
3. Minimal ﬁltration There is always at least one ﬁltration with respect to which
Xn is adapted. Let F X = (F X
1 , F X
2 , . . . ), where F X
n
= σ(X1, X2, . . . , Xn) ⊆A.
Clearly F X
n ⊆F X
n+1, for all n = 1, 2, . . ., so that F X is a ﬁltration and moreover
X is adapted to F X. The ﬁltration F X is called the minimal ﬁltration of X. Suppose
that (X, F ) is a MG. Then
E(Xn+1|F X
n ) = E(E(Xn+1|Fn)|F X
n ) = E(Xn|F X
n ) = Xn a.s.,
and thus (X, F X) is also a MG, where F X
n
⊆Fn, since F X
n is the smallest σ-
ﬁeld from F X or F with respect to which Xn is measurable. Therefore F X is the
smallest ﬁltration with respect to which X is a MG. When we say that X is a MG
without indicating a ﬁltration we simply mean relative to F X. Finally, note that if

212
RANDOM SEQUENCES
one augments a ﬁltration with additional information, an adapted sequence remains
adapted.
4. {Xn, Fn} is a SMG if and only if {−Xn, Fn} is a sMG. This result allows us to
prove claims for a sMG only and then immediately obtain the claim for a SMG.
5. (X, F ) is a MG if and only if it is both a sMG and a SMG.
6. If (X, F ) is a MG then E(Xn+k|Fn) = Xn, for all n, k = 1, 2, . . .. To see this write
E(Xn+2|Fn) = E(E(Xn+2|Fn+1)|Fn) = E(Xn+1|Fn) = Xn a.s.,
and use induction.
7. If X is a MG then E(Xn+1|X1, . . . , Xn) = Xn a.s.
8. Condition (iv) may be rewritten in many forms. Starting with the integral form
of the deﬁnition of conditional probability E(Xn+1|Fn) = Xn a.s. if and only if
R
F
Xn+1dP =
R
F
XndP,
∀F ∈Fn. Letting F = Ω, we obtain E(X1) = E(X2) = . . . . Now letting ∆1 = X1,
∆n = Xn −Xn−1, n = 2, 3, . . ., linearity allows condition (iv) to be written equiva-
lently as E(∆n+1|Fn) = 0 a.s. Note that since Xn = ∆1 + ∆2 + · · · + ∆n, the random
vectors (X1, . . . , Xn) and (∆1, . . . , ∆n) generate the same σ-ﬁeld: σ(X1, . . . , Xn) =
σ(∆1, . . . , ∆n).
9. If {Xn, Fn} and {Yn, Fn} are MGs then so are {Xn ± Yn, Fn} and {max(Xn, Yn), Fn}.
10. If X is an iid sequence of integrable random variables with E(Xn) = 0, for all
n = 1, 2, . . ., then S n =
nP
i=1 Xi is a MG. To see this, ﬁrst note that σ(X1, . . . , Xn) =
σ(S 1, . . . , S n). Therefore we can write
E(S n+1|σ(S 1, . . . , S n)) = E(S n+1|σ(X1, . . . , Xn)) = E(S n + Xn+1|X1, . . . , Xn) = S n,
a.s., since E(Xn+1|σ(X1, . . . , Xn)) = 0.
11. If Y is an integrable random variable deﬁned on (Ω, A, P) and F is any ﬁltra-
tion in A then the sequence X with Xn = E(Y|Fn) is a MG relative to F .
12. Let g be a convex, increasing, R-valued function and (X, F ) a sMG such that
g(Xn) is integrable. Then {g(Xn), Fn} is a sMG.
13. Let (X, F ) be a MG and g be a convex, R-valued function, such that g(Xn) is
integrable. Then {g(Xn), Fn} is a sMG.
14. Etemadi inequality Let X be a sMG. Then for c > 0 : P

max
0≤k≤nXk > c

≤
E (|Xn|) /c.
15. Kolmogorov inequality Let X be a MG. Then for all c > 0 and p ≥1 :
P

max
0≤k≤n|Xk| > c

≤E (|Xn|p) /cp.

MARTINGALES
213
Example 6.4 (Martingale transformations) If X is a MG then so is {X+
n }. If |Xn|n
is integrable then {|Xn|n} is a sMG.
Example 6.5 (Random walks and martingales) Recall example 4.12.
1. In view of remark 6.1.10, if the steps have zero mean, i.e., E(Xn) = 0, then the
random walk S is a MG.
2. Let S
be an R-valued random walk, starting at 0, with steps from
the random sequence X
=
(X1, X2, . . . ) having ﬁnite mean. Deﬁne Hn
=
σ (S n/n, S n+1/(n + 1), . . . ) and note that Hn = σ(S n, Xn+1, Xn+2, . . . ). Then
E
 1
nS n|Hn+1
!
= 1
n
nP
i=1 E(Xi|Hn+1) = 1
n
nP
i=1 S n+1/(n + 1) = S n+1/(n + 1)
a.s., since E(Xi|X1+· · ·+Xn) = (X1 + · · · + Xn) /n, i = 1, 2, . . . , n. Now let Zn = S n/n
so that E(Zn|Hn+1) = Zn+1 a.s., leads to a reverse version of the deﬁnition of a MG.
3. Let S = (S 0, S 1, S 2, . . . ) be a random walk with steps {Xn} that are independent
with E(Xn) = 0 and σ2
n = E(X2
n) < +∞. Then S is a MG from remark 6.1.10 and
remark 6.1.13 implies that Yn = S 2
n, is a sMG.
Random sequences may work backward in time as we saw in the latter example.
Deﬁnition 6.6 Reverse martingale
A random sequence Z = (Z1, Z2, . . . ) is called a reverse MG relative to a reverse
ﬁltration F = (F1, F2, . . . ) (Fn+1 ⊆Fn) if for all n = 1, 2, . . ., E(Zn|Fn+1) = Zn a.s.,
E(Zn) < +∞and Z is adapted to F . It is called a reverse SMG if E(Zn|Fn+1) ≤Zn
a.s. and a reverse sMG if E(Zn|Fn+1) ≥Zn a.s.
Example 6.6 (Gambling) Let X1, X2, . . . , be iid random variables on a probabil-
ity space (Ω, A, P) such that P(Xn = 1) = 1 −P(Xn = −1) = p, that is, we have
an inﬁnite coin ﬂip space with the assignment Heads↔1 and Tails↔−1. Assume
that at the nth toss of the coin we bet $b and we win $b if and only if Xn = 1 or
lose $b if and only if Xn = −1. A gambling strategy is a sequence of functions
bn : {−1, 1}n →[0, +∞), such that bn(X1, . . . , Xn) denotes the amount we bet in
the nth stage where {−1, 1}n is the space of all sequences of −1 and 1 of length n.
Let S 0 be our initial fortune and denote by S n the fortune after the nth stage. Then
S n+1 = S n + Xn+1bn(X1, . . . , Xn) is the fortune after stage n + 1. In our example
bn(X1, . . . , Xn) = b, the amount of dollars we bet at each stage.
Now the game is fair if p = 1/2, favorable if p > 1/2 and unfavorable
p < 1/2. Since σ(S 1, . . . , S n) = σ(X1, . . . , Xn) we may write
E(S n+1|S 1, . . . , S n) = E(S n+1|X1, . . . , Xn) = E(S n + Xn+1bn(X1, . . . , Xn)|X1, . . . , Xn),
where bn and S n are σ(X1, . . . , Xn)-measurable and Xn+1 is independent of

214
RANDOM SEQUENCES
X1, . . . , Xn so that
E(S n+1|S 1, . . . , S n)
=
S n + E(Xn+1)bn(X1, . . . , Xn)
=
S n + (2p −1)bn(X1, . . . , Xn),
since E(Xn+1) = 2p −1. Note that E(Xn+1) = 0, if p = 1/2, so that S = (S 1, S 2 . . . )
is a MG, negative, if p < 1/2 and hence S is a SMG and positive if p > 1/2, so that
S is a sMG. Moreover
E(S n+1) = E(E(S n+1|σ(S 1, . . . , S n))) = E(S n) + E(Xn+1)E(bn(X1, . . . , Xn)),
and therefore the average fortune after the n+1 stage, E(S n+1), is the same as E(S n)
if S is a MG (p = 1/2), at most E(S n) if S is a SMG (p < 1/2), and at least E(S n)
if S is a sMG (p > 1/2). Now assume that Yn denotes the gambler’s fortune after n
stages where at some of the stages the gambler is allowed to skip playing. The next
theorem shows that if the game is fair (that is, S is a MG) or favorable (S is a sMG)
the game remains of the same nature and the skipping strategy does not increase
the expected earnings.
The starting point in studying MG behavior is the optimal skipping theorem.
Theorem 6.2 (Optimal skipping by Halmos) Let {Xn, Fn} be a MG (or a sMG).
Deﬁne indicator random variables εk = I((X1, . . . , Xk) ∈Bk), where Bk ∈Bk. Let
Y1 = X1, Y2 = X1 + ε1(X2 −X1), . . . , Yn = Yn−1 + εn−1(Xn −Xn−1). Then {Yn, Fn} is
a MG (or a sMG) and E(Yn) = E(Xn) (or E(Yn) ≤E(Xn)), for all n = 1, 2, . . .
Proof. We have
E(Yn+1|Fn)
=
E(Yn + εn(Xn+1 −Xn)|Fn) = Yn + εnE(Xn+1 −Xn|Fn)
=
Yn + εn(E(Xn+1|Fn) −Xn) a.s.
Consequently, if {Xn, Fn} is a MG, E(Xn+1|Fn) = Xn, so that E(Yn+1|Fn) = Yn and
{Yn, Fn} is a MG. Moreover, if E(Xn+1|Fn) ≥Xn we have E(Yn+1|Fn) ≥Yn so that
{Yn, Fn} is a sMG. We prove E(Yn) = E(Xn), n = 1, 2, ..., using induction. Clearly
E(Y1) = E(X1) by deﬁnition. Assume that it holds for n = k : E(Xk −Yk) = 0 (MG)
or ≥0 (sMG). We show that the claim holds for n = k + 1. First we write
Xk+1 −Yk+1 = Xk+1 −Yk −εk(Xk+1 −Xk) = (1 −εk)(Xk+1 −Xk) + (Xk −Yk),
and taking expectation conditionally on Fk above we obtain
E(Xk+1 −Yk+1|Fk) = (1 −εk)E(Xk+1 −Xk|Fk) + Xk −Yk.
Since E(Xk+1 −Xk|Fk) = 0 (MG) or ≥0 (sMG), we have
E(Xk+1 −Yk+1|Fk)

= Xk −Yk (MG)
≥Xk −Yk (sMG)
.
Taking expectation in the latter equations we have
E[E(Xk+1 −Yk+1|Fk)] = E(Xk+1 −Yk+1)

= 0 (MG)
≥0 (sMG)
,
and the result is established.

MARTINGALES
215
6.3.1
Filtrations and Stopping Times
A ﬁltration represents the information obtained by observing an experiment up
to time n where time here refers to the index of the sequence X1, X2, . . . , Xn, . . . ,
so that at time n we observe the value Xn from the sequence. A stopping time N
can be thought of as the time at which the observations of the experiment are to
be stopped. The deﬁnition of a stopping time requires that the decision to stop
observing at a certain time n be based on the information available up to time n.
Recall that Z
+ = {1, 2, . . ., +∞} and for a ﬁltration F deﬁne F∞to be the σ-ﬁeld
σ(F1, F2, . . . ) = σ
 +∞
S
n=1 Fn
!
. The latter will be denoted by Fn ↑F∞.
Deﬁnition 6.7 Stopping time
Let (Ω, A) be a measurable space and let F = {Fn : n = 1, 2, . . .} denote a
ﬁltration in A. A Z
+-valued measurable function N deﬁned on (Ω, A) is called a
stopping time with respect to (or relative to) the ﬁltration F if {ω ∈Ω: N(ω) ≤
n} ∈Fn, for all n ∈Z
+. We say that an event A ∈A is prior to N if and only if
A ∩{ω ∈Ω: N(ω) ≤n} ∈Fn, for all n ∈Z
+. The collection of all prior events A is
denoted by FN.
If X is a random sequence then a stopping time for this sequence is by deﬁ-
nition a stopping time relative to the minimal ﬁltration F X. Next we collect some
additional results on stopping times.
Remark 6.2 (Stopping times) The following are straightforward to show.
1. A Z
+-valued random variable N is a stopping time relative to the ﬁltration F if
and only if {ω ∈Ω: N(ω) = n} ∈Fn, n ∈Z
+.
2. The prior events FN form a σ-ﬁeld. In terms of interpretation, consider that
when we observe a random sequence X = (X1, X2, . . . ) up to a stopping time N we
obtain a certain amount of information. If X is adapted to a ﬁltration F and N is
a stopping time relative to F then FN as deﬁned contains exactly this amount of
information.
3. A ∈FN if and only if A ∩{ω ∈Ω: N(ω) = n} ∈Fn, for all n ∈Z
+.
4. If M ≤N are two stopping times relative to the same ﬁltration F then FM ⊆FN.
Several examples on ﬁltrations and stopping times are collected next.
Example 6.7 (Filtrations and stopping times)
Assume the setup of deﬁnition
6.7 and let X be a random sequence deﬁned on (Ω, A, P).
1. Consider a random walk S with S n =
nP
i=1 Xi and assume that the step distribution

216
RANDOM SEQUENCES
is N(µ, σ2). Deﬁne N(ω) = inf{n ≥1 : S n(ω) ∈In}, where In = (an, bn), an open
interval. If the ﬁltration F is taken to be the minimal ﬁltration F X then N is a
stopping time.
2. Not every discrete random variable is also a stopping time. Indeed, consider
N to be the last time the sequence X is at some value x (we will refer to this as
“X visits x at time N” later in this chapter), that is, XN = x and XN+k , x, for all
k = 1, 2, . . . , thereafter. Then N is not a stopping time for X.
3. If N = c is some constant then N is a stopping time.
4. If N is a stopping time and {Xn} a MG then {XN∧n} is a SMG.
Example 6.8 (Gambler’s ruin)
Recall the setup of example 6.6. One usually
assumes that bn(X1, . . . , Xn) ≤S n, that is, we bet less than our total fortune at stage
n. Let τ = inf{n ≥1 : S n = 0} denote the ﬁrst stage n after which our fortune
is gone, also known as gambler’s ruin. Clearly, τ is a stopping time since (τ =
n) depends on X1, X2, . . . , Xn only. The interesting questions in this case include:
whether gambler’s ruin occurs at a ﬁnite stage, i.e., P(τ < ∞); how many stages on
the average until we lose our fortune, i.e., E(τ), and the probability that our game
will last at least n −1 stages, i.e., P(τ ≥n). These questions can be easily answered
using concepts presented later in this chapter.
6.3.2
Convergence Theorems
The concept of MGs is due to L´evy but it was Doob who developed the theory.
In particular, studying the asymptotic behavior of MGs is due to Doob and his
convergence theorem, which provides a great tool to assess the existence of a limit
distribution for a random sequence. The proof of the limit theorem requires the
concept of upcrossings by the random sequence and the corresponding upcrossings
theorem.
Deﬁnition 6.8 Upcrossings
Let X1, X2, . . . , Xn be random variables on a probability space (Ω, A, P) and let
a < b, be real constants. We deﬁne the number of upcrossings Uab of the interval
(a, b) by X1, . . . , Xn as follows: let
T1
=
min
1≤k≤n{k : Xk ≤a}, T2 = min
T1<k≤n{k : Xk ≥b},
T3
=
min
T2<k≤n{k : Xk ≤a}, T4 = min
T3<k≤n{k : Xk ≥b}, . . .,
and set Ti = +∞, if the condition speciﬁed cannot be met (e.g., if for all i = 4, 5, . . . ,
Xi < b, then T4 = +∞). Then we deﬁne N as the number of ﬁnite Ti’s and
Uab =

N/2
N is even
(N −1)/2
N is odd .

MARTINGALES
217
Next we collect the upcrossings theorem.
Theorem 6.3 (Upcrossings theorem by Doob) Let (X, F ) be a sMG. Then
E(Uab) ≤E[(Xn −a)+]/(b −a).
Proof. Assume for now that a = 0 and X j ≥0, for all j, so that we need
to show E(U0b) ≤
1
bE(Xn). Let Ti and εi be deﬁned as in the optimal skipping
theorem. Note that with the assumption a = 0 we have XTi ≤a ⇐⇒XTi = 0.
Deﬁne Yn = X1 +
n−1
P
i=1 εi(Xi+1 −Xi), that is, Yn denotes the total increase during
upcrossing plus X1 plus the contribution at the end and consequently Yn ≥bU0b,
the reason being that in order to reach b the sequence has to hit 0 and then go up
at least b units to get one upcrossing. This implies U0b ≤Yn
b and taking expectation
we have
E(U0b) ≤E(Yn)
b
≤E(Xn)
b
,
with the second inequality following by the optimal skipping theorem. Therefore
the claim holds for X j ≥0 and a = 0.
Now in the general case, {(Xk −a)+, Ak} is a sMG (from Jensen’s inequality)
and the number of upcrossings Uab by the sequence {Xk} is the same as the number
of upcrossings U0b−a by the sequence {(Xk −a)+} since Xk ≤a ⇐⇒Xk −a ≤0 and
(Xk −a)+ = 0 and moreover Xk ≥b ⇐⇒Xk −a ≥b −a so that (Xk −a)+ ≥b −a.
Hence the general result follows from the proof above for U0b.
We are now ready to present a ﬁrst MG convergence theorem.
Theorem 6.4 (Doob’s convergence theorem) Let {Xn, Fn} be a sMG. If
sup
n
E(X+
n ) < +∞then there exists an integrable random variable X∞such that
Xn
a.s.
→X∞.
Proof. We are interested in the probability of the event A = {ω ∈Ω: Xn(ω)
does not converge to a ﬁnite limit X∞(ω) or is ±∞} which can be written equiva-
lently as
A =
S
a<b
a,b rational
{ω ∈Ω: lim inf
n
Xn(ω) < a < b < lim sup
n
Xn(ω)}.
If for some a < b we have P({ω ∈Ω: lim inf
n
Xn(ω) < a < b < lim sup
n
Xn(ω)}) > 0
then this implies that {Xn} has an inﬁnite number of upcrossings of (a, b) on a set
of positive probability, that is, E(Uab) = +∞. Deﬁne Ua,b;n to be the number of
upcrossing of (a, b) by X1, ..., Xn. Then Ua,b;n ↑Uab and by the MCT: EUa,b;n ↑

218
RANDOM SEQUENCES
EUab = +∞. But by the upcrossing theorem
EUa,b;n ≤
1
b −aE((Xn −a)+) ≤
1
b −asup
n
E(Xn + a) < ∞,
so that the DCT yields EUab < +∞, a contradiction. Therefore Xn
a.s.
→X∞and it
remains to show that the limit is integrable.
Recall that |Xn| = X+
n + X−
n = 2X+
n −Xn and note that
E(|Xn|) ≤2E(X+
n ) −E(Xn) ≤2sup
n
E(X+
n ) −E(X1) = M < ∞,
since EXn ≥EX1 for a sMG. Applying Fatou’s lemma on |Xn| ≥0 we obtain
lim inf
n
E|Xn| ≥E[lim inf
n
|Xn|] = E|X∞|,
so that
E|X∞| ≤lim inf
n
E|Xn| ≤M < ∞,
and the integrability of the limit is established.
A special case of Doob’s convergence theorem is as follows; if Xn ≥0, n =
1, 2, . . ., is a SMG then Xn
a.s.
→X with E(X) ≤E(X1). The following example should
provide the motivation on the necessity of replacing a ﬁxed time of the sequence
Xn with a stopping time T, and study XT instead of Xn.
Example 6.9 (Gambling)
Let EXT denote the average fortune when we stop
playing the game where T is some random variable. If 1 = T1 < T2 < T3 <
· · · < Tn < . . . , is a sequence of stopping times a.s. ﬁnite (P(Ti < ∞) = 1)
and assuming that {XTn} is a MG then under certain conditions we can show that
E(XTn) = E(X1) (see theorem 6.5 below). Consequently, for a MG it does not matter
when we choose to stop playing since on the average we will have the same fortune
as that after the ﬁrst stage. This is a result of a general result we collect next.
Let us collect some conditions for well-behaved stopping times.
Deﬁnition 6.9 Sampling integrability conditions
Let X = (X1, X2, . . . ) be a random sequence of R-valued random variables with
ﬁnite mean. A Z
+-valued random variable T that is a.s. ﬁnite is said to satisfy the
sampling integrability conditions for X if the following two conditions hold
(A) : E(|XT|) < +∞,
(6.2)
and
(B) : lim inf
n
R
{T>n}
|Xn|dP = 0.
(6.3)
The main result that allows us to replace ﬁxed times n with a sequence of stop-
ping times, while maintaining the MG property of the sequence, is the optional
sampling theorem.

MARTINGALES
219
Theorem 6.5 (Optional sampling) Let {Xn} be a sMG and {Tn} an increasing
sequence of a.s. ﬁnite stopping times relative to the same ﬁltration (the minimal
ﬁltration F X). Deﬁne Yn(ω) = XTn(ω)(ω), Fn = F X
Tn, n = 1, 2, . . . and assume that
each Tn satisﬁes the sampling integrability conditions for X. Then {Yn, Fn} is a
sMG and if {Xn} is a MG then so is {Yn, Fn}.
Proof. First we show that F = {F1, F2, ...} is a ﬁltration. Let A ∈Fn = FTn so
that A ∩{Tn = i} ∈Fi for all i. Then
A ∩{Tn+1 ≤k} =
kS
i=1[A ∩{Tn = i}] ∩{Tn+1 ≤k}.
and since
A ∩{Tn = i} ∈Fi ⊆Fk,
for i ≤k and {Tn+1 ≤k} ∈Fk we have that A ∈FTn+1 and therefore F is a ﬁltration.
Furthermore, note that {Yn} is adapted to F .
Now by condition (A) of the sampling integrability conditions we only need
to show that E(Yn+1|Fn)
(= if X a MG)
≥
Yn or equivalently
R
A
Yn+1dP
(= if X a MG)
≥
R
A
YndP,
(6.4)
for all A ∈Fn. Fix n > 0 and A ∈Fn. Since Tn is a.s. ﬁnite we can write A =
+∞
S
j=1[A∩
{Tn = j}]. Then it suﬃces to show that (6.4) holds for sets D j = A ∩{Tn = j} ∈F j.
For ﬁxed k > j we have
R
Dj
Yn+1dP
=
kP
i=j
R
Dj∩{Tn+1=i}
Yn+1dP +
R
Dj∩{Tn+1>k}
Yn+1dP
=
kP
i=j
R
Dj∩{Tn+1=i}
XidP +
R
Dj∩{Tn+1>k}
XkdP −
R
Dj∩{Tn+1>k}
(Xk −Yn+1)dP,
where we used the fact that Yn = XTn in the ﬁrst integral of the RHS. Combine the
last term in the sum and the middle integral in order to write
R
Dj∩{Tn+1=k}
XkdP +
R
Dj∩{Tn+1>k}
XkdP =
R
Dj∩{Tn+1≥k}
XkdP.
Since {Tn+1 ≥k} = Ω∖{Tn+1 ≤k −1} ∈Fk−1, D j ∈F j ⊂
j<k Fk−1 and {Xn} is a sMG
it follows that
R
Dj∩{Tn+1>k}
XkdP
(= if X a MG)
≥
R
Dj∩{Tn+1≥k}
Xk−1dP.
Now combine this term with the i = k −1 from the sum and obtain again an
inequality due to the sMG structure of {Xn}. Continue this process by induction to
get
R
Dj
Yn+1dP
(= if X a MG)
≥
R
Dj∩{Tn+1≥j}
X jdP −
R
Dj∩{Tn+1>k}
(Xk −Yn+1)dP.

220
RANDOM SEQUENCES
By condition (B) of the sampling integrability conditions, we have
R
Dj∩{Tn+1>k}
XkdP →0,
as k →∞and
R
Dj∩{Tn+1≥k}
Yn+1dP →0,
since {Tn+1 ≥k} ↓∅as k →∞. Therefore
R
Dj
Yn+1dP
(= if X a MG)
≥
R
Dj∩{Tn+1>j}
X jdP,
and since D j ⊂{Tn = j} ⊂{Tn+1 ≥j}, we have D j ∩{Tn+1 > j} = D j and thus
R
Dj
Yn+1dP
(= if X a MG)
≥
R
Dj
YndP,
which completes the proof.
The sampling integrability conditions can be hard to show. The following theo-
rem discusses some special cases when conditions (A) and (B) hold.
Theorem 6.6 (Proving the sampling integrability conditions) Let {Xn} be a
sMG and {Tn} an increasing sequence of a.s. ﬁnite stopping times relative to the
minimal ﬁltration. The sampling integrability conditions (A) and (B) are valid if
one of the following conditions holds: (i) for any n ≥1 there exists an integer
kn > 0 such that Tn ≤kn a.s., or (ii) E
"
sup
n
|Xn|
#
< +∞. In particular this is true if
{Xn} are uniformly bounded.
Proof. To prove (i) =⇒(A) we write
R
|XTn|dP =
knP
i=1
R
{Tn=i}
|Xi|dP ≤
knP
i=1 E|Xi| < +∞.
To show (i) =⇒(B) note that P(Tn > k) = 0, for all k > kn.
Now let Z = sup
n
|Xn|. Then
R
|XTn|dP ≤E(Z) < +∞so that (ii) =⇒(A). To
show that (ii) =⇒(B) we have
R
{Tn>k}
|Xk|dP ≤
R
{Tn>k}
ZdP →0,
as k →∞since Z is integrable.
The following remark connects MGs, uniform integrability and the sampling
integrability conditions.
Remark 6.3 (Martingales and uniform integrability) Recall deﬁnition 3.17 and
remark 3.17 involving uniform integrability. In terms of a random sequence X =
(X1, X2, . . . ) we can write the deﬁnition as
lim
a→∞sup
n
R
|Xn|≥a
|Xn|dP = 0,

MARTINGALES
221
which is a stronger requirement than condition (B) of the sampling integrability
conditions. We brieﬂy discuss some uniform integrability results for random se-
quences.
1. By deﬁnition, {Xn} is uniformly integrable if and only if ∀ε > 0, ∃a ∈R such
that if c ≥a then
R
{|Xn|>c}
|Xn|dP < ε, for all n = 1, 2, . . ., which is also equivalent to
lim
c→∞lim
n
R
|Xn|≥c
|Xn|dP = 0.
2. If {Xn} is uniformly integrable then {Xn} are integrable. Indeed, we can write
R
|Xn|dP =
R
{|Xn|>c}
|Xn|dP +
R
{|Xn|≤c}
|Xn|dP ≤ε + c < +∞.
3. Assume that {Xn} is uniformly integrable. Then sup
n
R
|Xn|dP < +∞.
4. If |Xn| ≤Y and Y is integrable then {Xn} is uniformly integrable. To see this we
write
R
{|Xn|≥c}
|Xn|dP ≤
R
{Y≥c}
YdP →0, as c →∞.
5. Suppose that |Xn| ≤M < +∞. Then {Xn} is uniformly integrable.
6. Let {Xn} be uniformly integrable. Then we can show that (a)
R
lim
n
XndP ≤
lim
n
R
XndP and (b) if Xn
a.s.
→X or Xn
p→X then X is integrable and
R
XndP →
R
XdP, as n →∞.
7. The random sequence {Xn} is uniformly integrable if and only if both of the
following hold (a)
R
|Xn|dP are uniformly bounded and (b) λn(A) =
R
A
|Xn|dP is a
uniformly continuous set function, i.e., λn(A) →0, as P(A) →0, uniformly in P.
8. If the random sequence X is a reverse MG then it is uniformly integrable.
9. If S ≤T are stopping times that satisfy the sampling integrability conditions
for a sMG X, then E(XS) ≤E(XT), since for any sMG E(Xn) ≤E(Xn+m), n, m =
1, 2, . . ..
Recall remark 3.25.1. The following remark connects Lp-convergence (Section
3.7.5) with uniform integrability and MGs.
Remark 6.4 (Martingales and Lp-convergence) We can show the following.
1. If Xn
p→X and {|Xn|q}, q > 0, are uniformly integrable then Xn
Lq
→X. Note that
the other direction is always true, namely, Xn
Lq
→X =⇒Xn
p→X and no additional
requirements are needed.
2. If {Xn} are integrable and Xn
L1
→X then E[XnIA] →E[XIA], for any A ∈A.

222
RANDOM SEQUENCES
3. If {Xn} is a MG and Xn
L1
→X then Xn = E(X|F X
n ).
4. For a sMG the following are equivalent: (a) it is uniformly integrable, (b) it
converges a.s. and in L1 and (c) it converges in L1.
5. For a MG the following are equivalent: (a) it is uniformly integrable, (b) it con-
verges a.s. and in L1, (c) it converges in L1, (d) there exists an integrable random
variable Y such that Xn = E(Y|F X
n ) a.s.
The equivalent statements (a) and (d) in remark 6.4.5 state that a MG {X, F } has
its random sequence X uniformly integrable if and only if X has the representation
Xn = E(Y|Fn), for some random variable Y. The next lemma (in view of remark
6.1.11) collects the direction (d) =⇒(a). It can be used to prove the subsequent
convergence theorem.
Lemma 6.1 (Uniformly integrability and ﬁltrations) Let Y be an integrable
random variable on (Ω, A, P) and assume that F = (F1, F2, . . . ) is a ﬁltration in
A. Then the random variables {Xi = E(Y|Fi) a.s.}i∈I, for some index set I are uni-
formly integrable.
Proof. We have
R
{|Xi|≥c}
|Xi|dP ≤
R
{|Xi|≥c}
E[|Y| |Fi]dP =
R
{|Xi|≥c}
E[|Y|]dP,
by the deﬁnition of conditional expectation and since {|Xi| ≥c} ∈Fi. By Markov
inequality we can write
P(|Xi| ≥c) ≤E(|Xi|)
c
≤E(E[|Y| |Fi])
c
= E(|Y|)
c
→0,
as c →∞, which completes the proof since P over the set {|Xi| ≥c} goes to zero.
A second MG convergence is collected next.
Theorem 6.7 (Martingale convergence) Let (Ω, A, P) be a probability space
and F = (F1, F2, . . . ) be a ﬁltration of A. Let F∞= σ
 +∞
S
n=1 Fn
!
and assume that
Y is an integrable random variable on (Ω, A, P). If we set Xn = E(Y|Fn) a.s.,
n = 1, 2, . . . , then Xn
a.s.
→E(Y|F∞) and Xn
L1
→E(Y|F∞).
Proof. From remark 6.1.11 {Xn, An} is a MG and by the previous lemma X
is uniformly integrable so that remark 6.4.5 (a) =⇒(b), yields the convergence
statements Xn
a.s.
→X∞and Xn
L1
→X∞. We need to show that this “last element” X∞
of the sequence X is of the form X∞= E(Y|F∞). The deﬁnition of Xn and remark
6.4.3 imply E(Y|Fn) = Xn = E(X∞|Fn) a.s. so that
R
F
YdP =
R
F
X∞dP

MARTINGALES
223
for all F ∈Fn. Since Y and X∞are integrable and
+∞
S
n=1 Fn is a π-system, the π −λ
theorem implies that the latter equation holds for all F ∈F∞. Finally, since X∞is
F∞-measurable it follows that X∞= E(Y|F∞).
The following is an immediate consequence of the latter theorem.
Remark 6.5 (L´evy’s 0-1 law) An important consequence of theorem 6.7
is that E(Y|X1, . . . , Xn) converges a.s. and in L1 to the random variable
E(Y|X1, . . . , Xn, . . . ). In particular, L´evy’s 0-1 law is a special case, namely, if
Fn ↑F∞(i.e., Fn and F∞as in theorem 6.7), then E(IF|Fn) →IF a.s. for any
F ∈F∞.
Both convergence theorems we have seen can be used to assess whether a se-
quence of random variables X converges a.s. since if we show it is a MG or sMG
then theorems 6.4 and 6.7 can be applied.
In our treatment of MGs thus far we have assumed that the index of X =
(X1, X2, . . . ) and F
= (F1, F2, . . . ) is an integer n = 1, 2, . . . and this n is
treated as the “discrete time” parameter of the random sequence. The case where
X = (Xt : t ≥0) and F = (Ft : t ≥0), for “continuous time” t ≥0 requires sim-
ilar deﬁnitions for stopping times and ﬁltrations and will be treated extensively in
Chapter 7 including the all-important MG problem. Clearly, the discrete parameter
case involves a countable number of random objects and is much easier to han-
dle, while the continuous time parameter case assumes an uncountable collection
of random variables, making the study of such processes more complicated. We re-
visit this discussion in Chapter 7 with the formal deﬁnitions of stochastic sequences
and processes.
The next theorem, due to Doob, suggests that in order to study convergence of
sub and super MGs, all we need to work with are MGs. For a proof see Durrett
(2010, p. 237).
Theorem 6.8 (Doob decomposition) Let {Xn, Fn} be a sMG. There exist random
variables Yn and Zn with Xn = Yn + Zn, for all n, where {Yn, Fn} is a MG, Z1 = 0,
Zn is Fn−1-measurable for n ≥2 and Zn ≤Zn+1 a.e., for all n. The Yn and Zn are
uniquely determined.
6.3.3
Applications
We end this section with some examples and applications of MGs.
Example 6.10 (Polya urns)
An urn contains r red and b black balls and after
a single ball is drawn the ball is returned and c more balls of the color drawn
are added to the urn. Let X0 = r/ (b + r) and Xn, n ≥1, denote the proportion
of red balls after the nth draw. First we verify that {Xn} is a MG. Suppose that

224
RANDOM SEQUENCES
there are rn red balls and bn black balls at time n. Then we draw a red ball with
probability rn/ (rn + bn) in which case we need to add c+1 red balls back to the urn
(including the one drawn) and hence the fraction of red balls after the (n + 1) draw
is Xn+1 = (rn + c)/ (rn + bn + c) . Similarly, we draw a black ball with probability
bn/ (rn + bn) in which case we need to add c + 1 black balls back to the urn and
hence the fraction of red balls after the (n + 1) draw is Xn+1 = rn/ (rn + bn + c) .
Therefore, we have
Xn+1 =

(rn + c) / (rn + bn + c)
w.p. rn/ (rn + bn) = Xn
rn/ (rn + bn + c)
w.p. bn/ (rn + bn) = 1 −Xn
,
so that
E(Xn+1|Xn) =
rn + c
rn + bn + c
rn
rn + bn
+
rn
rn + bn + c
bn
rn + bn
=
rn
rn + bn
= Xn,
and hence {Xn} is a MG and Doob’s convergence theorem guarantees the existence
of a limit distribution, i.e., Xn
a.s.
→X∞, for some random variable X∞. The reader can
verify that the distribution of the limit X∞is a Beta (r/c, b/c) with density
fX∞(x) = xr/c−1(1 −x)b/c−1
B (r/c, b/c)
,
for 0 ≤x ≤1.
Example 6.11 (Radon-Nikodym derivatives)
Let (Ω, A, P) be a probability
space, F = (F1, F2, . . . ) a ﬁltration of A such that Fn ↑F∞and suppose that µ is
a ﬁnite measure on (Ω, A). Denote the restrictions of P and µ on (Ω, Fn) by Pn and
µn, respectively. Suppose that µn ≪Pn and deﬁne Xn =
h dµn
dPn
i
. First note that Xn is
integrable [P] and Fn-measurable by deﬁnition so that we can write
R
F
XndP = µ(F),
for all F ∈Fn. But Fn ⊂Fn+1 and therefore F ∈Fn+1 so that
R
F
Xn+1dP = µ(F).
Since the two integrals are equal we have
R
F
XndP =
R
F
Xn+1dP,
thus establishing that {Xn, Fn} is a MG.
Example 6.12 (Likelihood ratios) Let X = (X1, X2, . . . ) be a random sequence
and suppose that the joint densities (models) of (X1, . . . , Xn) are either pn or qn. To
help us choose one of the two models we introduce the random variables
Yn = Yn(X1, . . . , Xn) = qn(X1, . . . , Xn)
pn(X1, . . . , Xn),
for n = 1, 2, . . . and we expect Yn to be small or large depending on whether the
true density is pn or qn, respectively. Therefore, we are interested in the asymptotic
behavior of such ratios. First note that σ(Y1, . . . , Yn) = σ(X1, . . . , Xn). Wlog assume
that pn is positive. If pn is the true density then the conditional density of Xn+1 given
X1, . . . , Xn, is the ratio pn+1/pn and hence we can show that
E(Yn+1|X1 = x1, . . . , Xn = xn) = Yn(x1, . . . , xn),

MARTINGALES
225
for any x1 ∈R, . . . , xn ∈R, that is, {Yn} form a MG.
Example 6.13 (Gambling: double or nothing) Let X be a random sequence for
which the conditional distribution of Xn+1 given σ(X0, X1, X2, . . . ) is the discrete
uniform distribution on the two-point set {0, 2Xn}, where we have taken X0 = 1.
This sequence represents the fortune of a gambler that starts with $1 and bets his
entire fortune at each stage of a fair game. After each bet the fortune either doubles
or vanishes, with each event occurring with probability 1/2. Clearly, {Xn} is a MG
since
E(Xn+1|X0, . . . , Xn) = 2Xn0.5 = Xn a.s.,
and the remaining conditions are trivially satisﬁed by the sequence X. Deﬁne
T(ω) = inf{n > 0 : Xn(ω) = 0}, the ﬁrst hitting time of 0. Then T is a.s. ﬁnite
and since E(XT) = E(0) = 0 < 1 = E(X0), T does not satisfy the sampling integra-
bility conditions for {Xn}. In particular, condition (A) is satisﬁed but (B) is not.
6.3.4
Wald Identities and Random Walks
We connect random walks with the sampling integrability conditions (p. 218)
and MGs next.
Theorem 6.9 (Wald identities) Let S = (S 0 = 0, S 1, S 2, . . . ) be a random walk
on R with steps X = (X1, X2, . . . ) and E(|Xi|) < +∞. If τ is a stopping time that
satisﬁes the sampling integrability conditions then
E(S τ) = E(S 1)E(τ),
(6.5)
and
Var(S τ) = Var(S 1)E(τ),
(6.6)
where we agree that 0 ∗∞= 0.
Proof. Let µ = E(S 1) so that E(S n) = nµ and deﬁne Yn = S n −nµ. Then
{Yn : n ≥0} is a random walk whose step distribution has mean zero and hence it is
a MG. Now apply the Optional Sampling Theorem with Tn = min{T, n} to obtain
0 = E(YTn) = E(S Tn) −µE(Tn).
By the MCT we have
µE(T) = µ lim
n→+∞E(Tn) = lim
n→+∞E(S Tn),
(6.7)
and the ﬁrst sampling integrability condition E(|S T|) < +∞. Thus the second sam-
pling integrability condition and the DCT imply that
lim inf
n→+∞E(|S Tn −S T|)
≤
lim inf
n→+∞E (|S Tn| + |S T|)I(T > n)
≤
lim inf
n→+∞E |S Tn|I(T > n) + lim sup
n→+∞
E [|S T|I(T > n)] = 0.
The latter equation implies that S Tn
L1
→S T and since the steps are integrable, remark
6.2.2 with A = Ωyields E[S Tn] →E[S T], as n →+∞. Combine this with (6.7)

226
RANDOM SEQUENCES
and we have (6.5).
Now to prove (6.6), ﬁrst note that from (6.5) we have E(S T) = 0 and hence
Var(S T) = E(S 2
T). Deﬁne for all n ≥0, Zn = S 2
n −nσ2 and note that
E(Zn+1|S 0, S 1, ..., S n) = Zn,
so that {Zn} is a MG with respect to the minimal ﬁltration of S . We follow similar
steps as above and establish (6.6).
We ﬁnish this ﬁrst treatment of random walks with the following theorem. The
proof is requested as an exercise.
Theorem 6.10 (Conditional independence) Let (S 0 = 0, S 1, S 2, . . . ) a random
walk. Then for any n > 0 we can show that (S 0, S 1, S 2, . . . , S n) and (S n, S n+1, . . . )
are conditionally independent given σ(S n).
6.4
Renewal Sequences
We are interested in an important class of random sequences which is deﬁned
in terms of random walks.
Deﬁnition 6.10 Renewal sequences via random walks
Consider a random walk T = {Tm : m = 0, 1, 2, . . .} in Z
+ satisfying
Tm+1(ω) ≥1 + Tm(ω), and deﬁne the random sequence X = (X1, X2, . . . ) by
Xn(ω) = 1, if Tm(ω) = n, for some m ≥0 and 0, otherwise.
1. Renewal sequence X is called the renewal sequence corresponding to T. We
may write Xn(ω) =
+∞
P
m=0 I(Tm(ω) = n). The random walk T can be obtained from X
by setting T0(ω) = 0 and Tm = inf{n > Tm−1 : Xn = 1}, m > 0, where inf ∅= +∞.
2. Times of renewals The integers n > 0 for which Xn = 1 are called renewal
times. If Tm < ∞then Tm is called the mth renewal time.
3. Waiting times The time diﬀerence Tm −Tm−1 is called the mth waiting time. The
distribution of the ﬁrst renewal time T1 is known as the waiting distribution W (it is
the same as the step distribution of the random walk T).
Suppose that X is a 0-1 valued random sequence. The next theorem helps us
assess when X is a renewal sequence (or if T is a random walk). The proof is
requested as an exercise.
Theorem 6.11 (Renewal criterion) A random 0-1 sequence X = (X0, X1, X2, . . .

RENEWAL SEQUENCES
227
) is a renewal sequence if and only if X0 = 1 a.s. and
P(Xn = xn, 0 < n ≤r + s) = P(Xn = xn, 0 < n ≤r)P(Xn−r = xn, r < n ≤r + s),
(6.8)
for all r, s > 1 and all 0-1 sequences (x1, . . . , xr+s) such that xr = 1.
The latter theorem states that renewal processes are 0-1 sequences that start
over independently after each visit to state 1, i.e., they “regenerate” themselves.
Note that if xr = 0, equation (6.8) does not necessarily hold for a renewal sequence
(x0, x1, . . . ). In view of this the random set ΣX = {n : Xn = 1} is called a regenerative
set in Z+.
Two of the important quantities we need in order to study renewal processes are
the renewal and potential measures.
Deﬁnition 6.11 Renewal and potential measures
Let X = (X0, X1, X2, . . . ) denote a renewal sequence.
1. Renewal measure The random σ-ﬁnite measure R given by R(B) = P
n∈B Xn is
called the renewal measure of X, where R(B) describes the number of renewals
occurring during a time set B.
2. Potential measure The function Π(B) = E (R(B)) = E
 P
n∈B Xn
!
, is a (non-
random) σ-ﬁnite measure and is called the potential measure of the renewal se-
quence X.
3. Potential sequence The density of the potential measure Π of X with respect
to the counting measure is the potential sequence of X. It satisﬁes πn = Π({n}) =
P(Xn = 1), where (π0 = 1, π1, π2, . . . ) is the potential sequence of X.
We collect some results regarding the renewal and potential measures below.
Remark 6.6 (Renewal and potential measures) Let W be a waiting time distribu-
tion for a renewal sequence X corresponding to a random walk T in Z
+. For any
set B ⊆Z+ and any non-negative integer m, it follows that W∗m(B) = P(Tm ∈B),
where W∗0 is the delta distribution at 0 and since Tm = (Tm −Tm−1)+(Tm−1−Tm−2)+
. . . (T1 −T0), with each step Tm −Tm−1 of the random walk T being iid distributed
according to W.
1. Relating Π and W We can show that the potential measure Π and the waiting
time distribution W are related via
Π(B) =
+∞
P
m=0 W∗m(B),
(6.9)
for all sets B ⊆Z+. To see this, ﬁrst note that Xn =
+∞
P
m=0 I(Tm = n), with W∗m({n}) =

228
RANDOM SEQUENCES
E (I(Tm = n)) and use the Fubini theorem to obtain
Π(B)
=
P
n∈B E(Xn) = P
n∈B E
 +∞
P
m=0 I(Tm = n)
!
=
+∞
P
m=0
P
n∈B E (I(Tm = n))
=
+∞
P
m=0
P
n∈B W∗m({n}) =
+∞
P
m=0 W∗m(B).
2. Measure generating function The pgf of a distribution Q on Z+
0, is deﬁned by
ρ(s) =
+∞
P
n=0 Q({n})sn, 0 ≤s ≤1. It is useful to extend this deﬁnition to measures other
than probability measures and we accomplish this by removing 1 from the domain.
In particular, the measure generating function of a measure Π on Z+
0 that has a
bounded density (π0, π1, . . . ) with respect to the counting measure is the function
φ(s) =
+∞
P
n=0 πnsn,
(6.10)
for 0 ≤s < 1. It is straightforward to show that two measures on Z+
0 with bounded
densities are equal if their corresponding measure generating functions are the
same.
3. Consider a renewal sequence with W and Π the waiting time distribution and
potential measure, respectively, and let φW denote the measure generating function
of W.
(i) Then we can show that the measure generating function of Π is φΠ(s) =
 1 −φW(s)−1 , for 0 ≤s < 1.
(ii) Furthermore, we have Π = (Π ∗W) + δ0, where δ0 is the delta distribution at 0.
4. Let Wi, Πi, φi, i = 1, 2, the waiting time distribution, potential measure and
measure generating function of Wi, respectively, for two renewal sequences. The
following are equivalent: (i) W1 = W2, (ii) Π1 = Π2, (iii) φ1 = φ2.
Example 6.14 (Potential sequence) Consider the sequence u = (1, 0, 1/4, 1/4,
. . . ) . We show that it is a potential sequence. Note that the measure generating
function of u is given by ψ(s) = 1 + s2/ (4(1 −s)) . Setting this to (1 −ϕ)−1 yields
ϕ(s) = s2/ (2 −s)2 , the entertained measure generating function of Π. To show
that the given sequence is a potential sequence, we only need to show that ϕ is the
measure generating function of some probability distribution on Z+. For that we
need to obtain its expansion as a power series with coeﬃcients that are positive, the
coeﬃcient of s0 is 0 and φ(1−) < 1. At the same time we will get a formula for the
waiting time distribution W. Using the Binomial theorem we have
ϕ(s) = s2(2 −s)−2 = (s2/4)
+∞
P
n=0C−2
n (−s/2)n =
+∞
P
n=2 C−2
n−2 (−s/2)n =
+∞
P
n=1(n −1)2−nsn,
and therefore W({n}) = (n −1)2−n, n > 0.
Next we obtain some results involving the renewal measure R of X. The ﬁrst is
requested as an exercise.

RENEWAL SEQUENCES
229
Theorem 6.12 (Total number of renewals) Let X denote a renewal sequence
with potential measure Π and waiting time distribution W. If Π(Z+
0) < ∞then
the distribution of R(Z+
0) (the total number of renewals) is geometric with mean
Π(Z+
0). If Π(Z+
0) = ∞then R(Z+
0) = ∞a.s. and in any case Π(Z+
0) = 1/W({∞})
(with 1/0 = ∞).
Many asymptotic results for a random sequence X can be proven easily via
asymptotic results on an underlying renewal sequence used to describe X. The fol-
lowing SLLN will be utilized later in this chapter in order to study the limiting
behavior of Markov chains.
Theorem 6.13 (SLLN for renewal sequences) Let R denote the renewal mea-
sure, Π the potential measure, X the renewal sequence and µ =
+∞
P
n=1 nW({n}),
µ ∈[1, ∞], the mean of the waiting time distribution W. Then we can show that
lim
n→+∞Π({0, 1, . . ., n})/n = lim
n→+∞R({0, 1, . . ., n})/n = lim
n→+∞
nP
k=0 Xk/n = 1/µ a.s.
(6.11)
Proof. Recall that Π({0, 1, ..., n}) = E (R({0, 1, ..., n})) = E
 nP
k=0 Xk
!
and note
that Rn = R({0, 1, ..., n}) ≤n. Then using the BCT on the sequence Rn/n we estab-
lish the ﬁrst and second equalities in (6.11).
To prove the third equality, let T denote the random walk corresponding to X
and assume ﬁrst that µ < +∞. Note that for a ﬁxed ω, n ∈[Tm(ω), Tm+1(ω)] implies
that Rn(ω) = m so that
TRn(ω)−1(ω) ≤n ≤TRn(ω)(ω) ⇒TRn(ω)−1(ω)
Rn(ω) −1
Rn(ω) −1
Rn(ω)
≤
n
Rn(ω) ≤TRn(ω)(ω)
Rn(ω) .
(6.12)
Now by the SLLN Tm/m →µ a.s. as m →+∞, since Tm = T1 + (T2 −T1) +
... + (Tm −Tm−1) is a sum of iid random variables with distribution W. Furthermore,
lim
n→+∞Rn = +∞since if lim
n→+∞Rn = R(Z+) = M < ∞, for some ﬁnite M then by
the deﬁnition of Rn we would have TM+1 = +∞, a contradiction, because TM+1 has
ﬁnite mean (E(TM+1) = (M + 1)µ). Consequently,
lim
n→+∞
TRn(ω)(ω)
Rn(ω)
= lim
m→+∞
Tm(ω)
m
= µ a.s.,
so that taking the limit as n →+∞in (6.12) yields lim
n→+∞Rn/n = 1
µ a.s., as desired.
The case µ = +∞follows similar steps. First note that
TRn(ω)−1(ω) ≤n ⇒
n
Rn(ω) ≥TRn(ω)−1(ω)
Rn(ω) −1
Rn(ω) −1
Rn(ω)
,

230
RANDOM SEQUENCES
so that if lim
n→+∞Rn = +∞we have lim
n→+∞
n
Rn ≥µ = ∞or lim
n→+∞
Rn
n = 0. When lim
n→+∞Rn <
+∞we have right away that lim
n→+∞
Rn
n = 0.
We are now ready to state a classiﬁcation result for renewal sequences. As we
will see classiﬁcation of the states of any random sequence can be accomplished
via classiﬁcation of an appropriate renewal sequence.
Deﬁnition 6.12 Renewal sequence classiﬁcation
A renewal sequence and the corresponding regenerative set are called transient
if the renewal measure is ﬁnite a.s. and they are called recurrent otherwise. A recur-
rent renewal sequence is called recurrent null if the mean waiting time is inﬁnite;
otherwise, it is called positive recurrent (or recurrent non-null).
A simple modiﬁcation of a renewal sequence is required, if we do not have a
renewal at the initial state of the sequence.
Remark 6.7 (Delayed renewal sequence) Let S
= (S 0 = 0, S 1, S 2, . . . ) be
a random walk in Z and for a ﬁxed integer x consider the random sequence
X = (X0, X1, . . . ) deﬁned by
Xn = I(S n = x) =

1
S n = x
0
S n , x ,
so that X marks the visits made by S to x. If x , 0 then X is not a renewal sequence.
However, once the point x is reached by S then thereafter X behaves like a renewal
sequence.
Motivated by the latter example we have the formal deﬁnition of a delayed
renewal sequence.
Deﬁnition 6.13 Delayed renewal sequence
Let X be an arbitrary 0-1 sequence and let T = inf{n ≥0 : Xn = 1}. Then X
is called a delayed renewal sequence if either P(T = ∞) = 1 or P(T < ∞) < 1
and conditional on the event {T < ∞} the sequence Y = (XT+n : n = 0, 1, . . .) is a
renewal sequence.
The random variable T is called the delay time and the distribution of T is
called the delay distribution of X. The waiting time distribution of Y is the same as
that of X. Note that X reduces to a renewal sequence if and only if P(X0 = 1) = 1.
The last classiﬁcation type involves the concept of periodicity.
Deﬁnition 6.14 Period of a renewal sequence
The period γ of a renewal sequence is the greatest common divisor (GCD) of the

RENEWAL SEQUENCES
231
support of the corresponding potential measure with the GCD taken to be ∞if the
support is {0}. Note that the period is also the GCD of the support of the waiting
time distribution.
Example 6.15 (Renewal sequence period) Let Π be the potential measure and
assume that its support is S = {1, 2, 3, 4, . . .}. Then the period of the corresponding
renewal sequence is 1 and if S = {2, 4, 6, 8, . . .} then the period is 2.
The following theorem can be useful in studying the asymptotic behavior of
random sequences. The proof is requested as an exercise.
Theorem 6.14 (Renewal theorem) Let {πn : n ≥0} denote the potential se-
quence and µ =
+∞
P
n=1 nW({n}), µ ∈[1, ∞], the mean of the waiting time distribution
W, of a renewal sequence with ﬁnite period γ. Then we can show that
lim
n→+∞πnγ = γ
µ.
(6.13)
Next we give an alternative classiﬁcation criterion based on the potential se-
quence.
Corollary 6.2 (Classiﬁcation) Let {πn : n ≥0} denote the potential sequence of a
renewal sequence X. Then X is recurrent if and only if
+∞
P
n=0 πn = +∞, in which case
X is null recurrent if and only if lim
n→+∞πn = 0.
Proof. Follows immediately from theorem (6.12) and theorem (6.14).
The following remark summarizes several results connecting random walks and
renewal sequences.
Remark 6.8 (Random walks and renewal sequences) Let S
=
(S 0
=
0, S 1, S 2, . . . ) be a random walk in Z with step distribution Q and consider the
renewal sequence X deﬁned by Xn = I(S n = 0), n ≥0.
1. Classiﬁcation The random walk is called transient, positive recurrent or null
recurrent depending on the classiﬁcation of the renewal sequence X.
2. Potential sequence The potential sequence {πn : n ≥0} of X is related to the
step distribution as follows
πn = P(Xn = 1) = P(S n = 0) = Q∗n({0}).
(6.14)
3. Step and waiting time distributions Let β denote the cf of Q, i.e., β(u) =
R
eiuxQ(dx). From equation (6.14) and the inversion theorem (p. 196) we have
πn = Q∗n({0}) = (2π)−1
πR
−π
βn(u)du,

232
RANDOM SEQUENCES
where βn is the cf of Q∗n. Multiply by sn and sum over all n ≥0 to obtain

1 −φT1(s)
−1 =
+∞
P
n=0 πnsn = (2π)−1
πR
−π
(1 −sβ(v))−1 dv,
where φT1 is the measure generating function of the waiting time distribution W (or
ﬁrst return time T1 = inf{n > 0 : S n = 0}) of X. Note that we used remark 6.6.3,
part (i), for the LHS above. As a consequence, we have
φT1(s) = 1 −
"
(2π)−1
πR
−π
(1 −sβ(v))−1 dv
#−1
,
and we have related the ﬁrst return time T1 to 0 by S, with the cf of the step distri-
bution Q.
4. Every random walk on Z is either transient or null recurrent (except for the
random walk with S n = 0, for all n).
Example 6.16 (Potential sequence for a simple random walk)
Consider the
simple random walk S on Z with step distribution Q({1}) = p = 1 −Q({−1})
and deﬁne Xn = I{0} ◦S n, n ≥0. The cf of Q is
β(u) = E(eiuS 1) = peiu + (1 −p)e−iu,
u ∈R, so that the cf of S n ∼Q∗n is given by
βn(u) =

peiu + (1 −p)e−iun = E(eiuS n) =
+∞
P
k=0 eiukP(S n = k).
By the inversion theorem the density corresponding to βn (i.e., πn = Q∗n({0})) is
given by
πn = (2π)−1
πR
−π
βn(u)du = (2π)−1
nP
k=0 Cn
k pk(1 −p)n−k
πR
−π
e(2k−n)iudu.
and since
πR
−π
e(2k−n)iudu = 2 sin((2k −n)π)/(2k −n),
we can write
πn = (2π)−1
nP
k=0 Cn
k pk(1 −p)n−k2 sin((2k −n)π)/(2k −n).
Note that as k →n/2 we have lim
n→+∞sin((2k−n)π)/(2k−n) = π, while if k , n/2 we
have sin((2k−n)π)/(2k−n) = 0. Consequently, we can write the potential sequence
as πn = (2π)−1Cn
n/2pn/2(1 −p)n−n/22π and thus
πn = Cn
n/2,n/2pn/2(1 −p)n/2,
for n even. For odd n, we have πn = 0, since renewals (visits to 0) can only happen
at even times.

MARKOV CHAINS
233
6.5
Markov Chains
We have seen many sequences of random variables where the future states of
the process are independent of the past given the present, and this is the idea behind
the Markov property. We begin with the general deﬁnition of a Markov chain.
6.5.1
Deﬁnitions and Properties
Deﬁnition 6.15 Markov chain
The random sequence X = {Xn : n ≥0} that is adapted to a ﬁltration F = {Fn :
n ≥0} is called a Markov chain with respect to the ﬁltration F if ∀n > 0, Fn and
σ({Xm : m ≥n}) are conditionally independent given σ(Xn).
1. Transition function The transition functions Rn of a Markov chain X are de-
ﬁned as the conditional distributions of Xn+1 given σ(Xn), n ≥0. The distribution
R0 is called the initial distribution and Rn, n > 0, is called the nth transition function
(or kernel).
2. Time homogeneity If Rn is the same for all n > 0 then X is called time-
homogeneous (R1 = R2 = . . . ).
For any Markov chain X we may write
R(Xn, B) = P(Xn+1 ∈B|σ(X0, X1, . . . , Xn)) = P(Xn+1 ∈B|σ(Xn)),
for the nth transition function and read it as “the probability that the Markov chain
is in states B at time n + 1 given that it was at state Xn at time n.”
The study of Markov chains in discrete time, and Markov processes (Chapter
7) in continuous time is uniﬁed via the concept of a transition operator.
Deﬁnition 6.16 Transition operator
Let M = {µx : x ∈Ψ}, denote a collection of distributions where Ψ is a Borel
space (see appendix remark A.4) and assume that for each measurable set B ⊆Ψ
the function x 7→µx(B) is measurable (ﬁxed B).
1. Left transition operator The left transition operator T corresponding to the
collection M of transition distributions operates on the space of all bounded, R-
valued measurable functions on Ψ and is given by
(T(f ))(x) =
R
Ψ
f (y)µx(dy).
(6.15)
To simplify the notation we write T f (x).
1. Right transition operator The right transition operator corresponding to M,
also denoted by T, operates on the space of probability measures on Ψ and is given

234
RANDOM SEQUENCES
by
(Q(T))(B) =
R
Ψ
µx(B)Q(dx).
(6.16)
We simplify the notation by writing QT(B).
We connect transition functions and operators below.
Remark 6.9 (Transition functions and operators) Transition functions and op-
erators are related as follows. Given a transition function R the transition distribu-
tions are given by µx(.) = R(x, .), x ∈Ψ. The collection of transition distributions
M = {µx : x ∈Ψ} uniquely determines the left and right transition operators T so
that a transition function R uniquely determines a corresponding transition operator
T. On the other hand since
TIB(x) =
R
Ψ
IB(y)µx(dy) =
R
B
µx(dy) = µx(B),
for all x ∈Ψ and measurable sets B then a left transition operator T uniquely
determines a corresponding collection M, which in turn determines a transition
function R. Similarly, a right transition operator T determines the collection M and
the transition function since
δxT(B) =
R
Ψ
µy(B)δx(dy) = µx(B),
where δx(y) =

1
y = x
0
y , x , the delta distribution at x. Thus there is a one-to-one
correspondence between transition operators (left or right) and transition functions.
Next, we reformulate deﬁnition 6.15 in terms of transition operators as follows.
Deﬁnition 6.17 Time-homogeneous Markov chain
Let T denote a transition operator deﬁned in terms of a collection of probability
measures M = {µx : x ∈Ψ}, where Ψ is a Borel space. A random sequence X =
{Xn : n ≥0} of Ψ-valued random variables that is adapted to a ﬁltration F = {Fn :
n ≥0}, is called a time-homogeneous Markov chain with respect to the ﬁltration F
with state space Ψ and transition operator T if ∀n ≥0 the conditional distribution
of Xn+1 given Fn, is µXn.
Unless otherwise stated, for what follows we assume that the Markov chain X
is time-homogeneous and therefore µXn does not depend on n.
Example 6.17 (Transition operators of a Markov chain)
Let X be a Markov
chain with respect to a ﬁltration F with transition operator T and let Qn denote the
distribution of Xn. For a bounded measurable function f on Ψ we have
Qn+1(B)
=
P(Xn+1 ∈B) = E(P(Xn+1 ∈B|Fn)) = E(µXn(B))
=
R
Ψ
µx(B)Qn(dx) = (QnT)(B),

MARKOV CHAINS
235
so that Qn+1 = QnT. Moreover we have
E(f (Xn+1)|Fn) =
R
Ψ
f (y)µXn(dy) = (T f )(Xn).
The following remark presents several results on Markov chains.
Remark 6.10 (Markov chains) Let X = {Xn : n ≥0} be a Markov chain on Ψ
adapted to a ﬁltration F = {Fn : n ≥0}.
1. A time-homogeneous Markov chain consists of 1) a state space Ψ, 2) a ﬁltration
(typically the minimal), 3) a transition operator T, and 4) the distribution of the ﬁrst
term in the sequence, Q0.
2. The distribution of the Markov chain X is uniquely determined if we know T
and Q0. The converse is not true. Indeed, let Ψ = {0, 1} and consider the sequence
X = (0, 0, 0, . . .) (deterministic). Then we can ﬁnd two diﬀerent operators T1 and
T2 such that X is a Markov chain with respect to T1 and a Markov chain with respect
to T2.
3. Let v be a probability measure on Ψ and f : Ψ →R a bounded function and
then deﬁne vf =
R
Ψ
f dv. We show that (Q0T)(f ) = Q0(T f ). Indeed
(Q0T)(f ) =
R
Ψ
f (y)(Q0T)(dy) =
R
Ψ
f (y)
R
Ψ
µx(dy)Q0(dx) =
R
Ψ

R
Ψ
f (y)µx(dy)
Q0(dx),
so that
(Q0T)(f ) =
R
Ψ
(T f )(x)Q0(dx) = Q0(T f ).
4. Let T k denote the k-fold composition of T with itself deﬁned recursively for any
k ≥0, (for k = 0 it is deﬁned as the identity operator T 0 = I) based on
T 2 f (x) = T(T f (x)) =
R
Ψ
T f (y)µT f(x)(dy).
Then we can show that QnT k = Qn+k,
E(f (Xn+k)|Fn) = (T k f )(Xn) a.s.,
and
E(f (Xk)|X0 = x) = (T k f )(x),
for k ≥0.
Most properties of a Markov chain can be studied via generators and we collect
the deﬁnition next.
Deﬁnition 6.18 Discrete generator
Let X be a Markov chain with transition operator T. The discrete generator of X
is the operator G = T −I, where I is the identity operator.
It is very often the case that we view time as a random variable. The following

236
RANDOM SEQUENCES
deﬁnition suggests that if we replace a ﬁxed time n with an a.s. ﬁnite stopping time
τ, the Markov property is preserved and moreover, results obtained for ﬁxed time
n can be extended similarly for the stopping time τ (e.g., see exercises 6.28 and
6.30).
Deﬁnition 6.19 Strong Markov property
Let X be a Markov chain with respect to a ﬁltration F with transition operator T
and assume that τ is any a.s. ﬁnite stopping time relative to F . Then the random
sequence Yn = Xτ+n, n ≥0, is a Markov chain with transition operator T and it is
adapted to the ﬁltration {Fτ+n : n ≥0}. In this case we say that X has the strong
Markov property.
For discrete stopping times τ the extension from ﬁxed n to random times τ
is straightforward and the strong Markov property coincides with the standard
Markov property. This is not the case for continuous stopping times (see deﬁni-
tion 7.8).
6.5.2
Discrete State Space
Calculations simplify signiﬁcantly when the state space Ψ is discrete (ﬁnite or
countable). Assume that Ψ = {1, 2, . . ., m} (the case m = +∞can be developed
similarly). A transition operator T in this case is thought of as an m × m matrix
T = [(T(x, y)], where
1) T(x, y) ≥0, for all 1 ≤x, y ≤m, and
2) each row sums up to 1, i.e.,
mP
y=1 T(x, y) = 1, x = 1, 2, . . ., m, since µx({y}) =
T(x, y), where M = {µx : x ∈Ψ} are the transition distributions.
A transition matrix with these two properties is known as a Markov matrix over
Ψ. Note that
P(Xn+1 = y) =
mP
x=1 P(Xn+1 = y|Xn = x)P(Xn = x),
so that T(x, y) = P(Xn+1 = y|Xn = x) does not depend on n and Qn+1 = QnT. Now if
f is a vector, fm×1 = (f (1), . . . , f (m))′, then the matrix T operates on f by multipli-
cation on the left so that the notation T f is consistent with matrix multiplication,
that is
(T f )(x) =
R
Ψ
f (y)µx(dy) = Tm×m fm×1,
where µx(dy) = T(x, dy). Similarly, as a transition operator from the right T op-
erates on probability measures that are row vectors, Q1×m = (Q({1}), . . . , Q({m})).
Then the probability measure QT is identiﬁed with the vector Q1×mTm×m.
We present some results connecting transition probabilities with transition ma-
trices below.

MARKOV CHAINS
237
Remark 6.11 (Transition matrices) Let X be a Markov chain with transition ma-
trix T and state space Ψ.
1. For any n ≥0, k > 0 and x0, x1, . . . , xk ∈Ψ, we have
P(Xn+1 = x1, . . . , Xn+k = xk|Xn = x0) = T(x0, x1) . . . T(xk−1, xk),
so that
P(X0 = x0, . . . , Xk = xk) = Q0({x0})T(x0, x1) . . . T(xk−1, xk),
where Q0 is the initial distribution of X.
2. For any k ≥0 and x, y ∈Ψ we have
P(Xn+k = y|Xn = x) = T k(x, y),
with T 0 = I, the identity matrix, where T k(x, y) denotes the probability that the
Markov chain moves from state x to state y in k-steps and is obtained as the (x, y)th
element of the kth power of the transition matrix T. The probability distribution
Qk(= Q0T k) of Xk is easily found using
P(Xk = y) =
mP
x=1 P(Xk = y|X0 = x)P(X0 = x) =
mP
x=1 T k(x, y)P(X0 = x).
3. Chapman-Kolmogorov equations For any n, k ≥0, we have T k+n = T kT n or
in terms of the elements of the matrices
T k+n(x, y) = P
z∈Ψ T k(x, z)T n(z, y),
for any x, y ∈Ψ.
We collect the matrix development of transition operators we have discussed
thus far in the following theorem.
Theorem 6.15 (Matrix representation) Assume that T is a transition operator
for a Markov chain X with countable state space Ψ. Then T can be represented
as a matrix, denoted by T, with
T(x, y) = δxT fy = P(X1 = y|X0 = x),
x, y ∈Ψ, where δx is the delta function at x and fy is the indicator function of
the singleton {y}. The representation has the property that for any distribution
Q and bounded function f on Ψ, the distribution QT and the function T f are
represented by the corresponding matrix products QT and T f and for any k ≥0
the operator T k is represented by the matrix product T k.
Example 6.18 (Gambler’s ruin)
Let Ψ = {0, 1, 2, . . ., 10} and let X0 = 5, the

238
RANDOM SEQUENCES
starting state of the sequence. Consider the transition matrix
T11×11 =

1
0
0
0
0
3/5
0
2/5
0
0
. . .
. . .
. . .
. . .
. . .
0
0
3/5
0
2/5
0
0
0
0
1

,
so that T(x, y) = P(Xn+1 = y|Xn = x), is the transition operator of a Markov
chain that describes the gambler’s ruin problem, in which the gambler starts
with $5 and wins $1 with probability 2/5 and loses $1 with probability 3/5.
The game ends if he reaches 0 or 10 dollars. The gambler’s ruin at time
3 is a random variable with probability distribution given by the row vector
(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0)T 3 = (0, 0, 0.216, 0, 0.432, 0, 0.288, 0, 0.064, 0, 0) = Q3 =
(P(X3 = 0), P(X3 = 1), . . . , P(X3 = 10)), since Q0 = (0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0).
After 20 games the distribution is Q20 = Q0T 20 = (0.5748, 0.0678, 0, 0.1183,
0, 0.0975, 0, 0.0526, 0, 0.0134, 0.0757), with expected fortune after 20 games
given by $2.1555. In terms of the transition probability measures µx, we require
that µ0({0}) = 1 = µ10({10}) and µx({x −1}) = 3/5, µx({x + 1}) = 2/5, x = 1, . . . , 9,
otherwise µx({y}) = 0, for y ∈{x −1, x + 1}.
Now if the gambler keeps playing to win as much as he can (no upper bound)
then the state space is Ψ = {0, 1, 2, . . .} and the transition operator assumes the form
T =

1
0
0
0
. . .
3/5
0
2/5
0
. . .
0
3/5
0
2/5
. . .
0
0
3/5
0
. . .
. . .
. . .
. . .
. . .
. . .

,
which is nothing but a random walk in Z+
0 with step distribution Q({1}) = 2/5 =
1 −Q({−1}). In terms of the transition probability measures µx this Markov chain
requires µ0({0}) = 1.
Example 6.19 (Birth-Death sequences)
A Birth-Death sequence is a Markov
chain X = {Xn : n ≥0} with state space Z+
0 and transition probability measures
µx that satisfy µx({x −1, x, x + 1}) = 1, x > 0 and µ0({0, 1}) = 1. We can think of
Xn as if it represents the size of a population at time n. In terms of the transition
matrix T, T(x, y) = 0, if |x −y| > 1 and therefore |Xn+1 −Xn| ≤1 a.s. for all n ≥0.
If Xn+1 = Xn + 1 we say that a birth occurs at time n, whereas, if Xn+1 = Xn −1
we say that a death occurs at time n. If all the diagonal elements of T are 0 then all
transitions are births or deaths. If for some x > 0, T(x, x) > 0, then the population is
the same at time n +1, i.e., Xn+1 = Xn. Clearly, X as deﬁned is a time-homogeneous
Markov chain.

MARKOV CHAINS
239
Example 6.20 (Random walks) Let S = (S 0 = 0, S 1, . . . ) denote a random walk
on Z+
0 with step distribution {pk : k ∈Z+
0} and steps X = {Xn : n ∈Z+}. Since
S n+1 = S n + Xn+1 we have
P(S n+1 = y|S 0, . . . , S n) = P(Xn+1 = y −S n|S 0, . . . , S n) = py−Xn a.s.,
by the independence of Xn+1 and S 0, . . . , S n. Thus S is a Markov chain whose tran-
sition probabilities are T(x, y) = P(S n+1 = y|S n = x) = py−x, y ≥x, that is, the
transition matrix is of the form
T =

p0
p1
p2
p3
. . .
0
p0
p1
p2
. . .
0
0
p0
p1
. . .
. . .
. . .
. . .
. . .
. . .

.
Example 6.21 (Binomial process) Let Nn denote the number of successes in n
Bernoulli trials with p = P(S uccess) (recall example 6.1.2). Since Nn+m −Nm|
N0, N1, N2, . . . , Nm
d= Nn+m −Nm
d= Nn ∼Binom(n, p), the Binomial process N =
{Nn : n ≥0} is a Markov chain with state space Ψ = Z+
0, initial distribution
Q0({0}) = 1, Q0({x}) = 0, x ∈Z+ and transition probabilities
T(x, y) = P(Nn+1 = y|Nn = x) =

p
y = x + 1
q
y = x
0
otherwise
,
where q = 1 −p, so that the transition matrix is
T =

q
p
0
0
. . .
0
q
p
0
. . .
0
0
q
p
. . .
. . .
. . .
. . .
. . .
. . .

.
The m-step transition probabilities are easily computed as
T m(x, y) = P(Nn+m = y|Nn = x) = P(Nm = y −x) = Cm
y−xpy−xqm−y+x,
y = x, . . . , m + x.
Example 6.22 (Times of successes)
Let Tk denote the time that the kth suc-
cess occurs in a Bernoulli process (recall example 6.1.3). Since Tk+m −
Tm|T1, T2, . . . , Tm
d= Tk+m −Tm
d= Tk ∼NB(k, p), T = {Tk : k ∈Z+
0} is a Markov
chain with state space Ψ = {0, 1, 2, . . .}. Since T0 = 0, the initial distribution is
Q0({0}) = 1, Q0({x}) = 0, x ∈Z+. The transition probabilities are easily computed
as
T(x, y) = P(Tk+1 = y|Tk = x) = P(T1 = y −x) =

pqy−x−1
y ≥x + 1
0
otherwise ,

240
RANDOM SEQUENCES
so that the transition matrix is of the form
T =

0
p
pq
pq2
. . .
0
0
p
pq
. . .
0
0
0
p
. . .
. . .
. . .
. . .
. . .
. . .

.
For the m-step transition probabilities we have
T m(x, y) = P(Tk+m = y|Tk = x) = P(Tm = y −x) = Cy−x−1
m−1 pmqy−x−m,
for y ≥x + m.
Example 6.23 (Branching processes) Let µ be a probability distribution on Z+
0.
A branching process with branching distribution µ (also known as the oﬀspring dis-
tribution) is a Markov chain on Z+
0 having transition probability measures µx = µ∗x.
Branching processes can be interpreted as population models (like Birth-Death pro-
cesses). More precisely, if the process is at state x we think of the population as
having x members each reproducing independently. During a “birth” the member
dies and produces a random number of oﬀspring with µ({k}) denoting the probabil-
ity that there are k oﬀspring from a particular individual. Thus, the convolution µ∗x
is the distribution of the upcoming population given that the population is currently
at state x.
Example 6.24 (Renewal sequences) Assume that µ is a probability distribution
on Z
+ and deﬁne
µx =

µ
x = 1
δx−1
1 < x < ∞
δ∞
x = ∞
.
Let X = {Xn : n ≥0} be a Markov chain with initial state 1 and transition distribu-
tions µx, x ∈Z
+ and deﬁne Yn = I(Xn = 1). Then Y = {Yn : n ≥0} is a renewal
sequence with waiting time distribution µ. If we replace the initial distribution δ1
of X by an arbitrary distribution Q0 then the sequence Y is a delayed renewal se-
quence.
6.5.3
The Martingale Problem
In this section we connect MGs and Markov chains. The discrete version of the
MG problem is given next.
Deﬁnition 6.20 The martingale problem
Let BΨ denote all the bounded measurable functions on some Borel space Ψ.
Assume that G : BΨ →BΨ is some operator on BΨ and let F = {Fn : n ≥0}
a ﬁltration in a probability space (Ω, A, P). A sequence X of Ψ-valued random

MARKOV CHAINS
241
variables on (Ω, A, P) is said to be a solution to the MG problem for G and F if for
all bounded measurable functions f : Ψ →R the sequence Y deﬁned by
Yn = f (Xn) −
n−1
P
k=0(G f )(Xk),
is a martingale with respect to the ﬁltration F .
The following theorem shows the usefulness of the MG problem.
Theorem 6.16 (Markov chains via the martingale problem) Assume the no-
tation of the previous deﬁnition. A sequence X of Ψ-valued random variables is
a Markov chain with respect to F and X has generator G if and only if X is a
solution to the MG problem for G and F .
This theorem is useful in two ways; ﬁrst, it gives us a large collection of MGs
that can be used to analyze a given Markov chain. Second, it provides us with a
way of assessing that a random sequence is Markov. The latter is more useful in the
continuous time-parameter case and will be revisited later.
6.5.4
Visits to Fixed States: General State Space
We turn now to investigating the behavior of a Markov chain with respect to a
ﬁxed set of states and in particular, the random variables and probabilities of hitting
times and return times. For what follows let X = {Xn : n ≥0} be a Markov chain
with denumerable state space Ψ and transition operator T. For a ﬁxed state y ∈Ψ
or Borel set B ⊂Ψ, deﬁne the probability that, starting at x, the Markov chain X
ever visits (hits) state y or the set of states B by
H(x, y) = P(Xn = y, for some n ≥0|X0 = x),
(6.17)
and
H(x, B) = P(Xn ∈B, for some n ≥0|X0 = x).
(6.18)
Note that H(x, B) = 1, x ∈B. Similarly, deﬁne the probability that X ever returns
to y or B starting from y or a state x ∈B by
H+(y, y) = P(Xn = y, for some n > 0|X0 = y),
(6.19)
and
H+(x, B) = P(Xn ∈B, for some n > 0|X0 = x).
(6.20)
Clearly, H(x, B) = H+(x, B), for x ∈Bc. Note that for any transition operator T we
have H+(x, B) = (TH)(x, B), for ﬁxed B.
Deﬁnition 6.21 Harmonic functions
Let G be a discrete generator on a state space Ψ and let B be a Borel subset of Ψ.
A bounded measurable function f : Ψ →R is G-subharmonic on B if G f (x) ≥0,
∀x ∈B, whereas, it is called G-superharmonic on B if −f is G-subharmonic on B.

242
RANDOM SEQUENCES
It is called G-harmonic on B if it is both G-subharmonic and G-superharmonic on
B, i.e., G f (x) ≥0 and G(−f )(x) ≥0. If B = Ψ then f is simply called G-harmonic.
Note that harmonic functions f for generators G with G(−f )(x) = −G f (x), must
satisfy G f (x) = 0.
The following theorem gives us a way of calculating H(x, B). The proof is re-
quested as an exercise.
Theorem 6.17 (Finding hitting probabilities) Let G be the discrete generator
of a Markov chain X with state space Ψ and let B be a Borel subset of Ψ. Then
the function x 7→H(x, B) is the minimal bounded function h with the properties:
(i) h(x) = 1, for x ∈B,
(ii) h is G-harmonic on Bc,
(iii) h(x) ≥0, for all x ∈Ψ,
where H(x, B) is minimal means that for any other function h that satisﬁes prop-
erties (i)-(iii), we have H(x, B) ≤h(x), ∀x ∈Ψ.
Example 6.25 (Uncountable state space Ψ) Assume that X is a Markov chain
with state space R+ and transition distributions µx = 1
3δx/2 + 2
3δ4x, where δy is the
delta function at y. We ﬁx B = (0, 1] and calculate H(x, B) by applying theorem
6.17. First note that the discrete generator G of X is given by
Gh(x) = (T −I)h(x) = 1
3h(x/2) + 2
3h(4x) −h(x),
since (Th)(x) =
R
Ψ
h(y)µx(dy) = 1
3h(x/2) + 2
3h(4x) and (Ih)(x) = h(x). The second
requirement of theorem 6.17 requires the minimal function to be harmonic for x >
1, that is,
Gh(x) = 1
3h(x/2) + 2
3h(4x) −h(x) = 0,
and setting g(y) = h(2y), y ∈R, we can write the requirement as
1
3g(y −1) −g(y) + 2
3g(y + 2) = 0,
(6.21)
for y > 0. If we restrict to y ∈Z+, equation (6.21) is a third-order homogeneous
linear diﬀerence equation. We try solutions of the form g(y) = ky and obtain the
following condition on k
1
3 −k + 2
3k3 = 0.
This polynomial has three solutions: k1 = −1.366, k2 = 1 and k3 = 0.366. Conse-
quently, every solution of (6.21) is of the form
g(y) = aky
1 + b + cky
3.
By condition (iii) of the theorem, applied for large odd y if a ≥0 and large even y
if a ≤0, we conclude that a = 0. From condition (i) we obtain b + c = 1. Finally,

MARKOV CHAINS
243
to get a minimal solution without violating condition (iii) we take b = 0 so that
g(y) = (0.366)y ,
for y ∈Z+. But g(y) = g(⌈y⌉) for y > 0 (or x > 1) and g(y) = 1, for y ≤0
(or x ∈(0, 1]), where ⌈y⌉denotes the ceiling of the real number y. Therefore, the
hitting probability is given by
H(x, (0, 1]) =

1
x ∈(0, 1]
(0.366)m
2m−1 < x ≤2m
m ≥0.
Example 6.26 (Countable state space Ψ) Recall the Birth-Death Markov chain
of example 6.19, but now assume that µ0({0}) = 1, µx({x −1}) =
1
x+1, x > 0 and
µx({x}) =
x
x+1, x > 0. We calculate H(x, 0), the probability that the population dies
out (at any step). First note that for x > 0, the discrete generator G of X is given by
Gh(x) = (T −I)h(x) =
1
x + 1h(x −1) +
x
x + 1h(x) −h(x).
The second requirement of theorem 6.17 requires the minimal function to be har-
monic for x > 0, that is,
1
x + 1h(x −1) −
1
x + 1h(x) = 0,
or h(x) = h(x −1) and trying the solution h(x) = kx, for some constant k, we get
h(x) = k. From condition (i) h(0) = 1, so that k = 1 and condition (iii) is satisﬁed.
Therefore H(x, 0) = 1, x ≥0 and the population will surely die out.
6.5.5
Visits to Fixed States: Discrete State Space
We consider now the discrete state space case. Throughout this section assume
that X = {Xn : n ≥0} is a time-homogeneous Markov chain with discrete state space
Ψ and transition matrix T. Deﬁne Px(A) = P(A|X0 = x) and Ex(Y) = E(Y|X0 = x)
so that for ﬁxed x ∈Ψ, Px(A) is a probability measure on the measurable space
(Ω, A) where the Xn are deﬁned.
Remark 6.12 (Visits to ﬁxed states) We ﬁx a state y ∈Ψ and deﬁne the following
important quantities used in investigating visits to ﬁxed states.
1. Times of successive visits In general, we denote by T1(ω), T2(ω), . . . , the suc-
cessive indices n ≥1 for which Xn(ω) = y or, in the case that there is no such n, we
set T1(ω) = T2(ω)−T1(ω) = · · · = +∞. If y appears only a ﬁnite number of times m
on the sequence X then we let T1(ω), T2(ω), . . . , denote the successive indices n ≥1
for which Xn(ω) = y and let Tm+1(ω) −Tm(ω) = Tm+2(ω) −Tm+1(ω) = · · · = +∞.
Note that Tm(ω) ≤n if and only if y appears in {X1(ω), . . . , Xn(ω)} at least m-times.
Therefore, every Tm is a stopping time and the strong Markov property holds at Tm.
Moreover, XTm = y when {Tm < ∞} occurs. As a consequence, we can show that
Px(Tm+1 −Tm = k|T1, . . . , Tm) =

0
Tm = ∞
Py (T1 = k)
Tm < ∞a.s.,
(6.22)

244
RANDOM SEQUENCES
for any x ∈Ψ.
2. Visits in a ﬁxed number of steps Deﬁne the probability that the chain starting
at x moves to state y for the ﬁrst time in exactly k-steps as
Hk(x, y) = Px(T1 = k),
k ≥1, where x ∈Ψ and T1 = inf{n ≥1 : Xn = y} the ﬁrst time X visits state y
(ﬁrst return time if x = y). Note the diﬀerence between the probability Hk(x, y) of
visiting y for the ﬁrst time starting from x and T k(x, y) = Px(Xk = y) = δxT k fy, the
probability of visiting y (perhaps many times during the k-steps) starting from x,
both in exactly k-steps. Now for k = 1 we have
H1(x, y) = Px(T1 = 1) = Px(X1 = y) = T(x, y),
and for k ≥2 we can easily see that
Hk(x, y) =
P
z∈Ψ∖{y}
Px(X1 = z)Pz(X1 , y, . . . , Xk−1 , y, Xk = y),
and therefore
Hk(x, y) =

T(x, y)
k = 1
P
z∈Ψ∖{y}
T(x, z)Hk−1(z, y)
k ≥2 ,
(6.23)
which is a recursive formula that can be used to compute Hk(x, y). Clearly, we can
write
H(x, y) = Px(T1 < ∞) =
+∞
P
k=1 Hk(x, y),
(6.24)
for the probability that, starting from x, the Markov chain X ever visits y. Summing
over all k in (6.23) we have
H(x, y) = T(x, y) +
P
z∈Ψ∖{y} T(x, z)H(z, y),
for all x, y ∈Ψ, which is a system of linear equations in H(x, y) that can be used to
solve for H(x, y), x ∈Ψ.
Finally, assume that Ψ = {x1, x2, . . . } and let Hk = (Hk(x1, y), Hk(x2, y), . . . ).
Note that we omit state y from the calculations in (6.23) as if we have removed
the yth column of T and therefore deﬁne T−y to be the matrix obtained from T by
replacing its yth column with zeros. As a consequence, H1 is the yth column of T
and we can write (6.23) as
Hk = T−yHk−1,
k ≥2.
3. Total number of visits For a realization of the Markov chain X(ω), ω ∈Ω, let
Ny(ω) =
+∞
P
n=0 I(Xn(ω) = y) denote the total number of visits by the chain X to state
y, with Ny(ω) = m if and only if T1(ω) < ∞, . . . , Tm(ω) < ∞and Tm+1(ω) = ∞.
Using equation (6.22), the events {T1 < ∞}, {T2 −T1 < ∞}, . . . , {Tm −Tm−1 < ∞},
{Tm+1 −Tm = +∞} are independent and their probabilities are, starting at x, H(x, y),
H(y, y), . . . , H(y, y), 1 −H(y, y), respectively. Therefore, we have
Py(Ny = m) = H(y, y)m−1(1 −H(y, y)),
(6.25)

MARKOV CHAINS
245
m = 1, 2, . . . , so that Ny|X0 = y ∼Geo(1 −H(y, y)) and if we start at x ∈Ψ we have
Px(Ny = m) =

1 −H(x, y)
m = 0
H(x, y)H(y, y)m−1(1 −H(y, y))
m ≥1 .
(6.26)
Summing over all m in (6.25) we ﬁnd the probability that the total number of returns
to y is ﬁnite, that is, Py(Ny < ∞) =
+∞
P
m=1 Py(Ny = m).
4. Expected number of visits Now if H(y, y) = 1 then Py(Ny < ∞) = 0 (or Ny =
+∞a.s.) so that the expected total number of visits to state y, starting from y, is
Ey(Ny) = +∞. In contrast, if H(x, y) < 1 we have the geometric distribution of
(6.25) with Py(Ny < ∞) = 1 and Ey(Ny) = (1 −H(y, y))−1 . Similarly, we obtain
Ex(Ny) = H(x, y) (1 −H(y, y))−1 , for x , y. Putting these means in the form of a
matrix R = [(R(x, y))] with
R(x, y) = Ex(Ny) =

(1 −H(y, y))−1
x = y
H(x, y)R(y, y)
x , y ,
(6.27)
we have what is known as the potential matrix of X with the conventions 1/0 = ∞
and 0 ∗∞= 0.
5. Relating T and R Using the deﬁnition of Ny and the MCT we can write
R(x, y) = Ex
"+∞
P
n=0 I(Xn = y)
#
=
+∞
P
n=0 Ex
I(Xn = y) =
+∞
P
n=0 Px(Xn = y) =
+∞
P
n=0 T n(x, y),
or in terms of the matrices R and T, we have
R = I + T + T 2 + . . . ,
where I is the identity matrix and therefore we can compute R if we have all the
powers of T. It is useful in calculations to note that
RT = T + T 2 + T 3 + . . . ,
so that
R(I −T) = (I −T)R = I,
and if (I −T)−1 exists then we have right away R = (I −T)−1. In general, once we
solve for R we can invert equation (6.27) and compute H(x, y) as
H(x, y) =

(R(y, y) −1) R(y, y)−1
x = y
R(x, y)R(y, y)−1
x , y .
Appendix remark A.6 presents additional results on the calculation of H and R.
Example 6.27 (Hitting probabilities)
Consider a Markov chain X with state
space Ψ = {a, b, c} and transition matrix
T =

1
0
0
1/4
1/4
1/2
1/3
1/3
1/3

,

246
RANDOM SEQUENCES
ﬁx y = c and deﬁne Hk = (Hk(a, c), Hk(b, c), Hk(c, c))′, the k-step probabilities that
we visit state c for the ﬁrst time, starting from a or b or c. Writing
T−c =

1
0
0
1/2
1/4
0
1/3
1/3
0

,
and using equation Hk = T−cHk−1 we have for k = 1: H1 = (T(a, c), T(b, c), T(c, c))′
= (0, 1/2, 1/3)′ (the third column of T) and for k ≥2 we can easily see that H2 =

0, 1
2
1
4, 1
2
1
3
′, H3 =

0, 1
2
 1
4
2 , 1
2
1
3
2′
, H4 =

0, 1
2
 1
4
3 , 1
2
1
3
3′
and therefore
Hk(a, c)
=
0, k ≥1,
Hk(b, c)
=
2−1 (1/4)k−1 , k ≥1,
Hk(c, c)
=
1/3, k = 1, and
Hk(c, c)
=
2−1 (1/3)k−1 , k ≥2.
Thus starting at a the process never visits c, that is, Pa(T1 = +∞) = 1 (or H(a, c) =
0). Starting at b the probability that c is never visited is computed using equation
(6.24) as
Pb(T1 = +∞) = 1 −Pb(T1 < +∞) = 1 −
+∞
P
k=1 Hk(b, c) = 1 −
+∞
P
k=1 2−1 (1/4)k−1 = 1/3.
(6.28)
Finally, starting at c the probability that X never returns to c is given by
Pc(T1 = +∞) = 1 −
+∞
P
k=1 Hk(b, c) = 1 −1/3 −
+∞
P
k=2 2−1 (1/3)k−1 = 5/12.
(6.29)
The following remark shows us how we can use the renewal theory results in
order to study the behavior of a Markov chain, including a ﬁrst approach on how to
conduct classiﬁcation of states for the Markov chain.
Remark 6.13 (Renewal theory and visits to a ﬁxed state) We can use renewal
theory as an alternative way to obtain the probability H(x, y) as a limit via the
renewal theorem (p. 231). We restrict our development to the discrete state space
case Ψ, although some of the results can be applied in general. In particular, deﬁne
a random sequence Y = {Yn : n ≥0}, with Yn = I(Xn = y), so that Y indicates
the times n ≥0 for which the Markov chain X visits state y ∈Ψ. Assuming that
X0 = x , y and using the strong Markov property, we have that Y is a delayed
renewal sequence (or simply a renewal sequence if X0 = y) on the probability space
(Ω, A, Px), with Px(.) = P(.|X0 = x), for each x ∈Ψ. Note that Px(Yn = 1) =
T n(x, y) = Px(Xn = y), for ﬁxed x, y ∈Ψ, so that {T n(y, y) : n ≥0} denotes the
potential sequence {πy
n : n ≥0} for the renewal sequence corresponding to x = y.
Therefore, we can use the renewal theory results to obtain results for {T n(x, y) : n ≥
0}.
1. H(x, y) as a limit Assume that γy is the period and my the mean waiting time
of the renewal sequence Y corresponding to y. For the Markov chain X, my can be

MARKOV CHAINS
247
thought of as the mean return time to state y. Since πy
n = T n(y, y), using the renewal
theorem, we can show that
lim
n→+∞γ−1
y
γy−1
P
k=0 T n+k(x, y) = H(x, y)
my
,
(6.30)
and therefore
lim
n→+∞
nP
k=1 T k(x, y)/n = H(x, y)
my
.
(6.31)
2. Renewal measure and total number of visits Assume that X0 = y. In view of
the deﬁnition of Ny in remark 6.12.3 we can write the renewal measure Ry of the
renewal sequence Yn = I(Xn = y) in terms of the total number of visits Ny as
Ny = Ry(Z+
0) =
+∞
P
n=0 I(Xn = y).
An immediate consequence is a relationship between the potential measure and the
potential matrix as follows
R(y, y) = Ey(Ny) =
+∞
P
n=0 T n(y, y) =
+∞
P
n=0 πy
n = Πy(Z+
0),
where Πy is the potential measure corresponding to the renewal sequence Y for each
y ∈Ψ. Further note that the waiting time distribution Wy describes the distribution
of the time until the ﬁrst renewal, or in terms of the Markov chain X, the time X
visits state y for the ﬁrst time, that is,
Wy({k}) = Hk(y, y) = Py(T1 = k), k ≥1.
From theorem 6.12 Πy(Z+
0) = 1/Wy({+∞}) or in terms of potential matrix
Wy({+∞}) = 1/Πy(Z+
0) = 1/R(y, y).
Consequently, we can write the mean return time to y (mean of the waiting time
distribution Wy) as
my = Ey(T1) =
+∞
P
k=0 kHk(y, y).
3. State classiﬁcation The states of the Markov chain X can be classiﬁed accord-
ing to the corresponding classiﬁcation of the renewal sequences Yn = I(Xn = y)
(deﬁnition 6.12), with X0 = y, for each y ∈Ψ. Since classiﬁcation for renewal se-
quences works for both denumerable and countable state spaces Ψ we discuss the
general case brieﬂy here. In particular, a state y ∈Ψ is called recurrent if and only
if
+∞
P
n=0 πy
n = +∞in which case it is called null recurrent if and only if lim
n→+∞πy
n = 0.
The state is called transient if
+∞
P
n=0 πy
n < +∞and periodic with period γy, if γy > 1,
or aperiodic if γy = 1.
Although the latter can be used to classify the states of a Markov chain, we
provide an extensive and more systematic way of classiﬁcation next.

248
RANDOM SEQUENCES
6.5.6
State Classiﬁcation
In what follows we restrict the development and deﬁnitions to the discrete state
space Ψ. For denumerable Ψ one can use remark 6.13.3. For what follows let X =
{Xn : n ≥0} be a Markov chain with state space Ψ and transition matrix T.
Deﬁnition 6.22 State classiﬁcation
Fix a state y ∈Ψ and deﬁne T1 = inf{n ≥1 : Xn = y} as the time of ﬁrst visit to y.
1. Recurrent or transient state The state y is called recurrent if Py(T1 < ∞) = 1,
otherwise if Py(T1 = +∞) > 0 then y is called transient.
2. Recurrent null or non-null state A recurrent state y is called recurrent null if
Ey(T1) = +∞, otherwise it is called recurrent non-null.
3. Periodic state A recurrent state y is said to be periodic with period γ if γ ≥2 is
the largest integer for which Py(T1 = nγ for some n ≥1) = 1, otherwise if there is
no such γ ≥2, y is called aperiodic.
Equivalent forms of the classiﬁcation criteria above can be obtained based on
hitting probabilities and the potential matrix, as we see below.
Remark 6.14 (State classiﬁcation) Let Ny denote the total number of visits to a
ﬁxed state y ∈Ψ. In view of remark 6.13.4 it is straightforward to see that y is
recurrent if and only if
H(y, y) = 1 ⇔R(y, y) = +∞⇔Py(Ny = +∞) = 1.
Similarly, y is transient if and only if
H(y, y) < 1 ⇔R(y, y) < +∞⇔Py(Ny < +∞) = 1.
Now if y is periodic with period γ then the Markov chain X can return to y
only at times γ, 2γ, 3γ, . . . and the same is true for the second return time, the
third and so forth. Therefore, starting at y, T n(y, y) = Py(Xn = y) > 0, only
if n ∈{0, γ, 2γ, 3γ, . . . }. To check if T n(y, y) > 0, in view of the Chapman-
Kolmogorov equations, we only need to ﬁnd states y1, y2, . . . , yn−1 ∈Ψ such that
T(y, y1) > 0, T(y1, y2) > 0, . . . , T(yn−1, y) > 0. This leads to the following criterion
for periodicity.
Criterion 6.1 (Periodicity) Let γ be the GCD of the set of all n ≥1 such that
T n(y, y) > 0. If γ = 1 then y is aperiodic, otherwise, if γ ≥2 then y is periodic with
period γ.
Next we discuss some useful properties of a Markov chain, based on its states
properties.
Deﬁnition 6.23 Markov chain properties
Let X = {Xn : n ≥0} be a Markov chain with discrete state space Ψ.

MARKOV CHAINS
249
1. For any x, y ∈Ψ we say that y can be reached by x if H(x, y) > 0 and write
x →y.
2. A set of states B ⊂Ψ is said to be closed if the only states that can be reached
by states of B are those of the set B.
3. Absorbing state A state that forms a closed set is called absorbing.
4. The closed set B is irreducible if no proper subset of B is a closed set.
5. Irreducible A Markov chain is called irreducible if its only closed set is the state
space Ψ.
An immediate consequence of the latter deﬁnition are the following criteria.
Criterion 6.2 (Absorbing state) A state y is absorbing if T(y, y) = 1.
Criterion 6.3 (Irreducible) A Markov chain is irreducible if and only if all states
can be reached from each other.
Several results and deﬁnitions regarding the states of a Markov chain and the
chain itself, are collected below.
Remark 6.15 (State and Markov chain properties) For a discrete state space Ψ
we can show the following.
1. Let C = {x1, x2, . . . } ⊂Ψ be a closed set and deﬁne Q(x1, x2) = T(x1, x2),
x1, x2 ∈Ψ. Then Q is a Markov matrix.
2. If x →y and y →z then x →z for any x, y, z ∈Ψ.
3. If x is recurrent and x →y then y →x, H(x, y) = 1 and y is recurrent.
4. For each recurrent state y there exists an irreducible closed set to which y be-
longs.
5. For any Markov chain X the recurrent states can be arranged in a unique manner
into irreducible closed sets. As a consequence, any results obtained for irreducible
Markov chains can be applied to irreducible closed sets.
6. From a recurrent state, only recurrent states can be reached.
7. If an irreducible closed set has ﬁnitely many states, then it has no transient
states.
8. If Ψ is ﬁnite then no state is recurrent null and not all states are transient.
9. If x and y are recurrent states then either H(x, y) = H(y, x) = 1 or H(x, y) =
H(y, x) = 0.
10. If x is recurrent and y is transient then H(x, y) = 0.
11. If x, y ∈Ψ with x →y and y →x then they both have the same period and

250
RANDOM SEQUENCES
have the same classiﬁcation, i.e., both are recurrent null, or recurrent non-null or
transient.
12. If x, y ∈Ψ are in the same closed set then for any state z ∈Ψ we have
H(z, x) = H(z, y).
13. Using corollary 6.2 and remark 6.13.3 we have the following in terms of
the Markov chain X with transition matrix T. The state y ∈Ψ is recurrent if
and only if
+∞
P
n=0 T n(y, y) = +∞in which case it is null recurrent if and only if
lim
n→+∞T n(y, y) = 0. Moreover, the state y is transient if
+∞
P
n=0 T n(y, y) < +∞which
implies that lim
n→+∞T n(y, y) = 0. The only case that remains is for recurrent non-null
states, in which case lim
n→+∞T n(x, y) exists under the assumption of aperiodicity (see
theorem 6.19). From remark 6.13.1 the limit of equation (6.31) always exists.
The following theorem allows us to perform classiﬁcation in an eﬃcient way
for closed sets of states.
Theorem 6.18 (Irreducible Markov chain and classiﬁcation) Let X = {Xn :
n ≥0} be an irreducible Markov chain. Then either all states are transient or
all are recurrent null or all are recurrent non-null. Either all states are aperiodic,
otherwise if one state has period γ then all states are periodic with the same
period γ.
Proof. Since X is irreducible for any two states x, y ∈Ψ we must have x →y
and y →x, which implies that there are some integers j and k such that T j(x, y) > 0
and T k(y, x) > 0. For the smallest such j and k let a = T j(x, y)T k(y, x) > 0.
(i) If y is recurrent then by remark 6.15.3 x is recurrent. If y is transient then x must
also be transient (otherwise remark 6.15.6 would make y recurrent, a contradiction).
(ii) Suppose that y is recurrent null. Then by remark 6.15.13 T m(y, y) →0 as m →
∞. Since T n+j+k(y, y) ≥aT n(x, x) we also have T n(x, x) →0 as n →∞and since x
is recurrent this can happen only if it is recurrent null (remark 6.15.13). Replacing
y with x and x with y in the above formulation, we have the converse, namely, if x
is recurrent null then so is y.
(iii) Suppose that y is periodic with period γ. Since T j+k(y, y) ≥a > 0 we must have
j + k a multiple of γ (remark 6.14) and therefore T n+j+k(y, y) = 0 whenever n is not
a multiple of γ so that T n+j+k(y, y) ≥aT n(x, x) which implies that T n(x, x) = 0 if
n is not a multiple of γ. Consequently, x must be periodic with period γ′ ≥γ by
criterion 6.1. Replacing the role of y and x, we obtain γ′ ≤γ so that x and y have
the same period.
The following theorem connects state classiﬁcation with limiting behavior for
Markov chains.

MARKOV CHAINS
251
Theorem 6.19 (State classiﬁcation and limiting behavior) Fix a state y ∈Ψ.
(i) If y is transient or recurrent null then for any x ∈Ψ
lim
n→+∞T n(x, y) = 0.
(6.32)
(ii) If y is recurrent non-null aperiodic then
π(y) = lim
n→+∞T n(y, y) > 0,
(6.33)
and for any x ∈Ψ
lim
n→+∞T n(x, y) = H(x, y)π(y).
(6.34)
Proof. Let {πy
n : n ≥0} denote the potential sequence and my =
+∞
P
n=1 nWy({n}),
my ∈[1, ∞], the mean of the waiting time distribution Wy of the (possibly delayed)
renewal sequence deﬁned by Yn = I(Xn = y) with ﬁnite period γy < ∞(the case
γy = +∞is trivial since state y is visited only once and my = +∞). Note that by
deﬁnition we have πy
nγy = T nγy(y, y), n ≥0.
(i) Assume that X0 = y, so that Y = {Yn : n ≥0} is a renewal sequence. If y is
transient or recurrent null then my = +∞by deﬁnition in both cases and an appeal
to the renewal theorem yields
lim
n→+∞T nγy(y, y) =
γy
my
= 0,
with T k(y, y) = 0, if k is not a multiple of γy, so that lim
n→+∞T n(y, y) = 0, in general. If
X0 = x , y, let τ = inf{n ≥0 : Xn = y} and use the strong Markov property at τ to
obtain
T n+i(x, y)
=
n+i
P
k=0 Px(Xn+i = y, τ = k) =
n+i
P
k=0 Py(Xn+i−k = y)Px(τ = k)
=
+∞
P
k=0 I(k ≤n + i)T n+i−k(y, y)Px(τ = k).
Noting that H(x, y) =
+∞
P
k=0 Px(τ = k) we use the DCT for sums to pass the limit under
the inﬁnite sum and use the result for the case y = x already proven to establish
lim
n→+∞T n(x, y)
=
lim
n→+∞
+∞
P
k=0 I(k ≤n + i)T n+i−k(y, y)Px(τ = k)
=
+∞
P
k=0 lim
n→+∞T n+i−k(y, y)Px(τ = k)
=

lim
n→+∞T n(y, y)
 +∞
P
k=0 Px(τ = k) =
γy
my
H(x, y) = 0.
(ii) If y is recurrent non-null aperiodic then γy = 1, my < +∞and H(y, y) = 1, so
that applying equation (6.30) we obtain
lim
n→+∞T n(x, y) = H(x, y)
my
,

252
RANDOM SEQUENCES
with lim
n→+∞T n(y, y) =
1
my. Finally, we simply set π(y) =
1
my > 0 to establish (6.33) and
(6.34).
The next two theorems help with classiﬁcation, in particular, when Ψ is count-
able.
Theorem 6.20 (Recurrent non-null) Let X be an irreducible Markov chain with
transition matrix T and assume that Ψ = {x1, x2, . . . }. Then all states are recurrent
non-null if and only if there exists a solution to the system of equations
π(y) = P
x∈Ψ π(x)T(x, y), ∀y ∈Ψ,
(6.35)
with
P
x∈Ψ π(x) = 1,
(6.36)
or in matrix form, the system π = πT subject to π1 = 1, where π =
(π(x1), π(x2), . . . ) and 1 = (1, 1, . . . )T. If a solution exists it is unique and
π(y) = lim
n→+∞T n(x, y) > 0,
(6.37)
∀x, y ∈Ψ.
Proof. Assume that all states are recurrent non-null. Then ∀x, y ∈Ψ, we have
H(x, y) = 1 and by theorem 6.19 the limits
lim
n→+∞T n(x, y) = g(y),
(6.38)
exist and they satisfy
g(y) > 0 and P
x∈Ψ g(x) = 1.
(6.39)
Now if A ⊂Ψ with A ﬁnite then the Chapman-Kolmogorov equation yields
T n+1(x, y) = P
z∈Ψ T n(x, z)T(z, y) ≥P
z∈A T n(x, z)T(z, y),
and taking limit as n →+∞, in view of (6.38) we obtain
g(y) ≥P
z∈A g(z)T(z, y), y ∈Ψ.
(6.40)
Since A is arbitrary we may take a sequence of such subsets that increases to Ψ so
that we have
g(y) ≥P
z∈Ψ g(z)T(z, y), y ∈Ψ.
Now if the strict inequality above was true for some y then summing both sides
over y we have
P
z∈Ψ g(y) > P
z∈Ψ g(z),
since P
y∈Ψ T(z, y) = 1, a contradiction, and therefore (6.40) is an equality. Equations
(6.39) and (6.40) show that g is a solution to the system (6.35)-(6.36). To show that
the solution is unique let h be another solution to (6.35)-(6.36). Then writing in

MARKOV CHAINS
253
matrix form we have h = hT and using this iteratively we have h = hT = hT 2 =
... = hT n, that is,
h(y) = P
z∈Ψ h(z)T n(z, y).
(6.41)
Taking the limit as n →+∞and applying BCT we can write
h(y) = P
z∈Ψ h(z) lim
n→+∞T n(z, y) = P
z∈Ψ h(z)g(y) = g(y),
(6.42)
∀y ∈Ψ and therefore there is only one solution (6.37) is satisﬁed.
For the converse we need to show that existence of a solution to (6.35)-(6.36)
implies that all the states are recurrent non-null. Assume that they are not. Then
(6.41) would still hold and (6.42) becomes
h(y) = P
z∈Ψ h(z) lim
n→+∞T n(z, y) = P
z∈Ψ h(z)0 = 0,
∀y ∈Ψ which contradicts P
z∈Ψ h(z) = 1.
The proof of the following theorem is requested as an exercise.
Theorem 6.21 (Recurrent null or transient) Let X be an irreducible Markov
chain with state space Ψ = {x1, x2, . . . }, transition matrix T and let T−k be the
matrix obtained from T by deleting the kth row and the kth column for some
xk ∈Ψ. Then all states are recurrent if and only if the only solution to the system
of equations
u(y) =
P
x∈Ψ∖{xk} u(x)T−k(y, x), ∀y ∈Ψ,
(6.43)
with 0 ≤u(y) ≤1, is the solution u(y) = 0, ∀y ∈Ψ ∖{xk}.
We summarize state classiﬁcation below.
Remark 6.16 (Performing state classiﬁcation) The following steps should be
used in order to perform state or Markov chain classiﬁcation for a Markov chain X
with states space Ψ.
1. Finite Ψ Assume that Ψ is a ﬁnite state space.
(1) Find all the irreducible closed sets. Using remark 6.15 and theorem 6.18 we
conclude that all states belonging to an irreducible closed set are recurrent non-
null.
(2) The remaining states are all transient.
(3) Periodicity is assessed using criterion 6.1 for each irreducible closed set and
then using theorem 6.18.
2. Countable Ψ When Ψ is countable it is possible to have irreducible closed sets
with countably many states that are transient or recurrent null.
(1) Find all the irreducible closed sets and apply theorem 6.20 to check if the set
contains recurrent null or non-null states.
(2) If the states in a closed set are classiﬁed as recurrent non-null, apply theorem

254
RANDOM SEQUENCES
6.21 to determine if they are transient or not.
(3) Periodicity is assessed as in step (3) above.
Example 6.28 (State classiﬁcation for a simple random walk) Consider a sim-
ple random walk X on Ψ = {0, 1, 2, . . .} with step distribution Q({1}) = p,
Q({−1}) = q, 0 < p < 1, q = 1 −p and a barrier at 0, that is, the transition
matrix is given by
T =

0
1
0
0
. . .
q
0
p
0
. . .
0
q
0
p
. . .
. . .
. . .
. . .
. . .
. . .

.
Since all states can be reached from each other the chain is irreducible. Starting
at 0, in order to come back to 0 the chain must take as many steps forward as
backward and therefore a return to 0 can occur only at steps numbered 2, 4, 6, . . .,
so that state 0 is periodic with period γ = 2. From theorem 6.18, since the chain
is irreducible, all states are periodic with period γ = 2. Furthermore, all states are
transient, recurrent null or recurrent non-null. To assess if the states are recurrent
non-null we use theorem 6.20. The system (6.35)-(6.36) can be written as
π(0)
=
qπ(1), π(1) = π(0) + qπ(2),
π(2)
=
pπ(1) + qπ(3), π(3) = pπ(2) + qπ(4),
and so forth, which can be rewritten as
π(1)
=
π(0)/q,
π(2)
=
(π(0)/q −π(0)) /q = π(0)p/q2,
π(3)
=

p/q2 −p/q

π(0)/q = π(0)p2/q3,
and so on. Consequently, any solution is of the form
π(x) = π(0)q−1 (p/q)x−1 ,
(6.44)
x = 1, 2, . . ., for some constant π(0) that can be found from (6.44), namely,
+∞
P
x=0 π(x) = 1 ⇔π(0) + q−1π(0)
+∞
P
x=1 (p/q)x−1 = 1.
(6.45)
Now if p < q then p/q < 1 and the sum in (6.45) converges so that we can write
+∞
P
x=0 π(x) = π(0)+q−1π(0)
+∞
P
x=1 (p/q)x−1 = π(0)+q−1π(0) (1 −p/q)−1 = π(0)2q(q−p)−1,
and choosing π(0) = (q −p)(2q)−1 = (1 −p/q) /2, we can satisfy
+∞
P
x=0 π(x) = 1.
Therefore, if p < q
π(x) =

(1 −p/q) /2
x = 0
(1 −p/q) (2q)−1 (p/q)x−1
x ≥1 ,
is a solution to the system (6.35)-(6.36) and all states are recurrent non-null.

MARKOV CHAINS
255
When p ≥q, the solution to the system has
+∞
P
x=0 π(x) that is either 0 (if π(0) =
0) or inﬁnite (if π(0) , 0), so that there is no way to satisfy both (6.35)-(6.36)
and therefore all states are not recurrent non-null. We use theorem 6.21 to help us
decide if the states are transient or recurrent null. Excluding the row and column
corresponding to state 0 from T we obtain the matrix T−0 and write the system of
equations (6.43) as
u(1) = pu(2), u(2) = qu(1) + pu(3), u(3) = qu(2) + pu(4), . . . ,
so that in general, we have a second-order diﬀerence equation
u(x) = qu(x −1) + pu(x + 1),
x ≥2 and we solve it as follows. First write
p(u(x + 1) −u(x)) = q(u(x) −u(x −1)),
x ≥2 and from the ﬁrst equation we have p(u(2) −u(1)) = qu(1). Iterating on x we
obtain
u(x + 1) −u(x) = (q/p)x−1 (u(2) −u(1)) = (q/p)x u(1),
so that for any x ≥1 we have
u(x + 1)
=
(u(x + 1) −u(x)) + (u(x) −u(x −1)) + · · · + (u(2) −u(1)) + u(1)
=
h
(q/p)x + (q/p)x−1 + · · · + q/p + 1
i
u(1).
Consequently, we have obtained a solution to (6.43) of the form
u(x) = c
h
(q/p)x−1 + (q/p)x−2 + · · · + q/p + 1
i
,
x ≥1 where c is some constant. If p = q then u(x) = cx, x ≥1 is a solution to
(6.43) and the only way to have 0 ≤u(x) ≤1 is for c = 0, that is, if p = q the only
solution to (6.43) is u(x) = 0, x ≥1 and all states are recurrent. Since we already
know that they are not recurrent non-null they must be recurrent null.
For p > q choosing c = 1 −q/p we have that u(x) = 1 −(q/p)x , x ≥1, which
satisﬁes 0 ≤u(x) ≤1, for all x ≥1. Therefore, all states are transient for p > q.
6.5.7
Limiting Behavior
In theorem 6.19 we derived the limiting distribution for a Markov chain based
on state classiﬁcation and using renewal theory arguments. We formally collect the
general deﬁnition of a limiting distribution in the following.
Deﬁnition 6.24 Limiting behavior of a Markov chain
Let X = {Xn : n ≥0} be a Markov chain with state space Ψ, transition operator
T and corresponding transition distributions {µx : x ∈Ψ}. A measure π is called a
stationary (or invariant) measure for X (or T) if it satisﬁes
π(B) = πT(B) =
R
Ψ
µx(B)π(dx),
(6.46)
for all Borel sets B ⊂Ψ. If π is a probability measure then it is called the equilib-

256
RANDOM SEQUENCES
rium (or the stationary or the limiting or the invariant) distribution and the Markov
chain X is called stationary.
Several results based on the latter deﬁnition are collected below.
Remark 6.17 (Limiting behavior) From theorem 6.20 an irreducible recurrent
non-null Markov chain has a ﬁnite stationary distribution always. We note the fol-
lowing.
1. When Ψ = {x1, x2, . . . } (countable) we can write (6.46) as
π(y) = P
x∈Ψ π(x)T(x, y), ∀y ∈Ψ,
(6.47)
where π assumes the form π = (π(x1), π(x2), . . . ).
2. If π = Q0 is the initial distribution of the Markov chain then (6.46) states that
Q1 = Q0T = πT = π = Q0, that is, using the Markov property and induction we
have
P(Xn ∈B) = Qn(B) = Q0(B) = P(X0 ∈B),
for all k, n ≥0. Consequently, π represents a possible equilibrium for the chain
after an inﬁnite number of steps, i.e., the probability distribution of where the chain
settles eventually.
3. Solving (6.46) for π (subject to π(Ψ) = 1) can be a painstaking task in the
general case. However, for discrete Ψ, theorems 6.20 and 6.21 give us a way of
obtaining the solution π as the limit of the n-step transition probabilities T n(x, y).
For a general state space Ψ ⊂Rp there exist similar results (as the ones presented
in this section) for a collection of Markov chains called Harris chains. See Robert
and Casella (2004, Chapter 6) or Durrett (2010, Section 6.8), for a discussion and
proofs of some of the important results.
4. Applying (6.46) repeatedly we have π = πT = πT 2 = · · · = πT n, for any n ≥0.
5. Stationary measures may exist for transient chains such as random walks in
p ≥3 dimensions. However, if there is a stationary distribution π for a Markov
chain X then all states x for which π(x) > 0 are recurrent.
6. If X is irreducible and has stationary distribution π then
π(x) =
1
Ex(τ),
where τ = inf{n > 0 : Xn = x} is the ﬁrst visit (or return if X0 = x) time to state x.
7. If X is irreducible then the following are equivalent: (i) some x is positive recur-
rent, (ii) there is a stationary distribution, and (iii) all states are positive recurrent.
The two concepts that make showing that there is a stationary distribution easier
are collected next.

MARKOV CHAINS
257
Deﬁnition 6.25 Reversibility and detailed balance
Let X = {Xn : n ≥0} be a Markov chain on a state space Ψ with transition matrix
T.
(i) If X is stationary it is called (time) reversible when
Xn+1|(Xn+2 = x)
d= Xn+1|(Xn = x),
for any x ∈Ψ.
(ii) The Markov chain X is said to satisfy the detailed balance condition if there
exists a measure π such that
π(x)T(x, y) = π(y)T(y, x),
(6.48)
for all x, y ∈Ψ.
Note the following about proving detailed balance.
Remark 6.18 (Showing detailed balance) Time reversibility and detailed balance
are equivalent when π exists and is a density (in which case it is the equilibrium).
Detailed balance guarantees that a stationary measure exists (it is a suﬃcient con-
dition) but it is a stronger condition than π = πT. Indeed, summing (6.48) over x
we obtain
P
x∈Ψ π(x)T(x, y) = π(y) P
x∈Ψ T(y, x) = π(y),
since P
x∈Ψ T(y, x) = 1, which is equation (6.47), so that (6.48) gives a solution to
π = πT. Most Markov chain Monte Carlo (MCMC) algorithms utilize detailed
balance in order to show that a Markov chain X with transition operator T has
limiting distribution π. In terms of deﬁnition (6.24) and equation (6.46) we can
write equation (6.48) of detailed balance as
R
A
π(x)T(x, B)dx =
R
B
π(y)T(y, A)dy,
or equivalently
R
A
π(x)
R
B
T(x, y)dydx =
R
B
π(y)
R
A
T(x, y)dxdy,
and therefore
R
A
R
B
π(x)T(x, y)dydx =
R
A
R
B
π(y)T(x, y)dydx,
(6.49)
for all Borel sets A, B ∈B(Ψ) so that (6.49) holds if and only if (6.48) holds.
Example 6.29 (Birth-Death sequence) Recall the Birth-Death chain of example
6.19 with Ψ = Z+
0 and assume that µx({x −1}) = q, µx({x}) = r and µx({x + 1}) = p,
with µ0({0}) = 0 and µ0({1}) = 1, p + q + r = 1. Then the measure deﬁned by
π(x) =
xQ
i=1(pi−1/qi), x ∈Z+
0, satisﬁes detailed balance. Indeed, ﬁrst note that
π(x)T(x, x + 1) = px
xQ
i=1(pi−1/qi) = π(x + 1)T(x + 1, x),

258
RANDOM SEQUENCES
and since T(x, y) = 0, for |x −y| > 1, we have
π(x)T(x, y) = π(y)T(y, x),
for all x, y ∈Z+
0. As a consequence, Birth-Death Markov chains have an equilib-
rium distribution if and only if
+∞
P
x=0
xQ
i=1(pi−1/qi) < +∞since in this case we can make
π a valid density.
6.6
Stationary Sequences and Ergodicity
Stationary sequences are random sequences that model the behavior of some
system that is in equilibrium. For what follows assume that X = {Xn : n ≥0} is a
random sequence with Xn : (Ω, A, P) →(Ψ, G), where (Ω, A, P) is a probability
space and (Ψ, G) is some Borel space. The sequence X is a random vector in the
measurable space (Ψ∞, G∞). We begin with some basic deﬁnitions.
Deﬁnition 6.26 Stationary random sequence
Let τ : Ψ∞→Ψ∞be a measurable map (shift transformation) deﬁned by τ(X) =
τ((X0, X1, . . . )) = (X1, X2, . . . ).
1. Stationarity The random sequence X is called stationary if X and the shifted
sequence τ(X) have the same distribution, i.e., X
d= τ(X).
2. Shift-invariant measure If Q is the distribution of a stationary random se-
quence X then τ is a measure preserving transformation (or shift-invariant) on
(Ψ∞, G∞, Q), that is, Q(τ−1(A)) = Q(A), ∀A ∈G∞.
This following remark summarizes some of the consequences of the latter deﬁ-
nition.
Remark 6.19 (Stationarity and ergodic theory) The theory of measure preserv-
ing transformations is called ergodic theory. Denote by τk = τ ◦· · · ◦τ, the k-fold
composition of τ with itself so that τk(X) = (Xk, Xk+1, . . . ), k ≥0.
1. It is easy to see that the random sequence X is stationary if and only if X
d=
τk(X), for all k ≥0. In other words, for all k, m ≥0, the vectors (X0, X1, . . . , Xm)
and (Xk, Xk+1, . . . , Xk+m) have the same distribution.
2. Even if τ−1 does not exist (not one to one), we can still deﬁne τ−k, the k-fold
composition of τ−1 with itself. For any k ≥1, deﬁne Tk = {A ∈G∞: A = τ−k(A)},
which can be shown to be a sub-σ-ﬁeld in σ(Xk, Xk+1, . . . ) (by iterating τ−1 on
A ∈σ(X1, X2, . . . )). Then a tail σ-ﬁeld is deﬁned by H =
+∞
T
k=1 Tn, with H ⊂T ,
where T =
+∞
T
n=1 σ(Xn, Xn+1, . . . ), the tail σ-ﬁeld of the sequence X (see deﬁnition

STATIONARY SEQUENCES AND ERGODICITY
259
4.9). For k = 1 deﬁne
I = {A ∈G∞: A = τ−1(A)},
(6.50)
the collection of shift-invariant sets.
3. For any probability measure Q∗on (Ψ∞, G∞), τ need not be shift-invariant (it
is if Q∗= Q the distribution of a stationary random sequence X). We denote by Mτ
the collection of all probability measures on (Ψ∞, G∞) that are shift-invariant with
respect to τ. A member of Mτ may or may not be the distribution of some stationary
sequence.
4. A random sequence (. . . , X−1, X0, X1, . . . ) is called (two-sided) stationary if the
distribution of (Xk, Xk+1, . . . ) does not depend on k ∈Z. Deﬁnition 6.26 of station-
arity is referred to as one-sided stationarity.
5. A measurable map f : (Ω, A) →(R, B1) is I-measurable if and only if f ◦τ =
f.
We are now ready to collect the formal deﬁnition of an ergodic sequence.
Deﬁnition 6.27 Ergodicity
A stationary random sequence X with distribution Q is ergodic if the σ-ﬁeld I is
0-1 trivial with respect to Q. In this case the distribution Q is also called ergodic.
The main result about stationary sequences is called the ergodic theorem.
Theorem 6.22 (Birkhoﬀ’s ergodic theorem) Let X = (X0, X1, . . . ) be an R-
valued stationary sequence and set S n = X0+· · ·+Xn−1. Assume that E|X0| < +∞.
Then
lim
n→+∞S n/n = E(X0|X−1(I)) a.s.
(6.51)
and
lim
n→+∞E
S n/n −E(X0|X−1(I))


= 0,
(6.52)
that is, S n/n →E(X0|X−1(I)) a.s. and in L1. If in addition X is an ergodic se-
quence then
lim
n→+∞S n/n = E(X0) a.s.
(6.53)
Proof. We prove (6.51) in detail and request (6.52) as an exercise. Equation
(6.53) is an immediate consequence of (6.51) under ergodicity (see remark 6.20.1
below).
Without loss of generality we may assume that E(X0|X−1(I)) = 0 a.s. Indeed, since
E|X0| < +∞, E(X0|X−1(I)) is ﬁnite a.s. and therefore we can work with the se-
quence Xn −E(X0|X−1(I)) (which is stationary) once we prove the theorem for the

260
RANDOM SEQUENCES
special case E(X0|X−1(I)) = 0. Therefore, our goal is to prove that S n/n →0 a.s.
as n →∞.
Fix ε > 0 and deﬁne the set
Aε =
(
ω : lim sup
n→∞
S n(ω)
n
> ε
)
.
We need to prove that P(Aε) = 0, which implies P
 
lim sup
n→∞
S n(ω)
n
> 0
!
= 0. Then
applying the same argument to the sequence −X we have a.s. convergence since
replacing X by −X leads to
lim inf
n→∞
S n(ω)
n
≥0 a.s.,
and therefore lim
n→∞
S n(ω)
n
= 0 a.s. [P].
First we note that Aε ∈X−1(I) since the random variable Z = lim sup
n→∞
S n(ω)
n
is such that Z ◦τ = Z and remark 6.19.5 applies. Consequently, E(X0IAε) =
E(E(X0|X−1(I)IAε)) = 0 and thus E (X0 −ε)IAε
 = −εP(Aε). Therefore, in order
to show P(Aε) = 0 it suﬃces to show that
E (X0 −ε)IAε
 ≥0.
(6.54)
Deﬁne the sequence of shifted partial sums by S 0
n = S n −X0, n = 1, 2, . . . and let
Mn
=
max{S 1 −ε, S 2 −2ε, . . . , S n −nε},
M0
n
=
max{S 0
1 −ε, S 0
2 −2ε, . . . , S 0
n −nε}.
By the deﬁnition of S 0
n we have
X0 −ε + max(0, M0
n) ≥Mn,
for n = 0, 1, 2, . . . and since X0 = S 1 we have
X0 −ε ≥Mn −max(0, M0
n).
Therefore
E (X0 −ε)IAε

≥
E
h
(Mn −max(0, M0
n))IAε
i
=
E
h
(max(0, Mn) −max(0, M0
n))IAε
i
+ E (Mn −max(0, Mn))IAε
 ,
for n = 0, 1, 2, . . . and since X is stationary and Aε ∈X−1(I), the random
variables max(0, Mn)IAε and max(0, M0
n)IAε have the same distribution so that
E
h
(max(0, Mn) −max(0, M0
n))IAε
i
= 0. As a result,
E (X0 −ε)IAε
 ≥E (Mn −max(0, Mn))IAε
 ,
for n = 0, 1, 2, . . . and using the deﬁnitions of Mn and Aε we have lim
n→∞Mn(ω) ≥0,
for all ω ∈Aε so that
lim
n→∞
(Mn −max(0, Mn))IAε
 = 0,
a.s. [P]. But by the deﬁnition of Mn, X0 −ε ≤Mn so that
|X0 −ε| ≥|Mn −max(0, M0
n)|,
and since X0 is integrable, we can apply the DCT to swap the limit and the expec-

STATIONARY SEQUENCES AND ERGODICITY
261
tation sign to obtain
E (X0 −ε)IAε

≥
lim
n→∞E (Mn −max(0, Mn))IAε

=
E

lim
n→∞(Mn −max(0, Mn))IAε

= 0,
and equation (6.54) is proven.
Note the use of X−1(I) instead of I in Birkhoﬀ’s theorem. Since in our formula-
tion the σ-ﬁeld I is a sub-σ-ﬁeld in G∞, we have to condition on the inverse image
X−1(I), which is a sub-σ-ﬁeld in A, in order to properly deﬁne the conditional
expectation. Some consequences of the latter theorem follow.
Remark 6.20 (Consequences of Birkhoﬀ’s theorem) We can show the following.
1. If X is an ergodic sequence then the sub-σ-ﬁeld X−1(I) ⊂A, is 0-1 trivial with
respect to P. As a consequence, E(X0|X−1(I)) = E(X0) a.s. (a constant).
2. Hopf’s maximal ergodic lemma For X and S n as in theorem 6.22, with E|X0| <
+∞and Mn = max{S 1, . . . , S n} we have that E(X0I(Mn > 0)) ≥0.
3. If X is not ergodic then I is not 0-1 trivial meaning that the space can be split
into two sets A and Ac of positive probability such that A, Ac ∈I, i.e., τ(A) = A and
τ(Ac) = Ac, with P(A) < {0, 1}.
Example 6.30 (Stationary and ergodic sequences) Many of the sequences we
have seen are stationary and ergodic sequences.
1. If the Xn are iid then X is a stationary sequence. Moreover, Kolmogorov’s 0-
1 law (theorem 4.12) applies to give that T is 0-1 trivial and therefore I ⊂T
is 0-1 trivial so that X is ergodic. Since I is trivial, the ergodic theorem yields
lim
n→+∞S n/n = E(X0) a.s. (a constant) and therefore we have the SLLN as a special
case.
2. Exchangeable sequences A ﬁnite or inﬁnite sequence is called exchangeable
if its distribution is invariant under permutations of its arguments. Using remark
6.19.1 we can show that these sequences are stationary. It can be shown that ex-
changeable sequences are ergodic if and only if they are iid.
3. Markov chain at equilibrium Assume that X is a Markov chain with count-
able state space Ψ and initial distribution Q0 that is also its stationary distribution
(i.e., Q0 = π). Then X is a stationary sequence. Furthermore, it can be shown that
if the Markov chain is irreducible then the sequence is ergodic. Now if X is irre-
ducible and f a measurable function with Eπ(f (X)) = P
x∈Ψ f (x)π(x) < ∞then the
ergodic theorem applied on f (Xn) gives
lim
n→+∞(f (X0) + · · · + f (Xn−1)) /n = P
x∈Ψ f (x)π(x) a.s.
4. Moving averages If X = (X0, X1, . . . ) is a stationary sequence of R-valued ran-

262
RANDOM SEQUENCES
dom variables then Y = (Y0, Y1, . . . ) with Yn = (Xn + · · · + Xn+k−1) /k, n ≥0, for
some k > 0 deﬁnes a stationary sequence.
5. Stationary non-ergodic sequence There are many stationary sequences that
are not ergodic. For example, take X0 ∼Uni f (−1, 1) and set Xn = (−1)nX0, n ≥1.
Then the sequence X = (X0, X1, . . . ) is stationary but not ergodic.
When the σ-ﬁeld I is not easy to work with we can use the distribution Q of X
and the following approach in order to show ergodicity. For a proof of the ergodicity
criterion see Fristedt and Gray (1997, p. 562).
Deﬁnition 6.28 Extremal measure
Let C be a convex set of distributions. A measure Q ∈C is extremal in C if Q
cannot be written as a mixture of two distributions from C, that is, Q cannot be
written as the mixture distribution Q = pQ1 + (1 −p)Q2, for some 0 < p < 1 and
distinct Q1, Q2 ∈C.
Theorem 6.23 (Ergodicity criterion) Let Mτ be the collection of all shift-
invariant distributions with respect to τ on the measurable space (Ψ∞, G∞). A
measure Q ∈Mτ is ergodic if and only if it is extremal in Mτ.
The following remark shows us how to assess ergodicity.
Remark 6.21 (Extremal measure and ergodicity) Deﬁnition 6.28 can be
rephrased as: Q is extremal in C if Q = pQ1 + (1 −p)Q2 implies Q = Q1 = Q2,
when 0 < p < 1 and Q1, Q2 ∈C. We note the following.
1. If Q1, Q2 ∈C are distinct ergodic measures then Q1 ⊥Q2
2. Let X be a stationary Markov chain and denote by E the collection of all equi-
librium distributions of X. Then X is ergodic if and only if its initial distribution is
extremal in E.
6.7
Applications to Markov Chain Monte Carlo Methods
Next we present a few applications of Markov chains to computational methods
and in particular Bayesian computation and MCMC. We only discuss the discrete
case for our illustrations below but the algorithms are given for the general case.
Suppose that we wish to generate a random variable with pmf π(j) = h(j)/c, where
h(j), j ∈Ψ = {1, 2, . . ., m} are positive integers, m is very large and the normalizing
constant c =
mP
j=1 h(j) is hard to compute.
If we can somehow create an irreducible, aperiodic and time-reversible Markov
chain with limiting distribution π then we could run the chain for n-steps, where
n is large and approximate a realization from the discrete π using Xn. Moreover,

APPLICATIONS TO MARKOV CHAIN MONTE CARLO METHODS
263
we can use realizations we obtain after a large step k that can be thought of as
approximate generated values from the target distribution to estimate for example
E(g(X)) =
mP
i=1 g(i)π(i) using Monte Carlo integration by
E(g(X)) ≃
1
n −k
nP
i=k+1 g(Xi).
The combination of Markov chains and Monte Carlo gave rise to the widely
used approach called Markov Chain Monte Carlo (MCMC), where one constructs
a Markov chain X0, X1, . . . , that has equilibrium a given distribution π. The sam-
pled Xi are only approximately distributed according to π and furthermore they are
dependent.
The problem with the early states of the chain (the ﬁrst k for instance as above)
is that they are heavily inﬂuenced by the initial value X0 = j and hence we remove
them from subsequent calculations. One can use what is known as the burn-in pe-
riod, namely, use the ﬁrst k generated states to estimate a starting state for the chain
(e.g., take the average of the ﬁrst k states and start the chain again with X0 being this
average). There is no theoretical justiﬁcation for the burn-in, it is rather something
intuitive. If the chain is indeed an irreducible aperiodic Markov chain then where
we start does not matter. MCMC is best suited for high-dimensional multivariate
distributions where it is diﬃcult to ﬁnd other methods of generating samples such
as rejection samplers with high acceptance probabilities.
6.7.1
Metropolis-Hastings Algorithm
The ﬁrst method we discuss is known in the literature as the Metropolis-
Hastings (M-H) algorithm (Metropolis et al., 1953, and Hastings, 1970). Suppose
that we create an irreducible Markov chain with transition matrix Q = [(q(i, j))],
for i, j = 1, 2, . . ., m. We deﬁne now a Markov chain X = {Xn : n ≥0} where if
Xn = i we generate a random variable Y with pmf {q(i, j)}, j = 1, 2, . . ., m (the i-th
row from the transition matrix, known as the proposal kernel or distribution). Then
if Y = j we choose the next state as
Xn+1 =

j
w.p. a(i, j)
i
w.p. 1 −a(i, j) ,
where the a(i, j), will be determined.
Now the transition probabilities for the Markov chain {Xn} become
p(i, j)
=
q(i, j)a(i, j), if j , i, and
p(i, i)
=
q(i, i) +
mP
k=1,k,i q(i, k)(1 −a(i, k)),
and the chain will be time reversible with stationary probabilities {π(j)} if the de-
tailed balance condition holds, that is, π(i)p(i, j) = π(j)p(j, i), j , i, or
π(i)q(i, j)a(i, j) = π(j)q(j, i)a(j, i), j , i.

264
RANDOM SEQUENCES
The Metropolis et al. (1953) method selects the a(i, j) as
a(i, j) = min
(π(j)q(j, i)
π(i)q(i, j), 1
)
= min
(h(j)q(j, i)
h(i)q(i, j), 1
)
,
in order to produce transition probabilities p(i, j) of a time-reversible Markov chain
as desired. Note that calculation of the normalizing constant is no longer an issue.
The method proposed by Hastings (1970) is a special case when symmetry holds
for q, i.e., q(i, j) = q(j, i). The general algorithm for the discrete or continuous case
is as follows.
Algorithm 6.1
(Metropolis-Hastings) Assume that the target distribution is
π(x) ∝h(x), x ∈Ψ, with h(x) known.
Step 1: Select a proposal distribution q(y|x) that is a density with respect to y
for any x ∈Ψ. The collection {q(.|x) : x ∈Ψ} form the transition probability
measures of the source Markov chain Y and must be such that Y is irreducible
with state space Ψ and easy to sample from. Start the target Markov chain with
some X0 = x0.
Step 2: Given that the chain is at state Xn at iteration (time) n generate Y from
q(y|Xn) and generate U ∼Uni f (0, 1).
Step 3: If
U < h(Y)q(Xn|Y)
h(Xn)q(Y|Xn),
then set Xn+1 = Y, otherwise remain at the same state, i.e., Xn+1 = Xn. Go to the
previous step and continue to generate the next state.
6.7.2
Gibbs Sampling
The Gibbs sampler is an important case of the M-H algorithm and is widely
used in Bayesian statistics. In general, we wish to generate a random vector X =
(X1, . . . , Xp) with joint distribution f (x) known up to a constant, i.e., f (x) = cg(x),
where c is typically an intractable normalizing constant with the form of g(x)
known. The Gibbs sampler assumes that we can generate a random variable X with
pmf
P(X = x) = P(Xi = x|X j = xj, j , i),
for any i = 1, 2, . . . , p and x ∈Ψ. This distribution is also known as the full condi-
tional of Xi given the rest of the Xs.
Now the implementation of the M-H algorithm is as follows: assume that the
present state is x = (x1, x2, . . . , xp). We choose a coordinate equally likely, i.e., with
probability 1/p, say the ith coordinate. Then generate X = x from P(X = x) and
consider the candidate for the next state as y = (x1, . . . , xi−1, x, xi+1 . . . , xp). That is,
the Gibbs sampler uses the M-H algorithm with proposal kernel
q(y|x)=p−1P(Xi = x|X j = xj, j , i) =
f (y)
pP(X j = xj, j , i),

APPLICATIONS TO MARKOV CHAIN MONTE CARLO METHODS
265
and then accept the vector y as the next state with probability
a(x, y) = min
( f (y)q(x|y)
f (x)q(y|x), 1
)
= min
( f (y)f (x)
f (x)f (y), 1
)
= 1,
and therefore the Gibbs sampler always accepts the generated value! The Gibbs
sampler is particularly useful in Bayesian computation and in frameworks where
we work with a Gibbs distribution, e.g., Gibbs point process models or Markov
random ﬁelds.
We present the algorithm for a Bayesian context. Consider modeling an exper-
iment using a distribution for the data f (x|θ), x ∈X, i.e., we observe iid vectors
x1, . . . , xn from f and we are interested in a full posterior analysis of the parameter
vector θ = (θ1, . . . , θp). Notice that each θi has its own marginal prior distribu-
tion πi(θi|ai), where ai is a vector of hyper-parameters and often the joint prior is
π(θ) =
pQ
i=1 πi(θi|ai). To obtain the full conditionals we write ﬁrst the joint posterior
distribution as
π(θ|x1, . . . , xn) ∝f (x1, . . . , xn|θ)π(θ) = f (x1, . . . , xn|θ)
pQ
i=1 πi(θi|ai),
and from here we isolate θi to ﬁnd the full conditional for θi, i.e., πi(θi|x1:n, θ−i, ai).
Algorithm 6.2 (Gibbs sampler) We generate a random vector θ = (θ1, . . . , θp)
whose distribution is known up to a normalizing constant.
Step 1: Let θ−i = (θ1, . . . , θi−1, θi+1, . . . , θp), the vector without the ith coordinate
and obtain the forms (up to normalizing constants) of the full conditional distri-
butions πi(θi|x1:n, θ−i, ai), i = 1, 2, . . . , p.
Step 2: Suppose that the current state of the chain is θ(r) = (θ(r)
1 , . . . , θ(r)
p ). The
Gibbs sampler has the (typically ﬁxed for each step) updating order
draw θ(r+1)
1
from π1(θ1|θ(r)
2 , θ(r)
3 , . . . , θ(r)
p , x1:n, a1) and update θ1,
draw θ(r+1)
2
from π2(θ2|θ(r+1)
1
, θ(r)
3 , . . . , θ(r)
p , x1:n, a2) and update θ2,
. . .
draw θ(r+1)
i
from πi(θi|θ(r+1)
1
, θ(r+1)
2
, . . . , θ(r+1)
i−1 , θ(r)
i+1, . . . θ(r)
p , x1:n, ai)
and update θi,
. . .
draw θ(r+1)
p
from πp(θp|θ(r+1)
1
, θ(r+1)
2
, . . . , θ(r+1)
p−1 , x1:n, ap).
The new state now becomes θ(r+1) = (θ(r+1)
1
, . . . , θ(r+1)
p
).
6.7.3
Reversible Jump Markov Chain Monte Carlo
Sampling from varying dimension models can be a painstaking task since at
each iteration the dimension of the (parameter) space can change, e.g., when we

266
RANDOM SEQUENCES
sample the parameters of a mixture of normals with a random number of compo-
nents k. The problem is formulated best in terms of Bayesian model choice.
Remark 6.22 (Bayesian model choice) A Bayesian variable dimension model is
deﬁned as a collection of models
Mk = { fk(.|θk) : θk ∈Θk},
associated with a collection of priors πk(θk) on the parameters of these models and a
prior on the indices of these models ρ(k), k ∈K. The index set K is typically taken
to be a ﬁnite set, e.g., K = {1, 2, . . ., kmax}. The joint prior is given by a density
π(k, θk) = ρ(k)πk(θk),
with respect to the standard product measure of the counting measure over K and
(typically) the Lebesgue measure over Θk, where ρ(k) is the density of k with re-
spect to the counting measure over K and πk(θk) the density with respect to the
Lebesgue measure over Θk with
(k, θk) ∈Θ = S
m∈K
{m} × Θm,
where Θm ⊂Rdm for some dm ≥1. Now given the data x, the model selected is the
one with the largest posterior probability
π(Mk|x) =
ρ(k)
R
Θk
πk(θk)fk(x|θk)dθk
P
i∈K
ρ(i)
R
Θi
πi(θi)fk(x|θi)dθi
,
or one can use model averaging, i.e., obtain the posterior predictive distribution
f (y|x) = P
k∈K
ρ(k)
R
Θk
πk(θk|x)fk(y|θk)dθk.
The conditioning in π(k, θk) suggests that we condition on k before we sample θk.
In addition, note that the standard Gibbs sampler for (k, θk) will not provide moves
between model spaces {k} × Θk and {m} × Θm, k , m, since if we condition on k
we can sample θk but if we condition on θk then k cannot move. The solution is
given by a M-H type of algorithm known as Reversible Jump MCMC (RJMCMC),
deﬁned independently by Geyer and Møller (1994) and Green (1995).
Now we turn to the development of the RJMCMC algorithm.
Remark 6.23 (RJMCMC) Letting x = (k, θk) the RJMCMC utilizes the M-H setup
by deﬁning a reversible kernel K that satisﬁes the detailed balance condition
R
A
R
B
π(x)K(x, dy)dx =
R
A
R
B
π(y)K(dx, y)dy,
∀A, B ∈Θ, where π(x) is the target distribution (typically a posterior π(k, θk|x)).
The transition kernel is decomposed in terms of the proposed jump move; that is,
if we are to move to Mm, let qm(x, y) denote the corresponding transition distribu-
tion from state x to state y and am(x, y) the probability of accepting this move, the
RJMCMC algorithm has transition kernel K(x, B) given by
K(x, B) = P
m∈K
R
B
qm(x, dy)am(x, y) + s(x)IB(x)

APPLICATIONS TO MARKOV CHAIN MONTE CARLO METHODS
267
where
s(x) = P
m∈K
R
Θ
qm(x, dy)(1 −am(x, y)) + 1 −P
m∈K
qm(x, Θ),
the probability of no move. The detailed balance is satisﬁed when
P
m∈K
R
A
π(dx)
R
B
qm(x, dy)am(x, y) +
R
A∩B
π(dx)s(x) =
P
m∈K
R
B
π(dy)
R
A
qm(y, dx)am(y, x) +
R
B∩A
π(dy)s(y),
and therefore it suﬃces to have
R
A
π(dx)
R
B
qm(x, dy)am(x, y) =
R
B
π(dy)
R
A
qm(y, dx)am(y, x).
Green’s clever assumption at this stage is to require the product measure
π(dx)qm(x, dy) to have a ﬁnite density fm(x, y) with respect to a symmetric mea-
sure ξm on Θ × Θ. Then under this assumption we have
R
A
π(dx)
R
B
qm(x, dy)am(x, y)
=
R
A
R
B
ξm(dx, dy)fm(x, y)am(x, y)
=
R
B
R
A
ξm(dy, dx)fm(y, x)am(y, x)
=
R
B
π(dy)
R
A
qm(y, dx)am(y, x).
provided that
fm(x, y)am(x, y) = fm(y, x)am(y, x)
and therefore detailed balance holds by choosing
am(x, y) = min
(
1, fm(y, x)
fm(x, y)
)
= min
(
1, π(dy)qm(y, dx)
π(dx)qm(x, dy)
)
.
As elegant as it is mathematically, RJMCMC can be diﬃcult to apply in practice,
in particular, with respect to the construction of the symmetric measure ξm and
the corresponding density fm(x, y), given the symmetry constraint. In particular,
suppose that we entertain a move from Mk1 to Mk2 and wlog assume that k2 <
k1. Green’s idea was to supplement the parameter space Θk2 with artiﬁcial spaces
(additional dimensions) in order to create a bijection between Θk1 and Θk2. In this
case the move from Θk1 to Θk2 can be represented by a deterministic transformation
θ(k2) = T

θ(k1)
and in order to achieve symmetry the opposite move from Θk2 to Θk1
is concentrated on the curve {θ(k1) : θ(k2) = T

θ(k1)
}.
For the general case, if θ(k1) is augmented by a simulation z1 ∼f1(z) and θ(k2)
is augmented by a simulation z2 ∼f2(z) so that the mapping between

θ(k1), z1

and

θ(k2), z2

is a bijection

θ(k2), z2

= T

θ(k1), z1

,
then the acceptance probability for the move from Mk1 to Mk2 is given by
a(k1, k2) = min

π

k2, θ(k2)
π

k1, θ(k1) π21 f2(z2)
π12 f1(z1)

∂T

θ(k1), z1

∂

θ(k1), z1


, 1
,

268
RANDOM SEQUENCES
where πij denotes the probability of choosing a jump from Mki to Mkj and
J =

∂T(θ(k1),z1)
∂(θ(k1),z1)
 the Jacobian of the transformation. This proposal satisﬁes detailed
balance and the symmetry condition. The transformation T we choose and the cor-
responding Jacobian J is what causes this method to be hard to apply for compli-
cated models since J can be hard to calculate. The general algorithm is as follows.
Algorithm 6.3 (RJMCMC) Assume that we wish to generate from the posterior
distribution π(k, θk|x) which is known up to a normalizing constant. At iteration t
suppose that the chain is at state x(t) = (k, θ(t)
k ).
Step 1: Choose the model Mm to jump into w.p. πkm.
Step 2: Generate zkm ∼fkm(z) and zmk ∼fmk(z).
Step 3: Set (θm, zmk) = T

θ(t)
k , zkm

and generate U ∼Uni f (0, 1).
Step 4: Accept the move and set x(t+1) = (m, θm) if U < a(k, m), where
a(k, m) = min

π (m, θm)
π

k, θ(t)
k
 πmk fmk(zmk)
πkm fkm(zkm)

∂Tkm

θ(t)
k , zkm

∂

θ(t)
k , zkm


, 1

,
otherwise remain at the same state, i.e., set x(t+1) = x(t). Return to step 1.
The choice of transformation and its Jacobian is one of the major diﬃculties in
applying the RJMCMC. An elegant solution to the problem was given by Stephens
(2000) with the deﬁnition of the Birth-Death MCMC (BDMCMC). In particular,
BDMCMC is based on sampling methods from point process theory in order to
sample from a mixture model of varying dimension. The major advantage is that it
does not require calculation of a Jacobian, however, it is restrictive since it allows
either a single addition of a component or a single deletion of a component. This
algorithm is illustrated and utilized extensively in the TMSO-PPRS text.
6.8
Summary
The theory developed in this chapter provides the foundations of statistical mod-
eling for some of the most important random sequences such as random walks and
Markov chains. Standard and specialized texts in these topics include Feller (1968,
1971), Karatzas and Shreve (1991), Borodin and Salminen (1996), Durrett (1996,
2004, 2010), Fristedt and Gray (1997), Dudley (2002), C¸ inlar (1975, 2010), Lawler
and Limic (2010), Bass (2011), Billingsley (2012), Lindgren (2013) and Klenke
(2014). Building on the theory of Markov chains will allow us to model random
objects over discrete time and in particular, as we see in the TMSO-PPRS text,
point processes and random sets as they evolve over time. We collect some com-
plementary ideas on topics from this chapter below.
Martingales
The term martingale was introduced in probability theory by Ville in 1939

EXERCISES
269
but the concept had already been deﬁned by L´evy in 1934. L´evy’s 0-1 law is the
ﬁrst MG convergence theorem. Martingales owe their development to Doob (1940)
when he began to formulate a rigorous mathematical theory. For the historical ac-
count and references see Dudley (2002, p. 382). A great number of examples and
applications of MGs, in particular as they relate to gambling, can be found in Frist-
edt and Gray (1997), Durrett (2010), Billingsley (2012) and Klenke (2014).
Stochastic processes
It is amazing to think how many real-life processes exhibit a Markovian prop-
erty, that is, the future state of the random object is independent of past states given
its present state. The theoretical foundations of this idea were ﬁrst developed in
the early 1900s by Andrei Andreyevich Markov and for the ﬁnite state space case,
where he proved the existence of the limit of T n(x, y) for an aperiodic chain with
one recurrent class. In the 1930s, Andrei Nikolaevich Kolmogorov extended the
methods to the inﬁnite state space case and introduced the concepts of Markov
kernels and transition functions.
The modern theory begins with Kiyoshi Itˆo in the 1940s where motion dy-
namics are described using stochastic integral equations (see Section 7.5.1) with
transition functions and generators of the process being special cases. We studied
the ﬁrst-order Markov property here, however, there are models that allow the fu-
ture state of a random object to be independent of the past, given a number of past
realizations of the random object (e.g., time series models, see below). The study of
random sequences becomes much harder once we depart from certain assumptions
such as time-homogeneity and the Markov property.
Time series models for spatio-temporal random objects
A time series is simply a stochastic process X = {Xt : t ∈T}, with T ⊂R
typically discrete, T = {0, 1, 2, . . .}, and state space Ψ ⊂Rp, where the current state
of the random object Xt is assumed to be a function of one or more past states of
the object, e.g., Xt = f (Xt−1, Xt−2, . . . , Xt−k) + εt, t = k, k + 1, . . . , where the past is
assumed to be ﬁxed and the random noise is typically normal, i.e., εt
iid∼Np(0, Σ).
Obviously, a linear function is easier to handle and it leads to the vector auto-
regressive time series model of order k (VAR(k)), given by
Xt = Φ1Xt−1 + Φ2Xt−2 + · · · + Φt−kXt−k + εt,
(6.55)
t = k, k+1, . . . . Random walks in Rd are simply VAR(1) models, i.e., Xt = Xt−1+εt,
Φ1 = Ip. Time series are examples of stochastic process models for data collected
over time (temporal data). For more details on time series models see Cressie and
Wikle (2011, Chapter 3).
6.9
Exercises
Bernoulli and related processes
Exercise 6.1
Let X = {Xn : n ≥1} be a Bernoulli process with probability of

270
RANDOM SEQUENCES
success p, N = {Nn : n ≥0} the corresponding Binomial process with N0 = 0, and
let T = {Tk : k ≥1} be the process of success times.
(i) Show that P(Nn+1 = k) = pP(Nn = k −1) + qP(Nn = k), ∀n, k ≥0 and use this
result to ﬁnd P(Nn = k), k = 1, 2, . . ., n, n ≥0.
(ii) Find P(Nn+m −Nm
=
k), ∀m, n, k
≥
0 and show that Nn+m −
Nm|N0, N1, N2, . . . , Nm
d= Nn+m −Nm
d= Nn.
(iii) If n ≥k and k ≥1 then show that {ω : Tk(ω) ≤n} = {ω : Nn(ω) ≥k} and
{ω : Tk(ω) = n} = {ω : Nn−1(ω) = k −1} ∩{ω : Xn(ω) = 1}.
(iv) Show that Tk ∼NB(k, p).
(v) Prove that Tk+m −Tm|T1, T2, . . . , Tm
d= Tk+m −Tm
d= Tk.
(vi) Show that the increments T1, T2 −T1, T3 −T2, . . . , are iid according to a geo-
metric Geo(p).
Random walks
Exercise 6.2 Prove theorem 6.1.
Exercise 6.3 For the setup of example 6.3 ﬁnd the distribution of T1 the ﬁrst return
time to 0.
Exercise 6.4 Prove corollary 6.1.
Exercise 6.5 Let X = (X1, X2, . . . ) be an iid sequence of random variables, F X the
corresponding minimal ﬁltration and τ be an a.s. ﬁnite stopping time with respect
to F X. Deﬁne Yn(ω) = Xτ(ω)+n(ω) for n ≥1 and prove that the sequence Y =
(Y1, Y2, . . . ) has the same distribution as X and is independent of Fτ.
Exercise 6.6
Let τ1 ≤τ2 ≤. . . , be a sequence of stopping times with respect to
the minimal ﬁltration of a random walk S in Rp. Show that the variables S τ1, S τ2 −
S τ1, S τ3 −S τ2, . . . , are independent.
Exercise 6.7 Prove theorem 6.10.
Exercise 6.8
Let S denote a simple random walk on Zd and recall the setup
of example 6.3. Show that the following are equivalent: (i) P(T1 < ∞) = 1, (ii)
P(S n = 0 i.o) = 1, and (iii)
+∞
P
n=0 P(S n = 0) = +∞.
Exercise 6.9 Let S denote a simple random walk on Z with S 0 = 0 and show that
Xn = |S n| is a Markov chain and ﬁnd the transition matrix.
Martingales, ﬁltrations and stopping times
Exercise 6.10
Assume that F = (A1, A2, ...) is a ﬁltration. Show that
+∞
S
n=1 An is a
ﬁeld.
Exercise 6.11 Prove all statements of remark 6.2.
Exercise 6.12
A branching process is a sequence {Zn; n ≥0} of nonnegative
integer-valued random variables with Z0 = 1 and such that the conditional dis-
tribution of Zn+1 given (Z0, Z1, . . . , Zn) is that of a sum of Zn iid random vari-

EXERCISES
271
ables each having the same distribution as Z1. If E(Z1) = m ∈(0, ∞), show that
{Wn = Zn/mn; n ≥1} is a MG.
Exercise 6.13 Prove statements 5-9 of remark 6.3.
Exercise 6.14 Let X = {Xn; n ≥1} be a sequence of real-valued random variables
deﬁned on a probability space (Ω, A, P) and having ﬁnite mean. Prove that X is a
sMG with respect to a ﬁltration {Fn, n ≥1} to which it is adapted if and only if for
all m, n ≥0,
E(Xn+m|Fn) ≥Xn a.s.
Exercise 6.15 Prove statements 1-3 of remark 6.4.
Exercise 6.16 Consider a sequence of iid random variables {Xn}+∞
n=1 on a probability
space (Ω, A, P). Deﬁne the sequence of partial sums S n =
nP
i=1 Xi.
(i) Show in detail that σ(X1, X2, . . . , Xn) = σ(S 1, S 2, . . . , S n).
(ii) Assume that E(X−1
1 ) exists. Show that if m ≤n then E
S m
S n

= m/n.
Exercise 6.17 Let {Xn}+∞
n=1 be a sequence of random variables such that the partial
sums S n =
nP
i=1 Xi are a MG. Show that E(XiX j) = 0, i , j.
Renewal sequences
Exercise 6.18 For p ∈(0, 1) let X be a renewal sequence with waiting time distri-
bution f (n) = pn−1(1 −p), n > 0. Find the corresponding potential sequence.
Exercise 6.19 Prove the renewal sequence criterion, theorem 6.11.
Exercise 6.20
For p ∈[0, 1] let X be a renewal sequence with waiting time dis-
tribution W given by W({2}) = p = 1 −W({1}). Find the potential sequence of
X.
Exercise 6.21 Prove the statements of remark 6.6, parts 3 and 4.
Exercise 6.22
Let W, (π0, π1, π2, ...) and R denote the waiting time distribution,
the potential sequence and the renewal measure, respectively, for some renewal
sequence. Show that
P(R({k + 1, . . . , k + l}) > 0) =
kP
n=0 W({k + 1 −n, . . . , k + l −n})πn,
for all k, l = 0, 1, 2, ... and
kP
n=0 R({k + 1 −n, . . . , +∞})πn = 1,
for all k ∈Z+.
Exercise 6.23 Prove theorem 6.12.
Exercise 6.24
For which values of p is the sequence (1, 0, p, p, p, . . .) a poten-
tial sequence? For each such p ﬁnd the density of the corresponding waiting time
distribution.
Exercise 6.25 Prove theorem 6.14.
Exercise 6.26
Let W be a waiting time distribution with ﬁnite mean µ and let X

272
RANDOM SEQUENCES
be a delayed renewal sequence corresponding to W and the delay distribution D
deﬁned by
D({n}) = W({n + 1, . . ., ∞})/µ,
for n ≥0. Show that P(Xn = 1) = 1/µ, for all n ≥0.
Exercise 6.27 For a simple random walk S in Z let X denote the delayed renewal
sequence deﬁned by Xn = I{x} ◦S n for some x ∈Z. Find the pgf of the delay
distribution of X and use it to ﬁnd the probability that the delay equals ∞.
Markov chains
Exercise 6.28 For any Markov chain X with state space Ψ, show that
E(f (Xk, Xk+1, . . . )|σ({Xn : n ≤k})) = E(f (Xk, Xk+1, . . . )|σ(Xk)),
for all k ≥1 and bounded functions f on Ψ∞. Extend this result by replacing k with
any a.s. ﬁnite stopping time τ relative to F X and using the strong Markov property.
Exercise 6.29 Prove theorem 6.16.
Exercise 6.30
Let X be a Markov chain with discrete state space Ψ and operator
T. Show that
P(Xk+m = y|σ({Xn : n ≤k})) = T m(Xk, y),
for all k ≥1, m ≥0 and y ∈Ψ. Extend this result by replacing k with any a.s. ﬁnite
stopping time τ relative to F X and using the strong Markov property.
Hitting probabilities
Exercise 6.31
Consider a Markov chain X with state space Z+ and transition
measures µx({1}) =
1
x+1 and µx({x + 1}) =
x
x+1, x ∈Z+. Find H(x, 1).
Exercise 6.32 Let X be a Markov chain with state space Z and transition measures
µx({x −1}) = 1
2, µx({x + 2}) = 1
2, x ∈Z. Find H(x, 0).
Exercise 6.33 For all simple random walks X on Z, ﬁnd H(x, 0) and use it to ﬁnd
H+(x, 0).
Exercise 6.34 Prove theorem 6.17.
Exercise 6.35 Consider a simple random walk X on the integers {0, 1, . . ., k}, with
µ0({0}) = 1 = µk({k}) and µx({x −1}) = p = 1 −µx({x + 1}), 0 < p < 1. Find H(x, 0)
and H(x, k).
Exercise 6.36
Let X be a Markov chain with state space Z+ and transition mea-
sures µx({x + 1}) = (x−1)(x+2)
x(x+1)
and µx({1}) =
2
x(x+1), x ≥1. Calculate H(x, 1).
Exercise 6.37
Recall the Birth-Death Markov chain of example 6.19 but now
assume that µ0({0}) = 1, µx({x + 1}) =
x
x+1, x ∈Z+
0 and µx({x −1}) =
1
x+1, x > 0.
Find H(x, 0).
Exercise 6.38
Find Hk(x, y), the probability of visiting y, starting from x, for the
ﬁrst time in k-steps, ∀x, y ∈Ψ and k ≥1, for the Markov chains of examples 6.20
and 6.21.
State classiﬁcation

EXERCISES
273
Exercise 6.39 Prove equation (6.22) of remark 6.12.1.
Exercise 6.40 Prove the statement of remark 6.13.1.
Exercise 6.41
Consider an irreducible Markov chain with transition matrix T.
Show that if T(x, x) > 0 for some x, then all the states are aperiodic.
Exercise 6.42 Prove theorem 6.21.
Exercise 6.43 Classify the states of the Markov chains in examples 6.19, 6.20 and
6.21.
Stationarity and ergodicity
Exercise 6.44
Show in detail that the random sequences of example 6.30 are
stationary.
Exercise 6.45 Prove the statement of remark 6.19.5.
Exercise 6.46
Assume that X is a stationary Markov chain with countable state
space Ψ. Show that if X is irreducible then the sequence is ergodic.
Exercise 6.47 Show that the shift-invariant sets of equation (6.50) form a σ-ﬁeld.
Exercise 6.48
Consider a random sequence X with state space Ψ = {−1, 1} and
transition operator T such that T(−1, 1) = 1, T(1, −1) = 1 and initial distribution
P(X0 = 1) = P(X0 = −1) = 1/2. Show that X is a stationary Markov chain.
Exercise 6.49 Give an example of a Markov chain that has two diﬀerent stationary
distributions.
Exercise 6.50 Let S = (S 0, S 1, . . . ) be a random walk on Z+
0 with transition matrix
T given by T(x, x + 1) = p, T(x + 1, x) = 1 −p, x ≥0, with T(0, 0) = 1 −p.
(i) Show that S is an irreducible Markov chain.
(ii) Prove that the invariant measure for S is given by
π(x) = c (p/(1 −p))x ,
x = 1, 2, . . ., for some constant c. For what values of c and p is this the equilibrium
distribution of S ?
(iii) Find conditions in order for S to be ergodic.
Exercise 6.51 Prove Hopf’s maximal ergodic lemma (remark 6.20.2).
Exercise 6.52 Give an example of a reversible Markov chain that is periodic.
Exercise 6.53 Show that if X is irreducible and has a stationary distribution π then
any other stationary measure is a multiple of π.
Exercise 6.54 Prove equation (6.52).
Simulation and computation: use your favorite language to code the functions
Exercise 6.55
Consider the mixture of two beta distributions: 0.7 ∗Beta(4, 2) +
0.3 ∗Beta(1.5, 3).
(i) Build a Metropolis-Hastings algorithm to get a sample of size 1000 after a burn-
in sample of 500. Choose your own Beta(a, b) candidate distribution. What is the
M-H acceptance rate?

274
RANDOM SEQUENCES
(ii) Devise a random walk Metropolis algorithm to get a sample of size 1000 after
a burn-in sample of 500. Use a normal proposal distribution but be careful because
the target distribution only has support on [0, 1]. What is the standard deviation you
chose for the random walk? What is the acceptance rate?
Exercise 6.56 Consider a random sample X1, ..., Xn from N(µ, σ2).
(i) Suppose that we model the parameter vector θ = (µ, σ2), independently, accord-
ing to the conjugate priors N(ξ, τ2) and InvGamma(a, b).
(a) Show that the full conditional distributions are given by
µ|σ2, x
∼
N

τ2σ2
nτ2 + σ2

ξ
τ2 + nX
σ2
,
τ2σ2
nτ2 + σ2
, and
σ2|µ, x
∼
InvGamma

n
2 + a,
"1
b + 1
2
nP
i=1(xi −µ)2
#−1.
(b) Write a routine to implement this Gibbs sampler. Generate and plot the
chains for 10000 realizations (after 1000 burn-in). Choose a starting value θ0 =
[20, 1]T, and use ξ = 10, τ2 = 1, a = 10 and b = 0.1.
(ii) Now consider the Jeﬀreys independent prior.
(a) Give the marginal posterior distribution of µ and ﬁnd the posterior mean
of µ.
(b) Plot the marginal posterior density of µ.
(c) Find the 95% CS for µ.
(d) Give the marginal posterior distribution of σ2 and ﬁnd the posterior mean
of σ2.
(e) Plot the marginal posterior density of σ2.
(f) Find the 95% equal tail CS for σ2.
(iii) Now we consider a conjugate prior for (µ, σ2): µ|σ2 ∼N(0, σ2/κ) and σ2 ∼
InvGamma(a, b), where κ = 0.25, α = 0.5 and b = 0.5. Repeat parts (a)–(f) of part
(ii).

Chapter 7
Stochastic Processes
7.1
Introduction
Continuous time parameter stochastic processes can be used to create and in-
vestigate random objects as they evolve continuously in time. The random objects
we discuss in this chapter include real-valued random functions, random derivatives
and random integrals. We begin with the general deﬁnition of a stochastic process.
Deﬁnition 7.1 Stochastic process
A stochastic process with state space Ψ is a collection of random objects X =
{Xt : t ∈T} with Xt taking values in Ψ and deﬁned on the same probability space
(Ω, A, P).
Example 7.1 (Stochastic processes)
We discuss some speciﬁc examples of
stochastic processes next.
1. Suppose that the experiment consists of observing the acceleration of a vehicle
during the ﬁrst 10 seconds of a race. Then each possible outcome is a real-valued,
right-continuous function ω deﬁned for 0 ≤t ≤10 and the sample space Ωis the
set of all such functions. We deﬁne the acceleration at time t as the random variable
Xt(ω) = ω(t), for each ω ∈Ω, so that the collection X = {Xt : 0 ≤t ≤10}, is a
continuous time-parameter stochastic process with state space Ψ = R.
2. Arrival process Assume that the process N = {Nt : t ≥0} is such that the func-
tion Nt(ω), for ﬁxed ω and as a function of t, namely, t 7→Nt(ω), is non-decreasing,
right-continuous and increases by jumps only. Stochastic processes with these prop-
erties are known as arrival processes. The process N is a continuous time-parameter
stochastic process.
3. Interarrival times Consider the process of arrivals of customers at a store and
suppose that the experiment consists of measuring interarrival times. The sample
space Ωis the set of all sequences ω = (ω1, ω2, . . . ) with ωi ≥0, for all i. For
each ω ∈Ωand t ≥0, we set Nt(ω) = k if and only if the integer k is such that
275

276
STOCHASTIC PROCESSES
ω1 + · · · + ωk ≤t ≤ω1 + · · · + ωk+1 (Nt(ω) = 0, if t < ω1). Then for the outcome
ω, Nt(ω) denotes the number of arrivals in the time interval [0, t] and therefore
N = {Nt : t ≥0} is an arrival process with state space Ψ = {0, 1, . . .}.
4. Queueing systems Consider an experiment where customers arrive at a store re-
quiring service, e.g., airplanes arrive at an airport and need to land or patients arrive
at a hospital and need to be treated. The system may have certain restrictions, in-
cluding the number of servers (e.g., physicians in the emergency room of a hospital)
or the number of customers allowed in the waiting room. Now if there are too many
arrivals the physicians will not be able to keep up and the patients will experience
delays in their treatment, i.e., they have to wait in line (or in the queue) to get to
see the doctor and then have to wait an additional time period, the time they receive
their treatment, until they exit the system.
Queueing theory refers to the area of probability theory and stochastic pro-
cesses that studies the characteristics of such systems. There are several stochastic
processes required in order to study these systems. Arrivals at the system can be
modeled using a continuous time stochastic process A = {At : t ≥0}, where At
denotes the number of arrivals in the interval [0, t]. In contrast, a discrete time
stochastic process S = {S n : n = 1, 2, . . .} can be used to model the service times of
each customer (typically iid) and similarly, for the waiting time of each customer
we can build another discrete time stochastic process W = {Wn : n = 1, 2, . . . }.
Finally, we are interested in the queue-size process Y = {Yt : t ≥0}, where
Yt ∈Ψ = {0, 1, 2, . . .} denotes the total number of customers in the queue at time
t which includes the number of customers in the waiting room and the number of
customers being currently serviced. In addition, future values of the process Yt+s
at time t + s depend not only on the current value Yt but also on any departures or
arrivals in the time interval [t, t + s]. We will see that we can make the problem
more tractable by deﬁning an underlying Markov chain that allows us to describe
the moves of Yt (see example 7.6), as well as study the general behavior of Yt.
The following notation is used to summarize the important characteristics of
a queueing system; we write A/S/k/q queue, where A abbreviates the arrival pro-
cess, S denotes the distribution of the service times, k denotes the number of servers
and q the size of the waiting room of the queue. For example, G/M/2/∞denotes a
queue with a general arrival process, an exponential service time, 2 servers and an
inﬁnite waiting room. If q = ∞we typically write G/M/2 (omit the waiting room
size).
Similar to the discrete time parameter case, the following deﬁnitions can be
used to identify a wide collection of stochastic processes with certain desirable
properties.
Deﬁnition 7.2 Stationarity and independence of increments
A stochastic process X = {Xt : t ∈T} is said to have stationary increments if the

THE POISSON PROCESS
277
distribution of the increment Xt+s −Xt does not depend on t. The stochastic process
X is said to have independent increments if for any n ≥1 and s0 = 0 < s1 < s2 <
· · · < sn, the increments Xs1 −Xs0, Xs2 −Xs1, . . . , Xsn −Xsn−1 are independent.
7.2
The Poisson Process
The most important arrival process is the Poisson process.
Deﬁnition 7.3 Poisson process
An arrival process N = {Nt : t ≥0} is called a Poisson process if the following
properties hold:
(P1) each jump of the function t 7→Nt(ω) is of unit magnitude,
(P2) ∀t, s ≥0, Nt+s −Nt is independent of {Nu : u ≤t},
(P3) ∀t, s ≥0, the distribution of Nt+s −Nt is independent of t.
Each random variable Nt is interpreted as the number of arrivals (or events) of the
process in the interval [0, t].
Example 7.2 (Poisson process via random walks) Consider a random walk S
in R
+ with Exp(1) step distribution. Clearly, S n ∼Gamma(n, 1), n > 0 and for
ﬁxed t ∈R+ let Nt(ω) = #{n : 0 < S n(ω) ≤t}. Since each step is a.s. positive,
Nt a.s. equals the number of steps taken by the random walk before it reaches the
interval (t, +∞). For k ≥0 we have
P({ω
:
Nt(ω) = k}) = P({ω : Nt(ω) ≥k}) −P({ω : Nt(ω) ≥k + 1})
=
P({ω : S k(ω) ≤t}) −P({ω : S k+1(ω) ≤t}) = tke−t/k!,
and therefore Nt is a Poisson(t) random variable.
Some comments are in order regarding properties (P1)-(P3) and their conse-
quences.
Remark 7.1 (Poisson process properties) Note that once we relax the three as-
sumptions the general theory cannot be applied, although one can still obtain spe-
cialized results (see for example C¸ inlar, 1975).
1. Property (P2) expresses the independence of the number of arrivals in (t, t + s]
from past history of the process up to time t and leads to the property of independent
increments. Property (P3) simply states that the process is stationary.
2. Using deﬁnition 7.3 we can show that P(Nt = 0) = e−λt, t ≥0, for some constant
λ ≥0, known as the intensity of the process. Moreover, the number of events Nt
in the interval [0, t] is distributed according to a Poisson(λt) and as a consequence
we write N ∼PP(λ) to refer to a stationary Poisson process. Note that the average
number of arrivals in [0, t] is E(Nt) = λt.

278
STOCHASTIC PROCESSES
3. Using (P1)-(P3) and part 2 of this remark we can easily see that Nt+s −Nt|{Nu :
u ≤t}
d= Nt+s −Nt
d= Ns.
4. The three properties can be relaxed, leading to more complicated Poisson pro-
cesses. Relaxing (P1) to allow jumps of any size leads to the compound Poisson
process: (P1)′: the function t 7→Nt(ω) has ﬁnitely many jumps in any ﬁnite inter-
val a.e. Properties (P1) and (P2) are highly applicable in applications but (P3) is
not. Removing property (P3) from deﬁnition 7.3 deﬁnes the non-stationary Poisson
process. Although harder to work with, we can still obtain some characteristics of
the Poisson process even in this case.
5. First characterization A stochastic process N = {Nt : t ≥0} is a Poisson
process with intensity λ if and only if (P1) holds and E(Nt+s −Nt|Nu : u ≤t) = λs.
6. Second characterization A stochastic process N = {Nt : t ≥0} is a Poisson
process with intensity λ if and only if
P(NB = k) = e−λb(λb)k/k!,
k ≥0, for all sets B ⊂R+ that are unions of ﬁnite collections of disjoint intervals
with the length of B being µ1(B) = b. Here NB denotes the number of arrivals over
the set B.
7. Conditioning Let B = ∪n
i=1Ai for some disjoint intervals {Ai} with lengths ai
and set b = a1 + · · · + an. Then for k1 + · · · + kn = k with k1, . . . , kn = 0, 1, 2, . . ., we
can show that
P(NA1 = k1, . . . , NAn = kn|NB = k) = Ck
k1,k2,...,kn(a1/b)k1 . . . (an/b)kn.
Example 7.3 (Queueing systems: Poisson arrivals)
Consider a queueing sys-
tem and assume that the arrival process is N ∼PP(λ), that is, a Poisson process
with some rate λ so that the number of events (arrivals of customers) Nt in the
interval [0, t] is such that Nt ∼Poisson(λt) (the notation for the queue becomes
M/././. and if we further assume that service times follow an exponential distri-
bution we write M/M/./.). Arrivals in this case occur independently of each other.
The process of the times of arrivals {Tn}+∞
n=1 of the Poisson process can be particu-
larly useful in calculating lengths of time including a customer’s remaining time in
the queue or the time until their service begins. See exercise 7.4 for some insight
on this process. We discuss some classic examples of queueing systems below.
1. M/M/1 queue In the M/M/1 queue customers arrive at the times of a Pois-
son process and each requires an independent amount of service distributed as an
Exp(µ) distribution. Furthermore, there is one server and an inﬁnite size waiting
room. Examples of this queue include patients arriving at a doctor’s oﬃce and re-
quiring treatment or customers waiting to use an automated vending machine.
2. M/M/k queue Consider the M/M/1 queue and assume that there are k servers.

GENERAL STOCHASTIC PROCESSES
279
A bank with k tellers or a movie theater with k ticket counters are examples of such
queues.
3. M/M/∞queue Now assume that there is an inﬁnite number of servers so that
the instant a customer arrives their service begins immediately. Large parking lots
and telephone traﬃc can be approximated well by such a queueing system, e.g.,
assuming that the number of parking spots or phone lines is inﬁnite, a customer
will always ﬁnd a server.
4. M/M/k/m queue This queue is similar to the M/M/k queue but now there are
only m places in the waiting room. Customers arriving to ﬁnd the waiting room of
the queue full, leave the system immediately never to return. For example there is
a limited number of seats, say m, in the waiting room of the department of motor
vehicles and k servers to service the customers.
The Poisson process has been the subject of many texts (see Section 7.6). Here
we have discussed the basic deﬁnition and properties of this process and we will
discuss more of its uses once we collect Markov processes (see example 7.5). We
further revisit this process in the TMSO-PPRS text where we consider a random
object known as the Poisson point process. We collect general deﬁnitions and prop-
erties of stochastic processes next.
7.3
General Stochastic Processes
A general stochastic process is a collection of random objects X = {Xt : t ≥0}
on some probability space (Ω, A, P) where Xt(ω), for ﬁxed t ≥0, is assumed to be
a measurable map from (Ω, A) into some measurable space (Ψ, G). The space Ψ is
known as the state space of X. We collect some important deﬁnitions and properties
in the following.
Remark 7.2 (General deﬁnitions) Since Xt(ω) is really a function of two argu-
ments, for technical reasons, it is convenient to deﬁne joint measurability proper-
ties.
1. Joint measurability A stochastic process X is called measurable if for every
B ∈G the set {(ω, t) : Xt(ω) ∈B} belongs to the product σ-ﬁeld A
N
B(R+
0); in
other words, if the mapping
(ω, t) 7→Xt(ω) : (Ω× R+
0, A
N
B(R+
0)) →(Ψ, G)
is measurable.
2. Equivalent stochastic processes Consider two Rp-valued stochastic processes
X and Y deﬁned on the same probability space (Ω, A, P). When viewed as functions
of ω and t we would say that X and Y are the same if and only if Xt(ω) = Yt(ω),
for all ω ∈Ωand t ≥0. However, since we are working on a probability space we
weaken this requirement in three diﬀerent ways:
(a) Y is a modiﬁcation of X if we have P(Xt = Yt) = 1, ∀t ≥0.

280
STOCHASTIC PROCESSES
(b) X and Y have the same ﬁnite-dimensional distributions if ∀n ≥1, real numbers
0 ≤t1 < t2 < · · · < tn < ∞and B ∈Bnp, we have
P((Xt1, . . . , Xtn) ∈B) = P((Yt1, . . . , Ytn) ∈B).
(c) X and Y are called indistinguishable if P(Xt = Yt; ∀t ≥0) = 1.
Note that (c) ⇒(a) ⇒(b).
3. Sample paths and RCLL For ﬁxed ω ∈Ωthe function t 7→Xt(ω), t ≥0, is
called the sample path (realization or trajectory) of the process X. When the paths
of X are continuous maps then the stochastic process X is called continuous. We
say that a stochastic process X has RCLL (right-continuous and left-hand limits)
paths if Xt(ω), for ﬁxed ω ∈Ω, is right-continuous on [0, ∞) with ﬁnite left-hand
limits on (0, ∞). Therefore, X consists of random objects that are random maps
(or functions if Ψ = Rp) and we need to build an appropriate measurable space in
order to work with such objects. The main ingredient needed for such constructions
is a well-behaved space Ψ for maps (e.g., Polish spaces). In particular, consider the
following example.
Example 7.4 (Polish spaces)
Recall that a Polish space is a complete metric
space that has a countable dense subset. For example, R equipped with the Eu-
clidean metric is a Polish space with the rationals constituting a countable set. In
addition, R is a Polish space using as metric d(x, y) = | arctan y −arctan x| and the
rationals as a dense subset. Completeness of the space depends on the choice of
metric. Using d(x, y) in R we do not have completeness in R. Now Rp is also Polish
using the Euclidean metric
ρp(x, y) =
s
pP
i=1(xi −yi)2,
and a dense subset of the set of points with rational coordinates. Finally, R∞is the
space of all inﬁnite sequences (x1, x2, . . . .) of real numbers and it can be metrized
using
ρ∞(x, y) =
+∞
P
i=1 2−i min{1, |yi −xi|},
where x = (x1, x2, . . . .) and y = (y1, y2, . . . .).
The following remark summarizes general results for Polish spaces.
Remark 7.3 (Polish spaces) We note the following.
1. Let (Ψ j, ρj), j = 1, 2, . . ., be Polish spaces and deﬁne Ψ =
+∞
×
j=1Ψ j with the
topology on Ψ being the product topology and deﬁne
ρ(x, y) =
+∞
P
j=1 2−j min{1, ρj(xj, yj)}.
Then it can be shown that (Ψ, ρ) is a Polish space.

GENERAL STOCHASTIC PROCESSES
281
2. Let CR
[0,1] denote the space of continuous R-valued functions deﬁned on [0, 1]
and deﬁne a metric on CR
[0,1] by
d(f, g) = max{| f (t) −g(t)| : t ∈[0, 1]}.
Then (CR
[0,1], d) is a Polish space and the Borel σ-ﬁeld is given by
B(CR
[0,1]) = σ

{ f ∈CR
[0,1] : f (t) ∈B; t ∈[0, 1], B ∈B(R)}

.
Similar results exist for CR
[0,∞), the space of continuous R-valued functions deﬁned
on [0, ∞).
3. A closed subset of a Polish space is a Polish space with the same metric.
4. For any Polish space Ψ there exists a function f : Ψ →B where B ∈B([0, 1]∞)
such that f is continuous and f −1 exists.
7.3.1
Continuous Time Filtrations and Stopping Times
Next we present deﬁnitions regarding continuous time ﬁltrations and stopping
times, along with some of their properties.
Remark 7.4 (Continuous time ﬁltrations and stopping times) For what follows,
let (Ω, A, P) be some probability space and let Ft be a sub-σ-ﬁeld of A for each
t ≥0.
1. Filtrations The collection of sub-σ-ﬁelds F = {Ft : t ≥0} is called a continu-
ous time ﬁltration in (Ω, A) if s < t implies that Fs ⊆Ft. For any ﬁltration F we
set F∞= σ
 S
t≥0 Ft
!
. The ﬁltration F is called complete if F0 contains all null-sets
N(A) of A, where
N(A) = {N ∈A : inf{P(A) : N ⊂A, A ∈A} = 0}.
If Ft = σ({Xs : 0 ≤s ≤t}) then F is called the minimal ﬁltration denoted by
F X = {F X
t : t ≥0}.
2. Right continuous ﬁltrations Deﬁne Ft+ = T
ε>0 Ft+ε so that Ft ⊆Ft+. If Ft+ =
Ft, ∀t ≥0, then we say that the ﬁltration F+ is right-continuous. Most ﬁltrations we
require in order to build a theory for general stochastic processes are right continu-
ous. The ﬁltration F is said to satisfy the usual conditions if it is right-continuous
and complete.
3. Adapted process We say that the stochastic process X is adapted to a ﬁltration
F if Xt is measurable with respect to Ft, ∀t ≥0 and we can write the process and
ﬁltration as a pair {Xt, Ft, ∀t ≥0}.
4. Predictability Deﬁne the predictable σ-ﬁeld P on Ω× R+
0 by
P = σ({X : X is left continuous, bounded and adapted to F }),
which is equivalent to the generated collection of all sets of the form
{(ω, t) ∈Ω× R+
0 : Xt(ω) > a},

282
STOCHASTIC PROCESSES
for a ∈R and X is a left continuous, bounded stochastic process, adapted to F .
A stochastic process X : Ω× R+
0 →R is called predictable if it is measurable
with respect to P. Obviously, if X is continuous, bounded and adapted then it is
predictable.
5. Progressive measurability Assume that X is Rp-valued and it is adapted to a
ﬁltration F that satisﬁes the usual conditions. If the function Xs(ω), for (ω, s) ∈
Ω× [0, t] is measurable with respect to Ft
N
B([0, t]), ∀t ≥0, then X is said to
have the progressive measurability property with respect to F ; in other words, we
have
{(ω, s) ∈Ω× [0, t] : Xs(ω) ∈B} ∈Ft
N
B([0, t]),
for all t ≥0 and B ∈B(Rp). It can be shown that if X is adapted to F and has RCLL
paths (or simply continuous paths) then it is progressively measurable. If X is not
adapted but has RCLL paths then X is measurable (joint measurability, remark
7.2.1). Moreover, it can be shown that if X is adapted and right-continuous then
X is progressively measurable. Finally, if X is predictable then it is progressively
measurable.
6. Stopping times An R
+-valued random variable τ deﬁned on (Ω, A) is a stop-
ping time with respect to the ﬁltration F if {ω : τ(ω) ≤t} ∈Ft. The collection of
prior events is denoted by Fτ = {A ⊂Ω: A ∩{ω : τ(ω) ≤t} ∈Ft, ∀t ≥0} and it is a
σ-ﬁeld. Note that τ is measurable with respect to Fτ and if τ and ξ are two stopping
times with τ ≤ξ then Fτ ⊆Fξ and Fτ∧ξ = Fτ ∩Fξ. A stopping time τ is called an
optional time for the ﬁltration F if {ω : τ(ω) < t} ∈Ft, ∀t ≥0. It can be shown that
τ is optional if and only if it is a stopping time of the right-continuous ﬁltration F+.
7. Functions of stopping times If τ, ξ are stopping times with respect to F then
so are τ ∧ξ, τ ∨ξ and τ + ξ. If {τn}+∞
n=1 are stopping times then so is sup
n≥1
τn. If {τn}+∞
n=1
is a sequence of optional times then so are sup
n≥1
τn, inf
n≥1τn, lim
n→+∞τn and lim
n→+∞
τn (these
hold also for stopping times if F+ = F ).
8. Hitting times Let X be an Rp-valued stochastic process with continuous sample
paths and A a closed subset of B(Rp). Then the hitting time of A by X deﬁned by
HA(ω) = inf{t ≥0 : Xt(ω) ∈A} is a stopping time with respect to any ﬁltration with
respect to which X is adapted.
9. Subordinated Markov chains to stochastic processes If X has RCLL paths
and it is adapted to a ﬁltration F that satisﬁes the usual conditions then there exists
a sequence of stopping times {τn}+∞
n=1 with respect to F which exhausts the jumps of
X, i.e.,
{(ω, t) ∈Ω× R+ : Xt(ω) , Xt−(ω)} ⊆
+∞
S
n=1{(ω, t) ∈Ω× R+ : τn(ω) = t}.
(7.1)
Note that this allows us to build an underlying stochastic process Y in discrete time,
i.e., Yn = Xτn, n ∈Z+
0 and then use established results (e.g., for Markov chains) to

GENERAL STOCHASTIC PROCESSES
283
study the behavior of the stochastic process X. The stochastic process Yn is said to
be subordinated to the stochastic process X.
10. Increasing stochastic process An adapted stochastic process A is called in-
creasing if a.e. [P] (i) A0(ω) = 0, (ii) the function t 7→At(ω) is nondecreasing and
right-continuous, and (iii) E(At) < ∞, ∀t ≥0. An increasing stochastic process A
is called integrable if E(A∞) < ∞, where A∞= lim
t→+∞At. We write (A, F ) when A is
adapted to a ﬁltration F .
11. Classes of stopping times Let LF
a denote the collection of all stopping times
τ with respect to F that satisfy P(τ ≤a) = 1, for a given ﬁnite a > 0 and deﬁne LF
as the collection of all stopping times with respect to F that satisfy P(τ < +∞) = 1.
12. Classes of stochastic processes The right-continuous stochastic process {Xt,
Ft, t ≥0} is said to be of class D if the collection of random variables {Xτ}τ∈LF is
uniformly integrable and of class DL if the collection of random variables {Xτ}τ∈LF
a
is uniformly integrable for all 0 ≤a < +∞. Note that if Xt ≥0 a.s., ∀t ≥0, then X
is of class DL.
13. Uniform integrability A stochastic process X is said to be uniformly inte-
grable if the collection {Xt : t ≥0} is uniformly integrable (see deﬁnition 3.17).
14. Bounded variation A stochastic process X is said to have paths of bounded
variation if its sample paths are functions of bounded variation w.p. 1 (see remark
A.1.10). The stochastic process X is said to have paths of locally bounded varia-
tion if there exist stopping times τn →∞such that the process Xt∧τn has paths of
bounded variation w.p. 1, for all n.
7.3.2
Continuous Time Martingales
Continuous time ﬁltrations and MGs play a prominent role in the development
of many stochastic processes. The MG deﬁnitions and results of Section 6.3 can
be extended easily (for the most part) to the continuous time parameter case. The
following remark summarizes the deﬁnitions, properties and basic results regarding
continuous time MGs.
Remark 7.5 (Continuous time martingales) In view of remark 7.4 the generaliza-
tion of discrete time MGs (Section 6.3) to continuous time MGs is straightforward.
1. Deﬁnition Consider a real-valued stochastic process X = {Xt : t ≥0} on
(Ω, A, P) that is adapted to a given ﬁltration F = {Ft : t ≥0} and such that
E|Xt| < ∞, ∀t ≥0. The pair (X, F ) = {Xt, Ft : t ≥0} is said to be a continuous time
martingale (MG) if for every 0 ≤s < t < ∞we have E(Xt|Fs) = Xs a.s. [P]. It is
a submartingale (sMG) if E(Xt|Fs) ≥Xs a.s. [P] and a supermartingale (SMG) if
E(Xt|Fs) ≤Xs a.s. [P]. If F is not mentioned we assume the minimal ﬁltration F X.
If the sample paths are (right-) continuous functions then we call (X, F ) a (right-)
continuous MG (or sMG or SMG).

284
STOCHASTIC PROCESSES
2. Last element Assume that (X, F ) is a sMG. Let X∞be an integrable F∞-
measurable random variable. If ∀t ≥0 we have E(X∞|Ft) ≥Xt a.s. [P], then we say
that (X, F ) is a sMG with last element X∞. Similarly for the MG and SMG cases.
3. Convex transformation Let g be a convex, increasing, R-valued function and
(X, F ) a sMG such that g(Xt) is integrable, ∀t ≥0. Then {g(Xt), Ft : t ≥0} is a
sMG.
4. Convergence Assume that (X, F ) is a right-continuous sMG and assume that
sup
t≥0
E(X+
t )
<
∞. Then there exists an integrable last element X∞such that
lim
t→∞Xt(ω) = X∞(ω) a.e. [P].
5. Optional sampling Let (X, F ) be a right-continuous sMG with a last element
X∞and let τ ≤ξ be two optional times of the ﬁltration F . Then we have
E(Xτ|Fξ+) ≥Xξ a.s. [P], where Fξ+ = {A ⊂Ω: A ∩{ω : ξ(ω) ≤t} ∈Ft+, ∀t ≥0}. If
ξ is a stopping time then E(Xτ|Fξ) ≥Xξ a.s. [P]. In particular, E(Xτ) ≥E(X0) and
if (X, F ) is a MG then E(Xτ) = E(X0).
6. Regularity A sMG (X, F ) is called regular if for every a > 0 and every non-
decreasing sequence of stopping times {τn}+∞
n=1 ⊆Lτ
a with τ = lim
n→∞τn, we have
lim
n→∞E(Xτn) = E(Xτ). The reader can verify that a continuous, nonnegative sMG is
regular.
7. Square and uniform integrability A right-continuous MG (X, F ) is said to be
square integrable if EX2
t < +∞, ∀t ≥0 (equivalently sup
t≥0
EX2
t < +∞). In addition,
assume that X0 = 0 a.s. and denote the collection of all such MGs by M2 (or Mc
2
for continuous X). Moreover, if (X, F ) is a uniformly integrable MG then it is of
class D. Deﬁne the L2 (pseudo) norm on M2 by
∥X∥t =
q
E
h
X2
t
i
,
and set
∥X∥=
+∞
P
n=1 2−n min {1, ∥X∥n} ,
so that ρ(X, Y) = ∥X −Y∥, X, Y ∈M2, is a (pseudo) metric. It becomes a metric in
M2 if we identify indistinguishable processes, i.e., for any X, Y ∈M2, ∥X −Y∥= 0,
implies that X and Y are indistinguishable. As a result, M2 becomes a complete
metric space equipped with ρ.
8. Doob-Meyer decomposition An extension of Doob’s decomposition (theorem
6.8) to the continuous time parameter case is as follows; assume that F is a ﬁltration
that satisﬁes the usual conditions and let (X, F ), a right-continuous sMG, be of
class DL. Then X can be decomposed as Xt = Mt + At, ∀t ≥0, where (M, F ) is a
right-continuous MG and (A, F ) is an increasing process. Requiring (A, F ) to be

GENERAL STOCHASTIC PROCESSES
285
predictable makes the decomposition unique (up to indistinguishability). If X is of
class D then (M, F ) is a uniformly integrable MG and A is integrable.
9. Quadratic variation If X ∈M2 we have that (X2, F ) is a nonnegative sMG
(i.e., of class DL) and therefore X2 has a unique Doob-Meyer decomposition
X2
t = Mt + At, t ≥0,
(7.2)
where (M, F ) is a right-continuous MG and (A, F ) is an adapted, predictable, in-
creasing process. It is convenient to center M in order to have M0 = 0 a.s. and since
A0 = 0 a.s. [P], X2
0 = 0 a.s. [P]. If X ∈Mc
2 then Mt and At are continuous. We
deﬁne the quadratic (or square) variation of the MG X to be the stochastic process
⟨X⟩t
d= At,
i.e., ⟨X⟩is the unique, adapted, predictable, increasing process that satisﬁes ⟨X⟩0 =
0 a.s. and X2 −⟨X⟩(= M) is a MG. In addition, if X, Y ∈M2 we deﬁne the cross-
variation process ⟨X, Y⟩by
⟨X, Y⟩t = 1
4
⟨X + Y⟩t −⟨X −Y⟩t
 ,
t ≥0. Note that XY −⟨X, Y⟩is a MG and ⟨X, X⟩= ⟨X⟩.
10. Local martingales Let (X, F ) be a (continuous) stochastic process. We say
that (X, F ) is a (continuous) local MG if
(i) there exists a nondecreasing sequence of stopping times {τn}+∞
n=1 relative to F
such that {X(n)
t
= Xt∧τn, Ft : t ≥0} is a MG for all n ≥1 and
(ii) P( lim
n→+∞τn = ∞) = 1.
If, in addition, X0 = 0 a.s. [P] we write X ∈Mloc (or X ∈Mc,loc if X is continuous).
Note that every MG is a local MG, whereas, a local MG of class DL is a MG.
11. Semi martingales A semi MG is a stochastic process X of the form Xt = Mt +
At, where M ∈Mc,loc is a local MG and At is a stochastic process with RCLL paths
that are locally of bounded variation. As a result of the Doob-Meyer decomposition,
sMGs and SMGs are semi MGs.
7.3.3
Kolmogorov Existence Theorem
The stochastic process X = {Xt : t ∈T}, T ⊆R+
0, Ψ = R, of random variables
deﬁned on a probability space (Ω, A, P) is typically deﬁned in terms of the ﬁnite-
dimensional distributions it induces in Euclidean spaces. More precisely, for each
k-tuple t = (t1, . . . , tk) of distinct elements of T, the random vector (Xt1, . . . , Xtk) has
distribution
Pt(B) = P((Xt1, . . . , Xtk) ∈B) = Qt(Y−1
k (B)),
(7.3)
for all B ∈Bk and t ∈T k, where Yk = (Xt1, . . . , Xtk) and Qt denotes the induced
probability measure, which is deﬁned on the measurable space (Rk, Bk). Note that
this system of distributions does not completely determine the properties of a pro-
cess X. In particular, (7.3) does not adhere to properties of the sample paths of the

286
STOCHASTIC PROCESSES
process, e.g., when X has RCLL or if X is continuous. See the discussion in remark
7.6 below in order to appreciate why this problem arises.
When a process is deﬁned in terms of (7.3) then {Pt1,...,tk} is called a consistent
family of distributions and it is easily seen that it satisﬁes two (consistency) prop-
erties:
(a) if s = (ti1, . . . , tik) is any permutation of t = (t1, . . . , tk) then ∀Bi ∈B1,
i = 1, 2, . . ., k, we have
Pt(B1 × · · · × Bk) = Ps(Bi1 × · · · × Bik),
(7.4)
(b) if t = (t1, . . . , tk) with k ≥1, s = (t1, . . . , tk−1) and B ∈Bk−1 we have
Pt(B × R) = Ps(B).
(7.5)
Kolmogorov’s existence theorem goes the other direction. More precisely, if we are
given a system of ﬁnite-dimensional distributions {Pt1,...,tk} that satisfy the consis-
tency conditions then there exists a stochastic process X = {Xt : t ≥0} having
{Pt1,...,tk} as its ﬁnite dimensional distributions.
We collect this important theorem next. The proof is given for stochastic process
X with paths Xt ∈RR
[0,∞), where RR
[0,∞) is the space of all R-valued functions deﬁned
on [0, ∞). First we consider a k-dimensional cylinder set in RR
[0,∞) deﬁned as a set
of the form
Ck = {Xt ∈RR
[0,∞) : (Xt1, . . . , Xtk) ∈Bk},
(7.6)
with ti ∈[0, ∞), i = 1, 2, . . ., k and base Bk ∈Bk (recall remark 3.5.7). Let C denote
the ﬁeld of all cylinder sets deﬁned on all cylinder sets in RR
[0,∞) with k < ∞and let
B(RR
[0,∞)) be the Borel σ-ﬁeld on RR
[0,∞) (the smallest σ-ﬁeld containing C). Denote
by T ∗the set of ﬁnite sequences t = (t1, . . . , tk) of distinct, nonnegative numbers,
for all k = 1, 2, . . .. Note that this construction loses track of the underlying prob-
ability space (Ω, A, P), that is, we look for the induced probability measure on

RR
[0,∞), B(RR
[0,∞))

. However, as we see below, the existence theorem can be restated
equivalently in terms of a stochastic process deﬁned on (Ω, A, P).
Theorem 7.1 (Kolmogorov existence: of a probability distribution) Let {Pt}
be a consistent family of ﬁnite-dimensional distributions. Then there exists a
probability measure Q on the measurable space (RR
[0,∞), B(RR
[0,∞))) such that
Pt(B) = Q
n
Xt ∈RR
[0,∞) : (Xt1, . . . , Xtk) ∈B
o
,
(7.7)
for all B ∈Bk and t ∈T ∗.
Proof. First we deﬁne a set function QC on the cylinders C and then use the
Carath´eodory extension theorem to show that there is a unique extension of QC on
σ(C) = B(RR
[0,∞)), the desired probability measure Q. That is, we need to show that
the entertained QC is a probability measure on the ﬁeld C. In particular, if Ck ∈C
and t = (t1, . . . , tk) ∈T ∗deﬁne
QC(Ck) = Pt(Bk).

GENERAL STOCHASTIC PROCESSES
287
Clearly, QC(C) ≥0, ∀C ∈C and QC

RR
[0,∞)

= 1. Now let A1, A2 ∈C be two disjoint
k-dimensional cylinders with bases B1, B2 ∈Bk, respectively, and note that B1 and
B2 are also disjoint (otherwise A1 and A2 would not be disjoint). Therefore we can
write
QC(A1 ∪A2) = Pt(B1 ∪B2) = Pt(B1) + Pt(B2) = QC(A1) + QC(A2),
so that QC is ﬁnitely additive. It remains to show that QC is countably additive. Let
B1, B2, · · · ∈C be a sequence of disjoint cylinders and assume that B =
+∞
S
i=1 Bi ∈C.
Deﬁne Cm = B ∖
mS
i=1 Bi ∈C, m = 1, 2, . . . and note that
QC(B) = QC(Cm) +
mP
i=1 QC (Bi) .
Countable additivity will follow if we show that
lim
m→∞QC(Cm) = 0.
(7.8)
Since QC(Cm) = QC(Cm+1)+QC(Bm+1) ≥QC(Cm+1) the limit in (7.8) exists and note
that by construction C1 ⊇C2 ⊇C3 ⊇. . . , with lim
m→∞Cm =
+∞
T
m=1Cm = ∅, i.e., {Cm}+∞
m=1
is a decreasing sequence of C-sets with Cm ↓∅. From exercise 4.6 if we show that
QC(Cm) ↓0, then we will have that QC is countably additive and the result will be
established.
We prove the latter by contradiction, that is, assume that lim
m→∞QC(Cm) = ε > 0
and we show that this assumption leads to a contradiction, so that ε has to be 0.
More precisely, we will show that assuming ε > 0 leads to
+∞
T
m=1Cm , ∅, a contra-
diction, since
+∞
T
m=1Cm = ∅by construction.
Based on the cylinders {Cm}+∞
m=1 with bases {Akm}+∞
m=1 (where km , m in gen-
eral) we build another sequence {Dm}+∞
m=1 of C-sets with bases {Am}+∞
m=1 as follows;
assume ﬁrst that
Cm = {Xt ∈RR
[0,∞) : (Xt1, . . . , Xtkm) ∈Akm},
with Akm ∈Bkm and tkm = (t1, . . . , tkm) ∈T ∗, for some km = 1, 2, . . . and since
Cm+1 ⊆Cm we permute and expand the base of Cm+1 using the base representation
of Cm so that tkm+1 is an extension of tkm and Akm+1 ⊆Akm × Rkm+1−km. In particular,
deﬁne D1 = {Xt ∈RR
[0,∞) : Xt1 ∈R}, . . . , Dk1−1 = {Xt ∈RR
[0,∞) : (Xt1, . . . , Xtk1−1) ∈
Rk1−1}, Dk1 = C1, Dk1+1 = {Xt ∈RR
[0,∞) : (Xt1, . . . , Xtk1, Xtk1+1) ∈Ak1 × R}, . . . ,
Dk2 = C2 and continue this way for all Cm. Note that by construction we have:
D1 ⊇D2 ⊇D3 ⊇. . . , with lim
m→∞Dm =
+∞
T
m=1 Dm =
+∞
T
m=1Cm = ∅, lim
m→∞QC(Dm) = ε > 0,
Dm ∈C and each Dm is of the form
Dm = {Xt ∈RR
[0,∞) : (Xt1, . . . , Xtm) ∈Am},
with Am ∈Bm. From exercise 3.29.1 there exists a compact (closed and bounded)

288
STOCHASTIC PROCESSES
set Km ⊂Am that can be used as a base to deﬁne a cylinder
Em =
n
Xt ∈RR
[0,∞) : (Xt1, . . . , Xtm) ∈Km
o
,
and is such that Em ⊆Dm, QC(Dm ∖Em) = Ptm (Am ∖Km) < ε/2m. Set eEm =
mT
i=1 Ei
since the {Ek}+∞
k=1 may be nonincreasing and note that eEm ⊆Em ⊆Dm with
eEm =
n
Xt ∈RR
[0,∞) : (Xt1, . . . , Xtm) ∈eKm
o
,
where
eKm =

K1 × Rm−1
∩

K2 × Rm−2
∩· · · ∩(Km−1 × R) ∩Km,
which is a compact set. As a result, we can bound Ptm
eKm

away from zero since
Ptm
eKm

=
QC(eEm) = QC(Dm) −QC(Dm ∖eEm) = QC(Dm) −QC
 mS
i=1
(Dm ∖Ei)
!
≥
QC(Dm) −QC
 mS
i=1 (Di ∖Ei)
!
≥ε −
mP
i=1
ε
2i > 0,
because ε is assumed to be positive and by assumption QC(Dm) ≥ε. Therefore, eKm
is nonempty for all m = 1, 2, . . . and we can choose some point (x(m)
1 , . . . , x(m)
m ) ∈
eKm. Since eKm ⊆eK1 with eK1 compact, the sequence of points {x(m)
1 }+∞
m=1 must have
a convergent subsequence {x(mk)
1
}+∞
k=1 with limit x1. But {x(mk)
1
, x(mk)
2
}+∞
k=2 is contained
in eK2 and therefore it has a convergent subsequence with limit (x1, x2). Continue in
this way in order to construct a (x1, x2, . . . ) ∈R∞such that (x1, x2, . . . , xm) ∈eKm for
all m = 1, 2, . . . and as a result we have
n
Xt ∈RR
[0,∞) : Xt1 = xi, i = 1, 2, . . .
o
⊆eEm ⊆Dm,
for each m. This contradicts the fact that
+∞
T
m=1Cm =
+∞
T
m=1 Dm = ∅so that (7.8) holds.
Now we state an equivalent version of the latter theorem (for more details see
Billingsley, 2012, p. 517).
Theorem 7.2 (Kolmogorov existence: of a probability space) Let {Pt} be a
consistent family of ﬁnite-dimensional distributions. Then there exists on some
probability space (Ω, A, P) a stochastic process X = {Xt : t ∈T} having {Pt} as
its ﬁnite-dimensional distributions.
Unfortunately, this construction will not work for two very important processes,
the Poisson process and Brownian motion (Wiener process). Next we discuss why
the Kolmogorov existence theorem fails in those cases.
Remark 7.6 (Kolmogorov existence theorem shortcomings) Recall that the Pois-
son process has sample paths that are step functions (see deﬁnition 7.3), however,
(7.3) deﬁnes a stochastic process that has the same ﬁnite dimensional distributions
as a Poisson but it has sample paths that are not step functions. Therefore, we can-
not use the Kolmogorov existence theorem to deﬁne the Poisson process.

MARKOV PROCESSES
289
Moreover, using Kolmogorov’s existence theorem we can build a stochastic
process on the sample space RR
[0,∞), but there is no guarantee that the process is
continuous, which is required for Brownian motion. This problem arises since the
“event” CR
[0,+∞) is not even in the Borel σ-ﬁeld B(RR
[0,∞)), so that P(Xt ∈CR
[0,+∞))
is not even deﬁned, when it should be equal to 1 (see for example exercise 2.7,
Karatzas and Shreve, 1991, p. 53). Therefore, we need to construct the Wiener
measure adhering to the continuity property for the paths of the process, namely,
CR
[0,+∞)-valued random variables (functions) and not general R-valued random vari-
ables.
In order to build Brownian motion via Kolmogorov’s theorem, a modiﬁcation
is required, known as the Kolmorogov-ˇCentsov theorem. See Karatzas and Shreve
(1991, Section 2.2.B) for this construction and for additional discussion on the
shortcomings of RR
[0,∞) in deﬁning Brownian motion. See appendix remark A.5.6
for the general deﬁnition of a locally H¨older-continuous function.
Theorem 7.3 (Kolmorogov- ˇCentsov) Let X = {Xt : 0 ≤t ≤T} be a stochastic
process on a probability space (Ω, A, P), such that
E |Xt −Xs|a ≤c|t −s|1+b, 0 ≤s, t ≤T,
where a, b, c > 0, are some constants. Then there exists a continuous modi-
ﬁcation eX = {eXt : 0 ≤t ≤T} of X, which is locally H¨older-continuous of
order γ ∈(0, b
a), i.e., for every ε > 0 and T < ∞, there exists a ﬁnite constant
δ = δ(ε, T, a, b, c, γ) > 0, such that
P
n
ω :
eXt −eXs

a ≤δ|t −s|γ, 0 ≤s, t ≤T
o
≥1 −ε.
For a proof see Klenke (2014, p. 460). In view of remark 7.6, c`adl`ag spaces
discussed next are a natural starting point that allow us to build stochastic processes
such as the Poisson and Brownian motion in a uniﬁed framework.
7.4
Markov Processes
For what follows we concentrate on general stochastic processes that satisfy
the Markov property. The latter is not the only approach in studying stochastic
processes, and some texts may assume other general properties. For example, an
excellent treatment of the theory and applications of stochastic processes under the
assumption of stationarity can be found in Lindgren (2013).
7.4.1
C`adl`ag Space
Throughout this subsection assume that (Ψ, ρ) is a Polish space. The exotic
wording c`adl`ag is an acronym for “continue `a droite, limites `a gauche” which is
French for right continuous with left limits (RCLL).

290
STOCHASTIC PROCESSES
Deﬁnition 7.4 C`adl`ag space
A function φ : [0, +∞) →Ψ is called c`adl`ag if it is right continuous and the
limit lim
s↑t φ(s) exists, ∀t ≥0. The space of c`adl`ag functions is called the c`adl`ag space
and is denoted by DΨ
[0,+∞).
We collect some facts about c`adl`ag functions and spaces next.
Remark 7.7 (C`adl`ag spaces) To simplify the notation we write φt for φ(t), ∀t ≥0.
Note that if the sample paths of a stochastic process X are RCLL then at time t the
process is at state Xt(ω) which for ﬁxed ω ∈Ωis a c`adl`ag function, i.e., for ﬁxed
ω, Xt(ω) ∈DΨ
[0,+∞). Moreover, if lim
s↑t φ(s) = φ(t), then φ is continuous, and therefore
the c`adl`ag space contains the continuous functions CΨ
[0,+∞) from [0, +∞) to Ψ, that
is, CΨ
[0,+∞) ⊂DΨ
[0,+∞).
1. We turn the c`adl`ag space DΨ
[0,+∞) into a measurable space by introducing the
σ-ﬁeld generated by sets of the form {φ : φt ∈B}, for B ∈B(Ψ) and t ≥0, so that
the c`adl`ag measurable space is given by (DΨ
[0,+∞), DΨ
[0,+∞)) where
DΨ
[0,+∞) = σ({φ : φ(t) ∈B} : t ≥0, B ∈B(Ψ)).
2. For all φ1, φ2 ∈DR
[0,+∞) deﬁne
ρC(φ1, φ2) =
+∞
P
n=1 2−nmax
0≤t≤n{min(1, |φ1(t) −φ2(t)|)}.
(7.9)
It can be shown that ρC is a metric such that (DR
[0,+∞), ρC) is a Polish space and
DR
[0,+∞) is the σ-ﬁeld of Borel sets in DR
[0,+∞). The latter allows us to study conver-
gence of sequences of distributions on DR
[0,+∞) which is important when construct-
ing complicated stochastic processes by taking the limit of a sequence of simpler
stochastic processes. This result still holds if we replace R with a general Ψ and
appropriate metric ρC.
In order to study Markov processes we require a deﬁnition similar to the transi-
tion operator for Markov chains.
Deﬁnition 7.5 Transition semigroup
A transition semigroup for Ψ is a collection of transition operators TSG = {Tt : t ≥
0} for Ψ satisfying the following properties:
(i) T0 f = f, for all bounded measurable R-valued functions deﬁned on Ψ.
(ii) TsTt = Ts+t, ∀t, s ∈R+
0.
(iii) lim
t↓0 Tt f (x) = f (x), for all bounded, continuous, measurable R-valued functions
deﬁned on Ψ and x ∈Ψ.
We connect a transition semigroup with its corresponding transition distribu-
tions below.

MARKOV PROCESSES
291
Remark 7.8 (Transition semigroup and distributions) A transition semigroup
has associated transition distributions {µx,t : x ∈Ψ, t ≥0} on Ψ with
Tt f (x) =
R
Ψ
f (y)µx,t(dy),
(7.10)
∀x ∈Ψ and t ≥0. In particular, the transition distributions for any x, y ∈Ψ are
given by
µx,t(B) = P(Xt+s ∈B|Xs = x),
(7.11)
∀B ∈B(Ψ). In view of deﬁnition (7.5) and conditions (i)-(iii), the transition distri-
butions must satisfy the following conditions:
(a) for all x ∈Ψ, µx,t →µx,0 = δx, as t ↓0,
(b) for all B ∈B(Ψ) and t ≥0, the function x 7→µx,t(B) is measurable, and
(c) for all B ∈B(Ψ), s, t ≥0 and x ∈Ψ, µx,s+t(B) =
R
Ψ
µy,s(B)µx,t(dy).
We collect the formal deﬁnition of a time-homogeneous Markov process next.
Deﬁnition 7.6 Time-homogeneous Markov process
Let TSG = {Tt : t ≥0} be a transition semi-group for Ψ with corresponding
transition distributions {µx,t : x ∈Ψ, t ≥0}. A DΨ
[0,+∞)-valued random variable X
adapted to a ﬁltration F = {Ft : t ≥0} is called a time-homogeneous Markov
process with respect to F having state space Ψ and transition semi-group TSG if
∀t, s ∈R+
0 the conditional distribution of Xs+t given Fs is µXs,t, that is, P(Xs+t ∈
B|Fs) = µXs,t(B), ∀B ∈B(Ψ). The distribution of such a process is called a (time-
homogeneous) Markov distribution with transition semi-group TSG.
We will only consider time-homogeneous Markov processes in what follows
and therefore we simply refer to these processes as Markov processes.
Example 7.5 (Construction of the Poisson process) Assume that T1, T2, . . . , is
a sequence of independent, exponentially distributed random variables with λ > 0,
that is, fTn(t) = λe−λt, t ≥0. Deﬁne a random walk S = (S 0, S 1, . . . ) in R+
0 with
S 0 = 0 and S n =
nP
i=1 Ti, n ≥1. We can think of S n as the time of arrival of the
nth customer in some queue and of the random variables Ti, i = 1, 2, . . ., as the
interarrival times. Now deﬁne a continuous time parameter, integer-valued RCLL
process N by
Nt = max{n ≥0 : S n ≤t},
for t ≥0, which is adapted with respect to the minimal ﬁltration F N. We can think
of Nt as the number of customers that arrived in the queue up to time t. Clearly, N0 =
0 a.s. and it can be shown that for all 0 ≤s < t we have P(S Ns+1 > t|F N
s ) = e−λ(t−s),
a.s. [P] and Nt −Ns is a Poisson random variable with rate λ(t −s) independent of
F N
s . Therefore, Nt as deﬁned is a Poisson process (recall deﬁnition 7.3).

292
STOCHASTIC PROCESSES
From conditions (P2) and (P3) of deﬁnition 7.3 we can write
P(Nt+s = y|Nu, u ≤s) = P(Nt+s = y|Ns),
so that N is a Markov process and in addition
P(Nt+s = y|Ns = x) =

0
if y < x,
e−λt(λt)y−x
(y−x)!
if y ≥x ,
(7.12)
which is free of s ≥0. As a result, N is a time-homogeneous Markov process with
transition measures given by
µx,t({y}) = P(Nt = y|N0 = x) = e−λt(λt)y−x
(y −x)! I(y ≥x) = gt(y −x)I(y ≥x),
where gt is the pmf of a Poisson(λt) random variable. Now based on (7.10), the
transition semigroup for the Poisson process can be written as
Tt f (x) = P
y∈Ψ f (y)gt(y −x)I(y ≥x) = (f ∗gt)(x).
(7.13)
Example 7.6 (Queueing systems)
Consider the queue-size process X = {Xt :
t ≥0}, where Xt denotes the number of customers in the system (waiting or being
served) at time t. The future state of the process Xt+s at time t+ s is equal to the sum
of the current number of customers in the queue Xt and number of arrivals A(t,t+s]
during the time interval (t, t + s] minus the number of services S (t,t+s] completed
during (t, t + s], that is, Xt+s = Xt + A(t,t+s] −S (t,t+s]. If the numbers A(t,t+s] and S (t,t+s]
do not depend on any component of the queueing system before time t then the
process X is Markov with discrete state space Ψ = {0, 1, 2, . . .}.
For example, assume that the arrivals of customers follow a Poisson process
and there is one server with an exponential service time (i.e., M/M/1 queue). Then
the number of arrivals in (t, t+ s] is independent of everything else that happened in
the queue before time t. Now since the exponential distribution has the memoryless
property, the remaining service time of the customer being served at time t (if any)
is independent of the past before time t. Therefore, the number S (t,t+s] of services
completed during (t, t + s] depends only on Xt and the number of arrivals A(t,t+s]
during the time interval (t, t + s] and nothing before time t.
Deﬁnition 7.7 Markov family
A Markov family of processes is a collection of Markov processes {Xx : x ∈
Ψ} with common transition semi-group TSG such that Xx
0 = x a.s., ∀x ∈Ψ. The
corresponding collection of Markov distributions {Qx : x ∈Ψ} is called a Markov
family of distributions with transition semi-group TSG.
Typically, we build a collection of distributions {Qx : x ∈Ψ} on DΨ
[0,+∞) and treat
it as a candidate for being a Markov family and this is often done without specifying
the transition semi-group. The following result tells us when there exists a transition
semi-group that turns a collection of probability measures into a Markov family.

MARKOV PROCESSES
293
Criterion 7.1 (Markov family) For any t ≥0, let
Ct = σ({φs : s ∈[0, t], φ ∈DΨ
[0,+∞)}),
and deﬁne C = σ ({Ct : t ≥0}) . Then the family of distributions {Qx : x ∈Ψ} on
(DΨ
[0,+∞), C) is a Markov family if and only if the following conditions are satisﬁed:
(i) ∀x ∈Ψ, Qx({φ : φ0 = x}) = 1.
(ii) ∀t ≥0, the function x 7→Qx({φ : φt ∈·}) is a measurable function from Ψ to
the measurable space of probability measures on Ψ.
(iii) ∀x ∈Ψ, s, t ≥0,
Qx(φs+t ∈·|Ct)(w) = Qws(φ ∈·),
for almost all w ∈DΨ
[0,+∞) with respect to Qx.
Under these conditions the transition semi-group of the Markov family is
deﬁned by
Tt f (x) =
R
DΨ
[0,+∞)
f (φt)Qx(dφ),
for x ∈Ψ, t ≥0 and bounded measurable R-valued function f deﬁned on Ψ.
The next remark summarizes some important results on Markov processes.
Remark 7.9 (Markov process results) We note the following.
1. The initial distribution Q0 of a Markov process X is the distribution of X0. If
Q0 = δx, for some x ∈Ψ then we call x the initial state of X. The Markov process
X is uniquely determined by its initial distribution and transition semi-group.
2. Let {Qx : x ∈Ψ} be a Markov family with transition semi-group TSG and Q0 a
probability distribution on Ψ. Then
C 7→
R
Ψ
Qx(C)Q0(dx),
(7.14)
for C ∈DΨ
[0,+∞) is the Markov distribution having initial distribution Q0 and transi-
tion semi-group TSG. Now the function φ 7→φt (as a function of φ) from DΨ
[0,+∞) to
Ψ is measurable. This function and the Markov distribution of (7.14) induce a dis-
tribution on Ψ, namely, the distribution of the state of the Markov process at time t
when the initial distribution is Q0. If this induced distribution is also Q0 for all t ≥0
then we say that Q0 is an equilibrium for the Markov family and the corresponding
transition semi-group.
3. Markov process criterion Let TSG be a transition semi-group for a Polish
space Ψ, X a DΨ
[0,+∞)-valued random variable and F a ﬁltration to which X is
adapted. The following are equivalent:
(a) X is Markov with respect to F .
(b) For all s, t ≥0 and bounded measurable f : Ψ →R we have
E(f (Xs+t)|Fs) = Tt f (Xs).
(7.15)
(c) For all t > 0 the random sequence (X0, Xt, X2t, . . . ) is Markov with respect to the

294
STOCHASTIC PROCESSES
ﬁltration {Fnt : n = 0, 1, . . .} with transition operator Tt.
(d) Equation (7.15) holds for all s, t ≥0 and continuous functions f : Ψ →R that
vanish at ∞(i.e., ∀ε > 0, ∃a compact set C ⊂Ψ such that | f (x)| < ε, for x ∈Ψ ∖C,
such functions are necessarily bounded and uniformly continuous).
4. Chapman-Kolmogorov equation Let X be a Markov process with state space
Ψ, initial distribution Q0 and transition distributions µx,t. Then for all n > 0,
bounded measurable functions f : Ψn →R and times t1, . . . , tn we have
E(f (Xt1, Xt1+t2, . . . , Xt1+t2+···+tn))
=
R
Ψ
R
Ψ
. . .
R
Ψ
f (x1, . . . , xn)µxn−1,tn(dxn) . . .
µx0,t1(dx1)Q0(dx0).
For a discrete state space Ψ let pt(x, y) = µx,t({y}) denote the transition function.
Then the Chapman-Kolmogorov equation can be written as
P
z∈Ψ pt(x, z)ps(z, y) = pt+s(x, y).
(7.16)
In the discrete time parameter case, the Markov and Strong Markov properties
are equivalent. This is not the case in the continuous time parameter case, that is,
there are stochastic processes that are Markov but not strong Markov (see exercise
7.19).
Deﬁnition 7.8 Strong Markov property
Let X be a Markov process with respect to a ﬁltration F with transition distribu-
tions {µx,t : x ∈Ψ, t ≥0}. Then X is strong Markov with respect to F if for each
a.s. ﬁnite stopping time τ with respect to the ﬁltration F and for all t ≥0, the con-
ditional distribution of Xτ+t given Fτ is µXτ,t. The distribution of a strong Markov
process is called a strong Markov distribution. Markov families whose members
are all strong Markov are called strong Markov families.
7.4.2
Inﬁnitesimal Generators
We can describe the behavior of a Markov process locally via inﬁnitesimal
generators and then develop methods that describe the global characteristics of a
Markov process.
Deﬁnition 7.9 Inﬁnitesimal generator
Let TSG = {Tt : t ≥0} be a transition semi-group for a Polish space Ψ. Then an
operator G deﬁned by
G f (x) = lim
t↓0 (Tt f (x) −f (x)) /t = lim
t↓0 (Tt −I)f (x)/t,
(7.17)
for all bounded measurable functions f for which the limit above exists with
|(Tt f (x) −f (x)) /t| < c < ∞, for some constant c > 0 and for all t ≥0, is called the

MARKOV PROCESSES
295
inﬁnitesimal generator of the transition semi-group TSG and of the corresponding
Markov processes and Markov families based on TSG.
Let us discuss the relationship between the transition function and the inﬁnites-
imal generator for a Markov process.
Remark 7.10 (Transition function and inﬁnitesimal generator) Based on (7.11)
the transition probabilities for a Markov process are given by the transition func-
tions
pt(x, y) = P(Xt = y|X0 = x) = µx,t({y}),
(7.18)
∀x, y ∈Ψ and t ≥0. Now for a given y ∈Ψ, let f (x) = δy(x) = I(x = y) in (7.17)
and use (7.10) and (7.11) to obtain
Gδy(x)
=
lim
t↓0

Ttδy(x) −δy(x)

/t = lim
t↓0
1
t

R
Ψ
δy(t)µx,t(dt) −δy(x)

=
lim
t↓0
1
t

µx,t({y}) −δy(x)

= lim
t↓0
1
t (pt(x, y) −I(x = y)) .
Now if y , x then
Gδy(x) = lim
t↓0
1
t pt(x, y),
and for y = x
Gδx(x) = lim
t↓0
1
t (pt(x, x) −1),
so that
Gδy(x) = d
dt pt(x, y)|t=0,
provided that
p0(x, y) = lim
t↓0 pt(x, y) = δx(y).
(7.19)
The latter continuity condition ensures that the Markov process is “well behaved”
at 0. A transition function with this property is known as a standard transition func-
tion. Moreover, for stochastic processes with RCLL we have lim
ε↓0 Xt+ε(ω) = Xt(ω),
for all ω ∈Ω(right continuity) so that using the continuity of probability measure
we can write
lim
ε↓0 pt+ε(x, y)
=
lim
ε↓0 P(Xt+ε = y|X0 = x) = P(lim
ε↓0 Xt+ε = y|X0 = x)
=
P(Xt = y|X0 = x) = pt(x, y),
and therefore the transition function is right continuous and (7.19) holds.
The next theorem can be used to solve for a transition semi-group TSG given an
inﬁnitesimal generator G.
Theorem 7.4 (Inﬁnitesimal generator) Let TSG = {Tt : t ≥0} be a transition
semi-group for a Polish space Ψ and let G be the corresponding generator. Then

296
STOCHASTIC PROCESSES
for all f in the domain of G we have
TtG f (x) = lim
h→0 (Tt+h f (x) −Tt f (x)) /h = GTt f (x),
provided that |(Tt+h f (x) −Tt f (x)) /h| < c < ∞, for some constant c > 0 and for
all t, h ≥0.
7.4.3
The Martingale Problem
We connect MGs and Markov processes next.
Deﬁnition 7.10 The martingale problem
Let Ψ be Polish, BCR
Ψ denote the collection of all bounded continuous functions
f : Ψ →R and G be a functional from BCR
Ψ to the space of bounded measur-
able functions on Ψ. A DΨ
[0,+∞)-valued random variable X deﬁned on (Ω, A, P) is a
solution to the MG problem for (G, BCR
Ψ) if the function of t ≥0 deﬁned by
t 7→f (Xt) −
tR
0
G f (Xu)du,
(7.20)
is a continuous time MG with respect to the minimal ﬁltration of X for all f ∈BCR
Ψ.
The usefulness of the MG problem in identifying Markov processes is illus-
trated below.
Remark 7.11 (Markov process and the martingale problem) The random vari-
able X in (7.20) is DΨ
[0,+∞)-valued since f is assumed to be continuous. Letting F
denote the minimal ﬁltration of X and noting that
tR
0
G f (Xu)du is measurable with
respect to Ft we can rewrite equation (7.20) along with the assumption that X is a
MG equivalently as
E (f (Xt+s)|Fs) −E

t+sR
0
G f (Xu)du|Fs
= f (Xs),
for all s, t ≥0. We note the following.
1. Let G be the generator of a Markov process X with state space Ψ and let CG
denote the subset of the domain of G that contains only continuous functions. Then
X is a solution to the MG problem for (G, CG).
2. Under the setup of part 1, assume that for each x ∈Ψ there is a solution to
the MG problem for (G, CG) with initial state x. Then the collection of solutions
obtained by varying x over Ψ is a strong Markov family with a generator that agrees
with G on CG.

MARKOV PROCESSES
297
7.4.4
Construction via Subordinated Markov Chains
This construction works for stochastic processes that have RCLL sample paths.
In view of remark 7.4.9, we envision the construction of a stochastic process X with
RCLL sample paths and a countable number of jumps where time is measured in
an appropriate way, e.g., the stopping times {τn}+∞
n=1 of equation (7.1).
Remark 7.12 (Construction of a Markov process via a Markov chain) Assume
that Td is a transition operator for a discrete state space Ψ with discrete generator
Gd = Td −I, let x0 ∈Ψ and λ > 0 be some constant. We construct a Markov
process X with inﬁnitesimal generator G = λGd and initial state x0. Consider a
Markov chain Y with initial state x0 and transition operator Td and a random walk
S = (S 0, S 1, . . . ), S 0 = 0, having iid exponential distributed steps with parameter
λ > 0, i.e., E(S 1) = 1/λ. Further assume that Y and S are independent. For t ≥0,
deﬁne Mt = max{n ≥0 : S n ≤t}, a Poisson process with intensity λ (recall example
7.5), independent of Y. Finally, we deﬁne a stochastic process for any t ≥0 by
Xt = YMt,
so that the discrete time-parameter Markov chain Y was turned into a continuous
time-parameter stochastic process by viewing time through a Poisson process. Note
that both X and Y take values on the same discrete state space Ψ and X is a DΨ
[0,+∞)-
valued random variable.
1. Transition function The transition function of the stochastic process X is writ-
ten with respect to the transition operator Td of the discrete Markov chain Y as
follows:
pt(x, y)
=
P(Xt = y|X0 = x) = P(YMt = y|Y0 = x)
=
+∞
P
n=0 P(Yn = y|Y0 = x)P(Mt = n|Y0 = x) =
+∞
P
n=0 T n
d(x, y)(λt)ne−λt
n!
,
and therefore
pt(x, y) = e−λt +∞
P
n=0
λntn
n! T n
d(x, y),
(7.21)
where T n
d(x, y) is the n-step transition probability from x to y by the Markov chain
Y.
2. Matrix exponential function For a matrix T = [(T(x, y))] we deﬁne
etT =
+∞
P
n=0
tn
n!T n,
the matrix exponential function. As a result, we can write (7.21) in matrix form as
e−λt +∞
P
n=0
λntn
n! T n
d = e−λt +∞
P
n=0
(λt)n
n! T n
d = e−λtetλTd = etλ(Td−I) = etλGd = etG,
and letting Tt = [(pt(x, y))] we have the matrix representation of the transition
matrix of X based on the generator G as Tt = etG. We collect this result in the
theorem below.
3. State classiﬁcation Many of the properties and results we have seen based on

298
STOCHASTIC PROCESSES
the Markov chain Y carry over to the Markov process X, with slight modiﬁcations
since time t is continuous. In particular, classiﬁcation of states for the Markov pro-
cess X when Ψ is discrete can be accomplished by classifying the states of the
Markov chain Y. See Section 7.4.5.2 for more details.
Theorem 7.5 (Transition operator and discrete generator) Let {Xx : x ∈Ψ}
be the family of stochastic processes as deﬁned in remark 7.12 in terms of a
transition operator Td and discrete generator Gd. Then for each x ∈Ψ, Xx is
strong Markov. The corresponding Markov family of distributions has generator
G = λGd with transition semi-group TSG = {Tt : t ≥0} related to G as follows
Tt = etG = eλtGd,
(7.22)
for all t ≥0.
The Markov process X thus created above is known as a pure-jump process with
bounded rates. We collect the following regarding pure-jump processes.
Remark 7.13 (Pure-jump process with bounded rates) To justify the name, ﬁrst
note that the process has a countable number of jumps at the times of arrival of a
Poisson process. Furthermore, letting µx denote the transition distributions of the
Markov chain Y we write
G f (x) = λGd f (x) = λ(Td −I)f (x) = λ
R
Ψ
(f (y) −f (x))µx(dy),
∀x ∈Ψ. Deﬁne
r(x) = λµx(Ψ ∖{x}),
(7.23)
and build a probability measure on Ψ by taking
Px(B) = µx(B ∖{x})/r(x),
if r(x) > 0 and let Px = δx, if r(x) = 0, ∀B ∈B(Ψ). Then we can rewrite (7.23) as
G f (x) = r(x)
R
Ψ
(f (y) −f (x))Px(dy),
∀x ∈Ψ and bounded functions f : Ψ →R. The function r(x) is known as the
jump rate function and Px as the jump distribution. By construction, the jump rate
function is bounded above by λ, which rightly earns these processes the name pure-
jump process with bounded rates. There are ways of constructing such processes
with unbounded rates. For more details see Fristedt and Gray (1997, p. 636).
7.4.5
Discrete State Space
We now connect properties of RCLL paths of a stochastic process with sojourn
times, thus expanding on the development of the last two sections. In addition,
we will present an alternative subordinated Markov chain construction and per-
form state classiﬁcation. For what follows assume that X = {Xt, t ≥0} is a time-
homogeneous Markov process deﬁned on a discrete (ﬁnite or countable) state space

MARKOV PROCESSES
299
Ψ via the transition function
pt(x, y) = P(Xt = y|X0 = x) ≥0,
with P
y∈Ψ pt(x, y) = 1, ∀x ∈Ψ and any t ≥0. We write Pt = [(pt(x, y))] for the
transition matrix.
For the exposition of this section we only need to assume that pt(x, y) is a stan-
dard transition function (i.e., continuous at 0, recall remark 7.10). In fact, based
only on this condition, it can be shown that the transition function t 7→pt(x, y), for
ﬁxed x, y ∈Ψ, is continuous everywhere.
Example 7.7 (Poisson process) Let N = {Nt : t ≥0} be a Poisson process with
Ψ = {0, 1, 2, . . .} and transition function pt(x, y) given by (7.12). Then the transition
matrix is given by
Pt =

pt(0)
pt(1)
pt(2)
. . .
pt(0)
pt(1)
. . .
pt(0)
. . .
0
...

with
pt(k) = e−λt(λt)k
k!
,
k = 0, 1, 2, . . ., the Poisson distribution with parameter λt. From (7.13) we have
Tt f (x) =
+∞
P
y=0 f (y)pt(y −x)I(y ≥x) =
+∞
P
y=x f (y)pt(y −x),
and letting z = y −x ⇒y = z + x, we have
Tt f (x) =
+∞
P
z=0 f (z + x)pt(z).
Using the fact that
+∞
P
z=0 pt(z) = 1, the inﬁnitesimal generator of the Poisson process
is given by
G f (x)
=
lim
t↓0 (Tt f (x) −f (x)) /t = lim
t↓0
1
t
"+∞
P
z=0 f (z + x)pt(z) −f (x)
#
=
lim
t↓0
1
t
"+∞
P
z=0 (f (z + x) −f (x)) pt(z)
#
=
+∞
P
z=1
f (z + x) −f (x) lim
t↓0
1
t pt(z),
where the term for z = 0 in the sum vanishes and we use the DCT to sum to the
limit. Now for z > 1, the limits
lim
t↓0
1
t pt(z) = lim
t↓0
1
t
e−λt(λt)z
z!
= λz
z! lim
t↓0 e−λttz−1 = 0,
vanish, whereas, for z = 1 we have
lim
t↓0
1
t pt(1) = λ.

300
STOCHASTIC PROCESSES
Consequently, the inﬁnitesimal generator for the Poisson Markov process is given
by
G f (x) = λ f (x + 1) −f (x) .
Now note that
pt(x, y) = e−λt(λt)y−x
(y −x)! I(y ≥x),
(7.24)
so that p0(x, y) = I(y = x) and
d
dt pt(x, y) = −λpt(x, y) + λpt(x + 1, y).
Letting f (x) = δy(x), for some state y ∈Ψ, we have
Gδy(x) = λ
h
δy(x + 1) −δy(x)
i
= λ I(y = x + 1) −I(y = x) ,
and on the other hand
d
dt pt(x, y)|t=0 = −λp0(x, y) + λp0(x + 1, y) = −λI(y = x) + λI(y = x + 1),
(7.25)
so that Gδy(x) =
d
dt pt(x, y)|t=0, as expected (recall remark 7.10). The form of the
derivative at 0 suggests that given the transition function we can obtain the in-
ﬁnitesimal generator, but more importantly, the converse is also feasible; given an
inﬁnitesimal generator there may be a way to solve for the transition function of
the Markov process. We formalize the usefulness of such derivatives in the next
deﬁnition.
Deﬁnition 7.11 Jump rates and inﬁnitesimal generator
Let X be a Markov process with discrete state space Ψ and standard transition
function pt(x, y), x, y ∈Ψ. Deﬁne
A(x, y) = d
dt pt(x, y)|t=0 = lim
t↓0
1
t pt(x, y),
(7.26)
the derivative of the transition function at 0, ∀x, y ∈Ψ and assume that it exists. We
say that X jumps with rate A(x, y) from x to y and that the matrix A = [(A(x, y))] is
the inﬁnitesimal generator matrix of X.
Example 7.8 (Poisson process)
For a Poisson process N with intensity λ the
probability of at least two jumps by time t is given by
P(Nt
≥
2) = 1 −P(Nt = 0) −P(Nt = 1) = 1 −e−λt −λte−λt
=
1 −(1 + λt)
 
1 −λt + (λt)2
2!
+ . . .
!
=
(λt)2
2!
+ · · · = o(t2),
so that
lim
t↓0
1
t P(Nt ≥2) = 0,
and therefore the probability of two or more jumps in an inﬁnitesimal interval by

MARKOV PROCESSES
301
the Poisson process is zero. Using the transition function (7.24) and its derivative
in equation (7.25) we have that the jump rate from state x to y is given by
A(x, y) = d
dt pt(x, y)|t=0 = −λI(y = x) + λI(y = x + 1),
so that we could jump from x to x + 1 with rate λ, ∀x ∈Ψ.
In what follows, we will use inﬁnitesimal generators in order to construct
Markov processes via subordinated Markov chains. First we collect important def-
initions and characteristics of the sample paths of the process that can be used to
classify the states of the discrete Ψ.
7.4.5.1
Sample Paths and State Classiﬁcation
Now consider the length of time S t that the process X remains in the state
occupied at the instant t, i.e., for any ω ∈Ωand t ≥0, deﬁne
S t(ω) = inf {s > 0 : Xt+s(ω) , Xt(ω)} ,
(7.27)
the sojourn of X at the state Xt(ω) from time t and on. Based on the sojourn random
variables we can perform state classiﬁcation.
Deﬁnition 7.12 State classiﬁcation
For any given state x ∈Ψ and t ≥0, deﬁne the probability that the sojourn exceeds
some time s ≥0 given the current state of the process by
Wx,t(s) = P(S t > s|Xt = x).
The state x is called
(i) absorbing if Wx,t(s) = 1, i.e., S t = +∞a.s., Xt+s = x, ∀s ≥0,
(ii) instantaneous if Wx,t(s) = 0, i.e., S t = 0 a.s. and the process jumps out of the
state x the instant it enters it, or
(iii) stable if 0 < Wx,t(s) < 1, i.e., 0 < S t < +∞a.s. and the process stays at x for
an additional amount of time that is positive but ﬁnite.
The following remark summarizes properties of the sample paths t 7→Xt(ω), for
any ω ∈Ω. For more details on these results see C¸ inlar (1975) and Durrett (2004).
Remark 7.14 (Sample path behavior) Assume that Pt is a standard transition
function.
1. Stochastic continuity Then the stochastic process X is stochastically continu-
ous, that is,
lim
ε↓0 P(Xt−ε , Xt) = lim
ε↓0 P(Xt , Xt+ε) = 0,
(7.28)
or equivalently lim
t→s P(Xt = Xs) = 1, for any ﬁxed s ≥0. This property guaran-
tees well-behaved sample paths in the sense that there are no instantaneous states.
In particular, stochastic continuity does not imply continuity of the sample paths,
however, it ensures that X does not visit a state in Ψ only to jump out of it instanta-
neously.

302
STOCHASTIC PROCESSES
2. Modiﬁcation for smoothness For a discrete state space it is possible that
lim
t→∞Xt(ω) = X∞(ω) < Ψ, e.g., the limit escapes to +∞and Ψ does not contain
the state +∞(which is usually the case). We can take care of this situation by
considering a modiﬁcation eX of the original process X that leaves the probability
law the same, with eX deﬁned on the augmented state space Ψ = Ψ ∪{∆}, where
X∞(ω) = ∆. In particular, the modiﬁcation can have the following properties: for
any ω ∈Ωand t ≥0, either
(i) eXt(ω) = ∆and eXtn(ω) →∆for any sequence of times tn ↓t, or
(ii) eXt(ω) = x for some x ∈Ψ, there are sequences tn ↓t such that eXtn(ω) →x and
for any sequence tn ↓t such that eXtn(ω) has a limit in Ψ then we have lim
tn↓t
eXtn(ω) = x.
3. The state ∆Since eX is a modiﬁcation of X we have
P(eXt = ∆|eX0 = x) = 1 −P(Xt = ∆|X0 = x) = 1 −P
y∈Ψ pt(x, y) = 0,
so that the addition of the artiﬁcial point ∆does not alter the probability distribu-
tion of the original stochastic process. From now on we let X denote the modiﬁed
version eX.
4. RCLL Assume that there are no instantaneous states. For any ω ∈Ωand t ≥0,
if tn ↓t such that lim
tn↓t Xtn(ω) = x ∈Ψ then Xt(ω) = x, that is, X has right-continuous
paths. Moreover, X has left-hand limits everywhere, i.e., X has RCLL paths.
5. Let Y be a modiﬁcation of a stochastic process X and assume that both pro-
cesses have RCLL paths. Then it can be shown that X and Y are indistinguishable.
6. For any ω ∈Ωand t ≥0, one of the following holds:
(i) Xt(ω) = ∆, in which case lim
tn↓t Xtn(ω) = ∆for any sequence of times tn ↓t,
(ii) Xt(ω) = x, where x ∈Ψ is either absorbing or stable. Then lim
tn↓t Xtn(ω) = x for
any sequence of times tn ↓t, or
(iii) Xt(ω) = x, where x ∈Ψ is instantaneous. Then lim
tn↓t Xtn(ω) is either x or ∆or
does not exist, for diﬀerent sequences tn ↓t.
Now we take a closer look at the sojourn times and their use in classiﬁcation of
states.
Remark 7.15 (Sojourn times and classiﬁcation) Based on the deﬁnition of Wx,t(s)
and the sample path behavior of X we have the following.
1. Sojourn distribution For any x ∈Ψ and t ≥0 we have
Wx,t(s) = P(S t > s|Xt = x) = e−λ(x)s,
(7.29)
for all s ≥0, where λ(x) is some constant in [0, +∞].
2. Clearly, λ(x) denotes the sojourn rate, i.e., the expected length of time X spends
at state x is 1/λ(x). We can classify the states in Ψ based on the corresponding λ(x)
values. In particular, if λ(x) = 0 then Wx,t(s) = 1 and x is absorbing, whereas, if

MARKOV PROCESSES
303
0 < λ(x) < +∞then 0 < Wx,t(s) < 1 so that x is stable and S t|Xt = x ∼Exp(λ(x)).
Finally, if λ(x) = +∞then Wx,t(s) = 0 and x is instantaneous.
3. Note that the distribution of the sojourn times in (7.29) does not depend on t.
Furthermore, λ(x) can be interpreted as the rate at which the process X leaves state
x ∈Ψ, that is, if λ(x) = 0 we never leave (x is absorbing), whereas, if λ(x) = +∞
we leave the state immediately (x is instantaneous).
4. If Ψ is ﬁnite then no state is instantaneous.
5. The sample paths are right continuous if and only if there are no instantaneous
states.
6. Stable states For a stable state x ∈Ψ it makes sense to deﬁne and study the
total amount of time spent by the process X at x, i.e., deﬁne the random set
Wx(ω) = {t : Xt(ω) = x} .
(7.30)
In view of (7.29) the set Wx(ω) consists of a union of intervals each of which has
positive length and the lengths are iid exponential random variables Exp(λ(x)).
7.4.5.2
Construction via Jump Times
Assume for the moment that all states of a Markov process X are stable and
consider S = {S t : t ≥0}, the process of sojourn times and T = {Tn : n =
0, 1, 2, . . .}, the process of the times at which X performs a jump. We study the
behavior of X based on the sojourn process S, the jump times process T and an
underlying Markov chain Y = {Yn : n = 0, 1, 2, . . .} that keeps track of the states
that X jumps into.
More precisely, let T0 = 0 and deﬁne
Tn+1 = Tn + S Tn,
(7.31)
for all n = 1, 2, . . ., so that Tn+1 denotes the time that X performs its (n + 1)th
jump, which is expressed in terms of the sum of the time Tn of the nth jump and the
sojourn time S Tn = Tn+1 −Tn. We keep track of the states the process X jumps into
by deﬁning the Markov chain
Yn = XTn,
(7.32)
for all n = 0, 1, 2, . . . and as a result, T is the process of the instants of transitions
(jumps) for the process X and Y are the successive states visited by X.
Since all states are stable we have 0 < S Tn < +∞a.s. and every Yn takes values
in Ψ. Now for a typical ω ∈Ω, suppose that Tn(ω) and Yn(ω) are deﬁned as in
(7.31) and (7.32), respectively, for n = 0, 1, 2, . . ., m and assume that Ym(ω) = x, an
absorbing state. Then we expect that Xt(ω) = x for all t ≥Tm(ω). By the deﬁnition
of the sojourn time S t we have Tm+1(ω) = +∞, but we cannot use (7.31) to obtain
Tm+1(ω) since S ∞is not deﬁned and consequently we cannot obtain any of the
Tm+2(ω), Tm+3(ω) and so forth. In addition, (7.32) yields Ym+1(ω) = X∞(ω) = ∆. In
order to have Yn in Ψ and be able to deﬁne Tn(ω) and Yn(ω) beyond m, we alter the

304
STOCHASTIC PROCESSES
deﬁnition of the subordinated process as follows; let
T0 = 0, Y0 = X0, S ∞= +∞, Tn+1 = Tn + S Tn,
(7.33)
and
Yn+1 =

XTn+1
if Tn+1 < ∞
Yn
if Tn+1 = ∞,
(7.34)
for all n = 0, 1, 2, . . . and note that if there are no absorbing states these deﬁnitions
reduce to (7.31) and (7.32).
The following theorem shows that Y as constructed based on (7.31) and (7.32)
is a Markov chain. The proof is requested as an exercise.
Theorem 7.6 (Transitions and sojourn times) For any n = 0, 1, 2, . . . and stable
x ∈Ψ, s ≥0, assume that Yn = x. Then
P(Yn+1 = y, S Tn > s|Y0, Y1, . . . , Yn, T0, T1, . . . , Tn) = q(i, j)e−λ(x)s,
(7.35)
where Q = [(q(x, y))] is a Markov matrix, with q(x, x) = 0, ∀x ∈Ψ.
Note that setting s = 0 in the latter theorem yields that the subordinated process
Y is a Markov chain with transition matrix Q. We say that the Markov process X is
irreducible recurrent if the underlying Markov chain Y is irreducible recurrent.
Note that if X is right continuous and Tn, Yn are deﬁned based on (7.33) and
(7.34) then the latter theorem holds for absorbing states as well. In particular, if x
is absorbing for the Markov process X then it is absorbing for Markov chain Y as
well. Therefore, the transition matrix Q can be used for classiﬁcation; if q(x, x) = 1
then x is absorbing, whereas, for q(x, x) = 0 we have that x is stable.
Remark 7.16 (Construction of the Markov process) Now we turn to the con-
struction of a Markov process X = {Xt : t ≥0} based on T = {Tn : n = 0, 1, 2, . . .},
the jump times process and the underlying Markov chain Y = {Yn : n = 0, 1, 2, . . .}.
From deﬁnitions (7.33) and (7.34) we may write
Xt(ω) = Yn(ω), for t ∈[Tn(ω), Tn+1(ω))
(7.36)
for a typical ω ∈Ω. Therefore, if we ﬁnd an n such that we can bound any time
t ≥0 between two successive jump times Tn(ω) and Tn+1(ω), then Xt(ω) is well
deﬁned based on the underlying Markov chain Y. Such n exist provided that the
random variable ψ(ω) = sup
n
|Tn(ω)| = +∞a.s., since if ψ(ω) < +∞the process
makes an inﬁnite number of jumps in a ﬁnite amount of time. We avoid this case
by considering ﬁnite jump rates, or more formally, by introducing the concept of
regularity of Markov processes.
Deﬁnition 7.13 Regular Markov process
The stochastic process X is said to be regular if for almost all ω ∈Ωwe have:

MARKOV PROCESSES
305
(i) the sample paths t 7→Xt(ω) are right continuous, and
(ii) ψ(ω) = sup
n
|Tn(ω)| = +∞.
Several criteria for regularity are collected next.
Criterion 7.2 (Regularity criteria) The following criteria can used to assess reg-
ularity for a Markov process X.
1. If λ(x) ≤λ, for all x ∈Ψ for some constant λ < ∞then X is regular.
2. If Ψ is ﬁnite then X is regular.
3. If there are no instantaneous states for X and every state is recurrent in the
subordinated Markov chain Y then X is regular.
4. If for the underlying Markov chain Y the probability of staying in its transient
states forever is zero and there are no instantaneous states for X then X is regular.
5. Assume that there exists a set D of transient states with respect to the subordi-
nated Markov chain Y such that Y has positive probability of staying forever in D.
Deﬁne R =
+∞
P
n=0 Qn and assume that P
y∈D R(x, y)/λ(y) < ∞, for some x ∈D. Then X is
not regular.
7.4.5.3
Inﬁnitesimal Generator and Transition Function
For what follows assume that X is a regular Markov process with transition
function pt(x, y). In this section we connect the generator matrix A (recall deﬁnition
7.11) of the Markov process with the characteristics of the subordinated Markov
chain Y and the sojourn rates λ(x), x ∈Ψ. The following theorem connects the
transition function with Y and λ(.) recursively.
Theorem 7.7 (Recursive transition function) For any x, y ∈Ψ and t ≥0 we
have
pt(x, y) = e−λ(x)tI(x = y) +
tR
0
λ(x)e−λ(x)s P
z∈Ψ q(x, z)pt−s(z, y)ds.
We can think of this result as follows; the probability of a jump from x to y
is either the probability of remaining at state x after t time units if y = x, which
means that the sojourn has not ended and this happens with probability e−λ(x)t or if
there is a jump before time t, say at time s, the sojourn ends at s with probability
(inﬁnitesimally) λ(x)e−λ(x)s, we jump to a state z with probability q(x, z) and in the
remaining time t −s the Markov process transitions to y with probability pt−s(z, y).
This theorem allows us to connect the generator matrix with Q and λ(.).

306
STOCHASTIC PROCESSES
Theorem 7.8 (Transition function and jump rates) For any x, y ∈Ψ the transi-
tion function t →pt(x, y) as a function of t ≥0 is diﬀerentiable and the derivative
is continuous. The derivative at 0 is given by
A(x, y) = d
dt pt(x, y)|t=0 =

−λ(x)
if x = y,
λ(x)q(x, y)
if x , y.
(7.37)
Furthermore, for any t ≥0 we have
d
dt pt(x, y) = P
z∈Ψ A(x, z)pt(z, y) = P
z∈Ψ pt(x, z)A(z, y).
(7.38)
When pt(x, y) is known we can ﬁnd A and therefore solve for
λ(x) = −A(x, x),
and
q(x, y) =

A(i, j)/λ(x)
if λ(x) > 0,
0
if λ(x) = 0, x , y,
0
if λ(x) = 0, and x = y stable,
1
if λ(x) = 0, and x = y absorbing.
Now consider the converse of the problem. Given Q and λ(.), or equivalently the
generator matrix A, we wish to ﬁnd the transition functions Pt. In particular, we
have to solve the inﬁnite system of diﬀerential equations given in (7.38) or in terms
of matrices we solve
d
dtPt = APt,
(7.39)
known as Kolmogorov’s backward equations or
d
dtPt = PtA,
(7.40)
known as Kolmogorov’s forward equations. The solution to these systems of equa-
tions is similar to theorem 7.5 and we present it next for the speciﬁc construction
via the generator matrix A.
Theorem 7.9 (Transition matrix via generator matrix) For any t ≥0 we have
Pt = etA.
(7.41)
Example 7.9 (M/M/k queue) Consider an M/M/k queue where customers arrive
according to a Poisson process with intensity λ and the service time is exponential
with rate µ. Customers that arrive in the system to ﬁnd all servers busy, enter the
waiting room and have to wait for their turn to be serviced. Further assume that the
k-servers process customers independently. Increases in the queue size occur upon
an arrival and the transition has jump rate A(x, x + 1) = λ (recall example 7.8). To

MARKOV PROCESSES
307
model the departures we let
A(x, x −1) =

xµ
if x = 0, 1, . . . , k
kµ
if x > k
,
since when there are x ≤k customers in the system then they are all being served
and departures occur at rate xµ, whereas, if there are more than k customers, all k
servers are busy and departures occur independently at rate kµ.
Example 7.10 (M/M/k queue with balking)
Consider the previous example
with the twist that now customers choose to join the queue with some probabil-
ity ax that depends on the queue size at time t, i.e., Xt = x, or they choose to leave
(balk). Therefore, increases happen at rate A(x, x+1) = λax and the rest of the setup
for this queue remains the same.
7.4.5.4
Limiting Behavior
Suppose that X is a regular Markov process with transition matrix Pt and let Y
denote the underlying Markov chain with transition matrix Q. In order to assess the
limiting behavior of X we turn to established deﬁnitions and results for Y. Fix a state
y ∈Ψ and let lengths of successive sojourn times in y be denoted by S 1, S 2, . . . ,
with S i
iid∼Exp(λ(y)). Now the total time spent in state y is deﬁned in equation
(7.30) as
Wy(ω) = {t : Xt(ω) = y} =
+∞
R
0
δy(Ys(ω))ds,
for a typical ω ∈Ω. We turn to the underlying Markov chain Y and use its behav-
ior to derive results for X. The following remark summarizes the case where y is
transient.
Remark 7.17 (Transient state) Note that if y is transient then the total number of
visits Vy to y is ﬁnite w.p. 1 and starting at x ∈Ψ its distribution is given by (6.25).
Then we can write
Wy(ω) =

0
if Vy(ω) = 0
S 1(ω) + · · · + S Vy(ω)(ω)
if Vy(ω) > 0 ,
which is ﬁnite w.p. 1. We note the following.
1. Let F(x, y) denote the probability that starting from x the Markov chain Y ever
reaches y. It is straightforward to show that
P(Wy ≤t|X0 = x) = 1 −F(x, y) exp{−tλ(x)(1 −F(y, y))}.
2. For any state x ∈Ψ and y ∈Ψ transient, deﬁne R(x, y) = E(Vy|X0 = x), the
expected number of visits by the Markov chain Y to state y starting at x. Then
E(Wy|X0 = x) = R(x, y)
λ(y)
< ∞.

308
STOCHASTIC PROCESSES
3. For any state x ∈Ψ and y ∈Ψ transient, we have
lim
t→+∞P(Xt = y|X0 = x) = lim
t→+∞pt(x, y) = 0.
If y is recurrent and it can be reached from x then Wy = +∞a.s., otherwise
Wy = 0 a.s. and since the sojourn times are continuous (exponentially distributed)
we do not have to worry about periodicity. For what follows we consider the case
where the underlying Markov chain Y is irreducible recurrent.
Note that for a Markov chain with transition matrix P (recall deﬁnition 6.24),
solving π = πP can give us the stationary measure. However, for continuous time
parameter processes we need to satisfy equation (6.46) for all t ≥0, a much stronger
requirement.
Deﬁnition 7.14 Limiting behavior of a Markov process
Let X = {Xt : t ≥0} be a Markov process with discrete state space Ψ and transition
function Pt = [(pt(x, y))]. A measure π is called a stationary (or invariant) measure
for X if it satisﬁes
π = πPt,
(7.42)
for all t ≥0, where π = [π(x)]x∈Ψ is a strictly positive row vector denoting the
stationary measure (unique up to a constant multiplication). If π is a probability
measure then it is called the equilibrium (or stationary) distribution and the Markov
process X is called stationary.
Solving equation (7.42) is not easily accomplished since Pt are not easy to
compute. Instead, we use the following theorem to ﬁnd the stationary distribution
of X via its generator matrix A. For a proof see C¸ inlar (1975, p. 264).
Theorem 7.10 (Stationary distribution) Let X = {Xt : t ≥0} be an irreducible
recurrent Markov process with discrete state space Ψ and generator matrix A.
Then µ is a stationary measure for X if and only if µ is a solution to the system
µA = 0,
(7.43)
where µ = [µ(x)]x∈Ψ is a row vector denoting the stationary measure (unique up
to a constant multiplication), with µ(x) > 0, ∀x ∈Ψ. Moreover, the equilibrium
distribution is given by
π(y) = lim
t→+∞P(Xt = y|X0 = x) =
µ(y)
P
x∈Ψ µ(x) =
v(y)/λ(y)
P
x∈Ψ v(x)/λ(x),
(7.44)
where v is the stationary distribution for the underlying Markov chain Y, i.e.,
v = vQ and
µ(y) = v(y)/λ(y),
for all y ∈Ψ.

MARKOV PROCESSES
309
7.4.5.5
Birth-Death Processes
Now we discuss Birth-Death processes in continuous time (recall example
6.19). This is a special class of Markov processes with many applications, in-
cluding population models and queueing systems. For what follows suppose that
X = {Xt : t ≥0} is a stochastic process with state space Ψ = {0, 1, 2, . . .}, where Xt
denotes the size of a population at time t. Transitions occur when a birth or a death
occurs in the population so that a jump either increases or decreases the population
size by 1. The following remark summarizes the construction of the process.
Remark 7.18 (Birth-Death process) Let Gt and Dt denote the lengths of time from
t until the next birth and the next death event, respectively. Assume that if Xt = x,
then for some constants λ(x) and p(x) we have
P(Gt > s, Dt > s|Xu, u ≤t) = e−λ(x)s,
(7.45)
and
P(Gt ≤Dt|Xu, u ≤t) = p(x),
(7.46)
so that under these assumptions X is a Markov process. We build the generator
matrix of X based on λ(x) and p(x).
1. Generator Let
ax
=
p(x)λ(x) ≥0,
(7.47)
bx
=
(1 −p(x))λ(x) ≥0,
(7.48)
x ∈Ψ and note that ax and bx can be interpreted as the respective time rates of
births and deaths when the population size is x. Clearly, b0 = 0 since we cannot
have deaths in a population of size 0 and we cannot have (ax, bx) = (0, 0), ∀x ∈Ψ.
Therefore, from equation (7.37) we have
A =

−a0
a0
0
b1
−a1 −b1
a1
b2
−a2 −b2
a2
0
...
...
...

,
(7.49)
where the underlying Markov chain Y has transition probabilities
q(x, x −1) = 1 −q(x, x + 1) = bx/(ax + bx), x ≥1,
and q(0, 1) = 1.
2. Regularity Regularity of the Markov process X can be shown using the criteria
of remark 7.2. In particular, if X is not regular then the population size escapes
to ∞in a ﬁnite time with positive probability. Otherwise, the population remains
ﬁnite w.p. 1 but it may still reach ∞. More precisely, letting R(y, y) denote the
average number of visits to state y, starting from y, then if the series P
y∈Ψ R(y, y)/λ(y)
converges then X is not regular, whereas, if the series diverges then X is regular.
3. Limiting behavior From theorem 7.10 a limiting distribution π exists if and

310
STOCHASTIC PROCESSES
only if π satisﬁes πA = 0 and
+∞
P
y=0 π(y) = 1 with A given by (7.49). First we ﬁnd the
stationary measure by solving the system µA = 0 with µ = [µ(0), µ(1), µ(2), . . . ]
which yields the equations
µ(y) = ay−1
by
µ(y −1),
so that
µ(y) = a0a1 . . . ay−1
b1 . . . by
µ(0),
y = 1, 2, . . . and therefore the limiting distribution exists if
+∞
P
y=0 µ(y) = µ(0) + µ(0)
+∞
P
y=1
a0a1 . . . ay−1
b1 . . . by
= µ(0)c < ∞,
with
c = 1 +
+∞
P
y=1
a0a1 . . . ay−1
b1 . . . by
.
(7.50)
Now if c < ∞then choosing µ(0) = 1/c we have
+∞
P
y=0 µ(y) = 1 and the stationary
distribution is given by
π(y) = lim
t→+∞P(Xt = y|X0 = x) =

1
c
if y = 0
a0a1...ay−1
cb1...by
if y > 0 ,
(7.51)
for any x ∈Ψ, whereas, if c = +∞then
lim
t→+∞P(Xt = y|X0 = x) = 0,
for all x, y ∈Ψ.
We present several examples in order to illustrate these Birth-Death models.
Example 7.11 (Population model) Consider a population where each individual
has an exponential lifetime with parameter b and each individual can generate an
oﬀspring (birth) after an exponential time with parameter a. We are interested in the
Markov process X, with Xt denoting the population size at time t ≥0. Clearly this
is a Birth-Death process with birth rates ax = ax and death rates bx = bx. Then the
underlying Markov chain Y has transition probabilities q(0, 0) = 1, q(x, x + 1) = p,
q(x, x −1) = 1 −p, for x ≥1, where p =
a
a+b, so that λ(0) = 0 and λ(x) = x(a + b),
for x ≥1 (which are not bounded from above). Now if a ≤b the Markov chain
Y is absorbing (the population dies out) so that from remark 7.2.3 the process X is
regular. In addition, if a > b using remark 7.18.2 we have
+∞
P
y=1
R(y, y)
y(a + b) ≥
1
a + b
+∞
P
y=1
1
y = +∞,
so that X is regular in this case as well. Since the process X is regular, P(Xt < ∞) =
1, ∀t ≥0, however, for a > b the population tends to grow out of control leading to
lim
t→+∞P(Xt ≤y) = 0, ∀y ∈Ψ and therefore the population size becomes inﬁnite.
Example 7.12 (Queueing systems)
We utilize the results we presented in this

MARKOV PROCESSES
311
section on Birth-Death processes in order to illustrate several queueing systems.
As usual, let X = {Xt : t ≥0} denote the queue-size process and assume that
arrivals follow a Poisson process with rate a, whereas, services follow exponential
times with rate b.
1. M/M/1 queue This queue is a special case of a Birth-Death process with birth
rates ax = a and death rates bx = b. Let r = a/b denote the ratio of the rate
of arrivals to the rate of service times (known as the traﬃc intensity). Then the
normalizing constant of equation (7.50) is given by
c =
+∞
P
y=0 ry =

+∞
if a ≥b
1
1−r
if a < b ,
and therefore if r < 1 then the stationary distribution is
π(y) = (1 −r)ry,
y = 0, 1, 2, . . ., otherwise, if r ≥1 then lim
t→+∞P(Xt = y) = 0, ∀y ∈Ψ. Therefore, if
r < 1 the server can keep up with arrivals and the queue never explodes to inﬁnity,
otherwise, for r ≥1 arrivals occur faster than the server can handle and the queue
goes to inﬁnity.
2. M/M/1/m queue In this queue we need to modify the M/M/1 queue setup. In
particular, we have birth rates ax = a, x = 0, 1, . . . , m, ax = 0, x ≥m + 1 and
death rates bx = b, x = 1, 2, . . ., m, bx = 0, x ≥m + 1. Therefore, the set of states
{m + 1, m + 2, . . . } are transient and C = {0, 1, . . ., m} is a recurrent irreducible set.
The stationary distribution follows in similar steps as in the M/M/1 queue, however
we work with the set C only. More precisely, assuming r = a/b and a , b, we have
c =
mP
y=0 ry = 1 −rm+1
1 −r ,
so that the stationary distribution is given by
π(y) =
1 −r
1 −rm+1ry,
y ∈C and π(y) = 0, y < C. Note that the state space for X is Ψ = C but the
representation as a Birth-Death process can be given for Ψ = {0, 1, 2, . . .}.
3. M/M/k queue Now consider k-servers working independently. If there are x <
k customers in the system then x servers are busy so that (7.45) becomes
P(Gt > s, Dt > s|Xu, u ≤t) = e−ase−xbs,
whereas, if there are x ≥k customers in the queue, all servers are busy so that
P(Gt > s, Dt > s|Xu, u ≤t) = e−ase−bs.
As a result, the queue-size process is a Birth-Death process with birth rates ax = a,
∀x = 1, 2, . . . and death rates bx = bx, x = 1, 2, . . . , k and bx = kb, x ≥k + 1. The
stationary distribution exists when a < kb.
4. M/M/∞queue In this case the birth rates are ax = a, ∀x = 1, 2, . . . and the

312
STOCHASTIC PROCESSES
death rates are bx = bx, x ≥1. Let r = a/b. It is straightforward to show that the
stationary distribution is given by
π(y) = e−rry
y! ,
y = 0, 1, 2, . . ., which is nothing but a Poisson(r) density.
7.4.6
Brownian Motion
Now we turn to a classic continuous time parameter process with continuous
state space, known as Brownian motion. We deﬁne ﬁrst the one-dimensional Brow-
nian motion and discuss some of its properties. The construction of the Wiener
measure (the probability distribution of Brownian motion) is given in the next sub-
section.
Deﬁnition 7.15 One-dimensional Brownian motion
Given a ﬁltration F = {Ft : t ≥0}, a one-dimensional F -Brownian motion
(or Wiener process) in [0, +∞), denoted by BM([0, +∞), F ), is a continuous real-
valued process B = {Bt : t ≥0} deﬁned on some probability space (Ω, A, P), that is
adapted to F , with the following properties:
(a) B0 = 0 a.s. [P],
(b) for 0 ≤s < t the increment Bt −Bs is independent of Fs, and
(c) Bt −Bs ∼N(0, t −s), for all 0 ≤s < t.
We may deﬁne Brownian motion on [0, T], denoted by BM([0, T], F ), for some
T > 0, by considering the stochastic process {Bt, Ft : 0 ≤t ≤T}. We omit the
ﬁltration F from the notation of the Brownian motion when it is clear from the
context.
We summarize several properties and results on Brownian motion below.
Remark 7.19 (Properties of Brownian motion) Assume that B is a BM([0, +∞)).
1. Condition (b) can be restated as follows: if 0 = t0 < t1 < · · · < tn < ∞, then the
increments {Bti −Bti−1}n
i=1 are independent and Bti −Bti−1 ∼N(0, ti −ti−1). In other
words, B has stationary and independent increments.
2. We can easily see that B is a continuous, square integrable MG, a local MG
(taking τn = n) and the transformations B2
t −t and eaBt−a2t/2, a ∈R, are MGs.
However, B is not a uniformly integrable MG. In view of equation (7.2) we have
⟨B⟩t
d= t a.s. [P].
3. The choice of ﬁltration is important in the latter deﬁnition. When we do not
mention the ﬁltration but we are given only a Markov process B = {Bt : t ≥0} and
we assume that B has stationary and independent increments and Bt = Bt −B0 ∼
N(0, t), then {Bt, F B
t : t ≥0} is a Brownian motion.
4. The following transformations of B are also Brownian motions on [0, +∞) : (i)

MARKOV PROCESSES
313
t 7→−Bt, (ii) t 7→(Bs+t −Bs), ∀s ≥0, ﬁxed, (iii) t 7→√aBt/a, a > 0, ﬁxed, and (iv)
t 7→tB1/t, t > 0 and t 7→0, if t = 0.
5. We can deﬁne Brownian motion equivalently in terms of a Gaussian Process
(GP, i.e., G = {Gt : t ≥0} is a GP if all ﬁnite-dimensional distributions of G are
multivariate normal) as follows: (i) B is a GP, (ii) EBt = 0 and E(BsBt) = min(s, t),
and (iii) the sample paths t 7→Bt are continuous a.s.
6. Translation invariance We can show that {Bt −B0 : t ≥0} is independent of
B0 and it has the same distribution as Brownian motion with B0 = 0.
7. Strong law It can be shown that
lim
t→∞Bt/t = 0 a.s.
8. Non-diﬀerentiable For almost all ω ∈Ωthe sample paths of a Brownian mo-
tion Bt(ω) are nowhere diﬀerentiable.
9. Strong Markov property Assume that B is a BM([0, +∞)). For any a.s. ﬁnite
stopping time τ the stochastic process B∗
t = Bτ+t −Bτ, ∀t ≥0, is BM([0, +∞)) and
is independent of F B
τ . In this case we say that B has the strong Markov property at
τ.
10. Laws of iterated logarithm Brownian motion satisﬁes
lim sup
t→∞
|Bt|/
p
2t ln(ln t) = 1 a.s.,
and
lim sup
t→0
|Bt|/
p
2t ln(ln(1/t)) = 1 a.s.
11. L´evy’s characterization Let X ∈Mc,loc with X0 = 0. The following are equiv-
alent:
(i) X is Brownian motion,
(ii) ⟨B⟩t
d= t a.s. [P], for all t ≥0, and
(iii) X2
t −t is a local MG.
The extension to the multivariate case is straightforward.
Deﬁnition 7.16 Multi-dimensional Brownian motion
Consider Q0, a probability measure on (Rp, Bp), and let B = {Bt : t ≥0} be a
continuous process deﬁned on a probability space (Ω, A, P) that takes values on
Ψ = Rp and assume that B is adapted to a ﬁltration F = {Ft : t ≥0}. The process
B is called a p-dimensional Brownian motion (denoted by MBM([0, +∞))) with
initial distribution Q0 if
(a) P(B0 ∈A) = Q0(A), ∀A ∈Bp, and
(b) for all 0 ≤s < t, the increment Bt −Bs is independent of Fs and has a multivari-
ate normal distribution Np(0, (t −s)Id).

314
STOCHASTIC PROCESSES
Figure 7.1: Brownian motion realizations: (a) univariate, (b) bivariate.
If Q0({x}) = δx then we say that B is a p-dimensional Brownian motion start-
ing at x ∈Rp.
Example 7.13 (Brownian motion)
In Figure 7.1 (a), we simulate and display
three realizations of a univariate Brownian motion, whereas in plot (b) we have a
realization of a bivariate Brownian motion. See appendix section A.7 for details on
the code used to generate these plots.
7.4.7
Construction of the Wiener Measure
Two questions arise when deﬁning a Markov process with certain properties;
ﬁrst, does it exist (can we construct a Markov process with these properties) and
second is it unique. Construction of Brownian motion has been accomplished in
many diﬀerent ways over the past century. The typical approach is to apply the
modiﬁcation of Kolmogorov’s existence theorem (Kolmorogov-ˇCentsov, theorem
7.3) and construct a stochastic process with ﬁnite-dimensional distributions, such
as those under Brownian motion assumptions. Another approach is to exploit the
Gaussian property of this stochastic process and is based on Hilbert space theory.
The approach we discuss here is based on weak limits of a sequence of ran-
dom walks on the space of continuous R-valued functions CR
[0,∞). See Karatzas and
Shreve (1991), Fristedt and Gray (1997), Durrett (2010), Bass (2011) and Klenke
(2014) for excellent treatments of Brownian motion and examples, as well as the
construction of the Wiener measure under the diﬀerent settings discussed above.
Note that since CR
[0,∞) is a subset of the c`adl`ag space DR
[0,∞), topological results
we have studied for DR
[0,∞) carry over immediately to the subspace CR
[0,∞), e.g., re-
sults on Polish spaces. In addition, working on the c`adl`ag space (RCLL paths) we
are able, as we saw in example 7.5, to deﬁne the Poisson process, and as we will

MARKOV PROCESSES
315
see next, construct Brownian motion, thus providing a unifying framework that can
handle both continuity and the countable number of jumps for the sample paths of
a stochastic process.
The construction that follows considers a setup similar to that of example 4.2.2,
where we deﬁned random functions on CR
[0,1] and based on such functions we build
the Wiener measure initially on [0, 1] and then extend it to [0, +∞). Alternatively,
one can deﬁne CR
[0,+∞)-valued random variables directly (Karatzas and Shreve, 1991,
Section 2.4).
Remark 7.20 (Sequences of random variables in CR
[0,1]) Consider a sequence of
iid random variables {Zn}+∞
n=0 with mean µ and variance σ2, 0 < σ2 < ∞and de-
ﬁne the random walk S = (S 0, S 1, . . . ), with S 0 = 0 and S k =
kP
i=1 Zi, k ≥1. We
deﬁne CR
[0,1]-valued random variables X(n) by deﬁning the function t 7−→X(n)
t (ω)
as follows. First consider values for t equal to a multiple of 1
n, that is, for each
n = 1, 2, . . ., we let
X(n)
k/n =
1
σ √n
kP
i=1(Zi −µ) = S k −kµ
σ √n ,
for k = 0, 1, 2, . . ., n and the random variable X(n)
t , 0 ≤t ≤1, can be made
continuous for t ∈[0, 1] by assuming linearity over each of the intervals Ik,n =
[(k −1)/n, k/n], that is, we linearly interpolate the value of X(n)
t , for any t ∈Ik,n,
based on the boundary values at X(n)
(k−1)/n and X(n)
k/n. Now note that the increment
X(n)
(k+1)/n −X(n)
k/n = (Zk+1 −µ)/(σ √n) is independent of F X(n)
k/n
= σ(Z1, . . . , Zk) and
X(n)
(k+1)/n −X(n)
k/n has zero mean and variance 1/n = (k + 1)/n −k/n so that apart from
the normality assumption {X(n)
t
: t ≥0} is behaving like a BM([0, 1]).
The next theorem shows that even though the steps of the random walk are not
necessarily normal, CLT guarantees that the limiting distributions of the increments
of X(n) are multivariate normal.
Theorem 7.11 (Convergence in law) Let {X(n)
t
: t ≥0, n ≥1} be deﬁned as in
remark 7.20.
(i) For any 0 ≤t0 < t1 < · · · < tp ≤1, we have

X(n)
t1 −X(n)
t0 , . . . , X(n)
tp −X(n)
tp−1

w→(Bt1−t0, . . . , Btp−tp−1),
as n →+∞, where Btk−tk−1 are iid N(0, tk −tk−1), k = 1, 2, . . ., p.
(ii) Any subsequential limit B of the sequence {X(n) : n ≥1} has the properties:
(a) for 0 ≤t ≤1, Bt ∼N(0, t), and
(b) B has independent and stationary increments,
and as a result of (a) and (b), B is a BM([0, 1]).
(iii) If there exists a B∗, B that satisﬁes properties (a) and (b) then B and B∗
have the same distribution so that all Wiener processes on [0, 1] have the same

316
STOCHASTIC PROCESSES
distribution and consequently all subsequential limits of {X(n) : n ≥1} have the
same distribution.
The latter part of the theorem above leads to the following deﬁnition.
Deﬁnition 7.17 Wiener measure
The common distribution Qw of all Brownian motions on [0, 1] is called the
Wiener measure on [0, 1].
Exercises 7.31 and 7.32 summarize the important results one needs to show in
succession in order to show existence and uniqueness of the Wiener measure as
well as existence of subsequential limits of {X(n) : n ≥1}. Theorem 7.12 below is
also a consequence of these results.
Remark 7.21 (Existence and uniqueness) For what follows assume that Qn is the
distribution of the CR
[0,1]-valued random variable X(n), n ≥1. In view of remark 5.3,
parts 4-8, we require the sequence {Qn}+∞
n=1 to be uniformly tight. In that case every
subsequence of {Qn}+∞
n=1 has a further subsequence that converges. Then from theo-
rem 7.11, part (ii), since a convergent subsequence exists then the Wiener measure
Qw exists on CR
[0,1]. Uniqueness follows via theorem 7.11, part (iii), and since all
limits of convergent subsequences are identical using remark 5.3.7, Qw is deﬁned
as the weak limit of {Qn}+∞
n=1, that is, Qn
w→Qw, as n →+∞. The latter result is
known as Donsker’s invariance principle.
Theorem 7.12 (Donsker invariance principle) Consider a sequence of iid ran-
dom variables {Zn}+∞
n=1 with mean µ and variance σ2, 0 < σ2 < ∞, deﬁned
on some probability space (Ω, A, P). Let {X(n)
t
: t ≥0} be as deﬁned in re-
mark 7.20 and let Qn denote the distribution of X(n) on the measurable space

CR
[0,1], B

CR
[0,1]

. Then {Qn}+∞
n=1 converges weakly to Qw.
The generalization of the Wiener process from [0, 1] to [0, +∞) is straightfor-
ward as we see below.
Remark 7.22 (Wiener measure on [0, +∞)) In order to construct the Wiener mea-
sure on CR
[0,+∞) we deﬁne a CR
[0,+∞)-valued random variable that has independent and
stationary increments and a Wiener process when restricted to [0, 1]. In particular,
let {B(n) : n ≥0} be an iid sequence of CR
[0,1]-valued Wiener processes and for
n = 0, 1, 2, . . . and t ∈[n, n + 1] deﬁne
Bt = B(n)
t−n +
n−1
P
k=0 B(k)
1 ,
so that the random function Bt is BM([0, +∞)) and its distribution is the Wiener
measure on CR
[0,+∞).
Example 7.14 (Measurable functionals) There are several functionals of inter-

BUILDING ON MARTINGALES AND BROWNIAN MOTION
317
est that are measurable in

CR
[0,1], B

CR
[0,1]

, for example
M(φ) = max{φ(t) : 0 ≤t ≤1},
I(φ) =
1R
0
φ(t)dt,
and
L(φ) = µ1({t ∈[0, 1] : φ(t) > 0}),
where µ1 denotes the Lebesgue measure on [0, 1]. The functionals M(φ) and I(φ)
are continuous but L(φ) is not. It can be shown that under the Wiener measure and
for any t ≥0 we have
Qw({φ : M(φ) ≤t}) =
p
2/π
tR
0
e−u2/2du,
and the distribution of I(φ) is N(0, 1/3). Moreover, for 0 ≤t ≤1 we have
Qw({φ : L(φ) ≤t}) = π−1
tR
0
(u(1 −u))−1/2 du = 2π−1 arcsin
√
t.
7.5
Building on Martingales and Brownian Motion
In this section we deﬁne two important random objects, stochastic integrals and
stochastic diﬀerential (or diﬀusion) equations (SDEs), with applications ranging
from biological sciences, physics and engineering to economics. Stochastic calcu-
lus is required when ordinary diﬀerential and integral equations are extended to
involve continuous stochastic processes.
Since the most important stochastic process, Brownian motion, is not diﬀer-
entiable, one cannot use the same approach to deﬁning stochastic derivatives and
integrals as with ordinary calculus. Instead, we begin by deﬁning a stochastic inte-
gral as a Riemann-Stieltjes integral with respect to a random continuous function
and then deﬁne stochastic derivatives, with the latter describing the inﬁnitesimal
rate of change of a stochastic process.
We restrict the exposition to stochastic calculus with Brownian motion integra-
tors and brieﬂy discuss the extension to semi-MG integrators. Theoretical develop-
ment, deﬁnitions and examples of stochastic integrals and diﬀerential equations can
be found in Karatzas and Shreve (1991), Durrett (1996), Fristedt and Gray (1997),
Durrett (2010), Bass (2011) and Klenke (2014).
7.5.1
Stochastic Integrals
For what follows let (B, F ) = {Bt, Ft : t ≥0} be Brownian motion deﬁned on
a probability space (Ω, A, P) with B0 = 0 a.s. [P]. Our goal in this section is to
deﬁne a random object that is represented notationally by the integral
IB
t (X) =
tR
0
XsdBs,

318
STOCHASTIC PROCESSES
for a large class of integrands X : Ω×[0, +∞) →R with Xt chosen in such a way that
IB
t (X) is a continuous MG with respect to F . At ﬁrst glance the stochastic integral
IB
t (X) can be thought of as the stochastic version of the corresponding deterministic
integral of the sample paths for a given ω ∈Ω, i.e.,
IB
t (X)(ω) =
tR
0
Xs(ω)dBs(ω).
However, since almost all paths s 7→Bs(ω) of Brownian motion are a.s. nowhere
diﬀerentiable (Paley-Wiener-Zygmud theorem, Klenke, 2014, p. 467) and hence
have locally inﬁnite variation, IB
t (X) cannot be deﬁned and understood as a
Riemann-Stieltjes integral in the framework of classical integration theory. There-
fore, we must use a diﬀerent approach to deﬁne it and in particular, we establish the
stochastic integral as an L2-limit. The approach of the construction is as follows:
ﬁrst we deﬁne IB
t (X) for simple integrands X (i.e., the sample path of X, t 7→Xt(ω)
is a simple function), that is, IB
t (X) is a ﬁnite sum of stochastic processes. Next we
extend the deﬁnition to integrands that can be approximated by simple integrands
in a certain L2-space.
Example 7.15 (Stock prices) We can think of Bs as the price of a stock at time
s and Xs as the number of shares we have (may be negative, selling short). The
integral IB
t (X) represents the net proﬁts at time t relative to our wealth at time 0.
The inﬁnitesimal rate of change of the integral dIB
t (X) = XtdBt represents the rate
of change of the stock multiplied by the number of shares we have.
The following remarks summarize the steps required.
Remark 7.23 (Conditions on integrands) First we need to discuss the kinds of
integrands X that are appropriate in deﬁning IB
t (X).
1. Measurability condition In order for the stochastic process IB(X) = {IB
t (X) :
t ≥0} to be a MG the integrands X will be required to be progressively measurable
(which implies that X is adapted and measurable). The measurability requirement
depends on the type of integrator we use to deﬁne the integral. See Karatzas and
Shreve (1991, p. 131) for some discussion on how to develop the integral IM
t (X)
under diﬀerent types of integrators M (other than B) and integrands X, e.g., for M ∈
M2 we require X to be a smaller class, namely, predictable stochastic processes.
2. Integrability condition The second condition required is a certain type of in-
tegrability. For any measurable, adapted process X we deﬁne two L2-norms by
[X]2
T = E

TR
0
X2
t dt
,
for any T > 0 and let
[X]2
∞= E

+∞
R
0
X2
t dt
.
Denote by E∗the collection of all adapted, measurable stochastic processes satis-

BUILDING ON MARTINGALES AND BROWNIAN MOTION
319
fying [X]∞< ∞. We deﬁne a metric on E∗by ρE∗(X, Y) = [X −Y]∞, for X, Y ∈E∗
and as a result we can deﬁne limits of sequences of elements of E∗.
Now we discuss the types of integrands used in order to deﬁne the stochastic
integral.
Remark 7.24 (Simple integrands) Consider step functions (simple stochastic pro-
cesses) X : Ω× [0, +∞) →R of the form
Xt(ω) =
nP
i=1 xi−1(ω)I(ti−1,ti](t),
(7.52)
where 0 = t0 < t1 < · · · < tn, xi is an Fti-measurable random variable, for i =
1, . . . , n, n ≥0 and sup
k≥0
|xk(ω)| < +∞, for all ω ∈Ωand any t ≥0. The collection
of all such simple stochastic processes is denoted by E0. Note that by deﬁnition all
X ∈E0 are progressively measurable and bounded with
[X]2
∞= E

+∞
R
0
X2
t dt
=
nP
i=1 E

x2
i−1

(ti −ti−1) < +∞,
so that X ∈E0 ⊂E∗.
1. Integrating simple stochastic processes For any X ∈E0 and t ≥0 we deﬁne
the stochastic integral of X with respect to B by
IB
t (X) =
tR
0
XsdBs
d=
nP
i=1 xi−1
 Bti∧t −Bti−1∧t
 ,
which leads to a stochastic process IB(X) = {IB
t (X) : t ≥0}, whereas
IB
∞(X) =
+∞
R
0
XsdBs
d=
nP
i=1 xi−1
 Bti −Bti−1
 ,
is an R-valued random variable and both integrals are simply linear transformations
of Brownian motion. Note that E
h
IB
∞(X)
i2 = [X]2
∞< ∞.
2. Basic properties Let X, Y ∈E0 and 0 ≤s < t < ∞. By construction, it is easy
to show that IB
t (X) is a CR
[0,+∞)-valued random variable, that is a continuous time,
square integrable MG with respect to F and moreover
IB
0 (X)
=
0 a.s. [P],
E
h
IB
t (X)|Fs
i
=
IB
s (X) a.s. [P],
E
h
IB
t (X)
i
=
[X]2
t = E

tR
0
X2
udu
< ∞,
E

IB
t (X) −IB
s (X)
2 |Fs

=
E
" tR
s
X2
udu|Fs
#
a.s. [P],
and
IB
t (aX + bY) = aIB
t (X) + bIB
t (Y) a.s. [P],
for all a, b ∈R.

320
STOCHASTIC PROCESSES
3. Approximations in E∗via simple stochastic processes If X ∈E∗then the se-
quence of simple processes
X(n)
t (ω) = X0(ω)I{0}(t) +
2n−1
P
k=0 XkT2−n(ω)I[kT2−n,(k+1)T2−n](t),
n ≥1, is such that X(n)
t
∈E0 and
lim
n→∞
h
X(n)
t
−Xt
i2
T = lim
n→∞E

TR
0
X(n)
t
−Xt

2 dt
= 0,
for any T > 0. See Karatzas and Shreve (1991, p. 132) or Klenke (2014, p. 566)
for more details on the construction of the simple integrands X(n) ∈E0 and their
limiting behavior. Consequently, limits of simple stochastic processes are suitable
integrands in E∗and therefore we augment the original collection of simple inte-
grands E0 with all stochastic processes that are limits of elements of E0, that is, we
take the closure E0 of the set E0, denoted by E = E0. Note that E0 ⊂E ⊂E∗. It can
be shown that if X is progressively measurable and [X]∞< ∞then X ∈E.
Based on the discussion of the last two remarks, the stochastic integral is now
naturally introduced.
Deﬁnition 7.18 Itˆo integral as a random variable
For any X ∈E consider a sequence of simple integrands X(n) ∈E0 such that
lim
n→∞
h
X(n) −X
i
∞= 0.
Then the stochastic integral (Itˆo integral random variable) of X with respect to
Brownian motion B = {Bt, Ft : t ≥0} over [0, +∞) is the square-integrable random
variable IB
∞(X) that satisﬁes
lim
n→∞
IB
∞

X(n)
−IB
∞(X)
2 = 0,
for every such sequence X(n) (in other words IB
∞

X(n) L2
→IB
∞(X), recall remarks 3.24
and 3.25). We write
IB
∞(X) =
∞R
0
XsdBs.
The following remark illustrates how we can build a stochastic integral over a
ﬁnite interval [0, t].
Remark 7.25 (Itˆo integral as a stochastic process) A similar approach can be
used in the deﬁnition of the stochastic integral of X with respect to B over [0, t].
For any progressively measurable X we weaken the strong integrability condition
[X]2
∞< ∞by requiring
TR
0
X2
t dt < ∞,
(7.53)
for all T > 0. Denote by Eloc the collection of all progressively measurable stochas-

BUILDING ON MARTINGALES AND BROWNIAN MOTION
321
tic processes satisfying (7.53). The deﬁnition that follows requires the following re-
sult; for any X ∈Eloc there exists a sequence {τn}+∞
n=1 of stopping times with τn ↑∞
a.s. and [X]2
τn < ∞and hence such that X(τn) ∈E, where
X(τn)
t
= XtI(t ≤τn),
for all n ≥1.
Using the ideas of the last remark we can deﬁne the stochastic integral as a
stochastic process.
Deﬁnition 7.19 Itˆo integral as a stochastic process
For any X ∈Eloc let {τn}+∞
n=1 and X(τn) be as in remark 7.25. The Itˆo integral of X
with respect to Brownian motion B = {Bt, Ft : t ≥0} over [0, t] is a stochastic
process IB(X) = {IB
t (X), Ft : t ≥0} that satisﬁes
IB
t (X) =
tR
0
XdB = lim
n→∞
tR
0
X(τn)
s
dBs a.s.,
(7.54)
for any choice of {τn}+∞
n=1. We write
tR
s
XudBu = IB
t (X) −IB
s (X),
for the integral of X over [s, t], 0 ≤s ≤t ≤∞.
Example 7.16 (Stochastic integration)
We compute IB
t (B) =
tR
0
BsdBs. Since
B ∈E we can deﬁne the Itˆo integral using simple integrands B(n)
t
and then taking
the limit as follows
IB
t (B)
=
tR
0
BsdBs = lim
n→+∞
2n−1
P
k=0 Bkt2−n(ω)  B(k+1)t2−n −Bkt2−n
=
lim
n→+∞
1
2
2n−1
P
k=0
h
B2
(k+1)t2−n −B2
kt2−n

− B(k+1)t2−n −Bkt2−n2i
=
B2
t /2 −lim
n→+∞
1
2
2n−1
P
k=0
 B(k+1)t2−n −Bkt2−n2 ,
where  B(k+1)t2−n −Bkt2−n2 ∼N

t2−n, 2t22−2n
and therefore the sum is N

t, t221−n
.
Consequently, the sum converges in L2 to t so that
IB
t (B) =
tR
0
BsdBs = B2
t /2 −t/2.
The term t/2, although unexpected, guarantees that IB
t (B) is a continuous-time MG
as required by construction. This example shows that ordinary rules of calculus do
not apply to the Itˆo integral.
We summarize several important results about IB(X) below, including how to
ﬁnd solutions to stochastic integral equations.

322
STOCHASTIC PROCESSES
Remark 7.26 (Properties and extensions of the stochastic integral) We note the
following properties and results.
1. Assume that X is progressively measurable with [X]2
T < ∞, for all T > 0.
The integral IB(X) as deﬁned in (7.54) is a continuous, square integrable MG, that
satisﬁes all the properties of remark 7.24.2. Moreover, the stochastic process Nt =
IB
t (X) −
tR
0
X2
udu, t ≥0, is a continuous MG with N0 = 0 a.s.
2. Quadratic variation Assume that X ∈Eloc. The integral IB(X) is a continuous
local MG, with a square variation stochastic process given by
D
IB(X)
E
t =
tR
0
X2
udu,
t ≥0.
3. Beyond Brownian motion The Itˆo integral need not be deﬁned with respect to
Brownian motion. In particular, starting with a simple integrand X ∈E0 of the form
(7.52) and a (local) MG (M, F ) the integral
IM
t (X)
d=
nP
i=1 xi−1
 Mti∧t −Mti−1∧t
 ,
is a (local) MG with
E

IM
∞(X)
2
=
nP
i=1 E xi−1
 Mti∧t −Mti−1∧t
 =
nP
i=1 E
h
xi−1

⟨M⟩ti∧t −⟨M⟩ti−1∧t
i
=
E

∞R
0
X2
t d ⟨M⟩t
,
provided that the RHS is ﬁnite. We can repeat the construction procedure for B to
deﬁne the integral with respect to M provided that we equip E0 with the norm
[X]2
M = E

+∞
R
0
X2
t d ⟨M⟩t
,
and follow similar steps. Recall that ⟨B⟩t = t so that the procedure is a direct ex-
tension of the construction with respect to Brownian motion B. The extension is
completed once we identify which integrands belong to the closure E = E0, e.g., if
M is a continuous MG then X needs to be predictable in order for the integral to
be a MG. In order to have progressively measurable integrands in E the additional
requirement needed is that the square variation ⟨M⟩is absolutely continuous with
respect to the Lebesgue measure. See Klenke (2014, p. 572) for more details. We
summarize the extension in the next part of this remark.
4. Local martingale integrators Let (M, F ) be a continuous local MG with abso-
lutely continuous square variation ⟨M⟩(with respect to the Lebesgue measure) and
assume that X is a progressively measurable process that satisﬁes
TR
0
X2
t d ⟨M⟩t < ∞,
a.s. for all T ≥0. Let {τn}+∞
n=1 be as in remark 7.25, with
h
X(τn)i
M < ∞and deﬁne
simple stochastic processes Xn,m ∈E0, m ≥1, with lim
m→+∞
h
Xn,m −X(τn)i
M = 0, n ≥1.

BUILDING ON MARTINGALES AND BROWNIAN MOTION
323
The Itˆo integral of X with respect to M is deﬁned as a limit in probability by
IM
t (X) =
tR
0
XdM =
tR
0
XudMu = lim
n→+∞lim
m→+∞IM
t (Xn,m),
for all t ≥0 and it is a continuous local MG with square variation
D
IM(X)
E
t =
tR
0
X2
ud ⟨M⟩u .
5. Arithmetic of integration Stochastic integrals are the same as ordinary inte-
grals when it comes to linearity as we have seen in terms of linear integrands.
Linearity carries over to integrators as well. In particular, if a, b ∈R, are constants,
X, Y ∈Mc,loc and Z, W are continuous stochastic processes, then for all t ≥0 we
have
tR
0
Zud(aXu + bYu) = a
tR
0
ZudXu + b
tR
0
ZudbYu,
i.e., linearity with respect to the integrator. Moreover
tR
0
XudYu = XtYt −X0Y0 −
tR
0
YudXu + ⟨X, Y⟩t ,
(7.55)
is known as the product rule or integration by parts and if Yt =
tR
0
ZudXu then
tR
0
WudYu =
tR
0
(WuZu) dXu.
6. Itˆo formula Let X be a continuous local MG with paths locally of bounded
variation and assume that f : R →R is two times continuously diﬀerentiable with
bounded derivatives (denote this set of functions by C2(R, R)). Then
f (Xt) = f (X0) +
tR
0
f ′(Xs)dXs + 1
2
tR
0
f ′′(Xs)d ⟨X⟩s ,
(7.56)
a.s., for all t ≥0. The Itˆo formula is also known as the change-of-variable for-
mula and it is of extreme importance in ﬁnding solutions to stochastic integral and
diﬀerential equations. For X ∈Mc,loc the Itˆo formula shows that f (X) is always
a semi-MG but it is not a local MG unless f ′′(x) = 0, for all x. The multivariate
extension is straightforward: assume that X = {Xt = (Xt1, . . . , Xtp), Ft; t ≥0}, is a
p-variate local MG (i.e., Xi ∈Mc,loc), where all cross-variation stochastic processes
D
Xi, X j
E
t exist and are continuous for all i, j = 1, 2, . . ., p and f ∈C2(Rp, R). Then
we can write
f (Xt) = f (X0) +
pP
i=1
tR
0
∂f (Xs)
∂xi
dXsi + 1
2
pP
i, j=1
tR
0
∂2 f (Xs)
∂xi∂xj
d
D
Xi, X j
E
s ,
(7.57)
provided that
tR
0
∂f(Xs)
∂xi dXsi exist, for all i = 1, 2, . . . , p.
7. Girsanov formula Girsanov’s formula is another useful tool in solving stochas-

324
STOCHASTIC PROCESSES
tic integral equations. Assume that B = {Bt = (Bt1, . . . , Btp), Ft; t ≥0}, is p-
dimensional Brownian motion, where F satisﬁes the usual conditions. Let X =
{Xt = (Xt1, . . . , Xtp), Ft; t ≥0} be a vector of measurable, adapted processes satis-
fying
TR
0
X2
tidt < ∞a.s., for all i = 1, 2, . . ., p and T ≥0. Then for each i the integral
IBi(Xi) = {IBti(Xti) : t ≥0} is well deﬁned with IBi(Xi) ∈Mc,loc so that we can deﬁne
Yt(X) = exp

pP
i=1
tR
0
XsidBsi −1
2
tR
0
XT
s Xsds
,
which is a MG. Then the p-variate stochastic process eB = {eBt =
eBt1, . . . , eBtp

,
Ft; 0 ≤t ≤T} deﬁned by
eBti = Bti −
tR
0
Xsids,
is a p-dimensional Brownian motion for all T ≥0.
Example 7.17 (Applying Itˆo’s formula) Let X be a continuous local MG with
X0 = 0 and deﬁne the stochastic process Yt = exp {Xt −⟨X⟩t /2} known as the
exponential of the MG X. Applying Itˆo’s formula to Zt = Xt−⟨X⟩t /2 with f (x) = ex
yields
Yt = exp {Xt −⟨X⟩t /2} = 1 +
tR
0
eZsd (Xs −⟨X⟩s /2) + 1
2
tR
0
eZsd ⟨X⟩s = 1 +
tR
0
YsdXs,
and since Y is the stochastic integral with respect to a local MG, Y is a local MG.
This stochastic integral equation can be written in terms of diﬀerentials as dYt =
YtdXt, deﬁning a stochastic diﬀerential equation as we see in following section.
7.5.2
Stochastic Diﬀerential Equations
We turn now to the extension of ordinary diﬀerential equations and investigate
random objects obtained as solutions to stochastic diﬀerential equations (SDEs).
Such random processes are known as diﬀusion processes and they are typically Rp-
valued, time-homogeneous strong Markov processes. We begin with the discrete
case.
Remark 7.27 (Stochastic diﬀerence equations) Let B be Brownian motion, a :
R →R+
0, b : R →R, two measurable functions and deﬁne the diﬀerence operator
∆ε f (t) = f (t + ε) −f (t), t ≥0. Given ε > 0 consider the equations
X(n+1)ε = Xnε + a(Xnε)(B(n+1)ε −Bnε) + b(Xnε)ε,
(7.58)
n ≥0. For a given starting value X0 = x0 and known a, b, these equations have a
unique solution that can be shown to be a Markov chain. The solution is calculated
recursively and it depends on Brownian motion. Equation (7.58) can be rewritten
using the diﬀerence operator as
∆εXt = a(Xt)∆Bt + b(Xt)∆εt,
(7.59)
for t = nε, n ≥0. Equation (7.59) is known as a stochastic diﬀerence equation with
coeﬃcients a and b.

BUILDING ON MARTINGALES AND BROWNIAN MOTION
325
A generalization of (7.59) is considered next.
Remark 7.28 (Stochastic diﬀerential equations as stochastic integrals) Assume
that B = {Bt = (Bt1, . . . , Btp), Ft; t ≥0} is p-variate Brownian motion over [0, +∞)
in a probability space (Ω, A, P) with F chosen appropriately and let X be a p-
variate stochastic process with continuous paths which is at a minimum, adapted to
a ﬁltration built using F and progressively measurable. Consider Borel measurable
functions aij, bi : R+
0 × Rp →R, i = 1, 2, . . ., p, j = 1, 2, . . ., r and deﬁne the
drift vector b(t, x) = (b1(t, x), . . ., bp(t, x))T and the dispersion matrix A(t, x) =
[(aij(t, x))]p×r. Our goal in this section is to study SDEs of the form
dXt = A(t, Xt)dBt + b(t, Xt)dt,
(7.60)
with the starting condition X0 = ξ where the stochastic process X = {Xt =
(Xt1, . . . , Xtp), Ft; t ≥0} assumes the role of a solution (in some sense) of the SDE,
whereas, the drift vector and the dispersion matrix are treated as the coeﬃcients of
the SDE. The random variable ξ = (ξ1, . . . , ξp) has distribution Qξ and is assumed
to be independent of B. Since the sample paths of B are nowhere diﬀerentiable,
(7.60) has only notational usefulness. Thinking in terms of ordinary calculus one
can write the SDE equivalently as
Xt = ξ +
tR
0
A(s, Xs)dBs +
tR
0
b(s, Xs)ds,
(7.61)
t ≥0. The stochastic integral equation (SIE) holds a.s. [P] and in order to be able to
deﬁne it we need any solution Xt to be adapted to the ﬁltration F or an augmented
ﬁltration based on F . Moreover, if X is a solution to (7.61) then it automatically
has continuous paths by construction of the stochastic integral.
Development and examples of SDEs along with the proofs of existence and
uniqueness of strong and weak solutions and their properties (e.g., strong Markov)
can be found in Karatzas and Shreve (1991), Durrett (1996), Durrett (2010), Bass
(2011) and Klenke (2014). The choice of ﬁltration we use leads to diﬀerent solu-
tions. A strong solution X requires a stronger measurability condition.
A SDE can be reformulated as a MG problem and the solution is equivalent to
constructing a weak solution. Pathwise uniqueness can be used to connect existence
and uniqueness of weak and strong solutions (see Klenke, 2014, p. 601). Next we
collect the deﬁnitions for completeness along with some illustrative examples.
Deﬁnition 7.20 Strong solution
Consider the setup of remark 7.28. Assume that the following conditions hold:
(a)
tR
0

|bi(s, Xs)| + a2
ij(s, Xs)

ds < ∞a.s. [P], for all i = 1, 2, . . ., p, j = 1, 2, . . ., r,
t ≥0,
(b) X is a stochastic process that satisﬁes (7.61) a.s. or equivalently in terms of its

326
STOCHASTIC PROCESSES
coordinates
Xti = ξi +
pP
j=1
tR
0
aij(s, Xs)dBs j +
tR
0
bi(s, Xs)ds,
(7.62)
holds a.s., for all i = 1, 2, . . ., p,
(c) P(X0 = ξ) = 1, and
(d) F = F B, the minimal ﬁltration for B, deﬁne the augmented minimal ﬁltration
Ht = σ

σ(ξ) ∪F B
t

and using the null sets
NΩ= {N ⊆Ω: ∃H ∈H∞, with N ⊆H and P(G) = 0} ,
further augment the ﬁltration by deﬁning Gt = σ(Ht ∪NΩ), t ≥0, so that (Bt, Ht)
or (Bt, Gt) is Brownian motion and G = {Gt : t ≥0} satisﬁes the usual conditions.
Then the solution X is called a strong solution of (7.60) if it is adapted to G.
Further assume that the drift vector and the dispersion matrix are given. A strong
solution X is said to satisfy strong uniqueness if for any other strong solution X′
t of
the SDE we have P(Xt = X′
t, t ≥0) = 1.
The deﬁnition of a weak solution follows.
Deﬁnition 7.21 Weak solution
Consider the setup of remark 7.28 and assume that conditions (a) and (b) of def-
inition 7.20 hold. In addition, assume that F satisﬁes the usual conditions so that
(B, F ) is Brownian motion and (X, F ) is a continuous, adapted stochastic process.
Then the triple {(X, B), (Ω, A, P), F } is called a weak solution of (7.60) and Qξ
is called the initial distribution of the solution. We have two types of uniqueness
properties.
1. Pathwise uniqueness A weak solution {(X, B), (Ω, A, P), F } is said to satisfy
pathwise uniqueness if for any other weak solution {(X′, B), (Ω, A, P), F ′} of the
SDE with P(X0 = X′
0) = 1 we have P(Xt = X′
t, t ≥0) = 1.
2. Distributional uniqueness A weak solution {(X, B), (Ω, A, P), F } is said to
satisfy distributional uniqueness if for any other weak solution {(X′, B′),
(Ω′, A′, P′), F ′} of the SDE with the same initial distribution QX0 = QX′
0, the two
processes X and X′ have the same distribution.
There are several approaches that we can take in trying to solve SDEs, from
applying the Itˆo and Girsanov formulas to solving a MG problem. For an excellent
detailed exposition see Karatzas and Shreve (1991, Chapter 5). We collect some
comments below before we discuss some classic examples.
Remark 7.29 (Solving stochastic diﬀerential equations) We note the following.
1. Cross-variation Let Σ(s, Xs) = A(s, Xs)A(s, Xs)T = [(Σij(s, Xs))]p×p. The cross-

BUILDING ON MARTINGALES AND BROWNIAN MOTION
327
variation process of a solution X to the SDE (7.60) is given by
D
Xi, X j
E
t =
pP
k=1
tR
0
aik(s, Xs)a jk(s, Xs)ds =
tR
0
Σij(s, Xs)ds.
(7.63)
2. Local martingale problem We say that a p-variate stochastic process X de-
ﬁned on a probability space (Ω, A, P) is a solution to the local MG problem for
Φ =
h
Φij(s, Xs)
i
p×p and v = [(vi(s, Xs))]p×1 (denoted by LMG(Φ, v)) if the stochas-
tic process
Mti = Xti −
tR
0
vi(s, Xs)ds,
is a continuous local MG with a cross-variation process
D
Mi, M j
E
t =
tR
0
Φij(s, Xs)ds,
for all i, j = 1, 2, . . ., p, t ≥0. The solution is unique if for any other solution X′
we have QX = QX′. The local MG problem is said to be well posed if there exists a
solution and it is unique.
3. Weak solutions via the local martingale problem The stochastic process X is
a solution of LMG(AAT, b) if and only if there exists a Brownian motion B such that
{(X, B), (Ω, A, P), F } is a weak solution of (7.60) (where F is an appropriate ﬁltra-
tion, see proposition 4.6 of Karatzas and Shreve, 1991). Moreover, if LMG(AAT, b)
has a unique solution then there exists a unique weak solution to the SDE (7.60)
that is strong Markov. Results on existence and uniqueness of the local MG prob-
lem can be found in Karatzas and Shreve (1991), Bass (2011) and Klenke (2014).
Example 7.18 (Counterexample) A strong solution is also a weak solution but
the existence of a weak solution does not imply that of a strong solution. Similarly,
if pathwise uniqueness holds then distributional uniqueness holds but the converse
is not true. Indeed, consider the SDE
dXt = sign(Xt)dBt,
t ≥0, where sign(x) = 2I(x > 0) −1. Then it can be shown that there exists a weak
solution but no strong solutions (see Karatzas and Shreve, 1991, p. 302).
Example 7.19 (Ornstein-Uhlenbeck (O-U) process) The O-U process is the so-
lution to the SDE
dXt = adBt −bXtdt,
for X0 = ξ, a, b > 0 and it can be used to model ﬂuid dynamics, meaning that Xt
models the velocity of a small particle suspended in a ﬂuid. The stochastic diﬀer-
ential adBt represents changes in velocity as the particle moves and it hits the ﬂuid
molecules and −bXtdt models a friction eﬀect. This SDE is tractable and it can be
solved using the Itˆo formula. In particular, multiply both sides by ebt and use the
Itˆo formula to obtain
d

ebtXt

= ebtdXt + bebtXtdt = aebtdBt,

328
STOCHASTIC PROCESSES
so that
ebtXt = ξ + a
tR
0
ebsdBs,
which leads to
Xt = e−btξ + ae−bt
tR
0
ebsdBs.
Now if ξ ∼N(0, σ2) then it can be shown that X is a Gaussian process with mean
0 and covariance function
C(s, t) = σ2e−b(s+t) + a2
2b

e−b|s−t| −e−b|s+t|
.
Choosing σ2 = a2
2b we obtain C(s, t) = σ2e−b|s−t| which yields an isotropic covari-
ance structure.
Example 7.20 (Brownian bridge) Consider the one-dimensional equation
dXt = [(b −Xt)/(T −t)] dt + dBt,
0 ≤t ≤T, with X0 = ξ, for given T > 0, ξ and b constants. Using the Itˆo formula
we can verify that
Xt = ξ(1 −t/T) + bt/T + (T −t)
tR
0
(T −s)−1dBs,
is a weak solution that satisﬁes pathwise uniqueness.
Ordinary partial diﬀerential equations play a pivotal role in describing phe-
nomena deterministically, especially in physics. The following remark summarizes
some of the ideas involved.
Remark 7.30 (Stochastic partial diﬀerential equations) The solutions of many
classic elliptic and parabolic ordinary partial diﬀerential equations (OPDEs) can
be represented as expectations of stochastic functionals in particular functionals of
Brownian motion. Introducing stochastic partial diﬀerential equations (SPDEs) is
accomplished via an application of the Itˆo formula. Recall the notation of remark
7.28 and further assume that Φ =
h
Φij(x)
i
p×p and v = [(vi(x))]p×1 have Borel mea-
surable elements and bounded, ∀x ∈Rp. Deﬁne a second-order partial diﬀerential
operator O for f ∈C2(Rp, R) by
(O f )(x, Φ, v) = 1
2
pP
i, j=1 Φij(t, x)∂2 f (x)
∂xi∂xj
+
pP
i=1 vi(x)∂f (x)
∂xi
,
(7.64)
and assume that the operator if uniformly elliptic, that is, there exists c > 0, such
that
pP
i, j=1 yiΦij(x)yj ≥c ∥y∥2
2 ,
for all x, y ∈Rp, which implies that Φ is positive deﬁnite, uniformly in x ∈Rp.
Uniform ellipticity has many welcomed consequences (see Bass, 2011, Chapter
40). For parabolic OPDEs an extension of (7.64) is required, namely, we deﬁne the

BUILDING ON MARTINGALES AND BROWNIAN MOTION
329
operator Ot in a similar fashion by
(Ot f )(x, Φ, v) = 1
2
pP
i, j=1 Φij(t, x)∂2 f (t, x)
∂xi∂xj
+
pP
i=1 vi(x)∂f (t, x)
∂xi
.
(7.65)
1. Assume that {(X, B), (Ω, A, P), F } is a weak solution of (7.60) and let
Σ(s, Xs) = A(s, Xs)A(s, Xs)T = [(Σij(s, Xs))]p×p. Using the Itˆo formula we can show
that the stochastic process Ml deﬁned by
Ml
t = f (Xt) −f (X0) −
tR
0
 ∂f (Xs)
∂s
+ (O f )(x, Σ, b)
!
ds
is a continuous local MG and a continuous MG if aij, bi are bounded on the support
of f . More importantly, we can verify that the stochastic process M deﬁned by
Mt = f (Xt) −f (X0) −
tR
0
(O f )(x, Φ, v)ds,
(7.66)
is a solution to LMG(AAT, b) and therefore of the SDE (7.60). Note that
(O f )(x, Σ, b) depends on s via the dispersion Σ(s, Xs) and drift b(s, Xs) coeﬃcients.
2. Building stochastic partial diﬀerential equations By construction the opera-
tor O contains partial derivatives of the second order for any f ∈C2(Rp, R) and it
appears in the weak solution of the SDE (7.60). Therefore, one can describe OPDEs
using the operator O on deterministic functions and then obtain their solution as an
expected value with respect to the weak solution to the corresponding SPDE. Some
examples are deﬁnitely in order.
Example 7.21 (Solving stochastic partial diﬀerential equations) For what fol-
lows assume that {(X, B), (Ω, A, P), F } is a weak solution of (7.60) with initial
condition X0 = ξ and let Qξ
X be the distribution of X.
1. Poisson stochastic partial diﬀerential equations Suppose that λ > 0, f
∈
C2(Rp, R) and g ∈C1(G, R), G ⊂Rp, has compact support. The Poisson SPDE
in Rp is given by
(O f )(x, Φ, v) −λ f (x) = −g(x),
(7.67)
x ∈Rp, where Φ and v are as in equation (7.64). Assume that f is a solution of
(7.67). By remark 7.30.1 and equation (7.66) we have
f (Xt) −f (X0) = Mt +
tR
0
(O f )(Xu, Φ, v)du,
where M is a MG and using integration by parts (equation 7.55, on the stochastic
processes e−λt and f (Xt)) we obtain
e−λt f (Xt) −f (X0) =
tR
0
e−λudMu +
tR
0
e−λu(O f )(Xu, Φ, v)du −λ
tR
0
e−λu f (Xs)ds.
Finally, taking expectation on both sides above with respect to Qξ
X and sending

330
STOCHASTIC PROCESSES
t →∞leads to
−f (ξ) = EQξ
X

+∞
R
0
e−λu (O f )(Xu, Φ, v) −λ f (Xs) du
,
and since (O f )(x, Φ, v) −λf (x) = −g(x) the solution of (7.67) can be written as
f (ξ) = EQξ
X

+∞
R
0
e−λug(Xu)du
.
2. Dirichlet stochastic partial diﬀerential equations The Laplacian operator OL
is obtained from equation (7.64) by setting Φij(t, x) = 2δij, vi(x) = 0, δij = I(i = j),
for all i, j = 1, 2, . . . , p, i.e., (OL f ) (x) =
pP
i=1
∂2 f(x)
∂x2
i . The Dirichlet SPDE (or Dirichlet
problem) is deﬁned using the Laplacian operator as follows; given an open ball
D = b(c, r) in Rp (or disc for p = 2), r > 0 and g is a continuous function on the
ball boundary ∂b(c, r), we want to ﬁnd f such that f ∈C2(b(c, r), R) and
(OL f )(x) = 0,
(7.68)
x ∈b(c, r), subject to the initial condition f (x) = g(x), x ∈∂b(c, r) with g bounded.
When such function f exists it is called the solution to the Dirichlet problem (D, g).
We can think of f (x) as the temperature at x ∈b(c, r), when the boundary tempera-
tures ∂b(c, r) are speciﬁed by g. In order to solve this equation we need to bring in
a Brownian motion B explicitly, that is, let τD = inf {t ≥0 : Bt ∈Dc} the ﬁrst time
that B exits the ball D and since D is a bounded open set, τD < ∞a.s. (for a proof
see Durrett, 1996, p. 95). Further deﬁne Dn = {x ∈D : inf
y∈∂D{∥x −y∥} > 1/n}. By
equation (7.66) we have
f (Xt∧Dn) −f (X0) = Mt +
t∧Dn
R
0
(OL f )(Xu)du.
Since (OL f )(x) = 0, for x ∈b(c, r), taking expectations with respect to Qξ
X yields
f (a) = EQξ
X f (Xt∧Dn) ,
a ∈Dn and then sending t →∞ﬁrst and then n →∞we have that f (Xt∧Dn)
converges to g(XτD) a.s. Finally, applying the DCT we pass the limit under the
expectation and the solution is given by
f (ξ) = EQξ
X g(XτD) , ξ ∈D.
3. Cauchy stochastic partial diﬀerential equations This is a parabolic OPDE
given by
∂
∂t f (t, x) = (Ot f )(x, Φ, v),
(7.69)
where now f is a function of t ∈R+
0 and x ∈Rp. The Cauchy problem involves
ﬁnding an f that (i) satisﬁes (7.69), (ii) is bounded, (iii) f is twice continuously
diﬀerentiable with bounded ﬁrst and second partial derivatives with respect to x ∈
Rp, (iv) f is continuously diﬀerentiable with respect to t > 0, and (v) f satisﬁes the
initial conditions f (0, x) = g(x), x ∈Rp, where g is a continuous function of x ∈Rp

SUMMARY
331
with compact support. Using the multivariate Itˆo formula of equation (7.57) we can
show that the solution is given by
f (t, ξ) = EQξ
X g(Xt) ,
(7.70)
t > 0, x ∈Rp. An important special case of (7.69) is the heat equation. Consider an
inﬁnite rod extended along the x-axis of the (t, x)-plane. At t = 0 the temperature
of the rod is g(x) at location x ∈R. If f (t, x) denotes the temperature of the rod at
time t ≥0 and location x ∈R then f satisﬁes the heat equation if
∂
∂t f (t, x) = 1
2
∂2
∂x2 f (t, x),
(7.71)
with initial condition f (0, x) = g(x), x ∈Rp and it can be solved in terms of
the transition density of the one-dimensional Brownian motion (see Karatzas and
Shreve, 1991, Section 4.3).
7.6
Summary
We have developed theory for continuous time parameter stochastic processes,
such as Markov processes, continuous-time MGs, the Poisson process and Brow-
nian motion, and based on these processes, we collected elements of stochastic
calculus. Standard and specialized texts in these topics include Feller (1968, 1971),
Karatzas and Shreve (1991), Karr (1991), Borodin and Salminen (1996), Durrett
(1996, 2004, 2010), Fristedt and Gray (1997), Dudley (2002), C¸ inlar (1975, 2010),
Lawler and Limic (2010), Bass (2011), Billingsley (2012) and Klenke (2014).
Building on the theory of stochastic processes will allow us to model random ob-
jects over time and in particular, as we see in the TMSO-PPRS text, point processes
and random sets as they evolve over time. For a historical account of stochastic pro-
cesses see Dellacherie and Meyer (1978). We collect some complementary ideas on
topics from this chapter below.
Brownian motion
Brownian motion is named after Robert Brown, a botanist, who was the ﬁrst to
utilize it in the 1820s in order to model the erratic motion of particles in suspension.
The rigorous foundations are due to Norbert Wiener and his work in the 1920s,
where he proved the existence of the distribution of Brownian motion for the ﬁrst
time. A historical account and references can be found in Karatzas and Shreve
(1991, pp. 126-127).
Continuous-time martingales
An extension of Doob’s decomposition theorem to the continuous time param-
eter case (Doob-Meyer decomposition theorem) was given by Meyer in the 1960s.
Applications of MGs to ﬁnancial modeling can be found in Musiela and Rutkowski
(2005). See Karatzas and Shreve (1991, p. 46) for a historical account and refer-
ences on continuous-time MGs.
Filtering

332
STOCHASTIC PROCESSES
Stochastic calculus results have found many applications in engineering and
signal processing via the method of stochastic (linear) ﬁltering. For example, the
Kalman-Bucy ﬁlter arises as the solution to the SDE
dXt
=
A(t)dB(1)
t
+ B(t)Xtdt,
dYt
=
dB(2)
t
+ C(t)Xtdt,
where A, B and C are p × p deterministic matrices, with elements continuous in
t, B(1)
t , and B(2)
t
are independent Brownian motions with Xt and B(2)
t
independent.
See Øksendal (2003) and Bass (2011, Chapter 29) for more details on ﬁltering and
stochastic calculus.
Stochastic (partial) diﬀerential equations
Independent of Itˆo’s work, Gihman developed a theory of SDEs in the late
1940s. For additional details and references on the theoretical foundations based
on Itˆo’s formulation see Karatzas and Shreve (1991, pp. 394-398), Durrett (1996)
and Bass (2011). For a connection between probability theory and SPDEs see Bass
(1997). Moreover, SPDEs can be used to model spatio-temporal data as illustrated
in Cressie and Wikle (2011, Chapter 6 and the references therein). Additional de-
tails on SPDEs can be found in Øksendal (2003) and for PDEs the interested reader
can turn to the text by Jost (2013).
Stochastic integration
The concept of a stochastic integral was ﬁrst introduced by Paley, Wiener and
Zygmud in the early 1930s for deterministic integrands and then by Itˆo in the 1940s
for integrands that are stochastic processes. Doob was the ﬁrst to study stochastic
integrals via MG theory in the 1950s and provided a uniﬁed treatment of stochastic
integration. More details and references can be found in Karatzas and Shreve (1991,
pp. 236-238), Durrett (1996), Bass (2011) and Klenke (2014).
7.7
Exercises
Poisson process
Exercise 7.1 Assume that Nt is a Poisson process deﬁned via deﬁnition 7.3. Show
the following:
(i) P(Nt = 0) = e−λt, t ≥0, for some constant λ ≥0.
(ii) lim
t↓0
1
t P(Nt ≥2) = 0.
(iii) lim
t↓0
1
t P(Nt = 1) = λ, for the same constant λ as in (i).
(iv) Nt ∼Poisson(λt), t ≥0, for the same constant λ as in (i).
Exercise 7.2
For any Poisson process with intensity λ show that Nt+s −Nt|{Nu :
u ≤t}
d= Nt+s −Nt
d= Ns.
Exercise 7.3 Prove the statements of remark 7.1, parts 5-7.
Exercise 7.4 Let T = {Tk : k = 1, 2, . . .} denote the stochastic process of succes-
sive instants of arrivals of a Poisson process N ∼PP(λ).

EXERCISES
333
(i) Show that the interarrival times T1, T2 −T1, T3 −T2, . . . , are iid Exp(λ).
(ii) Find the distribution of Tk.
(iii) For any measurable function f ≥0, show that
E
 +∞
P
n=1 f (Tn)
!
= λ
+∞
R
0
f (t)dt.
Exercise 7.5
Consider a non-stationary Poisson process (property (P3) of deﬁni-
tion 7.3 is removed), let a(t) = E(Nt), t ≥0, and deﬁne the time inverse of a(.) as
the function τ(t) = inf{s : a(s) > t}, t ≥0.
(i) Show that in general a(.) is non-decreasing and right continuous. For the remain-
ing parts assume that a(.) is continuous.
(ii) Deﬁne the process Mt(ω) = Nτ(t)(ω), t ≥0. Show that M = {Mt : t ≥0} is a
stationary Poisson process with intensity 1.
(iii) Show that
P(Nt+s −Nt = k) = e−(a(t+s)−a(t))(a(t + s) −a(t))k/k!, ∀t, s ≥0.
(iv) If T1, T2, . . . , are the successive arrival times, show that ∀n = 1, 2, . . . and t ≥0,
we have P(Tn+1 −Tn > t|T1, . . . , Tn) = e−(a(Tn+t)−a(Tn)).
Polish spaces and continuous time ﬁltrations
Exercise 7.6 Recall example 7.4.
(i) Show in detail that d(x, y), ρp(x, y) and ρ∞(x, y) are metrics in R, Rp and R∞,
respectively.
(ii) Prove that (R, d), (Rp, ρp) and (R∞, ρ∞) are Polish spaces.
Exercise 7.7 Prove all statements of remark 7.3.
Exercise 7.8 Show that if τ and ξ are two stopping times with respect to a ﬁltration
F = {Ft, t ≥0} and τ ≤ξ then Fτ ⊆Fξ .
Exercise 7.9
Let τ and ξ be two stopping times with respect to a ﬁltration F =
{Ft, t ≥0} and Y an integrable random variable in a probability space (Ω, A, P).
Show that
(i) E [Y|Fτ] = E
h
Y|Fτ∧ξ
i
a.s. [P] on the set {ω : τ(ω) ≤ξ(ω)},
(ii) E
h
E [Y|Fτ] |Fξ
i
= E
h
Y|Fτ∧ξ
i
a.s. [P].
Exercise 7.10
Give an example of a stochastic process X with minimal ﬁltration
F X that is not right continuous.
Exercise 7.11 Prove all the statements of remark 7.4.7.
Exercise 7.12 Show that (DR
[0,+∞), ρC) is a Polish space with ρC deﬁned in (7.9).
Markov processes
Exercise 7.13
Show that the Poisson process Nt (based on deﬁnition 7.3) is a
Markov process that is an adapted, integer-valued RCLL process such that N0 = 0
a.s. and for 0 ≤s < t, Nt −Ns is independent of F N
s and it is Poisson distributed
with mean λ(t −s).

334
STOCHASTIC PROCESSES
Exercise 7.14
Given a Poisson process N with intensity λ > 0, adapted to the
minimal ﬁltration F N, deﬁne the compensated Poisson process by Mt = Nt −λt, for
all t ≥0 and adapted to some ﬁltration F . First show that F N = F and then prove
that (M, F ) is a MG.
Exercise 7.15
Show that the Borel σ-ﬁeld over CR
[0,1], the space of continuous,
R-valued functions deﬁned on [0, 1], is the same as the σ-ﬁeld generated by the
ﬁnite dimensional sets (cylinders) {φ ∈CR
[0,1] : (φ(t1), . . . , φ(tn)) ∈B} for Borel sets
B ∈Bn, n ≥1.
Exercise 7.16 Prove theorem 7.4.
Exercise 7.17 (Dynkin’s formula) Let X be a Markov process with generator G
and let ξ ≤τ be a.s. ﬁnite stopping times with respect to F X. Show that
E(f (Xτ) −f (Xξ)) = E

τR
ξ
G f (Xu)du
,
for any continuous function f in the domain of G.
Exercise 7.18 Prove theorem 7.5.
Exercise 7.19
Give an example of a Markov process that is not strong Markov.
Hint: take E = R+ and deﬁne X0 = 0, Xt = (t −T)+, t > 0, where T ∼Exp(c)
denotes the sojourn at state 0 (see C¸ inlar, 2010, p. 449).
Exercise 7.20 Consider a Markov chain with state space Ψ = {1, 2} and generator
A =

−µ
µ
λ
−λ
,
where λµ > 0.
(i) Give the form of Kolmogorov’s forward equations and solve them to obtain the
transition function pt(x, y), x, y ∈Ψ.
(ii) Find An and use it in equation (7.41). Compare the result with that of part (i).
(iii) Find the stationary distribution π using theorem 7.10 and verify that pt(x, y) →
π(y), as t →∞.
Markov processes in discrete state space
Exercise 7.21 Prove the statements of remark 7.14, parts 1-2 and 4-6.
Exercise 7.22
Assume that X is a discrete-valued Markov process and with a
standard transition function pt(x, y). Show that the transition function t 7→pt(x, y),
for ﬁxed x, y ∈Ψ, is continuous everywhere.
Exercise 7.23 Prove the statement of remark 7.15.1.
Remark 7.31 Show that if Ψ is ﬁnite then no state is instantaneous.
Remark 7.32 Prove all the regularity criteria of remark 7.2.
Remark 7.33 Prove that the sample paths of a Markov process X over a discrete
state space Ψ are right continuous if and only if there are no instantaneous states.
Exercise 7.24 Prove theorem 7.6.

EXERCISES
335
Exercise 7.25
For a regular Markov process X and any x, y ∈Ψ and t ≥0, show
that
pt(x, y) = e−λ(x)tI(x = y) +
tR
0
λ(x)e−λ(x)s P
z∈Ψ q(x, z)pt−s(z, y)ds.
Exercise 7.26 Prove theorem 7.8.
Exercise 7.27 Prove all statements of remark 7.17 in succession.
Brownian motion and the Wiener measure
Exercise 7.28 Prove all statements of remark 7.19.
Exercise 7.29 Let B ∼BM([0, +∞)) with B0 = 0. Show that
P(Bs > 0, Bt > 0) = 1
4 + 1
2π arcsin
 r
s
t
!
,
for 0 < s < t.
Exercise 7.30 Let µ1 denote the Lebesgue measure on [0, 1]. Show that
Qw({φ : µ1({t ∈[0, 1] : φ(t) = 0}) = 0}) = 1.
Exercise 7.31 Let {Qn}+∞
n=0 be deﬁned as in remark 7.20. Show (sequentially) that
(i) for c > 0 and k = 0, 1, 2, . . .,
lim
m→+∞lim sup
n→+∞
 
m sup
k≤mt≤k+1
(
Qn
 (
φ :
φ(t) −φ
 k
m
! > c
)!)!
= 0,
(ii) for c > 0 and k = 0, 1, 2, . . .,
lim
m→+∞lim sup
n→+∞
 
mQn
 (
φ :
sup
k≤mt≤k+1
(φ(t) −φ
 k
m
!
)
> c
)!!
= 0,
(iii) for c > 0
lim
m→+∞lim sup
n→+∞
 
Qn
 (
φ : sup
m|t−s|≤1
{|φ(t) −φ (s)|} > c
)!!
= 0.
Exercise 7.32
Let {Qn}+∞
n=0 be deﬁned as in remark 7.20. Show that {Qn}+∞
n=0 is
uniformly tight.
Exercise 7.33 Prove theorem 7.11.
Exercise 7.34 Let Ha = inf{t ≥0 : Bt = a} be the ﬁrst time a Brownian motion Bt
hits the state a ∈R.
(i) Show that {Ha : a ≥0} has independent and stationary increments.
(ii) Prove the reﬂection principle for Brownian motion, that is, for a > 0
P(Ha ≤t, Bt < a) = P(Ha ≤t, Bt > a).
(iii) Show that P(Ha ≤t) = 2P(Bt > a).
Exercise 7.35
Let Bt be Brownian motion and deﬁne Ha,b = inf{t > 0 : Bt <
[−a, b]}, a, b > 0. Show that P(BHa,b = −a) = b/(a + b) = 1 −P(BHa,b = b) and
E(Ha,b) = ab.

336
STOCHASTIC PROCESSES
Exercise 7.36
Let S = (S 0, S 1, . . . ), S 0 = 0, be a random walk in R with steps
that have mean 0 and ﬁnite positive variance. Show that
lim
n→+∞P
 1
n#{k ≤n : S k > 0} ≤c
!
= 2
π arcsin √c,
for 0 ≤c ≤1.
Exercise 7.37 Prove theorem 7.12.
Exercise 7.38 (L´evy process and Blumenthal 0-1 law) A L´evy process L is a
generalization of a Wiener process. In particular, L is a DR
[0,+∞)-valued random vari-
able deﬁned on (Ω, A, P), such that L0 = 0 a.s. and for any 0 = t0 < t1 < · · · < tn
and Borel sets A j ∈B1, j = 1, 2, . . ., n, we have
P(Ltj −Ltj−1 ∈A j, j = 1, 2, . . ., n) =
nQ
j=1 P(Ltj−tj−1 ∈A j).
If {Ft+ : t ≥0} denotes the right-continuous minimal ﬁltration of a L´evy process L
in R then show that F0+ is 0-1 trivial.
Stochastic integration and diﬀerential equations
For the following exercises assume that B ∼BM([0, +∞)).
Exercise 7.39 Prove all statements of remark 7.24.3.
Exercise 7.40
Assume that τ is a ﬁnite stopping time adapted to F B. Show that
+∞
R
0
I[0,τ]dBt = Bτ.
Exercise 7.41 (Kunita-Watanabe inequality) Assume that X, Y ∈Mc
2. Show that
⟨X, Y⟩t ≤(⟨X⟩t)1/2 (⟨Y⟩t)1/2 a.s.
Exercise 7.42 Verify that the linear SDE
dXt = aXtdBt + bXtdt,
with initial condition X0 = x0, where a, b ∈R, are constants, has the solution given
by Xt = x0 exp
n
aBt + (b −a2/2)t
o
.
Exercise 7.43
Show in detail that equation (7.70) is the solution of the Cauchy
problem.

Appendix A
Additional Topics and Complements
A.1
Mappings in Rp
Remark A.1 (Maps and functions in Rd) By writing “f is a mapping (or map)
from X to Y” we express the fact that f : X →Y, i.e., f is a function that assigns
elements of X to elements Y and f takes values in the (target) set f (X) = {y ∈Y :
∃x such that y = f (x)}. The term function is used for maps in Rd.
1. The image of A ⊂X through a map f is f (A) = {y ∈Y : ∃x such that [x ∈A
and y = f (x)]}.
2. The inverse image of B is the set f −1(B) = {x ∈X : f (x) ∈B} and exists always
even if the inverse map f −1 does not exist.
3. The map f is called “one to one” (1:1) when f (x1) = f (x2) ⇐⇒x1 = x2,
∀x1, x2 ∈X. The map is called “onto” when f (X) = Y.
4. The map f is called bijective when it is 1:1 and onto. In this case there exists
g : Y →X, with g(f (x)) = x and f (g(y)) = y, x ∈X, y ∈Y. The map g is called
the inverse of f and is denoted by f −1.
5. If f : X →Y and g : Y →Z then deﬁne h : X →Z, by h(x) = g(f (x)), x ∈X,
the composition of f and g, denoted by h = f ◦g.
6. If f : X →Y then the map g : A →Y, deﬁned as g(x) = f (x), ∀x ∈A, is called
the restriction of f to A and is denoted by f |A.
7. Positive and negative parts Now consider f : Ω→R and note that it can
be written in terms of its positive and negative part as f
=
f + −f −,where
f +(ω)
=

f (ω),
if 0 ≤f (ω) ≤+∞
0,
if −∞≤f (ω) ≤0 , the positive part of f and f −(ω)
=

−f (ω),
if −∞≤f (ω) ≤0
0,
if 0 ≤f (ω) ≤+∞, the negative part of f. Note that f + and f −are non-
negative and | f | = f + + f −≥0.
337

338
ADDITIONAL TOPICS AND COMPLEMENTS
8. Taylor expansion If f : Rp →R and if ∇2 f =
h ∂2 f(x)
∂xi∂xj
i
is continuous on the
sphere {x : |x −x0| < r} then for |t| < r
f (x0 + t) = f (x0) + ∇f (x0)t + tT

1R
0
1R
0
v∇2 f (x0 + uvt)dudv
t.
9. Mean value If f : Rp →Rk and if ∇f is continuous on the sphere {x : |x−x0| <
r} then for |t| < r
f (x0 + t) = f (x0) +

1R
0
∇f (x0 + ut)du
t.
10. Bounded variation Consider a partition Ξ = {a = ξ0 < ξ1 < · · · < ξk = b}
of the bounded interval [a, b] and for any R-valued function f over [a, b] deﬁne
∥f ∥Ξ =
kP
i=1
 f (ξi) −f (ξi−1)
 . The function f is said to be of bounded variation over
[a, b] if sup
Ξ
∥f ∥Ξ < ∞. If f is absolutely continuous then it is of bounded variation.
A.2
Topological, Measurable and Metric Spaces
Deﬁnition A.1 Topological space
A topological space (X, O), where X is some space and O is a collection of sets
from X, is a pair satisfying (i) ∅∈T and X ∈O, (ii) O is a π−system and (iii) O is
closed under arbitrary unions. We say that the space O is a topology on X or that O
induces a topology on X, with the members of O treated as the “open” sets, while
their complements are called “closed” sets. A subfamily O0 ⊂O is called the base
of the topology if each open set O ∈O can be represented as a union of sets from
O0. A sub-base of a topology is a subfamily of sets O1 ⊂O such that their ﬁnite
intersections form the topology base O0.
Remark A.2 (Topological spaces) Assume that (X, O) is a topological space.
1. A subset A of X is compact if every open cover {Oi}+∞
i=1 of A (i.e., A ⊆
+∞
S
i=1 Oi,
with Oi open) has a ﬁnite subcovering (i.e., A ⊆
nS
i=1 Oai, for some indices ai). If
every set in 2X is compact then the space X is compact.
2. A neighborhood of a point in a topological space is any set that contains some
open set of which the point is a member.
3. Hausdorﬀspace A topological space is Hausdorﬀif for any two points x and y
in X there exist neighborhoods of x and y that have empty intersection.
4. A topological space X is locally compact if for any x ∈X there exists an open
set O containing x such that O is compact, where O denotes the closure of O, which
is its smallest closed superset.

TOPOLOGICAL, MEASURABLE AND METRIC SPACES
339
5. The empty set ∅and the whole space X are open sets but also closed since
∅c = X and Xc = ∅.
Remark A.3 (Borel σ-ﬁelds) We collect some examples and important results on
Borel and topological spaces from real analysis.
1. In any topological space, all closed sets, being the complements of open sets,
are Borel sets. We could have Borel sets that are neither open nor closed. The
intersection of countably many open sets is Borel but may be neither open nor
closed. So what does a generic Borel set look like? Since B ∈B = σ(O) then B can
be written in terms of arbitrary unions, intersections and complements of open sets
and in view of theorem 3.2, open intervals.
2. The real line, R, is a topological space, e.g., using the topology induced from
the usual metric ρ(x, y) = |x−y|. Standard topological spaces of interest in classical
topology include the positive real line R+ = {x ∈R : x > 0}, the nonnegative real
line R+
0 = {x ∈R : x ≥0}, the p-dimensional real line Rp, the inﬁnite dimensional
real line R∞, the extended real line R = R ∪{−∞}∪{+∞} and the positive extended
real line R
+, using appropriately deﬁned metrics to induce the topology.
3. All intervals in R are Borel sets. In particular, (a, b) is an open ball
b((a + b) /2, (b −a) /2) = {x ∈R : |x −(a + b) /2| < (b −a) /2}, which is an
open set and hence Borel. Moreover, the intervals [a, b) =
+∞
T
n=1(a −1/n, b), (a, b] =
+∞
T
n=1(a, b+1/n), [a, b] =
+∞
T
n=1(a−1/n, b+1/n), (−∞, b) =
+∞
S
n=1(−n, b), (a, +∞) =
+∞
S
n=1(a, n),
R = (−∞, +∞) =
+∞
S
n=1(−n, n) and the singleton {a} =
+∞
T
n=1(a −1/n, a + 1/n) are Borel
sets as operations on countable collections of open (Borel) sets. Countable sets like
the set of rational numbers Q are Borel sets.
4. Every open subset O of R can be written as a countable disjoint union of open
intervals, that is, O =
+∞
S
n=1(an, bn), with an < bn, real numbers.
5. Consider any collection of open sets C in R. Then there exists a countable sub-
collection {On}+∞
n=1 of C, such that S
O∈C O =
+∞
S
n=1 On.
6. A mapping f of a topological space (X, T ) into a topological space (Y, O) is
said to be continuous if the inverse image of every open set is open, that is, if
O ∈O =⇒f −1(O) ∈T . We say that the map f is continuous at a point x0 ∈X, if
given any open set O ∈O, containing f (x0), there is an open set T ∈T , containing
x0 such that f (T) ⊂O. The map f is continuous if and only if it is continuous at
each point of X.
Remark A.4 (Measurable spaces) Note the following.

340
ADDITIONAL TOPICS AND COMPLEMENTS
1. For any space Ω, since 2Ωis a σ-ﬁeld, (Ω, 2Ω) is a measurable space, in fact the
largest possible we can build on Ω. So what is the point of looking for the smallest
σ-ﬁeld generated by some collection of sets of the space Ω, when we have already a
wonderful σ-ﬁeld 2Ωto work with where every set is measurable? The σ-ﬁeld 2Ωis
in general very large, so large that it can contain sets that will cause mathematical
and logical problems with the Axiom of Choice (AC) and the various forms of
the Continuum Hypothesis, two foundational items from modern set theory and
analysis (see Vestrup, 2003, p. 17).
The AC can be described as follows: given an arbitrary family of nonempty
sets we can form a new set consisting of one element from each set in the family.
Such logical arguments make sense and will be taken as true since if they are not
true, almost all our measure theoretic results are rendered invalid and we can build
probability theory, for example, that leads to probabilities of events being 110%.
These mathematical paradoxes manifest themselves when one deﬁnes measures
in order to attach numbers to each of the measurable sets, for example, length of
intervals in R is one such measure.
To further clarify this point, recall example 3.15 of a set that belongs to 2Ω
and hence is measurable in (Ω, 2Ω), but it is not Borel measurable in the measurable
space (Ω, B(Ω)) since in general B(Ω) ⊆2Ω. Another major reason we look for
generated σ-ﬁelds has to do with the cardinality of the σ-ﬁeld of the measurable
space (Ω, A), i.e., how many sets are there in A. For example, it is much easier to
assign measures to a (possibly ﬁnite or countable) collection of sets that generate
A rather than to all possible members of A directly. Clearly, it makes no sense to
say that a set is “measurable” when the measure theory we develop cannot assign a
measure to it! Such sets are naturally called nonmeasurable sets.
2. Two measurable spaces are called isomorphic if there exists a bijective function
ϕ between them such that both ϕ and ϕ−1 are measurable.
3. A measurable space is called a Borel space if it is isomorphic to some (A, B(A))
where A is a Borel set in [0, 1]. A product of a ﬁnite or countable number of Borel
spaces is a Borel space. Every measurable subset A of a Borel space B is itself a
Borel space.
The construction (or description) of the open sets of X, can also be accom-
plished by equipping the space X with a metric ρ and then describe the open sets
using this metric, thus, inducing a topology on X. However, a topological space as
deﬁned is a generalization of the topological space obtained via a metric.
Deﬁnition A.2 Metric spaces
A metric space consists of a set X and a function ρ : X × X →R+, satisfying
(i) ρ(x, y) = 0 if and only if x = y, (ii) ρ(x, y) = ρ(y, x) (symmetry), and (iii)

TOPOLOGICAL, MEASURABLE AND METRIC SPACES
341
ρ(x, z) ≤ρ(x, y) + ρ(y, z) (triangular inequality). The function ρ is called a metric,
while the metric space is denoted by (X, ρ).
Remark A.5 (Topology via a metric) Using the metric we can deﬁne, for all r > 0,
the sets b(x, r) = {y ∈X : ρ(x, y) < r}, b(x, r) = {y ∈X : ρ(x, y) ≤r} and
∂b(x, r) = {y ∈X : ρ(x, y) = r}, interpreted as the open ball, the closed ball and the
boundary of the ball, respectively, centered at x ∈X of radius r.
1. An open set B ⊂X is a set that satisﬁes: ∀x ∈B, ∃r > 0, such that the open ball
of radius r centered at x is a subset of B. A set is closed if its complement is open.
2. The interior of a set B ⊂X, denoted by BInt, is the largest open subset of B,
while the closure of C ⊂X, denoted by C, is its smallest closed superset. The
boundary of a set B, denoted by ∂B, is deﬁned by ∂B = B\BInt.
3. Heine–Cantor theorem If f : X →Y is a continuous function between two
metric spaces and X is compact then f is uniformly continuous, that is, ∀ε > 0,
∃δ > 0 : |x −y| < δ =⇒| f (x) −f (y)| < ε. The diﬀerence between uniformly
continuous and simply continuous f is that for uniform continuity the bound δ
depends only on ε and not the points x and y.
4. A subset B of a set C in a metric space is dense in C if every ball centered
at a point in C contains a member of B. A metric space is separable if it contains
a countable dense subset. The metric space is complete if every Cauchy sequence
converges to an element of the space.
5. A Polish space is a complete separable metric space. Every Polish space is a
Borel space.
6. Let (X1, ρ1) and (X2, ρ2) be two metric spaces and consider a map φ : X1 →X2.
Given a γ ∈(0, 1], we say that φ is locally H¨older-continuous of order γ at a point
x ∈X, if there exists ε > 0 and c > 0 such that, for any y ∈X, with ρ1(x, y) < ε, we
have
ρ2(φ(x), φ(y)) ≤cρ1(x, y)γ.
(A.1)
We say that φ is locally H¨older-continuous of order γ if for every x ∈X, there
exists ε > 0 and c = c(x, ε) > 0 such that, for all y, z ∈X with ρ1(x, y) < ε and
ρ1(x, z) < ε, equation (A.1) holds. Finally, φ is called H¨older-continuous of order
γ, if there exists a c > 0 such that (A.1) holds for all x, y ∈X.
Halmos (1950) developed measure theory based on rings, which are smaller
classes of sets than ﬁelds.
Deﬁnition A.3 Ring
Given a set Ω, a collection of sets A ⊂2Ωis called a ring if and only if (i) ∅∈A,

342
ADDITIONAL TOPICS AND COMPLEMENTS
and (ii) for all A, B ∈A we have A ∪B ∈A and B ∖A ∈A. A ring is a ﬁeld if and
only if Ω∈A.
A.3
Baire Functions and Spaces
Since we are often working with measures on a space that is also a topological
space then it is natural to consider conditions on the measure so that it is connected
with the topological structure, thus introducing additional properties. There seem
to be two classes of topological spaces where this can be accomplished: locally
compact Hausdorﬀspaces (LCHS, see remark A.2 parts 3 and 4) and complete
metric spaces (Rp enjoys both properties). Royden (1989, Chapters 13 and 15) and
Dudley (2002, Section 7.1) present results on Baire and Borel spaces, and introduce
the concept of regularity for measures and treat mappings of measure spaces. In
particular, locally compact Hausdorﬀseparable spaces (LCHSS) play a pivotal role
in studying random set theory; see for example Molchanov (2005).
When deﬁning the measurable space (Ω, A), the choice of the σ-ﬁeld A deter-
mines the types of events we can describe and deﬁne probabilities for. Similarly, in
deﬁning random objects X : (Ω, A, P) →(X, G) the mapping X and the measurable
space (X, G) determine the types of random objects we can deﬁne. In order to treat
the two problems we have studied Borel sets and functions throughout the book
and we deﬁned most of our random objects to be Borel measurable functions.
Certainly Borel functions make great candidates for random variables and vec-
tors but one naturally wonders what other classes of functions would be acceptable
as random variables. Baire functions allow us to deﬁne such well-behaved measur-
able maps and they coincide with Borel functions in Rp, so a sceptic would argue
that they are not that useful. But our purpose is to slowly move away from ran-
dom vectors in Rp and deﬁne random objects in more complicated spaces such as
the space of all Radon counting measures (point process objects) or the space of
compact-convex sets (random set objects). In many of these complicated spaces
what we knew about Rp does not work anymore and we require additional tools to
deﬁne and study probability and measurability of mappings. We collect the deﬁni-
tion of a Baire set next.
Deﬁnition A.4 Baire set
Assume that X is a locally compact Hausdorﬀspace, and let Cc(X) denote the
family of all continuous, real-valued functions that vanish outside a compact subset
of X. The class of Baire sets BX is deﬁned to be the smallest σ-ﬁeld of subsets of
X such that each function in Cc(X) is measurable with respect to BX.
Note that by deﬁnition every Baire set is also a Borel set, i.e., BX ⊆B(X), in
general. The converse is true when X is a locally compact separable metric space.
The deﬁnition of a Baire function follows.

FISHER INFORMATION
343
Deﬁnition A.5 Baire functions
The smallest family of functions that contains the continuous functions and is
closed with respect to pointwise limits is called the Baire class of functions and is
denoted by BC. The elements of BC are called Baire functions.
There are two conditions here; ﬁrst, if X ∈BC, then X is continuous, and sec-
ond, if Xn ∈BC and the limit lim
n→+∞Xn(x) = X(x) exists for all x ∈X, then X ∈BC.
The deﬁnition depends only on continuity and no other properties of the space X
and therefore it is applicable to any topological space. For Rp in particular, it can
be shown that BC coincides with the class of Borel measurable functions.
The importance of Baire functions as random objects in Baire spaces was seen
in example 4.2, parts 2 and 3 and deﬁnition 4.16. More precisely, in deﬁning ran-
dom objects we need to build a measurable map X from (Ω, A, P) into (X, G), that
is, X must satisfy X−1(G) ∈A, ∀G ∈G which occurs if X−1(G) ⊆A. We could also
deﬁne the probability space based on the smallest σ-ﬁeld with respect to which X
is measurable, since in general X−1(G) = σ(X) = {X−1(G) ⊂Ω, for all G ∈G} ⊆A,
so that X : (Ω, σ(X), P) →(X, G). Baire functions X possess this structure by deﬁ-
nition in a Baire space, namely, the Baire sets form the smallest σ-ﬁeld with respect
to which X is measurable. For more details on the development and applicability
to deﬁning random variables as Baire functions, the interested reader can turn to
Feller (1971).
A.4
Fisher Information
Deﬁnition A.6 Fisher information
The information contained in a vector X ∼f (x|θ) about a parameter θ ∈Θ ⊆Rp
is deﬁned as the matrix IF
x (θ) = [(Iij)] where
Iij = E
 ∂ln f (x|θ)
∂θi
∂ln f (x|θ)
∂θ j
!
.
(A.2)
If ∂2 ln f(x|θ)
∂θi∂θ j , i, j = 1, 2, . . ., p, exists and is ﬁnite then Iij = E

−∂2 ln f(x|θ)
∂θi∂θ j

.
Regularity conditions Let θ ∈Θ ⊆Rp and assume that the following hold.
1. Θ is an open interval in Rp.
2. The set {x : f (x|θ) > 0} is independent of θ.
3. For all θ ∈Θ the derivative ∂ln f(x|θ)
∂θ j
exists almost everywhere with respect to
x.
4. The quantity
'
D
nQ
i=1 f (xi|θ)dx can be diﬀerentiated with respect to θ inside
the integral for any set D.
5. IF
x (θ) =
h
E
∂ln f(x|θ)
∂θi
∂ln f(x|θ)
∂θ j
i
is positive deﬁnite, ∀θ ∈Θ.

344
ADDITIONAL TOPICS AND COMPLEMENTS
6. The quantity
'
D
T(x)
nQ
i=1 f (xi|θ)dx can be diﬀerentiated with respect to θ
inside the integral for any statistic T(X) and set D.
Remark A.6 Note that if Xi
iid∼f (x|θ), i = 1, 2, . . ., n, then the Fisher information
contained in X about θ is simply IF
x (θ) =
nP
i=1 IF
Xi(θ).
A.5
Multivariate Analysis Topics
Decision theoretic results in a multivariate setting can be found in Anderson
(2003) and Muirhead (2005). In terms of the (joint) decision problem for a mean
vector and covariance matrix, we can use a generalization of the weighted quadratic
loss, namely, the loss function
L(δ, θ) = (d −µ)T J(d −µ) + tr((D −Σ)G(D −Σ)H),
where the parameter is θ = {µ, Σ}, the action is δ = {d, D} and J, G and H are
known matrices with the appropriate dimensions.
Generalized multivariate analysis involves multivariate analysis techniques un-
der the assumption of the elliptical family of distributions, i.e., the p × 1 vector X
has density given by
f (x|µ, Σ) = kp |Σ|−1/2 h
h
(x −µ)TΣ−1(x −µ)
i
,
where kp is a constant that depends only on p and h(.) is a real function that could
depend on p, called the generator function. We write Elp(µ, Σ; h) to denote the
elliptical family of distributions with generator h, mean vector µ = [µ1, . . . , µp]T
and covariance structure proportional to Σ = [(σij)]. When µ = 0 and Σ = Ip, the
family Elp(0, Ip; h) is called spherical. Notice that h is such that
kpπp/2
Γ(p/2)
+∞
R
0
zp/2−1h(z)dz = 1,
since z = (x −µ)TΣ−1(x −µ) has density
fz(z) = kpπp/2
Γ(p/2)zp/2−1h(z),
z > 0. The multivariate normal is a special case for h(z) = e−z/2.
The family can be alternatively deﬁned via its characteristic function which is
given by
ϕX(t) = eitT µg(tTΣt),
for some non-negative function g (that depends on h if the density exists). For
additional results one can turn to Fang and Zhang (1990) and Muirhead (2005).
We recite here three important results. Assuming that X ∼Elp(µ, Σ; h) then we can
show the following:
(a) E(X) = µ and Var(X) = aΣ, where a = −2g′(0).

STATE CLASSIFICATION
345
(b) If X has independent coordinates then X ∼Np(µ, Σ), where Σ is a diagonal
matrix.
(c) If B is an r × p matrix of rank r (r ≤p) then BX ∼Elr(Bµ, BΣBT; h).
Consequently, elliptical and spherical families are related as follows: if Y ∼
Elp(0, Ip; h) and Σ = AAT, for some p× p matrix A then X = AY+µ ∼Elp(µ, Σ; h).
A.6
State Classiﬁcation
Remark A.7 (Calculating H and R) We have seen several ways of obtaining the
matrix of hitting probabilities H and the potential matrix R. We complete the dis-
cussion in the following remark. Let Ψ denote the state space of a Markov chain
X.
1. Recurrent state x or y Assume that y is recurrent. Then by remark 6.14 we
have H(y, y) = 1 and therefore R(y, y) = +∞. Now if x →y then by equation
(6.27), H(x, y) > 0 and hence R(x, y) = +∞. However, if x ↛y then H(x, y) = 0
and hence R(x, y) = 0. Consequently, for y recurrent and any type x we have
R(x, y) =

0, if H(x, y) = 0
+∞, if H(x, y) > 0
.
(A.3)
Now if y is transient and x recurrent then from remark 6.15.6, y cannot be reached
by x and therefore H(x, y) = 0 and R(x, y) = 0. The only remaining case is when x
and y are transient.
2. Transient states x and y Let D ⊂Ψ denote the set of transient states and as-
sume that x, y ∈D. Deﬁne TD and RD to be the matrices obtained from T and R,
respectively, by deleting all rows and columns corresponding to recurrent states so
that TD(x, y) = T(x, y) and RD(x, y) = R(x, y), for x, y ∈D. Rearrange the states
in Ψ so that the recurrent states precede the transient states and write the transition
matrix as T =

TR
0
K
TD
, where TR is the transition probabilities for the recur-
rent states and K is the matrix of transition probabilities from the transient states
to the recurrent states. Then for any m ≥0 we have T m =

T m
R
0
Km
T m
D
. Since
R =
+∞
P
m=0 T m =

+∞
P
m=0 T m
R
0
+∞
P
m=0 Km
+∞
P
m=0 T m
D

we have established that
RD =
+∞
P
m=0 T m
D = I + TD + T 2
D + T 3
D + . . . ,
and therefore we can easily compute RD from the systems of equations
(I −TD)RD = I = RD(I −TD).
(A.4)
If D is ﬁnite then RD = (I −TD)−1. In general, RD is the minimal solution to the

346
ADDITIONAL TOPICS AND COMPLEMENTS
system of equations
(I −TD)Y = I, Y ≥0.
(A.5)
Indeed, RD satisﬁes (A.4) so it remains to show it is minimal. Let Y be another
solution of (A.5). Then
Y = I + TDY,
so that repeatedly replacing Y on the RHS we obtain
Y = I + TD + T 2
D + · · · + T n
D + T n+1
D Y ≥
nP
m=0 T m
D,
(A.6)
since T n+1
D
≥0 and Y ≥0. Taking limits above as n →+∞we have Y ≥RD, as
claimed.
A.7
MATLAB R⃝Code Function Calls
See the book website in order to download the MATLAB functions.
Remark A.8 (Goodness-of-ﬁt Monte Carlo test) The function calls for example
2.18 (with the results presented in Table 2.2) are as follows:
x=unifrnd(0,1,10,1);MCModelAdequacyex1(100000,1,x);
x=unifrnd(0,1,50,1);MCModelAdequacyex1(100000,1,x);
x=unifrnd(0,1,100,1);MCModelAdequacyex1(100000,1,x);
x=gamrnd(10,10,10,1);MCModelAdequacyex1(100000,1,x);
x=gamrnd(10,10,50,1);MCModelAdequacyex1(100000,1,x);
x=gamrnd(10,10,100,1);MCModelAdequacyex1(100000,1,x);
x=normrnd(-10,1,10,1);MCModelAdequacyex1(100000,1,x);
x=normrnd(-10,1,50,1);MCModelAdequacyex1(100000,1,x);
x=normrnd(-10,1,100,1);MCModelAdequacyex1(100000,1,x);
Remark A.9 (Random object realizations) Figure 4.1 presents realizations for
several random objects. The function calls are as follows:
%Plot a)
RandomFunctionex1([1,1,0,1,0,0,0,1,0,0,1,1,1,1,0],[2,5,7]);
%Plot b)
GenerateMarkovPP([.1],50,1,1,[0,1],[0,1],0,5000,1);
%Plot c)
HPPP(2,100,[0,1],[0,1],[0,1],1);
%seting up a mixture parameters for plot d)
trueps=[1/4,1/4,1/4,1/4];truemus=[[0,0];[0,1];[1,0];[1,1]];
truesigmas=zeros(4,2,2);truesigmas(1,:,:)=eye(2);truesigmas(2,:,:)=eye(2);
truesigmas(3,:,:)=eye(2);truesigmas(4,:,:)=eye(2);
GenPPPFromGivenMixture2d([0,1],[0,1],100,100,0,trueps,truemus,0.01*
truesigmas,1,0);
%Plot e)
RandomDiscex1(5,10,1);

COMMONLY USED DISTRIBUTIONS AND THEIR DENSITIES
347
%Plot f)
GRF2d([],[0,10],[0,10],[1/2,10,0,1],50,1);
Remark A.10 (Brownian motion) Figure 7.1 presents realizations for the one- and
two-dimensional Brownian motion. The MATLAB calls are:
BrownianMotion(1,1000,1000,3,1);
BrownianMotion(2,1000,1000,3,1);
A.8
Commonly Used Distributions and Their Densities
We begin with commonly used discrete random variables.
Remark A.11 (Discrete distributions) We collect some of the important charac-
teristics of commonly used discrete random variables.
1. Binomial We write X ∼Binom(n, p) to denote a binomial random variable
on n trials with probability of success p ∈[0, 1]. The pmf is given by fX(x|p) =
Cn
xpx(1 −p)n−x, x = 0, 1, . . ., n, with E(X) = np, Var(X) = np(1 −p) and mgf
mX(t) = (pet + 1 −p)n. If n = 1 this is called the Bernoulli random variable.
2. Discrete uniform We write X ∼DUni f (N) to denote a discrete uniform ran-
dom variable, N = 1, 2, . . .. The pmf is given by fX(x|N) = 1/N, x = 1, . . . , N, with
E(X) = N+1
2 , Var(X) = (N+1)(N−1)
12
and mgf mX(t) = 1
N
NP
j=1 ejt.
3. Geometric We write X ∼Geo(p) to denote a geometric random variable with
probability of success p ∈[0, 1]. The pmf is given by fX(x|p) = (1 −p)px−1, x =
1, 2, . . ., with E(X) = 1
p, Var(X) = 1−p
p2 and mgf mX(t) =
pet
1−(1−p)et , t < −log(1 −p).
4. Hypergeometric We write X ∼HyperGeo(N, M, n) to denote a hypergeometric
random variable. The pmf is given by fX(x|N, M, n) =
CM
x CN−M
n−x
CNn
, x = 1, 2, . . ., n,
max(0, M −(N −n)) ≤x ≤min(M, n), with E(X) = nM
N , Var(X) = nM
N
(N−M)(N−n)
N(N−1)
.
Letting p = M
N we may write X ∼HyperGeo(N, n, p).
5. Multinomial We write X ∼Multi(n, p), X = [X1, . . . , Xk]T,
kP
i=1 Xi = n, to
denote the multinomial random variable on n trials with probabilities of success
p = [p1, . . . , pk]T, pi ∈[0, 1]. The pmf is given by fX(x|p) = Cn
x1,...,xk px1
1 . . . pxk
k ,
kP
i=1 xi = n, xi = 0, 1, . . . , n, i = 1, 2, . . ., k, with E(Xi) = npi, Var(Xi) = npi(1 −pi)
and Cov(Xi, X j) = −npip j, i, j = 1, 2 . . ., k. Note that for k = 2 we get the
Binom(n, p = p1 = 1 −p2) distribution.
6. Negative binomial We write X ∼NB(r, p) to denote a negative binomial ran-
dom variable with probability of success p ∈[0, 1] and r = 1, 2, . . .. The pmf
is given by fX(x|r, p) = Cr+x−1
x
pr(1 −p)x, x = 1, 2, . . ., with E(X) =
r(1−p)
p
,

348
ADDITIONAL TOPICS AND COMPLEMENTS
Var(X) =
r(1−p)
p2
and mgf mX(t) =

pet
1−(1−p)et
r , t < −log(1 −p). Note that
Geo(p) ≡NB(r = 1, p).
7. Poisson We write X ∼Poisson(λ) to denote a Poisson random variable with rate
λ ∈[0, +∞). The pmf is given by fX(x|λ) = λxe−λ
x! , x = 0, 1, . . . , with E(X) = λ =
Var(X) and mgf mX(t) = eλ(et−1).
Remark A.12 (Continuous distributions) We now collect some of the important
characteristics of commonly used continuous random variables.
1. Beta We write X ∼Beta(a, b) to denote the beta random variable with param-
eters a, b > 0. The pdf is given by fX(x|a, b) = xa−1(1−x)b−1
Be(a,b)
, 0 ≤x ≤1, with Be(a, b)
the Beta function Be(a, b) =
1R
0
xa−1(1 −x)b−1dx, E(X) =
a
a+b, Var(X) =
ab
(a+b)2(a+b+1)
and mgf mX(t) = 1 +
+∞
P
k=1
 k−1
Q
r=0
a+r
a+b+r
!
tk
k!. Note that Be(a, b) = Γ(a)Γ(b)
Γ(a+b) , where Γ(a) =
+∞
R
0
xa−1e−xdx, the gamma function.
2. Dirichlet We write X ∼Dirichlet(a), X = [X1, . . . , Xk]T,
kP
i=1 Xi = 1, 0 ≤Xi ≤1,
to denote the Dirichlet random vector with parameter a = [a1, . . . , ak]T, with ai > 0,
i = 1, 2 . . ., k. Let a0 =
kP
i=1 ai. The pdf is given by fX(x|a) =
Γ(a0)
Γ(a1)...Γ(ak)xa1−1
1
. . . xak−1
k
,
kP
i=1 xi = 1, 0 ≤xi ≤1, i = 1, 2, . . ., k, with E(Xi) =
ai
a0, Var(Xi) =
(a0−ai)ai
a2
0(a0+1) and
Cov(Xi, X j) = −
aiaj
a2
0(a0+1), i, j = 1, 2 . . ., k. Note that for k = 2 we get the Beta(a =
a1, b = a2) distribution.
3. Gamma and related distributions We write X ∼Gamma(a, b) to denote the
gamma random variable with shape parameter a > 0 and scale parameter b > 0.
The pdf is given by fX(x|a, b) =
xa−1e−x
b
Γ(a)ba , x > 0, with E(X) = ab, Var(X) = ab2
and mgf mX(t) = (1 −bt)−a, t < 1/b. The χ2
n random variable with n degrees of
freedom is deﬁned by χ ∼χ2
n ≡Gamma(a =
n
2, b = 2), with E(χ) = n and
Var(χ) = 2n. Given two independent random variables χ1 ∼χ2
n1 and χ2 ∼χ2
n2 we
deﬁne the F-distribution with n1 and n2 degrees of freedom via the transformation
F = χ1/n1
χ2/n2 ∼Fn1,n2. The exponential random variable with rate θ is given by X ∼
Exp(θ) ≡Gamma(a = 1, b =
1
θ), with E(X) = θ and Var(X) = θ2. Note that
Y = 1/X follows the Inverse Gamma distribution, denoted by InvGamma(a, b)
with density fX(x|a, b) =
e
−1
bσ2
Γ(a)ba(σ2)
a+1, x > 0, with E(X) =
1
b(a−1), if a > 1 and
Var(X) =
1
b2(a−1)2(a−2), if a > 2.
4. Uniform We write X ∼Uni f (a, b) to denote the uniform random variable with

COMMONLY USED DISTRIBUTIONS AND THEIR DENSITIES
349
parameters a, b ∈R, a < b. The pdf is given by fX(x|a, b) =
1
b−a, a ≤x ≤b, with
E(X) = a+b
2 and Var(X) = (b−a)2
12 . Note that Uni f (0, 1) ≡Beta(a = 1, b = 1).
5. Univariate normal and related distributions We write X ∼N(µ, σ2) to de-
note the normal random variable with parameters µ ∈R and σ > 0. The pdf is
given by fX(x|µ, σ2) =
1
√
2πσ2e−
1
2σ2 (x−µ)2, x ∈R, with E(X) = µ, Var(X) = σ2
and mgf mX(t) = eµt+ t2σ2
2 . Given independent random variables Z ∼N(µ, σ2) and
χ ∼χ2
n, the t-distribution with n-degrees of freedom is deﬁned via the transforma-
tion t =
Z√χ
n
, denoted by t ∼tn(µ, σ2), with E(tn) = µ, if n > 1 and Var(tn) = nσ2
n−2,
if n > 2. The density is given by ft(t|µ, σ2) =
Γ( n+1
2 )
√
nπσ2Γ( n
2)
h
1 + (x−µ)2
nσ2
i−n+1
2 , t ∈R. Note
that (t−µ
σ )2 ∼F1,n. The Cauchy distribution is deﬁned as a special case of the tn for
n = 1 by X ∼Cauchy(µ, σ2) = t1(µ, σ2). Note that the mean and the variance do
not exist for the Cauchy distribution.


References
Anderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis.
3rd edition, Wiley, New Jersey.
Bass, R. F. (1997). Diﬀusions and Elliptic Operators. New York, Springer-
Verlag.
Bass, R. F. (2011). Stochastic Processes. Cambridge Series in Statistical and
Probabilistic Mathematics. Cambridge University Press.
Bayarri, M. J. and Berger, J. O. (1999). Quantifying surprise in the data and
model veriﬁcation. Bayesian Statistics, 6, (eds. Bernardo, J. M. et al.), Oxford Sci-
ence Publication, 53-82.
Berger, J. O. (1984). The robust Bayesian viewpoint (with discussion). In Ro-
bustness of Bayesian Analysis, J. Kadane (ed.), North Holland, Amsterdam.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis, 2nd
edition. Springer-Verlag, New York.
Berger, J.O. (1994). An overview of robust Bayesian analysis. Test, 3, 5-124
(with discussion).
Berger, J. O., Bernardo, J. M., and Sun, D. (2009). The formal deﬁnition of
reference priors. The Annals of Statistics, 37, 2, 905–938.
Berger J. O., and Delampady, M. (1987). Testing precise hypothesis, (with dis-
cussion). Statistical Science, 2, 317-352.
Berliner, L. (1996). Hierarchical Bayesian time-series models. Fundamental
Theories of Physics 79, 15-22.
Bernardo, J. M., and Ramon, J. M. (1998). An introduction to Bayesian refer-
ence analysis: inference on the ratio of multinomial parameters. The Statistician,
47, 101-35.
Bernardo, J. M., and Smith, A. F. M. (2000). Bayesian Theory. Wiley.
Besag, J. E. (1974). Spatial interaction and the statistical analysis of lattice sys-
tems. Journal of the Royal Statistical Society, Series B, 36, 192-225.
Billingsley, P. (2012). Probability and Measure. Anniversary Edition. Wiley.
351

352
REFERENCES
Borodin, A. N. and Salminen, P. (1996). Handbook of Brownian Motion: Facts
and Formulae. Birkh¨auser, Basel.
Box, G. E. P. (1980). Sampling and Bayes inference in scientiﬁc modelling and
robustness. Journal of the Royal Statistical Society, Series A, 143, 383-430.
Brown, L. D. (1975). Estimation with incompletely speciﬁed loss functions.
Journal of the American Statistical Association, 70, 417-427.
Capp´e, O., Robert, C.P., and Ryd´en, T. (2003). Reversible jump, birth-and-death
and more general continuous time Markov chain Monte Carlo samplers. Journal of
the Royal Statistical Society, B, 65, 679–700.
Carath´eodory, C. (1918). Vorlesungen ¨uber reelle Funktionen. Teubner,
Leipzig. 2d edition, 1927.
Carter, D. S. and Prenter, P. M. (1972). Exponential spaces and counting pro-
cesses. Zeitschrift f¨ur Wahrscheinlichkeitstheorie und verwandte Gebiete, 21, 1-19.
Casella G. and Berger R. L. (1987). Reconciling Bayesian and Frequentist evi-
dence in the one-sided testing problem. Journal of the American Statistical Associ-
ation, 82, 106-111.
Casella, G. and Berger, R. L. (2002). Statistical Inference. 2nd edition, Duxbury
Advanced Series.
Chung, K.L. (1974). A Course in Probability Theory, 2nd edition. Academic
Press, New York.
C¸ inlar, E. (1975). Introduction to Stochastic Processes. Prentice Hall, Inc., New
Jersey.
C¸ inlar, E. (2010). Probability and Stochastics. Springer, New York.
Cox, D. R. (1977). The role of signiﬁcance tests, (with discussion). Scandina-
vian Journal of Statistics, 4, 49-70.
Cressie, N., and Pardo, L. (2000). Minimum ϕ-divergence estimator and hierar-
chical testing in loglinear models. Statistica Sinica, 867-884.
Cressie, N., and Wikle, C. K. (2011). Statistics for Spatio-Temporal Data. Wi-
ley, Hoboken, NJ.
De la Horra, J. (2005). Reconciling Classical and Prior Predictive P-Values in
the Two-Sided Location Parameter Testing Problem. Communications in Statistics:
Theory and Methods, 34, 575 - 583.
De la Horra, J., and Rodrıguez-Bernal, M.T. (1997). Asymptotic behavior of the
posterior predictive p-value. Communications in Statistics: Theory and Methods.
26, 2689-2699.
De la Horra, J., and Rodrıguez-Bernal, M.T. (1999). The posterior predictive
p-value for the problem of goodness of ﬁt. Test, 8, 117-128.

REFERENCES
353
De la Horra, J., and Rodrıguez-Bernal, M.T. (2000). Optimality of the poste-
rior predictive p-value based on the posterior odds. Communications in Statistics-
Theory and Methods, 29, 181-192.
De la Horra, J., and Rodrıguez-Bernal, M.T. (2001). Posterior predictive p-
values: what they are and what they are not. Test, 10, 75-86.
Dellacherie, C. and Meyer, P. A. (1978). Probability and Potential. Amsterdam,
North-Holland.
Dellaportas, P., and Papageorgiou, I. (2006). Multivariate mixtures of normals
with unknown number of components. Statistics and Computing, 16, 57–68.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood
from incomplete data via the EM algorithm. Journal of the Royal Statistical Society,
B, 39, 1-38.
Dey, D. K., Lu, K. and Bose, S. (1998). A Bayesian approach to loss robustness.
Statistics and Decisions, 16, 65-87.
Dey, D. K. and Micheas, A. C. (2000). Ranges of posterior expected losses
and epsilon-robust actions. In: Robust Bayesian Analysis (Eds. D. Rios Insua and
F. Ruggeri; Springer-Verlag). Lecture Notes Monograph Series, Series #152, 145-
160.
Dey, R., and Micheas, A. C. (2014). Modeling the growth of objects through
a stochastic process of random sets. Journal of Statistical Planning and Inference,
151–152, 17–36.
Diaconis, P., and Ylvisaker, D. (1979). Conjugate Priors for Exponential Fami-
lies. The Annals of Statistics, 7, 269-281.
Diebolt, J., and Robert, C. P. (1994). Estimation of ﬁnite mixture distributions
through Bayesian sampling. Journal of the Royal Statistical Society B, 56, 2, 363-
375.
Doob, J. L. (1940). Regularity properties of certain families of chance variables.
Trans. Amer. Math. Soc., 47, 455–486.
Dudley, R. M. (2002). Real Analysis and Probability, 2nd edition. Cambridge
University Press.
Durrett, R. (1996). Stochastic Calculus: A Practical Introduction. CRC Press.
Durrett, R. (2004). Essentials of Stochastic Processes. Springer.
Durrett, R. (2010). Probability: Theory and Examples, 4rth edition. Cambridge
University Press.
Escobar, M. and West, M. (1995). Bayesian density estimation and inference
using mixtures. Journal of the American Statistical Association, 90, 577–588.

354
REFERENCES
Everitt, B.S., and Hand, D. J. (1981). Finite Mixture Distributions. Chapman
and Hall, London.
Fang, K. T. and Zhang, Y. T. (1990). Generalized Multivariate Analysis.
Springer-Verlag, Berlin Heidelberg.
Feller, W. (1968). An Introduction to Probability Theory and Its Applications,
Vol. 1, 3rd edition. Wiley.
Feller, W. (1971). An Introduction to Probability Theory and Its Applications,
Vol. 2, 2nd edition, Wiley.
Ferguson, T.S. (1996). A Course in Large Sample Theory. Chapman and
Hall/CRC.
Fristedt, B., and Gray, L. (1997). A Modern Approach to Probability Theory.
Birkh¨auser.
Fr¨uhwirth-Schnatter, S. (2006). Finite Mixture and Markov Switching Models.
Springer.
Fu, Q., and Banerjee, A. (2008). Multiplicative mixture models for overlapping
clustering, in: International Conference on Data Mining, IEEE, Pisa, Italy, 791–
796.
Gelfand, A. E. and Smith, A. (1990). Sampling-based approaches to calculating
marginal densities. Journal of the American Statistical Association, 85, 410, DOI:
10.1080/01621459.1990.10476213.
Gelman, A., Carlin, J.B., Stern, H.S., and Rubin, D.B. (2004). Bayesian Data
Analysis, 2nd edition. Chapman and Hall.
Gelman, A., Meng, X.L., and Stern, H. (1996). Posterior predictive assessment
of model ﬁtness via realized discrepancies. Statistica Sinica, 6, 733-807 (with dis-
cussion).
Geman, S., and Geman, D. (1984). Stochastic relaxation, Gibbs distributions,
and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6, 6, 721–741.
Geyer, C. J., and Thompson, E. A. (1992). Constrained Monte Carlo maximum
likelihood for dependent data. Journal of the Royal Statistical Society. Series B,
657-699.
Geyer, C. J. and Møller, J. (1994). Simulation procedures and likelihood infer-
ence for spatial point patterns. Scandinavian Journal of Statistics, 21, 359-373.
Green, P. J. (1995). Reversible jump Markov chain Monte Carlo computation
and Bayesian model determination. Biometrika 82, 711-732.
Halmos, P.R. (1950). Measure Theory. Van Nostrand, New York.

REFERENCES
355
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains and
their application. Biometrika, 57, 97-109.
Heller, K.A., and Ghahramani, Z. (2007). A nonparametric Bayesian approach
to modeling overlapping clusters, in: International Conference on Artiﬁcial Intelli-
gence and Statistics, 187–194.
Hwang, J. T., Casella, G., Robert, C., Wells, M. T. and Farrell, R. H. (1992).
Estimation of accuracy in testing. Annals of Statistics, 20, 490-509.
James, W., and Stein, C. (1961). Estimation with quadratic loss. In Proc. Fourth
Berkeley Symp. Math. Statist. Probab., 1, 361-380, University of California Press.
Jasra, A., Holmes, C.C. and Stephens, D. A. (2005). Markov chain Monte Carlo
methods and the label switching problem in Bayesian mixture. Statistical Science,
20, 50-67.
Jeﬀreys, H. (1946). An invariant form for the prior probability in estimation
problems. Proceedings of the Royal Statistical Society of London (Ser. A), 186,
453-461.
Jeﬀreys, H. (1961). Theory of Probability, 3rd edition. Oxford University Press,
London.
Ji, C., Merly, D., Keplerz, T.B., and West, M. (2009). Spatial mixture mod-
elling for unobserved point processes: Examples in immunoﬂuorescence histology.
Bayesian Analysis, 4, 297-316.
Johnson, M. E. (1987). Multivariate Statistical Simulation. Wiley.
Johnson, R. A., and Wichern, D. W. (2007). Applied Multivariate Statistical
Analysis, 6th ed., Pearson.
Jost, J. (2013). Partial Diﬀerential Equations. 3rd ed., Graduate Texts in Math-
ematics, vol 214, Springer, New York.
Jozani, M. J., Marchand, ´E., and Parsian, A. (2012). Bayesian and Robust
Bayesian analysis under a general class of balanced loss functions. Statistical Pa-
pers, 53, 1, 51-60.
Kadane, J. B., and Chuang, D. T. (1978). Stable decision problems. The Annals
of Statistics, 1095-1110.
Kallenberg, O. (1986). Random Measures. 4th ed., Akademie, Berlin.
Kallenberg, O. (2002). Foundations of Modern Probability, 2nd ed., Probability
and Its Applications, Springer, New York.
Karatzas, I., and Shreve S. E. (1991). Brownian Motion and Stochastic Calcu-
lus. 2nd ed., Springer, New York.
Karr, A. (1991). Point Processes and Their Statistical Inference. 2nd ed., CRC
/ Marcel Dekker, Inc.

356
REFERENCES
Kass, R. E., and Wasserman, L. (1996). The selection of prior distributions by
formal rules. Journal of the American Statistical Association, 91, 1343-70.
Klenke, A. (2014). Probability Theory: A Comprehensive Course, 2nd edition.
Springer.
Kolmogorov, A. N. (1933 and 1956). Grundbegriﬀe derWahrscheinlichkeit-
srechnung, Ergebnisse der Math., Springer, Berlin. English translation: Founda-
tions of the Theory of Probability, 2nd edition. Chelsea, New York, 1956.
Kottas, A. and Sanso, B. (2007). Bayesian mixture modeling for spatial Poisson
process intensities, with applications to extreme value analysis. Journal of Statis-
tical Planning and Inference (Special Issue on Bayesian Inference for Stochastic
Processes), 137, 3151-3163.
Lawler, G. F. and Limic V. (2010). Random Walk. Cambridge Univ. Press, Cam-
bridge.
Lebesgue, H. (1902). Int´egrale, longueur, aire (th`ese, Univ. Paris). Annali Mat.
purae appl. (Ser. 3) 7: 231–359.
Lehmann, E. L. (1986). Testing Statistical Hypothesis, 2nd edition. Wiley, New
York, USA.
Lehmann, E. L. (2004). Elements of Large-Sample Theory. Springer.
Lehmann, E. L. and Casella, G. (1998). Theory of Point Estimation, 2nd edition.
Springer-Verlag, New York, USA.
Lin, G. D. (2017). Recent Developments on the Moment Problem. arXiv
preprint arXiv:1703.01027.
Lindgren, G. (2013). Stationary Stochastic Processes: Theory and Applications.
Chapman & Hall/CRC, Boca Raton.
Lindsay, B. G. (1995). Mixture Models: Theory, Geometry and Applications,
volume 5 of NSF-CBMS Regional Conference Series in Probability and Statistics,
Institute of Mathematical Statistics and the American Statistical Association.
Makov, U. E. (1994). Some aspects of Bayesian loss robustness. Journal of
Statistical Planning and Inference, 38, 359-370.
Martin, J.-M., Insua, D. R. and Ruggeri, F. (1998). Issues in Bayesian loss ro-
bustness. Sankhya A, 60, 405-417.
Mat´ern, B. (1986). Spatial variation. Lecture Notes in Statistics, 36, Springer-
Verlag, Berlin.
McLachlan, G. J., and Peel, D. (2000). Finite Mixture Models. Wiley-
Interscience.
Meng, X. L. (1994) Posterior predictive p-values. Annals of Statistics, 22, 1142-
1160.

REFERENCES
357
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., Teller, E.
(1953). Equation of state calculations by fast computing machines. Journal of
Chemical Physics, 21, 1087-1092.
Micheas, A. C. Textbook website: https://www.crcpress.com/9781466515208.
Micheas, A. C. (2006). A uniﬁed approach to Prior and Loss Robustness. Com-
munications in Statistics: Theory and Methods, 35, 309-323.
Micheas, A. C. (2014). Hierarchical Bayesian modeling of marked non-
homogeneous Poisson processes with ﬁnite mixtures and inclusion of covariate
information. Journal of Applied Statistics, 41, 12, 2596-2615.
Micheas, A. C. and Dey, D. K. (2003). Prior and posterior predictive p-values
in the one-sided location parameter testing problem. Sankhya, A, 65, 158-178.
Micheas, A.C. and Dey, D.K. (2004). On measuring loss robustness using max-
imum a posteriori estimate. Communications in Statistics: Theory and Methods,
33, 1069-1085.
Micheas, A.C., and Dey, D.K. (2007). Reconciling Bayesian and frequentist ev-
idence in the one-sided scale parameter testing problem. Communications in Statis-
tics: Theory and Methods, 36, 6, 1123-1138.
Micheas, A.C., Wikle, C.K., and Larsen, D.R. (2012). Random set modelling of
three dimensional objects in a hierarchical Bayesian context. Journal of Statistical
Computation and Simulation, DOI:10.1080/00949655.2012.696647.
Micheas, A.C., and Zografos, K. (2006). Measuring stochastic dependence us-
ing ϕ-divergence. Journal of Multivariate Analysis 97.3, 765-784.
Mengersen, K., Robert, C. P., and Titterington, M. (2011). Mixtures: Estimation
and Applications. Wiley.
Molchanov, I. (2005). Theory of Random Sets. Springer-Verlag, London.
Muirhead, R. J. (2005). Aspects of Multivariate Statistical Theory, 2nd edition.
Wiley.
Musiela, M. and Rutkowski, M. (2005). Martingale Methods in Financial Mod-
elling. 2nd ed., Stochastic modelling and applied probability, vol 36, Springer,
Berlin.
Neyman, J., and Scott, E.L. (1948). Consistent estimates based on partially con-
sistent observations. Econometrica: Journal of the Econometric Society, 1-32.
Nguyen, H. T. (2006). An Introduction to Random Sets. Chapman & Hall/CRC,
Boca Raton.
Øksendal, B. (2003). Stochastic Diﬀerential Equations: An Introduction with
Applications. 6th ed., Springer-Verlag, Berlin.

358
REFERENCES
Pearson, K. (1894). Contributions to the mathematical theory of evolution.
Philosophical Transactions of the Royal Society of London A, 185, 71–110.
Press, J.S. (2003). Subjective and Objective Bayesian Statistics, 2nd edition.
Wiley.
Ramsay, J. O. and Novick, M. R. (1980). PLU robust Bayesian decision. The-
ory: point estimation. Journal of the American Statistical Association, 75, 901-907.
R´enyi, A. (1970). Foundations of Probability. Holden Day, San Francisco, 1970.
Richardson, S., and Green, P.J. (1997). On Bayesian analysis of mixtures with
an unknown number of components (with discussion). Journal of the Royal Statis-
tical Society, B, 59, 731-792.
Rios Insua, D. and Ruggeri, F. (2000). Robust Bayesian Analysis. Lecture Notes
Monograph Series, Series #152, Springer-Verlag.
Robert, C. P. (2007). The Bayesian Choice, 2nd edition. Springer.
Robert, C. P., and Casella, G. (2004). Monte Carlo Statistical Methods, 2nd
edition. Springer.
Robert, C. P., and Casella, G. (2010). Introducing Monte Carlo Methods with
R. Springer-Verlag.
Royden, H.L. (1989). Real Analysis, 3rd edition. Prentice Hall.
Rubin, D. B. (1984). Bayesianly justiﬁable and relevant frequency calculations
for the applied statistician. Annals of Statistics, 12, 1151-1172.
Scricciolo, C. (2006). Convergence rates for Bayesian density estimation of
inﬁnite-dimensional exponential families. The Annals of Statistics, 34, 2897–2920.
Sethuraman, J. (1994). A constructive deﬁnition of Dirichlet priors. Statistica
Sinica, 4, 639–650.
Shafer, G. (1982). Lindley’s Paradox, (with discussion). Journal of the Ameri-
can Statistical Association, 77, 325-351.
Skorohod, A. V. (1956). Limit theorems for stochastic processes. Theory of
Probability and Its Applications, SIAM, 261-290.
Stephens, M. (2000). Bayesian analysis of mixture models with an unknown
number of components: An alternative to reversible jump methods. Annals of
Statistics, 28, 40–74.
Stummer, W., and Vajda, I. (2010). On divergences of ﬁnite measures and their
applicability in statistics and information theory. Statistics 44.2, 169-187.
Tanner, M. and Wong, W. (1987). The calculation of posterior distributions by
data augmentation. Journal of the American Statistical Association, 82, 528-550.
Thomopoulos, N. T. (2013). Essentials of Monte Carlo Simulation: Statistical
Methods for Building Simulation Models. Springer.

REFERENCES
359
Titterington, D. M., Smith, A. F. M., and Makov, U. E. (1985). Statistical Anal-
ysis of Finite Mixture Distributions. Wiley, New York.
Vajda, I. (1989). Theory of Statistical Inference and Information, Kluwer Aca-
demic Publishers.
Varian, H.R. (1975). A Bayesian approach to real estate assessment. In Stud-
ies in Bayesian Econometrics and Statistics in Honor of Leonard J. Savage, (S.E.
Feinberg and A. Zellner eds.), 195-208, Amsterdam: North Holland.
Vestrup, E. M., (2003). The Theory of Measure and Integration. John Wiley &
Sons, Inc., Hoboken, New Jersey.
Wasserman, L., (2009). All of Nonparametric Statistics. Springer.
Widom, B. and Rowlinson, J. S. (1970). A new model for the study of liquid-
vapor phase transitions. Journal of Chemichal Physics, 52, 1670-1684.
Wikle, C.K., Berliner, L. and Cressie, N. (1998). Hierarchical Bayesian space-
time models. Environmental and Ecological Statistics 5, 2, 117-154.
Wolpert, R. and Ickstadt, K. (1998). Poisson/Gamma random ﬁeld models for
spatial statistics. Biometrika, 85, 251-267.
Zellner, A. (1986). Bayesian estimation and prediction using asymmetric loss
functions. Journal of the American Statistical Association, 81, 446-451.
Zellner, A. and Geisel, M.S. (1968). Sensitivity of control to the form of the
criterion function. In: The Future of Statistics, D.G. Watts, ed., Academic Press,
269-289. (With comments by J.C. Kiefer and H.O. Hartley and the authors’ re-
sponses).


Index
Lp-space
Deﬁnition, 130
H¨older’s Inequality, 130
Minkowski’s Inequality, 130
ϕ-Divergence
Loss function, 28
Algorithm
Acceptance-Rejection method, 18
Composition method, 20
Expectation-Maximization (EM),
34
Gibbs sampler, 265
Inverse transform method, 18
Metropolis-Hastings, 264
Model adequacy via Monte Carlo
tests, 62
Newton-Raphson, 32
RJMCMC, 268
Almost Everywhere, 3, 92
Almost Surely, 3
Axiom of Choice, 340
Baire Function
Deﬁnition, 343
Baire functions and spaces
References, 342
Baire Set
Deﬁnition, 342
Bayes
Action, 29
Asymptotic normality of the poste-
rior distribution, 205
Bayes factor via Monte Carlo, 58
Bayesian model selection, 57
Bayesian model selection via the
Bayes factor, 57
Choice of prior, 14
Classes of priors, 15
Conjugate prior, 12
Credible set, 45
Empirical, 15
Estimation considerations, 39
Exchangeable prior, 14
Extended Bayes rule, 29
Factor, 55
Full conditional distribution, 16
Generalized Bayes rule, 29
Generalized MLE, 40
Hierarchical Bayesian models
(HBM), 16, 21
Hierarchical priors, 16
Highest posterior density (HPD),
46
HPD credible Set, 46
Informative and non-informative
prior, 14
Jeﬀreys’ prior, 15
MAP (Maximum a posteriori), 40
Model averaging, 266
Model choice, 266
Monte Carlo HPD set, 47
Multivariate normal HBM, 17
Objective, 15, 22
Objective or reference prior, 22
Point estimation, 40
Posterior predictive inference, 58
Posterior predictive p-value, 61
Posterior propriety, 39
Prior predictive p-value, 61
Prior selection methods, 15
Rule, 29
Theorem, 3
361

362
INDEX
Using the Bayes factor, 55
Bayesian Model Selection
via the Bayes factor, 57
Borel
Sets, 81
Borel σ-ﬁeld, 81
Generating from open cylinders, 82
Generating from open intervals, 81
Product σ-ﬁeld, 85
Borel Space
Deﬁnition, 340
Brownian Motion
Arcsine law, 317
Deﬁnition, multi-dimensional, 313
Deﬁnition, one-dimensional, 312
Donsker’s invariance principle, 316
Existence and uniqueness, 315, 316
Gaussian process (GP), 313
L´evy process, 336
L´evy’s characterization, 313
Law of iterated logarithm, 313
Properties, 312
Strong law, 313
Strong Markov property, 313
Translation invariance, 313
Wiener measure, 316
Wiener measure on [0, +∞), 316
Central Limit Theorems
CLT under m-dependence and sta-
tionarity, 203
Lindeberg condition, 201
Lindeberg’s CLT, 201
Lyapounov’s CLT, 203
Lyapounov’s condition, 201
Triangular arrays, 201
Usual CLT, 200
Characteristic Function
Continuity theorem, 197
Exponential family, 8
Inversion theorem, 196
of convolutions, 196
Cholesky Decomposition
Multivariate normal, 20
Compactness
of the space of distribution func-
tions, 192
Complex Number
Conjugate, 162
Concepts
Convergence, 42
Expectation, 7, 159
Extension of measure, 102
Independence, 153
Interval estimation, 44
Measure theory, 77
requirements of a statistical model,
1
Conditional
Density given a σ-ﬁeld, 173
Conditional Expectation
Conditional Chebyshev inequality,
176
Conditional DCT, 175
Conditional Jensen inequality, 175
Conditional MCT, 175
Distribution given a σ-ﬁeld, 174
given the value of a random vari-
able, 168
Successive conditioning, 176
via the Radon-Nikodym theorem,
168
Conditional Independence
Given a σ-ﬁeld, 177
Conditional Probability
Deﬁnition, 3
Distribution given a σ-ﬁeld, 172
Distribution given the value of a
random variable, 167
given a σ-ﬁeld, 170
given the value of a random vari-
able, 166
via the Radon-Nikodym theorem,
166
Conjugate Prior
Multivariate normal, 12

INDEX
363
Convergence
Almost everywhere, 93
Almost sure, 42
Cauchy criterion, 199
CLT under m-dependence and sta-
tionarity, 203
Estimator consistency, 43
In law or distribution or weak, 43
In normed vector spaces, 131
In probability, 42
in the rth mean, 204
in the space of probability mea-
sures, 192
Lindeberg’s CLT, 201
Lyapounov’s CLT, 203
of random series, 200
SLLN, 206
Strong law of large numbers
(SLLN), 44
Weak law of large numbers
(WLLN), 43
Convolution
Deﬁnition using densities, 195
Deﬁnition using distributions, 194
Random, 194
Cumulative Distribution Function
Conditional, 6
Deﬁnition, 4
Generalization, 4
Requirements, 4
Cylinder Sets
Measurable, 84
Open, 83
with base, 84
Decision Theory
ϕ-divergence as loss function, 28
Admissibility principle, 29
Admissible test function, 59
Bayes action, 29
Bayes risk, 28
Bayes risk principle, 29
Bayes rule, 29
Bayes test function, 59, 60
Blyth’s lemma, 41
Complete class of decision rules,
41
Components, 27
Equalizer rule, 40
Frequentist risk, 28
Hypothesis testing, 59
Loss function, 27
Minimax principle, 29
Minimax rule, 29
Minimax test function, 59, 60
Posterior expected loss (PEL), 29
Principles, 28
R-better, 29
Randomized decision rules or ac-
tions, 41
References, 66
Shrinkage estimators, 42
Utility function, 41
Density
Deﬁnition, 151
Deterministic Sets
λ-system, 78
π-system, 78
σ-ﬁeld of sets, 79
Axiom of choice, 340
Basic deﬁnitions and properties, 77
Borel σ-ﬁeld, 81
Collection of open sets, 81
Cross section, 85
Cylinder open sets, 82
Eventually, 78
Field of sets, 79
Generated σ-ﬁeld of sets, 80
Generated ﬁeld, 79
Inﬁnitely often, 78
Inverse image, 337
Measurable rectangles, 85
Measurable set, 86
Measurable space, 86
Nonmeasurable, 106, 340
Open rectangles, 83

364
INDEX
Product of σ-ﬁelds, 85
References, 133
Ring, 341
Sierpi´nski class, 141
sub-σ-ﬁeld of sets, 79
Symmetric diﬀerence, 77
Topological space, 338
Distribution
Bernoulli, 347
Beta, 224, 348
Binomial, 347
Cauchy, 349
Chi-square, 348
Delta, 234
Dirac or point mass, 73
Dirichlet, 348
Discrete uniform, 347
Elliptical family, 344
Exponential, 348
F, 348
Gamma, 348
Geometric, 347
Hypergeometric, 347
Inﬁnitely divisible, 196
Inverse gamma, 348
Inverted Wishart, 12
Joint for two order statistics, 24
Laplace (or double exponential),
70, 181
Location-scale exponential, 67
Maximum order statistic, 24
Minimum order statistic, 24
Multinomial, 347
Negative binomial, 347
Normal, bivariate, 10
Normal, univariate, 349
Pareto, 26, 73
Poisson, 348
Stable, 195
Standard normal percentile, 47
t, 349
Trinomial, 67
Uniform, 349
Wishart, 17, 39
Wishart, characteristic function, 17
Distribution Function
Convergence of integral in the
limit, 190
Deﬁnition, 148
determined by non-central mo-
ments, 193
Support, 150
via a probablity measure, 148
Weak convergence, 189
Divergence
Hellinger, 71
Kullback-Leibler, 179
Mutual information, 181
Empirical Distribution Function
Deﬁnition, 205
Estimation
Ancillary statistic, 31
Asymptotic eﬃciency, 205
Asymptotic normality of the MLE,
204
Boundedly complete statistic, 31
Complete statistic, 31
Conﬁdence coeﬃcient, 45
Conﬁdence interval, 45
Consistent estimator, 43
Coverage probability, 45
Decision theory, 27
Equal tail conﬁdence interval, 46
Factorization theorem, 31
Fisher information, 343
Frequentist methods, 45
Interval, 44
Maximum likelihood estimator
(MLE), 30
Mean square error (MSE), 28
Methods of ﬁnding estimators, 30
Methods of moments estimator, 30
Minimal suﬃcient statistic, 31
Minimum width conﬁdence inter-
val, 46

INDEX
365
MLE consistency, 186
MLE via EM algorithm, 33
MLE via Newton-Raphson, 32
Obtaining MLEs, 32
Obtaining UMVUEs, 35
Pivotal quantity, 31
Properties of estimators, 30
Proving consistency, 43
Relationships between decision
rules, 40
Sample non-central moments, 30
Suﬃciency principle, 31
Suﬃcient statistic, 30
U-estimable, 31
Unbiased estimator, 31
Uniformly minimum variance un-
biased estimator (UMVUE),
32
Event, 1
Events
Positively and negatively corre-
lated, 158
Example
M/M/k queue, 306
M/M/k queue with balking, 307
p-dimensional Lebesgue measure,
104
A statistic that is not complete, 37
Applying Itˆo’s formula, 324
Asymptotic normality of the t-test
statistic, 185
Bayes factor for point null hypothe-
sis testing, 56
Bayes rule, 40
Birth-Death sequence, 257
Borel product σ-ﬁeld, 85
Brownian bridge, 328
Brownian motion, 314
Change of variable, 114
Coin ﬂip spaces, 86
Coin toss, 2
Conditional expectation given a
σ-ﬁeld, 176
Conditional probability, 167
Conditional probability given a σ-
ﬁeld, 170
Conjugacy for exponential family,
12
Conjugate priors for multivariate
normal models, 12
Construction of a Monte Carlo ade-
quacy test, 63
Construction of the Poisson pro-
cess, 291
Counterexample for non-additive
measure, 93
Counterexample for uniqueness of
measure, 97
Counterexamples to Helly-Bray
theorem, 188
Counting measure, 93
Cylinder sets, 82
Data augmentation for mixture
models, 10
Delta method, 185
Discrete measure, 93
Exponential distribution, HPD
credible set, 47
Exponential distribution, simula-
tion, 18
Exponential family, 8
Exponential family, MLE via EM
algorithm, 34
Exponential family, MLR property,
53
Exponential family, UMP test func-
tion, 53
Exponential spaces, 87
Filtrations and stopping times, dis-
crete, 215
Finite mixtures, 169
Fubini theorem and change of vari-
able, 129
Fubini theorem and polar coordi-
nate space, 129
Function measurability, 91

366
INDEX
Gambler’s ruin, 216
Gambling, 218
Gambling and martingales, 213
Gambling, double or nothing, 225
Gamma distribution, simulation,
18, 19
Hit-or-miss topology and random
closed sets, 88
Integrability counterexample, 110
Integrating with respect to counting
measure, 107
Lebesgue measure in (0, 1], 104
Markov chains, binomial process,
239
Markov chains, Birth-Death se-
quences, 238
Markov chains, branching pro-
cesses, 240
Markov chains, gambler’s ruin, 237
Markov chains, hitting probabili-
ties, 245
Markov chains, hitting probabilities
for countable state space, 243
Markov chains, hitting probabilities
for uncountable state space,
242
Markov chains, random walks, 239
Markov chains, renewal sequences,
240
Markov chains, times of successes,
239
Martingale transformations, 213
Martingales and likelihood ratios,
224
Martingales and Polya urns, 223
Martingales and Radon-Nikodym
derivatives, 224
Mixture of exponential family com-
ponents, 9
Monte Carlo p-values, 61
Multivariate normal HBM, 17
Multivariate normal, HPD credible
set for the mean vector, 47
Multivariate normal, MLE and
UMVUE, 38
Nonmeasurable set, 105
Normal characteristic function, 163
Normal distribution, simulation, 19
Normal mean, Bayes test function,
60
Normal mean, decision theoretic
interval estimation, 48
Normal mean, decision theoric esti-
mators, 41
Normal mean, HPD credible set, 47
One-parameter exponential family,
UMVUE, 38
Ornstein-Uhlenbeck (O-U) process,
327
Poisson distribution, UMVUE, 37
Poisson process, 299, 300
Poisson process via random walks,
277
Polish spaces, 280
Population model, 310
Potential sequence, 228
Potential sequence for a simple
random walk, 232
Probability for coin ﬂips, 139
Probability measure via the cdf, 97
Queueing systems, 292
Queueing systems via birth-death
processes, 311
Queueing systems, Poisson arrivals,
278
Random convolutions, 194
Random object examples, 145
Random objects via densities, 156
Random sequence examples, 207
Random walk and conditional inde-
pendnece, 178
Random walk, return times to state
0, 209
Random walks and martingales,
213

INDEX
367
Reconciling Bayesian and frequen-
tist evidence, 65
Regular conditional probability,
167
Reliability analysis, 208
Renewal sequence period, 231
Riemann integrability does not im-
ply Lebesgue integrability, 116
Riemann integral counterexample,
115
Series of real numbers and MCT,
110
Solving stochastic partial diﬀeren-
tial equations, 329
State classiﬁcation, simple random
walk, 254
Stationary and ergodic sequences,
261
Stochastic diﬀerential equation,
stock prices, 318
Stochastic diﬀerential equation,
weak and strong solution, 327
Stochastic integration, 321
Stochastic processes, 275
Tail events, 157, 159
The MLE is not always consistent,
186
Transition operators of a Markov
chain, 234
Uniform distribution, MLE and
UMVUE, 36
Uniform distribution, UMP and
GLRT tests, 54
Union of σ-ﬁelds, 81
Using the MCT for proofs, 111
Wiener measure, measurable func-
tionals, 316
Expectation
and independence, 161
Conditional, 7
Deﬁnition, 7, 159
Exponential family, 8
Non-central moment, 7
Pearson correlation, 8
Random, 168
Variance-covariance matrix, 7
Exponential Family
Canonical, deﬁnition, 8
Complete-suﬃcient statistic, 36
Conjugate prior, 12
Covariance, 8
Expected value, 8
Full rank, 8
Minimal, 8
MLR property, 53
Multivariate normal, canonical
form, 38
Poisson, canonical form, 38
UMP test function, 53
UMVUE, 38
Filtration
Complete, 281
Deﬁnition, continuous time, 281
Deﬁnition, discrete time, 210
Minimal, 211
Minimal, continuous time, 281
Reverse, 213
Right continuous, 281
Usual conditions, 281
Formula
DeMoivre’s, 162
Fourier Transform
Deﬁnition, 162
Function
Beta, 348
Bijective, 337
Bounded variation, 338
Composition, 337
Gamma, 137, 348
Indicator, 89
Limit inferior, 89
Limit superior, 89
Locally H¨older-continuous, 341
Mapping or map, 337
Measurable, 89

368
INDEX
Multivariate gamma, 13, 17, 39
Positive and negative part, 337
Positive deﬁnite, 163
Projection in product spaces, 83
Restriction, 337
Section, 122
Simple, 90
Step, 90
Uniformly continuous, 341
Upper semicontinuous, 134
Functional
Bounded, deﬁnition, 132
Gaussian Process
as Brownian motion, 313
Gradient
Exponential family, 8
HBMs
Hierarchical Bayesian models, 15
Hessian
Exponential family, 8
Hypothesis Testing
Alternative hypothesis, 49
Bayes factor, 55
Bayesian p-values, 61
Complementary alternative, 49
Composite hypothesis, 50
Extension of simple vs simple hy-
potheses, 54
Frequentist p-value, 61
Fundamental lemma of Neyman-
Pearson, 52
Generalized likelihood ratio test
(GLRT), 52
Large sample GLRT, 52
Monotone likelihood ratio (MLR),
53
Most powerfull (MP) test function,
52
Non-randomized test function, 51
Null hypothesis, 49
Power, 50
Power of the test function, 51
Randomized test function, 50
Rejection region (RR), 49
Signiﬁcance level, 50
Simple hypothesis, 50
Test statistic, 49
Type I error, 49
Type II error, 50
UMP tests and MLR property, 53
UMP unbiased test, 51
Unbiased test, 51
Uniformly most powerful (UMP)
test function, 51
Independence
Deﬁnitions, 153
Existence of a random sequence
under independence, 153
of events, 3, 158
of random variables, 5
Inequality
Bonferroni’s, 179
Boole’s, 94
Cauchy-Schwarz, for functions,
132
Chebyshev, 25
H¨older, 130
Jensen, 25
Kolmogorov’s maximal, 199
Kounias’, 179
Markov, 25
Minkowski, 130
Integration Theory
Change of variable, general mea-
sure, 114
Change of variable, Lebesgue inte-
gral, 128
Fundamental theorem of calculus,
117
Inﬁnite integral, 107
Integrability, 110
Integral deﬁnition, 106
Integral operator, 113
Integration by parts, 117

INDEX
369
Integration over sets, 114
Lebesgue, 107
Lebesgue integral, 115
Liebnitz’s rule, 152
Multivariate change of variable,
Lebesgue integral, 129
Properties of the integral, 111
References, 133
Riemann and Lebesgue compari-
son, 115
Riemann and Riemann-Stieltjes
comparison, 117
Riemann-Stieltjes (RS) integral,
116
Undeﬁned integral, 107
Uniform integrability (UI), 113
Using the MCT for proofs, 110
Interval Estimation
Using pivotal quantities, 45
Using suﬃciency, 46
Large Sample (Asymptotic) Theory
Convergence concepts, 42
Likelihood function
Deﬁnition, 6
Linear Model
GLRT, 72
Local Martingale Problem
Well posed, 327
Locally Compact HausdorﬀSeparable
Spaces (LCHSS)
Baire space, 342
Mapping
Continuous, 339
Set function, 89
Set-valued, 89
Markov Chain
Absorbing state, 249
Absorbing state criterion, 249
as a stationary sequence, 261
Binomial process, 239
Birth-Death sequence, 238, 257
Branching process, 240
Chapman-Kolmogorov equations,
237
Classiﬁcation and limiting behav-
ior, 251
Classiﬁcation via the transition ma-
trix, 250
Classiﬁcation, theorem for recur-
rent non-null states, 252
Classiﬁcation, theorem for recur-
rent null or transient states, 253
Closed set of states, 249
Deﬁnition, 233
Detailed balance condition, 257
Expected number of visits to ﬁxed
state, 245
Finding hitting probabilities, 242
Gambler’s ruin, 237
Generator, 235
Harmonic function, 241
Hitting probability, 241
Hitting probability as a limit, 246
Hitting probability via the potential
matrix, 245
Irreducible, 249
Irreducible Markov chain and clas-
siﬁcation, 250
Irreducible set of states, 249
Irreducible, criterion, 249
k-fold transition operator, 235
Left transition operator, 233
Limiting distribution, 255
Markov matrix, 236
Notes on limiting behavior, 256
Performing state classiﬁcation, 253
Periodic state, 248
Periodicity criterion, 248
Potential matrix, 245
Properties, countable state space,
248
Random walk, 239
Recurrent non-null state, 248
Recurrent null state, 248
Recurrent state, 248

370
INDEX
Renewal measure and total number
of visits, 247
Renewal sequences, 240
Renewal theory and visits to a ﬁxed
state, 246
Return probability, 241
Reversible, 257
Right transition operator, 233
Showing the detailed balance con-
dition, 257
State classiﬁcation, 248
State classiﬁcation via renewal the-
ory, 247
State properties, 249
State reached by another state, 248
Stationary, 256
Strong Markov property, 236
The martingale problem, 240
Time homogeneity, 233
Times of successes, 239
Times of visits to ﬁxed state, 243
Total number of visits to ﬁxed state,
244
Transient state, 248
Transition functions, 233
Transition functions and operators,
234
Transition matrix, 236
Transition operator, 233
Using the martingale problem, 241
Visits in a ﬁxed number steps to
ﬁxed state, 244
Markov Process
Birth-Death, 309
Birth-Death, generator, 309
Birth-Death, limiting behavior, 309
Birth-Death, regularity, 309
C`adl`ag measurable space, 290
C`adl`ag space, 290
Chapman-Kolmogorov equation,
294
Deﬁnition, 291
Dynkin’s formula, 334
Equilibrium, 293
Inﬁnitesimal generator, 294, 300
Inﬁnitesimal generator via transi-
tion semi-group, 295
Inﬁnitesimal generators and transi-
tion functions, 295
Irreducible recurrent, 304
Jump rates, 300
Markov family, 292
Markov family of distributions cri-
terion, 293
Markov process criterion, 293
Modiﬁcation for smoothness, 302
Poisson, 291, 299, 300
Population model, 310
Properties and results, 293
Pure-jump process with bounded
rates, 298
Sojourn times, 301
Standard transition function, 295,
299
State classiﬁcation, 297, 304
State classiﬁcation, via sojourn
times, 301, 302
Stochastic continuity, 301
Strong Markov property, 294
Subordinated Markov chain, transi-
tions and sojourn times, 304
The martingale problem, 296
Transition matrix via generator ma-
trix, 306
Transition semi-group based on
subordinated Markov chain,
298
Transition semigroup, 290
Transition semigroup and distribu-
tions, 291
Martingale
Branching process, 270
Continuous time, 283
Convergence, 284
Convergence theorem by Doob,
217

INDEX
371
Convergence to a last element, 222
Cross-variation, 285
Deﬁnition, 210, 211
Doob decomposition, 223
Doob-Meyer decomposition, 284
Etemadi inequality, 212
Gambling, 213
Gambling, double or nothing, 225
Kolmogorov inequality, 212
Likelihood ratios, 224
Local, 285
Martingales and Lp-convergence,
221
Optimal skipping theorem by Hal-
mos, 214
Optional sampling, 219, 284
Polya urn, 223
Properties and deﬁnitions, 211
Quadratic variation, 285
Radon-Nikodym derivatives, 224
References, 268
Regular, 284
Reverse, 213
Sampling integrability conditions,
218
Semi, 285
Square-integrable, 284
Sub-MG, deﬁnition, 211
Super-MG, deﬁnition, 211
Uniform integrability, 220
Upcrossings, 216
Upcrossings theorem by Doob, 217
Mat´ern class of covariance functions
Deﬁnition, 148
MCMC
for computation of averages, 263
Gibbs sampler, 265
Gibbs sampler algorithm, 265
Metropolis-Hastings algorithm, 264
Metropolis-Hastings Markov chain
construction, 263
Reversible jump, 266
RJMCMC algorithm, 268
Measurability
References, 133
Measurable
Borel mapping, 89
Carath´eodory measurability, 100
Cylinder, 84
Extended real-valued function, 90
Function, 89
Lebesgue, p-dimensional function,
104
Mapping, 88
Object, 143
Rectangles, 85
Set, 86
Simple function, 90
Space, 86
Measurable Space
Isomorphic, 340
of probability measures, 173
Measure
σ-ﬁnite, 92
Absolutely continuous, 92
Atom of a, 93
Continuity from above, 94
Continuity from below, 94
Countable additivity, 91
Countable subadditivity, 94
Counting, 93
Discrete, 92, 93
Extension and uniqueness, 102
Extremal, 262
Finite, 92
Generating function, 228
Inclusion-exclusion formula, 94
Induced outer, 102
Inﬁnite product, 125
Inner probability, 141
Lebesgue, 104
Lebesgue on a ﬁeld, 104
Lebesgue, p-dimensional, 104
Locally ﬁnite, 93
Monotonicity, 94
Mutually singular, 92

372
INDEX
Negative set, 98
Nonatomic, 93
Null sets, 98, 105
on a ﬁeld, 91
Outer, 100
Positive set, 98
preserving transformation (or shift-
invariant), 258
Probability, 92
Product measure space, 124
Properties, 91
Radon, 93
Regularity, 135
Semiﬁnite, 92
Signed, 97
Space, 91
Space, complete, 92
Space, completion of, 105
Total variation, 99
Transformation, 92
Uniqueness, 96
Measure Theory
Incomplete measure space paradox,
105
Nonmeasurable set, 106, 340
Metric Space
Boundary of a set, 341
Closed set, 341
Closure of a set, 341
Complete, 341
Deﬁnition, 340
Dense subset, 341
Euclidean metric, 82
Interior of a set, 341
Locally H¨older-continuous func-
tion, 341
Open set, 341
Open, closed and boundary of a
ball, 341
Polish, 280
Polish space, 190, 341
Separable, 341
Minkowski
Addition, 194
Mixture Distribution
Additive mixture model, 10
Bivariate normal components, 10
Data augmentation, 10
Deﬁnition, 9
Deﬁnition using conditional distri-
butions, 169
Dirichlet processes, 22
Exponential family components, 9
Multiplicative mixture model, 11
References, 22
Moment Generating Function
Exponential family, 8
Moments
Deﬁnition, 7
Probability generating function, 9
via the characteristic function, 8
Monte Carlo Tests
Model adequacy, 62
p-values, 61
Multivariate Analysis
Elliptical family of distributions,
344
References, 23
Multivariate Normal
Complete-suﬃcient statistic, 39
Conjugacy for covariance matrix,
13
Conjugacy for mean vector, 12
Conjugate prior, 12
Joint conjugate prior, 14
UMVUE and MLE of covariance
matrix, 39
UMVUE and MLE of mean vector,
39
Non-Parametric Statistical Inference
References, 66
Nonmeasurable Set, 106, 340
Operator
Diﬀerence, 150
Integral (deterministic), 113

INDEX
373
Laplacian, 330
Second-order partial diﬀerential,
328
Order Statistics
Deﬁnition, 24
p-value
Discussion and criticisms, 63
Poisson Process
Compound, 278
Conditioning, 278
Deﬁnition, 277
First characterization, 278
Non-stationary, 333
Properties, 332
Queueing system arrivals, 278, 292
Second characterization, 278
Stationary, 277
Transition function, 292
via random walk, 277
via times of arrivals, 291
Polya Urn
Deﬁnition, 223
Posterior Distribution
Improper, 39
Posterior expected loss (PEL), 29
Predictive Inference
Introduction, 58
Monte Carlo model adequacy test,
62
Probability
Arithmetic, 3
Inclusion, 4
Measure and space, 139
Random, 167
Rudimentary deﬁnition, 2
Probability Distribution
Conditional, 6
Exponential family, 8
Exponential family (canonical), 9
Laplace, 24
Posterior, 6
Power series family, 69
Prior, 6
Probability density function, 5
Probability mass function, 5
Transformations, 11
Probability Measure
Continuity, 158
Countable additivity, 139
Uniqueness, 142
Uniqueness via characteristic func-
tion, 165
via a cdf, 150
Probability Space
Defective, 139
Produce Measure
Properties, 124
Product Spaces
Deﬁnition, 83
Exponential, 87
Projection function, 83
Topology, 83
Queueing Systems
M/M/1 queue, 278
M/M/∞queue, 279
M/M/k queue, 278, 306
M/M/k queue with balking, 307
M/M/k/m queue, 279
as Markov processes, 292
Concepts, 276
Notation, 276
Poisson arrivals, 278
via birth-death processes, 310
Random
Convolution, 194
Counting measure, 146
Density, 173
Diﬀerential equation, 325
Diﬀusion, strong solution to SDE,
326
Diﬀusion, weak solution to a
stochastic diﬀerential equation,
326
Distribution, 173

374
INDEX
Expectation, 175
Field, Gaussian, 147
Function, continuous, real valued,
145
Integral, as a random variable, 320
Integral, as a stochastic process,
321
Interval, 45
Probability, 170
Sequence adapted to ﬁltration, 210
Test function, 50
Random Field
Gaussian, 147
Random Matrix
Expectation, 160
Random Object
Containment functional, 4
Deﬁnition, 143
Distribution, 143
Generated σ-ﬁeld, 144
Random closed set (RACS), 4, 88
Random Sample, 1
Random Sequence
Adapted to ﬁltration, 210
Bernoulli, 208
Bernoulli, times of successes, 208
Binomial, 208
Birkhoﬀ’s ergodic theorem, 259
Consequences of Birkhoﬀ’s theo-
rem, 261
Deﬁnition, 207
Ergodicity, 259
Ergodicity criterion, 262
Existence of iid random variables,
184
Hopf’s maximal ergodic lemma,
261
Independent increments, 209
m-dependence, 203
of CR
[0,1]-valued random variables,
315
Random walk return times to 0, 209
Random walks, 209
Shift-invariant sets, 259
Shift-invariant transformation, 258
Stationarity, 203
Stationarity of increments, 209
Stationarity of moving averages,
261
Stationary, 258
Stationary non-ergodic sequence,
262
Random Set
Hit-or-miss topology, 88
Random Variable
Deﬁnition, 3
Discrete or continuous, 4
Expectation, 159
Random Vector
Deﬁnition, 3
Expectation, 160
Transformations, 11
Random Walk
Classiﬁcation, 231
Conditional independence, 178,
226
Martingale, 213
Number of visits, 210
Potential sequence, 231
Recurrent and transient, 210
Return times distribution, 210
Return times to state 0, 209
Simple and symmetric, 209
Step and waiting time distributions,
231
Wald identities, 225
Reconciling Bayesian and Frequentist
methods
Bayes factor, 56
Composite null hypothesis, 65
Generalized MLE, 40
Rectangles
as cylinder bases, 84
Bounded, 84
Measurable, 85
Open, 83

INDEX
375
Rational, 84
Renewal Sequence
Classiﬁcation, 230
Criterion, 227
Deﬁnition, 226
Delayed, 230
Distribution of the renewal mea-
sure, 229
Limiting behavior, SLLN, 229
Period, 230
Potential measure, 227
Potential sequence, 227
Regenerative set, 227
Relating the potential measure and
waiting time distribution, 227
Renewal measure, 227
Renewal theorem, 231
Renewal times, 226
Waiting distribution, 226
Robustness Methods
with respect to choice of prior, 14,
21
Sample Space, 1
Sequence of Probability Measures
Tightness, 192
Weak convergence, 192
Weakly convergent, 189
Set Function
Uniformly continuous, 221
Simple Event, 1
Simulation Methods
Discrete mixture of continuous
components, 20
Exponential, 18
Gamma, 18, 19
Goodness of ﬁt (gof) tests, 62
Importance sampling, 206
Inverse transform algorithm, 18
Inverted Wishart distribution, 21
Monte Carlo, 44
Monte Carlo integration, 44
Monte Carlo p-value, 61
Monte Carlo test, 61
Multivariate normal, 20
Non-central chi-square, 20
Normal, 19
pdf plot, 26
References, 23
Rejection algorithm, 18
Wishart distribution, 21
Space
Banach, 131
C`adl`ag, 290
Linear, 130
Polish, examples, 280
Statistical Paradoxes
Discussion, 23
Stochastic Diﬀerence Equation
Deﬁnition, 324
Stochastic Diﬀerential Equation
Cross-variation, 327
Kalman-Bucy ﬁlter, 332
Local martingale problem, 327
Ornstein-Uhlenbeck process, 327
Solving a, 326
Stochastic diﬀerential equations as
stochastic integrals, 325
Stock prices example, 318
Strong solution, 326
Strong solution, strong uniqueness,
326
Weak solution, 326
Weak solution, distributional
uniqueness, 326
Weak solution, pathwise unique-
ness, 326
Weak solutions via the local mar-
tingale problem, 327
Stochastic Integral Equation
Deﬁnition, 325
Stochastic Integration
Basic properties of the integral for
simple stochastic processes,,
319
Conditions on integrands, 318

376
INDEX
Girsanov formula, 323
Integrating simple stochastic pro-
cesses, 319
Integration by parts, 323
Itˆo formula, 323
Itˆo integral as a random variable,
320
Itˆo integral as a stochastic process,
321
Itˆo integral with respect to local
martingale, 322
Kunita-Watanabe inequality, 336
Properties and extensions of the
stochastic integral, 322
Quadratic variation, 322
Simple integrands, 319
Stochastic Partial Diﬀerential Equation
Cauchy, 330
Deﬁnition (SPDEs), 328
Dirichlet, 330
Heat equation, 331
Poisson, 329
Solving a, 329
Stochastic Process
Adapted to ﬁltration, 281
Arrival process, 275
Birth-Death, 309
Bounded variation, 283
Class D and DL, 283
Construction via subordinated
Markov chains, 297, 304
Deﬁnition, 275
Diﬀusion, 324
Finite-dimensional distributions,
280, 285
Increasing, 283
Independent increments, 277
Indistinguishable, 280
Interarrival times, 276
Kolmogorov existence theorem,
286, 288
Kolmogorov’s backward and for-
ward equations, 306
Limiting behavior, 307, 308
Measurable, 279
Modiﬁcation, 279
Poisson, 277
Predictable, 282
RCLL (right-continuous and left-
hand limits), 280, 286
Recursive transition function, 305
Regular, 304
Regularity criteria, 305
Sample path, 280
Simple, 319
Stationarity of increments, 277
Stationary, 308
Stationary distribution, 308
Subordinated Markov chain, 283
Transition function and jump rates,
306
Transition function via subordi-
nated Markov chains, 297
Uniform integrability, 283
Stopping Time
Deﬁnition, 215
Interpretations, 215
Optional, 282
Prior sets, 215
with respect to continuous time
ﬁltration, 282
Tail σ-ﬁeld, 157
Theorem
Basu, 36
Bernstein-von Mises, 205
Birkhoﬀ’s ergodic, 259
Blyth’s lemma, 41
Bochner’s, 163
Borel lemma, 159
Borel-Cantelli lemma, 159
Bounded convergence theorem
(BCT), 112
Carath´eodory, 103
Central limit theorem (CLT), 75
Chebyshev’s inequality, 25

INDEX
377
Cram´er, 185
Delta method, 185
Doob’s, 217
Dynkin’s π −λ, 80
Extension of probability measure,
142
Factorization (Fisher-Neyman), 31
Fatou’s lemma, 112
Finding the UMVUE, 35
Finding the UMVUE (Cramer-Rao
lower bound), 35
Finding the UMVUE (Lehmann-
Scheﬀ´e), 35
Finding the UMVUE (Rao-
Blackwell), 35
Fr´echet-Shohat, 193
Fubini, 127
Fundamental lemma of Neyman-
Pearson, 52
Glivenko-Cantelli, 205
Hahn decomposition, 98
Heine-Cantor, 341
Helly selection, 191
Helly-Bray, 188
Jensen’s inequality, 25
Jordan decomposition, 99
Kochen-Stone lemma, 159
Kolmogorov 0-1 Law, 157
Kolmogorov’s existence, of a prob-
ability distribution, 286
Kolmogorov’s existence, of a prob-
ability space, 288
Kolmogorov’s three series, 200
Kolmorogov-ˇCentsov, 289
Lebesgue decomposition, 93
Lebesgue’s dominated convergence
(DCT), 112
Lebesgue-Stieltjes, 97
Markov’s inequality, 25
Mean value, 338
Monotone convergence theorem
(MCT), 109
Monotone limit of simple func-
tions, 90
Optimal skipping theorem by Hal-
mos, 214
Optional sampling, 219
Parseval relation, 163
Portmanteau, 190
Prohorov’s, 193
Pythagorean, for functions, 132
Radon-Nikodym (R-N), 118
Renewal, 231
Riemann-Lebesgue, 137
Riesz representation, 132, 138
Riesz-Fischer, 131
Sierpi´nski class, 141
Skorohod, 189
Slutsky, 184
Taylor expansion, 337
Tonelli, 126
Tychonoﬀ’s, 83
Uniqueness of measure, 96
Uniqueness of probability measure,
142
Upcrossings theorem by Doob, 217
Wald’s identities, 225
Time Series
Vector autoregressive model, 269
Topological Space
Closure of a set, 338
Compact, 338
Hausdorﬀ, 338
Locally compact, 338
Topology, 338
Hit-or-miss, 88
Uniform Continuity
Deﬁnition, 163
Unit Simplex
Exponential family, 9
Wiener Process
see Brownian motion, 312
With Probability 1, 3

378
INDEX
Zero-One Law
Blumenthal 0-1 law, 336
Kolmogorov, 157, 261
L´evy, 223

