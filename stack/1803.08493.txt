Contextual Salience for Fast and Accurate Sentence Vectors
Eric Zelikman 1 Richard Socher 2
ezelikman@cs.stanford.edu, richard@socher.org
Abstract
Unsupervised vector representations of sentences
or documents are a major building block for
many language tasks such as sentiment classi-
ﬁcation.
However, current methods are unin-
terpretable and slow or require large training
datasets.
Recent word vector-based proposals
implicitly assume that distances in a word em-
bedding space are equally important, regardless
of context.
We introduce contextual salience
(CoSal), a measure of word importance that uses
the distribution of context vectors to normalize
distances and weights. CoSal relies on the insight
that unusual word vectors disproportionately af-
fect phrase vectors. A bag-of-words model with
CoSal-based weights produces accurate unsu-
pervised sentence or document representations
for classiﬁcation, requiring little computation to
evaluate and only a single covariance calculation
to “train.” CoSal supports small contexts, out-of
context words and outperforms SkipThought on
most benchmarks, beats tf-idf on all benchmarks,
and is competitive with the unsupervised state-
of-the-art.
1. Introduction
Global context, a representation of an analyzed text’s na-
ture, such as recognizing that one is reading a movie re-
view, is useful for transfer learning and document interpre-
tation (Lu et al., 2014; Peters et al., 2018). Yet, current
solutions that account for global context are black boxes:
an algorithm takes a document and returns some function
mapping word vectors to a context representations. Method
outputs vary from a vector (McCann et al., 2017) to an-
other deep structure (Peters et al., 2018). This results in
obfuscated approaches to using context that are uninter-
pretable and resistant to further extension. Further, these
1Stanford University 2Salesforce Research. Correspondence
to: Eric Zelikman <ezelikman@cs.stanford.edu>.
Preprint. Copyright 2018 by the author(s).
blue
sky
the
is
The   sky   is    blue
Words
Word Vectors
Paper about nature
Context
Context’s Word 
Vector Distribution
The   sky   is    blue
Vectors
σ
Sentence Vector
The sky is blue
Distancei 
2μ(Distance)
Figure 1. Toy visualization of a sentence vector generated using
contextual salience for the phrase “The sky is blue” in the context
of a paper about nature.
algorithms often still require thousands of sentences in a
transfer dataset to train effectively, especially when unsu-
pervised (McCann et al., 2017). We introduce CoSal, an
approach relying on normalization of distances in a seman-
tic space with respect to the word vector distribution of a
context.
It is generally accepted that words that are common in a
document but rare in a corpus are important to the docu-
ment: tf-idf is a standard technique to evaluate word im-
portance, comparing document word frequency to over-
all corpus frequency, used by at least 70% of text based
recommender-systems (Beel et al., 2016). This premise is
also the basis of the inverse-frequency model in Arora et
al.’s “A Simple but Tough to Beat Baseline for Sentence
arXiv:1803.08493v6  [cs.CL]  2 Nov 2020

Contextual Salience for Fast and Accurate Sentence Vectors
Embeddings,” which remains nearly state-of-the-art (Arora
et al., 2017).
Finding that CoSal correlates with tf-idf in cases where
tf-idf works well (large documents and corpora), we an-
alyze the relationship between the contextual salience of
words and their contribution to a compositional represen-
tation. In order to do this, we build a vector space with
both phrases and words. This space showed a clear rela-
tionship: words that are slightly more contextually salient
than the others in a sentence contribute much more to the
meaning of the sentence than words that are slightly less
contextually salient. Combining this sigmoidal importance
pattern with a weighted bag-of-words for unsupervised sen-
tence and document representations yields better perfor-
mance than widely-used supervised and unsupervised clas-
siﬁcation models on most benchmarks.
Then, for espe-
cially small corpora, we extend CoSal with a method for
augmenting the covariance matrix with that of a broader
context.
Initially, cosine distance was used to measure the distance
between a generated sentence vector and a target sentence
vector, providing numerous advantages over Euclidean dis-
tance (Emmery, 2017). However, cosine distance still im-
plies that all dimensions are equally valuable and uncorre-
lated, calculated as a dot product. Thus, it still produces
context-blind measurements. Noting that M-distance can
be calculated between any two points given an underlying
distribution, an application of the law of cosines is used to
deﬁne a robust and context-aware alternative to cosine dis-
tance, capable of realizing that “cardinal” (A color but also
a metonym for Stanford) and “red” are less related to one
another in the context of news about Stanford research than
in an article about the color green.
2. Background and Related Work
2.1. Mahalanobis Distance
Mahalanobis distance (M-distance) is a metric to ﬁnd a dis-
tance between a point and a distribution or between two
points sampled from a distribution, normalized for devi-
ation and covariance and can be interpreted as a multi-
dimensional z-score (Gnanadesikan & Kettenring, 1972).
The formula, given two points p1 and p2 and distribution
covariance S is:
d(p1, p2, S) =
p
(p1 −p2)T S−1(p1 −p2)
While M-distance has previously been applied in the con-
text of word vectors, the uses have ranged from measuring
distances between “Gaussian word embeddings” 1 (Vilnis
1Word vectors as multivariate Gaussians, for spanning ambi-
guities. The covariance used was the joint covariance of the word
embeddings
Figure 2. A visualization of the sigmoidal relationship between
the contextual salience of a word and its weight in a given sen-
tence.
& McCallum, 2014), to a substitute for cosine distance for
one-shot image recognition2 (Rahman et al., 2017) to rule-
based movie review sentiment analysis (Balasubramanian
et al., 2016) or to help discriminate word sense (De Marn-
effe et al., 2018). Consistently, the measurement is used to
measure the distance between two words in a sample, not
the distance of a word to the distribution.
2.2. tf-idf
Term frequency-inverse document frequency (tf-idf) com-
pares a word’s frequency in a document to its frequency
in a corpus to determine importance. tf-idf comes with a
variety of limitations. First, producing meaningful results
even for common words requires a large corpus. Further,
tf-idf provides no useful results for out-of-corpus tokens,
limiting its usefulness in comparisons. Additionally, tf-idf
is stratiﬁed for rare tokens, resulting in lines of importance
as words get increasingly rare. It also ignores covariance or
similarity in word meaning, and thus is highly word-choice
dependent. One central idea we present in this paper is that
tf-idf works because word frequency happens to be a good
proxy for the unusualness of their meanings relative to their
context.
2.3. Context Representations
Other research has generated context-vectors for words,
calculated by an autoencoder trained on a translation model
(McCann et al., 2017) or used an approach to transform
word vectors based on global context using a biLSTM (Pe-
ters et al., 2017). While techniques exist for small scale
transfer learning (Lu et al., 2014), the learning of global
context (Peters et al., 2018), and multiple-word meaning
word vectors (Huang et al., 2012), this paper provides a
more transparent technique which works on small contexts.
2This technique results in rare words being treated as dis-
tant. For example “dog” and “and” are both common words, and
will generally have a smaller M-distance than “Dachshund” and
“dog,” even unnormalized

Contextual Salience for Fast and Accurate Sentence Vectors
Table 1. Transfer Performance. The unsupervised state-of-the-art sentence embedding transfer learning technique, SkipThought with
normalized layers is listed as ST-LN (Conneau et al., 2017). SkipThought, more widely used, is ST. A biLSTM was the best unsupervised
model in its class (trained on unordered sentences) in (Conneau et al., 2017). Wang refers to Baselines and Bigrams (Wang & Manning,
2012), where NBSVM-bi is the best overall model, interpolating between SVM and multinomial naive Bayes with bigrams. Iyyer is the
Deep Averaging Network (Iyyer et al., 2015). The Global model uses the distance to the global average word vector while Sent uses the
distance to the sentence average word vectors, single-parameter learns the sigmoid steepness for the sentence average model (per task),
while ANN uses the unsupervised embeddings from the sentence average as inputs to a neural network
Evaluation
MR
MPQA
MRPC
SICK
SST
SUBJ
CR
TREC
SentEval
(unsupervised)
tf-idf
73.7
82.4
73.6/81.7
-
-
90.3
79.2
85.0
ST
76.5
86.1
73.0/82.0
82.3
82.0
93.6
80.1
92.2
ST-LN
79.4
89.3
-
79.5
82.9
93.7
83.1
88.4
BiLSTM
77.5
88.7
73.2/81.6
83.4
80.7
89.6
81.3
85.8
Arora (unsupervised, ANN)
-
-
-
84.6
82.2
-
-
85.0
Wang
(sup.)
NBSVM-bi
-
86.3
-
-
-
93.2
81.8
-
SVM-bi
-
86.7
-
-
-
91.7
80.1
-
MNB-bi
-
86.3
-
-
-
93.6
80.0
-
Iyyer (supervised)
-
-
-
84.5
86.3
-
-
-
CoSal (Ours)
Global
78.4
87.7
73.5/81.7
78.6
82.6
92.8
81.5
82.2
Sentence
78.3
88.1
73.9/82.0
78.7
82.7
92.5
81.1
86.2
+ SP
78.4
88.4
74.1/82.3
78.9
82.8
92.5
81.2
86.6
+ ANN
78.4
88.9
74.1/82.3
80.5
82.8
92.6
81.2
89.8
2.4. Arora et al.’s “Smooth Inverse Frequency”
As discussed in (Arora et al., 2017), a frequency-weighted
average of word vectors3 yields a remarkably robust ap-
proximate sentence vector embedding.
However, this
“smooth inverse frequency” (SIF) approach comes with
practical and theoretical limitations.
Practically, calcu-
lating “estimated” word frequency using and performing
common component removal are both indirect measures of
contextual relevance that ignore substantial amounts of in-
formation and are thus not as “well-suited for domain adap-
tation settings” as may be implied.
Most importantly, despite the common component re-
moval, their model still assumes that all distances in all di-
rections in all corpora are equally signiﬁcant. In results of
(Arora et al., 2017), the lower-than-expected performance
on sentiment analysis is attributed to the issue of the dis-
tributional hypothesis: words related to sentiment show
up often together so are close as vectors. A variation to
our sentence model draws from (Arora et al., 2017), incor-
porating the distance to the sentence’s average instead of
the context average, using the context’s covariance. (Arora
et al., 2017)’s calculation of word frequencies from the un-
igram count of words4 also means that their approach does
not work for out-of-vocab words and can’t be generated
3With this average’s projection onto the corpus’ sentence vec-
tors’ ﬁrst principal component removed
4Which they note is calculated using various corpora of online
text including Wikipedia and political blogs
from the word vectors alone, suffering some of the issues
of tf-idf.
3. Method
In short, contextual salience is the distance between two
points in a word vector space when accounting for the cor-
relation and variance of the dimensions of the word’s con-
text. For sentence representations, every word’s weight is
the sigmoid of its contextual salience, relative to the mean
salience of the words in the sentence, though the salience
measure itself is more widely applicable. This is visualized
in Figure 1.
While global context is powerful, that approach alone is
limited when dealing with small contexts: We hope to place
less emphasis on features like tense or plurality that may be
consistent in a text but vary in language or a larger contex-
tual corpus.
For this, mirroring tf-idf, we weight the covariance of the
document and the covariance of a larger corpus, roughly
corresponding to tf and idf respectively. Then, we take
the (element-wise) geometric mean of the weighted co-
variance matrices, as the document distribution is theo-
retically drawn from a speciﬁc subset of the corpus dis-
tribution, and this transformation emphasizes the relation-
ships that are signiﬁcant in both the corpus distribution and
in the weighted document distribution. Letting S’ be the

Contextual Salience for Fast and Accurate Sentence Vectors
weighted covariance S, CoSal is calculated as follows:
CoSal(v) =
q
vT (S′
doc ⊙S′corp)−1v
Like tf-idf, CoSal can be used with arbitrary weighting
schemes; that is, the interpolation applied between the
document and corpus can mirror most tf-idf weighting
schemes.
The recommended weighting scheme, draws
from the common “augmented” (Manning et al., 2009) tf-
idf weighting scheme5, with
+√x as the sign-preserving
square root
S′
corp =
+p
Scorp and S′
doc =
r
|Sdoc| + |Scorp|
2
Conﬁdence
We can also generalize S′
doc =
p
p|Sdoc| + (1 −p)|Scorp|,
where p corresponds to the representativeness of the
smaller context, logarithmically proportional to the num-
ber of words in the context.
Sentence Average
Using the sentence average in place of the global average
improves performance because it makes a useful approxi-
mation: the more important the difference is between the
word and the approximate meaning of the sentence, the
more role that word likely plays in deﬁning the sentence.
This helps account for shortcomings of the distributional
hypothesis (Discussed in 2.4), since dimensional covari-
ances which are typically smaller but prominent in a par-
ticular corpus (For example, sentiment-related words in a
sentiment corpus) will be automatically identiﬁed as im-
portant.
5The “natural” (Manning et al., 2009) scheme, which is essen-
tially just tf, is used in this paper for very large corpora
Algorithm 1 Training: Finds the distribution of the docu-
ment’s word vector cloud
Input: trainingSet, a large document or transfer learn-
ing training set
Result: Returns Mahalanobis metric of training set and
average of training word vectors
vecs =[getV ec(wi) for word wi in trainingSet]
globalAvg = avg(vecs)
metric = MahalanobisMetric(cov(globalCovariance))
Return: globalAvg, metric
4. Performance
4.1. Evaluation
The benchmarks perfomed include subjectivity detection
(SUBJ), product reviews (CR), entailment (SICK), opin-
ion polarity (MPQA), question-type identiﬁcation (TREC),
movie reviews (MR), paraphrase detection (MRPC), se-
mantic similarity (STS) through Facebook’s SentEval
(Conneau et al., 2017) suite of tests as well as the IMDB
review dataset.
To evaluate the quality of the transfer-
learned sentence embeddings, the benchmarks perform a
linear regression classiﬁcation on the returned sentence
vectors. Unlike in previous sections, fastText6 (Mikolov
et al., 2018) is used, which has less predictive word vectors
(Conneau et al., 2017), but supports character-level word
vector prediction for misspelled or esoteric words7. The
downstream tests were performed by feeding the unsuper-
vised sentence embeddings to a neural network for super-
vised classiﬁcation. Because the word vector model con-
tained fewer capitalized words, lowercased performance
was generally better than case-sensitive performance.
4.2. Procedure
The following Algorithms 1 and 2 include the sentence
variation discussed earlier in this paper as well as the global
context only version, which measures to the global aver-
age instead of the sentence average. The sentence variation
approximates the sigmoid linearly, as the distances in the
sentence case tend to cluster more.
4.3. Analysis
Notably, while this model performs comparably or bet-
ter than SkipThought with Layers Normalized (ST-LN) in
almost all tests, and outperforms the more widely used
62m vocabulary 300d word vectors trained w/ Common Crawl
7Patel & Alex (2018) was used for fast vector lookup
Algorithm 2 Sentence embedding algorithm. Returns the
sentence vector in a trained context
Input: sentence: a list of words; globalAvg and metric
from training algorithm;
useGlobal:
a boolean for
whether to compare to the global average or the sen-
tence’s average word vector.
Result: Sentence embedding, weighting words by their
contextual importances
vecs = [getV ec(wi) for word wi in sentence]
avgV ec = globalAvg if useGlobal else unit(avg(vecs))
dists = [metric.dist(vec, avgV ec) for vec in vecs]
dists /=2∗avg(dists)
weights = sigmoid(dists)
Return: sum(weightsi * vecsi for i = 0 to len(vecs))

Contextual Salience for Fast and Accurate Sentence Vectors
Figure 3. Relationship of Mahalanobis distance to tf-idf on the Stanford Treebank Dataset as a large document with 300d GloVe word
vectors, normalized (left) and unnormalized (right)
SkipThought on most (Conneau et al., 2017), it is vastly
more efﬁcient with computationally and data efﬁcient. It
should be some cause for concern
4.3.1. EFFICIENCY
This model requires orders of magnitude fewer sentence
examples than models like SkipThought or the bidirec-
tional LSTM to produce high quality results and no train-
ing beyond that of the underlying word vectors. One ques-
tion is how many samples are needed to establish a context.
This model’s performance exceeds tf-idf’s maximum per-
formance for most tests after only 10 sentences. Further,
every example seems to improve performance logarithmi-
cally. On the MRPC dataset, 99% of the model’s ultimate
accuracy is reached with 60 sentences, with 5, 10 and 30
example sentences resulting in accuracies of 94%, 96% and
97% of the ultimate performance.
On a 2015 Macbook Pro without a GPU, it takes 22ms
per thousand sentences in the training dataset to “train” the
model given unique training dataset word vectors8, requir-
ing less than 200ms for all datasets listed besides IMBD
and SST. On the other hand, on the models for which train-
ing curves are available, where Skipthoughts-LN outper-
forms CoSal-sentence model, it took, on average, approxi-
mately a week of training on more powerful hardware for
them to earn comparable scores(Lei Ba et al., 2016). The
ST-LN model used for comparison in the table was trained
for a month.
4.3.2. STABILITY
Finally, notably, while the single-parameter (SP) adjusted
sigmoids had marginally better test performance in most
8That is, to calculate their dimensional covariance
cases, the size of the difference tended to be small (an av-
erage improvement of < 0.2%). This indicates that this
baseline is robust.
5. Derivation
First, the Mahalanobis distance of words in a corpus was
compared to their tf-idf.
The context used here was
the Stanford Sentiment Analysis Treebank dataset (Socher
et al., 2013), treated as a document for the sake of a
maximal comparison. Both normalized and unnormalized
GloVe word vectors9 (Pennington et al., 2014) were com-
pared, as shown in Figure 3. The normalized approach re-
sulted in substantially better correlation (r2 = 0.496 vs
0.291) between a word’s tf-idf and Mahalanobis distance,
presumably because the length of the original word vec-
tors was correlated with word frequency, skewing the vec-
tor distribution’s density.
We modiﬁed the GloVe algorithm, a widely used approach
to generate word-vectors (Simon, 2017), to create a sin-
gle vector space which contained vectors corresponding to
both two-word phrases and words. We used spaCy (Hon-
nibal & Johnson, 2015) to identify two-word grammatical
phrases, randomly treating a two-word phrase as a single
token 50% of the time, and the remaining times replacing it
with one of the underlying words10. We then calculated the
linear combination of the vectors of the two words compos-
ing a phrase which was closest to the phrase vector. Then
the relationship between the proportion of the ﬁrst word
in the linear combination and the proportion of the ﬁrst
9300d vectors from 42B token Common Crawl
10To prevent the phrase’s sentence position from biasing its
word vector, given the reduced context at the beginning or end
of a sentence

Contextual Salience for Fast and Accurate Sentence Vectors
word of the Mahalanobis distance was plotted to produce
the curve in Figure 5 in the appendix.
To extend the two-word pattern to a sentence embedding
model, the word vector’s distances are rescaled so that their
mean distance corresponds to the sigmoid’s center, as visu-
alized in Figure 2.
6. Conclusion
Contextual salience provides a fast and transparent mecha-
nism to account for the variation in the importance of fea-
tures of a text in differing contexts. While the benchmarks
and methods presented in this paper demonstrate its use-
fulness, we believe CoSal has broad applications and can
be easily integrated into a variety of natural language al-
gorithms. For example, the base contextual salience tech-
nique can likely be augmented with contextual word vec-
tors (Peters et al., 2018) and could be used as an addi-
tional input to a recurrent neural network to account for
order. One extension would be to combine this approach
with ELMo vectors, which generate word vectors for words
based on the contexts in which they appear (Gardner et al.,
2017).
Beyond
accurate
sentence
embeddings
to
augment
language-processing algorithms, a technique to extract
meaning from generated vectors, and new metrics, the
results in this paper may carry implications about the
compositional structure of language. The idea that a rep-
resentation of context allows one to consistently disregard
and overemphasize information when analyzing collective
meanings can be broadly applied. It suggests that, although
all parts play a role in establishing collective meaning of
several related pieces of information, the presence of a few
salient details plays a disproportionate role.
References
Paul erdos (wikipedia), Mar 2018a. URL https://en.
wikipedia.org/wiki/Green.
Green (wikipedia), Mar 2018b.
URL https://en.
wikipedia.org/wiki/Green.
Arora, S., Liang, Y., and Ma, T. A simple but tough-to-beat
baseline for sentence embeddings. 2017.
Balasubramanian, V., Gupta, S., and Veerappagoundar, P.
Mahalanobis distance-the ultimate measure for senti-
ment analysis. 13:252–257, 01 2016.
Beel, J., Gipp, B., Langer, S., and Breitinger, C. Research-
paper recommender systems: a literature survey.
In-
ternational Journal on Digital Libraries, 17(4):305–
338, Nov 2016.
ISSN 1432-1300.
doi:
10.1007/
s00799-015-0156-0. URL https://doi.org/10.
1007/s00799-015-0156-0.
Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and
Bordes, A.
Supervised learning of universal sentence
representations from natural language inference data.
arXiv preprint arXiv:1705.02364, 2017.
De Marneffe, M.-C., Archambeau, C., Dupont, P., and Ver-
leysen, M. Local vector-based models for sense discrim-
ination. 03 2018.
Emmery,
C.
Euclidean vs. cosine distance,
Mar
2017. URL https://cmry.github.io/notes/
euclidean-v-cosine.
Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi,
P., Liu, N. F., Peters, M., Schmitz, M., and Zettlemoyer,
L. S. Allennlp: A deep semantic natural language pro-
cessing platform. 2017.
Gnanadesikan, R. and Kettenring, J. R.
Robust esti-
mates, residuals, and outlier detection with multire-
sponse data.
Biometrics, 28(1):81–124, 1972.
ISSN
0006341X, 15410420. URL http://www.jstor.
org/stable/2528963.
Honnibal, M. and Johnson, M.
An improved non-
monotonic transition system for dependency parsing. In
Proceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pp. 1373–1378,
Lisbon, Portugal, September 2015. Association for
Computational Linguistics. URL https://aclweb.
org/anthology/D/D15/D15-1162.
Huang, E. H., Socher, R., Manning, C. D., and Ng, A. Y.
Improving word representations via global context and
multiple word prototypes. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL), 2012.
Iyyer, M., Manjunatha, V., Boyd-Graber, J., and Daum´e III,
H. Deep unordered composition rivals syntactic methods
for text classiﬁcation. In Association for Computational
Linguistics, 2015.
Lei Ba, J., Kiros, J. R., and Hinton, G. E. Layer Normal-
ization. arXiv e-prints, July 2016.
Lu, Z., Zhu, Y., Pan, S., Xiang, E., Wang, Y., and Yang,
Q.
Source free transfer learning for text classiﬁca-
tion, 2014. URL https://www.aaai.org/ocs/
index.php/AAAI/AAAI14/paper/view/8361.
Manning, C. D., Raghavan, P., and Hinrich, S. Introduction
to information retrieval.
Cambridge University Press,
2009.

Contextual Salience for Fast and Accurate Sentence Vectors
McCann, B., Bradbury, J., Xiong, C., and Socher, R.
Learned in Translation: Contextualized Word Vectors.
ArXiv e-prints, July 2017.
Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and
Joulin, A. Advances in pre-training distributed word rep-
resentations. In Proceedings of the International Con-
ference on Language Resources and Evaluation (LREC
2018), 2018.
Patel, A. and Alex. plasticityai/magnitude: Release 0.1.14,
March 2018. URL https://doi.org/10.5281/
zenodo.1196590.
Pennington, J., Socher, R., and Manning, C. D.
Glove:
Global vectors for word representation. In EMNLP, vol-
ume 14, pp. 1532–1543, 2014.
Peters, M. E., Ammar, W., Bhagavatula, C., and Power,
R. Semi-supervised sequence tagging with bidirectional
language models. ArXiv e-prints, April 2017.
Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
C., Lee, K., and Zettlemoyer, L. Deep contextualized
word representations. ArXiv e-prints, February 2018.
Rahman, S., Khan, S. H., and Porikli, F. A Uniﬁed ap-
proach for Conventional Zero-shot, Generalized Zero-
shot and Few-shot Learning. ArXiv e-prints, June 2017.
Simon,
G.
tensorﬂow-glove,
Mar 2017.
URL
https://github.com/GradySimon/
tensorflow-glove.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C. D., Ng, A., and Potts, C.
Recursive deep models
for semantic compositionality over a sentiment treebank.
2013.
Vilnis, L. and McCallum, A.
Word Representations via
Gaussian Embedding. ArXiv e-prints, December 2014.
Wang, S. and Manning, C. D.
Baselines and bigrams:
Simple, good sentiment and topic classiﬁcation.
In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics:
Short Pa-
pers - Volume 2, ACL ’12, pp. 90–94, Stroudsburg,
PA, USA, 2012. Association for Computational Lin-
guistics. URL http://dl.acm.org/citation.
cfm?id=2390665.2390688.

Contextual Salience for Fast and Accurate Sentence Vectors
Appendix
A. Context-Adjusted Cosine Distance
Due to the context-blind nature of cosine distance, we also suggest an approach to account for the underlying vector
distribution. Treating the CoSal of the difference between the words as the opposite leg of the triangle and treating the
word’s salience measures as the legs, the law of cosines is solved for the cosine of the angle between the words. Where a
and b are the legs (the distance between the word and the average vector) and c is the opposite side (the distance between
the two words), we calculate cosC = a2+b2−c2
2ab
.
This approach corresponds to the following intuition: The more salient the difference between the meanings of two words
is relative to their individual saliences (c/ab), the farther apart the meaning of the two words are. For example, in the
context of an article about a development in Stanford self-driving car tech, “cardinal” and “red” have a distance of 0.943,
while in random excerpts from a Wikipedia article about the color green (wik, 2018b), “cardinal” and “red” have a distance
of 0.909.
B. Short Text Visualization
We include an example of the CoSal weights being applied to a set of sentences, when given a short context about Erdos
in Figure 4.
Figure 4. The global sigmoid weights generated for a short excerpt about Erdos from Wikipedia (wik, 2018a), using only the shown text
as global context and TREC for corpus context, using the recommended weighting with p = 0.2

Contextual Salience for Fast and Accurate Sentence Vectors
C. Hyperparameters
The sigmoid used throughout the paper is motivated by and derived from that in Figure 5, scaled vertically by 0.7, horizon-
tally by 0.11 (slope = 1.58), and the resulting sentence vector is normalized. The single-parameter (SP) sigmoids have the
same vertical scaling, but have steepnesses optimized per-task (CR = 0.11, MR = 0.12, SST = 0.14, SICK = 0.17, MRPC
= 0.18, SUBJ = 0.22, MPQA = 0.24, TREC = 0.26). The neural network used in the ANN results had two hidden layers
with 150 neurons, ReLU activation functions, a dropout probability of 0.3, and layer normalization. As shown in Table 1,
the sensitivity to tuning is low.
Note that the shape of the curve is almost identical when the phrases containing stop words are removed. Stop words can
be approximately calculated as the least important 15% of words by distance.
Figure 5. The proportion of a two-word phrase’s vector that is determined by a given word, as a function of the word’s proportion
of the sum of the two words’ importances., Each two-word phrase was identiﬁed by spaCy (Honnibal & Johnson, 2015) from the
Stanford Sentiment Analysis Treebank (Socher et al., 2013). The proportions are shown with a moving average (gray) and a sigmoid
(black, erf) ﬁt to the data. The y-axis is calculated by ﬁnding the x such that x(v1) + (1 −x)(v2) is as close to the phrase vector as
possible. The x-axis is
d(v1,vavg)
d(v1,vavg)+d(v2,vavg)

