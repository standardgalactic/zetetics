HAL Id: tel-01230851
https://tel.archives-ouvertes.fr/tel-01230851
Submitted on 3 Dec 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.
Advances in computational Bayesian statistics and the
approximation of Gibbs measures
James Ridgway
To cite this version:
James Ridgway.
Advances in computational Bayesian statistics and the approximation of Gibbs
measures.
Statistics [math.ST]. UniversitÃ© Paris Dauphine - Paris IX, 2015.
English.
ï¿¿NNT :
2015PA090030ï¿¿. ï¿¿tel-01230851ï¿¿

UniversitÂ´e Paris-Dauphine
Â´Ecole Doctorale de Dauphine
Centre de Recherche en MathÂ´ematiques de la DÂ´ecision
Th`ese presentÂ´ee par:
James L.P. Ridgway
Pour obtenir le grade de:
Docteur en Mathematiques AppliquÂ´ees
SpÂ´ecialitÂ´e: Statistiques
ADVANCES IN COMPUTATIONAL
BAYESIAN STATISTICS AND THE
APPROXIMATION OF GIBBS MEASURES
Jury composÂ´e de:
M. Christophe ANDRIEU
Bristol University
Rapporteur
M. Olivier CATONI
CNRS CREST-ENSAE
Rapporteur
M. Nicolas CHOPIN
CREST-ENSAE
Directeur de Th`ese
M. Randal DOUC
Telecom SudParis
Examinateur
M. Erwan LE PENNEC
Â´Ecole Polytechnique
Examinateur
M. Christian ROBERT
UniversitÂ´e Paris Dauphine
Examinateur


Remerciements
Je voudrais tout dâ€™abord remercier vivement mon directeur de th`ese, Nicolas
Chopin, pour son engagement, sa disponibilitÂ´e et son enthousiasme contagieux
lors de nos discussions scientiï¬ques. Pour toutes ces raisons, travailler sous sa
direction a Â´etÂ´e une exprience tr`es agrÂ´eable et enrichissante.
Jâ€™adresse Â´egalement mes remerciements `a Randal Douc, Erwan Le Pennec et
Christian Robert dâ€™avoir acceptÂ´e de participer `a mon jury de th`ese ainsi quâ€™`a
Christophe Andrieu et Olivier Catoni pour avoir acceptÂ´e de la rapporter et pour
leur lecture attentive et leurs commentaires constructifs.
Je remercie les doctorants ayant sÂ´ejournÂ´e au bureau E28: AdÂ´elaÂ¨Ä±de, Edwin, JB
et Mathieu ainsi que ceux du CREST et de Dauphine: Clara, Marco, Medhi, Pierre
et Vincent qui ont contribuÂ´e `a rendre ces trois annÂ´ees agrÂ´eables.
Ma gratitude va Â´egalement au CREST pour son environnement de travail tr`es
stimulant. Les chercheurs, notamment Pierre Alquier, Arnak Dalalyan et Judith
Rousseau, ont toujours t disponibles et mâ€™ont beaucoup appris sur le plan scien-
tiï¬que. Merci aussi au CREST dâ€™avoir mis la salle cafÂ´e `a cË†otÂ´e de notre bureau,
dÂ´ecuplant ainsi notre productivitÂ´e!
Ma reconnaissance va Â´egalement `a mes parents et `a Vicky pour leur soutien et
les dimanches soirs passÂ´es en leur compagnie. Je pense aussi `a mes amis (partic-
uli`erement Zago, Seb, Yann et Khalid ) grË†ace auxquels jâ€™ai passÂ´e de bons moments
tout au long de cette aventure. Je nâ€™oublie pas non plus les joueurs RC Val de
Bi`evre pour toutes les victoires qui ont rythmÂ´e ces trois derni`eres annÂ´ees. Enï¬n,
je dÂ´edie ce mÂ´emoire de th`ese `a CÂ´eline dont le soutien ne mâ€™a jamais fait dÂ´efaut.
3


Contents
1
Introduction
1
1.1
Bayesian statistics
. . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Risk minimizer
. . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.2
Model choice
. . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
PAC-Bayesian Bounds
. . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2.1
Minimizing the bound
. . . . . . . . . . . . . . . . . . . . .
6
1.2.2
PAC Bayesian oracle inequality . . . . . . . . . . . . . . . .
7
1.3
Computational aspects . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.1
Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.2
Approximate Inference . . . . . . . . . . . . . . . . . . . . .
22
1.4
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.4.1
Leave Pima indians alone: binary regression as a benchmark
for Bayesian computation
. . . . . . . . . . . . . . . . . . .
27
1.4.2
Computation of Gaussian orthant probabilities in high di-
mension . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.4.3
Theoretical and computational aspects of PAC Bayesian rank-
ing and scoring . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.4.4
Properties of variational approximations of Gibbs posteriors
29
1.4.5
Towards automatic calibration of SMC2 . . . . . . . . . . . .
30
2
Leave Pima indians alone: binary regression as a benchmark for Bayesian
computation
31
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.2
Preliminaries: binary regression models . . . . . . . . . . . . . . . .
33
2.2.1
Likelihood, prior
. . . . . . . . . . . . . . . . . . . . . . . .
33
5

Contents
2.2.2
Posterior maximisation (Gaussian prior)
. . . . . . . . . . .
34
2.2.3
Posterior maximisation (Cauchy prior) . . . . . . . . . . . .
35
2.3
Fast approximation methods . . . . . . . . . . . . . . . . . . . . . .
36
2.3.1
Laplace approximation . . . . . . . . . . . . . . . . . . . . .
36
2.3.2
Improved Laplace, connection with INLA . . . . . . . . . . .
37
2.3.3
The EM algorithm of Gelman et al. [2008] (Cauchy prior)
.
38
2.3.4
Expectation-Propagation . . . . . . . . . . . . . . . . . . . .
38
2.3.5
Discussion of the diï¬€erent approximation schemes . . . . . .
40
2.4
Exact methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.4.1
Our gold standard: Importance sampling . . . . . . . . . . .
41
2.4.2
Improving importance sampling by Quasi-Monte Carlo . . .
43
2.4.3
MCMC
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.4.4
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . .
49
2.5
Numerical study . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.5.1
Datasets of moderate size
. . . . . . . . . . . . . . . . . . .
51
2.5.2
Bigger datasets . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.6
Variable selection . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.6.1
SMC algorithm of SchÂ¨afer and Chopin [2011] . . . . . . . . .
61
2.6.2
Adaptation to binary regression . . . . . . . . . . . . . . . .
62
2.6.3
Numerical illustration
. . . . . . . . . . . . . . . . . . . . .
62
2.6.4
Spike and slab . . . . . . . . . . . . . . . . . . . . . . . . . .
64
2.7
Conclusion and extensions . . . . . . . . . . . . . . . . . . . . . . .
64
2.7.1
Our main messages to users . . . . . . . . . . . . . . . . . .
64
2.7.2
Our main message to Bayesian computation experts . . . . .
65
2.7.3
Big data and the p3 frontier . . . . . . . . . . . . . . . . . .
65
2.7.4
Generalising to other models . . . . . . . . . . . . . . . . . .
66
3
Computation of Gaussian orthant probabilities in high dimension
67
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
3.2
Geweke-Hajivassiliou-Keane (GHK) simulator . . . . . . . . . . . .
69
3.3
The Markovian case . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.3.1
Toy example . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.3.2
Particle ï¬lter (PF) . . . . . . . . . . . . . . . . . . . . . . .
71
3.4
Non Markovian case
. . . . . . . . . . . . . . . . . . . . . . . . . .
75
3.4.1
Variable ordering . . . . . . . . . . . . . . . . . . . . . . . .
76
3.4.2
A sequential Monte Carlo (SMC) algorithm
. . . . . . . . .
77
3.4.3
Move steps
. . . . . . . . . . . . . . . . . . . . . . . . . . .
79
3.5
Extentions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.5.1
Student Orthant
. . . . . . . . . . . . . . . . . . . . . . . .
83
3.5.2
SMC as a truncated distribution sampler . . . . . . . . . . .
84
6

Contents
3.6
Numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
3.6.1
Covariance simulation, tunning parameters . . . . . . . . . .
85
3.6.2
GHK for moderate dimensions . . . . . . . . . . . . . . . . .
86
3.6.3
High dimension orthant probabilities . . . . . . . . . . . . .
88
3.6.4
Student orthant probabilities
. . . . . . . . . . . . . . . . .
89
3.6.5
Application to random utility models . . . . . . . . . . . . .
89
3.7
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
Appendices
92
3.A Proof of proposition 2.1
. . . . . . . . . . . . . . . . . . . . . . . .
92
3.B Resampling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.C Variable Ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.D Hamiltonian Monte Carlo
. . . . . . . . . . . . . . . . . . . . . . .
95
4
Theoretical and computational aspects of PAC Bayesian ranking and
scoring
97
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
4.2
Theoretical bounds from the PAC-Bayesian Approach . . . . . . . .
98
4.2.1
Notations
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4.2.2
Assumptions and general results . . . . . . . . . . . . . . . .
99
4.2.3
Independent Gaussian Prior . . . . . . . . . . . . . . . . . . 100
4.2.4
Spike and slab prior for feature selection . . . . . . . . . . . 101
4.3
Practical implementation of the PAC-Bayesian approach
. . . . . . 101
4.3.1
Choice of hyper-parameters
. . . . . . . . . . . . . . . . . . 101
4.3.2
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . . 102
4.3.3
Expectation-Propagation (Gaussian prior) . . . . . . . . . . 104
4.3.4
Expectation-Propagation (spike and slab prior)
. . . . . . . 105
4.4
Extension to non-linear scores . . . . . . . . . . . . . . . . . . . . . 106
4.5
Numerical Illustration
. . . . . . . . . . . . . . . . . . . . . . . . . 106
4.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
Appendices
110
4.A PAC-Bayes bounds for linear scores . . . . . . . . . . . . . . . . . . 110
4.A.1
Suï¬ƒcient condition for Dens(c)
. . . . . . . . . . . . . . . . 110
4.A.2
Proof of Lemma 2.1 . . . . . . . . . . . . . . . . . . . . . . . 110
4.A.3
Proof of Theorem 2.3 (Independent Gaussian prior) . . . . . 114
4.A.4
Proof of Theorem 2.4 (Independent Gaussian prior) . . . . . 115
4.A.5
Proof of Theorem 2.5 (Spike and slab prior for feature selec-
tion) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
4.B Practical implementation of the PAC-Bayesian approach
. . . . . . 117
4.B.1
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . . 117
7

Contents
4.B.2
Expectation-Propagation (Gaussian prior) . . . . . . . . . . 117
4.B.3
Expectation-Propagation (spike and slab prior)
. . . . . . . 119
4.C Numerical illustration . . . . . . . . . . . . . . . . . . . . . . . . . . 120
5
Properties of variational approximations of Gibbs posteriors
123
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
5.2
PAC-Bayesian framework . . . . . . . . . . . . . . . . . . . . . . . . 125
5.3
Numerical approximations of the pseudo-posterior . . . . . . . . . . 127
5.3.1
Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.3.2
Variational Bayes . . . . . . . . . . . . . . . . . . . . . . . . 127
5.4
General results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.4.1
Bounds under the Hoeï¬€ding assumption
. . . . . . . . . . . 129
5.4.2
Bounds under the Bernstein assumption
. . . . . . . . . . . 130
5.5
Application to classiï¬cation
. . . . . . . . . . . . . . . . . . . . . . 131
5.5.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.5.2
Three sets of Variational Gaussian approximations
. . . . . 132
5.5.3
Theoretical analysis . . . . . . . . . . . . . . . . . . . . . . . 132
5.5.4
Implementation and numerical results . . . . . . . . . . . . . 134
5.6
Application to classiï¬cation under convexiï¬ed loss . . . . . . . . . . 134
5.6.1
Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . 135
5.6.2
Numerical application
. . . . . . . . . . . . . . . . . . . . . 136
5.7
Application to ranking . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.7.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.7.2
Theoretical study . . . . . . . . . . . . . . . . . . . . . . . . 139
5.7.3
Algorithms and numerical results . . . . . . . . . . . . . . . 140
5.8
Application to matrix completion . . . . . . . . . . . . . . . . . . . 141
5.8.1
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.9
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
Appendices
145
5.A Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.A.1
Preliminary remarks
. . . . . . . . . . . . . . . . . . . . . . 145
5.A.2
Proof of the theorems in Subsection 5.4.1 . . . . . . . . . . . 146
5.A.3
Proof of Theorem 5.4.3 (Subsection 5.4.2)
. . . . . . . . . . 148
5.A.4
Proofs of Section 5.5 . . . . . . . . . . . . . . . . . . . . . . 149
5.A.5
Proofs of Section 5.6 . . . . . . . . . . . . . . . . . . . . . . 151
5.A.6
Proofs of Section 5.7 . . . . . . . . . . . . . . . . . . . . . . 153
5.A.7
Proofs of Section 5.8 . . . . . . . . . . . . . . . . . . . . . . 155
5.B Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . 156
5.B.1
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . . 156
5.B.2
Optimizing the bound
. . . . . . . . . . . . . . . . . . . . . 158
8

Contents
5.C Stochastic gradient descent . . . . . . . . . . . . . . . . . . . . . . . 160
6
Towards the automatic calibration of the number of particles in SMC2161
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
6.2
Background on SMC2 . . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.2.1
IBIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.2.2
SMC2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
6.2.3
PMCMC moves . . . . . . . . . . . . . . . . . . . . . . . . . 166
6.2.4
Choosing Nx
. . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.3
Proposed approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
6.3.1
Particle Gibbs and memory cost . . . . . . . . . . . . . . . . 168
6.3.2
Nonparametric estimation of Nx . . . . . . . . . . . . . . . . 169
6.3.3
Additional considerations
. . . . . . . . . . . . . . . . . . . 170
6.4
Numerical example . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
9


ResumÂ´e
Ce mÂ´emoire de th`ese traite de plusieurs mÂ´ethodes de calcul dâ€™estimateur en statis-
tiques bayÂ´esiennes. Le premier chapitre consiste en une br`eve introduction des
th`ematiques abordÂ´ees. Nous en donnons ici une premi`ere vue dâ€™ensemble.
Plusieurs approches dâ€™estimation seront considÂ´erÂ´ees dans ce manuscrit. Dâ€™abord
en estimation nous considÂ´ererons une approche standard dans le paradigme bayesien
en utilisant des estimateurs sous la forme dâ€™intÂ´egrales par rapport `a des lois a poste-
riori. Dans un deuxi`eme temps nous relacherons les hypoth`eses faites dans la phase
de modÂ´elisation. Nous nous intÂ´eresserons alors `a lâ€™Â´etude dâ€™estimateurs rÂ´epliquant
les propriÂ´etÂ´es statistiques du minimiseur du risque de classiï¬cation ou de ranking
thÂ´eorique et ceci sans modÂ´elisation du processus gÂ´enÂ´eratif des donnÂ´ees.
Dans les deux approches, et ce malgrÂ´e leur dissemblance, le calcul numÂ´erique des
estimateurs nÂ´ecessite celui dâ€™intÂ´egrales de grande dimension. La plus grande partie
de cette th`ese est consacrÂ´ee au dÂ´eveloppement de telles mÂ´ethodes dans quelques
contextes spÂ´eciï¬ques.
Nous diviserons les algorithmes en deux grandes classes. Dâ€™abord les algorithmes
de Monte Carlo basÂ´es sur la gÂ´enÂ´eration de variables alÂ´eatoires dans lâ€™Â´epigraphe de
lâ€™intÂ´egrande. Ces derniers permettent le calcul dâ€™intÂ´egrales dans la mesure ou nous
arrivons `a gÂ´enÂ´erer eï¬ƒcacement de tels points. Le chapitre 2 compare certaines
des mÂ´ethodes couramment utilisÂ´ees pour lâ€™estimation bayÂ´esienne de mod`eles pro-
bit. Nous y donnons des recommandations sur la mÂ´ethodologie `a adopter. Dans
le chapitre 3 nous dÂ´evelopperons un algorithme permettant le calcul de proba-
bilitÂ´es gaussiennes de rectangles de grandes dimensions.
Ce chapitre traite un
probl`eme plus gÂ´eneral que celui de lâ€™estimation, cependant, nous pouvons lier la
problÂ´ematique `a celle de lâ€™Â´evaluation de certaines vraisemblances.
La deuxi`eme classe dâ€™algorithme que nous considÂ´erons consiste en lâ€™approximation
de distributions par des distributions dont les moments peuvent Ë†etre calculÂ´es ex-
11

Contents
plicitement. Il sâ€™agira dans ce cas de dÂ´eï¬nir une mÂ´etrique et de trouver les ap-
proximations les plus proches dans une classe donnÂ´ee. Dans le chapitre 4 nous
Â´etudierons les proprietÂ´es des mesures de Gibbs pour rÂ´epliquer les propriÂ´etÂ´es du
minimiseur dâ€™un risque de ranking. Nous dÂ´evelopperons Â´egalement des mÂ´ethodes
dâ€™approximation pour ces lois. Dans le chapitre 5 nous nous intÂ´eresserons plus
spÂ´eciï¬quement `a une mani`ere dâ€™approcher les distributions et nous Â´etudierons
les propriÂ´etÂ´es theoriques des approximations elles-mË†emes, i.e. leurs capacitÂ´es `a
rÂ´epliquer des propriÂ´etÂ´es du minimiseur du risque.
Une description plus dÂ´etaillÂ´ee de chaque chapitre est donnÂ´ee en ï¬n dâ€™introduction.
12

1
Introduction
In this thesis we study some computational aspects of Bayesian statistics, as well as
Gibbs posteriors. We describe both statistical approaches in the ï¬rst two sections.
We then give a brief overview of computational aspects linked to the implemention
of those estimators in Section 1.3 of this chapter.
This chapter is devoted to
discussing the diï¬€erent issues that will arise in the manuscript; detailed accounts
will be found in the following chapters.
1.1 Bayesian statistics
Statistical analysis starts with a collection of probability distributions PÎ¸ indexed
by a parameter Î¸ âˆˆÎ˜, where Î˜ is an arbitrary set. In this thesis most examples will
be taken from parametric statistics, with Î˜ âŠ‚Rd. The statistician then confronts
the model to some observations on the probability space (X, A, {PÎ¸, Î¸ âˆˆÎ˜}).
The goal of this thesis is not the choice of the collection of probability distribu-
tion but rather, given the model and the data, to discuss ways of improving the
computation of estimators of Î¸.
We describe in the following the Bayesian paradigm that will be used in this
manuscript. In particular we discuss the minimization of the integrated posterior
risk. As we will see, this criterion leads to computational diï¬ƒculties because it
requires the evaluation of high dimensional integrals.
In Section 1.2, we will take another approach: we will not suppose a speciï¬c
probability model but rather minimize an empirical risk over classes of classiï¬ers
and give theoretical guaranties for such an approach.
In all the following we suppose the model to be dominated by some measure
and we deï¬ne the likelihood as the probability density of the observation given the
parameter.
1

1 Introduction
Deï¬nition 1.1.1 We call likelihood the probability density of the collection (X1:n) âˆˆ
X n conditioned on Î¸. We denote it by L(X1:n|Î¸).
We give two examples of models that will be used in the rest of the thesis. Those
models will be used in the introduction as examples and will be further studied in
the diï¬€erent chapters of this thesis.
Example 1.1.1 Probit. This is a special case of generalized linear models where
the probability of a binary random variable is expressed as a linear combination of
some covariates. Conditionally on Î¸ and a vector of deterministic covariates x the
model can be expressed in a hierarchical form:
Yi = âœ¶Zi>0,
Zi|Î² âˆ¼N(xiÎ², 1).
Each observation is sampled independently from this model. The likelihood is given
by
L(y1:n|Î¸) =
n
Y
i=1
(Î¦(xiÎ¸))yi (1 âˆ’Î¦(xiÎ¸))1âˆ’yi .
This model will be studied in details in Chapter 2.
In Chapter 3 we give an
example of a model where we allow correlations between the (Zi)i and a way to
compute the likelihood.
Example 1.1.2 State space model (SSM). SSM is another model that we will use
in this thesis (see Chapter 6). This is a time series model with an unobserved
Markov process (xt)tâ‰¥0 with transition density
x0 âˆ¼g0(.; Î¸);
xt|x1:tâˆ’1 âˆ¼gt(.|xtâˆ’1; Î¸)
and an observation yt at each time t depending on the current value of the chain,
yt âˆ¼ft(.|xt; Î¸).
The likelihood of the observations up to time T is given by
L(y1:T; Î¸) =
Ë†
TY
t=1
f(yt|xt; Î¸)gt(xt|xtâˆ’1; Î¸)Âµ0(x0)dx0:T.
The fact that the likelihood takes the form of an integral with respect to a Markov
process leads to computational issues. Those will be discussed in more details in
Chapter 6.
2

1.1 Bayesian statistics
The main idea of the Bayesian paradigm is to endow Î˜ with the structure of a
probability space (Î˜, B, Ï€) where the probability measure Ï€ is referred to as the
prior. We do not discuss how to choose Ï€; discussion of this subject can be found
in Robert [2007].
Once we have deï¬ned the prior, we can use the likelihood as a conditional
probability and using Bayes identity one can deï¬ne the posterior distribution:
Deï¬nition 1.1.2 The posterior distribution is the distribution of the parameter
given the observations,
Ï€(dÎ¸|D) =
L(x1:n|Î¸)Ï€(dÎ¸)
Â´
Î˜ L(x1:n|Î¸â€²)Ï€(dÎ¸â€²).
We call the integral mÏ€(x1:n) :=
Â´
Î˜ L(x1:n|Î¸â€²)Ï€(dÎ¸â€²) the marginal likelihood.
The posterior is the distribution of the parameter given a ï¬xed observed sam-
ple. We derive in the following section criteria to choose an estimator using this
distribution.
1.1.1 Risk minimizer
Given a model {PÎ¸, Î¸ âˆˆÎ˜}, the data and a prior Ï€(dÎ¸), with support Î˜, the goal
of the statistician is to take a decision Î´ minimizing a risk. Several types of risk
can be considered and lead to diï¬€erent estimators. We now give examples that are
used in this manuscript.
Once we have deï¬ned the risk one cannot simply minimize over value of â„“because
of the unknown Î¸.
To deal with this issue within the Bayesian paradigm the
statistician deï¬nes the posterior risk.
Deï¬nition 1.1.3 The Posterior risk is the loss function integrated with respect to
the posterior distribution:
Ï(Ï€, Î´|D) =
Ë†
â„“(Î¸, Î´)Ï€(dÎ¸|D).
A Bayesian estimator is a minimizer of the posterior risk. The most usual loss
is the quadratic loss â„“(Î´, Î¸) = âˆ¥Î¸ âˆ’Î´âˆ¥2
2; direct computation yields Î´Ï€ = EÏ€ (Î¸|D).
An other common loss is the following 0-1 loss,
â„“(Î¸, Î´) = âœ¶Î˜1(Î¸)âœ¶Î´=1 + âœ¶Î˜2(Î¸)âœ¶Î´=0.
Direct computations yield:
Î´Ï€ =
(
1 if PÏ€(Î¸ âˆˆÎ˜1|D) > PÏ€(Î¸ âˆˆÎ˜2|D)
0 otherwise
3

1 Introduction
In the light of those examples we see the main computational issue appear.
Those two examples show that the paradigm leads to non trivial integration prob-
lems. In both case one wants to compute:
Â´
Î˜ h(Î¸)Ï€(dÎ¸)L(Î¸|D)
Â´
Î˜ Ï€(dÎ¸)L(Î¸|D)
,
for some function h; i.e. an expectation with respect to a distribution that is
known only up to the normalizing constant mÏ€(D) =
Â´
Î˜ Ï€(dÎ¸)L(Î¸|D).
1.1.2 Model choice
We can encode model uncertainty by deï¬ning the parameter space Î˜ = Q
j Î˜jÃ—{j}
where {j} indexes the model and Î˜j is the parameter space of models j. The
standard approach in the Bayesian paradigm is to endow the space of model indexes
with a prior distribution and to treat it as an additional parameter. This is easily
encapsulated in the framework described in the previous section.
Most of the time the likelihood itself will not be tractable in this case. We can
still solve the problem using specially tailored MCMC algorithms [Green, 1995]. In
Chapter 2 we will propose an alternative to this for the case of covariate uncertainty
for probit models.
We have deï¬ned the normalizing constant of the posterior in Deï¬nition 1.1.2 as
mÏ€(x) =
Â´
Ï€(Î¸)L(x|Î¸)dÎ¸. We write the likelihood of model j, mÏ€(x|j). We assign
a prior probability Ï€j to model j. We end up with an integration problem with
respect to the following distribution,
Ï€(j|D) =
Ï€jmÏ€(x|j)
P
j Ï€jmÏ€(x|j).
For covariate selection in a Gaussian linear model one can compute the marginal
likelihood provided that the prior is conjugate to the Gaussian family (i.e. a Gaus-
sian distribution). An example of the approach is given in Chopin and Shaeï¬€er
[2011]. In general the marginal likelihood is not tractable. A way to get around
this issue is to replace it by an estimator (see Chapter 2 for an application to the
probit model).
1.2 PAC-Bayesian Bounds
When no clear mathematical model is available to the statistician or that existing
models are too costly to evaluate or to build, one may still be able to construct
a pseudo-posterior to replicate the behavior of the risk minimizer. We use for
4

1.2 PAC-Bayesian Bounds
this matter a Gibbs posterior (deï¬ned bellow) for the chosen risk. For this pos-
terior we prove nonasymptotic bound. The idea originated in machine learning
(Shawe-Taylor and Williamson [1997],McAllester [1998]) as way to bound the the-
oretical risk in probability by a computable empirical quantity. We follow Catoni
[2007] and use the approach to get oracle inequalities on the risk integrated under
our given pseudo-posterior.
We observe a sample (X1, Y1), Â· Â· Â· , (Xn, Yn) taking values in X Ã— Y where the
pairs (Xi, Yi) have the same distribution P.
The statistician deï¬nes a set of predictor {fÎ¸ : X â†’Y}. We suppose we also
have at our disposal a risk function R(Î¸) and its empirical counterpart rn(Î¸).
The approach is summarized for classiï¬cation in this section. That is we take
Y = {âˆ’1, 1}, R(Î¸) = P (Y fÎ¸(X) â‰¤0) and rn(Î¸) = 1
n
Pn
i=1 âœ¶YifÎ¸(Xi)â‰¤0. In the fol-
lowing we also suppose linear classiï¬ers fÎ¸(x) = 2âœ¶<x,Î¸>â‰¥0 âˆ’1 âˆˆ{âˆ’1, 1}. More
details on the derivation of the estimator and theoretical results are given in Chap-
ter 4 and 5.
Deï¬nition 1.2.1 The Gibbs posterior for an empirical risk rn(Î¸) and a prior Ï€Î¾
is given by
Ë†ÏÎ»(dÎ¸) =
1
ZÎ»,Î¾
exp {âˆ’Î»rn(Î¸)} Ï€Î¾(dÎ¸)
where Î¾ is the vector of hyperparameters, and ZÎ¾,Î» =
Â´
Î˜ eâˆ’Î»rn(Î¸)Ï€Î¾(dÎ¸).
Contrarily to the posterior deï¬ned in the preceding section we do not derive this
probability density from Bayesâ€™ formula. The Gibbs posterior deï¬ned in deï¬nition
1.2.1 is the solution of the following variational problem:
âˆ’log ZÎ»,Î¾ = inf
ÏâˆˆM1
+
{Î»Ï(rn(Î¸)) + K(Ï, Ï€Î¾)} ,
where M1
+ is the set of all probability measures. This is easily deduced from the
following equality: for any Ï âˆˆM1
+
âˆ’log ZÎ»,Î¾ + K(Ï, Ë†ÏÎ») = Î»Ï(rn(Î¸)) + K(Ï, Ï€Î¾).
Part of this thesis is concerned with proving oracle inequalities for those mea-
sures and providing approximations to the Gibbs posterior.
We give a quick
overview of an approach to derive bounds for the expected risk under a Gibbs
posterior. We follow Catoni [2007] and refer the reader to this reference for a
complete account and tighter bounds.
We give a simpliï¬ed proof for the classiï¬cation loss, to give an idea of the tools
used in Chapter 4 and 5. In Chapter 4 we will extend these results to the case of
an AUC loss. The loss will be used to propose a method to rank instances from
bipartite data.
5

1 Introduction
Using the above relation we derive the following results,
P
(
sup
ÏâˆˆM1
+
Î» (Ï(rn(Î¸)) âˆ’Ï(R(Î¸))) + K(Ï, Ï€Î¾) + Î· â‰¥0
)
= P {log Ï€Î¾ [exp {âˆ’Î»(rn(Î¸) âˆ’R(Î¸)) + Î·}] â‰¥0}
= P {Ï€Î¾ [exp {âˆ’Î»(rn(Î¸) âˆ’R(Î¸)) + Î·}] â‰¥1}
â‰¤PÏ€Î¾ [exp {âˆ’Î»(rn(Î¸) âˆ’R(Î¸)) + Î·}]
= Ï€Î¾P [exp {âˆ’Î»(rn(Î¸) âˆ’R(Î¸)) + Î·}] .
Since rn(Î¸) is bounded from above, Hoeï¬€dingâ€™s inequality allows us to write:
Ï€P [exp {âˆ’Î»(rn(Î¸) âˆ’R(Î¸)) + Î·}] â‰¤Ï€

exp
 Î»2
2n + Î·

see Lemma 2.2 an Theorem 2.1 in Boucheron et al. [2013]. A great part of the
eï¬€ort will be put on ï¬nding concentration inequality at this step. In Chapter 4
for instance we prove a Bernstein inequality for the AUC risk to this aim.
We put Î· = âˆ’Î»2
2n âˆ’log 2
Ç« and get âˆ€Ï âˆˆM1
+ with probability less than Ç«/2:
Î» (Ï(R(Î¸)) âˆ’Ï(rn(Î¸))) + K(Ï, Ï€) âˆ’Î»2
2n âˆ’log 2
Ç« â‰¥0
(1.1)
such that with probability at least 1âˆ’Ç«/2 we can upper bound the theoretical risk
under the Gibbs posterior
Ë†ÏÎ»(RÎ¸) â‰¤inf
ÏâˆˆM1
+

Ï(rn(Î¸)) + 1
Î»K(Ï, Ï€)

+ Î»
2n + 1
Î» log 2
Ç«.
(1.2)
Chapter 4 will be concerned with extensions of the framework to rank data, and
with computational tools to compute approximations of the Gibbs posterior.
Notice that this intermediate result comes under the only hypothesis that the
data is iid.
In fact we can even weaken this assumption by taking a weakly
dependent sample (see for instance Alquier and Li [2012]).
1.2.1 Minimizing the bound
Another quantity of interest is the normalizing constant. In the framework de-
scribed previously we can write inequality (1.2) using the dual formulation intro-
duced in the beginning of the section w.p. at least 1 âˆ’Ç«
Ë†Ï(R(Î¸)) â‰¤âˆ’1
Î» log ZÎ»,Î¾ + Î»
2n + 1
Î» log 2
Ç«.
6

1.2 PAC-Bayesian Bounds
Several authors have proposed to use this bound not only to give an empirical
bound on the true risk but also for estimation. That is, choose the hyper-parameter
of the prior or an approximation of the model, such that the empirical bound is
the tightest. The log-normalizing constant is not always easy to compute, however
we show in the following sections some tools to ï¬nd an unbiased estimator of this
integral. In Chapter 5 we show how to replace the optimal measure by the optimal
measure in a smaller class of distribution for which the integral is tractable.
1.2.2 PAC Bayesian oracle inequality
We call oracle some estimator based on an unobservable quantity. In most of this
thesis it will be given by the minimizer of the theoretical risk.
Here we write
Â¯Î¸ = arg minÎ¸âˆˆÎ˜ R(Î¸). From equation 1.1 we get simultaneously âˆ€Ï âˆˆM1
+(Î˜) with
probability 1 âˆ’Ç«,
Ï(R(Î¸)) â‰¤Ï(rn(Î¸)) + 1
Î»K(Ï, Ï€Î¾) + Î»
2n + 1
Î» log 2
Ç«,
Ï(rn(Î¸)) â‰¤Ï(R(Î¸)) + 1
Î»K(Ï, Ï€Î¾) + Î»
2n + 1
Î» log 2
Ç«.
Specifying Ï as the Gibbs posterior in the ï¬rst of the two equations and replacing
Ï(rn) by its upper bound one gets with probability 1 âˆ’Ç«,
Ë†ÏÎ»(R(Î¸)) â‰¤inf
ÏâˆˆM+
1

Ï(R(Î¸)) + 2
Î»K(Ï, Ï€Î¾)

+ Î»
n + 2
Î» log 2
Ç«.
The inequality gives rise to a bound on the integrated theoretical risk. To obtain
a speciï¬c bound and introduce the oracle risk we will introduce an additional
assumption.
Deï¬nition 1.2.2 There exists a constant c > 0 such that âˆ€(Î¸, Î¸â€²) âˆˆÎ˜2 with
âˆ¥Î¸âˆ¥= âˆ¥Î¸â€²âˆ¥= 1 we have P (< X, Î¸ >< X, Î¸â€² >â‰¤0) â‰¤câˆ¥Î¸ âˆ’Î¸â€²âˆ¥.
The assumption implies that for any two given linear classiï¬ers we can control
the amount of points that are classiï¬ed diï¬€erently. More speciï¬cally, suppose that
X admits a density, with respect to the d âˆ’1 spherical measure, upper bounded
by B then we can show,
P {< X, Î¸ >< X, Î¸â€² >â‰¤0} â‰¤B
2Ï€ arccos(< Î¸, Î¸â€² >)
â‰¤B
2Ï€
r
5
2âˆ¥Î¸ âˆ’Î¸â€²âˆ¥
7

1 Introduction
Î¸1
X
Î¸2
X
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
8
10
12
14
2
4
6
8
Figure 1.1: Illustration of the assumption
The hypothesis imposes that for any two classiï¬ers there is enough mass in the region where their
conclusion diï¬€ers.
By specifying the prior to be a Gaussian distribution with variance Ï‘ we get
an oracle inequality. That is we take Ï€(Î¸) = Qd
i=1 Ï•(Î¸i; 0, Ï‘2). Several other prior
can be considered (see Chapter 4 for an application to spike and slab to induce
sparsity).
Theorem 1.2.1 Take Î» =
âˆš
nd and Ï‘ =
1
âˆš
d. Under the assumption of Deï¬nition
1.2.2, for any Îµ > 0, with probability at least 1 âˆ’Îµ we have,
Ë†ÏÎ»(R(Î¸)) â‰¤R(Â¯Î¸) +
r
d
n log
 4ne2
+
c
âˆšn +
r
d
4n3 + 2 log
  2
Îµ

âˆš
nd
.
The proof of this result is provided in Chapter 5. Note that we can also de-
duce bounds in expectation from bound of Theorem 1.2.1, using the fact that
Â´ âˆ
0 P {X > t} dt = E (X).
In Chapter 4 we specify other types of priors with the aim of dealing with
diï¬€erent hypotheses i.e. a spike and slab for sparsity and a Gaussian process prior
for non-linear classiï¬er.
The issue with the result is that the estimator is intractable. We will give ways
to approximate this measure in the subsequent chapters. In Chapter 5 in particular
we give an approximation for which we can obtain oracle inequalities directly.
8

1.3 Computational aspects
1.3 Computational aspects
The general idea behind Bayesian computational statistics is easily summed up
by Minka [2001a]. â€œ[. . . ] instantiate the things we know, and integrate over the
things we donâ€™t know, to compute whatever expectation or probability we seekâ€
We will therefore be interested in computing the integral
Ë†
Î˜
h(Î¸)Ï€(dÎ¸|D).
We have seen that for a quadratic loss the optimal estimator is the posterior
mean. But we can also take interest in other quantities such as moments, the mode,
posterior probabilities etc. In all these cases we have to deal with the intractability
of the integral with respect to the posterior as well as the intractability of the
normalizing constant. We will divide the introduction of the diï¬€erent algorithms
in two sections, the ï¬rst dealing with Monte Carlo , i.e. based on random samples,
the second on deterministic variational approximations.
1.3.1 Monte Carlo
Principle of Monte Carlo
Monte Carlo is based on the use of the law of large numbers (LLN) to approximate
integrals. We will be interested in computing integrals of the form Ih := EP (h(X)).
The LLN gives us, under the hypothesis that the (Xi)â‰¥0 are iid P and the existence
of Ih, that:
1
N
N
X
i=1
h(Xi)
P
âˆ’âˆ’âˆ’â†’
Nâ†’âˆIh.
A stronger result is given by the central limit theorem under the hypothesis of
ï¬nite second order moment,
âˆš
N
 
1
N
N
X
i=1
h(Xi) âˆ’Ih
!
L
âˆ’âˆ’âˆ’â†’
Nâ†’âˆN(0, Ïƒ2),
with Ïƒ2 the variance of h(X) under P.
We need a way to sample from P in the most general way possible to apply
these results. Several approaches exist for sampling, the two direct ones one may
consider are:
â€¢ Inverting the c.d.f: for sampling from a distribution with c.d.f F one can
sample a uniform U âˆ¼U[0,1] and compute Y = F âˆ’1(U).
This requires
knowledge of the c.d.f. which we have seen will not happen often in practice,
9

1 Introduction
and the ability to compute its inverse or pseudo-inverse. We give an example
below for the truncated Gaussian distribution which we will use repeatedly
in Chapter 3.
â€¢ Rejection sampling Accept reject (AR) is based on the fact that sampling
uniformly in the epigraph of a density leads to points marginally distributed
under it. The additional layer here is that we use a proposal distribution g
such that uniformly on the support of the target f we have that f â‰¤Mg for a
known constant M. Then AR consists in sampling from the joint distribution
(U, X) âˆ¼Mâœ¶[0â‰¤Uâ‰¤f(X)
Mg(X)]g(X). The proposal g is a degree of freedom for the
user and will impact the eï¬ƒciency of the algorithm. Finally, note that for
log concave densities there exists algorithms to automatically construct g,
see Chapter 3 of Robert and Casella [2004b] for an example.
In the above we have left one question unanswered: all of the algorithms require
the ability to generate i.i.d sequence of uniform distribution. In this thesis we
will consider that such an algorithm is given. An account is given in Niederreiter
[1978].
We give an example in the following on how to apply the above methodology to
a truncated Gaussian (see Chapter 3).
Example 1.3.1 Truncated Normal distribution
The aim is to sample Y âˆ¼N[a,b](m, Ïƒ2). The c.d.f of a (m, Ïƒ2) Gaussian trun-
cated to [a, b] is given by:
F(x; m, Ïƒ, a, b) = Î¦
  xâˆ’m
Ïƒ

âˆ’Î¦
  aâˆ’m
Ïƒ

Î¦
  bâˆ’m
Ïƒ

âˆ’Î¦
  aâˆ’m
Ïƒ

leading to the algorithm consisting in sampling a uniform distribution U on [0, 1]
and taking
Y = m + ÏƒÎ¦âˆ’1

U

Î¦
b âˆ’m
Ïƒ

âˆ’Î¦
a âˆ’m
Ïƒ

+ Î¦
a âˆ’m
Ïƒ

This particular sampler will be used extensively in Chapter 3.
An AR algorithm has also been developed for this example [Robert, 1995], using
a translated exponential where the parameters are chosen as to maximize the ac-
ceptance probability. Chopin [2011a] proposes a more eï¬ƒcient algorithm based on
the Ziggurat algorithm.
10

1.3 Computational aspects
Importance sampling
Importance sampling is based on the identity
EP (h(X)) = EQ

h(X) dP
dQ(X)

under the hypothesis that Q >> P (measure Q dominates P).
As previously importance sampling is based on replacing the integration with
respect to Q(dx) with its empirical counterpart Qn(dx) =
1
N
PN
i=1 Î´Xi(dx) with
Xi âˆ¼Q(dx) hence the estimator Ih â‰ˆ
1
M
PM
i=1 wih(Xi), where we write wi(Xi) :=
dP
dQ(Xi).
It is often the case that the weights w are known only up to a normalizing con-
stant (in particular in the case of posterior sampling). One can use a normalized
estimator. That is we will replace the unknown normalizing constant by the cor-
responding IS estimator
1
M
PM
i=1 wi(Xi). Because this is a convergent estimator
bounded away from 0, one can show that the ratio
PM
i=1 wi(Xi)h(Xi)
PM
i=1 wi(Xi)
converges towards the correct integral. One can get results for ï¬nite M through
concentration inequalities (CappÂ´e et al. [2005] chapter 9).
In the rest of the chapter we suppose that both measures are dominated with
respect to a base measure and write p
q(X) the ratio of densities with respect to
this measure.
Remark 1.3.1 Unbiased target
It is interesting to note that importance sampling remains valid if one replaces
the weights with an unbiased estimator. Suppose we have a quantity Ë†TZ(X) such
that ES Ë†TZ(X) = p
q(X). In this case we can build an algorithm consisting in sam-
pling Zi âˆ¼S and Xi âˆ¼Q and forming the estimator
Ë†Ih = 1
N
N
X
i=1
Ë†TZi(Xi)h(Xi),
one readily sees that integrating with respect to Z yields an IS estimator of Ih.
In the following we give a running example that we use to illustrate the case
of unbiased targets. In Chapter 6 we will use such results in the context of state
space models extending the algorithm of Chopin et al. [2013b].
11

1 Introduction
Example 1.3.2 Symmetric Skellam distribution
The diï¬€erence of two random variables with Poisson distributions is known as
the Skellam distribution.
We can write its density as a convolution between a
Poisson and a negative Poisson,
p(z; Î») =
âˆ
X
i=0
ÌŸ(i + z; Î»)ÌŸ(i; Î»).
(1.3)
where ÌŸ(x; Î») is the Poisson pdf with parameter Î» evaluated at x.
This distribution can be written as the expectation of EÌŸ(z + X) where the
variable X is distributed as a Poisson of parameter Î». An unbiased estimator is
obtained by averaging over MX samples of a Poisson distribution.
We can apply the methodology described above to this case (see Figure 1.2).
We use a Gaussian proposal with moments calibrated from the method of moment
estimator. In Figure 1.2 we show the eï¬€ect of changing the value of MX and hence
modifying the variance of our unbiased estimator.
On a side note, notice that the estimator of the normalizing constant introduced
above is unbiased. Hence it can be used inside other algorithm to perform inference
on an intractable posterior, we use this approach for instance in Chapter 2, for
model selection.
Resampling
Another important alternative is to sample from the empirical mixture formed by
the weighted sample obtained with importance sampling,
Ë†ÂµRIS(dx) =
M
X
i=1
wi
P
j wj
Î´xi(dx),
where (wi, xi)1â‰¤Â·Â·Â·â‰¤M is a consistently weighted sample from the target distribution
(i.e. the IS estimator converges to the correct distribution).
Several algorithms exist to perform resampling. In this thesis we will use system-
atic resampling (see Douc et al. [2005] for a comparison of resampling algorithms
for particle ï¬lters). The convergence of the measure to the target in ensured by
portmanteau lemma. Finite sample results also exist: Chapter 7 of CappÂ´e et al.
[2005] provides a Hoeï¬€ding type inequality.
Monte Carlo Markov Chain (MCMC)
MCMC relies on a diï¬€erent idea. It consists in building a Markov kernel with
the target distribution as an invariant distribution. MCMC relies on the ergodic
12

1.3 Computational aspects
2.0
2.5
3.0
3.5
4.0
0
2500
5000
7500
10000
samples
(a) MX = 5
2.0
2.5
3.0
3.5
4.0
0
2500
5000
7500
10000
samples
(b) MX = 10
2.0
2.5
3.0
3.5
4.0
0
2500
5000
7500
10000
samples
(c) MX = 15
2.0
2.5
3.0
3.5
4.0
0
2500
5000
7500
10000
samples
(d) MX = 20
Figure 1.2: Sequence of estimator of the parameter of a symmetric
Skellam distribution
We provide the convergent sequence of estimator of the conditional expectation of the parameter
for diï¬€erent value of MX. We show the IS estimator and 95% conï¬dence bands (obtained by
replication). We use a simulated dataset for illustration with n = 40 observations. The value of
MX has as expected an inï¬‚uence on the variance.
theorem to approach integration with respect to the invariant distribution. That
is, we rely on the following theorem.
Theorem 1.3.1 (Robert and Casella [2004b]) Let (Xn)nâ‰¥0 be a Harris recur-
rent Markov chain with invariant probability Ï€ for any f such that f âˆˆL1(Ï€) we
have that
1
N
N
X
i=1
f(Xi) âˆ’âˆ’âˆ’â†’
Nâ†’âˆ
Ë†
f(x)Ï€(dx).
As for the iid case we can get a CLT under additional assumptions. We refer
the reader to Roberts and Rosenthal [2004] for a review on Markov chain theory
13

1 Introduction
applied to MCMC and to Robert and Casella [2004b] for some applications of the
results.
Gibbs sampler
We start our very brief overview of MCMC methods with Gibbs
sampling. The methodology consists in constructing a Markov Chain by sampling
alternatively from conditional distributions. One can easily show that this results
in a Markov Chain targeting the joint distribution.
We describe the algorithm in pseudo-code below
Algorithm 1 Two stage Gibbs sampler
Input: Conditional distributions p(X|Z) and p(Z|X), a starting point Z0.
Output: (X, Z)1â‰¤tâ‰¤T a Markov chain with invariant probability p(X, Z)
For
t âˆˆ{1, . . . , T}
a. Sample Xt âˆ¼p(.|Ztâˆ’1).
b. Sample Zt âˆ¼p(.|Xt).
End For
The Gibbs sampler needs conditional conjugacy in its simplest form to work.
For our example 1.1.1 on the probit model we can see that putting a Gaussian
prior and sampling alternatively from p(Î²|Z1:n, Y1:n) and p(Z1:n|Î², Y1:n) leads to a
sampler targeting the correct joint distribution Ï€(Î², Z1:n|X1:n). It is direct to see
that the joint distribution of Î², Z is given by,
Ï€(Î², Z1:n|X1:n) =
n
Y
i=1
{âœ¶Yi=1âœ¶Ziâ‰¥0 + âœ¶Yi=0âœ¶Ziâ‰¤0} Ï•(Zi; xiÎ², 1)Ï•(Î²; 0p, Ï‘Ip).
Hence the two conditionals are given by:
(
Î²|(Z, Y )1:n âˆ¼N (Qâˆ’1xtY, Qâˆ’1)
Zi|Î², Y âˆ¼N(xiÎ², 1) {âœ¶Yi=1âœ¶Ziâ‰¥0 + âœ¶Yi=0âœ¶Zi<0}
with Q =
 xTx + Ï‘Ip
âˆ’1. One iterates between the two distributions as shown
in Algorithm 1. In Chapter 2 we discuss alternatives to the Gibbs sampler that
outperform it on many datasets.
14

1.3 Computational aspects
Unbiased target
In the case where only an unbiased version of the density is
available, one can still construct a Gibbs sampler. Suppose that we can write the
following joint distribution:
p(Î¸, X1:MX) =
 
1
MX
MX
X
i=1
Ë†TXi(Î¸)
! MX
Y
i=1
qi(Xi)
such that integrating with respect to X gives the correct target density (with q an
auxiliary distribution). That is such that Ë†T(Î¸) is an unbiased estimator under q
of our target.
We can augment the space with an index k chosen with probability
Ë†TXk(Î¸)
MX . The
joint distribution is then given by:
p(k, Î¸, X1:MX) = Ë†TXk(Î¸) 1
MX
MX
Y
i=1
q(Xi)
Summing over k gives the correct distribution.
Hence the Gibbs sampler con-
structed on this joint distribution, which samples iteratively from k|Î¸, X1:MX,
Î¸|k, Xk and X1:MX|Î¸, k gives the correct distribution.
This idea will be the building block of the parameter update in Chapter 6; the
algorithm is developed in the special case of state space models. We use again the
Skellam distribution as a toy example to illustrate the principle.
Example 1.3.3 Symmetric Skellam distribution (continued)
We put a Î“(1, 1) prior on Î» as was done previously. Using what was described
above we can write an algorithm to sample from the posterior. The eï¬ƒciency of
the algorithm could be discussed but it is however given here just as an example to
prepare the reader for more advanced use of this idea in Chapter 6.
Algorithm 2 A Gibbs sampler for the Estimation of the Skellam parameter.
Input: Î»0, MX and MÎ¸
Output: (Î»i)0â‰¤iâ‰¤MÎ¸
For i âˆˆ{1, Â· Â· Â· , MÎ¸}
a. Sample MX i.i.d samples Xj
1:N âˆ¼PN(Î») for j Ì¸= k
b. Sample k|Î», X1:MX
1:n
âˆ¼PMX
i=1
Q
j ÌŸ(yi + Xi
j; Î»)

Î´i
c. Sample Î»|Xk
1:n, y âˆ¼Î“(2 + Pn
i=1(2yi + Xk
i ), 2n + 1)
End For
15

1 Introduction
0.00
0.25
0.50
0.75
1.00
0
10
20
30
40
50
lag
acf
(a) ACF
2
3
4
0
2500
5000
7500
10000
samples
(b) MX = 130
Figure 1.3: ACF and trace plot of the Gibbs sampler for the Skellam
distribution
The experiment is performed on the same simulated dataset as for importance sampling. It can
be compared to Figure 1.2. At ï¬rst glance the scheme does not seem that eï¬ƒcient as we need a
large MX to get a variance comparable to the IS version.
Remark 1.3.2 Before we start explaining a more general class of algorithms,let
us note a property of the autocorrelations of Gibbs samplers.
Assuming without loss of generality Eh(X) = 0, we have that
E (h(X)h(Xâ€²)) =
Ë†
h(X)h(Xâ€²)
Ë†
p(X|Z)p(Z|Xâ€²)dZp(Xâ€²)dXâ€²dX
=
Ë† Ë†
h(X)p(X|Z)dX
2
p(Z)dZ â‰¥0
This is not the case for Metropolis-Hastings. In fact, using a certain version of
a MCMC algorithm, Hamiltonian Monte Carlo (HMC) (see Chapter 2) we can
construct an example where the correlation is negative between samples therefore
leading to a gain in eï¬ƒciency as compared to iid sampling.
16

1.3 Computational aspects
0.5
1.0
1.5
2.0
âˆ’2
âˆ’1
0
5.0eâˆ’40
1.0eâˆ’39
1.5eâˆ’39
2.0eâˆ’39
2.5eâˆ’39
level
(a)
lag
acf
âˆ’0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
30
35
(b)
Figure 1.4: An example of an MCMC run with negative autocorre-
lation
We show the trace plot for the two ï¬rst marginals of the posterior of a probit model on the Pima
dataset, the second panel shows the ACF of the ï¬rst marginal.
Metropolis-Hasting
A more general approach to building a Markov chain with
the correct invariant distribution is the Metropolis-Hastings (MH) algorithm. The
algorithm uses a proposal and acceptance mechanism. A new state is proposed
according to a Markov kernel, with density q. The proposal is accepted with a given
probability (see Algorithm 3). The degree of freedom of the algorithm is given by
the choice of the proposal q. Several common choices have been adopted in the
literature. We give a few. First one may choose to propose independently from the
current state. The diï¬ƒculty with the approach is that we need to ensure that the
proposal dominates the target. By adopting this view we are back to the problem
of AR and IS. A more useful approach is to move locally around the current point
by using a random walk (i.e. a distribution such that q(x|y) = q(y|x)). The density
of the proposal therefore cancels in the acceptance ratio.
Many other proposals will be considered in the sequel, in particular we will make
use of the gradient of the target to explore more eï¬ƒciently the state space.
Unbiased target
As for IS one requirement of those methods is the availability
of the density up to a multiplicative factor. Suppose we have access to a random
variable Ë†TX(Î¸) such that Eq

Ë†TX(Î¸)

= T(Î¸) is our target distribution. By taking
17

1 Introduction
Algorithm 3 Metropolis Hastings algorithm
Input: Î¸0, M
Output: (Î¸t)tâ‰¥0
For t âˆˆ{1, Â· Â· Â· , M}
a. Sample Î¸prop âˆ¼q(.|Î¸tâˆ’1).
b. Sample U âˆ¼U([0, 1]).
c. If U â‰¤Ï€(Î¸prop|D)q(Î¸tâˆ’1|Î¸prop)
Ï€(Î¸tâˆ’1|D)q(Î¸prop|Î¸tâˆ’1) , set Î¸t â†Î¸prop, otherwise set Î¸t â†Î¸tâˆ’1.
End For
for proposal on (Î¸, X), q(Î¸â€²|Î¸)q(Xâ€²), and for target distribution Ë†TX(Î¸)q(X) one
gets the correct invariant with the acceptance ratio:
Î±(Î¸, Î¸â€²) = 1 âˆ§
Ë†TXâ€²(Î¸â€²)âœŸâœŸâœŸ
âœŸ
q(Xâ€²)q(Î¸|Î¸â€²)âœŸâœŸâœŸ
q(X)
Ë†TX(Î¸)âœŸâœŸâœŸ
q(X)q(Î¸â€²|Î¸)âœŸâœŸâœŸ
âœŸ
q(Xâ€²)
.
In addition after simpliï¬cation the acceptance ratio writes:
Î±(Î¸, Î¸â€²) = 1 âˆ§
Ë†TXâ€²(Î¸â€²)q(Î¸|Î¸â€²)
Ë†TX(Î¸)q(Î¸â€²|Î¸)
.
Hence applying the methodology of Algorithm 3 to an unbiased estimator of the
target yields a Markov chain with the correct invariant probability.
Sequential Monte Carlo (SMC)
The ï¬rst step to building the SMC algorithm is to deï¬ne a sequence of densities
(Ï€n)âˆˆT where T is an index set, and such that there is one N âˆˆT such that Ï€N
is the target. The idea is to apply IS to move from one distribution to another.
We will explain in the rest of the section how we can take proï¬t of resampling and
ideas from MCMC to improve upon this. SMC is in particular useful for moderate
dimensions in the case where MCMC explores the space too slowly.
We give a brief overview of the idea of the algorithm and relay the full description
to Chapter 2 for static models and to Chapter 3 and 6 for SSM.
Particle ï¬lter (PF)
Particle ï¬lters originated in Gordon et al. [1993] for state
estimation in SSM. We describe the algorithm using the notations of Example
1.1.2.
18

1.3 Computational aspects
The index set in this case is time and we want to sample recursively from the
predictive distribution p(xt+1|y1:t) and the ï¬ltering distribution p(xt|y1:t). To this
aim we introduce the forward backward recursion,
p(xt+1|y1:t) =
Ë†
X
p(xt|y1:t)g(xt|xtâˆ’1)dxt
p(xt+1|y1:t+1) âˆp(xt+1|y1:t)f(yt+1|xt+1)
This suggests the following importance sampling based algorithm: start by
sampling an array of size M independently according to the initial distribution
xi âˆ¼g0(xi
0); we call the elements particles. Move the particles according to the
Markov kernel g(xi
1|xi
0) weight each particles according to wi
1 = f0(y1|x1). The
operation is iterated this way for every time steps t. In the end we sample recur-
sively from the Markov chain QT
t=1 g(xt|xtâˆ’1) and compute the product of weights
wi
t = QT
t=1 f(yt|xi
t). This algorithm amounts to an importance sampling algorithm
where the simulations and weight computation are done sequentially. It is referred
in the literature as sequential importance sampling (SIS) [CappÂ´e et al., 2005].
We can use the algorithm to get an unbiased estimator of the normalizing con-
stant, for the case of SSM given a value of the parameters Î¸ it corresponds to the
likelihood, L(y1:T; Î¸) =
Â´ Q
t p(yt|xt; Î¸)gt(xt|xtâˆ’1; Î¸)Âµ0(dx0)dx0:T. In chapter 3 we
reinterpret the GHK algorithm as a SIS algorithm for a given state space model,
we use it to build a more eï¬ƒcient estimator of the Gaussian orthant probability.
The SIS algorithm as described proposes the state of the Markov chain using
the true process gt(xt|xtâˆ’1) at each time t. On can easily see, that as for IS, we can
choose an auxiliary distribution as long as we correct accordingly in the weights.
We will use this feature in Chapter 3 and in fact propose each state by the optimal
proposal (minimizing the variance).
One issue of SIS is the fact that we have to compute a product of weights;
this leads to weight degeneracy [CappÂ´e et al., 2005]. As T grows the number of
particles with small weights will grow, and the normalized weight will go to zero.
One can measure the variance of the weigths by introducing the ESS (eï¬€ective
sample size) Kong et al. [1994],
ESSt =
nPM
i=1 wi
t
o2
PM
i=1(wi
t)2 âˆˆ[0, M].
As the time horizon grows it is common to observe that this quantity goes
to zero. In Chapter 3 we show that for a speciï¬c model, when computing the
orthant probability with GHK, some quantity closely related to the ESS goes to 0
exponentially fast.
19

1 Introduction
The solution proposed in the literature is to introduce resampling at each step or
in an adaptive manner whenever the ESS falls under a given value. The particles
are resampled according to their current weights and the later are set to one. We
do not describe the case of PF with unbiased estimator of the weights although
the interested reader can refer to Fearnhead et al. [2010].
Algorithm 4 Particle Filter
Input: M the number of particles
Sample: Sample xi
0 âˆ¼g0(.)
for t = 1 : T âˆ’1 do
if ESS < Î·â‹†then
Z â†Z Ã— { 1
M
PM
i=1 wi
t}
Resample aj
t âˆ¼P
i
wi
t
P
j wj
t Î´i, set wj
t â†1
else
a1:M
t
= 1 : M
end if
Sample xi
t+1 âˆ¼qt+1(.|x
ai
t
t )
Set wi
t+1 â†wi
t
ft+1(yt+1|xi
t+1)gt+1(xi
t+1|x
ai
t
t )
qt+1(xi
t+1|x
ai
t
t )
end for
return Z Ã— 1
M
PM
i=1 wi
T and (xi
T, wi
T)1â‰¤iâ‰¤M
Particle ï¬lters can also be used to compute the likelihood of the model. In fact
it can be shown that the quantity deï¬ned by Z in Algorithm 4 is an unbiased esti-
mator of the likelihood (Del Moral [1996a], Lemma 3). This is useful in particular
in lights of the remarks made on the use of algorithms with unbiased estimator
of the target. Andrieu et al. [2010a] suggested using a Gibbs sampler and a MH
using the estimator of the likelihood given by a particle ï¬lter.
In the following we will show that this algorithm can be generalized to more
general problems.
Static models
The idea of using this framework to sample from other types of
distribution originated in Neal [2001a], Chopin [2002a], (see also Del Moral et al.
[2006b]).
The user needs to deï¬ne a sequence of distribution; several choices can be con-
sidered. We describe the two used in this thesis.
â€¢ The ï¬rst one was proposed under the name IBIS (iterated batch importance
sampling) [Chopin, 2002a].
Ï€n(Î¸) âˆÏ€(Î¸)L(y1:n|Î¸)
20

1.3 Computational aspects
and consists in adding one or several data points in a sequential manner. It
is used in particular in Chapter 6 for parameter estimation of state space
models.
â€¢ The sequence builds a bridge between two distributions,
Ï€n(Î¸) âˆ{Ï€(Î¸)L(y1:N|Î¸)}Î³n {q(Î¸)}1âˆ’Î³n
whith Î³n a sequence such that 0 = Î³0 < Î³1 < Â· Â· Â· < Î³N = 1. The choice of the
sequence is of paramount importance for the eï¬ƒciency of the algorithm. In
what follows we use an adaptive choice for Î³ such that the ESS does not fall
under a given threshold (see Jasra et al. [2011b] and the diï¬€erent chapters
of this thesis).
After a few resampling steps all particles Î¸ would tend to be equal. We introduce
an additional step to increase the diversity of the particle system, at time n we
move the particles according to a kernel Kn that leaves Ï€n invariant. The choice
of Kn is of paramount importance as it will condition the diversity of the particle
system.
The main steps are described in Algorithm (12) bellow:
Algorithm 5 Sequential Monte Carlo
Input Î“,a, M, Î±â‹†, Set Z â†1
Each computation involving m is done âˆ€m âˆˆ1 : M
Init Î·m
1 âˆ¼Ï€0(.), and wm
1 = 1
for t âˆˆ1 : T âˆ’1 do
At time t the weighted system is distributed as (wm
t , Î·m
1:t) âˆ¼Ï€t(.).
if ESS(w1:M
t
) < Î±â‹†then
Z â†Z Ã— { 1
M
PM
i=1 wi
t}.
Resample: Î·â€²m
t
âˆ¼PM
j=1 wj
tÎ´Î·j
t , wm
t â†1.
Move: Î·m
t âˆ¼Kt(Î·â€²m
t , dÎ·m
t ) where Kt leaves Ï€t(Î·1:t) invariant.
end if
wm
t+1 â†wm
t Ã— Ï€t+1(Î·m
t )
Ï€t(Î·m
t ) .
end for
return Z Ã— { 1
M
PM
i=1 wi
T}, and (wi
T, Î·i
T)
A nice feature of SMC is that at a given step one can use estimators based on
the system of particles to build adaptive proposals and MCMC steps. We will
make some use of this in the diï¬€erent chapters, see in particular Chapter 6 where
we use SMC in the context of intractable targets.
21

1 Introduction
1.3.2 Approximate Inference
In the previous section we justiï¬ed the algorithms as being exact with the CPU
eï¬€ort growing to inï¬nity. Another approach is to approximate the target by the
closest distribution in a tractable family.
We give a brief overview of the two
most used algorithms of this type and relay a full description to Chapter 2 for
expectation propagation (EP) and Chapter 4 for variational Bayes (VB).
Expectation Propagation
In most applications of EP we will consider a posterior that can be written as a
product of a tractable distribution and intractable factors. We call tractable a dis-
tribution for which we have easy access to moments and is stable by multiplication
(as are exponential family models). We write:
Ï€(Î¸|Y ) âˆÏ€(Î¸)
Y
i
ti(Î¸).
EP works by successively approximating each sites by a given parametric distri-
bution. To be more speciï¬c let us describe the global approximation Q to the
previous posterior as a product:
Q(Î¸) âˆÏ€(Î¸)
Y
i
Ëœti(Î¸),
where each Ëœti(Î¸) approximates its corresponding site ti(Î¸). EP works by cycling
through the sites and updating them. Once a site has been updated we can use this
to update the global approximation. Let us describe the process in more detail.
We start by computing the cavity distribution as the approximation removed of
the inï¬‚uence of site j,
Q\j(Î¸) âˆÏ€(Î¸)
Y
iÌ¸=j
Ëœti(Î¸).
This distribution is easily obtained for exponential models, and consists only in
a modiï¬cation of the natural parameters. We multiply the cavity distribution by
the corresponding new site tj(Î¸) to obtain the hybrid distribution:
hj(Î¸) =
Q\j(Î¸)tj(Î¸)
Â´
Q\j(Î¸)tj(Î¸)dÎ¸.
This new distribution would ideally be our new distribution updating information
of site j, it is however intractable. The approach consists in deï¬ning the new
approximation as the one minimizing the KL divergence of Q with respect to
the hybrid over the considered exponential family. Because the approximating
22

1.3 Computational aspects
distribution is in an exponential family the minimization can be done by computing
the moments of the distribution (see Seeger [2005a]). This is done in cyclic manner
for each site until a convergence criterion is achieved. We give an example where
an EP algorithm can be used, and an iteration of the algorithm for a simple case.
Example 1.3.4 PAC-Bayesian 0-1 Classiï¬cation The pseudo posterior we want
to approximate is given by:
Ï€Î¾,Î³(Î¸|D) âˆ
n
Y
i=1
exp

âˆ’Î»
nâœ¶Yi<Xi,Î¸>â‰¤0

Ï€(Î¸)
and we suppose that the prior Ï€(Î¸) is Gaussian.
We can use EP to get a Gaussian approximation of the posterior because the mo-
ments of the distribution hi(Î¸) âˆÏ•(Î¸; m\i, Î£\i) exp
 âˆ’Î»
nâœ¶Yi<Xi,Î¸>â‰¤0

can be com-
puted exactly.
To make this application clearer, in Figure 1.5 we show the iterations of an EP
approximation for a site of the form t(Î¸) = âœ¶Î¸<0.
More details on the computation of each terms in speciï¬c cases are given in
the chapters of this thesis. Note in particular the EP is used in Chapter 2 to
compute an approximation of the posterior of a probit model, and in Chapter 4
to approximate the Gibbs posterior under a AUC loss.
Fractional Expectation Propagation
Fractional EP is similar to the algorithm described previously with the excep-
tion that only a fraction of each site is treated at each iteration.
We will de-
note this fraction by Î± â‰¤1. The corresponding cavity distribution is now given
by Q\Î±,j(Î¸) âˆQn
i=1 Ëœti(Î¸)
 Ëœtj(Î¸)
1âˆ’Î±. The hybrid distribution can be written has
hj(Î¸) âˆQ\Î±,j(Î¸) (tj(Î¸))Î±. The rest of the algorithm follows the steps of standard
EP.
The rationale behind this strategy is twofold [JylÂ¨anki et al., 2011]. First taking
Î± < 1 for non log-concave sites ï¬‚attens the distribution and therefore prevents
numerical issues linked to multimodality. Second it prevents the cavity moment
of becoming too small (or even negative). Taking speciï¬c values of Î± can also
allow us to transform a intractable problem into a tractable one. We give such an
example in the following.
Example 1.3.5 Consider the Gaussian regression model with Student prior dis-
tribution as advocated for instance by Gelman et al. [2008].
Ï€(Î²|Y ) âˆexp

âˆ’1
2âˆ¥Y âˆ’XtÎ²âˆ¥2
2

d
Y
i=1
tÎ½(Î²i)
23

1 Introduction
0.00
0.25
0.50
0.75
1.00
âˆ’4
âˆ’2
0
2
(a) Global approximation
0.00
0.25
0.50
0.75
1.00
âˆ’4
âˆ’2
0
2
(b) Cavity distribution
0.00
0.25
0.50
0.75
1.00
âˆ’4
âˆ’2
0
2
(c) New site
0.00
0.25
0.50
0.75
1.00
âˆ’4
âˆ’2
0
2
(d) Hybrid distribution
0.00
0.25
0.50
0.75
1.00
âˆ’4
âˆ’2
0
2
(e) New global approxima-
tion
Figure 1.5: Expectation Propagation in Action
In panel (a) we show the global approximation Q (initial point), the site approximation is removed
to get Q\j in panel (b).
Panel (c) and (d) show the site (indicator function) and the hybrid
h(Î¸) âˆN(Î¸; m, s)âœ¶Î¸>0 (in purple). The closest Gaussian to the hybrid is the new approximation
panel (e) (the one with the same mean and variance).
where tÎ½,Î³(Î²i) is the Student density with degree of freedom Î½ and scale Î³. This
prior leads to an intractable problem.
Standard EP algorithm also leads to intractable integrals using fractional EP we
can get a tractable problem that leads to tractable sites.
We can write
tÎ½,Î³(Î²i) âˆ
1

1 +

Î²i
Î³
2 Î½+1
2
Taking a fractional parameter Î· = âˆ’
2
Î½+1 for a Gaussian approximation one gets
a tractable EP approximation for an otherwise intractable case. This can also be
used in the Gaussian process case with Student likelihood to prevent from using
numerical integration [JylÂ¨anki et al., 2011].
24

1.3 Computational aspects
0
20
40
60
âˆ’0.12
âˆ’0.10
âˆ’0.08
(a) Î²1
0
50
100
0.04
0.05
0.06
(b) Î²2
0
10
20
30
âˆ’0.025
0.000
0.025
0.050
0.075
(c) Î²3
Figure 1.6: Marginal of the Gaussian approximation of a regression
with Student priors
Regression with student prior marginal of the 3 ï¬rst coeï¬ƒcients on the Boston dataset. In green
the EP approximation and in blue the marginal from the sampled posterior (RWMH).
Variational Bayes
The variational Bayes algorithm appears as a way to transform an integral form
problem in a ï¬nite dimension optimization problem (Jordan et al. [1999],MacKay
[2002] and Chap. 10 in Bishop [2006a]).
The idea of the approach is to minimize the KL divergence of a measure Ï with
respect to the posterior measure of interest.
In most cases of interest it is an
intractable problem because of the unknown normalizing constant, however we
can write the following useful decomposition of the log marginal likelihood,
log m(D) = EÏ

log Ï€(Î¸)L(D|Î¸)
Ï(Î¸)

+ K(Ï|Ï€(.|D)).
Because the left handside does not depend on Ï, minimizing the ï¬rst term of the
right handside is equivalent to minimizing the KL divergence. This surrogate ob-
jective function can be seen as a lower bound of the marginal likelihood. The above
deï¬nition gives a natural optimization algorithm, minimizing the objective on all
probability distribution yields the posterior, we minimize the objective restricted
to a set of tractable distributions.
We now review two types of families popular in the VB literature.
â€¢ Mean ï¬eld VB: for a certain decomposition Î˜ = Î˜1 Ã— Â· Â· Â· Ã— Î˜d, F is the set
25

1 Introduction
of product probability measures
FMF =
(
Ï âˆˆM1
+(Î˜) : Ï(dÎ¸) =
d
Y
i=1
Ïi(dÎ¸i), âˆ€i âˆˆ{1, . . . , d}, Ïi âˆˆM1
+(Î˜i)
)
.
(1.4)
The inï¬mum of the KL divergence K(Ï, Ï€(.|D)), relative to Ï = Q
i Ïi satisï¬es
the following ï¬xed point condition [Bishop, 2006a; Parisi, 1988, Chap. 10]:
âˆ€j âˆˆ{1, Â· Â· Â· , d}
Ïj(dÎ¸j) âˆexp
 Ë†
{log L (D|Î¸) + log Ï€(Î¸)}
Y
iÌ¸=j
Ïi(dÎ¸i)
!
.
(1.5)
This leads to a natural algorithm were we update successively every Ïj until
stabilization.
â€¢ Parametric family:
FP =

Ï âˆˆM1
+(Î˜) : Ï(dÎ¸) = f(Î¸; m)dÎ¸, m âˆˆM
	
;
and M is ï¬nite-dimensional; say FP is the family of Gaussian distributions
(of dimension d). In this case, several methods may be used to compute the
inï¬mum. As above, one may use ï¬xed-point iterations, provided an equation
similar to (5.4) is available. Alternatively, one may directly maximize the
bound with respect to parameter m, using numerical optimization routines.
Mean ï¬eld VB has been shown to underestimate the variance of the distribution.
This is partly due to the fact the KL divergence takes inï¬nite values if the target
does not dominate Ï. In particular the condition is violated if the tails of Ï are
lighter than the target.
To illustrate this problematic behavior let us look at an experiment proposed
in Bishop [2006a], namely the mean ï¬eld approximation of a centered bivariate
Gaussian with variance 1 and correlation Ï. One readily checks that the iterations
are given by variance of 1 âˆ’Ï2 and a sequence of means converging to 0. Figure
1.7 illustrates the two distributions, with the approximation constrained by the
variance in the direction of the smallest eigenvalue.
26

1.4 Overview
âˆ’2
âˆ’1
0
1
2
âˆ’2
âˆ’1
0
1
2
x
y
Figure 1.7: Mean ï¬eld VB approximation (red) of bivariate Gaussian
distribution with correlation Ï = 0.99 (blue)
In chapter 5 we give conditions under which VB approximations of the Gibbs
posterior do not deteriorate the rate of convergence of the approximation.
1.4 Overview
This thesis is divided into 5 chapters, we give a short descriprion of each of them
in the following.
1.4.1 Leave Pima indians alone: binary regression as a
benchmark for Bayesian computation
ResumÂ´e
Quand un nouvel algorithme est introduit en statistique bayÂ´esienne le
mod`ele probit sur de petits jeux de donnÂ´ees est frÂ´equemment employÂ´e pour le
tester. Ce chapitre Â´etudie le bien fondÂ´e de cette approche. Il donne, de plus,
une revue de la littÂ´erature dans le domaine avec pour exemple ce mod`ele. Les
algorithmes Â´etudiÂ´es sont divisÂ´es en deux catÂ´egories: dâ€™un cË†otÂ´e ceux employant
des mÂ´ethodes dâ€™Â´echantillonnage (Â´echantillonnage prÂ´eferentiel, mÂ´ethode de Monte
Carlo par chaË†Ä±ne de Markov et methode de Monte Carlo sÂ´equentielle), de lâ€™autre
les algorithmes dâ€™approximation rapide (Laplace et EP). De nombreux rÂ´esultats
numÂ´eriques sont prÂ´esentÂ´es.
27

1 Introduction
Abstract
Whenever a new approach to perform Bayesian computation is intro-
duced, a common practice is to showcase this approach on a binary regression
model and datasets of moderate size. This chapter discusses to which extent this
practice is sound. It also reviews the current state of the art of Bayesian com-
putation, using binary regression as a running example.
Both sampling-based
algorithms (importance sampling, MCMC and SMC) and fast approximations
(Laplace and EP) are covered. Extensive numerical results are provided, some
of which might go against conventional wisdom regarding the eï¬€ectiveness of cer-
tain algorithms. Implications for other problems (variable selection) and other
models are also discussed.
1.4.2 Computation of Gaussian orthant probabilities in high
dimension
ResumÂ´e
Nous Â´etudions dans ce chapitre le calcul de probabilitÂ´es dâ€™orthants (la
probabilitÂ´e quâ€™une rÂ´ealisation Gaussienne ait toutes ses composantes positives).
Nous basons cette Â´etude sur lâ€™algorithme GHK couramment utilisÂ´e pour rÂ´esoudre
ce probl`eme en dimensions plus grandes que 10.
Dans ce chapitre nous mon-
trons, pour des matrices de variance-covariance markoviennes que lâ€™algorithme
peut sâ€™interprÂ´eter comme un algorithme dâ€™Â´echantillonnage prÂ´efÂ´erentiel sÂ´equentiel
(SIS pour acronyme anglais). Pour un AR(1) la variance normalisÂ´ee de GHK di-
verge `a une vitesse exponentielle avec la dimension. Pour corriger ce probl`eme
nous introduisons une Â´etape de rÂ´eechantillonnage transformant ainsi lâ€™algorithme
en un ï¬ltre particulaire. Nous gÂ´enÂ´eralisons dans un deuxi`eme temps cette idÂ´ee au
cas de matrices de variance covariance gÂ´enerales en introduisant un algorithme de
Monte Carlo sÂ´equentiel.
Abstract
We study the computation of Gaussian orthant probabilities, i.e. the
probability that a Gaussian variable falls inside a quadrant. The Geweke-Hajivassiliou-
Keane (GHK) algorithm [Genz, 1992; Geweke, 1991; Hajivassiliou et al., 1996;
Keane, 1993], is currently used for integrals of dimension greater than 10. In this
chapter we show that for Markovian covariances GHK can be interpreted as the
estimator of the normalizing constant of a state space model using sequential im-
portance sampling (SIS). We show for an AR(1) the variance of the GHK, properly
normalized, diverges exponentially fast with the dimension. As an improvement
we propose using a particle ï¬lter (PF). We then generalize this idea to arbitrary
covariance matrices using Sequential Monte Carlo (SMC) with properly tailored
MCMC moves. We show empirically that this can lead to drastic improvements on
currently used algorithms. We also extend the framework to orthants of mixture of
Gaussians (Student, Cauchy etc.), and to the simulation of truncated Gaussians.
28

1.4 Overview
1.4.3 Theoretical and computational aspects of PAC Bayesian
ranking and scoring
ResumÂ´e
Nous dÂ´eveloppons une procÂ´edure dâ€™ordonancement et de classiï¬cation
basÂ´ee sur une approche PAC-Bayesienne et sur la minimisation dâ€™un crit`ere AUC.
Nous dÂ´emontrons des inegalitÂ´es oracles pour diï¬€Â´erentes distributions a priori : un
prior Gaussien et un prior â€œspike and slabâ€. Dans un deuxi`eme temps nous pro-
posons deux algorithmes pour calculer lâ€™estimateur numÂ´eriquement. Dâ€™abord un
algorithme de Monte Carlo sequentiel comme mÂ´ethode â€œexacteâ€ au sens de Monte
Carlo, puis un algorithme dâ€™approximation EP pour introduire des mÂ´ethodes plus
rapides.
Abstract
We develop a scoring and classiï¬cation procedure based on the PAC-
Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially
on the class of linear score functions. We derive PAC-Bayesian non-asymptotic
bounds for two types of prior for the score parameters: a Gaussian prior, and a
spike and slab prior; the latter makes it possible to perform feature selection. One
important advantage of our approach is that it is amenable to powerful Bayesian
computational tools. We derive in particular a Sequential Monte Carlo algorithm,
as an eï¬ƒcient method which may be used as a gold standard, and an expectation
propagation algorithm, as a much faster but approximate method. We also ex-
tend our method to a class of non-linear score functions, essentially leading to a
nonparametric procedure, by considering a Gaussian process prior.
1.4.4 Properties of variational approximations of Gibbs
posteriors
ResumÂ´e
Lâ€™approche PAC-bayÂ´esienne permet le dÂ´eveloppement de bornes non
assymptotiques sur le risque thÂ´eorique. La distribution sur les estimateurs qui en
dÂ´ecoulent nâ€™est cependant pas calculable. Alors que lâ€™approche usuelle est dâ€™utiliser
des mÂ´ethodes de Monte Carlo par chaË†Ä±ne de Markov nous proposons pour plus
dâ€™eï¬ƒcacitÂ´e une approximation variationelle des estimateurs. Nous Â´etudions alors
les propriÂ´etÂ´es de ces estimateurs.
Nous donnons des conditions sous lesquelles
lâ€™approximation variationelle converge `a la mË†eme vitesse que les procÂ´edures PAC
usuellement considÂ´erÂ´ees. Nous appliquons ensuite ces rÂ´esultats `a diï¬€erents probl`emes
dâ€™apprentissage (classiï¬cation, ranking et completion de matrices). Lâ€™implÂ´ementation
des algorithmes est Â´egalement abordÂ´ee.
Abstract
The PAC-Bayesian approach is a powerful set of techniques to derive
non-asymptotic risk bounds for random estimators. The corresponding optimal
29

1 Introduction
distribution of estimators, usually called the Gibbs posterior, is unfortunately in-
tractable. One may sample from it using Markov chain Monte Carlo, but this
is often too slow for big datasets.
We consider instead variational approxima-
tions of the Gibbs posterior, which are fast to compute. We undertake a general
study of the properties of such approximations. Our main ï¬nding is that such
a variational approximation has often the same rate of convergence as the origi-
nal PAC-Bayesian procedure it approximates. We specialise our results to several
learning tasks (classiï¬cation, ranking, matrix completion), discuss how to imple-
ment a variational approximation in each case, and illustrate the good properties
of said approximation on real datasets.
1.4.5 Towards automatic calibration of SMC2
ResumÂ´e
Nous Â´etudions SMC2, un algorithme pour lâ€™estimation de param`etres
dans les mod`eles `a espace dâ€™Â´etat. Nous gÂ´enerons pour cela NÎ¸ particules Î¸m et
pour chacune dâ€™elle nous lanÂ¸cons un ï¬ltre particulaire de taille Nx (i.e.
pour
chaque pas de temps, Nx particules sont gÂ´enÂ´erÂ´ees dans lâ€™espace dâ€™Â´etat X). Le but
de ce chapitre est dâ€™automatiser le choix de Nx au cours de lâ€™algorithme. Nous
utilisons pour cela un algorithme de Monte Carlo sÂ´equentiel conditionÂ´e `a une
trajectoire. Pour rÂ´eduire le coË†ut mÂ´emoriel nous proposons de sauvegarder lâ€™Â´etat
initial des gÂ´enerateurs pseudo-alÂ´eatoires pour chaque ï¬ltre particulaire. Le choix
de Nx est conditionÂ´e `a un estimateur de la variance de lâ€™estimateur sans biais de
la vraisemblance obtenu par des techniques non parametriques. Les applications
numÂ´eriques sont eï¬€ectuÂ´ees `a coË†ut computationel constant et dÂ´ebouchent sur une
plus petite erreur de Monte Carlo que lâ€™algorithme original.
Abstract
SMC2 (Chopin et al., 2013) is an eï¬ƒcient algorithm for sequential es-
timation and state inference of state-space models. It generates NÎ¸ parameter
particles Î¸m , and, for each Î¸m , it runs a particle ï¬lter of size Nx (i.e. at each
time step, Nx particles are generated in the state space X ). We discuss how to
automatically calibrate Nx in the course of the algorithm. Our approach relies on
conditional Sequential Monte Carlo updates, monitoring the state of the pseudo
random number generator and on an estimator of the variance of the unbiased es-
timate of the likelihood that is produced by the particle ï¬lters, which is obtained
using nonparametric regression techniques. We observe that our approach is both
less CPU intensive and with smaller Monte Carlo errors than the initial version of
SMC2 .
30

2
Leave Pima indians alone: binary regression as a
benchmark for Bayesian computation
This is joint work with Nicolas Chopin
Status: submitted to Statistical Science
2.1 Introduction
The ï¬eld of Bayesian computation seems hard to track these days, as it is blos-
soming in many directions.
MCMC (Markov chain Monte Carlo) remains the
main approach, but it is no longer restricted to Gibbs sampling and Hastings-
Metropolis, as it includes more advanced, Physics-inspired methods, such as HMC
[Hybrid Monte Carlo,
Neal, 2010b] and its variants [Girolami and Calderhead,
2011; Hoï¬€man and Gelman, 2013; Shahbaba et al., 2011].
On the other hand,
there is also a growing interest for alternatives to MCMC, such as SMC (Sequen-
tial Monte Carlo, e.g. Del Moral et al., 2006a), nested sampling [Skilling, 2006], or
the fast approximations that originated from machine learning, such as Variational
Bayes [e.g. Bishop, 2006b, Chap. 10], and EP [Expectation Propagation, Minka,
2001b]. Even Laplace approximation has resurfaced in particular thanks to the
INLA methodology [Rue et al., 2009].
One thing however that all these approaches have in common is they are almost
always illustrated by a binary regression example; see e.g. the aforementioned
papers. In other words, binary regressions models, such as probit or logit, are a
de facto benchmark for Bayesian computation.
This remark leads to several questions. Are binary regression models a reason-
able benchmark for Bayesian computation? Should they be used then to develop
a â€˜benchmark cultureâ€™ in Bayesian computation, like in e.g. optimisation? And
31

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
practically, which of these methods actually â€˜works bestâ€™ for approximating the
posterior distribution of a binary regression model?
The objective of this chapter is to answer these questions. As the ironic ti-
tle suggests, our ï¬ndings shall lead to us be critical of certain current practices.
Speciï¬cally, most papers seem content with comparing some new algorithm with
Gibbs sampling, on a few small datasets, such as the well-known Pima Indians di-
abetes dataset (8 covariates). But we shall see that, for such datasets, approaches
that are even more basic than Gibbs sampling are actually hard to beat. In other
words, datasets considered in the literature may be too toy-like to be used as a
relevant benchmark. On the other hand, if ones considers larger datasets (with
say 100 covariates), then not so many approaches seem to remain competitive.
We would also like to discuss how Bayesian computation algorithms should be
compared. One obvious criterion is the error versus CPU time trade-oï¬€; this im-
plies discussing which posterior quantities one may need to approximate. A related
point is whether the considered method comes with a simple way to evaluate the
numerical error. Other criteria of interest are: (a) how easy to implement is the
considered method? (b) how generic is it? (does changing the prior or the link
function requires a complete rewrite of the source code?) (c) to which extent does
it require manual tuning to obtain good performance? (d) is it amenable to par-
allelisation? Points (a) and (b) are rarely discussed in Statistics, but relate to the
important fact that, the simpler the program, the easier it is to maintain, and to
make it bug-free. Regarding point (c), we warn beforehand that, as a matter of
principle, we shall refuse to manually tune an algorithm on a per dataset basis.
Rather, we will discuss, for each approach, some (hopefully reasonable) general
recipe for how to choose the tuning parameters. This has two motivations. First,
human time is far more valuable than computer time: Cook [2014] mentions that
one hour of CPU time is today three orders of magnitude less expensive than one
hour of pay for a programmer (or similarly a scientist). Second, any method re-
quiring too much manual tuning through trial and error may be practically of no
use beyond a small number of experts.
Finally, we also hope this chapter may serve as an up to date review of the state
of Bayesian computation. We believe this review to be timely for a number of
reasons. First, as already mentioned, because Bayesian computation seems to de-
velop currently in several diï¬€erent directions. Second, and this relates to criterion
(d), the current interest in parallel computation [Lee et al., 2010; Suchard et al.,
2010] may require a re-assessment of Bayesian computational methods: method
A may perform better than method B on a single core architecture, while per-
forming much worse on a parallel architecture. Finally, although the phrase â€˜big
dataâ€™ seems to be a tired trope already, it is certainly true that datasets are get-
ting bigger and bigger, which in return means that statistical methods needs to
32

2.2 Preliminaries: binary regression models
be evaluated on bigger and bigger datasets. To be fair, we will not really consider
in this work the kind of huge datasets that pertain to â€˜big dataâ€™, but we will at
least strive to move away from the kind of â€˜ridiculously smallâ€™ data encountered
too often in Bayesian computation papers.
The chapter is structured as follows. Section 2.2 covers certain useful prelimi-
naries on binary regression models. Section 2.3 discusses fast approximations, that
is, deterministic algorithms that oï¬€er an approximation of the posterior, at a lower
cost than sampling-based methods. Section 2.4 discusses â€˜exactâ€™, sampling-based
methods. Section 2.5 is the most important part of the chapter, as it contains an
extensive numerical comparison of all these methods. Section 2.6 discusses vari-
able selection. Section 2.7 discusses our ï¬ndings, and their implications for both
end users and Bayesian computation experts.
2.2 Preliminaries: binary regression models
2.2.1 Likelihood, prior
The likelihood of a binary regression model have the generic expression
p(D|Î²) =
nD
Y
i=1
F(yiÎ²Txi)
(2.1)
where the data D consist of n responses yi âˆˆ{âˆ’1, 1} and n vectors xi of p covari-
ates, and F is some CDF (cumulative distribution function) that transforms the
linear form yiÎ²Txi into a probability. Taking F = Î¦, the standard normal CDF,
gives the probit model, while taking F = L, the logistic CDF, L(x) = 1/ (1 + eâˆ’x),
leads to the logistic model. Other choices could be considered, such as e.g. the
CDF of a Student distribution (robit model) to better accommodate outliers.
We follow Gelman et al. [2008]â€™s recommendation to standardise the predictors
in a preliminary step: non-binary predictors have mean 0 and standard deviation
0.5, binary predictors have mean 0 and range 1, and the intercept (if present)
is set to 1. This standardisation facilitates prior speciï¬cation: one then may set
up a â€œweakly informativeâ€ prior for Î², that is a proper prior that assigns a low
probability that the marginal eï¬€ect of one predictor is outside a reasonable range.
Speciï¬cally, we shall consider two priors p(Î²) in this work: (a) the default prior rec-
ommended by Gelman et al. [2008], a product of independent Cauchys with centre
0 and scale 10 for the constant predictor, 2.5 for all the other predictors (hence-
forth, the Cauchy prior); and (b) a product of independent Gaussians with mean
0 and standard deviation equal to twice the scale of the Cauchy prior (henceforth
the Gaussian prior).
33

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Of course, other priors could be considered, such as e.g. Jeï¬€reysâ€™ prior [Firth,
1993], or a Laplace prior [KabÂ´an, 2007]. Our main point in considering the two
priors above is to determine to which extent certain Bayesian computation methods
may be prior-dependent, either in their implementation (e.g. Gibbs sampling) or
in their performance, or both. In particular, one may expect the Cauchy prior to
be more diï¬ƒcult to deal with, given its heavy tails.
2.2.2 Posterior maximisation (Gaussian prior)
We explain in this section how to quickly compute the mode, and the Hessian at
the mode, of the posterior:
p(Î²|D) = p(Î²)p(D|Î²)
p(D)
,
p(D) =
Ë†
Rd p(Î²)p(D|Î²) dÎ²,
where p(Î²) is one of the two priors presented in the previous section, and Z(D) is
the marginal likelihood of the data (also known as the evidence). These quantities
will prove useful later, in particular to tune certain of the considered methods.
The two ï¬rst derivatives of the log-posterior density may be computed as:
âˆ‚
âˆ‚Î² log p(Î²|D) = âˆ‚
âˆ‚Î² log p(Î²) + âˆ‚
âˆ‚Î² log p(D|Î²),
âˆ‚2
âˆ‚Î²âˆ‚Î²T log p(Î²|D) =
âˆ‚2
âˆ‚Î²âˆ‚Î²T log p(Î²) +
âˆ‚2
âˆ‚Î²âˆ‚Î²T log p(D|Î²)
where
âˆ‚
âˆ‚Î² log p(D|Î²) =
nD
X
i=1
(log F)â€² (yiÎ²Txi)yixi
âˆ‚2
âˆ‚Î²âˆ‚Î²T log p(D|Î²) =
nD
X
i=1
(log F)â€²â€² (yiÎ²Txi)xixT
i
and (log F)â€² and (log F)â€²â€² are the two ï¬rst derivatives of log F. Provided that log F
is concave, which is the case for probit and logit regressions, the Hessian of the
log-likelihood is clearly a negative deï¬nite matrix. Moreover, if we consider the
Gaussian prior, then the Hessian is of the log-posterior is also negative (as the sum
of two negative matrices, as Gaussian densities are log-concave). We stick to the
Gaussian prior for now.
This suggests the following standard approach to compute the MAP (maximum
a posterior) estimator, that is the point Î²MAP that maximises the posterior density
p(Î²|D): to use Newton-Raphson, that is, to iterate
34

2.2 Preliminaries: binary regression models
Î²(new) = Î²(old) âˆ’Hâˆ’1
 âˆ‚
âˆ‚Î² log p(Î²(old)|D)

(2.2)
until convergence is reached; here H is Hessian of the log posterior at Î² = Î²(old),
as computed above.
The iteration above corresponds to ï¬nding the zero of a
local, quadratic approximation of the log-posterior.
Newton-Raphson typically
works very well (converges in a small number of iterations) when the function to
maximise is concave.
We note two points in passing.
First, one may obtain the MLE (maximum
likelihood estimator) by simply taking p(Î²) = 1 above (i.e.
a Gaussian with
inï¬nite variance). But the MLE is not properly deï¬ned when complete separation
occurs, that is, there exists a hyperplane that separates perfectly the two outcomes:
yiÎ²T
CSxi â‰¥0 for some Î²CS and all i âˆˆ1 : N. This remark gives an extra incentive
for performing Bayesian inference, or at least MAP estimation, in cases where
complete separation may occur, in particular when the number of covariates is
large [Firth, 1993; Gelman et al., 2008].
Second, H in (2.2) is sometimes replaced by some approximation, leading to so-
called quasi-Newton algorithms. Some of these algorithms such as IRLS (iterated
reweighted least squares) have a nice statistical interpretation. For our purposes
however, all these variants seem to show similar performance, so we will stick to
the standard version of Newton-Raphson.
2.2.3 Posterior maximisation (Cauchy prior)
The log-density of the Cauchy prior is not concave:
log p(Î²) = âˆ’
p
X
j=1
log (Ï€Ïƒj) âˆ’
p
X
j=1
log(1 + Î²2
j /Ïƒ2
j)
for scales Ïƒj chosen as explained in Section 2.2.1. Hence, the corresponding log-
posterior is no longer guaranteed to be concave, which in turn means that Newton-
Raphson algorithm might fail to converge.
However, we shall observe that, for most of the datasets considered in this
chapter, Newton-Raphson does converge quickly even for our Cauchy prior. In
each case, we used as starting point for the Newton-Raphson iterations the OLS
(ordinary least square) estimate.
We suspect what happens is that, for most
standard datasets, the posterior derived from a Cauchy prior remains log-concave,
at least in a region that encloses the MAP estimator and our starting point.
35

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2.3 Fast approximation methods
This section discusses fast approximation methods, that is methods that are de-
terministic, fast (compared to sampling-based methods), but which comes with an
approximation error which is diï¬ƒcult to assess. These methods include the Laplace
approximation, which was popular in Statistics before the advent of MCMC meth-
ods, but also recent Machine Learning methods, such as EP (Expectation Propa-
gation, Minka, 2001b), and VB (Variational Bayes, e.g. Bishop, 2006b, Chap. 10).
However, we will not discuss VB, as Consonni and Marin [2007] give convincing
(formal and numerical) arguments that this type of approach does not work well
for probit models.
Concretely, we will focus on the approximation of the following posterior quan-
tities: the marginal likelihood p(D), as this may be used in model choice; and the
marginal distributions p(Î²i|D) for each component Î²i of Î². Clearly these are the
most commonly used summaries of the posterior distribution, and other quantities,
such as the posterior expectation of Î², may be directly deduced from them.
Finally, one should bear in mind that such fast approximations may be used as
a preliminary step to calibrate an exact, more expensive method, such as those
described in Section 2.4.
2.3.1 Laplace approximation
The Laplace approximation is based on a Taylor expansion of the posterior log-
density around the mode Î²MAP:
log p(Î²|D) â‰ˆlog p(Î²MAP|D) âˆ’1
2 (Î² âˆ’Î²MAP)T Q (Î² âˆ’Î²MAP) ,
where Q = âˆ’H, i.e. minus the Hessian of log p(Î²|D) at Î² = Î²MAP; recall that
we explained how to compute these quantities in Section 2.2.2. One may deduce
a Gaussian approximation of the posterior by simply exponentiating the equation
above, and normalising:
qL(Î²) = Np
 Î²; Î²MAP, Qâˆ’1
:= (2Ï€)âˆ’p/2 |Q|1/2 exp

âˆ’1
2 (Î² âˆ’Î²MAP)T Q (Î² âˆ’Î²MAP)

.
(2.3)
In addition, since for any Î²,
p(D) = p(Î²)p(D|Î²)
p(Î²|D)
36

2.3 Fast approximation methods
one obtains an approximation to the marginal likelihood p(D) as follows:
p(D) â‰ˆZL(D) := p(Î²MAP)p(D|Î²MAP)
(2Ï€)âˆ’p/2 |Q|1/2
.
From now on, we will refer to this particular Gaussian approximation qL as the
Laplace approximation, even if this phrase is sometimes used in Statistics for
higher-order approximations, as discussed in the next Section. We defer to Section
2.3.5 the discussion of the advantages and drawbacks of this approximation scheme.
2.3.2 Improved Laplace, connection with INLA
Consider the marginal distributions p(Î²j|D) =
Â´
p(Î²|D)dÎ²âˆ’j for each component
Î²j of Î², where Î²âˆ’j is Î² minus Î²j. A ï¬rst approximation may be obtained by
simply computing the marginals of the Laplace approximation qL. An improved
(but more expensive) approximation may be obtained from:
p(Î²j|D) âˆp(Î²)p(D|Î²)
p(Î²âˆ’j|Î²j, D)
which suggests to choose a ï¬ne grid of Î²j values (deduced for instance from qL(Î²)),
and for each Î²j value, compute a Laplace approximation of p(Î²âˆ’j|Î²j, D), by com-
puting the mode Ë†Î²âˆ’j(Î²j) and the Hessian
Ë†
H(Î²j) of log p(Î²âˆ’j|Î²j, D), and then
approximate (up to a constant)
p(Î²j|D) â‰ˆqIL(Î²j) âˆ
p

Ë†Î²(Î²j)

p(D| Ë†Î²(Î²j))
 Ë†H(Î²j)

1/2
where Ë†Î²(Î²j) is the vector obtained by inserting Î²i at position i in Ë†Î²âˆ’j(Î²j), and
IL stands for â€œImproved Laplaceâ€. One may also deduce posterior expectations
of functions of Î²j in this way. See also Tierney and Kadane [1986], Tierney et al.
[1989] for higher order approximations for posterior expectations.
We note in passing the connection to the INLA scheme of Rue et al. [2009].
INLA applies to posteriors p(Î¸, x|D) where x is a latent variable such that p(x|Î¸, D)
is close to a Gaussian, and Î¸ is a low-dimensional hyper-parameter. It constructs
a grid of Î¸âˆ’values, and for each grid point Î¸j, it computes an improve Laplace
approximation of the marginals of p(x|Î¸j, D). In our context, Î² may be identiï¬ed
to x, Î¸ to an empty set, and INLA reduces to the improved Laplace approximation
described above.
37

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2.3.3 The EM algorithm of Gelman et al. [2008] (Cauchy prior)
Gelman et al. [2008] recommend against the Laplace approximation for a Student
prior (of which our Cauchy prior is a special case), because, as explained in Section
2.2.3, the corresponding log-posterior is not guaranteed to be concave, and this
might prevent Newton-Raphson to converge. In our simulations however, we found
the Laplace approximation to work reasonably well for a Cauchy prior. We now
brieï¬‚y describe the alternative approximation scheme proposed by Gelman et al.
[2008] for Student priors, which we call for convenience Laplace-EM.
Laplace-EM is based on the well-known representation of a Student distribution,
Î²j|Ïƒ2
j âˆ¼N1(0, Ïƒ2
j), Ïƒ2
j âˆ¼Inv âˆ’Gamma(Î½/2, sjÎ½/2); take Î½ = 1 to recover our
Cauchy prior.
Conditional on Ïƒ2 = (Ïƒ2
1, . . . , Ïƒ2
p), the prior on Î² is Gaussian,
hence, for a ï¬xed Ïƒ2 one may implement Newton-Raphson to maximise the log-
density of p(Î²|Ïƒ2, D), and deduce a Laplace (Gaussian) approximation of the same
distribution.
Laplace-EM is an approximate EM [Expectation Maximisation, Dempster et al.,
1977] algorithm, which aims at maximising in Ïƒ2 = (Ïƒ2
1, . . . , Ïƒ2
p) the marginal pos-
terior distribution p(Ïƒ2|D) =
Â´
p(Ïƒ2, Î²|D) dÎ². Each iteration involves an expec-
tation with respect to the intractable conditional distribution p(Î²|Ïƒ2, D), which
is Laplace approximated, using a single Newton-Raphson iteration. When this
approximate EM algorithm has converged to some value Ïƒ2
â‹†, one more Newton-
Raphson iteration is performed to compute a ï¬nal Laplace approximation of p(Î²|Ïƒ2
â‹†, D),
which is then reported as a Gaussian approximation to the posterior. We refer the
readers to Gelman et al. [2008] for more details on Laplace-EM.
2.3.4 Expectation-Propagation
Like Laplace, Expectation Propagation [EP, Minka, 2001b] generates a Gaussian
approximation of the posterior, but it is based on diï¬€erent ideas. The consensus
in machine learning seems to be that EP provides a better approximation than
Laplace [e.g.
Nickisch and Rasmussen, 2008]; the intuition being that Laplace is
â€˜too localâ€™ (i.e. it ï¬tted so at to match closely the posterior around the mode),
while EP is able to provide a global approximation to the posterior.
Starting from the decomposition of the posterior as product of (nD + 1) factors:
p(Î²|D) =
1
p(D)
nD
Y
i=0
li(Î²),
li(Î²) = F(yiÎ²Txi) for i â‰¥1,
and l0 is the prior, l0(Î²) = p(Î²), EP computes iteratively a parametric approxi-
38

2.3 Fast approximation methods
mation of the posterior with the same structure
qEP(Î²) =
nD
Y
i=0
1
Zi
qi(Î²).
(2.4)
Taking qi to be an unnormalised Gaussian densities written in natural exponential
form
qi(Î²) = exp

âˆ’1
2Î²TQiÎ² + Î²Tri

,
one obtains for qEP a Gaussian with natural parameters Q = Pn
i=0 Qi and ri =
Pn
i=0 ri; note that the more standard parametrisation of Gaussians may be recov-
ered by taking
Î£ = Qâˆ’1,
Âµ = Qâˆ’1r.
Other exponential families could be considered for q and the qiâ€™s, see e.g. Seeger
[2005b], but Gaussian approximations seems the most natural choice here.
An EP iteration consists in updating one factor qi, or equivalently (Zi, Qi, ri),
while keeping the other factors as ï¬xed, by moment matching between the hybrid
distribution
h(Î²) âˆli(Î²)
Y
jÌ¸=i
qj(Î²)
and the global approximation q deï¬ned in (2.4): compute
Zh
=
Ë†
li(Î²)
Y
jÌ¸=i
qj(Î²) dÎ²
Âµh
=
1
Zh
Ë†
Î²li(Î²)
Y
jÌ¸=i
qj(Î²) dÎ²
Î£h
=
1
Zh
Ë†
Î²Î²Tli(Î²)
Y
jÌ¸=i
qj(Î²) dÎ²
and set
Qi = Î£âˆ’1
h âˆ’Qâˆ’i,
ri = Î£âˆ’1
h Âµh âˆ’râˆ’i,
log Zi = log Zh âˆ’Î¨(r, Q) + Î¨(râˆ’i, Qâˆ’i)
where râˆ’i = P
jÌ¸=i rj, Qâˆ’i = P
jÌ¸=i Qj, and Ïˆ(r, Q) is the normalising constant of
a Gaussian distribution with natural parameters (r, Q),
Ïˆ(r, Q) =
Ë†
Rp exp

âˆ’1
2Î²TQÎ² + Î²Tr

dÎ² = âˆ’1
2 log |Q/2Ï€| + 1
2rTQr.
39

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
In practice, EP proceeds by looping over sites, updating each one in turn until
convergence is achieved.
To implement EP for binary regression models, two points must be addressed.
First, how to compute the hybrid moments? For the probit model, these moments
may be computed exactly, see the supplement to the paper, while for the other
links function (such as logistic), numerical (one-dimensional) quadrature may be
used.
Second, how to deal with the prior?
If the prior is Gaussian, one may
simply set q0 to the prior, and never update q0 in the course of the algorithm. For
a Cauchy prior, q0 is simply treated as an extra site.
EP being a fairly recent method, it is currently lacking in terms of supporting
theory, both in terms of algorithmic convergence (does it converge in a ï¬nite num-
ber of iterations?), and statistical convergence (does the resulting approximation
converges in some sense to the true posterior distribution as nD â†’+âˆ?). On the
other hand, there is mounting evidence that EP works very well in many problems;
again see e.g. Nickisch and Rasmussen [e.g. 2008].
2.3.5 Discussion of the diï¬€erent approximation schemes
Laplace (and its variants) have complexity O(nD + p3), while EP has complexity
O(nDp3). Incidentally, one sees that the number of covariates p is more critical
than the number of instances nD in determining how â€˜bigâ€™ (how time-intensive to
process) is a given dataset. This will be a recurring point in this chapter.
The p3 term in both complexities is due to the pÃ—p matrix operations performed
by both algorithms; e.g. the Newton-Raphson update (2.2) requires solving a linear
system of order p. EP requires to perform such p3 operations at each site (i.e. for
each single observation), hence the O(nDp3) complexity, while Laplace perform
such operations only once per iteration.
EP is therefore expected to be more
expensive than Laplace.
This remark may be mitigated as follows.
First, one may modify EP so as
to update the global approximation only at the end of each iteration (complete
pass over the data). The resulting algorithm [Van Gerven et al., 2010] may be
easily implemented on parallel hardware: simply distribute the nD factors over
the processors. Even without parallelisation, parallel EP requires only one single
matrix inversion per iteration.
Second, the â€˜improved Laplaceâ€™ approximation for the marginals described in
Section 2.3.1 requires to perform quite a few basic Laplace approximations, so its
speed advantage compared to standard EP essentially vanishes.
Points that remain in favour of Laplace is that it is simpler to implement than
EP, and the resulting code is very generic: adapting to either a diï¬€erent prior,
or a diï¬€erent link function (choice of F in 2.1), is simply a matter of writing a
function that evaluates the corresponding function. We have seen that such an
40

2.4 Exact methods
adaptation requires more work in EP, although to be fair the general structure of
the algorithm is not model-dependent. On the other hand, we shall see that EP is
often more accurate, and works in more examples, than Laplace; this is especially
the case for the Cauchy prior.
2.4 Exact methods
We now turn to sampling-based methods, which are â€˜exactâ€™, at least in the limit:
one may make the approximation error as small as desired, by running the cor-
responding algorithm for long enough. We will see that all of these algorithms
requires some form of calibration that requires prior knowledge on the shape of
the posterior distribution. Since the approximation methods covered in the pre-
vious section are faster by orders of magnitude than sampling-based methods, we
will assume that a Gaussian approximation q(Î²) (say, obtained by Laplace or EP)
has been computed in a preliminary step.
2.4.1 Our gold standard: Importance sampling
Let q(Î²) denote a generic approximation of the posterior p(Î²|D).
Importance
sampling (IS) is based on the trivial identity
p(D) =
Ë†
p(Î²)p(D|Î²) dÎ² =
Ë†
q(Î²)p(Î²)p(D|Î²)
q(Î²)
dÎ²
which leads to the following recipe: sample Î²1, . . . , Î²N âˆ¼q, then compute as an
estimator of p(D)
ZN = 1
N
N
X
n=1
w(Î²n),
w(Î²) := p(Î²)p(D|Î²)
q(Î²)
.
(2.5)
In addition, since
Ë†
Ï•(Î²)p(Î²|D) dÎ² =
Â´
Ï•(Î²)q(Î²)w(Î²) dÎ²
Â´
q(Î²)w(Î²) dÎ²
one may approximate any posterior moment as
Ï•N =
PN
n=1 w(Î²n)Ï•(Î²n)
PN
n=1 w(Î²n)
.
(2.6)
Approximating posterior marginals is also straightforward; one may instance use
kernel density estimation on the weighted sample (Î²n, w(Î²n))N
n=1.
41

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Concerning the choice of q, we will restrict ourselves to the Gaussian approxima-
tions generated either from Laplace or EP algorithm. It is sometimes recommended
to use a Student distribution instead, as a way to ensure that the variance of the
above estimators is ï¬nite, but we did not observe any beneï¬t for doing so in our
simulations.
It is of course a bit provocative to call IS our gold standard, as it is sometimes
perceived as an obsolete method. We would like to stress out however that IS is
hard to beat relative to most of the criteria laid out in the introduction:
â€¢ because it is based on IID sampling, assessing the Monte Carlo error of the
above estimators is trivial: e.g. the variance of ZN may be estimated as N âˆ’1
times the empirical variance of the weights w(Î²n).
The auto-normalised
estimator 2.6 has asymptotic variance
Eq

w(Î²)2 {Ï•(Î²) âˆ’Âµ(Ï•)}2
,
Âµ(Ï•) =
Ë†
Ï•(Î²)p(Î²|D) dÎ²
which is also trivial to approximate from the simulated Î²nâ€™s.
â€¢ Other advantages brought by IID sampling are: (a) importance sampling
is easy to parallelize; and (b) importance sampling is amenable to QMC
(Quasi-Monte Carlo) integration, as explained in the following section.
â€¢ Importance sampling oï¬€ers an approximation of the marginal likelihood p(D)
at no extra cost.
â€¢ Code is simple and generic.
Of course, what remains to determine is whether importance sampling does well
relative to our main criterion, i.e. error versus CPU trade-oï¬€. We do know that IS
suï¬€ers from a curse of dimensionality: take both q and and the target density Ï€ to
be the density of IID distributions: q(Î²) = Qp
j=1 q1(Î²j), Ï€(Î²) = Qp
j=1 Ï€1(Î²j); then
it is easy to see that the variance of the weights grows exponentially with p. Thus
we expect IS to collapse when p is too large; meaning that a large proportion of the
Î²n gets a negligible weight. On the other hand, for small to moderate dimensions,
we will observe surprising good results; see Section 2.5. We will also present below
a SMC algorithm that automatically reduces to IS when IS performs well, while
doing something more elaborate in more diï¬ƒcult scenarios.
The standard way to assess the weight degeneracy is to compute the eï¬€ective
sample size [Kong et al., 1994],
ESS =
nPN
n=1 w(Î²n)
o2
PN
n=1 w(Î²n)2
âˆˆ[1, N],
42

2.4 Exact methods
which roughly approximates how many simulations from the target distribution
would be required to produce the same level of error. In our simulations, we will
compute instead the eï¬ƒciency factor EF, which is simply the ratio EF = ESS/N.
2.4.2 Improving importance sampling by Quasi-Monte Carlo
Quasi-Monte Carlo may be seen as an elaborate variance reduction technique:
starting from the Monte Carlo estimators ZN and Ï•N, see (2.5) and (2.6), one
may re-express the simulated vectors as functions of uniform variates un in [0, 1]d;
for instance:
Î²n = Âµ + CÎ¶n,
Î¶n = Î¦âˆ’1(un)
where Î¦âˆ’1 is Î¦âˆ’1, the N(0, 1) inverse CDF, applied component-wise. Then, one
replaces the N vectors un by a low-discrepancy sequence; that is a sequence of N
vectors that spread more evenly over [0, 1]d; e.g. a Halton or a Sobolâ€™ sequence.
Under appropriate conditions, QMC error converges at rate O(N âˆ’1+Ç«), for any
Ç« > 0, to be compared with the standard Monte Carlo rate OP(N âˆ’1/2). We refer
to Lemieux [2009] for more background on QMC, as well as how to construct QMC
sequences.
Oddly enough, the possibility to use QMC in conjunction with importance sam-
pling is very rarely mentioned in the literature; see however HÂ¨ormann and Leydold
[2005]. More generally, QMC seems often overlooked in Statistics. We shall see
however that this simple IS-QMC strategy often performs very well.
One drawback of IS-QMC is that we lose the ability to evaluate the approxi-
mation error in a simple manner. A partial remedy is to use randomised Quasi-
Monte Carlo (RQMC), that is, the un are generated in such a way that (a) with
probability one, u1:N is a QMC point set; and (b) each vector un is marginally
sampled from [0, 1]d. Then QMC estimators that are empirical averages, such as
ZN = N âˆ’1 PN
n=1 w(Î²n) become unbiased estimators, and their error may be as-
sessed through the empirical variance over repeated runs. Technically, estimators
that are ratios of QMC averages, such as Ï•N, are not unbiased, but for all prac-
tical purposes their bias is small enough that assessing error through empirical
variances over repeated runs remains a reasonable approach.
2.4.3 MCMC
The general principle of MCMC (Markov chain Monte Carlo) is to simulate a
Markov chain that leaves invariant the posterior distribution p(Î²|D); see Robert and Casella
[2004a] for a general overview. Often mentioned drawbacks of MCMC simulation
are (a) the diï¬ƒculty to parallelize such algorithms (although see e.g. Jacob et al.,
43

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2011 for an attempt at this problem); (b) the need to specify a good starting point
for the chain (or alternatively to determine the burn-in period, that is, the length
of the initial part of the chain that should be discarded) and (c) the diï¬ƒculty to
assess the convergence of the chain (that is, to determine if the distribution of Î²t
at iteration t is suï¬ƒciently close to the invariant distribution p(Î²|D)).
To be fair, these problems are not so critical for binary regression models. Re-
garding (b), one may simply start the chain from the posterior mode, or from
a draw of one of the Gaussian approximations covered in the previous section.
Regarding (c) for most standard datasets, MCMC converges reasonably fast, and
convergence is easy to assess visually. The main issue in practice is that MCMC
generates correlated random variables, and these correlations inï¬‚ate the Monte
Carlo variance.
Gibbs sampling
Consider the following data-augmentation formulation of binary regression:
zi
=
Î²Txi + Ç«i
yi
=
sgn(zi)
where z = (z1, . . . , znD)T is a vector of latent variables, and assume for a start
that Ç«i âˆ¼N(0, 1) (probit regression). One recognises p(Î²|z, D) as the posterior
of a linear regression model, which is tractable (for an appropriate prior). This
suggests to sample from p(Î², z|D) using Gibbs sampling [Albert and Chib, 1993]:
i.e. iterate the two following steps: (a) sample from z|Î², D; and (b) sample from
Î²|z, D.
For (a), the ziâ€™s are conditionally independent, and follows a truncated Gaussian
distribution
p(zi|Î², D) âˆN1
 zi; Î²Txi, 1

âœ¶{ziyi > 0}
which is easy to sample from [Chopin, 2011b]. For Step (b) and a Gaussian prior
Np(0, Î£prior), one has, thanks to standard conjugacy properties:
Î²|z, D âˆ¼Np (Âµpost(z), Î£post) ,
Î£âˆ’1
post = Î£âˆ’1
prior + xxT,
Âµpost(z) = Î£âˆ’1
postxz
where x is the n Ã— p matrix obtained by stacking the xT
i . Note that Î£post and its
inverse need to be computed only once, hence the complexity of a Gibbs iteration
is O(p2), not O(p3).
The main drawback of Gibbs sampling is that it is particularly not generic: its
implementation depends very strongly on the prior and the model. Sticking to
the probit case, switching to another prior requires deriving a new way to update
Î²|z, D. For instance, for a prior which is a product of Students with scales Ïƒj
44

2.4 Exact methods
(e.g. our Cauchy prior), one may add extra latent variables, by resorting to the
well-known representation: Î²j|sj âˆ¼N1(0, Î½Ïƒ2
j/sj), sj âˆ¼Chi2(Î½); with Î½ = 1 for
our Cauchy prior. Then the algorithm have three steps: (a) an update of the ziâ€™s,
exactly as above; (b) an update of Î², as above but with Î£prior replaced by the diag-
onal matrix with elements Î½Ïƒ2
j/sj, j = 1, . . . , p; and (c) an (independent) update of
the p latent variables sj, with sj|Î², z, D âˆ¼Gamma
 (1 + Î½)/2,
 1 + Î½Î²2
j /Ïƒ2
j

/2

.
The complexity of Step (b) is now O(p3), since Î£prior and Î£post must be recom-
puted at each iteration.
Of course, considering yet another type of prior would require deriving an-
other strategy for sampling Î². Then if one turns to logistic regression, things
get rather complicated. In fact, deriving an eï¬ƒcient Gibbs sampler for logistic
regression is a topic of current research; see FrÂ¨uhwirth-Schnatter and FrÂ¨uhwirth
[2009]; Gramacy and Polson [2012]; Holmes and Held [2006]; Polson et al. [2013].
In a nutshell, the two ï¬rst papers use the same data augmentation as above, but
with Ç«i âˆ¼Logistic(1) written as a certain mixture of Gaussians (inï¬nite for the ï¬rst
paper, ï¬nite but approximate for the second paper), while Polson et al. [2013] use
instead a representation of a logistic likelihood as an inï¬nite mixture of Gaussians,
with a Polya-Gamma as the mixing distribution. Each representation leads to in-
troducing extra latent variables, and discussing how to sample their conditional
distributions.
Since their implementation is so model-dependent, the main justiï¬cation for
Gibbs samplers should be their greater performance relative to more generic algo-
rithms. We will investigate if this is indeed the case in our numerical section.
Hastings-Metropolis
Hastings-Metropolis consists in iterating the step described as Algorithm 6. Much
like importance sampling, Hastings-Metropolis is both simple and generic, that is,
up to the choice of the proposal kernel Îº(Î²â‹†|Î²) (the distribution of the proposed
point Î²â‹†, given the current point Î²). A naive approach is to take Îº(Î²â‹†|Î²) inde-
pendent of Î², Îº(Î²â‹†|Î²) = q(Î²â‹†), where q is some approximation of the posterior.
In practice, this usually does not work better than importance sampling based on
the same proposal, hence this strategy is hardly used.
A more usual strategy is to set the proposal kernel to a random walk: Îº(Î²â‹†|Î²) =
Np(Î², Î£prop). It is well known that the choice of Î£prop is critical for good perfor-
mance. For instance, in the univariate case, if Î£prop is too small, the chain moves
slowly, while if too large, proposed moves are rarely accepted.
A result from the optimal scaling literature [e.g. Roberts and Rosenthal, 2001]
is that, for a Np(0, Ip) target, Î£prop = (Î»2/p)Ip with Î» = 2.38 is asymptotically
optimal, in the sense that as p â†’âˆ, this choice leads to the fastest exploration.
Since the posterior of a binary regression model is reasonably close to a Gaussian,
45

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Algorithm 6 Hastings-Metropolis iteration
Input Î²
Output Î²â€²
1 Sample Î²â‹†âˆ¼Îº(Î²â‹†|Î²).
2 With probability 1 âˆ§r,
r = p(Î²â‹†)p(D|Î²â‹†)Îº(Î²|Î²â‹†)
p(Î²)p(D|Î²)Îº(Î²â‹†|Î²) ,
set Î²â€² = Î²â‹†; otherwise set Î²â€² = Î².
we adapt this result by taking Î£prop = (Î»2/p)Î£q in our simulations, where Î£q is the
covariance matrix of a (Laplace or EP) Gaussian approximation of the posterior.
This strategy seems validated by the fact we obtain acceptance rates close to the
optimal rate, as given by Roberts and Rosenthal [2001].
The bad news behind this optimality result is that the chain requires O(p) steps
to move a O(1) distance. Thus random walk exploration tends to become slow for
large p. This is usually cited as the main motivation to develop more elaborate
MCMC strategies, such as HMC, which we cover in the following section.
HMC
Hamiltonian Monte Carlo (HMC, also known as Hybrid Monte Carlo, Duane et al.,
1987) is a new type of MCMC algorithm, where one is able to perform several
steps in the parameter space before determining if the new position is accepted
or not. Consequently, HMC is able to make much bigger jumps in the parameter
space than standard Metropolis algorithms.
See Neal [2010b] for an excellent
introduction.
Consider the pair (Î², Î±), where Î² âˆ¼p(Î²|D), and Î± âˆ¼Np(0, M âˆ’1), thus with
joint un-normalised density exp {âˆ’H(Î², Î±)}, with
H(Î², Î±) = E(Î²) + 1
2Î±TMÎ±,
E(Î²) = âˆ’log {p(Î²)p(D|Î²)} .
The physical interpretation of HMC is that of a particle at position Î², with velocity
Î±, potential energy E(Î²), kinetic energy 1
2Î±TMÎ±, for some mass matrix M, and
therefore total energy given by H(Î², Î±).
The particle is expected to follow a
trajectory such that H(Î², Î±) remains constant over time.
In practice, HMC proceeds as follows:
ï¬rst, sample a new velocity vector,
Î± âˆ¼Np(0, M âˆ’1). Second, move the particle while keeping the Hamiltonian H
46

2.4 Exact methods
constant; in practice, discretisation must be used, so L steps of step-size Ç« are
performed through leap-frop steps; see Algorithm 7 which describes one such step.
Third, the new position, obtained after L leap-frog steps is accepted or rejected
according to probability 1 âˆ§exp {H(Î², Î±) âˆ’H(Î²â‹†, Î±â‹†)}; see Algorithm 8 for a
summary. The validity of the algorithm relies on the fact that a leap-frog step is
â€œvolume preservingâ€; that is, the deterministic transformation (Î², Î±) â†’(Î²1, Î±1)
has Jacobian one. This is why the acceptance probability admits this simple ex-
pression.
Algorithm 7 Leap-frog step
Input (Î², Î±)
Output (Î²1, Î±1)
1 Î±1/2 â†Î± âˆ’Ç«
2âˆ‡Î²E(Î²)
2 Î²1 â†Î² + Ç«Î±1/2
3 Î±1 â†Î±1/2 âˆ’Ç«
2âˆ‡Î²E(Î²1)
Algorithm 8 HMC iteration
Input Î²
Output Î²â€²
1 Sample momentum Î± âˆ¼Np(0, M).
2 Perform L leap-frog steps (see Algorithm 7), starting from (Î², Î±); call (Î²â‹†, Î±â‹†)
the ï¬nal position.
3 With probability 1 âˆ§r,
r = exp {H(Î², Î±) âˆ’H(Î²â‹†, Î±â‹†)}
set Î²
â€² = Î²â‹†; otherwise set Î²
â€² = Î².
The tuning parameters of HMC are M (the mass matrix), L (number of leap-
frog steps), and Ç« (the stepsize). For M, we follow Neal [2010b]â€™s recommendation
and take M âˆ’1 = Î£q, an approximation of the posterior variance (again obtained
from either Laplace or EP). This is equivalent to rescaling the posterior so as to
47

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
have a covariance matrix close to identity. In this way, we avoid the bad mixing
typically incurred by strong correlations between components.
The diï¬ƒculty to choose L and Ç« seems to be the main drawback of HMC. The
performance of HMC seems very sensitive to these tuning parameters, yet clear
guidelines on how to choose them seem currently lacking. A popular approach
is to ï¬x LÇ« to some value, and to use vanishing adaptation [Andrieu and Thoms,
2008] to adapt Ç« so as to target acceptance rate of 0.65 (the optimal rate according
to the formal study of HMC by Beskos et al., 2013): i.e. at iteration t, take Ç« = Ç«t,
with Ç«t = Ç«tâˆ’1 âˆ’Î·t(Rt âˆ’0.65), Î·t = tâˆ’Îº, Îº âˆˆ(1/2, 1) and Rt the acceptance rate
up to iteration t. The rationale for ï¬xing LÇ« is that quantity may be interpreted
as a â€˜simulation lengthâ€™, i.e. how much distance one moves at each step; if too
small, the algorithm may exhibit random walk behaviour, while if too large, it
may move a long distance before coming back close to its starting point. Since the
spread of is already taken into account through M âˆ’1 = Î£q, we took Ç«L = 1 in our
simulations.
NUTS and other variants of HMC
Girolami and Calderhead [2011] proposed an interesting variation of HMC, where
the mass matrix M is allowed to depends on Î²; e.g. M(Î²) is set to the Fisher
information of the model. This allows the corresponding algorithm, called RHMC
(Riemanian HMC), to adapt locally to the geometry of the target distribution.
The main drawback of RHMC is that each iteration involves computing derivatives
of M(Î²) with respect to Î², which is very expensive, especially if p is large. For
binary regression, we found RMHC to be too expensive relative to plain HMC, even
when taking into account the better exploration brought by RHMC. This might
be related to the fact that the posterior of a binary regression model is rather
Gaussian-like and thus may not require such a local adaptation of the sampler.
We now focus on NUTS [No U-Turn sampler, Hoï¬€man and Gelman, 2013], a
variant of HMC which does not require to specify a priori L, the number of leap-
frog steps. Instead, NUTS aims at keeping on doing such steps until the trajectory
starts to loop back to its initial position. Of course, the diï¬ƒculty in this exercise
is to preserve the time reversibility of the simulated Markov chain. To that eï¬€ect,
NUTS constructs iteratively a binary tree whose leaves correspond to diï¬€erent
velocity-position pairs (Î±, Î²) obtained after a certain number of leap-frog steps.
The tree starts with two leaves, one at the current velocity-position pair, and
another leaf that corresponds to one leap-frop step, either in the forward or back-
ward direction (i.e. by reversing the sign of velocity); then it iteratively doubles
the number of leaves, by taking twice more leap frog steps, again either in the
forward or backward direction.
The tree stops growing when at least one leaf
corresponds to a â€œU-turnâ€; then NUTS chooses randomly one leaf, among those
48

2.4 Exact methods
leaves that would have generated the current position with the same binary tree
mechanism; in this way reversibility is preserved. Finally NUTS moves the new
position that corresponds to the chosen leaf.
We refer the readers to Hoï¬€man and Gelman [2013] for a more precise descrip-
tion of NUTS. Given its complexity, implementing directly NUTS seems to require
more eï¬€orts than the other algorithms covered in this chapter. Fortunately, the
STAN package (http://mc-stan.org/) provides a C++ implementation of NUTS
which is both eï¬ƒcient and user-friendly: the only required input is a description
of the model in a probabilistic programming language similar to BUGS. In partic-
ular, STAN is able to automatically derive the log-likelihood and its gradient, and
no tuning of any sort is required from the user. Thus, we will use STAN to assess
NUTS in our numerical comparisons.
2.4.4 Sequential Monte Carlo
Sequential Monte Carlo (SMC) is a class of algorithms for approximating iter-
atively a sequence of distributions Ï€t, t = 0, . . . , T, using importance sampling,
resampling, and MCMC steps. We focus here on the non-sequential use of SMC
[Chopin, 2002b; Del Moral et al., 2006a; Neal, 2001b], where one is only inter-
ested in approximating the ï¬nal distribution Ï€T (in our case, set to the posterior
p(Î²|D)), and the previous Ï€tâ€™s are designed so as to allow for a smooth progression
from some Ï€0, which is easy to sample from, to Ï€T.
At iteration t, SMC produces a set of weighted particles (simulations) (Î²n, wn)N
n=1
that approximates Ï€t, in the sense that
1
PN
n=1 wn
N
X
n=1
wnÏ•(Î²n) â†’EÏ€t [Ï•(Î²)]
as N â†’+âˆ. At time 0, one samples Î²n âˆ¼Ï€0, and set wn = 1. To progress
from Ï€tâˆ’1 to Ï€t, one uses importance sampling: weights are multiplied by ratio
Ï€t(Î²n)/Ï€tâˆ’1(Î²n). When the variance of the weights gets too large (which indicates
that too few particles contribute signiï¬cantly to the current approximation), one
resamples the particles: each particle gets reproduced On times, where On â‰¥0
is random, and such that E(On) = Nwn/ PN
m=1 wm, and PN
n=1 On = N with
probability one. In this way, particles with a low weights are likely to die, while
particles with a large weight get reproduced many times. Finally, one may re-
introduce diversity among the particles by applying one (or several) MCMC steps,
using a MCMC kernel that leaves invariant the current distribution Ï€t.
We focus in this chapter on tempering SMC, where the sequence
Ï€t(Î²) âˆq(Î²)1âˆ’Î´t {p(Î²)p(D|Î²)}Î´t
49

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
corresponds to a linear interpolation (on the log-scale) between some distribution
Ï€0 = q, and Ï€T(Î²) = p(Î²|D), our posterior. This is a convenient choice in our
case, as we have at our disposal some good approximation q (either from Laplace
or EP) of our posterior. A second advantage of tempering SMC is that one can
automatically adapt the â€œtemperature ladderâ€ Î´t [Jasra et al., 2011a]. Algorithm
9 describes a tempering SMC algorithm based on such an adaptation scheme:
at each iteration, the next distribution Ï€t is chosen so that the eï¬ƒciency factor
(deï¬ned in Section 2.4.1) of the importance sampling step from Ï€tâˆ’1 to Ï€t equals
a pre-deï¬ned level Ï„ âˆˆ(0, 1); a default value is Ï„ = 1/2.
Algorithm 9 tempering SMC
Operations involving index n must be performed for all n âˆˆ1 : N.
0 Sample Î²n âˆ¼q(Î²) and set Î´ â†0.
1 Let, for Î´ âˆˆ[Î´, 1],
EF(Î´) = 1
N
nPN
n=1 wÎ³(Î²n)
o2
nPN
n=1 wÎ³(Î²n)2
o,
uÎ´(Î²) =
p(Î²)p(D|Î²)
q(Î²)
Î´
.
If EF(1) â‰¥Ï„, stop and return (Î²n, wn)n=1:N with wn = u1(Î²n); otherwise,
use the bisection method [Press et al., 2007, Chap. 9] to solve numerically
in Î´ the equation EF(Î³) = Ï„.
2 Resample according to normalised weights Wn = wn/ PN
m=1 wm, with wn =
uÎ´(Î²n); see the supplement for one such resampling algorithm.
3 Update the Î²nâ€™s through m MCMC steps that leaves invariant Ï€t(Î²), using
e.g. Algorithm 6 with Îº(Î²â‹†|Î²) = Np(Î², Î£prop), Î£prop = Î» Ë†Î£, where Ë†Î£ is the
empirical covariance matrix of the resampled particles.
4 Set Î´ â†Î´. Go to Step 1.
Another part of Algorithm 9 which is easily amenable to automatic calibration
is the MCMC step. We use a random walk Metropolis step, i.e. Algorithm 6
with proposal kernel Îº(Î²â‹†|Î²) = Np(Î², Î£prop), but with Î£prop calibrated to the
empirical variance of the particles Ë†Î£: Î£prop = Î» Ë†Î£, for some Î». Finally, one may
also automatically calibrate the number m of MCMC steps, as in Chapter 3, but
in our simulations we simply took m = 3.
In the end, one obtains essentially a black-box algorithm. In practice, we shall
50

2.5 Numerical study
often observe that, for simple datasets, our SMC algorithm automatically reduces
to a single importance sampling step, because the eï¬ƒciency factor of moving from
the initial distribution q to the posterior is high enough. In that case, our SMC
sampler performs exactly as standard importance sampling.
2.5 Numerical study
The point of this section is to compare numerically the diï¬€erent methods discussed
in the previous sections, ï¬rst on several datasets of standard size (that are repre-
sentative of previous numerical studies), then in a second time on several bigger
datasets.
We focus on the following quantities: the marginal likelihood of the data, p(D),
and the p marginal posterior distributions of the regression coeï¬ƒcients Î²j. Re-
garding the latter, we follow Faes et al. [2011] in deï¬ning the â€˜marginal accuracyâ€™
of approximation q for component j to be
MAj = 1 âˆ’1
2
Ë† +âˆ
âˆ’âˆ
|q(Î²j) âˆ’p(Î²j|D)| dÎ²j.
This quantity lies in [0, 1], and is scale-invariant. Since the true marginals p(Î²j|D)
are not available, we will approximate them through a Gibbs sampler run for a
very long time. To give some scale to this criterion, assume q(Î²j) = N1(Î²j; Âµ1, Ïƒ2),
p(Î²j|D) = N1(Î²j; Âµ2, Ïƒ2), then MAj is 2Î¦(âˆ’Î´/2) â‰ˆ1 âˆ’0.4 Ã— Î´ for Î´ = |Âµ1 âˆ’Âµ2|/Ïƒ
small enough; e.g. 0.996 for Î´ â‰ˆ0.01, 0.96 for Î´ â‰ˆ0.1.
In our results, we will refer to the following four prior/model â€˜scenariosâ€™: Gaus-
sian/probit, Gaussian/logit, Cauchy/probit, Cauchy/logit, where Gaussian and
Cauchy refer to the two priors discussed in Section 2.2.1. All the algorithms have
been implemented in C++, using the Armadillo and Boost libraries, and run on
a standard desktop computer (except when explicitly stated). Results for NUTS
were obtained by running STAN (http://mc-stan.org/) version 2.4.0.
2.5.1 Datasets of moderate size
Table 2.1 lists the 7 datasets considered in this section (obtained from the UCI ma-
chine learning repository, except Elections, which is available on the web page of
Gelman and Hill [2006]â€™s book). These datasets are representative of the numerical
studies found in the literature. In fact, it is a super-set of the real datasets consid-
ered in Girolami and Calderhead [2011], Shahbaba et al. [2011], Holmes and Held
[2006] and also (up to one dataset with 5 covariates) Polson et al. [2013]. In each
case, an intercept have been included; i.e. p is the number of predictors plus one.
51

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Dataset
nD
p
Pima (Indian diabetes)
532
8
German (credit)
999
25
Heart (Statlog)
270
14
Breast (cancer)
683
10
Liver (Indian Liver patient)
579
11
Plasma (blood screening data)
32
3
Australian (credit)
690
15
Elections
2015
52
Table 2.1: Datasets of moderate size (from UCI repository, ex-
cept Elections, from web-site of Gelman and Hill [2006]â€™s
book): name (short and long version), number of instances
nD, number of covariates p (including an intercept)
Fast Approximations
We compare the four approximation schemes described in Section 2.3: Laplace, Im-
proved Laplace, Laplace EM, and EP. We concentrate on the Cauchy/logit scenario
for two reasons: (i) Laplace EM requires a Student prior; and (ii) Cauchy/logit
seems the most challenging scenario for EP, as (a) a Cauchy prior is more diï¬ƒcult
to deal with than a Gaussian prior in EP ; and (b) contrary to the probit case,
the site update requires some approximation; see Section 2.3.4 for more details.
Left panel of Fig. 2.1 plots the marginal accuracies of the four approximation
schemes across all components and all datasets; Fig.
2.2 does the same, but
separately for four selected datasets; results for the remaining datasets are available
in the supplement.
EP seems to be the most accurate method on these datasets: marginal accuracy
is about 0.99 across all components for EP, while marginal accuracy of the other
approximation schemes tend to be lower, and may even drop to quite small values;
see e.g. the German dataset, and the left tail in the left panel of Fig. 2.1.
EP also fared well in terms of CPU time: it was at most seven times as intensive
as standard Laplace across the considered datasets, and about 10 to 20 times faster
than Improved Laplace and Laplace EM. As expected (see Section 2.3.5), the
largest running time for EP was observed for the dataset with the largest number
of observations (German credit): i.e. 0.14s, while Laplace took 0.02s on the same
data. Of course, the usual caveats apply regarding CPU time comparison, and
how they may depend on the hardware, the implementation, and so on.
We also note in passing the disappointing performance of Laplace EM, which
was supposed to replace standard Laplace when the prior is Student, but which
52

2.5 Numerical study
0
50
100
0.4
0.6
0.8
1.0
value
density
EP
Improved Laplace
Laplace
Laplace EM
â—
â—
â—
â—
â—
â—
â—
â—
0.00
0.05
0.10
0.15
0.20
0.25
10
20
30
40
50
dim
absolute log error
â—EP
Laplace
Figure 2.1: Comparison of approximation schemes across all datasets
of moderate size: marginal accuracies (left), and abso-
lute error for log-evidence versus the dimension p (right);
xâˆ’axis range of the left plot determined by range of
marginal accuracies (i.e.
marginal accuracy may drop
below 0.4 for e.g. Laplace-EM).
actually performs not as well as standard Laplace on these datasets.
We refer the reader to the supplement for similar results on the three other
scenarios, which are consistent with those above. In addition, we also represent
the approximation error of EP and Laplace for approximating the log-evidence in
the right panel of Fig. 2.1. Again, EP is found to be more accurate than Laplace
for most datasets (except for the Breast dataset).
To conclude, it seems that EP may be safely be used as a complete replacement
of sampling-based methods on such datasets, as it produces nearly instant results,
and the approximation error along all dimensions is essentially negligible.
Importance sampling, QMC
We now turn to importance sampling (IS), which we deemed our â€œgold standardâ€
among sampling-based methods, because of its ease of use and other nice properties
as discussed in Section 2.4.1. We use N = 5 Ã— 105 samples, and a Gaussian EP
proposal.
(Results with a Laplace proposal are roughly similar.)
We consider
ï¬rst the Gaussian/probit scenario, because this is particularly favorable to Gibbs
sampling; see next section. Table 2.2 reports for each dataset the eï¬ƒciency factor
of IS (as deï¬ned in Section 2.4.1), the CPU time and two other quantities discussed
53

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
â—
â—
0.925
0.950
0.975
1.000
EP
Improved Laplace
Laplace
Laplace EM
value
(a) Pima
â—
0.92
0.96
1.00
EP
Improved Laplace
Laplace
Laplace EM
value
(b) Heart
0.925
0.950
0.975
1.000
EP
Improved Laplace
Laplace
Laplace EM
value
(c) Breast
â—
â—
â—
â—
â—
â—
0.80
0.85
0.90
0.95
1.00
EP
Improved Laplace
Laplace
Laplace EM
value
(d) German
Figure 2.2: Box-plots of marginal accuracies across the p dimensions,
for the four approximation schemes, and four selected
datasets; plots for remaining datasets are in the supple-
ment. For the sake of readability, scale of yâˆ’axis varies
across plots.
54

2.5 Numerical study
below.
IS
IS-QMC
Dataset
EF
CPU
MT
MSE improv.
MSE improv.
= ESS/N
time
speed-up
(expectation)
(evidence)
Pima
99.5%
37.54 s
4.39
28.9
42.7
German
97.9%
79.65 s
4.51
13.2
8.2
Breast
82.9%
50.91 s
4.45
2.6
6.2
Heart
95.2%
22.34 s
4.53
8.8
9.3
Liver
74.2 %
35.93 s
4.76
7.6
11.3
Plasma
90.0%
2.32 s
4.28
2.2
4.4
Australian
95.6%
53.32 s
4.57
12
20.3
Elections
21.39%
139.48 s
3.87
617.9
3.53
Table 2.2: Performance of importance sampling (IS), and QMC im-
portance sampling (IS-QMC), on all datasets, in Gaus-
sian/probit scenario: eï¬ƒciency factor (EF), CPU time
(in seconds), speed gain when using multi-threading In-
tel hyper-threaded quad core CPU (Speed gain MT), and
eï¬ƒciency gain of QMC (see text).
We see that all these eï¬ƒciency factors are all close to one, which means IS works
almost as well as IID sampling would on such datasets.
Further improvement
may be obtained by using either parallelization, or QMC (Quasi-Monte Carlo, see
Section 2.4.2). Table 2.2 reports the speed-up factor obtained when implementing
multi-threading on our desktop computer which has a multi threading quad core
CPU (hence 8 virtual cores). We also implemented IS on an Amazon EC2 instance
with 32 virtual CPUs, and obtained speed-up factors about 20, and running times
below 2s.
Finally, Table 2.2 also reports the MSE improvement (i.e. MSE ratio of IS rela-
tive to IS-QMC) obtained by using QMC, or more precisely RQMC (randomised
QMC), based on a scrambled Sobolâ€™ sequence [see e.g.
Lemieux, 2009]. Speciï¬-
cally, the table reports the median MSE improvement for the p posterior expecta-
tions (ï¬rst column), and the MSE improvement for the evidence (second column).
The improvement brought by RQMC varies strongly across datasets.
The eï¬ƒciency gains brought by parallelization and QMC may be combined,
because the bulk of the computation (as reported by a proï¬ler) is the N likelihood
evaluations, which are trivial to parallelize.
It is already clear that other sampling-based methods do not really have a ï¬ght-
ing chance on such datasets, but we shall compare them in the next section for
55

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
the sake of completeness. See also the supplement for results for other scenarios,
which are very much in line with those above.
MCMC schemes
In order to compare the diï¬€erent sampling-based methods, we deï¬ne the IRIS
(Ineï¬ƒciency Relative to Importance Sampling) criterion, for a given method M
and a given posterior estimate, as follows:
MSEM
MSEIS
Ã— CPUIS
CPUM
where MSEM (resp. MSEIS) is the mean square error of the posterior estimate
obtained from method M (resp. from importance sampling), and CPUM the CPU
time of method M (resp. importance sampling). The comparison is relative to
importance sampling without parallelisation or quasi-Monte Carlo sampling. In
terms of posterior estimates, we consider the expectation and variance of each
posterior marginal p(Î²j|D). We observe that, in both cases, IRIS does not vary
much across the p components, so we simply report the median of these p values.
Fig 2.3 reports the median IRIS across all datasets. We refer the reader to Section
2.4.3 for how we tuned these MCMC algorithms.
The ï¬rst observation is that all these MCMC schemes are signiï¬cantly less eï¬ƒ-
cient than importance sampling on such datasets. The source of ineï¬ƒciency seems
mostly due to the autocorrelations of the simulated chains (for Gibbs or random
walk Metropolis), or, equivalently, the number of leap-frog steps performed at
each iteration in HMC and NUTS. See the supplement for ACFâ€™s (Autocorrela-
tion plots) to support this statement.
Second, HMC and NUTS do not perform signiï¬cantly better than random-
walk Metropolis.
As already discussed, HMC-type algorithms are expected to
outperform random walk algorithms as p â†’+âˆ. But the considered datasets
seem too small to give evidence to this phenomenon, and should not be considered
as reasonable benchmarks for HMC-type algorithms (not to mention again that
these algorithms are signiï¬cantly outperformed by IS on such datasets). We note
in passing that it might be possible to get better performance for HMC by ï¬nely
tuning the quantities Ç« and L on per dataset basis. We have already explained
in the introduction why we think this is bad practice, and we also add at this
stage that the fact HMC requires so much more eï¬€ort to obtain good performance
(relative to other MCMC samplers) is a clear drawback.
Regarding Gibbs sampling, it seems a bit astonishing that an algorithm spe-
cialised to probit regression is not able to perform better than more generic ap-
proach on such simple datasets. Recall that the Gaussian/probit case is particu-
larly favourable to Gibbs, as explained in Section 2.4.3. See the supplement for
56

2.5 Numerical study
â—
â—
1e+01
1e+03
1e+05
Gibbs
HMC
NUTS
RW
IRIS
(a) Median IRIS for the p posterior expec-
tations E[Î²j|D]
â—
â—
â—
â—
â—
10
1000
Gibbs
HMC
NUTS
RW
IRIS
(b) Median IRIS for the p posterior vari-
ances Var[Î²j|D]
Figure 2.3: IRIS (Ineï¬ƒciency relative to importance sampling) across
all datasets for MCMC schemes and Gaussian/probit sce-
nario; left (resp. right) panel shows median IRIS when
estimating the p posterior expectations (resp. the p pos-
terior variances).
57

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Dataset
nD
p
Musk
476
95
Sonar
208
61
DNA
400
180
Table 2.3: Datasets of larger size (from UCI repository): name, num-
ber of instances nD, number of covariates p (including an
intercept)
a comparison of MCMC schemes in other scenarios than Gaussian/probit; results
are roughly similar, except that Gibbs is more signiï¬cantly outperformed by other
methods, as expected.
2.5.2 Bigger datasets
Finally, we turn our attention to the bigger datasets summarised by Table 2.3.
These datasets not only have more covariates (than those of the previous sec-
tion), but also stronger correlations between these covariates (especially Sonar
and Musk). We consider the probit/Gaussian scenario.
Regarding fast approximations, we observe again that EP performs very well,
and better than Laplace; see Figure 2.5. It is only for DNA (180 covariates) that
the EP approximation starts to suï¬€er.
Regarding sampling-based methods, importance sampling may no longer be used
as a reference, as the eï¬€ective sample size collapses to a very small value for these
datasets. We replace it by the tempering SMC algorithm described in Section
2.4.4. Moreover, we did not manage to calibrate HMC so as to obtain reason-
able performance in this setting. Thus, among sampling-based algorithms, the
four remaining contenders are: Gibbs sampling, NUTS, RWHM (random walk
Hastings-Metropolis), and tempering SMC. Recall that the last two are calibrated
with the approximation provided by EP.
Figure 2.5 reports the â€œeï¬€ective sample sizeâ€ of the output of these algorithms
when run for the same ï¬xed CPU time (corresponding to 5 Ã— 105 iterations of
RWHM), for the p posterior expectations (left panels), and the p posterior vari-
ances (right panels); here â€œeï¬€ective sample sizeâ€ is simply the posterior variance
divided by the MSE of the estimate (across 50 independent runs of the same al-
gorithm).
No algorithm seems to vastly outperform the others consistently across the three
datasets.
If anything, RWMH seems to show consistently best or second best
performance.
Still, these results oï¬€er the following insights. Again, we see that Gibbs sam-
58

2.5 Numerical study
â—
â—â—â—â—
â—
â—â—â—â—
â—
â—
â—â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—â—
â—
â—
0.85
0.90
0.95
1.00
EP
Laplace
value
(a) Musk
â—
â—â—â—â—
â—
0.85
0.90
0.95
1.00
EP
Laplace
value
(b) Sonar
â—
â—
â—
â—
â—
0.8
0.9
1.0
EP
Laplace
value
(c) DNA
Figure 2.4: Marginal accuracies across the p dimensions of EP and
Laplace, for datasets Musk, Sonar and DNA
59

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
â—
â—
â—â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
0
2000
4000
6000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
0
10000
20000
30000
40000
50000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
(a) Musk
â—
â—â—
â—
â—â—
â—
â—
â—
â—
â—â—
â—â—
â—â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
0
1000
2000
3000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
â—
â—â—â—
â—
â—â—â—
â—
â—
â—
â—â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
0
1000
2000
3000
4000
5000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
(b) Sonar
â—
â—
â—
â—
â—â—
â—â—
â—
â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—
0
500
1000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
â—
â—
â—
â—
â—â—
â—â—â—
â—
â—
â—
â—
â—
â—â—â—
â—
â—â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—â—
â—â—
â—
â—
â—
â—â—
â—
â—
â—
â—
0
500
1000
1500
2000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
(c) DNA
Figure 2.5: Eï¬€ective sample size for a ï¬xed CPU time for sampling-
based algorithms: posterior expectations (left), and pos-
terior variances (right) for datasets (from top to bottom):
Musk, Sonar, and ADN
60

2.6 Variable selection
pling, despite being a specialised algorithm, does not outperform signiï¬cantly more
generic algorithms. Recall that the probit/Gaussian scenario is very favourable to
Gibbs sampling; in other scenarios (results not shown), Gibbs is strongly domi-
nated by other algorithms.
More surprisingly, RWHM still performs well despite the high dimension. In
addition, RHHM seems more robust than SMC to an imperfect calibration; see
the DNA example, where the error of the EP approximation is greater.
On the other hand, SMC is more amenable to parallelisation, hence on a parallel
architecture, SMC would be likely to outperform the other approaches.
2.6 Variable selection
We discuss in this section the implications of our ï¬ndings on variable selection.
The standard way to formalise variable selection is to introduce as a parameter
the binary vector Î³ âˆˆ{0, 1}p, and to deï¬ne the likelihood
p(D|Î², Î³) =
nD
Y
i=1
F(yiÎ²T
Î³ xÎ³,i)
where Î²Î³ (resp. xÎ³,i) is the vector of length |Î³| that one obtains by excluding
from Î² (resp. xi) the components j such that Î³j = 0. Several priors may be
considered for this problem [Chipman et al., 2001], but for simplicity, we will take
p(Î², Î³) = p(Î²)p(Î³) where p(Î²) is either the Cauchy prior or the Gaussian prior
discussed in Section 2.2.1, and p(Î³) is the uniform distribution with respect to the
set {0, 1}p, p(Î³) = 2âˆ’p.
Computationally, variable selection is more challenging than parameter esti-
mation, because the posterior p(Î², Î³|D) is a mixture of discrete and continuous
components. If p is small, one may simply perform a complete enumeration: for all
the 2p possible values of Î³, approximate p(D|Î³) using e.g. importance sampling. If
p is large, one may adapt the approach of SchÂ¨afer and Chopin [2011], as described
in the next sections.
2.6.1 SMC algorithm of SchÂ¨afer and Chopin [2011]
In linear regression, yi = Î²T
Î³ xÎ³,i + Îµi, Îµi âˆ¼N1(0, Ïƒ2), the marginal likelihood
p(D|Î³) is available in close form (for a certain class of priors). SchÂ¨afer and Chopin
[2011] use this property to construct a tempering SMC sampler, which transi-
tions from the prior p(Î³) to the posterior p(Î³|D), through the tempering sequence
Ï€t(Î³) âˆp(Î³)p(D|Î³)Î´t, with Î´t growing from 0 to 1. This algorithm has the same
structure as Algorithm 9 (with the obvious replacements of the Î²â€™s by Î³â€™s and so
61

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
on.) The only diï¬€erence is the MCMC step used to diversify the particles after re-
sampling. Instead of a random walk step (which would be ill-deï¬ned on a discrete
space), SchÂ¨afer and Chopin [2011] use a Metropolis step based on an independent
proposal, constructed from a sequence of nested logistic regressions: proposal for
ï¬rst component Î³1 is Bernoulli, proposal for second component Î³2, conditional on
Î³1, corresponds to a logistic regression with Î³1 and an intercept as covariates, and
so on. The parameters of these p successive regressions are simply estimated from
the current particle system. SchÂ¨afer and Chopin [2011] show that their algorithm
signiï¬cantly outperform several MCMC samplers on datasets with more than 100
covariates.
2.6.2 Adaptation to binary regression
For binary regression models, p(D|Î³) is intractable, so the approach of SchÂ¨afer and Chopin
[2011] cannot be applied directly. On the other hand, we have seen that (a) both
Laplace and EP may provide a fast approximation of the evidence p(D|Î³); and
(b) both importance sampling and the tempering SMC algorithm may provide an
unbiased estimator of p(D|Î³).
Based on these remarks, SchÂ¨afer [2012] in his PhD thesis considered the following
extension of the SMC algorithm of SchÂ¨afer and Chopin [2011]: in the sequence
Ï€t(Î³) âˆp(Î³)p(D|Î³)Î´t, the intractable quantity p(D|Î³) is simply replaced by an
unbiased estimator (obtained with importance sampling and the Gaussian proposal
corresponding to Laplace). The corresponding algorithm remains valid, thanks to
pseudo-marginal arguments [see e.g. Andrieu and Roberts, 2009]. Speciï¬cally, one
may re-interpret the resulting algorithm as a SMC algorithm for a sequence of
distribution of an extended space, such that marginal in Î³ is exactly the posterior
p(D|Î³) at time t = T. In fact, it may be seen as a particular variant of the SMC2
algorithm of Chopin et al. [2013a].
2.6.3 Numerical illustration
We now compare the proposed SMC approach with the Gibbs sampler of Holmes and Held
[2006] for sampling from p(Î², Î³|D), on the Musk dataset. Both algorithms were
given the same CPU budget (15 minutes), and were run 50 times; see Figure 2.6.
Clearly, the SMC sampler provides more reliable estimates of the inclusion proba-
bilities p(Î³j = 1|D) on such a big dataset. See also the PhD dissertation of SchÂ¨afer
[2012] for results consistent with those, on other datasets, and when comparing to
the adaptive reversible jump sampler of Lamnisos et al. [2013].
62

2.6 Variable selection
0
25
50
75
100
0.00
0.25
0.50
0.75
1.00
covariates
(a) Gibbs
0
25
50
75
100
0.00
0.25
0.50
0.75
1.00
covariates
(b) SMC
Figure 2.6: Variation of estimated inclusion probabilities p(Î³j = 1|D)
over 50 runs for the p covariates of Musk dataset: median
(red line), 80% conï¬dence interval (white box); the black-
box extends until the maximum value.
63

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2.6.4 Spike and slab
We also note in passing that a diï¬€erent approach to the variable selection problem
is to assign a spike and slab prior to Î² [George and McCulloch, 1993a]:
p(Î²) =
p
Y
j=1

Î»N1(Î²j; 0, v2
0) + (1 âˆ’Î»)N1(Î²j; 0, v2
1)
	
,
v2
0 â‰ªv2
1
where Î» âˆˆ(0, 1), v2
0 and v2
1 are ï¬xed hyper-parameters. This prior generates a
continuous posterior (without point masses at Î²j = 0), which is easier to sample
from than the discrete-continuous mixture obtained in the standard formulation
of Bayesian variable selection. It would be interesting to see to which extent our
discussion and ï¬ndings extend to this particular type of posteriors; see for Chapter
4 for how to deal with such priors in EP.
2.7 Conclusion and extensions
2.7.1 Our main messages to users
Our ï¬rst and perhaps most important message to end users is that Bayesian com-
putation (for binary regression) is now suï¬ƒciently fast for routine use: if the right
approach is used, results may be obtained near instantly on a standard computer,
at least on simple datasets.
Concretely, as far as binary regression is concerned, our main recommendation
is to always use EP. It is very fast, and its approximation error is negligible in
most cases (for such models). EP requires some expertise to implement, but the
second author will release shortly a R package that computes the EP approximation
for any logit or probit model. The only drawback of EP is the current lack of
theoretical support. We learnt however while ï¬nishing this manuscript that Simon
BarthelmÂ´e and Guillaume Dehaene (personal communication) established that the
error rate of EP is O(nâˆ’2
D ) in certain models (where nD is the sample size). This
seems to explain why EP often performs so well.
In case one wishes to assess the EP error, by running in a second step some exact
algorithm, we would recommend to use the SMC approach outlined in Section 2.4.4
(i.e.
with initial particles simulated from the EP approximation).
Often, this
SMC sampler will reduce to a single importance sampling step, and will perform
extremely well. Even when it does not, it should provide decent performance,
especially if run on (and implemented for) a parallel architecture. Alternatively, on
a single-core machine, random walk Metropolis is particularly simple to implement,
and performs surprisingly well on high-dimensional data (when properly calibrated
using EP).
64

2.7 Conclusion and extensions
2.7.2 Our main message to Bayesian computation experts
Our main message to Bayesian computation scientists was already in the title of
this chapter: leave Pima Indians alone, and more generally, letâ€™s all refrain from
now on from using datasets and models that are too simple to serve as a reasonable
benchmark.
To elaborate, letâ€™s distinguish between specialised algorithms and generic algo-
rithms.
For algorithms specialised to a given model and a given prior (i.e. Gibbs sam-
plers), the choice of a â€œbenchmarkâ€ reduces to the choice of a dataset. It seems
unfortunate that such algorithms are often showcased on small datasets (20 co-
variates or less), for which simpler, more generic methods perform much better.
As a matter of fact, we saw in our simulations that even for bigger datasets Gibbs
sampling does not seem to oï¬€er better performance than generic methods.
For generic algorithms (Metropolis, HMC, and so on), the choice of a bench-
mark amounts to the choice of a target distribution. A common practice in papers
proposing some novel algorithm for Bayesian computation is to compare that al-
gorithm with a Gibbs sampler on a binary regression posterior for a small dataset.
Again, we see from our numerical study that this benchmark is of of limited in-
terest, and may not be more informative than a Gaussian target of the same
dimension. If one wishes to stick with binary regression, then datasets with more
than 100 covariates should be used, and numerical comparisons should include at
least a properly calibrated random walk Metropolis sampler.
2.7.3 Big data and the p3 frontier
Several recent papers [Bardenet et al., 2015; Scott et al., 2013; Wang and Dunson,
2013] have approached the â€™big dataâ€™ problem in Bayesian computation by fo-
cussing on the big nD (many observations) scenario. In binary regression, and
possibly in similar models, the big p problem (many covariates) seems more criti-
cal, as the complexity of most the algorithms we have discussed is O(nDp3). Indeed,
we do not believe that any of the methods discussed in this chapter is practical for
p â‰«1000. The large p problem may be therefore the current frontier of Bayesian
computation for binary regression.
Perhaps one way to address the large p problem is to make stronger approxima-
tions; for instance by using EP with an approximation family of sparse Gaussians.
Alternatively, one may use a variable selection prior that forbids that the number
of active covariates is larger than a certain threshold.
65

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2.7.4 Generalising to other models
We suspect some of our ï¬ndings may apply more generally to other models (such
as certain generalised linear models), but, of course, further study is required to
assess this statement.
On the other hand, there are two aspects of our study which we recommend to
consider more generally when studying other models: parallelisation, and taking
into account the availability of fast approximations. The former has already been
discussed. Regarding the latter, binary regression models are certainly not the only
models such that some fast approximations may be obtained, whether through
Laplace, INLA, Variational Bayes, or EP. And using this approximation to cal-
ibrate sampling-based algorithms (Hastings-Metropolis, HMC, SMC, and so on)
will often have a dramatic impact on the relative performance of these algorithms.
Alternatively, one may also discover in certain cases that these approximations are
suï¬ƒciently accurate to be used directly.
66

3
Computation of Gaussian orthant probabilities in high
dimension
Status: To appear in Statistics and Computing.
3.1 Introduction
There are many applications where computing an orthant probability in high di-
mension with respect to a Gaussian or Student distribution is an issue of interest.
For instance it is common in statistics to compute the likelihood of models, where
we observe only an event with respect to multivariate Gaussian random variables.
In Econometrics, the multivariate probit model [Train, 2009], where we observe a
decision among J alternative choices each of them corresponding to a Gaussian
utility, is commonly studied. It can be written as an orthant problem. Other
such models are the spatial probit [LeSage et al., 2011] and Thurstonian models
[Yao and Bockenholt, 1999]. Other applications than direct modelization can be
found, such as multiple comparison tests [Hochberg and Tamhane, 1987], where
the integration is done with respect to a Student (see Bretz et al. [2001] for an
example). Orthant probabilities are also of interest in other ï¬elds than statistics,
i.e. stochastic programming [Prekopa, 1970], structural system reliability [Pandey,
1998], engineering, ï¬nance, etc.
The problem at hand is the computation of the integral,
Ë†
[a,b]
(2Ï€)âˆ’d
2 |Î£|âˆ’1
2 exp

âˆ’1
2(y âˆ’m)tÎ£âˆ’1(y âˆ’m)

dy.
(3.1)
where a, b âˆˆRd. The Student case will be written as a mixture of the above
integral with an inverse Chi-square (see Section 3.5.1).
67

3 Computation of Gaussian orthant probabilities in high dimension
Many algorithms have been proposed to compute (3.1); for a review see Genz and Bretz
[2009].
They can be divided into two groups.
The ï¬rst are numerical algo-
rithms to deal with small dimensional integrals. In dimension 3 there exist algo-
rithms [Genz and Bretz, 2009] where after sphericization, such that the Gaussian
has an identity covariance matrix, one applies recursively numerical computa-
tions of the error function. For higher dimensions than three Minwa et al. [2003]
propose to express orthant probabilities as diï¬€erences of orthoscheme probabili-
ties, where an orthoscheme is (3.1) with correlation matrix â„¦= (Ï‰ij) satisfying
Ï‰ij = 0
âˆ€i, j
|i âˆ’j| > 1. This can be easily computed by recursion. How-
ever the decomposition in orthoscheme probabilities has factorial complexity. The
second group of algorithms is Monte Carlo based and may be used for dimen-
sions higher than 10. In particular GHK due to Geweke [1991], Keane [1993] and
Hajivassiliou et al. [1996] and conjointly to Genz [1992], has been widely adopted
for the applications described above.
In this chapter we show that in the case of Markovian covariances (i.e. covari-
ances that can be written as those of Markovian processes), the GHK algorithm
estimates the normalizing constant of a state space model (SSM), using sequen-
tial importance sampling (SIS) with optimal proposal. We show in addition for a
ï¬rst order autoregressive process (henceforth AR(1)) that the normalized variance
diverges exponentially fast.
To avoid this behavior we propose to use a particle ï¬lter. We extend this method-
ology to the non Markovian case by using Sequential Monte Carlo (SMC). SMC
allows additional gain in eï¬ƒciency by considering diï¬€erent MCMC moves and pro-
posals. In addition the algorithm is adaptive and simpliï¬es automatically to the
GHK if the integral is simple enough. In our numerical experiments we ï¬nd a
substantial improvement.
We start by reviewing the existing GHK algorithm (Section 3.2), we then discuss
the algorithmâ€™s behavior for Markovian covariance matrices and propose an exten-
sion to higher dimensions (Section 3.3). In Section 3.4 we extend this proposal to
arbitrary covariance matrices. We propose some extensions for the simulation of
truncated distributions and for other distributions (3.5). Finally we present some
numerical results and conclude (Sections 3.6 and 3.7).
Notations
For any vector x âˆˆRp for i â‰¤p we write x<i âˆˆRi for the vector of
the i âˆ’1 ï¬rst components, and we take a : b = {a, Â· Â· Â· , b}. We let x<1 = âˆ…, and
also write xi:j for the vector (xi, xi+1, Â· Â· Â· , xj); Î¦, Ï• are respectively the N(0, 1)
Gaussian cdf and pdf, we write Ï•(x|A) for the pdf,
Ï•(x)
Î¦(A)âœ¶A(x), of a Gaussian
truncated to the set A âŠ‚R evaluated in x. We will also abuse notation and use
Î¦(A) to denote the probability of a set when A âŠ‚R. For instance Î¦([a, b]) =
Î¦(b) âˆ’Î¦(a).
68

3.2 Geweke-Hajivassiliou-Keane (GHK) simulator
3.2 Geweke-Hajivassiliou-Keane (GHK) simulator
From now on to simplify notations, and without loss of generality, we limit our-
selves to the study of the following multidimensional integral:
F(a, b, Î£) =
Ë†
[a,b]
(2Ï€)âˆ’d
2|Î£|âˆ’1
2 exp

âˆ’1
2ytÎ£âˆ’1y

dy
(3.2)
with a, b âˆˆRd. Note that the extension to integrals where some components of
the vectors a, b are respectively âˆ’âˆand âˆis direct.
Let Î“ be the Cholesky decomposition of Î£, i.e. Î£ = Î“Î“t with Î“ = (Î³ij), Î³ii > 0
and Î³ij = 0 if j > i. We can write the previous equation after the change of
variable Î· = Î“âˆ’1y for which dÎ· = |Î“|âˆ’1dy:
F(a, b, Î£) =
Ë†
bâ‰¥Î“Î·â‰¥a
(2Ï€)âˆ’d
2 exp

âˆ’1
2Î·tÎ·

dÎ·,
the i-th truncation being such that
1
Î³ii

ai âˆ’Piâˆ’1
j=1 Î³ijÎ·j

â‰¤Î·i â‰¤
1
Î³ii

bi âˆ’Piâˆ’1
j=1 Î³ijÎ·j

,
from the positivity of the (Î³ii). Thus we can write:
F(a, b, Î£) =
Ë†
d
Y
i=1
Ï•(Î·i)1{Bi(Î·<i)}(Î·i)dÎ·1:d =
Ë†
d
Y
i=1
Î¦ (Bi(Î·<i)) Ï•(Î·i|Bi (Î·<i)) dÎ·1:d,
where the set Bi(Î·<i) = {Î·i :
1
Î³ii

ai âˆ’Piâˆ’1
j=1 Î³ijÎ·j

â‰¤Î·i â‰¤
1
Î³ii

ai âˆ’Piâˆ’1
j=1 Î³ijÎ·j

}
is an interval.
The GHK algorithm is an importance sampling algorithm based on this struc-
ture.
It proposes particles distributed under Qd
i=1 Ï•(Î·i|Bi (Î·<i)) and evaluates
the average of the weights wn = Qd
i=1 Î¦ (Bi(Î·n
<i)). The algorithm is described in
pseudo-code in Alg. 10.
Algorithm 10 GHK simulator
for m âˆˆ1 : M do
Sample: Î·m
1:d âˆ¼Qd
i=1 Ï•(Î·i|Bi(Î·<i))
Weightsâ‹†: wm = Qd
i=1 Î¦ (Bi(Î·m
<i))
end for
return
1
M
PM
i=1 wi
â‹†Recall that Î¦(Bi(Î·m
<i) can be computed as a diï¬€erence of two one dimensional cdf for the trun-
cation deï¬ned above.
Algorithm 10 outputs an unbiased estimator of integral (3.1).
69

3 Computation of Gaussian orthant probabilities in high dimension
To generate truncated Gaussian variables the usual approach in the GHK sim-
ulator is to use the inverse cdf method. We follow this approach in the rest of
the chapter except where stated otherwise. When the numerical stability of the
inverse cdf is an issue we will use the algorithm proposed in Chopin [2011a].
In the next section we will study with more care the case where the covariance
matrix of the underlying Gaussian vector has a Markovian structure.
3.3 The Markovian case
When the covariance matrix is Markovian, that is a matrix for which the inverse is
tri-diagonal, the simulation step of Alg. 10 is the simulation of a Markov process
(x1:t). At time t the weights depend on xtâˆ’1 only. Let us take a lag 1 autoregressive
process (AR(1)) for the purpose of exposition, and study the probability of it being
in some hyperrectangle [a, b] = [a1, b1] Ã— Â· Â· Â· Ã— [aT, bT]. The integral of interest is
therefore:
Ë†
TY
t=1
1{[at,bt]}(xt)Ï•(xt; Ìºtxtâˆ’1, Ïƒ2
t )dx1:T.
(3.3)
The GHK algorithm consists in sampling from the Markov process:
xt|xtâˆ’1 âˆ¼Ï•
 Ìºtxtâˆ’1, Ïƒ2
t |Bt(xt)

.
The matrix Î£âˆ’1 is tridiagonal, the weights at time t are therefore Î¦( btâˆ’Ìºtxtâˆ’1
Ïƒt
) âˆ’
Î¦( atâˆ’Ìºtxtâˆ’1
Ïƒt
).
Eq.
3.3 can be seen as the likelihood of the state space model
[CappÂ´e et al., 2005]:
xt|xtâˆ’1 âˆ¼Ï•(xt; Ìºtxtâˆ’1, Ïƒ2
t )
yt|xt âˆ¼1{[at,bt]}(xt)
where (yt)t is observed. The GHK can be interpreted as a sequential importance
sampler (SIS) using proposal Ï•(Ìºtxtâˆ’1, Ïƒ2
t |Bt(xt)).
3.3.1 Toy example
Let us specify a bit more the problem to simplify notation and show some prop-
erties of a thus deï¬ned algorithm.
Consider the problem of ï¬nding the probability that an AR(1),
Xt = ÌºXtâˆ’1 + Îµt,
|Ìº| < 1
is inside the hyper-cube [0, b] Ã— Â· Â· Â· Ã— [0, b], for some b > 0. We have set Ïƒ = 1,
a = 0 and Ìº a constant.
70

3.3 The Markovian case
The GHK algorithm consists in this case in simulating the above Markov chain
constrained to [0, b] and in computing under this distribution the products of the
weights QT
t=1 [Î¦(b âˆ’ÌºXt) âˆ’Î¦(âˆ’ÌºXt)]. The simulations are therefore generated by
the Markov probability kernel
P b(x, dy) =
Ï•(y; Ìºx, 1)
Î¦(b âˆ’Ìºx) âˆ’Î¦(âˆ’Ìºx)âœ¶[0,b](y)dy,
|Ìº| < 1.
(3.4)
For this model we have the following proposition:
Proposition 3.3.1 For the Markov model deï¬ned by (3.4), the normalized square
product of weights of the normalizing constant has the following behavior:
lim inf
Tâ†’âˆ
(
E
" 
QT
t=1
 Î¦ (b âˆ’ÌºXt) âˆ’Î¦ (âˆ’ÌºXt)
2
exp{2TEÏ€ log (Î¦(b âˆ’ÌºX) âˆ’Î¦(âˆ’ÌºX))}
!#)
1
âˆš
T
> exp {VÏ€ [Ïˆ(X)] + Ï„} ,
(3.5)
where subscript Ï€ denotes integration with respect to the invariant distribution of
P b(x, dy), the other expectation is taken relatively to the Markov chain (Xt), and
Ïˆ : x 7â†’log (Î¦(b âˆ’Ìºx) âˆ’Î¦(âˆ’Ìºx)), Ï„ = 2 Pâˆ
k=1 cov(X0, Xk).
Proof:
A detailed proof is given in appendix 3.A.
â–¡
Under V -Uniform ergodicity, that follows from our proof, the denominator is the
square of the limit of the product of weights and can be interpreted as a scaling
factor.
Thus the result above shows that this renormalized squared estimator
diverges exponentially fast as the dimension of the integral increases.
Remark 3.3.1 In the course of the proof we showed that the normalizing constant
has a log-normal limiting distribution, resulting in a skewed distribution. We expect
that the distribution of the estimator will have its mode away from the expected
value resulting in some apparent bias. In fact one can show that the normalized
third order moment will also grow exponentially.
GHK has quadratic complexity however we can show that for at least one co-
variance structure the variance diverges exponentially fast. This fully justiï¬es the
use of an algorithm of higher computational complexity. In the following section
we propose a natural extension to deal with this issue in the Markovian case.
3.3.2 Particle ï¬lter (PF)
PF is a common extension of SIS that corrects the weight degeneracy problem. The
solution brought by particle ï¬ltering [Gordon et al., 1993] is to use a resampling
71

3 Computation of Gaussian orthant probabilities in high dimension
step, i.e.
to kill those particles with low weights and to replicate those with
high contribution. At time t one resamples the particles by sampling from the
distribution PM
m=1 W m
t Î´xm
t (dx) where W m
t stands for the m-th renormalized weight
at time t, and Î´x(dxâ€²) the Dirac measure in x. All the weights are then set to one.
We use an adaptive version of this algorithm where the resampling step is trig-
gered only when the ESS of the weight is lower than some threshold, where the
ESS is deï¬ned as
PN
i=1 wi
2
P
i w2
i
âˆˆ[1, M] ,
and indicates the number of draws from the independent distribution to obtain
the same variance. Note that it is closely related to the inverse of equation (3.5),
hence we expect that without resampling it goes to zero with exponential speed.
We deï¬ne the state space model:
xt|xtâˆ’1 âˆ¼gt(xt|xtâˆ’1),
yt|xt âˆ¼ft(yt|xt)
One can use a PF to compute the likelihood of such model,
L(y1:T) =
Ë†
TY
t=1
gt(xt|xtâˆ’1)ft(yt|xt)g0(x0)dx0:T
A PF with proposal distribution qt(xt, xtâˆ’1) is described in Alg. 23. Our appli-
cation corresponds to the special case where:
gt(xt|xtâˆ’1) = Ï•(xt),
ft(yt|xt, xtâˆ’1) = 1{Bt(x<t)}(xt),
qt(xt|xtâˆ’1) = Ï•(xt|Bt(x<t)),
where the set Bt(x<t) depends on xtâˆ’1 only.
The proposal thus deï¬ned corresponds to the optimal one [Doucet et al., 2000],
that is the distribution proportional to ft(yt|xt)gt(xt|xtâˆ’1) in our case proportional
to Ï•(xt)1{Bt(x<t)}(xt) hence the truncated Gaussian. The weights are given by the
normalizing constant
Â´
ft(yt|xt)gt(xt|xtâˆ’1)dxt, in our case Î¦(Bt(x<t)).
To resample we propose to use systematic resampling [Carpenter et al., 1999]
(for other approaches see Douc et al. [2005]). Systematic resampling is described
in Algorithm 13 (Appendix 3.B).
The particle ï¬lter thus deï¬ned outputs an unbiased estimator of the likelihood
Del Moral [1996a], and thus the orthant probability in our case.
Note that the output of Algorithm 23 is of the form of a product of terms smaller
than one, in our case those terms can be very small and lead to numerical issues.
One way of dealing with this issue is to rewrite all the algorithm in log scale.
72

3.3 The Markovian case
Algorithm 11 Particle Filter
Input: M the number of particles
Sample: Sample xi
0 âˆ¼g0(.)
for t = 1 : T âˆ’1 do
if ESS < Î·â‹†then
Z â†Z Ã— { 1
M
PM
i=1 wi
t}
Resample aj
t âˆ¼P
i
wi
t
P
j wj
t Î´i using algorithm 13, set wj
t â†1
else
a1:M
t
= 1 : M
end if
Sample xi
t+1 âˆ¼qt+1(.|x
ai
t
t )
Set wi
t+1 â†wi
t
ft+1(yt+1|xi
t+1)gt+1(xi
t+1|x
ai
t
t )
qt+1(xi
t+1|x
ai
t
t )
end for
return Z Ã— 1
M
PM
i=1 wi
T
Remark 3.3.2 As we are here in the special case of being able to sample from
the optimal distribution (as shown in Section 3.3) one could resort to the auxiliary
particle ï¬lter (APF, Pitt and Shephard [1999]). In fact in this special case the
algorithm amounts to exchanging the resampling step and the move step of the
particle ï¬lter. We tested this approach on some Markov processes and observed no
improvements in term of variance on repeated draws.
Example 3.3.1 We can show that the previous process (Section 3.3.1) beneï¬ts
from resampling when the ESS goes beneath a given level.
Figure (3.1) shows that the GHK algorithmâ€™s variance increases more quickly
as compared to the PF (that seem to have some stable variance on the considered
dimension). In addition the distribution of the GHK estimator seem to be skewed
towards smaller values as T increases. This results in some bias on the last box-
plot. As described in remark 3.3.1 this behavior is due to the log-Normal limiting
distribution of the output of the algorithm. The skewness coeï¬ƒcient increases ex-
ponentially with T.
73

3 Computation of Gaussian orthant probabilities in high dimension
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
âˆ’50
âˆ’40
âˆ’30
100
100
120
120
140
140
160
160
180
180
200
200
dimension
value
(a) variance
0
250
500
750
1000
0
50
100
150
200
dimension
Ess
(b) ESS
Figure 3.1: Estimates of Orthant probabilities by Particle Filter
Estimation of the log probability that an AR(1) process (deï¬ned previously) with Ìº = 0.7 has all
its component in [0, 15]. GHK sampler (grey) and PF (white) on various dimension from 100 to
200. On the right panel the two ESS for dimension 200. On both cases M is set to 1000.
Thurstonian Model
Thurstonian models arise in Psychology and Economics [Yao and Bockenholt, 1999]
to describe the ranking of p alternatives by n individuals (referred to as judges).
Suppose that we observe the rank ri = (k1i, Â· Â· Â· , kp,i) of some p independent
Gaussian random variables,
xi,j = Î²j + ÏƒÎµi,j,
where Îµi,ji.i.d
âˆ¼N(0, 1). The likelihood of one observation is an orthant probability:
PÎ¸{Xp > Â· Â· Â· > X1} =
Ë†
p
Y
i=1
1{xi>xiâˆ’1}Ï•(xi|Î²j, Ïƒ2)dx1:p
(3.6)
with the convention that X0 = âˆ’âˆ.
This model is similar to the previous one but with Ï = 1.
74

3.4 Non Markovian case
â—
â—
â—
â—
â—
âˆ’80
âˆ’60
âˆ’40
âˆ’20
1
1
2
2
3
3
4
4
5
5
number of observations
value
(a) Estimates
0
250
500
750
1000
0
10
20
30
40
dimension
y1
(b) ESS
Figure 3.2: Estimates of the likelihood of a Thurstonian model by
Particle Filter and by GHK
Estimation of the likelihood of a Thurstoninan model with p = 10 and the number of observations
is ranging from 1 to 5 the PF (white) and the GHK (grey). The threshold ESS is set to 0.5M and
the number of particles is set to M = 1000. The right panel shows the ESS of both algorithms for
T = 1 and p = 40.
We ï¬nd that the likelihood is estimated with smaller variance.
In addition,
because of the heavy tail distribution of the GHK simulatorâ€™s output we observe
a bias (see Figure 3.2). Again one can explain the strong observed bias by remark
3.3.1 and the fact that we do not replicate enough the experiment to observe the
tail of the distribution. In addition as suggested above the ESS of the GHK seems
to decrease exponentially fast to zero.
From this observation we could apply this algorithm to perform inference by
using Particle MCMC [Andrieu et al., 2010a], where this estimation of the likeli-
hood can be plugged in a Random walk Metropolis Hastings and still target the
appropriate distribution.
3.4 Non Markovian case
For more general covariances we propose to use Sequential Monte Carlo (SMC)
[Del Moral et al., 2006b]. As previously we will base the algorithm on the proposal
75

3 Computation of Gaussian orthant probabilities in high dimension
of GHK, increasing the dimension of the problem at each time step. However we
now have an additional degree of freedom: the order in which we incorporate the
variables. In the following section we study an approach to ordering the variables.
3.4.1 Variable ordering
We follow Gibson et al. [1994] in ordering the variables from the most diï¬ƒcult to
the simplest, where diï¬ƒcult constraints are considered to be the one that impact
the most the probability.
However we cannot evaluate exactly the probabilities as it is our ï¬nal goal.
Instead Gibson et al. [1994] propose to replace the simulations by the expected
value of the truncated Gaussian.
The algorithm starts by choosing the ï¬rst index i1, and deï¬ning Î·1 as follows:
i1 = arg min
1â‰¤kâ‰¤T Î¦
 ak
Î³kk
, bk
Î³kk

,
Î·1 =
1
Î¦
h
ai1
Î³i1i1 ,
bi1
Î³i1i1
i
Ë†

ai1
Î³i1i1
,
bi1
Î³i1i1
 Î·Ï•(Î·)dÎ·
i.e. the smallest possible probability that the Gaussian will be in [ak, bk]. This
enables an approximation of the next probability as a function of i2.
i2 = arg min
2â‰¤kâ‰¤T Î¦
 1
ËœÎ³kk
(ak âˆ’ËœÎ³1,kÎ·1), 1
ËœÎ³kk
(bk âˆ’ËœÎ³1,kÎ·1)

.
where (ËœÎ³ij) = ËœÎ“ is the Cholesky decomposition of the matrix after substituting the
ï¬rst and the i1th variable.
We end up with the desired vector (i1, Â· Â· Â· , iT) that gives us the order in which
to choose the covariances and truncation points. The algorithm is summed up by
Alg. 14 in appendix 3.C. The algorithm has quadratic time complexity, however
its cost is negligible as compared to the subsequent Monte Carlo algorithm.
We show the use of the reordering in moderate dimensions (50 and 60) on the
GHK simulator. This is already a great improvement especially as the dimension
increases.
Figure (3.3), shows boxplots of 50 repetitions of the GHK for both
ordered (white) and non-ordered (grey) inputs.
76

3.4 Non Markovian case
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
âˆ’5.030805eâˆ’17
2.362210eâˆ’16
5.227500eâˆ’16
8.092791eâˆ’16
1.095808eâˆ’15
value
(a) Dimension 50
â—â—
â—
â—
â—â—â—â—
â—
â—â—â—â—
â—â—â—â—
â—
â—
â—â—â—
â—
â—
â—
â—
âˆ’2.664387eâˆ’20
1.198975eâˆ’19
2.664388eâˆ’19
4.129801eâˆ’19
5.595215eâˆ’19
value
(b) Dimension 60
Figure 3.3: Estimates of Orthant probabilities with (white) and with-
out (grey) variable ordering
Covariance matrices generated from random samples with heavy tails (see Section 3.6). In both
case we use a GHK simulator with variable ordering (white), without (grey). The various dimension
are simulated with the same algorithm and same seed, such that the small ones are subsets of the
others. When the variables are not ordered we observe some outliers, and this phenomenon is
reduced with Gibson et al. [1994]â€™s algorithm.
From dimension 50 and upwards we start observing some skewed distributions
for the GHK estimator as noted in remark 3.3.1. The phenomenon seems to be
reduced by ordering.
This eï¬€ect is relatively dangerous as some draws depart a lot from the mean
value.
The ordering will be used on all examples from now on to reduce the
variance. In Section 3.6 we have empirical evidence that using an appropriate
move step deals with this tail eï¬€ect, in our examples.
We have shown that in the particular case of Markov processes we can strikingly
beneï¬t from the use of resampling. In the next section we attempt to generalize
our ï¬nding to a broader range of problems.
3.4.2 A sequential Monte Carlo (SMC) algorithm
The algorithm discussed in Section 3.3 can be generalized to non Markovian Gaus-
sian vectors by applying the SMC methodology. Deï¬ne the following sequence of
distribution:
Ï€t(Î·1:t) = Î³n(Î·1:t)
Zt
,
Î³t(Î·1:t) =
tY
i=1
Ï•(Î·i)1{Bi(Î·<i)},
(3.7)
77

3 Computation of Gaussian orthant probabilities in high dimension
indexed by t, where the unnormalized quantity Î³t(Î·1:t) is our target integrand. We
thus want to compute an estimator of Zt for a given t,
Zt =
Ë†
tY
i=1
Ï•(Î·i)1{Bi(Î·<i)}dÎ·1:t.
SMC samplers are a class of algorithms that generalize particle ï¬lters to non
dynamic problems (Neal [2001a],Chopin [2002a], Del Moral et al. [2006b]). Their
aim is to sample from a sequence of measures (Ï€t)t where Ï€0 is easy to sample from
and Ï€T is our target. The algorithm works by moving from one target to the other
by importance sampling, and avoid degeneracy of the weights by resampling if the
ESS falls bellow a threshold. In the case of the GHK the sequence of distribution
consist of adding a dimension at each step.
To ensure particle diversity after
resampling the particles are moved according to a MCMC kernel targeting the
current distribution. This is the most computationally expensive step. Diï¬€erent
alternatives are described in the next section.
The main steps are described in Algorithm (12) bellow:
Algorithm 12 SMC for orthant probabilities
Input Î“,a, M, Î±â‹†, Set Z â†1
Each computation involving m is done âˆ€m âˆˆ1 : M
Init Î·m
1 âˆ¼Ï•(.)1Bk(Î·<1), and wm
1 = 1
for t âˆˆ1 : T âˆ’1 do
At
time
t
the
weighted
system
is
distributed
as
(wm
t , Î·m
1:t)
âˆ¼
Qt
k=1 Ï•(Î·k)1Bk(Î·<k) âˆÏ€t(Î·1:t).
if ESS(w1:M
t
) < Î±â‹†then
Z â†Z Ã— { 1
M
PM
i=1 wi
t}.
Resample: Î·â€²m
t
âˆ¼PM
j=1 wj
tÎ´Î·j
t , wm
t â†1.
Move: Î·m
t âˆ¼Kt(Î·â€²m
t , dÎ·m
t ) where Kt leaves Ï€t(Î·1:t) invariant.
end if
Î·m
t+1 âˆ¼Ï•(.|Bt+1(Î·m
<t+1)), wm
t+1 â†wm
t Ã— Î¦(Bt+1(Î·<t+1)).
end for
return Z Ã— { 1
M
PM
i=1 wi
T}
An interesting feature of Algorithm 12 is that if the integral is simple enough the
ESS will never fall under the threshold and the above algorithm breaks down to a
GHK simulator. This allows the algorithm to adapt to simple cases at a minimal
eï¬€ort, that of computing the ESS.
Note also that the estimator is still unbiased (Del Moral [1996a]) and can there-
fore be used in more complex schemes such as PMCMC (Andrieu et al. [2010a])
or SMC2 (Chopin et al. [2013b]).
78

3.4 Non Markovian case
3.4.3 Move steps
The moves step will have an important impact on the non-degeneracy of the par-
ticle system. We want to construct a Markov chain that moves the particles as
far away from their initial position as possible. In addition this step will be the
bulk of the added time complexity compared to GHK, so we want to make it as
eï¬ƒcient as possible.
Gibbs sampler
The structure of our target (3.7), where the dependence of the Gaussian compo-
nents lies within the truncation, does not allows a direct application of the Gibbs
sampler of Robert [1995], without a change of variable. In this section, to simplify
notations we consider the special case of b = âˆ. We write the conditional distri-
bution at time t as proportional to Qt
i=1 Ï•(Î·i)âœ¶(Î“<t,<tÎ·<t)>b<t, where Î“<t,<t is the
matrix built with the ï¬rst t âˆ’1 lines and columns of Î“. The conditional is given
by:
Î·i|Î·âˆ’i âˆ¼Ï•
 
.

t\
jâ‰¥i
(
sign(Î³ji)Î·i â‰¥
1
|Î³ji|
 
aj âˆ’
X
kÌ¸=i
Î·kÎ³jk
!)!
.
We therefore have to compute those sets for each component up to t and simulate
according to a truncated Gaussian. Computing the set can lead to one or two
sided truncations depending on the sign of the Î³ij.
The main drawback about having to compute this step each time we resample
is its complexity. This operation has time complexity O(d3) per time step. This
is easily seen as the set in the above equation is just the result of some matrix
inversion for a lower triangular system of dimension d. This leads to an SMC
algorithm that seem to have a prohibitive complexity of O(d4), where the GHK
simulator had an O(d2) complexity. However we have shown that GHKâ€™s variance
diverges exponentially quickly on some examples suggesting that this complexity
might be acceptable. In fact examples in high dimension show that even at con-
stant computational cost the algorithm is able to out-perform GHK (see Section
3.6).
Hamiltonian Monte Carlo
An alternative to Gibbs sampler is to use Hamiltoninan Monte Carlo (HMC) (see
[Neal, 2010a] for a survey), and the idea of Pakman and Paninski [2012] for trun-
cated Gaussians.
HMC is based on interpreting the variables of interest as the position of a particle
with potential the opposite of the log target and by simulating the momentum as a
Gaussian with given mass matrix. The proposal of the Metropolis-Hastings is then
79

3 Computation of Gaussian orthant probabilities in high dimension
constructed by applying the equations of motion up to a time horizon THMC to the
problem. This leads to an eï¬ƒcient algorithm that makes use of the gradient of the
target to explore its support. We refer the reader to Neal [2010a] for more details
on the algorithm and describe the approach proposed by Pakman and Paninski
[2012] to adapt the algorithm to truncated Gaussians.
Based on the fact that the log density of a Gaussian random variable is a
quadratic form, the movement equation can be dealt with explicitly. The scheme
is written as an exact HMC (i.e. not resulting in numerical integration). Remains
then to deal with the truncation. Pakman and Paninski [2012] show that they can
be treated as â€œwallsâ€ for the given particle, a reï¬‚ection principle can be applied
for any particle hitting the constraint during the algorithm. In particular we must
ï¬nd the time at which occurs the ï¬rst â€œhitâ€. In our experiment the time horizon
THMC is set to a uniform draw on [0, Ï€] as suggested in Neal [2010a]. The average
value Ï€/2 is advocated by Pakman and Paninski [2012].
The computation of the ï¬rst hitting time dominates the cost of the algorithm.
This is particularly true when the truncation are small as the number of hitting
times will be high. Figure 3.D.1 in appendix 3.D shows a comparison of the SMC
algorithm with the Gibbs sampler (grey) and exact HMC (white). Although this
Markov chain algorithm seems to perform very well for a wide range of problems
and has a neat formalism, we ï¬nd that it does not outperform Gibbs sampling
when used as a move. The speciï¬city of the move step in SMC is that the particles
are already distributed according to (3.7), therefore the move need not propagate
each particle across all the support. In particular the strength of HMC in quickly
exploring the target might be less useful in this context.
Overrelaxation
Overrelaxation for Gaussian random variables was proposed by Adler [1981] as a
way of improving Gibbs sampling for a distribution with Gaussian conditionals.
For each component the proposal is Î·â€²
i|Î·âˆ’i âˆ¼N(Âµi + Î±(Î·i âˆ’Âµi), Ïƒ2
i (1 âˆ’Î±2)) for
0 â‰¤Î± â‰¤1, and with Âµi and Ïƒ2
i the expectation and variance of Î·i|Î·âˆ’i. The case
Î± = 0 is the classical Gibbs sampler, the case Î± = 1 is a special case of random
walk Metropolis-Hasting proposal. One can check that if Î·i âˆ¼N(Âµi, Ïƒ2
i ) then Î·â€²
i
has the correct distribution.
Given a particle Î·, we propose a new one according to:
Î·â€²|Î· âˆ¼N(Î±Î·, (1 âˆ’Î±2)I)
Setting aside the constraint for a moment the invariant distribution of such kernel
is an independent (0,1)-Gaussian. If we add an acceptation step such that we
accept if it satisï¬es the constraint at time t, the Markov kernel leaves the current
distribution invariant (3.7).
80

3.4 Non Markovian case
We ï¬nd that the fact that overrelaxation is close to a Metropolis adjusted
Langevin algorithm (MALA) helps to calibrate the algorithm, log Ï€(Î·) = âˆ’1
2Î·TÎ·,
hence the proposal in MALA is N((1 âˆ’Îµ
2)Î·, Îµ2).
From Roberts and Rosenthal
[1998] we have that Îµ should be O(dâˆ’1
3). To calibrate the algorithm we propose to
match the two drifts. We ï¬nd that Î± = O(1 âˆ’0.5dâˆ’1
3), the constant should then
be close from a problem to an other because locally we are always in the case of
independent Gaussians (locally the constraints have less impact). We ï¬nd that in
our case taking Î± = 0.004Ã—(1âˆ’dâˆ’1/3) gives the expected behavior and acceptance
ratio.
Repeating the move step
Dubarry and Douc [2011] have shown, for particle ï¬lters, that applying some
Metropolis Hastings kernel targeting the ï¬ltering distribution on the particles leads
to a close to optimal variance (the variance is the same as one coming from an iid
sample). This convergence results happens after O(log M) iteration of the Markov
kernel. These results suggest repeating the move step after each resampling step
until some criterion of convergence is satisï¬ed.
We compute the sum of absolute distances that the particles have moved after
each step (a similar metric was used in SchÂ¨afer and Chopin [2011] for the discrete
case). We repeat the move until this scalar value stabilizes. The stabilization
of the total metric should be associated with the cancellation of the dependence
between the particles (leading to a close to independent system).
Block sampling
To diversify the particle system after each resampling we have relied until now
on invariant kernels targeting the current distribution Ï€t. An alternative to this
approach is given by Doucet et al. [2006], where importance sampling is done on
the space of Î·tâˆ’L+1:t+1 with a given number L of previous time steps. This limits
the behavior of the particles all stemming from one path after a few iterations.
We brieï¬‚y describe the idea in the following.
Suppose at time tâˆ’1 we have a weighted set of particles such that (wtâˆ’1, Î·1:tâˆ’1) âˆ¼
Ï€tâˆ’1(Î·1:tâˆ’1); instead of proposing a particle Î·â€²
t, propose a block of size L, Î·â€²
tâˆ’L+1:t âˆ¼
q(.|Î·1:tâˆ’1), and discard the particles Î·tâˆ’L+1:tâˆ’1. The distribution of the resulting
system is intractable because of the marginalization. However Doucet et al. [2006]
note that importance sampling is still possible on the extended set of particles
(Î·1:tâˆ’1, Î·â€²
tâˆ’L+1:t) by introducing some auxiliary distribution Î»t(Î·tâˆ’L+1:tâˆ’1|Î·â€²
1:t). This
leads to the correct marginal whatever Î»t and the algorithm has the following
81

3 Computation of Gaussian orthant probabilities in high dimension
incremental weights:
Ï€t(Î·1:tâˆ’L, Î·â€²
tâˆ’L+1:t)Î»t(Î·tâˆ’L+1:tâˆ’1|Î·â€²
tâˆ’L+1:t, Î·1:tâˆ’1)
Ï€tâˆ’1(Î·1:tâˆ’1)q(Î·â€²
tâˆ’L+1:t|Î·1:tâˆ’1)
.
The authors show that the optimal proposal and resulting weights are given by:
qopt(Î·â€²
tâˆ’L+1:t|Î·1:tâˆ’1) = Ï€t(Î·â€²
tâˆ’L+1:t|Î·1:tâˆ’L),
wt = wtâˆ’1
Ï€t(Î·â€²
1:t)
Ï€tâˆ’L(Î·1:tâˆ’L).
In our case the optimal proposal can then be shown to be:
qopt
t (Î·â€²
tâˆ’L+1:t|Î·1:tâˆ’L) =
Qt
i=1 âœ¶Bi(Î·<i)
Qt
i=tâˆ’L+1 Ï•(Î·i)
Â´ Qt
i=1 âœ¶Bi(Î·<i)
Qt
i=tâˆ’L+1 Ï•(Î·i)dÎ·tâˆ’L+1:t
.
Notice that this is the density of a truncated Gaussian distribution, yielding a
weight depending on an orthant probability (denominator). In most cases this
is not available and in our particular case it is the quantity of interest. We can
however compute explicitly this integral for L = 1 and L = 2. The former is the
usual case (block of size one). The case L = 2 did not bring any improvement
in terms of variance in all our simulation. We concentrated on the extension to
blocks of higher dimension.
In this case we have to resort to approximations of the proposal. The ï¬rst idea
would be to approximate it by a Gaussian using expectation propagation [Minka,
2001a].
However this approach did not perform better than the use of Gibbs
sampler mentioned earlier. Another approach to approximate the distribution is
to consider the Gibbs sampler on a block of size L with the GHK proposal.
Partial conclusion
We have shown that the proposed Gibbs sampler outperforms HMC. Concerning
block sampling the diï¬€erent approaches were tested on several dimensions only to
ï¬nd that the best performing approach was to use partial Gibbs sampling, i.e. a
Gibbs sampler on a block. In the numerical tests we provide in Section 3.6 we
show only the latter.
In our simulations we propose to repeat each kernels as was explained in Section
3.4.3. We propose to test the Gibbs sampler and the overrelaxed random walk.
In addition we have studied other kernels based on the geometry of the prob-
lem; in particular, one can draw random walks on the line between the current
particle and the basic solution of our constraint. Those approach did not however
outperform the proposals discussed above.
82

3.5 Extentions
3.5 Extentions
3.5.1 Student Orthant
We can easily extend our approach to the computation of orthant probabilities for
other distributions, in particular for mixtures of Gaussians, that is probabilities
that can be written as:
Ë†
fU(u)
Ë†
fH|U(Î·|u)1{b>Î·>a}dÎ·du,
(3.8)
where fH|U is a Gaussian.
Several distributions can be created as such.
For
instance, the Student distribution where the variance is marginally distributed as
an inverse-Ï‡2. Hence the distribution:
fH|U(Î·|u)1{b>Î·>a} =
n
Y
i=1
Ï•(Î·i)1{Bu
i (Î·<i)},
where Bu(Î·<i) is Bi(Î·<i) where we multiply a by u
Î½ and fU(u) = Ï‡2
Î½(u). They
are an interesting application to those algorithms because they come at a minimal
additional cost and are of use in multiple comparison [Bretz et al., 2001].
Another example is the logistic distribution where fU(u) is some transformation
of a Kolmogorov-Smirnov distribution (see Holmes and Held [2006]). This could
be used to perform Bayesian inference on multinomial logistic regression.
To deal with this integral we can extend the space on which the SMC is car-
ried out at time t.
Hence the move step is performed on the extended space
fU(u)fH|U(Î·|u). In our Student example it amounts to taking as a target distri-
bution
Ï€n(Î·1:n, u) âˆ
n
Y
i=1
Ï•(Î·i)1{Bu
i (Î·<i)}Ï‡2
Î½(u).
The normalizing constant that the SMC algorithm approximates is
Zn =
Ë†
n
Y
i=1
Ï•(Î·i)1{Bu
i (Î·<i)}Ï‡2
Î½(u)dÎ·1:ndu.
At each move step we therefore move the particles using a Metropolis-Hastings
algorithm targeting p(u|Î·1:n) and perform the remaining Gibbs sampler updates
conditionally on U. This additional step allows for further mixing. Beneï¬ts from
this step are already found in relatively low dimension as shown in Section 3.6.
83

3 Computation of Gaussian orthant probabilities in high dimension
3.5.2 SMC as a truncated distribution sampler
A natural extension is to use Alg. 12 to compute other integrals with respect to
truncated Gaussians. At time t the output of the algorithm is a weighted sample
(wi
t, Î·i
1:t)iâˆˆ[1,M] approximating Ï€t(Î·1:t) âˆQt
i=1 Ï•(Î·i)âœ¶B(Î·<i). Hence any integral of
the form EÏ€t (h(Î·)), where expectation is taken with respect to Ï€t, can be ap-
proximated by PM
i=1
wi
t
PM
j=1 wj
t h(Î·i
1:t). The same argument goes for the truncated
Student.
We test the idea for computing the expectation of truncated multivariate Stu-
dent. We use a Gibbs sampler as a benchmark based on Robert [1995]â€™s sampler
by adding a MH step to deal with u (see previous section). The Gibbs update is
done after a change of variable that leaves the truncations independent. This can
be shown to be more eï¬ƒcient. We allocate 100 times more computational time to
the Gibbs sampler than the SMC.
In Figure 3.4 we see that after thinning one out of 1000 points the ACF and
trace plots point to bad exploration of the targetâ€™s support. This behavior shows
that the convergence is too slow for the algorithm to be of practical use. On the
other hand the SMC is still stable as is shown in the next section.
In addition of outperforming the Gibbs sampler for fairly moderate dimension,
the SMC algorithm was found to be stable for approximating the expectation in
dimensions up to 100.
84

3.6 Numerical results
0
1
2
3
4
0
100
200
300
(a) Trace plot
0.00
0.25
0.50
0.75
1.00
0
25
50
75
100
lag
acf
(b) ACF
âˆ’2
âˆ’1
0
1
2
0
100
200
300
(c) Trace plot
0.00
0.25
0.50
0.75
1.00
0
25
50
75
100
lag
acf
(d) ACF
Figure 3.4: Truncated Student sampling after thinning one out of
every 1000, using a Gibbs sampler
The data are generated as explained in the â€œNumerical results sectionâ€, for dimension 50. The left
panels are trace plots for two components. The right panels are the ACFs. Both are shown after
a thinning of 1/1000. Both show a slow convergence, whereas we observe that SMC is stable.
3.6 Numerical results
3.6.1 Covariance simulation, tunning parameters
To build the covariance matrices we propose to use draws from a Cauchy distri-
bution. We start by sampling a matrix X and a vector a from an independent
Cauchy distribution Xij âˆ¼C(0, 0.01) and ai âˆ¼C(0, 0.01), then construct the co-
variance matrix as Î£ = XtX and the truncation as a = (ai). Because of the heavy
tails the resulting correlation matrix (ï¬gure 3.5a) has many close to zero entries
and some high correlations. The truncations also have some very high levels (ï¬gure
3.5b).
85

3 Computation of Gaussian orthant probabilities in high dimension
We ï¬nd that this approach leads to more challenging covariances than those
built by sampling the spectrum of the covariance matrix as proposed for instance
in Christen et al. [2012].
0
10
20
30
40
50
0
10
20
30
40
50
(a) Correlations
0
10
20
30
40
50
0
50
100
(b) Truncation
Figure 3.5: Generated correlations
Covariance matrices generated from random samples with heavy tails. The various dimension are
simulated with the same algorithm and same seed, such that the small ones are subsets of the
others. The left panel shows a heatmap of the correlation, the right panel the left trucation of the
integral.
The tuning parameters of the algorithm are the threshold that tunes the number
of steps of the MCMC kernel, and the targeted ESS under which we resample, Î±â‹†.
The former is set to some small value (0.01) and has not much inï¬‚uence. The latter
gives us a trade oï¬€between variance and computational cost. In our example we
have found that 0.5M allows good approximation, however this value should be
increased with the diï¬ƒculty of the problem.
3.6.2 GHK for moderate dimensions
All our results are shown at constant computational cost: we repeat the algorithm
in a ï¬rst time to get their execution time, and we then scale them accordingly. In
the above example (dimension 40) for instance the number of draws associated with
the GHK algorithm is 1, 065, 399. The number of particles of the SMC sampler
with Gibbs Markov transition is 5217.
86

3.6 Numerical results
â—
â—â—
â—
1eâˆ’10
2eâˆ’10
3eâˆ’10
BS
GHK
Gibbs
RW
value
(a) Dimension 20
â—
â—
â—â—
â—
â—
â—
â—
âˆ’2.560843eâˆ’16
2.184932eâˆ’15
4.625948eâˆ’15
7.066964eâˆ’15
9.507981eâˆ’15
BS
GHK
Gibbs
RW
value
(b) Dimension 30
â—
â—
â—
â—
2.063132eâˆ’23
5.551489eâˆ’23
9.039845eâˆ’23
1.252820eâˆ’22
1.601656eâˆ’22
BS
GHK
Gibbs
RW
value
(c) Dimension 40
â—
â—
â—â—â—
â—
â—
â—
â—
â—
2.380611eâˆ’23
1.473823eâˆ’22
2.709584eâˆ’22
3.945346eâˆ’22
5.181107eâˆ’22
BS
GHK
Gibbs
RW
value
(d) Dimension 50
Figure 3.6: Estimates of Orthant probabilities for GHK compared to
SMC with diï¬€erent moves.
The various dimensions are simulated as described in Section 3.6. Diï¬€erent moves are tested inside
SMC. As we have already discussed GHK leads to a higher variance and outliers. Block sampling
(BS) and Gibbs sampling (Gibbs) seem to outperform the overrelaxed random walk (RW). However
all three stay stable until dimension 50.
We ï¬nd that in moderate dimension (âˆ¼50) the GHK simulator breaks down in
attempting to compute the probability of the orthant generated by our simulation
scheme. This is all the more problematic as it gives an answer, and there is no
way of checking its departure from the true value.
Another interesting aspect is the fact that the block sampling algorithm performs
well in those dimensions. It is quicker than to move the particles in every dimension
as is done with MCMC. The truncations that lead to a drop of probability are
87

3 Computation of Gaussian orthant probabilities in high dimension
close together because of the ordering, hence once the diï¬ƒcult dimensions have
been â€œabsorbedâ€ it is less and less paramount to visit the past truncations.
3.6.3 High dimension orthant probabilities
In dimensions higher than p = 70, the covariance we simulate lead to integrals
that cannot be treated with the GHK algorithm. In our simulations GHK always
returned NaN values due to the low values of the weights. For the SMC an indicator
of the good behavior of the algorithm can be seen in either its reproducibility and
the fact that we do not encounter asymmetry (see Remark 3.3.1) as for the GHK
in the ï¬rst two Sections. Furthermore the ESS does not fall very low along the
particlesâ€™ draw (Figure 3.7c).
â—
5.133160eâˆ’50
1.979805eâˆ’49
3.446294eâˆ’49
4.912784eâˆ’49
6.379273eâˆ’49
BS
Gibbs
OR
value
(a) Dimension 130
â—
âˆ’1.064822eâˆ’73
3.710540eâˆ’72
7.527562eâˆ’72
1.134458eâˆ’71
1.516161eâˆ’71
BS
Gibbs
OR
value
(b) Dimension 180
0
1000
2000
3000
0
50
100
(c) Dimension 130
Figure 3.7: Estimates of Orthant probabilities p = 130 and p = 180
The various dimensions are simulated as described in Section 3.6. Diï¬€erent moves are tested inside
SMC. As we have already discussed GHK leads to a higher variance and outliers. Gibbs sampling
(Gibbs) seem to outperform the overrelaxed random walk (OR) and Block sampling (BS). However
all three stay stable until dimension 180. The ESS for the Gibbs sampler is shown in panel c for a
threshold of 0.5M and M = 3000. Despite some sudden drops it seems to be stable.
For those dimensions the Gibbs sampler performs best in terms of variance.
However if oneâ€™s goal is a fast algorithm, at the cost of higher variance the overre-
laxation might be preferable at some point as the dimension of the target increases.
The latter as a complexity smaller of one degree such that at constant computa-
tional cost it will have more and more particles allocated to it.
88

3.6 Numerical results
3.6.4 Student orthant probabilities
We use the same schemes as before to construct the covariance matrix and ï¬x a
degree of freedom of 3 in our experiments. As before we show an improvement as
compared to previous algorithms. This improvements appears also for moderate
dimensions. It seems that there is an important gain in considering the extended
target.
As for the Gaussian case we ï¬nd that the output of GHK is heavily skewed. It
seems that it is not the case for our algorithm.
â—
â—
â—
0.0e+00
5.0eâˆ’14
1.0eâˆ’13
1.5eâˆ’13
2.0eâˆ’13
GHK
SMC
value
(a) Dimension 30
â—
â—
â—
â—
â—
âˆ’4.542168eâˆ’19
2.191693eâˆ’18
4.837602eâˆ’18
7.483511eâˆ’18
1.012942eâˆ’17
GHK
SMC
value
(b) Dimension 50
Figure 3.8: Estimates of Student orthant probabilities SMC vs GHK
Covariance matrices generated from random samples with heavy tails. We ï¬nd that the SMC
outperforms the GHK. As for the previous cases the GHK leads to some outliers.
3.6.5 Application to random utility models
Random utility models are an important area of research in Economics to model
choice data [Train, 2009]. Consider an agent i confronted to J alternatives each
giving utility Y â‹†
ij âˆ€j âˆˆ{1, Â· Â· Â· , J} modeled by Y â‹†
ij = XiÎ² + uij with uij a Gaussian
noise. Individual i chooses alternative j if {âˆ€k Ì¸= j
Y â‹†
ij > Y â‹†
ik}. The likelihood is
the probability of this set integrated over the unobserved alternatives. Hence the
likelihood is given by:
L(Yi = j|â„¦, Î², X) = P
 \
kÌ¸=j
{Y â‹†
ij > Y â‹†
ik}}
!
89

3 Computation of Gaussian orthant probabilities in high dimension
= P
 \
kÌ¸=j
{(Xij âˆ’Xik)Î²â‹†> uik âˆ’uij}}
!
where integration is taken over u âˆ¼N(0, â„¦), where u = (uij). The above integral
is an orthant probability of dimension J âˆ’1. A yet more challenging case occurs
in the presence of panel data. The latter corresponds to sequential choices of an
individual in time. We denote those choices by the subscript t. We observe (jt)t<T
for every individual. Integration is now in dimension T(J âˆ’1) and takes the form:
L(Yi,1:T = j1:T|â„¦, Î², X) = P
 T\
t=1
\
kÌ¸=jt
{(Xijtt âˆ’Xikt)Î²â‹†> uikt âˆ’uijtt}}
!
We take the covariance structure studied in Bursh-Supan et al. [1992].
The
noise term is uitk = Î±ik + Î·ikt where Î·ikt = ÌºiÎ·iktâˆ’1 + Î½it, where (Î±ik) are correlated
amongst choices, so are Î½it. The terms are all Gaussian.
The dataset is simulated to allow for examples that are more complex, and of
variable size. In the model presented above individuals are independent so that
we present results in computing the integral for n = 1, and have already a big
advantage of using our methodology.
â—
â—
1.1eâˆ’11
1.3eâˆ’11
1.5eâˆ’11
1.7eâˆ’11
GHK
SMC
value
(a) J = 10, T = 10
â—
â—
â—
5.00eâˆ’14
7.50eâˆ’14
1.00eâˆ’13
1.25eâˆ’13
1.50eâˆ’13
GHK
SMC
value
(b) J = 10, T = 15
Figure 3.9: Estimates of the likelihood of a multivariate Probit
Dataset: The Data is simulated using the covariance proposed in Bursh-Supan et al. [1992], the
value of the parameter for which the likelihood is evaluated is taken at random.
Figure (3.9) shows, for two problems of diï¬€erent size, the gain in precision at
constant computational cost. The improvement is substantial and increases with
90

the size of the problem. Taking n > 1 would only increase this eï¬€ect as it would
consist in taking products of such estimators.
The latter result suggests that we could use the likelihood for inference using
either maximum likelihood or PMCMC. Computing the likelihood with lowest
possible variance is also a key issue in ï¬nding the evidence in the most precise
manner possible.
When comparing the two algorithms we set the number of particles of SMC to
M = 1000, for the same computational cost we allocate M = 881031 to GHK.
SMC has however still a lower variance. Regardless of computational time, the
ability to compute precise integrals for a small number of particles can also be of
importance. It is the case for instance in SMC2 [Chopin et al., 2013b] where whole
trajectories have to be kept in memory for several samplers.
One could extend these models to multivariate Probit with Student distribution
and to multivariate logit models for more robustness. In this case we can use the
algorithm based on the mixture representation built in Section 3.5.1.
3.7 Conclusion
We have shown empirically that the GHK algorithm collapses when the dimension
of the problem increases (returning NaN values). In other cases, the distribution
of estimates generated by GHK may have heavy tails (see also Remark 3.3.1).
Theoretically for at least one covariance structure we have shown that the variance
of the algorithm diverges exponentially fast with the dimension (Section 3.3). Our
SMC algorithm seems to correct this behavior, and was found to be of practical
use for many problems.
We have tested several kernels as part of the move step (Section 3.4.2). We
advise the practitioners to use Gibbs sampling as the standard â€œgo toâ€ move step.
However improvements in speed can be achieved for dimensions around 50 using
only a partial update.
In addition as the dimension increases one might want
to use a method with lower complexity at the cost of having to repeat the move
a bit more. In this case we recommend the use of an overrelaxed random walk
Metropolis-Hastings.
We have shown that the same idea can be use for computing probabilities of
mixtures of Gaussians. In addition we can use the weighted particles returned by
the algorithm to compute other integrals (mean, variance, .etc). This approach
91

3 Computation of Gaussian orthant probabilities in high dimension
can outperform a classical Gibbs sampler when the dimension exceeds 20.
Appendix
3.A Proof of proposition 2.1
Proof:
We have that for 0 < b < âˆthe transition density pb(x, y)dy, associated
with the kernel P b(x, dy) with respect to the Lebesgue measure, is lower bounded
by a constant and the transition is continuous. This Markov chain is a Ïˆ-irreducible
on a compact support. Hence we can show that the whole support [0, b] is small
[Meyn and Tweedie, 2009].
Hence by theorem 16.1.2 to show V-Uniform ergodicity the transition must
satisfy the drift condition:
Ë†
p(x, y)Ve(y)dy â‰¤(1 âˆ’Î²)Ve(x) + câœ¶[0,b](x)
for Î² > 0, c < âˆand a certain Ve(x) with value in [1, âˆ). We take Ve(x) = ex2+1,
e > 0. In the following we check this condition.
The left hand side is given by E(X2
t |Xtâˆ’1 = x) for the above transition proba-
bility,
E(X2
t |Xtâˆ’1 = x) = Ìº2x2 + 1 + ÌºÏ•(Ìºx) âˆ’bÏ•(b âˆ’Ìºx)
Î¦(b âˆ’Ìºx) âˆ’Î¦(âˆ’Ìºx)Ìºx
The ratio is continuous on the bounded set [0, b], and can be bounded by a constant,
such that the by taking Î² = 1 âˆ’Ìº2 > 0 the drift condition is satisï¬ed for a c(e)
depending on e.
In addition we can compute exactly the invariant measure. It is unique and
given by the solution of:
Ï€(y) =
Ë†
Ï•(y; Ìºx, 1)
Î¦(a âˆ’Ìºx) âˆ’Î¦(âˆ’Ìºx)âœ¶[0,b](y)Ï€(x)dx
92

3.A Proof of proposition 2.1
The solution of the above equation is a truncated skew-Normal distribution,
Ï€(dy) âˆ{Î¦(b âˆ’Ìºy) âˆ’Î¦(âˆ’Ìºy)} Ï•(y; 0, 1 âˆ’Ìº2)âœ¶[0,b](y)dy.
The moments of this distribution have been studied in Flecher et al. [2009], in
particular note that 0 < VÏ€(X) < âˆ.
Deï¬ne Ïˆ : x 7â†’log [Î¦(b âˆ’Ìºx) âˆ’Î¦(âˆ’Ìºx)], by theorem 17.0.1 [Meyn and Tweedie,
2009] to obtain a CLT for
1
âˆš
T
PT
t=1 Ïˆ(Xt) we must ensure that there exist a con-
stant e > 0 such that Ïˆ2(x) < Ve(x) on [0, b]. Such a constant can be found by
noting that Ïˆ(x) is bounded as long as b > 0 and that Ve is strictly increasing of
e > 0 with value on (0, âˆ). The value of e depends on b. We obtain the following
convergence result,
1
âˆš
T
 T
X
t=1
Ïˆ(Xt) âˆ’TEÏ€(Ïˆ(X))
!
â‡N (0, VÏ€ {Ïˆ(X)} + Ï„) ,
where the variance term is deï¬ned because Ïˆ is bounded on [0, b], and Ï„ =
2 Pâˆ
k=1 cov(X0, Xk). By taking the exponential and using the continuous map-
ping theorem (p.7 Van der Vaart [1998]) we get a log-normal limiting distribution
 
QT
t=1 (Î¦(b âˆ’ÌºXt) âˆ’Î¦(âˆ’ÌºXt))
exp{TEÏ€ log (Î¦(b âˆ’ÌºX) âˆ’Î¦(âˆ’ÌºX))}
!
1
âˆš
T
â‡EN (0, VÏ€ {Ïˆ(X)} + Ï„) .
By Portmanteauâ€™s Lemma (p.6 Van der Vaart [1998]) for x2 as a continuous and
positive function,
lim inf
Tâ†’âˆE
ï£®
ï£°
 
QT
t=1 (Î¦(b âˆ’ÌºXt) âˆ’Î¦(âˆ’ÌºXt))2
exp{2TEÏ€ log (Î¦(b âˆ’ÌºX) âˆ’Î¦(âˆ’ÌºX))}
!
1
âˆš
T
ï£¹
ï£»> exp {VÏ€ [Ïˆ(X)] + Ï„}
lim inf
Tâ†’âˆE
" 
QT
t=1 (Î¦(b âˆ’ÌºXt) âˆ’Î¦(âˆ’ÌºXt))2
exp{2TEÏ€ log (Î¦(b âˆ’ÌºX) âˆ’Î¦(âˆ’ÌºX))}
!#
1
âˆš
T
> exp {VÏ€ [Ïˆ(X)] + Ï„}
where the last line is obtained by Jensen inequality. The denominator is the square
of limit value of the normalizing constant under x2-Uniform ergodicity that follows
from the above statement.
â–¡
93

3 Computation of Gaussian orthant probabilities in high dimension
3.B Resampling
Algorithm 13 Systematic resampling, n particles
Input: Vector of weights w and vector x to sample from
Set v â†nw, j â†1, c = v1
Sample: Sample U âˆ¼U[0,1]
for k = 1, Â· Â· Â· , n do
while c < u do
Set j â†j + 1, c â†c + vj
end while
Set: Ë†xk â†xj, u â†u + 1
end for
return Ë†x
3.C Variable Ordering
Algorithm 14 Variable Ordering
INIT: i1 = arg min1â‰¤kâ‰¤T Î¦
h
ak
Î³kk , bk
Î³kk
i
Î·1 =
1
Î¦

ai1
Î³i1i1
,
bi1
Î³i1i1
 Â´
ai1
Î³i1i1
,
bi1
Î³i1i1
 Î·Ï•(Î·)dÎ·
for i âˆˆ{2, Â· Â· Â· , d} do
STEP 1 ij = arg minjâ‰¤kâ‰¤T Î¦
h
1
ËœÎ³kk
n
ak âˆ’Pjâˆ’1
l=1 ËœÎ³ilkÎ·k
o
,
1
ËœÎ³kk
n
bk âˆ’Pjâˆ’1
l=1 ËœÎ³ilkÎ·k
oi
STEP 2
Î·j =
1
Î¦
h
1
ËœÎ³kk
n
ak âˆ’Pjâˆ’1
l=1 ËœÎ³ilkÎ·k
o
,
1
ËœÎ³kk
n
bk âˆ’Pjâˆ’1
l=1 ËœÎ³ilkÎ·k
oiÃ—
Ë†
h
1
ËœÎ³kk {akâˆ’Pjâˆ’1
l=1 ËœÎ³ilkÎ·k},
1
ËœÎ³kk {bkâˆ’Pjâˆ’1
l=1 ËœÎ³ilkÎ·k}
i Î·Ï•(Î·)dÎ·
end for
return (i1, Â· Â· Â· , id)
Where ËœÎ³ is updated accordingly when the order is changed.
94

3.D Hamiltonian Monte Carlo
3.D Hamiltonian Monte Carlo
â—
â—
âˆ’1.739871eâˆ’18
7.901708eâˆ’18
1.754329eâˆ’17
2.718487eâˆ’17
3.682644eâˆ’17
Gibbs
Hamiltonian
value
(a) Dimension 30
â—
1.323594eâˆ’18
6.966259eâˆ’18
1.260892eâˆ’17
1.825159eâˆ’17
2.389425eâˆ’17
Gibbs
Hamiltonian
value
(b) Dimension 40
â—
1.243277eâˆ’23
4.412336eâˆ’23
7.581396eâˆ’23
1.075045eâˆ’22
1.391951eâˆ’22
Gibbs
Hamiltonian
value
(c) Dimension 50
â—
â—
â—
âˆ’1.250314eâˆ’30
5.859634eâˆ’30
1.296958eâˆ’29
2.007953eâˆ’29
2.718948eâˆ’29
Gibbs
Hamiltonian
value
(d) Dimension 60
Figure 3.D.1: Estimates of Orthant probabilities Gibbs vs HMC
Covariance matrices generated from random samples with heavy tails. The various dimension are
simulated with the same algorithm and same seed, such that the small ones are subsets of the
others. The grey boxplot corresponds to the Gibbs sampler the white to the HMC. The Gibbs
sampler seem to have smaller variance and no outliers.
95


4
Theoretical and computational aspects of PAC
Bayesian ranking and scoring
This is joint work with Pierre Alquier, Nicolas Chopin and Feng
Liang
Status: Published in NIPS proceedings.
4.1 Introduction
Bipartite ranking (scoring) amounts to rank (score) data from binary labels. An
important problem in its own right, bipartite ranking is also an elegant way to
formalise classiï¬cation: once a score function has been estimated from the data,
classiï¬cation reduces to chooses a particular threshold, which determine to which
class is assigned each data-point, according to whether its score is above or below
that threshold. It is convenient to choose that threshold only once the score has
been estimated, so as to get ï¬ner control of the false negative and false positive
rates; this is easily achieved by plotting the ROC (Receiver operating characteris-
tic) curve.
A standard optimality criterion for scoring is AUC (Area Under Curve), which
measures the area under the ROC curve. AUC is appealing for at least two rea-
sons.
First, maximising AUC is equivalent to minimising the L1 distance be-
tween the estimated score and the optimal score. Second, under mild conditions,
Cortes and Mohri [2003] show that AUC for a score s equals the probability that
s(Xâˆ’) < s(X+) for Xâˆ’(resp.
X+) a random draw from the negative (resp.
positive class). Yan et al. [2003] observed AUC-based classiï¬cation handles much
better skewed classes (say the positive class is much larger than the other) than
97

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
standard classiï¬ers, because it enforces a small score for all members of the nega-
tive class (again assuming the negative class is the smaller one).
One practical issue with AUC maximisation is that the empirical version of AUC
is not a continuous function. One way to address this problem is to â€convexifyâ€
this function, and study the properties of so-obtained estimators [ClÂ´emenÂ¸con et al.,
2008a].
We follow instead the PAC-Bayesian approach in this chapter, which
consists of using a random estimator sampled from a pseudo-posterior distribution
that penalises exponentially the (in our case) AUC risk. It is well known [see e.g.
the monograph of Catoni, 2007] that the PAC-Bayesian approach comes with a
set of powerful technical tools to establish non-asymptotic bounds; the ï¬rst part
of the chapter derive such bounds. A second advantage however of this approach,
as we show in the second part of the chapter, is that it is amenable to powerful
Bayesian computational tools, such as Sequential Monte Carlo and Expectation
Propagation.
4.2 Theoretical bounds from the PAC-Bayesian
Approach
4.2.1 Notations
The data D consist in the realisation of n IID (independent and identically dis-
tributed) pairs (Xi, Yi) with distribution P, and taking values in RdÃ—{âˆ’1, 1}. Let
n+ = Pn
i=1 âœ¶{Yi = +1}, nâˆ’= n âˆ’n+. For a score function s : Rd â†’R, the AUC
risk and its empirical counter-part may be deï¬ned as:
R(s) = P(X,Y ),(Xâ€²,Y â€²)âˆ¼P [{s(X) âˆ’s(Xâ€²)}(Y âˆ’Y â€²) < 0] ,
Rn(s) =
1
n(n âˆ’1)
X
iÌ¸=j
âœ¶[{s(Xi) âˆ’s(Xj)}(Yi âˆ’Yj) < 0] .
Let Ïƒ(x) = E(Y |X = x), Â¯R = R(Ïƒ) and Â¯Rn = Rn(Ïƒ). It is well known that Ïƒ is
the score that minimise R(s), i.e. R(s) â‰¥Â¯R = R(Ïƒ) for any score s.
The results of this section apply to the class of linear scores, sÎ¸(x) = âŸ¨Î¸, xâŸ©, where
âŸ¨Î¸, xâŸ©= Î¸Tx denotes the inner product. Abusing notations, let R(Î¸) = R(sÎ¸),
Rn(Î¸) = Rn(sÎ¸), and, for a given prior density Ï€Î¾(Î¸) that may depend on some
hyperparameter Î¾ âˆˆÎ, deï¬ne the Gibbs posterior density (or pseudo-posterior) as
Ï€Î¾,Î³(Î¸|D) := Ï€Î¾(Î¸) exp {âˆ’Î³Rn(Î¸)}
ZÎ¾,Î³(D)
,
ZÎ¾,Î³(D) =
Ë†
Rd Ï€Î¾(ËœÎ¸) exp
n
âˆ’Î³Rn(ËœÎ¸)
o
dËœÎ¸
for Î³ > 0. Both the prior and posterior densities are deï¬ned with respect to the
Lebesgue measure over Rd.
98

4.2 Theoretical bounds from the PAC-Bayesian Approach
4.2.2 Assumptions and general results
Our general results require the following assumptions.
Deï¬nition 4.2.1 We say that Assumption Dens(c) is satisï¬ed for c > 0 if
P(âŸ¨X1 âˆ’X2, Î¸âŸ©â‰¥0, âŸ¨X1 âˆ’X2, Î¸â€²âŸ©â‰¤0) â‰¤câˆ¥Î¸ âˆ’Î¸â€²âˆ¥
for any Î¸ and Î¸â€² âˆˆRd such that âˆ¥Î¸âˆ¥= âˆ¥Î¸â€²âˆ¥= 1.
This is a mild Assumption, which holds for instance as soon as (X1âˆ’X2)/âˆ¥X1âˆ’
X2âˆ¥admits a bounded probability density; see the appendix.
Deï¬nition 4.2.2 (Mammen & Tsybakov margin assumption) We say that
Assumption MA(Îº, C) is satisï¬ed for Îº âˆˆ[1, +âˆ] and C â‰¥1 if
E

(qÎ¸
1,2)2
â‰¤C

R(Î¸) âˆ’R
 1
Îº
where qÎ¸
i,j = âœ¶{âŸ¨Î¸, Xi âˆ’XjâŸ©(Yi âˆ’Yj) < 0} âˆ’âœ¶{[Ïƒ(Xi) âˆ’Ïƒ(Xj)](Yi âˆ’Yj) < 0} âˆ’
R(Î¸) + R.
This assumption was introduced for classiï¬cation by Mammen and Tsybakov
[1999], and used for ranking by ClÂ´emenÂ¸con et al. [2008b] and Robbiano [2013] (see
also a nice discussion in LecuÂ´e [2007]). The larger Îº, the less restrictive MA(Îº, C).
In fact, MA(âˆ, C) is always satisï¬ed for C = 4. For a noiseless classiï¬cation task
(i.e. Ïƒ(Xi)Yi â‰¥0 almost surely), R = 0,
E((qÎ¸
1,2)2) = Var(qÎ¸
1,2) = E[âœ¶{âŸ¨Î¸, X1 âˆ’X2âŸ©(Yi âˆ’Yj) < 0}] = R(Î¸) âˆ’R
and MA(1, 1) holds. More generally, MA(1, C) is satisï¬ed as soon as the noise
is small; see the discussion in Robiano 2013 (Proposition 5 p. 1256) for a formal
statement. From now, we focus on either MA(1, C) or MA(âˆ, C), C â‰¥1. It
is possible to prove convergence under MA(Îº, 1) for a general Îº â‰¥1, but at the
price of complications regarding the choice of Î³; see Catoni [2007], Alquier [2008]
and Robbiano [2013].
We use the classical PAC-Bayesian methodology initiated by McAllester [1998];
Shawe-Taylor and Williamson [1997] (see Alquier [2008]; Catoni [2007] for a com-
plete survey and more recent advances) to get the following results. Proof of these
and forthcoming results may be found in the appendix. Let K(Ï, Ï€) denotes the
Kullback-Liebler divergence, K(Ï, Ï€) =
Â´
Ï(dÎ¸) log{ dÏ
dÏ€(Î¸)} if Ï << Ï€, âˆotherwise,
and denote M1
+ the set of probability distributions Ï(dÎ¸).
99

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Lemma 4.2.1 Assume that MA(1, C) holds with C â‰¥1. For any ï¬xed Î³ with
0 < Î³ â‰¤(n âˆ’1)/(8C), for any Îµ > 0, with probability at least 1 âˆ’Îµ on the drawing
of the data D,
Ë†
R(Î¸)Ï€Î¾,Î³(Î¸|D)dÎ¸ âˆ’R â‰¤2 inf
ÏâˆˆM1
+
(Ë†
R(Î¸)Ï(dÎ¸) âˆ’R + 2K(Ï, Ï€Î¾) + log
  4
Îµ

Î³
)
.
Lemma 4.2.2 Assume MA(âˆ, C) with C â‰¥1. For any ï¬xed Î³ with 0 < Î³ â‰¤
(n âˆ’1)/8, for any Ç« > 0 with probability 1 âˆ’Ç« on the drawing of D,
Ë†
R(Î¸)Ï€Î¾,Î³(Î¸|D)dÎ¸ âˆ’Â¯R â‰¤inf
ÏâˆˆM1
+
Ë†
R(Î¸)Ï(dÎ¸) âˆ’Â¯R + 2K(Ï, Ï€Î¾) + log 2
Ç«
Î³

+ 16Î³
n âˆ’1.
Both lemmas bound the expected risk excess, for a random estimator of Î¸ gen-
erated from Ï€Î¾,Î³(Î¸|D).
4.2.3 Independent Gaussian Prior
We now specialise these results to the prior density Ï€Î¾(Î¸) = Qd
i=1 Ï•(Î¸i; 0, Ï‘), i.e. a
product of independent Gaussian distributions N(0, Ï‘); Î¾ = Ï‘ in this case.
Theorem 4.2.3 Assume MA(1, C), C â‰¥1, Dens(c), c > 0, and take Ï‘ = 2
d(1 +
1
n2d), Î³ = (n âˆ’1)/8C, then there exists a constant Î± = Î±(c, C, d) such that for any
Ç« > 0, with probability 1 âˆ’Ç«,
Ë†
R(Î¸)Ï€Î³(Î¸|D)dÎ¸ âˆ’Â¯R â‰¤2 inf
Î¸0

R(Î¸0) âˆ’Â¯R
	
+ Î±d log(n) + log 4
Ç«
n âˆ’1
.
Theorem 4.2.4 Assume MA(âˆ, C), C â‰¥1, Dens(c) c > 0, and take Ï‘ =
2
d(1 +
1
n2d), Î³ = C
p
dn log(n), there exists a constant Î± = Î±(c, C, d) such that for
any Ç« > 0, with probability 1 âˆ’Ç«,
Ë†
R(Î¸)Ï€Î³(Î¸|D)dÎ¸ âˆ’Â¯R â‰¤inf
Î¸0

R(Î¸0) âˆ’Â¯R
	
+ Î±
p
d log(n) + log 2
Ç«
âˆšn
.
The proof of these results is provided in the appendix. It is known that, un-
der MA(Îº, C), the rate (d/n)
Îº
2Îºâˆ’1 is minimax-optimal for classiï¬cation problems,
see LecuÂ´e [2007]. Following Robbiano [2013] we conjecture that this rate is also
optimal for ranking problems.
100

4.3 Practical implementation of the PAC-Bayesian approach
4.2.4 Spike and slab prior for feature selection
The independent Gaussian prior considered in the previous section is a natu-
ral choice, but it does not accommodate sparsity, that is, the possibility that
only a small subset of the components of Xi actually determine the membership
to either class.
For sparse scenarios, one may use the spike and slab prior of
Mitchell and Beauchamp [1988], George and McCulloch [1993b],
Ï€Î¾(Î¸) =
d
Y
i=1
[pÏ•(Î¸i; 0, v1) + (1 âˆ’p)Ï•(Î¸i; 0, v0)]
with Î¾ = (p, v0, v1) âˆˆ[0, 1]Ã—(R+)2, and v0 â‰ªv1, for which we obtain the following
result. Note âˆ¥Î¸âˆ¥0 is the number of non-zero coordinates for Î¸ âˆˆRd.
Theorem 4.2.5 Assume MA(1, C) holds with C â‰¥1, Dens(c) holds with c > 0,
and take p = 1 âˆ’exp(âˆ’1/d), v0 â‰¤1/(2nd log(d)), and Î³ = (n âˆ’1)/(8C). Then
there is a constant Î± = Î±(C, v1, c) such that for any Îµ > 0, with probability at least
1 âˆ’Îµ on the drawing of the data D,
Ë†
R(Î¸)Ï€Î³(dÎ¸|D) âˆ’R â‰¤2 inf
Î¸0
(
R(Î¸0) âˆ’R + Î±âˆ¥Î¸0âˆ¥0 log(nd) + log
  4
Îµ

2(n âˆ’1)
)
.
Compared to Theorem 4.2.3, the bound above increases logarithmically rather
than linearly in d, and depends explicitly on âˆ¥Î¸âˆ¥0, the sparsity of Î¸. This suggests
that the spike and slab prior should lead to better performance than the Gaussian
prior in sparse scenarios. The rate âˆ¥Î¸âˆ¥0 log(d)/n is the same as the one obtained
in sparse regression, see e.g. BÂ¨uhlmann and van de Geer [2011].
Finally, note that if v0 â†’0, we recover the more standard prior which assigns a
point mass at zero for every component. However this leads to a pseudo-posterior
which is a mixture of 2d components that mix Dirac masses and continuous distri-
butions, and thus which is more diï¬ƒcult to approximate (although see the related
remark in Section 4.3.4 for Expectation-Propagation).
4.3 Practical implementation of the PAC-Bayesian
approach
4.3.1 Choice of hyper-parameters
Theorems 4.2.3, 4.2.4, and 4.2.5 propose speciï¬c values for hyper-parameters Î³
and Î¾, but these values depend on some unknown constant C. Two data-driven
101

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
ways to choose Î³ and Î¾ are (i) cross-validation (which we will use for Î³), and (ii)
(pseudo-)evidence maximisation (which we will use for Î¾).
The latter may be justiï¬ed from intermediate results of our proofs in the ap-
pendix, which provide an empirical bound on the expected risk:
Ë†
R(Î¸)Ï€Î¾,Î³(Î¸|D)dÎ¸ âˆ’Â¯R â‰¤Î¨Î³,n inf
ÏâˆˆM1
+
Ë†
Rn(Î¸)Ï(dÎ¸) âˆ’Â¯Rn + K(Ï, Ï€) + log 2
Ç«
Î³

with Î¨Î³,n â‰¤2. The right-hand side is minimised at Ï(dÎ¸) = Ï€Î¾,Î³(Î¸|D)dÎ¸, and the
so-obtained bound is âˆ’Î¨Î³,n log(ZÎ¾,Î³(D))/Î³ plus constants. Minimising the upper
bound with respect to hyperparameter Î¾ is therefore equivalent to maximising
log ZÎ¾,Î³(D) with respect to Î¾. This is of course akin to the empirical Bayes ap-
proach that is commonly used in probabilistic machine learning. Regarding Î³ the
minimization is more cumbersome because the dependence with the log(2/Ç«) term
and Î¨n,Î³, which is why we recommend cross-validation instead.
It seems noteworthy that, beside Alquier and Biau [2013], very few papers dis-
cuss the practical implementation of PAC-Bayes, beyond some brief mention of
MCMC (Markov chain Monte Carlo). However, estimating the normalising con-
stant of a target density simulated with MCMC is notoriously diï¬ƒcult. In addition,
even if one decides to ï¬x the hyperparameters to some arbitrary value, MCMC
may become slow and diï¬ƒcult to calibrate if the dimension of the sampling space
becomes large. This is particularly true if the target does not (as in our case) have
some speciï¬c structure that makes it possible to implement Gibbs sampling. The
two next sections discuss two eï¬ƒcient approaches that make it possible to approx-
imate both the pseudo-posterior Ï€Î¾,Î³(Î¸|D) and its normalising constant, and also
to perform cross-validation with little overhead.
4.3.2 Sequential Monte Carlo
Given the particular structure of the pseudo-posterior Ï€Î¾,Î³(Î¸|D), a natural ap-
proach to simulate from Ï€Î¾,Î³(Î¸|D) is to use tempering SMC [Sequential Monte Carlo
Del Moral et al., 2006b] that is, deï¬ne a certain sequence Î³0 = 0 < Î³1 < . . . < Î³T,
start by sampling from the prior Ï€Î¾(Î¸), then applies successive importance sampling
steps, from Ï€Î¾,Î³tâˆ’1(Î¸|D) to Ï€Î¾,Î³t(Î¸|D), leading to importance weights proportional
to:
Ï€Î¾,Î³t(Î¸|D)
Ï€Î¾,Î³tâˆ’1(Î¸|D) âˆexp {âˆ’(Î³t âˆ’Î³tâˆ’1)Rn(Î¸)} .
When the importance weights become too skewed, one rejuvenates the particles
through a resampling step (draw particles randomly with replacement, with prob-
ability proportional to the weights) and a move step (move particles according to
a certain MCMC kernel).
102

4.3 Practical implementation of the PAC-Bayesian approach
One big advantage of SMC is that it is very easy to make it fully adaptive. For
the choice of the successive Î³t, we follow Jasra et al. [2007] in solving numerically
(5.18) in order to impose that the Eï¬€ective sample size has a ï¬xed value. This
ensures that the degeneracy of the weights always remain under a certain thresh-
old.
For the MCMC kernel, we use a Gaussian random walk Metropolis step,
calibrated on the covariance matrix of the resampled particles. See Algorithm 19
for a summary.
Algorithm 15 Tempering SMC
Input N (number of particles), Ï„ âˆˆ(0, 1) (ESS threshold), Îº > 0 (random walk
tuning parameter)
Init. Sample Î¸i
0 âˆ¼Ï€Î¾(Î¸) for i = 1 to N, set t â†1, Î³0 = 0, Z0 = 1.
Loop a. Solve in Î³t the equation
{PN
i=1 wt(Î¸i
tâˆ’1)}2
PN
i=1{wt(Î¸i
tâˆ’1))2}
= Ï„N,
wt(Î¸) = exp[âˆ’(Î³t âˆ’Î³tâˆ’1)Rn(Î¸)]
(4.1)
using bisection search. If Î³t â‰¥Î³T, set ZT = Ztâˆ’1 Ã—
n
1
N
PN
i=1 wt(Î¸i
tâˆ’1)
o
,
and stop.
b. Resample: for i = 1 to N, draw Ai
t in 1, . . . , N so that P(Ai
t = j) =
wt(Î¸j
tâˆ’1)/ PN
k=1 wt(Î¸k
tâˆ’1); see Algorithm 1 in the appendix.
c. Sample Î¸i
t âˆ¼Mt(Î¸
Ai
t
tâˆ’1, dÎ¸) for i = 1 to N where Mt is a MCMC kernel that
leaves invariant Ï€t; see Algorithm 3 in the appendix for an instance of
such a MCMC kernel, which takes as an input S = ÎºË†Î£, where Ë†Î£ is the
covariance matrix of the Î¸
Ai
t
tâˆ’1.
d. Set Zt = Ztâˆ’1 Ã—
n
1
N
PN
i=1 wt(Î¸i
tâˆ’1)
o
.
In our context, tempering SMC brings two extra advantages: it makes it possible
to obtain samples from Ï€Î¾,Î³(Î¸|D) for a whole range of values of Î³, rather than a
single value. And it provides an approximation of ZÎ¾,Î³(D) for the same range of Î³
values, through the quantity Zt deï¬ned in Algorithm 19.
103

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
4.3.3 Expectation-Propagation (Gaussian prior)
The SMC sampler outlined in the previous section works fairly well, and we will
use it as gold standard in our simulations. However, as any other Monte Carlo
method, it may be too slow for large datasets. We now turn our attention to
EP [Expectation-Propagation Minka, 2001a], a general framework to derive fast
approximations to target distributions (and their normalising constants).
First note that the pseudo-posterior may be rewritten as:
Ï€Î¾,Î³(Î¸|D) =
1
ZÎ¾,Î³(D)Ï€Î¾(Î¸) Ã—
Y
i,j
fij(Î¸),
fij(Î¸) = exp [âˆ’Î³â€²âœ¶{âŸ¨Î¸, Xi âˆ’XjâŸ©< 0}]
where Î³â€² = Î³/n+nâˆ’, and the product is over all (i, j) such that Yi = 1, Yj =
âˆ’1. EP generates an approximation of this target distribution based on the same
factorisation:
q(Î¸) âˆq0(Î¸)
Y
i,j
qij(Î¸),
qij(Î¸) = exp{âˆ’1
2Î¸TQijÎ¸ + rT
ijÎ¸}.
We consider in the section the case where the prior is Gaussian, as in Section
4.2.3.
Then one may set q0(Î¸) = Ï€Î¾(Î¸).
The approximating factors are un-
normalised Gaussian densities (under a natural parametrisation), leading to an
overall approximation that is also Gaussian, but other types of exponential fam-
ily parametrisations may be considered; see next section and Seeger [2005a]. EP
updates iteratively each site qij (that is, it updates the parameters Qij and rij),
conditional on all the sites, by matching the moments of q with those of the hybrid
distribution
hij(Î¸) âˆq(Î¸)fij(Î¸)
qij(Î¸) âˆq0(Î¸)fij(Î¸)
Y
(k,l)Ì¸=(i,j)
qkl(Î¸)
where again the product is over all (k, l) such that Yk = 1, Yl = âˆ’1, and (k, l) Ì¸=
(i, j).
We refer to the appendix for a precise algorithmic description of our EP imple-
mentation. We highlight the following points. First, the site update is particularly
simple in our case:
hij(Î¸) âˆexp{Î¸Trh
ij âˆ’1
2Î¸TQh
ijÎ¸} exp [âˆ’Î³â€²âœ¶{âŸ¨Î¸, Xi âˆ’XjâŸ©< 0}] ,
with rh
ij = P
(k,l)Ì¸=(i,j) rkl, Qh
ij = P
(k,l)Ì¸=(i,j) Qkl, which may be interpreted as: Î¸
conditional on T(Î¸) = âŸ¨Î¸, Xi âˆ’XjâŸ©has a d âˆ’1-dimensional Gaussian distribution,
and the distribution of T(Î¸) is that of a one-dimensional Gaussian penalised by a
step function. The two ï¬rst moments of this particular hybrid may therefore be
104

4.3 Practical implementation of the PAC-Bayesian approach
computed exactly, and in O(d2) time, as explained in the appendix. The updates
can be performed eï¬ƒciently using the fact that the linear combination (Xi âˆ’Xj)Î¸
is a one dimensional Gaussian. For our numerical experiment we used a parallel
version of EP Van Gerven et al. [2010]. The complexity of our EP implementation
is O(n+nâˆ’d2 + d3).
Second, EP oï¬€ers at no extra cost an approximation of the normalising constant
ZÎ¾,Î³(D) of the target Ï€Î¾,Î³(Î¸|D); in fact, one may even obtain derivatives of this
approximated quantity with respect to hyper-parameters. See again the appendix
for more details.
Third, in the EP framework, cross-validation may be interpreted as dropping
all the factors qij that depend on a given data-point Xi in the global approxima-
tion q. This makes it possible to implement cross-validation at little extra cost
[Opper and Winther, 2000].
4.3.4 Expectation-Propagation (spike and slab prior)
To adapt our EP algorithm to the spike and slab prior of Section 4.2.4, we introduce
latent variables Zk = 0/1 which â€chooseâ€ for each component Î¸k whether it comes
from a slab, or from a spike, and we consider the joint target
Ï€Î¾,Î³(Î¸, z|D) âˆ
( d
Y
k=1
B(zk; p)N(Î¸k; 0, vzk)
)
exp
"
âˆ’
Î³
n+nâˆ’
X
ij
âœ¶{âŸ¨Î¸, Xi âˆ’XjâŸ©> 0}
#
.
On top of the n+nâˆ’Gaussian sites deï¬ned in the previous section, we add a
product of d sites to approximate the prior. Following Hernandez-Lobato et al.
[2013], we use
qk(Î¸k, zk) = exp

zk log

pk
1 âˆ’pk

âˆ’1
2Î¸2
kuk + vkÎ¸k

that is a (un-normalised) product of an independent Bernoulli distribution for zk,
times a Gaussian distribution for Î¸k. Again that the site update is fairly straightfor-
ward, and may be implemented in O(d2) time. See the appendix for more details.
Another advantage of this formulation is that we obtain a Bernoulli approxima-
tion of the marginal pseudo-posterior Ï€Î¾,Î³(zi = 1|D) to use in feature selection.
Interestingly taking v0 to be exactly zero also yield stable results corresponding to
the case where the spike is a Dirac mass.
105

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
4.4 Extension to non-linear scores
To extend our methodology to non-linear score functions, we consider the pseudo-
posterior
Ï€Î¾,Î³(ds|D) âˆÏ€Î¾(ds) exp
ï£±
ï£²
ï£³âˆ’
Î³
n+nâˆ’
X
iâˆˆD+, jâˆˆDâˆ’
âœ¶{s(Xi) âˆ’s(Xj) > 0}
ï£¼
ï£½
ï£¾
where Ï€Î¾(ds) is some prior probability measure with respect to an inï¬nite-dimensional
functional class. Let si = s(Xi), s1:n = (s1, . . . , sn) âˆˆRn, and assume that Ï€Î¾(ds)
is a GP (Gaussian process) associated to some kernel kÎ¾(x, xâ€²), then using a stan-
dard trick in the GP literature [Rasmussen and Williams, 2006], one may derive
the marginal (posterior) density (with respect to the n-dimensional Lebesgue mea-
sure) of s1:n as
Ï€Î¾,Î³(s1:n|D) âˆNd (s1:n; 0, KÎ¾) exp
ï£±
ï£²
ï£³âˆ’
Î³
n+nâˆ’
X
iâˆˆD+, jâˆˆDâˆ’
âœ¶{si âˆ’sj > 0}
ï£¼
ï£½
ï£¾
where Nd (s1:n; 0, KÎ¾) denotes the probability density of the N(0, KÎ¾) distribution,
and KÎ¾ is the n Ã— n matrix (kÎ¾(Xi, Xj))n
i,j=1.
This marginal pseudo-posterior retains essentially the structure of the pseudo-
posterior Ï€Î¾,Î³(Î¸|D) for linear scores, except that the â€œparameterâ€ s1:n is now of
dimension n. We can apply straightforwardly the SMC sampler of Section 4.B.1,
and the EP algorithm of 4.B.2, to this new target distribution. In fact, for the EP
implementation, the particular simple structure of a single site:
exp [âˆ’Î³â€²âœ¶{si âˆ’sj > 0}]
makes it possible to implement a site update in O(1) time, leading to an overall
complexity O(n+nâˆ’+ n3) for the EP algorithm.
Theoretical results for this approach could be obtained by applying lemmas from
e.g. van der Vaart and van Zanten [2009], but we leave this for future study.
4.5 Numerical Illustration
Figure 1 compares the EP approximation with the output of our SMC sampler,
on the well-known Pima Indians dataset and a Gaussian prior. Marginal ï¬rst and
second order moments essentially match; see the appendix for further details. The
subsequent results are obtained with EP.
106

4.5 Numerical Illustration
0.0
0.5
1.0
âˆ’2
âˆ’1
0
(a) Î¸1
0.00
0.25
0.50
0.75
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
(b) Î¸2
0.0
0.5
1.0
1.5
âˆ’1
0
1
(c) Î¸3
Figure 4.1: EP Approximation (green), compared to SMC (blue) of
the marginal posterior of the ï¬rst three coeï¬ƒcients, for
Pima dataset (see the appendix for additional analysis).
We now compare our PAC-Bayesian approach (computed with EP) with Bayesian
logistic regression (to deal with non-identiï¬able cases), and with the rankboost al-
gorithm [Freund et al., 2003] on diï¬€erent datasets1; note that Cortes and Mohri
[2003] showed that the function optimised by rankbook is AUC.
As mentioned in Section 4.B, we set the prior hyperparameters by maximizing
the evidence, and we use cross-validation to choose Î³. To ensure convergence of EP,
when dealing with diï¬ƒcult sites, we use damping [Seeger, 2005a]. The GP version
of the algorithm is based on a squared exponential kernel. Table 5.2 summarises
the results; balance refers to the size of the smaller class in the data (recall that
the AUC criterion is particularly relevant for unbalanced classiï¬cation tasks), EP-
AUC (resp. GPEP-AUC) refers to the EP approximation of the pseudo-posterior
based on our Gaussian prior (resp. Gaussian process prior). See also Figure 2 for
ROC curve comparisons, and Table 2 in the appendix for a CPU time comparison.
Note how the GP approach performs better for the colon data, where the number
of covariates (2000) is very large, but the number of observations is only 40. It
seems also that EP gives a better approximation in this case because of the lower
dimensionality of the pseudo-posterior (Figure 4.2b).
Finally, we also investigate feature selection for the DNA dataset (180 covariates)
using a spike and slab prior. The regularization plot (4.3a) shows how certain
coeï¬ƒcients shrink to zero as the spikeâ€™s variance v0 goes to zero, allowing for some
1All available at http://archive.ics.uci.edu/ml/
107

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Dataset
Covariates
Balance
EP-AUC
GPEP-AUC
Logit
Rankboost
Pima
7
34%
0.8617
0.8557
0.8646
0.8224
Credit
60
28%
0.7952
0.7922
0.7561
0.788
DNA
180
22%
0.9814
0.9812
0.9696
0.9814
SPECTF
22
50%
0.8684
0.8545
0.8715
0.8684
Colon
2000
40%
0.7034
0.75
0.73
0.5935
Glass
10
1%
0.9843
0.9629
0.9029
0.9436
Table 4.1: Comparison of AUC.
The Glass dataset has originally more than two classes. We compare the â€œsiliconâ€ class against
all others.
sparsity. The aim of a positive variance in the spike is to absorb negligible eï¬€ects
into it [RoË‡ckovÂ´a and George, 2013]. We observe this eï¬€ect on ï¬gure 4.3a where
one of the covariates becomes positive when v0 decreases.
âˆ’0.3
âˆ’0.2
âˆ’0.1
0.0
0.1
1eâˆ’04
1eâˆ’02
v0
Î¸
(a) Regularization plot
â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—
â—â—â—â—â—â—â—â—
â—
â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—
â—â—
â—
â—â—
â—
â—â—
â—â—â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—
â—
â—â—
â—â—â—â—â—â—â—â—
â—â—â—â—
â—â—
â—
â—
â—â—
â—â—â—â—â—
â—
â—â—
â—â—â—
â—
â—
â—
â—
â—â—
â—â—â—
â—
â—
â—
â—
â—â—
â—
â—â—â—
â—â—
â—
â—â—
â—
â—â—
â—
â—â—
â—
â—â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—â—
â—
â—â—â—
â—â—â—â—
â—
â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—
â—â—â—â—â—â—â—â—
â—
â—â—â—â—
â—â—â—â—â—â—â—â—â—
â—
â—â—
â—
â—â—
â—
â—â—
â—â—â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
âˆ’0.1
0.0
0
50
100
150
V1
Î¸
(b) Estimate
Figure 4.3: Regularization plot for v0 âˆˆ

10âˆ’6, 0.1

and estimation for
v0 = 10âˆ’6 for DNA dataset; blue circles denote posterior
probabilities â‰¥0.5.
108

0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
(a) Rankboost
vs
EP-AUC
on
Pima
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
(b) Rankboost vs GPEP-AUC on
Colon
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
(c) Logistic vs EP-AUC on Glass
Figure 4.2: Some ROC curves associated to the example described in
a more systematic manner in table 5.2. In black is always
the PAC version.
4.6 Conclusion
The combination of the PAC-Bayesian theory and Expectation-Propagation leads
to fast and eï¬ƒcient AUC classiï¬cation algorithms, as observed on a variety of
datasets, some of them very unbalanced. Future work may include extending our
approach to more general ranking problems (e.g. multi-class), establishing non-
asymptotic bounds in the nonparametric case, and reducing the CPU time by
considering only a subset of all the pairs of datapoints.
109

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Appendix
4.A PAC-Bayes bounds for linear scores
4.A.1 Suï¬ƒcient condition for Dens(c)
A simple suï¬ƒcient condition for Dens(c) to hold is that (X1 âˆ’X2)/âˆ¥X1 âˆ’X2âˆ¥
admits a probability density with respect to the spherical measure of dimension
d âˆ’1 which is bounded above by B. Then
P(âŸ¨X1 âˆ’X2, Î¸âŸ©â‰¥0, âŸ¨X1 âˆ’X2, Î¸â€²âŸ©â‰¤0) â‰¤B arccos (âŸ¨Î¸, Î¸â€²âŸ©)
2Ï€
â‰¤B
2Ï€
p
5 âˆ’5 âŸ¨Î¸, Î¸â€²âŸ©
= B
2Ï€
r
5
2âˆ¥Î¸ âˆ’Î¸â€²âˆ¥.
4.A.2 Proof of Lemma 2.1
In order to prove Lemma 2.1 we need the following Bernstein inequality.
Proposition 4.A.1 (Bernsteinâ€™s inequality for U-statistics) For any Î³ > 0,
for any Î¸ âˆˆRd,
E exp[Î³|Rn(Î¸) âˆ’Rn âˆ’R(Î¸) + R|] â‰¤2 exp
" Î³2
nâˆ’1E((qÎ¸
1,2)2)
 1 âˆ’
4Î³
nâˆ’1

#
.
Proof of Proposition 4.A.1. Fix Î¸. Remember that
qÎ¸
i,j = 1{âŸ¨Î¸, Xi âˆ’XjâŸ©(Yi âˆ’Yj) < 0} âˆ’1{[Ïƒ(Xi) âˆ’Ïƒ(Xj)](Yi âˆ’Yj) < 0} âˆ’R(Î¸) + R
so that
Un := Rn(Î¸) âˆ’Rn âˆ’R(Î¸) + R =
1
n(n âˆ’1)
X
iÌ¸=j
qÎ¸
i,j.
First, note that
E exp[Î³|Un|] â‰¤E exp[Î³Un] + E exp[Î³(âˆ’Un)].
110

4.A PAC-Bayes bounds for linear scores
We will only upper bound the ï¬rst term in the r.h.s., as the upper bound for the
second term may be obtained exactly in the same way (just replace qÎ¸
i,j by âˆ’qÎ¸
i,j).
Now, use Hoeï¬€dingâ€™s decomposition Hoeï¬€ding [1948]: this is the technique used
by Hoeï¬€ding to prove inequalities on U-statistics. Hoeï¬€ding proved that
Un = 1
n!
X
Ï€
1
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
where the sum is taken over all the permutations Ï€ of {1, . . . , n}. Jensenâ€™s inequal-
ity leads to
E exp[Î³Un] = E exp
ï£®
ï£°Î³ 1
n!
X
Ï€
1
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£»
â‰¤1
n!
X
Ï€
E exp
ï£®
ï£°Î³
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£».
We now use, for each of the terms in the sum, Massartâ€™s version of Bernsteinâ€™s
inequality Massart [2007] (ineq. (2.21) in Chapter 2, the assumption is checked by
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹) âˆˆ[âˆ’2, 2] so E((qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹))k) â‰¤E((qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹))2)2kâˆ’2). We obtain:
E exp
ï£®
ï£°Î³
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£»â‰¤exp
ï£®
ï£°E((qÎ¸
Ï€(1),Ï€(1+âŒŠn
2 âŒ‹))2) Î³2
âŒŠn
2 âŒ‹
2

1 âˆ’2 Î³
âŒŠn
2 âŒ‹

ï£¹
ï£».
First, note that we have the inequality âŒŠn
2âŒ‹â‰¥(n âˆ’1)/2. Then, remark that as the
pairs (Xi, Yi) are iid, we have E((qÎ¸
Ï€(1),Ï€(1+âŒŠn
2 âŒ‹))2) = E((qÎ¸
1,2)2) so we have a simpler
inequality
E exp
ï£®
ï£°Î³
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£»â‰¤exp
"
E((qÎ¸
1,2)2) Î³2
nâˆ’1
 1 âˆ’
4Î³
nâˆ’1

#
.
This ends the proof of the proposition. â–¡
The following proposition is also of use in the proof of lemma 2.1.
Proposition 4.A.2 For any measure Ï âˆˆM1
+(Î˜) and any measurable function
h : Î¸ â†’R such that
Â´
exp(h(Î¸))Ï€(dÎ¸) < âˆ, we have
log
Ë†
exp(h(Î¸))Ï€(Î¸)

= sup
ÏâˆˆM1
+
Ë†
h(Î¸)Ï(dÎ¸) âˆ’K(Ï, Ï€)

.
111

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
In addition if h is bounded by above on the support of Ï€ the supremum is reached
for the Gibbs distribution,
Ï(dÎ¸) âˆexp (h(Î¸)) Ï€(dÎ¸).
Proof:
e.g. Catoni [2007].
â–¡
Proof of Lemma 2.1
From the proof of Proposition 4.A.1, and using the short-
hand qÎ¸ for qÎ¸
1,2, we deduce
E

exp{Ï
 Î³(Rn(Î¸) âˆ’Â¯Rn âˆ’R(Î¸) + Â¯R)} + Î·(Î¸)

â‰¤exp
 
Î³2
n âˆ’1
Ï (Eq2
Î¸)
(1 âˆ’4
Î³
nâˆ’1) + Ï (Î·(Î¸))
!
.
(4.2)
Using proposition 4.A.2, and the fact that ex â‰¥âœ¶{x â‰¥0} we have that
P{
sup
ÏâˆˆM1
+(Î˜)
Ï
 Î³(Rn(Î¸) âˆ’Â¯Rn âˆ’R(Î¸) + Â¯R) âˆ’Î·(Î¸)

âˆ’K(Ï, Ï€) â‰¥0}
â‰¤E
 Ï€{exp{Ï
 Î³(Rn(Î¸) âˆ’Â¯Rn âˆ’R(Î¸) + Â¯R) âˆ’Î·(Î¸)}

}
= Ï€
 E{exp{Ï
 Î³(Rn(Î¸) âˆ’Â¯Rn âˆ’R(Î¸) + Â¯R) âˆ’Î·(Î¸)}

}
, by Fubini
â‰¤Ï€
(
exp
 
Î³2Ï(Eq2
Î¸)
(n âˆ’1)(1 âˆ’
4Î³
nâˆ’1) âˆ’Ï(Î·(Î¸))
!)
, using (4.2).
In the following we take Î·(Î¸) = log 1
Ç« +
Î³2
nâˆ’1
Ï(Eq2
Î¸)
(1âˆ’4
Î³
nâˆ’1 ) leading to the following result
with probability at least 1 âˆ’Ç«, âˆ€Ï âˆˆM1
+(Î˜):
Ï(Rn(Î¸)) âˆ’Â¯Rn â‰¤Ï(R(Î¸)) âˆ’Â¯R + K(Ï, Ï€) + log 1
Ç«
Î³
+
Î³
n âˆ’1
Ï(Eq2
Î¸)
(1 âˆ’4
Î³
nâˆ’1).
(4.3)
Under MA(1, C) we can write:
Ï(Rn(Î¸)) âˆ’Â¯Rn â‰¤
 
1 +
Î³C
n âˆ’1
1
(1 âˆ’
4
nâˆ’1)
!
 Ï(R(Î¸)) âˆ’Â¯R

+ K(Ï, Ï€) + log 1
Ç«
Î³
.
Using Bernsteinâ€™s inequality in the symmetric case, with probability 1 âˆ’Ç« we can
assert that:
 
1 âˆ’
Î³C
n âˆ’1
1
(1 âˆ’Î³
4
nâˆ’1)
!
 Ï(R(Î¸)) âˆ’Â¯R

â‰¤Ï(Rn(Î¸)) âˆ’Â¯Rn + K(Ï, Ï€) + log 1
Ç«
Î³
.
112

4.A PAC-Bayes bounds for linear scores
The latter is true in particular for Ï = Ï€(Î¸|S), the Gibbs posterior:
 
1 âˆ’
Î³C
n âˆ’1
1
(1 âˆ’Î³
4
nâˆ’1)
! Ë†
Î˜
R(Î¸)Ï€Î³(dÎ¸|D) âˆ’Â¯R

â‰¤inf
ÏâˆˆM1
+

Ï(Rn(Î¸)) âˆ’Â¯Rn + K(Ï, Ï€) + log 1
Ç«
Î³

.
Making use of equation (4.3) and the fact that Î³ â‰¤(n âˆ’1)/8C we have with
probability 1 âˆ’2Ç«:
Ë†
Î˜
Rn(Î¸)Ï€Î³(dÎ¸|D) âˆ’Â¯Rn

â‰¤2 inf
ÏâˆˆM1
+

Ï(R(Î¸)) âˆ’Â¯R + 2K(Ï, Ï€) + log 1
Ç«
Î³

.
â–¡
Lemma 2.1 gives some approximately correct ï¬nite sample bound under hy-
pothesis MA(1, C). It is easy to extend those results to the more general case of
MA(âˆ, C). Note in particular that this assumption is always satisï¬ed for C = 4.
Proof of Lemma 2.2
First consider in our case that, the margin assumption is
always true for C = 4, E(q2
Î¸) â‰¤4, the rest of the proof is similar to that of lemma
2.1. From equation (4.3) with the above hypothesis:
Ï(Rn(Î¸)) âˆ’Â¯Rn â‰¤Ï(R(Î¸)) âˆ’Â¯R + K(Ï, Ï€) + log 1
Ç«
Î³
+
4Î³
n âˆ’1
1
(1 âˆ’
4
nâˆ’1)
From the Bernstein inequality with in the symmetric case we get with probability
1 âˆ’Ç«:
Ï(R(Î¸)) âˆ’Â¯R â‰¤Ï(Rn(Î¸)) âˆ’Â¯Rn + K(Ï, Ï€) + log 1
Ç«
Î³
+
4Î³
n âˆ’1
1
(1 âˆ’
4
nâˆ’1)
We get, after noting that the Gibbs posterior can be written as an inï¬mum
(Legendre transform), with probability 1 âˆ’2Ç«:
Ë†
(R(Î¸)Ï€Î³(dÎ¸|D) âˆ’Â¯R â‰¤
inf
ÏâˆˆM1
+(Î˜) Ï(R(Î¸)) âˆ’Â¯R + K(Ï, Ï€) + log 1
Ç«
Î³
+ 16Î³
n âˆ’1
(we also used Î³ â‰¤(n âˆ’1)/8).
â–¡
The two above lemma depend on some class complexity K(Ï, Ï€). The latter can
be specialized to diï¬€erent choice of prior measure Ï€. In the following we propose
two speciï¬cations to a Gaussian prior and a spike and slab prior.
113

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
4.A.3 Proof of Theorem 2.3 (Independent Gaussian prior)
For any Î¸0 âˆˆRp with âˆ¥Î¸0âˆ¥= 1 and Î´ > 0 we put
ÏÎ¸0,Î´(dÎ¸) âˆ1âˆ¥Î¸âˆ’Î¸0âˆ¥â‰¤Î´Ï€(dÎ¸).
Then we have, from Lemma 2.1, with probability at least 1 âˆ’Îµ,
Ë†
R(Î¸)Ï€Î³(dÎ¸|D) âˆ’R â‰¤2 inf
Î¸0,Î´
(Ë†
R(Î¸)ÏÎ¸0,Î´(dÎ¸) âˆ’R + 16C K(ÏÎ¸0,Î´, Ï€) + log
  4
Îµ

(n âˆ’1)
)
First, note that
R(Î¸) = E (âœ¶{âŸ¨Î¸, X âˆ’Xâ€²âŸ©(Y âˆ’Y â€²) < 0})
= E (âœ¶{âŸ¨Î¸0, X âˆ’Xâ€²âŸ©(Y âˆ’Y â€²) < 0})
+ E (âœ¶{âŸ¨Î¸, X âˆ’Xâ€²âŸ©(Y âˆ’Y â€²) < 0} âˆ’âœ¶{âŸ¨Î¸0, X âˆ’Xâ€²âŸ©(Y âˆ’Y â€²) < 0})
â‰¤R(Î¸0) + P(sign âŸ¨Î¸, X âˆ’Xâ€²âŸ©(Y âˆ’Y â€²) Ì¸= sign âŸ¨Î¸0, X âˆ’Xâ€²âŸ©(Y âˆ’Y â€²))
= R(Î¸0) + P(sign âŸ¨Î¸, X âˆ’Xâ€²âŸ©Ì¸= sign âŸ¨Î¸0, X âˆ’Xâ€²âŸ©)
â‰¤R(Î¸0) + c

Î¸
âˆ¥Î¸âˆ¥âˆ’Î¸0

â‰¤R(Î¸0) + 2câˆ¥Î¸ âˆ’Î¸0âˆ¥.
As a consequence
Â´
R(Î¸)ÏÎ¸0,Î´(dÎ¸) â‰¤R(Î¸0) + 2cÎ´.
The next step is to calculate K(ÏÎ¸0,Î´, Ï€). We have
K(ÏÎ¸0,Î´, Ï€) = log
1
Ï€ ({Î¸ : âˆ¥Î¸ âˆ’Î¸0âˆ¥â‰¤Î´}).
Assuming that Î¸0,1 > 0 (the proof is exactly symmetric in the other case)
âˆ’K(ÏÎ¸0,Î´, Ï€) = log Ï€
 
{Î¸ :
d
X
i=1
(Î¸i âˆ’Î¸0,i)2 â‰¤Î´2}
!
â‰¥d log Ï€

{Î¸ : (Î¸1 âˆ’Î¸0,1)2 â‰¤Î´2
d }

â‰¥d log
Ë†
Î¸0,1
âˆš
Ï‘ +
Î´
âˆš
Ï‘d
Î¸0,1
âˆš
Ï‘ âˆ’
Î´
âˆš
Ï‘d
Ï•(0,1)(x)dx
â‰¥d log

Î´
2
âˆš
Ï‘d
Ï•
Î¸0,1
âˆš
Ï‘
+
Î´
âˆš
Ï‘d

â‰¥d log

Î´
2
âˆš
Ï‘d
Ï•
 1
âˆš
Ï‘
+
Î´
âˆš
Ï‘d

114

4.A PAC-Bayes bounds for linear scores
= d log
 
Î´
2
âˆš
2Ï€Ï‘d
exp
"
âˆ’1
2
 1
âˆš
Ï‘
+
Î´
âˆš
Ï‘d
2#!
â‰¥d log

Î´
2
âˆš
2Ï€Ï‘d
exp

âˆ’1
Ï‘ âˆ’Î´2
Ï‘d

K(ÏÎ¸0,Î´, Ï€) â‰¤âˆ’d log{Î´} + d
2 log{8Ï€Ï‘d} + 1
Ï‘ + Î´2
Ï‘d
And we can plug the equation above in the result of lemma 2.1 with Î´ = 1
n
Ë†
R(Î¸)Ï€Î³(Î¸|D) âˆ’Â¯R â‰¤2 inf
Î¸0

R(Î¸0) âˆ’Â¯R + 2c1
n + 2
Î³

d log{n} + d
2 log{8Ï€Ï‘d} + 1
Ï‘ +
1
n2
Ï‘d + log 4
Ç«

Any Î³ = O(n) will lead to a convergence result. Taking Î³ = (n âˆ’1)/8C and
optimizing in Ï‘ we obtain a variance of Ï‘ =
2(1+
1
n2d )
d
.
4.A.4 Proof of Theorem 2.4 (Independent Gaussian prior)
As was done for the previous lemmas we can lift the MA(âˆ, C) and use the lemma
2.2 instead, which gives rise to Theorem 2.4.
Use Lemma 2.2 and the same steps as in the proof of Theorem 4.2.3, optimize
w.r.t. Î³ and Ï‘ to get the result.
We show the same kind of result in the following but for spike and slab priors.
4.A.5 Proof of Theorem 2.5 (Spike and slab prior for feature
selection)
As for the proof of theorem 4.2.3 we start by deï¬ning, for any Î¸0 âˆˆRp with
âˆ¥Î¸0âˆ¥= 1 and Î´ > 0,
ÏÎ¸0,Î´(dÎ¸) âˆ1âˆ¥Î¸âˆ’Î¸0âˆ¥â‰¤Î´Ï€(dÎ¸)
so that in the end, by a similar argument as previously it remains only to upper
bound the following quantity,
K(ÏÎ¸0,Î´, Ï€) = log
1
Ï€ ({Î¸ : âˆ¥Î¸ âˆ’Î¸0âˆ¥â‰¤Î´}).
Let Ï€0 denote the probability distribution such that the Î¸i are iid N(0, v0). So:
âˆ’K(ÏÎ¸0,Î´, Ï€) = log Ï€
 (
Î¸ :
d
X
i=1
(Î¸i âˆ’Î¸0,i)2 â‰¤Î´2
)!
115

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
â‰¥log Ï€

Î¸ : âˆ€i, (Î¸i âˆ’Î¸0,i)2 â‰¤Î´2
d

=
X
i:Î¸0,iÌ¸=0
log Ï€

(Î¸i âˆ’Î¸0,i)2 â‰¤Î´2
d

+ log Ï€

âˆ€i with Î¸0,i = 0, Î¸2
i < Î´2
d

â‰¥
X
i:Î¸0,iÌ¸=0
log Ï€

(Î¸i âˆ’Î¸0,i)2 â‰¤Î´2
d

+ log Ï€0

âˆ€i with Î¸0,i = 0, Î¸2
i < Î´2
d

+ d log(1 âˆ’p)
=
X
i:Î¸0,iÌ¸=0
log Ï€

(Î¸i âˆ’Î¸0,i)2 â‰¤Î´2
d

+ log

1 âˆ’Ï€0

âˆƒi, Î¸0,i = 0, Î¸2
i > Î´2
d

+ d log(1 âˆ’p)
â‰¥
X
i:Î¸0,iÌ¸=0
log Ï€

(Î¸i âˆ’Î¸0,i)2 â‰¤Î´2
d

+ log
"
1 âˆ’
X
i:Î¸i=0
Ï€0

Î¸2
i > Î´2
d
#
+ d log(1 âˆ’p).
Assume ï¬rst that i is such that Î¸0,i = 0. Then:
Ï€0

Î¸2
i > Î´2
d

= Ï€0

Î¸i
âˆšv0
 >
Î´
âˆšv0d

â‰¤exp

âˆ’Î´2
2v0d

,
and so
X
i:Î¸0,i=0
Ï€0

Î¸2
i > Î´2
d

â‰¤d exp

âˆ’Î´2
2v0d

â‰¤1
2
as soon as v0 â‰¤Î´2/(2d log(d)). Then, assume that i is such that Î¸0,i Ì¸= 0. Now
assume that Î¸0,i > 0 (the proof is exactly symmetric if Î¸0,i < 0):
Ï€

Î¸ : (Î¸i âˆ’Î¸0,i)2 â‰¤Î´2
d

â‰¥p
Ë†
Î¸0,i
âˆšv1 +
Î´
âˆš
v1d
Î¸0,i
âˆšv1 âˆ’
Î´
âˆš
v1d
Ï•(0,1)(x)dx
â‰¥
pÎ´
2âˆšv1dÏ•
 Î¸0,i
âˆšv1
+
Î´
âˆšv1d

116

4.B Practical implementation of the PAC-Bayesian approach
â‰¥
pÎ´
2âˆšv1dÏ•
 1
âˆšv1
+
Î´
âˆšv1d

=
pÎ´
2âˆš2Ï€v1d exp
"
âˆ’1
2
 1
âˆšv1
+
Î´
âˆšv1d
2#
â‰¥
pÎ´
2âˆš2Ï€v1d exp

âˆ’1
v1
âˆ’Î´2
v1d

.
Putting everything together:
K(ÏÎ¸0,Î´, Ï€) â‰¤âˆ’âˆ¥Î¸0âˆ¥0 log

pÎ´
2âˆš2Ï€v1d exp

âˆ’1
v1
âˆ’Î´2
v1d

+ log(2) + d log
1
1 âˆ’p
= âˆ¥Î¸0âˆ¥0

log
2âˆš2Ï€v1d
pÎ´

+ 1
v1
+ Î´2
v1d

+ log(2) + d log
1
1 âˆ’p.
So, we have:
Ë†
R(Î¸)Ï€Î³(dÎ¸|D) âˆ’R â‰¤2 inf
Î¸0,Î´
(
R(Î¸0) âˆ’R + 2cÎ´
+ 16C
âˆ¥Î¸0âˆ¥0
h
log

2âˆš2Ï€v1d
pÎ´

+ 1
v1 + Î´2
v1d
i
+ log(2) + d log
1
1âˆ’p + log
  4
Îµ

(n âˆ’1)
)
4.B Practical implementation of the PAC-Bayesian
approach
4.B.1 Sequential Monte Carlo
The resampling scheme we use in our SMC sampler is systematic resampling, see
Algorithm 20.
To move the particles while leaving invariant the current target Ï€Î¾,Î³(Î¸|D), we
use the standard random walk Metropolis strategy, but scaled to the current set
of particles, as outlined by Algorithm 17.
4.B.2 Expectation-Propagation (Gaussian prior)
EP aims at approximating posterior distributions of the form,
Ï€(Î¸|D) = 1
ZÏ€
P0(Î¸)
n
Y
i=1
ti(Î¸)
117

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Algorithm 16 Systematic resampling
Input: Normalised weights W j
t := wt(Î¸j
tâˆ’1)/ PN
i=1 wt(Î¸i
tâˆ’1).
Output: indices Ai âˆˆ{1, . . . , N}, for i = 1, . . . , N.
a. Sample U âˆ¼U([0, 1]).
b. Compute cumulative weights as Cn = Pn
m=1 NW m.
c. Set s â†U, m â†1.
d. For n = 1 : N
While Cm < s do m â†m + 1.
An â†m, and s â†s + 1.
End For
by approximating each site ti(Î¸) by a distribution from an exponential family qi(Î¸).
The algorithm cycles through each site, computes the cavity distribution Q\i(Î¸) âˆ
Q(Î¸)qâˆ’1
i (Î¸) and minimizes the Kullback-Leibler divergence between Q\i(Î¸)ti(Î¸) and
the global approximation Q(Î¸). This is eï¬ƒciently done by using properties of the
exponential family (e.g. Bishop [2006a]).
In the Gaussian case the EP approximation can be written as a product of some
prior and a product of sites:
Q(Î¸) âˆN(Î¸; 0, Î£)
Y
i,j
qij(Î¸),
for which the sites are unnormalized Gaussians for the natural parametrization
qij(Î¸) âˆexp
 âˆ’1
2Î¸TQijÎ¸ + Î¸rij

.
We can equivalently use the one dimensional
representation qij(sij) âˆexp
 âˆ’1
2s2
ijKij + sijhij

, going from one to the other is
easily done by multiplying Î¸ by (ei âˆ’ej)X where âˆ€i âˆˆ{1, Â· Â· Â· , n},
ei is a vector
of zeroes with one on the i-th line. Hence we keep in memory only (Kij)ij and
(hij)ij.
While computing the cavity moment we must compute (Q âˆ’(Xi âˆ’Xj)(Xi âˆ’Xj)Kij)
and its inverse. The latter can be computed eï¬ƒciently using Woodbury formula.
Equivalently one could use similar tricks where only the Cholesky factorisation is
saved and updated as in Seeger [2005a]. By precomputing some matrix multipli-
cation the later cavity moment computation can be done in complexity O(p2).
118

4.B Practical implementation of the PAC-Bayesian approach
Algorithm 17 Gaussian random walk Metropolis step
Input: Î¸, S (d Ã— d positive matrix)
Output: Î¸next
a. Sample Î¸prop âˆ¼N(Î¸, S).
b. Sample U âˆ¼U([0, 1]).
c. If log(U) â‰¤log Ï€Î¾,Î³(Î¸prop|D)/Ï€Î¾,Î³(Î¸|D), set Î¸next â†Î¸prop, otherwise set Î¸next â†
Î¸.
To update the sites we compute normalizing constant Zij =
Â´
N(s; m\ij, Ïƒ\ij)tij(s)ds
and use properties of exponential families.
Normalising Constant
The normalizing constant of the posterior can be com-
puted using EP. We have that for each sites tij(Î¸) = Cijqij(Î¸) we replace those
sites in integral we wish to approximate,
Ë†
N(Î¸; 0, Î£)
Y
ij
tij(Î¸)dÎ¸ â‰ƒ
Y
ij
Cij
Ë†
N(Î¸; 0, Î£)
Y
ij
qij(Î¸)dÎ¸
The integral on the right hand side is a Gaussian convolution and is therefore also
Gaussian. The Cijs can be approximated by matching the zeroth order moment
in the site update. As noted in the chapter we can also compute the derivatives
with respect to some prior hyper-parameter (see Seeger [2005a]).
4.B.3 Expectation-Propagation (spike and slab prior)
The posterior can be written as
Ï€(Î¸|D) âˆ
Y
i,j
tij(Î¸)
d
Y
k=1
tk(Î¸k, zk)Ber(zk; p),
where zk âˆˆ{0, 1} codes the origin of Î¸k, spike/slab, and where tk(Î¸k, zk) âˆ
zkN(Î¸k; 0, v0) + (1 âˆ’zk)N(Î¸k; 0, v1). The approximation given by EP is of the
form,
Q(Î¸, z) âˆ
Y
i,j
qij(Î¸)
d
Y
k=1
qk(Î¸k, zk)Ber(zk; pk),
119

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Algorithm 18 parallel EP for Gaussian Prior
Input: Ï‘, Î³
Output: m and V
Init: V â†Î£, m â†0
Untill Convergence Do
For all sites (i, j)Do in parallel
a. Compute the cavity moments m\ij, V \ij
b. Compute the 1st and 2nd order moments of q\ij(sij)tij(sij)
c. Update Kij and hij
End For
Update V = (Î£âˆ’1 + P
ij(Xi âˆ’Xj)T(Xi âˆ’Xj)Kij)âˆ’1, m = V (P
ij(Xi âˆ’Xj)hij)
End While
where qk(Î¸k, zk) âˆBer(zk, pk)N(Î¸k; mk, Ïƒ2
k), and tij(Î¸) is as in the previous section.
The cavity moments are easy to compute as the approximation is Gaussian in Î¸
and Bernoulli in z. In both cases we can deduce cavity moments because division
is stable inside those classes of functions. We get some distribution Q\k(Î¸k) âˆ
Ber(zk; p\k)N(Î¸k; m\k, Ïƒ2,\k).
We can compute the normalizing constant of the
distribution Q\ij(Î¸)tk(Î¸k, zk), namely,
Zk = p\k
Ë†
N(Î¸k; 0, v0)N(Î¸k; m\k, Ïƒ2,\k)dÎ¸k+(1âˆ’p\k)
Ë†
N(Î¸k; 0, v0)N(Î¸k; m\k, Ïƒ2,\k)dÎ¸k
Where we can ï¬nd the update by computing the derivatives of log Zk with respect
to p\k, m\k and Ïƒ2,\k
Initialization for the Gaussian is done to a given Î£0 that will be subtracted later
on. The initial pks are taken such that the approximation equals the prior p at the
ï¬rst iteration.
4.C Numerical illustration
Figure 4.C.1 shows the posterior marginals as given by EP and tempering SMC.
The later is exact in the sense that the only error stems from Monte Carlo; we
120

4.C Numerical illustration
see that the mode is well approximated however the variance is slightly underesti-
mated.
In Table 4.C.1 we show the CPU times in seconds, on all dataset studied. Ex-
periments where run with a i7-3720QM CPU @ 2.60GHz intel processor with 6144
KB cache. Our linear model is overall faster on those datasets. A caveat is that
Rankboost is implemented in Matlab, while our implementation is in C.
Dataset
Covariates
Balance
EP-AUC
GPEP-AUC
Rankboost
Pima
7
34%
0.06
7.75
3.26
Credit
60
28%
1.98
7.59
56.54
DNA
180
22%
11.26
63.47
141.60
SPECTF
22
50%
0.25
63.47
3.55
Colon
2000
40%
636.63
60.99
156.85
Glass
10
1%
0.23
1.33
2.36
Table 4.C.1: Computation times in seconds
121

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Figure 4.C.1: Comparison of the output of the two algorithms
0.0
0.5
1.0
âˆ’2
âˆ’1
0
(a) 1st covariate
0.00
0.25
0.50
0.75
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
(b) 2nd covariate
0.0
0.5
1.0
1.5
âˆ’1
0
1
(c) 3rd covariate
0.00
0.25
0.50
0.75
1.00
1.25
âˆ’1
0
1
(d) 4th covariate
0.0
0.3
0.6
0.9
âˆ’3
âˆ’2
âˆ’1
0
1
(e) 5th covariate
0.0
0.5
1.0
âˆ’2
âˆ’1
0
(f) 6th covariate
0.00
0.25
0.50
0.75
1.00
âˆ’3
âˆ’2
âˆ’1
0
1
(g) 7th covariate
Comparison of the Gaussian approximation obtained by Fractional EP (green) with the true density
generated by SMC (blue) on the Pima indians dataset
122

5
Properties of variational approximations of Gibbs
posteriors
This is joint work with Pierre Alquier and Nicolas Chopin
Status: Submitted to Journal of Machine Learning Rechearch
5.1 Introduction
A Gibbs posterior, also known as a PAC-Bayesian or pseudo-posterior, is a prob-
ability distribution for random estimators of the form:
Ë†ÏÎ»(dÎ¸) = exp[âˆ’Î»rn(Î¸)]
Â´
exp[âˆ’Î»rn]dÏ€Ï€(dÎ¸).
More precise deï¬nitions will follow, but for now, Î¸ may be interpreted as a pa-
rameter (in a ï¬nite or inï¬nite-dimensional space), rn(Î¸) as an empirical measure
of risk (e.g. prediction error), and Ï€(dÎ¸) a prior distribution.
We will follow in this chapter the PAC (Probably Approximatively Correct)-
Bayesian approach, which originates from machine learning [Catoni, 2004; McAllester,
1998; Shawe-Taylor and Williamson, 1997]; see Catoni [2007] for an exhaustive
study, and Dalalyan and Tsybakov [2008]; Jiang and Tanner [2008]; Yang [2004];
Zhang [2006] for related perspectives (such as the aggregation of estimators in
the last 3 papers). There, Ë†ÏÎ» appears as the probability distribution that min-
imises the upper bound of an oracle inequality on the risk of random estimators.
The PAC-Bayesian approach oï¬€ers sharp theoretical guarantees on the properties
of such estimators, without assuming a particular model for the data generating
process.
123

5 Properties of variational approximations of Gibbs posteriors
The Gibbs posterior has also appeared in other places, and under diï¬€erent mo-
tivations: in Econometrics, as a way to avoid direct maximisation in moment
estimation [Chernozhukov and Hong, 2003]; and in Bayesian decision theory, as
as way to deï¬ne a Bayesian posterior distribution when no likelihood has been
speciï¬ed [Bissiri et al., 2013]. Another well-known connection, although less di-
rectly useful (for Statistics), is with thermodynamics, where rn is interpreted as
an energy function, and Î» as the inverse of a temperature.
Whatever the perspective, estimators derived from Gibbs posteriors usually
show excellent performance in diverse tasks, such as classiï¬cation, regression, rank-
ing, and so on, yet their actual implementation is still far from routine. The usual
recommendation [Alquier and Biau, 2013; Dalalyan and Tsybakov, 2012; Guedj and Alquier,
2013] is to sample from a Gibbs posterior using MCMC [Markov chain Monte Carlo,
see e.g. Green et al., 2015]; but constructing an eï¬ƒcient MCMC sampler is often
diï¬ƒcult, and even eï¬ƒcient implementations are often too slow for practical uses
when the dataset is very large.
In this chapter, we consider instead VB (Variational Bayes) approximations,
which have been initially developed to provide fast approximations of â€˜trueâ€™ poste-
rior distributions (i.e. Bayesian posterior distributions for a given model); see Jordan et al.
[1999]; MacKay [2002] and Chap. 10 in Bishop [2006a].
Our main results are as follows: when PAC-Bayes bounds are available - mainly,
when a strong concentration inequality holds - replacing the Gibbs posterior by
a variational approximation does not aï¬€ect the rate of convergence to the best
possible prediction, on the condition that the KÂ¨ullback-Leibler divergence between
the posterior and the approximation is itself controlled in an appropriate way.
We also provide empirical bounds, which may be computed from the data so
as to ascertain the actual performance of estimators obtained by variational ap-
proximation. All the results gives strong incentives, we believe, to recommend
Variational Bayes as the default approach to approximate Gibbs posteriors.
The rest of the chapter is organized as follows. In Section 5.2 we introduce the
notations and assumptions. In Section 5.3 we introduce variational approxima-
tions and the corresponding algorithms. The main results are provided in general
form in Section 5.4: in Subsection 5.4.1, we give results under the assumption that
a Hoeï¬€ding type inequality holds (slow rates) and in Subsection 5.4.2, we give
results under the assumption that a Bernstein type inequality holds (fast rates).
Note that for the sake of shortness, we will refer to these settings as â€œHoeï¬€ding
assumptionâ€ and â€œBernstein assumptionâ€ even if this terminology is non stan-
dard. We then apply these results in various settings: classiï¬cation (Section 5.5),
convex classiï¬cation (Section 5.6), ranking (Section 5.7), and matrix completion
(Section 5.8). In each case, we show how to specialise the general results of Sec-
tion 5.4 to the considered application, so as to obtain the properties of the VB
124

5.2 PAC-Bayesian framework
approximation, and we also discuss its numerical implementation. All the proofs
are collected in the Appendix.
5.2 PAC-Bayesian framework
We observe a sample (X1, Y1), . . . , (Xn, Yn), taking values in X Ã— Y, where the
pairs (Xi, Yi) have the same distribution P. We will assume explicitly that the
(Xi, Yi)â€™s are independent in several of our specialised results, but we do not make
this assumption at this stage, as some of our general results, and more gener-
ally the PAC-Bayesian theory, may be extended to dependent observations; see
e.g. Alquier and Li [2012]. The label set Y is always a subset of R. A set of pre-
dictors is chosen by the statistician: {fÎ¸ : X â†’R, Î¸ âˆˆÎ˜}. For example, in linear
regression, we may have: fÎ¸(x) = âŸ¨Î¸, xâŸ©, the inner product of X = Rd, while in
classiï¬cation, one may have fÎ¸(x) = IâŸ¨Î¸,xâŸ©>0 âˆˆ{0, 1}.
We assume we have at our disposal a risk function R(Î¸); typically R(Î¸) is a
measure of the prevision error. We set R = R(Î¸), where Î¸ âˆˆarg minÎ˜ R; i.e. fÎ¸ is
an optimal predictor. We also assume that the risk function R(Î¸) has an empirical
counterpart rn(Î¸), and set rn = rn(Î¸). Often, R and rn are based on a loss function
â„“: R2 â†’R; i.e. R(Î¸) = E[â„“(Y, fÎ¸(X))] and rn(Î¸) = 1
n
Pn
i=1 â„“(Yi, fÎ¸(Xi)). (In this
chapter, the symbol E will always denote the expectation with respect to the
(unknown) law P of the (Xi, Yi)â€™s.) There are situations however (e.g. ranking),
where R and rn have a diï¬€erent form.
We deï¬ne a prior probability measure Ï€(Â·) on the set Î˜ (equipped with the
standard Ïƒ-algebra for the considered context), and we let M1
+(Î˜) denote the set
of all probability measures on Î˜.
Deï¬nition 5.2.1 We deï¬ne, for any Î» > 0, the pseudo-posterior Ë†ÏÎ» by
Ë†ÏÎ»(dÎ¸) = exp[âˆ’Î»rn(Î¸)]
Â´
exp[âˆ’Î»rn]dÏ€Ï€(dÎ¸).
The pseudo-posterior Ë†ÏÎ» (also known as the Gibbs posterior, Catoni [2004, 2007],
or the exponentially weighted aggregate, Dalalyan and Tsybakov [2008]) plays a
central role in the PAC-Bayesian approach.
It is obtained as the distribution
that minimises the upper bound of a certain oracle inequality applied to random
estimators.
Practical estimators (predictors) may be derived from the pseudo-
posterior, by e.g. taking the expectation, or sampling from it. Of course, when
exp[âˆ’Î»rn(Î¸)] may be interpreted as the likelihood of a certain model, Ë†ÏÎ» becomes
a Bayesian posterior distribution, but we will not restrict our attention to this
particular case.
The following â€˜theoreticalâ€™ counterpart of Ë†ÏÎ» will prove useful to state results.
125

5 Properties of variational approximations of Gibbs posteriors
Deï¬nition 5.2.2 We deï¬ne, for any Î» > 0, Ï€Î» as
Ï€Î»(dÎ¸) = exp[âˆ’Î»R(Î¸)]
Â´
exp[âˆ’Î»R]dÏ€Ï€(dÎ¸).
We will derive PAC-Bayesian bounds on predictions obtained by variational ap-
proximations of Ë†ÏÎ» under two types of assumptions: a Hoeï¬€ding-type assumption,
from which we may deduce slow rates of convergence (Subsection 5.4.1), and a
Bernstein-type assumption, from which we may obtain fast rates of convergence
(Subsection 5.4.2).
Deï¬nition 5.2.3 We say that a Hoeï¬€ding assumption is satisï¬ed for prior Ï€ when
there is a function f and an interval I âŠ‚Râˆ—
+ such that, for any Î» âˆˆI,
Ï€ (E exp {Î»[R(Î¸) âˆ’rn(Î¸)]})
Ï€ (E exp {Î»[rn(Î¸) âˆ’R(Î¸)]})

â‰¤exp [f(Î», n)] .
(5.1)
Inequality (5.1) can be interpreted as an integrated version (with respect to Ï€)
of Hoeï¬€dingâ€™s inequality, for which f(Î», n) â‰Î»2/n. In many cases the loss will
be bounded uniformly over Î¸; then Hoeï¬€dingâ€™s inequality will directly imply (5.1).
The expectation with respect to Ï€ in (5.1) allows us to treat some cases where the
loss is not upper bounded by specifying a prior with suï¬ƒciently light tails.
Deï¬nition 5.2.4 We say that a Bernstein assumption is satisï¬ed for prior Ï€
when there is a function g and an interval I âŠ‚Râˆ—
+ such that, for any Î» âˆˆI,
Ï€
 E exp

Î»[R(Î¸) âˆ’R] âˆ’Î»[rn(Î¸) âˆ’rn]
	
Ï€
 E exp

Î»[rn(Î¸) âˆ’rn] âˆ’Î»[R(Î¸) âˆ’R]
	

â‰¤Ï€
 exp

g(Î», n)[R(Î¸) âˆ’R]

.
(5.2)
This assumption is satisï¬ed for example by sums of i.i.d. sub-exponential ran-
dom variables, see Subsection 2.4 p. 27 in Boucheron et al. [2013], when a mar-
gin assumption on the function R(Â·) is satisï¬ed [Tsybakov, 2004]. This is dis-
cussed in Section 5.4.2.
Again, extensions beyond the i.i.d.
case are possible,
see e.g. Wintenberger [2010] for a survey and new results.
In all these exam-
ples, the important feature of the function g that we will use to derive rates of
convergence is the fact that there is a constant c > 0 such that when Î» = cn,
g(Î», n) = g(cn, n) â‰n.
As mentioned previously, we will often consider rn(Î¸) =
1
n
Pn
i=1 â„“(Yi, fÎ¸(Xi)),
however, the previous assumptions can also be satisï¬ed when rn(Î¸) is a U-statistic,
using Hoeï¬€dingâ€™s decomposition of U-statistics combined with the corresponding
inequality for sums of independent variables [Hoeï¬€ding, 1948]. This idea comes
from ClÂ´emenÂ¸con et al. [2008a] and we will use it in our ranking application.
126

5.3 Numerical approximations of the pseudo-posterior
Remark 5.2.1 We could consider more generally inequalities of the form
Ï€
 E exp

Î»[R(Î¸) âˆ’R] âˆ’Î»[rn(Î¸) âˆ’rn]
	
Ï€
 E exp

Î»[rn(Î¸) âˆ’rn] âˆ’Î»[R(Î¸) âˆ’R]
	

â‰¤Ï€
 exp

g(Î», n)[R(Î¸) âˆ’R]Îº
that allow to use the more general form of the margin assumption of Mammen and Tsybakov
[1999]; Tsybakov [2004]. PAC-Bayes bounds in this context are provided by Catoni
[2007]. However, the techniques involved would require many pages to be described
so we decided to focus on the cases Îº = 0 and Îº = 1 to keep the exposition simple.
5.3 Numerical approximations of the
pseudo-posterior
5.3.1 Monte Carlo
As already explained in the introduction, the usual approach to approximate Ë†ÏÎ» is
MCMC (Markov chain Monte Carlo) sampling. Ridgway [2015] proposed temper-
ing SMC (Sequential Monte Carlo, e.g. Del Moral et al. [2006b]) as an alternative
to MCMC to sample from Gibbs posteriors: one samples sequentially from Ë†ÏÎ»t,
with 0 = Î»0 < Â· Â· Â· < Î»T = Î» where Î» is the desired temperature. One advantage of
this approach is that it makes it possible to contemplate diï¬€erent values of Î», and
choose one by e.g. cross-validation. Another advantage is that such an algorithm
requires little tuning; see Appendix B for more details on the implementation of
tempering SMC. We will use tempering SMC as our gold standard in our numerical
studies.
SMC and related Monte Carlo algorithms tend to be too slow for practical use
in situations where the sample size is large, the dimension of Î˜ is large, or fÎ¸ is ex-
pensive to compute. This motivates the use of fast, deterministic approximations,
such as Variational Bayes, which we describe in the next section.
5.3.2 Variational Bayes
Various versions of VB (Variational Bayes) have appeared in the literature, but the
main idea is as follows. We deï¬ne a family F âŠ‚M1
+(Î˜) of probability distributions
that are considered as tractable. Then, we deï¬ne the VB-approximation of Ë†ÏÎ»: ËœÏÎ».
Deï¬nition 5.3.1 Let
ËœÏÎ» = arg min
ÏâˆˆF K(Ï, Ë†ÏÎ»),
where K(Ï, Ë†ÏÎ») denotes the KL (KÂ¨ullback-Leibler) divergence of Ë†ÏÎ» relative to Ï:
K(m, Âµ) =
Â´
log[ dm
dÂµ ]dm if m â‰ªÂµ (i.e. Âµ dominates m), K(m, Âµ) = +âˆotherwise.
127

5 Properties of variational approximations of Gibbs posteriors
The diï¬ƒculty is to ï¬nd a family F (a) which is large enough, so that ËœÏÎ» may be
close to Ë†ÏÎ», and (b) such that computing ËœÏÎ» is feasible. We now review two types
of families popular in the VB literature.
â€¢ Mean ï¬eld VB: for a certain decomposition Î˜ = Î˜1 Ã— Â· Â· Â· Ã— Î˜d, F is the set
of product probability measures
FMF =
(
Ï âˆˆM1
+(Î˜) : Ï(dÎ¸) =
d
Y
i=1
Ïi(dÎ¸i), âˆ€i âˆˆ{1, . . . , d}, Ïi âˆˆM1
+(Î˜i)
)
.
(5.3)
The inï¬mum of the KL divergence K(Ï, Ë†ÏÎ»), relative to Ï = Q
i Ïi satisï¬es
the following ï¬xed point condition [Bishop, 2006a; Parisi, 1988, Chap. 10]:
âˆ€j âˆˆ{1, Â· Â· Â· , d}
Ïj(dÎ¸j) âˆexp
 Ë†
{âˆ’Î»rn(Î¸) + log Ï€(Î¸)}
Y
iÌ¸=j
Ïi(dÎ¸i)
!
Ï€(dÎ¸j).
(5.4)
This leads to a natural algorithm were we update successively every Ïj until
stabilization.
â€¢ Parametric family:
FP =

Ï âˆˆM1
+(Î˜) : Ï(dÎ¸) = f(Î¸; m)dÎ¸, m âˆˆM
	
;
and M is ï¬nite-dimensional; say FP is the family of Gaussian distributions
(of dimension d). In this case, several methods may be used to compute the
inï¬mum. As above, one may used ï¬xed-point iteration, provided an equa-
tion similar to (5.4) is available. Alternatively, one may directly maximize
Â´
log[exp[âˆ’Î»rn(Î¸)] dÏ€
dÏ(Î¸)]Ï(dÎ¸) with respect to paramater m, using numerical
optimization routines. This approach was used for instance in Hoï¬€man et al.
[2013] with combination of some stochastic gradient descent to perform infer-
ence on a latent Dirichlet allocation model. See also e.g. Emtiyaz Khan et al.
[2013]; Khan [2014] for eï¬ƒcient algorithms for Gaussian variational approx-
imation.
In what follows (Subsections 5.4.1 and 5.4.2) we provide tight bounds for the
prevision risk of ËœÏÎ». This leads to the identiï¬cation of a condition on F such that
the risk of ËœÏÎ» is not worse than the risk of Ë†ÏÎ». We will make this condition explicit
in various examples, using either mean ï¬eld VB or parametric approximations.
Remark 5.3.1 An useful identity, obtained by direct calculations, is: for any
Ï â‰ªÏ€,
log
Ë†
exp [âˆ’Î»rn(Î¸)] Ï€(dÎ¸) = âˆ’Î»
Ë†
rn(Î¸)Ï(dÎ¸) âˆ’K(Ï, Ï€) + K(Ï, Ë†ÏÎ»).
(5.5)
128

5.4 General results
Since the left hand side does not depend on Ï, one sees that ËœÏÎ», which minimises
K(Ï, Ë†ÏÎ») over F, is also the minimiser of:
ËœÏÎ» = arg min
ÏâˆˆF
Ë†
rn(Î¸)Ï(dÎ¸) + 1
Î»K(Ï, Ï€)

This equation will appear frequently in the sequel in the form of an empirical upper
bound.
5.4 General results
This section gives our general results, under either a Hoeï¬€ding Assumption (Deï¬-
nition 5.2.3) or a Bernstein Assumption (Deï¬nition 5.2.4), on risks bounds for the
variational approximation, and how it relates to risks bounds for Gibbs posteri-
ors. These results will be specialised to several learning problems in the following
sections.
5.4.1 Bounds under the Hoeï¬€ding assumption
Empirical bounds
Theorem 5.4.1 Under the Hoeï¬€ding assumption (Deï¬nition 5.2.3), for any Îµ >
0, with probability at least 1 âˆ’Îµ we have simultaneously for any Ï âˆˆM1
+(Î˜),
Ë†
RdÏ â‰¤
Ë†
rndÏ + f(Î», n) + K(Ï, Ï€) + log
  1
Îµ

Î»
.
This result is a simple variant of a result in Catoni [2007] but for the sake of
completeness, its proof is given in Appendix 5.A. It gives us an upper bound on the
risk of both the pseudo-posterior (take Ï = Ë†ÏÎ») and its variational approximation
(take Ï = ËœÏÎ»). These bounds may be be computed from the data, and therefore
provide a simple way to evaluate the performance of the corresponding proce-
dure, in the spirit of the ï¬rst PAC-Bayesian inequalities [McAllester, 1999, 1998;
Shawe-Taylor and Williamson, 1997]. However, this bound do not provide the rate
of convergence of these estimators. For this reason, we also provide oracle-type
inequalities.
Oracle-type inequalities
Another way to use PAC-Bayesian bounds is to compare
Â´
RdË†ÏÎ» to the best possi-
ble risk, thus linking this approach to oracle inequalities. This is the point of view
developed in Catoni [2004, 2007]; Dalalyan and Tsybakov [2008].
129

5 Properties of variational approximations of Gibbs posteriors
Theorem 5.4.2 Assume that the Hoeï¬€ding assumption is satisï¬ed (Deï¬nition 5.2.3).
For any Îµ > 0, with probability at least 1 âˆ’Îµ we have simultaneously
Ë†
RdË†ÏÎ» â‰¤BÎ»(M1
+(Î˜)) :=
inf
ÏâˆˆM1
+(Î˜)
(Ë†
RdÏ + 2f(Î», n) + K(Ï, Ï€) + log
  2
Îµ

Î»
)
and
Ë†
RdËœÏÎ» â‰¤BÎ»(F) := inf
ÏâˆˆF
(Ë†
RdÏ + 2f(Î», n) + K(Ï, Ï€) + log
  2
Îµ

Î»
)
.
Moreover,
BÎ»(F) = BÎ»(M1
+(Î˜)) + 2
Î» inf
ÏâˆˆF K(Ï, Ï€ Î»
2 )
where we remind that Ï€Î» is deï¬ned in Deï¬nition 5.2.2.
In this way, we are able to compare
Â´
RdË†ÏÎ» to the best possible aggregation
procedure in M1
+(Î˜) and
Â´
RdËœÏÎ» to the best aggregation procedure in F. More
importantly, we are able to obtain explicit expressions for the right-hand side of
these inequalities in various models, and thus to obtain rates of convergence. This
will be done in the remaining sections. This leads to the second interest of this
result: if there is a Î» = Î»(n) that leads to BÎ»(M1
+(Î˜)) â‰¤R + sn with sn â†’0 for
the pseudo-posterior Ë†ÏÎ», then we only have to prove that there is a Ï âˆˆF such that
K(Ï, Ï€Î»)/Î» â‰¤csn for some constant c > 0 to ensure that the VB approximation
ËœÏÎ» also reaches the rate sn.
We will see in the following sections several examples where the approximation
does not deteriorate the rate of convergence. But ï¬rst let us show the equivalent
oracle inequality under the Bernstein assumption.
5.4.2 Bounds under the Bernstein assumption
In this context the empirical bound on the risk would depend on the minimal
achievable risk Â¯rn, and cannot be computed explicitly. We give the oracle inequality
for both the Gibbs posterior and its VB approximation in the following theorem.
Theorem 5.4.3 Assume that the Bernstein assumption is satisï¬ed (Deï¬nition 5.2.4).
Assume that Î» > 0 satisï¬es Î» âˆ’g(Î», n) > 0. Then for any Îµ > 0, with probability
at least 1 âˆ’Îµ we have simultaneously:
Ë†
RdË†ÏÎ» âˆ’R â‰¤BÎ»
 M1
+(Î˜)

,
130

5.5 Application to classiï¬cation
Ë†
RdËœÏÎ» âˆ’R â‰¤BÎ»(F),
where, for either A = M1
+(Î˜) or A = F,
BÎ»(A) =
1
Î» âˆ’g(Î», n) inf
ÏâˆˆA
(
[Î» + g(Î», n)]
Ë†
(R âˆ’R)dÏ + 2K(Ï, Ï€) + 2 log
2
Îµ
)
.
In addition,
BÎ»(F) = BÎ»
 M1
+(Î˜)

+
2
Î» âˆ’g(Î», n) inf
ÏâˆˆF K

Ï, Ï€ Î»+g(Î»,n)
2

.
The main diï¬€erence with Theorem 5.4.2 is that the function R(Â·) is replaced by
R(Â·) âˆ’R. This is well known way to obtain better rates of convergence.
5.5 Application to classiï¬cation
5.5.1 Preliminaries
In all this section, we assume that Y = {0, 1} and we consider linear classiï¬cation:
Î˜ = X = Rd, fÎ¸(x) = 1âŸ¨Î¸,xâŸ©â‰¥0. We put rn(Î¸) = 1
n
Pn
i=1 1{fÎ¸(Xi)Ì¸=Yi}, R(Î¸) = P(Y Ì¸=
fÎ¸(X)) and assume that the [(Xi, Yi)]n
i=1 are i.i.d. In this setting, it is well-known
that the Hoeï¬€ding assumption always holds. We state as a reminder the following
lemma.
Lemma 5.5.1 Hoeï¬€ding assumption (5.1) is satisï¬ed with f(Î», n) = Î»2/(2n).
The proof is given in Appendix 5.A for the sake of completeness.
It is also possible to prove that Bernstein assumption (5.2) holds in the case
where the so-called margin assumption of Mammen and Tsybakov is satisï¬ed.
This condition we use was introduced by Tsybakov [2004] in a classiï¬cation setting,
based on a related deï¬nition in Mammen and Tsybakov [1999].
Lemma 5.5.2 Assume that Mammen and Tsybakovâ€™s margin assumption is sat-
isï¬ed: i.e. there is a constant C such that
E[(1fÎ¸(X)Ì¸=Y âˆ’1fÎ¸(X)Ì¸=Y )2] â‰¤C[R(Î¸) âˆ’R].
Then Bernstein assumption (5.2) is satisï¬ed with g(Î», n) =
CÎ»2
2nâˆ’Î».
131

5 Properties of variational approximations of Gibbs posteriors
Remark 5.5.1 We refer the reader to Tsybakov [2004] for a proof that
P(0 < |

Î¸, X

| â‰¤t) â‰¤Câ€²t
for some constant Câ€² > 0 implies the margin assumption. In words, when X is
not likely to be in the region

Î¸, X

â‰ƒ0, where points are hard to classify, then
the problem becomes easier and the classiï¬cation rate can be improved.
We propose in this context a Gaussian prior: Ï€ = Nd(0, Ï‘2Id), and we consider a
VB approach based on Gaussian families. The corresponding optimization problem
is not convex, but remains feasible as we explain below.
5.5.2 Three sets of Variational Gaussian approximations
Consider the three following Gaussian families
F1 =

Î¦m,Ïƒ2, m âˆˆRd, Ïƒ2 âˆˆRâˆ—
+
	
,
F2 =

Î¦m,Ïƒ2, m âˆˆRd, Ïƒ2 âˆˆ(Râˆ—
+)2	
(mean ï¬eld approximation),
F3 =

Î¦m,Î£, m âˆˆRd, Î£ âˆˆSd+	
(full covariance approximation),
where Î¦m,Ïƒ2 is Gaussian distribution Nd(m, Ïƒ2Id), Î¦m,Ïƒ2 is Nd(m, diag(Ïƒ2)), and
Î¦m,Î£ is Nd(m, Î£). Obviously, F1 âŠ‚F2 âŠ‚F3 âŠ‚M1
+(Î˜), and
BÎ»(M1
+(Î˜)) â‰¤BÎ»(F3) â‰¤BÎ»(F2) â‰¤BÎ»(F1).
(5.6)
Note that, for the sake of simplicity, we will use the following classical notations
in the rest of the chapter: Ï•(Â·) is the density of N(0, 1) w.r.t.
the Lebesgue
measure, and Î¦(Â·) the corresponding c.d.f. The rest of Section 5.5 is organized as
follows. In Subsection 5.5.3, we calculate explicitly BÎ»(F2) and BÎ»(F1). Thanks
to (5.6) this also gives an upper bound on BÎ»(F3) and proves the validity of the
three types of Gaussian approximations.
Then, we give details on algorithms
to compute the variational approximation based on F2 and F3, and provide a
numerical illustration on real data.
5.5.3 Theoretical analysis
We start with the empirical bound for F2 (and F1 as a consequence), which is a
direct corollary of Theorem 5.4.1.
Corollary 5.5.3 For any Îµ > 0, with probability at least 1 âˆ’Îµ we have, for any
m âˆˆRd, Ïƒ2 âˆˆ(R+)d,
Ë†
RdÎ¦m,Ïƒ2 â‰¤
Ë†
rndÎ¦m,Ïƒ2 + Î»
2n +
Pd
i=1
h
1
2 log

Ï‘2
Ïƒ2
i

+ Ïƒ2
i
Ï‘2
i
+ âˆ¥mâˆ¥2
Ï‘2
âˆ’d
2 + log
  1
Îµ

Î»
.
132

5.5 Application to classiï¬cation
We now want to apply Theorem 5.4.2 in this context. In order to do so, we
introduce an additional assumption.
Deï¬nition 5.5.1 We say that Assumption A1 is satisï¬ed when there is a constant
c > 0 such that, for any (Î¸, Î¸â€²) âˆˆÎ˜2 with âˆ¥Î¸âˆ¥= âˆ¥Î¸â€²âˆ¥= 1, P(âŸ¨X, Î¸âŸ©âŸ¨X, Î¸â€²âŸ©< 0) â‰¤
câˆ¥Î¸ âˆ’Î¸â€²âˆ¥.
Note that this is not a stringent assumption. For example, it is satisï¬ed as soon
as X/âˆ¥Xâˆ¥has a bounded density on the unit sphere.
Corollary 5.5.4 Assume that the VB approximation is done on either F1, F2 or
F3. Take Î» =
âˆš
nd and Ï‘ =
1
âˆš
d. Under Assumption A1, for any Îµ > 0, with
probability at least 1 âˆ’Îµ we have simultaneously
Â´
RdË†ÏÎ»
Â´
RdËœÏÎ»

â‰¤R +
r
d
n log
 4ne2
+
c
âˆšn +
r
d
4n3 + 2 log
  2
Îµ

âˆš
nd
.
See the appendix for a proof. Note also that the values Î» =
âˆš
nd and Ï‘ =
1
âˆš
d allow
to derive this almost optimal rate of convergence, but are not necessarily the best
choices in practice.
Remark 5.5.2 Note that Assumption A1 is not necessary to obtain oracle inequal-
ities on the risk integrated under Ë†ÏÎ». We refer the reader to Chapter 1 in Catoni
[2007] for such assumption-free bounds. However, it is clear that without this as-
sumption the shape of Ë†ÏÎ» and ËœÏÎ» might be very diï¬€erent. Thus, it seems reasonable
to require that A1 is satisï¬ed for the approximation of Ë†ÏÎ» by ËœÏÎ» to make sense.
We ï¬nally provide an application of Theorem 5.4.3. Under the additional con-
straint that the margin assumption is satisï¬ed, we obtain a better rate.
Corollary 5.5.5 Assume that the VB approximation is done on either F1, F2
or F3. Under Assumption A1 (Deï¬nition 5.5.1 page 133), and under Mammen
and Tsybakov margin assumption, with Î» =
2n
C+2 and Ï‘ > 0, for any Îµ > 0, with
probability at least 1 âˆ’Îµ,
Â´
RdË†ÏÎ»
Â´
RdËœÏÎ»

â‰¤Â¯R+(C + 2)(C + 1)
2
d log n
Ï‘
n
+ 2dÏ‘
n2 + 2
Ï‘ âˆ’d
Ï‘n + 2
n log 2
Îµ

+
âˆš
d2c(2C + 1)
n
.
The prior variance optimizing the bound is Ï‘ = d/(d + 2 + 2d/n), this choice
or any constant instead will lead to a rate in d log(n)/n. Note that the rate d/n
is minimax-optimal in this context. This is, for example, a consequence of more
general results in LecuÂ´e [2007] under a general form of the the margin assumption.
See the Appendix for a proof.
133

5 Properties of variational approximations of Gibbs posteriors
5.5.4 Implementation and numerical results
For family F2 (mean ï¬eld), the variational lower bound (5.5) equals
LÎ»,Ï‘(m, Ïƒ) = âˆ’Î»
n
n
X
i=1
Î¦
 
âˆ’Yi
Xim
p
Xidiag(Ïƒ2)Xt
i
!
âˆ’mTm
2Ï‘
+ 1
2
d
X
k=1

log Ïƒ2
k âˆ’Ïƒ2
k
Ï‘

,
while for family F3 (full covariance), it equals
LÎ»,Ï‘(m, Î£) = âˆ’Î»
n
n
X
i=1
Î¦
 
âˆ’Yi
Xim
p
XiÎ£Xt
i
!
âˆ’mTm
2Ï‘
+ 1
2

log |Î£| âˆ’1
Ï‘trÎ£

.
Both functions are non-convex, but the multimodality of the latter may be more
severe due to the larger dimension of F3. To address this issue, we recommend
to use the reparametrisation of Opper and Archambeau [2009], which makes the
dimension of the latter optimisation problem O(n); see Khan [2014] for a related
approach.
In both cases, we found that deterministic annealing to be a good
approach to optimise such non-convex functions.
We refer to Appendix B for
more details on deterministic annealing and on our particular implementation.
We now compare the numerical performance of the mean ï¬eld and full covariance
VB approximations to the Gibbs posterior (as approximated by SMC, see Section
5.3.1) for the classiï¬cation of standard datasets; see Table 5.1. We also include
results for a kernel SVM (support vector machine); this comparison is not entirely
fair, since SVM is a non-linear classiï¬er, while all the other classiï¬ers are linear.
Still, except for the Glass dataset, the full covariance VB approximation performs
as well or better than both SMC and SVM (while being much faster to compute,
especially compared to SMC).
Interestingly, VB outperforms SMC in certain cases. This might be due to the
fact that a VB approximation tends to be more concentrated around the mode
than the Gibbs posterior it approximates. Mean ï¬eld VB does not perform so well
on certain datasets (e.g. Indian). This may be due either to the approximation
family being too small, or to the corresponding optmisation problem being strongly
multi-modal.
5.6 Application to classiï¬cation under convexiï¬ed
loss
Compared to the previous section, the advantage of convex classiï¬cation is that
the corresponding variational approximation will amount to minimising a convex
134

5.6 Application to classiï¬cation under convexiï¬ed loss
Dataset
Covariates
Mean Field (F2)
Full cov. (F3)
SMC
SVM
Pima
7
31.0
21.3
22.3
30.4
Credit
60
32.0
33.6
32.0
32.0
DNA
180
23.6
23.6
23.6
20.4
SPECTF
22
08.0
06.9
08.5
10.1
Glass
10
34.6
19.6
23.3
4.7
Indian
11
48.0
25.5
26.2
26.8
Breast
10
35.1
1.1
1.1
1.7
Table 5.1: Comparison of misclassiï¬cation rates (%).
Misclassiï¬cation rates for diï¬€erent datasets and for the proposed approximations of
the Gibbs posterior. The last column is the missclassiï¬cation rate given by a kernel-
SVM with radial kernel. The hyper-parameters are chosen by cross-validation.
function. This means that (a) the minimisation problem will be easier to deal
with; and (b) we will be able to compute a bound for the integrated risk after a
given number of steps of the minimisation procedure.
The setting is the same as in the previous section, except that for convenience
we now take Y = {âˆ’1, 1}, and the risk is based on the hinge loss,
rH
n (Î¸) = 1
n
n
X
i=1
max(0, 1 âˆ’Yi < Î¸, Xi >).
We will write RH for the theoretical counterpart and Â¯RH for its minimum in Î¸.
We keep the superscript H in order to allow comparison with the risk R under
the 0 âˆ’1 loss. We assume in this section that the Xi are uniformly bounded by a
constant, |Xi| < cx. Note that we do not require an assumption of the form (A1)
to obtain the results of this section, as we rely directly on the Lipschitz continuity
of the hinge risk.
5.6.1 Theoretical Results
Contrarily to the previous section, the risk is not bounded in Î¸, and we must
specify a prior distribution for the Hoeï¬€ding assumption to hold.
Lemma 5.6.1 Under an independent Gaussian prior Ï€ such that each component
is N(0, Ï‘2), and for Î» <
1
cx
p n
Ï‘2 and with bounded design |Xij| < cx, Hoeï¬€ding
assumption (5.1) is satisï¬ed with f(Î», n) = Î»2/(4n) âˆ’1
2 log

1 âˆ’Ï‘2Î»2c2
x
2n

.
135

5 Properties of variational approximations of Gibbs posteriors
The main impact of such a bound is that the prior variance cannot be taken too
big relative to Î».
Corollary 5.6.2 Assume that the VB approximation is done on either F1, F2 or
F3. Take Î» =
1
cx
p n
Ï‘2 and Ï‘ =
1
âˆš
d. For any Îµ > 0, with probability at least 1 âˆ’Îµ
we have simultaneously
Â´
RHdË†ÏÎ»
Â´
RHdËœÏÎ»

â‰¤R
H + cx
2
r
d
n log n
d + 2cx
d
n +
1
âˆš
nd
c2
x + 1
2cx
+ 2cx log 2
Ç«

The oracle inequality in the above corollary enjoys the same rate of convergence
as the equivalent result in the preceding section. In the following we link the two
results.
Remark 5.6.1 As stated in the beginning of the section we can use the estimator
speciï¬ed under the hinge loss to bound the excess risk of the 0-1 loss. We write Râ‹†
and RHâ‹†the respective risk for their corresponding Bayes classiï¬ers. From Zhang
[2004] (section 3.3) we have the following inequality, linking the excess risk under
the hinge loss and the 0 âˆ’1 loss,
R(Î¸) âˆ’Râ‹†â‰¤RH(Î¸) âˆ’RHâ‹†
for every Î¸ âˆˆRd. By integrating with respect to ËœÏH (the VB approximation on
any F1, F2, F3 of the Gibbs posterior for the hinge risk) and making use of Corol-
lary 5.6.2 we have with high probability,
ËœÏH (R(Î¸)) âˆ’Râ‹†â‰¤inf
Î¸âˆˆRp RH(Î¸) âˆ’RHâ‹†+ O
 r
d
n log
n
d
!
.
5.6.2 Numerical application
We have motivated the introduction of the hinge loss as a convex upper bound.
In the sequel we show that the resulting VB approximation also leads to a convex
optimization problem. This has the advantage of opening a range of possible opti-
mization algorithms [Nesterov, 2004]. In addition we are able to bound the error of
the approximated measure after a ï¬xed number of iterations (see Theorem 5.6.3).
Under the model F1 each individual risk is given by:
Ïm,Ïƒ(ri(Î¸)) = (1 âˆ’Î“im) Î¦
1 âˆ’Î“im
Ïƒâˆ¥Î“iâˆ¥2

+ Ïƒâˆ¥Î“iâˆ¥Ï•
1 âˆ’Î“im
Ïƒâˆ¥Î“iâˆ¥2

:= Îi
 m
Ïƒ

,
writting Î“i := YiXi.
136

5.6 Application to classiï¬cation under convexiï¬ed loss
Hence the lower bound to be maximized is given by
L(m, Ïƒ) = âˆ’Î»
n
( n
X
i=1
(1 âˆ’Î“im) Î¦
1 âˆ’Î“im
Ïƒâˆ¥Î“iâˆ¥2

+
n
X
i=1
Ïƒâˆ¥Î“iâˆ¥Ï•
1 âˆ’Î“im
Ïƒâˆ¥Î“iâˆ¥2
)
âˆ’âˆ¥mâˆ¥2
2
2Ï‘
+ d
2

log Ïƒ2 âˆ’Ï‘
Ïƒ2

.
It is easy to see that the function is convex in (m, Ïƒ), ï¬rst note that the map
Î¨ :
 x
y

7â†’xÎ¦
x
y

+ yÏ•
x
y

,
is convex and note that we can write Îi
 m
Ïƒ

= Î¨

A
 x
y

+ b

hence by
composition of convex function with linear mappings we have the result. Similar
reasoning could be held for the case F2 and F3, where in later the parametrization
should be done in C such that Î£ = CCt. The bound is however not universally
Lipschitz in Ïƒ, this impacts the optimization algorithms.
On the class of function F0 =
n
Î¦m, 1
n, m âˆˆRdo
, for which our Oracle inequalities
still hold we could get faster numerical algorithms. The objective function has
Lipschitz continuous derivatives and we would get a rate of
L
(1+k)2.
Other convex loss could be considered which could lead to convex optimization
problems. For instance one could consider the exponential loss.
Dataset
Covariates
Hinge loss
SMC
Pima
7
21.8
22.3
Credit
60
27.2
32.0
DNA
180
4.2
23.6
SPECTF
22
19.2
08.5
Glass
10
26.12
23.3
Indian
11
26.2
25.5
Breast
10
0.5
1.1
Table 5.2: Comparison of misclassiï¬cation rates (%).
Misclassiï¬cation rates for diï¬€erent datasets and for the proposed approximations of
the Gibbs posterior. The hyperparameters are chosen by cross-validation. This is to
be compared to Table 5.1.
137

5 Properties of variational approximations of Gibbs posteriors
Theorem 5.6.3 Assume that the VB approximation is done on F1, F2 or F3. De-
note by ËœÏk(dÎ¸) the VB approximated measure after the kth iteration of an optimal
convex solver using the hinge loss. Take Î» =
1
cx
p n
d and Ï‘ =
1
âˆš
d then under the
hypothesis of Corollary 5.6.2 with probability 1 âˆ’Ç«
Ë†
RHdËœÏk â‰¤R
H +
LM
âˆš
1 + k + cx
2
r
d
n log n
d + 2cx
d
n +
1
âˆš
nd
c2
x + 1
2cx
+ 2cx log 2
Ç«

where L is the Lipschitz coeï¬ƒcient on a ball of radius M of the objective function
maximized in VB.
From Theorem 5.6.3 we can compute the number of iterations to get a given
level of error at a given probability.
We ï¬nd that on average the misclassiï¬cation error (Table 5.2) is lower than for
the 0-1 loss where we have no guaranties that the maximum is attained.
5.7 Application to ranking
5.7.1 Preliminaries
In this section we take Y = {0, 1} and consider again linear classiï¬ers: Î˜ = X =
Rd, fÎ¸(x) = 1âŸ¨Î¸,xâŸ©â‰¥0. We consider however a diï¬€erent criterion: in ranking, not
only we want to classify well an object x, but we want to make sure that given
two diï¬€erent objects, the one that is more likely to correspond to a label 1 will be
assigned a larger score through the function fÎ¸. A usual way to measure this is to
introduce the risk function
R(Î¸) = P[(Y1 âˆ’Y2)(fÎ¸(X1) âˆ’fÎ¸(X2)) < 0]
and the empirical risk
rn(Î¸) =
1
n(n âˆ’1)
X
1â‰¤iÌ¸=jâ‰¤n
1{(Yiâˆ’Yj)(fÎ¸(Xi)âˆ’fÎ¸(Xj))<0}.
Then, again, we recall classical results.
Lemma 5.7.1 The Hoeï¬€ding-type assumption is satisï¬ed with f(Î», n) =
Î»2
nâˆ’1.
The variant of the margin assumption adapted to ranking was established by Robbiano
[2013] and Ridgway [2015].
Lemma 5.7.2 Assume the following margin assumption:
E[(1[fÎ¸(X1)âˆ’fÎ¸(X2)][Y1âˆ’Y2]<0 âˆ’1[fÎ¸(X1)âˆ’fÎ¸(X2)][Y1âˆ’Y2]<0)2] â‰¤C[R(Î¸) âˆ’R].
Then Bernstein assumption (5.2) is satisï¬ed with g(Î», n) =
CÎ»2
nâˆ’1âˆ’4Î».
138

5.7 Application to ranking
We still consider a Gaussian prior
Ï€(dÎ¸) =
d
Y
i=1
Ï•(Î¸i; 0, Ï‘2)dÎ¸i
and the approximation families will be the same as in Section 5.5: F1 = {Î¦m,Ïƒ2, m âˆˆ
Rd, Ïƒ2 âˆˆRâˆ—
+}, F2 = {Î¦m,Ïƒ2, m âˆˆRd, Ïƒ2 âˆˆ(Râˆ—
+)d} and F3 = {Î¦m,Î£, m âˆˆRd, Î£ âˆˆ
Sd+}.
5.7.2 Theoretical study
Here again, we start with the empirical bound.
Corollary 5.7.3 For any Îµ > 0, with probability at least 1 âˆ’Îµ we have, for any
m âˆˆRd, Ïƒ2 âˆˆ(R+)d,
Ë†
RdÎ¦m,Ïƒ2 â‰¤
Ë†
rndÎ¦m,Ïƒ2+
Î»
n âˆ’1+
Pd
j=1
h
1
2 log

Ï‘2
Ïƒ2
i

+ Ïƒ2
i
Ï‘2
i
+ âˆ¥mâˆ¥2
Ï‘2
âˆ’d
2 + log
  1
Îµ

Î»
.
In order to derive a theoretical bound, we introduce the following variant of
Assumption A1.
Deï¬nition 5.7.1 We say that Assumption A2 is satisï¬ed when there is a constant
c > 0 such that, for any (Î¸, Î¸â€²) âˆˆÎ˜2 with âˆ¥Î¸âˆ¥= âˆ¥Î¸â€²âˆ¥= 1, P(âŸ¨X1 âˆ’X2, Î¸âŸ©âŸ¨X1 âˆ’X2, Î¸â€²âŸ©<
0) â‰¤câˆ¥Î¸ âˆ’Î¸â€²âˆ¥.
Assumption A2 is satisï¬ed as soon as (X1 âˆ’X2)/âˆ¥X1 âˆ’X2âˆ¥has a bounded density
on the unit sphere.
Corollary 5.7.4 Use either F1, F2 or F3. Take Î» =
q
d(nâˆ’1)
2
and Ï‘ = 1. Under
(A2), for any Îµ > 0, with probability at least 1 âˆ’Îµ,
Â´
RdË†ÏÎ»
Â´
RdËœÏÎ»

â‰¤R +
r
2d
n âˆ’1

1 + 1
2 log (2d(n âˆ’1))

+
c
âˆš
2
âˆšn âˆ’1 + 2
âˆš
2 log
  2e
Îµ

p
(n âˆ’1)d
.
Finally, under an additional margin assumption, we have:
Corollary 5.7.5 Under Assumption A2 and the margin assumption of Lemma (5.7.2),
for Î» = nâˆ’1
C+5 and Ï‘ > 0, for any Îµ > 0, with probability at least 1 âˆ’Îµ,
Â´
RdË†ÏÎ»
Â´
RdËœÏÎ»

â‰¤Â¯R+(C + 5)(C + 1)
2
d log n
Ï‘
n âˆ’1 +
2dÏ‘
n(n âˆ’1) + 2
Ï‘ âˆ’
d
Ï‘n âˆ’1 +
2
n âˆ’1 log 2
Îµ

+
âˆš
d4c(C + 1)
n
.
139

5 Properties of variational approximations of Gibbs posteriors
The prior variance optimizing the bound is Ï‘ = d/(d + 2 + 2d/n). The proof is
similar to the ones of Corollaries 5.5.4, 5.5.5 and 5.7.4.
As in the case of classiï¬cation, ranking under an AUC loss can be done by re-
placing the indicator function by the corresponding upper bound given by an hinge
loss. In this case we can derive similar results as for the convexiï¬ed classiï¬cation
in particular we can get a convex minimization problem and obtain result without
requiring assumption (A2).
5.7.3 Algorithms and numerical results
As an illustration we focus here on family F2 (mean ï¬eld). In this case the VB
objective to maximize is given by:
L(m, Ïƒ2) = âˆ’
Î»
n+nâˆ’
X
i:yi=1,j:yj=0
Î¦
ï£«
ï£­âˆ’
Î“ijm
qPd
k=1(Î³k
ij)2Ïƒ2
k
ï£¶
ï£¸âˆ’âˆ¥mâˆ¥2
2
2Ï‘ +1
2
d
X
k=1

log Ïƒ2
k âˆ’Ïƒ2
k
Ï‘

,
(5.7)
where Î“ij = Xi âˆ’Xj, and where (Î³k
ij)k are the elements of Î“.
This function is expensive to compute, as it involves n+nâˆ’terms, the computa-
tion of which is O(p).
We propose to use a stochastic gradient descent in the spirit of Hoï¬€man et al.
[2013]. The model we consider is not in an exponential family, meaning we cannot
use the trick developed by these authors. We propose instead to use a standard
descent.
The idea is to replace the gradient by an unbiased version based on a batch of
size B as described in Algorithm 22 in the Appendix. Robbins and Monro [1951]
show that for a step-size (Î»t)t such that P
t Î»2
t < âˆand P
t Î»t = âˆthe algorithm
converges to a local optimum.
In our case we propose to sample pairs of data with replacement and use the
unbiased version of the derivative of the risk component. We use a simple gradient
descent without any curvature information. One could also use recent research on
stochastic quasi Newton-Raphson [Byrd et al., 2014].
For illustration, we consider a small dataset (Pima), and a larger one (Adult).
The latter is already quite challenging with n+nâˆ’= 193, 829, 520 pairs to compare.
In both cases with diï¬€erent size of batches convergence is obtained with a few
iterations only and leads to acceptable bounds.
In Figure 5.1 we show the empirical bound on the AUC risk as a function of
the iteration of the algorithm, for several batch sizes. The bound is taken for
95% probability, the batch sizes are taken to be B = 1, 10, 20, 50 for the Pima
dataset, and 50 for the Adult dataset. The ï¬gure shows an additional feature of
VB approximation in the context of Gibbs posterior: namely the possibility of
140

5.8 Application to matrix completion
0
1
2
3
0
25
50
75
100
Iterations
Emprical Bound 95%
(a) Pima
1
2
3
0
100
200
300
Iterations
Emprical Bound 95%
(b) adult
Figure 5.1: Error bound at each iteration, stochastic descent, Pima
and Adult datasets.
Stochastic VB with ï¬xed temperature Î» = 100 for Pima and Î» = 1000 for adult. The left panel
shows several curves that correspond to diï¬€erent batch sizes; these curves are hard to distinguish.
The right panel is for a batch size of 50.
The adult dataset has n = 32556 observation and
n+nâˆ’= 193829520 possible pairs. The convergence is obtained in order of seconds. The bounds
are the empirical bounds obtained in Corollary 5.7.3 for a probability of 95%.
computing the empirical upper bound given by Corollary 5.7.3. That is we can
check the quality of the bound at each iteration of the algorithm, or for diï¬€erent
values of the hyperparameters.
5.8 Application to matrix completion
The matrix completion problem has received increasing attention recently, partly
due to spectacular theoretical results [Cand`es and Tao, 2010], and to challenging
applications like the Netï¬‚ix challenge [Bennett and Lanning, 2007]. In the per-
spective of this chapter, the speciï¬c interest of this application is twofold. First,
this is a case where the family of approximations is not parametric, but rather of
the form (5.3), i.e. the family of products of independent components. Then, there
is no known theoretical result for the Gibbs estimator in the considered model, yet
we can still directly bound the loss induced by the variational approximation.
141

5 Properties of variational approximations of Gibbs posteriors
We observe i.i.d. pairs ((Xi, Yi))n
i=1 where Xi âˆˆ{1, . . . , m1} Ã— {1, . . . , m2}, and
we assume that there is a m1 Ã— m2-matrix M such that Yi = MXi + Îµi and the
Îµi are centred. Assuming that Xi is uniform on {1, . . . , m1} Ã— {1, . . . , m2}, that
fÎ¸(Xi) = Î¸Xi, and taking the quadratic risk, R(Î¸) = E [(Yi âˆ’Î¸Xi)2], we have that
R(Î¸) âˆ’R =
1
m1m2
âˆ¥Î¸ âˆ’Mâˆ¥2
F
where âˆ¥Â· âˆ¥F stands for the Frobenius norm.
A common way to parametrise the problem is
Î˜ = {Î¸ = UV T, U âˆˆRm1Ã—K, V âˆˆRm2Ã—K}
where K is large; e.g.
K = min(m1, m2).
Following Salakhutdinov and Mnih
[2008], we deï¬ne the following prior distribution: UÂ·,j âˆ¼N(0, Î³jI), VÂ·,j âˆ¼N(0, Î³jI)
where the Î³jâ€™s are i.i.d. from an inverse gamma distribution, Î³j âˆ¼IÎ“(a, b).
Note that VB algorithms were used in this context by Lim and Teh [2007] (with a
slightly simpler prior however: the Î³jâ€™s are ï¬xed rather than random). Since then,
this prior and variants were used in several papers [e.g. Lawrence and Urtasun,
2009; Zhou et al., 2010]. Until now, no theoretical results were proved up to our
knowledge. Two papers prove minimax-optimal rates for slightly modiï¬ed estima-
tors (by truncation), for which eï¬ƒcient algorithms are unknown [Mai and Alquier,
2015; Suzuki, 2014]. However, using Theorems 5.4.2 and 5.4.3 we are able to prove
the following: if there is a PAC-Bayesian bound leading to a rate for Ë†ÏÎ» in this
context, then the same rate holds for ËœÏÎ». In other words: if someone proves the
conjecture that the Gibbs estimator is minimax-optimal (up to log terms) in this
context, then the VB approximation will enjoy automatically the same property.
We propose the following approximation:
F =
(
Ï(d(U, V )) =
m1
Y
i=1
ui(dUi,Â·)
m2
Y
j=1
vj(dVj,Â·)
)
.
Theorem 5.8.1 Assume that M = UV T with |Ui,k|, |Vj,k| â‰¤C.
Assume that
rank(M) = r so that we can assume that UÂ·,r+1 = Â· Â· Â· = UÂ·,K = VÂ·,r+1 = Â· Â· Â· =
VÂ·,K = 0 (note that the prior Ï€ does not depend on the knowledge of r though).
Choose the prior distribution on the hyper-parameters Î³j as inverse gamma Invâˆ’Î“(a, b)
with b â‰¤1/[2Î²(m1 âˆ¨m2) log(2K(m1 âˆ¨m2))]. Then there is a constant C(a, C) such
that, for any Î² > 0,
inf
ÏâˆˆF K(Ï, Ï€Î²) â‰¤C(a, C)

r(m1 + m2) log [Î²b(m1 + m2)K] + 1
Î²

.
142

5.9 Discussion
See the Appendix for a proof.
For instance, in Theorem 5.4.3, in classiï¬cation and ranking we had Î», Î»âˆ’g(Î», n)
and Î» + g(Î», n) of order O(n). In this case we would have:
2
Î» âˆ’g(Î», n) inf
ÏâˆˆF K

Ï, Ï€ Î»+g(Î»,n)
2

= O
C(a, C)r(m1 + m2) log [nb(m1 + m2)K]
n

,
and note that in this context it is known that the minimax rate is at least r(m1 +
m2)/n [Koltchinskii et al., 2011].
5.8.1 Algorithm
As already mentioned, the approximation family is not parametric in this case, but
rather of type mean ï¬eld. The corresponding VB algorithm amounts to iterating
equation (5.4), which takes the following form in this particular case:
uj(dUj,.) âˆexp
(
âˆ’Î»
n
X
i
EV,Uâˆ’j

(YXi âˆ’(UV T)Xi)2
âˆ’
K
X
k=1
EÎ³j
 1
2Î³k

U 2
jk
)
vj(dVj,.) âˆexp
(
âˆ’Î»
n
X
i
EVâˆ’j,U

(YXi âˆ’(UV T)Xi)2
âˆ’
K
X
k=1
EÎ³j
 1
2Î³k

V 2
jk
)
p(Î³k) âˆexp
(
âˆ’1
2Î³k
 X
j
EUU 2
kj +
X
i
EV V 2
ik
!
+ (Î± + 1) log 1
Î³k
âˆ’Î²
Î³k
)
where the expectations are taken with respect to the thus deï¬ned variational ap-
proximations.
One recognises Gaussian distributions for the ï¬rst two, and an
inverse Gamma distribution for the third. We refer to Lim and Teh [2007] for
more details on this algorithm and for a numerical illustration.
5.9 Discussion
We showed in several important scenarios that approximating a Gibbs posterior
through VB (Variational Bayes) techniques does not deteriorate the rate of conver-
gence of the corresponding procedure. We also described practical algorithms for
fast computation of these VB approximations, and provided empirical bounds that
may be computed from the data to evaluate the performance of the so-obtained
VB-approximated procedure. We believe these results provide a strong incentive
to recommend VB as the default approach to approximate Gibbs posteriors, in
lieu of Monte Carlo methods.
We hope to extend our results to other applications beyond those discussed in
this chapter, such as regression. One technical diï¬ƒculty with regression is that
143

the risk function is not bounded, which makes our approach a bit less direct to
apply. In many papers on PAC-Bayesian bounds for regression, the noise can be
unbounded (usually, it is assumed to be sub-exponential), but one assumes that
the predictors are bounded, see e.g. Alquier and Biau [2013]. However, using the
robust loss function of Audibert and Catoni, it is possible to relax this assumption
[Audibert and Catoni, 2011; Catoni, 2012]. This requires a more technical analysis,
which we leave for further work.

Appendix
5.A Proofs
5.A.1 Preliminary remarks
We start by a general remark. Let h be a function Î˜ â†’R+ with
Â´
exp[âˆ’h(Î¸)]Ï€(dÎ¸) <
âˆ. Let us put
Ï€[h](dÎ¸) =
exp[âˆ’h(Î¸)]
Â´
exp[âˆ’h(Î¸â€²)]Ï€(dÎ¸â€²)Ï€(dÎ¸).
Direct calculation yields, for any Ï â‰ªÏ€ with
Â´
hdÏ < âˆ,
K(Ï, Ï€[h]) = Î»
Ë†
hdÏ + K(Ï, Ï€) + log
Ë†
exp(âˆ’h)dÏ€.
Two well known consequences are
Ï€[h] = arg
min
ÏâˆˆM1
+(Î˜)
Ë†
hdÏ + K(Ï, Ï€)

,
âˆ’log
Ë†
exp(âˆ’h)dÏ€ =
min
ÏâˆˆM1
+(Î˜)
Ë†
hdÏ + K(Ï, Ï€)

.
We will use these inequalities many times in the followings. The most frequent
application will be with h(Î¸) = Î»rn(Î¸) (in this case Ï€[Î»rn] = Ë†ÏÎ») or h(Î¸) =
Â±Î»[rn(Î¸) âˆ’R(Î¸)], the ï¬rst case leads to
K(Ï, Ë†ÏÎ») = Î»
Ë†
rndÏ + K(Ï, Ï€) + log
Ë†
exp(âˆ’Î»rn)dÏ€,
(5.8)
145

5 Properties of variational approximations of Gibbs posteriors
Ë†ÏÎ» = arg
min
ÏâˆˆM1
+(Î˜)

Î»
Ë†
rndÏ + K(Ï, Ï€)

,
(5.9)
âˆ’log
Ë†
exp(âˆ’Î»rn)dÏ€ =
min
ÏâˆˆM1
+(Î˜)

Î»
Ë†
rndÏ + K(Ï, Ï€)

.
(5.10)
We will use (5.8), (5.9) and (5.10) several times in this appendix.
5.A.2 Proof of the theorems in Subsection 5.4.1
Proof of Theorem 5.4.1. This proof follows the standard PAC-Bayesian approach
(see Catoni [2007]). Apply Fubiniâ€™s theorem to the ï¬rst inequality of (5.1):
E
Ë†
exp {Î»[R(Î¸) âˆ’rn(Î¸)] âˆ’f(Î», n)} Ï€(dÎ¸) â‰¤1
then apply the preliminary remark with h(Î¸) = Î»[rn(Î¸) âˆ’R(Î¸)]:
E exp
(
sup
ÏâˆˆM1
+(Î˜)
Ë†
Î»[R(Î¸) âˆ’rn(Î¸)]Ï(dÎ¸) âˆ’K(Ï, Ï€) âˆ’f(Î», n)
)
â‰¤1.
Multiply both sides by Îµ and use E[exp(U)] â‰¥P(U > 0) for any U to obtain:
P
"
sup
ÏâˆˆM1
+(Î˜)
Ë†
Î»[R(Î¸) âˆ’rn(Î¸)]Ï(dÎ¸) âˆ’K(Ï, Ï€) âˆ’f(Î», n) + log(Îµ) > 0
#
â‰¤Îµ.
Then consider the complementary event:
P

âˆ€Ï âˆˆM1
+(Î˜),
Î»
Ë†
RdÏ â‰¤Î»
Ë†
rndÏ + f(Î», n) + K(Ï, Ï€) + log
1
Îµ

â‰¥1 âˆ’Îµ.
â–¡
Proof of Theorem 5.4.2.
Using the same calculations as above, we have, with
probability at least 1 âˆ’Îµ, simultaneously for all Ï âˆˆM1
+(Î˜),
Î»
Ë†
RdÏ â‰¤Î»
Ë†
rndÏ + f(Î», n) + K(Ï, Ï€) + log
2
Îµ

(5.11)
Î»
Ë†
rndÏ â‰¤Î»
Ë†
RdÏ + f(Î», n) + K(Ï, Ï€) + log
2
Îµ

.
(5.12)
We use (5.11) with Ï = Ë†ÏÎ» and (5.9) to get
Î»
Ë†
RdË†ÏÎ» â‰¤
inf
ÏâˆˆM1
+(Î˜)

Î»
Ë†
rndÏ + f(Î», n) + K(Ï, Ï€) + log
2
Îµ

146

5.A Proofs
and plugging (5.12) into the right-hand side, we obtain
Î»
Ë†
RdË†ÏÎ» â‰¤
inf
ÏâˆˆM1
+(Î˜)

Î»
Ë†
RdÏ + 2f(Î», n) + 2K(Ï, Ï€) + 2 log
2
Îµ

.
Now, we work with ËœÏÎ» = arg minÏâˆˆF K(Ï, Ë†ÏÎ»). Plugging (5.8) into (5.11) we get,
for any Ï,
Î»
Ë†
RdÏ â‰¤f(Î», n) + K(Ï, Ë†ÏÎ») âˆ’log
Ë†
exp(âˆ’Î»rn)dÏ€ + log
2
Îµ

.
By deï¬nition of ËœÏÎ», we have:
Î»
Ë†
RdËœÏÎ» â‰¤inf
ÏâˆˆF

f(Î», n) + K(Ï, Ë†ÏÎ») âˆ’log
Ë†
exp(âˆ’Î»rn)dÏ€ + log
2
Îµ

and, using (5.8) again, we obtain:
Î»
Ë†
RdËœÏÎ» â‰¤inf
ÏâˆˆF

Î»
Ë†
rndÏ + f(Î», n) + K(Ï, Ï€) + log
2
Îµ

.
We plug (5.12) into the right-hand side to obtain:
Î»
Ë†
RdËœÏÎ» â‰¤inf
ÏâˆˆF

Î»
Ë†
RdÏ + 2f(Î», n) + 2K(Ï, Ï€) + 2 log
2
Îµ

.
This proves the second inequality of the theorem. In order to prove the claim
BÎ»(F) = BÎ»(M1
+(Î˜)) + 2
Î» inf
ÏâˆˆF K(Ï, Ï€ Î»
2 ),
note that
BÎ»(F) = inf
ÏâˆˆF
(Ë†
RdÏ + 2f(Î», n)
Î»
+ 2K(Ï, Ï€)
Î»
+ 2 log
  2
Îµ

Î»
)
= inf
ÏâˆˆF
(
âˆ’2
Î» log
Ë†
exp

âˆ’Î»
2R

dÏ€ + 2f(Î», n)
Î»
+
2K(Ï, Ï€ Î»
2 )
Î»
+ 2 log
  2
Îµ

Î»
)
= âˆ’2
Î» log
Ë†
exp

âˆ’Î»
2R

dÏ€ + 2f(Î», n)
Î»
+ 2 log
  2
Îµ

Î»
+ 2
Î» inf
ÏâˆˆF K(Ï, Ï€ Î»
2 )
= BÎ»(M1
+(Î˜)) + 2
Î» inf
ÏâˆˆF K(Ï, Ï€ Î»
2 ).
This ends the proof. â–¡
147

5 Properties of variational approximations of Gibbs posteriors
5.A.3 Proof of Theorem 5.4.3 (Subsection 5.4.2)
Proof of Theorem 5.4.3.
As in the proof of Theorem 5.4.1, we apply Fubini,
then (5.10) to the ï¬rst inequality of (5.2) to obtain
E exp

sup
Ï
Ë† 
Î»[R(Î¸) âˆ’R] âˆ’Î»[rn(Î¸) âˆ’rn] âˆ’g(Î», n)[R(Î¸) âˆ’R]

Ï(dÎ¸) âˆ’K(Ï, Ï€)

â‰¤1
and we multiply both sides by Îµ/2 to get
P
(
sup
Ï
"
[Î»âˆ’g(Î», n)]
Ë†
RdÏ âˆ’R

â‰¥Î»
Ë†
rndÏ âˆ’rn

+K(Ï, Ï€)+log
2
Îµ
#)
â‰¤Îµ
2.
(5.13)
We now consider the second inequality in (5.2):
E exp

Î»[rn(Î¸) âˆ’rn] âˆ’Î»[R(Î¸) âˆ’R] âˆ’g(Î», n)[R(Î¸) âˆ’R]
	
â‰¤1.
The same derivation leads to
P
(
sup
Ï
"
[Î»âˆ’g(Î», n)]
Ë†
rndÏ âˆ’rn

â‰¥Î»
Ë†
RdÏ âˆ’R

+K(Ï, Ï€)+log
2
Îµ
#)
â‰¤Îµ
2.
(5.14)
We combine (5.13) and (5.14) by a union bound argument, and we consider the
complementary event: with probability at least 1 âˆ’Îµ, simultaneously for all Ï âˆˆ
M1
+(Î˜),
[Î» âˆ’g(Î», n)]
Ë†
RdÏ âˆ’R

â‰¤Î»
Ë†
rndÏ âˆ’rn

+ K(Ï, Ï€) + log
2
Îµ

,
(5.15)
Î»
Ë†
rndÏ âˆ’rn

â‰¤[Î» + g(Î», n)]
Ë†
RdÏ âˆ’R

+ K(Ï, Ï€) + log
2
Îµ

.
(5.16)
We now derive consequences of these two inequalities (in other words, we focus on
the event where these two inequalities are satisï¬ed). Using (5.9) in (5.15) yields
[Î» âˆ’g(Î», n)]
Ë†
RdË†ÏÎ» âˆ’R

â‰¤
inf
ÏâˆˆM1
+(Î˜)

Î»
Ë†
rndÏ âˆ’rn

+ K(Ï, Ï€) + log
2
Îµ

.
We plug (5.16) into the right-hand side to obtain:
[Î» âˆ’g(Î», n)]
Ë†
RdË†ÏÎ» âˆ’R

â‰¤
inf
ÏâˆˆM1
+(Î˜)
(
[Î» + g(Î», n)]
Ë†
RdÏ âˆ’R

+ 2K(Ï, Ï€) + 2 log
2
Îµ
)
.
148

5.A Proofs
Now, we work with ËœÏÎ». Plugging (5.8) into (5.13) we get
[Î» âˆ’g(Î», n)]
Ë†
RdÏ âˆ’R

â‰¤K(Ï, Ë†ÏÎ») âˆ’log
Ë†
exp[âˆ’Î»(rn âˆ’rn)]dÏ€ + log
2
Îµ

.
By deï¬nition of ËœÏÎ», we have:
[Î» âˆ’g(Î», n)]
Ë†
RdËœÏÎ» âˆ’R

â‰¤inf
ÏâˆˆF

K(Ï, Ë†ÏÎ») âˆ’log
Ë†
exp[âˆ’Î»(rn âˆ’rn)]dÏ€ + log
2
Îµ

.
Then, apply (5.8) again to get:
[Î» âˆ’g(Î», n)]
Ë†
RdËœÏÎ» âˆ’R

â‰¤inf
ÏâˆˆF

Î»
Ë†
(rn âˆ’rn)dÏ + K(Ï, Ï€) + log
2
Îµ

.
Plug (5.16) into the right-hand side to get
[Î» âˆ’g(Î», n)]
Ë†
RdËœÏÎ» âˆ’R

â‰¤inf
ÏâˆˆF

[Î» + g(Î», n)]
Ë†
(R âˆ’R)dÏ + 2K(Ï, Ï€) + 2 log
2
Îµ

.
â–¡
5.A.4 Proofs of Section 5.5
Proof of Lemma 5.5.1.
Combine Theorem 2.1 p.
25 and Lemma 2.2 p.
27
in Boucheron et al. [2013]. â–¡
Proof of Lemma 5.5.2. Apply Theorem 2.10 in Boucheron et al. [2013], and plug
the margin assumption. â–¡
Proof of Corollary 5.5.4. We remind that thanks to (5.6) it is enough to prove the
claim for F1. We apply Theorem 5.4.2 to get:
BÎ»(F1) = inf
(m,Ïƒ2)
(Ë†
RdÎ¦m,Ïƒ2 + Î»
n + 2K(Î¦m,Ïƒ2, Ï€) + log
  2
Îµ

Î»
)
= inf
(m,Ïƒ2)
ï£±
ï£²
ï£³
Ë†
RdÎ¦m,Ïƒ2 + Î»
n + 2
d
h
1
2 log

Ï‘2
Ïƒ2

+ Ïƒ2
Ï‘2
i
+ âˆ¥mâˆ¥2
Ï‘2
âˆ’d
2 + log
  2
Îµ

Î»
ï£¼
ï£½
ï£¾.
149

5 Properties of variational approximations of Gibbs posteriors
Note that the minimizer of R, Î¸, is not unique (because fÎ¸(x) does not depend on
âˆ¥Î¸âˆ¥) and we can chose it in such a way that âˆ¥Î¸âˆ¥= 1. Then
R(Î¸) âˆ’R = E
h
1âŸ¨Î¸,XâŸ©Y <0 âˆ’1âŸ¨Î¸,XâŸ©Y <0
i
â‰¤E
h
1âŸ¨Î¸,XâŸ©âŸ¨Î¸,XâŸ©<0
i
= P
 âŸ¨Î¸, XâŸ©

Î¸, X

< 0

â‰¤c

Î¸
âˆ¥Î¸âˆ¥âˆ’Î¸
 â‰¤2câˆ¥Î¸ âˆ’Î¸âˆ¥.
So:
BÎ»(F1) â‰¤R + inf
(m,Ïƒ2)

2c
Ë†
âˆ¥Î¸ âˆ’Î¸âˆ¥Î¦m,Ïƒ2(dÎ¸)
+ Î»
n + 2
d
h
1
2 log

Ï‘2
Ïƒ2

+ Ïƒ2
Ï‘2
i
+ âˆ¥mâˆ¥2
Ï‘2
âˆ’d
2 + log
  2
Îµ

Î»

.
We now restrict the inï¬mum to distributions Î½ such that m = Î¸:
B(F1) â‰¤R + inf
Ïƒ2
ï£±
ï£²
ï£³2c
âˆš
dÏƒ + Î»
n +
d log

Ï‘2
Ïƒ2

+ 2dÏƒ2
Ï‘2 + 2
Ï‘2 âˆ’d + 2 log
  2
Îµ

Î»
ï£¼
ï£½
ï£¾.
We put Ïƒ =
1
2Î» and substitute
1
âˆš
d for Ï‘ to get
B(F1) â‰¤R + Î»
n + c
âˆš
d + d log(4 Î»2
d ) + d2
2Î»2 + d + 2 log
  2
Îµ

Î»
.
Substitute
âˆš
nd for Î» to get the desired result. â–¡
Proof of Corollary 5.5.5. We apply Theorem 5.4.3:
Ë†
(R âˆ’R)dËœÏÎ»
â‰¤inf
m,Ïƒ2
Î» + g(Î», n)
Î» âˆ’g(Î», n)
Ë†
(R âˆ’Â¯R)dÎ¦m,Ïƒ2 +
1
Î» âˆ’g(Î», n)

2K(Î¦m,Ïƒ2, Ï€) + 2 log 2
Ç«

where Î» <
2n
C+1. Computations similar to those in the the proof of Corollary 5.5.4
lead to
Ë†
RdËœÏÎ» â‰¤R + inf
m,Ïƒ2
(
2cÎ» + g(Î», n)
Î» âˆ’g(Î», n)
Ë†
âˆ¥Î¸ âˆ’Î¸âˆ¥Î¦m,Ïƒ2(dÎ¸)
+ 2
Pd
j=1
h
1
2 log

Ï‘2
Ïƒ2

+ Ïƒ2
Ï‘2
i
+ âˆ¥mâˆ¥2
Ï‘2
âˆ’d
2 + log
  2
Îµ

Î» âˆ’g(Î», n)
)
.
taking m = Â¯Î¸ and Î» =
2n
C+2, we get the result. â–¡
150

5.A Proofs
5.A.5 Proofs of Section 5.6
Proof of Lemma 5.6.1. For ï¬xed Î¸ we can upper bound the individual risk such
that:
0 â‰¤max(0, 1âˆ’< Î¸, Xi > Yi) â‰¤1 + | < Î¸, Xi > |
such that we can apply Hoeï¬€dingâ€™s inequality conditionally on Xi and ï¬xed Î¸.
We get,
E

exp
 Î»(RH âˆ’rH
n )

|X1, Â· Â· Â· , Xn

â‰¤exp
(
Î»2
8n2
n
X
i=1
(1 + | < Î¸, Xi > |)2
)
â‰¤exp
 Î»2
4n + Î»2c2
x
4n âˆ¥Î¸âˆ¥2

where the last inequality stems from the fact that (a + b)2 â‰¤2 (a2 + b2) and the
fact that we have supposed the Xi to be bounded. We can take the expectation
of this term with respect to the Xiâ€™s and with respect to our Gaussian prior.
Ï€

E

exp
 Î»(RH âˆ’rH
n )
	
â‰¤
exp

Î»2
4n

(2Ï€)
d
2âˆš
Ï‘2
Ë†
exp
Î»2c2
x
4n âˆ¥Î¸âˆ¥2 âˆ’
1
2Ï‘2âˆ¥Î¸âˆ¥2

dÎ¸
â‰¤
exp

Î»2
4n

(2Ï€)
d
2âˆš
Ï‘2
Ë†
exp

âˆ’1
2
 1
Ï‘2 âˆ’Î»2c2
x
2n

âˆ¥Î¸âˆ¥2

dÎ¸
The integral is a properly deï¬ned Gaussian integral under the hypothesis that
1
Ï‘2 âˆ’Î»2c2
x
2n > 0 hence Î» <
1
cx
q
n
Ï‘
2. The integral is proportional to a Gaussian and
we can directly write:
Ï€

E

exp
 Î»(RH âˆ’rH
n )
	
â‰¤
exp

Î»2
4n

q
1 âˆ’Ï‘2Î»2c2x
2n
writing everything in the exponential gives the desired result. â–¡
Proof of Corollary 5.6.2. We apply Theorem 5.4.2 to get:
BÎ»(F1) = inf
(m,Ïƒ2)
(Ë†
RHdÎ¦m,Ïƒ2 + Î»
2n âˆ’1
Î» log

1 âˆ’Ï‘2Î»2c2
x
2n

+ 2K(Î¦m,Ïƒ2, Ï€) + log
  2
Îµ

Î»
)
= inf
(m,Ïƒ2)
ï£±
ï£²
ï£³
Ë†
RHdÎ¦m,Ïƒ2 + 2
Pd
j=1
h
1
2 log

Ï‘2
Ïƒ2

+ Ïƒ2
Ï‘2
i
+ âˆ¥mâˆ¥2
Ï‘2
âˆ’d
2 + log
  2
Îµ

Î»
ï£¼
ï£½
ï£¾+
151

5 Properties of variational approximations of Gibbs posteriors
Î»
2n âˆ’1
Î» log

1 âˆ’Ï‘Î»2c2
x
2n

.
We use the fact that the hinge loss is Lipschitz and that the (Xi) are uniformly
bounded âˆ¥Xâˆ¥âˆ< cx. We get RH(Î¸) â‰¤Â¯RH +cx
âˆš
dâˆ¥Î¸âˆ’Â¯Î¸âˆ¥and restrict the inï¬mum
to distributions Î½ such that m = Î¸:
B(F1) â‰¤R
H+
inf
Ïƒ2
ï£±
ï£²
ï£³cxdÏƒ2 + Î»
2n âˆ’1
Î» log

1 âˆ’Ï‘2Î»2c2
x
2n

+
d log

Ï‘2
Ïƒ2

+ 2dÏƒ2
Ï‘2 + 2
Ï‘2 âˆ’d + 2 log
  2
Îµ

Î»
ï£¼
ï£½
ï£¾.
We specify Ïƒ2 =
1
âˆš
dn and Î» =
1
cx
p n
Ï‘2 such that we get:
B(F1) â‰¤RH + cx
r
d
n+
âˆš
Ï‘2
2cx
âˆšn âˆ’cx
r
Ï‘2
n log

1 âˆ’1
2

+dcxÏ‘
âˆšn log

Ï‘2âˆš
nd

+cxÏ‘
2d
nÏ‘2 + 2
Ï‘2 âˆ’d + 2 log
  2
Îµ

âˆšn
.
To get the correct rate we take the prior variance to be Ï‘2 = 1
d by replacing in the
above equation we get the desired result.
â–¡
Proof of Theorem 5.6.3. From Nesterov [2004] (th. 3.2.2) we have the following
bound on the objective function minimized by VB, (the objective is not uniformly
Lipschitz)
Ïk(rH
n ) + 1
Î»K(Ïk, Ï€) âˆ’inf
ÏâˆˆF1

Ï(rH
n ) + 1
Î»K(Ï, Ï€)

â‰¤
LM
âˆš
1 + k.
(5.17)
where the initial point was taken in a ball of radius M around ËœÏ.
We have from equation (5.11) speciï¬ed for measures Ïk probability 1 âˆ’Îµ,
Î»
Ë†
RHdÏk â‰¤Î»
Ë†
rH
n dÏk + f(Î», n) + K(Ïk, Ï€) + log
1
Îµ

Combining the two equations yields,
Ë†
RHdÏk â‰¤
LM
âˆš
1 + k + 1
Î»f(n, Î») + inf
ÏâˆˆF1

Ï(rH
n ) + 1
Î»K(Ï, Ï€)

+ 1
Î» log 1
Îµ
We can therefore write for any Ï âˆˆF1,
Ë†
RHdÏk â‰¤
LM
âˆš
1 + k + 1
Î»f(n, Î») + Ï(rH
n ) + 1
Î»K(Ï, Ï€) + 1
Î» log 1
Îµ
152

5.A Proofs
Using equation (5.11) a second time we get with probability 1 âˆ’Îµ
Ë†
RHdÏk â‰¤
LM
âˆš
1 + k + 2
Î»f(n, Î») + Ï(RH) + 2
Î»K(Ï, Ï€) + 2
Î» log 2
Îµ
Because this is true for any Ï âˆˆF1 in 1âˆ’Îµ we can write the bound for the smallest
measure in F1.
Ë†
RHdÏk â‰¤
LM
âˆš
1 + k + 2
Î»f(n, Î») + inf
ÏâˆˆF1

Ï(RH) + 2
Î»K(Ï, Ï€)

+ 2
Î» log 2
Îµ
By taking the Gaussian measure with variance
1
dn and mean Î¸ in the inï¬mum and
taking Î» =
1
cx
âˆš
nd and Ï‘ = 1
d, we can plug in the result of Corrolary 5.6.2 to get
the result.â–¡
5.A.6 Proofs of Section 5.7
Proof of Lemma 5.7.1. The idea of the proof is to use Hoeï¬€dingâ€™s decomposition
of U-statistics combined with Hoeï¬€dingâ€™s inequality for iid random variables. This
was done in ranking by ClÂ´emenÂ¸con et al. [2008a], and later in Ridgway [2015];
Robbiano [2013] for ranking via aggregation and Bayesian statistics. The proof is
as follows: we deï¬ne
qÎ¸
i,j = 1(Yiâˆ’Yj)(fÎ¸(Xi)âˆ’fÎ¸(Xj))<0 âˆ’R(Î¸)
so that
Un :=
1
n(n âˆ’1)
X
i,j
qÎ¸
i,j = rn(Î¸) âˆ’R(Î˜).
From Hoeï¬€ding [1948] we have
Un = 1
n!
X
Ï€
1
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
where the sum is taken over all the permutations Ï€ of {1, . . . , n}. Jensenâ€™s inequal-
ity leads to
E exp[Î»Un] = E exp
ï£®
ï£°Î» 1
n!
X
Ï€
1
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£»
â‰¤1
n!
X
Ï€
E exp
ï£®
ï£°Î»
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£».
153

5 Properties of variational approximations of Gibbs posteriors
We now use, for each of the terms in the sum the same argument as in the proof
of Lemma 5.5.1 to get
E exp[Î»Un] â‰¤1
n!
X
Ï€
exp
 Î»2
2âŒŠn
2âŒ‹

â‰¤exp
 Î»2
n âˆ’1

(in the last step, we used âŒŠn
2âŒ‹â‰¥(n âˆ’1)/2). We proceed in the same way to upper
bound E exp[âˆ’Î»Un]. â–¡
Proof of Lemma 5.7.2. As already done above, we use Bernstein inequality and
Hoeï¬€ding decomposition. Fix Î¸. We deï¬ne this time
qÎ¸
i,j = 1{âŸ¨Î¸, Xi âˆ’XjâŸ©(Yi âˆ’Yj) < 0} âˆ’1{[Ïƒ(Xi) âˆ’Ïƒ(Xj)](Yi âˆ’Yj) < 0} âˆ’R(Î¸) + R
so that
Un := rn(Î¸) âˆ’rn âˆ’R(Î¸) + R =
1
n(n âˆ’1)
X
iÌ¸=j
qÎ¸
i,j.
Then,
Un = 1
n!
X
Ï€
1
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹).
Jensenâ€™s inequality:
E exp[Î»Un] = E exp
ï£®
ï£°Î» 1
n!
X
Ï€
1
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£»
â‰¤1
n!
X
Ï€
E exp
ï£®
ï£°Î»
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£».
Then, for each of the terms in the sum, use Bernsteinâ€™s inequality:
E exp
ï£®
ï£°Î»
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£»â‰¤exp
ï£®
ï£°E((qÎ¸
Ï€(1),Ï€(1+âŒŠn
2 âŒ‹))2) Î»2
âŒŠn
2 âŒ‹
2

1 âˆ’2 Î»
âŒŠn
2 âŒ‹

ï£¹
ï£».
We use again âŒŠn
2âŒ‹â‰¥(n âˆ’1)/2.
Then, as the pairs (Xi, Yi) are iid, we have
E((qÎ¸
Ï€(1),Ï€(1+âŒŠn
2 âŒ‹))2) = E((qÎ¸
1,2)2) and then E((qÎ¸
1,2)2) â‰¤C[R(Î¸) âˆ’R] thanks to the
margin assumption. So
E exp
ï£®
ï£°Î»
âŒŠn
2âŒ‹
âŒŠn
2 âŒ‹
X
i=1
qÎ¸
Ï€(i),Ï€(i+âŒŠn
2 âŒ‹)
ï£¹
ï£»â‰¤exp
"
C[R(Î¸) âˆ’R] Î»2
nâˆ’1
 1 âˆ’
4Î»
nâˆ’1

#
.
154

5.A Proofs
This ends the proof of the proposition. â–¡
Proof of Corollary 5.7.4. The calculations are similar to the ones in the proof of
Corollary 5.5.4 so we donâ€™t give the details. Note that when we reach
BÎ»(F1) â‰¤R +
2Î»
n âˆ’1 + c
âˆš
d + d log(2Î») + 2 log
  2e
Îµ

Î»
,
an approximate minimization with respect to Î» leads to the choice Î» =
q
d(nâˆ’1)
2
.
â–¡
5.A.7 Proofs of Section 5.8
Proof. First, note that, for any Ï,
K(Ï, Ï€Î²) = Î²
Ë†
(R âˆ’R)dÏ + K(Ï, Ï€) + log
Ë†
exp

âˆ’Î²(R âˆ’R)

dÏ€
â‰¤Î²
Ë†
(R âˆ’R)dÏ + K(Ï, Ï€).
Now, we deï¬ne a subset of F that will be used for the calculation of the bound.
We deï¬ne for Î´ > 0 the probability distribution ÏU,V,Î´(dÎ¸) as Ï€ conditioned to
Î¸ = ÂµÎ½T with Âµ is uniform on {âˆ€(i, â„“), |Âµi,â„“âˆ’Ui,â„“| â‰¤Î´} and Î½ is uniform on
{âˆ€(j, â„“), |Î½i,â„“âˆ’Vj,â„“| â‰¤Î´}. Note that
Ë†
(R âˆ’R)dÏM,N,Î´ =
Ë†
E((Î¸X âˆ’MX)2)ÏU,V,Î´(dÎ¸)
â‰¤
Ë†
3E(((UV T)X âˆ’MX)2)ÏU,V,Î´(d(Âµ, Î½))
+ 3
Ë†
E(((UÎ½T)X âˆ’(UV T)X)2)ÏU,V,Î´(d(Âµ, Î½))
+ 3
Ë†
E(((ÂµÎ½T)X âˆ’(UÎ½T)X)2)ÏU,V,Î´(d(Âµ, Î½)).
By deï¬nition, the ï¬rst term is = 0. Moreover:
Ë†
E(((UÎ½T)X âˆ’(UV T)X)2)ÏU,V,Î´(d(Âµ, Î½))
=
Ë†
1
m1m2
X
i,j
"X
k
Ui,k(Î½j,k âˆ’Vj,k)
#2
ÏU,V,Î´(d(Âµ, Î½))
â‰¤
Ë†
1
m1m2
X
i,j
"X
k
U 2
i,k
# "X
k
(Î½j,k âˆ’Vj,k)2
#
ÏU,V,Î´(d(Âµ, Î½))
155

5 Properties of variational approximations of Gibbs posteriors
â‰¤KrC2Î´2.
In the same way,
Ë†
E(((ÂµÎ½T)X âˆ’(UÎ½T)X)2)ÏU,V,Î´(d(Âµ, Î½)) â‰¤
Ë†
âˆ¥Âµ âˆ’Uâˆ¥2
Fâˆ¥Î½âˆ¥2
FÏU,V,Î´(d(Âµ, Î½))
â‰¤Kr(C + Î´)2Î´2.
So:
Ë†
(R âˆ’R)dÏM,N,Î´ â‰¤2KrÎ´2(C + Î´2).
Now, let us consider the term K(ÏU,V,Î´, Ï€). An explicit calculation is possible but
tedious. Instead, we might just introduce the set GÎ´ = {Î¸ = ÂµÎ½T, âˆ¥Âµ âˆ’Uâˆ¥F â‰¤
Î´, âˆ¥Î½ âˆ’V âˆ¥F â‰¤Î´} and note that K(ÏU,V,Î´, Ï€) â‰¤log
1
Ï€(GÎ´). An upper bound for GÎ´
is calculated page 317-320 in Alquier [2014] and the result is given by (10) in this
reference:
K(ÏU,V,Î´, Ï€) â‰¤4Î´2 + 2âˆ¥Uâˆ¥2
F + 2âˆ¥Nâˆ¥2
F + 2 log(2)
+ (m1 + m2)r log
 
1
Î´
r
3Ï€(m1 âˆ¨m2)K
4
!
+ 2K log
Î“(a)3a+1 exp(2)
ba+12a

as soon as the restriction b â‰¤
Î´2
2m1K log(2m1K),
Î´2
2m2K log(2m2K) is satisï¬ed.
So we
obtain:
K(ÏU,V,Î´, Ï€Î²) â‰¤Î²2KrÎ´2(C + Î´2) + 4Î´2 + 2âˆ¥Uâˆ¥2
F + 2âˆ¥Nâˆ¥2
F + 2 log(2)
+ (m1 + m2)r log
 
1
Î´
r
3Ï€(m1 âˆ¨m2)K
4
!
+ 2K log
Î“(a)3a+1 exp(2)
ba+12a

.
Note that âˆ¥Uâˆ¥2
F â‰¤C2rm1, âˆ¥V âˆ¥2
F â‰¤C2rm2 and K â‰¤m1+m2 so it is clear that the
choice Î´ =
q
1
Î² and b â‰¤
1
2Î²(m1âˆ¨m2) log(2K(m1âˆ¨m2)) leads to the existence of a constant
C(a, C) such that
K(ÏU,V,Î´, Ï€Î²) â‰¤C(a, C)

r(m1 + m2) log [Î²b(m1 + m2)K] + 1
Î²

.
â–¡
5.B Implementation details
5.B.1 Sequential Monte Carlo
Tempering SMC approximates iteratively a sequence of distribution ÏÎ»t, with
ÏÎ»t(dÎ¸) = 1
Zt
exp (âˆ’Î»trn(Î¸)) Ï€(dÎ¸),
156

5.B Implementation details
and temperature ladder Î»0 = 0 < . . . < Î»T = Î». The pseudo code below is given
for an adaptive sequence of temperatures.
Algorithm 19 Tempering SMC
Input N (number of particles), Ï„ âˆˆ(0, 1) (ESS threshold), Îº > 0 (random walk
tuning parameter)
Init. Sample Î¸i
0 âˆ¼Ï€Î¾(Î¸) for i = 1 to N, set t â†1, Î»0 = 0, Z0 = 1.
Loop a. Solve in Î»t the equation
{PN
i=1 wt(Î¸i
tâˆ’1)}2
PN
i=1{wt(Î¸i
tâˆ’1))2}
= Ï„N,
wt(Î¸) = exp[âˆ’(Î»t âˆ’Î»tâˆ’1)rn(Î¸)]
(5.18)
using bisection search. If Î»t â‰¥Î»T, set ZT = Ztâˆ’1 Ã—
n
1
N
PN
i=1 wt(Î¸i
tâˆ’1)
o
,
and stop.
b. Resample: for i = 1 to N, draw Ai
t in 1, . . . , N so that P(Ai
t = j) =
wt(Î¸j
tâˆ’1)/ PN
k=1 wt(Î¸k
tâˆ’1); see Algorithm 20 in the appendix.
c. Sample Î¸i
t âˆ¼Mt(Î¸
Ai
t
tâˆ’1, dÎ¸) for i = 1 to N where Mt is a MCMC kernel that
leaves invariant Ï€t; see comments below.
d. Set Zt = Ztâˆ’1 Ã—
n
1
N
PN
i=1 wt(Î¸i
tâˆ’1)
o
.
The algorithm outputs a weighted sample (wi
T, Î¸i
T) approximately distributed as
target posterior, and an unbiased estimator of the normalizing constant ZÎ»T .
Step b. of algorithm 5.B.1 depends of a resampling algorithm. We choose to
use Systematic resampling, described in Algorithm 20.
157

5 Properties of variational approximations of Gibbs posteriors
Algorithm 20 Systematic resampling
Input: Normalised weights W j
t := wt(Î¸j
tâˆ’1)/ PN
i=1 wt(Î¸i
tâˆ’1).
Output: indices Ai âˆˆ{1, . . . , N}, for i = 1, . . . , N.
a. Sample U âˆ¼U([0, 1]).
b. Compute cumulative weights as Cn = Pn
m=1 NW m.
c. Set s â†U, m â†1.
d. For n = 1 : N
While Cm < s do m â†m + 1.
An â†m, and s â†s + 1.
End For
For the MCMC step, we used a Gaussian random-walk Metropolis kernel, with
a covariance matrix for the random step that is proportional to the empirical
covariance matrix of the current set of simulations.
5.B.2 Optimizing the bound
A natural idea to ï¬nd a global optimum of the objective is to try to solve a sequence
of local optimization problems with increasing inverse temperatures. For inverse
temperature Î» = 0 the problem can be solved exactly (as a KL divergence be-
tween two Gaussians). Then, for two consecutive temperatures, the corresponding
solutions should be close enough.
This idea has been coined under several names. It has a long history in varia-
tional Bayes litterature under the name deterministic annealing. Yuille uses it on
mean ï¬eld on Gibbs distribution for Markov random ï¬elds. In addition the inter-
mediate results can be of interest in our case for selecting the temperature. One
can compute the bound at almost no additional cost as a function of the current
risk. In turns this can be used to monitor the bound.
158

5.B Implementation details
Algorithm 21 Deterministic annealing
Input (Î»t)tâˆˆ[0,T] a sequence of inverse temperature
Init. Set m = 0 and Î£ = Ï‘Id, the values minimizing the KL-divergence for Î» = 0
Loop t=1,. . . ,T
a. mÎ»t, Î£Î»t = Minimize LÎ»t(m, Î£) using some local optimization routine with
initial points mÎ»tâˆ’1, Î£Î»tâˆ’1
b. Break if the empirical bound increases.
End Loop
 Î³ = 0
â—
â—
 Î³ = 125
â—
â—
 Î³ = 250
â—
â—
 Î³ = 375
â—
â—
 Î³ = 500
â—
â—
0
100
200
300
400
âˆ’10
âˆ’5
0
5
value
(a) A one dimensional problem
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
0.00
0.25
0.50
0.75
1.00
1.25
0
100
200
300
400
500
 Î»
95% bound
(b) Empirical bound
Figure 5.B.1: Deterministic annealing on a Pima Indians with one
covariate and full model resp.
The right panel gives the empirical bound obtained for the DA method (in red) and the dot are
direct global optimization based on L-BFGS algorithms from starting values drawn from the prior.
Each optimization problem is repeated 20 times.
We ï¬nd that using a deterministic annealing algorithm with a limited amount of
steps helps in ï¬nding a high enough optimum. On the left panel of Figure 5.B.1,
159

5 Properties of variational approximations of Gibbs posteriors
we can see the one dimensional case where the initial problem Î³ = 0 corresponds
to a convex minimization problem and where the increasing temperature gradually
complexiï¬es the optimization problem. Figure 5.B.1 shows that the solution given
by DA is in average lower than randomly initialized optimization.
5.C Stochastic gradient descent
The stochastic gradient descent algorithm used in Section 5.7 is described as Al-
gorithm 22.
Algorithm 22 Stochastic Gradient Descent
Input B a batch size, an unbiased estimator of the gradient Ë†âˆ‡Bf, Î· âˆˆ(0, 1) and c
While Â¬converged
a. xt+1 = xt âˆ’Î»t Ë†âˆ‡Bf(xt)
b. Update Î»t+1 =
1
(t+c)Î·
End Loop
In all our experiment we take c = 1 and Î· = 0.9.
160

6
Towards the automatic calibration of the number of
particles in SMC2
This is joint work with Nicolas Chopin, Mathieu Gerber and
Omiros Papaspiliopoulos
Status: To be presented at SYSID 2015
6.1 Introduction
Consider a state-space model, with parameter Î¸ âˆˆÎ˜, latent Markov process
(xt)tâ‰¥0, and observed process (yt)tâ‰¥0, taking values respectively in X and Y. The
model is deï¬ned through the following probability densities: Î¸ has prior p(Î¸),
(xt)tâ‰¥0 has initial law ÂµÎ¸(x0) and Markov transition f X
Î¸ (xt|xtâˆ’1), and the ytâ€™s are
conditionally independent, given the xtâ€™s, with density f Y
Î¸ (yt|xt). Sequential anal-
ysis of such a model amounts to computing recursively (in t) the posterior distri-
butions
p(Î¸, x0:t|y0:t) = p(Î¸)ÂµÎ¸(x0)
p(y0:t)
( tY
s=1
f X
Î¸ (xs|xsâˆ’1)
) ( tY
s=0
f Y
Î¸ (ys|xs)
)
or some of its marginals (e.g. p(Î¸|y0:t)); the normalising constant p(y0:t) of the
above density is the marginal likelihood (evidence) of the data observed up to
time t.
For a ï¬xed Î¸, the standard approach to sequential analysis of state-space models
is particle ï¬ltering: one propagates Nx particles in X over time through muta-
tion steps (based on proposal distribution qt,Î¸(xt|xtâˆ’1) at time t) and resampling
steps; see Algorithm 23. Note the conventions: 1 : Nx denotes the set of integers
161

6 Towards the automatic calibration of the number of particles in SMC2
{1, . . . , Nx}, y0:t is (y0, . . . , yt), x1:Nx
t
= (x1
t, . . . , xNx
t ), x1:Nx
0:t
= (x1:Nx
0
, . . . , x1:Nx
t
),
and so on.
Algorithm 23 Particle ï¬lter (PF, for ï¬xed Î¸)
Operations involving superscript n must be performed for all n âˆˆ1 : Nx.
At time 0:
(a) Sample xn
0 âˆ¼q0,Î¸(x0).
(b) Compute weights
w0,Î¸(xn
0) = ÂµÎ¸(xn
0)f Y (y0|xn
0)
q0,Î¸(xn
0)
normalised weights, W n
0,Î¸ = w0,Î¸(xn
0)/ PNx
i=1 w0,Î¸(xi
0), and incremental likeli-
hood estimate
Ë†â„“0(Î¸) = N âˆ’1
x
PNx
n=1 wn
0,Î¸.
Recursively, from time t = 1 to time t = T:
(a) Sample an
t âˆ¼M(W 1:Nx
tâˆ’1,Î¸), the multinomial distribution which generates value
i âˆˆ1 : Nx with probability W i
tâˆ’1,Î¸.
(b) Sample xn
t âˆ¼qt,Î¸(Â·|x
an
t
tâˆ’1).
(c) Compute weights
wt,Î¸(x
an
t
tâˆ’1, xn
t ) = f X(xn
t |x
an
t
tâˆ’1)f Y (yt|xn
t )
qt,Î¸(xn
t |x
an
t
tâˆ’1)
W n
t,Î¸ =
wt,Î¸(x
an
t
tâˆ’1, xn
t )
PNx
i=1 wt,Î¸(x
ai
t
tâˆ’1, xi
t)
and incremental likelihood estimate
Ë†â„“t(Î¸) = N âˆ’1
x
PNx
n=1 wt,Î¸(x
an
t
tâˆ’1, xn
t ).
The output of Algorithm 23 may be used in diï¬€erent ways: at time t, the
quantity PNx
n=1 W n
t,Î¸Ï•(xn
t ) is a consistent (as Nx â†’+âˆ) estimator of the ï¬ltering
expectation E[Ï•(xt)|y0:t, Î¸]; In addition, Ë†â„“t(Î¸) is an unbiased estimator of incre-
mental likelihood p(yt|y0:tâˆ’1, Î¸), and Qt
s=0 Ë†â„“(Î¸) is an unbiased estimator of the full
likelihood p(y0:t|Î¸) [Del Moral, 1996b, Lemma 3].
In order to perform joint inference on parameter Î¸ and state variables, Chopin et al.
162

6.2 Background on SMC2
[2013a] derived the SMC2 sampler, that is, a SMC (Sequential Monte Carlo) algo-
rithm in Î¸âˆ’space, which generates and propagates NÎ¸ values Î¸m in Î˜, and which,
for each Î¸m, runs a particle ï¬lter (i.e. Algorithm 23) for Î¸ = Î¸m, of size Nx. One
issue however is how to choose Nx: if too big, then CPU time is wasted, while if
taken too small, then the performance of the algorithm deteriorates. Chopin et al.
[2013a] give formal results (adapted from Andrieu et al. [2010b]) that suggest that
Nx should grow at a linear rate during the course of the algorithm. They also
propose a practical method for increasing Nx adaptively, based on an importance
sampling step where the NÎ¸ particle systems, of size Nx, are replaced by new
particle systems of size N new
x
. But this importance sampling step increases the
degeneracy of the weights, which in return may lead to more frequent resampling
steps, which are expensive. In this chapter, we derive an alternative way to in-
crease Nx adaptively, which is not based on importance sampling, but rather on a
CSMC (conditional Sequential Monte Carlo) update, which is less CPU intensive.
6.2 Background on SMC2
6.2.1 IBIS
To explain SMC2, we ï¬rst recall the structure of the IBIS algorithm [Chopin,
2002b] as Algorithm 24. For a model with parameter Î¸ âˆˆÎ˜, prior p(Î¸), data y0:T,
and incremental likelihood p(yt|y0:tâˆ’1, Î¸), IBIS provides at each iteration t an ap-
proximation of partial posterior p(Î¸|y0:t). In practice, IBIS samples NÎ¸ particles Î¸m
from the prior, then perfoms sequential importance sampling steps, from p(Î¸|y0:tâˆ’1)
to p(Î¸|y0:t) using incremental weight p(Î¸|y0:t)/p(Î¸|y0:tâˆ’1) âˆp(yt|y0:tâˆ’1, Î¸).
To avoid weight degeneracy, one performs a resample-move step (described as
Step (b) in Algorithm 24). When the ESS (eï¬€ective sample size) of the weights,
computed as:
ESS(Ï‰1:NÎ¸) = (PNÎ¸
m=1 Ï‰m)2
PNÎ¸
m=1(Ï‰m)2 âˆˆ[1, N]
goes below some threshold ESSmin (e.g. N/2), the Î¸mâ€™s are resampled, then moved
according to some Markov kernel Kt that leaves invariant the current target of the
algorithm, p(Î¸|y0:t). This resample-move step re-introduces diversity among the
Î¸-particles.
A convenient default choice for Kt is several iterations of random-walk Metropo-
lis, with the random step calibrated to the spread of the current particle population
(i.e. variance of random step equals some fraction of the covariance matrix of the
resampled particles).
The main limitation of IBIS is that it requires evaluating the likelihood incre-
ment p(yt|y0:tâˆ’1, Î¸), which is typically intractable for state-space models. On the
163

6 Towards the automatic calibration of the number of particles in SMC2
Algorithm 24 IBIS
Operations involving superscript m must be performed for all m âˆˆ1 : NÎ¸.
(Init) Sample Î¸m âˆ¼p(Î¸), set Ï‰m â†1.
From time t = 0 to time t = T, do
(a) Update importance weights
Ï‰m â†Ï‰m Ã— p(yt|y0:tâˆ’1, Î¸).
(b) If ESS(Ï‰1:NÎ¸) â‰¤ESSmin, sample (for all m) ËœÎ¸m from mixture
1
PNÎ¸
m=1 Ï‰m
NÎ¸
X
m=1
Ï‰mKt(Î¸m, dÎ¸),
where Kt is a Markov kernel with invariant distribution p(Î¸|y0:t); ï¬nally reset
particle system to
Î¸1:NÎ¸ â†ËœÎ¸1:NÎ¸,
Ï‰1:NÎ¸ â†(1, . . . , 1).
other hand, we have seen that this quantity may be estimated unbiasedly by par-
ticle ï¬ltering. This suggests combining IBIS (i.e. SMC in the Î¸-dimension) with
particle ï¬ltering (i.e. SMC in the xtâˆ’dimension), as done in the SMC2 algorithm.
6.2.2 SMC2
The general structure of SMC2 is recalled as Algorithm 25. Essentially, one recog-
nises the IBIS algorithm, where the intractable incremental weight p(yt|y0:tâˆ’1, Î¸m)
has been replaced by the unbiased estimate Ë†â„“t(Î¸m). This estimate is obtained from
a PF run for Î¸ = Î¸m; thus NÎ¸ PFs are run in parallel. Denote (x1:Nx,m
0:t
, a1:Nx,m
1:t
)
the random variables generated by the PF associated to Î¸m.
This â€˜double-layerâ€™ structure suggests that SMC2 suï¬€ers from two levels of ap-
proximation, and as such that it requires both Nx â†’+âˆand NÎ¸ â†’+âˆto
converge. It turns out however that SMC2 is valid for any ï¬xed value of Nx; that
is, for any ï¬xed Nx â‰¥1, it converges as NÎ¸ â†’+âˆ.
This property is intuitive in the simpliï¬ed case when resampling-move steps
are never triggered (i.e. take ESSmin = 0). Then SMC2 collapses to importance
164

6.2 Background on SMC2
Algorithm 25 SMC2
Operations involving superscript m must be performed for all m âˆˆ1 : NÎ¸.
(Init) Sample Î¸m âˆ¼p(Î¸), set Ï‰m â†1.
From time t = 0 to time t = T, do
(a) For each Î¸m, run iteration t of Algorithm 23, so as to obtain (x1:Nx,m
0:t
, a1:Nx,m
1:t
),
and Ë†â„“t(Î¸m).
(b) Update weights
Ï‰m â†Ï‰m Ã— Ë†â„“t(Î¸m).
(c) If ESS(Ï‰1:NÎ¸) â‰¤ESSmin, sample (for all m) (ËœÎ¸m, Ëœx1:Nx,m
0:t
, Ëœa1:Nx,m
1:t
) from mixture
1
PNÎ¸
m=1 Ï‰m
NÎ¸
X
m=1
Ï‰mKt

(Î¸m, x1:Nx,m
0:t
, a1:Nx,m
1:t
), dÂ·

,
where Kt is a PMCMC kernel with invariant distribution Ï€t(Î¸, x1:Nx
0:t , a1:Nx
1:t )
(see text); ï¬nally reset particle system to
(Î¸m, x1:Nx,m
0:t
, a1:Nx,m
1:t
) â†(ËœÎ¸m, Ëœx1:Nx,m
0:t
, Ëœa1:Nx,m
1:t
)
and Ï‰m â†1, for all m.
sampling, with weights replaced by unbiased estimates, and it is easy to show
convergence from ï¬rst principles.
We now give a brief outline of the formal justiï¬cation of SMC2 for ï¬xed Nx, and
refer to Chopin et al. [2013a] for more details. SMC2 may be formalised as a SMC
sampler for the sequence of extended distributions:
Ï€t(Î¸, x1:Nx
0:t , a1:Nx
1:t ) = p(Î¸)
p(y0:t)Ïˆt,Î¸(x1:Nx
0:t , a1:Nx
1:t )
tY
s=0
Ë†â„“s(Î¸)
where Ïˆt,Î¸ denotes the joint pdf of the random variables generated by a PF up
to time t (for parameter Î¸), and Ë†â„“s(Î¸) denotes the unbiased estimate of the like-
lihood increment computed from that PF, Ë†â„“0(Î¸) = N âˆ’1
x
PN
n=1 w0(xn
0), Ë†â„“s(Î¸) =
N âˆ’1
x
PN
n=1 ws,Î¸(x
an
t
sâˆ’1, xn
s) for s > 0; i.e. Ë†â„“s(Î¸) is actually a function of (Î¸, x1:Nx
0:s , a1:Nx
1:s ).
One recognises in Ï€t the type of extended target distribution simulated by PM-
CMC (Particle MCMC, Andrieu et al. [2010b]) algorithms. Note Ï€t is a proper
165

6 Towards the automatic calibration of the number of particles in SMC2
probability density (it integrates to one), and that the marginal distribution of
Î¸ is p(Î¸|y0:t). These two properties are easily deduced from the unbiasedness of
Qt
s=0 Ë†â„“s(Î¸) (as an estimator of p(y0:t|Î¸)). In addition,
Ï€t(Î¸, x1:Nx
0:t , a1:Nx
1:t ) = Ï€tâˆ’1(Î¸, x1:Nx
0:tâˆ’1, a1:Nx
1:tâˆ’1) Ïˆt,Î¸(x1:Nx
0:t , a1:Nx
1:t )
Ïˆtâˆ’1,Î¸(x1:Nx
0:tâˆ’1, a1:Nx
1:tâˆ’1)
Ë†â„“t(Î¸)
where one recognises in the second factor the distribution of the variables gener-
ated by a PF at time t, conditional on those variables generated up to time t âˆ’1.
Thus, the equation above justiï¬es both Step (a) of Algorithm 25, where the par-
ticle ï¬lters are extended from time t âˆ’1 to t, and Step (b), where the particles
(Î¸m, x1:Nx,m
0:t
, a1:Nx,m
1:t
) are reweighted by Ë†â„“t(Î¸m).
We describe in the following section PMCMC moves that may be used in Step
(c). Before, we note that a naive implementation of SMC2 has a O(tNxNÎ¸) memory
cost at time t, as one must stores in memory (Î¸m, x1:Nx,m
0:t
, a1:Nx,m
1:t
) for each m âˆˆ1 :
NÎ¸. This memory cost may be substantial even on a modern computer.
6.2.3 PMCMC moves
To make more explicit the dependence of the unbiased estimate of the likelihood
on the variables generated during the course of PF, deï¬ne
Lt(Î¸, x1:Nx
0:t , a1:Nx
1:t ) =
tY
s=0
Ë†â„“s(Î¸) =
(
1
Nx
Nx
X
n=1
w0,Î¸(xn
0)
)
tY
s=1
(
1
Nx
Nx
X
n=1
ws,Î¸(xan
s
sâˆ’1, xn
s)
)
.
The PMMH (Particle Markov Metropolis-Hastings) kernel, described as Algo-
rithm 26, may be described informally as a Metropolis step in Î¸-space, where
the likelihood of both the current value and the proposed value have been re-
placed by unbiased estimators. Formally, as proven in Andrieu et al. [2010b], it
is in fact a standard Metropolis step with respect to the extended distribution
Ï€t(Î¸, x1:Nx
0:t , a1:Nx
1:t ); in particular it leaves invariant p(Î¸|y0:t). (For convenience, our
description of PMMH assumes a random walk proposal, but PMMH is not re-
stricted to this kind of proposal.)
In practice, we set Î£t, the covariance matrix of the proposal, to a fraction of the
covariance matrix of the resampled Î¸-particles.
One advantage of using PMHMH within SMC2 is that it does not require storing
all the variables generated by the NÎ¸ PFs: operations at time t > 0 require only
having access to, for each m, (Î¸m, x1:Nx,m
tâˆ’1
, a1:Nx,m
tâˆ’1
) and Ltâˆ’1(Î¸m, x1:Nx,m
0:tâˆ’1 , a1:Nx,m
1:t
),
which is computed recursively. Memory cost then reduces to O(NÎ¸Nx).
The Particle Gibbs approach is an alternative PMCMC step, based on the fol-
lowing property of target Ï€t: if one extends Ï€t with random index k, such that
166

6.2 Background on SMC2
Algorithm 26 Random walk PMMH update
Input: (Î¸, x1:Nx
0:t , a1:Nx
1:t )
Output: (ËœÎ¸, Ëœx1:Nx
0:t , Ëœa1:Nx
1:t )
1. Î¸â‹†= Î¸ + z, z âˆ¼N(0, Î£t).
2. Generate PF (Algorithm 23) for parameter Î¸â‹†; let (x1:Nx,â‹†
0:t
, a1:Nx,â‹†
1:t
) the output.
3. With probability 1 âˆ§r,
r = p(Î¸â‹†)Lt(Î¸â‹†, x1:Nx,â‹†
0:t
, a1:Nx,â‹†
1:t
)
p(Î¸)Lt(Î¸, x1:Nx
0:t , a1:Nx
1:t )
let (ËœÎ¸, Ëœx1:Nx
0:t , Ëœa1:Nx
1:t )
â†
(Î¸â‹†, x1:Nx,â‹†
0:t
, a1:Nx,â‹†
1:t
);
otherwise (ËœÎ¸, Ëœx1:Nx
0:t , Ëœa1:Nx
1:t )
â†
(Î¸, x1:Nx
0:t , a1:Nx
1:t ).
k âˆˆ1 : Nx, and k âˆ¼M(W 1:Nx
T
), the normalised weights at the ï¬nal iteration,
then (a) the selected trajectory, together with Î¸, follow the posterior distribution
p(Î¸, x0:t|y0:t); and (b) the remaining arguments of Ï€t follow a CSMC (conditional
SMC) distribution, which corresponds to the distribution of the random variables
generated by a PF, but conditional on one trajectory ï¬xed to the selected trajec-
tory; see Algorithm 27.
In contrast with PMMH, implementing particle Gibbs steps within SMC2 re-
quires having access to all the variables (Î¸m, x1:Nx,m
0:t
, a1:Nx,m
1:t
) at time t, which as
we have already discussed, might incur too big a memory cost.
6.2.4 Choosing Nx
Andrieu et al. [2010b] show that, in order to obtain reasonable performance for
PMMH, one should take Nx = O(t). Andrieu et al. [2013] show a similar result
for Particle Gibbs.
In the context of SMC2, this suggests that Nx should be allowed to increase
in the course of the algorithm. To that eï¬€ect, Chopin et al. [2013a] devised an
exchange step, which consists in exchanging the current particle systems, of size
Nx, with new particle systems, of size N new
x
, through importance sampling. In
Chopin et al. [2013a]â€™s implementation, the exchange step is triggered each time
the acceptance rate of the PMMH step (as performed in Step 3. of Algorithm 26)
is below a certain threshold, and N new
x
= 2Nx (i.e. Nx doubles every time).
The main drawback of this approach is that it introduces some weight degener-
167

6 Towards the automatic calibration of the number of particles in SMC2
Algorithm 27 Particle Gibbs update
Input: (Î¸, x1:Nx
0:t , a1:Nx
1:t )
Output: (ËœÎ¸, Ëœx1:Nx
0:t , Ëœa1:Nx
1:t )
1. Sample bt âˆ¼M(W 1:Nx
t
), with W n
t = wt,Î¸(x
an
t
tâˆ’1, xn
t )/ PNx
i=1 wt,Î¸(x
ai
t
tâˆ’1, xi
t). From
s = t âˆ’1 to s = 0, set bs â†abs+1
s+1 . Set Ëœx1
s â†xbs
s , Ëœa1
s = 1 for all s âˆˆ0 : T.
2. Sample ËœÎ¸ from a MCMC step that leaves invariant distribution p(Î¸|x0:t, y0:t),
but with x0:t set to Ëœx1
0:t.
3. Sample (Ëœx2:Nx
0:t , Ëœa2:Nx
1:t ) as in Algorithm 23, but for parameter ËœÎ¸ and conditionally
on Ëœx1
0:t, that is: at time 0, generate Ëœxn
0 âˆ¼q0,ËœÎ¸ for n âˆˆ2 : N, at time 1, sample
an
t âˆ¼M(W 1:Nx
1
), for n âˆˆ2 : N, and xn
t âˆ¼q1,ËœÎ¸(Â·|Ëœx
Ëœan
1
tâˆ’1), and so on.
acy immediately after the resampling step. In particular, we will observe in our
simulations that this prevents us from changing Nx too frequently, as the ESS of
the weights then becomes too low.
In this chapter, we discuss how to use a Particle Gibbs step in order to increase
Nx without changing the weights.
6.3 Proposed approach
6.3.1 Particle Gibbs and memory cost
We ï¬rst remark that the Particle Gibbs step, Algorithm 27, oï¬€ers a very simple
way to change Nx during the course of the algorithm: In Step (2), simply re-
generate a particle system (conditional on selected trajectory Ëœx1
0:t) of size N new
x
.
But, as already discussed, such a strategy requires then to access past particle
values xn
s (and also an
s), rather than only current particle values xn
t .
This problem may be addressed in two ways. First, one may remark that, to
implement Particle Gibbs, one needs to store only those xn
s (and an
s) which have
descendant among the Nx current particles xn
t . Jacob et al. [2013] developed such
a path storage approach, and gave conditions on the mixing of Markov chain (xt)
under which this approach has memory cost O(t+Nx log Nx) (for a single PF with
Nx particles, run until time t). Thus, an implementation of this approach within
SMC2 would lead to a O(NÎ¸(t + Nx log Nx)) memory cost.
A second approach, developed here, exploits the deterministic nature of PRNGs
(pseudo-random number generators): a sequence z0, z1, . . . , zi, . . . of computer-
168

6.3 Proposed approach
generated random variates is actually a deterministic sequence determined by the
initial state (seed) of the PRNG. It is suï¬ƒcient to store that initial state and z0
in order to recover any zi in the future. The trade-oï¬€is an increase in CPU cost,
as each access to zi require re-computing z1, . . . , zi.
We apply this idea to the variables (x1:Nx,m
0:t
, a1:Nx,m
1:t
). By close inspection of
Algorithm 25, we note that variables in a â€˜time sliceâ€™ (x1:Nx,m
s
, a1:Nx,m
s
), 0 < s â‰¤t
(or x1:Nx,m
0
at time 0) are always generated jointly, either during Step (a), or
during Step (c). In both cases, this time-slice is a deterministic function of the
current PRNG state and the previous time slice. Thus, one may recover any time
slice (when needed) by storing only (i) the PNRG state (immediately before the
generation of the time slice); and (ii) in which Step (either (a) or (c)) the time
slice was generated. This reduces the memory cost of SMC2 from O(tNÎ¸Nx) to
O(NÎ¸(t + Nx)).
Compared to the path storage approach mentioned above, our PRNG recycling
approach has a larger CPU cost, a smaller memory cost, and does not require
any conditions on the mixing properties of process (xt). Note that the CPU cost
increase is within a factor of two, because each time a Particle Gibbs update is
performed, the number of random variables that must be re-generated (i.e. the xn
s
and an
s in Algorithm 27) roughly equals the number of random variables that are
generated for the ï¬rst time (i.e. the Ëœxn
s and Ëœan
s in Algorithm 27).
6.3.2 Nonparametric estimation of Nx
As seen in Algorithm 25, a Particle Gibbs step will be performed each time
the ESS goes below some threshold.
That the ESS is low may indicate that
Nx is also too low, and therefore that the variance of the likelihood estimates
Lt(Î¸m, x1:Nx,m
0:t
, a1:Nx,m
1:t
) is too high. Our strategy is to update (each time a Parti-
cle Gibbs step is performed) the current value of Nx to N new
x
= Ï„/Ë†Ïƒ2, where Ë†Ïƒ2 is
some (possibly rough) estimate of the variance of the log likelihood estimates. This
is motivated by results from Doucet et al. [2012], who also develop some theory
that supports choosing Ï„ â‰ˆ1 is optimal (although their optimality results do not
extend straightforwardly to our settings).
Assume Î˜ âŠ‚Rd. To estimate Ïƒ2, we use backï¬tting to ï¬t a GAM (generalized
additive model) to the responses Rm = log Lt(Î¸m, x1:Nx,m
0:t
, a1:Nx,m
1:t
):
Rm = Î± +
d
X
j=1
fj(Cm
j ) + Îµm,
using as covariates Cm
j
the d principal components of the resampled Î¸-particles.
The estimate Ïƒ2 is then the empirical variance of the residuals. See e.g. Chap. 9
of Hastie et al. [2009] for more details on backï¬tting and GAM modelling.
169

6 Towards the automatic calibration of the number of particles in SMC2
We found this strategy to work well, with the caveat that choosing Ï„ required
some trial and error.
6.3.3 Additional considerations
Using Particle Gibbs as our PMCMC move within SMC2 hast two advantages:
(a) it makes it possible to change Nx without changing the weights, as explained
above; and (b) it also makes it possible to update the Î¸m according to Gibbs or
Metropolis step that leaves Î¸|x0:t, y0:t invariant); see Step (3) of Algorithm 27. For
models where sampling from Î¸|x0:t, y0:t is not convenient, one may instead update
Î¸ through several PMMH steps performed after the Particle Gibbs step.
6.4 Numerical example
We consider the following stochastic volatility model: x0 âˆ¼N(Âµ, Ïƒ2/(1 âˆ’Ï2)),
xt âˆ’Âµ = Ï(xtâˆ’1 âˆ’Âµ) + ÏƒÇ«t,
Ç«t âˆ¼N(0, 1) and yt|xt âˆ¼N(0, ext); thus Î¸ = (Âµ, Ï, Ïƒ),
with Ï âˆˆ[âˆ’1, 1], Ïƒ > 0. We assign independent priors to the components of Î¸:
Âµ âˆ¼N(0, 22), Ï âˆ¼N(0, 1) constrained to [âˆ’1, 1], and Ïƒ2 âˆ¼IG(3, 0.5). The dataset
consists in log-returns from the monthly SP500 index, observed from 29/05/2013
to 19/12/2014; T = 401.
Figure 6.1 plots the marginal posterior p(Ï, Ïƒ2|y0:15), as approximated by SMC2,
run up to time 15. This ï¬gure illustrates the need for modelling nonparametrically
the true likelihood as a function of Î¸, in order to estimate the variance of the
estimated likelihood.
170

6.4 Numerical example
âˆ’2
âˆ’1
0
1
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
 
 
Figure 6.1: Marginal posterior p(Ïƒ2, Ï|y0:15), as approximated by
SMC2 run until t = 15, and linearly transformed so that
axes are the two principal components.
For this model, sampling jointly from Î¸|x0:t, y0:t is diï¬ƒcult, but it is easy to
perform a Gibbs step that leaves invariant Î¸|x0:t, y0:t, as the full conditionals of
each component (e.g. Âµ|Ïƒ, Ï, x0:t, y0:t and so on) are standard distributions. Letâ€™s
call â€˜full PGâ€™ Algorithm 27, where Step 2 consists of this Gibbs step for Î¸|x0:t, y0:t;
and conversely letâ€™s call â€˜partial PGâ€™ Algorithm 27 with ËœÎ¸ = Î¸ in Step 2 (Î¸ is not
updated).
We compare four versions of SMC2: (a) the standard version, as proposed in
Chopin et al. [2013a] (i.e. Step (c) of Algorithm 25 is a PMMH step, and that
step is followed by an exchange step to double Nx when the acceptance rate of
PMMH is below 20%); (b) the same algorithm, except that an exchange step
is systematically performed after Step (c), and Nx is set to the value obtained
with our non-parametric approach (see Section 6.3.2); (c) the version developed
in this chapter, with full PG steps (and Nx updated through the non-parametric
procedure); (d) the same algorithm, but with partial PG steps, followed by 3
PMMH steps to update Î¸.
The point of Algorithm (b) is to show that adapting Nx too often during the
course of the algorithm is not desirable when using the exchange step, as this leads
to too much variance. The point of Algorithm (d) is to see how our approach
performs when sampling from Î¸|x0:t, y0:t (either independently or through MCMC)
is not feasible.
Figure 6.2 plots the evolution of Nx over time for the four SMC2 algorithms. One
sees that, for these model and dataset, the CPU cost of the standard SMC2 algo-
rithm is quite volatile, as Nx increases very quickly in certain runs. In fact certain
171

6 Towards the automatic calibration of the number of particles in SMC2
runs are incomplete, as they were stopped when the CPU time exceeded 10 hours.
On the other hand, the CPU cost of other versions is more stable across runs, and,
more importantly, quite lower.
0
1000
2000
3000
0
100
200
300
400
time steps
NX
Figure 6.2: Evolution of Nx over time for 5 runs of the four considered
SMC2 algorithms; red dotted line is Algorithm (a), blue
dashed is (b), black solid is (c), green double-dashed is
(d). Results of (c) and (d) are nearly undistinguishable.
Figure 6.3 plots the empirical variance of the estimated marginal likelihood
(evidence, p(y0:t)), normalised with the running time up to time step t.
One
observes that version (c) does quite better than (d), and far much better than (a).
Results from Algorithm (b) were to variable to be included.
172

6.4 Numerical example
0.00
0.25
0.50
0.75
1.00
0
100
200
300
400
time steps
variance of the log evidence x CPU time
Figure 6.3: Empirical variance of estimated marginal likelihood
p(y0:t) multiplied by average CPU time; same legend as
Figure 6.2, results from Algorithm (b) are omitted.
0.00
0.25
0.50
0.75
1.00
0
100
200
300
400
time steps
Acceptance ratio
Figure 6.4: PMMH acceptance rate across time; same legend as Fig-
ure 6.2. Black line marks 20% target.
Figure 6.4 plots the acceptance rate of PMMH steps for Algorithms (a), (b)
and (d). (Recall that Algorithm (c) does not perform PMMH steps). Note the
poor performance of Algorithm (b). Figure 6.5 compares the box-plots of pos-
terior estimates of Ïƒ at ï¬nal time T, obtained from several runs of Algorithms
(c) and (d). Algorithm (c) shows slightly less variability, while being 30% faster
173

6 Towards the automatic calibration of the number of particles in SMC2
on average. One sees that the improvement brought by ability to sample from
Î¸|x0:t, y0:t is modest here for parameter estimation, but recall that in Figure 6.3,
the improvement was more substantial.
0.225
0.230
0.235
0.240
SMC^2 PG
SMC^2 PMMH
value
Figure 6.5: Box-plots of posterior estimate of parameter Ïƒ at ï¬nal
time T, over repeated runs of Algorithm (c) (left panel)
and Algorithm (d) (right panel).
174

BIBLIOGRAPHY
Bibliography
S. L. Adler. Over-relaxation method for the Monte Carlo evaluation of the partition
function for multiquadratic actions. Physical Review D, 23(12):2901, 1981.
J. H. Albert and S. Chib. Bayesian analysis of binary and polychotomous response
data. J. Am. Statist. Assoc., 88(422):669â€“79, 1993.
P. Alquier. Pac-bayesian bounds for randomized empirical risk minimizers. 17(4):
279â€“304, 2008.
P. Alquier. Bayesian methods for low-rank matrix estimation: short survey and
theoretical study. In S. Jain, R. Munos, F. Stephan, and T. Zeugmann, editors,
Algorithmic Learning Theory. Springer - Lecture Notes in Artiï¬cial Intelligence,
2014.
P. Alquier and G. Biau. Sparse single-index model. J. Mach. Learn. Res., 14(1):
243â€“280, 2013.
P. Alquier and X. Li. Prediction of quantiles by statistical learning and application
to GDP forecasting.
In J.-G. Ganascia, P. Lenca, and J.-M. Petit, editors,
Discovery Science. Springer - Lecture Notes in Artiï¬cial Intelligence, 2012.
C. Andrieu and G.O. Roberts. The pseudo-marginal approach for eï¬ƒcient Monte
Carlo computations. The Annals of Statistics, 37(2):697â€“725, 2009. doi: 10.
1214/07-AOS574.
C. Andrieu and J. Thoms. A tutorial on adaptive MCMC. Statist. Comput., 18
(4):343â€“373, 2008. doi: 10.1007/s11222-008-9110-y.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov Chain Monte Carlo.
J. R. Statist. Soc. B, 72:269â€“342, 2010a.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo
methods. J. R. Statist. Soc. B, 72(3):269â€“342, 2010b. doi: 10.1111/j.1467-9868.
2009.00736.x.
C. Andrieu, A. Lee, and M. Vihola. Uniform Ergodicity of the Iterated Conditional
SMC and Geometric Ergodicity of Particle Gibbs samplers.
ArXiv e-prints,
December 2013.
J.-Y. Audibert and O. Catoni. Robust linear least squares regression. Ann. Statist.,
39(5):2766â€“2794, 10 2011. doi: 10.1214/11-AOS918. URL http://dx.doi.org/
10.1214/11-AOS918.
175

BIBLIOGRAPHY
RÂ´emi Bardenet, Arnaud Doucet, and Chris Holmes. On markov chain monte carlo
methods for tall data. arXiv preprint arXiv:1505.02827, 2015.
J. Bennett and S. Lanning. The netï¬‚ix prize. In Proceedings of KDD Cup and
Workshop 07, 2007.
Alexandros Beskos, Natesh Pillai, Gareth Roberts, Jesus-Maria Sanz-Serna, and
Andrew Stuart. Optimal tuning of the hybrid monte carlo algorithm. Bernoulli,
19(5A):1501â€“1534, nov 2013. doi: 10.3150/12-bej414. URL http://dx.doi.
org/10.3150/12-BEJ414.
C. M. Bishop. Pattern Recognition and Machine Learning, chapter 10. Springer,
2006a.
C.M. Bishop.
Pattern recognition and machine learning.
Springer New York,
2006b.
P. Bissiri, C. Holmes, and S. Walker. A general framework for updating belief
distributions. arXiv preprint arXiv:1306.6430, 2013.
S. Boucheron, G. Lugosi, and P. Massart.
Concentration Inequalities.
Oxford
University Press, 2013.
F. Bretz, A. Genz, and L. A. Hothorn. On the numerical availability of multiple
comparison procedures. Biometrical journal, 5:645â€“656, 2001.
P. BÂ¨uhlmann and S. van de Geer. Statistics for High-Dimensionnal Data. Springer,
2011.
A. Bursh-Supan, V. Hajivassiliou, and L. J. Kotlikoï¬€. Health, children and elderly
arrangments: a multiperiod multinomial probit model with unobserved hetero-
geneity and autocorrelated errors.
Topics in the Economics of Aging, pages
79â€“108, 1992.
R. H. Byrd, S. L. Hansen, J. Nocedal, and Y. Singer. A stochastic quasi-Newton
method for large-scale optimization. arXiv preprint arXiv:1401.7020, 2014.
E. J. Cand`es and T. Tao. The power of convex relaxation: near-optimal matrix
completion. IEEE Trans. Inform. Theory, 56(5):2053â€“2080, 2010. ISSN 0018-
9448.
doi: 10.1109/TIT.2010.2044061.
URL http://dx.doi.org/10.1109/
TIT.2010.2044061.
O. CappÂ´e, E. Moulines, and T. Ryden.
Inference in Hidden Markov Models.
Springer Series in statistics, 2005.
176

BIBLIOGRAPHY
J. Carpenter, P. Cliï¬€ord, and P. Fearnhead. Improved particle ï¬lter for nonlinear
problems. IEE Proceedings-Radar, Sonar and Navigation, 146(1):2â€“7, 1999.
O. Catoni. Statistical learning theory and stochastic optimization, volume 1851
of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 2004. Lecture notes
from the 31st Summer School on Probability Theory held in Saint-Flour, July
8â€“25, 2001.
O. Catoni. PAC-Bayesian Supervised Classiï¬cation, volume 56. IMS Lecture Notes
& Monograph Series, 2007.
O. Catoni. Challenging the empirical mean and empirical variance: A deviation
study. Ann. Inst. H. PoincarÂ´e Probab. Statist., 48(4):1148â€“1185, 11 2012. doi:
10.1214/11-AIHP454. URL http://dx.doi.org/10.1214/11-AIHP454.
V. Chernozhukov and H. Hong.
An MCMC approach to classical estimation.
Journal of Econometrics, 115(2):293â€“346, 2003.
Hugh Chipman, Edward I George, and Robert E McCulloch. The practical imple-
mentation of Bayesian model selection, pages 65â€“134. 2001.
N. Chopin. A sequential particle ï¬lter method for static models. Biometrika, 89
(3):539â€“551, 2002a.
N. Chopin. A sequential particle ï¬lter for static models. Biometrika, 89:539â€“552,
2002b.
N. Chopin. Fast simulation of truncated Gaussian distributions. Statist. Comput.,
21(2):275â€“288, 2011a. ISSN 0960-3174. doi: 10.1007/s11222-009-9168-1.
N. Chopin. Fast simulation of truncated gaussian distributions. Statist. Comput.,
21(2):275â€“288, 2011b.
N. Chopin and C. Shaeï¬€er. Sequential monte carlo on large binary sampling spaces.
Statist. Comput., 2011.
N. Chopin, P. Jacob, and O. Papaspiliopoulos. SMC2: A sequential Monte Carlo
algorithm with particle Markov chain Monte Carlo updates. J. R. Statist. Soc.
B, 75(3):397â€“426, 2013a.
N. Chopin, O. Papaspiliopoulos, and P. E. Jacob. SMC2: an eï¬ƒcient algorithm for
sequential analysis of state space models. J. R. Statist. Soc. B, 75(3):397â€“426,
2013b.
177

BIBLIOGRAPHY
J. A. Christen, C. Fox, D. A. PÂ´erez-Ruiz, and M. Santana-Cibrian. On optimal
direction Gibbs sampling. arXiv preprint arXiv:1205.4062, 2012.
S. ClÂ´emenÂ¸con, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of
U-statistics. Ann. Stat., 36(2):844â€“874, 04 2008a.
S. ClÂ´emenÂ¸con, V.C. Tran, and H. De Arazoza.
A stochastic SIR model with
contact-tracincing: large population limits and statistical inference. Journal of
Biological Dynamics, 2(4):392â€“414, 2008b.
G. Consonni and J.M. Marin. Mean-ï¬eld variational approximate Bayesian infer-
ence for latent variable models. Comput. Stat. Data Anal., 52(2):790â€“798, 2007.
doi: 10.1016/j.csda.2006.10.028. URL http://dx.doi.org/10.1016/j.csda.
2006.10.028.
John D. Cook. Time exchange rate. The Endeavour (blog), 2014. URL http:
//www.johndcook.com/blog/2014/08/17/time-exchange-rate/.
C. Cortes and M. Mohri. Auc optimization vs. error rate minimization. In NIPS,
volume 9, 2003.
A. S. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting, sharp
PAC-Bayesian bounds and sparsity. Machine Learning, 72:39â€“61, 2008.
A. S. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation
and Langevin Monte-Carlo. Journal of Computer and System Science, 78(5):
1423â€“1443, 2012.
P. Del Moral. Non-linear ï¬ltering: interacting particle resolution. Markov processes
and related ï¬elds, 2(4):555â€“581, 1996a.
P. Del Moral. Non-linear ï¬ltering: interacting particle resolution. Markov processes
and related ï¬elds, 2(4):555â€“581, 1996b.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411â€“
436, 2006a.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. J. R.
Statist. Soc. B, 68(3):411â€“436, 2006b. ISSN 1467-9868.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incom-
plete data via the em algorithm. J. R. Statist. Soc. B, 39:1â€“38, 1977.
178

BIBLIOGRAPHY
R. Douc, O. Cappe, and E. Moulines.
Comparison of resampling schemes for
particle ï¬ltering.
In Proc. 4th Int. Symp. Image and Signal Processing and
Analysis ISPA 2005, pages 64â€“69, 2005.
A. Doucet, S. Godsill, and C. Andrieu. On sequential monte carlo sampling meth-
ods for bayesian ï¬ltering. Statistics and computing, 10(3):197â€“208, 2000.
A. Doucet, M. Briers, and S. Senecal.
Eï¬icient Block Sampling Strategies for
Sequential Monte Carlo. J. Comput. Graph. Statist., 15(3):693â€“711, 2006.
A. Doucet, M. Pitt, G. Deligiannidis, and R. Kohn. Eï¬ƒcient implementation of
Markov chain Monte Carlo when using an unbiased likelihood estimator. ArXiv
preprint, October 2012.
S. Duane, D. Kennedy, A., B. J. Pendelton, and D. Roweth. Hybrid monte carlo.
Physics Letters B, 195(2):216â€“222, september 1987.
C. Dubarry and R. Douc. Particle approximation improvement of the joint smooth-
ing distribution with on the ï¬‚y variance estimation. arXiv:1107.5524v1, pages
1â€“19, June 2011.
Mohammad Emtiyaz Khan,
Aleksandr Aravkin,
Michael Friedlander,
and
Matthias Seeger. Fast dual variational inference for non-conjugate latent gaus-
sian models. In Proceedings of The 30th International Conference on Machine
Learning, pages 951â€“959, 2013.
C. Faes, J. Ormeros, and M. Wand. Variational Bayesian Inference for Parametric
and Nonparametric Regression With Missing Data. J. Am. Statist. Assoc., 106
(495):959â€“971, September 2011.
P. Fearnhead, O. Papaspiliopoulos, G.O. Roberts, and A. Stuart. Random weight
particle ï¬ltering of continuous time processes.
J. R. Stat. Soc. Ser. B Stat.
Methodol., 72:497â€“513, 2010.
D. Firth. Bias reduction of maximum likelihood estimates. Biometrika, 80(1):
27â€“38, 1993.
C. Flecher, D. Allard, and P. Naveau. Truncated skew-normal distributions: Es-
timation by weighted moments and application to climatic data. Technical Re-
port 39, Institut National de la Recherche Agronomique, 2009.
Y. Freund, R. Iyer, R.E Schapire, and Y. Singer. An eï¬ƒcient boosting algorithm
for combining preferences. J. Mach. Learn. Res., 4:933â€“969, 2003.
179

BIBLIOGRAPHY
Sylvia FrÂ¨uhwirth-Schnatter and Rudolf FrÂ¨uhwirth.
Data augmentation and
mcmc for binary and multinomial logit models. In Statistical Modelling and
Regression Structures, pages 111â€“132. Physica-Verlag HD, Dec 2009.
doi:
10.1007/978-3-7908-2413-1 7.
Andrew Gelman and Jennifer Hill.
Data analysis using regression and multi-
level/hierarchical models. Cambridge University Press, 2006.
Andrew Gelman, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su.
A
weakly informative default prior distribution for logistic and other regression
models.
Ann. Appl. Stats., 2(4):1360â€“1383, 2008.
ISSN 1932-6157.
doi:
10.1214/08-AOAS191. URL http://dx.doi.org/10.1214/08-AOAS191.
A. Genz. Numerical Computation of Multivariate Normal Probabilities. J. Com-
put. Graph. Statist., 1(2):141â€“149, June 1992.
A. Genz and F. Bretz. Computation of Multivariate Normal and t Probabilities,
volume 195 of Lecture Notes in Statistics. Springer, 2009.
Edward I. George and Robert E. McCulloch. Variable selection via gibbs sampling.
J. Am. Statist. Assoc., 88(423):881â€“889, sep 1993a. doi: 10.1080/01621459.1993.
10476353. URL http://dx.doi.org/10.1080/01621459.1993.10476353.
E.I. George and R.E. McCulloch. Variable selection via Gibbs sampling. J. Am.
Statist. Assoc., 88(423):pp. 881â€“889, 1993b.
J. Geweke. Eï¬ƒcient simulation from the multivariate normal and student-t dis-
tributions subject to linear constraints. Computing Science and Statistics, 23:
571â€“578, 1991.
G. J. Gibson, C. A. Glasbey, and D. A. Elston. Monte-carlo evalution of multivari-
ate normal integrals and sensitivity to variate ordering. Advances in Numerical
Methods & applications, pages 120â€“126, 1994.
Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian
Monte Carlo methods. J. R. Statist. Soc. B, 73(2):123â€“214, 2011.
Neil J Gordon, David J Salmond, and Adrian FM Smith.
Novel approach to
nonlinear/non-gaussian bayesian state estimation. In IEE Proceedings F (Radar
and Signal Processing), volume 140, pages 107â€“113. IET, 1993.
Robert B. Gramacy and Nicholas G. Polson. Simulation-based regularized logistic
regression. 7(3):567â€“590, Sep 2012. doi: 10.1214/12-ba719.
180

BIBLIOGRAPHY
P. J. Green.
Reversible Jump Markov Chain Monte Carlo computation and
Bayesian Model Determination. Biometrika, 82(4):711â€“732, 1995.
P. J. Green, K. Latuszynski, M. Pereyra, and C. P. Robert. Bayesian computa-
tion: a perspective on the current state, and sampling backwards and forwards.
Preprint arXiv:1502.01148, 2015.
B. Guedj and P. Alquier. PAC-Bayesian estimation and prevision in sparse additive
models. Electronic Journal of Statistics, 7:264â€“291, 2013.
V. Hajivassiliou, D. McFadden, and P. Ruud.
Simulation of multivariate nor-
mal rectangle probabilities and their derivatives theoretical and computational
results. Journal of Econometrics, 72(1-2):85â€“134, Mayâ€“June 1996.
Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and
R Tibshirani. The elements of statistical learning, volume 2. Springer, 2009.
D. Hernandez-Lobato, J. Hernandez-Lobato, and P. Dupont. Generalized Spike-
and-Slab Priors for Bayesian Group Feature Selection Using Expectation Prop-
agation . J. Mach. Learn. Res., 14:1891â€“1945, 2013.
Y. Hochberg and A. C. Tamhane. Multiple comparison procedures. John Wiley &
Sons, Inc., 1987.
W. Hoeï¬€ding. Probability Inequalities for Sums of Random Variables. Ann. Math.
Stat., 10:293â€“325, 1948.
M. Hoï¬€man and A. Gelman. The no-U-turn sampler: Adaptively setting path
lengths in Hamiltonian monte carlo.
J. Mach. Learn. Res., page (in press),
2013.
M. D. Hoï¬€man, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational infer-
ence. The Journal of Machine Learning Research, 14(1):1303â€“1347, 2013.
C. Holmes and L. Held. Bayesian auxiliary variable models for binary and multi-
nomial regression. 1(1):145â€“168, 2006.
Wolfgang HÂ¨ormann and Josef Leydold. Quasi importance sampling. Technical
report, 2005.
P. Jacob, C. P. Robert, and M. H. Smith. Using parallel computation to improve
independent metropolisâ€“hastings based estimation. J. Comput. Graph. Statist.,
20(3):616â€“635, Jan 2011. doi: 10.1198/jcgs.2011.10167.
181

BIBLIOGRAPHY
P.E. Jacob, L. Murray, and S. Rubenthaler.
Path storage in the particle ï¬l-
ter.
Statist. Comput., pages 1â€“10, 2013.
ISSN 0960-3174.
doi:
10.1007/
s11222-013-9445-x. URL http://dx.doi.org/10.1007/s11222-013-9445-x.
A. Jasra, D. Stephens, and C. Holmes. On population-based simulation for static
inference. Statist. Comput., 17(3):263â€“279, 2007.
A. Jasra, D. Stephens, A. A. Doucet, and T. Tsagaris. Inference for LÂ´evy driven
stochastic volatility models via Sequential Monte Carlo. Scand. J. of Statist.,
38(1), 2011a.
Ajay Jasra, David A Stephens, Arnaud Doucet, and Theodoros Tsagaris. Inference
for LÂ´evy-Driven Stochastic Volatility Models via Adaptive Sequential Monte
Carlo. Scandinavian Journal of Statistics, 38(1):1â€“22, 2011b.
W. Jiang and M. A. Tanner.
Gibbs posterior for variable selection in high-
dimensional classiï¬cation and data mining.
The Annals of Statistics, 36(5):
2207â€“2231, 2008.
M. I. Jordan, Z. Ghahrapani, T. S. Jaakkola, and L. K. Saul. An introduction
to variational methods for graphical models. Machine Learning, (37):183â€“233,
1999.
Pasi JylÂ¨anki, Jarno Vanhatalo, and Aki Vehtari. Robust gaussian process regres-
sion with a student-t likelihood. The Journal of Machine Learning Research,
12:3227â€“3257, 2011.
Ata KabÂ´an. On bayesian classiï¬cation with laplace priors. Pattern Recognition
Letters, 28(10):1271â€“1282, 2007. doi: 10.1016/j.patrec.2007.02.010.
M. Keane. Simulation estimation for panel data models with limited dependent
variables. MPRA Paper 53029, University Library of Munich, Germany, 1993.
M. E. Khan. Decoupled variational Gaussian inference. In Advances in Neural
Information Processing Systems, pages 1547â€“1555, 2014.
V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear-norm penalization and
optimal rates for noisy low-rank matrix completion. The Annals of Statistics,
39(5):2302â€“2329, 2011.
Augustine Kong, Jun S Liu, and Wing Hung Wong. Sequential imputations and
bayesian missing data problems. Journal of the American statistical association,
89(425):278â€“288, 1994.
182

BIBLIOGRAPHY
Demetris Lamnisos, Jim E. Griï¬ƒn, and Mark F. J. Steel. Adaptive Monte Carlo
for Bayesian variable selection in regression models. J. Comput. Graph. Statist.,
22(3):729â€“748, 2013. ISSN 1061-8600. doi: 10.1080/10618600.2012.694756. URL
http://dx.doi.org/10.1080/10618600.2012.694756.
N. D. Lawrence and R. Urtasun. Non-linear matrix factorization with Gaussian
processes. In Proceedings of the 26th Annual International Conference on Ma-
chine Learning, pages 601â€“608. ACM, 2009.
G. LecuÂ´e. MÂ´ethodes dâ€™agrÂ´egation: optimalitÂ´e et vitesses rapides. Ph.D. thesis,
UniversitÂ´e Paris 6, 2007.
Anthony Lee, Christopher Yau, Michael B. Giles, Arnaud Doucet, and Christo-
pher C. Holmes. On the utility of graphics cards to perform massively parallel
simulation of advanced Monte Carlo methods. J. Comput. Graph. Statist., 19
(4):769â€“789, jan 2010.
doi: 10.1198/jcgs.2010.10039.
URL http://dx.doi.
org/10.1198/jcgs.2010.10039.
Christiane Lemieux.
Monte Carlo and Quasi-Monte Carlo Sampling (Springer
Series in Statistics). Springer, February 2009. ISBN 0387781641.
J. P. LeSage, Pace R. K., N. Lam, R. Campanella, and Liu X.
New Orleans
buisness recovery in the aftermath of huricane Katrina.
App. Stat., 174(4):
1007â€“1027, October 2011.
Y. J. Lim and Y. W. Teh. Variational Bayesian approach to movie rating predic-
tion. Proceedings of KDD Cup and Workshop, 7:15â€“21, 2007.
D. J. C. MacKay. Information theory, inference and learning algorithms. Cam-
bridge University Press, 2002.
T. T. Mai and P. Alquier. A Bayesian approach for matrix completion: optimal
rate under general sampling distribution. Electronic Journal of Statistics, 9:
823â€“841, 2015.
E. Mammen and A. Tsybakov. Smooth discrimination analysis. Ann. Stat., 27(6):
1808â€“1829, 12 1999.
P. Massart. Concentration Inequalities and Model Selection, volume 1896. Springer
Lecture Notes in Mathematics, 2007.
D. A. McAllester. PAC-Bayesian model averaging. In Proceedings of of the Twelth
Annual Conference On Computational Learning Theory, Santa Cruz, California
(Electronic), pages 164â€“170. ACM, New-York, 1999.
183

BIBLIOGRAPHY
D.A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh an-
nual conference on Computational learning theory, pages 230â€“234. ACM, 1998.
S. Meyn and R. L. Tweedie. Markov chains and Stochastic Stability. Cambridge
University Press, 2nd edition, 2009.
T. Minka. Expectation Propagation for approximate Bayesian inference. In Proc.
17th Conf. Uncertainty Artiï¬cial Intelligence, UAI â€™01, pages 362â€“369. Morgan
Kaufmann Publishers Inc., 2001a.
T.P. Minka. Expectation Propagation for approximate Bayesian inference. Pro-
ceedings of Uncertainty in Artiï¬cial Intelligence, 17:362â€“369, 2001b.
T. Minwa, A. J. Hayter, and S. Kuriki. The evaluation of general non-centered
orthant pro. J. R. Statist. Soc. B, 65:223â€“234, 2003.
T. J Mitchell and J. Beauchamp. Bayesian variable selection in linear regression.
J. Am. Statist. Assoc., 83(404):1023â€“1032, 1988.
M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte
Carlo, page 51, 2010a.
R. Neal. Annealed importance sampling. Statist. Comput., 11(2):125â€“139, 2001a.
R. M. Neal. Annealed importance sampling. Statist. Comput., 11:125â€“139, 2001b.
R. M. Neal.
MCMC using Hamiltonian dynamics.
In S. Brooks, A. Gelman,
G. L. Jones, and X.-L. Meng, editors, Handbook of Markov Chain Monte Carlo,
pages 113â€“162. Chapman & Hall / CRC Press, 2010b. URL http://www.cs.
utoronto.ca/~radford/ftp/ham-mcmc.pdf.
Y. Nesterov. Introductory lectures on convex optimization, volume 87. Springer
Science & Business Media, 2004.
H. Nickisch and C.E. Rasmussen. Approximations for Binary Gaussian Process
Classiï¬cation. J. Mach. Learn. Res., 9(10):2035â€“2078, October 2008.
H. Niederreiter. Quasi-Monte Carlo methods and pseudo-random numbers. Bul-
letin of the american mathematical society, 84(6):957â€“1041, November 1978.
M. Opper and C. Archambeau. The variational Gaussian approximation revisited.
Neural computation, 21(3):786â€“792, 2009.
M. Opper and O. Winther.
Gaussian Processes for Classiï¬cation: Mean-ï¬eld
Algorithms. Neural Computation, 12(11):2655â€“2684, November 2000.
184

BIBLIOGRAPHY
A. Pakman and L. Paninski. Exact Hamiltonian Monte Carlo for Truncated Mul-
tivariate Gaussians. arXiv:1208.4118, pages 1â€“30, 2012.
M.D. Pandey. An eï¬€ective approximation to evaluate multinormal integrals. Struc-
tural Safety, 20(1):51â€“67, 1998.
G. Parisi. Statistical ï¬eld theory. Addison-Wesley, New-York, 1988.
M. Pitt and N. Shephard. Filtering via simulation: Auxiliary particle ï¬lters. J.
Am. Statist. Assoc., 94(446):590â€“599, June 1999.
Nicholas G. Polson, James G. Scott, and Jesse Windle. Bayesian inference for
logistic models using pÂ´olyaâ€“gamma latent variables. Journal of the American
Statistical Association, 108(504):1339â€“1349, Dec 2013. doi: 10.1080/01621459.
2013.829001. URL http://dx.doi.org/10.1080/01621459.2013.829001.
A. Prekopa. On probabilistic constrained programming. In Proceedings of the
Princeton symposium on mathematical programming, pages 113â€“138. Princeton,
New Jersey: Princeton University Press, 1970.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical
Recipes: The Art of Scientiï¬c Computing. Cambridge University Press, 2007.
C. Rasmussen and C. Williams. Gaussian processes for Machine Learning. MIT
press, 2006.
James Ridgway. Computation of Gaussian orthant probabilities in high dimension.
(Minor revision) Statist. Comput.., 2015.
S. Robbiano. Upper bounds and aggregation in bipartite ranking. Elec. J. of Stat.,
7:1249â€“1271, 2013.
H. Robbins and S. Monro. A stochastic approximation method. The annals of
mathematical statistics, pages 400â€“407, 1951.
C. P. Robert. Simulation of truncated normal variables. Statist. Comput., 5(2):
121â€“125, 1995.
C. P. Robert and G. Casella. Monte Carlo Statistical Methods, 2nd ed. Springer-
Verlag, New York, 2004a.
C. P. Robert and G. Casella. Monte Carlo Statistical Methods, chapter 9. Springer,
2004b.
Christian Robert. The Bayesian choice: from decision-theoretic foundations to
computational implementation. Springer Science & Business Media, 2007.
185

BIBLIOGRAPHY
G. Roberts and J. Rosenthal.
Optimal scaling of discrete approximations to
langevin diï¬€usions. J. R. Statist. Soc. B, 60(1):255â€“268, 1998.
Gareth O. Roberts and Jeï¬€rey S. Rosenthal.
Optimal Scaling for Various
Metropolis-Hastings Algorithms. Statist. Science, 16(4):351â€“367, 2001. ISSN
08834237. doi: 10.1214/ss/1015346320.
Gareth O Roberts and Jeï¬€rey S Rosenthal. General state space Markov chains
and MCMC algorithms. Probability Surveys, 1:20â€“71, 2004.
V. RoË‡ckovÂ´a and E. George. Emvs: The EM approach to bayesian variable selection.
J. Am. Statist. Assoc., 2013.
H. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations.
J. R.
Statist. Soc. B, 71(2):319â€“392, 2009.
R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using
Markov chain Monte Carlo. In Proceedings of the 25th international conference
on Machine learning, pages 880â€“887. ACM, 2008.
C. SchÂ¨afer and N. Chopin. Sequential monte carlo on large binary sampling spaces.
Statistics and Computing, pages 1â€“22, 2011.
Christian SchÂ¨afer.
Monte Carlo methods for sampling high-dimensional binary
vectors. PhD thesis, UniversitÂ´e Paris Dauphine, 2012.
Steven L. Scott, Alexander W. Blocker, and Fernando V. Bonassi. Bayes and big
data: The consensus monte carlo algorithm. In Bayes 250, 2013.
M. Seeger. Expectation propagation for exponential families. Technical report, U.
of California, 2005a.
M. Seeger. Expectation Propagation for Exponential Families. Technical report,
Univ. California Berkeley, 2005b.
Babak Shahbaba, Shiwei Lan, Wesley O Johnson, and Radford M Neal. Split
hamiltonian monte carlo. Statist. Comput., pages 1â€“11, 2011. doi: 10.1007/
s11222-012-9373-1. URL http://arxiv.org/pdf/1106.5941.pdf.
J. Shawe-Taylor and R.C. Williamson. A PAC analysis of a Bayesian estimator.
In Proc. conf. Computat. learn. theory, pages 2â€“9. ACM, 1997.
J. Skilling. Nested sampling for general Bayesian computation. Bayesian Analysis,
1(4):833â€“860, 2006.
186

BIBLIOGRAPHY
Marc A. Suchard, Quanli Wang, Cliburn Chan, Jacob Frelinger, Andrew Cron,
and Mike West. Understanding GPU programming for statistical computation:
Studies in massively parallel massive mixtures. J. Comput. Graph. Statist., 19
(2):419â€“438, jan 2010.
doi: 10.1198/jcgs.2010.10016.
URL http://dx.doi.
org/10.1198/jcgs.2010.10016.
T. Suzuki. Convergence rate of Bayesian tensor estimator: Optimal rate with-
out restricted strong convexity. arXiv preprint arXiv:1408.3092 (accepted by
ICML2015), 2014.
L. Tierney, R. E. Kass, and J. B. Kadane. Fully exponential Laplace approxima-
tions to expectations and variances of non-positive functions. J. Am. Statist.
Assoc., 84:710â€“716, 1989.
Luke Tierney and Joseph B Kadane. Accurate approximations for posterior mo-
ments and marginal densities. J. Am. Statist. Assoc., 81(393):82â€“86, 1986.
K. E. Train.
Discrete Choice methods with simulation.
Cambridge University
Press, 2009.
A. Tsybakov. Optimal aggregation of classiï¬ers in statistical learning. The Annals
of Statistics, 32(1):135â€“166, 2004.
A. Van der Vaart. Asymptotic statistics. Cambridge university press, 1998.
A.W. van der Vaart and J.H. van Zanten. Adaptive Bayesian estimation using
a Gaussian random ï¬eld with inverse Gamma bandwidth. Ann. Stat., pages
2655â€“2675, 2009.
M. A.J. Van Gerven, B. Cseke, F. P. de Lange, and T. Heskes. Eï¬ƒcient Bayesian
multivariate fMRI analysis using a sparsifying spatio-temporal prior. NeuroIm-
age, 50:150â€“161, 2010.
Xiangyu Wang and David B Dunson. Parallelizing MCMC via Weierstrass sampler.
arXiv preprint arXiv:1312.4605, 2013.
O. Wintenberger. Deviation inequalities for sums of weakly dependent time series.
Electronic Communications in Probability, 15:489â€“503, 2010.
L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz. Optimizing classiï¬er perfor-
mance via an approximation to the Wilcoxon-Mann-Whitney statistic. Proc.
20th Int. Conf. Mach. Learn., pages 848â€“855, 2003.
Y. Yang. Aggregating regression procedures to improve performance. Bernoulli,
10:25â€“47, 2004.
187

BIBLIOGRAPHY
G. Yao and U Bockenholt. Bayesian estimation of Thurstonian ranking models
based on the Gibbs sampler. British Journal of Mathematical and Statistical
Psychology, 52:79â€“92, 1999.
A. Yuille. Belief Propagation, Mean-ï¬eld and the Bethe approximation. Technical
report, Dept. Statistics UCLA.
T. Zhang. Statistical behavior and consistency of classiï¬cation methods based on
convex risk minimization. Annals of Statistics, pages 56â€“85, 2004.
T. Zhang. Information theoretical upper and lower bounds for statistical estima-
tion. IEEE Transaction on Information Theory, 52:1307â€“1321, 2006.
M. Zhou, C. Wang, M. Chen, J. Paisley, D. Dunson, and L. Carin. Nonparametric
bayesian matrix completion. Proc. IEEE SAM, 2010.
188

