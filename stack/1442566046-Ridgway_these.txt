HAL Id: tel-01230851
https://tel.archives-ouvertes.fr/tel-01230851
Submitted on 3 Dec 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Advances in computational Bayesian statistics and the
approximation of Gibbs measures
James Ridgway
To cite this version:
James Ridgway.
Advances in computational Bayesian statistics and the approximation of Gibbs
measures.
Statistics [math.ST]. Université Paris Dauphine - Paris IX, 2015.
English.
￿NNT :
2015PA090030￿. ￿tel-01230851￿

Universit´e Paris-Dauphine
´Ecole Doctorale de Dauphine
Centre de Recherche en Math´ematiques de la D´ecision
Th`ese present´ee par:
James L.P. Ridgway
Pour obtenir le grade de:
Docteur en Mathematiques Appliqu´ees
Sp´ecialit´e: Statistiques
ADVANCES IN COMPUTATIONAL
BAYESIAN STATISTICS AND THE
APPROXIMATION OF GIBBS MEASURES
Jury compos´e de:
M. Christophe ANDRIEU
Bristol University
Rapporteur
M. Olivier CATONI
CNRS CREST-ENSAE
Rapporteur
M. Nicolas CHOPIN
CREST-ENSAE
Directeur de Th`ese
M. Randal DOUC
Telecom SudParis
Examinateur
M. Erwan LE PENNEC
´Ecole Polytechnique
Examinateur
M. Christian ROBERT
Universit´e Paris Dauphine
Examinateur


Remerciements
Je voudrais tout d’abord remercier vivement mon directeur de th`ese, Nicolas
Chopin, pour son engagement, sa disponibilit´e et son enthousiasme contagieux
lors de nos discussions scientiﬁques. Pour toutes ces raisons, travailler sous sa
direction a ´et´e une exprience tr`es agr´eable et enrichissante.
J’adresse ´egalement mes remerciements `a Randal Douc, Erwan Le Pennec et
Christian Robert d’avoir accept´e de participer `a mon jury de th`ese ainsi qu’`a
Christophe Andrieu et Olivier Catoni pour avoir accept´e de la rapporter et pour
leur lecture attentive et leurs commentaires constructifs.
Je remercie les doctorants ayant s´ejourn´e au bureau E28: Ad´ela¨ıde, Edwin, JB
et Mathieu ainsi que ceux du CREST et de Dauphine: Clara, Marco, Medhi, Pierre
et Vincent qui ont contribu´e `a rendre ces trois ann´ees agr´eables.
Ma gratitude va ´egalement au CREST pour son environnement de travail tr`es
stimulant. Les chercheurs, notamment Pierre Alquier, Arnak Dalalyan et Judith
Rousseau, ont toujours t disponibles et m’ont beaucoup appris sur le plan scien-
tiﬁque. Merci aussi au CREST d’avoir mis la salle caf´e `a cˆot´e de notre bureau,
d´ecuplant ainsi notre productivit´e!
Ma reconnaissance va ´egalement `a mes parents et `a Vicky pour leur soutien et
les dimanches soirs pass´es en leur compagnie. Je pense aussi `a mes amis (partic-
uli`erement Zago, Seb, Yann et Khalid ) grˆace auxquels j’ai pass´e de bons moments
tout au long de cette aventure. Je n’oublie pas non plus les joueurs RC Val de
Bi`evre pour toutes les victoires qui ont rythm´e ces trois derni`eres ann´ees. Enﬁn,
je d´edie ce m´emoire de th`ese `a C´eline dont le soutien ne m’a jamais fait d´efaut.
3


Contents
1
Introduction
1
1.1
Bayesian statistics
. . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Risk minimizer
. . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.2
Model choice
. . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
PAC-Bayesian Bounds
. . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2.1
Minimizing the bound
. . . . . . . . . . . . . . . . . . . . .
6
1.2.2
PAC Bayesian oracle inequality . . . . . . . . . . . . . . . .
7
1.3
Computational aspects . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.1
Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.2
Approximate Inference . . . . . . . . . . . . . . . . . . . . .
22
1.4
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.4.1
Leave Pima indians alone: binary regression as a benchmark
for Bayesian computation
. . . . . . . . . . . . . . . . . . .
27
1.4.2
Computation of Gaussian orthant probabilities in high di-
mension . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.4.3
Theoretical and computational aspects of PAC Bayesian rank-
ing and scoring . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.4.4
Properties of variational approximations of Gibbs posteriors
29
1.4.5
Towards automatic calibration of SMC2 . . . . . . . . . . . .
30
2
Leave Pima indians alone: binary regression as a benchmark for Bayesian
computation
31
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.2
Preliminaries: binary regression models . . . . . . . . . . . . . . . .
33
2.2.1
Likelihood, prior
. . . . . . . . . . . . . . . . . . . . . . . .
33
5

Contents
2.2.2
Posterior maximisation (Gaussian prior)
. . . . . . . . . . .
34
2.2.3
Posterior maximisation (Cauchy prior) . . . . . . . . . . . .
35
2.3
Fast approximation methods . . . . . . . . . . . . . . . . . . . . . .
36
2.3.1
Laplace approximation . . . . . . . . . . . . . . . . . . . . .
36
2.3.2
Improved Laplace, connection with INLA . . . . . . . . . . .
37
2.3.3
The EM algorithm of Gelman et al. [2008] (Cauchy prior)
.
38
2.3.4
Expectation-Propagation . . . . . . . . . . . . . . . . . . . .
38
2.3.5
Discussion of the diﬀerent approximation schemes . . . . . .
40
2.4
Exact methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.4.1
Our gold standard: Importance sampling . . . . . . . . . . .
41
2.4.2
Improving importance sampling by Quasi-Monte Carlo . . .
43
2.4.3
MCMC
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.4.4
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . .
49
2.5
Numerical study . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.5.1
Datasets of moderate size
. . . . . . . . . . . . . . . . . . .
51
2.5.2
Bigger datasets . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.6
Variable selection . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.6.1
SMC algorithm of Sch¨afer and Chopin [2011] . . . . . . . . .
61
2.6.2
Adaptation to binary regression . . . . . . . . . . . . . . . .
62
2.6.3
Numerical illustration
. . . . . . . . . . . . . . . . . . . . .
62
2.6.4
Spike and slab . . . . . . . . . . . . . . . . . . . . . . . . . .
64
2.7
Conclusion and extensions . . . . . . . . . . . . . . . . . . . . . . .
64
2.7.1
Our main messages to users . . . . . . . . . . . . . . . . . .
64
2.7.2
Our main message to Bayesian computation experts . . . . .
65
2.7.3
Big data and the p3 frontier . . . . . . . . . . . . . . . . . .
65
2.7.4
Generalising to other models . . . . . . . . . . . . . . . . . .
66
3
Computation of Gaussian orthant probabilities in high dimension
67
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
3.2
Geweke-Hajivassiliou-Keane (GHK) simulator . . . . . . . . . . . .
69
3.3
The Markovian case . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.3.1
Toy example . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.3.2
Particle ﬁlter (PF) . . . . . . . . . . . . . . . . . . . . . . .
71
3.4
Non Markovian case
. . . . . . . . . . . . . . . . . . . . . . . . . .
75
3.4.1
Variable ordering . . . . . . . . . . . . . . . . . . . . . . . .
76
3.4.2
A sequential Monte Carlo (SMC) algorithm
. . . . . . . . .
77
3.4.3
Move steps
. . . . . . . . . . . . . . . . . . . . . . . . . . .
79
3.5
Extentions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.5.1
Student Orthant
. . . . . . . . . . . . . . . . . . . . . . . .
83
3.5.2
SMC as a truncated distribution sampler . . . . . . . . . . .
84
6

Contents
3.6
Numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
3.6.1
Covariance simulation, tunning parameters . . . . . . . . . .
85
3.6.2
GHK for moderate dimensions . . . . . . . . . . . . . . . . .
86
3.6.3
High dimension orthant probabilities . . . . . . . . . . . . .
88
3.6.4
Student orthant probabilities
. . . . . . . . . . . . . . . . .
89
3.6.5
Application to random utility models . . . . . . . . . . . . .
89
3.7
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
Appendices
92
3.A Proof of proposition 2.1
. . . . . . . . . . . . . . . . . . . . . . . .
92
3.B Resampling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.C Variable Ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.D Hamiltonian Monte Carlo
. . . . . . . . . . . . . . . . . . . . . . .
95
4
Theoretical and computational aspects of PAC Bayesian ranking and
scoring
97
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
4.2
Theoretical bounds from the PAC-Bayesian Approach . . . . . . . .
98
4.2.1
Notations
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4.2.2
Assumptions and general results . . . . . . . . . . . . . . . .
99
4.2.3
Independent Gaussian Prior . . . . . . . . . . . . . . . . . . 100
4.2.4
Spike and slab prior for feature selection . . . . . . . . . . . 101
4.3
Practical implementation of the PAC-Bayesian approach
. . . . . . 101
4.3.1
Choice of hyper-parameters
. . . . . . . . . . . . . . . . . . 101
4.3.2
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . . 102
4.3.3
Expectation-Propagation (Gaussian prior) . . . . . . . . . . 104
4.3.4
Expectation-Propagation (spike and slab prior)
. . . . . . . 105
4.4
Extension to non-linear scores . . . . . . . . . . . . . . . . . . . . . 106
4.5
Numerical Illustration
. . . . . . . . . . . . . . . . . . . . . . . . . 106
4.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
Appendices
110
4.A PAC-Bayes bounds for linear scores . . . . . . . . . . . . . . . . . . 110
4.A.1
Suﬃcient condition for Dens(c)
. . . . . . . . . . . . . . . . 110
4.A.2
Proof of Lemma 2.1 . . . . . . . . . . . . . . . . . . . . . . . 110
4.A.3
Proof of Theorem 2.3 (Independent Gaussian prior) . . . . . 114
4.A.4
Proof of Theorem 2.4 (Independent Gaussian prior) . . . . . 115
4.A.5
Proof of Theorem 2.5 (Spike and slab prior for feature selec-
tion) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
4.B Practical implementation of the PAC-Bayesian approach
. . . . . . 117
4.B.1
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . . 117
7

Contents
4.B.2
Expectation-Propagation (Gaussian prior) . . . . . . . . . . 117
4.B.3
Expectation-Propagation (spike and slab prior)
. . . . . . . 119
4.C Numerical illustration . . . . . . . . . . . . . . . . . . . . . . . . . . 120
5
Properties of variational approximations of Gibbs posteriors
123
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
5.2
PAC-Bayesian framework . . . . . . . . . . . . . . . . . . . . . . . . 125
5.3
Numerical approximations of the pseudo-posterior . . . . . . . . . . 127
5.3.1
Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.3.2
Variational Bayes . . . . . . . . . . . . . . . . . . . . . . . . 127
5.4
General results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.4.1
Bounds under the Hoeﬀding assumption
. . . . . . . . . . . 129
5.4.2
Bounds under the Bernstein assumption
. . . . . . . . . . . 130
5.5
Application to classiﬁcation
. . . . . . . . . . . . . . . . . . . . . . 131
5.5.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.5.2
Three sets of Variational Gaussian approximations
. . . . . 132
5.5.3
Theoretical analysis . . . . . . . . . . . . . . . . . . . . . . . 132
5.5.4
Implementation and numerical results . . . . . . . . . . . . . 134
5.6
Application to classiﬁcation under convexiﬁed loss . . . . . . . . . . 134
5.6.1
Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . 135
5.6.2
Numerical application
. . . . . . . . . . . . . . . . . . . . . 136
5.7
Application to ranking . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.7.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.7.2
Theoretical study . . . . . . . . . . . . . . . . . . . . . . . . 139
5.7.3
Algorithms and numerical results . . . . . . . . . . . . . . . 140
5.8
Application to matrix completion . . . . . . . . . . . . . . . . . . . 141
5.8.1
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.9
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
Appendices
145
5.A Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.A.1
Preliminary remarks
. . . . . . . . . . . . . . . . . . . . . . 145
5.A.2
Proof of the theorems in Subsection 5.4.1 . . . . . . . . . . . 146
5.A.3
Proof of Theorem 5.4.3 (Subsection 5.4.2)
. . . . . . . . . . 148
5.A.4
Proofs of Section 5.5 . . . . . . . . . . . . . . . . . . . . . . 149
5.A.5
Proofs of Section 5.6 . . . . . . . . . . . . . . . . . . . . . . 151
5.A.6
Proofs of Section 5.7 . . . . . . . . . . . . . . . . . . . . . . 153
5.A.7
Proofs of Section 5.8 . . . . . . . . . . . . . . . . . . . . . . 155
5.B Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . 156
5.B.1
Sequential Monte Carlo . . . . . . . . . . . . . . . . . . . . . 156
5.B.2
Optimizing the bound
. . . . . . . . . . . . . . . . . . . . . 158
8

Contents
5.C Stochastic gradient descent . . . . . . . . . . . . . . . . . . . . . . . 160
6
Towards the automatic calibration of the number of particles in SMC2161
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
6.2
Background on SMC2 . . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.2.1
IBIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.2.2
SMC2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
6.2.3
PMCMC moves . . . . . . . . . . . . . . . . . . . . . . . . . 166
6.2.4
Choosing Nx
. . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.3
Proposed approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
6.3.1
Particle Gibbs and memory cost . . . . . . . . . . . . . . . . 168
6.3.2
Nonparametric estimation of Nx . . . . . . . . . . . . . . . . 169
6.3.3
Additional considerations
. . . . . . . . . . . . . . . . . . . 170
6.4
Numerical example . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
9


Resum´e
Ce m´emoire de th`ese traite de plusieurs m´ethodes de calcul d’estimateur en statis-
tiques bay´esiennes. Le premier chapitre consiste en une br`eve introduction des
th`ematiques abord´ees. Nous en donnons ici une premi`ere vue d’ensemble.
Plusieurs approches d’estimation seront consid´er´ees dans ce manuscrit. D’abord
en estimation nous consid´ererons une approche standard dans le paradigme bayesien
en utilisant des estimateurs sous la forme d’int´egrales par rapport `a des lois a poste-
riori. Dans un deuxi`eme temps nous relacherons les hypoth`eses faites dans la phase
de mod´elisation. Nous nous int´eresserons alors `a l’´etude d’estimateurs r´epliquant
les propri´et´es statistiques du minimiseur du risque de classiﬁcation ou de ranking
th´eorique et ceci sans mod´elisation du processus g´en´eratif des donn´ees.
Dans les deux approches, et ce malgr´e leur dissemblance, le calcul num´erique des
estimateurs n´ecessite celui d’int´egrales de grande dimension. La plus grande partie
de cette th`ese est consacr´ee au d´eveloppement de telles m´ethodes dans quelques
contextes sp´eciﬁques.
Nous diviserons les algorithmes en deux grandes classes. D’abord les algorithmes
de Monte Carlo bas´es sur la g´en´eration de variables al´eatoires dans l’´epigraphe de
l’int´egrande. Ces derniers permettent le calcul d’int´egrales dans la mesure ou nous
arrivons `a g´en´erer eﬃcacement de tels points. Le chapitre 2 compare certaines
des m´ethodes couramment utilis´ees pour l’estimation bay´esienne de mod`eles pro-
bit. Nous y donnons des recommandations sur la m´ethodologie `a adopter. Dans
le chapitre 3 nous d´evelopperons un algorithme permettant le calcul de proba-
bilit´es gaussiennes de rectangles de grandes dimensions.
Ce chapitre traite un
probl`eme plus g´eneral que celui de l’estimation, cependant, nous pouvons lier la
probl´ematique `a celle de l’´evaluation de certaines vraisemblances.
La deuxi`eme classe d’algorithme que nous consid´erons consiste en l’approximation
de distributions par des distributions dont les moments peuvent ˆetre calcul´es ex-
11

Contents
plicitement. Il s’agira dans ce cas de d´eﬁnir une m´etrique et de trouver les ap-
proximations les plus proches dans une classe donn´ee. Dans le chapitre 4 nous
´etudierons les propriet´es des mesures de Gibbs pour r´epliquer les propri´et´es du
minimiseur d’un risque de ranking. Nous d´evelopperons ´egalement des m´ethodes
d’approximation pour ces lois. Dans le chapitre 5 nous nous int´eresserons plus
sp´eciﬁquement `a une mani`ere d’approcher les distributions et nous ´etudierons
les propri´et´es theoriques des approximations elles-mˆemes, i.e. leurs capacit´es `a
r´epliquer des propri´et´es du minimiseur du risque.
Une description plus d´etaill´ee de chaque chapitre est donn´ee en ﬁn d’introduction.
12

1
Introduction
In this thesis we study some computational aspects of Bayesian statistics, as well as
Gibbs posteriors. We describe both statistical approaches in the ﬁrst two sections.
We then give a brief overview of computational aspects linked to the implemention
of those estimators in Section 1.3 of this chapter.
This chapter is devoted to
discussing the diﬀerent issues that will arise in the manuscript; detailed accounts
will be found in the following chapters.
1.1 Bayesian statistics
Statistical analysis starts with a collection of probability distributions Pθ indexed
by a parameter θ ∈Θ, where Θ is an arbitrary set. In this thesis most examples will
be taken from parametric statistics, with Θ ⊂Rd. The statistician then confronts
the model to some observations on the probability space (X, A, {Pθ, θ ∈Θ}).
The goal of this thesis is not the choice of the collection of probability distribu-
tion but rather, given the model and the data, to discuss ways of improving the
computation of estimators of θ.
We describe in the following the Bayesian paradigm that will be used in this
manuscript. In particular we discuss the minimization of the integrated posterior
risk. As we will see, this criterion leads to computational diﬃculties because it
requires the evaluation of high dimensional integrals.
In Section 1.2, we will take another approach: we will not suppose a speciﬁc
probability model but rather minimize an empirical risk over classes of classiﬁers
and give theoretical guaranties for such an approach.
In all the following we suppose the model to be dominated by some measure
and we deﬁne the likelihood as the probability density of the observation given the
parameter.
1

1 Introduction
Deﬁnition 1.1.1 We call likelihood the probability density of the collection (X1:n) ∈
X n conditioned on θ. We denote it by L(X1:n|θ).
We give two examples of models that will be used in the rest of the thesis. Those
models will be used in the introduction as examples and will be further studied in
the diﬀerent chapters of this thesis.
Example 1.1.1 Probit. This is a special case of generalized linear models where
the probability of a binary random variable is expressed as a linear combination of
some covariates. Conditionally on θ and a vector of deterministic covariates x the
model can be expressed in a hierarchical form:
Yi = ✶Zi>0,
Zi|β ∼N(xiβ, 1).
Each observation is sampled independently from this model. The likelihood is given
by
L(y1:n|θ) =
n
Y
i=1
(Φ(xiθ))yi (1 −Φ(xiθ))1−yi .
This model will be studied in details in Chapter 2.
In Chapter 3 we give an
example of a model where we allow correlations between the (Zi)i and a way to
compute the likelihood.
Example 1.1.2 State space model (SSM). SSM is another model that we will use
in this thesis (see Chapter 6). This is a time series model with an unobserved
Markov process (xt)t≥0 with transition density
x0 ∼g0(.; θ);
xt|x1:t−1 ∼gt(.|xt−1; θ)
and an observation yt at each time t depending on the current value of the chain,
yt ∼ft(.|xt; θ).
The likelihood of the observations up to time T is given by
L(y1:T; θ) =
ˆ
TY
t=1
f(yt|xt; θ)gt(xt|xt−1; θ)µ0(x0)dx0:T.
The fact that the likelihood takes the form of an integral with respect to a Markov
process leads to computational issues. Those will be discussed in more details in
Chapter 6.
2

1.1 Bayesian statistics
The main idea of the Bayesian paradigm is to endow Θ with the structure of a
probability space (Θ, B, π) where the probability measure π is referred to as the
prior. We do not discuss how to choose π; discussion of this subject can be found
in Robert [2007].
Once we have deﬁned the prior, we can use the likelihood as a conditional
probability and using Bayes identity one can deﬁne the posterior distribution:
Deﬁnition 1.1.2 The posterior distribution is the distribution of the parameter
given the observations,
π(dθ|D) =
L(x1:n|θ)π(dθ)
´
Θ L(x1:n|θ′)π(dθ′).
We call the integral mπ(x1:n) :=
´
Θ L(x1:n|θ′)π(dθ′) the marginal likelihood.
The posterior is the distribution of the parameter given a ﬁxed observed sam-
ple. We derive in the following section criteria to choose an estimator using this
distribution.
1.1.1 Risk minimizer
Given a model {Pθ, θ ∈Θ}, the data and a prior π(dθ), with support Θ, the goal
of the statistician is to take a decision δ minimizing a risk. Several types of risk
can be considered and lead to diﬀerent estimators. We now give examples that are
used in this manuscript.
Once we have deﬁned the risk one cannot simply minimize over value of ℓbecause
of the unknown θ.
To deal with this issue within the Bayesian paradigm the
statistician deﬁnes the posterior risk.
Deﬁnition 1.1.3 The Posterior risk is the loss function integrated with respect to
the posterior distribution:
ρ(π, δ|D) =
ˆ
ℓ(θ, δ)π(dθ|D).
A Bayesian estimator is a minimizer of the posterior risk. The most usual loss
is the quadratic loss ℓ(δ, θ) = ∥θ −δ∥2
2; direct computation yields δπ = Eπ (θ|D).
An other common loss is the following 0-1 loss,
ℓ(θ, δ) = ✶Θ1(θ)✶δ=1 + ✶Θ2(θ)✶δ=0.
Direct computations yield:
δπ =
(
1 if Pπ(θ ∈Θ1|D) > Pπ(θ ∈Θ2|D)
0 otherwise
3

1 Introduction
In the light of those examples we see the main computational issue appear.
Those two examples show that the paradigm leads to non trivial integration prob-
lems. In both case one wants to compute:
´
Θ h(θ)π(dθ)L(θ|D)
´
Θ π(dθ)L(θ|D)
,
for some function h; i.e. an expectation with respect to a distribution that is
known only up to the normalizing constant mπ(D) =
´
Θ π(dθ)L(θ|D).
1.1.2 Model choice
We can encode model uncertainty by deﬁning the parameter space Θ = Q
j Θj×{j}
where {j} indexes the model and Θj is the parameter space of models j. The
standard approach in the Bayesian paradigm is to endow the space of model indexes
with a prior distribution and to treat it as an additional parameter. This is easily
encapsulated in the framework described in the previous section.
Most of the time the likelihood itself will not be tractable in this case. We can
still solve the problem using specially tailored MCMC algorithms [Green, 1995]. In
Chapter 2 we will propose an alternative to this for the case of covariate uncertainty
for probit models.
We have deﬁned the normalizing constant of the posterior in Deﬁnition 1.1.2 as
mπ(x) =
´
π(θ)L(x|θ)dθ. We write the likelihood of model j, mπ(x|j). We assign
a prior probability πj to model j. We end up with an integration problem with
respect to the following distribution,
π(j|D) =
πjmπ(x|j)
P
j πjmπ(x|j).
For covariate selection in a Gaussian linear model one can compute the marginal
likelihood provided that the prior is conjugate to the Gaussian family (i.e. a Gaus-
sian distribution). An example of the approach is given in Chopin and Shaeﬀer
[2011]. In general the marginal likelihood is not tractable. A way to get around
this issue is to replace it by an estimator (see Chapter 2 for an application to the
probit model).
1.2 PAC-Bayesian Bounds
When no clear mathematical model is available to the statistician or that existing
models are too costly to evaluate or to build, one may still be able to construct
a pseudo-posterior to replicate the behavior of the risk minimizer. We use for
4

1.2 PAC-Bayesian Bounds
this matter a Gibbs posterior (deﬁned bellow) for the chosen risk. For this pos-
terior we prove nonasymptotic bound. The idea originated in machine learning
(Shawe-Taylor and Williamson [1997],McAllester [1998]) as way to bound the the-
oretical risk in probability by a computable empirical quantity. We follow Catoni
[2007] and use the approach to get oracle inequalities on the risk integrated under
our given pseudo-posterior.
We observe a sample (X1, Y1), · · · , (Xn, Yn) taking values in X × Y where the
pairs (Xi, Yi) have the same distribution P.
The statistician deﬁnes a set of predictor {fθ : X →Y}. We suppose we also
have at our disposal a risk function R(θ) and its empirical counterpart rn(θ).
The approach is summarized for classiﬁcation in this section. That is we take
Y = {−1, 1}, R(θ) = P (Y fθ(X) ≤0) and rn(θ) = 1
n
Pn
i=1 ✶Yifθ(Xi)≤0. In the fol-
lowing we also suppose linear classiﬁers fθ(x) = 2✶<x,θ>≥0 −1 ∈{−1, 1}. More
details on the derivation of the estimator and theoretical results are given in Chap-
ter 4 and 5.
Deﬁnition 1.2.1 The Gibbs posterior for an empirical risk rn(θ) and a prior πξ
is given by
ˆρλ(dθ) =
1
Zλ,ξ
exp {−λrn(θ)} πξ(dθ)
where ξ is the vector of hyperparameters, and Zξ,λ =
´
Θ e−λrn(θ)πξ(dθ).
Contrarily to the posterior deﬁned in the preceding section we do not derive this
probability density from Bayes’ formula. The Gibbs posterior deﬁned in deﬁnition
1.2.1 is the solution of the following variational problem:
−log Zλ,ξ = inf
ρ∈M1
+
{λρ(rn(θ)) + K(ρ, πξ)} ,
where M1
+ is the set of all probability measures. This is easily deduced from the
following equality: for any ρ ∈M1
+
−log Zλ,ξ + K(ρ, ˆρλ) = λρ(rn(θ)) + K(ρ, πξ).
Part of this thesis is concerned with proving oracle inequalities for those mea-
sures and providing approximations to the Gibbs posterior.
We give a quick
overview of an approach to derive bounds for the expected risk under a Gibbs
posterior. We follow Catoni [2007] and refer the reader to this reference for a
complete account and tighter bounds.
We give a simpliﬁed proof for the classiﬁcation loss, to give an idea of the tools
used in Chapter 4 and 5. In Chapter 4 we will extend these results to the case of
an AUC loss. The loss will be used to propose a method to rank instances from
bipartite data.
5

1 Introduction
Using the above relation we derive the following results,
P
(
sup
ρ∈M1
+
λ (ρ(rn(θ)) −ρ(R(θ))) + K(ρ, πξ) + η ≥0
)
= P {log πξ [exp {−λ(rn(θ) −R(θ)) + η}] ≥0}
= P {πξ [exp {−λ(rn(θ) −R(θ)) + η}] ≥1}
≤Pπξ [exp {−λ(rn(θ) −R(θ)) + η}]
= πξP [exp {−λ(rn(θ) −R(θ)) + η}] .
Since rn(θ) is bounded from above, Hoeﬀding’s inequality allows us to write:
πP [exp {−λ(rn(θ) −R(θ)) + η}] ≤π

exp
 λ2
2n + η

see Lemma 2.2 an Theorem 2.1 in Boucheron et al. [2013]. A great part of the
eﬀort will be put on ﬁnding concentration inequality at this step. In Chapter 4
for instance we prove a Bernstein inequality for the AUC risk to this aim.
We put η = −λ2
2n −log 2
ǫ and get ∀ρ ∈M1
+ with probability less than ǫ/2:
λ (ρ(R(θ)) −ρ(rn(θ))) + K(ρ, π) −λ2
2n −log 2
ǫ ≥0
(1.1)
such that with probability at least 1−ǫ/2 we can upper bound the theoretical risk
under the Gibbs posterior
ˆρλ(Rθ) ≤inf
ρ∈M1
+

ρ(rn(θ)) + 1
λK(ρ, π)

+ λ
2n + 1
λ log 2
ǫ.
(1.2)
Chapter 4 will be concerned with extensions of the framework to rank data, and
with computational tools to compute approximations of the Gibbs posterior.
Notice that this intermediate result comes under the only hypothesis that the
data is iid.
In fact we can even weaken this assumption by taking a weakly
dependent sample (see for instance Alquier and Li [2012]).
1.2.1 Minimizing the bound
Another quantity of interest is the normalizing constant. In the framework de-
scribed previously we can write inequality (1.2) using the dual formulation intro-
duced in the beginning of the section w.p. at least 1 −ǫ
ˆρ(R(θ)) ≤−1
λ log Zλ,ξ + λ
2n + 1
λ log 2
ǫ.
6

1.2 PAC-Bayesian Bounds
Several authors have proposed to use this bound not only to give an empirical
bound on the true risk but also for estimation. That is, choose the hyper-parameter
of the prior or an approximation of the model, such that the empirical bound is
the tightest. The log-normalizing constant is not always easy to compute, however
we show in the following sections some tools to ﬁnd an unbiased estimator of this
integral. In Chapter 5 we show how to replace the optimal measure by the optimal
measure in a smaller class of distribution for which the integral is tractable.
1.2.2 PAC Bayesian oracle inequality
We call oracle some estimator based on an unobservable quantity. In most of this
thesis it will be given by the minimizer of the theoretical risk.
Here we write
¯θ = arg minθ∈Θ R(θ). From equation 1.1 we get simultaneously ∀ρ ∈M1
+(Θ) with
probability 1 −ǫ,
ρ(R(θ)) ≤ρ(rn(θ)) + 1
λK(ρ, πξ) + λ
2n + 1
λ log 2
ǫ,
ρ(rn(θ)) ≤ρ(R(θ)) + 1
λK(ρ, πξ) + λ
2n + 1
λ log 2
ǫ.
Specifying ρ as the Gibbs posterior in the ﬁrst of the two equations and replacing
ρ(rn) by its upper bound one gets with probability 1 −ǫ,
ˆρλ(R(θ)) ≤inf
ρ∈M+
1

ρ(R(θ)) + 2
λK(ρ, πξ)

+ λ
n + 2
λ log 2
ǫ.
The inequality gives rise to a bound on the integrated theoretical risk. To obtain
a speciﬁc bound and introduce the oracle risk we will introduce an additional
assumption.
Deﬁnition 1.2.2 There exists a constant c > 0 such that ∀(θ, θ′) ∈Θ2 with
∥θ∥= ∥θ′∥= 1 we have P (< X, θ >< X, θ′ >≤0) ≤c∥θ −θ′∥.
The assumption implies that for any two given linear classiﬁers we can control
the amount of points that are classiﬁed diﬀerently. More speciﬁcally, suppose that
X admits a density, with respect to the d −1 spherical measure, upper bounded
by B then we can show,
P {< X, θ >< X, θ′ >≤0} ≤B
2π arccos(< θ, θ′ >)
≤B
2π
r
5
2∥θ −θ′∥
7

1 Introduction
θ1
X
θ2
X
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
8
10
12
14
2
4
6
8
Figure 1.1: Illustration of the assumption
The hypothesis imposes that for any two classiﬁers there is enough mass in the region where their
conclusion diﬀers.
By specifying the prior to be a Gaussian distribution with variance ϑ we get
an oracle inequality. That is we take π(θ) = Qd
i=1 ϕ(θi; 0, ϑ2). Several other prior
can be considered (see Chapter 4 for an application to spike and slab to induce
sparsity).
Theorem 1.2.1 Take λ =
√
nd and ϑ =
1
√
d. Under the assumption of Deﬁnition
1.2.2, for any ε > 0, with probability at least 1 −ε we have,
ˆρλ(R(θ)) ≤R(¯θ) +
r
d
n log
 4ne2
+
c
√n +
r
d
4n3 + 2 log
  2
ε

√
nd
.
The proof of this result is provided in Chapter 5. Note that we can also de-
duce bounds in expectation from bound of Theorem 1.2.1, using the fact that
´ ∞
0 P {X > t} dt = E (X).
In Chapter 4 we specify other types of priors with the aim of dealing with
diﬀerent hypotheses i.e. a spike and slab for sparsity and a Gaussian process prior
for non-linear classiﬁer.
The issue with the result is that the estimator is intractable. We will give ways
to approximate this measure in the subsequent chapters. In Chapter 5 in particular
we give an approximation for which we can obtain oracle inequalities directly.
8

1.3 Computational aspects
1.3 Computational aspects
The general idea behind Bayesian computational statistics is easily summed up
by Minka [2001a]. “[. . . ] instantiate the things we know, and integrate over the
things we don’t know, to compute whatever expectation or probability we seek”
We will therefore be interested in computing the integral
ˆ
Θ
h(θ)π(dθ|D).
We have seen that for a quadratic loss the optimal estimator is the posterior
mean. But we can also take interest in other quantities such as moments, the mode,
posterior probabilities etc. In all these cases we have to deal with the intractability
of the integral with respect to the posterior as well as the intractability of the
normalizing constant. We will divide the introduction of the diﬀerent algorithms
in two sections, the ﬁrst dealing with Monte Carlo , i.e. based on random samples,
the second on deterministic variational approximations.
1.3.1 Monte Carlo
Principle of Monte Carlo
Monte Carlo is based on the use of the law of large numbers (LLN) to approximate
integrals. We will be interested in computing integrals of the form Ih := EP (h(X)).
The LLN gives us, under the hypothesis that the (Xi)≥0 are iid P and the existence
of Ih, that:
1
N
N
X
i=1
h(Xi)
P
−−−→
N→∞Ih.
A stronger result is given by the central limit theorem under the hypothesis of
ﬁnite second order moment,
√
N
 
1
N
N
X
i=1
h(Xi) −Ih
!
L
−−−→
N→∞N(0, σ2),
with σ2 the variance of h(X) under P.
We need a way to sample from P in the most general way possible to apply
these results. Several approaches exist for sampling, the two direct ones one may
consider are:
• Inverting the c.d.f: for sampling from a distribution with c.d.f F one can
sample a uniform U ∼U[0,1] and compute Y = F −1(U).
This requires
knowledge of the c.d.f. which we have seen will not happen often in practice,
9

1 Introduction
and the ability to compute its inverse or pseudo-inverse. We give an example
below for the truncated Gaussian distribution which we will use repeatedly
in Chapter 3.
• Rejection sampling Accept reject (AR) is based on the fact that sampling
uniformly in the epigraph of a density leads to points marginally distributed
under it. The additional layer here is that we use a proposal distribution g
such that uniformly on the support of the target f we have that f ≤Mg for a
known constant M. Then AR consists in sampling from the joint distribution
(U, X) ∼M✶[0≤U≤f(X)
Mg(X)]g(X). The proposal g is a degree of freedom for the
user and will impact the eﬃciency of the algorithm. Finally, note that for
log concave densities there exists algorithms to automatically construct g,
see Chapter 3 of Robert and Casella [2004b] for an example.
In the above we have left one question unanswered: all of the algorithms require
the ability to generate i.i.d sequence of uniform distribution. In this thesis we
will consider that such an algorithm is given. An account is given in Niederreiter
[1978].
We give an example in the following on how to apply the above methodology to
a truncated Gaussian (see Chapter 3).
Example 1.3.1 Truncated Normal distribution
The aim is to sample Y ∼N[a,b](m, σ2). The c.d.f of a (m, σ2) Gaussian trun-
cated to [a, b] is given by:
F(x; m, σ, a, b) = Φ
  x−m
σ

−Φ
  a−m
σ

Φ
  b−m
σ

−Φ
  a−m
σ

leading to the algorithm consisting in sampling a uniform distribution U on [0, 1]
and taking
Y = m + σΦ−1

U

Φ
b −m
σ

−Φ
a −m
σ

+ Φ
a −m
σ

This particular sampler will be used extensively in Chapter 3.
An AR algorithm has also been developed for this example [Robert, 1995], using
a translated exponential where the parameters are chosen as to maximize the ac-
ceptance probability. Chopin [2011a] proposes a more eﬃcient algorithm based on
the Ziggurat algorithm.
10

1.3 Computational aspects
Importance sampling
Importance sampling is based on the identity
EP (h(X)) = EQ

h(X) dP
dQ(X)

under the hypothesis that Q >> P (measure Q dominates P).
As previously importance sampling is based on replacing the integration with
respect to Q(dx) with its empirical counterpart Qn(dx) =
1
N
PN
i=1 δXi(dx) with
Xi ∼Q(dx) hence the estimator Ih ≈
1
M
PM
i=1 wih(Xi), where we write wi(Xi) :=
dP
dQ(Xi).
It is often the case that the weights w are known only up to a normalizing con-
stant (in particular in the case of posterior sampling). One can use a normalized
estimator. That is we will replace the unknown normalizing constant by the cor-
responding IS estimator
1
M
PM
i=1 wi(Xi). Because this is a convergent estimator
bounded away from 0, one can show that the ratio
PM
i=1 wi(Xi)h(Xi)
PM
i=1 wi(Xi)
converges towards the correct integral. One can get results for ﬁnite M through
concentration inequalities (Capp´e et al. [2005] chapter 9).
In the rest of the chapter we suppose that both measures are dominated with
respect to a base measure and write p
q(X) the ratio of densities with respect to
this measure.
Remark 1.3.1 Unbiased target
It is interesting to note that importance sampling remains valid if one replaces
the weights with an unbiased estimator. Suppose we have a quantity ˆTZ(X) such
that ES ˆTZ(X) = p
q(X). In this case we can build an algorithm consisting in sam-
pling Zi ∼S and Xi ∼Q and forming the estimator
ˆIh = 1
N
N
X
i=1
ˆTZi(Xi)h(Xi),
one readily sees that integrating with respect to Z yields an IS estimator of Ih.
In the following we give a running example that we use to illustrate the case
of unbiased targets. In Chapter 6 we will use such results in the context of state
space models extending the algorithm of Chopin et al. [2013b].
11

1 Introduction
Example 1.3.2 Symmetric Skellam distribution
The diﬀerence of two random variables with Poisson distributions is known as
the Skellam distribution.
We can write its density as a convolution between a
Poisson and a negative Poisson,
p(z; λ) =
∞
X
i=0
̟(i + z; λ)̟(i; λ).
(1.3)
where ̟(x; λ) is the Poisson pdf with parameter λ evaluated at x.
This distribution can be written as the expectation of E̟(z + X) where the
variable X is distributed as a Poisson of parameter λ. An unbiased estimator is
obtained by averaging over MX samples of a Poisson distribution.
We can apply the methodology described above to this case (see Figure 1.2).
We use a Gaussian proposal with moments calibrated from the method of moment
estimator. In Figure 1.2 we show the eﬀect of changing the value of MX and hence
modifying the variance of our unbiased estimator.
On a side note, notice that the estimator of the normalizing constant introduced
above is unbiased. Hence it can be used inside other algorithm to perform inference
on an intractable posterior, we use this approach for instance in Chapter 2, for
model selection.
Resampling
Another important alternative is to sample from the empirical mixture formed by
the weighted sample obtained with importance sampling,
ˆµRIS(dx) =
M
X
i=1
wi
P
j wj
δxi(dx),
where (wi, xi)1≤···≤M is a consistently weighted sample from the target distribution
(i.e. the IS estimator converges to the correct distribution).
Several algorithms exist to perform resampling. In this thesis we will use system-
atic resampling (see Douc et al. [2005] for a comparison of resampling algorithms
for particle ﬁlters). The convergence of the measure to the target in ensured by
portmanteau lemma. Finite sample results also exist: Chapter 7 of Capp´e et al.
[2005] provides a Hoeﬀding type inequality.
Monte Carlo Markov Chain (MCMC)
MCMC relies on a diﬀerent idea. It consists in building a Markov kernel with
the target distribution as an invariant distribution. MCMC relies on the ergodic
12

1.3 Computational aspects
2.0
2.5
3.0
3.5
4.0
0
2500
5000
7500
10000
samples
(a) MX = 5
2.0
2.5
3.0
3.5
4.0
0
2500
5000
7500
10000
samples
(b) MX = 10
2.0
2.5
3.0
3.5
4.0
0
2500
5000
7500
10000
samples
(c) MX = 15
2.0
2.5
3.0
3.5
4.0
0
2500
5000
7500
10000
samples
(d) MX = 20
Figure 1.2: Sequence of estimator of the parameter of a symmetric
Skellam distribution
We provide the convergent sequence of estimator of the conditional expectation of the parameter
for diﬀerent value of MX. We show the IS estimator and 95% conﬁdence bands (obtained by
replication). We use a simulated dataset for illustration with n = 40 observations. The value of
MX has as expected an inﬂuence on the variance.
theorem to approach integration with respect to the invariant distribution. That
is, we rely on the following theorem.
Theorem 1.3.1 (Robert and Casella [2004b]) Let (Xn)n≥0 be a Harris recur-
rent Markov chain with invariant probability π for any f such that f ∈L1(π) we
have that
1
N
N
X
i=1
f(Xi) −−−→
N→∞
ˆ
f(x)π(dx).
As for the iid case we can get a CLT under additional assumptions. We refer
the reader to Roberts and Rosenthal [2004] for a review on Markov chain theory
13

1 Introduction
applied to MCMC and to Robert and Casella [2004b] for some applications of the
results.
Gibbs sampler
We start our very brief overview of MCMC methods with Gibbs
sampling. The methodology consists in constructing a Markov Chain by sampling
alternatively from conditional distributions. One can easily show that this results
in a Markov Chain targeting the joint distribution.
We describe the algorithm in pseudo-code below
Algorithm 1 Two stage Gibbs sampler
Input: Conditional distributions p(X|Z) and p(Z|X), a starting point Z0.
Output: (X, Z)1≤t≤T a Markov chain with invariant probability p(X, Z)
For
t ∈{1, . . . , T}
a. Sample Xt ∼p(.|Zt−1).
b. Sample Zt ∼p(.|Xt).
End For
The Gibbs sampler needs conditional conjugacy in its simplest form to work.
For our example 1.1.1 on the probit model we can see that putting a Gaussian
prior and sampling alternatively from p(β|Z1:n, Y1:n) and p(Z1:n|β, Y1:n) leads to a
sampler targeting the correct joint distribution π(β, Z1:n|X1:n). It is direct to see
that the joint distribution of β, Z is given by,
π(β, Z1:n|X1:n) =
n
Y
i=1
{✶Yi=1✶Zi≥0 + ✶Yi=0✶Zi≤0} ϕ(Zi; xiβ, 1)ϕ(β; 0p, ϑIp).
Hence the two conditionals are given by:
(
β|(Z, Y )1:n ∼N (Q−1xtY, Q−1)
Zi|β, Y ∼N(xiβ, 1) {✶Yi=1✶Zi≥0 + ✶Yi=0✶Zi<0}
with Q =
 xTx + ϑIp
−1. One iterates between the two distributions as shown
in Algorithm 1. In Chapter 2 we discuss alternatives to the Gibbs sampler that
outperform it on many datasets.
14

1.3 Computational aspects
Unbiased target
In the case where only an unbiased version of the density is
available, one can still construct a Gibbs sampler. Suppose that we can write the
following joint distribution:
p(θ, X1:MX) =
 
1
MX
MX
X
i=1
ˆTXi(θ)
! MX
Y
i=1
qi(Xi)
such that integrating with respect to X gives the correct target density (with q an
auxiliary distribution). That is such that ˆT(θ) is an unbiased estimator under q
of our target.
We can augment the space with an index k chosen with probability
ˆTXk(θ)
MX . The
joint distribution is then given by:
p(k, θ, X1:MX) = ˆTXk(θ) 1
MX
MX
Y
i=1
q(Xi)
Summing over k gives the correct distribution.
Hence the Gibbs sampler con-
structed on this joint distribution, which samples iteratively from k|θ, X1:MX,
θ|k, Xk and X1:MX|θ, k gives the correct distribution.
This idea will be the building block of the parameter update in Chapter 6; the
algorithm is developed in the special case of state space models. We use again the
Skellam distribution as a toy example to illustrate the principle.
Example 1.3.3 Symmetric Skellam distribution (continued)
We put a Γ(1, 1) prior on λ as was done previously. Using what was described
above we can write an algorithm to sample from the posterior. The eﬃciency of
the algorithm could be discussed but it is however given here just as an example to
prepare the reader for more advanced use of this idea in Chapter 6.
Algorithm 2 A Gibbs sampler for the Estimation of the Skellam parameter.
Input: λ0, MX and Mθ
Output: (λi)0≤i≤Mθ
For i ∈{1, · · · , Mθ}
a. Sample MX i.i.d samples Xj
1:N ∼PN(λ) for j ̸= k
b. Sample k|λ, X1:MX
1:n
∼PMX
i=1
Q
j ̟(yi + Xi
j; λ)

δi
c. Sample λ|Xk
1:n, y ∼Γ(2 + Pn
i=1(2yi + Xk
i ), 2n + 1)
End For
15

1 Introduction
0.00
0.25
0.50
0.75
1.00
0
10
20
30
40
50
lag
acf
(a) ACF
2
3
4
0
2500
5000
7500
10000
samples
(b) MX = 130
Figure 1.3: ACF and trace plot of the Gibbs sampler for the Skellam
distribution
The experiment is performed on the same simulated dataset as for importance sampling. It can
be compared to Figure 1.2. At ﬁrst glance the scheme does not seem that eﬃcient as we need a
large MX to get a variance comparable to the IS version.
Remark 1.3.2 Before we start explaining a more general class of algorithms,let
us note a property of the autocorrelations of Gibbs samplers.
Assuming without loss of generality Eh(X) = 0, we have that
E (h(X)h(X′)) =
ˆ
h(X)h(X′)
ˆ
p(X|Z)p(Z|X′)dZp(X′)dX′dX
=
ˆ ˆ
h(X)p(X|Z)dX
2
p(Z)dZ ≥0
This is not the case for Metropolis-Hastings. In fact, using a certain version of
a MCMC algorithm, Hamiltonian Monte Carlo (HMC) (see Chapter 2) we can
construct an example where the correlation is negative between samples therefore
leading to a gain in eﬃciency as compared to iid sampling.
16

1.3 Computational aspects
0.5
1.0
1.5
2.0
−2
−1
0
5.0e−40
1.0e−39
1.5e−39
2.0e−39
2.5e−39
level
(a)
lag
acf
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
30
35
(b)
Figure 1.4: An example of an MCMC run with negative autocorre-
lation
We show the trace plot for the two ﬁrst marginals of the posterior of a probit model on the Pima
dataset, the second panel shows the ACF of the ﬁrst marginal.
Metropolis-Hasting
A more general approach to building a Markov chain with
the correct invariant distribution is the Metropolis-Hastings (MH) algorithm. The
algorithm uses a proposal and acceptance mechanism. A new state is proposed
according to a Markov kernel, with density q. The proposal is accepted with a given
probability (see Algorithm 3). The degree of freedom of the algorithm is given by
the choice of the proposal q. Several common choices have been adopted in the
literature. We give a few. First one may choose to propose independently from the
current state. The diﬃculty with the approach is that we need to ensure that the
proposal dominates the target. By adopting this view we are back to the problem
of AR and IS. A more useful approach is to move locally around the current point
by using a random walk (i.e. a distribution such that q(x|y) = q(y|x)). The density
of the proposal therefore cancels in the acceptance ratio.
Many other proposals will be considered in the sequel, in particular we will make
use of the gradient of the target to explore more eﬃciently the state space.
Unbiased target
As for IS one requirement of those methods is the availability
of the density up to a multiplicative factor. Suppose we have access to a random
variable ˆTX(θ) such that Eq

ˆTX(θ)

= T(θ) is our target distribution. By taking
17

1 Introduction
Algorithm 3 Metropolis Hastings algorithm
Input: θ0, M
Output: (θt)t≥0
For t ∈{1, · · · , M}
a. Sample θprop ∼q(.|θt−1).
b. Sample U ∼U([0, 1]).
c. If U ≤π(θprop|D)q(θt−1|θprop)
π(θt−1|D)q(θprop|θt−1) , set θt ←θprop, otherwise set θt ←θt−1.
End For
for proposal on (θ, X), q(θ′|θ)q(X′), and for target distribution ˆTX(θ)q(X) one
gets the correct invariant with the acceptance ratio:
α(θ, θ′) = 1 ∧
ˆTX′(θ′)✟✟✟
✟
q(X′)q(θ|θ′)✟✟✟
q(X)
ˆTX(θ)✟✟✟
q(X)q(θ′|θ)✟✟✟
✟
q(X′)
.
In addition after simpliﬁcation the acceptance ratio writes:
α(θ, θ′) = 1 ∧
ˆTX′(θ′)q(θ|θ′)
ˆTX(θ)q(θ′|θ)
.
Hence applying the methodology of Algorithm 3 to an unbiased estimator of the
target yields a Markov chain with the correct invariant probability.
Sequential Monte Carlo (SMC)
The ﬁrst step to building the SMC algorithm is to deﬁne a sequence of densities
(πn)∈T where T is an index set, and such that there is one N ∈T such that πN
is the target. The idea is to apply IS to move from one distribution to another.
We will explain in the rest of the section how we can take proﬁt of resampling and
ideas from MCMC to improve upon this. SMC is in particular useful for moderate
dimensions in the case where MCMC explores the space too slowly.
We give a brief overview of the idea of the algorithm and relay the full description
to Chapter 2 for static models and to Chapter 3 and 6 for SSM.
Particle ﬁlter (PF)
Particle ﬁlters originated in Gordon et al. [1993] for state
estimation in SSM. We describe the algorithm using the notations of Example
1.1.2.
18

1.3 Computational aspects
The index set in this case is time and we want to sample recursively from the
predictive distribution p(xt+1|y1:t) and the ﬁltering distribution p(xt|y1:t). To this
aim we introduce the forward backward recursion,
p(xt+1|y1:t) =
ˆ
X
p(xt|y1:t)g(xt|xt−1)dxt
p(xt+1|y1:t+1) ∝p(xt+1|y1:t)f(yt+1|xt+1)
This suggests the following importance sampling based algorithm: start by
sampling an array of size M independently according to the initial distribution
xi ∼g0(xi
0); we call the elements particles. Move the particles according to the
Markov kernel g(xi
1|xi
0) weight each particles according to wi
1 = f0(y1|x1). The
operation is iterated this way for every time steps t. In the end we sample recur-
sively from the Markov chain QT
t=1 g(xt|xt−1) and compute the product of weights
wi
t = QT
t=1 f(yt|xi
t). This algorithm amounts to an importance sampling algorithm
where the simulations and weight computation are done sequentially. It is referred
in the literature as sequential importance sampling (SIS) [Capp´e et al., 2005].
We can use the algorithm to get an unbiased estimator of the normalizing con-
stant, for the case of SSM given a value of the parameters θ it corresponds to the
likelihood, L(y1:T; θ) =
´ Q
t p(yt|xt; θ)gt(xt|xt−1; θ)µ0(dx0)dx0:T. In chapter 3 we
reinterpret the GHK algorithm as a SIS algorithm for a given state space model,
we use it to build a more eﬃcient estimator of the Gaussian orthant probability.
The SIS algorithm as described proposes the state of the Markov chain using
the true process gt(xt|xt−1) at each time t. On can easily see, that as for IS, we can
choose an auxiliary distribution as long as we correct accordingly in the weights.
We will use this feature in Chapter 3 and in fact propose each state by the optimal
proposal (minimizing the variance).
One issue of SIS is the fact that we have to compute a product of weights;
this leads to weight degeneracy [Capp´e et al., 2005]. As T grows the number of
particles with small weights will grow, and the normalized weight will go to zero.
One can measure the variance of the weigths by introducing the ESS (eﬀective
sample size) Kong et al. [1994],
ESSt =
nPM
i=1 wi
t
o2
PM
i=1(wi
t)2 ∈[0, M].
As the time horizon grows it is common to observe that this quantity goes
to zero. In Chapter 3 we show that for a speciﬁc model, when computing the
orthant probability with GHK, some quantity closely related to the ESS goes to 0
exponentially fast.
19

1 Introduction
The solution proposed in the literature is to introduce resampling at each step or
in an adaptive manner whenever the ESS falls under a given value. The particles
are resampled according to their current weights and the later are set to one. We
do not describe the case of PF with unbiased estimator of the weights although
the interested reader can refer to Fearnhead et al. [2010].
Algorithm 4 Particle Filter
Input: M the number of particles
Sample: Sample xi
0 ∼g0(.)
for t = 1 : T −1 do
if ESS < η⋆then
Z ←Z × { 1
M
PM
i=1 wi
t}
Resample aj
t ∼P
i
wi
t
P
j wj
t δi, set wj
t ←1
else
a1:M
t
= 1 : M
end if
Sample xi
t+1 ∼qt+1(.|x
ai
t
t )
Set wi
t+1 ←wi
t
ft+1(yt+1|xi
t+1)gt+1(xi
t+1|x
ai
t
t )
qt+1(xi
t+1|x
ai
t
t )
end for
return Z × 1
M
PM
i=1 wi
T and (xi
T, wi
T)1≤i≤M
Particle ﬁlters can also be used to compute the likelihood of the model. In fact
it can be shown that the quantity deﬁned by Z in Algorithm 4 is an unbiased esti-
mator of the likelihood (Del Moral [1996a], Lemma 3). This is useful in particular
in lights of the remarks made on the use of algorithms with unbiased estimator
of the target. Andrieu et al. [2010a] suggested using a Gibbs sampler and a MH
using the estimator of the likelihood given by a particle ﬁlter.
In the following we will show that this algorithm can be generalized to more
general problems.
Static models
The idea of using this framework to sample from other types of
distribution originated in Neal [2001a], Chopin [2002a], (see also Del Moral et al.
[2006b]).
The user needs to deﬁne a sequence of distribution; several choices can be con-
sidered. We describe the two used in this thesis.
• The ﬁrst one was proposed under the name IBIS (iterated batch importance
sampling) [Chopin, 2002a].
πn(θ) ∝π(θ)L(y1:n|θ)
20

1.3 Computational aspects
and consists in adding one or several data points in a sequential manner. It
is used in particular in Chapter 6 for parameter estimation of state space
models.
• The sequence builds a bridge between two distributions,
πn(θ) ∝{π(θ)L(y1:N|θ)}γn {q(θ)}1−γn
whith γn a sequence such that 0 = γ0 < γ1 < · · · < γN = 1. The choice of the
sequence is of paramount importance for the eﬃciency of the algorithm. In
what follows we use an adaptive choice for γ such that the ESS does not fall
under a given threshold (see Jasra et al. [2011b] and the diﬀerent chapters
of this thesis).
After a few resampling steps all particles θ would tend to be equal. We introduce
an additional step to increase the diversity of the particle system, at time n we
move the particles according to a kernel Kn that leaves πn invariant. The choice
of Kn is of paramount importance as it will condition the diversity of the particle
system.
The main steps are described in Algorithm (12) bellow:
Algorithm 5 Sequential Monte Carlo
Input Γ,a, M, α⋆, Set Z ←1
Each computation involving m is done ∀m ∈1 : M
Init ηm
1 ∼π0(.), and wm
1 = 1
for t ∈1 : T −1 do
At time t the weighted system is distributed as (wm
t , ηm
1:t) ∼πt(.).
if ESS(w1:M
t
) < α⋆then
Z ←Z × { 1
M
PM
i=1 wi
t}.
Resample: η′m
t
∼PM
j=1 wj
tδηj
t , wm
t ←1.
Move: ηm
t ∼Kt(η′m
t , dηm
t ) where Kt leaves πt(η1:t) invariant.
end if
wm
t+1 ←wm
t × πt+1(ηm
t )
πt(ηm
t ) .
end for
return Z × { 1
M
PM
i=1 wi
T}, and (wi
T, ηi
T)
A nice feature of SMC is that at a given step one can use estimators based on
the system of particles to build adaptive proposals and MCMC steps. We will
make some use of this in the diﬀerent chapters, see in particular Chapter 6 where
we use SMC in the context of intractable targets.
21

1 Introduction
1.3.2 Approximate Inference
In the previous section we justiﬁed the algorithms as being exact with the CPU
eﬀort growing to inﬁnity. Another approach is to approximate the target by the
closest distribution in a tractable family.
We give a brief overview of the two
most used algorithms of this type and relay a full description to Chapter 2 for
expectation propagation (EP) and Chapter 4 for variational Bayes (VB).
Expectation Propagation
In most applications of EP we will consider a posterior that can be written as a
product of a tractable distribution and intractable factors. We call tractable a dis-
tribution for which we have easy access to moments and is stable by multiplication
(as are exponential family models). We write:
π(θ|Y ) ∝π(θ)
Y
i
ti(θ).
EP works by successively approximating each sites by a given parametric distri-
bution. To be more speciﬁc let us describe the global approximation Q to the
previous posterior as a product:
Q(θ) ∝π(θ)
Y
i
˜ti(θ),
where each ˜ti(θ) approximates its corresponding site ti(θ). EP works by cycling
through the sites and updating them. Once a site has been updated we can use this
to update the global approximation. Let us describe the process in more detail.
We start by computing the cavity distribution as the approximation removed of
the inﬂuence of site j,
Q\j(θ) ∝π(θ)
Y
i̸=j
˜ti(θ).
This distribution is easily obtained for exponential models, and consists only in
a modiﬁcation of the natural parameters. We multiply the cavity distribution by
the corresponding new site tj(θ) to obtain the hybrid distribution:
hj(θ) =
Q\j(θ)tj(θ)
´
Q\j(θ)tj(θ)dθ.
This new distribution would ideally be our new distribution updating information
of site j, it is however intractable. The approach consists in deﬁning the new
approximation as the one minimizing the KL divergence of Q with respect to
the hybrid over the considered exponential family. Because the approximating
22

1.3 Computational aspects
distribution is in an exponential family the minimization can be done by computing
the moments of the distribution (see Seeger [2005a]). This is done in cyclic manner
for each site until a convergence criterion is achieved. We give an example where
an EP algorithm can be used, and an iteration of the algorithm for a simple case.
Example 1.3.4 PAC-Bayesian 0-1 Classiﬁcation The pseudo posterior we want
to approximate is given by:
πξ,γ(θ|D) ∝
n
Y
i=1
exp

−λ
n✶Yi<Xi,θ>≤0

π(θ)
and we suppose that the prior π(θ) is Gaussian.
We can use EP to get a Gaussian approximation of the posterior because the mo-
ments of the distribution hi(θ) ∝ϕ(θ; m\i, Σ\i) exp
 −λ
n✶Yi<Xi,θ>≤0

can be com-
puted exactly.
To make this application clearer, in Figure 1.5 we show the iterations of an EP
approximation for a site of the form t(θ) = ✶θ<0.
More details on the computation of each terms in speciﬁc cases are given in
the chapters of this thesis. Note in particular the EP is used in Chapter 2 to
compute an approximation of the posterior of a probit model, and in Chapter 4
to approximate the Gibbs posterior under a AUC loss.
Fractional Expectation Propagation
Fractional EP is similar to the algorithm described previously with the excep-
tion that only a fraction of each site is treated at each iteration.
We will de-
note this fraction by α ≤1. The corresponding cavity distribution is now given
by Q\α,j(θ) ∝Qn
i=1 ˜ti(θ)
 ˜tj(θ)
1−α. The hybrid distribution can be written has
hj(θ) ∝Q\α,j(θ) (tj(θ))α. The rest of the algorithm follows the steps of standard
EP.
The rationale behind this strategy is twofold [Jyl¨anki et al., 2011]. First taking
α < 1 for non log-concave sites ﬂattens the distribution and therefore prevents
numerical issues linked to multimodality. Second it prevents the cavity moment
of becoming too small (or even negative). Taking speciﬁc values of α can also
allow us to transform a intractable problem into a tractable one. We give such an
example in the following.
Example 1.3.5 Consider the Gaussian regression model with Student prior dis-
tribution as advocated for instance by Gelman et al. [2008].
π(β|Y ) ∝exp

−1
2∥Y −Xtβ∥2
2

d
Y
i=1
tν(βi)
23

1 Introduction
0.00
0.25
0.50
0.75
1.00
−4
−2
0
2
(a) Global approximation
0.00
0.25
0.50
0.75
1.00
−4
−2
0
2
(b) Cavity distribution
0.00
0.25
0.50
0.75
1.00
−4
−2
0
2
(c) New site
0.00
0.25
0.50
0.75
1.00
−4
−2
0
2
(d) Hybrid distribution
0.00
0.25
0.50
0.75
1.00
−4
−2
0
2
(e) New global approxima-
tion
Figure 1.5: Expectation Propagation in Action
In panel (a) we show the global approximation Q (initial point), the site approximation is removed
to get Q\j in panel (b).
Panel (c) and (d) show the site (indicator function) and the hybrid
h(θ) ∝N(θ; m, s)✶θ>0 (in purple). The closest Gaussian to the hybrid is the new approximation
panel (e) (the one with the same mean and variance).
where tν,γ(βi) is the Student density with degree of freedom ν and scale γ. This
prior leads to an intractable problem.
Standard EP algorithm also leads to intractable integrals using fractional EP we
can get a tractable problem that leads to tractable sites.
We can write
tν,γ(βi) ∝
1

1 +

βi
γ
2 ν+1
2
Taking a fractional parameter η = −
2
ν+1 for a Gaussian approximation one gets
a tractable EP approximation for an otherwise intractable case. This can also be
used in the Gaussian process case with Student likelihood to prevent from using
numerical integration [Jyl¨anki et al., 2011].
24

1.3 Computational aspects
0
20
40
60
−0.12
−0.10
−0.08
(a) β1
0
50
100
0.04
0.05
0.06
(b) β2
0
10
20
30
−0.025
0.000
0.025
0.050
0.075
(c) β3
Figure 1.6: Marginal of the Gaussian approximation of a regression
with Student priors
Regression with student prior marginal of the 3 ﬁrst coeﬃcients on the Boston dataset. In green
the EP approximation and in blue the marginal from the sampled posterior (RWMH).
Variational Bayes
The variational Bayes algorithm appears as a way to transform an integral form
problem in a ﬁnite dimension optimization problem (Jordan et al. [1999],MacKay
[2002] and Chap. 10 in Bishop [2006a]).
The idea of the approach is to minimize the KL divergence of a measure ρ with
respect to the posterior measure of interest.
In most cases of interest it is an
intractable problem because of the unknown normalizing constant, however we
can write the following useful decomposition of the log marginal likelihood,
log m(D) = Eρ

log π(θ)L(D|θ)
ρ(θ)

+ K(ρ|π(.|D)).
Because the left handside does not depend on ρ, minimizing the ﬁrst term of the
right handside is equivalent to minimizing the KL divergence. This surrogate ob-
jective function can be seen as a lower bound of the marginal likelihood. The above
deﬁnition gives a natural optimization algorithm, minimizing the objective on all
probability distribution yields the posterior, we minimize the objective restricted
to a set of tractable distributions.
We now review two types of families popular in the VB literature.
• Mean ﬁeld VB: for a certain decomposition Θ = Θ1 × · · · × Θd, F is the set
25

1 Introduction
of product probability measures
FMF =
(
ρ ∈M1
+(Θ) : ρ(dθ) =
d
Y
i=1
ρi(dθi), ∀i ∈{1, . . . , d}, ρi ∈M1
+(Θi)
)
.
(1.4)
The inﬁmum of the KL divergence K(ρ, π(.|D)), relative to ρ = Q
i ρi satisﬁes
the following ﬁxed point condition [Bishop, 2006a; Parisi, 1988, Chap. 10]:
∀j ∈{1, · · · , d}
ρj(dθj) ∝exp
 ˆ
{log L (D|θ) + log π(θ)}
Y
i̸=j
ρi(dθi)
!
.
(1.5)
This leads to a natural algorithm were we update successively every ρj until
stabilization.
• Parametric family:
FP =

ρ ∈M1
+(Θ) : ρ(dθ) = f(θ; m)dθ, m ∈M
	
;
and M is ﬁnite-dimensional; say FP is the family of Gaussian distributions
(of dimension d). In this case, several methods may be used to compute the
inﬁmum. As above, one may use ﬁxed-point iterations, provided an equation
similar to (5.4) is available. Alternatively, one may directly maximize the
bound with respect to parameter m, using numerical optimization routines.
Mean ﬁeld VB has been shown to underestimate the variance of the distribution.
This is partly due to the fact the KL divergence takes inﬁnite values if the target
does not dominate ρ. In particular the condition is violated if the tails of ρ are
lighter than the target.
To illustrate this problematic behavior let us look at an experiment proposed
in Bishop [2006a], namely the mean ﬁeld approximation of a centered bivariate
Gaussian with variance 1 and correlation ρ. One readily checks that the iterations
are given by variance of 1 −ρ2 and a sequence of means converging to 0. Figure
1.7 illustrates the two distributions, with the approximation constrained by the
variance in the direction of the smallest eigenvalue.
26

1.4 Overview
−2
−1
0
1
2
−2
−1
0
1
2
x
y
Figure 1.7: Mean ﬁeld VB approximation (red) of bivariate Gaussian
distribution with correlation ρ = 0.99 (blue)
In chapter 5 we give conditions under which VB approximations of the Gibbs
posterior do not deteriorate the rate of convergence of the approximation.
1.4 Overview
This thesis is divided into 5 chapters, we give a short descriprion of each of them
in the following.
1.4.1 Leave Pima indians alone: binary regression as a
benchmark for Bayesian computation
Resum´e
Quand un nouvel algorithme est introduit en statistique bay´esienne le
mod`ele probit sur de petits jeux de donn´ees est fr´equemment employ´e pour le
tester. Ce chapitre ´etudie le bien fond´e de cette approche. Il donne, de plus,
une revue de la litt´erature dans le domaine avec pour exemple ce mod`ele. Les
algorithmes ´etudi´es sont divis´es en deux cat´egories: d’un cˆot´e ceux employant
des m´ethodes d’´echantillonnage (´echantillonnage pr´eferentiel, m´ethode de Monte
Carlo par chaˆıne de Markov et methode de Monte Carlo s´equentielle), de l’autre
les algorithmes d’approximation rapide (Laplace et EP). De nombreux r´esultats
num´eriques sont pr´esent´es.
27

1 Introduction
Abstract
Whenever a new approach to perform Bayesian computation is intro-
duced, a common practice is to showcase this approach on a binary regression
model and datasets of moderate size. This chapter discusses to which extent this
practice is sound. It also reviews the current state of the art of Bayesian com-
putation, using binary regression as a running example.
Both sampling-based
algorithms (importance sampling, MCMC and SMC) and fast approximations
(Laplace and EP) are covered. Extensive numerical results are provided, some
of which might go against conventional wisdom regarding the eﬀectiveness of cer-
tain algorithms. Implications for other problems (variable selection) and other
models are also discussed.
1.4.2 Computation of Gaussian orthant probabilities in high
dimension
Resum´e
Nous ´etudions dans ce chapitre le calcul de probabilit´es d’orthants (la
probabilit´e qu’une r´ealisation Gaussienne ait toutes ses composantes positives).
Nous basons cette ´etude sur l’algorithme GHK couramment utilis´e pour r´esoudre
ce probl`eme en dimensions plus grandes que 10.
Dans ce chapitre nous mon-
trons, pour des matrices de variance-covariance markoviennes que l’algorithme
peut s’interpr´eter comme un algorithme d’´echantillonnage pr´ef´erentiel s´equentiel
(SIS pour acronyme anglais). Pour un AR(1) la variance normalis´ee de GHK di-
verge `a une vitesse exponentielle avec la dimension. Pour corriger ce probl`eme
nous introduisons une ´etape de r´eechantillonnage transformant ainsi l’algorithme
en un ﬁltre particulaire. Nous g´en´eralisons dans un deuxi`eme temps cette id´ee au
cas de matrices de variance covariance g´enerales en introduisant un algorithme de
Monte Carlo s´equentiel.
Abstract
We study the computation of Gaussian orthant probabilities, i.e. the
probability that a Gaussian variable falls inside a quadrant. The Geweke-Hajivassiliou-
Keane (GHK) algorithm [Genz, 1992; Geweke, 1991; Hajivassiliou et al., 1996;
Keane, 1993], is currently used for integrals of dimension greater than 10. In this
chapter we show that for Markovian covariances GHK can be interpreted as the
estimator of the normalizing constant of a state space model using sequential im-
portance sampling (SIS). We show for an AR(1) the variance of the GHK, properly
normalized, diverges exponentially fast with the dimension. As an improvement
we propose using a particle ﬁlter (PF). We then generalize this idea to arbitrary
covariance matrices using Sequential Monte Carlo (SMC) with properly tailored
MCMC moves. We show empirically that this can lead to drastic improvements on
currently used algorithms. We also extend the framework to orthants of mixture of
Gaussians (Student, Cauchy etc.), and to the simulation of truncated Gaussians.
28

1.4 Overview
1.4.3 Theoretical and computational aspects of PAC Bayesian
ranking and scoring
Resum´e
Nous d´eveloppons une proc´edure d’ordonancement et de classiﬁcation
bas´ee sur une approche PAC-Bayesienne et sur la minimisation d’un crit`ere AUC.
Nous d´emontrons des inegalit´es oracles pour diﬀ´erentes distributions a priori : un
prior Gaussien et un prior “spike and slab”. Dans un deuxi`eme temps nous pro-
posons deux algorithmes pour calculer l’estimateur num´eriquement. D’abord un
algorithme de Monte Carlo sequentiel comme m´ethode “exacte” au sens de Monte
Carlo, puis un algorithme d’approximation EP pour introduire des m´ethodes plus
rapides.
Abstract
We develop a scoring and classiﬁcation procedure based on the PAC-
Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially
on the class of linear score functions. We derive PAC-Bayesian non-asymptotic
bounds for two types of prior for the score parameters: a Gaussian prior, and a
spike and slab prior; the latter makes it possible to perform feature selection. One
important advantage of our approach is that it is amenable to powerful Bayesian
computational tools. We derive in particular a Sequential Monte Carlo algorithm,
as an eﬃcient method which may be used as a gold standard, and an expectation
propagation algorithm, as a much faster but approximate method. We also ex-
tend our method to a class of non-linear score functions, essentially leading to a
nonparametric procedure, by considering a Gaussian process prior.
1.4.4 Properties of variational approximations of Gibbs
posteriors
Resum´e
L’approche PAC-bay´esienne permet le d´eveloppement de bornes non
assymptotiques sur le risque th´eorique. La distribution sur les estimateurs qui en
d´ecoulent n’est cependant pas calculable. Alors que l’approche usuelle est d’utiliser
des m´ethodes de Monte Carlo par chaˆıne de Markov nous proposons pour plus
d’eﬃcacit´e une approximation variationelle des estimateurs. Nous ´etudions alors
les propri´et´es de ces estimateurs.
Nous donnons des conditions sous lesquelles
l’approximation variationelle converge `a la mˆeme vitesse que les proc´edures PAC
usuellement consid´er´ees. Nous appliquons ensuite ces r´esultats `a diﬀerents probl`emes
d’apprentissage (classiﬁcation, ranking et completion de matrices). L’impl´ementation
des algorithmes est ´egalement abord´ee.
Abstract
The PAC-Bayesian approach is a powerful set of techniques to derive
non-asymptotic risk bounds for random estimators. The corresponding optimal
29

1 Introduction
distribution of estimators, usually called the Gibbs posterior, is unfortunately in-
tractable. One may sample from it using Markov chain Monte Carlo, but this
is often too slow for big datasets.
We consider instead variational approxima-
tions of the Gibbs posterior, which are fast to compute. We undertake a general
study of the properties of such approximations. Our main ﬁnding is that such
a variational approximation has often the same rate of convergence as the origi-
nal PAC-Bayesian procedure it approximates. We specialise our results to several
learning tasks (classiﬁcation, ranking, matrix completion), discuss how to imple-
ment a variational approximation in each case, and illustrate the good properties
of said approximation on real datasets.
1.4.5 Towards automatic calibration of SMC2
Resum´e
Nous ´etudions SMC2, un algorithme pour l’estimation de param`etres
dans les mod`eles `a espace d’´etat. Nous g´enerons pour cela Nθ particules θm et
pour chacune d’elle nous lan¸cons un ﬁltre particulaire de taille Nx (i.e.
pour
chaque pas de temps, Nx particules sont g´en´er´ees dans l’espace d’´etat X). Le but
de ce chapitre est d’automatiser le choix de Nx au cours de l’algorithme. Nous
utilisons pour cela un algorithme de Monte Carlo s´equentiel condition´e `a une
trajectoire. Pour r´eduire le coˆut m´emoriel nous proposons de sauvegarder l’´etat
initial des g´enerateurs pseudo-al´eatoires pour chaque ﬁltre particulaire. Le choix
de Nx est condition´e `a un estimateur de la variance de l’estimateur sans biais de
la vraisemblance obtenu par des techniques non parametriques. Les applications
num´eriques sont eﬀectu´ees `a coˆut computationel constant et d´ebouchent sur une
plus petite erreur de Monte Carlo que l’algorithme original.
Abstract
SMC2 (Chopin et al., 2013) is an eﬃcient algorithm for sequential es-
timation and state inference of state-space models. It generates Nθ parameter
particles θm , and, for each θm , it runs a particle ﬁlter of size Nx (i.e. at each
time step, Nx particles are generated in the state space X ). We discuss how to
automatically calibrate Nx in the course of the algorithm. Our approach relies on
conditional Sequential Monte Carlo updates, monitoring the state of the pseudo
random number generator and on an estimator of the variance of the unbiased es-
timate of the likelihood that is produced by the particle ﬁlters, which is obtained
using nonparametric regression techniques. We observe that our approach is both
less CPU intensive and with smaller Monte Carlo errors than the initial version of
SMC2 .
30

2
Leave Pima indians alone: binary regression as a
benchmark for Bayesian computation
This is joint work with Nicolas Chopin
Status: submitted to Statistical Science
2.1 Introduction
The ﬁeld of Bayesian computation seems hard to track these days, as it is blos-
soming in many directions.
MCMC (Markov chain Monte Carlo) remains the
main approach, but it is no longer restricted to Gibbs sampling and Hastings-
Metropolis, as it includes more advanced, Physics-inspired methods, such as HMC
[Hybrid Monte Carlo,
Neal, 2010b] and its variants [Girolami and Calderhead,
2011; Hoﬀman and Gelman, 2013; Shahbaba et al., 2011].
On the other hand,
there is also a growing interest for alternatives to MCMC, such as SMC (Sequen-
tial Monte Carlo, e.g. Del Moral et al., 2006a), nested sampling [Skilling, 2006], or
the fast approximations that originated from machine learning, such as Variational
Bayes [e.g. Bishop, 2006b, Chap. 10], and EP [Expectation Propagation, Minka,
2001b]. Even Laplace approximation has resurfaced in particular thanks to the
INLA methodology [Rue et al., 2009].
One thing however that all these approaches have in common is they are almost
always illustrated by a binary regression example; see e.g. the aforementioned
papers. In other words, binary regressions models, such as probit or logit, are a
de facto benchmark for Bayesian computation.
This remark leads to several questions. Are binary regression models a reason-
able benchmark for Bayesian computation? Should they be used then to develop
a ‘benchmark culture’ in Bayesian computation, like in e.g. optimisation? And
31

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
practically, which of these methods actually ‘works best’ for approximating the
posterior distribution of a binary regression model?
The objective of this chapter is to answer these questions. As the ironic ti-
tle suggests, our ﬁndings shall lead to us be critical of certain current practices.
Speciﬁcally, most papers seem content with comparing some new algorithm with
Gibbs sampling, on a few small datasets, such as the well-known Pima Indians di-
abetes dataset (8 covariates). But we shall see that, for such datasets, approaches
that are even more basic than Gibbs sampling are actually hard to beat. In other
words, datasets considered in the literature may be too toy-like to be used as a
relevant benchmark. On the other hand, if ones considers larger datasets (with
say 100 covariates), then not so many approaches seem to remain competitive.
We would also like to discuss how Bayesian computation algorithms should be
compared. One obvious criterion is the error versus CPU time trade-oﬀ; this im-
plies discussing which posterior quantities one may need to approximate. A related
point is whether the considered method comes with a simple way to evaluate the
numerical error. Other criteria of interest are: (a) how easy to implement is the
considered method? (b) how generic is it? (does changing the prior or the link
function requires a complete rewrite of the source code?) (c) to which extent does
it require manual tuning to obtain good performance? (d) is it amenable to par-
allelisation? Points (a) and (b) are rarely discussed in Statistics, but relate to the
important fact that, the simpler the program, the easier it is to maintain, and to
make it bug-free. Regarding point (c), we warn beforehand that, as a matter of
principle, we shall refuse to manually tune an algorithm on a per dataset basis.
Rather, we will discuss, for each approach, some (hopefully reasonable) general
recipe for how to choose the tuning parameters. This has two motivations. First,
human time is far more valuable than computer time: Cook [2014] mentions that
one hour of CPU time is today three orders of magnitude less expensive than one
hour of pay for a programmer (or similarly a scientist). Second, any method re-
quiring too much manual tuning through trial and error may be practically of no
use beyond a small number of experts.
Finally, we also hope this chapter may serve as an up to date review of the state
of Bayesian computation. We believe this review to be timely for a number of
reasons. First, as already mentioned, because Bayesian computation seems to de-
velop currently in several diﬀerent directions. Second, and this relates to criterion
(d), the current interest in parallel computation [Lee et al., 2010; Suchard et al.,
2010] may require a re-assessment of Bayesian computational methods: method
A may perform better than method B on a single core architecture, while per-
forming much worse on a parallel architecture. Finally, although the phrase ‘big
data’ seems to be a tired trope already, it is certainly true that datasets are get-
ting bigger and bigger, which in return means that statistical methods needs to
32

2.2 Preliminaries: binary regression models
be evaluated on bigger and bigger datasets. To be fair, we will not really consider
in this work the kind of huge datasets that pertain to ‘big data’, but we will at
least strive to move away from the kind of ‘ridiculously small’ data encountered
too often in Bayesian computation papers.
The chapter is structured as follows. Section 2.2 covers certain useful prelimi-
naries on binary regression models. Section 2.3 discusses fast approximations, that
is, deterministic algorithms that oﬀer an approximation of the posterior, at a lower
cost than sampling-based methods. Section 2.4 discusses ‘exact’, sampling-based
methods. Section 2.5 is the most important part of the chapter, as it contains an
extensive numerical comparison of all these methods. Section 2.6 discusses vari-
able selection. Section 2.7 discusses our ﬁndings, and their implications for both
end users and Bayesian computation experts.
2.2 Preliminaries: binary regression models
2.2.1 Likelihood, prior
The likelihood of a binary regression model have the generic expression
p(D|β) =
nD
Y
i=1
F(yiβTxi)
(2.1)
where the data D consist of n responses yi ∈{−1, 1} and n vectors xi of p covari-
ates, and F is some CDF (cumulative distribution function) that transforms the
linear form yiβTxi into a probability. Taking F = Φ, the standard normal CDF,
gives the probit model, while taking F = L, the logistic CDF, L(x) = 1/ (1 + e−x),
leads to the logistic model. Other choices could be considered, such as e.g. the
CDF of a Student distribution (robit model) to better accommodate outliers.
We follow Gelman et al. [2008]’s recommendation to standardise the predictors
in a preliminary step: non-binary predictors have mean 0 and standard deviation
0.5, binary predictors have mean 0 and range 1, and the intercept (if present)
is set to 1. This standardisation facilitates prior speciﬁcation: one then may set
up a “weakly informative” prior for β, that is a proper prior that assigns a low
probability that the marginal eﬀect of one predictor is outside a reasonable range.
Speciﬁcally, we shall consider two priors p(β) in this work: (a) the default prior rec-
ommended by Gelman et al. [2008], a product of independent Cauchys with centre
0 and scale 10 for the constant predictor, 2.5 for all the other predictors (hence-
forth, the Cauchy prior); and (b) a product of independent Gaussians with mean
0 and standard deviation equal to twice the scale of the Cauchy prior (henceforth
the Gaussian prior).
33

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Of course, other priors could be considered, such as e.g. Jeﬀreys’ prior [Firth,
1993], or a Laplace prior [Kab´an, 2007]. Our main point in considering the two
priors above is to determine to which extent certain Bayesian computation methods
may be prior-dependent, either in their implementation (e.g. Gibbs sampling) or
in their performance, or both. In particular, one may expect the Cauchy prior to
be more diﬃcult to deal with, given its heavy tails.
2.2.2 Posterior maximisation (Gaussian prior)
We explain in this section how to quickly compute the mode, and the Hessian at
the mode, of the posterior:
p(β|D) = p(β)p(D|β)
p(D)
,
p(D) =
ˆ
Rd p(β)p(D|β) dβ,
where p(β) is one of the two priors presented in the previous section, and Z(D) is
the marginal likelihood of the data (also known as the evidence). These quantities
will prove useful later, in particular to tune certain of the considered methods.
The two ﬁrst derivatives of the log-posterior density may be computed as:
∂
∂β log p(β|D) = ∂
∂β log p(β) + ∂
∂β log p(D|β),
∂2
∂β∂βT log p(β|D) =
∂2
∂β∂βT log p(β) +
∂2
∂β∂βT log p(D|β)
where
∂
∂β log p(D|β) =
nD
X
i=1
(log F)′ (yiβTxi)yixi
∂2
∂β∂βT log p(D|β) =
nD
X
i=1
(log F)′′ (yiβTxi)xixT
i
and (log F)′ and (log F)′′ are the two ﬁrst derivatives of log F. Provided that log F
is concave, which is the case for probit and logit regressions, the Hessian of the
log-likelihood is clearly a negative deﬁnite matrix. Moreover, if we consider the
Gaussian prior, then the Hessian is of the log-posterior is also negative (as the sum
of two negative matrices, as Gaussian densities are log-concave). We stick to the
Gaussian prior for now.
This suggests the following standard approach to compute the MAP (maximum
a posterior) estimator, that is the point βMAP that maximises the posterior density
p(β|D): to use Newton-Raphson, that is, to iterate
34

2.2 Preliminaries: binary regression models
β(new) = β(old) −H−1
 ∂
∂β log p(β(old)|D)

(2.2)
until convergence is reached; here H is Hessian of the log posterior at β = β(old),
as computed above.
The iteration above corresponds to ﬁnding the zero of a
local, quadratic approximation of the log-posterior.
Newton-Raphson typically
works very well (converges in a small number of iterations) when the function to
maximise is concave.
We note two points in passing.
First, one may obtain the MLE (maximum
likelihood estimator) by simply taking p(β) = 1 above (i.e.
a Gaussian with
inﬁnite variance). But the MLE is not properly deﬁned when complete separation
occurs, that is, there exists a hyperplane that separates perfectly the two outcomes:
yiβT
CSxi ≥0 for some βCS and all i ∈1 : N. This remark gives an extra incentive
for performing Bayesian inference, or at least MAP estimation, in cases where
complete separation may occur, in particular when the number of covariates is
large [Firth, 1993; Gelman et al., 2008].
Second, H in (2.2) is sometimes replaced by some approximation, leading to so-
called quasi-Newton algorithms. Some of these algorithms such as IRLS (iterated
reweighted least squares) have a nice statistical interpretation. For our purposes
however, all these variants seem to show similar performance, so we will stick to
the standard version of Newton-Raphson.
2.2.3 Posterior maximisation (Cauchy prior)
The log-density of the Cauchy prior is not concave:
log p(β) = −
p
X
j=1
log (πσj) −
p
X
j=1
log(1 + β2
j /σ2
j)
for scales σj chosen as explained in Section 2.2.1. Hence, the corresponding log-
posterior is no longer guaranteed to be concave, which in turn means that Newton-
Raphson algorithm might fail to converge.
However, we shall observe that, for most of the datasets considered in this
chapter, Newton-Raphson does converge quickly even for our Cauchy prior. In
each case, we used as starting point for the Newton-Raphson iterations the OLS
(ordinary least square) estimate.
We suspect what happens is that, for most
standard datasets, the posterior derived from a Cauchy prior remains log-concave,
at least in a region that encloses the MAP estimator and our starting point.
35

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2.3 Fast approximation methods
This section discusses fast approximation methods, that is methods that are de-
terministic, fast (compared to sampling-based methods), but which comes with an
approximation error which is diﬃcult to assess. These methods include the Laplace
approximation, which was popular in Statistics before the advent of MCMC meth-
ods, but also recent Machine Learning methods, such as EP (Expectation Propa-
gation, Minka, 2001b), and VB (Variational Bayes, e.g. Bishop, 2006b, Chap. 10).
However, we will not discuss VB, as Consonni and Marin [2007] give convincing
(formal and numerical) arguments that this type of approach does not work well
for probit models.
Concretely, we will focus on the approximation of the following posterior quan-
tities: the marginal likelihood p(D), as this may be used in model choice; and the
marginal distributions p(βi|D) for each component βi of β. Clearly these are the
most commonly used summaries of the posterior distribution, and other quantities,
such as the posterior expectation of β, may be directly deduced from them.
Finally, one should bear in mind that such fast approximations may be used as
a preliminary step to calibrate an exact, more expensive method, such as those
described in Section 2.4.
2.3.1 Laplace approximation
The Laplace approximation is based on a Taylor expansion of the posterior log-
density around the mode βMAP:
log p(β|D) ≈log p(βMAP|D) −1
2 (β −βMAP)T Q (β −βMAP) ,
where Q = −H, i.e. minus the Hessian of log p(β|D) at β = βMAP; recall that
we explained how to compute these quantities in Section 2.2.2. One may deduce
a Gaussian approximation of the posterior by simply exponentiating the equation
above, and normalising:
qL(β) = Np
 β; βMAP, Q−1
:= (2π)−p/2 |Q|1/2 exp

−1
2 (β −βMAP)T Q (β −βMAP)

.
(2.3)
In addition, since for any β,
p(D) = p(β)p(D|β)
p(β|D)
36

2.3 Fast approximation methods
one obtains an approximation to the marginal likelihood p(D) as follows:
p(D) ≈ZL(D) := p(βMAP)p(D|βMAP)
(2π)−p/2 |Q|1/2
.
From now on, we will refer to this particular Gaussian approximation qL as the
Laplace approximation, even if this phrase is sometimes used in Statistics for
higher-order approximations, as discussed in the next Section. We defer to Section
2.3.5 the discussion of the advantages and drawbacks of this approximation scheme.
2.3.2 Improved Laplace, connection with INLA
Consider the marginal distributions p(βj|D) =
´
p(β|D)dβ−j for each component
βj of β, where β−j is β minus βj. A ﬁrst approximation may be obtained by
simply computing the marginals of the Laplace approximation qL. An improved
(but more expensive) approximation may be obtained from:
p(βj|D) ∝p(β)p(D|β)
p(β−j|βj, D)
which suggests to choose a ﬁne grid of βj values (deduced for instance from qL(β)),
and for each βj value, compute a Laplace approximation of p(β−j|βj, D), by com-
puting the mode ˆβ−j(βj) and the Hessian
ˆ
H(βj) of log p(β−j|βj, D), and then
approximate (up to a constant)
p(βj|D) ≈qIL(βj) ∝
p

ˆβ(βj)

p(D| ˆβ(βj))
 ˆH(βj)

1/2
where ˆβ(βj) is the vector obtained by inserting βi at position i in ˆβ−j(βj), and
IL stands for “Improved Laplace”. One may also deduce posterior expectations
of functions of βj in this way. See also Tierney and Kadane [1986], Tierney et al.
[1989] for higher order approximations for posterior expectations.
We note in passing the connection to the INLA scheme of Rue et al. [2009].
INLA applies to posteriors p(θ, x|D) where x is a latent variable such that p(x|θ, D)
is close to a Gaussian, and θ is a low-dimensional hyper-parameter. It constructs
a grid of θ−values, and for each grid point θj, it computes an improve Laplace
approximation of the marginals of p(x|θj, D). In our context, β may be identiﬁed
to x, θ to an empty set, and INLA reduces to the improved Laplace approximation
described above.
37

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2.3.3 The EM algorithm of Gelman et al. [2008] (Cauchy prior)
Gelman et al. [2008] recommend against the Laplace approximation for a Student
prior (of which our Cauchy prior is a special case), because, as explained in Section
2.2.3, the corresponding log-posterior is not guaranteed to be concave, and this
might prevent Newton-Raphson to converge. In our simulations however, we found
the Laplace approximation to work reasonably well for a Cauchy prior. We now
brieﬂy describe the alternative approximation scheme proposed by Gelman et al.
[2008] for Student priors, which we call for convenience Laplace-EM.
Laplace-EM is based on the well-known representation of a Student distribution,
βj|σ2
j ∼N1(0, σ2
j), σ2
j ∼Inv −Gamma(ν/2, sjν/2); take ν = 1 to recover our
Cauchy prior.
Conditional on σ2 = (σ2
1, . . . , σ2
p), the prior on β is Gaussian,
hence, for a ﬁxed σ2 one may implement Newton-Raphson to maximise the log-
density of p(β|σ2, D), and deduce a Laplace (Gaussian) approximation of the same
distribution.
Laplace-EM is an approximate EM [Expectation Maximisation, Dempster et al.,
1977] algorithm, which aims at maximising in σ2 = (σ2
1, . . . , σ2
p) the marginal pos-
terior distribution p(σ2|D) =
´
p(σ2, β|D) dβ. Each iteration involves an expec-
tation with respect to the intractable conditional distribution p(β|σ2, D), which
is Laplace approximated, using a single Newton-Raphson iteration. When this
approximate EM algorithm has converged to some value σ2
⋆, one more Newton-
Raphson iteration is performed to compute a ﬁnal Laplace approximation of p(β|σ2
⋆, D),
which is then reported as a Gaussian approximation to the posterior. We refer the
readers to Gelman et al. [2008] for more details on Laplace-EM.
2.3.4 Expectation-Propagation
Like Laplace, Expectation Propagation [EP, Minka, 2001b] generates a Gaussian
approximation of the posterior, but it is based on diﬀerent ideas. The consensus
in machine learning seems to be that EP provides a better approximation than
Laplace [e.g.
Nickisch and Rasmussen, 2008]; the intuition being that Laplace is
‘too local’ (i.e. it ﬁtted so at to match closely the posterior around the mode),
while EP is able to provide a global approximation to the posterior.
Starting from the decomposition of the posterior as product of (nD + 1) factors:
p(β|D) =
1
p(D)
nD
Y
i=0
li(β),
li(β) = F(yiβTxi) for i ≥1,
and l0 is the prior, l0(β) = p(β), EP computes iteratively a parametric approxi-
38

2.3 Fast approximation methods
mation of the posterior with the same structure
qEP(β) =
nD
Y
i=0
1
Zi
qi(β).
(2.4)
Taking qi to be an unnormalised Gaussian densities written in natural exponential
form
qi(β) = exp

−1
2βTQiβ + βTri

,
one obtains for qEP a Gaussian with natural parameters Q = Pn
i=0 Qi and ri =
Pn
i=0 ri; note that the more standard parametrisation of Gaussians may be recov-
ered by taking
Σ = Q−1,
µ = Q−1r.
Other exponential families could be considered for q and the qi’s, see e.g. Seeger
[2005b], but Gaussian approximations seems the most natural choice here.
An EP iteration consists in updating one factor qi, or equivalently (Zi, Qi, ri),
while keeping the other factors as ﬁxed, by moment matching between the hybrid
distribution
h(β) ∝li(β)
Y
j̸=i
qj(β)
and the global approximation q deﬁned in (2.4): compute
Zh
=
ˆ
li(β)
Y
j̸=i
qj(β) dβ
µh
=
1
Zh
ˆ
βli(β)
Y
j̸=i
qj(β) dβ
Σh
=
1
Zh
ˆ
ββTli(β)
Y
j̸=i
qj(β) dβ
and set
Qi = Σ−1
h −Q−i,
ri = Σ−1
h µh −r−i,
log Zi = log Zh −Ψ(r, Q) + Ψ(r−i, Q−i)
where r−i = P
j̸=i rj, Q−i = P
j̸=i Qj, and ψ(r, Q) is the normalising constant of
a Gaussian distribution with natural parameters (r, Q),
ψ(r, Q) =
ˆ
Rp exp

−1
2βTQβ + βTr

dβ = −1
2 log |Q/2π| + 1
2rTQr.
39

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
In practice, EP proceeds by looping over sites, updating each one in turn until
convergence is achieved.
To implement EP for binary regression models, two points must be addressed.
First, how to compute the hybrid moments? For the probit model, these moments
may be computed exactly, see the supplement to the paper, while for the other
links function (such as logistic), numerical (one-dimensional) quadrature may be
used.
Second, how to deal with the prior?
If the prior is Gaussian, one may
simply set q0 to the prior, and never update q0 in the course of the algorithm. For
a Cauchy prior, q0 is simply treated as an extra site.
EP being a fairly recent method, it is currently lacking in terms of supporting
theory, both in terms of algorithmic convergence (does it converge in a ﬁnite num-
ber of iterations?), and statistical convergence (does the resulting approximation
converges in some sense to the true posterior distribution as nD →+∞?). On the
other hand, there is mounting evidence that EP works very well in many problems;
again see e.g. Nickisch and Rasmussen [e.g. 2008].
2.3.5 Discussion of the diﬀerent approximation schemes
Laplace (and its variants) have complexity O(nD + p3), while EP has complexity
O(nDp3). Incidentally, one sees that the number of covariates p is more critical
than the number of instances nD in determining how ‘big’ (how time-intensive to
process) is a given dataset. This will be a recurring point in this chapter.
The p3 term in both complexities is due to the p×p matrix operations performed
by both algorithms; e.g. the Newton-Raphson update (2.2) requires solving a linear
system of order p. EP requires to perform such p3 operations at each site (i.e. for
each single observation), hence the O(nDp3) complexity, while Laplace perform
such operations only once per iteration.
EP is therefore expected to be more
expensive than Laplace.
This remark may be mitigated as follows.
First, one may modify EP so as
to update the global approximation only at the end of each iteration (complete
pass over the data). The resulting algorithm [Van Gerven et al., 2010] may be
easily implemented on parallel hardware: simply distribute the nD factors over
the processors. Even without parallelisation, parallel EP requires only one single
matrix inversion per iteration.
Second, the ‘improved Laplace’ approximation for the marginals described in
Section 2.3.1 requires to perform quite a few basic Laplace approximations, so its
speed advantage compared to standard EP essentially vanishes.
Points that remain in favour of Laplace is that it is simpler to implement than
EP, and the resulting code is very generic: adapting to either a diﬀerent prior,
or a diﬀerent link function (choice of F in 2.1), is simply a matter of writing a
function that evaluates the corresponding function. We have seen that such an
40

2.4 Exact methods
adaptation requires more work in EP, although to be fair the general structure of
the algorithm is not model-dependent. On the other hand, we shall see that EP is
often more accurate, and works in more examples, than Laplace; this is especially
the case for the Cauchy prior.
2.4 Exact methods
We now turn to sampling-based methods, which are ‘exact’, at least in the limit:
one may make the approximation error as small as desired, by running the cor-
responding algorithm for long enough. We will see that all of these algorithms
requires some form of calibration that requires prior knowledge on the shape of
the posterior distribution. Since the approximation methods covered in the pre-
vious section are faster by orders of magnitude than sampling-based methods, we
will assume that a Gaussian approximation q(β) (say, obtained by Laplace or EP)
has been computed in a preliminary step.
2.4.1 Our gold standard: Importance sampling
Let q(β) denote a generic approximation of the posterior p(β|D).
Importance
sampling (IS) is based on the trivial identity
p(D) =
ˆ
p(β)p(D|β) dβ =
ˆ
q(β)p(β)p(D|β)
q(β)
dβ
which leads to the following recipe: sample β1, . . . , βN ∼q, then compute as an
estimator of p(D)
ZN = 1
N
N
X
n=1
w(βn),
w(β) := p(β)p(D|β)
q(β)
.
(2.5)
In addition, since
ˆ
ϕ(β)p(β|D) dβ =
´
ϕ(β)q(β)w(β) dβ
´
q(β)w(β) dβ
one may approximate any posterior moment as
ϕN =
PN
n=1 w(βn)ϕ(βn)
PN
n=1 w(βn)
.
(2.6)
Approximating posterior marginals is also straightforward; one may instance use
kernel density estimation on the weighted sample (βn, w(βn))N
n=1.
41

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Concerning the choice of q, we will restrict ourselves to the Gaussian approxima-
tions generated either from Laplace or EP algorithm. It is sometimes recommended
to use a Student distribution instead, as a way to ensure that the variance of the
above estimators is ﬁnite, but we did not observe any beneﬁt for doing so in our
simulations.
It is of course a bit provocative to call IS our gold standard, as it is sometimes
perceived as an obsolete method. We would like to stress out however that IS is
hard to beat relative to most of the criteria laid out in the introduction:
• because it is based on IID sampling, assessing the Monte Carlo error of the
above estimators is trivial: e.g. the variance of ZN may be estimated as N −1
times the empirical variance of the weights w(βn).
The auto-normalised
estimator 2.6 has asymptotic variance
Eq

w(β)2 {ϕ(β) −µ(ϕ)}2
,
µ(ϕ) =
ˆ
ϕ(β)p(β|D) dβ
which is also trivial to approximate from the simulated βn’s.
• Other advantages brought by IID sampling are: (a) importance sampling
is easy to parallelize; and (b) importance sampling is amenable to QMC
(Quasi-Monte Carlo) integration, as explained in the following section.
• Importance sampling oﬀers an approximation of the marginal likelihood p(D)
at no extra cost.
• Code is simple and generic.
Of course, what remains to determine is whether importance sampling does well
relative to our main criterion, i.e. error versus CPU trade-oﬀ. We do know that IS
suﬀers from a curse of dimensionality: take both q and and the target density π to
be the density of IID distributions: q(β) = Qp
j=1 q1(βj), π(β) = Qp
j=1 π1(βj); then
it is easy to see that the variance of the weights grows exponentially with p. Thus
we expect IS to collapse when p is too large; meaning that a large proportion of the
βn gets a negligible weight. On the other hand, for small to moderate dimensions,
we will observe surprising good results; see Section 2.5. We will also present below
a SMC algorithm that automatically reduces to IS when IS performs well, while
doing something more elaborate in more diﬃcult scenarios.
The standard way to assess the weight degeneracy is to compute the eﬀective
sample size [Kong et al., 1994],
ESS =
nPN
n=1 w(βn)
o2
PN
n=1 w(βn)2
∈[1, N],
42

2.4 Exact methods
which roughly approximates how many simulations from the target distribution
would be required to produce the same level of error. In our simulations, we will
compute instead the eﬃciency factor EF, which is simply the ratio EF = ESS/N.
2.4.2 Improving importance sampling by Quasi-Monte Carlo
Quasi-Monte Carlo may be seen as an elaborate variance reduction technique:
starting from the Monte Carlo estimators ZN and ϕN, see (2.5) and (2.6), one
may re-express the simulated vectors as functions of uniform variates un in [0, 1]d;
for instance:
βn = µ + Cζn,
ζn = Φ−1(un)
where Φ−1 is Φ−1, the N(0, 1) inverse CDF, applied component-wise. Then, one
replaces the N vectors un by a low-discrepancy sequence; that is a sequence of N
vectors that spread more evenly over [0, 1]d; e.g. a Halton or a Sobol’ sequence.
Under appropriate conditions, QMC error converges at rate O(N −1+ǫ), for any
ǫ > 0, to be compared with the standard Monte Carlo rate OP(N −1/2). We refer
to Lemieux [2009] for more background on QMC, as well as how to construct QMC
sequences.
Oddly enough, the possibility to use QMC in conjunction with importance sam-
pling is very rarely mentioned in the literature; see however H¨ormann and Leydold
[2005]. More generally, QMC seems often overlooked in Statistics. We shall see
however that this simple IS-QMC strategy often performs very well.
One drawback of IS-QMC is that we lose the ability to evaluate the approxi-
mation error in a simple manner. A partial remedy is to use randomised Quasi-
Monte Carlo (RQMC), that is, the un are generated in such a way that (a) with
probability one, u1:N is a QMC point set; and (b) each vector un is marginally
sampled from [0, 1]d. Then QMC estimators that are empirical averages, such as
ZN = N −1 PN
n=1 w(βn) become unbiased estimators, and their error may be as-
sessed through the empirical variance over repeated runs. Technically, estimators
that are ratios of QMC averages, such as ϕN, are not unbiased, but for all prac-
tical purposes their bias is small enough that assessing error through empirical
variances over repeated runs remains a reasonable approach.
2.4.3 MCMC
The general principle of MCMC (Markov chain Monte Carlo) is to simulate a
Markov chain that leaves invariant the posterior distribution p(β|D); see Robert and Casella
[2004a] for a general overview. Often mentioned drawbacks of MCMC simulation
are (a) the diﬃculty to parallelize such algorithms (although see e.g. Jacob et al.,
43

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2011 for an attempt at this problem); (b) the need to specify a good starting point
for the chain (or alternatively to determine the burn-in period, that is, the length
of the initial part of the chain that should be discarded) and (c) the diﬃculty to
assess the convergence of the chain (that is, to determine if the distribution of βt
at iteration t is suﬃciently close to the invariant distribution p(β|D)).
To be fair, these problems are not so critical for binary regression models. Re-
garding (b), one may simply start the chain from the posterior mode, or from
a draw of one of the Gaussian approximations covered in the previous section.
Regarding (c) for most standard datasets, MCMC converges reasonably fast, and
convergence is easy to assess visually. The main issue in practice is that MCMC
generates correlated random variables, and these correlations inﬂate the Monte
Carlo variance.
Gibbs sampling
Consider the following data-augmentation formulation of binary regression:
zi
=
βTxi + ǫi
yi
=
sgn(zi)
where z = (z1, . . . , znD)T is a vector of latent variables, and assume for a start
that ǫi ∼N(0, 1) (probit regression). One recognises p(β|z, D) as the posterior
of a linear regression model, which is tractable (for an appropriate prior). This
suggests to sample from p(β, z|D) using Gibbs sampling [Albert and Chib, 1993]:
i.e. iterate the two following steps: (a) sample from z|β, D; and (b) sample from
β|z, D.
For (a), the zi’s are conditionally independent, and follows a truncated Gaussian
distribution
p(zi|β, D) ∝N1
 zi; βTxi, 1

✶{ziyi > 0}
which is easy to sample from [Chopin, 2011b]. For Step (b) and a Gaussian prior
Np(0, Σprior), one has, thanks to standard conjugacy properties:
β|z, D ∼Np (µpost(z), Σpost) ,
Σ−1
post = Σ−1
prior + xxT,
µpost(z) = Σ−1
postxz
where x is the n × p matrix obtained by stacking the xT
i . Note that Σpost and its
inverse need to be computed only once, hence the complexity of a Gibbs iteration
is O(p2), not O(p3).
The main drawback of Gibbs sampling is that it is particularly not generic: its
implementation depends very strongly on the prior and the model. Sticking to
the probit case, switching to another prior requires deriving a new way to update
β|z, D. For instance, for a prior which is a product of Students with scales σj
44

2.4 Exact methods
(e.g. our Cauchy prior), one may add extra latent variables, by resorting to the
well-known representation: βj|sj ∼N1(0, νσ2
j/sj), sj ∼Chi2(ν); with ν = 1 for
our Cauchy prior. Then the algorithm have three steps: (a) an update of the zi’s,
exactly as above; (b) an update of β, as above but with Σprior replaced by the diag-
onal matrix with elements νσ2
j/sj, j = 1, . . . , p; and (c) an (independent) update of
the p latent variables sj, with sj|β, z, D ∼Gamma
 (1 + ν)/2,
 1 + νβ2
j /σ2
j

/2

.
The complexity of Step (b) is now O(p3), since Σprior and Σpost must be recom-
puted at each iteration.
Of course, considering yet another type of prior would require deriving an-
other strategy for sampling β. Then if one turns to logistic regression, things
get rather complicated. In fact, deriving an eﬃcient Gibbs sampler for logistic
regression is a topic of current research; see Fr¨uhwirth-Schnatter and Fr¨uhwirth
[2009]; Gramacy and Polson [2012]; Holmes and Held [2006]; Polson et al. [2013].
In a nutshell, the two ﬁrst papers use the same data augmentation as above, but
with ǫi ∼Logistic(1) written as a certain mixture of Gaussians (inﬁnite for the ﬁrst
paper, ﬁnite but approximate for the second paper), while Polson et al. [2013] use
instead a representation of a logistic likelihood as an inﬁnite mixture of Gaussians,
with a Polya-Gamma as the mixing distribution. Each representation leads to in-
troducing extra latent variables, and discussing how to sample their conditional
distributions.
Since their implementation is so model-dependent, the main justiﬁcation for
Gibbs samplers should be their greater performance relative to more generic algo-
rithms. We will investigate if this is indeed the case in our numerical section.
Hastings-Metropolis
Hastings-Metropolis consists in iterating the step described as Algorithm 6. Much
like importance sampling, Hastings-Metropolis is both simple and generic, that is,
up to the choice of the proposal kernel κ(β⋆|β) (the distribution of the proposed
point β⋆, given the current point β). A naive approach is to take κ(β⋆|β) inde-
pendent of β, κ(β⋆|β) = q(β⋆), where q is some approximation of the posterior.
In practice, this usually does not work better than importance sampling based on
the same proposal, hence this strategy is hardly used.
A more usual strategy is to set the proposal kernel to a random walk: κ(β⋆|β) =
Np(β, Σprop). It is well known that the choice of Σprop is critical for good perfor-
mance. For instance, in the univariate case, if Σprop is too small, the chain moves
slowly, while if too large, proposed moves are rarely accepted.
A result from the optimal scaling literature [e.g. Roberts and Rosenthal, 2001]
is that, for a Np(0, Ip) target, Σprop = (λ2/p)Ip with λ = 2.38 is asymptotically
optimal, in the sense that as p →∞, this choice leads to the fastest exploration.
Since the posterior of a binary regression model is reasonably close to a Gaussian,
45

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Algorithm 6 Hastings-Metropolis iteration
Input β
Output β′
1 Sample β⋆∼κ(β⋆|β).
2 With probability 1 ∧r,
r = p(β⋆)p(D|β⋆)κ(β|β⋆)
p(β)p(D|β)κ(β⋆|β) ,
set β′ = β⋆; otherwise set β′ = β.
we adapt this result by taking Σprop = (λ2/p)Σq in our simulations, where Σq is the
covariance matrix of a (Laplace or EP) Gaussian approximation of the posterior.
This strategy seems validated by the fact we obtain acceptance rates close to the
optimal rate, as given by Roberts and Rosenthal [2001].
The bad news behind this optimality result is that the chain requires O(p) steps
to move a O(1) distance. Thus random walk exploration tends to become slow for
large p. This is usually cited as the main motivation to develop more elaborate
MCMC strategies, such as HMC, which we cover in the following section.
HMC
Hamiltonian Monte Carlo (HMC, also known as Hybrid Monte Carlo, Duane et al.,
1987) is a new type of MCMC algorithm, where one is able to perform several
steps in the parameter space before determining if the new position is accepted
or not. Consequently, HMC is able to make much bigger jumps in the parameter
space than standard Metropolis algorithms.
See Neal [2010b] for an excellent
introduction.
Consider the pair (β, α), where β ∼p(β|D), and α ∼Np(0, M −1), thus with
joint un-normalised density exp {−H(β, α)}, with
H(β, α) = E(β) + 1
2αTMα,
E(β) = −log {p(β)p(D|β)} .
The physical interpretation of HMC is that of a particle at position β, with velocity
α, potential energy E(β), kinetic energy 1
2αTMα, for some mass matrix M, and
therefore total energy given by H(β, α).
The particle is expected to follow a
trajectory such that H(β, α) remains constant over time.
In practice, HMC proceeds as follows:
ﬁrst, sample a new velocity vector,
α ∼Np(0, M −1). Second, move the particle while keeping the Hamiltonian H
46

2.4 Exact methods
constant; in practice, discretisation must be used, so L steps of step-size ǫ are
performed through leap-frop steps; see Algorithm 7 which describes one such step.
Third, the new position, obtained after L leap-frog steps is accepted or rejected
according to probability 1 ∧exp {H(β, α) −H(β⋆, α⋆)}; see Algorithm 8 for a
summary. The validity of the algorithm relies on the fact that a leap-frog step is
“volume preserving”; that is, the deterministic transformation (β, α) →(β1, α1)
has Jacobian one. This is why the acceptance probability admits this simple ex-
pression.
Algorithm 7 Leap-frog step
Input (β, α)
Output (β1, α1)
1 α1/2 ←α −ǫ
2∇βE(β)
2 β1 ←β + ǫα1/2
3 α1 ←α1/2 −ǫ
2∇βE(β1)
Algorithm 8 HMC iteration
Input β
Output β′
1 Sample momentum α ∼Np(0, M).
2 Perform L leap-frog steps (see Algorithm 7), starting from (β, α); call (β⋆, α⋆)
the ﬁnal position.
3 With probability 1 ∧r,
r = exp {H(β, α) −H(β⋆, α⋆)}
set β
′ = β⋆; otherwise set β
′ = β.
The tuning parameters of HMC are M (the mass matrix), L (number of leap-
frog steps), and ǫ (the stepsize). For M, we follow Neal [2010b]’s recommendation
and take M −1 = Σq, an approximation of the posterior variance (again obtained
from either Laplace or EP). This is equivalent to rescaling the posterior so as to
47

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
have a covariance matrix close to identity. In this way, we avoid the bad mixing
typically incurred by strong correlations between components.
The diﬃculty to choose L and ǫ seems to be the main drawback of HMC. The
performance of HMC seems very sensitive to these tuning parameters, yet clear
guidelines on how to choose them seem currently lacking. A popular approach
is to ﬁx Lǫ to some value, and to use vanishing adaptation [Andrieu and Thoms,
2008] to adapt ǫ so as to target acceptance rate of 0.65 (the optimal rate according
to the formal study of HMC by Beskos et al., 2013): i.e. at iteration t, take ǫ = ǫt,
with ǫt = ǫt−1 −ηt(Rt −0.65), ηt = t−κ, κ ∈(1/2, 1) and Rt the acceptance rate
up to iteration t. The rationale for ﬁxing Lǫ is that quantity may be interpreted
as a ‘simulation length’, i.e. how much distance one moves at each step; if too
small, the algorithm may exhibit random walk behaviour, while if too large, it
may move a long distance before coming back close to its starting point. Since the
spread of is already taken into account through M −1 = Σq, we took ǫL = 1 in our
simulations.
NUTS and other variants of HMC
Girolami and Calderhead [2011] proposed an interesting variation of HMC, where
the mass matrix M is allowed to depends on β; e.g. M(β) is set to the Fisher
information of the model. This allows the corresponding algorithm, called RHMC
(Riemanian HMC), to adapt locally to the geometry of the target distribution.
The main drawback of RHMC is that each iteration involves computing derivatives
of M(β) with respect to β, which is very expensive, especially if p is large. For
binary regression, we found RMHC to be too expensive relative to plain HMC, even
when taking into account the better exploration brought by RHMC. This might
be related to the fact that the posterior of a binary regression model is rather
Gaussian-like and thus may not require such a local adaptation of the sampler.
We now focus on NUTS [No U-Turn sampler, Hoﬀman and Gelman, 2013], a
variant of HMC which does not require to specify a priori L, the number of leap-
frog steps. Instead, NUTS aims at keeping on doing such steps until the trajectory
starts to loop back to its initial position. Of course, the diﬃculty in this exercise
is to preserve the time reversibility of the simulated Markov chain. To that eﬀect,
NUTS constructs iteratively a binary tree whose leaves correspond to diﬀerent
velocity-position pairs (α, β) obtained after a certain number of leap-frog steps.
The tree starts with two leaves, one at the current velocity-position pair, and
another leaf that corresponds to one leap-frop step, either in the forward or back-
ward direction (i.e. by reversing the sign of velocity); then it iteratively doubles
the number of leaves, by taking twice more leap frog steps, again either in the
forward or backward direction.
The tree stops growing when at least one leaf
corresponds to a “U-turn”; then NUTS chooses randomly one leaf, among those
48

2.4 Exact methods
leaves that would have generated the current position with the same binary tree
mechanism; in this way reversibility is preserved. Finally NUTS moves the new
position that corresponds to the chosen leaf.
We refer the readers to Hoﬀman and Gelman [2013] for a more precise descrip-
tion of NUTS. Given its complexity, implementing directly NUTS seems to require
more eﬀorts than the other algorithms covered in this chapter. Fortunately, the
STAN package (http://mc-stan.org/) provides a C++ implementation of NUTS
which is both eﬃcient and user-friendly: the only required input is a description
of the model in a probabilistic programming language similar to BUGS. In partic-
ular, STAN is able to automatically derive the log-likelihood and its gradient, and
no tuning of any sort is required from the user. Thus, we will use STAN to assess
NUTS in our numerical comparisons.
2.4.4 Sequential Monte Carlo
Sequential Monte Carlo (SMC) is a class of algorithms for approximating iter-
atively a sequence of distributions πt, t = 0, . . . , T, using importance sampling,
resampling, and MCMC steps. We focus here on the non-sequential use of SMC
[Chopin, 2002b; Del Moral et al., 2006a; Neal, 2001b], where one is only inter-
ested in approximating the ﬁnal distribution πT (in our case, set to the posterior
p(β|D)), and the previous πt’s are designed so as to allow for a smooth progression
from some π0, which is easy to sample from, to πT.
At iteration t, SMC produces a set of weighted particles (simulations) (βn, wn)N
n=1
that approximates πt, in the sense that
1
PN
n=1 wn
N
X
n=1
wnϕ(βn) →Eπt [ϕ(β)]
as N →+∞. At time 0, one samples βn ∼π0, and set wn = 1. To progress
from πt−1 to πt, one uses importance sampling: weights are multiplied by ratio
πt(βn)/πt−1(βn). When the variance of the weights gets too large (which indicates
that too few particles contribute signiﬁcantly to the current approximation), one
resamples the particles: each particle gets reproduced On times, where On ≥0
is random, and such that E(On) = Nwn/ PN
m=1 wm, and PN
n=1 On = N with
probability one. In this way, particles with a low weights are likely to die, while
particles with a large weight get reproduced many times. Finally, one may re-
introduce diversity among the particles by applying one (or several) MCMC steps,
using a MCMC kernel that leaves invariant the current distribution πt.
We focus in this chapter on tempering SMC, where the sequence
πt(β) ∝q(β)1−δt {p(β)p(D|β)}δt
49

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
corresponds to a linear interpolation (on the log-scale) between some distribution
π0 = q, and πT(β) = p(β|D), our posterior. This is a convenient choice in our
case, as we have at our disposal some good approximation q (either from Laplace
or EP) of our posterior. A second advantage of tempering SMC is that one can
automatically adapt the “temperature ladder” δt [Jasra et al., 2011a]. Algorithm
9 describes a tempering SMC algorithm based on such an adaptation scheme:
at each iteration, the next distribution πt is chosen so that the eﬃciency factor
(deﬁned in Section 2.4.1) of the importance sampling step from πt−1 to πt equals
a pre-deﬁned level τ ∈(0, 1); a default value is τ = 1/2.
Algorithm 9 tempering SMC
Operations involving index n must be performed for all n ∈1 : N.
0 Sample βn ∼q(β) and set δ ←0.
1 Let, for δ ∈[δ, 1],
EF(δ) = 1
N
nPN
n=1 wγ(βn)
o2
nPN
n=1 wγ(βn)2
o,
uδ(β) =
p(β)p(D|β)
q(β)
δ
.
If EF(1) ≥τ, stop and return (βn, wn)n=1:N with wn = u1(βn); otherwise,
use the bisection method [Press et al., 2007, Chap. 9] to solve numerically
in δ the equation EF(γ) = τ.
2 Resample according to normalised weights Wn = wn/ PN
m=1 wm, with wn =
uδ(βn); see the supplement for one such resampling algorithm.
3 Update the βn’s through m MCMC steps that leaves invariant πt(β), using
e.g. Algorithm 6 with κ(β⋆|β) = Np(β, Σprop), Σprop = λ ˆΣ, where ˆΣ is the
empirical covariance matrix of the resampled particles.
4 Set δ ←δ. Go to Step 1.
Another part of Algorithm 9 which is easily amenable to automatic calibration
is the MCMC step. We use a random walk Metropolis step, i.e. Algorithm 6
with proposal kernel κ(β⋆|β) = Np(β, Σprop), but with Σprop calibrated to the
empirical variance of the particles ˆΣ: Σprop = λ ˆΣ, for some λ. Finally, one may
also automatically calibrate the number m of MCMC steps, as in Chapter 3, but
in our simulations we simply took m = 3.
In the end, one obtains essentially a black-box algorithm. In practice, we shall
50

2.5 Numerical study
often observe that, for simple datasets, our SMC algorithm automatically reduces
to a single importance sampling step, because the eﬃciency factor of moving from
the initial distribution q to the posterior is high enough. In that case, our SMC
sampler performs exactly as standard importance sampling.
2.5 Numerical study
The point of this section is to compare numerically the diﬀerent methods discussed
in the previous sections, ﬁrst on several datasets of standard size (that are repre-
sentative of previous numerical studies), then in a second time on several bigger
datasets.
We focus on the following quantities: the marginal likelihood of the data, p(D),
and the p marginal posterior distributions of the regression coeﬃcients βj. Re-
garding the latter, we follow Faes et al. [2011] in deﬁning the ‘marginal accuracy’
of approximation q for component j to be
MAj = 1 −1
2
ˆ +∞
−∞
|q(βj) −p(βj|D)| dβj.
This quantity lies in [0, 1], and is scale-invariant. Since the true marginals p(βj|D)
are not available, we will approximate them through a Gibbs sampler run for a
very long time. To give some scale to this criterion, assume q(βj) = N1(βj; µ1, σ2),
p(βj|D) = N1(βj; µ2, σ2), then MAj is 2Φ(−δ/2) ≈1 −0.4 × δ for δ = |µ1 −µ2|/σ
small enough; e.g. 0.996 for δ ≈0.01, 0.96 for δ ≈0.1.
In our results, we will refer to the following four prior/model ‘scenarios’: Gaus-
sian/probit, Gaussian/logit, Cauchy/probit, Cauchy/logit, where Gaussian and
Cauchy refer to the two priors discussed in Section 2.2.1. All the algorithms have
been implemented in C++, using the Armadillo and Boost libraries, and run on
a standard desktop computer (except when explicitly stated). Results for NUTS
were obtained by running STAN (http://mc-stan.org/) version 2.4.0.
2.5.1 Datasets of moderate size
Table 2.1 lists the 7 datasets considered in this section (obtained from the UCI ma-
chine learning repository, except Elections, which is available on the web page of
Gelman and Hill [2006]’s book). These datasets are representative of the numerical
studies found in the literature. In fact, it is a super-set of the real datasets consid-
ered in Girolami and Calderhead [2011], Shahbaba et al. [2011], Holmes and Held
[2006] and also (up to one dataset with 5 covariates) Polson et al. [2013]. In each
case, an intercept have been included; i.e. p is the number of predictors plus one.
51

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Dataset
nD
p
Pima (Indian diabetes)
532
8
German (credit)
999
25
Heart (Statlog)
270
14
Breast (cancer)
683
10
Liver (Indian Liver patient)
579
11
Plasma (blood screening data)
32
3
Australian (credit)
690
15
Elections
2015
52
Table 2.1: Datasets of moderate size (from UCI repository, ex-
cept Elections, from web-site of Gelman and Hill [2006]’s
book): name (short and long version), number of instances
nD, number of covariates p (including an intercept)
Fast Approximations
We compare the four approximation schemes described in Section 2.3: Laplace, Im-
proved Laplace, Laplace EM, and EP. We concentrate on the Cauchy/logit scenario
for two reasons: (i) Laplace EM requires a Student prior; and (ii) Cauchy/logit
seems the most challenging scenario for EP, as (a) a Cauchy prior is more diﬃcult
to deal with than a Gaussian prior in EP ; and (b) contrary to the probit case,
the site update requires some approximation; see Section 2.3.4 for more details.
Left panel of Fig. 2.1 plots the marginal accuracies of the four approximation
schemes across all components and all datasets; Fig.
2.2 does the same, but
separately for four selected datasets; results for the remaining datasets are available
in the supplement.
EP seems to be the most accurate method on these datasets: marginal accuracy
is about 0.99 across all components for EP, while marginal accuracy of the other
approximation schemes tend to be lower, and may even drop to quite small values;
see e.g. the German dataset, and the left tail in the left panel of Fig. 2.1.
EP also fared well in terms of CPU time: it was at most seven times as intensive
as standard Laplace across the considered datasets, and about 10 to 20 times faster
than Improved Laplace and Laplace EM. As expected (see Section 2.3.5), the
largest running time for EP was observed for the dataset with the largest number
of observations (German credit): i.e. 0.14s, while Laplace took 0.02s on the same
data. Of course, the usual caveats apply regarding CPU time comparison, and
how they may depend on the hardware, the implementation, and so on.
We also note in passing the disappointing performance of Laplace EM, which
was supposed to replace standard Laplace when the prior is Student, but which
52

2.5 Numerical study
0
50
100
0.4
0.6
0.8
1.0
value
density
EP
Improved Laplace
Laplace
Laplace EM
●
●
●
●
●
●
●
●
0.00
0.05
0.10
0.15
0.20
0.25
10
20
30
40
50
dim
absolute log error
●EP
Laplace
Figure 2.1: Comparison of approximation schemes across all datasets
of moderate size: marginal accuracies (left), and abso-
lute error for log-evidence versus the dimension p (right);
x−axis range of the left plot determined by range of
marginal accuracies (i.e.
marginal accuracy may drop
below 0.4 for e.g. Laplace-EM).
actually performs not as well as standard Laplace on these datasets.
We refer the reader to the supplement for similar results on the three other
scenarios, which are consistent with those above. In addition, we also represent
the approximation error of EP and Laplace for approximating the log-evidence in
the right panel of Fig. 2.1. Again, EP is found to be more accurate than Laplace
for most datasets (except for the Breast dataset).
To conclude, it seems that EP may be safely be used as a complete replacement
of sampling-based methods on such datasets, as it produces nearly instant results,
and the approximation error along all dimensions is essentially negligible.
Importance sampling, QMC
We now turn to importance sampling (IS), which we deemed our “gold standard”
among sampling-based methods, because of its ease of use and other nice properties
as discussed in Section 2.4.1. We use N = 5 × 105 samples, and a Gaussian EP
proposal.
(Results with a Laplace proposal are roughly similar.)
We consider
ﬁrst the Gaussian/probit scenario, because this is particularly favorable to Gibbs
sampling; see next section. Table 2.2 reports for each dataset the eﬃciency factor
of IS (as deﬁned in Section 2.4.1), the CPU time and two other quantities discussed
53

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
●
●
0.925
0.950
0.975
1.000
EP
Improved Laplace
Laplace
Laplace EM
value
(a) Pima
●
0.92
0.96
1.00
EP
Improved Laplace
Laplace
Laplace EM
value
(b) Heart
0.925
0.950
0.975
1.000
EP
Improved Laplace
Laplace
Laplace EM
value
(c) Breast
●
●
●
●
●
●
0.80
0.85
0.90
0.95
1.00
EP
Improved Laplace
Laplace
Laplace EM
value
(d) German
Figure 2.2: Box-plots of marginal accuracies across the p dimensions,
for the four approximation schemes, and four selected
datasets; plots for remaining datasets are in the supple-
ment. For the sake of readability, scale of y−axis varies
across plots.
54

2.5 Numerical study
below.
IS
IS-QMC
Dataset
EF
CPU
MT
MSE improv.
MSE improv.
= ESS/N
time
speed-up
(expectation)
(evidence)
Pima
99.5%
37.54 s
4.39
28.9
42.7
German
97.9%
79.65 s
4.51
13.2
8.2
Breast
82.9%
50.91 s
4.45
2.6
6.2
Heart
95.2%
22.34 s
4.53
8.8
9.3
Liver
74.2 %
35.93 s
4.76
7.6
11.3
Plasma
90.0%
2.32 s
4.28
2.2
4.4
Australian
95.6%
53.32 s
4.57
12
20.3
Elections
21.39%
139.48 s
3.87
617.9
3.53
Table 2.2: Performance of importance sampling (IS), and QMC im-
portance sampling (IS-QMC), on all datasets, in Gaus-
sian/probit scenario: eﬃciency factor (EF), CPU time
(in seconds), speed gain when using multi-threading In-
tel hyper-threaded quad core CPU (Speed gain MT), and
eﬃciency gain of QMC (see text).
We see that all these eﬃciency factors are all close to one, which means IS works
almost as well as IID sampling would on such datasets.
Further improvement
may be obtained by using either parallelization, or QMC (Quasi-Monte Carlo, see
Section 2.4.2). Table 2.2 reports the speed-up factor obtained when implementing
multi-threading on our desktop computer which has a multi threading quad core
CPU (hence 8 virtual cores). We also implemented IS on an Amazon EC2 instance
with 32 virtual CPUs, and obtained speed-up factors about 20, and running times
below 2s.
Finally, Table 2.2 also reports the MSE improvement (i.e. MSE ratio of IS rela-
tive to IS-QMC) obtained by using QMC, or more precisely RQMC (randomised
QMC), based on a scrambled Sobol’ sequence [see e.g.
Lemieux, 2009]. Speciﬁ-
cally, the table reports the median MSE improvement for the p posterior expecta-
tions (ﬁrst column), and the MSE improvement for the evidence (second column).
The improvement brought by RQMC varies strongly across datasets.
The eﬃciency gains brought by parallelization and QMC may be combined,
because the bulk of the computation (as reported by a proﬁler) is the N likelihood
evaluations, which are trivial to parallelize.
It is already clear that other sampling-based methods do not really have a ﬁght-
ing chance on such datasets, but we shall compare them in the next section for
55

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
the sake of completeness. See also the supplement for results for other scenarios,
which are very much in line with those above.
MCMC schemes
In order to compare the diﬀerent sampling-based methods, we deﬁne the IRIS
(Ineﬃciency Relative to Importance Sampling) criterion, for a given method M
and a given posterior estimate, as follows:
MSEM
MSEIS
× CPUIS
CPUM
where MSEM (resp. MSEIS) is the mean square error of the posterior estimate
obtained from method M (resp. from importance sampling), and CPUM the CPU
time of method M (resp. importance sampling). The comparison is relative to
importance sampling without parallelisation or quasi-Monte Carlo sampling. In
terms of posterior estimates, we consider the expectation and variance of each
posterior marginal p(βj|D). We observe that, in both cases, IRIS does not vary
much across the p components, so we simply report the median of these p values.
Fig 2.3 reports the median IRIS across all datasets. We refer the reader to Section
2.4.3 for how we tuned these MCMC algorithms.
The ﬁrst observation is that all these MCMC schemes are signiﬁcantly less eﬃ-
cient than importance sampling on such datasets. The source of ineﬃciency seems
mostly due to the autocorrelations of the simulated chains (for Gibbs or random
walk Metropolis), or, equivalently, the number of leap-frog steps performed at
each iteration in HMC and NUTS. See the supplement for ACF’s (Autocorrela-
tion plots) to support this statement.
Second, HMC and NUTS do not perform signiﬁcantly better than random-
walk Metropolis.
As already discussed, HMC-type algorithms are expected to
outperform random walk algorithms as p →+∞. But the considered datasets
seem too small to give evidence to this phenomenon, and should not be considered
as reasonable benchmarks for HMC-type algorithms (not to mention again that
these algorithms are signiﬁcantly outperformed by IS on such datasets). We note
in passing that it might be possible to get better performance for HMC by ﬁnely
tuning the quantities ǫ and L on per dataset basis. We have already explained
in the introduction why we think this is bad practice, and we also add at this
stage that the fact HMC requires so much more eﬀort to obtain good performance
(relative to other MCMC samplers) is a clear drawback.
Regarding Gibbs sampling, it seems a bit astonishing that an algorithm spe-
cialised to probit regression is not able to perform better than more generic ap-
proach on such simple datasets. Recall that the Gaussian/probit case is particu-
larly favourable to Gibbs, as explained in Section 2.4.3. See the supplement for
56

2.5 Numerical study
●
●
1e+01
1e+03
1e+05
Gibbs
HMC
NUTS
RW
IRIS
(a) Median IRIS for the p posterior expec-
tations E[βj|D]
●
●
●
●
●
10
1000
Gibbs
HMC
NUTS
RW
IRIS
(b) Median IRIS for the p posterior vari-
ances Var[βj|D]
Figure 2.3: IRIS (Ineﬃciency relative to importance sampling) across
all datasets for MCMC schemes and Gaussian/probit sce-
nario; left (resp. right) panel shows median IRIS when
estimating the p posterior expectations (resp. the p pos-
terior variances).
57

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
Dataset
nD
p
Musk
476
95
Sonar
208
61
DNA
400
180
Table 2.3: Datasets of larger size (from UCI repository): name, num-
ber of instances nD, number of covariates p (including an
intercept)
a comparison of MCMC schemes in other scenarios than Gaussian/probit; results
are roughly similar, except that Gibbs is more signiﬁcantly outperformed by other
methods, as expected.
2.5.2 Bigger datasets
Finally, we turn our attention to the bigger datasets summarised by Table 2.3.
These datasets not only have more covariates (than those of the previous sec-
tion), but also stronger correlations between these covariates (especially Sonar
and Musk). We consider the probit/Gaussian scenario.
Regarding fast approximations, we observe again that EP performs very well,
and better than Laplace; see Figure 2.5. It is only for DNA (180 covariates) that
the EP approximation starts to suﬀer.
Regarding sampling-based methods, importance sampling may no longer be used
as a reference, as the eﬀective sample size collapses to a very small value for these
datasets. We replace it by the tempering SMC algorithm described in Section
2.4.4. Moreover, we did not manage to calibrate HMC so as to obtain reason-
able performance in this setting. Thus, among sampling-based algorithms, the
four remaining contenders are: Gibbs sampling, NUTS, RWHM (random walk
Hastings-Metropolis), and tempering SMC. Recall that the last two are calibrated
with the approximation provided by EP.
Figure 2.5 reports the “eﬀective sample size” of the output of these algorithms
when run for the same ﬁxed CPU time (corresponding to 5 × 105 iterations of
RWHM), for the p posterior expectations (left panels), and the p posterior vari-
ances (right panels); here “eﬀective sample size” is simply the posterior variance
divided by the MSE of the estimate (across 50 independent runs of the same al-
gorithm).
No algorithm seems to vastly outperform the others consistently across the three
datasets.
If anything, RWMH seems to show consistently best or second best
performance.
Still, these results oﬀer the following insights. Again, we see that Gibbs sam-
58

2.5 Numerical study
●
●●●●
●
●●●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
0.85
0.90
0.95
1.00
EP
Laplace
value
(a) Musk
●
●●●●
●
0.85
0.90
0.95
1.00
EP
Laplace
value
(b) Sonar
●
●
●
●
●
0.8
0.9
1.0
EP
Laplace
value
(c) DNA
Figure 2.4: Marginal accuracies across the p dimensions of EP and
Laplace, for datasets Musk, Sonar and DNA
59

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
●
●
●●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
0
2000
4000
6000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
●●●●●●●●●●●●●●●●●●●
●
●
●●
●
●
●
●
●
●
●
●
●
0
10000
20000
30000
40000
50000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
(a) Musk
●
●●
●
●●
●
●
●
●
●●
●●
●●
●
●
●
●
●●
●
●
●
●
●
●
0
1000
2000
3000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
●
●●●
●
●●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
0
1000
2000
3000
4000
5000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
(b) Sonar
●
●
●
●
●●
●●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
0
500
1000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
●
●
●
●
●●
●●●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
0
500
1000
1500
2000
Gibbs
NUTS
RWMH
SMC
Ess for fixed CPU time
(c) DNA
Figure 2.5: Eﬀective sample size for a ﬁxed CPU time for sampling-
based algorithms: posterior expectations (left), and pos-
terior variances (right) for datasets (from top to bottom):
Musk, Sonar, and ADN
60

2.6 Variable selection
pling, despite being a specialised algorithm, does not outperform signiﬁcantly more
generic algorithms. Recall that the probit/Gaussian scenario is very favourable to
Gibbs sampling; in other scenarios (results not shown), Gibbs is strongly domi-
nated by other algorithms.
More surprisingly, RWHM still performs well despite the high dimension. In
addition, RHHM seems more robust than SMC to an imperfect calibration; see
the DNA example, where the error of the EP approximation is greater.
On the other hand, SMC is more amenable to parallelisation, hence on a parallel
architecture, SMC would be likely to outperform the other approaches.
2.6 Variable selection
We discuss in this section the implications of our ﬁndings on variable selection.
The standard way to formalise variable selection is to introduce as a parameter
the binary vector γ ∈{0, 1}p, and to deﬁne the likelihood
p(D|β, γ) =
nD
Y
i=1
F(yiβT
γ xγ,i)
where βγ (resp. xγ,i) is the vector of length |γ| that one obtains by excluding
from β (resp. xi) the components j such that γj = 0. Several priors may be
considered for this problem [Chipman et al., 2001], but for simplicity, we will take
p(β, γ) = p(β)p(γ) where p(β) is either the Cauchy prior or the Gaussian prior
discussed in Section 2.2.1, and p(γ) is the uniform distribution with respect to the
set {0, 1}p, p(γ) = 2−p.
Computationally, variable selection is more challenging than parameter esti-
mation, because the posterior p(β, γ|D) is a mixture of discrete and continuous
components. If p is small, one may simply perform a complete enumeration: for all
the 2p possible values of γ, approximate p(D|γ) using e.g. importance sampling. If
p is large, one may adapt the approach of Sch¨afer and Chopin [2011], as described
in the next sections.
2.6.1 SMC algorithm of Sch¨afer and Chopin [2011]
In linear regression, yi = βT
γ xγ,i + εi, εi ∼N1(0, σ2), the marginal likelihood
p(D|γ) is available in close form (for a certain class of priors). Sch¨afer and Chopin
[2011] use this property to construct a tempering SMC sampler, which transi-
tions from the prior p(γ) to the posterior p(γ|D), through the tempering sequence
πt(γ) ∝p(γ)p(D|γ)δt, with δt growing from 0 to 1. This algorithm has the same
structure as Algorithm 9 (with the obvious replacements of the β’s by γ’s and so
61

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
on.) The only diﬀerence is the MCMC step used to diversify the particles after re-
sampling. Instead of a random walk step (which would be ill-deﬁned on a discrete
space), Sch¨afer and Chopin [2011] use a Metropolis step based on an independent
proposal, constructed from a sequence of nested logistic regressions: proposal for
ﬁrst component γ1 is Bernoulli, proposal for second component γ2, conditional on
γ1, corresponds to a logistic regression with γ1 and an intercept as covariates, and
so on. The parameters of these p successive regressions are simply estimated from
the current particle system. Sch¨afer and Chopin [2011] show that their algorithm
signiﬁcantly outperform several MCMC samplers on datasets with more than 100
covariates.
2.6.2 Adaptation to binary regression
For binary regression models, p(D|γ) is intractable, so the approach of Sch¨afer and Chopin
[2011] cannot be applied directly. On the other hand, we have seen that (a) both
Laplace and EP may provide a fast approximation of the evidence p(D|γ); and
(b) both importance sampling and the tempering SMC algorithm may provide an
unbiased estimator of p(D|γ).
Based on these remarks, Sch¨afer [2012] in his PhD thesis considered the following
extension of the SMC algorithm of Sch¨afer and Chopin [2011]: in the sequence
πt(γ) ∝p(γ)p(D|γ)δt, the intractable quantity p(D|γ) is simply replaced by an
unbiased estimator (obtained with importance sampling and the Gaussian proposal
corresponding to Laplace). The corresponding algorithm remains valid, thanks to
pseudo-marginal arguments [see e.g. Andrieu and Roberts, 2009]. Speciﬁcally, one
may re-interpret the resulting algorithm as a SMC algorithm for a sequence of
distribution of an extended space, such that marginal in γ is exactly the posterior
p(D|γ) at time t = T. In fact, it may be seen as a particular variant of the SMC2
algorithm of Chopin et al. [2013a].
2.6.3 Numerical illustration
We now compare the proposed SMC approach with the Gibbs sampler of Holmes and Held
[2006] for sampling from p(β, γ|D), on the Musk dataset. Both algorithms were
given the same CPU budget (15 minutes), and were run 50 times; see Figure 2.6.
Clearly, the SMC sampler provides more reliable estimates of the inclusion proba-
bilities p(γj = 1|D) on such a big dataset. See also the PhD dissertation of Sch¨afer
[2012] for results consistent with those, on other datasets, and when comparing to
the adaptive reversible jump sampler of Lamnisos et al. [2013].
62

2.6 Variable selection
0
25
50
75
100
0.00
0.25
0.50
0.75
1.00
covariates
(a) Gibbs
0
25
50
75
100
0.00
0.25
0.50
0.75
1.00
covariates
(b) SMC
Figure 2.6: Variation of estimated inclusion probabilities p(γj = 1|D)
over 50 runs for the p covariates of Musk dataset: median
(red line), 80% conﬁdence interval (white box); the black-
box extends until the maximum value.
63

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2.6.4 Spike and slab
We also note in passing that a diﬀerent approach to the variable selection problem
is to assign a spike and slab prior to β [George and McCulloch, 1993a]:
p(β) =
p
Y
j=1

λN1(βj; 0, v2
0) + (1 −λ)N1(βj; 0, v2
1)
	
,
v2
0 ≪v2
1
where λ ∈(0, 1), v2
0 and v2
1 are ﬁxed hyper-parameters. This prior generates a
continuous posterior (without point masses at βj = 0), which is easier to sample
from than the discrete-continuous mixture obtained in the standard formulation
of Bayesian variable selection. It would be interesting to see to which extent our
discussion and ﬁndings extend to this particular type of posteriors; see for Chapter
4 for how to deal with such priors in EP.
2.7 Conclusion and extensions
2.7.1 Our main messages to users
Our ﬁrst and perhaps most important message to end users is that Bayesian com-
putation (for binary regression) is now suﬃciently fast for routine use: if the right
approach is used, results may be obtained near instantly on a standard computer,
at least on simple datasets.
Concretely, as far as binary regression is concerned, our main recommendation
is to always use EP. It is very fast, and its approximation error is negligible in
most cases (for such models). EP requires some expertise to implement, but the
second author will release shortly a R package that computes the EP approximation
for any logit or probit model. The only drawback of EP is the current lack of
theoretical support. We learnt however while ﬁnishing this manuscript that Simon
Barthelm´e and Guillaume Dehaene (personal communication) established that the
error rate of EP is O(n−2
D ) in certain models (where nD is the sample size). This
seems to explain why EP often performs so well.
In case one wishes to assess the EP error, by running in a second step some exact
algorithm, we would recommend to use the SMC approach outlined in Section 2.4.4
(i.e.
with initial particles simulated from the EP approximation).
Often, this
SMC sampler will reduce to a single importance sampling step, and will perform
extremely well. Even when it does not, it should provide decent performance,
especially if run on (and implemented for) a parallel architecture. Alternatively, on
a single-core machine, random walk Metropolis is particularly simple to implement,
and performs surprisingly well on high-dimensional data (when properly calibrated
using EP).
64

2.7 Conclusion and extensions
2.7.2 Our main message to Bayesian computation experts
Our main message to Bayesian computation scientists was already in the title of
this chapter: leave Pima Indians alone, and more generally, let’s all refrain from
now on from using datasets and models that are too simple to serve as a reasonable
benchmark.
To elaborate, let’s distinguish between specialised algorithms and generic algo-
rithms.
For algorithms specialised to a given model and a given prior (i.e. Gibbs sam-
plers), the choice of a “benchmark” reduces to the choice of a dataset. It seems
unfortunate that such algorithms are often showcased on small datasets (20 co-
variates or less), for which simpler, more generic methods perform much better.
As a matter of fact, we saw in our simulations that even for bigger datasets Gibbs
sampling does not seem to oﬀer better performance than generic methods.
For generic algorithms (Metropolis, HMC, and so on), the choice of a bench-
mark amounts to the choice of a target distribution. A common practice in papers
proposing some novel algorithm for Bayesian computation is to compare that al-
gorithm with a Gibbs sampler on a binary regression posterior for a small dataset.
Again, we see from our numerical study that this benchmark is of of limited in-
terest, and may not be more informative than a Gaussian target of the same
dimension. If one wishes to stick with binary regression, then datasets with more
than 100 covariates should be used, and numerical comparisons should include at
least a properly calibrated random walk Metropolis sampler.
2.7.3 Big data and the p3 frontier
Several recent papers [Bardenet et al., 2015; Scott et al., 2013; Wang and Dunson,
2013] have approached the ’big data’ problem in Bayesian computation by fo-
cussing on the big nD (many observations) scenario. In binary regression, and
possibly in similar models, the big p problem (many covariates) seems more criti-
cal, as the complexity of most the algorithms we have discussed is O(nDp3). Indeed,
we do not believe that any of the methods discussed in this chapter is practical for
p ≫1000. The large p problem may be therefore the current frontier of Bayesian
computation for binary regression.
Perhaps one way to address the large p problem is to make stronger approxima-
tions; for instance by using EP with an approximation family of sparse Gaussians.
Alternatively, one may use a variable selection prior that forbids that the number
of active covariates is larger than a certain threshold.
65

2 Leave Pima indians alone: binary regression as a benchmark for Bayesian computation
2.7.4 Generalising to other models
We suspect some of our ﬁndings may apply more generally to other models (such
as certain generalised linear models), but, of course, further study is required to
assess this statement.
On the other hand, there are two aspects of our study which we recommend to
consider more generally when studying other models: parallelisation, and taking
into account the availability of fast approximations. The former has already been
discussed. Regarding the latter, binary regression models are certainly not the only
models such that some fast approximations may be obtained, whether through
Laplace, INLA, Variational Bayes, or EP. And using this approximation to cal-
ibrate sampling-based algorithms (Hastings-Metropolis, HMC, SMC, and so on)
will often have a dramatic impact on the relative performance of these algorithms.
Alternatively, one may also discover in certain cases that these approximations are
suﬃciently accurate to be used directly.
66

3
Computation of Gaussian orthant probabilities in high
dimension
Status: To appear in Statistics and Computing.
3.1 Introduction
There are many applications where computing an orthant probability in high di-
mension with respect to a Gaussian or Student distribution is an issue of interest.
For instance it is common in statistics to compute the likelihood of models, where
we observe only an event with respect to multivariate Gaussian random variables.
In Econometrics, the multivariate probit model [Train, 2009], where we observe a
decision among J alternative choices each of them corresponding to a Gaussian
utility, is commonly studied. It can be written as an orthant problem. Other
such models are the spatial probit [LeSage et al., 2011] and Thurstonian models
[Yao and Bockenholt, 1999]. Other applications than direct modelization can be
found, such as multiple comparison tests [Hochberg and Tamhane, 1987], where
the integration is done with respect to a Student (see Bretz et al. [2001] for an
example). Orthant probabilities are also of interest in other ﬁelds than statistics,
i.e. stochastic programming [Prekopa, 1970], structural system reliability [Pandey,
1998], engineering, ﬁnance, etc.
The problem at hand is the computation of the integral,
ˆ
[a,b]
(2π)−d
2 |Σ|−1
2 exp

−1
2(y −m)tΣ−1(y −m)

dy.
(3.1)
where a, b ∈Rd. The Student case will be written as a mixture of the above
integral with an inverse Chi-square (see Section 3.5.1).
67

3 Computation of Gaussian orthant probabilities in high dimension
Many algorithms have been proposed to compute (3.1); for a review see Genz and Bretz
[2009].
They can be divided into two groups.
The ﬁrst are numerical algo-
rithms to deal with small dimensional integrals. In dimension 3 there exist algo-
rithms [Genz and Bretz, 2009] where after sphericization, such that the Gaussian
has an identity covariance matrix, one applies recursively numerical computa-
tions of the error function. For higher dimensions than three Minwa et al. [2003]
propose to express orthant probabilities as diﬀerences of orthoscheme probabili-
ties, where an orthoscheme is (3.1) with correlation matrix Ω= (ωij) satisfying
ωij = 0
∀i, j
|i −j| > 1. This can be easily computed by recursion. How-
ever the decomposition in orthoscheme probabilities has factorial complexity. The
second group of algorithms is Monte Carlo based and may be used for dimen-
sions higher than 10. In particular GHK due to Geweke [1991], Keane [1993] and
Hajivassiliou et al. [1996] and conjointly to Genz [1992], has been widely adopted
for the applications described above.
In this chapter we show that in the case of Markovian covariances (i.e. covari-
ances that can be written as those of Markovian processes), the GHK algorithm
estimates the normalizing constant of a state space model (SSM), using sequen-
tial importance sampling (SIS) with optimal proposal. We show in addition for a
ﬁrst order autoregressive process (henceforth AR(1)) that the normalized variance
diverges exponentially fast.
To avoid this behavior we propose to use a particle ﬁlter. We extend this method-
ology to the non Markovian case by using Sequential Monte Carlo (SMC). SMC
allows additional gain in eﬃciency by considering diﬀerent MCMC moves and pro-
posals. In addition the algorithm is adaptive and simpliﬁes automatically to the
GHK if the integral is simple enough. In our numerical experiments we ﬁnd a
substantial improvement.
We start by reviewing the existing GHK algorithm (Section 3.2), we then discuss
the algorithm’s behavior for Markovian covariance matrices and propose an exten-
sion to higher dimensions (Section 3.3). In Section 3.4 we extend this proposal to
arbitrary covariance matrices. We propose some extensions for the simulation of
truncated distributions and for other distributions (3.5). Finally we present some
numerical results and conclude (Sections 3.6 and 3.7).
Notations
For any vector x ∈Rp for i ≤p we write x<i ∈Ri for the vector of
the i −1 ﬁrst components, and we take a : b = {a, · · · , b}. We let x<1 = ∅, and
also write xi:j for the vector (xi, xi+1, · · · , xj); Φ, ϕ are respectively the N(0, 1)
Gaussian cdf and pdf, we write ϕ(x|A) for the pdf,
ϕ(x)
Φ(A)✶A(x), of a Gaussian
truncated to the set A ⊂R evaluated in x. We will also abuse notation and use
Φ(A) to denote the probability of a set when A ⊂R. For instance Φ([a, b]) =
Φ(b) −Φ(a).
68

3.2 Geweke-Hajivassiliou-Keane (GHK) simulator
3.2 Geweke-Hajivassiliou-Keane (GHK) simulator
From now on to simplify notations, and without loss of generality, we limit our-
selves to the study of the following multidimensional integral:
F(a, b, Σ) =
ˆ
[a,b]
(2π)−d
2|Σ|−1
2 exp

−1
2ytΣ−1y

dy
(3.2)
with a, b ∈Rd. Note that the extension to integrals where some components of
the vectors a, b are respectively −∞and ∞is direct.
Let Γ be the Cholesky decomposition of Σ, i.e. Σ = ΓΓt with Γ = (γij), γii > 0
and γij = 0 if j > i. We can write the previous equation after the change of
variable η = Γ−1y for which dη = |Γ|−1dy:
F(a, b, Σ) =
ˆ
b≥Γη≥a
(2π)−d
2 exp

−1
2ηtη

dη,
the i-th truncation being such that
1
γii

ai −Pi−1
j=1 γijηj

≤ηi ≤
1
γii

bi −Pi−1
j=1 γijηj

,
from the positivity of the (γii). Thus we can write:
F(a, b, Σ) =
ˆ
d
Y
i=1
ϕ(ηi)1{Bi(η<i)}(ηi)dη1:d =
ˆ
d
Y
i=1
Φ (Bi(η<i)) ϕ(ηi|Bi (η<i)) dη1:d,
where the set Bi(η<i) = {ηi :
1
γii

ai −Pi−1
j=1 γijηj

≤ηi ≤
1
γii

ai −Pi−1
j=1 γijηj

}
is an interval.
The GHK algorithm is an importance sampling algorithm based on this struc-
ture.
It proposes particles distributed under Qd
i=1 ϕ(ηi|Bi (η<i)) and evaluates
the average of the weights wn = Qd
i=1 Φ (Bi(ηn
<i)). The algorithm is described in
pseudo-code in Alg. 10.
Algorithm 10 GHK simulator
for m ∈1 : M do
Sample: ηm
1:d ∼Qd
i=1 ϕ(ηi|Bi(η<i))
Weights⋆: wm = Qd
i=1 Φ (Bi(ηm
<i))
end for
return
1
M
PM
i=1 wi
⋆Recall that Φ(Bi(ηm
<i) can be computed as a diﬀerence of two one dimensional cdf for the trun-
cation deﬁned above.
Algorithm 10 outputs an unbiased estimator of integral (3.1).
69

3 Computation of Gaussian orthant probabilities in high dimension
To generate truncated Gaussian variables the usual approach in the GHK sim-
ulator is to use the inverse cdf method. We follow this approach in the rest of
the chapter except where stated otherwise. When the numerical stability of the
inverse cdf is an issue we will use the algorithm proposed in Chopin [2011a].
In the next section we will study with more care the case where the covariance
matrix of the underlying Gaussian vector has a Markovian structure.
3.3 The Markovian case
When the covariance matrix is Markovian, that is a matrix for which the inverse is
tri-diagonal, the simulation step of Alg. 10 is the simulation of a Markov process
(x1:t). At time t the weights depend on xt−1 only. Let us take a lag 1 autoregressive
process (AR(1)) for the purpose of exposition, and study the probability of it being
in some hyperrectangle [a, b] = [a1, b1] × · · · × [aT, bT]. The integral of interest is
therefore:
ˆ
TY
t=1
1{[at,bt]}(xt)ϕ(xt; ̺txt−1, σ2
t )dx1:T.
(3.3)
The GHK algorithm consists in sampling from the Markov process:
xt|xt−1 ∼ϕ
 ̺txt−1, σ2
t |Bt(xt)

.
The matrix Σ−1 is tridiagonal, the weights at time t are therefore Φ( bt−̺txt−1
σt
) −
Φ( at−̺txt−1
σt
).
Eq.
3.3 can be seen as the likelihood of the state space model
[Capp´e et al., 2005]:
xt|xt−1 ∼ϕ(xt; ̺txt−1, σ2
t )
yt|xt ∼1{[at,bt]}(xt)
where (yt)t is observed. The GHK can be interpreted as a sequential importance
sampler (SIS) using proposal ϕ(̺txt−1, σ2
t |Bt(xt)).
3.3.1 Toy example
Let us specify a bit more the problem to simplify notation and show some prop-
erties of a thus deﬁned algorithm.
Consider the problem of ﬁnding the probability that an AR(1),
Xt = ̺Xt−1 + εt,
|̺| < 1
is inside the hyper-cube [0, b] × · · · × [0, b], for some b > 0. We have set σ = 1,
a = 0 and ̺ a constant.
70

3.3 The Markovian case
The GHK algorithm consists in this case in simulating the above Markov chain
constrained to [0, b] and in computing under this distribution the products of the
weights QT
t=1 [Φ(b −̺Xt) −Φ(−̺Xt)]. The simulations are therefore generated by
the Markov probability kernel
P b(x, dy) =
ϕ(y; ̺x, 1)
Φ(b −̺x) −Φ(−̺x)✶[0,b](y)dy,
|̺| < 1.
(3.4)
For this model we have the following proposition:
Proposition 3.3.1 For the Markov model deﬁned by (3.4), the normalized square
product of weights of the normalizing constant has the following behavior:
lim inf
T→∞
(
E
" 
QT
t=1
 Φ (b −̺Xt) −Φ (−̺Xt)
2
exp{2TEπ log (Φ(b −̺X) −Φ(−̺X))}
!#)
1
√
T
> exp {Vπ [ψ(X)] + τ} ,
(3.5)
where subscript π denotes integration with respect to the invariant distribution of
P b(x, dy), the other expectation is taken relatively to the Markov chain (Xt), and
ψ : x 7→log (Φ(b −̺x) −Φ(−̺x)), τ = 2 P∞
k=1 cov(X0, Xk).
Proof:
A detailed proof is given in appendix 3.A.
□
Under V -Uniform ergodicity, that follows from our proof, the denominator is the
square of the limit of the product of weights and can be interpreted as a scaling
factor.
Thus the result above shows that this renormalized squared estimator
diverges exponentially fast as the dimension of the integral increases.
Remark 3.3.1 In the course of the proof we showed that the normalizing constant
has a log-normal limiting distribution, resulting in a skewed distribution. We expect
that the distribution of the estimator will have its mode away from the expected
value resulting in some apparent bias. In fact one can show that the normalized
third order moment will also grow exponentially.
GHK has quadratic complexity however we can show that for at least one co-
variance structure the variance diverges exponentially fast. This fully justiﬁes the
use of an algorithm of higher computational complexity. In the following section
we propose a natural extension to deal with this issue in the Markovian case.
3.3.2 Particle ﬁlter (PF)
PF is a common extension of SIS that corrects the weight degeneracy problem. The
solution brought by particle ﬁltering [Gordon et al., 1993] is to use a resampling
71

3 Computation of Gaussian orthant probabilities in high dimension
step, i.e.
to kill those particles with low weights and to replicate those with
high contribution. At time t one resamples the particles by sampling from the
distribution PM
m=1 W m
t δxm
t (dx) where W m
t stands for the m-th renormalized weight
at time t, and δx(dx′) the Dirac measure in x. All the weights are then set to one.
We use an adaptive version of this algorithm where the resampling step is trig-
gered only when the ESS of the weight is lower than some threshold, where the
ESS is deﬁned as
PN
i=1 wi
2
P
i w2
i
∈[1, M] ,
and indicates the number of draws from the independent distribution to obtain
the same variance. Note that it is closely related to the inverse of equation (3.5),
hence we expect that without resampling it goes to zero with exponential speed.
We deﬁne the state space model:
xt|xt−1 ∼gt(xt|xt−1),
yt|xt ∼ft(yt|xt)
One can use a PF to compute the likelihood of such model,
L(y1:T) =
ˆ
TY
t=1
gt(xt|xt−1)ft(yt|xt)g0(x0)dx0:T
A PF with proposal distribution qt(xt, xt−1) is described in Alg. 23. Our appli-
cation corresponds to the special case where:
gt(xt|xt−1) = ϕ(xt),
ft(yt|xt, xt−1) = 1{Bt(x<t)}(xt),
qt(xt|xt−1) = ϕ(xt|Bt(x<t)),
where the set Bt(x<t) depends on xt−1 only.
The proposal thus deﬁned corresponds to the optimal one [Doucet et al., 2000],
that is the distribution proportional to ft(yt|xt)gt(xt|xt−1) in our case proportional
to ϕ(xt)1{Bt(x<t)}(xt) hence the truncated Gaussian. The weights are given by the
normalizing constant
´
ft(yt|xt)gt(xt|xt−1)dxt, in our case Φ(Bt(x<t)).
To resample we propose to use systematic resampling [Carpenter et al., 1999]
(for other approaches see Douc et al. [2005]). Systematic resampling is described
in Algorithm 13 (Appendix 3.B).
The particle ﬁlter thus deﬁned outputs an unbiased estimator of the likelihood
Del Moral [1996a], and thus the orthant probability in our case.
Note that the output of Algorithm 23 is of the form of a product of terms smaller
than one, in our case those terms can be very small and lead to numerical issues.
One way of dealing with this issue is to rewrite all the algorithm in log scale.
72

3.3 The Markovian case
Algorithm 11 Particle Filter
Input: M the number of particles
Sample: Sample xi
0 ∼g0(.)
for t = 1 : T −1 do
if ESS < η⋆then
Z ←Z × { 1
M
PM
i=1 wi
t}
Resample aj
t ∼P
i
wi
t
P
j wj
t δi using algorithm 13, set wj
t ←1
else
a1:M
t
= 1 : M
end if
Sample xi
t+1 ∼qt+1(.|x
ai
t
t )
Set wi
t+1 ←wi
t
ft+1(yt+1|xi
t+1)gt+1(xi
t+1|x
ai
t
t )
qt+1(xi
t+1|x
ai
t
t )
end for
return Z × 1
M
PM
i=1 wi
T
Remark 3.3.2 As we are here in the special case of being able to sample from
the optimal distribution (as shown in Section 3.3) one could resort to the auxiliary
particle ﬁlter (APF, Pitt and Shephard [1999]). In fact in this special case the
algorithm amounts to exchanging the resampling step and the move step of the
particle ﬁlter. We tested this approach on some Markov processes and observed no
improvements in term of variance on repeated draws.
Example 3.3.1 We can show that the previous process (Section 3.3.1) beneﬁts
from resampling when the ESS goes beneath a given level.
Figure (3.1) shows that the GHK algorithm’s variance increases more quickly
as compared to the PF (that seem to have some stable variance on the considered
dimension). In addition the distribution of the GHK estimator seem to be skewed
towards smaller values as T increases. This results in some bias on the last box-
plot. As described in remark 3.3.1 this behavior is due to the log-Normal limiting
distribution of the output of the algorithm. The skewness coeﬃcient increases ex-
ponentially with T.
73

3 Computation of Gaussian orthant probabilities in high dimension
●●
●
●
●
●
●
●
●
●
●
●
●
−50
−40
−30
100
100
120
120
140
140
160
160
180
180
200
200
dimension
value
(a) variance
0
250
500
750
1000
0
50
100
150
200
dimension
Ess
(b) ESS
Figure 3.1: Estimates of Orthant probabilities by Particle Filter
Estimation of the log probability that an AR(1) process (deﬁned previously) with ̺ = 0.7 has all
its component in [0, 15]. GHK sampler (grey) and PF (white) on various dimension from 100 to
200. On the right panel the two ESS for dimension 200. On both cases M is set to 1000.
Thurstonian Model
Thurstonian models arise in Psychology and Economics [Yao and Bockenholt, 1999]
to describe the ranking of p alternatives by n individuals (referred to as judges).
Suppose that we observe the rank ri = (k1i, · · · , kp,i) of some p independent
Gaussian random variables,
xi,j = βj + σεi,j,
where εi,ji.i.d
∼N(0, 1). The likelihood of one observation is an orthant probability:
Pθ{Xp > · · · > X1} =
ˆ
p
Y
i=1
1{xi>xi−1}ϕ(xi|βj, σ2)dx1:p
(3.6)
with the convention that X0 = −∞.
This model is similar to the previous one but with ρ = 1.
74

3.4 Non Markovian case
●
●
●
●
●
−80
−60
−40
−20
1
1
2
2
3
3
4
4
5
5
number of observations
value
(a) Estimates
0
250
500
750
1000
0
10
20
30
40
dimension
y1
(b) ESS
Figure 3.2: Estimates of the likelihood of a Thurstonian model by
Particle Filter and by GHK
Estimation of the likelihood of a Thurstoninan model with p = 10 and the number of observations
is ranging from 1 to 5 the PF (white) and the GHK (grey). The threshold ESS is set to 0.5M and
the number of particles is set to M = 1000. The right panel shows the ESS of both algorithms for
T = 1 and p = 40.
We ﬁnd that the likelihood is estimated with smaller variance.
In addition,
because of the heavy tail distribution of the GHK simulator’s output we observe
a bias (see Figure 3.2). Again one can explain the strong observed bias by remark
3.3.1 and the fact that we do not replicate enough the experiment to observe the
tail of the distribution. In addition as suggested above the ESS of the GHK seems
to decrease exponentially fast to zero.
From this observation we could apply this algorithm to perform inference by
using Particle MCMC [Andrieu et al., 2010a], where this estimation of the likeli-
hood can be plugged in a Random walk Metropolis Hastings and still target the
appropriate distribution.
3.4 Non Markovian case
For more general covariances we propose to use Sequential Monte Carlo (SMC)
[Del Moral et al., 2006b]. As previously we will base the algorithm on the proposal
75

3 Computation of Gaussian orthant probabilities in high dimension
of GHK, increasing the dimension of the problem at each time step. However we
now have an additional degree of freedom: the order in which we incorporate the
variables. In the following section we study an approach to ordering the variables.
3.4.1 Variable ordering
We follow Gibson et al. [1994] in ordering the variables from the most diﬃcult to
the simplest, where diﬃcult constraints are considered to be the one that impact
the most the probability.
However we cannot evaluate exactly the probabilities as it is our ﬁnal goal.
Instead Gibson et al. [1994] propose to replace the simulations by the expected
value of the truncated Gaussian.
The algorithm starts by choosing the ﬁrst index i1, and deﬁning η1 as follows:
i1 = arg min
1≤k≤T Φ
 ak
γkk
, bk
γkk

,
η1 =
1
Φ
h
ai1
γi1i1 ,
bi1
γi1i1
i
ˆ

ai1
γi1i1
,
bi1
γi1i1
 ηϕ(η)dη
i.e. the smallest possible probability that the Gaussian will be in [ak, bk]. This
enables an approximation of the next probability as a function of i2.
i2 = arg min
2≤k≤T Φ
 1
˜γkk
(ak −˜γ1,kη1), 1
˜γkk
(bk −˜γ1,kη1)

.
where (˜γij) = ˜Γ is the Cholesky decomposition of the matrix after substituting the
ﬁrst and the i1th variable.
We end up with the desired vector (i1, · · · , iT) that gives us the order in which
to choose the covariances and truncation points. The algorithm is summed up by
Alg. 14 in appendix 3.C. The algorithm has quadratic time complexity, however
its cost is negligible as compared to the subsequent Monte Carlo algorithm.
We show the use of the reordering in moderate dimensions (50 and 60) on the
GHK simulator. This is already a great improvement especially as the dimension
increases.
Figure (3.3), shows boxplots of 50 repetitions of the GHK for both
ordered (white) and non-ordered (grey) inputs.
76

3.4 Non Markovian case
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
−5.030805e−17
2.362210e−16
5.227500e−16
8.092791e−16
1.095808e−15
value
(a) Dimension 50
●●
●
●
●●●●
●
●●●●
●●●●
●
●
●●●
●
●
●
●
−2.664387e−20
1.198975e−19
2.664388e−19
4.129801e−19
5.595215e−19
value
(b) Dimension 60
Figure 3.3: Estimates of Orthant probabilities with (white) and with-
out (grey) variable ordering
Covariance matrices generated from random samples with heavy tails (see Section 3.6). In both
case we use a GHK simulator with variable ordering (white), without (grey). The various dimension
are simulated with the same algorithm and same seed, such that the small ones are subsets of the
others. When the variables are not ordered we observe some outliers, and this phenomenon is
reduced with Gibson et al. [1994]’s algorithm.
From dimension 50 and upwards we start observing some skewed distributions
for the GHK estimator as noted in remark 3.3.1. The phenomenon seems to be
reduced by ordering.
This eﬀect is relatively dangerous as some draws depart a lot from the mean
value.
The ordering will be used on all examples from now on to reduce the
variance. In Section 3.6 we have empirical evidence that using an appropriate
move step deals with this tail eﬀect, in our examples.
We have shown that in the particular case of Markov processes we can strikingly
beneﬁt from the use of resampling. In the next section we attempt to generalize
our ﬁnding to a broader range of problems.
3.4.2 A sequential Monte Carlo (SMC) algorithm
The algorithm discussed in Section 3.3 can be generalized to non Markovian Gaus-
sian vectors by applying the SMC methodology. Deﬁne the following sequence of
distribution:
πt(η1:t) = γn(η1:t)
Zt
,
γt(η1:t) =
tY
i=1
ϕ(ηi)1{Bi(η<i)},
(3.7)
77

3 Computation of Gaussian orthant probabilities in high dimension
indexed by t, where the unnormalized quantity γt(η1:t) is our target integrand. We
thus want to compute an estimator of Zt for a given t,
Zt =
ˆ
tY
i=1
ϕ(ηi)1{Bi(η<i)}dη1:t.
SMC samplers are a class of algorithms that generalize particle ﬁlters to non
dynamic problems (Neal [2001a],Chopin [2002a], Del Moral et al. [2006b]). Their
aim is to sample from a sequence of measures (πt)t where π0 is easy to sample from
and πT is our target. The algorithm works by moving from one target to the other
by importance sampling, and avoid degeneracy of the weights by resampling if the
ESS falls bellow a threshold. In the case of the GHK the sequence of distribution
consist of adding a dimension at each step.
To ensure particle diversity after
resampling the particles are moved according to a MCMC kernel targeting the
current distribution. This is the most computationally expensive step. Diﬀerent
alternatives are described in the next section.
The main steps are described in Algorithm (12) bellow:
Algorithm 12 SMC for orthant probabilities
Input Γ,a, M, α⋆, Set Z ←1
Each computation involving m is done ∀m ∈1 : M
Init ηm
1 ∼ϕ(.)1Bk(η<1), and wm
1 = 1
for t ∈1 : T −1 do
At
time
t
the
weighted
system
is
distributed
as
(wm
t , ηm
1:t)
∼
Qt
k=1 ϕ(ηk)1Bk(η<k) ∝πt(η1:t).
if ESS(w1:M
t
) < α⋆then
Z ←Z × { 1
M
PM
i=1 wi
t}.
Resample: η′m
t
∼PM
j=1 wj
tδηj
t , wm
t ←1.
Move: ηm
t ∼Kt(η′m
t , dηm
t ) where Kt leaves πt(η1:t) invariant.
end if
ηm
t+1 ∼ϕ(.|Bt+1(ηm
<t+1)), wm
t+1 ←wm
t × Φ(Bt+1(η<t+1)).
end for
return Z × { 1
M
PM
i=1 wi
T}
An interesting feature of Algorithm 12 is that if the integral is simple enough the
ESS will never fall under the threshold and the above algorithm breaks down to a
GHK simulator. This allows the algorithm to adapt to simple cases at a minimal
eﬀort, that of computing the ESS.
Note also that the estimator is still unbiased (Del Moral [1996a]) and can there-
fore be used in more complex schemes such as PMCMC (Andrieu et al. [2010a])
or SMC2 (Chopin et al. [2013b]).
78

3.4 Non Markovian case
3.4.3 Move steps
The moves step will have an important impact on the non-degeneracy of the par-
ticle system. We want to construct a Markov chain that moves the particles as
far away from their initial position as possible. In addition this step will be the
bulk of the added time complexity compared to GHK, so we want to make it as
eﬃcient as possible.
Gibbs sampler
The structure of our target (3.7), where the dependence of the Gaussian compo-
nents lies within the truncation, does not allows a direct application of the Gibbs
sampler of Robert [1995], without a change of variable. In this section, to simplify
notations we consider the special case of b = ∞. We write the conditional distri-
bution at time t as proportional to Qt
i=1 ϕ(ηi)✶(Γ<t,<tη<t)>b<t, where Γ<t,<t is the
matrix built with the ﬁrst t −1 lines and columns of Γ. The conditional is given
by:
ηi|η−i ∼ϕ
 
.

t\
j≥i
(
sign(γji)ηi ≥
1
|γji|
 
aj −
X
k̸=i
ηkγjk
!)!
.
We therefore have to compute those sets for each component up to t and simulate
according to a truncated Gaussian. Computing the set can lead to one or two
sided truncations depending on the sign of the γij.
The main drawback about having to compute this step each time we resample
is its complexity. This operation has time complexity O(d3) per time step. This
is easily seen as the set in the above equation is just the result of some matrix
inversion for a lower triangular system of dimension d. This leads to an SMC
algorithm that seem to have a prohibitive complexity of O(d4), where the GHK
simulator had an O(d2) complexity. However we have shown that GHK’s variance
diverges exponentially quickly on some examples suggesting that this complexity
might be acceptable. In fact examples in high dimension show that even at con-
stant computational cost the algorithm is able to out-perform GHK (see Section
3.6).
Hamiltonian Monte Carlo
An alternative to Gibbs sampler is to use Hamiltoninan Monte Carlo (HMC) (see
[Neal, 2010a] for a survey), and the idea of Pakman and Paninski [2012] for trun-
cated Gaussians.
HMC is based on interpreting the variables of interest as the position of a particle
with potential the opposite of the log target and by simulating the momentum as a
Gaussian with given mass matrix. The proposal of the Metropolis-Hastings is then
79

3 Computation of Gaussian orthant probabilities in high dimension
constructed by applying the equations of motion up to a time horizon THMC to the
problem. This leads to an eﬃcient algorithm that makes use of the gradient of the
target to explore its support. We refer the reader to Neal [2010a] for more details
on the algorithm and describe the approach proposed by Pakman and Paninski
[2012] to adapt the algorithm to truncated Gaussians.
Based on the fact that the log density of a Gaussian random variable is a
quadratic form, the movement equation can be dealt with explicitly. The scheme
is written as an exact HMC (i.e. not resulting in numerical integration). Remains
then to deal with the truncation. Pakman and Paninski [2012] show that they can
be treated as “walls” for the given particle, a reﬂection principle can be applied
for any particle hitting the constraint during the algorithm. In particular we must
ﬁnd the time at which occurs the ﬁrst “hit”. In our experiment the time horizon
THMC is set to a uniform draw on [0, π] as suggested in Neal [2010a]. The average
value π/2 is advocated by Pakman and Paninski [2012].
The computation of the ﬁrst hitting time dominates the cost of the algorithm.
This is particularly true when the truncation are small as the number of hitting
times will be high. Figure 3.D.1 in appendix 3.D shows a comparison of the SMC
algorithm with the Gibbs sampler (grey) and exact HMC (white). Although this
Markov chain algorithm seems to perform very well for a wide range of problems
and has a neat formalism, we ﬁnd that it does not outperform Gibbs sampling
when used as a move. The speciﬁcity of the move step in SMC is that the particles
are already distributed according to (3.7), therefore the move need not propagate
each particle across all the support. In particular the strength of HMC in quickly
exploring the target might be less useful in this context.
Overrelaxation
Overrelaxation for Gaussian random variables was proposed by Adler [1981] as a
way of improving Gibbs sampling for a distribution with Gaussian conditionals.
For each component the proposal is η′
i|η−i ∼N(µi + α(ηi −µi), σ2
i (1 −α2)) for
0 ≤α ≤1, and with µi and σ2
i the expectation and variance of ηi|η−i. The case
α = 0 is the classical Gibbs sampler, the case α = 1 is a special case of random
walk Metropolis-Hasting proposal. One can check that if ηi ∼N(µi, σ2
i ) then η′
i
has the correct distribution.
Given a particle η, we propose a new one according to:
η′|η ∼N(αη, (1 −α2)I)
Setting aside the constraint for a moment the invariant distribution of such kernel
is an independent (0,1)-Gaussian. If we add an acceptation step such that we
accept if it satisﬁes the constraint at time t, the Markov kernel leaves the current
distribution invariant (3.7).
80

3.4 Non Markovian case
We ﬁnd that the fact that overrelaxation is close to a Metropolis adjusted
Langevin algorithm (MALA) helps to calibrate the algorithm, log π(η) = −1
2ηTη,
hence the proposal in MALA is N((1 −ε
2)η, ε2).
From Roberts and Rosenthal
[1998] we have that ε should be O(d−1
3). To calibrate the algorithm we propose to
match the two drifts. We ﬁnd that α = O(1 −0.5d−1
3), the constant should then
be close from a problem to an other because locally we are always in the case of
independent Gaussians (locally the constraints have less impact). We ﬁnd that in
our case taking α = 0.004×(1−d−1/3) gives the expected behavior and acceptance
ratio.
Repeating the move step
Dubarry and Douc [2011] have shown, for particle ﬁlters, that applying some
Metropolis Hastings kernel targeting the ﬁltering distribution on the particles leads
to a close to optimal variance (the variance is the same as one coming from an iid
sample). This convergence results happens after O(log M) iteration of the Markov
kernel. These results suggest repeating the move step after each resampling step
until some criterion of convergence is satisﬁed.
We compute the sum of absolute distances that the particles have moved after
each step (a similar metric was used in Sch¨afer and Chopin [2011] for the discrete
case). We repeat the move until this scalar value stabilizes. The stabilization
of the total metric should be associated with the cancellation of the dependence
between the particles (leading to a close to independent system).
Block sampling
To diversify the particle system after each resampling we have relied until now
on invariant kernels targeting the current distribution πt. An alternative to this
approach is given by Doucet et al. [2006], where importance sampling is done on
the space of ηt−L+1:t+1 with a given number L of previous time steps. This limits
the behavior of the particles all stemming from one path after a few iterations.
We brieﬂy describe the idea in the following.
Suppose at time t−1 we have a weighted set of particles such that (wt−1, η1:t−1) ∼
πt−1(η1:t−1); instead of proposing a particle η′
t, propose a block of size L, η′
t−L+1:t ∼
q(.|η1:t−1), and discard the particles ηt−L+1:t−1. The distribution of the resulting
system is intractable because of the marginalization. However Doucet et al. [2006]
note that importance sampling is still possible on the extended set of particles
(η1:t−1, η′
t−L+1:t) by introducing some auxiliary distribution λt(ηt−L+1:t−1|η′
1:t). This
leads to the correct marginal whatever λt and the algorithm has the following
81

3 Computation of Gaussian orthant probabilities in high dimension
incremental weights:
πt(η1:t−L, η′
t−L+1:t)λt(ηt−L+1:t−1|η′
t−L+1:t, η1:t−1)
πt−1(η1:t−1)q(η′
t−L+1:t|η1:t−1)
.
The authors show that the optimal proposal and resulting weights are given by:
qopt(η′
t−L+1:t|η1:t−1) = πt(η′
t−L+1:t|η1:t−L),
wt = wt−1
πt(η′
1:t)
πt−L(η1:t−L).
In our case the optimal proposal can then be shown to be:
qopt
t (η′
t−L+1:t|η1:t−L) =
Qt
i=1 ✶Bi(η<i)
Qt
i=t−L+1 ϕ(ηi)
´ Qt
i=1 ✶Bi(η<i)
Qt
i=t−L+1 ϕ(ηi)dηt−L+1:t
.
Notice that this is the density of a truncated Gaussian distribution, yielding a
weight depending on an orthant probability (denominator). In most cases this
is not available and in our particular case it is the quantity of interest. We can
however compute explicitly this integral for L = 1 and L = 2. The former is the
usual case (block of size one). The case L = 2 did not bring any improvement
in terms of variance in all our simulation. We concentrated on the extension to
blocks of higher dimension.
In this case we have to resort to approximations of the proposal. The ﬁrst idea
would be to approximate it by a Gaussian using expectation propagation [Minka,
2001a].
However this approach did not perform better than the use of Gibbs
sampler mentioned earlier. Another approach to approximate the distribution is
to consider the Gibbs sampler on a block of size L with the GHK proposal.
Partial conclusion
We have shown that the proposed Gibbs sampler outperforms HMC. Concerning
block sampling the diﬀerent approaches were tested on several dimensions only to
ﬁnd that the best performing approach was to use partial Gibbs sampling, i.e. a
Gibbs sampler on a block. In the numerical tests we provide in Section 3.6 we
show only the latter.
In our simulations we propose to repeat each kernels as was explained in Section
3.4.3. We propose to test the Gibbs sampler and the overrelaxed random walk.
In addition we have studied other kernels based on the geometry of the prob-
lem; in particular, one can draw random walks on the line between the current
particle and the basic solution of our constraint. Those approach did not however
outperform the proposals discussed above.
82

3.5 Extentions
3.5 Extentions
3.5.1 Student Orthant
We can easily extend our approach to the computation of orthant probabilities for
other distributions, in particular for mixtures of Gaussians, that is probabilities
that can be written as:
ˆ
fU(u)
ˆ
fH|U(η|u)1{b>η>a}dηdu,
(3.8)
where fH|U is a Gaussian.
Several distributions can be created as such.
For
instance, the Student distribution where the variance is marginally distributed as
an inverse-χ2. Hence the distribution:
fH|U(η|u)1{b>η>a} =
n
Y
i=1
ϕ(ηi)1{Bu
i (η<i)},
where Bu(η<i) is Bi(η<i) where we multiply a by u
ν and fU(u) = χ2
ν(u). They
are an interesting application to those algorithms because they come at a minimal
additional cost and are of use in multiple comparison [Bretz et al., 2001].
Another example is the logistic distribution where fU(u) is some transformation
of a Kolmogorov-Smirnov distribution (see Holmes and Held [2006]). This could
be used to perform Bayesian inference on multinomial logistic regression.
To deal with this integral we can extend the space on which the SMC is car-
ried out at time t.
Hence the move step is performed on the extended space
fU(u)fH|U(η|u). In our Student example it amounts to taking as a target distri-
bution
πn(η1:n, u) ∝
n
Y
i=1
ϕ(ηi)1{Bu
i (η<i)}χ2
ν(u).
The normalizing constant that the SMC algorithm approximates is
Zn =
ˆ
n
Y
i=1
ϕ(ηi)1{Bu
i (η<i)}χ2
ν(u)dη1:ndu.
At each move step we therefore move the particles using a Metropolis-Hastings
algorithm targeting p(u|η1:n) and perform the remaining Gibbs sampler updates
conditionally on U. This additional step allows for further mixing. Beneﬁts from
this step are already found in relatively low dimension as shown in Section 3.6.
83

3 Computation of Gaussian orthant probabilities in high dimension
3.5.2 SMC as a truncated distribution sampler
A natural extension is to use Alg. 12 to compute other integrals with respect to
truncated Gaussians. At time t the output of the algorithm is a weighted sample
(wi
t, ηi
1:t)i∈[1,M] approximating πt(η1:t) ∝Qt
i=1 ϕ(ηi)✶B(η<i). Hence any integral of
the form Eπt (h(η)), where expectation is taken with respect to πt, can be ap-
proximated by PM
i=1
wi
t
PM
j=1 wj
t h(ηi
1:t). The same argument goes for the truncated
Student.
We test the idea for computing the expectation of truncated multivariate Stu-
dent. We use a Gibbs sampler as a benchmark based on Robert [1995]’s sampler
by adding a MH step to deal with u (see previous section). The Gibbs update is
done after a change of variable that leaves the truncations independent. This can
be shown to be more eﬃcient. We allocate 100 times more computational time to
the Gibbs sampler than the SMC.
In Figure 3.4 we see that after thinning one out of 1000 points the ACF and
trace plots point to bad exploration of the target’s support. This behavior shows
that the convergence is too slow for the algorithm to be of practical use. On the
other hand the SMC is still stable as is shown in the next section.
In addition of outperforming the Gibbs sampler for fairly moderate dimension,
the SMC algorithm was found to be stable for approximating the expectation in
dimensions up to 100.
84

3.6 Numerical results
0
1
2
3
4
0
100
200
300
(a) Trace plot
0.00
0.25
0.50
0.75
1.00
0
25
50
75
100
lag
acf
(b) ACF
−2
−1
0
1
2
0
100
200
300
(c) Trace plot
0.00
0.25
0.50
0.75
1.00
0
25
50
75
100
lag
acf
(d) ACF
Figure 3.4: Truncated Student sampling after thinning one out of
every 1000, using a Gibbs sampler
The data are generated as explained in the “Numerical results section”, for dimension 50. The left
panels are trace plots for two components. The right panels are the ACFs. Both are shown after
a thinning of 1/1000. Both show a slow convergence, whereas we observe that SMC is stable.
3.6 Numerical results
3.6.1 Covariance simulation, tunning parameters
To build the covariance matrices we propose to use draws from a Cauchy distri-
bution. We start by sampling a matrix X and a vector a from an independent
Cauchy distribution Xij ∼C(0, 0.01) and ai ∼C(0, 0.01), then construct the co-
variance matrix as Σ = XtX and the truncation as a = (ai). Because of the heavy
tails the resulting correlation matrix (ﬁgure 3.5a) has many close to zero entries
and some high correlations. The truncations also have some very high levels (ﬁgure
3.5b).
85

3 Computation of Gaussian orthant probabilities in high dimension
We ﬁnd that this approach leads to more challenging covariances than those
built by sampling the spectrum of the covariance matrix as proposed for instance
in Christen et al. [2012].
0
10
20
30
40
50
0
10
20
30
40
50
(a) Correlations
0
10
20
30
40
50
0
50
100
(b) Truncation
Figure 3.5: Generated correlations
Covariance matrices generated from random samples with heavy tails. The various dimension are
simulated with the same algorithm and same seed, such that the small ones are subsets of the
others. The left panel shows a heatmap of the correlation, the right panel the left trucation of the
integral.
The tuning parameters of the algorithm are the threshold that tunes the number
of steps of the MCMC kernel, and the targeted ESS under which we resample, α⋆.
The former is set to some small value (0.01) and has not much inﬂuence. The latter
gives us a trade oﬀbetween variance and computational cost. In our example we
have found that 0.5M allows good approximation, however this value should be
increased with the diﬃculty of the problem.
3.6.2 GHK for moderate dimensions
All our results are shown at constant computational cost: we repeat the algorithm
in a ﬁrst time to get their execution time, and we then scale them accordingly. In
the above example (dimension 40) for instance the number of draws associated with
the GHK algorithm is 1, 065, 399. The number of particles of the SMC sampler
with Gibbs Markov transition is 5217.
86

3.6 Numerical results
●
●●
●
1e−10
2e−10
3e−10
BS
GHK
Gibbs
RW
value
(a) Dimension 20
●
●
●●
●
●
●
●
−2.560843e−16
2.184932e−15
4.625948e−15
7.066964e−15
9.507981e−15
BS
GHK
Gibbs
RW
value
(b) Dimension 30
●
●
●
●
2.063132e−23
5.551489e−23
9.039845e−23
1.252820e−22
1.601656e−22
BS
GHK
Gibbs
RW
value
(c) Dimension 40
●
●
●●●
●
●
●
●
●
2.380611e−23
1.473823e−22
2.709584e−22
3.945346e−22
5.181107e−22
BS
GHK
Gibbs
RW
value
(d) Dimension 50
Figure 3.6: Estimates of Orthant probabilities for GHK compared to
SMC with diﬀerent moves.
The various dimensions are simulated as described in Section 3.6. Diﬀerent moves are tested inside
SMC. As we have already discussed GHK leads to a higher variance and outliers. Block sampling
(BS) and Gibbs sampling (Gibbs) seem to outperform the overrelaxed random walk (RW). However
all three stay stable until dimension 50.
We ﬁnd that in moderate dimension (∼50) the GHK simulator breaks down in
attempting to compute the probability of the orthant generated by our simulation
scheme. This is all the more problematic as it gives an answer, and there is no
way of checking its departure from the true value.
Another interesting aspect is the fact that the block sampling algorithm performs
well in those dimensions. It is quicker than to move the particles in every dimension
as is done with MCMC. The truncations that lead to a drop of probability are
87

3 Computation of Gaussian orthant probabilities in high dimension
close together because of the ordering, hence once the diﬃcult dimensions have
been “absorbed” it is less and less paramount to visit the past truncations.
3.6.3 High dimension orthant probabilities
In dimensions higher than p = 70, the covariance we simulate lead to integrals
that cannot be treated with the GHK algorithm. In our simulations GHK always
returned NaN values due to the low values of the weights. For the SMC an indicator
of the good behavior of the algorithm can be seen in either its reproducibility and
the fact that we do not encounter asymmetry (see Remark 3.3.1) as for the GHK
in the ﬁrst two Sections. Furthermore the ESS does not fall very low along the
particles’ draw (Figure 3.7c).
●
5.133160e−50
1.979805e−49
3.446294e−49
4.912784e−49
6.379273e−49
BS
Gibbs
OR
value
(a) Dimension 130
●
−1.064822e−73
3.710540e−72
7.527562e−72
1.134458e−71
1.516161e−71
BS
Gibbs
OR
value
(b) Dimension 180
0
1000
2000
3000
0
50
100
(c) Dimension 130
Figure 3.7: Estimates of Orthant probabilities p = 130 and p = 180
The various dimensions are simulated as described in Section 3.6. Diﬀerent moves are tested inside
SMC. As we have already discussed GHK leads to a higher variance and outliers. Gibbs sampling
(Gibbs) seem to outperform the overrelaxed random walk (OR) and Block sampling (BS). However
all three stay stable until dimension 180. The ESS for the Gibbs sampler is shown in panel c for a
threshold of 0.5M and M = 3000. Despite some sudden drops it seems to be stable.
For those dimensions the Gibbs sampler performs best in terms of variance.
However if one’s goal is a fast algorithm, at the cost of higher variance the overre-
laxation might be preferable at some point as the dimension of the target increases.
The latter as a complexity smaller of one degree such that at constant computa-
tional cost it will have more and more particles allocated to it.
88

3.6 Numerical results
3.6.4 Student orthant probabilities
We use the same schemes as before to construct the covariance matrix and ﬁx a
degree of freedom of 3 in our experiments. As before we show an improvement as
compared to previous algorithms. This improvements appears also for moderate
dimensions. It seems that there is an important gain in considering the extended
target.
As for the Gaussian case we ﬁnd that the output of GHK is heavily skewed. It
seems that it is not the case for our algorithm.
●
●
●
0.0e+00
5.0e−14
1.0e−13
1.5e−13
2.0e−13
GHK
SMC
value
(a) Dimension 30
●
●
●
●
●
−4.542168e−19
2.191693e−18
4.837602e−18
7.483511e−18
1.012942e−17
GHK
SMC
value
(b) Dimension 50
Figure 3.8: Estimates of Student orthant probabilities SMC vs GHK
Covariance matrices generated from random samples with heavy tails. We ﬁnd that the SMC
outperforms the GHK. As for the previous cases the GHK leads to some outliers.
3.6.5 Application to random utility models
Random utility models are an important area of research in Economics to model
choice data [Train, 2009]. Consider an agent i confronted to J alternatives each
giving utility Y ⋆
ij ∀j ∈{1, · · · , J} modeled by Y ⋆
ij = Xiβ + uij with uij a Gaussian
noise. Individual i chooses alternative j if {∀k ̸= j
Y ⋆
ij > Y ⋆
ik}. The likelihood is
the probability of this set integrated over the unobserved alternatives. Hence the
likelihood is given by:
L(Yi = j|Ω, β, X) = P
 \
k̸=j
{Y ⋆
ij > Y ⋆
ik}}
!
89

3 Computation of Gaussian orthant probabilities in high dimension
= P
 \
k̸=j
{(Xij −Xik)β⋆> uik −uij}}
!
where integration is taken over u ∼N(0, Ω), where u = (uij). The above integral
is an orthant probability of dimension J −1. A yet more challenging case occurs
in the presence of panel data. The latter corresponds to sequential choices of an
individual in time. We denote those choices by the subscript t. We observe (jt)t<T
for every individual. Integration is now in dimension T(J −1) and takes the form:
L(Yi,1:T = j1:T|Ω, β, X) = P
 T\
t=1
\
k̸=jt
{(Xijtt −Xikt)β⋆> uikt −uijtt}}
!
We take the covariance structure studied in Bursh-Supan et al. [1992].
The
noise term is uitk = αik + ηikt where ηikt = ̺iηikt−1 + νit, where (αik) are correlated
amongst choices, so are νit. The terms are all Gaussian.
The dataset is simulated to allow for examples that are more complex, and of
variable size. In the model presented above individuals are independent so that
we present results in computing the integral for n = 1, and have already a big
advantage of using our methodology.
●
●
1.1e−11
1.3e−11
1.5e−11
1.7e−11
GHK
SMC
value
(a) J = 10, T = 10
●
●
●
5.00e−14
7.50e−14
1.00e−13
1.25e−13
1.50e−13
GHK
SMC
value
(b) J = 10, T = 15
Figure 3.9: Estimates of the likelihood of a multivariate Probit
Dataset: The Data is simulated using the covariance proposed in Bursh-Supan et al. [1992], the
value of the parameter for which the likelihood is evaluated is taken at random.
Figure (3.9) shows, for two problems of diﬀerent size, the gain in precision at
constant computational cost. The improvement is substantial and increases with
90

the size of the problem. Taking n > 1 would only increase this eﬀect as it would
consist in taking products of such estimators.
The latter result suggests that we could use the likelihood for inference using
either maximum likelihood or PMCMC. Computing the likelihood with lowest
possible variance is also a key issue in ﬁnding the evidence in the most precise
manner possible.
When comparing the two algorithms we set the number of particles of SMC to
M = 1000, for the same computational cost we allocate M = 881031 to GHK.
SMC has however still a lower variance. Regardless of computational time, the
ability to compute precise integrals for a small number of particles can also be of
importance. It is the case for instance in SMC2 [Chopin et al., 2013b] where whole
trajectories have to be kept in memory for several samplers.
One could extend these models to multivariate Probit with Student distribution
and to multivariate logit models for more robustness. In this case we can use the
algorithm based on the mixture representation built in Section 3.5.1.
3.7 Conclusion
We have shown empirically that the GHK algorithm collapses when the dimension
of the problem increases (returning NaN values). In other cases, the distribution
of estimates generated by GHK may have heavy tails (see also Remark 3.3.1).
Theoretically for at least one covariance structure we have shown that the variance
of the algorithm diverges exponentially fast with the dimension (Section 3.3). Our
SMC algorithm seems to correct this behavior, and was found to be of practical
use for many problems.
We have tested several kernels as part of the move step (Section 3.4.2). We
advise the practitioners to use Gibbs sampling as the standard “go to” move step.
However improvements in speed can be achieved for dimensions around 50 using
only a partial update.
In addition as the dimension increases one might want
to use a method with lower complexity at the cost of having to repeat the move
a bit more. In this case we recommend the use of an overrelaxed random walk
Metropolis-Hastings.
We have shown that the same idea can be use for computing probabilities of
mixtures of Gaussians. In addition we can use the weighted particles returned by
the algorithm to compute other integrals (mean, variance, .etc). This approach
91

3 Computation of Gaussian orthant probabilities in high dimension
can outperform a classical Gibbs sampler when the dimension exceeds 20.
Appendix
3.A Proof of proposition 2.1
Proof:
We have that for 0 < b < ∞the transition density pb(x, y)dy, associated
with the kernel P b(x, dy) with respect to the Lebesgue measure, is lower bounded
by a constant and the transition is continuous. This Markov chain is a ψ-irreducible
on a compact support. Hence we can show that the whole support [0, b] is small
[Meyn and Tweedie, 2009].
Hence by theorem 16.1.2 to show V-Uniform ergodicity the transition must
satisfy the drift condition:
ˆ
p(x, y)Ve(y)dy ≤(1 −β)Ve(x) + c✶[0,b](x)
for β > 0, c < ∞and a certain Ve(x) with value in [1, ∞). We take Ve(x) = ex2+1,
e > 0. In the following we check this condition.
The left hand side is given by E(X2
t |Xt−1 = x) for the above transition proba-
bility,
E(X2
t |Xt−1 = x) = ̺2x2 + 1 + ̺ϕ(̺x) −bϕ(b −̺x)
Φ(b −̺x) −Φ(−̺x)̺x
The ratio is continuous on the bounded set [0, b], and can be bounded by a constant,
such that the by taking β = 1 −̺2 > 0 the drift condition is satisﬁed for a c(e)
depending on e.
In addition we can compute exactly the invariant measure. It is unique and
given by the solution of:
π(y) =
ˆ
ϕ(y; ̺x, 1)
Φ(a −̺x) −Φ(−̺x)✶[0,b](y)π(x)dx
92

3.A Proof of proposition 2.1
The solution of the above equation is a truncated skew-Normal distribution,
π(dy) ∝{Φ(b −̺y) −Φ(−̺y)} ϕ(y; 0, 1 −̺2)✶[0,b](y)dy.
The moments of this distribution have been studied in Flecher et al. [2009], in
particular note that 0 < Vπ(X) < ∞.
Deﬁne ψ : x 7→log [Φ(b −̺x) −Φ(−̺x)], by theorem 17.0.1 [Meyn and Tweedie,
2009] to obtain a CLT for
1
√
T
PT
t=1 ψ(Xt) we must ensure that there exist a con-
stant e > 0 such that ψ2(x) < Ve(x) on [0, b]. Such a constant can be found by
noting that ψ(x) is bounded as long as b > 0 and that Ve is strictly increasing of
e > 0 with value on (0, ∞). The value of e depends on b. We obtain the following
convergence result,
1
√
T
 T
X
t=1
ψ(Xt) −TEπ(ψ(X))
!
⇝N (0, Vπ {ψ(X)} + τ) ,
where the variance term is deﬁned because ψ is bounded on [0, b], and τ =
2 P∞
k=1 cov(X0, Xk). By taking the exponential and using the continuous map-
ping theorem (p.7 Van der Vaart [1998]) we get a log-normal limiting distribution
 
QT
t=1 (Φ(b −̺Xt) −Φ(−̺Xt))
exp{TEπ log (Φ(b −̺X) −Φ(−̺X))}
!
1
√
T
⇝EN (0, Vπ {ψ(X)} + τ) .
By Portmanteau’s Lemma (p.6 Van der Vaart [1998]) for x2 as a continuous and
positive function,
lim inf
T→∞E


 
QT
t=1 (Φ(b −̺Xt) −Φ(−̺Xt))2
exp{2TEπ log (Φ(b −̺X) −Φ(−̺X))}
!
1
√
T

> exp {Vπ [ψ(X)] + τ}
lim inf
T→∞E
" 
QT
t=1 (Φ(b −̺Xt) −Φ(−̺Xt))2
exp{2TEπ log (Φ(b −̺X) −Φ(−̺X))}
!#
1
√
T
> exp {Vπ [ψ(X)] + τ}
where the last line is obtained by Jensen inequality. The denominator is the square
of limit value of the normalizing constant under x2-Uniform ergodicity that follows
from the above statement.
□
93

3 Computation of Gaussian orthant probabilities in high dimension
3.B Resampling
Algorithm 13 Systematic resampling, n particles
Input: Vector of weights w and vector x to sample from
Set v ←nw, j ←1, c = v1
Sample: Sample U ∼U[0,1]
for k = 1, · · · , n do
while c < u do
Set j ←j + 1, c ←c + vj
end while
Set: ˆxk ←xj, u ←u + 1
end for
return ˆx
3.C Variable Ordering
Algorithm 14 Variable Ordering
INIT: i1 = arg min1≤k≤T Φ
h
ak
γkk , bk
γkk
i
η1 =
1
Φ

ai1
γi1i1
,
bi1
γi1i1
 ´
ai1
γi1i1
,
bi1
γi1i1
 ηϕ(η)dη
for i ∈{2, · · · , d} do
STEP 1 ij = arg minj≤k≤T Φ
h
1
˜γkk
n
ak −Pj−1
l=1 ˜γilkηk
o
,
1
˜γkk
n
bk −Pj−1
l=1 ˜γilkηk
oi
STEP 2
ηj =
1
Φ
h
1
˜γkk
n
ak −Pj−1
l=1 ˜γilkηk
o
,
1
˜γkk
n
bk −Pj−1
l=1 ˜γilkηk
oi×
ˆ
h
1
˜γkk {ak−Pj−1
l=1 ˜γilkηk},
1
˜γkk {bk−Pj−1
l=1 ˜γilkηk}
i ηϕ(η)dη
end for
return (i1, · · · , id)
Where ˜γ is updated accordingly when the order is changed.
94

3.D Hamiltonian Monte Carlo
3.D Hamiltonian Monte Carlo
●
●
−1.739871e−18
7.901708e−18
1.754329e−17
2.718487e−17
3.682644e−17
Gibbs
Hamiltonian
value
(a) Dimension 30
●
1.323594e−18
6.966259e−18
1.260892e−17
1.825159e−17
2.389425e−17
Gibbs
Hamiltonian
value
(b) Dimension 40
●
1.243277e−23
4.412336e−23
7.581396e−23
1.075045e−22
1.391951e−22
Gibbs
Hamiltonian
value
(c) Dimension 50
●
●
●
−1.250314e−30
5.859634e−30
1.296958e−29
2.007953e−29
2.718948e−29
Gibbs
Hamiltonian
value
(d) Dimension 60
Figure 3.D.1: Estimates of Orthant probabilities Gibbs vs HMC
Covariance matrices generated from random samples with heavy tails. The various dimension are
simulated with the same algorithm and same seed, such that the small ones are subsets of the
others. The grey boxplot corresponds to the Gibbs sampler the white to the HMC. The Gibbs
sampler seem to have smaller variance and no outliers.
95


4
Theoretical and computational aspects of PAC
Bayesian ranking and scoring
This is joint work with Pierre Alquier, Nicolas Chopin and Feng
Liang
Status: Published in NIPS proceedings.
4.1 Introduction
Bipartite ranking (scoring) amounts to rank (score) data from binary labels. An
important problem in its own right, bipartite ranking is also an elegant way to
formalise classiﬁcation: once a score function has been estimated from the data,
classiﬁcation reduces to chooses a particular threshold, which determine to which
class is assigned each data-point, according to whether its score is above or below
that threshold. It is convenient to choose that threshold only once the score has
been estimated, so as to get ﬁner control of the false negative and false positive
rates; this is easily achieved by plotting the ROC (Receiver operating characteris-
tic) curve.
A standard optimality criterion for scoring is AUC (Area Under Curve), which
measures the area under the ROC curve. AUC is appealing for at least two rea-
sons.
First, maximising AUC is equivalent to minimising the L1 distance be-
tween the estimated score and the optimal score. Second, under mild conditions,
Cortes and Mohri [2003] show that AUC for a score s equals the probability that
s(X−) < s(X+) for X−(resp.
X+) a random draw from the negative (resp.
positive class). Yan et al. [2003] observed AUC-based classiﬁcation handles much
better skewed classes (say the positive class is much larger than the other) than
97

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
standard classiﬁers, because it enforces a small score for all members of the nega-
tive class (again assuming the negative class is the smaller one).
One practical issue with AUC maximisation is that the empirical version of AUC
is not a continuous function. One way to address this problem is to ”convexify”
this function, and study the properties of so-obtained estimators [Cl´emen¸con et al.,
2008a].
We follow instead the PAC-Bayesian approach in this chapter, which
consists of using a random estimator sampled from a pseudo-posterior distribution
that penalises exponentially the (in our case) AUC risk. It is well known [see e.g.
the monograph of Catoni, 2007] that the PAC-Bayesian approach comes with a
set of powerful technical tools to establish non-asymptotic bounds; the ﬁrst part
of the chapter derive such bounds. A second advantage however of this approach,
as we show in the second part of the chapter, is that it is amenable to powerful
Bayesian computational tools, such as Sequential Monte Carlo and Expectation
Propagation.
4.2 Theoretical bounds from the PAC-Bayesian
Approach
4.2.1 Notations
The data D consist in the realisation of n IID (independent and identically dis-
tributed) pairs (Xi, Yi) with distribution P, and taking values in Rd×{−1, 1}. Let
n+ = Pn
i=1 ✶{Yi = +1}, n−= n −n+. For a score function s : Rd →R, the AUC
risk and its empirical counter-part may be deﬁned as:
R(s) = P(X,Y ),(X′,Y ′)∼P [{s(X) −s(X′)}(Y −Y ′) < 0] ,
Rn(s) =
1
n(n −1)
X
i̸=j
✶[{s(Xi) −s(Xj)}(Yi −Yj) < 0] .
Let σ(x) = E(Y |X = x), ¯R = R(σ) and ¯Rn = Rn(σ). It is well known that σ is
the score that minimise R(s), i.e. R(s) ≥¯R = R(σ) for any score s.
The results of this section apply to the class of linear scores, sθ(x) = ⟨θ, x⟩, where
⟨θ, x⟩= θTx denotes the inner product. Abusing notations, let R(θ) = R(sθ),
Rn(θ) = Rn(sθ), and, for a given prior density πξ(θ) that may depend on some
hyperparameter ξ ∈Ξ, deﬁne the Gibbs posterior density (or pseudo-posterior) as
πξ,γ(θ|D) := πξ(θ) exp {−γRn(θ)}
Zξ,γ(D)
,
Zξ,γ(D) =
ˆ
Rd πξ(˜θ) exp
n
−γRn(˜θ)
o
d˜θ
for γ > 0. Both the prior and posterior densities are deﬁned with respect to the
Lebesgue measure over Rd.
98

4.2 Theoretical bounds from the PAC-Bayesian Approach
4.2.2 Assumptions and general results
Our general results require the following assumptions.
Deﬁnition 4.2.1 We say that Assumption Dens(c) is satisﬁed for c > 0 if
P(⟨X1 −X2, θ⟩≥0, ⟨X1 −X2, θ′⟩≤0) ≤c∥θ −θ′∥
for any θ and θ′ ∈Rd such that ∥θ∥= ∥θ′∥= 1.
This is a mild Assumption, which holds for instance as soon as (X1−X2)/∥X1−
X2∥admits a bounded probability density; see the appendix.
Deﬁnition 4.2.2 (Mammen & Tsybakov margin assumption) We say that
Assumption MA(κ, C) is satisﬁed for κ ∈[1, +∞] and C ≥1 if
E

(qθ
1,2)2
≤C

R(θ) −R
 1
κ
where qθ
i,j = ✶{⟨θ, Xi −Xj⟩(Yi −Yj) < 0} −✶{[σ(Xi) −σ(Xj)](Yi −Yj) < 0} −
R(θ) + R.
This assumption was introduced for classiﬁcation by Mammen and Tsybakov
[1999], and used for ranking by Cl´emen¸con et al. [2008b] and Robbiano [2013] (see
also a nice discussion in Lecu´e [2007]). The larger κ, the less restrictive MA(κ, C).
In fact, MA(∞, C) is always satisﬁed for C = 4. For a noiseless classiﬁcation task
(i.e. σ(Xi)Yi ≥0 almost surely), R = 0,
E((qθ
1,2)2) = Var(qθ
1,2) = E[✶{⟨θ, X1 −X2⟩(Yi −Yj) < 0}] = R(θ) −R
and MA(1, 1) holds. More generally, MA(1, C) is satisﬁed as soon as the noise
is small; see the discussion in Robiano 2013 (Proposition 5 p. 1256) for a formal
statement. From now, we focus on either MA(1, C) or MA(∞, C), C ≥1. It
is possible to prove convergence under MA(κ, 1) for a general κ ≥1, but at the
price of complications regarding the choice of γ; see Catoni [2007], Alquier [2008]
and Robbiano [2013].
We use the classical PAC-Bayesian methodology initiated by McAllester [1998];
Shawe-Taylor and Williamson [1997] (see Alquier [2008]; Catoni [2007] for a com-
plete survey and more recent advances) to get the following results. Proof of these
and forthcoming results may be found in the appendix. Let K(ρ, π) denotes the
Kullback-Liebler divergence, K(ρ, π) =
´
ρ(dθ) log{ dρ
dπ(θ)} if ρ << π, ∞otherwise,
and denote M1
+ the set of probability distributions ρ(dθ).
99

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Lemma 4.2.1 Assume that MA(1, C) holds with C ≥1. For any ﬁxed γ with
0 < γ ≤(n −1)/(8C), for any ε > 0, with probability at least 1 −ε on the drawing
of the data D,
ˆ
R(θ)πξ,γ(θ|D)dθ −R ≤2 inf
ρ∈M1
+
(ˆ
R(θ)ρ(dθ) −R + 2K(ρ, πξ) + log
  4
ε

γ
)
.
Lemma 4.2.2 Assume MA(∞, C) with C ≥1. For any ﬁxed γ with 0 < γ ≤
(n −1)/8, for any ǫ > 0 with probability 1 −ǫ on the drawing of D,
ˆ
R(θ)πξ,γ(θ|D)dθ −¯R ≤inf
ρ∈M1
+
ˆ
R(θ)ρ(dθ) −¯R + 2K(ρ, πξ) + log 2
ǫ
γ

+ 16γ
n −1.
Both lemmas bound the expected risk excess, for a random estimator of θ gen-
erated from πξ,γ(θ|D).
4.2.3 Independent Gaussian Prior
We now specialise these results to the prior density πξ(θ) = Qd
i=1 ϕ(θi; 0, ϑ), i.e. a
product of independent Gaussian distributions N(0, ϑ); ξ = ϑ in this case.
Theorem 4.2.3 Assume MA(1, C), C ≥1, Dens(c), c > 0, and take ϑ = 2
d(1 +
1
n2d), γ = (n −1)/8C, then there exists a constant α = α(c, C, d) such that for any
ǫ > 0, with probability 1 −ǫ,
ˆ
R(θ)πγ(θ|D)dθ −¯R ≤2 inf
θ0

R(θ0) −¯R
	
+ αd log(n) + log 4
ǫ
n −1
.
Theorem 4.2.4 Assume MA(∞, C), C ≥1, Dens(c) c > 0, and take ϑ =
2
d(1 +
1
n2d), γ = C
p
dn log(n), there exists a constant α = α(c, C, d) such that for
any ǫ > 0, with probability 1 −ǫ,
ˆ
R(θ)πγ(θ|D)dθ −¯R ≤inf
θ0

R(θ0) −¯R
	
+ α
p
d log(n) + log 2
ǫ
√n
.
The proof of these results is provided in the appendix. It is known that, un-
der MA(κ, C), the rate (d/n)
κ
2κ−1 is minimax-optimal for classiﬁcation problems,
see Lecu´e [2007]. Following Robbiano [2013] we conjecture that this rate is also
optimal for ranking problems.
100

4.3 Practical implementation of the PAC-Bayesian approach
4.2.4 Spike and slab prior for feature selection
The independent Gaussian prior considered in the previous section is a natu-
ral choice, but it does not accommodate sparsity, that is, the possibility that
only a small subset of the components of Xi actually determine the membership
to either class.
For sparse scenarios, one may use the spike and slab prior of
Mitchell and Beauchamp [1988], George and McCulloch [1993b],
πξ(θ) =
d
Y
i=1
[pϕ(θi; 0, v1) + (1 −p)ϕ(θi; 0, v0)]
with ξ = (p, v0, v1) ∈[0, 1]×(R+)2, and v0 ≪v1, for which we obtain the following
result. Note ∥θ∥0 is the number of non-zero coordinates for θ ∈Rd.
Theorem 4.2.5 Assume MA(1, C) holds with C ≥1, Dens(c) holds with c > 0,
and take p = 1 −exp(−1/d), v0 ≤1/(2nd log(d)), and γ = (n −1)/(8C). Then
there is a constant α = α(C, v1, c) such that for any ε > 0, with probability at least
1 −ε on the drawing of the data D,
ˆ
R(θ)πγ(dθ|D) −R ≤2 inf
θ0
(
R(θ0) −R + α∥θ0∥0 log(nd) + log
  4
ε

2(n −1)
)
.
Compared to Theorem 4.2.3, the bound above increases logarithmically rather
than linearly in d, and depends explicitly on ∥θ∥0, the sparsity of θ. This suggests
that the spike and slab prior should lead to better performance than the Gaussian
prior in sparse scenarios. The rate ∥θ∥0 log(d)/n is the same as the one obtained
in sparse regression, see e.g. B¨uhlmann and van de Geer [2011].
Finally, note that if v0 →0, we recover the more standard prior which assigns a
point mass at zero for every component. However this leads to a pseudo-posterior
which is a mixture of 2d components that mix Dirac masses and continuous distri-
butions, and thus which is more diﬃcult to approximate (although see the related
remark in Section 4.3.4 for Expectation-Propagation).
4.3 Practical implementation of the PAC-Bayesian
approach
4.3.1 Choice of hyper-parameters
Theorems 4.2.3, 4.2.4, and 4.2.5 propose speciﬁc values for hyper-parameters γ
and ξ, but these values depend on some unknown constant C. Two data-driven
101

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
ways to choose γ and ξ are (i) cross-validation (which we will use for γ), and (ii)
(pseudo-)evidence maximisation (which we will use for ξ).
The latter may be justiﬁed from intermediate results of our proofs in the ap-
pendix, which provide an empirical bound on the expected risk:
ˆ
R(θ)πξ,γ(θ|D)dθ −¯R ≤Ψγ,n inf
ρ∈M1
+
ˆ
Rn(θ)ρ(dθ) −¯Rn + K(ρ, π) + log 2
ǫ
γ

with Ψγ,n ≤2. The right-hand side is minimised at ρ(dθ) = πξ,γ(θ|D)dθ, and the
so-obtained bound is −Ψγ,n log(Zξ,γ(D))/γ plus constants. Minimising the upper
bound with respect to hyperparameter ξ is therefore equivalent to maximising
log Zξ,γ(D) with respect to ξ. This is of course akin to the empirical Bayes ap-
proach that is commonly used in probabilistic machine learning. Regarding γ the
minimization is more cumbersome because the dependence with the log(2/ǫ) term
and Ψn,γ, which is why we recommend cross-validation instead.
It seems noteworthy that, beside Alquier and Biau [2013], very few papers dis-
cuss the practical implementation of PAC-Bayes, beyond some brief mention of
MCMC (Markov chain Monte Carlo). However, estimating the normalising con-
stant of a target density simulated with MCMC is notoriously diﬃcult. In addition,
even if one decides to ﬁx the hyperparameters to some arbitrary value, MCMC
may become slow and diﬃcult to calibrate if the dimension of the sampling space
becomes large. This is particularly true if the target does not (as in our case) have
some speciﬁc structure that makes it possible to implement Gibbs sampling. The
two next sections discuss two eﬃcient approaches that make it possible to approx-
imate both the pseudo-posterior πξ,γ(θ|D) and its normalising constant, and also
to perform cross-validation with little overhead.
4.3.2 Sequential Monte Carlo
Given the particular structure of the pseudo-posterior πξ,γ(θ|D), a natural ap-
proach to simulate from πξ,γ(θ|D) is to use tempering SMC [Sequential Monte Carlo
Del Moral et al., 2006b] that is, deﬁne a certain sequence γ0 = 0 < γ1 < . . . < γT,
start by sampling from the prior πξ(θ), then applies successive importance sampling
steps, from πξ,γt−1(θ|D) to πξ,γt(θ|D), leading to importance weights proportional
to:
πξ,γt(θ|D)
πξ,γt−1(θ|D) ∝exp {−(γt −γt−1)Rn(θ)} .
When the importance weights become too skewed, one rejuvenates the particles
through a resampling step (draw particles randomly with replacement, with prob-
ability proportional to the weights) and a move step (move particles according to
a certain MCMC kernel).
102

4.3 Practical implementation of the PAC-Bayesian approach
One big advantage of SMC is that it is very easy to make it fully adaptive. For
the choice of the successive γt, we follow Jasra et al. [2007] in solving numerically
(5.18) in order to impose that the Eﬀective sample size has a ﬁxed value. This
ensures that the degeneracy of the weights always remain under a certain thresh-
old.
For the MCMC kernel, we use a Gaussian random walk Metropolis step,
calibrated on the covariance matrix of the resampled particles. See Algorithm 19
for a summary.
Algorithm 15 Tempering SMC
Input N (number of particles), τ ∈(0, 1) (ESS threshold), κ > 0 (random walk
tuning parameter)
Init. Sample θi
0 ∼πξ(θ) for i = 1 to N, set t ←1, γ0 = 0, Z0 = 1.
Loop a. Solve in γt the equation
{PN
i=1 wt(θi
t−1)}2
PN
i=1{wt(θi
t−1))2}
= τN,
wt(θ) = exp[−(γt −γt−1)Rn(θ)]
(4.1)
using bisection search. If γt ≥γT, set ZT = Zt−1 ×
n
1
N
PN
i=1 wt(θi
t−1)
o
,
and stop.
b. Resample: for i = 1 to N, draw Ai
t in 1, . . . , N so that P(Ai
t = j) =
wt(θj
t−1)/ PN
k=1 wt(θk
t−1); see Algorithm 1 in the appendix.
c. Sample θi
t ∼Mt(θ
Ai
t
t−1, dθ) for i = 1 to N where Mt is a MCMC kernel that
leaves invariant πt; see Algorithm 3 in the appendix for an instance of
such a MCMC kernel, which takes as an input S = κˆΣ, where ˆΣ is the
covariance matrix of the θ
Ai
t
t−1.
d. Set Zt = Zt−1 ×
n
1
N
PN
i=1 wt(θi
t−1)
o
.
In our context, tempering SMC brings two extra advantages: it makes it possible
to obtain samples from πξ,γ(θ|D) for a whole range of values of γ, rather than a
single value. And it provides an approximation of Zξ,γ(D) for the same range of γ
values, through the quantity Zt deﬁned in Algorithm 19.
103

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
4.3.3 Expectation-Propagation (Gaussian prior)
The SMC sampler outlined in the previous section works fairly well, and we will
use it as gold standard in our simulations. However, as any other Monte Carlo
method, it may be too slow for large datasets. We now turn our attention to
EP [Expectation-Propagation Minka, 2001a], a general framework to derive fast
approximations to target distributions (and their normalising constants).
First note that the pseudo-posterior may be rewritten as:
πξ,γ(θ|D) =
1
Zξ,γ(D)πξ(θ) ×
Y
i,j
fij(θ),
fij(θ) = exp [−γ′✶{⟨θ, Xi −Xj⟩< 0}]
where γ′ = γ/n+n−, and the product is over all (i, j) such that Yi = 1, Yj =
−1. EP generates an approximation of this target distribution based on the same
factorisation:
q(θ) ∝q0(θ)
Y
i,j
qij(θ),
qij(θ) = exp{−1
2θTQijθ + rT
ijθ}.
We consider in the section the case where the prior is Gaussian, as in Section
4.2.3.
Then one may set q0(θ) = πξ(θ).
The approximating factors are un-
normalised Gaussian densities (under a natural parametrisation), leading to an
overall approximation that is also Gaussian, but other types of exponential fam-
ily parametrisations may be considered; see next section and Seeger [2005a]. EP
updates iteratively each site qij (that is, it updates the parameters Qij and rij),
conditional on all the sites, by matching the moments of q with those of the hybrid
distribution
hij(θ) ∝q(θ)fij(θ)
qij(θ) ∝q0(θ)fij(θ)
Y
(k,l)̸=(i,j)
qkl(θ)
where again the product is over all (k, l) such that Yk = 1, Yl = −1, and (k, l) ̸=
(i, j).
We refer to the appendix for a precise algorithmic description of our EP imple-
mentation. We highlight the following points. First, the site update is particularly
simple in our case:
hij(θ) ∝exp{θTrh
ij −1
2θTQh
ijθ} exp [−γ′✶{⟨θ, Xi −Xj⟩< 0}] ,
with rh
ij = P
(k,l)̸=(i,j) rkl, Qh
ij = P
(k,l)̸=(i,j) Qkl, which may be interpreted as: θ
conditional on T(θ) = ⟨θ, Xi −Xj⟩has a d −1-dimensional Gaussian distribution,
and the distribution of T(θ) is that of a one-dimensional Gaussian penalised by a
step function. The two ﬁrst moments of this particular hybrid may therefore be
104

4.3 Practical implementation of the PAC-Bayesian approach
computed exactly, and in O(d2) time, as explained in the appendix. The updates
can be performed eﬃciently using the fact that the linear combination (Xi −Xj)θ
is a one dimensional Gaussian. For our numerical experiment we used a parallel
version of EP Van Gerven et al. [2010]. The complexity of our EP implementation
is O(n+n−d2 + d3).
Second, EP oﬀers at no extra cost an approximation of the normalising constant
Zξ,γ(D) of the target πξ,γ(θ|D); in fact, one may even obtain derivatives of this
approximated quantity with respect to hyper-parameters. See again the appendix
for more details.
Third, in the EP framework, cross-validation may be interpreted as dropping
all the factors qij that depend on a given data-point Xi in the global approxima-
tion q. This makes it possible to implement cross-validation at little extra cost
[Opper and Winther, 2000].
4.3.4 Expectation-Propagation (spike and slab prior)
To adapt our EP algorithm to the spike and slab prior of Section 4.2.4, we introduce
latent variables Zk = 0/1 which ”choose” for each component θk whether it comes
from a slab, or from a spike, and we consider the joint target
πξ,γ(θ, z|D) ∝
( d
Y
k=1
B(zk; p)N(θk; 0, vzk)
)
exp
"
−
γ
n+n−
X
ij
✶{⟨θ, Xi −Xj⟩> 0}
#
.
On top of the n+n−Gaussian sites deﬁned in the previous section, we add a
product of d sites to approximate the prior. Following Hernandez-Lobato et al.
[2013], we use
qk(θk, zk) = exp

zk log

pk
1 −pk

−1
2θ2
kuk + vkθk

that is a (un-normalised) product of an independent Bernoulli distribution for zk,
times a Gaussian distribution for θk. Again that the site update is fairly straightfor-
ward, and may be implemented in O(d2) time. See the appendix for more details.
Another advantage of this formulation is that we obtain a Bernoulli approxima-
tion of the marginal pseudo-posterior πξ,γ(zi = 1|D) to use in feature selection.
Interestingly taking v0 to be exactly zero also yield stable results corresponding to
the case where the spike is a Dirac mass.
105

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
4.4 Extension to non-linear scores
To extend our methodology to non-linear score functions, we consider the pseudo-
posterior
πξ,γ(ds|D) ∝πξ(ds) exp


−
γ
n+n−
X
i∈D+, j∈D−
✶{s(Xi) −s(Xj) > 0}



where πξ(ds) is some prior probability measure with respect to an inﬁnite-dimensional
functional class. Let si = s(Xi), s1:n = (s1, . . . , sn) ∈Rn, and assume that πξ(ds)
is a GP (Gaussian process) associated to some kernel kξ(x, x′), then using a stan-
dard trick in the GP literature [Rasmussen and Williams, 2006], one may derive
the marginal (posterior) density (with respect to the n-dimensional Lebesgue mea-
sure) of s1:n as
πξ,γ(s1:n|D) ∝Nd (s1:n; 0, Kξ) exp


−
γ
n+n−
X
i∈D+, j∈D−
✶{si −sj > 0}



where Nd (s1:n; 0, Kξ) denotes the probability density of the N(0, Kξ) distribution,
and Kξ is the n × n matrix (kξ(Xi, Xj))n
i,j=1.
This marginal pseudo-posterior retains essentially the structure of the pseudo-
posterior πξ,γ(θ|D) for linear scores, except that the “parameter” s1:n is now of
dimension n. We can apply straightforwardly the SMC sampler of Section 4.B.1,
and the EP algorithm of 4.B.2, to this new target distribution. In fact, for the EP
implementation, the particular simple structure of a single site:
exp [−γ′✶{si −sj > 0}]
makes it possible to implement a site update in O(1) time, leading to an overall
complexity O(n+n−+ n3) for the EP algorithm.
Theoretical results for this approach could be obtained by applying lemmas from
e.g. van der Vaart and van Zanten [2009], but we leave this for future study.
4.5 Numerical Illustration
Figure 1 compares the EP approximation with the output of our SMC sampler,
on the well-known Pima Indians dataset and a Gaussian prior. Marginal ﬁrst and
second order moments essentially match; see the appendix for further details. The
subsequent results are obtained with EP.
106

4.5 Numerical Illustration
0.0
0.5
1.0
−2
−1
0
(a) θ1
0.00
0.25
0.50
0.75
−4
−3
−2
−1
0
(b) θ2
0.0
0.5
1.0
1.5
−1
0
1
(c) θ3
Figure 4.1: EP Approximation (green), compared to SMC (blue) of
the marginal posterior of the ﬁrst three coeﬃcients, for
Pima dataset (see the appendix for additional analysis).
We now compare our PAC-Bayesian approach (computed with EP) with Bayesian
logistic regression (to deal with non-identiﬁable cases), and with the rankboost al-
gorithm [Freund et al., 2003] on diﬀerent datasets1; note that Cortes and Mohri
[2003] showed that the function optimised by rankbook is AUC.
As mentioned in Section 4.B, we set the prior hyperparameters by maximizing
the evidence, and we use cross-validation to choose γ. To ensure convergence of EP,
when dealing with diﬃcult sites, we use damping [Seeger, 2005a]. The GP version
of the algorithm is based on a squared exponential kernel. Table 5.2 summarises
the results; balance refers to the size of the smaller class in the data (recall that
the AUC criterion is particularly relevant for unbalanced classiﬁcation tasks), EP-
AUC (resp. GPEP-AUC) refers to the EP approximation of the pseudo-posterior
based on our Gaussian prior (resp. Gaussian process prior). See also Figure 2 for
ROC curve comparisons, and Table 2 in the appendix for a CPU time comparison.
Note how the GP approach performs better for the colon data, where the number
of covariates (2000) is very large, but the number of observations is only 40. It
seems also that EP gives a better approximation in this case because of the lower
dimensionality of the pseudo-posterior (Figure 4.2b).
Finally, we also investigate feature selection for the DNA dataset (180 covariates)
using a spike and slab prior. The regularization plot (4.3a) shows how certain
coeﬃcients shrink to zero as the spike’s variance v0 goes to zero, allowing for some
1All available at http://archive.ics.uci.edu/ml/
107

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Dataset
Covariates
Balance
EP-AUC
GPEP-AUC
Logit
Rankboost
Pima
7
34%
0.8617
0.8557
0.8646
0.8224
Credit
60
28%
0.7952
0.7922
0.7561
0.788
DNA
180
22%
0.9814
0.9812
0.9696
0.9814
SPECTF
22
50%
0.8684
0.8545
0.8715
0.8684
Colon
2000
40%
0.7034
0.75
0.73
0.5935
Glass
10
1%
0.9843
0.9629
0.9029
0.9436
Table 4.1: Comparison of AUC.
The Glass dataset has originally more than two classes. We compare the “silicon” class against
all others.
sparsity. The aim of a positive variance in the spike is to absorb negligible eﬀects
into it [Roˇckov´a and George, 2013]. We observe this eﬀect on ﬁgure 4.3a where
one of the covariates becomes positive when v0 decreases.
−0.3
−0.2
−0.1
0.0
0.1
1e−04
1e−02
v0
θ
(a) Regularization plot
●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●
●●●●●●●●
●
●●●●
●●●●●●●●●
●
●●
●
●●
●
●●
●●●●●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●
●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●
●
●
●●
●●●●●●●●
●●●●
●●
●
●
●●
●●●●●
●
●●
●●●
●
●
●
●
●●
●●●
●
●
●
●
●●
●
●●●
●●
●
●●
●
●●
●
●●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●●
●●●●
●
●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●●
●
●●●●●●●●●●●●●●●●
●
●●●●●●●●●●●●●●●
●●●●●●●●●●
●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●
●●●●●●●●
●
●●●●
●●●●●●●●●
●
●●
●
●●
●
●●
●●●●●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●
●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●
−0.1
0.0
0
50
100
150
V1
θ
(b) Estimate
Figure 4.3: Regularization plot for v0 ∈

10−6, 0.1

and estimation for
v0 = 10−6 for DNA dataset; blue circles denote posterior
probabilities ≥0.5.
108

0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
(a) Rankboost
vs
EP-AUC
on
Pima
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
(b) Rankboost vs GPEP-AUC on
Colon
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
(c) Logistic vs EP-AUC on Glass
Figure 4.2: Some ROC curves associated to the example described in
a more systematic manner in table 5.2. In black is always
the PAC version.
4.6 Conclusion
The combination of the PAC-Bayesian theory and Expectation-Propagation leads
to fast and eﬃcient AUC classiﬁcation algorithms, as observed on a variety of
datasets, some of them very unbalanced. Future work may include extending our
approach to more general ranking problems (e.g. multi-class), establishing non-
asymptotic bounds in the nonparametric case, and reducing the CPU time by
considering only a subset of all the pairs of datapoints.
109

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Appendix
4.A PAC-Bayes bounds for linear scores
4.A.1 Suﬃcient condition for Dens(c)
A simple suﬃcient condition for Dens(c) to hold is that (X1 −X2)/∥X1 −X2∥
admits a probability density with respect to the spherical measure of dimension
d −1 which is bounded above by B. Then
P(⟨X1 −X2, θ⟩≥0, ⟨X1 −X2, θ′⟩≤0) ≤B arccos (⟨θ, θ′⟩)
2π
≤B
2π
p
5 −5 ⟨θ, θ′⟩
= B
2π
r
5
2∥θ −θ′∥.
4.A.2 Proof of Lemma 2.1
In order to prove Lemma 2.1 we need the following Bernstein inequality.
Proposition 4.A.1 (Bernstein’s inequality for U-statistics) For any γ > 0,
for any θ ∈Rd,
E exp[γ|Rn(θ) −Rn −R(θ) + R|] ≤2 exp
" γ2
n−1E((qθ
1,2)2)
 1 −
4γ
n−1

#
.
Proof of Proposition 4.A.1. Fix θ. Remember that
qθ
i,j = 1{⟨θ, Xi −Xj⟩(Yi −Yj) < 0} −1{[σ(Xi) −σ(Xj)](Yi −Yj) < 0} −R(θ) + R
so that
Un := Rn(θ) −Rn −R(θ) + R =
1
n(n −1)
X
i̸=j
qθ
i,j.
First, note that
E exp[γ|Un|] ≤E exp[γUn] + E exp[γ(−Un)].
110

4.A PAC-Bayes bounds for linear scores
We will only upper bound the ﬁrst term in the r.h.s., as the upper bound for the
second term may be obtained exactly in the same way (just replace qθ
i,j by −qθ
i,j).
Now, use Hoeﬀding’s decomposition Hoeﬀding [1948]: this is the technique used
by Hoeﬀding to prove inequalities on U-statistics. Hoeﬀding proved that
Un = 1
n!
X
π
1
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)
where the sum is taken over all the permutations π of {1, . . . , n}. Jensen’s inequal-
ity leads to
E exp[γUn] = E exp

γ 1
n!
X
π
1
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)


≤1
n!
X
π
E exp

γ
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)

.
We now use, for each of the terms in the sum, Massart’s version of Bernstein’s
inequality Massart [2007] (ineq. (2.21) in Chapter 2, the assumption is checked by
qθ
π(i),π(i+⌊n
2 ⌋) ∈[−2, 2] so E((qθ
π(i),π(i+⌊n
2 ⌋))k) ≤E((qθ
π(i),π(i+⌊n
2 ⌋))2)2k−2). We obtain:
E exp

γ
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)

≤exp

E((qθ
π(1),π(1+⌊n
2 ⌋))2) γ2
⌊n
2 ⌋
2

1 −2 γ
⌊n
2 ⌋


.
First, note that we have the inequality ⌊n
2⌋≥(n −1)/2. Then, remark that as the
pairs (Xi, Yi) are iid, we have E((qθ
π(1),π(1+⌊n
2 ⌋))2) = E((qθ
1,2)2) so we have a simpler
inequality
E exp

γ
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)

≤exp
"
E((qθ
1,2)2) γ2
n−1
 1 −
4γ
n−1

#
.
This ends the proof of the proposition. □
The following proposition is also of use in the proof of lemma 2.1.
Proposition 4.A.2 For any measure ρ ∈M1
+(Θ) and any measurable function
h : θ →R such that
´
exp(h(θ))π(dθ) < ∞, we have
log
ˆ
exp(h(θ))π(θ)

= sup
ρ∈M1
+
ˆ
h(θ)ρ(dθ) −K(ρ, π)

.
111

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
In addition if h is bounded by above on the support of π the supremum is reached
for the Gibbs distribution,
ρ(dθ) ∝exp (h(θ)) π(dθ).
Proof:
e.g. Catoni [2007].
□
Proof of Lemma 2.1
From the proof of Proposition 4.A.1, and using the short-
hand qθ for qθ
1,2, we deduce
E

exp{ρ
 γ(Rn(θ) −¯Rn −R(θ) + ¯R)} + η(θ)

≤exp
 
γ2
n −1
ρ (Eq2
θ)
(1 −4
γ
n−1) + ρ (η(θ))
!
.
(4.2)
Using proposition 4.A.2, and the fact that ex ≥✶{x ≥0} we have that
P{
sup
ρ∈M1
+(Θ)
ρ
 γ(Rn(θ) −¯Rn −R(θ) + ¯R) −η(θ)

−K(ρ, π) ≥0}
≤E
 π{exp{ρ
 γ(Rn(θ) −¯Rn −R(θ) + ¯R) −η(θ)}

}
= π
 E{exp{ρ
 γ(Rn(θ) −¯Rn −R(θ) + ¯R) −η(θ)}

}
, by Fubini
≤π
(
exp
 
γ2ρ(Eq2
θ)
(n −1)(1 −
4γ
n−1) −ρ(η(θ))
!)
, using (4.2).
In the following we take η(θ) = log 1
ǫ +
γ2
n−1
ρ(Eq2
θ)
(1−4
γ
n−1 ) leading to the following result
with probability at least 1 −ǫ, ∀ρ ∈M1
+(Θ):
ρ(Rn(θ)) −¯Rn ≤ρ(R(θ)) −¯R + K(ρ, π) + log 1
ǫ
γ
+
γ
n −1
ρ(Eq2
θ)
(1 −4
γ
n−1).
(4.3)
Under MA(1, C) we can write:
ρ(Rn(θ)) −¯Rn ≤
 
1 +
γC
n −1
1
(1 −
4
n−1)
!
 ρ(R(θ)) −¯R

+ K(ρ, π) + log 1
ǫ
γ
.
Using Bernstein’s inequality in the symmetric case, with probability 1 −ǫ we can
assert that:
 
1 −
γC
n −1
1
(1 −γ
4
n−1)
!
 ρ(R(θ)) −¯R

≤ρ(Rn(θ)) −¯Rn + K(ρ, π) + log 1
ǫ
γ
.
112

4.A PAC-Bayes bounds for linear scores
The latter is true in particular for ρ = π(θ|S), the Gibbs posterior:
 
1 −
γC
n −1
1
(1 −γ
4
n−1)
! ˆ
Θ
R(θ)πγ(dθ|D) −¯R

≤inf
ρ∈M1
+

ρ(Rn(θ)) −¯Rn + K(ρ, π) + log 1
ǫ
γ

.
Making use of equation (4.3) and the fact that γ ≤(n −1)/8C we have with
probability 1 −2ǫ:
ˆ
Θ
Rn(θ)πγ(dθ|D) −¯Rn

≤2 inf
ρ∈M1
+

ρ(R(θ)) −¯R + 2K(ρ, π) + log 1
ǫ
γ

.
□
Lemma 2.1 gives some approximately correct ﬁnite sample bound under hy-
pothesis MA(1, C). It is easy to extend those results to the more general case of
MA(∞, C). Note in particular that this assumption is always satisﬁed for C = 4.
Proof of Lemma 2.2
First consider in our case that, the margin assumption is
always true for C = 4, E(q2
θ) ≤4, the rest of the proof is similar to that of lemma
2.1. From equation (4.3) with the above hypothesis:
ρ(Rn(θ)) −¯Rn ≤ρ(R(θ)) −¯R + K(ρ, π) + log 1
ǫ
γ
+
4γ
n −1
1
(1 −
4
n−1)
From the Bernstein inequality with in the symmetric case we get with probability
1 −ǫ:
ρ(R(θ)) −¯R ≤ρ(Rn(θ)) −¯Rn + K(ρ, π) + log 1
ǫ
γ
+
4γ
n −1
1
(1 −
4
n−1)
We get, after noting that the Gibbs posterior can be written as an inﬁmum
(Legendre transform), with probability 1 −2ǫ:
ˆ
(R(θ)πγ(dθ|D) −¯R ≤
inf
ρ∈M1
+(Θ) ρ(R(θ)) −¯R + K(ρ, π) + log 1
ǫ
γ
+ 16γ
n −1
(we also used γ ≤(n −1)/8).
□
The two above lemma depend on some class complexity K(ρ, π). The latter can
be specialized to diﬀerent choice of prior measure π. In the following we propose
two speciﬁcations to a Gaussian prior and a spike and slab prior.
113

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
4.A.3 Proof of Theorem 2.3 (Independent Gaussian prior)
For any θ0 ∈Rp with ∥θ0∥= 1 and δ > 0 we put
ρθ0,δ(dθ) ∝1∥θ−θ0∥≤δπ(dθ).
Then we have, from Lemma 2.1, with probability at least 1 −ε,
ˆ
R(θ)πγ(dθ|D) −R ≤2 inf
θ0,δ
(ˆ
R(θ)ρθ0,δ(dθ) −R + 16C K(ρθ0,δ, π) + log
  4
ε

(n −1)
)
First, note that
R(θ) = E (✶{⟨θ, X −X′⟩(Y −Y ′) < 0})
= E (✶{⟨θ0, X −X′⟩(Y −Y ′) < 0})
+ E (✶{⟨θ, X −X′⟩(Y −Y ′) < 0} −✶{⟨θ0, X −X′⟩(Y −Y ′) < 0})
≤R(θ0) + P(sign ⟨θ, X −X′⟩(Y −Y ′) ̸= sign ⟨θ0, X −X′⟩(Y −Y ′))
= R(θ0) + P(sign ⟨θ, X −X′⟩̸= sign ⟨θ0, X −X′⟩)
≤R(θ0) + c

θ
∥θ∥−θ0

≤R(θ0) + 2c∥θ −θ0∥.
As a consequence
´
R(θ)ρθ0,δ(dθ) ≤R(θ0) + 2cδ.
The next step is to calculate K(ρθ0,δ, π). We have
K(ρθ0,δ, π) = log
1
π ({θ : ∥θ −θ0∥≤δ}).
Assuming that θ0,1 > 0 (the proof is exactly symmetric in the other case)
−K(ρθ0,δ, π) = log π
 
{θ :
d
X
i=1
(θi −θ0,i)2 ≤δ2}
!
≥d log π

{θ : (θ1 −θ0,1)2 ≤δ2
d }

≥d log
ˆ
θ0,1
√
ϑ +
δ
√
ϑd
θ0,1
√
ϑ −
δ
√
ϑd
ϕ(0,1)(x)dx
≥d log

δ
2
√
ϑd
ϕ
θ0,1
√
ϑ
+
δ
√
ϑd

≥d log

δ
2
√
ϑd
ϕ
 1
√
ϑ
+
δ
√
ϑd

114

4.A PAC-Bayes bounds for linear scores
= d log
 
δ
2
√
2πϑd
exp
"
−1
2
 1
√
ϑ
+
δ
√
ϑd
2#!
≥d log

δ
2
√
2πϑd
exp

−1
ϑ −δ2
ϑd

K(ρθ0,δ, π) ≤−d log{δ} + d
2 log{8πϑd} + 1
ϑ + δ2
ϑd
And we can plug the equation above in the result of lemma 2.1 with δ = 1
n
ˆ
R(θ)πγ(θ|D) −¯R ≤2 inf
θ0

R(θ0) −¯R + 2c1
n + 2
γ

d log{n} + d
2 log{8πϑd} + 1
ϑ +
1
n2
ϑd + log 4
ǫ

Any γ = O(n) will lead to a convergence result. Taking γ = (n −1)/8C and
optimizing in ϑ we obtain a variance of ϑ =
2(1+
1
n2d )
d
.
4.A.4 Proof of Theorem 2.4 (Independent Gaussian prior)
As was done for the previous lemmas we can lift the MA(∞, C) and use the lemma
2.2 instead, which gives rise to Theorem 2.4.
Use Lemma 2.2 and the same steps as in the proof of Theorem 4.2.3, optimize
w.r.t. γ and ϑ to get the result.
We show the same kind of result in the following but for spike and slab priors.
4.A.5 Proof of Theorem 2.5 (Spike and slab prior for feature
selection)
As for the proof of theorem 4.2.3 we start by deﬁning, for any θ0 ∈Rp with
∥θ0∥= 1 and δ > 0,
ρθ0,δ(dθ) ∝1∥θ−θ0∥≤δπ(dθ)
so that in the end, by a similar argument as previously it remains only to upper
bound the following quantity,
K(ρθ0,δ, π) = log
1
π ({θ : ∥θ −θ0∥≤δ}).
Let π0 denote the probability distribution such that the θi are iid N(0, v0). So:
−K(ρθ0,δ, π) = log π
 (
θ :
d
X
i=1
(θi −θ0,i)2 ≤δ2
)!
115

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
≥log π

θ : ∀i, (θi −θ0,i)2 ≤δ2
d

=
X
i:θ0,i̸=0
log π

(θi −θ0,i)2 ≤δ2
d

+ log π

∀i with θ0,i = 0, θ2
i < δ2
d

≥
X
i:θ0,i̸=0
log π

(θi −θ0,i)2 ≤δ2
d

+ log π0

∀i with θ0,i = 0, θ2
i < δ2
d

+ d log(1 −p)
=
X
i:θ0,i̸=0
log π

(θi −θ0,i)2 ≤δ2
d

+ log

1 −π0

∃i, θ0,i = 0, θ2
i > δ2
d

+ d log(1 −p)
≥
X
i:θ0,i̸=0
log π

(θi −θ0,i)2 ≤δ2
d

+ log
"
1 −
X
i:θi=0
π0

θ2
i > δ2
d
#
+ d log(1 −p).
Assume ﬁrst that i is such that θ0,i = 0. Then:
π0

θ2
i > δ2
d

= π0

θi
√v0
 >
δ
√v0d

≤exp

−δ2
2v0d

,
and so
X
i:θ0,i=0
π0

θ2
i > δ2
d

≤d exp

−δ2
2v0d

≤1
2
as soon as v0 ≤δ2/(2d log(d)). Then, assume that i is such that θ0,i ̸= 0. Now
assume that θ0,i > 0 (the proof is exactly symmetric if θ0,i < 0):
π

θ : (θi −θ0,i)2 ≤δ2
d

≥p
ˆ
θ0,i
√v1 +
δ
√
v1d
θ0,i
√v1 −
δ
√
v1d
ϕ(0,1)(x)dx
≥
pδ
2√v1dϕ
 θ0,i
√v1
+
δ
√v1d

116

4.B Practical implementation of the PAC-Bayesian approach
≥
pδ
2√v1dϕ
 1
√v1
+
δ
√v1d

=
pδ
2√2πv1d exp
"
−1
2
 1
√v1
+
δ
√v1d
2#
≥
pδ
2√2πv1d exp

−1
v1
−δ2
v1d

.
Putting everything together:
K(ρθ0,δ, π) ≤−∥θ0∥0 log

pδ
2√2πv1d exp

−1
v1
−δ2
v1d

+ log(2) + d log
1
1 −p
= ∥θ0∥0

log
2√2πv1d
pδ

+ 1
v1
+ δ2
v1d

+ log(2) + d log
1
1 −p.
So, we have:
ˆ
R(θ)πγ(dθ|D) −R ≤2 inf
θ0,δ
(
R(θ0) −R + 2cδ
+ 16C
∥θ0∥0
h
log

2√2πv1d
pδ

+ 1
v1 + δ2
v1d
i
+ log(2) + d log
1
1−p + log
  4
ε

(n −1)
)
4.B Practical implementation of the PAC-Bayesian
approach
4.B.1 Sequential Monte Carlo
The resampling scheme we use in our SMC sampler is systematic resampling, see
Algorithm 20.
To move the particles while leaving invariant the current target πξ,γ(θ|D), we
use the standard random walk Metropolis strategy, but scaled to the current set
of particles, as outlined by Algorithm 17.
4.B.2 Expectation-Propagation (Gaussian prior)
EP aims at approximating posterior distributions of the form,
π(θ|D) = 1
Zπ
P0(θ)
n
Y
i=1
ti(θ)
117

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Algorithm 16 Systematic resampling
Input: Normalised weights W j
t := wt(θj
t−1)/ PN
i=1 wt(θi
t−1).
Output: indices Ai ∈{1, . . . , N}, for i = 1, . . . , N.
a. Sample U ∼U([0, 1]).
b. Compute cumulative weights as Cn = Pn
m=1 NW m.
c. Set s ←U, m ←1.
d. For n = 1 : N
While Cm < s do m ←m + 1.
An ←m, and s ←s + 1.
End For
by approximating each site ti(θ) by a distribution from an exponential family qi(θ).
The algorithm cycles through each site, computes the cavity distribution Q\i(θ) ∝
Q(θ)q−1
i (θ) and minimizes the Kullback-Leibler divergence between Q\i(θ)ti(θ) and
the global approximation Q(θ). This is eﬃciently done by using properties of the
exponential family (e.g. Bishop [2006a]).
In the Gaussian case the EP approximation can be written as a product of some
prior and a product of sites:
Q(θ) ∝N(θ; 0, Σ)
Y
i,j
qij(θ),
for which the sites are unnormalized Gaussians for the natural parametrization
qij(θ) ∝exp
 −1
2θTQijθ + θrij

.
We can equivalently use the one dimensional
representation qij(sij) ∝exp
 −1
2s2
ijKij + sijhij

, going from one to the other is
easily done by multiplying θ by (ei −ej)X where ∀i ∈{1, · · · , n},
ei is a vector
of zeroes with one on the i-th line. Hence we keep in memory only (Kij)ij and
(hij)ij.
While computing the cavity moment we must compute (Q −(Xi −Xj)(Xi −Xj)Kij)
and its inverse. The latter can be computed eﬃciently using Woodbury formula.
Equivalently one could use similar tricks where only the Cholesky factorisation is
saved and updated as in Seeger [2005a]. By precomputing some matrix multipli-
cation the later cavity moment computation can be done in complexity O(p2).
118

4.B Practical implementation of the PAC-Bayesian approach
Algorithm 17 Gaussian random walk Metropolis step
Input: θ, S (d × d positive matrix)
Output: θnext
a. Sample θprop ∼N(θ, S).
b. Sample U ∼U([0, 1]).
c. If log(U) ≤log πξ,γ(θprop|D)/πξ,γ(θ|D), set θnext ←θprop, otherwise set θnext ←
θ.
To update the sites we compute normalizing constant Zij =
´
N(s; m\ij, σ\ij)tij(s)ds
and use properties of exponential families.
Normalising Constant
The normalizing constant of the posterior can be com-
puted using EP. We have that for each sites tij(θ) = Cijqij(θ) we replace those
sites in integral we wish to approximate,
ˆ
N(θ; 0, Σ)
Y
ij
tij(θ)dθ ≃
Y
ij
Cij
ˆ
N(θ; 0, Σ)
Y
ij
qij(θ)dθ
The integral on the right hand side is a Gaussian convolution and is therefore also
Gaussian. The Cijs can be approximated by matching the zeroth order moment
in the site update. As noted in the chapter we can also compute the derivatives
with respect to some prior hyper-parameter (see Seeger [2005a]).
4.B.3 Expectation-Propagation (spike and slab prior)
The posterior can be written as
π(θ|D) ∝
Y
i,j
tij(θ)
d
Y
k=1
tk(θk, zk)Ber(zk; p),
where zk ∈{0, 1} codes the origin of θk, spike/slab, and where tk(θk, zk) ∝
zkN(θk; 0, v0) + (1 −zk)N(θk; 0, v1). The approximation given by EP is of the
form,
Q(θ, z) ∝
Y
i,j
qij(θ)
d
Y
k=1
qk(θk, zk)Ber(zk; pk),
119

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Algorithm 18 parallel EP for Gaussian Prior
Input: ϑ, γ
Output: m and V
Init: V ←Σ, m ←0
Untill Convergence Do
For all sites (i, j)Do in parallel
a. Compute the cavity moments m\ij, V \ij
b. Compute the 1st and 2nd order moments of q\ij(sij)tij(sij)
c. Update Kij and hij
End For
Update V = (Σ−1 + P
ij(Xi −Xj)T(Xi −Xj)Kij)−1, m = V (P
ij(Xi −Xj)hij)
End While
where qk(θk, zk) ∝Ber(zk, pk)N(θk; mk, σ2
k), and tij(θ) is as in the previous section.
The cavity moments are easy to compute as the approximation is Gaussian in θ
and Bernoulli in z. In both cases we can deduce cavity moments because division
is stable inside those classes of functions. We get some distribution Q\k(θk) ∝
Ber(zk; p\k)N(θk; m\k, σ2,\k).
We can compute the normalizing constant of the
distribution Q\ij(θ)tk(θk, zk), namely,
Zk = p\k
ˆ
N(θk; 0, v0)N(θk; m\k, σ2,\k)dθk+(1−p\k)
ˆ
N(θk; 0, v0)N(θk; m\k, σ2,\k)dθk
Where we can ﬁnd the update by computing the derivatives of log Zk with respect
to p\k, m\k and σ2,\k
Initialization for the Gaussian is done to a given Σ0 that will be subtracted later
on. The initial pks are taken such that the approximation equals the prior p at the
ﬁrst iteration.
4.C Numerical illustration
Figure 4.C.1 shows the posterior marginals as given by EP and tempering SMC.
The later is exact in the sense that the only error stems from Monte Carlo; we
120

4.C Numerical illustration
see that the mode is well approximated however the variance is slightly underesti-
mated.
In Table 4.C.1 we show the CPU times in seconds, on all dataset studied. Ex-
periments where run with a i7-3720QM CPU @ 2.60GHz intel processor with 6144
KB cache. Our linear model is overall faster on those datasets. A caveat is that
Rankboost is implemented in Matlab, while our implementation is in C.
Dataset
Covariates
Balance
EP-AUC
GPEP-AUC
Rankboost
Pima
7
34%
0.06
7.75
3.26
Credit
60
28%
1.98
7.59
56.54
DNA
180
22%
11.26
63.47
141.60
SPECTF
22
50%
0.25
63.47
3.55
Colon
2000
40%
636.63
60.99
156.85
Glass
10
1%
0.23
1.33
2.36
Table 4.C.1: Computation times in seconds
121

4 Theoretical and computational aspects of PAC Bayesian ranking and scoring
Figure 4.C.1: Comparison of the output of the two algorithms
0.0
0.5
1.0
−2
−1
0
(a) 1st covariate
0.00
0.25
0.50
0.75
−4
−3
−2
−1
0
(b) 2nd covariate
0.0
0.5
1.0
1.5
−1
0
1
(c) 3rd covariate
0.00
0.25
0.50
0.75
1.00
1.25
−1
0
1
(d) 4th covariate
0.0
0.3
0.6
0.9
−3
−2
−1
0
1
(e) 5th covariate
0.0
0.5
1.0
−2
−1
0
(f) 6th covariate
0.00
0.25
0.50
0.75
1.00
−3
−2
−1
0
1
(g) 7th covariate
Comparison of the Gaussian approximation obtained by Fractional EP (green) with the true density
generated by SMC (blue) on the Pima indians dataset
122

5
Properties of variational approximations of Gibbs
posteriors
This is joint work with Pierre Alquier and Nicolas Chopin
Status: Submitted to Journal of Machine Learning Rechearch
5.1 Introduction
A Gibbs posterior, also known as a PAC-Bayesian or pseudo-posterior, is a prob-
ability distribution for random estimators of the form:
ˆρλ(dθ) = exp[−λrn(θ)]
´
exp[−λrn]dππ(dθ).
More precise deﬁnitions will follow, but for now, θ may be interpreted as a pa-
rameter (in a ﬁnite or inﬁnite-dimensional space), rn(θ) as an empirical measure
of risk (e.g. prediction error), and π(dθ) a prior distribution.
We will follow in this chapter the PAC (Probably Approximatively Correct)-
Bayesian approach, which originates from machine learning [Catoni, 2004; McAllester,
1998; Shawe-Taylor and Williamson, 1997]; see Catoni [2007] for an exhaustive
study, and Dalalyan and Tsybakov [2008]; Jiang and Tanner [2008]; Yang [2004];
Zhang [2006] for related perspectives (such as the aggregation of estimators in
the last 3 papers). There, ˆρλ appears as the probability distribution that min-
imises the upper bound of an oracle inequality on the risk of random estimators.
The PAC-Bayesian approach oﬀers sharp theoretical guarantees on the properties
of such estimators, without assuming a particular model for the data generating
process.
123

5 Properties of variational approximations of Gibbs posteriors
The Gibbs posterior has also appeared in other places, and under diﬀerent mo-
tivations: in Econometrics, as a way to avoid direct maximisation in moment
estimation [Chernozhukov and Hong, 2003]; and in Bayesian decision theory, as
as way to deﬁne a Bayesian posterior distribution when no likelihood has been
speciﬁed [Bissiri et al., 2013]. Another well-known connection, although less di-
rectly useful (for Statistics), is with thermodynamics, where rn is interpreted as
an energy function, and λ as the inverse of a temperature.
Whatever the perspective, estimators derived from Gibbs posteriors usually
show excellent performance in diverse tasks, such as classiﬁcation, regression, rank-
ing, and so on, yet their actual implementation is still far from routine. The usual
recommendation [Alquier and Biau, 2013; Dalalyan and Tsybakov, 2012; Guedj and Alquier,
2013] is to sample from a Gibbs posterior using MCMC [Markov chain Monte Carlo,
see e.g. Green et al., 2015]; but constructing an eﬃcient MCMC sampler is often
diﬃcult, and even eﬃcient implementations are often too slow for practical uses
when the dataset is very large.
In this chapter, we consider instead VB (Variational Bayes) approximations,
which have been initially developed to provide fast approximations of ‘true’ poste-
rior distributions (i.e. Bayesian posterior distributions for a given model); see Jordan et al.
[1999]; MacKay [2002] and Chap. 10 in Bishop [2006a].
Our main results are as follows: when PAC-Bayes bounds are available - mainly,
when a strong concentration inequality holds - replacing the Gibbs posterior by
a variational approximation does not aﬀect the rate of convergence to the best
possible prediction, on the condition that the K¨ullback-Leibler divergence between
the posterior and the approximation is itself controlled in an appropriate way.
We also provide empirical bounds, which may be computed from the data so
as to ascertain the actual performance of estimators obtained by variational ap-
proximation. All the results gives strong incentives, we believe, to recommend
Variational Bayes as the default approach to approximate Gibbs posteriors.
The rest of the chapter is organized as follows. In Section 5.2 we introduce the
notations and assumptions. In Section 5.3 we introduce variational approxima-
tions and the corresponding algorithms. The main results are provided in general
form in Section 5.4: in Subsection 5.4.1, we give results under the assumption that
a Hoeﬀding type inequality holds (slow rates) and in Subsection 5.4.2, we give
results under the assumption that a Bernstein type inequality holds (fast rates).
Note that for the sake of shortness, we will refer to these settings as “Hoeﬀding
assumption” and “Bernstein assumption” even if this terminology is non stan-
dard. We then apply these results in various settings: classiﬁcation (Section 5.5),
convex classiﬁcation (Section 5.6), ranking (Section 5.7), and matrix completion
(Section 5.8). In each case, we show how to specialise the general results of Sec-
tion 5.4 to the considered application, so as to obtain the properties of the VB
124

5.2 PAC-Bayesian framework
approximation, and we also discuss its numerical implementation. All the proofs
are collected in the Appendix.
5.2 PAC-Bayesian framework
We observe a sample (X1, Y1), . . . , (Xn, Yn), taking values in X × Y, where the
pairs (Xi, Yi) have the same distribution P. We will assume explicitly that the
(Xi, Yi)’s are independent in several of our specialised results, but we do not make
this assumption at this stage, as some of our general results, and more gener-
ally the PAC-Bayesian theory, may be extended to dependent observations; see
e.g. Alquier and Li [2012]. The label set Y is always a subset of R. A set of pre-
dictors is chosen by the statistician: {fθ : X →R, θ ∈Θ}. For example, in linear
regression, we may have: fθ(x) = ⟨θ, x⟩, the inner product of X = Rd, while in
classiﬁcation, one may have fθ(x) = I⟨θ,x⟩>0 ∈{0, 1}.
We assume we have at our disposal a risk function R(θ); typically R(θ) is a
measure of the prevision error. We set R = R(θ), where θ ∈arg minΘ R; i.e. fθ is
an optimal predictor. We also assume that the risk function R(θ) has an empirical
counterpart rn(θ), and set rn = rn(θ). Often, R and rn are based on a loss function
ℓ: R2 →R; i.e. R(θ) = E[ℓ(Y, fθ(X))] and rn(θ) = 1
n
Pn
i=1 ℓ(Yi, fθ(Xi)). (In this
chapter, the symbol E will always denote the expectation with respect to the
(unknown) law P of the (Xi, Yi)’s.) There are situations however (e.g. ranking),
where R and rn have a diﬀerent form.
We deﬁne a prior probability measure π(·) on the set Θ (equipped with the
standard σ-algebra for the considered context), and we let M1
+(Θ) denote the set
of all probability measures on Θ.
Deﬁnition 5.2.1 We deﬁne, for any λ > 0, the pseudo-posterior ˆρλ by
ˆρλ(dθ) = exp[−λrn(θ)]
´
exp[−λrn]dππ(dθ).
The pseudo-posterior ˆρλ (also known as the Gibbs posterior, Catoni [2004, 2007],
or the exponentially weighted aggregate, Dalalyan and Tsybakov [2008]) plays a
central role in the PAC-Bayesian approach.
It is obtained as the distribution
that minimises the upper bound of a certain oracle inequality applied to random
estimators.
Practical estimators (predictors) may be derived from the pseudo-
posterior, by e.g. taking the expectation, or sampling from it. Of course, when
exp[−λrn(θ)] may be interpreted as the likelihood of a certain model, ˆρλ becomes
a Bayesian posterior distribution, but we will not restrict our attention to this
particular case.
The following ‘theoretical’ counterpart of ˆρλ will prove useful to state results.
125

5 Properties of variational approximations of Gibbs posteriors
Deﬁnition 5.2.2 We deﬁne, for any λ > 0, πλ as
πλ(dθ) = exp[−λR(θ)]
´
exp[−λR]dππ(dθ).
We will derive PAC-Bayesian bounds on predictions obtained by variational ap-
proximations of ˆρλ under two types of assumptions: a Hoeﬀding-type assumption,
from which we may deduce slow rates of convergence (Subsection 5.4.1), and a
Bernstein-type assumption, from which we may obtain fast rates of convergence
(Subsection 5.4.2).
Deﬁnition 5.2.3 We say that a Hoeﬀding assumption is satisﬁed for prior π when
there is a function f and an interval I ⊂R∗
+ such that, for any λ ∈I,
π (E exp {λ[R(θ) −rn(θ)]})
π (E exp {λ[rn(θ) −R(θ)]})

≤exp [f(λ, n)] .
(5.1)
Inequality (5.1) can be interpreted as an integrated version (with respect to π)
of Hoeﬀding’s inequality, for which f(λ, n) ≍λ2/n. In many cases the loss will
be bounded uniformly over θ; then Hoeﬀding’s inequality will directly imply (5.1).
The expectation with respect to π in (5.1) allows us to treat some cases where the
loss is not upper bounded by specifying a prior with suﬃciently light tails.
Deﬁnition 5.2.4 We say that a Bernstein assumption is satisﬁed for prior π
when there is a function g and an interval I ⊂R∗
+ such that, for any λ ∈I,
π
 E exp

λ[R(θ) −R] −λ[rn(θ) −rn]
	
π
 E exp

λ[rn(θ) −rn] −λ[R(θ) −R]
	

≤π
 exp

g(λ, n)[R(θ) −R]

.
(5.2)
This assumption is satisﬁed for example by sums of i.i.d. sub-exponential ran-
dom variables, see Subsection 2.4 p. 27 in Boucheron et al. [2013], when a mar-
gin assumption on the function R(·) is satisﬁed [Tsybakov, 2004]. This is dis-
cussed in Section 5.4.2.
Again, extensions beyond the i.i.d.
case are possible,
see e.g. Wintenberger [2010] for a survey and new results.
In all these exam-
ples, the important feature of the function g that we will use to derive rates of
convergence is the fact that there is a constant c > 0 such that when λ = cn,
g(λ, n) = g(cn, n) ≍n.
As mentioned previously, we will often consider rn(θ) =
1
n
Pn
i=1 ℓ(Yi, fθ(Xi)),
however, the previous assumptions can also be satisﬁed when rn(θ) is a U-statistic,
using Hoeﬀding’s decomposition of U-statistics combined with the corresponding
inequality for sums of independent variables [Hoeﬀding, 1948]. This idea comes
from Cl´emen¸con et al. [2008a] and we will use it in our ranking application.
126

5.3 Numerical approximations of the pseudo-posterior
Remark 5.2.1 We could consider more generally inequalities of the form
π
 E exp

λ[R(θ) −R] −λ[rn(θ) −rn]
	
π
 E exp

λ[rn(θ) −rn] −λ[R(θ) −R]
	

≤π
 exp

g(λ, n)[R(θ) −R]κ
that allow to use the more general form of the margin assumption of Mammen and Tsybakov
[1999]; Tsybakov [2004]. PAC-Bayes bounds in this context are provided by Catoni
[2007]. However, the techniques involved would require many pages to be described
so we decided to focus on the cases κ = 0 and κ = 1 to keep the exposition simple.
5.3 Numerical approximations of the
pseudo-posterior
5.3.1 Monte Carlo
As already explained in the introduction, the usual approach to approximate ˆρλ is
MCMC (Markov chain Monte Carlo) sampling. Ridgway [2015] proposed temper-
ing SMC (Sequential Monte Carlo, e.g. Del Moral et al. [2006b]) as an alternative
to MCMC to sample from Gibbs posteriors: one samples sequentially from ˆρλt,
with 0 = λ0 < · · · < λT = λ where λ is the desired temperature. One advantage of
this approach is that it makes it possible to contemplate diﬀerent values of λ, and
choose one by e.g. cross-validation. Another advantage is that such an algorithm
requires little tuning; see Appendix B for more details on the implementation of
tempering SMC. We will use tempering SMC as our gold standard in our numerical
studies.
SMC and related Monte Carlo algorithms tend to be too slow for practical use
in situations where the sample size is large, the dimension of Θ is large, or fθ is ex-
pensive to compute. This motivates the use of fast, deterministic approximations,
such as Variational Bayes, which we describe in the next section.
5.3.2 Variational Bayes
Various versions of VB (Variational Bayes) have appeared in the literature, but the
main idea is as follows. We deﬁne a family F ⊂M1
+(Θ) of probability distributions
that are considered as tractable. Then, we deﬁne the VB-approximation of ˆρλ: ˜ρλ.
Deﬁnition 5.3.1 Let
˜ρλ = arg min
ρ∈F K(ρ, ˆρλ),
where K(ρ, ˆρλ) denotes the KL (K¨ullback-Leibler) divergence of ˆρλ relative to ρ:
K(m, µ) =
´
log[ dm
dµ ]dm if m ≪µ (i.e. µ dominates m), K(m, µ) = +∞otherwise.
127

5 Properties of variational approximations of Gibbs posteriors
The diﬃculty is to ﬁnd a family F (a) which is large enough, so that ˜ρλ may be
close to ˆρλ, and (b) such that computing ˜ρλ is feasible. We now review two types
of families popular in the VB literature.
• Mean ﬁeld VB: for a certain decomposition Θ = Θ1 × · · · × Θd, F is the set
of product probability measures
FMF =
(
ρ ∈M1
+(Θ) : ρ(dθ) =
d
Y
i=1
ρi(dθi), ∀i ∈{1, . . . , d}, ρi ∈M1
+(Θi)
)
.
(5.3)
The inﬁmum of the KL divergence K(ρ, ˆρλ), relative to ρ = Q
i ρi satisﬁes
the following ﬁxed point condition [Bishop, 2006a; Parisi, 1988, Chap. 10]:
∀j ∈{1, · · · , d}
ρj(dθj) ∝exp
 ˆ
{−λrn(θ) + log π(θ)}
Y
i̸=j
ρi(dθi)
!
π(dθj).
(5.4)
This leads to a natural algorithm were we update successively every ρj until
stabilization.
• Parametric family:
FP =

ρ ∈M1
+(Θ) : ρ(dθ) = f(θ; m)dθ, m ∈M
	
;
and M is ﬁnite-dimensional; say FP is the family of Gaussian distributions
(of dimension d). In this case, several methods may be used to compute the
inﬁmum. As above, one may used ﬁxed-point iteration, provided an equa-
tion similar to (5.4) is available. Alternatively, one may directly maximize
´
log[exp[−λrn(θ)] dπ
dρ(θ)]ρ(dθ) with respect to paramater m, using numerical
optimization routines. This approach was used for instance in Hoﬀman et al.
[2013] with combination of some stochastic gradient descent to perform infer-
ence on a latent Dirichlet allocation model. See also e.g. Emtiyaz Khan et al.
[2013]; Khan [2014] for eﬃcient algorithms for Gaussian variational approx-
imation.
In what follows (Subsections 5.4.1 and 5.4.2) we provide tight bounds for the
prevision risk of ˜ρλ. This leads to the identiﬁcation of a condition on F such that
the risk of ˜ρλ is not worse than the risk of ˆρλ. We will make this condition explicit
in various examples, using either mean ﬁeld VB or parametric approximations.
Remark 5.3.1 An useful identity, obtained by direct calculations, is: for any
ρ ≪π,
log
ˆ
exp [−λrn(θ)] π(dθ) = −λ
ˆ
rn(θ)ρ(dθ) −K(ρ, π) + K(ρ, ˆρλ).
(5.5)
128

5.4 General results
Since the left hand side does not depend on ρ, one sees that ˜ρλ, which minimises
K(ρ, ˆρλ) over F, is also the minimiser of:
˜ρλ = arg min
ρ∈F
ˆ
rn(θ)ρ(dθ) + 1
λK(ρ, π)

This equation will appear frequently in the sequel in the form of an empirical upper
bound.
5.4 General results
This section gives our general results, under either a Hoeﬀding Assumption (Deﬁ-
nition 5.2.3) or a Bernstein Assumption (Deﬁnition 5.2.4), on risks bounds for the
variational approximation, and how it relates to risks bounds for Gibbs posteri-
ors. These results will be specialised to several learning problems in the following
sections.
5.4.1 Bounds under the Hoeﬀding assumption
Empirical bounds
Theorem 5.4.1 Under the Hoeﬀding assumption (Deﬁnition 5.2.3), for any ε >
0, with probability at least 1 −ε we have simultaneously for any ρ ∈M1
+(Θ),
ˆ
Rdρ ≤
ˆ
rndρ + f(λ, n) + K(ρ, π) + log
  1
ε

λ
.
This result is a simple variant of a result in Catoni [2007] but for the sake of
completeness, its proof is given in Appendix 5.A. It gives us an upper bound on the
risk of both the pseudo-posterior (take ρ = ˆρλ) and its variational approximation
(take ρ = ˜ρλ). These bounds may be be computed from the data, and therefore
provide a simple way to evaluate the performance of the corresponding proce-
dure, in the spirit of the ﬁrst PAC-Bayesian inequalities [McAllester, 1999, 1998;
Shawe-Taylor and Williamson, 1997]. However, this bound do not provide the rate
of convergence of these estimators. For this reason, we also provide oracle-type
inequalities.
Oracle-type inequalities
Another way to use PAC-Bayesian bounds is to compare
´
Rdˆρλ to the best possi-
ble risk, thus linking this approach to oracle inequalities. This is the point of view
developed in Catoni [2004, 2007]; Dalalyan and Tsybakov [2008].
129

5 Properties of variational approximations of Gibbs posteriors
Theorem 5.4.2 Assume that the Hoeﬀding assumption is satisﬁed (Deﬁnition 5.2.3).
For any ε > 0, with probability at least 1 −ε we have simultaneously
ˆ
Rdˆρλ ≤Bλ(M1
+(Θ)) :=
inf
ρ∈M1
+(Θ)
(ˆ
Rdρ + 2f(λ, n) + K(ρ, π) + log
  2
ε

λ
)
and
ˆ
Rd˜ρλ ≤Bλ(F) := inf
ρ∈F
(ˆ
Rdρ + 2f(λ, n) + K(ρ, π) + log
  2
ε

λ
)
.
Moreover,
Bλ(F) = Bλ(M1
+(Θ)) + 2
λ inf
ρ∈F K(ρ, π λ
2 )
where we remind that πλ is deﬁned in Deﬁnition 5.2.2.
In this way, we are able to compare
´
Rdˆρλ to the best possible aggregation
procedure in M1
+(Θ) and
´
Rd˜ρλ to the best aggregation procedure in F. More
importantly, we are able to obtain explicit expressions for the right-hand side of
these inequalities in various models, and thus to obtain rates of convergence. This
will be done in the remaining sections. This leads to the second interest of this
result: if there is a λ = λ(n) that leads to Bλ(M1
+(Θ)) ≤R + sn with sn →0 for
the pseudo-posterior ˆρλ, then we only have to prove that there is a ρ ∈F such that
K(ρ, πλ)/λ ≤csn for some constant c > 0 to ensure that the VB approximation
˜ρλ also reaches the rate sn.
We will see in the following sections several examples where the approximation
does not deteriorate the rate of convergence. But ﬁrst let us show the equivalent
oracle inequality under the Bernstein assumption.
5.4.2 Bounds under the Bernstein assumption
In this context the empirical bound on the risk would depend on the minimal
achievable risk ¯rn, and cannot be computed explicitly. We give the oracle inequality
for both the Gibbs posterior and its VB approximation in the following theorem.
Theorem 5.4.3 Assume that the Bernstein assumption is satisﬁed (Deﬁnition 5.2.4).
Assume that λ > 0 satisﬁes λ −g(λ, n) > 0. Then for any ε > 0, with probability
at least 1 −ε we have simultaneously:
ˆ
Rdˆρλ −R ≤Bλ
 M1
+(Θ)

,
130

5.5 Application to classiﬁcation
ˆ
Rd˜ρλ −R ≤Bλ(F),
where, for either A = M1
+(Θ) or A = F,
Bλ(A) =
1
λ −g(λ, n) inf
ρ∈A
(
[λ + g(λ, n)]
ˆ
(R −R)dρ + 2K(ρ, π) + 2 log
2
ε
)
.
In addition,
Bλ(F) = Bλ
 M1
+(Θ)

+
2
λ −g(λ, n) inf
ρ∈F K

ρ, π λ+g(λ,n)
2

.
The main diﬀerence with Theorem 5.4.2 is that the function R(·) is replaced by
R(·) −R. This is well known way to obtain better rates of convergence.
5.5 Application to classiﬁcation
5.5.1 Preliminaries
In all this section, we assume that Y = {0, 1} and we consider linear classiﬁcation:
Θ = X = Rd, fθ(x) = 1⟨θ,x⟩≥0. We put rn(θ) = 1
n
Pn
i=1 1{fθ(Xi)̸=Yi}, R(θ) = P(Y ̸=
fθ(X)) and assume that the [(Xi, Yi)]n
i=1 are i.i.d. In this setting, it is well-known
that the Hoeﬀding assumption always holds. We state as a reminder the following
lemma.
Lemma 5.5.1 Hoeﬀding assumption (5.1) is satisﬁed with f(λ, n) = λ2/(2n).
The proof is given in Appendix 5.A for the sake of completeness.
It is also possible to prove that Bernstein assumption (5.2) holds in the case
where the so-called margin assumption of Mammen and Tsybakov is satisﬁed.
This condition we use was introduced by Tsybakov [2004] in a classiﬁcation setting,
based on a related deﬁnition in Mammen and Tsybakov [1999].
Lemma 5.5.2 Assume that Mammen and Tsybakov’s margin assumption is sat-
isﬁed: i.e. there is a constant C such that
E[(1fθ(X)̸=Y −1fθ(X)̸=Y )2] ≤C[R(θ) −R].
Then Bernstein assumption (5.2) is satisﬁed with g(λ, n) =
Cλ2
2n−λ.
131

5 Properties of variational approximations of Gibbs posteriors
Remark 5.5.1 We refer the reader to Tsybakov [2004] for a proof that
P(0 < |

θ, X

| ≤t) ≤C′t
for some constant C′ > 0 implies the margin assumption. In words, when X is
not likely to be in the region

θ, X

≃0, where points are hard to classify, then
the problem becomes easier and the classiﬁcation rate can be improved.
We propose in this context a Gaussian prior: π = Nd(0, ϑ2Id), and we consider a
VB approach based on Gaussian families. The corresponding optimization problem
is not convex, but remains feasible as we explain below.
5.5.2 Three sets of Variational Gaussian approximations
Consider the three following Gaussian families
F1 =

Φm,σ2, m ∈Rd, σ2 ∈R∗
+
	
,
F2 =

Φm,σ2, m ∈Rd, σ2 ∈(R∗
+)2	
(mean ﬁeld approximation),
F3 =

Φm,Σ, m ∈Rd, Σ ∈Sd+	
(full covariance approximation),
where Φm,σ2 is Gaussian distribution Nd(m, σ2Id), Φm,σ2 is Nd(m, diag(σ2)), and
Φm,Σ is Nd(m, Σ). Obviously, F1 ⊂F2 ⊂F3 ⊂M1
+(Θ), and
Bλ(M1
+(Θ)) ≤Bλ(F3) ≤Bλ(F2) ≤Bλ(F1).
(5.6)
Note that, for the sake of simplicity, we will use the following classical notations
in the rest of the chapter: ϕ(·) is the density of N(0, 1) w.r.t.
the Lebesgue
measure, and Φ(·) the corresponding c.d.f. The rest of Section 5.5 is organized as
follows. In Subsection 5.5.3, we calculate explicitly Bλ(F2) and Bλ(F1). Thanks
to (5.6) this also gives an upper bound on Bλ(F3) and proves the validity of the
three types of Gaussian approximations.
Then, we give details on algorithms
to compute the variational approximation based on F2 and F3, and provide a
numerical illustration on real data.
5.5.3 Theoretical analysis
We start with the empirical bound for F2 (and F1 as a consequence), which is a
direct corollary of Theorem 5.4.1.
Corollary 5.5.3 For any ε > 0, with probability at least 1 −ε we have, for any
m ∈Rd, σ2 ∈(R+)d,
ˆ
RdΦm,σ2 ≤
ˆ
rndΦm,σ2 + λ
2n +
Pd
i=1
h
1
2 log

ϑ2
σ2
i

+ σ2
i
ϑ2
i
+ ∥m∥2
ϑ2
−d
2 + log
  1
ε

λ
.
132

5.5 Application to classiﬁcation
We now want to apply Theorem 5.4.2 in this context. In order to do so, we
introduce an additional assumption.
Deﬁnition 5.5.1 We say that Assumption A1 is satisﬁed when there is a constant
c > 0 such that, for any (θ, θ′) ∈Θ2 with ∥θ∥= ∥θ′∥= 1, P(⟨X, θ⟩⟨X, θ′⟩< 0) ≤
c∥θ −θ′∥.
Note that this is not a stringent assumption. For example, it is satisﬁed as soon
as X/∥X∥has a bounded density on the unit sphere.
Corollary 5.5.4 Assume that the VB approximation is done on either F1, F2 or
F3. Take λ =
√
nd and ϑ =
1
√
d. Under Assumption A1, for any ε > 0, with
probability at least 1 −ε we have simultaneously
´
Rdˆρλ
´
Rd˜ρλ

≤R +
r
d
n log
 4ne2
+
c
√n +
r
d
4n3 + 2 log
  2
ε

√
nd
.
See the appendix for a proof. Note also that the values λ =
√
nd and ϑ =
1
√
d allow
to derive this almost optimal rate of convergence, but are not necessarily the best
choices in practice.
Remark 5.5.2 Note that Assumption A1 is not necessary to obtain oracle inequal-
ities on the risk integrated under ˆρλ. We refer the reader to Chapter 1 in Catoni
[2007] for such assumption-free bounds. However, it is clear that without this as-
sumption the shape of ˆρλ and ˜ρλ might be very diﬀerent. Thus, it seems reasonable
to require that A1 is satisﬁed for the approximation of ˆρλ by ˜ρλ to make sense.
We ﬁnally provide an application of Theorem 5.4.3. Under the additional con-
straint that the margin assumption is satisﬁed, we obtain a better rate.
Corollary 5.5.5 Assume that the VB approximation is done on either F1, F2
or F3. Under Assumption A1 (Deﬁnition 5.5.1 page 133), and under Mammen
and Tsybakov margin assumption, with λ =
2n
C+2 and ϑ > 0, for any ε > 0, with
probability at least 1 −ε,
´
Rdˆρλ
´
Rd˜ρλ

≤¯R+(C + 2)(C + 1)
2
d log n
ϑ
n
+ 2dϑ
n2 + 2
ϑ −d
ϑn + 2
n log 2
ε

+
√
d2c(2C + 1)
n
.
The prior variance optimizing the bound is ϑ = d/(d + 2 + 2d/n), this choice
or any constant instead will lead to a rate in d log(n)/n. Note that the rate d/n
is minimax-optimal in this context. This is, for example, a consequence of more
general results in Lecu´e [2007] under a general form of the the margin assumption.
See the Appendix for a proof.
133

5 Properties of variational approximations of Gibbs posteriors
5.5.4 Implementation and numerical results
For family F2 (mean ﬁeld), the variational lower bound (5.5) equals
Lλ,ϑ(m, σ) = −λ
n
n
X
i=1
Φ
 
−Yi
Xim
p
Xidiag(σ2)Xt
i
!
−mTm
2ϑ
+ 1
2
d
X
k=1

log σ2
k −σ2
k
ϑ

,
while for family F3 (full covariance), it equals
Lλ,ϑ(m, Σ) = −λ
n
n
X
i=1
Φ
 
−Yi
Xim
p
XiΣXt
i
!
−mTm
2ϑ
+ 1
2

log |Σ| −1
ϑtrΣ

.
Both functions are non-convex, but the multimodality of the latter may be more
severe due to the larger dimension of F3. To address this issue, we recommend
to use the reparametrisation of Opper and Archambeau [2009], which makes the
dimension of the latter optimisation problem O(n); see Khan [2014] for a related
approach.
In both cases, we found that deterministic annealing to be a good
approach to optimise such non-convex functions.
We refer to Appendix B for
more details on deterministic annealing and on our particular implementation.
We now compare the numerical performance of the mean ﬁeld and full covariance
VB approximations to the Gibbs posterior (as approximated by SMC, see Section
5.3.1) for the classiﬁcation of standard datasets; see Table 5.1. We also include
results for a kernel SVM (support vector machine); this comparison is not entirely
fair, since SVM is a non-linear classiﬁer, while all the other classiﬁers are linear.
Still, except for the Glass dataset, the full covariance VB approximation performs
as well or better than both SMC and SVM (while being much faster to compute,
especially compared to SMC).
Interestingly, VB outperforms SMC in certain cases. This might be due to the
fact that a VB approximation tends to be more concentrated around the mode
than the Gibbs posterior it approximates. Mean ﬁeld VB does not perform so well
on certain datasets (e.g. Indian). This may be due either to the approximation
family being too small, or to the corresponding optmisation problem being strongly
multi-modal.
5.6 Application to classiﬁcation under convexiﬁed
loss
Compared to the previous section, the advantage of convex classiﬁcation is that
the corresponding variational approximation will amount to minimising a convex
134

5.6 Application to classiﬁcation under convexiﬁed loss
Dataset
Covariates
Mean Field (F2)
Full cov. (F3)
SMC
SVM
Pima
7
31.0
21.3
22.3
30.4
Credit
60
32.0
33.6
32.0
32.0
DNA
180
23.6
23.6
23.6
20.4
SPECTF
22
08.0
06.9
08.5
10.1
Glass
10
34.6
19.6
23.3
4.7
Indian
11
48.0
25.5
26.2
26.8
Breast
10
35.1
1.1
1.1
1.7
Table 5.1: Comparison of misclassiﬁcation rates (%).
Misclassiﬁcation rates for diﬀerent datasets and for the proposed approximations of
the Gibbs posterior. The last column is the missclassiﬁcation rate given by a kernel-
SVM with radial kernel. The hyper-parameters are chosen by cross-validation.
function. This means that (a) the minimisation problem will be easier to deal
with; and (b) we will be able to compute a bound for the integrated risk after a
given number of steps of the minimisation procedure.
The setting is the same as in the previous section, except that for convenience
we now take Y = {−1, 1}, and the risk is based on the hinge loss,
rH
n (θ) = 1
n
n
X
i=1
max(0, 1 −Yi < θ, Xi >).
We will write RH for the theoretical counterpart and ¯RH for its minimum in θ.
We keep the superscript H in order to allow comparison with the risk R under
the 0 −1 loss. We assume in this section that the Xi are uniformly bounded by a
constant, |Xi| < cx. Note that we do not require an assumption of the form (A1)
to obtain the results of this section, as we rely directly on the Lipschitz continuity
of the hinge risk.
5.6.1 Theoretical Results
Contrarily to the previous section, the risk is not bounded in θ, and we must
specify a prior distribution for the Hoeﬀding assumption to hold.
Lemma 5.6.1 Under an independent Gaussian prior π such that each component
is N(0, ϑ2), and for λ <
1
cx
p n
ϑ2 and with bounded design |Xij| < cx, Hoeﬀding
assumption (5.1) is satisﬁed with f(λ, n) = λ2/(4n) −1
2 log

1 −ϑ2λ2c2
x
2n

.
135

5 Properties of variational approximations of Gibbs posteriors
The main impact of such a bound is that the prior variance cannot be taken too
big relative to λ.
Corollary 5.6.2 Assume that the VB approximation is done on either F1, F2 or
F3. Take λ =
1
cx
p n
ϑ2 and ϑ =
1
√
d. For any ε > 0, with probability at least 1 −ε
we have simultaneously
´
RHdˆρλ
´
RHd˜ρλ

≤R
H + cx
2
r
d
n log n
d + 2cx
d
n +
1
√
nd
c2
x + 1
2cx
+ 2cx log 2
ǫ

The oracle inequality in the above corollary enjoys the same rate of convergence
as the equivalent result in the preceding section. In the following we link the two
results.
Remark 5.6.1 As stated in the beginning of the section we can use the estimator
speciﬁed under the hinge loss to bound the excess risk of the 0-1 loss. We write R⋆
and RH⋆the respective risk for their corresponding Bayes classiﬁers. From Zhang
[2004] (section 3.3) we have the following inequality, linking the excess risk under
the hinge loss and the 0 −1 loss,
R(θ) −R⋆≤RH(θ) −RH⋆
for every θ ∈Rd. By integrating with respect to ˜ρH (the VB approximation on
any F1, F2, F3 of the Gibbs posterior for the hinge risk) and making use of Corol-
lary 5.6.2 we have with high probability,
˜ρH (R(θ)) −R⋆≤inf
θ∈Rp RH(θ) −RH⋆+ O
 r
d
n log
n
d
!
.
5.6.2 Numerical application
We have motivated the introduction of the hinge loss as a convex upper bound.
In the sequel we show that the resulting VB approximation also leads to a convex
optimization problem. This has the advantage of opening a range of possible opti-
mization algorithms [Nesterov, 2004]. In addition we are able to bound the error of
the approximated measure after a ﬁxed number of iterations (see Theorem 5.6.3).
Under the model F1 each individual risk is given by:
ρm,σ(ri(θ)) = (1 −Γim) Φ
1 −Γim
σ∥Γi∥2

+ σ∥Γi∥ϕ
1 −Γim
σ∥Γi∥2

:= Ξi
 m
σ

,
writting Γi := YiXi.
136

5.6 Application to classiﬁcation under convexiﬁed loss
Hence the lower bound to be maximized is given by
L(m, σ) = −λ
n
( n
X
i=1
(1 −Γim) Φ
1 −Γim
σ∥Γi∥2

+
n
X
i=1
σ∥Γi∥ϕ
1 −Γim
σ∥Γi∥2
)
−∥m∥2
2
2ϑ
+ d
2

log σ2 −ϑ
σ2

.
It is easy to see that the function is convex in (m, σ), ﬁrst note that the map
Ψ :
 x
y

7→xΦ
x
y

+ yϕ
x
y

,
is convex and note that we can write Ξi
 m
σ

= Ψ

A
 x
y

+ b

hence by
composition of convex function with linear mappings we have the result. Similar
reasoning could be held for the case F2 and F3, where in later the parametrization
should be done in C such that Σ = CCt. The bound is however not universally
Lipschitz in σ, this impacts the optimization algorithms.
On the class of function F0 =
n
Φm, 1
n, m ∈Rdo
, for which our Oracle inequalities
still hold we could get faster numerical algorithms. The objective function has
Lipschitz continuous derivatives and we would get a rate of
L
(1+k)2.
Other convex loss could be considered which could lead to convex optimization
problems. For instance one could consider the exponential loss.
Dataset
Covariates
Hinge loss
SMC
Pima
7
21.8
22.3
Credit
60
27.2
32.0
DNA
180
4.2
23.6
SPECTF
22
19.2
08.5
Glass
10
26.12
23.3
Indian
11
26.2
25.5
Breast
10
0.5
1.1
Table 5.2: Comparison of misclassiﬁcation rates (%).
Misclassiﬁcation rates for diﬀerent datasets and for the proposed approximations of
the Gibbs posterior. The hyperparameters are chosen by cross-validation. This is to
be compared to Table 5.1.
137

5 Properties of variational approximations of Gibbs posteriors
Theorem 5.6.3 Assume that the VB approximation is done on F1, F2 or F3. De-
note by ˜ρk(dθ) the VB approximated measure after the kth iteration of an optimal
convex solver using the hinge loss. Take λ =
1
cx
p n
d and ϑ =
1
√
d then under the
hypothesis of Corollary 5.6.2 with probability 1 −ǫ
ˆ
RHd˜ρk ≤R
H +
LM
√
1 + k + cx
2
r
d
n log n
d + 2cx
d
n +
1
√
nd
c2
x + 1
2cx
+ 2cx log 2
ǫ

where L is the Lipschitz coeﬃcient on a ball of radius M of the objective function
maximized in VB.
From Theorem 5.6.3 we can compute the number of iterations to get a given
level of error at a given probability.
We ﬁnd that on average the misclassiﬁcation error (Table 5.2) is lower than for
the 0-1 loss where we have no guaranties that the maximum is attained.
5.7 Application to ranking
5.7.1 Preliminaries
In this section we take Y = {0, 1} and consider again linear classiﬁers: Θ = X =
Rd, fθ(x) = 1⟨θ,x⟩≥0. We consider however a diﬀerent criterion: in ranking, not
only we want to classify well an object x, but we want to make sure that given
two diﬀerent objects, the one that is more likely to correspond to a label 1 will be
assigned a larger score through the function fθ. A usual way to measure this is to
introduce the risk function
R(θ) = P[(Y1 −Y2)(fθ(X1) −fθ(X2)) < 0]
and the empirical risk
rn(θ) =
1
n(n −1)
X
1≤i̸=j≤n
1{(Yi−Yj)(fθ(Xi)−fθ(Xj))<0}.
Then, again, we recall classical results.
Lemma 5.7.1 The Hoeﬀding-type assumption is satisﬁed with f(λ, n) =
λ2
n−1.
The variant of the margin assumption adapted to ranking was established by Robbiano
[2013] and Ridgway [2015].
Lemma 5.7.2 Assume the following margin assumption:
E[(1[fθ(X1)−fθ(X2)][Y1−Y2]<0 −1[fθ(X1)−fθ(X2)][Y1−Y2]<0)2] ≤C[R(θ) −R].
Then Bernstein assumption (5.2) is satisﬁed with g(λ, n) =
Cλ2
n−1−4λ.
138

5.7 Application to ranking
We still consider a Gaussian prior
π(dθ) =
d
Y
i=1
ϕ(θi; 0, ϑ2)dθi
and the approximation families will be the same as in Section 5.5: F1 = {Φm,σ2, m ∈
Rd, σ2 ∈R∗
+}, F2 = {Φm,σ2, m ∈Rd, σ2 ∈(R∗
+)d} and F3 = {Φm,Σ, m ∈Rd, Σ ∈
Sd+}.
5.7.2 Theoretical study
Here again, we start with the empirical bound.
Corollary 5.7.3 For any ε > 0, with probability at least 1 −ε we have, for any
m ∈Rd, σ2 ∈(R+)d,
ˆ
RdΦm,σ2 ≤
ˆ
rndΦm,σ2+
λ
n −1+
Pd
j=1
h
1
2 log

ϑ2
σ2
i

+ σ2
i
ϑ2
i
+ ∥m∥2
ϑ2
−d
2 + log
  1
ε

λ
.
In order to derive a theoretical bound, we introduce the following variant of
Assumption A1.
Deﬁnition 5.7.1 We say that Assumption A2 is satisﬁed when there is a constant
c > 0 such that, for any (θ, θ′) ∈Θ2 with ∥θ∥= ∥θ′∥= 1, P(⟨X1 −X2, θ⟩⟨X1 −X2, θ′⟩<
0) ≤c∥θ −θ′∥.
Assumption A2 is satisﬁed as soon as (X1 −X2)/∥X1 −X2∥has a bounded density
on the unit sphere.
Corollary 5.7.4 Use either F1, F2 or F3. Take λ =
q
d(n−1)
2
and ϑ = 1. Under
(A2), for any ε > 0, with probability at least 1 −ε,
´
Rdˆρλ
´
Rd˜ρλ

≤R +
r
2d
n −1

1 + 1
2 log (2d(n −1))

+
c
√
2
√n −1 + 2
√
2 log
  2e
ε

p
(n −1)d
.
Finally, under an additional margin assumption, we have:
Corollary 5.7.5 Under Assumption A2 and the margin assumption of Lemma (5.7.2),
for λ = n−1
C+5 and ϑ > 0, for any ε > 0, with probability at least 1 −ε,
´
Rdˆρλ
´
Rd˜ρλ

≤¯R+(C + 5)(C + 1)
2
d log n
ϑ
n −1 +
2dϑ
n(n −1) + 2
ϑ −
d
ϑn −1 +
2
n −1 log 2
ε

+
√
d4c(C + 1)
n
.
139

5 Properties of variational approximations of Gibbs posteriors
The prior variance optimizing the bound is ϑ = d/(d + 2 + 2d/n). The proof is
similar to the ones of Corollaries 5.5.4, 5.5.5 and 5.7.4.
As in the case of classiﬁcation, ranking under an AUC loss can be done by re-
placing the indicator function by the corresponding upper bound given by an hinge
loss. In this case we can derive similar results as for the convexiﬁed classiﬁcation
in particular we can get a convex minimization problem and obtain result without
requiring assumption (A2).
5.7.3 Algorithms and numerical results
As an illustration we focus here on family F2 (mean ﬁeld). In this case the VB
objective to maximize is given by:
L(m, σ2) = −
λ
n+n−
X
i:yi=1,j:yj=0
Φ

−
Γijm
qPd
k=1(γk
ij)2σ2
k

−∥m∥2
2
2ϑ +1
2
d
X
k=1

log σ2
k −σ2
k
ϑ

,
(5.7)
where Γij = Xi −Xj, and where (γk
ij)k are the elements of Γ.
This function is expensive to compute, as it involves n+n−terms, the computa-
tion of which is O(p).
We propose to use a stochastic gradient descent in the spirit of Hoﬀman et al.
[2013]. The model we consider is not in an exponential family, meaning we cannot
use the trick developed by these authors. We propose instead to use a standard
descent.
The idea is to replace the gradient by an unbiased version based on a batch of
size B as described in Algorithm 22 in the Appendix. Robbins and Monro [1951]
show that for a step-size (λt)t such that P
t λ2
t < ∞and P
t λt = ∞the algorithm
converges to a local optimum.
In our case we propose to sample pairs of data with replacement and use the
unbiased version of the derivative of the risk component. We use a simple gradient
descent without any curvature information. One could also use recent research on
stochastic quasi Newton-Raphson [Byrd et al., 2014].
For illustration, we consider a small dataset (Pima), and a larger one (Adult).
The latter is already quite challenging with n+n−= 193, 829, 520 pairs to compare.
In both cases with diﬀerent size of batches convergence is obtained with a few
iterations only and leads to acceptable bounds.
In Figure 5.1 we show the empirical bound on the AUC risk as a function of
the iteration of the algorithm, for several batch sizes. The bound is taken for
95% probability, the batch sizes are taken to be B = 1, 10, 20, 50 for the Pima
dataset, and 50 for the Adult dataset. The ﬁgure shows an additional feature of
VB approximation in the context of Gibbs posterior: namely the possibility of
140

5.8 Application to matrix completion
0
1
2
3
0
25
50
75
100
Iterations
Emprical Bound 95%
(a) Pima
1
2
3
0
100
200
300
Iterations
Emprical Bound 95%
(b) adult
Figure 5.1: Error bound at each iteration, stochastic descent, Pima
and Adult datasets.
Stochastic VB with ﬁxed temperature λ = 100 for Pima and λ = 1000 for adult. The left panel
shows several curves that correspond to diﬀerent batch sizes; these curves are hard to distinguish.
The right panel is for a batch size of 50.
The adult dataset has n = 32556 observation and
n+n−= 193829520 possible pairs. The convergence is obtained in order of seconds. The bounds
are the empirical bounds obtained in Corollary 5.7.3 for a probability of 95%.
computing the empirical upper bound given by Corollary 5.7.3. That is we can
check the quality of the bound at each iteration of the algorithm, or for diﬀerent
values of the hyperparameters.
5.8 Application to matrix completion
The matrix completion problem has received increasing attention recently, partly
due to spectacular theoretical results [Cand`es and Tao, 2010], and to challenging
applications like the Netﬂix challenge [Bennett and Lanning, 2007]. In the per-
spective of this chapter, the speciﬁc interest of this application is twofold. First,
this is a case where the family of approximations is not parametric, but rather of
the form (5.3), i.e. the family of products of independent components. Then, there
is no known theoretical result for the Gibbs estimator in the considered model, yet
we can still directly bound the loss induced by the variational approximation.
141

5 Properties of variational approximations of Gibbs posteriors
We observe i.i.d. pairs ((Xi, Yi))n
i=1 where Xi ∈{1, . . . , m1} × {1, . . . , m2}, and
we assume that there is a m1 × m2-matrix M such that Yi = MXi + εi and the
εi are centred. Assuming that Xi is uniform on {1, . . . , m1} × {1, . . . , m2}, that
fθ(Xi) = θXi, and taking the quadratic risk, R(θ) = E [(Yi −θXi)2], we have that
R(θ) −R =
1
m1m2
∥θ −M∥2
F
where ∥· ∥F stands for the Frobenius norm.
A common way to parametrise the problem is
Θ = {θ = UV T, U ∈Rm1×K, V ∈Rm2×K}
where K is large; e.g.
K = min(m1, m2).
Following Salakhutdinov and Mnih
[2008], we deﬁne the following prior distribution: U·,j ∼N(0, γjI), V·,j ∼N(0, γjI)
where the γj’s are i.i.d. from an inverse gamma distribution, γj ∼IΓ(a, b).
Note that VB algorithms were used in this context by Lim and Teh [2007] (with a
slightly simpler prior however: the γj’s are ﬁxed rather than random). Since then,
this prior and variants were used in several papers [e.g. Lawrence and Urtasun,
2009; Zhou et al., 2010]. Until now, no theoretical results were proved up to our
knowledge. Two papers prove minimax-optimal rates for slightly modiﬁed estima-
tors (by truncation), for which eﬃcient algorithms are unknown [Mai and Alquier,
2015; Suzuki, 2014]. However, using Theorems 5.4.2 and 5.4.3 we are able to prove
the following: if there is a PAC-Bayesian bound leading to a rate for ˆρλ in this
context, then the same rate holds for ˜ρλ. In other words: if someone proves the
conjecture that the Gibbs estimator is minimax-optimal (up to log terms) in this
context, then the VB approximation will enjoy automatically the same property.
We propose the following approximation:
F =
(
ρ(d(U, V )) =
m1
Y
i=1
ui(dUi,·)
m2
Y
j=1
vj(dVj,·)
)
.
Theorem 5.8.1 Assume that M = UV T with |Ui,k|, |Vj,k| ≤C.
Assume that
rank(M) = r so that we can assume that U·,r+1 = · · · = U·,K = V·,r+1 = · · · =
V·,K = 0 (note that the prior π does not depend on the knowledge of r though).
Choose the prior distribution on the hyper-parameters γj as inverse gamma Inv−Γ(a, b)
with b ≤1/[2β(m1 ∨m2) log(2K(m1 ∨m2))]. Then there is a constant C(a, C) such
that, for any β > 0,
inf
ρ∈F K(ρ, πβ) ≤C(a, C)

r(m1 + m2) log [βb(m1 + m2)K] + 1
β

.
142

5.9 Discussion
See the Appendix for a proof.
For instance, in Theorem 5.4.3, in classiﬁcation and ranking we had λ, λ−g(λ, n)
and λ + g(λ, n) of order O(n). In this case we would have:
2
λ −g(λ, n) inf
ρ∈F K

ρ, π λ+g(λ,n)
2

= O
C(a, C)r(m1 + m2) log [nb(m1 + m2)K]
n

,
and note that in this context it is known that the minimax rate is at least r(m1 +
m2)/n [Koltchinskii et al., 2011].
5.8.1 Algorithm
As already mentioned, the approximation family is not parametric in this case, but
rather of type mean ﬁeld. The corresponding VB algorithm amounts to iterating
equation (5.4), which takes the following form in this particular case:
uj(dUj,.) ∝exp
(
−λ
n
X
i
EV,U−j

(YXi −(UV T)Xi)2
−
K
X
k=1
Eγj
 1
2γk

U 2
jk
)
vj(dVj,.) ∝exp
(
−λ
n
X
i
EV−j,U

(YXi −(UV T)Xi)2
−
K
X
k=1
Eγj
 1
2γk

V 2
jk
)
p(γk) ∝exp
(
−1
2γk
 X
j
EUU 2
kj +
X
i
EV V 2
ik
!
+ (α + 1) log 1
γk
−β
γk
)
where the expectations are taken with respect to the thus deﬁned variational ap-
proximations.
One recognises Gaussian distributions for the ﬁrst two, and an
inverse Gamma distribution for the third. We refer to Lim and Teh [2007] for
more details on this algorithm and for a numerical illustration.
5.9 Discussion
We showed in several important scenarios that approximating a Gibbs posterior
through VB (Variational Bayes) techniques does not deteriorate the rate of conver-
gence of the corresponding procedure. We also described practical algorithms for
fast computation of these VB approximations, and provided empirical bounds that
may be computed from the data to evaluate the performance of the so-obtained
VB-approximated procedure. We believe these results provide a strong incentive
to recommend VB as the default approach to approximate Gibbs posteriors, in
lieu of Monte Carlo methods.
We hope to extend our results to other applications beyond those discussed in
this chapter, such as regression. One technical diﬃculty with regression is that
143

the risk function is not bounded, which makes our approach a bit less direct to
apply. In many papers on PAC-Bayesian bounds for regression, the noise can be
unbounded (usually, it is assumed to be sub-exponential), but one assumes that
the predictors are bounded, see e.g. Alquier and Biau [2013]. However, using the
robust loss function of Audibert and Catoni, it is possible to relax this assumption
[Audibert and Catoni, 2011; Catoni, 2012]. This requires a more technical analysis,
which we leave for further work.

Appendix
5.A Proofs
5.A.1 Preliminary remarks
We start by a general remark. Let h be a function Θ →R+ with
´
exp[−h(θ)]π(dθ) <
∞. Let us put
π[h](dθ) =
exp[−h(θ)]
´
exp[−h(θ′)]π(dθ′)π(dθ).
Direct calculation yields, for any ρ ≪π with
´
hdρ < ∞,
K(ρ, π[h]) = λ
ˆ
hdρ + K(ρ, π) + log
ˆ
exp(−h)dπ.
Two well known consequences are
π[h] = arg
min
ρ∈M1
+(Θ)
ˆ
hdρ + K(ρ, π)

,
−log
ˆ
exp(−h)dπ =
min
ρ∈M1
+(Θ)
ˆ
hdρ + K(ρ, π)

.
We will use these inequalities many times in the followings. The most frequent
application will be with h(θ) = λrn(θ) (in this case π[λrn] = ˆρλ) or h(θ) =
±λ[rn(θ) −R(θ)], the ﬁrst case leads to
K(ρ, ˆρλ) = λ
ˆ
rndρ + K(ρ, π) + log
ˆ
exp(−λrn)dπ,
(5.8)
145

5 Properties of variational approximations of Gibbs posteriors
ˆρλ = arg
min
ρ∈M1
+(Θ)

λ
ˆ
rndρ + K(ρ, π)

,
(5.9)
−log
ˆ
exp(−λrn)dπ =
min
ρ∈M1
+(Θ)

λ
ˆ
rndρ + K(ρ, π)

.
(5.10)
We will use (5.8), (5.9) and (5.10) several times in this appendix.
5.A.2 Proof of the theorems in Subsection 5.4.1
Proof of Theorem 5.4.1. This proof follows the standard PAC-Bayesian approach
(see Catoni [2007]). Apply Fubini’s theorem to the ﬁrst inequality of (5.1):
E
ˆ
exp {λ[R(θ) −rn(θ)] −f(λ, n)} π(dθ) ≤1
then apply the preliminary remark with h(θ) = λ[rn(θ) −R(θ)]:
E exp
(
sup
ρ∈M1
+(Θ)
ˆ
λ[R(θ) −rn(θ)]ρ(dθ) −K(ρ, π) −f(λ, n)
)
≤1.
Multiply both sides by ε and use E[exp(U)] ≥P(U > 0) for any U to obtain:
P
"
sup
ρ∈M1
+(Θ)
ˆ
λ[R(θ) −rn(θ)]ρ(dθ) −K(ρ, π) −f(λ, n) + log(ε) > 0
#
≤ε.
Then consider the complementary event:
P

∀ρ ∈M1
+(Θ),
λ
ˆ
Rdρ ≤λ
ˆ
rndρ + f(λ, n) + K(ρ, π) + log
1
ε

≥1 −ε.
□
Proof of Theorem 5.4.2.
Using the same calculations as above, we have, with
probability at least 1 −ε, simultaneously for all ρ ∈M1
+(Θ),
λ
ˆ
Rdρ ≤λ
ˆ
rndρ + f(λ, n) + K(ρ, π) + log
2
ε

(5.11)
λ
ˆ
rndρ ≤λ
ˆ
Rdρ + f(λ, n) + K(ρ, π) + log
2
ε

.
(5.12)
We use (5.11) with ρ = ˆρλ and (5.9) to get
λ
ˆ
Rdˆρλ ≤
inf
ρ∈M1
+(Θ)

λ
ˆ
rndρ + f(λ, n) + K(ρ, π) + log
2
ε

146

5.A Proofs
and plugging (5.12) into the right-hand side, we obtain
λ
ˆ
Rdˆρλ ≤
inf
ρ∈M1
+(Θ)

λ
ˆ
Rdρ + 2f(λ, n) + 2K(ρ, π) + 2 log
2
ε

.
Now, we work with ˜ρλ = arg minρ∈F K(ρ, ˆρλ). Plugging (5.8) into (5.11) we get,
for any ρ,
λ
ˆ
Rdρ ≤f(λ, n) + K(ρ, ˆρλ) −log
ˆ
exp(−λrn)dπ + log
2
ε

.
By deﬁnition of ˜ρλ, we have:
λ
ˆ
Rd˜ρλ ≤inf
ρ∈F

f(λ, n) + K(ρ, ˆρλ) −log
ˆ
exp(−λrn)dπ + log
2
ε

and, using (5.8) again, we obtain:
λ
ˆ
Rd˜ρλ ≤inf
ρ∈F

λ
ˆ
rndρ + f(λ, n) + K(ρ, π) + log
2
ε

.
We plug (5.12) into the right-hand side to obtain:
λ
ˆ
Rd˜ρλ ≤inf
ρ∈F

λ
ˆ
Rdρ + 2f(λ, n) + 2K(ρ, π) + 2 log
2
ε

.
This proves the second inequality of the theorem. In order to prove the claim
Bλ(F) = Bλ(M1
+(Θ)) + 2
λ inf
ρ∈F K(ρ, π λ
2 ),
note that
Bλ(F) = inf
ρ∈F
(ˆ
Rdρ + 2f(λ, n)
λ
+ 2K(ρ, π)
λ
+ 2 log
  2
ε

λ
)
= inf
ρ∈F
(
−2
λ log
ˆ
exp

−λ
2R

dπ + 2f(λ, n)
λ
+
2K(ρ, π λ
2 )
λ
+ 2 log
  2
ε

λ
)
= −2
λ log
ˆ
exp

−λ
2R

dπ + 2f(λ, n)
λ
+ 2 log
  2
ε

λ
+ 2
λ inf
ρ∈F K(ρ, π λ
2 )
= Bλ(M1
+(Θ)) + 2
λ inf
ρ∈F K(ρ, π λ
2 ).
This ends the proof. □
147

5 Properties of variational approximations of Gibbs posteriors
5.A.3 Proof of Theorem 5.4.3 (Subsection 5.4.2)
Proof of Theorem 5.4.3.
As in the proof of Theorem 5.4.1, we apply Fubini,
then (5.10) to the ﬁrst inequality of (5.2) to obtain
E exp

sup
ρ
ˆ 
λ[R(θ) −R] −λ[rn(θ) −rn] −g(λ, n)[R(θ) −R]

ρ(dθ) −K(ρ, π)

≤1
and we multiply both sides by ε/2 to get
P
(
sup
ρ
"
[λ−g(λ, n)]
ˆ
Rdρ −R

≥λ
ˆ
rndρ −rn

+K(ρ, π)+log
2
ε
#)
≤ε
2.
(5.13)
We now consider the second inequality in (5.2):
E exp

λ[rn(θ) −rn] −λ[R(θ) −R] −g(λ, n)[R(θ) −R]
	
≤1.
The same derivation leads to
P
(
sup
ρ
"
[λ−g(λ, n)]
ˆ
rndρ −rn

≥λ
ˆ
Rdρ −R

+K(ρ, π)+log
2
ε
#)
≤ε
2.
(5.14)
We combine (5.13) and (5.14) by a union bound argument, and we consider the
complementary event: with probability at least 1 −ε, simultaneously for all ρ ∈
M1
+(Θ),
[λ −g(λ, n)]
ˆ
Rdρ −R

≤λ
ˆ
rndρ −rn

+ K(ρ, π) + log
2
ε

,
(5.15)
λ
ˆ
rndρ −rn

≤[λ + g(λ, n)]
ˆ
Rdρ −R

+ K(ρ, π) + log
2
ε

.
(5.16)
We now derive consequences of these two inequalities (in other words, we focus on
the event where these two inequalities are satisﬁed). Using (5.9) in (5.15) yields
[λ −g(λ, n)]
ˆ
Rdˆρλ −R

≤
inf
ρ∈M1
+(Θ)

λ
ˆ
rndρ −rn

+ K(ρ, π) + log
2
ε

.
We plug (5.16) into the right-hand side to obtain:
[λ −g(λ, n)]
ˆ
Rdˆρλ −R

≤
inf
ρ∈M1
+(Θ)
(
[λ + g(λ, n)]
ˆ
Rdρ −R

+ 2K(ρ, π) + 2 log
2
ε
)
.
148

5.A Proofs
Now, we work with ˜ρλ. Plugging (5.8) into (5.13) we get
[λ −g(λ, n)]
ˆ
Rdρ −R

≤K(ρ, ˆρλ) −log
ˆ
exp[−λ(rn −rn)]dπ + log
2
ε

.
By deﬁnition of ˜ρλ, we have:
[λ −g(λ, n)]
ˆ
Rd˜ρλ −R

≤inf
ρ∈F

K(ρ, ˆρλ) −log
ˆ
exp[−λ(rn −rn)]dπ + log
2
ε

.
Then, apply (5.8) again to get:
[λ −g(λ, n)]
ˆ
Rd˜ρλ −R

≤inf
ρ∈F

λ
ˆ
(rn −rn)dρ + K(ρ, π) + log
2
ε

.
Plug (5.16) into the right-hand side to get
[λ −g(λ, n)]
ˆ
Rd˜ρλ −R

≤inf
ρ∈F

[λ + g(λ, n)]
ˆ
(R −R)dρ + 2K(ρ, π) + 2 log
2
ε

.
□
5.A.4 Proofs of Section 5.5
Proof of Lemma 5.5.1.
Combine Theorem 2.1 p.
25 and Lemma 2.2 p.
27
in Boucheron et al. [2013]. □
Proof of Lemma 5.5.2. Apply Theorem 2.10 in Boucheron et al. [2013], and plug
the margin assumption. □
Proof of Corollary 5.5.4. We remind that thanks to (5.6) it is enough to prove the
claim for F1. We apply Theorem 5.4.2 to get:
Bλ(F1) = inf
(m,σ2)
(ˆ
RdΦm,σ2 + λ
n + 2K(Φm,σ2, π) + log
  2
ε

λ
)
= inf
(m,σ2)



ˆ
RdΦm,σ2 + λ
n + 2
d
h
1
2 log

ϑ2
σ2

+ σ2
ϑ2
i
+ ∥m∥2
ϑ2
−d
2 + log
  2
ε

λ


.
149

5 Properties of variational approximations of Gibbs posteriors
Note that the minimizer of R, θ, is not unique (because fθ(x) does not depend on
∥θ∥) and we can chose it in such a way that ∥θ∥= 1. Then
R(θ) −R = E
h
1⟨θ,X⟩Y <0 −1⟨θ,X⟩Y <0
i
≤E
h
1⟨θ,X⟩⟨θ,X⟩<0
i
= P
 ⟨θ, X⟩

θ, X

< 0

≤c

θ
∥θ∥−θ
 ≤2c∥θ −θ∥.
So:
Bλ(F1) ≤R + inf
(m,σ2)

2c
ˆ
∥θ −θ∥Φm,σ2(dθ)
+ λ
n + 2
d
h
1
2 log

ϑ2
σ2

+ σ2
ϑ2
i
+ ∥m∥2
ϑ2
−d
2 + log
  2
ε

λ

.
We now restrict the inﬁmum to distributions ν such that m = θ:
B(F1) ≤R + inf
σ2


2c
√
dσ + λ
n +
d log

ϑ2
σ2

+ 2dσ2
ϑ2 + 2
ϑ2 −d + 2 log
  2
ε

λ


.
We put σ =
1
2λ and substitute
1
√
d for ϑ to get
B(F1) ≤R + λ
n + c
√
d + d log(4 λ2
d ) + d2
2λ2 + d + 2 log
  2
ε

λ
.
Substitute
√
nd for λ to get the desired result. □
Proof of Corollary 5.5.5. We apply Theorem 5.4.3:
ˆ
(R −R)d˜ρλ
≤inf
m,σ2
λ + g(λ, n)
λ −g(λ, n)
ˆ
(R −¯R)dΦm,σ2 +
1
λ −g(λ, n)

2K(Φm,σ2, π) + 2 log 2
ǫ

where λ <
2n
C+1. Computations similar to those in the the proof of Corollary 5.5.4
lead to
ˆ
Rd˜ρλ ≤R + inf
m,σ2
(
2cλ + g(λ, n)
λ −g(λ, n)
ˆ
∥θ −θ∥Φm,σ2(dθ)
+ 2
Pd
j=1
h
1
2 log

ϑ2
σ2

+ σ2
ϑ2
i
+ ∥m∥2
ϑ2
−d
2 + log
  2
ε

λ −g(λ, n)
)
.
taking m = ¯θ and λ =
2n
C+2, we get the result. □
150

5.A Proofs
5.A.5 Proofs of Section 5.6
Proof of Lemma 5.6.1. For ﬁxed θ we can upper bound the individual risk such
that:
0 ≤max(0, 1−< θ, Xi > Yi) ≤1 + | < θ, Xi > |
such that we can apply Hoeﬀding’s inequality conditionally on Xi and ﬁxed θ.
We get,
E

exp
 λ(RH −rH
n )

|X1, · · · , Xn

≤exp
(
λ2
8n2
n
X
i=1
(1 + | < θ, Xi > |)2
)
≤exp
 λ2
4n + λ2c2
x
4n ∥θ∥2

where the last inequality stems from the fact that (a + b)2 ≤2 (a2 + b2) and the
fact that we have supposed the Xi to be bounded. We can take the expectation
of this term with respect to the Xi’s and with respect to our Gaussian prior.
π

E

exp
 λ(RH −rH
n )
	
≤
exp

λ2
4n

(2π)
d
2√
ϑ2
ˆ
exp
λ2c2
x
4n ∥θ∥2 −
1
2ϑ2∥θ∥2

dθ
≤
exp

λ2
4n

(2π)
d
2√
ϑ2
ˆ
exp

−1
2
 1
ϑ2 −λ2c2
x
2n

∥θ∥2

dθ
The integral is a properly deﬁned Gaussian integral under the hypothesis that
1
ϑ2 −λ2c2
x
2n > 0 hence λ <
1
cx
q
n
ϑ
2. The integral is proportional to a Gaussian and
we can directly write:
π

E

exp
 λ(RH −rH
n )
	
≤
exp

λ2
4n

q
1 −ϑ2λ2c2x
2n
writing everything in the exponential gives the desired result. □
Proof of Corollary 5.6.2. We apply Theorem 5.4.2 to get:
Bλ(F1) = inf
(m,σ2)
(ˆ
RHdΦm,σ2 + λ
2n −1
λ log

1 −ϑ2λ2c2
x
2n

+ 2K(Φm,σ2, π) + log
  2
ε

λ
)
= inf
(m,σ2)



ˆ
RHdΦm,σ2 + 2
Pd
j=1
h
1
2 log

ϑ2
σ2

+ σ2
ϑ2
i
+ ∥m∥2
ϑ2
−d
2 + log
  2
ε

λ


+
151

5 Properties of variational approximations of Gibbs posteriors
λ
2n −1
λ log

1 −ϑλ2c2
x
2n

.
We use the fact that the hinge loss is Lipschitz and that the (Xi) are uniformly
bounded ∥X∥∞< cx. We get RH(θ) ≤¯RH +cx
√
d∥θ−¯θ∥and restrict the inﬁmum
to distributions ν such that m = θ:
B(F1) ≤R
H+
inf
σ2


cxdσ2 + λ
2n −1
λ log

1 −ϑ2λ2c2
x
2n

+
d log

ϑ2
σ2

+ 2dσ2
ϑ2 + 2
ϑ2 −d + 2 log
  2
ε

λ


.
We specify σ2 =
1
√
dn and λ =
1
cx
p n
ϑ2 such that we get:
B(F1) ≤RH + cx
r
d
n+
√
ϑ2
2cx
√n −cx
r
ϑ2
n log

1 −1
2

+dcxϑ
√n log

ϑ2√
nd

+cxϑ
2d
nϑ2 + 2
ϑ2 −d + 2 log
  2
ε

√n
.
To get the correct rate we take the prior variance to be ϑ2 = 1
d by replacing in the
above equation we get the desired result.
□
Proof of Theorem 5.6.3. From Nesterov [2004] (th. 3.2.2) we have the following
bound on the objective function minimized by VB, (the objective is not uniformly
Lipschitz)
ρk(rH
n ) + 1
λK(ρk, π) −inf
ρ∈F1

ρ(rH
n ) + 1
λK(ρ, π)

≤
LM
√
1 + k.
(5.17)
where the initial point was taken in a ball of radius M around ˜ρ.
We have from equation (5.11) speciﬁed for measures ρk probability 1 −ε,
λ
ˆ
RHdρk ≤λ
ˆ
rH
n dρk + f(λ, n) + K(ρk, π) + log
1
ε

Combining the two equations yields,
ˆ
RHdρk ≤
LM
√
1 + k + 1
λf(n, λ) + inf
ρ∈F1

ρ(rH
n ) + 1
λK(ρ, π)

+ 1
λ log 1
ε
We can therefore write for any ρ ∈F1,
ˆ
RHdρk ≤
LM
√
1 + k + 1
λf(n, λ) + ρ(rH
n ) + 1
λK(ρ, π) + 1
λ log 1
ε
152

5.A Proofs
Using equation (5.11) a second time we get with probability 1 −ε
ˆ
RHdρk ≤
LM
√
1 + k + 2
λf(n, λ) + ρ(RH) + 2
λK(ρ, π) + 2
λ log 2
ε
Because this is true for any ρ ∈F1 in 1−ε we can write the bound for the smallest
measure in F1.
ˆ
RHdρk ≤
LM
√
1 + k + 2
λf(n, λ) + inf
ρ∈F1

ρ(RH) + 2
λK(ρ, π)

+ 2
λ log 2
ε
By taking the Gaussian measure with variance
1
dn and mean θ in the inﬁmum and
taking λ =
1
cx
√
nd and ϑ = 1
d, we can plug in the result of Corrolary 5.6.2 to get
the result.□
5.A.6 Proofs of Section 5.7
Proof of Lemma 5.7.1. The idea of the proof is to use Hoeﬀding’s decomposition
of U-statistics combined with Hoeﬀding’s inequality for iid random variables. This
was done in ranking by Cl´emen¸con et al. [2008a], and later in Ridgway [2015];
Robbiano [2013] for ranking via aggregation and Bayesian statistics. The proof is
as follows: we deﬁne
qθ
i,j = 1(Yi−Yj)(fθ(Xi)−fθ(Xj))<0 −R(θ)
so that
Un :=
1
n(n −1)
X
i,j
qθ
i,j = rn(θ) −R(Θ).
From Hoeﬀding [1948] we have
Un = 1
n!
X
π
1
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)
where the sum is taken over all the permutations π of {1, . . . , n}. Jensen’s inequal-
ity leads to
E exp[λUn] = E exp

λ 1
n!
X
π
1
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)


≤1
n!
X
π
E exp

λ
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)

.
153

5 Properties of variational approximations of Gibbs posteriors
We now use, for each of the terms in the sum the same argument as in the proof
of Lemma 5.5.1 to get
E exp[λUn] ≤1
n!
X
π
exp
 λ2
2⌊n
2⌋

≤exp
 λ2
n −1

(in the last step, we used ⌊n
2⌋≥(n −1)/2). We proceed in the same way to upper
bound E exp[−λUn]. □
Proof of Lemma 5.7.2. As already done above, we use Bernstein inequality and
Hoeﬀding decomposition. Fix θ. We deﬁne this time
qθ
i,j = 1{⟨θ, Xi −Xj⟩(Yi −Yj) < 0} −1{[σ(Xi) −σ(Xj)](Yi −Yj) < 0} −R(θ) + R
so that
Un := rn(θ) −rn −R(θ) + R =
1
n(n −1)
X
i̸=j
qθ
i,j.
Then,
Un = 1
n!
X
π
1
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋).
Jensen’s inequality:
E exp[λUn] = E exp

λ 1
n!
X
π
1
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)


≤1
n!
X
π
E exp

λ
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)

.
Then, for each of the terms in the sum, use Bernstein’s inequality:
E exp

λ
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)

≤exp

E((qθ
π(1),π(1+⌊n
2 ⌋))2) λ2
⌊n
2 ⌋
2

1 −2 λ
⌊n
2 ⌋


.
We use again ⌊n
2⌋≥(n −1)/2.
Then, as the pairs (Xi, Yi) are iid, we have
E((qθ
π(1),π(1+⌊n
2 ⌋))2) = E((qθ
1,2)2) and then E((qθ
1,2)2) ≤C[R(θ) −R] thanks to the
margin assumption. So
E exp

λ
⌊n
2⌋
⌊n
2 ⌋
X
i=1
qθ
π(i),π(i+⌊n
2 ⌋)

≤exp
"
C[R(θ) −R] λ2
n−1
 1 −
4λ
n−1

#
.
154

5.A Proofs
This ends the proof of the proposition. □
Proof of Corollary 5.7.4. The calculations are similar to the ones in the proof of
Corollary 5.5.4 so we don’t give the details. Note that when we reach
Bλ(F1) ≤R +
2λ
n −1 + c
√
d + d log(2λ) + 2 log
  2e
ε

λ
,
an approximate minimization with respect to λ leads to the choice λ =
q
d(n−1)
2
.
□
5.A.7 Proofs of Section 5.8
Proof. First, note that, for any ρ,
K(ρ, πβ) = β
ˆ
(R −R)dρ + K(ρ, π) + log
ˆ
exp

−β(R −R)

dπ
≤β
ˆ
(R −R)dρ + K(ρ, π).
Now, we deﬁne a subset of F that will be used for the calculation of the bound.
We deﬁne for δ > 0 the probability distribution ρU,V,δ(dθ) as π conditioned to
θ = µνT with µ is uniform on {∀(i, ℓ), |µi,ℓ−Ui,ℓ| ≤δ} and ν is uniform on
{∀(j, ℓ), |νi,ℓ−Vj,ℓ| ≤δ}. Note that
ˆ
(R −R)dρM,N,δ =
ˆ
E((θX −MX)2)ρU,V,δ(dθ)
≤
ˆ
3E(((UV T)X −MX)2)ρU,V,δ(d(µ, ν))
+ 3
ˆ
E(((UνT)X −(UV T)X)2)ρU,V,δ(d(µ, ν))
+ 3
ˆ
E(((µνT)X −(UνT)X)2)ρU,V,δ(d(µ, ν)).
By deﬁnition, the ﬁrst term is = 0. Moreover:
ˆ
E(((UνT)X −(UV T)X)2)ρU,V,δ(d(µ, ν))
=
ˆ
1
m1m2
X
i,j
"X
k
Ui,k(νj,k −Vj,k)
#2
ρU,V,δ(d(µ, ν))
≤
ˆ
1
m1m2
X
i,j
"X
k
U 2
i,k
# "X
k
(νj,k −Vj,k)2
#
ρU,V,δ(d(µ, ν))
155

5 Properties of variational approximations of Gibbs posteriors
≤KrC2δ2.
In the same way,
ˆ
E(((µνT)X −(UνT)X)2)ρU,V,δ(d(µ, ν)) ≤
ˆ
∥µ −U∥2
F∥ν∥2
FρU,V,δ(d(µ, ν))
≤Kr(C + δ)2δ2.
So:
ˆ
(R −R)dρM,N,δ ≤2Krδ2(C + δ2).
Now, let us consider the term K(ρU,V,δ, π). An explicit calculation is possible but
tedious. Instead, we might just introduce the set Gδ = {θ = µνT, ∥µ −U∥F ≤
δ, ∥ν −V ∥F ≤δ} and note that K(ρU,V,δ, π) ≤log
1
π(Gδ). An upper bound for Gδ
is calculated page 317-320 in Alquier [2014] and the result is given by (10) in this
reference:
K(ρU,V,δ, π) ≤4δ2 + 2∥U∥2
F + 2∥N∥2
F + 2 log(2)
+ (m1 + m2)r log
 
1
δ
r
3π(m1 ∨m2)K
4
!
+ 2K log
Γ(a)3a+1 exp(2)
ba+12a

as soon as the restriction b ≤
δ2
2m1K log(2m1K),
δ2
2m2K log(2m2K) is satisﬁed.
So we
obtain:
K(ρU,V,δ, πβ) ≤β2Krδ2(C + δ2) + 4δ2 + 2∥U∥2
F + 2∥N∥2
F + 2 log(2)
+ (m1 + m2)r log
 
1
δ
r
3π(m1 ∨m2)K
4
!
+ 2K log
Γ(a)3a+1 exp(2)
ba+12a

.
Note that ∥U∥2
F ≤C2rm1, ∥V ∥2
F ≤C2rm2 and K ≤m1+m2 so it is clear that the
choice δ =
q
1
β and b ≤
1
2β(m1∨m2) log(2K(m1∨m2)) leads to the existence of a constant
C(a, C) such that
K(ρU,V,δ, πβ) ≤C(a, C)

r(m1 + m2) log [βb(m1 + m2)K] + 1
β

.
□
5.B Implementation details
5.B.1 Sequential Monte Carlo
Tempering SMC approximates iteratively a sequence of distribution ρλt, with
ρλt(dθ) = 1
Zt
exp (−λtrn(θ)) π(dθ),
156

5.B Implementation details
and temperature ladder λ0 = 0 < . . . < λT = λ. The pseudo code below is given
for an adaptive sequence of temperatures.
Algorithm 19 Tempering SMC
Input N (number of particles), τ ∈(0, 1) (ESS threshold), κ > 0 (random walk
tuning parameter)
Init. Sample θi
0 ∼πξ(θ) for i = 1 to N, set t ←1, λ0 = 0, Z0 = 1.
Loop a. Solve in λt the equation
{PN
i=1 wt(θi
t−1)}2
PN
i=1{wt(θi
t−1))2}
= τN,
wt(θ) = exp[−(λt −λt−1)rn(θ)]
(5.18)
using bisection search. If λt ≥λT, set ZT = Zt−1 ×
n
1
N
PN
i=1 wt(θi
t−1)
o
,
and stop.
b. Resample: for i = 1 to N, draw Ai
t in 1, . . . , N so that P(Ai
t = j) =
wt(θj
t−1)/ PN
k=1 wt(θk
t−1); see Algorithm 20 in the appendix.
c. Sample θi
t ∼Mt(θ
Ai
t
t−1, dθ) for i = 1 to N where Mt is a MCMC kernel that
leaves invariant πt; see comments below.
d. Set Zt = Zt−1 ×
n
1
N
PN
i=1 wt(θi
t−1)
o
.
The algorithm outputs a weighted sample (wi
T, θi
T) approximately distributed as
target posterior, and an unbiased estimator of the normalizing constant ZλT .
Step b. of algorithm 5.B.1 depends of a resampling algorithm. We choose to
use Systematic resampling, described in Algorithm 20.
157

5 Properties of variational approximations of Gibbs posteriors
Algorithm 20 Systematic resampling
Input: Normalised weights W j
t := wt(θj
t−1)/ PN
i=1 wt(θi
t−1).
Output: indices Ai ∈{1, . . . , N}, for i = 1, . . . , N.
a. Sample U ∼U([0, 1]).
b. Compute cumulative weights as Cn = Pn
m=1 NW m.
c. Set s ←U, m ←1.
d. For n = 1 : N
While Cm < s do m ←m + 1.
An ←m, and s ←s + 1.
End For
For the MCMC step, we used a Gaussian random-walk Metropolis kernel, with
a covariance matrix for the random step that is proportional to the empirical
covariance matrix of the current set of simulations.
5.B.2 Optimizing the bound
A natural idea to ﬁnd a global optimum of the objective is to try to solve a sequence
of local optimization problems with increasing inverse temperatures. For inverse
temperature λ = 0 the problem can be solved exactly (as a KL divergence be-
tween two Gaussians). Then, for two consecutive temperatures, the corresponding
solutions should be close enough.
This idea has been coined under several names. It has a long history in varia-
tional Bayes litterature under the name deterministic annealing. Yuille uses it on
mean ﬁeld on Gibbs distribution for Markov random ﬁelds. In addition the inter-
mediate results can be of interest in our case for selecting the temperature. One
can compute the bound at almost no additional cost as a function of the current
risk. In turns this can be used to monitor the bound.
158

5.B Implementation details
Algorithm 21 Deterministic annealing
Input (λt)t∈[0,T] a sequence of inverse temperature
Init. Set m = 0 and Σ = ϑId, the values minimizing the KL-divergence for λ = 0
Loop t=1,. . . ,T
a. mλt, Σλt = Minimize Lλt(m, Σ) using some local optimization routine with
initial points mλt−1, Σλt−1
b. Break if the empirical bound increases.
End Loop
 γ = 0
●
●
 γ = 125
●
●
 γ = 250
●
●
 γ = 375
●
●
 γ = 500
●
●
0
100
200
300
400
−10
−5
0
5
value
(a) A one dimensional problem
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.00
0.25
0.50
0.75
1.00
1.25
0
100
200
300
400
500
 λ
95% bound
(b) Empirical bound
Figure 5.B.1: Deterministic annealing on a Pima Indians with one
covariate and full model resp.
The right panel gives the empirical bound obtained for the DA method (in red) and the dot are
direct global optimization based on L-BFGS algorithms from starting values drawn from the prior.
Each optimization problem is repeated 20 times.
We ﬁnd that using a deterministic annealing algorithm with a limited amount of
steps helps in ﬁnding a high enough optimum. On the left panel of Figure 5.B.1,
159

5 Properties of variational approximations of Gibbs posteriors
we can see the one dimensional case where the initial problem γ = 0 corresponds
to a convex minimization problem and where the increasing temperature gradually
complexiﬁes the optimization problem. Figure 5.B.1 shows that the solution given
by DA is in average lower than randomly initialized optimization.
5.C Stochastic gradient descent
The stochastic gradient descent algorithm used in Section 5.7 is described as Al-
gorithm 22.
Algorithm 22 Stochastic Gradient Descent
Input B a batch size, an unbiased estimator of the gradient ˆ∇Bf, η ∈(0, 1) and c
While ¬converged
a. xt+1 = xt −λt ˆ∇Bf(xt)
b. Update λt+1 =
1
(t+c)η
End Loop
In all our experiment we take c = 1 and η = 0.9.
160

6
Towards the automatic calibration of the number of
particles in SMC2
This is joint work with Nicolas Chopin, Mathieu Gerber and
Omiros Papaspiliopoulos
Status: To be presented at SYSID 2015
6.1 Introduction
Consider a state-space model, with parameter θ ∈Θ, latent Markov process
(xt)t≥0, and observed process (yt)t≥0, taking values respectively in X and Y. The
model is deﬁned through the following probability densities: θ has prior p(θ),
(xt)t≥0 has initial law µθ(x0) and Markov transition f X
θ (xt|xt−1), and the yt’s are
conditionally independent, given the xt’s, with density f Y
θ (yt|xt). Sequential anal-
ysis of such a model amounts to computing recursively (in t) the posterior distri-
butions
p(θ, x0:t|y0:t) = p(θ)µθ(x0)
p(y0:t)
( tY
s=1
f X
θ (xs|xs−1)
) ( tY
s=0
f Y
θ (ys|xs)
)
or some of its marginals (e.g. p(θ|y0:t)); the normalising constant p(y0:t) of the
above density is the marginal likelihood (evidence) of the data observed up to
time t.
For a ﬁxed θ, the standard approach to sequential analysis of state-space models
is particle ﬁltering: one propagates Nx particles in X over time through muta-
tion steps (based on proposal distribution qt,θ(xt|xt−1) at time t) and resampling
steps; see Algorithm 23. Note the conventions: 1 : Nx denotes the set of integers
161

6 Towards the automatic calibration of the number of particles in SMC2
{1, . . . , Nx}, y0:t is (y0, . . . , yt), x1:Nx
t
= (x1
t, . . . , xNx
t ), x1:Nx
0:t
= (x1:Nx
0
, . . . , x1:Nx
t
),
and so on.
Algorithm 23 Particle ﬁlter (PF, for ﬁxed θ)
Operations involving superscript n must be performed for all n ∈1 : Nx.
At time 0:
(a) Sample xn
0 ∼q0,θ(x0).
(b) Compute weights
w0,θ(xn
0) = µθ(xn
0)f Y (y0|xn
0)
q0,θ(xn
0)
normalised weights, W n
0,θ = w0,θ(xn
0)/ PNx
i=1 w0,θ(xi
0), and incremental likeli-
hood estimate
ˆℓ0(θ) = N −1
x
PNx
n=1 wn
0,θ.
Recursively, from time t = 1 to time t = T:
(a) Sample an
t ∼M(W 1:Nx
t−1,θ), the multinomial distribution which generates value
i ∈1 : Nx with probability W i
t−1,θ.
(b) Sample xn
t ∼qt,θ(·|x
an
t
t−1).
(c) Compute weights
wt,θ(x
an
t
t−1, xn
t ) = f X(xn
t |x
an
t
t−1)f Y (yt|xn
t )
qt,θ(xn
t |x
an
t
t−1)
W n
t,θ =
wt,θ(x
an
t
t−1, xn
t )
PNx
i=1 wt,θ(x
ai
t
t−1, xi
t)
and incremental likelihood estimate
ˆℓt(θ) = N −1
x
PNx
n=1 wt,θ(x
an
t
t−1, xn
t ).
The output of Algorithm 23 may be used in diﬀerent ways: at time t, the
quantity PNx
n=1 W n
t,θϕ(xn
t ) is a consistent (as Nx →+∞) estimator of the ﬁltering
expectation E[ϕ(xt)|y0:t, θ]; In addition, ˆℓt(θ) is an unbiased estimator of incre-
mental likelihood p(yt|y0:t−1, θ), and Qt
s=0 ˆℓ(θ) is an unbiased estimator of the full
likelihood p(y0:t|θ) [Del Moral, 1996b, Lemma 3].
In order to perform joint inference on parameter θ and state variables, Chopin et al.
162

6.2 Background on SMC2
[2013a] derived the SMC2 sampler, that is, a SMC (Sequential Monte Carlo) algo-
rithm in θ−space, which generates and propagates Nθ values θm in Θ, and which,
for each θm, runs a particle ﬁlter (i.e. Algorithm 23) for θ = θm, of size Nx. One
issue however is how to choose Nx: if too big, then CPU time is wasted, while if
taken too small, then the performance of the algorithm deteriorates. Chopin et al.
[2013a] give formal results (adapted from Andrieu et al. [2010b]) that suggest that
Nx should grow at a linear rate during the course of the algorithm. They also
propose a practical method for increasing Nx adaptively, based on an importance
sampling step where the Nθ particle systems, of size Nx, are replaced by new
particle systems of size N new
x
. But this importance sampling step increases the
degeneracy of the weights, which in return may lead to more frequent resampling
steps, which are expensive. In this chapter, we derive an alternative way to in-
crease Nx adaptively, which is not based on importance sampling, but rather on a
CSMC (conditional Sequential Monte Carlo) update, which is less CPU intensive.
6.2 Background on SMC2
6.2.1 IBIS
To explain SMC2, we ﬁrst recall the structure of the IBIS algorithm [Chopin,
2002b] as Algorithm 24. For a model with parameter θ ∈Θ, prior p(θ), data y0:T,
and incremental likelihood p(yt|y0:t−1, θ), IBIS provides at each iteration t an ap-
proximation of partial posterior p(θ|y0:t). In practice, IBIS samples Nθ particles θm
from the prior, then perfoms sequential importance sampling steps, from p(θ|y0:t−1)
to p(θ|y0:t) using incremental weight p(θ|y0:t)/p(θ|y0:t−1) ∝p(yt|y0:t−1, θ).
To avoid weight degeneracy, one performs a resample-move step (described as
Step (b) in Algorithm 24). When the ESS (eﬀective sample size) of the weights,
computed as:
ESS(ω1:Nθ) = (PNθ
m=1 ωm)2
PNθ
m=1(ωm)2 ∈[1, N]
goes below some threshold ESSmin (e.g. N/2), the θm’s are resampled, then moved
according to some Markov kernel Kt that leaves invariant the current target of the
algorithm, p(θ|y0:t). This resample-move step re-introduces diversity among the
θ-particles.
A convenient default choice for Kt is several iterations of random-walk Metropo-
lis, with the random step calibrated to the spread of the current particle population
(i.e. variance of random step equals some fraction of the covariance matrix of the
resampled particles).
The main limitation of IBIS is that it requires evaluating the likelihood incre-
ment p(yt|y0:t−1, θ), which is typically intractable for state-space models. On the
163

6 Towards the automatic calibration of the number of particles in SMC2
Algorithm 24 IBIS
Operations involving superscript m must be performed for all m ∈1 : Nθ.
(Init) Sample θm ∼p(θ), set ωm ←1.
From time t = 0 to time t = T, do
(a) Update importance weights
ωm ←ωm × p(yt|y0:t−1, θ).
(b) If ESS(ω1:Nθ) ≤ESSmin, sample (for all m) ˜θm from mixture
1
PNθ
m=1 ωm
Nθ
X
m=1
ωmKt(θm, dθ),
where Kt is a Markov kernel with invariant distribution p(θ|y0:t); ﬁnally reset
particle system to
θ1:Nθ ←˜θ1:Nθ,
ω1:Nθ ←(1, . . . , 1).
other hand, we have seen that this quantity may be estimated unbiasedly by par-
ticle ﬁltering. This suggests combining IBIS (i.e. SMC in the θ-dimension) with
particle ﬁltering (i.e. SMC in the xt−dimension), as done in the SMC2 algorithm.
6.2.2 SMC2
The general structure of SMC2 is recalled as Algorithm 25. Essentially, one recog-
nises the IBIS algorithm, where the intractable incremental weight p(yt|y0:t−1, θm)
has been replaced by the unbiased estimate ˆℓt(θm). This estimate is obtained from
a PF run for θ = θm; thus Nθ PFs are run in parallel. Denote (x1:Nx,m
0:t
, a1:Nx,m
1:t
)
the random variables generated by the PF associated to θm.
This ‘double-layer’ structure suggests that SMC2 suﬀers from two levels of ap-
proximation, and as such that it requires both Nx →+∞and Nθ →+∞to
converge. It turns out however that SMC2 is valid for any ﬁxed value of Nx; that
is, for any ﬁxed Nx ≥1, it converges as Nθ →+∞.
This property is intuitive in the simpliﬁed case when resampling-move steps
are never triggered (i.e. take ESSmin = 0). Then SMC2 collapses to importance
164

6.2 Background on SMC2
Algorithm 25 SMC2
Operations involving superscript m must be performed for all m ∈1 : Nθ.
(Init) Sample θm ∼p(θ), set ωm ←1.
From time t = 0 to time t = T, do
(a) For each θm, run iteration t of Algorithm 23, so as to obtain (x1:Nx,m
0:t
, a1:Nx,m
1:t
),
and ˆℓt(θm).
(b) Update weights
ωm ←ωm × ˆℓt(θm).
(c) If ESS(ω1:Nθ) ≤ESSmin, sample (for all m) (˜θm, ˜x1:Nx,m
0:t
, ˜a1:Nx,m
1:t
) from mixture
1
PNθ
m=1 ωm
Nθ
X
m=1
ωmKt

(θm, x1:Nx,m
0:t
, a1:Nx,m
1:t
), d·

,
where Kt is a PMCMC kernel with invariant distribution πt(θ, x1:Nx
0:t , a1:Nx
1:t )
(see text); ﬁnally reset particle system to
(θm, x1:Nx,m
0:t
, a1:Nx,m
1:t
) ←(˜θm, ˜x1:Nx,m
0:t
, ˜a1:Nx,m
1:t
)
and ωm ←1, for all m.
sampling, with weights replaced by unbiased estimates, and it is easy to show
convergence from ﬁrst principles.
We now give a brief outline of the formal justiﬁcation of SMC2 for ﬁxed Nx, and
refer to Chopin et al. [2013a] for more details. SMC2 may be formalised as a SMC
sampler for the sequence of extended distributions:
πt(θ, x1:Nx
0:t , a1:Nx
1:t ) = p(θ)
p(y0:t)ψt,θ(x1:Nx
0:t , a1:Nx
1:t )
tY
s=0
ˆℓs(θ)
where ψt,θ denotes the joint pdf of the random variables generated by a PF up
to time t (for parameter θ), and ˆℓs(θ) denotes the unbiased estimate of the like-
lihood increment computed from that PF, ˆℓ0(θ) = N −1
x
PN
n=1 w0(xn
0), ˆℓs(θ) =
N −1
x
PN
n=1 ws,θ(x
an
t
s−1, xn
s) for s > 0; i.e. ˆℓs(θ) is actually a function of (θ, x1:Nx
0:s , a1:Nx
1:s ).
One recognises in πt the type of extended target distribution simulated by PM-
CMC (Particle MCMC, Andrieu et al. [2010b]) algorithms. Note πt is a proper
165

6 Towards the automatic calibration of the number of particles in SMC2
probability density (it integrates to one), and that the marginal distribution of
θ is p(θ|y0:t). These two properties are easily deduced from the unbiasedness of
Qt
s=0 ˆℓs(θ) (as an estimator of p(y0:t|θ)). In addition,
πt(θ, x1:Nx
0:t , a1:Nx
1:t ) = πt−1(θ, x1:Nx
0:t−1, a1:Nx
1:t−1) ψt,θ(x1:Nx
0:t , a1:Nx
1:t )
ψt−1,θ(x1:Nx
0:t−1, a1:Nx
1:t−1)
ˆℓt(θ)
where one recognises in the second factor the distribution of the variables gener-
ated by a PF at time t, conditional on those variables generated up to time t −1.
Thus, the equation above justiﬁes both Step (a) of Algorithm 25, where the par-
ticle ﬁlters are extended from time t −1 to t, and Step (b), where the particles
(θm, x1:Nx,m
0:t
, a1:Nx,m
1:t
) are reweighted by ˆℓt(θm).
We describe in the following section PMCMC moves that may be used in Step
(c). Before, we note that a naive implementation of SMC2 has a O(tNxNθ) memory
cost at time t, as one must stores in memory (θm, x1:Nx,m
0:t
, a1:Nx,m
1:t
) for each m ∈1 :
Nθ. This memory cost may be substantial even on a modern computer.
6.2.3 PMCMC moves
To make more explicit the dependence of the unbiased estimate of the likelihood
on the variables generated during the course of PF, deﬁne
Lt(θ, x1:Nx
0:t , a1:Nx
1:t ) =
tY
s=0
ˆℓs(θ) =
(
1
Nx
Nx
X
n=1
w0,θ(xn
0)
)
tY
s=1
(
1
Nx
Nx
X
n=1
ws,θ(xan
s
s−1, xn
s)
)
.
The PMMH (Particle Markov Metropolis-Hastings) kernel, described as Algo-
rithm 26, may be described informally as a Metropolis step in θ-space, where
the likelihood of both the current value and the proposed value have been re-
placed by unbiased estimators. Formally, as proven in Andrieu et al. [2010b], it
is in fact a standard Metropolis step with respect to the extended distribution
πt(θ, x1:Nx
0:t , a1:Nx
1:t ); in particular it leaves invariant p(θ|y0:t). (For convenience, our
description of PMMH assumes a random walk proposal, but PMMH is not re-
stricted to this kind of proposal.)
In practice, we set Σt, the covariance matrix of the proposal, to a fraction of the
covariance matrix of the resampled θ-particles.
One advantage of using PMHMH within SMC2 is that it does not require storing
all the variables generated by the Nθ PFs: operations at time t > 0 require only
having access to, for each m, (θm, x1:Nx,m
t−1
, a1:Nx,m
t−1
) and Lt−1(θm, x1:Nx,m
0:t−1 , a1:Nx,m
1:t
),
which is computed recursively. Memory cost then reduces to O(NθNx).
The Particle Gibbs approach is an alternative PMCMC step, based on the fol-
lowing property of target πt: if one extends πt with random index k, such that
166

6.2 Background on SMC2
Algorithm 26 Random walk PMMH update
Input: (θ, x1:Nx
0:t , a1:Nx
1:t )
Output: (˜θ, ˜x1:Nx
0:t , ˜a1:Nx
1:t )
1. θ⋆= θ + z, z ∼N(0, Σt).
2. Generate PF (Algorithm 23) for parameter θ⋆; let (x1:Nx,⋆
0:t
, a1:Nx,⋆
1:t
) the output.
3. With probability 1 ∧r,
r = p(θ⋆)Lt(θ⋆, x1:Nx,⋆
0:t
, a1:Nx,⋆
1:t
)
p(θ)Lt(θ, x1:Nx
0:t , a1:Nx
1:t )
let (˜θ, ˜x1:Nx
0:t , ˜a1:Nx
1:t )
←
(θ⋆, x1:Nx,⋆
0:t
, a1:Nx,⋆
1:t
);
otherwise (˜θ, ˜x1:Nx
0:t , ˜a1:Nx
1:t )
←
(θ, x1:Nx
0:t , a1:Nx
1:t ).
k ∈1 : Nx, and k ∼M(W 1:Nx
T
), the normalised weights at the ﬁnal iteration,
then (a) the selected trajectory, together with θ, follow the posterior distribution
p(θ, x0:t|y0:t); and (b) the remaining arguments of πt follow a CSMC (conditional
SMC) distribution, which corresponds to the distribution of the random variables
generated by a PF, but conditional on one trajectory ﬁxed to the selected trajec-
tory; see Algorithm 27.
In contrast with PMMH, implementing particle Gibbs steps within SMC2 re-
quires having access to all the variables (θm, x1:Nx,m
0:t
, a1:Nx,m
1:t
) at time t, which as
we have already discussed, might incur too big a memory cost.
6.2.4 Choosing Nx
Andrieu et al. [2010b] show that, in order to obtain reasonable performance for
PMMH, one should take Nx = O(t). Andrieu et al. [2013] show a similar result
for Particle Gibbs.
In the context of SMC2, this suggests that Nx should be allowed to increase
in the course of the algorithm. To that eﬀect, Chopin et al. [2013a] devised an
exchange step, which consists in exchanging the current particle systems, of size
Nx, with new particle systems, of size N new
x
, through importance sampling. In
Chopin et al. [2013a]’s implementation, the exchange step is triggered each time
the acceptance rate of the PMMH step (as performed in Step 3. of Algorithm 26)
is below a certain threshold, and N new
x
= 2Nx (i.e. Nx doubles every time).
The main drawback of this approach is that it introduces some weight degener-
167

6 Towards the automatic calibration of the number of particles in SMC2
Algorithm 27 Particle Gibbs update
Input: (θ, x1:Nx
0:t , a1:Nx
1:t )
Output: (˜θ, ˜x1:Nx
0:t , ˜a1:Nx
1:t )
1. Sample bt ∼M(W 1:Nx
t
), with W n
t = wt,θ(x
an
t
t−1, xn
t )/ PNx
i=1 wt,θ(x
ai
t
t−1, xi
t). From
s = t −1 to s = 0, set bs ←abs+1
s+1 . Set ˜x1
s ←xbs
s , ˜a1
s = 1 for all s ∈0 : T.
2. Sample ˜θ from a MCMC step that leaves invariant distribution p(θ|x0:t, y0:t),
but with x0:t set to ˜x1
0:t.
3. Sample (˜x2:Nx
0:t , ˜a2:Nx
1:t ) as in Algorithm 23, but for parameter ˜θ and conditionally
on ˜x1
0:t, that is: at time 0, generate ˜xn
0 ∼q0,˜θ for n ∈2 : N, at time 1, sample
an
t ∼M(W 1:Nx
1
), for n ∈2 : N, and xn
t ∼q1,˜θ(·|˜x
˜an
1
t−1), and so on.
acy immediately after the resampling step. In particular, we will observe in our
simulations that this prevents us from changing Nx too frequently, as the ESS of
the weights then becomes too low.
In this chapter, we discuss how to use a Particle Gibbs step in order to increase
Nx without changing the weights.
6.3 Proposed approach
6.3.1 Particle Gibbs and memory cost
We ﬁrst remark that the Particle Gibbs step, Algorithm 27, oﬀers a very simple
way to change Nx during the course of the algorithm: In Step (2), simply re-
generate a particle system (conditional on selected trajectory ˜x1
0:t) of size N new
x
.
But, as already discussed, such a strategy requires then to access past particle
values xn
s (and also an
s), rather than only current particle values xn
t .
This problem may be addressed in two ways. First, one may remark that, to
implement Particle Gibbs, one needs to store only those xn
s (and an
s) which have
descendant among the Nx current particles xn
t . Jacob et al. [2013] developed such
a path storage approach, and gave conditions on the mixing of Markov chain (xt)
under which this approach has memory cost O(t+Nx log Nx) (for a single PF with
Nx particles, run until time t). Thus, an implementation of this approach within
SMC2 would lead to a O(Nθ(t + Nx log Nx)) memory cost.
A second approach, developed here, exploits the deterministic nature of PRNGs
(pseudo-random number generators): a sequence z0, z1, . . . , zi, . . . of computer-
168

6.3 Proposed approach
generated random variates is actually a deterministic sequence determined by the
initial state (seed) of the PRNG. It is suﬃcient to store that initial state and z0
in order to recover any zi in the future. The trade-oﬀis an increase in CPU cost,
as each access to zi require re-computing z1, . . . , zi.
We apply this idea to the variables (x1:Nx,m
0:t
, a1:Nx,m
1:t
). By close inspection of
Algorithm 25, we note that variables in a ‘time slice’ (x1:Nx,m
s
, a1:Nx,m
s
), 0 < s ≤t
(or x1:Nx,m
0
at time 0) are always generated jointly, either during Step (a), or
during Step (c). In both cases, this time-slice is a deterministic function of the
current PRNG state and the previous time slice. Thus, one may recover any time
slice (when needed) by storing only (i) the PNRG state (immediately before the
generation of the time slice); and (ii) in which Step (either (a) or (c)) the time
slice was generated. This reduces the memory cost of SMC2 from O(tNθNx) to
O(Nθ(t + Nx)).
Compared to the path storage approach mentioned above, our PRNG recycling
approach has a larger CPU cost, a smaller memory cost, and does not require
any conditions on the mixing properties of process (xt). Note that the CPU cost
increase is within a factor of two, because each time a Particle Gibbs update is
performed, the number of random variables that must be re-generated (i.e. the xn
s
and an
s in Algorithm 27) roughly equals the number of random variables that are
generated for the ﬁrst time (i.e. the ˜xn
s and ˜an
s in Algorithm 27).
6.3.2 Nonparametric estimation of Nx
As seen in Algorithm 25, a Particle Gibbs step will be performed each time
the ESS goes below some threshold.
That the ESS is low may indicate that
Nx is also too low, and therefore that the variance of the likelihood estimates
Lt(θm, x1:Nx,m
0:t
, a1:Nx,m
1:t
) is too high. Our strategy is to update (each time a Parti-
cle Gibbs step is performed) the current value of Nx to N new
x
= τ/ˆσ2, where ˆσ2 is
some (possibly rough) estimate of the variance of the log likelihood estimates. This
is motivated by results from Doucet et al. [2012], who also develop some theory
that supports choosing τ ≈1 is optimal (although their optimality results do not
extend straightforwardly to our settings).
Assume Θ ⊂Rd. To estimate σ2, we use backﬁtting to ﬁt a GAM (generalized
additive model) to the responses Rm = log Lt(θm, x1:Nx,m
0:t
, a1:Nx,m
1:t
):
Rm = α +
d
X
j=1
fj(Cm
j ) + εm,
using as covariates Cm
j
the d principal components of the resampled θ-particles.
The estimate σ2 is then the empirical variance of the residuals. See e.g. Chap. 9
of Hastie et al. [2009] for more details on backﬁtting and GAM modelling.
169

6 Towards the automatic calibration of the number of particles in SMC2
We found this strategy to work well, with the caveat that choosing τ required
some trial and error.
6.3.3 Additional considerations
Using Particle Gibbs as our PMCMC move within SMC2 hast two advantages:
(a) it makes it possible to change Nx without changing the weights, as explained
above; and (b) it also makes it possible to update the θm according to Gibbs or
Metropolis step that leaves θ|x0:t, y0:t invariant); see Step (3) of Algorithm 27. For
models where sampling from θ|x0:t, y0:t is not convenient, one may instead update
θ through several PMMH steps performed after the Particle Gibbs step.
6.4 Numerical example
We consider the following stochastic volatility model: x0 ∼N(µ, σ2/(1 −ρ2)),
xt −µ = ρ(xt−1 −µ) + σǫt,
ǫt ∼N(0, 1) and yt|xt ∼N(0, ext); thus θ = (µ, ρ, σ),
with ρ ∈[−1, 1], σ > 0. We assign independent priors to the components of θ:
µ ∼N(0, 22), ρ ∼N(0, 1) constrained to [−1, 1], and σ2 ∼IG(3, 0.5). The dataset
consists in log-returns from the monthly SP500 index, observed from 29/05/2013
to 19/12/2014; T = 401.
Figure 6.1 plots the marginal posterior p(ρ, σ2|y0:15), as approximated by SMC2,
run up to time 15. This ﬁgure illustrates the need for modelling nonparametrically
the true likelihood as a function of θ, in order to estimate the variance of the
estimated likelihood.
170

6.4 Numerical example
−2
−1
0
1
−1.0
−0.5
0.0
0.5
1.0
 
 
Figure 6.1: Marginal posterior p(σ2, ρ|y0:15), as approximated by
SMC2 run until t = 15, and linearly transformed so that
axes are the two principal components.
For this model, sampling jointly from θ|x0:t, y0:t is diﬃcult, but it is easy to
perform a Gibbs step that leaves invariant θ|x0:t, y0:t, as the full conditionals of
each component (e.g. µ|σ, ρ, x0:t, y0:t and so on) are standard distributions. Let’s
call ‘full PG’ Algorithm 27, where Step 2 consists of this Gibbs step for θ|x0:t, y0:t;
and conversely let’s call ‘partial PG’ Algorithm 27 with ˜θ = θ in Step 2 (θ is not
updated).
We compare four versions of SMC2: (a) the standard version, as proposed in
Chopin et al. [2013a] (i.e. Step (c) of Algorithm 25 is a PMMH step, and that
step is followed by an exchange step to double Nx when the acceptance rate of
PMMH is below 20%); (b) the same algorithm, except that an exchange step
is systematically performed after Step (c), and Nx is set to the value obtained
with our non-parametric approach (see Section 6.3.2); (c) the version developed
in this chapter, with full PG steps (and Nx updated through the non-parametric
procedure); (d) the same algorithm, but with partial PG steps, followed by 3
PMMH steps to update θ.
The point of Algorithm (b) is to show that adapting Nx too often during the
course of the algorithm is not desirable when using the exchange step, as this leads
to too much variance. The point of Algorithm (d) is to see how our approach
performs when sampling from θ|x0:t, y0:t (either independently or through MCMC)
is not feasible.
Figure 6.2 plots the evolution of Nx over time for the four SMC2 algorithms. One
sees that, for these model and dataset, the CPU cost of the standard SMC2 algo-
rithm is quite volatile, as Nx increases very quickly in certain runs. In fact certain
171

6 Towards the automatic calibration of the number of particles in SMC2
runs are incomplete, as they were stopped when the CPU time exceeded 10 hours.
On the other hand, the CPU cost of other versions is more stable across runs, and,
more importantly, quite lower.
0
1000
2000
3000
0
100
200
300
400
time steps
NX
Figure 6.2: Evolution of Nx over time for 5 runs of the four considered
SMC2 algorithms; red dotted line is Algorithm (a), blue
dashed is (b), black solid is (c), green double-dashed is
(d). Results of (c) and (d) are nearly undistinguishable.
Figure 6.3 plots the empirical variance of the estimated marginal likelihood
(evidence, p(y0:t)), normalised with the running time up to time step t.
One
observes that version (c) does quite better than (d), and far much better than (a).
Results from Algorithm (b) were to variable to be included.
172

6.4 Numerical example
0.00
0.25
0.50
0.75
1.00
0
100
200
300
400
time steps
variance of the log evidence x CPU time
Figure 6.3: Empirical variance of estimated marginal likelihood
p(y0:t) multiplied by average CPU time; same legend as
Figure 6.2, results from Algorithm (b) are omitted.
0.00
0.25
0.50
0.75
1.00
0
100
200
300
400
time steps
Acceptance ratio
Figure 6.4: PMMH acceptance rate across time; same legend as Fig-
ure 6.2. Black line marks 20% target.
Figure 6.4 plots the acceptance rate of PMMH steps for Algorithms (a), (b)
and (d). (Recall that Algorithm (c) does not perform PMMH steps). Note the
poor performance of Algorithm (b). Figure 6.5 compares the box-plots of pos-
terior estimates of σ at ﬁnal time T, obtained from several runs of Algorithms
(c) and (d). Algorithm (c) shows slightly less variability, while being 30% faster
173

6 Towards the automatic calibration of the number of particles in SMC2
on average. One sees that the improvement brought by ability to sample from
θ|x0:t, y0:t is modest here for parameter estimation, but recall that in Figure 6.3,
the improvement was more substantial.
0.225
0.230
0.235
0.240
SMC^2 PG
SMC^2 PMMH
value
Figure 6.5: Box-plots of posterior estimate of parameter σ at ﬁnal
time T, over repeated runs of Algorithm (c) (left panel)
and Algorithm (d) (right panel).
174

BIBLIOGRAPHY
Bibliography
S. L. Adler. Over-relaxation method for the Monte Carlo evaluation of the partition
function for multiquadratic actions. Physical Review D, 23(12):2901, 1981.
J. H. Albert and S. Chib. Bayesian analysis of binary and polychotomous response
data. J. Am. Statist. Assoc., 88(422):669–79, 1993.
P. Alquier. Pac-bayesian bounds for randomized empirical risk minimizers. 17(4):
279–304, 2008.
P. Alquier. Bayesian methods for low-rank matrix estimation: short survey and
theoretical study. In S. Jain, R. Munos, F. Stephan, and T. Zeugmann, editors,
Algorithmic Learning Theory. Springer - Lecture Notes in Artiﬁcial Intelligence,
2014.
P. Alquier and G. Biau. Sparse single-index model. J. Mach. Learn. Res., 14(1):
243–280, 2013.
P. Alquier and X. Li. Prediction of quantiles by statistical learning and application
to GDP forecasting.
In J.-G. Ganascia, P. Lenca, and J.-M. Petit, editors,
Discovery Science. Springer - Lecture Notes in Artiﬁcial Intelligence, 2012.
C. Andrieu and G.O. Roberts. The pseudo-marginal approach for eﬃcient Monte
Carlo computations. The Annals of Statistics, 37(2):697–725, 2009. doi: 10.
1214/07-AOS574.
C. Andrieu and J. Thoms. A tutorial on adaptive MCMC. Statist. Comput., 18
(4):343–373, 2008. doi: 10.1007/s11222-008-9110-y.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov Chain Monte Carlo.
J. R. Statist. Soc. B, 72:269–342, 2010a.
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo
methods. J. R. Statist. Soc. B, 72(3):269–342, 2010b. doi: 10.1111/j.1467-9868.
2009.00736.x.
C. Andrieu, A. Lee, and M. Vihola. Uniform Ergodicity of the Iterated Conditional
SMC and Geometric Ergodicity of Particle Gibbs samplers.
ArXiv e-prints,
December 2013.
J.-Y. Audibert and O. Catoni. Robust linear least squares regression. Ann. Statist.,
39(5):2766–2794, 10 2011. doi: 10.1214/11-AOS918. URL http://dx.doi.org/
10.1214/11-AOS918.
175

BIBLIOGRAPHY
R´emi Bardenet, Arnaud Doucet, and Chris Holmes. On markov chain monte carlo
methods for tall data. arXiv preprint arXiv:1505.02827, 2015.
J. Bennett and S. Lanning. The netﬂix prize. In Proceedings of KDD Cup and
Workshop 07, 2007.
Alexandros Beskos, Natesh Pillai, Gareth Roberts, Jesus-Maria Sanz-Serna, and
Andrew Stuart. Optimal tuning of the hybrid monte carlo algorithm. Bernoulli,
19(5A):1501–1534, nov 2013. doi: 10.3150/12-bej414. URL http://dx.doi.
org/10.3150/12-BEJ414.
C. M. Bishop. Pattern Recognition and Machine Learning, chapter 10. Springer,
2006a.
C.M. Bishop.
Pattern recognition and machine learning.
Springer New York,
2006b.
P. Bissiri, C. Holmes, and S. Walker. A general framework for updating belief
distributions. arXiv preprint arXiv:1306.6430, 2013.
S. Boucheron, G. Lugosi, and P. Massart.
Concentration Inequalities.
Oxford
University Press, 2013.
F. Bretz, A. Genz, and L. A. Hothorn. On the numerical availability of multiple
comparison procedures. Biometrical journal, 5:645–656, 2001.
P. B¨uhlmann and S. van de Geer. Statistics for High-Dimensionnal Data. Springer,
2011.
A. Bursh-Supan, V. Hajivassiliou, and L. J. Kotlikoﬀ. Health, children and elderly
arrangments: a multiperiod multinomial probit model with unobserved hetero-
geneity and autocorrelated errors.
Topics in the Economics of Aging, pages
79–108, 1992.
R. H. Byrd, S. L. Hansen, J. Nocedal, and Y. Singer. A stochastic quasi-Newton
method for large-scale optimization. arXiv preprint arXiv:1401.7020, 2014.
E. J. Cand`es and T. Tao. The power of convex relaxation: near-optimal matrix
completion. IEEE Trans. Inform. Theory, 56(5):2053–2080, 2010. ISSN 0018-
9448.
doi: 10.1109/TIT.2010.2044061.
URL http://dx.doi.org/10.1109/
TIT.2010.2044061.
O. Capp´e, E. Moulines, and T. Ryden.
Inference in Hidden Markov Models.
Springer Series in statistics, 2005.
176

BIBLIOGRAPHY
J. Carpenter, P. Cliﬀord, and P. Fearnhead. Improved particle ﬁlter for nonlinear
problems. IEE Proceedings-Radar, Sonar and Navigation, 146(1):2–7, 1999.
O. Catoni. Statistical learning theory and stochastic optimization, volume 1851
of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 2004. Lecture notes
from the 31st Summer School on Probability Theory held in Saint-Flour, July
8–25, 2001.
O. Catoni. PAC-Bayesian Supervised Classiﬁcation, volume 56. IMS Lecture Notes
& Monograph Series, 2007.
O. Catoni. Challenging the empirical mean and empirical variance: A deviation
study. Ann. Inst. H. Poincar´e Probab. Statist., 48(4):1148–1185, 11 2012. doi:
10.1214/11-AIHP454. URL http://dx.doi.org/10.1214/11-AIHP454.
V. Chernozhukov and H. Hong.
An MCMC approach to classical estimation.
Journal of Econometrics, 115(2):293–346, 2003.
Hugh Chipman, Edward I George, and Robert E McCulloch. The practical imple-
mentation of Bayesian model selection, pages 65–134. 2001.
N. Chopin. A sequential particle ﬁlter method for static models. Biometrika, 89
(3):539–551, 2002a.
N. Chopin. A sequential particle ﬁlter for static models. Biometrika, 89:539–552,
2002b.
N. Chopin. Fast simulation of truncated Gaussian distributions. Statist. Comput.,
21(2):275–288, 2011a. ISSN 0960-3174. doi: 10.1007/s11222-009-9168-1.
N. Chopin. Fast simulation of truncated gaussian distributions. Statist. Comput.,
21(2):275–288, 2011b.
N. Chopin and C. Shaeﬀer. Sequential monte carlo on large binary sampling spaces.
Statist. Comput., 2011.
N. Chopin, P. Jacob, and O. Papaspiliopoulos. SMC2: A sequential Monte Carlo
algorithm with particle Markov chain Monte Carlo updates. J. R. Statist. Soc.
B, 75(3):397–426, 2013a.
N. Chopin, O. Papaspiliopoulos, and P. E. Jacob. SMC2: an eﬃcient algorithm for
sequential analysis of state space models. J. R. Statist. Soc. B, 75(3):397–426,
2013b.
177

BIBLIOGRAPHY
J. A. Christen, C. Fox, D. A. P´erez-Ruiz, and M. Santana-Cibrian. On optimal
direction Gibbs sampling. arXiv preprint arXiv:1205.4062, 2012.
S. Cl´emen¸con, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of
U-statistics. Ann. Stat., 36(2):844–874, 04 2008a.
S. Cl´emen¸con, V.C. Tran, and H. De Arazoza.
A stochastic SIR model with
contact-tracincing: large population limits and statistical inference. Journal of
Biological Dynamics, 2(4):392–414, 2008b.
G. Consonni and J.M. Marin. Mean-ﬁeld variational approximate Bayesian infer-
ence for latent variable models. Comput. Stat. Data Anal., 52(2):790–798, 2007.
doi: 10.1016/j.csda.2006.10.028. URL http://dx.doi.org/10.1016/j.csda.
2006.10.028.
John D. Cook. Time exchange rate. The Endeavour (blog), 2014. URL http:
//www.johndcook.com/blog/2014/08/17/time-exchange-rate/.
C. Cortes and M. Mohri. Auc optimization vs. error rate minimization. In NIPS,
volume 9, 2003.
A. S. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting, sharp
PAC-Bayesian bounds and sparsity. Machine Learning, 72:39–61, 2008.
A. S. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation
and Langevin Monte-Carlo. Journal of Computer and System Science, 78(5):
1423–1443, 2012.
P. Del Moral. Non-linear ﬁltering: interacting particle resolution. Markov processes
and related ﬁelds, 2(4):555–581, 1996a.
P. Del Moral. Non-linear ﬁltering: interacting particle resolution. Markov processes
and related ﬁelds, 2(4):555–581, 1996b.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–
436, 2006a.
P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. J. R.
Statist. Soc. B, 68(3):411–436, 2006b. ISSN 1467-9868.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incom-
plete data via the em algorithm. J. R. Statist. Soc. B, 39:1–38, 1977.
178

BIBLIOGRAPHY
R. Douc, O. Cappe, and E. Moulines.
Comparison of resampling schemes for
particle ﬁltering.
In Proc. 4th Int. Symp. Image and Signal Processing and
Analysis ISPA 2005, pages 64–69, 2005.
A. Doucet, S. Godsill, and C. Andrieu. On sequential monte carlo sampling meth-
ods for bayesian ﬁltering. Statistics and computing, 10(3):197–208, 2000.
A. Doucet, M. Briers, and S. Senecal.
Eﬁicient Block Sampling Strategies for
Sequential Monte Carlo. J. Comput. Graph. Statist., 15(3):693–711, 2006.
A. Doucet, M. Pitt, G. Deligiannidis, and R. Kohn. Eﬃcient implementation of
Markov chain Monte Carlo when using an unbiased likelihood estimator. ArXiv
preprint, October 2012.
S. Duane, D. Kennedy, A., B. J. Pendelton, and D. Roweth. Hybrid monte carlo.
Physics Letters B, 195(2):216–222, september 1987.
C. Dubarry and R. Douc. Particle approximation improvement of the joint smooth-
ing distribution with on the ﬂy variance estimation. arXiv:1107.5524v1, pages
1–19, June 2011.
Mohammad Emtiyaz Khan,
Aleksandr Aravkin,
Michael Friedlander,
and
Matthias Seeger. Fast dual variational inference for non-conjugate latent gaus-
sian models. In Proceedings of The 30th International Conference on Machine
Learning, pages 951–959, 2013.
C. Faes, J. Ormeros, and M. Wand. Variational Bayesian Inference for Parametric
and Nonparametric Regression With Missing Data. J. Am. Statist. Assoc., 106
(495):959–971, September 2011.
P. Fearnhead, O. Papaspiliopoulos, G.O. Roberts, and A. Stuart. Random weight
particle ﬁltering of continuous time processes.
J. R. Stat. Soc. Ser. B Stat.
Methodol., 72:497–513, 2010.
D. Firth. Bias reduction of maximum likelihood estimates. Biometrika, 80(1):
27–38, 1993.
C. Flecher, D. Allard, and P. Naveau. Truncated skew-normal distributions: Es-
timation by weighted moments and application to climatic data. Technical Re-
port 39, Institut National de la Recherche Agronomique, 2009.
Y. Freund, R. Iyer, R.E Schapire, and Y. Singer. An eﬃcient boosting algorithm
for combining preferences. J. Mach. Learn. Res., 4:933–969, 2003.
179

BIBLIOGRAPHY
Sylvia Fr¨uhwirth-Schnatter and Rudolf Fr¨uhwirth.
Data augmentation and
mcmc for binary and multinomial logit models. In Statistical Modelling and
Regression Structures, pages 111–132. Physica-Verlag HD, Dec 2009.
doi:
10.1007/978-3-7908-2413-1 7.
Andrew Gelman and Jennifer Hill.
Data analysis using regression and multi-
level/hierarchical models. Cambridge University Press, 2006.
Andrew Gelman, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su.
A
weakly informative default prior distribution for logistic and other regression
models.
Ann. Appl. Stats., 2(4):1360–1383, 2008.
ISSN 1932-6157.
doi:
10.1214/08-AOAS191. URL http://dx.doi.org/10.1214/08-AOAS191.
A. Genz. Numerical Computation of Multivariate Normal Probabilities. J. Com-
put. Graph. Statist., 1(2):141–149, June 1992.
A. Genz and F. Bretz. Computation of Multivariate Normal and t Probabilities,
volume 195 of Lecture Notes in Statistics. Springer, 2009.
Edward I. George and Robert E. McCulloch. Variable selection via gibbs sampling.
J. Am. Statist. Assoc., 88(423):881–889, sep 1993a. doi: 10.1080/01621459.1993.
10476353. URL http://dx.doi.org/10.1080/01621459.1993.10476353.
E.I. George and R.E. McCulloch. Variable selection via Gibbs sampling. J. Am.
Statist. Assoc., 88(423):pp. 881–889, 1993b.
J. Geweke. Eﬃcient simulation from the multivariate normal and student-t dis-
tributions subject to linear constraints. Computing Science and Statistics, 23:
571–578, 1991.
G. J. Gibson, C. A. Glasbey, and D. A. Elston. Monte-carlo evalution of multivari-
ate normal integrals and sensitivity to variate ordering. Advances in Numerical
Methods & applications, pages 120–126, 1994.
Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian
Monte Carlo methods. J. R. Statist. Soc. B, 73(2):123–214, 2011.
Neil J Gordon, David J Salmond, and Adrian FM Smith.
Novel approach to
nonlinear/non-gaussian bayesian state estimation. In IEE Proceedings F (Radar
and Signal Processing), volume 140, pages 107–113. IET, 1993.
Robert B. Gramacy and Nicholas G. Polson. Simulation-based regularized logistic
regression. 7(3):567–590, Sep 2012. doi: 10.1214/12-ba719.
180

BIBLIOGRAPHY
P. J. Green.
Reversible Jump Markov Chain Monte Carlo computation and
Bayesian Model Determination. Biometrika, 82(4):711–732, 1995.
P. J. Green, K. Latuszynski, M. Pereyra, and C. P. Robert. Bayesian computa-
tion: a perspective on the current state, and sampling backwards and forwards.
Preprint arXiv:1502.01148, 2015.
B. Guedj and P. Alquier. PAC-Bayesian estimation and prevision in sparse additive
models. Electronic Journal of Statistics, 7:264–291, 2013.
V. Hajivassiliou, D. McFadden, and P. Ruud.
Simulation of multivariate nor-
mal rectangle probabilities and their derivatives theoretical and computational
results. Journal of Econometrics, 72(1-2):85–134, May–June 1996.
Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and
R Tibshirani. The elements of statistical learning, volume 2. Springer, 2009.
D. Hernandez-Lobato, J. Hernandez-Lobato, and P. Dupont. Generalized Spike-
and-Slab Priors for Bayesian Group Feature Selection Using Expectation Prop-
agation . J. Mach. Learn. Res., 14:1891–1945, 2013.
Y. Hochberg and A. C. Tamhane. Multiple comparison procedures. John Wiley &
Sons, Inc., 1987.
W. Hoeﬀding. Probability Inequalities for Sums of Random Variables. Ann. Math.
Stat., 10:293–325, 1948.
M. Hoﬀman and A. Gelman. The no-U-turn sampler: Adaptively setting path
lengths in Hamiltonian monte carlo.
J. Mach. Learn. Res., page (in press),
2013.
M. D. Hoﬀman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational infer-
ence. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.
C. Holmes and L. Held. Bayesian auxiliary variable models for binary and multi-
nomial regression. 1(1):145–168, 2006.
Wolfgang H¨ormann and Josef Leydold. Quasi importance sampling. Technical
report, 2005.
P. Jacob, C. P. Robert, and M. H. Smith. Using parallel computation to improve
independent metropolis–hastings based estimation. J. Comput. Graph. Statist.,
20(3):616–635, Jan 2011. doi: 10.1198/jcgs.2011.10167.
181

BIBLIOGRAPHY
P.E. Jacob, L. Murray, and S. Rubenthaler.
Path storage in the particle ﬁl-
ter.
Statist. Comput., pages 1–10, 2013.
ISSN 0960-3174.
doi:
10.1007/
s11222-013-9445-x. URL http://dx.doi.org/10.1007/s11222-013-9445-x.
A. Jasra, D. Stephens, and C. Holmes. On population-based simulation for static
inference. Statist. Comput., 17(3):263–279, 2007.
A. Jasra, D. Stephens, A. A. Doucet, and T. Tsagaris. Inference for L´evy driven
stochastic volatility models via Sequential Monte Carlo. Scand. J. of Statist.,
38(1), 2011a.
Ajay Jasra, David A Stephens, Arnaud Doucet, and Theodoros Tsagaris. Inference
for L´evy-Driven Stochastic Volatility Models via Adaptive Sequential Monte
Carlo. Scandinavian Journal of Statistics, 38(1):1–22, 2011b.
W. Jiang and M. A. Tanner.
Gibbs posterior for variable selection in high-
dimensional classiﬁcation and data mining.
The Annals of Statistics, 36(5):
2207–2231, 2008.
M. I. Jordan, Z. Ghahrapani, T. S. Jaakkola, and L. K. Saul. An introduction
to variational methods for graphical models. Machine Learning, (37):183–233,
1999.
Pasi Jyl¨anki, Jarno Vanhatalo, and Aki Vehtari. Robust gaussian process regres-
sion with a student-t likelihood. The Journal of Machine Learning Research,
12:3227–3257, 2011.
Ata Kab´an. On bayesian classiﬁcation with laplace priors. Pattern Recognition
Letters, 28(10):1271–1282, 2007. doi: 10.1016/j.patrec.2007.02.010.
M. Keane. Simulation estimation for panel data models with limited dependent
variables. MPRA Paper 53029, University Library of Munich, Germany, 1993.
M. E. Khan. Decoupled variational Gaussian inference. In Advances in Neural
Information Processing Systems, pages 1547–1555, 2014.
V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear-norm penalization and
optimal rates for noisy low-rank matrix completion. The Annals of Statistics,
39(5):2302–2329, 2011.
Augustine Kong, Jun S Liu, and Wing Hung Wong. Sequential imputations and
bayesian missing data problems. Journal of the American statistical association,
89(425):278–288, 1994.
182

BIBLIOGRAPHY
Demetris Lamnisos, Jim E. Griﬃn, and Mark F. J. Steel. Adaptive Monte Carlo
for Bayesian variable selection in regression models. J. Comput. Graph. Statist.,
22(3):729–748, 2013. ISSN 1061-8600. doi: 10.1080/10618600.2012.694756. URL
http://dx.doi.org/10.1080/10618600.2012.694756.
N. D. Lawrence and R. Urtasun. Non-linear matrix factorization with Gaussian
processes. In Proceedings of the 26th Annual International Conference on Ma-
chine Learning, pages 601–608. ACM, 2009.
G. Lecu´e. M´ethodes d’agr´egation: optimalit´e et vitesses rapides. Ph.D. thesis,
Universit´e Paris 6, 2007.
Anthony Lee, Christopher Yau, Michael B. Giles, Arnaud Doucet, and Christo-
pher C. Holmes. On the utility of graphics cards to perform massively parallel
simulation of advanced Monte Carlo methods. J. Comput. Graph. Statist., 19
(4):769–789, jan 2010.
doi: 10.1198/jcgs.2010.10039.
URL http://dx.doi.
org/10.1198/jcgs.2010.10039.
Christiane Lemieux.
Monte Carlo and Quasi-Monte Carlo Sampling (Springer
Series in Statistics). Springer, February 2009. ISBN 0387781641.
J. P. LeSage, Pace R. K., N. Lam, R. Campanella, and Liu X.
New Orleans
buisness recovery in the aftermath of huricane Katrina.
App. Stat., 174(4):
1007–1027, October 2011.
Y. J. Lim and Y. W. Teh. Variational Bayesian approach to movie rating predic-
tion. Proceedings of KDD Cup and Workshop, 7:15–21, 2007.
D. J. C. MacKay. Information theory, inference and learning algorithms. Cam-
bridge University Press, 2002.
T. T. Mai and P. Alquier. A Bayesian approach for matrix completion: optimal
rate under general sampling distribution. Electronic Journal of Statistics, 9:
823–841, 2015.
E. Mammen and A. Tsybakov. Smooth discrimination analysis. Ann. Stat., 27(6):
1808–1829, 12 1999.
P. Massart. Concentration Inequalities and Model Selection, volume 1896. Springer
Lecture Notes in Mathematics, 2007.
D. A. McAllester. PAC-Bayesian model averaging. In Proceedings of of the Twelth
Annual Conference On Computational Learning Theory, Santa Cruz, California
(Electronic), pages 164–170. ACM, New-York, 1999.
183

BIBLIOGRAPHY
D.A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh an-
nual conference on Computational learning theory, pages 230–234. ACM, 1998.
S. Meyn and R. L. Tweedie. Markov chains and Stochastic Stability. Cambridge
University Press, 2nd edition, 2009.
T. Minka. Expectation Propagation for approximate Bayesian inference. In Proc.
17th Conf. Uncertainty Artiﬁcial Intelligence, UAI ’01, pages 362–369. Morgan
Kaufmann Publishers Inc., 2001a.
T.P. Minka. Expectation Propagation for approximate Bayesian inference. Pro-
ceedings of Uncertainty in Artiﬁcial Intelligence, 17:362–369, 2001b.
T. Minwa, A. J. Hayter, and S. Kuriki. The evaluation of general non-centered
orthant pro. J. R. Statist. Soc. B, 65:223–234, 2003.
T. J Mitchell and J. Beauchamp. Bayesian variable selection in linear regression.
J. Am. Statist. Assoc., 83(404):1023–1032, 1988.
M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte
Carlo, page 51, 2010a.
R. Neal. Annealed importance sampling. Statist. Comput., 11(2):125–139, 2001a.
R. M. Neal. Annealed importance sampling. Statist. Comput., 11:125–139, 2001b.
R. M. Neal.
MCMC using Hamiltonian dynamics.
In S. Brooks, A. Gelman,
G. L. Jones, and X.-L. Meng, editors, Handbook of Markov Chain Monte Carlo,
pages 113–162. Chapman & Hall / CRC Press, 2010b. URL http://www.cs.
utoronto.ca/~radford/ftp/ham-mcmc.pdf.
Y. Nesterov. Introductory lectures on convex optimization, volume 87. Springer
Science & Business Media, 2004.
H. Nickisch and C.E. Rasmussen. Approximations for Binary Gaussian Process
Classiﬁcation. J. Mach. Learn. Res., 9(10):2035–2078, October 2008.
H. Niederreiter. Quasi-Monte Carlo methods and pseudo-random numbers. Bul-
letin of the american mathematical society, 84(6):957–1041, November 1978.
M. Opper and C. Archambeau. The variational Gaussian approximation revisited.
Neural computation, 21(3):786–792, 2009.
M. Opper and O. Winther.
Gaussian Processes for Classiﬁcation: Mean-ﬁeld
Algorithms. Neural Computation, 12(11):2655–2684, November 2000.
184

BIBLIOGRAPHY
A. Pakman and L. Paninski. Exact Hamiltonian Monte Carlo for Truncated Mul-
tivariate Gaussians. arXiv:1208.4118, pages 1–30, 2012.
M.D. Pandey. An eﬀective approximation to evaluate multinormal integrals. Struc-
tural Safety, 20(1):51–67, 1998.
G. Parisi. Statistical ﬁeld theory. Addison-Wesley, New-York, 1988.
M. Pitt and N. Shephard. Filtering via simulation: Auxiliary particle ﬁlters. J.
Am. Statist. Assoc., 94(446):590–599, June 1999.
Nicholas G. Polson, James G. Scott, and Jesse Windle. Bayesian inference for
logistic models using p´olya–gamma latent variables. Journal of the American
Statistical Association, 108(504):1339–1349, Dec 2013. doi: 10.1080/01621459.
2013.829001. URL http://dx.doi.org/10.1080/01621459.2013.829001.
A. Prekopa. On probabilistic constrained programming. In Proceedings of the
Princeton symposium on mathematical programming, pages 113–138. Princeton,
New Jersey: Princeton University Press, 1970.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical
Recipes: The Art of Scientiﬁc Computing. Cambridge University Press, 2007.
C. Rasmussen and C. Williams. Gaussian processes for Machine Learning. MIT
press, 2006.
James Ridgway. Computation of Gaussian orthant probabilities in high dimension.
(Minor revision) Statist. Comput.., 2015.
S. Robbiano. Upper bounds and aggregation in bipartite ranking. Elec. J. of Stat.,
7:1249–1271, 2013.
H. Robbins and S. Monro. A stochastic approximation method. The annals of
mathematical statistics, pages 400–407, 1951.
C. P. Robert. Simulation of truncated normal variables. Statist. Comput., 5(2):
121–125, 1995.
C. P. Robert and G. Casella. Monte Carlo Statistical Methods, 2nd ed. Springer-
Verlag, New York, 2004a.
C. P. Robert and G. Casella. Monte Carlo Statistical Methods, chapter 9. Springer,
2004b.
Christian Robert. The Bayesian choice: from decision-theoretic foundations to
computational implementation. Springer Science & Business Media, 2007.
185

BIBLIOGRAPHY
G. Roberts and J. Rosenthal.
Optimal scaling of discrete approximations to
langevin diﬀusions. J. R. Statist. Soc. B, 60(1):255–268, 1998.
Gareth O. Roberts and Jeﬀrey S. Rosenthal.
Optimal Scaling for Various
Metropolis-Hastings Algorithms. Statist. Science, 16(4):351–367, 2001. ISSN
08834237. doi: 10.1214/ss/1015346320.
Gareth O Roberts and Jeﬀrey S Rosenthal. General state space Markov chains
and MCMC algorithms. Probability Surveys, 1:20–71, 2004.
V. Roˇckov´a and E. George. Emvs: The EM approach to bayesian variable selection.
J. Am. Statist. Assoc., 2013.
H. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations.
J. R.
Statist. Soc. B, 71(2):319–392, 2009.
R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using
Markov chain Monte Carlo. In Proceedings of the 25th international conference
on Machine learning, pages 880–887. ACM, 2008.
C. Sch¨afer and N. Chopin. Sequential monte carlo on large binary sampling spaces.
Statistics and Computing, pages 1–22, 2011.
Christian Sch¨afer.
Monte Carlo methods for sampling high-dimensional binary
vectors. PhD thesis, Universit´e Paris Dauphine, 2012.
Steven L. Scott, Alexander W. Blocker, and Fernando V. Bonassi. Bayes and big
data: The consensus monte carlo algorithm. In Bayes 250, 2013.
M. Seeger. Expectation propagation for exponential families. Technical report, U.
of California, 2005a.
M. Seeger. Expectation Propagation for Exponential Families. Technical report,
Univ. California Berkeley, 2005b.
Babak Shahbaba, Shiwei Lan, Wesley O Johnson, and Radford M Neal. Split
hamiltonian monte carlo. Statist. Comput., pages 1–11, 2011. doi: 10.1007/
s11222-012-9373-1. URL http://arxiv.org/pdf/1106.5941.pdf.
J. Shawe-Taylor and R.C. Williamson. A PAC analysis of a Bayesian estimator.
In Proc. conf. Computat. learn. theory, pages 2–9. ACM, 1997.
J. Skilling. Nested sampling for general Bayesian computation. Bayesian Analysis,
1(4):833–860, 2006.
186

BIBLIOGRAPHY
Marc A. Suchard, Quanli Wang, Cliburn Chan, Jacob Frelinger, Andrew Cron,
and Mike West. Understanding GPU programming for statistical computation:
Studies in massively parallel massive mixtures. J. Comput. Graph. Statist., 19
(2):419–438, jan 2010.
doi: 10.1198/jcgs.2010.10016.
URL http://dx.doi.
org/10.1198/jcgs.2010.10016.
T. Suzuki. Convergence rate of Bayesian tensor estimator: Optimal rate with-
out restricted strong convexity. arXiv preprint arXiv:1408.3092 (accepted by
ICML2015), 2014.
L. Tierney, R. E. Kass, and J. B. Kadane. Fully exponential Laplace approxima-
tions to expectations and variances of non-positive functions. J. Am. Statist.
Assoc., 84:710–716, 1989.
Luke Tierney and Joseph B Kadane. Accurate approximations for posterior mo-
ments and marginal densities. J. Am. Statist. Assoc., 81(393):82–86, 1986.
K. E. Train.
Discrete Choice methods with simulation.
Cambridge University
Press, 2009.
A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals
of Statistics, 32(1):135–166, 2004.
A. Van der Vaart. Asymptotic statistics. Cambridge university press, 1998.
A.W. van der Vaart and J.H. van Zanten. Adaptive Bayesian estimation using
a Gaussian random ﬁeld with inverse Gamma bandwidth. Ann. Stat., pages
2655–2675, 2009.
M. A.J. Van Gerven, B. Cseke, F. P. de Lange, and T. Heskes. Eﬃcient Bayesian
multivariate fMRI analysis using a sparsifying spatio-temporal prior. NeuroIm-
age, 50:150–161, 2010.
Xiangyu Wang and David B Dunson. Parallelizing MCMC via Weierstrass sampler.
arXiv preprint arXiv:1312.4605, 2013.
O. Wintenberger. Deviation inequalities for sums of weakly dependent time series.
Electronic Communications in Probability, 15:489–503, 2010.
L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz. Optimizing classiﬁer perfor-
mance via an approximation to the Wilcoxon-Mann-Whitney statistic. Proc.
20th Int. Conf. Mach. Learn., pages 848–855, 2003.
Y. Yang. Aggregating regression procedures to improve performance. Bernoulli,
10:25–47, 2004.
187

BIBLIOGRAPHY
G. Yao and U Bockenholt. Bayesian estimation of Thurstonian ranking models
based on the Gibbs sampler. British Journal of Mathematical and Statistical
Psychology, 52:79–92, 1999.
A. Yuille. Belief Propagation, Mean-ﬁeld and the Bethe approximation. Technical
report, Dept. Statistics UCLA.
T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on
convex risk minimization. Annals of Statistics, pages 56–85, 2004.
T. Zhang. Information theoretical upper and lower bounds for statistical estima-
tion. IEEE Transaction on Information Theory, 52:1307–1321, 2006.
M. Zhou, C. Wang, M. Chen, J. Paisley, D. Dunson, and L. Carin. Nonparametric
bayesian matrix completion. Proc. IEEE SAM, 2010.
188

