MentorNet: Learning Data-Driven Curriculum
for Very Deep Neural Networks on Corrupted Labels
Lu Jiang 1 Zhengyuan Zhou 2 Thomas Leung 1 Li-Jia Li 1 Li Fei-Fei 1 2
Abstract
Recent deep networks are capable of memoriz-
ing the entire data even when the labels are com-
pletely random. To overcome the overﬁtting on
corrupted labels, we propose a novel technique
of learning another neural network, called Men-
torNet, to supervise the training of the base deep
networks, namely, StudentNet. During training,
MentorNet provides a curriculum (sample weight-
ing scheme) for StudentNet to focus on the sample
the label of which is probably correct. Unlike the
existing curriculum that is usually predeﬁned by
human experts, MentorNet learns a data-driven
curriculum dynamically with StudentNet. Ex-
perimental results demonstrate that our approach
can signiﬁcantly improve the generalization per-
formance of deep networks trained on corrupted
training data. Notably, to the best of our knowl-
edge, we achieve the best-published result on We-
bVision, a large benchmark containing 2.2 million
images of real-world noisy labels. The code are at
https://github.com/google/mentornet.
1. Introduction
Zhang et al. (2017a) found that deep convolutional neural
networks (CNNs) are capable of memorizing the entire data
even with corrupted labels, where some or all true labels are
replaced with random labels. It is a consensus that deeper
CNNs usually lead to better performance. However, the abil-
ity of deep CNNs to overﬁt or memorize the corrupted labels
can lead to very poor generalization performance (Zhang
et al., 2017a). Recently, Neyshabur et al. (2017) and Arpit
et al. (2017) proposed deep learning generalization theories
to explain this interesting phenomenon.
This paper studies how to overcome the corrupted label for
1Google Inc., Mountain View, United States 2Stanford Uni-
versity, Stanford, United States. Correspondence to: Lu Jiang
<lujiang@google.com>.
Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).
deep CNNs, so as to improve generalization performance
on the clean test data. Although learning models on weakly
labeled data might not be novel, improving deep CNNs on
corrupted labels is clearly an under-studied problem and
worthy of exploration, as deep CNNs are more prone to
overﬁtting and memorizing corrupted labels (Zhang et al.,
2017a). To address this issue, we focus on training very deep
CNNs from scratch, such as resnet-101 (He et al., 2016) or
inception-resnet (Szegedy et al., 2017) which has a few
hundred layers and orders-of-magnitude more parameters
than the number of training samples. These networks can
achieve the state-of-the-art result but perform poorly when
trained on corrupted labels.
Inspired by the recent success of Curriculum Learning (CL),
this paper tackles this problem using CL (Bengio et al.,
2009), a learning paradigm inspired by the cognitive process
of human and animals, in which a model is learned grad-
ually using samples ordered in a meaningful sequence. A
curriculum speciﬁes a scheme under which training samples
will be gradually learned. CL has successfully improved the
performance on a variety of problems. In our problem, our
intuition is that a curriculum, similar to its role in education,
may provide meaningful supervision to help a student over-
come corrupted labels. A reasonable curriculum can help
the student focus on the samples whose labels have a high
chance of being correct.
However, for the deep CNNs, we need to address two lim-
itations of the existing CL methodology. First, existing
curriculums are usually predeﬁned and remain ﬁxed during
training, ignoring the feedback from the student. The learn-
ing procedure of deep CNNs is quite complicated, and may
not be accurately modeled by the predeﬁned curriculum.
Second, the alternating minimization, commonly used in
CL and self-paced learning (Kumar et al., 2010) requires
alternative variable updates, which is difﬁcult for training
very deep CNNs via mini-batch stochastic gradient descent.
To this end, we propose a method to learn the curriculum
from data by a network called MentorNet. MentorNet learns
a data-driven curriculum to supervise the base deep CNN,
namely StudentNet. MentorNet can be learned to approx-
imate an existing predeﬁned curriculum or discover new
data-driven curriculums from data. The learned data-driven
arXiv:1712.05055v2  [cs.CV]  13 Aug 2018

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
curriculum can be updated a few times taking into account of
the StudentNet’s feedback. Whenever MentorNet is learned
or updated, we ﬁx its parameter and use it together with
StudentNet to minimize the learning objective, where Men-
torNet controls the timing and attention to learn each sample.
At the test time, StudentNet makes predictions alone without
MentorNet.
The proposed method improves existing curriculum learn-
ing in two aspects. First, our curriculum is learned from
data rather than predeﬁned by human experts. It takes into
account of the feedback from StudentNet and can be dynam-
ically adjusted during training. Intuitively, this resembles a
“collaborative” learning paradigm, where the curriculum is
determined by the teacher and student together. Second, in
our algorithm, the learning objective is jointly minimized
using MentorNet and StudentNet via mini-batch stochastic
gradient descent. Therefore, the algorithm can be conve-
niently parallelized to train deep CNNs on big data. We
show the convergence and empirically verify it on large-
scale benchmarks.
We verify our method on four benchmarks. Results show
that it can signiﬁcantly improve the performance of deep
CNNs trained on both controlled and real-world corrupted
training data. Notably, to the best of our knowledge, it
achieves the best-published result on WebVision (Li et al.,
2017a), a large benchmark containing 2.2 million images of
real-world noisy labels. To summarize, the contribution of
this paper is threefold:
• We propose a novel method to learn data-driven cur-
riculums for deep CNNs trained on corrupted labels.
• We discuss an algorithm to perform curriculum learn-
ing for deep networks via mini-batch stochastic gradi-
ent descent.
• We verify our method on 4 benchmarks and achieve
the best-published result on the WebVision benchmark.
2. Preliminary on Curriculum Learning
We formulate our problem based on the model in (Ku-
mar et al., 2010) and (Jiang et al., 2015).
Consider
a classiﬁcation problem with the training set D
=
{(x1, y1), · · · , (xn, yn)}, where xi denotes the ith ob-
served sample and yi ∈{0, 1}m is the noisy label vector
over m classes. Let gs(xi, w) denote the discriminative
function of a neural network called StudentNet, parame-
terized by w ∈Rd. Further, let L(yi, gs(xi, w)), a m-
dimensional column vector, denote the loss over m classes.
Introduce the latent weight variable, v ∈Rn×m, and opti-
mize the objective:
min
w∈Rd,v∈[0,1]n×mF(w,v) =
1
n
n
X
i=1
vT
i L(yi,gs(xi,w)) + G(v; λ) + θ∥w∥2
2
(1)
where ∥·∥2 is the l2 norm for weight decay, and data augmen-
tation and dropout are subsumed inside gs. vi ∈[0, 1]m×1
is a vector to represent the latent weight variable for the i-th
sample. The function G deﬁnes a curriculum, parameterized
by λ. This paper focuses on the one-hot label. For notation
convenience, denote the loss L(yi,gs(xi,w))=ℓi, vi as a
scalar vi, and yi as an integer yi ∈[1, m].
In the existing literature, alternating minimization (Csiszar,
1984), or its related variants, is commonly employed to min-
imize the training objective, e.g. in (Kumar et al., 2010;
Ma et al., 2017a; Jiang et al., 2014).
This is an algo-
rithmic paradigm where w and v are alternatively mini-
mized, one at a time while the other is held ﬁxed. When
v is ﬁxed, the weighted loss is typically minimized by
stochastic gradient descent. When w is ﬁxed, we com-
pute vk = arg minv F(vk−1, wk) using the most recently
updated wk at epoch k. For example, Kumar et al. (2010)
employed G(v) = −λ∥v∥1. When w is ﬁxed, the optimal
v can be easily derived by:
v∗
i = 1(ℓi ≤λ), ∀i ∈[1, n],
(2)
where 1 is the indicator function. Eq. (2) intuitively explains
the predeﬁned curriculum in (Kumar et al., 2010), known as
self-paced learning. First, when updating v with a ﬁxed w,
a sample of smaller loss than the threshold λ is treated as
an “easy” sample, and will be selected in training (v∗
i = 1).
Otherwise, it will not be selected (v∗
i = 0). Second, when
updating w with a ﬁxed v, the classiﬁer is trained only
on the selected “easy” samples. The hyperparameter λ
controls the learning pace and corresponds to the “age” of
the model. When λ is small, only samples of small loss will
be considered. As λ grows, more samples of larger loss will
be gradually added to train a more “mature” model.
As shown, the function G speciﬁes a curriculum, i.e., a
sequence of samples with their corresponding weights to
be used in training. When w is ﬁxed, its optimal solution,
e.g. Eq. (2), computes the time-varying weight that controls
the timing and attention to learn every sample. Recent stud-
ies discovered multiple predeﬁned curriculums and veriﬁed
them in many real-world applications, e.g., in (Fan et al.,
2017; Ma et al., 2017a; Sangineto et al., 2016; Fan et al.,
2017; Chang et al., 2017).
This paper studies learning curriculum from data. In the
rest of this paper, Section 3 presents an approach to learn
data-driven curriculum by MentorNet. Section 4 discusses
an algorithm to optimize Eq. (1) using MentorNet and Stu-
dentNet together via mini-batch training.
3. Learning Curriculum from Data
Existing curriculums are either predetermined as an analytic
expression of G or a function to compute sample weights.
Such predeﬁned curriculums cannot be adjusted accordingly,
taking into account of the feedback from the student. This

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
section discusses a new way to learn data-driven curriculum
by a neural network, called MentorNet. The MentorNet gm
is learned to compute time-varying weights for each training
sample. Let Θ denote the parameters in gm. Given a ﬁxed
w, our goal is to learn an Θ∗to compute the weight:
gm(zi; Θ∗) = arg min
vi∈[0,1] F(w, v), ∀i ∈[1, n]
(3)
where zi = φ(xi, yi, w) indicates the input feature to Men-
torNet about the i-th sample.
3.1. Learning Curriculum
MentorNet can be learned to 1) approximate existing cur-
riculums or 2) discover new curriculums from data.
Learning to approximate predeﬁned curriculums. Our
ﬁrst task is to learn a MentorNet to approximate a predeﬁned
curriculum. To do so, we minimize the objective in Eq. (1):
arg min
Θ
X
(xi,yi)∈D
gm(zi; Θ)ℓi + G(gm(zi; Θ); λ)
(4)
Eq. (4) applies for both convex and non-convex G. This
paper employs the following predeﬁned curriculum. It is
derived from (Jiang et al., 2015) and works well in our
experiments. As will be discussed later, it is also related to
robust non-convex penalties.
G(v; λ) =
n
X
i=1
1
2λ2v2
i −(λ1 + λ2)vi,
(5)
where λ1, λ2 ≥0 are hyper-parameters. As G is convex,
there exists a closed-form solution for the optimal value of
Eq. (3). Given a ﬁxed w, deﬁne Fw(v) = Pn
i=1 f(vi):
f(vi) = viℓi + 1
2λ2v2
i −(λ1 + λ2)vi
(6)
The minima are obtained at ∇vFw(v) = 0, and can be
decoupled by setting ∂f/∂vi = 0. We then have:
gm(zi; Θ∗) =
(
1(ℓi ≤λ1)
λ2 = 0
min(max(0, 1 −ℓi−λ1
λ2
), 1)
λ2 ̸= 0 ,
(7)
where Θ∗is the optimal MentorNet parameter obtained
by SGD. The closed-form solution in Eq. (7) gives some
intuitions about the curriculum. When λ2 = 0, it is sim-
ilar to self-paced learning (Kumar et al., 2010) i.e. only
“easy” samples of ℓi < λ1 will be selected in training
(gm(zi; Θ∗) = 1).
When λ2 ̸= 0, samples of loss
ℓi ≥λ2 +λ1 will not be selected in training. These samples
represent the “hard” samples of greater loss. Otherwise,
samples will be weighted linearly w.r.t. 1 −(ℓi −λ1)/λ2.
As in (Kumar et al., 2010), the hyper-parameters λ1 and λ2
control the learning pace.
Learning data-driven curriculums.
Our next task is
to learn a curriculum solely derived from labeled data.
To this end, Θ is learned on another dataset D′
=
{(φ(xi, yi, w), v∗
i )}, where (xi, yi) is sampled from D
and |D′| ≪|D|.
v∗
i is a given annotation and we as-
sume it approximates the optimal weight, i.e., v∗
i
≃
arg minvi∈[0,1] F(v, w). In this paper, we assign binary
labels to v∗
i , where v∗
i = 1 iff yi is a correct label. As
v∗
i is binary, Θ is learned by minimizing the cross-entropy
loss between v∗
i and g(zi; Θ). Intuitively, this process is
similar to a mock test for the teacher (MentorNet) to learn
to update her teaching strategy (curriculum). The student
(StudentNet) provides features φ(·, ·, w) for the mock test
using the latest model w. The teacher can learn an updated
curriculum from the data to better supervise the latest stu-
dent model. The learned curriculum is jointly determined
by the teacher and student together.
The information on the correct label may not always be
available on the target dataset D. In this case, we learn the
curriculum on a different small dataset where the correct
labels are available. Intuitively, it resembles ﬁrst learning a
teaching strategy with the student on one topic and transfer
the strategy on a similar topic. Empirically, Section 5.1
substantiates that the learned curriculum on a small subset of
CIFAR-10 can be applied to the target CIFAR-100 dataset.
A burn-in period is introduced before learning Θ. In the ﬁrst
20% training epoch of the StudentNet, MentorNet is initial-
ized and ﬁxed as gm(zi; Θ∗) = ri, where ri ∼Bernoulli(p)
is the Bernoulli random variable. This is equivalent to ran-
domly dropping out p% training samples. We found that
the burn-in process helps StudentNet stabilize the prediction
and focus on learning simple and common patterns.
MentorNet architecture. We found that MentorNet can
have a simple architecture. Appendix D shows that even
MentorNet based on the two-layer perceptron can reason-
ably approximate the existing curriculum in the literature.
Nevertheless, we use a MentorNet architecture shown in
Fig. 1, which works reasonably well compared to classical
network architectures. It takes the input of a mini-batch of
samples, and outputs their corresponding sample weights.
The feature zi = φ(xi, yi, w) includes the loss, loss dif-
ference to the moving average, label and epoch percentage.
ℓpt maintains an exponential moving average on the p-th
percentile of the loss in each mini-batch. For a sample, its
loss ℓand loss difference ℓ−ℓpt over the last few epochs
can be encoded by a bidirectional LSTM network to capture
the prediction variance (Chang et al., 2017). We verify the
LSTM encoder in the experiments in Appendix D. For sim-
plicity, we set the step size of the LSTM to 1 in Section 5.1
and only consider the loss and the loss difference of the
current epoch.
The label and the training epoch percentage are encoded
by two separate embedding layers. The epoch percentage
is represented as an integer between 0 and 99. It is used
to indicate the StudentNet’s training progress, where 0 rep-

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
...
...
LSTM
emb
StudentNet
MentorNet
LSTM
LSTM
fc1 (tanh)
fc2 (sigmoid)
prob sampling
weights
mini
batch
emb
loss
label
features
weights
training 
epoch 
percentage
Figure 1. The MentorNet architecture used in experiments. emb, fc
and prob sampling stand for the embedding, fully-connected and
probabilistic sampling layer.
resents the ﬁrst and 99 represents the last training epoch.
The concatenated outputs from the LSTM and the embed-
ding layers are fed into two fully-connected layers fc1, fc2,
where fc2 uses the sigmoid activation to ensure the output
weights bounded between 0 and 1. The last layer in Fig. 1
is a probabilistic sampling layer, and is used to implement
the sample dropout in the burn-in process on the already
learned MentorNet.
3.2. Discussions
MentorNet is a general framework for both predeﬁned and
data-driven curriculum learning, where various curriculums
can be learned by the same MentorNet structure with differ-
ent parameters. This framework is conceptually general and
practically ﬂexible as we can switch curriculums by attach-
ing different MentorNets without modifying the pipeline.
Therefore, we also learn MentorNets for predeﬁned curricu-
lums. For predeﬁned curriculums where G is unknown, we
directly minimize the error between the MentorNet’s out-
puts and desired weights. For example, the desired weight
for focal loss (Lin et al., 2017b) is computed by:
v∗
i = [1 −exp{−ℓi}]γ,
(8)
where γ is a hyperparameter for smoothing the distribution.
This paper tackles the problem of overcoming corrupted la-
bels. It is interesting to analyze why the learned curriculum
can improve the generalization performance. It turns out
that StudentNet, when jointly learned with MentorNet, may
optimize an underlying robust objective and the objective is
also related to the robust M-estimator (Huber, 2011).
To show this, let v∗(λ, x) represent the optimal weight func-
tion for a loss variable x, and we deﬁne:
v∗(λ, x) = argminv∈[0,1] vx + G(v, λ).
(9)
As gm is an approximator to Eq. (9), its property can then
be analyzed by the function v∗(λ, x). Meng et al.(2015)
investigated the insights of self-paced objective function,
and proved that the optimization of SPL algorithm is in-
trinsically equivalent to minimizing a robust loss function.
They showed that given a ﬁxed λ and a decreasing v∗(λ, x)
with respect to x, the underlying objective of Eq. (1) can be
obtained by:
Fλ(w) = 1
n
n
X
i=1
Z ℓi
0
v∗(λ, x)dx,
(10)
Based on it, the underlying learning objective of the curricu-
lum in Eq. (5) can then be derived.
Remark 1. When λ1, λ2 are ﬁxed and λ2 ̸= 0, the un-
derlying objective function of the curriculum in Eq. (5) is
calculated from:
Fλ(w)= 1
n
n
X
i=1





ℓi
ℓi ≤λ1
(λ2 + 2λ1)/2
ℓi ≥λ2 + λ1
θℓi−ℓ2
i /(2λ2)−(θ−1)2λ2
2
otherwise
(11)
where θ = (λ2 + λ1)/λ2. When θ = 1 it is equivalent to
the minimax concave penalty (Zhang, 2010).
As shown in Eq. (11), the underlying objective has a form
of Fλ(w) = P
i ρ(ℓi)/n, where ρ is the penalty function
in M-estimator (Candes et al., 2008). Particularly, when
θ = 1, ρ(ℓ) is equivalent to the minimax concave plus
penalty (Zhang, 2010), a popular non-convex robust loss.
The result indicates the learned MentorNet that approx-
imates our predeﬁned curriculum in Eq. (5) leads to an
underlying robust objective of the StudentNet.
For the data-driven curriculum, if the learned MentorNet
satisﬁes certain conditions, we have:
Proposition 1. Suppose (x, y) denotes a training sample
and its corrupted label. For simplicity, let the MentorNet
input φ(x, y, w) = ℓbe the loss computed by the StudentNet
model parameter w. The MentorNet gm(ℓ; Θ) = v, where
v is the sample weight. If gm decreases with respect to ℓ,
then there exists an underlying robust objective F:
F(w) = 1
n
n
X
i=1
ρ(ℓi),
where ρ(ℓi) =
R ℓi
0 gm(x; Θ)dx. In the special cases, ρ(ℓ)
degenerates to the robust M-estimator: Huber (Huber et al.,
1964) and the log-sum penalty (Candes et al., 2008).
-5
-4
-3
-2
-1
0
1
2
3
4
5
0
1
2
3
4
5
Huber
Log Sum
Self-Paced
Learned Curriculum
Figure 2. Visualization of sample loss and learning objective. The
learned curriculum represents the best data-driven curriculum
found in experiments.
The proposition indicates that there exist some learned Men-
torNets that are related to the robust M-estimator. On noisy

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
data, the effect of the robust objective is evident, i.e., prevent-
ing StudentNet from being dominated by corrupted labels.
Fig. 2 visualizes curves of the sample loss ℓ= yi−gs(xi, w)
and the learning objective for the Huber loss (Huber et al.,
1964), log-sum penalty (Candes et al., 2008), self-paced (Ku-
mar et al., 2010), and our learned data-driven curriculum.
We use the best learned curriculum Θ∗on CIFAR-10 in
our experiments and plot |gm(φ(x, y, w); Θ∗) × ℓ| since
the G in the objective function is unknown. As shown, all
curves are robust to great loss to different extents. The cor-
rupted labels in our problem are harmful. As the sample
loss grows bigger beyond some value, MentorNet starts
to sharply decrease the sample’s weight. The subtlety of
learned curriculum is difﬁcult to be predeﬁned by the ana-
lytic expression. Proposition 1 does not guarantee there is
an underlying robust objective for every learned MentorNet.
Instead, it shows MentorNet’s capability of learning such
robust objective.
4. The Algorithm
The alternating minimization algorithm (Csiszar, 1984) used
in related work is intractable for deep CNNs, especially on
big datasets, for two important reasons. First, in the sub-
routine of minimizing w when ﬁxing v, stochastic gradient
descent often takes many steps before converging. This
means that it can take a long time before moving past this
single sub-step. However, such computation is often waste-
ful, particularly in the initial part of training, because, when
v is far away from the optimal point, there is not much
gain in ﬁnding the exact optimal w corresponding to this
v. Second, the subroutine of minimizing v when ﬁxing w
is often difﬁcult, because the ﬁxed vector v may not only
consume a considerable amount of the memory but also
hinder the parallel training on multiple machines. Therefore,
optimizing the objective with deep CNNs requires some
thought on the algorithmic level.
To minimize Eq. (1), we propose an algorithm called SPADE
(Scholastic gradient PArtial DEscent). The algorithm op-
timizes the StudentNet model parameter w jointly with a
given MentorNet. It provides a simple and elegant way to
minimize w and v stochastically over mini-batches. As
a general approach, it can also take an input of G. Let
Ξt = {(xj, yj)}b
j=1 denotes a mini-batch of b samples,
fetched uniformly at random and vt
Ξ = [vt
1, ..., vt
b] represent
the sample weights in Ξt. The MentorNet computes:
vt
Ξ =gm(φ(Ξt, wt−1))=arg min
vΞ F(wt−1,vt−1),
(12)
where φ is the feature extraction function deﬁned in Eq. (3).
Θ denotes the learned MentorNet discussed in Section 3.1.
As shown in Algorithm 1, for w, a stochastic gradient is
computed (via a mini-batch) and applied (Step 12), where
αt is the learning rate. For the latent weight variables v,
gradient descent is only applied to a small subset thereof
parameters corresponding only to the mini-batch (Step 9 or
11). The partial gradient update on weight parameters is
performed when G is used (Step 9). Otherwise, we directly
apply the weights computed by the learned MentorNet (Step
11). In both cases, the weights are computed on-the-ﬂy
within a mini-batch and thus do not need to be ﬁxed. As a
result, the algorithm can be conveniently parallelized across
multiple machines.
Algorithm 1 SPADE for minimizing Eq. (1)
Input
:Dataset D, a predeﬁned G or a learned gm(·; Θ)
Output :The model parameter w of StudentNet.
1 Initialize w0, v0, t = 0
2 while Not Converged do
3
Fetch a mini-batch Ξt uniformly at random
4
For every (xi, yi) in Ξt compute φ(xi, yi, wt)
5
if update curriculum then
6
Θ ←Θ∗, where Θ∗is learned in Sec. 3.1
7
end
8
if G is used then
9
vt
Ξ ←vt−1
Ξ
−αt∇vF(wt−1, vt−1)|Ξt
10
end
11
else vt
Ξ ←gm(φ(Ξt, wt−1); Θ) ;
12
wt ←wt−1 −αt∇wF(wt−1, vt)|Ξt
13
t ←t + 1
14 end
15 return wt
The curriculum can change during training. MentorNet is
updated a few times in Algorithm 1. In Step 6, the Men-
torNet parameter Θ is updated to adapt to the most recent
model parameters of StudentNet. In experiments, we update
Θ twice after the learning rate is changed. Each time, a data-
driven curriculum is learned from the data generated by the
most recent w using the method discussed in Section 3.1.
The update is consistent with existing curriculum learning
methodology (Bengio et al., 2009; Kumar et al., 2010) and
the difference here is that for each update, the curriculum is
learned rather than speciﬁed by human experts.
Under standard assumptions, Theorem 1 shows that the algo-
rithm stabilizes and converges to a stationary point (conver-
gence to global/local minima cannot be guaranteed unless
in specially structured non-convex objectives (Chen et al.,
2018; Zhou et al., 2017b;a)). The proof is in Appendix B.
The theorem is a characterization of stability of the model
parameters w. For the weight parameters v, as it is re-
stricted in a compact set, convergence to a stationary point
is not always guaranteed. As the model parameters are more
important, we only provide a detailed characterization of
the model parameter.
Theorem 1. Let the objective F(w, v) deﬁned in Eq. (1)
be differentiable, L(·) be Lipschitz continuous in w and
∇vG(·) be Lipschitz continuous in v. Let wt, vt be iterates
from Algorithm 1 and P∞
t=0 αt = ∞, P∞
t=0 α2
t < ∞.
Then, limt→∞E[∥∇wF(wt, vt)∥2
2] = 0.
For the manually designed curriculums, it may be unclear

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
where or even whether such predeﬁned curriculum would
converge via mini-batch training. Theorem 1 shows that
the learned curriculum can converge and produce a stable
StudentNet model. The algorithm can be used to replace the
alternating minimization method in related work.
5. Experiments
This section empirically veriﬁes the proposed method on
four benchmarks of controlled corrupted labels in Sec-
tion 5.1 and real-world noisy labels in Section 5.2. The
code can be found at https://github.com/google/
mentornet.
5.1. Experiments on controlled corrupted labels
This section validates MentorNet on the controlled corrupted
label. We follow a common setting in (Zhang et al., 2017a)
to train deep CNNs, where the label of each image is inde-
pendently changed to a uniform random class with probabil-
ity p, where p is noise fraction and is set to 0.2, 0.4 and 0.8.
The labels of validation data remain clean for evaluation.
Dataset and StudentNet: We use the same benchmarks
in (Zhang et al., 2017a): CIFAR-10, CIFAR-100 and Im-
ageNet. CIFAR-10 and CIFAR-100 (Krizhevsky & Hin-
ton, 2009) consist of 32 × 32 color images arranged in 10
and 100 classes. Both datasets contain 50,000 training and
10,000 validation images. ImageNet ILSVRC2012 (Deng
et al., 2009) contain about 1.2 million training and 50k
validation images, split into 1,000 classes. Each image is
resized to 299x299 with 3 color channels.
We employ 3 recent deep CNNs as our StudentNets: incep-
tion (Szegedy et al., 2016), resnet-101 (He et al., 2016) with
wide ﬁlters (Zagoruyko & Komodakis, 2016) and inception-
resnet v2 (Szegedy et al., 2017). Table 1 shows their #model
parameters, training, and validation accuracy when we train
them on the clean training data (noise= 0). As shown, they
achieve reasonable accuracy on each task.
Table 1. StudentNet and their accuracies on the clean training data.
Dataset
Model
#para
train acc
val acc
CIFAR10
inception
1.7M
0.83
0.81
resnet101
84M
1.00
0.96
CIFAR100
inception
1.7M
0.64
0.49
resnet101
84M
1.00
0.79
ImageNet
inception resnet
59M
0.88
0.77
Baselines: MentorNet is compared against the following
baselines: FullMode is the standard StudentNet trained us-
ing l2 weight decay, dropout (Srivastava et al., 2014) and
data augmentation (Krizhevsky et al., 2012). The hyper-
parameters are set to the best ones found on the clean
training data. Unless speciﬁed otherwise, for a fair com-
parison, the StudentNet with the same hyperparameters is
used in all baseline and our model. Forgetting was intro-
duced in (Arpit et al., 2017), in which the dropout parameter
is searched in the range of (0.2-0.9). Self-paced (Kumar
et al., 2010) and Focal Loss (Lin et al., 2017b) represent
well-known predeﬁned curriculums in the literature. We
implemented Reed (2014) and Goldberger (Goldberger &
Ben-Reuven, 2017) as the recent weakly-supervised learn-
ing methods. The above baseline methods are a mixture of
the curriculum learning and the recent methods dealing with
corrupted labels.
Our Model: MentorNet PD is the network learned using
our predeﬁned curriculum in Eq. (5) using no additional
clean labels. MentorNet DD is the learned data-driven
curriculum. It is trained on 5,000 images of true labels, ran-
domly sampled from the CIFAR-10 training set. The same
data are used to learn MentorNet DD on CIFAR-100. Note
CIFAR-10 and CIFAR-100 are two different datasets that
have not only different classes but also the different num-
ber of classes. Therefore, it is fair to compare MentorNet
DD with other methods using no true labels on CIFAR-100.
Algorithm 1 is used to optimize the StudentNet. The de-
cay factor in computing the loss moving average is set to
0.95. The loss percentile in the moving average is set by the
cross-validation. As mentioned, a burn-in process is used
in the ﬁrst 20% training epoch for both MentorNet DD and
MentorNet PD. More details are discussed in Appendix E.
We ﬁrst show the comparison to the baseline method on
CIFAR-10 and CIFAR-100 in Table 2. On both datasets,
each method is veriﬁed with two StudentNets (resnet-101
and inception) under the noise fraction of 0.2, 0.4, and 0.8.
As we see on both datasets, MentorNet improves FullModel
across different noise fractions, and the learned data-driven
curriculum (MentorNet DD) achieves the best results. The
improvement is more signiﬁcant for the deeper CNN model
resnet-101. For example, on the CIFAR-10 of 40% noise,
MentorNet DD (with resnet-101) yields an absolute 20%
gain over FullModel. After inspecting the result, we found
that it may be because Mentor DD learns a more appropriate
curriculum to give high weights to samples of correct labels.
As a result, it helps the StudentNet focus on samples of
correct labels. The results indicate that the learned Mentor-
Net can improve the generalization performance of recent
deep CNNs, and outperform the predeﬁned curriculums
(Self-paced and Focal Loss).
Fig. 3 plots the training and test error on the clean validation
data, under a representative setting: resnet-101 on CIFAR-
100 of 40% noise, where the x-axis denotes the training
iteration. The y-axis is the validation error on the clean
validation in Fig. 3(a) and the mini-batch training error on
corrupted labels in Fig. 3(b). For MentorNet, the training
error is computed by P
i viℓi. The ﬁgure shows two insights.
First, the training error of MentorNet approaches zero. This
empirically veriﬁes the convergence of the model. Second,
MentorNet can overcome the overﬁtting to the corrupted
label. While the training error is decreasing, the test error

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
Table 2. Comparison of validation accuracy on CIFAR-10 and CIFAR-100 under different noise fractions.
Resnet-101 StudentNet
Inception StudentNet
CIFAR-100
CIFAR-10
CIFAR-100
CIFAR-10
Method
0.2
0.4
0.8
0.2
0.4
0.8
0.2
0.4
0.8
0.2
0.4
0.8
FullModel
0.60
0.45
0.08
0.82
0.69
0.18
0.43
0.38
0.15
0.76
0.73
0.42
Forgetting
0.61
0.44
0.16
0.78
0.63
0.35
0.42
0.37
0.17
0.76
0.71
0.44
Self-paced
0.70
0.55
0.13
0.89
0.85
0.28
0.44
0.38
0.14
0.80
0.74
0.33
Focal Loss
0.59
0.44
0.09
0.79
0.65
0.28
0.43
0.38
0.15
0.77
0.74
0.40
Reed Soft
0.62
0.46
0.08
0.81
0.63
0.18
0.42
0.39
0.12
0.78
0.73
0.39
MentorNet PD
0.72
0.56
0.14
0.91
0.77
0.33
0.44
0.39
0.16
0.79
0.74
0.44
MentorNet DD
0.73
0.68
0.35
0.92
0.89
0.49
0.46
0.41
0.20
0.79
0.76
0.46
0.5
1
1.5
2
2.5
3
3.5
4
Training steps (batch size=128)
10 4
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Error on clean validation
FullModel
Fogetting
MentorNet PD
MentorNet DD
(a) Test error on clean validation
0.5
1
1.5
2
2.5
3
3.5
Training steps (batch size=128)
10 4
0
1
2
3
4
5
6
Training error on corrupted data
FullModel
Fogetting
MentorNet PD
MentorNet DD
(b) Training error on corrupted labels
Figure 3. Error of resnet trained on CIFAR-100 of 40% noise.
does not increase in Fig. 3(a). It suggests that the learned
curriculum is beneﬁcial for StudentNet. The sharp change
around the 20k iteration in Fig. 3 is due to the learning rate
change. Besides, our result is consistent with (Zhang et al.,
2017a) that deep CNNS is able to get 0 training error on
the corrupted training data. Forgetting (the dashed curve)
is the only one that does not converge within 30k steps.
As indicated in (Arpit et al., 2017), it is because forgetting
reduces the speed at which DNNs memorize. As suggested
in (Zhang et al., 2017b), a not converged model might yield
a better result, e.g., stop the model at 20K in Fig. 3. However,
as it is hard to predetermine the time for early stopping, our
focus is comparing the converged model.
Fig. 4 illustrates the best learned data-driven curriculum
in our experiments, where the z-axis denotes the weights
computed by gm; the y and x axes denote the sample loss
and the loss difference to the moving average, where λ is
the loss moving average. Two observations can be found
in Fig. 4. First, the learned curriculum changes during the
training of the StudentNet. Fig. 4 (a) and (b) are MentorNet
learned at different epochs. As shown, (a) assigns greater
weights to samples of big loss more aggressively. Second,
the learned curriculums in Fig. 4 generally satisfy the con-
dition in Proposition 1, i.e., the weight generally decreases
with the loss. It suggests that joint learning of StudentNet
and MentorNet optimizes an underlying robust objective.
weight
 weight
(a) epoch percentage=21
(b) epoch percentage=76
Figure 4. The data-driven curriculums learned by MentorNet with
the resnet-101 at epoch 21 in (a) and 76 in (b).
Table 3 compares to recent published results under the set-
ting: CIFAR of 40% noise fraction. We cite the number
in (Azadi et al., 2016), and implement other methods using
the same resnet-101 StudentNet. The results show that our
result is comparable and even better than the state-of-the-art.
Table 3. Validation accuracy comparison to representative pub-
lished results on CIFAR of 40% noise.
ID
Method
CIFAR-10
CIFAR-100
1
Reed Hard (Resnet)
0.62
0.47
2
Reed Soft (Resnet)
0.61
0.46
3
Goldberger (Resnet)
0.70
0.46
4
Azadi (AlexNet) (2016)
0.75
-
5
MentorNet (Resnet)
0.89
0.68
To verify MentorNet for large-scale training, we apply our
method on the ImageNet ILSVRC12 (Deng et al., 2009)
benchmark to improve the inception-resnet v2 (Szegedy
et al., 2017) model. We train the model on the ImageNet
of 40% noise. Inspired by (Zhang et al., 2017a), we start
with an inception-resnet (NoReg) with no regularization
(NoReg) and add weight decay, dropout, and data augmen-
tation to the model. Table 4 shows the comparison. As
shown, MentorNet improves the performance of both the
inception-resnet without regularization (NoReg) and with
full regularization (FullModel). It also outperforms the
forgetting baseline (dropout keep probability = 0.2). The
results suggest that MentorNet can improve deep CNNs on
the large-scale training on corrupted labels.
5.2. Experiments on real-world noisy labels
To verify MentorNet on real-world noisy labels, we conduct
experiments on the large WebVision benchmark (Li et al.,

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
Table 4. Accuracy on the clean ImageNet ImageNet validation set.
Models are trained on the ImageNet training data of 40% noise.
Method
P@1
P@5
NoReg
0.538
0.770
NoReg+WeDecay
0.560
0.809
NoReg+Dropout
0.575
0.807
NoReg+DataAug
0.522
0.772
NoReg+MentorNet
0.590
0.814
FullModel
0.612
0.844
Forgetting(FullModel)
0.628
0.845
MentorNet(FullModel)
0.651
0.859
2017a). It contains 2.4 million images of real-world noisy
labels, crawled from the web using the 1,000 concepts in Im-
ageNet ILSVRC12. We download the resized images from
the ofﬁcial website1. The inception-resenet v2 (Szegedy
et al., 2017) is used as our StudentNet, trained using a dis-
tributed asynchronized momentum optimizer on 50 GPUs.
Since the dataset is very big, for quick experiments, we com-
pare baseline methods using the Google image subset on the
ﬁrst 50 classes. We use Mini to denote this subset and Entire
for the entire WebVision. All the models are evaluated on
the clean ILSVRC12 and WebVision validation set.
Table 5 lists the comparison result. As we see, the pro-
posed MentorNet signiﬁcantly improves baseline methods
on real-world noisy labels. The method marked by the
start indicates it uses a pre-trained ImageNet model to ob-
tain additional 30k labels for 118 classes. Following the
same protocol, MentorNet* is trained using the additional
labels. The results show that our method outperforms the
baseline methods on real-world noisy labels. To the best of
our knowledge, it achieves the best-published result on the
WebVision (Li et al., 2017a) benchmark.
Table 5. Validation accuracy on the ImageNet ILSVRC12 and Web-
Vision validation set. The number outside (inside) the parentheses
denotes top-1 (top-5) classiﬁcation accuracy (%). * marks the
method trained using additional veriﬁcation labels.
Dataset
Method
ILSVRC12
WebVision
Entire
Li et al. (2017a)
0.476 (0.704)
0.570 (0.779)
Entire
Forgetting
0.590 (0.808)
0.666 (0.856)
Entire
Lee et al. (2017)*
0.602 (0.811)
0.685 (0.865)
Entire
MentorNet
0.625 (0.830)
0.708 (0.880)
Entire
MentorNet*
0.642 (0.848)
0.726 (0.889)
Mini
FullModel
0.585 (0.818)
-
Mini
Forgetting
0.562 (0.816)
-
Mini
Reed Soft
0.565 (0.827)
-
Mini
Self-paced
0.576 (0.822)
-
Mini
MentorNet
0.638 (0.858)
-
6. Related Work
Curriculum learning (CL), proposed by Bengio et al. (2009),
is a learning paradigm in which a model is learned by grad-
ually including from easy to complex samples in training
so as to increase the learning entropy (Bengio et al., 2009).
From the human behavioral perspective, Khan et al. (2011)
have shown that CL is consistent with the principle of hu-
1https://www.vision.ee.ethz.ch/webvision/download.html
man teaching. CL has been empirically veriﬁed in a va-
riety of problems, such as computer vision (Supancic &
Ramanan, 2013; Chen & Gupta, 2015), natural language
processing (Turian et al., 2010), multitask learning (Graves
et al., 2017). A common CL approach is to predeﬁne a
curriculum. For example, Kumar et al. (2010) proposed a
curriculum called self-paced learning which favors training
samples of smaller loss. After that, many predeﬁned curricu-
lums were proposed, e.g., in (Supancic & Ramanan, 2013;
Jiang et al., 2014; 2015; Sangineto et al., 2016; Chang et al.,
2017; Ma et al., 2017a;b). For example, Jiang et al. (2014)
introduced a curriculum of using easy and diverse sam-
ples. Fan et al. (2017) proposed to use predeﬁned sample
weighting schemes as an implicit way to deﬁne a curriculum.
Previous work has shown that predeﬁned curriculums are
useful in overcoming noisy labels (Chen & Gupta, 2015;
Liang et al., 2016; Lin et al., 2017a). In parallel to CL, the
sample weighting schemes were also studied in (Lin et al.,
2017a; Wang et al., 2017; Fan et al., 2018; Dehghani et al.,
2018). Compared to the existing work, our paper presents
a new way of learning data-driven curriculums for deep
networks trained on corrupted labels.
Our work is related to the weakly-supervised learning meth-
ods. Among recent contributions, Reed et al. (2014) de-
veloped a robust loss to model “prediction consistency”.
Menon et al. (2015) used class-probability estimation to
study the corruption process. Sukhbaatar et al. (2014) pro-
posed a noise transformation to estimate the noise distribu-
tion. The transformation matrix needs to be periodically
updated and is non-trivial to learn. To address the issue,
Goldberger et al. (2017) proposed to add an additional
softmax layer end-to-end with the base model. Azadi et
al. (2016) tackled this problem by a regularizer called AIR.
This method was shown to be effective but it relied on addi-
tional clean labels to train the representation. More recently,
methods utilized additional labels for label cleaning (Veit
et al., 2017), knowledge distillation (Li et al., 2017b) or
semi-supervised learning (Vahdat, 2017; Dehghani et al.,
2017). Different from previous work, we focus on learn-
ing curriculum to train very deep CNNs on corrupted labels
from scratch. In addition, clean labels are not always needed
for our method. In Section 5.1, the MentorNet is learned on
a small subset of CIFAR-10 and applied to CIFAR-100
7. Conclusions
In this paper, we presented a novel method for training deep
CNNs on corrupted labels. Our work was built on curricu-
lum learning and advanced the methodology by proposing
to learn data-driven curriculum via a neural network called
MentorNet. We proposed an algorithm for jointly optimiz-
ing deep CNNs with MentorNet on large-scale data. We
conducted comprehensive experiments on datasets of con-
trolled and real-world noise. Our empirical results showed

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
that generalization performance of deep CNNs trained on
corrupted labels can be effectively improved by the learned
data-driven curriculum.
Acknowledgements
The authors would like to thank anonymous reviewers for
helpful comments and Deyu Meng, Sergey Ioffe, and Chong
Wang for meaningful discussions and kind support.
References
Arpit, D., Jastrzkebski, S., Ballas, N., Krueger, D., Bengio,
E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville, A.,
Bengio, Y., et al. A closer look at memorization in deep
networks. In ICML, 2017.
Azadi, S., Feng, J., Jegelka, S., and Darrell, T. Auxiliary im-
age regularization for deep cnns with noisy labels. ICLR,
2016.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
Curriculum learning. In ICML, 2009.
Candes, E. J., Wakin, M. B., and Boyd, S. P. Enhancing spar-
sity by reweighted l1 minimization. Journal of Fourier
analysis and applications, 14(5-6):877–905, 2008.
Chang, H.-S., Learned-Miller, E., and McCallum, A. Ac-
tive bias: Training a more accurate neural network by
emphasizing high variance samples. NIPS, 2017.
Chen, X. and Gupta, A.
Webly supervised learning of
convolutional networks. In ICCV, 2015.
Chen, Y., Chi, Y., Fan, J., and Ma, C. Gradient descent with
random initialization: Fast global convergence for non-
convex phase retrieval. arXiv preprint arXiv:1803.07726,
2018.
Csiszar, I. Information geometry and alternating minimiza-
tion procedures.
Statistics and decisions, 1:205–237,
1984.
Dehghani, M., Severyn, A., Rothe, S., and Kamps, J.
Avoiding your teacher’s mistakes: Training neural net-
works with controlled weak supervision. arXiv preprint
arXiv:1711.00313, 2017.
Dehghani, M., Mehrjou, A., Gouws, S., Kamps, J., and
Schlkopf, B. Fidelity-weighted learning. In ICLR, 2018.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In CVPR, 2009.
Fan, Y., He, R., Liang, J., and Hu, B.-G. Self-paced learning:
An implicit regularization perspective. In AAAI, 2017.
Fan, Y., Tian, F., Qin, T., Li, X.-Y., and Liu, T.-Y. Learning
to teach. In ICLR, 2018.
Goldberger, J. and Ben-Reuven, E. Training deep neural-
networks using a noise adaptation layer. In ICLR, 2017.
Graves, A., Bellemare, M. G., Menick, J., Munos, R., and
Kavukcuoglu, K. Automated curriculum learning for
neural networks. In ICML, 2017.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In CVPR, 2016.
Huber, P. J. Robust statistics. In International Encyclopedia
of Statistical Science, pp. 1248–1251. Springer, 2011.
Huber, P. J. et al. Robust estimation of a location parame-
ter. The Annals of Mathematical Statistics, 35(1):73–101,
1964.
Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Haupt-
mann, A. Self-paced learning with diversity. In NIPS,
2014.
Jiang, L., Meng, D., Zhao, Q., Shan, S., and Hauptmann,
A. G. Self-paced curriculum learning. In AAAI, 2015.
Khan, F., Mutlu, B., and Zhu, X. How do humans teach:
On curriculum learning and teaching dimension. In NIPS,
2011.
Krizhevsky, A. and Hinton, G. Learning multiple layers of
features from tiny images. 2009.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In NIPS, 2012.
Kumar, M. P., Packer, B., and Koller, D. Self-paced learning
for latent variable models. In NIPS, 2010.
Lee, K.-H., He, X., Zhang, L., and Yang, L. Cleannet:
Transfer learning for scalable image classiﬁer training
with label noise. arXiv preprint arXiv:1711.07131, 2017.
Li, W., Wang, L., Li, W., Agustsson, E., and Van Gool, L.
Webvision database: Visual learning and understanding
from web data. arXiv preprint arXiv:1708.02862, 2017a.
Li, Y., Yang, J., Song, Y., Cao, L., Li, J., and Luo, J. Learn-
ing from noisy labels with distillation. In ICCV, 2017b.
Liang, J., Jiang, L., Meng, D., and Hauptmann, A. G. Learn-
ing to detect concepts from webly-labeled video data. In
IJCAI, 2016.
Lin, L., Wang, K., Meng, D., Zuo, W., and Zhang, L. Active
self-paced learning for cost-effective and progressive face
identiﬁcation. TPAMI, 2017a.

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll´ar, P.
Focal loss for dense object detection. ICCV, 2017b.
Ma, F., Meng, D., Xie, Q., Li, Z., and Dong, X. Self-paced
co-training. In ICML, 2017a.
Ma, Z., Liu, S., and Meng, D.
On convergence prop-
erty of implicit self-paced objective.
arXiv preprint
arXiv:1703.09923, 2017b.
Meng, D., Zhao, Q., and Jiang, L. What objective does
self-paced learning indeed optimize?
arXiv preprint
arXiv:1511.06049, 2015.
Menon, A., Van Rooyen, B., Ong, C. S., and Williamson,
B.
Learning from corrupted binary labels via class-
probability estimation. In ICML, 2015.
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro,
N. Exploring generalization in deep learning. In NIPS,
2017.
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and
Rabinovich, A. Training deep neural networks on noisy la-
bels with bootstrapping. arXiv preprint arXiv:1412.6596,
2014.
Sangineto, E., Nabi, M., Culibrk, D., and Sebe, N. Self
paced deep learning for weakly supervised object detec-
tion. arXiv preprint arXiv:1605.07651, 2016.
Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: a simple way to prevent
neural networks from overﬁtting. Journal of machine
learning research, 15(1):1929–1958, 2014.
Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., and Fer-
gus, R. Training convolutional networks with noisy labels.
arXiv preprint arXiv:1406.2080, 2014.
Supancic, J. S. and Ramanan, D. Self-paced learning for
long-term tracking. In CVPR, 2013.
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,
Z. Rethinking the inception architecture for computer
vision. In CVPR, 2016.
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. A.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, 2017.
Turian, J., Ratinov, L., and Bengio, Y. Word representations:
a simple and general method for semi-supervised learning.
In ACL, 2010.
Vahdat, A. Toward robustness against label noise in training
deep discriminative neural networks. In NIPS, 2017.
Veit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., and
Belongie, S. Learning from noisy large-scale datasets
with minimal supervision. In CVPR, 2017.
Wang, Y., Kucukelbir, A., and Blei, D. M. Robust prob-
abilistic modeling with bayesian data reweighting. In
ICML, 2017.
Zagoruyko, S. and Komodakis, N. Wide residual networks.
In BMVC, 2016.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
Understanding deep learning requires rethinking general-
ization. In ICLR, 2017a.
Zhang, C.-H. Nearly unbiased variable selection under
minimax concave penalty. The Annals of Statistics, pp.
894–942, 2010.
Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz,
D. mixup: Beyond empirical risk minimization. arXiv
preprint arXiv:1710.09412, 2017b.
Zhou, Z., Mertikopoulos, P., Bambos, N., Boyd, S., and
Glynn, P. Mirror descent in non-convex stochastic pro-
gramming. arXiv preprint arXiv:1706.05681, 2017a.
Zhou, Z., Mertikopoulos, P., Bambos, N., Boyd, S., and
Glynn, P. W. Stochastic mirror descent in variationally
coherent optimization problems. In NIPS, 2017b.

Supplementary Materials:
MentorNet Learning Data-Driven Curriculum
for Very Deep Neural Networks on Corrupted Labels
Lu Jiang Zhengyuan Zhou Thomas Leung Li-Jia Li Li Fei-Fei
A. Derivation of Remark 1
Our objective function is:
min
w∈Rd,v∈[0,1]nF(w,v) = 1
n
n
X
i=1
viL(yi,gs(xi,w)) + G(v; λ) + θ∥w∥2
2
(1)
Let ℓi = L(yi, gs(xi, w)) denote the loss of the i-th sample (ℓi ≥0). The predeﬁned curriculum is deﬁned as:
G(v; λ) =
n
X
i=1
1
2λ2v2
i −(λ2 + λ1)vi.
(2)
Denote Fw as the objective function when the w is ﬁxed. We have
Fw(v) = 1
n
n
X
i=1
viℓi + G(v; λ) + θ∥w∥2
2
= 1
n
n
X
i=1
viℓi + 1
2λ2v2
i −(λ2 + λ1)vi + θ∥w∥2
2
= 1
n
n
X
i=1
f(vi) + θ∥w∥2
2
(3)
where f(vi) = viℓi + 1
2λ2v2
i −(λ2 + λ1)vi.
As f(vi) is convex with respect to vi (λ2 ≥0). Its minimum is obtained at
arg
min
v∈[0,1]n ∇vFw(v) = 0
⇒∂f(vi)
∂vi
= ℓi + λ2vi −λ2 −λ1 = 0, (∀i ∈[1, n], vi ∈[0, 1])
(4)
Since λ1, λ2 ≥0 and vi is bounded in [0, 1]. When λ2 ̸= 0, the optimal v∗
i is calculated from:
v∗
i =





1
(ℓi ≤λ1) ∧(λ2 ̸= 0)
1 −ℓi−λ1
λ2
(λ1 < ℓi < λ2 + λ1) ∧(λ2 ̸= 0)
0
(ℓi ≥λ2 + λ1) ∧(λ2 ̸= 0)
,
(5)
When λ2 = 0, the optimal weight writes as:
v∗
i =
(
1
(ℓi < λ1) ∧(λ2 = 0)
0
(ℓi ≥λ1) ∧(λ2 = 0) .
(6)
arXiv:1712.05055v2  [cs.CV]  13 Aug 2018

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
Combined Eq. (5) and Eq. (6), we have
v∗
i =
(
1(ℓi ≤λ1)
λ2 = 0
min(max(0, 1 −ℓi−λ1
λ2
), 1)
λ2 ̸= 0
(7)
According to the deﬁnition of Θ, we have gm(φ(xi, yi, w); Θ∗) = v∗
i . Incorporating Eq. (7), we have
gm(φ(xi, yi, w); Θ∗) =
(
1(ℓi ≤λ1)
λ2 = 0
min(max(0, 1 −ℓi−λ1
λ2
), 1)
otherwise
(8)
Now we derive its underlying objective. First, we deﬁne a function v∗(λ, x) and incorporate the above optimal solution:
v∗(λ, x) = arg min
v∈[0,1] vx + G(v, λ) =
(
1(x ≤λ1)
λ2 = 0
min(max(0, 1 −x−λ1
λ2 ), 1)
otherwise
(9)
Let ϵ > 0 denote a small positive constant. Using the condition λ2 ≥0, we incorporate Eq. (7):
v∗(λ, x + ϵ) −v∗(λ, x) ≤0
(10)
That indicates v∗(λ, x) decreases with respect to x. According to (Meng et al., 2015), given the ﬁxed hyperparameter λ
(i.e. λ1, λ2), its underlying objective function has the form of:
Fλ(w) = 1
n
n
X
i=1
Z ℓi
0
v∗(λ; x)dx,
(11)
After incorporating Eq. (5) and Eq. (6) into Eq. (11), we have when λ2 = 0
Fλ(w) = 1
n
n
X
i=1
min(ℓi, λ1)
(12)
When λ2 ̸= 0, we have:
Fλ(w) = 1
n
n
X
i=1





ℓi
ℓi ≤λ1
ℓi + λ1
λ2 ℓi −
1
2λ2 ℓ2
i −λ2
1
2λ2
λ1 < ℓi < λ2 + λ1
λ2+2λ1
2
ℓi ≥λ2 + λ1
= 1
n
n
X
i=1





ℓi
ℓi ≤λ1
θℓi −ℓ2
i /(2λ2) −(θ−1)2λ2
2
λ1 < ℓi < λ2 + λ1
(λ2 + 2λ1)/2
ℓi ≥λ2 + λ1
,
(13)
where θ = (λ2 + λ1)/λ2 and λ1, λ2 ≥0. We λ2 ̸= 0, we have the Eq.(10) in the Remark 1 in the paper.
Fλ(w) = 1
n
n
X
i=1





ℓi
ℓi ≤λ1
(λ2 + 2λ1)/2
ℓi ≥λ2 + λ1
θℓi −ℓ2
i /(2λ2) −(θ−1)2λ2
2
otherwise
,
(14)
When θ = 1 we have λ1 = 0 and the above equation becomes:
Fλ(w) = 1
n
n
X
i=1
(
ℓi −ℓ2
i /(2λ2)
ℓi < λ2
λ2/2
ℓi ≥λ2
.
(15)
The above equation is equivalent to the minimax concave penalty (MCP) (Gong et al., 2013; Zhang, 2010). Below is its
formula presented in (Gong et al., 2013) (with the regularization hyperparameter setting to 1):
Z ℓ
0
[1 −x
t ]dx =
(
ℓ−ℓ2/(2t)
ℓ< t
t/2
ℓ≥t,
(16)

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
B. Proof of Theorem 1
Theorem 1 Let the objective F(w, v) deﬁned in Eq. (1) be differentiable, L(·) be Lipschitz continuous in w and ∇vG(·)
be Lipschitz continuous in v. Let wt, vt be iterates from Algorithm 1 and P∞
t=0 αt = ∞, P∞
t=0 α2
t < ∞. Then,
limt→∞E[∥∇wF(wt, vt)∥2
2] = 0.
Proof 1 First, by the deﬁnition of the objective function F(w, v), it can be easily checked that L(·) being Lipschitz
continuous in w implies that ∇wF(w, v) is a Lipschitz function in w for every v. Similarly, ∇vG(·) being Lipschitz
continuous in v implies that ∇vF(w, v) is a Lipschitz function in v for all w. Further, without loss of generality, we assume
all the Lipschitz constants are (upper bounded by) L.
Throughout the proof, as in the main text, n is the size of the training dataset. Deﬁne the n-dimensional vector ek
as et
i = 1 if (xi, yi) ∈Ξt and 0 otherwise and denote by ⊗the point-wise product operation between two vectors:
[a1, a2] ⊗[b1, b2] = [a1b1, a2b2]. When G is used, the update in each iteration is two consecutive gradient steps as follows:
wt+1
=
wt −αt∇wF(wt, vt)|Ξt,
vt+1
=
vt −αt∇vF(wt+1, vt)|Ξt.
Since the mini-batch Ξt is draw uniformly at random, we can rewrite the update as:
wt+1
=
wt −αt[∇wF(wk, vt) + ξt],
vt+1
=
vt −αt[et ⊗∇vF(wt+1, vt)],
where ξt = ∇wF(wt, vt)|Ξt −∇wF(wk, vt). Note that both ξt and et are iid random variables with ﬁnite variance, since
Ξt are drawn iid with a ﬁnite number (b) of samples. Further, E[∇wF(wt, vt)|Ξt −∇wF(wk, vt)] = 0, since samples are
drawn uniformly at random.
By Lipschitz continuity of ∇wF(w, v) (and L being the Lipschitz constant), we obtain the following:
F(wt+1, vt) −F(wt, vt) ≤⟨∇wF(wt, vt), wt+1 −wt⟩+ L
2 ∥wt+1 −wt∥2
2
= ⟨∇wF(wt, vt), −αt[∇wF(wt, vt) + ξt]⟩+ L
2 ∥wt+1 −wt∥2
2
= −αt{∥∇wF(wt, vt)∥2
2 + ⟨∇wF(wt, vt), ξt⟩} + L
2 ∥wt+1 −wt∥2
2
= −αt{∥∇wF(wt, vt)∥2
2 + ⟨∇wF(wt, vt), ξt⟩} + Lα2
t
2 ∥∇wF(wt, vt) + ξt∥2
2
= −(αt −Lα2
t
2 )∥∇wF(wt, vt)∥2
2 + Lα2
t
2 ∥ξt∥2
2 −(αt −Lα2
t)⟨∇wF(wt, vt), ξt⟩.
Similarly, by the Lipschitz continuity of ∇vF(w, v), we have:
F(wt+1, vt+1) −F(wt+1, vt) ≤⟨∇vF(wt+1, vt), vt+1 −vt⟩+ L
2 ∥vt+1 −vt∥2
2
= ⟨∇vF(wt+1, vt), −αtet ⊗∇vF(wt+1, vt)⟩+ L
2 ∥vt+1 −vt∥2
2
= −αt⟨∇vF(wt+1, vt), ek ⊗∇vF(wt+1, vt)⟩+ Lα2
t
2 ∥et ⊗∇vF(wt+1, vt)∥2
2.
Note that when G is not used, the bound for F(wt+1, vt) −F(wt, vt) is still the same, but the bound for F(wt+1, vt+1) −
F(wt+1, vt) is now simply F(wt+1, vt+1) −F(wt+1, vt) ≤0.
Combining the above two equations, we then have:

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
1. If G is used,
F(wt+1, vt+1) −F(wt, vt) = F(wt+1, vt+1) −F(wt+1, vt) + F(wt+1, vt) −F(wt, vt)
≤−(αt −Lα2
t
2 )∥∇wF(wt, vt)∥2
2 + Lα2
t
2 ∥ξt∥2
2 −(αt −Lα2
t)⟨∇wF(wt, vt), ξt⟩
−αt⟨∇vF(wt+1, vk), et ⊗∇vF(wt+1, vt)⟩+ Lα2
t
2 ∥et ⊗∇vF(wt+1, vt)∥2
2.
If G is not used,
F(wt+1, vt+1) −F(wt, vt) ≤F(wt+1, vt) −F(wt, vt)
≤−(αt −Lα2
t
2 )∥∇wF(wt, vt)∥2
2 + Lα2
t
2 ∥ξt∥2
2 −(αt −Lα2
t)⟨∇wF(wt, vt), ξt⟩
Taking expectation of both sides and since E[ξt] = 0, we have if G is used:
E[F(wt+1, vt+1)] −E[F(wt, vt)]
≤−(αt −Lα2
t
2 )E[∥∇wF(wt, vt)∥2
2] + Lα2
t
2 E[∥ξt∥2
2] −αtE[⟨∇vF(wt+1, vt), et ⊗∇vF(wt+1, vt)⟩]
+ Lα2
t
2 E[∥et ⊗∇vF(wt+1, vt)∥2
2]
= −(αt −Lα2
t
2 )E[∥∇wF(wt, vt)∥2
2] + Lα2
t
2 E[∥ξt∥2
2] −αtb
n E[∥∇vF(wt+1, vt)∥2
2]
+ Lα2
t
2
n
X
i=1
E[∥et
i
∂viF(wt+1, vt)
∂vi
∥2
2]
≤−(αt −Lα2
t
2 )E[∥∇wF(wt, vt)∥2
2] + Lα2
t
2 E[∥ξt∥2
2] −αtb
n E[∥∇vF(wt+1, vt)∥2
2]
+ Lα2
t
2
n
X
i=1
E[∥∂viF(wt+1, vt)
∂vi
∥2
2]
= −αtE[∥∇wF(wt, vt)∥2
2] −αtb
n E[∥∇vF(wt+1, vt)∥2
2]
+ Lα2
t
2 {E[∥∇wF(wt, vt)∥2
2] + E[∥∇vF(wt+1, vt)∥2
2] + E[∥ξt∥2
2].}
where the second equality follows from E[⟨∇vF(wt+1, vt), et ⊗∇vF(wt+1, vt)⟩] = b
nE[∥∇vF(wt+1, vt)∥2
2], which can
be checked by a straightforward combinatorial argument.
Following a similar chain of steps, we have the following bound if G is not used: E[F(wt+1, vt+1)] −E[F(wt, vt)] ≤
−αtE[∥∇wF(wt, vt)∥2
2]+ Lα2
t
2 {E[∥∇wF(wt, vt)∥2
2]+E[∥ξt∥2
2].} Since there are only a ﬁnite number of training samples,
all the random quantities have bounded support and all the second moments are upper bounded, leading to E[∥ξt∥2
2] <
∞, E[∥∇wF(wt, vt)∥2
2] < ∞, E[∥∇vF(wt+1, vt)∥2
2] < ∞, and let B be an upper bound on E[∥∇wF(wt, vt)∥2
2] +
E[∥∇vF(wt+1, vt)∥2
2] + E[∥ξt∥2
2].
By telescoping, if G is used, we have: E[F(wT +1, vT +1)] −F(w0, v0) = PT
k=0

E[F(wk+1, vk+1)] −E[F(wk, vk)]
	
≤−PT
k=0 αkE[∥∇wF(wk, vk)∥2
2]−PT
k=0
αkb
n E[∥∇vF(wk+1, vk)∥2
2]+ LB
2
PT
k=0 α2
k. Taking the limit T →∞of both
sides, we obtain:
−
∞
X
k=0
αkE[∥∇wF(wk, vk)∥2
2] −b
n
∞
X
k=0
αkE[∥∇vF(wk+1, vk)∥2
2] + LB
2
∞
X
k=0
α2
k
≥lim
T →∞E[F(wT +1, vT +1)] −E[F(w0, v0)] ≥min
w,v F(w, v)] −F(w0, v0) > −∞.

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
Since P∞
k=0 α2
k
< ∞, the above inequality immediately implies that P∞
k=0 αkE[∥∇wF(wk, vk)∥2
2] < ∞and
P∞
k=0 αkE[∥∇vF(wk+1, vk)∥2
2] < ∞.
If G is not used, then by a similar argument, we have P∞
k=0 αkE[∥∇wF(wk, vk)∥2
2] < ∞.
By Lemma A.5 in (Mairal, 2013), to show limk→∞E[∥∇wF(wk, vk)∥2
2] = 0, since αk is not summable, it sufﬁces to show
E[∥∇wF(wk+1, vk+1)∥2
2] −E[∥∇wF(wk, vk)∥2
2]
 ≤Cαk for some constant C. To do so, we ﬁrst recall a useful fact:
for any two vectors a, b and any ﬁnite-dimensional vector norm ∥· ∥,
(∥a∥+ ∥b∥)(∥a∥−∥b∥)
 ≤∥a + b∥∥a −b∥.
(17)
We have:
E[∥∇wF(wt+1, vt+1)∥2
2] −E[∥∇wF(wt, vt)∥2
2]

=
E
 ∥∇wF(wt+1, vt+1)∥2 + ∥∇wF(wt, vt)∥2
 ∥∇wF(wt+1, vt+1)∥2 −∥∇wF(wt, vt)∥2

≤E
∥∇wF(wt+1, vt+1)∥2 + ∥∇wF(wt, vt)∥2
 ·
∥∇wF(wt+1, vt+1)∥2 −∥∇wF(wt, vt)∥2


≤E
∇wF(wt+1, vt+1) + ∥∇wF(wt, vt)

2 ·
∇wF(wt+1, vt+1)∥2 −∥∇wF(wt, vt)∥2


≤2LBE
h
∥(wt+1, vt+1)−(wt, vt)∥2
i
≤2LBαtE
"

∇wF(wt, vt) + ξk, et ⊗∇wF(wt+1, vt)

2
#
= 2LBαtE
"r∇wF(wt, vt) + ξk

2
2 +
ret ⊗∇wF(wt+1, vt)

2
2
#
≤2LBαt
v
u
u
tE
"∇wF(wt, vt) + ξk

2
2
#
+ E
"et ⊗∇wF(wt+1, vt)

2
2
#
≤2LBαk
v
u
u
t2E
"∇wF(wt, vt)

2
2
#
+ 2E
"ξt

2
2
#
+ E
"∇wF(wt+1, vt)

2
2
#
≤2LBαt
√
5B2 = 2
√
5B2Lαt,
where the ﬁrst inequality is an application of Jensen’s inequality, the second inequality follows from Equation (17) the third
inequality follows from Lipschitz continuity and the sixth inequality follows from another application of Jensen’s inequality.
Consequently, by Lemma and the above chain of inequalities, it follows that limt→∞E[∥∇wF(wt, vt)∥2
2] = 0. Note that if
G is used, then by a similar argument, it follows that limt→∞E[∥∇vF(wt, vt)∥2
2] = 0. Consequently, if G is used, then
limt→∞E[∥∇F(wt, vt)∥2
2] = 0.
C. Proof of Proposition 1
Proposition 1 Suppose (x, y) denotes a training sample and its corrupted label. For simplicity, let the MentorNet input
φ(x, y, w) = ℓbe the loss computed by the StudentNet model parameter w. The MentorNet gm(ℓ; Θ) = v, where v is the
sample weight. If gm decreases with respect to ℓ, then there exists an underlying robust objective F:
F(w) = 1
n
n
X
i=1
ρ(ℓi),
where ρ(ℓi) =
R ℓi
0 gm(x; Θ)dx. In the special cases, ρ(ℓ) degenerates to the classical robust M-estimator: Huber (Huber
et al., 1964) and the log-sum penalty (Candes et al., 2008).

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
Proof 2 Given a MentorNet gm and its ﬁxed parameter Θ. As φ(x, y, w) = ℓ, we have gm(φ(x, y, w); Θ) = gm(ℓ; Θ).
gm is a neural network and hence is continuous with respect to ℓ. Deﬁne ρ(ℓ) as
ρ(ℓ) =
Z ℓ
0
gm(x; Θ)dx
(18)
Given the condition in the proposition, gm is decreasing with respect to ℓ. The function ρ is then bounded by its 1st term of
the Taylor series about a point w∗. We have:
ρ(φ(x, y, w)) ≤ρ(φ(x, y, w∗)) + gm(φ(x, y, w∗); Θ)(φ(x, y, w) −φ(x, y, w∗))
(19)
According to (Meng et al., 2015), the right-hand side in Eq. (19) is a tractable surrogate for ρ(φ(x, y, w)) and there exists
an underlying robust objective. For the i-th sample, we have:
ρ(φ(xi, yi, w)) = ρ(ℓi) =
Z ℓi
0
gm(x; Θ)dx
(20)
Finally, we have a robust objective derived from:
F(w) = 1
n
n
X
i=1
ρ(φ(xi, yi, w)) = 1
n
n
X
i=1
ρ(ℓi)
(21)
Now we show the connection between Eq. (20) to the robust M-estimator. For simplicity, we assume that the loss ℓ≥0 is
non-negative for every training sample. For the Huber loss, there exists an Θ∗such that:
gm(ℓ; Θ∗) =
(
1
2
ℓ≤λ2
λ
2
√
ℓ
otherwise ,
(22)
where λ > 0. It is easy to verify gm is decreasing with respect to ℓ, and we have:
ρ(ℓ) =
Z ℓ
0
gm(x; Θ∗)dx =
(
1
2ℓ
ℓ≤λ2
λ(
√
ℓ−1
2λ)
otherwise
(23)
Therefore we have:
ρ(ℓ2) =
(
1
2ℓ2
ℓ≤λ2
λ(ℓ−1
2λ)
otherwise
(24)
Eq. (24) has a similar form of the Huber M-estimator (Huber et al., 1964).
Likewise, there exists an Θ∗and a positive ϵ such that
gm(ℓ; Θ∗) =
λ
ℓ+ ϵ
(25)
Its underlying objective is identical to the log-sum penalty (Candes et al., 2008):
ρ(ℓ) =
Z ℓ
0
gm(x; Θ∗)dx = λ log(ℓ+ ϵ) −λ log(ϵ) = λ log(1 + ℓ
ϵ)
(26)
It leads to the following log-sum penalty:
F(w) = λ 1
n
n
X
i=1
log(1 + ℓi
ϵ )
(27)

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
For the Lorentzian (Black & Anandan, 1996). We have
gm(ℓ; Θ∗) =
2ℓ
2δ2 + ℓ2
(28)
In special cases, we assume that δ is a small positive such that all sample loss ℓ≥
√
2δ, the underlying objective is
ρ(ℓ) =
Z ℓ
0
gm(x; Θ∗)dx = log(2δ2 + ℓ2) −log 2δ2 = log(1 + 1
2( ℓ
δ )2)
(29)
The above objective becomes Lorentzian (Black & Anandan, 1996), also known as Lorentzian/Cauchy.
D. Comparison on MentorNet Architectures
This section examines MentorNet’s architecture in terms of learning existing predeﬁned curriculums (or weighting scheme).
To generate the data for training MentorNet, we enumerate the feasible input space of zi including the loss, difference to the
loss moving average, label, and epoch percentage. For this experiment, the dataset contains a total of 300k samples. For
each sample, we label a weight according to the weighting scheme in the predeﬁned curriculum. For example, we compute
the weight for focal loss by
v∗
i = [1 −exp{−ℓi}]γ,
(30)
where γ is a hyperparameter for smoothing the distribution. We consider the following predeﬁned curriculums: self-
paced (Kumar et al., 2010), hard negative mining (Felzenszwalb et al., 2010), linear weighting (Jiang et al., 2015), focal
loss (Lin et al., 2017), and prediction variance (Chang et al., 2017). Besides, we also consider a temporal mixture weighting
which is a mix of the self-paced (when the epoch percentage < 50) and the hard negative mining (when the epoch percentage
≥50). In some curriculums, the form G is unknown. We directly minimize the mean squared error between the MentorNet’s
output and the true weight.
We compare the MentorNet architecture in Fig. 1 in the paper to a simple logistic regression and 3 classical networks: a
2-layer Multiple Layer Perceptron (MLP), a 2-layer CNN with mean pooling, and an LSTM network (RNN) to sequentially
encode the features of every example in a mini-batch. The same features are used across all networks. The objective is
minimized by the Adam optimizer (Kingma & Ba, 2015). We use a very simple network for the proposed MentorNet. The
bidirectional LSTM has a single layer of 10 base LSTM units (step size = 10). We use an embedding layer (size = 2) for the
labels and an embedding layer (size = 5) for the integer epoch percentage between 0 and 99. The fully connected layer fc1
uses the tangent activation function and has 20 hidden nodes.
The performance is evaluated by the Mean Squared Error (MSE) to the true weight produced by each curriculum. Each
experiment is repeated 5 times using random starting values, and the average MSE (with the 90% conﬁdence interval) is
reported. Table 1 shows the comparison results. As we see, the simple network structure MLP performs well except for
complex weighting schemes that are prediction variance (Chang et al., 2017) and Temporal mixture weighting. Nevertheless,
the bi-LSTM structure in Fig.1 of the paper performs better than other classical network architectures. Fig. 1 illustrates the
error curve of different MentorNet architecture during training, where the x-axis is the training epoch and y-axis is the MSE.
Table 1. The MSE comparison of different MentorNet architecture on predeﬁned curriculums.
Weighting Scheme
Logistic
MLP
CNN
RNN
MentorNet (bi-LSTM)
Self-paced (Kumar et al., 2010)
8.9±0.8E-3
1.1±0.3E-5
4.9±1.0E-2
1.6±1.0E-2
1.6±0.5E-6
Hard negative mining (Felzenszwalb et al., 2010)
7.1±0.7E-3
1.6±0.6E-5
2.7±0.6E-3
2.2±0.4E-3
6.6±4.5E-7
Linear weighting (Jiang et al., 2015)
9.2±0.1E-4
1.2±0.4E-4
1.1±0.2E-4
2.0±0.3E-2
4.4±1.3E-5
Prediction variance (Chang et al., 2017)
6.8±0.1E-3
4.0±0.1E-3
2.8±0.4E-2
6.2±0.2E-3
1.4±0.7E-3
Focal loss (Lin et al., 2017)
1.7±0.0E-3
6.0±3.5E-5
1.2±0.3E-2
1.2±0.3E-2
1.5±0.8E-5
Temporal mixture weighting
1.8±0.0E-1
1.9±0.4E-2
1.2±0.6E-1
6.6±0.4E-2
1.2±1.1E-4
In Table 1, we learn a MentorNet by minimizing the MSE between its output and the true weight. We call it implicit training.
When the form of G is known, we can learn a MentorNet by directly minimizing Eq.(4) in the paper, called explicit training.
These two training approaches are theoretically identical and we empirically compare them on two curriculums of known G
in Fig. 2. As shown, we found implicit training converges faster.

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
2
4
6
8
10
#epoch
0
0.02
0.04
0.06
0.08
0.1
0.12
Mean Squared Error
Logistic
MLP
CNN
RNN
MentorNet
(a) Hard negative mining
2
4
6
8
10
#epoch
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Mean Squared Error
Logistic
MLP
CNN
RNN
MentorNet
(b) Temporal mixture weighting
Figure 1. Comparison of explicit and implicit MentorNet training.
0
5
10
15
20
#epoch
0
0.05
0.1
0.15
Mean Squared Error
-8
-6
-4
-2
0
Objective Value
MSE implicit training
MSE explicit training
Objective Value
(a) Self-paced
0
5
10
15
20
#epoch
0
0.05
0.1
0.15
Mean Squared Error
-2.5
-2
-1.5
-1
-0.5
0
Objective Value
MSE implicit training
MSE explicit training
Objective Value
(b) Linear weighting
Figure 2. Convergence comparison of different MentorNet architectures.
E. Implementation Details
E.1. Dataset and StudentNet
CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) consist of 32 × 32 color images arranged in 10 and 100 classes.
Both datasets contain 50,000 training and 10,000 validation images. The inception (Szegedy et al., 2016) and wide-resnet-
101 (He et al., 2016; Zagoruyko & Komodakis, 2016) are used as the StudentNet. Their implementations are based on
the TensorFlow slim implementation1, where the inception is detailed in (Zhang et al., 2017) and the wider resnet is
in (Zagoruyko & Komodakis, 2016).
In training, the batch size is set to 128 and we train 39K iterations for resnet model and 120k for the inception model. The
Step 12 in Algorithm 1 in the paper is implemented by the momentum SGD optimizer (momentum = 0.9) on a single GPU.
We use the common learning rate scheduling strategy and set the starting learning rate as 0.1 and multiply it by a factor of
0.1 at the 19.5k, 25k and 30k iteration for the resnet model, and 78k for the inception model. Training in this way on the
clean training dataset, the validation accuracy reaches about 81.4% and 95.5% for inception and resnet-101 on CIFAR-10,
and about 49.2% and 78.8% on CIFAR-100.
By default, the StudentNet incorporates three types of regularization: 1) the weight decay which adds an l2 norm of the
model parameters into the learning objective; 2) data augmentation (Krizhevsky et al., 2012) which augments the training
images via domain-speciﬁc transformations random cropping, perturbation and contrast; 3) dropout (Srivastava et al., 2014)
masks out network fully-connected layer outputs randomly. We use the best hyperparameter found on the clean training
data. In the inception network, weight decay is set 4e−3 and the dropout keep probability is 0.5. In the resnet-101, weight
decay is 2e−4 and the dropout keep probability is 1.0. In the Forgetting baseline (Arpit et al., 2017), we search the dropout
parameter in the range of (0.2-0.9) and report the best accuracy on the validation set. The data augmentation is the same
for the inception and the resnet-101: we pad 4 pixels to each side and randomly sampling a 32 x 32 crop, randomly ﬂip
the image horizontally (left to right), and then linearly scale the image to have zero mean and unit norm. Unless speciﬁed
otherwise, the StudentNet of the same hyperparameter, discussed above, is used in all baseline and the proposed model.
ImageNet ILSVRC2012 (Deng et al., 2009) contains about 1.2 million training and 50k validation images, split into 1,000
classes. Each image is resized to 299 × 299 with 3 color channels. For the ImageNet, we use the inception-resnet v2 slim
1https://github.com/tensorﬂow/models/tree/master/research/slim

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
implementation2 as our StudentNet. We train the model on the ImageNet of 40% noise. The Step 12 in Algorithm 1 in
the paper is implemented as a distributed asynchronized momentum SGD optimizer (momentum = 0.9) on 50 GPUs. We
set the batch size to 32 and train the model until it converges. That is 500 thousand steps for the StudentNet without any
regularization and 1 million steps for the StudentNet with full regularization (weight decay, dropout and data augmentation).
The starting learning rate is 0.05 and is decreased by a factor of 0.1 every 10 training epochs. The default dropout-keep-
probability hyperparameter set to 0.8. In the Forgetting baseline, we set it to 0.2, which is the best parameter found on
CIFAR-100. The weight decay is 4e−5. The batch norm is used and its decay is set to 0.9997 and the epsilon is set to 0.001.
The default data augmentation in the slim implementation is used. Training in this way on the clean training dataset, the
validation accuracy is Hit@1=0.765.
E.2. Baselines
Regarding the baseline method. FullMode is the standard StudentNet trained using l2 weight decay, dropout (Srivastava
et al., 2014) and data augmentation (Krizhevsky et al., 2012). The same parameters discussed in Appendix E.1 are used.
Forgetting was introduced in (Arpit et al., 2017). It is same as the FullModel except that the dropout parameter is tuned
in the range of (0.2-0.9). For the self-paced learning (Kumar et al., 2010), we gradually increase λ in training by 20%.
Following (Jiang et al., 2014) we tune the parameter in the range of the 50th, 60th and 75th percentile of average sample
loss. For the focal loss (Lin et al., 2017), we tune its γ in the range of {1, 2, 3} in our experiments. It is easy to verify that
Eq. (30) leads to the same classiﬁcation objective in the focal loss (Lin et al., 2017), an award-winning method for object
detection. For the Reed (Reed et al., 2014), we implement two versions: the soft and hard version. Let q = [q1, . . . , qm] be
the prediction (logits after softmax) for m classes:
ℓi = −(β
m
X
j=1
1(yi = j) log(qj) + (1 −β)
m
X
j=1
qj log(qj)),
(31)
where 1 is the indicator function and a hard version:
ℓi = −(β
m
X
j=1
1(yi = j) log(qj) + (1 −β) max
j
log(qj)),
(32)
We tune the parameters β in the range of {0.7, 0.8, 0.9, 0.95}. Goldberger (Goldberger & Ben-Reuven, 2017) is a recent
baseline weakly-supervised learning method. We implement the S-Model in the paper by appending an additional layer to
the StudentNet.
E.3. Setups of Our Model
The details about the MentorNet architecture (Fig. 1 in the paper) are discussed in Appendix D. MentorNet PD is learned
using the curriculum in Eq. (5) in the paper. The loss moving average ℓpt is set to the 75th-percentile loss in a mini-batch.
We tune the hyperparameter λ1 and λ2. MentorNet DD is the learned data-driven curriculum. It is trained on 5,000 images
of true labels, randomly sampled from CIFAR-10. We learn MentorNet DD on this dataset and apply it to CIFAR-100 on
which no true labels are used. We use the CIFAR-10 subset of the same level of the noise fraction corresponding to the
CIFAR-100. CIFAR-10 and CIFAR-100 are two different datasets that have not only different classes but also the different
number of classes. As CIFAR-100 and CIFAR-10 have the different number of classes, to apply a MentorNet, we ﬁx the
class label to 0.
Algorithm 1 is used to train the MentorNet together with the StudentNet. The decay factor in computing the loss moving
average is set to 0.95. As mentioned in the paper, a burn-in process is used in the ﬁrst 20% training epoch for both MentorNet
DD and MentorNet PD. We update and learn MentorNet twice after the learning rate is changed. That is on the 21% and
75% of the total epoch. More updates lead to insigniﬁcant performance difference. For each mini-batch, the weight decay
parameter θ in Eq. (1) in the paper is normalized by the sum of the weight in a mini-batch. That is θt = 1
bθ0 Pb
i=1 vΞi,
where θ0 is the original weight decay parameter. The same learning rate scheduling strategy in the StudentNet is used in
Algorithm 1.
2https://github.com/tensorﬂow/models/blob/master/research/slim/nets/inception resnet v2.py

MentorNet: Learning Data-Driven Curriculum for Deep Neural Networks
References
Arpit, D., Jastrzkebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville, A.,
Bengio, Y., et al. A closer look at memorization in deep networks. In ICML, 2017.
Black, M. J. and Anandan, P. The robust estimation of multiple motions: Parametric and piecewise-smooth ﬂow ﬁelds.
Computer vision and image understanding, 63(1):75–104, 1996.
Candes, E. J., Wakin, M. B., and Boyd, S. P. Enhancing sparsity by reweighted l1 minimization. Journal of Fourier analysis
and applications, 14(5-6):877–905, 2008.
Chang, H.-S., Learned-Miller, E., and McCallum, A. Active bias: Training a more accurate neural network by emphasizing
high variance samples. NIPS, 2017.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009.
Felzenszwalb, P. F., Girshick, R. B., McAllester, D., and Ramanan, D. Object detection with discriminatively trained
part-based models. IEEE transactions on pattern analysis and machine intelligence, 32(9):1627–1645, 2010.
Goldberger, J. and Ben-Reuven, E. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.
Gong, P., Zhang, C., Lu, Z., Huang, J. Z., and Ye, J. A general iterative shrinkage and thresholding algorithm for non-convex
regularized optimization problems. In ICML, 2013.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR, 2016.
Huber, P. J. et al. Robust estimation of a location parameter. The Annals of Mathematical Statistics, 35(1):73–101, 1964.
Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Hauptmann, A. Self-paced learning with diversity. In NIPS, 2014.
Jiang, L., Meng, D., Zhao, Q., Shan, S., and Hauptmann, A. G. Self-paced curriculum learning. In AAAI, 2015.
Kingma, D. and Ba, J. Adam: A method for stochastic optimization. In ICLR, 2015.
Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In NIPS,
2012.
Kumar, M. P., Packer, B., and Koller, D. Self-paced learning for latent variable models. In NIPS, 2010.
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll´ar, P. Focal loss for dense object detection. ICCV, 2017.
Mairal, J. Stochastic majorization-minimization algorithms for large-scale optimization. In NIPS, 2013.
Meng, D., Zhao, Q., and Jiang, L.
What objective does self-paced learning indeed optimize?
arXiv preprint
arXiv:1511.06049, 2015.
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A. Training deep neural networks on noisy labels
with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.
Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to prevent neural
networks from overﬁtting. Journal of machine learning research, 15(1):1929–1958, 2014.
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision.
In CVPR, 2016.
Zagoruyko, S. and Komodakis, N. Wide residual networks. In BMVC, 2016.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization.
In ICLR, 2017.
Zhang, C.-H. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, pp. 894–942,
2010.

