
STATISTICAL INFERENCE AND 
PREDICTION IN CLIMATOLOGY: 
A BAYESIAN APPROACH 

METEOROLOGICAL MONOGRAPHS 
Volume 1 
No. 
1 Wartime Developments in Applied Climatology, 1947 (Out of Print) 
No. 
2 The Observations and Photochemistry of Atmospheric Ozone, 1950 (Out of Print) 
No. 
3 On the Rainfall of Hawaii, 1951 (Out of Print) 
No. 4 On Atmospheric Pollution, 1951. ISBN 0-933876-00-9 
No. 
5 Forecasting in Middle Latitudes, 1952 (Out of Print) 
Volume 2 
No. 
6 Thirty-Day Forecasting, 1953. ISBN 0-933876-01-7 
No. 
7 The Jet Stream, 1954. ISBN 0-933876-02-5 
No. 
8 Recent Studies in Bioclimatology, 1954. ISBN 0-933876-03-3 
No. 
9 Industrial Operations under Extremes of Weather, 195 7. ISBN 0-933876-04-1 
No. 
10 Interaction of Sea and Atmosphere, 1957. ISBN 0-933876-05-X 
No. 
11 Cloud and Weather Modification, 1957. ISBN 0-933876-06-8 
Volume 3 
Nos. 
12-20 Meteorological Research Reviews, 1957. Review of Climatology. Meteorological In-
struments. Radiometeorology. Weather Observations, Analysis and Forecasting. Applied 
Meteorology. Physics of the Upper Atmosphere. Physics of Clouds. Physics of Precip-
itation. Atmosphere Electricity 
Bound in One Volume. ISBN 0-933876-07-6 
Volume 4 
No. 
21 Studies of Thermal Convection, 1959. ISBN 0-933876-09-2 
No. 
22 Topics in Engineering Meteorology, 1960. ISBN 0-933876-10-6 
No. 
23 Atmospheric Radiation Tables, 1960. ISBN-0933876-11-4 
No. 
24 Fluctuations in the Atmospheric Inertia, 1961. ISBN 0-933876-12-2 
No. 
25 Statistical Prediction by Discriminant Analysis, 1962. ISBN 0-933876-13-0 
No. 
26 The Dynamical Prediction of Wind Tides of Lake Erie, 1963. ISBN 0-933876-15-7 
Volume 5 
No. 
27 Severe Local Storms, 1963. Paperbound, ISBN 0-933876-17-3 
Volume 6 
No. 
28 Agricultural Meteorology, 1965. Paperbound, ISBN 0-933876-19-X; Clothbound, ISBN 
0-933876-18-1 
Volume 7 
No. 
29 Scattered Radiation in the Ozone Absorption Bands at Selected Levels of a Terrestrial, 
Rayleigh Atmosphere, 1966. Paperbound, ISBN 0-933876-22-X; Clothbound, ISBN 0-933876-
21-1 
VolumeS 
No. 
30 The Causes of Climatic Change, 1968. ISBN 0-933876-28-9 
Volume 9 
No. 
31 Meteorological Investigations of the Upper Atmosphere, 1968. ISBN 0-933876-29-7 
Volume 10 
No. 
32 On the Distribution and Continuity of Water Substance in Atmospheric Circulations, 1969. 
ISBN 0-933876-30-0 
Volume 11 
No. 
33 Meteorological Observations and Instrumentation, 1970. ISBN 0-933876-31-9 
Volume 12 
No. 
34 Long-Period Global Variations of Incoming Solar Radiation, 1972. ISBN 0-933876-37-8 
Volume 13 
No. 
35 Meteorology of the Southern Hemisphere, 1972. ISBN 0-933876-38-6 
Volume 14 
No. 
36 Alberta Hailstorms, 1973. ISBN 0-933876-39-4 
Volume 15 
No. 
37 The Dynamic Meteorology of the Stratosphere and Mesosphere, 1975. ISBN 0-933876-41-6 
Volume 16 
No. 
38 Hail: Review of Hail Science and Hail Suppression, 1977. ISBN 0-933876-46-7 
Volume 17 
No. 
39 Solar Radiation and Clouds, 1980. ISBN 0-933876-49-1 
Volume 18 
No. 40 METROMEX: A Review and Summary, 1981. ISBN 0-933876-52-1 
Volume 19 
No. 
41 Tropical Cyclones-Their Evolution, Structure and Effects, 1982. ISBN 0-933876-54-8 
Volume 20 
No. 42 Statistical Inference and Prediction in Climatology: A Bayesian Approach, 1985. ISBN 
0-933876-62-9 
Orders for the above publications should be sent to: 
THE AMERICAN METEOROLOGICAL SOCIETY 
45 Beacon St., Boston, Mass. 02108 

METEOROLOGICAL MONOGRAPHS 
Volume 20 
September 1985 
Number 42 
STATISTICAL INFERENCE AND 
PREDICTION IN CLIMATOLOGY: 
A BAYESIAN APPROACH 
EdwardS. Epstein 
CLIMATE ANALYSIS CENTER 
NATIONAL METEOROLOGICAL CENTER 
NWS/NOAA 
WASHINGTON, D.C. 
American Meteorological Society 

ISSN 0065-940 I 
American Meteorological Society 
45 Beacon Street, Boston, Massachusetts 
ISBN 978-1-935704-27-0 (eBook)
DOI 10.1007/978-1-935704-27-0

Table of Contents 
l. INTRODUCTION 
1.1 
Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
I 
1.2 
Probability, the Language of Uncertainty . . . . . . . . . . . . 
3 
1.3 
Stochastic Processes and Climate Prediction . . . . . . . . . 
6 
1.4 
Final Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
I 0 
2. 
SOME FUNDAMENTALS OF PROBABILITY 
2.1 
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
II 
2.2 
Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
11 
2.3 
Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . 
13 
2.4 
Expectations and Moments . . . . . . . . . . . . . . . . . . . . . . . 
16 
2.5 
Joint, Marginal and Conditional Probabilities . . . . . . . . 
18 
2.6 
Bayes' Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
20 
2. 7 Sufficient Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
23 
2.8 
Conjugate Distributions; Prior and Posterior Parameters 
25 
3. 
BERNOULLI PROCESSES 
3.1 
Definition of a Bernoulli Process . . . . . . . . . . . . . . . . . . 
29 
3.2 
Distributions of Sufficient Statistics . . . . . . . . . . . . . . . . 
31 
3.3 
Prior and Posterior Probabilities . . . . . . . . . . . . . . . . . . . 
33 
3.4 Conjugate Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
37 
3.5 
Selecting Prior Parameters . . . . . . . . . . . . . . . . . . . . . . . . 
40 
3.6 
Predictions of Future Results . . . . . . . . . . . . . . . . . . . . . 
47 
3. 7 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
49 
4. 
POISSON PROCESSES 
4.1 
Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
53 
4.2 
Distributions of Sufficient Statistics . . . . . . . . . . . . . . . . 
54 
4.3 
Conjugate Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 
58 
4.4 
Selection of Prior Parameters . . . . . . . . . . . . . . . . . . . . . 
62 
4.5 
Predictive Distributions and Probabilities . . . . . . . . . . . 
67 
4.6 
An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
72 
5. 
NORMAL DATA-GENERATING PROCESSES 
5.1 
Normal Distributions and the Central Limit Theorem 
77 
5.2 
Sufficient Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
79 
5.3 
Bivariate Prior and Posterior Densities: Prior and Posterior 
Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
81 

5.4 Conjugate Density, Precision Known . . . . . . . . . . . . . . . 
82 
5.5 
Predictive Distribution, Precision Known . . . . . . . . . . . 
84 
5.6 
An Example: Normal Data-Generating Process with Pre-
cision Known . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
85 
5.7 
Conjugate Distribution, Precision Unknown . . . . . . . . . 
88 
5.8 
The Normal-Gamma Distribution: Marginal and Con-
ditional Densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
91 
5.9 
Predictive Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 
94 
5.10 An Example of Inference and Prediction: Normal Data-
Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
98 
6. 
NORMAL LINEAR REGRESSION 
6.1 
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
105 
6.2 
Sufficient Statistics for Simple Linear Regression . . . . . 
107 
6.3 
Diffuse Prior-Simple Linear Regression . . . . . . . . . . . . 
109 
6.4 
Simple Linear Regression with a Nondiffuse Conjugate 
Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
113 
6.5 
Predictive Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 
118 
6.6 
An Example: Normal Simple Linear Regression 
123 
7. 
FIRST-ORDER AUTOREGRESSION 
7.1 
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
139 
7.2 
First-Order Normal Autoregression . . . . . . . . . . . . . . . . 
140 
7.3 
Inferences and Predictions . . . . . . . . . . . . . . . . . . . . . . . . 
142 
7.4 
A Numerical Example: Annual Streamflow . . . . . . . . . . 
146 
7.5 
Comments on Computational Methods . . . . . . . . . . . . . 
151 
7.6 
Results When the Prior Is Relatively Uninformative . . 
155 
7. 7 Results When the Prior Is Informative . . . . . . . . . . . . . . 
161 
Appendix A: 
SUMMARY OF BASIC INFORMATION ON 
PROBABILITY DISTRIBUTIONS ENCOUN-
TERED . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
167 
Appendix B: 
SELECfED TABLES OF PROBABILITY DIS-
TRIBUTIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
181 
Appendix C: 
FORTRAN PROGRAM TO IMPLEMENT EX-
AMPLE GIVEN IN CHAPTER 7 . . . . . . . . . . . . 
191 
REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
197 
INDEX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
198 

Chapter 
1 
Introduction 
1.1 
OBJECTIVE 
The objective of this monograph is to introduce to the climatological 
and meteorological community a set of statistical techniques for making 
predictions about events that can at best be described as being the output 
of a stochastic process. These techniques are especially useful when one's 
knowledge of the system is incomplete and there is only limited empirical 
evidence; these are situations where most of the more widely known ap-
proaches are oflittle help. The techniques themselves are very general, but 
they will be presented here in the context of climatological and meteoro-
logical applications, especially the former. 
Most applications of climatological information involve, in one way 
or another, predictions. Climatological predictions are not based on detailed 
projections of the evolution of weather events, but rather on knowledge 
and empirical evidence of the collective behavior of weather (or climate) 
at time scales that extend beyond the limit within which weather events 
are predictable in any detail. The ultimate objective of (short-range) me-
teorological predictions, however unattainable, is totally accurate forecasts 
of weather at specific times and locations. In the usual climatological con-
text, the ultimate is a description of what may be expected, and how likely 
1.1 

2 
CHAPTER 1 
or unlikely various alternatives are. Whether generated by statistical or 
physical methods, climatological predictions are inherently uncertain. Even 
if we discovered a "perfect" climate prediction technique, our predictions 
would still necessarily be imperfect, because inherently unpredictable 
weather events will give rise to a background of "noise" that cannot be 
avoided. This concept of noise due to weather has been utilized, especially 
by Madden (1976}, to estimate the limits of predictability of monthly and 
seasonal climate. 
Thus the language of climatological predictions must be probability, 
even in the best of circumstances when the "climate" is very well-known. 
When the "climate" is not so well-known, either because of our lack of 
understanding or because of insufficient empirical information, then there 
is even more reason to turn to probabilities to express that uncertainty. 
If the empirical evidence is very substantial, and directly relevant to 
the situation for which a prediction is desired, then the problem is an easy 
one. The necessary frequencies are simply extracted from the data and 
interpreted as predictions. For example, if one is interested in the maximum 
temperatures to be expected next July on an experimental farm in a remote 
rural location for which a long and homogeneous climatological record is 
available, the relative frequencies of maximum temperatures in past Julys 
are examined and accepted as the probabilities of what will happen in the 
next year. 
But now consider how to deal with the same situation when the farm 
is being moved to a new location, and for purposes of experimental design 
one wants to know how the July maximum temperatures at the new location 
will differ from those at the old location. If the move has been well planned, 
measurements at the new location for the last year or two may be available. 
How is this information used to make a credible prediction? How does 
one at the same time use more qualitative information about location, 
drainage, land use, etc., that tells the trained climatologist a good deal 
about the likely differences between the two locations? This is clearly a 
more difficult problem than the former one, and it cannot be solved 
uniquely. 
But there do exist methods that optimize the combination of these 
partial sources of information: the climatologist's useful although incom-
plete knowledge on the one hand, and the too-short empirical record on 
the other. We will describe a series of such methods; while they are not an 
exhaustive set, they cover a wide sampling of situations with which the 
climatologist must deal. These are not methods that can be mechanically 
applied. They do involve rigorous procedures and manipulations, but they 
also rely particularly strongly on the judgement and expertise of the prac-
titioner. Although there will be some very useful applications to the drawing 

INTRODUCTION 
3 
of inferences when large quantities of relevant data are available, the em-
phasis will always be on situations where data are relatively scarce. 
The limit of "relatively scarce" data is no data at all. In that limiting 
condition, all that is left to the climatologist who is required to make a 
prediction is his or her judgement and expertise. The methods we will 
describe allow for a smooth transition from no data to ample data. They 
indeed are applicable when no data are available. This is the case when 
the need for the climatologist to be able to quantify his or her judgement 
is most critical. The climatologist can learn from the formal developments 
described later how to-we believe better and more systematically-express 
his or her judgements when data are absent or limited. The predictions 
generated with no or meager data will not warrant as much confidence as 
those based on substantial empirical evidence-that will be quite clear. 
But it will also be clear how much can be gained from additional obser-
vations. We will not extend our analysis in this monograph into the im-
portant question of the value of additional observations. Instead we will 
limit the scope of our treatment. The goal is to produce useful predictions 
that are consistent with one's best judgements, and to allow consistent 
revisions of such judgements as data do become available. 
1.2 
PROBABILITY, THE LANGUAGE OF UNCERTAINTY 
We use probability to express and to quantify our uncertainty. For 
the most part the concepts we employ correspond quite closely to our 
intuitive notions of what we mean when we use the term in our everyday 
language. We insist on adhering to certain formal rules for assigning and 
manipulating probabilities, but these are in general necessary to ensure 
that we will be consistent in applying the basic concepts of probability 
under circumstances that are occasionally quite complicated. 
Probabilities are real numbers in the interval between 0 and l (inclu-
sive) that are associated with "events" or "occurrences". If S represents 
the set of all possible events (the sample space), then the probability we 
assign to S, P{S} is one. Iftwo events are mutually exclusive (their inter-
section cannot occur-it has probability zero), then the probability of at 
least one of the two events occurring (the probability of their union) is the 
sum of the probabilities of the two individual events. There are numerous 
texts that discuss these basic axioms for dealing with probabilities and the 
consequent rules for manipulating probabilities of compound and condi-
tional events. We will not try to repeat such a development here, but in 
order to understand the developments and discussions that follow, the 
reader should be quite familiar with the basic rules for manipulating prob-
abilities. In Chapter 2 we will review some of the formal concepts and 

4 
CHAPTER 1 
mathematical devices for manipulating probabilities that will be encoun-
tered and relied on most frequently in subsequent chapters. First we will 
discuss in some detail how probabilities are interpreted; this in turn relates 
to the nature of the sets of events to which the notion of probability applies. 
There are some subtle but significant differences between the classical view 
of probability and the "Bayesian" or "subjective" view that forms the basis 
of the methods we will describe. It should be emphasized that the differences 
in interpretation do not in any way imply any mathematical differences-
there are none. 
The classical interpretation of probability is that of relative frequency. 
In one variation we contemplate many repetitions of an identical experi-
ment (like tossing a die) and identify the probability of an outcome (or 
event) with the relative frequency of occurrences in the long run. Another 
view requires us to conceive of the sample space as being made up of 
equally likely elementary events. The probability of an event is the ratio 
of the number of elementary events comprising the event whose probability 
is being assessed, to the total number of elementary events in the 
sample space. 
The probabilities of most gambling games can be identified with either 
of these interpretations. We can throw a die many thousands of times (or 
imagine the experiment being carried out) and discover that the number 
3 side comes up, in the long run, on one-sixth of the trials. Or we can assert 
that the sample space consists of six equally likely elementary events, of 
which precisely one is the event "3", and therefore the probability of a 3 
is 1/6. 
The relative frequency interpretation is sound and useful and has a 
strong empirical foundation. However, situations are encountered in which 
the notion of probability seems to be appropriate, but in which a relative 
frequency interpretation is not possible. Either it is not possible to describe 
the sample space as being made up of equally likely elementary events, or 
it is not possible to imagine (let alone carry out) the necessary large number 
of identical experiments. For example, consider the statement: "The odds 
favoring the mayor's reelection are 2 to 1." This clearly does not mean 
that there are three elementary events of equal likelihood, exactly two of 
which are associated with the mayor being reelected. Neither does it imply 
that there could be a large number of identical races of which the mayor 
would win about two-thirds. (If we really tried to imagine exact replications 
of the situation, then we would get the same result every time. Voters are 
not stochastic machines who mark ballots in some random way.) In a 
somewhat convoluted sense it could be interpreted to suggest that in like 
situations, where the information available is equivalent to that now at 
hand, the incumbent wins two-thirds of the time. But the notion of exact 

INTRODUCTION 
5 
replications is difficult to apply. What the statement does mean is that 
there is uncertainty as to who will win the election, and (either in some 
collective way like mutual betting, or in some individual's mind, depending 
on the context in which the statement was made) that the degree of belief 
in the event that the mayor will win is 2/3. 
The concept of degree of belief is inseparable from a subjective view 
of probability. Everyone need not agree on the value of the probability of 
an event. How can we deny that someone else will consider the mayor's 
reelection prospects only 50-50? There is no empirical way to determine 
who is right and who is wrong. This is a drawback to the personal or 
subjective view of probability. On the other hand, this view has the unique 
advantage of being applicable in one-of-a-kind situations. 
The degree ofbelief-subjective view of probability does not negate in 
any sense the significance of empirical evidence and the relevance of relative 
frequencies to probability. Rather it extends the applicability of probabilistic 
considerations to a much broader class of situations. A reasonable person 
will use the available empirical evidence, or if appropriate the notion of 
elementary events having equal probability, or the relative frequencies that 
an experiment would produce "in the long run," to establish his or her 
degree of belief. The probability of the head resulting from the toss of an 
honest coin is still 0.5, and the chance of getting a 7 on the next toss of 
two honest dice is still 1/6 to the subjectivist. But there will usually not be 
complete agreement on the probability that someone will win an election 
or a sporting event, or that next month's precipitation will be less than 
that in the same month last year. 
In the material that follows it is essential to understand that we take 
advantage of the subjective interpretation of probability. We will have much 
to say about how we assess and quantify our own degree of belief, i.e., use 
professional judgement, and how we should respond consistently and rig-
orously in modifying that degree of belief in the light of relevant empirical 
evidence. 
The subjectivist view of probability greatly expands the realm of ap-
plicability of probability. It allows probability to be applied to unique events 
and situations. The notion of degree of belief also finds important appli-
cability in dealing with hypotheses: it describes the degree of belief that 
one may have with regard to statements that are, in fact, either false or 
true. There is a very real difference between the statements: "the next toss 
of the coin will result in a head," and "the coin is an honest coin." The 
first statement refers to a repeatable experiment; given the coin is honest, 
we certainly would expect that the probability of the statement being correct 
is 0.5. The second statement, however, is an assertion offact. The speaker's 
degree of belief (it may be 0.00 1, 0.99 or 0.5000) is a probability in the 

6 
CHAPTER 1 
subjective sense; only under special contrived circumstances is it a prob-
ability from a classical perspective. This latter probability is an expression 
of some special knowledge of the circumstances surrounding the selection 
of the coin, i.e., what the "expert" knows of the uniqueness of the situation. 
This is much like the special knowledge that the climatologist may have 
about the July mean temperature of one location (previously unmonitored) 
compared to that at a nearby weather observing site. There are two parallel 
statements to consider: l) The new site will be colder, on the average, next 
July, with probability 0.5; and 2) the new site's July mean temperature, 
compared to that of the old site, behaves like an honest coin, generating a 
random series of independent, equally likely warmer and colder events. 
This type of situation will be examined in detail in Chapter 3. 
We will develop procedures designed to help the climatological make 
the best possible statement-a probability statement-about the relative 
temperatures at these two locations next July. The procedure will start with 
a careful examination of the climatologist's knowledge and beliefs, leading 
to a quantification of these views. While these specific views may not be 
transferable to someone else, the methods used to expose and quantify 
them can be shared. This is a subject that has not been given adequate 
attention in any field, let alone climatology. Indeed, the methods we describe 
should be of interest in many fields of application. 
The next step is to augment such views as rigorously as possible, in 
the light of any data that become available after the initial views are for-
mulated. The beliefs held when a new site is first identified will not in 
general be valid after even one-and even more surely after several-year's 
data. The final step, of course, is to generate a prediction based on that 
accumulated knowledge. 
An essential aspect of our procedure will be the specification of rules 
by which new data should be used by the climatologist to modify his or 
her subjective belief. Almost regardless of the beliefs held before any mon-
itoring is done, if in the first three years when monitoring is done a new 
station is colder, some adjustment of the prior beliefs is in order. Bayes' 
Theorem (Section 2.6), a fundamental consequence of basic rules of prob-
ability and induction, will tell us how such adjustments are to be made. 
We will apply Bayes' Theorem repeatedly; hence the term "Bayesian ap-
proach." 
1.3 STOCHASTIC PROCESSES AND CLIMATE PREDICfiON 
There are two general sources of uncertainty that are inherent in all 
the predictions that face the climatologist. One involves the fundamental 
lack of predictability of the climate system. It means that if his knowledge 

INTRODUCTION 
7 
were perfect, the climatologist would still have to resort to statements in-
volving probabilities to express his knowledge of the future. The other 
source of uncertainty, of course, is the fact that our knowledge is imperfect. 
It is generally accepted by meteorologists that the details of atmospheric 
behavior cannot be predicted beyond a few hours to a few weeks, depending 
on the scale of the phenomenon being considered. Even perfect knowledge 
of the initial state of the atmosphere and of the laws governing its behavior 
could not allow any escape from this fundamental limitation which is 
imposed by the turbulent nature of the atmosphere. On the other hand, 
even though the details of the weather cannot be predicted at long range, 
there is no known fundamental limitation on how far into the future the 
climate (defined as some hypothetical average condition) can be predicted 
There may be some such limitation, but even if there is none, the unpre-
dictable sequence of weather events which will go into every particular 
realization of the future will impose some noise, some uncertainty, some 
unpredictability. 
In other words, the very best we can ever hope to do in the way of 
climate prediction will involve probabilities. And what we can actually do 
is still very far short of the best we can hope for. As a practical matter we 
must think of the climate system as a number-generating machine-a sto-
chastic process-that creates future values of relevant climate variables. 
This climate data-generating process operates according to rules that we 
hope we understand or can learn to understand, but which necessarily have 
some randomness built in. In many cases the physical processes that are 
operating are only vaguely known, but the nature of the stochastic output 
can be modeled with fidelity. 
Stochastic data-generating processes become a key component of our 
treatment of the problem of climate prediction. They are our surrogates 
for the physical-numerical models that are useful for deterministic weather 
prediction. The climatologist chooses the proper process (as one would a 
model) and tries to assess as well as possible the necessary parameters of 
the process. In this way a probabilistic prediction is possible. 
Several data-generating processes will be described in Chapters 3 
through 7, and formal methods for dealing with them will be developed. 
Chapter 3 will be concerned with Bernoulli processes, Chapter 4 with Pois-
son processes, Chapter 5 with normal data-generating processes, and 
Chapter 6 with normal regression. In each of these chapters we will make 
use of some analytic niceties (conjugate distributions) that simplify the 
analysis. The final chapter, dealing with a first-order autoregressive process, 
will illustrate that the general approach is useful even when these analytic 
advantages are absent. 

8 
CHAPTER 1 
The essential characteristics of each of the processes will be described 
in detail, and examples will be given. What we will not treat in any detail 
is the question of assuring that the process selected is indeed appropriate 
to the particular problem. That we leave to the discretion and wisdom of 
the climatologist. This is an element of the climatologist's expertise that 
must supplement the judgemental considerations that are included in the 
formal analysis. 
Given that the suitable climatic data-generating process has been 
identified, its use will always entail the selection of one or more parameter 
values. A great deal of what follows will be nothing more than a formal 
way of proceeding logically to choose appropriate values for the parameters 
and then altering these selections whenever new information becomes 
available. But just as all the predictions themselves must necessarily be 
uncertain, our knowledge will not be so complete that the parameters will 
be known exactly. The fact that we are willing to revise our opinions as to 
the best values for these parameters as more information becomes available 
necessarily implies that we consider them to be at least somewhat uncertain. 
The uncertainty that we attribute to the values of the parameters has 
two significant consequences. First, in making predictions we must expect 
that the predictions we would make if the parameter(s) were known will 
have to be altered to reflect the uncertainty of our knowledge. Second, we 
will have to apply the language of probability to the parameter values in 
much the same way, and for much the same reasons, that we must apply 
it to the predictions. Much of what we will be discussing in the following 
chapters deals with probability statements about parameters, and how these 
are made and revised. This will be the most significant single step as we 
proceed then to use information about the climate data-generating process 
and about the parameters of the process, in order to make the predictions 
that are our goals. 
Figure 1.1 illustrates schematically the role of the climate data-gen-
erating process and the sources of uncertainty in the predictions. Note that 
we use the phrase "infinitely wise" to suggest that we know everything 
possible about the climate system (i.e., the climate data-generating process 
and its parameters), but that we still lack the omniscience that would allow 
us to see into the future and overcome the fundamental unpredictability 
of climate. 
In reality we have to deal not only with the inherent uncertainty, but 
also with uncertainty related to the unknown parameters of the data-gen-
erating process. We want to reduce that uncertainty as much as possible. 
Therefore we try to make use of the information available to us to formulate 
the best judgements possible. 

INTRODUCTION 
I 
PARAMETERS I.__,..~;~A.:o.dll:l·u~st.._~ 
CLIMATE DATA 
<known! 
lr 
GENERATING PROCESS 
<How the climate system works) 
PREDICTIONS I 
(Probabilities of future events) 
PARAMETERS 
CLIMATE DATA 
<Best judgement: probabilities 1-..:;A:;:;du:,·u.._s~t -+o-i GENERATING PROCESS 
of possible values) 
(Perhaps different from above! this is how 
we thmk the system worksl 
FIG. 1.1. Schematic diagram of the sources of uncertainty and their role 
in prediction of data-generating processes. 
9 
Note that there is a closed loop in the lower diagram of Fig. l.l. After 
making predictions we will in general be able to observe the outcomes and 
use these additional data to revise our judgements about the parameters, 
and as a consequence improve our predictions. The importance of this 
loop very much affects the nature of the discussion that follows and indeed 
even the data-generating processes with which we have chosen to be con-
cerned. 
In Chapter 2 the concept of conjugate distributions is introduced. We 
have chosen to rely, except in the final chapter, on methods that allow us 
to employ identical formulas for expressing judgements about parameters 
and for making subsequent predictions, before and after the availability of 
new data. This requires that there be a special, conjugate, relationship 
between how we express our judgements about the parameters and form 
of the data-generating process. Not all data-generating processes permit 
the existence of conjugate distributions, but the binomial, Poisson and 
normal processes do. Thus these have been chosen for detailed treatment. 
making predictions we 
making predictions we 

10 
CHAPTER 1 
This is not as limiting as it may at first appear: a remarkably large array 
of situations arise for which these particular processes are immediately 
applicable. It will also be noted, but not dealt with in nearly as much detail, 
that the use of conjugate forms is a convenience and not a necessity for 
Bayesian inference and prediction. The final chapter, which deals with a 
process-first-order autoregression-in which conjugate methods are not 
very useful, illustrates both the versatility of.the general approach and the 
penalty of requiring a more numerical and less analytic approach. 
1.4 FINAL COMMENTS 
This is not intended to be a manual of how to make predictions, or 
even how to make probabilistic predictions. In some very limited cases the 
specific methods we describe may indeed be directly applicable to the user's 
problem. More likely, the problem to be faced will be more complex than 
the necessarily limited examples we give. But what we do hope and antic-
ipate is that the reader will develop an understanding of the general ap-
proach, and learn to adapt it to specific cases that he or she may encounter. 

Chapter 
2 
Some Fundamentals of 
Probability 
2.1 
INTRODUCTION 
This monograph is not a text in probability theory, and this chapter 
does not give a thorough treatment of probability. Rather, it is intended 
as a review and reinforcement of important concepts and tools for readers 
who have studied probability and statistics at the level of an introductory 
college course that makes use of the calculus. Topics are chosen with the 
very specific purpose of introducing the tools and methods that will be 
encountered extensively in subsequent chapters. We also make use of this 
chapter to introduce some of the more specialized notation that we will 
employ. 
We assume that the reader is familiar with the basic notions of prob-
ability, of sample spaces, simple and compound events, and similar con-
cepts. While we may mention some of these concepts, we will not dwell 
on them. Our development is not intended to be entirely rigorous or thor-
ough. It is meant to provide a background that will allow the reader to be 
comfortable with the material that follows in subsequent chapters. 
2.2 
RANDOM VARIABLES 
It is often convenient and always possible to associate numerical values 
with each event in a sample space. These values are then referred to as the 
11 

12 
CHAPTER 2 
values of a random variable. A random variable is simply any real-valued 
function defined on a sample space. For example, if we are considering 
the toss of a coin with two possible outcomes, heads and tails, we could 
label the outcomes "H" and "T", or alternatively we could define x as a 
variable that takes on the value 0 if the event "heads" occurs and the value 
1 if the event "tails" occurs. Here xis a random variable; a variable defined 
to take on non-numerical values is not. 
It is not necessary for all distinguishable events to take on different 
values. It is only necessary that there be a rule that allows a value to be 
associated uniquely with each event. For example, if a coin were tossed 
four times, the outcomes would be sequences such as HHHT, THTH, 
HTTH, etc; each toss is an elementary event. Altogether, 16 (24) distin-
guishable sequences (ordered sets of elementary events) are of course pos-
sible. These could each be assigned a separate value (such as the binary 
numbers 0001, 1010, 0110, respectively), and such a rule would define a 
random variable. Alternatively, however, a random variable could be de-
fined as the number of occurrences, in each sequence, of tails. In this case 
the values of the random variable (let us call it y) corresponding to the 
three events are 1, 2 and 2. When we deal with the probability that y takes 
on some particular value, we are then dealing with the sum of the proba-
bilities of all the mutually exclusive compound events (in turn sequences 
of elementary events) for which that value is assigned. 
If the number of distinguishable elementary events in a sample space 
is finite, then the random variable necessarily can take on only a finite set 
of values. Such random variables are referred to as discrete, in contrast to 
continuous random variables which are useful when the sample space is a 
continuum (as, for example, temperature). We will have reason to deal 
with both kinds of random variable, and each requires the appropriate 
mathematical treatment. The notation we will use will not distinguish be-
tween discrete and continuous random variables, though in every context 
it will be made clear in the definition of the variable with which kind we 
are dealing. As will be explained below, the reader will have to interpret 
the mathematics in terms which specifically recognize whether the random 
variable is discrete or continuous. 
We will, in most cases, use the notation of a tilde over a letter to 
emphasize that we are representing a random variable. This device could 
be dispensed with, but it is frequently useful to emphasize the context of 
a mathematical expression. This is especially true in the early developments 
when the reader is getting used to treating a variable sometimes as random 
and then, in a slightly different context, as a fixed parameter of arbi-
trary value. 

SOME FUNDAMENTALS OF PROBABILITY 
13 
2.3 
PROBABILITY DISTRIBUTIONS 
The random variable is a quantity associated with each point in the 
sample space. The probability distribution is a function of the random 
variable that defines the probability associated with different values of the 
random variable. When xis a discrete random variable, then the probability 
distributionf(x) is the (finite) probability that x has the value x. In other 
words, for discrete x 
f(x) = prob{x = x} = P{x = x}, 
On the other hand, when the random variable is continuous, we will 
use the same notation, but interpret it differently. For a continuous random 
variable x, the probability function f(x) is a probability density defined 
such that 
ji) 
l. 
P{x<x+Llx}-P{x<x} 
(x = 1m 
. 
Ax~O 
Ll_x 
This is equivalent to 
P{a < x ~ b} = Lb f(x)dx. 
For convenience, the various functionsf(x) will be referred to as probability 
density functions, or as probability distributions, regardless of whether the 
random variable is continuous or discrete. 
Two of the fundamental properties of probabilities are that no prob-
abilities can be negative and that the total probability of all possible events 
must be unity. This places restrictions on the kinds of functions that can 
be probability densities. First, regardless of whether the random variable 
is discrete or continuous, f(x) ~ 0 for all values of x. Second, for discrete 
random variables, L:f(x) = 1, where the summation is over all permissible 
values of x. If the random variable is continuous, then necessarily J f(x)dx 
= l, where the integral is over the entire range for whichf(x) may take on 
nonzero values. 
These conditions allow us to examine particular functions and deter-
mine if they could be probability densities. We will consider several func-
tions and examine the conditions under which they could or could not be 
considered probability densities. First, we will deal with a couple of examples 
where the random variable is discrete. 
1) Let the random variable x be defined over the finite set x = 0, l, 
2, ... , n, and consider the function <f> 1(x) = 1/n, where n is a positive 
integer. Certainly <I>I(x), a constant, is everywhere non-negative. If we care-

14 
CHAPTER 2 
fully examine the definition of x, however, we find that it can take on 
n + 1 discrete values, and that therefore the sum 
L: ri>t(X) = (n + 1)/n i= 1. 
If we want to use ¢ 1(x) as a probability density we must allow only n 
members to the set of possible values of the random variable: e.g., x = 1, 
2, ... , n. Defined in this way, with n = 6, ¢ 1 is a reasonable probability 
distribution for the number on the side that comes up when an honest die 
is thrown. When we want to point out that the exact nature of the probability 
distribution depends on the value of n, we will write ¢ 1(xln). This emphasizes 
that n is a parameter that defines the member of a family of distributions 
with which we are dealing. 
2) As an example of a probability density for a discrete random vari-
able which can take on an infinite number of possible values, consider 
rf>ix) = A ax, where A and a are positive constants, a < 1, and x = 1, 2, 3, 
· · · is the set of all positive integers. It is not difficult to show that L: ¢ 2(x) 
converges when a < 1, and this sum is given by A a/( 1 -
a). Therefore 
¢ 2(x) is a possible probability density if and only if 
A = (1 - a)/a, 
or rf>ix) = (1 - a)a<x-o. 
We will encounter this "geometric" distribution in a practical application 
in Chapter 3. 
3) The continuous analog to the discrete example 1) is the function 
0 
X< 0} 
¢3(x) = 1/N 0 ~ x < N 
. 
0 
X> N 
(2.1) 
This "uniform" distribution is clearly a proper probability density if only 
N > 0. Note that, in this continuous case, it is not really significant if 
x = 0 is included or excluded from the set of possible values of x. We can 
generally neglect the infinitesimal probability that a continuous random 
variable will take on exactly any specific value. (Exceptions to this general 
rule occur in meteorology and have to be treated as special cases. For 
example, if precipitation is treated as a continuous variable, we must con-
tend with the finite probability of zero precipitation. Another possible ex-
ception is cloudiness, although this is less frequently treated as a continuous 
variable.) 
4) As a final example, let us consider rt>ix) such that ¢ 4(x) = 0 for x 
outside the range (0, 1), and has the form rf>ix) = ax+ b (a straight line) 
in the interval 0 < x < 1. To determine permissible values of a and b we 
will consider first the condition that 

SOME FUNDAMENTALS OF PROBABILITY 
15 
f ¢ix)dx = l, 
and then the condition that f(x) ~ 0 everywhere. First we find that the 
integral condition is that (a/2) + b = l, so that the density can be rewritten 
¢ 4(x) = l - (a/2) + ax. Then, since we are dealing with a linear function 
of x, we note that it will be positive everywhere if it is positive at each of 
the endpoints x = 0 and x = l. These two conditions result in the require-
ment that -2 < a < 2. This in turn implies that b lies in the range (0, 2). 
Another way, rather than with the probability density, in which to 
define a probability distribution is in terms of the cumulative probability 
function. For discrete random variables the cumulative probability function 
is defined as 
F(x) = L f(y) = P{x < x}, 
y~x 
while for continuous random variables the cumulative distribution function 
lS 
F(x) = ixoo f(x)dx. 
Note that this implies that, if F(x) is given, we can determinef(x) as dF(x)/ 
dx. Also, for both discrete and continuous random variables, 
P{a < x < b} = F(b)- F(a). 
We will use throughout the notation thatf( 
) represents, in a generic 
sense, a probability density function and that F( 
), in the same general 
way, represents a cumulative probability function. Also, G( 
) = l - F( 
). 
When we want to indicate a specific probability distribution we will use a 
subscript on the name of the function. For example, we will define the 
uniform distribution, as in 3) above, as 
0 
x< 0 
} 
fu(xiN) = I IN 0 ~ x ~ N 
. 
0 
x~N 
Note in this case that 
0 
Fu(x) = x/N 
x<O 
} 
O~x~N . 
x~N 

16 
CHAPTER 2 
In fact we have here defined a family of distributions: the family of uniform 
distributions having a single parameter N. 
Now let us consider the geometric distribution, which involves a dis-
crete random variable, encountered in 2) above. We will introduce a rel-
atively inconsequential change by replacing a with b = 1 - a (since 0 < a 
< l , so also 0 < b < 1 ), and then we will write the geometric probability 
density as 
fg(xlb) = b(l - bY- 1 x = 1, 2, 3, · · ·. 
The notationfg(xlb) may be read as "the geometric probability density of 
x, given b" or as "the geometric probability density of x, with parameter 
b." The functionfg(xlb) represents a "family" of distributions, all referred 
to as "geometric", but distinguished by different values of the para-
meter b. 
The cumulative probability function in this case is 
Fg(xlb) = L: fg(ylb) = b L: (l - b'r1 
y.<;x 
= 1-0- bY. 
This last relationship can be derived by noting that Fg(xlb) could also have 
been written as 
co 
Fg(xlb) = 1 -
L: fg(ylb) = 1 -
L: (l - b'r1 
y>x 
y=x+l 
and simplifying the infinite sum. 
2.4 
EXPECTATIONS AND MOMENTS 
If we are dealing with a random variable, then the probability distri-
bution expresses fully what we know about the variable prior to an actual 
observation. Frequently, a description of the entire distribution is more 
information than is really needed or sought. In many cases we are content 
with a statement of the average or expected value of the random variable, 
and perhaps with information on the dispersion anticipated among different 
realizations. The moments of the distribution, often only the first and sec-
ond moments, are frequently all the information that is really needed. 
If g(x) is some function of the value of a random variable, and f(x) is 
its density, 1 then if .X is discrete the expectation of g(x) is 
1 Note that g(x) is an arbitrary function but that.ftx), consistent with the usage convention 
we have adopted, has all the necessary features to be a probability density. 

SOME FUNDAMENTALS OF PROBABILITY 
17 
E[g(X)] = L g(x;)f(x;), 
where the X; are the possible realizations of x, and the sum is over all values 
of i for which X; is defined. If x is continuous then 
E[g(x)] = i: g(x)f(x)dx. 
Moments are defined as expectations of (positive integral) powers of 
the random variable. Thus the first moment is E(X), the second E(x2), etc. 
The variance, which we shall frequently use as a measure of the dispersion 
of a distribution, is defined as 
V(X) = E[x - E(x)f = E(x2) - [E(x)f. 
Partly as an exercise to become familiar with the concepts of expec-
tations and moments, and partly to become more familiar with the notation 
we shall be using, let us consider the first two moments and the variances 
of the distributions we defined earlier as the uniform distribution.fu(x) and 
the geometric distribution /g(x). First, for the uniform distribution we write 
Eu(x!N) = LN xfu(x!N)dx = LN (x/ N)dx = N/2, 
which is the first moment, or mean. The second moment is 
The variance of a random variable with the uniform distribution is then 
The standard deviation sdu(x) is the square root of the variance, 
or .f3N/6. 
Now consider the geometric density /g(xlb) = b( 1 - b)O-x>, defined for 
x = 1, 2, · · • . To determine the moments of this distribution we must 
recall how to simplify a couple of infinite sums. In particular, we note that 
r + 2r2 + 3r3 + · · · = rj(l - r)2, 
r + 4r2 + 9r3 + · · · = r(r + 1)/(2- r)3. 
(2.2) 
(2.3) 
[To prove (2.2) note that if its sum isS, then rS is also a power series in 
r, as is r2S. By grouping terms, it is easy to show that (1 - rfS = r. In a 
similar way, if Tis the sum in (2.3), then ( 1 - r)T = ( 1 + r)S.] With these 
infinite sums known, it is a direct application of the definition of the mo-
ments of a discrete random variable to determine 

18 
Eg(xlb) = o - b)/b, 
Eg(x 2lb) = o - b)(2 - b)fb2, 
Vg(xlb) = o - b)/b2. 
CHAPTER 2 
2.5 
JOINT, MARGINAL AND CONDITIONAL PROBABILITIES 
We will frequently be forced to consider two or more random variables 
in conjunction with one another. The most frequent situation involves an 
observation (or a statistic representing a collection of observations) and a 
parameter about which we are trying to gain information. In other cases, 
however, we might want to consider the joint probabilities of two or more 
statistics to be determined from a set of observations, or that we want to 
predict on the basis of our knowledge of how, stochastically, they are gen-
erated. 
The complete description of what is known about a group of two or 
more random variables is contained in their joint distribution. We will 
discuss the attributes of joint distributions here in terms of a bivariate, or 
two-variable distribution, but everything is rather generally applicable to 
distributions of three or more random variables. We shall find it necessary 
to deal with multivariate distributions in Chapters 5, 6 and 7. 
As with univariate probability densities, the specific interpretation of 
a bivariate, joint probability density depends on whether the random vari-
ables are discrete or continuous. If x and yare both discrete random vari-
ables, then 
f(x, y) = P{(x = x) and 
(jl = y)}. 
If x and yare both continuous, then 
f(x,y)dxdy=P{(x<x~x+dx) and 
(y<y~y+dy)}. 
It is also possible that one is discrete while the other is continuous. If x 
happens to be discrete and y is continuous, then 
f(x, y)dy = P{(x = x) and 
(y < y ~ y + dy)}. 
The specific interpretation is always apparent by the context. 
The individual probability densities fix) andf(y) retain their meanings: 
they refer to the probability densities of x and y regardless of (or with no 
information about) the value of the other. The individual densities are also 
referred to as marginal densities, and are related to the joint densities by 
f(x) = J 
f(x, y)dy or f(x) = :2: f(x, y), 
y 

SOME FUNDAMENTALS OF PROBABILITY 
19 
whichever is appropriate (the former if y is continuous, the latter if it is 
discrete). (The integral or the sum must of course be over the entire allow-
able range of the random variable }i.) It should be recognized that the sum 
or integral of f(x), as appropriate, must be one for f(x, y) to be properly a 
probability density. 
As an example, consider the function 
f(x, y) = {b(x + Y - xy), 
0.;;;; X, y.;;;; 1 
otherwise. 
Since these are intended to be continuous random variables, 
f(y) = f ~(x + y - xy)dx = 2(1 + y)/3, 
and similarly 
f(x) = 2( 1 + x)/3. 
(2.4) 
It is easy to confirm that the integrals ofj(x) andf(y) are both 1. This, plus 
the fact that f(x, y) as defined is everywhere non-negative, allows us to 
accept the function as a probability density. 
We frequently deal with the conditional probability of one random 
variable, given that the other takes on some specific value. We have already 
encountered the notation of conditional probability in the situation where 
the exact form of the distribution depends on some parameter. (Remember 
that when dealing with parameters our interpretation of probability is in 
terms of "degree of belief", i.e., in terms of our state of knowledge of the 
parameter. Thus the conditional density of a datum, given the parameter, 
can be thought of as the probability function of the observation under the 
condition that the parameter has a specific value.) The conditional prob-
ability density is defined in terms of the joint and marginal distributions 
as 
f(xiy) = f(x, y)/f(y) f(y) > 0. 
(2.5) 
If f(y) = 0, then necessarily f(x, y) = 0, and f(xly) is undefined (and un-
interesting). The interpretation of this definition is quite direct. To evaluate 
the probability of .X when y is given, evaluate the joint probability for the 
given y, and normalize it by dividing by f(y) to ensure that its sum or 
integral over x will be unity. 
Consider the joint density defined in (2.4). We have already shown 
that the marginal densities are f(y) = 2( 1 + y)/3 and f(x) = 2( 1 + x)/3. 
Therefore 
f(xiy) = 2(x + y - xy)/( 1 + y). 

20 
CHAPTER 2 
We can write, for example, 
f(xiO) = 2x, 
!(xI~) = 2(x: 1) , 
f(xil) = 1. 
In other words, given that y = 0, the probability that xis near zero is small, 
and the probability density increases linearly with x. But when y = 1, all 
values of x are equally likely. 
Just as we often deal with marginal and conditional distributions, we 
also often deal with marginal and conditional expectations and moments. 
For example, the conditional mean of x, given y, is J xf(xiy)dx. In the 
present example this reduces to 
2+y 
E(xiy) = 3(1 + y). 
One of the attributes of a joint distribution that may be significant 
when it occurs is independence. Independence between two random vari-
ables means that knowledge of one provides no information about the 
other. This is so if and only if f(x, y) = f(x)f(y). In that case f(xiy) = f(x) 
and f(ylx) = f(y). In other words, no matter what is known about y, one 
always has the same beliefs about x, and vice versa. The conditional and 
marginal distributions are the same. 
2.6 
BAYES' THEOREM 
Equation (2.5) can be rewritten in several ways. In particular we will 
note first the symmetric relationship f(x, y) = f(x)f(yix), and substitute this 
in (2.5) to obtain 
/11 I ) = f(yix)f(x) 
\X y 
f(y) 
. 
(2.6) 
This is one way to represent Bayes' Theorem. We can modify it somewhat 
by noting that the denominator can be rewritten 
f(y) = I f(x, y)dx = I f(yix)f(x)dx, 
and therefore (2.6) becomes 
f(xiy) = 
f(yix)f(x) 
. 
I f(yix)f(x)dx 
(2.7) 

SOME FUNDAMENTALS OF PROBABILITY 
21 
This is the form of Bayes' Theorem that we will apply repeatedly in the 
pages to follow. 
To discuss the significance of this relationship, and the context in 
which it will be applied, we will introduce a minor change of notation. 
First, let us consider that the random variable .X represents the small set of 
parameters that govern the stochastic data-generating process, i.e., the 
probability distribution, from which the observations y are obtained. To 
emphasize this interpretation we will write 1r (for parameters) in place of 
x, and w (for observations) in place of y. Note that w, like 1r, may represent 
a group of variables that can be thought of as vector random variables. The 
first term in the numerator of (2. 7) is now 
which is specifically the probability density of obtaining the observations 
(w) when the parameter(s) has (have) the value(s) 1r. Viewed as a function 
of 1r, this is referred to as the "likelihood." It is important to recognize that 
the functionfdgp(wl'll"), as a probability density, when summed or integrated 
over all values of w must be unity. The same function, as a likelihood, 
summed or integrated over all values of 1r, will in general not be unity. If 
two likelihood functions are proportional to one another then they are 
entirely equivalent. 
We shall write the second factor in the numerator asfo(1r) and refer 
to it as the prior probability distribution on the process parameters. It 
represents our beliefs, i.e., our knowledge about the parameters before the 
observations w are made or noted. As we will demonstrate and emphasize 
later, this prior belief includes all information that may have been contained 
in earlier observations: it is only prior to that which is not yet available or 
known; it is posterior to all information that preceeds it. 
We shall write the left-hand side of(2.7) asjj(1rlw). This is the posterior 
distribution of the parameters 1r. It represents a probability statement about 
the parameters that is conditional on having observed a particular set of 
data, i.e., posterior to the data represented by w. It is also, by implication, 
conditional on all the knowledge encoded in the prior distribution fo(1r). 
Bayes' Theorem represents a rigorous rule by which a prior set of beliefs 
about some parameters is modified in the light of new information. 
The denominator on the right-hand side of(2.7) is the prior probability 
of the observations. It serves here as a normalization factor that assures 
thatjj(1rlw) is indeed a proper probability density. This is the probability 
density we will use when trying to predict future observations, and our 
knowledge of the parameter( s) ( 1r) of the data generating process is encoded 
as the prior density fo( 1r ). Once a particular group of observations is available 

22 
CHAPTER 2 
to us, however, our knowledge is encoded by ./i(1rlw). To make a predictive 
statement about some subsequent observations, Wsub• we will use 
f(wsublw) = I fcigp(Wsubl1r)./i(1rlw)d1r. 
(2.8) 
Note carefully that the prior and posterior probabilities play identical 
roles with respect to predictions. We use the one that represents the current 
state of knowledge. Furthermore, the probability density posterior to one 
set of observations becomes the prior probability with respect to a subse-
quent set of observations. If two groups of observations, say w1 and w2 are 
available, we have the choice of applying Bayes' Theorem once to the 
combined set of observations, or sequentially, first to w1 to obtain an interim 
posterior density, and then to w2 to obtain a final posterior density. It does 
not matter which we choose: the final posterior density, and thus any sub-
sequent predictions, will be identical. [In the common situation where the 
observations are independent, i.e.,flw1, w2) = f(w 1)flw2), their order is of 
no consequence, and Bayes' Theorem could be applied first to w2 and then 
to w1, again giving an identical result.] 
Let us reemphasize here the nature of the interpretations we are ap-
plying to our probability statements. They are all statements of degree of 
belief. In the case of the likelihood function there would be no substantial 
difference between the interpretation given here and that which would be 
given from a classical sampling theory perspective. For this aspect of the 
analysis we may indeed be able to imagine many trials from a specific 
data-generating process with given parameters, and the subjective view of 
probability will then coincide with the relative-frequency view. But the 
prior, posterior, and predictive probabilities are personal and subjective 
and depend very explicitly on the statistician's or meteorologist's judgement. 
Everyone should revise his or her probabilities in the same way, according 
to (2. 7), but since prior judgements can differ, so also will the posterior 
and predictive judgements. We will illustrate repeatedly, however, that 
these differences tend to become small when the data become plentiful. 
When the data are few in number (or perhaps do not exist) then the dif-
ferences among individuals' degrees of belief-probability statements-
may be large. Only ifthe initial belief was very dogmatic (e.g., vanishingly 
small or zero probabilities assigned to parameter values which turn out to 
have, in view of subsequent data, large likelihood) will reasonable amounts 
of data fail to lead to convergent assessments of posterior probabilities. 
Note, from (2. 7), that if the prior probability we assign to some value of 
i is zero, then no amount of empirical evidence can convince us that such 
a value may actually be valid. Thus we rarely assign prior probabilities that 
are identically zero. 

SOME FUNDAMENTALS OF PROBABILITY 
23 
2.7 
SUFFICIENT STATISTICS 
Now let us consider how we use the information inherent in the ob-
served data to revise judgements about the process parameters. Assume 
that there are n observations: w1, w2 , •.• , wn. The likelihood function in 
(2.7) depends on all the observations. We will introduce some statistics 
(e.g., means, modes, standard deviations, extrema) which are, by definition, 
functions of the observations. One such function may be written symbol-
ically as T(w~> w2 , •• , , wn) = T(w). Consider the possibility that the like-
lihood function can be rewritten, with the help of the statistic T, as 
./dgp(wl?r) = K(w)L[T(w), 1r], 
(2.9) 
in other words that the likelihood can be factored into two terms, one of 
which depends on the process parameter and the data only in terms of the 
statistic T, and the other depends perhaps on other aspects of the data, but 
not on the parameters 1r. If this is the case, we can substitute (2.9) into 
(2. 7) and obtain the result 
};(1rlw) = 
L[T(w), 1r]/o(1r) 
I L[T(w), 1r]/o(1r)d1r 
The factor K(w) does not appear because, not being a function of 1r, it can 
be factored outside the integral in the denominator, where it cancels the 
same factor in the numerator. This means that the statistic T, by itself, 
contains as much information about the parameter(s) as is contained in 
all the observations. 
Any statistic which meets the criterion (2.9) is a sufficient statistic with 
respect to the parameter(s) 1r. More generally, T(w) may itself be a vector 
of separate components, e.g., T(w) = T[t 1(w), t2(w]. In that case we talk of 
t1 and t2 as beingjointly sufficient statistics. If we are able to find a sufficient 
statistic (and whether or not this is possible depends on the functional form 
of the likelihood) then it is much easier to deal with the information content 
of the observations. We need only consider the sufficient statistic(s) instead 
of repeatedly examining all the data. 
We shall illustrate the identification of sufficient statistics with two 
examples: the uniform data-generating process and the geometric data-
generating process, which were both defined earlier. 
1) First consider n observations from a geometric data-generating 
process with parameter b. Then observations are X;, i = I, 2, ... , n. The 
probability density for each, as given earlier, is 

24 
CHAPTER 2 
The n observations are independent events; there is no dependence in the 
probability density for any X; on the order or value of other observations. 
Therefore the joint probability of the n observations is the product of the 
probabilities of the individual x;: 
/g(x,, Xz, ... , Xnlb) = /g(x,lb)/g(xzlb) • • ·/g(xnlb) 
= bn(l - b)-n(l - b)l:x;. 
Viewed as a function of the parameter b, this is the likelihood function. It 
depends on the observations only through their contribution to the sum 
L X;. This sum, therefore, is a sufficient statistic. The data can provide no 
more information about the parameter of the process, b, than is inherent 
in knowledge of the sum of n observations of X;. 
The geometric distribution is appropriate when a series of trials are 
made, each trial being independent of the results of prior or subsequent 
trials, and each having the same probability of some particular event such 
as the total seasonal discharge of a watershed exceeding some critical value. 
The term/g(xlb) describes the probability that for x- 1 consecutive times 
the critical values will not be exceeded and that the first exceedence occurs 
on the xth trial, given that the probability of an exceedence in each year 
is b. Thus if we observed the first exceedence in the fourth year, wait eight 
more years for the second, and the third occurs in the very next year, then 
the total information pertinent to inferences about b is that 2: x; = 13 and 
rt = 3. Nothing else involving the observations is of interest. 
2) The uniform distribution with parameter N generates random, and 
again independent observations with probability densities 
0 
X;< 0 
} 
fu(x;IN) = 1/N 0 <X;~ N 
. 
0 
X;>N 
It follows that the joint probabilities of n observations can be written 
This, in turn, can be stated as 
N-n 
fu(X,, Xz, . .. , XniN) = O 
0 <X;~ N 
otherwise 
i = 1, 2, ... , n} . 
max{x~, Xz, ... , Xn} ~ N1 . 
otherwise 
f 
(2.10) 
In this case the data enter into the likelihood [viewing (2.10) as a function 
of N, the parameter] only through the number of observations and the 

SOME FUNDAMENTALS OF PROBABILITY 
25 
maximum among the observations. These two quantities are therefore 
jointly sufficient statistics for N. 
Consider visiting at infrequent intervals in winter a site of frequent 
snowfall and slightly less frequent avalanches, and measuring snow depth. 
The amount of snow measured might be approximately linearly related to 
the time since the most recent avalanche. The snow depth cannot exceed 
the (unknown) critical depth at which, for the slope in question, an ava-
lanche will spontaneously occur. (Clearly such a dependence on depth only 
is a simplification.) In this situation we can look upon our observations of 
snow depth as being taken from a uniform distribution with an unknown 
upper bound N. The information that is useful to us is a) the maximum 
depth observed-clearly N is at least that large-and b) the number of 
times we have made the observations. 
Identifying sufficient statistics is usually not troublesome. The likeli-
hood of the process parameter(s), expressed in terms of the sufficient sta-
tistics, can then be used directly in Bayes' Theorem. Usually it will not be 
necessary to evaluate the denominator, but instead to use only the pro-
portionality relationship 
./i('ll"lw) oc L{T{w), 1r}/o(1r). 
(2.11) 
2.8 CONJUGATE DISTRIBUTIONS; PRIOR AND POSTERIOR 
PARAMETERS 
The parameters 1r that appear in the preceding section are parameters 
of the data-generating process by which the observations w are produced. 
This is equivalent to saying that they are parameters that identify which 
particular member of a family of probability distributions best describes 
our knowledge of what observations will occur. 
We have also expressed our knowledge of the process parameter(s) in 
terms of a probability distribution fo( 1r ), and this also will often be identified 
as one member of a family of distributions. Thus we shall write the prior 
probability density .fo( 1r) as f.( 1rl8), where ¢ identifies the family of densities 
to which the prior belongs, and 8 identifies the particular member of that 
family; 8 is a parameter (or set of parameters) that defines specifically the 
state of our knowledge of the process parameter 1r. 
Given a particular data-generating process, the functional form of the 
likelihood is fixed: the functional form of the prior is determined by the 
choice of the family of distributions,¢, used to codify the prior beliefs; the 
functional form of the posterior density then follows from Eq. (2.11). There 
is nothing to guarantee that the posterior density will resemble, functionally, 
anything familiar or tabulated. For some likelihood functions, however, 

26 
CHAPTER 2 
the prudent choice of the family cf> for the prior belief results in a posterior 
density that is a member of the same family of densities. When this is 
possible (and it is not possible for all likelihood functions), and if we can 
accept some member of that family as an accurate statement of our prior 
belief, then this leads to a very convenient recursion relationship for chang-
ing our beliefs as data accumulate. We begin with a prior parameter, say 
(),and calculate, through use of an algorithm based on (2.11) and depending 
on the observed sufficient statistic(s), T(w), the parameter value for the 
posterior density (the posterior parameter) 0'. The parameter ()' can then 
be considered a prior parameter with respect to some subsequent obser-
vation(s) w'. Using the same algorithm as before, from()' and w' we determine 
a new posterior parameter ()". This can be repeated any number of times. 
The final parameter value identifies the member of the family of probability 
densities, ¢, that best describes our knowledge about 1r consistent with the 
prior belief and all the data. 
When a family of densities can be found for a particular likelihood 
(i.e., for a particular data-generating process) that has the characteristic 
that the posterior and prior densities are of the same family, this is referred 
to as a conjugate family. We often refer to a conjugate prior as a prior 
density for which the posterior density is of the same family. In subsequent 
chapters conjugate priors will be identified for a variety of data-generating 
processes. In fact, we find the use of conjugate distributions to be so con-
venient that we will deal almost exclusively with situations in which they 
can be used; this turns out to be a very wide range of situations. 
Let us return to the uniform data-generating process over the interval 
(0, N), where N is the parameter about which we are trying to make in-
ferences. We showed earlier that sufficient statistics are n and the largest 
of the n observations, and that the likelihood function is proportional to 
1/N", 0 < Xmax < N, but zero otherwise. 
What might have been the prior probability density of N before ex-
amining the data? There are literally an infinite number of possibilities. 
We will examine two of them, both of which will be two-parameter families 
of distributions. One parameter will be N0 , a lower bound on the values 
of N that are considered possible. The other parameter in each case will 
be a scale parameter, which we will again call 0. One prior density, family 
(a), is given by 
fa(NJNo, 0) = () exp[-O(N- No)], 
N> N0 
oc exp( -ON). 
The other prior density, family (b), will be given by 

SOME FUNDAMENTALS OF PROBABILITY 
27 
I:(NjN, ()) = ()N, ON-(0+1) 
Jb 
o, 
0 
' 
N>No 
We have explicitly excluded as irrelevant, factors that do not involve N 
and therefore would cancel out in Bayes' Theorem (2. 7), using (2.1 0). 
We determine the functional form of the posterior density by multi-
plying the likelihood by the prior density (remembering that the likelihood 
is zero for N < Xmax. and that both priors are zero for N < N0). For case 
(a), then, the posterior density is proportional to 
N-n exp(-ON), N> max{No, Xmax}, 
which specifies a reasonable probability density, but has a more complicated 
form than the simple exponential form of the prior. 
For case (b), the posterior density becomes 
Jb(NJNo, 0) oc N-<n+O+I), 
N > max{No, Xmax}, 
which is clearly the same form as the prior. If the prior parameters were 
1.5 
1.0 
0.5 
0. 0 L__jL__j~_j__::::::;:::3~~=-....1.---l _ _J 
0 
2 
3 
4 
5 
6 
7 
8 
N 
FIG. 2.1. Pareto probability densities representing prior and posterior probabilities on N, 
the critical snow depth for avalanche. (a) The prior belief with parameters N0 = 2 and fJ = 3. 
(b) The posterior belief with parameters N0 = 2 and fJ = 5, (c) A posterior belief if the largest 
of the two observations is 3, so that the posterior parameters become N0 = 3 and 0 = 5. 

28 
CHAPTER 2 
N0 and 8, then the posterior parameters, referring to the same family of 
distributions, are N'o = max{N0, Xmax} and 8' = 8 + n. This distribution 
family, known as the Pareto distribution, is conjugate to the uniform data-
generating process. 
Let us suppose that we do indeed choose a Pareto distribution to 
represent our prior views, and select as appropriate parameters 8 = 3 and 
N0 = 2.0 m. This would represent the prior view, referring to the critical 
snow depth for an avalanche, that the critical_value is certainly at least as 
great as 2.0 m. It implies a prior expected value for N of 3.0 m but a median 
value of about 2.6 m (N02116}. Larger values of the parameter 8 would imply 
greater confidence that the critical depth is near 2.0 m. 
We will suppose that two observations are made, the maximum of 
which is 1.8 m. The posterior probability density will then be proportional 
to N-<n+B+t> = N-6• The prior and the posterior probability distributions 
are both shown in Fig. 2.1, the prior as curve (a) and the posterior as curve 
(b). The posterior belief is somewhat stronger that N is near 2.0. The op-
portunity for the observation of larger values has passed without their oc-
currence. 
Also shown in Fig. 2.1 [curve (c)] is the posterior density that would 
be appropriate had the maximum of the two observations been 3.0. Of 
course, N < 3 is now excluded. Values near 3.0 are now considered much 
more likely than was the case before the observations were made, but the 
posterior probability density still falls off as N-6, compared to N-4 for the 
prior, and so probabilities of very large values (say N > 7) are decreased. 

Chapter 
3 
Bernoulli Processes 
3.1 
DEFINITION OF A BERNOULLI PROCESS 
A Bernoulli process deals with individual occurrences or trials. On 
each trial one and only one of two possible events can occur. The essential 
characteristic of a Bernoulli process is that the probability of each of the 
two mutually exclusive and exhaustive outcomes, say A and -A (not A), 
is constant from trial to trial. In particular, these probabilities, p and 
1 - p, do not depend on how many trials have taken place, the outcomes 
of other trials, or the order in which the events occurred. The outcome of 
any trial is independent of the outcome of any other trial and depends 
solely on the probability p that characterizes the process. 
Traditionally the outcomes of Bernoulli trials are referred to as "suc-
cesses" and "failures". The notion of success or failure is entirely arbitrary 
and should not be given a literal valuative connotation. On each trial either 
a success (with probability p) or a failure (with probability 1 - p) will occur. 
The probability of success, p, is a parameter that completely describes a 
particular Bernoulli process and distinguishes it from all other Bernoulli 
processes. 
Let us introduce the notation that S; means success on the ith trial. 
We then have 
P{S;} = p 
i = 1, 2, · • •. 
29 

30 
CHAPTER 3 
From our definition of a Bernoulli process we know also that 
In other words, knowledge of the outcome of the jth trial in no way influ-
ences our knowledge (the probability) of what will occur on the ith trial: 
the trials are independent. 
Classical examples of the Bernoulli process are the tossing of a coin 
and the throwing of dice. The "success" may be defined as "heads" or the 
occurrence of any well-defined outcome, such as each of two dice landing 
with sixes up. Note that it is immaterial whether the coin or the dice are 
"honest." The Bernoulli process refers to the notion that the probability 
of the relevant event does not change from trial to trial. The question of 
"honesty" refers to whether the probability of a "head" is exactly l/2, or 
whether the probability of two sixes is exactly l/36. It is important to 
recognize that the concept of a Bernoulli process is useful whatever the 
value ofthe probability of the event in question. 
It is also important to recognize that repeated trials are not a necessary 
characteristic of a Bernoulli process. The probability of rain tomorrow 
depends on today's unique conditions; the today-tomorrow sequence of 
weather will never be completely replicated. We may still treat the event 
of rain tomorrow as a Bernoulli process. It is much like tossing an irregularly 
shaped, multifaceted die so that it will land in wet cement, the outcome 
of the single toss remaining there for all to see; the specific experiment will 
never again be attempted. 
From a climatological point of view there is a very large class of events 
that may be thought of as outcomes of Bernoulli processes. For example, 
will the surface temperature during the month of January at Fairbanks, 
Alaska fall below -45 oc? Each January is a trial; in each trial month the 
minimum temperature either does or does not fall below -45 oc. Barring 
additional information (and perhaps also if we have additional information) 
we may reasonably assume that the probability of this event is the same 
from one year to the next. 
In more general terms we may consider as a Bernoulli process the 
exceedence of some specific value by any random variable whose probability 
distribution does not change with time (or precursor event). In climato-
logical applications this generally means that the individual exceedences 
are sufficiently separated in time and/or space that they can be considered 
independent events. Similarly we can define an event in terms of a (random) 
variable falling within some limits. A standard procedure, for example, is 

BERNOULLI PROCESSES 
31 
to categorize monthly or seasonal mean temperatures or total precipitation 
as below normal, near normal, or above normal. Here the parameter p 
might represent the probability of "near normal" occurring. 
In addition to exceedences, there are discrete events such as tornadoes 
or hail, or ice on rivers or lakes, whose occurrences may, in the suitable 
climatological context, be treated as Bernoulli processes. 
For all these examples, and for many more like them, the climatologist 
or meteorologist may be expected to make predictions. Necessarily these 
must be probabilistic. If the parameters of the processes are known, it is 
relatively easy to determine the probability of future outcomes ofBernoulli 
trials, either singly or in combinations. In other words, if one knows that 
a coin is "honest" then probabilistic predictions of future tosses are quite 
simple. If one has a large number of observations of tosses with the same 
coin then the process parameter, for all intents and purposes, can be treated 
as known (and given by the relative frequency of observed "successes.") 
The problem and the challenge arise when the coin may not be well bal-
anced, and one has been able to observe only a few tosses of the coin to 
acquire relevant empirical evidence. How, then, are we to make optimal 
probabilistic predictions, and how good will these predictions be? Or, re-
turning to the probability of low temperatures in Fairbanks, how would 
we use the information contained in a short period of record (ifthat were 
all that was available in a particular situation), and indeed any other relevant 
information or knowledge, to make useful statements about conditions in 
future Januaries? 
3.2 DISTRIBUTIONS OF SUFFICIENT STATISTICS 
We will consider a series of trials with a Bernoulli process having a 
parameter (probability of "success" on each trial) p. A random variable X; 
is defined for each trail such that X; = 1 if there is a "success" on the ith 
trial and X;= 0 otherwise. Clearly, X; is a discrete random variable and its 
probability density can be written 
P{.X;} = pxi(l - p)I-x; 
X;= 0 or 1, 
i = 1, 2, .... 
This simply says that P{ 1} = p and P{ 0} = 1 - p. 
If there are n trials, independent by definition, then the joint probability 
distribution of the n values of X; is given by 
(3.1) 
The summations are over all values of i from 1 to n. As should have been 
anticipated, the joint probability does not depend on the order in which 

32 
CHAPTER 3 
the X; occurred, but only on their sum which is simply the number of 
"successes" that occurred in the n trials. 
Looked at from the perspective of making inferences about the process 
parameter p, Eq. (3.1) is the likelihood function. As discussed in Chapter 
2, if the likelihood function can be factored into one term that involves 
the parameter itself and some specific functions of the observations, and 
another term that does not involve the parameter, then the functions of 
the observations in the first term are sufficient statistics. In this case the 
only functions of the observations that appear in the likelihood function 
are LX; and n; these therefore are sufficient statistics for the parameter of 
a Bernoulli process. All the information about p that is available from the 
observations is contained in the values of n and LX;. We will write 
r = LX;, so the sufficient statistics are nand r. 
For purposes of making inferences about p, it is only necessary to 
note that the likelihood function is proportional to p'( 1 -
p)n-r. Before 
proceeding, however, to deal with using the observations to refine our 
knowledge about p, we will consider what we might say about future ob-
servations if the parameter were known. 
The probability density for r, given p and n that is, the probability of 
r successes in n trials when the probability of a success on each trial is p), 
is, from (3.1) 
P{fln, p} = Cp'(l - p)n-r. 
The proportionality factor C is needed to ensure that the sum of these 
probabilities over all possible values off(= 0, 1, ... , n) is unity. It is shown 
in most elementary statistics texts that this factor must be n!/[(n - r)!r!]. 
Thus 
P{fln, p} = I( n! t (1 - p)n-r 
r. n- r 
= Jb(fln, p), 
(3.2) 
where n ~ r ~ 0, 0 ~ p ~ 1, and n ~ 0. Equation (3.2) defines the "binomial" 
probability distribution for r, given n and p. That is, the probability of r 
successes in n trials of a Bernoulli process with parameter p is given by 
fb(f1n, p), the binomial density. Appendix A (SectionAl) gives a few per-
tinent facts concerning the binomial distribution. 
There are situations where we are interested not in the number of 
successes in n trials, but in the number of trials required to achieve r suc-
cesses. Clearly the probability of a success on the nth trial (whatever the 
value of n) is simply p. In order for a success on that trial to be the rth 
success there must have been exactly r- 1 successes in the first n- 1 trials. 
The probability of this isfb(r- lin- l,p). The probability of the intersection 

BERNOULLI PROCESSES 
33 
of these two independent events, i.e., r -
1 successes on the first n -
1 
trials, and success on the nth trial, is their product. This product defines 
the "Pascal" probability density on the random variable fi, given rand p: 
!Pa(fiir, p) = pfi,(r -
lin -
1, p) 
(n- 1)! 
~(1 
)n-r 
(r- l)!(n- r)! ~" 
- p 
(3.3) 
for n ~ r > 0, 0 ::;:;;; p ::;:;;; 1. More details on the Pascal distribution are given 
in Appendix A (Section A2). 
One other way of examining the results of a series of Bernoulli trials 
is to treat the number of successes as given, and to treat the number of 
failures x that occur before the rth success as the random variable. Of 
course x = n - r, and the probability distribution of .X, given r and p, can 
be derived directly from the Pascal distribution for fi given r and p. This 
distribution is called the negative binomial and its form is given in Appendix 
A (Section A3). Note that here, too, just as it was with the binomial and 
Pascal distributions, the dependence on p is in terms of p'( 1 - p)n-r. 
Whether we deal with the joint distribution of all the X; (3.1 ), the 
distribution of r with n given (3.2), the distribution of n with r given (3.3), 
or the distribution of n - r given r, the functional dependence on p, the 
process parameter, is the same. For the purpose of making inferences about 
p, it is only this dependence of the likelihood on p that matters. In particular, 
it does not matter whether we stop making observations because of a total 
of n trials have been made, or because the rth success has been observed. 
It is just the values of the sufficient statistics, n and r, which appear in the 
term p'(l - p)n-r, that are needed. 
3.3 PRIOR AND POSTERIOR PROBABILITIES 
We will now recognize that our knowledge of the Bernoulli process 
parameter is incomplete and treat the parameter as a random variable. 
With the help of some observations and Bayes' Theorem we will then make 
inferences about p, the process parameter. The situation with which we 
are dealing has the following characteristics: 
I) We are convinced that the process by which the data are generated 
is Bernoulli. 
2) The process parameter of the Bernoulli process is uncertain. We 
will codify our knowledge about pas a probability density function.fo(p), 
a prior probability density. 
3) We will have available to us some data (rand n) obtained after the 
prior probability statement about p was formulated, and we will want to 

34 
CHAPTER 3 
use that information, in conjunction with.fo(p), to infer a posterior prob-
ability density fi(p). 
Bayes' Theorem (Eq. 2.7) says that the relationship between the prior 
and posterior densities is given by 
Ji(ftlr, n) oc fo(p} L(plr, n) 
which we can now write as 
Ji(ftlr, n) oc fo(ft)fb(fln, p) 
or 
Ji(ftlr, n) oc fo(p)fPa(filr, p) 
or 
Ji(filr, n) oc fo(p)f,b(xlr, p), 
all of which reduce simply to 
fi<P'Ir, n) oc fo(p)J/(1 - Pr'. 
In each case the proportionality factor will be chosen to ensure that 
f fi(plr, n)dp = l 
or 2:fi(pir, n) = l, 
p 
whichever is appropriate (i.e., depending on whether pis treated as a con-
tinuous or a discrete variable). 
In order to apply Bayes' Theorem we must first choose a statement 
of the prior density. The only valid and limiting constraint on the selection 
of the prior density is that it validly represents our beliefs before we have 
made any observations implicit in the statistics r and n. In Section 3.4 we 
will develop some arguments that are meant to persuade the practitioner 
that one particular form for defining the prior is usually (but not universally) 
preferable to others. However, the overriding consideration must always 
be that our beliefs are properly represented by the prior. 
It is not always obvious how to assess or describe our judgements. The 
discussions in this and the next two sections are intended to offer some 
procedural suggestions. We know of no single superior way in which to 
accomplish this task. For example, the prior density can be either discrete 
or continuous. For most climatological applications p can take on any 
value in the interval [0, l ], so most of our attention will be focused on 
treating pas a continuous random variable. We will start, however, with 
a discrete example. 

BERNOULLI PROCESSES 
35 
Let us say that we are concerned with the occurrence or nonoccurrence 
of frost in Bowling Green, Ohio between 15 May and 15 September. We 
consider this to be a Bernoulli process with parameter p whose value is 
not known to us. We will declare, just for the sake of this exercise, that 
there are six allowable values for p: 0.0, 0.2, 0.4, 0.6, 0.8 and 1.0. Although 
we have seen no data, we do know a little about the location, elevation, 
and general setting, and this leads us to summarize our prior beliefs as in 
Table 3.1. 
Before any data .are obtained, we believe, according to the probabilities 
given in Table 3.1, that the probability that p = 0 (i.e., frost never occurs 
at that location during this period) is 0.3, the probability that p = 0.2 (i.e., 
that frost occurs one year in five on average) is 0.25, etc. We are also saying 
that it is totally incredible to us (zero probability) that frost occurs every 
year during this period. Note that the statement p = 0.2 is also equivalent 
to saying that we believe that it is four times more likely next year for frost 
to be absent than present. The notion that frost is a Bernoulli process and 
occurs on the average one year in five implies that we also believe that the 
same probability is relevant in each year. 
It helps us to appreciate the implications of this expression of prior 
beliefs if we consider the impact on our beliefs of some data. Let us imagine, 
in this contrived example, that we make observations at the location for 
five years (or, equivalently, allow ourselves to examine, for the first time, 
five years of data) and find that frost occurred in one and only one of these 
years. What should our beliefs about p now be, according to Bayes' 
Theorem? 
The discrete form of Bayes' Theorem that we will apply is 
.t;<ftl1, 5) = !o<P>fb< 115, p)!L. [fo(p)Jb( 115, p)J. 
p 
The necessary calculations are shown in Table 3.2. 
Note in particular that the single occurrence of frost completely elim-
inates p = 0 as a possibility. The data would also have eliminated p = 1 
as a credible alternative, if that had not already been excluded by the prior. 
If, perchance, r = n = 5, or even r = n = lOO had occurred, we would still 
have obtained the result that the posterior probability of p = l was zero. 
If we feel that we might be persuaded by such evidence to admit that 
p = 1.0 is possible, then it is inconsistent to have stated, a priori, that 
Parameter value p 
Prior probability frl..p) 
0.0 
0.30 
TABLE 3.1. Prior beliefs. 
0.2 
0.25 
0.4 
0.20 
0.6 
0.15 
0.8 
0.10 
1.0 
0.00 

36 
CHAPTER 3 
TABLE 3.2. Calculation of posterior probabilities after observing frost in one year of five. 
Parameter 
Prior probability 
Likelihood factor 
Posterior probability 
p 
frJ.p) 
f,.(1l5,p) 
Ji(pl1. 5) 
0.0 
0.30 
0.000 
0.000 
0.2 
0.25 
0.410 
0.615 
0.4 
0.20 
0.259 
0.312 
0.6 
0.15 
0.077 
0.069 
0.8 
0.10 
0.006 
0.004 
1.0 
0.00 
0.000 
0.000 
fi( l) = 0.0. We must always exercise care, in selecting a prior, to include 
every eventuality that we feel is at all possible, and not to exclude, by 
setting a prior probability exactly to zero, any unlikely though marginally 
credible possibility. 
The differences between the prior and the posterior probabilities are 
very dramatic, even though there were only five observations. According 
to the prior beliefs, there was less than a 50% probability that the parameter 
was other than 0.2 or 0.4. After the five observations, the probability is 
greater than 90% that the parameter is either 0.2 or 0.4. 
By way of comparison, let us also examine the posterior probabilities 
(with the same prior beliefs) if there had been three years with frost out of 
five instead of only one. These results are given in Table 3.3 
Note that the probability that fi > 1/2, after observing three frosts in 
five years, is 0.55, even though it was 0.25 before the observations were 
included. After only one frost in five years, the probability of fi > 1/2 is 
only 0.07. In this case the observations tend to confirm the prior judgement, 
while in the other case the prior view and the data tend to oppose each 
other. The posterior probabilities are a combination of prior knowledge 
and the statistical evidence. If the empirical evidence had been much stron-
ger, i.e., n was much larger, the posterior probabilities would have been 
much more influenced by the data. In the limit, as n becomes large, the 
TABLE 3.3. As in table 3.2 but for three years with frost out of five. 
Parameter 
Prior probability 
Likelihood factor 
Posterior probability 
p 
fr/..p) 
f,.(3l5,p) 
Ji(pl3, 5) 
0.0 
0.30 
0.000 
0.000 
0.2 
0.25 
0.051 
0.098 
0.4 
0.20 
0.230 
0.351 
0.6 
0.15 
0.346 
0.395 
0.8 
0.10 
0.205 
0.156 
1.0 
0.00 
0.000 
0.000 

BERNOULLI PROCESSES 
37 
influence of the prior disappears, except when a prior probability of exactly 
zero has been assigned. 
3.4. CONJUGATE ANALYSIS 
It was pointed out in Chapter 2 that there are some advantages when 
the prior is expressed in a functional form that is conjugate to the likelihood 
function for the particular data-generating process. We will now introduce 
a particular family of densities and show that if one member of this family 
is used as the prior density, then the posterior density will be another 
member of this same family; only the parameters of the density will change. 
In the following section we will ofter some suggestions as to how the prior 
parameters might be specified so that the selected prior indeed represents 
our beliefs. 
We will treat jj as a continuous random variable that can take on any 
value in the interval [0, 1 ]. The family of distributions we will use is the 
beta family, which requires two parameters, and is defined by 
ftl..ftla, b) = ~ -l(l - p)b-a-1/B(a, b), 
(3.4) 
where the parameters a and b are positive numbers with b > a > 0, and 
B(a, b) is a normalization factor defined by the requirement that f ftf..pla, 
b)= l. Usually referred to as the (complete) beta function, 1 it is related to 
the widely tabulated gamma function [r(a)] by 
B(a, b) = r(a)r(b)jr(a + b). 
More information on the beta distribution is provided in Appendix A 
(Section A4). 
With (3.4) as the prior density on jj, formal application of Bayes' 
Theorem allows us to write, for the posterior density after observing the 
sufficient statistics r and n, 2 
Ji(jjjr, n, a, b) oc p'(l -
p)n-r~-1(1 - p)b-a-1. 
The first two factors on the right come from the likelihood, and the latter 
two factors from the prior. Of course, this can be rewritten 
j;(jjjr, n, a, b) oc ~+r-1(1 - p)b-a+n-r-1. 
1 The incomplete beta function is ftfti.Pia, b)dpfB(a, b). 
2 Note that here r and n are treated as parameters and p is treated as a random variable 
while in the previous section, and again later, r or n are random variables and pis a parameter: 

38 
CHAPTER 3 
All that is needed to completely define the posterior density is a normal-
ization factor, and from the definition of the beta function [Eq. (3.4)] this 
is simply B(a + r, b + n). Thus we can write the posterior density as 
ft(fi!r, n, a, b)= p"+'- 1(1 - Pt+b-a-r- 1/B(a + r, b+ n) 
= frlfila + r, b + n). 
(3.5) 
If the sufficient statistics arising from a Bernoulli data generating pro-
cess are r and n, and the prior beliefs are expressesd as a beta density with 
parameters a and b, then the posterior beliefs about p are also given by a 
beta density, but with parameters a + r and b + n. The beta density is a 
conjugate prior for the Bernoulli data-generating process. 
A conjugate prior, used in conjunction with the process to which it 
relates, always produces a posterior density of the same form. This makes 
it particularly convenient to treat our state of knowledge posterior to (i.e., 
after) one group of observations as the state of knowledge prior to (i.e., 
before) a second set of observations. Not all data-generating processes have 
conjugate priors, but when they do exist,3 if they can be adapted to express 
our state of knowledge, then they are especially advantageous. 
It turns out that most conjugate families are "rich" in the sense that 
by careful choice of parameters a large range of statements of prior beliefs 
can be obtained. This is especially true of the beta family of distributions. 
Figure 3.1 illustrates some of the wide variety of statements of prior belief 
about p that can be accommodated through the use of beta densities and 
the appropriate choice of parameters. 
One of the interesting characteristics of Eq. (3.5) is the manner in 
which the prior parameters a and b relate to the statistics r and n, and the 
parameters of the posterior distribution. Note that if there were two sets 
of observations, one with statistics r 1 and n1, and the other with statistics 
r2 and n2 , then the posterior parameters would be a+ r1 + r2 and b+ n1 
+ n2 • The number of successes is additive to the prior parameter a, and 
the total number of trials is additive to b. To emphasize this relationship 
we will introduce a change of notation. We will write, for the prior param-
eters, r' and n'; and for the posterior parameters we will write r" and n". 
Then 
r' =a, n' = b, 
r" = a + r = r' + r, 
n" = b + n = n' + n. 
3 A conjugate family will exist if the sufficient statistics are of fixed dimension (De Groot, 
1970), i.e., if no matter how the data were collected, or how large the number of observations, 
a fixed number of functions of the observations form the sufficient statistics. 

BERNOULLI PROCESSES 
f(xir,n) 
4 .--~.,.-....,..-,--..,.--,.--~.,........,..-n 
3 
2 
0.2 
0.4 
0.6 
0.8 
1.0 
X 
39 
f(xir,n) 
4rr-"T"""-r--,--....--..--~.,.--,--r--. 
3 
2 
0.2 
0.4 
0.6 
0.8 
1.0 
X 
FIG. 3.1. Examples of beta probability distributions. Parameters for the various curves 
are: (a) n = 2, r = I; (b) n = 3, r = I; (c) n = 4, r = 2; (d) n = 1.5, r =I; (e) n = 2.5, r = 1.5; 
(f) n = 5, r = 3; (g) n = 10, r = 6; (h) n = I, r = 0.333. 
This suggests that r' and n', as parameter values for the prior, imply that 
the prior knowledge is equivalent, in terms of information about p, to 
having observed n' hypothetical trials of which r' were successes. 
The definition of the beta density [Eq. (3.4)] implies that the only 
requirement on r' and n' (a and b) is that n' > r' > 0. Specifically, r' and 
n' need not be integers and, in general, will not be. Of courser and n can 
only be integers (or zero). Thus the analogy of r' and n' representing hy-
pothetical data cannot be taken entirely literally. 
When there are large numbers of trials such that n ~ n' and r ~ r', 
then the specific values of the prior parameters are of negligible importance. 
The empirical evidence then outweighs the prior judgement. When n and 
rare not large, meaning that there is a lack of empirical information, the 
prior parameters are more important. In some situations when n is very 
small, the prior information is dominant and becomes the principal source 
of information on the basis of which any decisions can be made. The 
posterior distribution, and the posterior parameters, tell us how logically 
to combine prior knowledge and subsequent empirical information. 
As an illustration of the relation of prior and posterior densities, and 
the relative importance of prior knowledge and empirical data, let us ex-
amine Fig. 3.2. Two different prior densities (r'1 = l, n'1 = 5, and r2 = 4, 
n2 = 5) are combined with two different sets of data (r1 = 2, n1 = 4 and 
r 2 = l 0, n2 = 20). This leads to four sets of posterior parameters: rij = rj 
+ r1, nij = nj + n1; i, j = I, 2. The two posterior densities based on the 
larger data set (n = 20) are quite close to each other; already the difference 
in the priors is almost of no consequence. On the other hand, when only 

40 
CHAPTER 3 
f(xlr',n1 
f(xlr'.n1 
4 .--~-.-""T"""__,..,.,........--.,-,..-~-.--. 
4.--..---.--,---,-----,....,....,..--,.--..,--,---, 
3 
3 
2 
2 
oL-..L..-...... e::..-L..-L---L--L--ll:>.....~ 
0.2 
0.4 
0.6 
0.8 
1.0 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
FIG. 3.2. Beta prior and posterior probability distributions. Two alternative prior distri-
butions are shown (dashed curves): left, r' = I and n' = 5; right, r' = 4 and n' = 5. The 
posterior distributions (solid curves) are those appropriate following observations of r = 2 and 
n = 4 (the less-peaked curves) and r = 10 and n = 20. 
a few data are all that are available, the prior parameters are quite important. 
When there is little direct evidence on which to base decisions, the cli-
matologists's judgement may be critical. 
3.5 SELECTING PRIOR PARAMETERS 
The selection of prior parameters, unlike the revision of the prior 
parameters in the light of data, is not a rigorous process for which one set 
of rules apply to all situations; it is an exercise in the quantification of our 
views. Before we can even start it is necessary to recognize that the selection 
of a conjugate prior to express these views implies a smooth probability 
density that can take on only a limited number of forms (Fig. 3.1 ). If we 
have some very strongly held views about p (perhaps based on some relevant 
observations or theory, for example), that can only be represented by a 
different functional form, then we should use that different functional form 
and forget about the conjugate distribution. If for some reason we can 
accept only certain discrete values of p, or a range of values for p that is 
less than the interval [0, l ], then also we must abandon the use of the 
conjugate form. The first and overriding requirement on the establishment 
of prior parameters is that the distribution we describe is what we believe. 
If the conjugate prior does not represent our prior beliefs then the posterior 
distribution derived from it will not tell us what our reaction to the data 
truly should be. On the positive side, however, note that prior knowledge 
is usually not so rigidly or precisely formulated that a reasonable fit to it 
cannot be found among the family of conjugate distributions. 

BERNOULLI PROCESSES 
41 
The most frequent problem is not one of beliefs that are too structured 
to be adequately described by a beta density; the smooth nature of the 
density and the flexibility with which preferences in different parts of the 
[0, 1] interval can be expressed generally make the beta more than adequate 
for quantifying our views. The problem is more likely to be that the prior 
knowledge is so meager that no useful choice of parameters can be made. 
It is very rare that such a complete state of ignorance is really justifiable, 
but it is nevertheless useful to pursue it as an exercise in logic. 
It is a natural extension of the earlier conclusion that n' represents, as 
the prior parameter, the (hypothetical) number of trials upon which the 
prior information is based, to consider using n' = 0 to represent a state of 
total prior ignorance. Necessarily, of course, this means that r' = 0 (because 
r' ~ n' if one is treating them as artificial data). The artificiality of this is 
immediately evident since the beta density is only defined for n' > r' > 0. 
The use of r' = n' = 0 would give a density proportional to p- 1(1 - p)- 1, 
the integral of which does not converge over the interval 0 ~ p ~ 1. 
Nevertheless, if we ignore this deficiency and apply Eq. (3.5) using 
r' = n' = 0 as prior parameters, then the posterior parameters become 
r" = rand n" = n. The posterior density, unlike the prior, is proper (its 
integral converges) if r =I= 0 and r =I= n. In other words, if we feign "total 
ignorance" and then obtain a set of data with at least one success and one 
failure, then the resulting posterior density is a mathematically proper form; 
not only does its integral exist, but so do its moments. In particular [see 
Eqs. (A4.2) and (A4.3) in Appendix A] the posterior expected value of fl 
(the mean of the posterior density) is r/n and the variance is r(n- r)/n2(n 
+ 1 ). For no other set of prior parameters would the posterior density 
yielding this expected value have as large a variance. Maximum prior ig-
norance, in other words, maximizes posterior uncertainty. 
The prior representing "total ignorance" is often referred to as a 
"vague" prior. It is a device that allows us to be more influenced by data 
than would be the case with any other choice of prior parameters, but it 
results in the least possible confidence in the final inference. We will refer 
repeatedly in later chapters to vague priors. It is a useful option to have 
available, but the situation in which a vague prior truly expresses our prior 
state of knowledge is an anomaly. It may sometimes be used as a conser-
vative comparison, the limiting case as prior knowledge becomes weaker 
and weaker, to see what the result would have been if the prior information 
had been ignored. In general, however, where other than vague information 
is available, an inference based on a vague prior should not be expected 
to lead to optimum decisions. The exception is when n is going to be large. 
If there are many observations available they will dominate and should 
dominate the final conclusions. When n ~ n' it does not matter what n' 

42 
CHAPTER 3 
is-it might as well be zero. However, our purpose is not to be concerned 
with situations in which n is large. These are easily dealt with by classical 
methods. It is the situation where n is relatively small, so that prior infor-
mation is of real value, that interests us. In such cases it will pay for the 
climatologist to take care how he expresses his prior, making certain that 
the parameters chosen and the form of the density function do indeed 
express his understanding and knowledge. 
For purposes of discussing the selection of prior parameters, let us 
consider a specific example. We shall assume that we are planning a new 
ski resort in a previously undeveloped mountain area, and we want to 
know the probability that there will be at least 30 em of snow on the ground 
on 1 December. We know the elevation and exposure of the site, and 
something about the snow conditions at somewhat comparable locations 
in the vicinity. We also know that there are years when there is no snow 
anywhere in the region before mid-December, and that in other years deep 
snow is found everywhere. Thus we are quite certain that p is neither 0 
nor 1, and so to assume a prior proportional to p- 1( 1 - p}- 1 would not be 
appropriate. This is sufficient reason for us to choose prior parameters such 
that n' > r' > 0. The question still remains as to what specific values to 
choose. We shall consider several approaches. 
1) Fitting by moments. In this approach we attempt to quantify our 
beliefs about p by first describing, as well as we can, the probability density 
itself. Then we determine the first two moments of this density (the mean 
and the standard deviation or variance) and solve for the parameters of 
the beta distribution that would give those two moments. If we were able, 
a priori, to assert the first two moments of the function that describes our 
degree of belief, then the first step could be skipped. Usually, however, we 
will infer the moments from the distribution. 
Let us, in the example at hand, try to assess our relative degree of 
belief that p falls within each of the ten intervals of width 0.1 that comprise 
the total allowable interval [0, 1]. We might arrive at the statement of 
relative degrees of belief shown in the second column of Table 3.4. This 
is equivalent to saying that we find it four times more likel5' that plies 
between 0.2 and 0.3 than between 0.1 and 0.2, but it is equally credible to 
us that plies between 0.1 and 0.2 as between 0. 7 and 0.8. It may be more 
effective to construct the entire distribution through such comparisons, or 
it may be easier to approximate our belief by sketching a distribution and 
estimating the relative areas under the curve. It may require some consid-
erable self-interrogation to arrive at a set of relative degrees of belief that 
we are satisfied with, but this can result in a useful statement of our knowl-
edge. (If this seems like an unreasonable expectation, and the reader has 
some difficulty imagining how he or she could generate a set of relative 

BERNOULLI PROCESSES 
TABLE 3.4. Possible prior beliefs concerning probability of snow depth 
exceeding 30 em on 1 December. 
Prior relative degree 
Prior probability of 
p 
of belief 
p in interval 
0.0-{).1 
I 
0.01 
0.1-{).2 
5 
0.04 
0.2-{).3 
20 
0.17 
0.3-{).4 
35 
0.29 
0.4-{).5 
25 
0.21 
0.5-{).6 
15 
0.12 
0.6-{).7 
10 
0.08 
0.7-{).8 
5 
0.04 
0.8-{).9 
3 
0.02 
0.9-1.0 
O.DI 
Total 
120 
1.00 
43 
degrees of belief and have confidence in them, please be patient. Later 
approaches to defining the prior parameters may seem intuitively more 
acceptable. Also, by following through the logic of this and the other ap-
proaches, the rationale of each may become clearer, and those at first re-
jected may later seem more palatable.) The numbers in the third column 
of Table 3.4 are simply the equivalent prior probabilities of each interval, 
determined by dividing the numbers of column two by their sum. One 
essential characteristic of the statement of prior belief is that no interval is 
absolutely excluded. Clearly values of jj between about 0.2 and 0.6 appear 
more credible than smaller or larger values, but there are no sharp bound-
aries to the credible region. 
We can now calculate, from this prior probability distribution, the 
implied prior expected value and prior variance of jj. These are 
Eo(JJ) = L p;P{p;} = 0.42 
and 
Vo(P) = L p/P{p;} - [Eo(fl)f = 0.0297, 
where the p; were taken to be the midpoints of the intervals, and P{p;} is 
the prior probability assigned to each interval. 
We now make use ofthe fact that we wish to represent the prior by a 
member of the beta family of distributions. From Eqs. (A4.2) and (A4.3) 
of Appendix A we can write for beta distributions 
Ep(jjir', n') = r'/n', 
V p(jjir', n') = r'(n'- r')/n'2(n' + 1). 

44 
CHAPTER 3 
By equating the prior expectation and variance from Table 3.4 to those 
for the beta distribution we obtain two equations in r' and n'. Solving them 
simultaneously gives n' = 7.2 and r' = 3.0. These are the prior parameters 
to choose to represent the relative degrees of belief that have been stated. 
Figure 3.3 compares the particular beta density we have thus selected with 
the probabilities of Table 3.4. 
Note that the parameters are not integers-in general they will not 
be. The value of n' carries the implication that our prior knowledge is 
equivalent to a little more than seven observations. It requires an n' this 
large to express the view that pis probably less than one half (although a 
value near one half is not unlikely}, but a value less than 0.2 is not very 
credible. 
2) Use of fractiles. Some readers will find it easier to express their 
prior judgements by giving values of convenient fractiles of the prior dis-
tribution, rather than trying to specify the entire distribution as we have 
just done. For instance, in the present example we might choose to say 
that there is only a probability of 1/10 thatjj < 0.25 (i.e., the tenth percentile 
of the prior distribution is at p = 0.25}, and also that there is a 10% prob-
ability that p > 0.65. These two conditions, or in general any two distinct 
and consistent such conditions, are sufficient information to determine the 
parameters r' and n' of a beta prior distribution. One has to enter a table 
of the incomplete beta function and, through a process that usually requires 
some kind of double interpolation, determine the appropriate values of 
the parameters r' and n'. 
Some tables of the incomplete 13-function are given in Appendix B, 
where Table Bl gives the tenth percentile of the beta distribution for various 
f(pl 
4r--r-,--.,--r--r-...,..._,..---r....,......, 
3 
2 
0.2 
0.4 
0.6 
0.8 
1.0 
p 
FIG. 3.3. Beta distribution fit by the method of moments to the beliefs stated 
in Table 3.4 (illustrated by histogram). 

BERNOULLI PROCESSES 
45 
values of r' and (n' - r'). The table can also be used to determine the upper 
I 0% limiting values of the beta distribution by noting that there is a sym-
metry between the lower tail of fJxlr', n') and the upper tail of fJxln' -
r', n'). Thus if z is the lOth percentile of the beta distribution with parameters 
r' and n', then l - z is the 90th percentile of the distribution having pa-
rameters n'- r' and n'. To find the upper 10% (i.e., the 90%) point of the 
distribution, enter Table B l with r' and n' - r' reversed, and subtract the 
result from one. 
To make use of these tables in the present example we must search 
for the combination of parameters r' and n' such that the lower 10% limit 
of the beta is 0.25, while the 90th percentile is 0.65. From Table Bl (which 
gives the lower limits) we find, by interpolating, the values of n' - r' cor-
responding to each value of r' such that the lower limit is 0.25. These points 
are plotted and connected by a solid line in Fig. 3.4. Similarly enter Table 
B l to find the combinations of values of r' and n' - r' such that the upper 
12 
10 
8 
4 
2 
0 
0 
// 
/ 
/ v 
v 
~/ 
v 
/ 
/ 
/' 
1/ 
2 
4 
6 
I, 
I 
I 
/ v 
v 
10 
12 
14 
FIG. 3.4. Determination of appropriate parameter values for n' and r'. The solid curve 
joins pairs of values of r' and n'- r', interpolated from Table 81, for which the lower 10% 
limit of the beta distribution is 0.25. The dashed curve reflects pairs of parameter values of a 
beta distribution such that there is a I 0% probability of exceeding 0.65. Where the two curves 
intersect both conditions are met, defining the desired prior parameters n' - r' = 5.4 and 
r' = 4.3. The curves are somewhat irregular because of the approximations implicit in linear 
interpolation and roundoff errors in the table. 

46 
CHAPTER 3 
10% limit is 0.65. These too are plotted in Fig. 3.4. Where the two lines 
intersect both conditions are met and we have determined the appropriate 
values of the parameters for the beta distribution as determined by the two 
specific conditions. The result here indicates parameter values r' = 4.3, 
n' = 9.7. 
This procedure can be applied somewhat more readily if, in place of 
one of the statements of fractiles, we define either the mean or the mode 
of the prior distribution. For beta distributions, the mean is given by r'/n', 
and the mode by (r'- 1)/(n'- 2). The use of one of these algebraic con-
straints on r' and n' makes it easier to search in the tables for a pair of 
values that fits that constraint and also the other condition. 
3) Consideration of hypothetical future data. The definition of the 
prior parameters is essentially an exercise in specifying in concrete terms 
what we believe. One way of doing this is to imagine how we would react 
to the outcome of a future event. For example, let us assume, in the case 
of the occurrence of frost in Ohio, that our prior expectation for jJ (i.e., 
the expected value of our prior distribution) is E0(jJ) = 0.3. (This is, indeed, 
the mean of the discrete prior distribution of Table 3.2.) In this treatment 
of the problem, however, we will treat the parameter of the Bernoulli process 
as a continuous random variable (i.e., we will no longer restrict it to the 
discrete values shown in Table 3.2). 
Now imagine the situation in which there is a single trial which happens 
to result in a "success" (the occurrence of frost between 15 May and 15 
September at the particular location). What, then, would our new expec-
tation be? 
It must be at least as large as the prior expectation (each "success" 
would support the notion of a larger jJ; each "failure" must lead us to 
believe a somewhat lower value). But how much larger? If we are able to 
give expression to our judgement as to how much we would be influenced 
by some initial data, then we can use that as a statement of our belief and 
translate it into prior parameters. 
We know that if the prior parameters are r' and n' (assuming the prior 
is a beta distribution), and if n = r = 1, then the posterior parameters are 
r" = r' + 1 and n" = n' + 1. The prior expectation is r'/n' and the posterior 
expectation is r"jn" and (r' + 1)/(n' + 1). If we decide that the occurrence 
of that one success would induce us to increase our judgement about the 
expected value of jJ from 0.3 to 0.5 (a rather large change), then we have 
the two conditions 
r'/n' = 0.3, 
(r' + 1)/(n' + 1) = 0.5. 
Solving these two equations simultaneously gives r' = 0.75 and n' = 2.5. 

BERNOULLI PROCESSES 
47 
These, then, are the appropriate prior parameters that represent our stated 
beliefs that E0(jj) = 0.3 and E 1(ftlr = 1, n = 1) = 0.5, using the conjugate 
distribution for the Bernoulli process. It is not surprising that n' is quite 
small, since we were willing to substantially modify our beliefs in the light 
of only one observation. 
The consideration of hypothetical future data does not have to be 
constrained to a single observation. For example, let us say instead that 
we expressed the view that three successive May-September seasons with 
frost would barely induce us to accept an expected value for jj as large as 
1/2. This leads to the algebraic constraint that (r' + 3)/(n' + 3) = 0.5, and 
the conclusion that n' = 7.5 and r' = 2.25. Clearly the second statement 
expresses much more confidence in the prior judgement that the value of 
jj is small than does the first statement, and this is reflected in the values 
of n'. The first statement implied that the prior judgement was equivalent 
to having observed, in the mind's eye, 2.5 trials; the second implies a weight 
equivalent to having observed 7.5 trials. 
Consistency demands that these hypothetical exercises be hypothetical 
indeed, and be carried out before the data are actually available or known. 
In principle the expert can imagine what he or she believed before seeing 
the data, but in practice it would be next to impossible to be certain of 
avoiding biases. The logic of Bayes' Theorem demands that prior knowledge 
be strictly prior. To confuse the codification of the prior belief by the 
knowledge of observations that are to be treated as posterior is wrong. If 
observations are known, include those observations as part of the infor-
mation being codified in the statement of belief about the parameter(s) in 
question. It seems reasonable to suppose that, using the conjugate prior in 
the present case, if m observations have been made then our current belief 
(prior to additional observations) would be expressed by n' > m. To choose 
n' < m would imply the belief that there is something wrong with those 
observations. 
3.6 
PREDICfiONS OF FUTURE RESULTS 
In most cases the real reason for making inferences abqut the data-
generating process (here the Bernoulli process) is not to estimate the process 
parameters, but rather to make predictions of future outcomes. When we 
know, or think we know with certainty the values of p for a Bernoulli 
process, then we can state the probabilities of future statistics in terms of 
binomial, Pascal, or negative binomial distributions, depending on which 
statistics are of interest. Now we consider what we can say about the future 
when our knowledge of the process parameter is incomplete (i.e., we do 
not know its value with certainty), but is fully described by a beta distri-

48 
CHAPTER 3 
bution Jflftlr 11, n 11 ). [It does not matter, really, if the distribution for pis 
prior or posterior; the two parameters r 11 and n 11 (orr' and n') contain all 
the information about p that is available.] 
Let us start by asking specifically the probability of r successes in n 
future trials. We will write this, for now, as P{rin, r 11, n 11 }. To evaluate this 
quantity we will make use of the fact that P{rln, p} = .fb(rln, p) and that 
we have chosen to represent our knowledge of pas a beta distribution. We 
use Eq. (2.8) to write 
P { rln, T11, nil} = f Jb(rln, p)f{3(plr 11, n 11 )dp 
= 
n! 
r(n
11
) 
t pr+r"-l(l _ Pt+n"-r-r"-ldp 
r!(n - r)! f(r 11)f(n 11 -
r 11 ) Jo 
n! 
f(n 11 ) 
f(r + r 11 )f(n + n 11 -
r -
T11 ) 
r!(n - r)! f(r 11)f(n 11 -
r 11 ) 
f(n + n 11 ) 
(3.6) 
This rather cumbersome expression defines the "beta-binomial" probability 
density, which we will denote as./ilb(rln, r 11, n 11 ). Some tabulated values of 
the beta-binomial distribution are given in Table B2 (Appendix B). The 
moments of the distribution [see Eqs. (A5.2) and (A5.3) in Appendix A] 
are worth noting. The expected number of successes in n future trials of a 
Bernoulli process, when our knowledge about p is codified as a beta dis-
tribution with parameters T11 and n 11, is Eflb(rln, T11, n 11 ) = nr11/n 11• This is 
also equal to np, where p = T11/n 11 is the expected value of p, E{l(ftlr 11, n 11 ). 
The variance of the predicted number of successes is 
V(~l 
II 
11 )-
-
- -
[ 
T11 n 11 -
T11] n + n 11 
r n, r , n 
-
n 
11 
11 
11 
1 . 
n 
n 
n + 
The factor within the brackets could also be written np( 1 - p). If we had 
known with certainty that this were the true value of p, then the variance 
of the prediction based on the binomial distribution would have been 
np(l - p). The factor (n + n 11 )/(n 11 + 1) is the increase in the variance of 
the prediction because of uncertainty in p. The larger n is in comparison 
to n 11 the greater is the effect on the variance of the prediction of the un-
certainty in ft. Conversely, when n11 is large in comparison to n, then the 
process parameter is as good as known (and equal to r 11jn 11 ) insofar as 
prediction of n future events is concerned. 
In the very same way that we obtained Eq. (3.5), we can derive the 
predictive distribution for the number of trials that will be required to 
achieve r successes. We again use Eq. (2.8), and write 

BERNOULLI PROCESSES 
49 
./PPa(nlr, r", n") 
_ 
(n -
1)! 
I'(n") 
{1 ..r+r'-1(1 
)"+n•-r-r'-ld 
- (r- 1)!(n - r)! I'(r")I'(n"- r") Jo P 
-
P 
'P 
(n- 1)! 
I'(n") 
I'(r + r")I'(n + n"- r- r") 
(r - l)!(n - r)! l'(r")I'(n" - r") 
I'(n + n") 
(3.7) 
This is the "beta-Pascal" density. Some tabulations of beta-Pascal proba-
bilities appear in Table B3 of Appendix B. It is also instructive here to 
compare the mean and variance of the beta-Pascal [Eqs. (A6.2) and (A6.3) 
of Appendix A] with the mean and variance of a prediction that would be 
made if ftwere known, and given by r"/n". Note especially that VPa(rilr, r", 
n") does not exist if r" ~ 2. For such small values of r" the probabilities 
on large values ofn do not decrease fast enough for the sum 'Ln%Pa(n) to 
converge. 
(We could also derive a "beta-negative binomial" distribution. Its 
properties follow directly from those of the beta-Pascal. It would represent 
the probabilities of obtaining :i future failures before the rth future success, 
when knowledge of the process parameter was codified as a beta density 
with parameters r" and n".) 
3.7 AN EXAMPLE 
Referring back to the ski resort example of Section 3.5, let us first 
accept the prior parameters determined by fitting moments of the beta 
density to our prior probabilities. Thus we haver'= 3.0 and n' = 7.2. We 
ask first what are our probabilities, before obtaining any observations, of 
0, 1, 2 or 3 occurrences of suitable snow depths in the next three years. 
From (3.6) we know that the required probabilities are given by the beta-
binomial distribution and are 
fpb(rln = 3, r" = r' = 3.0, n" = n' = 7.2) 
3! 
1'(7.2) 
I'(r + 3.0)1'(7.2- r) 
r!(3 - r)! 1'(3.0)1'(4.2) 
1'(10.2) 
It is possible to calculate these probabilities without reference to a 
table of gamma functions. We need only to apply the recursion relation 
that I'(a + 1) = al'(a). This allows us to write 
./Pb(r + 1) = 
r!(3 - r)! 
I'(r + 4.0)1'(6.2 - r) 
./Pb(r) 
(r + 1)!(2 - r)! l'(r + 3.0)1'(7.2 - r) 
3- rr + 3.0 
= r + 1 6.2- r · 

50 
CHAPTER 3 
Therefore, 
jpb(l) = 1.452/pb(O), 
jpb(2) = 0.769/pb(l) = 1.117/pb{O), 
/pb{3) = 0.397/pb{2) = 0.443/pb(O). 
Since the sum of the probabilities over all possible values of r (i.e., from 
r = 0 to r = 3) must equal 1.0, it is a simple matter to solve for /pb{O), 
obtaining 0.26. Thus 
/pb{O) = 0.26,/pb{1) = 0.37,/pb(2) = 0.29,/pb{3) = 0.08. 
These are our probabilities, under the conditions given, based on our 
prior beliefs about p. If, on the other hand, the parameter p were known 
with certainty to be 0.42 (its expected value), the relevant probabilities of 
0, 1, 2 or 3 successes in three trials would have been, from the binomial 
distribution, 0.20, 0.42, 0.31 and 0.07, respectively. The major impact of 
the uncertainty in pis to increase the probability of no occurrences of snow 
cover in the next three years, recognizing that p might actually be less than 
its expected value of0.42. It might also be larger, but the (prior) probability 
of this is apparently not sufficient to significantly increase the probability 
of three events. 
Now let us imagine that three years pass, and in none of them is the 
criterion of 30 em of snow on the ground on 1 December met. In other 
words, we accumulate the statistics r = 0, n = 3. The parameters of our 
posterior distribution of pare now r" = r' + r = 3.0 and n" = n' + n 
= 1 0.2. How many years do we now expect to have to wait to have the 
prescribed snow cover on 1 December? The answer, of course, can be 
obtained from the beta-Pascal distribution, fPPa(fl11, 3.0, 10.2). We can 
answer the question about the expected waiting period directly from the 
relationship for the expected value of a random variable with a beta-Pascal 
distribution. As given in Eq. (A6.2), 
EPJ>a(nir, r", n") = r(n"- 1)/(r"- 1) = 1 X 9.2/2.0 = 4.6 years. 
Note that prior to having made the observations of three years without 
adequate snow, the expected waiting time until the first successful event 
was 6.2/2.0 = 3.1 years. The lack of observed snow cover in the first three 
years should not have come as a great surprise, but it does strongly influence 
additional predictions. 
Table 3.5 gives (in the columns labeled as beta-Pascal probabilities) 
the probabilities that the first event is observed in n years both prior and 
posterior to the observation. The columns labeled as Pascal probabilities 
give the probabilities of the first event occurring in n years if the process 

BERNOULLI PROCESSES 
51 
TABLE 3.5. Comparison of beta-Pascal and Pascal probabilities. 
n 
!~<nil, 3, 7, 2) 
/Pa(nil, 3/7.2) 
!~<nil, 3, 10.2) 
!Pa(nil, 3/10.2) 
I 
0.372 
0.417 
0.273 
0.294 
2 
0.211 
0.243 
0.184 
0.208 
3 
0.128 
0.142 
0.128 
0.147 
4 
0.082 
0.083 
0.092 
0.103 
5 
0.055 
0.048 
0.068 
0.073 
6 
0.039 
0.028 
0.051 
0.052 
7 
0.028 
0.016 
0.039 
0.036 
8 
0.020 
0.010 
0.031 
0.026 
9 
O.oJ5 
0.006 
0.024 
O.oJ8 
10 
0.012 
0.003 
0.019 
0.013 
II 
0.009 
0.002 
0.016 
0.009 
12 
0.007 
0.001 
0.013 
0.006 
E(n} 
3.1 
2.4 
4.6 
3.4 
S.D.(n") 
5.37 
1.83 
7.97 
2.86 
parameter ft were known with certainty to be given by r'/n' and r"/n", 
respectively. Also shown are the means and standard deviations for each 
of the four distributions. Note the very large values for the standard de-
viations of the beta-Pascal distributions compared to those for the Pascal 
distributions. A penalty must be paid for uncertainty. Uncertainty in ft 
makes it very difficult to assert that there will not be a long waiting period 
before the occurrence of a "success." 

Chapter 
4 
Poisson Processes 
4.1 
DEFINITION 
The second data-generating process with which we shall deal is the 
Poisson process, which, like the Bernoulli process, is quite frequently very 
representative of real processes of climatological interest. A Poisson process 
is characterized by a series of discrete events whose occurrence can be 
related to an ordered sequence of some continuous independent variable. 
Examples of events and their independent variables are: 
homicides in Detroit versus time 
tornadoes during a season versus area 
service stations along a highway versus distance 
freezing nuclei occurrences versus volume 
tropical depressions versus time 
Not all of these represent Poisson processes. The additional necessary con-
dition for a Poisson process is that the probability ofthe occurrence of any 
particular number of events within an interval depends only on the size 
of the interval. It should not depend on where the interval is located or 
where it begins or ends, or whether events have occurred in adjacent in-
tervals. 
53 

54 
CHAPTER 4 
We might expect that the number of service stations along some stretch 
of highway would depend on whether that part of the road contained any 
intersections. Consequently we would not expect this to be a Poisson pro-
cess. Also, considering the United States as a whole, it is clear that the 
probability of tornadoes in areas of equal size would be greater in Kansas 
than in Idaho. Since this represents dependence on location, it also should 
not to be modeled as a Poisson process. On the other hand, if we restrict 
our attention to a limited region-say the southern half of Illinois-then 
independence from location and dependence only on the size of the area 
become much more reasonable. The Poisson process often becomes a very 
reasonable model for a stochastic process if consideration is restricted to 
some suitable interval of the independent variable. 
We can define large adjacent regions of the atmosphere over which it 
is reasonable to represent the occurrences of freezing nuclei by a Poisson 
process. The larger the volume of air examined, the more nuclei that are 
likely to be found, independent of where, within the entire larger region, 
the sample is taken. The outstanding exception to this statement relates 
to volumes of air which are so small that they are comparable to the volumes 
occupied by individual nuclei. Since two nuclei cannot occupy precisely 
the same location, the probability that a nucleus occupies a very small 
volume will depend upon where other nuclei are located. This is a very 
trivial point since we would not normally even consider such small volumes. 
For example, we might never even examine volumes smaller than l cm3, 
in which case the limiting case of two nuclei trying to occupy the same 
space would not come up at all. 
In view of similar considerations, it may be necessary to place a lower 
boundary on the size of single elements of the independent variable in 
other cases in order to use the Poisson model. For example, a time unit 
smaller than one day would not be reasonable for events like episodes of 
calm winds and high stability near the ground, which have large diurnal 
variations in probability of occurrence. It is often quite simple to limit the 
size of the interval we are willing to consider without restricting the ap-
plicability of the model to the problem at hand, while also retaining the 
appropriateness of the model. 
4.2 
DISTRIBUTIONS OF SUFFICIENT STATISTICS 
A single parameter A is used to characterize a Poisson process. It is 
known as the intensity of the process and is the average number of events 
per unit interval of the independent variable. Thus A may be the average 
number of nuclei per unit volume of air, or the average number of tornadoes 
per unit area that occur in a season. As with the Bernoulli process parameter, 
knowledge of its value allows us to make probability statements predicting 

POISSON PROCESSES 
55 
the outcomes of future events: how likely is it that a 10 cm3 sample will 
contain fewer than six nuclei, or what is the probability that there will be 
no tornadoes next season in a specific 100 km3 area? Also, as with the 
parameter of the Bernoulli process, we will generally have only partial 
knowledge of A. 
The numbers that describe the outcome of a Poisson process are the 
size of the interval being sampled or examined, T, and the number of 
events that occur, r. As we will see below, r and Tare sufficient statistics 
for A. Given A, we require that the probability of any particular value of r 
in an interval T be only a function of T. In the next several paragraphs we 
will derive expressions for the probability of r given T (A known), and of 
f given r (A known). 
Since A is the intensity of the process, if we consider an infinitesimally 
small interval dT then the probability of exactly one occurrence is AdT. 
The probability of more than one occurrence is so small an interval is zero 
[or more precisely of order (dT}2, which is negligible in comparison to 
AdT]. Furthermore, according to our definition of the Poisson process, the 
occurrence or nonoccurrence of an event in any small interval has no effect 
on the probability of occurrence of an event in any other (nonoverlapping) 
small interval. 
Any large interval of length T may be thought of as being made up 
of very many (T/dT) small intervals oflength dT. The occurrence or non-
occurrence of the event in each small interval constitutes a Bernoulli process 
with process parameter AdT. The small intervals can contain at most one 
event. The total number of events in the large interval is just the sum of 
the number of occasions, r, out of a total number of trials, n = T/dT, that 
a "success" occurs (using the language of the Bernoulli process), when the 
probability of success on each trial is AdT = AT/n. From the binomial 
distribution we get, for the probability of r occurrences in n = T/ dT trials 
with a probability of occurrence in each trial of AdT = AT/n, 
_ 
n! 
[AT]'[ 
AT]n-r 
Ji,(rin; AT/n) = I( _ )' -
1 - -
. 
r. n 
r. 
n 
n 
Taking the limit of this expression as n ____, oo, the size of each interval 
shrinks to zero, and we obtain the probability density for the number of 
events r in a Poisson process on a continuous interval T, with parameter 
A: 
hm 
-
1--
. 
n! 
[AT]'[ 
AT]n-r 
n-oo r!(n - r)! 
n 
n 
=(AT)' lim n(n- 1) · · · r[n- (r- 1)] [ 1 _ ATJ
11[ 1 _AT]-'. (4_1) 
r! 
n-oo 
n 
n 
n 

56 
CHAPTER 4 
But 
. 
n(n-1)···[n-(r-1)] 
hm 
r 
n~oo 
n 
= !~~ [ 1 • { 1 - ~) ..• { 1 - r ~ 1) J 
= 1 
and 
( A8_, 
lim 
1 --
= 1. 
n~oo 
n 
Therefore, ( 4.1) becomes 
hm 
-
l --
= -- hm 1 --
. 
n! 
[AT]'[ 
AT]n-r (AT)' . [ 
AT]n 
n~oo r!(n - r)! 
n 
n 
r! 
n~oo 
n 
{ 
r = 0, 1, 2, 
A>O 
T>O 
(4.2) 
This defines the well-known Poisson family of probability densities. Note 
that the random variable r, the number of events, must be a positive integer 
or zero, while the intensity A and the interval size T may be any positive 
real numbers. 
Figure 4.1 presents several graphs of Poisson distributions. Since A 
and T appear in ( 4.2) only in the form of their product, it is not necessary 
to consider separately the values of A and T, but only their product AT 
which is the mean of the distribution of r, i.e., £p0(riA, T) = AT. In other 
words the Poisson distributions are a one-parameter family of distributions. 
We will nevertheless continue to treat A and T separately; one is a statistic 
and the other a parameter. 
Since the product AT is the only quantity on which Poisson proba-
bilities depend, all the moments of the distribution can be given in terms 
of it. In particular, the variance Vp0(riA, T) =AT is equal to the mean (see 
Appendix A, Section A 7). 
There are times when we are concerned not with the number of events 
that will occur in a given interval, but with the size of the interval T required 
for exactly r events to occur. For example, we may wish to make a study 
of severe storms. If it were necessary to observe say five storms meeting 
some specific criterion of intensity in order to obtain enough data to verify 
a particular hypothesis, then it would be important to be able to estimate 
the probability of having to wait particular lengths of time. This could be 
done if we were willing to accept the model that storm occurrences in the 
particular region and season were Poisson with some parameter A. 

POISSON PROCESSES 
57 
f ( r) 
f ( r l 
0.6 
0.6 
0.5 
MEAN = .5 
0.5 
MEAN = 
0.4 
0.4 
0.3 
0.3 
0.2 
0.2 
0. I 
0. I 
0.0 
0.0 
0 
5 
I 0 
15 
20 
0 
5 
10 
15 
20 
r 
r 
f( r l 
f ( r l 
0.6 
0.6 
0.5 
MEAN = 3 
0.5 
MEAN - 10 
0.4 
0.4 
0.3 
0.3 
0.2 
0.2 
0. I 
0.1 
0.0 
0.0 
0 
5 
10 
15 
20 
0 
5 
10 
15 
20 
r 
r 
FIG. 4.1. Examples of Poisson distributions. 
The probability that the rth event will occur between the times T and 
T + dT is simply the product of the probabilities of 1) exactly r - 1 events 
occurring in the interval T, and 2) an event occurring in the interval of 
length dT immediately following. From Eq. (4.2) the probability of the 
former is 
We noted earlier that the latter probability is simply A.dT. Since the two 
occurrences are independent the joint probability is just the product of the 
probabilities of the separate events. In other words, the probability of the 
rth event occurring in the interval between T and T + dT is 
{
T>O 
'A>O 
. 
r = 1 2 • · · 
' ' 

58 
CHAPTER 4 
This defines the "gamma" probability density 
~ 
·)I..'T'-le->-T 
fr(Tir, A) = 
r(r) 
T, r, A > 0. 
(4.3) 
This particular form, with r(r) written in place of(r- 1)!, is entirely equiv-
alent to the former so long as r is an integer. From the perspective of its 
derivation above, only integer values make sense. Nevertheless, the gamma 
density is proper, mathematically speaking, for all positive values of r (i.e., 
the moments of the distribution exist1). We will find it useful to consider 
noninteger values of r as a parameter in later applications. 
The gamma group of distributions is a two-parameter family. If f has 
a gamma distribution with parameters r and A, then the expected value of 
Tis E.y(Tir, A)= r/A, and the variance is V'Y(Tir, A)= r/A2• (See Appendix 
A, Section A8.) Figure 4.2 illustrates gamma distributions for several com-
binations of parameters. 
The Poisson and gamma distributions are two distinct families: in the 
former the random variable is discrete and in the latter is continuous; the 
former is dependent on only one parameter while the latter is a two-pa-
rameter family. For inference about the parameter A, however, they are 
equivalent. The likelihood of A, given rand T, is proportional to A' e->-T in 
either case. From either perspective r and T are sufficient statistics for 
inference about A. Note also that it does not matter if the observations are 
stopped because r events have occurred, or because an interval of size T 
has been examined: the stopping rule is noninformative. 
4.3 CONJUGATE DISTRIBUTIONS 
The process parameter that is of interest to us once we have determined 
that we are dealing with a Poisson process is the intensity A. If we know 
the value of A then we know everything we can about the process. Since 
the Poisson process is stochastic it is not possible to make categorical state-
ments about future outcomes, and predictions must still be couched in 
terms of probabilities. Moreover, in general, our knowledge of A will be 
incomplete and we will want to use whatever data are available to sharpen 
the statements we are able to make about A. We will then use this best a 
posteriori knowledge of A in making predictions. 
As with the Bernoulli process, we will in general want to treat A as a 
continuous random variable that can take on any positive real value. We 
1 A proper distribution is one for which the integral or sum of the density converges over 
all permissible values of the random variable. In the case of the gamma distribution with 
r > 0, not only does this integral converge, but so do the integrals defining the moments of 
the distribution. 

POISSON PROCESSES 
59 
4 
2 
4 
6 
1.0 
.8 
.6 
.-< 
.... 
.4 
.2 
f) 
I 0 
12 
14 
16 
18 
20 
A 
1.0 
.8 
.6 
.-< 
.... 
.4 
.2 
2 
4 
6 
8 
10 
A 
FIG. 4.2. Examples of gamma distributions for parameter values of: (a) T = 0.25, 
r = 0.5; (b) T = I, r = 2; (c) T = 4, r = 8; (d) T = 4, r = 4; (e) T = I, r = 4; (f) T = 0.5, r = 
4; (g) T = 4, r = I; (h) T = I, r = I; (i) T = 0.5, r = I. 
can construct examples in which .\ is a discrete variable: these are clearly 
contrived but for purposes of illustration let us nevertheless contrive one 
and use it as an example. 
As part of the design of a weather modification experiment we want 
to consider the number of storms that produce cloud tops in the temperature 

60 
CHAPTER 4 
range -20 to -25 oc over the Cascade Mountains in Oregon during winter. 
We shall assume that during the period from December through March 
the occurrences of such storms are suitably modeled as a Poisson process. 
Because of the condition on the cloud-top temperature, the data on such 
storms are very limited. We wish to choose among three discrete values of 
~: two storms per month, one storm per month, and one-half storm per 
month. (A separate experimental design has been prepared for each-which 
one should we choose?) 
For this exercise we will not attempt to consider the origin of the prior 
probabilities assigned to each of the alternative values of ~. Let us simply 
assert that the values chosen are P0 {~ = 2} = 0.5, P0{~ = l} = 0.3, and 
Po{~= 0.5} = 0.2. 
We now find records for the last two years (eight months of relevant 
information) which indicate that there were six such winter storms. The 
application of Bayes' Theorem, with the likelihood computed from 
./P0(r = 6IA, 8) (Eq. 4.2) is given in Table 4.1 where the posterior probability 
that each of the three hypotheses is correct is given in the last column. 
Note how strongly these few data argue against the hypothesis that A = 2, 
which was originally the most credible of the three alternatives. 
Let us use our judgements about A to calculate the probability of some 
future event, say the probability of no more than three storms in the next 
year (four months). This requires that we calculate 
Fp0(3l2, 4) X P,(2) + Fp0(3ll, 4) X P,(l) + FPo(31!, 4) X P,(0.5) 
= 0.043 X 0.022 + 0.433 X 0.622 + 0.857 X 0.356 = 0.576. 
A similar calculation, using the prior probabilities on the three alternative 
values of A, would have given 0.323. Thus the evidence of observing six 
events in eight months would, in this case, cause us to increase the predictive 
probability of zero, one, two or three events in any single year from 0.323 
to 0.576. 
TABLE 4.1. Application of Bayes' Theorem to a Poisson process considering 
only discrete values of X. 
Parameter 
Prior 
Posterior 
value 
probability 
Likelihood 
probability 
>. 
Po{ A} 
ex: /Po(61>., 8) 
Po{ A} /...,(61>., 8) 
P.{AI6, 8} 
2 
0.5 
0.0026 
0.0013 
0.022 
0.3 
0.1221 
0.0366 
0.622 
0.2 
0.1042 
0.0209 
0.356 

POISSON PROCESSES 
61 
It is much more general not to restrict ~ to a small set of discrete 
values, but instead to treat it as a continuous random variable. We wish 
now to find a functional form for expressing our judgement about A.. This 
should have several attributes: in particular, it should be a flexible form 
that can be adjusted to fit any reasonably held beliefs about A.. We would 
also like it to be conjugate in the sense that if the prior probability statement 
about A. has that form, then so will the posterior distribution. We have 
already noted that the likelihood is proportional to A.' e->-T. 
If we select as a prior density for ~ a gamma density with parameters 
r' and T', i.e., 
fo(~lr', T') = f,(~lr', T') 
T"'A.r'-l 
= 
r(r') exp(-A.T'), 
(4.4) 
then the posterior density is proportional to 
A.' exp(-A.T) · A.''-t exp(-A.T') = A.'+r'-t exp[-A(T + T')], 
which also has the form of a gamma distribution, although with somewhat 
different parameters. Expressed mathematically, 
ft(~lr', T', r, T) = f,(~lr + r', T + T') 
is the posterior density of~ if the prior is given by ( 4.4 ). In other words, 
if the prior density on ~ is a gamma distribution with parameters r' and 
T', and the statistics rand Tare subsequently observed, then the posterior 
density of ~ is also gamma, but with parameters r + r' and T + T'. The 
gamma density is the conjugate prior for A., the intensity of a Poisson 
process. By reference to Fig. 4.2 we can note the range of shapes and forms 
that a gamma distribution can take. We will examine further the degree 
to which the gamma distribution has the second attribute required for a 
useful conjugate family, i.e., whether it has the flexibility to represent a 
wide variety of expressions of belief, when we look at some specific examples 
in a later section. 
Following the notation of the previous chapter we will write r" = r' 
+ r and T" = T' + T as the parameters of the posterior density. The 
additive nature of the prior parameters with the sample statistics r and T 
suggests how we ought to interpret the prior parameters. If we assign r' and 
T' as prior parameters we are stating that our prior belief is equivalent to 
having observed r' events in a hypothetical interval T'. Note that while r 
(the statistic) is necessarily an integer, r' (the prior parameter) need only 
be real and positive. A prior in which r' is not an integer is entirely ac-
ceptable, and may be the most appropriate way of expressing our true 
belief; however, its interpretation in terms of events becomes awkward. 

62 
CHAPTER 4 
4.4 SELECTION OF PRIOR PARAMETERS 
We shall now consider how we might proceed to assign values to the 
prior parameters of a gamma distribution that conform most faithfully to 
our beliefs. In what follows we shall assume that a member of the gamma 
family of densities can be found that adequately represents our beliefs. 
Later we will examine some examples to determine the validity of this 
assumption. 
In selecting values for the prior parameters r' and T', it is useful to 
remember certain characteristics of the gamma distribution. First we note 
(Section A8 of Appendix A) that if A. has a gamma distribution then its 
expectation is 
and the variance is 
E-y(Xir', T') = r'/T', 
V-y(Xir', T') = r'/T'2. 
(4.5) 
(4.6) 
It is also useful to note that the mode of the distribution occurs at A. = (r' 
-
1)/T' if r' > 1; the mode is always less than the mean. If 0 < r' ~ 1 the 
density has its greatest value at A. = 0. There is no finite upper bound on 
X, although values very much larger than the mean, r'/T', are considered 
very improbable. 
We have already seen that r' and T', as prior parameters, are equivalent 
to hypothetical data (r' events imagined to occur in an interval of length 
T'). It is natural, therefore, to consider that no prior knowledge is repre-
sented by r' = T' = 0. This is, in reality, a most unlikely situation. It implies 
that we are unwilling or unable to comment on or distinguish among values 
of the process parameter as diverse as one event per thousand unit intervals, 
or a thousand events per unit interval. Nevertheless it is conceptually useful 
if it is known that plentiful data will be available. Such a "null" prior 
implies a density _h(XIO, 0) oc 1/A.. This is not a proper probability density 
since J0oo A. - 1dA. does not converge. Such a null prior plays a useful role in 
representing vague information when we anticipate that prior information 
is going to be overwhelmed by later data, but it cannot be interpreted 
literally as a probability density. If only a single event is subsequently ob-
served, however, the formal application of r' = T' = 0 as prior parameters 
will yield a proper posterior density. 
This is a good point at which to examine the gamma distribution 
more closely and try to understand the role of the two parameters in de-
termining the characteristics of the distribution. By examining the func-
tional form of the distribution (Eq. 4.4) and the plots for various combi-
nations of parameter values (Fig. 4.2), it should become apparent that r' 
plays the role of a shape parameter, while T' is a scale parameter. If 
r' ~ l then the maximum probability density occurs at A. = 0. As r' decreases 

POISSON PROCESSES 
63 
from 1 toward 0, the peak density at A= 0 becomes sharper, but all positive 
values of A remain possible. The extent to which probability densities de-
crease with increasing A depends on the scale factor T'. 
For r' > 1 the distribution has a relative maximum, the location of 
which depends on the quotient (r' - 1 )/T'. The skewness of the distribution 
depends on how much r' exceeds unity. For example, in Fig. 4.2 com-
pare the gamma densities for r' = 4, T' = 1 (curve e) and r' = 8, T' = 4 
(curve c). 
The major restrictions on the gamma family of distributions are that 
they are unimodal and positively skewed. Reasonable beliefs that we might 
hold concerning the parameter of a Poisson process can usually be ade-
quately represented by such a distribution, but only through experience 
are we likely to be convinced of this. 
We may use any of the methods described in the Chapter 3 for choosing 
prior parameters. The particular method that is to be used should be gov-
erned by how the meteorologist of climatologist is best able to express 
judgements about the parameters in question. Among the statements one 
might make are: 
My expectation for A is ... (=r'/T'). 
The standard deviation of my prior density for A is . . . ( = W /T' ). 
The 50th percentile (median) of my prior density for A is .... 
The first and/or third quartile(s) of my prior density for A are (is) 
The most probable value (mode) of A is ... [ =(r' - 1)/T']. 
Since there are only two parameters to determine, any two statements are 
sufficient to give values of r' and T'. If three statements are made, the result 
will often reflect some internal inconsistency among the statements. It 
would then be necessary to reconsider the statements, deciding how to 
resolve the differences. 
Let is say, with reference to the occurrence of winter storms in Oregon 
(see the previous section), that we originally made the following statements: 
Eo(~)= 1.0, 
S.D.0(~) = 0.3 [or V0(~) = 0.09], 
Mode0(~) = 0.8. 
(A) 
(B) 
(C) 
Statements (A) and (B) taken together (i.e., solved simultaneously) imply 
r' = T' = 11.1. Statements (A) and (C) imply r' = T' = 5.0. Statements (B) 
and (C) jointly imply r' = 9.0 and T' = 10.0. These three prior densities 
are shown in Fig. 4.3. The differences are subtle and may or may not be 

64 
1.4 
1.2 
I .D 
.8 
.< 
4--
.6 
.4 
. 2 
.0 
r' = 9. 
T'=IO. 
0 
r'=5. 
T'=5 . 
2 
CHAPTER 4 
3 
FIG. 4.3. Candidate depictions of the prior distribution on X, corresponding to the 
three possible pairings of the three slightly inconsistent statements of belief. 
of consequence. If they do turn out to be of consequence (i.e., there is not 
much data to throw new light on the value of X, and the difference among 
the resulting predictions-to be discussed in a later section-turns out to 
be operationally significant) then it is particularly important that the cli-
matologist or meteorologist takes care in formulating his or her judgement. 
In the present example, for instance, the climatologist, after examining 
the implied probabilities in various intervals of 5:., might choose r' = 8, 
T' = 8. This would imply that (A) is indeed a valid statement of his or her 
belief, but that values of 0.354 for the standard deviation and 0.875 for 
the mode can be accepted in lieu of the first judgements. They form a 
consistent set that suitably portrays an acceptable prior distribution. The 
answer to the question as to whether this is the best set of parameters for 
the prior, however, depends on whether it faithfully portrays the expert's 
beliefs. 

POISSON PROCESSES 
65 
We might have chosen one of the other means of expressing our beliefs. 
Let us suppose that we stated that our prior judgement was that there was 
one chance in four that A < 0.6 and a like probability that A > 1.6. In 
other words the (central) 50% credible interval for ~ is (0.6, 1.6). To use 
this information we make use of the close relationship of the gamma dis-
tribution and the chi-squared (x2) distribution for which convenient tables 
are readily available. Specifically [see Appendix A, Eq. (A9.4)], if~ is gamma 
with parameters r' and T', then the random variable i = 2~T' is chi-
squared with parameter ("degrees of freedom") 2r'. 
To make use of this relationship, we first note that since A and Z, the 
two random variables, are proportional to one another, the ratio of the 
upper and lower quartiles of the two distributions must be the same. More 
specifically, the probability of A < 0.6 being 0.25 means that 1.2T' 
= x~.o.2s, where x~.o.2s is the lower quartile of a chi-square variable having 
n degrees of freedom. Similarly, 3.2T' = x~.o.75 because the probability that 
A< 1.6 is 0.75. This means that the ratio of the upper to lower quartiles 
of the x2 variable must be 1.6/0.6 = 2.67. 
Table 4.2 is a small portion of a table of the cumulative x2 distribution, 
with some added columns to indicate the ratios of selected percentile limits. 
TABLE 4.2. Cumulative chi-squared distribution and ratios of percentiles. 
P,,(Z) 
n 
0.100 
0.250 
0.500 
0.750 
0.900 
Zo.7S/Zo.2S 
Zo .• !Zo .• 
1 
O.Dl58 
0.102 
0.455 
1.32 
2.71 
12.9 
172. 
2 
0.211 
0.575 
1.39 
2.77 
4.61 
4.82 
21.8 
3 
0.584 
1.21 
2.37 
4.11 
6.25 
3.40 
10.7 
4 
1.06 
1.92 
3.36 
5.39 
7.78 
2.81 
7.34 
5 
1.61 
2.67 
4.35 
6.63 
9.24 
2.48 
5.74 
6 
2.20 
3.45 
5.35 
7.84 
10.6 
2.27 
4.82 
7 
2.83 
4.25 
6.35 
9.04 
12.0 
2.13 
4.24 
8 
3.49 
5.07 
7.34 
10.2 
13.4 
2.01 
3.84 
9 
4.17 
5.90 
8.34 
11.4 
14.7 
1.93 
3.53 
10 
4.87 
6.74 
9.34 
12.5 
16.0 
1.85 
3.29 
11 
5.58 
7.58 
10.3 
13.7 
17.3 
1.81 
3.10 
12 
6.30 
8.44 
11.3 
14.8 
18.5 
1.75 
2.94 
13 
7.04 
9.30 
12.3 
16.0 
19.8 
1.72 
2.81 
14 
7.79 
10.2 
13.3 
17.1 
21.1 
1.68 
2.71 
15 
8.55 
11.0 
14.3 
18.2 
22.3 
1.65 
2.61 
16 
9.31 
11.9 
15.3 
19.4 
23.5 
1.63 
2.52 
17 
10.1 
12.8 
16.3 
20.5 
24.8 
1.60 
2.46 
18 
10.9 
13.7 
17.3 
21.6 
26.0 
1.58 
2.39 
19 
11.7 
14.6 
18.3 
22.7 
27.2 
1.55 
2.32 
20 
12.4 
15.5 
19.3 
23.8 
28.4 
1.54 
2.29 

66 
CHAPTER 4 
Quite clearly the ratio of the upper to lower quartiles varies monotonically 
with the degrees of freedom. For this ratio to be 2.67, n must lie between 
4 and 5. By the same token, l.2T' must lie between 1.92 and 2.67, and 
3.2T' must lie between 5.39 and 6.63. The proper value of n, and of 
r' = n/2, can be approximated by interpolation, but this must be done with 
some care. The x2 values in the table change more linearly with n than do 
their ratios, so it is better to interpolate within the table itself than directly 
on the ratio. The procedure we recommend is to interpolate linearly and 
simultaneously for 0.6T' and 1.6T' as follows: 
5.39 + (6.63 - 5.39)x = 2.67[1.92 + (2.67- l.92)x], 
where n = 4 + xis the interpolated degrees of freedom, the left-hand side 
is the interpolated value of 3.2 T', and the quantity in brackets on the right 
is l.2T'. Solving this equation for x gives n = 4.3, r' = 2.2 and T' = 1.82. 
The statement that there is one chance in two that the value of X lies 
outside the interval (0.6, 1.6) implies relatively great uncertainty and cor-
respondingly a relatively small value for r'. 
Just to offer a comparison, consider that the same interval (0.6, 1.6) 
represents not a 50% credible interval, but instead an 80% credible interval. 
The corresponding parameter values would be r' = 14.4 and T' = 13.4. 
Another approach to the assessment of prior parameters is through 
the consideration of hypothetical future data. In the present example, let 
us accept the statement that the prior expectation for ~ is 1.0, and ask what 
our expectation would become if the first storm were to occur after 
T = 2.0 months of observation. If we consider that one event in two months 
is too little data to cause us to change our beliefs except very slightly, then 
we might say that we would revise our expectation downward, say to 0.95. 
If we feel that even meager data should in these circumstances be weighed 
heavily, then we might revise our expected value to 0.75. 
In the former case we would write 
Eo(~) = r'/T' = 1.0 
and 
£1 (~) = (r' + 1)/(T' + 2) = 0.95, 
which leads to the solution r' = T' = 18.0. However, the latter judgement 
would cause us to write (r' + 1')/(T' + 2) = 0.75, and this leads to the 
solution r' = T' = 2. These are clearly markedly different judgements. 
The selection of prior parameters requires some familiarity and ex-
perience with the form of the density function, and the procedure should 
be carried out with care. No one feels comfortable with a situation in 
which, because additional data cannot be obtained, critical decisions have 

POISSON PROCESSES 
67 
to depend on the choice of a prior parameter. Under such circumstances, 
however, the expert's judgement is all that is available, and it must be 
expressed in useful terms. That judgement is made useful through the 
careful determination of prior parameters. If there will be appreciable data 
available then the impact of the particular choice of prior will be minimal. 
Care should still be exercised to ensure the result is consistent with our 
best judgement. 
4.5 
PREDICTIVE DISTRIBUTIONS AND PROBABILITIES 
Knowledge of a parameter such as A, the intensity of the Poisson 
process, is seldom very useful by itself. Of much greater interest is the 
prediction of future outcomes of the process. For example, what can we 
say about the probability of observing r events in a future interval of size 
T? Clearly, if A is known the answer is given by the Poisson density jp0(riA, 
T). We do not in general know A, however, except in terms of a probability 
statement. We will assume that our knowledge about >.is codified in the 
two parameters r" and T" of a gamma density. 
The predictive distribution for r is obtained [see Eq. (2.8)] by multi-
plying the probability of obtaining a particular r if A were known, by the 
probability density that this value of A is true, and summing or integrating 
over all values of A. Thus 
P{riT, r", T"} = Loo fi.o(riA, T)fr(>.ir", T")dA 
T' T"r" Loo 
= _ __ 
A' ->.TAr'-1 ->.r A'l. 
I f( ") 
e 
e 
Ul\ 
r. 
r 
o 
T' T"r" 
r(r' + r") 
---
r! f(r") (T + T"y+r-
r(r' + r") [ 
T" ]"[ 
T 
]' 
= 
f(r")r! 
T+ T" 
T+ T" . 
We may note, from Eq. (A3.1), that this expression has the form of the 
negative binomial distribution. Indeed, 
P{rlT, r", T"} = fnb(rir", T :"T") · 
(4.7) 
Thus the predictive density for obtaining r Poisson events in an interval 
of size T, when knowledge of the intensity is codified as a gamma density 
with parameters r" and T'', is equal to the probability of obtaining r successes 

68 
CHAPTER 4 
before the r"th failure for a Bernoulli process with parameter T/(T + T"). 
(Here r" and T" are used as the appropriate parameters of the posterior 
distribution. In practice we will develop a prediction on the basis of whatever 
information is available. If all that is known about A is the prior information, 
i.e., no new data, then the prior parameters r' and T' would be used as 
though they were posterior parameters.) 
In the example of storms in Oregon, we chose (at one point) r' = T' 
= 8.0. If no other information were available and we wanted to predict 
the probability of, say, exactly five storms in the next eight months of 
record, then we would write 
P{r = SIT= 8, r" = 8, Til= 8} = fnb(SI8, O.S) 
= 0.097. 
If, however, we first observed eight storms in eight months, we would have 
r 11 = T 11 = 16. Then the probability that we would see exactly five storms 
in the next eight months of record would be 
P{r =SIT= 8, Y11 = 16, Til= 16} = fnb(SI8, 0.67) 
= 0.127. 
This provides some indication of the influence of the choice of prior 
parameter on the result. Figure 4.4, with a selection of negative binomial 
distributions for various parameter values, also illustrates the significance 
of the parameters (prior or posterior) on the predictive distribution. 
It is especially useful to consider the moments of the negative binomial 
distribution: 
T 
T+ T 11 
r 11 
Enb[rlr11, T 11/(T + T11)] = rll T + Til 
T" 
= T Til, 
(4.8) 
~ 
11 
11 
, 
_ 
11 
T 
(T + T 11 ) 2 _ 
r" T + T" 
Vnb[rlr, T /(T+ T )] - r T+ T" 
T 112 
-
T T" 
Til 
(4.9) 
Recall [Eq. (4.S)] that the posterior expected value of~ is r 11/T11• From (4.8) 
we learn that the number of events expected in an interval Tis simply the 
size of the interval times the expected intensity of the process. If we were 
certain about A and had used the Poisson distribution for r, our expectation 
would have been n. 
If we had (incorrectly) used the Poisson distribution, then the variance 
of rwould also have been n. However, we see in (4.9) that the variance 
of the predictive distribution is T£ 1 (~) (T + T 11)/T11• This last factor is 
always greater than 1. Therefore the variance of the predictive density of 
r, when we are uncertain about A, is always larger than it would be if A 

POISSON PROCESSES 
.20-
.IS-
~ .10-
...... 
.OS-
.00 
.04-
.03-
~ .02-
...... 
• 01-
.oo 
0.3-
~0.2-
X 
...... 0.1-
I 
I 
0 
I 
0 
2 
0 
5 
0 
I 
I 
4 
6 
10 
15 
2 
3 
69 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I I' 
I 
I 
I 
I 
I 
I 
I 
I I' I' 
I 
I 
8 
10 
12 
14 
16 
18 
20 
X 
20 
25 
30 
35 
40 
45 
50 
55 
X 
4 
5 
6 
7 
8 
9 
X 
FIG. 4.4. Example of negative binomial probability distributions.f.b(xlr, p) for: (bottom) 
r = 20, p = 0.9 (solid), r = 2, p = 0.5 (dashed); (middle) r = 2, p = 0.1; (top) r = 8, p = 0.667 
(solid), r = 8, p = 0.5 (dashed). 
were known to be equal to its expected value. However, when T" is large 
compared to T, this effect is small. In other words, if the posterior density 
for X is based on an equivalent interval that is large compared to the one 
for which the prediction is being made, then the uncertainty in the pre-

70 
CHAPTER 4 
diction would be almost entirely due to the stochastic nature of the process 
and only minimally due to our lack of knowledge about the process. 
Thus far only the predictive distribution for r, the number of Poisson 
events occurring in a given interval, has been discussed. We may also be 
interested in predicting the interval required to obtain a given number of 
events. Following the same approach that we used to obtain the predictive 
distribution for r, we write, to obtain the probability density for t, given 
r, 
hrectCflr, r", T") = Loo fr(Tir, A)fr(\ir", T")d;\. 
(r- 1)!r(r")(T + T")'+r" 
= fitJ2(flr, r", T") 
oc !F(~~~~~~2r, 2r"). 
(4.10) 
( 4.11) 
The distribution given in (4.10) is referred to as the inverted beta-2. The 
name implies a relationship to the beta family of densities discussed in 
Chapter 3. Indeed [Eq. (A12.4)], there is a relationship to the beta, but 
more useful is the relation to the frequently tabulated F-distribution usually 
encountered in·the analysis ofvariance [Eq. (Al3.4)]. Equation (4.11) states 
that if r" and T" are the (posterior) parameters of the gamma distribution 
that describes our beliefs about the Poisson parameter A, and if we choose 
to wait for the occurrence of the rth event, then the quantity r"T/rT" will 
have an F-density as its predictive distribution. 
The first two moments of the F-distribution (Section A 13 of Appendix 
A) are 
EF(XIn,, n2) = n2/Cn2 - 2) 
n2 > 2, 
~ 
(n, + n2 - 2)n/ 
VF(XIn,, n2) = ( 
2)2( 1 
2) 
n2 > 4. 
n2-
2n2-
n1 
Recognizing the change in variable f = (rT"/r")X, we have the results 
~ 
T" 
T" 
r" 
E;f32(Tir, r", T") = r r" _ 1 = '7 r" _ 1 r" > 1, 
( 4.12) 
~ 
[ 
T" ] 2 r + r" -
1 
V;il2(Tir, r", T") = r r" _ 1 
r" _ 2 
r" > 2. 
(4.13) 

POISSON PROCESSES 
71 
Again we can compare these results with those that would be obtained 
if X were known with certainty (and the moments were based on a gamma 
distribution for T). This is why ( 4.12) is written in the form [r/ E 1(X)] 
X [r"/(r"- 1)]; r/X is the expected value ofT when X is known with certainty. 
We cannot, in this case, simply substitute, in the formula for the expected 
value of a "(-variable, the expectation of X for X, and expect to obtain a 
valid predictive expectation for T. Only if r" is large (and therefore X is 
known relatively precisely) does this give a good approximation. The pre-
dictive expectation of fis always greater, because of uncertainty in X, than 
what would be obtained if that uncertainty were ignored and the value of 
our expectation for X were used in place of X in a gamma density for f. 
In the case ofthe predictive variance off, the effect of uncertainty in 
X is again always to increase the uncertainty in the prediction. The increased 
variance of a prediction due to uncertainty in the process parameter (the 
intensity) becomes especially large for large r. 
If r" ~ 1 the expectation for f does not exist; for r" ~ 2 the variance 
does not exist. It is nevertheless still possible to calculate the probability 
that flies within any finite interval since the F-density (the inverted beta-
2 density) is proper as long as r" > 0. Much of that area, however, is 
contained in the positive tail of the distribution when the value of r" (the 
denominator degrees of freedom for the F-distribution) is small. 
Table 4.3 presents some characteristics of the predictive distribution 
of f. The first column gives the expected value of f normalized by 
rT"/r". This can be interpreted as the expected waiting period per event 
relative to the expected waiting period per event if the process parameter 
were known with certainty. The other columns give the ratio of the pre-
dictive standard deviation to the expectation of the waiting period. The 
greater the number of events being awaited (r), the less the relative uncer-
TABLE 4.3. Measures of the predictive-distributions of the waiting 
period for a Poisson process. 
E(flr) 
Coefficient of variation 
,.. 
r"E(~) 
r= I 
r= 2 
r = 10 
r= 50 
r = oo 
2 
2.000 
Undefined 
2.5 
1.667 
2.236 
1.871 
1.517 
1.435 
1.414 
3 
1.500 
1.732 
1.414 
1.095 
1.020 
1.000 
5 
1.250 
1.291 
1.000 
0.683 
0.622 
0.577 
10 
!.Ill 
1.118 
0.829 
0.487 
0.412 
0.354 
30 
1.034 
1.035 
0.744 
0.373 
0.278 
0.189 
50 
1.020 
1.021 
0.729 
0.351 
0.248 
0.144 
100 
1.010 
1.010 
0.718 
0.334 
0.225 
0.101 
00 
1.000 
1.000 
0.707 
0.316 
0.200 

72 
CHAPTER 4 
tainty of the prediction. With meager information (small r"), not only is 
the expected waiting period longer, but its relative uncertainty is system-
atically greater. 
Figure 4.5 illustrates several examples of predictive distributions of f. 
The gamma distributions shown are the limiting forms of the inverted 
beta-2 distributions as r", T" ___, oo, with r"/T" = E(i-.. ) kept constant. 
4.6 
AN EXAMPLE 
As an example let us consider the occurrence of storm surges along a 
particular coastal area during the winter half year. Examination of tidal 
records from coastal areas directly north and south indicates that the events 
do occur as though generated by a Poisson process. Unfortunately, for the 
particular region in which we are interested, no records exist. We wish to 
make predictions of the number of surges that will be encountered during 
the next five years while an extensive construction project is underway. 
Because of variations in offshore topography, there are large variations 
from place to place along the coast in the frequency of surges. The data 
from locations in either direction along the coast can be of only limited 
applicability. We do note, however, that estimates of A for five reasonably 
nearby locations range between 0.08 and 0.20 events per month. Consid-
ering everything we know about how well or how poorly determined these 
estimates are, and how representative they may be of the site we are con-
cerned with, we judge that there is a 10% probability that ~ < 0.10, and 
the same probability that ~ > 0.25. To determine the prior parameters r' 
and T', we make use of the method discussed in Section 4.5, and with 
reference to Table 4.2 we determine that Z0.9 = 24, Z0.1 = 9.6, and 
n = 16.3 in order that Z0.9/Zo. 1 = 2.5. Therefore the prior parameters 
corresponding to our judgement, given that we are willing to codify our 
beliefs through a gamma distribution, are r' = 8.2 and T' = 48. This implies 
that our (prior) expected value for A is 0.17 with a standard deviation of 
0.06. This prior density is illustrated in Fig. 4.6. 
A prediction of the number of events in a five-winter (30-month) 
period would then be given by the negative binomial distributionfnb(rl8.2, 
30/78). The mean and standard deviation of this distribution are 5.1 and 
3.22, respectively. Figure 4.7 presents this predictive distribution. 
Also shown in Fig. 4. 7 is the result that would have been obtained by 
using (incorrectly) a Poisson distribution for r, with A being given by its 
prior expected value of0.17l (=r'/T'). The expected value for rwould still 
be 5.1, but the standard deviation would be only 2.26. Clearly, removing 
the uncertainty with regard to A tends to concentrate the probabilities closer 
to the expected value, and the chances of significant departures from the 
mean are underestimated. 

POISSON PROCESSES 
f( Tl 0.8 
0.6 
0.4 
0.2 
f(T) 0.10 
0.08 
0.06 
0.04 
0.02 
f(T) 
0.6 
0.4 
0.2 
0:0 
0 
5 
GAM-1A 
.... - .... 
/ 
' 
I 
\ 
(r=2, r"=20, T"=IOI 
T 
I 
\ 
I 
',,/ 
GAM-1A 
\ 
(r=5, r"=5, T'"=IO) 
I 0 
15 
~0 
T 
/~=1, 
r'"=6, T"=12l 
1o. 
1o. 
2 
4 
6 
8 
T 
73 
10 
12 
FIG. 4.5. Predictive distributions for Texpressed as inverted beta-2 densities. The gamma 
is the limiting form of the inverted beta-2 as r" and T" become very large and their ratio 
remains fixed. 
Now let us consider the situation as it may appear three winters later. 
Let us assume that in the intervening 18 winter months exactly one surge 
occurred. What do we now expect for the next two winters? How many 
months are likely to pass before two more surges occur? 

74 
CHAPTER 4 
10 
t' 
>--
8 
I \ 
f-
(f) 
I 
z 
w 
0 
6 
I 
>--
I 
f-
_j 
I 
~ 4 
m 
<( 
I 
m 
0 
Q:: 
I 
()_ 
2 
I 
b 
0 0.0 
O. l 
0.2 
0.3 
0.4 
SURGES PER MONTH 
FIG. 4.6. Prior and posterior beliefs about the intensity of storm surges, A. The prior 
probability that A > 0.25 is 10%; the prior probability that A < 0.10 is also 10%. The posterior 
belief is subsequent to observing one surge in 18 months. 
0.2------------------------------------------
e: 0. 1 
Q_ 
0.0 
Posterior, 
-
Poisson 
\ 
Prior\ 
\ 
rL f 
~~flfi.r._~-
0 
2 
3 
4 
5 
6 
7 
8 
9 
10 II 
12 13 14 15 
R 
FIG. 4. 7. Predictive distribution of the number of events in a 30-month period. The prior 
and posterior probabilities are negative binomial distributions. The Poisson distribution gives 
the probabilities that would have been assigned to ; if A had been known to be equal to its 
prior expected value. The posterior probabilities are assigned subsequent to observing only 
one surge in 18 months. 

POISSON PROCESSES 
75 
.OS 
.04 
;= .03 
<t--
.02 
.01 
.oo 0 
10 
20 
30 
40 
50 
60 
T 
FIG. 4.8. Posterior predictive distribution of the time until two surges are observed. There 
is one chance in four that the second surge will occur within 6.9 months, and one chance in 
100 that the second surge will occur after 52 months have passed (shaded areas). 
First we correct our prior parameters by incorporating the information 
contained in the new data. Our posterior parameters become r" = r + r' 
= 9.2 and T" = T + T' = 66. The expected value of X is now 0.139. (The 
posterior density for X is shown in Fig. 4.6.) The probability distribution 
for the number of surges in the next 12 months isfnb(rl9.2, 12/78). (The 
new predictive distribution for the number of surges in five years is shown 
in Fig. 4. 7 .) The expected number of surges in the next 12 months is now 
1.67, whereas 18 months earlier the same expectation would have 
been 2.04. 
The probability density for the waiting time before two surges occur 
is/;~ifl2, 9.2, 66) (Fig. 4.8). The expected value for Tis 16.1 months (the 
expected time to the first surge is 8.0 months) and the standard deviation 
is 13.5 months. 
We can also use tables of the F-distribution, and the fact that (r"T)/ 
(rT") = 0.070T will have an F-distribution with 4 and 18.4 (2r and 2r") 
degrees of freedom, to make probability statements about the predictive 
waiting times for two surges. For example, the 25 and 99% cumulative 
values of a variable having an F-distribution with 4 and 18.4 degrees of 
freedom are 0.4801 and 3.618, respectively. Thus there is a probability of 
0.25 that two events will occur before T = 0.48/0.07 = 6.9 months, and 
one chance in 100 that the second event will not occur until about 52 
winter months (more than 10 years) have passed. 

Chapter 
5 
Normal Data-Generating 
Processes 
5.1 
NORMAL DISTRIBUTIONS AND THE CENTRAL 
LIMIT THEOREM 
The normal distribution and the normal data-generating process ap-
pear almost everywhere in statistics and are encountered frequently in 
geophysical applications. The reason for the omnipresence of the normal 
process lies mostly in the Central Limit Theorem, a very important and 
powerful theorem of statistics which states that in the limit, as n becomes 
large, the mean value of a set of n independent realizations from any prob-
ability distribution will have a normal distribution. The only condition is 
that the original variable should come from a distribution with finite mean 
and variance. 1 The mean of the resulting normal distribution will be the 
mean of the initial distribution and the variance will be 1/n times the 
variance of the original population. 
Even more remarkable is the fact that the mean of a relatively small 
sample from distributions that are decidedly non-normal will frequently 
1 This is not a condition that is universally met. We have already encountered, in dealing 
with the formulation of null priors, several examples where the variance and even the mean 
do not exist. 
77 

78 
CHAPTER 5 
have a very nearly normal distribution. This means that the Central Limit 
Theorem is not merely a theoretical abstraction, but is indeed a very useful 
and verifiable reality. 
The implications and applications of this idea in meteorology are 
many. It means, just to cite an example, that even though daily or weekly 
totals of precipitation usually have highly skewed distributions that are 
decidedly non-normal, the normal distribution often represents quite closely 
the distribution of monthly means, and is almost always a good approxi-
mation for the distribution of annual means. Exceptions occur in extremely 
dry climates in which zero precipitation often occurs over many successive 
days or weeks. In such situations, monthly or even annual means do not 
represent a sufficient number of realizations of precipitation events for the 
Central Limit Theorem to govern the distribution of the means. 
In contrast, the monthly means of such quantities as temperature, 
wind components, and even cloudiness are in general well approximated 
by normal distributions, even though the distributions of the daily values, 
especially for cloudiness, are certainly not normal. 
A particularly appealing justification for employing the normal as a 
model for a data-generating process when many of the details of that process 
are unknown, is given by Tribus ( 1969) who employs the concept, often 
encountered in information theory, that the uncertainty associated with a 
probability distribution depends on - L Pi lnpi where the Pi are the prob-
ability densities associated with a set of realizations. He goes on to show 
that if the only available information about a process is the mean and 
variance of the generated random variables, then a model for the process 
using the normal distribution implies less information than any other dis-
tribution with that mean and variance. Clearly, we should try to avoid 
imposing a model which implies any more knowledge about the process 
than is in fact available. With the given constraints, the normal accom-
plishes this. 
We will not derive here the mathematical form of the normal distri-
bution. Several approaches to its derivation are possible. Just to emphasize 
the importance of the Central Limit Theorem, though, it is worth pointing 
out that proofs appear in numerous statistics texts that the limiting form 
for both the binomial and Poisson distributions, as n---+ oo, is normal. This 
may be seen in Figs. 3.1 and 4.1, in which the probability distributions for 
large n (if the mean is not too close to zero or one) do indeed resemble 
normal distributions. 
The mathematical form of the probability density that defines the 
normal distribution is 

NORMAL DATA-GENERATING PROCESSES 
79 
fN(xi#L, h) = (h/27r)i exp[ -(h/2)(x- ~~il h > 0. 
(5.1) 
By inspection it can be seen that the distribution is symmetric about 
x = #'·Because x- #L = 0 maximizes the density, this is the single mode 
of the distribution and, because of the symmetry, the parameter #L must 
also be the mean and the median of the distribution. It is not difficult to 
prove that not only the mean, but also all moments exist if only h > 0. 
This second parameter, h, which we shall refer to as the precision, 
controls the dispersion of the distribution. We can also prove that the 
variance 
VN(xi#L, u) = E(x2)- #'2 = 1/h. 
We could have used 11 ~ in place of h, where u is the conventional notation 
for the standard deviation (the square root of the variance) of the distri-
bution. In the discussions below we will use both h and ~ somewhat in-
terchangeably, the former because it simplifies some of the mathematics 
and notation, and the latter because it is more familiar to most readers. 
5.2 SUFFICIENT STATISTICS 
Let us consider the joint density of n independent realizations from 
a normal data-generating process. The independence of the realizations 
allows us to write, for the joint density, the product of the individual den-
sities: 
n 
f(x" .X2, ... , Xn) = (h/27rt12 exp[-h L (X;- #')2/2]. 
(5.2) 
i=l 
Let us rewrite the sum in the argument of the exponential function as 
n 
L (X; - #')2 = L (X; - X+ X- #Li 
i=l 
= L (X; - x)2 + n(x- #')2 
= (n- l)s 2 + n(x- #Lf, 
where we have introduced and defined the statistics 
n 
n 
X= LX; and s2 = L [(X;- x)2/(n- 1)] 
I= I 
i=l 
This allows us to write (5.2) as 
f(x., X2, ... , Xn) = (27r)-n12hn12 exp[-h(n - l)s2/2] 
(5.3) 
(5.4) 
x exp[-nh(x- #')2/2]. 
(5.5) 

80 
CHAPTER 5 
Since the likelihood does not depend on observations, except through 
terms involving :i, s 2 and n, it is clear from (5.5) that these are jointly 
sufficient statistics for a normal data-generating process. The full infor-
mation content of the sample of n observations is available if only these 
few statistics are evaluated. There is no need to monitor any other aspects 
of the individual observations. 
As we have seen in Chapter 2, factors of (5.5) that do not involve IL 
or h can be neglected for the purpose of making inferences about the process 
parameters. We will make use of this property to express the likelihood in 
a more convenient form. First, let us note from (5.1) that 
fN(:iiiL, nh) oc hi exp[ -nh(:i- P-)2/2]. 
(5.6) 
Since the exponential term of (5.6) is identical to the last factor in (5.5), 
we are able to write 
f(:i, s 2 I~J-, h, n) oc h<n-1)!2 exp[-h(n- l)s 2/2]/N{.XIP-, nh). 
(5.7) 
Now, if we compare the first two factors of(5.7) to the functional form of 
the gamma distribution [Eqs. (4.3) or (A8.1)], we find that we can also 
write, again except for proportionality factors that involve n and s2, but 
not h or IJ-, 
f(:i, s 2IP-. h, n) oc fN(:iiiL. nh).{;,[s2l(n - 1)/2, (n -
1)h/2]. 
(5.8) 
The right-hand side of(5.8), ifwe include all the factors, is 
[ n -
1 J(n-1)/2 
h - 2-
s<n-J) exp[-h(n- l)s2/2] 
X (nh/27r)i exp[-nh(:i- P-)2/2]. 
(5.9) 
None of the terms present in (5.9) but missing from (5.8) contain either h 
or IL· Therefore, insofar as inferences about these parameters are concerned, 
the proportionality indicated in (5.8) is all that is needed. However, we 
will later use this result with the proportionality sign replaced by an equal 
sign. This asserts that if a normal generating process is used to generate n 
random variables, the sufficient statistics for the mean and precision will 
have a joint distribution that is the product of a normal distribution on :i 
and a gamma distribution on s 2• To prove this statement, which we will 
not do here, it is necessary to consider how the sample space consisting of 
all individual realizations [i.e., f(x1, x2 , ••• , Xn)] maps into the sample 
space defined by :i and s2 [i.e.,f(:i, s2)]. The factors needed to accommodate 
this transformation of variables are just the terms that have been dropped 
as irrelevant to the application of Bayes' Theorem. 
Notice that :i and s 2 are independent random variables when IL and 

NORMAL DATA-GENERATING PROCESSES 
81 
h are known. We will later find out that uncertainty about IL and h generally 
negates this independence. For the present, however, in order to discuss 
some of the interesting aspects of applying Bayes' Theorem, we only have 
to know that the likelihood function for the parameters of the normal 
process is proportional to (5.8). 
5.3 
BIVARIATE PRIOR AND POSTERIOR DENSITIES: 
PRIOR AND POSTERIOR INDEPENDENCE 
A normal data-generating process is determined by two parameters: 
,.,, the mean, and h, the precision (or u2, the variance). To make simulta-
neous inferences about the two parameters, it is necessary to deal with 
two-dimensional or bivariate prior or posterior densities. Similarly, since 
both parameters must be known (in a probabilistic sense), to make pre-
dictions of future realizations of the process, the bivariate prior or posterior 
density must be available. Even if we wish to make inferences about only 
one ofthe parameters, both parameters must be considered in interpreting 
the data, as we will soon see. The bivariate nature of the prior information 
must always be explicitly considered. 
Let us write the bivariate prior density for the mean and the precision 
asfo(P,, h)= fo(itih)fo(h), where the unconditional or marginal prior density 
for h isfo(h), andfo(itih) is the conditional prior density for,.,. Note that 
once fo(it, h) is known it is in general always possible to determine 
fo(h) = f~ fo('jt, h)dit, and then fo(itih) as fo('jt, fz)/fo(h). If fo(itih) does not 
depend on h, then it and h are, a priori, independent. (The joint prior 
density could also be written fo('jt)fo(hi,.,); in this case the condition for 
independence is thatfo(hi,.,) = fo(h) does not depend on,.,.) 
To determine a posterior distribution for the parameters IL and h, we 
use Bayes' Theorem and (5.8): 
fi( ~ h-i _ 2 
) _ 
fo(it, h)f(x, s2ill, h, n) 
I #J., 
X, S ' n -
roo roo 
Jo J_oo fo('jt, h)f(x, s2ill, h, n)d,.,dh 
oc fo<P.ih)fo(fz)fN(xi,.,, nh)_h(s2i(n - 1)/2, (n - 1)h/2). 
(5.10) 
Because,., appears in only the first and third factors of(5.10), their product 
must constitute the posterior conditional density of,.,: 
fi(itih; x, s2, n) oc fo(it ih)fJI...Xi#J., nh). 
(5.11) 
It is a conditional density because of its explicit dependence on h. The 
product ofthe other two factors in (5.10) is the posterior marginal density 
of h; this part of the function does not depend on IL· 

82 
CHAPTER 5 
If ji, and h were a priori independent, then fo(ji, I h) would not depend 
on h. However, the posterior density fi(iLih) would still depend on h because 
of the explicit dependence in ( 5.11) off 
N{Xi~t, nh) on h. Prior independence 
of the parameters does not imply posterior independence. Given only one 
application of Bayes' Theorem, i.e., incorporating the information con-
tained in a single observation, the appropriate statement of belief about 
the parameters must include their dependence. There is therefore no reason 
to seek to restrict prior densities to those for which the parameters are 
independent, as we might have done if this analysis had not been pursued. 
To the contrary, a functional form that requires the independence of It 
and h cannot be a conjugate distribution. 
5.4 CONJUGATE DENSITY, PRECISION KNOWN 
We now proceed to follow the format of previous chapters and seek 
conjugate densities for the normal data-generating process. For simplicity, 
let us first consider the situation in which the process precision is known 
with certainty to be h = h* . 
This is not as restrictive an assumption as it may first appear: h* may 
be the precision associated with some measuring device which has been 
well-tested and calibrated; or 1/h* may be the variance of a particular 
random variable for which extensive climatological records exist such that 
the intrinsic precision is known even though some uncertainty remains 
concerning the process mean. In such circumstances no additional infer-
ences can be made about h, and in place of (5.10) we can write, for the 
posterior density on ~'• 
(5.12) 
The appropriate form of fo(ji,) that is conjugate to the normal distri-
bution for X, given ~'• is a normal density itself. We can demonstrate this 
by writingfo(ji,) = fN(ji,lm', n'h.), where m' and n' are prior parameters. 
Then 
oc exp{(h./2)[n'(m'- ~t)2 + n(x- ~t)2]}. (5.13) 
Using the identity 
[ n.:i + n'm' 
] 2 
nn' 
= (n + n') 
-
~t + --(.X- m'f, 
n + n' 
n + n' 
(5.14) 

NORMAL DATA-GENERATING PROCESSES 
83 
the posterior density becomes 
N _ 
. 
1 
1 
-
{ 
h(n + n
1
) [n.X + n
1m
1 
] 2} 
fi(#lh- h., m, n, x, n) oc exp -
2 
n + n 1 
-
# 
. (5.15) 
The second term on the right of (5.14) has been absorbed into the un-
specified proportionality factor of(5.15) because it does not contain#· 
Although the expression on the right of (5.15) may, at first glance, 
seem complicated, it is not difficult to identify it with the basic form of 
the normal density. The result may be formally stated: for a normal data-
generating process with known precision (h.), if the prior density on 'it is 
normal with mean m 1 and precision n1h*, and if the subsequent data consist 
of n observations whose mean is :X, then the posterior density on 'it is 
I'(Nlh- h • 
I 
I 
-
) -jj':fNI 
II 
"h ) 
1 1 # 
-
*, m , n , x, n -
M# m , n * , 
(5.16) 
where m" and n" are posterior parameters defined by 
n" = n + n
1 
} 
n.X+ n1m 1 
• 
m"=----
n + n 1 
(5.17) 
An interpretation of the prior (or posterior) parameters follows from 
the way in which they combine with the statistics: m 1 is the prior expected 
value of#; n 1 is the weight we are willing to give to our prior knowledge 
of#, as measured in terms of an equivalent number of hypothetical ob-
servations, each with precision h*. The posterior expected value of # is 
m", a weighted average of the prior mean and the mean of the observations. 
As we obtain additional observations, the precision with which we know 
the mean increases (i.e., the variance decreases) in proportion to n", the 
sum of the real and hypothetical prior observations. 
Note that there is no need in this situation to calculate the statistic s 2• 
It provides no information about the process mean [as is evident from Eq. 
(5.12)], and we have assumed that we already have perfect knowledge of 
the precision of the process~ 
By way of example, we will consider the introduction of a new tech-
nique for estimating paleoclimatic conditions, in this case a proxy for total 
summer precipitation (perhaps the application of a new method of analysis 
of recently located varved ponds). Other methods (perhaps dendrochron-
ological) have already yielded some useful estimates of the precipitation 
during the period being studied, but uncertainties remain large and the 
new method, although difficult to implement, should provide useful in-
formation. Extensive studies based on comparisons with historical records 
have established that the new technique produces decadal summer precip-

84 
CHAPTER 5 
itation estimates that are well represented by a normal distribution with 
the mean given by the observed regional average precipitation and a vari-
ance of 400 mm2 (or h = 0.00250 mm-2). On the basis of all previously 
obtainable information, the best estimate of the total seasonal precipitation 
is 380 mm, but this is assigned a standard error of 25 mm. What is the 
revised estimate of the growing season precipitation for the time and place 
in question after two independent applications of the new technique yield 
estimates of 315 and 360 mm? 
First of all let us assume that the statement of the prior best estimate 
and its standard error can be translated into a normal prior with a mean 
of 380 and a standard deviation of 25. Since the precision of the process 
(that generates the observations) is h = 0.00250, and the variance of the 
prior is (n'hf 1 = 625, then n' = 0.64. Of course, m' = 380. Figure 5.1 is 
a plot of the prior densities implied by these statements. 
The observations are summarized by the statements n = 2 and 
:X= 337.5. From (5.17), we can now determine n" = 2.64 and m" = 347. 
The posterior (normal) density with these parameters is also shown in Fig. 
5.1. The standard deviation ofthe posterior is 12.3 mm, compared to 25 
mm for the prior. Indeed, we would now infer that there is only about one 
chance in 300 that the "true" precipitation was as high as the prior expected 
amount. 
5.5 
PREDICTIVE DISTRIBUTION, PRECISION KNOWN 
We now want to state what we can about future observations from a 
normal data-generating process with precision h = h* known, and our 
.04 
>-
1-
...... 
~ .03 
f 1 (l.tl m"=347, n"=2.64] 
~ 
(posterior) 
>-
1- .02 
...... 
_j 
...... m 
<I: 
m .01 
f 0 [~1m'=380, n'=0.64] 
/ 
(prior·) 
0 
0::: 
Q_ 
.00 
300 
320 
340 
360 
380 
400 
420 
440 
460 
SEASC)\JAL TOTAL PRECIPITATIO\J 
FIG. 5 .I. Prior and posterior probability densities of seasonal precipitation amounts. Two 
observations with known precision of(400 mm2t 1 are the basis for revising the probabilities. 

NORMAL DATA-GENERATING PROCESSES 
85 
knowledge of the mean codified as a normal density with parameters m" 
and n". In particular, we are interested in the distribution of the statistic 
.X. The arguments leading to (5.8) are easily modified (none of the pro-
portionality factors neglected involved xl to permit us to write 
(5.18) 
as the probability density for .X when J.L is known. To determine the ap-
propriate probability density that recognizes the uncertainty about J.L implied 
by the posterior probability density, we must write 
f(xin, m", n") = I: fN(xiJ.L, nh*)Ji(J.Lin", m")dJ.L 
= I: fN(xiJ.L, nh*)fN{J.Lim", n"h*)dJ.L 
v,;;;;; h r 
00 
= ~ 
J_oo exp{ -(h*/2)[n(J.L - X)2 + n"(J.L - m")2]}dJ.L 
= (Vn;"h*/27r) exp -(h*/2) --,(.X- m")2 
[ 
nn" 
J 
n+n 
X I: exp{(h*/2)(n + n") [n.: ++n;':"- J.L T}dJ.L 
_ [ nn"h* ]! [ h* nn" ( _ 
")2] 
-
exp ---- x- m 
n + n"27r 
2 n + n" 
= ftv(xlm", nuh*), 
(5.19) 
where nu = nn"/(n + n") or 1/nu = 1/n + 1/n". 
If our knowledge of J.L is codified as a normal distribution with mean 
m" and precision n"h* (regardless of whether these are prior or posterior 
parameters), then our best prediction about n realizations of the process 
is that each is normal with precision h*n"/(n" + 1), and the mean is normal 
with precision h*n"n/(n + n"), and with expected values m". If we had 
expressed the results in terms of variance rather than in terms of precision, 
we would have found that the variance of the prediction is simply the sum 
of the variance associated with the uncertainty of the mean plus the variance 
about the true mean associated with the data-generating process itself. 
5.6 AN EXAMPLE: NORMAL DATA-GENERATING PROCESS 
WITH PRECISION KNOWN 
The example we used to discuss inferences concerning the parameters 
of a normal data-generating process with known precision is not appropriate 

86 
CHAPTER 5 
for generating a prediction: we are probably not really interested in what 
the next observation of paleoclimatic evidence will be. In that case the 
principal objective was to make inferences about the parameters that de-
scribed the past climate. Now let us consider a situation in which a pre-
diction of future observations is of particular concern. Our example will 
deal with heating degree-days. 
The January total of heating degree-days is available from numerous 
stations. We will assume that analysis of these data reveals a very regular 
pattern of standard deviation, with little variation over small distances. 
This will permit us to conclude that we know the precision associated with 
January heating degree-days with certainty. (In subsequent sections we will 
see how to deal with the situation in which we admit to uncertainty in the 
precision of the process.) For a new location, remote from prior temperature 
observations, we wish to estimate the probability that January heating de-
gree-days will fall within some specific range of values. 
Let us accept a value of h* = 0.01 as the precision for the one-month 
total. On the basis of the known pattern of January heating degree-days at 
locations from which data are available, the expected value at the location 
in question is 965. However, the location in irregular terrain leaves some 
uncertainty as to whether to expect this to be typified by the gross pattern, 
and the uncertainty in this expected value is expressed by choosing a range 
of values between which the climatologist has 50% confidence that the true 
value falls. This range is 965 ± 75. Thus the probable error associated with 
the (normal) prior distribution is 0.67 a = 75. This allows us to write 
n'h* = 1/a2 = (0.0089)2 = 0.79 X 10-4 
and n' = 0.79 X 10-2, 
which is very little information indeed compared to that which one ob-
servation will provide. Of course, m' = 965. 
With only this information available the predictive distribution for 
next January's heating degree total is a normal distribution with mean 
E(:i) = m' and precision nuh* where nu = n'/(n' + 1) = 0.0071. Therefore 
the standard deviation of the prediction is (0. 71 X 10-4)-t = 119. The 
probability that the next observation will fall within the interval 900 to 
1000, for example, is 
Erf[(lOOO- 965)/119]- Erf[(900- 965)/119] = 0.615-0.192 = 0.42 
(see Fig. 5.2.) Here Erf(X) = FN(XiO, l). If X= hi(Y- J.L), then Erf(X) 
= FN(YIJ.L, h). The prior and predictive distributions are almost indistin-
guishable. The precision of the process is so much greater than that of the 
prior that almost all the variance in the prediction is due to uncertainty 
about the mean, and relatively, little additional variance is caused by the 
data-generating process itself. 

NORMAL DATA-GENERATING PROCESSES 
87 
>-
.04 
I-
........ 
(f) 
Posterior 
z 
w 
.03 
0 
>-
I-
1-< 
.02 
_j 
........ m 
<( 
~ 
.01 
Prior and 
0:: 
prior predictive 
0.... 
-......... 
Posterior predictive 
.00 
700 
BOO 
900 
1000 
1100 
1200 
1300 
JANUARY HEATING DEGREE DAYS 
FIG. 5.2. Prior and posterior (normal) probability distributions on the mean, and predictive 
probabilities of a single (future) observation before and after one datum is available. Because 
the process precision in this case is known (and large compared to the precision of the prior 
belief about the mean), the single observation very substantially increases the precision of the 
prediction. The prior predictive probability (0.42) of an observation in the interval (900, 1000) 
is represented by the shaded area. Also shaded is the area representing the posterior predictive 
probability (0.0 I) of such an event. Note that the posterior probability of the mean being less 
than 1000 is very small although an observation of such a value, while unlikely, is not ruled 
out. 
Now we wait a year and observe the temperatures at the site in question 
and measure a January heating degree total of 1035. Let us follow through 
the analysis to obtain, first, the posterior parameters, and then the predictive 
probability for the interval of concern. 
Applying (5.17) with n = 1 and x = 1035, we have 
m" = (0.0072 X 965 + l. X 1035)/1.0072 = 1034, and n" = 1.007. 
The predictive distribution for one additional year would then have E(x) 
= 1034 and, since nu = 1.007/2.007, the precision of the predictive distri-
bution would be 0.0050. This allows us now to estimate the probability of 
a subsequent year's observation falling within the interval (900, 1000) as 
Erf[-34 X 0.07] - Erf[ -134 X 0.07] = 0.01 (0.07 is just the square root 
of0.0050 and therefore the reciprocal of the predictive standard deviation.) 
The various distributions and areas are illustrated in Fig. 5.2. The predictive 
distribution is much narrower now than it was before the one observation 
because we know that the process precision is large compared to the prior 
precision on the mean. Thus a single observation, which we are quite sure 
cannot be very far from the true mean, tells us a great deal about the mean 
that we did not already know. By the same token, additional years' data 

88 
CHAPTER 5 
1.0 ,-..---:===========:!:=::::::r:=~ 
0.8 
0.6 
c 
........ 
:> 
c: 0.4 
0.2 
10 
20 
n 
FIG. 5.3. n.Jn" as a function of n and n"; n. < n• fo~ all values of n and n". 
will continue to increase the precision with which we know the true January 
expected heating degree total, but the precision of the prediction will not 
continue to increase dramatically with time. As time passes and data ac-
cumulate, the precision of the prediction will always remain smaller than 
that which is inherent in the process. This is because (see Fig. 5.3) 
nu < n", no matter how large n becomes, meaning that the prediction can 
never be more precise than the process itself permits. 
5.7 CONJUGATE DISTRIBUTION, PRECISION UNKNOWN 
The starting point for our discussion identifying a form for conjugate 
distributions for a normal data-generating process when both the mean 
and the precision of the process are uncertain will be: 
f(X, s 2 1~. h) oc JN(xl~. nh) Xj,(s 2i(n- 1)/2, (n- 1)h/2). 
(5.8) 
Viewed as a function of ~ and h, this is their joint likelihood given that 
the statistics X, s2 and n have been observed. We want to introduce this 
likelihood into Bayes' Theorem to determine the joint posterior density 
fi('[L, hlx, s2) with the help of a prior density fo('[L, h), in such a manner that 
the functional forms of the prior and posterior are identical. 
In this case we will start with a particular form of a vague prior density. 
Note that in all previous examples of vague priors we were dealing with 
improper densities, i.e., functional forms that are not real probability den-
sities at all because the integrals over the range for which the random 
variables are defined do not converge. In the case of the Bernoulli process, 
the vague prior was proportional to p- 1(1 - pt 1; for the Poisson process 
the vague prior was proportional to 1/'A.. We do not discuss the notion of 
a vague prior for the normal process with known precision, but by ex-

NORMAL DATA-GENERATING PROCESSES 
89 
amining the limiting behavior of the conjugate (normal) prior as n ---> 0, 
we conclude that the vague prior in this case is represented by fo(IL) = con-
stant > 0. In each case there is no apparent attribute to recommend the 
vague prior 2 other than it happens to be the limiting form of the conjugate 
prior density as the information content of the prior goes to zero. They 
are improper because the very notion of a vague prior precludes the notion 
of a fully meaningful prior probability statement. However, they have in 
common the attribute that 1) they vary very little within any range of 
values of the parameter over which the likelihood function might be sizable3; 
and 2) they do not vary by many orders of magnitude over ranges of pa-
rameter values where the likelihood function may be nonzero. Because of 
these attributes the inferences we will draw based on a set of data will be 
almost entirely insensitive to which of these vague priors is chosen. With 
vague prior information, no matter how we choose to express such a prior, 
the information content of the data should dominate any inference, and 
will if the vague prior has the above attributes. 
In the present case we choose a vague prior for it and h proportional 
to 1 I h. This is done because it is known that this particular form of the 
vague prior will lead to a conjugate distribution. That the vague prior 
should be independent of IL was suggested by the results of Section 5.4. 
Writingfo(jl, h) oc 1/h, we have, for a density posterior to obtaining 
n observations with statistics x and s2, 
fi(it, h-ln, X, s 2) oc h-'fN(xlm, nh)f.,.{s 2 1 n; 1 , (n - 1)h/2) 
oc h- 1hi exp[-nh(x-
~J-)2/2]h<n-Il/2 exp[-(n- l)hs 2/2] 
oc fN(itlx, nh)h"12- 1 exp(-vhs2/2) 
oc !N(itlx, nh)f.,.(hlv/2, vs2/2), 
(5.20) 
where the substitution v = n - 1 has been introduced. 
The choice of h- 1 as a vague prior leads to a joint posterior density 
that resembles (5.8) quite closely. It is different, however, in one extremely 
important way: the precision ofthe normally distributed variable it depends 
on the value of the random variable that is the argument of the gamma 
2 Each vague prior we encounter does represent a uniform distribution on some function 
of the random variable(s). For the Bernoulli process, the vague prior is equivalent to a uniform 
density on ln[p/(1 - p)]. For the Poisson process the vague prior is equivalent to a uniform 
density on InA, and the for a normal process with known h, the vague prior is uniform on JA.. 
Using h-1 as the vague prior for the normal data-generating process assigns probabilities that 
are equal for equal intervals of lnh and JA.. 
3 To make this statement implies some knowledge about the possible data, which means 
that the knowledge concerning the parameter is not wholly void. In practice the distinction 
as to what constitutes truly vague information is unclear. 

90 
CHAPTER 5 
distribution. Quite unlike the random variables of (5.8), which are inde-
pendent, the random variables of (5.20) are dependent. Equation (5.20) 
defines a very useful bivariate distribution which we will refer to as normal-
gamma (see Appendix A, Section A16). It is a proper density on the con-
dition that v > 1. In other words, at least two observations would be required 
to obtain a proper posterior density if we started with a vague prior pro-
portional to 1/h. More will be said in the following section about the im-
plications of a normal-gamma joint density. 
Let us now, mechanically, introduce the normal-gamma as the prior 
joint density for the parameters of a normal data-generating process and 
demonstrate that when we do so the posterior density is also normal-gamma. 
In doing so we will also determine the relationships between the prior and 
posterior parameters. 
We shall write the prior density as normal-gamma with parameters 
n', m', s' and v': 
fo(jJ., hin', m', s', v') = fN(P.Im', n'h)fr(hiv'/2, v's'2/2) 
= fN-y(jJ., him', n', s', v'). 
(5.21) 
It should be noted that v' is not necessarily related to n' -
1; they are 
separate parameters. Here n' describes how well the mean is known while 
v' relates to the state of knowledge of the precision. If the knowledge of 
both parameters stems solely from the same set of observations, then the 
two parameters will indeed be related. In general, however, this is not the 
case. Now we make use of Bayes' Theorem: 
h(jJ., hln', m', s', v'; n, X, s) rx !Nr<P.. hin', m', s'2, 
v')fN(xl~. nh) 
Xfr(s 2lv/2, vh/2) 
[ 
n'h 
J . 
rx h! exp - T (~- m)2 h"l2- 1 exp(-v's'2h/2) 
X M exp[- ~h (.X- ~)2 }"12 exp( -vs 2h/2). 
Making the following substitutions: 
n" = n + n' 
m" = (nX + n'm)/n", 
and using an identity essentially identical to (5.14), we obtain 
fi(jJ., hin', m', s', v'; n, X, s) rx hi exp[-n"h(m"- ~)2/2] M<•'l2>-1+112+<•t2>1 
X exp{- ~ [v's'2 + vs2 + __!!:!i_ (.X- m')2]} • 
2 
n + n' 

NORMAL DATA-GENERATING PROCESSES 
91 
Making two more substitutions, 
v" = ·v' + v + 1 = v' + n, 
v"s"2 = v's'2 + vs2 + n:X2 + n'm'2 - n"m"2 
nn' 
= v's'2 + vs2 + -
(x- m'f 
n" 
, 
we are now able to write 
fi(jJ., hln', m', s', v'; n, x, s) oc f.JP.Im", n"h) Xfr(hlv"/2, v"s"2/2) 
oc !Nr(P., hln", m", s"2, v"). 
(5.22) 
What this tedious algebraic manipulation demonstrates is that if the 
prior on the parameters of a normal data-generating process is expressed 
as a normal-gamma distribution with parameters m', n', s'2 and v', 
then the posterior distribution, after observing the statistics n, x, s and 
v = n- 1, is also normal-gamma, but with new parameters n", m", s"2 and 
v". Note in particular that nand n' are additive, as are v and v' (referred 
to as the "degrees of freedom"). The new expected value of jJ. is the weighted 
average of the prior mean and the mean of the observations, the weights 
being n' and its observed counterpart n, the number of observations. The 
quantity v"s"2 behaves very much like a sum of squares. There are three 
contributions to it: a "sum of squares" v's'2 attributable to the prior density, 
the sum of squares of the differences between the individual observations 
and their mean, and a contribution from the difference between the prior 
and observed means. 
5.8 THE NORMAL-GAMMA DISTRIBUTION: MARGINAL 
AND CONDITIONAL DENSITIES 
It is one thing to follow this mathematical manipulation, but it is 
more important to be able to interpret the results and understand the im-
plications of using the normal-gamma as a prior distribution. We stressed 
earlier that a particular formulation of a prior is useful only if it satisfactorily 
can represent our beliefs. In order to make such a judgement we must be 
able to visualize the joint distribution. It is helpful in this regard to develop 
the conditional and marginal densities implied by the joint distribution. 
In the form in which we have been writing the normal-gamma density, 
!Nr(P., hln', m', s', v') = f.JP.Im', n'h)fr(hlv'/2, v's'2/2) 
we have been expressing it as the product of the conditional density of jJ. 
(for any given value of h) times the marginal density of h, i.e., the probability 
density of h given no knowledge about jJ.. Before proceeding to examine 
the marginal density of jJ., however, let us make note of the first two mo-

92 
CHAPTER 5 
ments of the marginal density of h. Since the density is in the form of a 
gamma distribution, we know what the implied mean and variance must 
be. They are [Eqs. (A8.2) and (A8.3)] EN-r(h) = l/s'2 and VN··lh) = 2/v's'2• 
We will discuss this further after we have first obtained some additional 
results. 
To determine the marginal density of ji we must integrate their joint 
density over h. It is not very difficult to verify that the result of such an 
integration is 
f(jiln', m', s', v') = fo !N.y(ji, hin', m', s'2, v')dh 
oc [v' + n'(J.L- m')2/s'2r<•'+tlt2U/s'. 
This turns out to be the density for the Student's distribution [compare 
Eq. (A 11.11 )] that is the basis of the frequently used t-test. Thus we can 
write, for the marginal density of ji, 
f(iiln', m', s', v') = Js<iilm', n'/s'2, v'). 
(5.23) 
To put this in terms of the !-distribution that is generally tabulated, the 
quantity i = Vn' (ji- m')/s' will have a (standardized) !-distribution with 
v' degrees of freedom, i.e.,f{i) = fs*(tlv'). This makes it relatively easy to 
use available tables to assess suitable parameter values for the prior distri-
bution, if our (marginal) judgement about ji is expressed in terms of per-
centiles. Equation (5.23) also allows us to use known characteristics of the 
Student's distribution [Eqs. (All.2), (All.3), (All.5) and (All.6)] to eval-
uate the moments of the marginal distribution of J.l.. We find that 
E(ji) = m' 
v' > I, 
(5.24) 
s'2 
v' 
V(ji) = ---
v' > 2. 
n' v'- 2 
(5.25) 
Thus we have the unsurprising results that the marginal expectations 
of ji and h are given by precisely the parametric quantities we would have 
expected: m' and l/s'2 respectively. Here n' plays the role of a number of 
observations; the variance of the mean, ji, is proportional to I/n'. The 
number of degrees of freedom plays a somewhat similar role with regard 
to the uncertainty in the precision. The variance of the precision is pro-
portional to 1/v', but the number of degrees of freedom, especially when 
it is small, also plays a very important role in the marginal distribution of 
the mean. If v' ~ 1 then the expectation does not exist (although the dis-
tribution itself is still proper if v' > 0). If v' ~ 2, the integral defining the 
variance does not converge. In general the uncertainty in the precision of 
the process is reflected in additional uncertainty in the process mean. 

NORMAL DATA-GENERATING PROCESSES 
93 
Frequently we are interested not so much in h as in u2 = 1/h, or in 
u. To determine the marginal density of u2 we note that ldh/du2 1 = h2 
= u - 4, and therefore 
f(u2ls12, V1) = u-~(hlv 1/2, V1S12/2). 
In a similar way we can write down the marginal density of u as 
f(D-Is12, V1) = 2s-3fr(hlv1/2, V 1S 12 /2). 
These lead to two new families of distributions, closely related to the 
gamma distribution. The details of these distributions (called "inverted-
gamma-!" and "inverted-gamma-2") are given in Appendix A (Sections 
Al4 and Al5). We can use the results described in Appendix A to write 
down the expected values and variances of u2 and u. We find 
vi 
E(D-21sl2, vi) = sl2 -~--2 
V 1 > 2, 
(5.26) 
v -
2 12 
V(U21 12 
1) _ 
14 
v 
V 1 > 4, 
(5.27) 
s ' v - s (vi - 2)2{vl - 4) 
ECI 12 
I)= 10/ I)! r[(vl- 1)/2] 
us ' v 
s 
v 
r{vl/2) 
V 1 < 1, 
(5.28) 
V(Ujs 12 , v1) = V 1S12/(v 1 -
2) - [E(u}f 
V 1 < 2. 
(5.29) 
For purposes of evaluating probabilities and credible intervals for h, 
D-2 and u, or for inferring prior parameters if such probability statements 
are made, it is useful to make use of readily available tables of the chi-
square (x2) distribution. This is possible because x2- and -y-distributions 
are closely related. When h has a gamma distribution with parameters 
v1S 12/2 and V1/2, then 
Prob{h >h.} = Prob{D-2 < ul = 1/h.} 
= Prob{ u < u*} 
(5.30) 
In other words, we can look up, in a table of x2 with v1 degrees of freedom, 
the probability that the quantity v1s12ju2 will be exceeded. This will serve 
to define the required probability statements about h, D-2 and o-. The example 
that we will pursue in Section 5.10 will help clarify these relationships. 
At this point the only aspect of the normal-gamma joint density that 
has yet to be discussed is the conditional density of h when jL is known. 
Although knowledge of this density does not have wide application, it is 

94 
CHAPTER 5 
given here for completeness. Since the joint probability density f(jt, h) can 
always be written asj(hill)f(jt), we can write 
After some manipulation it can be shown that this, like the marginal density 
for h, is also a gamma density, but with different parameters: 
f(hill) = j,[hl(v' + 1)/2, W(v' + 1)/2], 
(5.31) 
where 
W = [v's2 + n'(m' - 11-f]/(v' + 1). 
We can infer from (5.31) [and also (A8.2)] that the expected value of 
h, given IJ., is 1/W. This is largest of 11- = m' and decreases monotonically 
as Ill - m'l increases. The conditional expectation of h becomes equal to 
the marginal expectation when 11- = m' ± s'y;;,, and decreases further as ll 
differs from m' by larger amounts. 
The conditional variance of h is everywhere less than the marginal 
variance, except at 11- = m' where they are equal. 
Figure 5.4 is a schematic illustration of a normal-gamma distribution. 
It is symmetric about the line 11- = m', and each slice through the three-
dimensional surface representing the probability density made perpendic-
ular to the h-axis gives a curve that is proportional to a normal distribution. 
However, the width of that normal slice through the surface varies inversely 
as hl. Also the area under the curve depends strongly on h, and in fact 
follows the gamma distribution that is the marginal density of h. 
If we take a slice through the density surface perpendicular to the ~J.­
axis, then we will obtain a family of gamma densities as the conditional 
densities for h given IJ.. The areas under these gamma densities will depend 
on 11-, varying according to the Student distribution that is the marginal 
density for jt, and therefore being greatest at 11- = m'. At 11- = m', the con-
ditional distribution for h has its smallest expected value [(v' + I )/v's'2] 
and also its largest variance. 
5.9 
PREDICTIVE DISTRIBUTIONS 
The joint density of the statistics x and s 2, for n observations, if 11- and 
hare known, is given by (5.8) as the product of normal and gamma densities 
for x and s2, respectively. Since we usually do not know 11- and h, except 
in a probabilistic sense, we must consider the implications of this uncer-
tainty on the predictions we will be able to make. We will find, among 

NORMAL DATA-GENERATING PROCESSES 
95 
FIG. 5.4. Graphical representation of the normal-gamma probability distribution. By 
taking slices through the three-dimensional density surface normal to the MEAN- (p.-) axis, 
as in the upper graph, we obtain curves that represent the conditional distribution of h (the 
PRECISION). These are all gamma distributions. The areas under the gamma distribution 
curves are given by a Student distribution (not shown), the marginal distribution on p.. The 
marginal distribution on h, which we would obtain by integrating the densities with respect 
to p., is represented by the bars in the DENSITY -PRECISION plane of the lower graph. The 
slices through the three-dimensional surface normal to the h-axis, shown in the lower graph, 
are all proportional to normal densities having the same mean value, but variances that are 
just 1/h. The areas under these conditional distributions of p. given hare the gamma distribution 
represented by the bars on the left. 
other things, that the independence of :i and s2, when IL and hare known, 
does not carry over to the situation where the parameters are uncertain. 
We assume that our information about jL and his codified as a normal-
gamma distribution with parameters n', m', s'2, and v'. (The parameters 
could just as well be posterior as prior: everything that follows would be 
identical. We use the parameters that best represent our current state of 
knowledge.) The predictive distributions for the statistics are 

96 
CHAPTER 5 
f(X, s 2lm', n', s'2, v') = Loo Loo f(X, s 2IJ.t, nh)f(P,, hlm', n', s'2, v')dhdJ.t 
= Loo Loo f~.iiJ.t, nh)fr( s2 1 n ; 1 , (n - I)h/2) 
X !N.y(P,, him', n', s'2, v')dhdJ.t. 
(5.32) 
The double integral on the right-hand side of (5.32) can be represented in 
either of two mathematically identical forms: 
= f.s{ilrn', n,JW, v' + v)./ip2(s2lv/2, v'/2, v's'2/v) 
(5.33) 
or 
= ./ip2[s2lv/2, (v' + l)/2, (v's'2 + M)/v] Xj.s{.ilm', nu/s'2, v'), 
(5.34) 
where 
v = n- l, 
nn' 
nu=--,, 
n+n 
v's'2 + vs 2 
W=---
v + v' 
The "inverted beta-2" distribution was encountered in (4.10); it is really 
a variant of the F-distribution [see Appendix A, especially Eq. (Al3.6)]. 
We can rewrite (5.33) as 
f(.i, s2lm', n', s'2, v') oc f.s{.ilm', nu/W, v' + v) Xfp{s 2/s'2lv, v'), 
(5.35) 
and (5.34) can be rewritten as 
f(.i, s2lm', n', s'2, v') 
oc fF[s 2(v' + l)/(v's'2 + M)lv, v' + I] X f.s{.ilm', n,Js12, v'). 
(5.36) 
In (5.33) and (5.35) the predictive distribution is expressed as the product 
of the conditional density for .i, given s2, and the marginal or unconditional 
density for s 2• In (5.34) and (5.36) it is expressed as the conditional density 
for s2 when .i is known, times the marginal density for .i. 
Both the marginal and conditional densities of .i are Student's distri-
butions, and the marginal and conditional densities for s2 take the form 

NORMAL DATA-GENERATING PROCESSES 
97 
ofF-distributions. The parameters differ, of course, depending on whether 
they are marginal or conditional distributions. It is apparent, in any case, 
that x and s2 are no longer independent, as they are when there is no 
uncertainty concerning IL or h. The conditional densities explicitly contain 
the other variable. Note, however, that the dependence on s2 in the con-
ditional distribution for x tends to vanish as knowledge of h tends toward 
certainty, i.e., if v' ~ "· The dependence on x (through M) in the conditional 
density for s2 also goes to zero for large 11'. 
Invoking some of the characteristics of the F-distribution (Appendix 
A, Section A13) allows us to draw some interesting conclusions about the 
conditional and marginal distributions of s2• In general, if the random 
variable i has an F-distribution with n1 and n2 degrees of freedom, then 
the median value of i is 1, and EF(iln1, n2) = n2/(n2 -
2). If n2 ,.. 0 the 
mean does not exist. Thus we conclude that the median unconditional 
predicted value of s 2 is s'2, but the expected value of s 2 is v's'2/(v' - 2). 
Thus the statistic calculated from the next group of data is no more likely 
to be greater or smaller than our current estimate of the variance (s'2), but 
when it is larger it is likely to exceed s'2 by more than the amount by which 
it is smaller than s'2 when smaller values occur. This may also be compared 
with (5.26), in which we noted that the expected value of the variance r? 
is v's'2/(v' -
1). Thus the predicted expected value of the statistic s2 is 
(11' -
I )/(v' - 2) times the expected value of the parameter of which it is 
an estimator. 
The conditional expectation for s2, when xis known, is 
[v's'2 + nu(x- m')2]/(v'- 1). 
This could have been written 
E(s 2ix) = E(rr) + nu(x- m')2/(v'- 1). 
In this form we see that the expected value of s2 when x is known is the 
expected value of the variance plus another term that depends on how 
much the means of the new sample and the prior expected population 
differ, as well as on how well we think the variance is known. If it happens 
that x differs substantially from m', then the data may be implying that 
the variance is larger than previously thought [compare the definition of 
11" preceding (5.21)], which is one way of explaining the factor nu(x- m')2; 
but if we have confidence in the estimate of the variance then this will be 
a minor change in belief, whence the factor 1/(v'- 1). 
The expected value of x, whether conditional or unconditional, is m'. 
We will likely most often be interested in the marginal variance of a pre-
diction of the mean of n observations, given by 
(1 I) 
v' 
V5(xlm', n,Js'2, v') = s'2 - +--; -,--1 . 
n 
n "-
(5.37) 

98 
CHAPTER 5 
There is a contribution from the uncertainty of the mean, l/n', and a 
contribution from the sampling variability 1/n. The factor s'2v'/(v'- l) is 
of course just the expected value of ti. 
5.10 AN EXAMPLE OF INFERENCE AND PREDICTION: 
NORMAL DATA-GENERATING PROCESS 
The scenario: A new community is being established in an area of 
diverse climate some distance from the location of any previous climato-
logical station. For purposes of planning, it is necessary to estimate fuel 
consumption requirements for residential heating. The appropriate cli-
matological parameter that would provide a measure of such requirements 
is the seasonal total of heating degree-days. The nearest station with a 
substantial record has a mean number of heating degree-days over the last 
28 years of 6300 and a standard deviation of 300. For purposes of making 
inferences about the new location, we will examine the difference in annual 
seasonal total heating degree-days between the new and established loca-
tions. This is the quantity whose annual value, we assume, will be a normally 
distributed random variable. 
Part of the prior information is knowledge about the location of the 
new community: elevation, exposure, proximity to large water bodies, etc. 
On the basis of this information, let us suppose that we believe, with con-
siderable certainty, that the cold season temperatures at the new location 
will be lower (and therefore the number of heating degree-days greater) 
than those at the established station. We have less confidence concerning 
just how much colder it will be. 
Let p. be the expected difference in seasonal total degree-days, old 
station less new. We might express our judgements in the previous para-
graph by statements such as 
Prob{ji < 200} = 0.01]. 
Prob{ji > 600} = 0.1 
(5.38) 
The first condition implies 99% confidence that, during the cold seasons, 
the new station will average at least 1 o colder than the older station. The 
second suggests the belief that there is one chance in ten that the difference 
over many years might amount to 3 o or more. 
To determine the prior parameters it is also necessary to express 
judgements concerning the precision or the variance of the data-generating 
process. 
It may be reasonable to assume that the variance of total degree-days 
(year-to-year) is very nearly the same at the two locations. If the degree-
days at the two locations were independent, then the variance of the dif-

NORMAL DATA-GENERATING PROCESSES 
99 
ference would be twice the variance at each location, or 2 X 3002 = 18 
X 104• Almost certainly, however, the two locations would have highly 
correlated heating degree totals. If p is the correlation coefficient, then the 
variance of the difference is 2 (1 - 1l) times the variance at each location. 
Judging that the variance at the new location is indeed very nearly the 
same as that at the old location, and estimating that p2, with a probability 
of 90%, lies in the interval 0.9 > p2 > 0.5, then leads to the conclusion 
that 
Prob{ u2 < 1.8 X 104 } = 0.05 } 
Prob{u2 < 9.0 x 104} = 0.05 
(5.39) 
is a reasonable statement of prior belief about Q-2. To interpret these in 
terms of prior parameters of a normal-gamma distribution, however, it is 
more convenient to express them as conditions on the reciprocal of Q-2 or 
h: 
Prob{h > 5.6 X 10-5} = 0.05, 
Prob{h < 1.1 X 10-5} = 0.05. 
The marginal density of his gamma, with parameters v'/2 and v's'2/ 
2. This is equivalent to saying that hv's 2 has a chi-squared distribution 
with v' degrees of freedom. The conditions on h, then, can be interpreted 
as 1.1 X 10-5 v's'2 and 5.6 X 10-5 v's'2, representing the 5 and 95% tails, 
respectively, of a x2 distribution with v' degrees of freedom. In Table 5.1 
we have extracted a small section of a table ofthe x2 distribution, showing 
the values of x2 that are exceeded with probabilities 0.05 and 0.95. We 
have also calculated the ratio of these percentiles, with the object of deter-
mining the value of v for which that ratio is 9/1.8 = 5. By interpolation 
(see Section 4.4) we find that x2 •• (0.05) = 3.4, x2 •• (0.95) = 17.0, and 
v' = 9.1. Therefore 
s'2 = 9 X 104 X 3.4/v' = 1.8 X 104 X 17 .0/v' = 3.4 X 104• 
Now let us tum our attention to the other two prior parameters, m' 
and n', and make use of the conditions on the mean expressed as (5.38), 
TABLE 5.1. Determination of the parameters t2 and ,• of the prior distribution. 
x,2(0.05) 
x.2(0.05) 
(I) 
(2) 
(2)/(1) 
7 
2.17 
14.1 
6.50 
8 
2.73 
15.5 
5.68 
9 
3.33 
16.9 
5.08 
10 
3.94 
18.3 
4.64 

100 
CHAPTER 5 
above. In the preceding section we have seen that stating the prior belief 
as a normal-gamma joint density implies that the marginal density for ji, 
isfs{ji,lm', n'/s'2, v'), or, equivalently, that i = (ji, - m')U/s' has a !-dis-
tribution with v' degrees of freedom. Using v' = 9.1 (and interpolating) we 
find that the upper 10% tail of the !-distribution corresponds tot= 1.38, 
and the lower 1% tail corresponds tot= -2.82. Using s' = 1.84 X 102, we 
can then write 
U(200 - m')/184 = -2.82, 
U(600- m')/184 = 1.38. 
Solving these two equations simultaneously gives m' = 469 and n' = 3.7. 
Summarizing, the prior parameters implied by the conditions ex-
pressed in (5.38) and (5.39) are 
m' = 469, n' = 3.7, s'2 = 3.4 X 104, 
v' = 9.1. 
These prior parameters in turn imply E0(ji,) = 469 and 
V0(ji,) = (s'2/n')[v'/(v'- 2)] = 1.18 X 10\ 
or S.D.0(ji,) = 109 is the standard deviation of our prior distribution for IJ.. 
The marginal prior density on IJ. is illustrated in Fig. 5.5. 
At this point it is prudent to examine the implications of these prior 
parameters. Do they make sense? What, do we imply, is the probability 
that IJ. os;;; 0? Do we really believe it is that small? What is our 50% credible 
interval on IJ.? Are we satisfied that we really believe that the probability 
that IJ. is outside this range is 50%? Only when we are satisfied that the 
prior parameters, and the prior distribution, truly represent our beliefs 
should we proceed. 
~ .010 
en 
c .008 
OJ 
0 
""'. 008 
: 
• 004 
.c 
nl • 002 
.c 
0 
~ .ooo~·2•o=o==~-3-o~o-----=4~oo~~--5~o-o __ .__s~o~o--~~7~oo==~a..soo 
Difference in Degree Days 
FIG. 5.5. Prior (shallow solid curve) and posterior (solid curve to the right) probability 
densities on the mean, and the prior predictive density of a single observation (dashed curve). 
All are Student's t distributions. 

NORMAL DATA-GENERATING PROCESSES 
101 
Having satisfied ourselves that the parameters are to our liking, let us 
now make a prediction. We will seek a 95% credible interval on the value 
of x to be observed during the first year of data collection. 
The predictive density for x is fs(xlm', n,js'2, v'), where nu = nn'/ 
(n + n') = n'/(n' + 1) since in this case n = 1 (dashed curve in Fig. 5.5). 
In other words, the quantity 
=X- 469 [3.7]! 
t 
184 
4.7 
(5.40) 
has the standard !-distribution with 9.1 "degrees of freedom." From readily 
available tables the 95% limiting values oft are ±2.26. The 95% credible 
interval on xis then, from (5.40), 469 ± 469. The standard deviation of 
our predictive distribution for x [compare Eq. (5.33)] is 235. 
Note how much larger this is than the standard deviation of jl that 
we calculated above (109). The difference is because the uncertainty in the 
predictive value involves both the variance ofthe data-generating process 
and the uncertainty of the true mean of that process. Indeed, 
,2 n' + 1 
v' 
V(x) = s --,- -,--2 , 
n 
v -
~ -
t2 1 
v' 
Vo(#L) - s --; -;--2 , 
n v -
~ 
12 
v' 
Eo(Ol) = s -,-2 , 
v -
and therefore V(x) = V0(jl) + Eo(Ol). 
For predictions of means (n > 1) of future realizations, the variance 
of the prediction is reduced: 
'+ 
I 
V( -)-
,2 n 
n 
v 
x -s ----
nn' v'- 2 
= Vo(iJ.) + ! Eo(Ol). 
n 
Clearly the precision of our predictions of the mean of a number of ob-
servations will always be less (the variance more) than the precision that 
is associated with the population mean. If we are making a prediction of 
the mean of a large number of future observations, then the precision of 
the prediction will only differ slightly from that ofthe population (or pro-
cess) mean. 

102 
Year 
I 
2 
3 
4 
5 
TABLE 5.2. Observations. 
X 
509 
1032 
568 
623 
640 
CHAPTER 5 
Now let us consider that several years pass and observations become 
available as in Table 5.2. The (sufficient) statistics are x = 674.4, 
s2 = 42595., n = 5, v = 4. Parameters for the posterior density of ji. and 
hare then 
m" = 587.0, 
n" = 8.7, 
v" = 14.1, 
104 X (30.94 + 17.04 + 227.41 + 81.39- 299.78) = 57.00 X 104, 
The uncertainty in ji. is now measured by V1(jj.) = 0.541 X 104 and 
S.D. 1(jj.) = 73.6 (see Fig. 5.5). This reduction from the prior values is a 
measure of the information content of the five years' data. The posterior 
expectation of the variance £ 1(112), however, is somewhat larger than was 
the prior expectation ( 4. 71 X 104 versus 4.36 X 104). The prior and posterior 
distributions of 112 are shown in Fig. 5.6. As data are added, they must 
x 1 
o-<~ 
0.3 
-.... ' ' ' 
0.2 
' 
"' 
' 
0 
' ' ' ' ' 
0.1 
' ........ 
............ .... _ 
0.0 
0 
2 
4 
6 
8 
I 0 
12 
X I 04 
02 
FIG. 5.6. Prior (solid) and posterior (dashed) probability densities on the variance. These 
are inverted-gamma-! densities. Note the slightly narrower distribution of the posterior com-
pared to the prior. 

NORMAL DATA-GENERATING PROCESSES 
103 
always contribute to decreasing the uncertainty in the parameters, i.e., IL 
and h, but the expectations of the parameters can either increase or decrease. 
In this example the prior and posterior variances of lz are 6.46 X 10-6 and 
3.51 x 10-6• 
As criteria for assessing the prior parameters, we chose conditions on 
IL and r?-. It may be instructive to examine the probabilities we would assign 
to those conditions a posteriori. 
Prob{it < 200} = F{8
· 7(20~0~ 587)114.1 J 
= F,(-16.5114.1);;;;;; 0 
(A) 
It was our prior judgement that IL < 200 was unlikely; we assigned to it a 
probability of 1%. Now we are willing to all but completely exclude such 
a possibility. 
Prob{it > 600} = G{8
·7(6i0~ 587)114.1 J 
= 1 - F,(0.563j14.1) = 0.29. 
(B) 
The prior probability assigned tom> 600 was 10%. The five-year mean 
was greater than 600, but because of the relatively strong prior belief that 
such large values would occur only infrequently, the posterior view is still 
in favor of it < 600, but not nearly as strongly as it had been. 
~ 2 
4 } _ 
[14.1 X 4.04 X 104 1 J 
Prob{ u < 1.8 X 10 
- Gx2 
1.8 X 104 
14.1 
= 1 - Fx2(31.6j14.1 = 0.004. 
(C) 
{ ~ 2 
4 } 
[14.1 X 4.04 X 104 1 J 
Prob u > 9 X 10 
= Fx2 
9 X 104 
14.1 
= Fx2(6.33j14.1) = 0.04. 
(D) 
The prior probabilities to each of these conditions was 5%. Since the pos-
terior parameter s"2 is larger than the prior parameter s12, the center of the 
probability density function for ~ is shifted toward higher values. It is not 
surprising that probabilities assigned to very small values of i12 decreased 
substantially from prior to posterior. We must also note, however, that the 
probability of (12 > 9 X 104 has also decreased, although only slightly. This 
is because the greater information content of the data has allowed us to be 
more precise in our judgements about i12• 
Now let us consider a prediction of :X and s2 for an additional five 
years (n = 5, v = 4) of data. Our primary interest is in the marginal densities 

104 
CHAPTER 5 
and probabilities, i.e., probability statements about one of the parameters 
without regard to what we may conclude about the other. The appropriate 
marginal densities, from Eqs. (5.35) and (5.36), are 
ft[(x- m")Vnu!s"lv"] 
and 
With these, and the appropriate tables, we can construct the required pre-
dictive statements about the two statistics. For example, let us again de-
termine 95% credible intervals. From tables, the critical values of the ar-
gument of the !-distribution are ±2.144, and the critical values of the £-
distribution are 0.115 and 3.91. We also note that 
nu = nn"j(n + n") = 5n"/(5 + n") = 3.18. 
Therefore we should be willing to assign a probability of95% to the state-
ment that x will be in the interval 587 ± 2.144s";v,:;;: = 587 ± 242. We 
should also believe, with 95% confidence, that s 2 will fall in the interval 
(0.46 X 10\ 15.8 X 104). This last interval may appear at first glance to 
be quite large, but even if u2 were known with confidence, we would expect 
large sampling fluctuations in s 2 calculated from only five observations. 
As a final exercise, let us enquire into the 95% credible interval for s2, 
as above, except in the situation that we already know that x = 500. The 
appropriate conditional density is then 
J(v" + l)s 2 1 
J 
f1v"s"2 + M v, v" + 1 ' 
where M = nu(x- m")2 = 2.407 X 104• The limits on the arguments of 
the £-distribution are 0.116 and 3.80. Therefore the interval for s2 is (0.46 
X 104, 14.9 X 104). This is somewhat less wide than the marginal 95% 
credible interval; the lower limit is the same but the upper limit is somewhat 
smaller. The interval is narrower because the information about x removes 
some of the uncertainty concerning the nature of the data, and it has been 
shifted toward lower values in recognition of the fact that x is not too 
different from m". One of the reasons for anticipating the possibility of 
large values of s 2 is the chance that an anomalous observation would occur 
and distort the statistics. Since the mean is not particularly anomalous, 
there is less reason to believe that any of the data are very anomalous. 

Chapter 
6 
Normal Linear Regression 
6.1 
INTRODUCfiON 
A particularly useful extension of the normal data-generating process 
is one in which the random variable-the datum being generated-is nor-
mally distributed about an expected value that is a function of other known 
or observable quantities. The term regression is used to connote the de-
pendence of the expectation of a dependent random variable (the "pre-
dictand") on other "predictor" variables. If the functional dependence of 
the expected value of the random variable on the other quantities is linear, 
then the model is referred to as a linear regression model. In a normal 
linear regression model, the distribution of the random variable, given the 
independent variable(s) in the linear regression function, is normal. In the 
specific model with which we shall deal, not only is the distribution of the 
random variable normal, with mean given by a linear function of the other, 
predictor variables, but the precision of that normal distribution is taken 
to be constant, and in particular is not dependent on the values of the 
predictors. 
The linear regression equation has the form 
E(y;) = L {jjXj;, 
}=I 
105 
(6.1) 

106 
CHAPTER 6 
where Y; is the value of the random variable on the ith occurrence, the {31 
are the regression coefficients, and the x1; are the set of r predictors, as 
observed on the ith occurrence. The model of the data-generating process 
lS 
Y; = E(y;) + ~;, 
(6.2) 
where the ~; are normally distributed independent random variables with 
zero means and precision h, i.e., with probability densities fN{t;iO, h). As 
defined in this way, r + 1 parameters are required to specify the data-
generating process: the r regression coefficients ({31, j = 1, ... , r), and the 
precision h. As before, we want to make predictions of the Y; consistent 
with our limited information about the parameters of the process, and we 
will want to use the few observations available (of Y; and the corresponding 
x1;) to improve our knowledge of the parameters and to make predictions 
of they;. 
We will adopt the convention that xli = 1 for all values of i. This is 
entirely consistent with the manner in which the x1; are defined, and allows 
us to have a constant term in the regression equation. Then {31 becomes 
they-intercept of the regression line or surface. (Of course, if the situation 
being modeled dictates that the intercept is, with certainty, zero, then we 
can set {31 = 0.) 
If we choose r = 2, then we have a single predictor x 2 ; this situation 
is referred to as simple linear regression and will be discussed first in the 
subsequent sections. When r > 2, the model is referred to as multiple 
regression. Multiple regression complicates some of the computations, but 
we will be able to develop the necessary equations as a direct extension of 
simple linear regression. 
Regression of precisely this form is probably the most commonly used 
statistical tool of meteorological prediction. In particular, most of the MOS 
(model output statistics) products that provide primary guidance to Na-
tional Weather Service forecasters are based on multiple linear regression. 
(A major factor in the success of this procedure has been the care taken in 
the definition and selection of predictors.) The NWS Techniques Devel-
opment Laboratory has used regression models to produce predictions of 
precipitation probability, maximum and minimum temperature, cloudi-
ness, and numerous other parameters. 
At the other end of the prediction spectrum, many, if not most pub-
lished descriptions of efforts to produce long-range weather or climate fore-
casts employ regression. Regression is also used both in a research mode 
and operationally to represent in practical forms what are otherwise ex-
tremely complex physical relationships. For example, multiple regression 
is the basic procedure by which temperature soundings are retrieved from 

NORMAL LINEAR REGRESSION 
107 
satellite measurements of radiance. Hardly a meteorological journal appears 
that does not contain several applications of regression. It is unfortunate, 
in this context, that more attention has not been paid to errors that are 
inherent in estimating parameters on the basis of regression models. 
6.2 SUFFICIENT STATISTICS FOR SIMPLE 
LINEAR REGRESSION 
Setting r = 2, and observing the convention that Xu = 1, the data-
generating process for simple linear regression is expressed as 
Y; = f3t + f3zXz; + E;} 
f(E;) = fN(E;iO, h) 
. 
(6.3) 
We are able to observe Y; and also necessarily x2;. We are not, however, 
able to observe directly E;, the random variable that is the essence of the 
stochastic process in which we are interested. 
Note that Y; and E; are random variables, but x2; is not. It is possible 
that x2 is a manifestation of a stochastic process (an observation with or 
without error of a random variable, or an observation with random error 
of a deterministic quantity), but we are not interested in the stochastic 
properties that x2 may have. Our model says that the expectation of jJ 
depends on the value obtained for x2 , regardless of whether this value of 
x2 is or is not an estimate, or is derived from some other variable. 
We shall consider the likelihood function for a set of n observations 
of this process. These n observations will have a set of values x 2;, i = 1, 
... , n, with .i2 = L: x 2;/n. For convenience in the analysis of the properties 
of the statistics of simple linear regression we will introduce a new parameter 
11: 
(6.4) 
The regression line is now written E(y) = ~ + {32(x2 - .i2). We will use the 
pair~ and {32 as the regression coefficients in place of {31 and {32. 
With this change of notation, the joint likelihood of a set of obser-
vations of y;, i = I, ... , n, is 
f(jJI, Y2, ... , Yni~, {32, h; X21, ... , X2n) 
= D 
[(h/211")! exp(- ~ {y;- ~- {32(x2;- .i2)} 2)] 
= (27r)-n12hn12 exp(- ~ i~ {Y;- [1] + f3z(X2;- Xz)]} 2) . (6.5) 

108 
CHAPTER 6 
With some manipulation the summation within the exponent of(6.5) can 
be expanded as 
n(ij - if + ({32 -
b2)2 2: (X2i - x2f + L (y; - YJ2 
+ b2 L (y;- y)(x2;- x2), 
(6.6) 
where 
2: (y; - y)(x2; - x2) 
b2 = ------.,.-
L (x2;- x2)2 
(6.7) 
This introduces the statistic b2, an estimate of the value of {32. Another 
statistic, i, is an estimate of ij [compare (6.3) and (6.4).] Then 
f:; = Y;- [i + b2(X2;- X2)] 
(6.8) 
is an estimate of the random component of the process on the ith occasion. 
We note further that the last two terms of ( 6.6) can be expressed as 
2: (y; - y)2 + b2 L (y; - i)(x2; - x2) 
= L {Y;- [i + b2(X2i- x2)]}2 = 2: f:/ = (n - 2}s2, 
(6.9) 
which serves as a definition of the statistic s 2• Through these algebraic 
manipulations we have rewritten the sum in the exponent of (6.5) as 
Thus we are able to write the likelihood as 
f(Y, b2, s 211i, {32, h, n) oc hn12 X exp[- ~ n(ij- i)2] 
[ 
h 
2 ""' 
-2] 
2 
X exp - 2 (bz- {32) 
LJ (x;- x) 
X exp[-(n- 2)hs /2]. 
(6.10) 
Because the likelihood can be written in this form, i, b2 and s 2 are sufficient 
statistics for the parameters ij, {32 and h. Other combinations of the data 
can be found that are also jointly sufficient statistics (in particular we will 
later use b1 = i- b2x 2 as one of the sufficient statistics in lieu of y), but 
this set has certain desirable attributes. 
In the first place, if we had substituted any other estimates of the 
coefficients of i and b2 in the first line of the right-hand side of (6.8), the 
resulting sum of squares of the residuals, 2: ~ 1 2 , would have been at least 
as large. In other words, i and bz are "least-squares" estimates of ij and {32 
(and b1 is a "least-squares" estimate of {3!). Also, with b2 = {32 and ij = Y, 
the second and third factors of ( 6.1 0) are necessarily maximized. Since we 

NORMAL LINEAR REGRESSION 
109 
also now know that L ~? and consequently s2 are minimized, the fourth 
factor of ( 6.10) also takes on its largest possible value. Therefore the statistics 
we have chosen are maximum likelihood estimators of the parameters, as 
well as "least-squares" estimators. 
Let us rewrite this joint likelihood one more time as 
f(Y, b2, s2ii1, {32, h, n) 
oc h112 exp[n2h (77 -:- y}2 J 
X h112 exp[~ (b2 -
{32)2 L (X2;- x2)2 J 
X h(n-2>12 exp[-(n- 2)hs2/2], (6.11) 
oc !N<Yii1, nh) X fN[b2if32, h L (x2; - .i2)2] 
Xfr[s 2i(n- 2)/2, (n- 2)h/2]. (6.12) 
By expressing the likelihood in this way as the product of normal densities 
for y and b2 , and a gamma density of s2, a strong resemblance to the 
likelihood for the simple normal data-generating process appears. The factor 
n- 2 in the definition of s 2 (6.9) was chosen to agree with the exponent 
of the precision, h, in the last pair of terms in (6.11). Note that y and b2 
have independent distributions. It was the choice of ij in lieu of {31 as one 
of the parameters that led to this result. 
6.3 DIFFUSE PRIOR-SIMPLE LINEAR REGRESSION 
Given the statistics from a set of observations, and prior information, 
we wish to make inferences about the parameters of the model. Because 
of the similarity of (6.12) to (5.8), we proceed in a manner that is a direct 
extension of the procedure for a normal data-generating process. We begin 
by assuming that a vague prior can be represented by 
fo(ij, {32, h) oc 1/h. 
(6.13) 
Entirely analogously to the arguments of the last chapter, this leads to a 
posterior density 
Thus with a vague prior the posterior joint density has a functional form 
that very much resembles a higher-dimensional form of the normal-gamma 
distribution. 

110 
CHAPTER 6 
Because in this situation 11 and /32 are independent, marginal densities 
are readily obtained. If we integrate ( 6.14) over all values of 11 we are left 
with the joint marginal density of /32 and h. Since the second and third 
factors of (6.14) do not contain ij, and since the integral over the entire 
function must be unity, the joint marginal density of /32 and h is the product 
of the second and third factors of ( 6.14 ), which is a normal-gamma joint 
density. Similarly the joint marginal density of 11 and h is the product of 
the first and third factors-also a normal-gamma density. Therefore we 
can follow the results of Section 5.8 and write for the separate posterior 
marginal densities of 11 and /32: 
};(1ii.Y. b2, s2, n) = fs(1iiY, n/s 2, n- 2), 
};(/3ziY. bz, S2, n) = fs(!32ib2, L (xzi- x2)2/s2, n- 2). 
(6.15) 
(6.16) 
That is, the posterior marginal densities of the regression coefficients, when 
the prior is diffuse, are Student densities with n - 2 degrees of freedom 
and with means given by the respective statistical estimates. 
We earlier chose to write the linear regression relation between y and 
x2 as y = 7i + /32(x2 - x2); therefore 11 is the ordinate of the regression line 
corresponding to x2 = x2 • This allowed us to obtain the result that, given 
a diffuse prior, the joint posterior distributions of 11 and /32 (like the sampling 
distributions of jland b2) are uncorrelated, i.e., are statistically independent. 
Had we written the regression equation in its original form, y = /3 1 + x 2{32 , 
we would not have been able to obtain, in general, uncorrelated estimates 
of the parameters. Defining 17* as the ordinate corresponding to some gen-
eral value of x2 , say x 2 = x*, then 
(6.17) 
For example, if x* = 0, then 17* = 7i - /32x2 = (31 is the y-intercept of the 
regression line. 
From (6.17) we can write 
E(ij*) = £(~) + (x*- x2)£(i3'2) 
= Y + bz(X* - X2), 
(6.18) 
where the expectations have been written as the means of ( 6.15) and ( 6.16). 
Also, from (6.17) 1, 
I V(A) = £(A2) -
[E(A)f, 
V(A - B) = E(A - B)2 -
[£(A - B)f 
= £(A2)- 2£(AB) + £(B2)- [E(A)f + 2£(A)£(B) - [E(B)f 
= V(A) + V(B) - 2 Cov(A, B). 

NORMAL LINEAR REGRESSION 
111 
~ 
2 
~ 
~ 
~ 
v(7j*) = v(Tj) + (x* - x2) v({j2) + 2(x* - .X2) Cov(f, {j2). 
(6.19) 
From (6.15) and (6.16) we can determine the variances of~ and {i2 to be 
V(~) = n - 2 s2 
(6.20) 
n- 4 n' 
(6.21) 
Since, as we have already seen, 7i and {j2 are independent given a diffuse 
prior their covariance is zero. Therefore 
u,~) _ n- 2 2[1 
(x* -.X2)2 J 
"\'17* - -- s - + 
. 
n - 4 
n 
L (x2 - x2)2 
(6.22) 
Setting x* = 0 in (6.17) gives '17• = {j1; therefore the variance of {j1 is 
V({II) = n - 2 s2 [ 
L xl J . 
(6.23) 
n - 4 n L (x2 - x2)2 
From (6.22) we see that the uncertainty of the ordinate of the regression 
line is least when x* = x2 , i.e., we are most certain of the position of the 
regression line at the value of the abcissa corresponding to the mean ofthe 
data. The further the value of x2 for which we wish to know the ordinate 
of the regression line is from that mean value, the greater is our uncertainty. 
This is a warning that we should not place much credence in the extrap-
olation of a regression line to values of the predictor variable that deviate 
from the sample mean by more than two standard deviations. (Remember 
that this refers to our knowledge of the regression parameters based on a 
sample of observations and only vague or negligible prior information. If 
the prior information were not vague then extrapolation would still be 
unwise beyond some limit, as we shall later see, but that limit may be 
somewhat greater than that defined by the spread of the sample predictor 
variables.) 
It is not only true that the variance of n* exceeds that of 1j for 
x* =I= x2 , but also the covariance between '17• and {j2 will be different from 
0. To see this we note from (6.17) that 
Cov({j2, '17•) = E[7i{j2 + {jl(x* - .X2)] - E({j2)E(17*) 
= £(7i{j2) + (x* - x2)E(f3l) - E((32)E(1j - (32(x* - .X2) 
= Cov(Tj, (32) + (x* - .X2) V({32) 
= (x* - .X2) V((32) 
_ n- 2 
2 
x*- x2 
---4s ""'( 
-)2· 
n-
L... X2;- X2 
(6.24) 

112 
CHAPTER 6 
In general this is zero if and only if x* = x2 • For x* = 0 we have 77* = {j1 
and 
(6.25) 
We will now introduce a variation on the notation that we have been 
using. This will be most helpful when we extend our results to informative 
priors, and also to multiple, as opposed to linear regression. The joint 
posterior density on ij, {j2 and h (Eq. 6.14) can be rewritten, using boldface 
symbols to represent matrices and vectors, in terms of bivariate normal 
density (see Appendix A, Section A 17) as 
Ji(n, rJ2, hiY, b2, s2) = JN<2>CBIB', Nh)f-t(hlv/2, vs2/2), 
(6.26) 
where 
8 = (~) 
(6.27) 
is the vector of coefficients, 
B' = (~)' 
(6.28) 
N=(on 
o 
) 
L (x2; - x2)2 ' 
(6.29) 
and v = n - 2. This form of N with zero covariance terms (the off-diagonal 
entries) has been convenient for dealing with a diffuse prior and simple 
linear regression. For extensions to nondiffuse priors and multiple linear 
regression it is more convenient to return to the original notation, and 
express the posterior density in terms of {j1 and {j2 • If we write 
p = (:~)' 
b = GJ = (y -b:2.x2), 
then the appropriate posterior density with a diffuse prior is 
Ji(P, h, lb, s2; n, v) = fN<2l(Pib, nh)f-t(hiv/2, vs 2/2), 
where 
(6.30) 
(6.31) 
(6.32) 
(6.33) 

NORMAL LINEAR REGRESSION 
113 
( 
1 Xzt) 
1 Xzz 
X= 
• 
• 
. 
1 Xzn 
(6.34) 
Note that the inverse of n is 
1 
( 2: Xz2 - Ln Xz) 
n-I = n 2:;x/-(L:xz)2 -2:;x2 
(6.35) 
Thus individual elements of n- 1 are the factors that distinguish among 
(6.21), (6.23) and (6.25), the variances of ~ 1 and ~2 , and their covariance. 
To complete the introduction of matrix notation, we note that the 
values ofthe predictand variable are 
and the values of the sufficient statistics may be written as 
b = xTy, 
s2 = (1/v)(y - xb)T(y - xb). 
(6.36) 
(6.37) 
6.4 SIMPLE LINEAR REGRESSION WITH A NONDIFFUSE 
CONJUGATE PRIOR 
In Section 6.3 we chose a diffuse prior proportional to 1/h, and ob-
tained the result that the resulting posterior distribution was bivariate nor-
mal-gamma (Eq. 6.32). This step was taken because we expected the pos-
terior distribution to be conjugate to the normal linear regression data-
generating model. We shall not go through the details of the derivation, 
but instead will simply state the result that this is indeed the case: if the 
joint prior density on the vector of regression coefficients 1J and the precision 
h is bivariate normal-gamma with parameters b', n', s'2 and v', i.e., 
(6.38) 
then the posterior density will have the same form with the posterior pa-
rameters given by 
b" = (n' + n)- 1(n'b' + nb), 
n" = n' + n, 
(6.39) 
(6.40) 

114 
CHAPTER 6 
v" = v' + n, 
(6.41) 
(6.42) 
Here n' must be a 2 X 2 square symmetric positive definite matrix; n is 
also generally a square symmetric and positive definite matrix containing 
the statistics as defined in (6.33). The scalar n is the number of observations. 
When the data do not contain at least two distinct values of the predictor, 
n is a singular matrix and b is not defined. In this case we must use, in lieu 
of (6.39), 
These relationships resemble very closely those for the normal data-
generating process (Chapter 5) and their derivation is entirely analogous. 
In particular the posterior expected value of the regression coefficient vector 
has the appearance of a weighted mean of the prior expectation, b', and 
the least-squares estimates of the coefficients based on the new data. Because 
the weights are matrices, however, they have interesting properties; a dis-
cussion of these properties will help to elucidate the manner in which 
judgements concerning regression coefficients are modified by the data and 
influenced by the prior. 
First let us consider the use of scalar weights. Let a and a' be two 
scalar quantities to be combined with positive weights w and w' such that 
a= (aw + a'w')/(w + w'). It is easy to show that a must be greater than or 
equal to the smaller of a and a' and less than or equal to the larger. The 
relative weights tell us how close the weighted average is to one or the other 
of the end points, but it must always lie between them. If we had been 
combining vectors with scalar weights, a similar result would have been 
obtained. If, as in Fig. 6.1, A and A' are points that represent two vectors, 
and these are to be combined through a (scalar) weighted average as 
A = (Aw + A'w')/(w + w'), where again wand w' are both positive, then 
the point representing A must lie on the line segment joining A and A'. 
When the weights by which we combine vectors are matrices, however, 
the point representing the weighted average will in general lie off the line 
connecting the original two points, and may even lie well beyond either 
of the individual points. This is so even with the restriction that the matrix 
weights be positive definite symmetric matrices. It is easiest to demonstrate 
this for two-dimensional vectors and 2 X 2 weight matrices; this is of par-
ticular interest to simple linear regression, but it holds of course in the 
more general n-dimensional case that applies to multiple linear regression. 
Consider first the situation when both weight matrices W and W' are 
strictly diagonal, i.e., their off-diagonal elements are zero. In this case, the 

NORMAL LINEAR REGRESSION 
115 
A" 
FIG. 6.1. Weighted averages of two vectors A and A'. In the case of A1 the weights are 
positive scalars, so A1 must lie on the line segment connecting A and A'. When the weights 
are strictly diagonal (all diagonal elements positive; all off-diagonal elements zero) the weighted 
average must lie in the rectangle defined by the endpoints of the two vectors, as does A2• In 
the case of A" the weights are more general positive definite matrices. 
weighted average A, defined by A= (AW + A'W')(W + W')- 1, will always 
lie within the rectangle whose diagonal is the line segment connecting the 
points A and A'. It will lie on the diagonal if and only if the diagonal 
elements of the two weight matrices are in the same ratio to one another, 
i.e., if we can write W = cW' where cis a (positive) scalar. In this case each 
of the two scalar elements of the vector A is individually a weighted average 
of the corresponding scalar elements of the original vectors. However, if 
the weights are not in the same proportion to each other, the effective 
weighting of the scalar elements of the vectors will be different. The com-
ponents of A will each lie between the corresponding components of A 
and A', but in different relative positions. 
If we now consider weights which, although still symmetrical and pos-
itive definite, have nonzero off-diagonal elements, we obtain a very different 
result. It is now possible for one or more of the elements of A to lie outside 
the limits defined by the values of the corresponding elements of A and 
A'. This is perhaps best illustrated by an example. Let A and A' be given 
(see Fig. 6.1) by 

116 
CHAPTER 6 
A = ( 1, 1), 
A' = ( -1, !), 
and their weights by 
w = ( 5 -2) 
w = (5 2) . 
-2 
1 
' 
2 
1 
(6.43) 
Carrying out the calculations indicated, we find 
(W + W'f 1 = (~ ~) , 
(6.44) 
(6.45) 
i.e., the second element of A is negative! This is clearly outside the range 
defined by the second elements of A and A'. 
It is particularly illuminating to further interpret this result in terms 
of the role that (6.39) plays in our inference concerning regression coeffi-
cients. According to (6.32) the prior density on p, given h, is bivariate 
normal with expectation b' and precision n'h. Consider that the values 
given above for A' and Ware indeed the appropriate prior parameters. Let 
us locate the most credible values of A', according to this statement of 
prior parameters, by its 50% credible region, an elliptical region in the a1ar 
plane, as illustrated in Fig. 6.2. (Since we have not given a specific value 
for h, the size of this ellipse is still arbitrary. However, its shape and ori-
entation are completely determined by W'.) Similarly we can represent the 
likelihood function for the estimates of the regression coefficients as a bi-
variate normal distribution. If A and W are taken to correspond to the 
statistics b and n, then we can represent the likelihood function also as an 
elliptical region in the a 1a2-plane (Fig. 6.2). 
The posterior density is proportional to the product of the prior density 
and the likelihood function. Therefore we expect the posterior density to 
be maximum where the credible regions outlined by the ellipses representing 
the prior density and the likelihood function overlap. Because of the par-
ticular orientations and shapes of the ellipses determined by our choice of 
matrix weights, this region of overlap lies below the a1 (or {:11) axis. The 
expressed prior judgement contained in n' was that (31 and (32 were negatively 
correlated, i.e., that a positive departure of one from its expected value 
would likely be associated with a negative departure by the other. The 
statistics (i.e., b and n) suggested that (31 and (32 were both positive, that 
the newly determined estimates of (31 and (32 were positively correlated, 
and that we should have considerably more confidence in the estimate of 
(31 than of {:J;. Thus the positive value of the statistic b1 combined with the 
negative lf1 (the prior expectation of (31) led to the conclusion that (31 was 

NORMAL LINEAR REGRESSION 
117 
FIG. 6.2. Interpretation of positive definite weight matrices as credible ellipses. The location 
of A", and the shape and orientation of its credible ellipse (or its weighting matrix), are deter-
mined by the shapes and orientations of the ellipses surrounding A and A'. 
near zero; with {3-I near zero, both the prior belief and the likelihood function 
lead us to prefer a value of {J2 that is negative, despite the fact that both b2 
and b2 are positive. Figure 6.2 also shows the shape and orientation of the 
credible region as given by the posterior parameters. Because off-diagonal 
elements ofn" = (W + W') are zero [see Eq. (6.44)], there is no correlation 
between the posterior beliefs concerning {J1 and {J2 • 
As with the previous data-generating processes, the prior parameters 
may be thought of as the statistics corresponding to a hypothetical (or real) 
set of prior observations. The more complex interactions among the several 
parameters in this case (there are, in all, seven scalar quantities that must 
be specified to give a complete set of prior parameters) place a greater 
burden on the climatologist when taking care that the prior does indeed 
represent his or her prior views. 
Our discussion here has been limited to the situation of simple linear 
regression in which we are estimating only two regression coefficients. 
However, the concepts, and indeed the mathematics, apply to multivariate 
regression in which there may be r > 2 coefficients. In such situations n, 

118 
CHAPTER 6 
for example, is an r X r matrix, and Eqs. (6.39)-(6.42) are the appropriate 
ones for determining posterior parameters on the basis of prior parameters 
and a set of observations. 
6.5 PREDICfiVE DISTRIBUTIONS 
We will now consider the distribution of a set of m observations of 
the random variables Y; generated by a simple linear regression process. 
The m-component vector 
Y = (yi, Y2, · · ·, Ym)T 
represents these observations which correspond to values of the predictor 
variables given by the matrix 
x= 
(6.46) 
Here we have used the convention that xli = 1 for all values of i. Knowledge 
of the process parameters {j and his codified as a bivariate normal-gamma2 
density as given in (6.32), with parameters b', n', s'2 and v'. (Double primes, 
indicating posterior parameters if they represent the current state of knowl-
edge, could be also used.) 
For known values of {j and h we would write the joint density of the 
y;as 
(6.47) 
The precision is written as hi, where I is an m X m identity matrix, to 
correspond with the earlier assertions that the precision is not dependent 
on the value of the predictor, and that the departures of Y; from their 
expected values (which do depend on the predictors) are independent (i.e., 
the departure from the expected value for one observation does not influ-
e·nce the departure for any other observation). To obtain predictive prob-
abilities that recognize our uncertainty in {J and h, we must write instead 
f(ylx; b', n', s'2, v') =I I I 
fN<m>(ylxP, hi) XfN<2>({ilb', n'h) 
Xf..,(hlv's'2/2, v'/2)d{31d{32dh. 
(6.48) 
2 See the Appendix A, Section A 17, for information about the multivariate normal, and 
Section A16 for a description of the (univariate) normal-gamma distributions. From these 
two distributions it is not difficult to discern the properties of the bivariate normal-gamma. 

NORMAL LINEAR REGRESSION 
119 
Performing the indicated integrations first over all values of {11 and {12 , and 
then over h, yields 
where 
f(ylx; b', n', s'2, v') = J 
JN<m>(ylxb', nyh)fr(hiv's'2/2, v'/2)dh 
ny = I -
x(n' + xTx)- 1xT 
= I - x(n' + n)- 1xT. 
(6.49) 
(6.50) 
In (6.49)fim> signifies an m-dimensional Student's distribution (Appendix 
A, Section A18), which is an extension tom dimensions ofthe usual Stu-
dent's distribution in a manner entirely analogous to the way in which we 
extended the normal distribution from one dimension to m. Also, the 
substitution of n = xT x is consistent with the definitions of n and x given 
in (6.33) and (6.34). 
We rarely have cause to be concerned with the joint distribution of 
observations of the Y;; in general we are more concerned with either a 
single realization or with the statistics of a set of realizations. We will 
examine the distribution of the statistics in the following. First, however, 
we will comment on (6.49) and (6.50). 
Each of the marginal densities for the j; is a Student density, and their 
individual marginal expectations are given by the prior regression line 
E(y;) = b1 + b2x2; they are not independent of one another, however. The 
m X m precision matrix for the predictions, which is proportional tony. 
will generally have nonzero off-diagonal elements. This is a reflection of 
the fact that the predictions are based on incomplete knowledge of the 
regression coefficients, and errors in judgements about the coefficients will 
have systematic effects on our judgements about the Y; themselves, de-
pending on what the predictor values happen to be. In other words, if the 
information available leads us to believe that the regression line is too high, 
or too steep, etc., then our predictions will be too high where the regression 
line is too high and too low where the line is too low. The error in one 
prediction will systematically be reflected by errors in other predictions. 
A situation of particular interest is when m = 1, i.e., when we are 
making a prediction of a single realization ofthe regressive data-generating 
process. The observed predictor matrix is simply x = ( 1, x2f, where x2 
(without a second subscript) is the single observation of the predictor vari-
able corresponding to which we are predicting a value of y. The identity 
matrix in (6.50) is in general an m X m matrix, but now m = 1, I= 1 is a 
scalar as is ny, which can be written for this limited situation as ny. With 
some manipulation it is possible to demonstrate that 

120 
CHAPTER 6 
_ 
1 
x2 - n 12 n 11 
[ 
1 
( 
, I , )2]-1 
ny -
+ -,- + , 
•2 
, 
• 
nu 
n22- n12lnu 
Then, with (6.49) we can write 
f<.Plx2; b', n', s'2, v') 
_ 
{ ~ 
[ 
1 
(x2 - n'12ln'u)2]-l/ ,2 
) 
- fs Yllfl + lf2X2, 
1 + -,- + , 
t2 I , 
s ' ,; ' 
nu 
n22-n12n11 
(6.51) 
where n'11 , n'12 and ni2 are the elements ofthe matrix n'. Ifthe prediction 
is being made posterior to a set of n observations, and the preceding prior 
was vague, then the values for the n' matrix are just (6.33) n'11 = n, n'12 
= ~ x2; and ni2 = ~ x2/. This means that the quantity 
with 
(6.52) 
will have a !-distribution with,; degrees of freedom. Clearly the predictand's 
expected value depends on the predictor variable: it is the ordinate of the 
estimated regression line. Now, however, we find that the variance of the 
prediction also depends on the value of the predictor. The prediction's 
variance is s'2 Wv'l(v'- 2). Looking at (6.52), the variance of the prediction 
has three components: 
1) s'2[v'l(v'- 2)] is the variance we would associate with a prediction 
of a normal random variable if the mean were known with certainty, but 
the variance had to be estimated with v' degrees of freedom. 
2) The term 11n introduces a contribution to the variance due to 
uncertainty in the position of the regression line in the y-direction. It rep-
resents the uncertainty implicit in estimating 11 by the average y of n ob-
servations. 
3) The final term in (6.52), (x2- X2)21L. (x2i- x2)2, is the contribution 
to the variance due to uncertainty in the slope of the regression line, i.e., 
uncertainty due to possible rotation of the line about (x2 , y). The more 
the current value of the predictor, x2 , differs from the mean value of the 
group of observations on the basis of which the regression coefficients were 
estimated, the more serious is this source of uncertainty. Predictions are 
especially hazardous for departures of x2 from. x2 that are large compared 
to the root-mean-square deviation of x2 from its mean in the previous 
observations. 
Now we shall turn our attention to predictions of future statistics 
(specifically future sufficient statistics) and their joint, conditional, and 

NORMAL LINEAR REGRESSION 
121 
marginal distributions. Within the framework of the data-generating model 
for simple linear regression and the conjugate form for expressing our 
beliefs about the parameters of the process, the formal derivation of the 
predictive distribution requires that we integrate over all possible values 
of the parameters: 
ffb- -2lbl 
I 
12 
I. 
) 
J\ , s 
, n, s , v, n, v 
= J J J f(b, S12l/:1, h; n, v) Xf({J, hlb1, n1, S12, V1)df31d{32dh 
= J J J N2>(bl/:1, nh)f-/s2ivh/2, v/2)fN(2)(filb1, n1h) 
Xfr(hiv1S12/2, v1/2)d{31d/32dh. 
(6.53) 
When this integration is carried out, the result can be written either in the 
form f(b)f(s2lb) or as f(s2)/(bls2). The former gives the marginal density 
for b and the conditional density for §'2 given that b is known, and the 
latter gives the marginal density for §'2 and the conditional predictive density 
for b once s2 has been determined. Written in the first form the result is 
ffb-
-2lbl 
I 
12 
I. 
) 
J, , s 
, n , s , v, n, v 
= JP>(blb1, nu/S12, v1) X./i132[s2lv/2, (v1 + 2)/2, (v1S12 + B)/v], 
(6.54) 
where B = (b - b1f n0(b - b1) and nu = ( n1- 1 + n - 1 )- 1• With the help of 
(Al3.6), (6.54) can be rewritten in a slightly more convenient form as 
/(b, s2lh1, n1, S12, v1; n, v) 
ex: JP>(blb1, nu/S12, v1) xJJ 1 ~~s
2 
lv, V1 + 2). 
(6.55) 
~vs +B 
Note the similarity of(6.54) and (5.33), and of(6.55) and (5.35). The 
first factor in (6.55) is the marginal predictive distribution for b. The ex-
pected values of the statistics which are the estimates (the least squares as 
well as the maximum likelihood estimates) of the regression coefficients 
are the same as the expectations that had been assigned to the true values 
of the coefficients. The variance associated with these predictions of the 
statistics is 
vi 
v(blb1 n1 S12 v1· n v) = -- S12n - 1 
' 
' 
' 
' ' 
v'- 2 
u 
(6.56) 
This result should be compared with (5.37). The term involving n- 1 rep-
resents the variance that would be associated with a finite sample from the 

122 
CHAPTER 6 
regression process if the true values of the regression coefficients were known 
with certainty. The term involving n'- 1 is the contribution to the variance 
due to uncertainty in the values of the coefficients. Whether or not the 
estimates of the individual coefficients are uncorrelated depends on the 
off-diagonal elements ofnu. Only ifthese are zero will the estimates ofthe 
two coefficients be statistically independent; this would be an unusual cir-
cumstance. 
The second factor in (6.54) is the conditional density of the residual 
variance within the sample, i.e., L 'E2/v, when the sample estimates of the 
coefficients are known. From (6.54) or (6.55) we can determine the con-
ditional expectation of this residual variance to be 
E(s-2lb', n', s'2, v'; n, v; b) = s'2 + B/v'. 
(6.57) 
Thus we expect the variance estimate based on the sample to be at least 
as large as s'2 (since B is a quadratic form and cannot be negative). It will 
increasingly tend to exceed s'2 the more the statistics b differ from the pre-
data expectation of the coefficients. 
The other form for writing the joint predictive distribution of the 
statistics is [cf. (5.33)] 
f(b, rib', n', s'2, v'; n, v) 
oc Js<2l(blb', nu/W, v + v'} X fi{J2(s 2lv/2, v'/2, v's'2/v), 
(6.58) 
which may also be written as 
f(b, s-2lb', n', s'2, v'; n, v) 
oc Js<2>(blb', nu/W. v + v') X fF (;,:I v, v') , 
(6.59) 
where W = (v's'2 + vs 2)/(v' + v). The second factor in (6.59) is the marginal 
predictive distribution for s 2; this implies that the marginal expectation 
for s 2 is 
v' 
E(s-2lb' n' s'2 v'· n v) = -- s'2 
' 
' 
' 
' ' 
v'- 2 
' 
(6.60) 
which is identical to the result obtained for the marginal predictive expec-
tation of the sample variance from a normal data-generating process. 
The first factor in (6.55) is the conditional density of b when s 2 is 
known. This is not likely to be of much practical value since it is unlikely 
that we would have determined s 2 without at the same time having cal-
culated b. 
In discussing the predictive distribution of the statistics band s 2, we 
have implicitly assumed that 

NORMAL LINEAR REGRESSION 
123 
exists. This requires that there are at least two distinct values of x2; on the 
basis of which band s 2 are to be calculated. Unless this condition holds, 
band s 2 do not exist, and their predictive distributions, which involve n- 1, 
do not exist. Nevertheless, the data could still be used to revise current 
beliefs about {J and h. 
6.6 
AN EXAMPLE: NORMAL SIMPLE LINEAR REGRESSION 
As an example of the application of simple linear regression to pre-
diction, we will consider the prediction from one month to the next of the 
areal extent of snow cover over North America, as determined from satellite 
images. It is anticipated that the snow cover in one of the earlier winter 
months, say December, should be a useful predictor of the snow cover in 
subsequent months. Satellite determinations of Northern Hemisphere snow 
cover are available only since 1967, so the data available for defining reliable 
regression and error parameters are limited. It is reasonable that we should 
want to be updating continually the parameters we use for making pre-
dictions as each new year's data become available. 
We will pursue this example in several stages. First we will consider 
predictions of February snow cover on the basis of January snow cover, 
and then January snow cover on the basis of December snow cover. We 
will attempt to assign prior parameters as they might have been assigned 
before the first year of data became available (or, equivalently, by an analyst 
to whom the satellite data since 1966 have not been available). This is not 
a simple task. We know of no simple, straightforward methods by which 
to fix the several prior parameters for a regression process. It is necessary 
to use our best judgement and then to examine the results and assure 
ourselves that the numbers assigned do indeed correspond with our beliefs. 
After assigning prior parameters we will then follow the sequence of 
making predictions, revising the parameters on the basis of subsequent 
observations, and making further predictions, on the basis of all available 
information, about snow cover in the months to follow. We will contrast 
those posterior parameters and predictions with those that would have 
resulted if the prior beliefs had been uninformative, i.e., vague or diffuse. 
The rationale that the snow cover in one month will be a useful pre-
dictor of the cover in the subsequent month consists of several arguments. 
First it is supposed that snow cover is a persistent phenomenon; ground 
covered by snow early in the winter will tend to remain snow covered 
during later stages of that winter. Second, the weather patterns that resulted 

124 
CHAPTER 6 
in the snowfall, or the lack thereof, reflect synoptic and long-wave atmo-
spheric wind patterns that change slowly and are likely to continue to 
produce similar weather patterns. Third, the snow-albedo feedback mech-
anism should tend to keep surface temperatures cool where snow is present, 
and warmer where bare ground is exposed, thus helping to maintain the 
pre-existing pattern of snow cover. Due to these and perhaps other mech-
anisms, it seems reasonable to expect not only that one month's snow 
cover is a useful predictor of the next month's snow cover, but also that 
the slope of the regression line is positive. 
In the present example, let us write xi, Yi and zi for the snow cover in 
December, January and February, respectively of the ith season (i = 1 
corresponds to the 1966-67 winter season; hence i = 17 represents the 
1982-83 season). We believe, on the basis of less systematic estimates of 
global snow cover, that the typical midwinter areal extent of snow cover 
is about 17 X 106 km2, with the maximum occurring in either late January 
or early February. We are also aware of physical limitations, such as total 
land area and the extent of permanent glaciers in the Northern Hemisphere, 
that limit the extreme variability that snow cover can reasonably have. 
Some of the characteristics of our prior beliefs are that both the mean and 
the variability of snow cover in January and February are about equal; the 
December mean is only slightly less but the variability of December snow 
cover is believed to be about the same as in January and February. Also, 
when the snow cover in December or January is above or below its mean, 
we should predict a January or February snow cover about half as far above 
or below its mean value. 
We can summarize these statements by writing, for the January to 
February prediction (we will return later to the December to January pre-
diction), 
E(i;) = ~I + ~2Yi 
as the regression equation. It is believed that E(z) = E(y) and also that 
V(z) = V(Y). This leads to the conclusion that 
b'1 = E(fil) = E(i)- E(f{2)E(y) 
= ( 1 - b2)E(i). 
Also, the condition that 
E(ily) = E(i) + HY - E(y)] 
implies that b2 = 1/2. If we estimate that y = i = 17 (measured in 106 
km2), then b'1 = 8.5. These are the prior expected values of the regression 
coefficients. 

NORMAL LINEAR REGRESSION 
125 
One of the implications of equal variances for i and y is that the 
correlation coefficient r is equal to b'z (b'z = r[V(z)/V(y)]!). In other words, 
in this case r2 = 0.25 is the fraction by which the variance of the predictand 
variable is reduced when the predictor variable is available. If we now 
suppose that the expected values for the (equal) variances of the snow cover 
in each of January and February are 1.0, then we conclude, according to 
our prior beliefs, that the remaining variance about the regression line (or, 
better, our expectation of that variance) is 1.0- 0.25 = 0.75 = s'2• Thus 
we have a third prior parameter. 
The remaining prior parameters relate to how much confidence we 
have in the three we have just evaluated. The matrix n' may be interpreted 
as defining the number, mean value and dispersion of the hypothetical 
observations of the predictor variable on the basis of which the prior judge-
ment about the regression coefficients was based. If n (the scalar) is large, 
the linear combination b'1 + b'zy, which is an estimate of the mean value 
of the predictand, is relatively well known. The quantity L: y 2 -
[L: yf/n, 
in which all four components of the matrix n' appear, determines the extent 
of the interdependence of the prior knowledge of {3 1 and f3z. 
The value of v' may be thought of as representing the number of 
hypothetical observations which contributed to the evaluation of s'2• It is 
an index of our uncertainty about the value of u or h~ We will examine 
various choices of v' with regard to their implications for the credible limits 
of u. Table 6.1, listing the 80 and 90% intervals on h~ u2 and IT, was con-
structed making use of(6.38) and (5.30). Examination of the table suggests 
that the upper limits of the implied marginal prior distribution on IT re-
sponds most strongly to changes in the value of v'. We must be willing 
again to express a judgement as to our subjective probability that u > 1.5 
or other critical values. We have chosen to aver that we are quite confident 
that u < 2.0, but we will accept a 10% probability that u > 1.5. We are led 
to the choice of v' = 5. If the expert, in examining the credible intervals 
that are implied by the various choices of v', which need not necessarily 
be integers, cannot satisfy simultaneously his or her beliefs regarding upper 
and lower bounds, it may be necessary to reevaluate the choice of s'2• If 
no combination of values of s'2 and v' satisfactorily represents our views, 
it may be necessary to find another way of stating the prior distribution, 
but a choice other than the normal-gamma distribution would not be con-
jugate and would involve extensive calculations. 
Once we are satisfied with the implications of s'2 = 0.75 and v' = 5, 
all that remains to fully define the prior distribution is the determination 
of n'. We will examine the implications of several choices for the three 
scalar quantities that make up this 2 X 2 matrix. To start this process let 
us pretend that the estimates of the regression coefficients were based on 

126 
CHAPTER 6 
TABLE 6.1. Implications of choice of v' on credible intenals for ii, ii1 and ii. 
ho .• 
ho .• 
O'Q.I 2 
ao.9 2 
v' 
XO.I 
2 
Xo.9 
2 
= xo., 2/rls2 
= xo . .'lv's2 
= 1/ho .• 
= 1/ho .• 
O"o.t 
O'Q.9 
3 
0.584 
6.25 
0.260 
2.73 
0.36 
3.85 
0.60 
1.97 
4 
1.06 
7.78 
0.353 
2.59 
0.39 
2.83 
0.62 
1.68 
5 
1.61 
9.24 
0.430 
2.46 
0.41 
2.33 
0.64 
1.53 
6 
2.20 
10.64 
0.489 
2.36 
0.42 
2.05 
0.65 
1.43 
7 
2.83 
12.02 
0.539 
2.29 
0.44 
1.86 
0.66 
1.36 
8 
3.49 
13.36 
0.582 
2.23 
0.46 
1.72 
0.67 
1.31 
9 
4.17 
14.68 
0.618 
2.17 
0.46 
1.62 
0.68 
1.27 
10 
4.87 
15.99 
0.649 
2.13 
0.47 
1.54 
0.68 
1.24 
ho.os 
ho .• , 
ao.os 2 
uo.9s 
2 
Xo.os 
2 
Xo.9s 2 
= Xo.os 2/riS' 
= xo .• s'!v's2 
= 1/ho .• , 
= 1/ho.os 
ao.os 
O'Q.95 
0.35 
7.81 
0.156 
3.47 
0.288 
6.41 
0.54 
2.53 
4 
0.71 
9.49 
0.237 
3.16 
0.316 
4.23 
0.56 
2.06 
5 
1.15 
11.07 
0.305 
2.95 
0.339 
3.28 
0.58 
1.81 
6 
1.64 
12.59 
0.364 
2.80 
0.357 
2.74 
0.60 
1.66 
7 
2.17 
14.07 
0.413 
2.68 
0.373 
2.42 
0.61 
1.56 
8 
2.73 
15.51 
0.455 
2.59 
0.387 
2.20 
0.62 
1.48 
9 
3.33 
16.92 
0.492 
2.51 
0.398 
2.03 
0.63 
1.43 
10 
3.94 
18.31 
0.525 
2.44 
0.410 
1.90 
0.64 
1.38 
three observations, with values of y given by 16, 17 and 18. Then [see 
(6.33)] 
51 ) 
869 . 
Now we can proceed to examine the implications of these parameter values, 
retaining the option, if we discover some inconsistency with our beliefs, 
of changing them. 
First let us consider the separate marginal distributions of {3, and 
{32 • These are Student distributions [see (6.38) and (5.23)] with five degrees 
of freedom and expected values, respectively, of 8.5 and 0.5. Since the 
inverse of n' is 
the following quantities have standardized !-distributions with five degrees 
of freedom: 

NORMAL LINEAR REGRESSION 
127 
/[869 
]! 
({jl - bl) 6 (0.75) 
= ({jl - bl)/10.4, 
({j2- b2)/[0.75/2]! = ({j2- b2)/0.61. 
The 90% credible interval for a t-variate with five degrees of freedom is 
-2.01 to + 2.0 1. This implies a belief that the marginal 90% credible in-
tervals for {j 1 and {j2 are 8.5 ± 21.0 and 0.5 ± 1.2, respectively. 
These intervals, we will declare, are too large by a factor of 2 or 3 to 
be acceptable in terms· of our prior belief as to what values should be 
considered credible. Recall that we were quite confident that the regression 
slope was positive, and this is inconsistent with an implied 90% credible 
interval of ( -0.7, 1. 7) on {j2 • In this case we note also that the upper limit 
is similarly too large in terms of our prior beliefs. 
To remedy this deficiency we could revise v', s'2, or both, but we have 
already declared that we are satisfied with the implied probability statements 
about h, and quite substantial changes would be needed to effect a change 
in the credible intervals on {j 1 and {j2 as large as we are seeking. Instead, 
therefore, we will restate the prior parameter n'. Increasing each element 
of the matrix by a factor of 4 would be the same as dividing each element 
ofn'- 1 by 4 and would indeed decrease the size of the credible intervals by 
a factor of 2. This would be equivalent to increasing the number of hy-
pothetical prior observations by 4, but retaining equal frequencies of values 
ofthe predictor variable at 16, 17 and 18. 
We can also decrease the width of the credible intervals on /f1 and 
/f2 by increasing ni2 while holding the other elements of n' fixed. (This too 
can be understood in terms of the imaginary observations of y that are 
used to fix n'. The more the hypothetical data points are spread out along 
the predictor axis, which can increase ni2 without affecting n'12 or n'11 , the 
larger is n'11ni2- (n!2)2 and the better determined is the slope of the regres-
sion line. As n'11 becomes larger, so the value of the regression line at 
y = n'12/n'11 becomes better known.) We will pursue both courses, increasing 
all values ofn' and just increasing n22 in order to examine the implications 
of each. 
Let us first take 
I 
(12 
204 ) 
n = 204 
3476 ' 
which is just four times the original matrix. We now have 
( 
3476 
204) 
n'_ 1 = 
96 
96 
204 
12 
' 
---
96 
96 

128 
CHAPTER 6 
which is one-fourth the original inverse. The implied 90% credible interval 
on {j 1 is now 
8.5 ± 2.015 X [0.75 X 3476/96P12 = 8.5 ± 10.5, 
while the interval on {j2 is 
0.5 ± 2.015 X [0.75 X 12/96P12 = 0.5 ± 0.67. 
As anticipated, these intervals are just one half the previous set. This is 
because they are based on a hypothetical data set that is precisely four 
times as numerous and as informative as the previous set. 
As an alternative, let us consider retaining n'11 = 3 and nb = 51, but 
trying n22 = 900. We then find that the 90% credible intervals on b1 and 
b2 , respectively, are 
8.5 ± 2.015 X [0.75 X 900/97P12 = 8.5 ± 5.32, 
0.5 ± 2.015 X [0.75 X 3/97P12 = 0.5 ± 0.31. 
Thus without increasing the number of hypothetical prior observations 
(3), we can very effectively reduce the implied uncertainty in the regression 
parameters (here by a factor of 4 ). Below we give five possible candidates 
for designation as n', including the three already introduced. We will try 
to choose among them, or introduce others on the basis of what they imply 
about our beliefs and the acceptability of these implications. 
The candidate matrices will be: 
n' = Gl 
51 ) 
869 ' 
(A) 
n' = G1 
51 ) 
875 ' 
(B) 
n' = G1 
51 ) 
900 ' 
(C) 
n' = c2 
204 
204 ) 
3476 ' 
(D) 
n'= e8 
816 
816 
) 
13904 . 
(E) 

NORMAL LINEAR REGRESSION 
129 
These were selected to allow us to examine priors whose fiducial intervals 
on the regression coefficients were half and a quarter of the interval implied 
by (A), the original parameter chosen. In (B) and (C) this was accomplished 
by increasing only n22 ; in (D) and (E) all the elements of the matrix were 
increased by the factors 4 and 16. 
(Here n'11 can take on any positive value. Nevertheless, we use integers 
both for convenience and because we do not wish to ascribe unwarranted 
precision to our prior beliefs. n'12/n'11 is the average value of the hypothetical 
prior observations of the predictor variable, which in the present case is 
17; once n'11 is given, n'12 is known. The only restriction on n22 is that it 
be greater than n't2/n'11 in order that the matrix n' be positive definite. 
In effect, once E(y) is given, there remain only two adjustable quantities 
inn'.) 
Even though the effects on the marginal prior densities on {j1 and {j2 
of changing n22 or all the elements of the matrix by the same factor may 
be quite similar, there are actually quite different implications for the im-
plied prior statement about the regression line. These differences are explicit 
in the details of the joint distribution of {i1 and {i2 , but in this case they 
are quite difficult to interpret from that perspective. Part 1 of Table 6.2 
details some of these differences. It is not apparent, in terms of the table, 
how to make any further judgements as to which n' to select. In a different 
application such a display may prove more useful. 
The differences among the priors become more discernible if we ex-
amine instead the responses to hypothetical future observations that a par-
ticular choice of prior parameters implies. Let us imagine that we are able 
to obtain two observations of the predictor y and the corresponding predict-
and z. For our hypothetical exercise we can choose the values of y and z 
arbitrarily, but it will be most informative if we select values that are 1) 
far enough off the prior expected regression line that a noticeable revision 
of our prior beliefs would be called for, but 2) not so far from the regression 
line that they appear, a priori, entirely unreasonable. (It is difficult to make 
rational decisions about the implications offacts that we do not really want 
to accept as facts.) Arbitrarily, then, let us imagine that we obtain two 
observations, (y = 15, z = 15) and (y = 19, z = 15). For each of the 
following five candidate values of n' we will calculate the posterior param-
eters according to (6.38)-(6.41). Part 2 of Table 6.2 lists several relevant 
measures of the distributions posterior to the hypothetical observation. 
Figure 6.3 shows the original expected regression line and the posterior 
expected regression lines for each of the five priors. 
In every case the two new data points cause us to reduce the slope of 
the regression line and also decrease the value of the predictand that we 
associate with E(y). This is because the two "observations" define a hori-

130 
CHAPTER 6 
TABLE 6.2. Measures of the prior and posterior distributions corresponding to 
selected choices of the prior parameter t( (other prior parameters are b'1 = 8.5, 
111 = 0.5, s11 = 0.75, v = 5). 
Prior 
(A) 
(B) 
(C) 
(D) 
(E) 
Part I: Characteristics of the prior distributions: 
90% fiducial 
interval on (J 1 -12.50/29.50 
-2.04/19.04 
3.24/13.76 
-2.00/19.00 
3.25/13.75 
90% fiducial 
interval on (J2 
-0.734/1.734 
-0.117/1.117 
0.196/0.804 
-0.117/1.117 
0.192/0.808 
SO% predictive 
interval given 
y= E(y) 
±0.727 
±0.727 
±0.727 
±0.655 
±0.636 
SO% predictive 
interval given 
y = E(y) ± 2. 
±1.149 
±0.852 
±0.759 
±0.792 
±0.674 
Marginal 
distribution 
of (J1 and (J2 : 
Major axis• 
24.809 
12.37 
6.165 
12.405 
6.202 
Minor axis• 
0.0692 
0.689 
0.680 
0.0346 
0.0266 
Part 2: Posterior distribution after hypothetical observations at (I 5, I 5) and at ( 15, 19): 
E((J.) 
14.50 
11.95 
9.36 
12.46 
10.12 
E(fJ2l 
0.100 
0.250 
0.402 
0.250 
0.400 
90% fiducial 
interval on (J1 
2.94/26.06 
2.49/21.41 
3.21/15.51 
2.07/22.86 
3.16/17.08 
90% fiducial 
interval on {12 
-0.578/0.778 
-0.303/0.803 
0.046/0.759 
-0.360/0.860 
-0.001/0.809 
$" = E(u") 
1.28 
1.36 
1.45 
1.66 
1.86 
• Lengths of the major and minor axes of the ellipses defining the 50% credible region in (J1-(J2 space. 
In all cases the axes of the ellipses are inclined to the coordinate axes by between 3.2° and 3.6°. 
zontalline at y = 15, well below the prior expected mean value of z (given 
y = 1 7) which is 1 7. The "new" regression lines are in effect a weighted 
mean ofthe prior and the line describing the "observations." The amounts 
by which the slope is changed and the line is translated depend on the prior 
parameters. Note that the slopes of the posterior regression lines for cases 
(B) and (D) are the same, as are those for (C) and (E), but the larger values 
of n'11 in (D) and (E) imply that we would be less influenced to alter the 
value of a prediction, for any value of y, because of only two new data 
points. 
Another interesting distinguishing feature among the various priors 
is the implication for altering our expectation about the variance of the 
process. As we can see from Table 6.2, in all cases our expectations con-
cerning the process variance are increased because of the two "observations" 
that fall so far from the regression line. However, in some cases (small 
n'11) we tend to allow the regression line to move nearer the new points, 

NORMAL LINEAR REGRESSION 
131 
and therefore a smaller alteration is required to the value we assign to the 
process variance. When n' is such that we imply that we have more con-
fidence in the position of the regression line, then outlying observations 
will necessarily suggest to us that the variance of the process is larger than 
we had earlier thought [see especially case (B)]. 
This information allows us one additional opportunity to distinguish 
among the alternatives and to make reasonable judgements (although cer-
tainly not uniquely defensible decisions) about what n' to choose. For ex-
ample, the regression line labeled "A" in Fig. 6.3 is too much influenced 
by the two "data"; it would be rejected at this stage if it had not already 
been rejected. We also reject "E" as discounting too much the two hypo-
thetical observations. Let us decide that (B) best represents our beliefs. 
With our prior information fully codified, we can now examine data 
that have accumulated over 17 years, and ask how our judgements are 
modified, what predictions we would now make, and how important was 
our particular choice of prior parameters. The data we will be working 
with are listed in Table 6.3. 
20,-------~--------------------~ 
:::E 
19 
~ 
a 
(/) 
:::E 
18 
prior 
(A) 
(B) 
(C) 
(D) 
(E) 
..... -············: 
0::: 
. ,.;............ 
-- --;:;:-/' 
~ 17 
;.-;.··:...--- .......-
-
-
0 
....... ~-:.:-:.: -- --
...........:: ~ 
---
u 
~-
,&.. 
-
~ 16 
-
-
-::-~-~ 
.,;::; 
z 
-::::-
····· . 
- ;::; 
(/) 
.. -····..:.-.- -.......-
... -· 
.......-
>-
;....._ . 
/"' 
0:: 
15 
/"' /"' 
+ 
; 
/ 
Ci: 
14 
+ 
JANUARY SNOWCOVER (M SO KMJ 
FIG. 6.3. The regression line implied by the prior beliefs, two hypothetical observations 
(crosses), and five alternative posterior regression lines corresponding to different selections 
of n', a matrix of prior parameters that expresses our confidence in the selection of the coefficients 
of the regression line. 

132 
CHAPTER 6 
TABLE 6.3. North American snowcover (units: 106 km1 ) 
Index 
I 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
Winter 
season 
1966-67 
1967-68 
1968-69 
1969-70 
1970-71 
1971-72 
1972-73 
1973-74 
1974-75 
1975-76 
1976-77 
1977-78 
1978-79 
1979-80 
1980-81 
1981-82 
1982-83 
Dec 
x, 
15.1 
15.1 
16.4 
14.7 
16.0 
15.9 
17.2 
16.6 
16.2 
16.2 
15.3 
15.9 
17.4 
14.4 
14.3 
16.5 
16.2 
Jan 
y, 
17.0 
17.3 
17.4 
17.3 
16.6 
16.8 
16.9 
16.8 
17.0 
16.6 
17.6 
18.2 
18.3 
17.0 
15.8 
18.2 
17.0 
Feb 
z, 
16.9 
15.1 
17.0 
15.6 
16.7 
16.6 
17.0 
16.5 
17.2 
16.6 
15.5 
18.9 
18.8 
18.6 
15.4 
17.6 
16.7 
We will consider the data as they accumulated year by year and follow 
the changes in judgement as influenced by successive observations. Table 
6.4 lists the sequences of observations, prediction, prediction errors, stan-
dard errors of prediction, and the posterior estimates of the regression 
TABLE 6.4. Year-by-year sequence of predictions with informative and noninformative 
priors, January to February 
Informative prior (B) 
Year 
y 
z, 
a% 
Zc- Z 
1967 
17.0 
16.9 
17.00 
1.29 
+0.10 
1968 
17.3 
15.1 
17.12 
1.09 
+2.02 
1969 
17.4 
17.0 
16.75 
1.30 
-0.25 
1970 
17.3 
15.6 
16.75 
1.17 
+1.15 
1971 
16.6 
16.7 
16.28 
1.17 
-0.42 
1972 
16.8 
16.6 
16.43 
1.09 
-0.17 
1973 
16.9 
17.0 
16.49 
1.02 
-0.51 
1974 
16.8 
16.5 
16.50 
0.98 
0.00 
1975 
17.0 
17.2 
16.58 
0.92 
-0.62 
1976 
16.6 
16.6 
16.47 
0.91 
-0.13 
1977 
17.6 
15.5 
16.87 
0.88 
+1.37 
1978 
18.2 
18.9 
16.90 
0.96 
-2.00 
1979 
18.3 
18.8 
17.31 
1.06 
-1.48 
1980 
17.0 
18.6 
16.72 
1.02 
-1.88 
1981 
15.8 
15.4 
16.09 
1.17 
+0.69 
1982 
18.2 
17.6 
17.62 
1.11 
+0.02 
1983 
17.0 
16.7 
16.79 
1.04 
+0.09 
Vague prior 
b, 
b, 
z, 
8.47 
+0.500 
9.10 
+0.440 
8.99 
+0.448 
9.20 
+0.426 
16.25 
+0.65 
9.66 
+0.403 
17.16 
+0.46 
9.76 
+0.398 
16.58 
1.90 
-O.Q2 
9.94 
+0.390 
16.48 
1.24 
-0.52 
9.94 
+0.390 
16.69 
1.07 
+0.19 
10.01 
+0.390 
16.44 
0.88 
-0.76 
10.11 
+0.384 
16.96 
0.96 
+0.36 
11.49 
+0.297 
15.95 
0.96 
+0.45 
8.0 I 
+0.509 
15.09 
1.08 
-3.81 
5.66 
+0.651 
17.33 
1.42 
-1.47 
6.22 
+0.624 
16.58 
1.15 
-2.02 
5.06 
+0.690 
15.64 
1.54 
+0.24 
5.09 
+0.688 
17.90 
1.31 
+0.30 
5.07 
+0.689 
16.71 
1.15 
+0.01 
b, 
36.88 
42.98 
33.30 
33.49 
35.90 
34.70 
35.03 
31.96 
35.87 
6.49 
-0.95 
1.08 
0.10 
0.87 
0.86 
-1.192 
-1.556 
-0.995 
-1.006 
-1.144 
-1.074 
-1.089 
-o.910 
-1.142 
+0.593 
+1.031 
+0.921 
+0.978 
+0.932 
+0.932 

NORMAL LINEAR REGRESSION 
133 
coefficients. These sequences are shown for two initial priors: 1) the prior 
(B), and 2) a vague prior. With a vague initial prior the estimates of the 
regression coefficients are the conventional least-squares estimates based 
on data accumulated to that point. Of course, with the vague initial prior 
some quantities cannot be estimated until enough observations become 
available. 
Shown in Figs. 6.4-6.7 are the posterior regression lines after 4, 8, 12 
and 17 observations. Also shown are the prior regression line, the obser-
20 
20 
i 
19 
19 
y: 
a 
(f) 
18 
~ 18 
------
/ 
/ 
. 
------
0:: 
/ 
17 
.•. ·.:;·...,......-/ 
w 
17 
/ 
> 
/ 
/ 
0 !i 
------
. 
/ 
. 
/ 
16 
. 
/ 
~ 
16 
.. :., ........................ 
. / 
. / 
"/ 
(f) 
'/ 
. 
/ 
,.... 
>-
. 
/ 
.... 
15 . 
/ 
15 
'a: 
/ 
ffi 
14 
14 
w 
u. 
113 
14 
15 
16 
17 
18 
19 
20 
113 
14 
15 
16 
17 
18 
19 
20 
FIG. 6.4. 
FIG. 6.5. 
20 
20 
19 
i 
19 
y: 
a 
/ 
/, 
18 
(f) 
/' 
,.... 
18 
/•. 
"" 
~ 
/' 
y 
17 
.. . 
0:: 
. . 
. •.; 
w 
17 
,+.; 
, ':j: A 
> 
0 u 
/ 
~ 
16 
/ 
/ 
.. 
/ 
(f) 
/ 
>-
/ 
0:: 
15 
/ 
<r 
/ 
::J 
/ 
Si 
/ 
14 
w 
14 
u. 
11a 
14 
IS 
16 
17 
18 
19 
20 
1313 
14 
15 
16 
17 
18 
19 
20 
JANJARY SNJWCOVER IM SO KM) 
JANUARY SNOWCOVER IM SO KMJ 
FIG. 6.6. 
FIG. 6.7. 
FIGS. 6.4-6. 7. Successive stages, as the data accumulate, of the posterior expected regression 
Jines (dashed lines). The dotted line in each is the original prior regression line. Also shown 
are the observations (the first four in Fig. 6.4, the first eight in Fig. 6.5, the first 12 in Fig. 6.6, 
and the first 17 in Fig. 6. 7). The solid Jines are conventional least-squares fits to the observations 
shown, and also represent posterior expected regression lines if the original prior is vague. 

134 
CHAPTER 6 
vations that have influenced the judgement, and the posterior regression 
line that would have been adopted if the original prior probability statement 
had been completely vague. This latter line is simply the least-squares fit 
to the observations. 
With a vague prior no prediction is possible initially. Although pre-
dictions can be made for n > 2, the second moments associated with such 
predictions are unbounded until n > 4 (i.e., 11 > 2). 
When the data are few, judgements based solely on these data can be 
deceptive. After four observations, the least-squares estimate of the slope 
is -1.8. The same data, used in conjunction with an informative prior 
[prior (B)], imply a regression line with a slope of +0.43. Note that this 
slope is much closer to the prior value for the slope (0.5) than to the least-
squares estimate (which is b2) despite the fact that the prior is based on the 
equivalent of three observations (n'11 = 3) and this was combined with four 
actual observations (n = 4). Figure 6.4 can help to explain this difference. 
It shows the prior regression line and the two posterior regression lines 
after observing four pairs of snowcover data. Also shown are the four ob-
servations. What can be seen now (but is not obvious in Table 6.4) is that, 
first, a strong correlation between y and z is not evident in these few data, 
and second, the observations cover a relatively narrow range of the predictor 
variable (y) and therefore contain very little information about the slope. 
Given these particular observations, the prior information about the slope 
remains quite significant. 
Figures 6.5-6.7 illustrate the accumulation of data and the deduced 
posterior regression lines after 8, 12 and 17 observations. After eight ob-
servations the least-squares slope is still negative, and the slope inferred 
using the informative prior is still positive. As can be seen from Table 6.4, 
this situation persists through 1977 ( 11 years of data). The first 11 years 
of January snowcover remained in a narrow range and therefore provided 
little information about the slope. Finally, in 1978 a January snowcover 
of 18.2 X 106 km2 was observed, followed by a February value of 18.9. 
Figure 6.6 shows the strong influence that one data point has on the best 
fit regression line to the data, and why there is a sudden shift from negative 
to positive slope. With the informative initial prior, the estimated values 
of b'2 are much more stable; they are influenced by the data, but within 
reason. 
The predictions themselves, and their errors, are not noticeably dif-
ferent between the two initial priors. The ratios of prediction errors to 
standard errors of estimate of the predictions [(zc -
z)/az] for the most 
part have reasonable values. The one exception is the ratio for 1978, with 
a vague prior, where the value exceeds 3.0. It is a very rare event when 
t > 3.0 with nine degrees of freedom. 

NORMAL LINEAR REGRESSION 
135 
By the time 17 years of data have accumulated the differences between 
the predictions made with and without prior information are smaller but 
they do not vanish (see Fig. 6.7 and the last row of Table 6.4). Indeed, 
between 1977 and 1983 the least-squares estimate of the slope changed 
from -1.1 to +0.9. The 1983 slope estimate based on an informative initial 
prior is 0.7; thus even after 17 pairs of observations the initial prior pa-
rameters retain some influence. 
The exercise we have been through for the prediction of February 
snowcover given January snowcover can similarly be carried out for the 
prediction of January snowcover given December data. The arguments 
leading to the selection of prior parameters are almost identical. Since the 
prior expected mean value for December snowcover is 16 (rather than 17) 
X 106 km2, the prior value for b'1 is 9.0 rather than 8.5. As before, the prior 
expected value of the slope is 0.5, and the prior values of s 2' and v' are 
taken as 0.75 and 5, respectively. For the selection of the matrix n', a 
similar procedure is followed. Five candidate matrices are defined: 
n' = (!8 48 ) 
770 ' 
20r-----------------------------~ 
· · · · · prior 
:L 
19 
(A) 
~ 
---
(B) 
0 
---
(C) 
Ul 
18 
----
(D) 
--
! 
17 ::;·-~-~-~---~~~~-:-~~;;;;-~~ 
16 --- -
. ..:.-
-
-
;::::. _.;;;---
~ 
------
(1') 
------
>-
~ 
::J 
z 
<( 
14 
J 
+ 
+ 
DECEMBER SNOWCOVER (M SO KMJ 
FIG. 6.8. Same as Fig. 6.3 except that January snowcover is being predicted 
on the basis of December snowcover. 
(A) 

136 
CHAPTER 6 
n' = (!8 48 ) 
776 ' 
(B) 
n' = (!8 48 ) 
801 ' 
(C) 
n' = c2 
192 
192 ) 
3080 ' 
(D) 
n'= e8 
768 
768 
) 
12320 . 
(E) 
The matrix (A), based on hypothetical prior observations at x = 15, 
16 and 17, implies unacceptably large credible intervals for {32 . Figure 6.8 
is analagous to Fig. 6.3, giving the posterior regression lines after obser-
vations at (15, 15) and (19, 15). An interesting feature of Fig. 6.8 is line 
(A), which has a negative slope! From a statistical point of view, this arises 
because we are trying to find a line that fits both three hypothetical prior 
observations whose center of mass happens to be at ( 16, 17), and two 
hypothetical observations whose center of mass is at ( 17, 15). A line through 
these two points would of course have a negative slope. 
From a mathematical point of view, b" is a weighted average ofb' (the 
prior) and b (the hypothetical statistics), where the weights are matrices 
TABLE 6.5. Year-by-year sequence of predictions with informative and noninformative 
priors, December to January. 
Informative prior (B) 
Vague prior 
Year 
X 
y 
y, 
Uy 
y,- y 
b, 
b2 
y, 
Uy 
y,- y 
b, 
b2 
66/67 
15.1 
17.0 
16.55 
1.34 
-0.45 
9.67 
+0.465 
67/68 
15.1 
17.3 
16.69 
1.12 
-0.61 
10.37 
+0.428 
68/69 
16.4 
17.4 
17.39 
1.03 
-O.oJ 
10.36 
+0.429 
14.25 
+0.192 
69/70 
14.7 
17.3 
16.66 
0.95 
-0.64 
11.32 
+0.373 
17.07 
-0.23 
15.62 
+0.106 
70/71 
16.0 
16.6 
17.29 
0.86 
+0.69 
11.58 
+0.351 
17.32 
+0.72 
18.47 
-0.087 
71/72 
15.9 
16.8 
17.16 
0.83 
+0.36 
11.65 
+0.344 
17.08 
0.73 
+0.28 
19.16 
-0.135 
72/73 
17.2 
16.9 
17.56 
0.86 
+0.66 
12.72 
+0.272 
16.84 
0.76 
-0.06 
18.88 
-0.117 
73/74 
16.6 
16.8 
17.24 
0.78 
+0.44 
13.04 
+0.249 
16.95 
0.45 
+0.15 
19.19 
-0.137 
74/75 
16.2 
17.0 
17.08 
0.74 
+0.08 
13.06 
+0.248 
16.97 
0.38 
-0.03 
19.17 
-0.135 
75/76 
16.2 
16.6 
17.07 
0.71 
+0.47 
13.16 
+0.239 
16.97 
0.33 
+0.37 
19.42 
-0.154 
76/77 
15.3 
17.6 
16.82 
0.70 
-0.78 
13.77 
+0.205 
17.07 
0.35 
-0.53 
20.33 
-0.208 
77/78 
15.9 
18.2 
17.02 
0.69 
-1.18 
13.86 
+0.204 
17.02 
0.37 
-1.18 
20.37 
-0.205 
78/79 
17.4 
18.3 
17.40 0.79 
-0.90 
12.64 
+0.283 
16.81 
0.63 
-1.49 
16.23 
+0.061 
79/80 
14.4 
17.0 
16.72 
0.79 
-0.28 
13.03 
+0.206 
17.12 
0.75 
+0.12 
15.95 
+0.078 
80/81 
14.3 
15.8 
16.75 
0.77 
+0.95 
11.87 
+0.330 
17.08 
0.69 
+1.28 
13.49 
+0.229 
81/82 
16.5 
17.6 
17.31 
0.74 
-0.89 
11.49 
+0.357 
17.27 
0.70 
-0.93 
12.79 
+0.277 
82/83 
16.2 
17.0 
17.27 
0.74 
+0.27 
11.54 
+0.353 
17.28 
0.71 
+0.28 
12.89 
+0.270 

NORMAL LINEAR REGRESSION 
137 
(Eq. 6.39). As discussed in Section 6.4, when the weights are matrices then 
the correlation structure of both the prior and the statistics will influence 
the outcome. In this case a negative slope results. 
In this case of course we reject prior (A) again. We have averred that 
we believe (or believed, in 1966) that a negative slope is so unlikely that 
only two observations such as these would not convince us to accept a 
negative slope. As before, we will select (B) as the proper codification of 
our prior beliefs. 
Table 6.5 and Figs. 6.9-6.12 are the results of accumulating the data, 
year by year, revising the parameters representing our best current judge-
20r-----------------------~ 
::E 
19 
:,:: 
&l 
18 
>-
15 
~ 
~ 14 
FIG. 6.9. 
20.-----------------------, 
i 
19 
:,:: 
til 18 
1 1~3 
~-:'":14,.........,1=-5 -7':16~--:17:-----:'=18~.,..19,.....----,!20 
DECEMBER SNOWCOVER IM SO KMl 
FIG. 6.11. 
20 r-------------------------, 
19 
16 
15 
14 
FIG. 6.10. 
20.-----------------------, 
19 
16 
15 
14 
11"""3 _...,14~-'15,..........,.,16~u.17,.........,1.,.8 --':19,........,20 
DECEMBER SI\OWCOVER I M SO KM l 
FIG. 6.12. 
FIGS. 6.9-6.12. Same as Figs. 6.4-6. 7 except that January rather than February snowcover 
is being predicted from the value of the preceeding month. 

138 
CHAPTER 6 
ment, and making predictions. Again we contrast a vague initial prior and 
the informative initial prior. 
The results are quite similar. Using the vague prior results in negative 
estimates of the slopes between the fifth and twelfth years. The estimates 
using the informative prior are less vulnerable to wide vacillations, and 
will not take on (unless the data are very persuasive) values that are deemed, 
a priori, unlikely. The winters 1977-78 and 1978-79 are difficultto predict, 
but the relationship between the observed errprs and the standard errors 
of estimate tum out to be more reasonable using the informative prior. 
After 1 7 years of observations, the differences between the two series of 
estimates and predictions is becoming small, although it has by no means 
vanished. 
Necessarily, as the number of observations increases the prior infor-
mation will have progressively less influence, and it will eventually make 
no difference whether the prior was formally vague or not. When the ob-
servations are few in number, however, prior information can be of sub-
stantial importance. 
An informative prior will not always lead to better judgements, but 
rather to judgements that are consistent with the expert's beliefs and the 
data. If the initial judgements happen to be in error then we might expect 
poorer predictions with than without the faulty "information." In these 
examples we chose priors which expressed quite strongly the belief that the 
slope of the regression line is positive, so a modest amount of early data 
indicating a negative slope was not persuasive. 

Chapter 
7 
First-Order Autoregression 
7.1 
INTRODUCfiON 
In all the situations with which we have dealt so far, we have assumed 
independence among the observations. However, it is commonplace in 
meteorological and climatological time series (e.g., monthly precipitation, 
drought indices) for successive elements to be statistically related to one 
another. Persistence and cyclical behavior are both manifestations of a lack 
of independence between observations. In the example considered in the 
previous chapter, the successive values of the random elements t; from one 
December to the next or from one January to the next were explicitly 
assumed to be independent, even though we did allow for month-to-month 
correlation. In other words, y; depended on X; but not on Y;- 1• We will 
now consider inferences involving time series in which the value of each 
member depends, in a statistical sense, on the previous value. 
We will not be dealing with the most general of situations: we will 
consider only normally distributed random elements and will deal only 
with first-order autoregression, where the influence of an earlier member 
of the series is manifested only through a one-lag correlation. Because the 
most recent observation was influenced by the preceding one, and the 
preceding observation by the one before that, they all have some bearing 
139 

140 
CHAPTER 7 
on subsequent members of the series. We will limit our attention, however, 
to series in which, conditional on knowing the value of the most recent 
member of the series, we can gain no additional knowledge about the 
forthcoming datum from any or all of the earlier data. 
Even with these restrictions, we will be unable to follow completely 
the approach that has served us so well in each of the preceding four chap-
ters. In particular, we will be unable to define a conjugate family of dis-
tributions to codify our beliefs about the process parameters. Lacking the 
advantages of a conjugate analytic solution, we will resort to numerical 
methods. Because of this difference in approach, this chapter should es-
tablish two important points. First, it should make quite clear the advantages 
we have been able to realize by employing conjugate distributions. They 
have made things much easier than they otherwise would have been. Sec-
ond, however, we will demonstrate that the availability of numerical meth-
ods and powerful computers (and today most personal computers are quite 
powerful) greatly expands the range of data-generating processes to which 
the tools of Bayesian inference can be applied. The method is by no means 
limited to the processes for which conjugate distributions exist, nor to the 
few processes discussed in this monograph. 
7.2 FIRST-ORDER NORMAL AUTOREGRESSION 
The data-generating process with which we shall deal will be defined 
by 
(7.1) 
where the 'E; are normal and independent random variables with mean zero 
and precision h = 1/u'l, and {j 1 and {j2 are unknown coefficients. In all 
there are three process parameters, {j 1, {j2 and h, and the situation very 
much resembles simple linear regression. However, there are some very 
crucial differences. Most importantly, the order (i.e., which comes first, 
second, etc.) of the dependent random variables (the y1, not the 'E;) is very 
important. 
Consider first the variance (over time; we will think of the index i as 
a sequential measure of time) of theY; when {j 1 and {32 are known. From 
(7 .1 ), since Y;- 1 and E; are uncorrelated, 
(7.2) 
We assume that the time series is stationary, meaning that the statistical 
properties of y; are independent of i. Then 
V(y;l{jl, {j2) = V(Y;-d{j~> {j2) = u'l/(1 - [jl). 
(7.3) 
Since the variance must be positive, (7.3) places a restriction on the values 

FIRST-ORDER AUTOREGRESSION 
141 
that {32 can have: lfJ21 < 1. It would be inconsistent with the assumption of 
stationarity to consider normal distributions (or any other unbounded dis-
tribution) to describe our beliefs about the value of {12 unless the standard 
deviation of the distribution is much smaller than the difference between 
the mean and either endpoint, so that we could, without any real loss of 
generality, truncate the distribution at {32 = ± l. 
Let the first observation we are able to make be given by y0 • From 
(7.1 ), 
Yo = fJ, + f12Y-1 + Eo 
= flt + fJifJt + fJ2Y-2 + Lt) +Eo 
= flt + fJ2(fJ1 + fJifJt + f12Y-3 + E-2) + E-t) + Eo 
= fJ,(l + fJ2 + fJl + ... + fJh + fJl+ 1Y-k-l + L {1/E-k. 
(7.4) 
If we replace the infinite sum 1 + {12 + fJl + · • ·, by (l -
{32)-1, its 
equivalent, also note that {J/+1, the coefficient of Y-k-t. approaches zero 
as k approaches infinity, and take advantage of the condition that E(E;) 
= 0 for all i, then we can write 
(7.5) 
Similarly, we can use the identity 1 + fJl + {124 + · · · = 1/(l -
{122), to 
note that 
(7.6) 
which is the same result as (7.3). Because the series is assumed to be sta-
tionary, (7.5) and (7.6) hold for ally;, not only y0 • Since the sum of normally 
distributed variables [theE; in (7.4)] is also a normal random variable, we 
can now write for the probability distribution of y0 : 
ftvolfJ,, f12, h)= fN[YolfJ,/(1- fJ2), (l- fJl)h]. 
(7.7) 
It is important to bear in mind that the results (7.5)-(7.7) are specifically 
unconditional with regard to the values of preceding members of the series. 
We will also note [multiply (7.1) by y_ 1 and take the expectation of 
both sides] that 
Cov(y;, Yi+t) = E(Y;Yi+t) - E(y;)E(Yi+t) 
= u2fJ2/0 - fJ/), 
and therefore the correlation coefficient between successive values of Y; is 
Pi,i+t = Cov(y;, Yi+t)/V(y;) = fJ2, 
It can also be shown that the multistep (lag) correlation coefficient falls off 

142 
CHAPTER 7 
as Pi,i+m = {32m. A time series with such an autocorrelation function, with 
{32 > 0, is often characterized as "red noise." It does not show evidence of 
any cyclical behavior. If we believe there is, or may be, cyclical behavior 
in the time series in question, then the use of (7 .1) as a model for the data-
generating process is inappropriate. Of course we may elect to study the 
sensitivity of a system to the various assumptions involved in asserting 
first-order autoregression by carrying out the analysis explicitly on the con-
dition that the suspected cyclical behavior is nil and then relaxing these 
assumptions and also pursuing the more complex analyses. 
First-order autoregression is encountered frequently in climatological 
time series. However, the methods of this chapter should be applied only 
after considerable care, or with explicit recognition that some compromises 
may be necessary. A typical example might be a sequence of monthly 
mean temperatures or pressures. Although the model of (7 .1) does not 
apply if the expectations or variances are different from month to month, 
as is most likely the case, we might attempt to overcome this difficulty by 
normalizing the series-subtracting out the mean for each month and di-
viding by the standard deviation. But this implies that the mean and vari-
ance for each month are known. In most cases some model other than 
(7 .1) should probably be used for seasonally varying series. On the other 
hand, there are many annual series of climatological data for which the 
model should be quite applicable. 
7.3 
INFERENCES AND PREDICfiONS 
From (7 .1) we know that the conditional probability density of y 1 , 
given Yo, is 
(7.8) 
Replacing the subscript 0 with that of the most recent observation (say n), 
so that ji1 becomes Yn+I• (7.8) will be used to determine the predictive 
distribution on the future observation. To make that prediction, however, 
we will first have to describe our beliefs about the parameters {31 , {32 and 
h. Specifically, the predictive distribution on Yn+I is 
We must express, as well as we can, our knowledge about the param-
eters. If we do indeed haven observations, then we are interested in using 
them to make inferences about the parameters. The joint probability density 
of Yo andy= (YI, Y2, ... , Yn) is, from (7.7) and (7.8), 

FIRST-ORDER AUTOREGRESSION 
143 
n 
= fN[Yol/1,/(1- /12), (1- f1})h] X ITfN(Y;l/1, + f12Yi-1> h) 
i~I 
+ L (y;- {1,- f12Yi-!)2}). (7.10) 
Viewed as a function of the parameters {1 1, {12 and a2, (7.10) is of course 
the likelihood function which we will use to make inferences about the 
parameters. 
There are no known analytical function of the three parameters {1 1 , 
{12 and ~that is conjugate to the likelihood function (7 .1 0). It is informative 
nevertheless to explore the results of various assumptions that we may 
make about the prior probabilities, and in particular about vague or un-
informative priors. To start this analysis we first write an explicit general 
statement of the posterior probability density as 
Jl ({1, , f12, sly, Yo) = fo(/1, , f12, a )(1 - {1})l a -(n+ 1) 
X exp[-(c/2a2)({11 - {i,f] exp(-d/2a2], 
(7.11) 
which is derived directly from (7 .10), but involves the following substitu-
tions: 
c = (1 + {12)/(1 - {12) + s,, 
d = (1/c)[nS, + S2(1 + /12)/(1 - /12)], 
s, = L [y;- y- f12(Yi-l - Y-1 )]2, 
s2 = L [y;- f12Yi-1 - (1 - f12)Yof, 
Y-1 = L YH/n, 
fi, = [(1 + f12)Yo + L (y;- f12Y;-I)]/c. 
(7 .12) 
(7 .13) 
(7 .14) 
(7 .15) 
(7 .16) 
(7 .17) 
As before, fo and Jl indicate prior and posterior probability densities on 
the indicated parameters; the summations are over i = 1, ... , n. 
Note how the posterior probability densities depend explicitly on the 
three parameters. The dependence on {1 1 in the likelihood function [that 
part of the right-hand side of(7.11) excluding the prior probability density] 
is entirely in the term involving ({1 1 -
{i, ); {i, is of course an estimate of 
{J,. It can be demonstrated, following procedures essentially identical to 

144 
CHAPTER 7 
some used in Section 5.4, that if the prior is entirely vague about rf,, i.e., 
does not depend on {j" then the posterior conditional density on {j1 (i.e., 
when {j2 and h are known) will be normal, with mean {f, and variance 
u2/c. We can also anticipate, again based on the experience and derivations 
of the preceding two chapters, that if the prior density for {f,, conditional 
on {j2 and u, is normal, then the conditional posterior distribution will be 
normal. 
Now let us examine the manner in which the likelihood function 
depends on u2 (or on its reciprocal h). The -similarity between (7 .11) and, 
say, the second line of (5.20), is strong. This suggests that the normal-
gamma family be tried as the joint conditional prior on {j1 and h, when {j2 
is given. We will thus write 
fo({f" rf2, u} = !N"Y({j" him', n', v', v')fo({j2), 
(7.18) 
where the prior parameters may or may not depend on {j2 • If they do not 
then there will be prior independence, but we recognize that this does not 
ensure posterior independence. 
It is cumbersome but not difficult to show that the introduction of 
(7 .18) as the prior into (7 .11) yields a posterior distribution in which the 
conditional joint density on {j 1 and h, given {j2 , is again normal-gamma. 
However, the marginal posterior distribution on {j2 becomes quite complex, 
even if fo({J-;) is very simple. Explicitly, 
j;({f" rf2. hin', m', v', v'; n, c, {i" d) ex: (;:)~N"(({f" him", n", v'', v''} 
where 
X (v"v'T""12(v'v'(12(1 - [jl'Jfo({f2), 
(7.19) 
n" = n' + c, 
m" = (n'm' + c{i1 )/(n' + c), 
v" = v' + n + 1, 
[ 
n~ 
J 
v" = {1/v"} v'v' + d + n" ({i1 -
m')2 
• 
(7.20) 
(7.21) 
(7.22) 
(7.23) 
The parameters of the posterior joint conditional distribution of {j1 and h 
now depend explicitly on {j2 (through c and d) even when the prior param-
eters do not depend on {j2 • This is another case of prior independence not 
assuring posterior independence. 
By integrating (7.19) over all values of {j1 and h [this is easy to do 
since the dependence on {j 1 and h in (7 .19) is entirely in the normal-gamma 
density] we can also write, for the posterior marginal distribution of {f2, 

FIRST-ORDER AUTOREGRESSION 
145 
Nifzin, c, fi1, d) a:. (n'/n")1(v"v'T""12(v'v'{12(1 - f3l)!Jo(/fz). 
(7.24) 
If there is prior independence between (32 and the other two parameters, 
i.e., the prior parameters do not depend on (32 , then some of the factors 
in (7.24) can be absorbed in the proportionality factor, and the posterior 
marginal density on /f2 becomes 
(7.25) 
By analogy with the preceding chapters, the extension of the normal-gamma 
prior to the limit of an entirely vague conditional statement about (3 1 and 
h, is to write fo(/f1 , h) a:. 1 I h. This leads to the result for the posterior joint 
density 
fi(/fJ, lfz, hin, c, {iJ, d) a:.fN·/IfJ, izl{ih c, d, n)fi(lfzin, c, d), 
(7.26) 
with the marginal posterior density on (3~ given by 
fi(f3~in, c, d) a:. c4d-n12(1 - f3l)4Jo(/fz). 
(7.27) 
Whatever the functional form selected for fo(/f2}, it appears that the 
evaluation of the posterior marginal density on (32 must be done numerically 
even though the conditional posterior densities on (3 1 and h can be defined 
analytically. Of course, in order to make predictions about future values 
of y; we need to know the joint posterior density on the parameters. Since 
this will be available to us only numerically (i.e., not analytically}, the 
predictive probabilities [Eq. (7.9)] will also have to be evaluated through 
numerical integration. 
The three forms of the posterior marginal distribution of (32 , (7 .24 ), 
(7 .25) and (7 .27), are of decreasing analytical complexity. However, from 
the point of view of carrying out the indicated numerical evaluations and 
integration with a computer, these differences in analytical complexity in-
volve only minor modifications to the necessary computer algorithm and 
program. For this reason we are able to choose a form for the conditional 
prior that best expresses our beliefs without compromising to adapt it to 
independence, if that seems inappropriate, or to slightly simpler algebra. 
In particular, there is no reason to adopt any particular artificial statement 
of a vague prior if our beliefs are not truly vague. By the same token, there 
is no compelling reason to adhere to the normal-gamma distribution for 
prior statements about the conditional joint probabilities of (3 1 and h. If it 
is convenient to do so, i.e., it provides a convenient way to express our 
view, then we can use it. But since the problem is going to be solved nu-
merically anyway, we can just as well introduce a numerical statement of 
the prior that has no particular analytical basis. Its only essential basis is 
that it is a valid statement of our beliefs. 

146 
CHAPTER 7 
It does help, nevertheless, to have available analytical forms that can 
serve as priors, such as, for example, the normal-gamma as a joint prior 
on {31 and h. For {32 , bounded in the interval ( -1, 1) a natural choice is 
the beta distribution modified so that the random variable is defined over 
the interval ( -1, 1) instead of (0, 1 ). With a simple change of variables, 
y = 2i- 1, Eq. (A4.1) of Appendix A can be rewritten 
(1 + y)a-1(1 _ y)b-a-1 
f(YJa, b) = 
2<b-I>B(a, b) 
(7.28) 
By an appropriate choice of the parameters a and b, the various shapes of 
distributions illustrated in Fig. 3.1, but over the interval ( -1, 1 ), can be 
represented. Again, the analytical form is available as a convenience but 
should be used only if we are satisfied that it faithfully codifies our beliefs. 
There are some potential pitfalls in the use of numerical methods. 
One is that we may have available more degrees of freedom for assigning 
prior probabilities than are warranted. It may be, for example, that we 
wish to assign a set of prior probabilities for {32 such that P{/32 < 0.0} = 0.1 
and P{/32 > 0.6} = 0.25. Numerically, we can find an infinite number of 
distributions that have these characteristics, but only one set1 of beta pa-
rameters a and b would satisfy this constraint. To try to determine which 
of the many plausible numerical statements of the prior is best may in-
advertently lead us to impose more prior information than we actually 
have. It may be advantageous and prudent to use the obvious analytic form 
of the density, if only to evaluate the initial numerical probabilities. 
7.4 A NUMERICAL EXAMPLE: ANNUAL STREAMFLOW 
First-order autoregression as a model is most interesting when applied 
to series in which there is good reason to expect substantial serial correlation. 
We will use as an illustration a hydrological application: the annual flow 
of a river. We would generally expect soil moisture, for example, to intro-
duce some memory into the hydrological system that will result in a positive 
correlation at a lag of one year even in the absence of any interannual 
correlation of precipitation. On the other hand, negative lag-1 autocorre-
lations may also show up-the quasi-biennial oscillation is manifested, 
although frequently weakly, in many tropospheric phenomena. 
The particular context of our application will necessarily influence 
our choice of prior distributions. For example, dealing with the flow of a 
river we can be sure that /31 is positive. If we were expressing the prior in 
terms of an analytic expression we would be very wary of a normal distri-
1 a = 6.8, b = 9.8. The implied expected value of {12 is 0.34. 

FIRST-ORDER AUTOREGRESSION 
147 
bution. Only if the implied probability of values less than zero was van-
ishingly small (perhaps less than 10-5, which translates to a ratio of mean 
to standard deviation greater than about 4: 1) should such a prior be con-
templated. Since in our example we will be expressing the probabilities 
numerically, it will be very easy to define the prior probability (and therefore 
also all posterior probabilities) of {31 < 0 to be zero exactly. 
Of more concern is the assumption in the model that the £;, and 
therefore values of Y;- {j 1 -
{j2y;_ 1, are normal. This assumption is explicit 
in (7 .II) which forms the basis of our analysis. To be satisfied with our 
model we are supposing, a priori, that the mean flow is much greater, 
measured in terms ofthe variability of the process, than zero. Specifically 
we must be confident that £(YJ[j1, {j2 , a) is sufficiently greater than [ V(YJ[j 1, 
{j2 , a)] 112• From (7.5) and (7.6) this condition reduces to 
a 2/(1 - [jl) ~ {j12/(1 - {jz)2• 
(7.29) 
Our choice of joint prior probabilities on all three parameters should reflect 
this constraint. Note that the constraint on {j1 is more severe for negative 
compared to positive values of {j2 , the lag-! autocorrelation coefficient. 
This implies that we may want to beware of the simple expedient of sup-
posing prior independence among the parameters. 
There has been considerable discussion of the applicability and ad-
vantages of Bayesian analysis in the context of annual streamflow data. In 
particular, Lenton et al. (1974) presented some data compiled by Yevjevich 
on the relative frequency of computed values of the one-year autocorrelation 
for annual flow series of substantial length. They used a beta distribution, 
fitted to the relative frequency data by the method of moments, as their 
prior distribution. Figure 7.1, taken from Lenton et a!., compares their 
beta distribution to the frequency distribution of the 140 values of p (sample 
estimates of {j1) given by Yevjevich. Lenton et al. suggest that less diffuse 
priors, compared to this example which is based on an aggregation of world-
wide data, might be obtained if the hydrologist had, and could interpret, 
additional information on the drainage characteristics of the specific basin 
in question. Certainly this is so, but in any specific example the expert may 
also have reason to doubt that this sample of world-wide data is relevant 
and may have to turn to a prior that is more diffuse. It may be of most 
interest to note that all of the 140 recorded values of p have been in the 
interval ( -0.4, 0.8). 
We will use, as a primary statement of our marginal prior on {j~, the 
beta distribution illustrated in Fig. 7 .II: 
fo(ffz) oc (I - {jz)9.89(1 + {jz)t4.so. 
(7.30) 
Now let us consider what to choose as appropriate priors for {j1 and 

148 
20 
15 
10 
5 
(!) Yf'!Vjevich Frequency Histogram 
lor p 
® Beta Distribution 
CHAPTER 7 
p(p) 
3 
FiG. 7 .I. Beta distribution (2) fit to observed relative frequencies [Yevjevich frequency 
histogram for p( I)) oflag-one autocorrelation coefficients calculated from annual streamtlows 
(after Lenton et a/., 197 4 ). 
u. We have already remarked that for this example fJ 1 must be positive. 
We can choose a reasonably symmetric distribution for fJ1 only if we ensure 
that the standard deviation of this distribution is small compared to the 
mean. We must also ensure that we exclude the joint occurrence of small 
values of fJ 1 and large values of u as prescribed by (7.29). If we want to 
limit the expected value ofyto be at least three times its standard deviation, 
then (7 .29) implies 
fJ 1 > 4.6u if fl2 = -0.4 
fJ1 > 3.0u if fJ2 = 0.0 
(7.31) 
{11 > 2.0u if fl2 = +0.4 
fJ1 > l.Ou if fJ2 = +0.8 
These constraints exist entirely because of the assumed normality of 
the E and the requirement that negative values of y be highly improbable. 
In situations in which negative values of yare plausible (if our example 
concerned the Palmer drought index, for example), such limitations might 
never arise. In the present example, however, these constraints cannot be 
ignored. In particular, conventional vague priors of the types we have been 
dealing with in the preceding chapters, suggesting uniform distributions 
and prior independence among the parameters, are not reasonable, and, 
with few data to contribute to the posterior probabilities, can lead to pre-

FIRST-ORDER AUTOREGRESSION 
149 
dictions that we would not accept. Here, of course, as in the earlier appli-
cations of the method, as the data accumulate the specifics of the prior 
become unimportant. This limitation on the use of vague priors is exac-
erbated by the requirement that the evaluation of the posterior and pre-
dictive probabilities be done by numerical integration. The numerical 
analysis is less troublesome when the integrand remains finite and takes 
on nonzero values only over a finite range of the independent variable. 
Indeed, in our example, we will treat the prior and posterior densities in 
this way. Therefore the vaguest priors we shall consider here are not vague 
in the absolute sense that we have been using in the earlier chapters. They 
are priors that mimic conventional vague priors, but only over limited 
ranges of values for the parameters in question. These are the ranges of 
parameter values that are, a priori, deemed possible. We take the prior 
probabilities to be zero outside these ranges. 
Dealing with geophysical quantities it is usually not difficult to define 
such limits. We can be sure that the mean monthly precipitation at some 
location in the eastern United States does not exceed 50 em; we can be 
sure that the mean annual flow out of some drainage basin will not exceed 
the maximum credible integrated precipitation over the basin. For the 
present example, therefore, we will accept the hydrological judgement that 
values of {31 in excess of 10 X 105 acre-feet, or values of u greater than 
5 X 105 acre-feet, can be excluded from consideration even if we are oth-
erwise considering our prior as vague. (We will be able to confirm that this 
was a reasonable decision after acquiring some data. If it then appears to 
be not reasonable we will have the option of redefining the limits of our 
numerical integration. This action is equivalent to restating a prior after 
viewing the data, and would be unacceptable were we not here representing 
our prior as vague and limiting the range of integration for the convenience 
of the numerical calculations only.) 
The initial prior we will consider in the context of this example will 
take the form 
(7.32) 
wherefo({f2) is given by (7.30),fo({f1) is a constant, andfo(u) oc 1/u, subject 
to the constraints that 0 < u < 5 X 105 and 0 < {31 < 10 X 105• The 
constraint of (7 .29) further limits the permitted values of u and is the 
reason that the last term in (7.32) indicates a dependence on {31 and {32 . 
A second prior that we will consider will assume that the hydrologist 
has some additional insight into the water basin being considered. (Perhaps 
this is a basin that has been studied considerably in the past, but only in 
the last five years have the conditions with regard to upstream diversion 
and other projects of hydrological significance been reasonably stable.) In 

150 
CHAPTER 7 
fact, his prior belief about {32 , he avers, is that its expected value is 0.0, and 
he is 95% confident that its value is between -0.3 and +0.3. He also expects 
a mean flow of 300 000 acre-feet, and is quite certain (99% probability) 
that it is less than 400 000. Finally, he expresses his views about the process 
variance by stating that he believes, because the drainage basin is in a 
relatively humid climate without a strong seasonal cycle in precipitation, 
that the coefficient of variation2 for the annual streamflow is relatively 
small. He avers that he has 80% confidence that it falls in the range 
(0.10, 0.20). 
A beta distribution would appear to serve well as a more complete 
expression of the views concerning ~~; the conditions given can be satisfied 
by writing 
fo({i2) oc 0 + ~d0.50 -
~2)20.5, 
On the other hand, it may be just as proper, and perhaps somewhat easier 
computationally, to approximate this with a normal distribution. We will 
try the alternative of writing the marginal prior on {32 as 
(7.33) 
The numerical value for the precision is chosen to ensure that 95% of the 
area under the curve, which for a normal distribution will be within± 1.960 
standard deviations of the mean, covers the interval ±0.3. 
We will also use a normal distribution to codify the stated beliefs 
about the mean flow. We interpret these as a conditional statement about 
rJ1, given ~2. Since the expected value of the flow is ~dO -
~2 ), we write 
fo[rJJ/0 -
~2)] = fN[rJJ/0 -
~2)13 X 105, (2.326/1 X 105}2], 
(7.34) 
This, in turn, allows us to write 
fo({JJ!~2) = fN[{JJi3 X 1050 -
~2), (2.326 X 10-5)20 -
~2)-2], 
(7.35) 
Finally, we consider how we might give convenient analytical expres-
sion to the stated beliefs about the coefficient of variation. The coefficient 
of variation is, of course, the ratio of the standard deviation of the flow to 
the mean. From (7.5) and (7.6), this ratio, to which we have already in 
(7.29) attributed an upper bound, is (u/~!}[0 -
~2 )/0 + ~2 )]l. We will 
interpret the condition given above, that (0.10, 0.20) is an 80% credible 
2 The condition (7.29) is a constraint on the coefficient of variation to the effect that it 
should be less than about 0.3. Iflarger values ofthe coefficient are to be considered credible 
[larger values are frequently encountered in nature (Lenton et al., 1974)], and negative values 
of yare ruled out, then some other model than (7.1), or some other assumption about the 
distribution off must be employed. One possibility is to write z = lny, and define the auto-
regressive normal process in terms of the transformed variable. 

FIRST-ORDER AUTOREGRESSION 
151 
interval, as a prior conditional statement about G-, given both f3t and (32. 
Here it is natural to follow the approach found useful in earlier chapters 
and try to codify this belief in terms of a gamma distribution, or its close 
cousin, the x2-distribution. 
Following the same procedure used in Section 5.10, we first attempt 
to determine the appropriate parameter for x2 such that the 90th and 1Oth 
percentile points of the distribution are in the ratio 0.20/0.10 = 2.0. The 
required parameter value (degrees offreedom) turns out to be only slightly 
less than 30. For a degrees of freedom value this large, the normal distri-
bution is a very reasonable approximation to x2, or the gamma. Thus here 
again we can use the normal distribution to express our prior beliefs: 
fo(alf3t, (32) = fN[ul0.l5f3t/C', (l.644c'/.05(3t)2], 
where c' = [(l + (32)!(1 - (32)]. 
The final joint prior probability densities are given by 
fo({ft, {32, a)= fo(alf3t, (32) Xfo({ftlf32) Xfo({32), 
(7.36) 
where the three factors on the right are given, respectively, by (7.33), (7.35) 
and (7.36). The use of normal densities to represent our prior densities 
appears permissible here despite the unbounded nature of the normal ran-
dom variable, because. 
1) Although the normal random variable is in principle unbounded, 
we have used parameters in the distributions such that the probability 
densities will become vanishingly small as the boundaries of the permissible 
ranges of (3 1 , (32 and a are approached. 
2) Our numerical procedures will, in any case, set probabilities to 
zero outside these boundaries, regardless of the value of the normal densities. 
3) In terms of our ability to define our beliefs, it is probably not possible 
to distinguish between the normal densities and the other densities they 
are approximating. There is no reason, a priori, to expect one to be better 
than the other. If, after looking at numerical values or graphs of the prior 
densities, or after applying some test such as examining the implications 
of hypothetical future data, we see some characteristic that conflicts with 
our views, we must change the prior. The use of the normal is intended as 
a numerical aid, not a constraint. 
7.5 
COMMENTS ON COMPUTATIONAL METHODS 
It would be out of place here to go into great detail on the numerical 
methods for the algorithms that could be employed to carry out these 
computations. In Appendix C we present a FORTRAN program that was 
used to calculate the results discussed below. The program proceeds through 

152 
CHAPTER 7 
three stages. It calculates numerically the posterior probabilities according 
to (7 .ll ), generates as output the marginal posterior probabilities and the 
marginal expectations of the three parameters, and then generates predictive 
probabilities by carrying out the integration indicated in (7.9). The main 
body of the program is quite general (except for the short section beginning 
with statement 581, explained below). The constraints and conditions of 
the present example are introduced through the selection of prior proba-
bilities. It is assumed that the posterior distributions are to be calculated 
from the original prior and the accumulated statistics. Equivalently, but 
more cumbersomely, the program could have been designed to produce 
as output the entire posterior probability density on the parameters, and 
to use this in lieu of the original prior as input to the next application. In 
that case only the incremental statistics would be needed. 
There are several items of data or other input that the program requires 
to be supplied: 
l) A value for the index IPR l which tells the program which prior to 
use (IPR l = l for the relatively vague prior called VPRIOR, and IPR l 
= 2 if the informative prior is to be used). 
2) The lower limit, upper limit and interval for the calculation of {31• 
3) The lower limit, upper limit and interval for the calculation of (32 • 
4) The lower limit and upper limit for the calculation of (J, and the 
(geometric) mean interval for calculations. The calculations are actually 
carried out over a grid that is uniform in ln(J. 
5) The observational data, consisting of n, y0 , Yn and the average 
values of Y;, y/ and Yi-1 Y;. If there is no observation at all, then n = 0 and 
Yo = Yn = 0. If a single datum Yo is available, then n = 0 and Yn = Yo =I= 0. 
The integrations are all done using Simpson's Rule, in which the def-
inite integral is approximated as 
Lb f(x)dx = (Ax/3)[f(xo) + 4f(xd + 2(fxz) + 4f(x3) 
+ • • • + 4/(Xn-3) + 2/(Xn-2) + 4/(Xn-d + f(Xn)], 
where ~x =X;- X;- 1 = (xn- x 0 )/n is the uniform interval between suc-
cessive values of x at which the function is evaluated, and the number of 
intervals n is an even number. Subroutine SMPS implements this numerical 
integration. 
As will be discussed below, for one aspect of the calculations it turned 
out to be preferable to use ln(J rather than (J as the independent variable. 
Since 

FIRST-ORDER AUTOREGRESSION 
153 
I f(u)du =I u/(u)dlnu, 
all that is required is to introduce the extra factor u at the correct places. 
Most of the calculations can be done, with identical results, either way. 
The calculation of the joint priors (subroutine VPRIOR for the more 
vague prior and subroutine PRIOR2 for the informative prior) involve 
relatively simple computer algorithms. In both cases the outermost loop 
of the algorithm is over the index that defines the values of (32 • In the less 
informative situation we calculate the kernel of the beta function according 
to (7.30) (the variable G in the program). We also calculate the factor 
depending on (32 that will be needed to define the limits over which (31 and 
u will have nonzero probabilities. 
For each value of {32 , an iteration is carried out over (3 1, and within 
that over the index that defines the values of u. The variable E is defined 
as 1/u if u is sufficiently small (7.29), and zero otherwise. F2 is defined 
from E by multiplying by u, and an integration over lnu is carried out. 
This integral is S, which used to normalize E and assure that the third 
factor in (7.32) is a proper probability density. The prior density returned 
to the main calling program is then F which is G times E/S. 
The computation of the prior in the case of the more informative 
prior is less cumbersome. Again we loop first over the index that defines 
the values of (32 ; this time we calculate the normal density given by (7 .33), 
calling it Dl. We then iterate over the values of the index that define the 
values of (31, calculate the value of (31, and multiply Dl by the normal 
density of (7.35) to get 02. Within this second loop we enter a third loop 
to get the values of u, and calculate 02 times the normal density of(7.36). 
This product, which depends on all three indices, is the desired joint density. 
Note that the main program requires only that the subroutine output 
(VPRIOR or PRIOR 1) be proportional to the intended prior densities. 
The program multiplies the prior by the likelihood, which depends on the 
data, and then normalizes the product of the prior and likelihood. (In both 
normalized and unnormalized form, before and after multiplication by 
the prior, this function is called LIKE.) 
Most of the computations are straightforward. A complication arises, 
however, in attempting to calculate the predictive probabilities when dealing 
with a vague prior. In none of the previous chapters, where we were dealing 
with analytical solutions, were we able to attempt to derive predictive prob-
abilities starting with a vague prior and no data. The integrals did not 
converge; the vague priors were not proper density functions. With the 
present numerical example, however, our "vague" prior is not entirely 

154 
CHAPTER 7 
vague. It has been defined in such a way that a prediction, in a formal 
sense, can be calci.llated; but such a calculation should not be given very 
much weight. It is based on some very arbitrary choices, and, it turns out, 
is numerically cumbersome and subject to inaccuracies if extreme care is 
not taken. 
The problem in carrying out the calculations relates to the numerical 
integration over a fixed grid of the independent variables of the product 
of two functions: one that can vary very rapidly between neighboring grid 
points, and one that takes on large values in the same region of the grid. 3 
The first is either the probability density of y, given {3 1 , {32 and u; or the 
likelihood function when n = 0 and Yo is known. For small values of u 
both functions can go through their entire range of values between grid 
points when the grid intervals (t.{3 1 and t..{32 , called Dl and 02 in the 
program) have reasonable values. The second term in the product is of 
course the prior density which, according to (7 .32), is large when u is small. 
The situation is helped somewhat by using lnu instead of u as the inde-
pendent variable, but so long as the prior is vague, and there is not at least 
one pair of observations, the finite probabilities associated with arbitrarily 
small u remain troublesome. 
In principle, the difficulty can be overcome by expanding the grid, 
but this can impose severe penalties in terms of memory requirements and 
computer time. (More efficient algorithms could help.) However, at least 
in the case of the predictive probabilities with n = 0 and y0 not observed 
(N.EQ.O.AND. YZ.EQ.O in the program), we can take advantage of the 
same attributes of the vague prior that give rise to the problem, in order 
to find an approximation that offers an effective remedy. 
The predictive probabilities we seek are 
J J J f(ylf3t. f3z, u)fo(f3t, f3z, u)df3tdf3zdu. 
When we introduce lnu as the independent variable in lieu of u, there 
appears in the integrand the product ufo({31, {32, u), which is almost invariant 
in {3 1 and u. Let us concentrate on the innermost integration: 
J 
f(ylf3t. f3z, u)ufo(f3t. f3z, u)df3t 
~ ufo(f3t, f3z, u) J 
f(ylf3t. f3z, u)d{31• 
(7.37) 
From (7. 7) we can write this last integral as 
3 The error in applying Simpson's Rule for numerical integration is proportional to (~x)5 
times the fourth derivative of the integrand. 

FIRST-ORDER AUTOREGRESSION 
155 
[(I- i3l)/27r~T2] 1 I exp{-[y- !'3t/O- /3z)f(l- /3l)/2~T2 }di3t 
= [(l -
/3l)/27r~T2]! I exp{ -[!3t - yf(l -
/3z)f/2~T2 }di3t, 
where s = IT[(! -
{32)/(1 + {32)P12. This integral needs to be evaluated over 
the range of {31 for which the prior probability density [which in (7.37) was 
taken outside the integral] is effectively constant; this range is from 0 to 
10. For values of {3 1 outside this range, the (vague) prior density is zero. In 
effect, what is needed is a calculation of the appropriate "error function"; 
thus the call to the FORTRAN function ERF in the segment of the program 
(beginning with statement 5 81) that is used only when the prior is vague 
and there is no datum at all. 
7.6 
RESULTS WHEN THE PRIOR IS RELATIVELY 
UNINFORMATIVE 
We will examine in succession three situations. First we will ask what 
the prior we have chosen implies about our beliefs before we have obtained 
any data at all. Are there any implications that were unexpected and perhaps 
contrary to some unexpressed beliefs? Does the implied prediction (of y0 ) 
seem sensible in light of the vagueness ofthe prior? We will then examine 
the implications of, first, minimal data, and then five and fifteen years 
of data. 
Figure 7.2 illustrates the implied vague marginal prior density on lniT. 
The departure of the density ofln~T from a constant, and the corresponding 
departure of the density on IT from the function l I IT (explicit but not dis-
cernible in Fig. 7.6), are due to the truncations of the conditional densities 
at appropriate values of {31 and {32 , in compliance with the conditions of 
(7.29). The marginal density on {31 is a uniform distribution, and that on 
{32 is the beta function as shown in Fig. 7 .1. (The prior densities on {31 and 
{32 are shown in Figs. 7.4 and 7.5.) 
0.3' 
~ 0.2 
-
0.1 
-------
~0.0~~·--------~·~~·~-------7'-~~~----~~· 
.01 
.05 
.I 
.5 
\, 
5. 
a (I 0 5 Acre-feet) 
FIG. 7.2. Marginal prior density function on Ina. The truncation of the function at small 
values of a (a < 0.0 I) is an important characteristic that distinguishes this numerical prior 
from the analytic vague priors on a in Chapters 5 and 6. The departure from a straight 
horizontal line for large a is an accommodation to the conditions necessary to assure that 
negative flows are not expected. 

156 
CHAPTER 7 
As already indicated, the prediction implied by a vague prior must 
not be taken too literally. We cannot expect much of a prediction with a 
prior that is not very informative, and with a complete absence of data. 
The predictive probabilities that are generated in this case are shown, nev-
ertheless, in Fig. 7.3. Consistent with the way in which the prior was defined, 
negative values of y are almost ruled out. (The implied probability that 
y0 < 0 is about 0.00 1.) On the other hand, the predictive probability density 
is almost precisely uniform over the interval 0 < y < 7. This is indeed a 
vague prediction. The calculated expected value of y is near 6.5 X 105 acre-
feet. (Hereafter the units of {j1, u andy will be taken as 105 acre-feet but 
will not be explicitly given.) This is, necessarily, the expected value of 
tf1 /( 1 - {32 ). Since the prior is such that fj 1 and {j2 are independent, this 
can be written as 
Evaluating these expectations in terms of the known moments of fj2 con-
firms that the calculation is at least approximately correct. Of course the 
prediction illustrated in Fig. 7.3 is really not a very informative one; being 
based on a quite vague prior and no data it should not be. The predictive 
standard deviation is 4.1. The artificial "vague" prior we have imposed 
appears to codify reasonably well the intended very limited prior knowledge. 
Table 7.1 list a series of observations covering a span of 16 years, from 
n = 0 to n = 15. We will examine the state of our beliefs and the predictions 
we would make after n = 1, n = 5 and n = 15. 
The single pair of observations available at n = 1 are sufficient to cause 
.10 
.08 
.06 
>-
<o--
.04 
.02 
.00 
0 
8 
12 
16 
20 
Y (IQSAcre-feet) 
FIG. 7.3. Prediction of the first datum implied by the nearly vague joint prior density 
function. The waviness in the curve at large values of y is a manifestation of the calculations 
over a finite grid. The prior is truncated at specific values of u, depending on the values of 
the other parameters, and takes the form of a step function. This leads to ledge-like results in 
some of the subsequent calculations. 

FIRST-ORDER AUTOREGRESSION 
157 
TABLE 7 .1. Observations of annual streamflow (10 acre-feet). 
n 
y 
0 
4.872 
3.018 
2 
2.621 
3 
4.462 
4 
2.986 
5 
3.050 
6 
2.613 
7 
3.985 
8 
3.957 
9 
4.100 
10 
5.585 
11 
4.560 
12 
3.502 
13 
2.586 
14 
6.571 
15 
5.518 
us to make impressive modifications in our previously vague beliefs. We 
can see from Fig. 7.4 (short-dashed curve) that we should now believe that 
{j1 is much more likely to lie in the interval from 3.0 to 5.0 say, than to be 
1.0 
.8 
.6 
.4 
.2 
2 
4 
6 
8 
10 
~ 1 (I 0 5 Acre-feet l 
FIG. 7.4. The marginal prior probability density on [:J, (dotted line, a uniform probability 
density), and the posterior marginal densities after n = I (shorter dashes), n = 5 (longer dashes) 
and n = 15 (solid line). 
2 

158 
CHAPTER 7 
greater than 6.0. Similarly, two observations in the vicinity of 4 have con-
vinced us that values of {j 1 less than 1.5 are very improbable. Of course, 
these judgements were influenced by our prior views about {j2 , which were 
not so very vague. Large negative values of {j2 together with small values 
of {j 1 could have given rise to the observed values, but our prior has excluded 
values of {j2 near -1. 
The prior and posterior distributions on {j2 are shown in Fig. 7.5. The 
single pair of observations has led us to a somewhat smaller expected value 
of {j2 (influenced by the difference between y0 and y 1, plus our views on u: 
if u is small the correlation must be small; if u is large then any {j2 is 
plausible), but our uncertainty concerning {j2 is almost unchanged. It would 
have been surprising indeed to have gained much information about what 
is really a correlation coefficient from a single pair of observations. 
The posterior marginal distribution on u is shown in Fig. 7 .6. With 
just one pair of observations we are able effectively to exclude from con-
sideration values of u less than 0.2. Before the observations became avail-
able, it was considered more likely than not that u < 0.2 (see the dotted 
curve in Fig. 7 .6). In effect the observations have allowed us to fix the scale 
of the process with which we are dealing-the prior left this wide open-
and this has allowed us to very quickly say much more about u and {j 1• 
Because the prior was vague we were able to change our expected value of 
» 
+-' 
4-
·v; 
3 -
c 
G) 
0 
» 
+-' 
..0 
(T] 
..0 
0 
L 
Q_ 
2-
I -
-.8 
-.6 
-.4 
-.2 
0.0 
62 
·\'• 
.... 
~ .... 
... 
\,. · . ,, ·. 
\\' · .. 
\'i · .. 
'\'· ..... . 
+ .2 
+.4 
+.6 
+.8 
FIG. 7.5. The marginal prior density on ~2 (dotted curve), and the posterior densities 
after n = I (shorter dashes), n = 5 (longer dashes) and n = 15 (solid line). 

FIRST-ORDER AUTOREGRESSION 
159 
4 
2 
3 
4 
5 
a (I o5 Acre-feet l 
FIG. 7.6. The marginal prior density (dotted curve) and the posterior marginal densities 
on u, when the original joint prior is relatively vague, after n = I (shorter dashes), n = 5 
(longer dashes) and n = 15 (solid line). Note that, unlike Fig. 7.2, the ordinate here is u. 
u from its prior value of 0.37 to a posterior value of 1.01 on the basis of 
only one pair of observations. Because the prior was vague, however, the 
standard deviation of our posterior distribution on u remains quite 
large: 0.36. 
The prediction of y2 is made on the basis of current beliefs about the 
parameters and the known value ofy1 • Uncertainty in the prediction arises 
in part from the expected value assigned to u, according to current beliefs, 
and in part from uncertainty in all three parameters. With the vague prior 
and n = 1, the expected value of y2 is 4.2 and its standard deviation is 
1.32. Note from Table 7.1 that the "prediction," if the expected value of 
the predictive distribution is interpreted as a point prediction, is off by 1.6, 
slightly more than one standard deviation of the predictive distribution. 
The single pair of observations has allowed us to decrease the credible 
interval of a prediction by almost a factor of 3. In effect, with one pair of 
observations and the modest amount of information in the prior, our beliefs 
about the process are no longer properly classified as vague. 
TABLE 7 .2. Posterior means and standard deviations of procesS parameters, with original 
prior relatively vague 
{J, 
f12 
11 
n 
Mean 
S.D. 
Mean 
S.D. 
Mean 
S.D. 
0 
5.042 
2.862 
0.175 
0.188 
0.373 
0.605 
I 
3.890 
1.043 
0.102 
0.188 
1.014 
0.360 
5 
3.309 
0.671 
0.081 
0.178 
0.885 
0.187 
15 
3.423 
0.615 
0.167 
0.147 
1.121 
0.142 

160 
... 
<:0. 
1.0 
.8 
.6 
.4 
.2 
I 
I 
I 
I 
I 
I 
I 
I 
I 
\ I 
I I 
\ I 
\ I 
\ I 
I I 
\ I \\ 
\\ ' 
CHAPTER 7 
· 0 o~--L---2~~~--~4--~~~6~~~~8~--~~IO 
61 (I 0 5 Acre-feet l 
FIG. 7.7. The informative marginal prior probability density on ~ 1 (dotted line), and the 
posterior densities after n = I (shorter dashes), n = 5 (longer dashes) and n = 15 (solid line). 
Figures 7.4-7.6 also give the posterior probability densities on the 
process parameters after the observations of n = 5 and n = 15, and Table 
7.2 lists the posterior moments. The expected value of /3 1 decreases no-
ticeably between n = I and n = 5 and the uncertainty in /3 1 also decreases. 
At the same time the uncertainty in u decreases even more noticeably (its 
6 
5 
r 
r 
.. I \ 
) \ \ 
. 1·. li 
I· 
I 
! \ 1\ \ 
I :I 
! ·,: \ 
! ; \ \ 
· 1 
;·· I 
.: J 
..... \ 
. , 
· .. 
4 
0 
3 
2 
0 0.0 
0.5 
1.0 
I .5 
2.0 
0 ( 10 5 Acre-feet) 
FIG. 7.8. The informative marginal prior probability density on u (dotted line), and the 
posterior densities after n = I (shorter dashes), n = 5 (longer dashes) and n = 15 (solid line). 

FIRST-ORDER AUTOREGRESSION 
161 
TABLE 7 .3. Posterior means and standard deviations on the process parameters: 
Initial prior informative. 
{J, 
{J2 
(J 
n 
Mean 
S.D. 
Mean 
S.D. 
Mean 
S.D. 
0* 
3.000 
0.632 
0.000 
0.153 
0.445 
0.111 
0** 
3.740 
0.652 
0.000 
0.153 
0.614 
0.099 
3.750 
0.615 
-0.041 
0.146 
0.611 
0.092 
3.660 
0.501 
-0.057 
0.135 
0.624 
0.081 
15 
3.705 
0.458 
0.062 
0.108 
0.851 
0.078 
* Before any observation. 
** After observation of Yo. 
standard deviation changes from 0.36 to 0.19). With n = 5, however, the 
uncertainty in (32 is almost the same as it was at n = 0 (0.178 versus 0.188). 
The uncertainty in (32 does, however, begin to decrease after n = 5, and by 
n = 15 the standard deviation of its posterior marginal distribution is 0.14 7. 
7.7 
RESULTS WHEN THE PRIOR IS INFORMATIVE 
Although the informative prior is based on conditional densities that 
are normal, the implied prior marginal densities on {31 and u, shown in 
Figs. 7. 7 and 7.8 as dotted curves, are not. Since {31 is conditionally normal 
with mean given by 3(1 -
(32 ), and since the expectation of (3~ is 0, the 
prior expected value of {31 is 3. The prior variance of {31 is 0.6322 = 0.400, 
and the marginal prior mean and standard deviation of o- are 0.445 and 
0.111, respectively (see Table 7.3) . 
. 6 
.5 
.4 
~ .3 
"-
.2 
.I 
.0 0 
6 
8 
10 
12 
y 
FIG. 7.9. Predictive distribution on the first observation, based on 
only the information contained in the informative prior. 
15 
15 

162 
CHAPTER 7 
The predictive distribution for y, when all that is known about the 
process is contained in this prior, is shown in Fig. 7.9. Clearly this is a 
much more useful prediction than that allowed by the vague prior (Fig. 
7.3). The prior expected value of y is 3.000 and its standard deviation is 
0.632. The equality of E(y) and E({i1 ) is expected: E(y) = E({ii /(1 
- {32 )), and E({i1) = 3( 1 -
/32 ). However, the fact that V(,Y) = V({iJ) is 
largely an accident, depending on the choice of values for the constants 
controlling the prior distributions of {32 and u. To demonstrate this (and 
validate our calculations), we first note that 
V(l - /32) = V(i32) = E(f3l)- [E(/32)]2 = 0.1532, 
since E(/32 ) = 0. Also, 
E(l -!3d= E(l - 2/32 + i3l) = 1 + 0.1532. 
Now we note that 
V(f3J) = V[E(i3IIi32)] + E[V(/3J!/32)] 
= V[3(1 -
/32)] + E[(l - /3z>l/5.414] 
= 9(0.153f + (l + 0.1532)/5.414. 
Similarly, 
V(y) = V[f3If(l- /32)] + E[u2/(l - /3l)], 
but now we note from (7 .36) that 
E(u2) = 0.152E[i3I2(l + /32)/(l - /32)] + 0.03042E[/3J2(1 + /32)/(1- /32)] 
= (0.152 + 0.03042)E[i3I2(l + /32)/(1 - /32)]. 
Therefore, 
Thus 
E[u2/(l- f3l)] = (0.152 + 0.03042)E[i3I2/(l- !3d] 
= (0.152 + 0.03042)(9 + 1/5.414). 
V(y) = (l/5.414) + (0.152 + 0.03042)(9 + 1/5.414) = 0.400. 
The two results, V(f3J) and V(y), are numerically equal because 0.1532, 
which happened to be the variance assigned to /32 , is approximately equal 
to 0.152 + 0.03042, which comes from the conditional mean and variance 
assigned to u. 
Once the value of Yo has been observed the prior beliefs are of course 
modified. Given an informative prior this is not a troublesome computa-
tion. The resulting posterior marginal moments of /3 1, /32 and u are given 

FIRST-ORDER AUTOREGRESSION 
163 
in Table 7 .3. The moments of {32 are unchanged from their prior values, 
but the marginal beliefs about the other two parameters are altered. Note 
particularly that the posterior expectation of {3 1 is 3.74, an increase from 
its prior value because y0 was observed to be larger that its prior expectation, 
and its standard deviation is 0.652 which is larger than its prior value 
of0.632. 
We normally expect the uncertainty in the value of a parameter to 
decrease with the addition of information, so the change in the standard 
deviation of {3 1 from 0.632 to 0.652 may seem anomalous. However, this 
apparent dilemma, is explained in Table 7.4. The conditional uncertainty 
of f3I given {32 is indeed smaller after observing y0 , but the conditional 
expectations of {3 1 are then more strongly dependent on {32 • The variance 
of {3 1 is made up of two additive terms: the variance of {3 1 given {32 , and 
the variance of the expected value of {3 1 given {32 • In this case the added 
information that y0 = 4.872 decreases the former but increases the latter 
more than enough to compensate. 
Another effect that the observation of y0 has on our beliefs is to change 
the expectation of u from its prior value of 0.445 to a posterior value of 
0.614. The standard deviation of the posterior density on u is 0.099. The 
prior had stated that the conditional expectation of u was larger, given 
larger values of {3 1• Now a single observation has implied that larger values 
of {3 1 are more likely; therefore, the expected value of u must be increased. 
The predictive distribution of y subsequent to the observation of y0 , 
like the posterior density of {31 , is wider than it had been prior to the 
observation. The standard deviation of the predictive distribution on y 
increases from 0.632 to 0. 717. Part of this increase is due to the larger 
marginal uncertainty in {3 1 , but most of it is because we are now assigning 
a larger value to the process standard deviation u. 
Table 7.3 lists the means and standard deviations of the posterior 
TABLE 7 .4. Prior and posterior conditional mean and standard deviation of {J1 • 
Prior values 
Posterior values 
{32 
Mean 
S.D. 
Mean 
S.D. 
-0.35 
4.050 
0.579 
5.049 
0.417 
-0.25 
3.750 
0.536 
4.675 
0.386 
-0.15 
3.450 
0.493 
4.301 
0.355 
-0.05 
3.150 
0.450 
3.927 
0.324 
+0.05 
2.850 
0.407 
3.553 
0.295 
+0.15 
2.550 
0.364 
3.182 
0.261 
+0.25 
2.250 
0.321 
2.803 
0.223 
+0.35 
1.950 
0.281 
2.419 
0.211 

164 
» 
.... 
Ul 
c v 
0 
» 
.... 
.0 
co 
.0 
0 
L 
Q_ 
CHAPTER 7 
4-
3-
2 -
I -
0 ~~~--~~~~~~-L~~~~L-~~~ 
-.8 
-.6 
-.4 
-.2 
0.0 
+.2 
+.4 
+.6 
+.8 
82 
FIG. 7.10. The informative marginal prior probability density on {12 (dotted line), and 
the posterior densities after n = I (shorter dashes), n = 5 (longer dashes) and n = 15 
(solid line). 
distributions on fJ., {12 and u after n = 1, 5 and 15 observations when the 
original prior is informative. This should be compared with Table 7 .2. 
Figures 7. 7, 7.8 and 7.10 should likewise be compared to Figs. 7 .4, 7.5 and 
7 .6. The patterns of changes are similar to those when the prior was taken 
0.5 
0.4 
0.3 
\ 
;: 
\ 
\ 
\ 
.... 
\ 
\ 
\ 
0.2 
\ \ \ 
\, 
0.1 
'\\, ,, ,_ 
0.0 
...................... 
0 
3 
4 
5 
Y( I 0 5 Acre-feet l 
s 
7 
8 
9 
10 
FIG. 7.11. Predictive distributions on y16 after observing Y1s, depending on whether the 
original prior was vague (dashed curve) or informative (solid curve). 

FIRST-ORDER AUTOREGRESSION 
165 
to be vague, but the influence of the observations is less. The uncertainty 
about the parameters, both originally and after the fact, is much less. Since 
we started with strongly held beliefs, we are less likely to be influenced by 
a relatively modest collection of observations. It is worth noting, however, 
that our beliefs concerning a in particular undergo major shifts. The pos-
terior density for n = 15 has essentially no overlap at all with the original 
prior density. The empirical evidence of the data that a is larger than say 
0.6 is convincing. 
Figure 7.11 compares predictions made subsequent to the observation 
y 15 = 5.518. Using the more vague initial prior leads us to expect a slightly 
larger value of y 16 because of the larger posterior expectation for ~2 • Another 
noticeable difference between the two predictions is the width of the pre-
dictive distributions. The prediction arising from the more informative 
initial prior is more narrow; this is partly due to the lesser residual uncer-
tainty about the values of the parameters, but it is even more a consequence 
of the smaller posterior expectation concerning the process standard de-
viation a. 

Appendix 
A Summary of Basic 
Information on Probability 
Distributions Encountered 
The following notation is used: 
C) 
a random variable. It is used for emphasis; its absence 
does not necessarily imply that the variable in ques-
tion is not random. 
J4,(d1b, c, • · ·) 
a probability density of the </>-type (or belonging to the 
</>-family) of probability densities. The random vari-
able a may be either discrete (in which case J4, is in-
terpreted as a probability mass function) or continu-
ous. b, c, · · · are parameters of the distribution; the 
number of parameters depends on the family to 
which it belongs. In referring to the family we say "a 
is r/> with parameters b, c, .... " 
E4,(dJb, c, · · ·) the expected value of a given that a is </> with parame-
ters b, c, .... 
Vct>(dlb, c, · · · ) the variance of a given that a is r/> with parameters b, c, 
167 

168 
APPENDIX A 
2: };,(alb, c, • • ·)when ii is a discrete random 
a.;y 
Fq,(ylb, c, ••• ) = 
variable. 
J:<XJ };,(alb, c, · • ·) when ii is continuous. 
{ 
2: };,(alb, c, 
a>y 
Gq,(ylb, c, • • ·) = 
<XJ 
· i };,(alb, c, 
· · · ), when ii is discrete. 
· · · ), when ii is continuous. 
Therefore, if ii is discrete or continuous 
Gq,(ylb, c, • • ·) + Fq,(ylb, c, · • ·) = 1. 
r(r) 
= L<XJ x'-le-xdx is the usual gamma function. r(r + 1) = rr(r). 
If r is a positive integer, then r(r) = (r -
1 )!. The factorial 
notation is used below whenever the argument must be an 
integer. When the gamma function notation is used the argu-
ment may or may not be an integer. 
r(a)r{b) . 
. 
B(a, b) = 
b 1s the (complete) beta functwn. 
r(a + ) 
Al 
BINOMIAL 
0<p<1 
Jb(rln, p) = '< ~ )' p'( 1 - Pr' 
n, r = 0, 1, 2, ... , 
(A 1.1) 
r. n 
r. 
n~r 
(A1.2) 
Vb(fin, p) = np(l - p). 
(Al.3) 
Tables of binomial probabilities may be found in, for example, Winkler 
(1972, pp. 455-471). 

PROBABILITY DISTRIBUTIONS 
A2 
PASCAL AND GEOMETRIC 
1) Pascal 
fi ( ::1 
) -
(n -
1 )! 
'( 1 
)n-r 
Pa nlr, p - (r- 1)!(n- r)! p 
- p 
EPa(mr, p) = r/p, 
Vpa(flr, p) = r(1 - p)/p2• 
i
0<p<1, 
n, r = 1, 2, ... , 
n:;;.r 
Pascal and binomial probabilities are related by 
Fpa(n!r, p) = Gb(rin, p). 
169 
(A2.1) 
(A2.2) 
(A2.3) 
(A2.4) 
Table Bl. (Appendix B) contains Pascal probabilities for a small selection 
of parameters. 
2) Geometric (a special case of Pascal) 
h<mP) = JPa<n11, p) = p(1- Pr 1• 
(A2.s) 
A3 
NEGATIVE BINOMIAL 
0<p<1 
/' 
~ 
r(r + X) 
r 
)X 
Jnb(x1r, p) = x!r(r) p ( 1 - P 
X = 0, 1, 2, ... , 
(A3.1) 
Enb(xir, p) = r( 1 - p)/p. 
Vnb(xir, p) = r(l - p)/v. 
r> 0 
(A3.2) 
(A3.3) 
The negative binomial is related to both the binomial and the Pascal by 
A4 
BETA 
.fnb(Xjr, p) = fPa(r + xir, 1 - p), 
Gnb(Xjr, p) = Gb(xir +X- 1, p). 
(A3.4) 
(A3.5) 
1 
{O~Z~l, 
frtiia, b)= 
za- 1(1 - zt-a- 1 
(A4.1) 
m~b-~ 
b>a>O 
E{3(iia, b) = a/b, 
(A4.2) 
~ 
a(b- a) 
V~(Zia, b) = b2(b + 1) . 
(A4.3) 

170 
APPENDIX A 
An abbreviated set of tables of the beta distribution is given in Appendix 
B. A more extensive set of tables of fractiles of the beta distribution can 
be found in Winkler (1972, pp. 477-515), and some graphs ofbeta densities 
are shown in Fig. 3.1. Note that the modal value of Z is at (a - 1 )/(b - 2) 
if a > 1 and b -
a > l. If a < 1 and b -
a < 1, then there is a local 
minimum density at Z = (1 - a)/(2 - b). If a ~ 1 and b - a > 1, the 
modal Z is Z = 0; if a> 1 and b- a~ 1, then Z = 1 is the modal density. 
If b - a = a = 1 the density is uniform. 
The beta distribution is related to the binomial by 
F~(Zia, b)= Gb(aiZ, b- 1). 
(A4.4) 
Corresponding relationships to the Pascal and negative binomial can be 
derived from (A2.4) and A(3.5). Another important relationship is with 
the F-distribution. [Eq. (Al4.6)]. 
AS 
BETA-BINOMIAL 
-
I 
I 
_ rcr + Y1)r(n + nl - r -
Y1)n!r(n 1) 
hb(rln, r' n) -
r!r(r1)r(n 1 -
r 1)(n - r)!r(n + n 1) 
' 
(A5.1) 
where r = 0, 1, 2, ... , n = 1, 2, ... , n;;:;;; rand n 1 > r 1 > 0. 
E~b(fln, Y1, n1) = r 1n/n 1, 
(A5.2) 
r 1(n 1 -
r 1) 
V~b(fln, r 1, n 1) = n(n + n 1) n 12(n 1 + l). 
(A5.3) 
Table B2 (in Appendix B) contains some examples of beta-binomial prob-
abilities. For the n 1 = oo columns, the numbers given are the binomial 
probabilities with parameter p = r1/n 1• This makes the expected values the 
same [compare (Al.2) and (A5.2)]. The beta-binomial's variance is always 
larger than that of the corresponding binomial, but they converge in the 
limit as n1 ___, oo with Y1/n 1 held constant. 
A6 
BETA-PASCAL 
1 
1 
r(r + r 1)r(n + nl- r- r1)(n- 1)!r(n 1) 
.kPa(li1r, r' n) = (r -
1 )!r(r1)(n - r)!r(n 1 -
Y1)r(n + n 1) 
' 
(A6.1) 
where r, n = 1, 2, ... , n;;:;;; r, and n 1 > r 1 > 0. 
E~Pa(li1r, Y1, n 1) = r(n 1 -
1 )/(r1 -
1 ), r 1 > 1, 
(A6.2) 
v: 
( ~ 
I~)-
( 
I 
1)(n~-1)(nl-rl) 
I 
2 
~Pa n 1r, r, n 
-
r r + r -
( 1 
) 2( 
1 
2 , r > . 
r-1 r-) 
(A6.3) 

12/1/IP.S 
•atati•tical lnf.r.nc• and Pr~ictian in Cli .. talogya a &av--ian 
Appraach" by Ed..rd B. Ep•t•in 
Errata 
paQ• 6, lin• 13a 
~ad "cli .. talogi•t• in plac• af "cli .. talogical". 
paQ• S1, Tabl• 3.Sa 
~ding af .-cand calu.n ~auld b• 
fjiiPaCni1,3,7.2J. 
pag- 44 and 4Sa 
Th• ••v.ral r•f.r.nc- ta Tabl• 81 uauld b• ta 
Tabl• 81c. 
PAG• 46, Un• 7 batta.a 
~lac• "and" by "•"· 
Pav• 68, lin• 11 
Th• n~atar af th• fractian ~ld b• T". 
pao• 72. Un• 9 batta.a 
~ad •4&/78" in plac• af "30/78". 
PAG• 7S, lin• Sa 
~d "66/78" in plac• af· "12/78". 
Pav• 79, lin• 10• 
~ad "h" i• plac• af ·~· in th• argu.ent af VN. 
PAG• 79, Ml• CS.4)1 
• • Ex11n and ••• 
pao• a2, -.. CS.13)a 
Pav- EP.S, Ml• CS.19Ja 
Pav• 86, lin•• 23-:ZSa 
On .-cand Un• in..rt •lnu• •ton b.t-.n ( and c. 
Ca) On th• fifth lin• af th• -.uatian in-.rt a 
•inu• •ton b•far• h •• 
Cb) Th• d.na.inatar af th• factar in brack.t• an 
th• n•t ta la•t Unaa Df th• -.uaUan 
~auld r•ad Cn+n")21t. 
Th• par.nth•._ 
n~ ta b• add~. 
T•Mt ~auld r .. da 
••• Thi• rang• l• 965i79. 
Thu• th• prababl• ••• i• 
0.67~79. Thi• alia.. u• ta Mrit• 
n'h••1/~2•CO.OOB9) 2•0.72X10-4 and n'.0.72XI0-2 
pav• 86, lin• 7 batta.a 
Right hand •id• af -.uatian uauld r•ad• 
0.61S - 0.292 • 0.32. 
pao• &7, flour• 1-o.nd, lin• Sa 
Nwlb.r in par.nth-- •hauld b• 0.32. 
pag• 89, .... CS.20)1 
Arguaa.nt af fN an fir•t lin• Df -.uatian 
~auld b• CKI~,nhJ. 

pae• 125, lin• 161 
R•lac:• "n" by "n • •. 
pae• 127, lin•• 19-201 
Shauld r.ad • ••• incr•a•tne th• n~.r af 
hypath•ttcal ab-.rvation• by 9, i ••• by 
a factor af 4, ••• • 
pae• 127, lin• 281 
Dltl•t• th• MCrd •.a•. 
pae• 131, lin• ~~ 
Bhauld b• "ca .. CE)." 
pag• 132, Tabl• 6.4J al•a pae• 136, Tabl• 6.~1 
Th• horizontal lin• 
und.r "infor .. u~ prior CB)" Mauld 
•Mtend ta th• l.tt only a• far a• zc. 
pag• 143, -.. C7.1U 1 
Th• la•t l Mauld b• ). 
pag• 1~1, lin• fallONing IMI• C7.36)1 
The quantity d•ftntng c' •hauld 
b• rai.ad ta th• pON.r 112. 

;JaQ• 90, eq. U5. 21) I 
On la•t lin• r~lac• •• •ith ••2 • 
3aQ• 90, eq. b•law C5.21)a 
In th• fir•t lin•, in th• arv~t• of 
th• Nor .. l-Q .... function, chane• n' , •• to 
•• ,n • • 
l)aQ• 91' eq. C5. 22)1 
On .-cond lin•, chane• n",•" to •",n". 
paQ• 92, lin• 9a 
In arv~t of function in int-vral chane• n • ,. • to 
a• ,n. • 
pae• 93, lin• 6a 
-3 
-3 
Th• factor • 
on th• riQht hand •id• 8hould b• a 
• 
pag• 93, eq. C5.28)t 
pag• 93, eq. C:l. 29) • 
Condition •hauld b• v>1. 
Condition 8hould b• v>2. 
pae• 94, lin• Sa 
R•ad ••2 in plac• of • 2 • 
pag• 94, lin• 101 
Thi• i• larv .. t ~ 
~··· 
l)aQ• 99, eq. (:1.39)1 
On th• •.cond lin•, r•ad ">" in plac• of "<". 
pae• 108, lin• pr.c~inQ eq. C6.8)1 
In•~t "• Ev1tn" aft~ •v." 
pae• 108, lin•• :1-6 botta.1 
Should r•ad " ••• oth~ ••tl•at .. of th• 
ca.ffici.nt•, -
and ~ 2 , than 9 and b2 , in 
th• flr•t lin• ••• • 
pae• 109, •q. c6.11)1 
PAQ• 109, •q• (6.14) I 
pag• 111, eq. C6.19)1 
pag• 118, IICI• (6. 48) 1 
pag• 121, eq. C6.53) 1 
On th• •.cond lin•, plac• a •inu• •ten C-) 
Ju•t in•id• •ach of th• t.a l•ft brack•t• ([). 
Th• la•t factor on th• riQht •hould r•ad 
f,th1Cn-2)/2,Cn-2)82/2l. 
R~lac• v •lth Y at th• thr•• plac•• it app•ar•. 
Th• la•t factor of th• int-erand •hould r•ad 
f,CRiv'/2,v'•' 2/2). 
R•v~- th• ord~ of th• conditional arvu..nt• 
of both th• Q .... di•tribution• 
app•arinQ in th• int-erand. 
pae• 121, lin• follawtnv eq. C6.S4)1 
Th• •ub.cript app•arinQ in th• 
d•finition of 8 •hould b• a "u" rath~ 
than a •o•. 
-
pae• 121, •q· C6. 56)1 
Th• fir•t "v" on th• l•ft •hould b• a "Y". 
pae• 12:1, lin• 141 
In plac• of "n Cth• •calar)" r•ad "n' Cth• 
.calar, •n 1 i) ". 

PROBABILITY DISTRIBUTIONS 
171 
The beta-Pascal and beta-binomial bear a relationship analogous to that 
between the Pascal and binomial densities (A2.4): 
F13Pa(nir, r'n') = G13b(rin, r', n'). 
(A6.4) 
Some examples of beta-Pascal probabilities are given in Table B3; these 
should be compared with the Pascal probabilities for which p = r'/n'. 
A7 
POISSON 
e-mmr {' = 0, 1, 2, ... , 
fpo<flm) = -,-,_ -
m>O 
Epo(flm) = m, 
V p0(flm) = m. 
(A7.1) 
(A7.2) 
(A7.3) 
Tables of Poisson probabilities are given by Winkler (1972, pp. 448-454). 
AS 
GAMMA 
~ 
e-yZ(yzy-t 
{Z;;::. 0 
j,(Zix, y) = 
r(x) 
y 
x, y > 0 ' 
E'Y(ilx, y) = xjy, 
V-y(ilx, y) = xjy2• 
(A8.1) 
(A8.2) 
(A8.3) 
A frequently encountered special case of the gamma distribution occurs 
when x = 1. This is the "exponential" distribution 
!,(ill, y) = Je<ily) = ye-Y2. 
(A8.4) 
The moments of the exponential distribution are readily determined from 
(A8.2) and (A8.3). 
We can also define a "standardized" gamma distribution by intro-
ducing the variable change i = yi. The density function for i is then a 
one-parameter family: 
e-zzx-l 
sz ;;::. 0, 
J,.(ilx) = r(x) 
l.x > 0 
E-y*(ilx) = X, 
v'Y.<ilx) = x. 
(A8.5) 
(A8.6) 
(A8.7) 
The relationship of the moments to the single parameter is thus the same 
for the continuous standardized gamma random variable and the discrete 

172 
APPENDIX A 
Poisson random variable. A further relationship between the Poisson and 
the gamma is 
(A8.8) 
A9 
CHI-SQUARE 
The chi-square distribution is a special case of the gamma distribution. 
Specifically, 
-
(-1" 1) 
.fx2(Ziv) = fr Z 2 , 2 
e-Z/2(Z/2)(v/2)-I 
r(v/2) 
(A9.1) 
The moments of the chi-square follow directly from (A8.2) and (A8.3) by 
substituting x = v/2 andy= !. Thus 
Ex2(Ziv) = v, 
(A9.2) 
(A9.3) 
The chi-square distribution is frequently tabulated. Therefore the following 
relationship is particularly useful. If X is distributed as a gamma distribution 
with parameters r and T, then the random variable Z defined by Z = 2XT 
has a chi-square distribution with parameter (degrees of freedom) 2r. In 
other words, 
(A9.4) 
The chi-square distribution also has a special relation to the normal dis-
tribution [see Eq. (A.lO)]. Ifx;, i =I, ... , nare n independent realizations 
of a normal process with zero mean and unit precision, then y = L: xf will 
have a chi-square distribution with parameter n. 
AlO. 
NORMAL AND STANDARDIZED NORMAL 
1) Normal 
fN(Xl~. h) = (h/211")! exp[- ~ h(x - ~f J h > 0, 
EN(XJ~. h) = ~. 
VN(Xl~. h) = 1/h. 
(AlO.l) 
(Al0.2) 
(Al0.3) 
The parameter h is referred to as the "precision." Most frequently h is 
replaced by its equivalent l/u2, where u2 is the variance. 

PROBABILITY DISTRIBUTIONS 
173 
2) Standardized normal 
If xis normal with mean JL and precision h, then the variable ii = (x 
- JL)hi is a standardized normal variable with probability density 
(Al0.4) 
Many tables of the standardized normal distribution are available. Both 
the density and the cumulative frequency are frequently given. The cu-
mulative distribution is often referred to as the "error function" 
All STUDENT'S AND STANDARDIZED STUDENT'S 
l) Student's 
fs(ZIJL, h, v) = (.!. !:.) [v + h(Z- J.L)2r<•+I>t2M 
B 2' 2 
Es(ZIJL, h, v) = J.1. 
~ 
1 
v 
Vs(ZIJL, h, v) = h v _ 2 
2) Standardized Student's or !-distribution 
h, v > 0, 
v > 1, 
v > 2. 
(Al0.5) 
(All.l) 
(A11.2) 
(A11.3) 
If i has a Student's distribution with parameters JL, h and v, then 
t = (Z- JL)hi has a standardized Student's- or !-distribution with v degrees 
offreedom, i.e., with parameter v. 
v•/2 
r 
(~tlv) = 
(v + t2)<•+Il/2 
> 0 
JS. 
B(l/2, v/2) 
v 
' 
Es.(tlv) = 0 
v > 1, 
v s.alv) = vf(v - 2) 
v > 2. 
(A11.4) 
(All.5) 
(All.6) 
It is the standardized !-distribution that is very frequently tabulated in 
statistics texts. From the relation of the Student's distribution to its stan-
dardized form: 
(Al1.7) 
The Student's and standardized Student's distributions are symmetric 
about their means. As v becomes large the Student's density approaches 
the normal density (with the same value of precision parameter) and the 
t-density approaches a standardized normal density. For all finite values 

174 
APPENDIX A 
of v, however, the Student's and !-variables have larger variances than their 
normal counterparts. 
The Student's distribution is usually introduced as the distribution of 
the quotient of two independent random variables, the numerator having 
a normal distribution and the denominator being the square root of a chi-
square random variable divided by its degrees of freedom. That is, if i1 has 
the density fN(iliO, 1) and w has the density .fx2(wlv), then i = u(wfv)-i has 
the density given by (A1l.l). The derivation is essentially identical to that 
for (5.23). 
A12 
INVERTED BETA-2 
1 
r~o 
firdYIP, q, b) = :8(} yP-tbq(y + bfp-q 
(A12.1) 
p,q 
p, q, b > 0' 
E;,n(Yip, q, b) = bpf(q- 1) 
q > 1, 
(A12.2) 
p(p + q- 1) 
V;p2(Yip, Q, b) = (q _ 1)2(q _ 2) ~ 
q> 2. 
(A12.3) 
Inverted beta-2 is derived from the beta distribution by a change of variables. 
If i is beta with parameters p and p + q, then y = Zb/( 1 - Z) is inverted 
beta-2 with parameters p, q and b. This implies that 
F;112(Ji1p, q, b)= F11(y: b,p, p + q). 
(A12.4) 
A13 
F 
EF(yvlvt. v2) = v2/(v2 - 2) 
v2 > 2, 
(A13.2) 
(vt + v2 - 2)vl 
VF(Yivt. v2) = ( 
2)2e 
2) 
V2 > 4. 
(A13.3) 
V2-
2v2 -
Vt 
The F distribution usually arises in the analysis of variance since it can be 
related to the chi-square and normal distributions in the following way. If 
i1 is x2 with parameter v1, and v is independently x2 with parameter v2, 
then 
(A13.2) 

PROBABILITY DISTRIBUTIONS 
175 
has an F distribution with parameters v1 and v2 • The association with the 
normal distribution is through the relation between x2 and the normal as 
explained after Eq. (A9.3). For this reason tables of the F distribution (or 
at least partial tables: values of Z for which FF(Ziv 1, v2 ) = 0.995, 0.975, 
0.95) are given in many statistics texts. 
In the context of this monograph, the significance of the F distribution 
is, at least in part, related to its relationship to the inverted beta-2 and beta 
distributions. F is a special case of inverted b~ta-2: 
(Al3.4) 
That is, if y is inverted beta-2 with parameters p, q and b, and if b = q/p, 
then y has an F distribution with parameters 2p and 2q. But we have 
already seen that ybeing inverted beta-2 with parameters p, q and b implies 
[see Eq. (Al2.4)] thatx = yf(y +b) has a beta distribution with parameters 
p and p + q. Put differently, if x has a beta distribution with parameters p 
and p + q, then the quantity (qfp)x/(1 - x) will have an F distribution 
with parameters 2p and 2q. In other words, 
Fp(:ilp, p + q) = FF[(q/p)(x/(1 - X))l2p, 2q]. 
(Al3.5) 
This relation is useful for evaluating the incomplete beta function when 
tables of the F distribution are available. 
Using (Al3.5) in conjunction with (Al2.4) provides another relation-
ship that is useful for evaluating incomplete beta-2 probabilities: 
F;132(Y1fl, q, b) = F ~! fl2p, 2q) . 
(Al3.6) 
A14 
INVERTED GAMMA-I 
The inverted gamma- I is defined as the distribution of 1/jJ, given that 
yhas a gamma distribution with parameters rand t. Writing x = 1/jJ, then 
F;-y 1(:ilr, t) = G-y(Ji1r, t), 
/;-y,(:ilr, t) = .hO!Xlr, t)ld(I/x)/dxl-i 
e-rtx(t!x)'+' 
r(r)t 
E;-y,(Xlr, t) = t/(r- I) 
12 
V;-y!(Xlr, t) = (r- 1)2(r- 2) 
{
X ;a. 0 
r, t > 0' 
r> l, 
r> 2. 
(Al4.1) 
(Al4.2) 
(Al4.3) 
(Al4.4) 

176 
APPENDIX A 
A15 
INVERTED GAMMA-2 
If the random variable v has a gamma distribution with parameters r 
and t, then the distribution of y = (il/r)-; is said to be inverted gamma-2 
with parameters 2r and t!: 
.fi.y2(.YI2r, t;) = fr(vir, t)idyjdvi- 1 
= exp(-rtjy2)(rtjy2)'- 1t (2r 
3) 
r(r) 
IY 
2 exp( -rtjy2)(rtjy2y+o/2l 
r(r)(rt); 
(A15.1) 
If we now make the substitution V = 2r and s2 = t, we write this density 
in its complex but somewhat conventional form as 
Ji'Y2(.Yis, v) = 
2 exp(-! IIS2 jy2)(! IIS2 jy2)(v+ 1)/2 r;;. 0 
(A15.2) 
r(!v)(!vs2); 
s, "> o' 
-
; r[(v - 1)/2] 
E;'Y2(.Yls, v) - s(v/2) 
r(v/2) 
"> 1, 
(A15.3) 
v;'Y2(.Yls, v) = s2[v/(v - 2)] - [E;'Y2(71is, vf 
v> 2, 
(A15.4) 
G;-y2(.Yis, v) = F'Y*( !vs2/y2 1~) 
(A15.5) 
= Gp0(!vi!vs2/y2). 
(A15.6) 
A16 
NORMAL-GAMMA 
The normal-gamma is a bivariate distribution defined as the product 
of a normal density on the first random variable ji., conditional on the 
precision being known, and a marginal gamma distribution with parameter 
v/2 on the quantity h: 
fN-y(ji., him, v, n, v) = fN(ji.im, hn)fr(hiv/2, vv/2). 
(A 16.1) 
It may also be written as 
!N-y(ji., him, v, n, v) 
= fr[hi(v + 1)/2, W(v + 1)/2lfs(ji.lm, n/v, v), 
(A16.2) 
where 
W = { vv + n(J.t - m)2}/(v + 1). 
In (A 16.1) the joint distribution is written as the marginal density of 

PROBABILITY DISTRIBUTIONS 
177 
htv(v/2) multiplied by the conditional density on ji. when his known. For 
all values of h the random variable ji. is normally distributed with mean 
m. Thus the expectation for ji., whether conditional on h or unconditional, 
is m. In other words, 
ENy(ji.lm, v, n, v) = m, 
EN .. r<iilh; m, v, n, v) = m. 
(A16.3) 
(A16.4) 
It can also be seen in (A 16.2) from the marginal distribution of ji., a Student's 
distribution, that the marginal expectation [see (A1l.2)] of ji. must be m. 
Also, from (A 16.2) and (A 11.3) the marginal variance of ji. is 
VNy(ji.lm, v, n, v) = ~ ~2 . 
nv-
(A16.5) 
When his known, the conditional variance of !J., from (A16.1) and (A10.3) 
is simply 
VN/iilm, v, n, v) = 1/hn. 
(A16.6) 
The marginal (unconditional) mean and variance of h follows from 
the second term of(A16.1) with (A8.2) and (A8.3): 
ENy(him, v, n, v) = 1/v, 
(A16.7) 
VNy(him, v, n, v) = 2/vv2• 
(A16.8) 
Finally, from the first term of (Al6.2), again with (A8.2) and (A8.3), the 
conditional expectation and variance of h are 
-
(v + 1) 
ENy(hi!J.; m, v, n, v) = 1/W = 
2 , 
vv + n(IJ.- m) 
(A16.9) 
VNy(hi!J.; m, v, n, v) = 2[(v + l)V2r 1 
2(v + 1) 
(A16.10) 
Al7 MULTIVARIATE NORMAL 
The multivariate normal is a straightforward extension to multiple 
dimensions of the (univariate) normal distribution. 
(A17.1) 
where x and IJ. are r-element column vectors and H is an r X r positive 
definite symmetrical matrix. The first two joint moments are 

178 
£N(r)(xl~t, H) = #L, 
VN<r>(xl~t, H) = H-1 = V. 
APPENDIX A 
(A17.2) 
(A17.3) 
Thus the vector IL contains the expectation (mean) of the vector random 
variable x in the multivariate normal, in the same way that in the normal 
distribution (A 10.1) the scalar IL is the mean of the scalar random variable 
x. Similarly, H is an r-dimensional precision matrix whose inverse gives 
the covariances among the r elements of x. The diagonal elements of the 
inverse of H, V, are the (marginal) variances of the elements of x. 
To consider marginal and conditional densities of subsets of the r 
scalar random variables Xt. x 2 , ••• , x,, we write, for any integer q, where 
1 :r:;,q:r:;,r-1, 
X= [Xt, ... , Xq; Xq+l• •.. , x,f = [Xtr, XzT] 
1L = [Itt, · · · , #Lq; #Lq+t. · · · , ILrf = [~t?, ILzr], 
and correspondingly we partition H and V as 
where H11 and V11 are q X q matrices; Hzz and V22 are (r- q) X (r- q) 
matrices; H 12 and V12 are q X (r- q) matrices; and V21 = V 12r and H21 
= H12T are (r- q) X qmatrices. The partial precision and variance matrices 
are related by 
H11-1 = V11- V12Vzz- 1Vzt· 
By symmetry, similar relations hold for Vu, V22 and H2z. 
The marginal and conditional densities, when the joint density is mul-
tinormal, are all multivariate (or, if q = 1 orr, univariate) normal. Marginal 
densities are 
(A17.4) 
In other words, the marginal covariance matrix of a subset of the r random 
variables is made up of the corresponding rows and columns of the original 
r X r variance-covariance matrix. 
Conditional densities are 
f(xtlxz; #L, H)= fN<q){ILt- H11-1Hdx2- 1'2), H11} 
= JN<q>{ILt + VtzY22- 1(X2- ILz), V11}. 
(A17.5) 
Thus the conditional precision is made up of the appropriate corresponding 
elements of the original precision matrix, just as the marginal variance is 
made up of the proper elements of the original variance matrix. 

PROBABILITY DISTRIBUTIONS 
179 
Another useful property of the multivariate normal density is that 
linear combinations of random variables that are jointly multivariate nor-
mal are themselves multivariate normal. Define a new random vector as 
y = ax, where a is an s X r matrix of coefficients (of rank s ~ r). Then the 
density of y is 
(Al7.6) 
The quantity Q = (x -
~t)TH(x -
~t) is referred to as the quadratic 
form of the multivariate normal density. The surface Q = constant is an 
ellipsoid in r-dimensional space; the center of the ellipse is at x = ~'· Here 
Q is itself a random variable with a x/ distribution (i.e., x2 with r degrees 
of freedom). Thus the probability that x lies within or without any shell 
determined by Q = constant can be determined from tables of the x2 
distribution. 
A18 
MULTIVARIATE STUDENT DISTRIBUTION 
The expression 
Js<'>(xi~t, H, 11) 
11r(11/2 + r/2) 
= 
jHj![ll +(X- ~t{H(x- ~tW(v+r)/Z 
11 > 0 
(18.1) 
1r'12r(11/2) 
defines the multivariate Student density. The marginal moments are 
(Al8.2) 
(Al8.3) 
Marginal and conditional densities are entirely analogous to those for the 
multivariate normal, except that they are all multivariate Student densities 
with parameter (degrees of freedom) 11. Specifically, 
f(xtl~t, H, 11) = fiq>(xt!l't. Vtt- 1, 11}, 
(Al8.4) 
j(xtlx2; #L, H, 11) = Js<q>[xti~tt- Hll- 1H12(xz- ~tz), Hl!, 11]. 
(Al8.5) 
Also, linear combinations of elements of the random vector variable x will 
have multivariate Student distributions. 

Appendix 
B Selected Tables of 
Probability Distributions 
TABLE Bla. Lower 2.5% limit of the beta distribution. 
(rt- r') 
r' 
0.5 
1.0 
1.5 
2.0 
2.5 
3.0 
3.5 
4.0 
4.5 
5.0 
6.0 
7.5 
10.0 
15.0 
0.5 0.015 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 
1.0 0.049 0.025 0.017 0.013 0.0 10 0.008 0.007 0.006 0.006 0.005 0.004 0.003 0.003 0.002 
1.5 0.147 0.085 0.061 0.047 0.039 0.033 0.028 0.025 0.023 0.020 0.017 0.014 0.010 0.007 
2.0 0.247 0.158 0.118 0.094 0.079 0.068 0.059 0.053 0.048 0.043 0.037 0.030 0.023 0.016 
2.5 0.333 0.229 0.177 0.145 0.123 0.107 0.094 0.085 0.077 0.070 0.060 0.049 0.038 0.026 
3.0 0.405 0.292 0.233 0.194 0.167 0.147 0.131 0.118 0.108 0.099 0.085 0.071 0.055 0.038 
3.5 0.464 0.349 0.284 0.241 0.209 0.186 0.167 0.152 0.139 0.128 0.111 0.093 0.073 0.051 
4.0 0.514 0.398 0.330 0.284 0.258 0.223 0.202 0.184 0.169 0.157 0.137 0.115 0.091 0.064 
4.5 0.555 0.441 0.371 0.323 0.286 0.258 0.234 0.215 0.199 0.185 0.162 0.137 0.109 O.D78 
5.0 0.590 0.478 0.409 0.359 0.321 0.290 0.266 0.245 0.227 0.212 0.187 0.159 0.128 0.091 
6.0 0.647 0.541 0.472 0.421 0.381 0.349 0.322 0.299 0.280 0.262 0.234 0.201 0.163 0.119 
7.5 0.708 0.611 0.546 0.496 0.456 0.423 0.394 0.370 0.348 0.322 0.297 0.259 0.214 0.159 
10.0 0.773 0.692 0.633 0.587 0.549 0.516 0.487 0.462 0.439 0.419 0.384 0.341 0.289 0.221 
15.0 0.843 0.782 0.736 0.697 0.665 0.636 0.609 0.586 0.564 0.544 0.509 0.464 0.406 0.325 
181 

182 
APPENDIX B 
TABLE B.l b. Lower 5% limit of the beta distribution. 
(tt- r') 
r' 
0.5 
1.0 
1.5 
2.0 
2.5 
3.0 
3.5 
4.0 
4.5 
5.0 
6.0 
7.5 
10.0 
15.0 
0.5 0.006 0.002 0.002 0.001 0.001 0.001 0.001 0.001 0.000 0.000 0.000 0.000 0.000 0.000 
1.0 0.097 0.050 0.034 0.025 0.020 0.0 17 O.Ql5 0.013 0.011 0.010 0.009 0.007 0.005 0.003 
1.5 0.229 0.136 0.097 0.074 0.062 0.053 0.046 0.041 0.036 0.033 0.028 0.022 0.017 0.011 
2.0 0.342 0.224 0.168 0.135 0.113 0.098 0.086 0.076 0.069 0.063 0.053 0.044 0.033 0.023 
2.5 0.431 0.302 0.236 0.194 0.165 0.144 0.128 0.115 0.104 0.096 0.082 0.067 0.052 0.036 
3.0 0.501 0.368 0.296 0.249 0.215 0.189 0.169 0.153 0.140 0.129 0.111 0.092 0.072 0.050 
3.5 0.556 0.425 0.349 0.298 0.261 0.232 0.209 0.190 0.175 0.161 0.140 0.117 0.092 0.065 
4.0 0.601 0.473 0.396 0.343 0.303 0.271 0.246 0.225 0.208 0.193 0.169 0.142 0.113 0.080 
4.5 0.638 0.514 0.437 0.382 0.341 0.308 0.281 0.258 0.239 0.223 0.196 0.166 0.133 0.095 
5.0 0.668 0.549 0.473 0.418 0.376 0.341 0.313 0.289 0.269 0.251 0.222 0.190 0.153 0.110 
6.0 0.717 0.607 0.534 0.479 0.436 0.400 0.370 0.345 0.323 0.304 0.271 0.234 0.191 0.140 
7.5 0.768 0.671 0.603 0.551 0.508 0.473 0.442 0.415 0.392 0.371 0.336 0.294 0.244 0.182 
10.0 0.821 0.741 0.683 0.636 0.596 0.562 0.532 0.505 0.182 0.460 0.423 0.377 0.320 0.246 
15.0 0.878 0.819 0.774 0.736 0.703 0.674 0.647 0.623 0,601 0.581 0.544 0.498 0.437 0.352 
TABLE Blc. Lower 10% limit of the beta distribution. 
(n'- r') 
r' 
0.5 
1.0 
1.5 
2.0 
2.5 
3.0 
3.5 
4.0 
4.5 
5.0 
6.0 
7.5 
10.0 
15.0 
0.5 0.024 0.010 0.006 0.004 0.003 0.003 0.002 0.002 0.002 0.002 0.001 0.001 0.001 0.001 
1.0 0.190 0.100 0.068 0.051 0.041 0.035 0.030 0.026 0.023 0.021 0.017 0.014 0.010 0.007 
1.5 0.351 0.215 0.156 0.123 0.102 0.086 0.075 0.067 0.060 0.054 0.046 0.037 0.028 0.019 
2.0 0.468 0.316 0.241 0.196 0.165 0.143 0.126 0.112 0.101 0.093 0.079 0.064 0.049 0.034 
2.5 0.552 0.398 0.315 0.262 0.225 0.197 0.175 0.158 0.143 0.132 0.113 0.093 0.072 0.050 
3.0 0.614 0.464 0.378 0.320 0.279 0.247 0.221 0.201 0.184 0.170 0.147 0.122 0.096 0.067 
3.5 0.661 0.518 0.432 0.372 0.327 0.292 0.264 0.241 0.222 0.206 0.179 0.151 0.119 0.084 
4.0 0.698 0.562 0.477 0.416 0.370 0.333 0.303 0.279 0.258 0.240 0.210 0.178 0.142 0.101 
4.5 0.728 0.599 0.516 0.455 0.408 0.370 0.339 0.313 0.291 0.271 0.240 0.204 0.164 0.117 
5.0 0.753 0.631 0.550 0.490 0.442 0.404 0.372 0.345 0.321 0.301 0.267 0.229 0.185 0.134 
6.0 0.791 0.681 0.606 0.547 0.501 0.462 0.429 0.401 0.376 0.354 0.318 0.275 0.226 0.166 
7.5 0.830 0.736 0.668 0.614 0.569 0.531 0.498 0.470 0.444 0.421 0.383 0.336 0.280 0.211 
10.0 0.871 0.794 0.737 0.690 0.650 0.614 0.583 0.556 0.531 0.508 0.468 0.420 0.358 0.277 
15.0 0.912 0.858 0.815 0.778 0.745 0.716 0.690 0.666 0.643 0.622 0.585 0.537 0.474 0.384 

SELECTED TABLES OF PROBABILITY DISTRIBUTIONS 
183 
TABLE Bid. Lower 25% limit of the beta distribution. 
(li- r') 
r' 
0.5 
1.0 
1.5 
2.0 
2.5 
3.0 
3.5 
4.0 
4.5 
5.0 
6.0 
7.5 
10.0 
15.0 
0.5 0.146 0.063 0.039 0.028 0.022 O.ot8 O.ot5 0.013 0.012 O.otl 0.009 0.007 0.005 0.003 
1.0 0.438 0.250 0.175 0.134 0.109 0.091 0.079 0.069 0.062 0.056 0.047 O.Q38 O.o28 0.019 
1.5 0.597 0.397 0.298 0.239 0.199 0.171 0.150 0.133 0.120 0.109 0.093 O.o75 0.057 0.039 
2.0 0.689 0.500 0.394 0.326 0.279 0.243 0.216 0.194 0.176 0.161 0.138 0.113 0.088 0.060 
2.5 0.747 0.574 0.469 o:398 0.345 0.305 0.274 0.248 0.227 0.209 0.181 0.150 0.117 0.081 
3.0 0.787 0.630 0.528 0.456 0.402 0.359 0.325 0.297 0.273 0.253 0.221 0.185 0.146 0.103 
3.5 0.816 0.673 0.576 0.505 0.450 0.406 0.370 0.340 0.315 0.293 0.257 0.218 0.173 0.123 
4.0 0.839 0.707 0.615 0.546 0.491 0.447 0.410 0.379 0.352 0.329 0.291 0.248 0.199 0.143 
4.5 0.856 0.735 0.648 0.581 0.527 0.482 0.445 0.413 0.386 0.362 0.322 0.276 0.224 0.162 
5.0 0.870 0.758 0.675 0.611 0.558 0.514 0.477 0.445 0.417 0.392 0.351 0.303 0.247 0.181 
6.0 0.891 0.794 0.719 0.659 0.609 0.567 0.530 0.498 0.470 0.445 0.402 0.351 0.290 0.216 
7.5 0.913 0.831 0.767 0.713 0.668 0.628 0.593 0.562 0.534 0.509 0.465 0.412 0.347 0.264 
10.0 0.934 0.871 0.818 0.773 0.734 0.699 0.667 0.639 0.612 0.588 0.546 0.493 0.424 0.332 
15.0 0.956 0.912 0.874 0.840 0.810 0.782 0.756 0.733 0.710 0.690 0.652 0.602 0.535 0.438 
TABLE Ble. Median of the beta distribution. 
2(11- r') 
2r' 
0.5 
1.0 
1.5 
2.0 
2.5 
3.0 
3.5 
4.0 
4.5 
5.0 
6.0 
7.5 
10.0 
15.0 
0.5 0.500 0.250 0.163 0.121 0.096 0.079 0.067 0.059 0.052 0.047 0.039 0.031 0.023 0.015 
1.0 0.750 0.500 0.370 0.293 0.242 0.206 0.180 0.159 0.143 0.129 0.109 0.088 0.067 0.045 
1.5 0.837 0.630 0.500 0.414 0.352 0.307 0.272 0.244 0.221 0.202 0.173 0.142 0.109 0.075 
2.0 0.879 0.707 0.586 0.500 0.436 0.386 0.346 0.314 0.287 0.264 0.228 0.190 0.148 0.103 
2.5 0.904 0.758 0.648 0.564 0.500 0.449 0.407 0.372 0.343 0.318 0.277 0.233 0.184 0.129 
3.0 0.921 0.794 0.693 0.614 0.551 0.500 0.457 0.421 0.391 0.364 0.321 0.272 0.217 0.154 
3.5 0.933 0.820 0.728 0.654 0.593 0.543 0.500 0.464 0.432 0.405 0.359 0.307 0.247 0.178 
4.0 0.941 0.841 0.756 0.686 0.628 0.579 0.536 0.500 0.468 0.440 0.393 0.339 0.275 0.200 
4.5 0.948 0.857 0.779 0.713 0.657 0.609 0.568 0.532 0.500 0.472 0.424 0.368 0.301 0.221 
5.0 0.953 0.871 0.798 0.736 0.682 0.636 0.595 0.560 0.528 0.500 0.452 0.395 0.326 0.242 
6.0 0.961 0.891 0.827 0.772 0.723 0.679 0.641 0.607 0.576 0.548 0.500 0.442 0.370 0.279 
7.5 0.969 0.912 0.858 0.810 0.767 0.728 0.693 0.661 0.632 0.605 0.558 0.500 0.426 0.328 
10.0 0.977 0.933 0.891 0.852 0.916 0.783 0.753 0.725 0.699 0.674 0.630 0.574 0.500 0.397 
15.0 0.985 0.955 0.925 0.897 0.871 0.846 0.822 0.800 0.779 0.758 0.721 0.672 0.603 0.500 

184 
APPENDIX B 
TABLE 8.2. Beta-binomial probabilities. Top row of 
10 
20 
00 
10 
20 
00 
I 
3 
n 
0.1 
0.3 
0.6 
n 
0.2 
0.6 
1.2 
2 
4 
n 
0.3 
0.9 
0 0.855 0.832 0.823 0.818 0.814 0.810 
0 0.720 0.680 0.663 0.655 0.648 0.640 
2 
0 0.595 0.542 
0.090 0.135 0.154 0.164 0.171 0.180 
0.160 0.240 0.274 0.291 0.305 0.320 
0.210 0.315 
0.055 0.032 0.023 0.018 0.014 0.010 
0.120 0.080 0.063 0.055 0.048 0.004 
0.195 0.142 
0 0.806 0. 743 0. 710 0.692 0.676 0.656 
4 
0 0.638 0.539 0.488 0.462 0.438 0.410 
4 
0 0.495 0.378 
0.083 0.157 0.203 0.231 0.257 0.292 
I 0.134 0.239 0.300 0.336 0.369 0.410 
0.161 0.267 
0.047 0.065 0.066 0.063 0.058 0.049 
2 0.086 0.131 0.146 0.151 0.154 0.154 
0.116 0.186 
0.035 0.027 O.Ql8 0.013 0.008 0.004 
0.070 0.067 0.054 0.045 0.036 0.026 
0.105 0.116 
4 0.030 0.008 0.003 0.001 0.001 0.000 
4 0.070 0.025 0.012 0.007 0.004 0.002 
4 0.123 0.054 
0 0.756 0.642 0.572 0.529 0.487 0.430 
0 0.561 0.400 0.313 0.265 0.221 0.168 
0 0.407 0.240 
0.077 0.159 0.221 0.265 0.312 0.383 
0.115 0.204 0.254 0.282 0.307 0.336 
0.127 0.190 
0.043 0.083 0.109 0.124 0.136 0.136 
0.071 0.136 0.181 0.212 0.244 0.294 
0.086 0.156 
0.030 0.050 0.054 0.053 0.047 0.033 
0.054 0.096 0.118 0.130 0.140 0.147 
0.070 0.127 
0.024 0.031 0.026 0.020 0.013 0.005 
4 0.045 0.067 0.071 0.068 0.061 0.046 
4 0.061 0.102 
0.020 O.Ql8 0.011 0.007 0.003 0.000 
5 0.040 0.046 0.038 0.030 0.021 0.009 
0.057 0.078 
0.017 0.010 0.004 0.002 0.00 I 0.000 
6 0.037 0.029 0.017 0.010 0.005 0.00 I 
0.056 0.056 
0.016 0.005 0.00 I 0.000 0.000 0.000 
0.036 0.016 0.006 0.003 0.001 0.000 
0.059 0.036 
0.016 0.002 0.000 0.000 0.000 0.000 
0.041 0.006 0.00 I 0.000 0.000 0.000 
0.077 0.017 
16 
0 0.707 0.541 0.431 0.360 0.287 0.185 16 
0 0.491 0.282 0.175 0.120 0.074 0.028 16 
0 0.333 0.141 
0.071 0.147 0.202 0.240 0.279 0.329 
0.099 0.156 0.170 0.167 0.153 0.113 
0.102 0.119 
2 0.039 0.086 0.125 0.157 0.196 0.275 
0.060 0.114 0.149 0.171 0.191 0.211 
0.068 0.104 
3 0.028 0.059 0.083 0.100 0.118 0.142 
0.045 0.090 0.125 0.152 0.184 0.246 
0.053 0.094 
4 0.022 0.043 0.056 0.062 0.064 0.051 
4 0.037 0.073 0.102 0.123 0.150 0.200 
4 0.045 0.085 
5 O.Ql8 0.032 0.037 0.037 0.032 0.014 
5 0.031 0.060 0.080 0.093 0.107 0.120 
5 0.039 0.076 
6 O.Ql5 0.025 0.025 0.021 O.Ql5 0.003 
6 0.028 0.050 0.062 0.067 0.068 0.055 
6 0.038 0.068 
0.013 0.019 0.016 0.012 0.006 0.000 
0.025 0.041 0.046 0.045 0.039 0.006 
0.033 0.061 
0.012 O.Ql5 0.010 0.006 0.002 0.000 
0.023 0.034 0.033 0.028 0.020 0.00 I 
0.031 0.053 
0.011 O.Qll 0.006 0.003 0.001 0.000 
9 0.021 0.028 0.023 0.017 0.009 0.000 
9 0.030 0.046 
10 0.010 0.008 0.004 0.001 0.000 0.000 
10 0.020 0.022 0.015 0.009 0.004 0.000 
10 0.029 0.040 
II 0.010 0.006 0.002 0.001 0.000 0.000 
II 0.019 0.017 0.009 0.005 0.001 0.000 
II 0.029 0.033 
12 0.009 0.004 0.001 0.000 0.000 0.000 
12 0.019 0.013 0.005 0.002 0.000 0.000 
12 0.029 0.027 
13 0.009 0.003 0.000 0.000 0.000 0.000 
13 0.018 0.009 0.003 0.001 0.000 0.000 
13 0.029 0.021 
14 0.008 0.002 0.000 0.000 0.000 0.000 
14 0.019 0.006 0.001 0.000 0.000 0.000 
14 0.031 O.Ql5 
15 0.008 0.001 0.000 0.000 0.000 0.000 
15 0.020 0.004 0.000 0.000 0.000 0.000 
15 0.035 0.013 
16 0.009 0.000 0.000 0.000 0.000 0.000 
16 0.024 0.001 0.000 0.000 0.000 0.000 
16 0.048 0.005 

SELECTED TABLES OF PROBABILITY DISTRIBUTIONS 
185 
column heads is w; bottom row is r'. 
1020oo 
6 
1020oo 
6 
1020oo 
1.8 
6 
n 
r 
0.4 
1.2 
2.4 
4 
n 
r 
0.5 
1.5 
3 
10 
0.520 0.509 0.500 0.490 2 0 0.480 0.420 0.394 0.382 0.371 0.360 2 0 0.375 0.312 0.286 0.273 0.262 0.250 
0.360 0.381 0.400 0.420 
I 0.240 0.360 0.411 0.436 0.457 0.480 
I 0.250 0.375 0.429 0.455 0.476 0.500 
0.120 0.109 0.100 0.009 
2 0.280 0.220 0.195 0.182 0.171 0.160 
2 0.375 0.312 0.286 0.273 0.262 0.250 
0.322 0.294 0.269 0.240 4 0 0.374 0.255 0.202 0.176 0.154 0.130 4 
0.322 0.352 0.379 0.412 
I 0.166 0.255 0.294 0.313 0.329 0.346 
0.218 0.235 0.249 0.265 
2 0.134 0.222 0.268 0.294 0.317 0.346 
0.106 0.098 0.089 0.076 
3 0.134 0.169 0.171 0.168 0.162 0.154 
0.030 0.021 0.014 0.008 
4 0.190 0.099 0.064 0.049 0.037 0.026 
0.162 0.124 0.092 0.058 
0.208 0.212 0.210 0.198 
0.200 0.228 0.257 0.296 
0.165 0.190 0.216 0.254 
0.121 0.130 0.135 0.136 
O.o78 0.073 0.064 0.047 
0.043 0.032 0.022 0.010 
O.DI8 0.010 0.005 0.00 I 
0.005 0.002 0.00 I 0.000 
0 0.288 0.137 O.o78 0.053 0.034 0.017 
I 0.121 0.150 0.142 0.130 0.115 0.090 
2 0.090 0.148 0.176 0.190 0.201 0.209 
3 0.077 0.139 0.180 0.207 0.236 0.279 
4 0.071 0.126 0.160 0.181 0.203 0.232 
5 0.070 0.109 0.124 0.129 0.130 0.124 
6 0.072 0.089 0.082 0.073 0.060 0.041 
7 0.083 0.065 0.043 0.030 0.019 0.008 
8 0.128 0.037 0.014 0.007 0.003 0.001 
0 0.273 0.164 0.119 0.098 0.081 0.062 
I 0.156 0.219 0.238 0.245 0.248 0.250 
2 0.141 0.234 0.286 0.315 0.342 0.375 
3 0.156 0.219 0.238 0.24S 0.248 0.2SO 
4 0.273 0.164 0.119 0.098 0.081 0.062 
o o.l96 o.o74 oms o.o2o 0.011 0.004 
I 0.10S O.IOS 0.084 0.068 O.OS2 0.031 
2 0.085 0.122 0.131 0.130 0.124 0.109 
3 0.077 0.132 0.163 0.181 0.198 0.219 
4 O.o7S 0.13S 0.17S 0.202 0.230 0.273 
s 0.071 0.132 0.163 0.181 0.198 0.219 
6 0.08S 0.122 0.131 0.130 0.124 0.109 
7 0.10S 0.105 0.084 0.068 O.OS2 0.031 
8 0.196 0.074 0.03S 0.020 0.011 0.004 
0.067 0.037 0.017 0.003 16 
0 0.220 0.067 0.024 0.010 0.003 0.000 16 
0 0.140 0.030 0.008 0.002 0.001 0.000 
0.100 0.080 0.055 0.022 
I 0.090 0.077 0.049 0.030 O.DI S 0.003 
I 0.072 0.044 0.020 0.009 0.003 0.000 
0.116 0.114 0.104 o.o13 
2 o.06s o.o8o o.o1o o.os1 o.040 O.Dls 
2 o.os6 o.os3 oms 0.022 o.ou 0.002 
0.119 0.136 0.143 0.146 
3 O.OS3 0.081 0.087 0.084 0.074 0.047 
3 0.048 0.060 O.OS2 0.041 0.027 0.008 
O.IIS 0.136 0.161 0.204 
4 0.047 0.080 0.098 0.106 0.110 0.101 
4 0.044 0.06S 0.067 0.062 O.OS2 O.o28 
0.105 0.127 O.ISS 0.210 
s 0.143 0.178 0.103 0.120 0.138 0.162 
s 0.041 0.068 0.080 0.084 0.083 0.067 
0.092 0.110 0.130 0.16S 
6 0.040 O.o7S 0.103 0.123 0.149 0.198 
6 0.040 0.071 0.091 0.103 0.114 0.122 
O.o78 0.088 0.097 0.101 
7 0.038 0.072 0.099 0.118 0.142 0.189 
7 0.039 0.072 0.097 O.IIS 0.137 0.17S 
0.063 0.066 0.06S 0.049 
8 0.037 0.068 0.089 0.104 0.120 0.141 
8 0.039 0.072 0.100 0.120 0.146 0.196 
0.049 0.046 om8 0.019 
9 0.036 0.063 O.o78 0.08S 0.090 0.084 
9 0.039 0.072 0.097 O.IIS 0.137 0.17S 
0.036 0.030 0.020 0.006 
10 0.036 0.057 0.065 0.06S O.OS9 0.039 
10 0.040 0.071 0.091 0.103 0.114 0.122 
O.D25 O.DI8 0.009 0.001 
II 0.036 0.052 0.051 0.045 0.034 0.014 
II 0.041 0.068 0.080 0.084 0.083 0.067 
0.017 0.009 0.004 0.000 
0.0 I 0 0.004 0.00 I 0.000 
0.005 0.002 0.000 0.000 
0.002 0.000 0.000 0.000 
0.00 I 0.000 0.000 0.000 
12 oms 0.045 0.037 0.028 0.017 0.004 
13 0.040 0.038 0.025 0.0 I 5 0.007 0.00 I 
14 0.044 0.031 O.DI 5 0.007 0.002 0.000 
IS 0.053 0.022 0.007 0.002 0.001 0.000 
16 0.085 0.012 0.002 0.000 0.000 0.000 
12 0.044 0.065 0.067 0.062 0.052 0.028 
13 0.048 0.060 0.052 0.041 0.027 0.008 
14 0.056 0.053 0.03S 0.022 0.011 0.002 
I 5 0.072 0.044 0.020 0.009 0.003 0.000 
16 0.140 0.030 0.008 0.002 0.00 I 0.000 

186 
APPENDIX B 
TABLE 83. Beta-Pascal Probabilities. Top row of column heads is n'; bottom row is r'. 
4 
n 
2 
1.6 
4 
3.2 
8 
6.4 
16 
12.8 
00 
I 
2 
0.800 
0.800 
0.800 
0.800 
0.800 
2 
4 
5 
6 
7 
8 
9 
10 
0.107 
0.128 
0.037 
0.038 
O.Dl8 
O.Dl5 
0.010 
0.007 
0.006 
0.004 
0.004 
0.002 
0.003 
0.001 
0.002 
0.001 
0.002 
0.002 
0.516 
0.206 
0.103 
0.058 
O.D35 
0.142 
0.151 
0.160 
0.037 
O.D35 
0.032 
0.012 
0.0 10 
0.006 
0.005 
0.003 
0.001 
0.002 
0.001 
0.000 
0.001 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.472 
0.252 
0.126 
0.065 
O.D35 
0.444 
0.284 
0.142 
0.067 
0.032 
0.410 
0.328 
0.164 
0.066 
0.023 
4 
0.574 
5 
0.153 
6 
0.077 
7 
0.046 
8 
0.030 
9 
0.021 
10 
0.016 
0.022 
0.0 19 
O.Dl5 
0.007 
O.Dl5 
0.0 II 
0.007 
0.002 
II 
0.012 
12 
0.009 
13 
0.008 
14 
0.006 
15 
0.005 
16 
0.004 
0.010 
0.007 
0.004 
0.001 
0.007 
0.004 
0.002 
0.000 
0.005 
0.003 
0.00 I 
0.000 
0.004 
0.002 
0.001 
0.000 
0.003 
0.00 I 
0.000 
0.000 
0.002 
0.001 
0.000 
0.000 
17 
0.004 
0.002 
18 
0.003 
0.002 
19 
0.003 
0.00 I 
20 
0.002 
21 
0.002 
22 
0.002 
23 
0.002 
24 
0.001 
2 
1.2 
0.001 
0.001 
0.001 
0.001 
0.000 
4 
2.4 
I 
0.600 
0.600 
2 
0.160 
0.192 
0.072 
0.083 
4 
0.040 
0.043 
5 
0.026 
O.D25 
0.001 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
8 
4.8 
0.600 
0.213 
0.090 
0.042 
0.022 
16 
9.6 
0.600 
0.226 
0.093 
0.041 
0.019 
00 
0.600 
0.240 
0.096 
0.038 
0.015 
8 
2 
n 
2 
1.6 
2 
0.693 
3 
0.139 
4 
0.058 
5 
0.031 
6 
0.019 
7 
0.012 
8 
0.009 
9 
0.006 
10 
0.005 
II 
0.004 
12 
0.003 
13 
0.002 
14 
0.002 
15 
0.002 
16 
0.001 
4 
3.2 
0.672 
0.179 
0.069 
0.032 
0.017 
0.010 
0.006 
0.004 
0.003 
0.002 
0.001 
0.001 
0.001 
0.001 
0.000 
8 
6.4 
0.658 
0.210 
0.075 
0.030 
0.013 
0.006 
0.003 
0.002 
0.001 
0.001 
0.000 
0.000 
0.000 
0.000 
0.000 
16 
12.8 
0.649 
0.231 
0.077 
0.027 
0.010 
0.004 
0.002 
0.001 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
8 
9 
10 
II 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
0.458 
0.361 
0.284 
0.233 
0.147 
0.192 
0.227 
0.248 
0.084 
0.120 
0.156 
0.188 
0.056 
0.080 
0.104 
0.125 
0.040 
0.056 
0.069 
0.079 
0.030 
0.040 
0.047 
0.049 
0.024 
0.030 
0.032 
0.030 
0.019 
0.022 
0.022 
0.018 
2 
3 
4 
5 
6 
O.Dl5 
0.017 
O.Dl5 
0.0 II 
0.013 
0.013 
0.011 
0.007 
0.0 II 
0.0 II 
0.008 
0.004 
0.009 
0.009 
0.006 
0.003 
0.008 
0.007 
0.004 
0.002 
0.007 
0.006 
0.005 
0.005 
0.004 
0.004 
0.003 
0.003 
0.003 
0.002 
0.002 
0.002 
2 
1.2 
0.440 
0.176 
0.095 
0.059 
0.040 
0.006 
0.005 
0.004 
0.003 
0.003 
0.002 
0.002 
0.002 
0.002 
0.001 
0.001 
0.001 
4 
2.4 
0.408 
0.218 
0.121 
0.073 
0.046 
0.003 
0.002 
0.002 
0.001 
0.001 
0.001 
0.001 
0.001 
0.000 
0.000 
0.000 
0.000 
8 
4.8 
0.387 
0.247 
0.143 
0.082 
0.049 
0.001 
0.001 
0.001 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
16 
9.6 
0.374 
0.266 
0.155 
0.087 
0.049 
00 
0.640 
0.256 
0.077 
0.020 
0.005 
0.001 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.168 
0.268 
0.242 
0.161 
0.089 
0.043 
O.Dl8 
0.007 
0.003 
0.001 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
00 
0.360 
0.288 
0.173 
0.092 
0.046 

SELECTED TABLES OF PROBABILITY DISTRIBUTIONS 
187 
TABLE 83. (Continued) 
2 
4 
8 
16 
00 
2 
4 
8 
16 
00 
1.2 
2.4 
4.8 
9.6 
1.2 
2.4 
4.8 
9.6 
6 
O.Dl8 
O.DI5 
0.012 
0.010 
0.006 
7 
0.029 
0.031 
0.030 
0.028 
0.022 
7 
0.013 
0.010 
0.007 
0.005 
0.002 
8 
0.022 
0.022 
0.019 
0.016 
0.010 
8 
0.010 
0.007 
0.003 
0.003 
0.001 
9 
0.017 
0.016 
0.013 
0.009 
0.005 
9 
0.007 
0.005 
0.003 
0.001 
0.000 
10 
0.013 
0.012 
0.009 
0.006 
0.002 
10 
0.006 
0.004 
0.002 
0.001 
0.000 
II 
0.011 
0.009 
0.006 
0.004 
0.001 
II 
0.005 
0.003 
0.001 
0.001 
0.000 
12 
0.009 
0.007 
0.004 
0.002 
0.000 
12 
0.004 
0.002 
0.001 
0.000 
0.000 
13 
0.008 
0.006 
0.003 
0.001 
0.000 
13 
0.003 
0.002 
0.001 
0.000 
0.000 
14 
0.007 
0.004 
0.002 
0.001 
0.000 
14 
0.003 
0.001 
0.000 
0.000 
0.000 
15 
0.006 
0.004 
0.002 
0.001 
0.000 
15 
0.003 
0.001 
0.000 
0.000 
0.000 
16 
0.005 
0.003 
0.001 
0.000 
0.000 
16 
0.002 
0.001 
0.000 
0.000 
0.000 
17 
0.004 
0.002 
0.001 
0.000 
0.000 
17 
0.002 
0.001 
0.000 
0.000 
0.000 
18 
0.004 
0.002 
0.001 
0.000 
0.000 
18 
0.002 
0.001 
0.000 
0.000 
0.000 
19 
0.003 
0.002 
0.001 
0.000 
0.000 
20 
0.001 
0.000 
0.000 
0.000 
0.000 
20 
0.003 
0.001 
0.000 
0.000 
0.000 
4 
4 
0.296 
0.231 
0.186 
0.160 
0.130 
8 
8 
0.186 
0.109 
0.063 
0.039 
0.017 
5 
0.158 
0.185 
0.199 
0.205 
0.207 
9 
0.119 
0.116 
0.100 
0.083 
0.054 
6 
0.101 
0.133 
0.161 
0.180 
0.207 
10 
0.088 
0.105 
0.111 
0.110 
0.097 
7 
0.071 
0.096 
0.119 
0.138 
0.166 
II 
0.068 
0.090 
0.107 
0.118 
0.129 
8 
0.052 
0.070 
0.086 
0.098 
0.116 
12 
0.055 
0.076 
0.096 
0.113 
0.142 
9 
0.040 
0.052 
0.062 
0.068 
0.074 
13 
0.045 
0.064 
0.083 
0.101 
0.136 
10 
0.032 
0.040 
0.045 
0.047 
0.045 
14 
O.o38 
0.053 
0.070 
0.086 
0.118 
II 
0.026 
0.031 
0.033 
0.032 
O.o25 
15 
0.032 
0.045 
0.059 
0.071 
0.094 
12 
0.021 
0.024 
0.024 
0.022 
0.014 
16 
0.028 
O.o38 
0.049 
0.058 
0.071 
13 
O.Dl8 
0.020 
O.Dl8 
O.DI5 
0.007 
17 
0.024 
0.033 
0.041 
0.046 
0.050 
14 
O.DI5 
0.016 
0.014 
0.010 
0.004 
18 
0.021 
0.028 
0.034 
0.037 
0.034 
15 
0.013 
0.013 
0.010 
0.007 
0.002 
19 
0.019 
0.024 
0.030 
0.029 
0.022 
16 
O.Dll 
O.oii 
0.008 
0.005 
0.001 
20 
0.017 
0.021 
0.023 
0.023 
0.014 
17 
0.010 
0.009 
0.006 
0.004 
0.000 
21 
0.015 
0.018 
0.019 
O.oi8 
0.009 
18 
0.009 
0.008 
0.005 
0.003 
0.000 
22 
0.013 
0.016 
0.016 
0.014 
0.005 
19 
0.008 
0.006 
0.004 
0.002 
0.000 
23 
0.012 
0.014 
0.014 
0.011 
0.003 
20 
0.007 
0.006 
0.003 
0.001 
0.000 
24 
0.011 
0.012 
0.012 
0.009 
0.002 
21 
0.006 
0.005 
0.003 
0.001 
0.000 
25 
0.010 
0.011 
0.010 
0.007 
0.001 
22 
0.006 
0.004 
0.002 
0.001 
0.000 
26 
0.009 
0.010 
0.008 
0.005 
0.001 
23 
0.005 
0.004 
0.002 
0.001 
0.000 
27 
0.008 
0.009 
0.007 
0.004 
0.000 
24 
0.005 
0.003 
0.001 
0.000 
0.000 
28 
0.008 
0.008 
0.006 
0.003 
0.000 
25 
0.004 
0.003 
0.001 
0.000 
0.000 
29 
0.007 
0.007 
0.005 
0.003 
0.000 
26 
0.004 
0.002 
0.001 
0.000 
0.000 
30 
0.007 
0.006 
0.005 
0.002 
0.000 
27 
0.004 
0.002 
0.001 
0.000 
0.000 
31 
0.006 
0.006 
0.004 
0.002 
0.000 
28 
0.003 
0.002 
0.001 
0.000 
0.000 
32 
0.006 
0.005 
0.003 
0.001 
0.000 
29 
0.003 
0.002 
0.001 
0.000 
0.000 
33 
0.005 
0.005 
0.003 
0.001 
0.000 
30 
0.003 
0.002 
0.000 
0.000 
0.000 
34 
0.005 
0.004 
0.003 
0.001 
0.000 
31 
0.003 
0.001 
0.000 
0.000 
0.000 
35 
0.005 
0.004 
0.002 
0.001 
0.000 
32 
0.002 
0.001 
0.000 
0.000 
0.000 
36 
0.004 
0.004 
0.002 
0.001 
0.000 
37 
0.004 
0.003 
0.002 
0.001 
0.000 
38 
0.004 
0.003 
0.002 
0.000 
0.000 
39 
0.004 
0.003 
0.001 
0.000 
0.000 
40 
0.004 
0.003 
0.001 
0.000 
0.000 
45 
0.003 
0.002 
0.001 
0.000 
0.000 
50 
0.002 
0.001 
0.000 
0.000 
0.000 

188 
APPENDIX B 
TABLE 83. (Continued) 
2 
4 
8 
16 
00 
2 
4 
8 
16 
00 
n 
0.8 
1.6 
3.2 
6.4 
n 
0.8 
1.6 
3.2 
6.4 
I 
0.400 
0.400 
0.400 
0.400 
0.400 
2 
2 
0.024 
0.208 
0.187 
0.174 
0.160 
2 
0.160 
0.192 
0.213 
0.226 
0.240 
3 
0.144 
0.166 
0.179 
0.186 
0.192 
3 
0.088 
0.109 
0.124 
0.133 
0.144 
4 
0.095 
0.121 
0.141 
0.155 
0.173 
4 
0.056 
0.068 
0.076 
0.081 
0.086 
5 
0.068 
0.089 
0.107 
0.120 
0.138 
5 
0.039 
0.046 
0.050 
0.051 
0.052 
6 
0.051 
0.067 
0.080 
0.090 
0.104 
6 
0.029 
0.033 
0.034 
0.033 
0.031 
7 
0.040 
0.051 
0.061 
0.067 
0.075 
7 
0.023 
0.024 
0.024 
0.022 
0.019 
8 
0.032 
0.040 
0.046 
0.050 
0.052 
8 
O.ot8 
0.019 
0.017 
O.ot5 
O.otl 
9 
0.026 
0.032 
0.036 
0.037 
0.036 
9 
O.ot5 
O.ot5 
0.013 
0.010 
0.007 
10 
0.022 
0.026 
0.028 
0.027 
0.024 
10 
0.012 
0.012 
0.009 
0.007 
0.004 
II 
0.019 
0.022 
0.022 
0.021 
0.016 
II 
0.011 
0.009 
0.007 
0.005 
0.002 
12 
0.013 
0.016 
O.ot8 
O.ot8 
0.017 
12 
0.009 
0.008 
0.006 
0.004 
0.001 
13 
0.011 
0.014 
o.ot5 
0.016 
0.014 
13 
0.008 
0.007 
0.004 
0.003 
0.001 
14 
0.010 
0.013 
0.014 
0.013 
0.011 
14 
0.007 
0.006 
0.004 
0.002 
0.001 
15 
0.009 
0.011 
0.012 
0.011 
0.009 
15 
0.006 
0.005 
0.003 
0.002 
0.000 
16 
0.009 
0.010 
0.011 
0.010 
0.007 
16 
0.006 
0.004 
0.002 
0.001 
0.000 
17 
0.008 
0.009 
0.009 
0.009 
0.006 
17 
0.005 
0.004 
0.002 
0.001 
0.000 
18 
0.007 
0.009 
0.008 
0.007 
0.005 
18 
0.005 
0.003 
0.002 
0.001 
0.000 
19 
0.007 
0.008 
0.008 
0.007 
0.004 
19 
0.004 
0.003 
0.001 
0.001 
0.000 
20 
0.006 
0.007 
0.007 
0.006 
0.003 
20 
0.004 
0.002 
0.001 
0.000 
0.000 
21 
0.006 
0.007 
0.006 
0.005 
0.002 
21 
0.003 
0.002 
0.001 
0.000 
0.000 
22 
0.006 
0.006 
0.006 
0.005 
0.002 
22 
0.003 
0.002 
0.001 
0.000 
0.000 
23 
0.005 
0.006 
0.005 
0.004 
0.001 
23 
0.003 
0.002 
0.001 
0.000 
0.000 
24 
0.005 
0.005 
0.005 
0.004 
0.001 
24 
0.003 
0.002 
0.001 
0.000 
0.000 
25 
0.005 
0.005 
0.004 
0.003 
0.001 
25 
0.003 
0.001 
0.001 
0.000 
0.000 
26 
0.004 
0.005 
0.004 
0.003 
0.001 
30 
0.002 
0.001 
0.000 
0.000 
0.000 
27 
0.004 
0.004 
0.004 
0.003 
0.001 
28 
0.004 
0.004 
0.003 
0.002 
0.000 
29 
0.004 
0.004 
0.003 
0.002 
0.000 
30 
0.004 
0.004 
0.003 
0.002 
0.000 
35 
0.003 
0.003 
0.002 
0.001 
0.000 
40 
0.002 
0.002 
0.002 
0.001 
0.000 
45 
0.002 
0.002 
0.001 
0.001 
0.000 
50 
0.002 
0.002 
0.001 
0.000 
0.000 
2 
4 
8 
16 
00 
2 
4 
8 
16 
00 
n 
.8 
1.6 
3.2 
6.4 
n 
.8 
1.6 
3.2 
6.4 
4 
4 
0.128 
0.082 
0.055 
0.040 
0.026 
8 
8 
0.062 
0.025 
0.009 
0.004 
0.001 
5 
0.102 
0.098 
0.088 
0.077 
0.061 
9 
0.060 
0.040 
0.022 
0.012 
0.003 
6 
0.080 
0.093 
0.098 
0.097 
0.061 
10 
0.054 
0.047 
0.034 
0.023 
0.008 
7 
0.064 
0.082 
0.095 
0.103 
0.111 
II 
0.048 
0.049 
0.043 
0.034 
0.017 
8 
0.052 
0.070 
0.086 
0.098 
0.116 
12 
0.043 
0.049 
0.048 
0.043 
0.028 
9 
0.044 
0.060 
0.076 
0.089 
0.111 
13 
0.038 
0.047 
0.051 
0.051 
0.040 
10 
0.037 
0.051 
0.066 
0.078 
0.100 
14 
0.034 
0.044 
0.052 
0.055 
0.052 
II 
0.032 
0.044 
0.056 
0.067 
0.086 
15 
0.031 
0.041 
0.051 
0.057 
0.063 
12 
0.027 
0.038 
0.048 
0.057 
0.071 
16 
0.028 
0.038 
0.049 
0.058 
0.071 
13 
0.024 
0.033 
0.041 
0.047 
0.058 
17 
0.025 
0.035 
0.046 
0.056 
0.076 
14 
0.021 
0.029 
0.035 
0.040 
0.044 
18 
0.023 
0.033 
0.043 
0.054 
0.077 

SELECTED TABLES OF PROBABILITY DISTRIBUTIONS 
189 
TABLE 83. (Continued) 
2 
4 
8 
16 
ro 
2 
4 
8 
16 
ro 
n 
.8 
1.6 
3.2 
6.4 
n 
.8 
1.6 
3.2 
6.4 
15 
0.019 
0.025 
0.030 
0.033 
0.033 
19 
0.021 
0.030 
0.041 
0.051 
0.076 
16 
0.017 
0.022 
0.026 
0.027 
0.025 
20 
0.019 
0.028 
0.038 
0.047 
0.072 
17 
O.ot5 
0.020 
0.022 
0.023 
0.019 
21 
O.ot8 
0.026 
0.035 
0.044 
0.066 
18 
0.014 
0.017 
0.019 
0.019 
0.014 
22 
0.017 
0.024 
0.032 
0.040 
0.060 
19 
0.013 
0.016 
0.017 
0.016 
0.010 
23 
O.ot5 
0.022 
0.029 
0.037 
0.053 
20 
0.012 
0.014 
O.ot5 
0.013 
0.007 
24 
0.014 
0.020 
0.027 
0.033 
0.045 
21 
0.011 
0.013 
0.013 
0.011 
0.005 
25 
0.013 
0.019 
0.025 
0.030 
O.o38 
22 
0.010 
0.011 
0.011 
0.009 
0.003 
26 
0.012 
O.ot8 
0.023 
0.027 
0.032 
23 
0.009 
0.010 
0.010 
0.008 
0.002 
27 
0.012 
0.016 
0.021 
0.024 
0.026 
24 
0.008 
0.009 
0.009 
0.007 
0.002 
28 
0.011 
O.ot5 
0.019 
0.022 
0.021 
25 
0.008 
0.009 
0.008 
0.006 
0.001 
29 
0.010 
0.014 
O.ot8 
0.020 
0.017 
26 
0.007 
0.008 
0.007 
0.005 
0.001 
30 
0.010 
0.013 
0.016 
0.018 
0.013 
27 
0.007 
0.007 
0.006 
0.004 
0.001 
31 
0.009 
0.012 
O.ot5 
0.016 
0.011 
28 
0.006 
0.007 
0.005 
0.003 
0.000 
32 
0.009 
0.012 
0.014 
0.014 
0.008 
29 
0.006 
0.006 
0.005 
0.003 
0.000 
33 
0.008 
0.011 
0.013 
0.013 
0.006 
30 
0.006 
0.006 
0.004 
0.002 
0.000 
34 
0.008 
0.010 
0.012 
0.011 
0.005 
32 
0.005 
0.005 
0.004 
0.002 
0.000 
35 
0.008 
0.010 
0.011 
0.010 
0.004 
34 
0.005 
0.004 
0.003 
0.001 
0.000 
36 
0.007 
0.009 
0.010 
0.009 
0.003 
36 
0.004 
0.004 
0.002 
0.001 
0.000 
37 
0.007 
0.009 
0.009 
0.008 
0.002 
38 
0.004 
0.003 
0.002 
0.001 
0.000 
38 
0.007 
0.008 
0.008 
0.007 
0.001 
40 
0.003 
0.003 
0.002 
0.001 
0.000 
40 
0.006 
0.007 
0.007 
0.006 
0.001 
45 
0.003 
0.002 
0.001 
0.000 
0.000 
45 
0.005 
0.006 
0.005 
0.004 
0.000 
50 
0.002 
0.002 
0.001 
0.000 
0.000 
50 
0.004 
0.004 
0.004 
0.002 
0.000 
55 
0.002 
0.001 
0.001 
0.000 
0.000 
55 
0.003 
0.004 
0.003 
0.001 
0.000 
60 
0.002 
0.001 
0.000 
0.000 
0.000 
60 
0.003 
0.003 
0.002 
0.001 
0.000 
2 
4 
8 
16 
ro 
2 
4 
8 
16 
ro 
n 
0.4 
0.8 
1.6 
3.2 
n 
0.4 
0.8 
1.6 
3.2 
4 
4 
0.038 
O.ot8 
0.009 
0.005 
0.002 
8 
8 
0.014 
0.003 
0.001 
0.000 
0.000 
5 
0.041 
0.029 
0.019 
0.012 
0.005 
9 
O.ot8 
0.007 
0.002 
0.001 
0.000 
6 
O.o38 
0.034 
0.026 
0.020 
0.010 
10 
0.019 
0.011 
0.004 
0.001 
0.000 
7 
0.034 
0.035 
0.032 
0.026 
0.016 
II 
0.019 
0.013 
0.006 
0.002 
0.000 
8 
0.030 
0.035 
0.035 
0.032 
0.023 
12 
0.019 
O.ot5 
0.009 
0.004 
0.000 
9 
0.027 
0.034 
0.036 
0.035 
0.029 
13 
O.ot8 
0.016 
0.011 
0.006 
0.001 
10 
0.024 
0.032 
0.036 
O.o38 
0035 
14 
0.017 
0.017 
0.012 
0.008 
0.001 
II 
0.022 
0.030 
0.036 
0.039 
0.040 
15 
0.016 
0.017 
0.014 
0.009 
0.002 
12 
0.020 
0.028 
0.035 
0.039 
0.044 
16 
O.ot5 
0.017 
O.ot5 
0.011 
0.003 
13 
O.ot8 
0.026 
0.033 
0.039 
0.047 
17 
o.ot5 
0.017 
0.016 
0.013 
0.004 
14 
0.017 
0.024 
0.032 
0.038 
0.049 
18 
0.014 
0.017 
0.017 
O.ot5 
0.005 
15 
0.016 
0.023 
0.030 
0.037 
0.050 
19 
0.013 
0.017 
O.ot8 
0.016 
0.007 
16 
0.014 
0.021 
0.028 
O.o35 
0.050 
20 
0.013 
0.016 
O.ot8 
0.017 
0.009 
17 
0.013 
0.020 
0.027 
0.034 
0.049 
21 
0.012 
0.016 
O.ot8 
O.ot8 
0.011 
18 
0.013 
0.019 
0.025 
0.032 
0.048 
22 
0.011 
O.ot5 
O.ot8 
0.019 
0.013 
19 
0.012 
0.017 
0.024 
0.030 
0.046 
23 
0.011 
O.ot5 
O.ot8 
0.020 
O.ot5 
20 
0.011 
0.016 
0.022 
0.029 
0.044 
24 
0.010 
0.014 
O.ot8 
0.020 
O.ot8 
21 
0.010 
O.ot5 
0.021 
0.027 
0.041 
25 
0.010 
0.014 
O.ot8 
0.021 
0.020 
22 
0.010 
0.014 
0.020 
0.025 
0.038 
26 
0.009 
0.014 
0.018 
0.021 
0.022 
23 
0.009 
0.014 
0.019 
0.024 
0.036 
27 
0.009 
0.013 
0.017 
0.021 
0.024 
24 
0.009 
0.013 
O.ot8 
0.022 
0.033 
28 
0.009 
0.013 
0.017 
0.021 
0.026 

190 
APPENDIX B 
TABLE 83. (Continued) 
2 
4 
8 
16 
OCJ 
2 
4 
8 
16 
OCJ 
n 
0.4 
0.8 
1.6 
3.2 
n 
0.4 
0.8 
1.6 
3.2 
25 
0.008 
0.012 
0.017 
0.021 
0.030 
29 
0.008 
0.012 
0.017 
0.021 
0.028 
26 
0.008 
0.012 
0.016 
0.020 
0.027 
30 
0.008 
0.012 
0.016 
0.021 
0.029 
28 
0.007 
0.010 
0.014 
0.017 
0.022 
31 
0.008 
O.Oll 
0.016 
0.020 
0.031 
30 
0.007 
0.010 
0.013 
O.o!5 
O.o!8 
32 
0.007 
O.Oll 
O.o!5 
0.020 
0.032 
32 
0.006 
0.009 
O.Oll 
0.013 
0.014 
33 
0.007 
O.Oll 
O.o!5 
0.020 
0.033 
34 
0.006 
0.008 
0.010 
0.012 
O.Oll 
34 
0.007 
0.010 
O.ol5 
0.019 
0.033 
36 
0.005 
0.007 
0.009 
0.010 
0.008 
36 
0.006 
0.010 
0.014 
O.o!8 
0.033 
38 
0.005 
0.007 
0.008 
0.009 
0.006 
38 
0.006 
0.009 
0.013 
0.017 
0.033 
40 
0.005 
0.006 
0.008 
0.008 
0.005 
40 
0.006 
0.009 
0.012 
0.016 
0.031 
45 
0.004 
0.005 
0.006 
0.006 
0.002 
45 
0.005 
0.007 
0.010 
0.014 
0.025 
50 
0.003 
0.004 
0.005 
0.005 
0.001 
50 
0.004 
0.006 
0.009 
0.012 
0.019 
55 
0.003 
0.004 
0.004 
0.004 
0.001 
55 
0.004 
0.006 
0.008 
0.010 
0.013 
60 
0.003 
0.003 
0.003 
0.003 
0.000 
60 
0.003 
0.005 
0.007 
0.008 
0.008 
65 
0.003 
0.004 
0.006 
0.007 
0.005 
70 
0.003 
0.004 
0.005 
0.006 
0.003 
75 
0.003 
0.004 
0.004 
0.005 
0.001 
80 
0.002 
0.003 
0.004 
0.004 
0.001 

Appendix 
C 
Fortran Program to 
Implement Example Given 
in Chapter 7 
191 

192 
APPENDIX C 
DIMENSION YV<46),F(46>,G<46>,P1C46),P2C46) 
DIMENSION B1<46),B2<46),SGC46>,PAC46,46>,PBC46>,PD<46> 
DIMENSION PF<46),SLC46> 
REAL NH, Ll, L2, LS, LV, LIKE<46,46,46>, LT 
READ (6,7886) IPRI 
C 
ENTER LOWER AND UPPER LIMITS ON 81, PLUS INTERVAL 
READ <6,7881> L1,T1,D1 
M1•<T1-L1l/D1+1,881 
WRITE (6,8888) L1,T1,D1,M1 
DO 188 11•1.H1 
188 81CI1>•L1+(I1-1>*D1 
C 
ENTER LOWER AND 
UPPER LIMITS ON 82, PLUS INTERVAL 
WRITE <6,7868) <Bl<I>.I•l.Hl> 
READ <6,7881> L2,T2,02 
H2•<T2-L2l/D2+1.881 
WRITE <6,8888) L2,T2,D2,H2 
DO 168 I2•1,H2 
168 82<I2>•L2+<I2-1>*D2 
C 
ENTER LOWER AND UPPER LIMITS ON SIGMA, PLUS INTERVAL 
WRITE <6,7868) <B2<I>.I•l,H2> 
READ <6,7881) LS,TS,DS 
HS•<TS-LS>IDS+1.881 
LT•ALOG<LS> 
TT•ALOG<TS> 
DT•<TT-LT>/<HS-1> 
WRITE <6,8888) LS,TS,OS,HS 
DO 288 IS•l,HS 
SL<IS>•LT+<IS-l>*DT 
288 SG<IS>•EXP<SL<IS>> 
WRITE <6,7868> <SG<I>,I•1,MS> 
C 
CALCULATE PRIOR 
IF <IPRI.E0.1> CALL VPRIOR<Ml,M2,MS,B1,82,SG,LIKE> 
IF <IPRI.EQ.2) CALL PRIOR2CM1,H2,HS,B1,82,SG,LIKE> 
C 
ENTER STATISTICS 
READ <6,7882> N,VZ,YN,VB,V2,VL 
WRITE (6,8812> N,VZ,YN,YB,Y2,YL 
C 
IF NO OBSERVATION THEN POSTERIORsPRIOR 
IF <N.EQ.8 .AND. YZ.EQ.8.) GOTO 278 
CC•N*YB+YZ-YN 
BC•N*Y2+YZ*VZ-YN*YN 
00 268 Il•l,Ml 
DO 268 12•1,H2 
C 
DON'T DIVIDE BY ZERO 
IF <ABS<B2<I2>>.GE.1.> GOTO 218 
B•<l.-B2<12)*B2<I2>>"<VZ-BlCill/Cl.-B2Cl2)))**2 + 
1 
N*CY2+81CI1l*B1Cll)l+B2CI2>*B2<I2>*BC 
2 
-2.*B1Cl1)*CVB*N-B2CI2>*CC>-2.*N*YL*B2CI2> 
DO 268 IS • 1,HS 
GOTO 228 
218 DO 212 IT•l,HS 
212 LIKE<l1,I2,IT>•8. 
GOTO 268 
228 S•SQRT<1.-B2<I2>*B2<I2>>1SGCIS>**(N+1) 
D•S*EXP<BI<-2.*SG<IS>*SG<IS>>> 
LIKECI1,12,IS>•D"LIKE<I1,12,IS> 
268 CONTINUE 
C 
POSTERIOR DENSITY. NOT NORMALIZED. STORED AS LIKE 
C 
CALCULATE NORMALIZING FACTOR NH 
278 DO 388 I1•1,H1 
DO 298 I2•1,H2 
DO 288 IS•1,HS 
C 
MULTIPLY BY SIGMA TO INTEGRATE WITH RESPECT TO LOG SIGMA 
LIKE<I1,I2,IS>•LIKE<Il,I2,IS)*SG<IS> 
288 FCIS>•LIKECI1,I2,IS> 
PACI1,I2>•SHPSCF,HS,LT,TT> 
298 F<I2>•PACI1,I2> 
388 PB<I1>•SHPS<F.H2,L2,T2> 
NH • SHPS<PB,H1,L1,T1> 
WRITE <6,7888) NH 
C 
PA AND PB ARE MARGINAL DENSITIES <WHEN NORMALIZED>. 
OTHER 
C 
MARGINAL DENSITIES, VARIOUS POSTERIOR EXPECTATIONS WILL NOW 
C 
BE CALCULATED. 
DO 316 I2•1,H2 
DO 316 Il•l.H1 
316 F<I1>•PA<I1,I2> 
88888818 
88888828 
88888838 
88888848 
88888868 
88888868 
88888878 
88888888 
88888898 
88888188 
88888118 
88888128 
88888138 
88888148 
88888168 
88888168 
88888178 
88888188 
88888198 
88888288 
88888218 
88888228 
88888238 
88888248 
88888268 
88888268 
88888278 
88888288 
88888298 
88888388 
88888318 
88888328 
88888338 
888ilil348 
88il8il368 
il88il8368 
il8ill!il37il 
l!il898388 
88888398 
il8888488 
98888418 
8888842il 
88888438 
88il88448 
8il8il8468 
il8ililil468 
il8ilil8478 
88ilil8488 
888il8498 
88ililil688 
88888618 
8il888528 
81l888538 
88888648 
81l881l568 
88888568 
81l888678 
8illlll868il 
88881l598 
88888688 
81l888618 
81l888628 
88888638 
8888864B 
88881l668 
88888668 
88888678 
88889688 
88il88698 
88888788 
89888718 
89888728 
88888738 
88888748 
88888768 

FORTRAN PROGRAM 
316 PF<I2l•SHPS(F,H1,L1,T1l/NM 
328 DO 488 IS•1,HS 
DO 398 I1•1,H1 
DO 388 I2•1,H2 
388 F<I2>•LIKE<I1,12,IS> 
398 G<I1>•SHPS<F,H2,L2,T2> 
488 PD<IS>•SHPS<G,H1,L1,T1l/NH 
C 
MARGINAL DENSITIES DONE. 
NEXT GET MARGINAL EXPECTATIONS. 
DO 438 Il•1,H1 
c 
438 F<I1>•81<I1l"P8<I1> 
X1•SHPS(F,H1,L1,T1> 
DO 448 I1•1,H1 
448 F<I1>•81<I1>•F<I1> 
V1•SHPS<F,H1,L1,T1>-X1"X1 
DO 468 I2•1,H2 
468 F<I2>•82<I2>"PF<I2> 
X2•SHPS<F,H2,L2,T2> 
DO 468 I2•1,H2 
468 F<I2>•F<I2>"82CI2> 
V2•SHPS<F,H2,l2,T2>-X2"X2 
DO 478 IS•1,HS 
478 F<IS>•SG<IS>•PD<IS> 
XS•SHPS<F,HS,LT,TT> 
DO 488 IS•1,HS 
G<IS>•PD<IS)/SG(IS> 
488 F<IS>•F<IS>"SG<IS> 
VS•SHPS<F,HS,LT,TT>-XS"XS 
SV1•SORT< V1 > 
SV2•SORT<V2> 
SVS•SORT<VS> 
WRITE <6,8881) (81(11),P8<I1>,I1•1,Hl> 
WRITE <6,8811) X1,SV1 
WRITE <6, 8882> <B2<I2l,PF<I2l,I2•1,H2> 
WRITE (6,8811) X2,SV2 
WRITE < 6,8883) <SG< ISl,G<IS>, IS•l,HS> 
WRITE <6,8814) <SL<ISl,PD<ISl,IS•l,HS> 
WRITE <6,8811> XS,SVS 
C 
ENTER 
UPPER AND LOWER LIMITS, INTERVAL FOR CALCULATION OF 
C 
PREDICTIVE DENSITIES 
666 READ <6,7883) UY,LV,NY 
DY•<UV-LV>INV 
NY•NV+1 
DO 688 1•1,NV 
VV<l>•LV+OY"<l-1) 
DO 698 IS•1,HS 
DO 686 I2•1,H2 
IF CABS<82<I2>>.GT •• 99) GOTO 684 
EY•B1<I1l/(1.-82<I2>> 
SD•SG<IS>ISORT<1.-82(I2>"82(I2>> 
S8•SD"<1.-82<I2>> 
IF <IPRI.E0.1 .AND. N.EQ.8 .AND. VZ.EQ.8,) GOTO 681 
678 DO 688 I1•1,H1 
IF <N.EQ.8 .AND. VZ.EQ.8,) GOTO 676 
F<I1>•<LIKE<I1,I2,IS>INH>•EXP<-<<YY<I>-81<I1>-82<I2)0 
1 
VN>""2l/(2, 0SG<IS>""2l)/(SQRT<6.28314l"SG<IS>> 
GOTO 688 
676 CONTINUE 
ARG•<YV<I>-EV>""2/(2."SD"SD> 
IF <ARG.GT.98l GOTO 678 
F<I1>•<LIKE<I1,I2,ISl/NM)"EXP<-ARGl/(SQRT<6.28314l"SDl 
GOTO 688 
678 F<I1>•8. 
688 CONTINUE 
G<I2>•SHPS<F,H1,L1,T1l 
GOTO 686 
681 CONTINUE 
V8•VV<I>"<1.-82<I2>> 
J•<V8-L1+D1l/D1 +,6 
IF <J.GT.H1) J•H1 
IF <J.LE.1> J•2 
ARG1 • <18.-V8)/(S8"1.414> 
ARG2 • -V8/(S8"1.414> 
G<I2>•.6"(1.-82<I2>>"<ERF<ARG1l-ERF<ARG2>>"LIKE<J,I2,IS)/NH 
GOTO 686 
684 G<I2>•8. 
686 CONTINUE 
698 P1<IS>•SHPS(G,H2,L2,T2> 
P2<I>•SHPS<P1,HS,LT,TT> 
193 
88888768 
88888778 
88888788 
88888798 
88888888 
88888818 
88888828 
88888838 
88888848 
88888868 
88888868 
88888878 
88888888 
88888898 
88888988 
88888918 
88888928 
88888938 
88888948 
88888968 
88888968 
88888978 
88888988 
88888998 
88881888 
88881818 
88881828 
88881838 
88881848 
88881868 
88881868 
88881878 
88881888 
88881898 
88881188 
88881118 
811111111211 
88881138 
88881148 
88881168 
88881168 
88881178 
811881188 
88881198 
88881288 
88881218 
88881228 
88881238 
88881248 
88881268 
88881268 
88881278 
881181288 
888111298 
88881388 
88881318 
88881328 
88881338 
88881348 
88881358 
88881368 
88881378 
88881388 
88881398 
88881488 
88881418 
88881428 
88881438 
88881448 
88881468 
88881468 
88881478 
88881488 
88881498 
88881688 
88881618 
88881628 
881181638 
88881648 

194 
SUBROUTINE VPRIORCM1,H2,H3,X1,X2,X3,F> 
DIMENSION X1<4&>,X2<4&>,X3C4&>,FC46,46,46>,E<4&>,F2C4&> 
ST•ALOG<X3<1>> 
TT•ALOGC X3< M3)) 
DO 4 I2•1,M2 
G•(1,+X2CI2>>""14.68 • C1.-X2CI2>>••9.89 
H•18. 
IF <X2CI2>.LT.1.> H•SQRT<<1.+X2CI2>>1<1.-X2CI2>>> 
DO 2 I1•1,M1 
HX•H"Xl< I1 )/3. 
DO 68 I3•1,M3 
E< I3>•8. 
IF <X3CI3l.LT.HX> E<I3>•1./X3CI3> 
F2CI3>•ECI3>"X3CI3> 
68 CONTINUE 
S•SMPSCF2,M3,ST,TT> 
IF <S .EO. 8.) S•1. 
DO 1 I3•1,M3 
F<I1,I2,I3>•G"ECI3>/S 
1 CONTINUE 
2 CONTINUE 
4 CONTINUE 
RETURN 
END 
SUBROUTINE PRIOR2CM1,M2,M3,X1,X2,X3,F> 
DIMENSION X1C46>,X2C46),X3C46l,FC46,46,46) 
DIMENSION F1<46l,F3<46> 
DO 18 I2•1,M2 
IF 
CX2<I2l.EQ.1 •• OR. X2CI2>.E0.-1.> GOTO 41 
C•188. 
IF CX2CI2l.LT.1.> C•C1.+X2CI2>>1<1.-X2CI2>> 
H•SORTCC> 
G•FNRMCX2CI2>,8.,42.684> 
GOTO 48 
41 DO 42 Il•1,M1 
DO 42 I3•1,M3 
42 F<I1,I2,I3>•8. 
GOTO 18 
48 DO 9 Il•1,M1 
IF <X1CI1>.EQ.8.> GOTO 16 
F1CI1>•FNRH<X1CI1>,3."C1.-X2<I2)),6.441/C1.-X2CI2>>••2) 
DO 8 I3•1,M3 
F3CI3>•FNRHCX3CI3>,.16"X1<11)"H,C.8384"H"X1<I1>>••(-2)) 
8 F<I1,I2,I3>•G"F1CI1>"F3Cl3> 
GOTO 9 
16 DO 16 I3•1,M3 
16 FCI1,I2,I3>•8. 
9 CONTINUE 
18 CONTINUE 
RETURN 
END 
FUNCTION FNRM<A,B,C> 
ARG•CB-A> 
ARG•.6"ARG•ARG•C 
FNRM-8. 
IF CARG.LT.128.> FNRM-SORT(C/6.283186>"EXP<-ARG> 
RETURN 
END 
APPENDIX C 
88881948 
88881968 
88881968 
88881978 
1188111988 
811881998 
1111118211118 
88811211111 
1111111121128 
81111821138 
11111182848 
8118821168 
811882868 
81111828711 
888821188 
11118828911 
1111118211111 
8811112118 
811111121211 
111111112138 
811111121411 
8111182168 
111111821611 
1111111121711 
1111882188 
11811112198 
1188822118 
881182218 
8811112228 
11111182238 
11811112248 
8811112268 
8111182268 
1111882278 
1188112288 
888822911 
81111823118 
118882318 
81111823211 
1188823311 
118882348 
118882368 
118882368 
8811823711 
118882388 
81111112398 
8118824811 
1111882418 
118882428 
1188824311 
11111182448 
88882468 
111111824611 
811882478 
88882488 
88882498 
11111182688 
111181126111 

FORTRAN PROGRAM 
195 
WRITE <6,7818>I,YY<I>,P2<I> 
88881668 
688 CONTINUE 
88881668 
WRITE (6,8889) <YY<I>,P2<I>,I•1,NY> 
88881578 
7881 FO~T<3F5.2> 
88881588 
7882 F~T <I6,6F7.3> 
88881598 
7883 FO~T<2F5.2,I5> 
88881688 
7886 FO~T <I6) 
88881618 
7818 FO~T <I18,F18.2,F18.4> 
88881628 
7868 FO~T <12F18.3> 
88881638 
7888 FO~T <6H NM • , E18.3> 
88881648 
8888 FO~T (3F8.3,18) 
88881668 
8881 FO~T<1H1,'POSTERIOR MARGINAL PROBABILITY DENSITIES ON 81'///6X, 88881668 
1 '81 
DENSITY'//CF6.2,F11.4)) 
88881678 
8882 FO~T(////' POSTERIOR MARGINAL PROBABILITY DENSITIES ON B2'///6X,88881688 
1 'B2 
DENSITY'//(f6.2,F11.4>> 
88881698 
8883 FO~TC////' POSTERIOR MARGINAL PROBABILITY DENSITIES ON SIGMA'// 88881788 
1 ' 
SIGMA 
DENSITY'//CF6.2,F11.4)) 
88881718 
8889 FO~T(/////' PREDICTIVE DISTRIBUTION OF NEXT OBSERVATION'/// 
88881728 
1 6X,'Y 
DENSITY'//(F6.2,F11.4)) 
88881738 
8811 FO~T (28X,'MEAN • ',F7.3,', 
STANDARD DEVIATION • ',F7.3) 
88881748 
8812 FO~T CI8,5F8.3> 
88881768 
8814 FO~TC////' POSTERIOR MARGINAL PROBABILITY DENSITIES ON LOG SIG'/88881768 
1//' LOG SIGMA DENSITY'//CF6.2,F11.4>> 
88881778 
STOP 
88881788 
ENO 
88881798 
FUNCTION SHPS<F,N,ST,FN> 
DII£HSION F<68> 
IF <<Nt2>•2.EQ.N) N•H-1 
N1•N-1 
N2•H/2 
S•F< 1 )+f(H) 
DO 1 I•2,N1 
1 S•S+2.•F<I> 
DO 2 I•1,N2 
2 S•S+2. •F<2•I> 
S•S•<FN-ST)/(3.•N1> 
SHPS•S 
RETURN 
END 
88881888 
88881818 
88881828 
88881838 
88881848 
88ll81868 
88881868 
88881878 
88881888 
88881898 
88881988 
88881918 
88881928 
88881938 

REFERENCES 
DeGroot, M. H., 1970: Optimal Statistical Decisions, McGraw-Hill, 489 pp. 
Lenton, R. L., I. Rodriguez-Iturbi and J. C. Schaake, Jr., 1974: The estimation of pin the 
first-order autoregressive model: A Bayesian approach. Water Resour. Res .. 10, 227-
241. 
Madden, R. A., 1976: Estimates of the natural variability of time-averaged sea-level pressure. 
Mon. Wea. Rev., 104, 942-952. 
Tribus, M., 1969: Rational Descriptions, Decisions and Designs. Pergamon, 478 pp. 
Winkler, R. L., 1972: An Introduction to Bayesian Inference and Decision. Holt, Rinehart 
and Winston, 563 pp. 
197 

INDEX 
Autocorrelation, 141, 146, 147 
Autoregression data-generating process, 139-
165 
Bayes' Theorem, 6, 20-22, 27, 34, 35, 37, 
60, 80,81-82,88,90 
Bernoulli process, 29-51, 55, 68, 88 
Beta distribution, 37-40, 41, 43, 70, 146, 
147-148, 150, 155 
Beta-binomial distribution, 48, 49-50 
Beta-Pascal distribution, 49, 50 
Binomial distribution, 32, 50, 55, 78 
Bivariate distributions, 81 
Bivariate normal distribution, 112 
Bivariate normal-gamma distribution, 113, 
118 
Central limit theorem, 77 
Chi-squared distribution, 65, 93, 99, 103, 
126, 151 
Coefficient of variation, 71, 150 
Conditional probability, 19, 81, 121 
Conjugate distributions, 9, 26, 143 
Bernoulli process, 37 
normal process, precision known, 82 
normal process, precision unknown, 88-
91 
Poisson process, 61 
simple linear regression, 113 
uniform data-generating process, 28 
Correlation, 116, 139 
Correlation coefficient, 99, 125, 141 
Covariance, 110-112, 113, 141 
Credible interval, 65, 93, 100, 103, 116, 125, 
127-130, 150-151 
Data-generating process, 7-9, 21, 26, 106, 
107, 140 
Diffuse prior (see vague prior) 
Error function, 86, !55 
Exceedences, 24, 30 
Expected value, defined, 16 
Extrapolation, Ill, 120 
F-distribution, 70, 75, 96, 104, 121 
Fiducial interval (see credible interval) 
First-order autoregression, 139-165 
Fractiles, -;14. 63, 65, 92 
Frost occurrences, 35-37, 46-47 
Gamma distribution, 58, 59,61-66, 71-72, 
73, 80, 88, 91, 93,94-95,99, 109, 151 
Geometric distribution, 14, 16, 17 
198 
Heating degree days, 86-88, 98-104 
Hypothetical future data, 46, 66, 129-131, 
136, 151 
Independence,20 
of prior and posterior parameters, 81-82, 
110, 144 
of statistics, 80-81, 94, 109, 118 
Inverted beta-2 distribution, 70, 72-73, 96, 
121, 122 
Inverted gamma-! distribution, 92, 102 
Inverted gamma-2 distribution, 93 
Joint probabilities, process parameters, 81, 
129, 144-145, 147, 149, 151, 153 
Joint probability, 18 
observations of autoregressive series, 142 
observations of Bernoulli process, 31 
observations of geometric data-generating 
process, 24 
observations of normal data-generating 
process, 79, 80 
observations of simple linear regression, 
107, 118 
observations of uniform data-generating 
process, 24 
Least squares, 108, 114, 133 
Likelihood, 21-22, 23, 25, 27, 89, 116 
autoregression, 143, 153 
Bernoulli process, 32, 34 
geometric data-generating process, 24 
normal data-generating process, 80, 88 
Poisson process, 58 
simple linear regression, 107 
Marginal probability, 18, 81, 91, 94-95, 96, 
121 
Maximum likelihood, 109 
Mode,62,63 
Model output statistics (MOS), 106 
Moments, 17 
fitting by, 42, 147 
Negative binomial distribution, 33, 67-68, 
69, 72, 74, 75 
Normal data-generating process, 77-104, 
105, 109, 114 
Normal distribution, 17, 79, 82-83, 84, 85, 
89, 105, 109, 141, 146, 150, 151 
Normal-gamma distribution, 90, 91-95,99, 
109-110, 113, 144, 146 
Null prior (see vague prior) 
Pareto distribution, 27-28 
Pascal distribution, 33, 50 

INDEX 
Percentiles (see fractiles) 
Poisson distribution, 56-58, 60, 68, 72, 74, 
78 
Poisson process, 53-75, 88 
Posterior mean and variance, autoregression, 
160, 163-165 
Bernoulli process, 41, 46 
normal process, 92, 102 
simple linear regression, 110-111, 114 
Posterior parameters, 26 
Bernoulli process, 38 
normal process, precision known, 83 
normal process, precision unknown, 90-
91, 102 
Poisson process, 61, 75 
simple linear regression, 113 
Posterior probability, 21, 25, 116, 143, !52 
compared to prior probability 39, 60, 87, 
102-103, 156-159, 161-165 
Precipitation, 78, 83 
Precision, 79, 85, 95, 106, 118, 140 
of prediction, 88 
Predictive mean and variance, autoregres-
sion, 156, 159, 162, 165 
Bernoulli process, 48-49 
linear regression, 119-120, 121-122 
normal process, precision known, 85 
normal process, precision unknown, 94, 
97, 101 
Poisson process, 68, 70, 72, 75 
Predictive probability, 21-22 
autoregression, 142, 156, 153, 156, 162 
Bernoulli process, 46-49, 49-51 
linear regression, I 18-123 
normal process, precision known, 84-88 
normal process, precision unknown, 94-
98, 101, 104 
Poisson process, 60, 67-72, 72-75 
role of prior and posterior probabilities, 
22,68 
Prior mean and variance, autoregression, 161 
Bernoulli process, 43 
linear regression, 130 
normal process, 10 I 
Poisson process, 62, 72 
Prior parameters, 26, 39, 100 
as hypothetical prior observations, 39, 41, 
61, 5/8, 125, 127-129, 136 
assessment of, 40-47, 62-67, 72, 86, 98-
100, 124-131, 135-137 
Prior probability, 21, 25, 33, 116 
assessment of, 34, 146-151 
Probability as a relative frequency, 4 
Probability density, 13, 15 
Probable error, 86 
Process parameters, 8-9, 25 
autoregression, 140 
Bernoulli process, 33 
199 
normal data-generating process, 79, 81 
normal linear regression, 106 
Poisson process, 54 
Quartile (see fractile) 
Random variable, 11-12, 31, 105, 107, 140 
Regression, 105-138 
linear, I 06, 140 
multiple, 106, 112, 117 
Regression coefficients, 106, 107, 124, 133 
Regression line (or regression equation), 105, 
107, 110, 120, 124, 129-131, 133-134 
Sample space, 3, II, 80 
Simpson's rule, !52 
Snow cover, 42,49-51, 123-138 
Standard deviation, 17, 63, 93, Ill, 141, 148 
Standard error, 84, 132, 134, 138 
Stationarity, 140 
Streamflow, 146-164 
Student's distribution, 92, 96, I 00, 10 I, 110, 
119-120, 121-122, 126 
Storm surges, 72-75 
Subjective probability, 4, 22 
Sufficient statistics, 23 
Bernoulli process, 32, 38 
geometric data-generating process, 24 
normal data-generating process, 80, 102 
Poisson process, 55 
simple linear regression, 108, 113, 120 
uniform data-generating process, 25 
t-distribution, 92, I 00, 10 I, 103, 120, 126 
Uniform distribution, 14, 15, 17, 24, 26-28, 
!55 
Vague prior, 41, 89, 148-149, !53 
autoregression, 145, 149, 155-161 
Bernoulli process, 41, 42 
normal process, 89 
Poisson process, 62 
simple linear regression, 109, 133, 134, 138 
Variance, 17, 98, 140 
marginal distribution, 9 3 
Weights, scalar and matrix, 114-117, 136 
Winter storms, 59, 68 

