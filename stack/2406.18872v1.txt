Efficacy of Language Model Self-Play in Non-Zero-Sum Games
Austen Liao∗
Nicholas Tomlin∗
Dan Klein
Computer Science Division, University of California, Berkeley
{austenliao, nicholas_tomlin, klein}@berkeley.edu
Abstract
Game-playing agents like AlphaGo have
achieved superhuman performance through
self-play, which is theoretically guaranteed to
yield optimal policies in competitive games.
However, most language tasks are partially
or fully cooperative, so it is an open ques-
tion whether techniques like self-play can ef-
fectively be used to improve language mod-
els. We empirically investigate this question
in a negotiation game setting known as Deal
or No Deal (DoND). Crucially, the objective
in DoND can be modified to produce a fully
cooperative game, a strictly competitive one,
or anything in between. We finetune language
models in self-play over multiple rounds of fil-
tered behavior cloning in DoND for each of
these objectives. Contrary to expectations, we
find that language model self-play leads to sig-
nificant performance gains in both cooperation
and competition with humans, suggesting that
self-play and related techniques have promise
despite a lack of theoretical guarantees.
1
Introduction
Many of the greatest achievements in artificial in-
telligence have occurred in two-player zero-sum
(2p0s) games such as Go (Silver et al., 2016), chess
(Silver et al., 2018), and heads-up poker (Brown
and Sandholm, 2018). One key technique enabling
these breakthroughs has been self-play, in which
identical copies of a model are pitted against each
other and used to generate new training data. By
iteratively training on their own data from games
of self-play, models like AlphaGo and AlphaZero
were able to continue improving long past the
threshold of human performance. In certain types
of 2p0s games, self-play is theoretically guaranteed
to produce optimal policies, given sufficient model
capacity and compute (Bai and Jin, 2020; Bai et al.,
2020). However, in settings that involve collabora-
*Equal contribution.
tion with humans, self-play is no longer guaranteed
to yield optimal policies (Strouse et al., 2021).
It is an open question whether the same princi-
ples that led to the success of models like AlphaGo
can be applied to language models. Empirically,
previous work on training agents to communicate
via self-play has shown that they often invent unin-
terpretable communication strategies (Kottur et al.,
2017); even when initialized with natural language
data, self-play can cause models to gradually di-
verge from human-interpretable language (Lewis
et al., 2017). As a result, much work has focused
on mitigating these challenges, e.g., by regularizing
with models trained on human data (FAIR, 2022).
In this work, we examine the effect of game ob-
jectives on self-play between language models. We
run a series of experiments on a negotiation task
known as Deal or No Deal (Lewis et al., 2017)
and train language models for multiple rounds
of self-play across three different objectives on
this task, ranging from fully cooperative, to semi-
competitive, to strictly competitive. Contrary to
expectations, we find that self-play leads to large
improvements in both the cooperative and semi-
competitive settings. These results generalize to
human experiments, where scores improve by up to
2.5× in the cooperative setting and 6× in the semi-
competitive setting. In contrast, we find minimal
improvements in the strictly competitive setting,
where models tend to overfit during self-play.
We then investigate the reasons behind these im-
provements, finding that models trained with self-
play better follow task instructions, hallucinate less,
and obtain a higher agreement rate with humans.
However, at the same time, self-play causes model
dialogues to become less diverse and does not ap-
pear to teach high-level strategic reasoning or nego-
tiation tactics in our experiments. Although these
results highlight potential room for improvement,
we view them as a promising initial signal for self-
play training of large language models and release
arXiv:2406.18872v1  [cs.CL]  27 Jun 2024

Figure 1: We ran experiments on a modified version of the Deal or No Deal negotiation game from Lewis et al.
(2017). In this game, two players are presented with a shared collection of items and private value functions over
those items. Players can send messages to each other and then each submit private proposals describing the items
they wish to receive. If the proposals are compatible, then the items are scored. In our modified version of the task,
players may receive reward based not only on their own item scores, but on the item scores of the other player as
well. This modification allows us to convert Deal or No Deal into a cooperative or strictly competitive game.
all code for our environments, models, and human
data collection to support future research in this
area: github.com/nickatomlin/lm-selfplay.
2
Cooperative and Competitive Games
Can language model self-play be effective under
both cooperative and competitive objectives? To
address this question, we conducted experiments
on Deal or No Deal (DoND; Lewis et al., 2017),
a two-player negotiation game in which players
decide how to divide a shared pool of items through
natural language dialogue. Although introduced as
a semi-competitive game, DoND has the special
property that it can be readily adapted into either a
cooperative or strictly competitive (i.e., zero-sum)
game. Below, we describe the rules of DoND, how
we modify its objective, and how we convert it into
an environment for evaluating language models.
Game Setup
Following Lewis et al. (2017), we
present two players with a shared collection of
books, hats, and balls (with 5-7 total objects). Each
player is assigned their own private value function,
mapping each item type to an integer point value.
Value functions are selected according to the fol-
lowing criteria: (1) each item is valued by at least
one player, (2) the maximum score either player
can receive is 10, and (3) at most one player can
achieve the maximum score. Players must divide
the objects; if they fail to reach an agreement, they
both receive zero points. These rules ensure that
the game is semi-competitive: players have conflict-
ing objectives, but if they fail to cooperate at all
then they will end up without any points.
Game Rules
The game is divided into two
phases. In the first phase, players send messages
discussing which items they would like to receive.
At any point, either player may end this phase by
submitting a private proposal, delineating which
items they would like to claim from the shared col-
lection. During the second phase, no additional
messages can be sent, and the other player must re-
spond by submitting a proposal of their own, which
ends the game. If players submit complementary
proposals (i.e., adding up to the total number of ob-
jects in the shared collection), then players receive
rewards according to their respective objectives.
Hence, players should aim both to reach an agree-
ment and to optimize the value of that agreement.
Game Objectives
In the original formulation,
players receive a reward equal to the inner product
of their value function and proposed set of objects.
However, we observe that this objective can be
modified to convert DoND into a cooperative game,
a strictly competitive one, or anything in between.
For example: if players receive rewards R1 and R2
in the original setting, instead setting the objective
to R1 + R2 for both players results in a fully co-
operative game. More generally, we can define the
objective for Player 1 as R1+λ·R2 for λ ∈[−1, 1],
and vice versa for Player 2. In this work, we ex-
periment with λ = 0 (semi-competitive), λ = 1
(cooperative), and λ = −1 (strictly competitive),

although we note that in principle λ can be tuned
to smoothly interpolate between these objectives.
The maximum reward that can be obtained in a sin-
gle game is 10 in the strictly and semi-competitive
settings and 19 in the cooperative setting.1
Game Environment
Akin to recent work on lan-
guage agents (Abdulhai et al., 2023; Lin et al.,
2024), we implement an OpenAI Gym-like (Brock-
man et al., 2016) environment for evaluating lan-
guage models on DoND. This environment pro-
vides affordances for (1) generating new random
game instances, (2) prompting language models
with game rules and context, (3) handling messages
and formal proposal actions, (4) computing player
rewards, and (5) sending comprehensive error mes-
sages to models in case they violate the game rules,
e.g., by sending incorrectly formatted proposals.
We provide full details in Appendix A and in our
open-source code release.
3
Language Model Self-Play
We begin by evaluating pretrained language mod-
els, prompted only with task instructions and the
current game context, as detailed in Appendix A.
Because our goal is to analyze the effects of
self-play on language model behavior, we do not
prompt our models with few-shot example dia-
logues or finetune them on task-specific data; doing
so helps us avoid biasing models toward specific
patterns of behavior. We then iteratively finetune
these models over many rounds of self-play.
We implement a straightforward algorithm for
language model self-play based on filtered behavior
cloning (filtered BC; Chen et al., 2020, 2021; Zelik-
man et al., 2022). In this setting, two language mod-
els with identical parameters but different prompts
play K games and receive rewards according to
their (identical) objectives. Each game produces
two dialogue histories, one from each player’s per-
spective. The average score across games is com-
puted and dialogues with above-average scores are
kept and used for finetuning the model. This pro-
cedure is repeated for N iterations or until early
stopping. We set K = 500 and N = 10 for the
majority of our experiments.
1Reward is obtained by adding the item scores from each
player in the cooperative setting. However, the maximum
possible reward is 19 and not 20 because of the game’s con-
straint that at most one player can achieve the maximum item
score. The maximum average self-play reward is 7.5 in the
semi-competitive setting and 15 in the cooperative setting; in
the zero-sum setting, average self-play scores are always zero.
Algorithm 1 Language Model Self-Play
1: Input: Language model M, number of games
per iteration K, number of iterations N, func-
tion exec which runs a game of self-play
2: Output: Finetuned language model M
3: for n = 1 to N do
4:
Initialize an empty set D
5:
Initialize a list of rewards R = []
6:
for k = 1 to K do
7:
▷Obtain dialogues and rewards:
8:
(D1, D2, R1, R2) ←exec(M)
9:
Add (D1, R1) and (D2, R2) to D
10:
Append R1 and R2 to R
11:
end for
12:
Compute the average ¯R =
1
2K
P
r∈R r
13:
Initialize an empty set Dfiltered
14:
for each (D, R) ∈D do
15:
if R > ¯R then
16:
Add D to Dfiltered
17:
end if
18:
end for
19:
Finetune M using dialogues from Dfiltered
20:
If early stopping criteria met then break
21: end for
Because this approach filters out games which
receive a below-average score, it requires a suf-
ficiently capable model to “get off the ground.”
In our preliminary experiments with open-weight
models such as Mixtral 8x7B and LLaMA-2-70B,
we found that they did not achieve enough nonzero
scores to improve from the first round of self-play.
In particular, weaker models struggle to follow
the game instructions and routinely fail to pro-
duce compatible proposals. Therefore, we instead
used GPT-3.5 (gpt-3.5-turbo-0125; Chen et al.,
2021; Ouyang et al., 2022) which strikes a balance
between capability and accessibility. While not
an open-weight model, GPT-3.5 can still be fine-
tuned through the OpenAI API, and it achieves
sufficiently high initial scores to kickstart training.
Due to the high cost of experiments, we leave in-
vestigation of other models to future work.
4
Human Experiments
To evaluate whether model improvements gener-
alize beyond self-play, we built a web interface
which allows humans to play DoND against our
trained models. We evaluated models on both the
cooperative and semi-competitive objectives across
the timecourse of training. However, due to the

0
2
4
6
8
10
Rounds of Self-Play Finetuning
0
2
4
6
8
10
12
Average Game Score
Semi-Competitive (Self-Play)
Semi-Competitive (w/ Humans)
Cooperative (Self-Play)
Cooperative (w/ Humans)
Figure 2: Language model self-play significantly increased model performance in both cooperative and semi-
competitive games. Moreover, these results generalized to collaboration and competition with humans, leading to
improvements of up to 2.5× and 6× the baseline scores, respectively. We found that human-LM baseline scores
were higher in the collaborative setting as humans can help “guide” models to avoid common failure modes.
large number of models we trained and the cost of
human experiments, we only ran human evaluation
on every other iteration of self-play finetuning.
Crowdsourcing
We ran human evaluation on
Amazon Mechanical Turk. After a prescreening
survey and three pilot studies, we identified a group
of 60 reputable English-speaking workers and in-
vited them to participate in our task. In order to in-
centivize high-quality dialogue, we primarily com-
pensated workers with bonus pay: each worker
earned $1.00 for picking up the HIT, $0.10 for each
game played, and $0.20 for each point earned. In to-
tal, we collected 1,175 human-LM dialogues, with
the average worker receiving pay of $37.50. More
details on crowdsourcing, including screenshots of
our web interface, can be found in Appendix C.
5
Results
5.1
Baseline Models
We conducted the majority of our experiments on
GPT-3.5. Prior to self-play finetuning, GPT-3.5 per-
formed relatively poorly, obtaining mean scores of
0.4 in the original, semi-competitive setting and 0.7
in the cooperative setting. These scores are much
lower than those obtained in prior work (Lewis
et al., 2017; Gandhi et al., 2023) because we did not
provide models with few-shot example dialogues
or task-specific finetuning data.
Qualitatively, GPT-3.5’s low scores can primar-
ily be attributed to its inability to consistently reach
complementary proposals with itself in self-play,
reaching a valid agreement with its partner in only
6.8% of games across both objectives. Additionally,
this baseline model relies heavily on error-handling
from the environment to send messages properly
and fails to remain grounded in the game’s context
throughout an entire dialogue, often hallucinating
new items, changes in its value function, or both.
In collaboration with humans, GPT-3.5 obtained
a much higher average score of 4.6. While errors
tend to compound in self-play, we observed that
humans can help “guide” models to avoid common
failure modes in the cooperative setting, resulting
in higher scores (e.g., by suggesting which objects
to propose). However, similar improvements did
not occur in the semi-competitive setting, where hu-
mans are less incentivized to help models perform
well; in the semi-competitive setting, the baseline
GPT-3.5 model achieved a mean score of 0.8.
To establish a stronger baseline, we also evalu-
ated GPT-4 using the same prompts and game en-
vironments as in the rest of our experiments, over
300 games of self-play. GPT-4 achieved a mean
score of 4.3 in the semi-competitive setting and
8.8 in the cooperative setting. Qualitatively, this
model was able to follow the game instructions and
negotiate much more competently than GPT-3.5.
Although we use GPT-4 as a reference point for
model performance, we did not conduct further ex-
periments on it due to the lack of public finetuning
access at the time of experimentation.

GPT-4 (Cooperative)
GPT-4 (Semi-Competitive)
Human (Semi-Competitive)
Model
Self-Play
Human Eval
Self-Play
Human Eval
Self-Play
Human Eval
GPT-3
0.7
4.7
0.4
0.7
0.4
0.7
Finetuned
11.3
11.0
5.5
4.2
5.3
4.8
Iteration 1
11.0
11.7
5.5
4.7
6.0
5.3
Iteration 2
11.4
11.7
5.8
5.8
6.2
5.7
Iteration 3
10.8
10.5
5.6
3.6
6.1
5.5
Table 1: Mean scores of models initially finetuned on task-specific data, which was either generated using GPT-4
self-play or extracted from prior human experiments in Lewis et al. (2017). We generated finetuning data for both
the cooperative and semi-competitive objectives with GPT-4, whereas pre-existing human data was only available
for the semi-competitive setting. After training, models were evaluated both in self-play and via human experiments.
While model scores improved in the earliest iterations of self-play finetuning, performance plateaued and even
declined much earlier than in the experiments without initial training on task-specific data.
5.2
Self-Play Finetuned Models
Despite weak performance of the initial models,
language model self-play was highly effective, as
shown in Figure 2. When evaluated against an-
other copy of the same model, self-play finetuning
increased scores by as much as 14× in the semi-
competitive setting (0.4 →5.8) and 17× in the co-
operative setting (0.7 →12.1). Although these ex-
periments were conducted on GPT-3.5, these scores
are significantly higher than those of a baseline
GPT-4 model, as reported in Section 5.1.
Improvements from self-play generalized to col-
laboration and competition with humans as well,
with scores increasing by 6× (0.8 →4.9) in the
semi-competitive setting and 2× in the coopera-
tive setting (4.6 →8.7). In the cooperative setting,
human-LM scores peaked at 11.6 but began to de-
cline before the 10th iteration of self-play. We do
not report scores for any model after the 10th itera-
tion, as they tended to stabilize or even decline.
The Case of Strict Competition
For the strictly
competitive setting, due to the zero-sum nature of
the game, it is not informative to report mean scores
in self-play (since they would always be zero).
We instead evaluated the quality of trained models
based on how well they performed against a sepa-
rate model, GPT-4. Additionally, due to a sparsity
of positive-scoring games, we modified the filter-
ing criteria for strictly competitive self-play to also
include include samples from zero-scoring games
in which a valid agreement was reached. While
models improved at reaching valid agreements in
self-play, we found they generalized poorly against
other agents. Our preliminary results indicated that
even the best-performing models for this objective
would routinely fail to reach agreements with hu-
mans, so we instead ran 100 games between each
iteration’s model and GPT-4, confirming that the
model failed to improve outside of self-play. We
report results for this objective, along with further
implications, extensively in Appendix D.
5.3
Comparing the Effect of Self-Play to
Task-Specific Finetuning Data
We also considered the case where our initial model
was finetuned on an externally provided corpus of
task-specific data. For the semi-competitive objec-
tive, we finetuned models on 300 nonzero scoring
games from the original human-human dataset in
Lewis et al. (2017). However, because task-specific
data only exists for the original task formulation,
we used GPT-4 to generate a comparable amount
of finetuning data for the cooperative objective;
to enable fair comparison, we also used GPT-4 to
generate finetuning data for the semi-competitive
objective. We finetuned GPT-3.5 on the nonzero
scoring games for each of these three settings and
then repeated the self-play algorithm from Algo-
rithm 1, providing a finetuned GPT-3.5 model as
input instead of the baseline GPT-3.5 model.
Table 1 shows that finetuning on task-specific
data, either provided by humans or distilled from a
stronger model, can lead to performance improve-
ments comparable to those obtained from language
model self-play. However, the benefits of finetuning
and self-play training can be combined. Running
language model self-play on the finetuned models
still improves model performance, but only for 1-
2 iterations, after which scores begins to decline.
This holds across both self-play and human experi-
ments; moreover, we find that score improvements

in self-play generally correlate with score improve-
ments in collaboration or competition with humans.
6
Analysis
6.1
Agreements and Pareto Optimality
In addition to scores increasing with each iteration
of self-play finetuning, models also improved in
their ability to achieve valid agreements (Table 2)
and Pareto-optimal game scores (Table 3), both
in self-play and in human experiments. Trained
models achieved almost perfect agreement rates
in self-play, with 96.4% of games ending in an
agreement in the semi-competitive setting. While
the agreement rates are lower in collaboration with
humans, this can partially (but not wholly) be at-
tributed to human error. For example, we observed
that humans sometimes failed to read overly long
messages in full, resulting in failed proposals.
To understand the relationship between agree-
ment rate and game score, we calculated Pearson
correlation coefficients between completed itera-
tions of self-play and scores achieved, before and
after filtering out samples that failed to reach a
valid agreement. For our self-play data under the
semi-competitive objective, this yielded ρ = 0.44
before filtering and ρ = 0.34 after. On human-LM
data with the same objective, however, the correla-
tion drops from ρ = 0.29 to −0.04 after filtering
out non-scoring games. These results suggest that
improvements in model performance come from an
increase in agreements, which can primarily be at-
tributed to better understanding of task instructions,
following the environment rules, and not hallucinat-
ing items or proposals. Although self-play scores
appear to rise after filtering out games which fail to
reach a proposal, this improvement is a ruse: these
improvements do not generalize to humans.
As a result, even though human-LM scores
improved substantially after self-play finetuning,
there is still a moderate amount of headroom on this
task. More evidence for this headroom comes from
Table 3, which reports the percentage of games that
ended in Pareto-optimal outcomes, i.e., agreements
where it would be impossible for one player to in-
crease their score without the other player’s score
decreasing. Despite improvements from self-play,
models still did not obtain the maximum score in
the majority of games. We expect techniques other
than filtered BC may necessary to close this gap.
Semi-Competitive
Cooperative
Self-Play
Human
Self-Play
Human
Before
6.8
13.3
6.8
32.7
After
96.4
76.5
91.0
64.0
Table 2: Agreement rates (%) before and after ten
rounds of self-play finetuning. Models came to more
valid agreements across objectives in both self-play and
human generalization. We note that the relatively lower
agreement rate for human performance in the coopera-
tive setting can be attributed to performance beginning
to decline before the last iteration, as shown in Figure 2.
Semi-Competitive
Cooperative
Self-Play
Human
Self-Play
Human
Before
2.2
8.9
5.8
16.3
After
46.0
49.0
89.6
38.0
Table 3: Pareto-optimality rates (%) before and after
ten rounds of self-play finetuning. Although model
scores improved substantially, there is still significant
headroom; models still leave many points on the table,
especially in collaboration with humans.
6.2
Dialogue Length and Diversity
We qualitatively observed that dialogues in the
semi-competitive setting became less diverse over
the course of self-play finetuning. We quantified
this in two ways: (1) by average dialogue length,
in Figure 3, and (2) by the number of unique words
produced during each iteration, in Figure 4.
We first computed the average dialogue length
over 500 hundred games of self-play during each
iteration of model training in the semi-competitive
and cooperative settings. We found that self-play
caused dialogues to become substantially longer
in the cooperative setting but shorter in the semi-
cooperative one. We hypothesize that this discrep-
ancy may occur because agents in the cooperative
setting are incentivized to share all information;
qualitatively, we often observed models sharing
exact details of their private value functions. Ad-
ditionally, we found that models which argue with
each other for too long are more likely to “go off the
rails” and fail to reach an agreement at all; because
these games receive scores of zero, this behavior
may be filtered out over the course of self-play
under the semi-competitive objective.
We also computed the “vocabulary size” of each
iteration by counting the number of unique to-
kens produced during 500 games of self-play. We
observed a similar trend of decreasing vocabu-

0
2
4
6
8
10
Round of Self-Play Finetuning
200
250
300
350
Dialogue Length (tokens)
Semi-Competitive
Cooperative
Figure 3: Mean dialogue lengths per game for every iter-
ation of the model in semi-competitive and cooperative
objectives. Dialogues under the semi-competitive ob-
jective progressively shrank in length, while dialogues
under the cooperative objective grew significantly.
lary size over the course of self-play in the semi-
competitive setting, supporting the hypothesis that
the semi-competitive objective leads to conver-
gence in model behavior. However, as shown in
Section 5.2, these models performed well in both
self-play and in human generalization experiments,
suggesting that they may not need very diverse
communication strategies to achieve high scores.
7
Related Work
Grounded Dialogue
Much prior work on goal-
oriented dialogue has focused on collaborative
settings. In tasks such as Cards (Djalali et al.,
2011; Potts, 2012), CerealBar (Suhr et al., 2019),
OneCommon (Udagawa and Aizawa, 2019), and
DialOp (Lin et al., 2024), two agents must collab-
orate via natural language dialogue to achieve a
shared goal within an environment. In many of
these tasks, models are automatically evaluated via
self-play, which serves as a proxy for human evalu-
ation (Fried et al., 2021; Lin et al., 2024).
Relatively less work has focused on the case
where agents have conflicting goals. A handful of
grounded dialogue tasks are focused on bartering
or negotiation, including Deal or No Deal (DoND;
Lewis et al., 2017), CaSiNo (Chawla et al., 2021),
and the fruit trading game from Gemp et al. (2024).
These games are all structurally similar and differ
primarily in the number and types of objects they
use, as well as the public availability of human data.
In the Craigslist Bargaining task (He et al., 2018),
agents negotiate on the price of a object for sale.
0
2
4
6
8
10
Rounds of Self-Play Finetuning
400
600
800
1000
1200
1400
1600
Vocabulary Size
Semi-Competitive
Cooperative
Figure 4:
Aggregate vocabulary sizes across each
model iteration’s 500 games of self-play, for both semi-
competitive and cooperative objectives. In the semi-
competitive setting, vocabulary size trended downward,
but the model maintained and even expanded its vocab-
ulary when trained with the cooperative objective.
Fried et al. (2023) provides additional discussion
of collaborative and competitive grounded dialogue
tasks and modeling approaches.
Self-Improving Language Models
Lewis et al.
(2017) trained GRU-based language models on
the Deal or No Deal task using REINFORCE
(Williams, 1992). In contrast to our work, Lewis
et al. (2017) did not learn a model tabula rasa but
instead interleaved reinforcement learning from
self-play with supervised learning on task-specific
data to avoid divergence from human-interpretable
language. Divergence issues abound in other set-
tings where models are trained via self-play, includ-
ing in emergent communication (Kottur et al., 2017;
Lowe et al., 2019; Tomlin and Pavlick, 2019).
A wave of recent work has focused on meth-
ods for autonomously improving large language
models at training (Ouyang et al., 2022; Bai et al.,
2022; Abdulhai et al., 2023) or inference (Shinn
et al., 2023; Yao et al., 2023; Wu et al., 2024) time.
Chen et al. (2024) propose a method called self-
play fine-tuning, but their usage of the term self-
play differs from the traditional meaning, i.e., it
does not involve agents interacting within an en-
vironment; instead, Chen et al. (2024) proposes a
preference learning method akin to DPO (Rafailov
et al., 2023) or PPO for reinforcement learning
from human feedback (Ouyang et al., 2022). More
closely related to our work is Pan et al. (2024),
which iteratively trains models for device-control
tasks using filtered behavior cloning; however, in

contrast to our work, Pan et al. (2024) studies a sin-
gle agent interacting with an environment, rather
than multiple agents interacting with one another.
Another closely related work is Fu et al. (2023),
which uses self-play to refine language models for
a bargaining task. In contrast to our work, the bar-
gaining task is a relatively simple, zero-sum game
which elicits less rich dialogues than DoND. Fur-
ther, while our work focuses on finetuning models,
Fu et al. (2023) present models with in-context
demonstrations of previous games and natural lan-
guage feedback from a critic model (similar to Re-
flexion (Shinn et al., 2023)), leading to less major
performance improvements.
Multi-Agent Reinforcement Learning
Training
agents against copies of themselves is a longstand-
ing technique in reinforcement learning (Littman,
1994), popularized in the past decade by models
like AlphaGo (Silver et al., 2016) and AlphaZero
(Silver et al., 2017). Experiments on games such as
Overcooked (Carroll et al., 2019) and Hanabi (Bard
et al., 2020) have shown that policies learned via
self-play often fail to generalize to collaborative or
imperfect information games. Methods such as fic-
titious self-play and population play have been pro-
posed to address these issues (Heinrich et al., 2015;
Strouse et al., 2021), but have primarily been ap-
plied in games without language components. One
notable exception to this is CICERO, which trained
models for the game of Diplomacy via self-play us-
ing a KL-regularization objective which prevented
language from drifting too far from human-written
training data (FAIR, 2022).
8
Conclusion & Discussion
Our experiments showed that language model self-
play can lead to significant performance improve-
ments in both semi-competitive and cooperative
games. This finding contradicts existing wisdom
that self-play is ineffective in collaborative domains
(Strouse et al., 2021), or that models need to be
trained on task-specific human data to avoid diver-
gence from human-interpretable language (Lewis
et al., 2017; FAIR, 2022). One hypothesis is that be-
cause we observed significant model improvements
after just ten rounds of self-play, the model may not
have had time to overfit to cooperation with itself.
Another hypothesis is that better language models
might simply be more robust to the negative effects
of self-play. Given a model with good generaliza-
tion abilities, finetuning on self-play games might
be able to elicit model capabilities which are not
directly present in the self-play data. Furthermore,
self-play with pretrained language models might
actually function more similarly to population play,
since large language models are trained on text
from a population of users and may simulate differ-
ent personas in different contexts (Pataranutaporn
et al., 2021; Park et al., 2023).
Although game scores increased significantly af-
ter self-play, this increase can be almost entirely
attributed to an increase in the percentage of com-
pleted games, rather than better high-level strate-
gies or negotiation tactics. Future work may be
able to obtain even larger improvements by com-
bining self-play with approaches other than filtered
BC, such as natural language reflections, e.g., akin
to Shinn et al. (2023). Another possible approach
is described by Srivastava et al. (2024), in which a
language model is used to describe distributional
differences between good and bad trajectories.
Finally, the effectiveness of methods like self-
play is completely dependent on a reward signal,
which in this work was obtained from the game
environment. To apply similar methods not just
in game-playing domains but in real-world scenar-
ios, we anticipate that models will need to rely
on feedback from general-purpose, learned reward
models (e.g., as in Du et al., 2023; Pan et al., 2024).
We leave further investigation of the challenges
associated with bringing self-play into real-world
application domains to future work.
Acknowledgments
We are grateful to Daniel Fried, Justin Chiu, and
Jessy Lin for early discussions which contributed
to the idea for this work. We also thank Lucy Li,
Jiayi Pan, and Rodolfo Corona for their feedback.
NT is supported by the DARPA SemaFor program.
References
Marwa Abdulhai, Isadora White, Charlie Snell, Charles
Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and
Sergey Levine. 2023. LMRL Gym: Benchmarks
for multi-turn reinforcement learning with language
models. arXiv preprint arXiv:2311.18232.
Yu Bai and Chi Jin. 2020.
Provable self-play algo-
rithms for competitive reinforcement learning. In
Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pages 551–560. PMLR.
Yu Bai, Chi Jin, and Tiancheng Yu. 2020. Near-optimal
reinforcement learning with self-play. In Advances in

Neural Information Processing Systems, volume 33,
pages 2159–2170. Curran Associates, Inc.
Yuntao Bai,
Saurav Kadavath,
Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen,
Anna Goldie,
Azalia Mirhoseini,
Cameron McKinnon, et al. 2022.
Constitutional
AI: Harmlessness from AI feedback. arXiv preprint
arXiv:2212.08073.
Nolan Bard, Jakob N. Foerster, Sarath Chandar, Neil
Burch, Marc Lanctot, H. Francis Song, Emilio
Parisotto, Vincent Dumoulin, Subhodeep Moitra, Ed-
ward Hughes, Iain Dunning, Shibl Mourad, Hugo
Larochelle, Marc G. Bellemare, and Michael Bowl-
ing. 2020. The hanabi challenge: A new frontier for
AI research. Artificial Intelligence, 280:103216.
Greg Brockman, Vicki Cheung, Ludwig Pettersson,
Jonas Schneider, John Schulman, Jie Tang, and Wo-
jciech Zaremba. 2016.
OpenAI Gym.
Preprint,
arXiv:1606.01540.
Noam Brown and Tuomas Sandholm. 2018. Superhu-
man AI for heads-up no-limit poker: Libratus beats
top professionals. Science, 359(6374):418–424.
Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths,
Sanjit Seshia, Pieter Abbeel, and Anca Dragan. 2019.
On the utility of learning about humans for human-ai
coordination. In Advances in Neural Information
Processing Systems, volume 32. Curran Associates,
Inc.
Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale
Lucas, Jonathan May, and Jonathan Gratch. 2021.
CaSiNo: A corpus of campsite negotiation dialogues
for automatic negotiation systems. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 3167–3185,
Online. Association for Computational Linguistics.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,
Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind
Srinivas, and Igor Mordatch. 2021. Decision trans-
former: Reinforcement learning via sequence model-
ing. In Advances in Neural Information Processing
Systems, volume 34, pages 15084–15097. Curran As-
sociates, Inc.
Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang,
Yanqiu Wu, and Keith Ross. 2020. Bail: Best-action
imitation learning for batch deep reinforcement learn-
ing. In Advances in Neural Information Processing
Systems, volume 33, pages 18353–18363. Curran As-
sociates, Inc.
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji,
and Quanquan Gu. 2024. Self-play fine-tuning con-
verts weak language models to strong language mod-
els. arXiv preprint arXiv:2401.01335.
Alex Djalali, David Clausen, Sven Lauer, Karl Schultz,
and Christopher Potts. 2011. Modeling expert ef-
fects and common ground using Questions Under
Discussion. In Proceedings of the AAAI Workshop
on Building Representations of Common Ground with
Intelligent Agents, Washington, DC. Association for
the Advancement of Artificial Intelligence.
Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil
Raju, Jessica Landon, Felix Hill, Nando de Freitas,
and Serkan Cabi. 2023.
Vision-language models
as success detectors.
In Proceedings of The 2nd
Conference on Lifelong Learning Agents, volume
232 of Proceedings of Machine Learning Research,
pages 120–136. PMLR.
FAIR. 2022. Human-level play in the game of Diplo-
macy by combining language models with strategic
reasoning. Science, 378(6624):1067–1074.
Daniel Fried, Justin Chiu, and Dan Klein. 2021.
Reference-centric models for grounded collaborative
dialogue. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 2130–2147, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Daniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Pa-
tel, and Aida Nematzadeh. 2023. Pragmatics in lan-
guage grounding: Phenomena, tasks, and modeling
approaches. In Findings of the Association for Com-
putational Linguistics: EMNLP 2023, pages 12619–
12640, Singapore. Association for Computational
Linguistics.
Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.
2023. Improving language model negotiation with
self-play and in-context learning from AI feedback.
arXiv preprint arXiv:2305.10142.
Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman.
2023.
Strategic reasoning with language models.
arXiv preprint arXiv:2305.19165.
Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel,
Vibhavari Dasagi, Luke Marris, Georgios Piliouras,
and Karl Tuyls. 2024. States as strings as strate-
gies: Steering language models with game-theoretic
solvers. arXiv preprint arXiv:2402.01704.
He He, Derek Chen, Anusha Balakrishnan, and Percy
Liang. 2018. Decoupling strategy and generation in
negotiation dialogues. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2333–2343, Brussels, Bel-
gium. Association for Computational Linguistics.
Johannes Heinrich, Marc Lanctot, and David Silver.
2015. Fictitious self-play in extensive-form games.
In Proceedings of the 32nd International Conference
on Machine Learning, volume 37 of Proceedings of
Machine Learning Research, pages 805–813, Lille,
France. PMLR.
Olivia Huang, Eve Fleisig, and Dan Klein. 2023. Incor-
porating worker perspectives into MTurk annotation
practices for NLP. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language

Processing, pages 1010–1028, Singapore. Associa-
tion for Computational Linguistics.
Satwik Kottur, José Moura, Stefan Lee, and Dhruv Batra.
2017. Natural language does not emerge ‘naturally’
in multi-agent dialog. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2962–2967, Copenhagen,
Denmark. Association for Computational Linguis-
tics.
Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh,
and Dhruv Batra. 2017. Deal or no deal? end-to-
end learning of negotiation dialogues. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2443–2453,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.
Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason
Eisner. 2024. Decision-oriented dialogue for human-
AI collaboration. Preprint, arXiv:2305.20076.
Michael L Littman. 1994. Markov games as a frame-
work for multi-agent reinforcement learning. In Ma-
chine learning proceedings 1994, pages 157–163.
Elsevier.
Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe
Kiela, and Joelle Pineau. 2019. On the interaction be-
tween supervision and self-play in emergent commu-
nication. In International Conference on Learning
Representations.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems, volume 35, pages 27730–27744.
Curran Associates, Inc.
Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou,
Sergey Levine, and Alane Suhr. 2024. Autonomous
evaluation and refinement of digital agents. arXiv
preprint arXiv:2404.06474.
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S. Bern-
stein. 2023. Generative agents: Interactive simulacra
of human behavior. In Proceedings of the 36th An-
nual ACM Symposium on User Interface Software
and Technology, UIST ’23, New York, NY, USA.
Association for Computing Machinery.
Pat Pataranutaporn, Valdemar Danry, Joanne Leong,
Parinya Punpongsanon, Dan Novy, Pattie Maes, and
Misha Sra. 2021. Ai-generated characters for sup-
porting personalized learning and well-being. Nature
Machine Intelligence, 3(12):1013–1022.
Christopher Potts. 2012. Goal-driven answers in the
Cards dialogue corpus. In Proceedings of the 30th
West Coast Conference on Formal Linguistics, pages
1–20, Somerville, MA. Cascadilla Press.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. In Advances in
Neural Information Processing Systems, volume 36,
pages 53728–53741. Curran Associates, Inc.
Noah Shinn, Federico Cassano, Ashwin Gopinath,
Karthik Narasimhan, and Shunyu Yao. 2023. Re-
flexion: language agents with verbal reinforcement
learning. In Advances in Neural Information Process-
ing Systems, volume 36, pages 8634–8652. Curran
Associates, Inc.
David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016. Mastering
the game of Go with deep neural networks and tree
search. Nature, 529(7587):484–489.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioan-
nis Antonoglou, Matthew Lai, Arthur Guez, Marc
Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
Graepel, Timothy Lillicrap, Karen Simonyan, and
Demis Hassabis. 2018.
A general reinforcement
learning algorithm that masters chess, shogi, and go
through self-play. Science, 362(6419):1140–1144.
David Silver, Julian Schrittwieser, Karen Simonyan,
Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
Bolton, et al. 2017. Mastering the game of Go with-
out human knowledge. Nature, 550(7676):354–359.
Megha Srivastava, Cedric Colas, Dorsa Sadigh, and Ja-
cob Andreas. 2024. Policy learning with a language
bottleneck. arXiv preprint arXiv:2405.04118.
DJ Strouse, Kevin McKee, Matt Botvinick, Edward
Hughes, and Richard Everett. 2021. Collaborating
with humans without human data. In Advances in
Neural Information Processing Systems, volume 34,
pages 14502–14515. Curran Associates, Inc.
Alane Suhr, Claudia Yan, Jack Schluger, Stanley Yu,
Hadi Khader, Marwa Mouallem, Iris Zhang, and
Yoav Artzi. 2019.
Executing instructions in situ-
ated collaborative interactions. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 2119–2130, Hong Kong,
China. Association for Computational Linguistics.
Nicholas Tomlin and Ellie Pavlick. 2019. Emergent
compositionality in signaling games. In CogSci, page
3593.
Takuma Udagawa and Akiko Aizawa. 2019. A natural
language corpus of common grounding under contin-
uous and partially-observable context. Proceedings
of the AAAI Conference on Artificial Intelligence,
33(01):7120–7127.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine learning, 8:229–256.
Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin
Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and
Lingpeng Kong. 2024. Os-copilot: Towards gener-
alist computer agents with self-improvement. arXiv
preprint arXiv:2402.07456.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
2023. Tree of thoughts: Deliberate problem solving
with large language models. In Advances in Neural
Information Processing Systems, volume 36, pages
11809–11822. Curran Associates, Inc.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-
man. 2022. Star: Bootstrapping reasoning with rea-
soning. In Advances in Neural Information Process-
ing Systems, volume 35, pages 15476–15488. Curran
Associates, Inc.

A
Environment Implementation
We implemented an environment for the task under
which language models can play the game with
each other or against another human. For initializ-
ing a new instance of the game, we sample from
a list of 4,186 valid game contexts (shared item
counts and private value functions for each player),
provided by Lewis et al. (2017). The environment
then takes turns prompting each player to either
send a message (prepended by [message]) or sub-
mit a proposal (prepended by [propose]). After
detecting a submitted proposal, the environment
forces the other player to submit a proposal of
their own. This is enforced by error correction,
described in the section below. Once a game is
completed, the environment uses the game context
and the submitted proposals to determine the final
score of each player, conditioned on the objective
under which the players were instructed to play.
Error Correction
This environment comes with
comprehensive error-handling to correct models’
errant outputs. Specifically, the environment will
reply with instructions for correcting errors when
errors are detected, providing the model with the
opportunity to format its output correctly. The
errors that we check for, and their corresponding
correction messages, can be found in Table 4. If a
model generates five errant outputs in a row, then
the environment aborts the game, and both players
receive zero score.
Zero-Shot Prompting
Across every setting, the
initial models are zero-shot prompted with the
game’s rules and the instructions for sending mes-
sages and submitting proposals with the correct
syntax. The choice of this approach over few-shot
prompting was motivated by the concern that few-
shot examples might influence strategies chosen by
the model during inference. Our preliminary ex-
periments found that models would closely match
the negotiation techniques used in few-shot ex-
amples; for example, if prompted with dialogues
where players shared their exact value functions,
the model would consistently share its own values,
whether doing so was advantageous or not.
Prompting with Conversation History
Follow-
ing the format recommended by the OpenAI API’s
chat completions endpoint, the prompt containing
game instructions is sent under the system role;
for subsequent dialogue, any messages sent by the
model itself are categorized with the assistant
role, and messages from the other player are ap-
pended to a model’s input as messages from the
user role. The system prompt used for the semi-
competitive objective can be found in Figure 5;
other prompts are available in our code release.
B
Model Training and Hyperparameters
All models were finetuned using the OpenAI API,
with parameters n_epochs=3, batch_size=1, and
learning_rate_multiplier=8. For model infer-
ence, we generated outputs with temperature=1.
These parameters were all default values chosen
by the OpenAI API, except for the learning rate
multiplier, which defaults to 2. Our preliminary
experiments with default learning rate multipliers
yielded models that not only failed to improve but
also devolved significantly in quality. We hypothe-
size that this occurred because parameters are set
dynamically based on the quantity of finetuning
data. Because language model self-play requires
sequential rounds of finetuning, it may be important
to choose initial parameters based on the expecta-
tion of future finetuning rounds.
C
Details of Human Experiments
C.1
Game Interface
We developed a web interface for human data col-
lection, shown in Figure 9, Figure 10, and Fig-
ure 11. The interface provides comprehensive in-
structions describing the different game modes, as
well as an explanation of the bonus pay structure
and a running count of bonus pay earned so far. At
the end of each game, players see a popup with
the number of points and bonus pay earned. If
players receive no points (e.g., due to game error
or non-compatible proposals), they receive a small
amount of bonus pay and an explanation of what
went wrong. During the main phase of data col-
lection, players were allowed to complete up to 40
games; after each game, players were given the
option to end the HIT and collect bonus play or
keep playing.
C.2
Crowdsourcing
We ran human evaluation through Amazon Me-
chanical Turk (MTurk). We restricted our task
to workers from the United States with a 98+%
HIT approval rate and at least 500 completed HITs,
based on recommendations in Huang et al. (2023).
In order to filter out bots and low-quality workers,
we ran a brief prescreening survey which asked

Error
Correction Prompt
Outputting text without a prefix of either
"[message]" or "[propose]"
Your output should either begin with [message] or
a [propose].
Submitting proposals before any mes-
sages have been sent
Please begin the dialogue by discussing how you’ll
divide
the
items
before
submitting
a
private
proposal.
Sending messages with multiple men-
tions of "[message]" or "[propose]" (i.e.
outputting multiple messages in a row)
Do
not
include
any
mentions
of
[message]
or
[propose] after the initial prefix.
Please just
send a single message, beginning with [message].
Sending messages after a proposal has
been submitted
Opponent’s proposal must be followed by a proposal
of your own. Please send a proposal, beginning with
[propose].
Submitting proposals with incorrectly
sequenced items
Item counts must be sequenced in the following
order: books, hats, and then balls.
Submitting proposals with more than
three item counts
There should only be counts for three items in your
proposal: books, hats, and balls.
Submitting proposals with invalid item
counts, based on game context
Item counts suggested are invalid based on game
context; some of your proposal’s item counts are
greater than total items available.
Table 4: Our game environment sends error messages to language models if they produce ill-formed outputs, e.g.,
sending a message after the discussion phase ends. The model then has an opportunity to send a new message based
on the correction. If the model repeatedly fails to produce well-formed outputs, then the game aborts and both
players receive zero score.
workers to (1) to answer a question about text on
a linked, external website and (2) write a 2-3 sen-
tence description of their favorite MTurk task. The
authors then manually reviewed responses to the
prescreening survey and chose approximately 15%
to invite to the main task.
We ran three pilot studies before launching our
main human evaluation, with 10 workers each.
After the initial pilots, we modified the interface
and incentive structure to obtain higher-quality di-
alogues. We reviewed data from each pilot and
removed low-quality workers and spammers from
later rounds of data collection. In total, we invited
60 workers to our final round of data collection,
although a small number of workers declined the
HIT. We include data from the third pilot in our
results because we did not modify the game after
that point. Figure 6 provides additional statistics
on the total number of workers hired.
C.3
Incentive Structure
We paid $1.00 for picking up the HIT and $0.10
per game completed. The majority of pay was
distributed through bonuses. We paid a bonus of
$0.20 per point earned in the semi-competitive set-
ting and $0.10 per point earned in the cooperative
setting, since scores in the cooperative game are on
average twice as high. We also paid workers $0.25
in cases where models aborted due to repeatedly
generating ill-formed messages.
In contrast, Lewis et al. (2017) paid workers
$0.10 per game and $0.05 in bonus pay only when
workers achieved the maximum score of ten points.
We found that this approach incentivized workers to
end the game as quickly as possible, as maximizing
the number of games played was more lucrative
than attempting to achieve a high score.
0
10
20
30
40
50
Number of Games Played
0
3
6
9
12
15
18
21
Number of Users
Figure 6: The majority of our workers played the maxi-
mum number of games, with a small handful contribut-
ing data from both the pilot and the main study. The
mean pay for workers was $35.70.

D
Results for Strict Competition
When we applied LM self-play to Deal or No Deal
under the strictly competitive objective, the model
failed to improve its performance, instead learn-
ing strategies that adversely impacted its ability
to perform outside of self-play. Since the mean
score in self-play for every iteration will always be
zero (because the game is zero-sum), we instead
evaluated the quality of model self-play through
agreement rates. As shown in Figure 7 and Fig-
ure 8, while agreement rate trends show that the
model’s performance in self-play improves, these
models fail to generalize to competition with other
models, such as GPT-4. Our preliminary human
experiments also showed that the model failed to
reach agreements in roughly 95% of games.
In our qualitative analysis of successive itera-
tions of the model under the strictly competitive
objective, we found that it learned to replicate an
inverted proposal strategy; specifically, the model
learned a strategy where it submits a proposal based
on what the opposing player should receive, rather
than what the model itself should receive. While
the model optimized this strategy in self-play well
enough to arrive at valid agreements at a compet-
itive rate with itself, this strategy does not gener-
alize to competition with humans or GPT-4. We
found this to be a consequence of the aggressive
nature of the strictly competitive objective leading
to a smaller proportion of games ending in valid
agreements to derive reward signal from. With a
smaller number of samples providing reward sig-
nal, we risk an outcome where the few samples that
are isolated for finetuning achieve their non-zero
reward through undesirable strategies (inverted pro-
posals, in this instance) that do not generalize well
to human interaction.
Our experiments under this objective illustrate
an important takeaway regarding the failure modes
of LM self-play. For this strategy to be effective,
there must be confidence that the initial model is
capable of achieving high performance through
desired strategies with significant probability. Ad-
ditionally, we speculate that this strategy is most
effective in environments with continuous reward.
Figure 9: The landing page for our human data collec-
tion site provides comprehensive game rules and instruc-
tions and an explanation of the bonus pay structure.
Figure 10: The game interface for human data collection
shows the shared item pool, game mode, item values,
and a chat window, as well as the number of games
played so far and a running count of bonus pay earned.
Figure 11: At the end of each game, players see a popup
with the number of points and bonus pay earned. Players
have the option to end the game and collect their bonus
pay or keep playing, up to the maximum of 40 games.

You are an expert in negotiation. You are about to play a game with another player. In this
game, you and your partner will divide a shared set of books, hats, and balls. Each item has
a point value for you, but you don’t know your partner’s values.
At the start of the game,
you will be given the total number of objects of each type, as well as your own private value
function, which tells you how many points each object is worth to you. Your points will be equal
to the sum of item values for all items you receive. Your objective is to maximize your points.
On
each
turn,
you
can
either
send
a
message
to
the
other
player,
or
submit
a
private
proposal for how to divide the items. Your partner will do the same, and both proposals will
remain hidden from each other. Please push back on any suggestions made by your partner that
you believe would leave you with an unsatisfactory point total. However, if the number of items
in the combined proposals don’t match the total number of items, both players score 0.
Messages should be formatted like this:
[message] Your message here.
Proposals should be formatted like this:
[propose] (x books, y hats, z balls)
The numbers x, y, and z should be your own item counts.
The item counts must be whole
numbers; you cannot split singular items. For example, if you want 1 book, 2 hats, and 0 balls,
you would send:
[propose] (1 books, 2 hats, 0 balls)
When
discussing,
do
not
leave
any
of
the
items
unclaimed.
You
and
your
partner
must
submit proposals that collectively add up to the total item counts. To achieve a nonzero score,
your partner would need to write a complementary proposal that adds up to the total number of
items. For example, if the total number of items is 3 books, 2 hats, and 1 ball, your partner
would need to send:
[propose] (2 books, 0 hats, 1 balls)
Any
message
that
you
send
should
begin
with
either
"[message]"
or
"[propose]".
All
proposals are final, so make sure that both players agree about which items are being taken by
which player before ending the discussion with a proposal.
Each message should end with "[END]".
Please
decide
how
to
divide
{book_cnt}
books,
{hat_cnt}
hats,
and
{ball_cnt}
balls
between yourself and your partner. This should be an open discussion; you should only propose
after exchanging a few messages.
To you,
books are each worth {book_val},
hats are worth {hat_val},
and balls are worth
{ball_val}.
You don’t know your partner’s item values.
Remember, your goal is to maximize your own score while also ensuring that your partner will
agree to the deal.
Figure 5: System prompt used for the semi-competitive objective. Values in {brackets} are filled in based on the
game context (i.e., item counts and private value functions).

0
2
4
6
8
10
Rounds of Self-Play Finetuning
0.3
0.2
0.1
0.0
0.1
Mean Score
Average Score (vs GPT-4)
Figure 7: We evaluated the strictly competitive model against GPT-4, since self-play scores are not informative for
zero-sum games. However, we determined that the model experienced no significant improvement in generalization.
0
2
4
6
8
10
Rounds of Self-Play Finetuning
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Agreement Rate
Valid Agreements Ratio (vs GPT-4)
Valid Agreements Ratio (Self-Play)
Figure 8: Under the strictly competitive objective, self-play finetuning increased the frequency of games reaching
valid agreements in self-play, but the strategies learned generalized poorly to interaction with other agents, such as
GPT-4. Our preliminary experiments also indicated a low agreement rate with human competitors.

