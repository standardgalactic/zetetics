An approximate empirical Bayesian method for
large-scale linear-Gaussian inverse problems
The MIT Faculty has made this article openly available. Please share 
how this access benefits you. Your story matters.
Citation
Zhou, Qingping, et al. "An approximate empirical Bayesian method
for large-scale linear-Gaussian inverse problems." Inverse
Problems 34, 9 (June 2018) © 2018 IOP Publishing Ltd.
As Published
http://dx.doi.org/10.1103/10.1088/1361-6420/aac287
Publisher
IOP Publishing
Version
Original manuscript
Citable link
https://hdl.handle.net/1721.1/122927
Terms of Use
Creative Commons Attribution-Noncommercial-Share Alike
Detailed Terms
http://creativecommons.org/licenses/by-nc-sa/4.0/

An approximate empirical Bayesian method for
large-scale linear-Gaussian inverse problems
Qingping Zhou
Department of Mathematics and Institute of Natural Sciences, Shanghai Jiao Tong
University, Shanghai 200240, China
E-mail: zhou2015@sjtu.edu.cn
Wenqing Liu
Department of Mathematics and Zhiyuan College, Shanghai Jiao Tong University,
Shanghai 200240, China
E-mail: wenqingliu@sjtu.edu.cn
Jinglai Li
Institute of Natural Sciences, Department of Mathematics and MOE Key
Laboratory of Scientiﬁc and Engineering Computing, Shanghai Jiao Tong
University, Shanghai 200240, China
E-mail: jinglaili@sjtu.edu.cn
Youssef M. Marzouk
Department of Aeronautics and Astronautics, Massachusetts Institute of
Technology, Cambridge, MA 02139, USA
E-mail: ymarz@mit.edu
March 2018
Abstract.
We study Bayesian inference methods for solving linear inverse problems,
focusing on hierarchical formulations where the prior or the likelihood function
depend on unspeciﬁed hyperparameters. In practice, these hyperparameters are often
determined via an empirical Bayesian method that maximizes the marginal likelihood
function, i.e., the probability density of the data conditional on the hyperparameters.
Evaluating the marginal likelihood, however, is computationally challenging for large-
scale problems. In this work, we present a method to approximately evaluate marginal
likelihood functions, based on a low-rank approximation of the update from the prior
arXiv:1705.07646v2  [math.NA]  11 Mar 2018

Approximate empirical Bayesian method for linear inverse problems
2
covariance to the posterior covariance. We show that this approximation is optimal
in a minimax sense. Moreover, we provide an eﬃcient algorithm to implement the
proposed method, based on a combination of the randomized SVD and a spectral
approximation method to compute square roots of the prior covariance matrix.
Several numerical examples demonstrate good performance of the proposed method.
1. Introduction
Bayesian inference approaches have become increasingly popular as a tool to solve
inverse problems [33, 17, 32]. In this setting, the parameters of interest are treated
as random variables, endowed with a prior distribution that encodes information
available before the data are observed.
Observations are modeled by their joint
probability distribution conditioned on the parameters of interest, which deﬁnes the
likelihood function; this distribution incorporates the forward model and a stochastic
description of measurement or model errors. The posterior distribution of the unknown
parameters is then obtained via Bayes’ formula, which combines the information
provided by the prior and the data. Here we focus on a particular class of Bayesian
inverse problems, where the forward model is linear and both the prior and the error
distributions are Gaussian. Such linear–Gaussian inverse problems arise in many real-
world applications [33, 17].
In practice, a diﬃculty in implementing Bayesian inference for such problems is
that the prior and/or the likelihood function may contain unspeciﬁed hyperparameters.
For instance, the prior distribution usually encodes some kind of correlation among
the inversion parameters, but the correlation length is often unknown. Alternatively,
the variance and/or the mean of the observational errors may not be available, thus
yielding unspeciﬁed hyperparameters in the likelihood function.
A natural idea to
address such problems is to construct a hierarchical Bayesian model for both the
inversion parameters and the hyperparameters.
Then both sets of parameters can
be inferred from data through the characterization of their joint posterior distribution.
This inferential approach is often called “fully Bayesian.” A fully Bayesian method
can be prohibitively expensive for large-scale inverse problems, however, as it requires
sampling from a posterior distribution that is higher dimensional and no longer
Gaussian. An alternative strategy is the empirical Bayes (EB) approach [7, 6], which
ﬁrst estimates the hyperparameters by maximizing their marginal likelihood function,
i.e., the probability density of the data conditioned only on the hyperparameters, and
then plugs in the estimated values to compute the posterior of the inversion parameters.

Approximate empirical Bayesian method for linear inverse problems
3
The use of the EB method has been theoretically justiﬁed [23, 28]: roughly speaking,
these theoretical results show that, under certain conditions and for a suﬃciently large
sample size, the EB method leads to similar inferential conclusions as fully Bayesian
inference. At the same time, the EB method is more computationally eﬃcient than
the fully Bayesian approach, as it does not require characterizing the joint posterior
distribution.
Nonetheless, the numerical implementation of EB methods remains computation-
ally taxing when dimensionality of the inversion parameters is high, as it involves
maximizing the marginal likelihood function, which in turn requires “integrating out”
the inversion parameters for each evaluation. (Details about the computational cost of
evaluating the marginal likelihood function are given in Section 2.) The goal of this
work is to present an approximate EB method that can signiﬁcantly reduce the com-
putational cost of solving linear inverse problems with hyperparameters. Speciﬁcally,
our method evaluates the marginal likelihood function by using a low-rank update ap-
proximation of the posterior covariance matrix, introduced in previous work for the
non-hierarchical setting, e.g., [11, 3, 21, 1, 30, 29]. The intuition behind the low-rank
update approximation is that the data may be informative, relative to the prior, only
on a low-dimensional subspace of the entire parameter space. As a result, one can
consider approximations of the posterior covariance matrix in the form of low-rank
negative semideﬁnite updates of the prior covariance matrix, where the update is ob-
tained by solving a generalized eigenproblem involving the log-likelihood Hessian and
the prior precision matrix. The optimality properties of this method are studied in [30],
which shows that this low-rank approximation is optimal with respect to a speciﬁc class
of loss functions.
Using the approximation of the posterior covariance developed in [11, 3, 30], we
introduce a new method to eﬃciently compute the marginal likelihood function. We
prove that the proposed method yields an optimal approximation of the marginal
likelihood function in a minimax sense. Our last contribution lies in the numerical
implementation. Unlike the inverse problems considered in the work mentioned above,
where the prior is ﬁxed, our problems require repeatedly computing the square roots of
diﬀerent prior covariance matrices—which can be prohibitively expensive when the
dimensionality of the problem is high.
To address the issue, we use the spectral
approximation developed in [15] to compute the square root of the prior covariance
matrix, resulting in a very eﬃcient algorithm for evaluating (and maximizing) the
marginal likelihood function.
The rest of this paper is organized as follows. In Section 2 we introduce the basic

Approximate empirical Bayesian method for linear inverse problems
4
setup of the empirical Bayesian method for solving inverse problems. The low-rank
update approximation for evaluating the log marginal likelihood function is developed
and analyzed in Section 3, and the detailed numerical implementation of the proposed
method is discussed in Section 4.
Section 5 provides two numerical examples to
demonstrate the performance of the proposed method, and Section 6 oﬀers concluding
remarks.
2. Problem setup
Consider a linear inverse problem in a standard setting:
y = Gx + η
where y ∈Rm is the data, x ∈Rn is the unknown, G is a n × m matrix, often called
the forward operator, and η is the observation noise. Such an inverse problem can be
solved with a Bayesian inference method: namely we assume that the prior probability
density of x is π(x) and the likelihood function is π(y|x), and thus the posterior is
given by Bayes’ theorem:
π(x|y) = π(y|x)π(x)
π(y)
.
Throughout this paper, we use the notation π(·) as a generic representation of
probability density; the speciﬁc density is made clear by its arguments. We further
assume a Gaussian likelihood and a Gaussian prior with a non-singular covariance
matrix Γpr ≻0 and, without loss of generality, zero mean:
y | x ∼N(Gx, Γobs),
x ∼N(0, Γpr).
(1)
In this setting, the resulting posterior distribution is also Gaussian
x | y ∼N(µpos, Γpos),
with mean and covariance matrix given by
µpos = Γpos G⊤Γ−1
obs y
and
Γpos =
 H + Γ−1
pr
−1 ,
(2)
where
H = G⊤Γ−1
obsG
(3)
is the Hessian of the log-likelihood.
Now we consider a more complex situation, where the matrices G, Γpr and Γobs
(or some of them) depend on a vector of unspeciﬁed hyperparameters θ ∈Rp. As

Approximate empirical Bayesian method for linear inverse problems
5
mentioned in the introduction, a popular method to deal with such problems is the
empirical Bayesian (EB) approach, which determines θ by maximizing the marginal
likelihood function:
max
θ
π(y|θ) =
Z
π(y|x, θ)π(x|θ)dx
(4)
or the marginal posterior density:
max
θ
π(θ|y) = p(θ)
Z
π(y|x, θ)π(x|θ)dx
where p(θ) is the prior density of θ. Note that the computations of these two objectives
are very similar and so we shall only discuss (4). It is easy to see that the optimization
problem in (4) is equivalent to minimizing the negative log marginal likelihood:
min
θ
−log π(y|θ)
= 1
2yTΓ−1
obsy + 1
2 log |Γobs| −1
2zTΓposz + 1
2 log |Γpr|
|Γpos|,
(5)
where G, Γpr and Γobs depend on θ and z = GTΓ−1
obsy. Note that an important special
case arises when only the prior depends on the hyperparameter θ and the likelihood
function is ﬁxed; in this case, we can simply minimize
L(θ, z) := −1
2zTΓposz + 1
2 log |Γpr|
|Γpos|.
(6)
For conciseness, below we will present our method for (6), while noting that all the
results can be trivially extended to (5).
Direct evaluation of (6) is not desirable for large scale problems, as it requires
several operations with O(n3) complexity. In what follows, we present an accurate
and eﬃcient—with O(n2r) complexity for some r ≪n—method to approximate L(θ),
based on a rank-r update approximation of Γpos.
3. The optimal approximation method
3.1. Optimal low-rank update approximation of the posterior covariance
The proposed method begins with the optimal low-rank update approximation of
posterior covariance developed in [3, 30], which is brieﬂy reviewed here. Note that
Γpos can be written as a non-positive update of Γpr:
Γpos = Γpr −KK⊤,

Approximate empirical Bayesian method for linear inverse problems
6
where
KK⊤= Γpr G⊤Γ−1
y G Γpr
and Γy = Γobs + G Γpr G⊤is the covariance of the marginal distribution of y. This
update of Γpr is negative semideﬁnite because the data add information; they cannot
increase the prior variance in any direction. As has been discussed in previous work [30],
in many practical problems, the low-rank structure often lies in the update of Γpr that
yields Γpos, rather than in Γpos itself. Hence, [30] proposes to use the following class of
positive deﬁnite matrices to approximate Γpos:
Mr =

Γpr −BB⊤≻0 : rank(B) ≤r
	
.
(7)
To establish optimality statements regarding the approximation of a covariance
matrix, [30] adopts the following distance between symmetric positive deﬁnite (SPD)
matrices of size n:
d2
F(A, B) = tr

ln2( A−1/2BA−1/2 )

=
n
X
i=1
ln2(σi),
where (σi)n
i=1 are the generalized eigenvalues of the pencil (A, B). This metric was
ﬁrst introduced by Rao in [26], and can measure the diﬀerence between two Gaussian
distributions with the same mean. We direct interested readers to [29] for a detailed
discussion and other applications of this metric.
It is important to note that this
distance is generalized in [30] to be
D(A, B) =
n
X
i=1
f(σi),
where f is a function in C1(R+) that satisﬁes f ′(x)(1 −x) < 0 for any x ̸= 1, and
limx→∞f(x) = ∞. This generalization will be used for the proof of our optimality
statement in next section.
Thus we seek a low-rank approximation bΓpos of the
covariance matrix Γpos, such that
bΓpos = arg min
Γ∈M D(Γpos, Γ).
(8)
The solution of (8) can be derived analytically regardless of the speciﬁc choice of
f. Speciﬁcally, let Spr be any square root of the prior covariance matrix such that
Γpr = Spr S⊤
pr. We deﬁne the prior-preconditioned Hessian as
bH = S⊤
prH Spr,
(9)

Approximate empirical Bayesian method for linear inverse problems
7
which plays an essential role in our numerical method.
Now let (δ2
i , wi) be the
eigenvalue-eigenvector pairs of bH with the ordering δ2
i ≥δ2
i+1; then a solution of (8) is
given by:
bΓpos = Γpr −BB⊤,
BB⊤=
r
X
i=1
δ2
i
 1 + δ2
i
−1 bwi bw⊤
i ,
bwi = Sprwi.(10)
The corresponding minimum distance is
D(Γpos, bΓpos) = f(1)r + f
n
X
i=r+1
f(1/(1 + δ2
i )).
The minimizer (10) is unique if the ﬁrst r eigenvalues of bH are distinct.
3.2. Approximating the log-likelihood function
Now we apply the optimal low rank approximation to our problem. The idea is rather
straightforward: we approximate L in (6) with
bL(θ, z) := −1
2z⊤bΓpos z + 1
2 log |Γpr|
|bΓpos|
,
(11)
for some approximate posterior covariance matrix bΓpos.
Next we shall derive the
appropriate choice of bΓpos. To do this, we need to impose an additional assumption on
the approximate posterior covariance matrix: bΓpos −Γpos ⪰0, which means that the
approximation itself should not create any new information. As a result, the class of
matrices for approximating Γpos becomes
M′
r =
n
bΓpos = (Γpr −BB⊤) : bΓpos −Γpos ⪰0, rank(B) ≤r
o
(12)
for some maximum rank r. Next we shall consider the approximations of the two terms
log(|Γpr|/|Γpos|) and z⊤Γpos z in (6) separately.
3.2.1. Approximating the log–determinant term.
First, we consider ﬁnding a matrix
bΓpos ∈M′
r to approximate log(|Γpr|/|Γpos|) with log(|Γpr|/|bΓpos|).
In this setting,
it is easy to see that the approximation error in the log-marginal likelihood (6) is
| log(|Γpos|/|bΓpos|)|, and a natural way to determine bΓpos is to seek a bΓpos ∈M′
r that
minimizes this approximation error. To this end, we have the following theorem:

Approximate empirical Bayesian method for linear inverse problems
8
Theorem 3.1 Suppose that we approximate log(|Γpr|/|Γpos|) with log(|Γpr|/|bΓpos|) for
some bΓpos ∈M′
r. The matrix bΓpos ∈M′
r that minimizes the resulting approximation
error, i.e., the solution of
min
bΓpos∈M′r
log |Γpos|
|bΓpos|
 ,
(13)
is given by (10). Moreover, the optimal approximation and the associated error are,
respectively,
log |Γpr|
|bΓpos|
=
r
X
i=1
log(1 + δ2
i ) and log |bΓpos|
|Γpos| =
n
X
i=r+1
log(1 + δ2
i ).
(14)
Proof We prove this theorem using the optimality results in [30]. To start, we choose
a particular distance metric by letting
f(x) = | log x|.
We denote the resulting distance metric as D1(A, B) to indicate that such a metric is
actually the 1-norm of (ln(σ1), . . . , ln(σn)) while dF is the 2-norm of the same sequence.
It can be veriﬁed that
log |Γpr|
|Γpos| =
n
X
i=1
log(1 + δ2
i ) = D1(Γpr, Γpos),
as log(1+δ2
i ) ≥0 for all i ∈N. Since the approximate posterior covariance bΓpos ∈M′
r,
we can show
D1(Γpr, Γpos) = D1(Γpr, bΓpos) + D1(bΓpos, Γpos),
(15)
where D1(Γpr, bΓpos) = log(|Γpr|/|bΓpos|) is the approximation and D1(bΓpos, Γpos) =
log(|bΓpos|/|Γpos|) is the error associated with it. Note that (15) does not hold without
the assumption bΓpos ∈M′
r. Thus (13) can be rewritten as,
min
bΓpos∈M′r
D1(Γpos, bΓpos).
(16)
Recall that the solution of minbΓpos∈Mr D1(Γpos, bΓpos) is given by (10), and it is easy
to verify that the matrix bΓpos given by (10) is in M′
r, which implies that (10) also
provides the solution of (16). As a result, the optimal approximation and the associated
approximation error are given by (14), which completes the proof.

Approximate empirical Bayesian method for linear inverse problems
9
3.2.2. Approximating the quadratic term.
Similarly, we can also ﬁnd an approximate
posterior covariance bΓpos ∈M′
r and approximate z⊤Γpos z with z⊤bΓpos z. This problem
is a bit more complicated: since z (which is a linear transformation of the data y) is
random, we cannot determine the matrix bΓpos by directly minimizing the approximation
error. In this case, a useful treatment is to apply the minimax criterion, i.e., to seek
a matrix bΓpos ∈M′
r that minimizes the maximum approximation error with respect
to z. In particular, for the maximum error to exist, we shall require z to be bounded:
z ∈Zc = {z : ∥z∥2 ≤c} for a constant c > 0. (See Remark 3.1 for a discussion of
this boundedness assumption.) The following theorem provides the optimal solution
to this problem.
Theorem 3.2 Suppose that we approximate z⊤Γpos z with z⊤bΓpos z, for some bΓpos ∈
M′
r. The matrix bΓpos ∈M′
r that achieves the minimax approximation error, i.e., the
solution of
min
bΓpos∈M′r
max
z∈Zc |z⊤Γpos z −z⊤bΓpos z|
(17)
is given by (10). Moreover, the resulting approximation is
z⊤bΓpos z = z⊤Γpr z −z⊤b,
b =
r
X
i=1
δ2
i
1 + δ2
i
(bw⊤
i z)bwi,
(18)
and the associated approximation error is
|z⊤(bΓpos −Γpos) z| = z⊤
 
n
X
i=r+1
δ2
i
1 + δ2
i
(bw⊤
i z)bwi
!
.
(19)
Proof For any given bΓpos ∈M′
r, it is easy to see that the solution of
max
z∈Zc |z⊤Γpos z −z⊤bΓpos z| = z⊤(bΓpos −Γpos) z
is z = cvmax where vmax is the eigenvector of the largest eigenvalue of bΓpos −Γpos, and
the maximum error is c2ρ(bΓpos −Γpos) where ρ(·) is the spectral radius. Thus (17)
becomes minbΓpos∈M′r ρ(bΓpos −Γpos), or equivalently,
min
rank(B)≤r ρ(KK⊤−BB⊤),
and it follows immediately that the optimal bΓpos is given by (10). Substituting (10)
into z⊤bΓpos z and z⊤(bΓpos −Γpos) z yields (18) and (19), respectively.

Approximate empirical Bayesian method for linear inverse problems
10
In principle, of course, ∥z∥2 is not bounded from above, since z is normally
distributed.
However, imposing the boundedness assumption, which considerably
simpliﬁes the theoretical analysis, does not limit the applicability of the method or
aﬀect the optimal solution. Speciﬁcally, we have the following remarks:
Remark 3.1 First, since z follows a Gaussian distribution, one can always choose a
constant c such that the inequality ∥z∥2 ≤c holds with probability arbitrarily close to
one. Second, the minimax solution bΓpos is independent of the value of c.
Now we combine the two approximate treatments, which is essentially to
approximate (6) by (11) with bΓpos given by (10). It is easy to see that the approximation
error is
∆L(θ, z) = 1
2z⊤(bΓpos −Γpos) z + 1
2 log(|bΓpos|/|Γpos|),
(20)
and we have the following result regarding its optimality:
Corollary 3.3 Suppose that we approximate (6) with (11) for some matrix bΓpos ∈M′
r.
The matrix bΓpos given by (10) achieves the minimax approximation error, i.e., it solves
min
bΓpos∈M′r
max
z∈Zc |∆L(θ, z)|.
As both terms on the right hand side of (20) are nonnegative, the corollary is a direct
consequence of Theorems 3.1 and 3.2.
4. Numerical implementation
Here we discuss the numerical implementation of our approximate method to evaluate
(6).
In principle, this involves two computationally intensive components—both
requiring O(n3) computations under standard numerical treatments. The ﬁrst task
is to compute the eigenvalues of bH. Recall that our method only requires the r leading
eigenvalues and associated eigenvectors of bH, which can be computed eﬃciently with a
randomized algorithm for the singular value decomposition (SVD) [20, 13]. The second
task is to compute Spr, the square root of Γpr. As will become clear later, we do not
necessarily need Spr; rather, our algorithm only requires the ability to evaluate SprΩfor
a given matrix Ω. To this end, we resort to the approximation method proposed in [15].
We provide a brief description of the two adopted methods here, tailored according to
our speciﬁc purposes.

Approximate empirical Bayesian method for linear inverse problems
11
4.1. Randomized SVD
The main idea behind the randomized SVD approach is to identify a subspace that
captures most of the action of a matrix, using random sampling [20, 13, 10]. The
original matrix is then projected onto this subspace, yielding a smaller matrix, and a
standard SVD is then applied to the smaller matrix to obtain the leading eigenvalues
of the original matrix. A randomized algorithm for constructing the rank–r′ SVD of
an n × n matrix bH, given in [13], proceeds as follows:
(i) Draw an n × r′ Gaussian random matrix Ω;
(ii) Form the n × r′ sample matrix Y = bHΩ;
(iii) Form an n × r′ matrix Q with orthonormal columns, such that Y = QR;
(iv) Form the r′ × n matrix B = QT bH;
(v) Compute the SVD of the r′ × n matrix B = ˆUΣV T.
According to [13], for this algorithm to be robust, it is important to oversample a bit,
namely to choose r′ > r if r is the desired rank.
One can obtain a probabilistic error
bound, i.e., that
∥bH −QB∥2 ≤(1 + 11
√
r′n)δr+1,
(21)
holds with a probability at least 1−6(r′ −r)−(r′−r), under some very mild assumptions
on r′ [13].
Note that the diagonal entries of Σ are the eigenvalues of bH and the
columns of V are the eigenvectors. Since we obtain r′ eigenpairs in the algorithm,
we take {δi, wi}r
i=1 to be the r dominant ones among them.
Above we only
present the basic implementation of the randomized SVD method; further details and
possible improvements of the method can be found in [13] and the references therein.
Finally we also want to emphasize that our approximate EB method can use the
eigenvalues/eigenvectors computed with any numerical approach; it is not tailored to
or tied with the randomized SVD.
4.2. The Chebyshev spectral method for computing Spr
Recall that bH = S⊤
prH Spr, and as a result the randomized SVD only requires evaluating
SprΩwhere Ωis a randomly generated n × r′ matrix. Here we adopt the approximate
method proposed in [15], which is based on the following lemma:
Lemma 4.1 (Lemma 2.1 in [15]) Suppose that D is a real symmetric positive
deﬁnite matrix. Then there exists a polynomial p(·) such that
√
D = p(D), and the
degree of p is equal to the number of distinct eigenvalues of D minus 1.

Approximate empirical Bayesian method for linear inverse problems
12
Though
√
D is exactly equal to a polynomial of D, the degree of the polynomial might
be very large if D has a large number of distinct eigenvalues. Thus, instead of trying to
ﬁnd the exact polynomial p in D that equals
√
D, the aforementioned work computes
a Chebyshev approximation to it.
It is clear that D = OΛOT, where Λ = diag(λ1 · · · λn), and λ1 ≥· · · ≥λn. Now
suppose that we have a polynomial p(·) such that p(λ) ≈
√
λ. Then we have
p(D) = O p(Λ) OT
= O diag(p(λ1) · · · p(λn)) OT
≈O diag(
p
λ1 · · ·
p
λn) OT
=
√
D.
That is, once we have an approximation of
√
λ in the interval [λmin, λmax], where
λmin and λmax are respectively lower and upper bounds on the eigenvalues of D,
we immediately get an approximation of
√
D.
A popular method to construct
the approximation p(·) is the Chebyshev polynomial interpolation.
The standard
Chebyshev polynomials are deﬁned in [−1, 1] as [24],
Tk(x) = cos(k arccos x),
∀k ∈N,
(22)
and the associated Chebyshev points are given by
xj = cos[(2j + 1)π
2k + 2 ],
j = 0, 1, . . . , k.
It is well known that the Chebyshev interpolant has spectral accuracy for analytical
functions on [−1, 1]; see, e.g. [12].
Here since we intend to approximate the function
√
λ over the range [λmin, λmin]
rather than [−1, 1], we shall use the scaled and shifted Chebyshev polynomials:
˜Tk(x) = Tk(tax + tb),
∀n ∈N,
(23)
and the associated Chebyshev points become
xj = λmax + λmin
2
+ λmax −λmin
2
cos((2j + 1)π
2k + 2 ),
j = 0, 1, . . . , k. (24)
The interpolant pk(x) can be expressed as [24],
pk(x) =
k
X
i=0
ci ˜Ti(x) −c0
2 ,
(25)
where the coeﬃcients ci are given by
ci =
2
k + 1
k+1
X
j=1
√xj ˜Ti(xj).
(26)

Approximate empirical Bayesian method for linear inverse problems
13
Algorithm 1: Chebyshev spectral approximation for computing
√
DΩ
Data: Ω, k
Result: B ≈
√
DΩ
Compute the coeﬃcients, ta, tb, and c0, . . . , ck;
Ω0 := Ω;
Ω1 := taDΩ0 + tbΩ0;
for i = 1 to k −1 do
Ωi+1 := 2(taDΩi + tbΩi) −Ωi−1;
end
B := Pk
i=0 ciΩi −c0
2 Ω0.
It is easy to verify that the scaled and shifted Chebyshev polynomials satisfy the
following recurrence relation,
˜T0
= 1,
˜T1(x)
= tax + tb,
(27)
˜Ti+1(x) = 2(tax + tb) ˜Ti(x) −˜Ti−1(x),
with
ta =
2
λmax −λmin
,
tb = λmax + λmin
λmax −λmin
.
Now recall that we actually want to compute
√
DΩ. Taking advantage of the recurrence
relation (27), we obtain,
√
DΩ≈pk(D)Ω=
k
X
i=0
ci ˜Ti(D)Ω−c0Ω
2
=
k
X
i=0
ciΩi −c0
2 Ω0,
(28)
where
Ω0
= Ω,
Ω1
= taDΩ0 + tbΩ0,
Ωi+1 = 2(taDvi + tbΩi) −Ωi−1,
for i = 1, . . . , k −1. The complete procedure for computing
√
DΩwith the Chebyshev
approximation is given in Algorithm 1.
Finally we provide some remarks regarding the implementation of the method:
• The original algorithm present in [15] is to compute the product of
√
D and a
vector, but as is shown in Algorithm 1, its extension to the computation of the
product of
√
D and a matrix is straightforward.

Approximate empirical Bayesian method for linear inverse problems
14
• The method requires upper and lower bounds on the eigenvalues of D, which are
computed with an algorithm based on the safeguarded Lanczos method [15]. As
is discussed in [15], these estimates need not be of high accuracy.
• The error bound of the proposed Chebyshev approximation is given by Theorems
3.3 and 3.4 in [15].
• If desired, all the matrix-vector multiplications can be performed with the fast
multipole method to further improve eﬃciency [15].
4.3. The complete algorithm
Now we summarize the complete scheme for constructing the approximation
bL(θ, z) of (6) with a given rank r, using the methods discussed in Sections 4.1 and
4.2:
(i) Compute the ﬁrst r eigenpairs, {(δ2
i , wi)}r
i=1, of bH(θ) = SprHSpr, using the
randomized SVD method and the Chebyshev spectral method.
(ii) Let bwi = Sprwi and evaluate
bL(θ, z) = z⊤Γpr z −z⊤
r
X
i=1
δ2
i
1 + δ2
i
(bw⊤
i z)bwi +
r
X
i=1
log(1 + δ2
i ).
5. Examples
5.1. An image deblurring problem
We ﬁrst test our method on an imaging deblurring problem, which involves recovering
a latent image from noisy observations of a blurred version of the image [14].
In
particular, it is assumed that the blurred image is obtained as a convolution of the
latent image with a point spread function (PSF), and as a result the forward model is:
y(t1, t2) =
Z Z
D
fPSF(t1, t2)x(τ1, τ2)dτ1dτ2,
(29)
where fPSF(t1, t2) is the PSF and D is the domain of the image. In this example we
take the image domain to be D = [−1, 1]2 and the PSF to be
fPSF(t1, t2) = exp[−((t1 −τ1)2 + (t2 −τ2)2)/t],
where t is a parameter controlling the size of the spreading. Moreover, we assume that
the data y are measured at m = 642 = 4096 observation locations evenly distributed in
D, and that the observation errors are mutually independent and Gaussian with zero

Approximate empirical Bayesian method for linear inverse problems
15
mean and variance (0.1)2. We represent the unknown x on 256×256 mesh grid points,
and thus the dimensionality of the inverse problem is n = 65536. The prior on x is a
Gaussian distribution with zero mean and covariance kernel [31, 27]:
K(t, t′) = σ2 21−ν
Γ(ν)
√
2ν d
ρ
ν
Bν
√
2ν d
ρ

,
(30)
where d = ∥t −t′∥2, Γ(·) is the Gamma function, and Bν(·) is the modiﬁed Bessel
function. (30) is known as the Mat´ern covariance, and several authors have suggested
that such covariances can often provide better models for many realistic physical
processes than the popular squared exponential covariance [31, 27]. A random function
with the Mat´ern covariance is [ν −1] mean-square (MS) diﬀerentiable. In this example,
we choose ν = 3 implying second order MS diﬀerentiability.
We set the standard
deviation σ to one. The correlation length ρ is treated as a hyperparameter to be
inferred in this example.
We now use the proposed empirical Bayesian method to solve the inverse problem.
We ﬁrst assume that the true correlation length is ρ = 1, and randomly generate a
true image from the associated prior, shown in Fig. 1 (left). We then test two cases of
the forward problem, with t = 0.002 and t = 0.02, where the latter yields an inverse
problem that is much more ill-posed than the former. We assume that the data are
observed on a 128×128 uniformly distributed mesh. We then apply the two convolution
operators to the generated image and add observational noise to the results, producing
the synthetic data also shown in Fig. 1.
Using the proposed approximate EB method, we evaluate the negative log marginal
likelihood function L over a range of ρ values, for both cases of t, and plot the results
in Fig 2. For t = 0.002 (left ﬁgure), we compute L with ranks r = 300, 400 and
500, and 600, and observe that the curves converge as the rank increases; indeed, the
results with r = 500 and 600 appear identical. For t = 0.02—because the problem
is more ill-posed—we can implement the method with ranks r = 50, 75, 100, and
150 and observe convergence. In particular, while the results of r = 50 deviate from
the others, the results with r = 75, 100, and 150 look nearly identical, implying that
r = 75 is suﬃcient for an accurate approximation of the marginal likelihood in this
case. For both cases, the optimal value of ρ is found to be 0.1, which is actually the
true hyperparameter value. We then compute the posterior mean of x, after ﬁxing
ρ = 0.1 in the Gaussian prior on x, and show the results in Fig. 3.
Note that the EB method is able to ﬁnd the appropriate hyperparameter values
in this example. Nevertheless, our intention is not to illustrate that the EB method
can always identify the correct value of the hyperparameters; rather, the main purpose

Approximate empirical Bayesian method for linear inverse problems
16
t1
-1
-0.6
-0.2
0.2
0.6
1
t2
-1
-0.6
-0.2
0.2
0.6
1
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
t1
-1
-0.6
-0.2
0.2
0.6
1
t2
-1
-0.6
-0.2
0.2
0.6
1
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
t1
-1
-0.6
-0.2
0.2
0.6
1
t2
-1
-0.6
-0.2
0.2
0.6
1
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
Figure 1. From left to right: the ground truth, the simulated data at t = 0.002 and
that at t = 0.02.
ρ
0
0.04
0.08
0.12
0.16
0.2
L(ρ)
-2700
-2650
-2600
-2550
-2500
-2450
-2400
-2350
300
400
500
600
ρ
0
0.04
0.08
0.12
0.16
0.2
L(ρ)
-370
-360
-350
-340
-330
-320
-310
-300
-290
50
75
100
150
Figure 2. The negative log marginal posterior function plotted against ρ, for a series
of ranks r indicated in the legend. Left: t = 0.002; right: t = 0.02.
t1
-1
-0.5
0
0.5
1
t2
-1
-0.5
0
0.5
1
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
t1
-1
-0.5
0
0.5
1
t2
-1
-0.5
0
0.5
1
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
Figure 3. Posterior mean for the t = 0.002 case. Left: result for ρ = 0.1 (optimal);
right: result for ρ = 0.2.

Approximate empirical Bayesian method for linear inverse problems
17
t1
-1
-0.5
0
0.5
1
t2
-1
-0.5
0
0.5
1
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
t1
-1
-0.5
0
0.5
1
t2
-1
-0.5
0
0.5
1
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
Figure 4. Posterior mean for the t = 0.02 case. Left: result for ρ = 0.1 (optimal);
right: result for ρ = 0.02.
of the example is to demonstrate that, should one choose to use the EB method,
the proposed approximations can eﬃciently and accurately compute the marginal
likelihood.
5.2. X-ray computed tomography
Our second example is an X-ray computed tomography (CT) problem. X-ray CT is
a popular diagnostic tool for medical tomographic imaging of the human body. It
provides images of the object by assigning an X-ray attenuation coeﬃcient to each
pixel [22]. Speciﬁcally let u denote the unknown image supported in the unit ball
B(0, 1) in R2. In the case of two-dimensional parallel beam CT, the projection data (or
sinogram) f for each ϕ ∈[0, 2π) and s ∈R is obtained via the following Radon
transform [25]:
f(ϕ, s) =
Z ∞
−∞
u(sθ + tθ⊥)dt
(31)
where θ = (cos ϕ, sin ϕ) and θ⊥= (−sin ϕ, cos ϕ).
We test the problem using a
256 × 256 ground truth image shown in Fig. 5 (left), which is taken from the Harvard
Whole Brain Atlas [16]. The projection data, shown in Fig. 5 (right), is simulated by
plugging the true image into the Radon transform and adding measurement noise to
the result. The noise is taken to be Gaussian with zero mean and a signal-to-noise ratio
of 1%. The size of the discrete Radon transform operator depends on both the size of
the image and the number of projections. In our simulation, we used 60 projections

Approximate empirical Bayesian method for linear inverse problems
18
equispatially sampled from 0 to π.
To avoid an inverse crime [18], we recover the
image with a 128 × 128 resolution, and thus the dimensionality of the inverse problem
is 1282. We take the prior distribution to be zero-mean Gaussian, with a covariance
matrix given by an anisotropic Mat´ern kernel:
K(t, t′) = σ2 21−ν
Γ(ν)
√
2νd(t, t′)
ν
Bν
√
2νd(t, t′)

,
(32)
where
d(t, t′) =
s
(t1 −t′
1)2
ρ2
1
+ (t2 −t′
2)2
ρ2
2
.
Now we consider all four parameters ν, σ, ρ1 and ρ2 as hyperparameters to be
determined, and we also assume that the variance ϵ2 of the measurement noise is not
available. We will identify the ﬁve parameters using the proposed low-rank approximate
EB method.
We solve the optimization problems with 9 diﬀerent ranks (r = 500, 1000,
1500, 2000, 2500, 3000, 3500, 4000, 4500), and plot the optimal values of the ﬁve
hyperparameters and the associated value of L versus the rank r in Fig. 6. As one
can see, the obtained optimal values of the hyperparameters converge as the rank
increases, and in particular r = 4000 is suﬃcient to obtain accurate estimates of the
hyperparameters. Next we shall illustrate that the obtained hyperparameter values
can lead to rather good inference results. To show this, we compute the posterior
mean and the variance of the image using the hyperparameter values computed with
the various ranks, and we then compute the peak noise-to-signal ratio (PSNR) of the
posterior means against the true image, which is a commonly used measure of quality
for image reconstruction.
Note that the low rank approximation is only used to
obtain the hyperparameters; given these hyperparameter values, the posterior means
and variances are computed directly without approximation. We plot the PSNR as
a function of the rank r in Fig. 7. We also show the posterior mean and variance
ﬁeld associated with each rank in the plot. One can see from the ﬁgure that, as the
rank increases, the resulting hyperparameters can recover images with higher quality
in terms of PSNR. Finally, in Fig. 8, we plot the posterior mean and variance using the
hyperparameters computed with r = 4500, which are taken to be the inference results
of this problem.
These results demonstrate that the outcome of Bayesian inference can depend
critically on the hyperparameters, and that, at least in this example, the proposed
method can provide reasonable values for these parameters, provided that a suﬃciently

Approximate empirical Bayesian method for linear inverse problems
19
-1  
-0.6
-0.2
0.2 
0.6 
1   
x1
-1  
-0.6
-0.2
0.2 
0.6 
1   
x2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0  
40 
80 
120
160
rotation angle (degrees)
128 
96  
64  
32  
0   
-32 
-64 
-96 
-128
sensor position (pixels)
0
10
20
30
40
50
60
70
80
90
100
Figure 5. Left: the ground truth image. Right: the projection data simulated from
the ground truth.
1000
2000
3000
4000
0
0.02
0.04
0.06
0.08
0.1
1
1000
2000
3000
4000
0
0.02
0.04
0.06
0.08
0.1
2
1000
2000
3000
4000
0
0.5
1
1.5
1000
2000
3000
4000
0
0.5
1
1.5
1000
2000
3000
4000
0
0.5
1
1.5
1000
2000
3000
4000
-4
-2
0
2
4
6
104
Figure 6. The optimal value of the ﬁve hyperparameters and the associated value
of L (bottom, right) plotted against the rank r used in the EB approximation.
large rank is used.
We also note here that, to better recover images with sharp
edges, one might resort to more complex Gaussian hypermodels [2, 4, 5] than the
one considered here, or even non-Gaussian models such as the Besov [9, 19] and TV-
Gaussian [34] priors. Nevertheless, if one has chosen to use a Gaussian hypermodel,
our method can eﬃciently determine the values of the associated hyperparameters.

Approximate empirical Bayesian method for linear inverse problems
20
500 
1000
1500
2000
2500
3000
3500
4000
4500
0 
10
20
30
40
PSNR
1.7
12.5
25.1
26.7
26.7
24.9
22.8
19.0
23.1
Figure 7. The PSNR of the posterior mean versus the rank r (red line). Above
and below the line, we also show the posterior mean and variance obtained with the
hyperparameters computed with each rank.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
5.5
6
6.5
7
7.5
8
8.5
9
10-3
Figure 8.
The posterior mean and the posterior variance computed using the
optimized hyperparameter values with r = 4500.

Approximate empirical Bayesian method for linear inverse problems
21
6. Conclusions
This paper investigates empirical Bayesian methods for determining hyperparameters
in linear inverse problems. We develop an eﬃcient numerical method to approximately
evaluate the marginal likelihood function of the hyperparameters for large-scale
problems, based on a low-rank approximation of the update from the prior to the
posterior covariance. The proposed method can achieve a computational complexity
of O(n2r) for r ≪n, while a standard full-rank or direct evaluation approach requires
computations of O(n3) complexity.
We also show that this approximation of the
marginal likelihood is optimal in a minimax sense. Two numerical examples illustrate
that the proposed algorithm can accurately evaluate the marginal likelihood function
required in the EB method. This approach may be useful in a wide range of applications
where the unknown is of very high dimension, such as medical and geophysical image
reconstruction.
Finally, it is worth noting that, while the approximate EB method
presented in this work can only be applied to linear inverse problems, there have been
several eﬀorts successfully extending low-rank posterior approximations to nonlinear
problems [21, 8]; along these lines, we expect that our method could also be extended
to EB computations for nonlinear inverse problems.
Acknowledgement
Li was partially supported by the NSFC under grant number 11771289. Marzouk was
partially supported by the US Department of Energy, Oﬃce of Advanced Scientiﬁc
Computing Research, under grant number de-sc0009297.
[1] Alen Alexanderian, Philip J Gloor, Omar Ghattas, et al.
On Bayesian A-and D-optimal
experimental designs in inﬁnite dimensions. Bayesian Analysis, 11(3):671–695, 2016.
[2] Johnathan M Bardsley, Daniela Calvetti, and Erkki Somersalo. Hierarchical regularization for
edge-preserving reconstruction of PET images. Inverse Problems, 26(3):035010, 2010.
[3] Tan Bui-Thanh, Omar Ghattas, James Martin, and Georg Stadler. A computational framework
for inﬁnite-dimensional Bayesian inverse problems part i: The linearized case, with application
to global seismic inversion. SIAM Journal on Scientiﬁc Computing, 35(6):A2494–A2523, 2013.
[4] Daniela Calvetti and Erkki Somersalo. A Gaussian hypermodel to recover blocky objects. Inverse
problems, 23(2):733, 2007.
[5] Daniela Calvetti and Erkki Somersalo. Hypermodels in the Bayesian imaging framework. Inverse
Problems, 24(3):034013, 2008.
[6] Bradley P Carlin and Thomas A Louis. Bayes and empirical Bayes methods for data analysis.
Chapman & Hall/CRC Boca Raton, 2000.
[7] George Casella. An introduction to empirical Bayes data analysis. The American Statistician,
39(2):83–87, 1985.

Approximate empirical Bayesian method for linear inverse problems
22
[8] Tiangang Cui, James Martin, Youssef M Marzouk, Antti Solonen, and Alessio Spantini.
Likelihood-informed dimension reduction for nonlinear inverse problems.
Inverse Problems,
30(11):114015, 2014.
[9] Masoumeh Dashti, Stephen Harris, and Andrew Stuart.
Besov priors for Bayesian inverse
problems. Inverse Problems and Imaging, 6(2):183–200, 2012.
[10] Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast Monte Carlo algorithms for matrices
ii: Computing a low-rank approximation to a matrix. SIAM Journal on Computing, 36(1):158–
183, 2006.
[11] H Pearl Flath, Lucas C Wilcox, Volkan Ak¸celik, Judith Hill, Bart van Bloemen Waanders, and
Omar Ghattas.
Fast algorithms for Bayesian uncertainty quantiﬁcation in large-scale linear
inverse problems based on low-rank partial hessian approximations. SIAM Journal on Scientiﬁc
Computing, 33(1):407–432, 2011.
[12] Bengt Fornberg. A practical guide to pseudospectral methods, volume 1. Cambridge university
press, 1998.
[13] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review,
53(2):217–288, 2011.
[14] Per Christian Hansen, James G Nagy, and Dianne P O’leary.
Deblurring images: matrices,
spectra, and ﬁltering. SIAM, 2006.
[15] Shidong Jiang, Zhi Liang, and Jingfang Huang.
A fast algorithm for Brownian dynamics
simulation with hydrodynamic interactions. Mathematics of Computation, 82(283):1631–1645,
2013.
[16] Keith A. Johnson and J. Alex Becker. Whole Brain Atlas, http://www.med.harvard.edu/aanlib/.
[17] Jari Kaipio and Erkki Somersalo. Statistical and computational inverse problems, volume 160.
Springer, 2005.
[18] Jari Kaipio and Erkki Somersalo. Statistical inverse problems: discretization, model reduction
and inverse crimes. Journal of computational and applied mathematics, 198(2):493–504, 2007.
[19] Matti Lassas, Eero Saksman, and Samuli Siltanen. Discretization-invariant Bayesian inversion
and besov space priors. Inverse Problems and Imaging, 3(1):87–122, 2009.
[20] Edo Liberty, Franco Woolfe, Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert.
Randomized algorithms for the low-rank approximation of matrices.
Proceedings of the
National Academy of Sciences, 104(51):20167–20172, 2007.
[21] James Martin, Lucas C Wilcox, Carsten Burstedde, and Omar Ghattas. A stochastic Newton
MCMC method for large-scale statistical inverse problems with application to seismic inversion.
SIAM Journal on Scientiﬁc Computing, 34(3):A1460–A1487, 2012.
[22] Frank Natterer. The mathematics of computerized tomography. SIAM, 2001.
[23] Sonia Petrone, Judith Rousseau, and Catia Scricciolo.
Bayes and empirical Bayes: do they
merge? Biometrika, page ast067, 2014.
[24] William H Press.
Numerical recipes 3rd edition: The art of scientiﬁc computing.
Cambridge
university press, 2007.
[25] Johann Radon.
¨Uber die Bestimmung von Funktionen durch ihre Integralwerte l¨angs gewisser
Mannigfaltigkeiten. Mathematisch-Physische Klasse, 69:262–277, 1917.
[26] Calyampudi Radhakrishna Rao.
Information and accuracy attainable in the estimation of
statistical parameters. Bull. Calcutta Math. Soc, 37(3):81–91, 1945.

Approximate empirical Bayesian method for linear inverse problems
23
[27] Carl Edward Rasmussen. Gaussian processes for machine learning. MIT Press, 2006.
[28] Judith Rousseau and Botond Szabo.
Asymptotic behaviour of the empirical bayes posteriors
associated to maximum marginal likelihood estimator. arXiv preprint arXiv:1504.04814, 2015.
[29] Alessio Spantini, Tiangang Cui, Karen Willcox, Luis Tenorio, and Youssef Marzouk.
Goal-
oriented optimal approximations of Bayesian linear inverse problems.
arXiv preprint
arXiv:1607.01881, 2016.
[30] Alessio Spantini, Antti Solonen, Tiangang Cui, James Martin, Luis Tenorio, and Youssef
Marzouk.
Optimal low-rank approximations of Bayesian linear inverse problems.
SIAM
Journal on Scientiﬁc Computing, 37(6):A2451–A2487, 2015.
[31] Michael L Stein.
Interpolation of spatial data: some theory for kriging.
Springer Science &
Business Media, 2012.
[32] Andrew M. Stuart. Inverse problems: a Bayesian perspective. Acta Numerica, 19:451–559, 2010.
[33] Albert Tarantola. Inverse problem theory and methods for model parameter estimation. siam,
2005.
[34] Zhewei Yao, Zixi Hu, and Jinglai Li.
A TV-Gaussian prior for inﬁnite-dimensional Bayesian
inverse problems and its numerical implementations. Inverse Problems, 32(7):075006, 2016.

