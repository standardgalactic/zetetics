Framing RNN as a kernel method:
A neural ODE approach
Adeline Fermanian1∗
Pierre Marion1∗
Jean-Philippe Vert2
Gérard Biau1
1 Sorbonne Université, CNRS, LPSM, Paris
{adeline.fermanian, pierre.marion, gerard.biau}@sorbonne-universite.fr
2 Google Research, Brain team, Paris
jpvert@google.com
Abstract
Building on the interpretation of a recurrent neural network (RNN) as a continuous-
time neural differential equation, we show, under appropriate conditions, that the
solution of a RNN can be viewed as a linear function of a speciﬁc feature set of
the input sequence, known as the signature. This connection allows us to frame
a RNN as a kernel method in a suitable reproducing kernel Hilbert space. As a
consequence, we obtain theoretical guarantees on generalization and stability for a
large class of recurrent networks. Our results are illustrated on simulated datasets.
1
Introduction
Recurrent neural networks (RNN) are among the most successful methods for modeling sequential
data. They have achieved state-of-the-art results in difﬁcult problems such as natural language
processing (e.g., Mikolov et al., 2010; Collobert et al., 2011) or speech recognition (e.g., Hinton
et al., 2012; Graves et al., 2013). This class of neural networks has a natural interpretation in terms
of (discretization of) ordinary differential equations (ODE), which casts them in the ﬁeld of neural
ODE (Chen et al., 2018). This observation has led to the development of continuous-depth models
for handling irregularly-sampled time-series data, including the ODE-RNN model (Rubanova et al.,
2019), GRU-ODE-Bayes (De Brouwer et al., 2019), or neural CDE models (Kidger et al., 2020;
Morrill et al., 2020a). In addition, the time-continuous interpretation of RNN allows to leverage
the rich theory of differential equations to develop new recurrent architectures (Chang et al., 2019;
Herrera et al., 2020; Erichson et al., 2021), which are better at learning long-term dependencies.
On the other hand, the development of kernel methods for deep learning offers theoretical insights on
the functions learned by the networks (Cho and Saul, 2009; Belkin et al., 2018; Jacot et al., 2018).
Here, the general principle consists in deﬁning a reproducing kernel Hilbert space (RKHS)—that is, a
function class H —, which is rich enough to describe the architectures of networks. A good example
is the construction of Bietti and Mairal (2017, 2019), who exhibit a RKHS for convolutional neural
networks. This kernel perspective has several advantages. First, by separating the representation of
the data from the learning process, it allows to study invariances of the representations learned by
the network. Next, by reducing the learning problem to a linear one in H , generalization bounds
can be more easily obtained. Finally, the Hilbert structure of H provides a natural metric on neural
networks, which can be used for example for regularization (Bietti et al., 2019).
Contributions.
By taking advantage of the neural ODE paradigm for RNN, we show that RNN
are, in the continuous-time limit, linear predictors over a speciﬁc space associated with the signature
of the input sequence (Levin et al., 2013). The signature transform, ﬁrst deﬁned by Chen (1958) and
∗Equal contribution
Preprint. Under review.
arXiv:2106.01202v1  [stat.ML]  2 Jun 2021

central in rough path theory (Lyons et al., 2007; Friz and Victoir, 2010), summarizes sequential inputs
by a graded feature set of their iterated integrals. Its natural environment is a tensor space that can
be endowed with a RKHS structure (Király and Oberhauser, 2019). We exhibit general conditions
under which classical recurrent architectures such as feedforward RNN, Gated Recurrent Units (GRU,
Cho et al., 2014), or Long Short-Term Memory networks (LSTM, Hochreiter and Schmidhuber,
1997), can be framed as a kernel method in this RKHS. This enables us to provide generalization
bounds for RNN as well as stability guarantees via regularization. The theory is illustrated with some
experimental results.
Related works.
The neural ODE paradigm was ﬁrst formulated by Chen et al. (2018) for residual
neural networks. It was then extended to RNN in several articles, with a focus on handling irregularly
sampled data (Rubanova et al., 2019; Kidger et al., 2020) and learning long-term dependencies
(Chang et al., 2019). The signature transform has recently received the attention of the machine
learning community (Levin et al., 2013; Kidger et al., 2019; Liao et al., 2019; Toth and Oberhauser,
2020; Fermanian, 2021) and, combined with deep neural networks, has achieved state-of-the-art
performance for several applications (Yang et al., 2016, 2017; Perez Arribas, 2018; Wang et al., 2019;
Morrill et al., 2020b). Király and Oberhauser (2019) use the signature transform to deﬁne kernels for
sequential data and develop fast computational methods. The connection between continuous-time
RNN and signatures has been pointed out by Lim (2021) for a speciﬁc model of stochastic RNN.
Deriving generalization bounds for RNN is an active research area (Zhang et al., 2018; Akpinar et al.,
2019; Tu et al., 2019). By leveraging the theory of differential equations, our approach encompasses
a large class of RNN models, ranging from feedforward RNN to LSTM. This is in contrast with most
existing generalization bounds, which are architecture-dependent. Close to our point of view is the
work of Bietti and Mairal (2017) for convolutional neural networks.
Mathematical context.
We place ourselves in a supervised learning setting. The input data is
a sample of n i.i.d. vector-valued sequences {x(1), . . . , x(n)}, where x(i) = (x(i)
1 , . . . , x(i)
T ) ∈
(Rd)T , T ≥1. The outputs of the learning problem can be either labels (classiﬁcation setting)
or sequences (sequence-to-sequence setting). Even if we only observe discrete sequences, each
x(i) is mathematically considered as a regular discretization of a continuous-time process X(i) ∈
BV ([0, 1], Rd), where BV ([0, 1], Rd) is the space of continuous functions from [0, 1] to Rd of ﬁnite
total variation. Informally, the total variation of a process corresponds to its length. Formally, for any
[s, t] ⊂[0, 1], the total variation of a process X ∈BV ([0, 1], Rd) on [s, t] is deﬁned by
∥X∥T V ;[s,t] =
sup
(t0,...,tk)∈Ds,t
k
X
j=1
∥Xtj −Xtj−1∥,
where Ds,t denotes the set of all ﬁnite partitions of [s, t] and ∥· ∥the Euclidean norm. We therefore
have that x(i)
j
= X(i)
j/T, 1 ≤j ≤T, where X(i)
t
:= X(i)(t). We make two assumptions on the
processes X(i). First, they all begin at zero, and second, their lengths are bounded by L ∈(0, 1).
These assumptions are not too restrictive, since they amount to data translation and normalization,
common in practice. Accordingly, we denote by X the subspace of BV ([0, 1], Rd) deﬁned by
X =

X ∈BV ([0, 1], Rd) | X0 = 0
and
∥X∥T V ;[0,1] ≤L
	
and assume therefore that X(1), . . . , X(n) are i.i.d. according to some X ∈X . The norm on all
spaces Rm, m ≥1, is always the Euclidean one. Observe that assuming that X ∈X implies that,
for any t ∈[0, 1], ∥Xt∥= ∥Xt −X0∥≤∥X∥T V ;[0,1] ≤L.
Recurrent neural networks.
Classical RNN are deﬁned by a sequence of hidden states
h1, . . . , hT ∈Re, where, for x = (x1, . . . , xT ) a generic data sample,
hj+1 = f(hj, xj+1),
h0 = 0.
At each time step 1 ≤j ≤T, the output of the network is zj = ψ(hj), where ψ is a linear function.
In the present article, we rather consider the following residual version, which is a natural adaptation
of classical RNN in the neural ODE framework (see, e.g., Yue et al., 2018):
hj+1 = hj + 1
T f(hj, xj+1),
h0 = 0.
(1)
2

The simplest choice for the function f is the feedforward model, say fRNN, deﬁned by
fRNN(h, x) = σ(Uh + V x + b),
(2)
where σ is an activation function, U ∈Re×e and V ∈Re×d are weight matrices, and b ∈Re is the
bias. The function fRNN, equipped with a smooth activation σ (such as the logistic or hyperbolic
tangent functions), will be our leading example throughout the paper. However, the GRU and LSTM
models can also be rewritten under the form (1), as shown in Appendix A.1. Thus, model (1) is
ﬂexible enough to encompass most recurrent networks used in practice.
Overview.
Section 2 is devoted to framing RNN as linear functions in a suitable RKHS. We start
by embedding iteration (1) into a continuous-time model, which takes the form of a controlled
differential equation (CDE). This allows, after introducing the signature transform, to deﬁne the
appropriate RKHS, and, in turn, to show that model (1) boils down, in the continuous-time limit, to a
linear problem on the signature. This framework is used in Section 3 to derive generalization bounds
and stability guarantees. We conclude with some experiments in Section 4. All proofs are postponed
to the supplementary material.
2
Framing RNN as a kernel method
Roadmap.
First, we quantify the difference between the discrete recurrent network (1) and its
continuous-time counterpart (Proposition 1). Then, we rewrite the corresponding ODE as a CDE
(Proposition 2). Under appropriate conditions, Proposition 4 shows that the solution of this equation
is a linear function of the signature of the driving process. Importantly, these assumptions are valid
for a feedforward RNN, as stated by Proposition 5. We conclude in Theorem 1.
2.1
From discrete to continuous time
Recall that h0, . . . , hT denote the hidden states of the RNN (1), and let H : [0, 1] →Re be the
solution of the ODE
dHt = f(Ht, Xt)dt,
H0 = h0.
(3)
By bounding the difference between Hj/T and hj, the following proposition shows how to pass from
discrete to continuous time, provided f satisﬁes the following assumption:
(A1)
The function f is Lipschitz continuous in h and x, with Lipschitz constants Kh and Kx.
We let Kf = max(Kh, Kx).
Proposition 1. Assume that (A1) is veriﬁed. Then there exists a unique solution H to (3) and, for
any 0 ≤j ≤T,
∥Hj/T −hj∥≤c1
T ,
where c1 = KfeKf  L +
sup
∥h∥≤M,∥x∥≤L
∥f(h, x)∥eKf 
and M =
sup
∥x∥≤L
∥f(h0, x)∥eKf . Moreover,
for any t ∈[0, 1], ∥Ht∥≤M.
Then, following Kidger et al. (2020), we show that the ODE (3) can be rewritten under the form of a
CDE. At the cost of increasing the dimension of the hidden state from e to e + d, this allows us to
reframe model (3) as a linear model in dX, in the sense that X has been moved ‘outside’ of f.
Proposition 2. Assume that (A1) is veriﬁed. Let H : [0, 1] →Re be the solution of (3), and let
¯X : [0, 1] →Rd+1 be the time-augmented process ¯Xt = (X⊤
t , 1−L
2 t)⊤. Then there exists a tensor
ﬁeld F : R¯e →R¯e× ¯d, ¯e = e + d, ¯d = d + 1, such that if ¯H : [0, 1] →R¯e is the solution of the CDE
d ¯Ht = F( ¯Ht)d ¯Xt,
¯H0 = (H⊤
0 , X⊤
0 )⊤,
(4)
then its ﬁrst e coordinates are equal to H.
Equation (4) can be better understood by the following equivalent integral equation:
¯Ht = ¯H0 +
Z t
0
F( ¯Hu)d ¯Xu,
3

where the integral should be understood as Riemann-Stieljes integral (Friz and Victoir, 2010, Sec-
tion I.2). Thus, the output of the RNN can be approximated by the solution of the CDE (4), and,
according to Proposition 1, the approximation error is O(1/T).
Example 1. Consider fRNN as in (2). If σ is Lipschitz continuous with constant Kσ, then, for any
h1, h2 ∈Re, x1, x2 ∈Rd,
∥fRNN(h1, x1) −fRNN(h2, x1)∥= ∥σ(Uh1 + V x1 + b) −σ(Uh2 + V x1 + b)∥
≤Kσ∥U∥op∥h1 −h2∥,
where ∥· ∥op denotes the operator norm—see Appendix A.3. Similarly, ∥f(h1, x1) −f(h1, x2)∥≤
Kσ∥V ∥op∥x1 −x2∥. Thus, assumption (A1) is satisﬁed. The tensor ﬁeld FRNN of Proposition 2
corresponding to this network is deﬁned for any ¯h ∈R¯e by
FRNN(¯h) =

0e×d
2
1−Lσ(W¯h + b)
Id×d
0d×1

,
where
W = (U
V ) ∈Re×¯e.
(5)
2.2
The signature
An essential ingredient towards our construction is the signature of a continuous-time process, which
we brieﬂy present here. We refer to Chevyrev and Kormilitzin (2016) for a gentle introduction and to
Lyons et al. (2007); Levin et al. (2013) for details.
Tensor Hilbert spaces.
We denote by (Rd)⊗k the kth tensor power of Rd with itself, which is a
Hilbert space of dimension dk. The key space to deﬁne the signature and, in turn, our RKHS, consists
in inﬁnite square-summable sequences of tensors of increasing order:
T =
n
a = (a0, . . . , ak, . . . )
 ak ∈(Rd)⊗k,
∞
X
k=0
∥ak∥2
(Rd)⊗k < ∞
o
.
(6)
Endowed with the scalar product ⟨a, b⟩T := P∞
k=0⟨ak, bk⟩(Rd)⊗k, T is a Hilbert space, as shown in
Appendix A.4.
Deﬁnition 1. Let X ∈BV ([0, 1], Rd). For any t ∈[0, 1], the signature of X on [0, t] is deﬁned by
S[0,t](X) = (1, X1
[0,t], . . . , Xk
[0,t], . . . ), where, for each k ≥1,
Xk
[0,t] = k!
Z
· · ·
Z
0≤u1<···<uk≤t
dXu1 ⊗· · · ⊗dXuk ∈(Rd)⊗k.
Although this deﬁnition is technical, the signature should simply be thought of as a feature map that
embeds a bounded variation process into an inﬁnite-dimensional tensor space. The signature has
several good properties that make it a relevant tool for machine learning (e.g., Levin et al., 2013;
Chevyrev and Kormilitzin, 2016; Fermanian, 2021). In particular, under certain assumptions, S(X)
characterizes X up to translations and reparameterizations, and has good approximation properties.
We also highlight that fast libraries exist for computing the signature (Reizenstein and Graham, 2020;
Kidger and Lyons, 2021).
The expert reader is warned that this deﬁnition differs from the usual one by the normalization
of Xk
[0,t] by k!, which is more adapted to our context. When the signature is taken on the whole
interval [0, 1], we simply write S(X) and Xk. In the sequel, for any index (i1, . . . , ik) ⊂{1, . . . , d}k,
S(i1,...,ik)
[0,t]
(X) denotes the term associated with the coordinates (i1, . . . , ik) of Xk
[0,t].
Example 2. Let X be the d-dimensional linear path deﬁned by Xt = (a1 + b1t, . . . , ad + bdt)⊤,
ai, bi ∈R. Then S(i1,...,ik)(X) = bi1 · · · bik and Xk = b⊗k.
The next proposition, which ensures that S[0,t]( ¯X) ∈T , is an important step.
Proposition 3. Let X ∈X and ¯Xt = (X⊤
t , 1−L
2 t)⊤as in Proposition 2. Then, for any t ∈[0, 1],
∥S[0,t]( ¯X)∥T ≤2(1 −L)−1.
4

The signature kernel.
By taking advantage of the structure of Hilbert space of T , it is natural to
introduce the following kernel:
K : X × X →R
(X, Y ) 7→⟨S( ¯X), S( ¯Y )⟩T ,
which is well deﬁned according to Proposition 3. We refer to Király and Oberhauser (2019) for a
general presentation of kernel methods with signatures and to Cass et al. (2020) for a kernel trick.
The RKHS associated with K is the space of functions
H =

ξα : X →R | ξα(X) = ⟨α, S( ¯X)⟩T , α ∈T
	
,
(7)
with scalar product ⟨ξα, ξβ⟩H = ⟨α, β⟩T (see, e.g., Schölkopf and Smola, 2002).
2.3
From the CDE to the signature kernel
An important property of signatures is that the solution of the CDE (4) can be written, under certain
assumptions, as a linear function of the signature of the driving process X. This operation can be
thought of as a Taylor expansion for CDE. More precisely, let us rewrite (4) as
dHt = F(Ht)dXt =
d
X
i=1
F i(Ht)dXi
t,
(8)
where Xt = (X1
t , . . . , Xd
t )⊤, F : Re →Re×d, and F i : Re →Re are the columns of F—to avoid
heavy notation, we momentarily write e, d, H, and X instead of ¯e, ¯d, ¯H, and ¯X. Throughout, the
bold notation is used to distinguish tensor ﬁelds and vector ﬁelds. We recall that a vector ﬁeld
F : Re →Re or a tensor ﬁeld F : Re →Re×d are said to be smooth if each of their coordinates is
C ∞.
Deﬁnition 2. Let F, G : Re →Re be smooth vector ﬁelds and denote by J(·) the Jacobian matrix.
Their differential product is the smooth vector ﬁeld F ⋆G : Re →Re deﬁned, for any h ∈Re, by
(F ⋆G)(h) =
e
X
j=1
∂G
∂hj
(h)Fj(h) = J(G)(h)F(h).
In differential geometry, F ⋆G is simply denoted by FG. Since the ⋆operation is not associative, we
take the convention that it is evaluated from right to left, i.e., F 1 ⋆F 2 ⋆F 3 := F 1 ⋆(F 2 ⋆F 3).
Taylor expansion.
Let H be the solution of (8), where F is assumed to be smooth. We now show
that H can be written as a linear function of the signature of X, which is the crucial step to embed
the RNN in the RKHS H . The step-N Taylor expansion of H (Friz and Victoir, 2008) is deﬁned by
HN
t
= H0 +
N
X
k=1
1
k!
X
1≤i1,...,ik≤d
S(i1,...,ik)
[0,t]
(X)F i1 ⋆· · · ⋆F ik(H0).
Throughout, we let
Λk(F) =
sup
∥h∥≤M,1≤i1,...,ik≤d
∥F i1 ⋆· · · ⋆F ik(h)∥.
Example 3. Let F = FRNN deﬁned by (5) with an identity activation. Then, for any ¯h ∈R¯e,
1 ≤i ≤d + 1, F i
RNN(¯h) = Wi¯h + bi, where bi is the (i + d)th vector of the canonical basis of R¯e,
and
Wi = 0¯e×¯e,
Wd+1 =

2
1−LW
0d×¯e

,
and
bd+1 =

2
1−Lb
0d

.
The vector ﬁelds F i
RNN are then afﬁne, J(F i
RNN) = Wi, and the iterated star products have a simple
expression: for any 1 ≤i1, . . . , ik ≤d, F i1
RNN ⋆· · · ⋆F ik
RNN(¯h) = Wik · · · Wi2(Wi1¯h + bi1).
The next proposition shows that the step-N Taylor expansion HN is a good approximation of H.
Proposition 4. Assume that the tensor ﬁeld F is smooth. Then, for any t ∈[0, 1],
∥Ht −HN
t ∥≤
dN+1
(N + 1)!ΛN+1(F).
(9)
5

Thus, provided that ΛN(F) is not too large, the right-hand side of (9) converges to zero, hence
Ht = H0 +
∞
X
k=1
1
k!
X
1≤i1,...,ik≤d
S(i1,...,ik)
[0,t]
(X)F i1 ⋆· · · ⋆F ik(H0).
We conclude from the above representation that the solution H of (8) is in fact a linear function of the
signature of X. A natural concern is to know whether the upper bound of Proposition 4 vanishes with
N for standard architectures. This property is encapsulated in the following more general assumption:
(A2)
The tensor ﬁeld F is smooth and
∞
X
k=0
dk
k! Λk(F)
2
< ∞.
Clearly, if (A2) is veriﬁed, then the right-hand side of (9) converges to 0. The next proposition states
formally the conditions under which (A2) is veriﬁed for FRNN. It is further illustrated in Figure 1,
which shows that the convergence is fast with two common activation functions. We let ∥σ∥∞=
sup∥h∥≤M,∥x∥≤L ∥σ(Uh + V x + b)∥and ∥σ(k)∥∞= sup∥h∥≤M,∥x∥≤L ∥σ(k)(Uh + V x + b)∥.
Proposition 5. Let FRNN be deﬁned by (5). If σ is the identity function, then (A2) is satisﬁed. In the
general case, (A2) holds if σ is smooth and there exists a > 0 such that, for any k ≥0,
∥σ(k)∥∞≤ak+1k!
and
∥W∥F < 1 −L
8a2d ,
(10)
where ∥· ∥F is the Frobenius norm. Moreover, ΛN(FRNN) ≤
√
2a

8a2∥W ∥F
1−L
N−1
N! .
The proof of Proposition 5, based on the manipulation of higher-order derivatives of tensor ﬁelds, is
highly non-trivial. We highlight that the conditions on σ are mild and veriﬁed for common smooth
activations. For example, they are veriﬁed for the logistic function (with a = 2) and for the hyperbolic
tangent function (with a = 4)—see Appendix A.5. The second inequality of (10) puts a constraint on
the norm of the weights, and can be regarded as a radius of convergence for the Taylor expansion.
Putting everything together.
We now have all the elements at hand to embed the RNN into the
RKHS H . To ﬁx the idea, we assume in this paragraph that we are in a ±1 classiﬁcation setting.
In other words, given an input sequence x, we are interested in the ﬁnal output zT = ψ(hT ) ∈R,
where hT is the solution of (1). The predicted class is 1(zT > 0).
By Propositions 1 and 2, zT is approximated by the ﬁrst e coordinates of the solution of the CDE (4),
which outputs a Re+d-valued process ¯H. According to Proposition 4, ¯H is a linear function of the
signature of the time-augmented process ¯X. Thus, on top of ¯H, it remains to successively apply the
projection Proj on the e ﬁrst coordinates followed by the linear function ψ to obtain an element of the
RKHS H . This mechanism is summarized in the following theorem.
Theorem 1. Assume that (A1) and (A2) are veriﬁed. Then there exists a function ξα ∈H such that
|zT −ξα(X)| ≤∥ψ∥op
c1
T ,
(11)
where ξα(X) = ⟨α, S( ¯X)⟩T and ¯Xt = (X⊤
t , 1−L
2 t)⊤. We have α = (αk)∞
k=0, where each
αk ∈(Rd)⊗k is deﬁned by
α(i1,...,ik)
k
= 1
k!ψ ◦Proj
 F i1 ⋆· · · ⋆F ik( ¯
H0)

.
Moreover, ∥α∥2
T ≤∥ψ∥2
op
P∞
k=0

dk
k! Λk(F)
2
.
We conclude that in the continuous-time limit, the output of the network can be interpreted as a
scalar product between the signature of the (time-augmented) process ¯X and an element of T . This
interpretation is important for at least two reasons: (i) it facilitates the analysis of generalization of
RNN by leveraging the theory of kernel methods, and (ii) it provides new insights on regularization
strategies to make RNN more robust. These points will be explored in the next section. Finally, we
stress that the approach works for a large class of RNN, such as GRU and LSTM. The derivation of
conditions (A1) and (A2) beyond the feedforward RNN is left for future work.
6

3
Generalization and regularization
3.1
Generalization bounds
Learning procedure.
A ﬁrst consequence of framing a RNN as a kernel method is that it gives natu-
ral generalization bounds under mild assumptions. In the learning setup, we are given an i.i.d. sample
of n random pairs of observations (x(i), y(i)) ∈(Rd)T × Y , where x(i) = (x(i)
1 , . . . , x(i)
T ). We
distinguish the binary classiﬁcation problem, where Y = {−1, 1}, from the sequential prediction
problem, where Y = (Rp)T and y(i) = (y(i)
1 , . . . , y(i)
T ). The RNN is assumed to be parameterized
by θ ∈Θ ⊂Rq, where Θ is a compact set. To clarify the notation, we use a θ subscript whenever a
quantity depends on θ (e.g., fθ for f, etc.). In line with Section 2, it is assumed that the tensor ﬁeld
Fθ associated with fθ satisﬁes (A1) and (A2), keeping in mind that Proposition 5 guarantees that
these requirements are fulﬁlled by a feedforward recurrent network with a smooth activation function.
Let gθ : (Rd)T →Y denote the output of the recurrent network. The parameter θ is ﬁtted by
empirical risk minimization using a loss function ℓ: Y × Y →R+. The theoretical and empirical
risks are respectively deﬁned, for any θ ∈Θ, by
R(θ) = E[ℓ(y, gθ(x))]
and
b
Rn(θ) = 1
n
n
X
i=1
ℓ
 y(i), gθ(x(i))

,
where the expectation E is evaluated with respect to the distribution of the generic random pair (x, y).
We let bθn ∈argmin θ∈Θ b
Rn(θ) and aim at upper bounding P(y ̸= gbθn(x)) in the classiﬁcation
regime (Theorem 2) and R(bθn) in the sequential regime (Theorem 3). To reach this goal, our strategy
is to approximate the RNN by its continuous version and then use the RKHS machinery of Section 2.
Binary classiﬁcation.
In this context, the network outputs a real number gθ(x) = ψ(hT ) ∈R and
the predicted class is 1(zT > 0). The loss ℓ: R × R →R+ is assumed to satisfy the assumptions of
Bartlett and Mendelson (2002, Theorem 7), that is, for any y ∈{−1, 1}, ℓ(y, gθ(x)) = φ(ygθ(x)),
where φ(u) ≥1(u ≤0), φ(0) = 0, and φ is Lipschitz-continuous with constant Kℓ. For example,
the cross-entropy loss satisﬁes such assumptions. We let ξαθ ∈H be the function of Theorem 1 that
approximates the RNN with parameter θ. Thus, zT ≈ξαθ( ¯X) = ⟨αθ, S( ¯X)⟩T , up to a O(1/T) term.
Theorem 2. Assume that for all θ ∈Θ, (A1) and (A2) are veriﬁed. Assume, in addition, that there
exists a constant B > 0 such that for any θ ∈Θ, ∥ξαθ∥H ≤B. Then with probability at least 1 −δ,
P
 y ̸= gbθn(x)

≤b
Rn(bθn) + c2
T +
16KℓB
(1 −L)√n + 2BKℓ
1 −L
r
log(1/δ)
2n
,
(12)
where c2 = Kℓsupθ

∥ψ∥opKfθeKfθ  L + ∥fθ∥∞eKfθ 
.
Close to our result are the bounds obtained by Zhang et al. (2018), Tu et al. (2019), and Chen et al.
(2020). The main difference is that the term in 1/T does not usually appear, since it comes from the
continuous viewpoint on RNN, whereas the speed in 1/√n is more classical. The take-home message
is that the detour by continuous-time neural ODE provides a theoretical framework adapted to RNN,
at the modest price of an additional O(1/T) term. Moreover, we note that the bound (12) is ‘simple’
and holds under mild conditions for a large class of RNN. More precisely, for any recurrent network
of the form (1), provided (A1) and (A2) are satisﬁed, then (12) is valid with constants c2 and B
depending on the architecture. Such constants are given below in the example of a feedforward RNN.
Example 4. Take a feedforward RNN with logistic activation, and Θ = {(W, b, ψ) | ∥W∥F ≤
KW < (1 −L)/32d, ∥b∥≤Kb, ∥ψ∥op ≤Kψ}. Then, Proposition 5 states that (A2) is satisﬁed and,
with Theorem 1, ensures that
sup
θ∈Θ
∥ξαθ∥H ≤
√
2Kψ(1 −L)
1 −L −32dKW
:= B,
Kfθ = max(∥U∥op, ∥V ∥op),
and
∥fθ∥∞= 1.
Sequence-to-sequence learning.
We conclude by showing how to extend both the RKHS embed-
ding of Theorem 1 and the generalization bound of Theorem 2 to the setting of sequence-to-sequence
learning. In this case, the output of the network is a sequence
gθ(x) = (z1, . . . , zT ) ∈(Rp)T .
7

An immediate extension of Theorem 1 ensures that there exist p elements α1,θ, . . . , αp,θ ∈T such
that, for any 1 ≤j ≤T,
zj −
 ⟨α1,θ, S[0,j/T]( ¯X)⟩T , . . . , ⟨αp,θ, S[0,j/T]( ¯X)⟩T
⊤ ≤∥ψ∥op
c1
T .
(13)
The properties of the signature guarantee that S[0,j/T](X) = S( ˜X[j]) where ˜X[j] is the process equal
to ¯X on [0, j/T] and then constant on [j/T, 1]—see Appendix A.6. With this trick, we have, for any
1 ≤ℓ≤p, ⟨αℓ,θ, S[0,j/T]( ¯X)⟩T = ⟨αℓ,θ, S( ˜X[j])⟩T , so that we are back in H . Observe that the
only difference with (11) is that we consider vector-valued sequential outputs, which requires to
introduce the process ˜X[j], but that the rationale is exactly the same.
We let ℓ: (Rp)T × (Rp)T →R+ be the L2 distance, that is, for any y = (y1, . . . , yT ), y′ =
(y′
1, . . . , y′
T ), ℓ(y, y′) =
1
T
PT
j=1 ∥yj −y′
j∥2. It is assumed that y takes its values in a compact
subset of Rq, i.e., there exists Ky > 0 such that ∥yj∥≤Ky.
Theorem 3. Assume that for all θ ∈Θ, (A1) and (A2) are veriﬁed. Assume, in addition, that there
exists a constant B > 0 such that for any 1 ≤ℓ≤p, θ ∈Θ, ∥ξαℓ,θ∥H ≤B . Then with probability
at least 1 −δ,
R(bθn) ≤b
Rn(bθn) + c3
T + 8pc4B(1 −L)−1
√n
+
r
2c5 log(1/δ)
n
,
(14)
where c3 = sup
θ
 c1,θ + ∥ψ∥op∥fθ∥∞

+ 2√pB(1 −L)−1 + 2Ky, c4 = B(1 −L)−1 + Ky, and
c5 = 4pB(1 −L)−1c4 + K2
y.
3.2
Regularization and stability
In addition to providing a sound theoretical framework, framing deep learning in a RKHS provides a
natural norm, which can be used for regularization, as shown for example in the context of convo-
lutional neural networks by Bietti et al. (2019). This regularization ensures stability of predictions,
which is crucial in particular in a small sample regime or in the presence of adversarial examples
(Gao et al., 2018; Ko et al., 2019). In our binary classiﬁcation setting, for any inputs x, x′ ∈(Rd)T ,
by the Cauchy-Schwartz inequality, we have
∥zT −z′
T ∥≤2∥ψ∥op∥c1
T + ∥ξαθ( ¯X) −ξαθ( ¯X′)∥≤2∥ψ∥op∥c1
T + ∥ξαθ∥H ∥S( ¯X) −S( ¯X′)∥T .
If x and x′ are close, so are their associated continuous processes X and X′ (which can be approx-
imated for example by taking a piecewise linear interpolation), and so are their signatures. The
term ∥S( ¯X) −S( ¯X′)∥T is therefore small (Friz and Victoir, 2010, Proposition 7.66). Therefore,
when T is large, we see that the magnitude of ∥ξαθ∥H determines how close the predictions are.
A natural training strategy to ensure stable predictions, for the types of networks covered in the
present article, is then to penalize the problem by minimizing the loss b
Rn(θ) + λ∥ξαθ∥2
H . From a
computational point of view, it is possible to compute the norm in H , up to a truncation at N of the
Taylor expansion, which we know by Proposition 4 to be reasonable. It remains that computing this
norm is a non-trivial task, and implementing smart surrogates is an interesting problem for the future.
4
Numerical illustrations
This section is here for illustration purposes. Our objective is not to achieve competitive performance,
but rather to illustrate the theoretical results. We refer to Appendix D for implementation details.
Convergence of the Taylor expansion towards the solution of the ODE.
We illustrate Proposi-
tion 4 on a toy example. The process X is a 2-dimensional spiral, and we take feedforward RNN
with 2 hidden units. Repeating this procedure with 103 uniform random weight initializations, we
observe in Figure 1a that the signature approximation converges exponentially fast in N. As seen
in Figure 1b, the rate of convergence depends in particular on the norm of the weight matrices, as
predicted by Proposition 5. However, condition (10) seems to be over-restrictive, since convergence
happens even for weights with norm larger than the bound (we have 1/(8a2d) ≃0.01 here).
8

1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Step N
10−5
10−4
10−3
10−2
10−1
Error
Activation
sigmoid
tanh
(a) Error on a logarithmic scale as a function of N
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Frobenius norm of the weights
10−8
10−7
10−6
10−5
10−4
10−3
Error for N = 5
(b) Error as a function of the norm of the weights
Figure 1: Approximation of the RNN ODE by the step-N Taylor expansion
0.0
0.2
0.4
0.6
0.8
1.0
ε
0.4
0.5
0.6
0.7
0.8
0.9
Adversarial accuracy
RNN
Penalized RNN
Figure 2: Adversarial accuracy as a function
of the adversarial perturbation ε
Adversarial robustness.
We illustrate the penal-
ization proposed in Section 3.2 on a toy task that
consists in classifying the rotation direction of 2-
dimensional spirals. We take a feedforward RNN
with 32 hidden units and hyperbolic tangent activa-
tion. It is trained on 50 examples, with and without
penalization, for 200 epochs. Once trained, the RNN
is tested on adversarial examples, generated with the
projected gradient descent algorithm with Frobenius
norm (Madry et al., 2018), which modiﬁes test ex-
amples to maximize the error while staying in a ball
of radius ε. We observe in Figure 2 that adding the
penalization seems to make the network more stable.
Comparison of the trained networks.
The evolu-
tion of the Frobenius norm of the weights ∥W∥F and
the RKHS norm ∥ξαθ∥H during training is shown in Figure 3. This points out that the penalization,
which forces the RNN to keep a small norm in H , leads indeed to learning different weights than the
non-penalized RNN. The results also suggest that the Frobenius and RKHS norms are decoupled,
since both networks have Frobenius norms of similar magnitude but very different RKHS norms. The
ﬁgures show one random run, but we observe similar qualitative behavior on others.
0
25
50
75
100
125
150
175
200
Epoch
5
10
15
20
25
30
Frobenius norm of the weights
RNN
Penalized RNN
0
25
50
75
100
125
150
175
200
Epoch
10−3
10−1
101
103
RKHS norm (N=3)
RNN
Penalized RNN
Figure 3: Evolution of the Frobenius norm of the weights and of the RKHS norm during training
5
Conclusion
By bringing together the theory of neural ODE, the signature transform, and kernel methods, we have
shown that a recurrent network can be framed in the continuous-time limit as a linear function in a
well-chosen RKHS. In addition to giving theoretical insights on the function learned by the network
and providing generalization guarantees, this framing suggests regularization strategies to obtain
9

more robust RNN. We have only scratched the surface of the potentialities of leveraging this theory
to practical applications, which is a subject of its own and will be tackled in future work.
Acknowledgements
Authors thank Thierry Lévy for his inputs on the Picard-Lindelöf theorem. A. Fermanian has been
supported by a grant from Région Ile-de-France and P. Marion by a stipend from Corps des Mines.
References
N.-J. Akpinar, B. Kratzwald, and S. Feuerriegel. Sample complexity bounds for recurrent neural
networks with application to combinatorial graph problems. arXiv:1901.10289, 2019.
P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3:463–482, 2002.
M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel learning.
In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, volume 80, pages 541–549. PMLR, 2018.
A. Bietti and J. Mairal. Invariance and stability of deep convolutional representations. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Ad-
vances in Neural Information Processing Systems, volume 30, pages 6210–6220. Curran Associates,
Inc., 2017.
A. Bietti and J. Mairal. Group invariance, stability to deformations, and complexity of deep convolu-
tional representations. Journal of Machine Learning Research, 20:1–49, 2019.
A. Bietti, G. Mialon, D. Chen, and J. Mairal. A kernel perspective for regularizing deep neural
networks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, volume 97, pages 664–674. PMLR, 2019.
T. Cass, T. Lyons, C. Salvi, and W. Yang. Computing the untruncated signature kernel as the solution
of a Goursat problem. arXiv:2006.14794, 2020.
B. Chang, M. Chen, E. Haber, and E. H. Chi. AntisymmetricRNN: A dynamical system view on
recurrent neural networks. In International Conference on Learning Representations, 2019.
K.-T. Chen. Integration of paths–a faithful representation of paths by non-commutative formal power
series. Transactions of the American Mathematical Society, 89:395–407, 1958.
M. Chen, X. Li, and T. Zhao. On generalization bounds of a family of recurrent neural networks. In
S. Chiappa and R. Calandra, editors, Proceedings of the Twenty Third International Conference on
Artiﬁcial Intelligence and Statistics, volume 108, pages 1233–1243, 2020.
R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential
equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 31, pages 6572–6583. Curran
Associates, Inc., 2018.
I. Chevyrev and A. Kormilitzin.
A primer on the signature method in machine learning.
arXiv:1603.03788, 2016.
K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio.
Learning phrase representations using RNN encoder-decoder for statistical machine translation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,
pages 1724–1734. Association for Computational Linguistics, 2014.
Y. Cho and L. Saul. Kernel methods for deep learning. In Y. Bengio, D. Schuurmans, J. Laf-
ferty, C. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems,
volume 22, pages 342–350. Curran Associates, Inc., 2009.
10

R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language
processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.
E. De Brouwer, J. Simm, A. Arany, and Y. Moreau. GRU-ODE-Bayes: Continuous modeling of
sporadically-observed time series. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32,
pages 7379–7390. Curran Associates, Inc., 2019.
N. B. Erichson, O. Azencot, A. Queiruga, L. Hodgkinson, and M. W. Mahoney. Lipschitz recurrent
neural networks. In International Conference on Learning Representations, 2021.
A. Fermanian. Embedding and learning with signatures. Computational Statistics & Data Analysis,
157:107148, 2021.
P. Friz and N. Victoir. Euler estimates for rough differential equations. Journal of Differential
Equations, 244:388–412, 2008.
P. K. Friz and N. B. Victoir. Multidimensional Stochastic Processes as Rough Paths: Theory and
Applications, volume 120 of Cambridge Studies in Advanced Mathematics. Cambridge University
Press, Cambridge, 2010.
J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. Black-box generation of adversarial text sequences to
evade deep learning classiﬁers. In 2018 IEEE Security and Privacy Workshops, pages 50–56, 2018.
A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks.
In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6645–
6649, 2013.
C. Herrera, F. Krach, and J. Teichmann. Theoretical guarantees for learning conditional expectation
using controlled ODE-RNN. arXiv:2006.04727, 2020.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal Processing Magazine, 29:82–97, 2012.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9:1735–1780,
1997.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 31, pages 8580–8589. Curran
Associates, Inc., 2018.
J. Kelly, J. Bettencourt, M. J. Johnson, and D. K. Duvenaud. Learning differential equations that are
easy to solve. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances
in Neural Information Processing Systems, volume 33, pages 4370–4380. Curran Associates, Inc.,
2020.
P. Kidger and T. Lyons. Signatory: Differentiable computations of the signature and logsignature
transforms, on both CPU and GPU. In International Conference on Learning Representations,
2021.
P. Kidger, P. Bonnier, I. Perez Arribas, C. Salvi, and T. Lyons. Deep signature transforms. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 32, pages 3099–3109. Curran Associates, Inc.,
2019.
P. Kidger, J. Morrill, J. Foster, and T. Lyons. Neural controlled differential equations for irregular
time series. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances
in Neural Information Processing Systems, volume 33, pages 6696–6707. Curran Associates, Inc.,
2020.
D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd
International Conference on Learning Representations (ICLR), 2015.
11

F. J. Király and H. Oberhauser. Kernels for sequentially ordered data. Journal of Machine Learning
Research, 20:1–45, 2019.
Klaus Greff, Aaron Klein, Martin Chovanec, Frank Hutter, and Jürgen Schmidhuber. The Sacred
Infrastructure for Computational Research. In Katy Huff, David Lippa, Dillon Niederhut, and
M. Pacer, editors, Proceedings of the 16th Python in Science Conference, pages 49 – 56, 2017.
C.-Y. Ko, Z. Lyu, L. Weng, L. Daniel, N. Wong, and D. Lin. POPQORN: Quantifying robustness of
recurrent neural networks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th
International Conference on Machine Learning, volume 97, pages 3468–3477. PMLR, 2019.
D. Levin, T. Lyons, and H. Ni. Learning from the past, predicting the statistics for the future, learning
an evolving system. arXiv:1309.0260, 2013.
S. Liao, T. Lyons, W. Yang, and H. Ni. Learning stochastic differential equations using RNN with
log signature features. arXiv:1908.08286, 2019.
S. H. Lim. Understanding recurrent neural networks using nonequilibrium response theory. Journal
of Machine Learning Research, 22:1–48, 2021.
T. Lyons. Rough paths, signatures and the modelling of functions on streams. arXiv:1405.4537,
2014.
T. J. Lyons, M. J. Caruana, and T. Lévy. Differential Equations Driven by Rough Paths, volume 1908
of Lecture Notes in Mathematics. Springer, Berlin, 2007.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant
to adversarial attacks. In International Conference on Learning Representations, 2018.
T. Mikolov, M. Karaﬁát, L. Burget, J. ˇCernock`y, and S. Khudanpur. Recurrent neural network
based language model. In Proceedings of the 11th Annual Conference of the International Speech
Communication Association, volume 2, pages 1045–1048, 2010.
A. A. Minai and R. D. Williams. On the derivatives of the sigmoid. Neural Networks, 6:845–853,
1993.
J. Morrill, C. Salvi, P. Kidger, J. Foster, and T. Lyons. Neural rough differential equations for long
time series. arXiv:2009.08295, 2020a.
J. H. Morrill, A. Kormilitzin, A. J. Nevado-Holgado, S. Swaminathan, S. D. Howison, and T. J.
Lyons. Utilization of the signature method to identify the early onset of sepsis from multivariate
physiological time series in critical care monitoring. Critical Care Medicine, 48:e976–e981, 2020b.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages
8024–8035. Curran Associates, Inc., 2019.
I. Perez Arribas. Derivatives pricing using signature payoffs. arXiv:1809.09466, 2018.
J. F. Reizenstein and B. Graham. Algorithm 1004: The iisignature library: Efﬁcient calculation of
iterated-integral signatures and log signatures. ACM Transactions on Mathematical Software, 46:
article 8, 2020.
J. Riordan. An Introduction to Combinatorial Analysis. John Wiley & Sons, New York, 1958.
Y. Rubanova, R. T. Q. Chen, and D. K. Duvenaud. Latent ordinary differential equations for
irregularly-sampled time series. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32,
pages 5320–5330. Curran Associates, Inc., 2019.
B. Schölkopf and A. J. Smola. Learning with kernels: support vector machines, regularization,
optimization, and beyond. MIT press, Cambridge, Massachusetts, 2002.
12

C. Toth and H. Oberhauser. Bayesian learning from sequential data using Gaussian processes with
signature covariances. In H. Daumé III and A. Singh, editors, Proceedings of the 37th International
Conference on Machine Learning, volume 119, pages 9548–9560, 2020.
Z. Tu, F. He, and D. Tao. Understanding generalization in recurrent neural networks. In International
Conference on Learning Representations, 2019.
P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski,
P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman,
N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat, Y. Feng, E. W.
Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R.
Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors.
SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:
261–272, 2020.
B. Wang, M. Liakata, H. Ni, T. Lyons, A. J. Nevado-Holgado, and K. Saunders. A path signature
approach for speech emotion recognition. In Proceedings of Interspeech 2019, pages 1661–1665,
2019.
W. Yang, L. Jin, and M. Liu. DeepWriterID: An end-to-end online text-independent writer identiﬁca-
tion system. IEEE Intelligent Systems, 31:45–53, 2016.
W. Yang, T. Lyons, H. Ni, C. Schmid, and L. Jin. Developing the path signature methodology and its
application to landmark-based human action recognition. arXiv:1707.03993, 2017.
B. Yue, J. Fu, and J. Liang. Residual recurrent neural networks for learning sequential representations.
Information, 9:56, 2018.
J. Zhang, Q. Lei, and I. Dhillon. Stabilizing gradients for deep neural networks via efﬁcient SVD
parameterization. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference
on Machine Learning, volume 80, pages 5806–5814. PMLR, 2018.
13

A
Mathematical details
A.1
Writing the GRU and LSTM in the neural ODE framework
GRU.
Recall that the equations of a GRU take the following form: for any 1 ≤j ≤T,
rj+1 = σ(Wrxj+1 + br + Urhj)
zj+1 = σ(Wzxj+1 + bz + Uzhj)
nj+1 = tanh
 Wnxj+1 + bn + rj+1 ∗(Unhj + cn)

hj+1 = (1 −zj+1) ∗hj + zj+1 ∗nj+1,
where σ is the logistic activation, tanh the hyperbolic tangent, ∗the Hadamard product, rj the reset
gate vector, zj the update gate vector, Wr, Ur, Wz, Uz, Wn, Un weight matrices, and br, bz, bn, cn
biases. Since rj+1, zj+1, and nj+1 depend only on xj+1 and hj, it is clear that these equations can
be rewritten in the form
hj+1 = hj + f(hj, xj+1).
We then obtain equation (1) by normalizing f by 1/T.
LSTM.
The LSTM networks are deﬁned, for any 1 ≤j ≤T, by
ij+1 = σ(Wixj+1 + bi + Uihj)
fj+1 = σ(Wfxj+1 + bf + Ufhj)
gj+1 = tanh(Wgxj+1 + bg + Ughj)
oj+1 = σ(Woxj+1 + bo + Uohj)
cj+1 = fj+1 ∗cj + ij+1 ∗gj+1
hj+1 = oj+1 ∗tanh(cj+1),
where σ is the logistic activation, tanh the hyperbolic tangent, ∗the Hadamard product, ij the input
gate, fj the forget gate, gj the cell gate, oj the output gate, cj the cell state, Wi, Ui, Wf, Uf, Wg, Ug
Wo, Uo weight matrices, and bi, bf, bg, bo biases. Since ij+1, fj+1, gj+1, oj+1 depend only on xj+1
and hj, these equations can be rewritten in the form
hj+1 = f1(hj, xj+1, cj+1)
cj+1 = f2(hj, xj+1, cj).
Let ˜hj = (h⊤
j , c⊤
j )⊤be the hidden state deﬁned by stacking the hidden and cell state. Then, clearly,
˜h follows an equation of the form
˜hj+1 = f(˜hj, xj+1).
We obtain (1) by subtracting ˜hj and normalizing by 1/T.
A.2
Picard-Lindelöf theorem
Consider a CDE of the form (8). We recall the Picard-Lindelöf theorem as given by Lyons et al.
(2007, Theorem 1.3), and provide a proof for the sake of completeness.
Theorem 4 (Picard-Lindelöf theorem). Assume that X ∈BV ([0, 1], Rd) and that F is Lipschitz-
continuous with constant KF. Then, for any H0 ∈Re, the differential equation (8) admits a unique
solution H : [0, 1] →Re.
Proof. Let C ([s, t]), Re) be the set of continuous functions from [s, t] to Re. For any [s, t] ⊂[0, 1],
ζ ∈Re, let Ψ be the function
Ψ : C ([s, t]), Re) →C ([s, t], Re)
Y 7→
 v 7→ζ +
Z v
s
F(Yu)dXu

.
14

For any Y, Y ′ ∈C ([s, t]), Re), v ∈[s, t],
∥Ψ(Y )v −Ψ(Y ′)v∥≤
Z v
s
 F(Yu) −F(Y ′
u)

dXu

≤
Z v
s
∥F(Yu) −F(Y ′
u)∥op∥dXu∥
≤
Z v
s
KF∥Yu −Y ′
u∥∥dXu∥
≤KF∥Y −Y ′∥∞
Z v
s
∥dXu∥
≤KF∥Y −Y ′∥∞∥X∥T V ;[s,t].
This shows that the function Ψ is Lipschitz-continuous on C ([s, t]), Re) endowed with the supremum
norm, with Lipschitz constant KF∥X∥T V ;[s,t]. Clearly, the function t 7→∥X∥T V ;[0,t] is non-
decreasing and uniformly continuous on the compact interval [0, 1]. Therefore, for any ε > 0, there
exists δ > 0 such that
|t −s| < δ ⇒
∥X∥T V ;[0,t] −∥X∥T V ;[0,s]
 < ε.
Take ε = 1/KF. Then on any interval [s, t] of length smaller than δ, one has ∥X∥T V ;[s,t] =
∥X∥T V ;[0,t] −∥X∥T V ;[0,s] < 1/KF, so that the function Ψ is a contraction. By the Banach ﬁxed-
point theorem, for any initial value ζ, Ψ has a unique ﬁxed point. Hence, there exists a solution to (8)
on any interval of length δ with any initial condition. To obtain a solution on [0, 1] it is sufﬁcient to
concatenate these solutions.
A corollary of this theorem is a Picard-Lindelöf theorem for initial value problems of the form
dHt = f(Ht, Xt)dt,
H0 = ζ,
(15)
where f : Re × Rd →Re, ζ ∈Re.
Corollary 1. Assume that f is Lipschitz continuous in its ﬁrst variable. Then, for any ζ ∈Re, the
initial value problem (15) admits a unique solution.
Proof. Let fX : (h, t) 7→f(h, Xt). Then the solution of (15) is solution of the differential equation
dHt = fX(Ht, t)dt.
Let d = 1, ¯e = e + 1, and F be the vector ﬁeld deﬁned by
F : h 7→

fX(h1:e, he+1)
1

,
where h1:e denotes the projection of h on its ﬁrst e coordinates. Then, since fX is Lipschitz, so is the
vector ﬁeld F. Theorem 4 therefore applies to the differential equation
dHt = F(Ht)dt,
H0 = (ζ⊤, 0)⊤.
Projecting this differential equation on the last coordinate gives dHe+1
t
= dt, that is, He+1
t
= t.
Projecting on the ﬁrst e coordinates exactly provides equation (15), which therefore has a unique
solution, equal to H1:e.
A.3
Operator norm
Deﬁnition 3. Let (E, ∥· ∥E) and (F, ∥· ∥F ) be two normed vector spaces and let f ∈L (E, F),
where L (E, F) is the space of linear functions from E to F. The operator norm of f is deﬁned by
∥f∥op =
sup
u∈E,∥u∥E=1
∥f(u)∥F .
Equipped with this norm, L (E, F) is a normed vector space.
This deﬁnition is valid when f is represented by a matrix.
15

A.4
Tensor Hilbert space
Let us ﬁrst brieﬂy recall some elements on tensor spaces. If e1, . . . , ed is the canonical basis of Rd,
then (ei1 ⊗· · · ⊗eik)1≤i1,...,ik≤d is a basis of (Rd)⊗k. Any element a ∈(Rd)⊗k can therefore be
written as
a =
X
1≤i1,...,ik≤d
a(i1,...,ik)ei1 ⊗· · · ⊗eik,
where a(i1,...,ik) ∈R. The tensor space (Rd)⊗k is a Hilbert space of dimension dk, with scalar
product
⟨a, b⟩(Rd)⊗k =
X
1≤i1,...,ik≤d
a(i1,...,ik)b(i1,...,ik)
and associated norm ∥· ∥(Rd)⊗k.
We now consider the space T deﬁned by (6). The sum, multiplication by a scalar, and scalar product
on T are deﬁned as follows: for any a = (a0, . . . , ak, . . . ) ∈T , b = (b0, . . . , bk, . . . ) ∈T ,
λ ∈R,
a + λb = (a0 + λb0, . . . , ak + λbk, . . . )
and
⟨a, b⟩T =
∞
X
k=0
⟨ak, bk⟩(Rd)⊗k,
with the convention (Rd)⊗0 = R.
Proposition 6. (T , +, ·, ⟨·, ·⟩T ) is a Hilbert space.
Proof. By the Cauchy-Schwartz inequality, ⟨·, ·⟩T is well-deﬁned: for any a, b ∈T ,
|⟨a, b⟩T | ≤
∞
X
k=0
|⟨ak, bk⟩(Rd)⊗k| ≤
∞
X
k=0
∥ak∥(Rd)⊗k∥bk∥(Rd)⊗k
≤
 ∞
X
k=0
∥ak∥2
(Rd)⊗k
1/2 ∞
X
k=0
∥bk∥2
(Rd)⊗k
1/2
< ∞.
Moreover, T is a vector space: for any a, b ∈T , λ ∈R, since
a + λb = (a0 + λb0, . . . , ak + λbk, . . . ),
and
∞
X
k=0
∥ak + λbk∥2
(Rd)⊗k =
∞
X
k=0
∥ak∥2
(Rd)⊗k + λ2
∞
X
k=0
∥bk∥2
(Rd)⊗k
+ 2λ
∞
X
k=0
⟨ak, bk⟩(Rd)⊗k
≤
∞
X
k=0
∥ak∥2
(Rd)⊗k + λ2
∞
X
k=0
∥bk∥2
(Rd)⊗k + 2λ⟨a, b⟩T < ∞,
we see that a + λb ∈T . The operation ⟨·, ·⟩T is also bilinear, symmetric, and positive deﬁnite:
⟨a, a⟩T = 0 ⇔
∞
X
k=0
∥ak∥2
(Rd)⊗k = 0 ⇔∀k ∈N, ∥ak∥2
(Rd)⊗k = 0 ⇔∀k ∈N, ak = 0 ⇔a = 0.
Therefore ⟨·, ·⟩T is an inner product on T . Finally, let (a(n))n∈N be a Cauchy sequence in T . Then,
for any n, m ≥0,
∥a(n) −a(m)∥2
T =
∞
X
k=0
∥a(n)
k
−a(m)
k
∥2
(Rd)⊗k,
so for any k ∈N, the sequence (a(n)
k )n∈N is Cauchy in (Rd)⊗k. Since (Rd)⊗k is a Hilbert space,
(a(n)
k )n∈N converges to a limit a(∞)
k
∈(Rd)⊗k. Let a(∞) = (a(∞)
0
, . . . , a(∞)
k
, . . . ). To ﬁnish the
16

proof, we need to show that a(∞) ∈T and that a(n) converges to a(∞) in T . First, note that there
exists a constant B > 0 such that for any n ∈N,
∥a(n)∥T ≤B.
To see this, observe that for ε > 0, there exists N ∈N such that for any n ≥N, ∥a(n) −a(N)∥T < ε,
and so ∥a(n)∥T ≤ε + ∥a(N)∥T . Take B = max(∥a(1)∥T , . . . , ∥a(N)∥T , ε + ∥a(N)∥T ). Then, for
any K ∈N,
K
X
k=0
∥a(n)
k ∥2
(Rd)⊗k ≤∥a(n)∥T ≤B.
Letting K →∞, we obtain that ∥a(∞)∥T ≤B, and therefore a(∞) ∈T . Finally, let ε > 0 and let
N ∈N be such that for any n, m ≥N, ∥a(n) −a(m)∥T < ε. Clearly, for any K ∈N,
K
X
k=0
∥a(n)
k
−a(m)
k
∥2
(Rd)⊗k < ε2.
Letting m →∞leads to
K
X
k=1
∥a(n)
k
−a(∞)
k
∥2
(Rd)⊗k < ε2,
and letting K →∞gives
∥a(n) −a(∞)∥T < ε,
which completes the proof.
A.5
Bounding the derivatives of the logistic and hyperbolic tangent activations
Lemma 1. Let σ be the logistic function deﬁned, for any x ∈R, by σ(x) = 1/(1+e−x). Then, for any
n ≥0,
∥σ(n)∥∞≤2n−1n! .
Proof. For any x ∈R, one has (Minai and Williams, 1993, Theorem 2)
σ(n)(x) =
n+1
X
k=1
(−1)k−1(k −1)!
n + 1
k

σ(x)k,
where
n
k
	
stands for the Stirling number of the second kind (see, e.g., Riordan, 1958). Let
un =
n+1
X
k=1
(k −1)!
n + 1
k

for n ≥1 and u0 = 1. Since 0 ≤σ(x) ≤1, it is clear that |σ(n)(x)| ≤un. Using the fact that the
Stirling numbers satisfy the recurrence relation
n + 1
k

= k
n
k

+

n
k −1

,
valid for all 0 ≤k ≤n, we have
un =
n
X
k=1
(k −1)!

k
n
k

+

n
k −1

+ n! =
n
X
k=1
k!
n
k

+
n−1
X
k=0
k!
n
k

+ n! = 2
n
X
k=1
k!
n
k

(since
n
0
	
= 0)
≤2n
n
X
k=1
(k −1)!
n
k

= 2nun−1.
Thus, by induction, un ≤2n−1n!, from which the claim follows.
17

Lemma 2. Let tanh be the hyperbolic tangent function. Then, for any n ≥0,
∥tanh(n)∥∞≤4nn! .
Proof. Let σ be the logistic function. Straightforward calculations yield the equality, valid for any
x ∈R,
tanh(x) = 2σ(2x) −1.
But, for any n ≥1,
tanh(n)(x) = 2n+1σ(n)(2x),
and thus, by Lemma 1,
∥tanh(n)∥∞≤2n+1∥σ(n)∥∞≤4nn! .
The inequality is also true for n = 0 since ∥tanh∥∞≤1.
A.6
Chen’s formula
First, note that it is straightforward to extend the deﬁnition of the signature to any interval [s, t] ⊂
[0, 1]. The next proposition, known as Chen’s formula (Lyons et al., 2007, Theorem 2.9), tells us that
the signature can be computed iteratively as tensor products of signatures on subintervals.
Proposition 7. Let X ∈BV ([s, t], Rd) and u ∈(s, t). Then
S[s,t](X) = S[s,u](X) ⊗S[u,t](X).
Next, it is clear that the signature of a constant path is equal to 1 = (1, 0, . . . , 0, . . . ) which is the
null element in T . Indeed, let Y ∈BV ([s, t], Rd) be a constant path. Then, for any k ≥1,
Yk
[s,t] = k!
Z
· · ·
Z
s≤u1<···<uk≤t
dYu1 ⊗· · · ⊗dYuk = k!
Z
· · ·
Z
s≤u1<···<uk≤t
0 ⊗· · · ⊗0 = 0.
Now let X ∈BV ([0, 1], Rd) and consider the path ˜X[j] equal to the time-augmented path ¯X on
[0, j/T] and then constant on [j/T, 1]—see Figure 4. We have by Proposition 7
S[0,1]( ˜X[j]) = S[0,j/T]( ˜X[j]) ⊗S[j/T,1]( ˜X[j]) = S[0,j/T]( ¯X) ⊗1 = S[0,j/T]( ¯X).
0.0
0.2
0.4
0.6
0.8
1.0
Time t
0.2
0.3
0.4
0.5
0.6
0.7
0.8
X
0.0
0.2
0.4
0.6
0.8
1.0
Time t
0.2
0.3
0.4
0.5
0.6
0.7
0.8
X
˜X[1]
˜X[2]
˜X[j]
Figure 4: Example of a path X ∈BV ([0, 1], R) (left) and its corresponding paths ˜X[j], plotted
against time, for different values of j ∈{1, . . . , T} (right)
B
Proofs
B.1
Proof of Proposition 1
According to Assumption (A1), for any h1, h2 ∈Re, x1, x2 ∈Rd, one has
∥f(h1, x1) −f(h2, x1)∥≤Kf∥h1 −h2∥
and
∥f(h1, x1) −f(h1, x2)∥≤Kf∥x1 −x2∥.
18

Under assumption (A1), by Corollary 1, the initial value problem (3) admits a unique solution H.
Let us ﬁrst show that for any t ∈[0, 1], Ht is bounded independently of X. For any t ∈[0, 1],
∥Ht −H0∥=

Z t
0
f(Hu, Xu)du
 ≤
Z t
0
∥f(Hu, Xu)∥du
=
Z t
0
∥f(Hu, Xu) −f(H0, Xu) + f(H0, Xu)∥du
≤
Z t
0
∥f(Hu, Xu) −f(H0, Xu)∥+
Z t
0
∥f(H0, Xu)∥du
≤Kf
Z t
0
∥Hu −H0∥du + t sup
∥x∥≤L
∥f(H0, x)∥.
Applying Grönwall’s inequality to the function t 7→∥Ht −H0∥yields
∥Ht −H0∥≤t sup
∥x∥≤L
∥f(H0, x)∥exp
 Z t
0
Kfdu

≤sup
∥x∥≤L
∥f(H0, x)∥eKf := M.
Given that H0 = h0 = 0, we conclude that ∥Ht∥≤M.
Next, let
∥f∥∞=
sup
∥x∥≤L,∥h∥≤M
f(h, x).
By similar arguments, for any [s, t] ⊂[0, 1], Grönwall’s inequality applied to the function t 7→
∥Ht −Hs∥yields
∥Ht −Hs∥≤(t −s)∥f∥∞eKf .
Therefore, for any partition (t0, . . . , tk) of [s, t],
k
X
i=1
∥Hti −Hti−1∥≤∥f∥∞eKf
k
X
i=1
(ti −ti−1) ≤∥f∥∞eKf (t −s),
and, taking the supremum over all partitions of [s, t], ∥H∥T V ;[s,t] ≤∥f∥∞eKf (t −s). In other
words, H is of bounded variation on any interval [s, t] ⊂[0, 1]. Let (t0, . . . , tT ) denote the regular
partition of [0, 1] with tj = j/T. For any 1 ≤j ≤T, we have
∥Htj −hj∥=
Htj−1 +
Z tj
tj−1
f(Hu, Xu)du −hj−1 −1
T f(hj−1, xj)

≤∥Htj−1 −hj−1∥+
Z tj
tj−1
f(Hu, Xu) −f(hj−1, xj)
du.
Writing
f(Hu, Xu) −f(hj−1, xj)
 =
f(Hu, Xu) −f(Hu, xj) + f(Hu, xj) −f(hj−1, xj)

≤
f(Hu, Xu) −f(Hu, xj)
 +
f(Hu, xj) −f(hj−1, xj)

≤Kf
Xu −xj
 + Kf
Hu −hj−1
,
we obtain
∥Htj −hj∥≤∥Htj−1 −hj−1∥+ Kf
Z tj
tj−1
∥Hu −hj−1∥du + Kf
Z tj
tj−1
∥Xu −xj∥du
≤∥Htj−1 −hj−1∥+ Kf
Z tj
tj−1
 ∥Hu −Htj−1∥+ ∥Htj−1 −hj−1∥

du
+ Kf
T ∥X∥T V ;[tj−1,tj]
≤
 1 + Kf
T

∥Htj−1 −hj−1∥+ Kf
T
 ∥H∥T V ;[tj−1,tj] + ∥X∥T V ;[tj−1,tj]

.
19

By induction, we are led to
∥Htj −hj∥≤Kf
T
j−1
X
k=0

1 + Kf
T
k ∥H∥T V ;[tk,tk+1] + ∥X∥T V ;[tk,tk+1]

≤Kf
T

1 + Kf
T
T  ∥X∥T V ;[0,1] + ∥H∥T V ;[0,1]

≤KfeKf
T
 L + ∥f∥∞eKf 
,
which concludes the proof.
B.2
Proof of Proposition 2
Let ¯h ∈R¯e and let ¯hi:j = (¯hi, . . . , ¯hj) be its projection on a subset of coordinates. It is sufﬁcient to
take F deﬁned by
F(¯h) =

0e×d
2
1−Lf(¯h1:e, ¯he+1:e+d)
Id×d
0d×1

,
where Id×d denotes the identity matrix and 0·×· the matrix full of zeros. The function ¯H is then
solution of
d ¯Ht =

0e×d
2
1−Lf( ¯H1:e
t
, ¯He+1:e+d
t
)
Id×d
0d×1
  dXt
1−L
2 dt

.
Note that under assumption (A1), the tensor ﬁeld F satisﬁes the assumptions of the Picard-Lindelöf
theorem (Theorem 4) so that ¯H is well-deﬁned. The projection of this equation on the last d
coordinates gives
d ¯He+1:e+d
t
= dXt,
¯He+1:e+d
0
= X0,
and therefore ¯He+1:e+d
t
= Xt. The projection on the ﬁrst e coordinates gives
d ¯H1:e
t
=
2
1 −Lf( ¯H1:e
t
, Xt)1 −L
2
dt = f( ¯H1:e
t
, Xt)dt,
¯H1:e
0
= h0,
which is exactly (3).
B.3
Proof of Proposition 3
According to Lyons (2014, Lemma 5.1), one has
∥¯Xk
[0,t]∥(Rd)⊗k ≤∥¯X∥k
T V ;[0,t].
Let (t0, . . . , tk) be a partition of [0, t]. Then
k
X
j=1
∥¯Xtj −¯Xtj−1∥=
k
X
j=1
r
∥Xtj −Xtj−1∥2 +
1 −L
2
2
(tj −tj−1)2
≤
k
X
j=1
∥Xtj −Xtj−1∥+ 1 −L
2
k
X
j=1
(tj −tj−1)
=
k
X
j=1
∥Xtj −Xtj−1∥+ 1 −L
2
t.
Taking the supremum over any partition of [0, t] we obtain
∥¯X∥T V ;[0,t] ≤∥X∥T V ;[0,t] + 1 −L
2
t ≤L + 1 −L
2
= 1 + L
2
< 1,
and thus ∥¯Xk
[0,t]∥(Rd)⊗k ≤

1+L
2
k
. It is then clear that
∥S[0,t]( ¯X)∥T =
 ∞
X
k=0
∥¯Xk
[0,t]∥2
(Rd)⊗k
1/2
≤
∞
X
k=0
∥¯Xk
[0,t]∥(Rd)⊗k ≤
∞
X
k=0
1 + L
2
k
= 2(1 −L)−1.
20

B.4
Proof of Proposition 4
We ﬁrst recall the fundamental theorem of calculus for line integrals (also known as gradient theorem).
Theorem 5. Let g : Re →R be a continuously differentiable function, and let γ : [a, b] →Re be a
smooth curve in Re. Then
Z b
a
∇g(γt)dγt = g(γb) −g(γa),
where ∇g denotes the gradient of g.
The identity above immediately generalizes to a function g : Re →Re:
Z b
a
J(g)(γt)dγt = g(γb) −g(γa),
where J(g) ∈Re×e is the Jacobian matrix of g. Let us apply Theorem 5 to the vector ﬁeld F i
between 0 and t, with γ = H. We have
F i(Ht) −F i(H0) =
Z t
0
J(F i)(Hu)dHu =
Z t
0
J(F i)(Hu)
d
X
j=1
F j(Hu)dXu
=
d
X
j=1
Z t
0
J(F i)(Hu)F j(Hu)dXu =
d
X
j=1
Z t
0
F j ⋆F i(Hu)dXu.
Iterating this procedure (N −1) times for the vector ﬁelds F 1, . . . , F d yields
Ht = H0 +
d
X
i=1
Z t
0
F i(Hu)dXi
u
= H0 +
d
X
i=1
Z t
0
F i(H0)dXi
u +
d
X
i=1
Z t
0
d
X
j=1
Z u
0
F j ⋆F i(Hv)dXj
vdXi
u
= H0 +
d
X
i=1
F i(H0)S(i)(X)[0,t] +
X
1≤i,j≤d
Z
0≤v≤u≤t
F j ⋆F i(Hv)dXj
vdXi
u
= · · ·
= H0 +
N
X
k=1
X
1≤i1,...,ik≤d
F i1 ⋆· · · ⋆F ik(H0) 1
k!S(i1,...,ik)
[0,t]
(X)
+
X
1≤i1,...,iN+1≤d
Z
∆N+1;[0,t]
F i1 ⋆· · · ⋆F iN+1(Hu1)dXi1
u1 · · · dXiN+1
uN+1,
where ∆N;[0,t] := {(u1, · · · , uN) ∈[0, t]N | 0 ≤u1 < · · · < uN ≤t} is the simplex in [0, t]N. The
ﬁrst (N + 1) terms equal HN
t . Hence,
∥Ht −HN
t ∥
=

X
1≤i1,...,iN+1≤d
Z
∆N+1;[0,t]
F i1 ⋆· · · ⋆F iN+1(Hu1)dXi1
u1 · · · dXiN+1
uN+1

≤
X
1≤i1,...,iN+1≤d
Z
∆N+1;[0,t]
∥F i1 ⋆· · · ⋆F iN+1(Hu1)∥|dXi1
u1| · · · |dXiN+1
uN+1|
≤
X
1≤i1,...,iN+1≤d
Z
∆N+1;[0,t]
sup
1≤i1,...,iN+1≤d,∥h∥≤M
∥F i1 ⋆· · · ⋆F iN+1(h)∥|dXi1
u1| · · · |dXiN+1
uN+1|
≤ΛN+1(F)
X
1≤i1,...,iN+1≤d
Z
∆N+1;[0,t]
|dXi1
u1| · · · |dXiN+1
uN+1|.
21

Thus,
∥Ht −HN
t ∥≤ΛN+1(F)
X
1≤i1,...,iN+1≤d
Z
∆N+1;[0,t]
|dXi1
u1| · · · |dXiN+1
uN+1|
≤ΛN+1(F)
X
1≤i1,...,iN+1≤d
Z
∆N+1;[0,t]
∥dXu1∥· · · ∥dXuN+1∥
= ΛN+1(F)
dN+1
(N + 1)!
Z
[0,t]N+1 ∥dXu1∥· · · ∥dXuN+1∥
= ΛN+1(F)
dN+1
(N + 1)!
 Z t
0
∥dXu∥
N+1
= ΛN+1(F)
dN+1
(N + 1)!∥X∥N+1
T V ;[0,t] ≤ΛN+1(F)
dN+1
(N + 1)!.
B.5
Proof of Proposition 5
For simplicity of notation, since the context is clear, we now use the notation ∥·∥instead of ∥·∥(Re)⊗k.
According to Proposition 1, the solution ¯H of (4) veriﬁes ∥¯Ht∥≤M + L := ¯
M. We therefore place
ourselves in the ball B ¯
M. Recall that for any 1 ≤i1, . . . , iN ≤d, ¯h ∈B ¯
M,
F i1 ⋆· · · ⋆F iN (¯h) = J(F i2 ⋆· · · ⋆F iN )(¯h)F i1(¯h).
(16)
Linear case.
We start with the proof of the linear case before moving on to the general case.
When σ is chosen to be the identity function, each F i
RNN is an afﬁne vector ﬁeld, in the sense that
F i
RNN(¯h) = Wi¯h + bi, where Wi = 0¯e×¯e, bi is the i + dth vector of the canonical basis of Re+d, and
Wd+1 =

2
1−LW
0d×¯e

and
bd+1 =

2
1−Lb
0d

.
Since J(F i
RNN) = Wi, we have, for any ¯h ∈Re+d and any 1 ≤i1, . . . , ik ≤d,
F i1
RNN ⋆· · · ⋆F ik
RNN(¯h) = Wik · · · Wi2(Wi1¯h + bi1).
Thus, for any ¯h ∈B ¯
M,
∥F i1
RNN ⋆· · · ⋆F ik
RNN(¯h)∥≤∥Wik∥op · · · ∥Wi2∥op(∥Wi1∥op ¯
M + ∥bi1∥).
For i ̸= d + 1, ∥Wi1∥op = 0, and so
Λk(FRNN) ≤C∥Wd+1∥k−1
op
,
with C = ∥Wd+1∥op ¯
M + max(1, 2(1 −L)−1∥b∥). Therefore,
∞
X
k=1
dk
k! Λk(FRNN) ≤Cd
∞
X
k=0
1
k!
 2d(1 −L)−1∥W∥op
k−1 < ∞.
General case.
In the general case, the proof is two-fold. First, we upper bound (16) by a function
of the norms of higher-order Jacobians of F i1, . . . , F iN . We then apply this bound to the speciﬁc
case F = FRNN. We refer to Appendix C for details on higher-order derivatives in tensor spaces. Let
F : Re →Re be a smooth vector ﬁeld. If F(h) = (F1(h), . . . , Fe(h))⊤, each of its coordinates Fi
is a function from Re to R, C ∞with respect to all its input variables. We deﬁne the derivative of
order k of F as the tensor ﬁeld
Jk(F) : Re →(Re)⊗k+1
h 7→Jk(F)(h),
where
Jk(F)(h) =
X
1≤j,i1,...,ik≤e
∂kFj(h)
∂hi1 . . . ∂hik
ej ⊗ei1 ⊗· · · ⊗eik.
We take the convention J0(F) = F, and note that J(F) = J1(F) is the Jacobian matrix, and that
Jk(Jk′(F)) = Jk+k′(F).
22

Lemma 3. Let A1, . . . , Ak : Re →Re be smooth vector ﬁelds. Then, for any h ∈Re
Ak ⋆· · · ⋆A1(h)
 ≤
X
n1+···+nk=k−1
C(k; n1, . . . , nk)∥Jn1(A1)(h)∥· · · ∥Jnk(Ak)(h)∥,
where C(k; n1, . . . , nk) is deﬁned by the following recurrence on k: C(1; 0) = 1 and for any
n1, . . . , nk+1 ≥0,
C(k + 1; n1, . . . , nk+1) =
k
X
ℓ=1
C(k; n1, . . . , nℓ−1, . . . , nk)
if
nk+1 = 0,
(17)
C(k + 1; n1, . . . , nk+1) = 0
otherwise.
Proof. We refer to Appendix C for the deﬁnitions of the tensor dot product ⊙and tensor permutations,
as well as for computation rules involving these operations. We show in fact by induction a stronger
result, namely that there exist tensor permutations πp such that
Ak ⋆· · ·⋆A1(h) =
X
n1+···+nk=k−1
X
1≤p≤C(k;n1,...,nk)
πp

Jn1(A1)(h) ⊙· · · ⊙Jnk(Ak)(h)

. (18)
Note that we do not make explicit the permutations nor the axes of the tensor dot operations since we
are only interested in bounding the norm of the iterated star products. Also, for simplicity, we denote
all permutations by π, even though they may change from line to line.
We proceed by induction on k. For k = 1, the formula is clear. Assume that the formula is true at
order k. Then
J(Ak ⋆· · · ⋆A1)
=
X
n1+···+nk=k−1
X
1≤p≤C(k;n1,...,nk)
J
h
πp[ Jn1(A1) ⊙· · · ⊙Jnk(Ak) ]
i
=
X
n1+···+nk=k−1
X
1≤p≤C(k;n1,...,nk)
πp
h
J[ Jn1(A1) ⊙· · · ⊙Jnk(Ak) ]
i
=
X
n1+···+nk=k−1
X
1≤p≤C(k;n1,...,nk)
k
X
ℓ=1
πp ◦πℓ
h
Jn1(A1) ⊙
· · · ⊙Jnℓ+1(Aℓ) ⊙· · · ⊙Jnk(Ak)
i
.
In the inner sum, we introduce the change of variable pi = ni for i ̸= ℓand pℓ= nℓ+ 1. This yields
J(Ak ⋆· · · ⋆A1)
=
X
p1+···+pk=k
k
X
ℓ=1
X
1≤p≤C(k;p1,...,pℓ−1,...,pk)
πp ◦πℓ
h
Jn1(A1) ⊙
· · · ⊙Jnℓ+1(Aℓ) ⊙· · · ⊙Jnk(Ak)
i
=
X
p1+···+pk+1=k
X
1≤q≤C(k+1;p1,...,pk+1)
πq
h
Jn1(A1) ⊙· · · ⊙Jpk(Ak)
i
,
where in the last sum the only non-zero term is for pk+1 = 0. To conclude the induction, it remains
to note that
Ak+1 ⋆· · · ⋆A1 = J(Ak ⋆· · · ⋆A1) ⊙Ak+1 = J(Ak ⋆· · · ⋆A1) ⊙J0(Ak+1).
Hence,
Ak+1 ⋆· · · ⋆A1
=
X
p1+···+pk+1=k
X
1≤q≤C(k+1;p1,...,pk+1)
πq

Jn1(A1) ⊙· · · ⊙Jpk(Ak)

⊙Jpk+1(Ak+1)
=
X
p1+···+pk+1=k
X
1≤q≤C(k+1;p1,...,pk+1)
πq

Jn1(A1) ⊙· · · ⊙Jpk(Ak) ⊙Jpk+1(Ak+1)

.
The result is then a consequence of (18) and of Lemma 6.
23

We now restrict ourselves to the case F = FRNN as deﬁned by (5) and give an upper bound on the
higher-order derivatives of the tensor ﬁelds F i1, . . . , F iN .
Lemma 4. For any i ∈{1, . . . , d + 1}, ¯h ∈B ¯
M, for any k ≥0,
∥Jk(F i
RNN)(¯h)∥≤

2
1 −L∥W∥F
k
∥σ(k)∥∞.
Proof. For any 1 ≤i ≤d, F i
RNN(¯h) is constant, so Jk(F 1
RNN) = · · · = Jk(F d
RNN) = 0. For i = d+1,
we have, for any 1 ≤j ≤e,
∂kF d+1
RNN,j(¯h)
∂¯hi1 . . . ∂¯hik
=

2
1 −L
k
Wji1 · · · Wjikσ(k)(Wj·¯h + b),
where Wj· denotes the jth row of W and for e + 1 ≤j ≤¯e, F d+1
j
= 0. Therefore,
∥Jk(F d+1
RNN)(¯h)∥2 ≤

2
1 −L
2k
X
1≤j,i1,...,ik≤e
|Wji1 · · · Wjikσ(k)(Wj·¯h + b)|2
=

2
1 −L
2k
∥σ(k)∥2
∞
X
j
  X
i
|Wji|2k
≤

2
1 −L
2k
∥σ(k)∥2
∞∥W∥2k
F .
We are now in a position to conclude the proof using condition (10). By Lemma 3 and 4, for any
1 ≤i1, . . . , iN ≤d + 1,
F i1
RNN ⋆· · · ⋆F iN
RNN(¯h)

≤
X
n1+···+nN=N−1
C(N; nN, . . . , n1)∥JnN (F iN
RNN)(¯h)∥· · · ∥Jn1(F i1
RNN)(¯h)∥
≤

2
1 −L∥W∥F
N−1
X
n1+···+nN=N−1
C(N; nN, . . . , n1)an1+1n1! · · · anN+1nN!
≤a

2
1 −La2∥W∥F
N−1
X
n1+···+nN=N−1
C(N; nN, . . . , n1)n1! · · · nN! .
Assume for the moment that C(N; nN, . . . , n1) is smaller than the multinomial coefﬁcient
 N
nN,...,n1

.
Then, using the fact that there are
 n+k−1
k−1

weak compositions of n in k parts and Stirling’s approxi-
mation, we have
ΛN(F) ≤a

2
1 −La2∥W∥F
N−1
N! × Card
 {n1 + · · · + nN = N −1}

≤a

2
1 −La2∥W∥F
N−1
N!
2N −2
N −1

≤a
2

2
1 −La2∥W∥F
N−1
N!
2N
N

≤a
√
2e
π

8
1 −La2∥W∥F
N−1 N!
√
N
.
Hence, provided ∥W∥F < (1−L)/8a2d,
∞
X
k=1
dk
k! Λk(F) ≤ad
√
2e
π
∞
X
k=1
8da2∥W∥F
1 −L
k−1 1
√
k
< ∞,
and (A2) is veriﬁed.
To conclude the proof, it remains to prove the following lemma.
24

Lemma 5. For any k ≥1 and n1, . . . , nk ≥0, C(k; n1, . . . , nk) ≤
 k−1
n1,...,nk

.
Proof. The proof is done by induction, by comparing the recurrence formula (17) with the following
recurrence formula for multinomial coefﬁcients:

k
n1, . . . , nk+1

=
k+1
X
ℓ=1

k −1
n1, . . . , nℓ−1, . . . , nk+1

.
More precisely, for k = 1, C(1; 0) = 1 ≤
 0
0

= 1 and C(1; 1) = 0 ≤
 0
1

= 0. Assume
that the formula is true at order k. Then, at order k + 1, there are two cases. If nk+1 ̸= 0,
C(k + 1; n1, . . . , nk+1) = 0, and the result is clear. On the other hand, if nk+1 = 0,
C(k + 1; n1, . . . , nk, 0) =
k
X
ℓ=1
C(k; n1, . . . , nℓ−1, . . . , nk)
≤
k
X
ℓ=1

k −1
n1, . . . , nℓ−1, . . . , nk

≤
k+1
X
ℓ=1

k −1
n1, . . . , nℓ−1, . . . , nk+1

≤

k
n1, . . . , nk+1

.
B.6
Proof of Theorem 1
First, Propositions 1 and 2 state that if ¯H is the solution of (4) and Proj denotes the projection on the
ﬁrst e coordinates, then
zT −ψ
 Proj( ¯H1)
 =
ψ(hT ) −ψ
 Proj( ¯H1])
 ≤∥ψ∥op
hT −Proj( ¯H1)
 ≤∥ψ∥op
c1
T .
For any 1 ≤k ≤N, we let Dk( ¯H0) : (Rd)⊗k →Re be the linear function deﬁned by
Dk( ¯H0)(ei1 ⊗· · · ⊗eik) = F i1 ⋆· · · ⋆F ik( ¯H0),
(19)
where e1, . . . , ed denotes the canonical basis of R ¯d. Then, under assumptions (A1) and (A2), if ¯Xk
denotes the signature of order k of the path ¯Xt = (X⊤
t , 1−L
2 t)⊤, according to Propositions 4 and 5,
¯H1 = ¯H0 +
∞
X
k=1
1
k!
X
1≤i1,...,ik≤d
S(i1,...,ik)
[0,t]
(X)F i1 ⋆· · · ⋆F ik( ¯H0) =
∞
X
k=1
1
k!Dk( ¯H0)(Xk
[0,t]),
and
ψ ◦Proj( ¯H1) = ψ ◦Proj
 ∞
X
k=0
1
k!Dk( ¯H0)(¯Xk)

=
∞
X
k=0
1
k!ψ ◦Proj
 Dk( ¯H0)(¯Xk)

,
by linearity of ψ and Proj. Since the maps Dk( ¯H0) : (Rd)⊗k →Re are linear, the above equality
takes the form
ψ ◦Proj( ¯H1) =
∞
X
k=0
⟨αk, ¯Xk⟩(Rd)⊗k,
(20)
25

where αk ∈(Rd)⊗k is the coefﬁcient of the linear map 1
k!ψ ◦Proj ◦Dk( ¯H0) in the canonical basis.
Let α = (α0, . . . , αk, . . . ). Under assumption (A2),
∞
X
k=0
∥αk∥2
(Rd)⊗k ≤
∞
X
k=0
X
1≤i1,...,ik≤d
 1
k!
2
∥ψ∥2
op∥F i1 ⋆· · · ⋆F ik( ¯H0)∥2
≤∥ψ∥2
op
∞
X
k=0
X
1≤i1,...,ik≤d
 1
k!
2
Λk(F)2
≤∥ψ∥2
op
∞
X
k=0
dk
k! Λk(F)
2
< ∞.
This shows that α ∈T , and therefore, using (20), we conclude
∥zT −⟨α, S( ¯X)⟩T ∥≤∥ψ∥op
c1
T .
B.7
Proof of Theorem 2
Let
G =
n
gθ : (Rd)T →R | gθ(x) = zT , θ ∈Θ
o
be the function class of (discrete) RNN and
S =
n
ξαθ : X →R | ξαθ(X) = ⟨αθ, S( ¯X)⟩T , θ ∈Θ
o
,
be the class of their RKHS embeddings, where αθ is deﬁned by (20). For any θ ∈Θ, we let
RG (θ) = E[ℓ(y, gθ(x))],
and
RS (θ) = E[ℓ(y, ξαθ( ¯X))],
and denote by b
Rn,G and b
Rn,S the corresponding empirical risks. We also let θ∗
G , θ∗
S , bθn,G , and
bθn,S be the corresponding minimizers. We have
P
 y ̸= gbθn,G (x)

−b
Rn,G (bθn,G ) ≤E

ℓ(y, gbθn,G (x))

−b
Rn,G (bθn,G )
= RG (bθn,G ) −b
Rn,G (bθn,G )
= RG (bθn,G ) −RS (bθn,G ) + RS (bθn,G ) −b
Rn,S (bθn,G )
+ b
Rn,S (bθn,G ) −b
Rn,G (bθn,G )
≤sup
θ
|RG (θ) −RS (θ)| + sup
θ
|RS (θ) −b
Rn,S (θ)|
+ sup
θ
| b
Rn,G (θ) −b
Rn,S (θ)|.
Using Theorem 1, we have
sup
θ
|RG (θ) −RS (θ)| = sup
θ
E

ℓ(y, gθ(x)) −ℓ(y, ξαθ( ¯X))

≤sup
θ
E

|φ(ygθ(x)) −φ(yξαθ( ¯X))|

≤sup
θ
E

Kℓ|y| × |gθ(x) −ξαθ( ¯X)|

≤Kℓsup
θ
(∥ψ∥opc1,θ) 1
T := c2
2T ,
where c1,θ = KfθeKfθ  L + ∥fθ∥∞eKfθ 
(the inﬁnite norm ∥fθ∥∞is taken on the balls BL and
BM). One proves with similar arguments that
sup
θ
| b
Rn,G (θ) −b
Rn,S (θ)| ≤c2
2T .
26

Under the assumption of the theorem, there exists a ball B ⊂H of radius B such that S ⊂B.
This yields
sup
θ
|RS (θ) −b
Rn,S (θ)| ≤
sup
α∈T ,∥α∥T ≤B
|RB(α) −b
Rn,B(α)|,
where
RB(α) = E[ℓ(Y, ξα( ¯X))]
and
b
Rn,B(α) = 1
n
n
X
i=1
ℓ(Y (i), ξα( ¯X)).
We now have reached a familiar situation where the supremum is over a ball in a RKHS. It is known
(see, e.g., Bartlett and Mendelson, 2002, Theorem 8) that with probability at least 1 −δ,
sup
α∈T ,∥α∥T ≤B
|RB(α) −b
Rn,B(α)| ≤4KℓERadn(B) + 2BKℓ(1 −L)−1
r
log(1/δ)
2n
,
where Radn(B) denotes the Rademacher complexity of B. Observe that we have used the fact that
the loss is bounded by KℓB(1 −L)−1 since, for any ξα ∈B, by the Cauchy-Schwartz inequality,
ℓ(y, ξα( ¯X)) = φ(y⟨α, S( ¯X)⟩T ) ≤Kℓ|y⟨α, S( ¯X)⟩T | ≤Kℓ∥α∥T ∥S( ¯X)∥T
≤2KℓB(1 −L)−1.
Finally, the proof follows by noting that Rademacher complexity of B is bounded by
Radn(B) ≤2B
n
v
u
u
t
n
X
i=1
K(X(i), X(i)) = 2B
n
v
u
u
t
n
X
i=1
∥S( ¯X(i))∥2
T ≤4B(1 −L)−1
√n
.
B.8
Proof of Theorem 3
Let
G =
n
gθ : (Rd)T →(Rp)T | gθ(x) =
 z1, . . . , zT

, θ ∈Θ
o
be the function class of discrete RNN in a sequential setting. Let
S =
n
Γθ : X →(Rp)T | Γθ(X) =
 Ξθ( ˜X[1]), . . . , Ξθ( ˜X[T ])
o
,
be the class of their RKHS embeddings, where ˜X[j] is the path equal to X on [0, j/T] and then
constant on [j/T, 1] (see Figure 4). For any X ∈X ,
Ξθ(a) =



⟨α1,θ, S( ¯X)⟩T
...
⟨αp,θ, S( ¯X)⟩T


=



ξα1,θ(X)
...
ξαp,θ(X)


∈Rp,
where (α1,θ, . . . , αp,θ)⊤∈(T )p are the coefﬁcients of the linear maps
1
k!ψ ◦Proj ◦Dk( ¯H0) :
(Rd)⊗k →Rp, k ≥0, in the canonical basis, where Dk is deﬁned by (19).
We start the proof as in Theorem 2, until we obtain
RG (bθn,G ) −b
Rn,G (bθn,G ) ≤sup
θ
|RG (θ) −RS (θ)| + sup
θ
|RS (θ) −b
Rn,S (θ)|
+ sup
θ
| b
Rn,G (θ) −b
Rn,S (θ)|.
27

By deﬁnition of the loss, for any θ ∈Θ,
|RG (θ) −RS (θ)| =
E

ℓ
 y, gθ(x)

−ℓ
 y, Γθ(X)

≤E
h 1
T
T
X
j=1
 ∥yj −zj∥2 −∥yj −Ξθ( ˜X[j])∥2
i
≤E
h 1
T
T
X
j=1

zj + Ξθ( ˜X[j]) −2yj, zj −Ξθ( ˜X[j])

i
≤E
h 1
T
T
X
j=1
∥zj + Ξθ( ˜X[j]) −2yj∥× ∥zj −Ξθ( ˜X[j])∥
i
(by the Cauchy-Schwartz inequality).
According to inequality (13), one has
∥zj −Ξθ( ˜X[j])∥≤∥ψ∥op
c1,θ
T ,
where c1,θ = KfθeKfθ  L + ∥fθ∥∞eKfθ 
. Moreover,
Ξθ( ˜X[j])
2 =
p
X
ℓ=1
⟨αℓ,θ, S( ˜X[j])⟩T
2 ≤
p
X
ℓ=1
∥αℓ,θ∥2
T ∥S( ˜X[j])∥2
T ≤pB2 2(1 −L)−12,
since ∥S( ˜X[j])∥T = ∥S[0,j/T]( ¯X)∥T ≤∥S( ¯X)∥T . This yields
∥zj + Ξθ( ˜X[j]) −2yj∥≤∥zj∥+ ∥Ξθ( ˜X[j])∥+ 2∥yj∥
≤∥ψ∥op∥fθ∥∞+ 2√pB(1 −L)−1 + 2Ky.
Finally,
sup
θ
|RG (θ) −RS (θ)| ≤c3
2T ,
where c3 = sup
θ
 c1,θ +∥ψ∥op∥fθ∥∞

+2√pB(1−L)−1 +2Ky. One proves with similar arguments
that
sup
θ
| b
Rn,G (θ) −b
Rn,S (θ)| ≤c3
2T .
We now turn to the term sup
θ
|RS (θ) −b
Rn,S (θ)|. We have
RS (θ) −b
Rn,S (θ)
= E[ℓ(y, Γθ(X))] −1
n
n
X
i=1
ℓ(y(i), Γθ(X(i)))
= 1
T
T
X
j=1

E[∥yj −Ξθ( ˜X[j])]∥2 −1
n
n
X
i=1
y(i)
j
−Ξθ( ˜X(i)
[j] )
2
.
Therefore,
sup
θ
|RS (θ) −b
Rn,S (θ)| ≤1
T
T
X
j=1
sup
θ
E[∥yj −Ξθ( ˜X[j])]∥2 −1
n
n
X
i=1
y(i)
j
−Ξθ( ˜X(i)
[j] )
2.
Note that for a ﬁxed j, the pairs ( ˜X(i)
[j] , y(i)
j ) are i.i.d. Under the assumptions of the theorem, there
exists a ball B ⊂H such that for any 1 ≤ℓ≤p, θ ∈Θ, ξαℓ,θ ∈B . We denote by Bp the sum of
p such spaces, that is,
Bp =

fα : X →Rp | fα(X) = (fα1(X), . . . , fαp(X))⊤, fαℓ∈B
	
.
28

Clearly, Ξθ ∈Bp, and it follows that
sup
θ
E[∥yj −Ξθ( ˜X[j])]∥2 −1
n
n
X
i=1
y(i)
j
−Ξθ( ˜X(i)
[j] )
2
≤
sup
fα∈Bp
E

∥yj −fα( ˜X[j])∥2
−1
n
n
X
i=1
∥y(i)
j
−fα( ˜X(i)
[j] )∥2.
We have once again reached a familiar situation, which can be dealt with by an easy extension of
Bartlett and Mendelson (2002, Theorem 12). For any fα ∈Bp, let ˜φ ◦fα : X × Rp : (X, y) 7→
∥y −fα(X)∥2 −∥y∥2. Then, ˜φ ◦fα is upper bounded by
|˜φ ◦fα(X, y)| =
∥y −fα(X)∥2 −∥y∥2 ≤∥fα(X)∥
 ∥fα(X)∥+ 2∥y∥

≤2√pB(1 −L)−1(2√pB(1 −L)−1 + 2Ky)
≤4pB(1 −L)−1(B(1 −L)−1 + Ky).
Let c4 = B(1 −L)−1 + Ky and c5 = 4pB(1 −L)−1c4 + K2
y. Then with probability at least 1 −δ,
sup
fα∈Bp
E

∥yj −fα( ˜X[j])∥

−1
n
n
X
i=1
∥y(i)
j
−fα( ˜X(i)
[j] )∥
 ≤Radn(˜φ ◦Bp) +
r
2c5 log(1/δ)
n
,
where ˜φ ◦Bp =

(X, y) 7→˜φ ◦fα(X, y)|fα ∈Bp
	
. Elementary computations on Rademacher
complexities yield
Radn(˜φ ◦Bp) ≤2pc4Radn(B) ≤8pc4B(1 −L)−1
√n
,
which concludes the proof.
C
Differentiation with higher-order tensors
C.1
Deﬁnition
We deﬁne the generalization of matrix product between square tensors of order k and ℓ.
Deﬁnition 4. Let a ∈(Re)⊗k, b ∈(Re)⊗ℓ, p ∈{1, . . . , k}, q ∈{1, . . . , ℓ}. Then the tensor dot
product along (p, q), denoted by a ⊙p,q b ∈(Re)⊗(k+ℓ−2), is deﬁned by
(a ⊙p,q b)(i1,...,ik−1,j1,...,jℓ−1) =
e
X
j=1
a(i1,...,ip−1,j,ip,...,ik−1)b(j1,...,jq−1,j,jq,...,jℓ−1).
This operation just consists in computing a ⊗b, and then summing the pth coordinate of a with the
qth coordinate of b. The ⊙operator is not associative. To simplify notation, we take the convention
that it is evaluated from left to right, that is, we write a ⊙b ⊙c for (a ⊙b) ⊙c.
Deﬁnition 5. Let a ∈(Re)⊗k. For a given permutation π of {1, . . . , k}, we denote by π(a) the
permuted tensor in (Re)⊗k such that
π(a)(i1,...,ik) = a(iΠ(1),...,iΠ(k)).
Example 5. If A is a matrix, then AT = π(A), with π deﬁned by π(1) = 2, π(2) = 1.
C.2
Computation rules
We need to obtain two computation rules for the tensor dot product: bounding the norm (Lemma 6)
and differentiating (Lemma 7).
Lemma 6. Let a ∈(Re)⊗k, b ∈(Re)⊗ℓ. Then, for all p, q,
∥a ⊙p,q b∥(Re)⊗k+ℓ−2d ≤∥a∥(Re)⊗k∥b∥(Re)⊗ℓ.
29

Proof. By the Cauchy-Schwartz inequality,
∥a ⊙p,q b∥2
(Re)⊗k+ℓ−2
=
X
1≤i1,...,ik−1,j1,...,jℓ−1≤e
(a ⊙p,q b)2
(i1,...,ik−1,j1,...,jℓ−1)
=
X
1≤i1,...,ik−1,j1,...,jℓ−1≤e
 X
1≤j≤e
a(i1,...,ip−1,j,ip,...,ik−1)b(j1,...,jq−1,j,jq,...,jℓ−1)
2
≤
X
i1,...,ik−1,j1,...,jℓ−1
 X
j
a2
(i1,...,ip−1,j,ip,...,ik−1)
 X
j
b2
(j1,...,jq−1,j,jq,...,jℓ−1)

≤
X
i1,...,ik−1,j
a2
(i1,...,ip−1,j,ip,...,ik−1)
X
j1,...,jℓ−1,j
b2
(j1,...,jq−1,j,jq,...,jℓ−1)
≤∥a∥2
(Re)⊗k∥b∥2
(Re)⊗ℓ.
Lemma 7. Let A : Re →(Re)⊗k, B : Re →(Re)⊗ℓbe smooth vector ﬁelds, p ∈{1, . . . , k},
q ∈{1, . . . , ℓ}. Let A ⊙p,q B : Re →(Re)⊗k+ℓ−2 be deﬁned by A ⊙p,q B(h) = A(h) ⊙p,q B(h).
Then there exists a permutation π such that
J(A ⊙p,q B) = π(J(A) ⊙p,q B) + A ⊙p,q J(B).
Proof. The left-hand side takes the form
(J(A ⊙p,q B))i1,...,ik−1,j1,...,jℓ−1,m =
X
j
h ∂A
∂hm (i1,...,ip−1,j,ip,...,ik−1)
B(j1,...,jq−1,j,jq,...,jℓ−1)
+ A(i1,...,ip−1,j,ip,...,ik−1)
∂B
∂hm (j1,...,jq−1,j,jq,...,jℓ−1)
i
.
The ﬁrst term of the right-hand side writes
(J(A) ⊙p,q B)i1,...,ik−1,m,j1,...,jℓ−1 =
X
j
h ∂A
∂hm (i1,...,ip−1,j,ip,...,ik−1)
B(j1,...,jq−1,j,jq,...,jℓ−1)
i
,
and the second one
(A ⊙p,q J(B))i1,...,ik−1,j1,...,jℓ−1,m =
X
j
h
A(i1,...,ip−1,j,ip,...,ik−1)
∂B
∂hm (j1,...,jq−1,j,jq,...,jℓ−1)
i
.
Let us introduce the permutation π which keeps the ﬁrst (k −1) axes unmoved, and rotates the
remaining ℓones such that the last axis ends up in kth position. Then
π(J(A) ⊙p,q B)i1,...,ik−1,j1,...,jℓ−1,m =
X
j
h ∂A
∂hm (i1,...,ip−1,j,ip,...,ik−1)
B(j1,...,jq−1,j,jq,...,jℓ−1)
i
.
Hence J(A ⊙p,q B) = π(J(A) ⊙p,q B) + A ⊙p,q J(B), which concludes the proof.
The following two lemmas show how to compose the Jacobian and the tensor dot operations with
permutations. Their proofs follow elementary operations and are therefore omitted.
Lemma 8. Let A : Re →(Re)⊗k and π a permutation of {1, . . . , k}. Then there exists a permutation
˜π of {1, . . . , k + 1} such that
J(π(A)) = ˜π(J(A)).
Lemma 9. Let a ∈(Re)⊗k, b ∈(Re)⊗ℓ, p ∈{1, . . . , k}, q ∈{1, . . . , ℓ}, π a permutation of
{1, . . . , k}. Then there exists ˜p ∈{1, . . . , k}, ˜q ∈{1, . . . , ℓ}, and a permutation ˜π of {1, . . . , k +
ℓ−2} such that
π(a) ⊙p,q b = ˜π(a ⊙˜p,˜q b).
30

The following result is a generalization of Lemma 7 to the case of a dot product of several tensors.
Lemma 10. For ℓ∈{1, . . . , k}, nℓ∈N, let Aℓ: Re →(Re)⊗nℓbe smooth tensor ﬁelds. For
any (pℓ)1≤ℓ≤k−1 and (qℓ)1≤ℓ≤k−1 such that pℓ∈{1, . . . , nℓ}, qℓ∈{1, . . . , nℓ+1}, there exist k
permutations (πℓ)1≤ℓ≤k such that
J(A1 ⊙p1,q1 A2 ⊙p2,q2 · · · ⊙pk−1,qk−1 Ak) =
k
X
ℓ=1
πℓ[A1 ⊙A2 ⊙· · · ⊙J(Aℓ) ⊙· · · ⊙Ak] ,
where the dot products of the right-hand side are along some axes that are not specify for simplicity.
Proof. The proof is done by induction on k. The formula for k = 1 is straightforward. Assume that
the formula is true at order k. As before, we do not specify indexes for tensor dot products as we are
only interested in their existence. By Lemma 9, we have
J(A1 ⊙· · · ⊙Ak+1)
= J((A1 ⊙· · · ⊙Ak) ⊙Ak+1)
= π(J(A1 ⊙· · · ⊙Ak) ⊙Ak+1) + A1 ⊙· · · ⊙Ak ⊙J(Ak+1)
= π
" k
X
ℓ=1
πℓ[A1 ⊙A2 ⊙· · · ⊙J(Aℓ) ⊙· · · ⊙Ak] ⊙Ak+1
#
+ A1 ⊙· · · ⊙Ak ⊙J(Ak+1)
= π
" k
X
ℓ=1
˜πℓ[A1 ⊙A2 ⊙· · · ⊙J(Aℓ) ⊙· · · ⊙Ak ⊙Ak+1]
#
+ A1 ⊙· · · ⊙Ak ⊙J(Ak+1)
=
k
X
ℓ=1
ˆπℓ[A1 ⊙A2 ⊙· · · ⊙J(Aℓ) ⊙· · · ⊙Ak ⊙Ak+1] + A1 ⊙· · · ⊙Ak ⊙J(Ak+1)
(where ˆπ = π ◦˜π)
=
k+1
X
ℓ=1
ˆπℓ[A1 ⊙A2 ⊙· · · ⊙J(Aℓ) ⊙· · · ⊙Ak ⊙Ak+1] .
D
Experimental details
All the code to reproduce the experiments is available on GitHub at https://github.com/
afermanian/rnn-kernel.
Our experiments are based on the PyTorch (Paszke et al., 2019)
framework. When not speciﬁed, the default parameters of PyTorch are used.
Convergence of the Taylor expansion.
For Figure 1, 103 random RNN with 2 hidden units are
generated, with the default weight initialization. The activation is either the logistic or the hyperbolic
tangent. In Figure 1b, only the results with the logistic activation are plotted. The process X is
taken as a 2-dimensional spiral. The reference solution to the ODE (3) is computed with a numerical
integration method from SciPy (Virtanen et al., 2020, scipy.integrate.solve_ivp with the
‘LSODA’ method). The signature in the step-N Taylor expansion is computed with the package
Signatory (Kidger and Lyons, 2021).
The step-N Taylor expansion requires computing higher-order derivatives of tensor ﬁelds (up to
order N). This is a highly non-trivial task since standard deep learning frameworks are optimized
for ﬁrst-order differentiation only. We refer to, for example, Kelly et al. (2020), for a discussion on
higher-order differentiation in the context of a deep learning framework. To compute it efﬁciently, we
manually implement forward-mode higher-order automatic differentiation for the operations needed
in our context (described in Appendix C). A more efﬁcient and general approach is left for future
work. Our code is optimized for GPU.
Penalization on a toy example.
For Figure 2, the RNN is taken with 32 hidden units and hyperbolic
tangent activation. The data are 50 examples of spirals, sampled at 100 points and labeled ±1
31

according to their rotation direction. We do not use batching and the loss is taken as the cross entropy.
It is trained for 200 epochs with Adam (Kingma and Ba, 2015) with an initial learning rate of 0.1.
The learning rate is divided by 2 every 40 epochs. For the penalized RNN, the RKHS norm is
truncated at N = 3 and the regularization parameter is selected at λ = 0.1. Earlier experiments
show that this order of magnitude is sensible. We do not perform hyperparameter optimization since
our goal is not to achieve high performance. The initial hidden state h0 is learned (for simplicity
of presentation, our theoretical results were written with h0 = 0 but they extend to this case). The
accuracy is computed on a test set of size 1000. We generate adversarial examples using 50 steps of
projected gradient descent (following Bietti et al., 2019). The whole methodology (data generation +
training) is repeated 20 times. The average training time on a Tesla V100 GPU for the RNN is 8.5
seconds and for the penalized RNN 12 seconds.
Figure 3 is obtained by selecting randomly one run among the 20 of Figure 2.
Libraries.
We use PyTorch (Paszke et al., 2019) as our overall framework, Signatory (Kidger and
Lyons, 2021) to compute the signatures, and SciPy (Virtanen et al., 2020) for ODE integration. We
use Sacred (Klaus Greff et al., 2017) for experiment management. The links and licences for the
assets are given in the following table:
Name
Homepage link
License
PyTorch
GitHub repository
BSD-style License
Sacred
GitHub repository
MIT License
SciPy
GitHub repository
BSD 3-Clause "New" or "Revised" License
Signatory
GitHub repository
Apache License 2.0
32

