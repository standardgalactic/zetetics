1
Beyond Euclid:
An Illustrated Guide to Modern Machine Learning
with Geometric, Topological, and Algebraic Structures
Sophia Sanborn∗1 Johan Mathe∗2 Mathilde Papillon∗1 Domas Buracas3 Hansen J Lillemark3
Christian Shewmake3,5 Abby Bertics1 Xavier Pennec4 Nina Miolane1,2,3
1University of California, Santa Barbara
2Atmo, Inc.
3New Theory AI
4Universit´e Cˆote d’Azur & Inria
5University of California, Berkeley
Abstract—The enduring legacy of Euclidean geometry
underpins classical machine learning, which, for decades,
has been primarily developed for data lying in Euclidean
space. Yet, modern machine learning increasingly en-
counters richly structured data that is inherently non-
Euclidean. This data can exhibit intricate geometric,
topological and algebraic structure: from the geometry
of the curvature of space-time, to topologically complex
interactions between neurons in the brain, to the alge-
braic transformations describing symmetries of physical
systems. Extracting knowledge from such non-Euclidean
data necessitates a broader mathematical perspective.
Echoing the 19th-century revolutions that gave rise to
non-Euclidean geometry, an emerging line of research is
redefining modern machine learning with non-Euclidean
structures. Its goal: generalizing classical methods to
unconventional data types with geometry, topology, and
algebra. In this review, we provide an accessible gateway
to this fast-growing field and propose a graphical tax-
onomy that integrates recent advances into an intuitive
unified framework. We subsequently extract insights into
current challenges and highlight exciting opportunities
for future development in this field.
Index
Terms—Geometric
Deep
Learning,
Geometry,
Topology, Algebra, Machine Learning
I. INTRODUCTION
For nearly two millennia, Euclid’s Elements of Ge-
ometry formed the backbone of our understanding
of space and shape. This ‘Euclidean’ view of ge-
ometry—characterized by flat planes and straight
lines—remained unquestioned until the 19th century.
Only then, did mathematicians venture “beyond” to
develop the principles of non-Euclidean geometry on
curved spaces. Their pioneering work revealed that
there is no singular, true geometry. Instead, Euclidean
geometry is but one in a mathematical universe of
geometries, each of which can be used to illuminate
different structures in nature—from the mechanics of
celestial bodies embracing the curvature of spacetime
to the topologically and algebraically complex electri-
cal patterns of neurons in natural and artificial neural
networks.
This non-Euclidean revolution was part of a greater
trend towards generalization and abstraction in 19th
and 20th century mathematics. In addition to expand-
ing the realm of geometry, mathematicians proceeded
to define more abstract notions of space, freed from
rigid geometric concepts like distances and angles.
This gave rise to the field of topology, which examines
the properties of a space that are preserved under con-
tinuous transformations such as stretching and bending.
By abstracting away from the rigidity of geometric
structures, topology emphasizes more general spatial
properties such as continuity and connectedness. In-
deed, two structures that look very different from a
geometric perspective may be considered topologically
equivalent. The famous of example of this is a donut
and coffee mug, which are topologically equivalent
since one can be continuously deformed into the other.
This notion of abstract equivalence was supported by
the simultaneous development of the field of abstract
algebra, which examines the symmetries of an ob-
ject—the transformations that leave its fundamental
structure unchanged. These mathematical ideas quickly
found applications in the natural sciences, and revolu-
tionized how we model the world.
A similar revolution is now unfolding in machine
learning. In the last two decades, a burgeoning body
of research has expanded the horizons of machine
learning, moving beyond the flat, Euclidean spaces
traditionally used in data analysis to embrace the
rich variety of structures offered by non-Euclidean
geometry, topology, and abstract algebra. This move-
ment includes the generalization of classical statistical
theory and machine learning in the field of Geometric
Statistics (Guigui et al., 2023, Pennec, 2006) as well as
deep learning models in the fields of Geometric, (Bron-
arXiv:2407.09468v1  [cs.LG]  12 Jul 2024

2
stein et al., 2021), Topological (Bodnar, 2023, Hajij
et al., 2023a), and Equivariant (Cohen et al., 2021)
Deep Learning. In the 20th century, non-Euclidean
geometry radically transformed how we model the
world with pen and paper. In the 21st century, it is
poised to revolutionize how we model the world with
machines.
This review article provides an accessible introduction
to the core concepts underlying this movement. We
organize the models in this body of literature into a co-
herent taxonomy defined by the mathematical structure
of both the data and the machine learning model. In
so doing, we clarify distinctions between approaches
and we highlight challenges and high-potential areas
of research that are as-of-yet unexplored. We begin
by introducing the essential mathematical background
in Section II, and turn to an analysis of mathematical
structure in data in Section III before introducing
our ontology of machine learning and deep learning
methods in Section IV and V. We explore the as-
sociated landscape of open-source software libraries
in Section VI, and delve into the movement’s key
application domains in Section VII. Accordingly, this
review article reveals how this new framework for ma-
chine learning—born from the elegant mathematics of
geometry, topology, and algebra—has been developed,
implemented, and adapted to propose transformative
solutions to real-world challenges.
Topological Properties
• Connectedness
• Continuity
→Relationships
Geometric Properties
• Distance
• Angle
→Measurements
Algebraic Properties
• Symmetry
• Invariance
→Transformations
II. ELEMENTS OF NON-EUCLIDEAN GEOMETRY
We first provide an accessible and concise introduction
to the essential mathematical concepts. For readability,
we define the concepts primarily linguistically here,
and refer the reader to the works Bronstein et al.
(2021), Cohen et al. (2021), Guigui et al. (2023),
Hajij et al. (2023a) for their precise mathematical
definitions.
Topology, geometry and algebra are branches of math-
ematics that study the properties of abstract spaces.
In machine learning, data can possess explicit spatial
structure—such as an image of a brain scan, or a
rendering of a protein surface. Even when data is not
overtly spatial, a dataset can be naturally conceptual-
ized as a set of samples drawn from an abstract surface
embedded in a high-dimensional space. Understanding
the “shape” of data—that is, the shape of the space to
which this data belongs—can give important insights
into the patterns of relationships that give data its
meaning.
Topology, geometry and algebra each provide a differ-
ent lens and set of tools for studying the properties
of data spaces and their “shapes,” summarized in
the textbox below. Topology lends the most abstract,
flexible perspective and considers spaces as stretchy
structures that can be continuously deformed so long
as connectivity and continuity are preserved. Topology
thus studies the relationships between points. Geom-
etry allows us to quantify familiar properties such as
distances and angles, in other words: perform measure-
ments on points. Algebra provides the tools to study
the symmetries of an object—the transformations that
can be applied while leaving its fundamental structure
invariant.
A. Topology: Relationships
A topological space is a set of points equipped with
a structure known as a topology that establishes which
points in the set are “close” to each other. The topology
gives spatial structure to an otherwise unstructured
set. Formally, a topology is defined as a collection of
open sets. An open set is a collection of points in a
spatial region that excludes points on the boundary.
By grouping points into open sets, we can discuss
‘neighborhoods’ of points and ‘paths’ between points,
giving us a way to formalize concepts like continuity
(can you travel from one location to another without
teleporting?) and connectedness (are two locations in
the same neighborhood or region?).
Given the generality of topological structures, topo-
logical spaces can be quite exotic. Within this paper,
we do not consider continuous topological spaces, and
we explicitly restrict the term topology to discrete
topological structures, such as graphs, cellular com-
plexes, and hypergraphs. We consider these spaces to
be a generalization of the discretized Euclidean space.
Indeed, Euclidean space discretizes into a regular grid,
while graphs, cellular complexes and hypergraphs relax
the regularity assumption and allow points to be con-

3
nected with more complex relationships as illustrated
in Figure
B. Geometry: Measurements
A manifold is a continuous space that locally “re-
sembles” (is homeomorphic to) Euclidean space in
the neighborhood of every point. In other words, it is
locally linear, and it does not intersect itself. Though
locally it resembles flat space, its global shape may
exhibit curvature. Without additional structure, the
manifold can be seen as a soft and elastic surface.
To give it more rigid, definite form, a manifold can
be equipped with a notion of distance between points,
that freezes how far each point is from each other,
and therefore freezes the overall shape of the mani-
fold.
There are two main approaches to defining a distance
on the manifold, which turn it into either (i) a metric
space, where an extrinsic distance may be chosen
independently of the manifold’s properties, or into (ii)
a length space (also called path-metric spaces), where
the distance between two points on the manifold is
defined intrinsically by the infimum of the length of
curves connecting the points. Within this paper, we
focus on length spaces. A powerful way of establishing
a notion of distance for this approach is to consider
a Riemannian manifold, that is: a smooth manifold
equipped with a positive definite inner product on the
tangent space at every point on the manifold. This fam-
ily of inner products defines the Riemannian metric,
which defines the lengths of tangent vectors at a given
point on the manifold and the angle between them.
Ultimately, the Riemannian metric induces a distance
along a curve of the manifold through integration of its
tangent vector. A geodesic generalizes the Euclidean
concept of a “straight line” to curved manifolds. A
Riemannian geodesic is a curve that traces the locally
shortest path between two points.
There exist different flavors of Riemannian manifolds,
such as the spheres, hyperbolic spaces, and tori. We
consider these spaces to be generalizations of the
continuous Euclidean space. Indeed, Euclidean spaces
are globally flat, while spheres, hyperbolic spaces and
tori can exhibit curvature —as illustrated in Figure 2.
It is no coincidence that the manifolds most com-
monly studied in the literature—the Euclidean space,
the hypersphere, and the hyperbolic space—are the
three manifolds of constant curvature. The Euclidean
space has curvature 0, the sphere (and hyperspheres)
have curvature 1, while the hyperbolic spaces have
curvature -1. The uniformity of their curvature makes
them simpler to model than more general, arbitrary
manifolds.
C. Algebra: Transformations
A group is a set of transformations, such as 3D rota-
tions, that can be composed. Groups can be discrete or
continuous. A Lie group is a continuous group, defined
as a smooth manifold equipped with a compatible
group structure such that the composition of elements
on the manifold obeys the group axioms. An example
of a Lie group is the set of special orthogonal matrices
in R3×3 under matrix multiplication, which defines the
group of 3-dimensional rotations, SO(3). Each matrix
is an element of the group and defines rotation at
a certain angle. Lie groups are extensively used in
physics where they describe symmetries in physical
systems.
A group of transformations, such as SO(3)—the group
of 3D rotations—may act on another manifold to trans-
form its elements. A group action maps an element in
a manifold to new location, determined by the group
element that transforms them. For example, a group
action on a Euclidean space can translate, rotate and
reflect its elements, as illustrated in Figure 3. In this
paper, we use the term algebra to denote that we equip
a space with a group action.
III. STRUCTURE IN DATA
The mathematics of topology, geometry and algebra
provide a conceptual framework to categorize the na-
ture of data found in machine learning. We generally
encounter two types of data: either data as coordinates
in space—for example the coordinate of the position
of an object in a 2D space; or data as signals over a
space—for example, an image is a 3D (RGB) signal
defined over a 2D space. In each case, the space can
either be a Euclidean space or it can be equipped with
topological, geometric and algebraic structures such as
the ones introduced in Section II.
Understanding the structure of this space provides
essential insights into the nature of the data. These
insights are, in turn, crucial for selecting the machine
learning model that will be most suitable to extract
knowledge from this data. In this section, we provide
a graphical taxonomy, shown in Figure 4, to categorize
the structures of data based on the mathematics of
topology, geometry and algebra.
A. Data as Coordinates in Space
We start by categorizing the mathematical structures
of data, when data points are coordinates in space—as

4
!"#$%&'()
*+(#'
,-(+.
,')'-($
/'$$"$(-
#01+$'2
34+'-5-(+.
6780+0$054
Fig. 1: Beyond Euclid: Discrete Topological Structures. Left: Euclidean space discretized into a regular grid. Right: Discrete topological
spaces that go beyond the classical discretized Euclidean space. Graphs, Cellular Complexes, Hypergraphs relax the assumption of the
regular grid and allow points to be connected with more complex relationships. The arrow +topology indicates the addition of a non-
Euclidean, discrete topological structure. Adapted from Papillon et al. (2023).
!"#$%&'()
*+(#'
,-.'/0'123
4+5'2'
63+'27/$%#
*+(#'
8')'2($
9/2"*
Fig. 2: Beyond Euclid: Continuous Geometric Structures. Left: Euclidean space. Right: Riemannian manifolds that go beyond the
classical Euclidean space. Spheres, hyperbolic spaces, and tori relax the assumption of flatness of the Euclidean space and can exhibit
positive or negative curvature. The arrow +geometry indicates the addition of a non-Euclidean, continuous geometric structure.
!"#$%&'()
*+(#'
,-()*$(.%/)
0/.(.%/)
,-()*$(.%/)12
0/.(.%/)
0'3'#.%/)
21($4'5-(
Fig. 3: Beyond Euclid: Algebraic Transformations. Left: Euclidean space. Right: Group transformations that act on the elements of
an Euclidean space: 2D translation from the group R2, 2D rotation from the group SO(2), 2D reflection from the group {1, −1}, and
a combination of translation and rotation from the Special Euclidean group SE(2). The arrow +algebra indicates the addition of the
non-Euclidean algebraic structure defining a group action.
shown in the top two rows of Figure 4, and described
in detail below. We denote with x a data point within
a dataset x1, ..xi, ..., xN, and drop its subscript i for
conciseness of notations.
Card C1: Point in Euclidean space: This category
of data points forms the basis of conven-
tional machine learning and deep learning
approaches. Card C1 (white) in Figure 4
shows an example of a point in Euclidean
space Rn, which represents the dimensions
of a flower from the Iris dataset (Fisher,
1936), where n represents the number of
features studied.
Card C2: Point in manifold: Adding a non-Euclidean
geometry to the coordinate space, card C2
(orange) illustrates a data point on a mani-
fold M, where the manifold is the sphere.
For example, this data point can represent
the geographic coordinates of a storm event,
i.e., its location on the surface of the earth
represented as a sphere.
Card C3: Point in topological space: Card C3 (pur-
ple) considers a data point that resides in
a topological space Ω, here corresponding
to a node in a graph. An example in this
category would be a data point representing
a person in the graph of a social network,
where edges represent social relationships.
The next three data categories add a group action to
the data spaces mentioned above.
Card C4: Point in Euclidean space equipped with
group action: Card C4 (blue) displays a
data point in a Euclidean space that has been
equipped with a group action. Group actions
enable us to model and preserve symmetries
in the data. For example, a group action on

5
the flower dimensions could be the change
of units, from centimeters to millimeters. In
this case, the group of interest is the group of
scalings R∗
+. The action of this group does
not change the information contained in the
data (the flower will still have the same size
in the real world), it only changes the way
we encode that information.
Card C5: Point in manifold equipped with group
action: Card C5 adds the notion of group
action to manifold. An example of such
a group action could be a change of co-
ordinate systems on the sphere, changing
the origin of longitudes. In this case, the
group of interest would be the group of 3D
rotations SO(3). Again, the action of this
group represents symmetries in the data: the
information content is unchanged (a storm
will still happen at the same geographical
location in the real world) but the way we
encode that information has changed.
Card C6: Point in topological space equipped with
group action: Card C6 equips a topologi-
cal space with the notion of group action.
An example of this is a graph of people
equipped with an action that can change the
way indexing is done on the dataset. The
order in which we index people does not
change their social relationships, only the
way we represent this data in the computer.
The group of interest in this case in the
group of permutations.
These categories describe the mathematical structure
of the data spaces encountered in machine learning.
However, even when data points belong to spaces with
topological, geometric, or algebraic structures, their
computational representations typically take the form
of arrays. These data points may thus appear as vectors
in a computer’s memory, but this is merely a con-
venience for processing and storage. The underlying
mathematical structure is preserved through constraints
imposed on the values of these vectors.
For example, a data point lying on a sphere might be
represented as a 3D coordinate vector, but constraints
on the norm of this vector encode the properties that
define the mathematical space. We note that a data
point can be represented by arrays of different sizes.
For example, the data point on the sphere can be
represented with vector of size 3 with values encoding
its Cartesian coordinates, or with a vector of size 2
with values representing its latitude and longitude. In
both cases, the mathematical nature of the data point
is unchanged: it still represents a point on a manifold,
independently of the array used to process it in the
computer.
B. Data as Signals
In many applications, data points are not coordinates
in space but rather functions defined over a space,
typically assigning a vector in Rm′ to every point in
the space Rm. Formally, we can write such a function
as x : Rm →Rm′ where the input space (here, Rm)
is called the domain and the output space (here, Rm′)
is called the codomain. We refer to data of this type
as a signal. Elements of the codomain are called the
values of the signal x.
Color images provide a clear example of this. The do-
main is a bounded region of R2: for example, limited to
the range [0, 1920] on the horizontal axis and [0, 1080]
on the vertical axis such that each point in the domain
represents a pixel location. Each pixel location is
assigned a vector in the codomain R3, which specifies
the intensity on each RGB color channel.
Figure 4 introduces several types of data points as
functions from a domain to a codomain, together with
real-world data examples. Formalizing a signal in this
way gives us the flexibility to represent more general
classes of signals using non-Euclidean structures. The
domain or the codomain (or both) may be any one of
the non-Euclidean spaces introduced in the previous
section. We present the different options in the four
bottom rows of Figure 4 and detail them below.
Card S1: Euclidean signal on Euclidean domain:
This represents one of the most common
type of data in classical deep learning, func-
tions as x : Rm →Rm′, going from points
in domain Rm to features in codomain Rm′.
A gray-scale image provides a clear example
of this. The domain is a bounded region of
R2 representing pixel locations. Each pixel
location is assigned a vector in codomain R,
which specifies the intensity in gray scale.
Card S2: Manifold-valued signal on Euclidean do-
main: This represents signals that can be
formalized as a function x : Rm 7→M ′,
which assigns an element of a manifold M ′
to every point in the Euclidean domain Rm.
An example of this occurs in diffusion tensor
imaging, in which 3D images of the brain
are taken. Attached to each voxel location

6
!"#"$"%$&''()*+"#,%$*+$-."/,
!"#"$"%$-*0+"1%
2$0,'3,#(4
2$"10,5("
2$"10,5("
2$"10,5("
2$0,'3,#(4
2$"10,5("
2$"10,5("
2$"10,5("
2$0,'3,#(4
2$"10,5("
2$"10,5("
2$"10,5("
2$#'.'1'04
2$0,'3,#(4
678*.$#'.'1'0*/"1
)'3"*+$"+)
68/1*),"+$
/')'3"*+
9*#:$0('8.$"/#*'+
;'#"#,$.'%,$"+)
<,1'/*#4$<,/#'(%
-==
;'#"#,$<,/#'($>,1)
678*.$68/1*),"+
)'3"*+$"+)
/')'3"*+$9*#:
0('8.$"/#*'+
-=?
678*.$3"+*@'1)
"+)$68/1*),"+$
/')'3"*+$9*#:
0('8.$"/#*'+
;'#"#,$<,/#'($>,1)
-=A
;'#"#,$#,3.,("#8(,%
2$#'.'1'04
2$0,'3,#(4
-B
678*.$#'.'1'0*/"1
)'3"*+$w*#:
0('8.$"/#*'+
&:"+0,$C'*+#$*+)*/,%
678*.$68/1*),"+
)'3"*+$9*#:
0('8.$"/#*'+
-D
;'#"#,$#:,$*3"0,
678*.$3"+*@'1)
9*#:$0('8.$"/#*'+
-E
2$#'.'1'04
2$0,'3,#(4
68/1*),"+$'+$
#'.'1'0*/"1
)'3"*+
-F
&''()*+"#,
.,($C'*+#
68/1*),"+$'+$
68/1*),"+
)'3"*+
-=
G("4%/"1,$*3"0,
68/1*),"+$'+
3"+*@'1)
-H
I,3.,("#8(,%
2$#'.'1'04
2$0,'3,#(4
-J
K"+*@'1)L<"18,)
'+$#'.'1'0*/"1
)'3"*+
;'#"#*'+$.,(
C'*+#
-A
!*M8%*'+$#,+%'(
*3"0,
K"+*@'1)L<"18,)
'+$68/1*),"+
)'3"*+
-N
O*($#("P/
/'<"(*"+/,
K"+*@'1)L<"18,)
'+$3"+*@'1)
2$#'.'1'04
2$0,'3,#(4
Q'*+#$*+$
#'.'1'0*/"1$%."/,
Q,(%'+$*+$
%'/*"1$+,#9'(R
&N
Q'*+#$*+$
68/1*),"+$%."/,
K,"%8(,3,+#%
'@$"$S'9,(
&=
Q'*+#$*+$3"+*@'1)
G,'/''()*+"#,%$
'@$"$%#'(3
&A
2$0,'3,#(4
2$#'.'1'04
678*.$#'.'1'0*/"1
%."/,$9*#:$
0('8.$"/#*'+
&:"+0,$'@$+'),$*+),T*+0
&H
678*.$68/1*),"+$
%."/,$9*#:$
0('8.$"/#*'+
&:"+0,$'@$3,"%8(,3,+#$
8+*#%
&J
678*.$3"+*@'1)
9*#:$0('8.$"/#*'+
&:"+0,$'@$%.:,(*/"1$
/''()*+"#,$%4%#,3
&F
Fig. 4: Geometric, Topological, and Algebraic Structures in Data. We categorize data into two types: data as coordinates in a mathematical
space (top 2 rows) or data as signals, i.e., as a function over a mathematical space with values in another space (bottom four rows). Each
card illustrates the structure of data and presents a real-world example. The arrows + topology, + geometry and + algebra between
cards indicate the addition of non-Euclidean topological, geometric and algebraic structures respectively. Notations: Rm: Euclidean space
of dimension m, M: Manifold, Ω: Topological space; x: data as point or as signal.

7
in domain R3 is a 3 × 3 covariance matrix
that describes how water molecules diffuse
in the brain—i.e., an element from the man-
ifold of symmetric positive definite matrices
M ′ = SPD(3) (Pennec et al., 2006).
Card S3: Manifold-valued signal on manifold do-
main: In this category, both the domain and
the codomain are manifolds M and M ′,
respectively, i.e., x : M 7→M ′. Air traffic
covariance data is a use case where the
domain is the sphere M = S2 (earth surface)
and the values are 2×S2 SPD matrices, i.e.,
elements of the manifold codomain M ′ =
SPD(2). These matrices can encode covari-
ance matrices representing representing dif-
ferent levels of local complexity (Le Brigant
and Puechmorel, 2018).
Card S4: Manifold-valued signal on topological do-
main: Here, the signal inputs live in a topo-
logical domain Ωand the signal values live
on a manifold M ′, such that x : Ω7→M ′.
A representative example for this setting is
human pose. Each joint in the body is repre-
sented by it 3D orientation, i.e., by a rotation
matrix on the special orthogonal group of
3D rotations M ′ = SO(3). The domain,
representing the indexing of the body joints,
can be a encoded as a non-directed graph Ω.
Card S5: Euclidean signal on topological domain:
This represents a signal with a topological
domain Ωand values in a Euclidean space
Rm′, that is: x : Ω7→Rm′. Reusing the
example of encoding the human pose, each
body joint can instead be represented by its
3D location in space, i.e., with a value in
Rm′ = R3 defined over the non directed
graph domain Ω.
Card S6: Euclidean signal on manifold domain:
This represents the signal inputs on a man-
ifold M and the signal values in Euclidean
space Rm′, i.e., x : M 7→Rm′. An example
of this is a datapoint representing a snapshot
of the earth at a given time showing the
distribution of temperatures across the globe:
the earth surface temperatures locations live
on the sphere manifold M = S2 and the
temperature themselves are in Rm′ = R
(image and dataset credits: Atmo, Inc.).
The next categories of data structure add a group action
in either the domain or the codomain (or both) of a
given signal x.
Card S7: Euclidean signal on topological domain
equipped with domain action: The signal
here is x : Ω7→Rm′, where Ωis equipped
with a group action. An example of appli-
cation here is the pose of the human body,
where the domain is a undirected graph
representing the body joints and the group
action on the domain is the group of permu-
tations. Permutations can re-index the order
of the body joints by changing the indexing
of the nodes of the graph representing the
joints.
Card S8: Euclidean signal on manifold domain
equipped with domain action: Here, the
signal is x : M 7→Rm′, where the manifold
domain M is equipped with a group action.
Using the earth surface temperature example
from Card S6, we can apply a rotation on the
domain by changing the origin of the spheri-
cal coordinates for instance. In that case, the
action group is the group of rotations SO(3)
acting on M = S2.
Card S9: Euclidean signal on Euclidean domain
equipped with domain and codomain ac-
tions: This represents signals such as x :
Rm →Rm′, where both domain Rm and
codomain Rm′ are equipped with group ac-
tions. The illustration in the cards shows a
vector field in a domain R2 with values as
vectors in R2. In this example, the actions
on both the domain and the codomain are
from the group of rotations SO(2), which
applies the same rotation to each point in
the domain and for each independent vector
in the vector field.
Card S10: Euclidean signal on topological domain
equipped with domain and codomain ac-
tions: Here, the function is x : Ω7→Rm′,
where both Ωand Rm′ are equipped with
group actions. The permutation group acts
on the domain by changing the joint indices
in the graph Ωand the 3D rotation group
SO(3) acts on the values representing ve-
locity vectors at each joint.
Card S11: Euclidean signal on manifold domain
equipped with domain and codomain ac-

8
tions: Here x : M 7→Rm′, where both
M and Rm′ are equipped with group ac-
tions. We present an example of a vector
field defined over the sphere S2 with vector
values in Rm′. In this example, applying
the action of a 3D rotation rotates both the
spherical coordinates of the vectors and their
actual vector directions. This can be useful
for changes of coordinates.
As in the previous subsection, we emphasize the dif-
ference between the mathematical representation of
data and the computational representation of data.
Until now, we described data as signals represented as
functions x over a domain. However, most of the time,
the functions x are discretized in the computer. For
example, the domain of a function representing a 2D
color image as the signal x : R2 7→R3 is discretized
into a grid with p2 pixels, where each pixel has a value
in R3. The data point x is therefore represented, in the
computer, by an array of length p2 ∗3.
A recent line of research—the literature of operators
and neural operators (Kovachki et al., 2023)—offers a
different approach by treating each data point x as a
continuous function without discretization.
Remark: Opportunities: The cases of manifold-valued
signals on manifold domain, topological-valued signals
on topological domains, or topological-valued signals
on manifold domains are not included in the table,
since they have rarely been considered in the ma-
chine learning and deep learning literatures. These
classes represents an interesting avenue for future
research.
Remark: Data as Spaces: Beyond data as coordinates
and data as signals, a third class of data types can be
considered: data as spaces. In this class, a data point
may instead represent an entire space: the data point x
is a manifold or topological space itself. For example,
consider a dataset of molecules. Each molecule x can
be represented as a distinct graph that captures the
molecular structure, with nodes representing atoms and
edges representing bonds. The dataset is comprised of
the set of different graphs. Alternatively, the dataset
may consist of 3D surface scans of the human body,
e.g., a dataset of heart surfaces—each data point x is a
distinct manifold. Most of the time, however, there is
additional information associated with each data point
x, so that x is not only a space.
For example, a molecule would not only be described
as the relationships between its atoms, but also as
the positions of its atoms, or as the distance between
two atoms. In other words, there would be a feature
associated to each atom representing the 3D location
of the atom. Hence, the molecule would be, in fact,
represented as a signal from a graph Ωto R3, and not
just as a graph: it will fall in the category described
by Card S5.
Similarly, processing organ shapes such as heart sur-
faces requires knowing the 3D coordinates of each
point on the surface of the heart. Each heart would
be, in fact, represented as a signal from a manifold
M to R3 and not just as a manifold. Consequently,
these examples fall into the category of data as sig-
nals.
In this review, we will use the two main classes of data
representations—as coordinates, and as signals—to
characterize non-Euclidean structure in machine learn-
ing and deep learning models.
IV. SURVEY: NON-EUCLIDEAN MACHINE
LEARNING
We now review a large and disparate body of literature
of non-Euclidean generalizations of algorithms clas-
sically defined for data residing in Euclidean spaces.
The generalization of machine learning methods to
non-Euclidean data relies on the generalization of
their mathematical foundations—probability and statis-
tics—to non-Euclidean spaces. In most cases, this
requires non-trivial algorithmic innovations. Such gen-
eralizations comprise the bulk of the work reviewed
here.
However, we note that there are two “simple” ap-
proaches to generalizing Euclidean models that do
not requires significant algorithmic innovation. They
do not work for all algorithms, and they have limi-
tations. Yet, when possible, they have the benefit of
implementational simplicity. We briefly describe two
classes of such approaches here—what we call “plug-
in” methods and tangent space methods.

9
Non-Euclidean Probability and Statistics.
In a non-Euclidean space, many essential
mathematical concepts must be modified to
respect the inherent structure of the space.
Consider, for example, a dataset consisting
of points that lie on the surface of the
sphere—for example, the coordinates of dif-
ferent cities around the globe. The Euclidean
mean of the points is a value that lies off-
manifold—a point lying somewhere “inside”
the sphere but not on the sphere. To find the
centroid of these points on the manifold, we
must instead use the Fr´echet mean, which
is defined as the point that minimizes the
sum of squared geodesic distances to all other
points in the dataset. By using geodesics, the
Fr´echet mean is constrained to the manifold,
and results in a natural generalization of the
notion of a mean to non-Euclidean space.
The field of Geometric Statistics defines
such non-Euclidean generalizations. We refer
the reader to Pennec (2006) for theoretical
foundations on manifolds, to Guigui et al.
(2023) for a comprehensive introduction to
these foundations and to Pennec et al. (2019)
for real-world applications.
“Plug-In” Methods: The most straightforward way to
generalize machine learning methods to non-Euclidean
spaces is to simply replace the definitions of addition,
subtraction, distances, and means employed in the Eu-
clidean method with their non-Euclidean counterparts.
For example, the k-nearest-neighbors algorithm can be
naturally defined for arbitrary non-Euclidean manifolds
by replacing Euclidean distance with geodesics. Simi-
larly, k-means can be generalized using geodesic dis-
tances and the Fr´echet mean. Any kernel method that
makes use of geodesic distance in the kernel function
fall into this category as well. Many popular implemen-
tations of machine learning algorithms—for example
in the scikit-learn package—permit the user
specification of a distance function, thus facilitating
easy generalization to non-Euclidean spaces.
Tangent Space Methods: An alternative approach is
particularly convenient for non-Euclidean spaces that
are manifolds. We call it the tangent space method.
It consists in projecting the data from the manifold
into the tangent space of a particular point on the
manifold, using the so-called exponential map. Impor-
tantly, the tangent space can be seen as a Euclidean
space. Once the data are mapped to Euclidean space,
traditional Euclidean machine learning can be applied.
This approach typically achieves better results than ap-
plying Euclidean methods directly to the original non-
Euclidean data. However, in many cases—particularly
for manifolds with greater curvature—it induces biases
in the results due to errors in the local Euclidean
approximation of the manifold. Nonetheless, the ap-
proach is relatively simple, and can be worthwhile for
data lying on relatively flat manifolds, or on manifolds
that are flat at the scale of the spread of the data.
Both the plug-in and tangent space methods have the
advantage of implementational simplicity. However,
many machine learning algorithms require more than
just the specification of distances and means, which
limits the applicability of the plug-in method. Ad-
ditionally, in many cases, the biases induced by the
tangent space projection are intolerable for the appli-
cation, which limits its scope. Consequently, in many
scenarios, it is necessary to explicitly constrain aspects
of the algorithms to the manifolds of interest. This
comprises the bulk of the work of Non-Euclidean Ma-
chine Learning, which we cover in this section. We first
cover regression methods, followed by dimensionality
reduction methods. In the following section, we will
cover Non-Euclidean Deep Learning methods, which
may be used for regression, dimensionality reduction,
or classification.
We note that the Non-Euclidean Machine Learning
methods reviewed in this paper assume that cer-
tain topological, algebraic, or geometric structures are
known a priori to be present in the data or learning
problem. These methods thus require pre-specifying
these structures. Sometimes, however, the underly-
ing structure is unknown. A class of methods aims
to discover and characterize unknown non-Euclidean
structure in data. This class includes topological data
analysis, certain manifold learning approaches that
learn parameters of a latent manifold, such as in metric
learning, and algebraic methods for discovering latent
group structure, also known as group learning. Such
methods can be used to identify the non-Euclidean
structures present in data so that the appropriate non-
Euclidean machine learning method can be specified
and applied. These methods are beyond the scope of
this review, as they are used “prior to” non-Euclidean
machine learning.
A. Regression
This subsection reviews regression on manifolds. We
first introduce our taxonomy defined by the mathemati-

10
cal structure of both the data spaces and the regression
model. Then, we review the literature on this topic in
Figure 5 with details in the text.
Taxonomy: In machine learning, the problem of re-
gression can be defined as learning a function f going
from an input space X to an output space Y . Figure 5
organizes regression models into a taxonomy that is
first based on the geometric properties of input and
output spaces—see first two columns in Figure 5.
Here, we differentiate conventional Euclidean spaces
from more complex manifold spaces, setting the stage
for a detailed exploration of five key configurations:
Euclidean to Euclidean, one-dimensional Euclidean to
Manifold, Euclidean to Manifold, Manifold to Eu-
clidean, and Manifold to Manifold.
Each configuration is then distinguished by its re-
gression model—see third column in Figure 5. Lin-
ear methods assume a relationship between the input
variable x ∈X and output variable y ∈Y that can
be represented by a linear function as y = Ax + b,
where A is a matrix of coefficients, and b is a vector
of constants—hence constraining the y’s to belong to
a linear space characterized by the parameters A, b.
We note that linear regressions are not appropriate
on manifolds since the addition operation, a linear
operation, is not well-defined on a manifold, a non-
linear space. Consequently, linear methods become
geodesic methods on manifolds on rows 2 and 3 of
Figure 5, where the relationship between x and y
can be represented by a geodesic characterized by
a set of parameters analogous to A, b above. Next,
non-linear parametric and non-geodesic parametric
methods involve relationships described by a fixed set
of parameters but in a more complex form, such as
polynomials or other non-linear equations y = fθ(x)
where θ represents the parameters. Non-parametric
approaches, on the other hand, do not assume a para-
metric form for the relationship between x and y,
providing flexibility to model relationships that are
directly derived from data. The term nonparametric
does not necessarily mean that such models completely
lack parameters but that the number and nature of
the parameters are flexible and not fixed in advance,
contrarily to parametric approaches.
Further, these methods are categorized as Bayesian or
non-Bayesian—represented by a fuzzy line or a line
in Figure 5. Bayesian methods integrate prior proba-
bilistic distributions with observed data, facilitating the
updating of knowledge about the parameters in light
of new evidence. This approach contrasts with non-
Bayesian methods, which rely exclusively on observed
data, typically employing so-called frequentist statisti-
cal principles without incorporating prior distributions
on parameters.
In what follows, we review regression models ac-
cording to this geometric taxonomy, row by row in
Figure 5. For each category of regression models,
we showcase one emblematic, category-defining, paper
that reflects the transition from traditional Euclidean
analysis to the manifold paradigm. We consider the
following papers to be out of scope: 1) Papers that
generalize a class of curves (e.g., splines) on manifolds,
but do not leverage this generalization to perform
regression, 2) Papers that perform interpolation on
manifolds, but not regression, 3) Papers whose regres-
sion method has been developed for only one type
of manifold (e.g., only for spheres). Additionally, the
methods reviewed in this section exclusively apply to
one of the data types presented in Section III: regres-
sion inputs x and outputs y are data as coordinates in
a space, where the space is either a Euclidean space
or a manifold.
1) Euclidean Input, Euclidean Output: We review
classical regression models that have been gen-
eralized to manifolds, in the first row of Fig-
ure 5. Initiated with linear regression by Leg-
endre and Gauss (Legendre, 1805), this cate-
gory has expanded to include nonlinear para-
metric models, particularly polynomial models
introduced by Gergonne (Gergonne, 1815). The
inclusion of non-parametric methods is marked by
the Nadaraya-Watson kernel methods (Nadaraya,
1964), further local linear models (Fan, 1993),
and Breiman’s development of random forests
(Breiman, 2001). In Bayesian analysis, the linear
and polynomial approaches are respectively rep-
resented by Bayesian Multilinear (Box and Tiao,
1968) and Polynomial Bayesian models (Halpern,
1973), with the Gaussian Process (Williams and
Rasmussen, 1995) illustrating the non-parametric
Bayesian perspective.
2) One-dimensional Euclidean Input, Manifold
Output: The earlier generalizations of classical
regression models to manifolds involve generaliz-
ing the output space. Many of these models con-
sider a one-dimensional input x, and are shown in
the second row of Figure 5. Geodesic regression
extends linear models into manifolds from one-
dimensional Euclidean inputs (Fletcher, 2011).
We note a difference of 206 years between the
establishment of linear regression to its geometric
counterpart. Polynomial regressions on manifolds

11







	








 



­











 





























­



­



	












	


























Fig. 5: Geometric Structures in Regression categorized according to the geometry of the input and output data spaces (first two columns)
and the geometry of the regression model (last column). Yellow boxes correspond to Euclidean space, while orange corresponds to non-
Euclidean space. Partially Euclidean cases are light orange. Each pictogram represents the kind of parametrization used: linear (geodesic),
nonlinear (nongeodesic) parametric, or nonparametric, with or without Bayesian priors.

12
(Hinkle et al., 2012a) and Bezier-splines fitting
on manifolds (Hanik et al., 2020) generalize their
Euclidean counterparts to manifolds in the out-
put space, 197 years and 35 years later respec-
tively. Non-parametric methods with output values
on manifolds include Fr´echet-casted Nadaraya-
Watson kernel regression (Davis et al., 2010), and
local geodesic regression (Sch¨otz, 2022)—the lat-
ter being the counterpart of local linear regression
to manifolds. In terms of Bayesian methods, the
geodesic regression model was made Bayesian in
(Zhang et al., 2020), while manifold polynomial
regression turned Bayesian in Muralidharan et al.
(2017). Lastly, one non-geodesic, non-parametric,
Bayesian model belongs to this category: ker-
nel regressions by Devito and Wang’s Brownian
motion model (Wang, 2015) which generalizes
Nadaraya-Watson’s approach 51 years later.
3) Euclidean Input, Manifold Output: Next, we
review regression models with outputs on a man-
ifold, for which the input is not restricted to be
one-dimensional, in the third row of Figure 5. The
Fr´echet regression (Petersen and Muller, 2016)
generalizes the geodesic regression to higher-
dimensional Euclidean inputs. We also find sev-
eral nongeodesic parametric regression models.
One of the earliest works in this category deals
with the parametric regression of regularized man-
ifold valued functions (Pennec et al., 2006). A
key idea is to rephrase convolutions as weighted
Fr´echet means of manifold variables, which be-
come the parameters of the implicit function.
Also in the nongeodesic parametric regression
category, we find the semi-parametric intrinsic
regression model by (Shi et al., 2009), or the
stochastic development regression by K¨uhnel and
Sommer (2017). Non-parametric methods with
manifold-valued outputs include the manifold ran-
dom forests (Tsagkrasoulis and Montana, 2018)
that generalize their Euclidean counterpart 15
years later, the local Fr´echet regression (Petersen
and Muller, 2016), another multi-dimensional
generalization of the local linear regression, and
the local extrinsic regression by (Lin et al., 2017).
In terms of Bayesian approaches, we observe a
lack of methods within geodesic and nongeodesic
parametric models. However, we find Bayesian
non-parametric methods, such as Manifold Gaus-
sian processes (Yang and Dunson, 2016) and
Mallasto’s wrapped Gaussian processes (Mallasto
and Feragen, 2018) which generalizes Williams
and Rasmussen’s Gaussian processes 9 and 13
years later.
4) Manifold Input, Euclidean Output: We now
review geometric generalizations of Euclidean re-
gression models that have received less attention:
models in which the regression input space is
a manifold. We start with methods for which
the output space is a Euclidean space, in the
fourth row of Figure 5. Johnson and Wehrly
defined a regression model for an angular (one-
dimensional) variable with a Euclidean scalar
(one-dimensional) output variable (Johnson and
Wehrly, 1978). Chernov’s circular regression out-
puts to higher-dimensional Euclidean spaces, typ-
ically two- or three-dimensional (Chernov, 2010).
Pelletier’s non-parametric regression approach
stands out for estimating functions from manifold
inputs to Euclidean outputs (Pelletier, 2006). Both
methods are non-Bayesian. In terms of Bayesian
methods, we find the Bayesian circular-linear re-
gression method (Gill and Hangartner, 2010), but
no Bayesian nonparametric methods—indicating
an opportunity for further exploration.
5) Manifold Input, Manifold Output: The fifth
row of Figure 5 presents the most general case
of regression, from a geometric perspective. In
these models, both the regression input and output
spaces are manifolds. This category includes a
method that first transforms both input and output
data from manifolds to Euclidean spaces, and
then apply an auxiliary classical regression model
on Euclidean spaces (Guo et al., 2019). In the
nonparametric methods, we find Steinke’s regular
splines methods (Steinke et al., 2008) and Baner-
jee’s manifold kernel regression (Banerjee et al.,
2015) generalizing in 2015 both the classical
kernel regression from 1964 and its generalization
to manifold output space from 2010. At the time
of the review, there is no method in this category
that adopts the Bayesian point of view.
B. Dimensionality Reduction
This
subsection
reviews
dimensionality
reduction
methods, which include: manifold learning methods,
where a manifold of lower-dimension is learned within
a Euclidean space of higher dimension; submanifold
learning methods, where a submanifold of lower-
dimension is learned within a manifold of higher
dimension; and encoding methods, where data in a
high-dimension Euclidean or manifold is mapped to
a lower-dimensional Euclidean or manifold via an
encoder. We first introduce our taxonomy defined by

13
the mathematical structure of both the spaces and the
dimensionality reduction model. Then, we review the
literature on this topic in Figure 6 with details in the
text.
Taxonomy: We define the problem of dimensionality
reduction as transforming high-dimensional data from
their data space X into a latent space Y of lower
dimension, i.e., dim(Y ) < dim(X). Figure 6 organizes
dimensionality reduction methods into a taxonomy first
based on the geometric properties of the data and latent
spaces—see the first two columns. It differentiates
between conventional Euclidean spaces and the more
complex manifold spaces, setting the stage for the
following four key configurations: Euclidean Data to
Euclidean Latents, Manifold Data to One-Dimensional
Euclidean Latents, Manifold Data to Euclidean Latents,
Euclidean Data to Manifold Latents, and Manifold
Data to Manifold Latents. For each configuration, the
lower dimensional latent space Y
is schematically
represented as a space of dimension 1: a line for Eu-
clidean spaces, and a circle for manifolds. The higher-
dimensional data space X is schematically represented
as a space of dimension 2: a plane for Euclidean
spaces, and a sphere for manifolds. Yet, we emphasize
that we review all dimensionality reduction methods
here; not only the methods going from dimension 2 to
dimension 1.
Each configuration is then distinguished by the ap-
proach to dimensionality reduction—see third column
in Figure 6. First, dimensionality reduction approaches
are organized depending on whether they leverage
a decoder, and if so, of what type: linear (resp.
geodesic), nonlinear (resp. nongeodesic) parametric,
non-parametric—represented by full line, full curved
line, and dashed curved line in the pictograms of
Figure 6. While every dimensionality reduction method
converts high-dimensional data into lower-dimensional
latents, only some methods introduce a decoder D that
converts low-dimensional latents y’s back into high-
dimensional data x’s: x = D(y). When a decoder
D exists, the latent space Y can be embedded into
the data space X via D(Y ) ⊂X. The yellow and
orange curves in the pictograms represent that em-
bedding. We further distinguish two subcases: whether
the embedded space D(Y ) is linear (geodesic, in
the manifold case), or nonlinear (or non-geodesic).
The presence of a decoder is represented by a black
arrow and the legend D; the black arrow is a dotted
arrow if the decoder is non-parametric. Dimensionality
reduction methods that do not leverage any decoder
are reviewed independently at the bottom of Figure 6.
Additionally, we highlight in the pictograms if the data
is assumed to come from a generative model. A gen-
erative model explains how data points are generated
from latents using probability distributions. When a
generative model is used for a given dimensionality
reduction approach, it is denoted by a shaded yellow
or orange dot. When there is no generative model,
we only use a non-shaded yellow or orange dot. For
example, principal component analysis (PCA) has a
decoder, but no generative model; while probabilistic
PCA uses a decoder with a generative model.
Second,
approaches
are
organized
depending
on
whether they leverage an encoder E, where E is a
function mapping data x’s to latents y’s: E(x) = y.
Indeed, while every dimensionality reduction methods
converts high-dimensional data into lower-dimensional
latents, only some methods do so through an explicit
encoder function E. Others might only compute the
latent yi corresponding to a data point xi through the
result of an optimization minyCost(xi). The presence
of an encoder is represented by a black arrow and
the legend E, and the arrow is a dotted arrow is the
encoder is non-parametric. The use of optimization is
represented by the ! and no black arrow.
Third, approaches are organized depending on whether
they compute an uncertainty on the latents y associated
with the data points x. Uncertainty on the latents is
represented by the posterior distribution p(y|x). If the
method provides a posterior distribution—or at least
an approximation of it—we use a shaded blue dot.
If not, we use a non-shaded blue dot. The two cases
of without and with posterior form the subrows of
a given configuration in the table. We mark with an
S the cases where the posterior distribution p(y|x)
is not computed analytically, but provided through a
sampling approach.
Fourth, approaches are categorized based on how they
compute uncertainty on the parameters of the decoder,
i.e., on whether they are Bayesian or not. For example,
traditional PCA does not incorporate uncertainty, but
Bayesian PCA does. This distinction represents the last
subrow of each configuration, illustrated by a shaded
orange line.
We now survey the various categories of dimensional-
ity reduction row by row in Figure 6.
1) Euclidean Data, Euclidean Latents: We de-
scribe approaches from this configuration subcol-
umn by subcolumn. Principal Component Anal-
ysis (PCA) (Pearson, 1901) learns a linear sub-
space, while Probabilistic PCA (PPCA) (Tipping

14





		





 
­

­
­



	





­
­
­









­
­
		



­
	





­



­	












­
	


­
	
­	
­
­





	
­
­

­


­

























	



	
­













­




­

­
­


­




 	

	
 
 
	

	




­
	


­­

Fig. 6: Geometric Structures in Dimensionality Reduction categorized according to the geometry of the data and latent spaces (first two
columns) and of the dimensionality reduction model (last column). Yellow boxes correspond to Euclidean space, while orange corresponds
to non-Euclidean space. Partially Euclidean cases are light orange. Each model is further classified by use and type of encoder/decoder,
as well as whether it computes a posterior on the latents, and whether it is Bayesian. (P)PC: (Probabilistic) Principal Curves; GP LVM:
Gaussian Process Latent Variable Model; PGA: Principal Geodesic Analysis; GPCA: Geodesic PCA.

15
and Bishop, 1999) achieves the same goal within
a probabilistic framework relying on a latent vari-
able generative model, which provides a posterior
distribution on the latents. Bayesian PCA (Bishop,
1998) additionally learns a probability distribution
on the parameters of the models, representing the
parameters of the linear subspace (e.g., slope and
intercept). These methods are restricted in the
type of subspace that can be fitted to the data:
only linear subspaces. To lift this restriction, we
also find numerous methods that learn nonlinear
manifolds from Euclidean data. The whole field
of manifold learning fits in this case, and can be
subdivided into further subcategories depending
on how the learned manifold is being represented:
by a nonlinear parametric decoder, by a nonpara-
metric decoder, or without any decoder at all.
Here, we only introduce one category-defining
method for each of the subcategories we consider.
In the nonlinear parametric decoder category,
we introduce autoencoders. Autoencoders (AEs)
(Rumelhart et al., 1986) learn a nonlinear sub-
space of a Euclidean space, while variational au-
toencoders (VAES) (Kingma and Welling, 2014)
achieve the same goal with a probabilistic frame-
work relying on a latent variable generative
model. The Full-VAE model proposed in (Kingma
and Welling, 2014) additionally learns a poste-
rior on the parameters of the model, i.e., the
parameters of the decoder. The methods in these
category all leverage an encoder. In the nonpara-
metric decoder category, we introduce principal
curves (Hastie and Stuetzle, 1989), which fit a
nonlinear manifold to the data, but do not leverage
a posterior on the latents. In this same category,
we also introduce Local Linear Embedding (LLE)
(Roweis and Saul, 2000) and Gaussian Process
Latent Variable Models (GP LVM) (Lawrence,
2003), which all provide posteriors on the latents.
A probabilistic approach to principal curves (PPS)
developed in Chang and Ghosh (2001) also falls
under this category. Finally, we introduce the
Bayesian GP LVM, which additionally provides a
posterior on the model’s parameters. We note that
the methods of this category do not leverage any
encoder, and instead compute the latent associated
to a given data point by solving an optimization
problem.
These techniques are based on vector space op-
erations that make them unsuitable for data on
manifolds. As a consequence, researchers have
developed methods for manifold data, which take
into account the geometric structure; see next row
in the Table, described in the next paragraph.
2) Manifold Data, One-Dimensional Euclidean
Latents: Fitting a geodesic to manifold data
points has been performed using least-squares
or other likelihood criteria in many domains,
in particular as a first step of the extension of
PCA to manifolds (see next paragraph). In the
nongeodesic parametric decoder category, a nat-
ural extension with one-dimensional latents is to
define approximating splines using higher order
polynomials that are then fitted to a set of points
on a Riemannian manifold (Machado et al., 2010,
Machado and Leite, 2006) or on a Lie group
(Gay-Balmaz et al., 2012). In the nongeodesic,
nonparametric decoder category, principal flows
(Panaretos et al., 2014) and Riemannian principal
curves (Hauberg, 2016) generalize traditional Eu-
clidean principal curves to Riemannian manifolds,
25 years later. The probabilistic Riemannian prin-
cipal curves (Kang and Oh, 2024) further intro-
duce a probabilistic framework relying on a latent
variable generative model, hence generalizing the
Euclidean probabilistic principal curves, 23 years
later.
3) Manifold Data, Euclidean Latents: We describe
approaches subcolumn by subcolumn. We note
that approaches from this configuration can be
applied to the previous configuration, i.e., to
one-dimensional latent spaces as well. In the
geodesic decoder category, Principal Geodesic
Analysis (PGA) (Fletcher et al., 2004, Sommer
et al., 2014), tangent PGA (tPGA) (Fletcher et al.,
2004), and Geodesic Principal Component Analy-
sis (GPCA) (Huckemann et al., 2010). learn vari-
ants of geodesic subspaces, generalizing the con-
cept of a linear subspace to manifolds. As such,
these methods represent different generalizations
of PCA to manifolds. Probabilistic PGA (Zhang
and Fletcher, 2013) achieves the same goal, while
adding a latent variable model generating data on
a manifold, and hence generalizes probabilistic
PCA, 16 years later. Similarly, Bayesian PGA
(Zhang and Fletcher, 2013) generalizes Bayesian
PCA by including the posterior distribution of
the parameters defining the submanifolds, i.e.,
the base point and tangent vectors defining the
principal (geodesic) components. However, these
methods are restricted in the type of submanifold
that can be fitted to the data, that is: geodesic

16
subspaces at a point.
The restriction to globally defined subspaces
based on geodesics can be considered both a
strength and a weakness. While it protects from
the problem of overfitting with a submanifold that
is too flexible, it also prevents the method from
capturing possibly nonlinear effects. With current
dataset sizes exploding (even within biomedical
imaging datasets which have been historically
much smaller), the investigation of flexible sub-
manifold learning techniques may become in-
creasingly important.
In the nongeodesic parametric decoder category,
we consider autoencoder approaches on mani-
folds. Variational autoencoders have been gener-
alized to manifold data in (Miolane and Holmes,
2020), a methodology that can be applied to
both AEs and Full-VAEs on manifolds. This is
the only method that learns a multidimensional
nongeodesic submanifold parameterized with a
latent variable model. Lastly, we feature a method
that leverages a nonparametric decoder: the Rie-
mannian LLE (Maignant et al., 2023), which
generalizes its Euclidean counterpart, the LLE, 23
years later. We note the absence of works perform-
ing dimensionality reduction via a nonparametric
decoder in a generative model nor in a Bayesian
framework. This represents a possible avenue for
research.
4) Euclidean Data, Manifold Latents. We describe
approaches from this configuration subcolumn by
subcolumn. We first find approaches that belong
to the VAE framework, where the latent space is
a manifold, even though the data belong to a Eu-
clidean space. For example, the hypersphere VAE
by Davidson et al. (2018) proposes a hyperspher-
ical latent space, Falorsi et al. (2018) propose a
Lie group latent space, and Mikulski and Duda
(2019) propose a toroidal latent space. All of these
approaches provide an (approximate, amortized)
posterior on the latent variables, represented as a
probability distribution on the manifold of inter-
est. In some sense, these approaches represent the
counterpart of Miolane and Holmes (2020) which
considers a manifold data space and a Euclidean
latent space. Next, we find nonparametric de-
coder approaches, such as the manifold Gaussian
Process Latent Variable Model (GPLVM) (Jensen
et al., 2020) which generalizes the GPLVM from
the Euclidean case (Lawrence, 2003).
5) No Decoder. Few dimensionality reduction mod-
els avoid using a decoder. For this reason, we
briefly survey these models across all geomet-
ric data and latent structures. In the Euclidean
data with Euclidean latent case (corresponding
to row 1), we find t-distributed stochastic neigh-
bor embedding (tSNE) (van der Maaten and
Hinton, 2008), Uniform Manifold Approximation
and Projection (UMAP) (McInnes et al., 2018)
and Isomap (Tenenbaum et al., 2000). These ap-
proaches learn lower-dimensional representations
of data but do not provide a latent variable
generative model, nor a parameterization of the
recovered subspace. In the manifold data with
Euclidean latent case (corresponding to row 3), a
flexible generalization of linear subspaces to man-
ifolds is given by barycentric subspaces (Pennec,
2018) where the submanifold is defined implicitly
through geodesics to several reference points. One
approach of interest in this line of work is to
provide sequences of nested spaces of increasing
dimensions that better and better approximate the
data, a notion conceptualized by (Damon and
Marron, 2013) with sequences of nested rela-
tions of possibly non-geodesic subspaces. Iterated
Frame Bundle Development (IFBD) (Sommer,
2013) is another optimization method iteratively
building principal coordinates along new direc-
tions. In the Euclidean data with manifold latent
case (corresponding to row 4), we first find the
Poincar´e embeddings Nickel and Kiela (2017)
that embeds data points in X into a hyperbolic
latent space Y , which is useful for representing
data with hierarchical structure. We note that this
approach does not leverage any encoder function
E, and instead solves an optimization problem
to find the latent variable yi that corresponds to
each data point xi, represented on the Figure 6
by the pictogram “!”. Second, the Riemannian
SNE (Bergsson and Hauberg, 2022) generalizes
traditional Euclidean SNE (van der Maaten and
Hinton, 2008) but 14 years later. Finally, the last
decoder-free model, called principal subbundles
(Akhøj et al., 2023), is the only dimensionality
reduction model designed for manifold data and
manifold latents. Indeed, we note that there are
no decoder-based approaches for dimensionality
reduction for this case. The principal subbundles
method does not leverage either an encoder or
decoder, but instead solves optimization problems
to find the low-dimensional latent variable yi as-
sociated with each data point xi. The authors also

17
propose a decoding approach that uses optimiza-
tion rather than an explicit decoder to find the data
point xi that decodes a given latent variable yi.
Overall, the sparsity of decoder-free dimension-
ality reduction models leaves open opportunities
for many choices of geometric data/latents.
This concludes our review and categorization of non-
Euclidean generalizations of machine learning methods
for regression and dimensionality reduction. While sev-
eral deep neural network models—such as variational
autoencoders—appear in our dimensionality reduction
taxonomy, we have saved a more complete treatment
of deep learning for the next section. Deep neural
networks stand out from more traditional machine
learning methods in that they are flexible compositions
of functions that progressively transform data between
spaces. In our consideration of dimensionality reduc-
tion methods, we abstracted away from the transfor-
mations performed by individual neural network layers
to consider only the structure of the input and latent
spaces (which may be many layers deep in a model). In
the next section, we explicitly consider the input-output
structure of individual neural network layers, and the
ways in which topology, geometry, and algebra have
been incorporated into these layers.
V. SURVEY: NON-EUCLIDEAN DEEP LEARNING
We now review non-Euclidean structures in deep learn-
ing, particularly focusing on how geometry, topology,
and algebra can enrich the structure of a given layer
within a deep neural network. A neural network layer
is a function f : X →Y , and thus can be analyzed and
categorized in terms of the mathematical structure of
the input space X and output space Y . We first cover
neural network layers without an attention mechanism,
followed by layers with attention. We then compile a
list of datasets and benchmarks used in the literature
to compare deep learning methods with geometric,
topological, or algebraic properties.
A. Neural Network Layers Without Attention
We categorize layers from neural networks according
to their mathematical structure. A given paper can
appear several times in this subsection (and the next),
if this paper introduces several novel layers. Neural
networks using the attention mechanism are out of
scope for this subsection and will be presented in the
next subsection.
Taxonomy: Figure 7 organizes deep learning methods
into a taxonomy based on the mathematical properties
of the input and output of neural network layers (first
two columns), as well as on the properties of the layer
model (third column). The rows show different types
of layers that have been published in the literature,
organizing them by use of geometry, algebra (group
actions), and topology. The figure graphically contex-
tualizes existing methods and identifies potential areas
for innovation. We survey the field row by row in
Figure 7.
Geometry in Neural Network Layers: Here, we cate-
gorize neural network layers that consider their inputs
and outputs as coordinates in space, where the spaces
can be equipped with geometric structures.
1) Euclidean
input,
Euclidean
output:
The
category-defining layer for this configuration
is
the
Perceptron
layer
(Rosenblatt,
1958),
commonly used as a component in a deep neural
network comprised of several identical layers:
the celebrated Multi-Layer Perceptron (MLP).
2) Euclidean
input,
manifold
output:
The
Perceptron-Exp layer from Miolane and Holmes
(2020) generalizes the Perceptron layer to layer
outputs on manifolds. Here, Exp denotes the
Riemannian exponential map, which is applied
to the result of the Perceptron. Indeed, Exp is an
operation that maps tangent vectors to points on a
manifold, i.e., that maps an input in an Euclidean
space to an output on the manifold. Only the
Perceptron component of this layer has learnable
weights; the manifold needs to be known a priori
in order to specify and implement the appropriate
exponential map. Additionally, in general, there
is no analytical expression for the Exp map,
which needs to be computed numerically. To
avoid this computational cost, this layer is best
implemented for manifolds whose Exp enjoys an
analytical expression.
3) Manifold input, Euclidean output: The Log-
Perceptron layer from (Davidson et al., 2018)
generalizes the Perceptron layer to layer inputs
on manifolds. Here, Log denotes the Riemannian
logarithm map, which is applied just before the
Perceptron. Indeed, Log is an operation that maps
points on manifolds to tangent vectors; that is, it is
the inverse of the Exp. The layer maps an input on
a manifold to an output in Euclidean space where
the Perceptron with its learnable weights can be
applied. This layer can thus be thought of as the
inverse of the Perceptron-Exp layer from Miolane

18




	


	


	







 ­
 ­
 ­
­








	
 

	


  



	
	








 ­
	
		
­
­

­


	






	


 


­



	
­


	
­






	




	


	




	




	







	



­
­
	
	
	
	
	
	
	
	

	



	


 


­
Fig. 7: Topology, Geometry, and Algebra in Neural Network Layers organized according to the mathematical properties of the layer
inputs x and outputs y (first two columns) and of the layer model (third column). The first five rows show inputs and outputs that are
data represented as coordinates in a space, typically, a Euclidean space Rn or a manifold M. The next rows show inputs and outputs that
are signals, i.e., functions from a domain to a codomain. The black curved arrows represent a group action on a space, such as a signal’s
domain or codomain. Notations: Rn: Euclidean space; M: Manifold; Gp: group, Gp/H Homogeneous manifold for the group Gp; P:
point set; G: graph; Ω: topological space.

19
and Holmes (2020). Just as for the Perceptron-
Exp, the manifold needs to be known for this
layer to be implemented, and manifolds whose
Log enjoys an analytical expression are preferred.
4) Manifold input, manifold output: This config-
uration is first illustrated with the Bimap and
SPDNet layers from (Huang and Van Gool, 2016),
where SPD refer to symmetric positive definite
matrices. This layer is indeed restricted to the
manifolds of SPD matrices. More generally, Man-
ifoldNet (Chakraborty et al., 2022) builds on the
reformulation of convolutions as weighted Fr´echet
means of Pennec et al. (2006) to propose a layer
whose inputs and outputs are both coordinates on
a manifold. In this layer, a weighted mean of the
inputs is computed, where the weights are learned
with classical back propagation. Since convolu-
tions are equivalent to computing weighted sums,
this layer has also been termed the manifold-
valued data convolution. We note that, in general,
there is no analytical expression for the weighted
Fr´echet mean, which needs to be obtained by
optimization. To avoid this computational cost,
approximations using tangent means are also con-
sidered.
Algebra in Neural Network Layers: Here, we consider
neural network layers that fall into the category of
equivariant deep learning, which leverages the con-
cepts of symmetries and group actions. We refer the
reader to Cohen et al. (2021) and Weiler et al. (2024)
for extensive treatments of the foundations of this field.
A core idea underlying this line of work is to build the
symmetries natural to a data domain (e.g. translational
and rotational symmetries for object classification) into
the structure of the model. This facilitates weight
sharing across different transformations so that the
same convolutional filter can be used to detect a given
feature in an image in, for example, all orientations.
To accomplish this, the input and output spaces of
these layers are equipped with group actions, and
the equations defining the layer are written to be
compatible with these actions. Intuitively, if a layer
is equivariant to a group action, this means that, if
the layer produces an output y for a given input x,
then it should also produce a transformed y for any
transformed x—where transformed here means “acted
upon by the same group element.” Here, we highlight
key examples of different equivariant layers that fall
into each category of our taxonomy.
5) Input and output: Euclidean with group action.
The Equivariant-Perceptron layer from (Finzi
et al., 2021) and the linear layer in the Geomet-
ric Algebra Transformer (GATr) (Brehmer et al.,
2023) illustrate this configuration. They represent
the only equivariant layers processing inputs and
outputs that are coordinates in space, as opposed
to signals over a space. The work by (Finzi
et al., 2021) allows us to build an Equivariant-
Perceptron layer given any matrix group action
on the input and output spaces.
6) Input and output: Euclidean signal on Eu-
clidean domain with group action. While the
previous row focused on data as coordinates in
space, this row considers data as signals. Indeed,
both inputs and outputs are defined by function
x : Rn 7→Rn′ and y : Rm 7→Rm′. For
example, the input and output could be images,
or feature maps, defined over the domains Rn and
Rm respectively. The classic convolutional layer
(LeCun et al., 1998) is a layer with translation
equivariance: a translation of the input image
x yields a translated feature map y as output.
Regular steerable convolutional layers (Cohen and
Welling, 2017) generalize this approach to more
complicated cases of equivariances, using groups
beyond the group of translations. In this row, the
group action is only on the domain of the input
and output signals: this is the meaning of the ad-
jective “regular” in regular steerable convolutions.
7) Input and output: Euclidean signal with group
action on Euclidean domain with group action.
Similar to the previous row, both inputs and
outputs are signals over a space: x, y : Rn 7→Rn′.
By contrast to the previous row, however, both
the domain and the codomain of each signal
are equipped with a group action. The Steerable
Convolutional layer (Cohen and Welling, 2017) il-
lustrates this configuration. Compared to the other
layer from the same paper presented above, the
group action can affect the domain and codomain
simultaneously.
8) Input: Euclidean signal on Euclidean domain
with group action; output: Euclidean signal on
manifold with group action. This row presents
layers whose input can be described as a sig-
nal x : Rn 7→Rn′, typically an image, and
whose output is a more complicated signal of the
form y : Gp 7→Rn′, i.e., defined over a Lie
group Gp—where we recall that a Lie group is
a manifold that is also a group. The first layer
of the Group Convolutional Network by (Cohen
and Welling, 2016) illustrates this configuration.

20
In this network, the second layer necessarily in-
puts the output of the first layer, i.e., inputs a
signal defined over a Lie group. Consequently,
the second (and following) layers of this neural
network are presented in the next row.
9) Input and output: Euclidean signal on mani-
fold with group action. Here the manifold do-
mains of both the input and output signals are
equipped with a group action. The second layer of
the Group Convolutional Network by (Cohen and
Welling, 2016) showcases this example, where the
manifold is in fact a Lie group, and the group
action is the group composition.
10) Input and output: Euclidean signal on ho-
mogeneous manifold with group action. This
row presents layers processing inputs and out-
puts represented as signals of the form x, y :
Gp/H
7→Rn. Here, the domain Gp/H de-
fines a so-called homogeneous manifold, that is:
a manifold equipped with a group action that
is such that, for every pair of points on the
manifold, there exists a group element that can
transform one point onto the other via the action.
The General Steerable Convolutional Layer with
the so-called quotient representation (Cohen and
Welling, 2017) falls in this category.
11) Input and output: Euclidean signal with group
action on homogeneous manifold with group
action. Similar to the previous row, signals are
represented by functions x, y : Gp/H 7→Rn,
i.e., with an homogeneous manifold domain with
group action. By contrast to the previous row,
however, there is also a group action on the
Euclidean codomains of both the input and the
output signals. The group convolutional layer for
homogeneous spaces by (Cohen et al., 2019b)
falls into this category, as well as the General
Steerable layer.
12) Input and output: Euclidean signal on mani-
fold. In this row, the input and output signals have
manifold domains, and Euclidean codomains. Sig-
nals are thus represented as x, y : M 7→Rn.
The channel-wise gauge convolutional layer by
(Cohen et al., 2019a) processes this type of sig-
nals. Here, the concept of gauge equivariance
replaces the group equivariance. Gauge equivari-
ance describes the idea of being agnostic to the
orientation of a local coordinate systems on the
manifold of interest M.
13) Input and output: Euclidean signal with group
action on manifold. This row generalizes the pre-
vious row, where an action can transform elements
of the codomains of the signals x, y : M 7→Rn.
An example of this category is the general gauge
convolutional layer with induced representation
(Cohen et al., 2019a).
Topology in Neural Network Layers: Here, we catego-
rize neural network layers that consider their inputs and
outputs as signals over a topological domain. We refer
the reader to the survey by Papillon et al. (2023) for a
comprehensive overview of the neural network layers
within this category, and we focus only on selected
illustrative examples. These layers are all equivariant
to the permutation of the elements of their topological
domains: permutation of the points in a set, of the
nodes in a graph, etc. As such, all these layers are,
at a minimum, equipped with group action on their
domain.
14) Input and output: Euclidean signal on a set,
with domain group action. Layers in PointNet++
(Qi et al., 2017) process point clouds, which have
signal of the form x, y : P 7→Rn where P is a set.
In this work, the codomain is restricted to the 2D
or 3D Euclidean spaces R2, R3 as it is designed
to process point clouds in 2D or 3D. Thanks to
its domain group action, the model is equivariant
to a permutation in P. However, PointNet++
is only approximately invariant to 3D rotations
and translations operating in the codomain R3.
Namely, it leverages a preprocessing step that first
aligns point clouds in position and orientation.
15) Input and output: Euclidean signal on a set,
with domain and codomain group actions.
Here, the input and output signals are point
clouds, just like the row above. Layers in Tensor
Field Networks (TFN) (Thomas et al., 2018), and
PONITA (Bekkers et al., 2024) process signals in
this category. Like PointNet++, TFN and PONITA
process 2D and 3D point clouds and are equivari-
ant to the permutation of their points in P. TFN
and PONITA are additionally equivariant to 3D
rotations and translations in R3.
16) Input and output: Euclidean signal on graph,
with domain group action. Layers in this row
process signals of the form x, y : G 7→Rn,
i.e., defined over a graph G. This category is
exemplified by Graph Convolutional layer (Kipf
and Welling, 2017) and Message Passing layers
(Gilmer et al., 2017).
17) Input and output: Euclidean signal on graph,

21
with domain and codomain group action. Lay-
ers in this row are similar to layers in the previous
row, except that they add a group action on
the codomain of the signals. We find, for exam-
ple, the E(n-Equivariant Graph Neural Networks
(EGNN) by (Satorras et al., 2021).
18) Input and output: Euclidean signal on cellular
complex, with domain group action. This row
considers signals defined over a cellular complex
Ω, i.e., functions of the form x, y : Ω7→Rn.
Examples include simplicial convolutional (Ebli
et al., 2020), cellular convolutional (Roddenberry
et al., 2022), simplicial message passing (Bodnar
et al., 2021), and cellular message passing (Hajij
et al., 2020) layers.
19) Input and output: Euclidean signal on cellular
complex, with domain and codomain group
actions. Layers in this row are similar to layers
in the previous row, except that they add a group
action on the codomain of the signals. We find, for
example, the E(n)-Equivariant Message Passing
Simplicial Networks (EMPSN) (Eijkelboom et al.,
2023) and the Clifford group Equivariant Message
Passing Simplicial Networks (EMPSN) (Liu et al.,
2024) which both introduce group actions on the
codomain, but for different groups.
20) Input and output: Euclidean signal on hyper-
graph, with domain group action. This row
turns to signals x, y : Ω7→Rn, where Ωnow
denotes a hypergraph, which is another type of
topological space. This configuration is exempli-
fied by Hypergraph Convolutional layer (Arya
et al., 2020) and Hypergraph Message Passing
layer (Heydari and Livi, 2022).
21) Input and output: Euclidean signal on com-
binatorial complex, with domain group ac-
tion. This row considers signals with the most
general type of topological domain, the com-
binatorial complex Ω, i.e., signals of the form
x, y : Ω7→Rn. Examples of neural network
layers that process this type of signals are the
Combinatorial Convolutions (Hajij et al., 2023b)
and Combinatorial Message Passing (Hajij et al.,
2023b).
22) Input and output: Euclidean signal on combi-
natorial complex, with domain and codomain
group action. Similar to the previous row, this
last configuration process signals defined on com-
binatorial complex domain Ωwith Euclidean
codomain. However, by contrast with the previous
row, the Euclidean codomain is equipped with a
group action. The E(n)-equivariant topological
neural networks from (Battiloro et al., 2024) is
an example of this configuration. The layers of
this network can process geometric features in
the codomain Rn, such as velocities or positions
associated with the elements of the topological
domain Ω.
B. Neural Network Layers with Attention
We now review mathematical structures in neural net-
work layers that leverage the attention mechanism.
In particular, we focus on how geometry, topology,
and algebra can enrich the structure of the attention
coefficients and the attention layers.
Taxonomy: Figure 8 organizes attention coefficients
and layers into a taxonomy based on the mathemat-
ical properties of their inputs and outputs. First, the
attention coefficient α is computed from a query q
and a key k. Hence, we examine the structure of the
key k and the query q inputs, represented as signals
over a domain. For example, the traditional transformer
considers keys and queries as functions over a one-
dimensional domain R representing time. The output
attention coefficient α is represented as a signal over
the product domain: in the transformer example, it is
a function over the product domain R × R. Second,
the attention layer transforms input values v to output
values v′ through the attention coefficients α. Hence,
we look at the mathematical properties of v and v′,
both represented as signals over a domain. In the
classical transformers, they are signals over the time
domain R.
In Figure 8, we highlight the properties of the domains
and codomains of the key k, query q, coefficient
α, input value v and output value v′ signals. This
graphical taxonomy helps readers understand where
current methods fit within our framework and identify
potential areas for innovation. We survey the field row
by row in Figure 8.
Geometry in Attention Mechanisms.:
1) Euclidean signal on Euclidean domain. This
row illustrates the classical transformer (Vaswani
et al., 2017), with key, query, and input values
represented as signals k, q, v : R 7→Rn. Attention
coefficients are α : R2 7→R, while the layer
output values are v′
: R 7→Rn. This row
also illustrates the setting for the Vision Trans-
former (Dosovitskiy et al., 2021), which divides
an image into a sequence of image patches: hence,

22




	

	

 
­
 

	

 


 



 

 


 

 


 


 


 

 

 

 
























­čć 

 


 

­ 
Fig. 8: Topology, Geometry, and Algebra in Attention Mechanisms categorized according to the mathematical properties of the attention
coefficients (first subrow of each row) and of the attention layer (second subrow of each row). The key k and the query q are the inputs to
the attention coefficient α; the value v is the input to the attention layer, and the output value v′is the weighted result of that layer. Inputs
and outputs are represented as signals, i.e., as functions from a domain to a codomain. The black curved arrows represent the action of a
group on a signal’s domain or codomain. Notations: Rn: Euclidean space; M: Manifold; P: point set; G: graph; Ω: topological space.

23
patches in Rn over R. We highlight the difference
between the mathematical representation of these
signals, and their computational representation
during an actual implementation of the trans-
former. Mathematically, we represent the signals’
domain as the continuous real line R. Compu-
tationally, however, this real line is discretized
into T steps and associated T discrete tokens
(for either words or image patches). Yet, the
representation as the real line is useful to unify the
original transformer with the more complicated
layers introduced next.
2) Manifold signal on Euclidean domain for keys
and queries. Compared to the classical trans-
former of the previous row, this row introduces
geometry in the codomains of the keys and query
signals, which write k, q : R 7→M. Meanwhile,
the attention coefficients α, the input and output
values v, v′ have the same structure as in the
classic transformer. The multimanifold attention
mechanisms by (Konstantinidis et al., 2023) il-
lustrates this configuration. Specifically, this work
considers the classical attention coefficient α as
the computation of a Euclidean distance between
key and query. Accordingly, their proposed geo-
metric attention coefficient replaces the Euclidean
distance by a Riemannian geodesic distance be-
tween key and query, which are interpreted as
elements of a manifold: the manifold of SPD
(symmetric positive definite) matrices, the Grass-
mann manifold, or both—hence the term “multi”-
manifold. We note that the input data to this
transformer architecture is still Euclidean, since
this attention mechanism is proposed for images
in vision transformers. However, the way this data
is processed by the transformer’s internal layers is
non-Euclidean.
Equivariance in Attention Mechanisms.:
3) Euclidean signal on Euclidean domains for
keys, queries and values, with group action on
codomain. This row illustrates an attention mech-
anism that includes additional algebraic structure
in both keys, queries, input and output values.
The Geometric Algebra Transformer represents
this configuration Brehmer et al. (2023). Its layers
were designed to process “geometric data” de-
fined as scalars, vectors, lines, planes, objects and
their transformations (e.g., rotations) in 3D space.
Such geometric data is encoded into multivectors,
which are elements of the projective geometric al-
gebra also called the Clifford algebra. For simplic-
ity, we consider this space as a Euclidean space
Rn with algebraic structure. The attention coef-
ficients are group invariant, while the attention
layer is group equivariant, for the Euclidean group
E(3) of translations, rotations and reflections in
3D space. This configuration is also illustrated by
the Steerable Transformer (Kundu and Kondor,
2024) which processes Euclidean codomains Rn
with equivariance to the special Euclidean group
SE(n), for application to image processing and
machine learning tasks.
4) Euclidean signal on manifold domain for all
inputs / outputs, with domain group action.
Similar to the previous row, this row also intro-
duces geometric structure in the keys, queries,
values. In contrast to the previous row, however,
this layer brings geometry into the domains of the
signals: k, q, v, v′ whereas the above layer brought
geometry in their codomains. Consequently, for
this row, the attention coefficients α : M × M 7→
R have the product manifold M × M as their
domains=. This configuration is illustrated in the
Lie Transformer (Hutchinson et al., 2020), where
the manifolds of interest are Lie groups and their
subgroups. We note that the data processed by
this architecture does not have to belong to a Lie
group; only to be acted upon by a Lie group. A
lifting layer is introduced to convert the raw data
into Lie group elements, which are then handled
by the Lie Transformer. The attention layer is then
equivariant.
5) Euclidean signals on manifold domains for
all inputs / outputs. This row is similar to the
previous row, using manifold domains for keys,
queries, values, and attention coefficients. By con-
trast with the previous row, the group equivariance
is replaced by the concept of gauge equivariance
and invariance. The Gauge invariant transformer
in He et al. (2021) presents such configuration:
the attention coefficients are gauge invariant, and
the attention layer is gauge equivariant. This work
exclusively focuses on two-dimensional manifolds
M embedded in 3D Euclidean space.
Topology in Attention Mechanisms.:
6) Euclidean signal on set domain, with domain
group action: This row introduces topological
structure on the domains of the signals k, q, v, v′ :
P 7→Rn, through the set of points P. The Set
Transformer (ST) (Lee et al., 2019) and the Point
Cloud Transformer (PCT) (Guo et al., 2021) are
two examples of this configuration. They enjoy

24
equivariance to the group of permutations, which
acts by permuting the indexing of the points in
the set (resp., in the point cloud).
7) Manifold signals on set domains, with do-
main group action. This row includes geometric
structure on top of the topological structure of
the previous row, using manifold codomains for
keys k, queries q and Euclidean signals for v
and v′, all on set domains. The geodesic trans-
former (Li et al., 2022b) provides an architecture
that processes this configuration. Similar to the
Multi-Manifold Attention of row 2, the attention
coefficient of the geodesic transformer is com-
puted using a geodesic distance between keys and
queries: either a graph-based geodesic distance, or
a Riemannian geodesic distance on the so-called
oblique manifold.
8) Euclidean signals on graph domain, with do-
main group action. This row is similar to the
previous row, but without a group action on the
signals codomains. This is illustrated in the Graph
Attention Transformer (GAT) (Veliˇckovi´c et al.,
2018).
9) Euclidean signals on graph domains, with do-
main and codomain group actions. This row
also uses a graph domain G such that k, q, v, v′ :
G 7→Rn. An example of this configuration
is the SE(3)-Transformer, where the codomain
of these signals is additionally restricted to R3,
equipped with an action of the group of trans-
lations and rotations in 3D SE(3) (Fuchs et al.,
2020). This transformer was proposed to process
3D point clouds, and provides SE(3)-invariant
attention coefficients and SE(3)-equivariant at-
tention layer.
10) Euclidean signals on cellular complex domain,
with domain group action. Starting on this row,
more complicated topological structure is intro-
duced on the domains Ωof the signals. We refer
the reader to the survey (Papillon et al., 2023) for
a detailed exposition of the attention mechanisms
using topology. This row presents architectures
that specifically use a cellular complex or a sim-
plicial complex as the domain. Cell Attention Net-
works (CAN) (Giusti et al., 2023) and Simplicial
Graph Attention Network (SGAT) (Lee et al.,
2022) illustrate this type of attention mechanism,
while the Cellular Transformer (Ballester et al.,
2024) additionally include positional encodings.
11) Euclidean signals on hypergraph domain, with
domain group action. Similar to the previous
row, this configuration uses topological structure
on the domains of the signals: here, leverag-
ing hypergraphs. Examples of this configuration
include Dynamic Hypergraph Neural Networks
(DHGNN) (Jiang et al., 2019) and Hypergraph
Attention Networks (Hyper GAT) (Ding et al.,
2020).
12) Euclidean signals on combinatorial complex
domain, with domain group action. This final
row uses a combinatorial complex domain for the
signals defining inputs of the attention coefficients
and attention layers. A Higher Order Attention
Network (HOAN) architecture is a recent example
of this last category (Hajij et al., 2023b).
C. Benchmarks
We now turn to a brief review of the benchmarks that
have been considered in the non-Euclidean deep learn-
ing literature, compiling results from a broad sample
of neural networks with topological, geometric, and al-
gebraic layers in Table I, and highlighting the diversity
of tasks and datasets used in the literature.
Tasks and Datasets: We first observe that a wide
variety of task and benchmark datasets have been used
in the literature, with little overlap between models. In
other words, it is rare that two different models have
been benchmarked on the same dataset. This is not
surprising, since different models use different geomet-
ric, topological, and algebraic structures and different
structures are well suited for different tasks.
There are, however several benchmarks that appear
across models: MNIST and CIFAR for image clas-
sification, and Cora, Citeseer, and Pubmed for graph
classification. Many geometrical models are tested by
examining how well they model dynamical or physical
systems. These results are not easily comparable across
models, as the tasks are often customized for each
paper.
Number of parameters: A key benefit of building
mathematical structure into neural networks is that it
constrains the hypothesis search space. If the structure
is well matched to the problem, the model should
require fewer parameters and fewer computations.
Many papers mention this, but only a few report the
number of parameters (see right column of Table I).
As parameter and data efficiency are frequently cited
as advantages of building structure into neural network
models, we encourage authors to more regularly report

25
TABLE I: Applications and Benchmark of Neural Networks with Geometric, Topological and Algebraic Structures.
We organize models according to whether it uses attention and their geometric, topological and algebraic structure, with the abbreviations:
M: manifold, Gp: group, S: set, G: graph, Ω: topological domain, A: algebra. Models are also organized based on which task they perform,
and on which benchmark datasets. We include accuracies for benchmarks that two or more models use, converting test error to accuracy
when needed, along with standard error if reported. Model parameters are listed if the paper reports them. N. R. means Not Reported.
Model
Structure
Task
Benchmark datasets
# Params
Without Attention
Riemannian VAE
Miolane19
M
Dimension Reduction
Human Connectome Project (HCP)
N.R.
S-VAE/VGAE
Davidson18
M
Latent representation for image
classification and link prediction
MNIST (93.4± 0.2*), Cora (94.1±0.3),
Citeseer (95.2±0.2), Pubmed (96.0±0.1)
N.R.
SPDNet
Huang,VanGool16
M
Visual classification (emotion,
action, face)
AFEW, HDM05 and PaSC
N.R.
EMLP Finzi21
Gp
Dynamical modeling
Double pendulum
N.R.
LeNet-5 LeCun98
Gp
Image classification
MNIST (99.2±0.1)
N.R.
Steerable CNN
Cohen,Welling17
Gp
Image classification
CIFAR (10: 76.3; 10+: 96.4; 100+: 81.2)
4.4M
9.1M
G-CNN
Cohen,Welling16
Gp
Image classification
Rotated MNIST, CIFAR (10: 93.5; 10+:
95.1)
2.6M
G-CNN Cohen19a
Gp
Climate, pointcloud segmentation
Climate Segmentation, Stanford 2D-3D-S
N.R.
E(n)-EGNN
Satorras2021
Gp
Molecular property prediction,
dynamical modeling
QM9, N-body, Graph autoencoder
N.R.
PONITA Bekkers2024
Gp
Molecular property prediction and
generation, dynamical modeling
rMD17, QM9, N-body
N.R.
PointNet++ Qi17
S
Image, 3D, scene classification
MNIST (99.49), ModelNet40 (91.9),
SHREC15, ScanNet
1.7M
Tensor field network
Thomas18
S
3D-point-cloud prediction
QM9
N.R.
GCN Kipf,Welling17
G
Link prediction
Cora, Citeseer, Pubmed, NELL
N.R.
enn-s2s Gilmer17
G
Molecular property prediction
QM9
N.R.
SNN Ebli20
Ω
Coauthorship prediction
Semantic Scholar Open Research Corpus
N.R.
MPSN Bodnar21
Ω
Trajectory, graph classification
TUDataset
N.R.
CXN Hajij20
Ω
-
-
N.R.
HMPNN Heydari22
Ω
Citation node classification
Cora (92.2)
N.R.
CCNN Hajij23
Ω
Image segmentation, image, mesh,
graph classification
Human Body, COSEG, SHREC11
N.R.
E(n)-EMPSN
Eijkelboom2023
Ω
Molecular property prediction,
dynamical modeling
QM9, N-body
200K
Clifford-EMPSN
Liu2024
Ω
Pose estimation, dynamical
modeling
CMU MoCap, MD17
200K
E(n) Equivariant TNN
Battiloro2024
Ω
Molecular property, air pollution
prediction
QM9, Air Pollution Downscaling
1.5M
With Attention
Transformer Vaswani17
-
Machine translation
WMT 2014
N.R.
MMA ViT
Konstantinidis23
M
Image classification, segmentation
CIFAR (10: 94.7, 100+: 77.5),
T-ImageNet, ImageNet, ADE20K
3.9M
GATr Brehmer23
A
Dynamical modeling
N-body, artery stress, diffusion robotics
4.0M
Steerable Transformer
Kundu2024
Gp
Point-cloud, Image classification
Rotated MNIST (99.03), ModelNet10
(90.4)
0.9M
Lie Transformer
Hutchinson20
Gp
Regression, dynamics
QM9, ODE spring simulation
0.9M
GET He21
M
Shape classification, segmentation
SHREC07, Human Body Segmentation
0.15M
Set Transformer Lee19
S
Max value regression, clustering
Omniglot, CIFAR (100: 0.92±0.01
N.R.
PCT Guo21
S
Point-cloud classification,
regression, segmentation
ModelNet40 (93.2), ShapeNet (86.4),
S3DIS
1.4M
GSA Li22
S
Object classification, segmentation
ModelNet40 (93.3), ScanObjectNN,
ShapeNet (85.9)
18.5M
SE(3)-Transformer
Fuchs20
G
Dynamics, classification,
regression
N-body, ScanObjectNN, QM9
N.R.
GAT Veliˇckovi´c18
G
Link prediction
Cora, Citeseer, Pubmed, PPI
N.R.
CAN Giusti23
Ω
Graph classification
TUDataset
N.R.
Cellular Transformer
Ballester24
Ω
Graph classifical, Graph regression
GCB, Zinc, Ogbg Molhiv
N.R.
SGAT Lee22
Ω
Node classification
DBLP2, ACM, IMDB
N.R.
DHGNN Jiang19
Ω
Link, sentiment prediction
Cora (82.5), Microblog
0.13M
HyperGAT Ding20
Ω
Text classification
20NG, R8, R52, Ohsumed, MR
N.R.

26
parameter counts and computational cost in their pa-
pers along with performance metrics.
D. Summary
This concludes our review on non-Euclidean in deep
neural network layers. While a great diversity of
structures, layer types, and mechanisms have been
proposed, we hope that our illustrated taxonomy may
aid researchers in understanding their differences and
similarities, and in identifying opportunities for inno-
vation and application. We now turn to a brief summary
of the existing software libraries for non-Euclidean
machine learning made available online, and end with
an overview of the domains in which non-Euclidean
ML methods have been applied.
VI. NON-EUCLIDEAN SOFTWARE
In this section, we highlight publicly available software
libraries that make the methods of this field compu-
tationally accessible, organized in Table II. Here, we
limit our discussion to libraries whose commit history
suggests continued development and have a following
indicated by at least 50 Github Stars.
Table II highlight libraries core to the field. As shown
by the number of stars and actively developed reposi-
tories, packages for topological methods are the most
well developed, including important engineering foun-
dations such as CUDA and C++ accelerated network
primitives, and large collections of model implemen-
tations that continue to be maintained. The library
ecosystem for geometric learning methods is quickly
growing in interest and contributors, extending the
packages beyond optimizers over specific manifolds
to more general differential geometry tools. While the
packages for algebra in machine learning are the most
nascent, there have been exciting new developments
within the past few years in making more specialized
packages for accelerating group convolutions and other
algebraic operations as the need for more specialized
applications have emerged.
VII. APPLICATIONS OF NON-EUCLIDEAN
GEOMETRY
Many problems in science and engineering are in-
trinsically non-Euclidean and thus provide an exciting
opportunity for the application of non-Euclidean ML
methods. Here, we briefly highlight key developments
in selected application areas. We refer the reader to
Bronstein et al. (2021), Gaudelet et al. (2021), Li
et al. (2022a), Rajpurkar et al. (2022), Wang et al.
(2023), Wu et al. (2022, 2021) for more comprehensive
discussions of applications.
Chemistry and Drug Development
Within computational chemistry, molecular analysis
and design are traditionally lengthy and expensive
processes, sometimes requiring decades of develop-
ment time, and billions of dollars of investment and
infrastructure. Molecular data inherently has geometric
and graphical structure, well suited for non-Euclidean
approaches (Atz et al., 2021).
Graph neural networks have become a workhorse
for molecular analysis, treating molecules as graphs
with atoms as nodes and bonds as edges (Bronstein
et al., 2021, Gilmer et al., 2017). Progress in this
field has largely involved the construction of message-
passing neural networks with favorable properties,
such as equivariance to a growing family of group
transformations, novel forms of weight sharing, more
expressive primitives, and more efficient formulations
for parameterization and computation (Batzner et al.,
2022, Bekkers et al., 2024, Satorras et al., 2022, Sch¨utt
et al., 2017, Thomas et al., 2018). Deep networks
with geometric structure have also been used directly
for drug screening to discover new antibiotics (Stokes
et al., 2020).
Recently, deep equivariant generative modeling has
emerged as a powerful framework for molecule synthe-
sis. Prior work by (Gebauer et al., 2020, Simm et al.,
2020, Simonovsky and Komodakis, 2018) establishes
the importance of leveraging geometric properties for
the synthesis of molecules. (Hoogeboom et al., 2022)
introduces equivariant denoising diffusion models for
molecule generation by directly generating 3D atomic
coordinates, demonstrating improved quality and effi-
ciency. This was recently extended by (Xu et al., 2023)
to perform equivariant diffusion over a molecular latent
space, and by (Vignac et al., 2023) which achieves
much higher stability for generated molecules on the
GEOM-DRUGS dataset. Another line of work gener-
ates molecular invariants such as angles and distances,
which then are used to produce coordinates (Luo
and Ji, 2022). Recent work has also demonstrated
the importance of equivariances and invariances for
molecular conformer generation (Reidenbach and Kr-

27
ishnapriyan, 2023, Xu et al., 2022).
Structural Biology and Protein Engineering
Proteins are 1D sequences of amino acids that fold into
a 3D structure. The function of a protein is defined
by its structure, so the prediction of protein structure
from the original 1D sequence is crucial to the analysis
of their uses and interactions with other biomolecules.
AlphaFold 2, which incorporates a graph-structured
backbone, equivariant attention, and many geometric
priors about molecules (Jumper et al., 2021), has
emerged as a landmark paper for predicting protein
structure. Such protein structure predictions are com-
monly utilized for applications in biology and medicine
(Yang et al., 2023). Prediction of the interactions of
proteins with other molecules is also a challenging
problem well represented by graphs and equivariant
methods (Anand and Achim, 2022, Ingraham et al.,
2023, 2019).
Computer Vision
Computer vision entails the inference of properties of
the visual world from images or other measurements
such as LIDAR. There are many sub-tasks in computer
vision such as object recognition, semantic segmen-
tation, image and video generation, depth estimation,
etc. Historically, network primitives that capture the
topological structure and symmetries of images have
dominated vision benchmarks, including CNNs, GC-
NNs, and Vision transformers (Cohen and Welling,
2016, Dosovitskiy et al., 2021, Krizhevsky et al., 2012,
LeCun et al., 1998).
Recently, one of the most successful applications
of non-Euclidean deep learning has been that of
graph neural networks, implemented on data native
to or lifted to point-clouds. Pioneering works such
as Pointnet++ and PointTransformer introduced graph-
structured deep networks as breakthrough methods
in 3D semantic segmentation and object detection at
whole-room scales (Engel et al., 2020, Qi et al., 2017).
Another promising computational primitive, Slot At-
tention, introduces a novel messaging-passing strat-
egy to perform unsupervised object discovery using
permutation invariant slots, which incorporates spatial
symmetries using slot-centric reference frames (Biza
et al., 2023, Locatello et al., 2020).
Biomedical Imaging
Biomedical imaging involves inferring the structure of
biological tissues from measurements of their physical
properties, typically in the form of electromagnetic
fields, acoustic waves, and other physical phenomena.
Quantities of interest include shape, composition, or in-
ternal state. Thus, geometric and topological structure
play an important role in their analysis.
Many machine learning problems in medical imaging
require reasoning about 3D structures, including their
shape, their variations throughout a population, and
changes throughout time. As tissue states are non-
Euclidean, their statistics and evolution require a ge-
ometric treatment (Pennec et al., 2019). Variations
in organ shapes lie on low-dimensional manifolds,
and geometry-aware dimensionality-reduction methods
such as tangent PCA (Boisvert et al., 2008) or Principle
Geodesic Analysis (PGA) enable meaningful repre-
sentations for downstream tasks (Fletcher and Joshi,
2007, Fletcher et al., 2004, Hinkle et al., 2012a).
Geometric methods have been applied the analysis of
the effects of aging in the corpus collosum with MRI
scans (Hinkle et al., 2012b), to brain connectomics data
in Diffusion Tensor Imaging (Pennec et al., 2006), and
to the segmentation of 3D anatomical structures from
CT and MRI scans in the lateral cerebral ventricle, the
kidney parenchyma and pelvis, and the hippocampus
(Pizer et al., 2003).
Medical image registration has historically steered
the need for the development of statistics on Lie
groups from the finite-dimensional group SE(3) (Pen-
nec, 2006, Pennec and Thirion, 1997) to the infinite-
dimensional group of diffeomorphisms (Trouv´e, 1998,
Younes, 2010). Unfortunately, any attempt to rely on
Riemannian geometric structures for machine learning
on Lie groups is generally ill-posed. Specifically, clas-
sical Riemannian metrics on Lie groups, called left
or right invariant, generally fail to be invariant by all
the group operations, notably inversion. This triggered
the search for alternative geometric structures (Miolane
and Pennec, 2015, Pennec and Arsigny, 2012) and
the replacement of the (non bi-invariant) Riemannian
metric by the canonical bi-invariant symmetric Cartan-
Shouten connection Pennec and Lorenzi (2020). In
infinite dimensions, this grounds the parametrization
of diffeomorphisms by the flow of Stationary Velocity
Fields (SVFs). This geometrically grounded statistical
framework on Lie groups is now used in numerous
non-linear medical image registration algorithms (Ar-
signy et al., 2006, Lorenzi and Pennec, 2013) and to
model shape evolution such as the brain changes in
Alzheimer’s disease (Hadj-Hamou et al., 2016, Lorenzi
et al., 2015, Sivera et al., 2019).
In medical image segmentation, estimation errors on
the order of a few pixels can lead to significant mis-
interpretations. The domain has recently seen benefits

28
Geometry
Packages
Domains
Core Features
Stars
GeomStats 2019
Manifolds, Lie Groups, Fiber Bundles,
Shape Spaces, Information Manifolds,
Graphs
Manifold operations, Algorithms, Statistics, Optimizers
1.2k
GeoOpt 2020
Manifolds
Layers, Manifold operations, Stochastic optimizers for
deep learning
812
PyManOpt 2016
Manifolds, Lie Groups
Manifold operations, Optimizers
734
PyRiemann 2023
SPD Matricies
Machine Learning, Data Analysis for biosignals
606
Topology
Packages
Domains
Core Features
Stars
PytorchGeometric 2019
Graphs
Baseline Models, Layers, Fast Basic Graph Operations,
Datasets, Dataloaders
20.6k
NetworkX Hagberg et al.
(2008)
Graphs, Digraphs, Multigraphs
Data structures, Graph generators, Graph Algorithms,
Network Analysis Measures
14.5k
DGL 2019
Graphs
Baseline Models, Layers, Fast Basic Graph Operations,
Datasets, Dataloaders, Framework-agnostic (PyTorch,
Tensorflow, etc. are swappable)
13.2k
DIG 2021
Graphs
Baseline models, Datasets, Evaluation Metrics
1.8k
AutoGL 2021
Graphs
Neural Architecture Search, Hyper-Parameter Tuning,
Ensembles
1.1k
HyperNetX 2023
Hypergraphs
Machine Learning Algorithms, Analysis, Visualization
502
DHG 2022
Graphs, hypergraphs, bipartite graphs,
hypergraphs, directed hypergraphs, ...
Models, Basic Operations, Dataloaders, Visualization,
Auto ML, Metrics, Graph generators
566
TopoModelX 2024
Graphs,
colored
hypergraphs,
com-
plexes
Baseline Models, Layers, Higher-order message passing
205
TopoNetX 2024
Graphs,
colored
hypergraphs,
com-
plexes
Topography Generators, Computing topological proper-
ties, Arbitrary cell attributes
168
TopoEmbedX 2024
Graphs,
colored
hypergraphs,
com-
plexes
Representation learning, embeddings
72
TopoBenchmarkX 2024
Graphs, hypergraphs, complexes
Benchmarks, lifting, dataloaders, losses, training frame-
work
54
XGI 2023
Hypergraphs,
directed
hypergraphs,
symplical complexes
Graph generators, metrics, algorithms, dataloaders, vi-
sualization
172
Algebra
Packages
Domains
Core Features
Stars
E3NN 2022
E(3) Equivariant Feature Fields
Group Convolutions, Steerable Group Convolutions
891
ESCNN & E2CNN 2022
E(n) Equivariant Feature Fields, Graphs
Group Convolutions, Steerable Group Convolutions
584 1
NequIP 2022
E(3) Equivariance on Graphs
Group Convolutions, Steerable Group Convolutions
538
EMLP 2021
Matrix Groups, Tensors, Irreducible
Representations, Induced Representa-
tions
Programmatic generation of equivariant MLPs for arbi-
trary matrix groups in JAX
249
PyQuaternion
Quaternions
Quaternion operations, rotation representation conver-
sions, differentiation, integration
336
TABLE II: Software Packages for Machine Learning with Topology, Geometry, and Algebra. We organize packages according to the
mathematical, non-Euclidean structures they focus on.

29
from the addition of topological structure. Examples of
topological constraints that improve consistency and
accuracy include preserving membrane connectivity
for cell images and incorporating anatomically correct
relative position of organs (Clough et al., 2022, Gupta
et al., 2022, Hu et al., 2019, 2021).
Cryo-EM data provide multiple 2D image projections
of a 3D biomolecule from different angles. The task
of inferring the original 3D structure of a biomolecule
benefits greatly from the incorporation of topological
and geometric structure (Donnat et al., 2022, Miolane
et al., 2020).
Histology images have no canonical orientation, mean-
ing each orientation of a particular cell is equally likely
to appear. Recent works have explored equivariant
and steerable CNNs to leverage these symmetries in
the data (Adnan et al., 2020, Bekkers et al., 2018,
Graham et al., 2020, Lafarge et al., 2020, Veeling et al.,
2018)
Recommender Systems and Social Networks
Recommender systems were one of the first successful
applications of graph representation learning, where
the proximity of a particular node to other nodes can
be learned through graph convolutions to represent
similarity. This has clear economic utility for recom-
mending advertisements, products, music, and content
based on the user’s data and preferences. PinSage from
Pinterest was one of the first to demonstrate their
potential at a commercial scale (Ying et al., 2018). See
(Wu et al., 2022) for a comprehensive review on graph
neural networks in recommender systems.
Physics
Physics data naturally has many symmetries and often
takes the form of relations between unordered sets, an
ideal setting for topological and equivariant methods.
Dynamics between particles or nodes in a mesh can
be effectively computed using learned graph message
passing for various types of physics data (Pfaff et al.,
2021, Sanchez-Gonzalez et al., 2020). Equivariant
transformer and graph neural network architectures
have been successfully applied to data analysis for
the Large Hadron Collider and other simulations such
as gravity for the n-body problem (Brandstetter et al.,
2022, Fuchs et al., 2020). Topological methods are well
suited to process the hundreds of petabytes of highly
relational data produced by experiments from the Large
Hadron Collider and have demonstrated their utility for
the next stage of fundamental discoveries in particle
physics (DeZoort et al., 2023). Recent work (Brehmer
et al., 2023) proposes an architecture for embedding
geometric data into a Geometric Algebra representa-
tion to be processed by an equivariant Transformer
network, and demonstrates its efficacy on mesh inter-
action estimation and n-body simulation. Astrophysics
data is also well suited for application of equivariant
networks. Some examples include the classification of
radio galaxies using group-equivariant CNNs (Scaife
and Porter, 2021), optimal cosmological analysis using
equivariant normalizing flows (Dai and Seljak, 2022),
and cosmic microwave background radiation analysis
using spherical equivariant CNNs (McEwen et al.,
2022).
Other Applications
There are many other interesting applications of non-
Euclidean methods to domains that do not neatly
fall into these categories, such as GNNs for arrival
time forecasting (Derrow-Pinion et al., 2021), weather
forecasting (Keisler, 2022, Lam et al., 2023), materials
science (Reiser et al., 2022), patient electronic health
records (Li et al., 2022a), and many more. As a bur-
geoning young field, the application of non-Euclidean
machine learning to novel domains is constantly ad-
vancing. We expect (and hope) that the highlights
presented here will represent only a small fraction of
what is to come.
VIII. CONCLUSION
As the availability of richly structured, non-Euclidean
data grows across application domains, there is an
increasing need for machine learning methods that
can fully leverage the underlying geometry, topology,
and symmetries to extract insights. Driven by this
need, a new paradigm of non-Euclidean machine learn-
ing is emerging that generalizes classical techniques
to curved manifolds, topological spaces, and group-
structured data. This paradigm shift echoes the non-
Euclidean revolution in mathematics in the 19th cen-
tury, which radically expanded our notion of geometry
and catalyzed significant advancements across the nat-
ural sciences.
In this review, we have provided an accessible
overview of this emerging field, unifying disparate
threads in the literature into a common organizational
framework. Our illustrated taxonomy contextualizes,
classifies, and differentiates existing approaches and
illuminates gaps that present opportunities for inno-
vation. We hope this serves as an invitation for both
theoreticians and practitioners to further explore the
potential of non-Euclidean geometry, topology, and
algebra to reshape modern machine learning, just as

30
they reshaped our fundamental understanding of space
over a century ago.
By empowering models with the tools to respect the
underlying structure of data, non-Euclidean machine
learning greatly expands the frontiers of the types of
learning problems that can be tackled. With a growing
theoretical understanding and the design of architec-
tures that fully leverage the mathematical formalisms
of geometry, topology and algebra, non-Euclidean
approaches hold immense potential to transform the
broader machine learning landscape and its application
to engineering problems and the natural sciences in the
coming years.
REFERENCES
Mohammed Adnan, Shivam Kalra, and Hamid R. Tizhoosh. Rep-
resentation learning of histopathology images using graph neural
networks, 2020.
Morten Akhøj, James Benn, Erlend Grong, Stefan Sommer,
and Xavier Pennec.
Principal subbundles for dimension re-
duction.
working paper or preprint, July 2023.
URL
https://inria.hal.science/hal-04156036.
Namrata Anand and Tudor Achim. Protein structure and sequence
generation with equivariant denoising diffusion probabilistic mod-
els, 2022.
Vincent Arsigny, Olivier Commowick, Xavier Pennec, and Nicholas
Ayache. A Log-Euclidean Framework for Statistics on Diffeo-
morphisms.
In Proc. of the 9th International Conference on
Medical Image Computing and Computer Assisted Intervention
(MICCAI’06), Part I, LNCS, pages 924–931, October 2006. doi:
10.1007/11866565 113.
Devanshu Arya, Richard Olij, Deepak K. Gupta, Ahmed El Gazzar,
Guido van Wingen, Marcel Worring, and Rajat Mani Thomas.
Fusing structural and functional mris using graph convolutional
networks for autism classification. In Tal Arbel, Ismail Ben Ayed,
Marleen de Bruijne, Maxime Descoteaux, Herve Lombaert, and
Christopher Pal, editors, Proceedings of the Third Conference on
Medical Imaging with Deep Learning, volume 121 of Proceed-
ings of Machine Learning Research, pages 44–61. PMLR, 06–08
Jul 2020. URL https://proceedings.mlr.press/v121/arya20a.html.
Kenneth Atz, Francesca Grisoni, and Gisbert Schneider. Geometric
deep learning on molecular representations, 2021.
Rub´en Ballester, Pablo Hern´andez-Garc´ıa, Mathilde Papillon, Clau-
dio Battiloro, Nina Miolane, Tolga Birdal, Carles Casacu-
berta, Sergio Escalera, and Mustafa Hajij.
Attending to
topological spaces: The cellular transformer.
arXiv preprint
arXiv:2405.14094, 2024.
Monami Banerjee, Rudrasis Chakraborty, Edward Ofori, David Vail-
lancourt, and Baba C. Vemuri. Nonlinear Regression on Rieman-
nian Manifolds and Its Applications to Neuro-Image Analysis,
page 719–727. Springer International Publishing, 2015. ISBN
9783319245539.
doi: 10.1007/978-3-319-24553-9 88.
URL
http://dx.doi.org/10.1007/978-3-319-24553-9 88.
Alexandre Barachant, Quentin Barth´elemy, Jean-R´emi King, Alexan-
dre Gramfort, Sylvain Chevallier, Pedro L. C. Rodrigues,
Emanuele
Olivetti,
Vladislav
Goncharenko,
Gabriel
Wag-
ner
vom
Berg,
Ghiles
Reguig,
Arthur
Lebeurrier,
Erik
Bj¨areholt, Maria Sayu Yamamoto, Pierre Clisson, and Marie-
Constance Corsi.
pyriemann/pyriemann: v0.5, 2023.
URL
https://doi.org/10.5281/zenodo.8059038.
Claudio Battiloro, Ege Karaismailo˘glu, Mauricio Tec, George
Dasoulas, Michelle Audirac, and Francesca Dominici.
E
(n) equivariant topological neural networks.
arXiv preprint
arXiv:2405.15429, 2024.
Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger,
Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari,
Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural
networks for data-efficient and accurate interatomic potentials.
Nature communications, 13(1):2453, 2022.
Erik J Bekkers, Maxime W Lafarge, Mitko Veta, Koen AJ Eppenhof,
Josien PW Pluim, and Remco Duits. Roto-translation covariant
convolutional networks for medical image analysis, 2018.
Erik J Bekkers, Sharvaree Vadgama, Rob D Hesselink, Putri A
van der Linden, and David W Romero.
Fast, expressive
se(n) equivariant networks through weight-sharing in position-
orientation space, 2024.
Andri Bergsson and Søren Hauberg. Visualizing riemannian data
with rie-sne. arXiv preprint arXiv:2203.09253, 2022.
Charles
M.
Bishop.
Bayesian
pca.
In
Neural
Information
Processing
Systems,
1998.
URL
https://api.semanticscholar.org/CorpusID:43329106.
Ondrej
Biza,
Sjoerd
van
Steenkiste,
Mehdi
S.
M.
Sajjadi,
Gamaleldin F. Elsayed, Aravindh Mahendran, and Thomas Kipf.
Invariant slot attention: Object discovery with slot-centric refer-
ence frames, 2023.
Cristian Bodnar. Topological Deep Learning: Graphs, Complexes,
Sheaves. PhD thesis, 2023.
Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter,
Guido F Montufar, Pietro Li´o, and Michael Bronstein.
We-
isfeiler and lehman go topological: Message passing simpli-
cial networks.
In Marina Meila and Tong Zhang, editors,
Proceedings of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning
Research, pages 1026–1037. PMLR, 18–24 Jul 2021.
URL
https://proceedings.mlr.press/v139/bodnar21a.html.
Jonathan Boisvert, Farida Cheriet, Xavier Pennec, Hubert Labelle,
and Nicholas Ayache. Geometric Variability of the Scoliotic Spine
using Statistics on Articulated Shape Models. IEEE Transactions
on Medical Imaging, 27(4):557–568, 2008. doi: 10.1109/TMI.
2007.911474.
G. E. P. Box and G. C. Tiao.
A bayesian approach to
some outlier problems.
Biometrika, 55(1):119–129, 1968.
ISSN
1464-3510.
doi:
10.1093/biomet/55.1.119.
URL
http://dx.doi.org/10.1093/biomet/55.1.119.
Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J
Bekkers, and Max Welling. Geometric and physical quantities
improve e(3) equivariant message passing, 2022.
Johann Brehmer, Pim De Haan, S¨onke Behrends, and Taco Co-
hen.
Geometric algebra transformer.
In Thirty-seventh Con-
ference on Neural Information Processing Systems, 2023. URL
https://openreview.net/forum?id=M7r2CO4tJC.
Leo Breiman.
Machine Learning, 45(1):5–32, 2001.
ISSN
0885-6125.
doi:
10.1023/a:1010933404324.
URL
http://dx.doi.org/10.1023/A:1010933404324.
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c.
Geometric deep learning: Grids, groups, graphs, geodesics, and
gauges. arXiv preprint arXiv:2104.13478, 2021.
Gabriele Cesa, Leon Lang, and Maurice Weiler. A program to build
E(N)-equivariant steerable CNNs. In International Conference on
Learning Representations (ICLR), 2022.
Rudrasis Chakraborty, Jose Bouza, Jonathan H. Manton, and
Baba
C.
Vemuri.
ManifoldNet:
A
Deep
Neural
Net-
work for Manifold-Valued Data With Applications.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
44(2):799–810,
February
2022.
ISSN
0162-8828,
2160-
9292, 1939-3539.
doi: 10.1109/TPAMI.2020.3003846.
URL
https://ieeexplore.ieee.org/document/9122448/.
Kui Yu Chang and Joydeep Ghosh. A unified model for probabilistic
principal surfaces. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 23(1):22–41, 2001. ISSN 01628828. doi:
10.1109/34.899944.
Nikolai Chernov.
Circular and Linear Regression: Fitting Cir-
cles and Lines by Least Squares.
CRC Press, June 2010.
ISBN 9780429151415.
doi: 10.1201/ebk1439835906.
URL

31
http://dx.doi.org/10.1201/EBK1439835906.
James R. Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A. Zim-
mer, Julia A. Schnabel, and Andrew P. King.
A topological
loss function for deep-learning based image segmentation using
persistent homology.
IEEE Transactions on Pattern Analysis
and Machine Intelligence, 44(12):8766–8778, December 2022.
ISSN 1939-3539.
doi: 10.1109/tpami.2020.3013679.
URL
http://dx.doi.org/10.1109/TPAMI.2020.3013679.
Taco Cohen and Max Welling.
Group equivariant convolu-
tional networks.
In Maria Florina Balcan and Kilian Q.
Weinberger, editors, Proceedings of The 33rd International
Conference on Machine Learning, volume 48 of Proceed-
ings of Machine Learning Research, pages 2990–2999, New
York, New York, USA, 20–22 Jun 2016. PMLR.
URL
https://proceedings.mlr.press/v48/cohenc16.html.
Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling.
Gauge equivariant convolutional networks and the icosahedral
cnn.
In International conference on Machine learning, pages
1321–1330. PMLR, 2019a.
Taco Cohen et al. Equivariant convolutional networks. PhD thesis,
Taco Cohen, 2021.
Taco S. Cohen and Max Welling.
Steerable CNNs.
In Inter-
national Conference on Learning Representations, 2017. URL
https://openreview.net/forum?id=rJQKYt5ll.
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory
of equivariant cnns on homogeneous spaces. Advances in neural
information processing systems, 32, 2019b.
Biwei Dai and Uroˇs Seljak.
Translation and rotation equivariant
normalizing flow (trenf) for optimal cosmological analysis. 2022.
URL https://doi.org/10.1093/mnras/stac2010.
James Damon and J. S. Marron. Backwards Principal Component
Analysis and Principal Nested Relations. Journal of Mathematical
Imaging and Vision, 50(1-2):107–114, October 2013.
ISSN
0924-9907, 1573-7683. doi: 10.1007/s10851-013-0463-2. URL
http://link.springer.com/article/10.1007/s10851-013-0463-2.
Tim R. Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and
Jakub M. Tomczak.
Hyperspherical variational auto-encoders.
34th Conference on Uncertainty in Artificial Intelligence 2018,
UAI 2018, 2:856–865, 2018.
Brad C. Davis, P. Thomas Fletcher, Elizabeth Bullitt, and Sarang
Joshi.
Population shape regression from random design data.
International Journal of Computer Vision, 90(2):255–266, jul
2010. ISSN 1573-1405. doi: 10.1007/s11263-010-0367-1. URL
http://dx.doi.org/10.1007/s11263-010-0367-1.
Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange,
Todd Hester, Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying
Guo, Brett Wiltshire, Peter W. Battaglia, Vishal Gupta, Ang
Li, Zhongwen Xu, Alvaro Sanchez-Gonzalez, Yujia Li, and
Petar Velickovic. Eta prediction with graph neural networks in
google maps.
In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management, CIKM
’21. ACM, October 2021. doi: 10.1145/3459637.3481916. URL
http://dx.doi.org/10.1145/3459637.3481916.
Gage DeZoort, Peter W Battaglia, Catherine Biscarat, and Jean-Roch
Vlimant.
Graph neural networks at the large hadron collider.
Nature Reviews Physics, 5(5):281–303, 2023.
Kaize Ding, Jianling Wang, Jundong Li, Dingcheng Li, and
Huan Liu.
Be more with less: Hypergraph attention networks
for inductive text classification.
In Conference on Empiri-
cal Methods in Natural Language Processing, 2020.
URL
https://api.semanticscholar.org/CorpusID:226226607.
Claire Donnat, Axel Levy, Frederic Poitevin, Ellen Zhong, and Nina
Miolane. Deep generative modeling for volume reconstruction in
cryo-electron microscopy, 2022.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-
senborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby.
An image is worth 16x16 words:
Transformers for image recognition at scale, 2021.
Stefania Ebli, Micha¨el Defferrard, and Gard Spreemann.
Sim-
plicial neural networks.
In TDA & Beyond, 2020.
URL
https://openreview.net/forum?id=nPCt39DVIfk.
Floor Eijkelboom, Rob Hesselink, and Erik J Bekkers.
E (n)
equivariant message passing simplicial networks. In International
Conference on Machine Learning, pages 9071–9081. PMLR,
2023.
Nico Engel, Vasileios Belagiannis, and Klaus C. J. Dietmayer.
Point transformer. IEEE Access, 9:134826–134840, 2020. URL
https://api.semanticscholar.org/CorpusID:226227046.
Luca Falorsi, Pim de Haan, Tim R. Davidson, Nicola De Cao,
Maurice Weiler, Patrick Forr´e, and Taco S. Cohen. Explorations
in Homeomorphic Variational Auto-Encoding.
2018.
URL
http://arxiv.org/abs/1807.04689.
Jianqing Fan.
Local linear regression smoothers and their min-
imax efficiencies.
The Annals of Statistics, 21(1), March
1993.
ISSN 0090-5364.
doi: 10.1214/aos/1176349022.
URL
http://dx.doi.org/10.1214/aos/1176349022.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning
with PyTorch Geometric. In ICLR Workshop on Representation
Learning on Graphs and Manifolds, 2019.
Marc Finzi, Max Welling, and Andrew Gordon Gordon Wilson. A
practical method for constructing equivariant multilayer percep-
trons for arbitrary matrix groups.
In Marina Meila and Tong
Zhang, editors, Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of Machine
Learning Research, pages 3318–3328. PMLR, 18–24 Jul 2021.
URL https://proceedings.mlr.press/v139/finzi21a.html.
R. A. Fisher.
The use of multiple measurements in taxonomic
problems. Annals of Eugenics, 7(7):179–188, 1936.
P. Thomas Fletcher and Sarang Joshi.
Riemannian geometry
for the statistical analysis of diffusion tensor data.
Signal
Processing,
87(2):250–262,
2007.
ISSN
0165-1684.
doi:
https://doi.org/10.1016/j.sigpro.2005.12.018.
URL
https://www.sciencedirect.com/science/article/pii/S0165168406001691.
P Thomas Fletcher, Conglin Lu, Stephen M Pizer, and Sarang Joshi.
Principal geodesic analysis for the study of nonlinear statistics of
shape. IEEE transactions on medical imaging, 23(8):995–1005,
2004.
Thomas Fletcher.
Geodesic regression on riemannian manifolds.
In Proceedings of the Third International Workshop on Mathe-
matical Foundations of Computational Anatomy-Geometrical and
Statistical Methods for Modelling Biological Shape Variability,
pages 75–86, 2011.
Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling.
Se(3)-transformers:
3d
roto-translation
equivariant
attention
networks.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F.
Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 1970–1981. Curran
Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper
files/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf.
Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. Hgnn: General
hypergraph neural networks.
IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2022.
Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman,
Cristian Regep, Gertrude Liu, Jeremy B R Hayter, Richard Vick-
ers, Charles Roberts, Jian Tang, David Roblin, Tom L Blundell,
Michael M Bronstein, and Jake P Taylor-King. Utilizing graph
machine learning within drug discovery and development. 2021.
URL https://doi.org/10.1093/bib/bbab159.
F Gay-Balmaz, DD Holm, DM Meier, TS Ratiu, and F-X
Vialard.
Invariant
Higher-Order
Variational
Problems.
Communications
in
Mathematical
Physics,
309:413–
458,
2012.
doi:
10.1007/s00220-011-1313-y.
URL
http://dx.doi.org/10.1007/s00220-011-1313-y.
Niklas W. A. Gebauer, Michael Gastegger, and Kristof T. Sch¨utt.
Symmetry-adapted generation of 3d point sets for the targeted
discovery of molecules, 2020.
Mario Geiger, Tess Smidt, Alby M., Benjamin Kurt Miller,
Wouter Boomsma, Bradley Dice, Kostiantyn Lapchevskyi, Mau-
rice Weiler, Michał Tyszkiewicz, Simon Batzner, Dylan Madis-

32
etti, Martin Uhrin, Jes Frellsen, Nuri Jung, Sophia Sanborn,
Mingjian Wen, Josh Rackers, Marcel Rød, and Michael Bai-
ley.
Euclidean neural networks: e3nn, April 2022.
URL
https://doi.org/10.5281/zenodo.6459381.
J.D. Gergonne. Application de la m´ethode des moindres carr´es `a
l’interpolation des suites. Annales des Math´ematiques Pures et
Appliqu´ees, 6:242–252, 1815.
Jeff Gill and Dominik Hangartner.
Circular data in political sci-
ence and how to handle it.
Political Analysis, 18(3):316–336,
2010.
ISSN 1476-4989.
doi: 10.1093/pan/mpq009.
URL
http://dx.doi.org/10.1093/pan/mpq009.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals,
and George E. Dahl.
Neural message passing for quantum
chemistry.
In Doina Precup and Yee Whye Teh, editors,
Proceedings of the 34th International Conference on Machine
Learning, volume 70 of Proceedings of Machine Learning Re-
search, pages 1263–1272. PMLR, 06–11 Aug 2017.
URL
https://proceedings.mlr.press/v70/gilmer17a.html.
Lorenzo
Giusti,
Claudio
Battiloro,
Lucia
Testa,
Paolo
Di
Lorenzo,
Stefania
Sardellitti,
and
Sergio
Barbarossa.
Cell
attention
networks.
In
2023
International
Joint
Conference
on
Neural
Networks
(IJCNN).
IEEE,
June
2023.
doi:
10.1109/ijcnn54540.2023.10191530.
URL
http://dx.doi.org/10.1109/IJCNN54540.2023.10191530.
Simon Graham, David Epstein, and Nasir Rajpoot. Dense steerable
filter cnns for exploiting rotational symmetry in histology images,
2020.
Chaoyu Guan, Ziwei Zhang, Haoyang Li, Heng Chang, Zeyang
Zhang, Yijian Qin, Jiyan Jiang, Xin Wang, and Wenwu Zhu.
AutoGL: A library for automated graph learning. In ICLR 2021
Workshop on Geometrical and Topological Representation Learn-
ing, 2021. URL https://openreview.net/forum?id=0yHwpLeInDn.
Nicolas Guigui, Nina Miolane, Xavier Pennec, et al. Introduction to
riemannian geometry and geometric statistics: from basic theory
to implementation with geomstats. Foundations and Trends® in
Machine Learning, 16(3):329–493, 2023.
Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu,
Ralph R. Martin, and Shi-Min Hu.
Pct: Point cloud trans-
former. Computational Visual Media, 7(2):187–199, April 2021.
ISSN 2096-0662.
doi: 10.1007/s41095-021-0229-5.
URL
http://dx.doi.org/10.1007/s41095-021-0229-5.
Mengmeng Guo, Jingyong Su, Li Sun, and Guofeng Cao.
Sta-
tistical regression analysis of functional and shape data. Jour-
nal of Applied Statistics, 47(1):28–44, September 2019.
ISSN
1360-0532.
doi: 10.1080/02664763.2019.1669541.
URL
http://dx.doi.org/10.1080/02664763.2019.1669541.
Saumya Gupta, Xiaoling Hu, James Kaan, Michael Jin, Mutshipay
Mpoy, Katherine Chung, Gagandeep Singh, Mary Saltz, Tahsin
Kurc, Joel Saltz, Apostolos Tassiopoulos, Prateek Prasanna, and
Chao Chen.
Learning topological interactions for multi-class
medical image segmentation, 2022.
Mehdi Hadj-Hamou, Marco Lorenzi, Nicholas Ayache, and Xavier
Pennec.
Longitudinal Analysis of Image Time Series with
Diffeomorphic
Deformations:
A
Computational
Framework
Based on Stationary Velocity Fields. Frontiers in Neuroscience,
10, 2016. ISSN 1662-453X. doi: 10.3389/fnins.2016.00236. URL
https://www.frontiersin.org/articles/10.3389/fnins.2016.00236/full.
Aric Hagberg, Pieter J Swart, and Daniel A Schult. Exploring net-
work structure, dynamics, and function using networkx. Technical
report, Los Alamos National Laboratory (LANL), Los Alamos,
NM (United States), 2008.
Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi.
Cell com-
plex neural networks.
In TDA & Beyond, 2020.
URL
https://openreview.net/forum?id=6Tq18ySFpGU.
Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Nina Mi-
olane, Aldo Guzm´an-S´aenz, Karthikeyan Natesan Ramamurthy,
Tolga Birdal, Tamal K Dey, Soham Mukherjee, Shreyas N Sam-
aga, et al. Topological deep learning: Going beyond graph data.
2023a.
Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Nina Mi-
olane, Aldo Guzm´an-S´aenz, Karthikeyan Natesan Ramamurthy,
Tolga Birdal, Tamal K. Dey, Soham Mukherjee, Shreyas N.
Samaga, Neal Livesay, Robin Walters, Paul Rosen, and Michael T.
Schaub. Topological deep learning: Going beyond graph data,
2023b.
Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Ager-
berg, Ibrahem AlJabea, Ruben Ballester, Claudio Battiloro,
Guillermo Bern´ardez, Tolga Birdal, Aiden Brent, Peter Chin,
Sergio Escalera, Simone Fiorellino, Odin Hoff Gardaa, Gu-
rusankar Gopalakrishnan, Devendra Govil, Josef Hoppe, Ma-
neel Reddy Karri, Jude Khouja, Manuel Lecha, Neal Livesay, Jan
Meißner, Soham Mukherjee, Alexander Nikitin, Theodore Papa-
markou, Jaro Pr´ılepok, Karthikeyan Natesan Ramamurthy, Paul
Rosen, Aldo Guzm´an-S´aenz, Alessandro Salatiello, Shreyas N.
Samaga, Simone Scardapane, Michael T. Schaub, Luca Sco-
fano, Indro Spinelli, Lev Telyatnikov, Quang Truong, Robin
Walters, Maosheng Yang, Olga Zaghen, Ghada Zamzmi, Ali
Zia, and Nina Miolane.
Topox: A suite of python packages
for machine learning on topological domains, 2024.
URL
https://arxiv.org/abs/2402.02441.
Elkan
F.
Halpern.
Polynomial
regression
from
a
bayesian
approach.
Journal
of
the
American
Statistical
Association, 68(341):137–143, March 1973.
ISSN 1537-
274X.
doi:
10.1080/01621459.1973.10481352.
URL
http://dx.doi.org/10.1080/01621459.1973.10481352.
Martin Hanik, Hans-Christian Hege, Anja Hennemuth, and Christoph
von Tycowicz. Nonlinear regression on manifolds for shape anal-
ysis using intrinsic b´ezier splines. In International Conference on
Medical Image Computing and Computer-Assisted Intervention,
2020. URL https://api.semanticscholar.org/CorpusID:220486926.
Trevor Hastie and Werner Stuetzle. Principal Curves. Journal of the
American Statistical Association, 84(406):502–516, 1989.
Søren Hauberg. Principal Curves on Riemannian Manifolds. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 38
(9):1915–1921, 2016.
ISSN 01628828.
doi: 10.1109/TPAMI.
2015.2496166.
Lingshen He, Yiming Dong, Yisen Wang, Dacheng Tao, and
Zhouchen Lin. Gauge equivariant transformer. In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan, editors, Advances in Neural Information Processing
Systems, volume 34, pages 27331–27343. Curran Associates,
Inc.,
2021.
URL
https://proceedings.neurips.cc/paper
files/paper/2021/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf.
Sajjad Heydari and Lorenzo Livi. Message passing neural networks
for hypergraphs. In Elias Pimenidis, Plamen Angelov, Chrisina
Jayne, Antonios Papaleonidas, and Mehmet Aydin, editors, Ar-
tificial Neural Networks and Machine Learning – ICANN 2022,
pages 583–592, Cham, 2022. Springer Nature Switzerland.
Jacob Hinkle, Prasanna Muralidharan, P. Fletcher, and Sarang Joshi.
Polynomial regression on riemannian manifolds. 7574, 10 2012a.
doi: 10.1007/978-3-642-33712-3 1.
Jacob Hinkle, Prasanna Muralidharan, P Thomas Fletcher, and
Sarang Joshi. Polynomial regression on riemannian manifolds. In
European conference on computer vision, pages 1–14. Springer,
2012b.
Emiel Hoogeboom, Victor Garcia Satorras, Cl´ement Vignac, and
Max Welling. Equivariant diffusion for molecule generation in
3d, 2022.
Xiaoling Hu, Li Fuxin, Dimitris Samaras, and Chao Chen. Topology-
preserving deep image segmentation, 2019.
Xiaoling Hu, Yusu Wang, Li Fuxin, Dimitris Samaras, and Chao
Chen. Topology-aware segmentation using discrete morse theory,
2021.
Zhiwu Huang and Luc Van Gool.
A Riemannian Network
for SPD Matrix Learning.
pages 2036–2042, 2016.
URL
http://arxiv.org/abs/1608.04233.
Stephan Huckemann, Thomas Hotz, and Axel Munk.
Intrinsic
shape analysis: Geodesic PCA for riemannian manifolds modulo
isometric lie group actions. Statistica Sinica, 20(1):1–58, 2010.
ISSN 10170405.

33
Michael J. Hutchinson, Charline Le Lan, Sheheryar Zaidi, Em-
ilien Dupont, Yee Whye Teh, and Hyunjik Kim.
Lietrans-
former: Equivariant self-attention for lie groups.
In In-
ternational Conference on Machine Learning, 2020.
URL
https://api.semanticscholar.org/CorpusID:229340145.
J.B. Ingraham, M. Baranov, and Z. et al. Costello.
Illuminating
protein space with a programmable generative model. 2023. URL
https://doi.org/10.1038/s41586-023-06728-8.
John
Ingraham,
Vikas
Garg,
Regina
Barzilay,
and
Tommi
Jaakkola.
Generative
models
for
graph-based
protein
design.
2019.
URL
https://papers.nips.cc/paper
files/paper/2019/hash/f3a4ff4839c56a5f460c88cce3666a2b-Abstract.html.
Kristopher T. Jensen, Ta-Chu Kao, Marco Tripodi, and Guillaume
Hennequin. Manifold gplvms for discovering non-euclidean latent
structure in neural data. In Proceedings of the 34th International
Conference on Neural Information Processing Systems, NIPS
’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN
9781713829546.
Jianwen Jiang, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue
Gao. Dynamic hypergraph neural networks. In Proceedings of the
Twenty-Eighth International Joint Conference on Artificial Intel-
ligence, IJCAI-2019. International Joint Conferences on Artificial
Intelligence Organization, August 2019. doi: 10.24963/ijcai.2019/
366. URL http://dx.doi.org/10.24963/ijcai.2019/366.
Richard A. Johnson and Thomas E. Wehrly.
Some angular-
linear distributions and related regression models.
Journal of
the American Statistical Association, 73:602–606, 1978.
URL
https://api.semanticscholar.org/CorpusID:122640093.
J. Jumper, R. Evans, and A. et al. Pritzel.
Highly accurate pro-
tein structure prediction with alphafold.
Nature, 2021.
URL
https://doi.org/10.1038/s41586-021-03819-2.
Seungwoo Kang and Hee-Seok Oh. Probabilistic Principal Curves on
Riemannian Manifolds. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 46(7):4843–4849, July 2024.
ISSN
0162-8828, 2160-9292, 1939-3539. doi: 10.1109/TPAMI.2024.
3357801. URL https://ieeexplore.ieee.org/document/10413614/.
Ryan Keisler.
Forecasting global weather with graph neural net-
works, 2022.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational
Bayes. In Proceedings of the 2nd International Conference on
Learning Representations (ICLR), 2014.
Thomas N. Kipf and Max Welling.
Semi-supervised clas-
sification with graph convolutional networks.
In Interna-
tional Conference on Learning Representations, 2017.
URL
https://openreview.net/forum?id=SJU4ayYgl.
Max Kochurov, Rasul Karimov, and Serge Kozlukov.
Geoopt:
Riemannian optimization in pytorch, 2020.
Dimitrios Konstantinidis, Ilias Papastratis, Kosmas Dimitropou-
los, and Petros Daras.
Multi-manifold attention for vi-
sion transformers.
IEEE Access, 11:123433–123444, 2023.
ISSN 2169-3536.
doi: 10.1109/access.2023.3329952.
URL
http://dx.doi.org/10.1109/ACCESS.2023.3329952.
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzade-
nesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima
Anandkumar. Neural operator: Learning maps between function
spaces with applications to pdes. Journal of Machine Learning
Research, 24(89):1–97, 2023.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Im-
agenet classification with deep convolutional neural networks.
Communications of the ACM, 60:84 – 90, 2012.
URL
https://api.semanticscholar.org/CorpusID:195908774.
Line
K¨uhnel
and
Stefan
Sommer.
Stochastic
development
regression
on
non-linear
manifolds.
In
Information
Processing
in
Medical
Imaging,
2017.
URL
https://api.semanticscholar.org/CorpusID:122942.
Soumyabrata Kundu and Risi Kondor. Steerable transformers. arXiv
preprint arXiv:2405.15932, 2024.
Maxime W. Lafarge, Erik J. Bekkers, Josien P. W. Pluim, Remco
Duits, and Mitko Veta. Roto-translation equivariant convolutional
networks: Application to histopathology image analysis, 2020.
Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter
Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo
Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose,
Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott,
Alexander Pritzel, Shakir Mohamed, and Peter Battaglia. Graph-
cast: Learning skillful medium-range global weather forecasting,
2023.
Nicholas W. Landry, Maxime Lucas, Iacopo Iacopini, Giovanni
Petri, Alice Schwarze, Alice Patania, and Leo Torres.
XGI:
A Python package for higher-order interaction networks. Jour-
nal of Open Source Software, 8(85):5162, May 2023.
doi:
10.21105/joss.05162. URL https://doi.org/10.21105/joss.05162.
Neil Lawrence.
Gaussian process latent variable models for
visualisation
of
high
dimensional
data.
In
S.
Thrun,
L.
Saul,
and
B.
Sch¨olkopf,
editors,
Advances
in
Neural
Information
Processing
Systems,
volume
16.
MIT
Press,
2003.
URL
https://proceedings.neurips.cc/paper
files/paper/2003/file/9657c1fffd38824e5ab0472e022e577e-Paper.pdf.
Alice Le Brigant and St´ephane Puechmorel. Optimal Riemannian
quantization with an application to air traffic analysis.
arXiv
preprint arXiv:1806.07605, 2018.
Yann LeCun, Leon Botton, Yoshua Bengio, and Patrick Haffner.
Gradient-Based Learning Applied to Document Recognition.
Proc. of the IEEE, 86(11):2278 – 2324, 1998.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin
Choi, and Yee Whye Teh.
Set transformer: A framework
for attention-based permutation-invariant neural networks.
In
Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Pro-
ceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Re-
search, pages 3744–3753. PMLR, 09–15 Jun 2019.
URL
https://proceedings.mlr.press/v97/lee19d.html.
See Lee, Feng Ji, and Wee Peng Tay. Sgat: Simplicial graph attention
network. pages 3167–3175, 07 2022. doi: 10.24963/ijcai.2022/
440.
Adrien-Marie Legendre. Nouvelles m´ethodes pour la d´etermination
des orbites des com`etes. 1805.
M.M. Li, K. Huang, and M. Zitnik. Graph representation learning
in biomedicine and healthcare. Nature Biomedicine, 2022a. URL
https://doi.org/10.1038/s41551-022-00942-x.
Zhengyu
Li,
XUAN
TANG,
Zihao
Xu,
Xihao
Wang,
Hui
Yu,
Mingsong
Chen,
and
xian
wei.
Geodesic
self-
attention for 3d point clouds.
In S. Koyejo, S. Mohamed,
A.
Agarwal,
D.
Belgrave,
K.
Cho,
and
A.
Oh,
editors,
Advances
in
Neural
Information
Processing
Systems,
volume
35,
pages
6190–6203.
Curran
Associates,
Inc.,
2022b.
URL
https://proceedings.neurips.cc/paper
files/paper/2022/file/28e4ee96c94e31b2d040b4521d2b299e-Paper-Conference.pdf.
Lizhen Lin, Brian St. Thomas, Hongtu Zhu, and David B. Dunson.
Extrinsic local regression on manifold-valued data. Journal of
the American Statistical Association, 112(519):1261–1273, May
2017. ISSN 1537-274X. doi: 10.1080/01621459.2016.1208615.
URL http://dx.doi.org/10.1080/01621459.2016.1208615.
Cong Liu, David Ruhe, Floor Eijkelboom, and Patrick Forr´e. Clifford
group equivariant simplicial message passing networks.
arXiv
preprint arXiv:2402.10011, 2024.
Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Haonan Yuan,
Shurui Gui, Zhao Xu, Haiyang Yu, Jingtun Zhang, Yi Liu,
Keqiang Yan, Bora Oztekin, Haoran Liu, Xuan Zhang, Cong
Fu, and Shuiwang Ji.
Dig: A turnkey library for diving into
graph deep learning research. ArXiv, abs/2103.12608, 2021. URL
https://api.semanticscholar.org/CorpusID:232320529.
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Ar-
avindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey
Dosovitskiy, and Thomas Kipf. Object-centric learning with slot
attention, 2020.
Marco Lorenzi and Xavier Pennec. Geodesics, Parallel Transport
& One-parameter Subgroups for Diffeomorphic Image Regis-
tration. International Journal of Computer Vision, 105(2):111–
127, November 2013. doi: 10.1007/s11263-012-0598-4. URL

34
https://hal.inria.fr/hal-00813835. Publisher: Springer Verlag.
Marco Lorenzi, Xavier Pennec, Giovanni B. Frisoni, and Nicholas
Ayache.
Disentangling
normal
aging
from
Alzheimer’s
disease
in
structural
magnetic
resonance
images.
Neu-
robiology
of
Aging,
36:S42–S52,
January
2015.
ISSN
01974580.
doi: 10.1016/j.neurobiolaging.2014.07.046.
URL
https://linkinghub.elsevier.com/retrieve/pii/S0197458014005594.
Youzhi Luo and Shuiwang Ji.
An autoregressive flow model for
3d molecular geometry generation from scratch.
In Interna-
tional Conference on Learning Representations, 2022.
URL
https://api.semanticscholar.org/CorpusID:251647192.
L.
Machado,
F.
Silva
Leite,
and
K.
Krakowski.
Higher-
order smoothing splines versus least squares problems on
Riemannian manifolds.
Journal of Dynamical and Con-
trol Systems, 16(1):121–148, January 2010.
ISSN 1079-
2724, 1573-8698.
doi: 10.1007/s10883-010-9080-1.
URL
http://link.springer.com/10.1007/s10883-010-9080-1.
Luıs Machado and F Silva Leite. Fitting Smooth Paths on Rieman-
nian Manifolds. nternational Journal of Applied Mathematics and
Statistics, 4, 2006.
Elodie Maignant, Alain Trouv´e, and Xavier Pennec.
Riemannian
Locally Linear Embedding with Application to Kendall Shape
Spaces, page 12–20. Springer Nature Switzerland, 2023. ISBN
9783031382710.
doi: 10.1007/978-3-031-38271-0 2.
URL
http://dx.doi.org/10.1007/978-3-031-38271-0 2.
Anton Mallasto and Aasa Feragen. Application de la methode des
moindre quarres a l’interpolation des suites. In Annales des Math
Pures, pages 5580–5588, 2018.
Jason D. McEwen, Christopher G. R. Wallis, and Augustine N.
Mavor-Parker.
Scattering networks on the sphere for scalable
and rotationally equivariant spherical cnns, 2022.
Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger.
Umap: Uniform manifold approximation and projection. Journal
of Open Source Software, 3(29):861, 2018. doi: 10.21105/joss.
00861. URL https://doi.org/10.21105/joss.00861.
Maciej Mikulski and Jaroslaw Duda. Toroidal autoencoder, 2019.
N. Miolane and X. Pennec. Computing bi-invariant pseudo-metrics
on lie groups for consistent statistics. Entropy, 17(4), 2015. ISSN
10994300. doi: 10.3390/e17041850.
N. Miolane, A. Le Brigant, B. Hou, C. Donnat, M. Jorda, J. Mathe,
X. Pennec, and S. Holmes.
Geomstats: a python module for
computations and statistics on manifolds. Submitted to JMLR,
2019. URL https://github.com/geomstats/geomstats.
Nina Miolane and Susan Holmes. Learning weighted submanifolds
with variational autoencoders and riemannian variational autoen-
coders. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 14503–14511, 2020.
Nina Miolane, Frederic Poitevin, Yee-Ting Li, and Susan Holmes.
Estimation of orientation and camera parameters from cryo-
electron microscopy images with variational autoencoders and
generative adversarial networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)
Workshops, June 2020.
Prasanna Muralidharan, Jacob Hinkle, and P. Fletcher. A map estima-
tion algorithm for bayesian polynomial regression on riemannian
manifolds.
pages 215–219, 09 2017.
doi: 10.1109/ICIP.2017.
8296274.
E. A. Nadaraya. On estimating regression. Theory of Probability
and its Applications, 9:141–142, 1964.
Maximillian Nickel and Douwe Kiela.
Poincar´e embeddings for
learning hierarchical representations. Advances in neural infor-
mation processing systems, 30, 2017.
Victor M. Panaretos, Tung Pham, and Zhigang Yao.
Principal
flows. Journal of the American Statistical Association, 109(505):
424–436, 2014. ISSN 1537274X. doi: 10.1080/01621459.2013.
849199.
Mathilde Papillon, Sophia Sanborn, Mustafa Hajij, and Nina Mi-
olane. Architectures of topological deep learning: A survey on
topological neural networks. arXiv preprint arXiv:2304.10031,
2023.
Karl Pearson. Liii. on lines and planes of closest fit to systems of
points in space. The London, Edinburgh, and Dublin philosoph-
ical magazine and journal of science, 2(11):559–572, 1901.
Bruno
Pelletier.
Non-parametric
regression
estimation
on
closed
riemannian
manifolds.
Journal
of
Nonparametric
Statistics,
18:57
–
67,
2006.
URL
https://api.semanticscholar.org/CorpusID:17937994.
X. Pennec, S. Sommer, and T. Fletcher.
Riemannian Geo-
metric Statistics in Medical Image Analysis.
Elsevier Sci-
ence & Technology, 2019.
ISBN 978-0-12-814725-2.
URL
https://books.google.com/books?id=k8qsDwAAQBAJ.
Xavier Pennec. Intrinsic Statistics on Riemannian Manifolds: Basic
Tools for Geometric Measurements.
Journal of Mathematical
Imaging and Vision, 25(1):127–154, 2006. ISSN 0924-9907.
Xavier Pennec. Barycentric subspace analysis on manifolds. Annals
of Statistics, 46(6A):2711–2746, 2018.
ISSN 00905364.
doi:
10.1214/17-AOS1636.
Xavier Pennec and Vincent Arsigny. Exponential Barycenters of the
Canonical Cartan Connection and Invariant Means on Lie Groups.
In Frederic Barbaresco, Amit Mishra, and Frank Nielsen, editors,
Matrix Information Geometry, pages 123–168. Springer, 5 2012.
ISBN 978-3-642-30231-2 (Print) / 978-3-642-30232-9 (Online).
doi: 10.1007/978-3-642-30232-9.
Xavier Pennec and Marco Lorenzi. 5 - Beyond Riemannian geom-
etry: The affine connection setting for transformation groups.
In Xavier Pennec, Stefan Sommer, and Tom Fletcher, editors,
Riemannian Geometric Statistics in Medical Image Analysis,
pages 169–229. Academic Press, January 2020.
ISBN 978-0-
12-814725-2. doi: 10.1016/B978-0-12-814725-2.00012-1. URL
http://www.sciencedirect.com/science/article/pii/B9780128147252000121.
Xavier Pennec and Jean-Philippe Thirion.
A Framework for
Uncertainty and Validation of 3D Registration Methods based
on Points and Frames. Int. Journal of Computer Vision, 25(3):
203–229, December 1997. doi: 10.1023/A:1007976002485. URL
http://www.springerlink.com/openurl.asp?genre=article&issn=0920-5691&volume=25&iss
Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A Riemannian
Framework for Tensor Computing.
International Journal of
Computer Vision, 66(1):41–66, 1 2006.
Alexander
Petersen
and
Hans-Georg
Muller.
Fr´echet
regression
for
random
objects
with
euclidean
pre-
dictors.
The
Annals
of
Statistics,
2016.
URL
https://api.semanticscholar.org/CorpusID:13666043.
Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Pe-
ter W. Battaglia.
Learning mesh-based simulation with graph
networks, 2021.
Stephen M. Pizer, P. Thomas Fletcher, Sarang Joshi, Andrew Thall,
Jiahui Z. Chen, Yael Fridman, Daniel S. Fritsch, Grant Gash,
James M. Glotzer, Mark R. Jiroutek, Chiwu Lu, Keith E. Muller,
George Tracton, Paul Yushkevich, and Edward L. Chaney. De-
formable m-reps for 3d medical image segmentation. Interna-
tional Journal of Computer Vision, 55(2-3):85–106, November
2003. doi: 10.1023/a:1026313132218.
Brenda Praggastis, Sinan G. Aksoy, Dustin Arendt, Mark Bonicillo,
Cliff A Joslyn, Emilie Purvine, Madelyn R Shapiro, and Ji Young
Yun.
Hypernetx: A python package for modeling complex
network data as hypergraphs. ArXiv, abs/2310.11626, 2023. URL
https://api.semanticscholar.org/CorpusID:264288882.
PyT-Team. Topobenchmarkx, 2024.
Charles
Ruizhongtai
Qi,
Li
Yi,
Hao
Su,
and
Leonidas
J
Guibas.
Pointnet++:
Deep
hierarchical
feature
learning
on point sets in a metric space.
In I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and
R.
Garnett,
editors,
Advances
in
Neural
Information
Processing
Systems,
volume
30.
Curran
Associates,
Inc.,
2017.
URL
https://proceedings.neurips.cc/paper
files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf.
P.
Rajpurkar,
E.
Chen,
and
O.
et
al.
Banerjee.
Ai
in
health
and
medicine.
Nature
Medicine,
2022.
URL
https://doi.org/10.1038/s41591-021-01614-0.
Danny Reidenbach and Aditi S. Krishnapriyan.
Coarsenconf:

35
Equivariant coarsening with aggregated attention for molecular
conformer generation, 2023.
P. Reiser, M. Neubert, and A. et al. Eberhard.
Graph neural
networks for materials science and chemistry.
2022.
URL
https://doi.org/10.1038/s43246-022-00315-6.
T. Mitchell Roddenberry, Michael T. Schaub, and Mustafa Ha-
jij.
Signal processing on cell complexes.
In ICASSP 2022
- 2022 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pages 8852–8856, 2022. doi:
10.1109/ICASSP43922.2022.9747233.
Frank Rosenblatt. The perceptron: a probabilistic model for informa-
tion storage and organization in the brain. Psychological review,
65(6):386, 1958.
Sam
T.
Roweis
and
Lawrence
K.
Saul.
Nonlinear
Dimensionality
Reduction
by
Locally
Linear
Em-
bedding.
Science,
290(22):2323–2326,
2000.
URL
http://www.robots.ox.ac.uk/∼az/lectures/ml/lle.pdf.
David
E.
Rumelhart,
Geoffrey
E.
Hinton,
and
Ronald
J.
Williams.
Learning
representations
by
back-
propagating
errors.
Nature,
323:533–536,
1986.
URL
https://api.semanticscholar.org/CorpusID:205001834.
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex
Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate
complex physics with graph networks, 2020.
Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n)
equivariant graph neural networks. In International conference
on machine learning, pages 9323–9332. PMLR, 2021.
Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n)
equivariant graph neural networks, 2022.
Anna M M Scaife and Fiona Porter. Fanaroff–riley classification
of radio galaxies using group-equivariant convolutional neural
networks. Monthly Notices of the Royal Astronomical Society, 503
(2):2369–2379, February 2021. ISSN 1365-2966. doi: 10.1093/
mnras/stab530. URL http://dx.doi.org/10.1093/mnras/stab530.
Christof Sch¨otz. Nonparametric regression in nonstandard spaces.
Electronic Journal of Statistics, 16(2):4679 – 4741, 2022. doi:
10.1214/22-EJS2056. URL https://doi.org/10.1214/22-EJS2056.
Kristof T. Sch¨utt, Pieter-Jan Kindermans, Huziel E. Sauceda, Ste-
fan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M¨uller.
Schnet: A continuous-filter convolutional neural network for
modeling quantum interactions, 2017.
Xiaoyan
Shi,
Martin
Styner,
Jeffrey
Lieberman,
Joseph
G.
Ibrahim,
Weili
Lin,
and
Hongtu
Zhu.
Intrinsic
Regression
Models
for
Manifold-Valued
Data,
page
192–199.
Springer
Berlin
Heidelberg,
2009.
ISBN
9783642042713.
doi: 10.1007/978-3-642-04271-3 24.
URL
http://dx.doi.org/10.1007/978-3-642-04271-3 24.
Gregor N. C. Simm, Robert Pinsler, G´abor Cs´anyi, and Jos´e Miguel
Hern´andez-Lobato. Symmetry-aware actor-critic for 3d molecular
design, 2020.
Martin Simonovsky and Nikos Komodakis.
Graphvae: Towards
generation of small graphs using variational autoencoders, 2018.
Rapha¨el Sivera, Herv´e Delingette, Marco Lorenzi, Xavier Pennec,
and Nicholas Ayache. A model of brain morphological changes
related to aging and Alzheimer’s disease from cross-sectional
assessments. NeuroImage, 198:255–270, September 2019. ISSN
10538119.
doi: 10.1016/j.neuroimage.2019.05.040.
URL
https://linkinghub.elsevier.com/retrieve/pii/S105381191930432X.
Stefan
Sommer.
Horizontal
Dimensionality
Reduction
and
Iterated
Frame
Bundle
Development.
In
Frank
Nielsen
and
Fr´ed´eric
Barbaresco,
editors,
Geometric
Science
of
Information,
pages
76–83.
Springer
Berlin
Heidelberg,
2013.
ISBN 978-3-642-40019-3 978-3-642-40020-9.
URL
http://link.springer.com/chapter/10.1007/978-3-642-40020-9 7.
Stefan Sommer, Franc¸ois Lauze, and Mads Nielsen. Optimization
over geodesics for exact principal geodesic analysis. Advances in
Computational Mathematics, 40(2):283–313, 2014.
Florian Steinke, Matthias Hein, Jan Peters, and Bernhard Sch¨olkopf.
Manifold-valued thin-plate splines with applications in computer
graphics. Comput. Graph. Forum, 27:437–448, 04 2008. doi:
10.1111/j.1467-8659.2008.01141.x.
Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin,
Andres Cubillos-Ruiz, Nina M. Donghia, Craig R. MacNair,
Shawn French, Lindsey A. Carfrae, Zohar Bloom-Ackermann,
Victoria M. Tran, Anush Chiappino-Pepe, Ahmed H. Badran,
Ian W. Andrews, Emma J. Chory, George Church, Eric D. Brown,
T. Jaakkola, Regina Barzilay, and James J Collins.
A deep
learning approach to antibiotic discovery. Cell, 180:688–702.e13,
2020. URL https://api.semanticscholar.org/CorpusID:261621991.
Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A Global
Geometric Framework for Nonlinear Dimensionality Reduction.
Science, 290(5500):2319, 2000.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li,
Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-
and translation-equivariant neural networks for 3D point clouds.
2018. URL http://arxiv.org/abs/1802.08219.
Michael E Tipping and Christopher M Bishop. Probabilistic principal
component analysis.
Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 61(3):611–622, 1999.
James Townsend, Niklas Koep, and Sebastian Weichwald.
Py-
manopt: A python toolbox for optimization on manifolds using
automatic differentiation. J. Mach. Learn. Res., 17:137:1–137:5,
2016. URL https://api.semanticscholar.org/CorpusID:29194288.
Alain Trouv´e. Diffeomorphisms Groups and Pattern Matching in
Image Analysis. International Journal of Computer Vision, 28
(3):213–221, 1998. ISSN 1573-1405.
Dimosthenis Tsagkrasoulis and Giovanni Montana.
Random
forest
regression
for
manifold-valued
responses.
Pattern
Recognition
Letters,
101:6–13,
January
2018.
ISSN
0167-8655.
doi:
10.1016/j.patrec.2017.11.008.
URL
http://dx.doi.org/10.1016/j.patrec.2017.11.008.
Laurens van der Maaten and Geoffrey Hinton. Visualizing Data using
{t-SNE}. Journal of Machine Learning Research, 9:2579–2605,
2008.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia
Polosukhin.
Attention is all you need.
In I. Guyon,
U.
Von
Luxburg,
S.
Bengio,
H.
Wallach,
R.
Fergus,
S.
Vishwanathan,
and
R.
Garnett,
editors,
Advances
in
Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper
files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and
Max Welling.
Rotation equivariant cnns for digital pathology,
2018.
Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana
Romero, Pietro Li`o, and Yoshua Bengio. Graph Attention Net-
works. International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJXMpikCZ.
Clement Vignac, Nagham Osman, Laura Toni, and Pascal Frossard.
Midi: Mixed graph and 3d denoising diffusion for molecule
generation, 2023.
Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin
Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van
Katwyk, Andreea Deac, Anima Anandkumar, Karianne J. Bergen,
Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby,
Jure Leskovec, Tie-Yan Liu, Arjun K. Manrai, Debora S. Marks,
Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar
Velickovic, Max Welling, Linfeng Zhang, Connor W. Coley,
Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the
age of artificial intelligence.
Nature, 620:47–60, 2023.
URL
https://api.semanticscholar.org/CorpusID:260384616.
Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang
Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao,
Tong He, George Karypis, Jinyang Li, and Zheng Zhang. Deep
graph library: A graph-centric, highly-performant package for
graph neural networks. arXiv preprint arXiv:1909.01315, 2019.
Minmin
Wang.
Height
and
diameter
of
brownian
tree.
Electronic Communications in Probability, 20(none), January
2015.
ISSN 1083-589X.
doi: 10.1214/ecp.v20-4193.
URL

36
http://dx.doi.org/10.1214/ECP.v20-4193.
Maurice Weiler et al.
Equivariant and coordinate independent
convolutional networks: A gauge field theory of neural networks.
2024.
Christopher
Williams
and
Carl
Rasmussen.
Gaussian
processes
for
regression.
In
D.
Touretzky,
M.C.
Mozer,
and
M.
Hasselmo,
editors,
Advances
in
Neural
Information
Processing
Systems,
volume
8.
MIT
Press,
1995.
URL
https://proceedings.neurips.cc/paper
files/paper/1995/file/7cce53cf90577442771720a370c3c723-Paper.pdf.
Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph
neural networks in recommender systems: A survey.
ACM
Comput. Surv., 55(5), dec 2022. ISSN 0360-0300. doi: 10.1145/
3535101. URL https://doi.org/10.1145/3535101.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
Zhang, and Philip S. Yu. A comprehensive survey on graph neural
networks. IEEE Transactions on Neural Networks and Learning
Systems, 32(1):4–24, 2021. doi: 10.1109/TNNLS.2020.2978386.
Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and
Jian Tang. Geodiff: a geometric diffusion model for molecular
conformation generation, 2022.
Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, and Jure
Leskovec.
Geometric latent diffusion models for 3d molecule
generation, 2023.
Yun Yang and David B. Dunson.
Bayesian manifold regression.
The Annals of Statistics, 44(2):876 – 905, 2016. doi: 10.1214/
15-AOS1390. URL https://doi.org/10.1214/15-AOS1390.
Z. Yang, X. Zeng, and Y. et al. Zhao.
Alphafold2 and its ap-
plications in the fields of biology and medicine.
2023.
URL
https://doi.org/10.1038/s41392-023-01381-z.
Rex
Ying,
Ruining
He,
Kaifeng
Chen,
Pong
Eksombatchai,
William L. Hamilton, and Jure Leskovec.
Graph convolu-
tional neural networks for web-scale recommender systems.
In Proceedings of the 24th ACM SIGKDD International Con-
ference on Knowledge Discovery & Data Mining, KDD ’18.
ACM, July 2018.
doi: 10.1145/3219819.3219890.
URL
http://dx.doi.org/10.1145/3219819.3219890.
Laurent Younes. Shapes and diffeomorphisms. Number v. 171 in
Applied mathematical sciences. Springer, Heidelberg ; New York,
2010. ISBN 978-3-642-12054-1. OCLC: ocn632088149.
Liangliang Zhang, Yushu Shi, Robert R. Jenq, Kim-Anh Do, and
Christine B. Peterson. Bayesian compositional regression with
structured priors for microbiome feature selection. Biometrics,
77(3):824–838, July 2020. ISSN 1541-0420. doi: 10.1111/biom.
13335. URL http://dx.doi.org/10.1111/biom.13335.
Miaomiao Zhang and P. Thomas Fletcher.
Probabilistic principal
geodesic analysis. Advances in Neural Information Processing
Systems, pages 1–9, 2013. ISSN 10495258.

