US010194203B2 
( 12 ) United States Patent 
Avila et al . 
( 10 ) Patent No 
. : 
US 10 , 194 , 203 B2 
( 45 ) Date of Patent : 
Jan . 29 , 2019 
( 54 ) MULTIMODAL AND REAL - TIME METHOD 
FOR FILTERING SENSITIVE MEDIA 
( 71 ) Applicants : SAMSUNG ELECTRÔNICA DA 
AMAZÔNIA LTDA . , Campinas ( BR ) ; 
UNIVERSIDADE ESTADUAL DE 
CAMPINAS , Campinas ( BR ) 
( 52 ) U . S . CI . 
CPC . . . H04N 21 / 4542 ( 2013 . 01 ) ; G06F 17 / 30817 
( 2013 . 01 ) ; G06K 9 / 00718 ( 2013 . 01 ) ; 
( Continued ) 
( 58 ) 
Field of Classification Search 
None 
See application file for complete search history . 
( 56 ) 
References Cited 
U . S . PATENT DOCUMENTS 
( 72 ) Inventors : Sandra Avila , Campinas ( BR ) ; Daniel 
Moreira , Campinas ( BR ) ; Mauricio 
Perez , Campinas ( BR ) ; Daniel Moraes , 
Campinas ( BR ) ; Vanessa Testoni , 
Campinas ( BR ) ; Siome Goldenstein , 
Campinas ( BR ) ; Eduardo Valle , 
Campinas ( BR ) ; Anderson Rocha , 
Campinas ( BR ) 
8 , 285 , 118 B2 
2003 / 0126267 A1 
10 / 2012 Bronstein et al . 
7 / 2003 Gutta et al . 
( Continued ) 
OTHER PUBLICATIONS 
( 73 ) Assignees : SAMSUNG ELETRÔNICA DA 
AMACÔNIA LTDA . , Campinas , São 
Paulo ( BR ) ; UNIVERSIDADE 
ESTADUAL DE CAMPINAS , 
Campinas , São Paulo ( BR ) 
Caetano , Carlos , et al . “ Pornography detection using bossanova 
video descriptor . ” Signal Processing Conference ( EUSIPCO ) , 2014 
Proceedings of the 22nd European . IEEE , 2014 . * 
Primary Examiner — Sean M Conner 
( * ) Notice : 
Subject to any disclaimer , the term of this 
patent is extended or adjusted under 35 
U . S . C . 154 ( b ) by 192 days . 
( 21 ) Appl . No . : 15 / 198 , 626 
( 22 ) Filed : 
Jun . 30 , 2016 
( 65 ) 
Prior Publication Data 
US 2017 / 0289624 A1 
Oct . 5 , 2017 
( 57 ) 
ABSTRACT 
A multimodal and real - time method for filtering sensitive 
content , receiving as input a digital video stream , the method 
including segmenting digital video into video fragments 
along the video timeline ; extracting features containing 
significant information from the digital video input on 
sensitive media ; reducing the semantic difference between 
each of the low - level video features , and the high - level 
sensitive concept ; classifying the video fragments , generat 
ing a high - level label ( positive or negative ) , with a confi 
dence score for each fragment representation ; performing 
high - level fusion to properly match the possible high - level 
labels and confidence scores for each fragment ; and predict 
ing the sensitive time by combining the labels of the 
fragments along the video timeline , indicating the moments 
when the content becomes sensitive . 
( 30 ) 
Foreign Application Priority Data 
Apr . 1 , 2016 
( BR ) . . . . . . . . . . . . . . . . . . . . . . . . 102016007265 
( 51 
) 
Int . CI 
. 
G06K 9 / 62 
H04N 21 
/ 454 
( 2006 . 01 ) 
( 2011 . 01 ) 
( Continued ) 
13 Claims , 6 Drawing Sheets 
Low - levei Feature 
Extraczku SIG 
Movei supet 
Representatku Staro 
i 
Higg - level Snipec 
Clusification Stage 
MASS - level Fusion 
Strige 
100 
Features Extracoin 
21 
Frate 
090 
Visus Feature 
fxvarnon 
0ita 
eo 
+ idsY1Faith 
- 1 . 24 
14 
Van Sopot 
Samtatius 
: 
Video 
S 
et 
Mideal 
Voeo Sigos ! 
of fear 
Extrascola 
Excoche 
Recension 
SAX ! ! ve 
faften : 
10 
ext 
Text fexure 
Exaction 
Extractos 

US 10 , 194 , 203 B2 
Page 2 
( 51 ) 
Int . Cl . 
G06F 1730 
( 2006 . 01 ) 
H04L 29 / 06 
( 2006 . 01 
) 
H04N 21 / 4545 
( 2011 . 01 ) 
G06K 9 / 46 
( 2006 . 01 ) 
GIOL 25 / 57 
( 2013 . 01 
) 
G06K 9 / 00 
( 2006 . 01 ) 
U . S . CI . 
CPC . . . . . . . G06K 9 / 00765 ( 2013 . 01 ) ; G06K 9 / 4676 
( 2013 . 01 ) ; G06K 9 / 6269 ( 2013 . 01 ) ; GIOL 
25 / 57 ( 2013 . 01 ) ; H04L 65 / 60 ( 2013 . 01 ) ; 
H04N 21 / 4545 ( 2013 . 01 ) ; GOOK 98629 
( 2013 . 01 ) ; GOOK 9 / 6232 ( 2013 . 01 ) ; GOOK 
9 / 6293 ( 2013 . 01 ) 
( 52 ) 
( 56 ) 
References Cited 
U . S . PATENT DOCUMENTS 
2006 / 0111897 A1 * 
5 / 2006 Gemello . . . . . . . . . . . . . . . . . GIOL 15 / 16 
704 / 202 
2009 / 0274364 Al 
11 / 2009 Shakya et al . 
2012 / 0246732 AL 
9 / 2012 Burton 
2013 / 0283388 A 
10 / 2013 Ashok et al . 
2014 / 0207450 A17 / 2014 LaVoie et al . 
2014 / 0372876 Al 
12 / 2014 Bliss et al . 
2016 / 0314380 A1 * 
10 / 2016 Abdulkader . . . . . . . . . . . 
G06K 9 / 66 
* cited by examiner 

U . S . Patent 
Jan . 29 , 2019 
Sheet 1 of 6 
US 10 , 194 , 203 B2 
comment 
07 08 
w 
AAAA 
Ah 
wwwwwwwwwwwwwwwwwwwwwwww 
morussiarynga 
mom 
3 . Koxa 
? 
??? 
???? 
: : 
: : : 
: : 
: : : : 
: : : : : : : : : : : : : : : : : : : : : : : 
233333333333333333333 
opww . 
Figure 1 

U . S . Patent 
Jan . 29 , 2019 
Sheet 2 of 6 
US 10 , 194 , 203 B2 
U 
U 
UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU 
OS 
???????????????????????????????????????????????????????????????????????????????????????????????????????? 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
- - - - - 
- - - - - 
- - - - - - - 
- - - - - - - 
- - - - - - - 
- - 
' ' - ' - - " " - - - - - - - - - - - - - - - - - - - - - - - - - 
* 
+ + + + + 
+ + + + + + + + + + + + + + + + + + + + + + + 
* * * * * * * * 
ALAALAAAAAAUUAJAJAJAJAJAJAJU 
A 
M 
wond niemanden 
X 
Figure 2 

U . S . Patent 
Mid - 
level 
Snippet 
Representatima 
Stage 
High - 
level 
snippet 
High - 
level 
Fusion 
Extraction 
Sloge 
Features 
Extraction 
. 
22 
LAL 
ALALALALALA 
Jan . 29 , 2019 
Vou 
Fatura 
Nigar 
vides 
van 
article 
124 
Therefore 
w 
Video 
Snippet 
UCIO 
Visino 
Šišppet 
Audio F 
ury 
Extraction 
Pxresentator 
AAAAAAAAAAAAAAAAAAAAAAAA 
Sheet 3 of 6 
www 
. 
126 
LA 
Text 
Faiur 
Exeraction 
AL 
US 10 , 194 , 203 B2 
Figure 3 

Lowlevel 
Feoture 
Extroction 
Stage 
Mid - 
level 
Snipper 
Hepresentation 
Stage 
High - 
lever 
Snippet Cassification 
Stage 
atent 
Praordion 
tax 
W 
: 
07h 
. 
betonom 22 
anu 
* 
4 
Jan . 29 , 2019 
2 
Video Soins 
. 449YP 
Video 
Snigget 
Mid - 
level 
megrégate 
Representation 
Video 
Snippet 
Classification 
W 
: 
HIVI 
WAAAAAARIAN 
Dominant Component Analysis 
Prediction 
Snipote pooling 
Ann 
* 
* 
* 
Projection 
21 / 
123175 
Generati011 
w 
1 
3 
. 
32 
WWW WWW 
Sheet 4 of 6 
WA 
STEP 
Construction 
Buipoou3 
Visuak 
Aurin 
Text 
frast 
öxtruction 
* 
* 
* 
ww 
naurunanese 
- 
n 
* 
W 
Wri 
138 
A4 
PreXcient 
www 
122 / 
1248125 
3294043 
US 10 , 194 , 203 B2 
Figure 4 

U . S . Patent 
low - 
leves 
Feature Extraction 
Stage 
Mid - 
level 
Snippet 
Hepresentotion Stage 
High - 
level 
Snippet Cessikotion 
Stage 
Meyertaan 
mainintata 
22 
scien 
YOX09588 
AANMAAANNAAAAAANAAANAAAAAAANAN 
known 
Digital 
Video 
130 
Jan . 29 , 2019 
20diu 
cap wojewus was 
Video 
Sauippet MH - 
level 
Aggregate 
Representation 
Video 
Snippet 
Dosification 
OTT 
* 
* 
* 
* 
* 
* 
* 
* 
* 
* 
} 
Video 
Sgpet 
Data Projection 
Mid - 
level Encoding 
Snippet pooling 
1241231 
Franc / 
Autodi 
ext 
Extraction 
wwwwwwwwwwwww 
Sheet 5 of 6 
* 
+ + * 
* * 4 
* 
> > 
4 
. 
WWWWWW 
Visust / 
Audiofest Festust 
Extraction 
122 
/ 
124 
/ 
126 
21 
0000000000000001111111111 : 
US 10 , 194 , 203 B2 
Figure 5 

atent 
Jan . 29 , 2019 
Sheet 6 of 6 
US 10 , 194 , 203 B2 
Arriragyis 
4444444444444444 
4444444 
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 
" 
* 
* 
* 
* 
* 
* 
* 
* 
* 
- 
B 
- 
23 
AAAAAAAAAAAAAAAAAAAA 
WIFIR 
von 
* * 
. . . 
9 
On672 
MAAAAAAAAAAAAA 
. 
AAAAAAAAAAAAAAAAAAAAAAAAA 
: 
he 

US 10 , 194 , 203 B2 
MULTIMODAL AND REAL - TIME METHOD 
intrinsically semantic , since the class labels must be known 
FOR FILTERING SENSITIVE MEDIA 
during the training / learning phase . 
Bag - of - Visual - Words ( BoVW ) is the most popular mid 
CROSS - REFERENCE TO RELATED 
level image representation and the most widely used for 
APPLICATIONS 
5 
sensitive content classification problem . Inspired by the 
Bag - of - Words model from textual Information Retrieval , 
This application is a U . S . Utility Application , which 
where a document is represented by a set of words , the 
claims the foreign priority benefit under 35 U . S . C . $ 119 of 
BoVW representation describes an image as a histogram of 
Brazilian Patent Application No . 10 2016 007265 4 . filed 
the occurrence rate of “ visual words ” in a “ visual vocabu 
Apr . 1 , 2016 , the contents of which are incorporated herein 10 lary ” induced by quantizing the space of a local feature ( e . g . , 
by reference . 
SIFT – Scale Invariant Feature Transform , SURF - Speeded 
Up Robust Features , HOG — Histogram of Oriented Gradi 
FIELD OF THE INVENTION 
ents ) ) . The visual vocabulary of k visual words , also known 
as visual codebook or visual dictionary , is usually obtained 
The present invention relates to a method for filtering 15 by unsupervised learning ( e . g . , k - means clustering algo 
sensitive content in a video flow ( stream ) . More specifically , 
rithm ) over a sample of local descriptors from the training 
the present invention relates to a multimodal and real - time 
to a multimodal and real - time 
learning data . 
method for filtering sensitive content that uses machine - 
The BoVW representation has important limitations ( such 
learning techniques to extract and analyze visual , auditory 
as quantization error , spatial information loss ) , and several 
and / or textual of a video , resulting in an occurrence predic - 20 alternatives have been developed . One of the best mid - level 
tion sensitive content then filter them properly . The method 
aggregate representations currently reported in the literature , 
of the present invention can be applied for filtering sensitive 
the Fisher Vector is based upon the use of the Fisher kernel 
content in videos displayed on smart phones ( smartphones ) , 
framework with Gaussian mixture models ( GMM ) esti 
tablets , smart glasses ) , virtual reality devices , smart TVs 
mated over the training / learning data . For sensitive media 
( smart TVS ) , video cameras and the like . 
content classification , no commercial solutions took advan 
tage from that tip - top mid - level aggregate representation . 
BACKGROUND OF THE INVENTION 
Other approaches have employed audio features ( e . g . , 
MFCC — Mel - frequency Cepstral Coefficients , loudness , 
With the growth of video data generated by many devices , 
pitch ) to improve the classification of sensitive videos . The 
such as cameras , smartphones and closed - circuit televi - 
30 addition of audio analysis to the context of sensitive media 
sions — CCTVs 
, allied with the Internet as a fast spreading 
detection can be critical to detect challenging cases , which 
venue , smart and continuous content filtering becomes para 
can be a lot harder using visual features only ( e . g . , breast 
mount . In this context , classification of sensitive media 
feeding , hentai movies 
, gun shots ) . In addition , most of the 
content ( e . g . , pornography , violence , crowd ) retains a con - 
visual local descriptors are static ( i . e . , it does not take into 
siderable amount of attention because of its applications : it 35 account temporal information ) . Audio features , in the other 
can be used for detecting , via surveillance cameras , inap - 
hand , are purely temporal , since no spatial information is 
propriate behavior ; blocking undesired content from being 
available to be analyzed . Therefore , audio features can 
uploaded to ( or downloaded from ) general purpose websites 
overcome static visual descriptors when the nature of the 
( e . g . , social networks , online learning platforms , content 
sensitive content is fundamentally temporal ( e . g . , blows in a 
providers , forums 
) , or from being viewed on some places 40 fight ) . However , in the context of sensitive media analysis , 
( e . g . , schools , workplaces ) ; preventing children from 
the audio information should only be used along with the 
accessing adult content on personal computers , smart 
visual information . The audio features alone can be mis 
phones , smart glasses , tablets , cameras , Virtual Reality 
leading and unreliable since it often does not correspond to 
devices or smart TVs ; and avoiding that improper content is 
what is being visually displayed . It is very common , for 
distributed over phones by sexting , for instance . 
45 example , in movies , where there is plenty of background 
Sensitive media content may be defined as any material 
music which sometimes overlaps the action that is going on 
that represents threats to its audience . Regarding digital 
visually . Despite its importance and faster processing time , 
video , the typical sensitive representatives include pornog - 
no commercial solutions took advantage from the audio 
raphy and violence , but they may also cover disgusting 
information . 
scenes and other types of abstract concepts . Therefore , 50 
In addition , some approaches are based on multimodal 
automatically filtering sensitive content is a hard and chal - 
fusion , exploiting both auditory and visual features . Usually , 
lenging task , because of its high - level conceptual nature . 
the fusion of different modalities is performed at two levels : 
Most of the recent approaches on classification of sensi - 
( i ) feature level or early fusion , which combines the features 
tive content are typically composed of three steps : ( i ) 
before classification and ( ii ) decision level or late fusion , 
low - level visual feature extraction , ( ii ) mid - level feature 55 which combines the scores from individual classifier mod 
extraction , and ( iii ) high - level supervised classification . The 
els . 
low - level features are extracted from the image pixels . They 
The feature level fusion is advantageous in that it requires 
are still purely perceptual , but aim at being invariant to 
only one learning phase on the combined feature vector . 
viewpoint and illumination changes , partial occlusion , and 
However , in this approach it is hard to represent the time 
affine geometrical transformations . Mid - level features aim at 60 synchronization between the multimodal features . In addi 
combining the set of low - level features into a global and 
tion , the increase in the number of modalities makes it 
richer image representation of intermediate complexity . The 
difficult to learn the cross - correlation among the heteroge 
mid - level features may be purely perceptual or they may 
neous features . 
incorporate semantic information from the classes , the for - 
The strategy of the decision level fusion has many advan 
mer case being much more usual in the literature . Finally , the 65 tages over feature fusion . For instance , unlike feature level 
goal of supervised classification is to learn a function which 
fusion , where the features from different modalities ( e . g . , 
assigns ( discrete ) labels to arbitrary images . That step is 
audio and visual ) may have different representations , the 

US 10 , 194 , 203 B2 
decisions ( at the semantic level ) usually have the same 
Patent document U . S . Pat . No . 8 , 285 , 118 B2 titled “ Meth 
representation . Therefore , the fusion of decisions becomes 
ods and Systems for Media Content Control ” , published on 
easier . Moreover , the strategy of the decision level fusion 
Jan . 14 , 2010 , by NOVAFORA , INC , proposes methods and 
offers scalability in terms of the modalities used in the fusion 
systems to control the display of media content on media 
process , which is difficult to achieve in the feature level 5 player . The video media content is analyzed by extracting 
fusion . Another advantage of late fusion strategy is that it 
tage of late fusion strategy is that it 
only visual information local feature descriptors , such as 
allows us to use the most suitable methods for analyzing 
SIFT , spatio - temporal SIFT , or SURF descriptors . An even 
each single modality , such as hidden Markov model ( HMM ) 
tual similarity with the method of the present invention is the 
for audio and support vector machine ( SVM ) for image . This 
fact that U . S . Pat . No 
. 8 , 285 , 118 method computes the video 
10 signature using a BoVW - based mid - level representation . 
provides much more flexibility than the early fusion . 
However , while U . S . Pat . No . 8 , 285 , 118 proposes to match 
The method of the present invention relies on multimodal 
the BoVW signatures to a database of signatures ( it is time 
fusion of visual , auditory , and textual features for a fine 
consuming and it is not generalizable ) , the present invention 
grained classification of sensitive media content in video 
proposes to classify video signatures according a mathemati 
snippets ( short temporal series of video frames ) . 
15 cal model learned from the training / learning dataset ( it is 
Patent document US 2013 / 0283388 A1 
, titled “ Method 
very fast processing and it is generalizable ) . Furthermore , in 
and system for information content validation in electronic 
contrast with the present invention , document U . S . Pat . No . 
devices ” , published on Oct . 24 , 2013 by SAMSUNG ELEC 
8 , 285 , 118 does not use audio and / or textual content and does 
TRONICS CO . , LTD , proposes a method and system for 
not fuse information . 
content filtering in mobile communication devices . An even - 20 
Patent document US 2014 / 0372876 A1 titled " Selective 
tual similarity with the method of the present invention is the 
Content Presentation Engine ” , published on Dec . 18 , 2014 , 
fact that the method of document US 2013 / 0283388 ana 
by AMAZON TECHNOLOGIES , INC , proposes a method 
lyzes the information content including image , video and 
for suppressing content portion ( e . g . , audio portions that 
audio information in real - time . However , this approach does 
include profane language , video portions that include lewd 
not go into details . For example , for analyzing image / video 25 or violent behavior , etc . ) at an electronic device . In docu 
content , the authors only mentioned that an image analysis 
ment US 2014 / 0372876 A1 
, the selective content presenta 
engine comprises an “ Image and Video Filtering module 
tion engine may determine whether the content portion is to 
from IMAGEVISION located at Anna , Tex . , 75409 , U . S . A ” . 
be presented by the electronic device based on the user 
Nothing is specified in document US 2013 / 0283388 for 
preferences using visual or audio recognition . However 
analyzing audio information . Moreover , in contrast with the 30 document US 2014 / 0372876 Al only mentioned the differ 
present invention , US 20130283388 does not fuse informa - 
ent types of classifiers that recognize images or audio 
tion for classification and it does not classify sensitive 
segments and , it is not mentioned how the visual or audio 
content within a video timeline . 
content may be analyzed . In contrast with the present 
Patent document US 2012 / 0246732 A1 
, titled “ Apparatus , 
invention , US 20140372876 does not fuse information and 
Systems and Methods for Control of Inappropriate Media 35 it does not use mid - level aggregate representation . 
Content Events ” , published on Sep . 27 , 2012 , by BURTON 
Patent document US 2014 / 0207450 A1 titled “ Real - time 
DAVID ROBERT 
, proposes systems and methods to prevent 
Customizable Media Content Filter ” , published on Jul . 24 . 
presentation of inappropriate media content . The media 
2014 , by INTERNATIONAL BUSINESS MACHINES 
content analysis logic of document US 2012 / 0246732 A1 
CORPORATION , proposes a method for content filtering 
may comprise the audio recognition logic , the object rec - 40 ( e . g . , violence , profanity ) in real - time , with customizable 
ognition logic , the text recognition logic , and / or the char - 
preferences . Textual information , extracted from subtitles , 
acter recognition logic . However , it is not clear how sensi 
closed caption and audio stream , is analyzed by matching 
tive content is analyzed . The method of the present invention 
textual content with one or more blacklist table entries . 
exploits and evaluates a plurality of different characteristics 
Differently , the proposed method in the present invention 
( i . e . , fuse information ) , and also classifies sensitive content 45 analyzes textual content using a robust BoW - based frame 
within a video timeline , in contrast to the method proposed 
work . Additionally , the present invention uses visual infor 
in document US 2012 / 0246732 . 
mation , fuse information , and classifies sensitive content 
Patent document US 2009 / 0274364 A1 titled “ Apparatus 
within a video timeline . 
and Methods for Detecting Adult Videos ” proposes appara 
Patent document US 2003 / 0126267 A1 titled “ Method 
tus and methods for analyzing video content to determine 50 and Apparatus for Preventing Access to Inappropriate Con 
whether a video is adult or non - adult . Using a key frame 
tent Over a Network Based on Audio or Visual Content ” , 
detection system , the method of document US 2009 / 
published on Jul . 3 , 2003 , by KONINKLIJKE PHILIPS 
0274364 generates one or more models for adult video 
ELECTRONICS N . V , proposes a method and apparatus for 
detection . According to the inventors , any suitable key frame 
restricting access to electronic media objects having objec 
features may be extracted from each key frame _ _ 17 image ! 55 tionable content ( such as nudity , sexually explicit material , 
video analysis techniques are described ( including spatial 
violent content or bad language ) , based on an analysis of the 
and / or color distribution features and texture features ) . One 
audio or visual information . For example , image processing , 
drawback related to the present invention is that such 
speech recognition or face recognition techniques may be 
techniques are typically not robust to the changes in video 
employed to the identified inappropriate content . In contrast 
resolution , frame rate , cropping , orientation , or lighting . 60 with the present invention , document US 2003 / 0126267 
Differently , the method in the present invention proposes an 
does not fuse information , does not use mid - level aggregate 
end - to - end BoVW - based framework , which preserves more 
representation , and it does not classify sensitive content 
information while keeping the robustness to different 
within a video timeline . 
changes in video . Moreover , in contrast with the present 
Finally , patent document CN 104268284 A titled : " Web 
invention , document US 2009 / 0274364 does not use audio 65 Browse Filtering Soft dog Device Special for Juveniles ” , 
and / or textual content , does not fuse information and it does 
published on Jan . 4 , 2015 , by HEFEI XINGFU INFORMA 
not classify sensitive content within a video timeline . 
TION TECHNOLOGY CO . , LTD , provides a web browser 

US 10 , 194 , 203 B2 
filtering softdog ( USB dongle ) device which comprises a 
( IcoICT ) , pp . 30 - 35 , 2014 ) proposed aBoVW - based image ! 
pornographic content analysis module . As defined in docu - 
video nudity detection by using a skin filtering method and 
ment CN 104268284 , pornographic content analysis unit 
SVM classifiers . 
includes a text analysis module , image analysis module and 
By moving from nudity detection towards pornography 
video analysis module . However , it is not clear how porno - 5 classification , it arises the challenge in defining the notion of 
graphic content is analyzed . Furthermore , in contrast with 
pornography . Many scientific papers have adopted the defi 
the present invention , CN 104268284 does not use audio 
nition of pornography proposed by Short et al . ( Paper “ A 
content , it does not fuse information , it does not use mid 
review of internet pornography use research : methodology 
level aggregate representation and it does not classify sen 
and content from the past 10 years ” , M . Short , L . Black , A . 
sitive content within a video timeline . 
10 Smith , C . Wetterneck , and D . Wells , Cyberpsychology , 
Behavior , and Social Networking , vol . 15 , no . 1 , pp . 13 - 23 , 
In the following , it is summarized the scientific papers for 
2012 ) : “ any explicit sexual matter with the purpose of 
the two most important types of sensitive content considered 
eliciting arousal ” , which while still subjective , establishes a 
in this invention , pornography and violence . 
set of criteria that allow deciding the nature of the material 
Pornography Classification 
15 ( sexual content , explicitness , goal to elicit arousal , purpose 
The first efforts to detect pornography conservatively 
fulness ) . 
associated pornography with nudity , whereby the solutions 
Avila et al . proposed an extension to BoVw formalism , 
tried to identify nude or scantily - clad people ( Paper “ Auto 
BossaNova ( Paper “ Pooling in image representation : the 
matic detection of human nudes ” , D . Forsyth and M . Fleck , 
visual codeword point of view ” . S . Avila , N . Thome 
. M . 
International Journal of Computer Vision ( IJCV ) , vol . 32 , 20 Cord , E . Valle , and A . Araujo , Computer Vision and Image 
no . 1 , pp . 63 - 77 , 1999 ; Paper “ Statistical color models with 
Understanding ( CVIU ) , vol . 117 , pp . 453 - 465 , 2013 ) , with 
application to skin detection ” , M . Jones and J . Rehg , Inter - 
HueSIFT descriptors to classify pornographic videos using 
national Journal of Computer Vision ( IJCV ) , vol . 46 , no . 1 , 
majority voting . Recently , Caetano et al . ( Paper “ Represent 
pp . 81 - 96 , 2002 ; Paper , “ Naked image detection based on 
ing local binary descriptors with Bossa Nova for visual 
adaptive and extensible skin color model 
” , J . - S . Lee , Y . - M . 25 recognition ” , C . Caetano , S . Avila , S . Guimarães , and A . 
Kuo , P . - C . Chung , and E . - L . Chen , Pattern Recognition 
Araujo , in Symposium on Applied Computing ( SAC ) , pp . 
( PR ) , vol . 40 , no . 8 , pp . 2261 - 2270 , 2007 . ) . In such works , 
49 - 54 , 2014 ) achieved similar results by using Bossa Nova , 
the detection of human skin played a major role , followed by 
binary descriptors , and majority voting . In ( Paper “ Pornog 
the identification of body parts . 
raphy detection using Bossa Nova video descriptor ” , in 
The presence of nudity is not a good conceptual model of 30 European Signal Processing Conference ( EUSIPCO ) , pp . 
pornography . There are non - pornographic situations with 
1681 - 1685 , 2014 ) , Caetano et al . improved their previous 
plenty of body exposure . Conversely , there are pornographic 
results by establishing a single bag for the entire target 
scenes that involve very little exposed skin . Nevertheless , 
video , instead of a bag for each extracted video frame . A 
nudity detection is related to pornography detection , with a 
possible similarity to the method of the present invention is 
vast literature of its own . 
35 the fact that the method of Gaetano et al . calculates the 
The clear drawback of using skin detectors to identify 
signature using a mid - level video representation based on 
pornography is the high false - positive rate , especially in 
BoVW . However , while Gaetano et al . proposes the use of 
situations of non - pornographic body exposure ( e . g . , swim - 
BossaNova representation of mid - level video which 
ming , sunbathing , boxing ) . Therefore , Deselaers et al . ( Pa - 
extends BoVW method offering more pooling operation of 
per “ Bag - of - visual - words models for adult image classifi - 40 information maintenance based on a distribution of the 
cation and filtering ” , T . Deselaers , L . Pimenidis , and H . Ney , 
distance to the keyword , the present invention proposes 
in International Conference on Pattern Recognition ( ICPR ) , 
applying the representation of the Fisher vector - one of the 
pp . 1 - 4 , 2008 ) proposed , for the first time , to pose pornog - 
best mid - level aggregated representations 
currently 
raphy detection as a Computer Vision classification problem 
described in the literature , which extends the BoVW method 
( similar to object classification ) , rather than a skin - detection 45 to encode the first and second order mean differences among 
or segmentation problem . They extracted patches around 
the local descriptors and codebook elements . It is important 
difference - of - Gaussian interest points , and created a visual 
to mention that to the best of our knowledge , for sensitive 
codebook using a Gaussian mixture model ( GMM ) , to 
media content rating , there are no commercial solutions that 
classify images into different pornographic categories . Their 
use this best mid - level representation . Additionally , this 
Bag - of - Visual - Words ( BoVW ) model greatly improved the 50 article “ Pornography Detection Using BossaNova Video 
effectiveness of the pornography classification . 
Descriptor ” does not have most of the advantages offered by 
More recently , Lopes et al . developed a Bag - of - Visual - 
the proposed solution of the present invention : while the 
Words ( BoVW ) approach , which employed the HueSIFT 
method of Caetano et al . detects pornographic content only , 
color descriptor , to classify images ( Paper “ A bag - of - fea 
the present invention proposes a unified and easy structure 
tures approach based on hue - SIFT descriptor for nude 55 for extending to handle any kind of sensitive content ; while 
detection ” , A . Lopes , S . Avila , A . Peixoto , R . Oliveira , and 
the method of Gaetano et al focuses only on the visual 
A . Araujo , in European Signal Processing Conference ( EU 
signal , the present invention provides a method of high level 
SIPCO ) , pp . 1152 - 1156 , 2009 ) and videos ( Paper “ Nude 
multimodal fusion exploring auditory , visual and / or textual 
detection in video using bag - of - visual - features ” , A . Lopes , 
features ; while the method of Gaetano et al . classifies 
S . Avila , A . Peixoto , R . Oliveira , M . Coelho , and A . Araujo , 60 pornographic content in the video as a whole , the present 
in Conference on Graphics , Patterns and Images ( SIB - 
invention proposes a fine - tuning method of classifying sen 
GRAPI 
) , pp . 224 - 231 , 2009 ) of nudity . For video classifi - 
sitive media content in video fragments ( small parts or series 
cation , they proposed a majority - voting scheme over the 
short time of video frames ) . Furthermore , in contrast to the 
video frames . Similar to Dewantono and Supriana ( Paper 
present invention , the method of Caetano et al . cannot be 
“ Development of a real - time nudity censorship system on 65 performed in real - time nor on mobile platforms . 
images " , S . Dewantono and I . Supriana , in International 
Some prior art works rely on bags of static features . Few 
Conference on Information and Communication Technology 
works have applied space - temporal features or other motion 

US 10 , 194 , 203 B2 
information for the classification of pornography . Valle et al . 
ally from 2011 to present , the MediaEval VSD task poses the 
( Paper " Content - based filtering for video sharing social 
challenge of an automated detection of violent scenes in 
networks ” , E . Valle , S . Avila , F . Souza , M . Coelho , and A . 
Hollywood movies and web videos . The targeted violent 
Araujo , in Brazilian Symposium on Information and Com 
scenes are those " one would not let an 8 years old child see 
putational Systems Security ( SBSeg ) , pp . 625 - 638 , 2012 ) 5 
in a video because they contain physical violence ” . 
proposed the use of space - temporal local descriptors ( such 
The violence detection pipeline is typically composed of 
as STIP descriptor ) , in a BoVW - based approach for pornog - 
three steps : ( i ) low - level feature extraction from audio , 
raphy classification . In the same direction , Souza et al . 
visual or textual modalities , ( ii ) mid - level feature extraction 
( Paper “ An evaluation on color invariant based local spa - 
using bag - of - visual - words ( BoVW ) representation or exten 
tiotemporal features for action recognition ” , F . Souza , E . 10 sions , and ( iii ) supervised classification by employing sup 
Valle , G . Camara - Chavez , and A . Araujo , in Conference on 
port vector machines ( SVM ) , neural networks , or hidden 
Graphics , Patterns and Images ( SIBGRAPI 
) , pp . 31 - 36 , 
Markov models ( HMM ) . 
2011 ) improved Valle et al . ’ s results by applying ColorSTIP 
In the 2014 edition of the VSD task , for instance , all 
and HueSTIP , color - aware versions of the STIP detector and 
proposed techniques employed this three - step pipeline , 
descriptor , respectively . Both works established a single bag 15 except for one team , which used the provided violence 
for the entire target video , instead of keeping a bag for each 
related concept annotations as mid - level features . In the 
video frame , prior to voting schemes . 
low - level step , most of the approaches explored both audi 
Very recently , M . Moustafa ( Paper “ Applying deep learn 
tory ( e . g . , MFCC features ) and visual information ( e . g . , 
ing to classify pornographic images and videos " , M . 
dense trajectories ) . Avila et al . ( Paper “ RECOD at Media E 
Moustafa , in Pacific - Rim Symposium on Image and Video 20 val 2014 : Violent scenes detection task ” , S . Avila , D . Mor 
Technology ( PSIVT 
) , 2015 ) proposed a deep learning sys - 
eira , M . Perez , D . Moraes , I . Cota , V . Testoni , E . Valle , S . 
tem that analyzes video frames to classify pornographic 
Goldenstein , and A . Rocha , in Working Notes Proceedings 
content . This work focused on the visual cue only and 
of the MediaEval 2014 Workshop , 2014 ) additionally incor 
applied a majority - voting scheme . 
porated textual features extracted from 
the Hollywood 
In addition , other approaches have employed audio analy - 25 movie subtitles . In the mid - level step , the low - level features 
sis as an additional feature for the identification of porno - 
were frequently encoded using a Fisher Vector representa 
graphic videos . Rea et al . ( Paper “ Multimodal periodicity 
tion . Finally , in the last step , SVM classifiers were the most 
analysis for illicit content detection in videos ” , N . Rea , G . 
used for classification . 
Lacey , C . Lambe , and R . Dahyot 
, in European Conference 
Before the MediaEval campaign , several methods were 
on Visual Media Production ( CVMP 
) , pp . 106 - 114 , 2006 ) 30 proposed to detect violent scenes in video . However , due to 
combined skin color estimation with the detection of peri - 
the lack of a common definition of violence , allied with the 
odic patterns in a video ' s audio signal . Liu et al . ( Paper 
absence of standard datasets , the methods were developed 
“ Fusing audio - words with visual features for pornographic 
for a very specific type of violence ( e . g . , gunshot injury , war 
video detection ” , Y . Liu , X . Wang , Y . Zhang , and S . Tang , in 
violence , car chases ) and , consequently , the results were not 
IEEE International Conference on Trust , Security and Pri - 35 directly comparable . In the following , we overview some of 
vacy in Computing and Communications ( TrustCom ) , pp . 
those works for the sake of completeness . 
1488 - 1493 , 2011 ) demonstrated improvements by fusion 
One of the first proposals for violence detection in video 
visual features ( color moments and edge histograms ) with 
was introduced by Nam et al . ( Paper “ Audio - visual content 
" audio words ” . In a similar fashion , Ulges et al . ( Paper 
based violent scene characterization ” , J . Nam , M . Alghoni 
“ Pornography detection in video benefits ( a lot ) from a 40 emy 
, and A . Tewk , in International Conference on Image 
multi - modal approach ” , A . Ulges , C . Schulze , D . Borth , and 
Processing ( ICIP ) , pp . 353 - 357 , 1998 ) . They combined 
A . Stahl , in ACM International Workshop on Audio and 
multiple audio - visual features to identify violent scenes in 
Multimedia Methods for Large - scale Video Analysis , pp . 
movies , in which flames and blood are detected using a 
21 - 26 , 2012 ) proposed an approach of late fusion motion 
predefined color tables , and sound effects ( e . g . , beatings , 
histograms with audio words . 
45 gunshots , explosions ) are detected by computing the energy 
In addition to those scientific results , there are commer 
entropy . This approach of combined low - level features with 
cial software packages that block web sites with porno - 
specialized detectors for high - level events ( such as flames , 
graphic content ( e . g . , K9 Web Protection , CyberPatrol , 
explosions and blood ) is also applied by paper “ A multi 
NetNanny ) . Additionally , there are products that scan a 
modal approach to violence detection in video sharing 
computer for pornographic content ( e . g . , MediaDetective , 50 sites ” , T . Giannakopoulos , A . Pikrakis , and S . Theodoridis , 
Snitch Plus , NuDetective ) . MediaDetective and Snitch Plus 
in International Conference on Pattern Recognition ( ICPR ) , 
are off - the - shelf products that rely on the detection of human 
pp . 3244 - 3247 , 2010 ; Paper “ Violence detection in movies 
skin to find pictures or movies containing nude people . The 
with auditory and visual cues ” , J . Lin , Y . Sun , and W . Wang , 
work of Polastro and Eleuterio ( a . k . a . , NuDetective , Paper 
in International Conference on Computational Intelligence 
“ NuDetective : A forensic tool to help combat child pornog - 55 and Security ( ICCIS ) , pp . 561 - 565 , 2010 . 
raphy through automatic nudity detection ” , M . Polastro and 
Although most of the approaches on violence detection is 
P . Eleuterio , in Workshop on Database and Expert Systems 
multimodal , previous works ( before MediaEval ) have 
Applications ( DEXA ) , pp . 349 - 353 , 2010 ) also adopts skin 
mainly focused on single modalities . For instance , using 
detection , and it is intended for the Federal Police of Brazil , 
motion trajectory information and orientation information of 
in forensic activities . 
60 a person ' s limbs , Datta et al . ( Paper “ Person - on - person 
Violence Classification 
violence detection in video data ” , A . Datta , M . Shah , and N . 
Over the last few years , progress in violence detection has 
Lobo , in International Conference on Pattern Recognition 
been quantifiable thanks to the MediaEval Violent Scenes 
( ICPR ) , pp . 433 - 438 , 2002 ) addressed the problem of detect 
Detection ( VSD ) task , which provides a common ground 
ing human violence such as first fighting and kicking . Nievas 
truth and standard evaluation protocols . MediaEval is a 65 et al . ( Paper “ Violence detection in video using computer 
benchmarking initiative dedicated to evaluate new algo - 
vision techniques ” , E . B . Nievas , O . D . Suarez , G . B . Garca , 
rithms for multimedia access and retrieval . Organized annu 
and R . Sukthankar , in International Conference on Com 

line . 
US 10 , 194 , 203 B2 
10 
puter Analysis of Images and Patterns ( CAIP ) , pp . 332 - 339 , 
whether a video has sensitive content , but it also outputs the 
2011 ) employed a BoVW framework with MoSIFT features 
localization of the sensitive content within the video time 
to classify ice hockey clips . By exploiting audio features , 
Cheng et al . ( Paper “ Semantic context detection based on 
3 . Powerful mid - level video snippet representations : The 
hierarchical audio models ” . W . - H . Cheng , W . - T . Chu , and 5 method of the present invention employs powerful mid - level 
J . - L . Wu 
, in International Workshop on Multimedia Infor 
representations to encode the video snippets , as a manner to 
mation Retrieval ( MIR ) , 190 - 115 , 2003 ) recognized gun 
reduce the semantic gap between the low - level content 
shots , explosions and car - braking using a hierarchical 
representation ( e . g . , frame pixels , audio signals ) , and the 
approach based on GMM and HMM . Pikrakis et al . ( Paper 
high - level target concept ( e . g . , pornography , violence ) . 
" Gunshot detection in audio streams from movies by means 10 
4 . High - level fusion of characterization and classification 
methods : As aforementioned , the method of the present 
of dynamic programming and bayesian networks ” , A . Pikra 
invention analyzes visual , auditory , and textual video fea 
kis , T . Giannakopoulos , and S . Theodoridis , in IEEE Inter 
tures , to classify its content . The features are analyzed 
national Conference on Acoustics , Speech and Signal Pro 
independently , and then fused in a novel decision method , 
cessing ( ICASSP ) , pp . 21 - 24 , 2008 ) proposed a gunshot 15 that is statistically learned from the training dataset . This 
detection method based on statistics of audio features and 
fusion method allows the selection of diverse low - level 
Bayesian networks . 
feature descriptors ( e . g . , SIFT or SURF image descriptors , 
MFCC audio descriptor ) , and even the add - in of yet - to - be 
SUMMARY OF THE INVENTION 
developed future descriptors , in a very seamless way . 
20 
5 . Robustness to small sample sized ( SSS ) problems : The 
The present invention refers to a fine - grained classifica - 
method of the present invention can be learned with just a 
tion method of sensitive media content in video snippets 
few hours of video footage , keeping the generalization 
( short temporal series of video frames ) . The invention 
ability of the learned model to classify examples never seen 
classifies video snippets in sensitive ( a . k . a , positive , i . e . , 
by the learning algorithm during training phase . 
unsafe for disclosure ) or non - sensitive ( a . k . a . , negative , i . e . , 25 
6 . Large - scale content analysis : The method of the present 
safe for disclosure ) , according to the hints extracted from 
invention can be embodied as software that may run on 
multiple modalities of video features : 
plenty of platforms . If one disposes of enough computa 
1 . Visual features ( e . g . , the amount of light that was 
tional power , the solution can analyze thousands of video 
incident on the pixels of the frames , at the moment of 
samples at the same time , therefore attending eventually 
capture ) . 
30 high demands of Internet applications . 
2 . Auditory features ( eg soundtrack and special effects ) . 
7 . Deployable into low - powered devices ( e . g . , smart 
3 . Textual features ( e . g . , subtitles and closed caption ) . 
phones , tablets , smart TVs , smart glasses , Virtual Reality 
The method execution requires two phases : offline or 
devices , Etc . ) with Complete Local Execution : The method 
online . Initially , in the offline operation , a mathematical 
of the present invention can be performed into low - powered 
35 devices , i . e . , the end - to - end framework can be effectively 
model is statistically learned from the training / learning 
executed in place ( locally ) , without sending and retrieving 
dataset . This model is able to separate the positive samples 
data from the cloud ( remotely ) . 
from the negative ones , in a generalizable manner ( i . e . , the 
8 . Low memory , storage space , and computational foot 
model is able to predict the class of unknown video samples 
print : The method of the present invention is susceptible of 
with a high accuracy ) . The offline operation receives the 40 embodiments that purposely emplov , in each step . only 
training dataset as input , and it outputs the statistically 
solutions that present low memory usage and low compu 
learned mathematical model . It is supposed to be executed 
tational footprint , yet maintaining acceptable classification 
only a few times ( whenever the training / learning dataset is 
accuracy . Examples may comprise , but are not limited to , the 
updated ) . 
use of fast video descriptors , and / or cheap mid - level aggre 
In the online operation , an unknown video sample is input 45 gation representations , and / or small data classifiers . Besides , 
to the method , together with the mathematical model . The 
a storage space of up to 1 . 5 MB is suitable to locally store 
model is thus used to predict the moments the input video 
the prediction models and detect sensitive content in real 
becomes inappropriate for disclosure ( i . e . , the moments its 
time . 
content becomes sensitive ) . The online operation must be 
9 . No ad - hoc preprocessing : The method of the present 
performed every time a new unknown video is presented to 
50 invention does not require any specific environment - , con 
the method , and it may be executed locally on the target 
tent - or capture - dependent preprocessing of the videos to be 
electronic devices ( e . g . , smartphones , tablets , smart glasses , 
analyzed . It works for different video resolutions , frame 
Virtual Reality devices and smart TVs 
) . There is no need to 
rates , orientations , or lightings . 
send or retrieve data from a remote server during the online 
10 . No human supervision : The method of the present 
operation . 
55 invention is entirely executed with no need of human 
The novel and differential points of the present method 
supervision . It eliminates the necessity of hiring employees 
rely on the following issues : 
for the tedious task of content supervision , which depending 
1 . Unified and easy - to - extend framework : The method of 
on the kind of suppressed material , may indeed be upsetting . 
the present invention defines a universal protocol for dealing 
The previously mentioned objectives are achieved by a 
with any type of sensitive content , in other words 
, it does not 60 method for filtering sensitive content , which receives as 
work exclusively for a particular sensitive media content 
input a digital video stream , comprising the following steps : 
( e . g . , only pornography , or only violence ) , it can be extended 
segmenting the digital video into video fragments along 
to some other content - filtering approaches . 
the video timeline ; 
2 . Detection and localization of sensitive content in video 
extracting the features containing significant information 
snippets faster than real - time : The method of the present 65 from the input digital video on sensitive media ; 
invention performs a fine - grained classification of sensitive 
reducing the semantic difference between each of the low 
media content in video snippets . It does not only output 
level video features , and high level sensitive concept ; 

11 
US 10 , 194 , 203 B2 
12 
classifying video fragments issuing a positive or negative 
but is to be accorded the widest scope consistent with the 
high - level label , with a confidence score for each fragment 
principles and features herein disclosed . 
representation ; 
The detailed description of the present invention follows 
high - level fusing to deeply combine the possible high - 
a top - down approach . Hence , we start with the disclosure of 
level labels and confidence scores for each fragment ; and 
5 two sample embodiments ( FIGS . 1 and 2 ) , to clarify the 
predicting the sensitive time by combining the labels of 
purpose of the invention . In the sequence , we depict an 
the fragments along the video timeline , indicating the times 
overview of the proposed method ( FIG . 3 ) , and then we 
when the content becomes sensitive . 
delve into the details of the offline and online operations for 
The method of the present invention goes beyond the 
each type of extracted low - level features ( visual , auditory , 
existing solutions in the prior art , by proposing new efficient 10 and textual ) ( FIGS . 4 and 5 ) . In the end , we explain the 
and effective characterization techniques for sensitive media 
high - level technique of fusion ( FIG . 6 ) . 
analysis . The present invention discloses a classification 
FIG . 1 is a flowchart that depicts a possible embodiment 
method for sensitive media without using the trivial 
of the present invention . The solid arrow represents the 
approach based on skin detection , detecting and localizing 
sequence of events ( 2 ) within the embodiment execution . 
sensitive content in real - time , on low - powered devices ( e . g . , 15 
The action starts with a user 1 using her smartphone 2 , 
smartphones , tablets , smart glasses . Virtual Reality devices , 
where a system implementing the proposed invention was 
smart TVs , cameras ) , using low memory and computational 
previously deployed in the form of a scanning app 3 . The 
footprint . It presents better efficiency and performance than 
app locates all the video files stored in the device and 
current / existing solutions for sensitive ( pornography and / or 
additional videos that can be stored in memory cards 4 , and 
violence ) media analysis . 
20 starts scanning them to identify which files present sensitive 
As will be detailed , the method has a classification 
content ( e . g . , violence , pornography ) . 
accuracy higher than 90 % for pornography , higher than 80 % 
The progress of the file scanning can be checked by the 
for violence , and the analysis takes less than one second per 
means of a progress bar 5 , and the sensitive videos are 
analyzed frame in a mobile platform 
( which has computa 
iteratively enlisted 6 . One can note that the smartphone 2 
tional / hardware restrictions ) . The method does not require 25 may stay offline during the entire scanning process ( what is 
the analysis of all frames in the video in order to reach its 
shown by the means of flight mode 7 and no wireless 
high accuracy . Just one frame must be analyzed per second . 
connections 8 ) . It means that the scanning and sensitive 
For instance , if the video has 30 or 60 frames per second , the 
content detection processes are performed locally , with no 
required rate of frames to be analyzed is 1 every 30 or 60 
need of additional processing steps in external or remote 
frames . Therefore , in real - time execution , the analysis time 30 machines , despite eventual memory and processing restric 
is always lesser than video timeline . 
tions of the smartphone . 
In the process of sensitive video detection , visual , audi 
BRIEF DESCRIPTION OF THE DRAWINGS 
tory and / or textual features are extracted from the video 
files , to support the app execution . 
The objectives and advantages of the present invention 35 
FIG . 2 is a flowchart that depicts another possible embodi 
will become clearer through the following detailed descrip - 
ment of the present invention . The solid arrows represent the 
tion of the example and non - limitative figures presented at 
sequence of events ( 3 ) within the embodiment execution . 
the end of this document 
, wherein : 
The action starts with a user 1 operating her Smart TV 10 
FIG . 1 is a flowchart that depicts a sample embodiment of 
with a regular remote control 9 . The equipment runs under 
the present invention on a smartphone . 
40 a kid ' s safe mode ( represented by a proper status icon 11 ) . 
FIG . 2 is a flowchart that depicts a sample embodiment of 
When activated , the kid ' s safe mode provides — as a back 
the present invention on a Smart TV . 
ground service a real - time analysis of every video stream 
FIG . 3 is a flowchart that depicts the overview operation 
that the TV is demanded to play . The kid ' s safe mode can be 
of the present invention . 
activated and deactivated by the means of the TV settings 12 
FIG . 4 is a flowchart that depicts the offline operation of 45 or by a “ Safe mode button / function ” in the remote control 9 , 
the present invention , which corresponds to the training 
and it runs locally , with no need of connections to remote 
phase of the method . 
servers or external databases . 
FIG . 5 is a flowchart that depicts the online operation 
The user 1 chooses to watch a regular web video stream 
( connected ) according to an embodiment of the proposed 
13 , what leads to a video content being played 14 . Given that 
invention , which corresponds to the execution phase ( regu - 50 the TV is under safe mode , the video content is always 
lar use ) of the method . 
analyzed before disclosure , in a manner that is transparent to 
FIG . 6 is a flowchart that depicts the high - level fusion 
the user . Without any human supervision or awareness , 
solution according to an embodiment of the proposed inven 
whenever the video becomes sensitive , the visual and audi 
tory contents are censored on demand 15 . 
In the process of sensitive content detection , visual , 
DETAILED DESCRIPTION OF THE 
auditory and / or textual features are extracted from the video 
INVENTION 
streams , to support the service execution . 
FIG . 3 is a flowchart that depicts the overview operation 
The following description is presented to enable any 
of the method of the present invention . Each rectangular box 
person skilled in the art to make and to use the embodiments , 60 is an activity , and the arrows represent the precedence of 
and is provided in the context of particular applications and 
activities . Some activities are interleaved by black icons 16 , 
their requirements . Various modifications to the disclosed 
17 , 18 and 19 , that put in evidence the type of data that shall 
embodiments will be readily apparent to those skilled in the 
flow between the activities . Dashed arrows represent a 
art , and the general principles defined herein may be applied 
simple flow of data , and a parallelogram represents output . 
to other embodiments and applications without departing 65 
Regardless of being offline or online , the method opera 
from the spirit and scope of the present disclosure . Thus , the 
tion 100 starts from the Digital Video file or stream 16 , 
present invention is not limited to the embodiments shown , 
which is segmented in video snippets along the video 
tion . 

US 10 , 194 , 203 B2 
13 
14 
timeline . These snippets may have fixed or varied temporal 
represent any sort of storage device . The flowchart is generi 
length , and they may or may not present temporal overlap . 
cally represented to deal with visual , auditory and textual 
Once the snippets are produced by the Video Snippet 
information , since the step sequences are similar , and par 
Segmentation activity 110 , then Features Extraction activity 
ticularities regarding each type of information will be prop 
120 is performed , in order to generate a set of features ( a . k . a . 5 
erly described when necessary . 
" feature vectors ” ) which contains the significant information 
The depicted operation is offline , what means that it aims 
from the input data ( i . e . , the Digital Video 16 ) regarding 
at training the method . This Training Phase ( offline opera 
sensitive media . Each one of the snippets is subject to three 
tion ) must be done before the regular execution ( online 
types of low - level Feature Extraction 120 : 
operation ) , in order to generate a mathematical model that is 
1 . Visual Feature Extraction 122 regards the processes 10 able to predict the class of unknown videos that will be 
that analyze the Frames 17 of the video snippets , previously 
analyzed during the regular execution ( online operation ) . 
extracted by Frame Extraction 121 . These processes include 
The offline operation thus starts by taking known and 
any type of global or local still - image description method , 
previously labeled training samples , which are stored as 
interest point descriptor , or space - temporal video descrip - 
either Positive Video Snippets 20 , or Negative Video Snip 
tion solution that may be available to the method embodi - 15 pets 21 . 
ment 
. 
Following , video information ( frames , audio and / or text ) 
2 . Auditory Feature Extraction 124 is related to the 
is extracted in the Frame 
/ Audio / Text Extraction activities 
processes that analyze the Audio 18 of the video snippets , 
121 , 123 , 125 . At this point , if the video information is 
previously extracted by Audio Extraction 123 . These pro - 
visual , any type of frames may be used ( e . g . : I , P or B 
cesses include any type of audio description solution ( e . g . , 20 frames , or even all the frames taken at a chosen or random 
MFCC ) that may be available to the method embodiment 
frame rate ) . Moreover , the frame resolution may be simply 
3 . Textual Feature Extraction 126 concerns the processes 
maintained , or it may be reduced , whether for the sake of 
that analyze any Text 19 that may be associated to a video 
any further savings of computational time , or for any other 
snippet ( e . g . , subtitles and closed caption ) , previously 
reasons . 
extracted by Text Extraction 125 . These processes include 25 
In the sequence , the Visual / Audio / Text Feature Extraction 
any type of text description solution ( e . g . , stem frequency , 
activities 122 , 124 , 126 are performed . In case of visual 
etc . ) that may be available to the method embodiment . 
information ( frames ) , it provides the low - level description 
The activities of feature extraction 120 , and more spe 
of the extracted frames , by the means of any type of global 
cifically items 122 , 124 , and 126 conclude the low - level 
or local still - image descriptor , interest point descriptor , or 
stage of the proposed method ( Low - level Feature Extraction 30 space - temporal video descriptor . Typical examples from the 
stage , in FIG . 3 ) . 
literature may include ( but are not limited to ) Scale Invariant 
In the sequence , each possible process of low - level fea - 
Feature Transform ( SIFT ) , Speeded Up Robust Features 
ture extraction follows an independent path through the 
( SURF ) , Histogram of Oriented Gradients ( HOG ) , Space 
Video Snippet Mid - level Aggregate Representation 130 , 
Temporal Interest Points ( STIP ) , etc . In case of auditory 
which is responsible for reducing the semantic gap that 35 information ( audio ) , it provides the low - level description of 
exists between each one of the low - level video features , and 
the audio snippets , and solutions to perform it may include 
the high - level sensitive concept . For doing so , it constitutes 
( but are not limited to ) Mel - frequency Cepstral Coefficients 
the mid - level stage of the method operation ( Mid - level 
( MFCC ) , brightness , tonality , loudness , pitch , etc . 
Video Snippet Representation stage , in FIG . 3 ) . More details 
As a result of the feature extraction , the extracted infor 
on the mid - level representation ( 130 ) are given in FIGS . 4 40 mation ( visual , auditory and / or textual ) is translated into 
and 5 , which will be further explained . 
feature vectors , which are susceptible to the application of 
The Video Snippet Classification activity 140 , on turn , 
diverse algebraic transformations that may enhance the 
outputs a high - level label ( positive or negative ) , with a 
quality of data ( e . g . , decorrelate the feature vector compo 
confidence score , for each snippet representation . It thus 
nents , etc . ) . This is the aim of the Dominant Component 
starts the high - level stage of the proposed method ( High - 45 Analysis activity 131 , which analyses the numeric behavior 
level Snippet Classification stage , in FIG . 3 ) . 
of the feature vector components , and estimates algebraic 
Given that each snippet may have various representa 
transformations that may improve further separations of data 
tions — and therefore , various high - level labels and confi 
samples . An example of doing so is by the application of 
dence scores — the High - level Fusion activity 150 is respon - 
Principal Component Analysis ( PCA ) , but it is not limited to 
sible for taking the labels of the snippets and combining 50 that . Because of such step , parameters of the chosen alge 
them along the video timeline , in order to obtain the 
braic transformation are learned ( a . k . a . , estimated ) from the 
moments when the content becomes sensitive . In the end , 
training dataset , and they need to be stored for further use 
the Sensitive Moment Prediction 160 outputs the prediction 
( what leads to the Projection Transformation data 22 ) . 
of the sensitive video moments , what concludes the High 
Once the parameters of the algebraic transformation are 
level Fusion stage , in FIG . 3 . More details on the high - level 55 learned , the feature vectors are projected onto another vector 
fusion are given in FIG . 6 . 
space , task that is related to the Data Projection activity 132 . 
It is noteworthy to mention that the present method does 
Besides that , for the sake of saving computational time , it is 
not work exclusively for a particular type of sensitive 
common ( but it is not an indispensable requirement ) to 
content ( e . g . , only for pornography , or only for violence ) . It 
project the feature vectors to another space that presents less 
works for any concept of interest . 
60 components than the original one ( i . e . , the feature vectors 
Offline or Disconnected Execution 
( Training / Learning 
are converted to smaller vectors , a . k . a . dimensionality 
Phase ) 
reduction ) 
FIG . 4 is a flowchart that depicts the offline operation of 
Prior to the mid - level aggregation representation of the 
the proposed method , which corresponds to the training 
low - level features , there is the necessity to construct the 
phase of the method . Each rectangular box is an activity , and 65 Codebook 23 , for posterior reference . Such task is linked to 
the solid arrows represent the precedence of activities . 
the Codebook Construction activity 133 , and usually there 
Dashed arrows represent a simple flow of data , and cylinders 
may be a codebook for each type of video information 

16 
US 10 , 194 , 203 B2 
15 
( visual , auditory and textual ) . There , the basic idea is to 
The described operation is online , what means that it 
somehow split the space of low - level descriptions into 
represents a regular use of the method , when an Unknown 
multiple regions , where each region is associated to a 
Digital Video 25 is presented for analysis . As mentioned , at 
visual / auditory / textual word . Thus , by the storage of these 
this point , the training phase or offline operation ( depicted in 
visual / auditory / textual words , we have a representative 5 FIG . 4 ) was already done . 
codebook 23 . Strategies to construct the codebook may vary 
In the Low - level Feature Extraction stage , the video is 
a lot . For instance , they may comprise ( but are not limited 
first segmented into video snippets , along the video timeline 
to ) unsupervised learning techniques , such as k - means clus 
( Video Snippet Segmentation activity 110 ) . As mentioned in 
tering , or other clustering method ( e . g . , k - medians ) , etc . In 
the Method Overview ( FIG . 3 ) , these snippets may have 
a different fashion , other solution developers manage to use " 
fixed or varied temporal length , and they may or may not 
even simpler strategies , such as randomly sampling the 
present temporal overlap . 
description space , in order to raffle k representatives . Addi - 
In the sequence , Frame / Audio / Text Extraction ( activities 
tionally , more sophisticated strategies can also be used , such 
121 , 123 , 125 ) and Visual / Audio / Text Feature Extraction 
as the application of an Expectation - Maximization ( EM ) 15 ( activities 122 , 124 , 126 ) must be performed in the same 
algorithm to establish a Gaussian Mixture Model ( GMM ) on 
way as in the offline operation ( please refer to FIG . 4 ) . 
the low - level description space . In addition , content - aware 
Thereafter , Data Projection 132 , Mid - level Encoding 134 , 
approaches may be employed , where the codebook con - 
and Snippet Pooling 135 — that are also the same performed 
struction is done by the selection of a controlled number of 
in the offline operation ( see FIG . 4 ) — are executed one after 
representative feature vectors from each known problem 20 the other . These steps 132 , 134 and 135 are sub - tasks of 
class . 
Video Snippet Mid - level Aggregate Representation 130 
Once the codebook 23 is obtained , the next step comprises 
( FIG . 3 ) , and constitutes the Mid - level Video Snippet Rep 
the Mid - level Encoding activity 134 . This step aims at 
resentation stage . Please notice that , at this stage , the pre 
quantifying every low - level feature vector extracted from 
viously learned ( during the offline operation , training phase , 
the frames / audio / text ( previously on activities 122 , 124 , 25 FIG . 4 ) Projection Transformation 22 and Codebook 23 are 
126 ) , with respect to their similarity to the words that 
read / retrieved by activities Data Projection 132 and Mid 
compose the codebook 23 . Techniques to do that may 
odebook 23 . Techniques to do that may 
level Encoding 134 , respectively . 
include ( but are not limited to ) hard - or soft - coding , and 
In the end , in the High - level Video Snippet Classification 
Fisher Vectors . 
stage , the labels of each unknown video snippet are pre 
The following step , Snippet Pooling 135 , aggregates the 30 dicted , with a confidence score , based on the Prediction 
quantization obtained in the previous encoding step , by 
Model 24 that was previously learned / estimated in the 
summarizing in a single feature vector for each video 
offline operation ( FIG . 4 ) . The prediction task is thus related 
snippet — how often the visual / auditory / textual words are 
to the Video Snippet Class Prediction activity 142 , and it 
being manifested . Strategies to do that may include ( but are 
depends on the machine learning technique used to generate 
not limited to ) sum , average or max pooling . 
The steps 131 - 135 are considered sub - tasks of Video 
the Prediction Model 24 . Alternatives may comprise ( but are 
Snippet Mid - level Aggregate Representation 130 . 
not limited to ) Support Vector Machines ( SVM ) , Random 
Finally , from the mid - level aggregate representation of 
Forests , decision trees , etc . 
each training video snippet — whose labels are known in 
Despite it is not illustrated in FIG . 5 , the online operation 
advance — a supervised machine learning technique can be 40 of the proposed method continues to the next , final steps ( as 
employed to deduce a “ good ” video snippet classification 
depicted in FIG . 3 ) . Given that each snippet may have 
model ( i . e . , a mathematical model that is able to predict , 
various representations — and therefore , various high - level 
with high accuracy and enriched by a confidence score , the 
labels and confidence scores provided by the previous step 
label of unknown video snippets ) . That is related to the 
( Video Snippet Class Prediction 142 ) — , the High - level 
Prediction Model Generation activity 141 , and the learned / 45 Fusion activity 150 is responsible for soundly combining 
estimated Prediction Model 24 must be stored for further use 
them in 
a single answer . Then , in the end , the Sensitive 
( regular , online operation / execution ) . Usually , there may be 
Moment Prediction 160 outputs the prediction of the 
a Prediction Model 24 for each type of video information 
moments when the content becomes sensitive ( i . e . , pornog 
( visual , auditory and textual ) . Many machine learning solu - 
raphy , violence , adult content or any other concept of 
tions may be applied to this last classification process . 50 interest that was previously trained and modeled by the 
Alternatives may comprise ( but are not limited to ) Support 
offline operation of the proposed method ) . More details on 
Vector Machines ( SVM ) , including the many SVM varia 
the high - level fusion are given in FIG . 6 . 
tions regarding the type of kernel function that is used to 
in the online operation , when the proposed method 
learn the data separation hyperplane , Random Forests , deci - 
detects sensitive content within an Unknown Digital Video , 
sion trees , etc . 
55 many actions can be taken in order to avoid the presentation 
Online or Connected Execution ( Regular Use , or Execution 
of undesirable content , for instance ( but not limited to ) 
Phase ) 
substitute the set of video frames with sensitive content by 
FIG . 5 is a flowchart that describes the online operation of 
completely black frames , blurring the sensitive video 
the proposed method , which corresponds to the execution 
frames , or displaying an alert / warning . 
phase ( regular use ) of the method . Each rectangular box is 60 High - level Fusion Solution 
an activity , and the solid arrows represent the precedence of 
FIG . 6 is a flowchart that describes the high - level fusion 
activities . Dashed arrows represent a simple flow of data , 
solution 150 of the method of the present invention . Each 
and cylinders represent any sort of storage device . The 
rectangular box is an activity , and the solid arrows represent 
flowchart is generically represented to deal with visual , 
the precedence of activities . Dashed arrows represent a 
auditory and textual information , since the steps sequence 65 simple flow of data , and cylinders represent any sort of 
are similar , and particularities regarding each type of infor - 
storage device . The diamond , in turn , represents a condi 
mation will be properly described when necessary . 
tional branch , which provides two different paths on the 

17 
US 10 , 194 , 203 B2 
18 
flow : one for the offline method operation , and another for 
Concerning the online operation , an unknown video 
the online operation . A parallelogram represents input / out - 
sample and its respective video snippets have their classi 
put . 
fication scores properly combined into the N - dimensional 
As it is shown by the means of items 26 to 29 , the fusion 
score vectors ( on activity 152 ) . At this point , it is important 
starts from the class predictions of diverse video snippets , 5 to mention that the order in which the outputs of the video 
which are grouped accordingly to the low - level feature 
snippet classifiers are combined must be the same that was 
extraction method that was employed to describe them . 
adopted in the offline fusion operation . 
Therefore , item 26 , for instance , may refer to the output 
Thereafter , the N - dimensional Vector Class Prediction 
activity 154 retrieves the Late Fusion Model 32 , and predicts 
predictions of a visual - based video snippet classifier that 
relied on SIFT ( Low - level Feature 1 ) to describe the video 10 the labels of each N - dimensional vector , with a proper 
confidence score . Given that each N - dimensional vector 
content at the low - level stages of the proposed method . 
represents an instant of interest within the unknown video , 
Similarly , item 27 may refer to the outputs of an auditory 
the predicted labels actually predict every instant of interest 
based classifier that relied on MFCC ( Low - level Feature 2 ) , 
of the video . 
while item 28 may refer to a visual - , SURF - based video 15 
Notwithstanding , giving a classification confidence score 
snippet classifier ( Low - level Feature 3 ) . Finally , item 29 
for every video instant of interest may generate a very noise 
for every video in 
may refer to a textual - based classifier ( Low - level Feature 
answer in time , with interleaving positive and negative 
N ) . The number N of fused classifiers may be even or odd , 
segments at an unsound rate that may change too much and 
ranging from a single one , to a ton of classifiers . Moreover , 
too much fast , regarding the actual occurrence of enduring 
the nature of the employed low - level features may be any of 20 and relevant sensitive events . Hence , in the Classification 
the possible ones ( either visual , or auditory , or textual ) , no 
Score Noise Suppression activity 155 , any kind of denoising 
matter their order , majority or even absence ( no use of 
function can be used to flatten the classification score , along 
textual features , for instance ) . 
the video timeline . Strategies to do that may include ( but are 
In the sequence , the outputs of the N video snippet 
not limited to ) the use of Gaussian blurring functions . 
classifiers 26 to 29 may be aligned along the video timeline , 25 
Next , the Classification Score Fusion activity 156 aims at 
as a manner to organize how the different classifiers evalu - 
combining the scores of adjacent video instants of interest 
ated the sensitiveness of the video content . This is the task 
that belong to the same sensitive class , according to decision 
related to the optional Snippet Temporal Alignment activity 
thresholds . The inherent idea , therefore , is to substitute the 
151 , which presumes that the video snippets have a refer - 
sequences of diverse scores by a single and representative 
ence time ( i . e . , an instant within the original video timeline 30 one , which may persist for a longer time 
, thus better char 
that the snippet is more representative of ) . A snippet refer - 
acterizing the sensitive or non - sensitive video moments . 
ence time may be the first instant of the snippet , but 
Strategies to do that may comprise ( but are not limited to ) 
alternatives may consider the most central or even the last 
assuming a score threshold t , and then substituting all the 
instant . 
time adjacent scores equal to or greater than t by their 
Next , an N - dimensional vector is constructed for every 35 maximum ( or average ) value , and all the time adjacent 
instant of interest of the target video ( e . g . , for every second 
scores smaller than t by their minimum ( or average ) value . 
of video ) . Within this vector , every i - th component ( with i 
Finally , the Sensitive Moment Prediction 160 outputs the 
belonging to the natural interval [ 1 . . . N ] ) must hold the 
prediction of the moments when the content becomes sen 
classification confidence score of the i - th snippet classifier , 
sitive ( i . e . , pornography , violence , adult content or any other 
regarding the video snippet whose reference time coincides 40 concept of interest that was previously trained and modeled 
with the instant of interest . In the case of missing snippets , 
by the offline operation of the proposed method ) . 
the confidence score may be assumed as a value of complete 
Experiments and Results 
uncertainty ( e . g . , 0 . 5 , in the case of a normalized confidence 
In the context of the experiments using the proposed 
score , which varies from zero — i . e . , no confidence at all — to 
method of the present invention , we report the results for 
onei . e . , total confidence ) , or it may be interpolated . Such 45 pornography classification on Pornography - 2K dataset . It 
task of N - dimensional vector representation is related to the 
comprises nearly 140 hours of 1000 pornographic and 1000 
N - dimensional Vector Representation activity 152 . 
non - pornographic videos , which varies from six seconds to 
In the offline operation ( training / learning phase ) of the 
33 minutes . 
method , various training video samples and their respective 
To evaluate the results of our experiments , we apply a 
classified snippets have their classification scores combined 50 5x2 - fold cross - validation protocol . It consists of randomly 
into these N - dimensional score vectors . Considering that 
splitting the Pornography - 2K dataset five times into two 
each N - dimensional vector represents an instant of interest 
folds , balanced by class . In each time , training and testing 
within the target video , the labels of such vectors are 
sets are switched and consequently 10 analyses for every 
deductible from the training dataset groundtruth 30 , 31 , as 
model employed are conducted . 
long as the training dataset is annotated at frame level . 55 
The method of the present invention has a classification 
Therefore , the Late Fusion Model Generation activity 153 
accuracy of 96 % for pornography , and the analysis takes 
receives the training dataset groundtruth ( represented by the 
about one second per analyzed frame in a mobile platform 
Positive Groundtruth and Negative Groundtruth storages , 
( which has computational / hardware restrictions ) . The 
respectively 30 and 31 ) , and employs a supervised machine 
method does not require the analysis of all frames in the 
learning technique to generate a good late fusion model : i . e . , 60 video in order to reach its high accuracy . Just one frame must 
a mathematical model that is able to predict , with high 
be analyzed per second . For instance , if the video has 30 or 
accuracy and enriched by a confidence score , the label of an 
60 frames per second , the required rate of frames to be 
unknown N - dimensional vector . The learned Late Fusion 
analyzed is 1 every 30 or 60 frames . Therefore , in real - time 
Model 32 must be stored for further use ( during regular , 
execution , the analysis time is always lesser than video 
online use / execution ) . At this point , many machine learning 65 timeline . 
solutions may be applied , for instance ( but not limited to ) 
Regarding violence classification , as mentioned , there is a 
SVM , Random Forests , and decision trees . 
lack of a common definition of violence , absence of standard 

US 10 , 194 , 203 B2 
19 
20 
datasets , and the existing methods were developed for a very 
with a reference instant of interest , wherein i 
specific type of violence ( e . g . , gunshot injury , war violence , 
belongs to the natural interval [ 1 . . . N ] ) ; 
car chases ) . Consequently , the results were not directly 
in an offline operation , generating a late fusion model 
comparable . For this reason , the proposed method was tested 
from a training dataset , employing a supervised 
on a benchmarking initiative dedicated to evaluate new 
5 
machine learning method on the generated late 
methods to automated detection of violent scenes in Holly 
fusion model to generate a good late fusion model , 
wood movies and web videos , called MediaEval Violent 
and storing the good late fusion model ; and 
Scenes Detection ( VSD ) task , which provides a common 
in an online operation , retrieving the stored good late 
ground truth and standard evaluation protocols . The pro 
fusion model and using the retrieved good late 
posed method obtained a classification accuracy of 87 % for 10 
fusion model , a de - noising function , and a prede 
violence . 
termined threshold to correlate the generated high 
These results represent an efficient and effective classifi 
level labels and the confidence scores for each 
cation of diverse sensitive media on mobile platforms . 
Applications 
classified video fragment . 
There are many applications for the method of the present 15 . 2 . The method of claim 1 , wherein the low - level features 
invention : 
include visual , auditory , and text features . 
detecting , via surveillance cameras , inappropriate or vio - 
3 . The method of claim 
1 , wherein the reducing the 
lent behavior ; 
semantic difference comprises : 
blocking undesired content from being uploaded to ( or 
analyzing dominant components by transforming the 
downloaded from ) general purpose websites ( e . g . , social 20 
extracted low - level features into a feature vector ; 
networks , online learning platforms , content providers , 
projecting the feature vector into another vector space ; 
forums ) , or from being viewed on some places ( e . g . , schools , 
building a codebook by splitting a space of low - level 
workplaces ) ; 
descriptions in various regions into words , and storing 
preventing children from accessing adult content on per 
these words in the codebook ; 
sonal computers , smartphones , tablets , smart glasses , Virtual 25 
mid - level coding to quantify the feature vector by a 
Reality devices , or smart TVs ; and 
similarity to the stored words in the codebook ; and 
avoiding that improper content is distributed over phones 
grouping the video fragments by aggregating the quanti 
by sexting , for instance . 
fied feature vector . 
Although the present invention has been described in 
4 . The method of claim 
1 , wherein the reducing the 
connection with certain preferred embodiments , it should be 30 semantic difference comprises data projection , mid - level 
understood that it is not intended to limit the invention to 
coding , and grouping of the video fragments using a code 
those particular embodiments . Rather , it is intended to cover 
book . 
all alternatives , modifications and equivalents possible 
5 . The method of claim 1 , wherein the classifying the 
within the spirit and scope of the invention as defined by the 
video fragments comprises offline generating a prediction 
appended claims . 
35 model which applies a supervised machine learning tech 
The invention claimed is : 
nique to deduce an ideal video fragments classification 
1 . A method comprising : 
model 
. 
performing , by at least one processor , operations includ - 
6 . The method of claim 
1 , wherein the classifying the 
ing : 
video fragments comprises predicting a video segment class , 
segmenting a digital video stream into video fragments 40 wherein labels for each video segment are predicted with a 
along a video timeline ; 
confidence score based on a prediction model . 
extracting low - level features containing significant 
7 . The method of claim 
1 , wherein the operations are 
information on sensitive media from the video frag - 
performed in real time on at least one of smartphones , 
ments ; 
tablets , smart glasses , virtual reality devices , displays , and 
reducing a semantic difference between the extracted 45 smart TVs . 
low - level features , and a high - level concept ; 
8 . The method of claim 1 , wherein the video fragments 
classifying the video fragments , and generating a high - 
have at least one of a varied temporal size and a temporal 
level label as either positive or negative and a 
overlap . 
confidence score for each classified video fragment ; 
9 . The method of claim 
3 , wherein parameters in the 
performing high - level fusion to correlate the generated 50 analyzing the dominant components are learned from 
a 
high - level labels and the confidence scores for each 
training dataset , and stored in a projection transformation 
classified video fragment ; and 
dataset . 
identifying a particular content of the digital video 
10 . The method of claim 
1 , wherein the supervised 
stream by combining the generated high - level labels 
machine learning method comprises at least one of : support 
of the classified video fragments along the video 55 vector machine , Random Forests , and decision trees . 
timeline , 
11 . The method of claim 1 , wherein the confidence score 
wherein the performing the high - level fusion com - 
is interpolated . 
prises : 
12 . 
A 
non - transitory 
computer - readable 
recording 
temporally aligning N classified video fragments 
medium storing a program to implement a method compris 
along the video timeline ; 
60 ing : 
representing an N - dimensional vector , which builds 
performing , by at least one processor , operations includ 
an N - dimensional vector for each instant of inter 
ing : 
est of the digital video stream , and within the 
segmenting a digital video stream into video fragments 
N - dimensional vector , every i - th component holds 
along a video timeline ; 
a classification confidence score of the i - th clas - 65 
extracting low - level features containing significant 
sified video fragment , in relation to a video frag 
information on sensitive media from the video frag 
ment having a reference moment which coincides 
ments ; 

21 
US 10 , 194 , 203 B2 
22 
reducing a semantic difference between the extracted 
extracting low - level features containing significant 
low - level features and a high - level concept ; 
information on sensitive media from the video frag 
classifying the video fragments , and generating a high 
ments ; 
level label as either positive or negative and a 
reducing a semantic difference between the extracted 
confidence score for each classified video fragment ; 5 
low - level features and a high - level concept ; 
performing high - level fusion to correlate the generated 
classifying the video fragments , and generating a high 
high - level labels and the confidence scores for each 
level label as either positive or negative and a 
classified video fragment ; and 
confidence score for each classified video fragment ; 
identifying a particular content of the digital video 
performing high - level fusion to correlate the generated 
stream by combining the generated high - level labels 10 
of the classified video fragments along the video 
high - level labels and the confidence scores for each 
classified video fragment ; and 
timeline , 
wherein the performing the high - level fusion com 
identifying a particular content of the digital video 
prises : 
stream by combining the generated high - level labels 
temporally aligning N classified video fragments 15 
of the classified video fragments along the video 
along the video timeline ; 
timeline , 
representing an N - dimensional vector , which builds 
wherein the performing the high - level fusion com 
an N - dimensional vector for each instant of inter 
prises : 
est of the digital video stream , and within the 
temporally aligning N classified video fragments 
N - dimensional vector , every i - th component holds 20 
along the video timeline ; 
a classification confidence score of the i - th clas 
representing an N - dimensional vector , which builds 
sified video fragment , in relation to a video frag 
an N - dimensional vector for each instant of inter 
ment having a reference moment which coincides 
est of the digital video stream , and within the 
with a reference instant of interest , wherein i 
N - dimensional vector , every i - th component holds 
belongs to the natural interval [ 1 . . . N ] ) ; 
25 
a classification confidence score of the i - th clas 
in an offline operation , generating a late fusion model 
sified video fragment , in relation to a video frag 
from a training dataset , employing a supervised 
ment having a reference moment which coincides 
machine learning method on the generated late 
with a reference instant of interest , wherein i 
fusion model to generate a good late fusion model , 
belongs to the natural interval [ 1 . . . N ] ) ; 
and storing the good late fusion model ; and 
30 
in an offline operation , generating a late fusion model 
in an online operation , retrieving the stored good late 
from a training dataset , employing a supervised 
fusion model and using the retrieved good late 
fusion model , a de - noising function , and a prede 
machine learning method on the generated late 
fusion model to generate a good late fusion model , 
termined threshold to correlate the generated high 
level labels and the confidence scores for each 35 
and storing the good late fusion model ; and 
classified video fragment . 
in an online operation , retrieving the stored good late 
fusion model and using the retrieved good late 
13 . An apparatus comprising : 
at least one memory configured to store instructions ; and 
fusion model , a de - noising function , and a prede 
at least one processor configured to execute the stored 
termined threshold to correlate the generated high 
instructions to implement a method comprising : 
level labels and the confidence scores for each 
segmenting a digital video stream into video fragments 
classified video fragment . 
along a video timeline ; 

