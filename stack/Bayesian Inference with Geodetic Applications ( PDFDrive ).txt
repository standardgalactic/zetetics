Lecture Notes in 
Earth Sciences 
Edited by Somdev Bhattacharji, Gerald M. Friedman, 
Horst J. Neugebauer and Adolf Seilacher 
31 
Karl-Rudolf Koch 
Bayesian Inference 
with Geodetic Applications 
Springer-Verlag 
Berlin Heidelberg New York London 
Paris Tokyo Hong Kong Barcelona 

Author 
Professor Karl-Rudolf Koch 
Institute of Theoretical Geodesy, Unwerslty of Bonn 
Nussallee 17, D-5300 Bonn, FRG 
ISBN 3-540-53080-0 Spnnger-Verlag Berlin Heidelberg New York 
ISBN 0-387-53080-0 Sprmger-Verlag NewYork Berlin Hewdelberg 
This work is subject to copynght All rights are reserved, whether the whole or part of the matenal 
~s concerned, specifically the rights of translation, repnntmg, re-use of dlustrat~ons, recitation, 
broadcasting, reproduction on microfilms or in other ways, and storage in data banks. Duplication 
of th~s publication or parts thereof is only permitted under the provisions of the German Copyright 
Law of September 9, 1965, m ~ts current version, and a copyright fee must always be paid 
Wolatlons fall under the prosecution act of the German Copyright Law 
Â© Sprmger-Verlag Berhn Heidelberg 1990 
Printed in Germany 
Printing and binding Druckhaus Bettz, Hemsbach/Bergstr. 
2132/3140-543210- Printed on acid-free paper 

Preface 
There are problems, when applying statistical inference to the analysis of data, which are 
not readily solved by the inferential methods of the standard statistical techniques. One 
example is the computation of confidence intervals for variance components or for func- 
tions of variance components. Another example is the statistical inference on the random 
parameters of the mixed model of the standard statistical techniques or the inference on 
parameters of nonlinear models. Bayesian analysis gives answers to these problems. 
The advantage of the Bayesian approach is its conceptual simplicity. It is based on 
Bayes' theorem only. In general, the posterior distribution for the unknown parameters 
following from Bayes' theorem can be readily written down. The statistical inference is 
then solved by this distribution. Often the posterior distribution cannot be integrated ana- 
lytically. However, this is not a serious drawback, since efficient methods exist for the 
numerical integration. 
The results of the standard statistical techniques concerning the linear models can also 
be derived by the Bayesian inference. These techniques may therefore be considered as 
special cases of the Bayesian analysis. Thus, the Bayesian inference is more general. 
Linear models and models closely related to linear models will be assumed for the ana- 
lysis of the observations which contain the information on the unknown parameters of 
the models. The models, which are presented, are well suited for a variety of tasks con- 
nected with the evaluation of data. When applications are considered, data will be ana- 
lyzed which have been taken to solve problems of surveying engineering. This does not 
mean, of course, that the applications are restricted to geodesy. Bayesian statistics may 
be applied wherever data need to be evaluated, for instance in geophysics. 
After an introduction the basic concepts of Bayesian inference are presented in Chapter 
2. Bayes' theorem is derived and the introduction of prior information for the unknown 
parameters is discussed. Estimates of the unknown parameters, of confidence regions and 
the testing of hypotheses are derived and the predictive analysis is treated. Finally tech- 
niques for the numerical integration of the integrals are presented which have to be 
solved for the statistical inference. 
Chapter 3 introduces models to analyze data for the statistical inference on the unknown 
parameters and deals with special applications. First the linear model is presented with 
noninformative and informative priors for the unknown parameters. The agreement with 
the results of the standard statistical techniques is pointed out. Furthermore, the predic- 

IV 
tion of data and the linear model not of full rank are discussed. A method for identifying 
a model is presented and a less sensitive hypothesis test for the standard statistical tech- 
niques is derived. The Kalman-Bucy filter for estimating unknown parameters of linear 
dynamic systems is also given. 
Nonlinear models are introduced and as an example the fit of a straight line is treated. 
The resulting posterior distribution for the unknown parameters is analytically not tracta- 
ble, so that numerical methods have to be applied for the statistical inference. 
In contrast to the standard statistical techniques, the Bayesian analysis for mixed models 
does not discriminate between fixed and random parameters, it distinguishes the parame- 
ters according to their prior information. The Bayesian inference on the parameters, 
which correspond to the random parameters of the mixed model of the standard statisti- 
cal techniques, is therefore readily accomplished. 
Noninformafive priors of the variance and covariance components are derived for the 
linear model with unknown variance and covariance components. In addition, informa- 
tive priors are given. Again, the resulting posterior distributions are analytically not trac- 
table, so that numerical methods have to be applied for the Bayesian inference. 
The problem of classification is solved by applying the Bayes rule, i.e. the posterior ex- 
pected loss computed by the predictive density function of the observations is mini- 
mized. 
Robust estimates of the standard statistical techniques, which are maximum likelihood 
type estimates, the so-called M-estimates, may also be derived by Bayesian inference. 
But this approach not only leads to the M-estimates, but also any inferential problem for 
the parameters may be solved. 
Finally, the reconstruction of digital images is discussed. Numerous methods exist for 
the analysis of digital images. The Bayesian approach unites some of them and gives 
them a common theoretical foundation. This is due to the flexibility by which prior in- 
formation for the unknown parameters can be introduced. 
It is assumed that the reader has a basic knowledge of the standard statistical techniques. 
Whenever these results are needed, for easy reference the appropriate page of the book 
"Parameter Estimation and Hypothesis Testing in Linear Models" by the author (Koch 
1988a) is cited. Of course, any other textbook on statistical techniques can serve this 
purpose. 
To easily recognize the end of an example or a proof, it is marked by a A or a t~, respec- 
tively. 

V 
I want to thank all colleagues and students who contributed to this book. In particular, I 
thank Mr. Andreas Busch, Dipl.-Ing., for his suggestions. I also convey my thanks to 
Mrs. Karin Bauer, who prepared the copy of the book. The assistance of the Springer- 
Verlag in checking the English text is gratefully acknowledged. The responsibility of er- 
rors, of course, remains with the author. 
Bonn, June 1990 
Karl-Rudolf Koch 

Contents 
Introduction ................................................ 
1 
2 
Basic Concepts ............................................. 
3 
21 
Bayes' Theorem ............................................ 
4 
211 
Derivation ................................................. 
4 
212 
Recursive Application ........................................ 
8 
22 
Prior Density Functions ....................................... 
9 
221 
Unknown Parameters ......................................... 
9 
222 
Noninformative l~ors ........................................ 
9 
223 
Maximum Entropy Priors ...................................... 
15 
224 
Conjugate Priors ............................................ 
25 
225 
Constraints for Parameters ..................................... 
31 
23 
Point Estimation ............................................ 
33 
231 
Quadratic Loss ............................................. 
33 
232 
Different Estimators ......................................... 
34 
24 
Confidence Regions ......................................... 
37 
241 
H.P.D. Region .............................................. 
37 
242 
Boundary of a Confidence Region ............................... 
37 
25 
Hypothesis Testing .......................................... 
40 
251 
Different Hypotheses ......................................... 
40 
252 
Hypothesis Testing by Confidence Regions ........................ 
40 
253 
Posterior Probabilities of Hypotheses ............................. 
42 
254 
Special Priors for Hypotheses .................................. 
45 
26 
Predictive Analysis .......................................... 
49 
261 
Joint Conditional Density Function .............................. 
49 
262 
Predictive Distribution ........................................ 
49 
27 
Numerical Techniques ........................................ 
52 
271 
Monte Carlo Integration ...................................... 
52 
272 
Computation of Estimates, Confidence Regions and Posterior Probabilities of 
Hypotheses ................................................ 
54 

VIH 
273 
Marginal Distributions and Transformation of Variables ............... 
56 
274 
Approximate Computation of Marginal Distributions ................. 
58 
3 
Models and Special Applications ................................ 
61 
31 
Linear Models .............................................. 
62 
311 
Definition and Likelihood Function .............................. 
62 
312 
Noninformative Priors ........................................ 
64 
313 
Informative Priors ........................................... 
69 
314 
Prediction of Data ........................................... 
73 
315 
Linear Models Not of Full Rank ................................ 
77 
316 
Model Identification 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 
83 
317 
Less Sensitive Hypothesis Tests for the Standard Statistical Techniques ... 
88 
318 
Linear Dynamic Systems ...................................... 
92 
32 
Nonlinear Models ........................................... 
99 
321 
Definition and Likelihood Function .............................. 
99 
322 
Fit of a Straight Line ........................................ 
100 
33 
Mixed Models ............................................. 
109 
331 
Mixed Model of the Standard Statistical Techniques ................. 
109 
332 
Definition of the Mixed Model and Likelihood Function .............. 
1 I0 
333 
Posterior Distributions ....................................... 
111 
334 
Prediction and Filtering of Data ................................ 
115 
335 
Special Model for Prediction and Filtering of Data ................... 
118 
34 
Linear Models with Unknown Variance and Covariance Components ..... 
122 
341 
Definition and Likelihood Function .............................. 
122 
342 
Noninformative Priors ........................................ 
124 
343 
Informative Priors ........................................... 
132 
35 
Classification 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 
135 
351 
Decision by Bayes Rule ...................................... 
135 
352 
Known Parameters .......................................... 
t 37 
353 
Unknown Parameters ........................................ 
138 
36 
Posterior Analysis Based on Distributions for Robust Maximum Likelihood Type 
Estimates ................................................. 
144 
361 
Introduction ............................................... 
144 
362 
Likelihood Function ......................................... 
145 
363 
Posterior Distribution in the Case of Known Scale ................... 
146 
364 
Posterior Distribution in the Case of Unknown Scale ................. 
149 

IX 
365 
Predictive Distributions for Data Points ........................... 
150 
37 
Reconstruction of Digital Images ............................... 
156 
371 
Model Description and Likelihood Functions ....................... 
156 
372 
Normal-Gamma Distribution as Prior ............................ 
158 
373 
Normal Distribution and Gibbs Distribution as Priors ................. 
158 
374 
Prior Leading to Maximum Entropy Restoration .................... 
165 
A 
Appendix ................................................. 
169 
hl 
Univariate Distributions ............................ 
.......... 
170 
All 
Univariate Normal Distribution ................................. 
170 
A12 
Gamma Distribution ......................................... 
170 
A13 
Inverted Gamma Distribution ................................... 
171 
A2 
Multivariate Distributions ..................................... 
173 
A21 
Multivariate Normal Distribution ................................ 
173 
A22 
Multivariate t-Distribution ..................................... 
173 
A23 
Normal-Gamma Distribution ................................... 
182 
References 
..................................................... 
185 
Index ......................................................... 
191 

1 Introduction 
Bayesian inference, despite its conceptual simplicity, has its opponents. First, it is con- 
sidered subjective, because it requires the introduction of prior information. However, 
the use of noninformative priors, also called vague priors, leads to results which are 
equivalent to the results of the standard statistical techniques. In addition, informative 
priors can be derived using the concept of maximum entropy, thus making them, except 
for the information which is conveyed, as noninformative as possible. 
Another point of debate is the notion of the unknown parameter in Bayesian analysis. It 
is defined as a random variable, since a prior and a posterior distribution are associated 
with it. This does not mean, however, that the unknown parameter cannot represent a 
constant, like the velocity of light. The prior and posterior density function of the un- 
known parameter describe the state of knowledge on the parameter. The probability de- 
rived from these distributions need not be interpreted as a frequency, that means a meas- 
urable quantity. The probability merely represents the state of knowledge and may be 
considered a subjective quantity. Thus, the unknown parameter can very well mean a 
constant. For further discussions on the differences between the Bayesian approach and 
the standard statistical techniques, see Jaynes (1986). 
As mentioned in the Preface, geodetic applications, i.e. data analysis for surveying engi- 
neering, will be mostly considered. Pope in Dillinger et al. (1971) was the first geodesist 
to discuss Bayesian inference in order to interpret confidence regions computed for the 
determination of focal mechanisms. It was Bossier (1972), who introduced Bayesian 
methods into geodesy by investigating the Bayesian estimates of unknown parameters in 
linear models with respect to geodetic applications. In particular he dealt with the esti- 
mate of the variance of unit weight in the case of prior information. He drew attention to 
Theil's estimator (Theil 1963), which was later applied for an adjustment of a geodetic 
network (Bossler and Hanson 1980) and modified for a simpler computation (Harvey 
1987). An approximation to the Bayesian estimate of the square root of the variance of 
unit weight in the linear model was derived by Schaffrin (1987). 
Riesmeier (1984) dealt with the test of hypotheses formulated as inequality constraints 
and developed less sensitive hypothesis tests by modifying the standard statistical tech- 
niques (Koch 1988a, p. 310, 319). This method was extended from the univariate model 
for estimating parameters to the multivariate model by Koch and Riesmeier (1985) and 

applied to the deformation analysis by Koch (1984, 1985). 
Bayesian inference for variance components with the aim of deriving confidence inter- 
vals was presented based on noninformative priors by Koch (1987) and informative pri- 
ors by Koch (1988b). The resulting posterior density functions for the variance compo- 
nents are analytically not tractable, so that the density has to be integrated numerically 
(Koch 1989). Bayes estimates of variance components for a special model are presented 
in Ziqiang (1990). 

2 Basic Concepts 
The starting point of Bayesian inference is Bayes' theorem, which will be derived first. 
When working with Bayes' theorem the question arises, which prior density function 
should be associated with the unknown parameters. This, of course, depends on the kind 
of prior information, and different alternatives for incorporating this information are dis- 
cussed. Based on the posterior density function derived for the unknown parameters 
from Bayes" theorem, the estimates of the parameters and their confidence regions are 
then obtained and methods for testing hypotheses are discussed. The distributions of pre- 
dicted observations are derived and methods of numerical integration are presented, 
since integrals have to be solved which are analytically not tractable. 

21 Bayes" Theorem 
211 Derivation 
Let y be a random vector defined by digital registrations of the outcomes of a random 
experiment. The values which this random vector takes on or its realizations are called 
observations, measurements, or data. They are collected in a vector, which is also de- 
noted by y, to simplify the notation. Let the probability density function of the random 
vector y be dependent on unknown parameters. They are defined as random variables 
and collected in the random vector 0. The random parameter vector 0 has the probability 
density function p(O), where again for the sake of simple notation 0 denotes the vector 
of realizations. We assume 0 c o, with o being the set of admissible parameter vectors or 
the parameter space. The density function of the random vector y is introduced as the 
conditional density function p (yl0) given that the random parameter vector 0 is equal to 
its realization. 
Bayes" Theorem: The probability density function p(01 y) of the parameters 0 given the 
observations y is obtained from the density p(0) of the parameters 0 and the density 
p(y] 0) of the observations y given the parameters 0 by 
p(Oly) ~ p(0)p(y 10), 
where ~ denotes proportionality. 
(211.1) 
Proof: The conditional density function p(yt 0) is defined by (Koch 1988a, p. 107) 
p(y,0) 
P(Yl 0) - 
, 
(211.2) 
p(0) 
where p(y, 0) denotes the joint density function of y and 0. Solving for it and applying 
(211.2) once more leads to 
p(y,0) = p(0)p(y I 0) 
= p(y)p(Oly) 
or by equating the right-hand sides to 
P(0)p(yt0) 
p(0ly) - 
for 
p(y) > 0. 
(211.3) 
P(Y) 
This proves the theorem, since the observations y are given, so that p(y) is a constant. [] 

The density p(y) in (211.3) may be obtained from the joint density p(y,0) as a 
marginal density by (Koch 1988a, p.105) 
p(y) = f p(y,O)dO 
0 
= f p(O)p(ylO)dO, 
0 
where the domain of the integration is the parameter space o. We therefore obtain the 
alternative formulation of Bayes' theorem 
p(Oty) = cp(O)p(y 10) 
(211.4) 
with 
c = 1/(~ p(e)p(ylO)de ), 
(211.5) 
e 
where c is a normalization constant, which ensures that the density function p(ely) ful- 
fills the conditions (Koch 1988a, p. 104) 
p(01y) _> 0 
and 
~ p(ely)dS= 1. 
(211.6) 
e 
The density function p(0) of the parameters 0 in (211.1) summarizes what is known 
about the parameters 0, before the data y have been taken. It is therefore called the prior 
density or the prior distribution of the parameters e. With the introduction of the obser- 
vations y the density p(e[y) for 0 is obtained. It is called the posterior density or the 
posterior distribution of the parameters 0. The information on the parameters 0 coming 
from the data y enters through the density p(y[ 0). Since y is given, this density may be 
viewed not as a function of y but as a function of the parameters 0. The density p(y[ e) 
is therefore called the likelihood function. It is often denoted by 1 (0[ y) in order to indi- 
cate that it is a function of e. We may therefore express Bayes' theorem by 
posterior density function ~ prior density function x likelihood function. 
The observations modify through the likelihood function the prior knowledge of the pa- 
rameters, thus leading to the posterior density function of the parameters. 
The posterior distribution solves the problem of the statistical inference. By means of 
this distribution the estimates of the unknown parameters and their confidence regions 
are computed and the probabilities are determined to decide whether to accept or reject 
hypotheses for the unknown parameters. 
Example 1: Suppose we have n independent observations Y=[Yl ..... yn ] ', for instance 
of the length of a straight line or of an angle. Let each observation be normally distrib- 
uted with the unknown expected value # und the known variance crL We want to deter- 

mine the posterior density function P(#IY) for the unknown paramter # with the prior 
density function p(#) of # resulting from a normal distribution. With (211.1) we obtain 
P(#tY) " P(U)P(YI#). 
(211.7) 
As required, p(#) is represented by the normal distribution (All.l) 
= ___1__1 exp[- 
1 
P(#) 
e# 
2a~ (#'#p)2]" 
(211.8) 
The prior information on the expected value and the variance of the parameter # is there- 
fore given by Up and a~. 
Each observation Yi is normally distributed, hence 
_ 
1 
exp[- 2-~(yi-#)2]. 
(211.9) 
P(Yi 1#) 
~ 
a 
Since the observations Yi are independent, their joint distribution follows from the prod- 
uct of the individual distributions (Koch 1988a, p.107), so that the likelihood function is 
given by 
n 
p(y]#) = (2~a2) -n/2 exp[- 1_~ y, (yi_#)2]" 
(211.10) 
2a2 i=l 
^ 
With the estimate # of # (Koch 1988a, p.193) 
n 
^ 
1 I; Yi 
(21t.11) 
#=~i=l 
A 
and with the estimate a2 of a 2 
1 
n 
^ 
= 
~ (yi-#) 2 
(211.12) 
n- 1 i=l 
we obtain for the exponent 
n 
n 
A 
A 
Z 
(yi-#) 2= 
X ((yi-#)-(#-#)) 2 
i=1 
i=l 
n 
A 
A 
= 
Z (yi-#) 2 + n(#-#) 2, 
(211.13) 
i=1 
n 
A 
A 
n 
A 
since Z (Yi Â°#) (#-#)=0 because of Y~ (Yi-#)=0. Thus, we find instead of (211.10) 
i=l 
i=l 
, ,c2~re2~ -n/2 exp[- 
1 
^ 
^ 
P(Y]#) = 
( (n- 1)~2+n(#-#)2) ]. 
2# 
The likelihood function can be factored into a term only depending on y through ~x and 
A 
into a term depending on # and # 

P(YI#) 
(2;Za2) -n/2 exp[- 
1 
^ 
^ 
= 
(n_ 1)cr2]exp[_ 
n 
.......... (#.#)2]. 
(211.14) 
2ff2 
2(~2 
When substituting P(Yt#) in (211.7) to derive the posterior density P(#IY) of # given y, 
the first term is constant. Hence, by neglecting the constants also in (211.8) we find 
1 (#_#p) 2 
n 
^ 
p(#[y) ,~ exp[- ~( 
+ __(#.#)2)]. 
(211.15) 
On completing the squares of the exponent we find 
(#.#p)2 
n 
^ 
+ _(#_#)2 
- 
A 
A 
aJ+o21n 
#aJ+#pO2/n 
#oJ+#p OZln 
= 
(#2-2# 
+( 
I 
^ 
o.Jo.2(#2(no'J+o2) - 2#(ngo~t+#pff2 ) + n>o'~x + #~0 "2) 
o~xo21n 
oJ+02 ln 
a~x+o21n 
A 
A 
#aJ+#p~2/n 2 #x2o~#~a21n 
( 
) + 
). 
oJ+o2in 
oJ+o21n 
)2 
(211.16) 
The last two terms on the right-hand side are constant and may be neglected. We there- 
fore obtain the posterior density function p (#1 Y) of # 
A 
aJ+cr2/n 
#o~+#pO2/n 
p(#ly) <~ exp[- ~.a~d2in (#_ 
a~ x+O2/n 
)2]. 
(211.17) 
Now we recognize with (All.l) and (All.3) that p(#ly) is given by the normal distri- 
bution 
#ly 
- 
N(E(#), V(#)) 
with the expected value 
A 
l~(#) = #ffJ+#P(r21n 
o~+o2/n 
and the variance 
A 
-1 
- 
: #(o2in) 
+ p(OJ) 1 
(a21n)-l+(aJ) "1 
(211.18) 
oâ¢x 
a2 / n 
i 
V(#) = 
= (z2/n)_X+(a~). I â¢ 
(211.19) 
A 
The variance of the mean p is cr2/n (Koch 1988a, p.193) and the variance of the prior 
information for # is ff~. The reciprocals of the variances (~2/n)-i and (cr~) "1 are de- 
fined as weights (Koch 1988a, p. 120). They are introduced into (21 I. 18) and (211.19) by 

multiplying numerator and denominator by 1/(a2ua2/n). The expected value E(#) of the 
A 
unknown parameter/~ follows therefore as the weighted mean of the information # com- 
ing from the data and of the prior information gp. The weight (V(#)) -1 of # results 
from the sum of the two weights. 
If both the expected value # and the variance a2 of the observations are unknown, the 
posterior densities of # and o'2 are derived in Example 1 of Section 224. 
A 
212 Recursive Application 
If different and independent vectors Yi of observations have been collected with 
i~ { 1 ..... m}, Bayes' theorem may be applied recursively. With the first vector Yl we 
obtain from (211.1) 
p(0ly 1) ~, p(0)p(y 110). 
Using the posterior density function p(01Yl ) as prior density for the second vector Y2 of 
observations gives 
p(0[Yl,Y2) o, p(0tYl)p(y2t0) 
or with k vectors of observations 
p(0ly 1 ..... yk ) ~ p(Oly 1 ..... Yk_l)p(ykl0) 
for 
k~{2 .
.
.
.
.
 m}, 
(212.1) 
where 
p(0lYl) ~ p(0)p(Yll0 ). 
(212.2) 
Thus, with each experiment, that is with each observation vector Yi' the knowledge on 
the unknown parameters 0 is updated, which is equivalent to the process of learning 
from the gain on experience. 
Of course, we do not have to work recursively, but we may analyze all observations at 
the same time. The likelihood function of the observation vectors Yl to Yk is given by 
P(Y110)p(Y210)'" "P(Yk[ 0) (Koch 1988a, p.107), since the vectors Yi and yj are inde- 
pendent for iej. With Bayes' theorem (211.1) we thus obtain 
P(0t Yl ..... Yk ) ~" P(0)P(Yl t 0)p(Y210)... P(Ykl 0), 
(212.3) 
which is identical with (212.1) after substituting (212.2). 
An example for the recursive application of Bayes' theorem is given in Section 318. 

22 Prior Density Functions 
221 Unknown Parameters 
The unknown parameters in Bayes' theorem (211.1) are defined as random variables. 
But this does not mean that the parameters cannot represent constants, like the coordi- 
nates of fixed points. The prior and posterior density functions of the unknown parame- 
ters describe the state of knowledge on the parameters. By means of these distributions 
the probability is determined that the parameters cover certain regions. This probability 
expresses the subjective belief on the unknown parameters, and it needs not be inter- 
preted as a frequency, that means a measurable quantity, since the parameters are not 
defined by actual or hypothetical random experiments. Only theoretically a pr@ability is 
assigned to them, to express the state of knowledge. We may therefore interpret the pa- 
rameters as fixed quantities or variable quantities, variable for instance with time. When 
applying Bayes" theorem we obtain the posterior density function according to the val- 
ues the parameters had when the data were taken. 
There are situations when no prior information exists on the parameters. Hence, we have 
to decide how to express ignorance in the prior distribution. This is accomplished by the 
so-called noninformative or vague prior density functions, which will be treated next. 
222 Noninformative Priors 
If we do not know anything about an unknown parameter 0, it may take values from -= 
to +=. Hence, the appropriate prior probability density function is 
p(0) ~, const, 
with 
-~ < 0 < ~. 
(222.1) 
This is an improper density function because 
Ip(0)d0-~l. But the posterior density 
function can be normalized, so this is not a serious drawback. 
If a parameter assumes values only between 0 and ~, for example the variance o2, we 
take 
0 = tncr2, 
(222.2) 
where lncr 2 denotes the natural logarithm of or2, and again 
p(O) ~ const, 
with 
-~ < 0 < ~. 

10 
By transforming the variable from 0 to aa with (222.2) (Koch 1988a, p.108), we obtain 
with dO/d62=l/a a the noninformative prior density function for the variance 62 
p(a 2) 0~ 1/62 
with 
0 < 62 < ~. 
(222.3) 
F-rankle 1: The prior density function (222.1) and the meaning of the transformation 
(222.2), which leads from (222.1) to (222.3), shall be demonstrated by an example. We 
assume n independent observations Yi' each being normally distributed with the un- 
known expected value # and the known variance 62. The likelihood function follows 
from (211.14) by 
p(y[#) 
exp[ 
n 
^ 
~, 
. 
(#_~)2]. 
262 
The graph of this likelihood function is a normal curve, whose location on the # axis de- 
^ 
pends on #, which means on the data Yi according to (211.11). Different sets of observa- 
tions change only the location and not the shape of the likelihood curve. The likelihood 
is data translated (Box and Tiao 1973, p.26). If in such a situation no prior information 
is available for the unknown parameter #, so that the data have to decide on #, the 
appropriate prior density is a constant as formulated by (222.1). 
If, on the other hand, the expected value # is known and the variance Â¢r2 is unknown, we 
find with (211.14) the likelihood function 
^ 
p(y[62) ~, (62) "n/2 exp[-(n-1)62/262]. 
The graph of this function changes its shape and location on the 62 axis with varying 
^ 
sets of observations through 62 according to (211.12). However, by applying the transfor- 
marion (222.2) we find with 62=exp0 and d62/dO=-expO=62 the likelihood function 
A 
P(ylln62) 0" (62) -(n-2)/2 exp[-( n'l)a2/262]- 
We may multiply bY the constant (~r2)(n-2)/2 and obtain 
A 
^ 
-(n-2)/2 
n-1 exp[ln(a2/a2)] } 
P(Yl ln62) ,, exp{ln(62/62) 
" --2-- 
or finally 
p(ylln~2) ~: exp{- ~-~(ln62-1n~ 2) - ~-~ exp[-(ln62-1n~r2)]}. 
The graph of this likelihood function only changes its location on the lna 2 axis, if the 
A 
data are varied through lna2. If no prior information is available for ln62, the appropri- 
ate prior distribution for ln~ is therefore a constant or the prior distribution (222.3) for 
62. 
A 

tl 
Often it is convenient to parameterize according to a weight or precision parameter 
with 
= 1/a 2. 
(222.4) 
By transforming ~2 to z we find with da2/dz'= - 1/~: 2 instead of (222.3) 
p('c) ~ 1/z 
with 
0 < z < ~. 
(222.5) 
The density (222.3) is invariant to transformations of the form 
IÂ¢ = (a2) n, 
(222.6) 
which can already be recognized by (222.5). To show this, we transform ~2 to ~c and find 
with a2= ~Â¢1 / n and da 2/d ~n- 1 ~cl/n- 1 
da2/a2 o, d~Â¢/~Â¢ 
(222.7) 
and instead of (222.3) 
p(tÂ¢) ~ 1De with 
0 < ~c < ~. 
(222.8) 
The differential probability computed with the posterior density P(#tY) for ~ is ob- 
tained with Bayes" theorem (211.1), if (222.3) is used as prior density, by 
p(a21y)da2 ~, 1 
p(ylff2)dcr2" 
e2 
If, on the other hand, the variable ~ of (222.6) is chosen, we find with (222.8) 
1 
p(K:[y)d~c o, ~ p(y[oZ)d~Â¢. 
However, because of (222.7) 
p(aa[y)d# ~ p(rly)dr, 
(222.9) 
so that the posterior distributions give identical probabilities independent of the parame- 
ters 0'2 or to. The prior density (222.3) is therefore invariant to transformations of the 
form (222.6). 
The results (222.1) and (222.3) can be derived from a more general formula which is 
due to Jeffreys (196I, p.179) and which is also based on the invariance of a transforma- 
tion. A noninformative prior density function p(O) of the parameters O is given by 
p(0) ~ (detI0) l/2 , 
(222.10) 
where I 0 is Fisher's information matrix for the parameters O associated with the likeli- 
hood function. With I0= ( I i j )' 0= (0 i ) and i, j e { 1 ..... u } we have 

12 
a21np(ylO) 
I .... 
E( 
) 
for 
i,je{l ..... u}, 
1j 
OOiOOj 
where the expectation E is taken with respect to the distribution for y. 
(222.11) 
Example 2: The likelihood function (211.10) of the Example 1 of Section 211 depends 
on the unknown parameter #. Taking its natural logarithm gives 
n 
n 
1 
n 
lnp(yl#) = - ~ in2zc - ~;~ ln62 
E 
_(Yi-#) 2. 
262 i=l 
Furthermore, 
01np(yl#)/O/~ = 1 
n Z (yi-#) 
i=1 
and 
O21np(y[#)/O#2 = -n/62 
and therefore with (222.10) 
p(#) ~ const. 
in agreement with (222.1). 
We will now assume that the variance cr2 in (211.10) is the unknown parameter. We pa- 
rameterize according to (222.4), i.e. v=-1/6 z, and obtain 
n 
p(y[v) = (2~r)-n/2vn/Zexp[- ~ i__Za(Yi-#)2]. 
Taking the natural logarithm gives 
n 
lnp(ytz) =- ~ ln2g+ ~ tnz-~ 
i__21 (yi-#) 2 
and 
= 2-~- 2 
" = n  
1 l~l(Yi_#)2. 
Olnp(y[z)/Oz 
Furthermore, we have 
O21np(y I ,r)/Oz2 -- - 
n 
2~2 
and with (222.1 O) 
p('O ~ ll'c 
in agreement with (222.5). 
If in (211.10) both # and z are considered as unknown, we obtain from (222.11) with 
E(Y i )=# 

13 
n 
Ill = n'c, 
I12 = - E( Â£ (yi-#)) = 0, 
i=1 
n 
I21 = - E( 2 (yi-#)) = O, 
I22 = n/(2~ 2) 
i=l 
and from (222.10) 
p(#,z) ~, l/q~, 
which does not agree with (222.5). Thus, in order to obtain coinciding results, the un- 
known parameters # and or2 have to be considered as being independent. 
A 
The invariance property of Jeffreys' prior density function (222.10) can be shown by 
means of the 
Theorem:Let fl=t(O) with t(O)=(ti(O)) and i~{1 ..... u} be an injective differen- 
tiable transformation and Ifl and I 0 the information matrices for/] and O, then 
(detlfl) 1/2d~l = (detlo) 1/2dO. 
(222.12) 
Proof(Zellner 1971, p.48): We will first show by abbreviating p(y[O) with p the rela- 
tion 
-E(~j) 
=E(~. 
~.) 
for 
i,j~{l ..... u}. 
(222.13) 
1 
j 
By differentiation we find 
.... 
7 
i 
j 
since the value of the integral on the right-hand side is equal to one (Koch 1988a, 
p. 104). 
By differentiating lnp(y[ 0) with respect to the new variable fli with/l=(fl i ) we obtain 
01np 
u 
0tnp 00 k 
-y. 
0fli 
k=l 00 k 
0/3 i 

14 
and 
31np Olnp 
3~i 
3/3j 
By defining 
I]~,i,j 
we obtain 
I/3, i,j 
u 
u 30 k alnp 31np 301 
-X 
g 
k=l 1=10~i O0 k 
O01 O~j 
01np 01np 
31np 31np 
= E(3/~i 
--)3/3j 
and 
I0,k, 1 = E(30 k 30--)1 
u 
u 
00 k 
001 
=k__E1 X -- 
Io,k,1 
I=1 0/3 i 
3/3j 
With (222.11) and (222.13) we recognize 
113 = (I/3, i,j) and I 0 = (I0,k,1). 
Thus, by defining J=(30o/0/3p) with o,p~{1 ..... u} we obtain 
I~ = J'IoJ 
and 
(detI]~)l/2 = (detI0) 1/2det J. 
Furthermore, we have (Koch 1988a, p.85) 
dO = IdetJld~], 
since de t J is the Jacobian of the transformation which is inverse to fl= t (0). Hence, 
(de t I/3) 1/2dO = (de t I0) 1/2d0 ' 
which proves the theorem, 
n 
The invariance of the prior density function (222.10) is equivalent to the invariance de- 
monstrated in (222.9). This can be shown by using (222.10) as prior density in Bayes' 
theorem (211.1). We obtain with the posterior density P(0!Y) of the parameters 0 the 
differential probability 
p(01 y)dO ~, (detI 0) 1/2p(y I 0)d0. 
If the parameters p of (222.12) are chosen, we find 
o, (detI/3) l/2p(y I O)dp. 
P(PlY)dP 
Because of (222.12) we have 
p(Oly)dO o, p(/lty)dp, 
(222.14) 

15 
so that the posterior distributions give identical probabilities independent of the parame- 
ters 8 or ~. The prior density (222.10) is therefore invariant to transformations fl=t(0). 
gxtm~le 3: We go back to the Example 1 of Section 211 and introduce instead of an in- 
formative prior for the unknown parameter # the noninformative prior 
p(/2) = const. 
from (222.1). Together with the likelihood function (211.14) we obtain from Bayes" the- 
orem (211.7) after neglecting the constants the posterior density for # 
^ 
P(#tY) ~ exp[- 
n (/2-/2)2]. 
(222.15) 
2cr2 
It becomes obvious with (All.i) that this posterior density function is normal with the 
A 
expected value/2 and the variance cr2/n, hence 
A 
E(/2) = /2 and 
V(#) = cr2/n. 
(222.16) 
The same result is obtained with cr~-~ ~ in (211.18) and (211.19). Geometrically it means 
that the prior density function (211.8) of Example 1 of Section 2tl is spread out by 
cry-+ % leading in the limit to a noninformative prior. 
A 
223 Maximum Entropy Priors 
Entropy is a measure of uncertainty. By applying the principle of maximum entropy we 
may select distributions which contain the largest amount of uncertainty compatible with 
given constraints. Since prior information is generally incomplete, beyond the given in- 
formation the prior information should be as uncertain or noninformative as possible. If, 
for instance, the expected value and the variance of a random variable are given, among 
all possible distributions we want to select the one which contains the largest amount of 
uncertainty compatible with the known expected value and the known variance. 
We are looking for a real-valued function I, which measures the information gained by 
the random event A of an experiment. The information which is gained can be inter- 
preted as the uncertainty which is removed and which was existing before the outcome 
of the experiment. Hence, the uncertainty of the outcome of an experiment resulting in 
an event A is equal to the information gained, if the experiment leads to the event A. 
The function I (A) measuring the information or uncertainty of an event A should satisfy 
the following properties: 

16 
1. If an event A has probability one of occurring, P(A)=I, then its uncertainty is 
zero, I (A)=0. 
2. If P(A1)<P(A2), 
then 
I(A1)>I(A2), 
that is, the first event has larger 
uncertainty. 
3. IfA 1 and A 2 are independent, then I(AlaA2)=I(A1)+I(A2). 
The measure I has to depend on the probability of events. Hence, we are looking for a 
function, say G(P(A)), defined on the interval (0, 1). If it is a monotonic decreasing 
function on (0,1), vanishing at 1, it fulfills 1. and 2. If A 1 and A 2 are independent, then 
P(AlC~2)=P(A 1)P(A2) and with 3. we obtain 
G(P(A1)P(A2)) = G(P(A1) ) + G(P(A2) ). 
(223.1) 
The only function satisfying (223.1) is the natural logarithm multiplied by a constant, 
hence G(P(A))=c lop(A). This function vanishes at 1, but is increasing, so that we have 
to choose a negative constant. With I(A)=G(P(A)) the measure I (A) of information or 
uncertainty of an event A therefore follows by 
I(A) = - c loP(A). 
(223.2) 
Now we interpret P(A) as the probability density p(x i 10) of a discrete random variable 
x with values x i defined in the probability space of an experiment. Thus, 
n 
P(XiI0) > 0 
for 
i~{l ..... n} 
and 
Z 
P(Xil0) = 1, 
i=l 
where 0 is the parameter of the density function. The expected value of the information 
or uncertainty of the experiment is obtained with c=1 in (223.2) by 
n 
Hn = -i=lZ p(x i 10) Inp(x i 10). 
(223.3) 
The expected value H n is called discrete entropy. 
We assume p(x i 10) lnp(x i 10)=0 for p(x i [0)=0 according to the limit 1 ira xlnx=0. 
x-~0 
Let a continuous random variable x be defined by an experiment on the interval [a,b] 
with the probability density function p(x]0) depending on 0, thus 
b 
p(xl0) _ 0 
and 
f p(xl0)dx = 1. 
a 
The continuous entropy is defined correspondingly to (223.3) by 
b 
H = - I p(xl0) lnp(xl0)dx. 
(223.4) 
a 
The properties of the discrete and the continuous entropy are different, for instance, 

17 
while Hn_>O, tt may be also negative. This can be shown by assuming the uniform distri- 
bution 
1 
p(xla,b) = b-S- d 
for 
a < x < b, 
which gives 
b-a In(b-a) 
H = g-Z-ff 
with 
in(b-a) < 0 
for 
(b-a) < 1. 
The interpretation of the discrete and continuous entropy as measures of uncertainty is 
therefore different. However, a concurring interpretation in the discrete and continuous 
case is obtained, if the entropy is interpreted as a variation of information, if one passes 
from an initial probability measure given by the uniform distribution on the interval 
[ a, b] to a new probability measure defined by the probability density function p (xil0) 
and p(x I 0), respectively (Guiasu 1977, p.28). 
We use now the principle of maximum entropy to derive distributions, that is we 
maximize the entropy subject to some constraints. This will lead us to distributions 
which contain maximum uncertainty compatible with the constraints. These distributions 
are therefore well suited as prior distributions. The following theorem summarizes the 
results. 
Theorem: The density function of a random variable x, which is defined on the interval 
[ a, b] and which maximizes the entropy, is the density of the uniform distribution 
p(xla,b) = 1/(b-a) 
for 
a < x < b. 
(223.5) 
The density function of a random variable x with the expected value E(x)=# and the var- 
iance V(x)=cr2, which is defined on the interval (_o~,oo) and which maximizes the 
entropy, is the density of the normal distribution 
p(xt#,62 ) _ 
1 
e -(x-#)2/262 
for 
-~ < x < ~. 
(223.6) 
The density function of a random variable x with the expected value E(x)=#, which is 
defined on the interval [0, oo) and which maximizes the entropy, is the density of the ex- 
ponential distribution 
1 e-X/# 
p(xI#) = ~ 
for 
0 < x < ~. 
(223.7) 
The density function of a random variable x with the expected value E(x)=# and the var- 
iance V(x)=cr2, which is defined on the interval [0 ,~) and which maximizes the entropy, 
is the density of the truncated normal distribution 

18 
p(xl#,a 2) : exp(-ko) exp[-klx-k2(x-#)2] 
for k 2 > 0, a 2 < #2 
and 0 < x < 
with 
exp(k o) = 7 exp[-klx-k2(x-#)2]dx 
o 
exp(-k o) 7 x exp[-klX-k2(x-#)2]dx = # 
O 
exp(-k o) 7 (x-#) 2 exp[-klx-k2(x-#)2]dx = 0"2. 
O 
(223.8) 
Proof." We will first derive a general solution which contains the given distributions as 
special cases. Thus, the entropy H in (223.4) 
b 
H = - ~ p(xI0)lnp(xl0)dx 
(223.9) 
a 
for the distribution of a random variable x defined on the interval [a,b] has to be maxi- 
mized subject to the constraint of the normalization in (211.6) 
b 
I p(xl0)dx = 1 
(223.10) 
a 
and subject to the constraints 
b 
f fi(x)p(xl0)dx=# 
i 
for 
i~{1 ..... n} 
(223.11) 
a 
resulting from given expected values ~i' like the mean or the variance, for some func- 
tions fi (x)- 
For computing the extreme value of the entropy we introduce the Lagrange function 
(Koch 1988a, p.80) 
b 
b 
w(x) = - t p(xlO)lnp(xlO)dx - ko[I P(xlO)dx-l] 
a 
a 
n 
b 
E ki[~ fi(x)p(xlO)dx-#i], 
i=l 
a 
where -k o and -k i denote the Lagrange multipliers. Since we look for the maximum of 
w(x), we may neglect constant terms which change the height of the maximum but not 
its position. Hence, 
b 
n 
I p(xl0)[-lnp(x]0) 
- ko-iE 1 kifi(x)]dx 
a 

19 
b 
n 
1 
exp[- 
kifi(x)]}dx 
= aI p(x10)ln{p-(-i-[- ~ 
k-i~ 1 
b 
u 
1 
< I p(xlO){p(x--~ exp[- ko-" 
a 
1 
kifi(x)l-1}dx, 
where the inequality holds because of 
lnx = x - 1 
for 
x = 1 
and 
lnx < x - 1 
for 
x > 0 
and x e 1. 
The equality follows with 
n 
p(xt0) = exp[- k o -i l=Z k.fl i(x)]" 
(223.12) 
For this case the right-hand side of the inequality is constant, so that an upper limit, that 
is the maximum, of the Lagrange function is found. By substituting the result (223.12) in 
(223.10) the constant k o is determined. We have 
b 
n 
I exp[- k o- Z kifi(x)]dx = 1 
a 
i=l 
and therefore 
b 
n 
exp(k o) = I exp[- Y, kifi(x)]dx. 
(223.13) 
a 
i=l 
With this result the density function is obtained by 
n 
p(x[0) = exp(-ko) exp[- Y, k. fi(x)]. 
(223 14) 
i=l 
1 
We will now assume a random variable x defined on the interval [a,b] subject to no 
constraints (223.11), so that k i=0 in (223.13) and (223.14). We obtain from (223.14) 
p(xt0) = exp(-ko) 
and from (223.13) 
b 
exp(k o) = f dx = b - a 
a 
or 
k o = In(b-a) 
Substituting this resultin the density function leadsto 
p(xla,b) = e -ln(b'a) = t/(b-a), 

20 
which proves (223.5). 
Now we assume a random variable x with the expected value E(x)=/z and the variance 
V(x)=Â¢r2 defined on the interval (-~,~). We therefore have 
fl(x) = x, #1 = ~t, f2(x) = (x-#) z, /~2 = a2 
in (223.11) and the first constraint gives 
'~ xp(x[O)dx = U. 
We change the variable from x to y with 
x = y + ~t and 
dx/dy = 1 
and obtain instead 
yp(yt0)dy = 0. 
-OO 
Hence, we find kl=0 in (223.13) and (223.14) for the variable y and obtain the density 
function 
p(yt0) = exp(-k o) exp(-k2Y2) 
with 
exp(ko) = ~ exp(-k2Y2)dy. 
The integration gives (Gradshteyn and Ryzhik 1965, p.307) 
k o = In ~ exp(-k2Y2)dy = ln(g/k2 )1/2 
and therefore 
p(yl0) = (k2/g) l/2exp(-k2Y2) . 
The second constraint of (223.11) for y gives (Gradshteyn and Ryzhik 1965, p.337) 
(k2/g) 1/2 ~ y2 exp(-k2Y2)dy --- (k2/~) 1/2(1/2k2) (g/k 2) 1/2 = ff2 
or 
k 2 = 1/2Â¢r2. 
Thus, changing back to x with y=x-# we obtain 
p(x i#,cr2) _ 
1 
e- (x-#)2/262, 
v'~cr 
which proves (223.6) because of (A11.1). 

21 
We now introduce a random variable x with the expected value E(x)=# defined on the 
interval [0,=). We therefore have 
fl(x) = x 
and 
#1 = # 
in (223.11) and find from (223.14) the density function 
p(x[ 0) = exp(-k o) exp(-klX) 
(223.15) 
with 
exp(ko) = ~ exp(-klX)dx 
o 
from (223.13). The integration leads to 
1 
~ 
1 
exp(kÂ°) = [" kll exp(-klX)]Â° = Fll 
so that 
p(x[0) = k 1 exp(-klX ). 
The constant k 1 follows from the first constraint of (223. I 1) 
k 1 ~ x exp(-klX)dx = #. 
o 
The integration gives 
exp(-klX ) 
kt[ 
(-klX-1)] o = kt/k ~ = # 
k~ 
or 
k 1 = 1/#. 
We therefore find the exponential distribution 
1 
p(xl#) = ~ e-X/#, 
which proves (223.7). As shown, if x has the exponential distribution (223.7), its ex- 
pected value E(x) is given by E(x)=#. The variance follows with (A12.5) from 
1 
e~ 
1 ~ x2e-X/gdx_#2 = ~ [e-X/#(_#x2_2#2x_2~t3)] Â° _ #2 = #2. 
(223.16) 
V(x) = ~ o 
We finally assume a random variable x with the expected value E(x)=# and the variance 
V(x)=a 2 defined on the interval [0,~). Thus, we have 
fl(x) = x, /.t 1 = #, f2(x) = (x-#)2, #2 = a2 
in (223.11) and (223.14) gives the density 

22 
p(x[#,c#) = exp(-ko) exp[-klx-k2(x-#)2 ]. 
We require p(x]#, a2) to be a proper density, so that we must have k2>0, because other- 
wise the integral over the density function takes on an infinite value. Transforming the 
exponent gives 
k 1 
p(x[#,az) = exp(-ko) exp[-k2(k~ 2 x+xZ-2#x+#2)] 
k t 
= exp(-ko) exp[-kz(x2-2(# - Z~22)x+#2)] 
k 1 
k 1 
= exp(-ko) exp[-k2(#2-(# - k~-k-~2)Z)]exp[-kz(x-(p - 2---~2))2 ]. 
(223.17) 
Except for the normalization constant the density p(x[#,a2) has the form of a normal 
distribution with the expected value #-k 1/2k 2 and the variance 1/2k 2, since k2>0. The 
density therefore stems from a truncated normal distribution defined on the interval 
[0,~). It can be shown (Dowson and Wragg 1973) that the density function exists for 
(#<#2. For this case the normalization constant follows from (223.13) by 
exp(k o) = 7 exp[-klX-k2(x-u)2]dx. 
O 
The constants k 1 and k 2 are determined with (223.11 ) by 
exp(-k o) 7 x exp[-klx-k2(x-#)2]dx = # 
O 
and 
exp(-k o) 7 (x-#) 2 exp[-klX-k2(x-#)2]dx = 02, 
O 
so that (223.8) is proved. 
It should be noted that for k2=0 we obtain (223.15) and thus the exponential distribution, 
for which according to (223.16) 0"2=# 2 is valid, ff o2 approaches #2, the truncated normal 
distribution therefore adopts the shape of the exponential distribution. If, on the other 
hand, 0"2 goes to zero, the probability mass of the truncated normal distribution concen- 
trates around the expected value and the truncated part becomes meaningless. The trun- 
cated normal distribution then adopts the shape of the normal distribution, 
n 
The theorem (223.5) confirms the noninformative prior density function (222.1), 
which introduces a constant for the density function of a parameter for which no 
prior information is available. 

23 
The theorem (223.6) emphasizes the importance of the normal distribution as a 
prior density. According to (All.3) the density function of a normally distributed 
random variable is uniquely determined by its expected value and its variance. It 
is therefore a remarkable fact that if the expected value and the variance of a 
random variable are given, the normal distribution contains among all distributions 
the largest amount of uncertainty which is compatible with the given two values. 
The normal distribution should therefore be introduced as a prior distribution for a 
parameter which is defined for the whole real axis and for which prior information 
is available on its expected value and variance. 
If a parameter like a variance is defined for positive values only and if its ex- 
pected value and its variance are known in advance, then the truncated normal 
distribution (223.8) should be taken as a prior distribution provided o2<# 2. This 
distribution degenerates to an exponential distribution in the case of ~r2=#2, as 
shown above. The constants k 1 and k 2 of the truncated normal distribution have to 
be determined numerically, for instance by numerical iterations. For this procedure 
the integrals of (223.8) defining exp(ko), # and or2 need to be computed. By 
taking the representation (223.17) for the density of the truncated normal distribu- 
tion the following integrals have to be computed 
I 1 = ~ exp[-k2(x-#+kl/2k2)2]dx, 
O 
12 = ~ x exp[-k2(x-#+kl/2k2)2]dx, 
O 
13 = ~ (x-#)2 exp[-k2(x-#+kl/2k2)/]dx. 
(223.18) 
o 
To solve the first integral we substitute 
y = q~ (x-#+kl/2k 2) 
with dy = ~dx 
and find 
I1 _ 
1 
~ exp(-y/)dy 
q~ ~2 (-#+kl/2k2) 
1 (~__)1/2 
I {1-erf[q~ (-#+kl/2k2)]} , 
if 
-# + kl/2k 2 > 0 
= ~ k2 
t {l+erf[~ 2 (#-kl/Zk2)]}, 
if 
-# + kl/2k 2 < 0, 
(223.19) 
where erf denotes the error function (Abramowitz and Stegun 1965, p.297), which can 

24 
be readily computed, for instance by rational approximations (Abramowitz and Stegun 
1965, p.299). However, the absolute value of the argument of the error function should 
be small. This is no longer ensured, if the truncated normal distribution approaches the 
normal distribution or the exponential distribution, as explained above. A numerical inte- 
gration should then be applied. 
To compute the second integral 12 in (223.18) we introduce the identity 
12 = ~ (x-#+kl/2k2)exp[-k2(x-#+kl/2k2)2]dx + (#-kl/2k2)I 1. 
O 
By a change of variables 
y = (x-#+kl/2k2)2 
we find 
1 
12 = 
with dy = 2(x-#+kl/2k2)dx 
~exp(-k2Y)dy + (#-kl/2k2)I1 
(-#+kl/2k2)2 
1 [ 
1 exp(_k2Y ) 
+ 
= 2 
- F22 
(_#+kl/2k2)2 
(#-kl/2k2)I1 
1 exp[ 
+ 
= ~ 
-k2(-#+k1/2k2 )2] 
(#-k1/2k2)I 1 â¢ 
FinNlythe third integrN 13 of(223.18) gives 
I3 = J3 - 2#I2 + #211 
with 
J3 = 
To solve 
I1= 
Thus, we 
J3 = 
x exp [ -k2 (x-#+kl/2k2)2] dx. 
O 
J3 we integrate I t by parts 
~ exp[-k2(x-#+kl/2k2)2]dx = [x exp[-k2(x-#+kl/2k2)2]]o 
0 
x exp [ -k2(x-#+k 1/2k2)2] [ -2k 2 (x-#+k 1/2k 2) ]dx 
o 
2k2J 3 + 2k2(-#+kl/2k2)I 2. 
obtain 
1 
I1 - (-#+kl/2k2)I 2. 
(223.20) 
(223.21) 
(223.22) 

25 
224 Conjugate Priors 
Prior information may also be introduced by conjugate priors, also called natural conju- 
gate priors. If a conjugate prior belongs to a certain family of distributions, then for any 
number of observations the posterior density belongs to the same family of distributions. 
This is the most important property of conjugate priors. As a consequence we are able to 
work with analytically tractable distributions, since the start with a tractable prior will 
lead to a tractable posterior distribution. 
Only a short explanation of conjugate priors will be given. A more rigorous presentation 
can be found in DeGroot (1970, p.159), Raiffa and Schtaifer (1961, p.43), t~lz (1983, 
p.28). Let ti=ti(Y ) with t=(ti) and i~{1 ..... r} be sufficient statistics. They con- 
dense the information on the unknown parameters 0, which is contained in the data y, 
into single variables such that no information on 0 is lost. The statistics t are said to be 
sufficient for 0, if the likelihood function p(y[ 0) can be factored as 
p(y[ o) = g(tl0)h(y), 
(224.1) 
where the function g( t [ 0) is non negative and depends on y only through t and h(y) is 
non negative and does not contain t9. The relation (224.1) is called the factorization theo- 
rem (Raiffa and Schlaifer 1961, p.33; Rao 1973, p.131). 
A natural conjugate prior p(01 t) of the unknown parameters 0 as a function of t is de- 
rived by 
p(01 t) Â¢Â¢ g(t[0), 
(224.2) 
where the density p(0lt) obtains its functional form from g(tl0), but the roles of the 
statistics t and the parameters 0 are interchanged. In g(tl0) the parameters 0 are fixed 
and the statistics t are random, while in p(0 t t) the parameters 0 are random and the 
statistics t are fixed. 
Without mentioning it, a conjugate prior has been already applied in Example 1 of Sec- 
tion 211 for the unknown expected value of independent, identically and normally dis- 
tributed observations. This can be seen with the likelihood function (211.14), which can 
A 
be factored according to (224.1) with h(y)=l. Using (224.2) with #p=# and 6~=62/n 
leads to the conjugate prior (211.8), which is normal. The posterior distribution (211.17) 
is also normal. Conjugate priors for the parameters of various standard distributions are 
derived in DeGroot (1970, p. 164), Raiffa and Schlaifer (1961, p.261). 
Since we will discuss linear models in Section 31, the conjugate priors for the unknown 
parameters of these models will be derived here. According to (311.6) the likelihood 

26 
function follows with 
p(y]~l,~2) = (2zcr2) "n/2 exp[- ~ 
(y-X~)'(y-X~)], 
(224.3) 
where y is the nxl random vector of observations, X the nxu matrix of known coeffi- 
cients with rankX=u, ~ the uxl vector of unknown parameters and 0"2 the unknown vari- 
ance factor with c#>0. The statistics or the estimates ~ and ~r2 of the unknown parameters 
~1 and crZ are introduced by (Koch 1988a, p.t87,192) 
~= (X'X)'Ix'y 
and ~#= 
1 
(y-X~)'(y-X~). 
(224.4) 
n-u 
By completing the square of the exponent we find 
(y-X~)' (y-Xi6) = (y-X~-X(O-~))' (y-X~X(~-~)) 
= (y-~)'(y-~) 
+ (i~-~)'X'X(IL~) 
(224.5) 
because of 
(~)'X" (y-X~) = (~-~)'X' (y-X(X'X)- lx'y) = 0. 
This gives the likelihood function in the form of 
1 
^ 
p(yt~,cr 2) = (2zcr2)-n/2exp{- ~---d,2-[(n-u)ff2 + (~-~)'X'X(~-~)]}. 
With h(y)=l we see by the factorization theorem (224.1) that ~ and A 
e are sufficient sta- 
tistics for t[I and or2. Instead of a 2 we will use the weight or the precision parameter z 
with 
~r = 1/o 2 and 
â¢ > 0, 
(224.6) 
so that the likelihood function follows with 
,/7 
^ 
p(y]/},v) = (27c)-n/2~/2 exp{- ~[(n-u)e2 + (~I-~)'X'X(0-~)]}. 
(224.7) 
The conjugate prior for/~ and "c is now derived by rewriting (224.7) according to (224.2). 
With 
1 
^ 
~ 
=n 
# = 
, V "t = X'X, b = 2(n-u)e2, 
+ p - 1 
and neglecting the constants we obtain the conjugate prior 
p(t6,v]#,V,b,p) ~, vu/2+p-1 exp{- ~[2b+(~#)'v-l(~#)] }. 
(224.8) 
This is, after introducing the appropriate constants from (A23.1), the density of the nor- 
mal-gamma distribution 
/l,'r - NG(/~,V,b,p). 
(224.9) 

27 
Now we have to show that using (224.8) as a prior distribution leads to a normal-gamma 
distribution for the posterior distribution, if the likelihood function of the observations 
stems from a normal distribution. 
Theorem: Let the likelihood function of the observations y be determined by the normal 
distribution Y l 0, v - N(XO, "c-1 I) under the condition that 0 and "c are given. Let the pri- 
or distribution of the parameters ~ and "c have the normal-gamma distribution 
/I, "c - NG (gt, Â¥, b, p), then the posterior distribution of ~ and T is also normal-gamma 
/i, vly - NG(#o,Vo,bo,Po) 
with the parameters 
/.t o = (X'X+V-1)-I(x'y+V-1/t) 
v o = (x,x+v-1) 
-1 
b o = (2b+(/t-/to)'Y-l(/t-/to) 
+ (y-X~Uo)'(y-X/to))/2 
Po = (n+2p)/2. 
(224.10) 
Proof. By multiplying the prior density of 1~ and v from (224.8) by the likelihood func- 
tion (224.3) after substituting (224.6) we obtain with Bayes" theorem (211.1) the posteri- 
or density of ~ and "c as 
p(~l, viy ) o, vu/2+p-1 exp{- ~[2b+(/1-#)'Â¥-1(~-#)]} 
v n/2 exp[- ~(y-Xfl)'(y-X~i)] 
v n/z+p+u/2-1 exp{- ~[2b+(fl-/~)'Â¥'l(/J-/D+(y-Xfl)'(y-Xfl)]}. 
Completing the squares on ~1 gives 
2b + ~I'(X'X+V-1)/J - 2/~'(X'y+y-1/t) + y'y + #'Â¥-l/t 
= 2b + y'y + /t'V-1/t + (/J-/to)'(X'X+Â¥-l)(~#o) 
- /to'(X'X+V-1)g Â° 
with 
/t o = (X'X+g'l)-l(x'y+Â¥-l/t). 
The posterior density follows with 
p(0, vty) ~ z u/2+(n+2p)/2-1 exp{- ~[2b+y'y+/t'Y-1/t-/to'(X'X+y-1)g Â° 
+ (/l-/t o) '(x'x+v-1)(0-/to)] }. 

28 
Furthermore we have 
+ y,y + #,Â¥-1# _ #o,(X,X+V-1)/lo 
2b 
= 2b + y'y + /t'V- 1# . 2# o, (X'y+Â¥- 1#) + #o' (X'X+Â¥- 1)/to 
= 25 + (#-#o)'Â¥'1(#-/~o) + (y-g#o)'(y-X~o). 
(224.11) 
After substituting this result we recognize p(~,,ly) with (A23.1) as the density of the 
normal-gamma distribution with the parameters given in (224.10). 
n 
If as a prior density for the parameters ~ and ~ the normal-gamma distribution 
~,I: - NG(/~,V,b,p) 
(224.12) 
is chosen, the parameters #, Y, b, p of the normal-gamma distribution need to be speci- 
fied. This is readily done, if the expected value E(~) of ~! and its covariance matrix D(~) 
are given with 
E(~) = #p 
and D(~I) = Z/~ 
(224.13) 
and equivalently for Â¢r2 instead of "c with 
E(cr 2) = Â¢r~ and V(cr2) = Vcr 2. 
(224.14) 
According to (A23.3) the marginal distribution of ~ in (224.12) is the multivariate t-dis- 
tribution 
/} - t(/~,bÂ¥/p,2p) 
(224.15) 
with the expected value and the covariance matrix from (A22.7) 
E(/}) = /* and D(~) = b(p-1)-lÂ¥. 
(224.16) 
The marginal distribution of v in (224.12) is according to (A23.4) 
- G(b,p) 
(224.17) 
and for cr2=l/.r according to (A13.1) the inverted gamma distribution 
or2 - IG(b,p) 
(224.18) 
with the expected value and variance from (A13.2) 
E(Â¢#) = b/(p-1) 
and V(cr 2) = b2/((p-1)2(p-2)). 
(224.19) 
Substituting (224.16) and (224.19) in (224.13) and (224.14) gives 
#= /lp, Y : Z~/a~, p = (cr~)2/V(r 2 + 2, b = (p-1)a~, 
(224.20) 
which determines the parameters of the prior density (224.12) by means of the expected 
values and the variances and covariances of ~l and o'2. The result for the parameter g 

29 
could be expected, since ~ under the condition that "c is given has the covariance matrix 
~Y~-'c- 1Â¥=o'2V, as can be seen from (A23.2). 
We will apply the results of (224.10) and (224.20) to the special problem of extending 
the Example 1 of Section 211 such that both the expected value and the variance of the 
observations are unknown. 
Example 1: We assume X= [ 1 ..... 1 ]' and fl=# in (224.10). This gives with 
n 
(y-X~)'(y-X~) = 
Y~ (Yi-#)2 
i=l 
instead of (224.3) the likelihood function 
p(y[#,a2) = (2zra2) -n/2 exp[- 
1 
l~l(Yi_#)2] , 
(224.21) 
which has the functional form of the likelihood function (211.10) of Example 1 of Sec- 
tion 211. However, both # and (r2 are unknown parameters in (224.21). As prior distribu- 
tion for # and v=-I/a2 the normal-gamma distribution is assumed 
#, â¢ - NG(#,V,b,p), 
(224.22) 
whose parameters shall be determined by prior information on the expected values and 
variances of # and o 2. Hence, 
E(#) = #p, V(/l) = a~ 
E(o 2) = ~, V(a 2) = Va2 
(224.23) 
so that with (224.20) the parameters in (224.22) are determined by 
[z = #p, V = a21a 
2#_p, p = (a~)i/Va2 + 2, b = (p-1)a~. 
(224.24) 
The posterior distribution of # and "c is given by 
#, I: I Y - NG ( #o' Vo' bo' Po ) 
(224.25) 
with the parameters defined in (224.10). The marginal posterior distribution for # fol- 
lows from (A23.3) by the t-distribution 
#[y - t (#o'boVo/Po'2Po) 
(224.26) 
with the expected value and variance from (A22.7) 
E(#) = #o 
and 
V(#) = boVo/(Po-1 ). 
With (224.10) we find 
n 
E(#) = #o = (n+V-1)-l( Â£ Yi +V-I~) 
(224.27) 
i=1 

30 
and 
1 
n 
V(#) = ~ 
[2b+(~-/.to)2V- 1+ Â£ (Yi-#o)2] (n+V- 1)- 1. 
i=l 
^ 
By substituting # from (211.11) we obtain with (224.24) 
^ 
^ 
- 
-1 
n#f~+#p~ 
#(~/n)l+#p(6~) 
="Â°= 
n,a,g 
= 
E(#) 
and 
v(#) 
(224.28) 
(224.29) 
vlY - G(bo,P o) 
or with (A13.1) for ff 2 by the inve~ed gamma distribution 
a2]y - IG(bo,Po ). 
Thus, we have with (A13.2) 
E(62) = bo/(Po-1) 
and V(62) 
or with(224.10) 
i 
(2b+(h_.~2 V- 1 
n 
E(62) 
+ Â£ -u(Yi 
-~A)2) 
\ 
\r'P~O/ 
i=l 
and 
2(E(62)) 2 
V(6z) = n+2p-4 
By substituting (224.24) we finally obtain 
n 
E(62) = [262+2((y2)3/V,,r2p 
P 
~, + (#p-#o)26~/6~ +lZl(Yi'#o 
)2].= 
[n+2 (~)2/V62+2 ] - 1 
= bg/((Po-1)2(Po-2)) 
(224.31) 
(224.32) 
(224.33) 
(224.34) 
Because of (224.25) the marginal posterior distribution of "c is obtained with (A23.4) by 
the gamma distribution 
n 
= [2+2(a~)Z/Vg2+(#P'#Â°)2/~ 
+i 
(Yi-PÂ°)2/6~] 
[ (n+2(a~)2/V62+2) ((6~/n) - 1+(6~) - 1) ] - 1. 
(224.30) 
A 
The variance of gp is 6~ and the variance of # resulting from the prior information is 
6~/n. The expected value E(#) of # is therefore obtained with (224.29) as the weighted 
A 
mean of # and gp with (a~/n) -1 and (a~) -1 serving as weights. The weight (V(#))-1 of 
# from (224.30) results from the sum of the two weights modified by a factor. The re- 
sults are therefore similar to (211.18) and (211.t9). 

31 
and 
V(~2) = 
2(E(t~2)) 2 
n+2 (crn2) 2/Vtr 2 - 
( 224.35 ) 
1" 
A 
225 Constraints for Parameters 
In a latter section constraints will be imposed on the values which the parameters 0 can 
take. A logical way to proceed would be to choose the prior distribution for 0 such that 
the constraints are fulfilled. For instance, a linearly constrained least squares estimator 
for the parameters of a linear model may be derived with a limiting process by a se- 
quence of Bayes estimators with a corresponding sequence of prior distributions (Pilz 
1983, p.82). On the other hand, we may solve the unconstrained problem first and then 
impose the constraints on the posterior distribution. Both methods give identical results, 
as shown in the following 
Theorem: Let C be a constraint such that 
Oe e 
with 
e 
c o, 
c 
c 
i.e. e c is a subspace of the parameter space e. Let O r be a subset of O in the space o r 
with e r c e c. Then the posterior distribution of O r given the constraint C is equal to the 
posterior distribution for O r without the constraint C multiplied by a factor depending 
on C 
P(Cl0 ,y) 
P(Or [C,Y) : P(Or lY) 
(225.1) 
P(CIy) 
Proof." From the definition of the conditional probability P(A IB ,D) of the event A, given 
that the events B and D have occurred, we obtain (Koch 1988a, p.93) 
F(AnBnD) 
P(Ac~nD)/P(D) 
P(Ar~ [D) 
P(AtB'D) = 
P(BnD) 
= 
P(BnD)/P(D) 
= P(BID ) 
Furthermore, 
P(B IA,D) 
P(Acq3nD) P(Ar~3nD)/P(D) 
P(Ac'~ D) 
= P(AnD) 
= 
P(AnD)/P(D) 
= P(A D) 
and by substituting this result in the first expression we find 
P(AIB,D) = P(AID)P(B]A,D)/P(BID ). 
(225.2) 
Thus, 

32 
p(or~e r It,y) = P(O~o r [y)P(Cloree r,y)/p(C [y)- 
The probability for O r e e r can be thought of as resulting from a cumulative distribution 
function, so that the posterior distribution for O r given the constraint C follows with 
P(C[ O r,y) 
p(orlC,y) = p(Orty) 
n 
P(CIY) 

33 
23 Point Estimation 
231 Quadratic Loss 
Bayes" theorem (211.1) leads to the posterior distribution p(0ly ) of the unknown pa- 
rameters 0 given the data y. All inferential problems concerning the paramters 0 can 
now be solved by means of p(01y). Based on this distribution we will estimate the un- 
known parameters, establish confidence regions for them and test hypotheses concerning 
the parameters. 
We start with the estimation, also called point estimation, in contrast to the estimation of 
A 
confidence regions. The estimates 0 of 0 shall be determined by the observations y, 
A 
A 
hence 0=0(y). Based on the posterior distribution p(Oly) it has to be decided which are 
A 
the estimates 0 of the values of the parameters 0. The estimation may be therefore 
A 
viewed as a statistical decision problem. With each decision ending in an estimate 0 of 
A 
the parameters 0 a loss L(0, 0) is connected. The posterior expected loss of the estima- 
tion is defined by the expected value of the loss computed by the posterior density 
A 
A 
E(L(O,0)) = ~ L(0,0) p(0ly)d0. 
(231.1) 
O 
A Bayes estimator is now simply found by minimizing the posterior expected loss. In 
general we use the 
Definition: A decision, which minimizes the posterior expected loss, is called a Bayes 
rule. 
(231.2) 
In order to derive a Bayes estimator by means of the Bayes rule, we have to specify the 
A 
loss. It is very simple to work with the quadratic loss defined by means of the error O- 0 
A 
of the estimate 0 
A 
A 
A 
L(0,0) = (0-0)'P(0-0), 
(231.3) 
where P is assumed to be a given positive definite matrix of constants serving as a 
weight matrix. To compute the posterior expected loss we use the identity 
A 
A 
A 
A 
E[(O-O)'P(O-O)] = E{[O-E(O)-(O-E(O))]'P[O-E(O)-(O-E(O))] ) 
A 
A 
= E[(0-E(0))'P(0-E(0))] + (0-E(O))'P(0-E(0)) 
(231.4) 
because of 
A 
E[(O-E(O))']P(O-E(O)) = 0 
with E[O-E(O)] = O. 

34 
/k 
The ftrst term of (231.4) does not depend on 0, while the second term is minimum for 
A 
013 = E(O), 
(231.5) 
A 
since P is positive definite. Thus, 013 is the Bayes estimate, when the loss function is 
quadratic. The estimate is defined by the expected value for 0 computed with the posteri- 
or density 
= f OP(Oly)d0. 
(231.6) 
e 
A 
In the following we will mostly apply this Bayes estimate 0 B. To express its accuracy, 
A 
we introduce the covariance matrix Y'O of the estimate ~ in analogy to (231.5) and 
(231.6) by 
X 0 = E[(O-E(O))(O-E(O))'] = I (O-E(O))(O-E(O))'p(Oly)dO. 
O 
By substituting (231.7) in (231.4) we find (Koch 1988a, p.156) 
E[ (O-E(O)) "P(O-E(O) )] = E{ t r [P(0-E(O) ) (O-E(O)) '] } = trPZ O, 
so that the posterior expected loss for the Bayes estimate (231.5) is obtained by 
â¢ 
A 
E((O-~ B) P(O-0B)) = trPX 0. 
(23t.7) 
(231.8) 
Example 1: The expected value (211.18) of the parameter g of Example 1 of Section 211 
A 
A 
gives the Bayes estimate #B of #, hence #B=E(#) and the variance V(#) from (211.19) 
A 
leads to the variance or2 u of #B' thus ~r~=V(#). The same holds true for the expected value 
and the variance (222.16) of # of Example 3 of Section 222 and for the expected values 
(224.29) and (224.34) and the variances (224.30) and (224.35) of the parameters # and 
or2 of Example 1 of Section 224. 
A 
232 Different Estimators 
A 
Different loss functions lead to different estimators. Let 0 be the estimate of 0 with 
A 
A 
O=(0i), 0=(0i) and ie{1 ..... u}. If the absolute error 10i-~i ] of the estimate ~i is 
chosen as a loss, we minimize the posterior expected loss 
A 
A 
E(L(0 i,0i)) = I 10i-0ilp(0ly)d0. 
(232.1) 
O 
Let the domain of the integration, the parameter space o, be defined by the inequality 
0oi < 0 i < 01i 
for 
ie{1 ..... u}, 

35 
A 
so that we obtain, since t 0 i "0 i I is positive, 
^ 
A 
0 
01 
^ 
U 
^ 
E(L(0i,0i)) 
= I 
... I 
(0i-0 i) P(0Iy) d0 t...d0 u 
0o u 
0ol 
01u 
011 
^ 
+ ~ 
... ~ 
(Oi-O i) P(OIy) dO1.. 
A 
A 
0 u 
01 
^ 
^ 
0 
01 
^ 
A 
U 
- f 
... 
f 
0iP(Oly) 
d01...d0 
= OiP(O[ Y) 
Oo u 
0o 1 
u 
01 u 
+ 
S 
... 
^ 
0 
U 
where 
A 
0 
^ 
U 
P(Oly) : j" 
0 
OU 
.dO 
U 
011 
^ 
^ 
J 
0iP(Oiy) d01...d0 u 
0i(1-P(Oly)), 
A 
01 
(232.2) 
A 
01 
J" p(Oly) dO1.. 
ool 
.dO 
U 
denotes the cumulative posterior distribution function. To find the minimum of (232.2), 
A 
we differentiate with respect to 0 i and set the derivative equal to zero. We obtain, since 
^ 
the results of the differentiation with respect to the limits 0 i of the integrals in (232.2) 
cancel, 
A 
A 
A 
A 
0E(L(0i,0i))/00 i = P(01y) - 1 + P(01y ) = 0 
or 
^ 
1 
P(0IY) = 2 ' 
(232.3) 
Thus, the Bayes estimate 0 in 
^ 
case of the absolute errors I 0 i - 0 i I as a loss is the median 
A 
of the posterior density function, that is the value 0 for which according to (232.3) the 
cumulative distribution function is equal to 1/2. The median (232.3) minimizes (232.1), 
since the second derivative 02E(L(0i, 0i ) )/00~ is positive. 
In analogy to the standard statistical techniques the generalized maximum likelihood esti- 
mate ~ of the parameter vector 0 is the mode of the posterior density p(01y ), i.e. the 
value for 0 which maximizes p(Oly), hence 

36 
= supo P(OtY). 
This estimate is also called the maximum a posteriori or MAP estimate. 
(232.4) 

37 
24 Confidence Regions 
241 H.P.D. Region 
If the posterior density function p(0[ y) of the parameter vector 0 has been determined 
by Bayes' theorem (211.1), we may compute the probability that the vector 0 lies in a 
subspace o s of the parameter space o with OsC o by 
P(O~ Os[Y) = ~ p(0[y)dO. 
(241.1) 
O 
S 
Often we are interested in finding the subspace, where most, for instance 95 per cent, of 
the probability mass is concentrated. Obviously there are an infinite number of ways to 
specify such a region. To obtain a unique solution, provided the density has only one 
modal value, i.e. one maximum, the region should be defined such that the density of 
every point inside is at least as large as for any point outside of it. A region with such a 
property is called a region of highest posterior density (H.P.D. region) or Bayesian confi- 
dence region. In addition it has the property that for a given probability mass it occupies 
the smallest possible volume in the parameter space (Box and Tiao 1973, p.123). "/?he 
first property will be applied in the 
Definition: Let p(01 y) be a unimodal posterior density function of the parameter vector 
0. A subspace B of the parameter space of 0 is called H.P.D. region, Bayesian confi- 
dence region or just confidence region of content 1 - a, if 
P(0 e Bly ) = 1-a 
and 
P(011Y) >_ P(021Y ) 
for 
01 e B, 
02 ~ B. 
(241.2) 
As in the standard statistical techniques we will use small values of a, say a=0.0t, 
a---0.05 or a=0.1. In general we set a=0.05, so that a 95 per cent confidence region is 
defined. 
242 Boundary of a Confidence Region 
After having established a confidence region for the parameters 0 the question may arise, 
whether a particular value 0 o of the parameter vector lies inside or outside the confi- 

38 
dence region. The event OeB [y of (241.2) is equivalent to p(Oly)>b, where b is a con- 
stant and equal to the posterior density at the boundary of the confidence region 
b = P(Ob[Y) 
(242.1) 
with 0 b denoting a point at the boundary. Hence, we define instead of (241.2) 
P(0 ~ Bly) = P(p(0ly) > b) = 1-m 
(242.2) 
A particular value 0 o of 0 lies inside the confidence region, if 
P(p(O[y) > p(OolY)) < 1-a 
(242.3) 
or if the inequality is fulfilled 
p(Oo[y) > b. 
(242.4) 
The last inequality represents a very simple way of checking whether 0 o lies inside the 
confidence region. 
In the expressions (242.2) and (242.3) the density function p(01 y) has the meaning of a 
random variable. 
Example 1: Let us assume the posterior density p(01 y) of the uxl parameter vector 0 is 
given by the multivariate t-distribution t(#,N-1 ,v), which is obtained by (A23.3) as a 
marginal distribution of the normal-gamma distribution, which in turn is defined accord- 
ing to (224.10). As can be seen from (A22.1), p(0[y) is a monotonically decreasing 
function of (0-g)'N(0-#). But this quadratic form divided by u has according to 
(A22.13) the F-distribution F(u, v) 
(0-#)'N(0-#)/u - F(u,v). 
Hence, according to (242.2), where the greater than sign has to be replaced by the 
smaller than sign, since p ( 01 y) decreases when (0-p)' N(0-~t) increases, the confidence 
region of content 1 - c~ is defined by 
P((0-#)'N(O-/.t)/u < Fl_c~;u,v ) = 1-m 
(242.5) 
Fl_~;u, v denotes the upper a-percentage point of the F-distribution with u and v de- 
grees of freedom (Koch 1988a, p.150). The boundary of the confidence region is ob- 
tained by 
(0-#)'N(O-#)/u = Fl_a;u, v , 
(242.6) 
i.e. by a hyperellipsoid with the center at the point g (Koch 1988a, p.328). 
To answer the question whether a particular point 0 o lies inside the confidence region, 
we merely have to check according to (242.4), if 

39 
(Oo-~)'N(Oo-fl)/u < Fl_a;u, v 
is fulfilled. 
(242.7) 
A 

40 
25 Hypothesis Testing 
251 Different Hypotheses 
Assumptions about the unknown parameters are called hypotheses. Let OoC o and OlC o 
be two subsets of the set o of the parameter vectors, the parameter space. Let o o and 01 
be disjoint, hence Oon o1=0. Then the assumption that the parameter vector 0 belongs to 
the subset o o is called the null hypothesis and the assumption that 0 belongs to o 1 the 
alternative hypothesis, hence 
H o : 0~ o o 
versus 
H 1 : 0e 01 . 
(251.1) 
Frequently, o 1 is the complement of o o 
o 1 = o\o o. 
(251.2) 
The subset o o is assumed to contain more than one element, (251.1) is therefore called a 
composite hypothesis in contrast to the simple hypothesis 
H o : O= 0 o versus H 1 : 0= 01, 
(251.3) 
where the subsets o o and o 1 contain only the elements 0 o and 01, respectively. 
If the subset o o in (251.1) consists only of the point 0 o and the subset o 1 is the comple- 
ment of o o according to (251.2), we obtain the point null hypothesis 
I-I o : 0= 0 o 
versus 
H 1 : 0~ 0 o. 
(251.4) 
If not the parameter vector 0 itself but a linear combination FI0 needs to be tested, where 
1t denotes a given matrix of constants, we write instead of (251.1) 
H o : It0~ 6 o 
versus 
H I : lt0e 61 , 
(251.5) 
where 60 and 6 t are subsets of the parameter space 6 of the transformed parameters It0. 
The hypotheses (251.3) and (251.4) for the linearly transformed parameters are obtained 
accordingly. The latter hypothesis corresponds to the general linear hypothesis of a 
Gauss-Markoff model (Koch 1988a, p.307). 
252 Hypothesis Testing by Confidence Regions 
Let us assume that (251.2) in connection with (251.1) is valid, so that the hypothesis is 
given 

41 
H o : 0~ e o 
versus 
H 1 : ee e\e o. 
(252.1) 
To decide whether to accept or to reject the null hypothesis, we compute by means of 
the posterior density p(0Iy ) according to (241.1) the probability of 0 E o o with 
P(0 e Oo]Y) = ~ p(0ly)d0. 
(252.2) 
e 
o 
By following the argument of the standard statistical techniques we reject the null hy- 
pothesis of (252.1), if 
P(0 e OolY) > 1-co, 
(252.3) 
where 1-g denotes the content of the confidence region (241.2). This can be explained 
by assuming e\e Â° as the region of rejection of the test. The probability of the Type I er- 
ror of the standard statistical techniques, that is of the rejection of a true mill hypothesis, 
is then defined to be c~ 
P(0 e O\OolY) = 0~ or 
P(0 ~ OolY) = 1-a. 
If o o is now given such that (252.3) is valid, o o extends into the region of rejection and 
the null hypothesis has to be rejected. 
We can use the relation (252.3) also for the test of the point null hypothesis (251.4). The 
shape of the subspace e Â° in (252.3) may be arbitrarily defined, so that we give it the 
shape of the confidence region B defined by (241.2). The test of the point null hypothe- 
sis (251.4) is then reduced to the check, whether the point 0 o of (251.4) lies inside or 
outside the confidence region B. Thus, in case of testing 
Ho : 0= 0 o 
versus 
H I : 0# 0 o 
we reject the null hypothesis, if according to (242.4) 
p(0oly) < b 
(252.4) 
is fulfilled. The constant b is defined with (242.1) and denotes the posterior density at 
the boundary of the confidence region B. If not the parameter vector 0 itself but a linear 
combination It0 of the parameters has to be tested, the posterior density for the linear 
combination is determined and with it the test runs off correspondingly. 
The test procedure (252.4) can be substantiated by the fact that the posterior density con- 
tains the information on possible values of the parameters. If the value 0 Â° lies in a re- 
gion where the posterior density is low, we should not trust this value and should reject 
the hypothesis. 
When applying this method of hypothesis testing, the prior distribution for the parame- 
ters should be smooth in the vicinity of the point 0 o, i.e. the prior density should not 

42 
change much when using instead of 0 o a point in the neighborhood of 0 o (Lindley 1965, 
Part 2, p.61). This will be true for many applications and it will be always true in the 
case of noninformative priors. If the prior density changes rapidly in the vicinity of 0 o, 
or if special priors have to be associated with the hypotheses, the procedure of Section 
254 should be applied. As will be seen when testing the point null hypothesis, this meth- 
od has its deficiency, too, so that in the case of smooth priors it is recommended to test 
the point null hypotheses by means of confidence regions. 
Hypothesis testing by means of confidence regions is equivalent to the test of a general 
hypothesis in the Gauss-Markoff model by the standard statistical techniques (Koch 
1988a, p.331). Both methods therefore give identical results, if the posterior density for 
the unknown parameters or for functions of the parameters agrees with the density of the 
test statistic of the standard techniques. 
Example 1: The confidence region of a parameter vector 0 with a multivariate t-distribu- 
tion was determined in the Example 1 of Section 242. It was also checked by the ine- 
quality (242.7), whether a point 0 o lies inside the confidence region. Hence, if we test 
Ho : 0= 0 o 
versus 
H 1 : 0# 190 , 
the null hypothesis n Â° is accepted according to (252.4), if the inequality (242.7) is ful- 
filled. 
A 
253 Posterior Probabilities of Hypotheses 
Corresponding to the point estimation, the test of the hypothesis (251.1) can be viewed 
as a decision problem, to accept H Â° or to accept H 1, which are two actions. Two states of 
nature are connected with the hypothesis, either H Â° is true or H 1 is true, thus, we have a 
two-state-two-action problem. Correspondingly, the loss connected with an action H i in a 
specific state 0 ~ o i has to be defined for four values 
L(0e oi,ni) = 0 
for 
ie{0,1] 
L(0~ ei,Hj) # 0 
for 
i,j~{0,1}, i#j. 
(253.1) 
Zero loss is assumed for the correct decision, accept H o and H 1 if 0 ~ e Â° and 0 E o 1, re- 
spectively, and loss not equal to zero occurs for a wrong decision. 
To reach a decision based on the Bayes rule (231.2), the posterior expected loss of the 
actions has to be computed. Hence, we need the posterior probability P(H Â° ]y) for the 
hypothesis H o and P(HIIY ) for H 1. These probabilities are computed according to 
(241.1) by 

43 
P(HitY) = P(0~ oilY) = ~ p(01y)d0 for 
ie{0,1}. 
O. 
1 
If (251.2) holds ~ue, then 
P(H1]Y) = 1-P(Ho[Y ). 
The postefior expected loss for accepfingHofollows with 
E(LIHo) = P(Ho[Y)L(0 ~ Oo,H o) + P(HI[Y)L(0 ~ Ol,H o) 
= P(HIIY)L(O e Ol,Ho), 
since L(O e Oo,Ho)=O from(253.1). Correspondingly 
E(L]H1) = P(HoIY)L(0 ~ Oo,H1). 
By minimizing the expected loss we obtain the Bayes rule for accepting H o 
says, 
if 
E(L[Ho) < E(L[H1), 
accept 
n o 
otherwise accept H 1. Thus, fortesting hypothesis(251.1) 
H o : 0~ o o 
versus 
HI: 0e o 1 
we find the Bayesrule, 
if 
P(HIY)L(0 ~ Oo,H1) 
> 1, 
accept 
H o. 
P(HIIY)L(O ~ Ol, % ) 
In the following we will assign equallossesforthewrong decisions 
L(0 ~ Oo,H1) = L(0 ~ Ol,Ho) 
and obt~n instead of(253.5) the Bayesrule, 
if P(HÂ°Iy) > 1, 
accept 
H 
P(HllY ) 
o 
(253.2) 
(253.3) 
or H 1, which 
(253.4) 
(253.5) 
(253.6) 
otherwise accept H 1. The ratio P(H Â° I Y)/P0t 1 [Y) in (253.6) is called the posterior odds 
for H Â° to n 1, it is computed from (253.2). 
If the subspace o Â° in the hypothesis (251.1) shrinks to the point 0 o, so that in case 
(251.2) is valid, the point null hypothesis (251.4) is obtained, then P(HolY) in (253.6) 
goes to zero, since p(0[y) in (253.2) is continuous. This, of course, is not correct, so 
that a different procedure, which will be presented in the next section, has to be applied 
for testing the point null hypothesis. However, if both subspaces o o and o 1 in (251.1) 
shrink to the points 0 o and 01, so that instead of the composite hypothesis (251.1) the 
simple hypothesis (251.3) is tested, the posterior odds in (253.6) are computed with 

44 
(253.2) by 
P(HolY) 
P(HI[Y) 
lim 
~ 
p(0[y)dO/Ao o 
AOo-~0 Ao o 
P (0 o [Y) 
-
-
 
- 
= 
, 
(253.7) 
lim 
I 
p(Oly)dO/Ao I 
P(81 ly) 
AOl-)0 Ao 1 
where the domains Ao Â° and Ao 1 of both integrals consist of small spaces around the 
points 0 o and 01. For testing the hypothesis (251.3) 
I'I o : O= 0 o 
versus 
I-I 1 : O= 01 
we therefore apply the Bayes rule, 
P(0o]Y) 
if 
> 1, 
accept 
H 
(253.8) 
P(011Y) 
o 
otherwise accept H 1. 
With this result we are able to interpret the hypothesis testing by confidence regions. 
The simple hypothesis 
H o : 0= 0 o 
versus 
H 1 : 0= 0 b, 
(253.9) 
where 0 b denotes a point at the boundary of the confidence region, is tested by means of 
the Bayes rule (253.8), 
P(0o[Y) 
if 
> 1, 
accept 
H . 
(253.10) 
P(0blY ) 
o 
Because of b=p(0b [y) from (242.1), (253.10) is identical with (252.4). 
It is worth mentioning that for one-sided tests 
H o : 0_< 0 
versus 
H 1 : 0> 0 
the Bayes rule (253.6) gives results which are in agreement with the results of the 
standard statistical techniques (Casella and Berger 1987). 
Example 1: Let the observations Yi of Example 1 of Section 211 represent independent 
measurements of the length of a straight line. The prior information #p and (r~ on the un- 
known expected value # of Yi and its variance is given by #p=5319.0cm and 
a~=49.0 cm2. For n=5 observations with variance a2=9.0 cm 2 we obtained from (211.11) 
A 
#=5332.0 cm and therefore from (211.18) and (211.19) the expected value and variance 
of # 
E(#) = 5331.54 cm 
and 
V(#) = 1.74 cm 2. 

45 
Hence, # is normally distributed according to 
/~IY- N(5331.54, 1.74). 
We want to test the hypothesis 
H o : # < 5330 versus 
H 1 : #> 5330. 
It is a composite hypothesis of type (251.1), so that we apply the Bayes rule (253.6). If 
F(x;0,1) denotes the cumulative distribution function of the standard normal distribu- 
tion (Koch 1988a, p.127), we obtain with (253.2) the posterior probability of the null hy- 
pothesis given the data 
P(HolY) = F((5330-5331.54)/~17~;0, 1) = 0.12 
and with (253.3) the posterior probability of the alternative hypothesis 
P(H 1 lY) = 0.88. 
Thus, the posterior odds for H Â° result with 
P(HolY)/P(H 1 IY) = O. 14, 
so that the null hypothesis has to be rejected. This result, of course, could have been 
foreseen, since the normal distribution is symmetrical with the center at 5331.54 in our 
example. 
A 
254 Special Priors for Hypotheses 
The methods of hypothesis testing presented in the preceding Sections 252 and 253 are 
based on the posterior density p(01 y) of the parameters. There are applications, how- 
ever, when specific prior probabilities can be associated with the hypotheses. To deal 
with such a situation, the prior density function p (0) of the parameter vector 0 is con- 
veniently defined by 
p(0) = Poho(0) + (1-Po)hl(0) 
(254.1) 
with Po being a constant and ho(0 ) and hl(0 ) density functions so that (211.6) is ful- 
filled. The density ho(O ) is defined on the subspace o o of the null hypothesis of (251.1) 
and h 1 (0) on e 1. The distributions ho(O ) and h 1 (0) describe how the prior probability 
mass is spread out over the space of the null hypothesis and the alternative hypothesis. 
Using (254.1) together with the likelihood function p(yl0) we compute with Bayes' 
theorem (211.4) the posterior density p(0 t y) by 
P(OlY) = C[Poho(0)+(1-Po)hl(0)]p(yl0) 
(254.2) 

46 
with 
c = 1/~ [Poho(0)+(1-Po)hl(0)]p(y[0)d0. 
e 
Substituting this result in (253.2) leads to the posterior probability P(H o [y) of the hy- 
pothesis H o 
P(Ho[Y) = c~ Poho(0)p(y[0)d0 
(254.3) 
O 
o 
and correspondingly to 
P(H 1 ]y) = cJ (1-Po)hl(0)p(y [0)d0. 
(254.4) 
e 1 
The decision of accepting or rejecting the hypothesis (251.1) is then based on (253.6). 
We let the space e o in (251.1) shrink to the point 0 o by introducing the space he with a 
small volume around the point 0 o and obtain 
lim 
J Poho(0)p(yl0)d0 = pop(yt0o), 
(254.5) 
h~-~0 he 
since p(y[0) can be considered being constant in he and ho(0) is a density defined on 
he. In addition we let e 1 shrink to 01, so that the hypothesis (251.3) is obtained. The 
posterior odds then follow with substituting (254.5) in (254.3) and with an equivalent 
substitution in (254.4) by 
P(Ho I Y) 
PoP(Yl 0 o) 
P(ttl [Y) 
(1-Po)P(Y[01 ) 
For testing the hypothesis (251.3) 
H o 
0= 0 o 
versus 
H 1 : 0= 01 
we therefore apply according to (253.6) the Bayes rule, 
if 
PÂ°P(Y[ 0Â°) 
> 1, 
accept 
H o. 
(254.6) 
( 1 -po)p (y 101 ) 
Finally we let e o in (251.1) shrink to 0 o with (251.2) holding true, so that the point null 
hypothesis (251.4) is obtained. The posterior probability P(Ho[Y) of H o then follows 
with (254.2) and (254.5) from (254.3) by 
P(Ho]Y) = pop(y[ 0o)/[pop(y [ 0o)+(1-Po) ~ hl(0)p(y ] 0)d0]. 
(254.7) 
For testing the point null hypothesis (251.4) 
Ho : 0= 0 o 
versus 
H 1 : 0Â¢ 0 o 
we therefore obtain with (253.3) from (253.6) the Bayes rule, 

47 
PoP(Yl 0 o) 
if 
> 1 accept 
H o 
(254.8) 
(1-Po) ~ hl(0)p(yl0)d0 
otherwise accept H 1. 
It should be mentioned that using (254.8) for testing a point null hypothesis gives results 
that may differ from those of the standard statistical tests. This effect appears, if the pri- 
or density h 1 (0) is spread out considerably because of a large variance for the prior in- 
formation on 0. Then the likelihood function averaged by the integral in (254.8) over the 
space of the alternative hypothesis becomes smaller than the likelihood function 
p(y[ 0o) for the null hypothesis. Thus, H o is accepted although the standard statistical 
test may reject it. This discrepancy was first pointed out by Lindley (1957), see also 
Berger (1985, p.151), and it is called Lindley's paradox (Berger and Sellke 1987; Zell- 
ner 1971, p.304). A criticism of the Bayes rule (254.8) can be found, for instance, in 
(Casella and Berger 1987; Shafer 1982). 
Example 1: We want to test by means of (254.8) the point null hypothesis 
Ho : # = /'to versus 
H1 : # ~ go 
(254.9) 
for the parameter # of the Example 1 of Section 211. The likelihood function p(y[#) is 
therefore given by (21t.14). The density h 1 (#), which according to (254.1) spreads out 
the prior probability over the space of the alternative hypothesis, shall be given by 
(211.8). We therefore obtain the posterior odds P(HoIY)/P(H 1 lY) for H o to H 1 from 
(254.8) by 
P(HIY)/P(tt 1 lY) = ~ 
a# Po exp[- ~ 
(#o-~)21/ 
(#.#p)2 
n 
^ 
7 
1 
{(X-Po) 
exp[- ~ ( 
+ -- (#-#)2)]d#}. 
-oo 
0"~ 
17 2 
Because of (211.16) the integral with respect to # can be represented by an integral over 
the density function (211.17) of a normal distribution of expected value (211.18) and 
variance (211.19). We thus obtain with (All.2) 
(#.#p)2 
n 
^ 
exp[- 
( 
+ -- 
-~ 
cr~ 
a2 
a~cr2In 
112 
a?,+a2In 
= V~2-d (a~+a2/n) exp{- ~ [a~v2/n 
A 
A 

48 
A 
o-~a2/n 
112 
(#_#p)2 
= q"2"-ff ( 
) 
exp[- - 
] 
a~+a2/n 
2(6~+o'2/n) 
and finally 
A 
A 
P(HoIY) 
Po 
(a~+a2/n)a/2 
((#0-#)2 
(#-#p)/ 
- 
exp[- Â½ 
rr~ +a2/n )]. 
P(H 1 lY) 
t-P o 
aa/n 
Â°2/n 
(254.10) 
With large values of (r~ we can make the posterior odds arbitrarily large. As already 
mentioned, this is due to the fact that if we spread out the density hl(#) by means of a~, 
the likelihood averaged by hl(#) in the integral of (254.8) becomes smaller than the 
likelihood function at the point of the null hypothesis. For a special case we substitute in 
(254.10) 
A 
o'~t = a 2, Up=No, z = I -Uol/(cr/vs) 
and find 
P(Ho[Y) 
Po 
(n+l)l/2 exp[- Â½ 
n 
. 
-
-
 
- -- 
z2 (n+--Tf) ] 
(254. t t) 
P(H 1 [y) 
1-Po 
The quantity z is the test statistic of the hypothesis (254.9) by the standard statistical 
techniques (cf. Bosch 1985, p.84). If we fix z at the value for rejecting H o on a signifi- 
cance level of a, for instance z=1.96 for ~0.05, we will still find values for n which 
are large enough to make P(ttolY)/P(H 1 ly)>l, so that tt o has to be accepted. This is 
Lindley' s paradox. 
A 

49 
26 Predictive Analysis 
261 Joint Conditional Density Function 
In the following section and in latter sections we need joint conditional density functions 
of random vectors expressed by marginal conditional density functions. They are ob- 
tained by the definition (211.2) of a conditional density function. Let x 1 , x 2 and x 3 be 
vectors of random variables. Then the conditional density function of x 1 given x 2 and x 3 
follows from (211.2) with 
P(X 1 ,x2,x 31 
P(XllX2,x3) = 
P(X2,X3) ' = 
or 
P(Xl,X2,x3)/P(X 3) 
P(Xl,X2lx 3) 
P(X2,X3)/P(X3) 
= 
P(X2iX3 ) 
P(Xl,X2lX3) = p(x 11x2,x3 ) P(X2[X3). 
(261.1) 
Thus, the joint conditional density function p(x 1 ,x 2 Ix3) of x 1 and x 2 given x 3 is ob- 
tained by the conditional density function of x 1 given x 2 and x 3 and by the marginal 
conditional density function of x 2 given x 3. 
262 Predictive Distribution 
Collecting data or making measurements generally takes time and effort. It is therefore 
appropriate to look for ways of predicting observations. This may be interpreted as either 
interpolating given data or forecasting observations from given data. In either case un- 
observed data are predicted. 
We start from the given observations y, which were introduced in Section 211 as a func- 
tion of the parameter vector 0. Let the posterior density function p(01 y) of the parame- 
ters 0 given the data y be known and let the vector Yu denote the vector of unobserved 
data. The joint probability density function P(Yu' 01 y) of Yu and 0 given the data y is 
then obtained with (261.1) by 
P(Yu,0[Y) = P(Yul0,Y) P(0ly), 
(262.1) 
where p(yu[ 0,y) is the conditional probability density function of y u given 0 and y. If 
the same distribution for Yu is assumed as for the data y, then p(yu[0,y) is known. By 
computing from (262.1) the marginal distribution for Yu (Koch 1988a, p.105) we obtain 

50 
p(yuly ) = .f p(yulO, y)p(Oly)dO, 
(262.2) 
e 
where o again denotes the parameter space. The density (262.2) is the predictive density 
function of the unobserved data vector Yu" Any predictive inference for the unobserved 
data Yu is solved by the distribution P(Yu l Y)- 
Example 1: For the Example 1 of Section 211 we have assumed n independent observa- 
tions Y=[Yl ..... yn ] ', each being normally distributed with unknown expected value # 
and known variance (r2. We want to derive the predictive distribution for the unobserved 
data point Yu" By assuming the same distribution as for the observation Yi the condition- 
al density p(yut #, y) of Yu is obtained from (211.9) by 
p(yul/l,y) ~ exp[- 
1 
2--@7 (Yu -#)2] â¢ 
(262.3) 
The prior density function p(#) of the unknown parameter # follows from the normal 
distribution (211.8) and the posterior distribution p (#IY) from (211.17) to (211.19) by 
P(#1Y) = exp[- 
1 
(#-E(#))z]. 
(262.4) 
The predictive density function p(YulY) of Yu is therefore given with (262.2) by 
p(yuly) = 7 
p(yuI#,y)p(#ly)d/l 
-oo 
7 
exp{-~ [1 
1 
-~ 
0-- 2- (yu-#) 2 + ~ 
(#-E(#))2]}d#. 
(262.5) 
We complete the square on # in the exponent of (262.5) and obtain 
1 
1 
6-7 (Yu-#) 2 + ~ 
(~-E(#))2 
V(#)+o 2 
YuV(#)+E(#)ff 2 
y~V(#)+(E(#))2G 2 
- 
[~2-2# 
] + 
~2v(#) 
v(#)+a2 
a2v(#) 
V(~)+O 2 
YuV(#)+E(#)~ 2 
(Yu-E(fl))2 
- 
[# 
_ 
.]2 
+ 
~2v(#) 
v(#)+G2 
v(#)+a2 
By substituting thisresultin (262.5) we recognize that because of(All.2) the integration 
with respect to # yields a constant. Hence, 
p(yuty ) ~ exp[- 
1 
2(V(#)+~z) (Yu-E(#))z], 
(262.6) 
According to (Alt.1) and (All.3) the predictive distribution for Yu is the normal distri- 
bution 
YulY - N(E(#), V(#)+a2) 
(262.7) 
with the expected value E(#) from (211.18) and the variance V(#)+a 2 with V(#) from 

51 
(211.19). The expected value of the predicted observation Yu therefore agrees with the 
expected value of the parameter #, while 62 is added to its variance, to obtain the vari- 
ance of Yu" 
We now substitute cr2 u -~ ~, which introduces a noninformative prior for the unknown pa- 
l 
rameter ~ instead of the prior density (211.8). This substitution was already applied in 
Example 3 of Section 222 with the results from (222.16) 
A 
E(/z) = # 
and V(//)= ~2/n. 
In the case of a noninformative prior for the parameter # the predictive distribution for 
Yu therefore follows with 
A 
yuly - N(#, a2(n+l)/n). 
(262.8) 
A 

52 
27 Numerical Techniques 
271 Monte Carlo Integration 
In many applications the posterior density function for the unknown parameters resulting 
from Bayes' theorem can be readily written down. This is demonstrated for different 
models in Chapter 3. For estimating the parameters, for establishing confidence regions 
or for testing hypotheses these density functions have to be integrated with respect to the 
parameters. Frequently, however, the integration cannot be solved analytically, so that 
numerical methods have to be applied. 
Well-known methods exist for the numerical integration by quadrature. Special ap- 
proaches well suited for the integrals resulting from Bayesian inference can be found, for 
instance, in Press (1989, p.74). However, these methods become very inefficient with the 
increase of the dimension of the parameter space o. Monte Carlo integration helps to 
overcome this deficiency. It is based on generating random numbers by which an inte- 
gral is computed as an expected value of a function of a random variable. 
Let x contain the values of a random vector, also denoted by x, and let p(x) be a func- 
tion of x. We want to compute the integral 
I = S p(x)dx 
with 
x ~ A 
(271.1) 
A 
with A being the domain of the integration. Let u(x) be the density function of x. We 
rewrite I by 
I = S (p(x)/u(x))u(x)dx, 
A 
so that the integral I can be interpreted as the expected value of the function p(x)/u(x) 
of the random vector x 
I = E(p(x)/u(x)). 
(271.2) 
If a sequence of independent and identically distributed random vectors with the density 
u(x) on A is generated giving x 1 ,x 2 ..... x m, the expected value (271.2) is estimated by 
1 m 
= ~ 
E 
(P(Xi)/u(xi)) 
i=1 
and (Frtihwirth and Regler 1983, p.139; Hammersley and Handscomb 1964; p.57; 
Rubinstein 1981, p.122) 

53 
m 
1 
X 
(P(Xi)/u(xi)). 
(271.3) 
I p(x)dx = 
A 
i=l 
This method of integration is called importance sampling, since the generated data 
points x i are concentrated due to the distribution u(x) in the important part of A. 
The main problem of the Monte Carlo integration (271.3) is that of finding an appropri- 
ate density u(x). It should closely approximate the integrand p(x), so that the ratio 
p(x)/u(x) is nearly constant. This requirement will be seldom fulfilled, especially if 
p(x) is a marginal density function which itself has been determined by numerical inte- 
gration, cf. (273.2). 
The simplest solution to this problem is to spread out the generated data points x i 
evenly, which means assuming u(x) as being uniformly distributed. If the domain A in 
(271.1) can be bordered by parallels to the coordinate axes, we obtain with x=(x 1 ) and 
1= { 1 ..... u } the density function 
I u 
II 
1/(bl-al) 
for 
a 1 _< x 1 <_ b 1 
u(x) = 
I=1 
(271.4) 
0 
for 
x 1 < a 1 
and 
x 1 > b 1. 
By substituting this result in (271.3) the integral (271.1) is computed by 
u 
1 m 
f p(x)dx = [ 1I 
(bl-al) ] ~ 
ZlP(Xi), 
(271.5) 
A 
1=1 
i 
which is called the sample-mean or crude Monte Carlo method. 
If the domain A of the integration cannot be defined by parallels to the coordinate axes, 
then the density function of the uniform distribution of x in A is given by 
u(x) = 1/VA, 
(271.6) 
where V A denotes the volume of A. To generate uniformly distributed data points x i in A, 
we may start from generating in a rectangular space enclosing A and omit all vectors out- 
side A. With m vectors x i in A we obtain by substituting (271.6) in (271.3) 
m 
f p(x)dx = (VA/m) Z P(Xi). 
(271.7) 
A 
i=l 
All integrals which are encountered when computing estimates and confidence regions 
or when testing hypotheses can be solved by (271.5) or (271.7). This will be demon- 
strated in the next sections. 

54 
272 Computation of Estimates, Confidence Regions and 
Posterior Probabilities of Hypotheses 
A 
The Bayes estimate 013 of the parameter vector 0 with 0=(01 ) and 1E { 1 ..... u} is com- 
puted by the integral (231.6), whose domain is the parameter space o. Thus, the intervals 
[al,bl] with 1~{1 ..... u} of (271.4) on the coordinate axes for 0 have to be chosen 
such that outside the region defined by the intervals the relation 
01P(01y) < e 
(272.1) 
is fulfilled, where e denotes a small number. It is determined by the product of the pa- 
rameter value and the density value which ceases to contribute to the integral. 
Bayes' theorem (211.1) suggests working with posterior distributions which are not nor- 
malized and which shall be denoted by t3(01 y). The normalization constant follows from 
^ 
(211.5). The Bayes estimate 0 B of the parameter vector 0 is therefore computed instead 
of (231.6) by 
A 
0 B = S O p(Oly)dO/I ~(0ly)d0. 
(272.2) 
e 
o 
If we apply the Monte Carlo integration (271.5), we find 
~B 
u 
1 ! 
u 
1 m 
= [ 17 (bl-al) ] ~ 
0it3(0ily)/[ II (bl-al) ] ~ 
Z p(0i[y) 
1=1 
i 1 
1=1 
i=l 
or 
A 
m 
m 
= 
X Oil(OilY)/ Z ~(Oily), 
(272.3) 
i=l 
i=l 
where 0 i denotes the random vector generated with a uniform distribution in the param- 
eter space o bounded by the intervals resulting from (272.1). 
m 
The sum 
Y. 13(0ilY ) in (272.3) represents the normalization factor. The normalized 
i=1 
posterior density value p(0 i [Y) of the generated random vector 0 i therefore follows 
with 
m 
P(0 ily) = P(0 ily)/ X 13(0 ily). 
(272.4) 
i=1 
The values p(0 i lY) can be interpreted as the values of a density function of a discrete 
random vector 0 with the values 0 i for is { t ..... m} originating from the random num- 
ber generator. The density function p(~ I Y) fulfills the conditions for the density function 
of a discrete random vector (Koch 1988a, p.98) 

55 
m 
P(0 ily) > 0 
and 
Z p(0 ily) = 1. 
(272.5) 
i=l 
By means of this density function we may compute the confidence region for the param- 
eter vector 0 defined by (241.2). Let the density values p(0 i I Y) be ordered according to 
increasing values, so that the sequence p (0j [y) with j e { 1 ..... rn} is obtained. Then the 
value b in (242.1) is determined by 
b = p(0oly), 
(272.6) 
where p(0olY) follows from 
o 
Z p(0j lY) = a 
(272.7) 
j=l 
with a giving the content 1- a of the confidence region. The equation (272.7) can only be 
approximately fulfiUed. But the more vectors 0j are generated, the smaller will be the 
increase from p(0j ly) to p(0j +lly ) and the better the approximation. Already a linear 
interpolation between the density values p(0j lY) and p(Oj+ 1 lY) will improve the ap- 
proximation. When adding additional density values by interpolation, a new normaliza- 
tion factor has to be computed. 
The boundary of the confidence region is determined according to (242.1) by the vectors 
0 b for which 
b = p(Ob[y) 
is fulfilled. For a determination of the vectors O b at the boundary, we visualize the gen- 
erated vectors 0 i as points of the parameter space o and select neighboring points 0 i and 
O. with 
J 
p(0ity) <_ b 
and p(0jly ) ___ b. 
(272.8) 
By interpolation between these points the points 0 b at the boundary are obtained. 
For computing posterior probabilities associated with hypotheses the integrals (253.2) 
have to be solved. Again we assume that we work with a posterior density function 
t3 (01 y) which is not normalized. Thus, we obtain with (211.5) instead of (253.2) 
P(HklY) = ~ 13(OIy)dO/f 13(Oly)dO for 
ke{O,1}. 
(272.9) 
o k 
e 
We generate random vectors 0 i for iÂ¢ { 1 ..... m} with uniform distributions in the pa- 
rameter space e bounded by the intervals [a I ,b 1 ] on the coordinate axes for O. Corre- 
spondingly to (272.1) the intervals are established such that outside the region, defined 
by the intervals, the density values cease to contribute to the integrals. In addition we 
generate random vectors Ojk for je { 1 ..... n} and ke {0, 1 } with uniform distributions 

56 
in the subspace o k for ke{0,1}. Let these subspaces be defined by the intervals 
[alk,blk] with le{1 ..... u}. Hence, the Monte Carlo integration of (272.9) follows 
with (27t.5) by 
u 
1 
n 
P(HklY) 
= [ H (blk-alk)] K __Z 13(Ojk{Y)/ 
1=1 
j 1 
u 
1 
m 
[ rI (bl-al) ] ~ __Z 13(0i[y ) 
for 
1=1 
i 1 
The posterior odds (253.6) are thus computed by 
u 
n 
P(Ho{Y)/P(til[Y ) = [ 1t (blo-alo)] 
2 
1=1 
j=l 
u 
n 
[II (bll-all)] 
Y, 
1=1 
j=l 
ke{O,1}. 
(272.10) 
 (Ojoly)/ 
P(OjlIY). 
(272.11) 
If the subspaces o o and o I cannot be defined by intervals, but have volumes V Â° and V 1, 
respectively, we obtain with (271.7) instead of (272.11) 
n 
n 
P(ttolY)/P(lt I {Y) = VojY.l~(OjolY)/Vlj= 
f)(Oj 1 {y). 
(272.12) 
Example 1: Examples for the computation of the Bayes estimate according to (272.3) 
and the confidence limits according to (272.6) are given in the Examples 1 of Section 
322 and 342. 
A 
273 Marginal Distributions and Transformation of Variables 
Frequently the distribution of a subset of the set of parameters 0 is needed. It is obtained 
as a marginal distribution (Koch 1988a, p.105) of the posterior density function p(0{ y) 
resulting from Bayes' theorem (211.1). With 0={ 01,011', where 01 denotes the parame- 
ter vector whose marginal distribution is needed, we obtain the marginal posterior densi- 
ty function p(01 [y) of 01 by 
p(01]y) = S p(01,02{y)d02. 
(273.1) 
o 2 
The parameter space for 02 is denoted by o 2 with o2c o. The marginal distribution 
p(01 {y) shall be determined by a Monte Carlo integration and it shall be used again in 
an ensuing Monte Carlo integration for computing, for instance, the confidence region 
for the parameter vector 01. 

57 
Again we will work with a posterior density function p(01 , 02lY) which is not nor- 
realized. Thus, we obtain with (271.5) instead of (273.1) 
O 
]~(Oli IY) 
j__Zlt3(01i ,02j [y), 
(273.2) 
where the vectors 02j for je{ 1 ..... o} are generated with uniform distributions in the 
space e 2 for 02. Since non-normalized density functions are used, the constants in 
(271.5) may be omitted. The vectors 01 i result from the generation of random vectors 
with uniform distributions for the Monte Carlo integration in connection with the mar- 
^ 
ginal posterior density t3 (011y), for instance when computing the estimate 01B of 01 ac- 
cording to (272.3), when establishing the confidence region for 01 according to (272.6) 
or when testing hypotheses for 01 according to (272.11). 
Occasionally we have to go one step further, if the parameter vector 01 in (273.1) needs 
to be transformed and the distribution of a subset of the transformed parameters has to 
be computed. 
Let the parameter vector 01, which shall be of dimension qxl, be transformed to the qxl 
parameter vector 71. Let the inverse transformation with Ol=(01i ) and 71=(~'1i ) be 
given by 
01 = g(71) 
or 
01i = gi(~l 1 ..... 3,1q ) 
for 
i~{1 ..... q}. 
(273.3) 
If p(711Y) denotes the posterior density of the transformed parameters 7, we obtain 
(Koch 1988a, p.108) 
P(71 [Y) = P(g(71) lY)IdetJI, 
(273.4) 
where I de t J I denotes the absolute value of the Jacobian de t J with 
J= (0gi/071j) 
for 
i,jE{1 ..... q}. 
(273.5) 
The vector 71 is again partitioned into 7t = 1 ~11' ~12 t '- Only the marginal posterior den- 
sity function p ( 7111 Y) of the parameter vector 711 is needed. Hence, 
P(7111Y) = I p(711,7121Y)dT12, 
(273.6) 
['12 
where F12 denotes the parameter space for gl 2. By substituting (273.4) we find 
p(7111Y) = I p[g(711,712)[y] IdetJIdT12 
F12 
and by substituting (273.1) 
P(711lY) = I 
I p[g(711,712),021Y]IdetJfd02 dT12. 
(273.7) 
F12 e 2 

58 
The marginal posterior density p(711 IY) shall be computed by a Monte Carlo integra- 
tion, in order to be used again in a further Monte Carlo integration. Correspondingly to 
the step from (273.1) to (273.2) we obtain 
O 
r 
= Z 
2 
~[g(711i,712j),02klY]ldetJI. 
(273.8) 
P(711 i lY) 
j=l k=l 
The vector 711 i stems from the Monte Carlo integration in connection with the non- 
normalized density P(71 l ly), the vector T12j from the generation of the vectors with 
uniform distributions in the space F12 for 712 and the vector 02k from the generation 
with uniform distributions in o 2 for 02 . 
A 
With the density values P(711i [y) from (273.8) we may compute the estimate 711B of 
711 by (272.3), establish the confidence region for 711 by (272.6) or test hypotheses for 
711 by (272.11). 
Example 1: Examples for computing the marginal density function from (273.2) are 
given in the Examples 1 of Section 322 and 342. An example for applying (273.8) can 
be found in Koch (1988c). 
A 
274 Approximate Computation of Marginal Distributions 
If the vector 02 in (273.1) or the vectors 712 and 02 in (273.7) contain a large number of 
unknown parameters, the Monte Carlo integration (273.2) or (273.8) is very time-con- 
suming even with fast electronic computers. An approximate computation of the margin- 
al distribution instead of a numerical integration shall therefore be developed. The meth- 
od will be demonstrated for the definition (273.1) of the marginal distribution of 01, but 
it may, of course, be applied to (273.7). 
We have 
P(01 tY) = f P(01,02lY)d02 = p(01,02ctY), 
(274.1) 
Â° 2 
if 02c denotes an appropriately chosen constant. We will check now, whether 02c may 
A 
be replaced by an estimate 02 of 02 , thus, 
A 
02c = 02. 
(274.2) 
The posterior density function of the parameter vectors 01 and 02 is expressed with 
(261.1) by 
P(01,021Y ) = p(01102,y)p(02lY), 
(274.3) 

59 
where P(01102, y) denotes the conditional posterior density function of O 1 given 02 and 
y and p(02 l Y) the marginal posterior density function of 02. Corresponding to (273.1) 
the marginal posterior distribution p (OliY) of 01 is obtained from (274.3) by 
p(01 my) = f p(01[02,y)p(02ly)d02 = p(01102k,Y), 
(274.4) 
02 
where 02k denotes an appropriately chosen constant. 
The marginal posterior distribution P(02lY) of 02 contains the information on 02 re- 
suiting from the observations y and the prior information. If the density function 
p(O21y) has a pointed shape, its probability mass is concentrated over a small region in 
the vicinity of its modal value 02, i.e. the value for which p(02 lY) attains a maximum. 
Then p(02 [y) contains so much information on 02 that values for 02, which are not 
close to 02, may be excluded. Hence, the integration with respect to 02 is approximately 
equivalent to choosing for the constant 02k the medal value 02 
02k = 02. 
(274.5) 
According to (232.4) O 2 is the generalized maximum likelihood estimate of 02. 
On the other hand, if P(021Y) has a fiat shape, there is not much information on 02 
available. In order to justify under these circumstances a choice according (274.5), addi- 
tional information on 02 by the data y or by prior information has to be collected. 
With (274.5) we obtain instead of (274.4) 
P(OI !Y) = P(O 1102,Y) 
and from (274.3) 
P(011/)2,Y) ~ p(01,/)21y). 
The marginal posterior density function p (01 l y) then follows with 
P(O 1 lY) ~ P(O 1,021Y). 
(274.6) 
Thus, p(01 I Y) can be approximately determined by the maximum likelihood estimate 
02 of 02, if the density function p(02 {y) has a pointed shape. In such a case the modal 
value lies in the vicinity of the expected value, if the two values do not coincide. The 
maximum likelihood estimate 02 of 02 in (274.6) may therefore be replaced by the 
A 
Bayes estimate 02B of 02 from (231.5) so that we obtain 
A 
P(01 [Y) ~ P(01,02B lY). 
(274.7) 

60 
The density function P(02[Y ) has a pointed shape, if the observations y contain accurate 
information on the parameter vector 02 or if the prior information on 02 is accurate. 
Example 1: Examples for an approximate computation of a marginal posterior density 
function from (274.6) is given in Example 1 of Section 322 and in Example 1 of Sec- 
tion 365. Additional examples can be found in Koch (1989). Applications of (274.7) are 
contained in Section 373. 
A 

3 Models and Special Applications 
When observations are taken which contain information on unknown parameters, the re- 
lation between the unknown parameters and the observations has to be defined. In addi- 
tion, the statistical properties of the observations need to be defined. These definitions 
establish the model. It serves to analyze the data for the statistical inference on the pa- 
rameters. 
In general the expected values of the observations are defined as given functions of the 
unknown parameters, and the covariance matrix of the observations is assumed as known 
except for an unknown factor. Due to the central limit theorem (Cramer t946, 
p.214,316), we may assume the observations as being normally distributed. Since the 
density function of a normally distributed random vector is according to (A21.3) 
uniquely specified by its expected value and its covariance matrix, the assumptions for 
the model determine the likelihood function. Thus, with the likelihood function deter- 
mined by the model and the prior distribution chosen according to the available informa- 
tion, Bayes' theorem (211.1) readily leads to the posterior density function of the un- 
known parameters. 
In addition to the statistical inference on unknown parameters, special applications are 
presented in this chapter such as the inference for unknown parameters based on special 
likelihood functions, which originate from robust estimation. The problem of the classifi- 
cation and the reconstruction of digital images is also discussed. 

62 
31 Linear Models 
311 Definition and Likelihood Function 
The most important model of the standard statistical techniques is the Gauss-Markoff 
model or the linear model (Koch 1988a, p.182). An estimation of the parameters of this 
model is also called a regression analysis. To obtain for the Bayesian analysis a model 
which corresponds to the Gauss-Markoff model, one has to be aware that the parameters 
of the Gauss-Markoff model are fixed quantities, while in the Bayesian analysis the pa- 
rameters are random variables. Hence, the expected values of the observations and the 
covariance matrix have to be defined under the condition that the unknown parameters 
take oll fixed values. 
Definition: Let X be an nxu matrix of given coefficients with full column rank, i.e. 
rankX=u,/3 a uxl vector of unknown random parameters, y an nxl random vector of ob- 
servations and (r2 an unknown random parameter, which is called variance factor or var- 
iance of unit weight. Let E(yltl) and D(YlCr 2) be the expected value and the covariance 
matrix of the observation vector y under the condition that the unknown parameters i~ 
and ~2 are given, then 
E(y[O) = Xgl with 
is called a linear model. 
D(yl~r2) = ~2I 
(311.1) 
An equivalent formulation of this model is given by the so-called observation equations 
y+e = X~ with 
E(e[~l) = 0 
and 
D(e[o2) = D(y[cr2), 
(311.2) 
where e denotes the nxl random vector of the errors of the observations. 
The model (31 t. 1) or (311.2) is of sufficient generality. The more general model 
E(~I~ ) 
= X~ or 
.y+ e= F,~ with 
D(~lcr2) = ff2P -1, E(~I~I ) = O, 
D(eio~2 ) = D(y[62), 
(311.3) 
where the nxn positive definite matrix P is called the weight matrix, can be transformed 
to (311.1) or (311.2) with 
P= fiG', X= G'X, y= 6"y, e = 6% 
(311.4) 
(Koch 1988a, p.183). The matrix G results from the Cholesky factorization of P (Koch 
1988a, p.36) and denotes a regular lower triangular matrix. 

63 
As mentioned in Chapter 3, we will assume the normal distribution for the observation 
vector y under the condition that ~1 and a2 are given 
yl/i, a2 - N(X~,cr2I). 
(311.5) 
The likelihood function, i.e. the density function of the observations y given the parame- 
ters ~l and crz, then follows with (A21.1) by 
p(y]~l, cr2) = (2g)-n/Z(detcr2I)-l/2exp[- ~-~ (y-X~)'(y-X~)] 
or 
P(YIO ,a2) = (2z~z)-n/2exp[" ~-7 (y-Xfl)'(y-X~)]. 
Ifthe weight or precision parameter vis used in (311.1) with 
we obtain instead of (311.5) the distribution 
ytj~,-.r- N(X~,v-lI). 
The likelihood function therefore follows with 
p(y[/l,'c) = (2g)-n/2~/2exp[- ~(y-X~)'(y-X~l)] 
and by means of the sufficient statistics (224.4) because of (224.7) with 
p(yl~l,v) = (2z)-n/2vn/2exp{- ~[(n-u)~2 + (O-~)'X'X(/}-~)]}. 
(311.6) 
(311.7) 
(311.8) 
(311.9) 
E(yl~) = X/I with 
D(yfa2 ) = ffeI 
(311.12) 
with 
X= [1,1 ..... 1]" 
and 
~= #. 
Let in addition the observations Yi be normally distributed, then with (311.5) 
or in the representation (311.1) 
yle  
ill 
!0 011 0 
Y2 + e2 
g 
and 
D( Y2 
Io'2) = or2 
(311.11) 
.
.
.
.
.
.
.
.
.
.
.
.
 
Loo:::;j 
Yn + en = # 
Example 1: Let the length # of a straight line be measured n times and let the n observa- 
tions Yi be collected in the nxt random vector y with Y=(Yi )" Let the observations Yi 
be independent, have the expectation E(y i t#)=# and variance V(y i la2)=az. Let # and 
~2 be the unknown parameters and e i the errors. We therefore have the linear model 
(311.10) 

64 
yt#,cvz - N(x0,cy2i). 
(311.13) 
From (311.11) and (311.12) we have 
n 
(y-XI~)'(y-X/i) = 
Z (Yi-#)2' 
i=l 
so that the likelihood function follows with (311.6) by 
n 
p(yl#,a 2) = (2gcr2)'n/2exp[- 
1 
(Yi _#)2]. 
(311.14) 
The same likelihood function was already derived with (224.21) tbr the Example 1 of 
Section 224. 
h 
312 Noninformative 
Priors 
We will assume a noninformative prior density function for the unknown parameters/3 
and o2 of the linear model (311.1). We apply Jeffrey's invariance principle to derive the 
priors and consider ~ and o2 as being independent. Hence, we obtain with (222.11) and 
(311.7) after omitting the constants in (311.9) 
lnp(ytO,'c ) ~ ~ ln'c - ~(y-X~)'(y-X~)/2. 
(312.t) 
The derivative with respect to l[! follows with 
01np(yl/l,v)/0O ~, - v(-2X'y+2X'lql)/2. 
The second derivative with respect to tl gives constants. The noninformative prior densi- 
ty function p(~) for the parameter vector ~1 therefore follows with (222.10) by 
p(O) ~ const. 
(312.2) 
The derivative of (312.1) with respect to "r gives 
01np(y[~,'c)/0"c ~ n 
Â½ 
- 
(y-X~) '(y-X~) 
and the second derivative 
n 
021np(yl~l,~:)/0v2 ~ - ~-~-7 â¢ 
(312.3) 
Thus, we find with (222.10) the noninformative prior density function p(v) for the pa- 
rameter "c by 
p(~) ~ 1/~. 
(312.4) 
Applying Bayes" theorem (211.1) the posterior density function of the parameters l[l and 
then follows with (311.10), (312.2) and (312.4) after omitting the constants by 

65 
P(~,~rtY) 'Â¢ vn/2-1 exp{- " ^~[(n-u)a 2 + (~i-~)'X'X(~-~)]}. 
(312.5) 
By comparing this density function with (A23.1) it becomes evident because of 
n/2-1=u/2+(n-u)/2-1 that (312.5) is the density of the normal-gamma distribution. 
Thus, 
~l,'cly - NG(~, (X'X) -1 , (n-u)~r2/2, (n-u)/2). 
(312.6) 
The marginal posterior distribution of 0 is according to (A23.3) the multivariate t-distri- 
bution 
^ 
0[Y- t(~,cr2(X'X) 1,n-u) 
(312.7) 
with the expected value from (224.4) and (A22.7) 
E(~) = ~ = (X'X)-Ix'y 
(312.8) 
and the covariance matrix 
^ 
n-u 
cr2(X'X) 1 
(312.9) 
O(/~) = 
with 
~r2 = 
1 
(y-X~)'(y-X~). 
(312.10) 
n-u 
Applying (231.5) and (231.7) the Bayes estimate ~ of fl is obtained by 
-- 
~ = ~= (X'x)-lx'y 
(312.11) 
and the covariance matrix Z~ of the Bayes estimate ~B by 
^ 
Z~=~n-u 
(r2(X,X) 1. 
(312.12) 
The estimate (312.11) is in addition the best linear unbiased estimate of the parameter 
vector/~ of the Ganss-Markoff model by the standard statistical techniques (Koch 1988a, 
p. 187). It is identical with the estimate of the method of least squares and for normally 
distributed observations with the maximum likelihood estimate (Koch 1988a, p.191). 
Hence, in the case of the noninformative priors (312.2) and (312.4), the Bayes estimate 
(312.11) of the unknown parameters jil is identical with the estimates of the standard sta- 
tistical techniques. The covariance matrix Z~ of the estimate differs by the factor 
(n-u)/(n-u-2) (Koch 1988a, p.193). 
Because of (312.6) the marginal posterior distribution of the weight parameter "r is ac- 
cording to (A23,4) the gamma distribution 
A 
vlY - G((n-u)cr2/2, (n-u)/2). 
(312.13) 
The distribution of the variance a 2 of unit weight is therefore according to (A13.1) the 
inverted gamma distribution. Its expected value and variance (A13.2) give because of 

66 
A 
A 
(231.5) and (231.7) the Bayes estimate 6~ of a 2 and its variance V(a~). Thus, 
11-1.1 
=-~d2 
with 
^ 
~l~ 
2(n-u)2 (0"2) 2 
V( 
) = (n-u-2)2(n-u-4) " 
(312.14) 
(312.t5) 
For the sake of comparison the best unbiased estimate of a 2 resulting from the standard 
A 
A 
techniques is a s and its variance 2(62)2/(n-u) (Koch 1988a, p.277). 
Example 1: The Bayes estimates just derived shall be applied to the special linear model 
^ 
(311.11) of the Example 1 of Section 311. We obtain the Bayes estimate #B of the un- 
A 
known parameter # from (312.11) and its variance V(/IB) from (312.12). Thus, 
^ 
1 n 
#B = ~ i I=E Yi' 
(312.16) 
which is the well-known mean. As already mentioned, this estimate is identical with the 
^ 
estimate of the standard techniques. Furthermore with a 2 from (312.10) 
1 
n 
^ 
= 
(Yi-#B )2 
(312.17) 
we find 
A 
^ 
(n-l) 62 
(312.18) 
V(#B) = (n-3) n 
^ 
As mentioned, the result of the standard techniques is 62/n. 
A 
A 
The Bayes estimate 6~ of the variance as of unit weight and the variance V(a~) of the 
estimate follow from (312.14) and (312.15) with 
^a~ = n___~n-1 ~ 
(312.19) 
and 
^ 
~i~ 
2(n- 1)2(62)2 
V( 
) = (n_3)2(n_5) 
(312.20) 
A 
As already explained, the estimate of a 2 by the standard statistical techniques is 62 and 
A 
its variance 2(62)21 (n- 1 ). 
A 
Since the parameter vector ~l has the multivariate t-distribution (312.7), a confidence re- 
gion for/l is easily established, According to (A22.1) the posterior density function for ~l 
monotonically decreases, if the quadratic form (O-~)'X'X(]I-~)/~2 increases, which 
divided by u has the F-distribution F(u,n-u) with u and n-u degrees of freedom because 

67 
of (A22.13) 
(fl-~J)'X'X(fl-~J) / (u~r2) - F(u,n-u). 
(312.21) 
With the same reasoning, which leads to (242.6), the confidence region of content 1- a is 
therefore determined by 
(fl-~) 'X'X(fl-~) /(u~ 2) = F 1 -a;u,n-u' 
(312.22) 
where F 1 _ a; u,n-u denotes the upper a-percentage point of the F-distribution with u and 
n-u degrees of freedom. The confidence region is a hyperellipsoid with the center at ~. It 
J 
is identical with the confidence hyperellipsoid of the standard statistical techniques 
(Koch 1988a, p.328). 
If a confidence region needs to be established for a subset flj.. k of the parameters t, we 
define 
fl = (fli), flj..k = (ill) 
and ~= (~i), ~j..k = (~1) 
for 
le{j,j+l ..... k}, 
(312.23) 
(aij) = (X'X) -1 
and Y'j..k = (aim) 
for 
1,m~{j,j+l ..... k} 
and obtain with (A22.10) the marginal multivariate t-distribution 
A'2 
flj..kly - t(~j..k,fr Y~j..k,n-u) 
(312.24) 
This distribution leads to the confidence region of content 1- a 
(flj.. k-~j . .k)'~- 1 
.k(fljj. . .k-~j . .k)/((k-j+l)~r2) = Fl_a;k.j+l ,n_ u, (312.25) 
which again is identical with the confidence hyperellipsoid of the standard statistical 
techniques (Koch 1988a, p.328). 
We want to test the point null hypothesis (251.4) generalized by (251.5) 
H o : tl~= w versus 
H 1 : lt~e w, 
(312.26) 
where the rxu matrix H has full row rank and is given. The rxl vector w is also given. 
With (312.7) we obtain from (A22.12) the distribution 
1~1 
t(~, ^ 
-i1t' 
- 
a21t(X'X) 
,n-u) 
(312.27) 
and from (A22.13) 
(I~It~'(H(X'X)-IlI')-I(II~-I~)/(r~ 2) ~ F(r,n-u). 
(312.28) 
We will decide by (252.4), that is by means of the confidence region for Hfl, whether to 
accept or to reject the null hypothesis H Â° in (312.26). The confidence region for I~ fol- 
lows from (312.28) by 

68 
(Itfl-R~) ' (lt(X'X)" 1It')" 1 (Rfl-R~)/(r~ 2) = F l_a; r ,n-u" 
(312.29) 
According to (A22.1) the posterior density for Hfl increases, when the quadratic form on 
the left-hand side of (312.29) decreases. Thus, because of (252.4) and Hj~=w the null hy- 
pothesis H o in (312.26) is rejected, if 
-1 
^ 
(~w)'(H(X'X)'IH ") 
(lt~-w)/(r(r2) > Fl_a;r,n_ u. 
(312.30) 
An equivalent procedure is applied, when the hypothesis (312.26) is tested by the stand- 
ard statistical test. The quantity on the left-hand side of (312.30) is the test statistic for 
this test (Koch 1988a, p.308). 
Thus, Bayesian inference for the parameter vector ~ of a linear model gives, in the case 
of noninformative priors, results which are equivalent to the results of the standard statis- 
tical techniques. 
Example 2: A confidence interval of content 1-a shall be determined for the unknown 
parameter # of the Example 1 of Section 311, which is continued by Example 1 of this 
section. We obtain from (312.21) 
A 
A 
n(#-#)2/a2 - F(1,n-1) 
(312.31) 
A 
with g from (312.8) 
^ 
1 n 
# = -n i__ZlYi" 
From the definition of the upper a-percentage point of the F-distribution the probability 
follows 
^ 
A 
P(n(g-~)2/cr 2 < Fl_a;1,n_l ) = 1-a. 
(312.32) 
By a transformation of the variable of the integral from which this probability is com- 
puted we find 
A ^ 
1/2) 
P(+q~(#-#)/(r < (Fl_a;1,n_l) 
--- 1-a. 
(312.33) 
If a random variable x2 is distributed according to 
x2- F(1,n-1), then x- 
t(n-1) 
(Koch 1988a, p.155), where t(n-1) is the t-distribution with n-1 degrees of freedom 
having the density function (A22.6). With the quantity t a;n-1 of the t-distribution, 
which can be taken from a table of the t-distribution and which is equal to the square 
root of the upper a-percentage point of the F-distribution 
1/2 
ta;n_ 1 = (Fl_a;1,n.1) 
(312.34) 
we obtain instead of (312.33) 

69 
P(-ta;n_ 1 < q~(#-~)/~r < ta;n_l) = 1-a 
and finally the confidence interval of content 1- a for the parameter # 
A 
A 
A 
A 
P(#-(cr/q~)ta;n_ 1 < # < # + (cr/q~)t~;n_l) = 1-a. 
(312.35) 
A 
A 
A 
Since o'/Â¢fi is the standard deviation of # computed by the estimated variance a z of unit 
weight, the confidence interval (312.35) is identical with the confidence interval for # of 
the standard statistical techniques. 
In addition we want to test the hypothesis 
Ho : #= #o versus 
H 1 : /.t Â¢/~o, 
(312.36) 
where #o is a given constant. We will use (252.4) to decide whether to accept the hy- 
pothesis. Thus, if #o lies outside the confidence interval (312.35), B o is rejected. An 
equivalent condition for #o lying outside the confidence interval is obtained from 
(312.30). With It=l, j~# from (311.12) and w=-#o, the hypothesis (312.36) is represented 
in the general form (312.26). Thus, we obtain instead of (312.30) 
A 
(#'#O)2 
A 
> 
(312.37) 
Â¢r2/n 
Fl-a;l'n-l" 
If this inequality is fulfilled, H Â° is rejected. The same procedure is applied in the stand- 
ard statistical techniques to decide whether to accept or to reject the null hypothesis H o 
in (312.36). 
A 
313 Informative Priors 
We will assume that prior information is given for the parameters ]! and cr2 in the linear 
model (311.1) by means of the expected value/lp and the covariance matrix Y..fl of the 
parameter vector j~ and by the expected value (r~ and the variance Vcr 2 of the parameter 
or2. We introduce the normal-gamma distribution as prior for fl and z=l/cr2 
~i,z ~ NG(/t,Â¥,b,p). 
(313.1) 
The parameters of this distribution are determined with (224.20) by 
. = .p, 
p: 
+ 2, h= 
(313.2) 
The normal-gamma distribution (313.1) is a natural conjugate prior, so that together with 
the likelihood function (311.9) Bayes' theorem (211.1) gives the posterior distribution 
for ~, v which is also normal-gamma 
~l, "rty - NG(/Lo,Vo,bo,Po ) . 
(313.3) 

70 
The parameters of this distribution are given in (224.10). 
The marginal posterior distribution of the parameter vector ~ is therefore the multi- 
variate t-distribution 
~IY- t(/to,boVo/Po,2Po ) 
(313.4) 
according to (A23.3). The expected value and the covariance matrix of ~1 from (A22.7) 
give according to (231.5) and (231.7) the Bayes estimate ~ of ~ and the covariance ma- 
trix Z~ of the estimate ~13" Thus, with (224.10) 
I
T
 
~B = /to = (X'X+V-1)-I(x'Y+Â¥-I#) 
(313.5) 
and 
1 
,V-1 
Z~- 
n+2p-2 (2b+(/t-/to) 
(/t-/to)+(y-X/to)'(y-X/to))(X'X+7-1)-l. 
(313.6) 
The Bayes estimate ~B and its covariance matrix Z~ shall be also given in the more gen- 
eral model (311.3). By substituting (311.4) we find 
~13 = /to = (X'PX+g-1)-I(x'I~+Â¥-I/t) 
(313.7) 
and 
1 
,V-1 
Z~- n+2p-2 (2b+(/t-/to) (/t-/to) 
+ (Y-Y~V'o)'P(Y-X/to))(X'~+V-I)-I 
(313.8) 
For easier comparison with the estimates of the mixed model to be presented in Sec- 
tion 333, and for obtaining in special cases computationally more efficient formulas, the 
Bayes estimate ~B will be presented in the following form. We apply the two matrix 
identities (Koch 1988a, p.40) 
(X'pX+v-1)-I~'p = V~'(X'V~'+F-1) -1 
(X'PX+V-1) -1 = V - VX'(XWX'+p-1)-Ix'v 
(313.9) 
and obtain 
~B = /to = /t + VX'(X'VX'+p-1)-I()'-X/t)" 
(313.10) 
In general n>u holds for the linear model (311.1). However, when using prior informa- 
tion we may have n<u. In such a case, (313.10) is computationally more efficient than 
(313.7), since the inverse in (313.10) is of dimension nxn. If the vector/t and the matrix 
Â¥ stemming from the prior information are considered as results of a preceding parame- 
ter estimation, (313.10) represents a recursive estimation (Koch 1988a, p.208), which 
leads for a dynamic system to the Kalman-Bucy filter derived in Section 318. 

71 
It is worth mentioning that the Bayes estimate (313.10) is closely related to the estimate 
of the robust collocation in the presence of weak prior information (Schaffrin 1989) 
A 
1)- 
- - ^  
~1 = # co + V~'(X-~'+P" 
l(y-X~) 
A 
_ 
_ 
A 
= /* co + Â¥(I+X'P-XV)'Ix'p(y-X~) 
(313.11) 
with 
^ 
/t'X" (X'VX' +P- I)- 17 
/t' (I+X'P-XV) - tX'I~ 
co= 
/.t'X' (X'VX'+P- 1)- lX# = /1' (I+X'P-XV)- lX'PY4~ ' 
where ~ denotes the estimate of ~ and where the remaining quantities have the same 
meaning as in (313.10). The prior information # in (313.11) is considered as being weak, 
^ 
since it needs to be scaled by a factor co, which is estimated by co. The appropriate model 
A 
for deriving ~l and co in (313.11) is a random effects model of the standard statistical 
A 
techniques, but ~ and co may also be obtained by the following Gauss-Markoff model of 
the standard statistical techniques 
[:l + [Sj = [: 
_:][:J with 
D([:])= cr2[S "1 
i" 
(313.12) 
This can be shown by solving the matrix of normal equations resulting from (313.12) for 
A 
A 
co by means of the reverse of a block matrix (Koch 1988a, p.39). By substituting co the 
normal equations are then solved for ~l. For obtaining (313.11) the two matrix identities 
(313.9) are applied. 
To compare the result (313.5) with the result of the standard statistical techniques, we 
assume that in addition to the observation equations y+e,=X~ of the Gauss-Markoff mod- 
el, the unknown parameters ~ have been independently observed by the vector/t with the 
error vector v and the weight matrix V- 1. Thus, 
~] + [;1 =[II ~ with 
D(~]) = (#[ 0 ~l' 
(313.13) 
which gives with (224.4) the estimate 
= /*o = (X'X+V-1)-I(x'y+V-I#)" 
(313.14) 
It is identical with (313.5). If the prior information is interpreted as data, the Bayes esti- 
mate ~B of the parameter vector ill and the estimate of the standard statistical techniques 
are identical. The same, of course, holds true for (313.7). 

72 
With (313.3) the marginal posterior distribution of the weight parameter v is according 
to (A23.4) the gamma distribution 
'rty - G(bo,Po ). 
(313.15) 
The variance o'2 of unit weight therefore has the inverted gamma distribution (A13.1) 
A 
with the expected value and variance from (A13.2), leading to the Bayes estimate (r~ of 
and to the variance V(cr~) of 
. Thus, with (224.10) 
A 
1 
,Â¥- 
Cr~ -- n+2p-2 (2b+(/t-/to) 
t(/t-#o)+(Y-X#o)'(Y-X/to)) 
(313.16) 
and 
A 
^ 
2(@2 
V((r~) - 
(313.17) 
n+2p-4 
A 
A 
To compare the Bayes estimate a~ of a2 with the estimate cr2 of the standard statistical 
A 
techniques, we obtain with (224.4), where in the expression for cr2 the weight matrix 
from (313.13) has to be introduced (Koch 1988a, p.192), 
~r2 = K [(Y-X/to)" (/t-No) '] 
1 
V- 
/to 
1 ((/t_),y-l(g_ 
= ~ 
#o 
/to)+(Y-X/to )' (Y'X~o))" 
(313 18) 
By substituting b and p from (313.2) in (313.16), we recognize that the prior information 
on a2 enters the estimate in comparison to (313.18). In addition the number of degrees of 
freedom differs. 
By means of the substitution 
Â¥-1 -) 0, 
b -) 0, 
p -) -u/2 
(313.19) 
the parameters of the normal-gamma distribution (313.3) for 13 and 1: result with (224.4) 
and (224.10) by 
/to = ~ = (X'X)- 1X'y 
v 
= (x'x) -1 
O 
A 
b o = (n-u)a2/2 
Po = (n-u)/2. 
The distribution for 1} and v follows with 
~6, vly - NG(~, (X'X) -1 , (n-u)~2/2, (n-u)/2), 

73 
which is the normal-gamma distribution (312.6) of 1~ and 'r in the case of noninformative 
priors. Hence, (313.19) transforms the results with informative priors to the results with 
noninformative priors. One has to be aware, however, that substituting (313.19) in the 
informative prior density (313.1) for ~i and â¢ does not lead to the noninformative priors 
(312.2) and (312.4) for ~ and ~:. 
Example 1: Again the unknown parameters # and 62 of the linear model (311.11) of Ex- 
ample 1 of Section 311 shall be estimated. But instead of introducing noninformative 
priors as in Example 1 of Section 312, an informative prior is assumed for/1 and 62. It is 
given with v=- 1 / 62 by the normal- gamma distribution (313.1) 
#,~- 
NG@,V,b,p). 
(313.20) 
The parameters of this distribution are determined by the prior information on the ex- 
pected values and the variances of/~ and 62 
-- 
v(.) 
= 
E(62) = 6~, V(62) = V62, 
which gives with (313.2) 
= #p, V = 62/62#_ 
P' p = (~)2/V62 + 2, b = (p-1)6~. 
(313.21) 
The likelihood function of this example is given by (311.14). It is identical with the like- 
lihood function (224.21) of the Example 1 of Section 224. In addition the prior distribu- 
tion (313.20) for # and 62 with the parameters from (313.21) is identical with the prior 
distribution (224.22) with the parameters (224.24). Hence, the posterior distribution for # 
and 62 of this example is given by the posterior distribution (224.25). The Bayes esti- 
mates for # and 62 obtained from (313.5) and (313.16) and their variances from (313.6) 
and (313.17) therefore follow with (224.27) to (224.30) and with (224.32) to (224.35). A 
314 Prediction of Data 
If we want to predict m observations of a linear model, we may either start from 
the formulation (311.1) E(y[ll)=X/I of the linear model or from the equivalent for- 
mulation (311.2) y=X/l-e. In the first case we predict with 
yp = E(y*[ig) = X*/~ 
(314.1) 
the mxl vector yp of observations as the expected value of the mxl vector y* given/1. 
The matrix X* denotes the mxu matrix, which has to be known for the prediction. 

74 
The distribution for the unobserved data vector yp can be readily given. If we assume as 
the posterior distribution for the parameter vector fl the multivariate t-distribution 
(313.4), which results from the informative prior (313.1) for the parameters/1 and 7, the 
distribution of the linear function yp=X*~] of ~l follows with (A22.12) by 
Yp lY - t (X*/~o,boX*Vo x*'/Po,2Po )" 
(314.2) 
All inferential tasks for the predicted observation vector yp can be solved by this distri- 
A 
bution. For instance the Bayes estimate YpB of yp mad the covariance matrix I;~ of the 
^ 
estimate YpB are obtained from (231.5), (231.7), (313.5), (313.6) and (A22.7) by 
A YpB = X*#o = X*(X'X+V-1)-I(x'Y+Â¥'I/I) 
(314.3) 
and 
1 
I;~p = ~ 
[2b+(#-#o ) 'Â¥- l(/t-/~o)+(y-X#o ) ' (y-X#o) ]X*(X'X+Â¥- 1)- 1X,," 
(314.4) 
By substituting (313.19) in the density function defined by (314.2) the results are ob- 
tained which are based on noninformative priors for the parameters/1 and z. 
If we do not want to predict observations as expected values but as actual observations, 
we start as mentioned above from the formulation (311.2) y=X~-e of the linear model 
and predict the mxl vector Yu of unobserved data by 
Yu = X*I[I - u, 
(314.5) 
where the mxu matrix X* has to be known and where u denotes an mxl vector of un- 
known errors. 
The predictive density function of the unobserved data vector Yu follows from (262.2) by 
p(yuly) = ~ ~ ... 7 p(yul~,z,y) p(/],z[y) d/~l,..d/~ud'C, 
(314.6) 
O 
-oo 
-oo 
since the parameters of the linear model (311.1) are ll and z. In agreement with the nor- 
mat distribution (311.8) for the observation vector y we assume the normal distribution 
for the predicted vector Yu under the condition that fl, z and y are given 
Yul~,,,y- N(X*~,z-II). 
(314.7) 
This leads to the density 
"C. 
X*"" 
p(yulll, v,y) = ~/2exp[ - ~tYu- 
p) (Yu "X*~] 
(314.8) 
The posterior density p(O, z I y) in (314.6) shall be given by the normal-gamma distribu- 
tion (313.3), which is based on the normal-gamma distribution (313.1) as informative 

75 
prior distribution and the likelihood function (311.9). Thus, with (A23.1) 
p(~i, z[ y) o~ zu/2+p- lexp{ - ~[2b+(~-/t) 'V" 1 (~-/t) ] } 
,'n/2exp[- ~(y-X]~)' (y-X~) ]. 
(314.9) 
By substituting (314.8) and (314.9) in (314.6) we obtain 
p(yu[y) = ~ p(yu,'C[y)d'c 
(314.10) 
O 
with 
p(yu,Zly) ~ ~ ... ~ 
z(m+n+u)/2+p-lexp{- ~[2b+(Yu_X*/i )'(yu_X*~) 
-00 
-~0 
+(~-#)'v'l(~/t) + (y-Y41)'(y-T41)]}dilt...dil u, 
(314. tl) 
where p(yu,'rty) denotes the joint density of Yu and ~: given y. We complete the 
squares on/1 in the exponent of (314.11) and find 
2b + yuy u + #'V-I/t + y'y + /I'MJ[I - 2~'(X'y+X*'Yu+V-l#) 
' 
' +V- 1 . 
= 2b + yuy u + /t'V-I# + y'y - (X'y+X*'Yu+V-Ig)'M-I(x'y+X* Yu 
/t) 
+ (~I-M" i (X' y+X* 'Yu+V - I/t) ) 'M(~-M" 1 (X" y+X*'Yu+V - I/t) ) 
with 
M = X'X + X*'X* + V -I. 
(314.12) 
The integral in (314.11) with respect to 1] can now be solved and yields, because of 
(A21.2), 
... ~ exp{- ~[O-M-l(x'y+X*'Yu+V-1/t))'M(,8-M-l(x'y+X*'Yu+V-1/t)] } 
-00 
-00 
1 M- 1 
/2 
,c-u/2 
dill...dil u = (2z~)u/2(det(Â¥ 
))1 
o~ 
. 
The joint density p (Yu' "ely) of Yu and "c follows with 
P(Yu' I:1 y) ~ z(m+n)/2+p- lexp{ - ~[2b+yuYu+/t, Â¥- Ig+y,y 
- (X'y+X*'Yu+V - l/t) "M- l(x, y+X*'Yu+V" I/t) ] }. 
(314.13) 
The exponent of this density shall be denoted by E. We complete the squares on Yu in E 
and find 
E = 2b + y'y - y'XM-1X'y + /t,V-1/t _ /t,V-1M-1V-1/t 
+ y,~(I-X*M'lx*')y u 
2YuX*M-l(x'y+V-1/t ) _ 2y'xM-lv-1/t 

76 
= 2b + y'y + /t'v-l# - (X'y+V-1/t)'M-I(x'y+V-I#) 
(X'M- 1 (X" y+V- i/t) )" (I- X'M- lX*" )" 1 (X'M- 1 (X" y+V- 1#) ) 
+ (Yu- (I-X'M- lx*' ) - 1X'M- 1 (X'y+V- 1#) ), (I-X'M- 11(*" ) 
(Yu- (I-x-M- lX*' )" 1X'M" 1 (X" y+Â¥" 1#) ). 
(314.14) 
By substituting (314.14) in (314.13) we recognize with (A23.1) the joint density function 
P(Yu' I:Iy) as a normal-gamma density. The parameters of this distribution can be sim- 
plified first by means of the matrix identity (Koch 1988a, p.40) and then by (224.10) and 
(314.12) 
(I_X,M-1X,')-Ix, M-I(x'y+V-1/t) = X*(M_X,'X,)-I(x'y+V-1/t) 
= X, (X'X+V- 1) - 1 (X'y+V- 1#) 
= X*/t o. 
(314.15) 
By applying the same matrix identity we find in addition 
_(X'y+Â¥-l/t)'M-l(x,y+V-1/t) _ (X,M'I(x,y+V-1/t))'(I_X,M-Ix,') -1 
X'M- 1 (X'y+Â¥- l#) 
= - (X'y+V- l/t) 'M" 1 (X'y+V" l/t) - (X'y+Â¥- 1#) 'M- lx*'x* 
(X' X+V- 1)- 1 (X'y+V- i/t) 
= - (X'y+V- l/t), [M- I+M- 1X*'i(* (X'X+V- 1 ) - 1] (X'y+Â¥- l/t) 
= - (X" y+V- l/t) "M- 1 [X'X+V- I+X*'X*] (X' X+Â¥- 1 ) - 1 (X'y+V- llt ) 
= - (X'y+V- lp), (X'X+V- 1) - 1 (X" y+V- l/t) 
= -#o(X'X+Â¥- 1)/to. 
(314.16) 
We substitute (314.15) and (314.t6) in (314.14) and obtain 
E = 2b + y'y + #'V-I/* - /*o(X'X+Â¥-1)#o 
+ (Yu-X*/to)'(I-X*M'lx*')(Yu-X*#o). 
(314.17) 
Finally we substitute (224.11) in (314.17) and find instead of (314.13) 
P(Yu' "r[y) ~ ,(m+n)/2+p- lexp{ - ~[2b+(/t_/to ) ,y- l(v_/to ) 
+ (y-X/to)'(y-X/to) + (Yu-X*/Xo)'(I-X*M-lx*')(Yu-X*/to)]}. 
(314.18) 
By comparing this density with (A23.1) and by applying the matrix identity (Koch 
1988a, p.40) and (224.10) 

77 
(I-X'M-Ix*') -1 = I + X*(M-X*'X*)-Ix *' 
= I + X*V X*' 
o 
we find the joint distribution for Yu and I: given y 
Yu' "r[ y - NG(X*/~o, I+X*VoX*', b o,po ) . 
(314.19) 
(314.20) 
The marginal distribution for Yu' which gives according to (314.6) the predictive distri- 
bution for the unobserved data vector Yu' follows from (A23.3) by 
Yu I Y - t (X*bt o, b o ( I+X*VoX* ' )/Po' 2Po )" 
(314.21 ) 
All inferential problems arising for the predicted observation vector Yu can be solved by 
A 
this distribution. The Bayes estimate YuB of Yu' for instance, and its covariance matrix 
X~u follow from (231.5), (231.7) and (A22.7) by 
A 
YuB = X*/lo = X*(X'X+V'I)-I(x'y+V-I#) 
(314.22) 
and 
t 
"V- 
Z~u = n+2p-2 [2b+(bt-bto) 
I(#'#o)+(Y-X/Lo)'(Y-X/to)] 
( I+X* (X' X+V- 1) - 1X,, ). 
(314.23) 
By substituting (313.19) in the density function defined by (314.21) the predictive densi- 
ty function is obtained for the case that noninformative priors are chosen for the parame- 
ters ~ and "r. 
If we compare the distribution (314.2) for the unobserved data vector yp predicted as the 
expected value of the observations with the predictive distribution (314.21) of the unob- 
served data vector Yu predicted as an actual observation vector, we see that the distribu- 
tions agree except for the parameter which governs the covariance matrix of the Bayes 
estimate. The covariance matrix (314.4) gives smaller variances than (314.23). This is 
understandable, since to the linear combination X*fl in (314.1), which predicts yp, the 
error vector u enters (314.5) to predict Yu" 
315 Linear Models Not of Full Rank 
It has been assumed so far that the matrix X of coefficients of the linear model has full 
column rank. We will now allow a rank deficiency for X and define correspondingly to 
(311.1) the linear model not of full rank 
E(y[fl) = 741 with 
rankX = q < u 
and 
D(y[a 2) = o-21. 
(315.1) 

78 
Again let the vector y of observations be normally distributed according to (311.5), so 
that the likelihood functions (311.6) and (311.9) are valid. 
Since with rankX=rankX'X=q (Koch 19883, p.42) the matrix X'X of normal equations is 
singular, the estimates (224.4) of the unknown parameters ll and 32 cannot be computed. 
We therefore replace the parameter vector ~ by the projected parameter vector l~ b, which 
in contrast to ~ represents an estimable function (Koch 19883, p.217) 
]~b = (X'X)rsX'X~" 
(315.2) 
The matrix (X'X)r s is a symmetrical reflexive generalized inverse of X')[ Because of 
(Koch 1988a, p.60) 
]q]b = X(X'X)rsX'X~ = X~] 
(315.3) 
the model (315.1) can be rewritten in terms of/l b. The estimates of/l o and a 2 follow 
with (Koch 19883, p.214, 217) 
~ = (X'X)~. X'y 
and ~y2= 1 
X~ '(y-X~ 
~ 
n-q (y- 
) 
)" 
(315 4) 
On completing the square of the exponent of the likelihood function (311.9) with/]b as 
parameter we obtain with (315.3) and (315.4) correspondingly to (224.5) 
= 
+ 
'X'X 
because of (Koch 1988a, p.60) 
= 
(y-X(X'X)rsX'Y) = 0. 
The likelihood function is therefore given correspondingly to (224.7) by 
T 
^ 
p(y]16b,Z ) = (2z)-n/2"rn/2exp{ - 2[(n-q)a2 + 
(315.5) 
^ 
With h(y)=l we see again by the factorization theorem (224.1) that 
and a 2 are suffi- 
cient statistics for ~ and 32. 
a) Noninformative Priors 
We will first introduce the noninformative priors (312.2) and (312.4) for the unknown 
parameters of the model (315.1). Applying Bayes' theorem (211.1) the posterior density 
function of the parameters/~b and 7: follows with (315.5) after omitting the constants by 
p(/lb,,C]y) ~ -cn/2-1exp{-~[(n-q)~r2 + (]lb-~b)'X'X(lEIb-~b)] }. 
(315.6) 
Like the density function (312.5), this density is proportional to the density of the nor- 
mal-gamma distribution (A23.1) with the inverse of X'X as a parameter. However, this 
inverse does not exist, so that the normalization constant for (315.6) is equal to zero 
because of det (X'X)=0. We therefore use a symmetrical reflexive generalized inverse 
(X'X)rs of X'X, which was already introduced for (315.2). If we select q components 

79 
from fi and ~b and the corresponding elements from X'X such that the resulting matrix 
becomes regular because of rankX'X=q, then an inverse (X'X)rs may be computed 
whose corresponding elements also form a regular matrix (Koch 1988a, p.68). The nor- 
mal-gamma distribution can then be normalized. Correspondingly to (312.23) we define 
fi= 
(fli)' fi,j..k 
= (1~1) and ~b = (~i)' J~b,j..k = (~1) 
for 
le{j,j+l ..... k}, 
Z= (ffij) = (X'X)r s 
and Zj..k = (alm) 
for 
1,me{j,j+l ..... k}. 
(315.7) 
With k-j+l=q we compare the density (315.6) for fi,j..k with (A23.1) and find as the 
posterior distribution because of n/2-l=q/2+(n-q)/2-1 the normal-gamma distribution 
fi,j.,k,'C[y- 
NG(~b,j..k,Zj..k,(n-q)~r2/2,(n-q)/2) 
for 
k-j+l = q. 
(315.8) 
The marginal posterior distribution of fi,j..k 
for k-j+l=q is therefore according to 
(A23.3) the multivariate t-distribution and for k-j+l<q because of (A22.10) also the 
multivariate t-distribution. Thus, 
fi,j..k[y- 
t(~b,j..k,~r2Zj..k,n-q) 
for 
k-j+l _< q. 
(315.9) 
According to the definition (315.7) of the vector fi, j..k the Bayes estimate ~bB of the 
A 
entire vector ill9 from (231.5) and its covariance matrix Z/3 from (231.7) therefore follow 
with (315.4) and (A22.7) by 
~oB = (X'X):oX'y~ 
and 
= "-:-q-  
2(x'X)is. 
n-q-2 
(315.10) 
(315.11) 
The Bayes estimate (315.10) is also the best linear unbiased estimate of the projected pa- 
rameter fi of the standard statistical techniques, its covariance matrix differs by the fac- 
tor (n-q)/(n-q-2) (Koch 1988a, p.217). 
We now look at a linear function ttfi of the projected parameters fib' where tt denotes an 
rxu matrix. Starting from (315.6), 
we integrate out the parameter ~: with 
n/2- l=q/2+(n-q)/2-1 and with (A12.2) to obtain 
P(filY) Â¢` (n- q+ (fi- ~b) ' X'X(fi- ~b)/~z) "q/2. (n-q)/2 
This density function is proportional to the density function (A22.1) of the multivariate 

80 
A 
t-distribution with the inverse of X'X/# as a parameter. This inverse does not exist, so 
1% 
that we use cr2(X'X)" instead, where (X'X) - denotes a generalized inverse of X'XL If 
rank(lt(X'X)rsX'X ) = r 
for 
r <_ q 
(315.12) 
is fulfilled, then the matrix 
H(X'X)rsX'X(X'X)-X'X(X'X)rs H' = H(X'X)rs H' 
is regular for any choice of the generalized inverse (X'X)- (Koch 1988a, p.228). By 
choosing a regular inverse for the density we therefore obtain for ll/l b from (A22.12) the 
multivariate t-distribution 
A 
I~ly 
- t(t~,cr2H(X'X)r H',n-q)~ 
for 
r < q. 
(315.13) 
Furthermore, with (A22.13) we find the distribution, which corresponds to (312.28), 
1 
^ 
(lt/lb-~b)'(lI(X'X)rslt') - (tt~b-B~b)/(rcr2) - F(r,n-q). 
(315.14) 
Thus, when establishing confidence regions or when testing the hypothesis 
tt Â° : I~ 
= w versus 
H 1 : ~ 
Â¢ w, 
(315.15) 
Bayesian inference in the case of noninformative priors gives also for linear models not 
of full rank results which are equivalent to the standard statistical techniques. 
Because of (315.8) the marginal posterior distribution of the weight parameter "c is ac- 
cording to (A23.4) the gamma distribution 
A 
zly - G((n-q)cr2/2, (n-q)/2). 
(315.16) 
^ 
The Bayes estimate cr~ of or2 and its variance thus follow from (312.14) and (312.15) 
with n-u replaced by n-q. 
b) Informative Priors 
Now we start with informative priors for the parameters ll and cr2 of the linear model 
(315.1) not of full rank. We will assume as in Section 313 that the prior information is 
introduced by the normal-gamma distribution (313.1) 
/1,z - NG(#,Y,b,p) 
(315.17) 
with z=-I/if2 and the parameters of the distribution determined by (313.2). 
If the matrix V is positive definite and therefore regular, then the matrix Y Â° in (313.3) 
V = (X'X+V-1) -1 
(315.18) 
O 
is also regular, although X'X is singular. This is due to the fact that X'X is positive 
semidefinite. For such a case there is no difference to the model of full rank. 

81 
Let the matrix V now be singular. To interpret such a situation, we recapitulate that in 
the model of full rank the prior information on the parameter vector/] was introduced 
according to (313.2) by the vector ffp of expected values and by the covariance matrix 
Z/3. Solving for Z/~ we find 
with 
V -I = X~Xp. 
(315.19) 
The inverse V- 1 of the matrix V representing the prior information can therefore be inter- 
preted as originating from prior information X'X on the matrix X'X of the normal equa- 
PP 
tions for/3. Thus, if in a model not of full rank the matrix V is singular, we introduce the 
prior information by the matrix X'X , which represents the prior information on the ma- 
PP 
trix of normal equations. Furthermore let 
rank(X~Xp) = q 
and 
rank(X'X+X~Xp) = q. 
(315.20) 
The normal-gamma distribution (315.17) cannot be normed, if the matrix V is singular, 
but with the same reasoning, which leads from (315.6) to (315.10) and (315.11), and 
with 
rx)- 
v o -- (x'x+ 
rs 
we obtain instead of (313.5) the Bayes estimate _~hB of the projected parameter 
with 
(X'X+X'X)" 
X' Ã· ' 
v V rs( 
y X~Xpff) 
(315.21) 
and instead of (313.6) the covariance matrix Y,~ of the estimate 
1 
X~ = ~ 
[2b+(ll-#o)'X~Xp(li-llo)+(y-Xllo)" (y-X#o) ] (X'X+X~X)rs. 
(315.22) 
A 
The estimate a~ of# and its variance follows from (313.16) and (313.17) with replacing 
V -1 by X~Xp. 
Example 1: The coordinates of points of a geodetic network determined by distance 
measurements shall be estimated. Since information is lacking on the position of the net- 
work within the coordinate system, in which the coordinates of the netpoints have to be 
computed, one speaks of a free network, whose datum needs to be defined (Koch 1988a, 
p.219). Let ~ be the vector of the unknown coordinates of the netpoints, y the vector of 
the distance measurements and X'X the resulting matrix of normal equations. Let the pri- 
or information/tp on the coordinates of the network be available by the prior measure- 
ments yp. They lead via the coefficient matrix X to the prior information X~Xp on the 

82 
matrix of normal equation, so that (315.20) is fulfilled. The matrix g representing the 
information is therefore singular and the Bayes estimate vJ~hB of the coordinates 
prior 
projected by (315.2) follows from (315.21) with 
J~bB = (X'X+X'X) r" (X'y+X'X #). 
(315.23) 
pp 
~ 
p p  
The vector #p of prior information shall be determined by the estimates of the co- 
ordinates resulting from yp. Hence, we obtain with (313.2) and (315.4) 
la = #p = (X~Xp)rsX~Y p. 
(315.24) 
By substituting this result in (315.23) we find because of (Koch 1988a, p.60) 
X~L(~L) ;oK = X" 
(315.25) 
p 
pp 
lap 
p 
estimate _~bB of the projected coordinates 
the 
~bB = (X'X+X~'k) rs (X'y+X'y.). 
(315.26) 
P P 
P P 
The estimate ~bB does not depend on the datum chosen in (315.24) to compute the prior 
information/~p on the coordinates. This result had to be expected, since the prior infor- 
mation was assumed to come from the prior measurements yp and the resulting matrix 
Xp'X only, which do not contain information on the datum. 
P 
We will now assume that prior information comes from the prior measurements yp with 
the coefficient matrix Xp together with information on the datum of the network. This 
information is introduced by means of a (u-q)xu matrix B such that 
rank(X~Xp+B'B) = u. 
Then the matrix V with 
V -1 = X'X + B'B 
(315.27) 
PP 
is regular. The matrix B may be obtained from the matrix E whose rows constitute with 
XE'=O a basis for the null space of X. The estimate 
~b = (X~Xp+B'B) - 1X~yp 
(315.28) 
is then given in a datum defined by B (Koch 1988a, p.217). 
The prior information/Lp on the coordinates shall be given by J~b' so that with (313.2) 
=ltp = ~b" 
(315.29) 
/1 
With V defined by (315.27) the results of the model of full rank apply and we obtain 
from (313.5) the Bayes estimate ~B of the coordinates j8 of the netpoints 
~B = (X'X+X~Xp+B'B) -1"~' Ã·X' 
" 
tx y 
pyp). 
(315.30) 

83 
In contrast to the estimate (315.26) the result is now dependent on the datum, which was 
introduced by the prior information. 
A 
316 Model Identification 
There are special linear models for which it is not known at the beginning of the data 
analysis, how many unknown parameters have to be introduced or what the dimension of 
the matrix of coefficients should be. Examples are the polynomial model, the autore- 
gressive model for time series or the discrete Fourier series, whose coefficients can be 
interpreted as being obtained by the method of least squares (Steams 1975, p.15). We 
will present the first two models and show that they lead to a general problem of identi- 
fying a model. This is also true for the third model. Bayes' theorem is then applied to 
solve that problemâ¢ 
a) Polynomial Model 
The polynomial model is often used to fit a curve to data points. If a curve is given in a 
plane, where a rectangular (x,y) coordinate system is defined, and if on the curve 
points Pi with the coordinates (x i 'Yi ) are selected such that for given abscissae x i the 
ordinates Yi are measured, then the expected value of the measurement Yi given the u 
unknown parameters 131 to/3u collected in/3 may be represented by the polynomial 
u-1 
E(Yi[/I) = /31 + xi/32 + x~]33 + "'" + xi 
/3u 
for 
i~{1 ..... n}. 
(316.1) 
Polynomial models not in the plane but for higher dimensions are similarly constructed 
(Koch 1988a, p.231). 
For polynomial models generally the question arises, up to which degree the polynomial 
expansion has to be carried. This means that the integer u in addition to ~ is an unknown 
parameter. With 
u e {I ..... o}, 
(316.2) 
where o denotes the largest possible degree of the polynomial expansion, which is 
o=n- 1, since with o=n there would be no curve fit any more. We define 
il 
Y2 
, ~1 u 
/32u 
â¢ 
~ 
~ 
X u 
y = 
- 
u-l 
x0 xl 
Xl 
xO 
... 
o
i
 ........ 
x n 
x n 
â¢.. 
x n 
(316.3) 
and obtain instead of (316.1) by assuming independent observations with identical vari- 

84 
ances ~='r- 1 
E(yl~lu,U) = Xu~ u with D(y['c)= "flI. 
(316.4) 
This linear model contains in comparison to the linear model (311.1) together with 
(311.7) the integer u as unknown parameter. 
b) Autoregressive Model 
Let Yi with ie{1 ..... n} represent a time series, let fij with je{1 ..... u} denote an 
unknown parameter and e i with i ~ { 1 ..... n } an error, then 
Yi = Yi-t]31 + Yi-2/32 + "'" + Yi-uflu - ei 
(316.5) 
is called an autoregressive process (Box and Jenkins 1970, p.51). The values Yo' 
Y-1 ..... Yl-u are initial observations and assumed to be known. Again the integer u 
with us {1 ..... o} and o=n-1 is unknown. By defining 
[iyl 
[el I 
I~ 
/ 
[flu j 
[/31ul 
[Yo 
Y-1 
"'" 
YX-u 
..................... 
Y2 
e2 
/32u 
Yl 
YO 
" " " Y2- u 
y= 
, e= 
' ~u ='..u|' Xu 
' 
â¢ 
en" 
[Yn-1 Yn-2 
"'" 
Yn-uj 
u e {1 ..... o} 
(316.6) 
and assuming 
E(el/iu,U ) = 0 
we obtain the model 
and D(y]z) = z-lI 
(316.7) 
E(yl/]u,U ) = X~u 
with D(y['O = z-lI. 
(316.8) 
If stafionarity is not assumed, which means that no constraints are introduced for the pa- 
rameters, then model (316.8) is identical with model (316.4). 
We thus have obtained again a linear model, which contains in comparison to the linear 
model (311,1) the integer u as an additional unknown parameter. 
c) Likelihood Function 
To make inferences on u we assume the observations of model (316.4) or (316.8) 
as being normally distributed. Thus, 
yl~ u, ~:,u - N(Xu~ u, z-lI) 
(316.9) 
and with (A21.1) we obtain the likelihood function 
p(yl~ u, ~:,u) ~ zn/2exp[- ~(Y-Xu~u)' (Y-Xu~u) ]. 
(316.10) 
To apply Bayes' theorem we introduce noninformative and informative priors for the 

85 
unknown parameters flu' "rund u. 
d) Noninformative Priors 
In agreement with (312.2) and (312.4) we assume the following noninformative prior 
densities for the unknown parameters flu' ~ and u 
p(flu ) :< const., p(~:) :< l/T, p(u) ~< const. 
(316.11) 
If in addition the parameters are independent, we obtain with (316.10) from Bayes' the- 
orem (211.1) the joint posterior density function for flu' "r and u 
P(flu"C'u [y) :< ~/2-lexp[ - ~(y_Xflu), (Y.Xuflu)]" 
(3t6.12) 
With completing the square on flu in the exponent we find in analogy to (224.5) 
' 
= 
" 
X 
(flu-~u ) XuXu (flu- ~u) 
(316.13) 
(Y-Xufl u) (Y-Xufl u) 
(Y-Xu~} 
u) (y-u~!u)+ 
'' 
with 
~u = (XuX)- 1XuY" 
By substituting (316.13) in (316.12) and by integrating with respect to flu we obtain with 
(A21.2) 
7 â¢ .. T exp[------ 
-------- 
--~(~]-~")'X'X(fl''~")]dfiiu'"dfi"u 
= (21c)u/2~r-u/2(detXÂ£Xu)- 1/2 
(316.14) 
The marginal posterior density of â¢ and u therefore follows with 
p(~,uly ) ~ (2zQu/2(detXÂ£x)-l/2z(n-u)/2-1exp[_ ~(y-i~u)'(y-X~u)] " 
(316.15) 
Now we integrate with respect to 7: and obtain with (A12.2) 
T z(n-u)/2-1exp[- ~(y-X~u )' (y-Xu~u)]d~ 
o 
= F((n-u)/2)[~(,-Xu~u)' (y-X ~u)]- (n-u)/2 
(316.16) 
With this result we finally obtain the posterior density of u 
p(uly) ~ /rU/2F((n-u)/2) (detXÂ£Xu)-I/2 ((y_x~u)' (Y_iu~u))-(n-u)/2 
(316.17) 
For each integer u with ue {i ..... o} we may compute the probability p(uly ) from 
(316.17) and then select the model for which p(u I Y) attains a maximum. The density 
p(uly) depends on the sum of squares (y-Xu~u)' (y-X~u) of the residuals. This sum 
also enters the information criterion AIC and the final prediction error FPE 
(Akaike 1979) by which the order of an autoregressive model is estimated (Ulrych and 

86 
Bishop 1975). 
e) Informative Priors 
Again we will assume that no information is available on u. This results in the prior 
p(u) ~ const. 
(316.18) 
However, for the parameters fu and "c we introduce the normal-gamma distribution as 
prior (Broemeling 1985, p.204) 
fu,~r- NG(N,V,b,p). 
With the vector Np of expected values and the covariance matrix Xfl of fu and the ex- 
pected value cr~ and the variance Vc# of cr2=l/z given, the parameters of this distribution 
are determined by (224.20) 
N = Np, V = Xf/cr~, p = (~)2/Va2 + 2, b = (p-1)cr~. 
Thus, with (A23.1) we obtain the prior density 
P(fu' z) ~ (2g)-u/2(detV)- 1/2zu/2+P- lexp{- ~[2b+(~u-N) "g" 1 (flu_N)] }. 
(316.19) 
Bayes' theorem (211.1) together with (316.10), (316.18) and (316.19) gives the joint 
posterior density of fu' z and u by 
P(fu' l:,u[y) ~ (2g)-u/2(detÂ¥)- 1/2z(n+u)/2+p- 1 
exp{- ~[2b+(~lu- N) 'V-X(~u-N)+(y-X~u)" (y-Xu~lu)] }. 
(316.20) 
We complete the squares on ~1 u and obtain with the derivations leading to (224.11) 
P(fu' z, u ly) ~ (2~r)-u/2(de tV)- 1/2z(n+u)/2+p- 1 
exp{-~[2b+(N-No)'V-l(N-go ) + (y-XNo)'(y-XNo) 
(fu-#o)' (XuX+Â¥" 1) (0u-No) ] } 
(316.21) 
+ 
with 
#o = (XuXu+V- 1 )- 1 (XÂ£y+g- 1#). 
(316.22) 
We integrate with respect to ~u and obtain with (A21.2) 
7 ... 7 
exp[-~(~lu-No)'(X~X+v-X)(~lu-No)]dfllu 
... dflu u 
-Â¢~ 
-0o 
= (2~)u/2z-u/2(de t (XuXu+V- 1 ) ) - 1/2 
(316.23) 
The marginal density p(~,u [y) of z and u therefore follows from (316.21) with 

87 
p(,,uly) ~ (detÂ¥ det(X~X+Â¥-l))-l/2"cn/2+p-1 
exp{- ~[2b+(#-go)'V-l(g-#o)+(y-X~o)'(y-X#o)]}. 
Finally we integrate with respect to T and obtain from (A12.2) 
zn/2+p-lexp{- ~[2b+(~-~o)'V'l(~-#o)+(y-X#o)'(y-X~o)]}dz 
O 
= F(n/2+p){~[2b+(#-#o)'Â¥'l(#-#o)+(Y-Xu#o)'(Y-Xu~o)]} "(n/2+p) . 
(316.24) 
(316.25) 
With this result we find the posterior density function p(u[y) of u 
p(uly) ~ (detYdet(XuXu+Â¥-l)) "1/2 
[2b+(#_#o) ,Â¥- 1 (#_#o) + (y_ Xu/lo ) , (y_ Xu~to) ] - (n/2+p). 
(316.26) 
Again p(u[y) depends on the sum of squares (Y-Xu#o)'(y-X#o) of the residuals but 
now computed with the estimate (316.22) of/]u" The integer u with ue { 1 ..... o} for the 
model identification is chosen such that p(u [y) attains a maximum. 
We will now assume that prior information is only available for the variance factor 
e2=l/, by the expected value a~ and the variance Ve2 of a 2. We choose the gamma dis- 
tribution for the weight parameter ~ as prior 
- G(b,p) 
(316.27) 
and for/]u and u in agreement with (316.11) the priors 
p(/]u ) ~ const., p(u) ~ const. 
With "c having the gamma distribution, a2 follows from the inverted gamma distribution 
a2-IG(b,p) with expected value and variance from (A13.2). The parameters b and p are 
therefore determined by 
p = (a~)2/Va2 + 2, 
b = (p-1)a~ 
in agreement with (224.20). 
With the prior distribution thus defined, the joint posterior density of flu' 1: and u follows 
from Bayes' theorem (211.1) with (316.10) and (A12.1) by 
p(/I u, z,u IY) ~ ~/2+p- lexp{" ~[2b+(Y_Xu/lu ) , (Y_Xu/lu) ] }. 
(316.28) 
The integration with respect to/l u has been solved by (316.13) and (316.14). Thus, 
p('r,ulY) ~, (2jr)u/2(detX~Xu)" 1/2,(n-u)/2+p- 1 
â¢ 
X 
. 
exp{-~[2b+(Y-Xu~u) (y-u~u)]} 

88 
The integration with respect to "c similar to (316.25) finally 
distribution for u 
p(u l Y) 
(de tXuX)- 1/2 
[ 2b+ (y_ Xu~u), (y. Xu~Iu)]- ((n-u)/2+p) 
Again the integer u is selected such that p(uly) attains a maximum. 
gives the posterior 
(316.29) 
317 Less Sensitive Hypothesis Tests for the Standard Statistical 
Techniques 
As was shown, the decision to accept or reject the hypothesis (315.15) 
Ho : H/] b = w versus 
H 1 : I~ 
~ w 
(317.I) 
for the linear model not of full rank is based on the same criterion as in the standard sta- 
tistical tests. The same holds true for the hypothesis (312.26) of the linear model of full 
rank. This is due to the noninformative priors for the unknown parameters of the linear 
model. 
When applying the standard statistical techniques, there are cases that the information or 
the assumption which needs to be tested cannot be exactly expressed by the null hypoth- 
esis Bflb--W in (317.1). Instead of the null vector for the difference Hflb-W, small intervals 
or regions, which enclose the null vector, should be introduced. As an example, the ana- 
lysis of measurements is mentioned which are taken at two time epochs to detect the 
movements of man-made constructions or movements of the earth's crtlst. In such an 
analysis the hypothesis is tested that for so-called stable points the differences of the 
coordinates from two time epochs of observations are equal to zero (Koch 1985). Be- 
cause of the centering errors of the instruments or because of the residual errors, when 
taking into account the influence of the atmosphere, it would be more realistic to formu- 
late the hypothesis such that small regions are defined for the coordinate differences, in 
which they can vary. If in such a situation we nevertheless set Hflb-W=0, the test reacts 
too sensitively. This means that the null hypothesis is rejected in cases where, based on 
the knowledge of the situation to be tested, the acceptance would have been expected. 
Bayesian analysis presents no problems when testing hypotheses which are formulated 
by regions for the parameters, cf. (253.5) and (253.6). We will therefore use the 
Bayesian approach to develop a test for the hypothesis (317.1) which is less sensitive 
than the standard statistical test. It shall be derived such that it modifies the F-test of the 
standard statistical techniques (Koch 1984; Riesmeier 1984). The generalization from the 

89 
univariate to the multivariate model can be found in Koch and Riesmeier (1985). 
Let A denote the space of the linear combinations I~ of the unknown projected parame- 
ters 0b and let a o denote a subspace of A, thus AoC A. We formulate corresponding to 
(252.1) the hypothesis 
H o : ltfl b e A o 
versus 
HI: ltflb e A\A o. 
(317.2) 
The null hypothesis is rejected according to (252.3), if 
P(It/! b ~ Aoty) > 1-a 
(317.3) 
is fulfilled. As in Section 252, we give the subspace A Â° a special shape defined by means 
of the point 
]t~ b = w 
(317.4) 
in (317.1) with 
T= 
(H~o-W)' (H(X'X)rsH')-l(ll~b-W)/(r~.2), 
(317.5) 
such that 
A O = {I~:(t~b-I~)'(H(X'X)rsH')-I(~o-H~b)/(r~2)< 
T}. 
(317.6) 
The space A o thus defined has the shape of an hyperellipsoid, cf. (242.6). 
If 
T > Fl_cc;r,n_q, 
(317.7) 
where Fl_a; r ,n-q denotes the upper c~-percentage point of the F-distribution with r and 
n-q degrees of freedom, we obtain instead of (317.3) because of (315.14) 
T 
P(B~ b ~ AoIy) = f F(r,n-q)dT > 1-c~ 
(317.8) 
O 
and the null hypothesis in (317.2) is rejected. 
As a consequence of the definition of the subspace A Â° by (317.4) to (317.6), the hypothe- 
sis (317.2) may be replaced by the hypothesis (317.1). Hence, the null hypothesis of 
(317.1) is rejected, if (317.7) is fulfilled. This test procedure is equivalent to the standard 
F-test with T from (317.5) being the test statistic. This has already been mentioned in 
connection with (312.30) and (315.15). 
It was already pointed out that the test of (317.1) by means of (317.7) may react too sen- 
sitively. To obtain a less sensitive test, we restrict the parameter space A for H~b. Hence, 
we introduce a subspace A r with ArC A for the parameters, where they are allowed to 
vary without contributing to the statistical inference. In A r the posterior density function 
P(It/1 b l Y) for Ill) b is constrained to zero. Due to the constraint, the posterior density 

90 
p(I~[y) is truncated, which is admissible according to (225.1), if we renormalize the 
truncated density. 
With the subspace A r the parameter space A for ~ 
is restricted to the complement ACr 
of A r by 
5Cr = AkAr" 
(317.9) 
Let the posterior density function for ~ 
restricted to Ac r be denoted by Pc (I~ [y). It is 
renormalized on A c by 
r 
Pc(I~ly) = (1-I p(l~ly)dlt~b)-lp(l~bly). 
(317.10) 
A 
r 
The probability Pc(ll~b e AolY) computed by the truncated posterior density function 
Pc (l~b I Y) follows from (317.10), since Pc (Itl~b I Y)=0 in A r, with 
Pc(ll~b ~ AoiY) = (1-I p(t~iy)dll~b)-i 
I 
p(llflblY)d ~. 
(317.11) 
h r 
Ao\A r 
The subspace A r shall now be defined according to (317.4) to (317.6) by the given point 
tll~b--W r. It means that A r takes the shape of an hyperellipsoid. Thus, from (317.8) 
T 
r 
I p(tlflblY)d~6 b = S F(r,n-q)dT 
A 
o 
r 
with 
- 
- 1(It~ b 
^ 
T r = ( 
-Wr)' (lt(X'X)r sH') 
-Wr)/(ro-2 ) . 
(317.12) 
The probability for the less sensitive test now follows with 
T 
T 
r 
Pc(H~b ~ AolY) = 
T 
(1-S F(r,n-q)dT) -1 (~ F(r,n-q)dY 
r 
- 
F(r,n-q)dT) 
o 
o 
o 
for 
T < T 
(317.13) 
r 
and 
Pc(tt~b ~ AolY) = 0 
for 
T r > T. 
(317.14) 
We conclude with (317.8) that the null hypothesis in (317.1) is rejected by the less sensi- 
tive test, if 
Pc(I~ e Ao[Y) > 1-a 
or 
a T < a, 
(317.15) 

91 
where 
and 
= 1-(1-c)-1(~ 
F(r,n-q)dT-c) 
O 
(317.16) 
T r 
= 1 for 
T r ~ T, c = J F(r,n-q)dT. 
(317.17) 
O 
The boundary between the region of acceptance and rejection is given by (317.15) with 
g,l.=a. Thus, with (317.16) 
1 T 
(l-c)- (J F(r,n-q)dT-c) = 1-a 
O 
or 
T 
F(r,n-q)dT = 1-c~+ca. 
O 
The null hypothesis in (317.1) is therefore rejected, if 
T > Fl_a+ca;r,n_q, 
(317.18) 
where Fl_a+ca;r,n_q denotes the upper (a-ca)-percentage point of the F-distribution 
with r and n-q degrees of freedom. 
Because of 0<c<1, it is obvious from (317.18) that the test of the hypothesis (317.1) by 
(317.15) or (317.18) is less sensitive than the test of the standard statistical techniques. 
The sensitivity of the test depends on the size of the subspace A r, where the parameters 
are allowed to move without contributing to the statistical inference. The size of A is 
r 
determined by the quantity T r from (317.12), which in turn is controlled by the given 
vector lq'/~b-W r. The values for the components of t0~b-W r should be chosen such that 
^ 
they are smaller than the standard deviations computed with a 2 for the differences 
~b-W. If larger values are introduced to compute T r from (317.12), the test by (317.15) 
or (317.18) becomes too insensitive. 
Example 1: We want to test the hypothesis (312.36) of the Example 2 of Section 312 by 
the less sensitive test. The test statistic T from (317.5) follows with (312.37) by 
A (~-Uo)2 
T= 
^ 
(r2/n 
With the following values 
^ 
^ 
n = 10, # = 15026.2 cm, #o = 15027.0 cm, ~2 = 1.0 cm 2 

92 
we compute 
T = 6.40, 
so that with ~---0.05 and 
F1_0.05;1,9 = 5.12 
the null hypothesis in (312.36) has to be rejected because of (312.37) or (317.7) by the 
standard test. We now introduce the difference 
A 
#-#r = 0. 2223 cm 
to define the quantity T r in (317.12), which in turn determines the region to be excluded 
from the statistical inference. For this example it is the interval 
A 
# + 0.2223 cm. 
A 
A 
As recommended, the difference/Z-gr is smaller than the standard deviation of g-gr' 
A 
which is 6/qrn=0.32 cm. We compute with (317.12) 
A 
T r = n(#-#r)2 = 0.494 
and obtain from a table of percentage points of the F-distribution the value of c from 
(317.17) by 
T 
r 
c = I F(1,9)dT = 0.5. 
o 
With 1 - c~+c c~= 1- 0.05+0.025=I - 0.025 we find the percentage point 
F1_0.025;1, 9 = 7.21, 
so that according to (317.18) the null hypothesis in (312.36) is accepted by the less sen- 
sitive test, while it is rejected by the standard test. 
A 
318 Linear Dynamic Systems 
So far we have considered linear models whose unknown parameters do not change with 
time. Now we will introduce the parameters of a dynamic system which are functions of 
time. The differential equations governing the orbit of a satellite for instance constitute 
such a dynamic system. The state vector, that is the position and the velocity vector, re- 
presents the vector of unknown parameters which depend on time. These unknown pa- 
rameters have to be estimated. In geodesy, for instance, the parameters of the dynamic 
system describing the inertial navigation need to be estimated (Schwarz 1983). 

93 
Let/3k be the uÃl vector of unknown parameters, the so-called state vector, at the time 
epoch k. It is linearly transformed to the unknown uxl state vector/3k+l at the epoch 
k+l by the uxu transition matrix O(k+l ,k), which is known. A uxl vector w k of random 
disturbances is also added with E(Wk)=O and D(Wk)=Qk, where the uxu covariance matrix 
Qk is given. If N epochs are considered, we obtain 
/3k+l = ~(k+l'k)/3k + Wk with 
E(Wk) = O, D(w k) = Qk' ke{1 ..... N-l}. 
(318.1) 
This is called a linear dynamic system. It is obtained by the integration of the differential 
equations constituting the system, see for instance (Jazwinski 1970, p. 199). If the differ- 
ential equations are nonlinear, one has to linearize, e. g. (Jazwinski 1970, p.273). 
The observations, which contain the information on the unknown state vectors/3k' shall 
establish a linear model We will introduce it in the general form (311.3). First we as- 
sume that the variance factor 0-2 is known and later that it is unknown. 
a) Variance Factor Known 
Let Yk be the nxl vector of observations at the time epoch k, which contains information 
on/3k' and Pk the nxn known weight matrix of Yk' where we have absorbed the variance 
factor or2, which as mentioned is assumed as known. Thus, we obtain instead of (311.3) 
E(Yk]/3 k) = Xk/3 k 
with 
D(Yk) = Pk 1, 
(318.2) 
where the nÃu matrix X k of coefficients is given. 
Let the random disturbances w i and wj, the observations Yi and yj and w i and yj for 
iÂ¢j be independent. In addition, let the vectors be normally distributed 
w k - N(O,Q k) 
(318.3) 
and 
Ykl/lk - N(Xk/3k,Pkl). 
(318.4) 
The latter distribution leads to the likelihood function for the observation vector Yk" 
Based on the observation vector Yk we want to estimate the unknown state vector I1 k of 
the epoch k. The prior information results from the estimate for Ilk_ 1 of the previous 
epoch based on the observation vector Yk- 1" The estimate for/3 k_ 1 in turn uses prior in- 
formation based on Yk-2 and so on. Hence, we will recursively apply Bayes' theorem 
according to (212.1). 
Let the prior distribution of the state vector/31 of epoch 1 be given by the normal distri- 
bution 
/31 lYo - N(~I ,o'Z1 ,o )' 
(318.5) 

94 
whose parameters, the vector ~1, o and the covariance matrix Y't, o' are given by prior 
information. The first index in ~1, o and Z1, o refers to the epoch of E1 and the second 
index to the epoch of the observation vector Yo' from where the prior information stems. 
At the beginning no observations are available, therefore Yo" In the following, we will 
omit this vector Yo' 
The prior density function p(i~ll) defined by (318.5) and the likelihood function 
P(Yl ii{11) from (318.4) are now substituted in Bayes' theorem (211.1) to obtain the pos- 
terior density p(~l 1 lYl)" This density is normally distributed, as will be shown below, 
and after applying the transformation (318.1) gives the prior p(~2 ) for the parameter 
vector J~2 of epoch 2. This density has the same form as (318.5). 
Since we recursively apply Bayes' theorem, we immediately start with epoch k instead 
of epoch 1, i.e. with the prior distribution 
l~klYl ..... Yk-1 - N(~k,k-l'~k,k-1 )" 
(318.6) 
Bayes' theorem (211.1) together with the likelihood function from (318.4) gives 
p(~ly I ..... yk)~ exp{- ~[(~-~k,k_l)'XKl, k_l(~-~k,k_l ) 
+ (yk-Xk~)' Pk(Yk- XkA~k) ] }. 
(318.7) 
Completing the squares on/~k leads to 
j~(Xt~P,.Xi.+Y-~lv 1)j~ k - 2~(Xl~PkYk+l~Ik.l~k,k_ 1) + Yl~PkYk 
, 
-1 
+ ~l~,k-1Zk,k - lJ~k,k- 1 
â¢ 
~ 
-1 
~k 
~1, 
= Yl(PkYk + 
,k-lZk,k-1 
,k-1 + (~k'#o)'(XkPkXk + 
k-1)(l~k-tto) 
-#o(X~PkXk+XK}k_ 1 )#o 
with 
/to 
(Xl~PkXk+Y~I, k- 1 )-1 
" 
-1 
= 
(X~PkYk+Zk, k_ l~k, k_ 1 ) â¢ 
Substituting this result in (318.7) reveals with (A21.1) the normal distribution as posteri- 
or distribution for J~k" Thus, 
l]klYl ..... Yk - N(go'(XiPkXk+Zkl, k-1)'l)" 
(318.8) 
The expected value is chosen as Bayes estimate ~k,k of ~ according to (23t.5), so that 
we obtain 
~k,k = /to 
(318.9) 
and its covariance matrix denoted by Ek, k according to (231.7) 

95 
Zk, k = (X~PkXk+Z~I,k_I)-I 
(318.10) 
By applying the identities (313.9) we finally derive 
~k,k 
~k,k- 1 + Fk(Yk-Xk~k,k- 1 ) 
(318.11) 
with 
Fk = Ek,k-lXl~(XkZk,k-'X;+P~"i)-li 
K 
and 
Zk, k = (I-FkXk)Zk,k_ 1, 
(318.12) 
so that ~k,k and Zk, k are recursively computed from ~k,k- 1 and Zk,k_ 1" 
To complete the recursive estimation, we still have to show the transformation of the 
posterior distribution p(~_ l ty 1 ..... Yk-I ) for ~-1 to the prior distribution (318.6) 
for /~k" In other words, we have to transform __~k-l,k-1 to __~k,k-1 and Zk-l,k-1 to 
Zk,k_ 1. We obtain with (318.8) to (318.10) 
/lk-llYl 
..... 
Yk-1 
- 
We apply the transformation given by the dynamic system (318.1) to l~k_ 1 and find with 
(318.3) by the theorem for the linear transformation of normally distributed random vec- 
tors (Koch 1988a, p.142), since/lk_ 1 and Wk_ 1 are independent, the distribution for t~ k 
~klYl ..... Yk-1 - N(~k,k-l'Zk,k-1) 
(318.13) 
with 
~k,k- 1 = ~(k'k- 1)~k- 1,k- 1 
(318.14) 
and 
Ek,k-1 = O(k'k-1)Zk-l,k-lO'(k'k-1) + ~-1" 
(318.15) 
The distribution (318.13) is identical with (318.6), so that we could immediately start 
from epoch k instead of epoch 1. 
The recursive estimation (318.11) and (318.12) together with (318.14) and (318.15) is 
known as the Kalman-Bucy filter. It is applied as a forward filter which updates the 
knowledge about the state vector whenever new observations are available. These up- 
dates may be computed in real time, if the Kalman-Bucy filter is applied, for instance, in 
aircraft navigation. 
The observation vector Yk does not only contain information on/t k, ~k+l ..... /3 N, but 
also information on ~ik_l,l~k. 2 ..... /11. After having determined the estimates 

96 
~1,1' [~2,2 ..... ~N, N we may therefore go backwards and improve the estimates by in- 
troducing the dynamic system (318.1) as observation equation. This backward filter is 
called smoothing, see for instance (Koch 1982). 
b) Variance Factor Unknown 
We will now assume that the covariance matrix of the vector w k of random disturbances 
and the covariance matrix of the vector Yk of observations are known except for the un- 
known variance factor a 2. We therefore substitute in (318.1) 
D(Wk) = ff2Qk 
(318.16) 
and in (318.2) 
D(Yk) = a2Pk 1. 
(318.17) 
As in the linear model we will work with the weight parameter 
= 1/a 2 
(318.18) 
and obtain instead of (318.3) the normal distribution for w k under the condition that v is 
given 
Wkl â¢ - N(O, "c- 1Qk) 
and instead of (318.4) 
yk][lk,~- N(Xk[lk, v-lPkl). 
(318.19) 
(318.20) 
Let the prior distribution of the state vector E1 of epoch 1 given ~ be defined as in 
(318.5) by the normal distribution with the parameters ~1, o and ~-1V1, o 
v-lg 1 o) 
(318.21) 
I~il[~:'Yo - N(~I,o' 
, 
' 
where again the first index in ~1 ,o and V 1 ,o refers to the epoch of/11 and the second 
index to the epoch of the observation vector Yo' from where the prior information stems. 
The vector Yo indicates that no observations are available at the beginning, Yo will be 
omitted later. 
Let the prior distribution of the weight parameter v be defined by the gamma distribution 
~]Yo - G(bo'Po)" 
(318.22) 
The joint distribution of ~l 1 and v is therefore determined according to (A23.1) by the 
normal-gamma distribution 
[ll,~]y Â° - NG(~l,o,Vl,o,bo,Po). 
(318.23) 
If/tip denotes the expected value of El, I;1~ its covariance matrix, (r~ the expected val- 
ue of the variance factor (r 2 and V(r 2 its variance, the parameters of the normal-gamma 

97 
distribution (318.23) are determined with (224.20) by 
~1,o : #lp" Yl,o: Zlfl/(r~' Po = (a~)z/va2 + 2, b Â° : (Po-1)a~. 
(318.24) 
Again we apply Bayes" theorem recursively and immediately start with epoch k instead 
of epoch 1, i.e. with the prior distribution for il and z with the same form as (318.23) 
ilk' ZlYl ..... Yk-1 - NG(~k,k-1 'Vk,k-1 'bk-1 'Pk-1 )" 
(318.25) 
Bayes" theorem (211.1) together with the likelihood function from (318.20) leads to the 
posterior distribution 
il' 'C[Yl ..... Yk - NG(~Jk,k'Â¥k,k'bk'Pk ) 
(318.26) 
with 
~k,k = ~k,k-1 + Fk(Yk-Xk~k,k-1) 
(318.27) 
F k = Yk,k_lXÂ£(XkYk,k_lXl~+Pkl) "1 
(318.28) 
Vk, k = (I-FkXk)'dk,k_ 1 
(318.29) 
A 
Zk, k = O~Â¥k, k 
(318.30) 
A 
a~ = 2bk/(n+2Pk _1-2) 
(318.31) 
A 
A 
V(ai~) = 2(a~)2/(n+2Pk- 1-4) 
(318.32) 
, 
1 
bk= (2bk. l+(~k,k_l-~, k) Yk,k_l(~k,k_l-~k,k ) 
+ (Yk-Xk~k,k)'Pk(Yk-Xk~k,k))/2 
(318.33) 
Pk = (n+2Pk- 1)/2' 
(318.34) 
where Zk, k denotes the covariance matrix of the Bayes estimate ~k,k of il, ~ the 
Bayes estimate of epoch k for the variance factor a 2 and V(~) its variance. These re- 
sults follow from (224.10), (313.7) to (313.10), (313.16) and (313.17). 
To complete the recursive estimation, we still have to show the transformation of the 
posterior distribution p(il.l,Zlyl .... ,Yk_l ) for il-1 and ~ to the prior distribution 
(318.25) for il and "c. We obtain with (318.26) 
flk-l'~tYl ..... Yk-1 - NG(~k-l,k-l'Vk-l,k-l,bk_l,Pk_l)- 
Because of (A23.2) the distribution of ilk- 1 given "r follows with 
il-ll~'Yl ..... Yk-1 - N(~k-l,k-l'v-lVk-l,k-1 )" 
(318.35) 
We apply the transformation defined by the dynamic system (318.1) to ilk-1 and find 
with (318.19) correspondingly to the derivation leading to (318.13) 

98 
~l I:,Y 1 ..... Yk-1 - N(~k,k-l' z-lVk,k-1) 
(318.36) 
with 
~k,k- 1 = 0(k'k- 1)~k- 1,k- 1 
(318.37) 
and 
Vk,k-1 = ~(k'k-1)Vk-l,k-lO'(k'k-l) + ~-1" 
(318.38) 
Together with the marginal posterior distribution for "~ resulting with (A23.4) from 
(318.26) 
~[Yl ..... Yk-1 - G(bk-l'Pk-1) 
we obtain because of (A23.1) the prior distribution 
~'ZlYl ..... Yk-1 - NG(~k,k-l'Vk,k-l'bk-l'Pk-1 )" 
This distribution is identical with (318.25), so that we could immediately start from 
epoch k instead of epoch 1. 
Thus, the state vector ill< of epoch k is recursively estimated by __~k,k from (318.27) to- 
gether with (318.28), (318.29), (318.37) and (318.38). The covariance matrix ~-'k,k of the 
estimate 
,k follows from (318.30) with the estimate cr~ for epoch k of the variance 
A 
factor ~2 from (318.31). This estimate ~ is continuously updated by the observations Yk 
of each epoch, as can be seen from (318.33). The estimate ~k,k of the state vector ~ 
from (318.27) agrees with the estimate (318.11) of the Kalman-Bucy filter, while the co- 
variance matrix ~"k, k differs by the estimated variance factor. 
The dynamic system (318.1) together with the linear model (318.2) is also known as the 
dynamic linear model. It is used, particularly with 0(k+l ,k)=I, to forecast time de- 
pending phenomena (West and Harrison 1989). 

99 
32 
321 
Nonlinear Models 
Definition and Likelihood Function 
Nonlinear models in the standard statistical techniques are linearized by using approxi- 
mate values for the unknown parameters. The corrections to the approximate values be- 
come the unknown parameters and the solutions for the corrections are iterated, in order 
to improve the approximate values (Koch 1988a, p.185). 
For certain types of nonlinear models the exact least squares solution can be derived 
(Teunissen 1987, 1988). An example, which will be treated in the following section, is 
fitting a straight line to data points. However, no confidence intervals or hypothesis tests 
can be given for these least squares solutions. 
The Bayesian approach, on the other hand, allows statistical inference in nonlinear mod- 
els. This will be shown in the following. In general, however, numerical techniques will 
have to be applied to compute the estimates and the confidence regions of the unknown 
parameters or to test hypotheses. 
As for the linear model, we will assume that the expected values of the observations are 
defined as given functions of the unkown parameters and that the covariance matrix of 
the observations is assumed as known except for an unknown factor. But now we allow 
the observations to be nonlinear functions of the unknown parameters. Nevertheless, by 
introducing the normal distribution for the observations, the likelihood function of the 
observations is still specified. 
Definition: Let f(~)=(fi (~1)) be an nÃl given vector of nonlinear functions of the uÃl 
vector/~ of unknown random parameters, y an nxl random vector of observations, cr2 the 
unknown random variance factor or variance of unit weight and P the nxn positive defi- 
nite weight matrix, which is given. Let E(yl~l) and D(y I or2) be the expected value and 
the covariance matrix of the observation vector y under the condition that the unknown 
parameters/! and cr2 are given, then 
E(yl~ ) = f(~i) 
with D(yta2 ) = a2P -1 
is called a nonlinear model. 
(321.1) 
We will assume the normal distribution for the observation vector y under the condition 
that ~i and or2 are given 

100 
yl.&o-2 - N(f(~J),c72P-1). 
(321.2) 
The likelihood function, that is the density of the observations y given the parameters 
and 62, then follows with (A21.1) by 
p(ytt~,cr2) = (2g)'n/2(detcr2p-1)-I/2exp[ - ~ 
(y-f(~))'P(y-f(~))]. 
(321.3) 
Depending on the prior information we may use noninformative priors derived by 
(222.10) or informative priors. If knowledge is available about the expected values and 
the covariance matrix of the unknown parameters, the normal distribution for the un- 
known parameters ~ would be well suited for the prior distribution as in the case of the 
linear model. For the precision parameter v=-I/Â¢r2 the gamma distribution (A12.1) would 
be one choice. However, the resulting normal-gamma distribution will not be a conjugate 
prior, since the likelihood function in (224.10) is different from the likelihood function 
(321.3). Another choice for 1: would be the truncated normal distribution (223.8). In both 
cases the resulting posterior distribution for the unknown parameters is analytically not 
tractable, so that the numerical techniques discussed in Section 27 have to be applied for 
the statistical inference. 
322 Fit of a Straight Line 
As an application of the nonlinear model we will solve the problem of fitting a straight 
line to data points (x i 'Yi ) with ie { 1 .... n} in a plane. We will first consider as an in- 
troduction the following linear model, where the abscissae x i of the data points are as- 
sumed as known, while the ordinates Yi are independently measured with known vari- 
ances~2. Thus, 
E(Yi]~o'~l) = ~o + Xi~l 
and 
V(Yi) = ~2, C(Yi,Yj) = 0 
for 
i v j 
and 
i,je{1 ..... n}. 
(322.1) 
If the method of least squares is applied, the parameters/30 and/~1 are estimated such 
that the sum of the squares of the vertical distances of the data points (x i 'Yi ) to the 
straight line are minimized, see Fig. 322-1. 
If both the abscissae x i and the ordinates Yi of the data points are independently meas- 
ured with known variance 02, we obtain instead of (322.1) the model 
E(Yilflo'fll) = ~o + E(xi)fll 
for 
ie{1 ..... n} 

101 
ore t
~
 
J 
iYi, 
I 
x i 
Fig. 322-1 
and 
D([Yl ) = a2I with Y = (Yi) 
and x = (x i). 
By introducing the unknown coordinates gi with 
#i = E(xi ) or N= E(x) 
the model (322.2) is rewritten by 
E(IxYl ]~O,]31,/t) = [; I/~i1 j ~Oj 
with 
where 
e = [1,1 ..... 1]' 
(322.2) 
(322.3) 
D(IYxl ) = a2I, 
(322.4) 
In contrast to model (322.1), the model (322.4) is nonlinear because of the product ]31/L 
Thus, (322.4) is a special case of the nonlinear model (321.1). 
By assuming normally distributed observations, the likelihood function of the observa- 
tions follows with (321.3) by 
P(IYtll3o,flI,#) = (2~a2)-n/2exp[- ~a ([xYl-[; I]3ill~l)' 
( [Yxl. [; '/~1] ~o] ) ] 
or after omitting the constants 
p( [Y] [/3o,]31,#) ~ exp[- 
1 
((y-eflo-~q31) ' (y-et3o-/~ 1) + (x-/~)' (x-//))l. 
(322.5) 
From (322.5) it becomes obvious that the method of least squares or the maximum like- 
lihood method determines the unknown parameters in (322.4) by minimizing the sum of 

102 
squares of the perpendicular distances of the data points to the straight line, see 
Fig. 322-2. 
Y 
Ãi 
Fig. 322-2 
~X 
Prior information on the parameters 13o'131 and #i in (322.4) is assumed as given 
by means of the expected values and the variances. In addition the parameters are 
assumed as being independent, so that according to (223.6) the normal distribution 
is the appropriate distribution. We obtain with 
EOo) -- 
VOo) 
-- 
E(131) = #131' V(131) = V131 
E(#i) = #oi' V(#i) = Vgi 
(322.6) 
and with (A11.1) after omitting the constants the prior density for the parameters 13o'131 
and/2 
P (13o' 131' #) ~ exp[- (13o-#13o) 2/2V13o] ex p [- (j31 "#t31 )2/2V131 ] 
n 
II exp[- (#i-#oi)2/2V#i ]" 
(322.7) 
i=l 
With Bayes" theorem (211.1) and with (322.5) and (322.7) the posterior density function 
for the unknown parameters 13o' 13I and # follows by 
p(13o,131,#11Yx]) ~ exp{-Â½[(13o-#13o)2/V13 o + (131-#131)2/V131 
n 
n 
n 
+i~l (#i-~Â°i)2/V#i + i--1Z (yi-13o-#i131)2/o a +i=lZ (xi-#i)2/az] }. 
(322.8) 
The parameters 13o and 131' which determine the straight line, are of special interest. 
Their marginal distribution is obtained by integrating the parameters #i over their do- 
main, that is from -~ to ~o. The terms in (322.8) containing #i are 

103 
n 
exp{- Â½[iZl((#i-#oi)2/V#i 
+ (yi-/3o-#i/31)2/a 2 + (xi-#i)2/a2)]} 
n 
= exp{- ~[iXl((/3~/~2+l/Vpi+l/e2)#~-2((Yi-/3o)/31/~2+#oi/V#i+xi/a2)#i= 
+ (yi-/3o)2/~ 2 + ~i/Vgi+x~/a2)]} 
n 
(Yi -/3o)/31 / a2+#oi/V#i+xi/or2 )21 
II exp[- ~(/3~/cr2+l/Vpi+l/rr2)(#i 
i=l 
/~/(~2+1/V#i+1/~2 
exp[- ~( (Yi -/30)2/cr2+g02 i/V#i+x~/rr2 - ((Yi-/30)/31/Â¢r2+~0i/V#i+xi/or2) 2)]. 
/3~/crZ+l/Vgi+l/rr2 
The integration over #i from -~ to o~ leads with (All.2) after omitting the constant Â¢'2--ff 
to 
i!l(/3~/aZ+l/V#i+l/a2)- 1/2exp[ - Â½( (Yi-/3o)2/a2+Paoi/V#i+x~/ere 
( (Y i -/30)/31 / a2+#o i/V#i+x i/or2 ) 2 )]. 
/3~/Cr2+l/V#i+l/~72 
The marginal posterior density function for the unknown parameters/30 and/31 thus fol- 
lows with 
n 
p(/3o,/31[ 
) o, [ II (/3~/cr2+l/V/~i+l/cr2)-l/2]exp{- 
Â½[(/3o-#/3o)2/Vflo 
i=l 
n 
+ (/31-#/31)2/V/31 + Z ((yi-/3.)2/cr2+p~/V,.+x2/cr2 
i=l 
u 
~,, ~, 
, 
( (Yi -/3o)/31/rr2+#o i/Vpi+X i/(r2) 2) ] }. 
(322.9) 
/3~/(r2+1/V#i+l/cr2 
By integrating over/30 the marginal posterior density function for/31 is obtained. How- 
ever, the marginal distribution for 130 could not be analytically derived. The numerical 
techniques discussed in Section 27 should therefore be applied to (322.9) in order to 
compute the estimates and the confidence intervals for/30 and 131 or to test hypotheses. 
If the prior information given with (322.6) has large variances, so that terms with the 
variances in the denominator can be neglected, we obtain instead of (322.9) the posterior 
density function for/30 and/31 after omitting the constant o n 

104 
p(j3o,J31[ ix y]) ~ (13~+l)-n/2exp{ - 
1 
n 
[ Â£ ((yi-13o)2 + x~ 
i=l 
1 
((Yi-/~o)/~l+Xi)2 ) ] }. 
(322.10) 
Again numerical methods have to be applied for the statistical inference. 
Bayes estimates of the parameters ]3o and 81 shall be computed by means of the posteri- 
or density function (322.10) for an example. The estimates will be compared with the 
estimates obtained by applying the method of least squares to the nonlinear model 
(322.4) (Teunissen 1987). The same results as in Teunissen (1987) are found, if the prob- 
lem of minimizing the perpendicular distances of data points from a straight line is for- 
mulated by the so-called Hessian normal form of a straight line (Wolf 1968, p.419). For 
easy reference the derivation is given below. It should be pointed out that these methods 
only give the estimates for the unknown parameters, while the density functions (322.9) 
or (322.10) allow the estimates and additional statistical inference on the parameters. 
The Hessian normal form is given by 
d i = xisin Â¢ - YiCOSÂ¢ + z, 
(322.11) 
where d i denotes the perpendicular distance of the data point (x i 'Yi ) from the straight 
line, Â¢ the inclination angle of the line and z the perpendicular distance of the origin of 
the coordinate system from the line, see Fig. 322-3. 
We introduce the coordinates ~i 
n 
Ys=i~lY i/n of the points 
xi = xi'Xs" Yi = Yi-Ys 
'y 
Yi 
x i 
Fig. 322-3 
n 
and _Yi referred to the center Xs=lÂ£1xi/n.= and 
with 
n 
n 
Â£ ~i = O, 
Â£ Yi = O. 
(322.12) 
i=l 
i=l 

105 
By introducing the vectors 
d= (di), ~ = (~i), y= @i ) 
the sum d'd of the squares of the distances now follows from (322.11) with 
d'd = x'x sin2q + .y'y cos2~ - 2x'y sinÂ¢cosq + nz2, 
where z is the value for z with respect to the ~i' Yi coordinates. By setting the deriva- 
tives of d'd with respect to ~ and ~b equal to zero 
Od'd/c~ = 2nz = 0 
oM'd/O~ = 2x'x sin~cosÂ¢ - 2y'y sin~cos~ - 2~'~ cos2O = 0 
we obtain the estimates of ~ and 
A 
= 0 
(322.13) 
and 
^ 
tan2q~ = 2x'y/(x'x-y'y). 
(322.14) 
Starting from the identity 
tan2Â¢ = 2tanÂ¢/(1- tan2~) 
we find 
tanZÂ¢ + 2tanq}/tan2q~ - 1 = 0 
and therefore 
tanO= (- 1 + 41+tanZ2Â¢)/tan2~. 
Now we obtain instead of (322.14) 
^ 
)2)1/2] 
tanq~ = [~'~-~'~l:((y'y-x'x)2+4(x'y 
/2x'.y. 
(322. t5) 
^ 
With the estimate/31 of/31 
1 = tan~ 
(322.16) 
^ 
and z=0 from (322.13) we have 
A 
^ 
Yi-Ys = ~1 (xi -Xs) 
or 
^ 
^ 
~^ 
Yi = (Ys - /31Xs ) + 
lXi " 
This finally leads to the estimate 

106 
o = Ys - ~lXs" 
(322.17) 
gxan~le 1" For six points the abscissae x i and the ordinates Yi' given in the following 
table, 
x i 1..0!....1.97 3.00 ~i02 4.996.02 
Yi 0.48 1.01 1.49 
022i51 3.00 
have been independently measured with the given variance a~0.0003. Fitting a 
straight line through these data points by the method of least squares gives with 
(322.16) and (322.17) 
~o = - 0.0055 
and ~1 =0.5018. 
(322.18) 
The posterior density function (322.10) was then applied to compute the Bayes estimates 
v~nB and ~IB of the unknown parameters ]3 o and 131 of the straight line in model (322.4) 
and to determine the confidence interval for the unknown parameter ~o" To obtain this 
interval for /30' the marginal posterior density function for /3 Â° was computed from 
(322.10) by applying (273.2) with i=500 and j=500. This means, for each of the 500 
random numbers with uniform distributions generated for/3 o, a set of 500 random num- 
bers with uniform distributions was generated for /31, whose density values were 
summed according to (273.2). The density values were also multiplied by the random 
numbers for/31' in order to compute the Bayes estimate ~IB for E1 from (272.3). 
The marginal posterior density function for/30 was used to compute the Bayes estimate 
BoB of 
from 
and the confidence limits from 
Since the 
densi- 
(272.3) 
(272.6). 
marginal 
ty function is univariate, the random numbers generated for/3 o were ordered according 
to increasing values and along with them the density values. Then ten density values be- 
tween all consecutive pairs of density values were added by linear interpolation. Finally 
the confidence limits were computed from (272.6) together with (272.7) by starting from 
the density values of the minimum and maximum values for/30 and by checking numeri- 
cally, whether the inequality in (241.2) was fulfilled. 
The interval in (271.4) for the coordinate axis of/31' which defines the domain of the in- 
tegration, was determined in analogy to (272.1) by finding the density values, which 
cease to contribute to the integration. The interval for ]30 was set up such that the inter- 
val of the integration is ten to twenty per cent wider than the confidence interval for/30' 
which ensures that only those density values are neglected which do not contribute to the 
integration. 

107 
For each of the two following intervals defining the domain of the integration 
0.06 < /3 o < 0.05, 
0.45 < /31 < 0.54 
and 
0.05 < /3 o < 0.04, 
0.46 < ]3 t < 0.53 
the two sets of 500 times 500 random numbers were generated and ~oB' ~IB and the 
confidence limits for ]30 with the content O. 95 were computed. The results are presented 
(322.19). The Bayes estimates ~oB and ~IB in (322.19) agree very welt with 
in the table 
the least squares estimates in (322.18). 
-0.0045 
-0.0044 
-0.0059 
-0.0057 
Mean 
-0.0051 
0.5015 
0.5016 
0.5019 
0.5018 
0.5017 
Confidence Interval 
-0.039 < /3o < 0.027 
-0.041 < /30 < 0.029 
-0.039 < ]30 < 0.028 
-0.039 < ~o < 0.029 
-0.040 < ]30 < 0.028 
(322.19) 
was approximately computed 
A 
-0.0052 
- 0.0053 
- O. 0059 
- 0.0059 
Mean 
-0.0056 
Confidence Interval 
-0.021 < ]3o < 0.010 
-0.021 < ]30 < 0.010 
-0.021 < to < 0.010 
-0.021 < /30 < 0.010 
-0.021 < /3 Â° < 0.010 
(322.20) 
Again the Bayes estimate ~oB agrees very well with the results of (322.18) and (322.19). 
The confidence interval for 130 is smaller than that of (322.19). It can be explained by 
Furthermore, the marginal posterior density function of/5 o 
with (274.6) by introducing for ]31 the least squares estimate 91 from(322.18). For each 
of the two intervals defining the domain of the integration 
- 
0.04 < /3 Â° < 0.03 
and 
- 0.03 < ]3 o < 0.02 
two sets of 500 random numbers for/3 o were generated, giving the results (322.20). 

108 
the fact that the results of (322.20) are obtained with the parameter ]31 being fixed, while 
for the results of (322.19) ]31 varies. 
A 

109 
33 
Mixed Models 
331 Mixed Model of the Standard Statistical Techniques 
In the standard statistical techniques the mixed model is defined by (Koch 1988a, p.249) 
X~ + ZT= y 
with E(T) = 0 
and D(7) = a2I;77, 
(331.1) 
where X and Z denote the given coefficient matrices,/i the vector of unknown fixed pa- 
rameters, ~, the vector of unknown random parameters, y the random vector of observa- 
tions, a 2 the unknown variance factor and ~7~ / a given positive definite matrix. 
More important than model (331.1) is the following model, which is a special case of 
(331.1) (Koch 1988a, p.259), 
]ql+ZT= y+ e 
with 
E(7) = 0, E(e) = O, 
(331.2) 
I~(~,~ = a2~:~,~,, 0(e) = a2~ee, C(~,,e) = 0. 
In addition to (33t.1) we have the random vector e, which can be interpreted as error 
vector of y. The model (331.2) is also called the mixed model or the model of prediction 
and filtering, since the sum of the observation vector y and of the error vector e is repre- 
sented by the so-called systematic part or the trend X~ and the random part Z 7, which is 
called the signal. With the estimates ~ and ~ of the unknown parameters ~ and T we may 
filter the observations by computing X~h-ZT or predict the observations at points where 
no observations have been taken by forming X*~Z*; The matrices X* and Z* denote 
the appropriate coefficient matrices at the points of prediction. 
In geodesy, model (331.2) has become well known as the model of least squares colloca- 
tion (Moritz 1980). It has been successfully applied to interpolate observations or to 
combine different kinds of data for the determination of the parameters of the gravity 
field of the earth. If, in addition, coordinates of points at the surface of the earth are de- 
termined and if all available observations are used, one arrives at the concept of inte- 
grated geodesy (Eeg and Krarup 1973; Hein 1986). 
The main problem, when using model (331.2) for the concept of integrated geodesy, is 
the decision which unknown parameters to associate with the fixed parameters /1 and 
which with the random parameters T. The difference is important, since according to 
(331.2) the expected value of zero is assumed for ~. In general, coordinates of points 

110 
have been connected with ~ and parameters of the gravity field with ~ The choice might 
be reasonable for a special application, but it cannot be accepted as a general rule. 
While the well-known methods of the standard statistical techniques of testing hypothe- 
ses and computing confidence regions for the parameters of linear models can be applied 
to the fixed parameters fl of model (331.2), statistical inference for the random parame- 
ters ]Â¢is not well established. Using the approach of the standard statistical tests, hypoth- 
eses can be formulated which introduce additional information on the observations y in 
(331.2) (Wei 1987). Hypotheses may therefore be tested and confidence regions may be 
computed for the sum of the trend and the signal. 
In the next section we will give a Bayesian interpretation of the mixed model (331.2). 
Then the question of selecting the parameters according to the property fixed or random 
does not appear, it is replaced by the question, what prior information exists. In addition 
there is no difference in the statistical inference for either type of parameters in model 
(331.2) so that hypotheses for the parameters ]t may be tested. Hence, the mixed model 
is much more simply treated from the Bayesian point of view. 
332 Definition of the Mixed Model and Likelihood Function 
The unknown parameters of the Bayesian analysis are defined as random variables. But, 
as explained in Section 221, they may represent either fixed quantities or variable quan- 
rifles. Hence, there is no necessity to distinguish between a fixed parameter ~ and a ran- 
dom parameter ~ as is done in the mixed models of the standard statistical techniques. 
The Bayesian analysis distinguishes the parameters according to their prior information. 
Looking at model (331.2) under this aspect, it becomes obvious that the parameters 
and the parameters 7 differ from what is known about them a priori. No prior informa- 
tion for/J is available, while the prior information on 1Â¢ covers its expected value and its 
covariance matrix except for the unknown variance factor. Thus, we obtain the 
Definition: Let X be an nxu matrix and Z an nxr matrix of known coefficients with 
rankX=u, /~ a uxl vector and 7an rxl vector of unknown parameters, y an nxl vector of 
observations, e an nxl vector of errors of the observations with E(e)=O and O(e)=aZ~ee, 
where the variance factor 0"2 is unknown and the nxn matrix Zee is known and positive 
definite. Let prior information on the variance factor a2 and the parameter vector 7 be 
available by E(tr2)=a~, V(a2)=Va2 and by E(]Â¢)=O, D(],)=cr~Y.~,7, where the rxr matrix 
I:), 7 is known and positive definite, then 

111 
XIS+ZT=y+ 
e 
is called a mixed model 
(332. t) 
The expected value and the covariance matrix of the observation vector y under the con- 
dition that the unknown parameters ]1, Y and 62 are given follow with 
E(y[IS, 7) = X]I + Zâ¢ 
and o(yl/i,r, a2) = a2Zee, 
(332.2) 
where the result for the covariance matrix is derived with y=X/l+ZT-e by the law of error 
propagation (Koch 1988a, p.116). The relation (332.2) is an alternative formulation of 
the mixed model and is equivalent to the formulation (311.3) of the general linear mod- 
el. The mixed model (332.1) or (332.2) may therefore be treated as a special linear mod- 
el, as will be shown in the following sections. 
If we assume the observation vector y as being normally distributed, we obtain with 
(A21.1) and (A21.3) the likelihood function 
1 
,Z- 1 (y_X~_ZT) ] 
P(Y[li"LG2) = (2g)-n/2(deta2Zee)'l/2exp[- 
2-~ z(y-x4~-Z$) 
ee 
(332.3) 
333 Posterior Distributions 
There is no prior information available for the parameter vector ]i in (332.1), but prior 
information for 7 exists by the expected value and the covariance matrix and for t72 by 
the expected value and the variance. We therefore introduce with (224.20) and ~=l/cr2 
the normal-gamma distribution (A23.1) as prior distribution. For the parameter vector/1 
we choose the null vector as vector of expected values and the null matrix as weight ma- 
trix, thus expressing ignorance. After omitting the constants we find 
" 
o] 
P([~,~:) ~ 1:(u+r)/2+p-lexp[-2(2b+I7 
:] [: Ig-1 [ll 
7:1)]. 
(333.1) 
~,7 J 
This is an improper prior density function, since the matrix of the quadratic form is sin- 
gular, so that its determinant and the normalization constant for (333.1) are equal to 
zero. 
By applying Bayes' theorem (211.1) we obtain with the prior (333.1) and the likelihood 
function (332.3) the posterior distribution of/1, )'and "c 

112 
"r 
~- 
" 
p(E~,vIy) ~ ,'(u+r)/2+(n+2p)/2"lexp[- 7(2b+[], oOl I 0 
exp[- ~(y-[X,Z] 
) Eee(Y-[X,Z] 
)]. 
-1 
)] 
~'y 
7 
(333.2) 
We follow the derivation leading to (224.10) and obtain for the exponent of the expo- 
nential function 
2b + [i ~1[i 
2[i [Z'Eele ; l 
= 2b + y'Zel 
with 
â¢ -1 
+ y EeeY 
[/I- /I]e:] "Vo 1 [~]Â¢- /13Â¢oO]- [/I},:j 'Â¥o I [~:] 
iz,~el~x 
X'~e~e z 1 -~ 
Vo-- ,,~;lx Z,~elz+,~j 
i0ol 
ix ;] 
= VO ~;~-t 
ro 
[z ~ee 
Furthermore we have 
1 
I o] 
1 
1 
i,oi 
L~oJ 
lro 
This result finally leads to the posterior density 
P([~ ~:'Y)~ ,c(u+r)/2+(n+2p)/2-1exp[-"r(2b+,g'E:!~,+(y_[X,Z][~Â°])" 
, 
~ 
,, ;;,, 
7 o 
By comparing this density with (A23.1) we recognize that the posterior distribution for 
/t, ]Â¢and "c is gwen by the normal-gamma distribution 
,~IY - NG( 7o ,Vo,bo,Po) 
(333.3) 

113 
with 
[x, lx 
11 
]/'0 
t. 
ec 
b o = (2b+]/oZ~7o+(Y-X~o-ZTo)'Y.ele(Y-X~o-ZTo) )/2 
Po = (n+2p)/2 
and from (224.20) 
p = (~)2/Vcr 2 + 2, 
b = (p-1)cr~. 
The marginal posterior distribution for/3 and 7follows from (333.3) with (A23.3) by 
I~ 'y- 
t ( I~:] 'bÂ°go/PÂ°'2PÂ°)" 
(333.4) 
The Bayes estimates 
and 7B of the unknown parameter vectors/3 and 7 are computed 
with (231.5) and (A22.7) by 
^ 
= 
70 = gO[Z,~ee-1 
(333.5) 
A 
^ 
and the covariance matrix Z~, 7 of the estimates with (231.7) and (A22.7) by 
A 
~ 
~,]/ 
bo(Po_l) lgO. 
(333 6) 
^ 
We solve (333.5) for 7B and find 
^ 
'Y'B= (Z" Z] 1Z+Z: ! ) - 1Z'Ze le (Y- X~B)- 
By applying the first matrix identity of (313.9) we obtain 
^ 
, 
,+ 
"l(y.X~), 
TI3 = ZTyZ (ZZ77Z Zee) 
(333.7) 
which is identical with the estimate of Tin the mixed model (331.2) by the standard sta- 
^ 
tistical techniques (Koch 1988a, p.262). If we substitute the first solution for ~B in 
(333.5) and apply the second matrix identity of (313.9), we find 
,+ 
- IX) - 1 X, (ZZyÂ¢,+Zee) - ly, 
~B = (X'(ZZyTZ Y'ee ) 
(333.8) 
which again is identical with the estimate of/3 by the standard statistical techniques 
(Koch 1988a, p.263). 

114 
The Bayes estimate TB from (333.7) has a form similar to the Bayes estimate 
from 
(313.10). This is due to the fact that the mixed model is a special case of the linear mod- 
el, as already mentioned in connection with (332.2). 
Statistical inference on the individual parameter vectors ~i and 7 is solved by the margin- 
al distributions resulting from (333.4). We find with (A22.10) the marginal posterior dis- 
tribufion for ~ by 
[JiY t(~^,bo(X'Z-elX-X'ZeleZ(Z'Ze~Z+Z-~,],) "IZ'y'-lx'-I" 
~ 
u 
,, 
r/ 
ee ) 
/Po'2Po )" 
(333.9) 
Of special interest is the marginal posterior distribution of ]Â¢ 
71,- t(7o,bo(Z'ZeleZ+Z'l-z'Y'eleX(X'ZeteX)'lX'Ze~Z)'l/po,2Po ) 
(333.10) 
y~, 
All inferential tasks connected with the vector 7 of unknown parameters can be solved 
by this distribution. 
The marginal posterior distribution of the weight parameter v is obtained from (333.3) 
with (A23.4) by 
- G(bo,Po). 
(333.11) 
The posterior distribution for the variance factor G 2 therefore follows from (A13.1) with 
a2 - IG(bo,Po ) . 
(333.12) 
A 
Applying (231.5) and (231.7) gives with (A13.2) the Bayes estimate a~ of G 2 and its var- 
A 
lance V (G~) by 
A 
A 
= bo/(Po-1) 
and 
V(a~) = bU((Po-1)2(po-2)) 
(333.13) 
with b Â° and Po from (333.3). 
It was already mentioned in connection with (332.2) that the mixed model may be con- 
sidered as a special linear model. The posterior distribution (333.3) can therefore be di- 
rectly obtained by the results of the linear model. This will be shown in the following. 
We start from the mixed model (332.2) and introduce bars to differentiate it from the 
linear model (311.1) 
E(yl~l,7) = ~ 
+ Z1Â¢ with 
D(yt~l,7,a2 ) = a2Zee. 
(333.14) 
The matrix ~ee was assumed to be positive definite, so that we apply the Cholesky fac- 
torization Zeel=GG ', where G denotes a regular lower triangular matrix. With 
X= G'[X,Z], y = G' 7 
and 
11= [~',~']' 
(333.15) 
from (311.4) we obtain instead of (333.14) the linear model 

115 
E(yl~) = Xfl with 
D(ylcr2) = t#I, 
which is identical with (31 I. I). 
The prior density (333.1) for the parameters of the mixed model follows from the prior 
density (313.1) of the parameters of the linear model by the substitution 
11 
 333,6  
By substituting (333.15) and (333.16) in (313.3) we obtain for V o 
V o = (X'X+V-1) -1 
( 
~'[I(,Z] 
+ 
, 
X-1 
L "yyJ 
and the remaining parameters of the distribution (313.3) accordingly. By omitting the 
bars we find the distribution (333.3). 
334 Prediction and Filtering of Data 
As already mentioned in Section 331, the mixed model has been set up for the prediction 
and filtering of data. If we want to filter observations, we obtain from (332.1) (Koch 
1988a, p.260) 
yf = y + e = X~ + ZT, 
(334.1) 
where the vector yf contains the filtered observations. If we want to predict observa- 
tions, we have 
yp = X*/i + Z'y, 
(334.2) 
where X* and Z* denote the coefficient matrices needed to compute the trend and the 
signal for the predicted observations, which are labeled by yp. By comparing (334.1) and 
(334.2) with the alternative formulation (332.2) of the mixed model it becomes obvious 
that the observations yp are predicted as expected values. The distribution for this kind 
of observations in the linear model is given by (314.2). To derive from it the distribution 
for yf and yp, we solve yf=G%jf from (333.15) for yf and obtain yf=G"lyf, where 9f 
now denotes the vector of filtered observations in the mixed model, that is (334.1). 
Thus, with (314.2), (333.15), (333.16) and (A22.12) we obtain the distributions, where 

116 
the bars have been omitted, 
[X,Zl[~ly - 
and 
(334.3) 
[] 
r,"l 
Oo ,bo[X,,Z .]v Â° Lz*'j/Po'2Po ) 
[x*,z*] 
ly- t([x*,z*] Ito 
Of course, the same results can be found from (333.4) with (A22.12). 
(334.4) 
Instead of the data vector yp from (334.2), predicted as an expected value, the actual 
data vector Yu shall now be predicted by 
Yu = X*0 + Z* 7 - u, 
(334.5) 
where u denotes the vector of errors. The predictive distribution for Yu can be obtained 
from the predictive distribution (314.21), which was derived for the prediction of unob- 
served data in the linear model. With Yu=G ' "ly u from (333.15), where Yu now denotes 
the vector of predicted observations in the mixed model, that is (334.5), we find instead 
of (314.21) with (333.15), (333.16) and (A22.12), if the bars again are omitted, 
I~Â° 1 ,bo(Zee+[X*,Z*]go FX*'] 
YulY - t([X*,Z*] It Â° 
IZ,,J )/Po,2Po )" 
(334.6) 
The distribution (334.4) for yp and the distribution (334.6) for Yu agree except for the 
parameter which determines the covariance matrix. 
When using the mixed model (331.2) of the standard statistical techniques for the fil- 
tering or the prediction, in general not the coefficient matrix Z itself is given, but the fol- 
lowing products with Z are assumed as known 
Zss = ZlgyÂ¢' 
(334.7) 
and 
~ry = ~7~' 
with ~yr-- z~n" 
(334.s) 
The matrix aZZss is interpreted as the covariance matrix of the signal Z7 and a2ZTy as 
the covariance matrix of It and y. In addition, the matrix azI; 
with 
YY 
Z 
= E 
+ I; 
(334.9), 
yy 
s s 
ee 
is interpreted as the covariance matrix of the observations y. 

117 
We substitute (334.7) to (334.9) in (333.7) and (333.8) and obtain 
ix% 
 334 10) 
and 
7B = 7o = Z y 
(y- 
). 
(334.11) 
These are the well-known estimates of the least squares collocation. 
In order to substitute (334.7) to (334.9) in (333.9) and (333.10) we need the following 
inverses (Koch 1988a, p.40) 
_ 
c 
71 
= Z77 
7Â¢' (77Z ~'+X e )- 1Z~ , 
(334.12) 
Ill 
~ 
II 
and 
(Z,Z-e~Z+W !_Z,~X(X, 
XeleX)- IX,~eleZ)- I = (Z,Z:IZ+Z~,},)-I 
+ (Z'Z;~Z+X~I) - 1Z'Z- 1X(X' Z- 1X-X' Z- 1Z(Z' y7 IZ+Z- 1,) - lz' y,- lX) " 1 
ee 
ee 
ee 
ee 
77 
ee 
-1 
ee 
e~ 
rr 
(334.13) 
With the first inverse we find instead of (333.9) the marginal posterior distribution for ~1 
by 
flly- t(~ ,b (X'Z-Ix-x'z-I(z -Z Z-IE 
)z-lx)-l/po,2Po ) 
(334.14) 
o 
o 
ee 
ee 
~s 
ss yy ss 
ee 
By substituting (334.12) in (334.13), we find instead of (333.10) the marginal posterior 
distribution of 7 with 
7tY- t(y~,b [X .-Z Y.-1Z +(Z 
-X x-lx 
)Z-Ix(x'z-1X-X'Z-1 
o 
o 
77 ~3' YY Y7 
7Y 73t YY ss 
ee 
ee 
ee 
(Zss -Zss Wlxyy s s)Zele X) - 1X'~'- 1 
(Zee Y7 --Zss x'tZ'yy y~,') ]/Po' 2Po ) . 
(334.15) 
The covariance matrices D(~) and D(~) of the estimates of the parameter vectors ]~ and 
7in the model (331.2) of the standard statistical techniques are given by (Koch 1988a, 
p.252,260) 
D(~)= a2 (X'ZylyX) -1 
(334.16) 
and 
D(N 
= o-2(X_ ]~-1 x _]~ Z-1X(X'Z-tx)-Ix , 
7Y YY YY 73I YY 
YY 
7 ). 
(334 17) 
By comparing these covariance matrices with the covariance matrices for the Bayesian 
estimates ~B and ^ 7B, which follow with (231.7) and (A22.7) from (334.14) and (334.15), 
it becomes obvious that the covariance matrices disagree, although the estimates them- 
selves agree. 

118 
335 Special Model for Prediction and Filtering of Data 
We substitute in (332.1) 
s = Z~, 
(335.1) 
where the nxl vector s denotes the signal, so that the model is obtained 
Y41 + s = y + e. 
(335.2) 
The data y now directly contains the information on the signal s, while XJ6 again repre- 
sents the systematic part or the trend of the signal. Thus, (335.2) is a special model for 
the prediction and filtering of data. 
With (335.1) the posterior distribution (334.14) for ]1 does not change. However, instead 
of (334.15), we find with (334.7), (334.8), (334.11) and (A22.12) the posterior distribu- 
tion for the signal s 
sly- t(s~,b~tZoo-ZooZ~},Zoo+(Zoo:ZooZ,'!Zoo)z'lax(x'z21x-x'Z-! 
(Zss -Zss Z-1Zyy ss ) Z" 1X) - 1X" Ze le(Zs s-Zs sZ- 1Zs s) ]/Po' 2Po) 
'
e
e
 
Ys 
(335.3) 
A 
A 
where with SB=ZTB and So=Zy Â° 
'o -- 
Z-1 
,Z-1Z ~-1,, Z-1 .... 
_ ..... 
and with (333.3) because of ~o 7Y$O--(Y-X~o) yy yT~77]~Ty yytY'APO)-tY-APO) 
Z-1Z oZ- ! (y-X~) from (334.7), (334.8), (334.10) and (334.11) 
yy o~ yy 
v 
bo= (2b+(y-X~o)'Y.y~Y-ss~(y-X,Bo)+(y-X~o-s o)'Y.ele(Y-X~o-So ))/2 
= (n+2p)12, p = (~)21v 6 + 2, b = (p-l)a~. 
Po 
All inferential problems connected with the signal s can be solved by this distribution. 
We now derive the distribution of a predicted signal. We substitute in (334.2) 
s* = Z*~, 
(335.4) 
where the nxl vector s* denotes the predicted signal, and obtain the observations yp pre- 
dicted as expected values 
yp = X*~I+ s*. 
We introduce 
Zs*s* = Z*ZT~Z*" , Zs* s 
(335.5) 
* yTZ' 
* = z zTq/z*' 
(335.6) 
= zz 
and Zss 
where Zs*s* is interpreted as the covariance matrix of the predicted signal and Zs* s the 

119 
covariance matrix of the predicted and the original signal (Koch 1988a, p.261). With 
(335.4) and (335.6) we obtain instead of (334.15) the posterior distribution for the pre- 
dicted signal s* 
s*ly 
t( *. 
* *- 
* Z-Ix 
*+(g * -X * g-lx 
)Z'Ix(x'z-1X-X'Zâ¢ 1 
- 
So'bo[Zs s 
Zs s yy ss 
s s 
s s yy ss 
ee 
ee 
~ 
(Y- -Z Z-1Z 
)z-lx)-tx'x-I(x 
*-Z Z'Ix o*)]/Po,2Po), 
ss 
ss yy ss 
ee 
ee 
ss 
ss yy ~ 
(335.7) 
A 
A 
where with s~=Z*~, B and So=Z* "7o 
â¢ = Zs* sZyly(y-X~B ) 
~ 
= 
S O 
All inferential tasks connected with the predicted signal s* are solved by this distribu- 
tion. 
Example 1: Temperatures T* shall be predicted as expected values at given times t* 
J 
J 
with je{1 ..... r} by means of the temperatures T i measured at given times t i with 
ie{t ..... n}. Let the errors of the measurements be independent and have variances 
which equal the variance factor or2. Let the prior information on if2 be given by 
E(cr2) = cr~ and 
V(cr2) = Vcr 2. 
(335.8) 
Let the systematic part or the trend be represented by a polynomial of degree m de- 
pendent on t i and let with Zss=(Crik ) the elements crik of the covariance matrix Zss of 
the signal be given by the covariance function or( t i" tk) 
(t i- tk)2 
Crik = a(ti-t k) = ab 
, 
(335.9) 
where t i and t k refer to the times of the measurements and a and b are given constants. 
The covariance matrix Zs*s=(~ *i ) of the predicted signal and the signal and the covar- 
lance matrix Zs*s*=(aj*l*) of the predicted signal are given by the same covariance 
function with the appropriate times t .* 
J, t~ for the predicted signals and the time t i for 
the measurements, thus 
(t*.- ti)2 
crj* i = cr(t*.-t i) = ab 
J 
(335 10) 
j 
and 
* 
* 
2 
(t~-t 1) 
= 
*- * 
ab 
J 
" 
â¢ 
(335 11) 
aj*l* 
a(tj tl)= 
In addition to the predicted temperature T*. the 95 percent confidence interval of the pre- 
J 
dicted signal contained besides the trend in T*. shall be determined. 
J 

120 
The temperatures T*. are predicted as expected values by (335.5) with the Bayes esti- 
J 
mates ~B of the parameters ]1 of the trend from (334.10) 
~B = ~1o = (X'z':lx)-lx'K:lY' 
yy 
y v 
where the matrix X is formed by the coefficients of the polynomials. With 
X= [x 1 ,. .., Xn] ' , where x:l denotes a row of X, we have 
x' = [1,ti,tl ~ .... 
t m] 
with 
ie{1 
..,n}. 
The matrix Z 
follows from (334.9) by 
YY 
~yy = Zss + Y'ee 
with Zss from (335.9) and Zee from 
~. 
=I, 
ee 
since the errors are assumed as independent with equal variances a 2. Finally we have 
y = (Ti) 
with 
ie{1 ..... n}. 
^ 
A 
The Bayes estimate s B of the signal and the estimate s~ of the predicted signal are ob- 
tained from (335.3) and (335.7) by 
^ 
SB = So = Zss 
( y" 
) 
and 
,- 
s o 
A 
A 
with Zs* s from (335.10). The predicted temperatures yp=(T~) with j e{1 ..... r) follow 
from (335.5) by 
^ 
^ 
yp = X* 
+ s~. 
(335.12) 
, 
t 
With X*=[x~ ..... Xrl , where x~" denotes a row of X*, we have 
x~' = [1,t*. t~ 2, 
.,t~ m] 
with 
j~{1 
.. r}. 
The confidence interval of the predicted signal s*.j with s*=(s~) follows from the poste- 
rior distribution of s*. It is obtained as the marginal distribution of (335.7). Thus, with 
J 
(A22.10) and *- 
* 
s o- ( s j o ) we obtain the posterior distribution 
s~ly- 
t(S~o,1/f,2Po), 
(335.13) 
where 
1/f = bo(Zs*s*-Zs*sZylyZss*+(Zs*s-Z * ~!,Zo.)Z:Ix(x'z:Ix-x'z: 
1 
S 
S 
yy 
~ 
~ 
~,t. 
~, 
-Z 
Z'Iz 
)Z'Ix)-lx'Z'le(Z s*-Z sZ'!Z 
*))jj/Po 
(Zss 
ss yy ss 
ee 
e 
s 
s 
yy ss 

121 
and from (335.3) 
, 
-1 
bo= (2b+(y-X~o)'EylyEssZyly(y-X~o)+(y-X~o-S o) ~ee(Y-Xl~o-So))/2 
= (n+2p)/2, 
p = (a~)2/Vo. 2 + Z, 
b = (p-l)a~. 
Po 
The distribution for s* is the univariate t-distribution, whose density function is given in 
3 
(A22.4). We apply the transformation (A22.5), to obtain a random variable having Stu- 
dent's t-distribution, so that tables of the t-distribution can be used. Hence, with 
A 
S* -S* 
jB- jo and (A22.6) 
A 
Vrf(sj SjB) - t(2Po). 
(335 14) 
By introducing the quantity t 1 -a; 2Po' which was defined by (312.34) and which can be 
taken from a table of the t-distribution, we have 
A 
-/VCS*-S* ~ 
) = 1-a 
P('tl.~;2Po < w~ j 
jB j < tl.~;2p Â° 
and obtain the confidence interval for s* by 
J 
A 
A 
P(s*~-tt 
/~f < s* < s*~,+t. 
~ 
/qrf) __ 1-c~. 
(335.15) 
J~ 
~-~;2p o 
J 
J~ 
l-a;zp o 
A 
A 
Finally the Bayes estimate ~ of the variance factor if2 and its variance V(~i~ ) follow 
from (333.13) by 
~i~ = bo/(Po-1) 
and V(~) = bo2/((Po-1)2(Po-2)) 
with b Â° and Po from (335.13). 
A 

122 
34 Linear Models with Unknown Variance and Covariance 
Components 
341 Definition and Likelihood Function 
We will now introduce the model which is equivalent to the Gauss-Markoff model with 
unknown variance and covariance components of the standard statistical techniques 
(Koch 1988a, p.264). In contrast to this model, where the unknown variance and covari- 
ance components are fixed quantities, the model for the Bayesian analysis introduces the 
unknown parameters as random quantities. Hence, the expected values of the observa- 
tions and their covariance matrix have to be defined under the condition that the un- 
known parameters take on fixed values. 
Definition: Let X be an nxu matrix of given coefficients, fl a uxl vector of unknown ran- 
dom parameters and y an nxl random vector of observations. Let cr~ and 6ij be k un- 
known random parameters with ie{1 ..... 1}, i<j<l and 1_<~1(1+1)/2, which are 
called variance and covariance components, and let these k components be collected in 
the kxl vector a. Let E(yJfl ) and D(yJa) be the expected value and the covariance ma- 
trix of the observation vector y under the condition that the unknown parameter vectors 
/3 and a are given. Then 
E(yj/3) = ~ 
with D(ylo ) = E= cr~V 1 + 612V 2 + ... + 6~Â¥ k 
is called the linear model with unknown variance and covariance components, where the 
nxn covariance matrix E is positive definite and where the nxn matrices ~n with 
me { 1 ..... k} are known and symmetrical. 
(341.1) 
The model with unknown variance and covariance components can be explained by the 
mixed model (331.1) with Z~Zl~,I+...+ZI~, 1. If the parameter vectors 7i are unknown 
and unobservable and if their covariances C(Ti ,~j)=6ijRij are given except for the 
components 6ij' then the covariance matrix for the observations y adopts the structure 
of (341.1) (Koch 1988a, p.264). 
The mixed model is also used for a Bayesian analysis of variance components (Broeme- 
ling 1985, p.143). Such a model contains in addition to the variance components the pa- 
rameter vectors 7' i as unknown parameters and the variance components enter the ana- 
lysis by means of the prior distributions. The model (341.1) avoids the additional param- 
eter vectors ~i' so that it is preferred here. 

123 
As before~ we will assume the normal distribution for the observation vector y in (341.1) 
under the condition that the unknown parameter vectors j~ and ~r are given 
yl/I,a- N(XtI, Z), 
(341.2) 
where Z according to (341.1) is a function of ft. The likelihood function, that is the den- 
sity function of the observations y given the parameters ~ and or, then follows from 
(A21.1) with 
p(yl/],a) = (2rc)-n/Z(detZ)'l/2exp[- Â½(Y-X~)'Z-I(y-X~)]. 
(341.3) 
We are only interested in the parameter vector a. A noninformative prior is therefore in- 
troduced for the parameter vector ~ which for a linear model means a constant accord- 
ing to (312.2). Hence, the vector ~ may be directly integrated out of the density function 
(341.3). With the sufficient statistic ~ for ]] from (224.4) in connection with (311.4) 
= (X'Z- lx)- IX'Z" ly 
(341.4) 
the exponent in (341.3) is transformed using the identity 
(y-X~)'Z-I(y-X]]) 
= (y-X~-X(O-~))'Z-I(y-X~I-X(]$-~)) 
= (y-X~)'Z-1 (y-X~)+(~-~)'X'Z" lX(~-~) 
because of 
(~-~)" (X'Z- ly-X'Z" 1X~) = 0. 
The integration with respect to ~ gives because of (A21.2) 
~... 
~ exp[- ~(fl-~)'X'Z-IX(~I-~)]d~l= (270u/2(detX'Z-IX)-1/2 
With 
(y~X~)'Z-I(y-X~) = y'Wy 
(341.5) 
where 
W = Z'I-z-Ix(x'z-Ix)-Ix'z-1 
(341.6) 
we finally obtain after omitting the constants instead of (341.3) the likelihood 
function 
p(yta) ~ (detZ detX'z-lx)-l/2exp(-y'Wy/2), 
(341.7) 
which is only depending on the unknown parameter vector o. 
The likelihood function (341.7) is very well suited for the Bayesian analysis of variance 
and covariance components out of the following reasons. By a linear transformation of 
the observation vector y we may factor the likelihood function (341.3) into two products 

124 
L 1 and L 2, where L 1 is identical with the likelihood function (341.7) and therefore is 
only dependent on the parameter vector a (Koch 1987). L 2 depends on ~1 and o-and leads 
in a maximum likelihood estimate to the well-known estimator (341.4), if a is assumed 
as known. Using (341.7) in a maximum likelihood estimate gives the following estimator 
^ 
of a (Koch 1986). Instead of awe estimate ~by awith 
^ 
= ~ -1~, 
(341.8) 
where 
~= [ .... ~' .... ~ij ' ""]' 
' 
a= [ .... 
~:~1 1 ..... 
aijaij 
'- 
""]' 
S= (tr~/iW~/j) 
for 
i,je{1 .... k}, 
q= (y'~/iWy) 
for 
ie{1 ..... k} 
V m = Vm/a ~ or 
V m = Vm/aij, 
W = l;ol-I:olX(X'ZolX)-lX'Zo 1 
k 
~o m=Â£1Vm" 
Thus, for the estimation, approximate values a~ and a i j are split off the variance com- 
ponents erla and c7 ij to obtain ~ and ~i j' which can be assumed as having values close 
to one. The approximate values are absorbed in the matrices V m, which then become ~/m" 
The estimator (341.8) is not only a maximum likelihood estimator, but also a best in- 
variant quadratic unbiased estimator (Koch 1988a, p.270). It is generally applied when 
estimating variance and covariance components for geodetic applications. Hence, when 
introducing the likelihood function (341.7) for a Bayesian analysis, the estimate (232.4) 
will give approximately the same results as (341.8), if the prior information is nonin- 
formative or is available with large variances only. 
342 Noninformative Priors 
We will assume a noninformative prior density function for the unknown parameter vec- 
tor a and apply Jeffrey's invariance principle to derive it. Thus, according to (222.11) 
we take the natural logarithm of the likelihood function (341.7) and obtain after omitting 
the constant 
lnp(y[a) ~- 
lndetI:- lndetX'y.'lx- y'Wy. 
(342.1) 
For the differentiation of (342.1) with respect to the components of cr we apply the fol- 
lowing two rules (see, for instance, Kubik 1970). Let A be a regular symmetric mxrn 

125 
matrix, which is a function of the vector a with a=(a i ), then 
01ndetM0a i = trA-10A/0ai 
(342.2) 
and 
0A- 1/0ai = _ A- 10A/0ai A- 1 
(342.3) 
To prove (342.2) we use A=Â¥AY" with Y'Â¥=I, where A is the diagonal matrix of the 
eigenvalues Xi of A and Y the matrix of the eigenvectors (Koch 1988a, p.53). We find 
m 
m 
lndetA = lndetA = lnjlTt)~ 
j.= 
=j_~lln_ ,~j 
and 
m 
01ndetA/0a i = y~ 2"10,~j/0ai = trA-10A/0ai 
j=l j 
In addition, we obtain from A=YAÂ¥" 
O^/Oa i = 0Y/0aiAÂ¥" + YOA/0aiY' + YA0Â¥'/Oa i 
and from A- I=YA- 1g, 
trA-10M0ai = trY'0Y/Oa i + trA-10M0ai + trgo-Y'/0a i. 
Finally o~' / 0a i Y+Y' o'~//Oai =0 because of Y' Â¥= I, so that we find 
trA-10A/0ai = trA- 10A/0ai, 
which gives (342.2). 
From AA-1=I we obtain 
0A/0ai A'I + AOA-1/Oai = 0 
and 
0A-!/Oa i = _ A-1OA/0ai A-t, 
which is identical with (342.3). 
By applying (342.2) and (342.3) to the derivative of (342.1) we find with ff=(a i ) 
Olnp(yla)/Oa i ~ _ trz-loz/Oai 
+ tr(X'z-lx)-Ix'z-loz/Oaiz-lx 
y" [ -Z- 10Z/0~riZ- l+Z- lc5z/06i Y.- 1X(X'Z- 1X)" 1X'Z- 1 
- Z- 1X(X'Z- iX) - 1X'Z- lo~Z/0ffiZ- 1X(X'Z- 1X) - 1X'Z- 1 
+ Z- 1X(X'Z- 1X)- 1X'Z- t0z/0oiZ- 1]y = _ trW0Z/06 i + y,W0Z/0criWY ' 

126 
where 
3~/&r i --V i 
with 
ie{1 ..... k} 
because of (341.1). Thus, 
31np(Yla)/O~ i ~ .trlP/ i + y'~g~ilgy 
and 
o~/0cr i =. lg~'iW. 
The second derivative follows with 
o~lnp(ylo)/OaiO~ j ~ trlggjW i - 2y'lD/j~/iYy. 
Now we have to take the expected value of this derivative and obtain with (341.1) 
F.(yy" IF, a) = r. + xt)l)'x' 
(Koch 1988a, p. 114) and therefore 
E(y'lg~j~iWY ) = trWVj~iWE(yy' ) = tr~jW i 
because of 
W=lg~ 
and WX=O. 
Thus, we finally obtain with (222.10) the noninformative density function p(a) of the 
parameter vector a by 
p(o) ~ (detS) 1/2, 
(342.4) 
where 
S = (tr~rilP/j) 
for 
i,ja{1 ..... k}. 
With the prior density (342.4) and the likelihood function (341.7) the posterior density 
p(oly) of the variance and covariance components Crl~ and aij contained in a follows 
with Bayes" theorem (211.1) from 
p(a[ y) ~ (de tS) 1/2(d e tY. de tX'Y." lx) "1/2ex p ( -y'Wy/2). 
(342.5) 
This distribution depends through the matrix E and the matrices S and 9/on the unknown 
variance and covariance components (rl~ and crij in ~. By integrating over the domain of 
a, the normalization factor is determined according to (211.5). Unfortunately, this inte- 
gration could not be solved analytically. The same holds true for the integration (231.5), 
^ 
which gives the Bayes estimate o B for the variance and covariance components in a, and 
the integration leading to the confidence region (24t.2) or the hypothesis test for the var- 
iance and covariance components. Hence, the numerical methods of Section 27 have to 
be applied, to compute the Bayes estimates for the variance and covariance components, 

127 
their confidence regions, and to test hypotheses. 
Example 1: The length s of a straight line has been measured by two instruments such, 
that two independent sets of observations are obtained. The observations of the first set 
are collected in the nlXl vector Yl and of the second set in the n2xl vector Y2" The ob- 
servations in Yl are also independent and have equal weights Pl' the observations in Y2 
are independent, too, and have equal weights P2" The covariance matrix of y= [yl, y~] ', 
which is a diagonal matrix, shall be expressed by the variance component ai times the 
inverse 1/Pl of the weights for Yl and by tr~ times l/P2 for Y2" The variance compo- 
nents tri and a~ are the unknown parameters, whose Bayes estimates and whose 95 per 
cent confidence intervals have to be computed. 
From (341.1) we obtain the following model for the example 
E(ylfl ) = Xfl with 
D(yttr ) = â¢ = tr~V 1 + tr~V2, 
(342.6) 
where 
Iy 
l], 
fl = s 
Y= 
2 
E Â°1] 
X= 
e2 , e 1 = [1,1 ..... 1]', e 2 = [1,1 ..... 1]' 
Vl = (1/Pl) 
1 
' 1/2 = (l/P2) 
â¢ 
â¢ 
In2 
The vectors e I and e 2 are nlXl and n2xl vectors and Inl and In2 are nlxn 1 and n2xn 2 
identity matrices, respectively. 
The posterior den:sky function (342.5) will be applied for the statistical inference on the 
variance components tr~ and tr~ of the model (342.6). To compute the density values for 
a~ and try, we need the following matrices 
Ig = (cr~/pl) 
1 
+ (~/p2) 
_ 
In2 
E- 1 = I(pl/a~)Inl 
0 
L 
o 
(P2/a~)In2 
X'E-1X = nlPl/a ~ + n2P2/~ ~ 
-(Pl/Â°'~ ) eli 
, 
Y.-Ix= 
(p2/~r~)e 2 

128 
Z" IX(X'Z- ix) - IX'Z- 1 
[ (pl/r~) 2ele~ , 
(p lp 2 / o-~ o'~) eze 1 
=(nlP 1/o~+n2p2/o~)- 1 
(PlP2/O'~ ff~) el el , 
2 
' 
(P2/O'~) e2e 2 
â¢ : [:21: 
:1.21 
= 
_ 
2 
â¢ 
/~+n2P2/Cr~) 
Wll 
(Pl/e~) Inl 
(Pl/~) elel/(nlP 1 
W12 =- (plP2/C~)ele~/(nlPl/Cr~+n2P2/~) 
W22 = (p2/~Y~)In2 - (p2/Â¢y~)2e2e~/(nlPl/Â¢Y~+n2P2/C*~) 
-Wll/Pl 
~l 
[~ WI2/P2 
WÂ¢I = W21/Pl 
' 
WV2= 
W22/P2 
The elements of the matrix S follow from (342.4) with 
S = (sij) = (trWVi~W j) 
for 
i,je{1,2}, 
where 
Sll = nl/(ff~)2 - 2nlPl/(Â¢y~)3(nlPl/Cr~+n2P2/Cy~) 
+ [nlPl/(~)2(nlPl/~+n2P2/O~)]2 
Sl 2 = s21 = nln2PlP2/[cr~(nlPl/~+n2P2/O~)]2 
s22 = n2/((y~)2 - 2n2P2/(~)3(nlPl/Cr~+n2P2/ff~) 
+ [n2P2/((y~)2(nlP 1/o-~+n2p2/o~ ) ]2. 
The determinants of S, Z and X'x- IX are computed by 
detS = SllS22 - s~2 
n 1 
n 2 
detZ = (o~/pl) 
(o~/p2) 
detX'y.-1X = nlPl/ff ~ + n2P2/ff ~. 
(342.7) 
A 
The quadratic form y'Wy follows with (341.5) by means of the vector e=X~-y of the te- 
A 
siduals. We obtain with (341.4), yl=(Yi t ) and y2=(Yi2 ) the estimate s of the length s 
by 

129 
n 1 
A 
s = (nlPl/6~+n2P2/~)-I[(pl/~)--- 
Â£ Yil 
i=l 
A 
and the vector e of the residuals by 
n 2 
+ (P2/6~)i~lYi2] 
[Ol]  ,[;1 
~- 
S 
- 
. 
e 
e2 
The quadratic form of the residuals therefore follows with 
nl ^ 
n2 ^ 
y'Wy = (Pl/Cr~) Y, (s-Yil)2 + (p2/(r~) Y, (s-Yi2)2. 
(342.8) 
i=l 
i=l 
With the determinants (342.7) and the quadratic form (342.8) of the residuals all quanti- 
fies are available, to compute the density values of the posterior distribution for cr~ and 
cr~ from (342.5). 
In addition, the variance components shall be estimated by (341.8), We have 
Vl = (Â°g~/Pl)I~nl 
~1' v2 = (a~/P2)[ 00 
IOn2 l 
and 
Z = (a~/pl)[~nl 
~] + (a~/p2)[ 2 
0 ] 
o 
in 2 â¢ 
The elements of the matrix W therefore follow from the elements of the matrix W by re- 
^ 
placing 6~ and a~ with a~ and a~. The same applies for the estimate s. Furthermore, we 
have 
~1 = 
c~Wll/Pl 
~} 
I~ c~W12/P: 
a~W21/pl 
' 
~2 = 
a~W22/P: 
The elements of the matrix S are obtained with g=(sij ) by 
Sll = n 1 - 2nlPl/a~(nlPl/~+n2PZ/a~) + [nlPl/a~(nlPl/a~+nZpz/a~)]2 
s12 = s21 = nln2PlP2/C~c~(nlPl/a~+n2P2/a~)2 
s22 = n 2 - 2n2P2/a~(nlPl/~+n2P2/a22) + [n2P2/a~(nlPl/a~+n2P2/a~)]2. 
(342.9) 
The vector Tin (341.8) is computed from(Koch 1988a, p.269) 
^ 
1 
t^ 
= (qi) = (e o ViÂ£o e) 
for 
i~{1 21 

130 
with 
n 1 
n 2 
A 
A 
ql = (Pl/a~) y' (s-Yil)2' 
q2 = (P2/a~) y' (s-Yi2)2" 
(342.10) 
i=l 
i=1 
With the elements of ~ and q the estimates of the variance components are obtained 
from (341.8). 
The following measurements in units of millimeters have been taken 
F8 264.1 
[i  668 
18 267. ! 
8 262.| 
= 
= 
264.. 
Yl 
8 264. 
Y2 
!8 268. I 
267. 
18 263. ! 
269.J 
18 265._I 
The weights Pl and P2 of the two sets of measurements are given in units of 1/millime- 
ter2 by 
Pl = 0.250, 
P2 = 0.111. 
By starting with 
-- 0.8, 
-- 0.8 
the estimation (341.8) together with (342.9) and (342.10) has been iteratively applied 
A 
^ 
A 
^ 
with choosing a~=a~ and a~=a~ as new approximate values. After five iterations we 
obtain ~= 1.0000 and ~= 1.0000 and the iterated estimates of cr~ and 62 
A 
A 
a~ = 0.777, 
a~ = 0.691 
(342.11) 
A 
A 
with the standard deviations of the estimates a~ and e~ (Koch 1988a, p.273) 
A 
A 
= 
A 
A 
= 
(V(~r~)) 1/2 
0.45, 
(V(Â¢r~)) 1/2 
0.46. 
(342.12) 
The posterior density function (342.5) together with (342.7) and (342.8) was then used to 
compute the Bayes estimate of the variance component cr] and to obtain its confidence 
region. First the marginal posterior density function for a~ was computed from (342.5) 
by applying (273.2) with i=500 and j =500. Thus, for each of the 500 random numbers 
with uniform distributions generated for cry, 500 random numbers with uniform distribu- 
tions were generated for cry, whose density values were summed according to (273.2). 
The marginal posterior density values for cr~ were then used to compute the Bayes esti- 
A 
mate CriB of cr~ from (272.3) and the confidence interval for ~ from (272.6). Since the 
marginal posterior density function is univariate, the generated values for ~ 
were 

131 
ordered according to increasing values and along with them the density values. Then ten 
density values between all consecutive pairs of density values were added by linear in- 
terpolation and the confidence limits were computed from (272.6) together with (272.7) 
by starting from the density values of the minimum and maximum values for cr E and by 
numerically checking whether the inequality in (241.2) was fulfilled. 
The interval in (271.4) for the coordinate axis of fiE' which defines the domain of the in- 
tegration, was set up such that the interval of the integration is ten to twenty per cent 
wider than the confidence interval of cry. This ensures that in analogy to (272.1) only the 
density values are neglected, which cease to contribute to the integration. By computing 
the confidence interval for cr~ as described below, the limits of the integration for ~ 
were deterrained. 
For each of the two following intervals defining the domain of the integration 
0.05 < ~ < 2.4, 
0.05 < (r~ < 2.4 
and 
0.05 < cr~ < 2.2, 
0.05 < 0- 2 < 2.2 
(342.13) 
^ 
the two sets of 500 times 500 random numbers were generated and ~B and the confi- 
dence limits for a T were computed. The results are given in the table (342.14). The con- 
A 
fidence interval for aE is not symmetrical with respect to the Bayes estimate O~B, since 
the marginal posterior density function for cr~ is not symmetrical. Its maximum lies in 
the vicinity of the iterated estimate (342.11), which results from a maximum likelihood 
estimate based on the likelihood function (341.7). The interval [0.33, 1.23] defined by 
0.94 
0.94 
0.92 
0.91 
Mean 
0.93 
Confidence Interval 
0.25 < cr~ < 1.91 
0.25 < cr~ < 1.95 
0.26 < cr~ < 1.83 
0.26 < a~ < 1.88 
0.26 < a~ < 1.89 
(342.14) 
the estimate (342.11) of cr E and its standard deviation (342.12) lies within the confidence 
interval. 
The same procedure as described for the variance component ff~ was then applied to 
compute the Bayes estimate and the confidence interval for the variance component cry. 
The results for the two intervals (342.13) are presented in the table (342.15). Due to the 
different weights for the observation vectors Yl and 3'2 and due to the fact that the obser- 

132 
/k % 
0.86 
0.87 
0.85 
0.86 
Mean 
0.86 
Confidence Interval 
0.19 < cr~ < 1.97 
0.20 < ~ < 1.87 
0.20 < cr~ < 1.85 
0.20 < ~ < 1.84 
0.20 < cr~ < 1.88 
(342.15) 
vation vector Y2' for which a~ was introduced, contains only two observations less than 
Yl' the results of the table (342.15) are very similar to those of (342.14). 
A 
343 Informative Priors 
We will now assume informative priors for the unknown variance and covariance com- 
ponents a~ and aij. The prior density functions shall be determined by the given ex- 
pected values and variances of the k variance and covariance components, that is by 
E(a~) =#ai 
and V(a~)=Vai 
for 
ie{1 ..... 1} 
E(aii), = /~aij 
and V(aii ) = Vai i 
for 
i < j < 1, 
1 <k < 1(I+1)/2. 
J 
(343.1) 
Like the variance factor a 2 of the linear model (311.1), the variance components a~ take 
on positive values only. In the linear model the gamma distribution (A12.1) via the nor- 
real-gamma distribution (A23.1) is used as prior for the weight parameter "r=l/a 2. This 
corresponds to the inverted gamma distribution (A13.1) for 62. Thus, the gamma distri- 
bution would be the appropriate prior distribution for 1/o~ 1. This distribution has been 
used as prior for the Bayesian analysis of variance components in the mixed model 
(Broemeling 1985, p.146; Bunke and Bunke 1986, p.473), while the inverted gamma dis- 
tribution (A13.1) of the variance components a~ was applied for the Bayesian analysis in 
the model (341.1) (Koch 1988b). By assuming the 1 variance components ~1 ~ as being 
independent, the prior p(cr~l) 1 for the 1 components a~ is obtained from the inverted 
gamma distribution (A13.1) with neglecting the constants by 
1 
p.+l 
p(a~l) 1 ~ II (1/63) 1 exp(_bi/a~). 
(343.2) 
i=l 
The product is taken over the 1 variance components cry, and b i and Pi are the 
parameters of the distributions for the individual variance components. They follow with 
(343.1) from (A13.2) with 

133 
Pi = #gi/vai + 2, 
b i = (Pi-1)gai. 
(343.3) 
Provided Vai<#g i holds, then a very meaningful choice for the prior distribution of the 
variance components cr21 is the truncated normal distribution (223.8), which is based on 
the maximum entropy principle. By again assuming the 1 variance components a21 as be- 
ing independent, we obtain the prior p(a21)1 for the 1 components a21 from (223.8) with 
neglecting the constants by 
1 
H exp(-kliaT-k2i(aT-gai )2) 
for 
k2i > O, Vai < #~i" 
(343.4) 
ptaT) 1 
i=l 
The product is taken over the 1 variance components aT, and k 1 i and k2i are the param- 
eters of the truncated normal distributions for the individual variance components. To 
determine these parameters numerically, the results (223.19) to (223.22) may be used. 
The covariance components aij take on positive and negative values. Because of 
(223.6) and (343.1) the appropriate choice for the prior is therefore the normal distribu- 
tion (A11.1). By assuming the k-1 covariance components ffi j as being independent we 
find the prior p(aij )kl for the k-1 components aij with (All.3) after neglecting the 
constants by 
k-1 
p(aij)kl ~ H exp(- 
(343.5) 
1 
(aij -#aij )2/2Vaij )Â° 
The product is taken over the k- 1 covariance components a i j. 
By applying the likelihood function (341.7) B ayes' theorem (211.1) now gives the pos- 
terior distribution p (It t y) of the vector a of the variance and covariance components a21 
and aij. If we use the inverted gamma distribution as prior for the variance components 
a T, we find with (343.2) and (343.5) 
1 (1/al~)Pi+l 
k-1 
p(a]y) ~, II 
exp(-b i/aT) H exp(- (aij-/~aij)2/2Vaij) 
i=l 
1 
(de tY. de tX'E" IX) "1/2exp(_y,Wy/2) 
(343.6) 
with b i and Pi from (343.3). If we introduce the truncated normal distribution as prior 
for the variance components a21, we obtain with (343.4) and (343.5) the posterior distri- 
bution for the vector aof the variance and covariance components a T and aij 
1 
k-1 
P(oIY) ~ i=lIl exp(-kl'a~-kl 
1 2i (a~-i/~ai )2) H1 exp(- (aij "#aij )2/2Vaij ) 
(detlg detX'E-1x)'l/2exp(-y'Wy/2) 
for 
k2i > 0, Vai < /t2 i. 
(343.7) 

134 
By integrating over the domain of a, the normalization factors for the distributions 
(343.6) and (343.7) are determined. Neither for the distribution (343.6) nor for (343.7) 
could this integration be solved analytically. The same applies for the integration in 
A 
(231.5), which gives the Bayes estimate a B for the vector a of variance and covariance 
components, and for the integration leading to the confidence region (241.2) or to the 
hypothesis test for the variance and covariance components. To solve these inferential 
problems, the numerical methods of Section 27 need to be applied. 

135 
35 Classification 
351 Decision by Bayes Rule 
Classification solves the problem of attributing an object to a class based on the charac- 
teristics observed for the object. In other words, a sample of the characteristics has been 
drawn from a population within a set of different populations without knowing from 
which population. The sample then needs to be attributed to the population from where 
it stems. Classification has been widely applied in pattern recognition, for instance in 
automatic reading of numbers or letters. 
The predictive analysis of the Bayesian inference is well suited to solve the problem of 
classification (Geisser 1964; Press 1982, p.402). We will therefore concentrate on this 
approach. Since more than one characteristic is measured for the object, which needs to 
be classified, a multivariate analysis is asked for. However, this does not mean that 
Bayesian inference in a multivariate linear model has to be introduced together with the 
appropriate distributions, especially the matrix t distribution, see for instance (Koch and 
Riesmeier 1985). We need only a few results from the multivariate model of the stand- 
ard statistical techniques, in order to apply the Bayesian predictive analysis for the prob- 
lem of classification. The Bayesian approach in a multivariate linear model is treated, 
for instance, in Box and Tiao (1973), Broemeling (1985), Bunke and Bunke (1986), 
Press (1982), Press (1989) and Zellner (1971). 
We assume u different populations c0 i and a pxl observation vector y having the normal 
distribution with the pxl vector 0 i and the pxp positive definite matrix Y'i as parameters 
yJ0i,r-i - N(0i,1~ i) 
for 
ie{1 ..... u}, 
(351.1) 
if y originates from the population Â¢0 i. In an application the parameters 0 i and/~i' which 
according to (A21.3) are the expected value and the covafiance matrix of y, will be usu- 
ally unknown. We therefore need training samples, which are observations whose origin 
we know. Thus, 
Yll' Y12 ..... 
Yl,nl 
from 
Â¢01 
Y21' )'22 ..... 
Y2,n2 
from 
Â¢02 
............................. 
(351.2) 
Yul' Yu2 ..... 
Yu,nu 
from 
~0u , 

136 
where the pxl observation vectors Yij are independent and normally distributed accord- 
ing to 
Yijl0i,Zi-N(0i,Zi) 
for 
ie{1 ..... u}, 
je{1 ..... ni}. 
(351.3) 
Let Pi be the prior probability that the observation vector y stems from the population 
0~ i and let p(yIyeÂ¢0 i) be the probability density function of the observation vector y 
under the condition that y comes from 0) i. Bayes' theorem (211.1) then gives the poste- 
rior density function p(yeÂ¢0 i t Y) of the observation vector y being classified into the pop- 
ulation 09 i, given the data y, 
p(ye0~ i [y) o, pip(ylysc0i). 
(351.4) 
The classification is a decision problem. We therefore apply the Bayes rule (231.2) to 
reach a decision with minimum posterior expected loss. Let L(ye0) i , c0j ) denote the loss 
for the classification of y into the population 0) i, while actuatly the population o)j is pre- 
sent. We assign the values 
L(ysÂ¢0i,Â¢0 i) = 0 
for 
i = j 
L(yeÂ¢0i,~0j) ~ 0 
for 
i ~ j, 
(351.5) 
that is no loss for the correct classification and losses different from zero for the misclas- 
sification. 
The posterior expected loss of the classification y into 0~ i is given with (351.4) and 
(351.5) by 
u 
= 
Y~ L(ye0)i,0)j)p(y~c0jly ). 
(351.6) 
E(L(YeÂ¢Â°i)) 
j=l 
j~i 
By minimizing the expected loss, we obtain the Bayes rule for the classification, which 
says: classify the observation vector y into the population ,o2 i, if 
u 
u 
j=12 L(ye0)i,~)p(yeÂ¢0 j ]y) < j=l 
y~ L(Yer~k'CÂ°J)P(Ye~Â°J lY) 
for all ke{1,...,u}. 
jei 
jek 
(351.7) 
If we assign equal losses to the misclassifications, that is 
L(yeÂ¢oi, o)j ) = L(yao) k, ~01 ), 
(351.8) 
then (351.7) simplifies to the Bayes rule: classify the observation vector y into the popu- 
lation o) i, if 
p(ye~0itY ) > p(ye~0klY) 
for all 
ke{1 ..... u} 
with 
i e k. 
(351.9) 

137 
In other words, assign the observation vector y to the population f0 i, for which the 
posterior density function p (ys c0 i l Y) attains a maximum. 
So far we have not mentioned how the density function p(ya(0 i lY) in (351.7) or (351.9) 
is determined. It can be readily written down, if the parameters of the normal distribu- 
tion (351.1) are known. Although this would be an exception for an application, we start 
with this case for a comparison with the results, for which the parameters of the distribu- 
tion (351.1) are unknown. 
352 Known Parameters 
If the parameters of the normal distributions of the different populations are known, the 
probability density function P(Yl Ye~Â° i ) of the observation vector y under the condition 
that y comes from c0 i is given by (351.1) 
p(yly~r0 i) = p(yl0i,Zi) 
and we obtain with (A21.1) 
p(ylysÂ¢0 i) ~ (detI;i)-l/2 exp[- Â½(Y-0i)'I:il(y-0i) ] . 
(352.1) 
Substituting this expression in (351.4) gives the posterior density function p(ye(0 i I Y) for 
classifying y into Â¢0 i, given the data y, 
P(YeCÂ° i lY) ~ Pi(detlgi)'l/2exp[- Â½(Y-0 i)'7"il(y-0i)]. 
(352.2) 
The Bayes rule for the classification then follows from (351.7) or from (351.9), if equal 
losses are assigned to the misclassification. 
The latter case will be investigated further by introducing a discriminantfunction d i (y), 
which is obtained by taking the natural logarithm of the density (352.2). The Bayes rule 
then says: classify y into Â¢0 i, if 
di(Y) > dj(y) 
for 
all 
j~{1 ..... u} 
with 
i ~ j 
(352.3) 
and 
di(Y) = - ~(y-0i)'lgil(y-0i) 
- ~ In detlg i + In Pi" 
(352.4) 
If Y.i=Y. and Pi=C holds for all is { 1 ..... u}, we omit constants, which is admissible be- 
cause of (352.3), and obtain instead of (352.4) the negative discriminant function 
di(Y) = (Y-0i)'Y--l(y-0i). 
(352.5) 
It is known as the Mahalanobis distance between the observation vector y and the popu- 

138 
lation co i. This discriminant function classifies the vector y into the population co i from 
which y has the shortest Mahalanobis distance. If in addition Z=cr2I we find again after 
omitting the constant the negative discriminant function 
di(Y) = (y-0i)'(Y-0i), 
(352.6) 
which is known as minimum distance classifier. 
While the discriminant function (352.4) depends on the quadratic form y'Z i ly of the ob- 
servation vector y, the discriminant functions (352.5) and (352.6) are linear in y, since 
y'Z" ly and y'y are constants. The boundaries determined by (352.5) or (352.6) between 
different populations are therefore hyperplanes, while the boundaries resulting from 
(352.4) are surfaces of second order. 
353 Unknown Parameters 
We will now derive the density function p (y lye to i ) in (351.4) under the assumption that 
the parameters 0 i and Z i of the normal distribution for the population co i are unknown. 
This is typical for a practical application of the classification. 
We will later have to integrate with respect to 0 i and Z i. It is therefore convenient to 
introduce instead of the covariance matrix Z i as parameter the weight matrix Pi with 
P. = Z. 1 
(353.1) 
1 
1 
A 
Given are the training samples (351.2), from which we derive the unbiased estimate 0 i 
A 
of 0 i and Z i of Z i with (Koch 1988a, p.291) 
nÂ° 
^ 
1 
Oi = ~ii j--ZI Yij 
and 
(353.2) 
^ 
1 
Zi = n-~ fli 
with 
(353.3) 
n i 
A 
A 
ai 
j__E 1 (Yij-Oi) (Yij-Oi) 
Since the observation vectors Yij are independent and normally distributed according to 
A 
(351.3), the estimate 0 i under the condition that 0 i and Z i are given is normally distrib- 
uted (Koch 1988a, p.143) 

139 
~il0i ,I~ i - N(0 i ,nilxi). 
(353.4) 
With (353.1) and (A21.1) we therefore obtain the density function P(0i 10i 'Pi 1) of 0i 
with 
A 
A 
p 
A 
p(0 il0i,Pi 1) ~ (detPi)112 exp[- ~ ni(0i-0i) Pi(0i-0i)]. 
(353.5) 
The matrix fli in (353.3) has the Wishart distribution (Koch 1988a, p.302) 
fli 10i'~'i - W(ni-l'I;i)" 
(353.6) 
The density function p(Qi 10i ,I~ i ) of the matrix fli therefore follows with the substitu- 
tion (353.1) by (Koch 1988a, p.160) 
P(fli t0i'P[1) ~ (detPi)(ni-1)/2(detfli)(ni-P-2)/2exp[. 
~ tr(Pifli)l- 
(353.7) 
Now we are in the position to derive the posterior distribution for the parameters 0 i and 
A 
Pi" Let p(0 i ,Pi) be the prior density function of 0 i and Pi and p(0 i ,o i 10 i ,Pi) the 
A 
likelihood function of the estimates 0 i and fli determined by the training sample. The 
A 
A 
posterior density function p(0 i ,Pi 10i 'fli ) of 0 i and Pi given the estimates 0 i and Qi 
then follows with Bayes' theorem (2t 1.1) by 
A 
A 
P(0i 'Pi IÂ°i'Qi ) *' P(Â°i 'Pi)P(Â°i 'fli 10i 'Pi )" 
(353.8) 
For introducing the prior distribution p(0 i ,Pi) we have the choice between a nonin- 
formative prior and an informative prior. With respect to the practical application, where 
in general nothing is known in advance about the parameters 0 i and Pi of the different 
populations, we will restrict ourselves to noninformative priors. Informative priors may 
be introduced as conjugate priors. The normal-Wishart distribution used in the multivar- 
iate linear model (Broemeting 1985, p.377; Bunke and Bunke 1986, p.439) would be 
such a prior. 
Based on Jeffrey's principle of invariance the noninformative prior p(0 i 'Pi ) for 0 i and 
Pi follows from (222.10) with (Geisser 1965; Press 1982, p.80) 
P(0i,P i) *~ (detPi)'(P+l)/2 
(353.9) 
A 
Since 0 i and fli are independent, we obtain with (353.5), (353.7) and (353.9) instead of 
(353.8) 
P(0i 'Pi t0i 'fli ) ~ (detPi)(ni-p-1)/2 
exp{- ~ tr[Pi(fli+ni(0i-0i)(0i-0i)')l}. 
(353.10) 

140 
A 
Now we compute the predictive density function p (YI0i,fli) of the predicted observa- 
A 
tion vector y under the condition that the estimates 0 i and fli are given, which means 
that the observed data belong to the population c0 i. This density is obtained with (262.2) 
by 
p(ytOi,Qi) ~ I 
I p(ylOi,Pi,Oi,fli)P(Oi,PilOi,fli)clPidOi, 
(353.11) 
o. P. 
1 
1 
where o i denotes the parameter space of 0 i and Pi the parameter space of Pi" For the 
A 
density P(Yl 0 i 'Pi' 0i 'fli ) of the predicted observation vector y we assume the normal 
distribution (351.1) together with (353.1). Thus, with (A21.1) 
p(y[0i,Pi,0i,fli) 
~ (detPi)l/2exp[ - ~(Y-0i)'Pi(Y-0i)l. 
(353.12) 
Substituting (353.10) and (353.12) in (353.11) leads to 
p(y[0i,Â°i ) ~ I 
I (detPi)(ni-P)/2exp[- 
~ tr(PiA)]dPid0 i 
(353.13) 
o. P. 
1 
1 
with 
A 
^ 
A = fli + ni(0i-0i)(0i-0i)' 
+ (Y-Oi)(Y'Oi)' 
The integrand is proportional to the density of the Wishart distribution for the matrix Pi 
with n.+l and A as parameters. Because of (211.6) the integration with respect to P. 
1 
1 
therefore gives 
p(yIOi,fli) ~ I (detA)'(ni+l)/2d0i , 
(353.14) 
O. 
1 
For the integration with respect to 0 i we complete the squares on 0 i and obtain 
or 
A 
A 
A A 
A= (ni+l)0 
' - 
0: + ft. + ni0i0 ~ + yy' 
i0i 
0i(ni0i+Y') 
- (ni0i +y) 1 
1 
(ni+l)'l^ = (Oi-Oi)(Oi-Di)" 
+ B 
with 
(353.15) 
~i 
(ni+l)- 1 
^ 
= 
(ni0i+Y) 
and 
-1 
h 
A 
B = (ni+l) 
(fli+ni0i0~+yy ') 
-2 
^ 
^ 
(ni+l) 
(ni0i+Y)(ni0i+Y)' 
Substituting (353.15) in (353.14) and omitting the constants gives 

141 
P(Y[ Oi 'fli ) ~ I [det ( (0 i -0 i ) (0 i -0 i ) '+B) ]- (ni+l)/2d0i 
O. 
1 
(detB)'(ni+l)/2 f [det(I+B-l(oi-Oi)(Oi-Oi)')]-(ni+l)/2dOi.(353.16) 
O. 
1 
Using the identity (Press 1982, p.20) 
det(Ii+CO ) = det(Im+DC), 
where Cis an lxmand D an mxlmatrix, we obtain 
(353.17) 
p(ylOi,fli) ~ (detB)'(ni+l)/2 f (l+(0i_Oi)'B-l(0i_Oi))-(ni+l)/2d0i" 
O. 
1 
(353.18) 
By comparing the integrandwith (A22.3) we recognize 0 i as having themultivariate 
t-distribution. The integration with respect to 0 i therefore gives 
p(ylOi,fli) ~ (detB)-(ni+l)/2(detB) 1/2 = (detB)-ni/2. 
(353.19) 
With B from (353.15) we find 
n. 
^ A 
1 
[(ni+l)0i0 ~, + _  
(ni+l)B = fli + 
1 
n. 
^ 
A 
1 
â¢ 
= fli + ~ 
(Y'0i)(Y-0i) 
and 
n.+l 
1 
, 
1 
^ 
^ 
"I 
n. 
YY 
h-T (ni0i+Y)(ni0i+Y) 
1 
1 
(353.20) 
^ 
n i 
A 
A ') _ni/2 
p(y[0i,fl i) ~ [det(fl i + ~ 
(Y-0i)(Y'0 i) 
] 
1 
1 
A 
n i 
Oi (Y-0i) 
A 
_ni/2 
[det(I + ~ 
(Y-0i)')] 
(353.21) 
1 
By applying the identity (353.17) again we obtain with (353.3) 
^ 
^ 
n i 
A 
-1 
^ ))-ni/2" 
p(yl0i,Zi) o~ (1 + ~ 
(Y-0i)'~ i (y-0 i 
(353.22) 
A comparison with (A22.3) reveals that the predictive density stems from the multivar- 
iate t-distribution. Thus, 
A 
A 
A 
A 
YI0i,X i - t(0i,(n~-l)Zi/(ni(ni-P)),ni-P) 
(353.23) 
and 

142 
(ni)P/2F(ni/2) 
^ 
112 
^ 
^ 
(detZi)- 
p(ylO i,r- i) 
(nl~_ 1 p/2-. 
) 
l((ni-P)/2) 
ni 
. 
~.,~-1 
^ 
)-ni/2 
(1 + n-TST-1 ~Y-Ui) z'i (Y-0i) 
" 
(353.24) 
1 
^ 
A 
The predictive density function P(Yi 0 i 'Zi ) of the predicted observation vector y under 
the condition that the observed data belong to the population to i may be denoted by 
p(y[yeÂ¢0i). In analogy to (351.4) we therefore obtain the predictive density function 
p(y~c0i ly) of the observation vector y being classified into the population c0 i, given the 
data y, 
(n i)p/2F(ni/2) (det~ i)" 1/2pi 
P(Yec0 i [y) ~, 
(n~_l)P/2F((ni_P)/2). 
ni 
. 
~ .,~-1 
A 
-n./2 
(1 + n-TS]- tY-Ui) z'i (Y-0i)) 
1 
. 
(353.25) 
1 
The Bayes rule for classifying y into c0 i now follows from (351.7) or from (351.9) for 
equal losses of the misclassifications. 
We compare the density (353.25) for classifying y into co i with the result (352.2), which 
is valid for known parameters 0 i and ~;i" We recognize that these parameters, in case 
^ 
^ 
they are unknown, should not be just replaced by their estimates 0 i and I~ i. As (353.25) 
shows, the density for the classification should also account for different sizes n i of the 
training samples. 
With 
n. = n 
for 
ie{1 ..... u} 
(353.26) 
I 
we find instead of (353.25) 
^ 
^ 1 
^ 
)-n/2. 
^ 
-1/2(1 
n 
(y_ Oi) ,y.i. (y_ Oi) 
(353.27) 
P(YeCÂ°i [Y) ~ Pi(detI;i) 
+ n--g2q--1 
^ 
This result is very similar to (352.2) after substituting 0 i and I; i by the estimates 0 i and 
^ 
1 
Exonple 1: Digital images of the surface of the earth are recorded by special satellites. 
Multispectral scanners on board the satellites decompose the images of the surface into 
picture elements, so-called pixels, and measure the gray levels of the pixels in different 
bands of the frequency of electromagnetic waves. If there are p sensors for the different 
frequency bands, the data vector y of each pixel contains p different gray levels. 

143 
Let the data vector y of a pixel be classified into u different populations c0 i. We select 
training sites for each population. Different numbers of pixels are available within each 
training site, so that the observations (351.2) are collected. We assume equal losses for 
the misclassification. The Bayes rule for classifying the data vector y of a pixel into the 
population 0J i then follows from (351.9) with the posterior density function p(yer0i {y) 
for the classification from (353.25). 
A 

144 
36 Posterior Analysis Based on Distributions for Robust 
Maximum Likelihood Type Estimates 
361 Introduction 
In the recent past considerable attention has been paid to robust statistics. Robustness 
means insensitivity to small deviations from the underlying assumptions. In Bayesian an- 
alysis one studies, for instance, the robustness of the posterior distribution with respect 
to possible misspecifications of the prior density function (Berger 1985, p. 195). 
For the standard statistical techniques much effort has been spent to derive robust esti- 
mates (Hampel et al. 1986; Hoaglin et al. 1983; Huber 1981). The reason is that fre- 
quently deviations occur from the distributions assumed for the data. Because of a few 
outliers the observations are not normally distributed, as usually assumed, but obey a dis- 
tribution which slightly differs from the normal distribution. In geodesy robust estima- 
tion has therefore been mostly applied to detect outliers or to diminish the effect of out- 
liers on the estimation of parameters, but also to detect moving points in a deformation 
analysis (Caspary 1988; Jorgenson et al. 1985; Somogyi 1988). 
Huber's robust estimate is recommended, when the distribution of the data is close to 
the normal distribution, while for samptes very badly contaminated by outliers the robust 
redescending estimate, for instance, of Hampel's type should be applied (Hoaglin et al. 
1983, p.397). Both these estimates are maximum likelihood type estimates, so-called 
M-estimates. Distributions therefore exist, which, assumed for the data, yield in a maxi- 
mum likelihood estimation the robust estimates. 
Maximum likelihood estimates are also obtained by the Bayesian analysis as shown with 
(232.4), so that the robust M-estimates can be also derived by Bayesian inference. We 
only have to introduce the distributions leading to the M-estimates as likelihood func- 
tions and to assume noninformative priors for the unknown parameters, in order to ob- 
tain the posterior distribution for the parameters which gives as a maximum likelihood 
estimate the robust M-estimate. But by means of the posterior distribution we not only 
find the M-estimate, but we may also solve any inferential task for the parameters. For 
instance, we can apply the predictive analysis in order to decide which observations are 
outliers. 

362 Likelihood Function 
145 
Let the n observations Yi' collected in the observation vector y=(yi), be independent 
and have equal variances or2. By starting from the nonlinear model (321.1) we have with 
f(0)=(fi (~))' where f denotes an nxl given vector of nonlinear functions of the uxl 
vector ~i of unknown parameters, 
E(Yitl[l) = fi(O) 
with 
V(Yila2 ) = a2 
for 
ie{1 ..... n}. 
(362.1) 
In the case of a linear model, different weights may be given for the observations Yi" 
The transformation (311.4) reduces such a model to (362.1). However, a weight matrix P 
for the observations, which is not a diagonal matrix, cannot be handled, since outliers in 
the original and not in linearly transformed observations shall be accounted for by the 
robust estimation. 
We assume distributions of the following type for the observations Yi 
1 
P(Yil0'6) ~ ~ g(x), 
(362.2) 
where cr denotes the standard deviation of the observations with 6=Vrd 7 and 
x = (yi-fi(/i'))/cr. 
As can be seen in this expression, ~1 determines the location of the graph of the density 
function, while cr determines its scale. Especially in the context of robust estimation ~i is 
therefore referred to as location parameter and cr as scale parameter. 
By substituting 
g(x) = exp(-p(x)) 
(362.3) 
the distribution (362.2) includes 
a) the normal distribution (A11.1) with 
p(x) = x2/2, 
(362.4) 
b) the least informative distribution for location (Huber 1981, p.71) 
p(x) = x2/2 
for 
Ixt < c 
p(x) = clx I 
c2/2 
for 
Ix1 > c. 
(362.5) 
This distribution leads to Huber's robust M-estimate. c is a constant, it depends on the 
contamination of the observations by outliers. From studying empirical distributions of 
large samples (Huber 1981, p.87, 91) a choice of c=1.5 seems to be quite reasonable. 

146 
c) the distribution (Hoaglin et al. 1983, p.367) 
p(x) = x2/2 
for 
Ix[ -< a 
p(x) = alx [ - a2/2 
for 
a < txt 
a2 
p(x) = ab - ~ 
+ (c-b) ~[1-( 
)2] 
for 
b < [x] 
p(x) = ab - a2/2 + (c-b)a/2 
for 
Ixl > c. 
<b 
<c 
(362.6) 
This distribution gives the redescendingM-estimate of Hampel. a,b,care constants, for 
instance, a=2, b=4, c=8. 
The observations Yi were assumed as being independent, the distribution (362.2) for Yi 
therefore gives the likelihood function P(Yl/3, a) by 
n 
-fi(/3) 
p(yt/3,a) ~ 1 
H g(Yi 
.). 
o n i=1 
(362.7) 
363 Posterior Distribution in the Case of Known Scale 
We will assume that the standard deviation a of the observations is known in the likeli- 
hood function (362.7). We will also suppose a noninformative prior for the parameter 
vector 8. The likelihood function (362.7) is data translated, which means the data Yi 
only change the location of the graph of the density function. As already discussed in the 
Example 1 Section 222, the appropriate noninformative prior for the parameter vector/3 
is therefore 
p(/3) ~ const. 
(363.1) 
Bayes' theorem (211.1) leads with the likelihood function (362.7) to the posterior densi- 
ty function P(/31 Y) for/3 
n 
" fi (/3) 
p(/3ly) ~ H g(Yl 
). 
(363.2) 
i=1 
a 
All inferential tasks for the parameter vector/3 can be solved by this distribution. 
First we want to estimate 13 by means of the maximum likelihood estimate (232.4). We 
therefore take the natural logarithm of (363.2) 
n 
yi-fi(~) 
lnp(/3ly ) Â¢, Zlng( 
) 
i=l 
a 
and set the derivative with respect to/~j in ~=(~j ) equal to zero. This is admissible be- 

147 
cause of 
alnp(~ly)/8~j = (1/p(g]y))Op(~ly)/O~ j = 0 
so that Op(~ly)/O~j=O is obtNned. Thus, 
3tnp(~tY) 
1 n 
g" 
0fi(~) 
- - 
Z 
[(Yi-fi(O))/a] 
0~j 
a i=t 
g[(yi-fi(O))/a] 
0~j 
-0, 
which leads to 
n 
- fi(]~) 
Z 
g(Yi 
i=l 
a 
c?/3j 
with 
Ofi(~) 
) 
- 0 
for 
j e {1 ..... u} 
(363.3) 
g'(x) 
~(x) = - 
g(x) 
These equations have to be solved for the maximum likelihood estimate of the parameter 
vector ~. 
By substituting (362.3) in (363.3) we obtain 
V(x) = p'(x). 
Depending on the choice of the distributions introduced in Section 362 we have 
a) for the normal distribution (362.4) 
g(x) = x. 
(363.4) 
The equations to be solved according to (363.3)are 
n 
Of i (tl) 
Z 
(yi-fi(~)) 
~ 
= O, 
i=1 
Oflj 
which represent the least squares solution of the nonlinear model (362.1). 
For a linear model we have 
f(~) = X~ with 
X = (xij) 
and 
Ofi(~) 
-
-
-
-
X
*
 
*, 
a/3j 
Ij 
This gives with ~ being the estimate of ~1 
n 
A 
Z 
(yi-(X~)i)xij__ = 0 
i=l 

148 
or 
, 
X' 
=Xy, 
which are the welt-known normal equations, already derived with (312.11). 
b) for the least informative distribution (362.5) 
gt(x) = x 
for 
Ixl -< c 
~t(x) = c sign(x) 
for 
Ixl > c. 
(363.5) 
This choice of the function g(x) leads to Huber's robust M-estimate (Huber 1977). 
c) for the distribution (362.6) 
~t(x) = x 
for 
I x t -< a 
Vt(x) = a sign(x) 
for 
a < Ix I <_ b 
~t(x) = a x-c sign(x ) 
for 
b < [x I < c 
b-c 
~(x) = 0 
for 
I xt > c. 
(363.6) 
This gives the robust redescending M-estimate of Hampel (1974). 
For easy reference we give a simple numerical procedure based on modified residuals 
for computing robust M-estimates (Huber 198t, p.181). We start with a first approxima- 
tion ]l (1) obtained, for instance, by the method of least squares. In the ruth iteration we 
compute 
-el = Yi - fi (o(m)) 
(363.7) 
-e. 
-e* = ~t( _l)cr 
(363.8) 
1 
19 
0 
xij = ~j 
fi(~(m)), 
(363.9) 
where e* denotes the so-called Winsorized residual. Then the normal equations 
1 
X'X'c = X'r* 
(363.10) 
with X=(xij ) and r*=(-e~) are solved to obtain j~(m+l ) for the iteration m+l by 
/~(m+l) = ~(m) + "c. 
(363.11) 

t49 
364 Posterior Distribution in the Case of Unknown Scale 
We will now assume that the standard deviation o in the likelihood function (362.7) is 
unknown. Again we suppose a noninformative prior distribution for the parameter vector 
13. Thus, from (363.1) we take 
p(13) ~ const. 
(364.1) 
In order to obtain a posterior density function which leads in a maximum likelihood esti- 
mate to the robust estimates of 13 and ~, we introduce in contrast to the noninformative 
prior to be derived from (222.3) for ~ the prior distribution 
p(o) ~, const. 
(364.2) 
Bayes' theorem (211.1) together with the likelihood function (362.7) gives the posterior 
density function p (~, 6l Y) for 13 and 
P(I/,~IY) 
1 
n 
.- fi (13) 
-- FI g(Yl 
). 
(364.3) 
on i=1 
o 
All inferential problems for the parameter vector 13 and the standard deviation o of the 
observations can be solved by this distribution. 
We want to estimate 13 and ~ by the maximum likelihood estimate (232.4). For 13 we ob- 
tain (363.3), as shown in the preceding section. To estimate o in addition, we take the 
natural logarithm of (364.3) 
n 
lng(Yi- fi (13) 
lnp(13,oly) ~ in 1___ + 
E 
) 
o n 
i=l 
o 
and set the derivative with respect to cr equal to zero 
c~lnp(~,ryly) 
o n 
n 
g'[(yi-fi(13))/o] yi-fi(13) 
Y 
=0. 
eÂ¢ 
- 
n 
c~o 
~f 
i=l g[ (yi- fi(j~))/6] 
if2 
We obtain 
n 
Yi'-fi(O ) 
Z Z (-~ 
) = 0 
(364.4) 
i=l 
a 
with 
Z(x) 
= xÂ¢(x) 
- 1. 
This equation together with (363.3) has to be solved for a robust estimation of the pa- 
rameter vector ~ and the standard deviation ~ in the nonlinear model (362.1) (Huber 
1981, p.176). 

150 
For a numerical computation of the robust estimate of a we apply iteratively (Huber 
1977) 
1 
n 
fi(~(m)) 
(a(m+l)) 2 =~ 
Z g (Yi- 
)(er(m))2 
(364.5) 
i=l 
a (m) 
with Â¢r (m) and cr (m+l) being the estimates of Â¢r of the iteration m and re+l, respectively, 
and with 
a = (n-u) E[(g(x))2], 
(364.6) 
where x is a random variable with the standard normal distribution x-N(0, 1). With Iv(x) 
from (363.5) for instance, we find 
E[(Iv(x))2 ] = 
1 
[ I c 2 exp(- 
)dx + 
I x2 exp(- 
)dx 
+ 7 C 2 exp(-~)dx] 
C 
X2 
C 
X2 
= 
1 
[2c 2 7 exp(- 2---)dx + 
I x 2 exp(- 2---)dx]. 
(364.7) 
qe2"~ 
C 
-C 
Since a is estimated in addition, the iterative procedure for estimating lll has to be modi- 
fied by replacing (363.8) with 
-e. 
- e.* = g(~)a 
(m).- 
(364.8) 
1 
err m ) 
Hence, for simultaneously computing the robust estimates for the parameter vector ~ and 
the standard deviation ~ we apply (364.5), (363.7), (364.8), (363.9), (363.10) and 
(363.11). 
365 Predictive Distributions for Data Points 
As already mentioned in Section 361, robust estimation is generally applied to identify 
outliers or to avoid distorting the parameter estimation by outliers. After an observation 
has been marked as a possible outlier by the Winsorization according to (363.8) or 
(364.8), the question arises whether to reject the observation or to leave it in the data set 
for an ensuing parameter estimation, for instance, by the method of least squares. 
Such a question may be answered by means of the predictive distributions for the obser- 
vations. We want to predict actual observations. According to (314.5), we therefore com- 
pute from (262.2) the predictive distributions of the observations, which were identified 

151 
as possible outliers by the Winsorization. A confidence interval for the predicted obser- 
vation is then computed by means of the predictive distribution. If the actual observation 
lies outside the confidence interval, it is considered an outlier. 
Let the observation which is identified as a possible outlier be denoted by Yk" Let Yku be 
the observation predicted for Yk' For Yku' we assume the same distribution as for Yk' so 
that the density p(Yku 1/], a) of the predicted observation Yku follows from (362.2) by 
1 g(Yku- fk (tl) 
p(Ykul/l,a) o, 8 
). 
(365.1) 
6 
We suppose the standard deviation a as being known, the posterior density function 
p(tl ! y) for the parameter vector ~1 then follows from (363.2). Thus, we obtain the pre- 
dictive density function for the unobserved data point Yku with (262.2) by 
P(YkulY) 
~ 
7 g(yku-fk(~]) 
n 
g(Yi-fi(~) 
,* 
... 
-) 
ll 
-)d]~ 1 ... d/3 u. 
(365.2) 
-~ 
-~ 
a 
i=l 
a 
By means of this distribution we may estimate Yku or establish a confidence interval for 
Yku" If this interval contains the observed data point Yk' we decide that Yk is not an out- 
lier. If Yk lies outside the interval, we consider it an outlier. The distribution (365.2) is 
analytically not tractable, the numerical techniques of Section 27 therefore have to be 
applied to estimate Yku' to establish confidence intervals or to test hypotheses. 
The results obtained by the predictive distribution (365.2) of course depend on the con- 
stants in (362.5) or (362.6) of the distribution chosen for the data. 
Example 1: The abscissae x i for six points are given and the ordinates Yi are independ- 
ently measured with the given standard deviation of a=0.02. The observations are shown 
in the following table. A straight line shall be fitted through the data. Inspecting the data 
x i 
1 
2 
3 
4 
5 
6 
Yi 
0.48 
1.15 
1.49 
2.02 
2.51 
3.01 
reveals what seems to be an outlier for the observation Y2" Thus, we will apply a 
robust estimation and compute a confidence interval for the predicted observation 
Y2u' if the Winsorization marks Y2 as a possible outlier. 
The observation equations for fitting a straight line follow by 
Yi + ei = /30 + xifll 
with 
i a {1 ..... 6}, 
where flo and fll denote the unknown parameters. 

152 
With 
y+e=X~ 
the least squares fit is obtained from (312.11) with 
= (x'x)- lx'y 
and 
= 0.051 
~ 
= 0.493. 
(365.3) 
O 
' 
1 
compute Huber's robust M-estimate with c=1.5 and use/~(1)=~ 
13~1)=~ as first 
We 
o' 
1 
approximation. We iteratively apply (363:7) 
-ei = Yi - ]3(m)- xi13~m) 
and (363.8) together with (363.5) 
-e. 
-e.* = ~( -1)a-o- 
= -ca 
for 
-e i 
< -ca 
1 
= -e i 
for 
[e il < ca 
= ca 
for 
-e. 
> ca. 
I 
The normal equations (363.10) 
X'X~ = X'r* 
are solved, to give 
l](m+l) = ]~(rn) + ~:. 
After the change of the parameters is less than ~ times their standard deviations 
- 
1/2 
[1:i1 < ea(xii ) 
' 
where "c=('ci) and (X'x)-l=(xij), the iterations are stopped. With e=O.O001 twelve 
iterations are needed to obtain the robust estimates 
3 12) = - 0.004 
13112) = 0.503. 
(365 4) 
O 
~ 
Through all iterations the residual for the observation Y2 was changed by the Winsoriza- 
tion. 
The predictive density function for the unobserved data point Y2u follows from (365.2) 
with (362.3) and (362.5) by 

153 
1 
p(Y2ulY) 0" ~ 7 exp[-7((Y2u-/3o-X2/31)/cr)2] 
6 
exp[-~ i=Zl((Yi-/3o-Xi/31)/cr)2]d/31d/32 
for 
lyk-/3o-Xk/31] _< ca 
~ '~ exp[-~(2cl(Y2u-/3o-X2/31)/o'[-c2)] 
-oo 
-oo 
6 
exp[- Â½ i=~1(2cl(Yi-/3o-Xi/31)/~l-c2)ld/31d/32 
for [yk-/3o-Xk/311 > ccr. 
A 
By means of this density function we will numerically compute the Bayes estimate Y2uB 
of Y2u by (272.3) and the confidence interval by (272.6) and (272.7). The density func- 
tion P(Y2ulY), which is obtained as a marginal density, is computed by (273.2). We put 
i--200 and j=200x200 in (273.2), which means, we generate 200 random numbers with 
uniform distributions for Y2u' 200 random numbers for/3 o and for each of these random 
numbers 200 random numbers with uniform distributions for/31" 
Since the marginal density function of Y2u is univariate, the random numbers generated 
for Y2u were ordered according to increasing values and along with them the density val- 
ues. Ten additional density values were then linearly interpolated between all consecu- 
tive pairs of density values. The confidence limits were obtained from (272.6) together 
with (272.7) by starting from the densities for the minimum and maximum values gener- 
ated for Y2u and by checking numerically whether the inequality in (241.2) was fulfilled. 
The intervals in (271.4) on the coordinate axes for/30 and 131, which define the domain 
of the integration, were determined in analogy to (272.1) by finding the density values, 
which cease to contribute to the integrations. The interval for Y2u was set up such that 
the interval of the integration is ten to twenty per cent wider than the confidence interval 
for Y2u' which ensures that only those density values are neglected which do not contrib- 
ute to the integration. 
For each of the following intervals defining the domain of the integration 
-0.026 < /3o < 0.030, 0.494 < /31 < 0.509 
0.95 < Y2u < 1.05, 
and 
0.96 < Y2u < 1.04, -0.023 < /30 < 0.027, 0.495 < /31 < 0.508 
^ 
two sets of 200x200x200 random numbers were generated and Y2uB and the confidence 

154 
limits for Y2u with content 0.95 were computed. The results are given in the table 
(365.5). Although relatively few random numbers were generated for Y2u'/30 and/31' the 
results in (365.5) between the different sets of random numbers agree very well. As 
^ 
t, a( t2).~oa(12)-1 
could be expected, Y2uB comes close .v ~'o 
-~'1 
-.. 002. 
^ 
Y2uB 
Confidence Interval 
Mean 
0.999 
0.999 
0.998 
1.001 
0.999 
0.967 < Y2u < 1.030 
0.969 < Y2u < 1.030 
0.968 < Y2u < 1.030 
0.969 < Y2u < 1.028 
0.968 < Y2u < 1.030 
(365.5) 
was approxi- 
A 
Y2uB 
1.002 
1.002 
1.001 
1.002 
Mean 
1.002 
Confidence Interval 
0.975 < Y2u < 1.030 
0.973 < Y2u < 1.032 
0.975 < Y2u < 1.029 
0.975 < Y2u < 1.029 
0.974 < Y2u < 1.030 
(365.6) 
Y2u agree very well. The observations therefore contain enough information on the 
unknown parameters/30 and/31 to warrant an approximate computation of the marginal 
distribution. 
The confidence interval computed for the predicted observation Y2u does not contain the 
value of the observation Y2' which is Y2=l. 15. We therefore may consider this observa- 
tion an outlier. 
Furthermore, the marginal posterior density function p(Y2u[Y) of Y2u 
mately computed with (274.6) by introducing for/30 and/31 the maximum likelihood es- 
timate, that is the robust estimate (365.4). For each of the two intervals defining the 
domain of the integration for Y2u 
0.95 < Y2u < 1.05, 0.96 < Y2u < 1.04 
two sets of 500 random numbers for Y2u were generated giving the results (365.6). The 
A 
values in (365.5) and (365.6) for the Bayes estimate Y2uB and the confidence interval of 

155 
As already mentioned, the confidence interval computed for any predicted observation of 
this example depends on the constant c in (362.5). In fact, the length of the confidence 
interval computed for Y2u is approximately equal to 2co=0.06. In other words, if the re- 
sidual for the observation Y2 is changed by the Winsorization, Y2 is assumed to be an 
outlier. 
A 

156 
37 Reconstruction of Digital Images 
371 Model Description and Likelihood Functions 
The processing of digital images has been intensively investigated in many scientific dis- 
ciplines for a great variety of applications. For instance, photogrammetry with its geodet- 
ic applications has participated in the new developments and is actively engaged to try 
new developments in the field of image processing (F6rsmer 1988). 
Numerous methods exist for the analysis of digital images based on different assump- 
tions. While there is a similarity between the models which define the mathematical re- 
lation between the data and the unknown parameters, the statistical assumptions con- 
cerning the data and the methods for estimating the unknown parameters differ vastly. 
The Bayesian approach, due to its flexibility of introducing prior information for the pa- 
rameters, unites some of the approaches and puts them on a sound theoretical basis. It is 
therefore presented here. 
An object radiates energy, and if this energy passes an image recording system, an im- 
age is formed. A digital image consists of a rectangular array of picture elements, the 
so-called pixels. For each pixel, due to the different intensity of the energy radiated by 
the object, different gray levels are measured and digitally recorded. 
The imaging systems do not work perfectly. We therefore have to differentiate between 
the measured gray levels for the pixels, which shall be collected in the vector y of obser- 
vations, and the unknown parameters representing the gray levels obtained under ideal 
conditions. These unknown parameters or signals, collected in/3, have to be estimated. 
We will assume that given nonlinear functions f(~) with f(~)=(fi (~)) of the unknown 
parameters ~ represent the imaging system and give the mathematical relation between 
the unknown signal ~ and the observations y. The functions fill) therefore account for 
the distortions or the blurring of the image. In addition, let the covariance matrix of the 
observations be defined by the unknown variance factor (r2 and the known weight matrix 
P. Thus, we obtain the nonlinear model, already introduced by (321.1), 
E(y]0) = f(~i) 
with D(y[cr2) = cr2P -1 
(371.1) 
Gray levels are generally restricted to values from 0 to 255. The vectors y and /3 may 
therefore be interpreted as discrete random vectors. 

157 
In many cases the nonlinear function f(O) can be approximated by the linear function 
X~, for instance, if the blurring of an image can be modeled by a shift-invariant 
point-spread function. In such a case the matrix X might achieve a simple convolution 
over a small window, for instance a weighted mean of a pixel with its eight nearest 
neighbors. In the simplest case X is identical with the identity matrix I 
X= I. 
(371.2) 
By means of the linear functions X~] we obtain the linear model, see also (311.3), 
E(yl~ ) = X~ with 
D(yIcr2 ) = a2P -t. 
(371.3) 
We will also consider models where the variance factor a 2 is known, so that the covari- 
ance matrix Z of the observations y is given by 
Y. = cr2p -1. 
(371.4) 
This leads instead of (371.1) and (371.3) to the models 
E(ylO) = f(/~) 
with 
D(y) = X 
(371.5) 
and 
E(yl/~) = x~ with 
D(y) = X. 
(371.6) 
We will also introduce the very simple covariance matrix 
Z = I. 
(371.7) 
The observations y are assumed as normally distributed, so that the likelihood function 
follows with (A21.1) for the nonlinear model (371.5) by 
P(Yl l 3) ~ exp[- Â½(y- f(~l) ) "Z- 1 (y_ f(/3) )] 
(371.8) 
and for the linear model (371.6) by 
p(yl/3) ~ exp[- ~(y-X~)'x-l(y-X~)]. 
(371.9) 
The likelihood functions for the models (371.I) and (371.3) are correspondingly ob- 
tained. 
Depending on the choice of the prior distribution p(]3), Bayes" theorem (211. t) leads to 
different posteriori distributions p(/31 y) of the signal ~ and thus to different methods of 
the image restoration. In the following we will discuss three different choices of prior 
distributions. The first one agrees with the priors of the linear model, the second one is 
based on the normal distribution and the Gibbs distribution, the third choice leads to the 
maximum entropy method. 

158 
372 Normal-Gamma Distribution as Prior 
We will assume that the linear model (371.3) describes the imaging system. If prior in- 
formation on the unknown signal ~ is available by the expected value #p and the covari- 
ance matrix Z~ 
E(O) = /tp, 
D(/]) = X/3, 
(372.1) 
and if also the expected value and the variance of the variance factor or2 are known in 
advance, we may introduce the normal-gamma distribution (313.1) as prior distribution, 
whose parameters are determined by (313.2). The Bayes estimate ~ of the unknown pa- 
rameters ~1 then follows from (313.7), where we omit the bars in agreement with the no- 
tation of (371.3), 
~B = (X'PX+Â¥- 1 ) - 1 (X'PY+V- 1/~) â¢ 
(372.2) 
For the special case X=I from (371.2) we obtain 
~B = (P+Â¥- 1 ) - 1 (py+Â¥- lbt ) 
(372.3) 
and recognize the Bayes estimate ~B being the weighted mean of the prior information 
/t=/tp and of the data y, where P and V- 1 are the weight matrices. 
Reconstructing images by means of (372.2) or (372.3) is not very efficient, since accord- 
ing to (372.1) the prior information on the gray levels and their variances and covari- 
ances has to be introduced for each individual pixel. This is time-consuming and the ap- 
proach is therefore not well suited for an automatic processing of images. It has to be 
restricted to special cases. 
373 Normal Distribution and Gibbs Distribution as Priors 
We start from the simplified nonlinear model (371.5) and introduce the prior information 
on the unknown parameters ~1 as in (372.1) by the expected values/tp and the covariance 
matrix Z]3. If in addition the parameters ~ are normally distributed, the prior distribution 
p(~i) for ~lresults with (A21.1) from 
p(/]) oÂ¢ exp(- " ~(/]-#p)'Xf31(/]-/lp)) 
or from 
(373.1) 
p(~ ~ exp(-O(~)) 
(373.2) 
with 

159 
U(~I) = 1 ~(/]_/Lp),Z~l(~l./tp), 
(373.3) 
where lJ(~) is a scalar function of the unknown signal ~1. 
If our prior knowledge/tp agrees well with the signal/1 to be reconstructed, the function 
0(~) will have a small value. In general, images are smooth, so that our prior assump- 
tion of an image is a smooth picture. The function IJ(/]) in (373.3) therefore measures 
the roughness of an image, and U(/i) is called the energy attributed to the signal/1. We 
will prefer images with small energies as compared to images with large energies. 
If the covariance matrix Z~I in (373.3) has a diagonal form, we may represent the energy 
U(~) by 
U(/]) = .I; U i (/]), 
(373.4) 
1 
where the summation is extended over each pixel i of the image and U i (j~l) denotes the 
contribution of the pixel i to the energy lJ(~). We compute O i (~) locally from the dif- 
ferences of the gray levels of the pixels, which are neighbors of the pixel i. The function 
U(/]) then gives a measure of the roughness of the image. We define a neighborhood of 
the pixel i by the four pixels above, below and on both sides of the pixel. This is shown 
0 
0 
0 
0 
0 
0 
0 
0 
0 
â¢ 
0 
0 
â¢ 
0 
0 
0 
â¢ 
0 
0 
0 
0 
0 
0 
0 
0 
0 
o 
Fig. 373-1 
Fig. 373-2 
Fig. 373-3 
in Fig. 373-1, where the pixel i, for which we introduce the neighborhood, is indicated 
by a dot and the neighbors by circles. Larger neighborhoods are depicted in Figs. 373-2 
and 373-3. 
The contribution U i (~) in (373.4) to the energy lJ(~]) is now obtained by summing the 
square of the difference ~j -~i of the gray levels between the pixel i and a pixel j over 
all pixels j in the neighborhood of pixel i 
U i(~ = .E (]3j-/3 i)2. 
(373.5) 
J 
Large differences of gray levels give large contributions U i (~1) and therefore high ener- 
gies, which in turn lead to small values of the prior distribution p(~) in (373.2). On the 

160 
other hand, small differences of gray levels give high density values. This agrees with 
our prior conception that an image is smooth. Of course, different functions of/3j-]3 i 
could have been chosen in (373.5), to express the contribution U i (]3) of the pixel i to 
the energy U(/]). 
The local representation of the prior distribution (373.2) by (373.4) and (373.5) was 
given here by a heuristic argument. However, this distribution can also be derived, if we 
assume a Markoff random field for the unknown parameters ]3 i with/~=(/6 i ) such that 
the probability 
P(/3i~DI~ j, iej) depends only on ]3j with j being a neighbor of i, 
(373.6) 
where D denotes a subspace of the space for ~i' The prior distribution p(0) is then given 
by the Gibbs distribution (Spitzer 1971), whose functional form is identical with (373.2). 
The energy lJ(i~i) is locally defined and may be computed by (373.5) or similar expres- 
sions (Geman and Geman 1984; Geman et al. 1987). 
As mentioned, we assumed (371.5) as the model for the image reconstruction. If the co- 
variance matrix ~ of the observations y has a diagonal form 
Z = diag(cr~ ..... a~ .... ), 
(373.7) 
the likelihood function p(y[]3) is then given with (371.8) by 
p(yl/1) 
= exp[- z (yi-fi(O))2/(2a~)]. 
(373.8) 
i 
If fi (~) can be locally computed, which will be assumed, for instance from the pixel i 
and its neighbors, then the likelihood function (373.8) is given in a local representation. 
In the simplest case we have 
fi(~! ) = /3 i. 
(373.9) 
Bayes' theorem (211.1) leads from the prior density (373.2) together with (373.4), 
(373.5) and the likelihood function (373.8) to,the posterior density function p(/3ly) of 
the unknown signal /3 given in a local representation, if we introduce a constant b to 
weigh U i (~) with respect to the likelihood function, 
P(t[]IY) ~ exp{- Z [b Y. (/3j-/3i)2 + (yi-fi(O))2/(2cr~)]}. 
(373.10) 
i 
j 
The first expression in the exponent of p(tSI y) measures, as mentioned, the roughness or 
the energy of the signal ~i, while the second term gives a measure for the fidelity of the 
observations y to the signal ~. Thus, the posterior distribution p(/3] y) is a function of the 
roughness and the fidelity of the signal/3. Based on this posterior distribution, the image 
is reconstructed by means of the Bayes estimate ~B from (231.5) or the MAP estimate ~i 
from (232.4) of the unknown signal 0. 

161 
In general, the posterior distribution (373.10) is analytically not tractable, so that the es- 
timates ~B or/1 have to be computed numerically. But even a small digital picture may 
have 512x512 pixels and therefore the same number of unknown parameters ~. A com- 
putation of ~B from (272.3) or a numerical derivation of ~1 would involve a tremendous 
amount of computational work. But the numerical effort can be considerably reduced, if 
we take advantage of the local representation of the posterior distribution (373.10). This 
is done for computing the Bayes estimate ~B in the following approach. 
We compute the Bayes estimate of each parameter ]3 i by means of its marginal posterior 
distribution P(/~i l Y), which we approximately obtain from (274.7). Since the Bayes esti- 
mate of ~ is not known in advance, we have to go through iterations. Let ~oB with 
~oB=(~ioB ) be the Bayes estimate of ~ in the oth iteration. For the next iteration o+1 the 
marginal density P(/~i l Y) is therefore obtained from (274.7) by 
P(fli IY) ~ P(~loB,~2oB ..... ~i_l,oB,/ri,~i+l,oB .... lY) 
(373.11) 
or with (373.9) and (373.10), if q denotes the number of pixels in the neighborhood of 
pixel i 
p(]3 i [y) ~ exp{- [b 2. (~joB-/~i)2+(y i-~i)2/(261~ ) ] } 
J 
exp{-[(bq+l/(ZCr~a))/3~-Z(b 2. ~joB+Yi/(Zcr~))/3i] }. 
(373.12) 
J 
With completing the square on/~i' we obtain from (A11.1) for ~i the normal distribution 
/3 i tY - N((b 2. ~joB+Yi/(Zcr~))(bq+l/(2cr~)) -1, (2bq+l/Â¢r~) -1) 
(373.13) 
J 
A 
and the Bayes estimate /~i ,o+l,B for /3 i of the (o+l)th iteration from (231.5) and 
(A 11.3) by 
~i ,o+1 ,B = (b 2. ~joB+Yi/(Zcr~))(bq+1/(26~)) -1. 
(373.14) 
J 
The estimate ~i, o+1 ,B is then substituted in (373.1t) and the next pixel k is processed. 
The marginal density p (13k [y) of/3 k for this pixel follows with 
P(l~k ty) ~ P(~loB'~2oB ..... ~i-l,oB'~i,o+l,B'~i+l,oB ..... ~k .... [Y)" 
(373.15) 
This procedure is repeated until the last pixel and ~o+1 ,B of the (o+l)th iteration is ob- 
tained. The sequence of the processing of the pixels is random to avoid any systematic 
effect. Since gray levels are estimated, the value for ~ioB is rounded to the next integer. 

162 
At the first iteration, when estimates have not been computed yet, we use the observa- 
tion Yi with y=(yi) as estimate for fli" As soon as the estimates from one iteration to 
the next do not change any more, the iterations are stopped. 
If functions are used different from (373.5) and (373.9) to express the contribution U i (/]) 
to the enery O(/]) and to model the imaging system, it may not be possible any more to 
derive the expected value of the distribution p(Oi [y) analytically as in (373.14). Then 
the estimates ~ioB have to be numerically computed from (272.3) with (373.11) and 
(373.15). 
A similar approach to compute the MAP estimate ~1 of the signal/~ is proposed in Geman 
and Geman (1984). Because of its analogy to a process in chemistry to achieve a state of 
low energy by heating and slowly cooling a substance, it is called simulated annealing 
(Aarts and van Laarhoven 1987; Ripley 1988, p.95). The observations Yi are introduced 
as approximate values for the estimates of the parameters/~i" The pixels are then visited 
randomly and a random value is chosen from the distribution (373.10) for/3 i . With each 
iteration, which involves the visit of each pixel, the temperature T introduced into the 
exponent of (373.10) by 
Z [b Y. (flj-fli)2 + (yi-fi(~))2/(2a~)]/T 
i 
j 
is decreased, until a state of low energy and therefore the maximum of the posterior den- 
sity is attained. This procedure requires more iterations and therefore more computing 
time than the approach based on (373.15), although the results are similar (Busch and 
Koch 1990). 
Images very often contain edges, which have to be considered in a complete reconstruc- 
tion of an image, since the smoothing of a picture may not be extended across the edges. 
To solve this problem, we have to introduce line elements as unknown parameters in ad- 
dition to the unknown gray levels of the pixels considered so far. These unknown line 
elements take the value one, if they are present, or zero, if they are absent. The line ele- 
ments are placed between the pixels, as shown in Fig. 373-4. 
Another example where the need for additional unknown parameters arises is the dis- 
crimination of textures. Metal, plastic and wooden objects, for instance, have to be iden- 
tified in an image. An additional unknown parameter is then defined for each pixel. The 
value of this parameter indicates to which texture the pixel belongs. 
Let ~a denote the vector of additional parameters, which are discrete random variables. 
The prior distribution p(O, ~1 a) of the signal It and the additional parameters/]a shall be 

163 
olo1Â° 
Â°1Â°1Â° 
oloio 
Fig. 373-4 
given with (211.2) by 
p(/l,/Ja) = p([l[~a)p(/Ja), 
(373.16) 
where p (/~1/la) denotes the conditional distribution of ]3 given [I a. 
If the additional parameters [I a consist of line elements, the density function p(/~l/~a) is 
still represented by (373,2) together with (373.4) and (373.5). However, differences of 
gray levels across a line element may not contribute in (373.5) to the energy of the sig- 
nal. Thus, 
p([ll~a) ~ exp[-.~ Ui([ll/]a)] 
(373.17) 
1 
with 
ui(/ll/J a) = (b/qa) .Z (1-/3ija)(~j-/3i)2, 
(373.18) 
J 
where ]3ij a with/la= [ .... fiij a .... ]' denotes the line element between the pixel i and 
j with value one, if it is present, and value zero, if it is absent, and qa the number of 
line elements in the neigborhood of pixel i with fii j a =0" The constant b is again as in 
(373.10) a weight factor. 
The density function p(/Ja) of the additional unknown parameters is also locally repre- 
sented with (373.2) and (373.4) by 
p([la) ~ exp[- Z Uk(/la)] 
(373.19) 
k 
with as many functions Uk(/]a) as unknown additional parameters/]a" In the case of line 
elements we choose 
Uk([I a) = c Y. fl(/3ija,/3mna), 
(373.20) 
1 
where the summation is extended over special configurations of line elements ]3mn a in 
the neighborhood of the line element/3ij a" The function fl (/3ij a' ]3tuna) attributes low 
energy or large weight to lines which continue and high energy or low weight to isolated 

164 
lines and to beginnings and ending of lines or unlikely configurations of lines. The con- 
stant c weighs the contribution Uk(Ila) to the energy with respect to ui(/tl/I a) from 
(373.18). More detailed suggestions for representing the prior density function in the 
case of line elements or texture labels can be found in Geman et al. (1987) and in the 
case of line elements in Busch and Koch (1990). 
The likelihood function P(Y[O) of the observations y is not affected by the additional 
parameters. With 
p(yl~i, Oa) = p(yl/1) 
(373.21) 
we therefore obtain from the local representation (373.8) 
p(ytO, Oa) ~ exp[- ~ (yi-fi(/i))2/(2cr~)]. 
(373.22) 
i 
Bayes' theorem (211.1) now leads with (373.16) to (373.20) and with (373.22) to the 
posterior distribution of the parameters ~ and the additional line elements Oa given in a 
local representation 
p(O,~laty) ~ exp{- E [(b/qa) Â£ (1-/3ija)(/~j-/3i)2 
i 
j 
+ (yi-fi(~)2/(2cr~)] - Â£ [c Â£ fl(/3ija,/3mna)]}. 
(373.23) 
k 
1 
We compute the approximate marginal distribution p(]3 i ]y) for the gray level /3 i of 
pixel i correspondingly to (373.11) with __~aoB=(~ijaoB ) being the Bayes estimate for 
/i a of the oth iteration 
P(fli [Y) = P(~loB'~2oB ..... ~i-1 ,oB'~i '~i+l ,oB ..... ~i-1, j aoB'~ijaoB' 
~i+l, j aoB .... l Y). 
(373.24) 
The approximate marginal distribution P(/~ij a ly) of the line element/3ij a follows ac- 
cordingly by 
p(/3ijalY) = p(~loB,~2oB ..... ~i_t,oB,~ioB,~i+l,oB ..... ~i_l,jaoB,~ija, 
A 
]~i+l, j aoB .... l y). 
(373. 25) 
By substituting (373.9) and (373.24) in (373.23) we find 
[y) ~ exp{-[(b/qa)~.(1-~ijaoB)(~joB-~i)2+(Yi-[Ji)2/(2a~)]}__ 
P(Â¢I i 
J 
exp{- [ (b+l/(2a~) )/31~-2 ( (b/qa)2. ( 1-~i j aoB)~j oB+Yi/(2cry) )/3i] }. 
3 
(373.26) 

165 
A_ 
Thus, p(/3 i [y) is given by the normal distribution and the Bayes estimate/Ji ,o+1 ,B for 
/3 i of the (o+l)th iteration follows from (231.5) with (All.3) by 
~i,o+l,B = ( (b/qa)~. ( 1 -~i j aoB)/~j oB+Yi / (2crY)) (b+l / (2crY)) - 1 
(373.27) 
J 
The Bayes estimate ~ij a,o+l ,B for the line element/3ii ao of the (o+l)th iteration is ei- 
ther equal to zero or equal to one and is obtained by determining the maximum of 
p(flija=01y) 
and 
p(/3ija=t[y ). 
In a random sequence the Bayes estimates of the gray levels of all pixels and of all line 
elements are computed and substituted in (373.24) and (373.25). In a new iteration, 
again with a random sequence, the estimates for all pixels and line elements are recom- 
puted. The iterations stop, when no changes in the estimates occur between two itera- 
tions. At the beginning of the iterations it is assumed that no line elements are present. 
Examples for this method of image restoration can be found in Busch and Koch (1990). 
374 Prior Leading to Maximum Entropy Restoration 
The representation (373.2) of the prior distribution by means of the roughness of an im- 
age suggests introducing the entropy as a measure of roughness. If we interpret the gray 
level values/3 i with ]3=(fi i ) as discrete densities, the information H n or the uncertainty 
of an image follows from (223.3) by 
H n = - .Z /3 i ln~li, 
(374.1) 
1 
where the summation is extended over each pixet i. By defining a prior density function 
P(t]) with 
p(]3) ~, exp(-~, .Z /3 i ln/li) 
with 
~. >,0, 
(374.2) 
1 
where ~, is a constant, we use large prior density values for large values of the entropy or 
a large amount of uncertainty. In a reconstruction we therefore prefer images with large 
uncertainty. According to (223.5) these are pictures which are smooth and as uniformly 
gray as possible. 
Using the prior density (374.2) and the likelihood function (371.9) in Bayes" theorem 
(211.1) leads to the posterior density function P(01 Y) for the unknown signal [I 
P(01Y) o~ exp[-)~ .~/3 i ln/3 i - ~(y-X~)'z'l(y-/q~)]. 
(374.3) 
1 

166 
A 
This posterior distribution can now be used to derive the Bayes estimate fib from (231.5) 
or the MAP estimate ~i from (232.4) of the unknown parameters ~i. For obtaining the 
MAP estimate ]] we take the natural logarithm of (374.3) and set its derivative with 
respect to ~1 equal to zero corresponding to the derivation of (363.3). We obtain 
01np(~Sly)/0~8 = - )L(lnfl + 1) - X'I:'l(xfl-y) = 0 
(374.4) 
with 
lnj8 = (In]3i) 
and 
1 = (1,1 ..... 1)' 
Furthermore, we find 
Zlnfl = -Z1 + X'z-l(y-~/l), 
so that the MAP estimate ~ follows with 
= exp( - l+X" ~." 1 (y-X~/;0, 
(374.5) 
where exp(... ) has to be interpreted in analogy to ln/]. 
This estimate is also obtained by the maximum entropy method for restoring images. To 
derive this method, the entropy H n following from (374.1) 
H n = - .El3 i ln/~ i 
1 
is maximized subject to the constraint 
(y-Xfl) '~.- l(y.X~l) = Z2;n, 
(374.6) 
where Z2; n denotes the lower a-percentage point of the z2-distribution with n degrees 
of freedom (Koch 1988a, p.146). This constraint introduces the fidelity of the observa- 
tions mentioned in connection with (373.10). It results from the fact that, given the pa- 
rameters ~ the quadratic form on the left-hand side of (374.6) has the z2-distribution 
(Koch 1988a, p.145). 
To determine the extreme value of the entropy H n, we introduce the Lagrange function 
w(~,~,) with 
w(~l,Z) = - ~/~i ln/~i " ~-Z ((Y-X~'Â£'I(y-Xfl)-Z~;n)' 
(374.7) 
1 
where -1/2X denotes the Lagrange multiplier. Setting the derivative o~w(]],Z)/~]] equal 
to zero leads to (374.4) and therefore to the estimate ]] which is identical with the MAP 
estimate (374.5). 
Maximization of the entropy n n subject to the fidelity constraint (374.6) gives images 
with the largest amount of uncertainty compatible with the data. These images will 
therefore show no features for which there is no clear evidence in the data (Gull and 

167 
Skilling 1985). This certainly is an attractive property for the image reconstruction. Very 
convincing results of the maximum entropy restoration have been obtained in radio as- 
tronomical interferometry (SElling and Gull 1985). As shown, the maximum entropy 
restoration is also obtained with the Bayesian approach, if (374.2) is chosen as prior dis- 
tribution. 
The computation of the MAP estimate ~ or the maximum entropy estimate by (374.5) 
has not been discussed yet. Since ~1 appears on both sides of the equation, an iterative 
procedure seems advisable. However, the exponential function may introduce instabili- 
ties into the iterative procedure so that smoothing needs to be applied. A different ap- 
proach maximizes the entropy H n subject to (374.6) numerically (Skilling and Gull 
1985). 

A Appendix 
In the Appendix the propertiers of several univariate and multivariate distributions are 
collected, which were referred to at various places in the text before. As has been prac- 
ticed already in Section 211, we will not distinguish in our notation a random variable 
from the values it takes on, but use the same letter for both quantities. We start with the 
univariate distributions and then continue with the multivariate distributions. 

170 
A1 Univariate Distributions 
A 11 Univariate Normal Distribution 
Definition: The random variable x is said to be normally distributed with the parameters 
# and a a, which is written x-N(#, a2), if its density function p(xl#, a 2) is given by 
p(x]#,a2 ) _ 
1 
e -(x-#)z/262 
for 
-~ < x < **. 
(All.l) 
v7 6 
It is obvious that the fn'st condition in (211.6) is fulfilled by the density function in 
(All.i). This holds true also for the second one with (Koch t988a, p.125) 
1 
~ e-(X'#)2/262dx = 1. 
(All.2) 
6 
-oo 
ff x-N(#, a2), then (Koch 1988a, p.138) 
E(x) = # 
and 
V(x)= 6 2. 
(All.3) 
The density function of a normally distributed random variable is therefore uniquely de- 
termined by its expected value and its variance. 
A12 Gamma Distribution 
Definition: The random variable x has the gamma distribution G(b,p) with the real-val- 
ued parameters b and p, thus x-G(b,p), if its density function is given by 
p(xtb,p) = b p x p'I e'bX/F(p) 
for 
b > O, p > O, 0 < x < 
and p(x [b,p)=O for the remaining values of x. 
(A12.1) 
It is obvious that p(x I b,p)>0, and by the definition of the gamma function F(p) it can 
be shown (Koch 1988a, p.130) that 
b p (F(p)) -1 x p-1 e "bx dx = 1. 
(A12.2) 
O 
Hence, (211.6) is fulfilled. 
With b=-1/2 and p=n/2 we find the distribution which is known as z2-distribution (Koch 
1988a, p.144). We write x~X2(n), if 

171 
1 
xn/2-1 e-X/2 
p(xln) = 2n/2F(n/2~ 
andp(xln)=Oforthe remaining values ofx. 
for 
0 < x < ~ 
(A12.3) 
If the random variable x has the gamma distribution x-G(b,p), the moment generating 
function Mx(t) of x is given by (Koch 1988a, p.131) 
Mx(t) = (1-t/b) -p 
for 
t < b. 
(A12.4) 
The f~stmoment E(x) and the second moment E(x2) of x follow with (Koch 1988a, 
p.123) 
Â°~lx(t) ] 
02Mx(t) [ 
E(x) = ~ 
and E(x2) - 
Ot 
t=O 
Ot2 
t=O 
and the variance V(x) of x with 
V(x) = E[(x-E(x))2] = E[x2-2xE(x)+(E(x))2] = E(x2)-(E(x))2. 
(AI2.5) 
Hence, 
O~x(t) 
at 
-~ 
(1- ~)-p-1, 
olMx(t) 
Ot 
t=O = ~ 
02Mx(t) _ P 
t -p-2 
02Mx(t) 
P 
(p+l)(1 - ~) 
, 
= -- 
0t2 
t=0 
b 2 (p+l) 
0t2 
b 2 
so that we obtain the expected value and the variance ofthe random variable x having 
the gamma distribution x--G(b,p) 
E(x) = p/b 
and V(x) = p/b2. 
(A12.6) 
A13 Inverted Gamma Distribution 
Theorem: If the random variable x has the gamma distribution x-G(b,p), then the ran- 
dom variable z with z=l/x has the inverted gamma distribution, z-IG(b,p), with the 
density function 
b p 
1 p+l e-b/z 
p(zlb,p) = ~ 
(~) 
for 
b > O, p > 0, 0 < z < 0o 
and p(zlb,p)=0 for the remaining values of z. 
(A13.1) 
Proof." With the transformation x=l/z and its Jacobian detJ=-l/z2 (Koch 1988a, p.108) 
we obtain instead of the density function p(x[b,p) of (A12.1) the density function 
p(zlb,p) of (A13.1). 
n 

172 
The expected value and the variance of a random variable z having the inverted gamma 
distribution z-IG(b,p) are given by 
E(z) = b/(p-1) 
for 
p > 1 
and 
V(z) = b2/((p-1)2(p-2)) 
for 
p > 2. 
(A13.2) 
The first result follows with F (p) = (p- 1 ) F ( p - 1 ) and 
E(z) 
~ 
,~t'Jb 
p 
(1)p+l e-b/z 
= 
Z 
~ 
dz 
0 
b 
= p_--S]- for 
p > 1, 
b 
~ 
b p-1 
(1)p e-b/z 
= 
F(p_--CTy 
dz 
since the integrant represents the density function of a random variable z 
z-IG(b,p-1). The second result follows similarly with 
E(z 2) = ~ z2 
bP 
,1,p+l e-b/z 
o 
F-~ 
t~) 
dz 
b2 
7 bp-2 
e -b/z 
= 
~ 
dz 
for 
p > 2 
(p- 1) (p-2) o 
and with (A12.5) 
with 
b 2 
b 2 
V(z) = (p-l)(p-2) 
- (p-1)~ 
for 
p > 2. 

173 
A2 Multivariate Distributions 
A21 Multivariate Normal Distribution 
Definition: The nxl random vector x=[x 1 ..... Xn] ' is said to have a multivariate nor- 
mal distribution N(/t,Z) with the nxl vector # and the nxn positive definite matrix Z as 
parameters, thus x-N (#, Z), if the density function p (x I/L, Z) of x is given by 
p(xl/x,Y.) = (2zc)-n/2(detZ)-I/2 exp[- Â½(x-p)'z-i(x-~t)]. 
(A21.1) 
Since Z is assumed to be positive definite, detZ>0 and p(x[#,Z)>0 follow. In addition 
we have (Koch 1988a, p.I36) 
7 ... 7 exp[- ~(x-/a)'z'l(x-/0]dXl...dx n = (2~r)n/Z(detZ)l/2, 
(A21.2) 
-~ 
-CO 
so that (211.6) is fulfilled. 
The density function of a normally distributed random vector is uniquely determined by 
its vector of expected values and its covariance matrix. This is due to the following theo- 
rem (Koch 1988a, p.138), 
Theorem: Let the random vector x be distributed according to x-N(/~,Z), then E(x)=# 
and D(x)=X 
(A21.3) 
A22 Multivariate t-Distribution 
Theorem: Let the kxl random vector z=[z I ..... Zk]" be distributed according to 
z-N(O,N-1) with the kxk matrix N being positive definite. Furthermore let the random 
variabte h be distributed according to h-z2(v) with v degrees of freedom. Let the ran- 
dom vector z and the random variable h be independent. Then the kxl random vector 
x= [ x 1 ..... Xk] ', which originates from the transformation 
xi = zi(h/v)'l/2 + #i 
for 
iE{1 ..... k}, 
has the multivariate t-distribution with the kxl vector/t from /~t=(# i ), the matrix N-1 
and 
v as parameters, 
abbreviated 
by x-t(#,N-l,v), 
if the density 
function 
p(xl/~,N-l,v) ofx is given by 

174 
vV/2F((k+v)/2)(detN) 1/2 
P(xl~,N'l,v) = 
~k/2F(v/2) 
(v+(x-#)'N(x-~))'(k+v)/2" 
(A22.1) 
Proof'Since z and hare independent, the joint density p(z,hlN-l,v) of z and h is ob- 
mined ~om(A12.3)and (A21.1)by (Koch 1988a, p.107) 
p(z,htN-l,v) = (2g)-k/2(detN) 1/2 exp(-z'Nz/2) 
2-v/2(F(v/2)) -1 h v/2"l exp(-h/2). 
The random vector z is now transformed by 
z i = (h/v)l/2(xi-#i) 
with 
0zi/0x i = (h/v) 1]2 
To compute the density function of the transformed variables, the Jacobian de t J of the 
transformation is needed (Koch 1988a, p.108). It is determined by 
k 
detJ = 1I (h/v)l/2= (h/v) k/2, 
i=l 
so that the density function p (x, h t~t, N- 1, v) follows with 
p(x,hl#,N-l,v) ---2-(k+v)/2 (vrc)-k/2 (F(v/2)) -1 (detN) 1/2 
h (k+v)/2-1 exp[- ~(l+(x-#)'N(x-/~)/v)h]. 
(A22,2) 
We compute the marginal density of x (Koch 1988a, p.t05) 
p(xl#,N-l,v) = 2-(k+v)/2 (vg)-k/2 (F(v/2)) "1 (detN) 1/2 
h (k+v)/2-1 exp(-Qh)dh 
O 
with 
Q = ~(l+(x-#)'N(x-~)/v). 
From (A12.2) we obtain 
7 h (k+v)/2-1 exp(-Qh)dh = F((k+v)/2)Q- (k+v)/2 
o 

175 
and therefore 
F((k+v)/2) (de tN) 1/2 
p(xl/2,N-l,v)= 
(w)k/2r(v/2) 
(I+(x-/2)'N(x-/2)/V) -(k+v)/2 
(A22.3) 
This expression gives, after multiplying numerator and denominator by v v/2, the density 
function of (A22.1), which completes the proof, 
n 
Example 1: We take the density of the multivariate t-distribution in the form of (A22.3) 
and set k=l, x=x,/2=g, N=f and obtain the density function 
F((v+I)/2) 
f 1/2 
f(x_#)2)_(v+l)/2 
p(x[#,l/f,v) = 
(v) 
(1 + 
(A22.4) 
v~ r(v/2) 
This is the density function of a random variable x having the univariate t-distribution 
t (#, 1 / f, v), thus x- t (#, 1/f, v). The standardized form of this distribution follows 
from the transformation of the variable x to z by 
z = Â¢'f(x-#). 
(A22.5) 
With dx/dz=l/qrf we obtain the density function 
F((v+l)/2) 
z2 - (v+l)/2 
P(ZlV) 
Vrv-Â£ F(v/2) (1 + V-) 
, 
(A22.6) 
which is the density function of a random variable z having the t-distribution t (v), also 
called Student's t-distribution, thus z-t(v) (Koch 1988a, p.154). The distribution 
(A22.1) is therefore the multivariate generalization of the t-distribution. 
A 
The first moment and the second central moment of a random vector having the multi- 
variate t-distribution shall be given next. 
Theorem: Let the kxl random vector x be distributed according to x-t (/2, N-1, v), then 
E(x) = /2 for 
v> 1 
and 
D(x) = v(v-2)-lN -1 
for 
v > 2. 
(A22.7) 
Proof." We start from the density function (A22.2), whose marginal density gives the 
multivariate t-distribution. Rearranging leads to 

176 
p(x,hl/~,N'l,v) = 2 -v/2 (r(v/2)) -1 h v/2-t exp(-h/2) 
(2re) -k/2 (detN) 1/2 (h/v) k/2 exp(- Â½(h/v)(x-/l)'N(x-/a)) 
= p(h) p(xl/l,(v/h)N'l), 
(A22.8) 
so that the multivariate t-distribution is obtained by computing the marginal density for x 
p(xlp, N-l,v) = ~ p(h) p(xl/t,(v/h)N-1)dh. 
o 
The expected value E(x) of x follows with (Koch 1988a, p. 111) 
E(x) -- ~ ... ~ ~ x p(h) p(xl,,(v/h)N'l)dh dXl...dx k 
-e~ 
-oo 
O 
and after changing the sequence of integration 
E(x) = ~ [ 7... 
~ x p(xl/t,(v/h)N-1)dXl ...dxk] p(h) dh. 
(A22.9) 
O 
-oo 
-oo 
The inner integral represents the expected value of a random variable x having the nor- 
mal distribution x-N(/t, (v/h)N-1), as can be seen by comparing (A22.8) with (A21.1). 
Thus, we obtain with (A21.3) 
E(x) = ~ /i p(h) dh 
O 
and with (A12.2) 
2"v/2(F(v/2))-I h v/2-1 e "h/2 dh = 1, 
O 
so that E(x)=/L follows for v>l (Zellner 1971, p.385). 
To obtain the covariance matrix D(x) of random vector x with a multivariate t-distribu- 
tion, we only have to write instead of (A22.9) (Koch 1988a, p.116) 
D(x) = 7 [ ~ ... 7 (x-lt)(x-p)'p(xl/~,(v/h)N-1)dXl...dXk ] p(h) dh. 
O 
-OO 
-oo 
The inner integral represents the covariance matrix of a random variable x having the 
normal distribution x-N(/1, (v/h)N-1), thus, with (A21.3) 
D(x) = ~ (v/h)N -1 p(h) dh 
O 
and with substituting from (A22.8) 

177 
D(x) = N -1 7 2 -v/2 (F(v/2)) "1 
o 
The integral(A12.2) gives 
vh v/2-2 exp(-h/2) dh. 
2-(v/2-1)(F(v/2-1))-lhV/2-2 exp(-h/2) dh = 1 
O 
and therefore 
D(x) = N-12 -v/2 (F(v/2))-lv 2 v/2-1F(v/2-1). 
With F(v/2)=(v/2-1)F(v/2-1) we finally obtain 
D(x) 
= v(v-2)-lR -1 
which proves the theorem. 
for 
v > 2, 
The marginal and the conditional distribution of the multivariate t-distribution follow by 
the 
Theorem: Let the kxl vector x be distributed according to x-t(/~,N-l,v). If x is parti- 
tioned into the (k-m)xl vector x 1 and the mxl vector x 2 with x=[x~ ,x~]" and accord- 
ingly/.t= [#~ ,//~]" as well as the matrix N 
R= [~: 
~:] 
with 
N-1 
Illl 
I12 t 
= ~I21 
I22 J ' 
then the random vector x 2 has a multivariate t-distribution, too 
x 2 -t(//.2,N21.1,v) 
with the marginal density function 
vv/2r((m+v)/2)(detN22.1 )1/2 
a~n/2F(v/2) 
(v+ (x2-P2) 'N22. i (x2-/*2))- (m+v)/2 
wherc 
R22.1 = R22-R21NilN12 
or 
R21.1 = I22. 
(A22.10) 

178 
The distribution of the random vector x 1 under the condition that the second random 
vector takes on the values x 2 has also the form of a multivariate t-distribution with the 
conditional density function 
v (m+v)/2F( ((k-m)+(m+v))/2) (de tR1.2 ) 1/2 
P (Xl Ix2 '/11.2' Ri !2' v, m+v) = 
a(k-m) /2F( (m+v) /2) 
(v+(xl -/11.2 ) 'N1.2(Xl -/11.2 ) )- ((k-m)+(m+v))/2, 
where 
/11.2 = /11 - Ni~N12(x2-/12 ) 
and 
R1.2 = Rlt/(l+(x2-P2)'N22.1 (x2-P'2)/v)" 
(A22.11) 
Proof: To derive the marginal density function for x 2 we have to integrate the density 
function of the multivariate t-distribution with respect m x 1. We take the density in the 
form of (A22.3), where for the sake of simplification we substitute R/v=-N with v>0 and 
obtain 
F((k+v)/2) (de tM) 1/2 
p(x[/1, (dl)- 1, v) = 
~k/2F(v/2) 
(l+(x-/1) 'M(x-/1) )" (k+v)/2 
By substituting the partitioning of (A22.10) we rewrite the quadratic form 
(x-/1)'M(x-/1) 
= (Xl-#l)'Mll(Xl-/11) 
+ 2(Xl-/11)'M12(x2-/12) 
+ (x2-/J2) "~22 (x2-/12) 
= (x1-/11+~lillM12(x2-/12) )'Mll (x1-/11+MilM12(x2-P-2)) 
+ (x2-tS)'M22.1(x2%) 
= Q1.2 + Q2 
with 
M22.1 = M22-M21MillM12 . 
The determinant of the block matrix M is computed by (Koch 1988a, p.45) 
detM = detMll detM22 .1. 
Substituting these results into the density function leads to 

179 
r((m+v)/2)(detM22 1 )1/2 
P(Xl,X21P,(vM)-l,v ) = 
- 
(l+Q2)-(m+v)/2 
n~a/2F(v/2) 
F((k+v)/2)(detMll )1/2 (I+Q2)-(k-m)/2(I+QI.z/(I+Q2)) -(k+v)/2 
~-~(-m)/2F((mÃ·v)/2) 
We now substitute M=N/v and obtain after multiplying numerator and denominator of the 
first factor by v v/2 and of the second factor by v (re+v)/2 
)1/2 
vV/2F((m+v)/2) (detN22 1 
- (m+v)/2 
P(Xl,X2lP, N -1 v) = 
" 
(v+vQ2) 
" 
nm/2F(v/2) 
v (m+v)/2F((k+v)/2) [de t (N 11 / (l+Q2) ) ] 1/2 
- 
~(k-m)/2F(0n+v)/2) 
(v+vQ1 â¢ 2/(l+Q2) ) - (k+v)/2. 
The first factor gives the marginal density function of x 2 and the second factor the con- 
ditional density function of x I given x 2 because of 
P(X 1 ,x2lP, N'l,v) = P(x2l/~,N-1 ,v)p(x 11x2,~,N- 1,v,m+v) 
from (261.1). With the inverse of a block matrix (Koch 1988a, p.39) we obtain 
N22 t " 1=I22 . This proves the theorem. 
[] 
The distribution of a random vector which originates from a linear transformation of a 
random vector with the multivariate t-distribution is derived next. 
Theorem: Let the kxl vector x be distributed according to x-t (N,N "1 ,v), then the raxl 
random vector y obtained by the linear transformation y=Ax+e, where A denotes an rr~k 
matrix of constants with full row rank and c an mxl vector of constants, has the multi- 
variate t-distribution 
y- t(A/L+c,AN-1A',v) 
with the density function 
p(ylA/~c,AN-IA',v) 
= 
vV/2F((m+v)/2) (de tAN- 1A" ) - 1/2 
nm/2r(v/2) 
(v+ (y-A/a- c) ' (AN- 1A" )" 1 (y-A#- c) ) - (m+v)/2 
(A22.12) 

180 
Proof: We have to distinguish two cases, 
a) m=k. 
Since A is assumed to be of full row rank, A" 1 exists and 
x = A-l(y-c). 
The Jacobian of this transformation is given by detl=detA -1 (Koch 1988a, p.85). Intro- 
ducing it together with x-g-=A-1 (y-A/~-c) into the density function of (A22.1) gives the 
density (A22.12) because of detA- 1 (de t N) 1/2= (de t/~- 1 A, ) - 1/2 
b) m<k. 
Because of the full row rank of the mxk matrix A we may add k-m linearly independent 
rows to A, collected in the matrix B, so that the regular matrix C with e=[A' ,B']' is ob- 
tained. If the vector c is also augmented by a (k-m)xl vector d of constants, so that 
e=[c',d']' follows, we find by the linear transformation the random vector u=[y',z']' 
having according to case a) the multivariate t-distribution 
u- 
t(Qt+e,CN-lc',v). 
The marginal distribution of y has to be determined, whose parameters according to 
(A22.10) follow with A/~+c and the first block matrix on the diagonal of the matrix 
CN-1C', which is AN-1A'. But this gives the density (A22.12), so that the theorem is 
proved. 
[] 
Finally we will present the relation between the multivariate t-distribution and the F-dis- 
tribution. 
Theorem: Let the 1o(1 vector x be distributed according to x-t (It,N" 1, v), then the quad- 
ratic form (x-p) 'N(x-/L)/k has the F-distribution F(k, v) with k and v degrees of free- 
dom 
(x-It) 'N(x-/t)/k - F(k, v). 
(A22.13) 
Proof." We will apply the transformation 
y = A-l(x-#), 
where A is a regular kxk matrix with the property A'NA=I, so that N-I=M~ ' holds. Then 
the density of the multivariate t-distribution is transformed because of (A22.12) to the 
standardized form 

181 
vV/2F((k+v)]2) 
p(y[O,I v) = 
(v+y'y) -(k+v)/2 
' 
gk/2r(v/2) 
(A22.14) 
We make a transformation to polar coordinates with 
Yl = 4?- cosalcosa 2 ... cosak_ 1 
Y2 = qÂ¥ cÂ°salcÂ°sa2 "'" cÂ°sak-2sinak-1 
Yj = qÂ¥ cosalcosa 2 ... cosak_jsinak_j+ 1 
â¢ 
. 
, 
Yk = q~ sinal' 
where 
0 < r < ~, 0 < ak_ 1 < 2z, -z/2 < a i < z/2 
for 
ie{1 ..... k-2}, 
and obtain 
r = y'y = y~ + y~ + ... + y~. 
The Jacobian of this transformation follows from the kxk matrix J, whose jth row con- 
tains the elements 0yj/Or, 0yj/Oa 1 ..... 0yj/Oak_ 1" Furthermore, by multiplication we 
find 
J'J = diag(dl,d 2 ..... d k) 
= diag((4r) -1 
therefore 
k 
det(J'J) = I] d i 
i=l 
and finally 
,r,rcos2al,rcos2alCOS2a 2 ..... rcos2al...cos2ak.2), 
k 
or 
detJ --- l] 
i=l 
" 
1 
(1/2)rk/2-1cosk-2alcosk-3a2 
... cosak_ 2. 
detJ 
Substituting these results givesinsteadof(A22.14) 
p(r,a 1 .... ,ak_llk,v) = detJ 
vV/2F((k+v)/2) 
~/2F(v/2) 
(v+r)'(k+v)/2. 
The marginal distribution of r is found by integrating over a i . We start from the integral 
(Gradshteyn and Ryzhik 1965, p.369) 

182 
gt 2 
2k-j-2 F(~_~)F(9)/F(k. j ) 
cosk-J- lad a = 
o 
In addition we have 
gt 2 cosk-j-lada" 
tot2 
= 2 
cosk--'lada, 
j 
-1r/2 
o 
and using the recursion formula of the gamma function we fred with F( 1/2)=4-~ 
r(k'~ +1) F(9) 
= 4~ V(k-j)/2 k'j-1, 
so that we compute the integrals 
-~r/2 cÂ°sk-j'lajdaj = q~ F(~J-)/F(~) for 
js{1 ..... k-2} 
and 
27f 
f 
dak_ 1 = 2~c. 
0 
These results give the density 
p(rlk,v ) = g(k-2)/2vV/2F((k+v)/2) r k/2-1 (v+r) 
n-l~/2r(k/Z)r(v/2) 
-(k+v)/2 
We finally transform from r to w with r=kw and dr/dw=k, so that the density is obtained 
F((k+v)/2)kk/2vV/2wk/2-1 
p(w[k,v) 
F(k/2)F(v/2)(v+kw)(k+v)/2 for 
0 < w < ~. 
This is the density function of the F-distribution F(k, v) with k and v degrees of freedom 
(Koch 1988a, p.149). Because of w=y'y/k=(x-#) 'N(x-/~)/k the theorem is proved. 
[] 
A23 Normal-Gamma Distribution 
Theorem: Let x be an nxl random vector with x=(x i ) and "c a random variable. Let the 
distribution of x under the condition that v takes on the value v be given by the normal 
distribution N(#, ~" 1V) with the nxl vector # and the positive definite nxn matrix ~:- 1V 
as parameters, hence x[ "r~N(/t, "c-lV), and let the random variable v have the gamma 
distribution G(b,p) with b and p as parameters, thus v-G(b,p). Then the random vector 

183 
z=[x', "r]' is said to have the normal-gamma distribution NG(/t,V,b,p) with the param- 
eters #, V, b, p, hence x, ~:-NG(#, V, b, p), and the density function given by 
p(x, ~cl/a,V,b, p) = (2~) "n/2(detV) - 1/2bP(F(p) ) - 1 
v n/2+p- lexp{- ~[2b+(x-/t) 'V- l(x-#)] } 
forb>0, p>0,0<T<~, 
-~<x i<~. 
(A23.1) 
Proof: The conditional density function p(xl#,~:-lv) of x given v is obtained by the 
density (A21.1) of the multivariate normal distribution and the density function 
p('clb,p) of "c by the density (A12.1) of the gamma distribution. The joint density of x 
and "r follows from (211.2) with 
p(x,~l/~,V,b,p) = p(xl#,~:'lv)p(~:lb,p), 
(A23.2) 
which immediately leads to the density function (A23.1) of the normal-gamma distribu- 
tion. 
n 
The marginal distributions of x and "r shall be given next. 
Theorem: Let the vector z with z=[x' ,z]' have the normal gamma distribution x, z- 
NG(/~, V, b, p), then the marginal distribution of x is the multivariate t-distribution 
x- 
t(/~,bV/p,2p) 
(A23.3) 
and the marginal distribution of ~: is the gamma distribution 
~: - G(b,p). 
(A23.4) 
Proof." To obtain the marginal distribution of x the variable "c has to be integrated out of 
the density function of (A23.1). We find with (A12.2) 
v n/2+p- lexp{- ~[2b+(x-#) 'V- 1 (x-/a) ] }dz 
O 
= F(n/2+p) {~[2b+(x-/~) "V- 1 (x_N) ] } -n/2-Po 
Substituting this result in (A23.1) gives 
p(x]#,bg/p,2p) = (2b) p (z0-n/2(detV)-l/2(F(p))-i 
F(n/2+p) (2b+(x-/~) 'g- 1 (x-/t)) -n/2-p 

184 
F((n+2p)/2) (de t2pV- 1/2b) 1/2 
= 
(l+(x-g) "(2pV- 1/2b) (x-g)/2p) - (n+2p)/2 
(2p~r) n/2F(p) 
This is, according to (A22.3), the density function of the multivariate t-distribution 
t (x[g, bV/p,2p). 
The density of the normal-gamma distribution is obtained by (A23.2), where 
p(xl/L,~'lv) is the normal distribution. Hence, integrating with respect to x gives 
p (~lb, p) as the marginal distribution for ~, which proves the theorem, 
n 

References 
Aarts EHL, Laarhoven PJM van (1987) Simulated annealing: a pedestrian review of 
the theory and some applications. In: Devijver PA, Kittler J (eds) Pattern rec- 
ognition theory and applications, Springer, Berlin Heidelberg New York 
Tokyo, pp 179-192 
Abramowitz M, Stegun IA (ed) (1965) Handbook of mathematical functions. Dover, 
New York 
Akaike H (1979) A Bayesian extension of the minimum AIC procedure of autore- 
gressive model fitting. Biometrika, 66:237-242 
Berger JO (1985) Statistical decision theory and Bayesian analysis. Springer, Berlin 
Heidelberg New York Tokyo 
Berger JO, Sellke T (1987) Testing a point null hypothesis: the irreconcilability of 
P values and evidence. J Am Stat Assoc 82:112-t22 
Bosch K (1985) Elementare Einftihmng in die angewandte Statistik. Vieweg, 
Braunschweig 
Bossier JD (1972) Bayesian inference in geodesy. Dissertation, The Ohio State Uni- 
versity, Columbus 
Bossier JD, Hanson RH (1980) Application of special variance estimators to geode- 
sy. NOAA Technical Report NOS 84 NGS 15, US Department of Commerce, 
National Geodetic Survey, Rockville 
Box GEP, Jenkins GM (1970) Time series analysis. Holden-Day, San Francisco 
Box GEP, Tiao GC (1973) Bayesian inference in statistical analysis. Addison-Wes- 
ley, Reading 
Broemeling LD (1985) Bayesian analysis of linear models. Dekker, New York 
Bunke H, Bunke O (ed) (1986) Statistical inference in linear models, vol.I. Wiley, 
New York 
Busch A, Koch KR (1990) Reconstruction of digital images using Bayesian esti- 
mates. Z Photogrammetrie und Fernerkundung, 58, in print 
Casella G, Berger RL (1987) Reconciling Bayesian and frequentist evidence in the 
one-sided testing problem. J Am Stat Assoc 82:106-111 
Caspary W (1988) Fehlerverteilungen, Methode der kleinsten Quadrate und robuste 
Alternativen. Z Vermessungswes 113:123-133 

186 
Cramer H (1946) Mathematical methods of statistics. Princeton University Press, 
Princeton 
DeGroot MH (1970) Optimal statistical decisions. McGraw-Hill, New York 
Dillinger WH, Pope AJ, Harding ST (1971) The determination of focal mechanisms 
using P- and S-wave data. NOAA Technical Report NOS 44, US Department 
of Commerce, National Ocean Survey, Roclccille 
Dowson DC, Wragg A (1973) Maximum-entropy distributions having prescribed 
first and second moments. IEEE Transactions on Information Theory, IT-19: 
689-693 
Eeg J, Krarup T (1973) Integrated geodesy. The Danish Geodetic Institute, Int Rep 
7, Kobenhavn 
Ftrsmer W (1988) Statistische Verfahren fiir die automatische Bildanalyse und ihre 
Bewertung bei der Objekterkennung und -vermessung. Habilitationsschrift, 
Institut fiir Photogrammetrie der Universitat Stuttgart, Stuttgart 
Friihwirth R, Regler M (1983) Monte-Carlo-Methoden. Bibliograph Inst, Mannheim 
Geisser S (1964) Posterior odds for multivariate normal classifications. J Royal Stat 
Soc, Set B, 26:69-76 
Geisser S (1965) Bayesian estimation in multivariate analysis. Ann Math Statist 36: 
150-159 
Geman D, Geman S, Graffigne C (1987) Locating texture and object boundaries. In: 
Devijver PA, KittlerJ (eds) Pattern recognition theory and applications, 
Springer, Berlin Heidelberg New York Tokyo, pp 165-177 
Geman S, Geman D (1984) Stochastic relaxation, Gibbs distributions, and Bayesian 
restoration of images. IEEE Trans Pattern Anal Machine Intell, PAMI-6: 
721-741 
Gradshteyn IS, Ryzhik IM (1965) Table of integrals, series, and products. Academic 
Press, New York London 
Guiasu S (1977) Information theory with applications. McGraw-Hill, New York 
Gull SF, Skilling J (1985) The entropy of an image. In: Smith CR, Grandy WT 
(eds) Maximum-entropy and Bayesian methods in inverse problems, Reidel, 
Dodrecht, pp 287-301 
Hammersley JM, Handscomb DC (1964) Monte Carlo methods. Methuen, London 
Hampel FR (1974) The influence curve and its role in robust estimation. J Amer 
Statist Assoc 69:383-393 
Hampel FR, Ronchetti EM, Rousseeuw PJ, Stahel WA (1986) Robust statistics. 
Wiley, New York 

187 
Harvey BR (1987) Degrees of freedom-simplified. Aust J Geod Photogram Surv 46 
and 47:57-68 
HeinGW (1986) Integrated geodesy, state-of-the-art 1986 reference text. In: 
Siinkel H (ed) Lecture Notes in Earth Sciences, vol.7, Mathematical and nu- 
merical techniques in physical geodesy, Springer, Berlin Heidelberg New York 
Tokyo, pp 505-548 
Hoaglin DC, Mosteller F, Tukey JW (1983) Understanding robust and exploratory 
data analysis. Wiley, New York 
HuberPJ (1977) Robust methods of estimation of regression equations. Math 
Operationsforsch Statist, Ser. Stat 8:141-153 
Huber PJ (1981) Robust statistics. Wiley, New York 
Jaynes ET (1986) Bayesian methods: general background. In: Justice JH (ed) Maxi- 
mum entropy and Bayesian methods in applied statistics. Cambridge Univ 
Press, Cambrigde, pp 1-25 
Jazwinski AH (1970) Stochastic processes and filtering theory. Academic Press, 
New York London 
Jeffreys H (1961) Theory of probability. Clarendon, Oxford 
Jorgensen PC, Kubik K, Weng PFW (1985) Ah, robust estimation. Aust J Geod 
Photogram Surv 42:19-32 
Koch KR (1982) Kalman filter and optimal smoothing derived by the regression 
model. Manuscripta Geodaetica 7:133-t44 
Koch KR (1984) Statistical tests for detecting crustal movements using Bayesian 
inference. NOAA Technical Report NOS NGS 29, US Department of Com- 
merce, National Geodetic Survey, Rockville 
Koch KR (1985) Ein statistisches Auswerteverfahren fiir Deformationsmessungen. 
Allg Vermess Nachr 92:97-108 
Koch KR (1986) Maximum likelihood estimate of variance components, ideas by 
A.J. Pope. Bull Geod 60:329-338 
Koch KR (1987) Bayesian inference for variance components. Manuscripta Geo- 
daetica 12:309-313 
Koch KR (1988a) Parameter estimation and hypothesis testing in linear models. 
Springer, Berlin Heidelberg New York Tokyo 
Koch KR (1988b) Bayesian statistics for variance components with informative and 
noninformative priors. Manuscripta Geodaetica 13:370-373 
Koch FaR (t988c) Konfidenzintervalle der Bayes-Statistik fiir die Varianzen von 
Streckenmessungen auf Eichlinien. Vermessung Photogrammetrie Kultur- 
technik 86:337-340 

188 
Koch KR (1989) Bayes-Statistik mittets Monte-Carlo-Integration. Z Vermessungs- 
wes 114:302-310 
Koch KR, Riesmeier K (1985) Bayesian inference for the derivation of less sensi- 
tive hypothesis tests. Bull Geod 59:167-179 
Kubik K (1970) The estimation of the weights of measured quantities within the 
method of least squares. Bull Geod 95:21-40 
Lindley DV (1957) A statistical paradox. Biometrika 44:187-192 
Lindley DV (1965) Introduction to probability and statistics, Part 1 and Part 2. 
Cambridge Univ Press, Cambridge 
Moritz H (1980) Advanced physical geodesy. Wichmann, Karlsruhe 
Pilz J (1983) Bayesian estimation and experimental design in linear regression mod- 
els. Teubner, Leipzig 
Press SJ (1982) Applied multivariate analysis. Krieger, Malabar 
Press SJ (1989) Bayesian statistics: principles, models, and applications. Wiley, 
New York 
Raiffa H, Schlaifer R (1961) Applied statistical decision theory. Graduate School of 
Business Administration, Harvard University, Boston 
Rao CR (1973) Linear statistical inference and its applications. Wiley, New York 
Riesmeier K (1984) Test von Ungleichungshypothesen in linearen Modellen mit 
Bayes-Verfahren. Dtsch Geod Komm C, 292, Miinchen 
Ripley BD (1988) Statistical inference for spatial processes. Cambridge Univ Press, 
Cambridge 
Rubinstein RY (1981) Simulation and the Monte Carlo method. Wiley, New York 
Schaffrin B (1987) Approximating the Bayesian estimate of the standard deviation 
in a linear model. Bull Geod 61:276-280 
Schaffrin B (1989) An alternative approach to robust collocation. Bull Geod 63: 
395-404 
Schwarz KP (1983) Inertial surveying and geodesy. Rev Geophys Space Plays 21: 
878-890 
Shafer G (1982) Lindley's paradox. J Am Stat Assoc 77:325-334 
Skilling J, Gull SF (1985) Algorithms and applications. In: Smith CR, Grandy WT 
(eds) Maximum-entropy and Bayesian methods in inverse problems. Reidel, 
Dodrecht, pp 83-132 
Somogyi J (1988) Robust estimation and their use in geodesy. Acta Geod Geoph 
Mont Hung 23:45-53 
Spitzer F (1971) Markov random fields and Gibbs ensembles. Am Math Mon 78: 
142-154 

189 
Stearns SD (1975) Digital signal analysis. Hayden, Rochelle Park 
Teunissen PJG (1987) The 1 and 2D symmetric Helmert transformation: an exact 
non-linear least-squares solution. Reports of the Faculty of Geodesy, Delft Uni- 
versity of Technology, Delft 
Teunissen PJG (1988) The non-linear 2D symmetric Helmert transformation: an ex- 
act non-linear least-squares solution. Bull Geod 62:1-15 
Theil H (1963) On the use of incomplete prior information in regression analysis. J 
Am Stat Assoc 58:401-414 
Ulrych TJ, Bishop TN (1975) Maximum entropy spectral analysis and autoregres- 
sive decomposition. Rev Geophys Space Phys 13:183-200 
Wei M (1987) Statistical problems in collocation. Manuscripta Geodaetica 12: 
282-289 
West M, Harrison J (1989) Bayesian forecasting and dynamic models. Springer, 
Berlin Heidelberg New York Tokyo 
Wolf H (1968) Ausgleichungsrechnung nach der Methode der kleinsten Quadrate. 
Diimmler, Bonn 
Zellner A (1971) An introduction to Bayesian inference in econometrics. Wiley, 
New York 
Ziqiang O (1990) Bayes and emperical Bayes estimation for variance components. 
Allg Vermess Nachr-Internat Ed 7:25-31 

Index 
acceptance of a hypothesis, 41,43,46,67,88 
altematwe hypothesis, 40,43,46,67,80,88 
anneahng, 162 
approximate computation, 58,107,154 
autoregressave model, 84 
backward filter, 96 
Bayes estamate, 34,54,65,70,74,79,94,104,113,120,126,134,153,158,161 
- esnmator, 33 
- rule, 33,43,46 
- theorem, 4,8,27,54,64,69,78,85,93,102,111,126,133,136,146,160 
Bayesian reference, 1,3 
eharactensnc, 135 
Cholesky factonzauon, 62,114 
classlficanon, 135 
collocanon, 109,117 
composite hypothesis, 40,43 
con&nonal dens:ty function, 4,49,178,183 
confidence hyperelhpsold, 38,67 
- region, 37,41,67,80 
- -, Bayesian, 37 
conjugate pnor, 25,69,100 
constraint, 18,31 
coordinates of points, 81,83 
covanance component, 122,126,133 
- matrix, 28,34,62,65,70,74,79,86,93,99,111,113,122,138,156 

192 
data, 4,10,25,33,49,100,115,118,142 
- translated, 10,146 
datum of a free network, 81 
density functmn, 4,8,11,27,31,33,37,41,45,49,56,64,74,78,85,94,102,126,137,146,149, 
160,165,178,183 
- -, conchtaonal, 4,49,178,183 
- -, poster/or, 5,8,11,27,31,33,37,41,45,49,56,64,74,78,85,94,102,126,137,146,149,160, 
165 
- -, prior, 5,8,9,17,22,26,45,50,64,69,78,84,94,102,111,124,132,139,160,165 
chgltal image, 142,156 
d~scnrmnant function, 137 
dlstnbutmn, 5,6,10,17,23,38,56,65,70,79,85,94,111,123,133,146,157,170,173 
exponennal, 17,21 
- 
F-, 38,67,80,89,180 
- gamma, 28,65,72,96,114,132,170 
- Gibbs, 160 
- mverted gamma, 28,65,72,114,132,171 
- marginal, 5,28,56,58,65,70,79,85,102,113,130,161,177 
- 
multivariate normal, 27,63,84,93,99,111,123,135,157,173 
- multwanate t-, 28,38,65,70,77,79,113,141,173 
- 
normal, 6,10,17,27,50,63,84,93,99,111,123,135,157,170 
- 
normal-gamma, 26,65,69,77,78,86,97,111,183 
- posterior, 5,54,65,69,74,94,111,117,118,133,146,149 
- predlctave, 49,74,116,150 
-, prior, 5,9,17,23,93,139,158 
-, t-, 68,121,175 
-, umform, 17,53 
-, unlvanate, 170 
-, Wlshart, 139 
dynarmc model, 98 
- system, 93 
elgenvalue, 125 
elgenvector, 125 
energy, 159 
entropy, 15,17,133,166 

193 
-, contmuous, 16 
-, discrete, 16 
-, maximum, 15,17,133,166 
error funcnon, 23 
- vector, 62,74,109,110 
esumate, 26,34,54,65,70,74,79,94,104,113,120,126,134,138,144,153,158,161 
-, maximum hkehhood, 35,65,124,144 
expected value, 5,12,29,34,50,52,62,65,73,83,97,99,111,122 
exponential dlstnbuuon, 17,21 
factonzauon theorem, 25 
F-dastnbutlon, 38,67,80,89,180 
filter, 95 
-, backward, 96 
-, Kalman-Bucy, 95 
filtenng, 109,115,118 
-, model of, 109,118 
Ftsher" s mformauon matrix, 11 
Founer series, 83 
free network, 81 
gamma dlsmbuuon, 28,65,72,96,114,132,170 
Gauss-Markoff model, 40,62,71 
generahzed inverse, 78,80 
- maximum hkehhood estimate, 35 
geodettc network, 81 
Gibbs chsmbunon, 160 
gray level, 142,156,158,159,165 
Hessian normal form, 104 
highest posterior density region, 37 
H P.D region, 37 
hypothesis, 40,43,46,55,67,80,88 
-, altemauve, 40,43,46,67,80,88 

194 
-, composite, 40,43 
-, null, 40,43,46,67,80,88 
-, point null, 40,43,46,67 
-, sample, 40,43 
- test, 40,43,47,67,80,88 
- -, less sensmve, 88 
image processing, 156 
Importance samphng, 53 
information, 15 
- criterion, 85 
- matrix, 11 
mvarlance property, 11 
inverted gamma dasmbutaon, 28,65,72,114,132,171 
Jacoblan of a transformation, 14,57,171,174,181 
Jeffrey' s mvanance pnnclpte, 11,64,124 
Kalman-Bucy filter, 95 
Lagrange function, 18,166 
- mulnpher, 18,166 
least squares, 65,99,100,104 
- - collocation, 109,117 
less sensitive hypothesis test, 88 
hkehhood funcuon, 5,11,26,63,84,93,100,101,111,123,146,157 
Llndley' s paradox, 47 
hne element, 162 
hnear dynamic system, 93 
- model, 62,73,77,88,93,122,135,147,157 
- - not of full rank, 77,88 
loss, 33,42,136 

195 
l~tahalanobls distance, 137 
MAP estamate, 36,160 
marginal dlstnbuuon, 5,28,56,58,65,70,79,85,102,113,130,161,177 
Markoff random field, 160 
matrix identity, 70,76 
maximum a postenon estimate, 36,160 
- entropy, 15,17,133,166 
- - prior, 15,17,133 
- - restoration, 165 
- hkehhood estimate, 35,65,124,144 
measurement, 4,49,83,119,130 
medsan, 35 
M-estimate, 144,145,148 
rmmmum distance classifier, 138 
mlsclasslficauon, 136 
mixed model, 109,111 
mode, 35 
model, 61,62,73,77,83,88,93,99,101,109,111,122,135,145,147,156 
-, autoregresswe, 84 
-, dynamic, 98 
-, Gauss-Markoff, 40,62,71 
- ldentlficanon, 83 
-, hnear, 62,73,77,88,93,122,135,147,157 
-, mixed, 109,111 
-, nonhnear, 99,101,145,156 
- not of full rank, 77,88 
- of predxctlon and filtenng, 109,118 
-, polynomial, 83 
- w~th unknown variance and covanance components, 122 
Monte Carlo mtegrataon, 53,54,56,58 
- -, crude, 53 
- -, sample-mean, 53 
multivariate dlstrlbutlon, 27,38,63,70,84,93,99,111,123,135,141,157,173 
- normal distribution, 27,63,84,93,99,111,123,135,157,173 
- t-dlsmbuuon, 28,38,65,70,77,79,113,141,173 

196 
natural conjugate prior, 25,69 
nonlnformatlve prior, 9,22,64,78,84,124,146 
nonhnear model, 99,101,145,156 
normal chstrxbuuon, 6,10,17,27,50,63,84,93,99,111,123,135,157,170 
- -gamma dlsmbutaon, 26,65,69,77,78,86,97,111,183 
- -Wlshart chstnbunon, 139 
- -, multivariate, 27,63,84,93,99,111,123,135,157,173 
- -, truncated, 17,23 
normahzanon constant, 5,18,54,126 
null hypothesis, 40,43,46,67,80,88 
observanon, 4,8,26,49,62,73,77,84,93,99,110,115,118,122,127,135,145,156 
- equanon, 62,71 
- vector, 4,8,26,49,62,73,77,84,93,99,110,115,118,122,127,135,145,156 
outher, 144,145,150 
parameter, 4,9,26,33,40,45,49,54,56,62,64,69,78,83,88,93,99,110,122,135,145,150 
-, location, 145 
-, precxslon, 26,63 
-, projected, 78,88 
-, scale, 145 
- space, 4,31,37,40,54,56,88 
-, unknown, 4,9,26,33,62,64,78,83,88,93,99,110,122,145,156 
- vector, 4,26,33,40,45,49,54,56,62,64,69,78,83,88,93,99,11 O, 122,135,145,156 
-, weight, 26,63,65,96,114 
percentage point, 38,67,89 
plxel, 142,156,158,159,165 
point estlmauon, 33 
- null hypothesis, 40,43,46,67 
polynomial model, 83 
populanon, 135,138,142 
posterior density, 5,8,11,27,31,33,37,41,45,49,56,64,74,78,85,94,102,126,137,146,149 
160,165 
- dlstnbuuon, 5,54,65,69,74,94,111,117,118,133,146,149 
- expected loss, 33,42,136 

197 
- odds, 43,46,56 
- probablhty, 42,46,55 
precision parameter, 26,63 
prechctxon, 109,115,118 
- error, 85 
predxctlve dastnbutaon, 49,74,116,150 
prior density, 5,8,9,17,22,26,45,50,64,69,78,84,94,102,111,124,132,139,160,165 
- -, conjugate, 25,69,100 
- -, maximum entropy, 15,17,133 
- -, nonmformatlve, 9,22,64,78,84,124,146 
- -, vague, 9 
- dastnbuuon, 5,9,17,23,93,139,158 
- -, natural conjugate, 25,69 
probability, 11,14,16,31,41,42,46,55,90,160 
-, posterior, 42,46,55 
projected parameter, 78,88 
quadratic form, 38,1 t 1 
- loss, 33 
recurslve apphcatlon, 8,93 
reflexxve generahzed reverse, 78 
regressxon analysis, 62 
rejecuon of a hypothesis, 41,46,67,88 
robust, 71,144,145,148 
- collocauon, 71 
- maximum hkehhood type estimate, 144,145,148 
- M-estimate, 144,145,148 
mgnal, 109,118 
slgmficance level, 48 
simple hypothesis, 40,43 
smoothing, 96 
state vector, 93 

198 
straight hne, 5,100,151 
Student ' s t-dastnbut~on, 121,175 
sufficient stattsuc, 25 
systemauc part, 109,118 
t-dasmbutlon, 68,121,175 
- -, multivariate, 28,38,65,70,77,79,1 I3,141,173 
test, 40,43,47,67,80,88 
transformauon of variables, 11,13,56,171,174,181 
transmon mamx, 93 
trend, 109,118 
truncated normal dasmbuuon, 17,23 
Type I error, 41 
uncertainty, 15,23,166 
uniform dxsmbutxon, 17,53 
umvanate &smbut~on, 170 
vague prior density, 9 
variance, 5,9,17,29,34,50,66,72,87,97,114,132 
- component, 2,122,126,133 
- factor, 26,62,93,96,99,110,156 
- of umt weight, 62,99 
weight, 7,30,62,93,99,138,145,156 
- mamx, 62,93,99,138,145,156 
- parameter, 26,63,65,96,114 
Wlshart dlstnbutaon, 139 

